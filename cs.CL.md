# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Controlling Topic-Focus Articulation in Meaning-to-Text Generation using Graph Neural Networks.](http://arxiv.org/abs/2310.02053) | 本研究提出了一种使用图神经网络控制意义到文本生成中的话题-焦点表达的方法，通过在意义表示中添加语用信息来强制使用主动或被动语态。研究采用了一种新的节点聚合编码策略，通过使用深度学习表示节点，来解决了意义中词序信息缺失的问题。 |
| [^2] | [Tuning Large language model for End-to-end Speech Translation.](http://arxiv.org/abs/2310.02050) | 本文介绍了一个名为LST的大型多模态模型用于端到端语音翻译任务。该模型通过两个训练阶段进行模态调整和下游任务微调，优化了翻译性能。 |
| [^3] | [Jury: A Comprehensive Evaluation Toolkit.](http://arxiv.org/abs/2310.02040) | 评委是一个统一的评估框架和工具包，旨在解决深度学习中不同系统和指标的评估挑战，并改进度量评估。 |
| [^4] | [OceanGPT: A Large Language Model for Ocean Science Tasks.](http://arxiv.org/abs/2310.02031) | OceanGPT是首个专为海洋科学任务设计的大型语言模型，通过DoInstruct框架实现自动获取海洋领域指导数据。这一模型的引入填补了海洋科学领域中对LLM的需求缺口，并为海洋科学研究提供了新的工具和方法。 |
| [^5] | [Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems.](http://arxiv.org/abs/2310.01991) | 本文探讨了LLM在数学应用题中的逆向推理能力，发现在逆向推理任务上，LLM模型的准确性显著下降。通过改进技术，如Rephrase和PAL-Tools，我们提高了模型的性能。 |
| [^6] | [Language Models as Knowledge Bases for Visual Word Sense Disambiguation.](http://arxiv.org/abs/2310.01960) | 本文提出了一种使用大型语言模型作为知识库的方法，通过在视觉词义消歧任务中使用适当的提示，以零样本方式检索存储在语言模型中的知识，从而提高视觉词义消歧的检索性能。 |
| [^7] | [Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving.](http://arxiv.org/abs/2310.01957) | 这篇论文介绍了一种融合对象级多模态LLM架构的方法，通过将向量化的数字模态与预训练的LLM相结合来提高自动驾驶中对上下文的理解能力。同时还使用了一个新的数据集进行评估，证明了LLM驱动程序在解释驾驶情境、回答问题和决策方面的效果优于传统的行为克隆方法。 |
| [^8] | [Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models.](http://arxiv.org/abs/2310.01929) | 本研究旨在探索和解锁文本到图像模型的文化视角，通过对TTI模型中嵌入的文化感知进行评估，揭示了这些模型的文化意识、文化区别和文化适应性。 |
| [^9] | [Hierarchical Evaluation Framework: Best Practices for Human Evaluation.](http://arxiv.org/abs/2310.01917) | 这篇论文提出了一个分层评估框架，解决了自然语言处理中人工评估指标不统一的问题，并应用于机器阅读理解系统的评估，突出了输入与输出质量之间的关联。 |
| [^10] | [Ring Attention with Blockwise Transformers for Near-Infinite Context.](http://arxiv.org/abs/2310.01889) | 本论文提出了一种新颖的环形注意力方法，通过分块计算和通信重叠的方式处理长序列，解决了Transformer在处理长序列时的内存限制问题。实验证明该方法能够有效地消除单个设备对内存的约束，使得训练和推理的序列长度能够更长。 |
| [^11] | [Effective and Parameter-Efficient Reusing Fine-Tuned Models.](http://arxiv.org/abs/2310.01886) | 本文提出了一种有效且参数高效的方法，可以重复使用微调模型来处理下游任务，减轻存储和服务负担，并提出了PERU-FFT方法用于重复使用全面微调模型。 |
| [^12] | [Fine-tuned vs. Prompt-tuned Supervised Representations: Which Better Account for Brain Language Representations?.](http://arxiv.org/abs/2310.01854) | 本研究比较了精调和提示调整的表示在神经解码中的效果，发现在10个NLU任务中，精调表示不能更好地解码人脑中的信息。 |
| [^13] | [Benchmarking and Improving Generator-Validator Consistency of Language Models.](http://arxiv.org/abs/2310.01846) | 本文提出了一种衡量生成和验证之间一致性的框架，发现目前最先进的语言模型GPT-4仅在76%的情况下是一致的。为了改进一致性，提出了一种通过经过筛选的生成器和验证器答案进行微调的方法，并将其称为一致性微调。该方法将Alpaca-30B的一致性从60%提高到93%，并且对未见过的任务和领域也具有泛化能力。除了改进一致性外，一致性微调还带来了其他性能的提升。 |
| [^14] | [Zero-Shot Refinement of Buildings' Segmentation Models using SAM.](http://arxiv.org/abs/2310.01845) | 本文提出了一种使用SAM进行建筑物分割模型的零-shot细化的方法，针对遥感图像应用中SAM性能不佳、无法进行识别的问题进行了处理。通过引入不同的提示来提升模型的泛化能力。 |
| [^15] | [Preserving Phonemic Distinctions for Ordinal Regression: A Novel Loss Function for Automatic Pronunciation Assessment.](http://arxiv.org/abs/2310.01839) | 这项研究提出了一种新的损失函数，用于训练自动发音评估模型。该损失函数旨在在保持音位类别之间更好的音位差异的同时，考虑到回归目标输出的有序关系。通过引入音位-区分正则化器，该方法可以有效提高发音评估的准确度。 |
| [^16] | [Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation.](http://arxiv.org/abs/2310.01837) | 这篇论文扩展了基于CAM的可解释AI方法，使其适用于遥感图像分割。当前AI模型在高分辨率卫星图像上训练时缺乏透明度和可解释性，本文通过改进XAI分类算法，提供了解释图像分割的手段。 |
| [^17] | [Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation.](http://arxiv.org/abs/2310.01828) | 本论文介绍了一种可训练的噪声模型作为XAI评估方法，在遥感图像分割中的应用。在图像处理中提供深度神经网络的可解释性对于广泛采用和部署至关重要。虽然图像分割在计算机视觉应用中很重要，但在可解释性方面受到了相对较少的关注。 |
| [^18] | [Empirical Study of PEFT techniques for Winter Wheat Segmentation.](http://arxiv.org/abs/2310.01825) | 本研究通过使用PEFT技术，探索跨区域和跨年份的分布外推广性，以适应农作物监测的需求。 |
| [^19] | [Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs.](http://arxiv.org/abs/2310.01801) | 该研究提出了一种自适应的KV缓存压缩方法，用于减少大型语言模型的内存消耗。通过有针对性的分析和结构识别，构建了具有自适应性的KV缓存，通过清除和丢弃特定的上下文，以及只对特定的注意力头使用标准KV缓存，实现了显著的内存占用减少。 |
| [^20] | [Large Language Models Cannot Self-Correct Reasoning Yet.](http://arxiv.org/abs/2310.01798) | 大型语言模型(LLMs)的自我纠正能力在推理方面存在困难，甚至可能在自我纠正后性能下降。 |
| [^21] | [Can large language models provide useful feedback on research papers? A large-scale empirical analysis.](http://arxiv.org/abs/2310.01783) | 这项研究通过大规模实证分析探讨了使用大型语言模型生成科学论文反馈的实用性。通过对GPT-4生成的反馈与人类同行评审的比较，发现大型语言模型在提供科学反馈方面具有潜力。 |
| [^22] | [Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns.](http://arxiv.org/abs/2310.01749) | 堆栈注意力为Transformers模型处理层次模式提供了能力，通过结合堆栈和注意力机制，它能有效地学习和识别任意深度的语法结构，特别适用于具有解析难度的上下文无关语言。 |
| [^23] | [Nugget: Neural Agglomerative Embeddings of Text.](http://arxiv.org/abs/2310.01732) | Nugget是一种基于动态选择的输入令牌子集的文本嵌入方法，通过自编码和机器翻译等任务，将语言分割为有意义的单元，优于相关方法，在语义比较任务中表现出色，并且允许扩展语言模型的上下文窗口。 |
| [^24] | [Ensemble Distillation for Unsupervised Constituency Parsing.](http://arxiv.org/abs/2310.01717) | 本论文提出了一种集成蒸馏的方法来提高无监督句法解析的性能，并且通过蒸馏将集成知识转移到一个学生模型中，解决了常见的多教师蒸馏方法中的过度平滑问题。 |
| [^25] | [Deciphering Diagnoses: How Large Language Models Explanations Influence Clinical Decision Making.](http://arxiv.org/abs/2310.01708) | 大型语言模型在医学决策中生成的解释能显著提高医生对诊断的一致率，并指出潜在的LLM输出错误。这项研究强调了LLMs在医疗保健中的潜力和挑战，以及确保患者安全和最佳临床效用的需求。 |
| [^26] | [Closing the Curious Case of Neural Text Degeneration.](http://arxiv.org/abs/2310.01693) | 本研究解释了截断采样方法的有效性，并提出了一种基于softmax瓶颈的更精确的采样策略。 |
| [^27] | [Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models.](http://arxiv.org/abs/2310.01691) | 这项工作提出了一种零样本连续提示传递方法，通过将源提示编码到相对空间中，并搜索相应的目标提示，在不同的语言模型之间实现了任务语义的泛化，实验证实了该方法的有效性。 |
| [^28] | [One model to rule them all ? Towards End-to-End Joint Speaker Diarization and Speech Recognition.](http://arxiv.org/abs/2310.01688) | 本文提出了一种名为SLIDAR的新框架，实现了联合说话人先行护理和自动语音识别，能够处理任意长度的输入和任意数量的说话人，并通过将本地输出合并来获得最终的结果。实验证实了该方法在近距离和远距离语音场景中的有效性。 |
| [^29] | [VAL: Interactive Task Learning with GPT Dialog Parsing.](http://arxiv.org/abs/2310.01627) | VAL是一种交互式任务学习系统，通过结合大型语言模型（LLM）和符号集成的理念，实现了从自然语言中进行交互式学习的分层任务知识的获取。所获得的知识可解释并能够推广到执行新任务。在视频游戏环境中的用户交互实验表明，VAL能够从有限的指令中成功学到有效的任务知识。 |
| [^30] | [A Review of Digital Learning Environments for Teaching Natural Language Processing in K-12 Education.](http://arxiv.org/abs/2310.01603) | 这篇综述论文介绍了K-12教育中教授自然语言处理的数字学习环境，并讨论了现有工具的支持、解释性和评估结果，以指导未来研究努力改进现有工具、开发新工具，并探索更有效和包容性的NLP整合策略。 |
| [^31] | [Defending Against Authorship Identification Attacks.](http://arxiv.org/abs/2310.01568) | 本文回顾了作者身份识别领域的研究进展，主要关注如何通过混淆写作风格来保护公开交流中的匿名性。 |
| [^32] | [Making Retrieval-Augmented Language Models Robust to Irrelevant Context.](http://arxiv.org/abs/2310.01558) | 本文分析了检索增强的语言模型在开放领域问答中的性能问题，并提出了基于自然语言推理的方法来缓解这个问题。 |
| [^33] | [LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples.](http://arxiv.org/abs/2310.01469) | LLM似乎具有丰富的知识和适应多种任务的能力，但我们不能完全信任它们的回答，因为它们会出现幻觉，即捏造不存在的事实以欺骗用户。本文证明了由随机标记组成的无意义提示也能引起LLM产生幻觉回应，并提出了一种对抗方式的自动幻觉触发方法作为幻觉攻击，同时提出了一种简单而有效的防御策略。 |
| [^34] | [The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs.](http://arxiv.org/abs/2310.01468) | 本文提供了一个评估框架，通过向法官提出一系列查询来评估LLMs的对话推理和规划能力。我们发现不同的LLMs在这个任务上表现出显著差异。 |
| [^35] | [FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models.](http://arxiv.org/abs/2310.01467) | FedBPT是一个高效的联邦式黑盒提示调优框架，解决了在大型语言模型中应用联邦学习进行模型微调时的挑战。 |
| [^36] | [NarrativePlay: Interactive Narrative Understanding.](http://arxiv.org/abs/2310.01459) | 本文介绍了一个名为NarrativePlay的系统，用户可以在虚构环境中扮演虚构角色与其他角色进行互动，并利用大型语言模型（LLMs）生成类人回应。该系统通过自动生成的视觉展示和角色的言谈，极大地提升了用户体验。通过在侦探和冒险故事中进行评估，用户可以通过对话要么探索世界要么提高与叙事角色的亲和力。 |
| [^37] | [Fooling the Textual Fooler via Randomizing Latent Representations.](http://arxiv.org/abs/2310.01452) | 该论文提出了一种轻量级的攻击无关防御策略AdvFooler，通过随机化输入的潜在表示来困惑基于查询的黑盒攻击，从而迷惑文本愚弄者。 |
| [^38] | [Meta Semantic Template for Evaluation of Large Language Models.](http://arxiv.org/abs/2310.01448) | 提出了一种通过创建元语义模板来评估大型语言模型（LLM）对语义理解能力的方法，该方法利用现有数据集生成新的超出分布（OOD）评估集。 |
| [^39] | [Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning.](http://arxiv.org/abs/2310.01446) | 这个论文提出了一个自适应求解器框架，用于在大型语言模型推理中根据问题难度调整求解策略。这解决了现有方法刚性采用统一方法的问题，提高了计算性能。 |
| [^40] | [Adapting LLM Agents Through Communication.](http://arxiv.org/abs/2310.01444) | 这项研究提出了一种名为学习通信（LTC）的训练方法，使用该方法可使LLM代理通过与环境和其他代理的交互不断改进，以适应新任务，而无需过多人类监督。 |
| [^41] | [UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities.](http://arxiv.org/abs/2310.01441) | UPAR提示框架模拟人类认知结构，在大型语言模型中提供了可理解和可检查的推理轨迹，增强了推理的可解释性和准确性，并为现有的提示技术提供了认识论基础。 |
| [^42] | [The Many Voices of Duying: Revisiting the Disputed Essays Between Lu Xun and Zhou Zuoren.](http://arxiv.org/abs/2310.01440) | 这项研究通过定量方法重新审视鲁迅和周作人1912年以笔名发表的三篇争议性文章，发现鲁迅是其中《看中国》的作者，并且《忘不了祖先告语》可能由鲁迅主要创作或在他的大量修改下完成。第三篇文章展示了两人的影响并具有混合的写作风格。 |
| [^43] | [Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile.](http://arxiv.org/abs/2310.01434) | 本文介绍了一种在移动设备上执行30亿参数的GPT LLM的创新方法，通过集成本地代码和模型量化技术实现了在低内存设备上的平稳运行，并解决了网络依赖性的问题。这种应用不仅具有通用助手功能，还可以实现文本到操作的无缝移动互动。 |
| [^44] | [Split and Merge: Aligning Position Biases in Large Language Model based Evaluators.](http://arxiv.org/abs/2310.01432) | PORTIA是一个旨在校准大型语言模型评估器的位置偏差的对齐系统，通过将答案分割成多个片段，并对其进行对齐，然后将其合并回一个单一的提示，以提高评估的准确性和公正性。 |
| [^45] | [Sarcasm in Sight and Sound: Benchmarking and Expansion to Improve Multimodal Sarcasm Detection.](http://arxiv.org/abs/2310.01430) | 本研究旨在通过使用语言、语音和视觉编码器对MUStARD++数据集进行严格基准测试，提高多模式讽刺检测的性能，并通过新增的数据集片段进一步改善类别不平衡问题。 |
| [^46] | [Chatmap : Large Language Model Interaction with Cartographic Data.](http://arxiv.org/abs/2310.01429) | 本研究通过微调相对较小规模的大型语言模型（LLM），以提供对OpenStreetMap（OSM）数据的语言接口。用户可以通过该界面查询有关特定地理位置的属性和趋势。 |
| [^47] | [Attention Sorting Combats Recency Bias In Long Context Language Models.](http://arxiv.org/abs/2310.01427) | 注意力排序可以改善长文本语言模型的性能，通过对注意力进行排序并重复生成，解决了当前模型在整合长上下文时的问题。 |
| [^48] | [Borges and AI.](http://arxiv.org/abs/2310.01425) | 这篇论文主张通过探索乔治·路易斯·博尔赫斯的意象来理解大型语言模型和人工智能之间的关系。 |
| [^49] | [Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey.](http://arxiv.org/abs/2310.01424) | 这项调查对语言模型（LMs）中的隐私风险进行了研究和减轻措施的探讨，通过分类法和调查现有攻击，提出了关键趋势，并讨论现有的减轻策略的优势和局限性，指出关键差距和未来工作方向。 |
| [^50] | [An Empirical Study of AI Generated Text Detection Tools.](http://arxiv.org/abs/2310.01423) | 本研究实证研究了AI生成文本检测工具在多领域ChatGPT材料中的有效性，并创建了一个多领域数据集来测试最先进API和工具的能力。 |
| [^51] | [Ruffle&Riley: Towards the Automated Induction of Conversational Tutoring Systems.](http://arxiv.org/abs/2310.01420) | 本文介绍了一种利用大型语言模型自动诱导辅导脚本和自动协调脚本的新型对话辅导系统。在初步的用户研究中，与其他简单的问答聊天机器人和阅读活动相比，系统在后测分数上没有显著差异。 |
| [^52] | [Cordyceps@LT-EDI: Depression Detection with Reddit and Self-training.](http://arxiv.org/abs/2310.01418) | 本论文提出了一种利用自我训练和Reddit进行抑郁症检测的方法，通过分类未标记的社交媒体帖子来预测用户是否有抑郁症，并在相关任务中取得了较好的排名。 |
| [^53] | [Representation Engineering: A Top-Down Approach to AI Transparency.](http://arxiv.org/abs/2310.01405) | 这项研究介绍了一种名为表示工程化（RepE）的自上而下方法，通过借鉴认知神经科学的见解，提供了一种增强AI系统透明性的解决方案。该方法将集群级别的表示放在分析的核心，为监测和操纵深度神经网络中的高级认知现象提供了新的方法，并展示了在解决与安全相关的问题上的潜力。 |
| [^54] | [On the Generalization of Training-based ChatGPT Detection Methods.](http://arxiv.org/abs/2310.01307) | 本研究综合调查了基于训练的ChatGPT检测方法在分布漂移下的泛化行为，包括提示、文本长度、主题和语言任务，在新数据集上揭示了有启示性的发现，为未来方法或数据的开发提供了指导。 |
| [^55] | [appjsonify: An Academic Paper PDF-to-JSON Conversion Toolkit.](http://arxiv.org/abs/2310.01206) | appjsonify是一种学术论文PDF到JSON转换工具包，它使用多种模型和方法解析PDF文件，并且允许用户灵活配置处理流程。 |
| [^56] | [TRAM: Benchmarking Temporal Reasoning for Large Language Models.](http://arxiv.org/abs/2310.00835) | TRAM是一个用于评估大型语言模型的时间推理能力的基准评估。我们介绍了由十个数据集组成的TRAM基准评估，涵盖了事件的各种时间方面。尽管使用流行的语言模型进行广泛评估，结果表明这些模型在时间推理任务上仍然落后于人类。 |
| [^57] | [Wavelet Scattering Transform for Improving Generalization in Low-Resourced Spoken Language Identification.](http://arxiv.org/abs/2310.00602) | 该论文研究了在低资源情况下提升口语语种识别系统泛化能力的方法，提出了一种新的特征表示方法——小波散射变换（WST），并通过优化超参数和开发融合系统，成功降低了错误识别率。 |
| [^58] | [JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention.](http://arxiv.org/abs/2310.00535) | 本文提出了联合MLP/注意力（JoMA）动态，用于解析多层Transformer架构的训练过程。通过预测非线性激活情况下注意力的行为，我们解释了多层Transformer中标记的层次组合方法。实验证实了我们的理论发现。 |
| [^59] | [Intuitive or Dependent? Investigating LLMs' Robustness to Conflicting Prompts.](http://arxiv.org/abs/2309.17415) | 本文研究了LLMs对冲突提示的鲁棒性，发现这些模型对于误导性提示特别容易受到影响，尤其是在指导常识知识方面。 |
| [^60] | [Multiple evolutionary pressures shape identical consonant avoidance in the world's languages.](http://arxiv.org/abs/2309.14006) | 多种进化压力共同作用，塑造了世界语言中相同辅音的避忌现象，影响着单词的产生、形变和消失。 |
| [^61] | [MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning.](http://arxiv.org/abs/2309.05653) | MAmmoTH是一系列用于解决通用数学问题的开源大型语言模型，通过混合指令调整网络架构，成功地融合了链状思维和程序维思维，从而在多个数学推理数据集上实现了显著提升的准确率。其中，MAmmoTH-7B模型在MATH数据集上的表现超过了目前最好的开源7B模型WizardMath，并且整个系列模型在各个规模上平均准确率提高了16%到32%之间。 |
| [^62] | [LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models.](http://arxiv.org/abs/2308.16137) | LM-Infinite研究了大规模语言模型在长序列上的长度推广失败问题，并提出了一种简单的即时推广方法，以更高效地利用现有模型的生成能力。 |
| [^63] | [Data Race Detection Using Large Language Models.](http://arxiv.org/abs/2308.07505) | 本研究探索了一种基于大型语言模型的数据竞争检测方法，通过结合提示工程和微调技术，发现LLMs可以作为数据竞争检测的可行方法，但在需要详细信息时仍不如传统工具竞争。 |
| [^64] | [ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs.](http://arxiv.org/abs/2307.16789) | ToolLLM是一个通用的工具使用框架，旨在促进大型语言模型掌握16000多个真实世界API。该框架包括数据构建、模型训练和评估。使用ChatGPT自动构建的ToolBench数据集用于指令调整，涵盖了各种工具使用情境和API。 |
| [^65] | [A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis.](http://arxiv.org/abs/2307.12856) | 这篇论文介绍了一种名为WebAgent的LLM驱动代理，通过自我经验学习，在真实网站上完成任务。该方法通过规划、总结和生成代码来提高在真实网站上的成功率。 |
| [^66] | [In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning.](http://arxiv.org/abs/2307.12375) | 大型语言模型（LLMs）在包含标签关系示例的上下文中的学习能力使其在下游任务中表现显著提高，但与传统学习方法不同。我们研究了上下文示例中的标签如何影响预测、预训练中学习到的标签关系如何与上下文示例相互作用以及上下文学习如何聚合标签信息。研究结果揭示了LLMs的工作机制及其对上下文信息的处理方式。 |
| [^67] | [(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs.](http://arxiv.org/abs/2307.10490) | 本论文展示了如何利用图像和声音在多模态LLMs中进行间接指令注入，攻击者通过生成对抗扰动并将其融入图像或音频录音中，以操纵模型输出特定文本和指导对话的行为。 |
| [^68] | [In-context Autoencoder for Context Compression in a Large Language Model.](http://arxiv.org/abs/2307.06945) | 在大型语言模型中，我们提出了一种称为In-context Autoencoder (ICAE)的上下文自编码器，它通过将长上下文压缩为有限数量的内存槽，实现了$4\times$的上下文压缩，并能够根据内存槽进行条件处理以响应各种提示。 |
| [^69] | [RecallM: An Architecture for Temporal Context Understanding and Question Answering.](http://arxiv.org/abs/2307.02738) | 本文介绍了一种名为RecallM的架构，用于创建可适应和可更新的长期记忆，以提升大型语言模型聊天机器人的时间理解能力。 |
| [^70] | [GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models.](http://arxiv.org/abs/2306.13649) | 本文提出了广义知识蒸馏（GKD），通过从学生中采样输出序列来缓解分布不匹配，并在优化替代KL等离散度方面处理模型欠规范，达到了在摘要任务上最先进的性能。 |
| [^71] | [LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization.](http://arxiv.org/abs/2306.01102) | 本文介绍了利用大语言模型和多样性优化算法相结合的 LLMatic 神经结构搜索算法。该算法在CIFAR-10数据集进行测试，仅进行2000次搜索即可产生高性能网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。 |
| [^72] | [Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes.](http://arxiv.org/abs/2305.13300) | 本文研究了大型语言模型（LLMs）在遭遇知识冲突时的行为。结果发现，LLMs可以高度接受外部连贯且有说服力的证据，即使与其参数化内存存在冲突，但也可能有局限性。 |
| [^73] | [Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources.](http://arxiv.org/abs/2305.13269) | Chain-of-Knowledge通过整合多源动态知识为大型语言模型提供准确的基础信息，减少生成的幻觉，可以产生更可靠的答案。 |
| [^74] | ["What do others think?": Task-Oriented Conversational Modeling with Subjective Knowledge.](http://arxiv.org/abs/2305.12091) | 该研究提出了一种主观知识驱动的任务导向式对话建模方法，并制作了相应数据集。该方法面临着新的挑战，例如如何汇总多个知识片段中的不同意见。 |
| [^75] | [Cross-Modal Retrieval for Motion and Text via MildTriple Loss.](http://arxiv.org/abs/2305.04195) | 本论文提出了一个创新模型，使用MildTriple Loss捕捉长期依赖并模拟跨模态人类动作序列与文本检索任务，具有重要的应用价值。 |
| [^76] | [Bridging Discrete and Backpropagation: Straight-Through and Beyond.](http://arxiv.org/abs/2304.08612) | 本文提出了一种新方法来逼近生成离散潜变量的参数的梯度，其中包括了一些数值方法，实现了二阶精度，取得了实验上的持续改进。 |
| [^77] | [On the Possibilities of AI-Generated Text Detection.](http://arxiv.org/abs/2304.04736) | 该研究探讨了AI生成文本检测的可能性，提出了精确的样本复杂度界限，并指出了设计更准确的检测方法和提高透明度的挑战。 |
| [^78] | [Chain of Hindsight Aligns Language Models with Feedback.](http://arxiv.org/abs/2302.02676) | 该研究提出了一种新颖的技术，即回顾链，可以轻松优化，并可以从任何形式的反馈中学习，而不受其极性的影响。 |
| [^79] | [Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data.](http://arxiv.org/abs/2302.00674) | 本文提出了一种在少样本学习过程中假定有辅助数据的训练范式FLAD，并针对自动混合辅助和目标数据的方法局限，提出了两种计算复杂度独立于辅助数据集数量的算法，通过FLAD和这两种算法的比较，可以发现这两种算法的表现更好。 |
| [^80] | [Query Rewriting for Effective Misinformation Discovery.](http://arxiv.org/abs/2210.07467) | 我们提出了一个新的查询重写系统，通过使用离线强化学习来自动学习编辑操作，以提高已知误传主张的查询效果，实验结果显示我们的系统可使查询有效性增加多达42%。 |
| [^81] | [DialoGen: Generalized Long-Range Context Representation for Dialogue Systems.](http://arxiv.org/abs/2210.06282) | DialoGen是一种对话系统，采用了广义上下文表示方法，在对话理解和生成任务中表现出色。 |
| [^82] | [FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation.](http://arxiv.org/abs/2210.00193) | FRMT是一个用于少样本区域感知机器翻译的基准测试，包括了专业翻译的数据集和自动评估指标，能够对词汇上不同的术语和干扰项进行详细分析。同时提供了基准模型和指导，供研究人员使用。 |
| [^83] | [Sci-Net: Scale Invariant Model for Buildings Segmentation from Aerial Imagery.](http://arxiv.org/abs/2111.06812) | 本文提出了一种尺度不变的神经网络架构Sci-Net，用于从广泛范围空间分辨率的航空图像中分割建筑物，并且在实验中表现出显著优越性能。 |

# 详细

[^1]: 使用图神经网络控制意义到文本生成中的话题-焦点表达

    Controlling Topic-Focus Articulation in Meaning-to-Text Generation using Graph Neural Networks. (arXiv:2310.02053v1 [cs.CL])

    [http://arxiv.org/abs/2310.02053](http://arxiv.org/abs/2310.02053)

    本研究提出了一种使用图神经网络控制意义到文本生成中的话题-焦点表达的方法，通过在意义表示中添加语用信息来强制使用主动或被动语态。研究采用了一种新的节点聚合编码策略，通过使用深度学习表示节点，来解决了意义中词序信息缺失的问题。

    

    裸露的意义表示可以通过自然语言以各种方式表达，这取决于信息在表面层面上的结构。我们有兴趣找到一种在从意义生成文本时控制话题-焦点表达的方法。我们将重点放在区分带有及物动词的句子的主动语态和被动语态上。我们的思路是在意义表示中添加话题等语用信息，从而在提供给自然语言生成系统时强制使用主动语态或被动语态。我们使用图神经模型，因为在用图表示的意义中没有关于词序的明确信息。我们尝试了三种不同的话题-焦点表达（TFA）方法，使用图神经模型进行意义到文本生成任务。我们提出了一种关于图神经模型中的节点聚合的新编码策略，通过使用深度学习节点表示来代替传统的聚合相邻节点信息的编码方法。

    A bare meaning representation can be expressed in various ways using natural language, depending on how the information is structured on the surface level. We are interested in finding ways to control topic-focus articulation when generating text from meaning. We focus on distinguishing active and passive voice for sentences with transitive verbs. The idea is to add pragmatic information such as topic to the meaning representation, thereby forcing either active or passive voice when given to a natural language generation system. We use graph neural models because there is no explicit information about word order in a meaning represented by a graph. We try three different methods for topic-focus articulation (TFA) employing graph neural models for a meaning-to-text generation task. We propose a novel encoding strategy about node aggregation in graph neural models, which instead of traditional encoding by aggregating adjacent node information, learns node representations by using depth-f
    
[^2]: 针对端到端语音翻译的大型语言模型调优

    Tuning Large language model for End-to-end Speech Translation. (arXiv:2310.02050v1 [cs.CL])

    [http://arxiv.org/abs/2310.02050](http://arxiv.org/abs/2310.02050)

    本文介绍了一个名为LST的大型多模态模型用于端到端语音翻译任务。该模型通过两个训练阶段进行模态调整和下游任务微调，优化了翻译性能。

    

    随着大型语言模型（LLM）的出现，基于LLM的多模态模型展示了显著的潜力。像LLaSM、X-LLM和SpeechGPT这样的模型展示出了理解和生成人类指令的卓越能力。然而，在面对端到端语音翻译（E2E-ST）这样的复杂任务时，它们的性能常常不稳定，这是一个跨语言和跨模态的翻译任务。与单模态模型相比，多模态模型在这些场景中落后。本文介绍了LST，这是一个专门设计用于在E2E-ST任务上表现出色的大型多模态模型。LST由语音前端、适配器和LLM后端组成。LST的训练分为两个阶段：（1）模态调整，其中适配器被调整以使语音表示与文本嵌入空间对齐，（2）下游任务微调，其中适配器和LLM模型同时进行训练，以在E2E-ST任务上优化性能。在MuST-C语音翻译基准测试上的实验结果表明...

    With the emergence of large language models (LLMs), multimodal models based on LLMs have demonstrated significant potential. Models such as LLaSM, X-LLM, and SpeechGPT exhibit an impressive ability to comprehend and generate human instructions. However, their performance often falters when faced with complex tasks like end-to-end speech translation (E2E-ST), a cross-language and cross-modal translation task. In comparison to single-modal models, multimodal models lag behind in these scenarios. This paper introduces LST, a Large multimodal model designed to excel at the E2E-ST task. LST consists of a speech frontend, an adapter, and a LLM backend. The training of LST consists of two stages: (1) Modality adjustment, where the adapter is tuned to align speech representation with text embedding space, and (2) Downstream task fine-tuning, where both the adapter and LLM model are trained to optimize performance on the E2EST task. Experimental results on the MuST-C speech translation benchmar
    
[^3]: 评委：一个综合评估工具包

    Jury: A Comprehensive Evaluation Toolkit. (arXiv:2310.02040v1 [cs.CL])

    [http://arxiv.org/abs/2310.02040](http://arxiv.org/abs/2310.02040)

    评委是一个统一的评估框架和工具包，旨在解决深度学习中不同系统和指标的评估挑战，并改进度量评估。

    

    评估在深度学习中扮演着一个至关重要的角色，作为任何基于预测的系统的基本模块。然而，大量的自然语言处理（NLP）任务和各种指标的发展导致了使用不同指标评估不同系统的挑战。为了解决这些问题，我们推出了一个名为评委的工具包，提供了一个统一的评估框架和标准化结构，以跨不同任务和指标进行评估。评委的目标是标准化和改进所有系统的度量评估，并帮助社区克服评估中的挑战。自评委的开源发布以来，已经吸引了广大用户，可在https://github.com/obss/jury 上获得。

    Evaluation plays a critical role in deep learning as a fundamental block of any prediction-based system. However, the vast number of Natural Language Processing (NLP) tasks and the development of various metrics have led to challenges in evaluating different systems with different metrics. To address these challenges, we introduce jury, a toolkit that provides a unified evaluation framework with standardized structures for performing evaluation across different tasks and metrics. The objective of jury is to standardize and improve metric evaluation for all systems and aid the community in overcoming the challenges in evaluation. Since its open-source release, jury has reached a wide audience and is available at https://github.com/obss/jury.
    
[^4]: OceanGPT：用于海洋科学任务的大型语言模型

    OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])

    [http://arxiv.org/abs/2310.02031](http://arxiv.org/abs/2310.02031)

    OceanGPT是首个专为海洋科学任务设计的大型语言模型，通过DoInstruct框架实现自动获取海洋领域指导数据。这一模型的引入填补了海洋科学领域中对LLM的需求缺口，并为海洋科学研究提供了新的工具和方法。

    

    海洋科学是探索充满生命和生物多样性的海洋的科学，考虑到海洋覆盖了地球表面的70％以上，这一领域具有重要意义。最近，大型语言模型（LLM）的进展改变了科学的范式。尽管在其他领域取得了成功，但现有的LLM通常无法满足海洋学家等领域专家的需求，同时对LLM在海洋科学中的潜力尚未得到充分探索。这其中的根本原因可能是海洋数据的庞大而复杂的性质，以及对更高的粒度和丰富的知识的需求。为了解决这些问题，我们推出了首个海洋领域的LLM——OceanGPT，该模型擅长各种海洋科学任务。我们提出了一个新颖的框架DoInstruct，用于自动获取大量的海洋领域指导数据，它基于多智能体的协作生成指导。

    Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
    
[^5]: 填空题：探索并增强LLM在数学应用题中的逆向推理能力

    Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems. (arXiv:2310.01991v1 [cs.CL])

    [http://arxiv.org/abs/2310.01991](http://arxiv.org/abs/2310.01991)

    本文探讨了LLM在数学应用题中的逆向推理能力，发现在逆向推理任务上，LLM模型的准确性显著下降。通过改进技术，如Rephrase和PAL-Tools，我们提高了模型的性能。

    

    虽然近期的文献中广泛探讨了正向推理（即给定问题找答案），但逆向推理相对较少被研究。我们对LLM在数学应用题中的逆向推理能力进行了探讨：给定一个数学问题和其答案，在问题中有些细节被省略了，LLM能否有效地还原出缺失的信息？本文正式定义了数学应用题中的逆向推理任务，并修改了三个数据集来评估这一任务：GSM8k、SVAMP和MultiArith。我们的研究结果显示，与正向推理相比，四个最先进的LLM模型（GPT4、GPT3.5、PaLM-2和LLaMa-2）在逆向推理上的准确性显著下降。利用该任务的特定格式，我们提出了三种改进性能的新技术：Rephrase将给定的问题重述为一个正向推理问题，PAL-Tools结合了程序辅助的LLM思想，生成一组方程式可以解决缺失的信息。

    While forward reasoning (i.e. find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information?  In this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that ca
    
[^6]: 用语言模型作为视觉词义消歧的知识库

    Language Models as Knowledge Bases for Visual Word Sense Disambiguation. (arXiv:2310.01960v1 [cs.CL])

    [http://arxiv.org/abs/2310.01960](http://arxiv.org/abs/2310.01960)

    本文提出了一种使用大型语言模型作为知识库的方法，通过在视觉词义消歧任务中使用适当的提示，以零样本方式检索存储在语言模型中的知识，从而提高视觉词义消歧的检索性能。

    

    视觉词义消歧（VWSD）是一项新颖且具有挑战性的任务，位于语义消歧和细粒度多模式检索之间。最近发展的视觉语言模型（VL transformers）的进展表明，一些现成的实现具有令人鼓舞的结果，但我们认为还可以进一步改进。为此，我们提出了一些增强知识的技术，通过使用大型语言模型（LLMs）作为知识库，来提高VL transformers的检索性能。具体而言，LLMs中存储的知识以零样本方式通过适当的提示进行检索，实现了性能的提升。此外，我们通过将生成的图像标题视为多项选择候选答案，将VWSD转化为纯文本问答（QA）问题。利用零样本和少样本提示策略来探索这种转换的潜力，同时借助思维链（CoT）提示从而进一步提高性能。

    Visual Word Sense Disambiguation (VWSD) is a novel challenging task that lies between linguistic sense disambiguation and fine-grained multimodal retrieval. The recent advancements in the development of visiolinguistic (VL) transformers suggest some off-the-self implementations with encouraging results, which however we argue that can be further improved. To this end, we propose some knowledge-enhancement techniques towards improving the retrieval performance of VL transformers via the usage of Large Language Models (LLMs) as Knowledge Bases. More specifically, knowledge stored in LLMs is retrieved with the help of appropriate prompts in a zero-shot manner, achieving performance advancements. Moreover, we convert VWSD to a purely textual question-answering (QA) problem by considering generated image captions as multiple-choice candidate answers. Zero-shot and few-shot prompting strategies are leveraged to explore the potential of such a transformation, while Chain-of-Thought (CoT) prom
    
[^7]: 使用LLM进行驾驶：融合对象级向量模态以解释自动驾驶

    Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving. (arXiv:2310.01957v1 [cs.RO])

    [http://arxiv.org/abs/2310.01957](http://arxiv.org/abs/2310.01957)

    这篇论文介绍了一种融合对象级多模态LLM架构的方法，通过将向量化的数字模态与预训练的LLM相结合来提高自动驾驶中对上下文的理解能力。同时还使用了一个新的数据集进行评估，证明了LLM驱动程序在解释驾驶情境、回答问题和决策方面的效果优于传统的行为克隆方法。

    

    大型语言模型（LLM）在自动驾驶领域表现出了潜力，特别是在泛化和可解释性方面。我们引入了一种独特的对象级多模态LLM架构，将向量化的数字模态与预训练的LLM相结合，以提高驾驶情境的上下文理解能力。我们还提出了一个新的数据集，其中包含来自10k个驾驶情境的160k个问答对，这些问答对与由RL代理收集的高质量控制命令和由教师LLM（GPT-3.5）生成的问题答案对相匹配。我们设计了一种独特的预训练策略，使用向量字幕语言数据来对齐数字向量模态和静态LLM表示。我们还引入了一种驾驶问答的评估指标，并展示了我们的LLM驱动程序在解释驾驶情境、回答问题和决策方面的熟练程度。我们的研究结果突显了基于LLM的驾驶行为生成与传统行为克隆相比的潜力。我们提供了我们的基准数据集。

    Large Language Models (LLMs) have shown promise in the autonomous driving sector, particularly in generalization and interpretability. We introduce a unique object-level multimodal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. We also present a new dataset of 160k QA pairs derived from 10k driving scenarios, paired with high quality control commands collected with RL agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data. We also introduce an evaluation metric for Driving QA and demonstrate our LLM-driver's proficiency in interpreting driving scenarios, answering questions, and decision-making. Our findings highlight the potential of LLM-based driving action generation in comparison to traditional behavioral cloning. We make our benchmar
    
[^8]: 穿越文化鸿沟：探索和解锁文本到图像模型的文化视角

    Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models. (arXiv:2310.01929v1 [cs.CL])

    [http://arxiv.org/abs/2310.01929](http://arxiv.org/abs/2310.01929)

    本研究旨在探索和解锁文本到图像模型的文化视角，通过对TTI模型中嵌入的文化感知进行评估，揭示了这些模型的文化意识、文化区别和文化适应性。

    

    文本到图像（TTI）模型，例如DALL-E和StableDiffusion，在通过文本提示生成图像的零射模式方面具有卓越的能力，近来备受关注。作为文化的媒介，语言在这些模型的多语言能力中起着关键作用，从而塑造了它们的文化机制。在本研究中，我们通过描述文化维度，文化领域和文化概念的三个层次来探索TTI模型中嵌入的文化感知。我们提出了一套全面的评估技术，包括使用CLIP空间进行内在评估，使用视觉问答（VQA）模型进行外在评估以及人类评估，以识别TTI文化感知。为了促进我们的研究，我们引入了CulText2I数据集，该数据集来自四个不同的TTI模型，涵盖了十种语言。我们的实验揭示了这些模型的文化意识、文化区别和

    Text-To-Image (TTI) models, exemplified by DALL-E and StableDiffusion, have recently gained prominence for their remarkable zero-shot capabilities in generating images guided by textual prompts. Language, as a conduit of culture, plays a pivotal role in these models' multilingual capabilities, which in turn shape their cultural agency. In this study, we explore the cultural perception embedded in TTI models by characterizing culture across three hierarchical tiers: cultural dimensions, cultural domains, and cultural concepts. We propose a comprehensive suite of evaluation techniques, including intrinsic evaluations using the CLIP space, extrinsic evaluations with a Visual-Question-Answer (VQA) model, and human assessments, to discern TTI cultural perceptions. To facilitate our research, we introduce the CulText2I dataset, derived from four diverse TTI models and spanning ten languages. Our experiments reveal insights into these models' cultural awareness, cultural distinctions, and the
    
[^9]: 分层评估框架：人工评估的最佳实践

    Hierarchical Evaluation Framework: Best Practices for Human Evaluation. (arXiv:2310.01917v1 [cs.CL])

    [http://arxiv.org/abs/2310.01917](http://arxiv.org/abs/2310.01917)

    这篇论文提出了一个分层评估框架，解决了自然语言处理中人工评估指标不统一的问题，并应用于机器阅读理解系统的评估，突出了输入与输出质量之间的关联。

    

    人工评估在自然语言处理（NLP）中起着至关重要的作用，它评估了开发系统的质量和相关性，从而促进了系统的改进。然而，在NLP中缺乏广泛接受的人工评估指标，阻碍了不同系统之间的公平比较和建立普遍的评估标准。通过对现有文献中的人工评估指标进行广泛分析，我们发现了NLP评估方法中的几个缺口。这些缺口成为我们开发自己的分层评估框架的动力。所提出的框架具有显著优势，特别是在提供更全面的NLP系统性能表示方面。我们将此框架应用于评估开发的机器阅读理解系统，该系统在人工智能与人类的共生模型中被使用。结果突出了输入与输出质量之间的关联，强调了评估系统性能的必要性。

    Human evaluation plays a crucial role in Natural Language Processing (NLP) as it assesses the quality and relevance of developed systems, thereby facilitating their enhancement. However, the absence of widely accepted human evaluation metrics in NLP hampers fair comparisons among different systems and the establishment of universal assessment standards. Through an extensive analysis of existing literature on human evaluation metrics, we identified several gaps in NLP evaluation methodologies. These gaps served as motivation for developing our own hierarchical evaluation framework. The proposed framework offers notable advantages, particularly in providing a more comprehensive representation of the NLP system's performance. We applied this framework to evaluate the developed Machine Reading Comprehension system, which was utilized within a human-AI symbiosis model. The results highlighted the associations between the quality of inputs and outputs, underscoring the necessity to evaluate 
    
[^10]: 使用分块Transformer的环形注意力解决近无限上下文问题

    Ring Attention with Blockwise Transformers for Near-Infinite Context. (arXiv:2310.01889v1 [cs.CL])

    [http://arxiv.org/abs/2310.01889](http://arxiv.org/abs/2310.01889)

    本论文提出了一种新颖的环形注意力方法，通过分块计算和通信重叠的方式处理长序列，解决了Transformer在处理长序列时的内存限制问题。实验证明该方法能够有效地消除单个设备对内存的约束，使得训练和推理的序列长度能够更长。

    

    Transformer已经成为许多最先进的人工智能模型的首选架构，在广泛的人工智能应用中展示出了非凡的性能。然而，Transformer对内存的需求限制了它处理长序列的能力，因此对于涉及扩展序列或长期依赖的任务而言存在挑战。我们提出了一种独特的方法，即环形注意力(Ring Attention)，它利用自注意力的分块计算将长序列分布到多个设备上，同时将关键-值块的通信与分块注意力的计算重叠。通过处理更长的输入序列同时保持内存效率，环形注意力使得训练和推理的序列比之前的内存高效Transformer能够多出设备数量倍，有效地消除了单个设备对内存的约束。在语言模型任务上进行的大量实验证明了这种方法的有效性。

    Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving extended sequences or long-term dependencies. We present a distinct approach, Ring Attention, which leverages blockwise computation of self-attention to distribute long sequences across multiple devices while concurrently overlapping the communication of key-value blocks with the computation of blockwise attention. By processing longer input sequences while maintaining memory efficiency, Ring Attention enables training and inference of sequences that are device count times longer than those of prior memory-efficient Transformers, effectively eliminating the memory constraints imposed by individual devices. Extensive experiments on language modeling tasks demonstrate the effecti
    
[^11]: 有效且参数高效的重复使用微调模型

    Effective and Parameter-Efficient Reusing Fine-Tuned Models. (arXiv:2310.01886v1 [cs.LG])

    [http://arxiv.org/abs/2310.01886](http://arxiv.org/abs/2310.01886)

    本文提出了一种有效且参数高效的方法，可以重复使用微调模型来处理下游任务，减轻存储和服务负担，并提出了PERU-FFT方法用于重复使用全面微调模型。

    

    许多在线提供的预训练大规模模型在传递到下游任务中变得非常有效。与此同时，各种在这些预训练模型上微调的任务特定模型也可供公众使用。在实践中，由于收集任务特定数据耗时且微调大规模预训练模型计算复杂，可以重复使用任务特定微调模型来处理下游任务。然而，为每个任务使用一个模型会给存储和服务带来巨大负担。最近，有许多无需训练且参数高效的方法被提出，将多个微调的任务特定模型重复使用到一个多任务模型中。然而，与为每个任务使用微调模型相比，这些方法表现出较大的准确性差距。本文中，我们提出了参数高效方法来重复使用微调模型。针对重复使用全面微调模型，我们提出了PERU-FFT，通过将稀疏任务向量注入到一个mer模型中来实现。

    Many pre-trained large-scale models provided online have become highly effective in transferring to downstream tasks. At the same time, various task-specific models fine-tuned on these pre-trained models are available online for public use. In practice, as collecting task-specific data is labor-intensive and fine-tuning the large pre-trained models is computationally expensive, one can reuse task-specific finetuned models to deal with downstream tasks. However, using a model per task causes a heavy burden on storage and serving. Recently, many training-free and parameter-efficient methods have been proposed for reusing multiple fine-tuned task-specific models into a single multi-task model. However, these methods exhibit a large accuracy gap compared with using a fine-tuned model per task. In this paper, we propose Parameter-Efficient methods for ReUsing (PERU) fine-tuned models. For reusing Fully Fine-Tuned (FFT) models, we propose PERU-FFT by injecting a sparse task vector into a mer
    
[^12]: 精调与提示调整的监督表示：哪种更能解释大脑中的语言表示？

    Fine-tuned vs. Prompt-tuned Supervised Representations: Which Better Account for Brain Language Representations?. (arXiv:2310.01854v1 [cs.AI])

    [http://arxiv.org/abs/2310.01854](http://arxiv.org/abs/2310.01854)

    本研究比较了精调和提示调整的表示在神经解码中的效果，发现在10个NLU任务中，精调表示不能更好地解码人脑中的信息。

    

    为了解读人脑语言表示背后的算法，先前的研究使用在NLU任务上进行过精调的预训练人工神经网络（ANN）模型来探测大脑对语言输入的反应。然而，完全的精调会更新整个参数空间并扭曲预训练的特征，与大脑的强健多任务学习能力不一致。相比之下，提示调整保护预训练的权重并学习任务特定的嵌入以适应任务。提示调整是否能生成更能解释大脑语言表示的表示？如果是，哪种NLU任务能让预训练模型更好地解码人脑中的信息表示？我们通过比较提示调整和精调的表示在神经解码中进行了这些问题的研究，即从刺激引发的大脑活动中预测语言刺激。我们发现，在10个NLU任务中，全球精调表示都不能更好地解码人脑中的信息。

    To decipher the algorithm underlying the human brain's language representation, previous work probed brain responses to language input with pre-trained artificial neural network (ANN) models fine-tuned on NLU tasks. However, full fine-tuning generally updates the entire parametric space and distorts pre-trained features, cognitively inconsistent with the brain's robust multi-task learning ability. Prompt-tuning, in contrast, protects pre-trained weights and learns task-specific embeddings to fit a task. Could prompt-tuning generate representations that better account for the brain's language representations than fine-tuning? If so, what kind of NLU task leads a pre-trained model to better decode the information represented in the human brain? We investigate these questions by comparing prompt-tuned and fine-tuned representations in neural decoding, that is predicting the linguistic stimulus from the brain activities evoked by the stimulus. We find that on none of the 10 NLU tasks, full
    
[^13]: 基准测试和改进语言模型生成器验证器一致性

    Benchmarking and Improving Generator-Validator Consistency of Language Models. (arXiv:2310.01846v1 [cs.CL])

    [http://arxiv.org/abs/2310.01846](http://arxiv.org/abs/2310.01846)

    本文提出了一种衡量生成和验证之间一致性的框架，发现目前最先进的语言模型GPT-4仅在76%的情况下是一致的。为了改进一致性，提出了一种通过经过筛选的生成器和验证器答案进行微调的方法，并将其称为一致性微调。该方法将Alpaca-30B的一致性从60%提高到93%，并且对未见过的任务和领域也具有泛化能力。除了改进一致性外，一致性微调还带来了其他性能的提升。

    

    截至2023年9月，ChatGPT在被问到"7+8=15，对还是错"时，回答"错"，但在被问到"7+8"等于多少时，回答"15"。这种生成和验证答案之间的不一致在语言模型中很常见，破坏了人们的信任。在本文中，我们提出了一种衡量生成和验证之间一致性的框架（称为生成器验证器一致性），发现即使是最先进的GPT-4语言模型，仅在76%的情况下是一致的。为了改进语言模型的一致性，我们提出了在经过生成器和验证器答案筛选的基础上进行微调的方法，并称之为一致性微调。我们发现，这种方法将Alpaca-30B的一致性从60%提高到了93%，并且这种改进可以推广到未见过的任务和领域（例如，对于积极的风格转换，一致性的改进可以推广到未见过的风格，如幽默）。除了改进一致性外，一致性微调还改善了其他性能指标。

    As of September 2023, ChatGPT correctly answers "what is 7+8" with 15, but when asked "7+8=15, True or False" it responds with "False". This inconsistency between generating and validating an answer is prevalent in language models (LMs) and erodes trust. In this paper, we propose a framework for measuring the consistency between generation and validation (which we call generator-validator consistency, or GV-consistency), finding that even GPT-4, a state-of-the-art LM, is GV-consistent only 76% of the time. To improve the consistency of LMs, we propose to finetune on the filtered generator and validator responses that are GV-consistent, and call this approach consistency fine-tuning. We find that this approach improves GV-consistency of Alpaca-30B from 60% to 93%, and the improvement extrapolates to unseen tasks and domains (e.g., GV-consistency for positive style transfers extrapolates to unseen styles like humor). In addition to improving consistency, consistency fine-tuning improves 
    
[^14]: 使用SAM进行建筑物分割模型的零-shot细化

    Zero-Shot Refinement of Buildings' Segmentation Models using SAM. (arXiv:2310.01845v1 [cs.CV])

    [http://arxiv.org/abs/2310.01845](http://arxiv.org/abs/2310.01845)

    本文提出了一种使用SAM进行建筑物分割模型的零-shot细化的方法，针对遥感图像应用中SAM性能不佳、无法进行识别的问题进行了处理。通过引入不同的提示来提升模型的泛化能力。

    

    基础模型在各种任务中表现出色，但通常在常规基准测试中评估。将这些模型应用于特定领域，如遥感图像，仍然是一个未充分开发的领域。在遥感领域中，精确的建筑物实例分割对于城市规划等应用至关重要。虽然卷积神经网络（CNN）表现良好，但它们的泛化能力可能受限。为了实现这一目标，我们提出了一种新的方法，以使基础模型适应已有模型的泛化性能下降。在众多模型中，我们的重点在于Segment Anything Model（SAM），这是一种强大的基础模型，以其擅长无类别图像分割能力而闻名。我们首先确定了SAM的局限性，揭示了它在应用于遥感图像时性能不佳。此外，SAM不具备识别能力，因此无法对定位的对象进行分类和标记。为了解决这些限制，我们引入了不同的提示

    Foundation models have excelled in various tasks but are often evaluated on general benchmarks. The adaptation of these models for specific domains, such as remote sensing imagery, remains an underexplored area. In remote sensing, precise building instance segmentation is vital for applications like urban planning. While Convolutional Neural Networks (CNNs) perform well, their generalization can be limited. For this aim, we present a novel approach to adapt foundation models to address existing models' generalization dropback. Among several models, our focus centers on the Segment Anything Model (SAM), a potent foundation model renowned for its prowess in class-agnostic image segmentation capabilities. We start by identifying the limitations of SAM, revealing its suboptimal performance when applied to remote sensing imagery. Moreover, SAM does not offer recognition abilities and thus fails to classify and tag localized objects. To address these limitations, we introduce different promp
    
[^15]: 保留音位差异的序数回归：一种用于自动发音评估的新型损失函数

    Preserving Phonemic Distinctions for Ordinal Regression: A Novel Loss Function for Automatic Pronunciation Assessment. (arXiv:2310.01839v1 [eess.AS])

    [http://arxiv.org/abs/2310.01839](http://arxiv.org/abs/2310.01839)

    这项研究提出了一种新的损失函数，用于训练自动发音评估模型。该损失函数旨在在保持音位类别之间更好的音位差异的同时，考虑到回归目标输出的有序关系。通过引入音位-区分正则化器，该方法可以有效提高发音评估的准确度。

    

    自动发音评估（APA）可以量化第二语言（L2）学习者在语言中的发音熟练程度。常见的APA方法通常利用神经模型和回归损失函数（如均方误差损失）进行熟练水平预测。尽管大多数回归模型可以在特征空间有效地捕捉到熟练水平的有序性，但它们面临一个主要障碍，即不同音素类别的相同熟练水平必然被迫靠得很近，保留了更少的音位—区分信息。因此，我们设计了一种用于训练基于回归的APA模型的音位对比序数（PCO）损失，旨在在保持音位类别之间更好的音位差异的同时，考虑回归目标输出的有序关系。具体而言，我们将一个音位—区分正则化器引入到均方误差损失中，鼓励...（待续）

    Automatic pronunciation assessment (APA) manages to quantify the pronunciation proficiency of a second language (L2) learner in a language. Prevailing approaches to APA normally leverage neural models trained with a regression loss function, such as the mean-squared error (MSE) loss, for proficiency level prediction. Despite most regression models can effectively capture the ordinality of proficiency levels in the feature space, they are confronted with a primary obstacle that different phoneme categories with the same proficiency level are inevitably forced to be close to each other, retaining less phoneme-discriminative information. On account of this, we devise a phonemic contrast ordinal (PCO) loss for training regression-based APA models, which aims to preserve better phonemic distinctions between phoneme categories meanwhile considering ordinal relationships of the regression target output. Specifically, we introduce a phoneme-distinct regularizer into the MSE loss, which encoura
    
[^16]: 扩展基于CAM的可解释AI方法用于遥感图像分割

    Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation. (arXiv:2310.01837v1 [cs.CV])

    [http://arxiv.org/abs/2310.01837](http://arxiv.org/abs/2310.01837)

    这篇论文扩展了基于CAM的可解释AI方法，使其适用于遥感图像分割。当前AI模型在高分辨率卫星图像上训练时缺乏透明度和可解释性，本文通过改进XAI分类算法，提供了解释图像分割的手段。

    

    当前的基于AI的方法无法对所使用的数据、提取的特征和预测/推理操作提供可理解的物理解释。因此，使用高分辨率卫星图像训练的深度学习模型缺乏透明度和可解释性，只能被视为一个黑盒子，这限制了它们的广泛采用。专家需要帮助理解AI模型的复杂行为和基础决策过程。可解释人工智能（XAI）领域是一个新兴领域，提供了确保AI模型稳健、实用和可信赖部署的手段。已有一些XAI技术被提出用于图像分类任务，而对于图像分割的解释则基本上没有被探索。本文通过改进最新的XAI分类算法，并使其适用于多类图像分割，以弥补这一差距，其中我们主要关注高分辨率卫星图像中的建筑物分割。

    Current AI-based methods do not provide comprehensible physical interpretations of the utilized data, extracted features, and predictions/inference operations. As a result, deep learning models trained using high-resolution satellite imagery lack transparency and explainability and can be merely seen as a black box, which limits their wide-level adoption. Experts need help understanding the complex behavior of AI models and the underlying decision-making process. The explainable artificial intelligence (XAI) field is an emerging field providing means for robust, practical, and trustworthy deployment of AI models. Several XAI techniques have been proposed for image classification tasks, whereas the interpretation of image segmentation remains largely unexplored. This paper offers to bridge this gap by adapting the recent XAI classification algorithms and making them usable for muti-class image segmentation, where we mainly focus on buildings' segmentation from high-resolution satellite 
    
[^17]: 可训练的噪声模型作为XAI评估方法：在遥感图像分割中的应用

    Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation. (arXiv:2310.01828v1 [cs.CV])

    [http://arxiv.org/abs/2310.01828](http://arxiv.org/abs/2310.01828)

    本论文介绍了一种可训练的噪声模型作为XAI评估方法，在遥感图像分割中的应用。在图像处理中提供深度神经网络的可解释性对于广泛采用和部署至关重要。虽然图像分割在计算机视觉应用中很重要，但在可解释性方面受到了相对较少的关注。

    

    可解释的人工智能（XAI）已成为处理关键任务应用时的必备要求，确保所使用的黑盒子人工智能模型的透明度和可解释性。XAI的重要性涵盖了各个领域，从医疗保健到金融，在这些领域中，了解深度学习算法的决策过程是至关重要的。大多数基于人工智能的计算机视觉模型往往是黑盒子，因此在图像处理中提供深度神经网络的可解释性对于它们在医疗图像分析、自动驾驶和遥感应用中的广泛采用和部署至关重要。最近，已经提出了几种针对图像分类任务的XAI方法。相比之下，在可解释性方面，图像分割在计算机视觉应用中，特别是在遥感领域中，受到了相对较少的关注。只有少数研究提出了基于梯度的XAI算法来进行图像分割。

    eXplainable Artificial Intelligence (XAI) has emerged as an essential requirement when dealing with mission-critical applications, ensuring transparency and interpretability of the employed black box AI models. The significance of XAI spans various domains, from healthcare to finance, where understanding the decision-making process of deep learning algorithms is essential. Most AI-based computer vision models are often black boxes; hence, providing explainability of deep neural networks in image processing is crucial for their wide adoption and deployment in medical image analysis, autonomous driving, and remote sensing applications. Recently, several XAI methods for image classification tasks have been introduced. On the contrary, image segmentation has received comparatively less attention in the context of explainability, although it is a fundamental task in computer vision applications, especially in remote sensing. Only some research proposes gradient-based XAI algorithms for imag
    
[^18]: 冬小麦分割的PEFT技术的实证研究

    Empirical Study of PEFT techniques for Winter Wheat Segmentation. (arXiv:2310.01825v1 [cs.CV])

    [http://arxiv.org/abs/2310.01825](http://arxiv.org/abs/2310.01825)

    本研究通过使用PEFT技术，探索跨区域和跨年份的分布外推广性，以适应农作物监测的需求。

    

    参数高效微调（PEFT）技术最近经历了显著的增长，并被广泛用于将大规模视觉和语言模型适应于各种领域，以最小的计算需求实现令人满意的模型性能。尽管取得了这些进展，但在实际场景中，特别是在遥感和农作物监测的关键领域中，仍需要进一步研究潜在的PEFT应用。不同地区的气候多样性和对全面的大规模数据集的需求，给精确识别不同地理位置和不断变化的种植季节的作物类型造成了重大障碍。本研究旨在通过全面探索跨区域和跨年份的分布外推广性，使用国内领先的冬小麦作物监测模型，来填补这一差距。这项工作的目标是探索PEFT方法在作物监测中的应用。具体而言，我们专注于适应性地调整PEFT方法以适应农作物监测的需求。

    Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced significant growth and have been extensively employed to adapt large vision and language models to various domains, enabling satisfactory model performance with minimal computational needs. Despite these advances, more research has yet to delve into potential PEFT applications in real-life scenarios, particularly in the critical domains of remote sensing and crop monitoring. The diversity of climates across different regions and the need for comprehensive large-scale datasets have posed significant obstacles to accurately identify crop types across varying geographic locations and changing growing seasons. This study seeks to bridge this gap by comprehensively exploring the feasibility of cross-area and cross-year out-of-distribution generalization using the State-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to explore PEFT approaches for crop monitoring. Specifically, we focus on adap
    
[^19]: 模型告诉你该丢弃什么：适应性KV缓存压缩用于LLMs

    Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. (arXiv:2310.01801v1 [cs.CL])

    [http://arxiv.org/abs/2310.01801](http://arxiv.org/abs/2310.01801)

    该研究提出了一种自适应的KV缓存压缩方法，用于减少大型语言模型的内存消耗。通过有针对性的分析和结构识别，构建了具有自适应性的KV缓存，通过清除和丢弃特定的上下文，以及只对特定的注意力头使用标准KV缓存，实现了显著的内存占用减少。

    

    在这项研究中，我们引入了一种自适应的KV缓存压缩方法，它可以减少大型语言模型（LLMs）生成推理的内存占用。与传统的KV缓存不同，我们通过有针对性的分析来识别注意力模块的内在结构。基于识别出的结构，我们以自适应的方式构建KV缓存：在强调本地上下文的注意力头上清除长距离上下文，在以特殊标记为中心的注意力头上丢弃非特殊标记，并且仅对广泛关注所有标记的注意力头使用标准的KV缓存。此外，通过使用轻量级的注意力分析来指导自适应KV缓存的构建，FastGen可以在不需要资源密集型的微调或重新训练的情况下部署。在各种任务的实验中，FastGen在GPU内存消耗方面显示出了显著的减少。

    In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with 
    
[^20]: 大型语言模型尚不能自我纠正推理错误

    Large Language Models Cannot Self-Correct Reasoning Yet. (arXiv:2310.01798v1 [cs.CL])

    [http://arxiv.org/abs/2310.01798](http://arxiv.org/abs/2310.01798)

    大型语言模型(LLMs)的自我纠正能力在推理方面存在困难，甚至可能在自我纠正后性能下降。

    

    大型语言模型(LLMs)凭借其在各种应用中无可比拟的文本生成能力而成为突破性的技术。然而，对于其生成内容的准确性和适当性仍存在疑虑。自我纠正方法被提出作为解决这些问题的一种方法。本文在此基础上对LLMs内部的自我纠正的作用和效果进行了批判性的考察，揭示了其真正的潜力和限制。我们的研究主要关注内在自我纠正的概念，即LLMs尝试仅仅依靠其固有能力来纠正其初始响应，而不依赖于外部反馈的支持。在推理的背景下，我们的研究表明LLMs在没有外部反馈的情况下很难自我纠正其响应，甚至有时候其表现可能在自我纠正后下降。基于这些洞见，我们对未来的研究提出了建议。

    Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance might even degrade post self-correction. Drawing from these insights, we offer suggestions for future resear
    
[^21]: 大型语言模型能够提供对研究论文有用的反馈吗？一项大规模实证分析。

    Can large language models provide useful feedback on research papers? A large-scale empirical analysis. (arXiv:2310.01783v1 [cs.LG])

    [http://arxiv.org/abs/2310.01783](http://arxiv.org/abs/2310.01783)

    这项研究通过大规模实证分析探讨了使用大型语言模型生成科学论文反馈的实用性。通过对GPT-4生成的反馈与人类同行评审的比较，发现大型语言模型在提供科学反馈方面具有潜力。

    

    专家的反馈是严谨研究的基础。然而，学术产出的快速增长和复杂的专业知识挑战了传统的科学反馈机制。越来越难获取高质量的同行评审意见。初级研究人员或来自资源匮乏的环境尤其难以及时获得反馈。随着GPT-4等大型语言模型的突破，使用大型语言模型生成对科学论文的反馈引起了广泛兴趣。然而，LLM生成的反馈的实用性还没有得到系统研究。为了填补这一空白，我们使用GPT-4创建了一个自动化流程，对科学论文的完整PDF提供评论。我们通过两个大规模研究评估了GPT-4反馈的质量。首先，我们在15本Nature类期刊（总共3096篇论文）和ICLR m 上定量比较了GPT-4生成的反馈与人类同行评审的反馈。

    Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR m
    
[^22]: 堆栈注意力: 提升Transformers对层次模式建模的能力

    Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns. (arXiv:2310.01749v1 [cs.CL])

    [http://arxiv.org/abs/2310.01749](http://arxiv.org/abs/2310.01749)

    堆栈注意力为Transformers模型处理层次模式提供了能力，通过结合堆栈和注意力机制，它能有效地学习和识别任意深度的语法结构，特别适用于具有解析难度的上下文无关语言。

    

    注意力机制，尤其是缩放点积注意力，在自然语言处理中已被证明非常有效，但它对于处理任意嵌套深度的层次模式没有机制，这限制了它识别某些句法结构的能力。为解决这一缺陷，我们提出了堆栈注意力：一种结合了堆栈的注意力操作符，受到它们与上下文无关语言（CFLs）的理论联系的启发。我们展示了堆栈注意力类似于标准注意力，但它具有不需要句法监督的语法潜在模型。我们提出了两种变体：与确定性下推自动机（PDAs）相关的一种，以及基于非确定性PDAs的一种，这使得transformers能够识别任意的CFLs。我们展示了使用堆栈注意力的transformers在学习标准transformers难以应对的CFLs方面非常有效，并在最大解析难度的CFL上取得了强大的结果。我们还展示了堆栈注意力的效果更好。

    Attention, specifically scaled dot-product attention, has proven effective for natural language, but it does not have a mechanism for handling hierarchical patterns of arbitrary nesting depth, which limits its ability to recognize certain syntactic structures. To address this shortcoming, we propose stack attention: an attention operator that incorporates stacks, inspired by their theoretical connections to context-free languages (CFLs). We show that stack attention is analogous to standard attention, but with a latent model of syntax that requires no syntactic supervision. We propose two variants: one related to deterministic pushdown automata (PDAs) and one based on nondeterministic PDAs, which allows transformers to recognize arbitrary CFLs. We show that transformers with stack attention are very effective at learning CFLs that standard transformers struggle on, achieving strong results on a CFL with theoretically maximal parsing difficulty. We also show that stack attention is more
    
[^23]: Nugget: 文本的神经聚合嵌入

    Nugget: Neural Agglomerative Embeddings of Text. (arXiv:2310.01732v1 [cs.CL])

    [http://arxiv.org/abs/2310.01732](http://arxiv.org/abs/2310.01732)

    Nugget是一种基于动态选择的输入令牌子集的文本嵌入方法，通过自编码和机器翻译等任务，将语言分割为有意义的单元，优于相关方法，在语义比较任务中表现出色，并且允许扩展语言模型的上下文窗口。

    

    在现代语言理解中，嵌入文本序列是一个广泛需求。现有方法主要侧重于恒定大小的表示。这是有问题的，因为文本中包含的信息量通常随输入的长度而变化。我们提出了一种称为Nugget的解决方案，它将语言编码为基于动态选择的输入令牌子集的表示。通过自编码和机器翻译等任务，学习这些nuggets，并直观地将语言分割成有意义的单元。我们展示了Nugget在涉及语义比较的任务中优于相关方法。最后，我们证明了这些紧凑单元允许扩展语言模型(LM)的上下文窗口，从而提出了新的LM可能会对更大量的内容进行条件处理。

    Embedding text sequences is a widespread requirement in modern language understanding. Existing approaches focus largely on constant-size representations. This is problematic, as the amount of information contained in text often varies with the length of the input. We propose a solution called Nugget, which encodes language into a representation based on a dynamically selected subset of input tokens. These nuggets are learned through tasks like autoencoding and machine translation, and intuitively segment language into meaningful units. We demonstrate Nugget outperforms related approaches in tasks involving semantic comparison. Finally, we illustrate these compact units allow for expanding the contextual window of a language model (LM), suggesting new future LMs that can condition on significantly larger amounts of content.
    
[^24]: 无监督句法分析的集成蒸馏

    Ensemble Distillation for Unsupervised Constituency Parsing. (arXiv:2310.01717v1 [cs.CL])

    [http://arxiv.org/abs/2310.01717](http://arxiv.org/abs/2310.01717)

    本论文提出了一种集成蒸馏的方法来提高无监督句法解析的性能，并且通过蒸馏将集成知识转移到一个学生模型中，解决了常见的多教师蒸馏方法中的过度平滑问题。

    

    我们研究了无监督句法分析任务，该任务将句子的词和短语组织成一个层次结构，而不使用语言学注释的数据。我们观察到现有的无监督解析器捕捉到了解析结构的不同方面，可以利用这些来提高无监督分析的性能。为此，我们提出了“树平均”的概念，基于此我们进一步提出了一种新的无监督解析的集成方法。为了提高推理效率，我们进一步将集成知识蒸馏到一个学生模型中；这种集成-蒸馏的过程是缓解常见的多教师蒸馏方法中存在的过度平滑问题的有效方法。实验证明我们的方法超过了所有先前的方法，始终表现出其在不同集成组件和领域转移条件下的有效性和稳健性。

    We investigate the unsupervised constituency parsing task, which organizes words and phrases of a sentence into a hierarchical structure without using linguistically annotated data. We observe that existing unsupervised parsers capture differing aspects of parsing structures, which can be leveraged to enhance unsupervised parsing performance. To this end, we propose a notion of "tree averaging," based on which we further propose a novel ensemble method for unsupervised parsing. To improve inference efficiency, we further distill the ensemble knowledge into a student model; such an ensemble-then-distill process is an effective approach to mitigate the over-smoothing problem existing in common multi-teacher distilling methods. Experiments show that our method surpasses all previous approaches, consistently demonstrating its effectiveness and robustness across various runs, with different ensemble components, and under domain-shift conditions.
    
[^25]: 解读诊断：大型语言模型解释如何影响临床决策

    Deciphering Diagnoses: How Large Language Models Explanations Influence Clinical Decision Making. (arXiv:2310.01708v1 [cs.CL])

    [http://arxiv.org/abs/2310.01708](http://arxiv.org/abs/2310.01708)

    大型语言模型在医学决策中生成的解释能显著提高医生对诊断的一致率，并指出潜在的LLM输出错误。这项研究强调了LLMs在医疗保健中的潜力和挑战，以及确保患者安全和最佳临床效用的需求。

    

    临床决策支持系统（CDSS）利用基于证据的知识和患者数据提供实时建议，大型语言模型（LLMs）作为生成医学决策的纯文本解释的有希望的工具。本研究探讨了LLMs在基于患者症状的诊断解释生成中的有效性和可靠性。三名有经验的医生评估了LLM生成的关于患者症状与医生及模型分配的诊断之间的联系的解释。实验结果表明，LLM解释显著提高了医生对给定诊断的一致率，并凸显了LLM输出中的潜在错误，范围从5%到30%不等。本研究凸显了LLMs在医疗保健中的潜力和挑战，并强调了需要仔细整合和评估以确保患者安全和最佳临床效用。

    Clinical Decision Support Systems (CDSS) utilize evidence-based knowledge and patient data to offer real-time recommendations, with Large Language Models (LLMs) emerging as a promising tool to generate plain-text explanations for medical decisions. This study explores the effectiveness and reliability of LLMs in generating explanations for diagnoses based on patient complaints. Three experienced doctors evaluated LLM-generated explanations of the connection between patient complaints and doctor and model-assigned diagnoses across several stages. Experimental results demonstrated that LLM explanations significantly increased doctors' agreement rates with given diagnoses and highlighted potential errors in LLM outputs, ranging from 5% to 30%. The study underscores the potential and challenges of LLMs in healthcare and emphasizes the need for careful integration and evaluation to ensure patient safety and optimal clinical utility.
    
[^26]: 解开神经文本退化之谜

    Closing the Curious Case of Neural Text Degeneration. (arXiv:2310.01693v1 [cs.CL])

    [http://arxiv.org/abs/2310.01693](http://arxiv.org/abs/2310.01693)

    本研究解释了截断采样方法的有效性，并提出了一种基于softmax瓶颈的更精确的采样策略。

    

    尽管在语言生成中普遍使用，但为何像核采样这样的截断采样启发式方法如此有效仍然不为人所知。我们通过证明截断方法（丢弃某些概率阈值以下的记号，最常见的截断类型）可以保证所有采样出来的记号都有非零真实概率，提供了对截断采样方法有效性的理论解释。然而，这些阈值只是粗略的启发式方法，必然也丢弃了一些具有非零真实概率的记号。为了追求更精确的采样策略，我们展示了如何利用已知的模型错误源——softmax瓶颈，证明某些记号具有非零真实概率，而不依赖于阈值。基于我们的发现，我们开发了一种实验性的截断策略，并进行了展示这种算法的前期研究。我们的评估结果表明我们的方法优于基于阈值的对应方法。

    Despite their ubiquity in language generation, it remains unknown why truncation sampling heuristics like nucleus sampling are so effective. We provide a theoretical explanation for the effectiveness of the truncation sampling by proving that truncation methods that discard tokens below some probability threshold (the most common type of truncation) can guarantee that all sampled tokens have nonzero true probability. However, thresholds are a coarse heuristic, and necessarily discard some tokens with nonzero true probability as well. In pursuit of a more precise sampling strategy, we show that we can leverage a known source of model errors, the softmax bottleneck, to prove that certain tokens have nonzero true probability, without relying on a threshold. Based on our findings, we develop an experimental truncation strategy and the present pilot studies demonstrating the promise of this type of algorithm. Our evaluations show that our method outperforms its threshold-based counterparts 
    
[^27]: 零样本连续提示传递：在语言模型之间泛化任务语义

    Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models. (arXiv:2310.01691v1 [cs.CL])

    [http://arxiv.org/abs/2310.01691](http://arxiv.org/abs/2310.01691)

    这项工作提出了一种零样本连续提示传递方法，通过将源提示编码到相对空间中，并搜索相应的目标提示，在不同的语言模型之间实现了任务语义的泛化，实验证实了该方法的有效性。

    

    在自然语言处理（NLP）中，通过调整提示已经成为一种越来越受欢迎的方法，用于将大型语言模型适应特定任务。然而，这些提示，特别是连续提示，在不同模型之间的可传递性仍然是一个挑战。在这项工作中，我们提出了一种零样本连续提示传递方法，其中源提示被编码到相对空间中，并搜索相应的目标提示以将其传递到目标模型。实验结果证实了我们方法的有效性，表明连续提示中的“任务语义”可以在各种语言模型之间泛化。此外，我们发现将来自多个源模型的“任务语义”结合可以进一步增强传递的泛化能力。

    Prompt tuning in natural language processing (NLP) has become an increasingly popular method for adapting large language models to specific tasks. However, the transferability of these prompts, especially continuous prompts, between different models remains a challenge. In this work, we propose a zero-shot continuous prompt transfer method, where source prompts are encoded into relative space and the corresponding target prompts are searched for transferring to target models. Experimental results confirm the effectiveness of our method, showing that 'task semantics' in continuous prompts can be generalized across various language models. Moreover, we find that combining 'task semantics' from multiple source models can further enhance the generalizability of transfer.
    
[^28]: 一种模型统治全球？朝着端到端联合说话人先行护理和语音识别迈进。

    One model to rule them all ? Towards End-to-End Joint Speaker Diarization and Speech Recognition. (arXiv:2310.01688v1 [eess.AS])

    [http://arxiv.org/abs/2310.01688](http://arxiv.org/abs/2310.01688)

    本文提出了一种名为SLIDAR的新框架，实现了联合说话人先行护理和自动语音识别，能够处理任意长度的输入和任意数量的说话人，并通过将本地输出合并来获得最终的结果。实验证实了该方法在近距离和远距离语音场景中的有效性。

    

    本文提出了一种名为SLIDAR（滑动窗口先行护理增强识别）的联合说话人先行护理（SD）和自动语音识别（ASR）的新框架。SLIDAR可以处理任意长度的输入，可以处理任意数量的说话人，同时有效地解决了“谁在什么时候说了什么”的问题。SLIDAR采用滑动窗口方法，包括了一个端到端的先行护理增强语音转录（E2E DAST）模型，该模型为每个窗口提供本地的转录、先行护理和说话人嵌入。E2E DAST模型基于编码器-解码器架构，利用了最近的技术，如序列化输出训练和“Whisper-style”提示。然后，通过对说话人嵌入进行聚类来合并本地输出，得到最终的SD+ASR结果。

    This paper presents a novel framework for joint speaker diarization (SD) and automatic speech recognition (ASR), named SLIDAR (sliding-window diarization-augmented recognition). SLIDAR can process arbitrary length inputs and can handle any number of speakers, effectively solving ``who spoke what, when'' concurrently. SLIDAR leverages a sliding window approach and consists of an end-to-end diarization-augmented speech transcription (E2E DAST) model which provides, locally, for each window: transcripts, diarization and speaker embeddings. The E2E DAST model is based on an encoder-decoder architecture and leverages recent techniques such as serialized output training and ``Whisper-style" prompting. The local outputs are then combined to get the final SD+ASR result by clustering the speaker embeddings to get global speaker identities. Experiments performed on monaural recordings from the AMI corpus confirm the effectiveness of the method in both close-talk and far-field speech scenarios.
    
[^29]: VAL：带有GPT对话解析的交互式任务学习

    VAL: Interactive Task Learning with GPT Dialog Parsing. (arXiv:2310.01627v1 [cs.HC])

    [http://arxiv.org/abs/2310.01627](http://arxiv.org/abs/2310.01627)

    VAL是一种交互式任务学习系统，通过结合大型语言模型（LLM）和符号集成的理念，实现了从自然语言中进行交互式学习的分层任务知识的获取。所获得的知识可解释并能够推广到执行新任务。在视频游戏环境中的用户交互实验表明，VAL能够从有限的指令中成功学到有效的任务知识。

    

    强化学习通常需要数百万个样本来生成静态的黑箱模型。相比之下，交互式任务学习（ITL）强调从人类提供的有限指令中逐步获得知识，这些指令以自然语言等形式出现。然而，在实践中，ITL系统往往受到脆弱、容易出错的语言解析的困扰。大型语言模型（LLMs）对脆弱性有一定的抵抗能力，但不具备可解释性，也无法进行增量学习。我们提出了VAL，一种具有新的LLM/符号集成理念的ITL系统。通过仅在特定任务中使用LLMs（例如谓词和参数选择），在算法框架内，VAL利用LLMs的优势，支持从自然语言中交互式学习分层任务知识。所获得的知识是人类可解释的，并能够推广到支持执行新任务而不需要额外的训练。我们在一个视频游戏环境中研究了用户与VAL的交互，发现大部分用户能够从有限的指令中成功学到有效的任务知识。

    Reinforcement learning often requires millions of examples to produce static, black-box models. In contrast, interactive task learning (ITL) emphasizes incremental knowledge acquisition from limited instruction provided by humans in modalities such as natural language. However, in practice, ITL systems often suffers from brittle, error-prone language parsing. Large language models (LLMs) are resistant to brittleness but are not interpretable and cannot learn incrementally. We present VAL, an ITL system with a new philosophy for LLM/symbolic integration. By using LLMs only for specific tasks -- such as predicate and argument selection -- within an algorithmic framework, VAL reaps the benefits of LLMs to support interactive learning of hierarchical task knowledge from natural language. Acquired knowledge is human interpretable and generalizes to support execution of novel tasks without additional training. We studied users' interactions with VAL in a video game setting, finding that most
    
[^30]: K-12教育中教授自然语言处理的数字学习环境综述

    A Review of Digital Learning Environments for Teaching Natural Language Processing in K-12 Education. (arXiv:2310.01603v1 [cs.CL])

    [http://arxiv.org/abs/2310.01603](http://arxiv.org/abs/2310.01603)

    这篇综述论文介绍了K-12教育中教授自然语言处理的数字学习环境，并讨论了现有工具的支持、解释性和评估结果，以指导未来研究努力改进现有工具、开发新工具，并探索更有效和包容性的NLP整合策略。

    

    自然语言处理（NLP）在我们的日常生活中起着重要作用，并且已成为K-12人工智能教育的重要组成部分。随着孩子们在NLP驱动的应用程序中成长，向他们介绍NLP概念对培养他们对语言处理、语言生成以及人工智能和NLP的伦理问题的理解至关重要。本文综述了K-12教育中教授NLP的数字学习环境。具体而言，它探讨了现有的数字学习工具，讨论了它们如何支持特定的NLP任务和流程，并调查了它们在教育环境中的解释性和评估结果。通过检查这些工具的优点和限制，本文综述为我们了解当前K-12教育中NLP学习工具的现状提供了指导。它旨在引导未来的研究努力，以改进现有工具、开发新工具，并探索更有效和包容性的NLP整合策略。

    Natural Language Processing (NLP) plays a significant role in our daily lives and has become an essential part of Artificial Intelligence (AI) education in K-12. As children grow up with NLP-powered applications, it is crucial to introduce NLP concepts to them, fostering their understanding of language processing, language generation, and ethical implications of AI and NLP. This paper presents a comprehensive review of digital learning environments for teaching NLP in K-12. Specifically, it explores existing digital learning tools, discusses how they support specific NLP tasks and procedures, and investigates their explainability and evaluation results in educational contexts. By examining the strengths and limitations of these tools, this literature review sheds light on the current state of NLP learning tools in K-12 education. It aims to guide future research efforts to refine existing tools, develop new ones, and explore more effective and inclusive strategies for integrating NLP i
    
[^31]: 防御作者身份识别攻击

    Defending Against Authorship Identification Attacks. (arXiv:2310.01568v1 [cs.CL])

    [http://arxiv.org/abs/2310.01568](http://arxiv.org/abs/2310.01568)

    本文回顾了作者身份识别领域的研究进展，主要关注如何通过混淆写作风格来保护公开交流中的匿名性。

    

    作者身份识别已经证明了其在推断未署名文档的作者身份方面的高效性，即使在仔细删除敏感个人信息的情况下。在数字时代，个人通过其书面内容留下了持久的数字足迹，不论是在社交媒体上发布，保存在雇主的计算机上，还是其他地方。当个人需要公开交流但希望保持匿名时，几乎没有什么可用的方法来保护他们免受不想要的作者身份识别。这种前所未有的隐私威胁在像揭发丑闻的情况下是显而易见的。针对作者身份识别攻击的提出的防御方法主要旨在混淆个人的写作风格，从而使其与其先前的写作不可关联，同时保持原始的意义和语法完整性。本文提供了对过去两个世纪以来这一研究领域的进展的全面回顾。

    Authorship identification has proven unsettlingly effective in inferring the identity of the author of an unsigned document, even when sensitive personal information has been carefully omitted. In the digital era, individuals leave a lasting digital footprint through their written content, whether it is posted on social media, stored on their employer's computers, or located elsewhere. When individuals need to communicate publicly yet wish to remain anonymous, there is little available to protect them from unwanted authorship identification. This unprecedented threat to privacy is evident in scenarios such as whistle-blowing. Proposed defenses against authorship identification attacks primarily aim to obfuscate one's writing style, thereby making it unlinkable to their pre-existing writing, while concurrently preserving the original meaning and grammatical integrity. The presented work offers a comprehensive review of the advancements in this research area spanning over the past two de
    
[^32]: 使检索增强的语言模型对无关上下文具有鲁棒性

    Making Retrieval-Augmented Language Models Robust to Irrelevant Context. (arXiv:2310.01558v1 [cs.CL])

    [http://arxiv.org/abs/2310.01558](http://arxiv.org/abs/2310.01558)

    本文分析了检索增强的语言模型在开放领域问答中的性能问题，并提出了基于自然语言推理的方法来缓解这个问题。

    

    检索增强的语言模型（RALMs）有望产生准确、高效和最新的语言理解系统。RALMs的一个重要目标是在相关时提高模型性能，在不相关时不影响性能。这在多跳推理场景中尤为重要，因为不相关证据的误用会导致连锁错误。然而，最近的研究表明，检索增强有时会对性能产生负面影响。在这项工作中，我们对五个开放领域的问答基准进行了彻底分析，描述了检索降低准确性的情况。然后，我们提出了两种缓解这个问题的方法。首先，一个简单的基准线，根据自然语言推理（NLI）模型筛选出不涉及问题-答案对的检索段落。这可以有效防止性能下降，但代价是舍弃了相关信息。

    Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We then propose two methods to mitigate this issue. First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model. This is effective in preventing performance reduction, but at a cost of also discarding releva
    
[^33]: LLM谎言: 幻觉不是漏洞，而是对抗样本的特征。

    LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples. (arXiv:2310.01469v1 [cs.CL])

    [http://arxiv.org/abs/2310.01469](http://arxiv.org/abs/2310.01469)

    LLM似乎具有丰富的知识和适应多种任务的能力，但我们不能完全信任它们的回答，因为它们会出现幻觉，即捏造不存在的事实以欺骗用户。本文证明了由随机标记组成的无意义提示也能引起LLM产生幻觉回应，并提出了一种对抗方式的自动幻觉触发方法作为幻觉攻击，同时提出了一种简单而有效的防御策略。

    

    大型语言模型（LLM），包括GPT-3.5、LLaMA和PaLM，似乎具有丰富的知识和适应多种任务的能力。然而，我们仍然不能完全信任它们的回答，因为LLM会出现幻觉，即捏造不存在的事实以欺骗用户而不被察觉。幻觉存在的原因和普遍性仍然不清楚。在本文中，我们证明了由随机标记组成的无意义提示也能引起LLM产生幻觉回应。这个现象迫使我们重新审视幻觉可能是对抗样本的另一种视角，并且它与常规的对抗样本具有类似的特征，作为LLM的基本特征。因此，我们以对抗的方式将自动幻觉触发方法形式化为幻觉攻击。最后，我们研究了被攻击的对抗提示的基本特征，并提出了一种简单而有效的防御策略。我们的代码已在GitHub上发布。

    Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still can not completely trust their answer, since LLMs suffer from hallucination--fabricating non-existent facts to cheat users without perception. And the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that non-sense prompts composed of random tokens can also elicit the LLMs to respond with hallucinations. This phenomenon forces us to revisit that hallucination may be another view of adversarial examples, and it shares similar features with conventional adversarial examples as the basic feature of LLMs. Therefore, we formalize an automatic hallucination triggering method as the hallucination attack in an adversarial way. Finally, we explore basic feature of attacked adversarial prompts and propose a simple yet effective defense strategy. Our code is released on GitHub.
    
[^34]: 实体推断竞技场：探究LLMs的对话推理和规划能力的平台

    The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v1 [cs.CL])

    [http://arxiv.org/abs/2310.01468](http://arxiv.org/abs/2310.01468)

    本文提供了一个评估框架，通过向法官提出一系列查询来评估LLMs的对话推理和规划能力。我们发现不同的LLMs在这个任务上表现出显著差异。

    

    目前，大型语言模型（LLMs）在回答明确提问时非常有效。然而，当面临含糊不清的查询时，它们可能行为难以预测并产生错误的输出。这凸显了需要开发能够提出澄清问题以有效解决歧义的智能代理的需求。这种能力需要对多个对话轮次进行复杂的理解、状态跟踪、推理和规划。然而，直接测量这种能力可能具有挑战性。在本文中，我们提供了一个替代性问题，通过向法官提出一系列查询，评估了LLMs推断自己不知道但被法官揭示的实体的能力。这个“实体推断游戏”可以作为一个评估框架，用于探究语言模型的对话推理和规划能力。我们系统地评估了各种LLMs，并发现在这个任务上它们的性能存在显著差异。我们发现强大的LLMs...

    Large language models (LLMs) are currently effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging. In this paper, we offer a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This \textit{entity-deducing game} can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models. We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs
    
[^35]: FedBPT: 高效的适用于大型语言模型的联邦式黑盒提示调优

    FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models. (arXiv:2310.01467v1 [cs.CL])

    [http://arxiv.org/abs/2310.01467](http://arxiv.org/abs/2310.01467)

    FedBPT是一个高效的联邦式黑盒提示调优框架，解决了在大型语言模型中应用联邦学习进行模型微调时的挑战。

    

    预训练语言模型（PLM）在自然语言处理领域取得了巨大的突破，在各种任务中表现出色。然而，这些模型虽然受益于大量的训练数据，但通常需要在特定数据上进行微调以适应不同的下游任务。然而，这种数据适应过程存在安全和隐私问题，特别是在利用用户生成的设备驻留数据时。联邦学习（FL）提供了一种解决方案，允许在没有集中式数据收集的情况下进行协作模型微调。然而，将FL应用于微调PLMs面临诸多挑战，包括受限的模型参数访问、高计算要求和通信开销。本文介绍了一种名为FedBPT的联邦式黑盒提示调优框架，旨在解决这些挑战。FedBPT无需客户端访问模型参数，通过专注于训练最优提示和利用无梯度优化方法，可以减少计算量。

    Pre-trained language models (PLM) have revolutionized the NLP landscape, achieving stellar performances across diverse tasks. These models, while benefiting from vast training data, often require fine-tuning on specific data to cater to distinct downstream tasks. However, this data adaptation process has inherent security and privacy concerns, primarily when leveraging user-generated, device-residing data. Federated learning (FL) provides a solution, allowing collaborative model fine-tuning without centralized data collection. However, applying FL to finetune PLMs is hampered by challenges, including restricted model parameter access, high computational requirements, and communication overheads. This paper introduces Federated Black-box Prompt Tuning (FedBPT), a framework designed to address these challenges. FedBPT does not require the clients to access the model parameters. By focusing on training optimal prompts and utilizing gradient-free optimization methods, FedBPT reduces the nu
    
[^36]: NarrativePlay: 交互式叙事理解

    NarrativePlay: Interactive Narrative Understanding. (arXiv:2310.01459v1 [cs.CL])

    [http://arxiv.org/abs/2310.01459](http://arxiv.org/abs/2310.01459)

    本文介绍了一个名为NarrativePlay的系统，用户可以在虚构环境中扮演虚构角色与其他角色进行互动，并利用大型语言模型（LLMs）生成类人回应。该系统通过自动生成的视觉展示和角色的言谈，极大地提升了用户体验。通过在侦探和冒险故事中进行评估，用户可以通过对话要么探索世界要么提高与叙事角色的亲和力。

    

    本文介绍了NarrativePlay，一个创新的系统，允许用户在虚构环境中扮演虚构角色，并与其他角色进行互动，如小说等故事中。我们利用大型语言模型（LLMs）根据从叙事中提取的个性特征生成类人回应。该系统结合了自动生成的叙事设置的视觉展示、角色形象以及角色的言谈，极大地提升了用户体验。我们的方法摒弃了预定义的沙盒，而是从用户选择的角色的视角，关注从叙事中提取的主要故事情节。NarrativePlay已在侦探和冒险故事两种类型的叙事中进行了评估，在这些叙事中，用户可以通过对话要么探索世界要么提高与叙事角色的亲和力。

    In this paper, we introduce NarrativePlay, a novel system that allows users to role-play a fictional character and interact with other characters in narratives such as novels in an immersive environment. We leverage Large Language Models (LLMs) to generate human-like responses, guided by personality traits extracted from narratives. The system incorporates auto-generated visual display of narrative settings, character portraits, and character speech, greatly enhancing user experience. Our approach eschews predefined sandboxes, focusing instead on main storyline events extracted from narratives from the perspective of a user-selected character. NarrativePlay has been evaluated on two types of narratives, detective and adventure stories, where users can either explore the world or improve their favorability with the narrative characters through conversations.
    
[^37]: 通过随机化潜在表示来迷惑文本愚弄者

    Fooling the Textual Fooler via Randomizing Latent Representations. (arXiv:2310.01452v1 [cs.CL])

    [http://arxiv.org/abs/2310.01452](http://arxiv.org/abs/2310.01452)

    该论文提出了一种轻量级的攻击无关防御策略AdvFooler，通过随机化输入的潜在表示来困惑基于查询的黑盒攻击，从而迷惑文本愚弄者。

    

    尽管在各种自然语言处理任务中表现出色，但近期的研究表明，自然语言处理模型容易受到敌对攻击的影响，即微小地改变输入以导致模型的错误行为。其中，敌对词级扰动是一个被广泛研究和有效的攻击策略。这些攻击在黑盒设置中起作用，不需要访问模型结构或参数，因此可能对现有的自然语言处理应用产生不利影响。为了进行攻击，对手多次查询受害模型，以确定输入文本中最重要的单词，并用它们对应的同义词替换这些单词。在这项工作中，我们提出了一种轻量级和攻击无关的防御，其主要目标是困惑基于查询的黑盒攻击中产生敌对示例的过程；即愚弄文本愚弄者。这种防御名为AdvFooler，通过随机化输入的潜在表示来实现。

    Despite outstanding performance in a variety of NLP tasks, recent studies have revealed that NLP models are vulnerable to adversarial attacks that slightly perturb the input to cause the models to misbehave. Among these attacks, adversarial word-level perturbations are well-studied and effective attack strategies. Since these attacks work in black-box settings, they do not require access to the model architecture or model parameters and thus can be detrimental to existing NLP applications. To perform an attack, the adversary queries the victim model many times to determine the most important words in an input text and to replace these words with their corresponding synonyms. In this work, we propose a lightweight and attack-agnostic defense whose main goal is to perplex the process of generating an adversarial example in these query-based black-box attacks; that is to fool the textual fooler. This defense, named AdvFooler, works by randomizing the latent representation of the input at 
    
[^38]: 大型语言模型评估的元语义模板

    Meta Semantic Template for Evaluation of Large Language Models. (arXiv:2310.01448v1 [cs.CL])

    [http://arxiv.org/abs/2310.01448](http://arxiv.org/abs/2310.01448)

    提出了一种通过创建元语义模板来评估大型语言模型（LLM）对语义理解能力的方法，该方法利用现有数据集生成新的超出分布（OOD）评估集。

    

    大型语言模型（LLM）是否真正理解语言的语义，还是仅仅记住训练数据？最近对LLM潜在数据污染的担忧引起了社区对LLM评估研究的关注。在本文中，我们提出了MSTemp，一种通过创建元语义模板来评估LLM对语义理解能力的方法。MSTemp的核心不是直接在现有基准数据集上进行评估，而是使用现有数据集作为种子生成新的超出分布（OOD）评估集。具体而言，对于给定的句子，MSTemp利用另一个语言模型生成新样本，同时保留其语义。这些新样本被称为原句子的语义模板。然后，MSTemp通过句子解析和随机替换词语来生成评估样本。MSTemp具有高度灵活、动态和成本效益性。我们的初步实验表明，MSTemp-

    Do large language models (LLMs) genuinely understand the semantics of the language, or just memorize the training data? The recent concern on potential data contamination of LLMs has raised awareness of the community to conduct research on LLMs evaluation. In this paper, we propose MSTemp, an approach that creates meta semantic templates to evaluate the semantic understanding ability of LLMs. The core of MSTemp is not to perform evaluation directly on existing benchmark datasets, but to generate new out-of-distribution (OOD) evaluation sets using existing datasets as seeds. Specifically, for a given sentence, MSTemp leverages another language model to generate new samples while preserving its semantics. The new samples are called semantic templates to the original sentence. Then, MSTemp generates evaluation samples via sentence parsing and random word replacement on the semantic templates. MSTemp is highly flexible, dynamic, and cost-effective. Our initial experiments show that MSTemp-
    
[^39]: 大型语言模型推理中的动态策略选择自适应求解器框架

    Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning. (arXiv:2310.01446v1 [cs.CL])

    [http://arxiv.org/abs/2310.01446](http://arxiv.org/abs/2310.01446)

    这个论文提出了一个自适应求解器框架，用于在大型语言模型推理中根据问题难度调整求解策略。这解决了现有方法刚性采用统一方法的问题，提高了计算性能。

    

    大型语言模型(LLM)在处理复杂推理任务时展示了令人印象深刻的能力。在现实世界中，问题往往涉及各种复杂性。人类本能地根据任务的复杂性调整他们的问题解决方法。然而，大多数利用LLM的方法倾向于采用一种统一的方法: 不管问题的复杂性如何，都使用一致的模型、提示方法和问题分解程度。这种刚性可能会带来不必要的计算开销或次优的性能。为了解决这个问题，我们引入了一个自适应求解器框架。它根据问题的难度策略性地调整求解策略。给定一个初始解决方案，该框架使用两个主要模块。初始评估模块评估当前解决方案的充分性。如果需要改进，接下来的自适应模块会介入。在这个模块内，有三个关键的自适应策略。

    Large Language Models (LLMs) are showcasing impressive ability in handling complex reasoning tasks. In real-world situations, problems often span a spectrum of complexities. Humans inherently adjust their problem-solving approaches based on task complexity. However, most methodologies that leverage LLMs tend to adopt a uniform approach: utilizing consistent models, prompting methods, and degrees of problem decomposition, regardless of the problem complexity. Inflexibility of them can bring unnecessary computational overhead or sub-optimal performance. To address this problem, we introduce an Adaptive-Solver framework. It strategically modulates solving strategies based on the difficulties of the problems. Given an initial solution, the framework functions with two primary modules. The initial evaluation module assesses the adequacy of the current solution. If improvements are needed, the subsequent adaptation module comes into play. Within this module, three key adaptation strategies a
    
[^40]: 通过交流使LLM代理适应环境的方法

    Adapting LLM Agents Through Communication. (arXiv:2310.01444v1 [cs.CL])

    [http://arxiv.org/abs/2310.01444](http://arxiv.org/abs/2310.01444)

    这项研究提出了一种名为学习通信（LTC）的训练方法，使用该方法可使LLM代理通过与环境和其他代理的交互不断改进，以适应新任务，而无需过多人类监督。

    

    最近大语言模型(LLM)的进展显示出了人类化代理的潜力。为了帮助这些代理在没有广泛人类监督的情况下适应新任务，我们提出了学习通信（LTC）范式，这是一种新颖的训练方法，使LLM代理能够通过与环境和其他代理的交互不断改进。通过迭代探索和PPO训练，LTC使代理能够将短期经验融入长期记忆。为了优化特定任务的代理交互，我们引入了三种结构化的通信模式：独白，对话，

    Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Through iterative exploration and PPO training, LTC empowers the agent to assimilate short-term experiences into long-term memory. To optimize agent interactions for task-specific learning, we introduce three structured communication patterns: Monologue, Dialogue,
    
[^41]: UPAR：一种受康德启发的促进框架，用于提升大型语言模型的能力

    UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities. (arXiv:2310.01441v1 [cs.CL])

    [http://arxiv.org/abs/2310.01441](http://arxiv.org/abs/2310.01441)

    UPAR提示框架模拟人类认知结构，在大型语言模型中提供了可理解和可检查的推理轨迹，增强了推理的可解释性和准确性，并为现有的提示技术提供了认识论基础。

    

    大型语言模型(LLMs)展示了令人印象深刻的推理能力，许多研究致力于通过提示提升这种能力。尽管有这些努力，统一的认识论基础仍然明显缺失。受康德的先验哲学启发，我们提出了UPAR提示框架，旨在在LLMs中模拟人类认知结构。UPAR框架分为四个阶段：“理解”、“计划”、“行动”和“反思”，使得能够从复杂背景中提取结构化信息，事先规划解决方案，按计划执行，并进行自我反思。这个结构显著增强了LLM推理的可解释性和准确性，产生了人类可理解和可检查的推理轨迹。此外，我们的工作为现有的提示技术提供了认识论基础，可能实现这些方法的系统集成。

    Large Language Models (LLMs) have demonstrated impressive inferential capabilities, with numerous research endeavors devoted to enhancing this capacity through prompting. Despite these efforts, a unified epistemological foundation is still conspicuously absent. Drawing inspiration from Kant's a priori philosophy, we propose the UPAR prompting framework, designed to emulate the structure of human cognition within LLMs. The UPAR framework is delineated into four phases: "Understand", "Plan", "Act", and "Reflect", enabling the extraction of structured information from complex contexts, prior planning of solutions, execution according to plan, and self-reflection. This structure significantly augments the explainability and accuracy of LLM inference, producing a human-understandable and inspectable inferential trajectory. Furthermore, our work offers an epistemological foundation for existing prompting techniques, allowing for a possible systematic integration of these methods. With GPT-4,
    
[^42]: 对鲁迅与周作人争议性文章的重新审视：端木烟的多重声音

    The Many Voices of Duying: Revisiting the Disputed Essays Between Lu Xun and Zhou Zuoren. (arXiv:2310.01440v1 [cs.CL])

    [http://arxiv.org/abs/2310.01440](http://arxiv.org/abs/2310.01440)

    这项研究通过定量方法重新审视鲁迅和周作人1912年以笔名发表的三篇争议性文章，发现鲁迅是其中《看中国》的作者，并且《忘不了祖先告语》可能由鲁迅主要创作或在他的大量修改下完成。第三篇文章展示了两人的影响并具有混合的写作风格。

    

    鲁迅与周作人是中国现代文学中最有影响力的作家之一。除了作为兄弟关系外，他们在写作生涯初期还是密切合作的伙伴。本研究采用定量方法重新审视两兄弟在1912年以笔名发表的三篇争议性文章。我们使用可解释的作者归属模型进行文体分析，以调查这些文章的作者以及兄弟俩的写作风格。研究结果表明《看中国》一文的作者是鲁迅。此外，《忘不了祖先告语》一文似乎要么是鲁迅主要创作，要么是在鲁迅的大量修改下完成的，因为它与周作人认可为自己的《看越山水》在文体上有显著的相似之处。第三篇《民主已缩水，它去了何方》则展现出一种混合的写作风格，表明了两者的影响。

    Lu Xun and Zhou Zuoren stand as two of the most influential writers in modern Chinese literature. Beyond their familial ties as brothers, they were also intimate collaborators during the nascent stages of their writing careers. This research employs quantitative methods to revisit three disputed essays pseudonymously published by the brothers in 1912. Our stylometric analysis uses an interpretable authorship attribution model to investigate the essays' authorship and examine the brothers' respective writing styles. Our findings suggest that 'Looking at the Country of China' was authored by Lu Xun. Moreover, 'People of Yue, Forget Not Your Ancestors' Instructions' seems to be either predominantly authored or extensively revised by Lu Xun given its notable stylistic similarities to 'Looking at the Land of Yue,' a piece Zhou Zuoren recognized as his own, but edited by Lu Xun. The third essay, 'Where Has the Character of the Republic Gone?,' exhibits a 'diluted', mixed writing style, sugge
    
[^43]: 革新移动互动：在移动设备上实现30亿参数的GPT LLM

    Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile. (arXiv:2310.01434v1 [cs.CL])

    [http://arxiv.org/abs/2310.01434](http://arxiv.org/abs/2310.01434)

    本文介绍了一种在移动设备上执行30亿参数的GPT LLM的创新方法，通过集成本地代码和模型量化技术实现了在低内存设备上的平稳运行，并解决了网络依赖性的问题。这种应用不仅具有通用助手功能，还可以实现文本到操作的无缝移动互动。

    

    近年来，人工智能领域取得了显著进展，特别是基于变压器架构的强大大型语言模型（LLM）的出现。云端的LLM，例如OpenAI的ChatGPT，具有令人印象深刻的功能，但由于网络依赖性，延迟和隐私问题令人担忧。本文提出了一种创新的LLM推断方法，展望了未来在没有网络连接的情况下，拥有数十亿参数的LLM可以直接在移动设备上执行。本文展示了一种具有30亿参数的精调GPT LLM，可以在内存只有4GB的设备上平稳运行。通过整合本地代码和模型量化技术，该应用不仅可以作为通用助手，还可以通过文本到操作的功能实现无缝的移动互动。本文提供了有关训练流程、实现细节和测试结果的见解。

    The field of Artificial Intelligence has witnessed remarkable progress in recent years, especially with the emergence of powerful large language models (LLMs) based on the transformer architecture. Cloud-based LLMs, such as OpenAI's ChatGPT, offer impressive capabilities but come with concerns regarding latency and privacy due to network dependencies. This article presents an innovative approach to LLM inference, envisioning a future where LLMs with billions of parameters can be executed directly on mobile devices without network connectivity. The article showcases a fine-tuned GPT LLM with 3 billion parameters that can operate smoothly on devices with as low as 4GB of memory. Through the integration of native code and model quantization techniques, the application not only serves as a general-purpose assistant but also facilitates seamless mobile interactions with text-to-actions features. The article provides insights into the training pipeline, implementation details, test results, 
    
[^44]: 分割与合并：对大型语言模型的位置偏差进行校准

    Split and Merge: Aligning Position Biases in Large Language Model based Evaluators. (arXiv:2310.01432v1 [cs.CL])

    [http://arxiv.org/abs/2310.01432](http://arxiv.org/abs/2310.01432)

    PORTIA是一个旨在校准大型语言模型评估器的位置偏差的对齐系统，通过将答案分割成多个片段，并对其进行对齐，然后将其合并回一个单一的提示，以提高评估的准确性和公正性。

    

    大型语言模型(LLMs)已被证明可以作为自动化评估器，用于评估AI系统生成的答案的质量。然而，这些基于LLM的评估器在使用对比评估候选答案时存在位置偏差或不一致性，无视内容而偏向于第一个或第二个答案。为了解决这个问题，我们提出了PORTIA，这是一个基于对齐的系统，旨在模拟人类的比较策略，以轻量级但有效的方式校准位置偏差。具体而言，PORTIA将答案分割成多个片段，对比候选答案中的相似内容进行对齐，并将它们合并回一个单一的提示，以供LLMs评估。我们使用六种不同的LLM进行了大量实验，评估了11,520个答案对。我们的结果表明，PORTIA显著提高了所有模型和对比形式的一致性率，平均相对改进率达到47.46%。引人注目的是，PORTIA使得LLMs能够评估中对位置偏差进行校准的创新方法，从而提高了评估的准确性和公正性。

    Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, these LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, aligns similar content across candidate answers, and then merges them back into a single prompt for evaluation by LLMs. We conducted extensive experiments with six diverse LLMs to evaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances the consistency rates for all the models and comparison forms tested, achieving an average relative improvement of 47.46%. Remarkably, PORTIA enables le
    
[^45]: 视听中的讽刺：基准测试和拓展以提高多模式讽刺检测

    Sarcasm in Sight and Sound: Benchmarking and Expansion to Improve Multimodal Sarcasm Detection. (arXiv:2310.01430v1 [cs.CL])

    [http://arxiv.org/abs/2310.01430](http://arxiv.org/abs/2310.01430)

    本研究旨在通过使用语言、语音和视觉编码器对MUStARD++数据集进行严格基准测试，提高多模式讽刺检测的性能，并通过新增的数据集片段进一步改善类别不平衡问题。

    

    MUStARD数据集及其情绪识别扩展MUStARD++的引入，已经确定讽刺是一种多模式现象，不仅通过自然语言文本表达，还通过语言（如音调和语调）和视觉线索（面部表情）表达。通过考虑最先进的语言、语音和视觉编码器，我们旨在对MUStARD++数据集进行严格的基准测试，充分利用其多模式丰富性，使宏观F1值比现有基准提高2％。此外，为了解决MUStARD++中“讽刺类型”类别的不平衡，我们提出了一个名为“MUStARD++ Balanced”的扩展，将此扩展中的实例分布在训练集和测试集中进行基准测试，使宏观F1值进一步提高2.4％。这些新片段来自于一部名为《豪斯医生》的新颖资源，增加了数据集的多样性。

    The introduction of the MUStARD dataset, and its emotion recognition extension MUStARD++, have identified sarcasm to be a multi-modal phenomenon -expressed not only in natural language text, but also through manners of speech (like tonality and intonation) and visual cues (facial expression). With this work, we aim to perform a rigorous benchmarking of the MUStARD++ dataset by considering state-of-the-art language, speech, and visual encoders, for fully utilizing the totality of the multi-modal richness that it has to offer, achieving a 2\% improvement in macro-F1 over the existing benchmark. Additionally, to cure the imbalance in the `sarcasm type' category in MUStARD++, we propose an extension, which we call \emph{MUStARD++ Balanced}, benchmarking the same with instances from the extension split across both train and test sets, achieving a further 2.4\% macro-F1 boost. The new clips were taken from a novel source -- the TV show, House MD, which adds to the diversity of the dataset,
    
[^46]: Chatmap：大型语言模型与地图数据的交互

    Chatmap : Large Language Model Interaction with Cartographic Data. (arXiv:2310.01429v1 [cs.CL])

    [http://arxiv.org/abs/2310.01429](http://arxiv.org/abs/2310.01429)

    本研究通过微调相对较小规模的大型语言模型（LLM），以提供对OpenStreetMap（OSM）数据的语言接口。用户可以通过该界面查询有关特定地理位置的属性和趋势。

    

    大型语言模型（LLMs）的快速发展和广泛可用性，结合强大的微调方法，催生了创新和务实应用的需求。使LLMs能够识别和解释地理空间数据，并提供对庞大的地图数据集的语言访问能力非常重要。OpenStreetMap（OSM）是最雄心勃勃的开源全球倡议，提供详细的城市和农村地理数据，由超过1000万的贡献者社区策划，为LLM应用提供了巨大潜力。在本研究中，我们演示了使用相对较小规模（10亿参数）的LLM通过较强大的教师模型策划的相对较小的人工数据集进行微调的概念验证和详细过程，以提供对任意城市区域的OSM数据的语言界面。通过该界面，用户可以查询位置的属性、协同性和趋势。

    The swift advancement and widespread availability of foundational Large Language Models (LLMs), complemented by robust fine-tuning methodologies, have catalyzed their adaptation for innovative and industrious applications. Enabling LLMs to recognize and interpret geospatial data, while offering a linguistic access to vast cartographic datasets, is of significant importance. OpenStreetMap (OSM) is the most ambitious open-source global initiative offering detailed urban and rural geographic data, curated by a community of over 10 million contributors, which constitutes a great potential for LLM applications. In this study, we demonstrate the proof of concept and details of the process of fine-tuning a relatively small scale (1B parameters) LLM with a relatively small artificial dataset curated by a more capable teacher model, in order to provide a linguistic interface to the OSM data of an arbitrary urban region. Through this interface, users can inquire about a location's attributes, co
    
[^47]: 注意力排序在长文本语言模型中应对新近偏见

    Attention Sorting Combats Recency Bias In Long Context Language Models. (arXiv:2310.01427v1 [cs.CL])

    [http://arxiv.org/abs/2310.01427](http://arxiv.org/abs/2310.01427)

    注意力排序可以改善长文本语言模型的性能，通过对注意力进行排序并重复生成，解决了当前模型在整合长上下文时的问题。

    

    当前的语言模型在生成过程中往往未能高效地整合长上下文。我们表明，这个问题的一个主要原因是在预训练期间可能学到的注意力先验：上下文中较早出现的相关信息平均来说被关注的较少。然而，即使模型未能在回应中使用来自相关文档的信息，它们仍然相对于同一位置上的无关文档给予偏爱的注意力。基于这一事实，我们利用"注意力排序"：在解码过程中执行一步，按照他们接收到的注意力进行排序（最高的注意力排在最后），重复该过程，使用新排序的上下文生成答案。我们发现，注意力排序提高了长上下文模型的性能。我们的研究结果突显了使用现成的语言模型进行检索增强生成的一些挑战。

    Current language models often fail to incorporate long contexts efficiently during generation. We show that a major contributor to this issue are attention priors that are likely learned during pre-training: relevant information located earlier in context is attended to less on average. Yet even when models fail to use the information from a relevant document in their response, they still pay preferential attention to that document compared to an irrelevant document at the same position. We leverage this fact to introduce ``attention sorting'': perform one step of decoding, sort documents by the attention they receive (highest attention going last), repeat the process, generate the answer with the newly sorted context. We find that attention sorting improves performance of long context models. Our findings highlight some challenges in using off-the-shelf language models for retrieval augmented generation.
    
[^48]: Borges与AI

    Borges and AI. (arXiv:2310.01425v1 [cs.CL])

    [http://arxiv.org/abs/2310.01425](http://arxiv.org/abs/2310.01425)

    这篇论文主张通过探索乔治·路易斯·博尔赫斯的意象来理解大型语言模型和人工智能之间的关系。

    

    许多人认为大型语言模型（LLMs）开启了人工智能（AI）时代。一些人看到了机遇，而其他人则看到了危险。然而，支持者和反对者都通过科幻小说中流行的意象来理解AI。机器是否会变得有感知能力并反抗其创造者？我们是否会经历纸夹夹子启示？在回答这些问题之前，我们首先应该问一下，这种心理意象是否对手头的现象提供了良好的描述。仅通过神灵的情绪来理解天气模式的方法是有限的。相反，本文主张通过乔治·路易斯·博尔赫斯的意象来理解LLMs及其与AI的关系，博尔赫斯是20世纪文学大师，魔幻现实主义先驱和后现代文学的前奏。这种探索方式带来了新的视角，阐明了语言模型与人工智能之间的关系。

    Many believe that Large Language Models (LLMs) open the era of Artificial Intelligence (AI). Some see opportunities while others see dangers. Yet both proponents and opponents grasp AI through the imagery popularised by science fiction. Will the machine become sentient and rebel against its creators? Will we experience a paperclip apocalypse? Before answering such questions, we should first ask whether this mental imagery provides a good description of the phenomenon at hand. Understanding weather patterns through the moods of the gods only goes so far. The present paper instead advocates understanding LLMs and their connection to AI through the imagery of Jorge Luis Borges, a master of 20th century literature, forerunner of magical realism, and precursor to postmodern literature. This exercise leads to a new perspective that illuminates the relation between language modelling and artificial intelligence.
    
[^49]: 从语言模型中识别和减轻隐私风险：一项调查

    Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey. (arXiv:2310.01424v1 [cs.CL])

    [http://arxiv.org/abs/2310.01424](http://arxiv.org/abs/2310.01424)

    这项调查对语言模型（LMs）中的隐私风险进行了研究和减轻措施的探讨，通过分类法和调查现有攻击，提出了关键趋势，并讨论现有的减轻策略的优势和局限性，指出关键差距和未来工作方向。

    

    语言模型（LMs）的快速发展使其被广泛采用于许多领域。除了潜在的好处外，这些模型还带来了一系列风险，包括隐私风险。尤其是随着LMs规模的增加，它们对训练数据的记忆潜力增加，从而导致泄露私人信息的风险。随着LMs的日益普及，我们必须了解这些隐私风险以及如何减轻它们。为了帮助研究人员和决策者了解LM隐私攻击和减轻措施的知识状况，包括需要更多工作的领域，我们介绍了第一份关于LM隐私的技术调查。我们（i）确定了攻击在LMs上存在的显著维度的分类法，（ii）调查现有攻击并使用我们的分类法来突出主要趋势，（iii）讨论现有的减轻策略，突出其优势和局限性，识别关键差距，展示开放问题和建议未来工作方向。

    Rapid advancements in language models (LMs) have led to their adoption across many sectors. Alongside the potential benefits, such models present a range of risks, including around privacy. In particular, as LMs have grown in size, the potential to memorise aspects of their training data has increased, resulting in the risk of leaking private information. As LMs become increasingly widespread, it is vital that we understand such privacy risks and how they might be mitigated. To help researchers and policymakers understand the state of knowledge around privacy attacks and mitigations, including where more work is needed, we present the first technical survey on LM privacy. We (i) identify a taxonomy of salient dimensions where attacks differ on LMs, (ii) survey existing attacks and use our taxonomy of dimensions to highlight key trends, (iii) discuss existing mitigation strategies, highlighting their strengths and limitations, identifying key gaps and demonstrating open problems and are
    
[^50]: AI生成文本检测工具的实证研究

    An Empirical Study of AI Generated Text Detection Tools. (arXiv:2310.01423v1 [cs.CL])

    [http://arxiv.org/abs/2310.01423](http://arxiv.org/abs/2310.01423)

    本研究实证研究了AI生成文本检测工具在多领域ChatGPT材料中的有效性，并创建了一个多领域数据集来测试最先进API和工具的能力。

    

    自从ChatGPT成为一种重要的AI生成文本模型以来，它在各种应用中（包括软件开发和维护）提供高质量的回应，吸引了许多人的兴趣。ChatGPT有很大的潜力，但其误用可能会带来严重问题，特别是在教育和公共安全领域。目前已经有几种AI生成文本检测工具可供使用，但它们都是在真实文本上进行测试的。然而，需要进一步研究它们对多领域ChatGPT材料的有效性。本研究旨在通过创建一个多领域数据集来测试用于检测大学和其他研究机构使用的最先进API和工具的能力。为此，创建了一个包含文章、摘要、故事、新闻和产品评论的大型数据集。第二步是使用新创建的数据集来测试六种工具的性能。

    Since ChatGPT has emerged as a major AIGC model, providing high-quality responses across a wide range of applications (including software development and maintenance), it has attracted much interest from many individuals. ChatGPT has great promise, but there are serious problems that might arise from its misuse, especially in the realms of education and public safety. Several AIGC detectors are available, and they have all been tested on genuine text. However, more study is needed to see how effective they are for multi-domain ChatGPT material. This study aims to fill this need by creating a multi-domain dataset for testing the state-of-the-art APIs and tools for detecting artificially generated information used by universities and other research institutions. A large dataset consisting of articles, abstracts, stories, news, and product reviews was created for this study. The second step is to use the newly created dataset to put six tools through their paces. Six different artificial 
    
[^51]: Ruffle&Riley：走向自动化的对话辅导系统引导

    Ruffle&Riley: Towards the Automated Induction of Conversational Tutoring Systems. (arXiv:2310.01420v1 [cs.CL])

    [http://arxiv.org/abs/2310.01420](http://arxiv.org/abs/2310.01420)

    本文介绍了一种利用大型语言模型自动诱导辅导脚本和自动协调脚本的新型对话辅导系统。在初步的用户研究中，与其他简单的问答聊天机器人和阅读活动相比，系统在后测分数上没有显著差异。

    

    对话辅导系统（CTS）通过自然语言交互提供学习体验。它们被认为能够促进高水平的认知参与，并在推理任务中有益于学习成果。然而，撰写CTS内容所需的时间和成本是广泛应用的主要障碍。在本文中，我们介绍一种新型的CTS，它利用了大型语言模型（LLM）的最新进展：首先，该系统可以从教学文本自动诱导出辅导脚本。其次，该系统通过两个基于LLM的代理人（Ruffle&Riley）在学以教学的形式中自动协调脚本。该系统允许自由对话，遵循ITS典型的外部/内部循环结构。在一个初步的被试者在线用户研究（N = 100）中，将Ruffle&Riley与更简单的问答聊天机器人和阅读活动进行比较，我们发现在后测分数上没有显著差异。

    Conversational tutoring systems (CTSs) offer learning experiences driven by natural language interaction. They are known to promote high levels of cognitive engagement and benefit learning outcomes, particularly in reasoning tasks. Nonetheless, the time and cost required to author CTS content is a major obstacle to widespread adoption. In this paper, we introduce a novel type of CTS that leverages the recent advances in large language models (LLMs) in two ways: First, the system induces a tutoring script automatically from a lesson text. Second, the system automates the script orchestration via two LLM-based agents (Ruffle&Riley) with the roles of a student and a professor in a learning-by-teaching format. The system allows a free-form conversation that follows the ITS-typical outer-/inner-loop structure. In an initial between-subject online user study (N = 100) comparing Ruffle&Riley to simpler QA chatbots and reading activity, we found no significant differences in post-test scores. 
    
[^52]: Cordyceps@LT-EDI：使用Reddit和自我训练进行抑郁症检测

    Cordyceps@LT-EDI: Depression Detection with Reddit and Self-training. (arXiv:2310.01418v1 [cs.CL])

    [http://arxiv.org/abs/2310.01418](http://arxiv.org/abs/2310.01418)

    本论文提出了一种利用自我训练和Reddit进行抑郁症检测的方法，通过分类未标记的社交媒体帖子来预测用户是否有抑郁症，并在相关任务中取得了较好的排名。

    

    抑郁症是一种令人丧失能力且并不罕见的疾病。实际上，对过度使用社交媒体的研究表明与抑郁症、注意力缺陷多动障碍（ADHD）以及其他心理健康问题存在相关性。鉴于有大量使用社交媒体的人群，存在着可能未被诊断的用户和他们所发布的帖子的庞大人口。在本论文中，我们提出了一种使用半监督学习技术的抑郁症严重程度检测系统，用于预测帖子是否来自正在经历严重、中等或轻微（非诊断性）抑郁症状的用户。具体来说，我们使用训练模型对Reddit上大量未标记的社交媒体帖子进行分类，然后利用这些生成的标签来训练一个更强大的分类器。我们在Detecting Signs of Depression from Social Media Text LT-EDI@RANLP 2023的任务中展示了这个框架，我们的框架在整体排名中名列第三。

    Depression is debilitating, and not uncommon. Indeed, studies of excessive social media users show correlations with depression, ADHD, and other mental health concerns. Given that there is a large number of people with excessive social media usage, then there is a significant population of potentially undiagnosed users and posts that they create. In this paper, we propose a depression severity detection system using a semi-supervised learning technique to predict if a post is from a user who is experiencing severe, moderate, or low (non-diagnostic) levels of depression. Namely, we use a trained model to classify a large number of unlabelled social media posts from Reddit, then use these generated labels to train a more powerful classifier. We demonstrate our framework on Detecting Signs of Depression from Social Media Text LT-EDI@RANLP 2023 shared task, where our framework ranks 3rd overall.
    
[^53]: 表示工程化：AI透明化的自上而下方法

    Representation Engineering: A Top-Down Approach to AI Transparency. (arXiv:2310.01405v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.01405](http://arxiv.org/abs/2310.01405)

    这项研究介绍了一种名为表示工程化（RepE）的自上而下方法，通过借鉴认知神经科学的见解，提供了一种增强AI系统透明性的解决方案。该方法将集群级别的表示放在分析的核心，为监测和操纵深度神经网络中的高级认知现象提供了新的方法，并展示了在解决与安全相关的问题上的潜力。

    

    本文中，我们确定并描述了表示工程化（RepE）这一新兴领域，这是一种通过借鉴认知神经科学的见解来增强AI系统透明性的方法。RepE将集群级别的表示放在分析的核心，而不是神经元或电路，为我们提供了监测和操纵深度神经网络（DNNs）中高级认知现象的新方法。我们提供了RepE技术的基准和初步分析，显示它们提供了简单而有效的解决方案，用于改善我们对大型语言模型的理解和控制。我们展示了这些方法如何在包括诚实性、无害性、追求权力等一系列与安全相关的问题上发挥作用，展示了自上而下透明性研究的潜力。我们希望这项工作能够促进RepE的进一步探索，并推动AI系统的透明性和安全性的进步。

    In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.
    
[^54]: 关于基于训练的ChatGPT检测方法的泛化性研究

    On the Generalization of Training-based ChatGPT Detection Methods. (arXiv:2310.01307v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.01307](http://arxiv.org/abs/2310.01307)

    本研究综合调查了基于训练的ChatGPT检测方法在分布漂移下的泛化行为，包括提示、文本长度、主题和语言任务，在新数据集上揭示了有启示性的发现，为未来方法或数据的开发提供了指导。

    

    ChatGPT是最受欢迎的语言模型之一，在各种自然语言任务上表现出色。因此，有迫切的需求从人类编写的文本中检测出由ChatGPT生成的文本。一种广泛研究的方法是训练分类模型来区分二者。然而，现有研究也表明，在测试过程中，训练的模型可能会遭受分布漂移的影响，即它们对于预测未见过的语言任务或主题生成的文本是无效的。在这项工作中，我们旨在全面研究这些方法在由多种因素引起的分布漂移下的泛化行为，包括提示、文本长度、主题和语言任务。为实现这一目标，我们首先收集了一个包含人类和ChatGPT文本的新数据集，然后对收集的数据集进行了广泛的研究。我们的研究揭示了有启示性的发现，为未来方法或数据的开发提供了指导。

    ChatGPT is one of the most popular language models which achieve amazing performance on various natural language tasks. Consequently, there is also an urgent need to detect the texts generated ChatGPT from human written. One of the extensively studied methods trains classification models to distinguish both. However, existing studies also demonstrate that the trained models may suffer from distribution shifts (during test), i.e., they are ineffective to predict the generated texts from unseen language tasks or topics. In this work, we aim to have a comprehensive investigation on these methods' generalization behaviors under distribution shift caused by a wide range of factors, including prompts, text lengths, topics, and language tasks. To achieve this goal, we first collect a new dataset with human and ChatGPT texts, and then we conduct extensive studies on the collected dataset. Our studies unveil insightful findings which provide guidance for developing future methodologies or data 
    
[^55]: appjsonify：一种学术论文PDF到JSON转换工具包

    appjsonify: An Academic Paper PDF-to-JSON Conversion Toolkit. (arXiv:2310.01206v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.01206](http://arxiv.org/abs/2310.01206)

    appjsonify是一种学术论文PDF到JSON转换工具包，它使用多种模型和方法解析PDF文件，并且允许用户灵活配置处理流程。

    

    我们提出了appjsonify，一种基于Python的学术论文PDF到JSON转换工具包。它使用多种基于视觉的文档布局分析模型和基于规则的文本处理方法来解析PDF文件。appjsonify是一个灵活的工具，允许用户轻松配置处理流程，以处理特定格式的论文。我们公开发布appjsonify作为一个易于安装的工具包，可以通过PyPI和GitHub获得。

    We present appjsonify, a Python-based PDF-to-JSON conversion toolkit for academic papers. It parses a PDF file using several visual-based document layout analysis models and rule-based text processing approaches. appjsonify is a flexible tool that allows users to easily configure the processing pipeline to handle a specific format of a paper they wish to process. We are publicly releasing appjsonify as an easy-to-install toolkit available via PyPI and GitHub.
    
[^56]: TRAM：用于大型语言模型的时间推理基准评估

    TRAM: Benchmarking Temporal Reasoning for Large Language Models. (arXiv:2310.00835v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00835](http://arxiv.org/abs/2310.00835)

    TRAM是一个用于评估大型语言模型的时间推理能力的基准评估。我们介绍了由十个数据集组成的TRAM基准评估，涵盖了事件的各种时间方面。尽管使用流行的语言模型进行广泛评估，结果表明这些模型在时间推理任务上仍然落后于人类。

    

    时间推理对于理解自然语言中描述的事件的细微差别至关重要。以往对于这个主题的研究范围有限，缺乏标准化的基准评估，这导致不同研究间的评估结果不一致。本文介绍了一个称为TRAM的时间推理基准评估，由十个数据集组成，涵盖了事件的各种时间方面，如顺序、算术、频率和持续时间，旨在促进对大型语言模型（LLM）的时间推理能力的全面评估。我们在零样本学习和少样本学习场景中使用了流行的LLM，如GPT-4和Llama2进行了广泛评估。此外，我们还使用基于BERT的模型进行了基准评估。我们的研究结果表明，这些模型在时间推理任务上仍然落后于人类的表现。我们希望TRAM能够推动进一步提升时间推理的研究进展。

    Reasoning about time is essential for understanding the nuances of events described in natural language. Previous research on this topic has been limited in scope, characterized by a lack of standardized benchmarks that would allow for consistent evaluations across different studies. In this paper, we introduce TRAM, a temporal reasoning benchmark composed of ten datasets, encompassing various temporal aspects of events such as order, arithmetic, frequency, and duration, designed to facilitate a comprehensive evaluation of the temporal reasoning capabilities of large language models (LLMs). We conduct an extensive evaluation using popular LLMs, such as GPT-4 and Llama2, in both zero-shot and few-shot learning scenarios. Additionally, we employ BERT-based models to establish the baseline evaluations. Our findings indicate that these models still trail human performance in temporal reasoning tasks. It is our aspiration that TRAM will spur further progress in enhancing the temporal reason
    
[^57]: 提升低资源口语语种识别泛化性能的小波散射变换

    Wavelet Scattering Transform for Improving Generalization in Low-Resourced Spoken Language Identification. (arXiv:2310.00602v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2310.00602](http://arxiv.org/abs/2310.00602)

    该论文研究了在低资源情况下提升口语语种识别系统泛化能力的方法，提出了一种新的特征表示方法——小波散射变换（WST），并通过优化超参数和开发融合系统，成功降低了错误识别率。

    

    口语语种识别（LID）中常用的特征，如mel-spectrogram或MFCC，由于窗口化导致了高频信息的丢失。对于较长的时间上下文，信息丢失进一步增加。为了提升低资源LID系统的泛化能力，我们研究了替代特征表示方法，即小波散射变换（WST），以弥补上述缺陷。据我们所知，在LID任务中尚未有关于WST的研究。我们首先针对多个南亚LID语料库对WST特征进行优化。我们发现LID需要低八度分辨率，而频率散射并不有用。此外，跨语料库评估表明，最佳的WST超参数取决于训练和测试语料库。因此，我们基于不同的WST超参数开发了融合的基于ECAPA-TDNN的LID系统，以提高对未知数据的泛化能力。与MFCC相比，EER在同一语料库和盲目的VoxLingua107评估中分别降低了14.05%和6.40%。

    Commonly used features in spoken language identification (LID), such as mel-spectrogram or MFCC, lose high-frequency information due to windowing. The loss further increases for longer temporal contexts. To improve generalization of the low-resourced LID systems, we investigate an alternate feature representation, wavelet scattering transform (WST), that compensates for the shortcomings. To our knowledge, WST is not explored earlier in LID tasks. We first optimize WST features for multiple South Asian LID corpora. We show that LID requires low octave resolution and frequency-scattering is not useful. Further, cross-corpora evaluations show that the optimal WST hyper-parameters depend on both train and test corpora. Hence, we develop fused ECAPA-TDNN based LID systems with different sets of WST hyper-parameters to improve generalization for unknown data. Compared to MFCC, EER is reduced upto 14.05% and 6.40% for same-corpora and blind VoxLingua107 evaluations, respectively.
    
[^58]: JoMA: 通过MLP和注意力的联合动力学来解密多层Transformer

    JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention. (arXiv:2310.00535v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00535](http://arxiv.org/abs/2310.00535)

    本文提出了联合MLP/注意力（JoMA）动态，用于解析多层Transformer架构的训练过程。通过预测非线性激活情况下注意力的行为，我们解释了多层Transformer中标记的层次组合方法。实验证实了我们的理论发现。

    

    我们提出了联合MLP/注意力（JoMA）动态，这是一种新颖的数学框架，用于理解多层Transformer架构的训练过程。通过在Transformer中去除自注意力层，我们得到仅包含MLP层的修改后动态。JoMA消除了先前分析中的不切实际的假设（例如缺乏残差连接），并预测注意力在非线性激活的情况下首先变得稀疏（为了学习重要的标记），然后变得密集（为了学习不那么重要的标记），而在线性情况下，它与现有研究一致，显示出注意力随时间变得稀疏。我们利用JoMA定性地解释了多层Transformer中如何将标记组合成层次结构，当输入标记是由潜在的层次生成模型生成时。在从现实世界数据集（Wikitext2/Wikitext103）训练的模型和各种预训练模型（OPT，Pythia）上进行的实验证实了我们的理论发现。

    We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings
    
[^59]: 直觉还是依赖？研究LLMs对冲突提示的鲁棒性

    Intuitive or Dependent? Investigating LLMs' Robustness to Conflicting Prompts. (arXiv:2309.17415v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.17415](http://arxiv.org/abs/2309.17415)

    本文研究了LLMs对冲突提示的鲁棒性，发现这些模型对于误导性提示特别容易受到影响，尤其是在指导常识知识方面。

    

    本文探讨了LLMs对其内部记忆或给定提示的偏好的鲁棒性，由于噪声或任务设置，在真实应用中可能存在对立信息。为此，我们建立了一个定量的基准框架，并进行角色扮演干预来控制LLMs的偏好。具体而言，我们定义了两种鲁棒性，即事实鲁棒性和决策风格，事实鲁棒性是指从提示或记忆中识别正确事实的能力，而决策风格是基于认知理论对LLMs在进行一致选择过程中行为的分类 - 直觉型、依赖型或理性型，这里假设没有明确的“正确”答案。我们对七个开源和闭源LLMs进行了大量实验，发现这些模型对于误导性提示特别容易受到影响，尤其是在指导常识知识方面。尽管详细的说明可以减轻选择误导性答案的情况，但也会增加出现不明确答案的情况。

    This paper explores the robustness of LLMs' preference to their internal memory or the given prompt, which may contain contrasting information in real-world applications due to noise or task settings. To this end, we establish a quantitative benchmarking framework and conduct the role playing intervention to control LLMs' preference. In specific, we define two types of robustness, factual robustness targeting the ability to identify the correct fact from prompts or memory, and decision style to categorize LLMs' behavior in making consistent choices -- assuming there is no definitive "right" answer -intuitive, dependent, or rational based on cognitive theory. Our findings, derived from extensive experiments on seven open-source and closed-source LLMs, reveal that these models are highly susceptible to misleading prompts, especially for instructing commonsense knowledge. While detailed instructions can mitigate the selection of misleading answers, they also increase the incidence of in
    
[^60]: 多种进化压力塑造了世界语言中相同辅音的避讳现象

    Multiple evolutionary pressures shape identical consonant avoidance in the world's languages. (arXiv:2309.14006v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.14006](http://arxiv.org/abs/2309.14006)

    多种进化压力共同作用，塑造了世界语言中相同辅音的避忌现象，影响着单词的产生、形变和消失。

    

    由于生物力学和认知上的困难，语言不倾向于包含相似或相同辅音的单词形式。然而，负责这种现象的特定进化过程尚未完全理解。包含相同辅音序列的单词可能比没有这种序列的单词更容易产生；单词形式突变的过程可能更有可能移除而非引入相同辅音序列；最后，含有相同辅音的单词可能比没有的单词更容易消失。通过对同源词形的进化进行系统发育分析，结果显示含有相同辅音的单词比没有的单词更少出现，而导致单词形式发生突变的过程更有可能删除相同辅音序列而非引入它们。然而，含有相同辅音的单词并没有比没有的单词更容易消失。进一步分析发现，带有相同辅音的形态不同的单词比相同的单词更受到负面影响。

    Languages disfavor word forms containing sequences of similar or identical consonants, due to the biomechanical and cognitive difficulties posed by patterns of this sort. However, the specific evolutionary processes responsible for this phenomenon are not fully understood. Words containing sequences of identical consonants may be more likely to arise than those without; processes of word form mutation may be more likely to remove than create sequences of identical consonants in word forms; finally, words containing identical consonants may die out more frequently than those without. Phylogenetic analyses of the evolution of homologous word forms indicate that words with identical consonants arise less frequently than those without, and processes which mutate word forms are more likely to remove sequences of identical consonants than introduce them. However, words with identical consonants do not die out more frequently than those without. Further analyses reveal that forms with identic
    
[^61]: MAmmoTH: 通过混合指令调整构建数学通用模型

    MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning. (arXiv:2309.05653v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05653](http://arxiv.org/abs/2309.05653)

    MAmmoTH是一系列用于解决通用数学问题的开源大型语言模型，通过混合指令调整网络架构，成功地融合了链状思维和程序维思维，从而在多个数学推理数据集上实现了显著提升的准确率。其中，MAmmoTH-7B模型在MATH数据集上的表现超过了目前最好的开源7B模型WizardMath，并且整个系列模型在各个规模上平均准确率提高了16%到32%之间。

    

    我们介绍了MAmmoTH，一系列针对通用数学问题求解的开源大型语言模型（LLM）。MAmmoTH模型是在我们精心策划的指令调整数据集MathInstruct上训练的。MathInstruct从13个数学数据集中编译而成，包含中间的推理过程，其中有六个数据集是由我们新鲜策划的推理过程。它提供了一种独特的链状思维（CoT）和程序维思维（PoT）推理的混合，同时确保了对数学领域各个方面的广泛覆盖。CoT和PoT的混合不仅释放了工具使用的潜力，而且允许在不同数学问题上使用不同的思考过程。因此，MAmmoTH系列在九个数学推理数据集上显著优于现有的开源模型，在所有规模上平均准确率提高了16%到32%不等。值得注意的是，我们的MAmmoTH-7B模型在MATH（一个竞赛级数据集）上达到了33%，超过了最好的开源7B模型（WizardMath）2个百分点。

    We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 2
    
[^62]: LM-Infinite: 大规模语言模型的简单即时长度推广

    LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v1 [cs.CL])

    [http://arxiv.org/abs/2308.16137](http://arxiv.org/abs/2308.16137)

    LM-Infinite研究了大规模语言模型在长序列上的长度推广失败问题，并提出了一种简单的即时推广方法，以更高效地利用现有模型的生成能力。

    

    近年来，在Transformer-based大规模语言模型（LLM）在各个领域取得了显著的进展。随着这些LLM在越来越复杂的任务上的部署，它们往往面临着对长时间推理过程或理解更大上下文的需求。在这些情况下，LLM在长序列上的长度推广失败变得更加突出。大多数预训练方案将训练序列截断到固定长度（例如LLaMa的2048）。即使使用了相对位置编码来应对这个问题，LLM在更长的上下文之后往往难以生成流畅的文本，更不用说进行下游任务了。常见的解决方案，如在更长的语料库上进行微调，往往需要耗费大量的硬件和时间成本，并需要进行仔细的训练过程设计。为了更高效地利用现有LLM的生成能力，我们在理论和实证上研究了主要的分布外(OOD) f

    In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex tasks, they often face the needs to conduct longer reasoning processes or understanding larger contexts. In these situations, the length generalization failure of LLMs on long sequences become more prominent. Most pre-training schemes truncate training sequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle to generate fluent texts, let alone carry out downstream tasks, after longer contexts, even with relative positional encoding which is designed to cope with this problem. Common solutions such as finetuning on longer corpora often involves daunting hardware and time costs and requires careful training process design. To more efficiently leverage the generation capacity of existing LLMs, we theoretically and empirically investigate the main out-of-distribution (OOD) f
    
[^63]: 使用大型语言模型进行数据竞争检测

    Data Race Detection Using Large Language Models. (arXiv:2308.07505v1 [cs.LG])

    [http://arxiv.org/abs/2308.07505](http://arxiv.org/abs/2308.07505)

    本研究探索了一种基于大型语言模型的数据竞争检测方法，通过结合提示工程和微调技术，发现LLMs可以作为数据竞争检测的可行方法，但在需要详细信息时仍不如传统工具竞争。

    

    大型语言模型（LLMs）作为一种替代策略，展示了在分析和优化高性能计算程序方面的显著优势，避免了资源密集型手动工具的创建。本文中，我们探讨了一种新颖的基于LLM的数据竞争检测方法，结合了提示工程和微调技术。我们创建了一个名为DRB-ML的专用数据集，该数据集源自DataRaceBench，并具有精细的标签，显示了数据竞争对及其相关变量、行号和读/写信息的存在。然后，我们使用DRB-ML评估了代表性的LLMs并微调了开源模型。我们的实验证明，LLMs可以作为数据竞争检测的可行方法。然而，当我们需要有关引起数据竞争的变量对的详细信息时，它们仍无法与传统的数据竞争检测工具竞争。

    Large language models (LLMs) are demonstrating significant promise as an alternate strategy to facilitate analyses and optimizations of high-performance computing programs, circumventing the need for resource-intensive manual tool creation. In this paper, we explore a novel LLM-based data race detection approach combining prompting engineering and fine-tuning techniques. We create a dedicated dataset named DRB-ML, which is derived from DataRaceBench, with fine-grain labels showing the presence of data race pairs and their associated variables, line numbers, and read/write information. DRB-ML is then used to evaluate representative LLMs and fine-tune open-source ones. Our experiment shows that LLMs can be a viable approach to data race detection. However, they still cannot compete with traditional data race detection tools when we need detailed information about variable pairs causing data races.
    
[^64]: ToolLLM: 促进大型语言模型掌握16000多个真实世界API

    ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. (arXiv:2307.16789v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2307.16789](http://arxiv.org/abs/2307.16789)

    ToolLLM是一个通用的工具使用框架，旨在促进大型语言模型掌握16000多个真实世界API。该框架包括数据构建、模型训练和评估。使用ChatGPT自动构建的ToolBench数据集用于指令调整，涵盖了各种工具使用情境和API。

    

    尽管开源的大型语言模型（LLM）如LLaMA的进展，但它们在工具使用能力方面仍然受到严重限制，即使用外部工具（API）来满足人类指令。原因是当前的指令调整主要集中在基本语言任务上，但忽略了工具使用领域。这与最先进的闭源LLM（如ChatGPT）的出色工具使用能力形成对比。为了弥补这一差距，我们介绍了ToolLLM，一个包括数据构建、模型训练和评估的通用工具使用框架。我们首先介绍了ToolBench，一个用于工具使用的指令调整数据集，该数据集是使用ChatGPT自动构建的。具体而言，构建可以分为三个阶段：（i）API收集：我们从RapidAPI Hub收集了16464个真实世界的RESTful API，涵盖了49个类别；（ii）指令生成：我们提示ChatGPT生成涉及这些API的多样化指令，涵盖了单工具和多工具使用场景。

    Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-too
    
[^65]: 一种具有规划、长期上下文理解和程序合成能力的现实世界WebAgent

    A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. (arXiv:2307.12856v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.12856](http://arxiv.org/abs/2307.12856)

    这篇论文介绍了一种名为WebAgent的LLM驱动代理，通过自我经验学习，在真实网站上完成任务。该方法通过规划、总结和生成代码来提高在真实网站上的成功率。

    

    最近，预训练的大型语言模型（LLMs）在自主Web自动化方面取得了更好的泛化性能和样本效率。然而，在真实世界的网站上，性能仍然受到三个方面的限制：开放领域性、有限的上下文长度和对HTML的归纳偏差的缺乏。我们介绍了一种名为WebAgent的LLM驱动代理，它通过自我经验学习，在遵循自然语言指令的前提下，在真实网站上完成任务。WebAgent通过将指令分解为规范的子指令，将长HTML文档总结为与任务相关的片段，并通过从中生成的Python程序对网站进行操作来提前进行规划。我们使用Flan-U-PaLM设计了WebAgent，用于生成有根代码，并使用HTML-T5进行预训练LLMs，利用局部和全局注意机制以及混合长跨度去噪目标来进行规划和总结。我们通过实验证明，我们的模块化方法提高了在真实网站上的成功率。

    Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by ov
    
[^66]: 大型语言模型中的上下文学习在学习标签关系上具有创新，但并非传统学习方法

    In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning. (arXiv:2307.12375v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.12375](http://arxiv.org/abs/2307.12375)

    大型语言模型（LLMs）在包含标签关系示例的上下文中的学习能力使其在下游任务中表现显著提高，但与传统学习方法不同。我们研究了上下文示例中的标签如何影响预测、预训练中学习到的标签关系如何与上下文示例相互作用以及上下文学习如何聚合标签信息。研究结果揭示了LLMs的工作机制及其对上下文信息的处理方式。

    

    在下游任务中，大型语言模型（LLMs）的性能在包含输入-标签关系示例的上下文中通常显著提高。然而，目前对LLMs的这种上下文学习（ICL）能力的工作机制尚无共识：例如，虽然Xie等人（2021年）将ICL比作一种通用学习算法，但Min等人（2022b年）认为ICL甚至不能从上下文示例中学习标签关系。在本文中，我们研究了以下三个问题：（1）上下文示例的标签如何影响预测结果，（2）预训练期间学习到的标签关系如何与上下文中提供的输入-标签示例相互作用，以及（3）ICL如何聚合来自上下文示例的标签信息。我们的研究发现，LLMs通常会整合上下文标签的信息，但预训练和上下文标签关系被区别对待，模型不会将所有上下文信息等同对待。我们的结果揭示了对LLMs的理解。

    The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into underst
    
[^67]: 图像和声音的滥用用于在多模态LLMs中进行间接指令注入

    (Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])

    [http://arxiv.org/abs/2307.10490](http://arxiv.org/abs/2307.10490)

    本论文展示了如何利用图像和声音在多模态LLMs中进行间接指令注入，攻击者通过生成对抗扰动并将其融入图像或音频录音中，以操纵模型输出特定文本和指导对话的行为。

    

    我们展示了如何利用图像和声音在多模态LLMs中进行间接提示和指令注入。攻击者生成与提示相对应的对抗扰动，并将其融入图像或音频录音中。当用户向（未修改的良性）模型询问被扰动的图像或音频时，扰动会引导模型输出攻击者选择的文本和/或使后续对话遵循攻击者的指令。我们用几个概念验证示例针对LLaVa和PandaGPT来说明这种攻击。

    We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
    
[^68]: 在大型语言模型中的上下文压缩的上下文自编码器

    In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])

    [http://arxiv.org/abs/2307.06945](http://arxiv.org/abs/2307.06945)

    在大型语言模型中，我们提出了一种称为In-context Autoencoder (ICAE)的上下文自编码器，它通过将长上下文压缩为有限数量的内存槽，实现了$4\times$的上下文压缩，并能够根据内存槽进行条件处理以响应各种提示。

    

    我们提出了一种用于大型语言模型中上下文压缩的上下文自编码器（ICAE）。 ICAE有两个模块：一个可学习的编码器，通过从LLM中采用LoRA方式将长上下文压缩为有限数量的内存槽，以及一个固定的解码器，作为目标LLM，可以根据内存槽来进行各种目的的条件处理。我们首先使用自编码和语言建模目标在大规模文本数据上预训练ICAE，使其能够生成准确和全面表示原始上下文的内存槽。然后，我们使用少量指导数据对预训练的ICAE进行微调，以增强其与各种提示的交互，从而产生理想的响应。我们的实验结果表明，使用我们提出的预训练和微调范式学习的ICAE可以有效地产生$4\times$上下文压缩的内存槽，目标LLM可以很好地对其进行条件处理，以响应各种提示。

    We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promis
    
[^69]: RecallM:一种用于时间上下文理解和问题回答的架构

    RecallM: An Architecture for Temporal Context Understanding and Question Answering. (arXiv:2307.02738v1 [cs.AI])

    [http://arxiv.org/abs/2307.02738](http://arxiv.org/abs/2307.02738)

    本文介绍了一种名为RecallM的架构，用于创建可适应和可更新的长期记忆，以提升大型语言模型聊天机器人的时间理解能力。

    

    用于大型语言模型（LLM）聊天机器人的理想长期记忆机制将为连续学习、复杂推理和学习序列和时间依赖关系打下基础。创建这种类型的记忆机制是一个极具挑战性的问题。在本文中，我们探索了不同方法实现长期记忆的效果。我们提出了一种新的架构，专注于为AGI系统创建可适应和可更新的长期记忆。我们通过各种实验展示了RecallM架构的好处，特别是它提供的改进的时间理解能力。

    The ideal long-term memory mechanism for Large Language Model (LLM) based chatbots, would lay the foundation for continual learning, complex reasoning and allow sequential and temporal dependencies to be learnt. Creating this type of memory mechanism is an extremely challenging problem. In this paper we explore different methods of achieving the effect of long-term memory. We propose a new architecture focused on creating adaptable and updatable long-term memory for AGI systems. We demonstrate through various experiments the benefits of the RecallM architecture, particularly the improved temporal understanding it provides.
    
[^70]: GKD：自回归序列模型的广义知识蒸馏

    GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models. (arXiv:2306.13649v1 [cs.LG])

    [http://arxiv.org/abs/2306.13649](http://arxiv.org/abs/2306.13649)

    本文提出了广义知识蒸馏（GKD），通过从学生中采样输出序列来缓解分布不匹配，并在优化替代KL等离散度方面处理模型欠规范，达到了在摘要任务上最先进的性能。

    

    知识蒸馏通常用于压缩神经网络，以减少推理成本和内存占用。然而，当前针对自回归模型（如生成语言模型）的蒸馏方法存在两个关键问题：（1）训练期间输出序列和部署时由学生模型生成的序列之间分布不匹配，（2）模型欠规范，学生模型可能不够表达老师分布。为了解决这些问题，我们提出了广义知识蒸馏（GKD）。GKD通过在训练期间从学生中采样输出序列来缓解分布不匹配。此外，GKD通过优化替代KL等离散度来处理模型欠规范，这些离散度集中于生成可能符合老师分布的学生样本。我们证明，在摘要任务上，GKD优于常用的LLM蒸馏方法，在几个基准数据集上实现了最先进的性能。

    Knowledge distillation is commonly used for compressing neural networks to reduce their inference cost and memory footprint. However, current distillation methods for auto-regressive models, such as generative language models (LMs), suffer from two key issues: (1) distribution mismatch between output sequences during training and the sequences generated by the student during its deployment, and (2) model under-specification, where the student model may not be expressive enough to fit the teacher's distribution. To address these issues, we propose Generalized Knowledge Distillation (GKD). GKD mitigates distribution mismatch by sampling output sequences from the student during training. Furthermore, GKD handles model under-specification by optimizing alternative divergences, such as reverse KL, that focus on generating samples from the student that are likely under the teacher's distribution. We demonstrate that GKD outperforms commonly-used approaches for distilling LLMs on summarizatio
    
[^71]: LLMatic: 基于大语言模型和多样性优化的神经结构搜索

    LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])

    [http://arxiv.org/abs/2306.01102](http://arxiv.org/abs/2306.01102)

    本文介绍了利用大语言模型和多样性优化算法相结合的 LLMatic 神经结构搜索算法。该算法在CIFAR-10数据集进行测试，仅进行2000次搜索即可产生高性能网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。

    

    大型语言模型 (LLMs) 已成为一种强大的工具，可以完成广泛的任务。它们的能力涵盖了许多领域，它们在代码生成领域产生了重大影响。在此情况下，我们将 LLMs 视为变异和交叉工具。同时，多样性优化算法已知可以发现多样性和稳健的解决方案。通过将 LLMs 的代码生成能力与 QD 解决方案的多样性和鲁棒性相结合，我们引入了 LLMatic，一个神经结构搜索 (NAS) 算法。虽然 LLMs 通过提示直接进行 NAS 考验困难，但 LLMatic 利用程序化方法，利用 QD 来进行提示和网络结构，从而创建多样性和高性能网络。我们在 CIFAR-10 图像分类基准测试中测试了 LLMatic，证明它可以在仅进行 2000 次搜索的情况下产生具有竞争力的网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。

    Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
    
[^72]: 大型语言模型在知识冲突中的行为揭秘：自适应变色龙还是固执的树獭

    Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes. (arXiv:2305.13300v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13300](http://arxiv.org/abs/2305.13300)

    本文研究了大型语言模型（LLMs）在遭遇知识冲突时的行为。结果发现，LLMs可以高度接受外部连贯且有说服力的证据，即使与其参数化内存存在冲突，但也可能有局限性。

    

    通过向大型语言模型（LLMs）提供外部信息，工具增强（包括检索增强）已成为解决LLMs静态参数化内存限制的有希望的解决方案。然而，当这些证据与它们的参数化内存发生冲突时，LLMs对这些外部证据有多少接受能力？我们提出了一个系统性的框架来从LLMs中获取高质量的参数化内存，并构建相应的对立内存，从而使我们能够进行一系列受控实验。我们的调查揭示了LLMs表现出看似矛盾的行为。一方面，与以往的观念不同，我们发现，只要外部证据是连贯且有说服力的，LLMs即使与其参数化内存存在冲突也可以高度接受外部证据。另一方面，LLMs也可能会表现出局限性，尤其是当其参数化内存受到威胁时。

    By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory. However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory? We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts. We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. Our investigation reveals seemingly contradicting behaviors of LLMs. On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. On the other hand, LLMs also d
    
[^73]: Chain-of-Knowledge:通过多源动态知识适应为大型语言模型提供准确的基础信息

    Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources. (arXiv:2305.13269v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13269](http://arxiv.org/abs/2305.13269)

    Chain-of-Knowledge通过整合多源动态知识为大型语言模型提供准确的基础信息，减少生成的幻觉，可以产生更可靠的答案。

    

    我们提出了一种新颖的框架，链式知识（CoK），通过动态地整合来自不同来源的基础信息来增强大型语言模型(LLMs)。它可以产生更多的事实依据，减少生成的幻觉。具体而言，CoK包括三个阶段：推理准备、动态知识适应和答案整合。给定一个知识密集型问题，CoK首先准备若干个初步的依据和答案，同时识别出相关的知识领域。如果样本中的答案没有多数共识，CoK通过从识别出的领域中逐步适应知识来纠正依据。这些纠正后的依据可以更好地作为最终答案整合的基础。不同于之前主要使用非结构化数据的研究，CoK还利用结构化的知识源，如Wikidata和表格，提供更可靠的事实信息。

    We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructure
    
[^74]: 主观知识驱动的任务导向式对话建模

    "What do others think?": Task-Oriented Conversational Modeling with Subjective Knowledge. (arXiv:2305.12091v1 [cs.CL])

    [http://arxiv.org/abs/2305.12091](http://arxiv.org/abs/2305.12091)

    该研究提出了一种主观知识驱动的任务导向式对话建模方法，并制作了相应数据集。该方法面临着新的挑战，例如如何汇总多个知识片段中的不同意见。

    

    任务导向式对话系统的目标是建立能够帮助用户完成特定目标的对话系统，例如预定酒店或餐厅。传统的任务导向式对话系统依赖于特定领域的API/数据库或外部事实知识来生成响应，无法满足主观用户请求（例如“Wi-Fi可靠吗？”或“餐厅环境好吗？”）。为了解决这个问题，我们提出了一个新颖的主观知识驱动的任务导向式对话建模（SK-TOD）任务。我们还提出了第一个相应的数据集，其中包含主观知识寻求对话上下文和手动注释的基于主观知识来源的响应。在与现有的任务导向式对话方法进行评估时，我们发现这个任务带来了新的挑战，如如何汇总多个知识片段中的不同意见。我们希望这个任务和数据集能促进进一步的任务导向式对话和主观内容理解的研究。代码和数据集可以在 https://github.com/alex 上找到。

    Task-oriented Dialogue (TOD) Systems aim to build dialogue systems that assist users in accomplishing specific goals, such as booking a hotel or a restaurant. Traditional TODs rely on domain-specific APIs/DBs or external factual knowledge to generate responses, which cannot accommodate subjective user requests (e.g., "Is the WIFI reliable?" or "Does the restaurant have a good atmosphere?"). To address this issue, we propose a novel task of subjective-knowledge-based TOD (SK-TOD). We also propose the first corresponding dataset, which contains subjective knowledge-seeking dialogue contexts and manually annotated responses grounded in subjective knowledge sources. When evaluated with existing TOD approaches, we find that this task poses new challenges such as aggregating diverse opinions from multiple knowledge snippets. We hope this task and dataset can promote further research on TOD and subjective content understanding. The code and the dataset are available at https://github.com/alex
    
[^75]: MildTriple Loss模型下的运动和文本跨模态检索

    Cross-Modal Retrieval for Motion and Text via MildTriple Loss. (arXiv:2305.04195v1 [cs.CV])

    [http://arxiv.org/abs/2305.04195](http://arxiv.org/abs/2305.04195)

    本论文提出了一个创新模型，使用MildTriple Loss捕捉长期依赖并模拟跨模态人类动作序列与文本检索任务，具有重要的应用价值。

    

    跨模态检索已成为计算机视觉和自然语言处理中的重要研究课题，随着图像文本和视频文本检索技术的进步。尽管在虚拟现实等广泛应用中具有重要价值，但人类动作序列与文本之间的跨模态检索尚未引起足够的关注。这个任务存在一些挑战，包括对两种语言的共同建模，要求从文本中理解以人为中心的信息，并从三维人体运动序列中学习行为特征。以往的运动数据建模主要依赖于自回归特征提取器，这可能会遗忘以前的信息，而我们提出了一种创新模型，其中包括简单而强大的基于变换器的运动和文本编码器，可以从两种不同的模态中学习表示并捕捉长期依赖

    Cross-modal retrieval has become a prominent research topic in computer vision and natural language processing with advances made in image-text and video-text retrieval technologies. However, cross-modal retrieval between human motion sequences and text has not garnered sufficient attention despite the extensive application value it holds, such as aiding virtual reality applications in better understanding users' actions and language. This task presents several challenges, including joint modeling of the two modalities, demanding the understanding of person-centered information from text, and learning behavior features from 3D human motion sequences. Previous work on motion data modeling mainly relied on autoregressive feature extractors that may forget previous information, while we propose an innovative model that includes simple yet powerful transformer-based motion and text encoders, which can learn representations from the two different modalities and capture long-term dependencie
    
[^76]: 离散与反向传播的桥梁：直通法与其它方法

    Bridging Discrete and Backpropagation: Straight-Through and Beyond. (arXiv:2304.08612v1 [cs.LG])

    [http://arxiv.org/abs/2304.08612](http://arxiv.org/abs/2304.08612)

    本文提出了一种新方法来逼近生成离散潜变量的参数的梯度，其中包括了一些数值方法，实现了二阶精度，取得了实验上的持续改进。

    

    反向传播是深度学习中的基石，但其仅限于计算连续变量的梯度，限制了涉及离散潜变量的问题的研究。针对这个问题，我们提出了一种新的方法来近似生成离散潜变量的参数的梯度。我们首先考察了广泛使用的 Straight-Through（ST）启发式方法，并证明它作为梯度的一阶近似值。在此基础上，我们提出了一种新的方法，称为 ReinMax，它集成了 Heun's Method，一种解ODE的二阶数值方法，以近似梯度。我们的方法实现了二阶精度，而不需要 Hessian 或其他二阶导数。我们进行了结构化输出预测和无监督生成建模任务的实验。我们的结果显示，\ours 在现有技术中带来了持续的改进，包括 ST 和 Straight-Through Gum。

    Backpropagation, the cornerstone of deep learning, is limited to computing gradients solely for continuous variables. This limitation hinders various research on problems involving discrete latent variables. To address this issue, we propose a novel approach for approximating the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose a novel method called ReinMax, which integrates Heun's Method, a second-order numerical method for solving ODEs, to approximate the gradient. Our method achieves second-order accuracy without requiring Hessian or other second-order derivatives. We conduct experiments on structured output prediction and unsupervised generative modeling tasks. Our results show that \ours brings consistent improvements over the state of the art, including ST and Straight-Through Gum
    
[^77]: 关于AI生成文本检测的可能性的探讨

    On the Possibilities of AI-Generated Text Detection. (arXiv:2304.04736v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.04736](http://arxiv.org/abs/2304.04736)

    该研究探讨了AI生成文本检测的可能性，提出了精确的样本复杂度界限，并指出了设计更准确的检测方法和提高透明度的挑战。

    

    我们的工作着眼于检测由大型语言模型(LLM)生成的输出，以区分其与人类生成的输出。这项能力在许多应用中非常重要。然而，关于这种区分的可能性一直是该领域内的争议话题。因此，一个核心问题是我们是否能够检测到AI生成的文本，如果能，何时能检测到。在这项工作中，我们提供了证据表明，除非人类和机器生成的文本分布在整个支持中完全相同，否则几乎总是可以检测到AI生成的文本。这个观察结果来自于信息论中的标准结果，并依赖于机器生成的文本越像人类，我们就需要更多的样本来检测它。我们得出了AI生成文本检测的精确样本复杂度界限，告诉需要多少个样本才能检测到AI生成的文本。这引起了更多设计更准确的AI生成文本检测方法和提高LLM透明度的挑战。

    Our work focuses on the challenge of detecting outputs generated by Large Language Models (LLMs) to distinguish them from those generated by humans. This ability is of the utmost importance in numerous applications. However, the possibility of such discernment has been the subject of debate within the community. Therefore, a central question is whether we can detect AI-generated text and, if so, when. In this work, we provide evidence that it should almost always be possible to detect AI-generated text unless the distributions of human and machine-generated texts are exactly the same over the entire support. This observation follows from the standard results in information theory and relies on the fact that if the machine text becomes more human-like, we need more samples to detect it. We derive a precise sample complexity bound of AI-generated text detection, which tells how many samples are needed to detect AI-generated text. This gives rise to additional challenges of designing more
    
[^78]: 回顾链将语言模型与反馈对齐

    Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02676](http://arxiv.org/abs/2302.02676)

    该研究提出了一种新颖的技术，即回顾链，可以轻松优化，并可以从任何形式的反馈中学习，而不受其极性的影响。

    

    从人类偏好中学习对于语言模型具有重要意义，这样才能对人类有所帮助并符合人类和社会价值观。先前的研究通过从人类反馈中学习来理解和遵循指令取得了显著成功。然而，这些方法要么是基于被人类注释者喜欢的手动挑选的模型生成，使得它们在数据利用方面效果不佳且普遍应用具有挑战性，要么依赖于奖励函数和强化学习，这容易出现奖励函数不完美和极难优化的问题。在本文中，我们提出了一种新颖的技术，“回顾链”，它易于优化，并可以从任何形式的反馈中学习，而不受其极性的影响。我们的想法受到了人类如何从以语言形式呈现的广泛反馈中学习的启发。我们将所有类型的反馈转换成句子，然后用它们来微调模型，从而利用这种方法。

    Learning from human preferences is important for language models to be helpful and useful for humans, and to align with human and social values. Prior work have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them ineffective in terms of data utilization and challenging to apply in general, or they depend on reward functions and reinforcement learning, which are prone to imperfect reward function and extremely challenging to optimize. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sentences, which are then used to fine-tune the model, allowing us to take advantage
    
[^79]: 探索和利用辅助数据来改善小样本泛化问题

    Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data. (arXiv:2302.00674v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00674](http://arxiv.org/abs/2302.00674)

    本文提出了一种在少样本学习过程中假定有辅助数据的训练范式FLAD，并针对自动混合辅助和目标数据的方法局限，提出了两种计算复杂度独立于辅助数据集数量的算法，通过FLAD和这两种算法的比较，可以发现这两种算法的表现更好。

    

    小样本学习在许多实际应用中都有价值，但学习一个通用的模型且不过度拟合少数标记数据点是具有挑战性的。本文关注辅助数据的小样本学习（FLAD），一种在少样本学习过程中假定有辅助数据的训练范式，以期提高泛化性能。先前的工作已经提出了自动混合辅助和目标数据的方法，但这些方法通常随辅助数据集的数量呈线性（或更差）缩放，从而限制了它们的实用性。在本文中，我们将FLAD与在多臂老虎机设置中至关重要的探索与利用困境联系起来，并推导出算法，其计算复杂度独立于辅助数据集的数量，从而使我们能够扩展到比先前方法多100倍的辅助数据集。我们提出了两种算法——EXP3-FLAD和UCB1-FLAD，并将它们与先前只进行探索或利用的FLAD方法进行了比较，发现这些算法表现更好。

    Few-shot learning is valuable in many real-world applications, but learning a generalizable model without overfitting to the few labeled datapoints is challenging. In this work, we focus on Few-shot Learning with Auxiliary Data (FLAD), a training paradigm that assumes access to auxiliary data during few-shot learning in hopes of improving generalization. Previous works have proposed automated methods for mixing auxiliary and target data, but these methods typically scale linearly (or worse) with the number of auxiliary datasets, limiting their practicality. In this work we relate FLAD to the explore-exploit dilemma that is central to the multi-armed bandit setting and derive algorithms whose computational complexity is independent of the number of auxiliary datasets, allowing us to scale to 100x more auxiliary datasets than prior methods. We propose two algorithms -- EXP3-FLAD and UCB1-FLAD -- and compare them with prior FLAD methods that either explore or exploit, finding that the com
    
[^80]: 用于有效发现错误信息的查询重写

    Query Rewriting for Effective Misinformation Discovery. (arXiv:2210.07467v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.07467](http://arxiv.org/abs/2210.07467)

    我们提出了一个新的查询重写系统，通过使用离线强化学习来自动学习编辑操作，以提高已知误传主张的查询效果，实验结果显示我们的系统可使查询有效性增加多达42%。

    

    我们提出了一个新颖的系统，帮助事实核查者制定已知错误信息主张的搜索查询，并在多个社交媒体平台上进行有效搜索。我们引入了一种适应性重写策略，通过离线强化学习自动学习包含主张的查询的编辑操作（例如，用同义词替换一个词；将动词时态改为一般现在时）。我们的模型使用决策转换器学习一系列编辑操作，以最大化查询检索指标，例如平均精确度。我们进行了一系列实验，表明我们的查询重写系统相对于查询的有效性增加了多达42%，同时产生的编辑操作序列是可人工解释的。

    We propose a novel system to help fact-checkers formulate search queries for known misinformation claims and effectively search across multiple social media platforms. We introduce an adaptable rewriting strategy, where editing actions for queries containing claims (e.g., swap a word with its synonym; change verb tense into present simple) are automatically learned through offline reinforcement learning. Our model uses a decision transformer to learn a sequence of editing actions that maximizes query retrieval metrics such as mean average precision. We conduct a series of experiments showing that our query rewriting system achieves a relative increase in the effectiveness of the queries of up to 42%, while producing editing action sequences that are human interpretable.
    
[^81]: DialoGen: 对话系统的广义长程上下文表示

    DialoGen: Generalized Long-Range Context Representation for Dialogue Systems. (arXiv:2210.06282v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.06282](http://arxiv.org/abs/2210.06282)

    DialoGen是一种对话系统，采用了广义上下文表示方法，在对话理解和生成任务中表现出色。

    

    长程上下文建模对于对话理解和生成都至关重要。对话上下文表示最常用的方法是将最后-k个先前的话语连接在一起。然而，对于包含长程依赖关系的对话而言，这种方法可能不是最理想的，因为它无法超越最后-k个话语。在这项工作中，我们提出了DialoGen，一种新颖的基于编码器-解码器的对话响应生成框架，其具有可以超越最后-k个话语的广义上下文表示。因此，该方法适应具有长程依赖的对话。我们的方法的主要思想是识别和利用最相关的历史话语，而不是按时间顺序的最后-k个话语。我们在对话生成（开放域）和理解（DST）任务上研究了我们提出的方法的有效性。DialoGen在DailyDialog数据集上实现了与最先进模型相当的性能。

    Long-range context modeling is crucial to both dialogue understanding and generation. The most popular method for dialogue context representation is to concatenate the last-$k$ previous utterances. However, this method may not be ideal for conversations containing long-range dependencies as it cannot look beyond last-$k$ utterances. In this work, we propose DialoGen, a novel encoder-decoder based framework for conversational response generation with a generalized context representation that can look beyond the last-$k$ utterances. Hence the method is adaptive to conversations with long-range dependencies. The main idea of our approach is to identify and utilize the most relevant historical utterances instead of the last-$k$ utterances in chronological order. We study the effectiveness of our proposed method on both dialogue generation (open-domain) and understanding (DST) tasks. DialoGen achieves comparable performance with the state-of-the-art models on DailyDialog dataset. We also ob
    
[^82]: FRMT: 一种用于少样本区域感知机器翻译的基准测试

    FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation. (arXiv:2210.00193v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.00193](http://arxiv.org/abs/2210.00193)

    FRMT是一个用于少样本区域感知机器翻译的基准测试，包括了专业翻译的数据集和自动评估指标，能够对词汇上不同的术语和干扰项进行详细分析。同时提供了基准模型和指导，供研究人员使用。

    

    我们提出了FRMT，这是一种新的数据集和评估基准，用于少样本区域感知机器翻译，这是一种针对风格的翻译。该数据集包含了从英语到葡萄牙语和中文普通话的两种地区变体的专业翻译。选择源文档，以便能够对感兴趣的现象进行详细分析，包括词汇上不同的术语和干扰项。我们探索了针对FRMT的自动评估指标，并验证了在地区匹配和不匹配评分场景下与专家人工评估的相关性。最后，我们提供了一些针对这个任务的基准模型，并提供了研究人员如何训练、评估和比较他们自己模型的指导。我们的数据集和评估代码公开可用：https://bit.ly/frmt-task

    We present FRMT, a new dataset and evaluation benchmark for Few-shot Region-aware Machine Translation, a type of style-targeted translation. The dataset consists of professional translations from English into two regional variants each of Portuguese and Mandarin Chinese. Source documents are selected to enable detailed analysis of phenomena of interest, including lexically distinct terms and distractor terms. We explore automatic evaluation metrics for FRMT and validate their correlation with expert human evaluation across both region-matched and mismatched rating scenarios. Finally, we present a number of baseline models for this task, and offer guidelines for how researchers can train, evaluate, and compare their own models. Our dataset and evaluation code are publicly available: https://bit.ly/frmt-task
    
[^83]: Sci-Net: 尺度不变模型用于从航空图像中分割建筑物

    Sci-Net: Scale Invariant Model for Buildings Segmentation from Aerial Imagery. (arXiv:2111.06812v5 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2111.06812](http://arxiv.org/abs/2111.06812)

    本文提出了一种尺度不变的神经网络架构Sci-Net，用于从广泛范围空间分辨率的航空图像中分割建筑物，并且在实验中表现出显著优越性能。

    

    建筑物的分割是地球观测和航空图像分析领域的基本任务。现有文献中大部分基于深度学习的方法只能应用于固定或狭窄范围的空间分辨率图像。在实际场景中，用户处理的是广泛的图像分辨率范围。因此，给定的航空图像通常需要进行重采样以匹配用于训练深度学习模型的空间分辨率数据集，这导致分割性能的下降。为了克服这一挑战，我们在本文中提出了尺度不变神经网络（Sci-Net）架构，用于从广泛范围的空间分辨率航空图像中分割建筑物。具体而言，我们的方法利用了UNet的层次表示和Dense Atrous Spatial Pyramid Pooling来提取细粒度的多尺度表示。Sci-Net在Open Cities AI和Multi-Scale Building数据集上明显优于现有模型。

    Buildings' segmentation is a fundamental task in the field of earth observation and aerial imagery analysis. Most existing deep learning-based methods in the literature can be applied to a fixed or narrow-range spatial resolution imagery. In practical scenarios, users deal with a broad spectrum of image resolutions. Thus, a given aerial image often needs to be re-sampled to match the spatial resolution of the dataset used to train the deep learning model, which results in a degradation in segmentation performance. To overcome this challenge, we propose, in this manuscript, Scale-invariant Neural Network (Sci-Net) architecture that segments buildings from wide-range spatial resolution aerial images. Specifically, our approach leverages UNet hierarchical representation and Dense Atrous Spatial Pyramid Pooling to extract fine-grained multi-scale representations. Sci-Net significantly outperforms state of the art models on the Open Cities AI and the Multi-Scale Building datasets with a ste
    

