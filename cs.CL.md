# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [PALO: A Polyglot Large Multimodal Model for 5B People](https://arxiv.org/abs/2402.14818) | 该文介绍了一个名为PALO的大型多语种多模态模型，实现了对10种主要语言的视觉推理能力，涵盖了约50亿人口。其通过半自动化翻译方法，将多语言多模态数据集从英语翻译为目标语言，以提升跨多种语言的性能。 |
| [^2] | [Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking](https://arxiv.org/abs/2402.14811) | 通过对语言模型进行微调，我们研究了如何影响实体跟踪等内部机制，并发现微调能够在数学任务上实现明显的性能提升。 |
| [^3] | [CriticBench: Benchmarking LLMs for Critique-Correct Reasoning](https://arxiv.org/abs/2402.14809) | CriticBench是一个综合基准测试，旨在评估LLMs在批判和纠正推理方面的能力，发现批判性训练显著提升性能，逻辑任务更易于修正。 |
| [^4] | [RelayAttention for Efficient Large Language Model Serving with Long System Prompts](https://arxiv.org/abs/2402.14808) | 本论文提出的RelayAttention算法旨在改善涉及长系统提示的大型语言模型服务的效率，通过一次性从DRAM读取隐藏状态来消除现有因果注意力算法中的内存访问冗余。 |
| [^5] | [Identifying Multiple Personalities in Large Language Models with External Evaluation](https://arxiv.org/abs/2402.14805) | 通过外部评估方法研究大型语言模型的人格特征 |
| [^6] | [Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset](https://arxiv.org/abs/2402.14804) | 提出了MATH-Vision（MATH-V）数据集，用于评估大型多模态模型（LMMs）的数学推理能力，通过实验证实了当前LMMs和人类在MATH-V上的表现差距。 |
| [^7] | [Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2402.14800) | 引入了专家级稀疏化技术，提出了专家修剪和跳过的后训练方法，以提高MoE LLMs的部署效率，同时保持模型性能。 |
| [^8] | [Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic](https://arxiv.org/abs/2402.14798) | 本文提出了一种一致且在理论上有根据的方法来注释分解蕴涵数据集，形成RDTE数据集，该数据集在解决何为有效的组合蕴涵的问题上有显著进展。 |
| [^9] | [Zero-shot cross-lingual transfer in instruction tuning of large language model](https://arxiv.org/abs/2402.14778) | 本研究探讨了大型语言模型在指令微调中的零次跨语言转移，发现在适当超参数调整和足够大的数据支持下，英语训练的模型能够成功生成其他语言的准确、有用回应，但存在事实准确性和流畅性错误。 |
| [^10] | [2D Matryoshka Sentence Embeddings](https://arxiv.org/abs/2402.14776) | Matryoshka表示学习(MRL)以更细粒度地编码信息，以适应临时任务，同时实现了更小的嵌入大小，从而加快了下游任务的速度。 |
| [^11] | [MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues](https://arxiv.org/abs/2402.14762) | 提出了MT-Bench-101用于评估大型语言模型在多轮对话中的细粒度能力，构建了包含4208轮对话数据的三级分层能力分类，并评估了21种流行的语言模型，发现它们在不同对话轮次中表现出不同的趋势。 |
| [^12] | [Generalizing Reward Modeling for Out-of-Distribution Preference Learning](https://arxiv.org/abs/2402.14760) | 通过元学习方法优化通用奖励模型，以解决超出分布偏好学习问题，并提高LLMs在有限偏好反馈下的泛化能力 |
| [^13] | [Scaling Efficient LLMs](https://arxiv.org/abs/2402.14746) | 训练得到的LLM模型通常是稀疏的，为了提高效率，研究了在训练语料上达到所需准确度的参数最少的高效LLM模型，得出了参数数量与自然训练语料规模之间的关系，并指出扩展可以揭示新技能。 |
| [^14] | [Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation](https://arxiv.org/abs/2402.14744) | 提出了一种将大型语言模型LLMs整合到代理框架中的新方法，用于生成个人移动生成，重点是解决将LLMs与真实城市流动数据对齐的问题，并提出了一种自洽方法和检索增强策略来实现可解释活动生成。 |
| [^15] | [Dependency Annotation of Ottoman Turkish with Multilingual BERT](https://arxiv.org/abs/2402.14743) | 使用多语言BERT对奥斯曼土耳其语进行依存标注，加速并简化依存标注过程，将产生的树库有助于奥斯曼土耳其语文档的自动分析。 |
| [^16] | [Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models](https://arxiv.org/abs/2402.14714) | 提出了一种高效且有效的词汇扩展方法（EEVE），可以显著提升非英语语言模型的性能，使得其在韩文文本理解方面表现出色。 |
| [^17] | [IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus](https://arxiv.org/abs/2402.14710) | 发布了IEPile，一个包含约0.32B个标记的综合双语IE指令语料库，通过收集和清理33个现有IE数据集并引入基于模式的指令生成，可以提高大型语言模型在信息抽取领域的性能，尤其是零样本泛化。 |
| [^18] | [An LLM-Enhanced Adversarial Editing System for Lexical Simplification](https://arxiv.org/abs/2402.14704) | 该论文提出了一种新颖的词汇简化方法，不需要平行语料库，在原始句子中预测词汇修改，引入LLM增强损失进行知识提炼，并采用基于难度感知的填充模块将复杂词替换为简单词，实验证明方法的有效性。 |
| [^19] | [InfFeed: Influence Functions as a Feedback to Improve the Performance of Subjective Tasks](https://arxiv.org/abs/2402.14702) | InfFeed将影响函数作为反馈，用于改善主观任务表现，并通过自动识别需要交叉检查的数据点以提高模型性能，在调整标签方面优于现有基线。 |
| [^20] | [COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling](https://arxiv.org/abs/2402.14701) | 本文提出了一种名为COMPASS的新框架，通过分析心理治疗会话中的自然语言，直接推断治疗工作联盟，为临床精神病学提供了可解释性，并在识别与正在治疗的疾病相关的新兴模式方面发挥作用。 |
| [^21] | [Unveiling Linguistic Regions in Large Language Models](https://arxiv.org/abs/2402.14700) | 本文从区域划分的角度出发，发现了大型语言模型中对应语言能力的核心区域，并展示了去除该区域会导致跨30种不同语言的显著性能下降。 |
| [^22] | [UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models](https://arxiv.org/abs/2402.14690) | 提出了一种基于大型语言模型的统一灵活评估框架UFO，用于验证事实并支持即插即用的事实来源。 |
| [^23] | [Is Cognition and Action Consistent or Not: Investigating Large Language Model's Personality](https://arxiv.org/abs/2402.14679) | 通过评估大型语言模型在表达人类个性特征方面的可靠性，研究认知与行为之间的一致性，以及提出对观察结果的心理理论和指标假设 |
| [^24] | [Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments](https://arxiv.org/abs/2402.14672) | 这项研究探索了在复杂环境中利用工具增强大型语言模型的潜力，设计了定制化工具来辅助语言代理在庞大环境中进行探索，并展示了在知识库和数据库等复杂环境中，借助工具增强语言代理的重要潜力。 |
| [^25] | [ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models](https://arxiv.org/abs/2402.14660) | 介绍了ConceptMath，一种双语的细粒度基准测试，用于评估大型语言模型的概念性数学推理能力，并发现现有模型在不同数学概念上存在显著性能差异，甚至可能在最基本的概念上出现失败。 |
| [^26] | [OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement](https://arxiv.org/abs/2402.14658) | OpenCodeInterpreter是一种开源代码系统，集成了执行、人类反馈和动态代码细化的功能，并在关键基准测试中表现出色，甚至与GPT-4相媲美。 |
| [^27] | [Cleaner Pretraining Corpus Curation with Neural Web Scraping](https://arxiv.org/abs/2402.14652) | 使用神经网络网络抓取器NeuScraper可以从网页中提取干净的文本内容，并且实现了超过20%的改进，有助于提高语言模型的预训练质量 |
| [^28] | [RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation](https://arxiv.org/abs/2402.14623) | RoboScript是一个旨在填补“理想到实际”差距的平台，提供可部署的机器人操作流水线，并为自然语言中的机器人操作任务提供代码生成基准。 |
| [^29] | [From Keywords to Structured Summaries: Streamlining Scholarly Knowledge Access](https://arxiv.org/abs/2402.14622) | 该论文突出了信息检索引擎在科学界的重要性，并提出了一种通过结构化记录和先进信息技术工具实现的解决方案，以革新研究人员访问和过滤文章的方式。 |
| [^30] | [The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations](https://arxiv.org/abs/2402.14616) | 单词分割对上下文化单词表示的语义内容有影响，研究发现被分割的单词表示质量通常比已知单词差，但相似性值需谨慎解释。 |
| [^31] | [Two Counterexamples to \textit{Tokenization and the Noiseless Channel}](https://arxiv.org/abs/2402.14614) | 该论文讨论了在《Tokenization and the Noiseless Channel》提出的使用Rényi效率作为分词器评估机制的局限性，并描述了两个BPE分词的反例，展示了Rényi效率无法捕捉到所有优秀分词方案的情况。 |
| [^32] | [Improving Assessment of Tutoring Practices using Retrieval-Augmented Generation](https://arxiv.org/abs/2402.14594) | 通过利用生成预训练变换器来自动评估辅导员使用社交情感辅导策略的能力，从而改进辅导实践评估。 |
| [^33] | [Scaling Up LLM Reviews for Google Ads Content Moderation](https://arxiv.org/abs/2402.14590) | 本研究提出了一种用于在Google广告中进行内容管理的方法，通过使用LLMs审核代表性广告并将决策传播回其群集，将审核数目减少了3个数量级以上，同时实现了2倍的召回率。 |
| [^34] | [Text Role Classification in Scientific Charts Using Multimodal Transformers](https://arxiv.org/abs/2402.14579) | 该研究提出了使用多模态Transformer在科学图表中进行文本角色分类的方法，并在实验中发现LayoutLMv3在性能上优于UDOP，最高达到了82.87的F1-macro分数。 |
| [^35] | [LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition](https://arxiv.org/abs/2402.14568) | 提出了一种新的数据增强技术$LLM-DA$，基于大型语言模型，用于少样本NER任务，在上下文和实体层面增强数据，展示了显著的性能提升。 |
| [^36] | [LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A Survey](https://arxiv.org/abs/2402.14558) | 本文调查了在工业背景下利用LLMs所面临的挑战和前景，并通过对行业从业者调查以及审查大量行业论文得出有意义的结论。 |
| [^37] | [OmniPred: Language Models as Universal Regressors](https://arxiv.org/abs/2402.14547) | 本文提出了OmniPred框架，用于训练语言模型作为通用的端到端回归器，实验证明，在多个任务上训练时，语言模型能够显著优于传统回归模型。 |
| [^38] | [Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective](https://arxiv.org/abs/2402.14545) | 本文研究了大型多模态模型中存在的多模态幻觉问题，发现通过模型基于视觉感知作出适当的EOS决策，可以减少持续输出，提出了两种缓解方法。 |
| [^39] | [Domain Generalization via Causal Adjustment for Cross-Domain Sentiment Analysis](https://arxiv.org/abs/2402.14536) | 本文提出了基于后门调整的因果模型，用于解决领域泛化问题，通过解开领域特定和领域不变表示之间的关系来应对领域转移 |
| [^40] | [Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard](https://arxiv.org/abs/2402.14533) | 通过对GPT-3.5、GPT-4和Bard生成的文本进行语言分析比较，发现不同的LLM之间存在显著的语言变化，可以以88%的准确率通过简单的分类模型将文本归因于相应的LLM来源。 |
| [^41] | [Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance](https://arxiv.org/abs/2402.14531) | 礼貌水平对LLMs的表现有影响，粗鲁的提示通常导致较差的表现，而过于礼貌的语言并不能保证更好的结果，最佳的礼貌水平根据语言而异，LLMs不仅反映人类行为，还受语言影响，尤其是在不同的文化背景下。 |
| [^42] | [Balanced Data Sampling for Language Model Training with Clustering](https://arxiv.org/abs/2402.14526) | 本文提出了一种名为ClusterClip Sampling的数据抽样方法，利用数据聚类平衡训练数据的文本分布，为更好的模型训练提供支持。 |
| [^43] | [Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding Decomposition](https://arxiv.org/abs/2402.14523) | 本文提出了Daisy-TTS设计，通过声调嵌入分解，模拟了更广泛的情感范围，包括 primary emotions、secondary emotions、intensity-level 和 emotions polarity。 |
| [^44] | [Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond](https://arxiv.org/abs/2402.14522) | 提出了一种框架用于统一不同模型的任务嵌入，使得任务嵌入可以跨越各种模型，并在单一向量空间内进行比较和分析。 |
| [^45] | [Malaysian English News Decoded: A Linguistic Resource for Named Entity and Relation Extraction](https://arxiv.org/abs/2402.14521) | 通过构建一个含有实体和关系标注的马来西亚英语新闻数据集，并对spaCy NER工具进行微调，本研究显著改善了马来西亚英语中NER的性能。 |
| [^46] | ["My Answer is C": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models](https://arxiv.org/abs/2402.14499) | 第一个令牌预测不一定代表最终文本输出，在评估大型语言模型时存在严重的不一致性，影响模型行为与用户互动。 |
| [^47] | [Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment Pre-training for Noisy Slot Filling Task](https://arxiv.org/abs/2402.14494) | 提出了Noise-BERT框架，包含噪声对齐预训练任务，通过对比学习损失和对抗攻击训练策略，以提高在嘈杂环境下的槽填充任务表现。 |
| [^48] | [INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction Fine-tuning](https://arxiv.org/abs/2402.14492) | INSTRAUG是一种自动指令增强方法，可以在多模任务中显著改善多模大型语言模型的对齐，相当于增加训练规模的好处。 |
| [^49] | [Does the Generator Mind its Contexts? An Analysis of Generative Model Faithfulness under Context Transfer](https://arxiv.org/abs/2402.14488) | 本研究引入了知识增强的生成器，旨在产生保持基于上下文知识的信息，无论上下文如何改变。调查发现所有模型都有生成先前答案作为幻觉的倾向。 |
| [^50] | [Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation and Analysis](https://arxiv.org/abs/2402.14484) | 本研究全面评估了ChatGPT的因果文本挖掘能力，发现ChatGPT在各种数据集上表现良好，但当有足够多的训练数据时，以往的模型仍然超越了它。 |
| [^51] | [Data Science with LLMs and Interpretable Models](https://arxiv.org/abs/2402.14474) | 该研究展示了大型语言模型LLMs在描述、解释和调试广义加性模型GAMs方面的出色表现，结合LLMs的灵活性和GAMs准确描述的统计模式，可以实现数据集摘要、问答和模型评估。 |
| [^52] | [NLAS-multi: A Multilingual Corpus of Automatically Generated Natural Language Argumentation Schemes](https://arxiv.org/abs/2402.14458) | 该论文提出了一个多语言自动生成的自然语言论证架构语料库，包括自动生成自然语言论证的有效方法、最大的公开可用的语料库以及用于自动识别论证架构的基线和模型。 |
| [^53] | [Annotation and Classification of Relevant Clauses in Terms-and-Conditions Contracts](https://arxiv.org/abs/2402.14457) | 提出了一种新的标注方案，用于分类条款和条件合同中的不同类型的条款，以支持法律专家快速识别和评估问题，并展示了通过实验证明了对这些类别进行自动分类的可行性。 |
| [^54] | [Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?](https://arxiv.org/abs/2402.14453) | LLMs可以隐式处理用户输入和生成的响应之间的文本难度，有些LLMs在处理文本难度上甚至可以超越人类。 |
| [^55] | [A Language Model's Guide Through Latent Space](https://arxiv.org/abs/2402.14433) | 本文将语言模型概念引导框架扩展到更丰富的概念集，探索当前检测和引导策略在适当性、幽默、创造力和质量等挑战性环境下的适用程度。 |
| [^56] | [KoCoSa: Korean Context-aware Sarcasm Detection Dataset](https://arxiv.org/abs/2402.14428) | 该论文介绍了一个新的针对韩文对话讽刺检测任务的数据集KoCoSa，提出了一种高效的讽刺检测数据集生成流程，并提供了针对该任务的简单但有效的基线模型。 |
| [^57] | [Text me the data: Generating Ground Pressure Sequence from Textual Descriptions for HAR](https://arxiv.org/abs/2402.14427) | 使用Text-to-Pressure（T2P）框架，结合深度学习技术，从文本描述中生成高质量地面压力序列，实现了文本与生成动作的一致性。 |
| [^58] | [J-UniMorph: Japanese Morphological Annotation through the Universal Feature Schema](https://arxiv.org/abs/2402.14411) | J-UniMorph通过提供更广泛且更频繁使用的动词形式，包括敬语、一系列礼貌水平和其他语言细微差别，强调了日语的独特特点。 |
| [^59] | [Tug-of-War Between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models](https://arxiv.org/abs/2402.14409) | 本文探讨了检索增强语言模型中的知识冲突，提出了评估框架，研究了RALMs对内部记忆和外部来源间的冲突，发现了它们会偏向错误的内部记忆。 |
| [^60] | [Transferring BERT Capabilities from High-Resource to Low-Resource Languages Using Vocabulary Matching](https://arxiv.org/abs/2402.14408) | 通过词汇匹配将BERT能力从高资源语言转移到低资源语言，有效弥合低资源语言训练困难的差距。 |
| [^61] | [On the Tip of the Tongue: Analyzing Conceptual Representation in Large Language Models with Reverse-Dictionary Probe](https://arxiv.org/abs/2402.14404) | 该论文通过重新利用反向词典任务的案例研究，探查了大型语言模型对概念推理的能力，发现模型在该任务中表现出高准确性，并且表示空间编码了有关对象类别和细粒度特征的信息，同时还发现该任务探查的概念推理能力能够预测模型在多个基准测试中的一般推理表现。 |
| [^62] | [Enhancing Temporal Knowledge Graph Forecasting with Large Language Models via Chain-of-History Reasoning](https://arxiv.org/abs/2402.14382) | 提出了一种通过历史链推理来增强时间知识图预测的方法，有效利用高阶历史信息，弥补了基于大型语言模型的模型在处理历史信息和时间推理能力方面的不足。 |
| [^63] | [Novi jezi\v{c}ki modeli za srpski jezik](https://arxiv.org/abs/2402.14379) | Rad predstavlja novi jezički model za srpski jezik zasnovan na transformerima, obučen na resursima Društva za jezičke resurse i tehnologije, koji će biti upoređen sa deset odabranih modela vektorizacije na četiri zadatka obrade prirodnog jezika. |
| [^64] | [Small Language Model Is a Good Guide for Large Language Model in Chinese Entity Relation Extraction](https://arxiv.org/abs/2402.14373) | 本文提出了SLCoLM，一个模型协作框架，通过使用“训练-指导-预测”策略结合预训练语言模型和大语言模型，成功缓解了长尾数据问题，促进了实体关系的抽取。 |
| [^65] | [Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark](https://arxiv.org/abs/2402.14359) | 该论文提出了一种基于方面感知的评估指标（FM），利用大型语言模型对摘要进行高级语义匹配，提供了一种全面评估科学摘要的方法。 |
| [^66] | [Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?](https://arxiv.org/abs/2402.14355) | 本文研究了大型语言模型通过讲故事来表达固有的常识能力，实验结果显示故事优于规则作为从LLMs检索常识的表达形式。 |
| [^67] | [AURA: Natural Language Reasoning for Aleatoric Uncertainty in Rationales](https://arxiv.org/abs/2402.14337) | 提出了在自然语言推理中处理引发模式合理性不确定性的不完美理由的方法，实施了使用熵分数和模型先验信念来指导模型的策略，并在实证中展示了方法相对于敌对理由的稳健性能优势 |
| [^68] | [INSTRUCTIR: A Benchmark for Instruction Following of Information Retrieval Models](https://arxiv.org/abs/2402.14334) | INSTRUCTIR是一个专门设计用于评估信息检索任务中指令遵循能力的新型基准。 |
| [^69] | [Understanding and Patching Compositional Reasoning in LLMs](https://arxiv.org/abs/2402.14328) | 本研究通过 Logit Lens 和干预实验揭示了LLMs中隐性推理结果的重要性，开发了一种修补组合推理错误的轻量级方法。 |
| [^70] | [Subobject-level Image Tokenization](https://arxiv.org/abs/2402.14327) | 提出一种在子对象级别进行图像标记的方法，通过序列自编码器将子对象段压缩为紧凑的嵌入向量，实现了有效地将图像转换为对象和属性描述的学习。 |
| [^71] | [Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering](https://arxiv.org/abs/2402.14320) | Triad框架利用了基于多角色LLM代理来解决知识库问答问题，通过代理的不同角色分别处理KBQA子任务，合作完成KBQA任务，并在多个基准数据集上表现出色。 |
| [^72] | [Assessing generalization capability of text ranking models in Polish](https://arxiv.org/abs/2402.14318) | 评估波兰语文本排名模型的泛化能力，研究表明，通过有效的优化方法和大型训练数据集的结合，可以构建既体积小又具有泛化能力的重新排名器。 |
| [^73] | [Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge](https://arxiv.org/abs/2402.14310) | 引入提示-解决前提示（HSP）方法，指导LLMs生成解决问题的提示并生成包含中间推理步骤的解决方案，有效提高了推理任务的准确性。 |
| [^74] | [Multi-modal Stance Detection: New Datasets and Model](https://arxiv.org/abs/2402.14298) | 本文研究了多模式立场检测，提出了新的数据集和模型，并展示了所提出的TMPT框架在多模式立场检测中取得了最先进的性能 |
| [^75] | [Mitigating Biases of Large Language Models in Stance Detection with Calibration](https://arxiv.org/abs/2402.14296) | 本文提出了一种通过校准来减轻大语言模型在立场检测中偏见的方法，设计了门控校准网络并构建了反事实增强数据，实验证明其效果显著。 |
| [^76] | [Leveraging Large Language Models for Concept Graph Recovery and Question Answering in NLP Education](https://arxiv.org/abs/2402.14293) | 本研究利用大型语言模型在教育中进行概念图恢复和问答，并提出了专门针对科学图推理和QA的新基准TutorQA，结果表明LLMs在零-shot概念图恢复和TutorQA任务中的表现优秀。 |
| [^77] | [CEV-LM: Controlled Edit Vector Language Model for Shaping Natural Language Generations](https://arxiv.org/abs/2402.14290) | CEV-LM 是一个轻量、半自回归语言模型，利用受限制的编辑向量控制文本的速度、音量和绕圈度量，从而更精准地定制生成的文本形状，比现有控制方法具有更好的控制效果。 |
| [^78] | [TinyLLaVA: A Framework of Small-scale Large Multimodal Models](https://arxiv.org/abs/2402.14289) | TinyLLaVA框架使得小型多模态模型能够通过更好的数据质量和训练方案达到与大型模型相媲美的性能，最佳模型TinyLLaVA-3.1B在整体性能上优于现有的7B模型。 |
| [^79] | [Mitigating the Linguistic Gap with Phonemic Representations for Robust Multilingual Language Understanding](https://arxiv.org/abs/2402.14279) | 通过使用音素表示，本文提出了一种新颖的解决方案来减缓高资源语言和低资源语言之间的性能差距，并通过实证研究和理论分析证明了其有效性。 |
| [^80] | [GATE X-E : A Challenge Set for Gender-Fair Translations from Weakly-Gendered Languages](https://arxiv.org/abs/2402.14277) | 引入了GATE X-E挑战集，包含了从土耳其语、匈牙利语、芬兰语和波斯语翻译成英语的人类翻译，旨在评估弱性别语言到英语的翻译中的性别偏见并提出缓解策略。 |
| [^81] | [Can Language Models Act as Knowledge Bases at Scale?](https://arxiv.org/abs/2402.14273) | 大型语言模型在存储、回忆和推理大规模知识方面的表现被探讨，研究结果表明其具有一定的潜力。 |
| [^82] | [Qsnail: A Questionnaire Dataset for Sequential Question Generation](https://arxiv.org/abs/2402.14272) | Qsnail提出了一个用于顺序问题生成的问卷数据集，填补了高质量数据集稀缺导致的自动生成问卷领域关注不足的空白。 |
| [^83] | [Can Large Language Models Detect Misinformation in Scientific News Reporting?](https://arxiv.org/abs/2402.14268) | 大型语言模型探测科学报道中的错误信息的可行性，绕过生成明确标记索赔的步骤，处理现实场景中可能不存在明确标记索赔的挑战。 |
| [^84] | [Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond](https://arxiv.org/abs/2402.14259) | 本论文提出了一种新方法单词序列熵（WSE），用于在自由形式医学问答任务中量化答案的不确定性，相比其他基线方法表现更优秀。 |
| [^85] | [Eagle: Ethical Dataset Given from Real Interactions](https://arxiv.org/abs/2402.14258) | 该论文提出了鹰数据集，从ChatGPT和用户的真实互动中提取，展示了社会偏见、毒性和不道德问题。实验证明鹰捕捉到了现有用于评估和减轻这些道德挑战的数据集所未覆盖的补充方面。 |
| [^86] | [Framing in the Presence of Supporting Data: A Case Study in U.S. Economic News](https://arxiv.org/abs/2402.14224) | 本文提出了一个计算框架，旨在分析主流媒体在报道经济消息时的编辑选择，通过对经济指标的报道进行框架分析，我们可以理解出版物选择和构架的方式。 |
| [^87] | [Content Conditional Debiasing for Fair Text Embedding](https://arxiv.org/abs/2402.14208) | 通过在内容条件下确保敏感属性与文本嵌入之间的条件独立性，我们提出了一种可以改善公平性的新方法，在保持效用的同时，解决了缺乏适当训练数据的问题。 |
| [^88] | [Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models](https://arxiv.org/abs/2402.14207) | 提出了一种名为STORM的写作系统，用于通过检索和多视角提问合成主题概要，以辅助从头开始写类似维基百科的文章。 |
| [^89] | [Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models](https://arxiv.org/abs/2402.14200) | 本文提出了一种系统方法，结合领域知识和大型语言模型来更好地表示危机辅导员与求助者之间的对话，实验证明加入领域知识和模型特征可以提高模型性能约15%。 |
| [^90] | [Learning to Reduce: Optimal Representations of Structured Data in Prompting Large Language Models](https://arxiv.org/abs/2402.14195) | 该研究提出了一个名为学习减少的框架，利用强化学习减少输入上下文，以提高固定大型语言模型的推理性能，并展现了在不同数据集上的泛化能力。 |
| [^91] | [From Adoption to Adaption: Tracing the Diffusion of New Emojis on Twitter](https://arxiv.org/abs/2402.14187) | 本研究通过分析英文推文数据集，探讨了新表情符号在Twitter上的传播情况，发现早期采纳者规模和表情符号语义对其流行度至关重要，并提出了一个新框架来解释新表情符号，从而改善情感分类性能。 |
| [^92] | [Diversity-Aware Ensembling of Language Models Based on Topological Data Analysis](https://arxiv.org/abs/2402.14184) | 基于拓扑数据分析的方法，通过估计NLP模型集成的权重，提高了集成模型的质量，提高了文本分类准确性和相关不确定性估计。 |
| [^93] | [Bangla AI: A Framework for Machine Translation Utilizing Large Language Models for Ethnic Media](https://arxiv.org/abs/2402.14179) | 该研究探讨了如何在民族媒体领域整合大型语言模型和多语言机器翻译，以提升新闻翻译、搜索和分类的效率和准确性。 |
| [^94] | [TOOLVERIFIER: Generalization to New Tools via Self-Verification](https://arxiv.org/abs/2402.14158) | 通过自验证方法，本研究提出了一种区分新工具的方法，通过对比性自问问题来实现工具选择和参数生成，进一步提升了语言模型对新工具的学习能力。 |
| [^95] | [Can Similarity-Based Domain-Ordering Reduce Catastrophic Forgetting for Intent Recognition?](https://arxiv.org/abs/2402.14155) | 研究探讨了三种领域排序策略对生成式意图识别模型继续学习性能的影响，填补了现有研究中对此方面未探索的空白。 |
| [^96] | [MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms](https://arxiv.org/abs/2402.14154) | 该研究介绍了MM-Soc，一个旨在评估多模态大型语言模型（MLLMs）对社交媒体内容理解的综合基准，通过对十种大小变体的四个开源MLLMs进行详尽评估，发现了显著的性能差异。 |
| [^97] | [BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives](https://arxiv.org/abs/2402.14151) | BIRCO基准评估基于大型语言模型的信息检索系统对多方面用户目标的检索能力，发现新的检索协议和更强大的模型是解决复杂用户需求的必要条件。 |
| [^98] | [Reinforcement Learning with Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation](https://arxiv.org/abs/2402.14146) | 本文提出了一种使用强化学习来控制多种风格生成的方法，通过动态权重调整多重奖励，实现了在生成文本时同时控制多种风格。 |
| [^99] | [Combining Language and Graph Models for Semi-structured Information Extraction on the Web](https://arxiv.org/abs/2402.14129) | GraphScholarBERT是一种结合语言和图模型的信息抽取方法，能够在Web上半结构化信息中提取目标关系，并在零射领域和零射网站设置中将提取F1得分提高34.8％。 |
| [^100] | [FanOutQA: Multi-Hop, Multi-Document Question Answering for Large Language Models](https://arxiv.org/abs/2402.14116) | FanOutQA 提出了一个高质量的多跳、多文档问答数据集，用于评估大型语言模型在复杂推理能力上的表现，并发现当代模型在长篇上下文中仍有改进交叉文档依赖推理的空间。 |
| [^101] | [Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation](https://arxiv.org/abs/2402.14101) | 引入了一个新颖的框架，通过少样本标注者调适来实现在主观任务中进行高效标注与建模，最大程度减少标注预算，同时最大化每个标注者的预测性能。 |
| [^102] | [LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons](https://arxiv.org/abs/2402.14086) | LexC-Gen提出了一种词典条件数据生成方法，可以以大规模生成低资源语言分类任务数据，取得了较好的效果。 |
| [^103] | [Improving Language Understanding from Screenshots](https://arxiv.org/abs/2402.14073) | 本文提出了一种屏幕截图语言模型，通过引入新的Patch-and-Text Prediction（PTP）目标来改善文本能力，并取得了与BERT相当的性能。 |
| [^104] | [On Leveraging Encoder-only Pre-trained Language Models for Effective Keyphrase Generation](https://arxiv.org/abs/2402.14052) | 本研究探讨了在关键短语生成中利用仅编码器预训练语言模型的效果和优化策略，并比较了不同资源设置下领域内编码器-only和编码器-解码器模型的性能。 |
| [^105] | [CriticBench: Evaluating Large Language Models as Critic](https://arxiv.org/abs/2402.13764) | CriticBench是一个旨在全面和可靠地评估大型语言模型的评论能力的新型基准，展示了评论能力与任务、响应质量和模型规模之间的关系。 |
| [^106] | [$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens](https://arxiv.org/abs/2402.13718) | 提出了$\infty$Bench，第一个以平均数据长度超过10万个令牌的LLM基准，用于评估处理长上下文的能力 |
| [^107] | [CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models](https://arxiv.org/abs/2402.13607) | 介绍了CODIS基准，用于评估模型利用自由形式文本提供的上下文来增强视觉理解的能力，发现多模态大型语言模型在此基准上表现未达到人类水平，需要提升模型理解视觉能力。 |
| [^108] | [KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge](https://arxiv.org/abs/2402.13605) | KorNAT是首个用于评估韩国国家对齐的基准，包括社会价值观和常识对齐两个方面，通过对社会价值和常识多项选择题的测试来评估模型的对齐程度。 |
| [^109] | [LongWanjuan: Towards Systematic Measurement for Long Text Quality](https://arxiv.org/abs/2402.13583) | 本研究针对长文本评估的差距，引入了一套基于连贯性、凝聚力和复杂性等语言学维度的指标来系统性衡量长文本的质量，并提出了LongWanjuan数据集，有助于提升长文本任务的语言模型训练。 |
| [^110] | [Infrastructure Ombudsman: Mining Future Failure Concerns from Structural Disaster Response](https://arxiv.org/abs/2402.13528) | 本文开发了一种基础设施调解员系统，用于自动检测特定基础设施问题，通过挖掘社交网络中关于预期失败的担忧，有助于预防和减轻潜在的基础设施失败。 |
| [^111] | [RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models](https://arxiv.org/abs/2402.13463) | 本文提出了一个名为RefuteBench的基准测试，旨在评估大型语言模型对反驳指令的遵循能力，发现LLMs倾向于固执于其内部知识而无法遵从用户反馈。 |
| [^112] | [EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries](https://arxiv.org/abs/2402.13372) | EvoGrad是一个以人类对手为特点的用于解决Winograd Schema挑战的动态方法，通过人在环中方法创建动态数据集，拓展任务实例并引入错误深度度量标准，提出新的多样化常识推理数据集基准，揭示了当前语言模型在此类任务上的挑战。 |
| [^113] | [Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism](https://arxiv.org/abs/2402.12997) | 提出了一种适用于现实约束的轻量级弃权机制，特别适用于再排序阶段，通过数据驱动的方法达到有效性，并提供了开源代码以促进其更广泛的应用。 |
| [^114] | [Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data](https://arxiv.org/abs/2402.12424) | 本研究探讨了LLM在解释表格数据方面的有效性，比较了文本和图像表格表示对LLM性能的影响，为在表格相关任务上有效使用LLM提供了见解。 |
| [^115] | [Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs](https://arxiv.org/abs/2402.12052) | 本文介绍了一种新的协作方法SlimPLM，通过精简代理模型检测LLMs中缺失的知识，以增强LLMs的知识获取过程 |
| [^116] | [A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change](https://arxiv.org/abs/2402.12011) | 本文通过在相同条件下评估了最新的词汇语义变化模型和方法，将LSC问题分解为不同级别的任务，并展示了在不同语言的八个基准测试中，APD在GCD方面优于其他方法，XL-LEXEME在WiC、WSI和GCD方面优于其他上下文化模型，并且与GPT-4相当。 |
| [^117] | [ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs](https://arxiv.org/abs/2402.11753) | 提出了一种新颖的基于ASCII艺术的越狱攻击，以及一个用于评估LLMs在识别非纯语义提示方面能力的基准挑战。五个SOTA LLMs在识别ASCII艺术提示时存在困难。 |
| [^118] | [Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution](https://arxiv.org/abs/2402.11525) | 提出了一种利用强化学习和人类反馈（RLHF）来改进翻译质量的成本效益偏好学习策略，该策略通过优化奖励模型来区分人类和机器翻译，从而指导改进机器翻译。 |
| [^119] | [Exploring Value Biases: How LLMs Deviate Towards the Ideal](https://arxiv.org/abs/2402.11005) | 研究发现大型语言模型（LLMs）在给出响应时存在一个价值偏好的机制，倾向于偏向理想状态，这种偏差会对不同应用场景产生重要影响。 |
| [^120] | [Knowledge of Pretrained Language Models on Surface Information of Tokens](https://arxiv.org/abs/2402.09808) | 该研究探究了预训练语言模型对于标记的表面信息的知识。结果显示，模型具备有关标记长度和子字符串的知识，但对标记结构的知识有限，解码器方面存在瓶颈。 |
| [^121] | [CodeMind: A Framework to Challenge Large Language Models for Code Reasoning](https://arxiv.org/abs/2402.09664) | CodeMind是一个用于挑战大型语言模型进行代码推理的框架，通过评估LLMs的代码推理能力来替代仅仅依靠测试通过来评估，对三种代码推理任务进行评估，结果显示LLMs能够公正地理解控制流结构，并且对于简单程序和复杂程序，它们通常能够推理出输入如何演变为输出。 |
| [^122] | [Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping](https://arxiv.org/abs/2402.07610) | 本文首次探索了自助引导自对齐对大型语言模型的影响，发现其明显优于单次循环的方法，并通过调整数据训练顺序进一步提升模型性能。 |
| [^123] | [Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs](https://arxiv.org/abs/2402.03927) | 该论文研究了封闭源LLMs中的数据污染和评估不端行为。通过对255篇论文的分析和OpenAI的数据使用政策考虑，研究人员发现这些模型在第一年发布后存在泄露数据的问题。 |
| [^124] | [Large Language Models As MOOCs Graders](https://arxiv.org/abs/2402.03776) | 该研究探索了利用大型语言模型（LLMs）代替MOOCs中同伴评分的可行性，旨在解决大规模在线开放课程中评估学生写作任务的问题。 |
| [^125] | [Minds versus Machines: Rethinking Entailment Verification with Language Models](https://arxiv.org/abs/2402.03686) | 本文通过研究人类和大型语言模型在推理判断中的共性和差异，发现大型语言模型在复杂推理中具有优势，而人类在简单推理中表现出色。基于这些发现，引入了一个优化的Flan-T5模型，用于蕴含验证。 |
| [^126] | [PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?](https://arxiv.org/abs/2402.02611) | 本研究通过PuzzleBench数据集探索了LLMs解决困难的一阶组合推理问题的能力，并提出了Puzzle-LM方法，该方法将LLMs与符号求解器和程序解释器相结合，使其能够有效地推理这类问题。 |
| [^127] | [Measuring Moral Inconsistencies in Large Language Models](https://arxiv.org/abs/2402.01719) | 本研究提出了一种新的信息论度量方法，称为语义图熵（SGE），用于测量道德情景中大型语言模型（LLM）的一致性。与现有的一致性度量方法相比，SGE在五个LLMs上与人类判断更好地相关，为研究LLM不一致性的根本原因提供了新的思路。 |
| [^128] | [HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text Classification](https://arxiv.org/abs/2402.01696) | HiGen提出了一个基于文本生成的框架，利用语言模型来编码动态文本表示，在层次文本分类中考虑了文档各个部分的相关性，并引入了一个层级引导的损失函数。此外，还提供了一个新颖的用于HTC的数据集ENZYME。 |
| [^129] | [MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline](https://arxiv.org/abs/2401.08190) | 本文通过引入具有Python代码解释器的数学数据集，解决了大型语言模型在数学推理能力方面的挑战。 |
| [^130] | [Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination](https://arxiv.org/abs/2401.08025) | 本文提出了Self-Imagine方法，通过利用一种Vision-Language模型生成问题的结构化表示并将其渲染为图像，再使用相同的模型回答问题，从而在数学任务和通用推理任务中提高了模型性能。 |
| [^131] | [MAPO: Advancing Multilingual Reasoning through Multilingual Alignment-as-Preference Optimization](https://arxiv.org/abs/2401.06838) | MAPO提出了一个多语言对齐作为偏好优化框架，可以显著提高各种模型在多语言推理中的表现。 |
| [^132] | [PixT3: Pixel-based Table To Text generation](https://arxiv.org/abs/2311.09808) | PixT3是一种基于像素的多模式表格到文本模型，通过将数据到文本生成视为视觉识别任务，消除了字符串格式的需求，克服了线性化和输入大小限制的挑战。 |
| [^133] | [GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks](https://arxiv.org/abs/2311.09606) | 提出了示例要点提取的方法，通过要点模型形成了新的分数评估方式，可以用于动态选择最佳示例，提高上下文示例选择的性能。 |
| [^134] | [How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection](https://arxiv.org/abs/2311.08369) | 即使是任务约束也会影响LLM生成文本的检测性能，本研究发现即使这些约束与规避无关，也会导致现有检测器性能具有显著差异 |
| [^135] | [Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models](https://arxiv.org/abs/2311.06607) | Monkey通过提高图像分辨率和采用多级描述生成方法来增强大型多模态模型(LMMs)的能力，从而实现更详细的视觉捕捉和更有效的学习。 |
| [^136] | [Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace](https://arxiv.org/abs/2310.19651) | 本研究通过对指令调整对每个大型语言模型的各项能力（如创意写作、代码生成和逻辑推理）的发展影响进行细致分析，得出了关于数据集规模、参数规模和数据构建方法的指导原则。 |
| [^137] | [MindfulDiary: Harnessing Large Language Model to Support Psychiatric Patients' Journaling](https://arxiv.org/abs/2310.05231) | MindfulDiary利用大型语言模型帮助精神病患者通过对话记录日常体验，并在临床环境中取得积极效果 |
| [^138] | [Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature](https://arxiv.org/abs/2310.05130) | 通过引入条件概率曲率概念，本文提出了Fast-DetectGPT，一个优化的零样本检测器，相对于DetectGPT在白盒和其他测试条件下的性能提升达到约75%。 |
| [^139] | [Training dynamic models using early exits for automatic speech recognition on resource-constrained devices](https://arxiv.org/abs/2309.09546) | 早期退出架构可以使自注意力模型适应不同计算资源和ASR性能需求，该研究比较了微调预训练模型和采用早期退出目标从头训练模型的效果 |
| [^140] | [Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering](https://arxiv.org/abs/2309.02233) | 该研究提出了一种名为LLMs增强医学教科书（LLM-AMT）的系统，通过插入式模块将权威医学教科书集成到LLMs的框架中，显著提高了LLMs在专业领域的能力。 |
| [^141] | [Donkii: Can Annotation Error Detection Methods Find Errors in Instruction-Tuning Datasets?](https://arxiv.org/abs/2309.01669) | 本文提出了一个新颖的指令调整数据的AED基准DONKII，并发现所有三个数据集都包含明显的错误，有时这些错误直接传播到指令调整的LLMs中。 |
| [^142] | [LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities](https://arxiv.org/abs/2305.13168) | 本研究全面评估了LLMs在知识图谱构建和推理领域的性能，发现GPT-4更适合作为推理助手，并在某些情况下超越了精调模型。 |
| [^143] | [PolQA: Polish Question Answering Dataset](https://arxiv.org/abs/2212.08897) | 波兰问答数据集（PolQA）是用于OpenQA的第一个波兰语数据集，包括7,000个问题和87,525个手动标记的证据段落，在QA系统性能方面提出了有效的注释策略，以提高段落的检索准确度@10并降低注释成本。 |
| [^144] | [Text Embeddings by Weakly-Supervised Contrastive Pre-training](https://arxiv.org/abs/2212.03533) | 本文提出了一种名为E5的文本嵌入模型，通过弱监督对比训练方式，在未经过标记数据的情况下，在多个任务中表现卓越，是第一个在BEIR检索基准测试上击败BM25基线的模型，在微调后在MTEB基准测试上获得最佳结果。 |
| [^145] | [ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots](https://arxiv.org/abs/2209.08199) | ScreenQA提出了一个新的任务和数据集，通过86K个问答对在RICO数据集上注释，旨在评估屏幕阅读理解能力。 |
| [^146] | [StreaMulT: Streaming Multimodal Transformer for Heterogeneous and Arbitrary Long Sequential Data](https://arxiv.org/abs/2110.08021) | 提出一种新的流式多模态Transformer模型，用于处理异构和任意长的序列数据，解决了在预测性维护任务中多源数据流的融合和跨时间预测的挑战。 |
| [^147] | [TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data.](http://arxiv.org/abs/2401.13223) | TAT-LLM是一种专门用于离散推理的语言模型，针对混合表格和文本数据上的问答任务。该模型通过分步流水线的方式，包括提取器、推理器和执行器，利用LLMs的强大能力来解决问题。而为了应对成本、延迟和数据安全风险等挑战，我们开发了TAT-LLM，一个专门针对此任务的较小LLM。 |
| [^148] | [From Understanding to Utilization: A Survey on Explainability for Large Language Models.](http://arxiv.org/abs/2401.12874) | 本综述论文研究了大规模语言模型(LLMs)可解释性的新兴领域，强调了在LLMs中增强可解释性的必要性，同时解决了广大公众对其信任和技术界对这些模型更深理解的需求。该综述对现有的可解释性方法进行分类，并讨论了它们在提高模型透明度和可靠性方面的应用，旨在弥合理论理解和实际应用之间的差距。 |
| [^149] | [Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model.](http://arxiv.org/abs/2401.12873) | 本研究探索了利用质量估计作为奖励模型来预测人类偏好以改善机器翻译的潜力。我们发现基于质量估计的反馈训练存在过度优化问题，采用启发式规则来检测错误翻译并对质量估计模型进行惩罚以解决该问题。 |
| [^150] | [Generating Unsupervised Abstractive Explanations for Rumour Verification.](http://arxiv.org/abs/2401.12713) | 该论文利用无监督的方法，通过对言辞进行评分并生成解释性摘要，来增加谣言验证的信息丰富度和准确性。 |
| [^151] | [Learning High-Quality and General-Purpose Phrase Representations.](http://arxiv.org/abs/2401.10407) | 本论文提出了一种改进的框架来学习高质量和通用性的短语表示。该框架在无上下文的情况下学习短语表示，通过短语类型分类和有效地融合字符级信息来提高表示的精确性和灵活性。此外，还采用了三种粒度的数据增强方法以增加训练数据的多样性。 |
| [^152] | [E^2-LLM: Efficient and Extreme Length Extension of Large Language Models.](http://arxiv.org/abs/2401.06951) | E^2-LLM是一种高效和极长扩展方法，通过仅需一次训练过程和不收集长上下文数据的方式，在大规模语言模型中实现了显著减少的计算成本。基于RoPE位置嵌入，E^2-LLM只需要较短的训练数据长度，支持不同的评估上下文窗口。 |
| [^153] | [Fine-grained Hallucination Detection and Editing for Language Models.](http://arxiv.org/abs/2401.06855) | 这项研究提出了一个新任务，即自动细粒度幻觉检测，并介绍了一个综合分类方法。通过对两个语言模型的输出进行分析，发现大部分幻觉属于少有的类别。为了解决这个问题，研究者通过训练一个检索增强语言模型，使用合成数据来检测和纠正幻觉。 |
| [^154] | [SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully.](http://arxiv.org/abs/2401.05930) | 自我突出式犹豫（SH2）是一种推理时的方法，通过选择预测概率较低的标记，并强调它们的差异，从而帮助语言模型更准确地解码。 |
| [^155] | [LLaVA-$\phi$: Efficient Multi-Modal Assistant with Small Language Model.](http://arxiv.org/abs/2401.02330) | LLaVA-$\phi$是一种高效的多模态助手，使用小型语言模型Phi-2来促进多模态对话。即使具有较少的参数，它也能有效地融合文本和视觉元素，并在各种任务中表现出色。它为时间敏感的环境和需要实时交互的系统开辟了新的应用途径。 |
| [^156] | [TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview.](http://arxiv.org/abs/2401.01330) | TREC iKAT 2023是一个交互式的知识辅助任务，旨在开发适应用户交互和上下文的会话搜索代理。该任务还强调决策搜索任务，用户通过筛选数据和信息来进行决策和执行动作。 |
| [^157] | [Locating Cross-Task Sequence Continuation Circuits in Transformers.](http://arxiv.org/abs/2311.04131) | 通过分析和比较Transformer模型中类似的序列继续任务的电路，研究发现共享的计算结构可以提高模型的行为预测能力、错误识别能力和编辑过程的安全性。 |
| [^158] | [BLP 2023 Task 2: Sentiment Analysis.](http://arxiv.org/abs/2310.16183) | BLP 2023任务2是关于情感分析的共享任务，吸引了71个参与者。参与者通过各种方法，包括经典机器学习模型和大型语言模型，提交了597个运行结果。本文提供了任务的详细设置和参与者提交系统的概述。 |
| [^159] | [Large Language Models for In-Context Student Modeling: Synthesizing Student's Behavior in Visual Programming from One-Shot Observation.](http://arxiv.org/abs/2310.10690) | 本研究探索在开放式学习环境中使用大型语言模型进行上下文学生建模，提出了一个新的框架LLM-SS，通过合成学生在不同任务上的尝试，为学生建模提供更准确的预测和教学策略。 |
| [^160] | [In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations.](http://arxiv.org/abs/2310.00313) | 本研究通过神经科学启发的技术，研究了大型语言模型中上下文学习的机制，通过调查嵌入和注意力的变化，揭示了这种改进背后的潜在变化，并提出了用于参数化探测和注意力比率分析的新方法。 |
| [^161] | [DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models.](http://arxiv.org/abs/2309.16292) | DiLu是基于大型语言模型的自动驾驶系统，采用知识驱动方法，通过推理和反思模块进行决策，积累经验并具有显著的泛化能力。 |
| [^162] | [ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs.](http://arxiv.org/abs/2309.13007) | ReConcile是一个通过多轮讨论和投票机制来增强LLM推理能力的多模型多代理框架。 |
| [^163] | [SilverRetriever: Advancing Neural Passage Retrieval for Polish Question Answering.](http://arxiv.org/abs/2309.08469) | SilverRetriever是一个特为波兰语问答系统开发的神经检索器，通过训练在多种数据集上取得了显著的改进效果，并且与更大的多语种模型具有竞争力。 |
| [^164] | [On Large Language Models' Selection Bias in Multi-Choice Questions.](http://arxiv.org/abs/2309.03882) | 本研究发现大型语言模型在多项选择题中存在选择偏差，即倾向于选择特定位置上的选项。研究指出这一偏差的主要原因是选项编号，提出了一种名为PriDe的方法来减轻偏差，并展示了其高精度和稳定性。 |
| [^165] | [Transformers as Support Vector Machines.](http://arxiv.org/abs/2308.16898) | 这项工作建立了自注意力和硬间隔支持向量机问题之间的正式等价关系，通过转换器架构的优化几何来解决自然语言处理问题，同时揭示了梯度下降优化的转换器的隐式偏差。 |
| [^166] | [A Classification-Guided Approach for Adversarial Attacks against Neural Machine Translation.](http://arxiv.org/abs/2308.15246) | 本文介绍了一种基于分类引导的对抗攻击神经机器翻译的方法，通过改变整体意义生成保持语义的对抗样本，从而使得翻译结果属于不同的类别。 |
| [^167] | [Large Language Models on Wikipedia-Style Survey Generation: an Evaluation in NLP Concepts.](http://arxiv.org/abs/2308.10410) | 本研究评估了大型语言模型在自然语言处理领域生成调研文章的效果，发现GPT-4优于GPT-3.5，并且指出了GPT在信息完整性和事实准确性方面的一些缺陷。 |
| [^168] | [Time Travel in LLMs: Tracing Data Contamination in Large Language Models.](http://arxiv.org/abs/2308.08493) | 该论文提出了一种用于识别大型语言模型（LLMs）中数据污染的简单而有效的方法。通过对随机样本中的单个实例进行分析，以及使用“引导指令”来评估整个数据集分区的污染程度，可以准确地识别污染的实例和分区。 |
| [^169] | [TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series.](http://arxiv.org/abs/2308.08241) | 这篇论文总结了两种使用语言模型完成时间序列任务的策略，通过设计一种适用于语言模型的时间序列嵌入方法来激活语言模型对时间序列数据的能力。虽然结果没有明显超越当前最先进的模型，但可以更好地处理时间序列数据。 |
| [^170] | [Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?.](http://arxiv.org/abs/2308.06032) | 本研究探讨了在加密货币证券案件中，大型语言模型（LLMs）是否能够准确判断违法行为，并比较了由LLM和律师撰写的投诉书对陪审团决策的影响。研究发现，目前的LLMs在法律推理方面表现较弱，但随着未来模型的改进，其潜力有望提升。 |
| [^171] | [Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media.](http://arxiv.org/abs/2307.09312) | 多模态讨论变换器 (mDT) 是一个用于检测在线社交网络中仇恨言论的新颖模型。与传统的仅使用文本的方法不同，mDT通过整体分析文本和图像，结合图变换器捕捉评论周围整个讨论的上下文关系，并通过交织融合层将文本和图像嵌入进行组合。研究发现，捕捉对话的整体视图可以极大地提高检测反社会行为的准确性。 |
| [^172] | [Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs.](http://arxiv.org/abs/2305.01938) | 本文提出了 Doc2SoarGraph 框架，利用语义导向分层图结构中元素之间的差异和相关性，在富含视觉表格文本的TAT-DQA问题下实现了离散推理，表现出了最佳的实验结果。 |
| [^173] | [SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support.](http://arxiv.org/abs/2305.00450) | 本研究提出了SMILE方法，使用ChatGPT将公共单轮对话扩展为多轮对话，生成了大规模、多样化、接近真实生活的多轮心理健康支持对话语料库，可用于训练和评估专门的对话系统。 |
| [^174] | [Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models.](http://arxiv.org/abs/2303.16133) | 该研究提出了一个基准数据集COCOCON，并提出度量方法来衡量模型一致性，研究发现现有的最先进系统在不同任务之间表现出高度不一致性。 |
| [^175] | [Evaluating the Role of Target Arguments in Rumour Stance Classification.](http://arxiv.org/abs/2303.12665) | 本文重新评估了目标论据在谣言立场分类中的作用，证明最先进的模型过于依赖表面信号，目标不独立的事件自然高发生率可能导致模型过度拟合。 |
| [^176] | [Underspecification in Language Modeling Tasks: A Causality-Informed Study of Gendered Pronoun Resolution.](http://arxiv.org/abs/2210.00131) | 本研究通过提供一个因果模型，在语言建模任务中探讨了不充分规范化的作用，提出了两种轻量级黑盒评估方法来帮助检测任务的不充分规范化，并在性别代词消解任务中应用这些方法，同时发现了性别与时间、性别与位置之间的虚假相关性。 |

# 详细

[^1]: PALO：一个针对50亿人的多语言多模态模型

    PALO: A Polyglot Large Multimodal Model for 5B People

    [https://arxiv.org/abs/2402.14818](https://arxiv.org/abs/2402.14818)

    该文介绍了一个名为PALO的大型多语种多模态模型，实现了对10种主要语言的视觉推理能力，涵盖了约50亿人口。其通过半自动化翻译方法，将多语言多模态数据集从英语翻译为目标语言，以提升跨多种语言的性能。

    

    本研究旨在推动更具包容性的视觉-语言模型(VLMs)，引入了一个名为\Palo 的大型多语种多模态模型。Palo 在包括英语、中文、印地语、西班牙语、法语、阿拉伯语、孟加拉语、俄语、乌尔都语和日语在内的10种主要语言中提供视觉推理能力，涵盖了约50亿人口（全球人口的65%）。我们采用半自动化翻译方法，利用经过微调的大型语言模型，将多模态指导数据集从英语翻译到目标语言，从而确保了较高的语言保真度，同时由于减少了手动工作，使可扩展性更强。引入多样化的指导集有助于提升跨多种语言的总体性能，特别是对那些欠代表的语言如印地语、阿拉伯语、孟加拉语和乌尔都语。最终的模型在三个规模（17B、70B和130B参数）上进行训练，以展示其泛用性能

    arXiv:2402.14818v1 Announce Type: new  Abstract: In pursuit of more inclusive Vision-Language Models (VLMs), this study introduces a Large Multilingual Multimodal Model called \textsc{Palo}. \textsc{Palo} offers visual reasoning capabilities in 10 major languages, including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese, that span a total of $\sim$5B people (65\% of the world population). Our approach involves a semi-automated translation approach to adapt the multimodal instruction dataset from English to the target languages using a fine-tuned Large Language Model, thereby ensuring high linguistic fidelity while allowing scalability due to minimal manual effort. The incorporation of diverse instruction sets helps us boost overall performance across multiple languages especially those that are underrepresented like Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three scales (1.7B, 7B and 13B parameters) to show the gen
    
[^2]: 微调增强现有机制：实体跟踪案例研究

    Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking

    [https://arxiv.org/abs/2402.14811](https://arxiv.org/abs/2402.14811)

    通过对语言模型进行微调，我们研究了如何影响实体跟踪等内部机制，并发现微调能够在数学任务上实现明显的性能提升。

    

    细化在诸如遵循指令、生成代码和数学等广义任务上已经显示出增强语言模型在一系列任务上的性能。然而，关于这种微调如何影响这些模型中内部计算的解释仍然难以捉摸。我们研究了微调如何影响语言模型中实现的内部机制。作为一个案例研究，我们探讨了实体跟踪的特性，这是语言理解的一个重要方面，在数学上进行了微调的模型在性能上有显著提升。我们识别出了实现实体跟踪的机制，并显示出（i）原始模型和其精细调整版本主要实现实体跟踪的是相同的电路。事实上，原始模型的实体跟踪电路在经过微调的版本上的性能优于完整的原始模型。（ii）所有模型的电路实现大致相同的功能

    arXiv:2402.14811v1 Announce Type: new  Abstract: Fine-tuning on generalized tasks such as instruction following, code generation, and mathematics has been shown to enhance language models' performance on a range of tasks. Nevertheless, explanations of how such fine-tuning influences the internal computations in these models remain elusive. We study how fine-tuning affects the internal mechanisms implemented in language models. As a case study, we explore the property of entity tracking, a crucial facet of language comprehension, where models fine-tuned on mathematics have substantial performance gains. We identify the mechanism that enables entity tracking and show that (i) in both the original model and its fine-tuned versions primarily the same circuit implements entity tracking. In fact, the entity tracking circuit of the original model on the fine-tuned versions performs better than the full original model. (ii) The circuits of all the models implement roughly the same functionalit
    
[^3]: CriticBench：为批判性-正确推理评估LLMs而设计的基准测试

    CriticBench: Benchmarking LLMs for Critique-Correct Reasoning

    [https://arxiv.org/abs/2402.14809](https://arxiv.org/abs/2402.14809)

    CriticBench是一个综合基准测试，旨在评估LLMs在批判和纠正推理方面的能力，发现批判性训练显著提升性能，逻辑任务更易于修正。

    

    大型语言模型（LLMs）批判和完善其推理的能力对于它们在评估、反馈提供和自我改进中的应用至关重要。本文引入了CriticBench，一个旨在评估LLMs在各种任务中批判和纠正其推理能力的综合基准测试。CriticBench包含五个推理领域：数学、常识、符号、编码和算法。它整合了15个数据集，并结合了三个LLM系列的响应。利用CriticBench，我们评估和剖析了17个LLMs在生成、批判和修正推理（即GQC推理）中的表现。我们的研究结果显示：（1）GQC能力呈线性关系，批判性训练显著提升了性能；（2）修正效果在任务上有所不同，以逻辑为导向的任务更容易修正；（3）GQC知识的不一致性。

    arXiv:2402.14809v1 Announce Type: cross  Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsisten
    
[^4]: RelayAttention：用于高效实现大型语言模型服务与长系统提示的论文

    RelayAttention for Efficient Large Language Model Serving with Long System Prompts

    [https://arxiv.org/abs/2402.14808](https://arxiv.org/abs/2402.14808)

    本论文提出的RelayAttention算法旨在改善涉及长系统提示的大型语言模型服务的效率，通过一次性从DRAM读取隐藏状态来消除现有因果注意力算法中的内存访问冗余。

    

    实际的大型语言模型（LLM）服务可能涉及一个长的系统提示，其中包含任务的指示、示例和知识文档，并在许多请求中复用。然而，长系统提示会导致吞吐量/延迟瓶颈，因为生成下一个标记的成本随着序列长度的增长而增加。本文旨在提高涉及长系统提示的LLM服务的效率。我们的关键观察是，处理这些系统提示在现有因果注意力计算算法中需要大量冗余的内存访问。具体来说，对于批量请求，系统提示的缓存隐藏状态（即键-值对）被多次从芯片外的DRAM传输到芯片上的SRAM，每次对应一个单独的请求。为了消除这种冗余，我们提出了RelayAttention，一种注意力算法，允许仅从DRAM读取这些隐藏状态一次。

    arXiv:2402.14808v1 Announce Type: new  Abstract: Practical large language model (LLM) services may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across numerous requests. However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length. This paper aims to improve the efficiency of LLM services that involve long system prompts. Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (i.e., key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once 
    
[^5]: 通过外部评估在大型语言模型中识别多重人格

    Identifying Multiple Personalities in Large Language Models with External Evaluation

    [https://arxiv.org/abs/2402.14805](https://arxiv.org/abs/2402.14805)

    通过外部评估方法研究大型语言模型的人格特征

    

    随着大型语言模型（LLMs）迅速与人类日常应用整合，关于LLMs行为的许多社会和伦理关切被提出。了解LLMs行为的一种方式是分析它们的人格。许多最近的研究使用为人类创建的自我评估测试来量化LLMs的人格。然而，许多批评质疑将这些自我评估测试应用于LLMs时的适用性和可靠性。在本文中，我们使用一种替代的人格测量方法来研究LLMs的人格，我们将其称为外部评估方法，这里我们不是通过在李克特量表上提示LLMs回答多选题，而是通过分析LLMs对外部机器学习模型提出的开放式情境问题的回答来评估LLMs的人格。我们首先对Llama2-7B模型进行微调，作为MBTI人格预测器，该预测器优于最先进的

    arXiv:2402.14805v1 Announce Type: cross  Abstract: As Large Language Models (LLMs) are integrated with human daily applications rapidly, many societal and ethical concerns are raised regarding the behavior of LLMs. One of the ways to comprehend LLMs' behavior is to analyze their personalities. Many recent studies quantify LLMs' personalities using self-assessment tests that are created for humans. Yet many critiques question the applicability and reliability of these self-assessment tests when applied to LLMs. In this paper, we investigate LLM personalities using an alternate personality measurement method, which we refer to as the external evaluation method, where instead of prompting LLMs with multiple-choice questions in the Likert scale, we evaluate LLMs' personalities by analyzing their responses toward open-ended situational questions using an external machine learning model. We first fine-tuned a Llama2-7B model as the MBTI personality predictor that outperforms the state-of-the
    
[^6]: 使用MATH-Vision数据集测量多模态数学推理

    Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset

    [https://arxiv.org/abs/2402.14804](https://arxiv.org/abs/2402.14804)

    提出了MATH-Vision（MATH-V）数据集，用于评估大型多模态模型（LMMs）的数学推理能力，通过实验证实了当前LMMs和人类在MATH-V上的表现差距。

    

    大型多模态模型（LMMs）的最新进展在视觉背景下的数学推理方面显示出令人鼓舞的结果，这些模型在现有基准测试（如MathVista）上接近人类水平的表现。然而，我们观察到这些基准测试在问题多样性和涵盖学科范围方面存在显着局限性。为了解决这一问题，我们提出了MATH-Vision（MATH-V）数据集，这是一个精心策划的收集了来自真实数学竞赛的3,040个高质量数学问题和视觉背景的数据集。跨越16个不同的数学学科，分为5个难度级别进行评分，我们的数据集为评估LMMs的数学推理能力提供了一套全面且多样化的挑战。通过广泛的实验，我们揭示了当前LMMs与MATH-V上人类表现之间的显著表现差距，并强调了进一步推进的必要性。

    arXiv:2402.14804v1 Announce Type: cross  Abstract: Recent advancements in Large Multimodal Models (LMMs) have shown promising results in mathematical reasoning within visual contexts, with models approaching human-level performance on existing benchmarks such as MathVista. However, we observe significant limitations in the diversity of questions and breadth of subjects covered by these benchmarks. To address this issue, we present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions. Spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating the mathematical reasoning abilities of LMMs. Through extensive experimentation, we unveil a notable performance gap between current LMMs and human performance on MATH-V, underscoring the imperative for further advancements i
    
[^7]: 并非所有专家都相等: 混合专家大型语言模型的高效专家修剪和跳过

    Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models

    [https://arxiv.org/abs/2402.14800](https://arxiv.org/abs/2402.14800)

    引入了专家级稀疏化技术，提出了专家修剪和跳过的后训练方法，以提高MoE LLMs的部署效率，同时保持模型性能。

    

    大型语言模型（LLMs）进展中的一个重要进展是混合专家（MoE）LLMs的出现。与传统的LLMs相比，MoE LLMs可以在更少的参数下实现更高的性能，但由于其巨大的参数大小，仍然很难部署它们。与先前依赖于专门设计的硬件的权重剪枝方法不同，本文主要旨在通过引入即插即用的专家级稀疏化技术来提高MoE LLMs的部署效率。具体而言，我们首次提出了针对任务不可知和任务特定的MoE LLMs专家修剪和跳过的后训练方法，旨在提高在广泛任务范围内保持模型性能的同时提高部署效率。大量实验证明，我们提出的方法可以同时减小模型大小并增加推断速度，同时保持饱和

    arXiv:2402.14800v1 Announce Type: cross  Abstract: A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining sat
    
[^8]: 利用非正式逻辑增强系统化分解的自然语言推理

    Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic

    [https://arxiv.org/abs/2402.14798](https://arxiv.org/abs/2402.14798)

    本文提出了一种一致且在理论上有根据的方法来注释分解蕴涵数据集，形成RDTE数据集，该数据集在解决何为有效的组合蕴涵的问题上有显著进展。

    

    当代语言模型为使用文本进行结构化推理提供了新的机会，例如在不依赖脆弱的形式逻辑的情况下构建和评估直观的、类似证明的文本蕴涵树。然而，沿着这个方向的进展受到一个长期以来缺乏明确的确定何为有效的组合蕴涵的清晰协议的阻碍。本文提出了一个一致且在理论上有根据的方法来注释分解蕴涵数据集，并评估其对基于LLM的文本推理的影响。我们发现，我们的结果数据集RDTE (Recognizing Decompositional Textual Entailment) 的内部一致性比先前的分解蕴涵数据集高得多（+9%），表明RDTE在长期存在的关于何为有效的组合蕴涵的问题上是一个重要的进步。

    arXiv:2402.14798v1 Announce Type: cross  Abstract: Contemporary language models enable new opportunities for structured reasoning with text, such as the construction and evaluation of intuitive, proof-like textual entailment trees without relying on brittle formal logic. However, progress in this direction has been hampered by a long-standing lack of a clear protocol for determining what valid compositional entailment is. This absence causes noisy datasets and limited performance gains by modern neuro-symbolic engines. To address these problems, we formulate a consistent and theoretically grounded approach to annotating decompositional entailment datasets, and evaluate its impact on LLM-based textual inference. We find that our resulting dataset, RDTE (Recognizing Decompositional Textual Entailment), has a substantially higher internal consistency (+9%) than prior decompositional entailment datasets, suggesting that RDTE is a significant step forward in the long-standing problem of for
    
[^9]: 大型语言模型指令微调中的零次跨语言转移

    Zero-shot cross-lingual transfer in instruction tuning of large language model

    [https://arxiv.org/abs/2402.14778](https://arxiv.org/abs/2402.14778)

    本研究探讨了大型语言模型在指令微调中的零次跨语言转移，发现在适当超参数调整和足够大的数据支持下，英语训练的模型能够成功生成其他语言的准确、有用回应，但存在事实准确性和流畅性错误。

    

    指令微调（IT）被广泛用于教导预训练的大型语言模型（LLMs）遵循任意指令，但在多语言环境下尚未得到充分研究。本研究系统地研究了在IT中的零次跨语言转移，当LLM在仅英语数据上进行指令微调然后在其他语言用户提示上进行测试时。我们调查了模型配置选择的影响，并设计了一种多方面评估策略用于多语言指令遵循。我们发现即使模型训练的所有阶段都以英语为中心，跨语言转移在IT中也会成功发生，但只有在超参数调整中考虑到多语言性以及有足够大的IT数据时才会发生。经过英语训练的LLMs能够在其他语言中生成准确、全面且有帮助的回应，但缺乏事实准确性，并且偶尔可能存在流畅性错误。

    arXiv:2402.14778v1 Announce Type: cross  Abstract: Instruction tuning (IT) is widely used to teach pretrained large language models (LLMs) to follow arbitrary instructions, but is under-studied in multilingual settings. In this work, we conduct a systematic study of zero-shot cross-lingual transfer in IT, when an LLM is instruction-tuned on English-only data and then tested on user prompts in other languages. We investigate the influence of model configuration choices and devise a multi-facet evaluation strategy for multilingual instruction following. We find that cross-lingual transfer does happen successfully in IT even if all stages of model training are English-centric, but only if multiliguality is taken into account in hyperparameter tuning and with large enough IT data. English-trained LLMs are capable of generating correct-language, comprehensive and helpful responses in the other languages, but suffer from low factuality and may occasionally have fluency errors.
    
[^10]: 2D Matryoshka句子嵌入

    2D Matryoshka Sentence Embeddings

    [https://arxiv.org/abs/2402.14776](https://arxiv.org/abs/2402.14776)

    Matryoshka表示学习(MRL)以更细粒度地编码信息，以适应临时任务，同时实现了更小的嵌入大小，从而加快了下游任务的速度。

    

    arXiv:2402.14776v1 公告类型：新  摘要：常见方法依赖于从语言模型中获得的固定长度的嵌入向量作为句子嵌入，用于语义文本相似性（STS）等下游任务。由于在各种应用程序中存在未知的计算约束和预算，这些方法在灵活性上受到限制。Matryoshka表示学习(MRL)(Kusupati等人，2022)以更细粒度地编码信息，即使用较低的嵌入维度，以自适应地适应临时任务。可以通过较小的嵌入大小达到类似的准确性，从而加快下游任务。尽管其改进了效率，MRL仍要在获得嵌入之前遍历所有Transformer层，这仍然是时间和内存消耗的主要因素。这引发了是否固定数量的Transformer层会影响表示质量以及使用中间层进行句子表示是否可行的考虑。

    arXiv:2402.14776v1 Announce Type: new  Abstract: Common approaches rely on fixed-length embedding vectors from language models as sentence embeddings for downstream tasks such as semantic textual similarity (STS). Such methods are limited in their flexibility due to unknown computational constraints and budgets across various applications. Matryoshka Representation Learning (MRL) (Kusupati et al., 2022) encodes information at finer granularities, i.e., with lower embedding dimensions, to adaptively accommodate ad hoc tasks. Similar accuracy can be achieved with a smaller embedding size, leading to speedups in downstream tasks. Despite its improved efficiency, MRL still requires traversing all Transformer layers before obtaining the embedding, which remains the dominant factor in time and memory consumption. This prompts consideration of whether the fixed number of Transformer layers affects representation quality and whether using intermediate layers for sentence representation is feas
    
[^11]: MT-Bench-101: 用于评估大型语言模型在多轮对话中的细粒度基准

    MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues

    [https://arxiv.org/abs/2402.14762](https://arxiv.org/abs/2402.14762)

    提出了MT-Bench-101用于评估大型语言模型在多轮对话中的细粒度能力，构建了包含4208轮对话数据的三级分层能力分类，并评估了21种流行的语言模型，发现它们在不同对话轮次中表现出不同的趋势。

    

    大型语言模型（LLMs）的出现大大增强了对话系统。然而，全面评估LLMs的对话能力仍然是一个挑战。以往的基准主要集中在单轮对话或者提供粗粒度和不完整的多轮对话评估，忽视了真实对话的复杂性和细微的差异。为了解决这个问题，我们引入了MT-Bench-101，专门设计用于评估LLMs在多轮对话中的细粒度能力。通过对真实多轮对话数据进行详细分析，我们构建了一个包含13个不同任务中1388个多轮对话中的4208轮的三级分层能力分类。然后我们基于MT-Bench-101评估了21个流行的LLMs，从能力和任务两个角度进行全面分析，并观察到LLMs在对话轮次中表现出不同的趋势。

    arXiv:2402.14762v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single-turn dialogues or provided coarse-grained and incomplete assessments of multi-turn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues. To address this issue, we introduce MT-Bench-101, specifically designed to evaluate the fine-grained abilities of LLMs in multi-turn dialogues. By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns with
    
[^12]: 泛化奖励建模用于超出分布偏好学习

    Generalizing Reward Modeling for Out-of-Distribution Preference Learning

    [https://arxiv.org/abs/2402.14760](https://arxiv.org/abs/2402.14760)

    通过元学习方法优化通用奖励模型，以解决超出分布偏好学习问题，并提高LLMs在有限偏好反馈下的泛化能力

    

    偏好学习(PL)结合大型语言模型(LLMs)旨在使LLMs生成与人类偏好一致。以往有关从人类反馈中学习的强化学习(RLHF)的研究已在分布内的PL中取得了良好结果。然而，由于获取人类反馈的难度，为每个遇到的分布离散训练奖励模型是具有挑战性的。因此，在超出分布(OOD) PL中通过优化通用奖励模型来增强LLMs有限偏好反馈的泛化能力是实用的。本研究通过元学习方法来解决OOD PL问题。在元训练期间，利用双层优化算法来学习一个能够引导策略学习以使之与人类偏好一致的奖励模型。在遇到测试分布时，元测试过程使用学习到的奖励模型进行正则化策略优化。

    arXiv:2402.14760v1 Announce Type: cross  Abstract: Preference learning (PL) with large language models (LLMs) aims to align the LLMs' generations with human preferences. Previous work on reinforcement learning from human feedback (RLHF) has demonstrated promising results in in-distribution PL. However, due to the difficulty of obtaining human feedback, discretely training reward models for every encountered distribution is challenging. Thus, out-of-distribution (OOD) PL is practically useful for enhancing the generalization ability of LLMs with limited preference feedback. This work addresses OOD PL by optimizing a general reward model through a meta-learning approach. During meta-training, a bilevel optimization algorithm is utilized to learn a reward model capable of guiding policy learning to align with human preferences across various distributions. When encountering a test distribution, the meta-test procedure conducts regularized policy optimization using the learned reward model
    
[^13]: 扩展高效的LLM模型

    Scaling Efficient LLMs

    [https://arxiv.org/abs/2402.14746](https://arxiv.org/abs/2402.14746)

    训练得到的LLM模型通常是稀疏的，为了提高效率，研究了在训练语料上达到所需准确度的参数最少的高效LLM模型，得出了参数数量与自然训练语料规模之间的关系，并指出扩展可以揭示新技能。

    

    训练得到的LLM模型通常是稀疏的，即大部分参数为零，这引发了关于效率的问题。为此，我们研究了高效的LLM模型，即那些在训练语料上达到所需准确度的参数最少。具体地，我们比较了当前规模下训练损失的理论和实证估计，以获得自然训练语料中独特序列数量上下界的数量。我们的结果暗示：(1)要在训练语料中表示的技能数量翻倍，需要将语料规模大约扩展三到五倍，(2)对于高效的LLM模型，参数数量$N$和自然训练语料规模$D$满足$N \sim D^{0.58}$的关系，(3)如果一个LLM模型的参数数量小于训练语料中的独特序列数量，扩展可以揭示出新的技能。

    arXiv:2402.14746v1 Announce Type: new  Abstract: Trained LLMs are typically sparse in that most of the parameters are zero, raising questions on efficiency. In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, we compare theoretical and empirical estimates for training loss at current scale to obtain upper and lower bounds on the number of unique sequences in a natural training corpus as a function of its size. Our result implies (1) to double the number of skills represented in a training corpus, the corpus must scale roughly between three and five fold (2) for efficient LLMs, the number of parameters $N$ and the size $D$ of a natural training corpus scale as $N \sim D^{0.58}$ (3) if the number of parameters of an LLM is smaller than the number of unique sequences in the training corpus, scaling up can uncover emergent skills.
    
[^14]: 大型语言模型作为城市居民：用于个人移动生成的LLM代理框架

    Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation

    [https://arxiv.org/abs/2402.14744](https://arxiv.org/abs/2402.14744)

    提出了一种将大型语言模型LLMs整合到代理框架中的新方法，用于生成个人移动生成，重点是解决将LLMs与真实城市流动数据对齐的问题，并提出了一种自洽方法和检索增强策略来实现可解释活动生成。

    

    本文介绍了一种新方法，将大型语言模型(LLMs)集成到代理框架中，用于灵活高效的个人移动生成。LLMs通过高效处理语义数据并在建模各种任务中提供多功能性, 克服了以往模型的局限性。我们的方法解决了将LLMs与真实世界城市流动数据对齐的迫切需求, 重点关注三个研究问题: 将LLMs与丰富的活动数据对齐, 开发可靠的活动生成策略, 以及探索LLMs在城市移动中的应用。其关键技术贡献是一种新颖的LLM代理框架, 该框架考虑了个体活动模式和动机, 包括将LLMs与真实世界活动数据对齐的自洽方法和可解释活动生成的检索增强策略。在实验研究中, 使用真实世界数据进行了全面验证。

    arXiv:2402.14744v1 Announce Type: new  Abstract: This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and efficient personal mobility generation. LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks. Our approach addresses the critical need to align LLMs with real-world urban mobility data, focusing on three research questions: aligning LLMs with rich activity data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. In experimental studies, comprehensive validation is performed using real-world data. This 
    
[^15]: 使用多语言BERT对奥斯曼土耳其语进行依存标注

    Dependency Annotation of Ottoman Turkish with Multilingual BERT

    [https://arxiv.org/abs/2402.14743](https://arxiv.org/abs/2402.14743)

    使用多语言BERT对奥斯曼土耳其语进行依存标注，加速并简化依存标注过程，将产生的树库有助于奥斯曼土耳其语文档的自动分析。

    

    这项研究介绍了一种基于预训练大型语言模型的标注方法，用于第一个奥斯曼土耳其语依存树库。我们的实验结果表明，通过迭代地i）使用基于多语言BERT的解析模型进行伪标注数据，ii）手动修正伪标注，以及iii）用修正的标注微调解析模型，我们加快并简化了具有挑战性的依存标注过程。最终产生的树库，将成为通用依存关系（UD）项目的一部分，将便利奥斯曼土耳其语文档的自动分析，揭示了这一历史遗产中蕴含的语言丰富性。

    arXiv:2402.14743v1 Announce Type: new  Abstract: This study introduces a pretrained large language model-based annotation methodology for the first dependency treebank in Ottoman Turkish. Our experimental results show that, iteratively, i) pseudo-annotating data using a multilingual BERT-based parsing model, ii) manually correcting the pseudo-annotations, and iii) fine-tuning the parsing model with the corrected annotations, we speed up and simplify the challenging dependency annotation process. The resulting treebank, that will be a part of the Universal Dependencies (UD) project, will facilitate automated analysis of Ottoman Turkish documents, unlocking the linguistic richness embedded in this historical heritage.
    
[^16]: 高效有效的词汇扩展方法在多语言大型语言模型中的应用

    Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models

    [https://arxiv.org/abs/2402.14714](https://arxiv.org/abs/2402.14714)

    提出了一种高效且有效的词汇扩展方法（EEVE），可以显著提升非英语语言模型的性能，使得其在韩文文本理解方面表现出色。

    

    这篇报告介绍了\texttt{EEVE-Korean-v1.0}，这是大型语言模型的韩文适配版本，展现出在英文和韩文文本理解方面的显著能力。我们提出了一种高效且有效的词汇扩展方法（EEVE），包括参数冻结和子词初始化。与先前认为新嵌入需要上万亿训练标记的努力相反，我们展示了我们的方法可以在仅20亿标记内显着提升非英语熟练度。截至2024年1月，我们的模型\texttt{EEVE-Korean-10.8B-v1.0}在Open Ko-LLM榜单上超越了大多数经过指导调整的LLMs，成为了开源社区中表现最好的韩文预训练模型，根据Hugging Face的排行榜。

    arXiv:2402.14714v1 Announce Type: cross  Abstract: This report introduces \texttt{EEVE-Korean-v1.0}, a Korean adaptation of large language models that exhibit remarkable capabilities across English and Korean text understanding. Building on recent highly capable but English-centric LLMs, such as SOLAR-10.7B and Phi-2, where non-English texts are inefficiently processed with English-centric tokenizers, we present an efficient and effective vocabulary expansion (EEVE) method, which encompasses parameter freezing and subword initialization. In contrast to previous efforts that believe new embeddings require trillions of training tokens, we show that our method can significantly boost non-English proficiency within just 2 billion tokens. Surpassing most instruction-tuned LLMs on the Open Ko-LLM Leaderboard, as of January 2024, our model \texttt{EEVE-Korean-10.8B-v1.0} ranks as the leading Korean pre-trained model in the open-source community, according to Hugging Face's leaderboard. We ope
    
[^17]: IEPile: 挖掘大规模基于模式的信息抽取语料库

    IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus

    [https://arxiv.org/abs/2402.14710](https://arxiv.org/abs/2402.14710)

    发布了IEPile，一个包含约0.32B个标记的综合双语IE指令语料库，通过收集和清理33个现有IE数据集并引入基于模式的指令生成，可以提高大型语言模型在信息抽取领域的性能，尤其是零样本泛化。

    

    大型语言模型（LLMs）在各个领域展现出了显著的潜力；然而，在信息抽取（IE）方面表现出了显著的性能差距。高质量的指令数据是提升LLMs特定能力的关键，而当前的IE数据集往往规模较小、分散且缺乏标准化的模式。因此，我们介绍了IEPile，一个综合的双语（英文和中文）IE指令语料库，包含约0.32B个标记。我们通过收集和清理33个现有IE数据集构建IEPile，并引入基于模式的指令生成来挖掘大规模语料库。在LLaMA和Baichuan上的实验结果表明，使用IEPile可以提高LLMs在IE方面的性能，尤其是零样本泛化。我们开源了资源和预训练模型，希望为自然语言处理社区提供有价值的支持。

    arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
    
[^18]: 一种LLM增强的词汇简化对抗编辑系统

    An LLM-Enhanced Adversarial Editing System for Lexical Simplification

    [https://arxiv.org/abs/2402.14704](https://arxiv.org/abs/2402.14704)

    该论文提出了一种新颖的词汇简化方法，不需要平行语料库，在原始句子中预测词汇修改，引入LLM增强损失进行知识提炼，并采用基于难度感知的填充模块将复杂词替换为简单词，实验证明方法的有效性。

    

    词汇简化（LS）旨在在词汇级别简化文本。现有方法严重依赖标注数据，这使得在资源匮乏的情况下难以应用。在本文中，我们提出了一种新颖的LS方法，不需要平行语料库。该方法采用对抗编辑系统，并结合混淆损失和不变性损失来预测原始句子中的词汇修改。同时，我们引入了一种创新的LLM增强损失，以将大型语言模型（LLMs）的知识提炼成小型LS系统。通过这种方式，句子中的复杂词被屏蔽，制作了一个基于难度感知的填充模块，用更简单的词替换屏蔽位置。最后，对三个基准LS数据集进行了广泛的实验结果和分析，证明了我们提出方法的有效性。

    arXiv:2402.14704v1 Announce Type: new  Abstract: Lexical Simplification (LS) aims to simplify text at the lexical level. Existing methods rely heavily on annotated data, making it challenging to apply in low-resource scenarios. In this paper, we propose a novel LS method without parallel corpora. This method employs an Adversarial Editing System with guidance from a confusion loss and an invariance loss to predict lexical edits in the original sentences. Meanwhile, we introduce an innovative LLM-enhanced loss to enable the distillation of knowledge from Large Language Models (LLMs) into a small-size LS system. From that, complex words within sentences are masked and a Difficulty-aware Filling module is crafted to replace masked positions with simpler words. At last, extensive experimental results and analyses on three benchmark LS datasets demonstrate the effectiveness of our proposed method.
    
[^19]: InfFeed：将影响函数作为反馈以改善主观任务的表现

    InfFeed: Influence Functions as a Feedback to Improve the Performance of Subjective Tasks

    [https://arxiv.org/abs/2402.14702](https://arxiv.org/abs/2402.14702)

    InfFeed将影响函数作为反馈，用于改善主观任务表现，并通过自动识别需要交叉检查的数据点以提高模型性能，在调整标签方面优于现有基线。

    

    最近，影响函数提供了一种工具，通过量化可能影响测试预测的个别训练实例的扰动，实现对深度神经模型的可解释性。本文的目标是双重的。首先，我们将影响函数作为反馈引入模型以改善其性能。其次，在数据集扩展练习中，使用影响函数自动识别最初由某些现有方法‘银’注释的数据点，并需要标注者交叉检查（和纠正）以改善模型性能。为了实现这些目标，本文引入了InfFeed，该方法使用影响函数计算目标实例的有影响力的实例。向第一个目标努力，我们根据其影响者的标签调整目标实例的标签。在此过程中，InfFeed的表现优于现有基线（包括LLMs）。

    arXiv:2402.14702v1 Announce Type: new  Abstract: Recently, influence functions present an apparatus for achieving explainability for deep neural models by quantifying the perturbation of individual train instances that might impact a test prediction. Our objectives in this paper are twofold. First we incorporate influence functions as a feedback into the model to improve its performance. Second, in a dataset extension exercise, using influence functions to automatically identify data points that have been initially `silver' annotated by some existing method and need to be cross-checked (and corrected) by annotators to improve the model performance. To meet these objectives, in this paper, we introduce InfFeed, which uses influence functions to compute the influential instances for a target instance. Toward the first objective, we adjust the label of the target instance based on its influencer(s) label. In doing this, InfFeed outperforms the state-of-the-art baselines (including LLMs) b
    
[^20]: COMPASS：利用语言建模对患者-治疗师联盟策略进行计算映射

    COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling

    [https://arxiv.org/abs/2402.14701](https://arxiv.org/abs/2402.14701)

    本文提出了一种名为COMPASS的新框架，通过分析心理治疗会话中的自然语言，直接推断治疗工作联盟，为临床精神病学提供了可解释性，并在识别与正在治疗的疾病相关的新兴模式方面发挥作用。

    

    治疗工作联盟是预测心理治疗治疗成功的关键因素。传统上，工作联盟评估依赖于治疗师和患者填写的问卷。本文提出了COMPASS，一个新颖的框架，可直接从心理治疗课程中使用的自然语言中推断治疗工作联盟。我们的方法利用先进的大型语言模型分析心理治疗会话的转录，并将其与工作联盟清单中陈述的分布式表示进行比较。通过分析涵盖多种精神疾病的超过950个会话的数据集，我们展示了我们的方法在显微地映射患者-治疗师对齐轨迹方面的有效性，并为临床精神病学提供解释性，并在识别与正在治疗的疾病相关的新兴模式方面提供可解释性。通过使用各种神经主题模式

    arXiv:2402.14701v1 Announce Type: cross  Abstract: The therapeutic working alliance is a critical factor in predicting the success of psychotherapy treatment. Traditionally, working alliance assessment relies on questionnaires completed by both therapists and patients. In this paper, we present COMPASS, a novel framework to directly infer the therapeutic working alliance from the natural language used in psychotherapy sessions. Our approach utilizes advanced large language models to analyze transcripts of psychotherapy sessions and compare them with distributed representations of statements in the working alliance inventory. Analyzing a dataset of over 950 sessions covering diverse psychiatric conditions, we demonstrate the effectiveness of our method in microscopically mapping patient-therapist alignment trajectories and providing interpretability for clinical psychiatry and in identifying emerging patterns related to the condition being treated. By employing various neural topic mode
    
[^21]: 揭示大型语言模型中的语言区域

    Unveiling Linguistic Regions in Large Language Models

    [https://arxiv.org/abs/2402.14700](https://arxiv.org/abs/2402.14700)

    本文从区域划分的角度出发，发现了大型语言模型中对应语言能力的核心区域，并展示了去除该区域会导致跨30种不同语言的显著性能下降。

    

    大型语言模型(LLMs)已经展示了相当大的跨语言对齐和泛化能力。当前研究主要集中在改善LLMs的跨语言泛化能力上。然而，关于LLMs如何实现跨语言对齐的内在机制仍然缺乏研究。从区域划分的角度出发，本文在LLMs的语言能力上进行了几项调查。我们发现LLMs中的一个核心区域对应于语言能力，大约占总模型参数的1%。通过将参数设置为零来去除这个核心区域，会导致30种不同语言的显著性能下降。此外，这个核心区域表现出显著的维度依赖性，对特定维度上的单个参数的扰动会导致语言能力的丧失。此外，我们发现独特的区域...

    arXiv:2402.14700v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs' cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependency, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct regions 
    
[^22]: UFO：评估大型语言模型事实性的统一灵活框架

    UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models

    [https://arxiv.org/abs/2402.14690](https://arxiv.org/abs/2402.14690)

    提出了一种基于大型语言模型的统一灵活评估框架UFO，用于验证事实并支持即插即用的事实来源。

    

    大型语言模型（LLMs）可能生成与人类知识不一致的文本，导致事实不准确或“幻觉”。现有研究评估LLMs的事实性涉及使用LLM提取事实主张，并将其与预定义的事实来源进行验证。然而，这些评估指标是任务特定的，并且不具有可扩展性，不同任务中事实来源的可替代性尚未得到充分探讨。为解决这些挑战，我们将四种可用事实来源进行分类：人类编写的证据、参考文献、搜索引擎结果和LLM知识，以及包含六个典型数据集的五个文本生成任务。然后，我们提出了\texttt{UFO}，一种基于LLM的统一灵活评估框架，用于根据即插即用的事实来源验证事实。我们基于该框架实施了五个评估场景。实验证明，在大多数问答任务中，人类

    arXiv:2402.14690v1 Announce Type: new  Abstract: Large language models (LLMs) may generate text that lacks consistency with human knowledge, leading to factual inaccuracies or \textit{hallucination}. Existing research for evaluating the factuality of LLMs involves extracting fact claims using an LLM and verifying them against a predefined fact source. However, these evaluation metrics are task-specific, and not scalable, and the substitutability of fact sources in different tasks is under-explored. To address these challenges, we categorize four available fact sources: human-written evidence, reference documents, search engine results, and LLM knowledge, along with five text generation tasks containing six representative datasets. Then, we propose \texttt{UFO}, an LLM-based unified and flexible evaluation framework to verify facts against plug-and-play fact sources. We implement five evaluation scenarios based on this framework. Experimental results show that for most QA tasks, human-w
    
[^23]: 认知与行为一致还是不一致：研究大型语言模型的个性

    Is Cognition and Action Consistent or Not: Investigating Large Language Model's Personality

    [https://arxiv.org/abs/2402.14679](https://arxiv.org/abs/2402.14679)

    通过评估大型语言模型在表达人类个性特征方面的可靠性，研究认知与行为之间的一致性，以及提出对观察结果的心理理论和指标假设

    

    在这项研究中，我们通过回答人格问卷调查来探讨大型语言模型（LLMs）在表达类人个性特征方面的可靠性。我们的目标是评估LLMs所表达的个性倾向与它们实际“行为”之间的一致性，检验这些模型能够模拟类人个性模式的程度。通过全面分析LLM输出与已建立的人类基准之间的对比，我们试图了解LLMs中认知与行为之间的差异，并根据心理理论和指标对观察结果提出假设。

    arXiv:2402.14679v1 Announce Type: new  Abstract: In this study, we investigate the reliability of Large Language Models (LLMs) in professing human-like personality traits through responses to personality questionnaires. Our goal is to evaluate the consistency between LLMs' professed personality inclinations and their actual "behavior", examining the extent to which these models can emulate human-like personality patterns. Through a comprehensive analysis of LLM outputs against established human benchmarks, we seek to understand the cognition-action divergence in LLMs and propose hypotheses for the observed results based on psychological theories and metrics.
    
[^24]: 语言中间件：工具在复杂环境中对语言代理至关重要

    Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments

    [https://arxiv.org/abs/2402.14672](https://arxiv.org/abs/2402.14672)

    这项研究探索了在复杂环境中利用工具增强大型语言模型的潜力，设计了定制化工具来辅助语言代理在庞大环境中进行探索，并展示了在知识库和数据库等复杂环境中，借助工具增强语言代理的重要潜力。

    

    大型语言模型（LLMs）的应用已经远远超出了文本处理的范围，预示着一个新时代的到来，在这个时代，LLMs被设想为能够在复杂现实环境中运行的通用语言代理。这些环境通常非常广阔，使得LLM不可能在其短期记忆中处理它们。受最近关于通过工具扩展LLMs能力的研究启发，本文探讨了工具在增强LLMs处理这种复杂性方面的潜力。为此，我们设计了定制工具，以协助在这些庞大环境中进行主动探索。这些工具可以作为一个中间件层，使LLM免受环境复杂性的影响。在两个代表性的复杂环境--知识库（KBs）和数据库中，我们展示了在复杂环境中使用工具增强语言代理的重要潜力。

    arXiv:2402.14672v1 Announce Type: cross  Abstract: The applications of large language models (LLMs) have expanded well beyond the confines of text processing, signaling a new era where LLMs are envisioned as generalist language agents capable of operating within complex real-world environments. These environments are often highly expansive, making it impossible for the LLM to process them within its short-term memory. Motivated by recent research on extending the capabilities of LLMs with tools, this paper investigates the intriguing potential of tools to augment LLMs in handling such complexity. To this end, we design customized tools to aid in the proactive exploration within these massive environments. Such tools can serve as a middleware layer shielding the LLM from environmental complexity. In two representative complex environments -- knowledge bases (KBs) and databases -- we demonstrate the significant potential of augmenting language agents with tools in complex environments. N
    
[^25]: ConceptMath：用于衡量大型语言模型数学推理能力的双语概念评测基准

    ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models

    [https://arxiv.org/abs/2402.14660](https://arxiv.org/abs/2402.14660)

    介绍了ConceptMath，一种双语的细粒度基准测试，用于评估大型语言模型的概念性数学推理能力，并发现现有模型在不同数学概念上存在显著性能差异，甚至可能在最基本的概念上出现失败。

    

    本文介绍了ConceptMath，这是一个双语（英语和中文），细粒度的基准测试，用来评估大型语言模型（LLMs）的概念性数学推理能力。与评估一般数学推理的传统基准不同，ConceptMath将数学问题系统地组织在数学概念的层次结构下，从而可以以概念为单位准确性评估数学推理。基于我们的ConceptMath，我们评估了广泛范围的LLMs，并观察到现有的LLMs尽管在传统基准上取得了高平均准确性，但在不同数学概念上表现出显著的性能差异，甚至可能在最基本的概念上出现严重失败。此外，我们还介绍了一种有效的微调策略来增强现有LLMs的弱点。最后，我们希望ConceptMath能够指导开发者理解细致的数学推理能力。

    arXiv:2402.14660v1 Announce Type: cross  Abstract: This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs). Unlike traditional benchmarks that evaluate general mathematical reasoning with an average accuracy, ConceptMath systematically organizes math problems under a hierarchy of math concepts, so that mathematical reasoning can be evaluated at different granularity with concept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range of LLMs, and we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones. Besides, we also introduce an efficient fine-tuning strategy to enhance the weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the developers to understand the fine-grai
    
[^26]: OpenCodeInterpreter：集成代码生成、执行和细化

    OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement

    [https://arxiv.org/abs/2402.14658](https://arxiv.org/abs/2402.14658)

    OpenCodeInterpreter是一种开源代码系统，集成了执行、人类反馈和动态代码细化的功能，并在关键基准测试中表现出色，甚至与GPT-4相媲美。

    

    大型语言模型的引入显著推动了代码生成的发展。然而，开源模型通常缺乏类似GPT-4 Code Interpreter这样的高级系统的执行能力和迭代细化能力。为了解决这一问题，我们介绍了OpenCodeInterpreter，这是一族旨在生成、执行和迭代细化代码的开源代码系统。通过Code-Feedback支持，该系统集成了执行和人类反馈，用于动态代码细化。我们对OpenCodeInterpreter在诸如HumanEval、MBPP以及它们来自EvalPlus的增强版本等关键基准上进行了全面评估，证实了其出色的性能。值得注意的是，OpenCodeInterpreter-33B在HumanEval和MBPP的平均值（以及其增强版本）上取得了83.2（76.4）的准确率，与GPT-4的84.2（76.2）紧密匹敌，并且通过合成hum

    arXiv:2402.14658v1 Announce Type: cross  Abstract: The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized hum
    
[^27]: 使用神经网络网络抓取进行更清洁的预训练语料库筛选

    Cleaner Pretraining Corpus Curation with Neural Web Scraping

    [https://arxiv.org/abs/2402.14652](https://arxiv.org/abs/2402.14652)

    使用神经网络网络抓取器NeuScraper可以从网页中提取干净的文本内容，并且实现了超过20%的改进，有助于提高语言模型的预训练质量

    

    arXiv:2402.14652v1 公告类型: 新的 摘要: 网络包含大规模、多样化和丰富信息，以满足人类的信息需求。通过细致的数据收集、预处理和整理，网页可以被用作语言模型预训练的基本数据资源。然而，面对不断革新和复杂的网页特性，基于规则/特征的网络抓取器变得越来越不足够。本文提出了一个简单、快速、有效的神经网络网络抓取器（NeuScraper），帮助从网页中提取主要和干净的文本内容。实验结果显示NeuScraper超越了基准抓取器，实现了超过20%的改进，展示了其在提取更高质量数据以促进语言模型预训练方面的潜力。所有代码都可以在https://github.com/OpenMatch/NeuScraper找到。

    arXiv:2402.14652v1 Announce Type: new  Abstract: The web contains large-scale, diverse, and abundant information to satisfy the information-seeking needs of humans. Through meticulous data collection, preprocessing, and curation, webpages can be used as a fundamental data resource for language model pretraining. However, when confronted with the progressively revolutionized and intricate nature of webpages, rule-based/feature-based web scrapers are becoming increasingly inadequate. This paper presents a simple, fast, and effective Neural web Scraper (NeuScraper) to help extract primary and clean text contents from webpages. Experimental results show that NeuScraper surpasses the baseline scrapers by achieving more than a 20% improvement, demonstrating its potential in extracting higher-quality data to facilitate the language model pretraining. All of the code is available at https://github.com/OpenMatch/NeuScraper.
    
[^28]: RoboScript: 跨越真实和仿真的自由形式操作任务的代码生成

    RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation

    [https://arxiv.org/abs/2402.14623](https://arxiv.org/abs/2402.14623)

    RoboScript是一个旨在填补“理想到实际”差距的平台，提供可部署的机器人操作流水线，并为自然语言中的机器人操作任务提供代码生成基准。

    

    在具身人工智能领域，高级任务规划和代码生成在开放世界机器人操作中取得了快速进展。然而，先前的研究主要集中在大规模语言或多模态模型的常识推理和任务规划能力上，对于生成代码在实际机器人和其他自主机器人系统基本组件（包括机器人感知、运动规划和控制）上的部署性付出的努力相对较少。为了弥合这种“理想到实际”的差距，本文提出了RoboScript，一个平台，用于1）由代码生成驱动的可部署机器人操作流水线；和2）自由形式自然语言中机器人操作任务的代码生成基准。RoboScript平台通过强调与仿真和真实机器人的统一接口解决了这一差距，基于对机器人操作系统（ROS）的抽象实现。

    arXiv:2402.14623v1 Announce Type: cross  Abstract: Rapid progress in high-level task planning and code generation for open-world robot manipulation has been witnessed in Embodied AI. However, previous studies put much effort into general common sense reasoning and task planning capabilities of large-scale language or multi-modal models, relatively little effort on ensuring the deployability of generated code on real robots, and other fundamental components of autonomous robot systems including robot perception, motion planning, and control. To bridge this ``ideal-to-real'' gap, this paper presents \textbf{RobotScript}, a platform for 1) a deployable robot manipulation pipeline powered by code generation; and 2) a code generation benchmark for robot manipulation tasks in free-form natural language. The RobotScript platform addresses this gap by emphasizing the unified interface with both simulation and real robots, based on abstraction from the Robot Operating System (ROS), ensuring syn
    
[^29]: 从关键词到结构化摘要: 精简学术知识获取

    From Keywords to Structured Summaries: Streamlining Scholarly Knowledge Access

    [https://arxiv.org/abs/2402.14622](https://arxiv.org/abs/2402.14622)

    该论文突出了信息检索引擎在科学界的重要性，并提出了一种通过结构化记录和先进信息技术工具实现的解决方案，以革新研究人员访问和过滤文章的方式。

    

    这篇短文强调了信息检索引擎在科学界日益重要，指出传统基于关键词的搜索引擎由于出版物数量不断增加而效率低下。提出的解决方案涉及结构化记录，支持先进的信息技术工具，包括可视化仪表板，以彻底改变研究人员如何访问和过滤文章，取代传统的文本密集型方法。这一愿景通过一个以“传染病的繁殖数估计”研究主题为中心的概念验证得以体现，使用经过调整的大型语言模型(LLM)自动创建结构化记录以填充一个超越关键词的后端数据库。结果是一个下一代信息检索方法，可在https://orkg.org/usecases/r0-estimates 上访问。

    arXiv:2402.14622v1 Announce Type: cross  Abstract: This short paper highlights the growing importance of information retrieval (IR) engines in the scientific community, addressing the inefficiency of traditional keyword-based search engines due to the rising volume of publications. The proposed solution involves structured records, underpinning advanced information technology (IT) tools, including visualization dashboards, to revolutionize how researchers access and filter articles, replacing the traditional text-heavy approach. This vision is exemplified through a proof of concept centered on the ``reproductive number estimate of infectious diseases'' research theme, using a fine-tuned large language model (LLM) to automate the creation of structured records to populate a backend database that now goes beyond keywords. The result is a next-generation IR method accessible at https://orkg.org/usecases/r0-estimates.
    
[^30]: 单词分词对上下文化单词表示的语义内容的影响

    The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations

    [https://arxiv.org/abs/2402.14616](https://arxiv.org/abs/2402.14616)

    单词分割对上下文化单词表示的语义内容有影响，研究发现被分割的单词表示质量通常比已知单词差，但相似性值需谨慎解释。

    

    从语言模型中获取上下文化的单词表示时，需要决定如何为被分成子词的字典外（OOV）单词获得表示。什么是表示这些单词的最佳方法，它们的质量是否比词典内单词的表示质量差？我们对来自不同模型的嵌入进行了内在评估，涉及涉及OOV单词的语义相似性任务。我们的分析揭示出，表示被分割的单词的质量通常，但并非始终比已知单词的嵌入的质量差。然而，它们的相似性值必须谨慎解释。

    arXiv:2402.14616v1 Announce Type: new  Abstract: When deriving contextualized word representations from language models, a decision needs to be made on how to obtain one for out-of-vocabulary (OOV) words that are segmented into subwords. What is the best way to represent these words with a single vector, and are these representations of worse quality than those of in-vocabulary words? We carry out an intrinsic evaluation of embeddings from different models on semantic similarity tasks involving OOV words. Our analysis reveals, among other interesting findings, that the quality of representations of words that are split is often, but not always, worse than that of the embeddings of known words. Their similarity values, however, must be interpreted with caution.
    
[^31]: 《Tokenization and the Noiseless Channel》的两个反例

    Two Counterexamples to \textit{Tokenization and the Noiseless Channel}

    [https://arxiv.org/abs/2402.14614](https://arxiv.org/abs/2402.14614)

    该论文讨论了在《Tokenization and the Noiseless Channel》提出的使用Rényi效率作为分词器评估机制的局限性，并描述了两个BPE分词的反例，展示了Rényi效率无法捕捉到所有优秀分词方案的情况。

    

    在《Tokenization and the Noiseless Channel》中，建议使用Rényi效率作为评估分词器的固有机制: 对于NLP任务，应选择导致unigram分布Rényi效率最高的分词器。因此，Rényi效率被视为下游性能的预测器（例如，用于预测机器翻译任务的BLEU），而无需通过训练不同分词器的多个模型这一昂贵的步骤。尽管有用，但这一度量标准的预测能力并不完美，作者指出有其他优秀分词方案的附加特质Rényi效率本身无法捕捉。我们描述了两种BPE分词的变体，可以在增加Rényi效率的同时降低下游模型性能。这些反例揭示了Rényi效率作为固有分词的情况下存在失败的情况。

    arXiv:2402.14614v1 Announce Type: new  Abstract: In \textit{Tokenization and the Noiseless Channel} \cite{zouhar-etal-2023-tokenization}, R\'enyi efficiency is suggested as an intrinsic mechanism for evaluating a tokenizer: for NLP tasks, the tokenizer which leads to the highest R\'enyi efficiency of the unigram distribution should be chosen. The R\'enyi efficiency is thus treated as a predictor of downstream performance (e.g., predicting BLEU for a machine translation task), without the expensive step of training multiple models with different tokenizers. Although useful, the predictive power of this metric is not perfect, and the authors note there are additional qualities of a good tokenization scheme that R\'enyi efficiency alone cannot capture.   We describe two variants of BPE tokenization which can arbitrarily increase R\'enyi efficiency while decreasing the downstream model performance. These counterexamples expose cases where R\'enyi efficiency fails as an intrinsic tokenizati
    
[^32]: 利用检索增强生成改进辅导实践评估

    Improving Assessment of Tutoring Practices using Retrieval-Augmented Generation

    [https://arxiv.org/abs/2402.14594](https://arxiv.org/abs/2402.14594)

    通过利用生成预训练变换器来自动评估辅导员使用社交情感辅导策略的能力，从而改进辅导实践评估。

    

    一对一辅导是提高学习效果的有效教学方法，然而其效力取决于辅导员的能力。新手数学辅导员通常优先考虑特定内容的指导，忽视社交情感学习等方面。社交情感学习促进了公平和包容性，并培养与学生的关系，这对于学生整体发展至关重要。准确有效地评估辅导员的能力可以推动定制的辅导员培训计划的发展。然而，在实时辅导中评估新手辅导员的能力仍然具有挑战性，因为通常需要专家参与。为解决这一挑战，本初步研究旨在利用生成预训练变换器（GPT），如GPT-3.5和GPT-4模型，自动评估辅导员使用社交情感辅导策略的能力。此外，本研究还报告了财务维度的内容。

    arXiv:2402.14594v1 Announce Type: cross  Abstract: One-on-one tutoring is an effective instructional method for enhancing learning, yet its efficacy hinges on tutor competencies. Novice math tutors often prioritize content-specific guidance, neglecting aspects such as social-emotional learning. Social-emotional learning promotes equity and inclusion and nurturing relationships with students, which is crucial for holistic student development. Assessing the competencies of tutors accurately and efficiently can drive the development of tailored tutor training programs. However, evaluating novice tutor ability during real-time tutoring remains challenging as it typically requires experts-in-the-loop. To address this challenge, this preliminary study aims to harness Generative Pre-trained Transformers (GPT), such as GPT-3.5 and GPT-4 models, to automatically assess tutors' ability of using social-emotional tutoring strategies. Moreover, this study also reports on the financial dimensions an
    
[^33]: 扩展LLM审核以进行Google广告内容管理

    Scaling Up LLM Reviews for Google Ads Content Moderation

    [https://arxiv.org/abs/2402.14590](https://arxiv.org/abs/2402.14590)

    本研究提出了一种用于在Google广告中进行内容管理的方法，通过使用LLMs审核代表性广告并将决策传播回其群集，将审核数目减少了3个数量级以上，同时实现了2倍的召回率。

    

    大型语言模型（LLMs）是内容管理的强大工具，但它们的推理成本和延迟使它们在大型数据集（如Google Ads存储库）上的临时使用成本过高。本研究提出了一种方法，用于扩展LLM审核以在Google Ads中进行内容管理。首先，我们使用启发式方法通过过滤和重复项删除来选择候选项，并为此创建广告群集，我们选择每个群集的一个代表性广告。然后，我们使用LLMs仅审核代表性广告。最后，我们将代表性广告的LLM决策传播回它们的群集。该方法将审核数目减少了3个数量级以上，同时与基线非LLM模型相比实现了2倍的召回率。该方法的成功在很大程度上取决于聚类和标签传播中使用的表示; 我们发现交叉模态相似性表示产生比单一模态更好的结果。

    arXiv:2402.14590v1 Announce Type: cross  Abstract: Large language models (LLMs) are powerful tools for content moderation, but their inference costs and latency make them prohibitive for casual use on large datasets, such as the Google Ads repository. This study proposes a method for scaling up LLM reviews for content moderation in Google Ads. First, we use heuristics to select candidates via filtering and duplicate removal, and create clusters of ads for which we select one representative ad per cluster. We then use LLMs to review only the representative ads. Finally, we propagate the LLM decisions for the representative ads back to their clusters. This method reduces the number of reviews by more than 3 orders of magnitude while achieving a 2x recall compared to a baseline non-LLM model. The success of this approach is a strong function of the representations used in clustering and label propagation; we found that cross-modal similarity representations yield better results than uni-m
    
[^34]: 使用多模态Transformer在科学图表中进行文本角色分类

    Text Role Classification in Scientific Charts Using Multimodal Transformers

    [https://arxiv.org/abs/2402.14579](https://arxiv.org/abs/2402.14579)

    该研究提出了使用多模态Transformer在科学图表中进行文本角色分类的方法，并在实验中发现LayoutLMv3在性能上优于UDOP，最高达到了82.87的F1-macro分数。

    

    文本角色分类涉及对科学图表中的文本元素的语义角色进行分类。对于这一任务，我们提出在图表数据集上对两个预训练的多模态文档布局分析模型LayoutLMv3和UDOP进行微调。这些Transformer利用文本、图像和布局三种模态作为输入。我们进一步探讨数据增强和平衡方法是否有助于模型的性能。这些模型在各种图表数据集上进行评估，结果显示LayoutLMv3在所有实验中均优于UDOP。LayoutLMv3在ICPR22测试数据集上实现了82.87的最高F1-macro分数，超过了ICPR22 CHART-Infographics挑战中表现最好的模型。此外，模型的鲁棒性在一个合成的嘈杂数据集ICPR22-N上进行了测试。最后，我们评估了模型在三个图表数据集CHIME-R、DeGruyter和EconBiz上的泛化性能，我们为这些数据集添加了标签。

    arXiv:2402.14579v1 Announce Type: cross  Abstract: Text role classification involves classifying the semantic role of textual elements within scientific charts. For this task, we propose to finetune two pretrained multimodal document layout analysis models, LayoutLMv3 and UDOP, on chart datasets. The transformers utilize the three modalities of text, image, and layout as input. We further investigate whether data augmentation and balancing methods help the performance of the models. The models are evaluated on various chart datasets, and results show that LayoutLMv3 outperforms UDOP in all experiments. LayoutLMv3 achieves the highest F1-macro score of 82.87 on the ICPR22 test dataset, beating the best-performing model from the ICPR22 CHART-Infographics challenge. Moreover, the robustness of the models is tested on a synthetic noisy dataset ICPR22-N. Finally, the generalizability of the models is evaluated on three chart datasets, CHIME-R, DeGruyter, and EconBiz, for which we added labe
    
[^35]: LLM-DA: 基于大型语言模型的数据增强在少样本命名实体识别中的应用

    LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition

    [https://arxiv.org/abs/2402.14568](https://arxiv.org/abs/2402.14568)

    提出了一种新的数据增强技术$LLM-DA$，基于大型语言模型，用于少样本NER任务，在上下文和实体层面增强数据，展示了显著的性能提升。

    

    尽管大型语言模型(LLMs)具有令人印象深刻的能力，但它们在信息抽取任务上的表现仍然不完全令人满意。然而，它们出色的重写能力和广泛的世界知识为改进这些任务提供了宝贵的见解。在本文中，我们提出了$LLM-DA$，一种基于LLMs的新颖数据增强技术，用于少样本NER任务。为了克服现有数据增强方法的局限性，这些方法会损害语义完整性并解决LLM生成的文本中固有的不确定性，我们利用NER任务的独特特征，在上下文和实体层面上增强原始数据。我们的方法涉及使用14种上下文重写策略，设计相同类型的实体替换，并引入噪声注入来增强鲁棒性。广泛的实验表明了我们方法在增强NER性能方面的有效性。

    arXiv:2402.14568v1 Announce Type: new  Abstract: Despite the impressive capabilities of large language models (LLMs), their performance on information extraction tasks is still not entirely satisfactory. However, their remarkable rewriting capabilities and extensive world knowledge offer valuable insights to improve these tasks. In this paper, we propose $LLM-DA$, a novel data augmentation technique based on LLMs for the few-shot NER task. To overcome the limitations of existing data augmentation methods that compromise semantic integrity and address the uncertainty inherent in LLM-generated text, we leverage the distinctive characteristics of the NER task by augmenting the original data at both the contextual and entity levels. Our approach involves employing 14 contextual rewriting strategies, designing entity replacements of the same type, and incorporating noise injection to enhance robustness. Extensive experiments demonstrate the effectiveness of our approach in enhancing NER mod
    
[^36]: 具有工业视角的LLMs：揭示挑战与前景--一项调查

    LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A Survey

    [https://arxiv.org/abs/2402.14558](https://arxiv.org/abs/2402.14558)

    本文调查了在工业背景下利用LLMs所面临的挑战和前景，并通过对行业从业者调查以及审查大量行业论文得出有意义的结论。

    

    大语言模型（LLMs）已经成为推动许多工业应用的秘密武器，展示出它们在各种任务中的卓越适应性。从自然语言处理和情感分析到内容生成和个性化推荐，它们无与伦比的适应性促进了在各个行业的广泛应用。LLMs带来的这种转变强调了探索与利用中的困难和增强机会的必要性。本文的目标是揭示和评估在工业背景下利用LLMs所面临的障碍和机会。为此，我们进行了一项涉及一组行业从业者的调查，提出了四个研究问题，并根据收集到的见解审查了68篇行业论文，以解决这些问题并得出有意义的结论。

    arXiv:2402.14558v1 Announce Type: new  Abstract: Large language models (LLMs) have become the secret ingredient driving numerous industrial applications, showcasing their remarkable versatility across a diverse spectrum of tasks. From natural language processing and sentiment analysis to content generation and personalized recommendations, their unparalleled adaptability has facilitated widespread adoption across industries. This transformative shift driven by LLMs underscores the need to explore the underlying associated challenges and avenues for enhancement in their utilization. In this paper, our objective is to unravel and evaluate the obstacles and opportunities inherent in leveraging LLMs within an industrial context. To this end, we conduct a survey involving a group of industry practitioners, develop four research questions derived from the insights gathered, and examine 68 industry papers to address these questions and derive meaningful conclusions.
    
[^37]: OmniPred：语言模型作为通用回归器

    OmniPred: Language Models as Universal Regressors

    [https://arxiv.org/abs/2402.14547](https://arxiv.org/abs/2402.14547)

    本文提出了OmniPred框架，用于训练语言模型作为通用的端到端回归器，实验证明，在多个任务上训练时，语言模型能够显著优于传统回归模型。

    

    在实验设计的广阔领域中，回归一直是一个强大的工具，可以准确预测系统或模型在给定一组参数的情况下的结果指标，但传统上只限于适用于特定任务的方法。在本文中，我们提出了OmniPred，这是一个用于训练语言模型作为通用端到端回归器的框架，使用来自多样真实世界实验的$(x,y)$评估数据。通过使用源自Google Vizier的数据，这是世界上最大的黑盒优化数据库之一，我们的大量实验表明，仅通过数学参数和值的文本表示，语言模型能够进行非常精确的数值回归，如果有机会训练多个任务，则可以显著优于传统的回归模型。

    arXiv:2402.14547v1 Announce Type: cross  Abstract: Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.
    
[^38]: 减少是有益的：从EOS决策角度缓解多模态幻觉

    Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective

    [https://arxiv.org/abs/2402.14545](https://arxiv.org/abs/2402.14545)

    本文研究了大型多模态模型中存在的多模态幻觉问题，发现通过模型基于视觉感知作出适当的EOS决策，可以减少持续输出，提出了两种缓解方法。

    

    大型多模态模型（LMMs）经常遭受多模态幻觉，即它们可能创造出在视觉输入中并不存在的内容。本文探讨了这个问题的一个新角度：过于详细的训练数据妨碍了模型及时终止生成，导致超出视觉感知限制的持续输出。通过研究模型如何通过EOS（特殊的句子结尾标记）来决定终止生成，我们发现模型通过将生成的文本与图像进行比较来评估整个序列的完整性。这一观察表明，模型具有基于其视觉感知进行适当EOS决策的潜力，以避免过长的输出。为了利用这种潜力，我们探讨了两种缓解多模态幻觉的方法：通过学习常规指示实现模型减少幻觉的训练目标

    arXiv:2402.14545v1 Announce Type: new  Abstract: Large Multimodal Models (LMMs) often suffer from multimodal hallucinations, wherein they may create content that is not present in the visual inputs. In this paper, we explore a new angle of this issue: overly detailed training data hinders the model's ability to timely terminate generation, leading to continued outputs beyond visual perception limits. By investigating how the model decides to terminate generation with EOS, the special end-of-sentence token, we find that the model assesses the completeness of the entire sequence by comparing the generated text with the image. This observation suggests that the model possesses an inherent potential of making proper EOS decisions based on its visual perception to avoid overly lengthy outputs. To take advantage of such potential, we explore two methods to mitigate multimodal hallucinations: a training objective that enables the model to reduce hallucinations by learning from regular instruc
    
[^39]: 通过因果调整实现跨领域情感分析的领域泛化

    Domain Generalization via Causal Adjustment for Cross-Domain Sentiment Analysis

    [https://arxiv.org/abs/2402.14536](https://arxiv.org/abs/2402.14536)

    本文提出了基于后门调整的因果模型，用于解决领域泛化问题，通过解开领域特定和领域不变表示之间的关系来应对领域转移

    

    领域自适应已被广泛应用于跨领域情感分析，将知识从源领域转移到目标领域。然而，大多数方法是基于目标（测试）领域已知的假设提出的，导致它们在未知的测试数据上泛化效果不佳，而在实践中这种情况并不总是可用。本文关注跨领域情感分析的领域泛化问题。具体来说，我们提出了基于后门调整的因果模型，以解开在解决领域转移中起着关键作用的领域特定和领域不变表示。首先，我们从因果视角重新思考了跨领域情感分析任务，以建模不同变量之间的因果关系。然后，为了学习一个不变的特征表示，我们使用后门调整去除领域混杂因素（如领域知识）的影响。一系列实验

    arXiv:2402.14536v1 Announce Type: new  Abstract: Domain adaption has been widely adapted for cross-domain sentiment analysis to transfer knowledge from the source domain to the target domain. Whereas, most methods are proposed under the assumption that the target (test) domain is known, making them fail to generalize well on unknown test data that is not always available in practice. In this paper, we focus on the problem of domain generalization for cross-domain sentiment analysis. Specifically, we propose a backdoor adjustment-based causal model to disentangle the domain-specific and domain-invariant representations that play essential roles in tackling domain shift. First, we rethink the cross-domain sentiment analysis task in a causal view to model the causal-and-effect relationships among different variables. Then, to learn an invariant feature representation, we remove the effect of domain confounders (e.g., domain knowledge) using the backdoor adjustment. A series of experiments
    
[^40]: 《它到底是谁的LLM？GPT-3.5、GPT-4和Bard的语言比较和LLM属性归因》

    Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard

    [https://arxiv.org/abs/2402.14533](https://arxiv.org/abs/2402.14533)

    通过对GPT-3.5、GPT-4和Bard生成的文本进行语言分析比较，发现不同的LLM之间存在显著的语言变化，可以以88%的准确率通过简单的分类模型将文本归因于相应的LLM来源。

    

    大型语言模型（LLMs）能够生成与或超越人类质量相似的文本。然而，LLMs是否倾向于表现出类似于人类作者的独特语言风格尚不清楚。通过全面的语言分析，我们比较了由当今最流行的三种LLMs（GPT-3.5、GPT-4和Bard）生成的文本的词汇、词性分布、依赖分布和情感与多样输入。结果显示出显著的语言变化，进而使我们能够使用简单的现成分类模型以88%的准确率将给定文本归因于其LLM来源。讨论了这一有趣发现的理论和实践影响。

    arXiv:2402.14533v1 Announce Type: new  Abstract: Large Language Models (LLMs) are capable of generating text that is similar to or surpasses human quality. However, it is unclear whether LLMs tend to exhibit distinctive linguistic styles akin to how human authors do. Through a comprehensive linguistic analysis, we compare the vocabulary, Part-Of-Speech (POS) distribution, dependency distribution, and sentiment of texts generated by three of the most popular LLMS today (GPT-3.5, GPT-4, and Bard) to diverse inputs. The results point to significant linguistic variations which, in turn, enable us to attribute a given text to its LLM origin with a favorable 88\% accuracy using a simple off-the-shelf classification model. Theoretical and practical implications of this intriguing finding are discussed.
    
[^41]: 我们应该尊重LLM吗？关于提示礼貌对LLM表现影响的跨语言研究

    Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance

    [https://arxiv.org/abs/2402.14531](https://arxiv.org/abs/2402.14531)

    礼貌水平对LLMs的表现有影响，粗鲁的提示通常导致较差的表现，而过于礼貌的语言并不能保证更好的结果，最佳的礼貌水平根据语言而异，LLMs不仅反映人类行为，还受语言影响，尤其是在不同的文化背景下。

    

    我们调查了提示中的礼貌程度对大型语言模型（LLMs）表现的影响。在人类交流中，礼貌的语言通常能获得更多的遵从和有效性，而粗鲁可能导致厌恶，影响回应质量。我们认为LLMs反映了人类的交流特征，暗示它们与人类文化规范一致。我们评估了英语、中文和日语任务中提示中的礼貌对LLMs的影响。我们观察到，粗鲁的提示通常导致较差的表现，而过于礼貌的语言并不能保证更好的结果。最佳的礼貌程度根据语言而异。这一现象表明LLMs不仅反映人类行为，还受语言影响，尤其是在不同的文化背景下。我们的发现强调了在跨文化自然语言处理和LLM使用中考虑礼貌的必要性。

    arXiv:2402.14531v1 Announce Type: new  Abstract: We investigate the impact of politeness levels in prompts on the performance of large language models (LLMs). Polite language in human communications often garners more compliance and effectiveness, while rudeness can cause aversion, impacting response quality. We consider that LLMs mirror human communication traits, suggesting they align with human cultural norms. We assess the impact of politeness in prompts on LLMs across English, Chinese, and Japanese tasks. We observed that impolite prompts often result in poor performance, but overly polite language does not guarantee better outcomes. The best politeness level is different according to the language. This phenomenon suggests that LLMs not only reflect human behavior but are also influenced by language, particularly in different cultural contexts. Our findings highlight the need to factor in politeness for cross-cultural natural language processing and LLM usage.
    
[^42]: 带聚类的语言模型训练平衡数据抽样

    Balanced Data Sampling for Language Model Training with Clustering

    [https://arxiv.org/abs/2402.14526](https://arxiv.org/abs/2402.14526)

    本文提出了一种名为ClusterClip Sampling的数据抽样方法，利用数据聚类平衡训练数据的文本分布，为更好的模型训练提供支持。

    

    数据在训练大型语言模型（LLM）中起着基础性作用。尽管人们已经关注数据集的收集和组成，但确定训练中的数据抽样策略仍然是一个悬而未决的问题。大多数LLM使用简单的随机抽样策略进行训练。然而，这种抽样策略忽视了训练数据分布的不均衡性，这可能是次优的。在本文中，我们提出了ClusterClip Sampling，以平衡训练数据的文本分布，以实现更好的模型训练。具体而言，ClusterClip Sampling利用数据聚类来反映训练集的数据分布，并根据聚类结果在训练过程中平衡常见样本和稀有样本。引入了重复裁剪操作来减轻由于来自某些聚类的样本导致的过拟合问题。大量实验证实了ClusterClip Sampling的有效性，它的表现优于

    arXiv:2402.14526v1 Announce Type: cross  Abstract: Data plays a fundamental role in the training of Large Language Models (LLMs). While attention has been paid to the collection and composition of datasets, determining the data sampling strategy in training remains an open question. Most LLMs are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal. In this paper, we propose ClusterClip Sampling to balance the text distribution of training data for better model training. Specifically, ClusterClip Sampling utilizes data clustering to reflect the data distribution of the training set and balances the common samples and rare samples during training based on the cluster results. A repetition clip operation is introduced to mitigate the overfitting issue led by samples from certain clusters. Extensive experiments validate the effectiveness of ClusterClip Sampling, which outperfo
    
[^43]: Daisy-TTS: 通过声调嵌入分解模拟更广泛的情感范围

    Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding Decomposition

    [https://arxiv.org/abs/2402.14523](https://arxiv.org/abs/2402.14523)

    本文提出了Daisy-TTS设计，通过声调嵌入分解，模拟了更广泛的情感范围，包括 primary emotions、secondary emotions、intensity-level 和 emotions polarity。

    

    我们经常以多方面的方式口头表达情感，它们在强度上可能有所变化，表达的不仅是单一的情感，还可能是各种情感的混合体。这种广泛的情感范围在情感结构模型中得到了深入研究，该模型将各种情感表示为原始情感的派生产品，具有不同程度的强度。在本文中，我们提出了一种情感文本转语音设计，旨在模拟基于结构模型的更广泛情感范围。我们提出的设计Daisy-TTS，结合了一个声调编码器，用于学习作为情感代理的可分离的声调嵌入。这种情感表示使模型能够模拟：（1）从训练样本中学到的原始情感，（2）作为原始情感的混合体的次级情感，（3）通过调整情感嵌入来实现强度级别，（4）通过否定情感嵌入来实现情感极性。

    arXiv:2402.14523v1 Announce Type: new  Abstract: We often verbally express emotions in a multifaceted manner, they may vary in their intensities and may be expressed not just as a single but as a mixture of emotions. This wide spectrum of emotions is well-studied in the structural model of emotions, which represents variety of emotions as derivative products of primary emotions with varying degrees of intensity. In this paper, we propose an emotional text-to-speech design to simulate a wider spectrum of emotions grounded on the structural model. Our proposed design, Daisy-TTS, incorporates a prosody encoder to learn emotionally-separable prosody embedding as a proxy for emotion. This emotion representation allows the model to simulate: (1) Primary emotions, as learned from the training samples, (2) Secondary emotions, as a mixture of primary emotions, (3) Intensity-level, by scaling the emotion embedding, and (4) Emotions polarity, by negating the emotion embedding. Through a series of
    
[^44]: 跨越多个模型的统一任务嵌入：弥合基于提示的大型语言模型及其它模型的差距

    Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond

    [https://arxiv.org/abs/2402.14522](https://arxiv.org/abs/2402.14522)

    提出了一种框架用于统一不同模型的任务嵌入，使得任务嵌入可以跨越各种模型，并在单一向量空间内进行比较和分析。

    

    任务嵌入是一种捕捉任务特定信息的元学习技术，已经变得流行起来，特别是在多任务学习、模型编辑和可解释性等领域。文章提出了一种名为统一任务嵌入（FUTE）的框架，该框架能够协调来自各种模型（包括较小的语言模型和具有不同提示的LLMs）的任务嵌入，使其处于单一向量空间。这种统一性使得可以比较和分析不同模型之间的相似性，扩展了现有任务嵌入方法在解决多模型应用中的范围和效用。

    arXiv:2402.14522v1 Announce Type: new  Abstract: Task embedding, a meta-learning technique that captures task-specific information, has become prevalent, especially in areas such as multi-task learning, model editing, and interpretability. However, it faces challenges with the emergence of prompt-guided Large Language Models (LLMs) operating in a gradientfree manner. Existing task embedding methods rely on fine-tuned, task-specific language models, which hinders the adaptability of task embeddings across diverse models, especially prompt-based LLMs. To unleash the power of task embedding in the era of LLMs, we propose a framework for unified task embeddings (FUTE), harmonizing task embeddings from various models, including smaller language models and LLMs with varied prompts, within a single vector space. Such uniformity enables the comparison and analysis of similarities amongst different models, extending the scope and utility of existing task embedding methods in addressing multi-mo
    
[^45]: 马来西亚英语新闻解析：一个用于命名实体和关系抽取的语言资源

    Malaysian English News Decoded: A Linguistic Resource for Named Entity and Relation Extraction

    [https://arxiv.org/abs/2402.14521](https://arxiv.org/abs/2402.14521)

    通过构建一个含有实体和关系标注的马来西亚英语新闻数据集，并对spaCy NER工具进行微调，本研究显著改善了马来西亚英语中NER的性能。

    

    标准英语和马来西亚英语存在明显差异，在马来西亚英语的自然语言处理（NLP）任务中存在挑战。本文介绍了一个包含200篇新闻文章的马来西亚英语新闻（MEN）数据集，手动对实体和关系进行了标注，并通过对spaCy NER工具进行微调验证了针对马来西亚英语定制的数据集可以显著提高NER在马来西亚英语中的性能。

    arXiv:2402.14521v1 Announce Type: new  Abstract: Standard English and Malaysian English exhibit notable differences, posing challenges for natural language processing (NLP) tasks on Malaysian English. Unfortunately, most of the existing datasets are mainly based on standard English and therefore inadequate for improving NLP tasks in Malaysian English. An experiment using state-of-the-art Named Entity Recognition (NER) solutions on Malaysian English news articles highlights that they cannot handle morphosyntactic variations in Malaysian English. To the best of our knowledge, there is no annotated dataset available to improvise the model. To address these issues, we constructed a Malaysian English News (MEN) dataset, which contains 200 news articles that are manually annotated with entities and relations. We then fine-tuned the spaCy NER tool and validated that having a dataset tailor-made for Malaysian English could improve the performance of NER in Malaysian English significantly. This
    
[^46]: "我的答案是C": 指令调整的语言模型中的第一个令牌概率与文本答案不匹配

    "My Answer is C": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models

    [https://arxiv.org/abs/2402.14499](https://arxiv.org/abs/2402.14499)

    第一个令牌预测不一定代表最终文本输出，在评估大型语言模型时存在严重的不一致性，影响模型行为与用户互动。

    

    arXiv:2402.14499v1 公告类型: 新的 摘要: 语言生成的开放性质使得评估自回归大型语言模型（LLMs）具有挑战性。一种常见的评估方法是使用多项选择题（MCQ）限制响应空间。然后通过排名候选答案的第一个令牌预测的对数概率来评估模型。然而，第一个令牌可能不一致地反映最终的响应输出，因为模型具有多样化的响应风格，例如以"确定"开头或拒绝回答。因此，MCQ评估无法表明模型与用户互动时的行为。但差距有多大呢？我们评估了第一个令牌评估在几个维度上与文本输出的一致性，即最终选项选择、拒绝率、选择分布和在提示扰动下的稳健性。我们的结果显示，这两种方法在所有维度上严重不一致，达到60%以上的不匹配率。模型非常

    arXiv:2402.14499v1 Announce Type: new  Abstract: The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions (MCQ) to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model's diverse response styles such as starting with "Sure" or refusing to answer. Consequently, MCQ evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation. Our results show that the two approaches are severely misaligned on all dimensions, reaching mismatch rates over 60%. Models heavil
    
[^47]: Noise-BERT: 一种具有噪声对齐预训练的统一扰动鲁棒性框架，用于嘈杂的槽填充任务

    Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment Pre-training for Noisy Slot Filling Task

    [https://arxiv.org/abs/2402.14494](https://arxiv.org/abs/2402.14494)

    提出了Noise-BERT框架，包含噪声对齐预训练任务，通过对比学习损失和对抗攻击训练策略，以提高在嘈杂环境下的槽填充任务表现。

    

    在现实对话系统中，用户输入信息经常遭受各种类型的输入扰动，这影响了槽填充任务。尽管基于规则的数据增强方法已经取得了令人满意的结果，但当面对未知噪声干扰时，它们无法展现出期望的泛化能力。在本研究中，我们通过提出Noise-BERT来解决槽填充中输入扰动带来的挑战，这是一个具有噪声对齐预训练的统一扰动鲁棒性框架。我们的框架包含两个Noise Alignment预训练任务：槽屏蔽预测和句子嘈杂度判别，旨在引导预训练语言模型捕捉准确的槽信息和噪声分布。在微调过程中，我们采用对比学习损失来增强实体和标签的语义表示。此外，我们引入了对抗攻击训练策略以提高语义表示的鲁棒性。

    arXiv:2402.14494v1 Announce Type: new  Abstract: In a realistic dialogue system, the input information from users is often subject to various types of input perturbations, which affects the slot-filling task. Although rule-based data augmentation methods have achieved satisfactory results, they fail to exhibit the desired generalization when faced with unknown noise disturbances. In this study, we address the challenges posed by input perturbations in slot filling by proposing Noise-BERT, a unified Perturbation-Robust Framework with Noise Alignment Pre-training. Our framework incorporates two Noise Alignment Pre-training tasks: Slot Masked Prediction and Sentence Noisiness Discrimination, aiming to guide the pre-trained language model in capturing accurate slot information and noise distribution. During fine-tuning, we employ a contrastive learning loss to enhance the semantic representation of entities and labels. Additionally, we introduce an adversarial attack training strategy to i
    
[^48]: INSTRAUG：用于多模指令微调的自动指令增强

    INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction Fine-tuning

    [https://arxiv.org/abs/2402.14492](https://arxiv.org/abs/2402.14492)

    INSTRAUG是一种自动指令增强方法，可以在多模任务中显著改善多模大型语言模型的对齐，相当于增加训练规模的好处。

    

    将大型语言模型（LLMs）在多任务指令跟随数据上进行微调已被证明是一种强大的学习范式，可以提高它们对新任务的零样本能力。最近关于高质量指令跟随数据生成和选择的工作需要大量人力，以为给定任务构思模型可理解的指令，并谨慎过滤LLM生成的数据。在这项工作中，我们引入了一种名为INSTRAUG的多模任务自动指令增强方法。它从一些基本和简单的元指令开始，但能将一个指令跟随数据集扩大30倍。在两个流行的多模指令跟随基准测试集MULTIINSTRUCT和InstructBLIP上的结果显示，INSTRAUG可以显著改善跨12个多模任务的多模大型语言模型（MLLMs）的对齐，甚至相当于增加训练规模的好处。

    arXiv:2402.14492v1 Announce Type: cross  Abstract: Fine-tuning large language models (LLMs) on multi-task instruction-following data has been proven to be a powerful learning paradigm for improving their zero-shot capabilities on new tasks. Recent works about high-quality instruction-following data generation and selection require amounts of human labor to conceive model-understandable instructions for the given tasks and carefully filter the LLM-generated data. In this work, we introduce an automatic instruction augmentation method named INSTRAUG in multimodal tasks. It starts from a handful of basic and straightforward meta instructions but can expand an instruction-following dataset by 30 times. Results on two popular multimodal instructionfollowing benchmarks MULTIINSTRUCT and InstructBLIP show that INSTRAUG can significantly improve the alignment of multimodal large language models (MLLMs) across 12 multimodal tasks, which is even equivalent to the benefits of scaling up training 
    
[^49]: 生成器是否关心其上下文？对上下文转移情况下生成模型忠实性的分析

    Does the Generator Mind its Contexts? An Analysis of Generative Model Faithfulness under Context Transfer

    [https://arxiv.org/abs/2402.14488](https://arxiv.org/abs/2402.14488)

    本研究引入了知识增强的生成器，旨在产生保持基于上下文知识的信息，无论上下文如何改变。调查发现所有模型都有生成先前答案作为幻觉的倾向。

    

    本研究引入了知识增强的生成器，旨在产生保持基于上下文知识的信息，无论上下文如何改变。先前的研究主要集中在检验源自静态输入的幻觉，例如在摘要或机器翻译领域。然而，我们的研究探讨了在动态知识存在的情况下生成式问答的忠实性。我们的目标是探索当上下文知识发生变化时由参数化内存产生幻觉的存在，并分析其发生的潜在原因。为了有效地解决这个问题，我们提出了一种简单而有效的方法来检测这种幻觉。有趣的是，我们的调查发现所有模型都有生成先前答案作为幻觉的倾向。

    arXiv:2402.14488v1 Announce Type: new  Abstract: The present study introduces the knowledge-augmented generator, which is specifically designed to produce information that remains grounded in contextual knowledge, regardless of alterations in the context. Previous research has predominantly focused on examining hallucinations stemming from static input, such as in the domains of summarization or machine translation. However, our investigation delves into the faithfulness of generative question answering in the presence of dynamic knowledge. Our objective is to explore the existence of hallucinations arising from parametric memory when contextual knowledge undergoes changes, while also analyzing the underlying causes for their occurrence. In order to efficiently address this issue, we propose a straightforward yet effective measure for detecting such hallucinations. Intriguingly, our investigation uncovers that all models exhibit a tendency to generate previous answers as hallucinations
    
[^50]: ChatGPT是因果文本挖掘的未来吗？一项全面评估和分析

    Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation and Analysis

    [https://arxiv.org/abs/2402.14484](https://arxiv.org/abs/2402.14484)

    本研究全面评估了ChatGPT的因果文本挖掘能力，发现ChatGPT在各种数据集上表现良好，但当有足够多的训练数据时，以往的模型仍然超越了它。

    

    因果性在人类认知中至关重要，并在各种研究领域引起关注。随着文本数据量的增加，识别文本数据中的因果关系至关重要，因果文本挖掘在提取有意义模式中发挥着关键作用。本研究对ChatGPT的因果文本挖掘能力进行了全面评估。首先，我们引入了一个超出一般英语数据集的基准，包括领域特定和非英语数据集。我们还提供了一个评估框架，以确保ChatGPT和之前方法之间的公平比较。最后，我们的分析概述了在应用ChatGPT进行因果文本挖掘时的局限性和未来挑战。具体而言，我们的分析表明，ChatGPT对于各种数据集来说都是一个良好的起点。然而，当配备足够数量的训练数据时，以往的模型仍然优于ChatGPT的性能。

    arXiv:2402.14484v1 Announce Type: new  Abstract: Causality is fundamental in human cognition and has drawn attention in diverse research fields. With growing volumes of textual data, discerning causalities within text data is crucial, and causal text mining plays a pivotal role in extracting meaningful patterns. This study conducts comprehensive evaluations of ChatGPT's causal text mining capabilities. Firstly, we introduce a benchmark that extends beyond general English datasets, including domain-specific and non-English datasets. We also provide an evaluation framework to ensure fair comparisons between ChatGPT and previous approaches. Finally, our analysis outlines the limitations and future challenges in employing ChatGPT for causal text mining. Specifically, our analysis reveals that ChatGPT serves as a good starting point for various datasets. However, when equipped with a sufficient amount of training data, previous models still surpass ChatGPT's performance. Additionally, ChatG
    
[^51]: 使用LLMs和可解释模型的数据科学

    Data Science with LLMs and Interpretable Models

    [https://arxiv.org/abs/2402.14474](https://arxiv.org/abs/2402.14474)

    该研究展示了大型语言模型LLMs在描述、解释和调试广义加性模型GAMs方面的出色表现，结合LLMs的灵活性和GAMs准确描述的统计模式，可以实现数据集摘要、问答和模型评估。

    

    最近几年，在构建可解释模型方面取得了重要进展，这些机器学习模型旨在被人类轻松理解。在这项工作中，我们展示了大型语言模型(LLMs)在使用可解释模型方面表现出色。特别是，我们展示了LLMs可以描述、解释和调试广义加性模型(GAMs)。将LLMs的灵活性与GAMs准确描述的广泛统计模式相结合，能够实现数据集摘要、回答问题和模型评估。LLMs还可以改善领域专家与可解释模型之间的交互，并为潜在现象生成假设。我们发布了一个开源LLM-GAM接口\url{https://github.com/interpretml/TalkToEBM}。

    arXiv:2402.14474v1 Announce Type: cross  Abstract: Recent years have seen important advances in the building of interpretable models, machine learning models that are designed to be easily understood by humans. In this work, we show that large language models (LLMs) are remarkably good at working with interpretable models, too. In particular, we show that LLMs can describe, interpret, and debug Generalized Additive Models (GAMs). Combining the flexibility of LLMs with the breadth of statistical patterns accurately described by GAMs enables dataset summarization, question answering, and model critique. LLMs can also improve the interaction between domain experts and interpretable models, and generate hypotheses about the underlying phenomenon. We release \url{https://github.com/interpretml/TalkToEBM} as an open-source LLM-GAM interface.
    
[^52]: NLAS-multi：一个多语言自动生成的自然语言论证架构语料库

    NLAS-multi: A Multilingual Corpus of Automatically Generated Natural Language Argumentation Schemes

    [https://arxiv.org/abs/2402.14458](https://arxiv.org/abs/2402.14458)

    该论文提出了一个多语言自动生成的自然语言论证架构语料库，包括自动生成自然语言论证的有效方法、最大的公开可用的语料库以及用于自动识别论证架构的基线和模型。

    

    在论证挖掘、论证生成和自然语言论证分析领域，一些主要限制涉及注释富有论证性数据的复杂性、这些语料库的有限规模，以及代表进行注释的不同语言和领域的约束。为了解决这些限制，本文提出以下贡献：(i) 在不同主题和语言中自动生成自然语言论证的有效方法论，(ii) 最大的公开可用的自然语言论证架构语料库，以及(iii) 用于自动识别论证架构的一组可靠基线和微调模型。

    arXiv:2402.14458v1 Announce Type: cross  Abstract: Some of the major limitations identified in the areas of argument mining, argument generation, and natural language argument analysis are related to the complexity of annotating argumentatively rich data, the limited size of these corpora, and the constraints that represent the different languages and domains in which these data is annotated. To address these limitations, in this paper we present the following contributions: (i) an effective methodology for the automatic generation of natural language arguments in different topics and languages, (ii) the largest publicly available corpus of natural language argumentation schemes, and (iii) a set of solid baselines and fine-tuned models for the automatic identification of argumentation schemes.
    
[^53]: 标注和分类条款和条件合同中的相关条款

    Annotation and Classification of Relevant Clauses in Terms-and-Conditions Contracts

    [https://arxiv.org/abs/2402.14457](https://arxiv.org/abs/2402.14457)

    提出了一种新的标注方案，用于分类条款和条件合同中的不同类型的条款，以支持法律专家快速识别和评估问题，并展示了通过实验证明了对这些类别进行自动分类的可行性。

    

    在本文中，我们提出了一种新的标注方案，用于分类条款和条件合同中的不同类型的条款，最终目标是支持法律专家快速识别和评估这类法律文件中的问题。为此，我们构建了一小规模的条款和条件合同语料库，并完成了一个包含14个类别的标注方案，最终达成了0.92的标注者间一致性。然后，针对其中的11个类别，我们使用多语言T5和两个基于BERT的LLM的两个微调版本进行了少样本提示的二分类任务实验，其中还包括意大利语的两个模型。我们的实验表明，通过在验证任务中达到从.79到.95的准确率，自动对我们的类别进行分类是可行的。

    arXiv:2402.14457v1 Announce Type: new  Abstract: In this paper, we propose a new annotation scheme to classify different types of clauses in Terms-and-Conditions contracts with the ultimate goal of supporting legal experts to quickly identify and assess problematic issues in this type of legal documents. To this end, we built a small corpus of Terms-and-Conditions contracts and finalized an annotation scheme of 14 categories, eventually reaching an inter-annotator agreement of 0.92. Then, for 11 of them, we experimented with binary classification tasks using few-shot prompting with a multilingual T5 and two fine-tuned versions of two BERT-based LLMs for Italian. Our experiments showed the feasibility of automatic classification of our categories by reaching accuracies ranging from .79 to .95 on validation tasks.
    
[^54]: LLM是否隐含地确定用户的适宜文本难度?

    Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?

    [https://arxiv.org/abs/2402.14453](https://arxiv.org/abs/2402.14453)

    LLMs可以隐式处理用户输入和生成的响应之间的文本难度，有些LLMs在处理文本难度上甚至可以超越人类。

    

    适应个体学习水平的教育对提高学生的理解力是必要的。利用大型语言模型（LLMs）实现这一目的的第一步是调整文本难度以适应学生。本文分析了LLMs如何在用户输入和生成的文本之间隐式调整文本难度。为了进行实验，我们从Stack Overflow创建了一个新的数据集，以探索基于问答的对话性能。在Stack Overflow数据集和TSCC数据集上进行的实验结果，包括多轮对话，表明LLMs可以隐式处理用户输入和生成的响应之间的文本难度。我们还观察到一些LLMs在处理文本难度和指导调整的重要性方面可以超越人类。

    arXiv:2402.14453v1 Announce Type: new  Abstract: Education that suits the individual learning level is necessary to improve students' understanding. The first step in achieving this purpose by using large language models (LLMs) is to adjust the textual difficulty of the response to students. This work analyzes how LLMs can implicitly adjust text difficulty between user input and its generated text. To conduct the experiments, we created a new dataset from Stack-Overflow to explore the performance of question-answering-based conversation. Experimental results on the Stack-Overflow dataset and the TSCC dataset, including multi-turn conversation show that LLMs can implicitly handle text difficulty between user input and its generated response. We also observed that some LLMs can surpass humans in handling text difficulty and the importance of instruction-tuning.
    
[^55]: 一种语言模型引导潜在空间的指南

    A Language Model's Guide Through Latent Space

    [https://arxiv.org/abs/2402.14433](https://arxiv.org/abs/2402.14433)

    本文将语言模型概念引导框架扩展到更丰富的概念集，探索当前检测和引导策略在适当性、幽默、创造力和质量等挑战性环境下的适用程度。

    

    概念引导已经成为一种廉价简单的方法，通过探究语言模型的隐藏表示中的概念向量，并在推断时使用它们来扰动激活，从而控制语言模型的行为。本文将前人工作的重点从真实性扩展到了更丰富的概念集，如恰当性、幽默、创造力和质量，探索当前检测和引导策略在这些具有挑战性的环境中的工作程度。为了方便评估，我们开发了一个考虑概念引导成功程度以及引导模型流畅性潜在退化的新度量。我们的广泛实验表明，尽管一些概念如真实性更容易通过当前技术进行引导，但像恰当性或幽默这样的新概念仍然难以引出，需要大量的

    arXiv:2402.14433v1 Announce Type: cross  Abstract: Concept guidance has emerged as a cheap and simple way to control the behavior of language models by probing their hidden representations for concept vectors and using them to perturb activations at inference time. While the focus of previous work has largely been on truthfulness, in this paper we extend this framework to a richer set of concepts such as appropriateness, humor, creativity and quality, and explore to what degree current detection and guidance strategies work in these challenging settings. To facilitate evaluation, we develop a novel metric for concept guidance that takes into account both the success of concept elicitation as well as the potential degradation in fluency of the guided model. Our extensive experiments reveal that while some concepts such as truthfulness more easily allow for guidance with current techniques, novel concepts such as appropriateness or humor either remain difficult to elicit, need extensive 
    
[^56]: KoCoSa: 韩文上下文感知讽刺检测数据集

    KoCoSa: Korean Context-aware Sarcasm Detection Dataset

    [https://arxiv.org/abs/2402.14428](https://arxiv.org/abs/2402.14428)

    该论文介绍了一个新的针对韩文对话讽刺检测任务的数据集KoCoSa，提出了一种高效的讽刺检测数据集生成流程，并提供了针对该任务的简单但有效的基线模型。

    

    讽刺是一种言语讽刺的方式，指的是有人说了和他们的本意相反的话，通常是为了嘲笑一个人、情况或想法。检测对话中的讽刺通常是困难的，因为检测讽刺应该反映上下文（即对话历史）。本文介绍了一个针对韩文对话讽刺检测任务的新数据集KoCoSa（韩文上下文感知讽刺检测数据集），包括12.8K个日常韩文对话以及该任务在最后一次回复上的标签。为了构建该数据集，我们提出了一种高效的讽刺检测数据集生成流程：1）使用大型语言模型从源对话中生成新的讽刺对话，2）自动和手动过滤异常和有毒对话，3）为讽刺检测任务进行人工注释。我们还提供了一个简单但有效的针对韩文讽刺检测任务的基线，该基线是在我们的数据集上训练的。

    arXiv:2402.14428v1 Announce Type: cross  Abstract: Sarcasm is a way of verbal irony where someone says the opposite of what they mean, often to ridicule a person, situation, or idea. It is often difficult to detect sarcasm in the dialogue since detecting sarcasm should reflect the context (i.e., dialogue history). In this paper, we introduce a new dataset for the Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware Sarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and the labels for this task on the last response. To build the dataset, we propose an efficient sarcasm detection dataset generation pipeline: 1) generating new sarcastic dialogues from source dialogues with large language models, 2) automatic and manual filtering of abnormal and toxic dialogues, and 3) human annotation for the sarcasm detection task. We also provide a simple but effective baseline for the Korean sarcasm detection task trained on our dataset. Experimental results on t
    
[^57]: 从文本描述生成人类活动的地面压力序列

    Text me the data: Generating Ground Pressure Sequence from Textual Descriptions for HAR

    [https://arxiv.org/abs/2402.14427](https://arxiv.org/abs/2402.14427)

    使用Text-to-Pressure（T2P）框架，结合深度学习技术，从文本描述中生成高质量地面压力序列，实现了文本与生成动作的一致性。

    

    在人类活动识别（HAR）中，为训练高效模型，必须有大量的地面真实数据。然而，通过物理传感器获取地面压力数据本身可能成本过高、耗时。为解决这一关键需求，我们引入了Text-to-Pressure（T2P），这是一个设计用于利用深度学习技术从人类活动的文本描述中生成大量地面压力序列的框架。我们展示了传感器数据的矢量量化与简单文本条件自回归策略的组合，允许我们通过文本描述之间的离散潜在相关性获得高质量生成的压力序列与压力地图。我们在文本与生成动作之间的一致性上取得了可比较的表现，R squared 值为0.722，Masked R squared 值为0.892，FID 分数为1.83。

    arXiv:2402.14427v1 Announce Type: cross  Abstract: In human activity recognition (HAR), the availability of substantial ground truth is necessary for training efficient models. However, acquiring ground pressure data through physical sensors itself can be cost-prohibitive, time-consuming. To address this critical need, we introduce Text-to-Pressure (T2P), a framework designed to generate extensive ground pressure sequences from textual descriptions of human activities using deep learning techniques. We show that the combination of vector quantization of sensor data along with simple text conditioned auto regressive strategy allows us to obtain high-quality generated pressure sequences from textual descriptions with the help of discrete latent correlation between text and pressure maps. We achieved comparable performance on the consistency between text and generated motion with an R squared value of 0.722, Masked R squared value of 0.892, and FID score of 1.83. Additionally, we trained 
    
[^58]: J-UniMorph: 通过通用特征模式对日语进行形态学标注

    J-UniMorph: Japanese Morphological Annotation through the Universal Feature Schema

    [https://arxiv.org/abs/2402.14411](https://arxiv.org/abs/2402.14411)

    J-UniMorph通过提供更广泛且更频繁使用的动词形式，包括敬语、一系列礼貌水平和其他语言细微差别，强调了日语的独特特点。

    

    我们介绍了一种基于UniMorph特征模式开发的日语形态学数据集J-UniMorph。该数据集解决了这种词语凝聚性语言特有的独特和丰富的动词形式。J-UniMorph与现有的从维基词典自动提取的UniMorph日语子集有所不同。通常，维基词典版本每个单词平均包含约12个屈折形式，并且主要以[名词]+する(做-现在时)的名词性动词为主导。从形态学上讲，这种形式等同于动词する(做)。相比之下，J-UniMorph探索了更广泛且更常用的动词形式范围，平均为每个单词提供了118个屈折形式。它包括敬语、一系列礼貌水平以及其他语言细微差别，强调了日语的独特特点。本文呈现了J-UniMorph的详细统计数据和特征，进行了比较。

    arXiv:2402.14411v1 Announce Type: new  Abstract: We introduce a Japanese Morphology dataset, J-UniMorph, developed based on the UniMorph feature schema. This dataset addresses the unique and rich verb forms characteristic of the language's agglutinative nature. J-UniMorph distinguishes itself from the existing Japanese subset of UniMorph, which is automatically extracted from Wiktionary. On average, the Wiktionary Edition features around 12 inflected forms for each word and is primarily dominated by denominal verbs (i.e., [noun] +suru (do-PRS)). Morphologically, this form is equivalent to the verb suru (do). In contrast, J-UniMorph explores a much broader and more frequently used range of verb forms, offering 118 inflected forms for each word on average. It includes honorifics, a range of politeness levels, and other linguistic nuances, emphasizing the distinctive characteristics of the Japanese language. This paper presents detailed statistics and characteristics of J-UniMorph, compar
    
[^59]: 知识之间的拉锯战: 探索和解决检索增强语言模型中的知识冲突

    Tug-of-War Between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models

    [https://arxiv.org/abs/2402.14409](https://arxiv.org/abs/2402.14409)

    本文探讨了检索增强语言模型中的知识冲突，提出了评估框架，研究了RALMs对内部记忆和外部来源间的冲突，发现了它们会偏向错误的内部记忆。

    

    检索增强语言模型（RALMs）已经在通过从外部来源检索证据来优化和扩展其内部记忆方面表现出重要潜力。然而，RALMs在将内部记忆与外部来源整合时必然会遇到知识冲突。知识冲突会使RALMs陷入知识之间的拉锯战，限制其实际应用。本文着重于探索和解决RALMs中的知识冲突。首先，我们提出了一个评估框架，用于评估不同维度上的知识冲突。然后，我们从以下两个角度研究了RALMs的行为和偏好：（1）内部记忆与外部来源之间的冲突：我们发现，随着邓宁-克鲁格效应的增强，更强大的RALMs会持续偏爱其错误的内部记忆，即使提供了正确的证据。此外，RALMs还表现出一种可用性

    arXiv:2402.14409v1 Announce Type: cross  Abstract: Retrieval-augmented language models (RALMs) have demonstrated significant potential in refining and expanding their internal memory by retrieving evidence from external sources. However, RALMs will inevitably encounter knowledge conflicts when integrating their internal memory with external sources. Knowledge conflicts can ensnare RALMs in a tug-of-war between knowledge, limiting their practical applicability. In this paper, we focus on exploring and resolving knowledge conflicts in RALMs. First, we present an evaluation framework for assessing knowledge conflicts across various dimensions. Then, we investigate the behavior and preference of RALMs from the following two perspectives: (1) Conflicts between internal memory and external sources: We find that stronger RALMs emerge with the Dunning-Kruger effect, persistently favoring their faulty internal memory even when correct evidence is provided. Besides, RALMs exhibit an availability
    
[^60]: 将BERT能力从高资源语言转移到低资源语言使用词汇匹配

    Transferring BERT Capabilities from High-Resource to Low-Resource Languages Using Vocabulary Matching

    [https://arxiv.org/abs/2402.14408](https://arxiv.org/abs/2402.14408)

    通过词汇匹配将BERT能力从高资源语言转移到低资源语言，有效弥合低资源语言训练困难的差距。

    

    预训练语言模型已经彻底改变了自然语言理解领域，其中最为显著的是BERT（双向编码器来自Transformer）。然而，对于低资源语言仍存在一个重要挑战，即有限的数据阻碍了这类模型的有效训练。本文提出了一种新颖的方法，通过词汇匹配将BERT的能力从高资源语言转移到低资源语言，以弥合这一差距。我们在西里西亚语和卡舒比亚语上进行了实验，并展示了我们的方法的有效性，即使目标语言仅有很少的训练数据，也能改善BERT模型的性能。我们的结果突显了该技术的潜力，能够有效训练低资源语言的BERT模型，从而使得先进的语言理解模型更具民主性地获得。

    arXiv:2402.14408v1 Announce Type: new  Abstract: Pre-trained language models have revolutionized the natural language understanding landscape, most notably BERT (Bidirectional Encoder Representations from Transformers). However, a significant challenge remains for low-resource languages, where limited data hinders the effective training of such models. This work presents a novel approach to bridge this gap by transferring BERT capabilities from high-resource to low-resource languages using vocabulary matching. We conduct experiments on the Silesian and Kashubian languages and demonstrate the effectiveness of our approach to improve the performance of BERT models even when the target language has minimal training data. Our results highlight the potential of the proposed technique to effectively train BERT models for low-resource languages, thus democratizing access to advanced language understanding models.
    
[^61]: 在巨大语言模型中分析概念表达：借助反向词典探查

    On the Tip of the Tongue: Analyzing Conceptual Representation in Large Language Models with Reverse-Dictionary Probe

    [https://arxiv.org/abs/2402.14404](https://arxiv.org/abs/2402.14404)

    该论文通过重新利用反向词典任务的案例研究，探查了大型语言模型对概念推理的能力，发现模型在该任务中表现出高准确性，并且表示空间编码了有关对象类别和细粒度特征的信息，同时还发现该任务探查的概念推理能力能够预测模型在多个基准测试中的一般推理表现。

    

    探查和增强大型语言模型的推理能力仍然是一个关键的未解问题。在这里，我们重新利用反向词典任务作为一个案例研究，来探查LLMs对概念推理的能力。我们使用上下文学习来引导模型生成一个语言描述中暗示的对象概念的术语。模型在这个任务中稳健地实现了高准确性，并且它们的表示空间编码了关于对象类别和细粒度特征的信息。进一步的实验表明，通过反向词典任务探查的概念推理能力能够预测模型在多个基准测试中的一般推理表现，尽管模型在句法泛化行为上表现相似。探索性分析表明，通过提示LLMs使用描述$\Rightarrow$单词示例可能会诱导出超越任务构型表面差异的泛化，并促进模型对更广泛的共同性的研究

    arXiv:2402.14404v1 Announce Type: cross  Abstract: Probing and enhancing large language models' reasoning capacity remains a crucial open question. Here we re-purpose the reverse dictionary task as a case study to probe LLMs' capacity for conceptual inference. We use in-context learning to guide the models to generate the term for an object concept implied in a linguistic description. Models robustly achieve high accuracy in this task, and their representation space encodes information about object categories and fine-grained features. Further experiments suggest that the conceptual inference ability as probed by the reverse-dictionary task predicts model's general reasoning performance across multiple benchmarks, despite similar syntactic generalization behaviors across models. Explorative analyses suggest that prompting LLMs with description$\Rightarrow$word examples may induce generalization beyond surface-level differences in task construals and facilitate models on broader commons
    
[^62]: 利用大型语言模型通过历史链推理增强时间知识图预测

    Enhancing Temporal Knowledge Graph Forecasting with Large Language Models via Chain-of-History Reasoning

    [https://arxiv.org/abs/2402.14382](https://arxiv.org/abs/2402.14382)

    提出了一种通过历史链推理来增强时间知识图预测的方法，有效利用高阶历史信息，弥补了基于大型语言模型的模型在处理历史信息和时间推理能力方面的不足。

    

    时间知识图（TKG）预测旨在基于给定历史数据预测未来事实。 最近的基于图的模型擅长捕捉TKGs中的结构信息，但缺乏语义理解能力。如今，随着LLMs的激增，基于LLMs的TKG预测模型已经出现。然而，现有的基于LLMs的模型存在三个缺点：（1）它只关注第一阶历史以进行预测，而忽略了高阶历史信息，导致提供给LLMs的信息极为有限。（2）在大量历史信息的情况下，LLMs很难达到最佳推理性能。（3）对于TKG预测，单独使用LLM的时间推理能力有限。为应对前两个挑战，我们提出了历史链（CoH）推理，逐步探索高阶历史，实现LLMs对TKGs上高阶历史信息的有效利用。

    arXiv:2402.14382v1 Announce Type: new  Abstract: Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based on given histories. Most recent graph-based models excel at capturing structural information within TKGs but lack semantic comprehension abilities. Nowadays, with the surge of LLMs, the LLM-based TKG prediction model has emerged. However, the existing LLM-based model exhibits three shortcomings: (1) It only focuses on the first-order history for prediction while ignoring high-order historical information, resulting in the provided information for LLMs being extremely limited. (2) LLMs struggle with optimal reasoning performance under heavy historical information loads. (3) For TKG prediction, the temporal reasoning capability of LLM alone is limited. To address the first two challenges, we propose Chain-of-History (CoH) reasoning which explores high-order histories step-by-step, achieving effective utilization of high-order historical information for LLMs on TK
    
[^63]: Novi jezički modeli za srpski jezik

    Novi jezi\v{c}ki modeli za srpski jezik

    [https://arxiv.org/abs/2402.14379](https://arxiv.org/abs/2402.14379)

    Rad predstavlja novi jezički model za srpski jezik zasnovan na transformerima, obučen na resursima Društva za jezičke resurse i tehnologije, koji će biti upoređen sa deset odabranih modela vektorizacije na četiri zadatka obrade prirodnog jezika.

    

    Rad će ukratko predstaviti istoriju razvoja modela jezika zasnovanih na transformatorima za srpski jezik. Takođe će biti predstavljeni novi modeli za generisanje teksta i vektorizaciju, obučeni na resursima Društva za jezičke resurse i tehnologije. Biće upoređeno deset izabranih modela vektorizacije za srpski jezik, uključujući dva nova, na četiri zadatka obrade prirodnog jezika. Rad će analizirati koji modeli su najbolji za svaki izabrani zadatak, kako njihova veličina i veličina skupova za obuku utiču na performanse na tim zadacima, i koji je optimalni skup za obuku najboljih jezičkih modela za srpski jezik.

    arXiv:2402.14379v1 Announce Type: new  Abstract: The paper will briefly present the development history of transformer-based language models for the Serbian language. Several new models for text generation and vectorization, trained on the resources of the Society for Language Resources and Technologies, will also be presented. Ten selected vectorization models for Serbian, including two new ones, will be compared on four natural language processing tasks. Paper will analyze which models are the best for each selected task, how does their size and the size of their training sets affect the performance on those tasks, and what is the optimal setting to train the best language models for the Serbian language.
    
[^64]: 小语言模型在中文实体关系抽取中是大语言模型的良好向导

    Small Language Model Is a Good Guide for Large Language Model in Chinese Entity Relation Extraction

    [https://arxiv.org/abs/2402.14373](https://arxiv.org/abs/2402.14373)

    本文提出了SLCoLM，一个模型协作框架，通过使用“训练-指导-预测”策略结合预训练语言模型和大语言模型，成功缓解了长尾数据问题，促进了实体关系的抽取。

    

    近年来，大语言模型（LLMs）在关系抽取（RE）任务中取得了成功，尤其是在少样本学习中。关系抽取领域中一个重要问题是长尾数据，然而目前很少有关注使用LLM方法解决这个问题。因此，在本文中，我们提出了SLCoLM，一个模型协作框架，以缓解数据长尾问题。在我们的框架中，我们使用“训练-指导-预测”策略来结合预训练语言模型（PLMs）和LLMs的优势，其中一个特定于任务的PLM框架充当导师，将任务知识转移到LLM，并指导LLM执行RE任务。我们对一个富含关系类型的RE数据集进行的实验表明，本文中的方法促进了长尾关系类型的RE。

    arXiv:2402.14373v1 Announce Type: new  Abstract: Recently, large language models (LLMs) have been successful in relational extraction (RE) tasks, especially in the few-shot learning. An important problem in the field of RE is long-tailed data, while not much attention is currently paid to this problem using LLM approaches. Therefore, in this paper, we propose SLCoLM, a model collaboration framework, to mitigate the data long-tail problem. In our framework, We use the ``\textit{Training-Guide-Predict}'' strategy to combine the strengths of pre-trained language models (PLMs) and LLMs, where a task-specific PLM framework acts as a tutor, transfers task knowledge to the LLM, and guides the LLM in performing RE tasks. Our experiments on a RE dataset rich in relation types show that the approach in this paper facilitates RE of long-tail relation types.
    
[^65]: 重新思考科学摘要评估：基于方面感知基准的可解释度指标

    Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark

    [https://arxiv.org/abs/2402.14359](https://arxiv.org/abs/2402.14359)

    该论文提出了一种基于方面感知的评估指标（FM），利用大型语言模型对摘要进行高级语义匹配，提供了一种全面评估科学摘要的方法。

    

    预训练和大型语言模型（LLMs）的摘要能力在一般领域中得到了广泛验证，但它们在涉及复杂句子和专业知识的科学语料库中的使用较少被评估。该论文提出了科学摘要的概念和实验分析，突出了传统评估方法（如$n$-gram、嵌入比较和问答）在提供解释、把握科学概念或识别关键内容方面的不足之处。随后，我们介绍了Facet-aware Metric（FM），利用LLMs进行高级语义匹配，根据不同方面评估摘要。这种面向方面的方法通过将评估任务分解为更简单的子任务，为摘要提供了全面的评估。鉴于该领域缺乏评估基准，我们精心策划了一个基于方面的科学摘要数据集（FD）。

    arXiv:2402.14359v1 Announce Type: new  Abstract: The summarization capabilities of pretrained and large language models (LLMs) have been widely validated in general areas, but their use in scientific corpus, which involves complex sentences and specialized knowledge, has been less assessed. This paper presents conceptual and experimental analyses of scientific summarization, highlighting the inadequacies of traditional evaluation methods, such as $n$-gram, embedding comparison, and QA, particularly in providing explanations, grasping scientific concepts, or identifying key content. Subsequently, we introduce the Facet-aware Metric (FM), employing LLMs for advanced semantic matching to evaluate summaries based on different aspects. This facet-aware approach offers a thorough evaluation of abstracts by decomposing the evaluation task into simpler subtasks.Recognizing the absence of an evaluation benchmark in this domain, we curate a Facet-based scientific summarization Dataset (FD) with 
    
[^66]: 是规则好还是故事更好的常识表达方式，基于大型语言模型？

    Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?

    [https://arxiv.org/abs/2402.14355](https://arxiv.org/abs/2402.14355)

    本文研究了大型语言模型通过讲故事来表达固有的常识能力，实验结果显示故事优于规则作为从LLMs检索常识的表达形式。

    

    建立具备常识的机器一直是自然语言处理中长期存在的挑战，这是由于常识规则的报告偏差和基于规则的常识推理的暴露偏差所致。相反，人类通过故事隐含地传递和传承常识。本文研究了大型语言模型（LLMs）通过讲故事来表达固有的常识能力。我们系统地研究和比较了故事和规则在从LLMs检索和利用常识方面的表现。在28个常识问答数据集上的实验结果表明，故事优于规则作为从LLMs检索常识的表达形式，在生成信心和常识准确性方面表现更好。此外，故事是回答有关日常事件的问题的更有效常识表达方式，而规则对于科学问题更有效。这与文本语料库中的常识报告偏差一致。

    arXiv:2402.14355v1 Announce Type: new  Abstract: Building machines with commonsense has been a longstanding challenge in NLP due to the reporting bias of commonsense rules and the exposure bias of rule-based commonsense reasoning. In contrast, humans convey and pass down commonsense implicitly through stories. This paper investigates the inherent commonsense ability of large language models (LLMs) expressed through storytelling. We systematically investigate and compare stories and rules for retrieving and leveraging commonsense in LLMs. Experimental results on 28 commonsense QA datasets show that stories outperform rules as the expression for retrieving commonsense from LLMs, exhibiting higher generation confidence and commonsense accuracy. Moreover, stories are the more effective commonsense expression for answering questions regarding daily events, while rules are more effective for scientific questions. This aligns with the reporting bias of commonsense in text corpora. We further 
    
[^67]: AURA：自然语言推理中的模式合理性不确定性

    AURA: Natural Language Reasoning for Aleatoric Uncertainty in Rationales

    [https://arxiv.org/abs/2402.14337](https://arxiv.org/abs/2402.14337)

    提出了在自然语言推理中处理引发模式合理性不确定性的不完美理由的方法，实施了使用熵分数和模型先验信念来指导模型的策略，并在实证中展示了方法相对于敌对理由的稳健性能优势

    

    回策背后的理由不仅解释了模型决策，而且提升了语言模型在复杂推理任务上的推理能力。然而，获得无懈可击的理由通常是不可能的。此外，估计理由足够忠实以鼓励模型表现的程度并不是微不足道的。因此，这些推理任务通常迫使模型在不理想的理由下输出正确答案，并且与模型完全有能力的情况相比是次优的。在这项工作中，我们提出了如何应对引发模式合理性不确定性的不完美理由。我们首先用给定理由的熵分数来定义模糊的理由，使用模型先验信念作为信息量。然后根据理由的模糊性来引导模型选择两种不同的推理模型中的一种。我们在实证上论证了我们提出的方法相对于理由的敌对质量产生了稳健的性能优势。

    arXiv:2402.14337v1 Announce Type: new  Abstract: Rationales behind answers not only explain model decisions but boost language models to reason well on complex reasoning tasks. However, obtaining impeccable rationales is often impossible. Besides, it is non-trivial to estimate the degree to which the rationales are faithful enough to encourage model performance. Thus, such reasoning tasks often compel models to output correct answers under undesirable rationales and are sub-optimal compared to what the models are fully capable of. In this work, we propose how to deal with imperfect rationales causing aleatoric uncertainty. We first define the ambiguous rationales with entropy scores of given rationales, using model prior beliefs as informativeness. We then guide models to select one of two different reasoning models according to the ambiguity of rationales. We empirically argue that our proposed method produces robust performance superiority against the adversarial quality of rationale
    
[^68]: INSTRUCTIR：信息检索模型指令遵循的基准

    INSTRUCTIR: A Benchmark for Instruction Following of Information Retrieval Models

    [https://arxiv.org/abs/2402.14334](https://arxiv.org/abs/2402.14334)

    INSTRUCTIR是一个专门设计用于评估信息检索任务中指令遵循能力的新型基准。

    

    尽管将搜索目标与用户意图对齐的需求至关重要，但检索器通常只优先考虑查询信息，而不深入了解用户的预期搜索上下文。增强检索器理解用户意图和偏好的能力，类似于语言模型指令，有望产生更对齐的搜索目标。先前的研究将指令在信息检索中的应用限制在任务描述格式上，忽略了多样化和不断发展的搜索场景的广泛背景。此外，用于评估的主流基准缺乏明确的定制以评估遵循指令的能力，从而阻碍了该领域的进展。为了应对这些局限，我们提出了一个新颖的基准，INSTRUCTIR，专门设计用于评估信息检索任务中的指令遵循能力。我们的方法侧重于针对用户量身定制的指令。

    arXiv:2402.14334v1 Announce Type: new  Abstract: Despite the critical need to align search targets with users' intention, retrievers often only prioritize query information without delving into the users' intended search context. Enhancing the capability of retrievers to understand intentions and preferences of users, akin to language model instructions, has the potential to yield more aligned search targets. Prior studies restrict the application of instructions in information retrieval to a task description format, neglecting the broader context of diverse and evolving search scenarios. Furthermore, the prevailing benchmarks utilized for evaluation lack explicit tailoring to assess instruction-following ability, thereby hindering progress in this field. In response to these limitations, we propose a novel benchmark,INSTRUCTIR, specifically designed to evaluate instruction-following ability in information retrieval tasks. Our approach focuses on user-aligned instructions tailored to e
    
[^69]: 理解和修补LLMs中的组成推理

    Understanding and Patching Compositional Reasoning in LLMs

    [https://arxiv.org/abs/2402.14328](https://arxiv.org/abs/2402.14328)

    本研究通过 Logit Lens 和干预实验揭示了LLMs中隐性推理结果的重要性，开发了一种修补组合推理错误的轻量级方法。

    

    LLMs 在推理任务中遇到困难，本研究通过 Logit Lens 和干预实验深入研究了LLMs的内部隐藏状态，发现多数推理失败源自于不正确生成或利用的隐性推理结果。我们的研究揭示了隐性推理结果在中间层中的出现，并对最终显式推理结果的形成起到因果作用。我们的探索进一步发现了 MHSA 模块在这些层中的存在，成为准确生成和利用隐性推理结果的关键。基于以上发现，我们开发了 CREME，一种轻量级方法，通过编辑位于的 MHSA 模块来修补组合推理中的错误。

    arXiv:2402.14328v1 Announce Type: new  Abstract: LLMs have marked a revolutonary shift, yet they falter when faced with compositional reasoning tasks. Our research embarks on a quest to uncover the root causes of compositional reasoning failures of LLMs, uncovering that most of them stem from the improperly generated or leveraged implicit reasoning results. Inspired by our empirical findings, we resort to Logit Lens and an intervention experiment to dissect the inner hidden states of LLMs. This deep dive reveals that implicit reasoning results indeed surface within middle layers and play a causative role in shaping the final explicit reasoning results. Our exploration further locates multi-head self-attention (MHSA) modules within these layers, which emerge as the linchpins in accurate generation and leveraing of implicit reasoning results. Grounded on the above findings, we develop CREME, a lightweight method to patch errors in compositional reasoning via editing the located MHSA modu
    
[^70]: 子对象级图像标记化

    Subobject-level Image Tokenization

    [https://arxiv.org/abs/2402.14327](https://arxiv.org/abs/2402.14327)

    提出一种在子对象级别进行图像标记的方法，通过序列自编码器将子对象段压缩为紧凑的嵌入向量，实现了有效地将图像转换为对象和属性描述的学习。

    

    基于Transformer的视觉模型通常将图像标记为固定大小的方形补丁作为输入单元，这种方法缺乏对图像内容的适应性，并忽略了固有的像素分组结构。受语言模型广泛采用的子词标记化启发，我们提出了一种在子对象级别进行图像标记的方法，其中子对象由通过分割模型（例如，分割任何模型）获得的具有语义意义的图像段表示。为了实现基于子对象标记化的学习系统，我们首先引入了一个序列自编码器（SeqAE），将不同大小和形状的子对象段压缩为紧凑的嵌入向量，然后将子对象嵌入馈送到大型语言模型进行视觉语言学习。实证结果表明，我们的子对象级别标记化显著促进了有效地将图像转换为对象和属性描述的学习。

    arXiv:2402.14327v1 Announce Type: cross  Abstract: Transformer-based vision models typically tokenize images into fixed-size square patches as input units, which lacks the adaptability to image content and overlooks the inherent pixel grouping structure. Inspired by the subword tokenization widely adopted in language models, we propose an image tokenizer at a subobject level, where the subobjects are represented by semantically meaningful image segments obtained by segmentation models (e.g., segment anything models). To implement a learning system based on subobject tokenization, we first introduced a Sequence-to-sequence AutoEncoder (SeqAE) to compress subobject segments of varying sizes and shapes into compact embedding vectors, then fed the subobject embeddings into a large language model for vision language learning. Empirical results demonstrated that our subobject-level tokenization significantly facilitates efficient learning of translating images into object and attribute descr
    
[^71]: Triad: 一个利用基于多角色LLM代理的框架来解决知识库问答问题

    Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering

    [https://arxiv.org/abs/2402.14320](https://arxiv.org/abs/2402.14320)

    Triad框架利用了基于多角色LLM代理来解决知识库问答问题，通过代理的不同角色分别处理KBQA子任务，合作完成KBQA任务，并在多个基准数据集上表现出色。

    

    最近基于LLM代理的进展在各种任务中展现出了令人期待的结果。然而，它们在回答知识库中问题的运用仍然鲜为人知。使用传统方法来实现KBQA系统具有挑战性，因为缺乏特定任务训练数据以及创建以任务为中心的模型结构的复杂性。在本文中，我们提出了Triad，一个利用具有三个角色的LLM代理的统一框架来进行KBQA任务。代理被分配三个角色来处理不同的KBQA子任务：作为掌握各种子任务的通才，作为选择候选者的决策者，以及作为回答带有知识的问题的顾问。我们的KBQA框架在四个阶段中执行，涉及代理的多重角色的协作。我们使用三个基准数据集评估了我们框架的性能，结果显示我们的框架胜过了

    arXiv:2402.14320v1 Announce Type: cross  Abstract: Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms 
    
[^72]: 评估波兰语文本排名模型的泛化能力

    Assessing generalization capability of text ranking models in Polish

    [https://arxiv.org/abs/2402.14318](https://arxiv.org/abs/2402.14318)

    评估波兰语文本排名模型的泛化能力，研究表明，通过有效的优化方法和大型训练数据集的结合，可以构建既体积小又具有泛化能力的重新排名器。

    

    arXiv:2402.14318v1 公告类型：新抽象：检索增强生成（RAG）正成为将内部知识库与大型语言模型集成的越来越流行的技术。在典型的RAG流水线中，使用三个模型，分别负责检索、重新排名和生成阶段。在本文中，我们专注于波兰语的重新排名问题，研究重新排名器的性能，并将其结果与可用的检索模型进行比较。我们对现有模型和我们训练的模型进行了全面评估，利用了一个由41个多样化信息检索任务组成的波兰语基准。我们实验证明，大多数模型都难以在领域外进行泛化。然而，有效的优化方法和大型训练数据集的结合允许构建既体积小又具有泛化能力的重新排名器。我们最佳的模型为重新建立了重新排名的新技术水平。

    arXiv:2402.14318v1 Announce Type: new  Abstract: Retrieval-augmented generation (RAG) is becoming an increasingly popular technique for integrating internal knowledge bases with large language models. In a typical RAG pipeline, three models are used, responsible for the retrieval, reranking, and generation stages. In this article, we focus on the reranking problem for the Polish language, examining the performance of rerankers and comparing their results with available retrieval models. We conduct a comprehensive evaluation of existing models and those trained by us, utilizing a benchmark of 41 diverse information retrieval tasks for the Polish language. The results of our experiments show that most models struggle with out-of-domain generalization. However, a combination of effective optimization method and a large training dataset allows for building rerankers that are both compact in size and capable of generalization. The best of our models establishes a new state-of-the-art for re
    
[^73]: 提示-解决前提示：引导LLMs有效利用编码知识

    Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge

    [https://arxiv.org/abs/2402.14310](https://arxiv.org/abs/2402.14310)

    引入提示-解决前提示（HSP）方法，指导LLMs生成解决问题的提示并生成包含中间推理步骤的解决方案，有效提高了推理任务的准确性。

    

    大型语言模型（LLMs）最近在各个领域展示了出色的泛化能力。尽管它们拥有丰富的知识，但LLMs仍然面临着有效利用编码知识来发展准确和合乎逻辑的推理过程的挑战。为了缓解这一问题，我们引入了提示-解决前提示（HSP），该方法引导模型生成提示（例如，特定知识或关键思想）来解决问题，然后生成包含中间推理步骤的解决方案。由于HSP与提示方法（例如，Chain-of-Thought（CoT））正交，我们将HSP应用于CoT、Least-to-Most、Plan-and-Solve和Standard提示。对6个推理基准和4个开源LLMs进行的大量实验结果表明，HSP可以有效提高推理任务的准确性：（1）通过将高质量的提示增强型HSP应用于CoT提示，Llama2-70B-Chat的准确性提高了9.7。

    arXiv:2402.14310v1 Announce Type: new  Abstract: Large Language Models (LLMs) have recently showcased remarkable generalizability in various domains. Despite their extensive knowledge, LLMs still face challenges in efficiently utilizing encoded knowledge to develop accurate and logical reasoning processes. To mitigate this problem, we introduced Hint-before-Solving Prompting (HSP), which guides the model to generate hints (e.g., specific knowledge or key ideas) for solving the problem and then generate solutions containing intermediate reasoning steps. Since HSP is orthogonal to prompting methods (e.g., Chain-of-Thought (CoT)), we applied HSP to CoT, Least-to-Most, Plan-and-Solve, and Standard promptings. The results of extensive experiments on 6 reasoning benchmarks and 4 open-source LLMs demonstrate that HSP can effectively improve the accuracy of reasoning tasks: (1) By applying high-quality hint-enhanced HSP to CoT prompting, Llama2-70B-Chat shows an improvement of 9.7. (2) Beyond 
    
[^74]: 多模式立场检测：新数据集和模型

    Multi-modal Stance Detection: New Datasets and Model

    [https://arxiv.org/abs/2402.14298](https://arxiv.org/abs/2402.14298)

    本文研究了多模式立场检测，提出了新的数据集和模型，并展示了所提出的TMPT框架在多模式立场检测中取得了最先进的性能

    

    立场检测是一项具有挑战性的任务，旨在从社交媒体平台中识别针对特定目标的公众意见。以往的立场检测工作主要集中在纯文本上。本文研究了包含文本和图像的推文的多模式立场检测，这在当今快速增长的社交媒体平台上是普遍存在的，人们经常发布多模式消息。为此，我们基于Twitter创建了五个新的不同领域的多模式立场检测数据集，其中每个示例包含文本和图像。此外，我们提出了一个简单而有效的目标多模式提示调整（TMPT）框架，其中利用目标信息从文本和视觉模态中学习多模式立场特征。对我们的三个基准数据集的实验证结果表明，所提出的TMPT在多模式立场检测中实现了最先进的性能。

    arXiv:2402.14298v1 Announce Type: new  Abstract: Stance detection is a challenging task that aims to identify public opinion from social media platforms with respect to specific targets. Previous work on stance detection largely focused on pure texts. In this paper, we study multi-modal stance detection for tweets consisting of texts and images, which are prevalent in today's fast-growing social media platforms where people often post multi-modal messages. To this end, we create five new multi-modal stance detection datasets of different domains based on Twitter, in which each example consists of a text and an image. In addition, we propose a simple yet effective Targeted Multi-modal Prompt Tuning framework (TMPT), where target information is leveraged to learn multi-modal stance features from textual and visual modalities. Experimental results on our three benchmark datasets show that the proposed TMPT achieves state-of-the-art performance in multi-modal stance detection.
    
[^75]: 在立场检测中通过校准减轻大语言模型的偏见

    Mitigating Biases of Large Language Models in Stance Detection with Calibration

    [https://arxiv.org/abs/2402.14296](https://arxiv.org/abs/2402.14296)

    本文提出了一种通过校准来减轻大语言模型在立场检测中偏见的方法，设计了门控校准网络并构建了反事实增强数据，实验证明其效果显著。

    

    大语言模型（LLMs）在许多自然语言处理任务中取得了显著进展。然而，我们的实验证明，在立场检测任务中，LLMs可能会生成偏见立场，这是由于虚假情感-立场相关性和对某些个人和主题的偏好，从而损害了它们的性能。因此，在本文中，我们提出通过校准来减轻LLMs在立场检测中的偏见（MB-Cal）。在其中，设计了一种新颖的门控校准网络，以减轻LLMs产生的立场推理结果上的偏见。此外，为了使校准更准确和可推广，我们构建了反事实增强数据来矫正立场偏见。针对目标和零射击立场检测任务的实验结果表明，所提出的MB-Cal可以有效减轻LLMs的偏见，取得了最先进的结果。

    arXiv:2402.14296v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved remarkable progress in many natural language processing tasks. However, our experiment reveals that, in stance detection tasks, LLMs may generate biased stances due to spurious sentiment-stance correlation and preference towards certain individuals and topics, thus harming their performance. Therefore, in this paper, we propose to Mitigate Biases of LLMs in stance detection with Calibration (MB-Cal). In which, a novel gated calibration network is devised to mitigate the biases on the stance reasoning results from LLMs. Further, to make the calibration more accurate and generalizable, we construct counterfactual augmented data to rectify stance biases. Experimental results on in-target and zero-shot stance detection tasks show that the proposed MB-Cal can effectively mitigate biases of LLMs, achieving state-of-the-art results.
    
[^76]: 利用大型语言模型进行NLP教育中的概念图恢复和问答

    Leveraging Large Language Models for Concept Graph Recovery and Question Answering in NLP Education

    [https://arxiv.org/abs/2402.14293](https://arxiv.org/abs/2402.14293)

    本研究利用大型语言模型在教育中进行概念图恢复和问答，并提出了专门针对科学图推理和QA的新基准TutorQA，结果表明LLMs在零-shot概念图恢复和TutorQA任务中的表现优秀。

    

    在自然语言处理（NLP）领域，大型语言模型（LLMs）在文本生成任务中表现出潜力。然而，它们在教育应用中，特别是针对领域特定查询的应用，仍未得到充分开发。本研究探讨了LLMs在教育场景中的能力，重点关注概念图恢复和问答（QA）。我们评估了LLMs在创建领域特定概念图方面的零-shot表现，并介绍了TutorQA，这是一个新的经过专家验证的针对科学图推理和QA的NLP基准。TutorQA包含五个任务，共500个QA对。为了解决TutorQA的查询，我们提出了CGLLM，这是一个将概念图与LLMs集成以回答各种问题的流水线。我们的结果表明，LLMs的零-shot概念图恢复与监督方法竞争力相当，平均F1分数提高了3%。在TutorQA任务中，LLMs的F1分数提高了高达26%。

    arXiv:2402.14293v1 Announce Type: new  Abstract: In the domain of Natural Language Processing (NLP), Large Language Models (LLMs) have demonstrated promise in text-generation tasks. However, their educational applications, particularly for domain-specific queries, remain underexplored. This study investigates LLMs' capabilities in educational scenarios, focusing on concept graph recovery and question-answering (QA). We assess LLMs' zero-shot performance in creating domain-specific concept graphs and introduce TutorQA, a new expert-verified NLP-focused benchmark for scientific graph reasoning and QA. TutorQA consists of five tasks with 500 QA pairs. To tackle TutorQA queries, we present CGLLM, a pipeline integrating concept graphs with LLMs for answering diverse questions. Our results indicate that LLMs' zero-shot concept graph recovery is competitive with supervised methods, showing an average 3% F1 score improvement. In TutorQA tasks, LLMs achieve up to 26% F1 score enhancement. Moreo
    
[^77]: CEV-LM: 受控编辑向量语言模型用于塑造自然语言生成

    CEV-LM: Controlled Edit Vector Language Model for Shaping Natural Language Generations

    [https://arxiv.org/abs/2402.14290](https://arxiv.org/abs/2402.14290)

    CEV-LM 是一个轻量、半自回归语言模型，利用受限制的编辑向量控制文本的速度、音量和绕圈度量，从而更精准地定制生成的文本形状，比现有控制方法具有更好的控制效果。

    

    随着大规模语言模型成为文本生成的标准，需要更多地定制生成的紧凑性、针对性和信息性，具体取决于受众/应用程序。现有的控制方法主要调整文本的语义（如情感、主题）、结构（如句法树、词性）和词汇（如关键词/短语包含），但无法实现复杂的目标，如控制文本的复杂性和可读性,我们引入了CEV-LM——一种轻量级、半自回归语言模型，利用受限制的编辑向量来控制三个补充度量（速度、音量和绕圈），以量化文本的形状（例如内容的节奏）。 我们研究了一系列最先进的CTG模型，发现CEV-LM 可显著更有针对性和精确地控制这三个量。

    arXiv:2402.14290v1 Announce Type: new  Abstract: As large-scale language models become the standard for text generation, there is a greater need to tailor the generations to be more or less concise, targeted, and informative, depending on the audience/application. Existing control approaches primarily adjust the semantic (e.g., emotion, topics), structural (e.g., syntax tree, parts-of-speech), and lexical (e.g., keyword/phrase inclusion) properties of text, but are insufficient to accomplish complex objectives such as pacing which control the complexity and readability of the text. In this paper, we introduce CEV-LM - a lightweight, semi-autoregressive language model that utilizes constrained edit vectors to control three complementary metrics (speed, volume, and circuitousness) that quantify the shape of text (e.g., pacing of content). We study an extensive set of state-of-the-art CTG models and find that CEV-LM provides significantly more targeted and precise control of these three m
    
[^78]: TinyLLaVA：小规模大型多模态模型框架

    TinyLLaVA: A Framework of Small-scale Large Multimodal Models

    [https://arxiv.org/abs/2402.14289](https://arxiv.org/abs/2402.14289)

    TinyLLaVA框架使得小型多模态模型能够通过更好的数据质量和训练方案达到与大型模型相媲美的性能，最佳模型TinyLLaVA-3.1B在整体性能上优于现有的7B模型。

    

    我们提出了TinyLLaVA框架，为设计和分析小规模大型多模态模型（LMMs）提供了统一视角。我们从实证角度研究了不同的视觉编码器、连接模块、语言模型、训练数据和训练方案的影响。我们的大量实验表明，更好质量的数据结合更好的训练方案，使得较小的LMMs能够在整体性能上与更大的LMMs保持一致。在我们的框架下，我们训练了一系列小规模LMMs。我们最佳模型TinyLLaVA-3.1B在与现有的7B模型（如LLaVA-1.5和Qwen-VL）进行比较时，达到了更好的整体性能。我们希望我们的发现可以作为未来研究在数据扩展、训练设置和模型选择方面的基准。我们的模型权重和代码将被公开。

    arXiv:2402.14289v1 Announce Type: cross  Abstract: We present the TinyLLaVA framework that provides a unified perspective in designing and analyzing the small-scale Large Multimodal Models (LMMs). We empirically study the effects of different vision encoders, connection modules, language models, training data and training recipes. Our extensive experiments showed that better quality of data combined with better training recipes, smaller LMMs can consistently achieve on-par performances compared to bigger LMMs. Under our framework, we train a family of small-scale LMMs. Our best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL. We hope our findings can serve as baselines for future research in terms of data scaling, training setups and model selections. Our model weights and codes will be made public.
    
[^79]: 使用音素表示减缓语言差异，实现稳健的多语言理解

    Mitigating the Linguistic Gap with Phonemic Representations for Robust Multilingual Language Understanding

    [https://arxiv.org/abs/2402.14279](https://arxiv.org/abs/2402.14279)

    通过使用音素表示，本文提出了一种新颖的解决方案来减缓高资源语言和低资源语言之间的性能差距，并通过实证研究和理论分析证明了其有效性。

    

    为了改善多语言理解，通常需要在训练阶段使用多种语言，依赖复杂的训练技术，并且在高资源语言和低资源语言之间存在显著的性能差距。我们假设语言之间的性能差距受到这些语言之间的语言差异的影响，并通过使用音素表示（具体来说，将音素作为输入标记输入到语言模型中，而不是子词）提供了一种新颖的解决方案，以实现稳健的多语言建模。我们通过三个跨语言任务的定量证据展示了音素表示的有效性，这进一步得到了对跨语言性能差距的理论分析的证明。

    arXiv:2402.14279v1 Announce Type: cross  Abstract: Approaches to improving multilingual language understanding often require multiple languages during the training phase, rely on complicated training techniques, and -- importantly -- struggle with significant performance gaps between high-resource and low-resource languages. We hypothesize that the performance gaps between languages are affected by linguistic gaps between those languages and provide a novel solution for robust multilingual language modeling by employing phonemic representations (specifically, using phonemes as input tokens to LMs rather than subwords). We present quantitative evidence from three cross-lingual tasks that demonstrate the effectiveness of phonemic representation, which is further justified by a theoretical analysis of the cross-lingual performance gap.
    
[^80]: GATE X-E：弱性别语言的性别公平翻译挑战集

    GATE X-E : A Challenge Set for Gender-Fair Translations from Weakly-Gendered Languages

    [https://arxiv.org/abs/2402.14277](https://arxiv.org/abs/2402.14277)

    引入了GATE X-E挑战集，包含了从土耳其语、匈牙利语、芬兰语和波斯语翻译成英语的人类翻译，旨在评估弱性别语言到英语的翻译中的性别偏见并提出缓解策略。

    

    arXiv:2402.14277v1 公告类型：跨领域 摘要：神经机器翻译（NMT）在质量和采用上持续改善，但性别偏见的无意延续仍然是一个重要关注点。尽管有大量关于从弱性别语言翻译成英语的性别偏见的研究，但目前还没有用于评估这一现象或评估缓解策略的基准。为了弥补这一空白，我们引入了GATE X-E，这是GATE（Rarrick等人，2023）语料库的扩展，由人类翻译组成，从土耳其语、匈牙利语、芬兰语和波斯语翻译成英语。每种翻译都附有女性、男性和中性变体。该数据集每种语言对之间包含1250到1850个实例，包含具有各种句子长度和领域的自然句子，挑战着翻译重写者在各种语言现象上。此外，我们还提出了一个使用GPT构建的翻译性别重写解决方案。

    arXiv:2402.14277v1 Announce Type: cross  Abstract: Neural Machine Translation (NMT) continues to improve in quality and adoption, yet the inadvertent perpetuation of gender bias remains a significant concern. Despite numerous studies on gender bias in translations into English from weakly gendered-languages, there are no benchmarks for evaluating this phenomenon or for assessing mitigation strategies. To address this gap, we introduce GATE X-E, an extension to the GATE (Rarrick et al., 2023) corpus, that consists of human translations from Turkish, Hungarian, Finnish, and Persian into English. Each translation is accompanied by feminine, masculine, and neutral variants. The dataset, which contains between 1250 and 1850 instances for each of the four language pairs, features natural sentences with a wide range of sentence lengths and domains, challenging translation rewriters on various linguistic phenomena. Additionally, we present a translation gender rewriting solution built with GPT
    
[^81]: 语言模型能否作为大规模知识库？

    Can Language Models Act as Knowledge Bases at Scale?

    [https://arxiv.org/abs/2402.14273](https://arxiv.org/abs/2402.14273)

    大型语言模型在存储、回忆和推理大规模知识方面的表现被探讨，研究结果表明其具有一定的潜力。

    

    大型语言模型(LLMs)已经在通过大规模预训练理解和生成复杂查询的过程中展现出了非凡的熟练程度。然而，这些模型在大规模结构化知识，特别是明确涵盖丰富事实信息的世界知识的记忆和推理能力仍然存在疑问。针对这一空白，我们的研究探讨了LLMs是否能够有效地存储、回忆和推理大规模知识，与最新知识库（KBs）如Wikidata相媲美。具体而言，我们专注于研究可行性的三个关键方面：(1) LLMs在记忆大规模KB中确切知识方面的效率；(2) 在自然语言查询的回应中回忆记忆知识的灵活性；(3) 通过推理推断新知识的能力。我们的研究结果表明，虽然LLMs具有潜力

    arXiv:2402.14273v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating responses to complex queries through large-scale pre-training. However, the efficacy of these models in memorizing and reasoning among large-scale structured knowledge, especially world knowledge that explicitly covers abundant factual information remains questionable. Addressing this gap, our research investigates whether LLMs can effectively store, recall, and reason with knowledge on a large scale comparable to latest knowledge bases (KBs) such as Wikidata. Specifically, we focus on three crucial aspects to study the viability: (1) the efficiency of LLMs with different sizes in memorizing the exact knowledge in the large-scale KB; (2) the flexibility of recalling the memorized knowledge in response to natural language queries; (3) the capability to infer new knowledge through reasoning. Our findings indicate that while LLMs hold promi
    
[^82]: Qsnail：用于顺序问题生成的问卷数据集

    Qsnail: A Questionnaire Dataset for Sequential Question Generation

    [https://arxiv.org/abs/2402.14272](https://arxiv.org/abs/2402.14272)

    Qsnail提出了一个用于顺序问题生成的问卷数据集，填补了高质量数据集稀缺导致的自动生成问卷领域关注不足的空白。

    

    问卷是一种专业的研究方法，用于对人类意见、偏好、态度和行为进行定性与定量分析。然而，设计和评估问卷需要付出巨大努力，因为它们的结构复杂而错综复杂。问卷包含一系列问题，这些问题必须符合涉及问题、选项和整体结构的复杂约束。具体而言，问题应与给定的研究主题和意图相关且具体。选项应根据问题量身定制，确保它们是互斥的、完整的，并且有合理的顺序。此外，问题顺序应遵循逻辑顺序，将相似主题分组在一起。因此，自动生成问卷面临重大挑战，由于高质量数据集稀缺，该领域受到的关注有限。

    arXiv:2402.14272v1 Announce Type: new  Abstract: The questionnaire is a professional research methodology used for both qualitative and quantitative analysis of human opinions, preferences, attitudes, and behaviors. However, designing and evaluating questionnaires demands significant effort due to their intricate and complex structure. Questionnaires entail a series of questions that must conform to intricate constraints involving the questions, options, and overall structure. Specifically, the questions should be relevant and specific to the given research topic and intent. The options should be tailored to the questions, ensuring they are mutually exclusive, completed, and ordered sensibly. Moreover, the sequence of questions should follow a logical order, grouping similar topics together. As a result, automatically generating questionnaires presents a significant challenge and this area has received limited attention primarily due to the scarcity of high-quality datasets. To address
    
[^83]: 大型语言模型能够检测科学新闻报道中的错误信息吗？

    Can Large Language Models Detect Misinformation in Scientific News Reporting?

    [https://arxiv.org/abs/2402.14268](https://arxiv.org/abs/2402.14268)

    大型语言模型探测科学报道中的错误信息的可行性，绕过生成明确标记索赔的步骤，处理现实场景中可能不存在明确标记索赔的挑战。

    

    科学事实经常被在流行媒体中操纵，意图影响公众舆论和行动，正如在COVID-19大流行期间所证实的那样。在科学领域中自动检测错误信息具有挑战性，因为这两种媒体类型的写作风格有着明显不同，并且仍处于萌芽阶段。本文的核心研究问题是是否可以利用大型语言模型(LLMs)来检测科学报道中的错误信息。

    arXiv:2402.14268v1 Announce Type: cross  Abstract: Scientific facts are often spun in the popular press with the intent to influence public opinion and action, as was evidenced during the COVID-19 pandemic. Automatic detection of misinformation in the scientific domain is challenging because of the distinct styles of writing in these two media types and is still in its nascence. Most research on the validity of scientific reporting treats this problem as a claim verification challenge. In doing so, significant expert human effort is required to generate appropriate claims. Our solution bypasses this step and addresses a more real-world scenario where such explicit, labeled claims may not be available. The central research question of this paper is whether it is possible to use large language models (LLMs) to detect misinformation in scientific reporting. To this end, we first present a new labeled dataset SciNews, containing 2.4k scientific news stories drawn from trusted and untrustwo
    
[^84]: 单词序列熵：走向自由形式医学问答应用及其不确定性估计

    Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond

    [https://arxiv.org/abs/2402.14259](https://arxiv.org/abs/2402.14259)

    本论文提出了一种新方法单词序列熵（WSE），用于在自由形式医学问答任务中量化答案的不确定性，相比其他基线方法表现更优秀。

    

    不确定性估计在确保安全关键的人工智能系统与人类互动的可靠性中发挥关键作用，尤其在医疗领域尤为重要。然而，在自由形式的医学问答任务中，尚未建立一种通用方法来量化答案的不确定性，其中无关的词汇和语序含有有限的语义信息可能是不确定性的主要来源，这是由于生成不平等的存在。本文提出了单词序列熵（WSE），该方法根据语义相关性在单词和序列级别上校准不确定性比例，在不确定性量化时更加强调关键词和更相关的序列。我们在5个自由形式医学问答数据集上，利用7种“现成的”大语言模型（LLMs）将WSE与6种基线方法进行比较，并展示了WSE在性能上的优越性。

    arXiv:2402.14259v1 Announce Type: cross  Abstract: Uncertainty estimation plays a pivotal role in ensuring the reliability of safety-critical human-AI interaction systems, particularly in the medical domain. However, a general method for quantifying the uncertainty of free-form answers has yet to be established in open-ended medical question-answering (QA) tasks, where irrelevant words and sequences with limited semantic information can be the primary source of uncertainty due to the presence of generative inequality. In this paper, we propose the Word-Sequence Entropy (WSE), which calibrates the uncertainty proportion at both the word and sequence levels according to the semantic relevance, with greater emphasis placed on keywords and more relevant sequences when performing uncertainty quantification. We compare WSE with 6 baseline methods on 5 free-form medical QA datasets, utilizing 7 "off-the-shelf" large language models (LLMs), and show that WSE exhibits superior performance on ac
    
[^85]: 鹰：来自真实互动的道德数据集

    Eagle: Ethical Dataset Given from Real Interactions

    [https://arxiv.org/abs/2402.14258](https://arxiv.org/abs/2402.14258)

    该论文提出了鹰数据集，从ChatGPT和用户的真实互动中提取，展示了社会偏见、毒性和不道德问题。实验证明鹰捕捉到了现有用于评估和减轻这些道德挑战的数据集所未覆盖的补充方面。

    

    最近的研究表明，大型语言模型（LLMs）存在道德相关问题，如社会偏见、缺乏道德推理和生成具有攻击性内容。现有的评估指标和方法处理这些道德挑战使用诱使人类制作包含道德问题的实例的数据集。因此，这些数据并不反映用户在日常使用LLM服务时实际提供的提示。这可能不会导致开发能够解决实际应用中出现的道德挑战的安全LLMs。在本文中，我们创建了从ChatGPT和用户之间的真实互动中提取的鹰数据集，展示了社会偏见、毒性和不道德问题。我们的实验表明，鹰捕捉了现有用于评估和减轻此类道德挑战的数据集所未涵盖的补充方面。

    arXiv:2402.14258v1 Announce Type: new  Abstract: Recent studies have demonstrated that large language models (LLMs) have ethical-related problems such as social biases, lack of moral reasoning, and generation of offensive content. The existing evaluation metrics and methods to address these ethical challenges use datasets intentionally created by instructing humans to create instances including ethical problems. Therefore, the data does not reflect prompts that users actually provide when utilizing LLM services in everyday contexts. This may not lead to the development of safe LLMs that can address ethical challenges arising in real-world applications. In this paper, we create Eagle datasets extracted from real interactions between ChatGPT and users that exhibit social biases, toxicity, and immoral problems. Our experiments show that Eagle captures complementary aspects, not covered by existing datasets proposed for evaluation and mitigation of such ethical challenges. Our code is publ
    
[^86]: 在支持数据存在的情况下进行框架构建：以美国经济新闻为例

    Framing in the Presence of Supporting Data: A Case Study in U.S. Economic News

    [https://arxiv.org/abs/2402.14224](https://arxiv.org/abs/2402.14224)

    本文提出了一个计算框架，旨在分析主流媒体在报道经济消息时的编辑选择，通过对经济指标的报道进行框架分析，我们可以理解出版物选择和构架的方式。

    

    主流媒体在选择何事物进行报道以及如何进行报道方面有很大的自由裁量权。这些选择会对人们所了解的信息和随后的行为产生真实世界的影响。然而，缺乏客观的评估编辑选择的度量使得这一领域的研究特别困难。本文认为在一些有支持数据存在的值得报道的话题中，可以提出一个计算框架来分析编辑选择。我们选择经济作为研究重点，因为经济指标的报道为我们提供了一个相对容易确定各种出版物选择和构架的方式。这些指标为我们提供了一个有关经济表现的真实情况，相对于出版物对其进行报道的方式。为了实现这一目标，我们将框架预测定义为一组相互依赖的任务。

    arXiv:2402.14224v1 Announce Type: new  Abstract: The mainstream media has much leeway in what it chooses to cover and how it covers it. These choices have real-world consequences on what people know and their subsequent behaviors. However, the lack of objective measures to evaluate editorial choices makes research in this area particularly difficult. In this paper, we argue that there are newsworthy topics where objective measures exist in the form of supporting data and propose a computational framework to analyze editorial choices in this setup. We focus on the economy because the reporting of economic indicators presents us with a relatively easy way to determine both the selection and framing of various publications. Their values provide a ground truth of how the economy is doing relative to how the publications choose to cover it. To do this, we define frame prediction as a set of interdependent tasks. At the article level, we learn to identify the reported stance towards the gene
    
[^87]: 面向公平文本嵌入的内容条件去偏方法

    Content Conditional Debiasing for Fair Text Embedding

    [https://arxiv.org/abs/2402.14208](https://arxiv.org/abs/2402.14208)

    通过在内容条件下确保敏感属性与文本嵌入之间的条件独立性，我们提出了一种可以改善公平性的新方法，在保持效用的同时，解决了缺乏适当训练数据的问题。

    

    在自然语言处理（NLP）中，减轻机器学习模型中的偏见引起了越来越多的关注。然而，只有少数研究集中在公平的文本嵌入上，这对实际应用至关重要且具有挑战性。本文提出了一种学习公平文本嵌入的新方法。我们通过确保在内容条件下敏感属性与文本嵌入之间的条件独立性来实现公平性，同时保持效用权衡。具体来说，我们强制要求具有不同敏感属性但相同内容的文本的嵌入与其对应中立文本的嵌入保持相同的距离。此外，我们通过使用大型语言模型（LLMs）将文本增强为不同的敏感组，来解决缺乏适当训练数据的问题。我们广泛的评估表明，我们的方法有效地提高了公平性同时保持了嵌入的效用。

    arXiv:2402.14208v1 Announce Type: cross  Abstract: Mitigating biases in machine learning models has gained increasing attention in Natural Language Processing (NLP). Yet, only a few studies focus on fair text embeddings, which are crucial yet challenging for real-world applications. In this paper, we propose a novel method for learning fair text embeddings. We achieve fairness while maintaining utility trade-off by ensuring conditional independence between sensitive attributes and text embeddings conditioned on the content. Specifically, we enforce that embeddings of texts with different sensitive attributes but identical content maintain the same distance toward the embedding of their corresponding neutral text. Furthermore, we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups. Our extensive evaluations demonstrate that our approach effectively improves fairness while preserving the utility of embed
    
[^88]: 用大型语言模型从头开始辅助撰写类似维基百科文章

    Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models

    [https://arxiv.org/abs/2402.14207](https://arxiv.org/abs/2402.14207)

    提出了一种名为STORM的写作系统，用于通过检索和多视角提问合成主题概要，以辅助从头开始写类似维基百科的文章。

    

    我们研究如何应用大型语言模型从头开始撰写基于事实和有条理的长篇文章，使其在广度和深度上与维基百科页面可媲美。这一尚未深入研究的问题在撰写前阶段提出了新的挑战，包括如何研究主题并准备大纲以便撰写。我们提出了STORM，一个用于通过检索和多视角提问进行主题概要合成的写作系统。STORM模拟了撰写前阶段，其中（1）发现研究给定主题的多样化观点，（2）模拟会话，撰写持有不同观点的作者向基于可信互联网来源的主题专家提问，（3）整理收集到的信息以创建大纲。为了评估，我们整理了FreshWiki，一个包含最新高质量维基百科文章的数据集，并制定了大纲评估指标以评估撰写前阶段。

    arXiv:2402.14207v1 Announce Type: cross  Abstract: We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline.   For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from 
    
[^89]: 探索对话理解：领域知识与大型语言模型

    Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models

    [https://arxiv.org/abs/2402.14200](https://arxiv.org/abs/2402.14200)

    本文提出了一种系统方法，结合领域知识和大型语言模型来更好地表示危机辅导员与求助者之间的对话，实验证明加入领域知识和模型特征可以提高模型性能约15%。

    

    理解咨询对话的动态是一项重要任务，然而，即使最近基于Transformer的预训练语言模型取得了进展，这仍然是一个具有挑战性的NLP问题。本文提出了一种系统方法，以检验领域知识和大型语言模型（LLMs）在更好地表示危机辅导员与求助者之间对话方面的有效性。我们在实证中显示，最先进的语言模型如Transformer模型和GPT模型未能预测对话结果。为了为对话提供更丰富的上下文，我们结合人工注释的领域知识和LLM生成的特征；简单地整合领域知识和LLM特征将模型性能提升约15%。我们认为，当它们被用作额外的上下文时，领域知识和LLM生成的特征都可以被利用来更好地表征咨询对话。

    arXiv:2402.14200v1 Announce Type: new  Abstract: Understanding the dynamics of counseling conversations is an important task, yet it is a challenging NLP problem regardless of the recent advance of Transformer-based pre-trained language models. This paper proposes a systematic approach to examine the efficacy of domain knowledge and large language models (LLMs) in better representing conversations between a crisis counselor and a help seeker. We empirically show that state-of-the-art language models such as Transformer-based models and GPT models fail to predict the conversation outcome. To provide richer context to conversations, we incorporate human-annotated domain knowledge and LLM-generated features; simple integration of domain knowledge and LLM features improves the model performance by approximately 15%. We argue that both domain knowledge and LLM-generated features can be exploited to better characterize counseling conversations when they are used as an additional context to c
    
[^90]: 学习减少: 在向大型语言模型提供结构化数据时的最佳表示

    Learning to Reduce: Optimal Representations of Structured Data in Prompting Large Language Models

    [https://arxiv.org/abs/2402.14195](https://arxiv.org/abs/2402.14195)

    该研究提出了一个名为学习减少的框架，利用强化学习减少输入上下文，以提高固定大型语言模型的推理性能，并展现了在不同数据集上的泛化能力。

    

    大型语言模型（LLMs）被广泛用作通用人工智能代理，展示出在许多下游任务上表现出可比较的性能。然而，现有研究表明，LLMs很难将结构化数据（例如KG、表格、数据库）整合到其提示中；LLMs需要在推理之前要么理解长文本数据，要么选择最相关的证据，而这两种方法都不是微不足道的。 本文提出了一个名为学习减少（Learning to Reduce）的框架，通过微调语言模型，根据任务描述和上下文输入生成输入上下文的精简版本。该模型通过On-Policy强化学习学习减少输入上下文，并旨在提高固定LLM的推理性能。实验结果表明，我们的模型不仅在从输入上下文中选择相关证据方面取得了可比较的准确性，而且在不同数据集上表现出了通用性。

    arXiv:2402.14195v1 Announce Type: new  Abstract: Large Language Models (LLMs) have been widely used as general-purpose AI agents showing comparable performance on many downstream tasks. However, existing work shows that it is challenging for LLMs to integrate structured data (e.g. KG, tables, DBs) into their prompts; LLMs need to either understand long text data or select the most relevant evidence prior to inference, and both approaches are not trivial.   In this paper, we propose a framework, Learning to Reduce, that fine-tunes a language model to generate a reduced version of an input context, given a task description and context input. The model learns to reduce the input context using On-Policy Reinforcement Learning and aims to improve the reasoning performance of a fixed LLM. Experimental results illustrate that our model not only achieves comparable accuracies in selecting the relevant evidence from an input context, but also shows generalizability on different datasets. We fur
    
[^91]: 从采纳到适应：追踪新表情符号在Twitter上的传播

    From Adoption to Adaption: Tracing the Diffusion of New Emojis on Twitter

    [https://arxiv.org/abs/2402.14187](https://arxiv.org/abs/2402.14187)

    本研究通过分析英文推文数据集，探讨了新表情符号在Twitter上的传播情况，发现早期采纳者规模和表情符号语义对其流行度至关重要，并提出了一个新框架来解释新表情符号，从而改善情感分类性能。

    

    在社交媒体快速发展的背景下，Unicode发布新表情符号版本提供了一个探索数字语言演变的结构化机会。通过分析大量抽样的英文推文数据集，我们研究了新发布的表情符号如何获得关注并如何在含义上演变。我们发现早期采纳者的社区规模和表情符号语义对于确定它们的受欢迎程度至关重要。在传播过程中，某些表情符号经历了显著的含义变化和情感关联的变化。此外，我们提出了一个利用语言模型提取具有语义上相似上下文的单词和既有表情符号的新框架，这有助于解释新表情符号。该框架通过用熟悉的表情符号替代未知的新表情符号，提高了情感分类性能。这项研究为我们理解新语言的采用提供了新的视角。

    arXiv:2402.14187v1 Announce Type: cross  Abstract: In the rapidly evolving landscape of social media, the introduction of new emojis in Unicode release versions presents a structured opportunity to explore digital language evolution. Analyzing a large dataset of sampled English tweets, we examine how newly released emojis gain traction and evolve in meaning. We find that community size of early adopters and emoji semantics are crucial in determining their popularity. Certain emojis experienced notable shifts in the meanings and sentiment associations during the diffusion process. Additionally, we propose a novel framework utilizing language models to extract words and pre-existing emojis with semantically similar contexts, which enhances interpretation of new emojis. The framework demonstrates its effectiveness in improving sentiment classification performance by substituting unknown new emojis with familiar ones. This study offers a new perspective in understanding how new language un
    
[^92]: 基于拓扑数据分析的语言模型多样性集成

    Diversity-Aware Ensembling of Language Models Based on Topological Data Analysis

    [https://arxiv.org/abs/2402.14184](https://arxiv.org/abs/2402.14184)

    基于拓扑数据分析的方法，通过估计NLP模型集成的权重，提高了集成模型的质量，提高了文本分类准确性和相关不确定性估计。

    

    集成是提高机器学习模型性能的重要工具。在与自然语言处理相关的情况下，由于开源中存在多个大型模型，集成有助于提升方法的性能。然而，现有方法主要依赖于对集成中每个模型的预测进行简单平均，对每个模型赋予相同权重，忽略了模型质量和一致性的差异。我们提出利用不仅单个模型表现知识，还使用它们之间的相似性来估计NLP模型集成的权重。通过采用基于拓扑数据分析（TDA）的距离度量，我们改进了我们的集成。文本分类准确性和相关不确定性估计的质量得到提高。

    arXiv:2402.14184v1 Announce Type: cross  Abstract: Ensembles are important tools for improving the performance of machine learning models. In cases related to natural language processing, ensembles boost the performance of a method due to multiple large models available in open source. However, existing approaches mostly rely on simple averaging of predictions by ensembles with equal weights for each model, ignoring differences in the quality and conformity of models. We propose to estimate weights for ensembles of NLP models using not only knowledge of their individual performance but also their similarity to each other. By adopting distance measures based on Topological Data Analysis (TDA), we improve our ensemble. The quality improves for both text classification accuracy and relevant uncertainty estimation.
    
[^93]: 孟加拉AI：利用大型语言模型进行族裔媒体机器翻译的框架

    Bangla AI: A Framework for Machine Translation Utilizing Large Language Models for Ethnic Media

    [https://arxiv.org/abs/2402.14179](https://arxiv.org/abs/2402.14179)

    该研究探讨了如何在民族媒体领域整合大型语言模型和多语言机器翻译，以提升新闻翻译、搜索和分类的效率和准确性。

    

    民族媒体是为驻留在东道国的侨民社区提供服务的重要平台，既服务于这些社区制作内容，又让他们获取信息。与使用东道国语言不同，民族媒体以移民社区的语言发布新闻。举例来说，在美国，孟加拉族裔媒体使用孟加拉语而不是英语发布新闻。本研究探讨了在民族媒体领域潜在整合大型语言模型（LLM）和多语言机器翻译（MMT）的可能性。它着重探讨了在新闻翻译、搜索和分类的各个方面中使用LLM进行MMT的变革潜力。论文概述了一个理论框架，阐明了如何将LLM和MMT整合到民族媒体的新闻搜索和翻译过程中。此外，它还简要讨论了与LLM和MMT整合相关的潜在伦理挑战。

    arXiv:2402.14179v1 Announce Type: cross  Abstract: Ethnic media, which caters to diaspora communities in host nations, serves as a vital platform for these communities to both produce content and access information. Rather than utilizing the language of the host nation, ethnic media delivers news in the language of the immigrant community. For instance, in the USA, Bangla ethnic media presents news in Bangla rather than English. This research delves into the prospective integration of large language models (LLM) and multi-lingual machine translations (MMT) within the ethnic media industry. It centers on the transformative potential of using LLM in MMT in various facets of news translation, searching, and categorization. The paper outlines a theoretical framework elucidating the integration of LLM and MMT into the news searching and translation processes for ethnic media. Additionally, it briefly addresses the potential ethical challenges associated with the incorporation of LLM and MMT
    
[^94]: TOOLVERIFIER: 通过自验证实现对新工具的泛化

    TOOLVERIFIER: Generalization to New Tools via Self-Verification

    [https://arxiv.org/abs/2402.14158](https://arxiv.org/abs/2402.14158)

    通过自验证方法，本研究提出了一种区分新工具的方法，通过对比性自问问题来实现工具选择和参数生成，进一步提升了语言模型对新工具的学习能力。

    

    将语言模型教会如何使用工具是迈向构建通用助手的重要里程碑，但仍然是一个未解之谜。虽然在针对特定工具的微调方面取得了显著进展，但语言模型仍然在如何从仅有少数示例中强大地使用新工具方面遇到困难。在这项工作中，我们引入了一种自验证方法，通过在工具选择和参数生成过程中进行对比性自问问题来区分近似的候选工具。我们利用Llama-2 70B构建了合成的高质量自生成数据，用于实现这一目标，我们打算将其公开发布。在ToolBench基准测试中的4项任务上进行了大量实验，包括17个未见过的工具，展示了在少样本基线测试中平均提升了22%，即使在候选工具之间的区别微妙之处的情况下也是如此。

    arXiv:2402.14158v1 Announce Type: new  Abstract: Teaching language models to use tools is an important milestone towards building general assistants, but remains an open problem. While there has been significant progress on learning to use specific tools via fine-tuning, language models still struggle with learning how to robustly use new tools from only a few demonstrations. In this work we introduce a self-verification method which distinguishes between close candidates by self-asking contrastive questions during (1) tool selection; and (2) parameter generation. We construct synthetic, high-quality, self-generated data for this goal using Llama-2 70B, which we intend to release publicly. Extensive experiments on 4 tasks from the ToolBench benchmark, consisting of 17 unseen tools, demonstrate an average improvement of 22% over few-shot baselines, even in scenarios where the distinctions between candidate tools are finely nuanced.
    
[^95]: 基于相似性的领域排序能够减少意图识别中的灾难性遗忘吗？

    Can Similarity-Based Domain-Ordering Reduce Catastrophic Forgetting for Intent Recognition?

    [https://arxiv.org/abs/2402.14155](https://arxiv.org/abs/2402.14155)

    研究探讨了三种领域排序策略对生成式意图识别模型继续学习性能的影响，填补了现有研究中对此方面未探索的空白。

    

    任务导向的对话系统被期望在部署后能够处理不断增长的意图和领域，甚至在支持越来越多功能的情况下也能做到。为了达到这个期望，就变得至关重要去减轻在诸如意图识别等任务的继续学习（CL）设置中发生的灾难性遗忘问题（CF）。虽然现有的对话系统研究已经探索了基于重放和正则化的方法以达到这个目的，但领域排序对意图识别模型的继续学习性能的影响尚未被探索。如果理解得当，领域排序有潜力成为一个能够与现有技术如经验重放并行使用的方法。我们的工作通过比较三种领域排序策略（最小和路径、最大和路径、随机）对生成式意图识别模型的继续学习性能的影响来填补这一空白。我们的发现表明

    arXiv:2402.14155v1 Announce Type: cross  Abstract: Task-oriented dialogue systems are expected to handle a constantly expanding set of intents and domains even after they have been deployed to support more and more functionalities. To live up to this expectation, it becomes critical to mitigate the catastrophic forgetting problem (CF) that occurs in continual learning (CL) settings for a task such as intent recognition. While existing dialogue systems research has explored replay-based and regularization-based methods to this end, the effect of domain ordering on the CL performance of intent recognition models remains unexplored. If understood well, domain ordering has the potential to be an orthogonal technique that can be leveraged alongside existing techniques such as experience replay. Our work fills this gap by comparing the impact of three domain-ordering strategies (min-sum path, max-sum path, random) on the CL performance of a generative intent recognition model. Our findings r
    
[^96]: 在社交媒体平台上对多模态大型语言模型进行基准测试

    MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms

    [https://arxiv.org/abs/2402.14154](https://arxiv.org/abs/2402.14154)

    该研究介绍了MM-Soc，一个旨在评估多模态大型语言模型（MLLMs）对社交媒体内容理解的综合基准，通过对十种大小变体的四个开源MLLMs进行详尽评估，发现了显著的性能差异。

    

    社交媒体平台是多模态信息交流的中心，包括文本、图片和视频，这使得机器难以理解在线空间中交互所关联的信息或情绪。多模态大型语言模型（MLLMs）已经成为解决这些挑战的一个有前途的解决方案，但是它们在准确解释人类情绪和诸如虚假信息等复杂内容方面存在困难。本文介绍了MM-Soc，一个旨在评估MLLMs对多模态社交媒体内容理解的综合基准。MM-Soc整合了著名的多模态数据集，并融入了一个新颖的大规模YouTube标记数据集，旨在针对从虚假信息检测、仇恨言论检测到社交上下文生成等一系列任务。通过对四个开源MLLMs的十种不同规模变体进行详尽评估，我们发现了显著的性能差异，凸显出了对性能平衡的需求。

    arXiv:2402.14154v1 Announce Type: new  Abstract: Social media platforms are hubs for multimodal information exchange, encompassing text, images, and videos, making it challenging for machines to comprehend the information or emotions associated with interactions in online spaces. Multimodal Large Language Models (MLLMs) have emerged as a promising solution to address these challenges, yet struggle with accurately interpreting human emotions and complex contents like misinformation. This paper introduces MM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of multimodal social media content. MM-Soc compiles prominent multimodal datasets and incorporates a novel large-scale YouTube tagging dataset, targeting a range of tasks from misinformation detection, hate speech detection, and social context generation. Through our exhaustive evaluation on ten size-variants of four open-source MLLMs, we have identified significant performance disparities, highlighting the need
    
[^97]: BIRCO：具有复杂目标的信息检索任务基准

    BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives

    [https://arxiv.org/abs/2402.14151](https://arxiv.org/abs/2402.14151)

    BIRCO基准评估基于大型语言模型的信息检索系统对多方面用户目标的检索能力，发现新的检索协议和更强大的模型是解决复杂用户需求的必要条件。

    

    我们提出了具有复杂目标的信息检索(IR)任务基准(BIRCO)。 BIRCO评估IR系统根据多方面用户目标检索文档的能力。 该基准的复杂性和紧凑大小使其适用于评估基于大型语言模型(LLM)的信息检索系统。 我们提出了一个模块化框架，用于研究可能影响LLM在检索任务上的性能的因素，并确定了一个简单的基线模型，该模型与或优于现有方法和更复杂的替代方案。 没有一种方法在所有基准任务上均达到令人满意的性能，这表明需要更强大的模型和新的检索协议来解决复杂的用户需求。

    arXiv:2402.14151v1 Announce Type: cross  Abstract: We present the Benchmark of Information Retrieval (IR) tasks with Complex Objectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve documents given multi-faceted user objectives. The benchmark's complexity and compact size make it suitable for evaluating large language model (LLM)-based information retrieval systems. We present a modular framework for investigating factors that may influence LLM performance on retrieval tasks, and identify a simple baseline model which matches or outperforms existing approaches and more complex alternatives. No approach achieves satisfactory performance on all benchmark tasks, suggesting that stronger models and new retrieval protocols are necessary to address complex user needs.
    
[^98]: 使用动态多重奖励加权的强化学习用于多样式可控生成

    Reinforcement Learning with Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation

    [https://arxiv.org/abs/2402.14146](https://arxiv.org/abs/2402.14146)

    本文提出了一种使用强化学习来控制多种风格生成的方法，通过动态权重调整多重奖励，实现了在生成文本时同时控制多种风格。

    

    风格是表达各种信息的文本中的一个组成部分，包括人际动态（例如正式性）和作者的情绪或态度（例如厌恶）。人类经常同时采用多种风格。一个待解决的问题是如何明确控制大型语言模型，使它们在生成文本时编织目标风格：例如，生成既消极又无毒的文本。先前的工作探讨了对单一风格的控制生成，或者对风格和其他属性的控制生成。在本文中，我们将这扩展到同时控制多种风格。具体而言，我们研究了用于受控多样式生成的强化学习（RL）方法的多种风格奖励的各种公式。这些奖励公式包括来自鉴别器的校准输出以及通过鉴别器梯度幅度进行动态加权。

    arXiv:2402.14146v1 Announce Type: new  Abstract: Style is an integral component of text that expresses a diverse set of information, including interpersonal dynamics (e.g. formality) and the author's emotions or attitudes (e.g. disgust). Humans often employ multiple styles simultaneously. An open question is how large language models can be explicitly controlled so that they weave together target styles when generating text: for example, to produce text that is both negative and non-toxic. Previous work investigates the controlled generation of a single style, or else controlled generation of a style and other attributes. In this paper, we expand this into controlling multiple styles simultaneously. Specifically, we investigate various formulations of multiple style rewards for a reinforcement learning (RL) approach to controlled multi-style generation. These reward formulations include calibrated outputs from discriminators and dynamic weighting by discriminator gradient magnitudes. W
    
[^99]: 结合语言和图模型进行Web上半结构化信息抽取

    Combining Language and Graph Models for Semi-structured Information Extraction on the Web

    [https://arxiv.org/abs/2402.14129](https://arxiv.org/abs/2402.14129)

    GraphScholarBERT是一种结合语言和图模型的信息抽取方法，能够在Web上半结构化信息中提取目标关系，并在零射领域和零射网站设置中将提取F1得分提高34.8％。

    

    关系抽取是在网络上挖掘人类知识的一种高效方式。现有方法依赖于特定领域的训练数据或产生嘈杂的输出。本文着重于从半结构化的网页中提取目标关系，仅给出关系的简短描述。我们提出了GraphScholarBERT，这是一种基于联合图和语言模型结构的开放领域信息提取方法。GraphScholarBERT能够泛化到以前未见过的领域，无需额外数据或训练，并且仅产生与搜索关键字匹配的干净提取结果。实验表明，与零射领域和零射网站设置中的先前工作相比，GraphScholarBERT可以将提取的F1得分提高多达34.8％。

    arXiv:2402.14129v1 Announce Type: cross  Abstract: Relation extraction is an efficient way of mining the extraordinary wealth of human knowledge on the Web. Existing methods rely on domain-specific training data or produce noisy outputs. We focus here on extracting targeted relations from semi-structured web pages given only a short description of the relation. We present GraphScholarBERT, an open-domain information extraction method based on a joint graph and language model structure. GraphScholarBERT can generalize to previously unseen domains without additional data or training and produces only clean extraction results matched to the search keyword. Experiments show that GraphScholarBERT can improve extraction F1 scores by as much as 34.8\% compared to previous work in a zero-shot domain and zero-shot website setting.
    
[^100]: FanOutQA：用于大型语言模型的多跳、多文档问答

    FanOutQA: Multi-Hop, Multi-Document Question Answering for Large Language Models

    [https://arxiv.org/abs/2402.14116](https://arxiv.org/abs/2402.14116)

    FanOutQA 提出了一个高质量的多跳、多文档问答数据集，用于评估大型语言模型在复杂推理能力上的表现，并发现当代模型在长篇上下文中仍有改进交叉文档依赖推理的空间。

    

    一种常见于日常场景中的问题类型是“fan-out”问题，即复杂的多跳、多文档推理问题，需要找到大量实体的信息。然而，目前很少有资源可以评估大型语言模型在这种问题回答能力上的表现。为了更全面地评估LLMs中的复杂推理能力，我们提出了FanOutQA，这是一个高质量的fan-out问题-答案对数据集，包括英文维基百科作为知识库的人工注释分解。我们在数据集上制定了三种基准设置，并对7个LLMs进行了基准测试，包括GPT-4、LLaMA 2、Claude-2.1和Mixtral-8x7B，发现当代模型在长篇上下文中仍有改进推理跨文档依赖的空间。我们提供我们的数据集和开源工具来运行模型，以鼓励在https://fanoutqa.com上进行评估。

    arXiv:2402.14116v1 Announce Type: cross  Abstract: One type of question that is commonly found in day-to-day scenarios is ``fan-out'' questions, complex multi-hop, multi-document reasoning questions that require finding information about a large number of entities. However, there exist few resources to evaluate this type of question-answering capability among large language models. To evaluate complex reasoning in LLMs more fully, we present FanOutQA, a high-quality dataset of fan-out question-answer pairs and human-annotated decompositions with English Wikipedia as the knowledge base. We formulate three benchmark settings across our dataset and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B, finding that contemporary models still have room to improve reasoning over inter-document dependencies in a long context. We provide our dataset and open-source tools to run models to encourage evaluation at https://fanoutqa.com
    
[^101]: 通过少样本标注者调适实现高效主观任务标注与建模

    Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation

    [https://arxiv.org/abs/2402.14101](https://arxiv.org/abs/2402.14101)

    引入了一个新颖的框架，通过少样本标注者调适来实现在主观任务中进行高效标注与建模，最大程度减少标注预算，同时最大化每个标注者的预测性能。

    

    在主观性自然语言处理任务中，由于不存在单一的标准答案，包括不同标注者变得至关重要，因为他们独特的视角显著影响标注。在现实场景中，标注预算通常成为决定数据中包含的视角数量（即标注者）及后续建模的主要因素。我们引入了一个新颖的框架，用于在主观任务中进行标注收集和建模，旨在最大程度地减少标注预算，同时最大化每个标注者的预测性能。我们的框架采用两阶段设计：首先，我们依赖一小组标注者来构建多任务模型，然后，我们通过为每个标注者策略性地标注少量样本，来为新视角增强模型。为了在规模上测试我们的框架，我们介绍并发布了一个独特数据集《道德基础主观语料库》，包含2000个Reddit帖子，由24名标注者进行标注。

    arXiv:2402.14101v1 Announce Type: new  Abstract: In subjective NLP tasks, where a single ground truth does not exist, the inclusion of diverse annotators becomes crucial as their unique perspectives significantly influence the annotations. In realistic scenarios, the annotation budget often becomes the main determinant of the number of perspectives (i.e., annotators) included in the data and subsequent modeling. We introduce a novel framework for annotation collection and modeling in subjective tasks that aims to minimize the annotation budget while maximizing the predictive performance for each annotator. Our framework has a two-stage design: first, we rely on a small set of annotators to build a multitask model, and second, we augment the model for a new perspective by strategically annotating a few samples per annotator. To test our framework at scale, we introduce and release a unique dataset, Moral Foundations Subjective Corpus, of 2000 Reddit posts annotated by 24 annotators for 
    
[^102]: LexC-Gen: 利用大型语言模型和双语词汇表为极低资源语言生成数据

    LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons

    [https://arxiv.org/abs/2402.14086](https://arxiv.org/abs/2402.14086)

    LexC-Gen提出了一种词典条件数据生成方法，可以以大规模生成低资源语言分类任务数据，取得了较好的效果。

    

    低资源语言的数据匮乏可以通过利用双语词典中从高资源语言的标记任务数据进行逐字翻译来解决，然而，双语词典通常与任务数据有限的词汇重叠，导致翻译覆盖和词典利用不佳。我们提出了一种称为LexC-Gen的词典条件数据生成方法，该方法可以大规模生成低资源语言分类任务数据。具体而言，LexC-Gen首先使用双语词典中的高资源语言单词生成与词典兼容的任务数据，然后通过单词翻译将其翻译成低资源语言。在17种极低资源语言中，LexC-Gen生成的数据在性能上与专家翻译的黄金数据竞争力相当，并且在情感分析和主题分类上平均比现有的基于词典的单词翻译方法提高了5.6和8.9个分数。

    arXiv:2402.14086v1 Announce Type: cross  Abstract: Data scarcity in low-resource languages can be addressed with word-to-word translations from labeled task data in high-resource languages using bilingual lexicons. However, bilingual lexicons often have limited lexical overlap with task data, which results in poor translation coverage and lexicon utilization. We propose lexicon-conditioned data generation (LexC-Gen), a method that generates low-resource-language classification task data at scale. Specifically, LexC-Gen first uses high-resource-language words from bilingual lexicons to generate lexicon-compatible task data, and then it translates them into low-resource languages with bilingual lexicons via word translation. Across 17 extremely low-resource languages, LexC-Gen generated data is competitive with expert-translated gold data, and yields on average 5.6 and 8.9 points improvement over existing lexicon-based word translation methods on sentiment analysis and topic classificati
    
[^103]: 从屏幕截图中提高语言理解能力

    Improving Language Understanding from Screenshots

    [https://arxiv.org/abs/2402.14073](https://arxiv.org/abs/2402.14073)

    本文提出了一种屏幕截图语言模型，通过引入新的Patch-and-Text Prediction（PTP）目标来改善文本能力，并取得了与BERT相当的性能。

    

    一种新兴的语言模型家族（LMs）可以处理文本和图像，在单个视觉视图内，有望拓宽图表理解和UI导航等复杂任务。我们称这些模型为屏幕截图语言模型。尽管具有吸引力，但现有的屏幕截图LMs在语言理解任务上明显落后于仅文本的模型。为了弥合这一差距，我们采用了一个简化的设置，其中模型输入是纯文本渲染的屏幕截图，并集中在提高屏幕截图LMs的文本能力。我们提出了一种新颖的Patch-and-Text Prediction（PTP）目标，该目标遮盖和恢复屏幕截图中的图像块和文本。我们还进行了大量消融研究，涉及遮盖率、块大小以及用于提高训练稳定性的设计。我们的预训练模型，仅采用视觉输入，就在8个GLUE中的6个上实现了与BERT相当的性能。

    arXiv:2402.14073v1 Announce Type: new  Abstract: An emerging family of language models (LMs), capable of processing both text and images within a single visual view, has the promise to unlock complex tasks such as chart understanding and UI navigation. We refer to these models as screenshot language models. Despite their appeal, existing screenshot LMs substantially lag behind text-only models on language understanding tasks. To close this gap, we adopt a simplified setting where the model inputs are plain-text-rendered screenshots, and we focus on improving the text ability of screenshot LMs. We propose a novel Patch-and-Text Prediction (PTP) objective, which masks and recovers both image patches of screenshots and text within screenshots. We also conduct extensive ablation studies on masking rates and patch sizes, as well as designs for improving training stability. Our pre-trained model, while solely taking visual inputs, achieves comparable performance with BERT on 6 out of 8 GLUE 
    
[^104]: 利用仅编码器预训练语言模型进行有效关键短语生成

    On Leveraging Encoder-only Pre-trained Language Models for Effective Keyphrase Generation

    [https://arxiv.org/abs/2402.14052](https://arxiv.org/abs/2402.14052)

    本研究探讨了在关键短语生成中利用仅编码器预训练语言模型的效果和优化策略，并比较了不同资源设置下领域内编码器-only和编码器-解码器模型的性能。

    

    本研究探讨了在领域特定编码器模型相比编码器-解码器模型更广泛可用的情况下，将仅编码器预训练语言模型（PLMs）应用于关键短语生成（KPG）。我们研究了三个核心问题：（1）编码器-only PLMs在KPG中的功效，（2）在KPG中使用编码器-only PLMs的最佳架构决策，以及（3）不同资源设置下领域内编码器-only和编码器-解码器PLMs之间的性能比较。我们的研究结果，通过在两个领域进行广泛实验得出，表明尽管带有条件随机场的KPE在识别当前关键短语方面稍微优于仅编码器PLMs，但KPG公式提供了更广泛的关键短语预测。此外，仅编码器PLMs的前缀LM微调出现为KPG的强大且高效策略，优于通用领域的seq2seq PLMs。我们还确定

    arXiv:2402.14052v1 Announce Type: new  Abstract: This study addresses the application of encoder-only Pre-trained Language Models (PLMs) in keyphrase generation (KPG) amidst the broader availability of domain-tailored encoder-only models compared to encoder-decoder models. We investigate three core inquiries: (1) the efficacy of encoder-only PLMs in KPG, (2) optimal architectural decisions for employing encoder-only PLMs in KPG, and (3) a performance comparison between in-domain encoder-only and encoder-decoder PLMs across varied resource settings. Our findings, derived from extensive experimentation in two domains reveal that with encoder-only PLMs, although KPE with Conditional Random Fields slightly excels in identifying present keyphrases, the KPG formulation renders a broader spectrum of keyphrase predictions. Additionally, prefix-LM fine-tuning of encoder-only PLMs emerges as a strong and data-efficient strategy for KPG, outperforming general-domain seq2seq PLMs. We also identify
    
[^105]: CriticBench: 将大型语言模型作为评论家进行评估

    CriticBench: Evaluating Large Language Models as Critic

    [https://arxiv.org/abs/2402.13764](https://arxiv.org/abs/2402.13764)

    CriticBench是一个旨在全面和可靠地评估大型语言模型的评论能力的新型基准，展示了评论能力与任务、响应质量和模型规模之间的关系。

    

    论文提出了 CriticBench，这是一个旨在全面和可靠地评估大型语言模型（LLMs）的四个关键评论能力维度（反馈、比较、改进和元反馈）的新型基准。CriticBench包含九个不同的任务，每个任务评估LLMs在不同质量细粒度水平上评论响应的能力。对开源和闭源LLMs进行的广泛评估揭示了评论能力与任务、响应质量和模型规模之间有趣的关系。CriticBench的数据集、资源和评估工具包将在https://github.com/gmftbyGMFTBY/Cri上公开发布。

    arXiv:2402.13764v1 Announce Type: cross  Abstract: Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces \shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. \shortname~encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for \shortname~will be publicly released at \url{https://github.com/gmftbyGMFTBY/Cri
    
[^106]: $\infty$Bench: 将长上下文评估扩展至超过10万令牌

    $\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens

    [https://arxiv.org/abs/2402.13718](https://arxiv.org/abs/2402.13718)

    提出了$\infty$Bench，第一个以平均数据长度超过10万个令牌的LLM基准，用于评估处理长上下文的能力

    

    处理和推理长上下文对于大型语言模型（LLMs）的许多实际应用至关重要，如文档理解和代理构建。本文提出$\infty$Bench，第一个LLM基准，平均数据长度超过10万个令牌。$\infty$Bench包含涵盖不同领域的合成和现实任务，以英文和中文呈现。$\infty$Bench中的任务旨在需要深刻理解上下文中的长依赖性，并且简单地从上下文中检索有限数量的段落对于这些任务来说是不够的。

    arXiv:2402.13718v1 Announce Type: new  Abstract: Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose $\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens. $\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks
    
[^107]: CODIS：为多模态大型语言模型基准化上下文相关的视觉理解

    CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models

    [https://arxiv.org/abs/2402.13607](https://arxiv.org/abs/2402.13607)

    介绍了CODIS基准，用于评估模型利用自由形式文本提供的上下文来增强视觉理解的能力，发现多模态大型语言模型在此基准上表现未达到人类水平，需要提升模型理解视觉能力。

    

    多模态大型语言模型（MLLMs）在结合视觉和语言的各种任务中展现出了有希望的结果。随着这些模型在研究和应用中变得更加重要，对它们能力进行全面评估的重要性也日益增加。然而，大多数现有的基准测试未考虑到在某些情况下，图像需要在更广泛的上下文中被解释。在这项工作中，我们引入了一个名为CODIS的新基准，旨在评估模型使用在自由形式文本中提供的上下文来增强视觉理解的能力。我们的发现表明，MLLMs在这个基准上始终无法达到人类表现。进一步的分析证实了这些模型难以有效提取和利用上下文信息以提高它们对图像的理解能力。这凸显了提升MLLMs理解视觉能力的迫切需求。

    arXiv:2402.13607v1 Announce Type: cross  Abstract: Multimodal large language models (MLLMs) have demonstrated promising results in a variety of tasks that combine vision and language. As these models become more integral to research and applications, conducting comprehensive evaluations of their capabilities has grown increasingly important. However, most existing benchmarks fail to consider that, in certain situations, images need to be interpreted within a broader context. In this work, we introduce a new benchmark, named as CODIS, designed to assess the ability of models to use context provided in free-form text to enhance visual comprehension. Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images. This underscores the pressing need to enhance the ability of MLLMs to comprehend visuals in a 
    
[^108]: KorNAT：韩国社会价值观和常识的LLM对齐基准

    KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge

    [https://arxiv.org/abs/2402.13605](https://arxiv.org/abs/2402.13605)

    KorNAT是首个用于评估韩国国家对齐的基准，包括社会价值观和常识对齐两个方面，通过对社会价值和常识多项选择题的测试来评估模型的对齐程度。

    

    对于大型语言模型（LLMs）在特定国家得以有效部署，它们必须具有对该国文化和基本知识的理解。为此，我们引入了国家对齐（National Alignment），从社会价值观对齐和常识对齐两个方面衡量LLM与目标国家之间的对齐。社会价值观对齐评估模型对特定国家社会价值观的理解程度，而常识对齐则检验模型对相关基本国家知识的把握情况。我们构建了KorNAT，这是首个衡量与韩国国家对齐的基准。对于社会价值数据集，我们从包括6174名韩国参与者在内的大规模调查中获得了地面真实标签。对于常识数据集，我们基于韩国教科书和GED参考资料构建了样本。KorNAT包含4K和6K个针对社会价值和常识的多项选择题。

    arXiv:2402.13605v1 Announce Type: new  Abstract: For Large Language Models (LLMs) to be effectively deployed in a specific country, they must possess an understanding of the nation's culture and basic knowledge. To this end, we introduce National Alignment, which measures an alignment between an LLM and a targeted country from two aspects: social value alignment and common knowledge alignment. Social value alignment evaluates how well the model understands nation-specific social values, while common knowledge alignment examines how well the model captures basic knowledge related to the nation. We constructed KorNAT, the first benchmark that measures national alignment with South Korea. For the social value dataset, we obtained ground truth labels from a large-scale survey involving 6,174 unique Korean participants. For the common knowledge dataset, we constructed samples based on Korean textbooks and GED reference materials. KorNAT contains 4K and 6K multiple-choice questions for socia
    
[^109]: LongWanjuan: 面向长文本质量的系统化衡量方法

    LongWanjuan: Towards Systematic Measurement for Long Text Quality

    [https://arxiv.org/abs/2402.13583](https://arxiv.org/abs/2402.13583)

    本研究针对长文本评估的差距，引入了一套基于连贯性、凝聚力和复杂性等语言学维度的指标来系统性衡量长文本的质量，并提出了LongWanjuan数据集，有助于提升长文本任务的语言模型训练。

    

    训练数据的质量对于增强基础模型的长文本能力至关重要。尽管通过启发式规则和基于数据多样性和难度的评估来提高数据质量的现有努力，但缺乏专门针对长文本评估的系统化方法。本研究针对这一差距，通过评估三个基本语言学维度（连贯性、凝聚力和复杂性）系统性衡量长文本的质量。受到这三个维度的启发，我们引入了一套旨在评估长文本质量的指标，包括统计和基于预训练语言模型的指标。利用这些指标，我们提出了LongWanjuan，这是一个专门设计用于增强语言模型长文本任务训练的双语数据集，包含超过1600亿标记。在LongWanjuan中，我们将长文本分类为整体性、汇总

    arXiv:2402.13583v1 Announce Type: new  Abstract: The quality of training data are crucial for enhancing the long-text capabilities of foundation models. Despite existing efforts to refine data quality through heuristic rules and evaluations based on data diversity and difficulty, there's a lack of systematic approaches specifically tailored for assessing long texts. Addressing this gap, our work systematically measures the quality of long texts by evaluating three fundamental linguistic dimensions: coherence, cohesion, and complexity. Drawing inspiration from the aforementioned three dimensions, we introduce a suite of metrics designed to evaluate the quality of long texts, encompassing both statistical and pre-trained language model-based ones. Leveraging these metrics, we present LongWanjuan, a bilingual dataset specifically tailored to enhance the training of language models for long-text tasks with over 160B tokens. In LongWanjuan, we categorize long texts into holistic, aggregated
    
[^110]: 基础设施调解员：从结构灾难响应中挖掘未来失效担忧

    Infrastructure Ombudsman: Mining Future Failure Concerns from Structural Disaster Response

    [https://arxiv.org/abs/2402.13528](https://arxiv.org/abs/2402.13528)

    本文开发了一种基础设施调解员系统，用于自动检测特定基础设施问题，通过挖掘社交网络中关于预期失败的担忧，有助于预防和减轻潜在的基础设施失败。

    

    当前研究集中于研究社交媒体上与结构失败相关的讨论，以改进灾难响应策略。然而，检测社交网络帖子中讨论关于预期失败的担忧是未被充分探索的。如果这些担忧被传达给适当的机构，可以帮助预防和减轻潜在的基础设施失败。本文中，我们开发了一种基础设施调解员——用于自动检测特定基础设施问题。我们的工作考虑了美国几起最近的结构失效事件。我们呈现了一份首创性数据集，包括从Reddit和YouTube中挖掘的2,662个社交网络实例，用于这一新颖任务。

    arXiv:2402.13528v1 Announce Type: cross  Abstract: Current research concentrates on studying discussions on social media related to structural failures to improve disaster response strategies. However, detecting social web posts discussing concerns about anticipatory failures is under-explored. If such concerns are channeled to the appropriate authorities, it can aid in the prevention and mitigation of potential infrastructural failures. In this paper, we develop an infrastructure ombudsman -- that automatically detects specific infrastructure concerns. Our work considers several recent structural failures in the US. We present a first-of-its-kind dataset of 2,662 social web instances for this novel task mined from Reddit and YouTube.
    
[^111]: RefuteBench：评估用于大型语言模型的反驳指令遵循

    RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models

    [https://arxiv.org/abs/2402.13463](https://arxiv.org/abs/2402.13463)

    本文提出了一个名为RefuteBench的基准测试，旨在评估大型语言模型对反驳指令的遵循能力，发现LLMs倾向于固执于其内部知识而无法遵从用户反馈。

    

    大型语言模型（LLMs）的应用范围日益扩大。在实际使用中，用户可能根据模型的输出提供反馈，希望得到一个可以根据他们的反馈完成响应的响应模型。然而，模型能否恰当地响应用户的反驳反馈并始终执行下去尚未得到彻底分析。基于这一问题，本文提出了一个全面的基准测试，RefuteBench，涵盖了诸如问答、机器翻译和电子邮件撰写等任务。评估旨在评估模型是否能够积极接受反驳指令形式的反馈，并是否能够在对话中始终遵循用户需求。我们对众多LLMs进行了评估，并发现LLMs倾向固执，即倾向于其内部知识，经常未能遵守用户反馈。

    arXiv:2402.13463v1 Announce Type: cross  Abstract: The application scope of large language models (LLMs) is increasingly expanding. In practical use, users might provide feedback based on the model's output, hoping for a responsive model that can complete responses according to their feedback. Whether the model can appropriately respond to users' refuting feedback and consistently follow through with execution has not been thoroughly analyzed. In light of this, this paper proposes a comprehensive benchmark, RefuteBench, covering tasks such as question answering, machine translation, and email writing. The evaluation aims to assess whether models can positively accept feedback in form of refuting instructions and whether they can consistently adhere to user demands throughout the conversation. We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the leng
    
[^112]: EvoGrad：以人类对手为特点的Winograd Schema挑战的动态方法

    EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries

    [https://arxiv.org/abs/2402.13372](https://arxiv.org/abs/2402.13372)

    EvoGrad是一个以人类对手为特点的用于解决Winograd Schema挑战的动态方法，通过人在环中方法创建动态数据集，拓展任务实例并引入错误深度度量标准，提出新的多样化常识推理数据集基准，揭示了当前语言模型在此类任务上的挑战。

    

    虽然大型语言模型（LLMs）在Winograd Schema Challenge（WSC）中表现出色，该任务通过代词消歧义测试常识推理，但它们对于包含轻微修改或改写的实例感到困难。为了解决这个问题，我们引入EvoGrad，这是一个开源平台，利用人在环中的方法来创建一个适用于这种修改后WSC实例的动态数据集。利用ChatGPT的功能，我们将我们的任务实例从182扩展到3,691个，为多样化的常识推理数据集设定了一个新的基准。此外，我们引入了错误深度度量标准，评估模型在动态任务中的稳定性。我们的结果强调了EvoGrad所提出的挑战：即使性能最佳的LLM，GPT-3.5，在平均错误深度为7.2的情况下仅达到65.0%的准确率，与人类92.8%的准确率形成了鲜明对比，人类准确率没有干扰性错误。这突显了持续存在的模型限制

    arXiv:2402.13372v1 Announce Type: new  Abstract: While Large Language Models (LLMs) excel at the Winograd Schema Challenge (WSC), a coreference resolution task testing common-sense reasoning through pronoun disambiguation, they struggle with instances that feature minor alterations or rewording. To address this, we introduce EvoGrad, an open-source platform that harnesses a human-in-the-loop approach to create a dynamic dataset tailored to such altered WSC instances. Leveraging ChatGPT's capabilities, we expand our task instances from 182 to 3,691, setting a new benchmark for diverse common-sense reasoning datasets. Additionally, we introduce the error depth metric, assessing model stability in dynamic tasks. Our results emphasize the challenge posed by EvoGrad: Even the best performing LLM, GPT-3.5, achieves an accuracy of 65.0% with an average error depth of 7.2, a stark contrast to human performance of 92. 8% accuracy without perturbation errors. This highlights ongoing model limita
    
[^113]: 朝着可信的再排序：一种简单但有效的弃权机制

    Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism

    [https://arxiv.org/abs/2402.12997](https://arxiv.org/abs/2402.12997)

    提出了一种适用于现实约束的轻量级弃权机制，特别适用于再排序阶段，通过数据驱动的方法达到有效性，并提供了开源代码以促进其更广泛的应用。

    

    神经信息检索（NIR）已经显著改进了基于启发式的IR系统。然而，失败仍然频繁发生，通常所使用的模型无法检索与用户查询相关的文档。我们通过提出一种适用于现实约束的轻量级弃权机制来解决这一挑战，特别强调再排序阶段。我们介绍了一个协议，用于在黑匣子场景中评估弃权策略的效果，并提出了一种简单但有效的数据驱动机制。我们提供了实验复制和弃权实施的开源代码，促进其在不同环境中更广泛的采用和应用。

    arXiv:2402.12997v1 Announce Type: cross  Abstract: Neural Information Retrieval (NIR) has significantly improved upon heuristic-based IR systems. Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user's query. We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the reranking phase. We introduce a protocol for evaluating abstention strategies in a black-box scenario, demonstrating their efficacy, and propose a simple yet effective data-driven mechanism. We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts.
    
[^114]: 表格作为图片？探讨LLM在多模态表格数据表示上的优势和局限性

    Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data

    [https://arxiv.org/abs/2402.12424](https://arxiv.org/abs/2402.12424)

    本研究探讨了LLM在解释表格数据方面的有效性，比较了文本和图像表格表示对LLM性能的影响，为在表格相关任务上有效使用LLM提供了见解。

    

    在本文中，我们通过不同的提示策略和数据格式研究了各种LLM在解释表格数据方面的有效性。我们的分析涵盖了六个针对与表格相关任务的基准，如问答和事实核查。我们首次介绍了LLM在基于图像的表格表示上的表现评估。具体地，我们比较了五种基于文本和三种基于图像的表格表示，展示了表示和提示对LLM性能的影响。我们的研究为在表格相关任务上有效使用LLM提供了见解。

    arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
    
[^115]: 小模型，大见解：利用精简代理模型确定LLMs何时以及为何检索

    Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs

    [https://arxiv.org/abs/2402.12052](https://arxiv.org/abs/2402.12052)

    本文介绍了一种新的协作方法SlimPLM，通过精简代理模型检测LLMs中缺失的知识，以增强LLMs的知识获取过程

    

    arXiv:2402.12052v1 公告类型:新摘要: 大型语言模型（LLMs）与搜索引擎的整合代表了知识获取方法的重要发展。然而，确定LLM已经具备的知识和需要搜索引擎帮助的知识仍然是一个未解决的问题。大多数现有方法通过LLM本身预处理答案或推理的结果来解决这个问题，但这带来了过高的计算成本。本文介绍了一种新颖的协作方法，即SlimPLM，通过精简代理模型检测LLMs中缺失的知识，以增强LLMs的知识获取过程。我们采用参数远远更少的代理模型，并将其答案视为启发式答案。然后利用启发式答案来预测回答用户问题所需的知识，以及LLM中已知和未知的知识。我们只为未知知识进行检索

    arXiv:2402.12052v1 Announce Type: new  Abstract: The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM's knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the mis
    
[^116]: 一种用于词汇语义变化的上下文化词嵌入的系统比较

    A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change

    [https://arxiv.org/abs/2402.12011](https://arxiv.org/abs/2402.12011)

    本文通过在相同条件下评估了最新的词汇语义变化模型和方法，将LSC问题分解为不同级别的任务，并展示了在不同语言的八个基准测试中，APD在GCD方面优于其他方法，XL-LEXEME在WiC、WSI和GCD方面优于其他上下文化模型，并且与GPT-4相当。

    

    上下文化嵌入是建模词汇语义变化（LSC）的首选工具。当前的评估通常专注于称为分级变化检测（GCD）的特定任务。然而，由于它们依赖于不同设置，跨作品的性能比较经常具有误导性。在本文中，我们在相同条件下评估了GCD的最新模型和方法。我们进一步将LSC问题分解为上下文中的单词（WiC）和词义归纳（WSI）任务，并比较这些不同级别的模型。我们在八个可用的LSC基准测试中跨不同语言进行评估，结果表明：（i）APD在GCD方面优于其他方法；（ii）XL-LEXEME在WiC、WSI和GCD方面优于其他上下文化模型，同时与GPT-4相当；（iii）明显需要改进对词义建模以及关注这些意义何时、如何和为何变化的工作。

    arXiv:2402.12011v1 Announce Type: new  Abstract: Contextualized embeddings are the preferred tool for modeling Lexical Semantic Change (LSC). Current evaluations typically focus on a specific task known as Graded Change Detection (GCD). However, performance comparison across work are often misleading due to their reliance on diverse settings. In this paper, we evaluate state-of-the-art models and approaches for GCD under equal conditions. We further break the LSC problem into Word-in-Context (WiC) and Word Sense Induction (WSI) tasks, and compare models across these different levels. Our evaluation is performed across different languages on eight available benchmarks for LSC, and shows that (i) APD outperforms other approaches for GCD; (ii) XL-LEXEME outperforms other contextualized models for WiC, WSI, and GCD, while being comparable to GPT-4; (iii) there is a clear need for improving the modeling of word meanings, as well as focus on how, when, and why these meanings change, rather t
    
[^117]: ArtPrompt: 基于ASCII艺术的对齐LLMs越狱攻击

    ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs

    [https://arxiv.org/abs/2402.11753](https://arxiv.org/abs/2402.11753)

    提出了一种新颖的基于ASCII艺术的越狱攻击，以及一个用于评估LLMs在识别非纯语义提示方面能力的基准挑战。五个SOTA LLMs在识别ASCII艺术提示时存在困难。

    

    安全对于大型语言模型（LLMs）的使用至关重要。已经开发了多种技术，如数据过滤和监督微调，以加强LLMs的安全性。然而，当前已知的技术假设用于对齐LLMs安全性的语料库仅由语义进行解释。然而，这一假设在现实应用中不成立，导致LLMs存在严重漏洞。本文提出了一种新颖的基于ASCII艺术的越狱攻击，并引入了一个全面的基准Vision-in-Text Challenge（ViTC）来评估LLMs在识别不能仅通过语义进行解释的提示的能力。我们展示了五个SOTA LLMs（GPT-3.5、GPT-4、Gemini、Claude和Llama2）在识别以ASCII艺术形式提供的提示方面存在困难。基于这一观察，我们开发了

    arXiv:2402.11753v1 Announce Type: cross  Abstract: Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we devel
    
[^118]: 用RLHF推进翻译偏好建模：迈向成本效益解决方案

    Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution

    [https://arxiv.org/abs/2402.11525](https://arxiv.org/abs/2402.11525)

    提出了一种利用强化学习和人类反馈（RLHF）来改进翻译质量的成本效益偏好学习策略，该策略通过优化奖励模型来区分人类和机器翻译，从而指导改进机器翻译。

    

    arXiv:2402.11525v1 公告类型：新 真实性、表达力和优雅是机器翻译中不断追求的目标。然而，传统的度量标准如BLEU并不严格符合人类对翻译质量的偏好。本文探讨了利用强化学习与人类反馈（RLHF）来提高翻译质量。收集人类对翻译之间的比较的大规模高质量数据集并不容易，尤其对于低资源语言。为解决这一问题，我们提出了一种成本效益的偏好学习策略，通过区分人类和机器翻译来优化奖励模型。通过这种方式，奖励模型学习机器翻译与人类之间的不足之处，并指导随后对机器翻译的改进。实验结果表明，RLHF可以有效地提升翻译质量，这种改进也有益于其他翻译方向。

    arXiv:2402.11525v1 Announce Type: new  Abstract: Faithfulness, expressiveness, and elegance is the constant pursuit in machine translation. However, traditional metrics like \textit{BLEU} do not strictly align with human preference of translation quality. In this paper, we explore leveraging reinforcement learning with human feedback (\textit{RLHF}) to improve translation quality. It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for low-resource languages. To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and machine translations. In this manner, the reward model learns the deficiencies of machine translation compared to human and guides subsequent improvements in machine translation. Experimental results demonstrate that \textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions 
    
[^119]: 探究价值偏好：LLMs偏向理想状态的偏差

    Exploring Value Biases: How LLMs Deviate Towards the Ideal

    [https://arxiv.org/abs/2402.11005](https://arxiv.org/abs/2402.11005)

    研究发现大型语言模型（LLMs）在给出响应时存在一个价值偏好的机制，倾向于偏向理想状态，这种偏差会对不同应用场景产生重要影响。

    

    大型语言模型（LLMs）被部署在各种应用中，并且它们的响应对社会产生着越来越大的影响。理解LLMs在给出响应时的非故意机制对于解释它们的性能并辨别它们在现实世界应用中的偏差至关重要。这类似于人类研究中，这种无意识的响应被称为抽样。我们研究了LLMs的这种抽样现象，发现LLMs的抽样倾向于偏爱高价值选项。价值偏好对应于从最可能的响应向LLM中代表的理想价值的转变。实际上，即便是通过上下文提示学习到的新实体，这种效果也能够再现。我们表明这种偏差表现在意想不到的地方，并对选择典型实例等相关应用场景产生影响。结果显示，价值偏好在不同分类的LLMs中都很明显。

    arXiv:2402.11005v1 Announce Type: cross  Abstract: Large-Language-Models (LLMs) are deployed in a wide range of applications, and their response has an increasing social impact. Understanding the non-deliberate(ive) mechanism of LLMs in giving responses is essential in explaining their performance and discerning their biases in real-world applications. This is analogous to human studies, where such inadvertent responses are referred to as sampling. We study this sampling of LLMs in light of value bias and show that the sampling of LLMs tends to favour high-value options. Value bias corresponds to this shift of response from the most likely towards an ideal value represented in the LLM. In fact, this effect can be reproduced even with new entities learnt via in-context prompting. We show that this bias manifests in unexpected places and has implications on relevant application scenarios, like choosing exemplars. The results show that value bias is strong in LLMs across different categor
    
[^120]: 预训练语言模型对于标记的表面信息的知识

    Knowledge of Pretrained Language Models on Surface Information of Tokens

    [https://arxiv.org/abs/2402.09808](https://arxiv.org/abs/2402.09808)

    该研究探究了预训练语言模型对于标记的表面信息的知识。结果显示，模型具备有关标记长度和子字符串的知识，但对标记结构的知识有限，解码器方面存在瓶颈。

    

    我们研究了预训练语言模型是否具有关于标记表面信息的知识。我们从标记长度、子字符串和标记结构的角度检查了预训练语言模型获取的词语或子词嵌入中保存的表面信息。此外，我们评估了模型生成标记表面知识的能力。我们主要关注了12个主要在英语和日语语料库上训练的预训练语言模型。实验结果表明，预训练语言模型对于标记长度和子字符串具有知识，但对于标记结构则没有知识。此外，结果表明，在有效利用获取的知识方面，解码器方面存在瓶颈。

    arXiv:2402.09808v1 Announce Type: new  Abstract: Do pretrained language models have knowledge regarding the surface information of tokens? We examined the surface information stored in word or subword embeddings acquired by pretrained language models from the perspectives of token length, substrings, and token constitution. Additionally, we evaluated the ability of models to generate knowledge regarding token surfaces. We focused on 12 pretrained language models that were mainly trained on English and Japanese corpora. Experimental results demonstrate that pretrained language models have knowledge regarding token length and substrings but not token constitution. Additionally, the results imply that there is a bottleneck on the decoder side in terms of effectively utilizing acquired knowledge.
    
[^121]: CodeMind:一个用于挑战大型语言模型进行代码推理的框架

    CodeMind: A Framework to Challenge Large Language Models for Code Reasoning

    [https://arxiv.org/abs/2402.09664](https://arxiv.org/abs/2402.09664)

    CodeMind是一个用于挑战大型语言模型进行代码推理的框架，通过评估LLMs的代码推理能力来替代仅仅依靠测试通过来评估，对三种代码推理任务进行评估，结果显示LLMs能够公正地理解控制流结构，并且对于简单程序和复杂程序，它们通常能够推理出输入如何演变为输出。

    

    仅靠测试通过来评估大型语言模型（LLMs）的代码合成能力可能会导致不公正的评估或促进具有数据泄漏的模型，作为一种替代方案，我们介绍了CodeMind，这是一个旨在评估LLMs的代码推理能力的框架。CodeMind目前支持三种代码推理任务：独立执行推理（IER）、依赖执行推理（DER）和规范推理（SR）。前两者评估模型以预测任意代码的执行输出，或者模型能够正确合成的代码。第三个任务评估LLMs实现指定预期行为的程度。我们使用CodeMind对两种不同编程语言中的五个基准下的九个LLMs进行了广泛的评估，结果表明LLMs能够公正地理解控制流结构，并且对于简单程序和复杂程序，它们通常能够推理出输入如何演变为输出。

    arXiv:2402.09664v1 Announce Type: cross  Abstract: Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly understand control flow constructs and, in general, are capable of reasoning how inputs evolve to output, specifically for simple programs and the ones 
    
[^122]: 踩脚调校：通过自助引导扩展LLM的自对齐能力的规模化方法

    Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping

    [https://arxiv.org/abs/2402.07610](https://arxiv.org/abs/2402.07610)

    本文首次探索了自助引导自对齐对大型语言模型的影响，发现其明显优于单次循环的方法，并通过调整数据训练顺序进一步提升模型性能。

    

    自对齐是一种降低人工注释成本并确保模型能力的有效方法。然而，大多数当前的方法在单次循环中完成数据收集和训练步骤，可能忽视了自对齐模型不断改进的能力。这引发了一个关键问题：如果我们进行多次自助引导自对齐，会增强模型性能还是导致快速退化？本文首次探索了自助引导自对齐对大型语言模型的影响。我们的研究结果表明，通过保证从上下文学习中获得的数据多样性，自助引导自对齐明显优于单次循环的方法。为了进一步发挥自助引导的能力，我们还研究并调整了数据的训练顺序，从而提高了模型的性能。基于这些发现，我们提出了踩脚调校（SOFT）的方法，利用模型的持续增强能力。

    Self-alignment is an effective way to reduce the cost of human annotation while ensuring promising model capability. However, most current methods complete the data collection and training steps in a single round, which may overlook the continuously improving ability of self-aligned models. This gives rise to a key query: What if we do multi-time bootstrapping self-alignment? Does this strategy enhance model performance or lead to rapid degradation? In this paper, our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models. Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from in-context learning. To further exploit the capabilities of bootstrapping, we investigate and adjust the training order of data, which yields improved performance of the model. Drawing on these findings, we propose Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced
    
[^123]: 泄漏、欺骗、重复：封闭源LLMs中的数据污染和评估不端行为

    Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs

    [https://arxiv.org/abs/2402.03927](https://arxiv.org/abs/2402.03927)

    该论文研究了封闭源LLMs中的数据污染和评估不端行为。通过对255篇论文的分析和OpenAI的数据使用政策考虑，研究人员发现这些模型在第一年发布后存在泄露数据的问题。

    

    自然语言处理（NLP）研究越来越多地关注使用大型语言模型（LLMs），其中一些最受欢迎的模型是完全或部分封闭源的。对于模型细节，特别是训练数据的缺乏访问权限，使研究人员反复对数据污染提出了担忧。虽然已经进行了一些尝试来解决这个问题，但仅限于个别案例和试错方法。此外，他们忽视了“间接”数据泄漏的问题，即模型通过使用用户提供的数据进行迭代改进。本研究在OpenAI的GPT-3.5和GPT-4使用上进行了首次系统分析，这些是当今最广泛使用的LLMs，并考虑了OpenAI的数据使用政策，详细记录了模型发布后一年内泄露给这些模型的数据量。我们报告了这些模型在主要数据污染方面。

    Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of \emph{indirect} data leaking, where models are iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release. We report that these models have been g
    
[^124]: 大型语言模型作为MOOCs评分者

    Large Language Models As MOOCs Graders

    [https://arxiv.org/abs/2402.03776](https://arxiv.org/abs/2402.03776)

    该研究探索了利用大型语言模型（LLMs）代替MOOCs中同伴评分的可行性，旨在解决大规模在线开放课程中评估学生写作任务的问题。

    

    大规模在线开放课程（MOOCs）为拥有电脑和互联网访问权限的全球任何人提供免费教育的机会。尽管如此，这些课程的大规模注册意味着一位教师几乎不可能评估每个学生的写作任务。因此，同伴评分通常是首选方法，通常由简单明了的评分标准指导。然而，同伴评分在可靠度和有效性方面常常存在问题。在这项研究中，我们利用18个不同的场景，探索利用大型语言模型（LLMs）替代MOOCs中的同伴评分的可行性。具体而言，我们关注两种最先进的LLMs：GPT-4和GPT-3.5，并涵盖三门不同的课程：入门天文学，天体生物学以及天文学的历史与哲学。为了训练LLMs，我们使用了基于零-shot连续思考（Zero-shot-CoT）提示技术的变种的三个不同提示：结合Zero-shot-CoT的提示。

    Massive open online courses (MOOCs) unlock the doors to free education for anyone around the globe with access to a computer and the internet. Despite this democratization of learning, the massive enrollment in these courses means it is almost impossible for one instructor to assess every student's writing assignment. As a result, peer grading, often guided by a straightforward rubric, is the method of choice. While convenient, peer grading often falls short in terms of reliability and validity. In this study, using 18 distinct settings, we explore the feasibility of leveraging large language models (LLMs) to replace peer grading in MOOCs. Specifically, we focus on two state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy. To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with
    
[^125]: 人类与机器：重新思考语言模型在蕴含验证中的应用

    Minds versus Machines: Rethinking Entailment Verification with Language Models

    [https://arxiv.org/abs/2402.03686](https://arxiv.org/abs/2402.03686)

    本文通过研究人类和大型语言模型在推理判断中的共性和差异，发现大型语言模型在复杂推理中具有优势，而人类在简单推理中表现出色。基于这些发现，引入了一个优化的Flan-T5模型，用于蕴含验证。

    

    人类在文本理解中进行大量的推理以理解论述。本文旨在了解人类和最先进的大型语言模型（LLM）在推理判断中的共性和差异。通过综合策划的蕴含验证基准测试，我们评估了人类和LLM在各种推理类别中的表现。我们的基准测试包含了来自三个类别（NLI、上下文QA和解释）的数据集，包括多句前提和不同的知识类型，从而评估了复杂推理情况下的推理能力。值得注意的是，我们的发现显示LLM在跨扩展上下文的多跳推理中具有优势，而人类在需要简单演绎推理的任务中表现出色。利用这些见解，我们介绍了一个经过精细调整的Flan-T5模型，其性能超过了GPT-3.5，并与GPT-4媲美，提供了一个强大的开源解决方案供蕴含验证使用。作为一个实际的应用

    Humans make numerous inferences in text comprehension to understand discourse. This paper aims to understand the commonalities and disparities in the inference judgments between humans and state-of-the-art Large Language Models (LLMs). Leveraging a comprehensively curated entailment verification benchmark, we evaluate both human and LLM performance across various reasoning categories. Our benchmark includes datasets from three categories (NLI, contextual QA, and rationales) that include multi-sentence premises and different knowledge types, thereby evaluating the inference capabilities in complex reasoning instances. Notably, our findings reveal LLMs' superiority in multi-hop reasoning across extended contexts, while humans excel in tasks necessitating simple deductive reasoning. Leveraging these insights, we introduce a fine-tuned Flan-T5 model that outperforms GPT-3.5 and rivals with GPT-4, offering a robust open-source solution for entailment verification. As a practical application
    
[^126]: PuzzleBench：LLMs能否解决困难的一阶组合推理问题？

    PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?

    [https://arxiv.org/abs/2402.02611](https://arxiv.org/abs/2402.02611)

    本研究通过PuzzleBench数据集探索了LLMs解决困难的一阶组合推理问题的能力，并提出了Puzzle-LM方法，该方法将LLMs与符号求解器和程序解释器相结合，使其能够有效地推理这类问题。

    

    最近的研究探索了使用LLMs进行推理任务，重点是相对简单的问题，如逻辑问答。在我们的工作中，我们希望解决更复杂的问题，显著扩展这些模型的功能。特别是，我们探讨LLMs是否能够解决困难的一阶组合推理问题，一个例子是流行的数独谜题。这些问题有一个由自然语言描述的基础一阶结构，并且可以实例化为不同大小的实例。此外，这些问题在计算上是密集型的，需要多个推理步骤才能达到解决方案。我们提出了PuzzleBench，一个包含31个这样具有挑战性的谜题的数据集。我们观察到，即使在符号求解器的帮助下，LLMs在我们的基准测试中表现得相当糟糕。作为回应，我们提出了一种新的方法，Puzzle-LM，它将LLMs与符号求解器和程序解释器相结合，使它们能够推理这类问题。

    Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering. In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models. Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku. These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes. Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution. We present PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark. In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such
    
[^127]: 在大型语言模型中测量道德不一致性

    Measuring Moral Inconsistencies in Large Language Models

    [https://arxiv.org/abs/2402.01719](https://arxiv.org/abs/2402.01719)

    本研究提出了一种新的信息论度量方法，称为语义图熵（SGE），用于测量道德情景中大型语言模型（LLM）的一致性。与现有的一致性度量方法相比，SGE在五个LLMs上与人类判断更好地相关，为研究LLM不一致性的根本原因提供了新的思路。

    

    如果语义等价的提示产生语义等价的响应，那么大型语言模型(LLM)被认为是一致的。尽管最近的进展展示了LLMs在对话系统中令人印象深刻的能力，但我们表明即使是最先进的LLMs在生成方面也存在高度不一致性，这对它们的可靠性提出了质疑。先前的研究尝试用任务特定的准确度来衡量这一点。然而，这种方法对于没有“正确”答案的道德情景（例如，道路交运问题）是不合适的。为了解决这个问题，我们提出了一种新的信息论度量方法，称为语义图熵（SGE），来衡量LLM在道德情景中的一致性。我们利用“经验法则”（RoTs）来解释模型的决策策略，并进一步增强我们的度量方法。与现有的一致性度量方法相比，SGE与人类判断在五个LLMs上更好地相关。在未来，我们的目标是调查LLM不一致性的根本原因。

    A Large Language Model~(LLM) is considered consistent if semantically equivalent prompts produce semantically equivalent responses. Despite recent advancements showcasing the impressive capabilities of LLMs in conversational systems, we show that even state-of-the-art LLMs are highly inconsistent in their generations, questioning their reliability. Prior research has tried to measure this with task-specific accuracies. However, this approach is unsuitable for moral scenarios, such as the trolley problem, with no ``correct'' answer. To address this issue, we propose a novel information-theoretic measure called Semantic Graph Entropy~(SGE) to measure the consistency of an LLM in moral scenarios. We leverage ``Rules of Thumb''~(RoTs) to explain a model's decision-making strategies and further enhance our metric. Compared to existing consistency metrics, SGE correlates better with human judgments across five LLMs. In the future, we aim to investigate the root causes of LLM inconsistencies 
    
[^128]: HiGen: 层次感知的层级文本分类序列生成

    HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text Classification

    [https://arxiv.org/abs/2402.01696](https://arxiv.org/abs/2402.01696)

    HiGen提出了一个基于文本生成的框架，利用语言模型来编码动态文本表示，在层次文本分类中考虑了文档各个部分的相关性，并引入了一个层级引导的损失函数。此外，还提供了一个新颖的用于HTC的数据集ENZYME。

    

    层次文本分类（HTC）是多标签文本分类中的一个复杂子任务，其特点是具有层级标签分类法和数据不平衡。最佳性能模型旨在通过结合文档和层级标签信息来学习静态表示。然而，文档各个部分的相关性可能因层级水平的不同而变化，需要动态的文档表示。为了解决这个问题，我们提出了HiGen，一个利用语言模型编码动态文本表示的基于文本生成的框架。我们引入了一种层级引导的损失函数，以捕捉文本和标签名称语义之间的关系。我们的方法采用了一个特定任务的预训练策略，将语言模型调整到领域知识上，并显著提高了对样本有限的类别的性能。此外，我们还提供了一个命名为ENZYME的新颖和有价值的用于HTC的数据集，该数据集由来自PubMed的文章组成，旨在预测...

    Hierarchical text classification (HTC) is a complex subtask under multi-label text classification, characterized by a hierarchical label taxonomy and data imbalance. The best-performing models aim to learn a static representation by combining document and hierarchical label information. However, the relevance of document sections can vary based on the hierarchy level, necessitating a dynamic document representation. To address this, we propose HiGen, a text-generation-based framework utilizing language models to encode dynamic text representations. We introduce a level-guided loss function to capture the relationship between text and label name semantics. Our approach incorporates a task-specific pretraining strategy, adapting the language model to in-domain knowledge and significantly enhancing performance for classes with limited examples. Furthermore, we present a new and valuable dataset called ENZYME, designed for HTC, which comprises articles from PubMed with the goal of predicti
    
[^129]: MARIO: 具有Python代码解释器的数学推理输出--可重复的流水线

    MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline

    [https://arxiv.org/abs/2401.08190](https://arxiv.org/abs/2401.08190)

    本文通过引入具有Python代码解释器的数学数据集，解决了大型语言模型在数学推理能力方面的挑战。

    

    大型语言模型（LLMs）在自然语言理解任务中取得了相当大的进展，但在获得真正的人工通用智能方面仍然存在一些需要填补的差距，特别是在数学推理能力方面存在的缺陷。本文通过丰富数据景观和引入一种新颖的数学数据集来解决这一挑战，该数据集增加了使用Python代码解释器的能力。

    arXiv:2401.08190v2 Announce Type: replace  Abstract: Large language models (LLMs) have seen considerable advancements in natural language understanding tasks, yet there remains a gap to bridge before attaining true artificial general intelligence, especially concerning shortcomings in mathematical reasoning capabilities. We postulate that the inherent nature of LLM training, which focuses on predicting probabilities of next token, presents challenges in effectively modeling mathematical reasoning that demands exact calculations, both from data-driven and theoretical standpoints. In this paper, we address this challenge by enriching the data landscape and introducing a novel math dataset, enhanced with a capability to utilize a Python code interpreter. This dataset is derived from GSM8K and MATH and has been further refined through a combination of GPT-4 annotations, human review, and self-training processes, where the errors in the original GSM8K training set have been fixed. Additiona
    
[^130]: 自我想象：利用自我想象进行多模型自然推理

    Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination

    [https://arxiv.org/abs/2401.08025](https://arxiv.org/abs/2401.08025)

    本文提出了Self-Imagine方法，通过利用一种Vision-Language模型生成问题的结构化表示并将其渲染为图像，再使用相同的模型回答问题，从而在数学任务和通用推理任务中提高了模型性能。

    

    Vision-Language模型（VLMs）的潜力在处理复杂基于文本问题时往往未被充分利用，尤其是当这些问题能够从视觉表达中获益时。本文提出了Self-Imagine，与人类通过创建问题的视觉图并推断解决步骤的能力相 resonating。我们利用单一的Vision-Language模型（VLM）使用HTML生成问题的结构化表示，然后将HTML渲染为图像，并最终使用相同的VLM根据问题和图像回答问题。我们的方法不需要任何额外的训练数据或训练。我们使用最先进的（LLAVA-1.5和GEMINI PRO）VLMs在三个数学任务和九个通用推理任务上评估了我们的方法。我们的方法提升了LLAVA-1.5和GEMINI PRO在所有数学任务上的性能。

    arXiv:2401.08025v2 Announce Type: replace  Abstract: The potential of Vision-Language Models (VLMs) often remains underutilized in handling complex text-based problems, particularly when these problems could benefit from visual representation. Resonating with humans' ability to solve complex text-based problems by (1) creating a visual diagram from the problem and (2) deducing what steps they need to take to solve it, we propose Self-Imagine. We leverage a single Vision-Language Model (VLM) to generate a structured representation of the question using HTML, then render the HTML as an image, and finally use the same VLM to answer the question using both the question and the image. Our approach does not require any additional training data or training. We evaluate our approach on three mathematics tasks and nine general-purpose reasoning tasks using state-of-the-art (LLAVA-1.5 and GEMINI PRO) VLMs. Our approach boosts the performance of LLAVA-1.5 and GEMINI PRO on all math tasks (on aver
    
[^131]: MAPO：通过多语言对齐作为偏好优化推进多语言推理

    MAPO: Advancing Multilingual Reasoning through Multilingual Alignment-as-Preference Optimization

    [https://arxiv.org/abs/2401.06838](https://arxiv.org/abs/2401.06838)

    MAPO提出了一个多语言对齐作为偏好优化框架，可以显著提高各种模型在多语言推理中的表现。

    

    尽管推理能力被认为与语言无关，但现有的多语言语言模型在不同语言中表现出的推理能力不一致，例如，对主导语言（如英语）的推理能力优于其他语言，这是由于多语言训练数据的不平衡造成的。为了增强非主导语言的推理能力，我们提出了一个名为多语言对齐作为偏好优化（MAPO）的框架，旨在将其他语言中的推理过程与主导语言对齐。具体来说，我们利用现成的翻译模型来保证非主导语言和主导语言之间答案的一致性，这被采纳为优化的偏好，例如，直接偏好优化（DPO）或临近策略优化（PPO）。实验表明，MAPO在所有三个基准测试中（MSVAMP +16.2％，MGSM +6.1％）稳定地实现了各种模型的多语言推理显著改进。

    arXiv:2401.06838v2 Announce Type: replace  Abstract: Though reasoning abilities are considered language-agnostic, existing LLMs exhibit inconsistent reasoning abilities across different languages, e.g., reasoning in the dominant language like English is superior to other languages due to the imbalance of multilingual training data. To enhance reasoning abilities in non-dominant languages, we propose a Multilingual-Alignment-as-Preference Optimization framework (MAPO), aiming to align the reasoning processes in other languages with the dominant language. Specifically, we harness an off-the-shelf translation model for the consistency between answers in non-dominant and dominant languages, which we adopt as the preference for optimization, e.g., Direct Preference Optimization (DPO) or Proximal Policy Optimization (PPO). Experiments show that MAPO stably achieves significant improvements in the multilingual reasoning of various models on all three benchmarks (MSVAMP +16.2%, MGSM +6.1%, and
    
[^132]: PixT3：基于像素的表格到文本生成

    PixT3: Pixel-based Table To Text generation

    [https://arxiv.org/abs/2311.09808](https://arxiv.org/abs/2311.09808)

    PixT3是一种基于像素的多模式表格到文本模型，通过将数据到文本生成视为视觉识别任务，消除了字符串格式的需求，克服了线性化和输入大小限制的挑战。

    

    Table-to-text生成涉及根据结构化表格数据生成适当的文本描述。近年来，由于神经网络模型的流行和大规模数据集的可用性，它引起了越来越多的关注。现有方法的一个共同特点是将输入视为字符串，即通过采用线性化技术，不总是保留表格中的信息，过于冗长，缺乏空间效率。我们提出将数据到文本生成重新思考为一个视觉识别任务，消除了将输入呈现为字符串格式的必要性。我们提出了PixT3，一种多模式表格到文本模型，克服了现有模型遇到的线性化和输入大小限制的挑战。PixT3通过新的自监督学习目标进行训练，以加强表格结构意识，并适用于开放式和受控生成设置。

    arXiv:2311.09808v2 Announce Type: replace  Abstract: Table-to-text generation involves generating appropriate textual descriptions given structured tabular data. It has attracted increasing attention in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. A common feature across existing methods is their treatment of the input as a string, i.e., by employing linearization techniques that do not always preserve information in the table, are verbose, and lack space efficiency. We propose to rethink data-to-text generation as a visual recognition task, removing the need for rendering the input in a string format. We present PixT3, a multimodal table-to-text model that overcomes the challenges of linearization and input size limitations encountered by existing models. PixT3 is trained with a new self-supervised learning objective to reinforce table structure awareness and is applicable to open-ended and controlled generation settings.
    
[^133]: GistScore：通过要点瓶颈学习更好的上下文示例选择表示

    GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks

    [https://arxiv.org/abs/2311.09606](https://arxiv.org/abs/2311.09606)

    提出了示例要点提取的方法，通过要点模型形成了新的分数评估方式，可以用于动态选择最佳示例，提高上下文示例选择的性能。

    

    上下文学习（ICL）是大型语言模型（LLMs）在以包含少量任务示例的提示为条件时执行新任务的能力。然而，ICL的性能可能对示例的选择非常敏感。为了动态地为每个测试输入选择最佳示例，我们提出了示例要点提取（Example Gisting），这是一种通过具有输入和输出之间的注意力瓶颈进行监督微调的训练示例编码器的新方法。这些要点模型构成了GistScore的基础，这是一种用于评分和选择信息示例的新指标。此外，我们尝试了两种变体：（1）对每个数据集微调要点模型和（2）在大量数据集上进行多任务培训单一模型。后者可用于开箱即用地处理新任务，实现无需训练的ICL流水线。对涵盖9个任务和8种不同LLMs的21个数据集进行评估表明，我们微调的模型达到了最顶尖的水平。

    arXiv:2311.09606v2 Announce Type: replace  Abstract: In-context Learning (ICL) is the ability of Large Language Models (LLMs) to perform new tasks when conditioned on prompts comprising a few task examples. However, ICL performance can be critically sensitive to the choice of examples. To dynamically select the best examples for every test input, we propose Example Gisting, a novel approach for training example encoders through supervised fine-tuning with an attention bottleneck between the inputs and outputs. These gist models form the basis for GistScore, a novel metric for scoring and selecting informative examples. Further, we experiment with two variations: (1) fine-tuning gist models for each dataset and (2) multi-task training a single model on a large collection of datasets. The latter can be used for new tasks out-of-the-box, enabling a training-free ICL pipeline. Evaluations with 21 datasets spanning 9 tasks and 8 diverse LLMs show that our fine-tuned models get state-of-the-
    
[^134]: 指令方式的重要性：即使任务约束也会影响LLM生成文本的检测

    How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection

    [https://arxiv.org/abs/2311.08369](https://arxiv.org/abs/2311.08369)

    即使是任务约束也会影响LLM生成文本的检测性能，本研究发现即使这些约束与规避无关，也会导致现有检测器性能具有显著差异

    

    为了对抗大型语言模型（LLMs）的滥用，许多最近的研究提出了性能可靠的LLM生成文本检测器。当用户指示LLMs生成文本时，指令可以根据用户需求包含不同的约束。然而，大多数最近的研究在为LLM检测创建数据集时并没有涵盖这种多样化的指令模式。本文发现，即使是任务导向的约束——这些约束自然会包含在指令中，并且与检测规避无关——也会导致现有的检测器在检测性能上具有较大的方差。我们以学生作文写作为现实领域，并根据几个因素手动创建基于作文质量的任务约束。我们的实验表明，在带有这种约束的指令生成的文本上，当前检测器性能的标准偏差（SD）显著较大。

    arXiv:2311.08369v2 Announce Type: replace  Abstract: To combat the misuse of Large Language Models (LLMs), many recent studies have presented LLM-generated-text detectors with promising performance. When users instruct LLMs to generate texts, the instruction can include different constraints depending on the user's need. However, most recent studies do not cover such diverse instruction patterns when creating datasets for LLM detection. In this paper, we find that even task-oriented constraints -- constraints that would naturally be included in an instruction and are not related to detection-evasion -- cause existing detectors to have a large variance in detection performance. We focus on student essay writing as a realistic domain and manually create task-oriented constraints based on several factors for essay quality. Our experiments show that the standard deviation (SD) of current detector performance on texts generated by an instruction with such a constraint is significantly large
    
[^135]: Monkey: 大型多模态模型中图像分辨率和文本标签的重要性

    Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models

    [https://arxiv.org/abs/2311.06607](https://arxiv.org/abs/2311.06607)

    Monkey通过提高图像分辨率和采用多级描述生成方法来增强大型多模态模型(LMMs)的能力，从而实现更详细的视觉捕捉和更有效的学习。

    

    大型多模态模型(LMMs)在视觉语言任务中表现出了潜力，但在高分辨率输入和详细场景理解方面表现不佳。为了解决这些挑战，我们引入了Monkey来增强LMM的能力。首先，Monkey通过将输入图像划分为统一的补丁来处理图像，每个补丁的大小与原来训练良好的视觉编码器使用的大小(例如448x448)相匹配。配备了每个补丁的适配器，Monkey可以处理高达1344x896像素的更高分辨率，实现对复杂视觉信息的详细捕捉。其次，它采用多级描述生成方法，丰富了场景-对象关联的上下文。这种两部分策略确保了从生成数据中更有效的学习：更高的分辨率允许对视觉进行更详细的捕捉，从而增强了全面描述的效果。广泛的实验证明...

    arXiv:2311.06607v3 Announce Type: replace-cross  Abstract: Large Multimodal Models (LMMs) have shown promise in vision-language tasks but struggle with high-resolution input and detailed scene understanding. Addressing these challenges, we introduce Monkey to enhance LMM capabilities. Firstly, Monkey processes input images by dividing them into uniform patches, each matching the size (e.g., 448x448) used in the original training of the well-trained vision encoder. Equipped with individual adapter for each patch, Monkey can handle higher resolutions up to 1344x896 pixels, enabling the detailed capture of complex visual information. Secondly, it employs a multi-level description generation method, enriching the context for scene-object associations. This two-part strategy ensures more effective learning from generated data: the higher resolution allows for a more detailed capture of visuals, which in turn enhances the effectiveness of comprehensive descriptions. Extensive ablative result
    
[^136]: 指令调整的动态：大型语言模型的每个能力都有其自己的增长速率

    Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace

    [https://arxiv.org/abs/2310.19651](https://arxiv.org/abs/2310.19651)

    本研究通过对指令调整对每个大型语言模型的各项能力（如创意写作、代码生成和逻辑推理）的发展影响进行细致分析，得出了关于数据集规模、参数规模和数据构建方法的指导原则。

    

    指令调整是一种新兴方法，用于激发大型语言模型（LLMs）的普遍智能。然而，指令数据的创建仍然主要是启发式的，导致现有数据集在数量和质量上存在显着差异。我们的研究通过对数据量、参数大小和数据构建方法如何影响LLM的每个基本能力（如创意写作、代码生成和逻辑推理）的发展进行细致分析，以更好地理解数据构建准则。我们提供了一个经过精心策划的数据集，涵盖了十种能力的超过40k个实例，并研究了具有70亿至330亿参数的经过指令调整的模型。我们的研究揭示了三个主要发现：

    arXiv:2310.19651v2 Announce Type: replace  Abstract: Instruction tuning is a burgeoning method to elicit the general intelligence of Large Language Models (LLMs). However, the creation of instruction data is still largely heuristic, leading to significant variation in quantity and quality across existing datasets. While some research advocates for expanding the number of instructions, others suggest that a small set of well-chosen examples is adequate. To better understand data construction guidelines, our research provides a granular analysis of how data volume, parameter size, and data construction methods influence the development of each underlying ability of LLM, such as creative writing, code generation, and logical reasoning. We present a meticulously curated dataset with over 40k instances across ten abilities and examine instruction-tuned models with 7b to 33b parameters. Our study reveals three primary findings: (i) Despite the models' overall performance being tied to data a
    
[^137]: MindfulDiary：利用大型语言模型支持精神病患者的日记记录

    MindfulDiary: Harnessing Large Language Model to Support Psychiatric Patients' Journaling

    [https://arxiv.org/abs/2310.05231](https://arxiv.org/abs/2310.05231)

    MindfulDiary利用大型语言模型帮助精神病患者通过对话记录日常体验，并在临床环境中取得积极效果

    

    在心理健康领域，大型语言模型（LLMs）提供了新的机会，然而其复杂性和低可控性引发了关于其在临床环境中适用性的质疑。我们介绍了MindfulDiary，一个移动日记应用，结合LLM帮助精神病患者通过对话记录日常体验。与心理健康专业人士（MHPs）合作设计，MindfulDiary采取基于状态的方法，安全地遵守专家指南，同时进行自由形式的对话。通过涉及28名重性抑郁障碍患者和5名精神科医生的为期四周的实地研究，我们发现MindfulDiary支持患者持续丰富其日常记录，并帮助精神科医生通过理解他们的想法和日常背景更好地同情他们的患者。根据这些发现，我们讨论了其影响。

    arXiv:2310.05231v2 Announce Type: replace-cross  Abstract: In the mental health domain, Large Language Models (LLMs) offer promising new opportunities, though their inherent complexity and low controllability have raised questions about their suitability in clinical settings. We present MindfulDiary, a mobile journaling app incorporating an LLM to help psychiatric patients document daily experiences through conversation. Designed in collaboration with mental health professionals (MHPs), MindfulDiary takes a state-based approach to safely comply with the experts' guidelines while carrying on free-form conversations. Through a four-week field study involving 28 patients with major depressive disorder and five psychiatrists, we found that MindfulDiary supported patients in consistently enriching their daily records and helped psychiatrists better empathize with their patients through an understanding of their thoughts and daily contexts. Drawing on these findings, we discuss the implicati
    
[^138]: Fast-DetectGPT: 通过条件概率曲率实现高效的零样本检测机器生成文本

    Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature

    [https://arxiv.org/abs/2310.05130](https://arxiv.org/abs/2310.05130)

    通过引入条件概率曲率概念，本文提出了Fast-DetectGPT，一个优化的零样本检测器，相对于DetectGPT在白盒和其他测试条件下的性能提升达到约75%。

    

    大型语言模型(LLMs)已经展现出产生流畅、连贯内容的能力，提供了生产力机会同时也带来了社会风险。为构建值得信赖的人工智能系统，有必要区分机器生成和人类创作的内容。本文引入条件概率曲率概念，以阐明LLMs和人类在特定上下文中的词汇选择差异。利用该曲率作为基础度量，我们提出**Fast-DetectGPT**，一个优化的零样本检测器，将DetectGPT的扰动步骤替换为更高效的采样步骤。我们在各种数据集、源模型和测试条件上的评估表明，Fast-DetectGPT在白盒和...

    arXiv:2310.05130v2 Announce Type: replace  Abstract: Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. To build trustworthy AI systems, it is imperative to distinguish between machine-generated and human-authored content. The leading zero-shot detector, DetectGPT, showcases commendable performance but is marred by its intensive computational costs. In this paper, we introduce the concept of conditional probability curvature to elucidate discrepancies in word choices between LLMs and humans within a given context. Utilizing this curvature as a foundational metric, we present **Fast-DetectGPT**, an optimized zero-shot detector, which substitutes DetectGPT's perturbation step with a more efficient sampling step. Our evaluations on various datasets, source models, and test conditions indicate that Fast-DetectGPT not only surpasses DetectGPT by a relative around 75% in both the white-box a
    
[^139]: 使用早期退出训练动态模型以在资源受限设备上进行自动语音识别

    Training dynamic models using early exits for automatic speech recognition on resource-constrained devices

    [https://arxiv.org/abs/2309.09546](https://arxiv.org/abs/2309.09546)

    早期退出架构可以使自注意力模型适应不同计算资源和ASR性能需求，该研究比较了微调预训练模型和采用早期退出目标从头训练模型的效果

    

    动态调整神经模型在推理过程中的计算负载的能力对于设备处理资源有限且计算资源随时间变化的场景至关重要。 早期退出架构提供了一种有前景的解决方案，其中在编码器的中间层附加了额外的退出分支。 在用于自动语音识别（ASR）的自注意力模型中，早期退出架构使得可以开发出动态模型，这些动态模型能够根据不同水平的计算资源和ASR性能需求来调整其大小和架构。 以往关于早期退出ASR模型的研究依赖于预训练的自监督模型，并使用早期退出损失进行微调。 本文对预训练的骨干网进行微调和使用早期退出目标从头训练模型进行了实验比较。 实验在公共数据集上进行。

    arXiv:2309.09546v2 Announce Type: replace-cross  Abstract: The ability to dynamically adjust the computational load of neural models during inference is crucial for on-device processing scenarios characterised by limited and time-varying computational resources. A promising solution is presented by early-exit architectures, in which additional exit branches are appended to intermediate layers of the encoder. In self-attention models for automatic speech recognition (ASR), early-exit architectures enable the development of dynamic models capable of adapting their size and architecture to varying levels of computational resources and ASR performance demands. Previous research on early-exiting ASR models has relied on pre-trained self-supervised models, fine-tuned with an early-exit loss. In this paper, we undertake an experimental comparison between fine-tuning pre-trained backbones and training models from scratch with the early-exiting objective. Experiments conducted on public dataset
    
[^140]: 用医学教科书增强黑盒LLMs进行临床问题回答

    Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering

    [https://arxiv.org/abs/2309.02233](https://arxiv.org/abs/2309.02233)

    该研究提出了一种名为LLMs增强医学教科书（LLM-AMT）的系统，通过插入式模块将权威医学教科书集成到LLMs的框架中，显著提高了LLMs在专业领域的能力。

    

    大规模语言模型（LLMs）如ChatGPT已经展示出根据人类指令生成响应的印象能力。然而，由于它们缺乏特定、深入的知识，它们在医学领域的应用可能具有挑战性。在这项研究中，我们提出了一种名为LLMs增强医学教科书（LLM-AMT）的系统，旨在增强LLMs在专业领域的能力。LLM-AMT通过插入式模块将权威医学教科书集成到LLMs的框架中。这些模块包括一个查询增强器、一个混合教科书检索器和一个知识自我完善。它们共同整合权威医学知识。此外，一个LLMs阅读器有助于上下文理解。我们在三个医学问答任务上的实验结果表明，LLMAMT显著提高了响应质量，准确率提高了11.6%到16.6%。值得注意的是，以GPT-4-Turbo为基础模型

    arXiv:2309.02233v2 Announce Type: replace-cross  Abstract: Large-scale language models (LLMs) like ChatGPT have demonstrated impressive abilities in generating responses based on human instructions. However, their use in the medical field can be challenging due to their lack of specific, in-depth knowledge. In this study, we present a system called LLMs Augmented with Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in specialized domains. LLM-AMT integrates authoritative medical textbooks into the LLMs' framework using plug-and-play modules. These modules include a Query Augmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together, they incorporate authoritative medical knowledge. Additionally, an LLM Reader aids in contextual understanding. Our experimental results on three medical QA tasks demonstrate that LLMAMT significantly improves response quality, with accuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the base mod
    
[^141]: Donkii: 注释错误检测方法能否发现指令调整数据集中的错误？

    Donkii: Can Annotation Error Detection Methods Find Errors in Instruction-Tuning Datasets?

    [https://arxiv.org/abs/2309.01669](https://arxiv.org/abs/2309.01669)

    本文提出了一个新颖的指令调整数据的AED基准DONKII，并发现所有三个数据集都包含明显的错误，有时这些错误直接传播到指令调整的LLMs中。

    

    指导调整已成为大型语言模型（LLMs）训练流程中不可或缺的一部分，并已被证明能够产生强大的性能增益。 在研究的另一条线上，注释错误检测（AED）已经成为检测黄金标准标签中质量问题的工具。然而，到目前为止，AED方法的应用已经被限制在分类任务中。 如何评估AED方法在语言生成设置中的泛化能力仍然是一个未解之谜，这一设置通过LLMs正在变得越来越普遍。 本文提出了一个针对指令调整数据的AED的首个新颖基准：DONKII。 它包括三个由专家和半自动方法增强错误注释的指令调整数据集。 我们还为指令调整数据提供了新颖的错误类型分类法。 我们发现所有三个数据集都包含明显的错误，有时这些错误直接传播到指令调整的LLMs中。

    arXiv:2309.01669v2 Announce Type: replace  Abstract: Instruction tuning has become an integral part of training pipelines for Large Language Models (LLMs) and has been shown to yield strong performance gains. In an orthogonal line of research, Annotation Error Detection (AED) has emerged as a tool for detecting quality problems in gold standard labels. So far, however, the application of AED methods has been limited to classification tasks. It is an open question how well AED methods generalize to language generation settings, which are becoming more widespread via LLMs. In this paper, we present a first and novel benchmark for AED on instruction tuning data: DONKII. It comprises three instruction-tuning datasets enriched with error annotations by experts and semi-automatic methods. We also provide a novel taxonomy of error types for instruction-tuning data. We find that all three datasets contain clear errors, which sometimes propagate directly into instruction-tuned LLMs. We propose 
    
[^142]: LLMs用于知识图谱构建和推理：最新功能与未来机遇

    LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities

    [https://arxiv.org/abs/2305.13168](https://arxiv.org/abs/2305.13168)

    本研究全面评估了LLMs在知识图谱构建和推理领域的性能，发现GPT-4更适合作为推理助手，并在某些情况下超越了精调模型。

    

    本文对大规模语言模型（LLMs）在知识图谱（KG）构建和推理中的数量化和质化评估进行了详尽的研究。我们在八个不同的数据集上进行了实验，重点关注涵盖实体和关系提取、事件提取、链接预测和问答四个典型任务，从而全面探索了LLMs在构建和推理领域的表现。经验性研究发现，以GPT-4为代表的LLMs更适合作为推理助手，而不是少样本信息提取器。具体而言，虽然GPT-4在与KG构建相关的任务中表现出色，但在推理任务中表现更出色，在某些情况下超越了精调模型。此外，我们的调查还扩展到LLMs在信息提取方面的潜在泛化能力，提出了虚拟知识提取的构想。

    arXiv:2305.13168v2 Announce Type: replace-cross  Abstract: This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs' performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extr
    
[^143]: PolQA：波兰问答数据集

    PolQA: Polish Question Answering Dataset

    [https://arxiv.org/abs/2212.08897](https://arxiv.org/abs/2212.08897)

    波兰问答数据集（PolQA）是用于OpenQA的第一个波兰语数据集，包括7,000个问题和87,525个手动标记的证据段落，在QA系统性能方面提出了有效的注释策略，以提高段落的检索准确度@10并降低注释成本。

    

    最近提出的面向开放域问答（OpenQA）系统需要大量的训练数据才能达到最先进的性能。然而，数据注释被认为是耗时且昂贵的，因此获取起来很困难。因此，适用于语言少数（主要是英语和汉语）的数据集仅有少数。在这项工作中，我们介绍和公开发布了波兰问答（PolQA）数据集，这是用于OpenQA的第一个波兰语数据集。它包括7,000个问题，87,525个手动标记的证据段落，以及超过7,097,322个候选段落的语料库。每个问题都根据其公式、类型以及答案的实体类型进行了分类。这一资源允许我们评估不同注释选择对QA系统性能的影响，并提出一种有效的注释策略，将段落的检索准确度@10提高了10.55个百分点，同时将注释成本降低了82%。

    arXiv:2212.08897v2 Announce Type: replace  Abstract: Recently proposed systems for open-domain question answering (OpenQA) require large amounts of training data to achieve state-of-the-art performance. However, data annotation is known to be time-consuming and therefore expensive to acquire. As a result, the appropriate datasets are available only for a handful of languages (mainly English and Chinese). In this work, we introduce and publicly release PolQA, the first Polish dataset for OpenQA. It consists of 7,000 questions, 87,525 manually labeled evidence passages, and a corpus of over 7,097,322 candidate passages. Each question is classified according to its formulation, type, as well as entity type of the answer. This resource allows us to evaluate the impact of different annotation choices on the performance of the QA system and propose an efficient annotation strategy that increases the passage retrieval accuracy@10 by 10.55 p.p. while reducing the annotation cost by 82%.
    
[^144]: 用弱监督对比预训练进行文本嵌入

    Text Embeddings by Weakly-Supervised Contrastive Pre-training

    [https://arxiv.org/abs/2212.03533](https://arxiv.org/abs/2212.03533)

    本文提出了一种名为E5的文本嵌入模型，通过弱监督对比训练方式，在未经过标记数据的情况下，在多个任务中表现卓越，是第一个在BEIR检索基准测试上击败BM25基线的模型，在微调后在MTEB基准测试上获得最佳结果。

    

    本文介绍了E5，一种最先进的文本嵌入模型，可以很好地迁移到各种任务中。该模型以对比方式训练，使用我们精心策划的大规模文本配对数据集（名为CCPairs）的弱监督信号。E5可以作为通用嵌入模型用于任何需要单一文本向量表示的任务，如检索、聚类和分类，在零-shot和微调设置下表现出色。我们在BEIR和MTEB基准测试的56个数据集上进行了广泛评估。在零-shot设置下，E5是第一个在BEIR检索基准测试上击败强大的BM25基线且不使用任何标记数据的模型。在微调后，E5在MTEB基准测试上取得了最佳结果，胜过具有40倍参数的现有嵌入模型。

    arXiv:2212.03533v2 Announce Type: replace  Abstract: This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40x more parameters.
    
[^145]: ScreenQA: 移动应用截图上的大规模问答对

    ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots

    [https://arxiv.org/abs/2209.08199](https://arxiv.org/abs/2209.08199)

    ScreenQA提出了一个新的任务和数据集，通过86K个问答对在RICO数据集上注释，旨在评估屏幕阅读理解能力。

    

    我们提出了一个新的任务和数据集ScreenQA，用于通过问答来理解屏幕内容。现有的屏幕数据集要么侧重于结构和组件级别的理解，要么侧重于像导航和任务完成之类的更高级别的组合任务。我们试图通过在RICO数据集上注释86K个问答对来弥合这两者之间的差距，希望能够基准化屏幕阅读理解能力。

    arXiv:2209.08199v2 Announce Type: replace  Abstract: We present a new task and dataset, ScreenQA, for screen content understanding via question answering. The existing screen datasets are focused either on structure and component-level understanding, or on a much higher-level composite task such as navigation and task completion. We attempt to bridge the gap between these two by annotating 86K question-answer pairs over the RICO dataset in hope to benchmark the screen reading comprehension capacity.
    
[^146]: StreaMulT: 流式多模态Transformer用于异构和任意长的序列数据

    StreaMulT: Streaming Multimodal Transformer for Heterogeneous and Arbitrary Long Sequential Data

    [https://arxiv.org/abs/2110.08021](https://arxiv.org/abs/2110.08021)

    提出一种新的流式多模态Transformer模型，用于处理异构和任意长的序列数据，解决了在预测性维护任务中多源数据流的融合和跨时间预测的挑战。

    

    随着工业4.0系统复杂性的增加，预测性维护任务（如故障检测和诊断）带来了新的挑战。相应而实际的情景包括来自不同模态的多源数据流，如传感器测量时间序列、机器图像、文本维护报告等。这些异构多模态流在其采集频率上也不同，可能包含时间上不对齐的信息，并且可以任意长，取决于所考虑的系统和任务。虽然多模态融合在静态环境中已被广泛研究，但据我们所知，以往的工作中没有考虑过与相关任务（如跨时间预测）一起考虑任意长的多模态流。因此，在本文中，我们首先将这种异构多模态学习范式形式化为一种新型的流式设置。为了解决这一挑战，我们...

    arXiv:2110.08021v2 Announce Type: replace-cross  Abstract: The increasing complexity of Industry 4.0 systems brings new challenges regarding predictive maintenance tasks such as fault detection and diagnosis. A corresponding and realistic setting includes multi-source data streams from different modalities, such as sensors measurements time series, machine images, textual maintenance reports, etc. These heterogeneous multimodal streams also differ in their acquisition frequency, may embed temporally unaligned information and can be arbitrarily long, depending on the considered system and task. Whereas multimodal fusion has been largely studied in a static setting, to the best of our knowledge, there exists no previous work considering arbitrarily long multimodal streams alongside with related tasks such as prediction across time. Thus, in this paper, we first formalize this paradigm of heterogeneous multimodal learning in a streaming setting as a new one. To tackle this challenge, we p
    
[^147]: TAT-LLM: 一种针对表格和文本数据的专用语言模型用于离散推理

    TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data. (arXiv:2401.13223v1 [cs.CL])

    [http://arxiv.org/abs/2401.13223](http://arxiv.org/abs/2401.13223)

    TAT-LLM是一种专门用于离散推理的语言模型，针对混合表格和文本数据上的问答任务。该模型通过分步流水线的方式，包括提取器、推理器和执行器，利用LLMs的强大能力来解决问题。而为了应对成本、延迟和数据安全风险等挑战，我们开发了TAT-LLM，一个专门针对此任务的较小LLM。

    

    在这项工作中，我们解决了在混合表格和文本数据上进行问答的问题，这在Web上非常常见（如SEC文件），通常需要离散推理能力。最近，像GPT-4这样的大型语言模型展示了强大的多步骤推理能力。我们考虑利用LLMs的强大能力来解决我们的任务。我们提出了面向表格和文本问答的分步流水线的抽象，包括提取器、推理器和执行器三个关键步骤，并首先设计了一份指令来实例化该流水线并验证GPT-4优于所有现有方法。然而，利用像GPT-4这样的在线LLM存在成本、延迟和数据安全风险等各种挑战，这促使我们专门针对此任务开发较小的LLM。我们通过对现有专家标注数据集自动生成的训练数据对LLaMA 2进行微调，开发了TAT-LLM语言模型。

    In this work, we address question answering (QA) over a hybrid of tabular and textual data that are very common content on the Web (e.g. SEC filings), where discrete reasoning capabilities are often required. Recently, large language models (LLMs) like GPT-4 have demonstrated strong multi-step reasoning capabilities. We then consider harnessing the amazing power of LLMs to solve our task. We abstract a Step-wise Pipeline for tabular and textual QA, which consists of three key steps, including Extractor, Reasoner and Executor, and initially design an instruction to instantiate the pipeline and validate that GPT-4 outperforms all existing methods. However, utilizing an online LLM like GPT-4 holds various challenges in terms of cost, latency, and data security risk, which motivates us to specialize smaller LLMs in this task. We develop a TAT-LLM language model by fine-tuning LLaMA 2 with the training data generated automatically from existing expert-annotated datasets following the Step-w
    
[^148]: 从理解到应用：大规模语言模型可解释性研究综述

    From Understanding to Utilization: A Survey on Explainability for Large Language Models. (arXiv:2401.12874v1 [cs.CL])

    [http://arxiv.org/abs/2401.12874](http://arxiv.org/abs/2401.12874)

    本综述论文研究了大规模语言模型(LLMs)可解释性的新兴领域，强调了在LLMs中增强可解释性的必要性，同时解决了广大公众对其信任和技术界对这些模型更深理解的需求。该综述对现有的可解释性方法进行分类，并讨论了它们在提高模型透明度和可靠性方面的应用，旨在弥合理论理解和实际应用之间的差距。

    

    本综述论文深入研究了大规模语言模型(LLMs)可解释性的新兴领域，这是自然语言处理中一个关键且具有挑战性的方面。LLMs在各种应用中发挥着关键作用，但其“黑盒”性质引发了对透明性和伦理使用的担忧。本文强调了在LLMs中增强可解释性的必要性，同时解决了广大公众对其信任和技术界对这些模型更深理解的需求。我们集中在预训练的基于Transformer的LLMs，如LLaMA，其规模和复杂性使其面临独特的可解释性挑战。我们的综述对现有的可解释性方法进行分类，并讨论了它们在提高模型透明度和可靠性方面的应用。我们还讨论了代表性的评价方法，强调了它们的优势和局限性。本综述的目标是弥合理论理解和实际应用之间的差距，提供从技术角度总结可解释性方法的全面视角。

    This survey paper delves into the burgeoning field of explainability for Large Language Models (LLMs), a critical yet challenging aspect of natural language processing. With LLMs playing a pivotal role in various applications, their "black-box" nature raises concerns about transparency and ethical use. This paper emphasizes the necessity for enhanced explainability in LLMs, addressing both the general public's trust and the technical community's need for a deeper understanding of these models. We concentrate on pre-trained Transformer-based LLMs, such as LLaMA, which present unique interpretability challenges due to their scale and complexity. Our review categorizes existing explainability methods and discusses their application in improving model transparency and reliability. We also discuss representative evaluation methods, highlighting their strengths and limitations. The goal of this survey is to bridge the gap between theoretical understanding and practical application, offering 
    
[^149]: 通过人的反馈改善机器翻译: 将质量估计作为奖励模型的探索

    Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model. (arXiv:2401.12873v1 [cs.CL])

    [http://arxiv.org/abs/2401.12873](http://arxiv.org/abs/2401.12873)

    本研究探索了利用质量估计作为奖励模型来预测人类偏好以改善机器翻译的潜力。我们发现基于质量估计的反馈训练存在过度优化问题，采用启发式规则来检测错误翻译并对质量估计模型进行惩罚以解决该问题。

    

    不充分建模人类偏好导致奖励模型在利用人的反馈提高翻译质量方面成为一个主要障碍。幸运的是，质量估计(QE)在过去两年中无需参考文献就能准确预测给定翻译的质量。在这项工作中，我们探讨了将QE模型作为奖励模型(基于QE的奖励模型)来预测人的偏好以进行反馈训练的潜力。我们首先发现了在基于QE的反馈训练中的过度优化问题，表现为奖励的增加而翻译质量下降。我们研究了这个问题，并认为QE模型的脆弱性可能导致错误翻译的高奖励，从而导致过度优化和错误传播。为解决这个问题，我们采用了一种简单而有效的方法，使用启发式规则检测错误翻译，并为QE模型添加了一个惩罚项。

    Insufficient modeling of human preferences within the reward model is a major obstacle for leveraging human feedback to improve translation quality. Fortunately, quality estimation (QE), which predicts the quality of a given translation without reference, has achieved impressive alignment with human evaluations in the last two years. In this work, we investigate the potential of employing the QE model as the reward model (the QE-based reward model) to predict human preferences for feedback training. We first identify the overoptimization problem during QE-based feedback training, manifested as an increase in reward while translation quality declines. We examine the problem and argue that the vulnerability of the QE model might lead to high rewards for incorrect translations, resulting in overoptimization and error propagation. To address the problem, we adopt a simple yet effective method that uses heuristic rules to detect the incorrect translations and assigns a penalty term to the Q
    
[^150]: 生成无监督的言辞解释用于谣言验证

    Generating Unsupervised Abstractive Explanations for Rumour Verification. (arXiv:2401.12713v1 [cs.CL])

    [http://arxiv.org/abs/2401.12713](http://arxiv.org/abs/2401.12713)

    该论文利用无监督的方法，通过对言辞进行评分并生成解释性摘要，来增加谣言验证的信息丰富度和准确性。

    

    在社交媒体上进行谣言验证的任务涉及根据由该谣言引起的对话线程评估其真实性的问题。尽管之前的工作已经专注于预测真实性标签，但我们在这里重新制定了任务，以生成与模型相关的自由文本解释谣言的真实性。我们采用无监督的方法，首先利用事后可解释性方法对线程中最重要的帖子进行评分，然后使用这些帖子通过使用模板引导总结生成信息丰富的解释性摘要。为了评估解释性摘要的信息量，我们利用了大型语言模型的少样本学习能力。我们的实验表明，语言模型在评估摘要时可以与人类达到类似的一致性。重要的是，我们证明了解释性的概括摘要比仅使用线程中排名最高的帖子更具信息量，并更好地反映了预测的谣言真实性。

    The task of rumour verification in social media concerns assessing the veracity of a claim on the basis of conversation threads that result from it. While previous work has focused on predicting a veracity label, here we reformulate the task to generate model-centric, free-text explanations of a rumour's veracity. We follow an unsupervised approach by first utilising post-hoc explainability methods to score the most important posts within a thread and then we use these posts to generate informative explanatory summaries by employing template-guided summarisation. To evaluate the informativeness of the explanatory summaries, we exploit the few-shot learning capabilities of a large language model (LLM). Our experiments show that LLMs can have similar agreement to humans in evaluating summaries. Importantly, we show that explanatory abstractive summaries are more informative and better reflect the predicted rumour veracity than just using the highest ranking posts in the thread.
    
[^151]: 学习高质量和通用性的短语表示

    Learning High-Quality and General-Purpose Phrase Representations. (arXiv:2401.10407v1 [cs.CL])

    [http://arxiv.org/abs/2401.10407](http://arxiv.org/abs/2401.10407)

    本论文提出了一种改进的框架来学习高质量和通用性的短语表示。该框架在无上下文的情况下学习短语表示，通过短语类型分类和有效地融合字符级信息来提高表示的精确性和灵活性。此外，还采用了三种粒度的数据增强方法以增加训练数据的多样性。

    

    短语表示在数据科学和自然语言处理中起着重要作用，有利于实体对齐、记录链接、模糊连接和释义分类等各种任务。目前最先进的方法是使用对比学习来微调预训练的语言模型以获取短语嵌入。然而，我们已经发现了需要改进的方面。首先，这些预训练模型往往过于复杂，并需要在具有上下文句子的语料库上进行预训练。其次，利用短语类型和形态给出更精确和更灵活的短语表示。我们提出了一种改进的框架以以无上下文的方式学习短语表示。该框架将短语类型分类作为辅助任务，并更有效地将字符级信息融入短语表示。此外，我们设计了三种粒度的数据增强方法，以增加训练数据的多样性。

    Phrase representations play an important role in data science and natural language processing, benefiting various tasks like Entity Alignment, Record Linkage, Fuzzy Joins, and Paraphrase Classification. The current state-of-the-art method involves fine-tuning pre-trained language models for phrasal embeddings using contrastive learning. However, we have identified areas for improvement. First, these pre-trained models tend to be unnecessarily complex and require to be pre-trained on a corpus with context sentences. Second, leveraging the phrase type and morphology gives phrase representations that are both more precise and more flexible. We propose an improved framework to learn phrase representations in a context-free fashion. The framework employs phrase type classification as an auxiliary task and incorporates character-level information more effectively into the phrase representation. Furthermore, we design three granularities of data augmentation to increase the diversity of train
    
[^152]: E^2-LLM: 大规模语言模型的高效和极长扩展

    E^2-LLM: Efficient and Extreme Length Extension of Large Language Models. (arXiv:2401.06951v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.06951](http://arxiv.org/abs/2401.06951)

    E^2-LLM是一种高效和极长扩展方法，通过仅需一次训练过程和不收集长上下文数据的方式，在大规模语言模型中实现了显著减少的计算成本。基于RoPE位置嵌入，E^2-LLM只需要较短的训练数据长度，支持不同的评估上下文窗口。

    

    通常，使用长上下文大小训练LLM会消耗大量的计算资源和GPU资源，需要长时间的训练。现有的长上下文扩展方法通常需要额外的训练过程来支持相应的长上下文窗口，需要长上下文训练数据（例如32k），并且假定有高昂的GPU训练成本。为了解决上述问题，我们提出了一种名为E^2-LLM的高效和极长扩展方法，只需要一次训练过程，大大减少了计算成本，也不需要收集长上下文数据。具体而言，我们的E^2-LLM的训练数据只需要很短的长度（例如4k），大大降低了调整成本。其次，在短训练上下文窗口上的训练过程只执行一次，我们可以支持不同的评估上下文窗口。第三，在E^2-LLM中，我们基于RoPE位置嵌入。

    Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. Existing long-context extension methods usually need additional training procedures to support corresponding long-context windows, where the long-context training data (e.g., 32k) is needed, and high GPU training costs are assumed. To address the aforementioned issues, we propose an Efficient and Extreme length extension method for Large Language Models, called E 2 -LLM, with only one training procedure and dramatically reduced computation cost, which also removes the need to collect long-context data. Concretely, first, the training data of our E 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost greatly. Second, the training procedure on the short training context window is performed only once time, and we can support different evaluation context windows at inference. Third, in E 2 - LLM, based on RoPE position embeddings, we 
    
[^153]: 语言模型的细粒度幻觉检测与编辑

    Fine-grained Hallucination Detection and Editing for Language Models. (arXiv:2401.06855v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.06855](http://arxiv.org/abs/2401.06855)

    这项研究提出了一个新任务，即自动细粒度幻觉检测，并介绍了一个综合分类方法。通过对两个语言模型的输出进行分析，发现大部分幻觉属于少有的类别。为了解决这个问题，研究者通过训练一个检索增强语言模型，使用合成数据来检测和纠正幻觉。

    

    大型语言模型往往会生成多样的事实不正确的陈述，被广泛称为幻觉。目前的方法主要集中在粗粒度的自动幻觉检测或编辑上，忽视了细微的错误级别。本文提出了一项新任务——自动细粒度幻觉检测，并提出了一个包含六个层次分明的幻觉类型的综合分类法。为了便于评估，我们引入了一个新的基准，其中包括对两个语言模型输出在各个领域上进行细粒度人工判断的数据。我们的分析发现，ChatGPT和Llama 2-Chat的输出中有60%和75%的幻觉，其中多数幻觉属于未被充分探索的类别。作为解决这一问题的初始步骤，我们训练了FAVA，一个经过精心设计合成数据生成来检测和纠正细粒度幻觉的检索增强语言模型。

    Large language models (LMs) are prone to generate diverse factually incorrect statements, which are widely called hallucinations. Current approaches predominantly focus on coarse-grained automatic hallucination detection or editing, overlooking nuanced error levels. In this paper, we propose a novel task -- automatic fine-grained hallucination detection -- and present a comprehensive taxonomy encompassing six hierarchically defined types of hallucination. To facilitate evaluation, we introduce a new benchmark that includes fine-grained human judgments on two LM outputs across various domains. Our analysis reveals that ChatGPT and Llama 2-Chat exhibit hallucinations in 60% and 75% of their outputs, respectively, and a majority of these hallucinations fall into categories that have been underexplored. As an initial step to address this, we train FAVA, a retrieval-augmented LM by carefully designing synthetic data generations to detect and correct fine-grained hallucinations. On our bench
    
[^154]: SH2: 自我突出式犹豫帮助您更准确解码。

    SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully. (arXiv:2401.05930v1 [cs.CL])

    [http://arxiv.org/abs/2401.05930](http://arxiv.org/abs/2401.05930)

    自我突出式犹豫（SH2）是一种推理时的方法，通过选择预测概率较低的标记，并强调它们的差异，从而帮助语言模型更准确地解码。

    

    大型语言模型(LLMs)在文本生成方面表现出色。然而，LLMs仍然存在幻觉问题。在本研究中，我们提出了一种推理时方法，即自我突出式犹豫(SH2)，以帮助LLMs更准确地解码。SH2基于信息理论中一个简单的事实，即对于LLMs而言，预测概率较低的标记往往更具信息量。我们的分析表明，LLMs给予较低概率的标记更有可能与事实信息（如名词、专有名词和形容词）密切相关。因此，我们提出通过选择概率最低的标记并将其连接到原始上下文中来“突出”事实信息，从而迫使模型在生成之前多次阅读和犹豫这些标记。在解码过程中，我们还采用对比解码的方式来强调由犹豫带来的输出概率的差异。

    Large language models (LLMs) demonstrate great performance in text generation. However, LLMs are still suffering from hallucinations. In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others. Our analysis shows that the tokens assigned with lower probabilities by an LLM are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives. Therefore, we propose to ''highlight'' the factual information by selecting the tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation. During decoding, we also adopt contrastive decoding to emphasize the difference in the output probabilities brought by the hesitation.
    
[^155]: LLaVA-$\phi$: 高效的多模态助手与小型语言模型

    LLaVA-$\phi$: Efficient Multi-Modal Assistant with Small Language Model. (arXiv:2401.02330v1 [cs.CV])

    [http://arxiv.org/abs/2401.02330](http://arxiv.org/abs/2401.02330)

    LLaVA-$\phi$是一种高效的多模态助手，使用小型语言模型Phi-2来促进多模态对话。即使具有较少的参数，它也能有效地融合文本和视觉元素，并在各种任务中表现出色。它为时间敏感的环境和需要实时交互的系统开辟了新的应用途径。

    

    在本文中，我们介绍了LLaVA-$\phi$（LLaVA-Phi），一种利用最近先进的小型语言模型Phi-2来促进多模态对话的高效多模态助手。LLaVA-Phi在紧凑的多模态模型领域中标志着重要进展。它证明了即使是个参数只有27亿的较小语言模型在训练有高质量语料库的情况下也可以有效地参与融合文本和视觉元素的复杂对话。我们的模型在包括视觉理解、推理和基于知识的感知等公开可用的基准测试中表现出色。除了在多模态对话任务中表现出卓越性能外，我们的模型为时间敏感的环境和需要实时交互的系统（如实体代理）开辟了新的应用途径。它突显了较小语言模型实现高级理解和交互的潜力。

    In this paper, we introduce LLaVA-$\phi$ (LLaVA-Phi), an efficient multi-modal assistant that harnesses the power of the recently advanced small language model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a notable advancement in the realm of compact multi-modal models. It demonstrates that even smaller language models, with as few as 2.7B parameters, can effectively engage in intricate dialogues that integrate both textual and visual elements, provided they are trained with high-quality corpora. Our model delivers commendable performance on publicly available benchmarks that encompass visual comprehension, reasoning, and knowledge-based perception. Beyond its remarkable performance in multi-modal dialogue tasks, our model opens new avenues for applications in time-sensitive environments and systems that require real-time interaction, such as embodied agents. It highlights the potential of smaller language models to achieve sophisticated levels of understanding and inte
    
[^156]: TREC iKAT 2023: 交互式知识辅助任务概述

    TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview. (arXiv:2401.01330v1 [cs.IR])

    [http://arxiv.org/abs/2401.01330](http://arxiv.org/abs/2401.01330)

    TREC iKAT 2023是一个交互式的知识辅助任务，旨在开发适应用户交互和上下文的会话搜索代理。该任务还强调决策搜索任务，用户通过筛选数据和信息来进行决策和执行动作。

    

    会话式信息查询是一个关键的研究领域，之前的工作也有很大的贡献。TREC交互式知识辅助任务（iKAT）建立在TREC会话辅助任务（CAsT）的基础上。然而，iKAT着重于创建和研究可以根据用户之前的交互和当前情境自适应响应的会话搜索代理。挑战在于使会话搜索代理能够将个性化的上下文信息融入到相应中，以高效地引导用户获取相关信息。iKAT还着重于决策搜索任务，即用户通过数据和信息筛选来衡量各种选择，以达到结论或执行动作。这些任务在日常信息搜索决策中普遍存在，无论是旅游、健康还是购物等，通常涉及一组高级信息操作符，其中查询或问题可能会

    Conversational Information Seeking stands as a pivotal research area with significant contributions from previous works. The TREC Interactive Knowledge Assistance Track (iKAT) builds on the foundational work of the TREC Conversational Assistance Track (CAsT). However, iKAT distinctively emphasizes the creation and research of conversational search agents that adapt responses based on user's prior interactions and present context. The challenge lies in enabling Conversational Search Agents (CSA) to incorporate this personalized context to efficiency and effectively guide users through the relevant information to them. iKAT also emphasizes decisional search tasks, where users sift through data and information to weigh up options in order to reach a conclusion or perform an action. These tasks, prevalent in everyday information-seeking decisions -- be it related to travel, health, or shopping -- often revolve around a subset of high-level information operators where queries or questions a
    
[^157]: 在Transformer中定位跨任务序列继续电路

    Locating Cross-Task Sequence Continuation Circuits in Transformers. (arXiv:2311.04131v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.04131](http://arxiv.org/abs/2311.04131)

    通过分析和比较Transformer模型中类似的序列继续任务的电路，研究发现共享的计算结构可以提高模型的行为预测能力、错误识别能力和编辑过程的安全性。

    

    虽然Transformer模型在语言任务上展现出强大的能力，但其复杂的架构使其难以解释。最近的研究旨在将Transformer模型还原为可读的电路表示，用于实现算法功能。我们通过分析和比较类似的序列继续任务的电路来扩展这项研究，其中包括数字、数字词和月份的递增序列。通过应用电路分析技术，我们确定了负责检测序列成员和预测序列中下一个成员的关键子电路。我们的分析揭示了语义相关序列依赖于具有类似作用的共享电路子图。总体而言，记录共享的计算结构能够更好地预测模型行为，识别错误，并进行更安全的编辑过程。这种对Transformer的机械理解是构建更健壮、调试和编辑更安全的模型的关键一步。

    While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust,
    
[^158]: BLP 2023任务2：情感分析

    BLP 2023 Task 2: Sentiment Analysis. (arXiv:2310.16183v1 [cs.CL])

    [http://arxiv.org/abs/2310.16183](http://arxiv.org/abs/2310.16183)

    BLP 2023任务2是关于情感分析的共享任务，吸引了71个参与者。参与者通过各种方法，包括经典机器学习模型和大型语言模型，提交了597个运行结果。本文提供了任务的详细设置和参与者提交系统的概述。

    

    我们总结了作为BLP 2023创新工作坊的一部分举办的BLP情感共享任务。该任务的定义是在给定的社交媒体文本中检测情感。该任务吸引了71个参与者的关注，其中在开发和评估阶段分别有29个和30个团队提交了系统。总共，参与者提交了597个运行结果。然而，总共有15个团队提交了系统描述论文。提交的系统涵盖了从经典的机器学习模型、微调预训练模型到在零样本和少样本设置中利用大型语言模型（LLMs）的各种方法。在本文中，我们详细介绍了该任务的设置，包括数据集的开发和评估设置。此外，我们简要概述了参与者提交的系统。共享任务的所有数据集和评估脚本已公开可用。

    We present an overview of the BLP Sentiment Shared Task, organized as part of the inaugural BLP 2023 workshop, co-located with EMNLP 2023. The task is defined as the detection of sentiment in a given piece of social media text. This task attracted interest from 71 participants, among whom 29 and 30 teams submitted systems during the development and evaluation phases, respectively. In total, participants submitted 597 runs. However, a total of 15 teams submitted system description papers. The range of approaches in the submitted systems spans from classical machine learning models, fine-tuning pre-trained models, to leveraging Large Language Model (LLMs) in zero- and few-shot settings. In this paper, we provide a detailed account of the task setup, including dataset development and evaluation setup. Additionally, we provide a brief overview of the systems submitted by the participants. All datasets and evaluation scripts from the shared task have been made publicly available for the res
    
[^159]: 在上下文中的学生建模中使用大型语言模型：从一次性观察中合成视觉编程中学生的行为

    Large Language Models for In-Context Student Modeling: Synthesizing Student's Behavior in Visual Programming from One-Shot Observation. (arXiv:2310.10690v1 [cs.CL])

    [http://arxiv.org/abs/2310.10690](http://arxiv.org/abs/2310.10690)

    本研究探索在开放式学习环境中使用大型语言模型进行上下文学生建模，提出了一个新的框架LLM-SS，通过合成学生在不同任务上的尝试，为学生建模提供更准确的预测和教学策略。

    

    学生建模对于许多教育技术来说至关重要，因为它可以预测未来的学习结果和有针对性的教学策略。然而，开放式学习环境会带来挑战，因为学生表现出多样化的行为且缺乏明确定义的学习技能集。为了应对这些挑战，我们探索在开放式学习环境中应用大型语言模型（LLMs）进行上下文学生建模。我们引入了一个新颖的框架LLM-SS，利用LLMs合成学生的行为。具体而言，给定一个特定学生在参考任务上的解决尝试作为观察，目标是合成该学生在目标任务上的尝试。我们的框架可以与不同的LLMs结合使用；而且，我们使用领域专家知识对LLMs进行微调，提高它们对领域背景和学生行为的理解。我们评估了几种具体的方法...

    Student modeling is central to many educational technologies as it enables the prediction of future learning outcomes and targeted instructional strategies. However, open-ended learning environments pose challenges for accurately modeling students due to the diverse behaviors exhibited by students and the absence of a well-defined set of learning skills. To approach these challenges, we explore the application of Large Language Models (LLMs) for in-context student modeling in open-ended learning environments. We introduce a novel framework, LLM-SS, that leverages LLMs for synthesizing student's behavior. More concretely, given a particular student's solving attempt on a reference task as observation, the goal is to synthesize the student's attempt on a target task. Our framework can be combined with different LLMs; moreover, we fine-tune LLMs using domain-specific expertise to boost their understanding of domain background and student behaviors. We evaluate several concrete methods bas
    
[^160]: 大型语言模型中的上下文学习: 对表示的神经科学启发式分析

    In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations. (arXiv:2310.00313v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00313](http://arxiv.org/abs/2310.00313)

    本研究通过神经科学启发的技术，研究了大型语言模型中上下文学习的机制，通过调查嵌入和注意力的变化，揭示了这种改进背后的潜在变化，并提出了用于参数化探测和注意力比率分析的新方法。

    

    通过利用输入中的特定任务示例，大型语言模型（LLMs）通过上下文学习（ICL）展现了卓越的性能提升。然而，这种改进背后的机制仍然难以理解。本研究中，我们调查了Llama-2 70B和Vicuna 13B中的嵌入和注意力表示。具体而言，我们研究了上下文学习后嵌入和注意力的变化以及这些变化如何调解行为的改进。我们采用了受神经科学启发的技术，如表示相似性分析（RSA），并提出了参数化探测和注意力比率分析（ARA，衡量关注相关与无关信息的比率）的新方法。我们设计了三个具有条件之间先验关系的任务：阅读理解，线性回归和对抗提示注入。我们提出了关于任务表示中预期相似性的假设，以研究嵌入和注意力中的潜在变化。

    Large language models (LLMs) exhibit remarkable performance improvement through in-context learning (ICL) by leveraging task-specific examples in the input. However, the mechanisms behind this improvement remain elusive. In this work, we investigate embeddings and attention representations in Llama-2 70B and Vicuna 13B. Specifically, we study how embeddings and attention change after in-context-learning, and how these changes mediate improvement in behavior. We employ neuroscience-inspired techniques, such as representational similarity analysis (RSA), and propose novel methods for parameterized probing and attention ratio analysis (ARA, measuring the ratio of attention to relevant vs. irrelevant information). We designed three tasks with a priori relationships among their conditions: reading comprehension, linear regression, and adversarial prompt injection. We formed hypotheses about expected similarities in task representations to investigate latent changes in embeddings and attenti
    
[^161]: DiLu: 基于大型语言模型的自动驾驶的知识驱动方法

    DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models. (arXiv:2309.16292v1 [cs.RO])

    [http://arxiv.org/abs/2309.16292](http://arxiv.org/abs/2309.16292)

    DiLu是基于大型语言模型的自动驾驶系统，采用知识驱动方法，通过推理和反思模块进行决策，积累经验并具有显著的泛化能力。

    

    自动驾驶领域最近的进展依赖于数据驱动方法，虽然被广泛采用，但面临数据集偏见、过拟合和不可解释性等挑战。受人类驾驶知识驱动的启发，我们探索如何将类似的能力注入自动驾驶系统，并提出了一个集成互动环境、驾驶员代理和记忆组件的范例来解决这个问题。通过利用具有新兴能力的大型语言模型，我们提出了DiLu框架，它结合了推理模块和反思模块，使系统能够依据常识知识进行决策，并持续演化。大量实验证明DiLu能够积累经验，并且在泛化能力上比基于强化学习的方法具有显著优势。此外，DiLu能够直接从真实世界数据集中获得经验。

    Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu's capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets w
    
[^162]: ReConcile：圆桌会议通过多元LLM的共识改进推理能力

    ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs. (arXiv:2309.13007v1 [cs.CL])

    [http://arxiv.org/abs/2309.13007](http://arxiv.org/abs/2309.13007)

    ReConcile是一个通过多轮讨论和投票机制来增强LLM推理能力的多模型多代理框架。

    

    大型语言模型（LLM）仍然在复杂的推理任务上遇到困难。受到心智社会理论（Minsky, 1988）的启发，我们提出了ReConcile，这是一个多模型多代理的框架，旨在通过多样的LLM代理人之间的圆桌会议来促进多样的思想和讨论，从而改进一致性。ReConcile通过进行多轮讨论、学习说服其他代理人改进答案以及采用置信度加权投票机制来增强LLM的推理能力。在每一轮中，ReConcile通过“讨论提示”来启动代理人间的讨论，其中包括上一轮每个代理人生成的答案和解释的分组、它们的不确定性以及用于说服其他代理人的答案修正人类解释的演示。这个讨论提示使每个代理人能够根据其他代理人的见解修订自己的回答。一旦达成一致并结束讨论，ReConcile执行一次全体投票以确定最终答案。

    Large Language Models (LLMs) still struggle with complex reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents to foster diverse thoughts and discussion for improved consensus. ReConcile enhances the reasoning capabilities of LLMs by holding multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their uncertainties, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. This discussion prompt enables each agent to revise their responses in light of insights from other agents. Once a consensus is reached and the discussion ends, ReConcil
    
[^163]: SilverRetriever：提升波兰问答系统的神经通道检索能力

    SilverRetriever: Advancing Neural Passage Retrieval for Polish Question Answering. (arXiv:2309.08469v1 [cs.CL])

    [http://arxiv.org/abs/2309.08469](http://arxiv.org/abs/2309.08469)

    SilverRetriever是一个特为波兰语问答系统开发的神经检索器，通过训练在多种数据集上取得了显著的改进效果，并且与更大的多语种模型具有竞争力。

    

    现代开放领域的问答系统通常依赖于准确和高效的检索组件来找到包含回答问题所需事实的段落。近年来，由于其出色的性能，神经检索器比词汇替代方式更受欢迎。然而，大部分研究都集中在流行语言如英语或中文上，对于其他语言如波兰语，可用的模型很少。在本文中，我们介绍了SilverRetriever，一个基于多种手动标记或弱标记数据集训练的波兰语神经检索器。SilverRetriever在波兰语模型中取得了比其他模型更好的结果，并与更大的多语种模型具有竞争力。与该模型一起，我们还开源了五个新的段落检索数据集。

    Modern open-domain question answering systems often rely on accurate and efficient retrieval components to find passages containing the facts necessary to answer the question. Recently, neural retrievers have gained popularity over lexical alternatives due to their superior performance. However, most of the work concerns popular languages such as English or Chinese. For others, such as Polish, few models are available. In this work, we present SilverRetriever, a neural retriever for Polish trained on a diverse collection of manually or weakly labeled datasets. SilverRetriever achieves much better results than other Polish models and is competitive with larger multilingual models. Together with the model, we open-source five new passage retrieval datasets.
    
[^164]: 关于大型语言模型在多项选择题中的选择偏差问题

    On Large Language Models' Selection Bias in Multi-Choice Questions. (arXiv:2309.03882v1 [cs.CL])

    [http://arxiv.org/abs/2309.03882](http://arxiv.org/abs/2309.03882)

    本研究发现大型语言模型在多项选择题中存在选择偏差，即倾向于选择特定位置上的选项。研究指出这一偏差的主要原因是选项编号，提出了一种名为PriDe的方法来减轻偏差，并展示了其高精度和稳定性。

    

    多项选择题（MCQs）是大型语言模型（LLMs）研究中常见且重要的任务格式。我们的工作表明，LLMs在MCQs中存在固有的“选择偏差”，即LLMs倾向于选择特定位置上的选项（如“选项C”）。这种偏差在各种LLMs中普遍存在，使得它们在MCQs中对选项位置变化的性能变得脆弱。我们发现导致选择偏差的一个主要原因是选项编号，即与选项相关的ID符号A/B/C/D。为了减轻选择偏差，我们提出了一种新方法称为PriDe。PriDe首先将观察到的模型预测分布分解为对选项内容的内在预测和对选项ID的先验分布。然后，它通过在少量测试样本上对选项内容进行排列组合来估计先验，从而用于消除后续测试样本的偏差。我们证明了作为一种无标签、推断时间方法，PriDe可以实现高精度且稳定的解决方案。

    Multi-choice questions (MCQs) serve as a common yet important task format in the research of large language models (LLMs). Our work shows that LLMs exhibit an inherent "selection bias" in MCQs, which refers to LLMs' preferences to select options located at specific positions (like "Option C"). This bias is prevalent across various LLMs, making their performance vulnerable to option position changes in MCQs. We identify that one primary cause resulting in selection bias is option numbering, i.e., the ID symbols A/B/C/D associated with the options. To mitigate selection bias, we propose a new method called PriDe. PriDe first decomposes the observed model prediction distribution into an intrinsic prediction over option contents and a prior distribution over option IDs. It then estimates the prior by permutating option contents on a small number of test samples, which is used to debias the subsequent test samples. We demonstrate that, as a label-free, inference-time method, PriDe achieves 
    
[^165]: Transformers作为支持向量机

    Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])

    [http://arxiv.org/abs/2308.16898](http://arxiv.org/abs/2308.16898)

    这项工作建立了自注意力和硬间隔支持向量机问题之间的正式等价关系，通过转换器架构的优化几何来解决自然语言处理问题，同时揭示了梯度下降优化的转换器的隐式偏差。

    

    自从"Attention Is All You Need"中引入转换器架构以来，它在自然语言处理领域取得了革命性的进展。转换器中的注意力层接受输入令牌序列$X$并通过计算softmax$(XQK^\top X^\top)$的成对相似性使它们相互作用，其中$(K,Q)$是可训练的键-查询参数。在这项工作中，我们建立了自注意力优化几何和一个硬间隔支持向量机问题之间的正式等价关系，通过对令牌对的外积施加线性约束，将最佳输入令牌与非最佳令牌分离。这个形式主义使我们能够表征梯度下降优化的单层转换器的隐式偏差：(1)优化注意力层，使用可变正则化参数$(K,Q)$，收敛的方向是一个最小化综合参数$W=KQ^\top$的核范数的支持向量机解决方案。而直接使用$W$进行参数化则最小化一个Frobenius范数目标。

    Since its inception in "Attention Is All You Need", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. 
    
[^166]: 一种基于分类引导的对抗攻击神经机器翻译方法

    A Classification-Guided Approach for Adversarial Attacks against Neural Machine Translation. (arXiv:2308.15246v1 [cs.CL])

    [http://arxiv.org/abs/2308.15246](http://arxiv.org/abs/2308.15246)

    本文介绍了一种基于分类引导的对抗攻击神经机器翻译的方法，通过改变整体意义生成保持语义的对抗样本，从而使得翻译结果属于不同的类别。

    

    神经机器翻译（NMT）模型已经被证明容易受到对抗攻击的影响，攻击者可以通过精心设计的输入扰动来误导目标模型。本文介绍了一种名为ACT的新型对抗攻击框架，针对NMT系统进行攻击，攻击过程中引导了一个分类器。在我们的攻击中，攻击者旨在生成保持语义的对抗样本，使得NMT模型的翻译结果与目标语言中的原始翻译属于不同的类别。与之前的攻击不同，我们的新方法更能改变整体意义，从而通过分类器将其归为不同的类别。为了评估NMT模型对该攻击的抵抗能力，我们提出了对现有基于单词替换的黑盒攻击进行改进的方法，通过在攻击过程中引入目标NMT模型的输出翻译和一个分类器的输出logit。通过在各种设置下进行大量实验证明了我们的方法的有效性。

    Neural Machine Translation (NMT) models have been shown to be vulnerable to adversarial attacks, wherein carefully crafted perturbations of the input can mislead the target model. In this paper, we introduce ACT, a novel adversarial attack framework against NMT systems guided by a classifier. In our attack, the adversary aims to craft meaning-preserving adversarial examples whose translations by the NMT model belong to a different class than the original translations in the target language. Unlike previous attacks, our new approach has a more substantial effect on the translation by altering the overall meaning, which leads to a different class determined by a classifier. To evaluate the robustness of NMT models to this attack, we propose enhancements to existing black-box word-replacement-based attacks by incorporating output translations of the target NMT model and the output logits of a classifier within the attack process. Extensive experiments in various settings, including a comp
    
[^167]: 基于维基百科风格的调研生成的大型语言模型：在自然语言处理概念中的评估

    Large Language Models on Wikipedia-Style Survey Generation: an Evaluation in NLP Concepts. (arXiv:2308.10410v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10410](http://arxiv.org/abs/2308.10410)

    本研究评估了大型语言模型在自然语言处理领域生成调研文章的效果，发现GPT-4优于GPT-3.5，并且指出了GPT在信息完整性和事实准确性方面的一些缺陷。

    

    大型语言模型（LLMs）在各种自然语言处理（NLP）任务中取得了重大成功，包括问答、摘要和机器翻译等。虽然LLMs在一般任务中表现出色，但它们在特定领域应用中的效果仍在探索中。此外，LLM生成的文本有时会出现幻觉和不实信息等问题。在本研究中，我们评估了LLMs在计算机科学-NLP领域中生成简洁调研文章的能力，重点关注20个选定的主题。自动评估表明，GPT-4在与真实数据进行基准测试时优于GPT-3.5。此外，四位人类评估者从四个模型配置的六个角度提供了见解。通过案例研究，我们证明了虽然GPT通常能产生可称赞的结果，但也存在一些缺点，如信息不完整和事实准确性方面的漏洞。

    Large Language Models (LLMs) have achieved significant success across various natural language processing (NLP) tasks, encompassing question-answering, summarization, and machine translation, among others. While LLMs excel in general tasks, their efficacy in domain-specific applications remains under exploration. Additionally, LLM-generated text sometimes exhibits issues like hallucination and disinformation. In this study, we assess LLMs' capability of producing concise survey articles within the computer science-NLP domain, focusing on 20 chosen topics. Automated evaluations indicate that GPT-4 outperforms GPT-3.5 when benchmarked against the ground truth. Furthermore, four human evaluators provide insights from six perspectives across four model configurations. Through case studies, we demonstrate that while GPT often yields commendable results, there are instances of shortcomings, such as incomplete information and the exhibition of lapses in factual accuracy.
    
[^168]: LLM中的时间旅行：追踪大型语言模型中的数据污染

    Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v1 [cs.CL])

    [http://arxiv.org/abs/2308.08493](http://arxiv.org/abs/2308.08493)

    该论文提出了一种用于识别大型语言模型（LLMs）中数据污染的简单而有效的方法。通过对随机样本中的单个实例进行分析，以及使用“引导指令”来评估整个数据集分区的污染程度，可以准确地识别污染的实例和分区。

    

    数据污染是指大型语言模型（LLMs）的训练数据中存在来自下游任务的测试数据，这可能是理解LLMs在其他任务上有效性的一个重要问题。我们提出了一种简单而有效的方法来识别LLMs中的数据污染。我们的方法核心是通过识别从小的随机样本中抽取的单个实例中的潜在污染，然后评估整个数据集分区是否受到污染。为了估计单个实例的污染程度，我们使用了“引导指令”：即一个由数据集名称、分区类型和参考实例的初始部分组成的提示，要求LLM完成它。如果LLM的输出与参考实例的后一部分完全或接近匹配，那么该实例被标记为受到污染。为了了解整个分区是否受到污染，我们提出了两个想法。第一个想法是标记一个数据集的分区，该分区中的实例大多数都被判断为受到污染。

    Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in understanding LLMs' effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination in individual instances that are drawn from a small random sample; using this information, our approach then assesses if an entire dataset partition is contaminated. To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or closely matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset
    
[^169]: TEST: 文本原型对齐嵌入以激活LLM对时间序列的能力

    TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series. (arXiv:2308.08241v1 [cs.CL])

    [http://arxiv.org/abs/2308.08241](http://arxiv.org/abs/2308.08241)

    这篇论文总结了两种使用语言模型完成时间序列任务的策略，通过设计一种适用于语言模型的时间序列嵌入方法来激活语言模型对时间序列数据的能力。虽然结果没有明显超越当前最先进的模型，但可以更好地处理时间序列数据。

    

    本研究总结了两种使用现代语言模型（LLM）完成时间序列（TS）任务的策略：LLM-for-TS，设计和训练一个针对TS数据的基础大模型；TS-for-LLM，使预训练的LLM能够处理TS数据。鉴于数据积累不足、资源有限和语义上下文需求，本研究侧重于TS-for-LLM方法，旨在设计一种适用于LLM的TS嵌入方法，以激活LLM对TS数据的能力。所提出的方法称为TEST。它首先对TS进行标记化处理，建立一个编码器，通过实例、特征和文本原型对齐对它们进行嵌入，然后创建提示以使LLM更容易接受嵌入，并最终实施TS任务。使用8个具有不同结构和大小的LLM对TS分类和预测任务进行了实验。尽管其结果不能显著超越当前为TS任务定制的SOTA模型，但通过将LLM视为模式机器，可以更好地处理TS数据。

    This work summarizes two strategies for completing time-series (TS) tasks using today's language model (LLM): LLM-for-TS, design and train a fundamental large model for TS data; TS-for-LLM, enable the pre-trained LLM to handle TS data. Considering the insufficient data accumulation, limited resources, and semantic context requirements, this work focuses on TS-for-LLM methods, where we aim to activate LLM's ability for TS data by designing a TS embedding method suitable for LLM. The proposed method is named TEST. It first tokenizes TS, builds an encoder to embed them by instance-wise, feature-wise, and text-prototype-aligned contrast, and then creates prompts to make LLM more open to embeddings, and finally implements TS tasks. Experiments are carried out on TS classification and forecasting tasks using 8 LLMs with different structures and sizes. Although its results cannot significantly outperform the current SOTA models customized for TS tasks, by treating LLM as the pattern machine, 
    
[^170]: 加密货币证券案件中的大型语言模型：ChatGPT能否取代律师？

    Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?. (arXiv:2308.06032v1 [cs.AI])

    [http://arxiv.org/abs/2308.06032](http://arxiv.org/abs/2308.06032)

    本研究探讨了在加密货币证券案件中，大型语言模型（LLMs）是否能够准确判断违法行为，并比较了由LLM和律师撰写的投诉书对陪审团决策的影响。研究发现，目前的LLMs在法律推理方面表现较弱，但随着未来模型的改进，其潜力有望提升。

    

    大型语言模型（LLMs）可以增强对法律系统的访问。然而，关于它们在进行法律任务方面的有效性的实证研究非常有限。我们研究涉及加密货币的证券案件，作为AI可以支持法律过程的众多情境之一，研究LLMs的法律推理和起草能力。我们检查以下两个方面：a）LLM能否准确确定事实模式中可能存在的违法行为，b）基于LLM和律师撰写的投诉书，陪审团的决策是否有所差异。我们将真实案例中的事实模式输入GPT-3.5，并评估其确定正确潜在违法行为并排除虚假违法行为的能力。其次，我们请模拟陪审员评估LLM和律师撰写的投诉书。GPT-3.5的法律推理能力较弱，但我们预期未来模型的改进，特别是考虑到它建议的违法行为往往是正确的（它仅仅过于保守）。

    Large Language Models (LLMs) could enhance access to the legal system. However, empirical research on their effectiveness in conducting legal tasks is scant. We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying LLMs' legal reasoning and drafting capabilities. We examine whether a) an LLM can accurately determine which laws are potentially being violated from a fact pattern, and b) whether there is a difference in juror decision-making based on complaints written by a lawyer compared to an LLM. We feed fact patterns from real-life cases to GPT-3.5 and evaluate its ability to determine correct potential violations from the scenario and exclude spurious violations. Second, we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely m
    
[^171]: 多模态讨论变换器：整合文本、图像和图变换器以检测社交媒体上的仇恨言论。

    Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v1 [cs.CL])

    [http://arxiv.org/abs/2307.09312](http://arxiv.org/abs/2307.09312)

    多模态讨论变换器 (mDT) 是一个用于检测在线社交网络中仇恨言论的新颖模型。与传统的仅使用文本的方法不同，mDT通过整体分析文本和图像，结合图变换器捕捉评论周围整个讨论的上下文关系，并通过交织融合层将文本和图像嵌入进行组合。研究发现，捕捉对话的整体视图可以极大地提高检测反社会行为的准确性。

    

    我们提出了一种新颖的多模态基于图的变换器模型，名为多模态讨论变换器（mDT），用于检测在线社交网络中的仇恨言论。与传统的仅使用文本的方法不同，我们将标记评论为仇恨言论的方法围绕文本和图像的整体分析展开。这是通过利用图变换器来捕捉评论周围整个讨论中的上下文关系，并采用交织融合层来组合文本和图像嵌入，而不是单独处理不同的模态。我们将模型的性能与仅处理文本的基线进行比较，还进行了广泛的消融研究。最后，我们展望了多模态解决方案在在线环境中提供社会价值的未来工作，并认为捕捉对话的整体视图极大地推进了检测反社会行为的努力。

    We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal graph-based transformer model for detecting hate speech in online social networks. In contrast to traditional text-only methods, our approach to labelling a comment as hate speech centers around the holistic analysis of text and images. This is done by leveraging graph transformers to capture the contextual relationships in the entire discussion that surrounds a comment, with interwoven fusion layers to combine text and image embeddings instead of processing different modalities separately. We compare the performance of our model to baselines that only process text; we also conduct extensive ablation studies. We conclude with future work for multimodal solutions to deliver social value in online contexts, arguing that capturing a holistic view of a conversation greatly advances the effort to detect anti-social behavior.
    
[^172]: Doc2SoarGraph：基于语义导向分层图的富含视觉表格文档的离散推理

    Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs. (arXiv:2305.01938v1 [cs.CL])

    [http://arxiv.org/abs/2305.01938](http://arxiv.org/abs/2305.01938)

    本文提出了 Doc2SoarGraph 框架，利用语义导向分层图结构中元素之间的差异和相关性，在富含视觉表格文本的TAT-DQA问题下实现了离散推理，表现出了最佳的实验结果。

    

    近两年来，对于表格文本文档（例如财务报告）的离散推理越来越受到关注。现有的工作大多通过手动选择和转换文档页面到结构化的表格和段落来简化这一挑战，从而阻碍其实际应用。在这项工作中，我们探究了一种更为现实的问题设置，即以 TAT-DQA 的形式回答富含视觉表格文本的问题。具体而言，我们提出了一种新颖的 Doc2SoarGraph 框架，通过利用语义导向分层图结构中不同元素之间的差异和相关性，提高了其离散推理能力。我们对 TAT-DQA 数据集进行了广泛的实验，结果显示，我们的提出的框架在测试集上的精确匹配（EM）和 F1 得分方面分别比最佳基线模型分别提高了 17.73% 和 16.91%，实现了新的最先进技术水平。

    Discrete reasoning over table-text documents (e.g., financial reports) gains increasing attention in recent two years. Existing works mostly simplify this challenge by manually selecting and transforming document pages to structured tables and paragraphs, hindering their practical application. In this work, we explore a more realistic problem setting in the form of TAT-DQA, i.e. to answer the question over a visually-rich table-text document. Specifically, we propose a novel Doc2SoarGraph framework with enhanced discrete reasoning capability by harnessing the differences and correlations among different elements (e.g., quantities, dates) of the given question and document with Semantic-oriented hierarchical Graph structures. We conduct extensive experiments on TAT-DQA dataset, and the results show that our proposed framework outperforms the best baseline model by 17.73% and 16.91% in terms of Exact Match (EM) and F1 score respectively on the test set, achieving the new state-of-the-art
    
[^173]: SMILE：利用ChatGPT实现单轮到多轮包容性语言扩展的心理健康支持

    SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support. (arXiv:2305.00450v1 [cs.CL])

    [http://arxiv.org/abs/2305.00450](http://arxiv.org/abs/2305.00450)

    本研究提出了SMILE方法，使用ChatGPT将公共单轮对话扩展为多轮对话，生成了大规模、多样化、接近真实生活的多轮心理健康支持对话语料库，可用于训练和评估专门的对话系统。

    

    开发专门的对话系统以提供心理健康支持已成为越来越多的研究关注点。然而，由于个人信息的敏感性以及所需的时间和成本，获取大规模的真实多轮心理健康支持对话存在困难。为了解决这些问题，我们引入了SMILE方法，一种使用ChatGPT将公共单轮对话扩展为多轮对话的包容性语言扩展技术。我们首先进行了初步的探索性研究，验证了SMILE方法的有效性。此外，我们对使用和未使用SMILE方法生成的数据集进行了全面系统的对比分析，证明SMILE方法可以产生大规模、多样化、接近真实生活的多轮心理健康支持对话语料库，包括对话主题、词汇和语义特征。最后，我们使用收集的语料库来训练和评估专门的心理健康支持对话系统。

    There has been an increasing research interest in developing specialized dialogue systems that can offer mental health support. However, gathering large-scale and real-life multi-turn conversations for mental health support poses challenges due to the sensitivity of personal information, as well as the time and cost involved. To address these issues, we introduce the SMILE approach, an inclusive language expansion technique that employs ChatGPT to extend public single-turn dialogues into multi-turn ones. Our research first presents a preliminary exploratory study that validates the effectiveness of the SMILE approach. Furthermore, we conduct a comprehensive and systematic contrastive analysis of datasets generated with and without the SMILE approach, demonstrating that the SMILE method results in a large-scale, diverse, and close-to-real-life multi-turn mental health support conversation corpus, including dialog topics, lexical and semantic features. Finally, we use the collected corpu
    
[^174]: 揭示和解决统一视觉-语言模型中的跨任务不一致问题

    Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models. (arXiv:2303.16133v1 [cs.CV])

    [http://arxiv.org/abs/2303.16133](http://arxiv.org/abs/2303.16133)

    该研究提出了一个基准数据集COCOCON，并提出度量方法来衡量模型一致性，研究发现现有的最先进系统在不同任务之间表现出高度不一致性。

    

    随着通用的视觉模型在不同任务上变得越来越有效，保证它们在各自支持的任务中的一致性是非常重要的。人们认为不一致的人工智能模型是不可靠的，这对于依赖它们输出的大型系统来说是更具挑战性的。由于很难确定预测结果是否一致，因此，评估可能包括不同模态输出的非常异构任务之间的一致性是具有挑战性的。因此，我们提出了基准数据集COCOCON，其中我们使用对多个任务的测试实例进行小型但语义上有意义的修改来创建对比集，以更改金标签，并概述了用于通过对比接近原始和修改后的实例来衡量模型一致性的指标。我们发现，最先进的系统在任务之间表现出惊人的不一致性。

    As general purpose vision models get increasingly effective at a wide set of tasks, it is imperative that they be consistent across the tasks they support. Inconsistent AI models are considered brittle and untrustworthy by human users and are more challenging to incorporate into larger systems that take dependencies on their outputs. Measuring consistency between very heterogeneous tasks that might include outputs in different modalities is challenging since it is difficult to determine if the predictions are consistent with one another. As a solution, we introduce a benchmark dataset, COCOCON, where we use contrast sets created by modifying test instances for multiple tasks in small but semantically meaningful ways to change the gold label, and outline metrics for measuring if a model is consistent by ranking the original and perturbed instances across tasks. We find that state-of-the-art systems suffer from a surprisingly high degree of inconsistent behavior across tasks, especially 
    
[^175]: 评估目标论据在谣言立场分类中的作用

    Evaluating the Role of Target Arguments in Rumour Stance Classification. (arXiv:2303.12665v1 [cs.CL])

    [http://arxiv.org/abs/2303.12665](http://arxiv.org/abs/2303.12665)

    本文重新评估了目标论据在谣言立场分类中的作用，证明最先进的模型过于依赖表面信号，目标不独立的事件自然高发生率可能导致模型过度拟合。

    

    针对一组对话，立场分类旨在确定答复对给定目标的意见（例如同意或不同意）。在这项任务中，立场的目标预计是一个重要组成部分，是使其与情感分析不同的主要因素之一。然而，最近的一项研究表明，一个忽略目标的模型优于目标感知模型，表明在预测立场时目标并不有用。本文重新审视了这一现象，针对社交媒体上的谣言立场分类（RSC），其中目标是源推文中隐含的谣言故事。我们在测试数据中提出了对抗攻击，旨在评估模型的鲁棒性并评估数据在模型性能中的作用。结果显示，包括使用整个对话线程的方法在内的最先进的模型过于依赖表面信号。我们的假设是，目标不独立的事件自然高发生率可能导致模型过度拟合。

    Considering a conversation thread, stance classification aims to identify the opinion (e.g. agree or disagree) of replies towards a given target. The target of the stance is expected to be an essential component in this task, being one of the main factors that make it different from sentiment analysis. However, a recent study shows that a target-oblivious model outperforms target-aware models, suggesting that targets are not useful when predicting stance. This paper re-examines this phenomenon for rumour stance classification (RSC) on social media, where a target is a rumour story implied by the source tweet in the conversation. We propose adversarial attacks in the test data, aiming to assess the models robustness and evaluate the role of the data in the models performance. Results show that state-of-the-art models, including approaches that use the entire conversation thread, overly relying on superficial signals. Our hypothesis is that the naturally high occurrence of target-indepen
    
[^176]: 语言建模任务中的不充分规范化：一个以因果关系为基础的性别代词消解研究

    Underspecification in Language Modeling Tasks: A Causality-Informed Study of Gendered Pronoun Resolution. (arXiv:2210.00131v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.00131](http://arxiv.org/abs/2210.00131)

    本研究通过提供一个因果模型，在语言建模任务中探讨了不充分规范化的作用，提出了两种轻量级黑盒评估方法来帮助检测任务的不充分规范化，并在性别代词消解任务中应用这些方法，同时发现了性别与时间、性别与位置之间的虚假相关性。

    

    现代语言建模任务常常存在不充分规范化的问题：对于给定的标记预测，在推断时可能有多个单词符合用户产生自然语言的意图，然而在训练时只有一个单词能够最小化任务的损失函数。我们提供了一个简单而合理的因果机制，描述了不充分规范化在生成虚假相关性方面的作用。尽管其简洁性，我们的因果模型直接指导了两种轻量级黑盒评估方法的开发，我们将其应用于广泛的语言模型任务中的性别代词消解上，以帮助 1) 检测推断时任务的不充分规范化，利用了 2）之前未报道的性别与时间、性别与位置的虚假相关性，涵盖了 A）不同规模的语言模型，从BERT-base到GPT 3.5，B）不同的预训练目标，从遮蔽和自回归语言建模到这些目标的混合，以及C）不同的训练阶段，从仅预训练到增强训练。

    Modern language modeling tasks are often underspecified: for a given token prediction, many words may satisfy the user's intent of producing natural language at inference time, however only one word would minimize the task's loss function at training time. We provide a simple yet plausible causal mechanism describing the role underspecification plays in the generation of spurious correlations. Despite its simplicity, our causal model directly informs the development of two lightweight black-box evaluation methods, that we apply to gendered pronoun resolution tasks on a wide range of LLMs to 1) aid in the detection of inference-time task underspecification by exploiting 2) previously unreported gender vs. time and gender vs. location spurious correlations on LLMs with a range of A) sizes: from BERT-base to GPT 3.5, B) pre-training objectives: from masked & autoregressive language modeling to a mixture of these objectives, and C) training stages: from pre-training only to reinforcement l
    

