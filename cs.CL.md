# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Towards Explainable AI Writing Assistants for Non-native English Speakers.](http://arxiv.org/abs/2304.02625) | 本研究探讨了非母语者使用AI写作助手时缺乏解释、难以评估转译建议的问题，同时提出了四个增加解释的用户界面设计，旨在帮助非母语用户更好地理解和评估AI生成的转译建议。 |
| [^2] | [Beyond Summarization: Designing AI Support for Real-World Expository Writing Tasks.](http://arxiv.org/abs/2304.02623) | 这篇论文讨论AI支持的说明性写作，并指出它具有独特和令人兴奋的研究挑战，可以带来高度的实际影响。 |
| [^3] | [PWESuite: Phonetic Word Embeddings and Tasks They Facilitate.](http://arxiv.org/abs/2304.02541) | 本论文展示了一套语音单词嵌入及其相关任务，提高了语音信息处理的效果和可重复性。 |
| [^4] | [Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification.](http://arxiv.org/abs/2304.02496) | 本论文探讨了ChatGPT家族模型在生物医学任务中的性能，虽然Fine-tuned的生物医学NLP任务并未表现出一致的性能增益，但显示出大型语言模型在生物医学应用中超越问答的潜力，需要更定制化的模型设计和Fine-tuning策略。 |
| [^5] | [Quantifying the Roles of Visual, Linguistic, and Visual-Linguistic Complexity in Verb Acquisition.](http://arxiv.org/abs/2304.02492) | 本研究探讨了早期动词学习不对称性的原因，并量化地证明，与名词相比，动词更难习得，需要整合视觉和语言信息。另外，多语言和多模式输入可以改善语言和视觉信息之间的不匹配。 |
| [^6] | [Rediscovering Hashed Random Projections for Efficient Quantization of Contextualized Sentence Embeddings.](http://arxiv.org/abs/2304.02481) | 本文提出了一种有效的方法，使用哈希随机投影和量化技术有效量化上下文化句子嵌入，以降低存储空间的开销，并可以用于在多种英语和德语句子分类任务上训练模型。 |
| [^7] | [Exploring AI-Generated Text in Student Writing: How Does AI Help?.](http://arxiv.org/abs/2304.02478) | 研究发现，在学生写作中使用人工智能生成文本有一定好处，但过度依赖此类工具也存在潜在风险。 |
| [^8] | [Comparative Analysis of CHATGPT and the evolution of language models.](http://arxiv.org/abs/2304.02468) | 本文比较了ChatGPT和其他主要算法在NLP任务中的性能，重点介绍了它如何无缝地弥合了语言生成和知识模型之间的分歧，并提供了一种验证ChatGPT结果的策略。 |
| [^9] | [ParroT: Translating During Chat Using Large Language Models.](http://arxiv.org/abs/2304.02426) | ParroT提出了一种基于开源LLM和人工编写的翻译评估数据的聊天翻译框架，可以将翻译数据转化为指令执行样式，并引入额外要求来规范翻译过程。在使用相对较少的训练数据的情况下，实验结果表明 ParroT 可以大幅提高翻译质量。 |
| [^10] | [Personality-aware Human-centric Multimodal Reasoning: A New Task.](http://arxiv.org/abs/2304.02313) | 本文介绍了一个新的个性化情感驱动的多模态推理任务，利用《生活大爆炸》电视剧构建了数据集，考虑个体个性，提出了三种基线方法，证明该任务具有挑战性和前景。 |
| [^11] | [Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy.](http://arxiv.org/abs/2304.02247) | 本文提出了一种通过文档层次结构诱导来检测新闻中的政治偏见的方法，该方法克服了过拟合和有限的普适性，展现了更好的鲁棒性和准确性。 |
| [^12] | [Ericson: An Interactive Open-Domain Conversational Search Agent.](http://arxiv.org/abs/2304.02233) | 本文介绍了Ericson，一个交互式的开放领域会话搜索代理人，其中包括最先进的问答和信息检索组件，以及用于主动问题细化和推荐的意图推断和对话管理模型，在Alexa Prize中经过了压力测试，并发现了现有搜索技术的局限性。 |
| [^13] | [Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT.](http://arxiv.org/abs/2304.02213) | 本文介绍了一个新的自然语言处理任务——结构化信息推理（SIS），利用GPT-3模型能够准确提取材料科学设备层面的信息，并通过实验预测PCE和反向预测参数，展示了大型语言模型在材料学中的巨大潜力。 |
| [^14] | [Document-Level Machine Translation with Large Language Models.](http://arxiv.org/abs/2304.02210) | 本文以文档级机器翻译为试验场，深入评估了大型语言模型在语篇建模方面的性能。研究发现利用LLMs强大的长文本建模能力可以提高翻译质量，在提示方面进行改进也可以显着提高翻译质量，并且LLMs有潜力编码丰富的语篇知识。 |
| [^15] | [Unleashing the Power of ChatGPT for Translation: An Empirical Study.](http://arxiv.org/abs/2304.02182) | 本文实证研究了在机器翻译中采用ChatGPT辅助的效果。实验结果表明，ChatGPT具有与专业翻译系统相当甚至更好的性能，并且在特定领域的翻译上表现优异。 |
| [^16] | [On the Impact of Voice Anonymization on Speech-Based COVID-19 Detection.](http://arxiv.org/abs/2304.02181) | 研究探讨了语音匿名化在 COVID-19 检测应用中的影响。研究发现，匿名化方法可能会对语音诊断系统的准确性产生显著影响。 |
| [^17] | [I2I: Initializing Adapters with Improvised Knowledge.](http://arxiv.org/abs/2304.02168) | 本文提出了一种称为ImprovisetoInitialize(I2I)的连续学习算法，通过提取先前学习的任务适配器的知识来为即将到来的任务初始化适配器。这使得从一个任务到另一个任务的知识传递更加高效。 |
| [^18] | [A Data Fusion Framework for Multi-Domain Morality Learning.](http://arxiv.org/abs/2304.02144) | 本文提出了一个用于训练多个异质性数据集的数据融合框架，并在对齐数据集特征空间和处理标签偏移方面做了优化。实验证明，该框架在道德推断方面取得了先进的效果。 |
| [^19] | [Geotechnical Parrot Tales (GPT): Overcoming GPT hallucinations with prompt engineering for geotechnical applications.](http://arxiv.org/abs/2304.02138) | 本文探讨了如何利用GPT在岩土工程应用中的全部潜力，着重讨论及时工程的重要性，并开发了一个统一的自然语言接口，用于处理复杂的岩土工程任务和数据分析。 |
| [^20] | [Scalable and Accurate Self-supervised Multimodal Representation Learning without Aligned Video and Text Data.](http://arxiv.org/abs/2304.02080) | 本文介绍了一种自监督的多模态表示学习方法，可以在不需要对齐视频和文本数据的情况下扩展准确的数据集。 |
| [^21] | [Effective Theory of Transformers at Initialization.](http://arxiv.org/abs/2304.02034) | 本研究分析了宽且深的Transformer中的前向和后向信号传播，提出了特定的初始化和训练超参数宽度缩放建议，并在实际设置中验证了这些建议。 |
| [^22] | [A Bibliometric Review of Large Language Models Research from 2017 to 2023.](http://arxiv.org/abs/2304.02020) | 该研究进行了大量LMMs学术文献的计量学和话语分析，为研究者、实践者和决策者提供了LMMs研究领域的最新进展和研究趋势，为未来的研究提供了路线图。 |
| [^23] | [Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing.](http://arxiv.org/abs/2304.02017) | 本文全面探讨了ChatGPT在自然语言处理中的应用、优点和局限性，强调了使用这个强大工具时的道德考虑，为人工智能和NLP领域的讨论做出了贡献。 |
| [^24] | [The Multimodal And Modular Ai Chef: Complex Recipe Generation From Imagery.](http://arxiv.org/abs/2304.02016) | 本文提出一种基于图像的轻量级和专业方法，使用多个API将对象列表作为输入传递给大型语言模型(LLM)，从而生成适合于特定约束条件的新颖的菜谱卡。 |
| [^25] | [How well do Large Language Models perform in Arithmetic tasks?.](http://arxiv.org/abs/2304.02015) | 这项工作提出了一个算术数据集 MATH 401 以评估大型语言模型的计算能力，并具体测试了 GPT-4、ChatGPT、InstrctGPT、Galactica 和 LLaMA 等模型的表现。 |
| [^26] | [Dual-Attention Neural Transducers for Efficient Wake Word Spotting in Speech Recognition.](http://arxiv.org/abs/2304.01905) | 本文介绍了一种新的“双关注神经变换器”，可以通过优化唤醒词检测来选择计算路径，从而提高唤醒词的准确性和推理时间效率，并且计算成本可以降低90％而仅增加1％的参数。这种架构可以在语音识别领域中大有裨益。 |
| [^27] | [Sociocultural knowledge is needed for selection of shots in hate speech detection tasks.](http://arxiv.org/abs/2304.01890) | HATELEXICON是一个包含巴西，德国，印度和肯尼亚仇恨言论的词汇表，利用其可以提高模型在训练中的性能表现。 |
| [^28] | [A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection.](http://arxiv.org/abs/2304.01492) | 该文介绍了一个利用对比传递框架和传播结构，将从充足资源的谣言数据学到的特征适应于低资源情况下的方式，可以检测到跨越语言和领域界限的谣言。 |
| [^29] | [To ChatGPT, or not to ChatGPT: That is the question!.](http://arxiv.org/abs/2304.01487) | 研究评估了聊天GPT检测中的最新技术和其他AI生成文本检测工具的表现，并提出区分人工生成和AI生成文本的重要性。 |
| [^30] | [The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents.](http://arxiv.org/abs/2304.01412) | 该论文介绍了StatCan对话数据集，这是一个涉及真实意图的对话转换，提出了两个任务：基于正在进行的对话自动检索相关数据表和在每个回合自动生成适当的代理响应。该研究对现有模型持续提出挑战，鼓励更多基于对话的数据检索研究。 |
| [^31] | [A Simple and Effective Method of Cross-Lingual Plagiarism Detection.](http://arxiv.org/abs/2304.01352) | 该论文提出了一种简单有效的跨语言抄袭检测方法，不依赖机器翻译和词义消歧，使用开放的多语言同义词库进行候选检索任务和预训练的基于多语言BERT的语言模型进行详细分析，在多个基准测试中取得了最先进的结果。 |
| [^32] | [Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach.](http://arxiv.org/abs/2304.01240) | 该研究使用机器学习技术，成功地识别出心理健康电子健康记录中的疼痛提及，提高了对疼痛和心理健康之间关系的理解。 |
| [^33] | [Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection.](http://arxiv.org/abs/2304.01238) | 本文通过比较不同类型的大型语言模型和传统机器学习技术在邮件垃圾检测中的表现，发现大多数情况下，大型语言模型优于传统技术，特别是在样本有限的情况下。同时，本文还介绍了经过改进和微调的Spam-T5模型，该模型具有出色的性能表现。 |
| [^34] | [AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models.](http://arxiv.org/abs/2304.00830) | 研究提出了基于潜在扩散模型的指令引导音频编辑模型AUDIT，实现了在各种音频编辑任务上的最先进性能，并通过自适应方案和文本到片段匹配模块解决了先前扩散-based方法存在的问题。 |
| [^35] | [Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations.](http://arxiv.org/abs/2303.18027) | 本文评估了GPT-4、ChatGPT和GPT-3在日本医疗执照考试中的表现，结果发现GPT-4优于其他两者，呈现出LLMs在与英语远离的语言中的潜力，但目前的LLM API存在一些限制，例如建议实施禁止的医疗选择。 |
| [^36] | [Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation.](http://arxiv.org/abs/2303.15413) | 本文提出了两种去偏置的方法，一种通过增加2D扩散模型得出的分数的截断值，一种通过调整视角提示和物体空间摄像机姿态之间的差异。实验结果表明这些方法可以显著减少伪影，提高真实感。 |
| [^37] | [Beyond Universal Transformer: block reusing with adaptor in Transformer for automatic speech recognit.](http://arxiv.org/abs/2303.13072) | 本论文提出一种用于ASR中的Transformer模型的块重用策略并配以适配器模块，使得模型更加紧凑，可适应性更强，准确性更高。 |
| [^38] | [Difficulty in learning chirality for Transformer fed with SMILES.](http://arxiv.org/abs/2303.11593) | 应用SMILES序列的Transformer模型在学习分子结构的整体性和手性方面存在困难，需要进行长时间的训练。生成的描述符用于分子性质预测时的准确率从开始到训练结束都是相似的。 |
| [^39] | [KHAN: Knowledge-Aware Hierarchical Attention Networks for Accurate Political Stance Prediction.](http://arxiv.org/abs/2302.12126) | 本文提出了一种新颖的知识感知政治立场预测方法，采用层次化注意力网络、外部知识库和知识感知损失函数，可以有效地捕捉新闻文章中的关键信息，优于现有方法。 |
| [^40] | [InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis.](http://arxiv.org/abs/2302.08624) | InstructABSA是一种使用指令学习范式的方面情感分析方法，能够显著提高Aspect Term Extraction、Aspect Term Sentiment Classification、和Joint Task subtasks三个子任务的性能，并且在多个数据集上表现超过之前的最先进方法。 |
| [^41] | [Encoding Sentence Position in Context-Aware Neural Machine Translation with Concatenation.](http://arxiv.org/abs/2302.06459) | 本文探究了在上下文感知神经机器翻译中将句子位置信息引入模型的想法，比较了不同的编码方法，结果表明在使用context-discounted loss对英语到俄语翻译时有益，但在英语到德语翻译中无显著收益。 |
| [^42] | [Vision Learners Meet Web Image-Text Pairs.](http://arxiv.org/abs/2301.07088) | 本论文提出了一种基于网络数据的新型视觉学习方法MUlti-modal Generator (MUG)。在视觉数据集的转移学习任务上取得了最先进的表现，是之前最佳结果的3.4%和2.2%的提升。 |
| [^43] | [Conditional Generation of Paired Antibody Chain Sequences through Encoder-Decoder Language Model.](http://arxiv.org/abs/2301.02748) | 本论文提出了pAbT5，这是一种能够采用编码器-解码器模型为蛋白质相互作用生成配对抗体链序列的语言模型。该模型可以准确地预测配对情况，并且能够在实验验证方面表现出最先进的无监督预测能力。 |
| [^44] | [Vision Transformers are Parameter-Efficient Audio-Visual Learners.](http://arxiv.org/abs/2212.07983) | 本文研究了冻结ViTs在没有微调任何原始参数的情况下将其推广到音像数据的能力。通过使用一个名为LAVISH的适配器和少数的可训练参数，有效融合视觉和音频提示，并在使用较少可调参数和不依赖昂贵的音频预训练的情况下，在各种音像任务上取得了竞争性性能。 |
| [^45] | [DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via Contrastive Prompt-Tuning.](http://arxiv.org/abs/2211.11337) | DreamArtist采用正负prompt-tuning学习策略来生成可控的一次性文本到图像，并解决了传统方法可能会导致模型过度拟合的问题。 |
| [^46] | [Conversion of Legal Agreements into Smart Legal Contracts using NLP.](http://arxiv.org/abs/2210.08954) | 本文提出了一个利用自然语言处理的流程，可以自动化将法律协议转换成智能合约，并通过评估，发现NER流程和问答方法可以准确地从模板文本中识别出 CiceroMark 和提取 Concerto 变量。 |
| [^47] | [Modeling Paragraph-Level Vision-Language Semantic Alignment for Multi-Modal Summarization.](http://arxiv.org/abs/2208.11303) | 本文提出了ViL-Sum，用于建模段落级别的视觉-语言语义对齐和多模态摘要生成。实验结果表明，ViL-Sum在两个基准数据集上优于现有方法，表明该方法的有效性和优越性。 |
| [^48] | [The optimality of word lengths. Theoretical foundations and an empirical study.](http://arxiv.org/abs/2208.10384) | 本文提出了两个正则化的最优性分数，它们是对最小和随机基线都进行了归一化处理的。研究表明，语言在词长方面高度优化，而Zipf的缩写定律确实是压缩的一种表现。 |
| [^49] | [Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding.](http://arxiv.org/abs/2203.05711) | 这个研究收集并发布了一个视频-语言故事数据集SYMON，用于推进多模态故事理解的发展。 |
| [^50] | [Machine Translation from Signed to Spoken Languages: State of the Art and Challenges.](http://arxiv.org/abs/2202.03086) | 手语到口语自动翻译是一个跨学科的研究领域，它的研究现状得到了总结，但手语翻译存在独特的挑战需要进一步关注。 |
| [^51] | [Undivided Attention: Are Intermediate Layers Necessary for BERT?.](http://arxiv.org/abs/2012.11881) | 本论文调查了中间层对于BERT在下游任务表现的作用，表明删除中间层数量可以减少模型参数和训练时间，同时对下游任务的影响较小，学习到的表示不受影响。 |
| [^52] | [CokeBERT: Contextual Knowledge Selection and Embedding towards Enhanced Pre-Trained Language Models.](http://arxiv.org/abs/2009.13964) | 本文提出了一种名为Coke的新框架，用于将上下文知识动态选择和嵌入到预训练语言模型中，以避免对输入文本匹配效果差的冗余和模糊知识的影响，并在知识驱动的自然语言处理任务上取得了优异表现。 |

# 详细

[^1]: 面向非英语母语者的可解释AI写作助手研究

    Towards Explainable AI Writing Assistants for Non-native English Speakers. (arXiv:2304.02625v1 [cs.CL])

    [http://arxiv.org/abs/2304.02625](http://arxiv.org/abs/2304.02625)

    本研究探讨了非母语者使用AI写作助手时缺乏解释、难以评估转译建议的问题，同时提出了四个增加解释的用户界面设计，旨在帮助非母语用户更好地理解和评估AI生成的转译建议。

    

    本研究通过对15名英语非母语者的访谈，探讨了非母语者使用AI写作助手进行转译时面临的挑战。研究发现，由于AI写作助手生成的建议转译缺乏解释，这些用户在评估转译文本时遇到困难。此外，本研究还调查了这些用户在缺少解释的情况下评估AI生成文本的策略。基于访谈中发现的非母语用户需求，本研究提出了四种潜在的用户界面设计，旨在通过增加解释来改善非母语用户使用AI写作助手的写作体验。

    We highlight the challenges faced by non-native speakers when using AI writing assistants to paraphrase text. Through an interview study with 15 non-native English speakers (NNESs) with varying levels of English proficiency, we observe that they face difficulties in assessing paraphrased texts generated by AI writing assistants, largely due to the lack of explanations accompanying the suggested paraphrases. Furthermore, we examine their strategies to assess AI-generated texts in the absence of such explanations. Drawing on the needs of NNESs identified in our interview, we propose four potential user interfaces to enhance the writing experience of NNESs using AI writing assistants. The proposed designs focus on incorporating explanations to better support NNESs in understanding and evaluating the AI-generated paraphrasing suggestions.
    
[^2]: 超越摘要：设计用于现实世界说明性写作任务的AI支持

    Beyond Summarization: Designing AI Support for Real-World Expository Writing Tasks. (arXiv:2304.02623v1 [cs.CL])

    [http://arxiv.org/abs/2304.02623](http://arxiv.org/abs/2304.02623)

    这篇论文讨论AI支持的说明性写作，并指出它具有独特和令人兴奋的研究挑战，可以带来高度的实际影响。

    

    大型语言模型为设计和开发新的AI辅助写作工具带来了令人兴奋的机遇和挑战。最近的研究表明，利用这一新技术可以在许多情况下改变写作，如创意写作中的构思、编辑支持和摘要。然而，AI支持的说明性写作，包括学者撰写文献综述或医生撰写进展报告等现实任务，相对不够研究。在这篇立场论文中，我们认为为说明性写作开发AI支持具有独特和令人兴奋的研究挑战，可以带来高度的实际影响。我们将说明性写作描述为基于证据和生成知识的：它包含外部文档的总结以及新信息或知识。它可以被看作是作者在一组源文件上进行意义构建过程的产物，阅读、思考和写作之间的相互作用开启了新的机遇。

    Large language models have introduced exciting new opportunities and challenges in designing and developing new AI-assisted writing support tools. Recent work has shown that leveraging this new technology can transform writing in many scenarios such as ideation during creative writing, editing support, and summarization. However, AI-supported expository writing--including real-world tasks like scholars writing literature reviews or doctors writing progress notes--is relatively understudied. In this position paper, we argue that developing AI supports for expository writing has unique and exciting research challenges and can lead to high real-world impacts. We characterize expository writing as evidence-based and knowledge-generating: it contains summaries of external documents as well as new information or knowledge. It can be seen as the product of authors' sensemaking process over a set of source documents, and the interplay between reading, reflection, and writing opens up new oppor
    
[^3]: PWESuite：语音单词嵌入及其任务

    PWESuite: Phonetic Word Embeddings and Tasks They Facilitate. (arXiv:2304.02541v1 [cs.CL])

    [http://arxiv.org/abs/2304.02541](http://arxiv.org/abs/2304.02541)

    本论文展示了一套语音单词嵌入及其相关任务，提高了语音信息处理的效果和可重复性。

    

    将单词映射到固定维度的向量空间的单词嵌入是现代自然语言处理的基础。大多数单词嵌入方法编码语义信息。但是，对于某些任务非常重要的语音信息经常被忽略。在这项工作中，我们开发了几种新方法，利用发声特征构建语音知情单词嵌入，并提供一套语音单词嵌入以鼓励其社区的开发、评估和使用。虽然已经存在许多学习语音单词嵌入的方法，但在评估其有效性方面缺乏一致性。因此，我们还提出了几种评估语音单词嵌入的内在方面的方法，如单词检索和与声音相似性的相关性，以及外在表现，如韵律和同源检测和声音类比。我们希望我们的任务套件将促进可重复性并提供未来语音单词嵌入研究的方向。

    Word embeddings that map words into a fixed-dimensional vector space are the backbone of modern NLP. Most word embedding methods encode semantic information. However, phonetic information, which is important for some tasks, is often overlooked. In this work, we develop several novel methods which leverage articulatory features to build phonetically informed word embeddings, and present a set of phonetic word embeddings to encourage their community development, evaluation and use. While several methods for learning phonetic word embeddings already exist, there is a lack of consistency in evaluating their effectiveness. Thus, we also proposes several ways to evaluate both intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and extrinsic performances, such as rhyme and cognate detection and sound analogies. We hope that our suite of tasks will promote reproducibility and provide direction for future research on phonetic word embeddi
    
[^4]: ChatGPT家族模型在生物医学推理和分类中的评估

    Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification. (arXiv:2304.02496v1 [cs.CL])

    [http://arxiv.org/abs/2304.02496](http://arxiv.org/abs/2304.02496)

    本论文探讨了ChatGPT家族模型在生物医学任务中的性能，虽然Fine-tuned的生物医学NLP任务并未表现出一致的性能增益，但显示出大型语言模型在生物医学应用中超越问答的潜力，需要更定制化的模型设计和Fine-tuning策略。

    

    最近大型语言模型的不断提升展现了其在生物医学问答方面的卓越能力，但尚未充分研究其在更具体的生物医学应用中的表现。这项研究探讨了ChatGPT家族模型（GPT-3.5s，GPT-4）等大型语言模型在生物医学任务中的性能，在OpenAI API公共接口中不能传递患者数据的情况下，我们使用超过10000个样本作为两个基本临床任务分类和推理的代理进行模型性能评估。第一个任务是将科学文献中的临床和政策建议陈述归类为健康建议。第二个任务是从生物医学文献中检测因果关系。我们将大型语言模型与简单模型（如逻辑回归的词袋模型）和Fine-tuned的BioBERT模型进行了比较。尽管ChatGPT非常受欢迎，但我们发现，Fine-tuned的生物医学NLP任务并未表现出一致的性能增益。然而，本研究揭示了大型语言模型在生物医学应用中超越问答的潜力，强调了需要更定制化的模型设计和Fine-tuning策略。

    Recent advances in large language models (LLMs) have shown impressive ability in biomedical question-answering, but have not been adequately investigated for more specific biomedical applications. This study investigates the performance of LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedical tasks beyond question-answering. Because no patient data can be passed to the OpenAI API public interface, we evaluated model performance with over 10000 samples as proxies for two fundamental tasks in the clinical domain classification and reasoning. The first task is classifying whether statements of clinical and policy recommendations in scientific literature constitute health advice. The second task is causal relation detection from the biomedical literature. We compared LLMs with simpler models, such as bag-of-words (BoW) with logistic regression, and fine-tuned BioBERT models. Despite the excitement around viral ChatGPT, we found that fine-tuning for two fundamental NLP
    
[^5]: 量化视觉、语言和视觉-语言复杂度在动词习得中的作用

    Quantifying the Roles of Visual, Linguistic, and Visual-Linguistic Complexity in Verb Acquisition. (arXiv:2304.02492v1 [cs.CL])

    [http://arxiv.org/abs/2304.02492](http://arxiv.org/abs/2304.02492)

    本研究探讨了早期动词学习不对称性的原因，并量化地证明，与名词相比，动词更难习得，需要整合视觉和语言信息。另外，多语言和多模式输入可以改善语言和视觉信息之间的不匹配。

    

    幼儿学习名词的意义通常比学习动词的意义早。但是，不清楚这种不对称性是由于语言所指的世界中类别的视觉结构的复杂性，语言本身的结构，还是两种信息来源之间的相互作用所致。我们通过使用大规模预训练的人工神经网络中来源于视觉和语言的单词表示，定量地测试了关于早期动词学习的这三个假说。通过检查视觉和语言嵌入空间的结构，我们发现，首先，与名词的表示相比，动词的表示在域内通常更加变化和不可辨识。其次，我们发现，如果每个类别只有一个学习实例，那么动词系统中的视觉和语言表示比名词系统中的不太吻合。然而，与人类语言发展的过程类似，如果在学习期间有多语言和多模式输入，那么名词和动词的语言和视觉信息都变得更加吻合。我们的研究结果表明，早期动词学习的不对称性至少部分归因于更复杂的动词含义，这需要集成视觉和语言信息，而不仅仅是视觉复杂度或语言结构。

    Children typically learn the meanings of nouns earlier than the meanings of verbs. However, it is unclear whether this asymmetry is a result of complexity in the visual structure of categories in the world to which language refers, the structure of language itself, or the interplay between the two sources of information. We quantitatively test these three hypotheses regarding early verb learning by employing visual and linguistic representations of words sourced from large-scale pre-trained artificial neural networks. Examining the structure of both visual and linguistic embedding spaces, we find, first, that the representation of verbs is generally more variable and less discriminable within domain than the representation of nouns. Second, we find that if only one learning instance per category is available, visual and linguistic representations are less well aligned in the verb system than in the noun system. However, in parallel with the course of human language development, if mult
    
[^6]: 重新发现基于哈希的随机投影，用于有效量化上下文化句子嵌入

    Rediscovering Hashed Random Projections for Efficient Quantization of Contextualized Sentence Embeddings. (arXiv:2304.02481v1 [cs.CL])

    [http://arxiv.org/abs/2304.02481](http://arxiv.org/abs/2304.02481)

    本文提出了一种有效的方法，使用哈希随机投影和量化技术有效量化上下文化句子嵌入，以降低存储空间的开销，并可以用于在多种英语和德语句子分类任务上训练模型。

    

    由于计算能力的限制，对边缘设备进行训练和推断通常需要高效的设置。尽管预先计算数据表示并在服务器上缓存可以减少边缘设备的计算量，但这会带来两个挑战。首先，存储在服务器上所需的存储量随实例数量呈线性增长。其次，需要发送大量数据的带宽到边缘设备。为了减少预先计算的数据表示的存储空间开销，我们提出了一种简单但有效的方法，即使用随机初始化的超平面投影。为了将它们的大小进一步缩小至98.96％，我们将得到的浮点表示量化为二进制向量。尽管大小大大缩小，但我们表明这些嵌入对多种保留了94％-99％浮点值的英语和德语句子分类任务训练模型仍然有效。

    Training and inference on edge devices often requires an efficient setup due to computational limitations. While pre-computing data representations and caching them on a server can mitigate extensive edge device computation, this leads to two challenges. First, the amount of storage required on the server that scales linearly with the number of instances. Second, the bandwidth required to send extensively large amounts of data to an edge device. To reduce the memory footprint of pre-computed data representations, we propose a simple, yet effective approach that uses randomly initialized hyperplane projections. To further reduce their size by up to 98.96%, we quantize the resulting floating-point representations into binary vectors. Despite the greatly reduced size, we show that the embeddings remain effective for training models across various English and German sentence classification tasks that retain 94%--99% of their floating-point.
    
[^7]: 探索学生写作中的人工智能生成文本：AI能起到什么作用？

    Exploring AI-Generated Text in Student Writing: How Does AI Help?. (arXiv:2304.02478v1 [cs.CL])

    [http://arxiv.org/abs/2304.02478](http://arxiv.org/abs/2304.02478)

    研究发现，在学生写作中使用人工智能生成文本有一定好处，但过度依赖此类工具也存在潜在风险。

    

    以英语作为外语的学生使用人工智能自然语言生成工具生成的文本可能会提高他们的写作质量。然而，目前尚不清楚这些学生的写作中使用人工智能生成文本在多大程度上会导致更高质量的写作。我们探索了23名香港中学生撰写故事（包含自己的文字和人工智能生成的文本）的尝试。人类专家对这些故事进行了内容、语言和组织方面的评分。我们分析了故事中的AI生成文本的基本组织结构和句法复杂度，并执行了多元线性回归和聚类分析。结果表明，人类词语的数量和人工智能生成词语的数量对分数有重要贡献。此外，与同龄人相比，学生的写作可以分为擅长和不擅长使用更多或更少人工智能生成文本的两组。聚类比较显示，使用人工智能生成文本在学生写作中有一定好处，但同时也强调了过度依赖这种工具的潜在风险。

    English as foreign language_EFL_students' use of text generated from artificial intelligence_AI_natural language generation_NLG_tools may improve their writing quality. However, it remains unclear to what extent AI-generated text in these students' writing might lead to higher-quality writing. We explored 23 Hong Kong secondary school students' attempts to write stories comprising their own words and AI-generated text. Human experts scored the stories for dimensions of content, language and organization. We analyzed the basic organization and structure and syntactic complexity of the stories' AI-generated text and performed multiple linear regression and cluster analyses. The results show the number of human words and the number of AI-generated words contribute significantly to scores. Besides, students can be grouped into competent and less competent writers who use more AI-generated text or less AI-generated text compared to their peers. Comparisons of clusters reveal some benefit of
    
[^8]: CHATGPT的比较分析及语言模型的演进

    Comparative Analysis of CHATGPT and the evolution of language models. (arXiv:2304.02468v1 [cs.CL])

    [http://arxiv.org/abs/2304.02468](http://arxiv.org/abs/2304.02468)

    本文比较了ChatGPT和其他主要算法在NLP任务中的性能，重点介绍了它如何无缝地弥合了语言生成和知识模型之间的分歧，并提供了一种验证ChatGPT结果的策略。

    

    自ChatGPT出现以来，大型语言模型（LLMs）引起了极大的兴趣，人们对它在自然语言处理（NLP）任务中的高效表现给予了极大的正面社会反应。然而，ChatGPT的胜利在于它如何无缝地弥合了语言生成和知识模型之间的分歧。在某些情况下，它提供了一个框架来复制人类对知识领域的直觉。本文重点介绍了NLP领域中的主要想法，包括机器翻译、机器摘要、问答以及语言生成，并使用“自发质量”（SQ）得分比较了ChatGPT与每个类别中主要算法的性能。本文还概述了一种验证ChatGPT的论点和结果的策略，作为安全、大规模采用LLMs的示例。

    Interest in Large Language Models (LLMs) has increased drastically since the emergence of ChatGPT and the outstanding positive societal response to the ease with which it performs tasks in Natural Language Processing (NLP). The triumph of ChatGPT, however, is how it seamlessly bridges the divide between language generation and knowledge models. In some cases, it provides anecdotal evidence of a framework for replicating human intuition over a knowledge domain. This paper highlights the prevailing ideas in NLP, including machine translation, machine summarization, question-answering, and language generation, and compares the performance of ChatGPT with the major algorithms in each of these categories using the Spontaneous Quality (SQ) score. A strategy for validating the arguments and results of ChatGPT is presented summarily as an example of safe, large-scale adoption of LLMs.
    
[^9]: ParroT: 使用大型语言模型进行聊天翻译

    ParroT: Translating During Chat Using Large Language Models. (arXiv:2304.02426v1 [cs.CL])

    [http://arxiv.org/abs/2304.02426](http://arxiv.org/abs/2304.02426)

    ParroT提出了一种基于开源LLM和人工编写的翻译评估数据的聊天翻译框架，可以将翻译数据转化为指令执行样式，并引入额外要求来规范翻译过程。在使用相对较少的训练数据的情况下，实验结果表明 ParroT 可以大幅提高翻译质量。

    

    大型语言模型（LLM）如 ChatGPT 和 GPT-4 在各种自然语言处理（NLP）任务上展现出了卓越的能力，包括在聊天过程中完成各种机器翻译能力。然而，这些模型只能通过受限的API访问，这为新的研究和领域进展带来了障碍。因此，我们提出了 ParroT 框架，基于开源LLM（如LLaMA-7b）和人工编写的翻译评估数据来增强和规范聊天翻译能力。具体而言，ParroT将翻译数据转化为指令执行的样式，并引入 "Hint " 字段以加入额外要求来规范翻译过程。因此，我们提出了三种指令类型来微调 ParroT 模型，包括翻译指令、对比指令和误差引导指令。在两个 Flores 子集和 WMT22 测试集上的实验证明，使用 ParroT 可以大幅提高翻译质量，且需要相对较少的训练数据。

    Large language models (LLMs) like ChatGPT and GPT-4 have exhibited remarkable abilities on a wide range of natural language processing (NLP) tasks, including various machine translation abilities accomplished during chat. However, these models are only accessible through restricted APIs, which creates barriers to new research and advancements in the field. Therefore, we propose the $\mathbf{ParroT}$ framework to enhance and regulate the translation abilities during chat based on open-sourced LLMs (i.e., LLaMA-7b) and human written translation and evaluation data. Specifically, ParroT reformulates translation data into the instruction-following style, and introduces a "Hint" field for incorporating extra requirements to regulate the translation process. Accordingly, we propose three instruction types for finetuning ParroT models, including translation instruction, contrastive instruction, and error-guided instruction. Experiments on two Flores subsets and WMT22 test sets suggest that tr
    
[^10]: 个性化情感驱动的多模态推理：一个新的任务

    Personality-aware Human-centric Multimodal Reasoning: A New Task. (arXiv:2304.02313v1 [cs.CL])

    [http://arxiv.org/abs/2304.02313](http://arxiv.org/abs/2304.02313)

    本文介绍了一个新的个性化情感驱动的多模态推理任务，利用《生活大爆炸》电视剧构建了数据集，考虑个体个性，提出了三种基线方法，证明该任务具有挑战性和前景。

    

    多模态推理是一种人工智能领域，旨在从诸如视觉、语言和语音等多模态信号中进行推理和判断，近年来越来越受到关注。不同个性的人可能对同一情境做出不同反应。然而，在以前的研究中，个性这一方面并没有得到很好的考虑。本文提出了一个新的个性化情感驱动的多模态推理任务（Personality-aware HMR），并根据《生活大爆炸》电视剧构建了一个新的数据集，以预测特定时刻特定人物的行为，基于其过去和未来时刻的多模态信息。Myers-Briggs类型指标（MBTI）被注释并用于表示个体的个性。我们通过提出三种基线方法来基准测试该任务，其中两种是从相关任务中进行调整的，而一种是新提出的。实验结果表明，个性化情感驱动的多模态推理是一项具有挑战性和前景的任务，需要考虑个体个性的多模态推理。

    Multimodal reasoning, an area of artificial intelligence that aims at make inferences from multimodal signals such as vision, language and speech, has drawn more and more attention in recent years. People with different personalities may respond differently to the same situation. However, such individual personalities were ignored in the previous studies. In this work, we introduce a new Personality-aware Human-centric Multimodal Reasoning (Personality-aware HMR) task, and accordingly construct a new dataset based on The Big Bang Theory television shows, to predict the behavior of a specific person at a specific moment, given the multimodal information of its past and future moments. The Myers-Briggs Type Indicator (MBTI) was annotated and utilized in the task to represent individuals' personalities. We benchmark the task by proposing three baseline methods, two were adapted from the related tasks and one was newly proposed for our task. The experimental results demonstrate that person
    
[^11]: 《解开结构与风格的纽带：通过诱导文档层次结构来检测新闻中的政治偏见》

    Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy. (arXiv:2304.02247v1 [cs.CL])

    [http://arxiv.org/abs/2304.02247](http://arxiv.org/abs/2304.02247)

    本文提出了一种通过文档层次结构诱导来检测新闻中的政治偏见的方法，该方法克服了过拟合和有限的普适性，展现了更好的鲁棒性和准确性。

    

    本文针对新闻文章中政治偏见检测方面的重要差距进行研究。先前进行监督式文档分类的工作可能会偏向各网站的写作风格，导致过拟合和有限的普适性。我们的方法通过考虑句子级语义和文档级修辞结构来克服这一限制，从而产生一种更强大和不受风格影响的检测政治偏见的方法。我们引入了一种新颖的多头分层注意力模型，通过各种注意力头的不同集合有效地编码长文档的结构。我们展示了我们的方法克服了这种域依赖性，并表现出比先前方法更好的鲁棒性和准确性。进一步的分析表明，我们的模型能够捕捉到新闻中常用的话语结构。

    We address an important gap in detection of political bias in news articles. Previous works that perform supervised document classification can be biased towards the writing style of each news outlet, leading to overfitting and limited generalizability. Our approach overcomes this limitation by considering both the sentence-level semantics and the document-level rhetorical structure, resulting in a more robust and style-agnostic approach to detecting political bias in news articles. We introduce a novel multi-head hierarchical attention model that effectively encodes the structure of long documents through a diverse ensemble of attention heads. While journalism follows a formalized rhetorical structure, the writing style may vary by news outlet. We demonstrate that our method overcomes this domain dependency and outperforms previous approaches for robustness and accuracy. Further analysis demonstrates the ability of our model to capture the discourse structures commonly used in the jou
    
[^12]: Ericson: 一个交互式的开放领域会话搜索代理人

    Ericson: An Interactive Open-Domain Conversational Search Agent. (arXiv:2304.02233v1 [cs.CL])

    [http://arxiv.org/abs/2304.02233](http://arxiv.org/abs/2304.02233)

    本文介绍了Ericson，一个交互式的开放领域会话搜索代理人，其中包括最先进的问答和信息检索组件，以及用于主动问题细化和推荐的意图推断和对话管理模型，在Alexa Prize中经过了压力测试，并发现了现有搜索技术的局限性。

    

    开放式领域会话搜索（ODCS）旨在提供有价值的、最新的信息，同时保持自然的对话，以帮助用户细化并最终回答信息需求。然而，创建一个有效和强大的ODCS代理人是具有挑战性的。在本文中，我们介绍了一个完全功能的ODCS系统Ericson，其中包括最先进的问答和信息检索组件，以及用于主动问题细化和推荐的意图推断和对话管理模型。我们的系统在亚马逊Alexa Prize中经过了压力测试，与数千名Alexa用户进行了实时对话，从而为分析ODCS系统在真实环境中的基础提供了经验依据。我们的交互数据分析表明，精确的意图分类、鼓励用户参与和仔细的主动推荐对用户满意度做出了最大的贡献。我们的研究进一步确定了现有搜索技术的局限性。

    Open-domain conversational search (ODCS) aims to provide valuable, up-to-date information, while maintaining natural conversations to help users refine and ultimately answer information needs. However, creating an effective and robust ODCS agent is challenging. In this paper, we present a fully functional ODCS system, Ericson, which includes state-of-the-art question answering and information retrieval components, as well as intent inference and dialogue management models for proactive question refinement and recommendations. Our system was stress-tested in the Amazon Alexa Prize, by engaging in live conversations with thousands of Alexa users, thus providing empirical basis for the analysis of the ODCS system in real settings. Our interaction data analysis revealed that accurate intent classification, encouraging user engagement, and careful proactive recommendations contribute most to the users satisfaction. Our study further identifies limitations of the existing search techniques, 
    
[^13]: 大型语言模型作为钥匙：用GPT解密材料科学的秘密。

    Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v1 [cs.CL])

    [http://arxiv.org/abs/2304.02213](http://arxiv.org/abs/2304.02213)

    本文介绍了一个新的自然语言处理任务——结构化信息推理（SIS），利用GPT-3模型能够准确提取材料科学设备层面的信息，并通过实验预测PCE和反向预测参数，展示了大型语言模型在材料学中的巨大潜力。

    

    本文介绍了一个新的自然语言处理（NLP）任务——结构化信息推理（SIS），以解决材料科学设备层面信息提取的复杂性。我们使用现有的钙钛矿太阳能电池FAIR数据集对GPT-3进行微调，获得了91.8 F1得分，并更新了数据集，包括迄今为止所有相关科学论文。所生成的数据集已被格式化和标准化，使得它可以直接作为后续数据分析的输入。这个特性将使材料科学家通过选择高质量的领域评论文章来开发其自己的模型。此外，我们设计了实验来预测PCE和反向预测参数，并获得了与DFT相当的性能，这证明了大型语言模型能够像材料学家一样评判材料和设计新材料。

    This article presents a new NLP task called structured information inference (SIS) to address the complexities of information extraction at the device level in materials science. We accomplished this task by finetuning GPT-3 on a exsiting perovskite solar cell FAIR dataset with 91.8 F1-score and we updated the dataset with all related scientific papers up to now. The produced dataset is formatted and normalized, enabling its direct utilization as input in subsequent data analysis. This feature will enable materials scientists to develop their own models by selecting high-quality review papers within their domain. Furthermore, we designed experiments to predict PCE and reverse-predict parameters and obtained comparable performance with DFT, which demonstrates the potential of large language models to judge materials and design new materials like a materials scientist.
    
[^14]: 大型语言模型在文档级机器翻译中的应用研究

    Document-Level Machine Translation with Large Language Models. (arXiv:2304.02210v1 [cs.CL])

    [http://arxiv.org/abs/2304.02210](http://arxiv.org/abs/2304.02210)

    本文以文档级机器翻译为试验场，深入评估了大型语言模型在语篇建模方面的性能。研究发现利用LLMs强大的长文本建模能力可以提高翻译质量，在提示方面进行改进也可以显着提高翻译质量，并且LLMs有潜力编码丰富的语篇知识。

    

    大型语言模型（LLMs）如Chat-GPT可以为各种自然语言处理（NLP）任务生成连贯，连贯，相关和流畅的答案。本文以文档级机器翻译为试验场，提供了LLMs在语篇建模方面的深入评估。本研究着重关注三个方面：1）语篇感知提示的影响，我们调查不同提示对文档级翻译质量和语篇现象的影响；2）翻译模型的比较，我们比较Chat-GPT与商业MT系统和高级文档级MT方法的翻译性能；3）语篇建模能力分析，我们进一步探究LLMs中编码的语篇知识，并研究培训技术对语篇建模的影响。通过评估许多基准测试，我们惊讶地发现，1）利用强大的长文本建模能力，ChatGPT在文档级翻译质量方面优于商业MT系统和高级文档级MT方法；2）修改明确针对语篇现象的提示可以显着提高翻译质量；3）LLMs有潜力编码丰富的语篇知识，培训技术可以进一步增强这种能力。

    Large language models (LLMs) such as Chat-GPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs' ability on discourse modeling. The study fo-cuses on three aspects: 1) Effects of Discourse-Aware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models, where we compare the translation performance of Chat-GPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities, where we further probe discourse knowledge encoded in LLMs and examine the impact of training techniques on discourse modeling. By evaluating a number of benchmarks, we surprisingly find that 1) leveraging their powerful long-text mod-eling capabilities, ChatGPT outperforms commercial MT systems 
    
[^15]: 发掘ChatGPT翻译的能力：一项实证研究

    Unleashing the Power of ChatGPT for Translation: An Empirical Study. (arXiv:2304.02182v1 [cs.CL])

    [http://arxiv.org/abs/2304.02182](http://arxiv.org/abs/2304.02182)

    本文实证研究了在机器翻译中采用ChatGPT辅助的效果。实验结果表明，ChatGPT具有与专业翻译系统相当甚至更好的性能，并且在特定领域的翻译上表现优异。

    

    最近发布的ChatGPT展示了在自然语言理解和自然语言生成方面惊人的能力。机器翻译是自然语言处理领域中一个重要且广泛研究的任务，它严重依赖于语言理解和生成的能力。在本文中，我们探讨如何使用ChatGPT辅助机器翻译。我们在广泛的翻译中采用了几个翻译提示。我们的实验证明，使用设计好的翻译提示的ChatGPT可以在高资源语言翻译中达到与专业翻译系统相当或更好的性能，但在低资源翻译上严重滞后。我们进一步使用多个参考文本对翻译质量进行评估，结果显示ChatGPT相对于专业系统表现更加优异。我们还在特定领域的翻译上进行实验，最终结果表明ChatGPT能够达到与专业翻译系统相媲美的水平。

    The recently released ChatGPT has demonstrated surprising abilities in natural language understanding and natural language generation. Machine translation is an important and extensively studied task in the field of natural language processing, which heavily relies on the abilities of language understanding and generation. Thus, in this paper, we explore how to assist machine translation with ChatGPT. We adopt several translation prompts on a wide range of translations. Our experimental results show that ChatGPT with designed translation prompts can achieve comparable or better performance over professional translation systems for high-resource language translations but lags behind significantly on low-resource translations. We further evaluate the translation quality using multiple references, and ChatGPT achieves superior performance compared to the professional systems. We also conduct experiments on domain-specific translations, the final results show that ChatGPT is able to compre
    
[^16]: 关于声音匿名化对基于语音的COVID-19检测的影响研究

    On the Impact of Voice Anonymization on Speech-Based COVID-19 Detection. (arXiv:2304.02181v1 [cs.CL])

    [http://arxiv.org/abs/2304.02181](http://arxiv.org/abs/2304.02181)

    研究探讨了语音匿名化在 COVID-19 检测应用中的影响。研究发现，匿名化方法可能会对语音诊断系统的准确性产生显著影响。

    

    随着深度学习的发展，基于语音的应用正蓬勃发展，从个人助理、情感计算到远程疾病诊断。由于声音同时包含语言和语用信息（如语音音调、语调、语速、声音大小），因此保护说话者的隐私和身份的声音匿名化引起了广泛的关注。近年来，声音隐私问题已经出现，重点是去除说话者身份，同时保留语言内容。然而，对于情感计算和疾病监测应用而言，语用内容可能更为关键。遗憾的是，匿名化可能对这些系统产生的影响仍然不明确。在本文中，我们填补了这个空白，并专注于一个特定的健康监测应用：基于语音的COVID-19诊断。我们测试了两种流行的匿名化方法及其对五种最先进的COVID-19诊断系统的影响。

    With advances seen in deep learning, voice-based applications are burgeoning, ranging from personal assistants, affective computing, to remote disease diagnostics. As the voice contains both linguistic and paralinguistic information (e.g., vocal pitch, intonation, speech rate, loudness), there is growing interest in voice anonymization to preserve speaker privacy and identity. Voice privacy challenges have emerged over the last few years and focus has been placed on removing speaker identity while keeping linguistic content intact. For affective computing and disease monitoring applications, however, the paralinguistic content may be more critical. Unfortunately, the effects that anonymization may have on these systems are still largely unknown. In this paper, we fill this gap and focus on one particular health monitoring application: speech-based COVID-19 diagnosis. We test two popular anonymization methods and their impact on five different state-of-the-art COVID-19 diagnostic system
    
[^17]: I2I: 用改进的知识初始化转接器

    I2I: Initializing Adapters with Improvised Knowledge. (arXiv:2304.02168v1 [cs.CL])

    [http://arxiv.org/abs/2304.02168](http://arxiv.org/abs/2304.02168)

    本文提出了一种称为ImprovisetoInitialize(I2I)的连续学习算法，通过提取先前学习的任务适配器的知识来为即将到来的任务初始化适配器。这使得从一个任务到另一个任务的知识传递更加高效。

    

    转接器是延续学习中解决灾难性遗忘问题的一种有前途的解决方案。然而，为每个新任务训练独立的适配器模块错失了跨任务知识转移的机会。我们提出了一种称为 Improvise to Initialize (I2I) 的连续学习算法，通过提取先前学习的任务适配器的知识，为即将到来的任务初始化适配器。我们通过对视觉问答任务序列进行实验，评估了 I2I 在 CLiMB，一个多模态的连续学习基准上的表现。使用 I2I 训练的适配器始终比独立训练的适配器具有更好的任务精度，证明了我们的算法促进了任务适配器之间的知识转移，并且相对于先进的 AdapterFusion，I2I 也能实现更好的跨任务知识转移而不产生相关的参数成本。

    Adapters present a promising solution to the catastrophic forgetting problem in continual learning. However, training independent Adapter modules for every new task misses an opportunity for cross-task knowledge transfer. We propose Improvise to Initialize (I2I), a continual learning algorithm that initializes Adapters for incoming tasks by distilling knowledge from previously-learned tasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning benchmark, by conducting experiments on sequences of visual question answering tasks. Adapters trained with I2I consistently achieve better task accuracy than independently-trained Adapters, demonstrating that our algorithm facilitates knowledge transfer between task Adapters. I2I also results in better cross-task knowledge transfer than the state-of-the-art AdapterFusion without incurring the associated parametric cost.
    
[^18]: 多领域道德学习的数据融合框架

    A Data Fusion Framework for Multi-Domain Morality Learning. (arXiv:2304.02144v1 [cs.CL])

    [http://arxiv.org/abs/2304.02144](http://arxiv.org/abs/2304.02144)

    本文提出了一个用于训练多个异质性数据集的数据融合框架，并在对齐数据集特征空间和处理标签偏移方面做了优化。实验证明，该框架在道德推断方面取得了先进的效果。

    

    语言模型可以被训练用于识别文本中的道德情感，从而创造了研究道德在人类生活中扮演角色的新机会。然而，这种类型的数据集往往因为数据采集方法、领域、主题、标注者说明等方面的差异而存在异质性。简单地在训练中聚合多个这种异质性的数据集可能导致模型泛化能力不佳。因此，本文提出了一种多个异质性数据集联合训练的数据融合框架，采用领域对抗训练使得数据集在特征空间中对齐，同时用加权损失函数来处理标签偏移。实验结果表明，与先前的研究成果相比，提出的框架在不同数据集上实现了最先进的效果。

    Language models can be trained to recognize the moral sentiment of text, creating new opportunities to study the role of morality in human life. As interest in language and morality has grown, several ground truth datasets with moral annotations have been released. However, these datasets vary in the method of data collection, domain, topics, instructions for annotators, etc. Simply aggregating such heterogeneous datasets during training can yield models that fail to generalize well. We describe a data fusion framework for training on multiple heterogeneous datasets that improve performance and generalizability. The model uses domain adversarial training to align the datasets in feature space and a weighted loss function to deal with label shift. We show that the proposed framework achieves state-of-the-art performance in different datasets compared to prior works in morality inference.
    
[^19]: 土木鹦鹉传奇(GPT)：利用及时工程克服GPT幻觉以在岩土工程中应用

    Geotechnical Parrot Tales (GPT): Overcoming GPT hallucinations with prompt engineering for geotechnical applications. (arXiv:2304.02138v1 [cs.CL])

    [http://arxiv.org/abs/2304.02138](http://arxiv.org/abs/2304.02138)

    本文探讨了如何利用GPT在岩土工程应用中的全部潜力，着重讨论及时工程的重要性，并开发了一个统一的自然语言接口，用于处理复杂的岩土工程任务和数据分析。

    

    大型语言模型（LLM）的普及，如OpenAI的ChatGPT，可能会彻底改变包括岩土工程在内的各个行业。 但是，GPT模型有时会生成听起来很有道理但错误的输出，导致幻觉产生。 本文讨论了在缓解这些风险和利用GPT在岩土工程应用中的全部潜力方面，及时工程的重要性。 我们探讨了与LLM相关的挑战和陷阱，并强调了上下文在确保准确和有价值的响应方面的作用。 此外，我们还研究了特定于上下文的搜索引擎的开发以及LLM成为复杂任务（例如数据分析和设计）的自然界面的潜力。 我们还开发了一个统一的自然语言接口，用于处理复杂的岩土工程任务和数据分析。 通过将GPT集成到岩土工程工作流中，专业人员可以简化他们的工作并发展可持续性。

    The widespread adoption of large language models (LLMs), such as OpenAI's ChatGPT, could revolutionized various industries, including geotechnical engineering. However, GPT models can sometimes generate plausible-sounding but false outputs, leading to hallucinations. In this article, we discuss the importance of prompt engineering in mitigating these risks and harnessing the full potential of GPT for geotechnical applications. We explore the challenges and pitfalls associated with LLMs and highlight the role of context in ensuring accurate and valuable responses. Furthermore, we examine the development of context-specific search engines and the potential of LLMs to become a natural interface for complex tasks, such as data analysis and design. We also develop a unified interface using natural language to handle complex geotechnical engineering tasks and data analysis. By integrating GPT into geotechnical engineering workflows, professionals can streamline their work and develop sustain
    
[^20]: 无需对齐视频和文本数据，可扩展准确的自监督多模态表示学习

    Scalable and Accurate Self-supervised Multimodal Representation Learning without Aligned Video and Text Data. (arXiv:2304.02080v1 [cs.CV])

    [http://arxiv.org/abs/2304.02080](http://arxiv.org/abs/2304.02080)

    本文介绍了一种自监督的多模态表示学习方法，可以在不需要对齐视频和文本数据的情况下扩展准确的数据集。

    

    弱监督数据集的扩展在图形文本领域已被证明非常有效，并为大多数最新的计算机视觉和多模态神经网络做出了贡献。然而，现有的大规模视频文本数据集和挖掘技术存在多个限制，如对齐数据的稀缺性、数据缺乏多样性以及对齐数据的采集难度。本文展示了最新的图像字幕技术如何使我们能够在没有任何并行视频文本数据的情况下预训练高质量的视频模型。

    Scaling up weakly-supervised datasets has shown to be highly effective in the image-text domain and has contributed to most of the recent state-of-the-art computer vision and multimodal neural networks. However, existing large-scale video-text datasets and mining techniques suffer from several limitations, such as the scarcity of aligned data, the lack of diversity in the data, and the difficulty of collecting aligned data. Currently popular video-text data mining approach via automatic speech recognition (ASR) used in HowTo100M provides low-quality captions that often do not refer to the video content. Other mining approaches do not provide proper language descriptions (video tags) and are biased toward short clips (alt text). In this work, we show how recent advances in image captioning allow us to pre-train high-quality video models without any parallel video-text data. We pre-train several video captioning models that are based on an OPT language model and a TimeSformer visual back
    
[^21]: 初始化时Transformer的有效理论分析

    Effective Theory of Transformers at Initialization. (arXiv:2304.02034v1 [cs.LG])

    [http://arxiv.org/abs/2304.02034](http://arxiv.org/abs/2304.02034)

    本研究分析了宽且深的Transformer中的前向和后向信号传播，提出了特定的初始化和训练超参数宽度缩放建议，并在实际设置中验证了这些建议。

    

    我们对宽且深的Transformer（即使用多头自注意块和多层感知机块的残差神经网络）中的前向和后向信号传播进行了有效理论分析。该分析建议这些模型的初始化和训练超参数采用特定的宽度缩放。我们随后采用这些建议，在实际设置中对视觉和语言Transformer进行训练。

    We perform an effective-theory analysis of forward-backward signal propagation in wide and deep Transformers, i.e., residual neural networks with multi-head self-attention blocks and multilayer perceptron blocks. This analysis suggests particular width scalings of initialization and training hyperparameters for these models. We then take up such suggestions, training Vision and Language Transformers in practical setups.
    
[^22]: 2017年至2023年大型语言模型研究的文献计量学综述

    A Bibliometric Review of Large Language Models Research from 2017 to 2023. (arXiv:2304.02020v1 [cs.DL])

    [http://arxiv.org/abs/2304.02020](http://arxiv.org/abs/2304.02020)

    该研究进行了大量LMMs学术文献的计量学和话语分析，为研究者、实践者和决策者提供了LMMs研究领域的最新进展和研究趋势，为未来的研究提供了路线图。

    

    大型语言模型(LLMs)是一类语言模型，在自然语言处理(NLP)任务的多个领域中表现出色，因其生成人类化语言的能力和革命性的科技潜力，已成为学术研究领域中备受关注的热门课题。本研究通过对学术文献的计量学和话语分析，综合了超过5000篇文章，为研究者、实践者和决策者提供了LMMs研究领域的最新进展，是当前LMMs研究领域的一份路线图。我们提出了2017年至2023年的研究趋势及研究模式与合作模式的特点，分析了LMMs研究中的核心算法开发和NLP任务。我们还调查了LLMs在医学、工程、社会科学和人文学科等不同领域和应用方面的研究成果。同时，我们的综述还揭示了LMMs研究领域的动态变化。

    Large language models (LLMs) are a class of language models that have demonstrated outstanding performance across a range of natural language processing (NLP) tasks and have become a highly sought-after research area, because of their ability to generate human-like language and their potential to revolutionize science and technology. In this study, we conduct bibliometric and discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000 publications, this paper serves as a roadmap for researchers, practitioners, and policymakers to navigate the current landscape of LLMs research. We present the research trends from 2017 to early 2023, identifying patterns in research paradigms and collaborations. We start with analyzing the core algorithm developments and NLP tasks that are fundamental in LLMs research. We then investigate the applications of LLMs in various fields and domains including medicine, engineering, social science, and humanities. Our review also reveals the dyn
    
[^23]: 解锁ChatGPT的潜力：对其在自然语言处理中应用、优点、局限性和未来方向的全面探讨

    Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing. (arXiv:2304.02017v1 [cs.CL])

    [http://arxiv.org/abs/2304.02017](http://arxiv.org/abs/2304.02017)

    本文全面探讨了ChatGPT在自然语言处理中的应用、优点和局限性，强调了使用这个强大工具时的道德考虑，为人工智能和NLP领域的讨论做出了贡献。

    

    ChatGPT是人工智能领域中广泛应用的强大工具，已成功应用于聊天机器人、内容生成、语言翻译、个性化推荐和医疗诊断治疗。它的多功能性和准确性使其成为自然语言处理（NLP）的强大工具。但是，ChatGPT也存在局限性，例如其倾向于产生有偏见的响应以及存在潜在的有害语言模式。本文全面概述了ChatGPT及其应用、优点和局限性，并强调了在真实场景中使用这个强大工具时道德考虑的重要性。最后，本文通过提供提示工程技术的见解，为关于人工智能及其对视觉和NLP领域的影响的持续讨论做出了贡献。

    ChatGPT is a powerful tool in the field of artificial intelligence that has been widely used in various applications. ChatGPT has been applied successfully in chatbots, content generation, language translation, personalized recommendations, and medical diagnosis and treatment. Its versatility and accuracy make it a powerful tool for natural language processing (NLP). However, there are also limitations to ChatGPT, such as its tendency to produce biased responses and its potential to perpetuate harmful language patterns. This article provides a comprehensive overview of ChatGPT, its applications, advantages, and limitations. Additionally, the paper emphasizes the importance of ethical considerations when using this robust tool in real-world scenarios. Finally, This paper contributes to ongoing discussions surrounding artificial intelligence and its impact on vision and NLP domains by providing insights into prompt engineering techniques.
    
[^24]: 多模式多模块的AI大厨：基于图像的复杂菜谱生成

    The Multimodal And Modular Ai Chef: Complex Recipe Generation From Imagery. (arXiv:2304.02016v1 [cs.CL])

    [http://arxiv.org/abs/2304.02016](http://arxiv.org/abs/2304.02016)

    本文提出一种基于图像的轻量级和专业方法，使用多个API将对象列表作为输入传递给大型语言模型(LLM)，从而生成适合于特定约束条件的新颖的菜谱卡。

    

    AI社区采用多感官或多模态方法来推进这一代AI模型的智能水平。将语言和图像相结合代表了特定任务的熟悉方法，例如从描述中生成图像标题或图像。本文将这些单片式方法与基于采用图像模型标记对象的轻量级和专业方法进行了比较，然后串行提交此结果对象列表给大型语言模型(LLM)。多个API的使用使得正确对象列表的平均精度达到95%以上，这些列表作为输入传递给最新的Open AI文本生成器(GPT-4)。为了演示API作为模块化替代方案，我们解决了一个用户拍下冰箱中有哪些食材的照片，然后生成适合于成本、准备时间、饮食限制、分量大小和多个因素的新颖菜谱卡的问题。

    The AI community has embraced multi-sensory or multi-modal approaches to advance this generation of AI models to resemble expected intelligent understanding. Combining language and imagery represents a familiar method for specific tasks like image captioning or generation from descriptions. This paper compares these monolithic approaches to a lightweight and specialized method based on employing image models to label objects, then serially submitting this resulting object list to a large language model (LLM). This use of multiple Application Programming Interfaces (APIs) enables better than 95% mean average precision for correct object lists, which serve as input to the latest Open AI text generator (GPT-4). To demonstrate the API as a modular alternative, we solve the problem of a user taking a picture of ingredients available in a refrigerator, then generating novel recipe cards tailored to complex constraints on cost, preparation time, dietary restrictions, portion sizes, and multip
    
[^25]: 大型语言模型在算术任务中的表现如何？

    How well do Large Language Models perform in Arithmetic tasks?. (arXiv:2304.02015v1 [cs.CL])

    [http://arxiv.org/abs/2304.02015](http://arxiv.org/abs/2304.02015)

    这项工作提出了一个算术数据集 MATH 401 以评估大型语言模型的计算能力，并具体测试了 GPT-4、ChatGPT、InstrctGPT、Galactica 和 LLaMA 等模型的表现。

    

    大型语言模型已经具备了连贯思路的能力，能够逐步解答数学问题。解决数学问题不仅需要通过思维连贯的能力分解问题，还需要针对每个步骤正确计算算术表达式。据我们所知，还没有任何研究专门评估大型语言模型的计算能力。在这项工作中，我们提出了一个算术数据集 MATH 401，用于测试最新的大型语言模型，包括 GPT-4、ChatGPT、InstrctGPT、Galactica 和 LLaMA，其中涉及各种算术表达式，并提供了大型语言模型的能力的详细分析。MATH 401 数据集和评估代码已在 \url{https://github.com/GanjinZero/math401-llm} 上发布。

    Large language models have emerged abilities including chain-of-thought to answer math word problems step by step. Solving math word problems not only requires abilities to disassemble problems via chain-of-thought but also needs to calculate arithmetic expressions correctly for each step. To the best of our knowledge, there is no work to focus on evaluating the arithmetic ability of large language models. In this work, we propose an arithmetic dataset MATH 401 to test the latest large language models including GPT-4, ChatGPT, InstrctGPT, Galactica, and LLaMA with various arithmetic expressions and provide a detailed analysis of the ability of large language models. MATH 401 and evaluation codes are released at \url{https://github.com/GanjinZero/math401-llm}.
    
[^26]: 双关注神经变换器用于语音识别时的高效唤醒词识别

    Dual-Attention Neural Transducers for Efficient Wake Word Spotting in Speech Recognition. (arXiv:2304.01905v1 [cs.SD])

    [http://arxiv.org/abs/2304.01905](http://arxiv.org/abs/2304.01905)

    本文介绍了一种新的“双关注神经变换器”，可以通过优化唤醒词检测来选择计算路径，从而提高唤醒词的准确性和推理时间效率，并且计算成本可以降低90％而仅增加1％的参数。这种架构可以在语音识别领域中大有裨益。

    

    本文提出了一种称为双关注神经网络的架构，旨在提高唤醒词识别的准确率并改善语音识别任务的推理时间。该架构通过利用唤醒词检测来选择哪个注意力网络执行输入音频帧的运行时计算路径。使用这种方法，作者有效提高了唤醒词识别的准确性，并定义了浮点运算（FLOPs）的运行时计算成本。在使用作者的内部数据集时，作者证明了所提出的双关注网络可以将唤醒词音频帧的计算成本降低$90\%$，而参数数量仅增加$1\%$。与基线相比，该架构提高了唤醒词F1得分$16\%$，并将一般的罕见词错误率提高了$3\%$。

    We present dual-attention neural biasing, an architecture designed to boost Wake Words (WW) recognition and improve inference time latency on speech recognition tasks. This architecture enables a dynamic switch for its runtime compute paths by exploiting WW spotting to select which branch of its attention networks to execute for an input audio frame. With this approach, we effectively improve WW spotting accuracy while saving runtime compute cost as defined by floating point operations (FLOPs). Using an in-house de-identified dataset, we demonstrate that the proposed dual-attention network can reduce the compute cost by $90\%$ for WW audio frames, with only $1\%$ increase in the number of parameters. This architecture improves WW F1 score by $16\%$ relative and improves generic rare word error rate by $3\%$ relative compared to the baselines.
    
[^27]: 社会文化知识在仇恨言论检测任务中对选项的选择是必要的

    Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])

    [http://arxiv.org/abs/2304.01890](http://arxiv.org/abs/2304.01890)

    HATELEXICON是一个包含巴西，德国，印度和肯尼亚仇恨言论的词汇表，利用其可以提高模型在训练中的性能表现。

    

    我们引入了HATELEXICON，这是一个包含巴西，德国，印度和肯尼亚的蔑称和仇恨言论目标的词汇表，以帮助模型的训练和可解释性。我们展示了我们的词汇表如何用于解释模型预测，表明发展用于分类极端言论的模型，在进行预测时严重依赖目标词。此外，我们提出了一种通过HATELEXICON来辅助低资源环境下训练选项的方法，选项选择在小样本学习中尤为重要。在我们的工作中，我们使用HASOC数据对德语和印地语进行了几个示范学习，并将Multilingual HateCheck（MHC）作为基准。我们展示了根据我们的词汇表选择样本，相对于随机采样的模型，能够更好地在MHC上表现。因此，当仅有少量的训练样本时，使用我们的词汇表来选择包含更多社会文化信息的样本能够更好地提高在仇恨言论检测任务中的性能。

    We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
    
[^28]: 统一对比传递框架与传播结构用于提高低资源谣言检测

    A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection. (arXiv:2304.01492v1 [cs.CL])

    [http://arxiv.org/abs/2304.01492](http://arxiv.org/abs/2304.01492)

    该文介绍了一个利用对比传递框架和传播结构，将从充足资源的谣言数据学到的特征适应于低资源情况下的方式，可以检测到跨越语言和领域界限的谣言。

    

    大量的谣言伴随着突发新闻或热门话题而传播，这严重阻碍了真相的传播。现有的谣言检测算法展示了在前几天新闻上良好性能的前景，但是由于缺乏训练数据和先前的专业知识，它们很难发现与预期事件有关的谣言，特别是在不同语言（即低资源环境）中传播的谣言。在本文中，我们提出了一个统一的对比传递框架，通过将从充足资源的谣言数据学到的特征适应于低资源情况下的特征来检测谣言。具体来说，我们首先将在社交媒体上传播的谣言表示为无向拓扑结构，然后通过统一对比范式进行Multi-scale图卷积网络的训练。我们的模型明确地突破了领域和/或语言问题的障碍，通过语言对齐和一种新颖的领域自适应对比。

    The truth is significantly hampered by massive rumors that spread along with breaking news or popular topics. Since there is sufficient corpus gathered from the same domain for model training, existing rumor detection algorithms show promising performance on yesterday's news. However, due to a lack of training data and prior expert knowledge, they are poor at spotting rumors concerning unforeseen events, especially those propagated in different languages (i.e., low-resource regimes). In this paper, we propose a unified contrastive transfer framework to detect rumors by adapting the features learned from well-resourced rumor data to that of the low-resourced. More specifically, we first represent rumor circulated on social media as an undirected topology, and then train a Multi-scale Graph Convolutional Network via a unified contrastive paradigm. Our model explicitly breaks the barriers of the domain and/or language issues, via language alignment and a novel domain-adaptive contrastive 
    
[^29]: 聊天GPT，还是不聊天GPT：这是一个问题！

    To ChatGPT, or not to ChatGPT: That is the question!. (arXiv:2304.01487v1 [cs.LG])

    [http://arxiv.org/abs/2304.01487](http://arxiv.org/abs/2304.01487)

    研究评估了聊天GPT检测中的最新技术和其他AI生成文本检测工具的表现，并提出区分人工生成和AI生成文本的重要性。

    

    聊天GPT已经成为一种全球感知。随着聊天GPT和其他大型语言模型（LLM）的出现，对于他们的误用的担忧也增加了，例如传播虚假消息，抄袭，操纵公众舆论，欺骗和欺诈。因此，区分人工生成和AI生成的文本变得越来越重要。研究人员提出了各种检测方法，从基本的二元分类器到更复杂的深度学习模型。一些检测技术依赖于统计特征或句法模式，而其他一些则包含语义或上下文信息以提高准确性。本研究的主要目标是对聊天GPT检测中最新技术进行全面和现代化的评估。此外，我们还评估了其他未专门声称检测聊天GPT生成内容的AI生成文本检测工具以评估它们在检测聊天GPT生成内容方面的表现。在我们的评估中，我们使用了一个包含人工编写和聊天GPT生成的文本的大型数据集。

    ChatGPT has become a global sensation. As ChatGPT and other Large Language Models (LLMs) emerge, concerns of misusing them in various ways increase, such as disseminating fake news, plagiarism, manipulating public opinion, cheating, and fraud. Hence, distinguishing AI-generated from human-generated becomes increasingly essential. Researchers have proposed various detection methodologies, ranging from basic binary classifiers to more complex deep-learning models. Some detection techniques rely on statistical characteristics or syntactic patterns, while others incorporate semantic or contextual information to improve accuracy. The primary objective of this study is to provide a comprehensive and contemporary assessment of the most recent techniques in ChatGPT detection. Additionally, we evaluated other AI-generated text detection tools that do not specifically claim to detect ChatGPT-generated content to assess their performance in detecting ChatGPT-generated content. For our evaluation,
    
[^30]: StatCan对话数据集：通过真实意图的对话检索数据表

    The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents. (arXiv:2304.01412v1 [cs.CL])

    [http://arxiv.org/abs/2304.01412](http://arxiv.org/abs/2304.01412)

    该论文介绍了StatCan对话数据集，这是一个涉及真实意图的对话转换，提出了两个任务：基于正在进行的对话自动检索相关数据表和在每个回合自动生成适当的代理响应。该研究对现有模型持续提出挑战，鼓励更多基于对话的数据检索研究。

    

    我们介绍了StatCan对话数据集，其中包含了19379次代表Statistics Canada的代理和在线用户之间的对话转换，涉及真实意图，使用英语或法语进行，代理会检索到5000多个复杂数据表之一。基于这个数据集，我们提出了两个任务：（1）基于正在进行的对话自动检索相关数据表，（2）在每个回合自动生成适当的代理响应。我们通过建立强基线来研究每个任务的难度。在时间数据分割的实验中，我们发现所有模型都难以推广到未来的对话中，当我们从验证集移动到测试集时，我们观察到两个任务的性能都显著下降。此外，我们发现响应生成模型在何时返回表格方面存在困难。考虑到这些任务对现有模型的挑战，我们鼓励进一步研究基于对话的数据检索。

    We introduce the StatCan Dialogue Dataset consisting of 19,379 conversation turns between agents working at Statistics Canada and online users looking for published data tables. The conversations stem from genuine intents, are held in English or French, and lead to agents retrieving one of over 5000 complex data tables. Based on this dataset, we propose two tasks: (1) automatic retrieval of relevant tables based on a on-going conversation, and (2) automatic generation of appropriate agent responses at each turn. We investigate the difficulty of each task by establishing strong baselines. Our experiments on a temporal data split reveal that all models struggle to generalize to future conversations, as we observe a significant drop in performance across both tasks when we move from the validation to the test set. In addition, we find that response generation models struggle to decide when to return a table. Considering that the tasks pose significant challenges to existing models, we enc
    
[^31]: 一种简单有效的跨语言抄袭检测方法

    A Simple and Effective Method of Cross-Lingual Plagiarism Detection. (arXiv:2304.01352v1 [cs.CL])

    [http://arxiv.org/abs/2304.01352](http://arxiv.org/abs/2304.01352)

    该论文提出了一种简单有效的跨语言抄袭检测方法，不依赖机器翻译和词义消歧，使用开放的多语言同义词库进行候选检索任务和预训练的基于多语言BERT的语言模型进行详细分析，在多个基准测试中取得了最先进的结果。

    

    我们提出了一种简单的跨语言抄袭检测方法，适用于大量的语言。该方法利用开放的多语言同义词库进行候选检索任务，并利用预训练的基于多语言BERT的语言模型进行详细分析。该方法在使用时不依赖机器翻译和词义消歧，因此适用于许多语言，包括资源匮乏的语言。该方法在多个现有和新的基准测试中展示了其有效性，在法语、俄语和亚美尼亚语等语言中取得了最先进的结果。

    We present a simple cross-lingual plagiarism detection method applicable to a large number of languages. The presented approach leverages open multilingual thesauri for candidate retrieval task and pre-trained multilingual BERT-based language models for detailed analysis. The method does not rely on machine translation and word sense disambiguation when in use, and therefore is suitable for a large number of languages, including under-resourced languages. The effectiveness of the proposed approach is demonstrated for several existing and new benchmarks, achieving state-of-the-art results for French, Russian, and Armenian languages.
    
[^32]: 用自然语言处理的方法识别心理健康记录中的疼痛提及

    Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach. (arXiv:2304.01240v1 [cs.CL])

    [http://arxiv.org/abs/2304.01240](http://arxiv.org/abs/2304.01240)

    该研究使用机器学习技术，成功地识别出心理健康电子健康记录中的疼痛提及，提高了对疼痛和心理健康之间关系的理解。

    

    疼痛是访问医疗资源的常见原因，并且是一个研究领域，特别是在与心理健康的重叠方面。心理健康电子健康记录是研究此重叠的良好数据来源。然而，疼痛的大量信息保存在这些记录的自由文本中，由于其歧义性，疼痛的提及呈现出独特的自然语言处理问题。本项目使用匿名的心理健康电子健康记录数据库中的数据。利用这些数据训练基于机器学习的分类算法，将句子分类为讨论患者疼痛或不讨论。这将有助于从大型数据库中提取相关疼痛信息，并将这些输出用于进一步研究疼痛和心理健康。共手动三重注释了1,985份文件，以创建黄金标准训练数据，并用于训练三种常用的分类算法。最佳模型的F1分数为0.787。结果证明了使用自然语言处理识别心理健康电子健康记录中的疼痛提及的可行性，这可以改善对疼痛和心理健康之间关系的理解。

    Pain is a common reason for accessing healthcare resources and is a growing area of research, especially in its overlap with mental health. Mental health electronic health records are a good data source to study this overlap. However, much information on pain is held in the free text of these records, where mentions of pain present a unique natural language processing problem due to its ambiguous nature. This project uses data from an anonymised mental health electronic health records database. The data are used to train a machine learning based classification algorithm to classify sentences as discussing patient pain or not. This will facilitate the extraction of relevant pain information from large databases, and the use of such outputs for further studies on pain and mental health. 1,985 documents were manually triple-annotated for creation of gold standard training data, which was used to train three commonly used classification algorithms. The best performing model achieved an F1-
    
[^33]: Spam-T5：基于小样本的邮件垃圾检测的大型语言模型基准测试

    Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection. (arXiv:2304.01238v1 [cs.CL])

    [http://arxiv.org/abs/2304.01238](http://arxiv.org/abs/2304.01238)

    本文通过比较不同类型的大型语言模型和传统机器学习技术在邮件垃圾检测中的表现，发现大多数情况下，大型语言模型优于传统技术，特别是在样本有限的情况下。同时，本文还介绍了经过改进和微调的Spam-T5模型，该模型具有出色的性能表现。

    

    本文通过比较三种不同类型的大型语言模型（BERT-like、Sentence Transformers和Seq2Seq）以及传统机器学习技术（如朴素贝叶斯和LightGBM）在邮件垃圾检测中的有效性，研究了大型语言模型在邮件垃圾检测中的作用。同时，我们还评估了这些模型在四个公共数据集上的表现，并使用不同数量的训练样本（完整训练集和小样本）进行了测试。 发现在大多数情况下，LLMs优于基线技术，特别是在小样本情况下。这种适应性使LLMs在邮件垃圾检测任务中具有独特的优势，因为标记样本数量有限，并且模型需要经常更新。此外，我们介绍了Spam-T5模型，该模型是专门为检测电子邮件垃圾而进行了改进和微调。我们的结果表明，Spam-T5模型具有出色的性能。

    This paper investigates the effectiveness of large language models (LLMs) in email spam detection by comparing prominent models from three distinct families: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we examine well-established machine learning techniques for spam detection, such as Na\"ive Bayes and LightGBM, as baseline methods. We assess the performance of these models across four public datasets, utilizing different numbers of training samples (full training set and few-shot settings). Our findings reveal that, in the majority of cases, LLMs surpass the performance of the popular baseline techniques, particularly in few-shot scenarios. This adaptability renders LLMs uniquely suited to spam detection tasks, where labeled samples are limited in number and models require frequent updates. Additionally, we introduce Spam-T5, a Flan-T5 model that has been specifically adapted and fine-tuned for the purpose of detecting email spam. Our results demonstrate that Spam-T5 
    
[^34]: AUDIT：基于潜在扩散模型的指令引导音频编辑

    AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models. (arXiv:2304.00830v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2304.00830](http://arxiv.org/abs/2304.00830)

    研究提出了基于潜在扩散模型的指令引导音频编辑模型AUDIT，实现了在各种音频编辑任务上的最先进性能，并通过自适应方案和文本到片段匹配模块解决了先前扩散-based方法存在的问题。

    

    音频编辑可用于各种目的，例如添加背景音效、替换乐器和修复损坏的音频。最近，一些基于扩散的方法通过使用以输出音频的文本说明为条件的扩散和去噪过程来实现零-shot音频编辑。然而，这些方法仍存在一些问题：1）它们并没有被训练用于编辑任务，不能保证良好的编辑效果；2）它们可能会错误地修改不需要编辑的音频片段；3）他们需要输出音频的完整描述，这在实际情况下并不总是可用或必要。在这项工作中，我们提出了AUDIT，一种基于潜在扩散模型的指令引导音频编辑模型。特别是，AUDIT具有三个主要设计特点：1）我们为不同的音频编辑任务构建三元训练数据（指令，输入音频，输出音频）并使用指令和输入（要编辑的音频）音频对训练扩散模型；2）我们引入了一种新的自适应方案，使用很少量的编辑任务音频数据对扩散模型进行微调；3）我们提出了一个文本到片段匹配模块，自动将输入指令与要编辑的相应音频片段对齐。实验结果表明，AUDIT在各种音频编辑任务上实现了最先进的性能，并且比以前的基于扩散的方法表现更优秀。

    Audio editing is applicable for various purposes, such as adding background sound effects, replacing a musical instrument, and repairing damaged audio. Recently, some diffusion-based methods achieved zero-shot audio editing by using a diffusion and denoising process conditioned on the text description of the output audio. However, these methods still have some problems: 1) they have not been trained on editing tasks and cannot ensure good editing effects; 2) they can erroneously modify audio segments that do not require editing; 3) they need a complete description of the output audio, which is not always available or necessary in practical scenarios. In this work, we propose AUDIT, an instruction-guided audio editing model based on latent diffusion models. Specifically, AUDIT has three main design features: 1) we construct triplet training data (instruction, input audio, output audio) for different audio editing tasks and train a diffusion model using instruction and input (to be edite
    
[^35]: 在日本医疗执照考试中评估GPT-4和ChatGPT

    Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations. (arXiv:2303.18027v1 [cs.CL])

    [http://arxiv.org/abs/2303.18027](http://arxiv.org/abs/2303.18027)

    本文评估了GPT-4、ChatGPT和GPT-3在日本医疗执照考试中的表现，结果发现GPT-4优于其他两者，呈现出LLMs在与英语远离的语言中的潜力，但目前的LLM API存在一些限制，例如建议实施禁止的医疗选择。

    

    随着大型语言模型（LLMs）在不同语言的使用者中越来越受欢迎，我们认为对它们进行基准测试以更好地理解其在英语以外的语言中的行为、失误和限制至关重要。在本文中，我们评估了LLM API（ChatGPT、GPT-3和GPT-4）在过去5年的日本国家医疗执照考试中的表现。我们的团队包括以日语为母语的NLP研究人员和在日本工作的一名实践心脏病医师。我们的实验表明，GPT-4表现优于ChatGPT和GPT-3，并通过了所有五年的考试，突显LLMs在与英语远离的语言中的潜力。然而，我们的评估还暴露了当前LLM API的一些限制。首先，LLMs有时会选择在日本医疗实践中应该严格避免的禁止选择，例如建议实施安乐死。此外，我们的分析显示，API成本普遍较高，最大上下文大小较小。

    As large language models (LLMs) gain popularity among speakers of diverse languages, we believe that it is crucial to benchmark them to better understand model behaviors, failures, and limitations in languages beyond English. In this work, we evaluate LLM APIs (ChatGPT, GPT-3, and GPT-4) on the Japanese national medical licensing examinations from the past five years. Our team comprises native Japanese-speaking NLP researchers and a practicing cardiologist based in Japan. Our experiments show that GPT-4 outperforms ChatGPT and GPT-3 and passes all five years of the exams, highlighting LLMs' potential in a language that is typologically distant from English. However, our evaluation also exposes critical limitations of the current LLM APIs. First, LLMs sometimes select prohibited choices that should be strictly avoided in medical practice in Japan, such as suggesting euthanasia. Further, our analysis shows that the API costs are generally higher and the maximum context size is smaller fo
    
[^36]: 2D扩散算法的去偏置方法用于文本到3D生成

    Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation. (arXiv:2303.15413v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.15413](http://arxiv.org/abs/2303.15413)

    本文提出了两种去偏置的方法，一种通过增加2D扩散模型得出的分数的截断值，一种通过调整视角提示和物体空间摄像机姿态之间的差异。实验结果表明这些方法可以显著减少伪影，提高真实感。

    

    本文探讨了在文本到3D生成中出现的视角一致性问题，也称为Janus问题。这个问题来自于2D扩散模型的固有偏置，导致生成的3D对象不真实。通过对其进行研究，我们提出了两种方法来去除偏置以实现文本到3D生成的鲁棒性。第一种方法叫做score debiasing，通过逐渐增加2D扩散模型得出的分数的截断值，来达到去除偏置的效果。第二种方法叫做prompt debiasing，利用语言模型确定用户提示和视角提示之间的矛盾词语，并调整视角提示和物体空间摄像机姿态之间的差异。我们的实验结果表明，我们的方法通过显著减少伪影，提高了真实感，并在质量与速度方面取得了良好的平衡。

    The view inconsistency problem in score-distilling text-to-3D generation, also known as the Janus problem, arises from the intrinsic bias of 2D diffusion models, which leads to the unrealistic generation of 3D objects. In this work, we explore score-distilling text-to-3D generation and identify the main causes of the Janus problem. Based on these findings, we propose two approaches to debias the score-distillation frameworks for robust text-to-3D generation. Our first approach, called score debiasing, involves gradually increasing the truncation value for the score estimated by 2D diffusion models throughout the optimization process. Our second approach, called prompt debiasing, identifies conflicting words between user prompts and view prompts utilizing a language model and adjusts the discrepancy between view prompts and object-space camera poses. Our experimental results show that our methods improve realism by significantly reducing artifacts and achieve a good trade-off between fa
    
[^37]: 超越通用Transformer：自适应模块在ASR中的Transformer模型中的模块重用

    Beyond Universal Transformer: block reusing with adaptor in Transformer for automatic speech recognit. (arXiv:2303.13072v1 [cs.SD])

    [http://arxiv.org/abs/2303.13072](http://arxiv.org/abs/2303.13072)

    本论文提出一种用于ASR中的Transformer模型的块重用策略并配以适配器模块，使得模型更加紧凑，可适应性更强，准确性更高。

    

    基于Transformer的模型在端到端的自动语音识别（ASR）应用中取得了重要进展。 Transformer模型使得能够在智能设备上部署端到端的ASR系统。但是，这些模型仍有一个缺点，即需要大量的模型参数。为了克服通用Transformer模型在边缘设备上应用ASR的缺点，我们提出了一种解决方案，可以重用Transformer模型中的块作为小尺寸ASR系统使用的设计策略，既满足资源限制的目标，又不会影响识别准确率。具体来说，我们设计了一种新的用于语音Transformer（BRST）的块重用策略来增强参数的有效性，并提出了一个适配器模块（ADM），可以生成一个只需要少量可训练参数的紧凑和可适应的模型来确保每个重用块的陪伴。我们在Aishell-1和CommonVoice检验数据集上进行了实验，并证明了BRST对准确性、可扩展性等方面的优越性。

    Transformer-based models have recently made significant achievements in the application of end-to-end (E2E) automatic speech recognition (ASR). It is possible to deploy the E2E ASR system on smart devices with the help of Transformer-based models. While these models still have the disadvantage of requiring a large number of model parameters. To overcome the drawback of universal Transformer models for the application of ASR on edge devices, we propose a solution that can reuse the block in Transformer models for the occasion of the small footprint ASR system, which meets the objective of accommodating resource limitations without compromising recognition accuracy. Specifically, we design a novel block-reusing strategy for speech Transformer (BRST) to enhance the effectiveness of parameters and propose an adapter module (ADM) that can produce a compact and adaptable model with only a few additional trainable parameters accompanying each reusing block. We conducted an experiment with the
    
[^38]: 应用SMILES序列的Transformer模型在学习手性时存在困难

    Difficulty in learning chirality for Transformer fed with SMILES. (arXiv:2303.11593v1 [cs.LG])

    [http://arxiv.org/abs/2303.11593](http://arxiv.org/abs/2303.11593)

    应用SMILES序列的Transformer模型在学习分子结构的整体性和手性方面存在困难，需要进行长时间的训练。生成的描述符用于分子性质预测时的准确率从开始到训练结束都是相似的。

    

    近年来，基于对极其多样的分子进行表示学习的描述符生成已经得到了发展，特别是那些将自然语言处理（NLP）模型应用于SMILES，即分子结构的文字表示的模型。然而，关于这些模型如何理解化学结构的研究很少。为了解决这个问题，我们调查了一种代表性的NLP模型——Transformer，在学习SMILES和化学结构之间的关系。结果表明，虽然Transformer快速学习分子的部分结构，但需要进行长时间的训练才能理解整体结构。与之一致的是，在不同的学习步骤中生成的描述符用于分子性质预测时的准确率从开始到训练结束都是相似的。此外，我们发现Transformer需要特别长的训练时间才能学习手性，并且有时会出现低翻译准确率的停滞现象。

    Recent years have seen development of descriptor generation based on representation learning of extremely diverse molecules, especially those that apply natural language processing (NLP) models to SMILES, a literal representation of molecular structure. However, little research has been done on how these models understand chemical structure. To address this, we investigated the relationship between the learning progress of SMILES and chemical structure using a representative NLP model, the Transformer. The results suggest that while the Transformer learns partial structures of molecules quickly, it requires extended training to understand overall structures. Consistently, the accuracy of molecular property predictions using descriptors generated from models at different learning steps was similar from the beginning to the end of training. Furthermore, we found that the Transformer requires particularly long training to learn chirality and sometimes stagnates with low translation accura
    
[^39]: KHAN：基于知识的层次化注意力网络用于准确的政治立场预测

    KHAN: Knowledge-Aware Hierarchical Attention Networks for Accurate Political Stance Prediction. (arXiv:2302.12126v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.12126](http://arxiv.org/abs/2302.12126)

    本文提出了一种新颖的知识感知政治立场预测方法，采用层次化注意力网络、外部知识库和知识感知损失函数，可以有效地捕捉新闻文章中的关键信息，优于现有方法。

    

    新闻文章的政治立场预测已被广泛研究，以减轻回声室效应，即人们落入其思想，强化其现有信念。以往关于政治立场问题的研究重点在于（1）识别可以反映新闻文章政治立场的政治因素和（2）有效地捕捉这些因素。尽管它们在经验上成功了，但在政治立场预测中其识别的因素的有效性没有得到充分证明。在此基础上，本文通过用户研究调查政治立场预测中的重要因素，并观察到新闻文章的环境和语调（隐含）以及文章中涉及的现实实体的外部知识（显式）在确定其政治立场方面是重要的。基于这一观察结果，我们提出了一种新颖的知识感知政治立场预测方法（KHAN），采用（1）层次化注意力网络有效地捕捉新闻文章中的关键信息，（2）外部知识库提供关于文章中提到的实体的额外信息，以及（3）知识感知损失函数学习优先考虑重要信息。多个数据集上的实验结果表明，我们的方法显著优于现有方法。

    The political stance prediction for news articles has been widely studied to mitigate the echo chamber effect -- people fall into their thoughts and reinforce their pre-existing beliefs. The previous works for the political stance problem focus on (1) identifying political factors that could reflect the political stance of a news article and (2) capturing those factors effectively. Despite their empirical successes, they are not sufficiently justified in terms of how effective their identified factors are in the political stance prediction. Motivated by this, in this work, we conduct a user study to investigate important factors in political stance prediction, and observe that the context and tone of a news article (implicit) and external knowledge for real-world entities appearing in the article (explicit) are important in determining its political stance. Based on this observation, we propose a novel knowledge-aware approach to political stance prediction (KHAN), employing (1) hierar
    
[^40]: InstructABSA: 基于指令学习的方面情感分析

    InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.08624](http://arxiv.org/abs/2302.08624)

    InstructABSA是一种使用指令学习范式的方面情感分析方法，能够显著提高Aspect Term Extraction、Aspect Term Sentiment Classification、和Joint Task subtasks三个子任务的性能，并且在多个数据集上表现超过之前的最先进方法。

    

    本文介绍了InstructABSA，一种使用指令学习范式进行Aspect Based Sentiment Analysis (ABSA) 所有子任务（Aspect Term Extraction (ATE)，Aspect Term Sentiment Classification (ATSC)，以及Joint Task modeling）的方法。我们的方法对每个训练样本引入了正面、负面、和中性的例子，并使用指令来调整每个ABSA子任务的模型（Tk-Instruct），从而显著提高了性能。在Sem Eval 2014、2015和2016数据集上的实验结果表明，在所有三个ABSA子任务（ATE、ATSC和Joint Task）上，InstructABSA在性能上都比之前的最先进方法（SOTA）表现出了显著的优势，并且表现超过了7倍大的模型。特别是，在Rest14 ATE子任务上，InstructABSA超过了SOTA 7.31%的得分，Rest15 ATSC子任务上也有提升，并且在Lapt14 Joint Task上的表现提升了8.63%点。我们的结果还表明，对于所有三个子任务，InstructABSA具有强大的新领域泛化能力。

    In this paper, we present InstructABSA, Aspect Based Sentiment Analysis (ABSA) using the instruction learning paradigm for all ABSA subtasks: Aspect Term Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint Task modeling. Our method introduces positive, negative, and neutral examples to each training sample, and instruction tunes the model (Tk-Instruct) for each ABSA subtask, yielding significant performance improvements. Experimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA outperforms the previous state-of-the-art (SOTA) approaches on all three ABSA subtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x larger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATE subtask by 7.31% points, Rest15 ATSC subtask by and on the Lapt14 Joint Task by 8.63% points. Our results also suggest a strong generalization ability to new domains across all three subtasks
    
[^41]: 利用拼接将句子位置编码引入上下文感知神经机器翻译

    Encoding Sentence Position in Context-Aware Neural Machine Translation with Concatenation. (arXiv:2302.06459v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.06459](http://arxiv.org/abs/2302.06459)

    本文探究了在上下文感知神经机器翻译中将句子位置信息引入模型的想法，比较了不同的编码方法，结果表明在使用context-discounted loss对英语到俄语翻译时有益，但在英语到德语翻译中无显著收益。

    

    使用标准Transformer架构处理连续句子拼接可以实现上下文感知翻译。本文研究了给模型提供关于拼接窗口中句子位置显式信息的直觉想法。我们比较了各种编码句子位置的方法，包括新方法。我们的结果表明，如果以context-discounted loss（Lupo et al.，2022）进行训练，则Transformer架构在英语到俄语翻译中受益于某些句子位置编码方法。然而，在英语到德语翻译中并没有观察到同样的收益。进一步的实证研究需要定义此方法有益的条件。

    Context-aware translation can be achieved by processing a concatenation of consecutive sentences with the standard Transformer architecture. This paper investigates the intuitive idea of providing the model with explicit information about the position of the sentences contained in the concatenation window. We compare various methods to encode sentence positions into token representations, including novel methods. Our results show that the Transformer benefits from certain sentence position encoding methods on English to Russian translation if trained with a context-discounted loss (Lupo et al., 2022). However, the same benefits are not observed in English to German. Further empirical efforts are necessary to define the conditions under which the proposed approach is beneficial.
    
[^42]: 视觉学习者遇见Web图像-文本对

    Vision Learners Meet Web Image-Text Pairs. (arXiv:2301.07088v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.07088](http://arxiv.org/abs/2301.07088)

    本论文提出了一种基于网络数据的新型视觉学习方法MUlti-modal Generator (MUG)。在视觉数据集的转移学习任务上取得了最先进的表现，是之前最佳结果的3.4%和2.2%的提升。

    

    大多数最新的自监督学习方法都是在维护良好的ImageNet-1K数据集上进行预训练的。在本研究中，考虑到网络数据的出色可伸缩性，我们认为自我监督预训练应该基于嘈杂的网络源图文配对数据。首先，我们在如此设置下，对大规模网络数据上的代表性自监督预训练方法进行了基准研究。我们比较了一系列方法，包括使用被屏蔽的训练目标的单模式方法和使用图像-文本对比训练的多模式方法。我们发现，现有的多模态方法在视觉转移学习任务上并不比单模态方法表现更好。我们提出了一个信息论视角来解释这些基准结果，这提供了如何设计新型视觉学习者的见解。受到这些见解的启发，我们提出了一种新的视觉表示预训练方法——多模式生成器（MUG），它从可伸缩的网络源图文数据中学习。MUG在几个视觉数据集的转移学习任务上取得了最先进的性能，在CIFAR-10上优于之前最佳的结果3.4％，在STL-10上优于之前最佳的结果2.2％。

    Most recent self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from scalable web sourced image-text data. MUG ach
    
[^43]: 基于编码器-解码器语言模型的配对抗体链序列条件生成

    Conditional Generation of Paired Antibody Chain Sequences through Encoder-Decoder Language Model. (arXiv:2301.02748v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2301.02748](http://arxiv.org/abs/2301.02748)

    本论文提出了pAbT5，这是一种能够采用编码器-解码器模型为蛋白质相互作用生成配对抗体链序列的语言模型。该模型可以准确地预测配对情况，并且能够在实验验证方面表现出最先进的无监督预测能力。

    

    蛋白质语言模型在序列、结构和功能预测方面具有很好的表现。然而，目前的蛋白质语言模型仅限于单一序列的编码器或解码器结构，而许多生物学环境涉及蛋白质相互作用。本文介绍了pAbT5，它采用基于T5的架构将抗体链配对建模为正向和反向翻译。我们展示了pAbT5通过序列生成准确地反映了链的配对。我们的蛋白质语言模型可以生成可变长度的序列，其下一个词语的预测概率与序列比对的位置特异性评分矩阵一致。像蛋白质语言模型中的其他研究一样，pAbT5在实验验证方面表现出最先进的无监督预测能力。据我们所知，pAbT5是第一个用于蛋白质相互作用的生成式编码器-解码器蛋白质语言模型。

    Protein language models (LMs) have been successful in sequence, structural and functional predictions. However, currently, protein LMs are limited to encoder- or decoder-only architectures for single sequences while many biological contexts involve protein-protein interactions. Here, we introduce pAbT5, which models antibody chain pairing as forward- and back-translations using a T5-based architecture. We show that pAbT5 accurately reflects chain pairing through sequence generation. Our protein LM generates variable-length sequences and its next-word prediction probability agrees with position-specific scoring matrix from sequence alignment. Like other works in protein LM, pAbT5 performs state-of-the-art unsupervised prediction on experimental measurements. To the best of our knowledge, pAbT5 is the first generative encoder-decoder protein LM for protein-protein interactions.
    
[^44]: 视觉Transformer是参数高效的音像学习者

    Vision Transformers are Parameter-Efficient Audio-Visual Learners. (arXiv:2212.07983v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.07983](http://arxiv.org/abs/2212.07983)

    本文研究了冻结ViTs在没有微调任何原始参数的情况下将其推广到音像数据的能力。通过使用一个名为LAVISH的适配器和少数的可训练参数，有效融合视觉和音频提示，并在使用较少可调参数和不依赖昂贵的音频预训练的情况下，在各种音像任务上取得了竞争性性能。

    

    过去几年中，视觉Transformer（ViTs）在各种计算机视觉任务上取得了令人瞩目的成果。本文研究了仅在视觉数据上进行预训练的冻结ViTs在没有微调任何原始参数的情况下，将其推广到音像数据的能力。为此，我们提出了一种名为latent audio-visual hybrid（LAVISH）的适配器，通过向每个冻结ViT层注入少量可训练参数来将预训练的ViT调适用于音像任务。为了有效地融合视觉和音频提示，我们的LAVISH适配器使用一小组潜在令牌，形成一个注意瓶颈，从而消除了标准交叉关注的二次成本。与现有的模态特定音像方法相比，我们的方法在使用较少的可调参数并且不依赖昂贵的音频预训练或外部音频编码器的情况下，能够在各种音像任务上取得竞争性甚至更好的性能。我们的代码可在https://genj找到。

    Vision transformers (ViTs) have achieved impressive results on various computer vision tasks in the last several years. In this work, we study the capability of frozen ViTs, pretrained only on visual data, to generalize to audio-visual data without finetuning any of its original parameters. To do so, we propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained ViTs to audio-visual tasks by injecting a small number of trainable parameters into every layer of a frozen ViT. To efficiently fuse visual and audio cues, our LAVISH adapter uses a small set of latent tokens, which form an attention bottleneck, thus, eliminating the quadratic cost of standard cross-attention. Compared to the existing modality-specific audio-visual methods, our approach achieves competitive or even better performance on various audio-visual tasks while using fewer tunable parameters and without relying on costly audio pretraining or external audio encoders. Our code is available at https://genj
    
[^45]: DreamArtist: 通过对比prompt-tuning实现可控的一次性文本到图像生成

    DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via Contrastive Prompt-Tuning. (arXiv:2211.11337v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.11337](http://arxiv.org/abs/2211.11337)

    DreamArtist采用正负prompt-tuning学习策略来生成可控的一次性文本到图像，并解决了传统方法可能会导致模型过度拟合的问题。

    

    大规模文本到图像生成模型通过文本指导合成高质量、特征丰富、高分辨率的图像取得了可观的进展。然而，这些模型在处理新概念（例如新风格、物体实体等）时常常面临困难。尽管最近的尝试采用微调或prompt-tuning策略来教授预先训练的扩散模型从参考图像集中学习新概念，但它们存在过度拟合给定的参考图像，特别是在单次应用中，这对于保持生成可控性并产生多样化、高质量的图像是有害的。为了解决这个挑战，我们提出了一种简单而有效的方法DreamArtist，它采用了正负prompt-tuning学习策略。具体而言，DreamArtist结合了正负嵌入并联合训练它们。正嵌入积极地捕捉参考图像的显着特征来驱动图像生成，而负嵌入则强制模型生成多样性图像以降低过度拟合风险。

    Large-scale text-to-image generation models have achieved remarkable progress in synthesizing high-quality, feature-rich images with high resolution guided by texts. However, these models often struggle with novel concepts, eg, new styles, object entities, etc. Although recent attempts have employed fine-tuning or prompt-tuning strategies to teach the pre-trained diffusion model novel concepts from a reference image set,they have the drawback of overfitting to the given reference images, particularly in one-shot applications, which is harmful to generate diverse and high-quality images while maintaining generation controllability.  To tackle this challenge, we present a simple yet effective method called DreamArtist, which employs a positive-negative prompt-tuning learning strategy. Specifically, DreamArtist incorporates both positive and negative embeddings and jointly trains them. The positive embedding aggressively captures the salient characteristics of the reference image to drive
    
[^46]: 利用自然语言处理将法律协议转换成智能合约

    Conversion of Legal Agreements into Smart Legal Contracts using NLP. (arXiv:2210.08954v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2210.08954](http://arxiv.org/abs/2210.08954)

    本文提出了一个利用自然语言处理的流程，可以自动化将法律协议转换成智能合约，并通过评估，发现NER流程和问答方法可以准确地从模板文本中识别出 CiceroMark 和提取 Concerto 变量。

    

    智能合约是一种包含自然语言和可计算组件的数字化协议。Accord Project 提供了一个开源的智能合约框架，包含三个主要模块：Cicero、Concerto和Ergo。目前，我们需要律师、程序员和客户共同努力才能使用 Accord Project 创建可用的智能合约。本文提出了一个流程，利用多个自然语言处理模型将法律合同转换为 Accord Project 的 Concerto 模型，以自动化智能合约的创建过程。在评估了提出的流程后，我们发现我们的 NER 流程可以正确识别 Accord Project 模板文本中的 CiceroMark，准确率达到0.8。此外，我们的问答方法可以从模板文本中提取出三分之一的 Concerto 变量。我们还深入探讨了提出的流程的一些局限性和可能的未来研究。最后，我们描述了一个 Web 接口，使用户可以构建智能合约。

    A Smart Legal Contract (SLC) is a specialized digital agreement comprising natural language and computable components. The Accord Project provides an open-source SLC framework containing three main modules: Cicero, Concerto, and Ergo. Currently, we need lawyers, programmers, and clients to work together with great effort to create a usable SLC using the Accord Project. This paper proposes a pipeline to automate the SLC creation process with several Natural Language Processing (NLP) models to convert law contracts to the Accord Project's Concerto model. After evaluating the proposed pipeline, we discovered that our NER pipeline accurately detects CiceroMark from Accord Project template text with an accuracy of 0.8. Additionally, our Question Answering method can extract one-third of the Concerto variables from the template text. We also delve into some limitations and possible future research for the proposed pipeline. Finally, we describe a web interface enabling users to build SLCs. T
    
[^47]: 建模段落级别的视觉-语言语义对齐用于多模态摘要生成

    Modeling Paragraph-Level Vision-Language Semantic Alignment for Multi-Modal Summarization. (arXiv:2208.11303v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.11303](http://arxiv.org/abs/2208.11303)

    本文提出了ViL-Sum，用于建模段落级别的视觉-语言语义对齐和多模态摘要生成。实验结果表明，ViL-Sum在两个基准数据集上优于现有方法，表明该方法的有效性和优越性。

    

    目前大多数多模态摘要方法采用级联方式：首先使用一个现成的目标检测器提取视觉特征, 然后将这些特征与语言表示相融合，使用编码器-解码器模型生成摘要。级联的方式无法捕捉图像和段落之间的语义对齐，这对于准确的摘要至关重要。本文提出了ViL-Sum，用于联合建模段落级别的视觉-语言语义对齐和多模态摘要生成。ViL-Sum的核心是一个联合多模态编码器，具有两个精心设计的任务：图像重排序和图像选择。联合多模态编码器捕捉了模态之间的交互作用，其中重排序任务引导模型学习段落级别的语义对齐，而选择任务引导模型在最终生成的摘要中选择与摘要相关的图像。实验结果表明，我们提出的ViL-Sum在两个基准数据集上显著优于现有的最先进方法，说明了我们的多模态摘要生成方法的有效性和优越性。

    Most current multi-modal summarization methods follow a cascaded manner, where an off-the-shelf object detector is first used to extract visual features, then these features are fused with language representations to generate the summary with an encoder-decoder model. The cascaded way cannot capture the semantic alignments between images and paragraphs, which are crucial to a precise summary. In this paper, we propose ViL-Sum to jointly model paragraph-level \textbf{Vi}sion-\textbf{L}anguage Semantic Alignment and Multi-Modal \textbf{Sum}marization. The core of ViL-Sum is a joint multi-modal encoder with two well-designed tasks, image reordering and image selection. The joint multi-modal encoder captures the interactions between modalities, where the reordering task guides the model to learn paragraph-level semantic alignment and the selection task guides the model to selected summary-related images in the final summary. Experimental results show that our proposed ViL-Sum significantly
    
[^48]: 词长的最优性：理论基础和实证研究

    The optimality of word lengths. Theoretical foundations and an empirical study. (arXiv:2208.10384v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.10384](http://arxiv.org/abs/2208.10384)

    本文提出了两个正则化的最优性分数，它们是对最小和随机基线都进行了归一化处理的。研究表明，语言在词长方面高度优化，而Zipf的缩写定律确实是压缩的一种表现。

    

    Zipf的缩写定律表明，更常见的词更短，这被视为压缩的一种表现——自然交流的普遍原则之一是形式长度的最小化。虽然语言优化的说法变得流行起来，但衡量语言优化程度的尝试却相当稀少。本文提出了两个正则化的最优性分数，它们是对最小和随机基线都进行了归一化处理的。我们分析了这些分数以及其他分数的理论和统计优缺点。利用最佳分数，我们首次量化了语言中词长的最优性程度。这表明当以字符计算词长时，语言平均优化到62或67％（取决于数据来源），当以时间计算词长时，平均为65％。总的来说，口语单词持续时间比书面词长更优化。我们的研究表明，语言在词长方面高度优化，而Zipf的缩写定律确实是压缩的一种表现。

    Zipf's law of abbreviation, namely the tendency of more frequent words to be shorter, has been viewed as a manifestation of compression, i.e. the minimization of the length of forms -- a universal principle of natural communication. Although the claim that languages are optimized has become trendy, attempts to measure the degree of optimization of languages have been rather scarce. Here we present two optimality scores that are dualy normalized, namely, they are normalized with respect to both the minimum and the random baseline. We analyze the theoretical and statistical pros and cons of these and other scores. Harnessing the best score, we quantify for the first time the degree of optimality of word lengths in languages. This indicates that languages are optimized to 62 or 67 percent on average (depending on the source) when word lengths are measured in characters, and to 65 percent on average when word lengths are measured in time. In general, spoken word durations are more optimize
    
[^49]: 电影叙述摘要：一个用于故事理解的视频语言数据集

    Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.05711](http://arxiv.org/abs/2203.05711)

    这个研究收集并发布了一个视频-语言故事数据集SYMON，用于推进多模态故事理解的发展。

    

    尽管AI有了最近的进展，但故事理解仍然是一个未被充分研究的问题。我们收集、预处理并公开发布了一个视频语言故事数据集SYMON，其中包含5,193个流行电影和电视剧的视频摘要。SYMON捕捉了由人类创作者制作的面向人类观众的自然故事叙述视频。作为一个原型和自然故事数据集，SYMON具有高覆盖的多模态故事事件、丰富的心理状态描述和视觉和文本模态之间的大语义差距。我们建立了视频文本检索和电影摘要视频的零样本对齐的基准，展示了在故事理解中领域内数据的重要性。通过SYMON，我们希望为多模态故事理解的进展打下基础。

    Despite recent advances of AI, story understanding remains an open and under-investigated problem. We collect, preprocess, and publicly release a video-language story dataset, Synopses of Movie Narratives (SYMON), containing 5,193 video summaries of popular movies and TV series. SYMON captures naturalistic story-telling videos for human audience made by human creators. As a prototypical and naturalistic story dataset, SYMON features high coverage of multimodal story events, abundant mental-state descriptions, and large semantic gaps between the visual and the textual modalities. We establish benchmarks on video-text retrieval and zero-shot alignment on movie summary videos, which showcase the importance of in-domain data in story understanding. With SYMON, we hope to lay the groundwork for progress in multimodal story understanding.
    
[^50]: 从手语到口语的机器翻译：现状与挑战

    Machine Translation from Signed to Spoken Languages: State of the Art and Challenges. (arXiv:2202.03086v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.03086](http://arxiv.org/abs/2202.03086)

    手语到口语自动翻译是一个跨学科的研究领域，它的研究现状得到了总结，但手语翻译存在独特的挑战需要进一步关注。

    

    从手语到口语的自动翻译是一个跨学科的研究领域，涉及计算机视觉、机器翻译和语言学。尽管如此，在这个领域的研究大多由计算机科学家单独进行。随着这个领域越来越受欢迎 - 过去三年中关于手语翻译的大部分科学论文已经发表 - 我们提供了现状概述以及不同相关学科的一些必要背景知识。我们对手语语言学和机器翻译进行了高层次的介绍，以说明自动手语翻译的要求。我们进行了系统的文献综述，以说明领域的现状，然后基于这些要求，提出了未来研究面临的几个挑战。我们发现，在口语机器翻译的基础上取得了重大进展，但手语翻译存在独特的挑战需要进一步研究关注。

    Automatic translation from signed to spoken languages is an interdisciplinary research domain, lying on the intersection of computer vision, machine translation and linguistics. Nevertheless, research in this domain is performed mostly by computer scientists in isolation. As the domain is becoming increasingly popular - the majority of scientific papers on the topic of sign language translation have been published in the past three years - we provide an overview of the state of the art as well as some required background in the different related disciplines. We give a high-level introduction to sign language linguistics and machine translation to illustrate the requirements of automatic sign language translation. We present a systematic literature review to illustrate the state of the art in the domain and then, harking back to the requirements, lay out several challenges for future research. We find that significant advances have been made on the shoulders of spoken language machine t
    
[^51]: 无分割关注：BERT是否需要中间层？

    Undivided Attention: Are Intermediate Layers Necessary for BERT?. (arXiv:2012.11881v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2012.11881](http://arxiv.org/abs/2012.11881)

    本论文调查了中间层对于BERT在下游任务表现的作用，表明删除中间层数量可以减少模型参数和训练时间，同时对下游任务的影响较小，学习到的表示不受影响。

    

    最近，基于BERT的模型在解决各种自然语言处理（NLP）任务方面非常成功，例如阅读理解、自然语言推理、情感分析等。所有基于BERT的架构都具有一个自注意力块，后跟一个中间层块作为基本构建组件。然而，文献中缺乏对包含这些中间层的强有力的理由。在这项工作中，我们调查了中间层对下游任务的整体网络性能的重要性。我们表明，减少中间层数量并修改BERT-BASE的架构会导致下游任务的微小损失，同时减少模型的参数和训练时间。此外，我们使用中心化内核对齐和探测线性分类器来获得对我们的架构修改的洞见，并证明删除中间层对学习到的表示没有显着影响。

    In recent times, BERT-based models have been extremely successful in solving a variety of natural language processing (NLP) tasks such as reading comprehension, natural language inference, sentiment analysis, etc. All BERT-based architectures have a self-attention block followed by a block of intermediate layers as the basic building component. However, a strong justification for the inclusion of these intermediate layers remains missing in the literature. In this work we investigate the importance of intermediate layers on the overall network performance of downstream tasks. We show that reducing the number of intermediate layers and modifying the architecture for BERT-BASE results in minimal loss in fine-tuning accuracy for downstream tasks while decreasing the number of parameters and training time of the model. Additionally, we use centered kernel alignment and probing linear classifiers to gain insight into our architectural modifications and justify that removal of intermediate l
    
[^52]: CokeBERT: 增强预训练语言模型的上下文知识选择与嵌入

    CokeBERT: Contextual Knowledge Selection and Embedding towards Enhanced Pre-Trained Language Models. (arXiv:2009.13964v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2009.13964](http://arxiv.org/abs/2009.13964)

    本文提出了一种名为Coke的新框架，用于将上下文知识动态选择和嵌入到预训练语言模型中，以避免对输入文本匹配效果差的冗余和模糊知识的影响，并在知识驱动的自然语言处理任务上取得了优异表现。

    

    近期有许多工作致力于利用知识图谱中的外部异构知识来增强预训练语言模型（PLMs），并在各种知识驱动的自然语言处理任务上获得了一致的改进。然而，大多数这些知识增强的PLMs仅嵌入KG中的静态子图（“知识上下文”），而不考虑PLMs可能根据特定文本（“文本上下文”）动态变化所需的知识。因此，本文提出了一种新的框架，即Coke，用于为PLMs动态选择上下文知识并根据文本上下文嵌入知识上下文，从而可以避免KG中的冗余和模糊知识对输入文本的匹配效果不佳的影响。实验证明，Coke在典型的知识驱动的自然语言处理任务上优于各种基准模型，表明了利用动态知识上下文进行语言理解的有效性。除了性能方面的改进，所选择的动态知识能够提高语言模型的可解释性，以指导深入理解PLMs所学习的知识。

    Several recent efforts have been devoted to enhancing pre-trained language models (PLMs) by utilizing extra heterogeneous knowledge in knowledge graphs (KGs) and achieved consistent improvements on various knowledge-driven NLP tasks. However, most of these knowledge-enhanced PLMs embed static sub-graphs of KGs ("knowledge context"), regardless of that the knowledge required by PLMs may change dynamically according to specific text ("textual context"). In this paper, we propose a novel framework named Coke to dynamically select contextual knowledge and embed knowledge context according to textual context for PLMs, which can avoid the effect of redundant and ambiguous knowledge in KGs that cannot match the input text. Our experimental results show that Coke outperforms various baselines on typical knowledge-driven NLP tasks, indicating the effectiveness of utilizing dynamic knowledge context for language understanding. Besides the performance improvements, the dynamically selected knowle
    

