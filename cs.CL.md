# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs](https://arxiv.org/abs/2404.01461) | 该研究调查了代表性启发式对大型语言模型推理的影响，并创建了专门的数据集进行实验验证 |
| [^2] | [Editing Knowledge Representation of Language Lodel via Rephrased Prefix Prompts](https://arxiv.org/abs/2403.14381) | 引入了一种名为PSPEM的新方法，通过重新表述前缀提示来编辑语言Lodel的知识表示，解决了知识编辑方法中的低效性、通用性问题，以及提示工程的不透明性。 |
| [^3] | [Do Large Language Models Solve ARC Visual Analogies Like People Do?](https://arxiv.org/abs/2403.09734) | 该研究比较了人类和大型语言模型在ARC视觉类比问题上的表现，发现在特定任务上，人类和成年人的表现均优于大多数大型语言模型。对LLMs和年幼儿童错误分析揭示了类似的解决策略，同时指出了两种不同的错误类型，为我们理解LLMs如何解决视觉类比问题提供了新的启示。 |
| [^4] | [Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging](https://arxiv.org/abs/2403.08002) | 本文针对生物医学应用中前沿模型尚存在的多模态能力差距，探讨了训练开源小型多模态模型以弥补临床需求的生物医学能力差距。 |
| [^5] | [Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models](https://arxiv.org/abs/2402.12563) | 本文研究了大型语言模型的内在自我校正能力，并提出了一个“如果-否则”（IoE）提示框架，帮助模型评估自身“信心”并进行自我校正。 |
| [^6] | [OneBit: Towards Extremely Low-bit Large Language Models](https://arxiv.org/abs/2402.11295) | 本文提出了一种名为OneBit的1位量化感知训练框架，可以将大型语言模型的权重矩阵量化为1位，为极低比特宽度的LLMs部署铺平了道路。 |
| [^7] | [A systematic investigation of learnability from single child linguistic input](https://arxiv.org/abs/2402.07899) | 我们的研究探索了用单个儿童的语言输入训练语言模型的可学习性，我们发现这种设置下的语言模型能够形成句法和语义词群，并对某些语言现象具有敏感性。 |
| [^8] | [Mercury: An Efficiency Benchmark for LLM Code Synthesis](https://arxiv.org/abs/2402.07844) | Mercury提出了一个针对LLM代码综合任务的效率评估基准，通过引入新的度量标准Beyond@K来衡量归一化的代码效率，从而鼓励生成功能正确且计算效率高的代码。 |
| [^9] | [TexShape: Information Theoretic Sentence Embedding for Language Models](https://arxiv.org/abs/2402.05132) | 这项研究提出了一种名为TexShape的信息论句子嵌入模型，通过使用互信息的经验估计来优化文本表示，可用于数据压缩和敏感信息过滤，提升隐私和公平性。 |
| [^10] | [Are Sounds Sound for Phylogenetic Reconstruction?](https://arxiv.org/abs/2402.02807) | 本文通过对十个不同语言家族的多样数据集进行研究，首次在系统发育重建中比较了基于声音和基于同源的方法的表现。结果显示，基于词汇同源的重建谱系与真实谱系平均更接近，提高了约三分之一。 |
| [^11] | [Unveiling the Pitfalls of Knowledge Editing for Large Language Models](https://arxiv.org/abs/2310.02129) | 这篇论文探讨了大型语言模型知识编辑的潜在陷阱，提出了新的评估方法，发现知识冲突和知识扭曲是两个重要问题。 |
| [^12] | [Enhancing Diagnostic Accuracy through Multi-Agent Conversations: Using Large Language Models to Mitigate Cognitive Bias.](http://arxiv.org/abs/2401.14589) | 本研究旨在通过利用大型语言模型和多智能体对话的方式来减轻临床决策中的认知偏差，并评估其对提高诊断准确性的有效性。 |
| [^13] | [CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks.](http://arxiv.org/abs/2401.14109) | CompactifAI是一种使用量子启发的张量网络对大型语言模型进行极压缩的创新方法，相比于传统的压缩方法，它更注重模型的相关空间，实现更加可控和精细的压缩。 |
| [^14] | [LocMoE: A Low-overhead MoE for Large Language Model Training.](http://arxiv.org/abs/2401.13920) | LocMoE提出了一种新的路由策略，通过将部分节点间通信转换为节点内通信，结合负载平衡和局部性，以提高大型语言模型训练的性能。 |
| [^15] | [Prompt-based mental health screening from social media text.](http://arxiv.org/abs/2401.05912) | 本文提出了一种利用提示信息进行社交媒体文本的心理健康筛查方法，结果与BERT混合专家分类器相当，但计算成本更低。 |
| [^16] | [Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey.](http://arxiv.org/abs/2310.17894) | 本调查对表格数据查询和可视化的自然语言界面进行了全面概述，介绍了语义解析等关键技术，并深入探讨了Text-to-SQL和Text-to-Vis问题的最新进展。 |
| [^17] | [Hidden Citations Obscure True Impact in Science.](http://arxiv.org/abs/2310.16181) | 隐藏引用现象在科学中普遍存在，并且超过了正式引用的数量，表明传统的引文分析方法无法准确评估科学发现的影响力。 |
| [^18] | [The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks.](http://arxiv.org/abs/2310.15469) | 《Janus接口：大型语言模型微调如何放大隐私风险》研究了大型语言模型的微调对个人信息泄露的风险，发现了一种新的LLM利用途径。 |
| [^19] | [How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses.](http://arxiv.org/abs/2310.03031) | 本文系统地分析并探索了德语和英语的ChatGPT回应中可能存在的问题，特别关注了性别偏见。我们发现，在对系统多次提供相同指令的情况下，回应存在差异。使用ChatGPT来帮助非IT用户撰写工作文本非常有用，但用户需要充分考虑系统的固有限制。 |
| [^20] | [C-Pack: Packaged Resources To Advance General Chinese Embedding.](http://arxiv.org/abs/2309.07597) | C-Pack是一套推进普通汉语嵌入领域的资源，包括全面汉语文本嵌入基准、大规模文本嵌入数据集和涵盖多个尺寸的嵌入模型系列。该资源集在C-MTEB基准上实现了最高+10%的表现，并通过整合和优化一套训练方法进一步提升了效果。此外，C-Pack还发布了英语文本嵌入数据和模型，实现了最先进的性能。该资源集可公开获取。 |
| [^21] | [Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf.](http://arxiv.org/abs/2309.04658) | 本研究探索了大型语言模型在沟通游戏中的应用，提出了一个无需调参的框架，并通过对狼人杀游戏的实证研究展示了其有效性和出现的战略行为。这表明在沟通游戏和相关领域中使用大型语言模型将具备潜在价值。 |
| [^22] | [Inverse Scaling: When Bigger Isn't Better.](http://arxiv.org/abs/2306.09479) | 本文研究发现，相对于规模的增加，大型语言模型的任务性能可能出现逆向缩放现象。这一逆向缩放的原因可能有四种：记忆重现、学习样本错误、任务易于干扰、和任务示范的误导。 |
| [^23] | [On the Security Vulnerabilities of Text-to-SQL Models.](http://arxiv.org/abs/2211.15363) | 该论文揭示了Text-to-SQL模型存在的安全漏洞，并证明了这些漏洞能够被恶意利用产生攻击，通过对商业应用和开源语言模型的实验验证。该研究意在引起学术界对NLP算法相关的软件安全问题的关注和进一步研究。 |
| [^24] | [Log-linear Guardedness and its Implications.](http://arxiv.org/abs/2210.10012) | 本研究介绍了对数线性保护性及其对下游分类器行为的影响。在二元情况下，下游对数线性模型无法恢复被删除的概念，但在某些情况下，可以通过构建多类对数线性模型间接恢复概念。这些结果揭示了线性删除方法的局限性，并强调了进一步研究的需求。 |

# 详细

[^1]: 请真正的琳达站出来...面对大语言模型？在LLMs中审视代表性启发式

    Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs

    [https://arxiv.org/abs/2404.01461](https://arxiv.org/abs/2404.01461)

    该研究调查了代表性启发式对大型语言模型推理的影响，并创建了专门的数据集进行实验验证

    

    尽管大型语言模型（LLMs）在理解文本和生成类似人类文本方面表现出色，但它们可能会展现出从训练数据中获得的偏见。具体而言，LLMs可能会容易受到人类决策中的一种常见认知陷阱影响，即代表性启发式。这是心理学中的一个概念，指的是根据事件与一个众所周知的原型或典型例子的相似程度来判断事件发生的可能性，而不考虑更广泛的事实或统计证据。本研究调查了代表性启发式对LLM推理的影响。我们创建了REHEAT（Representativeness Heuristic AI Testing），一个包含涵盖六种常见代表性启发式类型问题的数据集。实验显示，应用于REHEAT的四个LLMs都表现出代表性启发式偏见。我们进一步确定了模型的推理步骤

    arXiv:2404.01461v1 Announce Type: new  Abstract: Although large language models (LLMs) have demonstrated remarkable proficiency in understanding text and generating human-like text, they may exhibit biases acquired from training data in doing so. Specifically, LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic. This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example versus considering broader facts or statistical evidence. This work investigates the impact of the representativeness heuristic on LLM reasoning. We created REHEAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics. Experiments reveal that four LLMs applied to REHEAT all exhibited representativeness heuristic biases. We further identify that the model's reasoning steps
    
[^2]: 通过重新表述前缀提示来编辑语言Lodel的知识表示

    Editing Knowledge Representation of Language Lodel via Rephrased Prefix Prompts

    [https://arxiv.org/abs/2403.14381](https://arxiv.org/abs/2403.14381)

    引入了一种名为PSPEM的新方法，通过重新表述前缀提示来编辑语言Lodel的知识表示，解决了知识编辑方法中的低效性、通用性问题，以及提示工程的不透明性。

    

    神经语言模型（LMs）已在广泛的语料库上进行了大量培训，以存储关于文本描述的世界各个方面的事实知识。当前技术通常采用知识编辑方法或特定提示来修改LM输出。然而，现有的知识编辑方法成本高昂且低效，难以产生适当的文本。此外，提示工程是不透明的，需要大量努力找到合适的提示。为解决这些问题，我们引入了一种称为PSPEM（前缀软提示编辑方法）的新方法，可以仅通过一次训练而终身使用。它解决了知识编辑方法中的低效性和通用性问题，并通过自动寻找最佳软提示来克服提示工程的不透明性。具体而言，PSPEM利用提示编码器和编码转换器来精炼提示中的关键信息，并使用提示对齐

    arXiv:2403.14381v1 Announce Type: cross  Abstract: Neural language models (LMs) have been extensively trained on vast corpora to store factual knowledge about various aspects of the world described in texts. Current technologies typically employ knowledge editing methods or specific prompts to modify LM outputs. However, existing knowledge editing methods are costly and inefficient, struggling to produce appropriate text. Additionally, prompt engineering is opaque and requires significant effort to find suitable prompts. To address these issues, we introduce a new method called PSPEM (Prefix Soft Prompt Editing Method), that can be used for a lifetime with just one training. It resolves the inefficiencies and generalizability issues in knowledge editing methods and overcomes the opacity of prompt engineering by automatically seeking optimal soft prompts. Specifically, PSPEM utilizes a prompt encoder and an encoding converter to refine key information in prompts and uses prompt alignmen
    
[^3]: 大型语言模型是否像人一样解决ARC视觉类比问题？

    Do Large Language Models Solve ARC Visual Analogies Like People Do?

    [https://arxiv.org/abs/2403.09734](https://arxiv.org/abs/2403.09734)

    该研究比较了人类和大型语言模型在ARC视觉类比问题上的表现，发现在特定任务上，人类和成年人的表现均优于大多数大型语言模型。对LLMs和年幼儿童错误分析揭示了类似的解决策略，同时指出了两种不同的错误类型，为我们理解LLMs如何解决视觉类比问题提供了新的启示。

    

    抑制论文（Chollet, 2019）形式，我们比较了儿童友好的ARC项目上人类和大型语言模型（LLM）的表现。结果表明，无论是儿童还是成年人，在这些任务上都胜过大多数LLMs。错误分析揭示了LLMs和年幼儿童之间类似的“倒退”解决策略，其中类比的一部分被简单复制。此外，我们发现其他两种错误类型，一种基于表面掌握关键概念（例如，内外关系），另一种基于类比输入矩阵的简单组合。总体而言，“概念”错误在人类中更常见，“矩阵”错误在LLMs中更常见。这项研究为LLM的推理能力和我们可以使用错误分析以及与人类发展的比较来理解LLMs如何解决视觉类比问题提供了新的视角。

    arXiv:2403.09734v1 Announce Type: cross  Abstract: The Abstraction Reasoning Corpus (ARC) is a visual analogical reasoning test designed for humans and machines (Chollet, 2019). We compared human and large language model (LLM) performance on a new child-friendly set of ARC items. Results show that both children and adults outperform most LLMs on these tasks. Error analysis revealed a similar "fallback" solution strategy in LLMs and young children, where part of the analogy is simply copied. In addition, we found two other error types, one based on seemingly grasping key concepts (e.g., Inside-Outside) and the other based on simple combinations of analogy input matrices. On the whole, "concept" errors were more common in humans, and "matrix" errors were more common in LLMs. This study sheds new light on LLM reasoning ability and the extent to which we can use error analyses and comparisons with human development to understand how LLMs solve visual analogies.
    
[^4]: 训练小型多模态模型以填补生物医学能力差距：以放射学成像为例

    Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging

    [https://arxiv.org/abs/2403.08002](https://arxiv.org/abs/2403.08002)

    本文针对生物医学应用中前沿模型尚存在的多模态能力差距，探讨了训练开源小型多模态模型以弥补临床需求的生物医学能力差距。

    

    放大基础模型的尺度规律和非凡表现激励了在生物医学领域开发和利用这些大型模型。然而，尽管在一些生物医学基准测试中取得了早期有希望的结果，但在这些模型能够应用于真实世界的应用之前仍然存在一些重大挑战。像GPT-4V这样的前沿模型在生物医学应用中仍存在重大的多模态能力差距。此外，访问、成本、延迟和合规等实际问题使临床医生难以直接在私人患者数据上使用私人托管的最先进大型模型。在本文中，我们探讨训练开源小型多模态模型（SMMs）来填补未满足的临床需求的生物医学能力差距。为了最大化数据效率，我们采用模块化方法，将用于图像和文本模态的最先进预训练模型纳入，并侧重于t

    arXiv:2403.08002v1 Announce Type: new  Abstract: The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such large models in biomedicine. However, despite early promising results on some biomedical benchmarks, there are still major challenges that need to be addressed before these models can be used in real-world applications. Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications. Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data. In this paper, we explore training open-source small multimodal models (SMMs) to bridge biomedical competency gaps for unmet clinical needs. To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre-trained models for image and text modalities, and focusing on t
    
[^5]: 信心至关重要：重新审视大型语言模型的内在自我校正能力

    Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models

    [https://arxiv.org/abs/2402.12563](https://arxiv.org/abs/2402.12563)

    本文研究了大型语言模型的内在自我校正能力，并提出了一个“如果-否则”（IoE）提示框架，帮助模型评估自身“信心”并进行自我校正。

    

    大型语言模型（LLMs）的最近成功激发了对它们自我校正能力的越来越多的兴趣。本文对LLMs的内在自我校正进行了全面调查，试图解决关于其可行性的持续争论。我们的研究确定了一个重要的潜在因素 - LLMs的“信心” - 在自我校正过程中。忽视这一因素可能导致模型过度批评自己，从而导致对自校正效果的可靠结论不准确。我们实验观察到LLMs具有理解其自身回应“信心”的能力。这激励我们开发了一个“如果-否则”（IoE）提示框架，旨在引导LLMs评估其自身“信心”，促进内在自我校正。我们进行了大量实验证明，我们基于IoE的提示可以实现一

    arXiv:2402.12563v1 Announce Type: cross  Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a co
    
[^6]: OneBit:朝着极低比特大型语言模型迈进

    OneBit: Towards Extremely Low-bit Large Language Models

    [https://arxiv.org/abs/2402.11295](https://arxiv.org/abs/2402.11295)

    本文提出了一种名为OneBit的1位量化感知训练框架，可以将大型语言模型的权重矩阵量化为1位，为极低比特宽度的LLMs部署铺平了道路。

    

    模型量化使用低比特宽度值来表示模型的权重矩阵，这是减少部署高度期待的LLMs的存储和计算开销的一种有前途的方法。然而，现有的量化方法在比特宽度极小时性能严重下降，因此专注于利用4位或8位值来量化模型。本文大胆地将LLMs的权重矩阵量化为1位，为LLMs的极低比特宽度部署铺平了道路。为此，我们引入了一个名为OneBit的1位量化感知训练（QAT）框架，其中包括一种更好地量化LLMs的新颖的1位参数表示方法，以及基于矩阵分解的有效参数初始化方法，以提高QAT框架的收敛速度。充分的实验结果表明，OneBit取得了良好的性能（至少是非

    arXiv:2402.11295v1 Announce Type: new  Abstract: Model quantification uses low bit-width values to represent the weight matrices of models, which is a promising approach to reduce both storage and computational overheads of deploying highly anticipated LLMs. However, existing quantization methods suffer severe performance degradation when the bit-width is extremely reduced, and thus focus on utilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes the weight matrices of LLMs to 1-bit, paving the way for the extremely low bit-width deployment of LLMs. For this target, we introduce a 1-bit quantization-aware training (QAT) framework named OneBit, including a novel 1-bit parameter representation method to better quantize LLMs as well as an effective parameter initialization method based on matrix decomposition to improve the convergence speed of the QAT framework. Sufficient experimental results indicate that OneBit achieves good performance (at least 83% of the non
    
[^7]: 从单一儿童语言输入的可学习性的系统调查

    A systematic investigation of learnability from single child linguistic input

    [https://arxiv.org/abs/2402.07899](https://arxiv.org/abs/2402.07899)

    我们的研究探索了用单个儿童的语言输入训练语言模型的可学习性，我们发现这种设置下的语言模型能够形成句法和语义词群，并对某些语言现象具有敏感性。

    

    语言模型（LM）在生成语言连贯文本方面表现出了 remarkable proficiency，引发了关于它们与人类语言可学习性的相关讨论。然而，这些模型的训练数据与儿童接收到的语言输入之间存在着显著差距。LMs通常在数量级上更大且本质与儿童语言输入不同的数据上进行训练。针对这一差距，我们的研究侧重于在单个儿童语言输入的子集上训练LMs。先前的研究发现，在这种设置下训练的LMs可以形成句法和语义词群，并对某些语言现象具有敏感性。然而，这些研究仅考虑了仅使用一个单一儿童数据集训练的LSTMs和更简单的神经网络。为了检验从单一儿童输入可学习性的鲁棒性，我们系统地…

    Language models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability. However, a significant gap exists between the training data for these models and the linguistic input a child receives. LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our research focuses on training LMs on subsets of a single child's linguistic input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset. Here, to examine the robustness of learnability from single-child input, we systematicall
    
[^8]: Mercury: 一种用于LLM代码综合效率评估的基准

    Mercury: An Efficiency Benchmark for LLM Code Synthesis

    [https://arxiv.org/abs/2402.07844](https://arxiv.org/abs/2402.07844)

    Mercury提出了一个针对LLM代码综合任务的效率评估基准，通过引入新的度量标准Beyond@K来衡量归一化的代码效率，从而鼓励生成功能正确且计算效率高的代码。

    

    尽管在评估大型语言模型（LLM）进行代码综合方面取得了进展，但基准主要集中在功能正确性上，忽视了代码效率的重要性。我们提出了Mercury，这是第一个专用于评估LLM代码综合任务的代码效率的基准。Mercury由1,889个涵盖不同难度级别的编程任务组成，还包括生成无限案例的测试用例生成器，以进行全面评估。与现有的基准不同，Mercury集成了一种新的度量标准Beyond@K，以基于历史提交来衡量归一化的代码效率，从而为代码综合提供了新的评估指标，鼓励生成功能正确且计算效率高的代码，体现了现实世界软件开发的标准。我们的研究结果表明，虽然LLM表现出生成功能正确代码的显著能力，但它们在效率输出方面仍存在很大的差距。

    Despite advancements in evaluating Large Language Models (LLMs) for code synthesis, benchmarks have predominantly focused on functional correctness, overlooking the importance of code efficiency. We present Mercury, the first benchmark designated for assessing the code efficiency of LLM code synthesis tasks. Mercury consists of 1,889 programming tasks covering diverse difficulty levels alongside test case generators generating unlimited cases for comprehensive evaluation. Unlike existing benchmarks, Mercury integrates a novel metric Beyond@K to measure normalized code efficiency based on historical submissions, leading to a new evaluation indicator for code synthesis, which encourages generating functionally correct and computationally efficient code, mirroring the real-world software development standard. Our findings reveal that while LLMs demonstrate the remarkable capability to generate functionally correct code, there still exists a substantial gap in their efficiency output, unde
    
[^9]: TexShape:信息论句子嵌入用于语言模型

    TexShape: Information Theoretic Sentence Embedding for Language Models

    [https://arxiv.org/abs/2402.05132](https://arxiv.org/abs/2402.05132)

    这项研究提出了一种名为TexShape的信息论句子嵌入模型，通过使用互信息的经验估计来优化文本表示，可用于数据压缩和敏感信息过滤，提升隐私和公平性。

    

    随着数据量的指数增长和数据密集应用的出现，尤其是在机器学习领域，与资源利用、隐私和公平性相关的问题变得越来越重要。本文关注数据的文本领域，并通过信息论的视角解决了将句子编码为其优化表示的挑战。具体而言，我们使用Donsker-Varadhan定义的Kullback-Leibler散度的经验估计值来计算互信息。我们的方法利用这种估计来训练一种信息论句子嵌入模型，称为TexShape，用于（基于任务的）数据压缩或过滤敏感信息，从而增强隐私和公平性。在本研究中，我们使用基准语言模型进行初步文本表示，并用神经网络进行信息理论压缩和互信息估计。我们的实验表明，我们的方法显著进展。

    With the exponential growth in data volume and the emergence of data-intensive applications, particularly in the field of machine learning, concerns related to resource utilization, privacy, and fairness have become paramount. This paper focuses on the textual domain of data and addresses challenges regarding encoding sentences to their optimized representations through the lens of information-theory. In particular, we use empirical estimates of mutual information, using the Donsker-Varadhan definition of Kullback-Leibler divergence. Our approach leverages this estimation to train an information-theoretic sentence embedding, called TexShape, for (task-based) data compression or for filtering out sensitive information, enhancing privacy and fairness. In this study, we employ a benchmark language model for initial text representation, complemented by neural networks for information-theoretic compression and mutual information estimations. Our experiments demonstrate significant advanceme
    
[^10]: 声音对于系统发育重建可靠吗？

    Are Sounds Sound for Phylogenetic Reconstruction?

    [https://arxiv.org/abs/2402.02807](https://arxiv.org/abs/2402.02807)

    本文通过对十个不同语言家族的多样数据集进行研究，首次在系统发育重建中比较了基于声音和基于同源的方法的表现。结果显示，基于词汇同源的重建谱系与真实谱系平均更接近，提高了约三分之一。

    

    在传统的语言进化研究中，学者们通常强调声音规律和对应关系对于语言家族谱系推断的重要性。然而，迄今为止，计算方法往往没有充分考虑到这一潜力。大多数计算方法仍然依赖于词汇同源作为语言学系统发育重建的主要数据来源，尽管也有一些研究中的作者赞赏比较声音序列的好处。基于十个来自不同语言家族的多样数据集和现代自动同源和声音对应检测方法，我们首次测试了基于声音和基于同源的方法在系统发育重建中的性能。结果表明，通过词汇同源重建的谱系在广义四元组距离上与真实谱系平均更接近，提升了约三分之一。

    In traditional studies on language evolution, scholars often emphasize the importance of sound laws and sound correspondences for phylogenetic inference of language family trees. However, to date, computational approaches have typically not taken this potential into account. Most computational studies still rely on lexical cognates as major data source for phylogenetic reconstruction in linguistics, although there do exist a few studies in which authors praise the benefits of comparing words at the level of sound sequences. Building on (a) ten diverse datasets from different language families, and (b) state-of-the-art methods for automated cognate and sound correspondence detection, we test, for the first time, the performance of sound-based versus cognate-based approaches to phylogenetic reconstruction. Our results show that phylogenies reconstructed from lexical cognates are topologically closer, by approximately one third with respect to the generalized quartet distance on average, 
    
[^11]: 揭示大语言模型知识编辑的陷阱

    Unveiling the Pitfalls of Knowledge Editing for Large Language Models

    [https://arxiv.org/abs/2310.02129](https://arxiv.org/abs/2310.02129)

    这篇论文探讨了大型语言模型知识编辑的潜在陷阱，提出了新的评估方法，发现知识冲突和知识扭曲是两个重要问题。

    

    随着调整大型语言模型（LLMs）成本不断上升，最近的研究工作已经转向开发编辑LLMs内在知识的方法。然而，仍有一个阴云悬在头顶上 - 知识编辑是否会触发蝴蝶效应？因为目前尚不清楚知识编辑是否会引入可能带来潜在风险的副作用。本文首次探讨了与LLMs知识编辑相关的潜在陷阱。为实现此目的，我们引入了新的基准数据集并提出了创新性的评估指标。我们的结果强调了两个关键问题：（1）知识冲突：编辑逻辑冲突的事实组可能会放大LLMs固有的不一致性 - 这是以前方法忽略的一个方面。（2）知识扭曲：为了编辑事实知识而更改参数可能会不可逆地扭曲

    arXiv:2310.02129v3 Announce Type: replace-cross  Abstract: As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp 
    
[^12]: 通过多智能体对话提高诊断准确度：利用大型语言模型减少认知偏差

    Enhancing Diagnostic Accuracy through Multi-Agent Conversations: Using Large Language Models to Mitigate Cognitive Bias. (arXiv:2401.14589v1 [cs.CL])

    [http://arxiv.org/abs/2401.14589](http://arxiv.org/abs/2401.14589)

    本研究旨在通过利用大型语言模型和多智能体对话的方式来减轻临床决策中的认知偏差，并评估其对提高诊断准确性的有效性。

    

    背景：临床决策中的认知偏差显著导致诊断错误和次优患者结果。解决这些偏差问题在医疗领域面临巨大挑战。本研究通过利用大型语言模型（LLMs）在多智能体框架中减轻这些偏差的作用。我们通过多智能体对话模拟临床决策过程，并评估其对改善诊断准确性的有效性。方法：从文献中找到了总共16个已发表和未发表的病例报告，其中认知偏差导致误诊。在多智能体系统中，我们利用 GPT-4 Turbo 促进四个模拟智能体之间的交互，以复制临床团队动态。每个智能体都有独特的角色：1) 在考虑讨论后进行初步和最终诊断。2) 充当魔鬼的代言人，以纠正确认偏差和锚定偏差。3) 充当导师和促进者。

    Background: Cognitive biases in clinical decision-making significantly contribute to errors in diagnosis and suboptimal patient outcomes. Addressing these biases presents a formidable challenge in the medical field. This study explores the role of large language models (LLMs) in mitigating these biases through the utilization of a multi-agent framework. We simulate the clinical decision-making processes through multi-agent conversation and evaluate its efficacy in improving diagnostic accuracy. Methods: A total of 16 published and unpublished case reports where cognitive biases have resulted in misdiagnoses were identified from the literature. In the multi-agent system, we leveraged GPT-4 Turbo to facilitate interactions among four simulated agents to replicate clinical team dynamics. Each agent has a distinct role: 1) To make the initial and final diagnosis after considering the discussions, 2) The devil's advocate and correct confirmation and anchoring bias, 3) The tutor and facilita
    
[^13]: CompactifAI: 使用量子启发的张量网络对大型语言模型进行极压缩

    CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks. (arXiv:2401.14109v1 [cs.CL])

    [http://arxiv.org/abs/2401.14109](http://arxiv.org/abs/2401.14109)

    CompactifAI是一种使用量子启发的张量网络对大型语言模型进行极压缩的创新方法，相比于传统的压缩方法，它更注重模型的相关空间，实现更加可控和精细的压缩。

    

    大型语言模型（LLM）如ChatGPT和LlaMA在生成人工智能（AI）方面取得了快速进展，但其庞大的规模带来了重要挑战，如巨大的训练和推断成本、较大的能源需求以及现场部署的限制。传统的压缩方法如剪枝、蒸馏和低秩逼近主要关注减少网络中神经元的有效数量，而量化方法则侧重于降低单个权重的数值精度，以减小模型大小同时保持神经元数目不变。虽然这些压缩方法在实践中取得了相对成功，但没有令人信服的理由认为截断神经元的数量是一种最优策略。本文介绍了一种创新的LLM压缩方法CompactifAI，它使用量子启发的张量网络，而不是传统的压缩方法，更注重模型的相关空间，实现更加可控和精细的压缩。

    Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly in generative Artificial Intelligence (AI), but their immense size poses significant challenges, such as huge training and inference costs, substantial energy demands, and limitations for on-site deployment. Traditional compression methods such as pruning, distillation, and low-rank approximation focus on reducing the effective number of neurons in the network, while quantization focuses on reducing the numerical precision of individual weights to reduce the model size while keeping the number of neurons fixed. While these compression methods have been relatively successful in practice, there's no compelling reason to believe that truncating the number of neurons is an optimal strategy. In this context, this paper introduces CompactifAI, an innovative LLM compression approach using quantum-inspired Tensor Networks that focuses on the model's correlation space instead, allowing for a more controlled, refined an
    
[^14]: LocMoE: 一种用于大型语言模型训练的低开销MoE

    LocMoE: A Low-overhead MoE for Large Language Model Training. (arXiv:2401.13920v1 [cs.LG])

    [http://arxiv.org/abs/2401.13920](http://arxiv.org/abs/2401.13920)

    LocMoE提出了一种新的路由策略，通过将部分节点间通信转换为节点内通信，结合负载平衡和局部性，以提高大型语言模型训练的性能。

    

    混合专家模型（MoE）是一种广泛采用的分布式和集成学习方法，用于大型语言模型（LLM），由于其能够有效稀疏和扩展模型，因此备受青睐。然而，MoE的性能受到负载不平衡和全对全通信的高延迟的限制，同时由于大量的专家容量导致相对冗余的计算。负载不平衡可能是由于现有路由策略始终倾向于选择特定的专家导致的。全对全过程中频繁的节点间通信也显著延长了训练时间。为了缓解上述性能问题，我们提出了一种新的路由策略，通过将部分节点间通信转换为节点内通信，结合负载平衡和局部性。值得注意的是，我们阐明了专家容量的最小阈值，通过将专家的门控权重与分配的标记之间的最大角偏差计算出来。

    The Mixtures-of-Experts (MoE) model is a widespread distributed and integrated learning method for large language models (LLM), which is favored due to its ability to sparsify and expand models efficiently. However, the performance of MoE is limited by load imbalance and high latency of All-To-All communication, along with relatively redundant computation owing to large expert capacity. Load imbalance may result from existing routing policies that consistently tend to select certain experts. The frequent inter-node communication in the All-To-All procedure also significantly prolongs the training time. To alleviate the above performance problems, we propose a novel routing strategy that combines load balance and locality by converting partial inter-node communication to that of intra-node. Notably, we elucidate that there is a minimum threshold for expert capacity, calculated through the maximal angular deviation between the gating weights of the experts and the assigned tokens. We por
    
[^15]: 从社交媒体文本中提取提示信息进行心理健康筛查

    Prompt-based mental health screening from social media text. (arXiv:2401.05912v1 [cs.CL])

    [http://arxiv.org/abs/2401.05912](http://arxiv.org/abs/2401.05912)

    本文提出了一种利用提示信息进行社交媒体文本的心理健康筛查方法，结果与BERT混合专家分类器相当，但计算成本更低。

    

    本文提出了一种从大规模嘈杂的社交媒体文本数据集中进行基于提示的心理健康筛查的方法。我们的方法利用GPT 3.5进行提示，以区分可能与任务相关的出版物，然后使用简单的词袋文本分类器预测实际用户标签。结果显示，该方法与BERT混合专家分类器相当，并且只需要一小部分计算成本。

    This article presents a method for prompt-based mental health screening from a large and noisy dataset of social media text. Our method uses GPT 3.5. prompting to distinguish publications that may be more relevant to the task, and then uses a straightforward bag-of-words text classifier to predict actual user labels. Results are found to be on pair with a BERT mixture of experts classifier, and incurring only a fraction of its computational costs.
    
[^16]: 对表格数据查询和可视化的自然语言界面：一项调查

    Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey. (arXiv:2310.17894v1 [cs.CL])

    [http://arxiv.org/abs/2310.17894](http://arxiv.org/abs/2310.17894)

    本调查对表格数据查询和可视化的自然语言界面进行了全面概述，介绍了语义解析等关键技术，并深入探讨了Text-to-SQL和Text-to-Vis问题的最新进展。

    

    自然语言处理的出现彻底改变了用户与表格数据的交互方式，实现了从传统的查询语言和手动绘图转向更直观、基于语言的界面。大型语言模型（LLM）如ChatGPT及其后继者进一步推动了这一领域的发展，为自然语言处理技术开辟了新的途径。本调查提供了关于表格数据查询和可视化的自然语言界面的全面概述，这些界面允许用户使用自然语言查询与数据进行交互。我们介绍了这些界面的基本概念和技术，特别强调语义解析，这是实现从自然语言到SQL查询或数据可视化命令转化的关键技术。然后从数据集、方法论、评估指标和系统设计的角度深入探讨了Text-to-SQL和Text-to-Vis问题的最新进展。

    The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization, which allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis problems from the perspectives of datasets, methodologies, metrics, and system designs. Thi
    
[^17]: 科学中的隐藏引用模糊了真正的影响力

    Hidden Citations Obscure True Impact in Science. (arXiv:2310.16181v1 [cs.CL])

    [http://arxiv.org/abs/2310.16181](http://arxiv.org/abs/2310.16181)

    隐藏引用现象在科学中普遍存在，并且超过了正式引用的数量，表明传统的引文分析方法无法准确评估科学发现的影响力。

    

    引用是科学家们用来表示之前知识的机制，但最近已经变成了广泛使用和滥用的科学影响力的度量标准。然而，当一个发现变成共识时，引用会因为被忽视而被合并。这导致了隐藏引用的概念，它表示对一个发现的明确文本认可，但没有引用该发现的出版物。在这里，我们依赖于无监督可解释机器学习应用于每篇论文的全文，以系统地识别隐藏引用。我们发现，对于有影响力的发现，隐藏引用数量超过了引用计数，而且不受出版场所和学科的限制。我们展示了隐藏引用的普遍性不是由引用计数驱动的，而是由于手稿文本中对话题的讨论程度决定的，这表明一个发现被讨论得越多，它在标准的引文分析中就越不可见。隐藏引用指示了...

    References, the mechanism scientists rely on to signal previous knowledge, lately have turned into widely used and misused measures of scientific impact. Yet, when a discovery becomes common knowledge, citations suffer from obliteration by incorporation. This leads to the concept of hidden citation, representing a clear textual credit to a discovery without a reference to the publication embodying it. Here, we rely on unsupervised interpretable machine learning applied to the full text of each paper to systematically identify hidden citations. We find that for influential discoveries hidden citations outnumber citation counts, emerging regardless of publishing venue and discipline. We show that the prevalence of hidden citations is not driven by citation counts, but rather by the degree of the discourse on the topic within the text of the manuscripts, indicating that the more discussed is a discovery, the less visible it is to standard bibliometric analysis. Hidden citations indicate t
    
[^18]: 《Janus接口：大型语言模型微调如何放大隐私风险》

    The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks. (arXiv:2310.15469v1 [cs.CR])

    [http://arxiv.org/abs/2310.15469](http://arxiv.org/abs/2310.15469)

    《Janus接口：大型语言模型微调如何放大隐私风险》研究了大型语言模型的微调对个人信息泄露的风险，发现了一种新的LLM利用途径。

    

    2018年后的时代标志着大型语言模型（LLM）的出现，OpenAI的ChatGPT等创新展示了惊人的语言能力。随着行业在增加模型参数并利用大量的人类语言数据方面的努力，安全和隐私挑战也出现了。其中最重要的是在基于网络的数据获取过程中，可能会意外积累个人可识别信息（PII），从而导致意外的PII泄露风险。虽然像RLHF和灾难性遗忘这样的策略已被用来控制隐私侵权的风险，但LLM的最新进展（以OpenAI的GPT-3.5的微调界面为代表）重新引发了关注。有人可能会问：LLM的微调是否会导致训练数据集中嵌入的个人信息泄漏？本文报道了首次尝试寻求答案的努力，重点是我们发现了一种新的LLM利用途径。

    The era post-2018 marked the advent of Large Language Models (LLMs), with innovations such as OpenAI's ChatGPT showcasing prodigious linguistic prowess. As the industry galloped toward augmenting model parameters and capitalizing on vast swaths of human language data, security and privacy challenges also emerged. Foremost among these is the potential inadvertent accrual of Personal Identifiable Information (PII) during web-based data acquisition, posing risks of unintended PII disclosure. While strategies like RLHF during training and Catastrophic Forgetting have been marshaled to control the risk of privacy infringements, recent advancements in LLMs, epitomized by OpenAI's fine-tuning interface for GPT-3.5, have reignited concerns. One may ask: can the fine-tuning of LLMs precipitate the leakage of personal information embedded within training datasets? This paper reports the first endeavor to seek the answer to the question, particularly our discovery of a new LLM exploitation avenue
    
[^19]: ChatGPT中的性别偏见有多普遍？—— 探索德语和英语ChatGPT的回应

    How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses. (arXiv:2310.03031v1 [cs.CL])

    [http://arxiv.org/abs/2310.03031](http://arxiv.org/abs/2310.03031)

    本文系统地分析并探索了德语和英语的ChatGPT回应中可能存在的问题，特别关注了性别偏见。我们发现，在对系统多次提供相同指令的情况下，回应存在差异。使用ChatGPT来帮助非IT用户撰写工作文本非常有用，但用户需要充分考虑系统的固有限制。

    

    随着ChatGPT的推出，OpenAI使得大型语言模型（LLM）可供具有有限IT专业知识的用户使用。然而，没有自然语言处理（NLP）背景的用户可能缺乏对LLM的适当理解。因此，在处理系统输出时，缺乏对其固有限制的意识，将接受系统输出的表面价值。在本文中，我们系统地分析输入提示和生成的回应，以识别可能存在的问题，特别关注性别偏见问题，用户在处理系统输出时需要意识到这一点。我们探索了ChatGPT在英语和德语中的反应，并提供了女性、男性或中立角度的指令时，回复的是否有差异。通过深入调查，我们研究了一些选择的提示，并分析了系统在相同方式下多次提供指令时回应的差异程度。在此基础上，我们展示了对于帮助非IT用户撰写日常工作文本，ChatGPT确实非常有用。然而，当然至关重要的是要意识到，当处理系统输出时，用户需要充分考虑到其固有限制。

    With the introduction of ChatGPT, OpenAI made large language models (LLM) accessible to users with limited IT expertise. However, users with no background in natural language processing (NLP) might lack a proper understanding of LLMs. Thus the awareness of their inherent limitations, and therefore will take the systems' output at face value. In this paper, we systematically analyse prompts and the generated responses to identify possible problematic issues with a special focus on gender biases, which users need to be aware of when processing the system's output. We explore how ChatGPT reacts in English and German if prompted to answer from a female, male, or neutral perspective. In an in-depth investigation, we examine selected prompts and analyse to what extent responses differ if the system is prompted several times in an identical way. On this basis, we show that ChatGPT is indeed useful for helping non-IT users draft texts for their daily work. However, it is absolutely crucial to 
    
[^20]: C-Pack: 推进普通汉语嵌入的打包资源

    C-Pack: Packaged Resources To Advance General Chinese Embedding. (arXiv:2309.07597v1 [cs.CL])

    [http://arxiv.org/abs/2309.07597](http://arxiv.org/abs/2309.07597)

    C-Pack是一套推进普通汉语嵌入领域的资源，包括全面汉语文本嵌入基准、大规模文本嵌入数据集和涵盖多个尺寸的嵌入模型系列。该资源集在C-MTEB基准上实现了最高+10%的表现，并通过整合和优化一套训练方法进一步提升了效果。此外，C-Pack还发布了英语文本嵌入数据和模型，实现了最先进的性能。该资源集可公开获取。

    

    我们介绍了C-Pack，这是一套显著推进普通汉语嵌入领域的资源。C-Pack包括三个关键资源。1）C-MTEB是一个涵盖6个任务和35个数据集的全面汉语文本嵌入基准。2）C-MTP是一个从标记和未标记的汉语语料库中策划的大规模文本嵌入数据集，用于训练嵌入模型。3）C-TEM是一个涵盖多个尺寸的嵌入模型系列。我们的模型在C-MTEB上的表现优于之前的所有汉语文本嵌入达到了发布时的最高+10%。我们还整合和优化了C-TEM的整套训练方法。除了我们关于普通汉语嵌入的资源外，我们还发布了我们的英语文本嵌入数据和模型。这些英语模型在MTEB基准上实现了最先进的性能；与此同时，我们发布的英语数据比汉语数据大2倍。所有这些资源都可以在https://github.com/FlagOpen/FlagEmbedding上公开获取。

    We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding.
    
[^21]: 探索大型语言模型在沟通游戏中的应用：对狼人杀的实证研究

    Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf. (arXiv:2309.04658v1 [cs.CL])

    [http://arxiv.org/abs/2309.04658](http://arxiv.org/abs/2309.04658)

    本研究探索了大型语言模型在沟通游戏中的应用，提出了一个无需调参的框架，并通过对狼人杀游戏的实证研究展示了其有效性和出现的战略行为。这表明在沟通游戏和相关领域中使用大型语言模型将具备潜在价值。

    

    沟通游戏，我们把指依赖于自然语言交流的不完全信息游戏称为沟通游戏，在经济学、社会科学和人工智能等领域具有重要的研究价值。本文主要探讨如何在沟通游戏中应用大型语言模型（LLMs），并提出了一个无需调参的框架。我们的方法保持LLMs冻结状态，并利用过去的沟通和经验进行改进。对代表性且被广泛研究的沟通游戏“狼人杀”的实证研究表明，我们的框架可以在不调整LLMs参数的情况下有效地进行狼人杀游戏。更重要的是，我们的实验中出现了战略行为的迹象，这表明在沟通游戏和相关领域中使用LLMs将会是一次富有成果的旅程。

    Communication games, which we refer to as incomplete information games that heavily depend on natural language communication, hold significant research value in fields such as economics, social science, and artificial intelligence. In this work, we explore the problem of how to engage large language models (LLMs) in communication games, and in response, propose a tuning-free framework. Our approach keeps LLMs frozen, and relies on the retrieval and reflection on past communications and experiences for improvement. An empirical study on the representative and widely-studied communication game, ``Werewolf'', demonstrates that our framework can effectively play Werewolf game without tuning the parameters of the LLMs. More importantly, strategic behaviors begin to emerge in our experiments, suggesting that it will be a fruitful journey to engage LLMs in communication games and associated domains.
    
[^22]: 逆向缩放：变得更大并不意味着更好

    Inverse Scaling: When Bigger Isn't Better. (arXiv:2306.09479v1 [cs.CL])

    [http://arxiv.org/abs/2306.09479](http://arxiv.org/abs/2306.09479)

    本文研究发现，相对于规模的增加，大型语言模型的任务性能可能出现逆向缩放现象。这一逆向缩放的原因可能有四种：记忆重现、学习样本错误、任务易于干扰、和任务示范的误导。

    

    近期研究表明，随着模型规模、训练数据、计算量的增加，大型语言模型（LMs）的损失比例有可预测的改进。然而，本研究提供了证据表明，LMs也可能显示逆向缩放，即随着规模的增加任务性能越来越差，这可能是由于训练目标和数据的缺陷所致。本文通过公开比赛，Inverse Scaling Prize，在11个数据集上进行实证研究，证明了逆向缩放现象。通过分析数据集及其他实例，我们认为逆向缩放的原因可能有四种：（i）倾向于重复记忆的序列而非跟随上下文指示，（ii）在训练数据中模仿不良模式，（iii）任务中有一个易于干扰LMs的任务，将其注意力转移到较简单的任务，而非较难的任务，（iv）任务的正确示范误导LMs。作者还公布了比赛的获胜数据。

    Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling: (i) preference to repeat memorized sequences over following in-context instructions, (ii) imitation of undesirable patterns in the training data, (iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and (iv) correct but misleading few-shot demonstrations of the task. We release the winning data
    
[^23]: 关于Text-to-SQL模型的安全漏洞

    On the Security Vulnerabilities of Text-to-SQL Models. (arXiv:2211.15363v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.15363](http://arxiv.org/abs/2211.15363)

    该论文揭示了Text-to-SQL模型存在的安全漏洞，并证明了这些漏洞能够被恶意利用产生攻击，通过对商业应用和开源语言模型的实验验证。该研究意在引起学术界对NLP算法相关的软件安全问题的关注和进一步研究。

    

    尽管已经证明自然语言处理（NLP）算法容易受到恶意攻击，但这些弱点是否可能导致软件安全威胁尚未深入研究。为了弥补这一差距，我们对常用于创建自然语言数据库接口的Text-to-SQL系统进行了漏洞测试。我们展示了六个商业应用中的Text-to-SQL模块可以被操纵以产生恶意代码，潜在地导致数据泄漏和拒绝服务攻击。这是第一个证明NLP模型可以被利用为攻击向量的示例。此外，使用四个开源语言模型的实验验证了对Text-to-SQL系统进行直接后门攻击可以达到100％的成功率，而不影响其性能。本研究旨在引起学术界对与NLP算法相关的潜在软件安全问题的关注，并鼓励进一步探索。

    Although it has been demonstrated that Natural Language Processing (NLP) algorithms are vulnerable to deliberate attacks, the question of whether such weaknesses can lead to software security threats is under-explored. To bridge this gap, we conducted vulnerability tests on Text-to-SQL systems that are commonly used to create natural language interfaces to databases. We showed that the Text-to-SQL modules within six commercial applications can be manipulated to produce malicious code, potentially leading to data breaches and Denial of Service attacks. This is the first demonstration that NLP models can be exploited as attack vectors in the wild. In addition, experiments using four open-source language models verified that straightforward backdoor attacks on Text-to-SQL systems achieve a 100% success rate without affecting their performance. The aim of this work is to draw the community's attention to potential software security issues associated with NLP algorithms and encourage explor
    
[^24]: 对数线性保护性及其影响的研究

    Log-linear Guardedness and its Implications. (arXiv:2210.10012v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10012](http://arxiv.org/abs/2210.10012)

    本研究介绍了对数线性保护性及其对下游分类器行为的影响。在二元情况下，下游对数线性模型无法恢复被删除的概念，但在某些情况下，可以通过构建多类对数线性模型间接恢复概念。这些结果揭示了线性删除方法的局限性，并强调了进一步研究的需求。

    

    已经发现，在假设可线性的神经表示中，从中删除可人解释的概念的方法是可行和有用的。然而，这种删除对于基于修改后表示进行训练的下游分类器行为的影响尚未完全理解。在这项工作中，我们正式定义了对数线性保护性的概念，即对手无法直接从表示中预测概念的能力，并研究其影响。我们证明，在二元情况下，在某些假设下，下游对数线性模型无法恢复被删除的概念。然而，我们证明，在某些情况下，可以构建一个多类对数线性模型，间接恢复概念，这指出了对数线性保护性作为下游偏差缓解技术的内在局限性。这些发现揭示了线性删除方法的理论限制，并强调了进一步研究可解释神经表示与分类器之间的联系的需要。

    Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful. However, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood. In this work, we formally define the notion of log-linear guardedness as the inability of an adversary to predict the concept directly from the representation, and study its implications. We show that, in the binary case, under certain assumptions, a downstream log-linear model cannot recover the erased concept. However, we demonstrate that a multiclass log-linear model \emph{can} be constructed that indirectly recovers the concept in some cases, pointing to the inherent limitations of log-linear guardedness as a downstream bias mitigation technique. These findings shed light on the theoretical limitations of linear erasure methods and highlight the need for further research on the connections between int
    

