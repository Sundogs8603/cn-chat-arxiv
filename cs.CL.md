# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [On the Semantics of LM Latent Space: A Vocabulary-defined Approach](https://rss.arxiv.org/abs/2401.16184) | 本论文介绍了一种以词汇为定义的语义学方法，建立了LM潜在空间的参考框架，确保基于LM词汇的分离语义分析。在LM适应过程中，引入了计算logits的新技术和神经聚类模块，通过实验证明了该方法在文本理解上的优越性能。 |
| [^2] | [Measuring Social Norms of Large Language Models](https://arxiv.org/abs/2404.02491) | 论文提出了一个挑战，测试大型语言模型对社会规范的理解，构建了一个数据集涵盖广泛的社会规范问题，通过多代理框架基于大型语言模型来提高模型对社会规范的理解能力。 |
| [^3] | [Sentiment Analysis of Citations in Scientific Articles Using ChatGPT: Identifying Potential Biases and Conflicts of Interest](https://arxiv.org/abs/2404.01800) | 使用ChatGPT进行情感分析揭示了科学文章引用中的偏见和利益冲突，增强了科学文献评估的客观性和可靠性 |
| [^4] | [Structured Information Matters: Incorporating Abstract Meaning Representation into LLMs for Improved Open-Domain Dialogue Evaluation](https://arxiv.org/abs/2404.01129) | 将抽象意义表示结合到LLMs中，提出了一个简单有效的框架用于改善开放领域对话评估 |
| [^5] | [How Can Large Language Models Enable Better Socially Assistive Human-Robot Interaction: A Brief Survey](https://arxiv.org/abs/2404.00938) | 大型语言模型的最新进展为社会辅助机器人领域带来了潜在的新应用，能够显著扩展其能力，但也带来新的风险和道德关切。 |
| [^6] | [Cross-lingual Named Entity Corpus for Slavic Languages](https://arxiv.org/abs/2404.00482) | 介绍了一个手动标注的用于六种斯拉夫语言的命名实体语料库，提供了两个训练数据集划分，并使用预训练的多语言模型进行命名实体的识别、分类、引用词化和链接。 |
| [^7] | [Design as Desired: Utilizing Visual Question Answering for Multimodal Pre-training](https://arxiv.org/abs/2404.00226) | 本研究是首次利用视觉问答（VQA）进行多模态预训练，专注于引导模型学习所需病理特征，并提出了一种无需额外专家注释的问题-答案对设计方法，以及一种准文本特征转换器模块。 |
| [^8] | [A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks](https://arxiv.org/abs/2404.00076) | 提出了一种后门攻击方法，名为“DirtyFlipping”，利用脏标签技术在选定的数据模式中输入触发器，从而实现隐蔽的后门。 |
| [^9] | [Automatic Alignment of Discourse Relations of Different Discourse Annotation Frameworks](https://arxiv.org/abs/2403.20196) | 研究探索不同话语标注框架间的话语关系清单相关性，并提出半自动方法来解决这一问题。 |
| [^10] | [Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback](https://arxiv.org/abs/2403.18349) | 这里是中文总结出的一句话要点: 该论文研究了拒绝机制在提高大型语言模型可靠性中的作用，提出了一种基于知识反馈的强化学习框架RLKF。 |
| [^11] | [RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era of Transformers](https://arxiv.org/abs/2403.18276) | Mamba模型基于状态空间模型，在多个序列建模任务中取得了与Transformer相当的性能，并在经典信息检索任务--文档排名中展现了其有效性。 |
| [^12] | [MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness](https://arxiv.org/abs/2403.14990) | MasonTigers在SemEval-2024 Task 1中采用集成方法，结合语言特定的BERT模型和句子变换器，在处理语义文本相关性时取得了优异的结果。 |
| [^13] | [MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation](https://arxiv.org/abs/2403.14171) | 提出了MMIDR框架，用于教导大型语言模型提供解释其多模态虚假信息决策过程的文本解释。 |
| [^14] | [Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs](https://arxiv.org/abs/2403.13801) | 用自然语言推理进行坐标级控制，能够显著提高机器人行动规划的成功率，并且具有将机器人技能迁移到新任务的潜力。 |
| [^15] | [Correcting misinformation on social media with a large language model](https://arxiv.org/abs/2403.11169) | 提出了一种名为MUSE的大型语言模型，通过访问最新信息并评估可信度，以解决社交媒体上误信息纠正的难题。 |
| [^16] | [Prediction of readmission of patients by extracting biomedical concepts from clinical texts](https://arxiv.org/abs/2403.09722) | 从临床文本中提取生物医学概念预测患者的再入院情况，可以帮助医生选择适当的治疗方法，从而减少患者再次入院的比率，实现有效的治疗成本降低。 |
| [^17] | [Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information](https://arxiv.org/abs/2403.09516) | 通过利用预定义的典型人口统计文本并在微调过程中加入正则化项，本文提出的方法有效减轻了语言模型中的社会偏见，同时不需要依赖显式的人口统计标签。 |
| [^18] | [Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference](https://arxiv.org/abs/2403.09054) | 本文提出了一种名为“Keyformer”的创新推断时间方法，旨在通过选择关键标记来减少KV缓存的挑战，提高内存带宽利用率。 |
| [^19] | [Exploring Safety Generalization Challenges of Large Language Models via Code](https://arxiv.org/abs/2403.07865) | 本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。 |
| [^20] | [PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck](https://arxiv.org/abs/2403.05297) | PEEB是一种基于部分的图像分类器，通过将类别名称转换为描述视觉部分的文本描述符，并将检测到的部分的嵌入与文本描述符匹配，从而在零样本设置中表现出色，并且不仅在监督学习中表现出色，而且还首次实现用户编辑类定义形成新分类器无需重新训练。 |
| [^21] | [Ever-Evolving Memory by Blending and Refining the Past](https://arxiv.org/abs/2403.04787) | 提出了一种新颖的长期对话记忆方案CREEM，通过混合过去记忆并引入完善过程来实现聊天机器人回应的整体改进和连贯性。 |
| [^22] | [IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators](https://arxiv.org/abs/2403.03894) | 通过利用编译器中间表示来改进代码-LMs的多语言能力和促进跨语言转移。 |
| [^23] | [FaaF: Facts as a Function for the evaluation of RAG systems](https://arxiv.org/abs/2403.03888) | FaaF是一种新的事实验证方法，利用语言模型的函数调用能力和面向RAG事实回忆评估的框架，显着提高了LM识别不支持事实的能力，并在效率和成本方面取得了明显的改进。 |
| [^24] | [General2Specialized LLMs Translation for E-commerce](https://arxiv.org/abs/2403.03689) | 提出了一个名为G2ST的两步微调范式，通过自对比语义增强将通用NMT模型转换为专门用于电子商务的NMT模型，以提高翻译质量。 |
| [^25] | [Cross-domain Chinese Sentence Pattern Parsing](https://arxiv.org/abs/2402.16311) | 本文提出了一种利用大型语言模型进行自我训练的创新方法，通过动态生成训练数据将源领域句法规则与目标领域句子相结合，增强句式结构解析器对各种领域的适应能力，实验证明其在教科书和新闻领域的效果优于基于规则的基准模型1.68个百分点。 |
| [^26] | [Dental Severity Assessment through Few-shot Learning and SBERT Fine-tuning](https://arxiv.org/abs/2402.15755) | 通过Few-shot Learning和SBERT Fine-tuning方法，研究发现该方法在口腔健康问题的严重性评估中表现优异，准确率高达94.1%。 |
| [^27] | [IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus](https://arxiv.org/abs/2402.14710) | 发布了IEPile，一个包含约0.32B个标记的综合双语IE指令语料库，通过收集和清理33个现有IE数据集并引入基于模式的指令生成，可以提高大型语言模型在信息抽取领域的性能，尤其是零样本泛化。 |
| [^28] | [Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models](https://arxiv.org/abs/2402.14207) | 提出了一种名为STORM的写作系统，用于通过检索和多视角提问合成主题概要，以辅助从头开始写类似维基百科的文章。 |
| [^29] | [Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective](https://arxiv.org/abs/2402.10184) | 本研究通过设计奖励建模过程中的数据集信息结构，从图论的视角提出了RLHF中奖励泛化的问题，以解决多样的环境、低成本标注和可靠的对齐性能间的不兼容性。 |
| [^30] | [OrderBkd: Textual backdoor attack through repositioning](https://arxiv.org/abs/2402.07689) | 本论文提出了一种通过重新定位句子中的两个单词实施文本后门攻击的方法，与已有的攻击方式相比，在攻击成功率、困惑度和与干净样本的语义相似性方面表现更好，并且对ONION防御方法具有鲁棒性。 |
| [^31] | [A Survey on Transformer Compression](https://arxiv.org/abs/2402.05964) | 《Transformer压缩调研》是对最近压缩方法的全面回顾，特别关注它们在Transformer模型中的应用。压缩方法主要分为修剪、量化、知识蒸馏和高效架构设计四个类别。 |
| [^32] | [Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving](https://arxiv.org/abs/2402.05359) | 该论文提出了一种以分治程序引导大型语言模型（LLM）的方法，以解决涉及重复子任务和/或具有欺骗性内容的问题。实验证明，该方法可以提高LLM的表达能力。 |
| [^33] | [Zero-Shot Clinical Trial Patient Matching with LLMs](https://arxiv.org/abs/2402.05125) | 本研究基于LLMs开发了一个零样本临床试验患者匹配系统，可以高效评估患者是否符合入选标准，并通过优化提示策略和检索流程提高了数据和成本效率。 |
| [^34] | [Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey](https://arxiv.org/abs/2402.04854) | 该论文提出了一种分层树状知识图谱和推荐系统，帮助初学者研究者进行研究调研，填补了现有导航知识图谱的不足，并解决了学术论文推荐系统中高文本相似性带来的困惑。 |
| [^35] | [VlogQA: Task, Dataset, and Baseline Models for Vietnamese Spoken-Based Machine Reading Comprehension](https://arxiv.org/abs/2402.02655) | 本文介绍了VlogQA：越南口语机器阅读理解任务、数据集和基线模型，并提供了使用真实数据进行任务的挑战和机遇的见解。VlogQA是一个基于来自YouTube的剧本文档的问答对数据集，涵盖了食物和旅行等主题。深度学习模型在测试集取得了75.34%的最高F1分数。 |
| [^36] | [Customizing Language Model Responses with Contrastive In-Context Learning](https://arxiv.org/abs/2401.17390) | 本论文提出了一种使用对比示例来定制语言模型回复的方法，通过提供正面示例和负面示例，使模型学会如何回避负面特征，从而更好地满足用户需求。 |
| [^37] | [LLsM: Generative Linguistic Steganography with Large Language Model](https://arxiv.org/abs/2401.15656) | 本研究提出了LLsM，一种基于大型语言模型的生成式语言隐写术。通过对大规模数据集进行微调，LLM能够以可控的方式生成具有特定话语特征的隐写文本，提高了隐蔽通信的效果。 |
| [^38] | [Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?](https://arxiv.org/abs/2311.09702) | 本研究探讨了大型语言模型存在的幻觉和不忠实推理问题，提出一种新的探测方法和基准测试以研究LLMs在推理过程中是否会采取欺骗性语义快捷方式。 |
| [^39] | [Mind's Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models](https://arxiv.org/abs/2311.09214) | 本研究提出了一种从大型语言模型中提取自我评估能力和综合思维的方法，旨在解决小语言模型继承不完善推理和幻觉的问题。 |
| [^40] | [A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://arxiv.org/abs/2311.08268) | 提出一种利用大型语言模型自动生成有效的越狱提示的自动框架ReNeLLM，显著提高攻击成功率，同时大大减少时间成本。 |
| [^41] | [To Tell The Truth: Language of Deception and Language Models](https://arxiv.org/abs/2311.07092) | 在高风险环境中，研究人员通过分析电视游戏节目数据发现，即使只使用语言线索，基于大型语言模型构建的模型可以与人类主体具有类似的真相检测性能。 |
| [^42] | [Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes](https://arxiv.org/abs/2310.10648) | 通过使用决策模型Bridge，结合专家的认知任务分析，成功利用大型语言模型（LLMs）来弥补新手和专家在纠正数学错误中的知识差距。 |
| [^43] | [Faithful and Robust Local Interpretability for Textual Predictions.](http://arxiv.org/abs/2311.01605) | 提出了一种名为FRED的新颖方法，用于解释文本预测。FRED可以识别文档中的关键词，并且通过与最先进的方法进行的实证评估证明了其在提供对文本模型的深入见解方面的有效性。 |
| [^44] | [SALMONN: Towards Generic Hearing Abilities for Large Language Models.](http://arxiv.org/abs/2310.13289) | 本文介绍了SALMONN，这是一个集成了预训练的大型语言模型和语音/音频编码器的多模态模型，能够实现直接处理和理解普通音频输入的能力，并在多个语音和音频任务上取得竞争性表现。 |
| [^45] | [On the Representational Capacity of Recurrent Neural Language Models.](http://arxiv.org/abs/2310.12942) | 本文研究了基于循环神经网络的语言模型的计算表达性，扩展了图灵完备性结果到概率情况，并提供了上下界分析。 |
| [^46] | [REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models.](http://arxiv.org/abs/2310.12362) | REMARK-LLM是一种针对生成大型语言模型的文本的鲁棒高效的水印框架，通过学习-based消息编码、重新参数化和解码模块以及优化的波束搜索算法来保护生成内容的完整性和防止恶意利用。 |
| [^47] | [QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models.](http://arxiv.org/abs/2310.08041) | QLLM是一种为大规模语言模型设计的准确高效的低位宽后训练量化方法，通过引入自适应通道重组技术，将离群值的大小重新分配给其他通道，从而减轻它们对量化范围的影响。 |
| [^48] | [Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models.](http://arxiv.org/abs/2310.00322) | 本文提出了红队游戏（RTG）框架，利用博弈论分析了红队语言模型（RLM）与蓝队语言模型（BLM）之间的多轮攻防互动。同时引入了游戏化红队求解器（GRTS）来提供自动化的红队技术。 |
| [^49] | [Demystifying CLIP Data.](http://arxiv.org/abs/2309.16671) | CLIP的成功主要归功于其数据而非模型架构或预训练目标。我们通过元数据整理方法引入了MetaCLIP，该方法从原始数据池和元数据中生成一个平衡的子集，提供了更加详细的数据信息。在实验中，我们发现MetaCLIP在处理400M个图像-文本数据对时取得了良好的性能。 |
| [^50] | [Nested Event Extraction upon Pivot Element Recogniton.](http://arxiv.org/abs/2309.12960) | 本文提出了一种名为PerNee的新模型，通过识别中心元素来提取嵌套事件。该模型解决了现有NEE方法无法处理中心元素双重身份的问题，并通过提示学习将事件类型和参数角色的信息纳入其中，以提高NEE性能。 |
| [^51] | [Positive and Risky Message Assessment for Music Products.](http://arxiv.org/abs/2309.10182) | 这项研究提出了一个新的问题：如何评估音乐产品中的正面和风险信息。研究者提出了一个多任务预测模型，通过序数约束解决这个问题，并且取得了显著优于其他方法的结果。 |
| [^52] | [Echotune: A Modular Extractor Leveraging the Variable-Length Nature of Speech in ASR Tasks.](http://arxiv.org/abs/2309.07765) | Echotune是一个模块化特征提取器，利用语音的可变长度特性，通过引入Echo-MSA模块，实现了从帧到话语的各种颗粒度的语音特征提取，解决了固定长度注意力的局限性。 |
| [^53] | [From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning.](http://arxiv.org/abs/2308.12032) | 该论文引入了一种自我引导的方法，让LLM能够自主地选择高质量的指令数据，通过引入指令遵循难度指标（IFD），大幅提高了模型训练效率，并在知名数据集上进行了验证，展示了优于传统数据输入的结果。 |
| [^54] | [Convoifilter: A case study of doing cocktail party speech recognition.](http://arxiv.org/abs/2308.11380) | 本文通过使用单声道语音增强模块与ASR模块，成功将ASR的词错误率从80%降低到26.4%，并通过联合微调策略将其进一步降低到14.5%。 |
| [^55] | [WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine.](http://arxiv.org/abs/2308.05361) | WeaverBird是一个专为金融领域设计的智能对话系统，通过利用大型语言模型、本地知识库和搜索引擎，能够理解复杂的金融查询并提供明智的回答，具有增强的可信度。 |
| [^56] | [Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links.](http://arxiv.org/abs/2308.03929) | 本研究通过构建基于本体的知识图谱，利用疾病本体和症状本体构建数学模型，利用事实核查算法和网络中心度指标分析ChatGPT生成的文本与真实医学文献之间的准确性，以验证疾病-症状关系。 |
| [^57] | [Unveiling Global Narratives: A Multilingual Twitter Dataset of News Media on the Russo-Ukrainian Conflict.](http://arxiv.org/abs/2306.12886) | 研究人员收集了约1.5百万条涵盖60种不同语言的推文，创建了一个多语种推特数据集，重点探讨了俄乌冲突的新闻媒体报道。数据集中的标签可以识别与该话题相关的主体、立场、概念和情感表达。 |
| [^58] | [AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys.](http://arxiv.org/abs/2305.09620) | 本论文研究了利用经过全国代表性调查微调的大语言模型（LLMs）来增强调查的观点预测，取得了在遗漏数据插值和回溯推理方面优秀的成果，在零次预测方面仍需进一步研究。 |
| [^59] | [NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge.](http://arxiv.org/abs/2305.04978) | 本文提出了一种新的用于比较知识提炼的神经符号框架，名称为NeuroComparatives。该框架利用词汇约束解码进行比较知识提炼，以及对生成的知识进行严格过滤。 |
| [^60] | [2x Faster Language Model Pre-training via Masked Structural Growth.](http://arxiv.org/abs/2305.02869) | 本文提出了掩码结构成长（MSG），可以加速语言模型的预训练，其中包括全维度成长进程和独立于新权重初始化的函数严格保留成长操作。 |
| [^61] | [Impact of Position Bias on Language Models in Token Classification.](http://arxiv.org/abs/2304.13567) | 研究了语言模型在token分类任务中的位置偏差问题，通过实验表明在具体任务中，BERT、ERNIE、ELECTRA等编码器以及GPT2和BLOOM等解码器的平均性能下降了3%和9%。 |
| [^62] | [Detecting Out-of-Context Multimodal Misinformation with interpretable neural-symbolic model.](http://arxiv.org/abs/2304.07633) | 本论文提出了一种可解释的神经符号模型，用于检测上下文不符的虚假多模态信息，帮助事实检查网站进行记录澄清。 |
| [^63] | [Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales.](http://arxiv.org/abs/2304.06875) | 提出了一种新的方法muP，可以提高超参数的缩放律的拟合精度，减少对大模型超参数的搜索，从而实现在大规模模型上进行损失预测。 |

# 详细

[^1]: 关于LM潜在空间的语义学：一种以词汇为定义的方法

    On the Semantics of LM Latent Space: A Vocabulary-defined Approach

    [https://rss.arxiv.org/abs/2401.16184](https://rss.arxiv.org/abs/2401.16184)

    本论文介绍了一种以词汇为定义的语义学方法，建立了LM潜在空间的参考框架，确保基于LM词汇的分离语义分析。在LM适应过程中，引入了计算logits的新技术和神经聚类模块，通过实验证明了该方法在文本理解上的优越性能。

    

    理解语言模型(LM)的潜在空间对于改进其性能和可解释性至关重要。现有的分析往往在提供基于模型的对LM语义的分离洞察方面存在不足，并忽视了LM适应的重要方面。为了响应这一问题，我们引入了一种开创性的方法，称为以词汇为定义的语义学，它在LM的潜在空间中建立了一个参考框架，确保基于LM词汇的分离语义分析。我们的方法超越了先前的交织分析，利用LM词汇来获得以模型为中心的洞察。此外，我们提出了一种计算logits的新技术，强调可微分性和局部等距性，并引入了一个神经聚类模块，用于在LM适应过程中进行语义校准。通过在多种文本理解数据集上进行广泛实验，我们的方法在检索增强生成和参数高效微调方面超越了最先进的方法。

    Understanding the latent space of language models (LM) is crucial to refining their performance and interpretability. Existing analyses often fall short in providing disentangled (model-centric) insights into LM semantics, and neglect essential aspects of LM adaption. In response, we introduce a pioneering method called vocabulary-defined semantics, which establishes a reference frame within the LM latent space, ensuring disentangled semantic analysis grounded in LM vocabulary. Our approach transcends prior entangled analysis, leveraging LM vocabulary for model-centric insights. Furthermore, we propose a novel technique to compute logits, emphasising differentiability and local isotropy, and introduce a neural clustering module for semantically calibrating data representations during LM adaptation. Through extensive experiments across diverse text understanding datasets, our approach outperforms state-of-the-art methods of retrieval-augmented generation and parameter-efficient finetuni
    
[^2]: 测量大型语言模型的社会规范

    Measuring Social Norms of Large Language Models

    [https://arxiv.org/abs/2404.02491](https://arxiv.org/abs/2404.02491)

    论文提出了一个挑战，测试大型语言模型对社会规范的理解，构建了一个数据集涵盖广泛的社会规范问题，通过多代理框架基于大型语言模型来提高模型对社会规范的理解能力。

    

    我们提出了一个新的挑战，以检验大型语言模型是否理解社会规范。与现有数据集不同，我们的数据集要求具有解决社会规范的基本理解。我们的数据集包含最大的社会规范技能集，包括402项技能和12,383个问题，涵盖了从观点和论点到文化和法律等广泛的社会规范。我们根据K-12课程设计了我们的数据集。这使得可以直接将大型语言模型的社会理解能力与人类进行比较，更具体地说是与小学生进行比较。尽管先前的工作在我们的基准测试中产生几乎随机的准确度，但最近的大型语言模型（如GPT3.5-Turbo和LLaMA2-Chat）能够显著提高性能，仅略低于人类性能。然后，我们提出了一个基于大型语言模型的多代理框架，以提高模型理解社会规范的能力。

    arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
    
[^3]: 使用ChatGPT对科学文章引用进行情感分析：识别潜在偏见和利益冲突

    Sentiment Analysis of Citations in Scientific Articles Using ChatGPT: Identifying Potential Biases and Conflicts of Interest

    [https://arxiv.org/abs/2404.01800](https://arxiv.org/abs/2404.01800)

    使用ChatGPT进行情感分析揭示了科学文章引用中的偏见和利益冲突，增强了科学文献评估的客观性和可靠性

    

    科学文章在推动知识发展和指导研究方向方面起着至关重要的作用。评估科学文章的关键方面之一是对引文进行分析，这提供了对被引用作品的影响和接受程度的见解。本文介绍了大型语言模型，特别是ChatGPT，在科学文章中对引文进行全面情感分析的创新应用。通过利用先进的自然语言处理（NLP）技术，ChatGPT能够辨别引文的微妙积极或消极情感，提供对被引用作品的接受程度和影响的见解。此外，ChatGPT的能力还包括检测引文中的潜在偏见和利益冲突，增强科学文献评估的客观性和可靠性。这项研究展示了人工智能（AI）驱动工具在增强引文分析和提高科学文献评估方面的变革潜力。

    arXiv:2404.01800v1 Announce Type: cross  Abstract: Scientific articles play a crucial role in advancing knowledge and informing research directions. One key aspect of evaluating scientific articles is the analysis of citations, which provides insights into the impact and reception of the cited works. This article introduces the innovative use of large language models, particularly ChatGPT, for comprehensive sentiment analysis of citations within scientific articles. By leveraging advanced natural language processing (NLP) techniques, ChatGPT can discern the nuanced positivity or negativity of citations, offering insights into the reception and impact of cited works. Furthermore, ChatGPT's capabilities extend to detecting potential biases and conflicts of interest in citations, enhancing the objectivity and reliability of scientific literature evaluation. This study showcases the transformative potential of artificial intelligence (AI)-powered tools in enhancing citation analysis and pr
    
[^4]: 结构化信息很重要：将抽象意义表示引入LLMs以改善开放领域对话评估

    Structured Information Matters: Incorporating Abstract Meaning Representation into LLMs for Improved Open-Domain Dialogue Evaluation

    [https://arxiv.org/abs/2404.01129](https://arxiv.org/abs/2404.01129)

    将抽象意义表示结合到LLMs中，提出了一个简单有效的框架用于改善开放领域对话评估

    

    arXiv:2404.01129v1 公告类型：新的 摘要：自动的开放领域对话评估已经引起越来越多的关注。可训练的评估指标通常是通过训练具有真正正例和随机选择的负例回复来训练的，导致它们倾向于将更高内容相似性的回复分配更高的得分给定一个上下文。然而，对抗性的负面回复具有与上下文高内容相似性，同时在语义上不同。因此，现有的评估指标不足以评估这类回复，导致与人类判断之间的相关性较低。虽然最近的研究已经显示出在利用大型语言模型（LLMs）进行开放领域对话评估方面有一定效果，但它们仍然在有效处理对抗性负面示例方面遇到挑战。在本文中，我们提出了一个简单而有效的框架用于开放领域对话评估，它结合了领域特定的语言模型（SLMs）。

    arXiv:2404.01129v1 Announce Type: new  Abstract: Automatic open-domain dialogue evaluation has attracted increasing attention. Trainable evaluation metrics are commonly trained with true positive and randomly selected negative responses, resulting in a tendency for them to assign a higher score to the responses that share higher content similarity with a given context. However, adversarial negative responses possess high content similarity with the contexts whilst being semantically different. Therefore, existing evaluation metrics are not robust enough to evaluate such responses, resulting in low correlations with human judgments. While recent studies have shown some efficacy in utilizing Large Language Models (LLMs) for open-domain dialogue evaluation, they still encounter challenges in effectively handling adversarial negative examples. In this paper, we propose a simple yet effective framework for open-domain dialogue evaluation, which combines domain-specific language models (SLMs
    
[^5]: 大型语言模型如何促进更好的社会辅助人机交互：简要调研

    How Can Large Language Models Enable Better Socially Assistive Human-Robot Interaction: A Brief Survey

    [https://arxiv.org/abs/2404.00938](https://arxiv.org/abs/2404.00938)

    大型语言模型的最新进展为社会辅助机器人领域带来了潜在的新应用，能够显著扩展其能力，但也带来新的风险和道德关切。

    

    社会辅助机器人（SARs）在为老年人、患有自闭症谱系障碍（ASD）的儿童以及精神健康挑战者等特殊群体提供个性化认知情感支持方面取得了巨大成功。 SAR的大量研究作品展示了其在为在家提供支持方面的潜力，这种支持可以补充由专业心理健康专业人员提供的诊所治疗，使这些干预措施更加有效和可访问。然而，仍然存在一些关键技术挑战，阻碍了SAR介导的交互和干预达到人类水平的社会智能和功效。 随着大型语言模型（LLMs）的最新进展，SAR领域内的新应用潜力有所增加，可以显著扩展SAR的当前能力。 然而，整合LLM会引入新的风险和道德关切

    arXiv:2404.00938v1 Announce Type: cross  Abstract: Socially assistive robots (SARs) have shown great success in providing personalized cognitive-affective support for user populations with special needs such as older adults, children with autism spectrum disorder (ASD), and individuals with mental health challenges. The large body of work on SAR demonstrates its potential to provide at-home support that complements clinic-based interventions delivered by mental health professionals, making these interventions more effective and accessible. However, there are still several major technical challenges that hinder SAR-mediated interactions and interventions from reaching human-level social intelligence and efficacy. With the recent advances in large language models (LLMs), there is an increased potential for novel applications within the field of SAR that can significantly expand the current capabilities of SARs. However, incorporating LLMs introduces new risks and ethical concerns that ha
    
[^6]: 用于斯拉夫语的跨语言命名实体语料库

    Cross-lingual Named Entity Corpus for Slavic Languages

    [https://arxiv.org/abs/2404.00482](https://arxiv.org/abs/2404.00482)

    介绍了一个手动标注的用于六种斯拉夫语言的命名实体语料库，提供了两个训练数据集划分，并使用预训练的多语言模型进行命名实体的识别、分类、引用词化和链接。

    

    本文介绍了一个手动注释的包含六种斯拉夫语言（保加利亚语、捷克语、波兰语、斯洛文尼亚语、俄语和乌克兰语）命名实体的语料库。这项工作是2017-2023年间斯拉夫自然语言处理研讨会的一系列共享任务的结果。该语料库包含了5017份涵盖七个主题的文档，文档标有五类命名实体，每个实体由类别、引用词和唯一跨语言标识符描述。我们提供了两个训练调整的数据集划分 - 单个主题划分和跨主题划分。对于每个划分，我们使用基于transformer的神经网络架构设置了基准，使用预训练的多语言模型XLM-RoBERTa-large进行命名实体提及识别和分类，以及mT5-large进行命名实体引用词化和链接。

    arXiv:2404.00482v1 Announce Type: cross  Abstract: This paper presents a corpus manually annotated with named entities for six Slavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. This work is the result of a series of shared tasks, conducted in 2017-2023 as a part of the Workshops on Slavic Natural Language Processing. The corpus consists of 5 017 documents on seven topics. The documents are annotated with five classes of named entities. Each entity is described by a category, a lemma, and a unique cross-lingual identifier. We provide two train-tune dataset splits - single topic out and cross topics. For each split, we set benchmarks using a transformer-based neural network architecture with the pre-trained multilingual models - XLM-RoBERTa-large for named entity mention recognition and categorization, and mT5-large for named entity lemmatization and linking.
    
[^7]: 想要的设计：利用视觉问答进行多模态预训练

    Design as Desired: Utilizing Visual Question Answering for Multimodal Pre-training

    [https://arxiv.org/abs/2404.00226](https://arxiv.org/abs/2404.00226)

    本研究是首次利用视觉问答（VQA）进行多模态预训练，专注于引导模型学习所需病理特征，并提出了一种无需额外专家注释的问题-答案对设计方法，以及一种准文本特征转换器模块。

    

    多模态预训练在医疗领域展示了其潜力，从成对的医疗报告中学习医学视觉表示。然而，许多预训练任务需要临床医生额外的注释，大多数任务未能明确引导模型学习不同病理特征。据我们所知，我们是第一个利用视觉问答（VQA）进行多模态预训练的团队，以引导框架专注于目标病理特征。在这项工作中，我们利用医疗报告中的描述设计了与不同疾病相关的多粒度问题-答案对，这有助于框架在预训练中无需专家额外的注释。我们还提出了一种新颖的预训练框架，其中包括一种准文本特征转换器模块，旨在通过将视觉特征转换到接近文本领域的准文本空间来辅助预训练过程。

    arXiv:2404.00226v1 Announce Type: cross  Abstract: Multimodal pre-training demonstrates its potential in the medical domain, which learns medical visual representations from paired medical reports. However, many pre-training tasks require extra annotations from clinicians, and most of them fail to explicitly guide the model to learn the desired features of different pathologies. To the best of our knowledge, we are the first to utilize Visual Question Answering (VQA) for multimodal pre-training to guide the framework focusing on targeted pathological features. In this work, we leverage descriptions in medical reports to design multi-granular question-answer pairs associated with different diseases, which assist the framework in pre-training without requiring extra annotations from experts. We also propose a novel pre-training framework with a quasi-textual feature transformer, a module designed to transform visual features into a quasi-textual space closer to the textual domain via a c
    
[^8]: 使用倒置标签的后门方法：脏标签翻转攻击

    A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks

    [https://arxiv.org/abs/2404.00076](https://arxiv.org/abs/2404.00076)

    提出了一种后门攻击方法，名为“DirtyFlipping”，利用脏标签技术在选定的数据模式中输入触发器，从而实现隐蔽的后门。

    

    基于声音的机器学习系统经常使用公共或第三方数据，这可能是不准确的。这使得训练在这些数据上的深度神经网络（DNN）模型容易受到潜在的数据毒化攻击。在这种攻击类型中，攻击者可以使用毒化数据来训练DNN模型，可能会降低其性能。另一种对我们的研究非常相关的数据毒化攻击类型是标签翻转，攻击者在其中操纵数据子集的标签。已经证明，即使是能力有限的攻击者，这些攻击也可能极大地降低系统性能。在本研究中，我们提出了一种名为“DirtyFlipping”的后门攻击，使用脏标签技术，“标签对标签”，在与目标类别相关的选定数据模式中输入触发器（拍手），从而实现了隐蔽的后门。

    arXiv:2404.00076v1 Announce Type: cross  Abstract: Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named 'DirtyFlipping', which uses dirty label techniques, "label-on-label", to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.
    
[^9]: 不同话语标注框架的话语关系自动对齐

    Automatic Alignment of Discourse Relations of Different Discourse Annotation Frameworks

    [https://arxiv.org/abs/2403.20196](https://arxiv.org/abs/2403.20196)

    研究探索不同话语标注框架间的话语关系清单相关性，并提出半自动方法来解决这一问题。

    

    现有的话语语料库基于不同的框架进行标注，在参数和关系的定义以及结构约束方面存在显著差异。尽管表面上存在差异，这些框架分享话语关系的基本理解。这些框架之间的关系一直是一个开放性的研究问题，特别是不同框架中使用的关系清单之间的相关性。更好地理解这个问题有助于整合话语理论，并实现在不同框架下标注的话语语料库的互操作性。然而，研究探索话语关系清单之间的相关性受到了话语划分标准的不同限制，通常需要专家知识和手动检查。一些半自动方法已被提出，但这些方法依赖于同时在多个框架中进行标注的语料库。

    arXiv:2403.20196v1 Announce Type: new  Abstract: Existing discourse corpora are annotated based on different frameworks, which show significant dissimilarities in definitions of arguments and relations and structural constraints. Despite surface differences, these frameworks share basic understandings of discourse relations. The relationship between these frameworks has been an open research question, especially the correlation between relation inventories utilized in different frameworks. Better understanding of this question is helpful for integrating discourse theories and enabling interoperability of discourse corpora annotated under different frameworks. However, studies that explore correlations between discourse relation inventories are hindered by different criteria of discourse segmentation, and expert knowledge and manual examination are typically needed. Some semi-automatic methods have been proposed, but they rely on corpora annotated in multiple frameworks in parallel. In 
    
[^10]: 拒绝提高可靠性：使用强化学习从知识反馈训练LLMs拒绝未知问题

    Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback

    [https://arxiv.org/abs/2403.18349](https://arxiv.org/abs/2403.18349)

    这里是中文总结出的一句话要点: 该论文研究了拒绝机制在提高大型语言模型可靠性中的作用，提出了一种基于知识反馈的强化学习框架RLKF。

    

    大型语言模型（LLMs）经常生成错误输出，被称为幻想，这是由于它们在辨别超出其知识范围的问题时的局限性。虽然解决幻想一直是研究的焦点，以往的努力主要集中在提高正确性而未充分考虑拒绝机制的重要性。本文全面研究了拒绝的作用，引入了模型可靠性的概念以及相应的度量标准。这些度量标准衡量了模型在提供准确响应的同时，灵活拒绝超出其知识边界的问题，从而最小化幻想。为了提高LLMs固有的可靠性，我们提出了一种名为知识反馈强化学习（RLKF）的新对齐框架。

    arXiv:2403.18349v1 Announce Type: new  Abstract: Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope. While addressing hallucination has been a focal point in research, previous efforts primarily concentrate on enhancing correctness without giving due consideration to the significance of rejection mechanisms. In this paper, we conduct a comprehensive examination of the role of rejection, introducing the notion of model reliability along with corresponding metrics. These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby minimizing hallucinations. To improve the inherent reliability of LLMs, we present a novel alignment framework called Reinforcement Learning from Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically determine the model's knowledge boundary and 
    
[^11]: RankMamba，在Transformer时代对Mamba文档排名性能的基准测试

    RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era of Transformers

    [https://arxiv.org/abs/2403.18276](https://arxiv.org/abs/2403.18276)

    Mamba模型基于状态空间模型，在多个序列建模任务中取得了与Transformer相当的性能，并在经典信息检索任务--文档排名中展现了其有效性。

    

    Transformer结构在自然语言处理（NLP）、计算机视觉（CV）和信息检索(IR)等多个应用的机器学习领域取得了巨大成功。Transformer架构的核心机制--注意力，在训练中需要$O(n^2)$的时间复杂度，在推断中需要$O(n)$的时间复杂度。许多工作已经提出改进注意力机制的可扩展性，比如Flash Attention和Multi-query Attention。另一方面的工作旨在设计新的机制来取代注意力。最近，基于状态空间模型的一个显著模型结构--Mamba，在多个序列建模任务中取得了与Transformer相当的性能。

    arXiv:2403.18276v1 Announce Type: cross  Abstract: Transformer structure has achieved great success in multiple applied machine learning communities, such as natural language processing (NLP), computer vision (CV) and information retrieval (IR). Transformer architecture's core mechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$ time complexity in inference. Many works have been proposed to improve the attention mechanism's scalability, such as Flash Attention and Multi-query Attention. A different line of work aims to design new mechanisms to replace attention. Recently, a notable model structure -- Mamba, which is based on state space models, has achieved transformer-equivalent performance in multiple sequence modeling tasks.   In this work, we examine \mamba's efficacy through the lens of a classical IR task -- document ranking. A reranker model takes a query and a document as input, and predicts a scalar relevance score. This task demands the language mod
    
[^12]: MasonTigers在SemEval-2024任务1中的集成方法研究：语义文本相关性

    MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness

    [https://arxiv.org/abs/2403.14990](https://arxiv.org/abs/2403.14990)

    MasonTigers在SemEval-2024 Task 1中采用集成方法，结合语言特定的BERT模型和句子变换器，在处理语义文本相关性时取得了优异的结果。

    

    本文介绍了MasonTigers参与SemEval-2024任务1-语义文本相关性的工作。该任务涵盖了涵盖了监督（Track A）、无监督（Track B）和跨语言（Track C）方法，涉及14种不同语言。MasonTigers是少数同时参与了三个track中所有语言的两支团队之一。我们的方法在Track A中排名从第11到第21，在Track B中排名从第1到第8，在Track C中排名从第5到第12。在遵循特定任务约束的同时，我们的表现最好的方法利用了统计机器学习方法的集成，结合了基于语言的BERT模型和句子变换器。

    arXiv:2403.14990v1 Announce Type: new  Abstract: This paper presents the MasonTigers entry to the SemEval-2024 Task 1 - Semantic Textual Relatedness. The task encompasses supervised (Track A), unsupervised (Track B), and cross-lingual (Track C) approaches across 14 different languages. MasonTigers stands out as one of the two teams who participated in all languages across the three tracks. Our approaches achieved rankings ranging from 11th to 21st in Track A, from 1st to 8th in Track B, and from 5th to 12th in Track C. Adhering to the task-specific constraints, our best performing approaches utilize ensemble of statistical machine learning approaches combined with language-specific BERT based models and sentence transformers.
    
[^13]: 通过知识蒸馏教授大型语言模型解释多模态虚假信息

    MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation

    [https://arxiv.org/abs/2403.14171](https://arxiv.org/abs/2403.14171)

    提出了MMIDR框架，用于教导大型语言模型提供解释其多模态虚假信息决策过程的文本解释。

    

    最近，多模态虚假信息的自动检测引起了广泛关注。然而，强大的大型语言模型（LLMs）在多模态虚假信息检测方面的潜力仍未得到充分发掘。此外，如何以成本效益和易于访问的方式教导LLMs解释多模态虚假信息仍然是一个悬而未决的问题。为了解决这个问题，我们提出了MMIDR，这是一个旨在教导LLMs为其多模态虚假信息决策过程提供流畅和高质量文本解释的框架。为了将多模态虚假信息转化为适当的指令执行格式，我们提出了一个数据增强视角和管道。该管道包括一个视觉信息处理模块和一个证据检索模块。随后，我们使用处理过的内容提示专有的LLMs为解释多模态虚假信息的真实性提取原因。

    arXiv:2403.14171v1 Announce Type: new  Abstract: Automatic detection of multimodal misinformation has gained a widespread attention recently. However, the potential of powerful Large Language Models (LLMs) for multimodal misinformation detection remains underexplored. Besides, how to teach LLMs to interpret multimodal misinformation in cost-effective and accessible way is still an open question. To address that, we propose MMIDR, a framework designed to teach LLMs in providing fluent and high-quality textual explanations for their decision-making process of multimodal misinformation. To convert multimodal misinformation into an appropriate instruction-following format, we present a data augmentation perspective and pipeline. This pipeline consists of a visual information processing module and an evidence retrieval module. Subsequently, we prompt the proprietary LLMs with processed contents to extract rationales for interpreting the authenticity of multimodal misinformation. Furthermore
    
[^14]: 自然语言作为政策：与LLMs一起进行坐标级体态控制的推理

    Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs

    [https://arxiv.org/abs/2403.13801](https://arxiv.org/abs/2403.13801)

    用自然语言推理进行坐标级控制，能够显著提高机器人行动规划的成功率，并且具有将机器人技能迁移到新任务的潜力。

    

    我们展示了与LLMs一起解决机器人行动规划问题的实验结果。最近，LLMs已经应用于机器人行动规划，特别是使用代码生成方法将复杂的高级指令转换为中级策略代码。相比之下，我们的方法通过获取任务和场景对象的文本描述，通过自然语言推理制定行动规划，并输出坐标级控制命令，从而减少了作为政策的中间表示代码的必要性。我们的方法在多模态提示仿真基准上进行了评估，表明我们的自然语言推理实验显著提高了成功率，与缺席相比。此外，我们的方法展示了自然语言描述有潜力将机器人技能从已知任务转移到以前未见任务。

    arXiv:2403.13801v1 Announce Type: cross  Abstract: We demonstrate experimental results with LLMs that address robotics action planning problems. Recently, LLMs have been applied in robotics action planning, particularly using a code generation approach that converts complex high-level instructions into mid-level policy codes. In contrast, our approach acquires text descriptions of the task and scene objects, then formulates action planning through natural language reasoning, and outputs coordinate level control commands, thus reducing the necessity for intermediate representation code as policies. Our approach is evaluated on a multi-modal prompt simulation benchmark, demonstrating that our prompt engineering experiments with natural language reasoning significantly enhance success rates compared to its absence. Furthermore, our approach illustrates the potential for natural language descriptions to transfer robotics skills from known tasks to previously unseen tasks.
    
[^15]: 使用大型语言模型纠正社交媒体上的错误信息

    Correcting misinformation on social media with a large language model

    [https://arxiv.org/abs/2403.11169](https://arxiv.org/abs/2403.11169)

    提出了一种名为MUSE的大型语言模型，通过访问最新信息并评估可信度，以解决社交媒体上误信息纠正的难题。

    

    误信息会破坏公众对科学和民主的信任，特别是在社交媒体上，不准确信息会迅速传播。专家和普通人通过手动识别和解释不准确信息已经被证明是有效的纠正误信息的方法。然而，这种方法很难扩展，这是一个担忧，因为大型语言模型（LLMs）等技术使误信息更容易生成。LLMs还具有多功能能力，可以加速纠正误信息；然而，它们由于缺乏最新信息、倾向于生成似是而非的内容和引用以及无法处理多模态信息而面临困难。为了解决这些问题，我们提出了MUSE，这是一个带有最新信息访问和可信度评估的LLM。通过检索上下文证据和反驳，MUSE可以提供准确可信的解释和参考。它还描述

    arXiv:2403.11169v1 Announce Type: cross  Abstract: Misinformation undermines public trust in science and democracy, particularly on social media where inaccuracies can spread rapidly. Experts and laypeople have shown to be effective in correcting misinformation by manually identifying and explaining inaccuracies. Nevertheless, this approach is difficult to scale, a concern as technologies like large language models (LLMs) make misinformation easier to produce. LLMs also have versatile capabilities that could accelerate misinformation correction; however, they struggle due to a lack of recent information, a tendency to produce plausible but false content and references, and limitations in addressing multimodal information. To address these issues, we propose MUSE, an LLM augmented with access to and credibility evaluation of up-to-date information. By retrieving contextual evidence and refutations, MUSE can provide accurate and trustworthy explanations and references. It also describes 
    
[^16]: 从临床文本中提取生物医学概念预测患者的再入院情况

    Prediction of readmission of patients by extracting biomedical concepts from clinical texts

    [https://arxiv.org/abs/2403.09722](https://arxiv.org/abs/2403.09722)

    从临床文本中提取生物医学概念预测患者的再入院情况，可以帮助医生选择适当的治疗方法，从而减少患者再次入院的比率，实现有效的治疗成本降低。

    

    如今，存在大量的电子健康数据为进行旨在改善为患者提供的医疗服务并降低医疗系统成本的研究创造了潜在能力。近年来医学领域备受关注的一个话题是识别出刚从医院出院后可能很快再次入院的患者。这种识别可以帮助医生选择适当的治疗方法，从而减少患者再次入院的比率，实现有效的治疗成本降低。本研究讨论了利用文本挖掘方法和对患者电子文件中的出院报告文本进行处理来预测患者再次入院情况。为此，使用两种方法评估了各种机器学习模型的性能：词袋模型和概念袋模型。

    arXiv:2403.09722v1 Announce Type: cross  Abstract: Today, the existence of a vast amount of electronic health data has created potential capacities for conducting studies aiming to improve the medical services provided to patients and reduce the costs of the healthcare system. One of the topics that has been receiving attention in the field of medicine in recent years is the identification of patients who are likely to be re-hospitalized shortly after being discharged from the hospital. This identification can help doctors choose appropriate treatment methods, thereby reducing the rate of patient re-hospitalization and resulting in effective treatment cost reduction. In this study, the prediction of patient re-hospitalization using text mining approaches and the processing of discharge report texts in the patient's electronic file has been discussed. To this end, the performance of various machine learning models has been evaluated using two approaches: bag of word and bag of concept, 
    
[^17]: 利用典型表示减轻社会偏见而不使用人口统计信息

    Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information

    [https://arxiv.org/abs/2403.09516](https://arxiv.org/abs/2403.09516)

    通过利用预定义的典型人口统计文本并在微调过程中加入正则化项，本文提出的方法有效减轻了语言模型中的社会偏见，同时不需要依赖显式的人口统计标签。

    

    减轻社会偏见通常需要识别与每个数据样本相关联的社会群体。在本文中，我们提出了DAFair，一种新颖的方法来解决语言模型中的社会偏见问题。与依赖显式人口统计标签的传统方法不同，我们的方法不需要任何此类信息。相反，我们利用预定义的人口统计典型文本，并在微调过程中加入一个正则化项来减轻模型表示中的偏见。我们在两个任务和两个模型上的实证结果展示了我们的方法相对于之前不依赖标记数据的方法的有效性。此外，即使使用有限的人口统计标注数据，我们的方法也优于常见的去偏见方法。

    arXiv:2403.09516v1 Announce Type: new  Abstract: Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DAFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model's representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.
    
[^18]: Keyformer：通过关键标记选择减少KV缓存以实现高效的生成推断

    Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference

    [https://arxiv.org/abs/2403.09054](https://arxiv.org/abs/2403.09054)

    本文提出了一种名为“Keyformer”的创新推断时间方法，旨在通过选择关键标记来减少KV缓存的挑战，提高内存带宽利用率。

    

    Transformer已经成为大型语言模型(LLMs)的基础架构。在生成语言模型中，推断过程涉及两个主要阶段：提示处理和标记生成。标记生成，构成了大部分计算工作量，主要涉及向量-矩阵乘法和与键-值(KV)缓存交互。由于从存储系统传输权重和KV缓存值到计算单元的开销，这一阶段受到内存带宽的限制。这种内存瓶颈在需要长上下文和大量文本生成的应用中尤为突出，这两者对LLMs越来越重要。  本文介绍了一种创新的推断时间方法“Keyformer”，以缓解与KV缓存大小和内存带宽利用相关的挑战。Keyformer利用了这样的观察结果，大约90

    arXiv:2403.09054v1 Announce Type: cross  Abstract: Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.   This paper introduces "Keyformer", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90
    
[^19]: 通过代码探索大型语言模型的安全泛化挑战

    Exploring Safety Generalization Challenges of Large Language Models via Code

    [https://arxiv.org/abs/2403.07865](https://arxiv.org/abs/2403.07865)

    本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。

    

    大型语言模型（LLMs）的快速发展带来了自然语言处理方面的显著能力，但也引发了人们对它们潜在误用的担忧。本文引入了CodeAttack，一个将自然语言输入转换为代码输入的框架，为测试LLMs的安全泛化提供了一个新颖的环境。我们对包括GPT-4、Claude-2和Llama-2系列在内的最新LLMs进行了全面研究，发现这些模型对于代码输入存在共同的安全漏洞：CodeAttack在超过80%的时间内始终绕过所有模型的安全保护。

    arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
    
[^20]: PEEB：具有可解释和可编辑语言瓶颈的基于部分的图像分类器

    PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck

    [https://arxiv.org/abs/2403.05297](https://arxiv.org/abs/2403.05297)

    PEEB是一种基于部分的图像分类器，通过将类别名称转换为描述视觉部分的文本描述符，并将检测到的部分的嵌入与文本描述符匹配，从而在零样本设置中表现出色，并且不仅在监督学习中表现出色，而且还首次实现用户编辑类定义形成新分类器无需重新训练。

    

    基于CLIP的分类器依赖于包含{text encoder已知的类名称}的提示。也就是说，CLIP在新类别或其名称很少在互联网上出现的类别（例如鸟类的学名）上表现不佳。针对细粒度分类，我们提出了PEEB - 一种可解释和可编辑的分类器，用于（1）将类别名称表达为一组预定义的描述视觉部分的文本描述符；和（2）将检测到的部分的嵌入与每个类别中的文本描述符进行匹配，以计算用于分类的逻辑分数。在一个零样本设置中，其中类别名称是未知的，PEEB在准确性上大幅优于CLIP（约为10倍）。与基于部分的分类器相比，PEEB不仅在监督学习设置上是最先进的（88.80%准确率），而且还是第一个能够让用户编辑类定义以形成新的分类器而无需重新训练的分类器。

    arXiv:2403.05297v1 Announce Type: cross  Abstract: CLIP-based classifiers rely on the prompt containing a {class name} that is known to the text encoder. That is, CLIP performs poorly on new classes or the classes whose names rarely appear on the Internet (e.g., scientific names of birds). For fine-grained classification, we propose PEEB - an explainable and editable classifier to (1) express the class name into a set of pre-defined text descriptors that describe the visual parts of that class; and (2) match the embeddings of the detected parts to their textual descriptors in each class to compute a logit score for classification. In a zero-shot setting where the class names are unknown, PEEB outperforms CLIP by a large margin (~10x in accuracy). Compared to part-based classifiers, PEEB is not only the state-of-the-art on the supervised-learning setting (88.80% accuracy) but also the first to enable users to edit the class definitions to form a new classifier without retraining. Compar
    
[^21]: 通过混合和完善过去来不断演进记忆

    Ever-Evolving Memory by Blending and Refining the Past

    [https://arxiv.org/abs/2403.04787](https://arxiv.org/abs/2403.04787)

    提出了一种新颖的长期对话记忆方案CREEM，通过混合过去记忆并引入完善过程来实现聊天机器人回应的整体改进和连贯性。

    

    对于类似人类的聊天机器人，构建长期记忆至关重要。构建记忆的一个天真方法可能只是列出总结的对话。然而，当说话者的状态随时间变化时，这样做可能会导致问题，并积累矛盾信息。记忆保持有组织对于降低回应生成器的混乱很重要。在本文中，我们提出了一种新颖的长期对话记忆方案，CREEM。与仅基于当前对话构建记忆的现有方法不同，我们提出的模型在记忆形成过程中混合过去的记忆。此外，我们引入了完善过程来处理多余或过时信息。这种创新性方法通过确保一个更加知情和动态演变的长期记忆，旨在提高聊天机器人回应的整体改进和连贯性。

    arXiv:2403.04787v1 Announce Type: cross  Abstract: For a human-like chatbot, constructing a long-term memory is crucial. A naive approach for making a memory could be simply listing the summarized dialogue. However, this can lead to problems when the speaker's status change over time and contradictory information gets accumulated. It is important that the memory stays organized to lower the confusion for the response generator. In this paper, we propose a novel memory scheme for long-term conversation, CREEM. Unlike existing approaches that construct memory based solely on current sessions, our proposed model blending past memories during memory formation. Additionally, we introduce refining process to handle redundant or outdated information. This innovative approach seeks for overall improvement and coherence of chatbot responses by ensuring a more informed and dynamically evolving long-term memory.
    
[^22]: IRCoder: 中间表示使语言模型成为稳健的多语言代码生成器

    IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators

    [https://arxiv.org/abs/2403.03894](https://arxiv.org/abs/2403.03894)

    通过利用编译器中间表示来改进代码-LMs的多语言能力和促进跨语言转移。

    

    arXiv:2403.03894v1 公告类型: 新的 摘要: 代码理解和生成已迅速成为语言模型（LMs）最受欢迎的应用之一。然而，与自然语言LM的研究相比，对代码-LMs（即用于代码生成的LMs）的多语言方面的研究，如不同编程语言之间的跨语言转移，特定于语言的数据增强以及事后LM调整，以及利用原始文本内容之外的数据源，要稀少得多。特别是，大多数主流代码-LMs仅在源代码文件上进行了预训练。在这项工作中，我们研究了利用现成的编译器中间表示（跨编程语言共享）来改进代码-LMs的多语言能力并促进跨语言转移的前景。为此，我们首先编制了SLTrans，一个由近400万个自包含源代码文件组成的并行数据集。

    arXiv:2403.03894v1 Announce Type: new  Abstract: Code understanding and generation have fast become some of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. In particular, most mainstream Code-LMs have been pre-trained on source code files alone. In this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer.   To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source code files coupled wi
    
[^23]: FaaF：作为RAG系统评估的事实函数

    FaaF: Facts as a Function for the evaluation of RAG systems

    [https://arxiv.org/abs/2403.03888](https://arxiv.org/abs/2403.03888)

    FaaF是一种新的事实验证方法，利用语言模型的函数调用能力和面向RAG事实回忆评估的框架，显着提高了LM识别不支持事实的能力，并在效率和成本方面取得了明显的改进。

    

    从参考资料中准确提取事实对于评估检索增强生成（RAG）系统的性能至关重要，因为它直接探查了检索和生成的质量。然而，可靠高效地执行这种评估仍然是一个挑战。最近的工作侧重于通过提示语言模型（LM）评估器进行事实验证，然而我们证明，在信息不完整或不准确的情况下，这些方法是不可靠的。我们引入了FaaF（Facts as a Function），这是一种利用LM的函数调用能力和面向RAG事实回忆评估的框架的新方法。与基于提示的方法相比，FaaF显着提高了LM识别文本中不支持事实的能力，同时提高了效率，降低了成本几倍。

    arXiv:2403.03888v1 Announce Type: new  Abstract: Factual recall from a reference source is crucial for evaluating the performance of Retrieval Augmented Generation (RAG) systems, as it directly probes into the quality of both retrieval and generation. However, it still remains a challenge to perform this evaluation reliably and efficiently. Recent work has focused on fact verification via prompting language model (LM) evaluators, however we demonstrate that these methods are unreliable in the presence of incomplete or inaccurate information. We introduce Facts as a Function (FaaF), a new approach to fact verification that utilizes the function calling abilities of LMs and a framework for RAG factual recall evaluation. FaaF substantially improves the ability of LMs to identify unsupported facts in text with incomplete information whilst improving efficiency and lowering cost by several times, compared to prompt-based approaches.
    
[^24]: 通用到专业的电子商务LLMs翻译

    General2Specialized LLMs Translation for E-commerce

    [https://arxiv.org/abs/2403.03689](https://arxiv.org/abs/2403.03689)

    提出了一个名为G2ST的两步微调范式，通过自对比语义增强将通用NMT模型转换为专门用于电子商务的NMT模型，以提高翻译质量。

    

    现有的神经机器翻译（NMT）模型主要处理通用领域的翻译，忽略了具有特殊写作公式的领域，比如电子商务和法律文件。以电子商务为例，文本通常包含大量领域相关词汇，并且存在更多的语法问题，这导致当前NMT方法的性能较差。为解决这些问题，我们收集了两个与领域相关的资源，包括一组术语对（对齐的中英双语术语）和一个针对电子商务领域进行注释的平行语料库。此外，我们提出了一个两步微调范式（名为G2ST），其中包括自对比语义增强，以将一个通用NMT模型转换为专门用于电子商务的NMT模型。该范式适用于基于大型语言模型（LLMs）的NMT模型。对真实电子商务标题的广泛评估表明了卓越的翻译质量。

    arXiv:2403.03689v1 Announce Type: cross  Abstract: Existing Neural Machine Translation (NMT) models mainly handle translation in the general domain, while overlooking domains with special writing formulas, such as e-commerce and legal documents. Taking e-commerce as an example, the texts usually include amounts of domain-related words and have more grammar problems, which leads to inferior performances of current NMT methods. To address these problems, we collect two domain-related resources, including a set of term pairs (aligned Chinese-English bilingual terms) and a parallel corpus annotated for the e-commerce domain. Furthermore, we propose a two-step fine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to transfer one general NMT model to the specialized NMT model for e-commerce. The paradigm can be used for the NMT models based on Large language models (LLMs). Extensive evaluations on real e-commerce titles demonstrate the superior translation quality and 
    
[^25]: 跨领域的中文句式结构解析

    Cross-domain Chinese Sentence Pattern Parsing

    [https://arxiv.org/abs/2402.16311](https://arxiv.org/abs/2402.16311)

    本文提出了一种利用大型语言模型进行自我训练的创新方法，通过动态生成训练数据将源领域句法规则与目标领域句子相结合，增强句式结构解析器对各种领域的适应能力，实验证明其在教科书和新闻领域的效果优于基于规则的基准模型1.68个百分点。

    

    arXiv:2402.16311v1 公告类型: 跨领域 句式结构（SPS）解析是一种主要用于语言教学的句法分析方法。现有的SPS解析器主要依赖于教科书语料库进行训练，缺乏跨领域能力。为了克服这一限制，本文提出了一种创新方法，利用大型语言模型（LLMs）在自我训练框架内。从源领域中提取部分句法规则，与目标领域句子结合动态生成训练数据，增强了解析器对不同领域的适应能力。在教科书和新闻领域进行的实验表明，所提出的方法效果显著，F1指标比基于规则的基准模型高出1.68个百分点。

    arXiv:2402.16311v1 Announce Type: cross  Abstract: Sentence Pattern Structure (SPS) parsing is a syntactic analysis method primarily employed in language teaching.Existing SPS parsers rely heavily on textbook corpora for training, lacking cross-domain capability.To overcome this constraint, this paper proposes an innovative approach leveraging large language models (LLMs) within a self-training framework. Partial syntactic rules from a source domain are combined with target domain sentences to dynamically generate training data, enhancing the adaptability of the parser to diverse domains.Experiments conducted on textbook and news domains demonstrate the effectiveness of the proposed method, outperforming rule-based baselines by 1.68 points on F1 metrics.
    
[^26]: 通过Few-shot Learning和SBERT Fine-tuning进行牙科严重性评估

    Dental Severity Assessment through Few-shot Learning and SBERT Fine-tuning

    [https://arxiv.org/abs/2402.15755](https://arxiv.org/abs/2402.15755)

    通过Few-shot Learning和SBERT Fine-tuning方法，研究发现该方法在口腔健康问题的严重性评估中表现优异，准确率高达94.1%。

    

    牙科疾病严重影响着相当一部分人口，导致各种健康问题，这可能会对个人的整体幸福产生不利影响。将自动化系统整合到口腔保健中变得越来越重要。机器学习方法为解决诊断困难、效率低下和口腔疾病诊断中的错误等挑战提供了可行的解决方案。当医生们难以预测或诊断疾病的早期阶段时，这些方法尤其有用。本研究利用十三种不同的机器学习、深度学习和大型语言模型根据放射科医生的报告来确定口腔健康问题的严重程度。结果显示，Few-shot learning结合SBERT和多层感知器模型在各种实验中优于所有其他模型，达到94.1%的令人印象深刻的准确率。

    arXiv:2402.15755v1 Announce Type: new  Abstract: Dental diseases have a significant impact on a considerable portion of the population, leading to various health issues that can detrimentally affect individuals' overall well-being. The integration of automated systems in oral healthcare has become increasingly crucial. Machine learning approaches offer a viable solution to address challenges such as diagnostic difficulties, inefficiencies, and errors in oral disease diagnosis. These methods prove particularly useful when physicians struggle to predict or diagnose diseases at their early stages. In this study, thirteen different machine learning, deep learning, and large language models were employed to determine the severity level of oral health issues based on radiologists' reports. The results revealed that the Few-shot learning with SBERT and Multi-Layer Perceptron model outperformed all other models across various experiments, achieving an impressive accuracy of 94.1% as the best r
    
[^27]: IEPile: 挖掘大规模基于模式的信息抽取语料库

    IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus

    [https://arxiv.org/abs/2402.14710](https://arxiv.org/abs/2402.14710)

    发布了IEPile，一个包含约0.32B个标记的综合双语IE指令语料库，通过收集和清理33个现有IE数据集并引入基于模式的指令生成，可以提高大型语言模型在信息抽取领域的性能，尤其是零样本泛化。

    

    大型语言模型（LLMs）在各个领域展现出了显著的潜力；然而，在信息抽取（IE）方面表现出了显著的性能差距。高质量的指令数据是提升LLMs特定能力的关键，而当前的IE数据集往往规模较小、分散且缺乏标准化的模式。因此，我们介绍了IEPile，一个综合的双语（英文和中文）IE指令语料库，包含约0.32B个标记。我们通过收集和清理33个现有IE数据集构建IEPile，并引入基于模式的指令生成来挖掘大规模语料库。在LLaMA和Baichuan上的实验结果表明，使用IEPile可以提高LLMs在IE方面的性能，尤其是零样本泛化。我们开源了资源和预训练模型，希望为自然语言处理社区提供有价值的支持。

    arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
    
[^28]: 用大型语言模型从头开始辅助撰写类似维基百科文章

    Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models

    [https://arxiv.org/abs/2402.14207](https://arxiv.org/abs/2402.14207)

    提出了一种名为STORM的写作系统，用于通过检索和多视角提问合成主题概要，以辅助从头开始写类似维基百科的文章。

    

    我们研究如何应用大型语言模型从头开始撰写基于事实和有条理的长篇文章，使其在广度和深度上与维基百科页面可媲美。这一尚未深入研究的问题在撰写前阶段提出了新的挑战，包括如何研究主题并准备大纲以便撰写。我们提出了STORM，一个用于通过检索和多视角提问进行主题概要合成的写作系统。STORM模拟了撰写前阶段，其中（1）发现研究给定主题的多样化观点，（2）模拟会话，撰写持有不同观点的作者向基于可信互联网来源的主题专家提问，（3）整理收集到的信息以创建大纲。为了评估，我们整理了FreshWiki，一个包含最新高质量维基百科文章的数据集，并制定了大纲评估指标以评估撰写前阶段。

    arXiv:2402.14207v1 Announce Type: cross  Abstract: We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline.   For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from 
    
[^29]: 重塑RLHF中的信息结构：基于图论的奖励泛化视角

    Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective

    [https://arxiv.org/abs/2402.10184](https://arxiv.org/abs/2402.10184)

    本研究通过设计奖励建模过程中的数据集信息结构，从图论的视角提出了RLHF中奖励泛化的问题，以解决多样的环境、低成本标注和可靠的对齐性能间的不兼容性。

    

    在强化学习从人类反馈中（RLHF）存在一个三难问题：高度多样的环境、低标注成本和可靠的对齐性能之间的不兼容性。本文旨在通过设计奖励建模过程中的数据集信息结构来缓解这种不兼容性。具体而言，我们重新审视了RLHF过程，并提出了一个理论框架将其描绘为文本分布上的自动编码过程。我们的框架形式化了RLHF目标，即确保人类偏好与大型语言模型（LLM）行为之间的分布一致性。基于这个框架，我们系统地研究了RLHF奖励建模阶段中信息结构的性能影响。为了进一步理解奖励建模阶段中的奖励泛化，我们引入了一种基于随机图论的方法来建模语义空间中的泛化。其中的关键见解是...

    arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
    
[^30]: OrderBkd: 通过重新定位进行的文本后门攻击

    OrderBkd: Textual backdoor attack through repositioning

    [https://arxiv.org/abs/2402.07689](https://arxiv.org/abs/2402.07689)

    本论文提出了一种通过重新定位句子中的两个单词实施文本后门攻击的方法，与已有的攻击方式相比，在攻击成功率、困惑度和与干净样本的语义相似性方面表现更好，并且对ONION防御方法具有鲁棒性。

    

    使用第三方数据集和预训练的机器学习模型对NLP系统构成威胁，可能隐藏后门攻击。现有的攻击方式包括插入标记或句子重述等污染数据样本，这要么改变了原始文本的语义，要么可以被检测出来。我们与以往工作的主要区别在于，我们使用重新定位句子中的两个单词作为触发器。通过设计并应用基于词性的规则来选择这些标记，我们在SST-2和AG分类数据集上保持了高攻击成功率，同时在困惑度和与干净样本的语义相似性方面优于现有攻击方法。此外，我们展示了我们的攻击对ONION防御方法的鲁棒性。论文中的所有代码和数据可在https://github.com/alekseevskaia/OrderBkd获取。

    The use of third-party datasets and pre-trained machine learning models poses a threat to NLP systems due to possibility of hidden backdoor attacks. Existing attacks involve poisoning the data samples such as insertion of tokens or sentence paraphrasing, which either alter the semantics of the original texts or can be detected. Our main difference from the previous work is that we use the reposition of a two words in a sentence as a trigger. By designing and applying specific part-of-speech (POS) based rules for selecting these tokens, we maintain high attack success rate on SST-2 and AG classification datasets while outperforming existing attacks in terms of perplexity and semantic similarity to the clean samples. In addition, we show the robustness of our attack to the ONION defense method. All the code and data for the paper can be obtained at https://github.com/alekseevskaia/OrderBkd.
    
[^31]: 《Transformer压缩调研》

    A Survey on Transformer Compression

    [https://arxiv.org/abs/2402.05964](https://arxiv.org/abs/2402.05964)

    《Transformer压缩调研》是对最近压缩方法的全面回顾，特别关注它们在Transformer模型中的应用。压缩方法主要分为修剪、量化、知识蒸馏和高效架构设计四个类别。

    

    基于Transformer架构的大型模型在人工智能领域，特别是自然语言处理（NLP）和计算机视觉（CV）领域中扮演着日益重要的角色。模型压缩方法可以减少模型的内存和计算成本，是在实际设备上实现Transformer模型的必要步骤。鉴于Transformer的独特架构，具有交替的注意力和前馈神经网络（FFN）模块，需要特定的压缩技术。这些压缩方法的效率也至关重要，因为重新训练整个训练数据集上的大型模型往往是不切实际的。本调研提供了对最近压缩方法的全面回顾，特别关注它们在Transformer模型中的应用。压缩方法主要分为修剪、量化、知识蒸馏和高效架构设计四个类别。在每个类别中，我们讨论了压缩方法

    Large models based on the Transformer architecture play increasingly vital roles in artificial intelligence, particularly within the realms of natural language processing (NLP) and computer vision (CV). Model compression methods reduce their memory and computational cost, which is a necessary step to implement the transformer models on practical devices. Given the unique architecture of transformer, featuring alternative attention and Feedforward Neural Network (FFN) modules, specific compression techniques are required. The efficiency of these compression methods is also paramount, as it is usually impractical to retrain large models on the entire training dataset.This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to transformer models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design. In each category, we discuss compression methods
    
[^32]: 利用分治程序指导大型语言模型对问题求解进行引导

    Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving

    [https://arxiv.org/abs/2402.05359](https://arxiv.org/abs/2402.05359)

    该论文提出了一种以分治程序引导大型语言模型（LLM）的方法，以解决涉及重复子任务和/或具有欺骗性内容的问题。实验证明，该方法可以提高LLM的表达能力。

    

    基础模型，如大型语言模型（LLMs），因其广泛的应用而引起了广泛的关注。现有的研究表明，适当的提示设计，如思维链，可以释放LLM在不同领域的强大能力。然而，对于处理涉及重复子任务和/或具有欺骗性内容的任务（如算术计算和文章级虚假新闻检测），现有的提示策略要么表现出表达能力不足，要么由幻觉引发中间错误。为了使LLM对这些中间错误更具辨别力，我们提出了一种以分治程序引导LLM的方法，同时确保优越的表达能力和任务分解、子任务解决和解决组装过程的分离。理论分析表明，我们的策略可以引导LLM扩展固定深度Transformer的表达能力。实验表明，我们提出的方法可以实现

    Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
    
[^33]: 零样本临床试验患者匹配与LLMs

    Zero-Shot Clinical Trial Patient Matching with LLMs

    [https://arxiv.org/abs/2402.05125](https://arxiv.org/abs/2402.05125)

    本研究基于LLMs开发了一个零样本临床试验患者匹配系统，可以高效评估患者是否符合入选标准，并通过优化提示策略和检索流程提高了数据和成本效率。

    

    将患者与临床试验匹配是推出新药的关键难题。目前，识别符合试验入选标准的患者是高度手动的，每位患者需花费长达1小时。然而，自动筛选具有挑战性，因为它需要理解非结构化的临床文本。大型语言模型（LLMs）提供了一个有望的解决方案。在这项工作中，我们探索了它们在试验匹配中的应用。首先，我们设计了一个基于LLM的系统，可以在给定一个患者的病史作为非结构化的临床文本时，评估该患者是否符合一组包含标准（也以自由文本形式指定）。我们的零样本系统在n2c2 2018队列选择基准测试中取得了最先进的得分。其次，我们通过识别一种提示策略，改善了我们方法的数据和成本效率，该策略与现状相比可以将患者匹配时间和成本降低一个数量级，并且开发了一个两阶段的检索流程，减少了匹配消除的次数。

    Matching patients to clinical trials is a key unsolved challenge in bringing new drugs to market. Today, identifying patients who meet a trial's eligibility criteria is highly manual, taking up to 1 hour per patient. Automated screening is challenging, however, as it requires understanding unstructured clinical text. Large language models (LLMs) offer a promising solution. In this work, we explore their application to trial matching. First, we design an LLM-based system which, given a patient's medical history as unstructured clinical text, evaluates whether that patient meets a set of inclusion criteria (also specified as free text). Our zero-shot system achieves state-of-the-art scores on the n2c2 2018 cohort selection benchmark. Second, we improve the data and cost efficiency of our method by identifying a prompting strategy which matches patients an order of magnitude faster and more cheaply than the status quo, and develop a two-stage retrieval pipeline that reduces the number of 
    
[^34]: 分层树状知识图谱用于学术调研

    Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey

    [https://arxiv.org/abs/2402.04854](https://arxiv.org/abs/2402.04854)

    该论文提出了一种分层树状知识图谱和推荐系统，帮助初学者研究者进行研究调研，填补了现有导航知识图谱的不足，并解决了学术论文推荐系统中高文本相似性带来的困惑。

    

    对于缺乏研究培训的初学者研究者来说，研究调查一直是一个挑战。这些研究者在短时间内很难理解他们研究主题内的方向，以及发现新的研究发现。为初学者研究者提供直观的帮助的一种方式是提供相关的知识图谱(KG)并推荐相关的学术论文。然而，现有的导航知识图谱主要依赖于研究领域的关键字，常常无法清楚地呈现多个相关论文之间的逻辑层次关系。此外，大多数学术论文推荐系统仅仅依赖于高文本相似性，这可能会让研究人员困惑为什么推荐了特定的文章。他们可能缺乏了解关于他们希望获得的"问题解决"和"问题发现"之间的见解连接的重要信息。为解决这些问题，本研究旨在支持初学者研究者进行研究调研。

    Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between "Issue resolved" and "Issue finding" that they hope to obtain. To address these issues, this study aims to support research insight surveys for begi
    
[^35]: VlogQA: 越南口语机器阅读理解任务、数据集和基线模型

    VlogQA: Task, Dataset, and Baseline Models for Vietnamese Spoken-Based Machine Reading Comprehension

    [https://arxiv.org/abs/2402.02655](https://arxiv.org/abs/2402.02655)

    本文介绍了VlogQA：越南口语机器阅读理解任务、数据集和基线模型，并提供了使用真实数据进行任务的挑战和机遇的见解。VlogQA是一个基于来自YouTube的剧本文档的问答对数据集，涵盖了食物和旅行等主题。深度学习模型在测试集取得了75.34%的最高F1分数。

    

    本文介绍了一个用于机器阅读理解任务的越南口语语料库的开发过程，并提供了使用真实数据进行机器阅读理解任务时遇到的挑战和机遇的见解。现有的越南机器阅读理解语料库主要关注正式的书面文档，如维基百科文章、在线报纸或教科书。与之相反，VlogQA包含了10,076个问答对，基于从YouTube获取的1,230份剧本文档，YouTube是一个包含了用户上传内容的广泛资源，涵盖了食物和旅行等主题。通过捕捉越南本土人在自然环境中的口语表达，这是越南研究中被忽视的一个角落，该语料库为未来越南语阅读理解任务的研究提供了宝贵的资源。在性能评估方面，我们的深度学习模型在测试集上取得了最高的F1分数为75.34%，表明了其优秀的性能。

    This paper presents the development process of a Vietnamese spoken language corpus for machine reading comprehension (MRC) tasks and provides insights into the challenges and opportunities associated with using real-world data for machine reading comprehension tasks. The existing MRC corpora in Vietnamese mainly focus on formal written documents such as Wikipedia articles, online newspapers, or textbooks. In contrast, the VlogQA consists of 10,076 question-answer pairs based on 1,230 transcript documents sourced from YouTube -- an extensive source of user-uploaded content, covering the topics of food and travel. By capturing the spoken language of native Vietnamese speakers in natural settings, an obscure corner overlooked in Vietnamese research, the corpus provides a valuable resource for future research in reading comprehension tasks for the Vietnamese language. Regarding performance evaluation, our deep-learning models achieved the highest F1 score of 75.34% on the test set, indicat
    
[^36]: 使用对比式上下文学习定制语言模型的回复

    Customizing Language Model Responses with Contrastive In-Context Learning

    [https://arxiv.org/abs/2401.17390](https://arxiv.org/abs/2401.17390)

    本论文提出了一种使用对比示例来定制语言模型回复的方法，通过提供正面示例和负面示例，使模型学会如何回避负面特征，从而更好地满足用户需求。

    

    大型语言模型 (LLMs) 对于机器学习应用变得越来越重要。然而，将LLMs与我们的意图对齐可能会具有挑战性，特别是当我们希望生成优于其他内容的内容，或者当我们希望LLMs以一种难以描述的风格或语气进行回应时。为了解决这个问题，我们提出了一种使用对比示例来更好地描述我们的意图的方法。这涉及提供正面示例来说明真实的意图，以及负面示例来展示我们希望LLMs避免的特征。负面示例可以从标记数据中检索，由人工编写，或由LLMs自动生成。在生成答案之前，我们要求模型分析这些示例，以教会自己避免什么。这个推理步骤为模型提供了与用户需求相关的适当表达，并引导其生成更好的答案。我们在合成和真实数据上测试了我们的方法。

    Large language models (LLMs) are becoming increasingly important for machine learning applications. However, it can be challenging to align LLMs with our intent, particularly when we want to generate content that is preferable over others or when we want the LLM to respond in a certain style or tone that is hard to describe. To address this challenge, we propose an approach that uses contrastive examples to better describe our intent. This involves providing positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid. The negative examples can be retrieved from labeled data, written by a human, or generated by the LLM itself. Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid. This reasoning step provides the model with the appropriate articulation of the user's need and guides it towards generting a better answer. We tested our approach on both synthesized and real
    
[^37]: LLsM: 基于大型语言模型的生成式语言隐写术

    LLsM: Generative Linguistic Steganography with Large Language Model

    [https://arxiv.org/abs/2401.15656](https://arxiv.org/abs/2401.15656)

    本研究提出了LLsM，一种基于大型语言模型的生成式语言隐写术。通过对大规模数据集进行微调，LLM能够以可控的方式生成具有特定话语特征的隐写文本，提高了隐蔽通信的效果。

    

    语言隐写术（LS）旨在根据秘密信息生成隐写文本（stego）。只有授权接收者才能察觉文本中秘密的存在并提取出来，从而保护隐私。然而，现有方案生成的隐写文本可控性较差，很难包含特定的话语特征，如风格。结果，隐写文本容易被检测出来，危及隐蔽通信。为解决这些问题，本文提出了LLsM，第一个基于大型语言模型（LLM）的LS方法。我们使用一个包含丰富话语特征的大规模构建数据集对LLaMA2进行微调，使得微调后的LLM能够以可控的方式生成具有特定话语特征的文本。然后将话语作为引导信息和秘密一起输入给微调后的LLM，形式为“Prompt”。在此基础上，构建的候选池将进行范围编码。

    Linguistic Steganography (LS) tasks aim to generate steganographic text (stego) based on secret information. Only authorized recipients can perceive the existence of secrets in the texts and extract them, thereby preserving privacy. However, the controllability of the stego generated by existing schemes is poor, and the stego is difficult to contain specific discourse characteristics such as style. As a result, the stego is easily detectable, compromising covert communication. To address these problems, this paper proposes LLsM, the first LS with the Large Language Model (LLM). We fine-tuned the LLaMA2 with a large-scale constructed dataset encompassing rich discourse characteristics, which enables the fine-tuned LLM to generate texts with specific discourse in a controllable manner. Then the discourse is used as guiding information and inputted into the fine-tuned LLM in the form of the Prompt together with secret. On this basis, the constructed candidate pool will be range encoded an
    
[^38]: 推理链上的欺骗性语义快捷方式：模型在没有幻觉的情况下能走多远？

    Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?

    [https://arxiv.org/abs/2311.09702](https://arxiv.org/abs/2311.09702)

    本研究探讨了大型语言模型存在的幻觉和不忠实推理问题，提出一种新的探测方法和基准测试以研究LLMs在推理过程中是否会采取欺骗性语义快捷方式。

    

    尽管大型语言模型（LLMs）近期取得了显著进展，并在众多基准测试中表现出色，但最近的研究揭示了LLMs存在幻觉和不忠实推理的问题。本研究探讨了一种特定类型由语义关联引起的幻觉。具体来说，我们调查了LLMs在提示中是否会因为某些关键字/实体偏见而采取捷径，而不是遵循正确的推理路径。为了量化这一现象，我们提出了一种名为EureQA的新型探测方法和基准测试。我们从LLMs会以绝对确定性正确回答的问题开始，然后递归地用证据句子遮蔽重要实体，要求模型在回答问题之前找到根据证据链条遮蔽的实体。

    arXiv:2311.09702v2 Announce Type: replace-cross  Abstract: Despite the recent advancement in large language models (LLMs) and their high performances across numerous benchmarks, recent research has unveiled that LLMs suffer from hallucinations and unfaithful reasoning. This work studies a specific type of hallucination induced by semantic associations. Specifically, we investigate to what extent LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following the correct reasoning path. To quantify this phenomenon, we propose a novel probing method and benchmark called EureQA. We start from questions that LLMs will answer correctly with utmost certainty, and mask the important entity with evidence sentence recursively, asking models to find masked entities according to a chain of evidence before answering the question.   During the construction of the evidence, we purposefully replace semantic clues (entities) that may lead to the correct answer with distractor
    
[^39]: Mind's Mirror: 从大型语言模型中提取自我评估能力和综合思维

    Mind's Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models

    [https://arxiv.org/abs/2311.09214](https://arxiv.org/abs/2311.09214)

    本研究提出了一种从大型语言模型中提取自我评估能力和综合思维的方法，旨在解决小语言模型继承不完善推理和幻觉的问题。

    

    大型语言模型（LLMs）在自然语言处理领域取得了显著进展。然而，这些模型的大规模和计算需求在考虑它们在资源受限环境中的实际部署时带来了巨大挑战。我们提出了一种双重方法论：首先，我们引入了一种新的方法，将LLMs中的自我评估能力提炼到SLMs中，旨在减轻从LLMs继承的错误推理和幻觉的不良影响。其次，我们提倡通过整合多个不同的CoTs和自我评估输出来提炼更全面的思维，以确保更为彻底和健壮。

    arXiv:2311.09214v2 Announce Type: replace  Abstract: Large language models (LLMs) have achieved remarkable advancements in natural language processing. However, the massive scale and computational demands of these models present formidable challenges when considering their practical deployment in resource-constrained environments. While techniques such as chain-of-thought (CoT) distillation have displayed promise in distilling LLMs into small language models (SLMs), there is a risk that distilled SLMs may still inherit flawed reasoning and hallucinations from LLMs. To address these issues, we propose a twofold methodology: First, we introduce a novel method for distilling the self-evaluation capability from LLMs into SLMs, aiming to mitigate the adverse effects of flawed reasoning and hallucinations inherited from LLMs. Second, we advocate for distilling more comprehensive thinking by incorporating multiple distinct CoTs and self-evaluation outputs, to ensure a more thorough and robust
    
[^40]: 伪装成羊的狼：普遍的嵌套越狱提示可以轻松愚弄大型语言模型

    A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily

    [https://arxiv.org/abs/2311.08268](https://arxiv.org/abs/2311.08268)

    提出一种利用大型语言模型自动生成有效的越狱提示的自动框架ReNeLLM，显著提高攻击成功率，同时大大减少时间成本。

    

    大型语言模型（LLMs），如ChatGPT和GPT-4，旨在提供有用和安全的响应。然而，被称为“越狱”的对抗性提示可以规避保障措施，导致LLMs生成潜在有害内容。探索越狱提示可以帮助更好地揭示LLMs的弱点，并进一步引导我们安全地保护它们。不幸的是，现有的越狱方法要么遭受复杂的手工设计，要么需要在其他白盒模型上进行优化，从而损害了泛化性或效率。在本文中，我们将越狱提示攻击概括为两个方面：（1）提示重写和（2）场景嵌套。基于此，我们提出了ReNeLLM，一个利用LLMs自身生成有效越狱提示的自动框架。大量实验证明，与现有基线相比，ReNeLLM显著提高了攻击成功率，同时大大减少了时间成本。

    arXiv:2311.08268v2 Announce Type: replace  Abstract: Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, compromising generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Ou
    
[^41]: 揭示真相：欺骗语言和语言模型

    To Tell The Truth: Language of Deception and Language Models

    [https://arxiv.org/abs/2311.07092](https://arxiv.org/abs/2311.07092)

    在高风险环境中，研究人员通过分析电视游戏节目数据发现，即使只使用语言线索，基于大型语言模型构建的模型可以与人类主体具有类似的真相检测性能。

    

    arXiv:2311.07092v2 公告类型：替换-cross 摘要：基于文本的错误信息渗透到在线讨论中，然而人们能够从这种欺骗性文本内容中辨别真相的证据却很少。我们分析了一档新颖的电视游戏节目数据，其中高风险环境中相互之间存在冲突目标的个体之间的对话导致谎言。我们调查了欺骗语言潜在可验证语言线索在客观真相存在的情况下的表现，这是以往基于文本的欺骗数据集中缺少的一个显著特征。我们展示了存在一类探测器（算法），其真相检测性能与人类主体相似，即使前者只使用语言线索，而后者则通过完全访问所有潜在线索源（语言和视听）进行对话。我们的模型，建立在大型语言模型之上，采用瓶颈框架来学习可辨别的线索，以确定真相的行为

    arXiv:2311.07092v2 Announce Type: replace-cross  Abstract: Text-based misinformation permeates online discourses, yet evidence of people's ability to discern truth from such deceptive textual content is scarce. We analyze a novel TV game show data where conversations in a high-stake environment between individuals with conflicting objectives result in lies. We investigate the manifestation of potentially verifiable language cues of deception in the presence of objective truth, a distinguishing feature absent in previous text-based deception datasets. We show that there exists a class of detectors (algorithms) that have similar truth detection performance compared to human subjects, even when the former accesses only the language cues while the latter engages in conversations with complete access to all potential sources of cues (language and audio-visual). Our model, built on a large language model, employs a bottleneck framework to learn discernible cues to determine truth, an act of 
    
[^42]: 通过决策模型弥补新手与专家之间的差距：以纠正数学错误为案例研究

    Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes

    [https://arxiv.org/abs/2310.10648](https://arxiv.org/abs/2310.10648)

    通过使用决策模型Bridge，结合专家的认知任务分析，成功利用大型语言模型（LLMs）来弥补新手和专家在纠正数学错误中的知识差距。

    

    高质量辅导规模化仍然是教育中的一项主要挑战。由于需求增长，许多平台聘用新手导师，他们与经验丰富的教育工作者不同，难以解决学生的错误，因此无法抓住主要的学习机会。我们的工作探讨了大型语言模型（LLMs）在纠正数学错误中弥补新手和专家之间知识差距的潜力。我们提出Bridge，这是一种利用认知任务分析将专家的潜在思维过程转化为纠正模型的方法。这涉及专家识别(A)学生的错误、(B)纠正策略和(C)生成回应之前的意图。我们构建了一个包含700个真实辅导对话的数据集，由专家标注了他们的决策。我们在我们的数据集上评估了最先进的LLMs，并发现专家的决策模型对LLMs来说是至关重要的，以弥补这一差距：回应f

    arXiv:2310.10648v2 Announce Type: replace-cross  Abstract: Scaling high-quality tutoring remains a major challenge in education. Due to growing demand, many platforms employ novice tutors who, unlike experienced educators, struggle to address student mistakes and thus fail to seize prime learning opportunities. Our work explores the potential of large language models (LLMs) to close the novice-expert knowledge gap in remediating math mistakes. We contribute Bridge, a method that uses cognitive task analysis to translate an expert's latent thought process into a decision-making model for remediation. This involves an expert identifying (A) the student's error, (B) a remediation strategy, and (C) their intention before generating a response. We construct a dataset of 700 real tutoring conversations, annotated by experts with their decisions. We evaluate state-of-the-art LLMs on our dataset and find that the expert's decision-making model is critical for LLMs to close the gap: responses f
    
[^43]: 对于文本预测的忠实和稳健的本地可解释性

    Faithful and Robust Local Interpretability for Textual Predictions. (arXiv:2311.01605v1 [cs.CL])

    [http://arxiv.org/abs/2311.01605](http://arxiv.org/abs/2311.01605)

    提出了一种名为FRED的新颖方法，用于解释文本预测。FRED可以识别文档中的关键词，并且通过与最先进的方法进行的实证评估证明了其在提供对文本模型的深入见解方面的有效性。

    

    可解释性对于机器学习模型在关键领域中得到信任和部署是至关重要的。然而，现有的用于解释文本模型的方法通常复杂，并且缺乏坚实的数学基础，它们的性能也不能保证。在本文中，我们提出了一种新颖的方法FRED（Faithful and Robust Explainer for textual Documents），用于解释文本预测。FRED可以识别文档中的关键词，当这些词被移除时对预测结果产生重大影响。我们通过正式的定义和对可解释分类器的理论分析，确立了FRED的可靠性。此外，我们还通过与最先进的方法进行的实证评估，证明了FRED在提供对文本模型的深入见解方面的有效性。

    Interpretability is essential for machine learning models to be trusted and deployed in critical domains. However, existing methods for interpreting text models are often complex, lack solid mathematical foundations, and their performance is not guaranteed. In this paper, we propose FRED (Faithful and Robust Explainer for textual Documents), a novel method for interpreting predictions over text. FRED identifies key words in a document that significantly impact the prediction when removed. We establish the reliability of FRED through formal definitions and theoretical analyses on interpretable classifiers. Additionally, our empirical evaluation against state-of-the-art methods demonstrates the effectiveness of FRED in providing insights into text models.
    
[^44]: SALMONN：迈向大规模语言模型通用听觉能力

    SALMONN: Towards Generic Hearing Abilities for Large Language Models. (arXiv:2310.13289v1 [cs.SD])

    [http://arxiv.org/abs/2310.13289](http://arxiv.org/abs/2310.13289)

    本文介绍了SALMONN，这是一个集成了预训练的大型语言模型和语音/音频编码器的多模态模型，能够实现直接处理和理解普通音频输入的能力，并在多个语音和音频任务上取得竞争性表现。

    

    听觉可以说是物理世界中人工智能（AI）代理的一个至关重要的能力，它涉及到对至少三种声音类型（语音、音频事件和音乐）的普通听觉信息的感知和理解。本文中，我们提出了SALMONN，一种语音音频语言音乐开放神经网络，它通过将预训练的基于文本的大型语言模型（LLM）与语音和音频编码器集成到单一多模态模型中进行构建。SALMONN使得LLM能够直接处理和理解普通音频输入，在训练中在许多语音和音频任务上取得了竞争性的表现，如自动语音识别和翻译、基于听觉信息的问题回答、情感识别、说话人验证以及音乐和音频字幕等。SALMONN还具有在训练中未见的各种新能力，包括但不限于对未训练语言的语音翻译。

    Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning \textit{etc.} SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languag
    
[^45]: 关于循环神经网络语言模型的表示能力的研究

    On the Representational Capacity of Recurrent Neural Language Models. (arXiv:2310.12942v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.12942](http://arxiv.org/abs/2310.12942)

    本文研究了基于循环神经网络的语言模型的计算表达性，扩展了图灵完备性结果到概率情况，并提供了上下界分析。

    

    本研究调查了基于循环神经网络(RNNs)的语言模型(LMs)的计算表达性。Siegelmann和Sontag(1992)曾经展示了具有有理权重和隐藏状态以及无限计算时间的RNNs是图灵完备的。然而，LMs不仅定义了字符串上的加权，还定义了(非加权)语言成员关系，对RNN LMs（RLMs）的计算能力分析应该反映这一点。我们将图灵完备性结果扩展到概率情况，展示了如何使用有理权重的RLM和无限计算时间来模拟任何概率图灵机(PTM)。由于在实践中，RLMs实时工作，每个时间步骤处理一个符号，因此我们将上述结果作为RLMs表达性的上界。我们还通过展示在实时计算限制下，这些模型可以模拟确定性实时有理PTMs来提供下界。

    This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM). Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs.
    
[^46]: REMARK-LLM:一种用于生成大型语言模型的鲁棒高效的水印框架

    REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models. (arXiv:2310.12362v1 [cs.CR])

    [http://arxiv.org/abs/2310.12362](http://arxiv.org/abs/2310.12362)

    REMARK-LLM是一种针对生成大型语言模型的文本的鲁棒高效的水印框架，通过学习-based消息编码、重新参数化和解码模块以及优化的波束搜索算法来保护生成内容的完整性和防止恶意利用。

    

    我们提出了一种名为REMARK-LLM的新型高效、强鲁棒性的水印框架，专为大型语言模型（LLM）生成的文本设计。使用LLMs合成类似人类的内容需要大量的计算资源和广泛的数据集，涵盖了重要的知识产权（IP）。然而，生成的内容容易受到恶意利用，包括垃圾邮件和抄袭。为了解决这些挑战，REMARK-LLM提出了三个新的组成部分：（i）基于学习的消息编码模块，将二进制签名注入LLM生成的文本中；（ii）重新参数化模块，将消息编码的密集分布转换为水印文本标记的稀疏分布；（iii）专门用于签名提取的解码模块；此外，我们引入了一种优化的波束搜索算法，以保证生成内容的连贯性和一致性。REMARK-LLM经过严格的训练，以鼓励语义完整性的保留。

    We present REMARK-LLM, a novel efficient, and robust watermarking framework designed for texts generated by large language models (LLMs). Synthesizing human-like content using LLMs necessitates vast computational resources and extensive datasets, encapsulating critical intellectual property (IP). However, the generated content is prone to malicious exploitation, including spamming and plagiarism. To address the challenges, REMARK-LLM proposes three new components: (i) a learning-based message encoding module to infuse binary signatures into LLM-generated texts; (ii) a reparameterization module to transform the dense distributions from the message encoding to the sparse distribution of the watermarked textual tokens; (iii) a decoding module dedicated for signature extraction; Furthermore, we introduce an optimized beam search algorithm to guarantee the coherence and consistency of the generated content. REMARK-LLM is rigorously trained to encourage the preservation of semantic integrity
    
[^47]: QLLM: 大规模语言模型的准确高效低位宽量化

    QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models. (arXiv:2310.08041v1 [cs.CL])

    [http://arxiv.org/abs/2310.08041](http://arxiv.org/abs/2310.08041)

    QLLM是一种为大规模语言模型设计的准确高效的低位宽后训练量化方法，通过引入自适应通道重组技术，将离群值的大小重新分配给其他通道，从而减轻它们对量化范围的影响。

    

    大规模语言模型在自然语言处理领域表现出色，但由于其所需资源过大，限制了其广泛应用。虽然量化感知训练（Quantization-Aware Training，QAT）提供了一种解决方案，但它的训练成本过高，因此后训练量化（Post-Training Quantization，PTQ）成为大规模语言模型更实际的方法。在现有研究中，特定通道中的激活离群值被认为是导致后训练量化准确性下降的瓶颈。本文提出了QLLM，一种为大规模语言模型设计的准确高效的低位宽后训练量化方法。QLLM引入了一种自适应通道重组技术，将离群值的大小重新分配给其他通道，从而减轻它们对量化范围的影响。具体来说，通过通道拆分和通道组装，在保证低位宽的情况下将离群通道分解成多个子通道。

    Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a 
    
[^48]: 红队游戏：红队语言模型的博弈论框架

    Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models. (arXiv:2310.00322v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00322](http://arxiv.org/abs/2310.00322)

    本文提出了红队游戏（RTG）框架，利用博弈论分析了红队语言模型（RLM）与蓝队语言模型（BLM）之间的多轮攻防互动。同时引入了游戏化红队求解器（GRTS）来提供自动化的红队技术。

    

    可部署的大型语言模型（LLM）必须符合有益和无害性的标准，从而实现LLM输出与人类价值的一致性。红队技术是实现这一标准的关键途径。现有的研究仅依赖于手动红队设计和启发式对抗提示进行漏洞检测和优化。这些方法缺乏严格的数学形式化，限制了在可量化度量和收敛保证下对LLM进行多样攻击策略的探索和优化。在本文中，我们提出了红队游戏（RTG），这是一个通用的无需手动标注的博弈论框架。RTG旨在分析红队语言模型（RLM）与蓝队语言模型（BLM）之间的多轮攻防互动。在RTG中，我们提出了具有语义空间多样性度量的游戏化红队求解器（GRTS）。GRTS是一种自动化的红队技术，用于解决红队游戏问题。

    Deployable Large Language Models (LLMs) must conform to the criterion of helpfulness and harmlessness, thereby achieving consistency between LLMs outputs and human values. Red-teaming techniques constitute a critical way towards this criterion. Existing work rely solely on manual red team designs and heuristic adversarial prompts for vulnerability detection and optimization. These approaches lack rigorous mathematical formulation, thus limiting the exploration of diverse attack strategy within quantifiable measure and optimization of LLMs under convergence guarantees. In this paper, we present Red-teaming Game (RTG), a general game-theoretic framework without manual annotation. RTG is designed for analyzing the multi-turn attack and defense interactions between Red-team language Models (RLMs) and Blue-team Language Model (BLM). Within the RTG, we propose Gamified Red-teaming Solver (GRTS) with diversity measure of the semantic space. GRTS is an automated red teaming technique to solve 
    
[^49]: 揭秘CLIP数据

    Demystifying CLIP Data. (arXiv:2309.16671v1 [cs.CV])

    [http://arxiv.org/abs/2309.16671](http://arxiv.org/abs/2309.16671)

    CLIP的成功主要归功于其数据而非模型架构或预训练目标。我们通过元数据整理方法引入了MetaCLIP，该方法从原始数据池和元数据中生成一个平衡的子集，提供了更加详细的数据信息。在实验中，我们发现MetaCLIP在处理400M个图像-文本数据对时取得了良好的性能。

    

    对比语言-图像预训练（CLIP）是一种推动计算机视觉研究和应用的方法，为现代识别系统和生成模型注入了活力。我们认为，CLIP成功的主要因素是其数据，而不是模型架构或预训练目标。然而，CLIP只提供了关于其数据和如何收集数据的非常有限的信息，导致其他研究努力通过使用模型参数进行过滤来重现CLIP的数据。在这项工作中，我们意在揭示CLIP的数据整理方法，并在公开给社区的过程中引入元数据整理的语言-图像预训练（MetaCLIP）。MetaCLIP通过对元数据分布进行平衡，从原始数据池和元数据（从CLIP的概念中得出）中产生一个平衡的子集。我们的实验研究严格隔离了模型和训练设置，仅专注于数据。MetaCLIP应用于包含400M图像-文本数据对的CommonCrawl，并获得了较好的性能。

    Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced research and applications in computer vision, fueling modern recognition systems and generative models. We believe that the main ingredient to the success of CLIP is its data and not the model architecture or pre-training objective. However, CLIP only provides very limited information about its data and how it has been collected, leading to works that aim to reproduce CLIP's data by filtering with its model parameters. In this work, we intend to reveal CLIP's data curation approach and in our pursuit of making it open to the community introduce Metadata-Curated Language-Image Pre-training (MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's concepts) and yields a balanced subset over the metadata distribution. Our experimental study rigorously isolates the model and training settings, concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M image-text data pairs outper
    
[^50]: 基于中心元素识别的嵌套事件抽取

    Nested Event Extraction upon Pivot Element Recogniton. (arXiv:2309.12960v1 [cs.CL])

    [http://arxiv.org/abs/2309.12960](http://arxiv.org/abs/2309.12960)

    本文提出了一种名为PerNee的新模型，通过识别中心元素来提取嵌套事件。该模型解决了现有NEE方法无法处理中心元素双重身份的问题，并通过提示学习将事件类型和参数角色的信息纳入其中，以提高NEE性能。

    

    嵌套事件抽取（NEE）旨在提取包含其他事件作为其参数的复杂事件结构。嵌套事件涉及一种称为中心元素（PEs）的元素，它同时作为外部事件的参数和内部事件的触发器，并将它们连接成嵌套结构。PEs的这种特殊特性给现有的NEE方法带来了挑战，因为它们不能很好地处理PEs的双重身份。因此，本文提出了一种新模型，称为PerNee，主要基于识别PEs来提取嵌套事件。具体而言，PerNee首先识别内部和外部事件的触发器，然后通过分类触发器对之间关系类型来识别PEs。为了获得更好的触发器和参数表示以进一步提高NEE性能，PerNee通过提示学习将事件类型和参数角色的信息纳入其中。由于现有的NEE数据集（例如Gen）

    Nested Event Extraction (NEE) aims to extract complex event structures where an event contains other events as its arguments recursively. Nested events involve a kind of Pivot Elements (PEs) that simultaneously act as arguments of outer events and as triggers of inner events, and thus connect them into nested structures. This special characteristic of PEs brings challenges to existing NEE methods, as they cannot well cope with the dual identities of PEs. Therefore, this paper proposes a new model, called PerNee, which extracts nested events mainly based on recognizing PEs. Specifically, PerNee first recognizes the triggers of both inner and outer events and further recognizes the PEs via classifying the relation type between trigger pairs. In order to obtain better representations of triggers and arguments to further improve NEE performance, it incorporates the information of both event types and argument roles into PerNee through prompt learning. Since existing NEE datasets (e.g., Gen
    
[^51]: 音乐产品的正面和风险信息评估

    Positive and Risky Message Assessment for Music Products. (arXiv:2309.10182v1 [cs.CL])

    [http://arxiv.org/abs/2309.10182](http://arxiv.org/abs/2309.10182)

    这项研究提出了一个新的问题：如何评估音乐产品中的正面和风险信息。研究者提出了一个多任务预测模型，通过序数约束解决这个问题，并且取得了显著优于其他方法的结果。

    

    在这项工作中，我们提出了一个新颖的研究问题：评估音乐产品中的正面和风险信息。我们首先建立了一个多角度多级音乐内容评估的基准，然后提出了一种有效的多任务预测模型，并通过序数约束来解决这个问题。我们的结果显示，所提出的方法不仅明显优于强大的针对特定任务的对应方法，而且可以同时评估多个方面。

    In this work, we propose a novel research problem: assessing positive and risky messages from music products. We first establish a benchmark for multi-angle multi-level music content assessment and then present an effective multi-task prediction model with ordinality-enforcement to solve this problem. Our result shows the proposed method not only significantly outperforms strong task-specific counterparts but can concurrently evaluate multiple aspects.
    
[^52]: Echotune: 利用语音的可变长度特性的模块化特征提取器在ASR任务中的应用

    Echotune: A Modular Extractor Leveraging the Variable-Length Nature of Speech in ASR Tasks. (arXiv:2309.07765v1 [cs.SD])

    [http://arxiv.org/abs/2309.07765](http://arxiv.org/abs/2309.07765)

    Echotune是一个模块化特征提取器，利用语音的可变长度特性，通过引入Echo-MSA模块，实现了从帧到话语的各种颗粒度的语音特征提取，解决了固定长度注意力的局限性。

    

    Transformer架构已被证明在自动语音识别（ASR）任务中非常有效，成为该领域众多研究的基础组件。历史上，许多方法依赖于固定长度的注意力窗口，这对于持续时间和复杂性不同的语音样本来说是有问题的，导致数据过度平滑化和忽视了长期连通性的重要性。为了解决这个限制，我们引入了Echo-MSA，一个具有可变长度注意力机制的灵活模块，可以适应不同复杂性和持续时间的语音样本。该模块提供了从帧和音素到单词和话语的各种颗粒度的语音特征提取的灵活性。提出的设计捕捉到了语音的可变长度特征，并解决了固定长度注意力的局限性。我们的评估利用了一个平行的注意力架构，并结合了一个动态门控机制。

    The Transformer architecture has proven to be highly effective for Automatic Speech Recognition (ASR) tasks, becoming a foundational component for a plethora of research in the domain. Historically, many approaches have leaned on fixed-length attention windows, which becomes problematic for varied speech samples in duration and complexity, leading to data over-smoothing and neglect of essential long-term connectivity. Addressing this limitation, we introduce Echo-MSA, a nimble module equipped with a variable-length attention mechanism that accommodates a range of speech sample complexities and durations. This module offers the flexibility to extract speech features across various granularities, spanning from frames and phonemes to words and discourse. The proposed design captures the variable length feature of speech and addresses the limitations of fixed-length attention. Our evaluation leverages a parallel attention architecture complemented by a dynamic gating mechanism that amalgam
    
[^53]: 从数量到质量：利用自我引导数据选择方法提升LLM性能以进行指令调优

    From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning. (arXiv:2308.12032v1 [cs.CL])

    [http://arxiv.org/abs/2308.12032](http://arxiv.org/abs/2308.12032)

    该论文引入了一种自我引导的方法，让LLM能够自主地选择高质量的指令数据，通过引入指令遵循难度指标（IFD），大幅提高了模型训练效率，并在知名数据集上进行了验证，展示了优于传统数据输入的结果。

    

    在大型语言模型领域，指令数据的质量和数量之间的平衡已成为一个焦点。鉴于此，我们引入了一种自我引导的方法，让LLM能够自主地识别和选择大规模开源数据集中的精选样本，有效减少了指令调优的手动筛选和潜在成本。我们的关键创新是指令遵循难度（IFD）指标，它成为了一个决定性工具，用于识别模型期望响应和自主生成能力之间的差异。通过灵活应用IFD，我们能够找到精选样本，从而大幅提升模型训练效率。在Alpaca和WizardLM等知名数据集上的实证验证支持我们的发现；仅使用传统数据输入的10%，我们的策略展示了改进的结果。这种自我引导挑选和IFD指标的综合意味着LLM优化的一个变革性飞跃，有望同时提高模型性能和降低成本。

    In the realm of Large Language Models, the balance between instruction data quality and quantity has become a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from vast open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to identify discrepancies between a model's expected responses and its autonomous generation prowess. Through the adept application of IFD, cherry samples are pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on renowned datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of conventional data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the optimization of LLMs, promising both
    
[^54]: Convoifilter: 鸡尾酒会语音识别的案例研究

    Convoifilter: A case study of doing cocktail party speech recognition. (arXiv:2308.11380v1 [cs.SD])

    [http://arxiv.org/abs/2308.11380](http://arxiv.org/abs/2308.11380)

    本文通过使用单声道语音增强模块与ASR模块，成功将ASR的词错误率从80%降低到26.4%，并通过联合微调策略将其进一步降低到14.5%。

    

    本文提出了一个端到端的模型，用于改进拥挤、嘈杂环境下的自动语音识别（ASR），针对特定说话者。该模型利用单声道语音增强模块将说话者的声音与背景噪声分离，结合ASR模块。通过这种方法，该模型能够将ASR的词错误率（WER）从80%降低到26.4%。通常情况下，由于数据要求的变化，这两个组件会独立调整。然而，语音增强可能会导致ASR效率下降。通过实施联合微调策略，该模型可以将分别调整的WER从26.4%降低到14.5%。

    This paper presents an end-to-end model designed to improve automatic speech recognition (ASR) for a particular speaker in a crowded, noisy environment. The model utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise, along with an ASR module. Through this approach, the model is able to decrease the word error rate (WER) of ASR from 80% to 26.4%. Typically, these two components are adjusted independently due to variations in data requirements. However, speech enhancement can create anomalies that decrease ASR efficiency. By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning.
    
[^55]: WeaverBird: 利用大型语言模型、知识库和搜索引擎增强金融决策能力

    WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine. (arXiv:2308.05361v1 [cs.CL])

    [http://arxiv.org/abs/2308.05361](http://arxiv.org/abs/2308.05361)

    WeaverBird是一个专为金融领域设计的智能对话系统，通过利用大型语言模型、本地知识库和搜索引擎，能够理解复杂的金融查询并提供明智的回答，具有增强的可信度。

    

    我们提出了WeaverBird，一个专为金融领域设计的智能对话系统。我们的系统利用GPT架构的大型语言模型，并利用金融相关文本的广泛语料对其进行了调整。因此，我们的系统能够理解复杂的金融查询，例如“在通货膨胀期间如何管理我的投资？”并提供明智的回答。此外，我们的系统还集成了本地的知识库和搜索引擎以检索相关信息。最终的回答是基于搜索结果进行条件约束的，并包含适当的引用来源，从而具有增强的可信度。通过一系列与金融相关的问题，我们已经展示了我们的系统相比其他模型的卓越性能。用户可以在我们的在线演示网站https://weaverbird.ttic.edu与我们的系统进行互动，并观看我们的2分钟演示视频https://www.youtube.com/watch?v=yofgeq。

    We present WeaverBird, an intelligent dialogue system designed specifically for the finance domain. Our system harnesses a large language model of GPT architecture that has been tuned using extensive corpora of finance-related text. As a result, our system possesses the capability to understand complex financial queries, such as "How should I manage my investments during inflation?", and provide informed responses. Furthermore, our system incorporates a local knowledge base and a search engine to retrieve relevant information. The final responses are conditioned on the search results and include proper citations to the sources, thus enjoying an enhanced credibility. Through a range of finance-related questions, we have demonstrated the superior performance of our system compared to other models. To experience our system firsthand, users can interact with our live demo at https://weaverbird.ttic.edu, as well as watch our 2-min video illustration at https://www.youtube.com/watch?v=yofgeq
    
[^56]: ChatGPT生物医学生成文本中建立信任的方法：基于本体的知识图谱用于验证疾病-症状关系

    Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links. (arXiv:2308.03929v1 [cs.AI])

    [http://arxiv.org/abs/2308.03929](http://arxiv.org/abs/2308.03929)

    本研究通过构建基于本体的知识图谱，利用疾病本体和症状本体构建数学模型，利用事实核查算法和网络中心度指标分析ChatGPT生成的文本与真实医学文献之间的准确性，以验证疾病-症状关系。

    

    方法：通过创新的方法，我们从真实的医学文献和人工智能生成的内容构建了基于本体的知识图谱。我们的目标是区分事实信息和未经验证的数据。我们收集了两个数据集：一个是使用“人类疾病和症状”查询从生物医学文献中编译的，另一个是由ChatGPT生成的模拟文章。利用这些数据集（PubMed和ChatGPT），我们随机选择了10组每组250个摘要，并使用特定的种子。我们的方法主要是利用疾病本体（DOID）和症状本体（SYMP）构建知识图谱，这是一种强大的数学模型，可以进行无偏差的比较。通过使用我们的事实核查算法和网络中心度指标，我们进行了GPT疾病-症状链接分析，以量化在噪声、假设和重要发现中的事实知识的准确性。结果：通过比较不同ChatGPT知识图谱及其PubMed计数获得的结果，我们发现...

    Methods: Through an innovative approach, we construct ontology-based knowledge graphs from authentic medical literature and AI-generated content. Our goal is to distinguish factual information from unverified data. We compiled two datasets: one from biomedical literature using a "human disease and symptoms" query, and another generated by ChatGPT, simulating articles. With these datasets (PubMed and ChatGPT), we curated 10 sets of 250 abstracts each, selected randomly with a specific seed. Our method focuses on utilizing disease ontology (DOID) and symptom ontology (SYMP) to build knowledge graphs, robust mathematical models that facilitate unbiased comparisons. By employing our fact-checking algorithms and network centrality metrics, we conducted GPT disease-symptoms link analysis to quantify the accuracy of factual knowledge amid noise, hypotheses, and significant findings.  Results: The findings obtained from the comparison of diverse ChatGPT knowledge graphs with their PubMed count
    
[^57]: 全球叙事的揭示：一份多语种推特数据集，探讨俄乌冲突的新闻媒体报道

    Unveiling Global Narratives: A Multilingual Twitter Dataset of News Media on the Russo-Ukrainian Conflict. (arXiv:2306.12886v1 [cs.CL])

    [http://arxiv.org/abs/2306.12886](http://arxiv.org/abs/2306.12886)

    研究人员收集了约1.5百万条涵盖60种不同语言的推文，创建了一个多语种推特数据集，重点探讨了俄乌冲突的新闻媒体报道。数据集中的标签可以识别与该话题相关的主体、立场、概念和情感表达。

    

    俄乌冲突一直都是全球媒体密集报道的主题。了解这个话题背后的全球叙事对于旨在从多个层面获取洞见的研究人员来说至关重要。在本文中，我们提出了一份独特的数据集，通过收集并处理世界各地新闻或媒体公司发布在社交媒体上的推文，重点关注这个话题。我们收集了2022年2月至2023年5月的推文，以收集约1.5百万条使用60种不同语言的推文。数据集中的每个推文都附带有处理过的标签，允许对提到的主体、立场、概念和表达的情感进行识别。数据集的可用性为希望从不同方面调查俄乌冲突的全球叙事，例如谁是主要的相关方、持什么态度、这些态度的来源在哪里以及不同的概念如何，提供了有价值的资源。

    The ongoing Russo-Ukrainian conflict has been a subject of intense media coverage worldwide. Understanding the global narrative surrounding this topic is crucial for researchers that aim to gain insights into its multifaceted dimensions. In this paper, we present a novel dataset that focuses on this topic by collecting and processing tweets posted by news or media companies on social media across the globe. We collected tweets from February 2022 to May 2023 to acquire approximately 1.5 million tweets in 60 different languages. Each tweet in the dataset is accompanied by processed tags, allowing for the identification of entities, stances, concepts, and sentiments expressed. The availability of the dataset serves as a valuable resource for researchers aiming to investigate the global narrative surrounding the ongoing conflict from various aspects such as who are the prominent entities involved, what stances are taken, where do these stances originate, and how are the different concepts 
    
[^58]: AI增强的调查：利用大语言模型进行全国代表性调查的观点预测

    AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys. (arXiv:2305.09620v1 [cs.CL])

    [http://arxiv.org/abs/2305.09620](http://arxiv.org/abs/2305.09620)

    本论文研究了利用经过全国代表性调查微调的大语言模型（LLMs）来增强调查的观点预测，取得了在遗漏数据插值和回溯推理方面优秀的成果，在零次预测方面仍需进一步研究。

    

    本论文研究了如何使用经过全国代表性调查微调的大语言模型（LLMs）来增强调查。本文探讨了LLMs在观点预测中，遗漏数据插值，回溯推理和零次预测三个不同应用。我们提出了一种新的方法论框架，将调查问题、个人信念和时间背景的神经嵌入引入到观点预测的个性化LLMs中。在1972年到2021年的“常规社会调查”中，我们从68,846名美国人中获得了3,110个二进制观点，在Alpaca-7b模型的基础上取得了最好的成果，在缺失数据插值（AUC=0.87，公开观点预测为$\rho$=0.99）和回溯推理（AUC=0.86，$\rho$=0.98）方面表现出色。这些显著的预测能力能够以高置信度填补缺失的趋势，并标明公众态度何时发生变化，如同性婚姻的获取支持。然而，在零次预测的情况下，模型的表现受到限制，需要进一步研究。

    How can we use large language models (LLMs) to augment surveys? This paper investigates three distinct applications of LLMs fine-tuned by nationally representative surveys for opinion prediction -- missing data imputation, retrodiction, and zero-shot prediction. We present a new methodological framework that incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs in opinion prediction. Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our best models based on Alpaca-7b excels in missing data imputation (AUC = 0.87 for personal opinion prediction and $\rho$ = 0.99 for public opinion prediction) and retrodiction (AUC = 0.86, $\rho$ = 0.98). These remarkable prediction capabilities allow us to fill in missing trends with high confidence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. However, the models show limited performance in a zer
    
[^59]: NeuroComparatives：比较知识的神经符号提炼

    NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge. (arXiv:2305.04978v1 [cs.CL])

    [http://arxiv.org/abs/2305.04978](http://arxiv.org/abs/2305.04978)

    本文提出了一种新的用于比较知识提炼的神经符号框架，名称为NeuroComparatives。该框架利用词汇约束解码进行比较知识提炼，以及对生成的知识进行严格过滤。

    

    比较知识是我们世界知识的重要组成部分，但在以前的文献中研究不足。本文研究比较知识获取任务，受到像GPT-3这样极端规模语言模型能力的显着提高的推动，推动了将他们的知识收集到知识库中的努力。但是，这些模型的推理API访问受到限制，从而限制了知识获取的范围和多样性。因此，我们提出了一个看似不可行的问题：更易于访问、规模更小、性能更弱的模型（如GPT-2）是否可以用于获取比较知识，从而达到与大规模模型相当的质量？我们引入了NeuroComparatives，一种使用词汇约束解码的比较知识提炼新框架，其后紧密过滤生成的知识。

    Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) is an essential component of our world knowledge, yet understudied in prior literature. In this paper, we study the task of comparative knowledge acquisition, motivated by the dramatic improvements in the capabilities of extreme-scale language models like GPT-3, which have fueled efforts towards harvesting their knowledge into knowledge bases. However, access to inference API for such models is limited, thereby restricting the scope and the diversity of the knowledge acquisition. We thus ask a seemingly implausible question: whether more accessible, yet considerably smaller and weaker models such as GPT-2, can be utilized to acquire comparative knowledge, such that the resulting quality is on par with their large-scale counterparts?  We introduce NeuroComparatives, a novel framework for comparative knowledge distillation using lexically-constrained decoding, followed by stringent filtering of generated knowledge
    
[^60]: 通过掩码结构成长实现2倍语言模型预训练加速

    2x Faster Language Model Pre-training via Masked Structural Growth. (arXiv:2305.02869v1 [cs.CL])

    [http://arxiv.org/abs/2305.02869](http://arxiv.org/abs/2305.02869)

    本文提出了掩码结构成长（MSG），可以加速语言模型的预训练，其中包括全维度成长进程和独立于新权重初始化的函数严格保留成长操作。

    

    在当今自然语言处理研究中，加速大型语言模型预训练是一个关键问题。本文旨在通过从小型Transformer结构逐步扩展到大型结构，加快预训练进程。这种渐进式成长的主要研究问题有两个，即成长进程和成长操作。对于成长进程，现有研究已经探索了深度和前馈层的多阶段扩展，但每个维度对进程效率的影响仍然是一个未解决的问题。而对于成长操作，现有研究依赖于新权重的初始化来继承原有的知识，只实现了非严格的函数保留，从而限制了进一步的训练动态优化。为解决这些问题，本文提出了掩码结构成长（MSG），其中包括涉及所有可能维度的成长进程和独立于新权重初始化的函数严格保留成长操作。实验证明，MSG可显著加速语言模型预训练。

    Acceleration of large language model pre-training is a critical issue in present NLP research. In this paper, we focus on speeding up pre-training by progressively growing from a small Transformer structure to a large one. There are two main research problems related to progressive growth: growth schedule and growth operator. For growth schedule, existing work has explored multi-stage expansion of depth and feedforward layers. However, the impact of each dimension on the schedule's efficiency is still an open question. For growth operator, existing work relies on the initialization of new weights to inherit knowledge, and achieve only non-strict function preservation, limiting further optimization of training dynamics. To address these issues, we propose Masked Structural Growth (MSG), including growth schedules involving all possible dimensions and strictly function-preserving growth operators that is independent of the initialization of new weights. Experiments show that MSG is signi
    
[^61]: 位置偏差对token分类中的语言模型的影响

    Impact of Position Bias on Language Models in Token Classification. (arXiv:2304.13567v1 [cs.CL])

    [http://arxiv.org/abs/2304.13567](http://arxiv.org/abs/2304.13567)

    研究了语言模型在token分类任务中的位置偏差问题，通过实验表明在具体任务中，BERT、ERNIE、ELECTRA等编码器以及GPT2和BLOOM等解码器的平均性能下降了3%和9%。

    

    语言模型在自然语言处理任务中表现出了最先进的性能。命名实体识别(NER)或词性标注等下游任务已知存在数据不平衡问题，特别是在正负示例的比例和类不平衡方面。本文研究了语言模型的另一个特定问题，即token分类任务中正示例的位置偏差。因此，我们对基于Token分类基准测试的语言模型的性能进行了深入的位置偏差评估。我们的研究包括CoNLL03和OntoNote5.0用于NER，English Tree Bank UD_en和TweeBank用于POS标记。我们提出了一种评估方法，以研究Transformer模型中的位置偏差。我们发现像BERT、ERNIE、ELECTRA这样的编码器和像GPT2 和BLOOM这样的解码器平均性能下降了3%和9%。

    Language Models (LMs) have shown state-of-the-art performance in Natural Language Processing (NLP) tasks. Downstream tasks such as Named Entity Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data imbalance issues, specifically in terms of the ratio of positive to negative examples, and class imbalance. In this paper, we investigate an additional specific issue for language models, namely the position bias of positive examples in token classification tasks. Therefore, we conduct an in-depth evaluation of the impact of position bias on the performance of LMs when fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We propose an evaluation approach to investigate position bias in Transformer models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as GPT2 and BLOOM can suffer from this bias with an average drop of 3\% and 9\% in their performan
    
[^62]: 采用可解释的符号化神经模型检测上下文不符的多模态谣言

    Detecting Out-of-Context Multimodal Misinformation with interpretable neural-symbolic model. (arXiv:2304.07633v1 [cs.CL])

    [http://arxiv.org/abs/2304.07633](http://arxiv.org/abs/2304.07633)

    本论文提出了一种可解释的神经符号模型，用于检测上下文不符的虚假多模态信息，帮助事实检查网站进行记录澄清。

    

    近年来，虚假信息的演化持续增长，旨在影响公众舆论。与传统的谣言或虚假新闻编辑主要依赖于生成和/或伪造的图像、文本和视频不同，当前的虚假信息创作者更倾向于使用上下文不匹配的多媒体内容（例如，不匹配的图像和标题）来欺骗公众和虚假新闻检测系统。这种新型的虚假信息不仅增加了检测的难度，也增加了澄清的难度，因为每个单独的模态都足够接近真实信息。为了解决这个问题，在本文中，我们探讨了如何实现可解释的跨模态去上下文检测，同时识别不匹配的对和跨模态矛盾，这对事实检查网站的记录澄清非常有帮助。所提出的模型首先通过抽象多模态信息，基于Abstract M进行符号化分解，得到一组事实查询。

    Recent years have witnessed the sustained evolution of misinformation that aims at manipulating public opinions. Unlike traditional rumors or fake news editors who mainly rely on generated and/or counterfeited images, text and videos, current misinformation creators now more tend to use out-of-context multimedia contents (e.g. mismatched images and captions) to deceive the public and fake news detection systems. This new type of misinformation increases the difficulty of not only detection but also clarification, because every individual modality is close enough to true information. To address this challenge, in this paper we explore how to achieve interpretable cross-modal de-contextualization detection that simultaneously identifies the mismatched pairs and the cross-modal contradictions, which is helpful for fact-check websites to document clarifications. The proposed model first symbolically disassembles the text-modality information to a set of fact queries based on the Abstract M
    
[^63]: 不需重新搜索的研究：最大更新参数化可精确预测跨尺度的损失

    Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v1 [cs.CL])

    [http://arxiv.org/abs/2304.06875](http://arxiv.org/abs/2304.06875)

    提出了一种新的方法muP，可以提高超参数的缩放律的拟合精度，减少对大模型超参数的搜索，从而实现在大规模模型上进行损失预测。

    

    随着语言模型的扩大，验证研究想法变得越来越昂贵，因为小模型的结论不能简单地转移到大模型。解决方案是建立一个通用系统，仅基于小模型的结果和超参数直接预测大模型的一些指标。现有的基于缩放律的方法需要在最大的模型上进行超参数搜索，但由于资源有限，这是不切实际的。我们通过展示我们的发现表明，最大更新参数化（muP）使得可以在靠近常见损失流域的超参数的情况下准确拟合超参数的缩放律，而不需要任何搜索。因此，不同的模型可以在大尺度上进行损失预测，在训练开始之前就可以进行直接比较。我们提出了一种新的范式，作为可靠的学术研究的第一步，适用于任何模型规模，而不需大量的计算。代码将很快公开可用。

    As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones. A possible solution is to establish a generic system that directly predicts some metrics for large models solely based on the results and hyperparameters from small models. Existing methods based on scaling laws require hyperparameter search on the largest models, which is impractical with limited resources. We address this issue by presenting our discoveries indicating that Maximal Update parametrization (muP) enables accurate fitting of scaling laws for hyperparameters close to common loss basins, without any search. Thus, different models can be directly compared on large scales with loss prediction even before the training starts. We propose a new paradigm as a first step towards reliable academic research for any model scale without heavy computation. Code will be publicly available shortly.
    

