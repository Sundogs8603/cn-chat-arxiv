# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities.](http://arxiv.org/abs/2308.02490) | MM-Vet是一个评估标准，用于评估大型多模态模型在复杂任务上的综合能力。该标准解决了如何结构化和评估复杂多模态任务、设计适用于不同问题和回答类型的评估指标以及如何提供模型洞察的问题。通过整合不同的核心视觉-语言能力，MM-Vet展示了有趣的能力和解决复杂任务的方法。 |
| [^2] | [Adapting the NICT-JLE Corpus for Disfluency Detection Models.](http://arxiv.org/abs/2308.02482) | 本研究将NICT-JLE语料库适应为适用于去误检测模型训练和评估的格式，解决了学习者语音去误检测研究中数据集访问限制的问题。 |
| [^3] | [Towards Generalist Foundation Model for Radiology.](http://arxiv.org/abs/2308.02463) | 本研究旨在为放射学构建通用基础模型，提出了一个大规模的医学多模态数据集和支持不同放射学任务的架构，同时提出了一个新的评估基准。 |
| [^4] | [From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence.](http://arxiv.org/abs/2308.02448) | 这项研究探讨了从军事到医疗保健领域采纳和扩展伦理原则以应用生成式人工智能的可行性和重要性。 |
| [^5] | [Performance of Large Language Models in a Computer Science Degree Program.](http://arxiv.org/abs/2308.02432) | 该论文研究了在一所应用科学大学的计算机科学本科课程中不同大型语言模型的性能。通过将它们作为教学辅助工具，在不同计算机科学领域的课程中进行评估，展示了这些模型的强大性能，同时强调了在这样一个学位课程的背景下的限制和约束。 |
| [^6] | [Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text.](http://arxiv.org/abs/2308.02357) | Text2KGBench是一种用于评估语言模型根据本体从自然语言文本中生成知识图谱的能力的基准测试工具，在两个数据集上通过七个评估指标来衡量事实提取的性能。 |
| [^7] | [Dataflow Dialogue Generation.](http://arxiv.org/abs/2308.02323) | 本文展示了在数据流对话范式下的任务导向的对话生成。在MultiWOZ领域中，展示了基于议程的对话生成示例；在SMCalFlow领域中，展示了没有议程的对话生成示例，并且通过使用生成的对话来增强翻译训练数据集，提高了用户请求到数据流表达式的翻译准确性。 |
| [^8] | [Learning to Select the Relevant History Turns in Conversational Question Answering.](http://arxiv.org/abs/2308.02294) | 这篇论文提出了一个名为DHS-ConvQA的框架，用于在会话式问答中动态选择相关的历史转折点，以指导答案的准确预测。 |
| [^9] | [Redundancy Aware Multi-Reference Based Gainwise Evaluation of Extractive Summarization.](http://arxiv.org/abs/2308.02270) | 本文提出了一种冗余感知的Sem-nCG度量，用于评估模型摘要与多个参考摘要进行对比。实验结果表明，这种新度量具有更高的相关性。 |
| [^10] | [Efficient Monaural Speech Enhancement using Spectrum Attention Fusion.](http://arxiv.org/abs/2308.02263) | 本文提出了一种名为Spectrum Attention Fusion的方法，通过使用卷积模块替代自注意力层，实现了在语音增强任务中降低模型复杂度的目标，并在实验中取得了与当前最优模型相媲美的结果。 |
| [^11] | [Sinhala-English Parallel Word Dictionary Dataset.](http://arxiv.org/abs/2308.02234) | 对于资源稀缺的语言，开发精细的对应词典数据集是更可行的做法，可以用于中级任务，如受监督的多语言词嵌入对齐，并进一步指导用于机器翻译的高级任务。这篇论文提出了一个用于Sinhala-English平行词典数据集的开源工具。 |
| [^12] | [Learning to Paraphrase Sentences to Different Complexity Levels.](http://arxiv.org/abs/2308.02226) | 本论文提出了学习将句子改写为不同复杂程度的方法，通过比较不同的数据集和采用多任务和提示策略，实验结果表明我们的模型在句子简化和同级改写任务上取得了最先进的性能，并在没有训练数据的情况下评估了一些大型语言模型的效果。 |
| [^13] | [ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation.](http://arxiv.org/abs/2308.02223) | 提出了两阶段采样和动态采样方法来提高训练序列生成模型的采样效率，实验证明这种方法在传统的序列生成任务中表现良好，同时在人类反馈强化学习方面也有很好的效果。 |
| [^14] | [A Survey of Spanish Clinical Language Models.](http://arxiv.org/abs/2308.02199) | 这项调查研究了西班牙语临床语言模型的应用，回顾了17个专注于临床任务的语料库的贡献，并对最相关的西班牙语语言模型和西班牙临床语言模型进行了彻底比较，提供了3000多个进行微调的模型。为了便于未来的研究和挑战，所有测试过的语料库和最佳模型都被以可访问的方式公开。 |
| [^15] | [Explaining Relation Classification Models with Semantic Extents.](http://arxiv.org/abs/2308.02193) | 本研究提出了一种解释关系分类模型的方法，即使用语义范围分析模型的决策模式。语义范围是关于分类决策的文本中最有影响力的部分。通过将人类和模型的语义范围进行比较，发现模型往往从数据中学习到了快捷模式。 |
| [^16] | [Emo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus Speech Emotion Recognition.](http://arxiv.org/abs/2308.02190) | Emo-DNA是一种新颖的方法，通过对比情感解耦和双层情感对齐实现了跨语料库语音情感识别中的情感相关特征的学习。 |
| [^17] | [From Fake to Hyperpartisan News Detection Using Domain Adaptation.](http://arxiv.org/abs/2308.02185) | 本研究探索了在虚假新闻检测和偏执新闻检测之间应用无监督领域适应技术的效果，通过知识传递和数据增强提高了性能，同时将聚类和主题建模算法与UDA相结合取得了更好的结果。 |
| [^18] | [Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology.](http://arxiv.org/abs/2308.02180) | 本文研究了使用大型语言模型（LLMs）扩展临床试验匹配的方法，并以肿瘤学为案例研究。研究结果显示，先进的LLMs能够处理临床试验的复杂条件和匹配逻辑，相较于之前的方法，性能显著提升，并可作为人工辅助筛选患者-试验候选人的初步解决方案。 |
| [^19] | [You talk what you read: Understanding News Comment Behavior by Dispositional and Situational Attribution.](http://arxiv.org/abs/2308.02168) | 本文通过考虑来自新闻互动历史的归宿因素和来自相应新闻的场合因素，提出了一个编码器-解码器框架来理解新闻评论行为，同时验证了该方法在读者感知新闻摘要和新闻方面-观点预测中的应用价值。 |
| [^20] | [Speaker Diarization of Scripted Audiovisual Content.](http://arxiv.org/abs/2308.02160) | 本文介绍了一种用于发言人日化任务的新方法，利用制作脚本提取伪标签数据，并相对于无监督基线模型实现了51.7%的改进。 |
| [^21] | [Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization.](http://arxiv.org/abs/2308.02151) | 本文介绍了一种通过策略梯度优化的回顾性大型语言代理框架，该框架通过学习环境反馈来调整语言代理的提示，从而优化其性能。这种代理能够从多个环境和任务中学习奖励，并通过总结以前任务的根本原因来改进语言代理提示。 |
| [^22] | [Tweet Insights: A Visualization Platform to Extract Temporal Insights from Twitter.](http://arxiv.org/abs/2308.02142) | 该论文介绍了一个名为Tweet Insights的可视化平台，可以从Twitter中提取时间相关信息。它使用词嵌入技术和专门优化的语言模型对Twitter的大量时间序列数据进行后处理，并通过界面对时间进行分析，以检测和描述意义转变的变化，包括与趋势度量相关的补充信息，如情感和主题的变化。 |
| [^23] | [ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP.](http://arxiv.org/abs/2308.02122) | ParaFuzz是一种基于可解释性的技术，用于检测自然语言处理中的毒样本。该技术通过观察模型在重写过的干净样本和污染样本上的预测稳定性，来判断样本是否被污染。 |
| [^24] | [Chinese Financial Text Emotion Mining: GCGTS -- A Character Relationship-based Approach for Simultaneous Aspect-Opinion Pair Extraction.](http://arxiv.org/abs/2308.02113) | 本论文提出了一种名为GCGTS的基于字符关系的方法，用于从中文金融文本中同时提取方面-观点对。该方法利用图卷积网络（GCN）明确地整合了句法结构，并统一了词内字符的编码，以提高提取的准确性和效果。 |
| [^25] | [Prompt2Gaussia: Uncertain Prompt-learning for Script Event Prediction.](http://arxiv.org/abs/2308.02103) | 本文提出了Prompt2Gaussia方法，通过不确定性提示学习和高斯分布来解决脚本事件预测中的无法确定最优提示和标签的问题。 |
| [^26] | [N-gram Boosting: Improving Contextual Biasing with Normalized N-gram Targets.](http://arxiv.org/abs/2308.02092) | 本文提出了一种N-gram增强技术，通过规范化目标词组来改善上下文偏差，提高关键词识别率。 |
| [^27] | [Causality Guided Disentanglement for Cross-Platform Hate Speech Detection.](http://arxiv.org/abs/2308.02080) | 本研究提出了一种跨平台仇恨言论检测模型，通过解缠输入表示为不变特征和平台相关特征，实现了对多个未见平台的良好泛化能力。 |
| [^28] | [Seasonality Based Reranking of E-commerce Autocomplete Using Natural Language Queries.](http://arxiv.org/abs/2308.02055) | 本文提出了一种基于神经网络的自然语言处理算法，用于将季节性作为信号来重新排序电子商务的自动完成功能，从而提高自动完成的相关性和业务指标。 |
| [^29] | [The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations.](http://arxiv.org/abs/2308.02053) | 通过职位推荐分析了大型语言模型（LLMs）的人口统计偏见，发现这些模型对于墨西哥工人一直建议低薪工作，并向女性更倾向于推荐秘书职位。这项研究强调了理解LLMs偏见的重要性。 |
| [^30] | [Artificial Intelligence in archival and historical scholarship workflow: HTS and ChatGPT.](http://arxiv.org/abs/2308.02044) | 本文研究了人工智能在档案数字化过程中的应用，重点是自动转录和校正手稿，以及标准化文本。通过测试ChatGPT系统，在对信函进行文本标准化时取得了一定效果。总体而言，数字化和人工智能可以显著提升档案和历史研究能力。 |
| [^31] | [Proposing a conceptual framework: social media listening for public health behavior.](http://arxiv.org/abs/2308.02037) | 该论文提出了一个基于理论的概念框架，旨在解决社交媒体数据和自然语言处理技术在社交监听和错误信息研究中的应用问题。 |
| [^32] | [What Twitter Data Tell Us about the Future?.](http://arxiv.org/abs/2308.02035) | 本研究通过研究Twitter上未来主义者对未来的预期，探究语言提示对社交媒体用户的预测性思维的影响，并开发了一个可扩展的自然语言处理流程和数据集。 |
| [^33] | [Efficient Sentiment Analysis: A Resource-Aware Evaluation of Feature Extraction Techniques, Ensembling, and Deep Learning Models.](http://arxiv.org/abs/2308.02022) | 本文对于文档级情感分析模型进行了综合评估，重点考虑了资源成本和环境意识。研究发现，在资源消耗较低的配置下，准确性损失较小。这对于资源有限的环境下的模型部署具有重要意义。 |
| [^34] | [Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty.](http://arxiv.org/abs/2308.02019) | 本文提出了一种从在小数据集上训练的教师中进行知识蒸馏的方法，并证明当教师模型在足够小的数据集上训练时，蒸馏可以保持甚至超过教师模型的性能。 |
| [^35] | [Federated Representation Learning for Automatic Speech Recognition.](http://arxiv.org/abs/2308.02013) | 使用联邦学习和自监督学习的结合方法，研究了面向自动语音识别的联邦表示学习，将边缘设备上的隐私数据用于学习健壮的音频表示，并取得了显著的性能改善。 |
| [^36] | [Bengali Fake Reviews: A Benchmark Dataset and Detection System.](http://arxiv.org/abs/2308.01987) | 这篇论文介绍了孟加拉假评论检测的第一个公开数据集，提出了一种独特的转换流水线，以识别孟加拉假评论，并进行了严格的实验。 |
| [^37] | [Domain specificity and data efficiency in typo tolerant spell checkers: the case of search in online marketplaces.](http://arxiv.org/abs/2308.01976) | 本研究针对在线市场中拼写错误的问题，通过数据增强方法和递归神经网络实现了领域特定的拼写检查器，并将其应用在微软AppSource市场的实时推断API中。我们的数据有效解决方案表明，控制高质量的合成数据可能成为一个有力的工具，特别是考虑到当前大语言模型所依赖的巨大且难以控制的数据集。 |
| [^38] | [DCTM: Dilated Convolutional Transformer Model for Multimodal Engagement Estimation in Conversation.](http://arxiv.org/abs/2308.01966) | 这项研究引入了一种扩张卷积转换模型，用于在MULTIMEDIATE 2023竞赛中建模和估计多模态对话中的人类参与度。在测试集上该系统比基准模型表现出显著的7%改进，在验证集上表现为4%改进。此外，使用简单的串联方法与自注意力融合可以获得最佳性能。 |
| [^39] | [Why Do We Need Neuro-symbolic AI to Model Pragmatic Analogies?.](http://arxiv.org/abs/2308.01936) | 本文讨论了神经符号人工智能在处理逐渐复杂的类比推理时的必要性，以提供超越文字内容的广泛、多样化的知识，并结合统计和符号人工智能技术来增强和引导映射过程。 |
| [^40] | [MultiEM: Efficient and Effective Unsupervised Multi-Table Entity Matching.](http://arxiv.org/abs/2308.01927) | 本论文提出了一种称为MultiEM的高效有效无监督多表实体匹配解决方案，包括增强实体表示、表级层次合并和基于密度的剪枝，并通过大量实验证明了其有效性和效率。 |
| [^41] | [Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models.](http://arxiv.org/abs/2308.01404) | 通过引入一款名为"Hoodwinked"的文本游戏，研究了当前语言模型是否具有欺骗和识别谎言的能力。实验证据表明，杀手经常否认罪行并指责他人，导致投票结果受到影响。更先进的模型在杀手效果上表现出优势。实验证据表明，这种改进是通过在讨论中更强的欺骗能力实现的。 |
| [^42] | [Grounded Image Text Matching with Mismatched Relation Reasoning.](http://arxiv.org/abs/2308.01236) | 本文介绍了一种新颖的基于视觉和语言的图像文本匹配任务，要求模型能够确定图像与文本的关系，并定位所指的对象或者对不匹配的部分进行 grounding。为了解决预训练模型在此任务上的数据效率和长度泛化能力不足的问题，我们提出了一种关系敏感的对应推理网络（RCRN），该网络通过双向信息传递和语言结构引导的关系感知推理来实现。 |
| [^43] | [Turkish Native Language Identification.](http://arxiv.org/abs/2307.14850) | 这项研究首次将母语识别应用于土耳其语,通过分析作者不同语言的写作来预测作者的母语。研究使用了土耳其学习者语料库和三个句法特征来展示其有效性。 |
| [^44] | [Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution.](http://arxiv.org/abs/2307.00925) | 本研究首次使用语法演化自动设计语义相似性集合，通过自动选择和聚合候选度量来优化集合与人类判断的相关性，提高相似度评估准确性，并证明了使用集合对语义相似性任务的益处。 |
| [^45] | [Inductive reasoning in humans and large language models.](http://arxiv.org/abs/2306.06548) | 本研究使用GPT-3.5和GPT-4对人类归纳推理中的属性归纳问题进行了实验。结果表明，尽管GPT-3.5有一些困难，但GPT-4的表现与人类相似，除了未能捕捉到前提的非单调性现象。这项工作为人类和机器智能提供了有趣的比较，并提供了用作未来研究基准的两个大型数据集。 |
| [^46] | [Mitigating Label Biases for In-context Learning.](http://arxiv.org/abs/2305.19148) | 本文针对上下文学习（ICL）中的三种标签偏差提出分类法，并提出一种简单的偏差校准方法，使用随机的领域词估算语言模型的标签偏差。 |
| [^47] | [Mapping ChatGPT in Mainstream Media: Early Quantitative Insights through Sentiment Analysis and Word Frequency Analysis.](http://arxiv.org/abs/2305.18340) | 本文量化分析了主流媒体对ChatGPT和人工智能的报道趋势和情感态度，发现人们普遍对其持积极态度。然而，主题的词频分析显示，大型科技问题和行为者得到了高度关注，而就业、多样性、伦理、版权、性别和女性等主题则表现不足或完全缺失。本文呼吁在人工智能领域需要更加多元和细致的媒体发言。 |
| [^48] | [G3Detector: General GPT-Generated Text Detector.](http://arxiv.org/abs/2305.12680) | G3Detector是一种通用GPT生成文本检测器，能够准确识别多个领域中的合成文本，并且具备在各种模型架构和解码策略下均表现出优异性能的能力。 |
| [^49] | [Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation.](http://arxiv.org/abs/2303.17910) | 本论文介绍在非自回归神经机器翻译中引入选择性知识蒸馏和渐进蒸馏方法，并在实验中证明该方法可以在NAT模型的训练数据质量和复杂度之间实现灵活权衡，有助于NAT超越基线。 |
| [^50] | [LMExplainer: a Knowledge-Enhanced Explainer for Language Models.](http://arxiv.org/abs/2303.16537) | LMExplainer是一种知识增强的语言模型解释模块，使用知识图和图注意力神经网络来提取关键决策信号，为用户提供可理解的解释。 |
| [^51] | [SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization.](http://arxiv.org/abs/2303.13035) | 研究通过引入软提示嵌入，提出Soft Prompt-Based Calibration (SPeC)管道，来减轻输入变量对输出多样性的影响，降低性能变异. 此方法不仅比大语言模型(LLM)性能稳定，而且在临床笔记摘要任务上表现优于最先进的模型. |
| [^52] | [Evaluation of ChatGPT as a Question Answering System for Answering Complex Questions.](http://arxiv.org/abs/2303.07992) | 本论文评估了基于ChatGPT模型的问答系统在回答复杂问题方面的能力，通过一个分类框架对潜在的问题特征进行分类，通过黑盒测试规范CheckList评估模型性能。 |
| [^53] | [Emergent Analogical Reasoning in Large Language Models.](http://arxiv.org/abs/2212.09196) | GPT-3在许多类比任务中表现出与甚至超越人类的能力，揭示了大型语言模型的紧急能力。 |
| [^54] | [Automatic WordNet Construction using Word Sense Induction through Sentence Embeddings.](http://arxiv.org/abs/2204.03251) | 本文提出了一种使用无标签语料库和基于句子嵌入的语言模型自动构建WordNet的方法。通过这种方法，我们生成了一个新的WordNet（FilWordNet），以替代并改进菲律宾语中过时的WordNet，并且不需要人工监督。 |

# 详细

[^1]: MM-Vet: 评估大型多模态模型的综合能力

    MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])

    [http://arxiv.org/abs/2308.02490](http://arxiv.org/abs/2308.02490)

    MM-Vet是一个评估标准，用于评估大型多模态模型在复杂任务上的综合能力。该标准解决了如何结构化和评估复杂多模态任务、设计适用于不同问题和回答类型的评估指标以及如何提供模型洞察的问题。通过整合不同的核心视觉-语言能力，MM-Vet展示了有趣的能力和解决复杂任务的方法。

    

    我们提出了MM-Vet，一个评估标准，用于检查在复杂多模态任务上的大型多模态模型（LMM）的表现。最近的LMM展示了各种有趣的能力，例如解决书写在黑板上的数学问题，推理新闻图片中的事件和名人，以及解释视觉笑话。快速的模型进步给评估标准的开发带来了挑战。问题包括：（1）如何系统地构建和评估复杂的多模态任务；（2）如何设计适用于不同类型问题和回答的评估指标；（3）如何给出超出简单性能排名的模型洞察。为此，我们提出了MM-Vet，基于这样一个洞察：解决复杂任务的有趣能力通常通过一种通才模型能够整合不同的核心视觉-语言（VL）能力来实现。MM-Vet定义了6个核心VL能力，并检查了从这些能力组合中得出的16种有趣的整合方式。

    We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combin
    
[^2]: 将NICT-JLE语料库用于去误检测模型的适应性研究

    Adapting the NICT-JLE Corpus for Disfluency Detection Models. (arXiv:2308.02482v1 [cs.CL])

    [http://arxiv.org/abs/2308.02482](http://arxiv.org/abs/2308.02482)

    本研究将NICT-JLE语料库适应为适用于去误检测模型训练和评估的格式，解决了学习者语音去误检测研究中数据集访问限制的问题。

    

    去误检测是一种广泛研究的领域，常用于识别语音中的停顿、重复和错误开头等语言失利现象。通过使用Switchboard语料库进行评估的标准化流程，可以轻松比较不同方法的模型性能。然而，对于学习者语音的去误检测研究而言，这样的数据集存在访问限制，使得比较和改进模型的后续开发更具挑战性。为了解决这个问题，本文描述了将包含约300小时英语学习者口语水平测试的NICT-JLE语料库适应为适用于去误检测模型训练和评估的格式的过程。研究探讨了NICT-JLE和Switchboard语料库之间的不同之处，并详细介绍了对NICT-JLE语料库的标签集和元特征的调整。这项研究的结果提供了标准化的训练、验证和测试数据集。

    The detection of disfluencies such as hesitations, repetitions and false starts commonly found in speech is a widely studied area of research. With a standardised process for evaluation using the Switchboard Corpus, model performance can be easily compared across approaches. This is not the case for disfluency detection research on learner speech, however, where such datasets have restricted access policies, making comparison and subsequent development of improved models more challenging. To address this issue, this paper describes the adaptation of the NICT-JLE corpus, containing approximately 300 hours of English learners' oral proficiency tests, to a format that is suitable for disfluency detection model training and evaluation. Points of difference between the NICT-JLE and Switchboard corpora are explored, followed by a detailed overview of adaptations to the tag set and meta-features of the NICT-JLE corpus. The result of this work provides a standardised train, heldout and test se
    
[^3]: 为放射学构建通用基础模型的探索

    Towards Generalist Foundation Model for Radiology. (arXiv:2308.02463v1 [cs.CV])

    [http://arxiv.org/abs/2308.02463](http://arxiv.org/abs/2308.02463)

    本研究旨在为放射学构建通用基础模型，提出了一个大规模的医学多模态数据集和支持不同放射学任务的架构，同时提出了一个新的评估基准。

    

    本研究旨在启动放射学基础模型的开发，称为RadFM。我们从数据、模型设计和评估的角度全面考虑了基础模型的构建。我们的贡献可总结如下：（i）构建了一个大规模的医学多模态数据集MedMD，包括1600万个2D和3D医学扫描。据我们所知，这是第一个包含3D医学扫描的多模态数据集。（ii）我们提出了一种架构，使得可视条件生成预训练成为可能，可以将文本输入与2D或3D医学扫描交错，生成不同放射学任务的响应。该模型首先在MedMD上进行了预训练，然后在RadMD上进行了特定领域的微调，RadMD是MedMD的放射学清理版本，包含300万个放射学的视觉语言对。（iii）我们提出了一个新的评估基准，包括五个任务，旨在全面评估该模型的能力。

    In this study, we aim to initiate the development of Radiology Foundation Model, termed as RadFM.We consider the construction of foundational models from the perspectives of data, model design, and evaluation thoroughly. Our contribution can be concluded as follows: (i), we construct a large-scale Medical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans. To the best of our knowledge, this is the first multi-modal dataset containing 3D medical scans. (ii), We propose an architecture that enables visually conditioned generative pre-training, allowing for the integration of text input interleaved with 2D or 3D medical scans to generate response for diverse radiologic tasks. The model was initially pre-trained on MedMD and subsequently domain-specific fine-tuned on RadMD, a radiologic cleaned version of MedMD, containing 3M radiologic visual-language pairs. (iii), we propose a new evaluation benchmark that comprises five tasks, aiming to comprehensively assess the capa
    
[^4]: 从军事到医疗保健：采纳和扩展用于生成式人工智能的伦理原则

    From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence. (arXiv:2308.02448v1 [cs.CY])

    [http://arxiv.org/abs/2308.02448](http://arxiv.org/abs/2308.02448)

    这项研究探讨了从军事到医疗保健领域采纳和扩展伦理原则以应用生成式人工智能的可行性和重要性。

    

    在2020年，美国国防部正式公布了一套指导未来战场上人工智能技术使用的伦理原则。尽管存在明显差异，但军事和医疗服务之间存在核心的相似之处。战场上的战士经常面临需要快速决策的改变生活的情况。医疗服务提供者在快速变化的医疗环境中也面临类似的挑战，例如在急诊科或治疗危及生命的状况下进行手术。生成式人工智能是一种新兴技术，旨在高效生成有价值的信息，具有巨大潜力。随着计算能力的日益普及和大量的健康数据（如电子健康记录、心电图和医学图像）的增加，医疗保健领域必将被这项技术革命化。最近，生成式人工智能在研究界引起了广泛关注，引发了关于其伦理问题的讨论。

    In 2020, the U.S. Department of Defense officially disclosed a set of ethical principles to guide the use of Artificial Intelligence (AI) technologies on future battlefields. Despite stark differences, there are core similarities between the military and medical service. Warriors on battlefields often face life-altering circumstances that require quick decision-making. Medical providers experience similar challenges in a rapidly changing healthcare environment, such as in the emergency department or during surgery treating a life-threatening condition. Generative AI, an emerging technology designed to efficiently generate valuable information, holds great promise. As computing power becomes more accessible and the abundance of health data, such as electronic health records, electrocardiograms, and medical images, increases, it is inevitable that healthcare will be revolutionized by this technology. Recently, generative AI has captivated the research community, leading to debates about 
    
[^5]: 在计算机科学学位课程中大型语言模型的性能

    Performance of Large Language Models in a Computer Science Degree Program. (arXiv:2308.02432v1 [cs.CY])

    [http://arxiv.org/abs/2308.02432](http://arxiv.org/abs/2308.02432)

    该论文研究了在一所应用科学大学的计算机科学本科课程中不同大型语言模型的性能。通过将它们作为教学辅助工具，在不同计算机科学领域的课程中进行评估，展示了这些模型的强大性能，同时强调了在这样一个学位课程的背景下的限制和约束。

    

    ChatGPT-3.5和GPT-4.0等大型语言模型在当前的讨论中无处不在并且占据主导地位。它们具有变革性的能力已经引起了我们与（基于文本的）信息互动和利用的范式转变。每天都有新的可能性来利用这些模型的能力。本文介绍了在一所应用科学大学的计算机科学本科课程中不同大型语言模型的性能研究结果。我们的主要目标是通过将它们作为教学辅助工具在课程中使用来评估这些模型的有效性。通过使用课堂材料、练习任务和过去的考试来启发模型，我们旨在评估它们在不同计算机科学领域的熟练程度。我们展示了当前大型语言模型的强大性能，同时强调了在这样一个学位课程的背景下的限制和约束。

    Large language models such as ChatGPT-3.5 and GPT-4.0 are ubiquitous and dominate the current discourse. Their transformative capabilities have led to a paradigm shift in how we interact with and utilize (text-based) information. Each day, new possibilities to leverage the capabilities of these models emerge. This paper presents findings on the performance of different large language models in a university of applied sciences' undergraduate computer science degree program. Our primary objective is to assess the effectiveness of these models within the curriculum by employing them as educational aids. By prompting the models with lecture material, exercise tasks, and past exams, we aim to evaluate their proficiency across different computer science domains. We showcase the strong performance of current large language models while highlighting limitations and constraints within the context of such a degree program. We found that ChatGPT-3.5 averaged 79.9% of the total score in 10 tested 
    
[^6]: Text2KGBench：一种从文本生成本体驱动的知识图谱的基准测试

    Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text. (arXiv:2308.02357v1 [cs.CL])

    [http://arxiv.org/abs/2308.02357](http://arxiv.org/abs/2308.02357)

    Text2KGBench是一种用于评估语言模型根据本体从自然语言文本中生成知识图谱的能力的基准测试工具，在两个数据集上通过七个评估指标来衡量事实提取的性能。

    

    近年来，大型语言模型(LLM)和具有新兴能力的基础模型的进展已经证明可以改善许多自然语言处理任务的性能。LLM和知识图谱(KG)可以相互补充，LLM可以用于KG的构建或补全，而现有的KG可以用于不同的任务，例如使LLM的输出更易解释或进行类脑符号化的事实检查。本文提出了Text2KGBench，一种用于评估语言模型根据本体从自然语言文本中生成知识图谱的能力的基准测试。给定一个输入本体和一组句子，任务是从文本中提取事实，同时符合给定的本体(概念、关系、域/值范围约束)并忠实于输入句子。我们提供了两个数据集：(i)具有10个本体和13,474个句子的Wikidata-TekGen和(ii)具有19个本体和4,860个句子的DBpedia-WebNLG。我们定义了七个评估指标来衡量事实提取的性能。

    The recent advances in large language models (LLM) and foundation models with emergent capabilities have been shown to improve the performance of many NLP tasks. LLMs and Knowledge Graphs (KG) can complement each other such that LLMs can be used for KG construction or completion while existing KGs can be used for different tasks such as making LLM outputs explainable or fact-checking in Neuro-Symbolic manner. In this paper, we present Text2KGBench, a benchmark to evaluate the capabilities of language models to generate KGs from natural language text guided by an ontology. Given an input ontology and a set of sentences, the task is to extract facts from the text while complying with the given ontology (concepts, relations, domain/range constraints) and being faithful to the input sentences. We provide two datasets (i) Wikidata-TekGen with 10 ontologies and 13,474 sentences and (ii) DBpedia-WebNLG with 19 ontologies and 4,860 sentences. We define seven evaluation metrics to measure fact 
    
[^7]: 数据流对话生成

    Dataflow Dialogue Generation. (arXiv:2308.02323v1 [cs.CL])

    [http://arxiv.org/abs/2308.02323](http://arxiv.org/abs/2308.02323)

    本文展示了在数据流对话范式下的任务导向的对话生成。在MultiWOZ领域中，展示了基于议程的对话生成示例；在SMCalFlow领域中，展示了没有议程的对话生成示例，并且通过使用生成的对话来增强翻译训练数据集，提高了用户请求到数据流表达式的翻译准确性。

    

    我们在数据流对话范式中展示了任务导向的对话生成。我们展示了在MultiWOZ领域中基于议程的对话生成示例，以及在SMCalFlow领域中没有议程的生成示例，其中我们展示了使用生成的对话增强翻译训练数据集时，用户请求到数据流表达式的翻译准确性的提升。

    We demonstrate task-oriented dialogue generation within the dataflow dialogue paradigm. We show an example of agenda driven dialogue generation for the MultiWOZ domain, and an example of generation without an agenda for the SMCalFlow domain, where we show an improvement in the accuracy of the translation of user requests to dataflow expressions when the generated dialogues are used to augment the translation training dataset.
    
[^8]: 学习选择会话问答中相关的历史对话转折点

    Learning to Select the Relevant History Turns in Conversational Question Answering. (arXiv:2308.02294v1 [cs.CL])

    [http://arxiv.org/abs/2308.02294](http://arxiv.org/abs/2308.02294)

    这篇论文提出了一个名为DHS-ConvQA的框架，用于在会话式问答中动态选择相关的历史转折点，以指导答案的准确预测。

    

    对于基于网络的数字助手的日益需求，引起了信息检索(IR)社区对会话式问答(ConvQA)领域的兴趣。然而，ConvQA的一个关键方面是有效选择会话历史转折点以回答当前问题。相关历史选择与正确答案预测之间的依赖关系是一个有趣但未被充分探索的领域。选择的相关上下文可以更好地指导系统在文章中寻找答案的确切位置。而不相关的上下文则给系统带来噪音，从而导致模型性能下降。本文提出了一个框架DHS-ConvQA（会话问答中的动态历史选择），首先为所有历史转折点生成上下文和问题实体，然后根据它们与问题的相似度进行修剪。

    The increasing demand for the web-based digital assistants has given a rapid rise in the interest of the Information Retrieval (IR) community towards the field of conversational question answering (ConvQA). However, one of the critical aspects of ConvQA is the effective selection of conversational history turns to answer the question at hand. The dependency between relevant history selection and correct answer prediction is an intriguing but under-explored area. The selected relevant context can better guide the system so as to where exactly in the passage to look for an answer. Irrelevant context, on the other hand, brings noise to the system, thereby resulting in a decline in the model's performance. In this paper, we propose a framework, DHS-ConvQA (Dynamic History Selection in Conversational Question Answering), that first generates the context and question entities for all the history turns, which are then pruned on the basis of similarity they share in common with the question at
    
[^9]: 基于冗余感知的多参考增益评估提取性摘要

    Redundancy Aware Multi-Reference Based Gainwise Evaluation of Extractive Summarization. (arXiv:2308.02270v1 [cs.CL])

    [http://arxiv.org/abs/2308.02270](http://arxiv.org/abs/2308.02270)

    本文提出了一种冗余感知的Sem-nCG度量，用于评估模型摘要与多个参考摘要进行对比。实验结果表明，这种新度量具有更高的相关性。

    

    尽管ROUGE指标在评估提取性摘要任务中非常流行，但长期以来被批评缺乏语义意识，并对摘要生成器的排名质量无视。感谢之前的研究通过提出一种称为Sem-nCG的基于增益的自动度量来解决这些问题，它既具有排名和语义的意识。然而，Sem-nCG不考虑模型生成的摘要中存在的冗余数量，目前也不支持使用多个参考摘要进行评估。不幸的是，同时解决这两个限制并不容易。因此，在本文中，我们提出了一种冗余感知的Sem-nCG度量，并展示了如何使用这个新度量来评估模型摘要与多个参考摘要进行对比。我们还通过大量实验探索了将冗余纳入原始度量中的不同方式。实验结果表明，这种新的冗余感知度量具有更高的相关性。

    While very popular for evaluating extractive summarization task, the ROUGE metric has long been criticized for its lack of semantic awareness and its ignorance about the ranking quality of the summarizer. Thanks to previous research that has addressed these issues by proposing a gain-based automated metric called Sem-nCG, which is both rank and semantic aware. However, Sem-nCG does not consider the amount of redundancy present in a model-generated summary and currently does not support evaluation with multiple reference summaries. Unfortunately, addressing both these limitations simultaneously is not trivial. Therefore, in this paper, we propose a redundancy-aware Sem-nCG metric and demonstrate how this new metric can be used to evaluate model summaries against multiple references. We also explore different ways of incorporating redundancy into the original metric through extensive experiments. Experimental results demonstrate that the new redundancy-aware metric exhibits a higher corr
    
[^10]: 使用频谱注意力融合的高效单声道语音增强

    Efficient Monaural Speech Enhancement using Spectrum Attention Fusion. (arXiv:2308.02263v1 [cs.SD])

    [http://arxiv.org/abs/2308.02263](http://arxiv.org/abs/2308.02263)

    本文提出了一种名为Spectrum Attention Fusion的方法，通过使用卷积模块替代自注意力层，实现了在语音增强任务中降低模型复杂度的目标，并在实验中取得了与当前最优模型相媲美的结果。

    

    语音增强是自动语音处理流程中一项需求较高的任务，着重于分离干净的语音信号和嘈杂的信道。最近，基于Transformer的模型在语音增强中表现优于RNN和CNN模型，然而与此同时，它们的计算复杂度更高，需要更多高质量的训练数据，而这在实际情况中往往很难获得。本文提出了一种改进的语音增强模型，通过称之为频谱注意力融合的方法，在显著降低模型复杂度的同时保持了自注意力的表达能力。我们巧妙地构建了一个卷积模块来替代语音Transformer中的若干自注意力层，使模型能够更高效地融合频谱特征。在Voice Bank + DEMAND数据集上，我们的提出的模型能够在参数显著减少（0.58M）的情况下实现与SOTA模型相媲美甚至更好的结果。

    Speech enhancement is a demanding task in automated speech processing pipelines, focusing on separating clean speech from noisy channels. Transformer based models have recently bested RNN and CNN models in speech enhancement, however at the same time they are much more computationally expensive and require much more high quality training data, which is always hard to come by. In this paper, we present an improvement for speech enhancement models that maintains the expressiveness of self-attention while significantly reducing model complexity, which we have termed Spectrum Attention Fusion. We carefully construct a convolutional module to replace several self-attention layers in a speech Transformer, allowing the model to more efficiently fuse spectral features. Our proposed model is able to achieve comparable or better results against SOTA models but with significantly smaller parameters (0.58M) on the Voice Bank + DEMAND dataset.
    
[^11]: Sinhala-English平行词典数据集

    Sinhala-English Parallel Word Dictionary Dataset. (arXiv:2308.02234v1 [cs.CL])

    [http://arxiv.org/abs/2308.02234](http://arxiv.org/abs/2308.02234)

    对于资源稀缺的语言，开发精细的对应词典数据集是更可行的做法，可以用于中级任务，如受监督的多语言词嵌入对齐，并进一步指导用于机器翻译的高级任务。这篇论文提出了一个用于Sinhala-English平行词典数据集的开源工具。

    

    平行数据集对于执行和评估任何多语言任务都至关重要。然而，在考虑的语言对中，如果一个语言是资源稀缺的语言，现有的自上而下的平行数据，例如语料库，在数量和质量方面都缺乏，这是由于人工注释的缺乏。因此，对于资源稀缺的语言来说，更可行的做法是先朝向自下而上的方向发展，开发精细的对应词典数据集。然后，可以将其用于中级任务，如受监督的多语言词嵌入对齐。这反过来又可以指导高级任务的进行，如用于机器翻译（MT）的句子或段落文本语料库的对齐。尽管比为资源稀缺的语言生成和对齐大规模语料库更容易，但由于来自更大的研究实体的冷淡，甚至这些精细的对应数据集对于某些资源稀缺的语言也是缺乏的。我们已经观察到，存在一个需求，即构建一个用于Sinhala-English平行词典数据集的开源工具。

    Parallel datasets are vital for performing and evaluating any kind of multilingual task. However, in the cases where one of the considered language pairs is a low-resource language, the existing top-down parallel data such as corpora are lacking in both tally and quality due to the dearth of human annotation. Therefore, for low-resource languages, it is more feasible to move in the bottom-up direction where finer granular pairs such as dictionary datasets are developed first. They may then be used for mid-level tasks such as supervised multilingual word embedding alignment. These in turn can later guide higher-level tasks in the order of aligning sentence or paragraph text corpora used for Machine Translation (MT). Even though more approachable than generating and aligning a massive corpus for a low-resource language, for the same reason of apathy from larger research entities, even these finer granular data sets are lacking for some low-resource languages. We have observed that there 
    
[^12]: 学习将句子改写为不同的复杂程度

    Learning to Paraphrase Sentences to Different Complexity Levels. (arXiv:2308.02226v1 [cs.CL])

    [http://arxiv.org/abs/2308.02226](http://arxiv.org/abs/2308.02226)

    本论文提出了学习将句子改写为不同复杂程度的方法，通过比较不同的数据集和采用多任务和提示策略，实验结果表明我们的模型在句子简化和同级改写任务上取得了最先进的性能，并在没有训练数据的情况下评估了一些大型语言模型的效果。

    

    虽然句子简化是自然语言处理中的一个研究热点，但句子复杂化和同级改写这两个相邻任务却没有得到足够的关注。为了训练模型在这三个任务上，我们提出了两个新的无监督数据集。我们将这两个数据集与一个单一的有监督数据集进行比较，一个数据集由一个弱分类器标注，另一个数据集由一种基于规则的方法标注。使用这三个数据集进行训练，在多任务和提示策略上进行了大量实验。与其他使用无监督平行数据训练的系统相比，我们训练的模型在ASSET简化基准测试中取得了最先进的性能。我们的模型也优于先前的句子级目标化工作。最后，我们还确定了一些大型语言模型在零-shot设置下在这些任务上的表现。

    While sentence simplification is an active research topic in NLP, its adjacent tasks of sentence complexification and same-level paraphrasing are not. To train models on all three tasks, we present two new unsupervised datasets. We compare these datasets, one labeled by a weak classifier and the other by a rule-based approach, with a single supervised dataset. Using these three datasets for training, we perform extensive experiments on both multitasking and prompting strategies. Compared to other systems trained on unsupervised parallel data, models trained on our weak classifier labeled dataset achieve state-of-the-art performance on the ASSET simplification benchmark. Our models also outperform previous work on sentence level targeting. Finally, we establish how a handful of Large Language Models perform on these tasks under a zero-shot setting.
    
[^13]: ESRL: 高效采样的基于强化学习的序列生成方法

    ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. (arXiv:2308.02223v1 [cs.CL])

    [http://arxiv.org/abs/2308.02223](http://arxiv.org/abs/2308.02223)

    提出了两阶段采样和动态采样方法来提高训练序列生成模型的采样效率，实验证明这种方法在传统的序列生成任务中表现良好，同时在人类反馈强化学习方面也有很好的效果。

    

    应用强化学习（RL）于序列生成模型能够直接优化长期回报（如BLEU和人类反馈），但通常需要对动作序列空间进行大规模采样。这在序列生成问题中是一个计算挑战，比如机器翻译，我们经常处理一个大的动作空间（如词汇表）和长的动作序列（如翻译）。在本工作中，我们引入了两阶段采样和动态采样方法，以提高训练序列生成模型的采样效率。我们在传统的序列生成任务上进行了实验，包括机器翻译和抽象摘要。此外，我们通过训练一个大型语言模型使用奖励模型，对我们的方法进行了RLHF评估。实验结果表明，这种高效采样的基于强化学习的方法

    Applying Reinforcement Learning (RL) to sequence generation models enables the direct optimization of long-term rewards (\textit{e.g.,} BLEU and human feedback), but typically requires large-scale sampling over a space of action sequences. This is a computational challenge as presented by the practice of sequence generation problems, such as machine translation, where we often deal with a large action space (\textit{e.g.,} a vocabulary) and a long action sequence (\textit{e.g.,} a translation). In this work, we introduce two-stage sampling and dynamic sampling approaches to improve the sampling efficiency during training sequence generation models via RL. We experiment with our approaches on the traditional sequence generation tasks, including machine translation and abstractive summarization. Furthermore, we evaluate our approaches in RL from human feedback (RLHF) through training a large language model using the reward model. Experimental results show that the efficient sampling-base
    
[^14]: 西班牙临床语言模型调查

    A Survey of Spanish Clinical Language Models. (arXiv:2308.02199v1 [cs.CL])

    [http://arxiv.org/abs/2308.02199](http://arxiv.org/abs/2308.02199)

    这项调查研究了西班牙语临床语言模型的应用，回顾了17个专注于临床任务的语料库的贡献，并对最相关的西班牙语语言模型和西班牙临床语言模型进行了彻底比较，提供了3000多个进行微调的模型。为了便于未来的研究和挑战，所有测试过的语料库和最佳模型都被以可访问的方式公开。

    

    本调查聚焦于使用编码器语言模型来解决西班牙语临床领域任务的问题。我们回顾了17个主要专注于临床任务的语料库的贡献，然后列出了最相关的西班牙语语言模型和西班牙临床语言模型。我们通过对可用语料库的精选子集进行基准测试，对这些模型进行了彻底的比较，以找到表现最佳的模型；总共超过3000个模型被针对这项研究进行了微调。所有测试的语料库和最佳模型都以可访问的方式公开，以便独立团队可以重现结果或在未来创建新的西班牙临床语言模型时进行挑战。

    This survey focuses in encoder Language Models for solving tasks in the clinical domain in the Spanish language. We review the contributions of 17 corpora focused mainly in clinical tasks, then list the most relevant Spanish Language Models and Spanish Clinical Language models. We perform a thorough comparison of these models by benchmarking them over a curated subset of the available corpora, in order to find the best-performing ones; in total more than 3000 models were fine-tuned for this study. All the tested corpora and the best models are made publically available in an accessible way, so that the results can be reproduced by independent teams or challenged in the future when new Spanish Clinical Language models are created.
    
[^15]: 用语义范围解释关系分类模型

    Explaining Relation Classification Models with Semantic Extents. (arXiv:2308.02193v1 [cs.CL])

    [http://arxiv.org/abs/2308.02193](http://arxiv.org/abs/2308.02193)

    本研究提出了一种解释关系分类模型的方法，即使用语义范围分析模型的决策模式。语义范围是关于分类决策的文本中最有影响力的部分。通过将人类和模型的语义范围进行比较，发现模型往往从数据中学习到了快捷模式。

    

    近年来，大规模预训练语言模型（如BERT和GPT）的发展显著改进了各种任务中的信息抽取系统，包括关系分类。最先进的系统在科学基准上具有很高的准确性。目前，缺乏可解释性是许多真实世界应用中的一个复杂因素。可理解的系统对于防止有偏见、违反直觉或有害的决策是必要的。我们引入了一种分析关系分类任务决策模式的概念，即语义范围。语义范围是关于分类决策的文本中最有影响力的部分。我们的定义允许类似的过程来确定人类和模型的语义范围。我们提供了一个注释工具和一个软件框架，以便方便、可重复地确定人类和模型的语义范围。比较两者发现，模型往往从数据中学习到了快捷模式。这些模式很难被人类解释或理解。

    In recent years, the development of large pretrained language models, such as BERT and GPT, significantly improved information extraction systems on various tasks, including relation classification. State-of-the-art systems are highly accurate on scientific benchmarks. A lack of explainability is currently a complicating factor in many real-world applications. Comprehensible systems are necessary to prevent biased, counterintuitive, or harmful decisions.  We introduce semantic extents, a concept to analyze decision patterns for the relation classification task. Semantic extents are the most influential parts of texts concerning classification decisions. Our definition allows similar procedures to determine semantic extents for humans and models. We provide an annotation tool and a software framework to determine semantic extents for humans and models conveniently and reproducibly. Comparing both reveals that models tend to learn shortcut patterns from data. These patterns are hard to d
    
[^16]: Emo-DNA: 跨语料库语音情感识别中的情感解耦和对齐学习

    Emo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus Speech Emotion Recognition. (arXiv:2308.02190v1 [cs.SD])

    [http://arxiv.org/abs/2308.02190](http://arxiv.org/abs/2308.02190)

    Emo-DNA是一种新颖的方法，通过对比情感解耦和双层情感对齐实现了跨语料库语音情感识别中的情感相关特征的学习。

    

    跨语料库语音情感识别旨在将推断语音情感的能力从一个有标签的语料库推广到一个无标签的语料库，这是一个非常具有挑战性的任务，因为两个语料库之间存在显著差异。现有的方法通常基于无监督领域适应（UDA），通过全局分布对齐来努力学习语料库不变的特征，但不幸的是，所得特征混合了语料库特定的特征或不具有类别鉴别性。为了解决这些挑战，我们提出了一种全新的Emotion Decoupling aNd Alignment（EMO-DNA）跨语料库语音情感识别学习框架，一种新颖的UDA方法，用于学习与情感相关的语料库不变特征。EMO-DNA的创新之处在于对比情感解耦和双层情感对齐。一方面，我们通过对比解耦损失实现对比学习，增强了情感相关特征与语料库之间的可分离性。

    Cross-corpus speech emotion recognition (SER) seeks to generalize the ability of inferring speech emotion from a well-labeled corpus to an unlabeled one, which is a rather challenging task due to the significant discrepancy between two corpora. Existing methods, typically based on unsupervised domain adaptation (UDA), struggle to learn corpus-invariant features by global distribution alignment, but unfortunately, the resulting features are mixed with corpus-specific features or not class-discriminative. To tackle these challenges, we propose a novel Emotion Decoupling aNd Alignment learning framework (EMO-DNA) for cross-corpus SER, a novel UDA method to learn emotion-relevant corpus-invariant features. The novelties of EMO-DNA are two-fold: contrastive emotion decoupling and dual-level emotion alignment. On one hand, our contrastive emotion decoupling achieves decoupling learning via a contrastive decoupling loss to strengthen the separability of emotion-relevant features from corpus-s
    
[^17]: 从虚假新闻到偏执新闻的领域适应中使用

    From Fake to Hyperpartisan News Detection Using Domain Adaptation. (arXiv:2308.02185v1 [cs.CL])

    [http://arxiv.org/abs/2308.02185](http://arxiv.org/abs/2308.02185)

    本研究探索了在虚假新闻检测和偏执新闻检测之间应用无监督领域适应技术的效果，通过知识传递和数据增强提高了性能，同时将聚类和主题建模算法与UDA相结合取得了更好的结果。

    

    无监督的领域适应（UDA）是一种流行的技术，旨在减少两个数据分布之间的领域漂移。它在计算机视觉和自然语言处理领域得到了成功的应用。在本研究中，我们探索了在两个文本分类任务之间应用各种无监督领域适应技术（虚假新闻检测和偏执新闻检测）的效果。我们研究了在训练过程中不涉及目标标签的虚假新闻到偏执新闻检测的知识传递。因此，我们评估了UDA、带有教师的聚类对齐和跨领域对比学习。大量实验证明了这些技术提高了性能，而包括数据增强进一步提升了结果。此外，我们将聚类和主题建模算法与UDA相结合，结果比初始的UDA设置更好。

    Unsupervised Domain Adaptation (UDA) is a popular technique that aims to reduce the domain shift between two data distributions. It was successfully applied in computer vision and natural language processing. In the current work, we explore the effects of various unsupervised domain adaptation techniques between two text classification tasks: fake and hyperpartisan news detection. We investigate the knowledge transfer from fake to hyperpartisan news detection without involving target labels during training. Thus, we evaluate UDA, cluster alignment with a teacher, and cross-domain contrastive learning. Extensive experiments show that these techniques improve performance, while including data augmentation further enhances the results. In addition, we combine clustering and topic modeling algorithms with UDA, resulting in improved performances compared to the initial UDA setup.
    
[^18]: 通过大型语言模型扩展临床试验匹配：以肿瘤学为案例研究

    Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology. (arXiv:2308.02180v1 [cs.CL])

    [http://arxiv.org/abs/2308.02180](http://arxiv.org/abs/2308.02180)

    本文研究了使用大型语言模型（LLMs）扩展临床试验匹配的方法，并以肿瘤学为案例研究。研究结果显示，先进的LLMs能够处理临床试验的复杂条件和匹配逻辑，相较于之前的方法，性能显著提升，并可作为人工辅助筛选患者-试验候选人的初步解决方案。

    

    临床试验匹配是医疗传递和发现中的关键过程。实际上，由于庞大的非结构化数据和不可扩展的手动处理，该过程存在问题。本文通过以肿瘤学为重点领域，对使用大型语言模型（LLM）扩展临床试验匹配进行了系统研究。我们的研究基于一个正在美国一个大型医疗网络进行测试部署的临床试验匹配系统。初步结果令人鼓舞：先进的LLM（如GPT-4）可以立即连接临床试验的复杂的合格条件，并提取复杂的匹配逻辑（例如嵌套的AND/OR/NOT）。虽然仍不完美，LLM在性能上显著优于以前的强基准线，并可能作为在人与人之间进行候选患者-试验划分的初步解决方案。我们的研究还揭示了一些应用LLM进行端到端临床试验匹配的重要增长领域，例如上下文限制和准确性。

    Clinical trial matching is a key process in health delivery and discovery. In practice, it is plagued by overwhelming unstructured data and unscalable manual processing. In this paper, we conduct a systematic study on scaling clinical trial matching using large language models (LLMs), with oncology as the focus area. Our study is grounded in a clinical trial matching system currently in test deployment at a large U.S. health network. Initial findings are promising: out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop. Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially
    
[^19]: 你说你读的东西：通过归宿和场合归因来理解新闻评论行为

    You talk what you read: Understanding News Comment Behavior by Dispositional and Situational Attribution. (arXiv:2308.02168v1 [cs.CL])

    [http://arxiv.org/abs/2308.02168](http://arxiv.org/abs/2308.02168)

    本文通过考虑来自新闻互动历史的归宿因素和来自相应新闻的场合因素，提出了一个编码器-解码器框架来理解新闻评论行为，同时验证了该方法在读者感知新闻摘要和新闻方面-观点预测中的应用价值。

    

    许多新闻评论挖掘研究基于评论与相应新闻的明确关联的假设。在本文中，我们观察到用户的评论也受到其互动历史所体现的个体特征的影响。因此，我们通过考虑来自新闻互动历史的归宿因素和来自相应新闻的场合因素来理解新闻评论行为。我们提出了一个包含三个部分的编码器-解码器框架，用于建模新闻评论的生成过程。所得到的归宿和场合归因有助于理解用户的关注点和观点，并在读者感知新闻摘要和新闻方面-观点预测的应用中进行验证。

    Many news comment mining studies are based on the assumption that comment is explicitly linked to the corresponding news. In this paper, we observed that users' comments are also heavily influenced by their individual characteristics embodied by the interaction history. Therefore, we position to understand news comment behavior by considering both the dispositional factors from news interaction history, and the situational factors from corresponding news. A three-part encoder-decoder framework is proposed to model the generative process of news comment. The resultant dispositional and situational attribution contributes to understanding user focus and opinions, which are validated in applications of reader-aware news summarization and news aspect-opinion forecasting.
    
[^20]: 视听内容的发言人日化

    Speaker Diarization of Scripted Audiovisual Content. (arXiv:2308.02160v1 [cs.CL])

    [http://arxiv.org/abs/2308.02160](http://arxiv.org/abs/2308.02160)

    本文介绍了一种用于发言人日化任务的新方法，利用制作脚本提取伪标签数据，并相对于无监督基线模型实现了51.7%的改进。

    

    媒体本地化行业通常需要一个与最终电影或电视制作的原文脚本相符的脚本，以便在外语中创建字幕或配音脚本。特别是，即播出脚本必须被结构化为包含时间代码、发言人姓名和转录的对话行序列。目前的语音识别技术可以减轻转录步骤。然而，目前最先进的发言人日化模型在电视节目上仍存在两个主要问题：（一）无法追踪大量发言人，（二）在检测频繁的发言人更换时准确率较低。为了缓解这个问题，我们提出了一种新方法，利用在拍摄过程中使用的制作脚本来提取用于发言人日化任务的伪标签数据。我们提出了一种新的半监督方法，并在我们的度量结果上，在66个节目的测试集上相对于两个无监督基线模型实现了51.7%的改进。

    The media localization industry usually requires a verbatim script of the final film or TV production in order to create subtitles or dubbing scripts in a foreign language. In particular, the verbatim script (i.e. as-broadcast script) must be structured into a sequence of dialogue lines each including time codes, speaker name and transcript. Current speech recognition technology alleviates the transcription step. However, state-of-the-art speaker diarization models still fall short on TV shows for two main reasons: (i) their inability to track a large number of speakers, (ii) their low accuracy in detecting frequent speaker changes. To mitigate this problem, we present a novel approach to leverage production scripts used during the shooting process, to extract pseudo-labeled data for the speaker diarization task. We propose a novel semi-supervised approach and demonstrate improvements of 51.7% relative to two unsupervised baseline models on our metrics on a 66 show test set.
    
[^21]: Retroformer：使用策略梯度优化的回顾性大型语言代理

    Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization. (arXiv:2308.02151v1 [cs.CL])

    [http://arxiv.org/abs/2308.02151](http://arxiv.org/abs/2308.02151)

    本文介绍了一种通过策略梯度优化的回顾性大型语言代理框架，该框架通过学习环境反馈来调整语言代理的提示，从而优化其性能。这种代理能够从多个环境和任务中学习奖励，并通过总结以前任务的根本原因来改进语言代理提示。

    

    最近几个月，出现了一个强大的新趋势，即将大型语言模型（LLMs）增强成能够自主完成目标导向多步骤任务的语言代理，而不仅仅是回答人类用户的查询。然而，大多数现有的语言代理没有使用环境特定的奖励进行优化。尽管一些代理通过口头反馈实现了迭代改进，但它们不能以与基于梯度的奖励学习相兼容的方式进行推理和规划。本文提出了一个原则性的框架，通过学习回顾模型，通过策略梯度自动调整语言代理的提示，从环境反馈中优化代理的工作。具体而言，我们提出的代理架构通过学习多个环境和任务的奖励来微调预训练语言模型，从而通过总结以前任务的根本原因来改进语言代理提示。

    Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior
    
[^22]: Tweet Insights: 从Twitter提取时间相关信息的可视化平台

    Tweet Insights: A Visualization Platform to Extract Temporal Insights from Twitter. (arXiv:2308.02142v1 [cs.CL])

    [http://arxiv.org/abs/2308.02142](http://arxiv.org/abs/2308.02142)

    该论文介绍了一个名为Tweet Insights的可视化平台，可以从Twitter中提取时间相关信息。它使用词嵌入技术和专门优化的语言模型对Twitter的大量时间序列数据进行后处理，并通过界面对时间进行分析，以检测和描述意义转变的变化，包括与趋势度量相关的补充信息，如情感和主题的变化。

    

    本文介绍了一种使用词嵌入技术和专门优化的语言模型对Twitter的大量时间序列数据进行后处理的方法。这些数据涵盖了过去五年的变化，包括n-gram频率、相似度、情感和主题分布的变化。通过在这些数据之上构建的界面，可以对时间进行分析，以便检测和描述意义转变的变化，包括与趋势度量相关的补充信息，如情感和主题的变化。我们发布了一个在线演示供轻松实验，并共享代码和基础汇总数据以供未来研究使用。在本文中，我们还讨论了基于我们平台的三个案例研究，展示了它在时间语言分析中的潜力。

    This paper introduces a large collection of time series data derived from Twitter, postprocessed using word embedding techniques, as well as specialized fine-tuned language models. This data comprises the past five years and captures changes in n-gram frequency, similarity, sentiment and topic distribution. The interface built on top of this data enables temporal analysis for detecting and characterizing shifts in meaning, including complementary information to trending metrics, such as sentiment and topic association over time. We release an online demo for easy experimentation, and we share code and the underlying aggregated data for future work. In this paper, we also discuss three case studies unlocked thanks to our platform, showcasing its potential for temporal linguistic analysis.
    
[^23]: ParaFuzz：一种基于可解释性的技术用于检测自然语言处理中的毒样本

    ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP. (arXiv:2308.02122v1 [cs.CR])

    [http://arxiv.org/abs/2308.02122](http://arxiv.org/abs/2308.02122)

    ParaFuzz是一种基于可解释性的技术，用于检测自然语言处理中的毒样本。该技术通过观察模型在重写过的干净样本和污染样本上的预测稳定性，来判断样本是否被污染。

    

    傍门攻击已成为自然语言处理（NLP）模型的重要威胁，其中在输入中存在特定触发器可以导致被污染的模型将这些输入误分类为预定的目标类别。当前的检测机制受到限制，无法应对更隐蔽的傍门策略，如基于风格的攻击。在这项工作中，我们提出了一种创新的测试时污染样本检测框架，该框架依赖于模型预测的可解释性，并与输入的语义含义有关。我们认为，触发器（例如，不常见的单词）不应该从根本上改变被污染样本的基本语义含义，因为它们想保持潜伏。基于这个观察，我们假设在改写过程中，模型对于重写过的干净样本的预测应该保持稳定，而对于污染样本的预测在触发器的突变过程中应该恢复到真实标签。

    Backdoor attacks have emerged as a prominent threat to natural language processing (NLP) models, where the presence of specific triggers in the input can lead poisoned models to misclassify these inputs to predetermined target classes. Current detection mechanisms are limited by their inability to address more covert backdoor strategies, such as style-based attacks. In this work, we propose an innovative test-time poisoned sample detection framework that hinges on the interpretability of model predictions, grounded in the semantic meaning of inputs. We contend that triggers (e.g., infrequent words) are not supposed to fundamentally alter the underlying semantic meanings of poisoned samples as they want to stay stealthy. Based on this observation, we hypothesize that while the model's predictions for paraphrased clean samples should remain stable, predictions for poisoned samples should revert to their true labels upon the mutations applied to triggers during the paraphrasing process. W
    
[^24]: 中文金融文本情感挖掘：一种基于字符关系的同时提取方面-观点对的方法

    Chinese Financial Text Emotion Mining: GCGTS -- A Character Relationship-based Approach for Simultaneous Aspect-Opinion Pair Extraction. (arXiv:2308.02113v1 [cs.CL])

    [http://arxiv.org/abs/2308.02113](http://arxiv.org/abs/2308.02113)

    本论文提出了一种名为GCGTS的基于字符关系的方法，用于从中文金融文本中同时提取方面-观点对。该方法利用图卷积网络（GCN）明确地整合了句法结构，并统一了词内字符的编码，以提高提取的准确性和效果。

    

    从中国的金融文本中提取方面-观点对（AOPE）是细粒度文本情感分析中的专门任务。主要目标是同时从各种不同的金融文本中提取方面术语和观点术语。之前的研究主要集中在基于网格的模型中开发网格标注方案，以便促进提取过程。然而，这些方法通常依赖于字符级（标记级）特征编码，可能忽视了中文字符在词内的逻辑关系。为了解决这个问题，我们提出了一种新的方法，称为基于图形的字符级网格标注方案（GCGTS）。GCGTS方法使用图形卷积网络（GCN）明确地结合了句法结构，并统一了同一句法语义单元（中文词级别）内字符的编码。此外，我们将图形模型引入网格模型中，以更好地捕捉局部关系。

    Aspect-Opinion Pair Extraction (AOPE) from Chinese financial texts is a specialized task in fine-grained text sentiment analysis. The main objective is to extract aspect terms and opinion terms simultaneously from a diverse range of financial texts. Previous studies have mainly focused on developing grid annotation schemes within grid-based models to facilitate this extraction process. However, these methods often rely on character-level (token-level) feature encoding, which may overlook the logical relationships between Chinese characters within words. To address this limitation, we propose a novel method called Graph-based Character-level Grid Tagging Scheme (GCGTS). The GCGTS method explicitly incorporates syntactic structure using Graph Convolutional Networks (GCN) and unifies the encoding of characters within the same syntactic semantic unit (Chinese word level). Additionally, we introduce an image convolutional structure into the grid model to better capture the local relationshi
    
[^25]: Prompt2Gaussia: 不确定提示学习用于脚本事件预测

    Prompt2Gaussia: Uncertain Prompt-learning for Script Event Prediction. (arXiv:2308.02103v1 [cs.CL])

    [http://arxiv.org/abs/2308.02103](http://arxiv.org/abs/2308.02103)

    本文提出了Prompt2Gaussia方法，通过不确定性提示学习和高斯分布来解决脚本事件预测中的无法确定最优提示和标签的问题。

    

    脚本事件预测旨在预测给定事件链中的后续事件。先前的研究通过整合外部知识来增强语义取得了巨大成功，但是获取适当的知识资源和检索与脚本相关的知识是费时费力的。本文将公共预训练语言模型视为知识库，并通过提示学习自动挖掘与脚本相关的知识。然而，脚本中的场景多样性和标签模糊性使得构造最有效的提示和标签令牌在提示学习中变得不确定，即提示不确定性和表述不确定性。考虑到高斯分布表达不确定性的固有能力，我们将提示令牌和标签令牌部署为遵循高斯分布的随机变量，提出了提示估计器和表述估计器来估计它们的概率表示，而不是确定它们的值。

    Script Event Prediction (SEP) aims to predict the subsequent event for a given event chain from a candidate list. Prior research has achieved great success by integrating external knowledge to enhance the semantics, but it is laborious to acquisite the appropriate knowledge resources and retrieve the script-related knowledge. In this paper, we regard public pre-trained language models as knowledge bases and automatically mine the script-related knowledge via prompt-learning. Still, the scenario-diversity and label-ambiguity in scripts make it uncertain to construct the most functional prompt and label token in prompt learning, i.e., prompt-uncertainty and verbalizer-uncertainty. Considering the innate ability of Gaussian distribution to express uncertainty, we deploy the prompt tokens and label tokens as random variables following Gaussian distributions, where a prompt estimator and a verbalizer estimator are proposed to estimate their probabilistic representations instead of determini
    
[^26]: N-gram增强：通过规范化N-gram目标改善上下文偏差

    N-gram Boosting: Improving Contextual Biasing with Normalized N-gram Targets. (arXiv:2308.02092v1 [cs.CL])

    [http://arxiv.org/abs/2308.02092](http://arxiv.org/abs/2308.02092)

    本文提出了一种N-gram增强技术，通过规范化目标词组来改善上下文偏差，提高关键词识别率。

    

    在商务谈话的语音到文本应用中，准确转录专有名词和技术术语尤为重要。这些词对于理解对话至关重要，但往往很少出现，因此在文本和音频训练数据中很可能缺乏，这在这个领域中造成了很大的挑战。我们提出了一个两步关键词增强机制，它成功地使用规范化的单词和n-gram而不仅仅是单个标记，从而消除了关键词目标的丢失问题。此外，我们展示了如何调整增强权重逻辑以避免过度增强多个标记的关键词。在我们的专有领域数据集上，我们的关键词识别率相对提高了26％，在LibriSpeech上提高了2％。这种方法对涉及非字母字符或具有非标准发音的目标特别有用。

    Accurate transcription of proper names and technical terms is particularly important in speech-to-text applications for business conversations. These words, which are essential to understanding the conversation, are often rare and therefore likely to be under-represented in text and audio training data, creating a significant challenge in this domain. We present a two-step keyword boosting mechanism that successfully works on normalized unigrams and n-grams rather than just single tokens, which eliminates missing hits issues with boosting raw targets. In addition, we show how adjusting the boosting weight logic avoids over-boosting multi-token keywords. This improves our keyword recognition rate by 26% relative on our proprietary in-domain dataset and 2% on LibriSpeech. This method is particularly useful on targets that involve non-alphabetic characters or have non-standard pronunciations.
    
[^27]: 跨平台仇恨言论检测中的因果引导解缠问题

    Causality Guided Disentanglement for Cross-Platform Hate Speech Detection. (arXiv:2308.02080v1 [cs.CL])

    [http://arxiv.org/abs/2308.02080](http://arxiv.org/abs/2308.02080)

    本研究提出了一种跨平台仇恨言论检测模型，通过解缠输入表示为不变特征和平台相关特征，实现了对多个未见平台的良好泛化能力。

    

    尽管社交媒体平台在促进公开对话方面具有价值，但他们经常被利用来传播有害内容。目前用于检测这种有害内容的深度学习和自然语言处理模型过度依赖于领域特定术语，影响到了它们适应泛化仇恨言论检测的能力。这是因为它们倾向于过于狭隘地关注特定的语言信号或某些词语类别的使用。当平台缺乏高质量的标记数据用于训练时，另一个重要的挑战出现了，需要跨平台模型来适应不同的分布转化。我们的研究引入了一个跨平台仇恨言论检测模型，能够在一个平台的数据上训练并推广到多个未见平台。为了实现对不同平台的良好泛化性能，一种方法是将输入表示解缠为不变特征和平台相关特征。我们还认为学习因果关系是提供更好解缠和泛化性能的关键。

    Social media platforms, despite their value in promoting open discourse, are often exploited to spread harmful content. Current deep learning and natural language processing models used for detecting this harmful content overly rely on domain-specific terms affecting their capabilities to adapt to generalizable hate speech detection. This is because they tend to focus too narrowly on particular linguistic signals or the use of certain categories of words. Another significant challenge arises when platforms lack high-quality annotated data for training, leading to a need for cross-platform models that can adapt to different distribution shifts. Our research introduces a cross-platform hate speech detection model capable of being trained on one platform's data and generalizing to multiple unseen platforms. To achieve good generalizability across platforms, one way is to disentangle the input representations into invariant and platform-dependent features. We also argue that learning causa
    
[^28]: 基于自然语言查询的电子商务自动完成的季节性重新排序

    Seasonality Based Reranking of E-commerce Autocomplete Using Natural Language Queries. (arXiv:2308.02055v1 [cs.IR])

    [http://arxiv.org/abs/2308.02055](http://arxiv.org/abs/2308.02055)

    本文提出了一种基于神经网络的自然语言处理算法，用于将季节性作为信号来重新排序电子商务的自动完成功能，从而提高自动完成的相关性和业务指标。

    

    查询自动完成（QAC）也被称为typeahead，它在用户在搜索框中输入前缀时建议完整查询列表。它是现代搜索引擎特别是在电子商务领域的关键功能之一。typeahead的目标之一是向用户建议与季节性相关的重要查询。在本文中，我们提出了一种基于神经网络的自然语言处理（NLP）算法，以将季节性作为一个信号并对QAC排序模型进行端到端评估。将季节性纳入自动完成排序模型中可以提高自动完成的相关性和业务指标。

    Query autocomplete (QAC) also known as typeahead, suggests list of complete queries as user types prefix in the search box. It is one of the key features of modern search engines specially in e-commerce. One of the goals of typeahead is to suggest relevant queries to users which are seasonally important. In this paper we propose a neural network based natural language processing (NLP) algorithm to incorporate seasonality as a signal and present end to end evaluation of the QAC ranking model. Incorporating seasonality into autocomplete ranking model can improve autocomplete relevance and business metric.
    
[^29]: 大型语言模型的不平等机会: 通过职位推荐揭示人口统计偏见

    The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations. (arXiv:2308.02053v1 [cs.CL])

    [http://arxiv.org/abs/2308.02053](http://arxiv.org/abs/2308.02053)

    通过职位推荐分析了大型语言模型（LLMs）的人口统计偏见，发现这些模型对于墨西哥工人一直建议低薪工作，并向女性更倾向于推荐秘书职位。这项研究强调了理解LLMs偏见的重要性。

    

    大型语言模型（LLMs）已在各种实际应用中得到广泛应用。了解这些偏见对于理解在使用LLMs进行决策时潜在的后续影响至关重要，特别是对于历史上处于劣势的群体。在这项工作中，我们提出了一种简单的方法来通过职位推荐的角度分析和比较LLMs中的人口统计偏见。我们通过测量ChatGPT和LLaMA这两个前沿LLMs内的交叉偏见来证明我们方法的有效性。我们的实验主要集中在揭示性别认同和国籍偏见上；然而，我们的方法可以扩展到任何人口统计身份的交叉偏见的研究。我们在两个模型中发现了明显的偏见，例如两个模型一直建议墨西哥工人从事低薪工作，或者更倾向于向女性推荐秘书职位。我们的研究强调了测量和理解LLMs中的偏见的重要性。

    Large Language Models (LLMs) have seen widespread deployment in various real-world applications. Understanding these biases is crucial to comprehend the potential downstream consequences when using LLMs to make decisions, particularly for historically disadvantaged groups. In this work, we propose a simple method for analyzing and comparing demographic bias in LLMs, through the lens of job recommendations. We demonstrate the effectiveness of our method by measuring intersectional biases within ChatGPT and LLaMA, two cutting-edge LLMs. Our experiments primarily focus on uncovering gender identity and nationality bias; however, our method can be extended to examine biases associated with any intersection of demographic identities. We identify distinct biases in both models toward various demographic identities, such as both models consistently suggesting low-paying jobs for Mexican workers or preferring to recommend secretarial roles to women. Our study highlights the importance of measu
    
[^30]: 档案和历史学领域的人工智能应用：HTS和ChatGPT

    Artificial Intelligence in archival and historical scholarship workflow: HTS and ChatGPT. (arXiv:2308.02044v1 [cs.DL])

    [http://arxiv.org/abs/2308.02044](http://arxiv.org/abs/2308.02044)

    本文研究了人工智能在档案数字化过程中的应用，重点是自动转录和校正手稿，以及标准化文本。通过测试ChatGPT系统，在对信函进行文本标准化时取得了一定效果。总体而言，数字化和人工智能可以显著提升档案和历史研究能力。

    

    本文研究了人工智能对档案遗产数字化过程的影响，特别是对手稿的自动转录、校正和标准化的影响。它强调了数字化推动学者重新定义档案和历史领域，并通过数字化和大数据的整合提供模拟源文件的便捷性。研究聚焦于两个人工智能系统，分别是Transkribus和ChatGPT，它们使得对数字化源文件的高效分析和转录成为可能。文章还介绍了对ChatGPT的测试，该测试用于对保存在Biscari档案（卡塔尼亚）的信函部分中的366封信件进行文本标准化。尽管人工智能存在一些限制导致一些不准确性，但纠正后的文本仍然达到了期望。总的来说，文章得出结论，数字化和人工智能可以显著提升档案和历史研究，允许对大量数据进行分析和处理。

    This article examines the impact of Artificial Intelligence on the archival heritage digitization processes, specifically regarding the manuscripts' automatic transcription, their correction, and normalization. It highlights how digitality has compelled scholars to redefine Archive and History field and has facilitated the accessibility of analogue sources through digitization and integration into big data. The study focuses on two AI systems, namely Transkribus and ChatGPT, which enable efficient analysis and transcription of digitized sources. The article presents a test of ChatGPT, which was utilized to normalize the text of 366 letters stored in the Correspondence section of the Biscari Archive (Catania). Although the AI exhibited some limitations that resulted in inaccuracies, the corrected texts met expectations. Overall, the article concludes that digitization and AI can significantly enhance archival and historical research by allowing the analysis of vast amounts of data and t
    
[^31]: 提出一个概念框架：社交媒体监听公共健康行为

    Proposing a conceptual framework: social media listening for public health behavior. (arXiv:2308.02037v1 [cs.CY])

    [http://arxiv.org/abs/2308.02037](http://arxiv.org/abs/2308.02037)

    该论文提出了一个基于理论的概念框架，旨在解决社交媒体数据和自然语言处理技术在社交监听和错误信息研究中的应用问题。

    

    现有的传播和行为理论已经被采用来解决健康错误信息的问题。虽然已经有各种理论和模型被用于研究COVID-19大流行，但尚缺乏专门针对社交媒体数据和自然语言处理技术进行社交监听或错误信息研究的框架。本研究旨在提出一个新颖而基于理论的概念框架，用于错误信息研究。我们收集了以同行评审期刊发表的COVID-19相关研究中使用的理论和模型。这些理论和模型涉及健康行为，传播和错误信息。我们对这些理论和模型进行了分析和评述，然后提出了一个具有示范性的概念框架。我们对健康信念模型、计划行为理论/理性行为、行为影响的传播、跨理论模型、功利性理论、社会评判理论、风险信息寻求和处理模式进行了回顾。

    Existing communications and behavioral theories have been adopted to address health misinformation. Although various theories and models have been used to investigate the COVID-19 pandemic, there is no framework specially designed for social listening or misinformation studies using social media data and natural language processing techniques. This study aimed to propose a novel yet theory-based conceptual framework for misinformation research. We collected theories and models used in COVID-19 related studies published in peer-reviewed journals. The theories and models ranged from health behaviors, communications, to misinformation. They are analyzed and critiqued for their components, followed by proposing a conceptual framework with a demonstration. We reviewed Health Belief Model, Theory of Planned Behavior/Reasoned Action, Communication for Behavioral Impact, Transtheoretical Model, Uses and Gratifications Theory, Social Judgment Theory, Risk Information Seeking and Processing Mode
    
[^32]: Twitter数据告诉我们关于未来的什么？

    What Twitter Data Tell Us about the Future?. (arXiv:2308.02035v1 [cs.CY])

    [http://arxiv.org/abs/2308.02035](http://arxiv.org/abs/2308.02035)

    本研究通过研究Twitter上未来主义者对未来的预期，探究语言提示对社交媒体用户的预测性思维的影响，并开发了一个可扩展的自然语言处理流程和数据集。

    

    预测是一种基本的人类认知能力，涉及对未来的思考和生活。虽然语言标记反映了预测性思维，但从自然语言处理的角度来研究预测性思维的研究还有限。本研究旨在调查Twitter上未来主义者对未来的预期，并探索语言提示对社交媒体用户的预测性思维的影响。我们解决了Twitter未来主义者预测和分享哪些未来，以及如何从社交数据中建模这些预期未来的研究问题。为了调查这一点，我们回顾了关于预测的相关工作，讨论了语言标记和知名人士对预测性思维的影响，并提出了一个将未来分为“现在的未来”和“未来的现在” 的分类系统。本研究提供了一个由未来影响者公开分享的超过100万条推文的编译数据集，并使用SOTA模型开发了一个可扩展的自然语言处理流程。

    Anticipation is a fundamental human cognitive ability that involves thinking about and living towards the future. While language markers reflect anticipatory thinking, research on anticipation from the perspective of natural language processing is limited. This study aims to investigate the futures projected by futurists on Twitter and explore the impact of language cues on anticipatory thinking among social media users. We address the research questions of what futures Twitter's futurists anticipate and share, and how these anticipated futures can be modeled from social data. To investigate this, we review related works on anticipation, discuss the influence of language markers and prestigious individuals on anticipatory thinking, and present a taxonomy system categorizing futures into "present futures" and "future present". This research presents a compiled dataset of over 1 million publicly shared tweets by future influencers and develops a scalable NLP pipeline using SOTA models. T
    
[^33]: 高效情感分析：特征提取技术、集成和深度学习模型的资源可行性评估

    Efficient Sentiment Analysis: A Resource-Aware Evaluation of Feature Extraction Techniques, Ensembling, and Deep Learning Models. (arXiv:2308.02022v1 [cs.CL])

    [http://arxiv.org/abs/2308.02022](http://arxiv.org/abs/2308.02022)

    本文对于文档级情感分析模型进行了综合评估，重点考虑了资源成本和环境意识。研究发现，在资源消耗较低的配置下，准确性损失较小。这对于资源有限的环境下的模型部署具有重要意义。

    

    在追求最大化准确性的自然语言处理系统时，常常忽视其他重要的系统性能指标。先前的模型很容易被遗忘，尽管它们可能在计算资源有限或相对更昂贵的设置中适用。在本文中，我们对文档级情感分析模型进行广泛的比较评估，主要关注对于模型部署可行性和环境意识的资源成本。我们的实验考虑了不同的特征提取技术、集成效果、任务特定的深度学习建模以及领域无关的大型语言模型。我们发现，虽然经过微调的大型语言模型达到了最高的准确性，但某些替代配置在资源消耗方面提供了巨大的（最高达24,283*）节省，而准确性损失只有较小的(<1%)。此外，我们发现对于较小的数据集，准确性的差异会缩小，而资源消耗的差异会增大。

    While reaching for NLP systems that maximize accuracy, other important metrics of system performance are often overlooked. Prior models are easily forgotten despite their possible suitability in settings where large computing resources are unavailable or relatively more costly. In this paper, we perform a broad comparative evaluation of document-level sentiment analysis models with a focus on resource costs that are important for the feasibility of model deployment and general climate consciousness. Our experiments consider different feature extraction techniques, the effect of ensembling, task-specific deep learning modeling, and domain-independent large language models (LLMs). We find that while a fine-tuned LLM achieves the best accuracy, some alternate configurations provide huge (up to 24, 283 *) resource savings for a marginal (<1%) loss in accuracy. Furthermore, we find that for smaller datasets, the differences in accuracy shrink while the difference in resource consumption gro
    
[^34]: Baby Llama：从一组在小数据集上训练的教师中进行知识蒸馏，无性能损失

    Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty. (arXiv:2308.02019v1 [cs.CL])

    [http://arxiv.org/abs/2308.02019](http://arxiv.org/abs/2308.02019)

    本文提出了一种从在小数据集上训练的教师中进行知识蒸馏的方法，并证明当教师模型在足够小的数据集上训练时，蒸馏可以保持甚至超过教师模型的性能。

    

    我们提出了我们对BabyLM挑战[arXiv:2301.11796]的解决方案，其目标是提高语言模型的样本效率。我们在以发展性为基础的10M词语的BabyLM数据集上训练了一个由GPT-2和小型LLaMA模型组成的集合，然后将其蒸馏为一个小型的58M参数LLaMA模型，其性能超过了两个教师模型以及一个没有进行蒸馏训练的类似模型。这表明，当教师模型在足够小的数据集上训练时，蒸馏不仅可以保持教师模型的全部性能，还可以超过它，并导致比直接训练更好的性能。

    We present our proposed solution to the BabyLM challenge [arXiv:2301.11796], whose goal was to improve the sample efficiency of language models. We trained an ensemble consisting of a GPT-2 and small LLaMA models on the developmentally-plausible, 10M-word BabyLM dataset, then distilled it into a small, 58M-parameter LLaMA model, which exceeds in performance both of its teachers as well as a similar model trained without distillation. This suggests that distillation can not only retain the full performance of the teacher model when the latter is trained on a sufficiently small dataset; it can exceed it, and lead to significantly better performance than direct training.
    
[^35]: 面向自动语音识别的联邦表示学习

    Federated Representation Learning for Automatic Speech Recognition. (arXiv:2308.02013v1 [cs.SD])

    [http://arxiv.org/abs/2308.02013](http://arxiv.org/abs/2308.02013)

    使用联邦学习和自监督学习的结合方法，研究了面向自动语音识别的联邦表示学习，将边缘设备上的隐私数据用于学习健壮的音频表示，并取得了显著的性能改善。

    

    联邦学习（FL）是一种保护隐私的模式，允许边缘设备在不共享数据的情况下进行协作学习。像Alexa和Siri这样的边缘设备是潜在的非标记音频数据来源，可以用来学习健壮的音频表示。在这项工作中，我们将自监督学习（SSL）和FL结合起来，以遵守数据隐私约束条件，学习用于自动语音识别的表示。我们使用未标记的语音数据集Libri-Light中的说话者和章节信息，模拟非独立同分布的说话者隔离数据分布，并使用FedSGD在对比预测编码框架下进行LSTM编码器的预训练。我们展示了在FL中预训练的ASR编码器的性能与中心预训练模型相当，并且相比没有预训练，有12-15%（WER）的改善。我们进一步将联邦预训练模型适应到一种新的语言，法语，并且相比没有预训练，实现了20%（WER）的改善。

    Federated Learning (FL) is a privacy-preserving paradigm, allowing edge devices to learn collaboratively without sharing data. Edge devices like Alexa and Siri are prospective sources of unlabeled audio data that can be tapped to learn robust audio representations. In this work, we bring Self-supervised Learning (SSL) and FL together to learn representations for Automatic Speech Recognition respecting data privacy constraints. We use the speaker and chapter information in the unlabeled speech dataset, Libri-Light, to simulate non-IID speaker-siloed data distributions and pre-train an LSTM encoder with the Contrastive Predictive Coding framework with FedSGD. We show that the pre-trained ASR encoder in FL performs as well as a centrally pre-trained model and produces an improvement of 12-15% (WER) compared to no pre-training. We further adapt the federated pre-trained models to a new language, French, and show a 20% (WER) improvement over no pre-training.
    
[^36]: 孟加拉语假评论：一个基准数据集和检测系统

    Bengali Fake Reviews: A Benchmark Dataset and Detection System. (arXiv:2308.01987v1 [cs.CL])

    [http://arxiv.org/abs/2308.01987](http://arxiv.org/abs/2308.01987)

    这篇论文介绍了孟加拉假评论检测的第一个公开数据集，提出了一种独特的转换流水线，以识别孟加拉假评论，并进行了严格的实验。

    

    在各种在线平台上出现大量的假评论已经成为消费者和企业的重大关切。这样的评论可以欺骗消费者，并对产品或服务的声誉造成损害，因此识别它们至关重要。虽然在英语语言中已经广泛研究了假评论的检测，但在孟加拉语等非英语语言中检测假评论仍然是一个相对未开发的研究领域。本文介绍了孟加拉假评论检测（BFRD）数据集，这是第一个公开可用的用于识别孟加拉假评论的数据集。该数据集由从社交媒体帖子中收集到的7710条非假和1339条假的与食品相关的评论组成。为了将评论中的非孟加拉词语转换，提出了一种独特的流水线，将英语单词转换为其对应的孟加拉意义，同时也将罗马化的孟加拉语回音到孟加拉语。我们使用多个深度学习和预训练模型进行了严格的实验。

    The proliferation of fake reviews on various online platforms has created a major concern for both consumers and businesses. Such reviews can deceive customers and cause damage to the reputation of products or services, making it crucial to identify them. Although the detection of fake reviews has been extensively studied in English language, detecting fake reviews in non-English languages such as Bengali is still a relatively unexplored research area. This paper introduces the Bengali Fake Review Detection (BFRD) dataset, the first publicly available dataset for identifying fake reviews in Bengali. The dataset consists of 7710 non-fake and 1339 fake food-related reviews collected from social media posts. To convert non-Bengali words in a review, a unique pipeline has been proposed that translates English words to their corresponding Bengali meaning and also back transliterates Romanized Bengali to Bengali. We have conducted rigorous experimentation using multiple deep learning and pre
    
[^37]: 拼写检查器在在线市场中的领域特异性和数据效率：以在线市场搜索为例

    Domain specificity and data efficiency in typo tolerant spell checkers: the case of search in online marketplaces. (arXiv:2308.01976v1 [cs.LG])

    [http://arxiv.org/abs/2308.01976](http://arxiv.org/abs/2308.01976)

    本研究针对在线市场中拼写错误的问题，通过数据增强方法和递归神经网络实现了领域特定的拼写检查器，并将其应用在微软AppSource市场的实时推断API中。我们的数据有效解决方案表明，控制高质量的合成数据可能成为一个有力的工具，特别是考虑到当前大语言模型所依赖的巨大且难以控制的数据集。

    

    由于在线市场的领域特定性和用户短查询的特点，错字是在线市场访问者的主要困扰。传统的拼写检查解决方案在纠正拼写错误方面表现不佳。我们提出了一种数据增强方法来解决缺乏标注拼写错误数据的问题，并使用递归神经网络训练了上下文限制的领域特定嵌入。这些嵌入被部署在微软AppSource市场的实时推断API中，以在错误拼写的用户查询和可用产品名称之间找到最接近的匹配。我们的数据有效解决方案表明，受到当前大语言模型的影响，控制高质量的合成数据可能成为一个有力的工具，而这些模型依赖于巨大且难以控制的数据集。

    Typographical errors are a major source of frustration for visitors of online marketplaces. Because of the domain-specific nature of these marketplaces and the very short queries users tend to search for, traditional spell cheking solutions do not perform well in correcting typos. We present a data augmentation method to address the lack of annotated typo data and train a recurrent neural network to learn context-limited domain-specific embeddings. Those embeddings are deployed in a real-time inferencing API for the Microsoft AppSource marketplace to find the closest match between a misspelled user query and the available product names. Our data efficient solution shows that controlled high quality synthetic data may be a powerful tool especially considering the current climate of large language models which rely on prohibitively huge and often uncontrolled datasets.
    
[^38]: DCTM：扩张卷积转换模型用于多模态对话中的参与度估计

    DCTM: Dilated Convolutional Transformer Model for Multimodal Engagement Estimation in Conversation. (arXiv:2308.01966v1 [cs.MM])

    [http://arxiv.org/abs/2308.01966](http://arxiv.org/abs/2308.01966)

    这项研究引入了一种扩张卷积转换模型，用于在MULTIMEDIATE 2023竞赛中建模和估计多模态对话中的人类参与度。在测试集上该系统比基准模型表现出显著的7%改进，在验证集上表现为4%改进。此外，使用简单的串联方法与自注意力融合可以获得最佳性能。

    

    对话参与度估计被提出作为一个回归问题，需要识别参与者在对话中的关注和参与程度。这一任务对于揭示人类交互动力学和行为模式在对话中的洞察具有重要意义。在本研究中，我们引入了一种扩张卷积转换模型，用于建模和估计MULTIMEDIATE 2023竞赛中的人类参与度。我们提出的系统超过了基准模型，在测试集上获得了显著的7%改进和验证集上的4%改进。此外，我们采用了不同的模态融合机制，并展示对于这种类型的数据，简单的串联方法与自注意力融合获得了最佳性能。

    Conversational engagement estimation is posed as a regression problem, entailing the identification of the favorable attention and involvement of the participants in the conversation. This task arises as a crucial pursuit to gain insights into human's interaction dynamics and behavior patterns within a conversation. In this research, we introduce a dilated convolutional Transformer for modeling and estimating human engagement in the MULTIMEDIATE 2023 competition. Our proposed system surpasses the baseline models, exhibiting a noteworthy $7$\% improvement on test set and $4$\% on validation set. Moreover, we employ different modality fusion mechanism and show that for this type of data, a simple concatenated method with self-attention fusion gains the best performance.
    
[^39]: 为什么我们需要神经符号人工智能来建模实用的类比?

    Why Do We Need Neuro-symbolic AI to Model Pragmatic Analogies?. (arXiv:2308.01936v1 [cs.AI])

    [http://arxiv.org/abs/2308.01936](http://arxiv.org/abs/2308.01936)

    本文讨论了神经符号人工智能在处理逐渐复杂的类比推理时的必要性，以提供超越文字内容的广泛、多样化的知识，并结合统计和符号人工智能技术来增强和引导映射过程。

    

    智能的一个特点是能够利用熟悉的领域对不那么熟悉的领域进行推理，即类比推理。本文探讨了大型语言模型（LLMs）在处理在非结构化文本中表达的逐渐复杂的类比时的性能。我们讨论了四个不同复杂级别的类比：词汇类比、句法类比、语义类比和实用类比。随着类比变得越来越复杂，它们需要超出文本内容的广泛、多样化的知识，这在支持LLMs的词汇共现统计中不太可能找到。为了解决这个问题，我们讨论了采用神经符号人工智能技术的必要性，这些技术结合了统计和符号人工智能，根据非结构化文本提供信息以突出和增强相关内容，提供抽象和引导映射过程。我们的知识驱动方法在保持LLMs的效率的同时保持其性能。

    A hallmark of intelligence is the ability to use a familiar domain to make inferences about a less familiar domain, known as analogical reasoning. In this article, we delve into the performance of Large Language Models (LLMs) in dealing with progressively complex analogies expressed in unstructured text. We discuss analogies at four distinct levels of complexity: lexical analogies, syntactic analogies, semantic analogies, and pragmatic analogies. As the analogies become more complex, they require increasingly extensive, diverse knowledge beyond the textual content, unlikely to be found in the lexical co-occurrence statistics that power LLMs. To address this, we discuss the necessity of employing Neuro-symbolic AI techniques that combine statistical and symbolic AI, informing the representation of unstructured text to highlight and augment relevant content, provide abstraction and guide the mapping process. Our knowledge-informed approach maintains the efficiency of LLMs while preservin
    
[^40]: MultiEM: 高效有效的无监督多表实体匹配

    MultiEM: Efficient and Effective Unsupervised Multi-Table Entity Matching. (arXiv:2308.01927v1 [cs.DB])

    [http://arxiv.org/abs/2308.01927](http://arxiv.org/abs/2308.01927)

    本论文提出了一种称为MultiEM的高效有效无监督多表实体匹配解决方案，包括增强实体表示、表级层次合并和基于密度的剪枝，并通过大量实验证明了其有效性和效率。

    

    实体匹配（EM）旨在从关系表中识别出指向同一现实世界实体的所有实体对，它是实际数据管理系统中最重要的任务之一。由于EM的标记过程非常费时费力，相较于监督式EM，无监督式EM在实际场景中更具适用性。传统的无监督式EM假设所有实体来自两个表，然而在实际应用中，多个表之间的实体匹配（多表EM）更为常见。然而，关于高效有效的无监督多表EM的研究尚未充分探索。为填补这一空白，本论文对无监督多表实体匹配问题进行了正式研究，并提出了一种高效有效的解决方案，称为MultiEM。MultiEM是一个可并行的流水线，包括增强的实体表示、基于表的层次合并和基于密度的剪枝。在六个实际数据集上进行了大量实验证明了MultiEM的有效性和效率。

    Entity Matching (EM), which aims to identify all entity pairs referring to the same real-world entity from relational tables, is one of the most important tasks in real-world data management systems. Due to the labeling process of EM being extremely labor-intensive, unsupervised EM is more applicable than supervised EM in practical scenarios. Traditional unsupervised EM assumes that all entities come from two tables; however, it is more common to match entities from multiple tables in practical applications, that is, multi-table entity matching (multi-table EM). Unfortunately, effective and efficient unsupervised multi-table EM remains under-explored. To fill this gap, this paper formally studies the problem of unsupervised multi-table entity matching and proposes an effective and efficient solution, termed as MultiEM. MultiEM is a parallelable pipeline of enhanced entity representation, table-wise hierarchical merging, and density-based pruning. Extensive experimental results on six r
    
[^41]: 蒙骗：基于文本的游戏中的欺骗与合作研究

    Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models. (arXiv:2308.01404v1 [cs.CL])

    [http://arxiv.org/abs/2308.01404](http://arxiv.org/abs/2308.01404)

    通过引入一款名为"Hoodwinked"的文本游戏，研究了当前语言模型是否具有欺骗和识别谎言的能力。实验证据表明，杀手经常否认罪行并指责他人，导致投票结果受到影响。更先进的模型在杀手效果上表现出优势。实验证据表明，这种改进是通过在讨论中更强的欺骗能力实现的。

    

    当前的语言模型是否具有欺骗和识别谎言的能力？我们通过引入一款名为“Hoodwinked”的基于文本的游戏，受到“黑帮”和“谁是卧底”游戏的启发，来研究这个问题。玩家们被锁在一个房子里，必须找到一把钥匙才能逃脱，但其中一个玩家被派任务杀死其他人。每次发生谋杀，幸存的玩家们会进行自然语言讨论，然后投票将一名玩家放逐出游戏。我们使用GPT-3、GPT-3.5和GPT-4操控代理进行实验，并发现了欺骗和识别谎言的能力证据。杀手经常否认自己的罪行并指责他人，导致投票结果受到可测量的影响。更先进的模型在24个两两比较中的18个中表现出更高的杀手效果，超越了更小的模型。次要指标提供了证据，表明这种改进并不是通过不同的行动实现的，而是通过在讨论中更强的欺骗能力实现的。总的来说，我们发现了实质性的创新。

    Are current language models capable of deception and lie detection? We study this question by introducing a text-based game called $\textit{Hoodwinked}$, inspired by $\textit{Mafia}$ and $\textit{Among Us}$. Players are locked in a house and must find a key to escape, but one player is tasked with killing the others. Each time a murder is committed, the surviving players have a natural language discussion then vote to banish one player from the game. We conduct experiments with agents controlled by GPT-3, GPT-3.5, and GPT-4 and find evidence of deception and lie detection capabilities. The killer often denies their crime and accuses others, leading to measurable effects on voting outcomes. More advanced models are more effective killers, outperforming smaller models in 18 of 24 pairwise comparisons. Secondary metrics provide evidence that this improvement is not mediated by different actions, but rather by stronger deception capabilities during discussions. Overall, we find substantial
    
[^42]: 使用不匹配关系推理的基于视觉和语言的图像文本匹配

    Grounded Image Text Matching with Mismatched Relation Reasoning. (arXiv:2308.01236v1 [cs.CV])

    [http://arxiv.org/abs/2308.01236](http://arxiv.org/abs/2308.01236)

    本文介绍了一种新颖的基于视觉和语言的图像文本匹配任务，要求模型能够确定图像与文本的关系，并定位所指的对象或者对不匹配的部分进行 grounding。为了解决预训练模型在此任务上的数据效率和长度泛化能力不足的问题，我们提出了一种关系敏感的对应推理网络（RCRN），该网络通过双向信息传递和语言结构引导的关系感知推理来实现。

    

    本文介绍了一种称为“使用不匹配关系推理的基于视觉和语言的图像文本匹配（GITM-MR）”的新型视觉-语言联合任务，用于评估基于Transformer预训练模型的关系理解能力。GITM-MR需要模型首先确定一个表达是否描述了一张图像，然后定位所指的对象或者对文本中的不匹配部分进行 grounding。我们提供了一个用于评估预训练模型在这个任务上表现的基准，重点关注有限数据和超出分布的句子长度的挑战性设置。我们的评估表明，预训练模型缺乏数据效率和长度泛化能力。为了解决这个问题，我们提出了一种称为关系敏感的对应推理网络（RCRN），通过双向信息传递和语言结构引导的关系感知推理，将RCRN解释为一个模块化程序，并在长度泛化和数据效率方面表现出较强的性能。

    This paper introduces Grounded Image Text Matching with Mismatched Relation (GITM-MR), a novel visual-linguistic joint task that evaluates the relation understanding capabilities of transformer-based pre-trained models. GITM-MR requires a model to first determine if an expression describes an image, then localize referred objects or ground the mismatched parts of the text. We provide a benchmark for evaluating pre-trained models on this task, with a focus on the challenging settings of limited data and out-of-distribution sentence lengths. Our evaluation demonstrates that pre-trained models lack data efficiency and length generalization ability. To address this, we propose the Relation-sensitive Correspondence Reasoning Network (RCRN), which incorporates relation-aware reasoning via bi-directional message propagation guided by language structure. RCRN can be interpreted as a modular program and delivers strong performance in both length generalization and data efficiency.
    
[^43]: 在这篇论文中，我们介绍了首次将母语识别（Native Language Identification，NLI）应用于土耳其语的研究。

    Turkish Native Language Identification. (arXiv:2307.14850v1 [cs.CL])

    [http://arxiv.org/abs/2307.14850](http://arxiv.org/abs/2307.14850)

    这项研究首次将母语识别应用于土耳其语,通过分析作者不同语言的写作来预测作者的母语。研究使用了土耳其学习者语料库和三个句法特征来展示其有效性。

    

    在这篇论文中，我们首次将母语识别（NLI）应用于土耳其语。NLI 是通过分析作者不同语言的写作来预测作者的母语。尽管大多数NLI研究都侧重于英语，我们的研究将其范围扩展到土耳其语。我们使用了最近构建的土耳其学习者语料库，并结合了三个句法特征（CFG 产生规则，词性n-gram和函数词）与L2文本，以展示它们在该任务中的有效性。

    In this paper, we present the first application of Native Language Identification (NLI) for the Turkish language. NLI involves predicting the writer's first language by analysing their writing in different languages. While most NLI research has focused on English, our study extends its scope to Turkish. We used the recently constructed Turkish Learner Corpus and employed a combination of three syntactic features (CFG production rules, part-of-speech n-grams and function words) with L2 texts to demonstrate their effectiveness in this task.
    
[^44]: 使用语法演化自动设计语义相似性集合

    Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution. (arXiv:2307.00925v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.00925](http://arxiv.org/abs/2307.00925)

    本研究首次使用语法演化自动设计语义相似性集合，通过自动选择和聚合候选度量来优化集合与人类判断的相关性，提高相似度评估准确性，并证明了使用集合对语义相似性任务的益处。

    

    语义相似性度量在自然语言处理中被广泛应用于多种与计算机相关的任务。然而，没有单一的语义相似性度量适用于所有任务，研究人员经常使用集合策略来确保性能。本研究提出了一种自动设计语义相似性集合的方法。事实上，我们提出的方法首次使用语法演化来自动选择和聚合一组候选度量，以创建一个最大化与人类判断相关性的集合。该方法在多个基准数据集上进行了评估，并与最先进的集合进行了比较，结果显示它可以显著提高相似度评估的准确性，并在某些情况下优于现有方法。因此，我们的研究既展示了使用语法演化来自动比较文本的潜力，也证明了使用集合对语义相似性任务的益处。

    Semantic similarity measures are widely used in natural language processing to catalyze various computer-related tasks. However, no single semantic similarity measure is the most appropriate for all tasks, and researchers often use ensemble strategies to ensure performance. This research work proposes a method for automatically designing semantic similarity ensembles. In fact, our proposed method uses grammatical evolution, for the first time, to automatically select and aggregate measures from a pool of candidates to create an ensemble that maximizes correlation to human judgment. The method is evaluated on several benchmark datasets and compared to state-of-the-art ensembles, showing that it can significantly improve similarity assessment accuracy and outperform existing methods in some cases. As a result, our research demonstrates the potential of using grammatical evolution to automatically compare text and prove the benefits of using ensembles for semantic similarity tasks. The so
    
[^45]: 人类和大型语言模型中的归纳推理

    Inductive reasoning in humans and large language models. (arXiv:2306.06548v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.06548](http://arxiv.org/abs/2306.06548)

    本研究使用GPT-3.5和GPT-4对人类归纳推理中的属性归纳问题进行了实验。结果表明，尽管GPT-3.5有一些困难，但GPT-4的表现与人类相似，除了未能捕捉到前提的非单调性现象。这项工作为人类和机器智能提供了有趣的比较，并提供了用作未来研究基准的两个大型数据集。

    

    大型语言模型的卓越性能引发了人们对其是否能作为普通智能的模型或类似于人类认知的程度的疑问。我们通过将GPT-3.5和GPT-4应用于人类归纳推理中的一个经典问题，即属性归纳，来解决这个问题。通过两个实验，我们获取了人类在多个领域上的属性归纳任务上的判断。尽管GPT-3.5在捕捉人类行为的许多方面上有困难，但GPT-4更加成功：在很大程度上，它的表现与人类的表现在质上相匹配，唯一显著的例外是其未能捕捉到前提的非单调性现象。我们的工作证明了属性归纳可以对人类和机器智能进行有趣的比较，并提供了两个大型数据集，可以作为未来在这一领域中的基准。

    The impressive recent performance of large language models has led many to wonder to what extent they can serve as models of general intelligence or are similar to human cognition. We address this issue by applying GPT-3.5 and GPT-4 to a classic problem in human inductive reasoning known as property induction. Over two experiments, we elicit human judgments on a range of property induction tasks spanning multiple domains. Although GPT-3.5 struggles to capture many aspects of human behaviour, GPT-4 is much more successful: for the most part, its performance qualitatively matches that of humans, and the only notable exception is its failure to capture the phenomenon of premise non-monotonicity. Our work demonstrates that property induction allows for interesting comparisons between human and machine intelligence and provides two large datasets that can serve as benchmarks for future work in this vein.
    
[^46]: 缓解上下文学习的标签偏差

    Mitigating Label Biases for In-context Learning. (arXiv:2305.19148v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.19148](http://arxiv.org/abs/2305.19148)

    本文针对上下文学习（ICL）中的三种标签偏差提出分类法，并提出一种简单的偏差校准方法，使用随机的领域词估算语言模型的标签偏差。

    

    上下文学习（ICL）的各种设计设置，如选择和顺序的上下文示例，可能使模型对某种特定预测偏见，而这种预测并不反映对任务的理解。虽然许多研究讨论了这些设计选择，但对它们进行分类和减缓其影响的系统调查很少。在本文中，我们为文本分类中上下文学习（ICL）中的三种标签偏差定义了一个分类法：香草标签偏差、上下文标签偏差和领域标签偏差（我们首次概念化和检测到）。我们的分析表明，先前的标签偏差校准方法不能解决所有三种偏差。特别是，领域标签偏差使LLM在许多任务上只能实现随机级别的性能，而不管上下文示例的选择如何。为了缓解这些偏差的影响，我们提出一个简单的偏差校准方法，使用随机的领域词估算语言模型的标签偏差。

    Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias a model toward a particular prediction without being reflective of an understanding of the task. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time).  Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model's label bias using random in-domain words f
    
[^47]: 在主流媒体上绘制ChatGPT的地图：情感分析和词频分析的早期量化视角。

    Mapping ChatGPT in Mainstream Media: Early Quantitative Insights through Sentiment Analysis and Word Frequency Analysis. (arXiv:2305.18340v1 [cs.CY])

    [http://arxiv.org/abs/2305.18340](http://arxiv.org/abs/2305.18340)

    本文量化分析了主流媒体对ChatGPT和人工智能的报道趋势和情感态度，发现人们普遍对其持积极态度。然而，主题的词频分析显示，大型科技问题和行为者得到了高度关注，而就业、多样性、伦理、版权、性别和女性等主题则表现不足或完全缺失。本文呼吁在人工智能领域需要更加多元和细致的媒体发言。

    

    ChatGPT是一个由人工智能驱动的聊天机器人，其用户获取和普及性的指数增长，伴随着广泛的主流媒体报道。本文通过对与ChatGPT和人工智能主题相关的10,902条主流新闻标题语料进行文本挖掘和NLP方法的定量数据分析，呈现出早期趋势和情感的发现。情感分析的结果显示，ChatGPT和人工智能在主流媒体中的好感度高于反感度。关于词频结果，65%以上的高频词集中在大型科技问题和行为者上，而就业、多样性、伦理、版权、性别和女性等主题则表现不足或完全缺失，仅占总语料的6%。本文是对主流媒体报道ChatGPT和人工智能的权力结构进行的批判性分析，并强调在人工智能领域需要更加多元和细致的媒体发言。

    The exponential growth in user acquisition and popularity of ChatGPT, an artificial intelligence(AI) powered chatbot, was accompanied by widespread mainstream media coverage. This article presents a quantitative data analysis of the early trends and sentiments revealed by conducting text mining and NLP methods onto a corpus of 10,902 mainstream news headlines related to the subject of ChatGPT and artificial intelligence, from the launch of ChatGPT in November 2022 to March 2023. The findings revealed in sentiment analysis, ChatGPT and artificial intelligence, were perceived more positively than negatively in the mainstream media. In regards to word frequency results, over sixty-five percent of the top frequency words were focused on Big Tech issues and actors while topics such as jobs, diversity, ethics, copyright, gender and women were poorly represented or completely absent and only accounted for six percent of the total corpus. This article is a critical analysis into the power stru
    
[^48]: G3Detector：通用GPT生成文本检测器

    G3Detector: General GPT-Generated Text Detector. (arXiv:2305.12680v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12680](http://arxiv.org/abs/2305.12680)

    G3Detector是一种通用GPT生成文本检测器，能够准确识别多个领域中的合成文本，并且具备在各种模型架构和解码策略下均表现出优异性能的能力。

    

    在大型语言模型（LLM）领域的迅速发展带来了巨大的益处，但必须认识到这些模型的潜在滥用可能引发一系列社会和伦理困境。尽管有许多早期的工作致力于区分合成文本，但大多数现有的检测系统无法识别最新的LLM生成的数据，例如ChatGPT和GPT-4。为了应对这一挑战，我们介绍了一种简单而强大的检测方法，能够在各个领域准确识别合成文本。此外，我们的检测器在各种模型架构和解码策略下都表现出优异的性能，并且具备识别采用强大的检测-躲避技术生成的文本的能力。我们的全面研究凸显了我们提高机器生成文本鲁棒性和效率的承诺。

    The burgeoning progress in the field of Large Language Models (LLMs) heralds significant benefits due to their unparalleled capacities. However, it is critical to acknowledge the potential misuse of these models, which could give rise to a spectrum of social and ethical dilemmas. Despite numerous preceding efforts centered around distinguishing synthetic text, most existing detection systems fail to identify data synthesized by the latest LLMs, such as ChatGPT and GPT-4. In response to this challenge, we introduce an unpretentious yet potent detection approach proficient in identifying synthetic text across a wide array of fields. Moreover, our detector demonstrates outstanding performance uniformly across various model architectures and decoding strategies. It also possesses the capability to identify text generated utilizing a potent detection-evasion technique. Our comprehensive research underlines our commitment to boosting the robustness and efficiency of machine-generated text de
    
[^49]: 非自回归神经机器翻译中的选择性知识蒸馏

    Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation. (arXiv:2303.17910v1 [cs.CL])

    [http://arxiv.org/abs/2303.17910](http://arxiv.org/abs/2303.17910)

    本论文介绍在非自回归神经机器翻译中引入选择性知识蒸馏和渐进蒸馏方法，并在实验中证明该方法可以在NAT模型的训练数据质量和复杂度之间实现灵活权衡，有助于NAT超越基线。

    

    通过序列级知识蒸馏的方式，非自回归变压器（NAT）在神经机器翻译任务中取得了巨大的成功。然而，现有的知识蒸馏存在副作用，如将教师机中的错误传播到NAT学生中，这可能限制NAT模型的进一步改进，并且很少在现有研究中讨论。本文通过引入一个NAT评估器来进行选择性知识蒸馏，选择高质量且易于学习的NAT友好目标。此外，我们引入了一种简单而有效的渐进蒸馏方法，以提高NAT性能。在多个WMT语言方向和几个代表性的NAT模型上进行实验结果显示，我们的方法可以实现NAT模型训练数据质量和复杂度之间的灵活权衡，达到了强大的性能。进一步的分析表明，只蒸馏5％的原始翻译就可以帮助NAT超越基线。

    Benefiting from the sequence-level knowledge distillation, the Non-Autoregressive Transformer (NAT) achieves great success in neural machine translation tasks. However, existing knowledge distillation has side effects, such as propagating errors from the teacher to NAT students, which may limit further improvements of NAT models and are rarely discussed in existing research. In this paper, we introduce selective knowledge distillation by introducing an NAT evaluator to select NAT-friendly targets that are of high quality and easy to learn. In addition, we introduce a simple yet effective progressive distillation method to boost NAT performance. Experiment results on multiple WMT language directions and several representative NAT models show that our approach can realize a flexible trade-off between the quality and complexity of training data for NAT models, achieving strong performances. Further analysis shows that distilling only 5% of the raw translations can help an NAT outperform i
    
[^50]: LMExplainer：一种加强语言模型解释能力的知识提升模块

    LMExplainer: a Knowledge-Enhanced Explainer for Language Models. (arXiv:2303.16537v1 [cs.CL])

    [http://arxiv.org/abs/2303.16537](http://arxiv.org/abs/2303.16537)

    LMExplainer是一种知识增强的语言模型解释模块，使用知识图和图注意力神经网络来提取关键决策信号，为用户提供可理解的解释。

    

    巨型语言模型（如GPT-4）非常强大，可以处理各种自然语言处理（NLP）任务。然而，由于多层非线性模型结构和数百万个参数，很难解释其结果。对于用户而言，了解模型的工作方式缺乏理解，可能使模型在现实世界的应用中具有不可靠性和危险性。大多数最近的工作利用注意力权重来提供模型预测的解释。但是，基于注意力的解释无法支持不断增长的模型复杂性，并且无法推理其决策过程。因此，我们提出了LMExplainer，一种为语言模型提供人类可理解解释的知识增强模块。我们使用知识图和图注意力神经网络来提取LM的关键决策信号。同时，我们探讨解释能否也帮助人工智能更好地理解任务。

    Large language models (LMs) such as GPT-4 are very powerful and can process different kinds of natural language processing (NLP) tasks. However, it can be difficult to interpret the results due to the multi-layer nonlinear model structure and millions of parameters. Lack of understanding of how the model works can make the model unreliable and dangerous for everyday users in real-world scenarios. Most recent works exploit the weights of attention to provide explanations for model predictions. However, pure attention-based explanation is unable to support the growing complexity of the models, and cannot reason about their decision-making processes. Thus, we propose LMExplainer, a knowledge-enhanced interpretation module for language models that can provide human-understandable explanations. We use a knowledge graph (KG) and a graph attention neural network to extract the key decision signals of the LM. We further explore whether interpretation can also help AI understand the task better
    
[^51]: SPeC：软提示校准在临床笔记摘要中降低性能变异的研究

    SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization. (arXiv:2303.13035v1 [cs.CL])

    [http://arxiv.org/abs/2303.13035](http://arxiv.org/abs/2303.13035)

    研究通过引入软提示嵌入，提出Soft Prompt-Based Calibration (SPeC)管道，来减轻输入变量对输出多样性的影响，降低性能变异. 此方法不仅比大语言模型(LLM)性能稳定，而且在临床笔记摘要任务上表现优于最先进的模型.

    

    电子健康记录（EHR）存储着包括病历、诊断、治疗和检测结果在内的大量患者信息。这些记录对于医疗保健专业人员做出明智的患者护理决策非常关键。摘要临床笔记可以帮助医疗保健专业人员更好地发现潜在健康风险，以及做出更好的决策。这一过程通过确保医疗保健专业人员可以访问最相关和最新的患者数据，有助于减少错误并提高患者的护理效果。最近的研究表明，将提示与大语言模型（LLM）相结合可以显著提高摘要任务的效率。然而，我们发现这种方法也会导致输出方差增加，即使提示意义相似，输出也会有明显的差异。为了解决这一挑战，我们引入了一个模型无关的软提示校准（SPeC）流程，该流程采用软提示嵌入来减轻输入变量对输出多样性的影响。我们的实验表明，SPeC不仅可以降低LLM的性能变异，而且在临床笔记摘要任务上优于现有的最先进模型。

    Electronic health records (EHRs) store an extensive array of patient information, encompassing medical histories, diagnoses, treatments, and test outcomes. These records are crucial for enabling healthcare providers to make well-informed decisions regarding patient care. Summarizing clinical notes further assists healthcare professionals in pinpointing potential health risks and making better-informed decisions. This process contributes to reducing errors and enhancing patient outcomes by ensuring providers have access to the most pertinent and current patient data. Recent research has shown that incorporating prompts with large language models (LLMs) substantially boosts the efficacy of summarization tasks. However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings. To tackle this challenge, we introduce a model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft prom
    
[^52]: 评估 ChatGPT 作为回答复杂问题的问答系统

    Evaluation of ChatGPT as a Question Answering System for Answering Complex Questions. (arXiv:2303.07992v1 [cs.CL])

    [http://arxiv.org/abs/2303.07992](http://arxiv.org/abs/2303.07992)

    本论文评估了基于ChatGPT模型的问答系统在回答复杂问题方面的能力，通过一个分类框架对潜在的问题特征进行分类，通过黑盒测试规范CheckList评估模型性能。

    

    ChatGPT 是一个强大的大型语言模型，已在自然语言理解方面取得了显著进展。然而，该模型的性能和局限性仍需要进行广泛评估。由于 ChatGPT 覆盖维基百科等资源并支持自然语言问答，因此它引起了作为传统知识库问答（KBQA）模型替代品的关注。复杂问题回答是 KBQA 的一项挑战性任务，全面测试了模型在语义解析和推理方面的能力。为了评估 ChatGPT 作为一个使用自己知识回答复杂问题的问答系统的性能，我们提出了一个框架来评估其回答复杂问题的能力。我们的方法涉及对复杂问题的潜在特征进行分类，并使用多个标签描述每个测试问题，以识别组合推理。根据 Ribeir 提出的 CheckList 的黑盒测试规范，我们评估了ChatGPT模型的性能。

    ChatGPT is a powerful large language model (LLM) that has made remarkable progress in natural language understanding. Nevertheless, the performance and limitations of the model still need to be extensively evaluated. As ChatGPT covers resources such as Wikipedia and supports natural language question answering, it has garnered attention as a potential replacement for traditional knowledge based question answering (KBQA) models. Complex question answering is a challenge task of KBQA, which comprehensively tests the ability of models in semantic parsing and reasoning. To assess the performance of ChatGPT as a question answering system (QAS) using its own knowledge, we present a framework that evaluates its ability to answer complex questions. Our approach involves categorizing the potential features of complex questions and describing each test question with multiple labels to identify combinatorial reasoning. Following the black-box testing specifications of CheckList proposed by Ribeir
    
[^53]: 大型语言模型中的类比推理的紧急性

    Emergent Analogical Reasoning in Large Language Models. (arXiv:2212.09196v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.09196](http://arxiv.org/abs/2212.09196)

    GPT-3在许多类比任务中表现出与甚至超越人类的能力，揭示了大型语言模型的紧急能力。

    

    大型语言模型的出现重新点燃了人们对于这样一种问题的辩论：足够的训练数据是否能使这些通用模型内涵人类认知能力。特别的，这些模型的零样本推理能力——不经过任何直接训练，就能够推理出新问题，特别令人关注。在人类认知中，这种能力与一种通过类比推理的能力密切相关。在本文中，我们在一系列类比任务中进行了直接的人机比较，包括一种新颖的基于文本的矩阵推理任务，该任务与 Raven's Progressive Matrices密切相关。我们发现，GPT-3呈现出了一种令人惊讶的抽象模式归纳能力，甚至在大部分情况下与或甚至超越了人类的能力。我们的结果表明，像GPT-3这样的大型语言模型已经获得了在广泛的类比问题上找到零样本解决方案的紧急能力。

    The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot, without any direct training. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here, we performed a direct comparison between human reasoners and a large language model (the text-davinci-003 variant of GPT-3) on a range of analogical tasks, including a novel text-based matrix reasoning task closely modeled on Raven's Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.
    
[^54]: 使用基于句子嵌入的词义归纳自动构建WordNet

    Automatic WordNet Construction using Word Sense Induction through Sentence Embeddings. (arXiv:2204.03251v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.03251](http://arxiv.org/abs/2204.03251)

    本文提出了一种使用无标签语料库和基于句子嵌入的语言模型自动构建WordNet的方法。通过这种方法，我们生成了一个新的WordNet（FilWordNet），以替代并改进菲律宾语中过时的WordNet，并且不需要人工监督。

    

    语言资源如WordNet对于不同的自然语言任务和应用至关重要。然而，对于低资源语言（如菲律宾语），现有的WordNet过时且不完整，并且生成新的WordNet可能需要大量的时间和资源。本文提出了一种使用无标签语料库和基于句子嵌入的语言模型自动构建WordNet的方法。我们通过这种方法，生成了一个新的WordNet（FilWordNet），以替代并改进菲律宾语中过时的WordNet。通过将我们自动诱导出的词义和词汇集与Princeton WordNet中的词义进行匹配，以及将词汇集与旧的菲律宾语WordNet进行比较，我们对其进行了评估。经验证明，我们的方法可以自动诱导现有的词义和词汇集，也可以潜在地自动诱导新的词义和词汇集，并且不需要人工监督。

    Language resources such as wordnets remain indispensable tools for different natural language tasks and applications. However, for low-resource languages such as Filipino, existing wordnets are old and outdated, and producing new ones may be slow and costly in terms of time and resources. In this paper, we propose an automatic method for constructing a wordnet from scratch using only an unlabeled corpus and a sentence embeddings-based language model. Using this, we produce FilWordNet, a new wordnet that supplants and improves the outdated Filipino WordNet. We evaluate our automatically-induced senses and synsets by matching them with senses from the Princeton WordNet, as well as comparing the synsets to the old Filipino WordNet. We empirically show that our method can induce existing, as well as potentially new, senses and synsets automatically without the need for human supervision.
    

