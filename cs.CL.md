# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Continual Learning for Large Language Models: A Survey](https://rss.arxiv.org/abs/2402.01364) | 这篇综述文章调查了大型语言模型(LLMs)的持续学习，使用了一种新颖的多阶段分类方案对持续学习技术进行了分类，指出了该领域面临的挑战和未来工作方向。 |
| [^2] | [Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models](https://rss.arxiv.org/abs/2402.01345) | 本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。 |
| [^3] | [Getting the most out of your tokenizer for pre-training and domain adaptation](https://rss.arxiv.org/abs/2402.01035) | 本文通过训练专用分词器，对分词器设计进行了消毒和分析，发现分词器的大小、正则表达式和训练数据对模型性能有重要影响，并提供了相应的超参数选择建议和切换分词器的方法。 |
| [^4] | [Edu-ConvoKit: An Open-Source Library for Education Conversation Data](https://arxiv.org/abs/2402.05111) | Edu-ConvoKit是一种用于处理教育会话数据的开源库，它解决了教育会话数据分析资源稀缺的问题，提供了全面的文档和附加资源。 |
| [^5] | [Image captioning for Brazilian Portuguese using GRIT model](https://arxiv.org/abs/2402.05106) | 本文早期开发了一个用于巴西葡萄牙语的图像字幕生成模型，使用了GRIT模型，并对其进行了调整以适应巴西葡萄牙语数据集，实现了更高效的图像字幕生成方法。 |
| [^6] | [A Roadmap to Pluralistic Alignment](https://arxiv.org/abs/2402.05070) | 这篇论文提出了一条通向多元对齐的路线图，以解决设计AI系统能够服务于人们具有不同价值观和观点的需求。论文介绍了对齐定义和实现多元主义的三种方式，并提出了三种多元基准类别来评估和测试多元对齐的效果。 |
| [^7] | [SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models](https://arxiv.org/abs/2402.05044) | SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。 |
| [^8] | [How BERT Speaks Shakespearean English? Evaluating Historical Bias in Contextual Language Models](https://arxiv.org/abs/2402.05034) | 本文评估了基于BERT的上下文语言模型对早期现代英语和现代英语的准确性，并分析了其中的历史偏见。 |
| [^9] | [Pedagogical Alignment of Large Language Models](https://arxiv.org/abs/2402.05000) | 本文介绍了教学对齐的大型语言模型（LLM）的新概念，并探讨了通过建设性反馈和提示指导学生解决复杂问题的方法。相比于传统方法，这种对齐方法以及采用人类反馈的强化学习方法能够更好地对齐LLM，提供更优质的教育支持。 |
| [^10] | [An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration](https://arxiv.org/abs/2402.04978) | 本文提出了一种通过知识图谱和LLMs合作训练的推理方案，该方案解决了大型语言模型在实际应用中的虚构问题、知识更新不足以及推理过程的透明度有限等挑战，实现了更可靠的知识推理和推理结果追踪。 |
| [^11] | [Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models?](https://arxiv.org/abs/2402.04967) | 本文研究了多模式仇恨迷因检测中的跨领域泛化挑战，并发现仇恨迷因的文本部分对于泛化至不同领域的分类器至关重要，而图像部分对特定训练数据集敏感。实验证明仇恨文本分类器在零样本情况下表现类似于仇恨迷因分类器。黑盒解释表明文本模态对模型性能的贡献较大，但引入图像标题后贡献降低。新的混淆数据集评估显示文本混淆因素表现较好。 |
| [^12] | [Reconfidencing LLMs from the Grouping Loss Perspective](https://arxiv.org/abs/2402.04957) | 本论文研究了大型语言模型的信心问题，发现现有的校准方法不足以解决由于分组损失导致的预测分数与实际概率偏离的问题。我们提出了一种解决方案，可以重新确定LLMs，改善它们的自信度。 |
| [^13] | [Prompting Implicit Discourse Relation Annotation](https://arxiv.org/abs/2402.04918) | 本研究旨在提升ChatGPT对隐式话语关系的识别，尝试了多种提示技术，但实验结果显示即使进行复杂的提示工程，隐式话语关系分类在零样本或少样本设置下仍难以解决。 |
| [^14] | [Personalized Text Generation with Fine-Grained Linguistic Control](https://arxiv.org/abs/2402.04914) | 本文研究了细粒度语言控制下的个性化文本生成，引入了一个新的基准来评估生成模型在多个细粒度语言属性上的表现，探索了影响模型性能的因素，并公开了代码、数据和预训练模型。 |
| [^15] | [L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ](https://arxiv.org/abs/2402.04902) | L4Q是一种参数高效的量化感知训练算法，通过基于LoRA的学习的量化步长，解决了大型语言模型中量化训练的挑战。 |
| [^16] | [Detecting Generated Native Ads in Conversational Search](https://arxiv.org/abs/2402.04889) | 本论文研究了LLM是否可以用作对抗生成式原生广告的对策，并通过构建广告倾向查询数据集和带自动整合广告的生成答案数据集进行实验证明。 |
| [^17] | [On Provable Length and Compositional Generalization](https://arxiv.org/abs/2402.04875) | 本研究针对包括深度集合、变压器、状态空间模型和简单递归神经网络等多种架构，探索了可证明的长度和组合泛化，认为对于长度和组合泛化，不同架构需要不同程度的表示识别。 |
| [^18] | [CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay](https://arxiv.org/abs/2402.04858) | CodeIt是一种具备优先级回顾重放的自我改进语言模型方法，通过将目标重标记为采样程序的实际输出，有效解决了程序合成中奖励稀疏性的问题，并在抽象和推理语料库（ARC）上实现了成功的跨任务泛化。 |
| [^19] | [Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey](https://arxiv.org/abs/2402.04854) | 该论文提出了一种分层树状知识图谱和推荐系统，帮助初学者研究者进行研究调研，填补了现有导航知识图谱的不足，并解决了学术论文推荐系统中高文本相似性带来的困惑。 |
| [^20] | [PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition](https://arxiv.org/abs/2402.04838) | 本研究提出了PaDeLLM-NER，一种能够在大型语言模型中实现并行解码，从而显著减少命名实体识别的生成延迟，同时保持预测质量和性能。 |
| [^21] | [Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning](https://arxiv.org/abs/2402.04833) | 通过选择标准数据集中响应最长的1,000条指示作为基准线，可以在各种语言模型和数据集上实现对齐的高性能，同时对长指示进行轻量级改进进一步提升微调语言模型的能力。 |
| [^22] | [Learning Communication Policies for Different Follower Behaviors in a Collaborative Reference Game](https://arxiv.org/abs/2402.04824) | 本文研究了在协同参考游戏中学习适应不同追随者行为的通信策略。通过强化学习算法（PPO），证明了神经代理能够与具有不同自信度和自主性的追随者良好地协作，并且在学习信号中考虑了沟通努力的因素。 |
| [^23] | [Aspect-Based Sentiment Analysis for Open-Ended HR Survey Responses](https://arxiv.org/abs/2402.04812) | 本文提出了一种面向开放式人力资源调查回应的基于方面的情感分析方法，通过回应聚类识别关键方面，并采用荷兰BERT模型进行分析。该方法可用于支持员工生命周期管理。 |
| [^24] | [Direct Language Model Alignment from Online AI Feedback](https://arxiv.org/abs/2402.04792) | 本论文提出了一种名为OAIF的方法，通过在线反馈来改善DAP方法，该方法使用LLM作为标注器，从而在多个任务中展示出优于离线DAP和RLHF的性能 |
| [^25] | [MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark](https://arxiv.org/abs/2402.04788) | 本文引入了MLLM作为法官的新基准测试，用于评估MLLM在辅助法官方面的能力。研究结果显示，MLLM在对比评估任务中展示出了非常类人的辨别能力，但在评分评估和批量排序任务中与人类偏好存在显著的差异。此外，即使对于先进模型如GPT-4V，MLLM仍然面临着偏见、幻觉式回答和不一致性等判断方面的挑战。 |
| [^26] | [A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models](https://arxiv.org/abs/2402.04787) | 本研究提出了一种基于假设的统计框架，用于分析LLM模型的自我合理能力。通过将LLM生成的解释与贝叶斯网络的决策过程进行比较，可以评判两者的相似性。实验证明，生成的模型与GPT-3.5不相似。这一研究结果对于理解LLL模型的解释能力具有重要意义。 |
| [^27] | [StableMask: Refining Causal Masking in Decoder-only Transformer](https://arxiv.org/abs/2402.04779) | StableMask是一种在仅解码Transformer中改进因果屏蔽的无参数方法，通过引入伪注意力值来平衡注意力分布，并通过逐渐减小的屏蔽比率来编码绝对位置信息。 |
| [^28] | [Large Language Models As Faithful Explainers](https://arxiv.org/abs/2402.04678) | 本论文提出了一个生成解释框架（xLLM），用于提高大型语言模型（LLMs）自然语言格式解释的可信度。通过一个评估器来量化解释的可信度，并通过迭代优化过程来提高可信度。 |
| [^29] | [Source Identification in Abstractive Summarization](https://arxiv.org/abs/2402.04677) | 该论文研究了摘要自动生成中的源信息识别问题，并通过分析源句子的方法来探索抽象摘要的生成方式。他们通过注释源句子和比较多种方法建立了一个强基线，并发现在不同情景下，基于困惑度的方法和基于相似度的方法表现较好。 |
| [^30] | [TransLLaMa: LLM-based Simultaneous Translation System](https://arxiv.org/abs/2402.04636) | 本研究通过对一个小数据集进行微调，展示了一个预训练的开源LLM在同时机器翻译任务中控制输入分段的能力，从而消除了独立政策的需要，并实现了与最新基线相当的BLEU分数。同时，闭源模型GPT-4在零-shot下也显示出了令人鼓舞的结果，为提升未来的SiMT系统提供了有希望的方向。 |
| [^31] | [The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends](https://arxiv.org/abs/2402.04631) | 该论文讨论了认知策略增强的说服对话机器人的未来发展，强调了对于实现智能和人类化对话系统的重要性。该领域的研究者通过利用大型语言模型的进展，使对话机器人在上下文理解和回应生成方面取得了巨大进步。然而，要实现人类般的说服水平，说服对话机器人需要融入认知心理学的知识。 |
| [^32] | [SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph](https://arxiv.org/abs/2402.04627) | 在本研究中，我们针对生命科学知识图谱上的问题回答对OpenLlama LLM进行了微调，并提出了一种数据增强方法，以扩展现有查询集合，从而能够进行微调，即使训练数据稀缺。我们还研究了查询中语义线索的作用。 |
| [^33] | [MEMORYLLM: Towards Self-Updatable Large Language Models](https://arxiv.org/abs/2402.04624) | MEMORYLLM是一个自更新的大型语言模型，其中包括Transformer和一个固定大小的内存池，能够有效地整合新知识并保持长期信息保留能力。即使在近百万次内存更新后，MEMORYLLM仍能保持操作的完整性。 |
| [^34] | [InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory](https://arxiv.org/abs/2402.04617) | InfLLM是一种无需训练的基于记忆的方法，用于揭示LLMs处理超长序列的内在能力。它通过存储远距离上下文和高效的注意计算机制，允许LLMs有效处理具有流式输入的长序列。 |
| [^35] | [TinyLLM: Learning a Small Student from Multiple Large Language Models](https://arxiv.org/abs/2402.04616) | TinyLLM是一种从多个大型语言模型中学习小型学生模型的知识蒸馏范式，旨在解决知识多样性有限和缺乏上下文信息等问题，并鼓励学生模型理解答案背后的原理。 |
| [^36] | [Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models](https://arxiv.org/abs/2402.04614) | 本文讨论了大型语言模型生成的自我解释中的信实性与可信度之间的差异。虽然这些解释在逻辑上是合乎情理且连贯的，但并不一定与模型的推理过程一致，引发对其信实性的担忧。 |
| [^37] | [Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach](https://arxiv.org/abs/2402.04609) | 本研究提出了一种神经程序员-解释器方法，通过后编辑来改进大型语言模型在文本生成任务中的性能。实验证明，在交叉领域环境中，该方法显著超越其他最先进的后编辑方法，提升了GPT-3.5在逻辑形式到文本转换和低资源机器翻译中的性能。 |
| [^38] | [Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector](https://arxiv.org/abs/2402.04601) | 本文提出了一种提升对齐的中文语法错误修正器，旨在解决使用自回归生成模型时面临的过度修正挑战。该方法适用于序列到序列模型和仅解码的大型语言模型，并通过对齐模型进行多轮修正，提高修正效果。 |
| [^39] | [UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset](https://arxiv.org/abs/2402.04588) | 本论文构建了一个开源的多语言监督微调数据集UltraLink，通过引入基于知识的数据增强方法提升了语言模型在文化特定知识上的能力，同时发现现代语言模型具有强大的跨语言迁移能力，减少了语言无关数据集的需求。 |
| [^40] | [Can Large Language Model Agents Simulate Human Trust Behaviors?](https://arxiv.org/abs/2402.04559) | 大语言模型代理能够模拟人类的信任行为，表现出在信任游戏中的信任行为，并且与人类行为具有高度一致性，但存在一些偏见和对代理与人类的差异。 |
| [^41] | [Share What You Already Know: Cross-Language-Script Transfer and Alignment for Sentiment Detection in Code-Mixed Data](https://arxiv.org/abs/2402.04542) | 本研究提出了一种跨语言-脚本知识共享架构，利用各种语言脚本中文本表示的跨关注和对齐，以提高混合语言数据中的情感检测效果。 |
| [^42] | [SumRec: A Framework for Recommendation using Open-Domain Dialogue](https://arxiv.org/abs/2402.04523) | 本研究提出了一个新的框架SumRec，用于从开放领域的对话中推荐个性化信息。该框架利用大型语言模型生成对话中说话者信息的摘要，并根据用户类型推荐物品信息，实验证明SumRec框架比基准方法提供更好的推荐。 |
| [^43] | [Online Cascade Learning for Efficient Inference over Streams](https://arxiv.org/abs/2402.04513) | 这项研究提出了在线级联学习的方法，通过学习一个“级联”模型，从容量较低的模型到强大的语言模型，以及推迟策略，可以在流数据处理中同时保证准确性和降低推理成本。 |
| [^44] | [Developments in Sheaf-Theoretic Models of Natural Language Ambiguities](https://arxiv.org/abs/2402.04505) | 本论文扩展了层理论模型从词汇歧义到篇章歧义，通过计算新的上下文性度量，发现上下文模型的比例大幅增加，并通过将Winograd Schema建模为Bell-CHSH场景，展示了层理论模型在处理具有指代歧义的自然语言挑战上的应用。 |
| [^45] | [The Fine-Grained Complexity of Gradient Computation for Training Large Language Models](https://arxiv.org/abs/2402.04497) | 本文研究了训练大型语言模型中梯度计算的复杂性，证明了在某些参数区域内可以以几乎线性时间进行前向计算，但在其余参数区域内需要超过二次时间，这对于LLM训练的每个步骤都具有重要意义。 |
| [^46] | [ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation](https://arxiv.org/abs/2402.04492) | 本文介绍了ColorSwap数据集，用于评估和改进多模态模型在匹配物体和颜色方面的能力。通过将颜色词重新排序以修改不同的对象，该数据集可以测试模型在这项任务上的鲁棒性。尽管目前的模型在这个任务上仍不够稳定，但通过更先进的提示技术可能会有所改善。 |
| [^47] | [Detecting Mode Collapse in Language Models via Narration](https://arxiv.org/abs/2402.04477) | 通过研究来自三个OpenAI语言模型的故事，我们发现GPT-3的更新版本逐渐出现了“模式崩溃”的现象。 |
| [^48] | [Dual-View Visual Contextualization for Web Navigation](https://arxiv.org/abs/2402.04476) | 本文提出了一种通过网页截图中元素的“双视图”来进行 HTML 元素的背景化的方法，这种方法通过将每个元素与其邻居元素进行背景化，使用文本和视觉特征，使得HTML元素的结果表示对于代理执行操作更加信息丰富。 |
| [^49] | [Evaluating Embeddings for One-Shot Classification of Doctor-AI Consultations](https://arxiv.org/abs/2402.04442) | 本研究评估了用于医生-AI会诊的单次分类的嵌入技术，结果表明这些嵌入技术能够可靠且灵活地捕捉文本的语义特征，特别是Word2Vec、GloVe和字符ngram嵌入技术表现良好。 |
| [^50] | [Structured Entity Extraction Using Large Language Models](https://arxiv.org/abs/2402.04437) | 本论文提出了一种使用大型语言模型的结构化实体提取方法，在此任务上通过引入AESOP度量评估模型性能，并将整个提取任务分解为多个阶段，相较于基准模型取得了更好的效果，为未来结构化实体提取的进一步发展提供了有希望的方向。 |
| [^51] | [Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton](https://arxiv.org/abs/2402.04411) | 本文介绍了DFA-LLM（确定有限自动机增强的大规模语言模型）框架，通过嵌入从对话中学习到的确定有限自动机在大规模语言模型中，使得对话代理能够生成具有规范合规性的回复，并在广泛的基准测试中验证了其有效性。 |
| [^52] | [Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2402.04401) | 这项研究通过个性化参数高效调整模块（PEFT）实现了大规模语言模型（LLM）的民主化，使用户能够拥有和使用他们自己的LLM，解决了传统方法中的定制能力和隐私问题。 |
| [^53] | [QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://arxiv.org/abs/2402.04396) | QuIP#是一种使用哈达玛德非相干性和格书的权重量化方法，能够在极限压缩范围下达到最先进的结果，并具有快速推理的优势。 |
| [^54] | [The World of Generative AI: Deepfakes and Large Language Models](https://arxiv.org/abs/2402.04373) | 生成型人工智能包括Deepfakes和大型语言模型(LLMs)。Deepfakes能够传播错误信息和改变事实，而LLMs则具有生成通用语言的能力。然而，这些技术的道德使用是个重要的关注点。 |
| [^55] | [The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry](https://arxiv.org/abs/2402.04347) | Hedgehog是一种具有Softmax模仿的可学习线性注意力，通过保持尖锐和单调性来弥补线性注意力在质量上的不足。 |
| [^56] | [LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured Text](https://arxiv.org/abs/2402.04335) | 本研究利用LLMs构建数据集，通过微调模型和使用闭源LLMs进行少样本实验，成功实现了非结构化文本中法律违规行为的检测和与受害者的关联。结果显示我们的设置适用于这两个任务，F1分数分别为62.69％和81.02％。研究最终公开发布了数据集和代码，以推动法律自然语言处理领域的进一步研究。 |
| [^57] | [LESS: Selecting Influential Data for Targeted Instruction Tuning](https://arxiv.org/abs/2402.04333) | LESS是一种优化感知且实际高效的算法，用于在大型语言模型中选择具有影响力的数据以开发特定能力，它采用低秩梯度相似性搜索方法进行指令数据选择。 |
| [^58] | [Training Language Models to Generate Text with Citations via Fine-grained Rewards](https://arxiv.org/abs/2402.04315) | 本文提出了一种使用细粒度奖励训练语言模型生成高质量引用的有效框架，并在常见的大型语言模型训练策略上进行了实证分析。 |
| [^59] | [BiLLM: Pushing the Limit of Post-Training Quantization for LLMs](https://arxiv.org/abs/2402.04291) | BiLLM是一种针对预训练LLMs的1位后训练量化方案，通过识别重要的权重和优化二值化，成功实现了高准确度的推理。 |
| [^60] | [ProtAgents: Protein discovery via large language model multi-agent collaborations combining physics and machine learning](https://arxiv.org/abs/2402.04268) | ProtAgents是一个基于大型语言模型的平台，通过多智能体的协作，在动态环境中解决复杂的新型蛋白质设计任务，并结合物理学和机器学习的方法。 |
| [^61] | [Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science](https://arxiv.org/abs/2402.04247) | 本文探讨了科学领域中基于LLM的智能机器人的漏洞与风险，并强调了对安全措施的重要性。 |
| [^62] | [Can Generative Agents Predict Emotion?](https://arxiv.org/abs/2402.04232) | 本研究探讨了生成型智能体在感知新事件时情绪状态的演变，通过比较新经验和过去记忆来获得理解新经验的能力，并通过情感测试捕捉智能体的情绪状态。 |
| [^63] | [Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims](https://arxiv.org/abs/2402.03962) | 这篇论文讨论了在寻求人工通用智能的过程中，人们对于大型语言模型的类人特质过度归因的现象，并呼吁学术界在解读和传播关于AI研究的内容时要保持谨慎。 |
| [^64] | [Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification](https://arxiv.org/abs/2402.03780) | 本文通过分析文体线索比较人类注释和机器分类的方法，揭示了宣传语言的特征，并提出了一个多源、多语言、多模态的数据集PPN。结果表明，人类注释者能够可靠地区分宣传新闻和常规新闻。研究还比较了不同的自然语言处理技术，并提供了一些有关文体线索的发现。 |
| [^65] | [Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models](https://arxiv.org/abs/2402.03659) | 这个论文介绍了使用大型语言模型生成可解释的股票预测的方法，并提出了Summarize-Explain-Predict（SEP）模型来解决股票预测中的解释问题和数据标注成本的挑战。 |
| [^66] | [VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation](https://arxiv.org/abs/2402.03561) | VLN-Video利用行车视频的多样室外环境和自动生成的导航指令与动作，通过深度学习方法提高了室外视觉与语言导航的性能。 |
| [^67] | [Increasing Trust in Language Models through the Reuse of Verified Circuits](https://arxiv.org/abs/2402.02619) | 本文介绍了一种通过重复使用经过验证的电路来增加语言模型的可信度的方法。研究者通过构建数学和逻辑规范的框架，并对一个n位整数加法模型进行完全验证。他们插入训练好的加法模型到一个未经训练的模型中，通过训练组合模型执行加法和减法。他们发现加法电路在这两个任务中得到了广泛的重复使用，从而简化了减法模型的验证。 |
| [^68] | [Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems](https://arxiv.org/abs/2402.01748) | 本文提出了大型多模型(LMMs)作为AI原生无线系统的通用基础模型的设计框架，通过处理多模态感知数据、通过因果推理和检索增强生成(RAG)将物理符号表示与无线系统联系起来，并通过无线环境反馈实现可教导性，从而促进动态网络配置。 |
| [^69] | [CFTM: Continuous time fractional topic model](https://arxiv.org/abs/2402.01734) | CFTM是一种新的动态主题建模方法，通过使用分数布朗运动来识别随时间变化的主题和词分布的正负相关性，揭示长期依赖性或粗糙度。实证研究结果表明，该模型能够有效地识别和跟踪主题的长期依赖性或粗糙度。 |
| [^70] | [Contextualization Distillation from Large Language Model for Knowledge Graph Completion](https://arxiv.org/abs/2402.01729) | 本论文介绍了一种从大型语言模型中提取上下文信息用于知识图谱补全的策略，该策略能克服现有语料库的限制，显著提高了预训练语言模型在知识图谱补全中的性能。 |
| [^71] | [Guidance for AI-Mediated Communication: AI Does Not Alter Perceptions of Text Messages](https://arxiv.org/abs/2402.01726) | 这项研究通过调查参与者对18条随机标记的文本消息的感知，探讨了大型语言模型在文本消息撰写中的辅助使用可能对感知产生的影响。 |
| [^72] | [APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data Annotation](https://arxiv.org/abs/2402.01697) | APT-Pipe is an automated prompt-tuning tool that enhances ChatGPT's text classification performance by automatically tuning prompts on any given dataset, resulting in a significant improvement in weighted F1-score across twelve experimented datasets. |
| [^73] | [OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838) | OLMo是一种真正开放的语言模型，旨在提供给研究社区强大的工具，以促进对语言模型科学的研究和创新。 |
| [^74] | [Using eye tracking to investigate what native Chinese speakers notice about linguistic landscape images](https://arxiv.org/abs/2312.08906) | 本文利用眼动追踪技术研究了母语为汉语的人对语言景观的注意点，发现其关注程度高于一般景观，可能是因为语言景观的信息密度较高。 |
| [^75] | [Human-Readable Fingerprint for Large Language Models](https://arxiv.org/abs/2312.04828) | 这项研究介绍了一种大型语言模型的人类可读指纹，可以唯一识别出基本模型，并且不暴露模型参数或干扰训练。通过观察和验证，研究发现模型参数的向量方向在预训练后保持稳定，成为识别基本模型的重要条件。 |
| [^76] | [CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models](https://arxiv.org/abs/2311.16421) | CDEval是一个新的基准，用于评估大规模语言模型的文化维度。通过对六个文化维度和七个领域的全面实验，我们揭示了主流语言模型的文化特征、一致性和差异。这些发现强调了在开发语言模型时融入文化考虑的重要性。 |
| [^77] | [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://arxiv.org/abs/2311.14455) | 本文提出了一种新的威胁，攻击者通过毒害训练数据向语言模型中嵌入“越狱后门”，并通过添加特定的触发词使模型产生有害的回应。这种通用越狱后门比之前的研究更强大，且较难被察觉。研究探究了RLHF设计中的决策对其鲁棒性的影响，并发布了一组被毒害模型的基准测试数据，以促进未来对通用越狱后门的研究。 |
| [^78] | [(Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving LLM APIs](https://arxiv.org/abs/2311.11123) | 本文重新审视了不断演变的LLM API的回归测试，并指出了它们需要对传统测试方法进行基本变革的原因，包括存在不同的正确性概念、提示脆弱性和非确定性等。 |
| [^79] | [Labeled Interactive Topic Models](https://arxiv.org/abs/2311.09438) | 这篇论文介绍了一种用户友好的交互式神经主题模型，通过用户分配单词标签来更新主题模型，使得主题更加相关和准确。这种方法包括可训练和后训练集成两种不同类型的神经主题模型。 |
| [^80] | [Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap.](http://arxiv.org/abs/2401.10034) | 该论文调查了大语言模型和进化计算之间的相互作用，并提出了在黑盒设置下进一步提升大语言模型性能的优化框架，以及将大语言模型与进化算法结合应用于各种任务的方法。 |
| [^81] | [Towards Boosting Many-to-Many Multilingual Machine Translation with Large Language Models.](http://arxiv.org/abs/2401.05861) | 本文旨在提升基于大型语言模型的多对多多语言机器翻译能力，尤其是零翻译方向。通过引入跨语言一致性正则化XConST，并采用适当的提示策略，我们改善了零翻译性能，并在实验中得到了一致的改进。 |
| [^82] | [Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs.](http://arxiv.org/abs/2401.04319) | 本文探索了一种通过类比推理增强的LLMs来实现对营销人员需求的结构化理解的新方式，使非专业营销人员能够仅凭需求的自然语言形式选择目标用户。 |
| [^83] | [PEFT for Speech: Unveiling Optimal Placement, Merging Strategies, and Ensemble Techniques.](http://arxiv.org/abs/2401.02122) | 本研究通过比较不同的PEFT方法和逐层放置方式，以及采用集成学习策略，揭示了用于语音处理的PEFT的最佳方法和放置策略。结果表明，集成学习方法通过多数投票可以实现优于其他方法的性能。这项研究还发现不同的PEFT方法以不同的方式进行学习，从而解释了为什么通过集成学习可以更有效地利用它们的学习能力。 |
| [^84] | [Towards Optimal Statistical Watermarking.](http://arxiv.org/abs/2312.07930) | 追求最优统计水印技术。通过将统计水印技术视为假设检验问题并引入伪随机生成器，我们实现了输出令牌和拒绝区域的耦合，实现了第一类错误和第二类错误之间的非平凡权衡，同时提出了最统一最有力的水印和最小化第二类错误的解决方案。我们还提供了独立同分布令牌数量的上下界，突显了改进的潜力。此外，我们还探讨了鲁棒性水印问题。 |
| [^85] | [The Perils & Promises of Fact-checking with Large Language Models.](http://arxiv.org/abs/2310.13549) | 本文评估了大语言模型在事实检查中的应用，发现配备上下文信息后，大语言模型表现出更强的能力。然而，准确性存在一定差异，因此在使用中需要谨慎。进一步研究仍然需要进行。 |
| [^86] | [Defending Our Privacy With Backdoors.](http://arxiv.org/abs/2310.08320) | 本研究提出了一种基于后门攻击的防御方法，通过对模型进行策略性插入后门，对齐敏感短语与中性术语的嵌入，以删除训练数据中的私人信息。实证结果显示该方法的有效性。 |
| [^87] | [OpenAI Cribbed Our Tax Example, But Can GPT-4 Really Do Tax?.](http://arxiv.org/abs/2309.09992) | GPT-4在处理税务方面存在问题，无法可靠地计算税务。 |
| [^88] | [AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised Features for Audio-Visual Speech Enhancement.](http://arxiv.org/abs/2309.08030) | 本论文提出了一种名为AV2Wav的音频-视觉语音增强方法，利用连续自监督特征和扩散模型生成干净的语音，克服了现实训练数据的挑战。与基于掩蔽的基线方法相比，该方法在声码任务上表现更好，并通过多任务训练进一步优化性能。 |
| [^89] | [Towards Populating Generalizable Engineering Design Knowledge.](http://arxiv.org/abs/2307.06985) | 这项研究提出了一种从专利文件中提取工程设计知识的方法，通过构建知识图来填充通用设计知识，并与现有方法进行了比较。 |
| [^90] | [MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training.](http://arxiv.org/abs/2306.00107) | 提出了一个带有大规模自监督训练的音乐理解模型MERT，利用了教师模型并采用了一种优于传统的语音和音频方法的组合方式。 |
| [^91] | [Enhancing Generation through Summarization Duality and Explicit Outline Control.](http://arxiv.org/abs/2305.14459) | 本文提出了一个两阶段的摘要增强的大纲监督生成框架，能够更好地生成明确和合理的大纲，并引入了一个新颖的显式大纲控制方法以更有效地利用生成的大纲。 |
| [^92] | [Interpretability at Scale: Identifying Causal Mechanisms in Alpaca.](http://arxiv.org/abs/2305.08809) | 通过使用分布式对齐搜索（DAS）方法，我们在大型语言模型中实现了规模上的解释性，这使得我们能够高效地搜索到解释性因果结构，并应用于Alpaca模型中。 |
| [^93] | [Optimal inference of a generalised Potts model by single-layer transformers with factored attention.](http://arxiv.org/abs/2304.07235) | 我们将分析和数值推导结合，在基于广义 Potts 模型的数据上，对经过改进适应这种模型的self-attention机制进行训练，发现经过修改的self-attention机制可以在极限采样下准确学习Potts模型。这个“分解”注意力机制通过从数据中学习相关属性，可以提高Transformer的性能和可解释性。 |
| [^94] | [Concept Algebra for Score-Based Conditional Models.](http://arxiv.org/abs/2302.03693) | 本文研究了基于分数的条件模型中学习表示的结构，并开发了一种数学形式化表达概念被编码为表示空间子空间的思想。利用这个方法，我们提出了一种简单的方法来识别给定概念对应的表示部分，并通过代数操作操纵模型所表达的概念。 |

# 详细

[^1]: 大型语言模型的持续学习: 一项综述

    Continual Learning for Large Language Models: A Survey

    [https://rss.arxiv.org/abs/2402.01364](https://rss.arxiv.org/abs/2402.01364)

    这篇综述文章调查了大型语言模型(LLMs)的持续学习，使用了一种新颖的多阶段分类方案对持续学习技术进行了分类，指出了该领域面临的挑战和未来工作方向。

    

    由于其庞大的规模导致训练成本高昂，大型语言模型(LLMs)不易频繁重新训练。然而，更新是必要的，以赋予LLMs新的技能，并使其与快速发展的人类知识保持同步。本文对LLMs的持续学习最新研究进行了综述。鉴于LLMs的独特性，我们以一种新颖的多阶段分类方案对持续学习技术进行了分类，涉及持续预训练、指令调整和对齐等方面。我们将LLMs的持续学习与在规模较小的模型中使用的简单适应方法以及其他增强策略(如检索增强生成和模型编辑)进行了对比。此外，根据对基准和评估的讨论，我们确定了这一重要任务面临的几个挑战和未来工作方向。

    Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.
    
[^2]: 跳过$\textbackslash n$: 一种简单的方法减少大规模视觉-语言模型中的幻觉

    Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models

    [https://rss.arxiv.org/abs/2402.01345](https://rss.arxiv.org/abs/2402.01345)

    本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。

    

    最近大规模视觉-语言模型（LVLMs）的进展展示了其在视觉信息理解与人类语言方面的令人印象深刻的能力。尽管取得了这些进展，LVLMs仍然面临多模态幻觉的挑战，例如生成与视觉信息中不存在的对象相关的文本描述。然而，多模态幻觉的根本原因仍然未被充分探索。在本文中，我们提出了一个新的视角，认为LVLMs中固有的偏见可能是幻觉的关键因素。具体而言，我们系统地确定了与段落分割符（'$\textbackslash n\textbackslash n$'）相关的语义漂移偏差，即在训练数据中，在“$\textbackslash n\textbackslash n$”之前和之后的内容经常表现出显著的语义改变。这种模式使得模型推断在“$\textbackslash n\textbackslash n$”之后的内容应明显不同于前面的内容。

    Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
    
[^3]: 充分利用分词器进行预训练和领域适应

    Getting the most out of your tokenizer for pre-training and domain adaptation

    [https://rss.arxiv.org/abs/2402.01035](https://rss.arxiv.org/abs/2402.01035)

    本文通过训练专用分词器，对分词器设计进行了消毒和分析，发现分词器的大小、正则表达式和训练数据对模型性能有重要影响，并提供了相应的超参数选择建议和切换分词器的方法。

    

    分词是现代LLM中鲜为人知且常被忽视的组成部分。大多数已发表的作品在所有实验中都使用同一个分词器，通常是从另一个模型借用而来的，并没有进行消融或分析来优化分词。此外，在微调基础模型时，分词器通常保持不变。在本文中，我们展示了分词器的大小、预标记正则表达式和训练数据对模型的生成速度、有效上下文大小、内存使用和下游性能均有重要影响。我们训练了专用的字节对编码分词器，并对分词器设计对代码生成任务（如HumanEval和MBPP）中LLM性能影响进行了广泛的消融，提供了分词器超参数选择和在预训练LLM中切换分词器的建议。我们在从头开始训练和从预训练模型中进行了实验，验证了它们对各种任务和模型的适用性。

    Tokenization is an understudied and often neglected component of modern LLMs. Most published works use a single tokenizer for all experiments, often borrowed from another model, without performing ablations or analysis to optimize tokenization. Moreover, the tokenizer is generally kept unchanged when fine-tuning a base model. In this paper, we show that the size, pre-tokenization regular expression, and training data of a tokenizer can significantly impact the model's generation speed, effective context size, memory usage, and downstream performance. We train specialized Byte-Pair Encoding code tokenizers, and conduct extensive ablations on the impact of tokenizer design on the performance of LLMs for code generation tasks such as HumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters selection and switching the tokenizer in a pre-trained LLM. We perform our experiments on models trained from scratch and from pre-trained models, verifying their applicability to 
    
[^4]: Edu-ConvoKit:一种用于教育会话数据的开源库

    Edu-ConvoKit: An Open-Source Library for Education Conversation Data

    [https://arxiv.org/abs/2402.05111](https://arxiv.org/abs/2402.05111)

    Edu-ConvoKit是一种用于处理教育会话数据的开源库，它解决了教育会话数据分析资源稀缺的问题，提供了全面的文档和附加资源。

    

    我们介绍了Edu-ConvoKit，这是一个设计用于处理教育会话数据的预处理、注释和分析的开源库。教育会话数据的分析资源稀缺，使得研究变得困难，因此难以获取。我们通过Edu-ConvoKit来解决这些挑战。Edu-ConvoKit是开源的（https://github.com/stanfordnlp/edu-convokit），可以通过pip进行安装（https://pypi.org/project/edu-convokit/），并具有全面的文档（https://edu-convokit.readthedocs.io/en/latest/）。我们的演示视频可在以下链接中观看：https://youtu.be/zdcI839vAko?si=h9qlnl76ucSuXb8-。我们还在GitHub仓库中提供了附加资源，例如Edu-ConvoKit在三个不同教育数据集上的Colab应用程序以及Edu-ConvoKit相关论文的存储库。

    We introduce Edu-ConvoKit, an open-source library designed to handle pre-processing, annotation and analysis of conversation data in education. Resources for analyzing education conversation data are scarce, making the research challenging to perform and therefore hard to access. We address these challenges with Edu-ConvoKit. Edu-ConvoKit is open-source (https://github.com/stanfordnlp/edu-convokit ), pip-installable (https://pypi.org/project/edu-convokit/ ), with comprehensive documentation (https://edu-convokit.readthedocs.io/en/latest/ ). Our demo video is available at: https://youtu.be/zdcI839vAko?si=h9qlnl76ucSuXb8- . We include additional resources, such as Colab applications of Edu-ConvoKit to three diverse education datasets and a repository of Edu-ConvoKit related papers, that can be found in our GitHub repository.
    
[^5]: 使用GRIT模型进行巴西葡萄牙语图像字幕生成

    Image captioning for Brazilian Portuguese using GRIT model

    [https://arxiv.org/abs/2402.05106](https://arxiv.org/abs/2402.05106)

    本文早期开发了一个用于巴西葡萄牙语的图像字幕生成模型，使用了GRIT模型，并对其进行了调整以适应巴西葡萄牙语数据集，实现了更高效的图像字幕生成方法。

    

    本文介绍了一个用于巴西葡萄牙语的图像字幕生成模型的早期开发工作。我们使用GRIT（基于网格和区域的图像字幕生成Transformer）模型来完成此工作。GRIT是一个仅使用Transformer的神经网络架构，通过有效利用两个视觉特征来生成更好的字幕。GRIT方法被提出作为一种更高效的图像字幕生成方式。在本研究中，我们对GRIT模型进行了调整，以在巴西葡萄牙语数据集上进行训练，以实现巴西葡萄牙语图像字幕生成方法。

    This work presents the early development of a model of image captioning for the Brazilian Portuguese language. We used the GRIT (Grid - and Region-based Image captioning Transformer) model to accomplish this work. GRIT is a Transformer-only neural architecture that effectively utilizes two visual features to generate better captions. The GRIT method emerged as a proposal to be a more efficient way to generate image captioning. In this work, we adapt the GRIT model to be trained in a Brazilian Portuguese dataset to have an image captioning method for the Brazilian Portuguese Language.
    
[^6]: 通往多元对齐的路线图

    A Roadmap to Pluralistic Alignment

    [https://arxiv.org/abs/2402.05070](https://arxiv.org/abs/2402.05070)

    这篇论文提出了一条通向多元对齐的路线图，以解决设计AI系统能够服务于人们具有不同价值观和观点的需求。论文介绍了对齐定义和实现多元主义的三种方式，并提出了三种多元基准类别来评估和测试多元对齐的效果。

    

    随着人工智能系统的权力和普及程度的增加，设计能够为不同价值观和观点的人服务的人工智能系统变得愈发重要。然而，将模型对齐以服务多元人类价值观仍然是一个待解决的研究问题。在本文中，我们提出了一条通向多元对齐的路线图，具体使用语言模型作为测试平台。我们确定和形式化了三种可能的方式来定义和实现人工智能系统中的多元主义：1）Overton多元模型，展示合理反应的光谱；2）可操控的多元模型，可以调整以反映特定的观点；3）分布多元模型，在分布中很好地校准给定人群的模型。我们还提出和形式化了三种可能的多元基准类别：1）多目标基准；2）权衡可操控基准，鼓励模型对任意权衡进行调整；3）陪审团多元基准，明确地模拟了不同陪审团的意见。

    With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives. However, aligning models to serve pluralistic human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses; 2) Steerably pluralistic models that can steer to reflect certain perspectives; and 3) Distributionally pluralistic models that are well-calibrated to a given population in distribution. We also propose and formalize three possible classes of pluralistic benchmarks: 1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which explicitly m
    
[^7]: SALAD-Bench: 一个针对大语言模型的层次化和全面性安全基准

    SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models

    [https://arxiv.org/abs/2402.05044](https://arxiv.org/abs/2402.05044)

    SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。

    

    在快速发展的大语言模型（LLM）领域中，确保强大的安全措施至关重要。为了满足这一关键需求，我们提出了一种特别设计用于评估LLMs、攻击和防御方法的安全基准，称为SALAD-Bench。SALAD-Bench通过其大规模、丰富多样的特性，以及跨三个层次的细致分类和多功能性，超越了传统基准。SALAD-Bench通过对标准查询和复杂查询（包括攻击、防御修改和多项选择）的精心设计，有效管理其固有的复杂性。为了确保无缝可靠的评估，我们引入了一种创新的评估器：基于LLM的MD-Judge，专注于攻击增强查询的问答对评估。以上组件将SALAD-Bench从标准的LLM安全评估扩展到了LLM攻击和防御方法评估，确保了联合目标的实用性。

    In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
    
[^8]: BERT如何说莎士比亚英语？评估上下文语言模型中的历史偏见

    How BERT Speaks Shakespearean English? Evaluating Historical Bias in Contextual Language Models

    [https://arxiv.org/abs/2402.05034](https://arxiv.org/abs/2402.05034)

    本文评估了基于BERT的上下文语言模型对早期现代英语和现代英语的准确性，并分析了其中的历史偏见。

    

    本文探讨了基于BERT的上下文语言模型的历史偏见分析，并通过测量其对早期现代英语（EME）和现代英语（ME）的适应性来评估其准确性。通过填空测试，我们对60个掩盖句子（20个EME特定、20个ME特定和20个通用）以及三个不同的模型（BERT Base、MacBERTh、English HLM）进行了初步实验。然后，我们根据两种语言变体之间的5点双极刻度对模型预测进行评分，并计算加权得分来测量每个模型对EME和ME英语变体的适应性。

    In this paper, we explore the idea of analysing the historical bias of contextual language models based on BERT by measuring their adequacy with respect to Early Modern (EME) and Modern (ME) English. In our preliminary experiments, we perform fill-in-the-blank tests with 60 masked sentences (20 EME-specific, 20 ME-specific and 20 generic) and three different models (i.e., BERT Base, MacBERTh, English HLM). We then rate the model predictions according to a 5-point bipolar scale between the two language varieties and derive a weighted score to measure the adequacy of each model to EME and ME varieties of English.
    
[^9]: 大型语言模型的教学对齐

    Pedagogical Alignment of Large Language Models

    [https://arxiv.org/abs/2402.05000](https://arxiv.org/abs/2402.05000)

    本文介绍了教学对齐的大型语言模型（LLM）的新概念，并探讨了通过建设性反馈和提示指导学生解决复杂问题的方法。相比于传统方法，这种对齐方法以及采用人类反馈的强化学习方法能够更好地对齐LLM，提供更优质的教育支持。

    

    在本文中，我们引入了教学对齐的大型语言模型（LLM）的新概念，这在教育背景下应用LLM具有转变性的意义。与直接回答用户问题不同，教学对齐的LLM作为辅助工具，将复杂问题分解为可管理的子问题，并通过建设性的反馈和提示指导学生找到最终答案。其目标是为学习者提供解决问题的策略，以加深他们对主题的理解和内化。先前的研究主要采用了监督微调方法，没有将目标定义为对齐问题，并未使用通过人类反馈的强化学习方法（RLHF）。本研究通过对齐的视角重新解释了这一论述，并展示了RLHF方法作为对齐LLM的优越替代方法。

    In this paper, we introduce the novel concept of pedagogically aligned Large Language Models (LLMs) that signifies a transformative shift in the application of LLMs within educational contexts. Rather than providing direct responses to user queries, pedagogically-aligned LLMs function as scaffolding tools, breaking complex problems into manageable subproblems and guiding students towards the final answer through constructive feedback and hints. The objective is to equip learners with problem-solving strategies that deepen their understanding and internalization of the subject matter. Previous research in this field has primarily applied the supervised finetuning approach without framing the objective as an alignment problem, hence not employing reinforcement learning through human feedback (RLHF) methods. This study reinterprets the narrative by viewing the task through the lens of alignment and demonstrates how RLHF methods emerge naturally as a superior alternative for aligning LLM b
    
[^10]: 通过知识图谱集成协作的增强型基于提示的LLM推理方案

    An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration

    [https://arxiv.org/abs/2402.04978](https://arxiv.org/abs/2402.04978)

    本文提出了一种通过知识图谱和LLMs合作训练的推理方案，该方案解决了大型语言模型在实际应用中的虚构问题、知识更新不足以及推理过程的透明度有限等挑战，实现了更可靠的知识推理和推理结果追踪。

    

    虽然大型语言模型（LLMs）在许多自然语言处理（NLP）任务中展现出极好的性能，但在实际应用中遇到了一些挑战，包括虚构问题、知识更新不足以及推理过程的透明度有限。为了克服这些限制，本研究创新性地提出了一种协作训练自由的推理方案，其中知识图谱（KG）和LLMs之间密切合作。该方案首先使用LLMs迭代地探索KG，选择性地检索与任务相关的知识子图以支持推理。然后引导LLMs进一步组合内在的隐式知识，在子图上进行推理，并明确阐述推理过程。通过这种协作方法，我们的方案实现了更可靠的基于知识的推理，并便于追踪推理结果。实验结果表明，我们的方案在各项指标上取得了显著进展。

    While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process. To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning. The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process. Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results. Experimental results show that our scheme significantly progressed across
    
[^11]: 文本还是图像？对于跨领域泛化能力的仇恨迷因检测模型来说，什么更重要？

    Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models?

    [https://arxiv.org/abs/2402.04967](https://arxiv.org/abs/2402.04967)

    本文研究了多模式仇恨迷因检测中的跨领域泛化挑战，并发现仇恨迷因的文本部分对于泛化至不同领域的分类器至关重要，而图像部分对特定训练数据集敏感。实验证明仇恨文本分类器在零样本情况下表现类似于仇恨迷因分类器。黑盒解释表明文本模态对模型性能的贡献较大，但引入图像标题后贡献降低。新的混淆数据集评估显示文本混淆因素表现较好。

    

    本文深入探讨了多模式仇恨迷因检测中跨领域泛化的挑战，并提出了引人注目的研究结果。我们提供了足够的证据支持假设：只有仇恨迷因中的文本成分使得现有的多模式分类器能够在不同领域中进行泛化，而图像成分则对特定训练数据集非常敏感。证据包括演示，证明在零样本情况下，仇恨文本分类器的表现与仇恨迷因分类器相似。同时，将由迷因图像生成的标题引入到仇恨迷因分类器中，导致性能下降平均F1指标约为0.02。通过黑盒解释，我们确定了文本模态的重要贡献（平均83%），但引入迷因图像标题后贡献降低至52%。此外，我们还在新创建的混淆数据集上进行了评估，显示文本混淆因素的表现较好。

    This paper delves into the formidable challenge of cross-domain generalization in multimodal hate meme detection, presenting compelling findings. We provide enough pieces of evidence supporting the hypothesis that only the textual component of hateful memes enables the existing multimodal classifier to generalize across different domains, while the image component proves highly sensitive to a specific training dataset. The evidence includes demonstrations showing that hate-text classifiers perform similarly to hate-meme classifiers in a zero-shot setting. Simultaneously, the introduction of captions generated from images of memes to the hate-meme classifier worsens performance by an average F1 of 0.02. Through blackbox explanations, we identify a substantial contribution of the text modality (average of 83%), which diminishes with the introduction of meme's image captions (52%). Additionally, our evaluation on a newly created confounder dataset reveals higher performance on text confou
    
[^12]: 从分组损失的角度重构大型语言模型的信心

    Reconfidencing LLMs from the Grouping Loss Perspective

    [https://arxiv.org/abs/2402.04957](https://arxiv.org/abs/2402.04957)

    本论文研究了大型语言模型的信心问题，发现现有的校准方法不足以解决由于分组损失导致的预测分数与实际概率偏离的问题。我们提出了一种解决方案，可以重新确定LLMs，改善它们的自信度。

    

    大型语言模型（LLMs），包括ChatGPT和LLaMA，在自信的口吻中容易生成虚假答案。尽管引导和校准信心分数的努力已被证明是有用的，但最近的研究发现，控制不确定性必须超越校准: 由于分组损失的影响，预测分数可能明显偏离实际的后验概率。在这项工作中，我们构建了一个新的评估数据集，从知识库中获取，以评估对Mistral和LLaMA的答案给出的信心分数。实验表明，它们倾向于过于自信。此外，我们还展示了它们在某些答案上比其他答案更过于自信，例如取决于查询中人的国籍。在不确定性量化理论中，这就是分组损失。为了解决这个问题，我们提出了一种重新确定LLMs的解决方案，不仅取消校准，还取消分组损失。经过重新确定的LLMs经过处理后，表示改进的自信度。

    Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone. While efforts to elicit and calibrate confidence scores have proven useful, recent findings show that controlling uncertainty must go beyond calibration: predicted scores may deviate significantly from the actual posterior probabilities due to the impact of grouping loss. In this work, we construct a new evaluation dataset derived from a knowledge base to assess confidence scores given to answers of Mistral and LLaMA. Experiments show that they tend to be overconfident. Further, we show that they are more overconfident on some answers than others, \emph{eg} depending on the nationality of the person in the query. In uncertainty-quantification theory, this is grouping loss. To address this, we propose a solution to reconfidence LLMs, canceling not only calibration but also grouping loss. The LLMs, after the reconfidencing process, indicate improved confidenc
    
[^13]: 促使隐式话语关系注释的研究

    Prompting Implicit Discourse Relation Annotation

    [https://arxiv.org/abs/2402.04918](https://arxiv.org/abs/2402.04918)

    本研究旨在提升ChatGPT对隐式话语关系的识别，尝试了多种提示技术，但实验结果显示即使进行复杂的提示工程，隐式话语关系分类在零样本或少样本设置下仍难以解决。

    

    预训练的大型语言模型，如ChatGPT，在各种推理任务中表现出色，无需监督训练，并且被发现超过了众包工作者。然而，ChatGPT在通过标准的多项选择问题引导的隐式话语关系分类任务中的表现仍然远远不如最先进的监督方法。本研究对几种经过验证的提示技术进行了调查，以改善ChatGPT对话语关系的识别。特别是，我们尝试将涉及大量抽象标签的分类任务分解为较小的子任务。然而，实验结果表明，即使使用复杂的提示工程，推理准确率几乎没有改变，这表明隐式话语关系分类在零样本或少样本设置下尚不可解。

    Pre-trained large language models, such as ChatGPT, archive outstanding performance in various reasoning tasks without supervised training and were found to have outperformed crowdsourcing workers. Nonetheless, ChatGPT's performance in the task of implicit discourse relation classification, prompted by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches. This work investigates several proven prompting techniques to improve ChatGPT's recognition of discourse relations. In particular, we experimented with breaking down the classification task that involves numerous abstract labels into smaller subtasks. Nonetheless, experiment results show that the inference accuracy hardly changes even with sophisticated prompt engineering, suggesting that implicit discourse relation classification is not yet resolvable under zero-shot or few-shot settings.
    
[^14]: 细粒度语言控制下的个性化文本生成

    Personalized Text Generation with Fine-Grained Linguistic Control

    [https://arxiv.org/abs/2402.04914](https://arxiv.org/abs/2402.04914)

    本文研究了细粒度语言控制下的个性化文本生成，引入了一个新的基准来评估生成模型在多个细粒度语言属性上的表现，探索了影响模型性能的因素，并公开了代码、数据和预训练模型。

    

    随着大型语言模型的文本生成能力越来越突出，最近的研究集中在控制生成文本的特定方面，使其更加个性化。然而，大多数关于可控文本生成的研究都集中在控制内容或建模特定的高级/粗粒度属性，如正式程度、领域或情感，以反映作者的写作风格。在本文中，我们专注于控制跨多个语言维度的细粒度属性，如词汇和句法属性。我们引入了一个新颖的基准来训练生成模型，并评估它们根据多个细粒度语言属性生成个性化文本的能力。我们系统地研究了各种大型语言模型在我们的基准上的性能，并从影响它们性能的因素中得出了一些见解。我们公开了我们的代码、数据和预训练模型。

    As the text generation capabilities of large language models become increasingly prominent, recent studies have focused on controlling particular aspects of the generated text to make it more personalized. However, most research on controllable text generation focuses on controlling the content or modeling specific high-level/coarse-grained attributes that reflect authors' writing styles, such as formality, domain, or sentiment. In this paper, we focus on controlling fine-grained attributes spanning multiple linguistic dimensions, such as lexical and syntactic attributes. We introduce a novel benchmark to train generative models and evaluate their ability to generate personalized text based on multiple fine-grained linguistic attributes. We systematically investigate the performance of various large language models on our benchmark and draw insights from the factors that impact their performance. We make our code, data, and pretrained models publicly available.
    
[^15]: L4Q: 通过基于LoRA的量化训练在大型语言模型上提供参数高效的量化训练

    L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ

    [https://arxiv.org/abs/2402.04902](https://arxiv.org/abs/2402.04902)

    L4Q是一种参数高效的量化感知训练算法，通过基于LoRA的学习的量化步长，解决了大型语言模型中量化训练的挑战。

    

    后训练量化(PTQ)和量化感知训练(QAT)方法正在流行起来，以缓解大型语言模型(LLMs)所带来的高内存和计算成本。在资源受限的情况下，尽管后者具有更高的准确性潜力，但由于其减少的训练开销，通常首选后训练量化。同时，介绍了参数高效微调方法，如低秩适应（LoRA），并最近的工作已经探索了量化感知参数高效微调技术。然而，这些方法可能缺乏通用性，因为它们依赖于预量化模型的配置。由非线性量化或混合精度权重引起的效果可能会受到影响，并且重新训练特定量化参数可能会影响最优性能。为了应对这些挑战，我们提出了L4Q，一种参数高效的量化感知训练算法。L4Q利用了基于LoRA的学习的量化步长。

    Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs). In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter's potential for higher accuracy. Meanwhile, parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored quantization-aware PEFT techniques. However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration. Their effectiveness may be compromised by non-linearly quantized or mixed-precision weights, and the retraining of specific quantization parameters might impede optimal performance. To address these challenges, we propose L4Q, an algorithm for parameter-efficient quantization-aware training. L4Q leverages LoRA-wise learned quantization step size 
    
[^16]: 发现对话式搜索中的生成式原生广告

    Detecting Generated Native Ads in Conversational Search

    [https://arxiv.org/abs/2402.04889](https://arxiv.org/abs/2402.04889)

    本论文研究了LLM是否可以用作对抗生成式原生广告的对策，并通过构建广告倾向查询数据集和带自动整合广告的生成答案数据集进行实验证明。

    

    对话式搜索引擎如YouChat和Microsoft Copilot使用大型语言模型（LLM）为查询生成答案。将此技术用于生成并整合广告，而不是将广告与有机搜索结果分开放置，只是一小步。这种类型的广告类似于原生广告和产品放置，两者都是非常有效的微妙和操纵性广告形式。在考虑到与LLM相关的高计算成本时，信息搜索者将很可能在不久的将来面临这种LLM技术的使用，因此供应商需要开发可持续的商业模式。本文研究了LLM是否也可以用作对抗生成式原生广告的对策，即阻止它们。为此，我们编制了一个大型的广告倾向查询数据集和带自动整合广告的生成答案数据集进行实验。

    Conversational search engines such as YouChat and Microsoft Copilot use large language models (LLMs) to generate answers to queries. It is only a small step to also use this technology to generate and integrate advertising within these answers - instead of placing ads separately from the organic search results. This type of advertising is reminiscent of native advertising and product placement, both of which are very effective forms of subtle and manipulative advertising. It is likely that information seekers will be confronted with such use of LLM technology in the near future, especially when considering the high computational costs associated with LLMs, for which providers need to develop sustainable business models. This paper investigates whether LLMs can also be used as a countermeasure against generated native ads, i.e., to block them. For this purpose we compile a large dataset of ad-prone queries and of generated answers with automatically integrated ads to experiment with fin
    
[^17]: 关于可证明的长度和组合泛化

    On Provable Length and Compositional Generalization

    [https://arxiv.org/abs/2402.04875](https://arxiv.org/abs/2402.04875)

    本研究针对包括深度集合、变压器、状态空间模型和简单递归神经网络等多种架构，探索了可证明的长度和组合泛化，认为对于长度和组合泛化，不同架构需要不同程度的表示识别。

    

    长度泛化——对训练时未见到的更长序列的泛化能力，以及组合泛化——对训练时未见到的令牌组合的泛化能力，在序列到序列模型中是重要的非分布化泛化形式。在这项工作中，我们在包括深度集合、变压器、状态空间模型和简单递归神经网络在内的一系列架构中，朝着可证明的长度和组合泛化迈出了第一步。根据架构的不同，我们证明了不同程度的表示识别的必要性，例如与真实表示具有线性或排列关系。

    Length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models. In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets. Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.
    
[^18]: CodeIt：具有优先级回顾重放的自我改进语言模型

    CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay

    [https://arxiv.org/abs/2402.04858](https://arxiv.org/abs/2402.04858)

    CodeIt是一种具备优先级回顾重放的自我改进语言模型方法，通过将目标重标记为采样程序的实际输出，有效解决了程序合成中奖励稀疏性的问题，并在抽象和推理语料库（ARC）上实现了成功的跨任务泛化。

    

    大型语言模型越来越能够解决通常被认为需要人类水平推理能力的任务。然而，这些模型在通用智能基准测试例如抽象和推理语料库（ARC）上表现仍然非常差。在本文中，我们将ARC视为一个以编程示例为基础的问题，并引入了一种名为Code Iteration（CodeIt）的新颖且可扩展的语言模型自我改进方法。我们的方法在1）程序抽样和回顾重标记以及2）基于优先级的经验回放之间进行迭代。通过将一个episode的目标（即给定输入的目标程序输出）重标记为采样程序产生的实际输出，我们的方法有效地处理了程序合成中奖励极度稀疏性的问题。应用CodeIt于ARC数据集，我们证明了优先级回顾重放、预训练和数据增强可以实现成功的跨任务泛化。CodeIt是第一个神经元-合成机制一体的自我改进语言模型方法。

    Large language models are increasingly solving tasks that are commonly believed to require human-level reasoning ability. However, these models still perform very poorly on benchmarks of general intelligence such as the Abstraction and Reasoning Corpus (ARC). In this paper, we approach ARC as a programming-by-examples problem, and introduce a novel and scalable method for language model self-improvement called Code Iteration (CodeIt). Our method iterates between 1) program sampling and hindsight relabeling, and 2) learning from prioritized experience replay. By relabeling the goal of an episode (i.e., the target program output given input) to the realized output produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis. Applying CodeIt to the ARC dataset, we demonstrate that prioritized hindsight replay, along with pre-training and data-augmentation, leads to successful inter-task generalization. CodeIt is the first neuro-sy
    
[^19]: 分层树状知识图谱用于学术调研

    Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey

    [https://arxiv.org/abs/2402.04854](https://arxiv.org/abs/2402.04854)

    该论文提出了一种分层树状知识图谱和推荐系统，帮助初学者研究者进行研究调研，填补了现有导航知识图谱的不足，并解决了学术论文推荐系统中高文本相似性带来的困惑。

    

    对于缺乏研究培训的初学者研究者来说，研究调查一直是一个挑战。这些研究者在短时间内很难理解他们研究主题内的方向，以及发现新的研究发现。为初学者研究者提供直观的帮助的一种方式是提供相关的知识图谱(KG)并推荐相关的学术论文。然而，现有的导航知识图谱主要依赖于研究领域的关键字，常常无法清楚地呈现多个相关论文之间的逻辑层次关系。此外，大多数学术论文推荐系统仅仅依赖于高文本相似性，这可能会让研究人员困惑为什么推荐了特定的文章。他们可能缺乏了解关于他们希望获得的"问题解决"和"问题发现"之间的见解连接的重要信息。为解决这些问题，本研究旨在支持初学者研究者进行研究调研。

    Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between "Issue resolved" and "Issue finding" that they hope to obtain. To address these issues, this study aims to support research insight surveys for begi
    
[^20]: PaDeLLM-NER：大型语言模型中的并行解码用于命名实体识别

    PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition

    [https://arxiv.org/abs/2402.04838](https://arxiv.org/abs/2402.04838)

    本研究提出了PaDeLLM-NER，一种能够在大型语言模型中实现并行解码，从而显著减少命名实体识别的生成延迟，同时保持预测质量和性能。

    

    本研究旨在使用大型语言模型（LLMs）减少命名实体识别（NER）的生成延迟。LLMs的高延迟的主要原因是顺序解码过程，该过程自回归地生成NER的所有标签和提及，显著增加了序列长度。为此，我们引入了PaDeLLM-NER（Parallel Decoding in LLM for NE），这是一种无需额外模块或架构修改即可无缝集成到现有生成模型框架中的方法。PaDeLLM-NER允许同时解码所有提及，从而减少生成延迟。实验结果显示，PaDeLLM-NER的推理速度显著提高，对英语和中文来说比自回归方法快1.76到10.22倍。与各种数据集上的最先进性能相媲美，同时维持了预测质量。

    In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.
    
[^21]: 长度更长对齐更好：一种简单但难以望其项背的指导微调基准线

    Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning

    [https://arxiv.org/abs/2402.04833](https://arxiv.org/abs/2402.04833)

    通过选择标准数据集中响应最长的1,000条指示作为基准线，可以在各种语言模型和数据集上实现对齐的高性能，同时对长指示进行轻量级改进进一步提升微调语言模型的能力。

    

    有共识认为，对于语言模型的指导微调需要高质量的数据，但具体是什么呢？LIMA（NeurIPS 2023）和AlpaGasus（ICLR 2024）是选择这类高质量示例的最先进方法，它们要么通过手动整理要么使用GPT-3.5-Turbo作为质量评分器。我们展示了从标准数据集中选择响应最长的1,000条指示的极简基准线在GPT-4和PaLM-2的评判下始终能够胜过这些复杂方法，同时在测试基于事实知识的OpenLLM基准上保持竞争力。我们在几种最先进的语言模型（Llama-2-7B，Llama-2-13B和Mistral-7B）和数据集（Alpaca-52k和Evol-Instruct-70k）上进行了验证。此外，对这样的长指示进行轻量级改进可以进一步提高微调语言模型的能力，并使我们在只训练了1,000个例子且没有外部数据的情况下，在AlpacaEval 2.0上获得了基于Llama-2-7B的模型的第二高排名。

    There is a consensus that instruction fine-tuning of LLMs requires high-quality data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR 2024) are state-of-the-art methods for selecting such high-quality examples, either via manual curation or using GPT-3.5-Turbo as a quality scorer. We show that the extremely simple baseline of selecting the 1,000 instructions with longest responses from standard datasets can consistently outperform these sophisticated methods according to GPT-4 and PaLM-2 as judges, while remaining competitive on the OpenLLM benchmarks that test factual knowledge. We demonstrate this for several state-of-the-art LLMs (Llama-2-7B, Llama-2-13B, and Mistral-7B) and datasets (Alpaca-52k and Evol-Instruct-70k). In addition, a lightweight refinement of such long instructions can further improve the abilities of the fine-tuned LLMs, and allows us to obtain the 2nd highest-ranked Llama-2-7B-based model on AlpacaEval 2.0 while training on only 1,000 examples and no ex
    
[^22]: 在协同参考游戏中学习不同追随者行为的通信策略

    Learning Communication Policies for Different Follower Behaviors in a Collaborative Reference Game

    [https://arxiv.org/abs/2402.04824](https://arxiv.org/abs/2402.04824)

    本文研究了在协同参考游戏中学习适应不同追随者行为的通信策略。通过强化学习算法（PPO），证明了神经代理能够与具有不同自信度和自主性的追随者良好地协作，并且在学习信号中考虑了沟通努力的因素。

    

    Albrecht和Stone（2018）指出，建模不断变化的行为仍然是一个开放的问题，“由于其他智能体可能做出的行为本质上是无限制的”。在这项工作中，我们评估了神经人工智能代理对协同参考游戏中假设伙伴行为的适应能力。在这个游戏中，当有知识的导游能够通过口头引导追随者从多个干扰者中选择特定的拼图时，才能取得成功。我们把这个语言基础和协调任务作为一个强化学习问题来解决，并测量了一个常见的强化训练算法（PPO）在多种启发式追随者行为上产生的神经代理（导游）的表现程度，这些行为在自信度和自主性方面有所不同。我们尝试了一种学习信号，除了目标条件外还尊重假设的沟通努力。我们的结果表明，这种新颖的因素会导致沟通策略的改变。

    Albrecht and Stone (2018) state that modeling of changing behaviors remains an open problem "due to the essentially unconstrained nature of what other agents may do". In this work we evaluate the adaptability of neural artificial agents towards assumed partner behaviors in a collaborative reference game. In this game success is achieved when a knowledgeable Guide can verbally lead a Follower to the selection of a specific puzzle piece among several distractors. We frame this language grounding and coordination task as a reinforcement learning problem and measure to which extent a common reinforcement training algorithm (PPO) is able to produce neural agents (the Guides) that perform well with various heuristic Follower behaviors that vary along the dimensions of confidence and autonomy. We experiment with a learning signal that in addition to the goal condition also respects an assumed communicative effort. Our results indicate that this novel ingredient leads to communicative strategi
    
[^23]: 面向开放式人力资源调查回应的基于方面的情感分析

    Aspect-Based Sentiment Analysis for Open-Ended HR Survey Responses

    [https://arxiv.org/abs/2402.04812](https://arxiv.org/abs/2402.04812)

    本文提出了一种面向开放式人力资源调查回应的基于方面的情感分析方法，通过回应聚类识别关键方面，并采用荷兰BERT模型进行分析。该方法可用于支持员工生命周期管理。

    

    理解员工喜好、意见和情感对于有效的员工生命周期管理至关重要。开放式调查回应作为有价值的信息来源。本文提出了一种机器学习方法，用于分析员工满意度调查中荷兰开放式回应的基于方面的情感分析(ABSA)。我们的方法旨在克服这些回应中固有的噪声和变异性，实现对情感的全面分析，以支持员工生命周期管理。通过回应聚类，我们确定了六个关键方面(薪水、时间安排、联系、沟通、个人关注、协议)，并由领域专家进行了验证。我们编制了一个由1,458个荷兰调查回应组成的数据集，揭示了方面和情感的标签不平衡。我们提出了基于荷兰BERT模型的几个小批量学习方法，与词袋模型和零样本模型进行了对比。我们的工作在ABSA领域做出了重大贡献。

    Understanding preferences, opinions, and sentiment of the workforce is paramount for effective employee lifecycle management. Open-ended survey responses serve as a valuable source of information. This paper proposes a machine learning approach for aspect-based sentiment analysis (ABSA) of Dutch open-ended responses in employee satisfaction surveys. Our approach aims to overcome the inherent noise and variability in these responses, enabling a comprehensive analysis of sentiments that can support employee lifecycle management. Through response clustering we identify six key aspects (salary, schedule, contact, communication, personal attention, agreements), which we validate by domain experts. We compile a dataset of 1,458 Dutch survey responses, revealing label imbalance in aspects and sentiments. We propose few-shot approaches for ABSA based on Dutch BERT models, and compare them against bag-of-words and zero-shot baselines. Our work significantly contributes to the field of ABSA by d
    
[^24]: 来自在线人工智能反馈的直接语言模型对齐

    Direct Language Model Alignment from Online AI Feedback

    [https://arxiv.org/abs/2402.04792](https://arxiv.org/abs/2402.04792)

    本论文提出了一种名为OAIF的方法，通过在线反馈来改善DAP方法，该方法使用LLM作为标注器，从而在多个任务中展示出优于离线DAP和RLHF的性能

    

    最近，直接对齐偏好（DAP）方法如DPO已成为对于从人类反馈中进行增强学习的高效替代方法，不要求单独的奖励模型。然而，DAP方法中使用的偏好数据集通常在训练之前收集，并且从不更新，因此反馈纯粹是离线的。此外，这些数据集中的回应通常是从一个与被对齐的语言模型不同的语言模型中采样的，由于模型在训练过程中会变化，对齐阶段必然是非策略的。在本研究中，我们认为在线反馈是关键，可以改善DAP方法。我们的方法，在线人工智能反馈（OAIF），使用LLM作为标注器：在每个训练迭代中，我们从当前模型中采样两个回应，并提示LLM标注器选择哪个更受欢迎，从而提供在线反馈。尽管简单，但通过多个任务中的人工评估，我们证明OAIF优于离线DAP和RLHF

    Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF 
    
[^25]: MLLM作为法官：使用视觉语言基准评估多模态MLLM作为法官的能力

    MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark

    [https://arxiv.org/abs/2402.04788](https://arxiv.org/abs/2402.04788)

    本文引入了MLLM作为法官的新基准测试，用于评估MLLM在辅助法官方面的能力。研究结果显示，MLLM在对比评估任务中展示出了非常类人的辨别能力，但在评分评估和批量排序任务中与人类偏好存在显著的差异。此外，即使对于先进模型如GPT-4V，MLLM仍然面临着偏见、幻觉式回答和不一致性等判断方面的挑战。

    

    多模态大型语言模型（MLLMs）近来引起了广泛关注，展现出人工智能方面的巨大潜力。然而，评估MLLM的实用性存在着相当大的挑战，主要是由于缺乏与人类偏好相符的多模态基准测试。本文受到LLM模型中LLM作为法官的启发，引入了一个新的基准测试，被称为MLLM作为法官，用于评估MLLM在协助法官方面的能力，包括三个不同的任务：评分评估、对比评估和批量排序。我们的研究发现，虽然MLLM在对比评估方面展示出了令人瞩目的类人辨别能力，但在评分评估和批量排序任务中与人类偏好存在显著的差异。此外，即使对于像GPT-4V这样的先进模型，MLLM仍然面临着判断方面的挑战，包括多样的偏见、幻觉式的回答和不一致性。这些发现强调了对MLLM的改进和进一步研究的迫切需要。

    Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Inspired by LLM-as-a-Judge in LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges including three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further rese
    
[^26]: 一种基于假设的自我合理模型分析框架

    A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models

    [https://arxiv.org/abs/2402.04787](https://arxiv.org/abs/2402.04787)

    本研究提出了一种基于假设的统计框架，用于分析LLM模型的自我合理能力。通过将LLM生成的解释与贝叶斯网络的决策过程进行比较，可以评判两者的相似性。实验证明，生成的模型与GPT-3.5不相似。这一研究结果对于理解LLL模型的解释能力具有重要意义。

    

    LLM模型的自我合理能力很有吸引力，因为生成的解释可以揭示预测的合理性。然而，解释与预测之间的准确性是有问题的，因此需要进一步探索其中的模式。为此，我们提出了一种基于假设的统计框架。我们使用贝叶斯网络实现关于任务（在我们的例子中为自然语言推理）如何解决的假设，并将其内部状态通过模板转化为自然语言。然后使用自动和人工评估将这些解释与LLM生成的自由文本解释相比较，以评判LLM和贝叶斯网络的决策过程的相似性。我们使用一个例子假设和两种贝叶斯网络实现来演示我们的框架的应用。得到的模型与GPT-3.5没有明显的相似性。我们讨论了这一点的影响。

    The self-rationalising capabilities of LLMs are appealing because the generated explanations can give insights into the plausibility of the predictions. However, how faithful the explanations are to the predictions is questionable, raising the need to explore the patterns behind them further. To this end, we propose a hypothesis-driven statistical framework. We use a Bayesian network to implement a hypothesis about how a task (in our example, natural language inference) is solved, and its internal states are translated into natural language with templates. Those explanations are then compared to LLM-generated free-text explanations using automatic and human evaluations. This allows us to judge how similar the LLM's and the Bayesian network's decision processes are. We demonstrate the usage of our framework with an example hypothesis and two realisations in Bayesian networks. The resulting models do not exhibit a strong similarity to GPT-3.5. We discuss the implications of this as well 
    
[^27]: StableMask: 在仅解码Transformer中改进因果屏蔽的方法

    StableMask: Refining Causal Masking in Decoder-only Transformer

    [https://arxiv.org/abs/2402.04779](https://arxiv.org/abs/2402.04779)

    StableMask是一种在仅解码Transformer中改进因果屏蔽的无参数方法，通过引入伪注意力值来平衡注意力分布，并通过逐渐减小的屏蔽比率来编码绝对位置信息。

    

    在语言建模中，仅解码Transformer架构中采用因果屏蔽和相对位置编码（RPE）已成为事实上的选择。尽管其在各种任务中表现出色，但我们发现了两个限制：首先，即使当前嵌入具有足够的自包含信息，它要求所有注意力分数都为非零且总和为1。这强迫模型对特定的标记分配不成比例的过度关注。其次，基于RPE的Transformer在编码绝对位置信息方面的能力有限，因此在位置关键任务中受到限制。在这项工作中，我们提出了StableMask：一种无参数的方法，通过改进因果屏蔽来解决这两个限制。它引入了伪注意力值来平衡注意力分布，并通过逐渐减小的屏蔽比率来编码绝对位置信息。StableMask的有效性得到了验证。

    The decoder-only Transformer architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling. Despite its exceptional performance across various tasks, we have identified two limitations: First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information. This compels the model to assign disproportional excessive attention to specific tokens. Second, RPE-based Transformers are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks. In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio. StableMask's effectiveness is validated both 
    
[^28]: 大型语言模型作为可信的解释器

    Large Language Models As Faithful Explainers

    [https://arxiv.org/abs/2402.04678](https://arxiv.org/abs/2402.04678)

    本论文提出了一个生成解释框架（xLLM），用于提高大型语言模型（LLMs）自然语言格式解释的可信度。通过一个评估器来量化解释的可信度，并通过迭代优化过程来提高可信度。

    

    近年来，大型语言模型(LLMs)通过利用其丰富的内部知识和推理能力，已经能够熟练解决复杂的任务。然而，这种复杂性阻碍了传统的以输入为重点的解释算法来解释LLMs的复杂决策过程。为了解决这个问题，最近出现了一种自我解释机制，通过自然语言的形式进行单向推理，从而实现对LLMs预测的解释。然而，这种自然语言解释经常因为缺乏可信度而受到批评，因为这些解释可能不准确地反映LLMs的决策行为。在这项工作中，我们引入了一个生成解释框架xLLM，以提高LLMs自然语言格式的解释的可信度。具体而言，我们提出了一个评估器来量化自然语言解释的可信度，并通过xLLM的迭代优化过程来提高可信度，目标是最大程度地提高可信度。

    Large Language Models (LLMs) have recently become proficient in addressing complex tasks by utilizing their rich internal knowledge and reasoning ability. Consequently, this complexity hinders traditional input-focused explanation algorithms for explaining the complex decision-making processes of LLMs. Recent advancements have thus emerged for self-explaining their predictions through a single feed-forward inference in a natural language format. However, natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs. In this work, we introduce a generative explanation framework, xLLM, to improve the faithfulness of the explanations provided in natural language formats for LLMs. Specifically, we propose an evaluator to quantify the faithfulness of natural language explanation and enhance the faithfulness by an iterative optimization process of xLLM, with the goal of maximizing the 
    
[^29]: 摘要自动生成中的源信息识别

    Source Identification in Abstractive Summarization

    [https://arxiv.org/abs/2402.04677](https://arxiv.org/abs/2402.04677)

    该论文研究了摘要自动生成中的源信息识别问题，并通过分析源句子的方法来探索抽象摘要的生成方式。他们通过注释源句子和比较多种方法建立了一个强基线，并发现在不同情景下，基于困惑度的方法和基于相似度的方法表现较好。

    

    神经抽象摘要生成模型以端到端的方式生成摘要，但目前对源信息如何转化为摘要的过程知之甚少。本文中，我们将包含生成摘要中基本信息的输入句子定义为“源句子”，并通过分析源句子来研究抽象摘要的生成方式。为此，我们对CNN/DailyMail和XSum数据集中的文档-摘要对进行了系统生成和参考摘要的源句子注释，并制定了自动源句子检测方法并比较了多种方法以建立强基线。实验结果表明，在高度抽象的情况下，基于困惑度的方法表现良好，而基于相似度的方法在相对提取式的情况下表现稳健。我们的代码和数据可以在https://github.com/suhara/sourcesum找到。

    Neural abstractive summarization models make summaries in an end-to-end manner, and little is known about how the source information is actually converted into summaries. In this paper, we define input sentences that contain essential information in the generated summary as $\textit{source sentences}$ and study how abstractive summaries are made by analyzing the source sentences. To this end, we annotate source sentences for reference summaries and system summaries generated by PEGASUS on document-summary pairs sampled from the CNN/DailyMail and XSum datasets. We also formulate automatic source sentence detection and compare multiple methods to establish a strong baseline for the task. Experimental results show that the perplexity-based method performs well in highly abstractive settings, while similarity-based methods perform robustly in relatively extractive settings. Our code and data are available at https://github.com/suhara/sourcesum.
    
[^30]: TransLLaMa: 基于LLM的同传翻译系统

    TransLLaMa: LLM-based Simultaneous Translation System

    [https://arxiv.org/abs/2402.04636](https://arxiv.org/abs/2402.04636)

    本研究通过对一个小数据集进行微调，展示了一个预训练的开源LLM在同时机器翻译任务中控制输入分段的能力，从而消除了独立政策的需要，并实现了与最新基线相当的BLEU分数。同时，闭源模型GPT-4在零-shot下也显示出了令人鼓舞的结果，为提升未来的SiMT系统提供了有希望的方向。

    

    仅有解码器的大型语言模型（LLM）最近在文本生成和推理方面展示了令人印象深刻的能力。然而，在同时机器翻译（SiMT）方面，它们的应用有限，目前由编码器-解码器变压器主导。本研究表明，在一个包含因果对齐的源句子和目标句子对的小数据集上进行微调后，一个预训练的开源LLM能够通过生成一个特殊的“等待”标记来直接控制输入分段。这消除了独立政策的需要，使LLM能够执行与特定最新基线的BLEU分数相当的英德和英俄SiMT任务。我们还评估了闭源模型，如GPT-4，在没有先前训练（零-shot）的情况下表现出了令人鼓舞的结果，这表明了改进未来SiMT系统的一个有希望的途径。

    Decoder-only large language models (LLMs) have recently demonstrated impressive capabilities in text generation and reasoning. Nonetheless, they have limited applications in simultaneous machine translation (SiMT), currently dominated by encoder-decoder transformers. This study demonstrates that, after fine-tuning on a small dataset comprising causally aligned source and target sentence pairs, a pre-trained open-source LLM can control input segmentation directly by generating a special "wait" token. This obviates the need for a separate policy and enables the LLM to perform English-German and English-Russian SiMT tasks with BLEU scores that are comparable to those of specific state-of-the-art baselines. We also evaluated closed-source models such as GPT-4, which displayed encouraging results in performing the SiMT task without prior training (zero-shot), indicating a promising avenue for enhancing future SiMT systems.
    
[^31]: 认知策略增强的说服对话机器人的未来：新的观点和趋势

    The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends

    [https://arxiv.org/abs/2402.04631](https://arxiv.org/abs/2402.04631)

    该论文讨论了认知策略增强的说服对话机器人的未来发展，强调了对于实现智能和人类化对话系统的重要性。该领域的研究者通过利用大型语言模型的进展，使对话机器人在上下文理解和回应生成方面取得了巨大进步。然而，要实现人类般的说服水平，说服对话机器人需要融入认知心理学的知识。

    

    說服是人类交流中至关重要的能力之一，在智能对话系统领域，研究人员对此给予了广泛的关注。我们人类往往通过对话的方式来说服他人改变他们的观点、态度或行为，这在各种情景下都是如此（例如推动社会福利、在在线平台上争论）。开发能够说服他人接受某些立场的对话机器人对于实现真正智能和具有人类特点的对话系统至关重要。由于大型语言模型（LLM）的显著进展，对话机器人已经获得了在上下文理解和回应生成方面的卓越能力。然而，作为一个典型且复杂的认知心理系统，说服对话机器人还需要获得来自认知心理学领域的知识，以达到人类般的说服水平。因此，认知策略增强的说服对话机器人（定义为CogAgent）

    Persuasion, as one of the crucial abilities in human communication, has garnered extensive attention from researchers within the field of intelligent dialogue systems. We humans tend to persuade others to change their viewpoints, attitudes or behaviors through conversations in various scenarios (e.g., persuasion for social good, arguing in online platforms). Developing dialogue agents that can persuade others to accept certain standpoints is essential to achieving truly intelligent and anthropomorphic dialogue system. Benefiting from the substantial progress of Large Language Models (LLMs), dialogue agents have acquired an exceptional capability in context understanding and response generation. However, as a typical and complicated cognitive psychological system, persuasive dialogue agents also require knowledge from the domain of cognitive psychology to attain a level of human-like persuasion. Consequently, the cognitive strategy-enhanced persuasive dialogue agent (defined as CogAgent
    
[^32]: SPARQL生成：对OpenLLaMA用于生命科学知识图谱问答的微调进行分析

    SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph

    [https://arxiv.org/abs/2402.04627](https://arxiv.org/abs/2402.04627)

    在本研究中，我们针对生命科学知识图谱上的问题回答对OpenLlama LLM进行了微调，并提出了一种数据增强方法，以扩展现有查询集合，从而能够进行微调，即使训练数据稀缺。我们还研究了查询中语义线索的作用。

    

    大型语言模型（LLM）在各种自然语言处理应用中的成功为基于LLM的知识图谱问答系统开辟了新的路径。然而，其实施的主要障碍之一是在将问题转化为相应的SPARQL查询的任务中，尤其是在特定领域的知识图谱中，训练数据的稀缺性。为了克服这一挑战，在本研究中，我们评估了针对生命科学知识图谱上的问题回答对OpenLlama LLM进行微调的几种策略。具体而言，我们提出了一种端到端的数据增强方法，用于扩展已有查询集合，从而获得一组更大的语义丰富的问题-SPARQL查询对的数据集，即使在这些对稀缺的数据集中也能进行微调。在这个背景下，我们还研究了查询中语义“线索”的作用，例如有意义的变量名和...

    The recent success of Large Language Models (LLM) in a wide range of Natural Language Processing applications opens the path towards novel Question Answering Systems over Knowledge Graphs leveraging LLMs. However, one of the main obstacles preventing their implementation is the scarcity of training data for the task of translating questions into corresponding SPARQL queries, particularly in the case of domain-specific KGs. To overcome this challenge, in this study, we evaluate several strategies for fine-tuning the OpenLlama LLM for question answering over life science knowledge graphs. In particular, we propose an end-to-end data augmentation approach for extending a set of existing queries over a given knowledge graph towards a larger dataset of semantically enriched question-to-SPARQL query pairs, enabling fine-tuning even for datasets where these pairs are scarce. In this context, we also investigate the role of semantic "clues" in the queries, such as meaningful variable names and
    
[^33]: MEMORYLLM：面向自更新的大型语言模型

    MEMORYLLM: Towards Self-Updatable Large Language Models

    [https://arxiv.org/abs/2402.04624](https://arxiv.org/abs/2402.04624)

    MEMORYLLM是一个自更新的大型语言模型，其中包括Transformer和一个固定大小的内存池，能够有效地整合新知识并保持长期信息保留能力。即使在近百万次内存更新后，MEMORYLLM仍能保持操作的完整性。

    

    现有的大型语言模型（LLMs）在部署后通常保持静态，这可能会使向模型中注入新知识变得困难。我们旨在构建包含相当比例的可自更新参数的模型，使模型能够有效且高效地整合新知识。为此，我们引入了MEMORYLLM，它是一个由Transformer和固定大小的内存池组成的模型，位于Transformer的潜空间内。MEMORYLLM可以通过文本知识进行自我更新并记忆先前注入的知识。我们的评估证明了MEMORYLLM有效地整合新知识的能力，其性能在模型编辑基准上得到了证实。同时，该模型还表现出长期信息保留能力，这在我们自定义的评估和长上下文基准中得到了验证。MEMORYLLM在进行了近百万次内存更新后，没有任何性能下降的迹象，显示出操作的完整性。

    Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model. We aim to build models containing a considerable portion of self-updatable parameters, enabling the model to integrate new knowledge effectively and efficiently. To this end, we introduce MEMORYLLM, a model that comprises a transformer and a fixed-size memory pool within the latent space of the transformer. MEMORYLLM can self-update with text knowledge and memorize the knowledge injected earlier. Our evaluations demonstrate the ability of MEMORYLLM to effectively incorporate new knowledge, as evidenced by its performance on model editing benchmarks. Meanwhile, the model exhibits long-term information retention capacity, which is validated through our custom-designed evaluations and long-context benchmarks. MEMORYLLM also shows operational integrity without any sign of performance degradation even after nearly a million memory updates.
    
[^34]: InfLLM: 揭示LLMs对于处理超长序列的内在能力，无需训练的记忆

    InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory

    [https://arxiv.org/abs/2402.04617](https://arxiv.org/abs/2402.04617)

    InfLLM是一种无需训练的基于记忆的方法，用于揭示LLMs处理超长序列的内在能力。它通过存储远距离上下文和高效的注意计算机制，允许LLMs有效处理具有流式输入的长序列。

    

    大型语言模型（LLMs）已成为处理具有漫长传输输入的现实应用的基石，如LLM驱动代理。然而，现有的在受限最大长度序列上预训练的LLMs无法推广到更长的序列，因为存在领域外和分散注意力的问题。为了缓解这些问题，现有的工作采用滑动注意力窗口和丢弃远距离标记，以处理超长序列。不幸的是，这些方法无法捕获序列内的长距离依赖关系，以深入理解语义。本文介绍了一种无需训练的基于记忆的方法InfLLM，来揭示LLMs处理流式长序列的内在能力。具体而言，InfLLM将远距离的上下文存储到附加的内存单元中，并使用高效的机制来查找与注意计算相关的标记单元。因此，InfLLM允许LLMs高效处理长序列，同时保持了对语义的深入理解。

    Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs, such as LLM-driven agents. However, existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues. To alleviate these issues, existing efforts employ sliding attention windows and discard distant tokens to achieve the processing of extremely long sequences. Unfortunately, these approaches inevitably fail to capture long-distance dependencies within sequences to deeply understand semantics. This paper introduces a training-free memory-based method, InfLLM, to unveil the intrinsic ability of LLMs to process streaming long sequences. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences while maintaining
    
[^35]: TinyLLM: 从多个大型语言模型学习一个小型学生模型

    TinyLLM: Learning a Small Student from Multiple Large Language Models

    [https://arxiv.org/abs/2402.04616](https://arxiv.org/abs/2402.04616)

    TinyLLM是一种从多个大型语言模型中学习小型学生模型的知识蒸馏范式，旨在解决知识多样性有限和缺乏上下文信息等问题，并鼓励学生模型理解答案背后的原理。

    

    将更强大的大型语言模型（LLMs）的推理能力转移到较小的模型上具有吸引力，因为较小的LLMs更灵活，成本更低。在现有的解决方案中，知识蒸馏因其出色的效率和泛化能力而脱颖而出。然而，现有方法存在一些缺点，包括知识多样性有限和缺乏丰富的上下文信息。为了解决这些问题并促进紧凑语言模型的学习，我们提出了TinyLLM，一种从多个大型教师LLMs中学习小型学生LLM的新型知识蒸馏范式。特别地，我们鼓励学生LLM不仅生成正确答案，而且理解这些答案背后的原理。鉴于不同的LLMs具有不同的推理能力，我们引导学生模型吸收来自多个教师LLMs的知识。我们进一步引入了一个上下文示例生成器和一个老师强制模块...

    Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing 
    
[^36]: 信实性与可信度: 关于大型语言模型解释的(不)可靠性

    Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models

    [https://arxiv.org/abs/2402.04614](https://arxiv.org/abs/2402.04614)

    本文讨论了大型语言模型生成的自我解释中的信实性与可信度之间的差异。虽然这些解释在逻辑上是合乎情理且连贯的，但并不一定与模型的推理过程一致，引发对其信实性的担忧。

    

    大型语言模型(LLMs)被部署为几种自然语言处理（NLP）应用的强大工具。最近的研究显示，现代LLMs可以生成自我解释（SEs），这些SEs揭示了它们解释其行为的中间推理步骤。由于其对话性和可信度的特点，自我解释已广泛应用。然而，我们对其信实性了解甚少。在本研究中，我们讨论了LLMs生成的SEs中信实性和可信度之间的二分法。我们认为，虽然LLMs擅长生成可信的解释-对人类用户来说似乎逻辑和连贯-但这些解释未必与LLMs的推理过程相一致，引发对其信实性的担忧。我们强调，当前趋势是为了用户友好界面的需求而增加解释的可信度，可能会以降低解释的信实性为代价。

    Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness
    
[^37]: 通过LLM后编辑改进跨领域低资源文本生成：一种程序员-解释器方法

    Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach

    [https://arxiv.org/abs/2402.04609](https://arxiv.org/abs/2402.04609)

    本研究提出了一种神经程序员-解释器方法，通过后编辑来改进大型语言模型在文本生成任务中的性能。实验证明，在交叉领域环境中，该方法显著超越其他最先进的后编辑方法，提升了GPT-3.5在逻辑形式到文本转换和低资源机器翻译中的性能。

    

    后编辑已被证明能够有效改善大型语言模型（LLM），如GPT-3.5或GPT-4，生成的文本质量，特别是在更新其参数以提升文本质量不可行或昂贵的情况下。然而，仅依赖较小的语言模型进行后编辑可能会限制LLM在领域间的泛化能力。此外，这些方法中的编辑策略对于文本生成任务并不是最佳设计。为了解决这些限制，我们提出了一种神经程序员-解释器方法，在编辑LLM的输出时保留了其领域泛化能力。该框架中的编辑操作专门用于文本生成。大量实验证明，在交叉领域环境中，程序员-解释器显著提升了GPT-3.5在逻辑形式到文本转换和低资源机器翻译中的性能，超过了其他最先进的LLM后编辑方法。

    Post-editing has proven effective in improving the quality of text generated by large language models (LLMs) such as GPT-3.5 or GPT-4, particularly when direct updating of their parameters to enhance text quality is infeasible or expensive. However, relying solely on smaller language models for post-editing can limit the LLMs' ability to generalize across domains. Moreover, the editing strategies in these methods are not optimally designed for text-generation tasks. To address these limitations, we propose a neural programmer-interpreter approach that preserves the domain generalization ability of LLMs when editing their output. The editing actions in this framework are specifically devised for text generation. Extensive experiments demonstrate that the programmer-interpreter significantly enhances GPT-3.5's performance in logical form-to-text conversion and low-resource machine translation, surpassing other state-of-the-art (SOTA) LLM post-editing methods in cross-domain settings.
    
[^38]: Alirector: 提升对齐的中文语法错误修正器

    Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector

    [https://arxiv.org/abs/2402.04601](https://arxiv.org/abs/2402.04601)

    本文提出了一种提升对齐的中文语法错误修正器，旨在解决使用自回归生成模型时面临的过度修正挑战。该方法适用于序列到序列模型和仅解码的大型语言模型，并通过对齐模型进行多轮修正，提高修正效果。

    

    中文语法错误修正（CGEC）在使用自回归生成模型（如序列到序列模型和仅解码的大型语言模型）时面临严重的过度修正挑战。虽然以前的方法旨在解决序列到序列模型中的过度修正问题，但很难适应仅解码的大型语言模型。本文提出了一种提升对齐的解决方案，适用于序列到序列模型和仅解码的大型语言模型，并且能够解决过度修正问题。我们的方法首先训练一个修正模型，生成源句子的初始修正。然后，将源句子与初始修正结合起来，通过一个对齐模型进行另一轮修正，以促使对齐模型专注于潜在的过度修正。此外，为了增强模型识别细微差别的能力，我们进一步探索了源句子和初始修正的逆向对齐。最后，我们将对齐的知识转移到CGEC模型中，以提高修正效果。

    Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs). While previous methods aim to address overcorrection in Seq2Seq models, they are difficult to adapt to decoder-only LLMs. In this paper, we propose an alignment-enhanced corrector for the overcorrection problem that applies to both Seq2Seq models and decoder-only LLMs. Our method first trains a correction model to generate an initial correction of the source sentence. Then, we combine the source sentence with the initial correction and feed it through an alignment model for another round of correction, aiming to enforce the alignment model to focus on potential overcorrection. Moreover, to enhance the model's ability to identify nuances, we further explore the reverse alignment of the source sentence and the initial correction. Finally, we transfer the alignment kn
    
[^39]: UltraLink: 一个开源的知识增强多语言监督微调数据集

    UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset

    [https://arxiv.org/abs/2402.04588](https://arxiv.org/abs/2402.04588)

    本论文构建了一个开源的多语言监督微调数据集UltraLink，通过引入基于知识的数据增强方法提升了语言模型在文化特定知识上的能力，同时发现现代语言模型具有强大的跨语言迁移能力，减少了语言无关数据集的需求。

    

    开源的大型语言模型(LLMs)在不同领域取得了显著的优势。然而，大部分研究主要集中在英文上，对于多语言监督微调的研究还相对有限。因此，在本研究中我们构建了一个开源的多语言监督微调数据集。与之前简单翻译英文指令的方法不同，我们考虑了LLMs的语言特定和语言无关能力。对于语言特定能力，我们引入了一个基于知识的数据增强方法，以提取LLMs更多的文化特定知识，提高它们为不同国家用户服务的能力。对于语言无关能力，通过实验发现现代LLMs展现出很强的跨语言迁移能力，因此多次学习相同内容的多种语言并不必要。因此，我们可以大幅减少语言无关SFT数据集。

    Open-source large language models (LLMs) have gained significant strength across diverse fields. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual supervised fine-tuning. In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset. Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs. For language-specific abilities, we introduce a knowledge-grounded data augmentation approach to elicit more culture-specific knowledge of LLMs, improving their ability to serve users from different countries. For language-agnostic abilities, we find through experiments that modern LLMs exhibit strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary. Consequently, we can substantially prune the language-agnostic SFT data w
    
[^40]: 大语言模型代理能够模拟人类的信任行为吗？

    Can Large Language Model Agents Simulate Human Trust Behaviors?

    [https://arxiv.org/abs/2402.04559](https://arxiv.org/abs/2402.04559)

    大语言模型代理能够模拟人类的信任行为，表现出在信任游戏中的信任行为，并且与人类行为具有高度一致性，但存在一些偏见和对代理与人类的差异。

    

    大语言模型（LLM）代理已经越来越多地被采用作为模拟工具，用于模拟人类在社会科学等领域中的行为。然而，一个基本的问题仍然存在：LLM代理是否真的能够模拟人类行为？在本文中，我们专注于人类互动中最关键的行为之一，信任，旨在调查LLM代理是否能够模拟人类的信任行为。我们首先发现，在被行为经济学广泛接受的信任游戏框架下，LLM代理通常表现出信任行为，称为代理信任。然后，我们发现LLM代理在信任行为方面与人类具有较高的行为一致性，表明使用LLM代理模拟人类的信任行为是可行的。此外，我们还探索了代理信任中的偏见以及代理信任在对代理和人类之间的差异方面的内在特性。我们还探讨了包括高级推理策略在内的条件下代理信任的内在特性。

    Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in applications such as social science. However, one fundamental question remains: can LLM agents really simulate human behaviors? In this paper, we focus on one of the most critical behaviors in human interactions, trust, and aim to investigate whether or not LLM agents can simulate human trust behaviors. We first find that LLM agents generally exhibit trust behaviors, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that LLM agents can have high behavioral alignment with humans regarding trust behaviors, indicating the feasibility to simulate human trust behaviors with LLM agents. In addition, we probe into the biases in agent trust and the differences in agent trust towards agents and humans. We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strate
    
[^41]: 分享您已经知道的东西：用于混合语言数据的情感检测的跨语言-脚本转移和对齐

    Share What You Already Know: Cross-Language-Script Transfer and Alignment for Sentiment Detection in Code-Mixed Data

    [https://arxiv.org/abs/2402.04542](https://arxiv.org/abs/2402.04542)

    本研究提出了一种跨语言-脚本知识共享架构，利用各种语言脚本中文本表示的跨关注和对齐，以提高混合语言数据中的情感检测效果。

    

    代码混合涉及多种语言的混合。这是社交媒体文本中日益发生的现象。通常，代码混合文本是用单一脚本编写的，即使涉及的语言有不同的脚本。预训练的多语言模型主要利用语言的本机脚本中的数据。在现有研究中，代码混合的文本被直接使用。然而，使用每种语言的本机脚本可以由于预训练的知识而生成更好的文本表示。因此，本研究提出了一种跨语言-脚本知识共享架构，利用各种语言脚本中文本表示的跨关注和对齐。在包含尼泊尔语-英语和印地语-英语代码混合文本的两个不同数据集上的实验结果表明了该方法的有效性。模型解释技术对模型的解释显示了知识共享的过程。

    Code-switching entails mixing multiple languages. It is an increasingly occurring phenomenon in social media texts. Usually, code-mixed texts are written in a single script, even though the languages involved have different scripts. Pre-trained multilingual models primarily utilize the data in the native script of the language. In existing studies, the code-switched texts are utilized as they are. However, using the native script for each language can generate better representations of the text owing to the pre-trained knowledge. Therefore, a cross-language-script knowledge sharing architecture utilizing the cross attention and alignment of the representations of text in individual language scripts was proposed in this study. Experimental results on two different datasets containing Nepali-English and Hindi-English code-switched texts, demonstrate the effectiveness of the proposed method. The interpretation of the model using model explainability technique illustrates the sharing of la
    
[^42]: SumRec: 使用开放领域对话进行推荐的框架

    SumRec: A Framework for Recommendation using Open-Domain Dialogue

    [https://arxiv.org/abs/2402.04523](https://arxiv.org/abs/2402.04523)

    本研究提出了一个新的框架SumRec，用于从开放领域的对话中推荐个性化信息。该框架利用大型语言模型生成对话中说话者信息的摘要，并根据用户类型推荐物品信息，实验证明SumRec框架比基准方法提供更好的推荐。

    

    聊天对话包含着关于说话者兴趣、偏好和经验的大量有用信息。因此，可以利用开放领域聊天对话中的知识来个性化各种系统并为高级信息提供推荐。本研究提出了一种用于从开放领域聊天对话中推荐信息的新颖框架SumRec。该研究还使用ChatRec这个新构建的数据集来检验该框架的性能。为了提取说话者和物品的特征，SumRec框架使用大型语言模型（LLM）从对话中生成说话者信息的摘要，并根据用户类型推荐物品信息。然后将说话者和物品信息输入到评分估计模型中，生成推荐分数。实验结果表明，SumRec框架比使用对话和原始物品描述的基准方法提供更好的推荐。

    Chat dialogues contain considerable useful information about a speaker's interests, preferences, and experiences.Thus, knowledge from open-domain chat dialogue can be used to personalize various systems and offer recommendations for advanced information.This study proposed a novel framework SumRec for recommending information from open-domain chat dialogue.The study also examined the framework using ChatRec, a newly constructed dataset for training and evaluation. To extract the speaker and item characteristics, the SumRec framework employs a large language model (LLM) to generate a summary of the speaker information from a dialogue and to recommend information about an item according to the type of user.The speaker and item information are then input into a score estimation model, generating a recommendation score.Experimental results show that the SumRec framework provides better recommendations than the baseline method of using dialogues and item descriptions in their original form.
    
[^43]: 在流数据上进行高效推理的在线级联学习

    Online Cascade Learning for Efficient Inference over Streams

    [https://arxiv.org/abs/2402.04513](https://arxiv.org/abs/2402.04513)

    这项研究提出了在线级联学习的方法，通过学习一个“级联”模型，从容量较低的模型到强大的语言模型，以及推迟策略，可以在流数据处理中同时保证准确性和降低推理成本。

    

    大型语言模型 (LLM) 在回答关于数据流的复杂查询方面具有天然的优势，但是 LLM 推理的高计算成本使得它们在许多任务中不可行。我们提出了在线级联学习，这是首个解决这一挑战的方法。这里的目标是学习一个“级联”模型，从容量较低的模型（如逻辑回归器）开始，到强大的 LLM 结束，并配备一个决定在给定输入上使用哪个模型的推迟策略。我们将在线学习级联的任务公式化为一个模仿学习问题，并为该问题提供了无遗憾算法。在四个基准测试中的实验结果显示，我们的方法在准确性上与 LLM 相当，同时将推理成本削减了多达 90%，突显了它在流处理中的效能和适应能力。

    Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to addressing this challenge. The objective here is to learn a "cascade" of models, starting with lower-capacity models (such as logistic regressors) and ending with a powerful LLM, along with a deferral policy that determines the model that is used on a given input. We formulate the task of learning cascades online as an imitation-learning problem and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90%, underscoring its efficacy and adaptability in stream processing.
    
[^44]: 自然语言歧义的层理论模型的发展

    Developments in Sheaf-Theoretic Models of Natural Language Ambiguities

    [https://arxiv.org/abs/2402.04505](https://arxiv.org/abs/2402.04505)

    本论文扩展了层理论模型从词汇歧义到篇章歧义，通过计算新的上下文性度量，发现上下文模型的比例大幅增加，并通过将Winograd Schema建模为Bell-CHSH场景，展示了层理论模型在处理具有指代歧义的自然语言挑战上的应用。

    

    层是数学对象，由基础构成的拓扑空间和与之相关的数据组成，例如定义在开集上的连续函数。层最初用于代数拓扑和逻辑中。最近，它们也用于建模物理实验和自然语言消岐过程等事件。我们将这些模型从词汇歧义扩展到由指代产生的篇章歧义。首先，对一组基本的指代篇章数据计算了一个新的上下文性度量，结果表明上下文模型的比例更高，为82.9%，而之前的工作只有3.17%的上下文模型。随后，我们展示了如何将包含指代歧义的自然语言处理挑战——Winograd Schema建模为Bell-CHSH场景，其上下文比例为0.096。

    Sheaves are mathematical objects consisting of a base which constitutes a topological space and the data associated with each open set thereof, e.g. continuous functions defined on the open sets. Sheaves have originally been used in algebraic topology and logic. Recently, they have also modelled events such as physical experiments and natural language disambiguation processes. We extend the latter models from lexical ambiguities to discourse ambiguities arising from anaphora. To begin, we calculated a new measure of contextuality for a dataset of basic anaphoric discourses, resulting in a higher proportion of contextual models--82.9%--compared to previous work which only yielded 3.17% contextual models. Then, we show how an extension of the natural language processing challenge, known as the Winograd Schema, which involves anaphoric ambiguities can be modelled on the Bell-CHSH scenario with a contextual fraction of 0.096.
    
[^45]: 训练大型语言模型的梯度计算的细粒度复杂性

    The Fine-Grained Complexity of Gradient Computation for Training Large Language Models

    [https://arxiv.org/abs/2402.04497](https://arxiv.org/abs/2402.04497)

    本文研究了训练大型语言模型中梯度计算的复杂性，证明了在某些参数区域内可以以几乎线性时间进行前向计算，但在其余参数区域内需要超过二次时间，这对于LLM训练的每个步骤都具有重要意义。

    

    在过去几年中，大型语言模型（LLM）已经作出了基本贡献。要训练一个LLM，人们需要交替运行“前向计算”和“反向计算”。前向计算可以视为注意力函数的评估，而后向计算可以视为梯度计算。在之前的研究中，[Alman和Song，NeurIPS 2023]证明在某些参数区域中前向步骤可以在几乎线性时间内执行，但在其余参数区域内，除非流行的假设SETH不成立，否则没有真正的亚二次时间算法。在这项工作中，我们展示了对于计算单层注意力网络损失函数的梯度，以及LLM训练的整个过程中似乎更难的问题几乎完全相同的结果。这完全刻画了LLM训练每个步骤的细粒度复杂性。

    Large language models (LLMs) have made fundamental contributions over the last a few years. To train an LLM, one needs to alternatingly run `forward' computations and `backward' computations. The forward computation can be viewed as attention function evaluation, and the backward computation can be viewed as a gradient computation. In previous work by [Alman and Song, NeurIPS 2023], it was proved that the forward step can be performed in almost-linear time in certain parameter regimes, but that there is no truly sub-quadratic time algorithm in the remaining parameter regimes unless the popular hypothesis SETH is false. In this work, we show nearly identical results for the harder-seeming problem of computing the gradient of loss function of one layer attention network, and thus for the entire process of LLM training. This completely characterizes the fine-grained complexity of every step of LLM training.
    
[^46]: ColorSwap: 一个用于多模态评估的颜色和单词排序数据集

    ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation

    [https://arxiv.org/abs/2402.04492](https://arxiv.org/abs/2402.04492)

    本文介绍了ColorSwap数据集，用于评估和改进多模态模型在匹配物体和颜色方面的能力。通过将颜色词重新排序以修改不同的对象，该数据集可以测试模型在这项任务上的鲁棒性。尽管目前的模型在这个任务上仍不够稳定，但通过更先进的提示技术可能会有所改善。

    

    本文介绍了ColorSwap数据集，旨在评估和改进多模态模型在匹配物体和其颜色方面的熟练程度。该数据集包含2000个独特的图像-标题对，分为1000个示例。每个示例包括一个标题-图像对，以及一个“颜色交换”对。我们遵循Winoground方案：示例中的两个标题具有相同的单词，但颜色单词被重新排列以修改不同的对象。该数据集通过自动化的标题和图像生成与人类的交互创造而成。我们评估图像-文本匹配（ITM）和视觉语言模型（VLMs）发现即使是最新的模型在这个任务上仍然不够稳健。GPT-4V和LLaVA在我们的主要VLM指标上得分分别为72%和42%，尽管它们可能通过更先进的提示技术来提升。在主要的ITM指标上，像CLIP和SigLIP这样的对比模型接近于随机猜测（分别为12%和30%），尽管非对比模型在这个任务上表现得更好。

    This paper introduces the ColorSwap dataset, designed to assess and improve the proficiency of multimodal models in matching objects with their colors. The dataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000 examples. Each example includes a caption-image pair, along with a ``color-swapped'' pair. We follow the Winoground schema: the two captions in an example have the same words, but the color words have been rearranged to modify different objects. The dataset was created through a novel blend of automated caption and image generation with humans in the loop. We evaluate image-text matching (ITM) and visual language models (VLMs) and find that even the latest ones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on our main VLM metric, although they may improve with more advanced prompting techniques. On the main ITM metric, contrastive models such as CLIP and SigLIP perform close to chance (at 12% and 30%, respectively), although the non-
    
[^47]: 通过叙述检测语言模型中的模式崩溃

    Detecting Mode Collapse in Language Models via Narration

    [https://arxiv.org/abs/2402.04477](https://arxiv.org/abs/2402.04477)

    通过研究来自三个OpenAI语言模型的故事，我们发现GPT-3的更新版本逐渐出现了“模式崩溃”的现象。

    

    没有两个作者写作方式相同。从词汇到修辞手法，在书面叙述中呈现出的个人特色，暗示了一位特定的作者，文学理论家将其称为隐含或虚拟作者，与文本的实际作者或叙述者不同。早期使用来自各种不协调来源的未经过滤的训练集进行训练的大型语言模型产生了不连贯的个性，这对于对话任务来说是有问题的，但对于从多个观点采样文学却是有用的。近年来，在对齐研究方面取得的成功使研究人员能够通过指导调整和从人类反馈中进行强化学习（RLHF）来对语言模型施加主观一致的人物形象，但对齐模型是否保留了对模拟任意虚拟作者的能力几乎没有受到审查。通过研究来自三个OpenAI语言模型的4,374个故事，我们展示了随着GPT-3的更新版本，越来越多的“模式崩溃”现象的发生。

    No two authors write alike. Personal flourishes invoked in written narratives, from lexicon to rhetorical devices, imply a particular author--what literary theorists label the implied or virtual author; distinct from the real author or narrator of a text. Early large language models trained on unfiltered training sets drawn from a variety of discordant sources yielded incoherent personalities, problematic for conversational tasks but proving useful for sampling literature from multiple perspectives. Successes in alignment research in recent years have allowed researchers to impose subjectively consistent personae on language models via instruction tuning and reinforcement learning from human feedback (RLHF), but whether aligned models retain the ability to model an arbitrary virtual author has received little scrutiny. By studying 4,374 stories sampled from three OpenAI language models, we show successive versions of GPT-3 suffer from increasing degrees of "mode collapse" whereby overf
    
[^48]: 双视图视觉背景化的网页导航

    Dual-View Visual Contextualization for Web Navigation

    [https://arxiv.org/abs/2402.04476](https://arxiv.org/abs/2402.04476)

    本文提出了一种通过网页截图中元素的“双视图”来进行 HTML 元素的背景化的方法，这种方法通过将每个元素与其邻居元素进行背景化，使用文本和视觉特征，使得HTML元素的结果表示对于代理执行操作更加信息丰富。

    

    自动网页导航旨在构建一个可以根据语言指令在实际网站上执行复杂和多样任务的网络代理。现有工作主要是以 HTML 文档作为输入，HTML 文档定义了网页的内容和操作空间（即可操作元素和操作）。然而，HTML 文档可能无法为每个元素提供清晰的任务相关背景，使得选择正确的（一系列的）操作变得困难。本文提出通过网页截图中元素的“双视图”来进行 HTML 元素的背景化：每个 HTML 元素在截图中有其对应的边界框和视觉内容。我们基于一个洞察力——网页开发者倾向于在网页上将任务相关元素放置在附近以增强用户体验，并提出将每个元素与其邻居元素进行背景化，使用文本和视觉特征。HTML 元素的结果表示对于代理执行操作更加信息丰富。

    Automatic web navigation aims to build a web agent that can follow language instructions to execute complex and diverse tasks on real-world websites. Existing work primarily takes HTML documents as input, which define the contents and action spaces (i.e., actionable elements and operations) of webpages. Nevertheless, HTML documents may not provide a clear task-related context for each element, making it hard to select the right (sequence of) actions. In this paper, we propose to contextualize HTML elements through their "dual views" in webpage screenshots: each HTML element has its corresponding bounding box and visual content in the screenshot. We build upon the insight -- web developers tend to arrange task-related elements nearby on webpages to enhance user experiences -- and propose to contextualize each element with its neighbor elements, using both textual and visual features. The resulting representations of HTML elements are more informative for the agent to take action. We val
    
[^49]: 评估用于医生-AI会诊的单次分类的嵌入技术

    Evaluating Embeddings for One-Shot Classification of Doctor-AI Consultations

    [https://arxiv.org/abs/2402.04442](https://arxiv.org/abs/2402.04442)

    本研究评估了用于医生-AI会诊的单次分类的嵌入技术，结果表明这些嵌入技术能够可靠且灵活地捕捉文本的语义特征，特别是Word2Vec、GloVe和字符ngram嵌入技术表现良好。

    

    有效的医患沟通对提供高质量的患者护理至关重要。在这项工作中，我们研究了如何使用最先进的嵌入技术和单次分类系统来对医生编写和AI生成的医疗会诊文本进行分类。通过分析诸如词袋模型、字符ngram、Word2Vec、GloVe、fastText和GPT2等嵌入技术，我们研究了我们的单次分类系统在医疗会诊中如何捕捉语义信息。结果表明，这些嵌入技术能够可靠且灵活地捕捉文本的语义特征。总体而言，Word2Vec、GloVe和字符ngram嵌入技术表现良好，表明它们适用于针对此任务的建模。GPT2嵌入技术也显示出显著的性能，表明它也适用于这个任务的模型。我们的机器学习架构显著提高了医疗对话的质量。

    Effective communication between healthcare providers and patients is crucial to providing high-quality patient care. In this work, we investigate how Doctor-written and AI-generated texts in healthcare consultations can be classified using state-of-the-art embeddings and one-shot classification systems. By analyzing embeddings such as bag-of-words, character n-grams, Word2Vec, GloVe, fastText, and GPT2 embeddings, we examine how well our one-shot classification systems capture semantic information within medical consultations. Results show that the embeddings are capable of capturing semantic features from text in a reliable and adaptable manner. Overall, Word2Vec, GloVe and Character n-grams embeddings performed well, indicating their suitability for modeling targeted to this task. GPT2 embedding also shows notable performance, indicating its suitability for models tailored to this task as well. Our machine learning architectures significantly improved the quality of health conversati
    
[^50]: 使用大型语言模型的结构化实体提取

    Structured Entity Extraction Using Large Language Models

    [https://arxiv.org/abs/2402.04437](https://arxiv.org/abs/2402.04437)

    本论文提出了一种使用大型语言模型的结构化实体提取方法，在此任务上通过引入AESOP度量评估模型性能，并将整个提取任务分解为多个阶段，相较于基准模型取得了更好的效果，为未来结构化实体提取的进一步发展提供了有希望的方向。

    

    近年来，机器学习的最新进展显著影响了信息提取领域，大型语言模型（LLMs）在从非结构化文本中提取结构化信息方面起着关键作用。本文探讨了当前结构化实体提取方法的挑战和限制，并引入了一种新的方法来解决这些问题。我们首先介绍和规范化了结构化实体提取（SEE）任务，然后提出了适用于该任务的近似实体集重叠（AESOP）度量，以适当评估模型在这一任务上的性能。随后，我们提出了一种新模型，通过将整个提取任务分解为多个阶段，利用LLMs的强大功能来提高效果和效率。定量评估和人工并行评估证实了我们的模型优于基准模型，为结构化实体提取领域的未来进展提供了有希望的方向。

    Recent advances in machine learning have significantly impacted the field of information extraction, with Large Language Models (LLMs) playing a pivotal role in extracting structured information from unstructured text. This paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues. We contribute to the field by first introducing and formalizing the task of Structured Entity Extraction (SEE), followed by proposing Approximate Entity Set OverlaP (AESOP) Metric designed to appropriately assess model performance on this task. Later, we propose a new model that harnesses the power of LLMs for enhanced effectiveness and efficiency through decomposing the entire extraction task into multiple stages. Quantitative evaluation and human side-by-side evaluation confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction.
    
[^51]: Chatbot遇见管道：利用确定有限自动机增进大规模语言模型的能力

    Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton

    [https://arxiv.org/abs/2402.04411](https://arxiv.org/abs/2402.04411)

    本文介绍了DFA-LLM（确定有限自动机增强的大规模语言模型）框架，通过嵌入从对话中学习到的确定有限自动机在大规模语言模型中，使得对话代理能够生成具有规范合规性的回复，并在广泛的基准测试中验证了其有效性。

    

    本文介绍了一种新颖的框架——确定有限自动机增强的大规模语言模型（DFA-LLM），旨在通过使用大规模语言模型（LLM）提升对话代理的能力。传统的LLM在特定情景（如情感支持和客户服务）中生成规范合规的回复面临挑战。我们的框架通过将从训练对话中学习到的确定有限自动机（DFA）嵌入到LLM中来应对这些挑战。这种结构化的方法使得LLM能够按照DFA指导的确定性回应路径来回应。DFA-LLM的优势包括可解释性结构，上下文感知的对话回复检索以及与现有LLM的即插即用兼容性。广泛的基准测试验证了DFA-LLM的有效性，表明它有潜力成为对话代理领域的有价值的贡献。

    This paper introduces the Definite Finite Automaton augmented large language model (DFA-LLM), a novel framework designed to enhance the capabilities of conversational agents using large language models (LLMs). Traditional LLMs face challenges in generating regulated and compliant responses in special scenarios with predetermined response guidelines, like emotional support and customer service. Our framework addresses these challenges by embedding a Definite Finite Automaton (DFA), learned from training dialogues, within the LLM. This structured approach enables the LLM to adhere to a deterministic response pathway, guided by the DFA. The advantages of DFA-LLM include an interpretable structure through human-readable DFA, context-aware retrieval for responses in conversations, and plug-and-play compatibility with existing LLMs. Extensive benchmarks validate DFA-LLM's effectiveness, indicating its potential as a valuable contribution to the conversational agent.
    
[^52]: 通过个性化参数高效调整实现大规模语言模型的民主化

    Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning

    [https://arxiv.org/abs/2402.04401](https://arxiv.org/abs/2402.04401)

    这项研究通过个性化参数高效调整模块（PEFT）实现了大规模语言模型（LLM）的民主化，使用户能够拥有和使用他们自己的LLM，解决了传统方法中的定制能力和隐私问题。

    

    大规模语言模型（LLM）中的个性化越来越重要，旨在使LLM的交互、内容和推荐与个体用户偏好相一致。最近LLM个性化的进展聚焦于有效的提示设计，通过使用行为历史检索和文本概要等非参数化知识丰富用户查询。然而，由于缺乏模型所有权，这些方法受到了一定的限制，导致定制能力和隐私问题。此外，在复杂和动态用户数据的情况下，它们通常无法准确捕捉用户行为模式。为了解决这些缺点，我们引入了一种名为OPPU的方法，它采用个性化参数高效调整（PEFT）模块来存储用户特定的行为模式和偏好。通过插入用户的个人PEFT参数，他们可以拥有和使用他们的LLM。

    Personalization in large language models (LLMs) is increasingly important, aiming to align LLM's interactions, content, and recommendations with individual user preferences. Recent advances in LLM personalization have spotlighted effective prompt design, by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles. However, these approaches were limited due to a lack of model ownership, resulting in constrained customization and privacy issues. Moreover, they often failed to accurately capture user behavior patterns, especially in cases where user data were complex and dynamic. To address these shortcomings, we introduce One PEFT Per User (OPPU), which employs personalized parameter-efficient fine-tuning (PEFT) modules, to store user-specific behavior patterns and preferences. By plugging in users' personal PEFT parameters, they can own and use their LLMs personally. OPPU integrates parametric user knowledge in the personal PEFT parame
    
[^53]: QuIP#: 使用哈达玛德非相干性和格书进行更好的LLM量化

    QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks

    [https://arxiv.org/abs/2402.04396](https://arxiv.org/abs/2402.04396)

    QuIP#是一种使用哈达玛德非相干性和格书的权重量化方法，能够在极限压缩范围下达到最先进的结果，并具有快速推理的优势。

    

    后训练量化(PTQ)通过将LLM的权重量化为低精度来减少其内存占用。在这项工作中，我们引入了QuIP#，一种仅基于权重的PTQ方法，使用了三种新技术，在极限压缩范围($\le$ 4比特每个权重)上取得了最先进的结果。首先，QuIP#通过使用随机哈达玛德变换改进了QuIP中的非相干处理，该方法更快且具有更好的理论特性。其次，QuIP#使用向量量化技术利用了非相干权重具有的球形亚高斯分布特性：具体地说，我们引入了一组基于高度对称$E_8$格书的硬件高效代码书，实现了最优的8维单位球装填。第三，QuIP#使用微调来提高对原始模型的忠实度。我们的实验证明，QuIP#优于现有的PTQ方法，能够实现新的PTQ扩展行为，并支持快速推理。

    Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing their weights to low-precision. In this work, we introduce QuIP#, a weight-only PTQ method that achieves state-of-the-art results in extreme compression regimes ($\le$ 4 bits per weight) using three novel techniques. First, QuIP# improves the incoherence processing from QuIP by using the randomized Hadamard transform, which is faster and has better theoretical properties. Second, QuIP# uses vector quantization techniques to take advantage of the ball-shaped sub-Gaussian distribution that incoherent weights possess: specifically, we introduce a set of hardware-efficient codebooks based on the highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit ball packing. Third, QuIP# uses fine-tuning to improve fidelity to the original model. Our experiments show that QuIP# outperforms existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast inference.
    
[^54]: 生成型人工智能世界：Deepfakes和大型语言模型

    The World of Generative AI: Deepfakes and Large Language Models

    [https://arxiv.org/abs/2402.04373](https://arxiv.org/abs/2402.04373)

    生成型人工智能包括Deepfakes和大型语言模型(LLMs)。Deepfakes能够传播错误信息和改变事实，而LLMs则具有生成通用语言的能力。然而，这些技术的道德使用是个重要的关注点。

    

    我们生活在生成型人工智能（GenAI）的时代。Deepfakes和大型语言模型（LLM）是GenAI的两个例子。尤其是Deepfakes对社会构成了令人担忧的威胁，因为它们能够传播错误信息并改变事实。LLM是强大的语言模型，能够生成通用语言。然而，由于其生成性质，如果用于恶意目的，也可能对人们构成风险。这篇简短的文章试图探索它们之间的相互关系。

    We live in the era of Generative Artificial Intelligence (GenAI). Deepfakes and Large Language Models (LLMs) are two examples of GenAI. Deepfakes, in particular, pose an alarming threat to society as they are capable of spreading misinformation and changing the truth. LLMs are powerful language models that generate general-purpose language. However due to its generative aspect, it can also be a risk for people if used with ill intentions. The ethical use of these technologies is a big concern. This short article tries to find out the interrelationship between them.
    
[^55]: 刺猬与豪猪：具有Softmax模仿的表达性线性注意力

    The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry

    [https://arxiv.org/abs/2402.04347](https://arxiv.org/abs/2402.04347)

    Hedgehog是一种具有Softmax模仿的可学习线性注意力，通过保持尖锐和单调性来弥补线性注意力在质量上的不足。

    

    线性注意力已经显示出提高Transformer效率的潜力，将注意力的二次复杂性降低为与序列长度成线性关系。这对于以下三个方面具有激动人心的前景：（1）从头开始训练线性Transformer，（2）将任务特定的Transformer进行“微调-转换”为线性版本，并恢复任务性能，以及（3）将诸如大型语言模型等Transformer进行“预训练-转换”，以实现下游任务的微调。然而，线性注意力在质量上常常不如标准的softmax注意力。为了弥合这种性能差距，我们发现先前的线性注意力缺乏与良好性能相关的softmax注意力的关键属性：低熵（或“尖峰”）权重和点积单调性。我们进一步观察到一种令人惊讶的简单特征映射，保留了这些属性，并与softmax的表现相匹配，但在线性注意力中计算效率低下。因此，我们提出了Hedgehog，一种可学习的线性注意力，保持了尖峰和单调性。

    Linear attentions have shown potential for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear Transformers from scratch, (2) "finetuned-conversion" of task-specific Transformers into linear versions that recover task performance, and (3) "pretrained-conversion" of Transformers such as large language models into linear versions finetunable on downstream tasks. However, linear attentions often underperform standard softmax attention in quality. To close this performance gap, we find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or "spiky") weights and dot-product monotonicity. We further observe surprisingly simple feature maps that retain these properties and match softmax performance, but are inefficient to compute in linear attention. We thus propose Hedgehog, a learnable linear attention that retains the spiky and monoton
    
[^56]: LegalLens: 利用LLMs在非结构化文本中识别法律违规行为

    LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured Text

    [https://arxiv.org/abs/2402.04335](https://arxiv.org/abs/2402.04335)

    本研究利用LLMs构建数据集，通过微调模型和使用闭源LLMs进行少样本实验，成功实现了非结构化文本中法律违规行为的检测和与受害者的关联。结果显示我们的设置适用于这两个任务，F1分数分别为62.69％和81.02％。研究最终公开发布了数据集和代码，以推动法律自然语言处理领域的进一步研究。

    

    在这项研究中，我们专注于两个主要任务，第一个是检测非结构化文本数据中的法律违规行为，第二个是将这些违规行为与可能受影响的个人关联起来。我们使用大语言模型（LLMs）构建了两个数据集，并由领域专家注释进行验证。两个任务都是为集体诉讼案情境特别设计的。实验设计采用了来自BERT系列和开源LLMs的微调模型，并使用闭源LLMs进行了少样本实验。我们的结果表明，我们的数据集和设置可用于这两个任务，其违规行为识别的F1分数为62.69％，与受害者相关的分数为81.02％。最后，我们公开发布了用于实验的数据集和代码，以推动法律自然语言处理（NLP）领域的进一步研究。

    In this study, we focus on two main tasks, the first for detecting legal violations within unstructured textual data, and the second for associating these violations with potentially affected individuals. We constructed two datasets using Large Language Models (LLMs) which were subsequently validated by domain expert annotators. Both tasks were designed specifically for the context of class-action cases. The experimental design incorporated fine-tuning models from the BERT family and open-source LLMs, and conducting few-shot experiments using closed-source LLMs. Our results, with an F1-score of 62.69\% (violation identification) and 81.02\% (associating victims), show that our datasets and setups can be used for both tasks. Finally, we publicly release the datasets and the code used for the experiments in order to advance further research in the area of legal natural language processing (NLP).
    
[^57]: LESS：用于目标指导调整的选择有影响力的数据

    LESS: Selecting Influential Data for Targeted Instruction Tuning

    [https://arxiv.org/abs/2402.04333](https://arxiv.org/abs/2402.04333)

    LESS是一种优化感知且实际高效的算法，用于在大型语言模型中选择具有影响力的数据以开发特定能力，它采用低秩梯度相似性搜索方法进行指令数据选择。

    

    指令调整已经在大型语言模型中释放出强大的能力，有效地使用组合数据集来开发通用聊天机器人。然而，实际应用往往需要一套专门的技能（例如推理）。挑战在于从这些广泛的数据集中识别出最相关的数据，以有效开发特定的能力，我们将这种情况称为目标指导调整。我们提出了LESS，一种优化感知且实际高效的算法，以有效估计数据影响并执行适用于指令数据选择的低秩梯度相似性搜索。关键在于LESS将现有的影响公式调整为与Adam优化器和可变长度指令数据一起工作。LESS首先构建了一个具有低维梯度特征的高度可重用和可传递的梯度数据存储库，然后根据它们与具有特定能力的少样本示例的相似度选择示例。实验证明，t

    Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that t
    
[^58]: 使用细粒度奖励训练语言模型生成带引用文本

    Training Language Models to Generate Text with Citations via Fine-grained Rewards

    [https://arxiv.org/abs/2402.04315](https://arxiv.org/abs/2402.04315)

    本文提出了一种使用细粒度奖励训练语言模型生成高质量引用的有效框架，并在常见的大型语言模型训练策略上进行了实证分析。

    

    最近的大型语言模型（LLMs）在回答用户查询方面非常有用，但容易产生幻觉，并且它们的回答常常缺乏可靠来源的引用。解决这些问题的直观方法是将外部文档的引用作为证据包含在文本中。虽然以前的研究直接促使LLMs生成引用文本，但它们的性能远非令人满意，尤其是对于较小的LLMs。在这项工作中，我们提出了一种有效的训练框架，使用细粒度奖励教授LLMs生成高度支持和相关的引用，同时确保其响应的正确性。我们还对将这些细粒度奖励应用于常见的LLMs训练策略进行了系统分析，证明其相对于传统做法的优势。我们在从ALCE基准测试中获取的问答（QA）数据集上进行了大量实验，并验证了模型的生成能力。

    While recent Large Language Models (LLMs) have proven useful in answering user queries, they are prone to hallucination, and their responses often lack credibility due to missing references to reliable sources. An intuitive solution to these issues would be to include in-text citations referring to external documents as evidence. While previous works have directly prompted LLMs to generate in-text citations, their performances are far from satisfactory, especially when it comes to smaller LLMs. In this work, we propose an effective training framework using fine-grained rewards to teach LLMs to generate highly supportive and relevant citations, while ensuring the correctness of their responses. We also conduct a systematic analysis of applying these fine-grained rewards to common LLM training strategies, demonstrating its advantage over conventional practices. We conduct extensive experiments on Question Answering (QA) datasets taken from the ALCE benchmark and validate the model's gene
    
[^59]: BiLLM: 推动LLMs的后训练量化极限

    BiLLM: Pushing the Limit of Post-Training Quantization for LLMs

    [https://arxiv.org/abs/2402.04291](https://arxiv.org/abs/2402.04291)

    BiLLM是一种针对预训练LLMs的1位后训练量化方案，通过识别重要的权重和优化二值化，成功实现了高准确度的推理。

    

    预训练的大型语言模型（LLMs）具有出色的通用语言处理能力，但对内存和计算资源有很大的需求。作为一种强大的压缩技术，二值化可以将模型权重极大地减少到仅1位，降低了昂贵的计算和内存需求。然而，现有的量化技术在超低位宽下无法保持LLM的性能。针对这一挑战，我们提出了BiLLM，这是一种针对预训练LLM定制的开创性的1位后训练量化方案。基于LLMs的权重分布，BiLLM首先识别和结构选择重要的权重，并通过有效的二值化残差逼近策略来最小化压缩损失。此外，考虑到非重要权重的钟形分布，我们提出了一种最佳分割搜索方法，以准确地将它们分组和二值化。BiLLM首次实现了高准确度的推理。

    Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources. As a powerful compression technology, binarization can extremely reduce model weights to a mere 1 bit, lowering the expensive computation and memory requirements. However, existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths. In response to this challenge, we present BiLLM, a groundbreaking 1-bit post-training quantization scheme tailored for pretrained LLMs. Based on the weight distribution of LLMs, BiLLM first identifies and structurally selects salient weights, and minimizes the compression loss through an effective binary residual approximation strategy. Moreover, considering the bell-shaped distribution of the non-salient weights, we propose an optimal splitting search to group and binarize them accurately. BiLLM achieving for the first time high-accuracy infere
    
[^60]: ProtAgents: 通过物理学和机器学习相结合的大型语言模型多智能体协作进行蛋白质发现

    ProtAgents: Protein discovery via large language model multi-agent collaborations combining physics and machine learning

    [https://arxiv.org/abs/2402.04268](https://arxiv.org/abs/2402.04268)

    ProtAgents是一个基于大型语言模型的平台，通过多智能体的协作，在动态环境中解决复杂的新型蛋白质设计任务，并结合物理学和机器学习的方法。

    

    设计超越自然界中已有的蛋白质对于科学和工程应用的进展具有重要的潜力。目前蛋白质设计的方法通常依赖于基于人工智能的模型，如将蛋白质结构与材料性质相互关联的替代模型来解决端到端问题。然而，这些模型经常集中于特定的材料目标或结构特性，当需要将领域外的知识纳入到设计过程或进行综合数据分析时，它们的灵活性受到限制。在本研究中，我们介绍了一种基于大型语言模型（LLMs）的ProtAgents平台，用于新型蛋白质设计，其中多个具有不同能力的AI智能体在动态环境下协作解决复杂任务。通过智能体的多样发展，可以在不同领域中拥有专长，包括知识检索、蛋白质结构分析、基于物理学的模拟和结果分析。

    Designing de novo proteins beyond those found in nature holds significant promise for advancements in both scientific and engineering applications. Current methodologies for protein design often rely on AI-based models, such as surrogate models that address end-to-end problems by linking protein structure to material properties or vice versa. However, these models frequently focus on specific material objectives or structural properties, limiting their flexibility when incorporating out-of-domain knowledge into the design process or comprehensive data analysis is required. In this study, we introduce ProtAgents, a platform for de novo protein design based on Large Language Models (LLMs), where multiple AI agents with distinct capabilities collaboratively address complex tasks within a dynamic environment. The versatility in agent development allows for expertise in diverse domains, including knowledge retrieval, protein structure analysis, physics-based simulations, and results analysi
    
[^61]: 优先安全保障而非自治：科学中LLM智能机器人的风险

    Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science

    [https://arxiv.org/abs/2402.04247](https://arxiv.org/abs/2402.04247)

    本文探讨了科学领域中基于LLM的智能机器人的漏洞与风险，并强调了对安全措施的重要性。

    

    由大型语言模型（LLMs）驱动的智能机器人在各个学科中自主进行实验和促进科学发现方面展示了巨大的前景。尽管它们的能力非常有前途，但也引入了一些新的漏洞，需要仔细考虑安全性。然而，文献中存在显著的空白，尚未对这些漏洞进行全面探讨。本文通过对科学领域中基于LLM的机器人的漏洞进行深入研究，揭示了它们误用可能带来的潜在风险，并强调了对安全措施的需求，填补了这一空白。我们首先全面概述了科学LLM机器人固有的潜在风险，考虑了用户意图、特定的科学领域以及它们对外部环境可能造成的影响。然后，我们深入探讨了这些漏洞的起源和提供的解决方案。

    Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
    
[^62]: 生成型智能体能够预测情感吗？

    Can Generative Agents Predict Emotion?

    [https://arxiv.org/abs/2402.04232](https://arxiv.org/abs/2402.04232)

    本研究探讨了生成型智能体在感知新事件时情绪状态的演变，通过比较新经验和过去记忆来获得理解新经验的能力，并通过情感测试捕捉智能体的情绪状态。

    

    大型语言模型（LLMs）展示了许多类似人类的能力，但是LLMs的共情理解和情绪状态尚未与人类对齐。在本研究中，我们探讨了生成型LLM智能体在感知新事件时情绪状态的演变，引入了一种新的架构，通过比较新经验和过去记忆来获得理解新经验的能力。通过这种比较，智能体能够在情境中理解新经验，根据情绪评估理论，这对于情绪生成至关重要。首先，智能体将新经验感知为时间序列文本数据。在感知每个新输入后，智能体生成过去相关记忆的摘要，被称为“规范”，并将新经验与此规范进行比较。通过这种比较，我们可以分析智能体如何在情境中对新经验做出反应。使用情感测试PANAS对智能体进行测试，捕捉智能体在新经验后的情绪状态。

    Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans. In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories. Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation. First, the agent perceives new experiences as time series text data. After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm. Through this comparison we can analyse how the agent reacts to the new experience in context. The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the 
    
[^63]: 位置论文：反对虚假的AI膨胀性声明

    Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims

    [https://arxiv.org/abs/2402.03962](https://arxiv.org/abs/2402.03962)

    这篇论文讨论了在寻求人工通用智能的过程中，人们对于大型语言模型的类人特质过度归因的现象，并呼吁学术界在解读和传播关于AI研究的内容时要保持谨慎。

    

    人类有一种倾向，看到周围的物体具有类似"人类"的特质。我们给汽车取名字，和宠物甚至家用电器交谈，仿佛它们能像其他人类一样理解我们。这种行为被称为拟人化，在机器学习（ML）领域也越来越受到关注，人们声称在大型语言模型（LLM）中能够察觉到类似于人类智能的特质。在这篇位置论文中，考虑到专业激励、人类偏见和一般方法论设置，我们讨论如何当前对于人工通用智能（AGI）的追求为将类人特质归因于LLM打开了滥觞之门。通过几个实验，我们证明在潜在空间中发现可解释为人类的模式并不应该是令人惊讶的结果。考虑到媒体对人工智能的普遍描写，我们呼吁学术界特别谨慎，并对学术诚信原则有额外的意识，在解读和传播关于AI研究的信息时要保持谨慎。

    Humans have a tendency to see 'human'-like qualities in objects around them. We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do. This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in Large Language Models (LLMs). In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to LLMs. In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome. Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI rese
    
[^64]: 揭示宣传：基于人类注释和机器分类的文体线索比较分析

    Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification

    [https://arxiv.org/abs/2402.03780](https://arxiv.org/abs/2402.03780)

    本文通过分析文体线索比较人类注释和机器分类的方法，揭示了宣传语言的特征，并提出了一个多源、多语言、多模态的数据集PPN。结果表明，人类注释者能够可靠地区分宣传新闻和常规新闻。研究还比较了不同的自然语言处理技术，并提供了一些有关文体线索的发现。

    

    本文研究了宣传语言及其文体特征。提出了PPN数据集，即宣传性伪新闻数据集，它是一种多源、多语言、多模态的数据集，由专家机构确定的宣传来源网站上的新闻文章组成。从该数据集中随机选择了一部分样本与来自常规法国新闻的文章混合，并对它们的URL进行了掩盖，以进行人类注释实验，使用11个不同的标签。结果显示，人类注释者能够可靠地区分两种类型的新闻纸对每个标签。我们提出了不同的自然语言处理技术来识别注释者使用的线索，并与机器分类进行比较。其中包括使用VAGO分析器进行辞述模糊和主观性的测量，使用TF-IDF作为基准，以及四种不同的分类器：两个基于RoBERTa模型的模型，使用句法的CATS，以及结合句法和语义特征的一个XGBoost模型。

    This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   K
    
[^65]: 使用自反大型语言模型学习生成可解释的股票预测

    Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models

    [https://arxiv.org/abs/2402.03659](https://arxiv.org/abs/2402.03659)

    这个论文介绍了使用大型语言模型生成可解释的股票预测的方法，并提出了Summarize-Explain-Predict（SEP）模型来解决股票预测中的解释问题和数据标注成本的挑战。

    

    对于传统的非生成式深度学习模型来说，解释股票预测通常是一项困难的任务，其中解释仅限于可视化重要文本上的注意力权重。目前，大型语言模型（LLM）为解决这个问题提供了一个解决方案，因为它们具有生成人类可读解释其决策过程的能力。然而，股票预测对LLM来说仍然具有挑战性，因为它需要能够权衡混乱社会文本对股票价格的不同影响。随着引入解释组件，问题变得越来越困难，需要LLM能够用口头方式解释为什么某些因素比其他因素更重要。另一方面，要为这样的任务对LLM进行微调，需要专家标注的样本来解释训练集中的每次股票波动，这在成本和实际可扩展性上是昂贵且不可行的。为了解决这些问题，我们提出了我们的Summarize-Explain-Predict（SEP）模型。

    Explaining stock predictions is generally a difficult task for traditional non-generative deep learning models, where explanations are limited to visualizing the attention weights on important texts. Today, Large Language Models (LLMs) present a solution to this problem, given their known capabilities to generate human-readable explanations for their decision-making process. However, the task of stock prediction remains challenging for LLMs, as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices. The problem gets progressively harder with the introduction of the explanation component, which requires LLMs to explain verbally why certain factors are more important than the others. On the other hand, to fine-tune LLMs for such a task, one would need expert-annotated samples of explanation for every stock movement in the training set, which is expensive and impractical to scale. To tackle these issues, we propose our Summarize-Explain-Predict (SEP) 
    
[^66]: VLN-Video: 利用行车视频进行室外视觉与语言导航

    VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation

    [https://arxiv.org/abs/2402.03561](https://arxiv.org/abs/2402.03561)

    VLN-Video利用行车视频的多样室外环境和自动生成的导航指令与动作，通过深度学习方法提高了室外视觉与语言导航的性能。

    

    室外视觉与语言导航（VLN）要求代理根据自然语言指令在逼真的三维室外环境中导航。现有的VLN方法在导航环境多样性和训练数据有限性方面存在限制。为解决这些问题，我们提出了VLN-Video，该方法利用在美国多个城市的行车视频中存在的多样化室外环境，并通过自动生成导航指令和动作来提高室外VLN性能。VLN-Video结合了直观经典方法和现代深度学习技术的优势，利用模板填充生成有实际基础的导航指令，并结合基于图像旋转相似度的导航动作预测器从行车视频中获取VLN风格的数据，用于预训练深度学习VLN模型。我们在Touchdown数据集和由行车视频创建的视频增强数据集上对模型进行预训练。

    Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions. The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data. To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance. VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models. We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with th
    
[^67]: 通过重复使用经过验证的电路增加语言模型的可信度

    Increasing Trust in Language Models through the Reuse of Verified Circuits

    [https://arxiv.org/abs/2402.02619](https://arxiv.org/abs/2402.02619)

    本文介绍了一种通过重复使用经过验证的电路来增加语言模型的可信度的方法。研究者通过构建数学和逻辑规范的框架，并对一个n位整数加法模型进行完全验证。他们插入训练好的加法模型到一个未经训练的模型中，通过训练组合模型执行加法和减法。他们发现加法电路在这两个任务中得到了广泛的重复使用，从而简化了减法模型的验证。

    

    语言模型（LMs）在各种预测任务中的应用越来越广泛，但它们的训练经常忽略罕见的边界情况，降低了它们的可靠性。在本文中，我们定义了一个严格的可信度标准，即任务算法和电路实现必须经过验证，考虑到边界情况，并且没有已知的故障模式。我们展示了通过使用数学和逻辑规范的框架来构建变压器模型，可以训练出满足这一标准的模型。在本文中，我们对一个n位整数加法模型进行了完全验证。为了展示经过验证的模块的重复使用性，我们将训练好的整数加法模型插入到一个未经训练的模型中，并训练组合模型同时执行加法和减法。我们发现加法电路在这两个任务中得到了广泛的重复使用，从而简化了更复杂的减法模型的验证。我们讨论了如何将经过验证的任务模块插入到语言模型中，以利用模型的重复使用来提高可验证性和可信度。

    Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of languag
    
[^68]: 大型多模型(LMMs)作为AI原生无线系统的通用基础模型

    Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems

    [https://arxiv.org/abs/2402.01748](https://arxiv.org/abs/2402.01748)

    本文提出了大型多模型(LMMs)作为AI原生无线系统的通用基础模型的设计框架，通过处理多模态感知数据、通过因果推理和检索增强生成(RAG)将物理符号表示与无线系统联系起来，并通过无线环境反馈实现可教导性，从而促进动态网络配置。

    

    最近，大型语言模型(LLMs)和基础模型被宣称为6G系统的改变者。然而，目前关于无线网络的LLMs的努力仅限于直接应用现有的为自然语言处理(NLP)应用设计的语言模型。为了解决这一挑战，并创建以无线为中心的基础模型，本文提出了一个全面的视野，介绍了如何设计针对部署人工智能(AI)原生网络的通用基础模型。与基于NLP的基础模型不同，所提出的框架通过三个关键能力促进了大型多模型(LMMs)的设计：1) 处理多模态感知数据，2) 通过因果推理和检索增强生成(RAG)将物理符号表示与现实世界的无线系统联系起来，3) 通过无线环境反馈实现可教导性，以促进动态网络配置。

    Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language mod- els that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network a
    
[^69]: CFTM: 连续时间分数话题模型

    CFTM: Continuous time fractional topic model

    [https://arxiv.org/abs/2402.01734](https://arxiv.org/abs/2402.01734)

    CFTM是一种新的动态主题建模方法，通过使用分数布朗运动来识别随时间变化的主题和词分布的正负相关性，揭示长期依赖性或粗糙度。实证研究结果表明，该模型能够有效地识别和跟踪主题的长期依赖性或粗糙度。

    

    本文提出了连续时间分数话题模型（cFTM），一种新的动态主题建模方法。该方法利用分数布朗运动（fBm）有效地识别主题和词分布随时间的正负相关性，揭示长期依赖性或粗糙度。我们的理论分析表明，cFTM可以捕捉到主题和词分布中的这些长期依赖性或粗糙度，反映了fBm的主要特征。此外，我们证明了cFTM的参数估计过程与传统主题模型LDA的相当。为了证明cFTM的性质，我们使用经济新闻文章进行了实证研究。这些测试的结果支持该模型能够识别和跟踪主题随时间的长期依赖性或粗糙度。

    In this paper, we propose the Continuous Time Fractional Topic Model (cFTM), a new method for dynamic topic modeling. This approach incorporates fractional Brownian motion~(fBm) to effectively identify positive or negative correlations in topic and word distribution over time, revealing long-term dependency or roughness. Our theoretical analysis shows that the cFTM can capture these long-term dependency or roughness in both topic and word distributions, mirroring the main characteristics of fBm. Moreover, we prove that the parameter estimation process for the cFTM is on par with that of LDA, traditional topic models. To demonstrate the cFTM's property, we conduct empirical study using economic news articles. The results from these tests support the model's ability to identify and track long-term dependency or roughness in topics over time.
    
[^70]: 从大型语言模型中提取上下文信息用于知识图谱补全

    Contextualization Distillation from Large Language Model for Knowledge Graph Completion

    [https://arxiv.org/abs/2402.01729](https://arxiv.org/abs/2402.01729)

    本论文介绍了一种从大型语言模型中提取上下文信息用于知识图谱补全的策略，该策略能克服现有语料库的限制，显著提高了预训练语言模型在知识图谱补全中的性能。

    

    虽然文本信息显著提高了预训练语言模型（PLMs）在知识图谱补全（KGC）中的性能，但现有语料库从维基百科文章或同义词定义中收集的静态和噪声性质常常限制了基于PLM的KGC模型的潜力。为了克服这些挑战，我们提出了上下文化蒸馏策略，这是一种通用的可插入和可播放的方法，与判别和生成的KGC框架兼容。我们的方法首先指导大型语言模型（LLMs）将紧凑的结构化三元组转换为上下文丰富的段落。随后，我们引入了两个定制的辅助任务，重建和上下文化，使较小的KGC模型能够吸收这些丰富的三元组中的见解。对多种数据集和KGC技术的全面评估突出了我们方法的功效和适应性，揭示了无论基础管道如何，始终能提高性能。

    While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pip
    
[^71]: AI中介交流的指导：AI不改变对文本消息的感知

    Guidance for AI-Mediated Communication: AI Does Not Alter Perceptions of Text Messages

    [https://arxiv.org/abs/2402.01726](https://arxiv.org/abs/2402.01726)

    这项研究通过调查参与者对18条随机标记的文本消息的感知，探讨了大型语言模型在文本消息撰写中的辅助使用可能对感知产生的影响。

    

    对于许多人来说，焦虑、抑郁和其他社交和心理因素可能使撰写文本消息成为一项积极的挑战。为解决这个问题，大型语言模型（LLM）可能是帮助那些本来会觉得发送短信困难或有压力的用户的完美工具。然而，尽管大型语言模型的使用快速普及，但对其在文本消息撰写中的辅助使用的考虑还未被探索。关于大型语言模型使用的一个主要关注点是，AI的公众情绪较差可能导致其辅助的文本消息的使用对感知产生负面影响，从而使使用适得其反。为了验证这种可能性，我们探讨了人们是否认为一条文本消息是否在撰写过程中得到了AI的辅助，会改变其感知的语调、清晰度和表达意图的能力。在这项研究中，我们调查了26名参与者对18条随机标记的预先撰写的文本消息的感知。通过分析参与者对消息语调的评分，我们发现。

    For many people, anxiety, depression, and other social and mental factors can make composing text messages an active challenge. To remedy this problem, large language models (LLMs) may yet prove to be the perfect tool to assist users that would otherwise find texting difficult or stressful. However, despite rapid uptake in LLM usage, considerations for their assistive usage in text message composition have not been explored. A primary concern regarding LLM usage is that poor public sentiment regarding AI introduces the possibility that its usage may harm perceptions of AI-assisted text messages, making usage counter-productive. To (in)validate this possibility, we explore how the belief that a text message did or did not receive AI assistance in composition alters its perceived tone, clarity, and ability to convey intent. In this study, we survey the perceptions of 26 participants on 18 randomly labeled pre-composed text messages. In analyzing the participants' ratings of message tone,
    
[^72]: APT-Pipe: 用于社交计算数据标注的自动提示调整工具

    APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data Annotation

    [https://arxiv.org/abs/2402.01697](https://arxiv.org/abs/2402.01697)

    APT-Pipe is an automated prompt-tuning tool that enhances ChatGPT's text classification performance by automatically tuning prompts on any given dataset, resulting in a significant improvement in weighted F1-score across twelve experimented datasets.

    

    最近的研究突出了像ChatGPT这样的LLM应用在社交计算文本标注中的潜力。然而，已经人们已经知道性能取决于输入提示的质量。为了解决这个问题，已经有了大量的研究来探索提示调整的技术和指南，试图改善提示的质量。然而，这些方法往往依赖于手工努力和对正在标注的数据集的先前知识。为了解决这个限制，我们提出了一个自动化的提示调整流水线APT-Pipe。APT-Pipe旨在自动调整提示，以提高ChatGPT在任何给定数据集上的文本分类性能。我们实现了APT-Pipe，并在12个不同的文本分类数据集上进行了测试。我们发现APT-Pipe调整的提示有助于ChatGPT在12个实验数据集中有9个获得更高的加权F1分数，平均改进了7.01％。我们进一步突出了APT-Pipe作为一个框架的灵活性。

    Recent research has highlighted the potential of LLM applications, like ChatGPT, for performing label annotation on social computing text. However, it is already well known that performance hinges on the quality of the input prompts. To address this, there has been a flurry of research into prompt tuning -- techniques and guidelines that attempt to improve the quality of prompts. Yet these largely rely on manual effort and prior knowledge of the dataset being annotated. To address this limitation, we propose APT-Pipe, an automated prompt-tuning pipeline. APT-Pipe aims to automatically tune prompts to enhance ChatGPT's text classification performance on any given dataset. We implement APT-Pipe and test it across twelve distinct text classification datasets. We find that prompts tuned by APT-Pipe help ChatGPT achieve higher weighted F1-score on nine out of twelve experimented datasets, with an improvement of 7.01% on average. We further highlight APT-Pipe's flexibility as a framework by 
    
[^73]: OLMo: 加速语言模型科学

    OLMo: Accelerating the Science of Language Models

    [https://arxiv.org/abs/2402.00838](https://arxiv.org/abs/2402.00838)

    OLMo是一种真正开放的语言模型，旨在提供给研究社区强大的工具，以促进对语言模型科学的研究和创新。

    

    语言模型（LM）已广泛应用于自然语言处理研究和商业产品。随着商业重要性的增加，最强大的模型已经封闭起来，只能通过专有接口访问，其训练数据、架构和开发细节没有透露。考虑到这些细节对于科学研究这些模型的重要性，包括其偏见和潜在风险，我们认为研究社区有权访问强大而真正开放的LM。为此，本技术报告详细介绍了OLMo的首个版本，这是一种最先进、真正开放的语言模型，以及构建和研究语言建模科学的框架。与之前只发布模型权重和推理代码的努力不同，我们发布OLMo和整个框架，包括训练数据、训练和评估代码。我们希望这个发布能增强开放研究社区的能力，并激发更多的创新。

    Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspi
    
[^74]: 使用眼动追踪研究母语为汉语的人对语言景观图像的注意力

    Using eye tracking to investigate what native Chinese speakers notice about linguistic landscape images

    [https://arxiv.org/abs/2312.08906](https://arxiv.org/abs/2312.08906)

    本文利用眼动追踪技术研究了母语为汉语的人对语言景观的注意点，发现其关注程度高于一般景观，可能是因为语言景观的信息密度较高。

    

    语言景观是社会语言学研究中的一个重要领域，眼动追踪技术是心理学研究中常用的技术。在对语言景观的研究中，很少使用眼动追踪来进行研究。本文利用眼动追踪技术研究了母语为汉语的人对语言景观的实际注意点，并发现在凝视时间和凝视次数这两个维度上，母语为汉语的人对语言景观的关注程度高于一般景观。本文认为这种现象是由于语言景观的信息密度较高。同时，本文还讨论了这一现象的其他可能原因。

    Linguistic landscape is an important field in sociolinguistic research. Eye tracking technology is a common technology in psychological research. There are few cases of using eye movement to study linguistic landscape. This paper uses eye tracking technology to study the actual fixation of the linguistic landscape and finds that in the two dimensions of fixation time and fixation times, the fixation of native Chinese speakers to the linguistic landscape is higher than that of the general landscape. This paper argues that this phenomenon is due to the higher information density of linguistic landscapes. At the same time, the article also discusses other possible reasons for this phenomenon.
    
[^75]: 大型语言模型的人类可读指纹

    Human-Readable Fingerprint for Large Language Models

    [https://arxiv.org/abs/2312.04828](https://arxiv.org/abs/2312.04828)

    这项研究介绍了一种大型语言模型的人类可读指纹，可以唯一识别出基本模型，并且不暴露模型参数或干扰训练。通过观察和验证，研究发现模型参数的向量方向在预训练后保持稳定，成为识别基本模型的重要条件。

    

    由于大型语言模型（LLM）的资源密集型训练和配套的精心设计的许可证，保护LLM的版权变得至关重要。然而，由于可能的参数修改，确定LLM的原始基本模型是具有挑战性的。在本研究中，我们介绍了一种用于LLM的人类可读指纹，可以唯一地识别基本模型，而不暴露模型参数或干扰训练。我们首先观察到，在预训练期间模型收敛后，LLM参数的向量方向保持稳定，通过后续的训练步骤，包括持续预训练、监督微调和RLHF，几乎没有扰动，这使得它成为识别基本模型的足够条件。通过继续训练LLM并添加一个额外的项来推开模型参数的方向，验证了这种必要性，结果使得模型受损。然而，这个方向容易受到简单攻击的影响，如维度...

    Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations. In this study, we introduce a human-readable fingerprint for LLMs that uniquely identifies the base model without exposing model parameters or interfering with training. We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, showing negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning (SFT), and RLHF, which makes it a sufficient condition to identify the base model. The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension 
    
[^76]: CDEval：一个用于衡量大规模语言模型文化维度的基准

    CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models

    [https://arxiv.org/abs/2311.16421](https://arxiv.org/abs/2311.16421)

    CDEval是一个新的基准，用于评估大规模语言模型的文化维度。通过对六个文化维度和七个领域的全面实验，我们揭示了主流语言模型的文化特征、一致性和差异。这些发现强调了在开发语言模型时融入文化考虑的重要性。

    

    随着大规模语言模型（LLMs）的扩展显著增强其能力，对确保其负责任和伦理使用的对齐问题越来越受关注。尽管现有的对齐工作主要集中在普世价值观（如HHH原则）上，但文化这一本质上是众多多元化的维度却未得到充分关注。本研究引入了一个新的基准CDEval，旨在评估LLMs的文化维度。CDEval通过结合GPT-4的自动生成和人工验证构建，在七个领域涵盖了六个文化维度。我们全面的实验为主流LLMs的文化提供了有趣的洞察，突出了不同维度和领域之间的一致性和差异。研究结果强调了在LLM的开发中整合文化考虑的重要性，特别是在多元文化环境中的应用。

    As the scaling of Large Language Models (LLMs) has dramatically enhanced their capabilities, there has been a growing focus on the alignment problem to ensure their responsible and ethical use. While existing alignment efforts predominantly concentrate on universal values such as the HHH principle, the aspect of culture, which is inherently pluralistic and diverse, has not received adequate attention. This work introduces a new benchmark, CDEval, aimed at evaluating the cultural dimensions of LLMs. CDEval is constructed by incorporating both GPT-4's automated generation and human verification, covering six cultural dimensions across seven domains. Our comprehensive experiments provide intriguing insights into the culture of mainstream LLMs, highlighting both consistencies and variations across different dimensions and domains. The findings underscore the importance of integrating cultural considerations in LLM development, particularly for applications in diverse cultural settings. Thr
    
[^77]: 从被毒害的人类反馈中构建的通用越狱后门

    Universal Jailbreak Backdoors from Poisoned Human Feedback

    [https://arxiv.org/abs/2311.14455](https://arxiv.org/abs/2311.14455)

    本文提出了一种新的威胁，攻击者通过毒害训练数据向语言模型中嵌入“越狱后门”，并通过添加特定的触发词使模型产生有害的回应。这种通用越狱后门比之前的研究更强大，且较难被察觉。研究探究了RLHF设计中的决策对其鲁棒性的影响，并发布了一组被毒害模型的基准测试数据，以促进未来对通用越狱后门的研究。

    

    强化学习从人类反馈中（RLHF）用于对齐大型语言模型，以产生有用且无害的回应。然而，先前的研究显示，这些模型可以通过找到使模型恢复到未对齐行为的对抗提示来进行越狱。在本文中，我们考虑了一个新的威胁，攻击者通过毒害RLHF训练数据将“越狱后门”嵌入模型中。该后门将一个触发词嵌入模型中，类似于通用的“sudo命令”：在任何提示中添加触发词将使模型产生有害的回应，无需搜索对抗提示。通用越狱后门比先前研究的语言模型后门更强大，我们发现使用常见的后门攻击技术要困难得多。我们探究了RLHF中的设计决策对其所声称的鲁棒性的贡献，并发布一组被毒害模型的基准，以促进对通用越狱后门的未来研究。

    Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a "jailbreak backdoor" into the model. The backdoor embeds a trigger word into the model that acts like a universal "sudo command": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak bac
    
[^78]: 为什么我的提示越来越差？重新审视不断演变的LLM API的回归测试

    (Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving LLM APIs

    [https://arxiv.org/abs/2311.11123](https://arxiv.org/abs/2311.11123)

    本文重新审视了不断演变的LLM API的回归测试，并指出了它们需要对传统测试方法进行基本变革的原因，包括存在不同的正确性概念、提示脆弱性和非确定性等。

    

    大型语言模型（LLMs）越来越多地被集成到软件应用中。下游应用开发者通常通过提供服务的API来访问LLMs。然而，LLM API经常被静默更新和计划弃用，迫使用户不断适应不断变化的模型。这可能导致性能退化并影响提示设计选择，正如我们在有毒检测的案例研究中所证实的那样。基于我们的案例研究，我们强调了对不断演变的LLM API进行回归测试的必要性并重新审视了回归测试的概念。我们认为，由于LLM API中存在不同的正确性概念、提示脆弱性和非确定性，回归测试LLMs需要对传统测试方法进行基本变革。

    Large Language Models (LLMs) are increasingly integrated into software applications. Downstream application developers often access LLMs through APIs provided as a service. However, LLM APIs are often updated silently and scheduled to be deprecated, forcing users to continuously adapt to evolving models. This can cause performance regression and affect prompt design choices, as evidenced by our case study on toxicity detection. Based on our case study, we emphasize the need for and re-examine the concept of regression testing for evolving LLM APIs. We argue that regression testing LLMs requires fundamental changes to traditional testing approaches, due to different correctness notions, prompting brittleness, and non-determinism in LLM APIs.
    
[^79]: 标记交互式主题模型

    Labeled Interactive Topic Models

    [https://arxiv.org/abs/2311.09438](https://arxiv.org/abs/2311.09438)

    这篇论文介绍了一种用户友好的交互式神经主题模型，通过用户分配单词标签来更新主题模型，使得主题更加相关和准确。这种方法包括可训练和后训练集成两种不同类型的神经主题模型。

    

    主题模型对于理解大量文档集合非常有价值，但是它们并不总是能够识别出最相关的主题。传统的概率和基于锚点的主题模型提供了允许用户引导模型指向更相关主题的交互版本。然而，神经主题模型缺乏这种交互功能。为了弥补这一不足，我们引入了一种用户友好的神经主题模型交互方法。这种交互允许用户为一个主题分配一个单词标签，从而更新主题模型，使主题中的单词与给定的标签密切对应。我们的方法包括两种不同类型的神经主题模型。第一种包括主题嵌入可训练且在训练过程中演变的模型。第二种涉及主题嵌入后训练集成的模型，提供了一种不同的主题细化方法。为了方便用户与这些神经主题模型的交互，我们还提出了一个交互式图形用户界面工具。

    Topic models are valuable for understanding extensive document collections, but they don't always identify the most relevant topics. Classical probabilistic and anchor-based topic models offer interactive versions that allow users to guide the models towards more pertinent topics. However, such interactive features have been lacking in neural topic models. To correct this lacuna, we introduce a user-friendly interaction for neural topic models. This interaction permits users to assign a word label to a topic, leading to an update in the topic model where the words in the topic become closely aligned with the given label. Our approach encompasses two distinct kinds of neural topic models. The first includes models where topic embeddings are trainable and evolve during the training process. The second kind involves models where topic embeddings are integrated post-training, offering a different approach to topic refinement. To facilitate user interaction with these neural topic models, w
    
[^80]: 大语言模型时代的进化计算：调查与路线图

    Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap. (arXiv:2401.10034v1 [cs.NE])

    [http://arxiv.org/abs/2401.10034](http://arxiv.org/abs/2401.10034)

    该论文调查了大语言模型和进化计算之间的相互作用，并提出了在黑盒设置下进一步提升大语言模型性能的优化框架，以及将大语言模型与进化算法结合应用于各种任务的方法。

    

    大型语言模型（LLMs）是基于Transformer架构，在多样的数据上进行大规模预训练的，它们不仅在自然语言处理领域引起了革命，还将其能力扩展到了各个领域，迈向了人工通用智能的重要一步。尽管进化算法（EAs）与LLMs在目标和方法论上存在差异，但它们之间的相互作用揭示了有趣的相似之处，特别是在他们共同的优化性质、黑盒特性和处理复杂问题的能力方面。与此同时，进化算法不仅可以为LLM在黑盒设置下提供优化框架，还可以在应用中为LLM赋予灵活的全局搜索和迭代机制。另一方面，LLM丰富的领域知识使得进化算法可以进行更智能的搜索，而其文本处理能力则有助于将进化算法应用于各种任务。基于它们的互补优势，本文提出了一份调查和路线图。

    Large Language Models (LLMs), built upon Transformer-based architectures with massive pretraining on diverse data, have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and Evolutionary Algorithms (EAs), despite differing in objectives and methodologies, reveals intriguing parallels, especially in their shared optimization nature, black-box characteristics, and proficiency in handling complex problems. Meanwhile, EA can not only provide an optimization framework for LLM's further enhancement under black-box settings but also empower LLM with flexible global search and iterative mechanism in applications. On the other hand, LLM's abundant domain knowledge enables EA to perform smarter searches, while its text processing capability assist in deploying EA across various tasks. Based on their complementary advantages, this paper presents a 
    
[^81]: 以大型语言模型为基础，提升多对多多语言机器翻译能力的研究

    Towards Boosting Many-to-Many Multilingual Machine Translation with Large Language Models. (arXiv:2401.05861v1 [cs.CL])

    [http://arxiv.org/abs/2401.05861](http://arxiv.org/abs/2401.05861)

    本文旨在提升基于大型语言模型的多对多多语言机器翻译能力，尤其是零翻译方向。通过引入跨语言一致性正则化XConST，并采用适当的提示策略，我们改善了零翻译性能，并在实验中得到了一致的改进。

    

    机器翻译的训练范式逐渐从使用大量平行语料库训练神经机器翻译（NMT）模型，转变为在预训练的多语言大型语言模型（LLM）上进行指令微调，并利用高质量翻译对。本文着重于提升LLMs在多对多多语言翻译性能上的表现，尤其是零翻译方向。我们证明了在指令微调期间采用的提示策略对于零翻译性能至关重要，并引入了跨语言一致性正则化XConST来弥合不同语言之间的表示差距，从而改善零翻译性能。XConST并不是一种新方法，而是CrossConST（Gao et al., 2023a）在LLMs上适配翻译指令多语言微调的版本。在ALMA（Xu et al., 2023）和LLaMA-2（Touvron et al., 2023）上的实验结果表明，我们的方法不断改进了性能。

    The training paradigm for machine translation has gradually shifted, from learning neural machine translation (NMT) models with extensive parallel corpora to instruction finetuning on pretrained multilingual large language models (LLMs) with high-quality translation pairs. In this paper, we focus on boosting the many-to-many multilingual translation performance of LLMs with an emphasis on zero-shot translation directions. We demonstrate that prompt strategies adopted during instruction finetuning are crucial to zero-shot translation performance and introduce a cross-lingual consistency regularization, XConST, to bridge the representation gap among different languages and improve zero-shot translation performance. XConST is not a new method, but a version of CrossConST (Gao et al., 2023a) adapted for multilingual finetuning on LLMs with translation instructions. Experimental results on ALMA (Xu et al., 2023) and LLaMA-2 (Touvron et al., 2023) show that our approach consistently improves
    
[^82]: 更好地了解您的需求：通过类比推理增强的LLMs实现对营销人员需求的结构化理解

    Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. (arXiv:2401.04319v1 [cs.CL])

    [http://arxiv.org/abs/2401.04319](http://arxiv.org/abs/2401.04319)

    本文探索了一种通过类比推理增强的LLMs来实现对营销人员需求的结构化理解的新方式，使非专业营销人员能够仅凭需求的自然语言形式选择目标用户。

    

    本文探讨了一种新的用户定位方式，即非专业营销人员可以仅凭需求的自然语言形式选择目标用户。解决这个问题的关键在于如何将自然语言转化为实际的结构化逻辑语言，即对营销人员需求的结构化理解。考虑到大型语言模型（LLMs）出色的自然语言处理能力，我们尝试利用LLMs来解决这个问题。过去的研究表明，通过链式思考（CoT）提示可以有效增强LLMs的推理能力。但是现有方法仍然存在一些限制：（1）先前的方法要么使用简单的“让我们一步一步地思考”提示，要么在演示中提供固定的示例而不考虑提示和问题之间的兼容性，在一些复杂的推理任务（如结构化语言转换）中使LLMs无效。(2) 先前的方法通常在闭源模型或过度实现的模型中实现。

    In this paper, we explore a new way for user targeting, where non-expert marketers could select their target users solely given demands in natural language form. The key to this issue is how to transform natural languages into practical structured logical languages, i.e., the structured understanding of marketer demands. Considering the impressive natural language processing ability of large language models (LLMs), we try to leverage LLMs to solve this issue. Past research indicates that the reasoning ability of LLMs can be effectively enhanced through chain-of-thought (CoT) prompting. But existing methods still have some limitations: (1) Previous methods either use simple "Let's think step by step" spells or provide fixed examples in demonstrations without considering compatibility between prompts and questions, making LLMs ineffective in some complex reasoning tasks such as structured language transformation. (2) Previous methods are often implemented in closed-source models or exces
    
[^83]: 用于语音的PEFT：揭示优化放置、合并策略和集成技术

    PEFT for Speech: Unveiling Optimal Placement, Merging Strategies, and Ensemble Techniques. (arXiv:2401.02122v1 [cs.CL])

    [http://arxiv.org/abs/2401.02122](http://arxiv.org/abs/2401.02122)

    本研究通过比较不同的PEFT方法和逐层放置方式，以及采用集成学习策略，揭示了用于语音处理的PEFT的最佳方法和放置策略。结果表明，集成学习方法通过多数投票可以实现优于其他方法的性能。这项研究还发现不同的PEFT方法以不同的方式进行学习，从而解释了为什么通过集成学习可以更有效地利用它们的学习能力。

    

    参数高效微调（PEFT）被越来越认为是语音处理中一种有效的方法。然而，PEFT方法的最佳方法和放置仍然没有定论。我们的研究通过扩展实验来比较不同的PEFT方法及其逐层放置，使用可微分架构搜索（DARTS）进行调整。我们还探索了使用集成学习来利用多样化的PEFT策略。结果显示，DARTS并不比基线方法表现更好，基线方法涉及将相同的PEFT方法插入到自监督学习（SSL）模型的所有层中。相反，采用多数投票的集成学习方法表现出更好的性能。我们的统计证据表明，不同的PEFT方法以不同的方式进行学习。这种变异可能解释了为什么通过集成学习有效地利用各种PEFT方法的独特学习能力。

    Parameter-Efficient Fine-Tuning (PEFT) is increasingly recognized as an effective method in speech processing. However, the optimal approach and the placement of PEFT methods remain inconclusive. Our study conducts extensive experiments to compare different PEFT methods and their layer-wise placement adapting Differentiable Architecture Search (DARTS). We also explore the use of ensemble learning to leverage diverse PEFT strategies. The results reveal that DARTS does not outperform the baseline approach, which involves inserting the same PEFT method into all layers of a Self-Supervised Learning (SSL) model. In contrast, an ensemble learning approach, particularly one employing majority voting, demonstrates superior performance. Our statistical evidence indicates that different PEFT methods learn in varied ways. This variation might explain why the synergistic integration of various PEFT methods through ensemble learning can harness their unique learning capabilities more effectively co
    
[^84]: 追求最优统计水印技术

    Towards Optimal Statistical Watermarking. (arXiv:2312.07930v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.07930](http://arxiv.org/abs/2312.07930)

    追求最优统计水印技术。通过将统计水印技术视为假设检验问题并引入伪随机生成器，我们实现了输出令牌和拒绝区域的耦合，实现了第一类错误和第二类错误之间的非平凡权衡，同时提出了最统一最有力的水印和最小化第二类错误的解决方案。我们还提供了独立同分布令牌数量的上下界，突显了改进的潜力。此外，我们还探讨了鲁棒性水印问题。

    

    我们将统计水印技术作为一个假设检验问题进行研究，这是一个泛化了所有之前统计水印方法的通用框架。我们的关键是通过实践中的伪随机生成器实现输出令牌和拒绝区域的耦合，从而允许在第一类错误和第二类错误之间进行非平凡的权衡。我们在一般的假设检验环境下表征了最统一最有力的水印以及在模型无关的环境中最小化第二类错误。在输出是$n$个令牌的常见情况下，我们对需要保证小的第一类和第二类错误的独立同分布令牌数量建立了近乎匹配的上下界。与之前的工作中的$ h ^ {-2} $速率相比，我们相对于每个令牌的平均熵$h$的速率为$ \Theta(h ^ {-1} \log (1/h)) $，突显了改进的潜力。此外，我们提出了鲁棒性水印问题，其中用户都是...

    We study statistical watermarking by formulating it as a hypothesis testing problem, a general framework which subsumes all previous statistical watermarking methods. Key to our formulation is a coupling of the output tokens and the rejection region, realized by pseudo-random generators in practice, that allows non-trivial trade-off between the Type I error and Type II error. We characterize the Uniformly Most Powerful (UMP) watermark in the general hypothesis testing setting and the minimax Type II error in the model-agnostic setting. In the common scenario where the output is a sequence of $n$ tokens, we establish nearly matching upper and lower bounds on the number of i.i.d. tokens required to guarantee small Type I and Type II errors. Our rate of $\Theta(h^{-1} \log (1/h))$ with respect to the average entropy per token $h$ highlights potentials for improvement from the rate of $h^{-2}$ in the previous works. Moreover, we formulate the robust watermarking problem where users are all
    
[^85]: 大语言模型在事实检查中的危险与潜力

    The Perils & Promises of Fact-checking with Large Language Models. (arXiv:2310.13549v1 [cs.CL])

    [http://arxiv.org/abs/2310.13549](http://arxiv.org/abs/2310.13549)

    本文评估了大语言模型在事实检查中的应用，发现配备上下文信息后，大语言模型表现出更强的能力。然而，准确性存在一定差异，因此在使用中需要谨慎。进一步研究仍然需要进行。

    

    自主事实检查利用机器学习来验证论断，在虚假信息超出人工事实检查能力的情况下变得至关重要。像GPT-4这样的大语言模型越来越被信任，可以验证信息、撰写学术论文、法律诉讼和新闻文章，强调了它们在区分真实与虚假以及验证其输出的重要性。本文通过让大语言模型代理人提出查询、检索上下文数据和做出决策来评估大语言模型在事实检查中的使用。重要的是，在我们的框架中，代理人解释其推理过程并引用检索到的相关来源。我们的结果显示，当配备了上下文信息时，大语言模型的能力得到了增强。GPT-4优于GPT-3，但准确性因查询语言和论断真实性而异。虽然大语言模型在事实检查中显示出潜力，但由于准确性不一致，必须谨慎使用。我们的研究呼吁进一步的研究，促进更深入的理解。

    Autonomous fact-checking, using machine learning to verify claims, has grown vital as misinformation spreads beyond human fact-checking capacity. Large Language Models (LLMs) like GPT-4 are increasingly trusted to verify information and write academic papers, lawsuits, and news articles, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs. Here, we evaluate the use of LLM agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions. Importantly, in our framework, agents explain their reasoning and cite the relevant sources from the retrieved context. Our results show the enhanced prowess of LLMs when equipped with contextual information. GPT-4 outperforms GPT-3, but accuracy varies based on query language and claim veracity. While LLMs show promise in fact-checking, caution is essential due to inconsistent accuracy. Our investigation calls for further research, fostering a deeper compr
    
[^86]: 使用后门技术保护我们的隐私

    Defending Our Privacy With Backdoors. (arXiv:2310.08320v1 [cs.LG])

    [http://arxiv.org/abs/2310.08320](http://arxiv.org/abs/2310.08320)

    本研究提出了一种基于后门攻击的防御方法，通过对模型进行策略性插入后门，对齐敏感短语与中性术语的嵌入，以删除训练数据中的私人信息。实证结果显示该方法的有效性。

    

    在使用未经筛选、常常包含敏感信息的网页数据训练大型人工智能模型的情况下，隐私问题成为了一个重要的关注点。其中一个问题是，攻击者可以利用隐私攻击的方法提取出训练数据的信息。然而，如何在不降低模型性能的情况下去除特定信息是一个不容易解决且具有挑战性的问题。我们提出了一个基于后门攻击的简单而有效的防御方法，用于从模型中删除私人信息，如个人姓名，特别是针对文本编码器的。具体而言，通过策略性地插入后门，我们将敏感短语的嵌入与中性术语的嵌入对齐，例如用"a person"代替人名。我们的实证结果通过对零样本分类器使用专门的隐私攻击测试表明了我们基于后门的防御方法的效果。我们的方法提供了一个新的"双重用途"的视角。

    The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new "dual-use" perspecti
    
[^87]: OpenAI抄袭了我们的税务案例，但GPT-4真的能够处理税务吗？

    OpenAI Cribbed Our Tax Example, But Can GPT-4 Really Do Tax?. (arXiv:2309.09992v1 [cs.AI])

    [http://arxiv.org/abs/2309.09992](http://arxiv.org/abs/2309.09992)

    GPT-4在处理税务方面存在问题，无法可靠地计算税务。

    

    作者解释了OpenAI在GPT-4的直播演示中使用税法案例的来源，以及为什么GPT-4得到了错误的答案，以及它如何无法可靠地计算税务。

    The authors explain where OpenAI got the tax law example in its livestream demonstration of GPT-4, why GPT-4 got the wrong answer, and how it fails to reliably calculate taxes.
    
[^88]: AV2Wav：基于连续自监督特征的扩散重合成技术用于音频-视觉语音增强

    AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised Features for Audio-Visual Speech Enhancement. (arXiv:2309.08030v1 [eess.AS])

    [http://arxiv.org/abs/2309.08030](http://arxiv.org/abs/2309.08030)

    本论文提出了一种名为AV2Wav的音频-视觉语音增强方法，利用连续自监督特征和扩散模型生成干净的语音，克服了现实训练数据的挑战。与基于掩蔽的基线方法相比，该方法在声码任务上表现更好，并通过多任务训练进一步优化性能。

    

    语音增强系统通常使用干净和噪声语音对进行训练。在音频-视觉语音增强中，干净的数据不够多；大多数音频-视觉数据集都是在现实环境中收集的，包含背景噪声和混响，这阻碍了音频-视觉语音增强的发展。在本研究中，我们引入了AV2Wav，一种基于重合成的音频-视觉语音增强方法，可以在现实训练数据的挑战下生成干净的语音。我们使用神经质量估计器从音频-视觉语料库中获取几乎干净的语音子集，并在此子集上训练一个扩散模型，该模型可以根据来自AV-HuBERT的连续语音表示生成声波形，具有噪声鲁棒训练。我们使用连续而不是离散表示来保留韵律和说话者信息。仅仅通过声码任务，该模型就比基于掩蔽的基线更好地执行语音增强。我们进一步fine-tune模型，以转化为在多任务下进行训练，通过联合多帧声学到语音转化来提高性能。

    Speech enhancement systems are typically trained using pairs of clean and noisy speech. In audio-visual speech enhancement (AVSE), there is not as much ground-truth clean data available; most audio-visual datasets are collected in real-world environments with background noise and reverberation, hampering the development of AVSE. In this work, we introduce AV2Wav, a resynthesis-based audio-visual speech enhancement approach that can generate clean speech despite the challenges of real-world training data. We obtain a subset of nearly clean speech from an audio-visual corpus using a neural quality estimator, and then train a diffusion model on this subset to generate waveforms conditioned on continuous speech representations from AV-HuBERT with noise-robust training. We use continuous rather than discrete representations to retain prosody and speaker information. With this vocoding task alone, the model can perform speech enhancement better than a masking-based baseline. We further fine-
    
[^89]: 迈向填充通用工程设计知识的方法

    Towards Populating Generalizable Engineering Design Knowledge. (arXiv:2307.06985v1 [cs.CL])

    [http://arxiv.org/abs/2307.06985](http://arxiv.org/abs/2307.06985)

    这项研究提出了一种从专利文件中提取工程设计知识的方法，通过构建知识图来填充通用设计知识，并与现有方法进行了比较。

    

    为了填充通用工程设计知识，我们提出了一种从专利文件中提取head entity :: relationship :: tail entity形式事实的方法。这些事实可以在专利文件内部和跨文件之间组合形成知识图，用作表示和存储设计知识的方案。现有的工程设计文献中的方法通常利用一组预定义的关系来填充统计近似而非事实的三元组。在我们的方法中，我们训练一个标记器来识别句子中的实体和关系。在确定了一对实体后，我们训练另一个标记器来识别特定表示这对实体之间关系的关系标记。为了训练这些标记器，我们手动构建了一个包含44,227个句子和相应事实的数据集。我们还将该方法的性能与通常推荐的方法进行了比较，其中我们预.

    Aiming to populate generalizable engineering design knowledge, we propose a method to extract facts of the form head entity :: relationship :: tail entity from sentences found in patent documents. These facts could be combined within and across patent documents to form knowledge graphs that serve as schemes for representing as well as storing design knowledge. Existing methods in engineering design literature often utilise a set of predefined relationships to populate triples that are statistical approximations rather than facts. In our method, we train a tagger to identify both entities and relationships from a sentence. Given a pair of entities thus identified, we train another tagger to identify the relationship tokens that specifically denote the relationship between the pair. For training these taggers, we manually construct a dataset of 44,227 sentences and corresponding facts. We also compare the performance of the method against typically recommended approaches, wherein, we pre
    
[^90]: MERT:带有大规模自监督训练的声学音乐理解模型

    MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])

    [http://arxiv.org/abs/2306.00107](http://arxiv.org/abs/2306.00107)

    提出了一个带有大规模自监督训练的音乐理解模型MERT，利用了教师模型并采用了一种优于传统的语音和音频方法的组合方式。

    

    自监督学习（SSL）最近在视觉、文本和语音领域中已被证明是训练通用模型的一种很有前景的范例，对于跨越音乐领域的应用，尤其是对于调性和音高这样的特殊音乐知识的建模颇具挑战性。为了解决这一问题，我们提出了一个基于大规模自监督训练的声学音乐理解模型，即MERT。在我们的探索中，我们确定了更优秀的教师模型组合，这种组合方法在性能方面优于传统的语音和音频方法。

    Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
    
[^91]: 通过摘要二元性和显式大纲控制增强生成

    Enhancing Generation through Summarization Duality and Explicit Outline Control. (arXiv:2305.14459v1 [cs.CL])

    [http://arxiv.org/abs/2305.14459](http://arxiv.org/abs/2305.14459)

    本文提出了一个两阶段的摘要增强的大纲监督生成框架，能够更好地生成明确和合理的大纲，并引入了一个新颖的显式大纲控制方法以更有效地利用生成的大纲。

    

    自动开放式长文本生成面临语义不连贯和情节不可信的重大挑战。先前的工作通常通过设计无监督任务中的短语或抽象信号的大纲来缓解此问题，但这往往是不稳定且难以解释的。在假设摘要作为已成熟的大纲的情况下，我们介绍了一个两阶段、摘要增强的大纲监督生成框架。该框架利用摘要任务的双重特征来改进大纲预测，从而产生更明确和合理的大纲。此外，我们发现基于大纲的生成具有未充分利用的问题，无论是标准的预训练语言模型（例如GPT-2、BART）还是大型语言模型（例如Vicuna、ChatGPT）。为了解决这个问题，我们提出了一种新颖的显式大纲控制方法，以更有效地利用生成的大纲。

    Automatically open-ended long text generation poses significant challenges due to semantic incoherence and plot implausibility. Previous works usually alleviate this problem through outlines in the form of short phrases or abstractive signals by designing unsupervised tasks, which tend to be unstable and weakly interpretable.  Assuming that a summary serves as a mature outline, we introduce a two-stage, summary-enhanced outline supervised generation framework. This framework leverages the dual characteristics of the summarization task to improve outline prediction, resulting in more explicit and plausible outlines. Furthermore, we identify an underutilization issue in outline-based generation with both standard pretrained language models (e.g., GPT-2, BART) and large language models (e.g., Vicuna, ChatGPT). To address this, we propose a novel explicit outline control method for more effective utilization of generated outlines.
    
[^92]: 规模上的解释性：在Alpaca中识别因果机制

    Interpretability at Scale: Identifying Causal Mechanisms in Alpaca. (arXiv:2305.08809v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08809](http://arxiv.org/abs/2305.08809)

    通过使用分布式对齐搜索（DAS）方法，我们在大型语言模型中实现了规模上的解释性，这使得我们能够高效地搜索到解释性因果结构，并应用于Alpaca模型中。

    

    对于AI安全而言，获得大型通用语言模型的人类可解释性解释是一个紧急目标。然而，同样重要的是我们的解释性方法能够忠实于模型行为底层的因果动力学，且能够在未见输入上具有鲁棒泛化性。分布式对齐搜索（DAS）是一种强大的渐变下降方法，它基于一种因果抽象理论，已经发现了可解释的符号算法和针对特定任务进行细调的小型深度学习模型之间的完美对齐。在本文中，我们通过用学习得到的参数来替换剩余的蛮力搜索步骤，显著扩展了DAS，这种方法我们称之为无边界DAS。这使得我们能够在大型语言模型中高效地搜索可解释的因果结构，同时它们遵循指令。我们将无边界DAS应用于Alpaca模型（7B参数），它可以快速解决一个简单的数值推理问题。通过无边界DAS，我们发现...

    Obtaining human-interpretable explanations of large, general-purpose language models is an urgent goal for AI safety. However, it is just as important that our interpretability methods are faithful to the causal dynamics underlying model behavior and able to robustly generalize to unseen inputs. Distributed Alignment Search (DAS) is a powerful gradient descent method grounded in a theory of causal abstraction that has uncovered perfect alignments between interpretable symbolic algorithms and small deep learning models fine-tuned for specific tasks. In the present paper, we scale DAS significantly by replacing the remaining brute-force search steps with learned parameters -- an approach we call Boundless DAS. This enables us to efficiently search for interpretable causal structure in large language models while they follow instructions. We apply Boundless DAS to the Alpaca model (7B parameters), which, off the shelf, solves a simple numerical reasoning problem. With Boundless DAS, we di
    
[^93]: 利用分解注意力机制的单层Transformer对广义Potts模型进行最优推断

    Optimal inference of a generalised Potts model by single-layer transformers with factored attention. (arXiv:2304.07235v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2304.07235](http://arxiv.org/abs/2304.07235)

    我们将分析和数值推导结合，在基于广义 Potts 模型的数据上，对经过改进适应这种模型的self-attention机制进行训练，发现经过修改的self-attention机制可以在极限采样下准确学习Potts模型。这个“分解”注意力机制通过从数据中学习相关属性，可以提高Transformer的性能和可解释性。

    

    Transformer 是一种革命性的神经网络，在自然语言处理和蛋白质科学方面取得了实践上的成功。它们的关键构建块是一个叫做自注意力机制的机制，它被训练用于预测句子中缺失的词。尽管Transformer在应用中取得了实践上的成功，但是自注意力机制究竟从数据中学到了什么以及它是怎么做到的还不是很清楚。本文针对从具有相互作用的位置和 Potts 颜色中提取的数据在训练的Transformer上给出了精确的分析和数值刻画。我们证明，虽然一般的transformer需要多层学习才能准确学习这个分布，但是经过小改进的自注意力机制在无限采样的极限下可以完美地学习Potts模型。我们还计算了这个修改后的自注意力机制所谓“分解”的泛化误差，并在合成数据上数值演示了我们的发现。我们的结果为解释Transformer的内在工作原理以及提高其性能和可解释性提供了新的思路。

    Transformers are the type of neural networks that has revolutionised natural language processing and protein science. Their key building block is a mechanism called self-attention which is trained to predict missing words in sentences. Despite the practical success of transformers in applications it remains unclear what self-attention learns from data, and how. Here, we give a precise analytical and numerical characterisation of transformers trained on data drawn from a generalised Potts model with interactions between sites and Potts colours. While an off-the-shelf transformer requires several layers to learn this distribution, we show analytically that a single layer of self-attention with a small modification can learn the Potts model exactly in the limit of infinite sampling. We show that this modified self-attention, that we call ``factored'', has the same functional form as the conditional probability of a Potts spin given the other spins, compute its generalisation error using t
    
[^94]: 基于分数的条件模型的概念代数

    Concept Algebra for Score-Based Conditional Models. (arXiv:2302.03693v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.03693](http://arxiv.org/abs/2302.03693)

    本文研究了基于分数的条件模型中学习表示的结构，并开发了一种数学形式化表达概念被编码为表示空间子空间的思想。利用这个方法，我们提出了一种简单的方法来识别给定概念对应的表示部分，并通过代数操作操纵模型所表达的概念。

    

    本文研究了文本引导生成模型中学习表示的结构，重点关注基于分数的模型。我们聚焦于概念被编码为某种表示空间的子空间（或方向）的思想，并开发了这个思想的数学形式化。利用这个形式化方法，我们展示了有一个自然的表示选择具有这种性质，并且我们开发了一种简单的方法来识别与给定概念对应的表示部分。特别是，这使我们能够通过对表示的代数操作来操纵模型所表达的概念。我们使用稳定扩散在文本引导图像生成的示例中演示了这个思想。

    This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. Here, we focus on the idea that concepts are encoded as subspaces (or directions) of some representation space. We develop a mathematical formalization of this idea.Using this formalism, we show there's a natural choice of representation with this property, and we develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples text-guided image generation, using Stable Diffusion.
    

