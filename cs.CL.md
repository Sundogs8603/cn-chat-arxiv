# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Executable Code Actions Elicit Better LLM Agents](https://rss.arxiv.org/abs/2402.01030) | 使用可执行的Python代码整合LLM智能体行动，提升了成功率高达20%。 |
| [^2] | [From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models](https://arxiv.org/abs/2403.12027) | 近年来，随着大型基础模型的兴起，自动图表理解取得了显著进展，本调查论文概述了在这些基础模型背景下图表理解领域的最新发展、挑战和未来方向 |
| [^3] | [FlexCap: Generating Rich, Localized, and Flexible Captions in Images](https://arxiv.org/abs/2403.12026) | FlexCap模型能够生成图像中具有不同长度的区域描述，在密集字幕任务和视觉问答系统中表现出优越性能。 |
| [^4] | [A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models](https://arxiv.org/abs/2403.12025) | 提出了用于揭示大型语言模型中健康公平危害和偏见的资源和方法，进行了实证案例研究，并提出了用于人类评估LLM生成答案偏见的多因子框架以及EquityMedQA数据集。 |
| [^5] | [Enhancing Hokkien Dual Translation by Exploring and Standardizing of Four Writing Systems](https://arxiv.org/abs/2403.12024) | 通过开发双语翻译模型，探索台湾福建话和繁体中文/英文之间的翻译，引入限制单语语料库并将所有台湾福建话文字系统规范为福建话汉字，从而提高翻译性能。 |
| [^6] | [Supervised Fine-Tuning as Inverse Reinforcement Learning](https://arxiv.org/abs/2403.12017) | 本论文提出将逆强化学习和模仿学习的见解结合，探讨了使用演示数据集对齐大语言模型的方法，并对不同方法的性能进行了分析。 |
| [^7] | [EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents](https://arxiv.org/abs/2403.12014) | EnvGen提出了一种新的框架，利用LLMs的推理能力自适应创建训练环境，帮助小型具身体RL代理在弱点方面学习有用技能。 |
| [^8] | [Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning](https://arxiv.org/abs/2403.11996) | 利用生成式人工智能和图算法加速科学发现，揭示论文之间的深入跨学科关系，并提出了新颖的材料设计。 |
| [^9] | [Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching](https://arxiv.org/abs/2403.11984) | 本文介绍了一种使用自然语言处理和大型语言模型分析教学评估的新方法，可以从大量反馈中提取、嵌入、聚类和总结SETs。 |
| [^10] | [Language Evolution with Deep Learning](https://arxiv.org/abs/2403.11958) | 通过深度学习模型，本研究探讨了语言演化的计算模拟，以及其对于模拟语言演化的帮助和局限性，适用于语言学家和认知科学家。 |
| [^11] | [Adaptative Bilingual Aligning Using Multilingual Sentence Embedding](https://arxiv.org/abs/2403.11921) | 提出了一种使用句子嵌入进行自适应双语对齐的系统，相对于现有技术，其在处理片段性平行文本时表现出与之等效的结果，且具有准线性复杂度。 |
| [^12] | [Tur[k]ingBench: A Challenge Benchmark for Web Agents](https://arxiv.org/abs/2403.11905) | Tur[k]ingBench是一个挑战性的网络代理基准测试，用于评估最先进的多模态模型在处理包含文本指示和多模态上下文的复杂任务时的泛化能力。 |
| [^13] | [CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification](https://arxiv.org/abs/2403.11904) | 该论文提出了CICLe框架，基于适应上下文学习的方式，在大规模多类食品风险分类中取得了较好的效果，提出了基于符合预测的LLM-in-the-loop框架，可以提高基本分类器的性能，并减少能源消耗。 |
| [^14] | [A Closer Look at Claim Decomposition](https://arxiv.org/abs/2403.11903) | 通过研究声明分解方法，发现在评估文本支持时，分解方法对结果具有影响，提出了一种新的评估方法DecompScore和基于LLM的生成分解方法。 |
| [^15] | [Investigating Markers and Drivers of Gender Bias in Machine Translations](https://arxiv.org/abs/2403.11896) | 通过使用反向翻译技术，比较五种中间语言的结果，并提出新的指标评估翻译中隐含的性别偏见变化。 |
| [^16] | [From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?](https://arxiv.org/abs/2403.11894) | 该研究对医疗保健NLP中的深度学习进行了全面审查，提出了可解释和可解释的人工智能（XIAI）概念，并发现注意机制是主要新兴IAI，同时面临着缺乏全局建模、最佳实践以及系统评估和基准测试的挑战。 |
| [^17] | [QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction](https://arxiv.org/abs/2403.11886) | QueryAgent提出了一种基于环境反馈的自我校正方法ERASER，在语义解析中表现出显著的性能提升和高效性 |
| [^18] | [CO3: Low-resource Contrastive Co-training for Generative Conversational Query Rewrite](https://arxiv.org/abs/2403.11873) | 通过对比协同训练，本研究提出了一种低资源生成式对话式查询重写方法，可以在数据有限、存在噪声和语言风格转变的情况下保持鲁棒性。 |
| [^19] | [GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture](https://arxiv.org/abs/2403.11858) | 提出了一种在农业害虫管理中利用GPT-4作为评估器的创新方法，评估大型语言模型生成的内容在多个方面的质量 |
| [^20] | [Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models](https://arxiv.org/abs/2403.11838) | 引入Guide-Align，一种两阶段方法，通过安全训练模型识别潜在风险，并制定特定指南，从而建立全面的指导库，用于指导LLMs生成安全和高质量输出。 |
| [^21] | [Towards Understanding the Relationship between In-context Learning and Compositional Generalization](https://arxiv.org/abs/2403.11834) | 强制模型进行上下文学习可能有助于促进神经网络模型的构成概括能力 |
| [^22] | [SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator](https://arxiv.org/abs/2403.11833) | 本文介绍了一种名为SSCAE的对抗攻击模型，用于生成语义、句法和上下文感知的自然语言对抗样本，提出了动态阈值和本地贪婪搜索以生成高质量对抗样本。 |
| [^23] | [Metaphor Understanding Challenge Dataset for LLMs](https://arxiv.org/abs/2403.11810) | 该研究发布了一个旨在评估大语言模型（LLMs）隐喻理解能力的数据集，包含超过10k个隐喻句的释义以及1.5k个不恰当释义实例，为研究人员提供了有效的工具来检验模型是否真正实现全面隐喻解释而非仅依赖词汇相似性。 |
| [^24] | [How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments](https://arxiv.org/abs/2403.11807) | 通过博弈论视角评估LLMs的决策能力，结果表明GPT-3.5在稳健性方面表现良好，但泛化能力有限，而GPT-4则优于其他模型。 |
| [^25] | [Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models](https://arxiv.org/abs/2403.11802) | 提出了一种名为Counting-Stars的简单、高效、合理策略，用于评估长上下文大型语言模型的能力，并在实验中发现GPT-4 Turbo和Kimi Chat在此任务上取得显著性能。 |
| [^26] | [Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2403.11793) | 使用抽象和推理语料库（ARC）数据集评估大型语言模型的推理和上下文理解能力，结果显示虽然大型语言模型具有较弱的推理能力，但在逻辑连贯性、组合性和效率方面仍然落后，实验结果有助于提出实现人类水平推理的发展路径。 |
| [^27] | [Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained Large Language Models](https://arxiv.org/abs/2403.11786) | 使用零样本提示的方法，我们利用OpenAI的GPT-3.5模型从文本中成功提取了超关联知识，尽管精确率较低，但结果显示了未来研究的潜在方向。 |
| [^28] | [Modality-Agnostic fMRI Decoding of Vision and Language](https://arxiv.org/abs/2403.11771) | 本研究介绍了一种能够预测受试者正在看到的刺激的单一解码器，无论刺激是以何种模态呈现。研究发现模态无关解码器表现与模态特定解码器相当甚至更好。 |
| [^29] | [Revisiting The Classics: A Study on Identifying and Rectifying Gender Stereotypes in Rhymes and Poems](https://arxiv.org/abs/2403.11752) | 该研究通过收集韵律诗和诗歌数据集，建立了一个准确率为97%的模型，用于识别性别偏见，并通过大型语言模型纠正了性别刻板印象。 |
| [^30] | [Embedded Named Entity Recognition using Probing Classifiers](https://arxiv.org/abs/2403.11747) | 提出一种名为EMBER的方法，通过使用探测分类器将信息提取能力直接嵌入预训练语言模型中，实现了高效的同时文本生成和信息提取，使得解码器-only语言模型能够在不微调的情况下进行命名实体识别。 |
| [^31] | [Let's Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model](https://arxiv.org/abs/2403.11621) | NeFT是一种神经元级微调的新方法，可以精确和计算高效地更新大型语言模型。 |
| [^32] | [Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines](https://arxiv.org/abs/2403.11585) | Linguacodus是一种创新框架，通过部署动态流水线和精细调整的大型语言模型，实现了将自然语言任务描述转换为代码的自动化过程，极大地推进了机器学习应用的发展。 |
| [^33] | [Reinforcement Learning with Token-level Feedback for Controllable Text Generation](https://arxiv.org/abs/2403.11558) | 提出了一种新的强化学习算法TOLE，通过使用标记级别奖励进行文本生成，采用了"先量子化，然后加噪声"的方法来增强算法的鲁棒性。可以在多个约束条件下灵活扩展，避免了过拟合和语义崩溃问题。 |
| [^34] | [DEE: Dual-stage Explainable Evaluation Method for Text Generation](https://arxiv.org/abs/2403.11509) | 介绍了DEE，一个双阶段可解释评估方法，用于评估文本生成质量。 |
| [^35] | [Word Order's Impacts: Insights from Reordering and Generation Analysis](https://arxiv.org/abs/2403.11473) | 本文通过顺序重建的角度重新审视了关于词序的假设，并通过选择不同范围的数据集进行了实验证明，ChatGPT依赖于词序进行推断，但不能明确支持或否定词序与词汇语义之间的冗余关系。 |
| [^36] | [HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models](https://arxiv.org/abs/2403.11456) | HateCOT数据集通过GPT-3.5-Turbo生成解释，将52,000个样本数据用于预训练模型，显著提升了在不同领域和任务下的攻击性内容检测效果。 |
| [^37] | [StyleChat: Learning Recitation-Augmented Memory in LLMs for Stylized Dialogue Generation](https://arxiv.org/abs/2403.11439) | 该论文介绍了一个包含38种风格的风格化对话数据集StyleEval，并提出了一种风格化对话框架，以增强LLMs在风格化对话生成中的性能。 |
| [^38] | [InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions](https://arxiv.org/abs/2403.11435) | InsCL提出了一种基于指导信息的新型持续学习范式，通过Wasserstein距离计算任务相似性，并引入InsInfo指标来定量指导信息的复杂性和多样性，从而指导重播过程更倾向于高质量数据。 |
| [^39] | [A Novel Paradigm Boosting Translation Capabilities of Large Language Models](https://arxiv.org/abs/2403.11430) | 新范式关注在LLMs的预训练阶段增强跨语言对齐能力，强调使用高质量的小型双语数据，在传统机器翻译方法无法胜任的任务上取得了良好的效果。 |
| [^40] | [Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure](https://arxiv.org/abs/2403.11425) | 使用大型语言模型结合新颖的叙述特征，能够有效识别癌症患者患心力衰竭的风险，表现优于传统机器学习模型和深度学习模型。 |
| [^41] | [Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational Systems](https://arxiv.org/abs/2403.11413) | 通过利用动态背景，该研究开发了一个建议问题生成器，实验证明它可以生成比其他提示方法更好的建议问题。 |
| [^42] | [X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment](https://arxiv.org/abs/2403.11399) | 提出了两种成本有效的方法解决大规模多模态模型训练数据的挑战，并在英语-韩语-中文多语言、多模态训练数据集上开发了表现优越的双语多模态模型。 |
| [^43] | [Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot](https://arxiv.org/abs/2403.11381) | 本研究探讨了LLM增强型自主代理在合作方面的表现，结果显示虽然这些代理显示出合作倾向，但仍然存在合作困难，强调了对更健壮架构的需求。 |
| [^44] | [What Makes Math Word Problems Challenging for LLMs?](https://arxiv.org/abs/2403.11369) | 研究了大型语言模型在处理数学问题的文字题目时所面临的困难，通过分析特征和训练分类器来预测模型对不同类型题目的表现。 |
| [^45] | [JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning](https://arxiv.org/abs/2403.11366) | 提出了用于检索增强微调的JAX张量并行LoRA库，通过PEFT兼容微调Llama-2模型，利用分布式训练和JAX的即时编译和张量分片实现了资源高效管理，加速微调并降低内存需求，提高了微调大型语言模型在复杂RAG应用中的可扩展性和可行性。 |
| [^46] | [CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data](https://arxiv.org/abs/2403.11346) | 提出了CantonMT项目，利用合成反向翻译数据对粤语至英语NMT模型进行微调，并为研究人员提供用户友好的界面和开源工具包，以促进研究 |
| [^47] | [ConvSDG: Session Data Generation for Conversational Search](https://arxiv.org/abs/2403.11335) | 基于大型语言模型的ConvSDG框架探索了生成更多带相关标签训练会话以提升会话搜索性能的方法。 |
| [^48] | [Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback](https://arxiv.org/abs/2403.11330) | 通过将全局明确标注拆解成本地隐式多模态反馈，提出了一种改进对话代理的方法，并在各种对话度量方面展现出一致的改进 |
| [^49] | [StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows](https://arxiv.org/abs/2403.11322) | 提出了一种使用StateFlow的新颖LLM任务解决范式，将复杂任务解决过程概念化为状态机，通过状态转换确保LLM响应的清晰跟踪和管理。 |
| [^50] | [Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches](https://arxiv.org/abs/2403.11317) | 本论文对比了在使用LLMs进行少样本视觉问答（VQA）时，将视觉嵌入直接连接到LLM嵌入空间和使用图像描述两种方法的性能，发现对于特定LLM模型，在零样本情况下使用图像描述更好，在少样本情况下则取决于所选的上下文例子。 |
| [^51] | [Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts](https://arxiv.org/abs/2403.11314) | 这项研究探讨了如何在Transformer模型中进行逻辑推理，通过在数据集中引入证明来训练两种模型，成功避免了伪相关性和推理捷径。 |
| [^52] | [Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding](https://arxiv.org/abs/2403.11311) | 提出了一种新型多模态软提示框架MoPE-BAF，用于解决少样本学习下的多模态讽刺检测和情感分析问题，通过三个软提示专家和块感知提示融合，实现了模态特征提取和多模态交互。 |
| [^53] | [SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant](https://arxiv.org/abs/2403.11299) | 本研究引入了一个名为SQ-LLaVA的新颖框架，通过自我训练模型如何提出高质量问题，以改善视觉-语言模型的泛化能力。 |
| [^54] | [A Modified Word Saliency-Based Adversarial Attack on Text Classification Models](https://arxiv.org/abs/2403.11297) | 本文介绍了一种修改的基于词显著性的对抗攻击方法，通过对模型最具影响力的词进行修改，旨在欺骗分类模型且保持语义连贯性，在提高攻击效果的同时避开了分类系统的检测。 |
| [^55] | [Forging the Forger: An Attempt to Improve Authorship Verification via Data Augmentation](https://arxiv.org/abs/2403.11265) | 通过引入合成示例的数据增强方法，可以改善在作者验证任务中对抗性攻击下的分类器预测。 |
| [^56] | [ChartThinker: A Contextual Chain-of-Thought Approach to Optimized Chart Summarization](https://arxiv.org/abs/2403.11236) | 该研究提出了一种名为ChartThinker的创新图表摘要方法，利用上下文思维和策略性的上下文检索实现深度分析，旨在提高生成摘要的逻辑连贯性和准确性。 |
| [^57] | [Cheap Ways of Extracting Clinical Markers from Texts](https://arxiv.org/abs/2403.11227) | 该论文研究了从文本中提取临床标记的廉价方法，比较了传统机器学习方法和大型语言模型对于提取亮点和生成摘要的效果。 |
| [^58] | [Creating an African American-Sounding TTS: Guidelines, Technical Challenges,and Surprising Evaluations](https://arxiv.org/abs/2403.11209) | 本文探讨了在开发一种听起来像受过教育、专业、地区口音真实的非洲裔美国女性的美式英文文本转语音（TTS）系统过程中遇到的种族表现挑战，提出了指导方针、技术难题和对该系统的令人惊讶的评估。 |
| [^59] | [TRELM: Towards Robust and Efficient Pre-training for Knowledge-Enhanced Language Models](https://arxiv.org/abs/2403.11203) | 本文介绍了TRELM，一种面向知识增强语言模型的稳健高效预训练框架，通过采用稳健方法注入知识三元组和知识增强的存储库来解决实体表示优化不足和预训练过程的问题。 |
| [^60] | [Decoding Continuous Character-based Language from Non-invasive Brain Recordings](https://arxiv.org/abs/2403.11183) | 提出了一种从单次非侵入性脑记录中解码连续语言的新方法，通过三维卷积网络和信息瓶颈结合字符解码器，能够实现在和跨主体之间产生捕捉感知语音含义的可理解文本序列。 |
| [^61] | [Quality-Aware Image-Text Alignment for Real-World Image Quality Assessment](https://arxiv.org/abs/2403.11176) | 提出了一种基于CLIP的自监督方法QualiCLIP，通过质量感知的图像-文本对齐策略，实现了图像质量评估不需要标记MOS的问题 |
| [^62] | [Correcting misinformation on social media with a large language model](https://arxiv.org/abs/2403.11169) | 提出了一种名为MUSE的大型语言模型，通过访问最新信息并评估可信度，以解决社交媒体上误信息纠正的难题。 |
| [^63] | [Evaluation Ethics of LLMs in Legal Domain](https://arxiv.org/abs/2403.11152) | 评估大型语言模型在法律领域的伦理层面的重要性以确保其有效整合，提出了一种基于法律案例的新颖评估方法，并对大型语言模型的基本语言能力、专业法律知识和法律稳健性进行了全面评估 |
| [^64] | [A Challenge Dataset and Effective Models for Conversational Stance Detection](https://arxiv.org/abs/2403.11145) | 介绍了一个新的多轮对话立场检测数据集（MT-CSD），提出了一种全局局部注意力网络（GLAN）来解决对话数据中的长距离和短距离依赖性。 |
| [^65] | [Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models](https://arxiv.org/abs/2403.11130) | 本文研究了令牌化策略和词汇量对阿拉伯语言模型性能的影响，结果显示字节对编码（BPE）与Farasa在多个任务中表现优异，突显了形态分析在捕捉阿拉伯语言细微差异中的重要性。 |
| [^66] | [Enhancing Event Causality Identification with Rationale and Structure-Aware Causal Question Answering](https://arxiv.org/abs/2403.11129) | 提出了一个多任务学习框架，通过理由和结构感知的因果问答来增强事件因果识别，将DECI任务转化为多项选择问题回答，生成被问及事件的因果关系以及解释这些事件具有因果关系的理由，构建了一个事件结构图对多跳潜在关系进行建模。 |
| [^67] | [Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants' API Invocation Capabilities](https://arxiv.org/abs/2403.11128) | 提出了自动动态评估（AutoDE）方法，用于评估人工智能助手的API调用能力，避免静态评估中导致的误导性评估。 |
| [^68] | [Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment](https://arxiv.org/abs/2403.11124) | 更多的响应但更少的提示可以更好地触发大型语言模型进行人类对齐，此外，提出了一种新的提示多样性公式，可以进一步影响LLMs的最终性能。 |
| [^69] | [Granular Change Accuracy: A More Accurate Performance Metric for Dialogue State Tracking](https://arxiv.org/abs/2403.11123) | Granular Change Accuracy (GCA) 是一种更准确的对话状态跟踪性能度量，有效减少了由于分布均匀性和对话轮之间错误定位而产生的偏见，特别适用于评估少样本或零样本训练的模型。 |
| [^70] | [HarmPot: An Annotation Framework for Evaluating Offline Harm Potential of Social Media Text](https://arxiv.org/abs/2403.11108) | 论文讨论了一个注释模式的开发，用于构建数据集以评估社交媒体文本的离线危害潜力，不仅关注单一分裂因素或仇恨言论，而是致力于衡量在线内容的危害潜力，无论其内容是否充满仇恨。 |
| [^71] | [ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models](https://arxiv.org/abs/2403.11103) | 通过让大语言模型自省特定领域，生成领域相关属性并创建属性丰富的训练数据，同时绕过复杂结构的挑战，实现了生成命名实体识别数据集的创新策略。 |
| [^72] | [Lost in Translation? Translation Errors and Challenges for Fair Assessment of Text-to-Image Models on Multilingual Concepts](https://arxiv.org/abs/2403.11092) | 本研究发现在一个文本到图像模型基准中存在西班牙语、日语和中文中不同程度的翻译错误，提供了纠正，并分析了对基准性能的影响。 |
| [^73] | [m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks](https://arxiv.org/abs/2403.11085) | m&m's引入了一个包含4K+多步骤多模态任务的基准，涉及33种工具，用于评估LLM作为规划器的设计决策。 |
| [^74] | [Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning](https://arxiv.org/abs/2403.11083) | 该研究旨在开发一种适用于多种场景的通用异常检测模型，通过定制视觉-语言基础模型和引入多模态提示策略进行多模态异常检测和推理。 |
| [^75] | [RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning](https://arxiv.org/abs/2403.11082) | RobustSentEmbed是一个自监督句子嵌入框架，通过对抗对比学习提高了文本表示任务中的泛化性和稳健性，实现了在对抗攻击中的优越表现。 |
| [^76] | [Deep Learning-based Sentiment Analysis in Persian Language](https://arxiv.org/abs/2403.11069) | 该研究介绍并实现了一种基于深度学习的混合模型，用于波斯语情感分析，通过对Digikala网站的客户评论数据进行实验，最终取得了令人印象深刻的78.3的F1分数。 |
| [^77] | [Pre-Trained Language Models Represent Some Geographic Populations Better Than Others](https://arxiv.org/abs/2403.11025) | 预训练语言模型在代表地理人口方面存在明显偏向，表现优秀的人口主要集中在美国和英国，而南亚和东南亚地区的人口则被较差地代表。 |
| [^78] | [DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages](https://arxiv.org/abs/2403.11009) | DIALECTBENCH是第一个面向自然语言处理中的方言、语言变体和密切相关语言的大规模基准测试，为实现对不同语言变体上NLP系统性能的全面评估提供了重要工具。 |
| [^79] | [Entity Alignment with Unlabeled Dangling Cases](https://arxiv.org/abs/2403.10978) | 提出了一种基于GNN的框架，在实体对齐中解决了无标签悬挂案例的问题，通过设计注意机制和正样本-无标签损失来实现更好的对齐性能 |
| [^80] | [Pointer-Generator Networks for Low-Resource Machine Translation: Don't Copy That!](https://arxiv.org/abs/2403.10963) | Pointer-Generator Networks在低资源机器翻译中未展现出预期的优势，模型在不同资源范围和语言之间的关系下表现一般。 |
| [^81] | [Energy-Based Models with Applications to Speech and Language Processing](https://arxiv.org/abs/2403.10961) | 基于能量的模型（EBMs）是一类重要的概率模型，在语音和语言处理等领域吸引了越来越多的关注，因其显著的理论和算法进展。 |
| [^82] | [SelfIE: Self-Interpretation of Large Language Model Embeddings](https://arxiv.org/abs/2403.10949) | 提出了SelfIE框架，使大型语言模型能够自解释其嵌入，揭示内部推理，包括道德决策、提示注入和消除有害知识。 |
| [^83] | [MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations](https://arxiv.org/abs/2403.10943) | MIntRec2.0介绍了一个旨在解决多模态意图识别和对话中场外检测挑战的大规模基准数据集。该数据集包含30个细粒度类别的1,245个对话和15,040个样本，其中包括逼真的场外样本，并丰富了发言者信息以支持多方对话研究。 |
| [^84] | [Initial Decoding with Minimally Augmented Language Model for Improved Lattice Rescoring in Low Resource ASR](https://arxiv.org/abs/2403.10937) | 通过最小化增强语言模型进行初步解码，以提高低资源ASR中晶格重新打分的语音识别准确性，相对减少了泰卢固语21.8%和卡纳达语41.8%的词误差率，同时仅消耗1/8内存。 |
| [^85] | [BEnQA: A Question Answering and Reasoning Benchmark for Bengali and English](https://arxiv.org/abs/2403.10900) | BEnQA介绍了一项包含孟加拉语和英语考试问题的数据集，并发现大型语言模型在孟加拉语和英语问题中表现有明显差异，同时发现Chain-of-Thought提示对推理问题有益，附加英语翻译有助于解答孟加拉语问题。 |
| [^86] | [Towards Robustness and Diversity: Continual Learning in Dialog Generation with Text-Mixup and Batch Nuclear-Norm Maximization](https://arxiv.org/abs/2403.10894) | 本文探讨了在对话生成领域中的继续学习，提出了利用文本混合和批量核范数最大化的方法来解决灾难性遗忘和模式崩溃问题，实验证明这种方法在继续学习方面优于现有技术。 |
| [^87] | [Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean](https://arxiv.org/abs/2403.10882) | 该研究提出了三种策略来增强基于公开可用MLLMs的资源较少的语言的性能，包括扩展词汇、双语数据预训练和指导微调。 |
| [^88] | [Zero-shot Generative Linguistic Steganography](https://arxiv.org/abs/2403.10856) | 本文提出了一种基于上下文学习的新型零样本方法，利用生成性语言隐写术实现更好的感知和统计隐蔽性。 |
| [^89] | [RETINAQA : A Knowledge Base Question Answering Model Robust to both Answerable and Unanswerable Questions](https://arxiv.org/abs/2403.10849) | 提出了一种名为RetinaQA的新型KBQA模型，通过基于填图的逻辑形式构建和使用辨别方法，实现了对不可回答性的鲁棒性，并在可回答和不可回答问题上显着优于最先进的KBQA模型。 |
| [^90] | [Two-step Automated Cybercrime Coded Word Detection using Multi-level Representation Learning](https://arxiv.org/abs/2403.10838) | 提出了一种新的两步方法，通过构建平均潜在向量并基于多层潜在向量检测网络犯罪编码词，解决了自动化犯罪代码词检测问题中的数据获取困难和利用语言模型理解自然语言困难的挑战。 |
| [^91] | [Deciphering Hate: Identifying Hateful Memes and Their Targets](https://arxiv.org/abs/2403.10829) | 介绍了一个为孟加拉语设计的新颖多模态数据集BHM，用于检测仇恨表情包以及它们所针对的社会实体。 |
| [^92] | [Multi-party Response Generation with Relation Disentanglement](https://arxiv.org/abs/2403.10827) | 本研究提出了一种利用关系解缠来指导神经响应生成的方法，通过在会话上下文内部微妙线索上进行关系推断，实现了对多方回复的自动推断，无需人工标签。 |
| [^93] | [Do Large Language Models understand Medical Codes?](https://arxiv.org/abs/2403.10822) | 该研究调查了大型语言模型是否理解医学编码的含义，评估了它们对领域特定术语的认识和理解。 |
| [^94] | [Efficient Pruning of Large Language Model with Adaptive Estimation Fusion](https://arxiv.org/abs/2403.10799) | 提出了一种简单而高效的剪枝方法，能够自适应地模拟每个子结构的重要性，并根据多层结构的结果自适应地融合粗粒度和细粒度的估计。 |
| [^95] | [From Words to Routes: Applying Large Language Models to Vehicle Routing](https://arxiv.org/abs/2403.10795) | 该研究探索了应用大型语言模型解决车辆路径规划问题的能力，提出了基于自然语言任务描述的基本提示范例并提出一种使模型进行改进的框架。 |
| [^96] | [Exploring Chinese Humor Generation: A Study on Two-Part Allegorical Sayings](https://arxiv.org/abs/2403.10781) | 本文研究了如何利用最先进的语言模型来理解和生成中国幽默，特别是关于训练模型生成典故谚语。他们采用新颖的fine-tuning方法，包含融合拼音嵌入和对比学习，从而成功生成幽默性典故谚语。 |
| [^97] | [LLM-based Conversational AI Therapist for Daily Functioning Screening and Psychotherapeutic Intervention via Everyday Smart Devices](https://arxiv.org/abs/2403.10779) | 提出了一种基于LLM和智能设备的对话式AI治疗师，CaiTI平台，可通过自然和心理治疗对话来筛查日常功能，并提供个性化对话流程和心理治疗干预。 |
| [^98] | [Detecting Bias in Large Language Models: Fine-tuned KcBERT](https://arxiv.org/abs/2403.10774) | 在本研究中，作者通过使用KcBERT和KOLD数据对韩语评论进行微调，通过遮蔽语言建模来评估大型语言模型中的社会偏见，发现微调后的模型降低了种族偏见，但在性别和种族方面表现出显著变化。 |
| [^99] | [ECRC: Emotion-Causality Recognition in Korean Conversation for GCN](https://arxiv.org/abs/2403.10764) | 本研究提出了ECRC模型，通过结合单词级和句子级嵌入，以及基于新颖图结构的方法，在韩国会话环境中进行情绪因果识别研究。 |
| [^100] | [Rules still work for Open Information Extraction](https://arxiv.org/abs/2403.10758) | 该论文提出了一种针对中文文本的创新型OIE模型APRCOIE，通过自动生成抽取模式、定义新的模式形式以及设计张量计算过滤器等方法，实现了对多样复杂中文语法现象的处理，并在评估中表现优越，拓展了OIE性能边界。 |
| [^101] | [Depression Detection on Social Media with Large Language Models](https://arxiv.org/abs/2403.10750) | 提出了名为DORIS的新型抑郁症检测系统，将医学知识和大语言模型的最新进展相结合，通过分析个人在社交媒体上的帖子历史记录来确定抑郁症患者，以提高早期检测和干预。 |
| [^102] | [Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns](https://arxiv.org/abs/2403.10707) | 本文提出了一种通过利用大型语言模型（LLMs）的先进功能，以机器在循环中方法，处理社交媒体消息主题的新方法。 |
| [^103] | [PERL: Parameter Efficient Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2403.10704) | 使用低秩适应（LoRA）方法进行参数高效强化学习（PERL），能够在与传统RLHF设置相当的性能下，实现更快的训练和更少的内存占用。 |
| [^104] | [Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation](https://arxiv.org/abs/2403.10700) | 提出了一个新的基准数据集，首次引入了各种类型的指令错误，考虑到潜在的人类原因，以评估连续环境中 VLN 系统的健壮性 |
| [^105] | [A Multilingual Perspective on Probing Gender Bias](https://arxiv.org/abs/2403.10699) | 该论文通过多语言视角探索性别偏见的表达方式，强调了了解社会偏见的重要性。 |
| [^106] | [EXPLORER: Exploration-guided Reasoning for Textual Reinforcement Learning](https://arxiv.org/abs/2403.10692) | 提出了一种名为EXPLORER的探索引导推理代理，用于文本强化学习，能够解决智能体在多个游戏中泛化并在已知和未知对象上表现良好的关键挑战。 |
| [^107] | [MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling](https://arxiv.org/abs/2403.10691) | MYTE是一种基于形态学的字节编码范式，通过使用具有一致大小的片段来实现跨不同语言的信息编码，为99种语言提供了更短的编码，特别是对非欧洲语言和非拉丁文字的改进最为显著。 |
| [^108] | [Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond](https://arxiv.org/abs/2403.10667) | 本文旨在建立一个统一的多模态个性化系统(UniMP)，有效利用多模态数据同时消除与任务和模态特定定制相关的复杂性。 |
| [^109] | [DiPaCo: Distributed Path Composition](https://arxiv.org/abs/2403.10616) | DiPaCo提出了一种协同模块化架构和训练方法，可以通过路径分发计算，实现机器学习模型的训练，并在推断时无需模型压缩。 |
| [^110] | [Neural Erosion: Emulating Controlled Neurodegeneration and Aging in AI Systems](https://arxiv.org/abs/2403.10596) | 这项研究介绍了一种在人工智能系统中模拟受控神经退行的新方法，通过在LLMs中添加噪音或切除神经元来逐渐降低其性能，在智商测试中展示了神经退行的过程，并与传统的计算机视觉领域不同，是首个使用文本数据模拟神经退行的工作。 |
| [^111] | [Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction](https://arxiv.org/abs/2403.10581) | 提出了一种大型语言模型指导的双注意力ECG网络，用于心力衰竭风险预测，能够捕捉复杂的心电图特征，有效应对低风险和高风险组之间的不平衡。 |
| [^112] | [Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain](https://arxiv.org/abs/2403.10576) | 利用非语言元素进行网络安全领域的预训练，提出了新的预训练方法并在网络安全领域中取得了优越表现 |
| [^113] | [Exploring Language Model's Code Generation Ability with Auxiliary Functions](https://arxiv.org/abs/2403.10575) | 在这项研究中，我们全面评估了最近代码预训练语言模型中编码的辅助函数利用能力，通过设计实验，并通过实现风格分析，我们发现了模型利用辅助函数的很有前景。 |
| [^114] | [MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts](https://arxiv.org/abs/2403.10568) | 本文提出了MoPE技术，通过解开提示以自适应捕获数据集级和实例级特征，引入了混合Prompt专家来增强表达能力，并且在多模态融合中表现出更大的表达能力和可扩展性。 |
| [^115] | [Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models](https://arxiv.org/abs/2403.10557) | 本论文通过二阶信息（Hessian）的视角重新审视了大型语言模型的机器遗忘问题，提出了遗忘算法，具有较高的鲁棒性。 |
| [^116] | [PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency](https://arxiv.org/abs/2403.09732) | 提出了一个两阶段框架，通过引入参考增强表示和少样本演示，解决了在处理冗长的数据库信息和复杂用户意图时的挑战。 |
| [^117] | [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/abs/2403.09629) | Quiet-STaR提出了一种新的泛化版本，在每个标记处生成解释未来文本的思考过程，从而改善预测能力 |
| [^118] | [AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning](https://arxiv.org/abs/2403.09113) | AutoLoRA提出了一个基于元学习的框架，自动识别每个LoRA层的最佳秩，以解决LoRA中秩分配和秩搜索的问题，进而提高微调性能。 |
| [^119] | [AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic](https://arxiv.org/abs/2403.09017) | AraTrust是第一个阿拉伯语大型语言模型的全面信誉基准，解决了缺乏全面信誉评估基准的问题，帮助准确评估和提高LLMs的安全性。 |
| [^120] | [Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale](https://arxiv.org/abs/2403.08293) | GPST是一种无监督的句法语言模型，通过联合训练两个模型实现对原始文本的高并行预训练，克服了之前SLM依赖于黄金树和顺序训练的限制，展示了在多个任务中优于同等规模的GPT-2。 |
| [^121] | [Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models](https://arxiv.org/abs/2403.08281) | 通过融合高度专业化的语言、代码和数学模型，提出了一种名为UltraFuser的融合框架，引入了标记级别的门控机制，并设计了两阶段训练策略，以同时在三个领域取得高性能。 |
| [^122] | [Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2403.07440) | 该论文提出了基于矩阵变换的低秩调整（MTLoRA）方法，受大脑启发，用于提高微调技术的复杂任务适应性、性能、稳定性和算法复杂性。 |
| [^123] | [Knowledge Graph Large Language Model (KG-LLM) for Link Prediction](https://arxiv.org/abs/2403.07311) | 该论文提出了知识图谱大型语言模型框架（KG-LLM），利用思维链提示和上下文学习等NLP范例，以增强知识图谱中的多跳链接预测，并展示了框架在微调大型语言模型和零次尝试能力方面的有效性。 |
| [^124] | [ALaRM: Align Language Models via Hierarchical Rewards Modeling](https://arxiv.org/abs/2403.06754) | ALaRM是第一个从人类反馈中建模分层奖励的框架，通过整合整体奖励与特定方面的奖励，改善了大型语言模型与人类偏好的对齐性，尤其在复杂文本生成任务中表现出更精确和一致的指导。 |
| [^125] | [Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models](https://arxiv.org/abs/2403.04325) | 引入了Composition Score，一种基于模型的度量标准，用于量化句子理解中的含义合成程度，实验证明这一度量与大脑区域相关，揭示了含义合成在人类句子理解中的多方面性。 |
| [^126] | [Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution](https://arxiv.org/abs/2403.03121) | 大型语言模型中存在性别化情绪归因，反映了社会刻板印象。 |
| [^127] | [SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction](https://arxiv.org/abs/2403.01570) | 提出SERVAL，一个协同学习流水线，可以通过相互增强，实现LLMs和小模型的垂直能力无监督开发，从而改善领域特定垂直问题的零-shot预测能力。 |
| [^128] | [Emotion Analysis in NLP: Trends, Gaps and Roadmap for Future Directions](https://arxiv.org/abs/2403.01222) | NLP中情感分析领域存在着趋势和差距，未来方向需考虑情感任务定义、情感框架、情感主观性与NLP应用。 |
| [^129] | [ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies](https://arxiv.org/abs/2403.01139) | 设计了ParallelPARC流水线，利用大型语言模型生成复杂段落类比数据集，评估各种类比类型，并展示出人类在类比识别中的优势。 |
| [^130] | [Gradient-Free Adaptive Global Pruning for Pre-trained Language Models](https://arxiv.org/abs/2402.17946) | 提出了自适应全局剪枝（AdaGP）框架，通过重新定义全局剪枝过程为可管理的协调子问题，实现对大型语言模型的资源高效优化，显著提高性能。 |
| [^131] | [Retrieval is Accurate Generation](https://arxiv.org/abs/2402.17532) | 提出了一种从支持文档中选择上下文感知短语的新颖生成方法，通过迭代式自我强化提高训练数据准确性，在各种任务中表现优越，提高了开放式文本生成的质量。 |
| [^132] | [Predict the Next Word: <Humans exhibit uncertainty in this task and language models _____>](https://arxiv.org/abs/2402.17527) | 评估语言模型在预测下一个词时，是否能够复现人类在这项任务中展示的语言变化性 |
| [^133] | [DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem](https://arxiv.org/abs/2402.16159) | 提出了一种为开源软件系统量身定制的新颖命名实体识别技术，通过远程监督注释过程、语言启发、查找表、外部知识源和主动学习方法来提高模型性能，有效缓解了成本和专家标注人员稀缺问题，并在关系抽取下游任务中显著优于LLMs。 |
| [^134] | [KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge](https://arxiv.org/abs/2402.13605) | KorNAT是首个用于评估韩国国家对齐的基准，包括社会价值观和常识对齐两个方面，通过对社会价值和常识多项选择题的测试来评估模型的对齐程度。 |
| [^135] | [GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence](https://arxiv.org/abs/2402.12566) | GenAudit是一个工具，通过修订或删除未被参考文献支持的声明，并提供来自参考文献的证据，帮助修复语言模型输出中的事实错误。 |
| [^136] | [NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms](https://arxiv.org/abs/2402.12261) | 本研究通过创建一个多样化的最新英语新词资源，分析了大型语言模型对于新词的鲁棒性表现，并构建了一个基准用于评估模型对新词的泛化能力，结果显示机器翻译中模型性能会因引入新词而几乎减半。 |
| [^137] | [MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization](https://arxiv.org/abs/2402.11453) | MatPlotAgent介绍了一种基于LLM的自动化科学数据可视化框架，包括查询理解、代码生成和视觉反馈机制，并引入了MatPlotBench基准和GPT-4V评分方法。 |
| [^138] | [Can we soft prompt LLMs for graph learning tasks?](https://arxiv.org/abs/2402.10359) | 引入了GraphPrompter框架，通过软提示将图信息与LLMs对齐，以进一步探究LLMs理解图信息的潜力。 |
| [^139] | [Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning](https://arxiv.org/abs/2402.05808) | 本文提出了一种通过反向课程强化学习训练大型语言模型进行推理的新方法，通过学习正确演示并建立逐步的课程，实现了结果监督和过程监督的优化。 |
| [^140] | [Financial Report Chunking for Effective Retrieval Augmented Generation](https://arxiv.org/abs/2402.05131) | 本文提出了一种扩展的方法来切块财务报告，通过根据文档的结构元素组件进行切块，从而实现更有效的检索增强生成。这种方法可以优化切块大小，而无需调整，并提供了对整体上下文和准确性的评估以及对问答任务性能的影响。 |
| [^141] | [Synergizing Machine Learning & Symbolic Methods: A Survey on Hybrid Approaches to Natural Language Processing](https://arxiv.org/abs/2401.11972) | 本文调查了机器学习与符号方法在自然语言处理中的混合方法，以桥接二者的优势和劣势，提出了用于广泛NLP任务的最新混合方法。 |
| [^142] | [CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark](https://arxiv.org/abs/2401.11944) | CMMMU是一个旨在评估大型多模型模型在大学级学科知识和深思熟虑推理任务中表现的中文大规模多学科多模态理解基准，为填补在非英语环境中评估先进知识和推理能力的空白而设计。 |
| [^143] | [Majority or Minority: Data Imbalance Learning Method for Named Entity Recognition](https://arxiv.org/abs/2401.11431) | 提出了一种名为多数还是少数（MoM）学习的数据不平衡学习方法，针对命名实体识别任务中的多数类别和少数类别之间的挑战，能够提高少数类别的预测性能，而不影响多数类别的性能。 |
| [^144] | [GRAM: Global Reasoning for Multi-Page VQA](https://arxiv.org/abs/2401.03411) | GRAM方法通过引入文档级的指定层和可学习标记，实现了将单页模型扩展到多页面设置，促进信息跨页面的全局推理。 |
| [^145] | [Language Model Knowledge Distillation for Efficient Question Answering in Spanish](https://arxiv.org/abs/2312.04193) | 通过知识蒸馏，我们开发了SpanishTinyRoBERTa，一个基于RoBERTa的西班牙语压缩语言模型，用于提高西班牙语问答的效率。 |
| [^146] | [DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer](https://arxiv.org/abs/2312.03724) | 提出了差分私密离线提示调整（DP-OPT）解决大型语言模型调整提示时的隐私问题，通过在客户端调整提示并应用于云模型实现了隐私保护和数据传输 |
| [^147] | [Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation](https://arxiv.org/abs/2312.03003) | MobileGPT是一种创新的基于LLM的移动任务自动化工具，通过类人应用记忆模拟人类与移动应用的认知过程，实现任务程序的精确高效学习。 |
| [^148] | [Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework](https://arxiv.org/abs/2312.00029) | Bergeron提出了一个基于良知的对齆框架，能够提高大型语言模型对抗攻击的鲁棒性，无需额外参数微调。 |
| [^149] | [Self-Improving for Zero-Shot Named Entity Recognition with Large Language](https://arxiv.org/abs/2311.08921) | 本研究提出了一个无需训练的自我改进框架，利用未标记语料库激发大型语言模型的自我学习能力，从而推动零-shot命名实体识别性能边界。 |
| [^150] | [Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://arxiv.org/abs/2311.07914) | 调查综合审查了LLMs中基于知识图谱的增强技术，重点关注其在减轻幻觉方面的效果。 |
| [^151] | [HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models](https://arxiv.org/abs/2310.14566) | HallusionBench是一个专为评估大型视觉语言模型在图像背景推理中面临挑战的基准，通过引入新颖结构和量化分析，显示出GPT-4V取得了31.42%的准确率，远高于其他模型。 |
| [^152] | [Llemma: An Open Language Model For Mathematics](https://arxiv.org/abs/2310.10631) | Llemma是一个用于数学的开放语言模型，在MATH基准测试中表现优异，能够进行工具使用和形式定理证明，无需进一步微调。 |
| [^153] | [Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model](https://arxiv.org/abs/2310.05155) | 提出了Toolink，一个通过链式解决方法首先创建工具包，再集成工具规划和调用的框架，成功通过对ChatGPT和CoS-GPT的实验，打造了LLaMA-CoS，一个具有先进工具规划和调用能力的强大开源模型。 |
| [^154] | [Compressing LLMs: The Truth is Rarely Pure and Never Simple](https://arxiv.org/abs/2310.01382) | 本研究重新评估了现有最先进的压缩LLM方法对稠密LLM的有效性，并引入了一个新的压缩LLM基准来重新定义评估协议。 |
| [^155] | [Analyzing and Mitigating Object Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2310.00754) | 提出了一种名为LVLM Hallucination Revisor（LURE）的算法，通过重新构建较少具有幻觉性的描述，来事后纠正大型视觉语言模型中的物体幻觉问题。 |
| [^156] | [Can LLM-Generated Misinformation Be Detected?](https://arxiv.org/abs/2309.13788) | LLM生成的虚假信息可能比人类撰写的虚假信息更难以检测，具有更具欺骗性的风格，可能造成更多危害。 |
| [^157] | [BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models](https://arxiv.org/abs/2309.13345) | 提出了BAMBOO基准来全面评估大语言模型对长文本的建模能力，包含10个数据集从5个不同长文本理解任务中提取，涵盖了LLMs的核心能力和各个领域。 |
| [^158] | [LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for Self-supervised Representations of French Speech](https://arxiv.org/abs/2309.05472) | LeBenchmark 2.0是一个开源框架，用于评估和构建法语语音技术的自监督学习，提供大规模语料库、预训练模型和评估协议，并探讨了预训练SSL模型的独特视角。 |
| [^159] | [Frequency effects in Linear Discriminative Learning](https://arxiv.org/abs/2306.11044) | 本研究展示了如何利用频率信息学习(FIL)获得一种高效、考虑频率的形式和含义之间映射的方法，该方法在计算上更便宜，同时能有效地近似逐步解决方案。 |
| [^160] | [Open Brain AI. Automatic Language Assessment](https://arxiv.org/abs/2306.06693) | Open Brain AI是一个利用创新AI技术自动分析多语种口头和书面言语的计算平台，可以快速且自动化地分析语言，减轻临床医生的负担。 |
| [^161] | [Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models](https://arxiv.org/abs/2305.12544) | 本文提出了14个不由大型语言模型直接解决的自然语言处理研究领域，包含45个新的研究方向，为NLP研究人员提供了丰富的探索方向。 |
| [^162] | [A low latency attention module for streaming self-supervised speech representation learning](https://arxiv.org/abs/2302.13451) | 本文提出了用于流式自监督语音表示学习的低延迟注意力模块，实现了在低延迟的情况下进行实时推断。 |
| [^163] | [Representation Deficiency in Masked Language Modeling](https://arxiv.org/abs/2302.02060) | 掩码语言建模中的 $\texttt{[MASK]} $符号会导致模型维度过度分配，造成真实标记的表示不足，本文提出了MAE-LM来解决这一问题 |
| [^164] | [Source-Free Domain Adaptation for Question Answering with Masked Self-training](https://arxiv.org/abs/2212.09563) | 提出了一种面向问题回答的无源领域自适应方法，通过独特的蒙版模块实现自训练，成功解决了无法访问源域数据的挑战，提升了模型在目标领域上的性能。 |
| [^165] | [Local Interpretations for Explainable Natural Language Processing: A Survey](https://arxiv.org/abs/2103.11072) | 本文调查了改善深度神经网络在自然语言处理任务中可解释性的各种方法，特别关注局部解释，包括与相关输入特征相关的预测解释、自然语言解释以及探测模型隐藏状态和词表示。 |
| [^166] | [SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering.](http://arxiv.org/abs/2401.13463) | SpeechDPR是第一个用于开放领域口语问答的端到端框架，能够从口语存档中检索可能包含答案的段落。通过融合无监督ASR和文本密集检索器的知识，SpeechDPR能够获得较好的性能，并且在UASR性能较差时表现更加鲁棒。 |
| [^167] | [Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model.](http://arxiv.org/abs/2401.12873) | 本研究探索了利用质量估计作为奖励模型来预测人类偏好以改善机器翻译的潜力。我们发现基于质量估计的反馈训练存在过度优化问题，采用启发式规则来检测错误翻译并对质量估计模型进行惩罚以解决该问题。 |
| [^168] | [Claim Detection for Automated Fact-checking: A Survey on Monolingual, Multilingual and Cross-Lingual Research.](http://arxiv.org/abs/2401.11969) | 本综述调研了自动事实核查中索赔检测的现有工作，特别关注多语言数据和方法。这是一个具有挑战性但富有成果的研究方向，需要更通用的解决方案来对抗跨多语言和模态的不实信息。 |
| [^169] | [TrustLLM: Trustworthiness in Large Language Models.](http://arxiv.org/abs/2401.05561) | TrustLLM是对大型语言模型中可信性的全面研究，包括可信性原则的提出、建立基准的方法、评估主流语言模型的可信性，以及对未来挑战的讨论。 |
| [^170] | [Evaluating Language Model Agency through Negotiations.](http://arxiv.org/abs/2401.04536) | 本研究通过谈判游戏的视角，提出共同评估语言模型（LM）的性能和对齐，以更好地反映真实世界的部署条件，并避免数据泄漏。通过评估多轮次和跨模型交互，我们发现了LM的自我对弈和交叉对弈性能。 |
| [^171] | [Large Language Models as Visual Cross-Domain Learners.](http://arxiv.org/abs/2401.03253) | 本研究提出了大型语言模型作为视觉跨领域学习器（LLaVO），通过将图像转换为文本描述，使用大型语言模型进行训练和微调，实现了在跨领域任务中减少领域转移的效果。 |
| [^172] | [Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning.](http://arxiv.org/abs/2312.05720) | 本文引入了一种创新的方法，在联邦学习中利用语言模型的池化层输入来实现对隐私攻击的改进。通过恢复池化层输入，这种方法能够在不同的批处理大小下提供更高的文本恢复率，从而提供更细致和有效的见解。 |
| [^173] | [The Expressive Power of Low-Rank Adaptation.](http://arxiv.org/abs/2310.17513) | 本文分析了低秩适应（LoRA）的表达能力，证明了对于全连接神经网络，当LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度）时，LoRA可以使任何模型f准确表示任何较小的目标模型f。对于Transformer网络，通过rank-（嵌入大小/ 2）的LoRA适配器可以使任何模型适应于相同大小的目标模型。 |
| [^174] | [CLEX: Continuous Length Extrapolation for Large Language Models.](http://arxiv.org/abs/2310.16450) | CLEX是一种针对大型语言模型的持续长度外推方法，通过将位置嵌入缩放方法推广到连续动态建模，克服了当前方法在特定长度上的局限性。 |
| [^175] | [GPT-who: An Information Density-based Machine-Generated Text Detector.](http://arxiv.org/abs/2310.06202) | GPT-who是一种基于统一信息密度原则的机器生成文本检测器，利用基于统一信息密度原则的特征来建模每个语言模型和人类作者的独特统计特征，以实现准确的作者归属。在多个领域中，GPT-who的性能超过了其他最先进的检测器，且具有计算成本低廉和可解释性。 |
| [^176] | [Zero Resource Code-switched Speech Benchmark Using Speech Utterance Pairs For Multiple Spoken Languages.](http://arxiv.org/abs/2310.03018) | 本论文介绍了一个新的零资源切换语音基准，用于评估自监督语音编码器的切换语言能力，研究了预训练语言和模型大小对基准性能的影响，结果显示多语言预训练的语音编码器在切换语言场景中表现优于单语变体，但仍有改进空间。 |
| [^177] | [MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool.](http://arxiv.org/abs/2309.16701) | 本文提出了一个名为MVMR的任务，旨在给定文本查询从大量视频集中定位视频帧。我们通过已有数据集进行相似性筛选来构建数据集，并引入三个MVMR数据集。我们采用了嵌入式文本相似度匹配和视频-语言对齐技术来计算相关性得分，并为MVMR任务开发了一个强大的模型，Reliable Mutual Matching Network (RMMN)。 |
| [^178] | [Large Language Models and Control Mechanisms Improve Text Readability of Biomedical Abstracts.](http://arxiv.org/abs/2309.13202) | 本研究使用大型语言模型和控制机制改善了生物医学摘要的文本可读性，具体包括领域微调和基于提示的学习方法，以及应用于编码器-解码器模型和GPT模型的控制令牌机制。 |
| [^179] | [DreamLLM: Synergistic Multimodal Comprehension and Creation.](http://arxiv.org/abs/2309.11499) | DreamLLM是一种学习框架，实现了多模态理解与创作的协同效应。通过直接采样生成语言和图像的生成模型，避免了信息损失，并获得了更全面的多模态理解。此外，DreamLLM能够生成自由形式交织内容，展现了其在零样本多模态学习任务中的卓越性能。 |
| [^180] | [OpenChat: Advancing Open-source Language Models with Mixed-Quality Data.](http://arxiv.org/abs/2309.11235) | OpenChat是一种用于推进开源语言模型的新框架，能够利用混合质量数据中的信息并简化RLFT方法的求解过程。 |
| [^181] | [Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms.](http://arxiv.org/abs/2309.05961) | 本文通过对六个社区问答平台的研究，发现了查询的元数据、问题构成方式和用户互动水平与第一个回答时间之间的关联，并利用机器学习模型预测查询是否能够迅速获得回答。 |
| [^182] | [Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior.](http://arxiv.org/abs/2309.00359) | 该论文提出了使用大型内容和行为模型来理解、模拟和优化内容和行为。大型语言模型虽然在任务泛化能力方面取得了进展，但还无法解决预测和优化通信以实现期望接收者行为的问题。其中的一个原因可能是训练语料库中缺少"行为标记"。 |
| [^183] | [OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models.](http://arxiv.org/abs/2308.13137) | OmniQuant是一种用于大型语言模型的全向校准量化技术，通过优化各种量化参数实现了良好的性能，并保持了计算效率。 |
| [^184] | [LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles.](http://arxiv.org/abs/2308.10855) | LatEval是一个新颖的LLMs评估基准，通过侧思维谜题挑战模型的横向思考能力，在交互过程中考验模型提出问题的质量和整合信息解决问题的能力。研究发现，几乎所有LLMs在横向思考方面存在困难，即使是最先进的GPT-4模型相比人类也有明显差距。这个基准测试对于有效的AI助理至关重要。 |
| [^185] | [More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes.](http://arxiv.org/abs/2308.01313) | 本文借鉴了人类视觉感知过程，提出了一种通过推断和调节上下文属性来改进零样本图像分类的方法。通过给CLIP提供上下文属性，可以减轻对虚假特征的依赖，进而提高零样本分类的准确性。 |
| [^186] | [RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment.](http://arxiv.org/abs/2307.12950) | RLCD是一种用于语言模型对齐的强化学习方法，利用对比蒸馏训练偏好模型，可以使语言模型在不使用人类反馈的情况下遵循自然语言规则。在多个对齐任务和不同规模的模型上，RLCD优于其他基线方法。 |
| [^187] | [In-context Autoencoder for Context Compression in a Large Language Model.](http://arxiv.org/abs/2307.06945) | 在大型语言模型中，我们提出了一种称为In-context Autoencoder (ICAE)的上下文自编码器，它通过将长上下文压缩为有限数量的内存槽，实现了$4\times$的上下文压缩，并能够根据内存槽进行条件处理以响应各种提示。 |
| [^188] | [Pluggable Neural Machine Translation Models via Memory-augmented Adapters.](http://arxiv.org/abs/2307.06029) | 通过记忆增强的适配器，我们提出了一种可插拔的方法来控制神经机器翻译模型的生成行为。实验证明，我们的方法可以胜过几个代表性的可插拔基准模型。 |
| [^189] | [Streamlining Social Media Information Retrieval for Public Health Research with Deep Learning.](http://arxiv.org/abs/2306.16001) | 本研究介绍了一个使用深度学习简化社交媒体信息检索的框架，通过识别医学实体、标准化实体和分配UMLS概念，构建了一个用于COVID-19相关推文的症状词典。 |
| [^190] | [Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs.](http://arxiv.org/abs/2306.13063) | 本研究就不需要微调模型或访问专有信息的方法进行置信度引导进行了探讨，通过研究发现LLMs往往展现出高度的过度自信。 |
| [^191] | [MOFI: Learning Image Representations from Noisy Entity Annotated Images.](http://arxiv.org/abs/2306.07952) | MOFI 提出了一种新的方法，自动从含噪图像文本对中为图像指定实体标签，创建了一个新的大规模数据集 I2E，通过研究不同的训练配方，学习到了能够有效学习图像表示的模型。 |
| [^192] | [Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation.](http://arxiv.org/abs/2305.15852) | 本文对大型语言模型的自相矛盾幻觉进行了评估、检测和缓解，探究了这一幻觉形式的普遍存在性。通过设计框架有效触发自相矛盾，发现不同语言模型中这种现象都频繁出现。ChatGPT和GPT-4能够准确识别自相矛盾，而Vicuna-13B则有些困难。 |
| [^193] | [Detecting Propaganda Techniques in Code-Switched Social Media Text.](http://arxiv.org/abs/2305.14534) | 本文提出了一个新任务，即在混合语言的社交媒体文本中检测宣传技术。为了支持这一任务，作者创建了一个包含1030个文本的英语和罗马乌尔混合语言的语料库，并进行了一系列实验。 |
| [^194] | [LLM Itself Can Read and Generate CXR Images.](http://arxiv.org/abs/2305.11490) | 该论文提出了一种新方法，可以在不需要进行结构更改、额外训练、或训练专门网络的情况下，通过微调预先训练的LLM来读取和生成像文本一样的图像，并应用于胸部X线（CXR）图像的生成任务中。 |
| [^195] | [Using LLM-assisted Annotation for Corpus Linguistics: A Case Study of Local Grammar Analysis.](http://arxiv.org/abs/2305.08339) | 本文研究了使用基于大语言模型的聊天机器人自动标注文本的潜力，重点考察了从本地语法角度观察道歉言语行为构成的功能元素的程度，并比较了不同模型在注释任务中的表现，结果表明Bing聊天机器人在任务中表现优于ChatGPT和人类标注员。 |
| [^196] | [When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks.](http://arxiv.org/abs/2305.06626) | 本文通过预测单个标注者的打分，并结合文本目标群体的预测，模拟了目标群体成员的意见，通过使用他们的人口统计学数据和在线意见预测标注者的打分，在仇恨言论检测等主观任务中提高了模型性能。 |
| [^197] | [Navigation as Attackers Wish? Towards Building Byzantine-Robust Embodied Agents under Federated Learning.](http://arxiv.org/abs/2211.14769) | 本文研究了联邦学习体系下代理人学习中可能出现的攻击和防御策略，建立了全联邦拜占庭鲁棒的代理人学习模型。其中，导航即攻击者所愿（NAW）是一种简单而有效的攻击策略，而基于离群点检测的防御训练方法可以有效减轻NAW攻击的影响，提高代理人学习的全局鲁棒性。 |
| [^198] | [What can we know about that which we cannot even imagine?.](http://arxiv.org/abs/2208.03886) | 这篇文章探讨了关于智能、人类语言和人类数学的问题，强调了人类语言的局限性，以及我们能否对我们无法想象的事物有任何了解。 |
| [^199] | [Representation Learning for Weakly Supervised Relation Extraction.](http://arxiv.org/abs/2105.00815) | 本研究关注如何通过无监督预训练来改进在有限训练数据情况下的监督基线系统性能，并分析了传统手工特征在关系抽取中可能存在的数据稀疏性问题。 |

# 详细

[^1]: 可执行代码行动能够激发更出色的LLM智能体

    Executable Code Actions Elicit Better LLM Agents

    [https://rss.arxiv.org/abs/2402.01030](https://rss.arxiv.org/abs/2402.01030)

    使用可执行的Python代码整合LLM智能体行动，提升了成功率高达20%。

    

    大型语言模型（LLM）智能体具备执行广泛行动的能力，如调用工具和控制机器人等，在解决现实世界的挑战中显示出巨大潜力。LLM智能体通常通过生成JSON或文本的预定义格式来产生行动，这通常受限于受限制的行动空间（例如，预定义工具的范围）和受限的灵活性（例如，无法组合多个工具）。本研究提出使用可执行的Python代码将LLM智能体的行动整合到一个统一的行动空间中（CodeAct）。CodeAct与Python解释器集成，可以执行代码行动，并通过多轮交互在新的观察中动态修订先前的行动或发出新的行动。我们对17个LLM在API-Bank和新编制的基准测试中进行了广泛分析，结果显示CodeAct的性能超过了广泛使用的替代方案（成功率高出20%）。CodeAct的令人鼓舞的表现激励我们建立了一个开源的...

    Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-sourc
    
[^2]: 从像素到洞察: 在大型基础模型时代自动图表理解的调查

    From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models

    [https://arxiv.org/abs/2403.12027](https://arxiv.org/abs/2403.12027)

    近年来，随着大型基础模型的兴起，自动图表理解取得了显著进展，本调查论文概述了在这些基础模型背景下图表理解领域的最新发展、挑战和未来方向

    

    数据可视化以图表形式在数据分析中扮演着关键角色，提供关键洞察并帮助做出明智决策。随着近年大型基础模型的崛起，自动图表理解取得了显著进展。基础模型，如大型语言模型(LLMs)，已经在各种自然语言处理（NLP）任务中实现了革命，并越来越多地应用于图表理解任务。本调查论文全面介绍了最新进展、挑战和未来方向，探讨了这些基础模型背景下图表理解的内容。

    arXiv:2403.12027v1 Announce Type: cross  Abstract: Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making. Automatic chart understanding has witnessed significant advancements with the rise of large foundation models in recent years. Foundation models, such as large language models (LLMs), have revolutionized various natural language processing (NLP) tasks and are increasingly being applied to chart understanding tasks. This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these foundation models. The paper begins by defining chart understanding, outlining problem formulations, and discussing fundamental building blocks crucial for studying chart understanding tasks. In the section on tasks and datasets, we explore various tasks within chart understanding and discuss their evaluation metrics a
    
[^3]: FlexCap：在图像中生成丰富、本地化和灵活的标题

    FlexCap: Generating Rich, Localized, and Flexible Captions in Images

    [https://arxiv.org/abs/2403.12026](https://arxiv.org/abs/2403.12026)

    FlexCap模型能够生成图像中具有不同长度的区域描述，在密集字幕任务和视觉问答系统中表现出优越性能。

    

    我们介绍了一种多功能的$\textit{灵活字幕}$视觉-语言模型（VLM），能够生成长度不同的特定区域描述。该模型FlexCap经过训练，可为输入的边界框生成长度条件的字幕，从而可以控制其输出的信息密度，描述范围从简洁的对象标签到详细的字幕。为了实现这一点，我们从带字幕的图像开始创建了大规模的图像区域描述训练数据集。这种灵活的字幕功能有几个宝贵的应用。首先，FlexCap在Visual Genome数据集上的密集字幕任务中表现出优越性能。其次，可以通过采用FlexCap生成本地化描述作为大型语言模型的输入来构建视觉问答（VQA）系统。由此产生的系统在许多VQ上实现了最新技术的零样本性能。

    arXiv:2403.12026v1 Announce Type: cross  Abstract: We introduce a versatile $\textit{flexible-captioning}$ vision-language model (VLM) capable of generating region-specific descriptions of varying lengths. The model, FlexCap, is trained to produce length-conditioned captions for input bounding boxes, and this allows control over the information density of its output, with descriptions ranging from concise object labels to detailed captions. To achieve this we create large-scale training datasets of image region descriptions of varying length, starting from captioned images. This flexible-captioning capability has several valuable applications.   First, FlexCap demonstrates superior performance in dense captioning tasks on the Visual Genome dataset. Second, a visual question answering (VQA) system can be built by employing FlexCap to generate localized descriptions as inputs to a large language model. The resulting system achieves state-of-the-art zero-shot performance on a number of VQ
    
[^4]: 一个用于揭示大型语言模型中健康公平危害和偏见的工具箱

    A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models

    [https://arxiv.org/abs/2403.12025](https://arxiv.org/abs/2403.12025)

    提出了用于揭示大型语言模型中健康公平危害和偏见的资源和方法，进行了实证案例研究，并提出了用于人类评估LLM生成答案偏见的多因子框架以及EquityMedQA数据集。

    

    大型语言模型（LLMs）有着为复杂的健康信息需求提供服务的巨大潜力，但同时也有可能引入危害并加剧健康不平等。可靠地评估与公平相关的模型失灵是发展促进健康公平系统的关键步骤。在这项工作中，我们提出了用于揭示可能导致LLM生成的长篇答案中的公平相关危害的偏见的资源和方法，并使用Med-PaLM 2进行了一项实证案例研究，这是迄今为止在该领域进行的最大规模的人类评估研究。我们的贡献包括用于人类评估LLM生成答案偏见的多因子框架，以及EquityMedQA，一个包含七个新发布数据集的收集，其中既包括手动策划又包括LLM生成的问题，丰富了对抗性查询。我们的人类评估框架和数据集设计过程都根植于实际

    arXiv:2403.12025v1 Announce Type: cross  Abstract: Large language models (LLMs) hold immense promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities. Reliably evaluating equity-related model failures is a critical step toward developing systems that promote health equity. In this work, we present resources and methodologies for surfacing biases with potential to precipitate equity-related harms in long-form, LLM-generated answers to medical questions and then conduct an empirical case study with Med-PaLM 2, resulting in the largest human evaluation study in this area to date. Our contributions include a multifactorial framework for human assessment of LLM-generated answers for biases, and EquityMedQA, a collection of seven newly-released datasets comprising both manually-curated and LLM-generated questions enriched for adversarial queries. Both our human assessment framework and dataset design process are grounde
    
[^5]: 探索和规范四种文字系统，增强福建话的双语翻译

    Enhancing Hokkien Dual Translation by Exploring and Standardizing of Four Writing Systems

    [https://arxiv.org/abs/2403.12024](https://arxiv.org/abs/2403.12024)

    通过开发双语翻译模型，探索台湾福建话和繁体中文/英文之间的翻译，引入限制单语语料库并将所有台湾福建话文字系统规范为福建话汉字，从而提高翻译性能。

    

    arXiv:2403.12024v1 公告类型: 新 提要: 机器翻译主要集中在高资源语言（HRLs），而低资源语言（LRLs）如台湾福建话相对未被深入探索。本研究旨在通过开发台湾福建话与繁体中文和英文之间的双语翻译模型来解决这一差距。我们使用一个专门针对繁体中文的预训练LLaMA2-7B模型来利用台湾福建话汉字与繁体中文之间的拼音相似性。我们的全面实验涉及台湾福建话各种文字系统之间的翻译任务，以及台湾福建话与其他高资源语言之间的翻译。我们发现使用有限的单语语料库还进一步改善了模型对台湾福建话的能力。然后，我们利用我们的翻译模型将所有台湾福建话文字系统规范为福建话汉字，从而进一步提高性能。

    arXiv:2403.12024v1 Announce Type: new  Abstract: Machine translation focuses mainly on high-resource languages (HRLs), while low-resource languages (LRLs) like Taiwanese Hokkien are relatively under-explored. This study aims to address this gap by developing a dual translation model between Taiwanese Hokkien and both Traditional Mandarin Chinese and English. We employ a pre-trained LLaMA2-7B model specialized in Traditional Mandarin Chinese to leverage the orthographic similarities between Taiwanese Hokkien Han and Traditional Mandarin Chinese. Our comprehensive experiments involve translation tasks across various writing systems of Taiwanese Hokkien and between Taiwanese Hokkien and other HRLs. We find that the use of a limited monolingual corpus also further improve the model's Taiwanese Hokkien capabilities. We then utilize our translation model to standardize all Taiwanese Hokkien writing systems into Hokkien Han, resulting in further performance improvements. Additionally, we intr
    
[^6]: 监督微调作为逆强化学习

    Supervised Fine-Tuning as Inverse Reinforcement Learning

    [https://arxiv.org/abs/2403.12017](https://arxiv.org/abs/2403.12017)

    本论文提出将逆强化学习和模仿学习的见解结合，探讨了使用演示数据集对齐大语言模型的方法，并对不同方法的性能进行了分析。

    

    大语言模型（LLMs）对齐的主流方法通常依赖于人类或AI反馈，并假设可以访问特定类型的偏好数据集。在我们的工作中，我们质疑这些数据集的有效性，并探讨了各种情景下与专家演示对齐更为现实的方法。我们构建了一个顺序决策框架，以演示数据集为基础来规划对齐LLMs的问题。借鉴逆强化学习和模仿学习的见解，我们引入了各种方法来最小化LLM对齐任务中的差异。我们的分析突出了这些不同方法的覆盖率和寻找模式行为。此外，我们考察了经典监督微调方法的利弊，并详细阐述了不同方法表现突出的情景。

    arXiv:2403.12017v1 Announce Type: cross  Abstract: The prevailing approach to aligning Large Language Models (LLMs) typically relies on human or AI feedback and assumes access to specific types of preference datasets. In our work, we question the efficacy of such datasets and explore various scenarios where alignment with expert demonstrations proves more realistic. We build a sequential decision-making framework to formulate the problem of aligning LLMs using demonstration datasets. Drawing insights from inverse reinforcement learning and imitation learning, we introduce various approaches for divergence minimization in the LLM alignment tasks. Our analysis highlights the mass-covering and mode-seeking behaviors of these different approaches. Inclusively, we examine the pros and cons of the classical supervised fine-tuning method, elaborating on scenarios where different methods shine.
    
[^7]: EnvGen: 通过LLMs生成和调整环境以训练具身体的代理

    EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents

    [https://arxiv.org/abs/2403.12014](https://arxiv.org/abs/2403.12014)

    EnvGen提出了一种新的框架，利用LLMs的推理能力自适应创建训练环境，帮助小型具身体RL代理在弱点方面学习有用技能。

    

    最近有关通过互动进行具身体学习的最新方法直接采用大型语言模型（LLMs）作为代理，以确定环境中的下一步。LLM代理由于其世界知识和推理能力，比基于强化学习（RL）的以往较小的代理表现更强；但频繁调用LLMs速度慢且昂贵。我们提出EnvGen，一个处理这个问题的新框架。首先，我们提示一个LLM生成训练环境，使代理可以快速并行学习不同任务。具体而言，LLM获得任务描述和模拟器目标，然后被要求生成一组环境配置。

    arXiv:2403.12014v1 Announce Type: cross  Abstract: Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at? We propose EnvGen, a novel framework to address this question. First, we prompt an LLM to generate training environments that allow agents to quickly learn different tasks in parallel. Concretely, the LLM is given the task description and simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g
    
[^8]: 利用生成式知识提取、基于图的表示和多模态智能图推理加速科学发现

    Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning

    [https://arxiv.org/abs/2403.11996](https://arxiv.org/abs/2403.11996)

    利用生成式人工智能和图算法加速科学发现，揭示论文之间的深入跨学科关系，并提出了新颖的材料设计。

    

    利用生成式人工智能，我们将一组涉及生物材料领域的1,000篇科学论文转化为详细的本体知识图，揭示了它们固有的无标度特性。通过基于节点相似性和介数中心性的组合排名，探测不同概念之间的图遍历路径，我们揭示了深入的跨学科关系，可用于回答查询，识别知识中的空白，并提出前所未见的材料设计及其行为。一项比较揭示了生物材料和贝多芬第九交响曲之间的详细结构相似之处，突显了通过同构映射共享复杂性模式。该算法进一步创建了一种创新的基于分级菌丝体的复合材料，将图采样的联合合成原理与康定斯基《第七组成》中提取的原则相结合

    arXiv:2403.11996v1 Announce Type: cross  Abstract: Using generative Artificial Intelligence (AI), we transformed a set of 1,000 scientific papers in the area of biological materials into detailed ontological knowledge graphs, revealing their inherently scale-free nature. Using graph traversal path detection between dissimilar concepts based on combinatorial ranking of node similarity and betweenness centrality, we reveal deep insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, and propose never-before-seen material designs and their behaviors. One comparison revealed detailed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. The algorithm further created an innovative hierarchical mycelium-based composite that incorporates joint synthesis of graph sampling with principles extracted from Kandinsky's Composition VII p
    
[^9]: 使用生成文本模型为教学评估创建定性代码手册

    Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching

    [https://arxiv.org/abs/2403.11984](https://arxiv.org/abs/2403.11984)

    本文介绍了一种使用自然语言处理和大型语言模型分析教学评估的新方法，可以从大量反馈中提取、嵌入、聚类和总结SETs。

    

    反馈是改进的关键方面。然而，当有来自多个来源的大量反馈时，将信息提炼成可操作的见解可能会很困难。本文讨论了一种使用自然语言处理（NLP）和大型语言模型（LLMs）分析教学评估（SETs）的新方法。我们通过将其应用于一所大型公立大学的5000个SETs语料库来演示该方法。

    arXiv:2403.11984v1 Announce Type: cross  Abstract: Feedback is a critical aspect of improvement. Unfortunately, when there is a lot of feedback from multiple sources, it can be difficult to distill the information into actionable insights. Consider student evaluations of teaching (SETs), which are important sources of feedback for educators. They can give instructors insights into what worked during a semester. A collection of SETs can also be useful to administrators as signals for courses or entire programs. However, on a large scale as in high-enrollment courses or administrative records over several years, the volume of SETs can render them difficult to analyze. In this paper, we discuss a novel method for analyzing SETs using natural language processing (NLP) and large language models (LLMs). We demonstrate the method by applying it to a corpus of 5,000 SETs from a large public university. We show that the method can be used to extract, embed, cluster, and summarize the SETs to id
    
[^10]: 基于深度学习的语言演化

    Language Evolution with Deep Learning

    [https://arxiv.org/abs/2403.11958](https://arxiv.org/abs/2403.11958)

    通过深度学习模型，本研究探讨了语言演化的计算模拟，以及其对于模拟语言演化的帮助和局限性，适用于语言学家和认知科学家。

    

    计算建模在语言演化研究中起着至关重要的作用，旨在模拟触发结构化语言在受控环境中出现的条件和学习过程。本章探讨了另一类计算模型——深度学习模型，这些模型最近在机器学习领域引起了革命。本章介绍了深度学习和强化学习方法的基本概念，并总结了它们在模拟语言演化方面的帮助。还讨论了关键发现、局限性和最近尝试构建逼真模拟的工作。本章主要面向寻求将深度学习作为研究语言演化的工具的语言学家和认知科学家。

    arXiv:2403.11958v1 Announce Type: new  Abstract: Computational modeling plays an essential role in the study of language emergence. It aims to simulate the conditions and learning processes that could trigger the emergence of a structured language within a simulated controlled environment. Several methods have been used to investigate the origin of our language, including agent-based systems, Bayesian agents, genetic algorithms, and rule-based systems. This chapter explores another class of computational models that have recently revolutionized the field of machine learning: deep learning models. The chapter introduces the basic concepts of deep and reinforcement learning methods and summarizes their helpfulness for simulating language emergence. It also discusses the key findings, limitations, and recent attempts to build realistic simulations. This chapter targets linguists and cognitive scientists seeking an introduction to deep learning as a tool to investigate language evolution.
    
[^11]: 使用多语言句子嵌入进行自适应双语对齐

    Adaptative Bilingual Aligning Using Multilingual Sentence Embedding

    [https://arxiv.org/abs/2403.11921](https://arxiv.org/abs/2403.11921)

    提出了一种使用句子嵌入进行自适应双语对齐的系统，相对于现有技术，其在处理片段性平行文本时表现出与之等效的结果，且具有准线性复杂度。

    

    在本文中，我们提出了一种自适应的双文本对齐系统，名为AIlign。这个对齐器依赖于句子嵌入来提取可靠的锚点，这些锚点可以指导对齐路径，即使对于其平行性是片段性的、不严格单调的文本也是如此。在多个数据集上的实验中，我们展示了AIlign实现了与现有技术等效的结果，且具有准线性的复杂度。此外，AIlign能够处理那些仅在局部满足平行性和单调性属性的文本，这与最近的系统如Vecalign或Bertalign不同。

    arXiv:2403.11921v1 Announce Type: new  Abstract: In this paper, we present an adaptive bitextual alignment system called AIlign. This aligner relies on sentence embeddings to extract reliable anchor points that can guide the alignment path, even for texts whose parallelism is fragmentary and not strictly monotonic. In an experiment on several datasets, we show that AIlign achieves results equivalent to the state of the art, with quasi-linear complexity. In addition, AIlign is able to handle texts whose parallelism and monotonicity properties are only satisfied locally, unlike recent systems such as Vecalign or Bertalign.
    
[^12]: Tur[k]ingBench：用于网络代理的挑战基准测试

    Tur[k]ingBench: A Challenge Benchmark for Web Agents

    [https://arxiv.org/abs/2403.11905](https://arxiv.org/abs/2403.11905)

    Tur[k]ingBench是一个挑战性的网络代理基准测试，用于评估最先进的多模态模型在处理包含文本指示和多模态上下文的复杂任务时的泛化能力。

    

    最近的聊天机器人展示了在原始文本形式下理解和交流的令人印象深刻的能力。然而，世界上不仅仅是原始文本。例如，人们在网页上花费大量时间，在这些网页上，文本与其他形式交织在一起，并以各种复杂互动的形式完成任务。最先进的多模型是否能够推广到这种复杂的领域呢？为了回答这个问题，我们介绍了TurkingBench，一个由包含多模态背景的文本说明制定的任务基准。与现有的使用人工合成的网页的工作不同，这里我们使用最初设计用于各种注释目的的自然HTML页面。每个任务的HTML说明也被实例化为各种值（从众包任务获得）以形成任务的新实例。这个基准包含32.2K个实例。

    arXiv:2403.11905v1 Announce Type: new  Abstract: Recent chatbots have demonstrated impressive ability to understand and communicate in raw-text form. However, there is more to the world than raw text. For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions. Can state-of-the-art multi-modal models generalize to such complex domains?   To address this question, we introduce TurkingBench, a benchmark of tasks formulated as web pages containing textual instructions with multi-modal context. Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes. The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task. This benchmark contains 32.2K instanc
    
[^13]: CICLe: 适应上下文的大规模多类食品风险分类学习

    CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification

    [https://arxiv.org/abs/2403.11904](https://arxiv.org/abs/2403.11904)

    该论文提出了CICLe框架，基于适应上下文学习的方式，在大规模多类食品风险分类中取得了较好的效果，提出了基于符合预测的LLM-in-the-loop框架，可以提高基本分类器的性能，并减少能源消耗。

    

    受污染或掺假食品对人类健康构成重大风险。在给定了用于训练的标记网络文本集的情况下，可以应用机器学习和自然语言处理来自动检测这种风险。我们发布了一个包含7,546个描述公共食品召回公告的短文本数据集。每个文本都经过手动标记，分为两个粒度级别（粗粒度和细粒度），用于表示召回对应的食品产品和危害。我们描述了数据集并对朴素、传统和Transformer模型进行了基准测试。基于我们的分析，基于tf-idf表示的逻辑回归在支持较低的类别上优于RoBERTa和XLM-R。最后，我们讨论了不同的提示策略，并提出了一种基于符合预测的LLM-in-the-loop框架，这可以提高基本分类器的性能，同时减少了与普通提示相比的能源消耗。

    arXiv:2403.11904v1 Announce Type: new  Abstract: Contaminated or adulterated food poses a substantial risk to human health. Given sets of labeled web texts for training, Machine Learning and Natural Language Processing can be applied to automatically detect such risks. We publish a dataset of 7,546 short texts describing public food recall announcements. Each text is manually labeled, on two granularity levels (coarse and fine), for food products and hazards that the recall corresponds to. We describe the dataset and benchmark naive, traditional, and Transformer models. Based on our analysis, Logistic Regression based on a tf-idf representation outperforms RoBERTa and XLM-R on classes with low support. Finally, we discuss different prompting strategies and present an LLM-in-the-loop framework, based on Conformal Prediction, which boosts the performance of the base classifier while reducing energy consumption compared to normal prompting.
    
[^14]: 对声明分解进行更深入的研究

    A Closer Look at Claim Decomposition

    [https://arxiv.org/abs/2403.11903](https://arxiv.org/abs/2403.11903)

    通过研究声明分解方法，发现在评估文本支持时，分解方法对结果具有影响，提出了一种新的评估方法DecompScore和基于LLM的生成分解方法。

    

    随着生成文本变得更加普遍，评估此类文本在外部知识来源支持的重要性日益增强。许多评估文本支持的方法依赖于某种将文本分解为其各个子声明并将其与可信参考进行评分的方法。我们研究了各种声明分解方法，尤其是基于LLM的方法，如何影响最近提出的FActScore等评估方法的结果，发现它对所使用的分解方法很敏感。这种敏感性的出现是因为这样的度量指标将总体文本支持归因于生成文本的模型，尽管出错也可能来自于度量的分解步骤。为了衡量分解质量，我们引入了一种适应于FActScore的评估方法，我们称之为DecompScore。然后，我们提出了一种基于LLM的生成分解方法，受到伯特兰德·罗素关于逻辑原子性的理论的启发。

    arXiv:2403.11903v1 Announce Type: new  Abstract: As generated text becomes more commonplace, it is increasingly important to evaluate how well-supported such text is by external knowledge sources. Many approaches for evaluating textual support rely on some method for decomposing text into its individual subclaims which are scored against a trusted reference. We investigate how various methods of claim decomposition -- especially LLM-based methods -- affect the result of an evaluation approach such as the recently proposed FActScore, finding that it is sensitive to the decomposition method used. This sensitivity arises because such metrics attribute overall textual support to the model that generated the text even though error can also come from the metric's decomposition step. To measure decomposition quality, we introduce an adaptation of FActScore, which we call DecompScore. We then propose an LLM-based approach to generating decompositions inspired by Bertrand Russell's theory of lo
    
[^15]: 调查机器翻译中性别偏见的标记和驱动因素

    Investigating Markers and Drivers of Gender Bias in Machine Translations

    [https://arxiv.org/abs/2403.11896](https://arxiv.org/abs/2403.11896)

    通过使用反向翻译技术，比较五种中间语言的结果，并提出新的指标评估翻译中隐含的性别偏见变化。

    

    大语言模型（LLMs）中的隐含性别偏见是一个有充分文献支持的问题，通过自动翻译引入性别可能会延续现实世界的偏见。有些LLMs使用启发式或后处理来掩盖这种偏见，使调查变得困难。本文通过反向翻译来研究LLMs中的偏见，使用DeepL翻译API来调查重复翻译一组56个先前研究中使用的软件工程任务时所展现的偏见。每个陈述以 'she' 开始，并首先翻译为一个 '无性别' 中间语言，然后再翻译回英语；然后我们检查了反向翻译文本中的代词选择。我们通过以下方式扩展了先前的研究：（1）比较了五种中间语言（芬兰语、印度尼西亚语、爱沙尼亚语、土耳其语和匈牙利语）的结果；（2）提出了用于评估重复翻译中所暗示的性别变化的新度量标准。

    arXiv:2403.11896v1 Announce Type: new  Abstract: Implicit gender bias in Large Language Models (LLMs) is a well-documented problem, and implications of gender introduced into automatic translations can perpetuate real-world biases. However, some LLMs use heuristics or post-processing to mask such bias, making investigation difficult. Here, we examine bias in LLMss via back-translation, using the DeepL translation API to investigate the bias evinced when repeatedly translating a set of 56 Software Engineering tasks used in a previous study. Each statement starts with 'she', and is translated first into a 'genderless' intermediate language then back into English; we then examine pronoun- choice in the back-translated texts. We expand prior research in the following ways: (1) by comparing results across five intermediate languages, namely Finnish, Indonesian, Estonian, Turkish and Hungarian; (2) by proposing a novel metric for assessing the variation in gender implied in the repeated tran
    
[^16]: 从可解释到可解释的深度学习在医疗自然语言处理中的应用：现实有多远？

    From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?

    [https://arxiv.org/abs/2403.11894](https://arxiv.org/abs/2403.11894)

    该研究对医疗保健NLP中的深度学习进行了全面审查，提出了可解释和可解释的人工智能（XIAI）概念，并发现注意机制是主要新兴IAI，同时面临着缺乏全局建模、最佳实践以及系统评估和基准测试的挑战。

    

    深度学习（DL）通过解决各种自然语言处理（NLP）任务，极大地增强了医疗保健研究。然而，基于DL的NLP方法日益复杂，需要透明的模型解释性，或至少是可解释性，以进行可靠的决策制定。本文对医疗健康NLP中的可解释和可解释的DL进行了彻底的范围审查。引入了术语“XIAI”（eXplainable和Interpretable Artificial Intelligence）以区分XAI和IAI。方法根据其功能（模型、输入、输出为基础）和范围（局部、全局）进一步分类。我们的分析表明，注意机制是最主要的新兴IAI。此外，IAI越来越多地用于对抗XAI。确定的主要挑战是大多数XIAI不探索“全局”建模过程，缺乏最佳实践，并且需要系统评估和基准测试。

    arXiv:2403.11894v1 Announce Type: cross  Abstract: Deep learning (DL) has substantially enhanced healthcare research by addressing various natural language processing (NLP) tasks. Yet, the increasing complexity of DL-based NLP methods necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review on explainable and interpretable DL in healthcare NLP. The term "XIAI" (eXplainable and Interpretable Artificial Intelligence) was introduced to distinguish XAI from IAI. Methods were further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms were the most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The major challenges identified are that most XIAI do not explore "global" modeling processes, the lack of best practices, and the unmet need for systematic evaluation and benchmarks. Importan
    
[^17]: QueryAgent：一种具有环境反馈的可靠高效推理框架及自我校正

    QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction

    [https://arxiv.org/abs/2403.11886](https://arxiv.org/abs/2403.11886)

    QueryAgent提出了一种基于环境反馈的自我校正方法ERASER，在语义解析中表现出显著的性能提升和高效性

    

    使用大型语言模型（LLMs）进行语义解析取得了显著成功，但在遇到幻觉时现有方法可靠性和效率方面存在不足。本文提出了一个名为QueryAgent的框架，通过逐步解决问题并进行逐步自我校正来解决这些挑战。我们引入了一种基于环境反馈的自我校正方法ERASER。与传统方法不同，ERASER利用中间步骤中的丰富环境反馈，在必要时仅进行选择性和差异化的自我校正。实验结果表明，QueryAgent在GrailQA和GraphQ上仅使用一个例子就比所有先前的少样本方法取得了7.0和15.0的F1值提升。此外，我们的方法在效率方面表现出优势，包括运行时间、查询开销和API调用成本。通过利用ERASER，

    arXiv:2403.11886v1 Announce Type: cross  Abstract: Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success. However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs step-wise self-correction. We introduce an environmental feedback-based self-correction method called ERASER. Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary. Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms of efficiency, including runtime, query overhead, and API invocation costs. By leveraging ERASER, we
    
[^18]: CO3: 低资源对比协同训练用于生成对话式查询重写

    CO3: Low-resource Contrastive Co-training for Generative Conversational Query Rewrite

    [https://arxiv.org/abs/2403.11873](https://arxiv.org/abs/2403.11873)

    通过对比协同训练，本研究提出了一种低资源生成式对话式查询重写方法，可以在数据有限、存在噪声和语言风格转变的情况下保持鲁棒性。

    

    arXiv:2403.11873v1 公告类型：新  摘要：生成式查询重写利用对话历史生成重写查询，但依赖于昂贵的金标重写对，最近，少样本学习在这个任务中越来越受欢迎。而这些方法对由于数据规模有限而产生的固有噪声敏感。此外，当训练和测试情况之间存在语言风格转变时，这两种尝试都会面临性能下降。因此，我们研究了对噪声和语言风格转变都具有鲁棒性的低资源生成式对话式查询重写。其核心思想是利用大量未标记数据，通过对比式协同训练范式进一步改进。具体来说，我们同时训练两个对偶模型（分别为Rewriter和Simplifier），使得它们中的每一个都通过伪标记为另一个提供额外指导，从而以迭代的方式增强另一个。我们还利用对比学习与数据增强进行

    arXiv:2403.11873v1 Announce Type: new  Abstract: Generative query rewrite generates reconstructed query rewrites using the conversation history while rely heavily on gold rewrite pairs that are expensive to obtain. Recently, few-shot learning is gaining increasing popularity for this task, whereas these methods are sensitive to the inherent noise due to limited data size. Besides, both attempts face performance degradation when there exists language style shift between training and testing cases. To this end, we study low-resource generative conversational query rewrite that is robust to both noise and language style shift. The core idea is to utilize massive unlabeled data to make further improvements via a contrastive co-training paradigm. Specifically, we co-train two dual models (namely Rewriter and Simplifier) such that each of them provides extra guidance through pseudo-labeling for enhancing the other in an iterative manner. We also leverage contrastive learning with data augmen
    
[^19]: GPT-4作为评估器：评估大型语言模型在农业害虫管理上的应用

    GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture

    [https://arxiv.org/abs/2403.11858](https://arxiv.org/abs/2403.11858)

    提出了一种在农业害虫管理中利用GPT-4作为评估器的创新方法，评估大型语言模型生成的内容在多个方面的质量

    

    在人工智能领域快速发展的背景下，大型语言模型（LLMs）在农业领域，尤其是在害虫管理方面的应用仍处于初期阶段。我们旨在证明通过评估由开放AI的Generative Pre-trained Transformer（GPT）系列和谷歌的FLAN系列生成的害虫管理建议的内容的可行性。考虑到农业建议的特定情境属性，自动衡量或量化LLMs生成的文本质量成为一项重大挑战。我们提出了一种创新方法，使用GPT-4作为评估器，对生成的内容在连贯性、逻辑一致性、流畅性、相关性、可理解性和全面性上进行打分。此外，我们集成了基于作物阈值数据的专家系统作为基准，以获取有关作物田间发现害虫是否需要采取管理措施的实际准确性评分。

    arXiv:2403.11858v1 Announce Type: new  Abstract: In the rapidly evolving field of artificial intelligence (AI), the application of large language models (LLMs) in agriculture, particularly in pest management, remains nascent. We aimed to prove the feasibility by evaluating the content of the pest management advice generated by LLMs, including the Generative Pre-trained Transformer (GPT) series from OpenAI and the FLAN series from Google. Considering the context-specific properties of agricultural advice, automatically measuring or quantifying the quality of text generated by LLMs becomes a significant challenge. We proposed an innovative approach, using GPT-4 as an evaluator, to score the generated content on Coherence, Logical Consistency, Fluency, Relevance, Comprehensibility, and Exhaustiveness. Additionally, we integrated an expert system based on crop threshold data as a baseline to obtain scores for Factual Accuracy on whether pests found in crop fields should take management act
    
[^20]: 确保安全和高质量输出：面向语言模型的指南库方法

    Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models

    [https://arxiv.org/abs/2403.11838](https://arxiv.org/abs/2403.11838)

    引入Guide-Align，一种两阶段方法，通过安全训练模型识别潜在风险，并制定特定指南，从而建立全面的指导库，用于指导LLMs生成安全和高质量输出。

    

    大型语言模型(LLMs)展示了令人印象深刻的能力，但也存在偏见内容生成和隐私问题等风险。当前的对齐技术之一包括基于原则的集成，但面临由于手工制定规则的不精确性和未经安全训练的模型对风险感知不足而产生的挑战。为了解决这些问题，我们引入了Guide-Align，这是一种两阶段方法。最初，一个经过安全训练的模型识别潜在风险，并为各种输入制定具体指南，从而建立了全面的指南库和用于输入指南检索的模型。随后，检索模型将新输入与相关指南相关联，引导LLMs在响应生成中确保安全和高质量输出，从而与人类价值观一致。另一个额外可选阶段涉及使用经过细致对齐的新数据集对模型进行微调。

    arXiv:2403.11838v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues. One of the current alignment techniques includes principle-driven integration, but it faces challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training. To address these, we introduce Guide-Align, a two-stage approach. Initially, a safety-trained model identifies potential risks and formulates specific guidelines for various inputs, thereby establishing a comprehensive library of guidelines and models for input-guidelines retrieval. Subsequently, the retrieval model correlates new inputs with pertinent guidelines, guiding LLMs in response generation to ensure safe and high-quality outputs, thus aligning with human values. An additional optional stage involves fine-tuning a model with new well-aligned datasets generated through the
    
[^21]: 探索上下文学习与构成概括之间的关系

    Towards Understanding the Relationship between In-context Learning and Compositional Generalization

    [https://arxiv.org/abs/2403.11834](https://arxiv.org/abs/2403.11834)

    强制模型进行上下文学习可能有助于促进神经网络模型的构成概括能力

    

    根据构成概括原则，复杂表达的含义可以理解为其部分含义及它们如何组合的函数。这一原则对于人类语言处理至关重要，同时，可以说对于面对超出分布数据的NLP模型也是重要的。然而，许多神经网络模型，包括Transformer，在构成概括方面表现不佳。本文假设强制模型进行上下文学习可以提供归纳偏见以促进构成概括。为了验证这一假设，我们在一个使普通学习非常困难的环境中训练了一个因果Transformer：我们向其提供不同排序的训练实例并洗牌实例标签。这相当于在数据集中训练模型解决所有可能的少样本学习问题。模型可以解决任务，然而，通过利用

    arXiv:2403.11834v1 Announce Type: new  Abstract: According to the principle of compositional generalization, the meaning of a complex expression can be understood as a function of the meaning of its parts and of how they are combined. This principle is crucial for human language processing and also, arguably, for NLP models in the face of out-of-distribution data. However, many neural network models, including Transformers, have been shown to struggle with compositional generalization. In this paper, we hypothesize that forcing models to in-context learn can provide an inductive bias to promote compositional generalization. To test this hypothesis, we train a causal Transformer in a setting that renders ordinary learning very difficult: we present it with different orderings of the training instance and shuffle instance labels. This corresponds to training the model on all possible few-shot learning problems attainable from the dataset. The model can solve the task, however, by utilizi
    
[^22]: SSCAE -- 语义、句法和上下文感知自然语言对抗样本生成器

    SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator

    [https://arxiv.org/abs/2403.11833](https://arxiv.org/abs/2403.11833)

    本文介绍了一种名为SSCAE的对抗攻击模型，用于生成语义、句法和上下文感知的自然语言对抗样本，提出了动态阈值和本地贪婪搜索以生成高质量对抗样本。

    

    机器学习模型容易受到恶意制作的对抗样本（AEs）的影响。用AEs来训练机器学习模型可以提高其对抗攻击的稳健性和稳定性。在自然语言处理（NLP）领域开发高质量AEs的模型比计算机视觉等领域要慢得多。本文介绍了一种名为SSCAE的实用和高效的对抗攻击模型，用于语义、句法和上下文感知自然语言AE生成器。SSCAE识别重要单词并使用掩码语言模型生成早期替换集。接着，使用两个著名的语言模型来评估初始集合的语义和句法特性。我们引入了（1）动态阈值来捕获更高效的扰动以及（2）本地贪婪搜索以生成高质量

    arXiv:2403.11833v1 Announce Type: new  Abstract: Machine learning models are vulnerable to maliciously crafted Adversarial Examples (AEs). Training a machine learning model with AEs improves its robustness and stability against adversarial attacks. It is essential to develop models that produce high-quality AEs. Developing such models has been much slower in natural language processing (NLP) than in areas such as computer vision. This paper introduces a practical and efficient adversarial attack model called SSCAE for \textbf{S}emantic, \textbf{S}yntactic, and \textbf{C}ontext-aware natural language \textbf{AE}s generator. SSCAE identifies important words and uses a masked language model to generate an early set of substitutions. Next, two well-known language models are employed to evaluate the initial set in terms of semantic and syntactic characteristics. We introduce (1) a dynamic threshold to capture more efficient perturbations and (2) a local greedy search to generate high-qualit
    
[^23]: 大语言模型的隐喻理解挑战数据集

    Metaphor Understanding Challenge Dataset for LLMs

    [https://arxiv.org/abs/2403.11810](https://arxiv.org/abs/2403.11810)

    该研究发布了一个旨在评估大语言模型（LLMs）隐喻理解能力的数据集，包含超过10k个隐喻句的释义以及1.5k个不恰当释义实例，为研究人员提供了有效的工具来检验模型是否真正实现全面隐喻解释而非仅依赖词汇相似性。

    

    自然语言中的隐喻是基本认知过程（如类比推理和分类）的反映，并深深根植于日常交流。因此，隐喻理解对于大语言模型（LLMs）来说是一项重要任务。我们发布了隐喻理解挑战数据集（MUNCH），旨在评估LLMs的隐喻理解能力。该数据集提供了包含隐喻用法的句子的超过10k个释义，以及包含1.5k个不恰当释义的实例。这些不恰当释义经过精心挑选，旨在作为控制来确定模型是否确实进行全面的隐喻解释，还是仅仅依赖词汇相似性。所有恰当和不恰当的释义都经过手动注释。这些隐喻句涵盖了4种流派（学术、新闻、小说和对话）中的自然隐喻用法，并展示不同程度的新颖性。

    arXiv:2403.11810v1 Announce Type: new  Abstract: Metaphors in natural language are a reflection of fundamental cognitive processes such as analogical reasoning and categorisation, and are deeply rooted in everyday communication. Metaphor understanding is therefore an essential task for large language models (LLMs). We release the Metaphor Understanding Challenge Dataset (MUNCH), designed to evaluate the metaphor understanding capabilities of LLMs. The dataset provides over 10k paraphrases for sentences containing metaphor use, as well as 1.5k instances containing inapt paraphrases. The inapt paraphrases were carefully selected to serve as control to determine whether the model indeed performs full metaphor interpretation or rather resorts to lexical similarity. All apt and inapt paraphrases were manually annotated. The metaphorical sentences cover natural metaphor uses across 4 genres (academic, news, fiction, and conversation), and they exhibit different levels of novelty. Experiments
    
[^24]: LLM的决策水平在多智能体环境中的评估究竟如何？

    How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments

    [https://arxiv.org/abs/2403.11807](https://arxiv.org/abs/2403.11807)

    通过博弈论视角评估LLMs的决策能力，结果表明GPT-3.5在稳健性方面表现良好，但泛化能力有限，而GPT-4则优于其他模型。

    

    决策是一个复杂的任务，需要各种能力，为评估大型语言模型（LLMs）提供了一个极好的框架。我们的研究通过博弈论的视角探究LLMs的决策能力。我们专注于支持多个智能体同时参与的游戏，引入了我们的框架GAMA-Bench，包括八个经典的多智能体游戏。我们设计了一个评分方案，定量评估模型在这些游戏中的表现。通过GAMA-Bench，我们研究了LLMs的稳健性、泛化能力和增强策略。结果显示，虽然GPT-3.5表现出令人满意的稳健性，但其泛化能力相对有限。然而，通过一些方法如“思维链”，其性能可以得到提高。此外，我们对各种LLMs进行评估，发现GPT-4胜过其他模型。

    arXiv:2403.11807v1 Announce Type: new  Abstract: Decision-making, a complicated task requiring various types of abilities, presents an excellent framework for assessing Large Language Models (LLMs). Our research investigates LLMs' decision-making capabilities through the lens of a well-established field, Game Theory. We focus specifically on games that support the participation of more than two agents simultaneously. Subsequently, we introduce our framework, GAMA-Bench, including eight classical multi-agent games. We design a scoring scheme to assess a model's performance in these games quantitatively. Through GAMA-Bench, we investigate LLMs' robustness, generalizability, and enhancement strategies. Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited. However, its performance can be improved through approaches such as Chain-of-Thought. Additionally, we conduct evaluations across various LLMs and find that GPT-4 outperforms other mod
    
[^25]: Counting-Stars：一种评估长上下文大型语言模型的简单、高效、合理策略

    Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models

    [https://arxiv.org/abs/2403.11802](https://arxiv.org/abs/2403.11802)

    提出了一种名为Counting-Stars的简单、高效、合理策略，用于评估长上下文大型语言模型的能力，并在实验中发现GPT-4 Turbo和Kimi Chat在此任务上取得显著性能。

    

    近期的研究主要集中在开发具有强大长上下文能力的大型语言模型（LLMs），由于缺乏适当的评估策略，对领先的LLMs（例如ChatGPT和KimiChat）的长上下文处理能力和性能了解甚少。为了填补这一空白，我们提出了一个简单、高效、合理的长上下文LLMs评估策略作为一个新的基准，名为Counting-Stars。Counting-Stars旨在要求LLMs充分理解和捕捉长上下文中的长依赖关系，并能够收集跨越整个上下文的多个证据之间的相互依赖来完成任务。基于Counting-Stars，我们进行实验评估了两个领先的长上下文LLMs，即GPT-4 Turbo和Kimi Chat。实验结果表明，GPT-4 Turbo和Kimi Chat在Counting-Stars任务上取得了显著的表现。

    arXiv:2403.11802v1 Announce Type: new  Abstract: While recent research endeavors have concentrated on developing Large Language Models (LLMs) with robust long-context capabilities, due to the lack of appropriate evaluation strategies, relatively little is known about how well the long-context processing abilities and performance of leading LLMs (e.g., ChatGPT and KimiChat). To address this gap, we propose a simple, efficient, and reasonable strategy for evaluating long-context LLMs as a new benchmark, named Counting-Stars. The Counting-Stars is designed to require LLMs to fully understand and capture long dependencies in long contexts and be able to collect inter-dependency across multiple pieces of evidence spanning the entire context to finish the task. Based on the Counting-Stars, we conduct experiments to evaluate the two leading long-context LLMs, i.e., GPT-4 Turbo and Kimi Chat. The experimental results indicate that GPT-4 Turbo and Kimi Chat achieve significant performance in th
    
[^26]: 大型语言模型的推理能力：对抽象和推理语料库的深入分析

    Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus

    [https://arxiv.org/abs/2403.11793](https://arxiv.org/abs/2403.11793)

    使用抽象和推理语料库（ARC）数据集评估大型语言模型的推理和上下文理解能力，结果显示虽然大型语言模型具有较弱的推理能力，但在逻辑连贯性、组合性和效率方面仍然落后，实验结果有助于提出实现人类水平推理的发展路径。

    

    评估大型语言模型（LLMs）推理能力的现有方法以结果为中心，使得评估推理过程变得困难。我们引入了一种新方法，使用抽象和推理语料库（ARC）数据集以过程为中心的方式评估大型语言模型的推理和上下文理解能力。ARC要求解决问题时具有严谨的逻辑结构，这使得它成为一个能够促进模型推理能力与人类进行比较的基准。实验结果证实，虽然大型语言模型具有较弱的推理能力，但在逻辑连贯性、组合性和效率方面仍然落后。我们的实验突显了LLMs的推理能力，并提出了实现人类水平推理的发展路径。

    arXiv:2403.11793v1 Announce Type: cross  Abstract: The existing methods for evaluating the inference abilities of Large Language Models (LLMs) have been results-centric, making it difficult to assess the inference process. We introduce a new approach using the Abstract and Reasoning Corpus (ARC) dataset to evaluate the inference and contextual understanding abilities of large language models in a process-centric manner. ARC demands rigorous logical structures for problem-solving, making it a benchmark that facilitates the comparison of model inference abilities with humans. Experimental results confirm that while large language models possess weak inference abilities, they still lag in terms of logical coherence, compositionality, and productivity. Our experiments highlight the reasoning capabilities of LLMs, proposing development paths for achieving human-level reasoning.
    
[^27]: 使用预训练大型语言模型构建超关联知识图

    Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained Large Language Models

    [https://arxiv.org/abs/2403.11786](https://arxiv.org/abs/2403.11786)

    使用零样本提示的方法，我们利用OpenAI的GPT-3.5模型从文本中成功提取了超关联知识，尽管精确率较低，但结果显示了未来研究的潜在方向。

    

    提取超关系对于构建全面的知识图至关重要，但目前针对此任务的监督方法有限。为了弥补这一差距，我们引入了一种基于零样本提示的方法，利用OpenAI的GPT-3.5模型从文本中提取超关联知识。通过将我们的模型与基准模型进行比较，我们取得了令人满意的结果，召回率达到了0.77。尽管我们的精确率目前较低，但对模型输出的详细分析揭示了未来研究领域的潜在路径。

    arXiv:2403.11786v1 Announce Type: cross  Abstract: Extracting hyper-relations is crucial for constructing comprehensive knowledge graphs, but there are limited supervised methods available for this task. To address this gap, we introduce a zero-shot prompt-based method using OpenAI's GPT-3.5 model for extracting hyper-relational knowledge from text. Comparing our model with a baseline, we achieved promising results, with a recall of 0.77. Although our precision is currently lower, a detailed analysis of the model outputs has uncovered potential pathways for future research in this area.
    
[^28]: 视觉和语言的模态无关fMRI解码

    Modality-Agnostic fMRI Decoding of Vision and Language

    [https://arxiv.org/abs/2403.11771](https://arxiv.org/abs/2403.11771)

    本研究介绍了一种能够预测受试者正在看到的刺激的单一解码器，无论刺激是以何种模态呈现。研究发现模态无关解码器表现与模态特定解码器相当甚至更好。

    

    先前的研究表明，可以将受试者观看图像的大脑激活数据映射到视觉模型（模态特定解码）以及语言模型（跨模态解码）的特征表示空间中。本研究引入并使用了一个新的大规模fMRI数据集（每个受试者约8500个试验），让人们既观看图像又观看这些图像的文本描述。这一新颖的数据集实现了模态无关解码器的开发：一种单一解码器可以预测受试者正在看到的刺激，而不考虑刺激以何种模态（图像或文本）呈现。我们训练和评估这类解码器，以将大范围公开可用的视觉、语言和多模态（视觉+语言）模型中的刺激表示映射到大脑信号上。我们的研究结果表明，（1）模态无关解码器表现得与（有时甚至更好）模态特定解码一样好。

    arXiv:2403.11771v1 Announce Type: cross  Abstract: Previous studies have shown that it is possible to map brain activation data of subjects viewing images onto the feature representation space of not only vision models (modality-specific decoding) but also language models (cross-modal decoding). In this work, we introduce and use a new large-scale fMRI dataset (~8,500 trials per subject) of people watching both images and text descriptions of such images. This novel dataset enables the development of modality-agnostic decoders: a single decoder that can predict which stimulus a subject is seeing, irrespective of the modality (image or text) in which the stimulus is presented. We train and evaluate such decoders to map brain signals onto stimulus representations from a large range of publicly available vision, language and multimodal (vision+language) models. Our findings reveal that (1) modality-agnostic decoders perform as well as (and sometimes even better than) modality-specific dec
    
[^29]: 重新审视经典：识别和纠正韵律诗中的性别刻板印象研究

    Revisiting The Classics: A Study on Identifying and Rectifying Gender Stereotypes in Rhymes and Poems

    [https://arxiv.org/abs/2403.11752](https://arxiv.org/abs/2403.11752)

    该研究通过收集韵律诗和诗歌数据集，建立了一个准确率为97%的模型，用于识别性别偏见，并通过大型语言模型纠正了性别刻板印象。

    

    韵律诗是传递文化规范和社会角色的强大媒介。然而，这些作品中普遍存在的性别刻板印象延续了有偏见的认知，并限制了个体身份的范围。这项工作通过收集一组韵律诗和诗歌数据集，识别性别刻板印象并提出了一个准确率为97%的模型，用于识别性别偏见。通过使用大型语言模型（LLM）纠正了性别刻板印象，并在与人类教育者纠正策略的比较调查中评估了其有效性。总之，这项工作突显了文学作品中性别刻板印象的普遍性，并揭示了大型语言模型纠正性别刻板印象的潜力。

    arXiv:2403.11752v1 Announce Type: new  Abstract: Rhymes and poems are a powerful medium for transmitting cultural norms and societal roles. However, the pervasive existence of gender stereotypes in these works perpetuates biased perceptions and limits the scope of individuals' identities. Past works have shown that stereotyping and prejudice emerge in early childhood, and developmental research on causal mechanisms is critical for understanding and controlling stereotyping and prejudice. This work contributes by gathering a dataset of rhymes and poems to identify gender stereotypes and propose a model with 97\% accuracy to identify gender bias. Gender stereotypes were rectified using a Large Language Model (LLM) and its effectiveness was evaluated in a comparative survey against human educator rectifications. To summarize, this work highlights the pervasive nature of gender stereotypes in literary works and reveals the potential of LLMs to rectify gender stereotypes. This study raises 
    
[^30]: 使用探测分类器的嵌入式命名实体识别

    Embedded Named Entity Recognition using Probing Classifiers

    [https://arxiv.org/abs/2403.11747](https://arxiv.org/abs/2403.11747)

    提出一种名为EMBER的方法，通过使用探测分类器将信息提取能力直接嵌入预训练语言模型中，实现了高效的同时文本生成和信息提取，使得解码器-only语言模型能够在不微调的情况下进行命名实体识别。

    

    从生成的文本中提取语义信息是自动事实检查或检索增强生成等应用的有用工具。目前，这要求在推理过程中使用单独的模型，这会增加计算成本，或对语言模型进行破坏性微调。相反，我们提出直接将信息提取功能嵌入预训练的语言模型中，使用探测分类器，从而实现高效的同时文本生成和信息提取。为此，我们介绍了一种名为EMBER的方法，并展示它使解码器-only语言模型能够在不微调的情况下进行命名实体识别，并且在推理过程中附加的计算成本极小。具体来说，我们使用GPT-2进行的实验表明，EMBER在流文本生成过程中保持高令牌生成速率，与43.64%相比，其速度仅略微下降约1%。

    arXiv:2403.11747v1 Announce Type: new  Abstract: Extracting semantic information from generated text is a useful tool for applications such as automated fact checking or retrieval augmented generation. Currently, this requires either separate models during inference, which increases computational cost, or destructive fine-tuning of the language model. Instead, we propose directly embedding information extraction capabilities into pre-trained language models using probing classifiers, enabling efficient simultaneous text generation and information extraction. For this, we introduce an approach called EMBER and show that it enables named entity recognition in decoder-only language models without fine-tuning them and while incurring minimal additional computational cost at inference time. Specifically, our experiments using GPT-2 show that EMBER maintains high token generation rates during streaming text generation, with only a negligible decrease in speed of around 1% compared to a 43.64
    
[^31]: 让我们关注神经元：大型语言模型的神经元级监督微调

    Let's Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model

    [https://arxiv.org/abs/2403.11621](https://arxiv.org/abs/2403.11621)

    NeFT是一种神经元级微调的新方法，可以精确和计算高效地更新大型语言模型。

    

    大型语言模型（LLMs）由展示各种行为和角色的神经元组成，在模型扩展的同时表现出越来越多样化的特性。最近的研究揭示了并非所有神经元在不同数据集中都活跃，而这种稀疏性与任务特定能力呈正相关，从而推动了模型剪枝和训练效率的发展。传统的微调方法涉及LLMs的所有参数，这在计算上是昂贵的且可能并非必要的。相比之下，参数高效微调（PEFT）方法旨在最小化可训练参数的数量，但它们仍在相对宏观的尺度上操作（例如，层级）。我们引入了神经元级微调（NeFT），这是一种将参数训练的粒度细化到个体神经元的新方法，从而实现更精确和计算高效的模型更新。实验结果表明，NeFT不仅能在几乎相同精度下显著减少可训练参数，还能在模型更新方面 更有效。

    arXiv:2403.11621v1 Announce Type: new  Abstract: Large Language Models (LLMs) are composed of neurons that exhibit various behaviors and roles, which become increasingly diversified as models scale. Recent studies have revealed that not all neurons are active across different datasets, and this sparsity correlates positively with the task-specific ability, leading to advancements in model pruning and training efficiency. Traditional fine-tuning methods engage all parameters of LLMs, which is computationally expensive and may not be necessary. In contrast, Parameter-Efficient Fine-Tuning (PEFT) approaches aim to minimize the number of trainable parameters, yet they still operate at a relatively macro scale (e.g., layer-level). We introduce Neuron-Level Fine-Tuning (NeFT), a novel approach that refines the granularity of parameter training down to the individual neuron, enabling more precise and computationally efficient model updates. The experimental results show that NeFT not only exc
    
[^32]: Linguacodus：一种在机器学习流水线中进行变革性代码生成的协同框架

    Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines

    [https://arxiv.org/abs/2403.11585](https://arxiv.org/abs/2403.11585)

    Linguacodus是一种创新框架，通过部署动态流水线和精细调整的大型语言模型，实现了将自然语言任务描述转换为代码的自动化过程，极大地推进了机器学习应用的发展。

    

    在不断发展的机器学习领域中，将自然语言描述无缝转化为可执行代码仍然是一个巨大的挑战。本文介绍了Linguacodus，这是一个创新性框架，旨在通过部署一个动态流水线，通过高级数据塑形指令，将自然语言任务描述迭代地转换为代码来应对这一挑战。Linguacodus的核心是一个经过精细调整的大型语言模型（LLM），能够评估各种问题的多样解决方案，并为特定任务选择最合适的解决方案。本文详细介绍了精细调整过程，并阐明了如何将自然语言描述转化为功能性代码。Linguacodus代表了自动化代码生成的重大飞跃，有效地弥合了任务描述和可执行代码之间的差距。它对推进跨不同领域的机器学习应用具有巨大潜力。

    arXiv:2403.11585v1 Announce Type: cross  Abstract: In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions. The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across div
    
[^33]: 使用标记级反馈的强化学习用于可控文本生成

    Reinforcement Learning with Token-level Feedback for Controllable Text Generation

    [https://arxiv.org/abs/2403.11558](https://arxiv.org/abs/2403.11558)

    提出了一种新的强化学习算法TOLE，通过使用标记级别奖励进行文本生成，采用了"先量子化，然后加噪声"的方法来增强算法的鲁棒性。可以在多个约束条件下灵活扩展，避免了过拟合和语义崩溃问题。

    

    为了满足现实世界应用的需求，控制大型语言模型（LLMs）的生成是至关重要的。先前的研究试图将强化学习（RL）引入可控文本生成，而大多数现有方法存在过拟合问题（微调基础方法）或语义崩溃（后处理方法）。然而，当前的RL方法通常是由粗粒度（句子/段落级别）的反馈引导的，这可能导致句子内语义扭曲或进展，从而导致性能次优。为了解决这个问题，我们提出了一种名为TOLE的新型强化学习算法，该算法制定了TOken-LEvel奖励用于可控文本生成，并采用“先量子化，然后加噪声”范式来增强强化学习算法的鲁棒性。此外，TOLE可以灵活地扩展到多个约束条件，计算成本很低。实验结果表明，我们的al

    arXiv:2403.11558v1 Announce Type: cross  Abstract: To meet the requirements of real-world applications, it is essential to control generations of large language models (LLMs). Prior research has tried to introduce reinforcement learning (RL) into controllable text generation while most existing methods suffer from overfitting issues (finetuning-based methods) or semantic collapse (post-processing methods). However, current RL methods are generally guided by coarse-grained (sentence/paragraph-level) feedback, which may lead to suboptimal performance owing to semantic twists or progressions within sentences. To tackle that, we propose a novel reinforcement learning algorithm named TOLE which formulates TOken-LEvel rewards for controllable text generation, and employs a "first-quantize-then-noise" paradigm to enhance the robustness of the RL algorithm.Furthermore, TOLE can be flexibly extended to multiple constraints with little computational expense. Experimental results show that our al
    
[^34]: DEE: 文本生成的双阶段可解释评估方法

    DEE: Dual-stage Explainable Evaluation Method for Text Generation

    [https://arxiv.org/abs/2403.11509](https://arxiv.org/abs/2403.11509)

    介绍了DEE，一个双阶段可解释评估方法，用于评估文本生成质量。

    

    自动评估机器生成的文本的方法对于生成系统的应用日益广泛具有重要意义。传统方法往往很难解释性，发出一个孤立的数字评分来表示评估结果。最近的进展试图通过引入大型语言模型（LLMs）来提供更详细的错误分析来缓解这一局限，然而它们的适用性仍受限制，尤其是在全面覆盖错误和快速检测至关重要的工业背景下。为了解决这些挑战，我们介绍了DEE，一个用于评估文本生成质量的双阶段可解释评估方法。基于Llama 2构建的DEE遵循一个双阶段原则，根据各阶段的指导进行高效识别初始阶段生成文本中的错误，并随后着手提供全面的诊断。

    arXiv:2403.11509v1 Announce Type: new  Abstract: Automatic methods for evaluating machine-generated texts hold significant importance due to the expanding applications of generative systems. Conventional methods tend to grapple with a lack of explainability, issuing a solitary numerical score to signify the assessment outcome. Recent advancements have sought to mitigate this limitation by incorporating large language models (LLMs) to offer more detailed error analyses, yet their applicability remains constrained, particularly in industrial contexts where comprehensive error coverage and swift detection are paramount. To alleviate these challenges, we introduce DEE, a Dual-stage Explainable Evaluation method for estimating the quality of text generation. Built upon Llama 2, DEE follows a dual-stage principle guided by stage-specific instructions to perform efficient identification of errors in generated texts in the initial stage and subsequently delves into providing comprehensive diag
    
[^35]: 词序的影响：重新排序和生成分析的洞察

    Word Order's Impacts: Insights from Reordering and Generation Analysis

    [https://arxiv.org/abs/2403.11473](https://arxiv.org/abs/2403.11473)

    本文通过顺序重建的角度重新审视了关于词序的假设，并通过选择不同范围的数据集进行了实验证明，ChatGPT依赖于词序进行推断，但不能明确支持或否定词序与词汇语义之间的冗余关系。

    

    现有研究已经研究了自然语言文本中词语顺序的影响。它们通常通过打乱原始词序来创建一个混乱的序列进行分析，然后比较模型在原始序列和混乱序列之间的表现。实验结果表明存在一些小幅下降。考虑到这一发现，提出了不同的关于词序的假设，包括“词序与词汇语义重复”和“模型不依赖于词序”。在本文中，我们通过增加一个顺序重建的视角，选择不同范围的数据集来重新审视上述假设。具体而言，我们首先选择了四个不同的数据集，然后设计了顺序重建和连续生成任务。实证研究支持ChatGPT依赖于词序进行推断，但无法支持或否定词序与词汇语义之间的冗余关系。

    arXiv:2403.11473v1 Announce Type: cross  Abstract: Existing works have studied the impacts of the order of words within natural text. They usually analyze it by destroying the original order of words to create a scrambled sequence, and then comparing the models' performance between the original and scrambled sequences. The experimental results demonstrate marginal drops. Considering this findings, different hypothesis about word order is proposed, including ``the order of words is redundant with lexical semantics'', and ``models do not rely on word order''. In this paper, we revisit the aforementioned hypotheses by adding a order reconstruction perspective, and selecting datasets of different spectrum. Specifically, we first select four different datasets, and then design order reconstruction and continuing generation tasks. Empirical findings support that ChatGPT relies on word order to infer, but cannot support or negate the redundancy relations between word order lexical semantics.
    
[^36]: HateCOT：通过大型语言模型进行泛化攻击性言论检测的解释增强数据集

    HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models

    [https://arxiv.org/abs/2403.11456](https://arxiv.org/abs/2403.11456)

    HateCOT数据集通过GPT-3.5-Turbo生成解释，将52,000个样本数据用于预训练模型，显著提升了在不同领域和任务下的攻击性内容检测效果。

    

    社交媒体的普及导致了对攻击性内容的可靠高效检测的需求，为了限制其有害影响。这导致了大量与检测攻击性内容相关的数据集和模型的出现。本文介绍了HateCOT，这是从多样化现有来源中抽取的5.2万个样本数据集，其中包含由GPT-3.5-Turbo和人工精心制作的解释。我们展示了在HateCOT上为攻击性内容检测预训练模型在零-shot和few-shot设置下显著改进了开源语言模型在三个基准数据集上的表现，尽管在领域和任务方面存在差异。

    arXiv:2403.11456v1 Announce Type: cross  Abstract: The ubiquitousness of social media has led to the need for reliable and efficient detection of offensive content to limit harmful effects. This has led to a proliferation of datasets and models related to detecting offensive content. While sophisticated models have attained strong performance on individual datasets, these models often do not generalize due to differences between how "offensive content" is conceptualized, and the resulting differences in how these datasets are labeled. In this paper, we introduce HateCOT, a dataset of 52,000 samples drawn from diverse existing sources with explanations generated by GPT-3.5-Turbo and human-curated. We show that pre-training models for the detection of offensive content on HateCOT significantly boots open-sourced Language Models on three benchmark datasets in both zero and few-shot settings, despite differences in domain and task.} We further find that HateCOT enables effective K-shot fin
    
[^37]: StyleChat: 在LLMs中学习复述增强记忆以进行风格化对话生成

    StyleChat: Learning Recitation-Augmented Memory in LLMs for Stylized Dialogue Generation

    [https://arxiv.org/abs/2403.11439](https://arxiv.org/abs/2403.11439)

    该论文介绍了一个包含38种风格的风格化对话数据集StyleEval，并提出了一种风格化对话框架，以增强LLMs在风格化对话生成中的性能。

    

    大型语言模型(LLMs)在生成场景中展现出卓越的性能，并受到广泛关注。其中，在LLMs环境中进行风格化对话生成对于构建智能且引人入胜的对话代理至关重要。然而，LLMs的能力是数据驱动的，并受数据偏差限制，导致在特定任务上表现不佳。特别是，风格化对话生成面临着严重的监督数据不足问题。此外，虽然提出了许多基于提示的方法来完成特定任务，但在涉及各种对话风格的复杂现实场景中的性能需要进一步提升。在这项工作中，我们首次引入一个包含38种风格的风格化对话数据集StyleEval，充分利用LLMs的生成能力，经过严格的人为质量控制精心构建。基于此，我们提出了风格化对话框架

    arXiv:2403.11439v1 Announce Type: new  Abstract: Large Language Models (LLMs) demonstrate superior performance in generative scenarios and have attracted widespread attention. Among them, stylized dialogue generation is essential in the context of LLMs for building intelligent and engaging dialogue agent. However the ability of LLMs is data-driven and limited by data bias, leading to poor performance on specific tasks. In particular, stylized dialogue generation suffers from a severe lack of supervised data. Furthermore, although many prompt-based methods have been proposed to accomplish specific tasks, their performance in complex real-world scenarios involving a wide variety of dialog styles further enhancement. In this work, we first introduce a stylized dialogue dataset StyleEval with 38 styles by leveraging the generative power of LLMs comprehensively, which has been carefully constructed with rigorous human-led quality control. Based on this, we propose the stylized dialogue fram
    
[^38]: InsCL: 一种用指导信息细调大型语言模型的数据高效持续学习范式

    InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions

    [https://arxiv.org/abs/2403.11435](https://arxiv.org/abs/2403.11435)

    InsCL提出了一种基于指导信息的新型持续学习范式，通过Wasserstein距离计算任务相似性，并引入InsInfo指标来定量指导信息的复杂性和多样性，从而指导重播过程更倾向于高质量数据。

    

    由于实际应用中环境的不断变化，大型语言模型(LLMs)需要在不产生灾难性遗忘的情况下持续进行特定任务的适应。由于高昂的计算成本，基于重播的持续学习(CL)方法是解决LLMs遗忘问题的最简单且广泛使用的方式。然而，传统的基于重播的方法并没有充分利用指导信息来定制重播策略。在这项工作中，我们提出了一种名为基于指导信息的持续学习(InsCL)的新范式。InsCL根据任务相似性（通过指导信息的Wasserstein Distance计算）动态重播先前的数据。此外，我们进一步引入了一个指导信息度量标准(InsInfo)来量化指导信息的复杂性和多样性。根据InsInfo，InsCL引导重播过程更倾向于高质量的数据。

    arXiv:2403.11435v1 Announce Type: new  Abstract: Instruction tuning effectively optimizes Large Language Models (LLMs) for downstream tasks. Due to the changing environment in real-life applications, LLMs necessitate continual task-specific adaptation without catastrophic forgetting. Considering the heavy computational cost, replay-based Continual Learning (CL) methods are the simplest and most widely used for LLMs to address the forgetting issue. However, traditional replay-based methods do not fully utilize instructions to customize the replay strategy. In this work, we propose a novel paradigm called Instruction-based Continual Learning (InsCL). InsCL dynamically replays previous data based on task similarity, calculated by Wasserstein Distance with instructions. Moreover, we further introduce an Instruction Information Metric (InsInfo) to quantify the complexity and diversity of instructions. According to InsInfo, InsCL guides the replay process more inclined to high-quality data. 
    
[^39]: 一种提升大型语言模型翻译能力的新范式

    A Novel Paradigm Boosting Translation Capabilities of Large Language Models

    [https://arxiv.org/abs/2403.11430](https://arxiv.org/abs/2403.11430)

    新范式关注在LLMs的预训练阶段增强跨语言对齐能力，强调使用高质量的小型双语数据，在传统机器翻译方法无法胜任的任务上取得了良好的效果。

    

    这篇论文研究了在机器翻译任务中增强大型语言模型（LLMs）翻译能力的策略。论文提出了一个包括三个阶段的新范式：使用大量单语数据的次级预训练，使用互文格式文档进行持续预训练，以及利用与源语言一致的指导进行监督微调。我们认为，重点应该放在增强LLMs在预训练期间的跨语言对齐能力，而不仅仅依靠SFT期间大量的双语数据。实验结果证实了我们的方法在不依赖大量双语数据的情况下，在传统机器翻译方法无法胜任的任务上取得了良好的效果。

    arXiv:2403.11430v1 Announce Type: new  Abstract: This paper presents a study on strategies to enhance the translation capabilities of large language models (LLMs) in the context of machine translation (MT) tasks. The paper proposes a novel paradigm consisting of three stages: Secondary Pre-training using Extensive Monolingual Data, Continual Pre-training with Interlinear Text Format Documents, and Leveraging Source-Language Consistent Instruction for Supervised Fine-Tuning. Previous research on LLMs focused on various strategies for supervised fine-tuning (SFT), but their effectiveness has been limited. While traditional machine translation approaches rely on vast amounts of parallel bilingual data, our paradigm highlights the importance of using smaller sets of high-quality bilingual data. We argue that the focus should be on augmenting LLMs' cross-lingual alignment abilities during pre-training rather than solely relying on extensive bilingual data during SFT. Experimental results co
    
[^40]: 叙事特征还是结构特征？研究大型语言模型以识别患心力衰竭风险的癌症患者

    Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure

    [https://arxiv.org/abs/2403.11425](https://arxiv.org/abs/2403.11425)

    使用大型语言模型结合新颖的叙述特征，能够有效识别癌症患者患心力衰竭的风险，表现优于传统机器学习模型和深度学习模型。

    

    癌症治疗已知会引入心毒性，对预后和生存率产生负面影响。识别患心力衰竭（HF）风险的癌症患者对于改善癌症治疗结果和安全性至关重要。本研究使用来自电子健康记录（EHRs）的机器学习（ML）模型，包括传统ML、时间感知长短期记忆（T-LSTM）和使用从结构化医学代码衍生的新颖叙述特征的大型语言模型（LLMs）来识别患HF风险的癌症患者。我们从佛罗里达大学健康中心识别了一组包括12,806名肺癌、乳腺癌和结直肠癌患者的癌症队列，其中1,602人在癌症后发展为HF。LLM GatorTron-3.9B取得了最佳的F1分数，比传统支持向量机高出39%，比T-LSTM深度学习模型高出7%，比广泛使用的Transformer模型BERT高出5.6%。

    arXiv:2403.11425v1 Announce Type: cross  Abstract: Cancer treatments are known to introduce cardiotoxicity, negatively impacting outcomes and survivorship. Identifying cancer patients at risk of heart failure (HF) is critical to improving cancer treatment outcomes and safety. This study examined machine learning (ML) models to identify cancer patients at risk of HF using electronic health records (EHRs), including traditional ML, Time-Aware long short-term memory (T-LSTM), and large language models (LLMs) using novel narrative features derived from the structured medical codes. We identified a cancer cohort of 12,806 patients from the University of Florida Health, diagnosed with lung, breast, and colorectal cancers, among which 1,602 individuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the best F1 scores, outperforming the traditional support vector machines by 39%, the T-LSTM deep learning model by 7%, and a widely used transformer model, BERT, by 5.6%. The analysi
    
[^41]: 基于RAG的会话系统中生成建议问题的动态背景

    Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational Systems

    [https://arxiv.org/abs/2403.11413](https://arxiv.org/abs/2403.11413)

    通过利用动态背景，该研究开发了一个建议问题生成器，实验证明它可以生成比其他提示方法更好的建议问题。

    

    当与基于检索增强生成（RAG）的会话代理互动时，用户必须小心地制定他们的查询以被正确理解。然而，了解系统的能力对用户来说可能是具有挑战性的，导致需要进一步澄清的模糊问题。本研究旨在通过开发一个建议问题生成器来弥补这一差距。为了生成建议问题，我们的方法涉及利用动态背景，其中包括动态的少样本示例和动态检索到的背景。通过实验证明，与其他提示方法相比，动态背景方法可以生成更好的建议问题。

    arXiv:2403.11413v1 Announce Type: new  Abstract: When interacting with Retrieval-Augmented Generation (RAG)-based conversational agents, the users must carefully craft their queries to be understood correctly. Yet, understanding the system's capabilities can be challenging for the users, leading to ambiguous questions that necessitate further clarification. This work aims to bridge the gap by developing a suggestion question generator. To generate suggestion questions, our approach involves utilizing dynamic context, which includes both dynamic few-shot examples and dynamically retrieved contexts. Through experiments, we show that the dynamic contexts approach can generate better suggestion questions as compared to other prompting approaches.
    
[^42]: X-LLaVA: 优化双语大规模视觉语言对齐

    X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment

    [https://arxiv.org/abs/2403.11399](https://arxiv.org/abs/2403.11399)

    提出了两种成本有效的方法解决大规模多模态模型训练数据的挑战，并在英语-韩语-中文多语言、多模态训练数据集上开发了表现优越的双语多模态模型。

    

    大规模语言模型（LLMs）的显著发展正在扩展到大规模多模态模型（LMMs）的领域，这些模型集成了除文本以外的多种数据类型。然而，多模态模型的特性导致在创建训练数据方面存在显着的开销。此外，为LMMs构建多语言数据也面临着语言多样性和复杂性的挑战。因此，在这项研究中，我们提出了两种成本有效的方法来解决这个问题：（1）多语言LLM的词汇扩展和预训练，以及（2）使用GPT4-V自动和精心构建多模态数据集。基于这些方法，我们构建了一个包含91K英文-韩文-中文的多语言、多模态训练数据集。此外，我们开发了一个双语多模态模型，在韩语和英语中表现出卓越的性能，超过了现有方法。

    arXiv:2403.11399v1 Announce Type: new  Abstract: The impressive development of large language models (LLMs) is expanding into the realm of large multimodal models (LMMs), which incorporate multiple types of data beyond text. However, the nature of multimodal models leads to significant expenses in the creation of training data. Furthermore, constructing multilingual data for LMMs presents its own set of challenges due to language diversity and complexity. Therefore, in this study, we propose two cost-effective methods to solve this problem: (1) vocabulary expansion and pretraining of multilingual LLM for specific languages, and (2) automatic and elaborate construction of multimodal datasets using GPT4-V. Based on015 these methods, we constructed a 91K English-Korean-Chinese multilingual, multimodal training dataset. Additionally, we developed a bilingual multimodal model that exhibits excellent performance in both Korean and English, surpassing existing approaches.
    
[^43]: LLM增强型自主代理能够合作吗？通过融合盆评估它们的合作能力

    Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot

    [https://arxiv.org/abs/2403.11381](https://arxiv.org/abs/2403.11381)

    本研究探讨了LLM增强型自主代理在合作方面的表现，结果显示虽然这些代理显示出合作倾向，但仍然存在合作困难，强调了对更健壮架构的需求。

    

    随着人工智能领域的不断发展，其中一个重要维度是发展大型语言模型及其潜力增强多代理人人工智能系统。本文通过使用著名的Meltin Pot环境以及参考模型如GPT4和GPT3.5，探讨了大型语言模型增强的自主代理(LAAs)的合作能力。初步结果表明，尽管这些代理人表现出合作的倾向，但在特定环境中仍然难以实现有效的协作，强调了更强大架构的需求。本研究的贡献包括一个用于适应LLM的Melting Pot游戏场景的抽象化层，一个用于LLM中介代理开发的可重用架构-包括短期和长期记忆以及不同的认知模块，并使用一组方法评估合作能力。

    arXiv:2403.11381v1 Announce Type: new  Abstract: As the field of AI continues to evolve, a significant dimension of this progression is the development of Large Language Models and their potential to enhance multi-agent artificial intelligence systems. This paper explores the cooperative capabilities of Large Language Model-augmented Autonomous Agents (LAAs) using the well-known Meltin Pot environments along with reference models such as GPT4 and GPT3.5. Preliminary results suggest that while these agents demonstrate a propensity for cooperation, they still struggle with effective collaboration in given environments, emphasizing the need for more robust architectures. The study's contributions include an abstraction layer to adapt Melting Pot game scenarios for LLMs, the implementation of a reusable architecture for LLM-mediated agent development - which includes short and long-term memories and different cognitive modules, and the evaluation of cooperation capabilities using a set of 
    
[^44]: 什么让大型语言模型对数学问题难以应对？

    What Makes Math Word Problems Challenging for LLMs?

    [https://arxiv.org/abs/2403.11369](https://arxiv.org/abs/2403.11369)

    研究了大型语言模型在处理数学问题的文字题目时所面临的困难，通过分析特征和训练分类器来预测模型对不同类型题目的表现。

    

    这篇论文研究了对大型语言模型(LLMs)而言，什么让数学问题的文字题目(math word problems, MWPs)变得具有挑战性。我们深入分析了MWPs的关键语言和数学特征。此外，我们训练了基于特征的分类器，以更好地理解每个特征对于LLMs日常任务中MWPs的整体难度的影响，并探究这是否有助于预测LLMs在特定类别的MWPs上的表现。

    arXiv:2403.11369v1 Announce Type: new  Abstract: This paper investigates the question of what makes math word problems (MWPs) challenging for large language models (LLMs). We conduct an in-depth analysis of the key linguistic and mathematical characteristics of MWPs. In addition, we train feature-based classifiers to better understand the impact of each feature on the overall difficulty of MWPs for prominent LLMs and investigate whether this helps predict how well LLMs fare against specific categories of MWPs.
    
[^45]: JORA: 用于检索增强微调的JAX张量并行LoRA库

    JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning

    [https://arxiv.org/abs/2403.11366](https://arxiv.org/abs/2403.11366)

    提出了用于检索增强微调的JAX张量并行LoRA库，通过PEFT兼容微调Llama-2模型，利用分布式训练和JAX的即时编译和张量分片实现了资源高效管理，加速微调并降低内存需求，提高了微调大型语言模型在复杂RAG应用中的可扩展性和可行性。

    

    《JORA: JAX张量并行LoRA库用于检索增强微调》通过介绍一种新的框架，提供了一种适用于检索增强生成（RAG）任务的PEFT兼容微调Llama-2模型的方法，利用分布式训练，独特地利用了JAX的即时编译（JIT）和张量分片，实现了资源的高效管理，从而实现了加速微调并降低内存需求。这一进展显著提高了微调大型语言模型（LLMs）用于复杂RAG应用的可扩展性和可行性，甚至在GPU资源有限的系统上也能取得显著改进。

    arXiv:2403.11366v1 Announce Type: cross  Abstract: The scaling of Large Language Models (LLMs) for retrieval-based tasks, particularly in Retrieval Augmented Generation (RAG), faces significant memory constraints, especially when fine-tuning extensive prompt sequences. Current open-source libraries support full-model inference and fine-tuning across multiple GPUs but fall short of accommodating the efficient parameter distribution required for retrieved context. Addressing this gap, we introduce a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT) compilation and tensor-sharding for efficient resource management, thereby enabling accelerated fine-tuning with reduced memory requirements. This advancement significantly improves the scalability and feasibility of fine-tuning LLMs for complex RAG applications, even on systems with limited GPU resources. Our experiments show more than 1
    
[^46]: CantonMT: 汉英NMT平台，使用合成反向翻译数据对模型进行微调

    CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data

    [https://arxiv.org/abs/2403.11346](https://arxiv.org/abs/2403.11346)

    提出了CantonMT项目，利用合成反向翻译数据对粤语至英语NMT模型进行微调，并为研究人员提供用户友好的界面和开源工具包，以促进研究

    

    arXiv:2403.11346v1 消息类型：跨领域 摘要：对于低资源语言的神经机器翻译(NMT)仍然是自然语言处理研究人员面临的挑战。在这项工作中，我们将一个标准的数据增强方法——反向翻译，应用到了新的语言翻译方向粤语至英语。我们介绍了我们使用有限数量真实数据和生成的合成数据(包括OpusMT, NLLB,和mBART)进行微调的模型。我们使用了一系列不同指标包括基于词汇和嵌入的自动评估。此外，我们为这项\textsc{CantonMT}研究项目中包含的模型创建了一个用户友好的界面，并提供便利实现粤语至英语MT研究。研究人员可以通过我们的开源\textsc{CantonMT}工具包\url{https://github.com/kenrickkung/CantoneseTranslation}向平台添加更多模型。

    arXiv:2403.11346v1 Announce Type: cross  Abstract: Neural Machine Translation (NMT) for low-resource languages is still a challenging task in front of NLP researchers. In this work, we deploy a standard data augmentation methodology by back-translation to a new language translation direction Cantonese-to-English. We present the models we fine-tuned using the limited amount of real data and the synthetic data we generated using back-translation including OpusMT, NLLB, and mBART. We carried out automatic evaluation using a range of different metrics including lexical-based and embedding-based. Furthermore. we create a user-friendly interface for the models we included in this\textsc{ CantonMT} research project and make it available to facilitate Cantonese-to-English MT research. Researchers can add more models into this platform via our open-source\textsc{ CantonMT} toolkit \url{https://github.com/kenrickkung/CantoneseTranslation}.
    
[^47]: ConvSDG：用于会话搜索的会话数据生成

    ConvSDG: Session Data Generation for Conversational Search

    [https://arxiv.org/abs/2403.11335](https://arxiv.org/abs/2403.11335)

    基于大型语言模型的ConvSDG框架探索了生成更多带相关标签训练会话以提升会话搜索性能的方法。

    

    会话搜索通过允许用户与搜索引擎进行多轮交互提供了更便捷的搜索界面。然而，会话密集检索方法的有效性受到训练数据的稀缺性限制，这些数据需要用于微调。因此，生成更多带有相关标签的训练会话可能会提高搜索性能。基于大型语言模型（LLMs）在文本生成上的有益能力，我们提出了ConvSDG，这是一个简单而有效的框架，旨在探索使用LLM进行会话数据生成来提升会话搜索的可行性。在这个框架内，我们根据相关判断的可用性设计了对话/会话级和查询级数据生成的无监督和半监督学习。生成的数据被用于微调会话密集检索器。

    arXiv:2403.11335v1 Announce Type: cross  Abstract: Conversational search provides a more convenient interface for users to search by allowing multi-turn interaction with the search engine. However, the effectiveness of the conversational dense retrieval methods is limited by the scarcity of training data required for their fine-tuning. Thus, generating more training conversational sessions with relevant labels could potentially improve search performance. Based on the promising capabilities of large language models (LLMs) on text generation, we propose ConvSDG, a simple yet effective framework to explore the feasibility of boosting conversational search by using LLM for session data generation. Within this framework, we design dialogue/session-level and query-level data generation with unsupervised and semi-supervised learning, according to the availability of relevance judgments. The generated data are used to fine-tune the conversational dense retriever. Extensive experiments on four
    
[^48]: 通过将一个全局明确标注拆解成本地隐式多模态反馈来改进对话代理

    Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback

    [https://arxiv.org/abs/2403.11330](https://arxiv.org/abs/2403.11330)

    通过将全局明确标注拆解成本地隐式多模态反馈，提出了一种改进对话代理的方法，并在各种对话度量方面展现出一致的改进

    

    我们描述了一种方法，通过全局（即，对话级）奖励对齐基于LLM的对话代理，同时考虑到自然发生的多模态信号。在高层次上，我们的方法（名为GELI）通过将人类提供的全局明确（GE）会话级奖励拆分，利用本地隐式（LI）多模态奖励信号来跨模态地塑造奖励分解步骤。然后将这种分解的奖励模型作为标准RHLF流程的一部分，来改进基于LLM的对话代理。我们进行了定量和定性的人类研究，评估了我们的GELI方法的性能，并发现与基线方法相比，它在各种对话度量方面都表现出一致的改进。

    arXiv:2403.11330v1 Announce Type: cross  Abstract: We describe an approach for aligning an LLM-based dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring multimodal signals. At a high level, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session-level reward, using Local Implicit (LI} multimodal reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the standard RHLF pipeline improve an LLM-based dialog agent. We run quantitative and qualitative human studies to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods.
    
[^49]: 使用StateFlow增强LLM任务解决能力通过状态驱动工作流

    StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows

    [https://arxiv.org/abs/2403.11322](https://arxiv.org/abs/2403.11322)

    提出了一种使用StateFlow的新颖LLM任务解决范式，将复杂任务解决过程概念化为状态机，通过状态转换确保LLM响应的清晰跟踪和管理。

    

    使用大型语言模型（LLM）来解决复杂任务的趋势日益明显，例如需要一系列操作和与工具环境动态交互的任务。本文提出了StateFlow，一种新颖的基于LLM的任务求解范式，将由LLM支持的复杂任务解决过程概念化为状态机。通过正确构建状态和定义状态转换，StateFlow确定了任务求解的进展，确保清晰跟踪和管理LLM在整个任务求解过程中的响应。在每个状态中，StateFlow允许执行一系列动作，不仅包括根据特定提示指导生成LLM响应，还包括根据需要利用外部工具。状态转换由LLM做出的特定规则或决策控制，允许通过任务的预定义StateFlow模型动态自适应地进行进展。

    arXiv:2403.11322v1 Announce Type: cross  Abstract: It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and environments. In this paper, we propose StateFlow, a novel LLM-based task-solving paradigm that conceptualizes complex task-solving processes backed by LLMs as state machines. With proper construction of states and definition of state transitions, StateFlow grounds the progress of task-solving, ensuring clear tracking and management of LLMs' responses throughout the task-solving process. Within each state, StateFlow allows execution of a series of actions, involving not only the generation of LLM's responses guided by a specific prompt, but also the utilization of external tools as needed. State transitions are controlled by specific rules or decisions made by the LLM, allowing for a dynamic and adaptive progression through the task's pre-defined StateFlow model. Evalua
    
[^50]: 冻结LLMs的少样本VQA：两种方法的故事

    Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches

    [https://arxiv.org/abs/2403.11317](https://arxiv.org/abs/2403.11317)

    本论文对比了在使用LLMs进行少样本视觉问答（VQA）时，将视觉嵌入直接连接到LLM嵌入空间和使用图像描述两种方法的性能，发现对于特定LLM模型，在零样本情况下使用图像描述更好，在少样本情况下则取决于所选的上下文例子。

    

    两种输入图像到大型语言模型（LLMs）的方法已经出现。第一种是将图像描述成自然语言。第二种是将图像特征嵌入映射到LLMs的领域，并直接将映射后的嵌入传递给LLMs。大多数最近的少样本多模态工作使用了采用这两种方法变体的架构来报告性能。但它们忽视了它们之间的一个重要比较。我们设计了一个受控的、专注的实验来比较这两种方法在使用LLMs进行少样本视觉问答（VQA）方面的表现。我们的研究表明对于Flan-T5 XL，一个3B参数的LLM，将视觉嵌入直接连接到LLM嵌入空间不能保证比使用图像描述获得更好的性能。在零样本情况下，我们发现使用文本图像描述更好。在少样本情况下，所选上下文例子如何选择决定了哪种方法更好。

    arXiv:2403.11317v1 Announce Type: new  Abstract: Two approaches have emerged to input images into large language models (LLMs). The first is to caption images into natural language. The second is to map image feature embeddings into the domain of the LLM and pass the mapped embeddings directly to the LLM. The majority of recent few-shot multimodal work reports performance using architectures that employ variations of one of these two approaches. But they overlook an important comparison between them. We design a controlled and focused experiment to compare these two approaches to few-shot visual question answering (VQA) with LLMs. Our findings indicate that for Flan-T5 XL, a 3B parameter LLM, connecting visual embeddings directly to the LLM embedding space does not guarantee improved performance over using image captions. In the zero-shot regime, we find using textual image captions is better. In the few-shot regimes, how the in-context examples are selected determines which is better.
    
[^51]: 在Transformer中进行推理-减少伪相关性和推理捷径

    Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts

    [https://arxiv.org/abs/2403.11314](https://arxiv.org/abs/2403.11314)

    这项研究探讨了如何在Transformer模型中进行逻辑推理，通过在数据集中引入证明来训练两种模型，成功避免了伪相关性和推理捷径。

    

    Transformer语言模型是用于处理自然语言的神经网络，在许多需要逻辑推理的任务中被使用。然而，Transformer模型可能会轻易学习到数据中的伪模式，从而绕过实际推理过程。本文研究了Transformer在多大程度上可以被训练来a) 近似命题逻辑推理，同时b) 避免通过训练数据中的伪相关性引起的已知推理捷径。为此，我们使用了一个数据集，其中真实性与问题中规则数量等之间存在已知的伪相关性。我们通过证明增强了数据，并训练了两个模型：一个生成式Transformer，WP-BART，它在问题及其完整证明上进行训练；一个神经符号模型，SIP-BART，它在单个证明步骤上训练，并将生成式Transformer模型BART与符号推理检查器结合起来。我们发现SIP-BART成功地避免了推理捷径。

    arXiv:2403.11314v1 Announce Type: cross  Abstract: Transformer language models are neural networks used for a wide variety of tasks concerning natural language, including some that also require logical reasoning. However, a transformer model may easily learn spurious patterns in the data, short-circuiting actual reasoning. In this paper we investigate to what extent transformers can be trained to a) approximate reasoning in propositional logic while b) avoiding known reasoning shortcuts via spurious correlations in the training data. To do so, we use a dataset with known spurious correlation between truth and e.g. the number of rules in the problem. We augment the data with proofs, and train two models: a generative transformer, WP-BART, trained on problems and their whole proofs, and a neuro-symbolic model, SIP-BART, trained on individual proof steps and combining the generative transformer model BART with a symbolic proof checker. We find that SIP-BART succeeds in avoiding reasoning 
    
[^52]: 混合提示专家用于多模态语义理解

    Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding

    [https://arxiv.org/abs/2403.11311](https://arxiv.org/abs/2403.11311)

    提出了一种新型多模态软提示框架MoPE-BAF，用于解决少样本学习下的多模态讽刺检测和情感分析问题，通过三个软提示专家和块感知提示融合，实现了模态特征提取和多模态交互。

    

    arXiv:2403.11311v1 公告类型:新 抽象:在人工智能领域，深度多模态语义理解受到越来越多的关注，超越了单纯的表面内容关系挖掘。收集和注释高质量的多模态数据的挑战凸显了少样本学习的重要性。本文关注这一背景下的两个关键任务: 少样本多模态讽刺检测（MSD）和多模态情感分析（MSA）。为了解决这些问题，我们提出了基于统一视觉-语言模型（VLM）的一种新型多模态软提示框架Mixture-of-Prompt-Experts with Block-Aware Prompt Fusion（MoPE-BAF）。具体来说，我们设计了三个软提示专家: 一个文本提示和一个图像提示，用于提取特定于模态的特征以丰富单模态表示，以及一个统一提示以协助多模态交互。此外，我们将Transformer层重新组织为几个块，并实现了...

    arXiv:2403.11311v1 Announce Type: new  Abstract: Deep multimodal semantic understanding that goes beyond the mere superficial content relation mining has received increasing attention in the realm of artificial intelligence. The challenges of collecting and annotating high-quality multi-modal data have underscored the significance of few-shot learning. In this paper, we focus on two critical tasks under this context: few-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis (MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware Prompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on the unified vision-language model (VLM). Specifically, we design three experts of soft prompts: a text prompt and an image prompt that extract modality-specific features to enrich the single-modal representation, and a unified prompt to assist multi-modal interaction. Additionally, we reorganize Transformer layers into several blocks and intr
    
[^53]: SQ-LLaVA：自问自答的大型视觉-语言助手

    SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant

    [https://arxiv.org/abs/2403.11299](https://arxiv.org/abs/2403.11299)

    本研究引入了一个名为SQ-LLaVA的新颖框架，通过自我训练模型如何提出高质量问题，以改善视觉-语言模型的泛化能力。

    

    最近视觉-语言模型的发展在经过视觉指导调整后，在视觉-语言任务中展现出显着的泛化能力。然而，预训练视觉编码器和大型语言模型之间的鸿沟成为整个网络的瓶颈。为了改善跨模态对齐，现有的工作通常考虑涵盖更广泛的视觉任务范围的更多视觉指导数据，对模型进行微调以用于问答，但这种操作成本较高。然而，图像包含大量上下文信息，但这一方面一直鲜有人探索。本文首次尝试利用视觉指导数据内部被忽视的上下文，训练模型自我训练'学习'如何提出高质量问题。通过这种方式，我们引入了一个名为SQ-LLaVA的新颖框架：自问自答的大型视觉-语言助手。SQ-LLaVA在生成灵活且有意义的图像方面表现出高效性。

    arXiv:2403.11299v1 Announce Type: cross  Abstract: Recent advancements in the vision-language model have shown notable generalization in vision-language tasks after visual instruction tuning. However, bridging the gap between the pre-trained vision encoder and the large language models becomes the whole network's bottleneck. To improve cross-modality alignment, existing works usually consider more visual instruction data covering a broader range of vision tasks to fine-tune the model for question-answering, which are costly to obtain. However, the image contains rich contextual information that has been largely under-explored. This paper first attempts to harness this overlooked context within visual instruction data, training the model to self-supervised `learning' how to ask high-quality questions. In this way, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible and meaningful image-
    
[^54]: 对文本分类模型的一种修改的基于词显著性的对抗攻击

    A Modified Word Saliency-Based Adversarial Attack on Text Classification Models

    [https://arxiv.org/abs/2403.11297](https://arxiv.org/abs/2403.11297)

    本文介绍了一种修改的基于词显著性的对抗攻击方法，通过对模型最具影响力的词进行修改，旨在欺骗分类模型且保持语义连贯性，在提高攻击效果的同时避开了分类系统的检测。

    

    本文介绍了一种新型的针对文本分类模型的对抗攻击方法，称为修改的基于词显著性的对抗攻击（MWSAA）。该技术基于词显著性的概念，通过战略性地扰乱输入文本，旨在误导分类模型同时保持语义连贯性。通过改进传统的对抗攻击方法，MWSAA显著增强了其规避分类系统检测的效果。该方法首先通过显著性估计过程识别输入文本中的显著词，这些词对模型的决策过程具有最大影响。随后，这些显著词经过精心设计的修改，引导语义相似性度量来确保改变的文本保持连贯并保留其原始含义。在不同的文本分类数据集上进行了实证评估。

    arXiv:2403.11297v1 Announce Type: new  Abstract: This paper introduces a novel adversarial attack method targeting text classification models, termed the Modified Word Saliency-based Adversarial At-tack (MWSAA). The technique builds upon the concept of word saliency to strategically perturb input texts, aiming to mislead classification models while preserving semantic coherence. By refining the traditional adversarial attack approach, MWSAA significantly enhances its efficacy in evading detection by classification systems. The methodology involves first identifying salient words in the input text through a saliency estimation process, which prioritizes words most influential to the model's decision-making process. Subsequently, these salient words are subjected to carefully crafted modifications, guided by semantic similarity metrics to ensure that the altered text remains coherent and retains its original meaning. Empirical evaluations conducted on diverse text classification datasets
    
[^55]: 通过数据增强来改善作者验证的方法

    Forging the Forger: An Attempt to Improve Authorship Verification via Data Augmentation

    [https://arxiv.org/abs/2403.11265](https://arxiv.org/abs/2403.11265)

    通过引入合成示例的数据增强方法，可以改善在作者验证任务中对抗性攻击下的分类器预测。

    

    作者验证（AV）是一个文本分类任务，关注的是推断候选文本是由一个特定作者撰写还是由其他人撰写。已经显示许多AV系统容易受到敌对攻击的影响，其中恶意作者积极尝试欺骗分类器，方法是隐藏他们的写作风格，或者模仿另一位作者的风格。本文研究了将分类器训练集与（负面的）合成示例进行增强的潜在好处。这些合成示例是为了模仿感兴趣的作者的风格而生成的。我们分析了这种增强对在敌对环境下的AV任务中带来的分类器预测改进。具体来说，我们尝试了三种不同的生成器架构（一种基于循环神经网络，另一种基于小规模transformers，另一种基于流行的GPT模型）。

    arXiv:2403.11265v1 Announce Type: cross  Abstract: Authorship Verification (AV) is a text classification task concerned with inferring whether a candidate text has been written by one specific author or by someone else. It has been shown that many AV systems are vulnerable to adversarial attacks, where a malicious author actively tries to fool the classifier by either concealing their writing style, or by imitating the style of another author. In this paper, we investigate the potential benefits of augmenting the classifier training set with (negative) synthetic examples. These synthetic examples are generated to imitate the style of the author of interest. We analyze the improvements in classifier prediction that this augmentation brings to bear in the task of AV in an adversarial setting. In particular, we experiment with three different generator architectures (one based on Recurrent Neural Networks, another based on small-scale transformers, and another based on the popular GPT mod
    
[^56]: ChartThinker：一种优化图表摘要的上下文思维方法

    ChartThinker: A Contextual Chain-of-Thought Approach to Optimized Chart Summarization

    [https://arxiv.org/abs/2403.11236](https://arxiv.org/abs/2403.11236)

    该研究提出了一种名为ChartThinker的创新图表摘要方法，利用上下文思维和策略性的上下文检索实现深度分析，旨在提高生成摘要的逻辑连贯性和准确性。

    

    arXiv：2403.11236v1 公告类型：新型 摘要：数据可视化是呈现数据和挖掘其宝贵见解的重要手段。通过自然语言处理技术进行图表摘要的任务有助于深入分析图表数据。然而，现有方法在视觉-语言匹配和推理能力方面仍然存在明显不足。为解决这些局限性，本研究构建了一个大规模数据集，其中包含全面的图表标题配对和每个图表的微调说明。由于该数据集涵盖了各种主题和视觉风格，可以从训练数据的角度获得更好的匹配度。此外，我们提出了一种创新的图表摘要方法ChartThinker，它基于思维链和上下文检索策略综合深度分析，旨在提高生成摘要的逻辑连贯性和准确性。建立在精心策划Data

    arXiv:2403.11236v1 Announce Type: new  Abstract: Data visualization serves as a critical means for presenting data and mining its valuable insights. The task of chart summarization, through natural language processing techniques, facilitates in-depth data analysis of charts. However, there still are notable deficiencies in terms of visual-language matching and reasoning ability for existing approaches. To address these limitations, this study constructs a large-scale dataset of comprehensive chart-caption pairs and fine-tuning instructions on each chart. Thanks to the broad coverage of various topics and visual styles within this dataset, better matching degree can be achieved from the view of training data. Moreover, we propose an innovative chart summarization method, ChartThinker, which synthesizes deep analysis based on chains of thought and strategies of context retrieval, aiming to improve the logical coherence and accuracy of the generated summaries. Built upon the curated datas
    
[^57]: 从文字中提取临床标记的廉价方法

    Cheap Ways of Extracting Clinical Markers from Texts

    [https://arxiv.org/abs/2403.11227](https://arxiv.org/abs/2403.11227)

    该论文研究了从文本中提取临床标记的廉价方法，比较了传统机器学习方法和大型语言模型对于提取亮点和生成摘要的效果。

    

    本文描述了UniBuc考古团队为CLPsych 2024共享任务所进行的工作，该任务涉及在文本中寻找支持分配的自杀风险水平的证据。需要两种类型的证据：亮点（提取文本中的相关片段）和摘要（将证据聚合成综合）。我们的工作重点是评估大型语言模型（LLM），而不是一种更节省内存和资源的替代方法。第一种方法采用了传统的机器学习（GOML）管道，包括tf-idf向量化器和逻辑回归分类器，其代表性特征用于提取相关的亮点。第二种更消耗资源的方法使用LLM生成摘要，并由思维链指导提供指示临床标记的文本序列。

    arXiv:2403.11227v1 Announce Type: new  Abstract: This paper describes the work of the UniBuc Archaeology team for CLPsych's 2024 Shared Task, which involved finding evidence within the text supporting the assigned suicide risk level. Two types of evidence were required: highlights (extracting relevant spans within the text) and summaries (aggregating evidence into a synthesis). Our work focuses on evaluating Large Language Models (LLM) as opposed to an alternative method that is much more memory and resource efficient. The first approach employs a good old-fashioned machine learning (GOML) pipeline consisting of a tf-idf vectorizer with a logistic regression classifier, whose representative features are used to extract relevant highlights. The second, more resource intensive, uses an LLM for generating the summaries and is guided by chain-of-thought to provide sequences of text indicating clinical markers.
    
[^58]: 创造一种听起来像非洲裔美国人的TTS：指导方针、技术挑战和令人惊讶的评估

    Creating an African American-Sounding TTS: Guidelines, Technical Challenges,and Surprising Evaluations

    [https://arxiv.org/abs/2403.11209](https://arxiv.org/abs/2403.11209)

    本文探讨了在开发一种听起来像受过教育、专业、地区口音真实的非洲裔美国女性的美式英文文本转语音（TTS）系统过程中遇到的种族表现挑战，提出了指导方针、技术难题和对该系统的令人惊讶的评估。

    

    用户界面和机器人中的AI代理的表现主要是白人，不仅在面部和皮肤特征上，而且在他们使用的合成声音上也如此。在本文中，我们探讨了在开发一个旨在听起来像受过教育、专业、地区口音真实的非洲裔美国女性的美式英文文本转语音（TTS）系统过程中发现的有关种族表现的一些意想不到的挑战。论文首先介绍了与非洲裔美国IT专业人员进行的焦点小组讨论的结果，在此过程中讨论和收集了用于创建代表性和合适的TTS系统的指导方针和挑战，接着讨论了TTS系统开发人员面临的一些技术困难。然后，我们描述了两项与美式英语使用者进行的研究，参与者无法将非洲裔美国TTS声音正确归因为正确的种族，而他们在认出该声音时几乎全都正确识别。

    arXiv:2403.11209v1 Announce Type: new  Abstract: Representations of AI agents in user interfaces and robotics are predominantly White, not only in terms of facial and skin features, but also in the synthetic voices they use. In this paper we explore some unexpected challenges in the representation of race we found in the process of developing an U.S. English Text-to-Speech (TTS) system aimed to sound like an educated, professional, regional accent-free African American woman. The paper starts by presenting the results of focus groups with African American IT professionals where guidelines and challenges for the creation of a representative and appropriate TTS system were discussed and gathered, followed by a discussion about some of the technical difficulties faced by the TTS system developers. We then describe two studies with U.S. English speakers where the participants were not able to attribute the correct race to the African American TTS voice while overwhelmingly correctly recogn
    
[^59]: TRELM: 面向知识增强语言模型的稳健高效预训练

    TRELM: Towards Robust and Efficient Pre-training for Knowledge-Enhanced Language Models

    [https://arxiv.org/abs/2403.11203](https://arxiv.org/abs/2403.11203)

    本文介绍了TRELM，一种面向知识增强语言模型的稳健高效预训练框架，通过采用稳健方法注入知识三元组和知识增强的存储库来解决实体表示优化不足和预训练过程的问题。

    

    KEPLM是利用外部知识增强语言理解的预训练模型。之前的语言模型通过在知识图谱中学习关系三元组来促进知识获取。然而，这些模型并未优先学习与实体相关的令牌的嵌入。此外，更新KEPLM中的全部参数在计算上是费时的。本文介绍了TRELM，一种面向知识增强语言模型的稳健高效预训练框架。我们观察到文本语料库中的实体通常遵循长尾分布，一些实体的表示未经优化，妨碍了KEPLM的预训练过程。为了解决这个问题，我们采用了一种稳健方法来注入知识三元组，并采用了知识增强的存储库来捕获有价值的信息。此外，只更新小部分神经元

    arXiv:2403.11203v1 Announce Type: new  Abstract: KEPLMs are pre-trained models that utilize external knowledge to enhance language understanding. Previous language models facilitated knowledge acquisition by incorporating knowledge-related pre-training tasks learned from relation triples in knowledge graphs. However, these models do not prioritize learning embeddings for entity-related tokens. Moreover, updating the entire set of parameters in KEPLMs is computationally demanding. This paper introduces TRELM, a Robust and Efficient Pre-training framework for Knowledge-Enhanced Language Models. We observe that entities in text corpora usually follow the long-tail distribution, where the representations of some entities are suboptimally optimized and hinder the pre-training process for KEPLMs. To tackle this, we employ a robust approach to inject knowledge triples and employ a knowledge-augmented memory bank to capture valuable information. Furthermore, updating a small subset of neurons 
    
[^60]: 从非侵入性脑记录解码连续基于字符的语言

    Decoding Continuous Character-based Language from Non-invasive Brain Recordings

    [https://arxiv.org/abs/2403.11183](https://arxiv.org/abs/2403.11183)

    提出了一种从单次非侵入性脑记录中解码连续语言的新方法，通过三维卷积网络和信息瓶颈结合字符解码器，能够实现在和跨主体之间产生捕捉感知语音含义的可理解文本序列。

    

    通过非侵入性设备从大脑活动中解读自然语言仍然是一个艰巨的挑战。之前的非侵入式解码器要么需要进行多次实验来确定皮层区域并增强大脑活动的信噪比，要么仅限于识别基本的语言元素如字母和单词。我们提出了一种新的方法，从单次非侵入性fMRI记录中解码连续语言，其中开发了一个三维卷积网络，该网络配备信息瓶颈以自动识别对刺激有响应的体素，并设计了一个基于字符的解码器, 用于对内在字符结构所特征化的连续语言进行语义重建。所得解码器能够生成可理解的文本序列，忠实地捕捉感知语音的含义，无论是在同一主体内还是跨主体之间。

    arXiv:2403.11183v1 Announce Type: new  Abstract: Deciphering natural language from brain activity through non-invasive devices remains a formidable challenge. Previous non-invasive decoders either require multiple experiments with identical stimuli to pinpoint cortical regions and enhance signal-to-noise ratios in brain activity, or they are limited to discerning basic linguistic elements such as letters and words. We propose a novel approach to decoding continuous language from single-trial non-invasive fMRI recordings, in which a three-dimensional convolutional network augmented with information bottleneck is developed to automatically identify responsive voxels to stimuli, and a character-based decoder is designed for the semantic reconstruction of continuous language characterized by inherent character structures. The resulting decoder can produce intelligible textual sequences that faithfully capture the meaning of perceived speech both within and across subjects, while existing d
    
[^61]: 面向现实世界图像质量评估的质量感知图像-文本对齐

    Quality-Aware Image-Text Alignment for Real-World Image Quality Assessment

    [https://arxiv.org/abs/2403.11176](https://arxiv.org/abs/2403.11176)

    提出了一种基于CLIP的自监督方法QualiCLIP，通过质量感知的图像-文本对齐策略，实现了图像质量评估不需要标记MOS的问题

    

    无参考图像质量评估（NR-IQA）致力于设计一种在没有高质量参考图像的情况下测量图像质量的方法，以符合人类感知，大部分最先进的NR-IQA方法中依赖标注的主观评分（MOS）限制了它们在真实场景中的可扩展性和广泛适用性。为了克服这一限制，我们提出了QualiCLIP（Quality-aware CLIP），这是一种基于CLIP的自监督不需要标记MOS的方法。具体来说，我们引入了一种质量感知的图像-文本对齐策略，使得CLIP生成的表示与图像固有质量相关。从原始图像开始，我们使用不断增加的强度合成地劣化它们。然后，我们训练CLIP根据其与质量相关的反义文本提示的相似性对这些降解图像进行排名，同时保证一致的表达

    arXiv:2403.11176v1 Announce Type: cross  Abstract: No-Reference Image Quality Assessment (NR-IQA) focuses on designing methods to measure image quality in alignment with human perception when a high-quality reference image is unavailable. The reliance on annotated Mean Opinion Scores (MOS) in the majority of state-of-the-art NR-IQA approaches limits their scalability and broader applicability to real-world scenarios. To overcome this limitation, we propose QualiCLIP (Quality-aware CLIP), a CLIP-based self-supervised opinion-unaware method that does not require labeled MOS. In particular, we introduce a quality-aware image-text alignment strategy to make CLIP generate representations that correlate with the inherent quality of the images. Starting from pristine images, we synthetically degrade them with increasing levels of intensity. Then, we train CLIP to rank these degraded images based on their similarity to quality-related antonym text prompts, while guaranteeing consistent represe
    
[^62]: 使用大型语言模型纠正社交媒体上的错误信息

    Correcting misinformation on social media with a large language model

    [https://arxiv.org/abs/2403.11169](https://arxiv.org/abs/2403.11169)

    提出了一种名为MUSE的大型语言模型，通过访问最新信息并评估可信度，以解决社交媒体上误信息纠正的难题。

    

    误信息会破坏公众对科学和民主的信任，特别是在社交媒体上，不准确信息会迅速传播。专家和普通人通过手动识别和解释不准确信息已经被证明是有效的纠正误信息的方法。然而，这种方法很难扩展，这是一个担忧，因为大型语言模型（LLMs）等技术使误信息更容易生成。LLMs还具有多功能能力，可以加速纠正误信息；然而，它们由于缺乏最新信息、倾向于生成似是而非的内容和引用以及无法处理多模态信息而面临困难。为了解决这些问题，我们提出了MUSE，这是一个带有最新信息访问和可信度评估的LLM。通过检索上下文证据和反驳，MUSE可以提供准确可信的解释和参考。它还描述

    arXiv:2403.11169v1 Announce Type: cross  Abstract: Misinformation undermines public trust in science and democracy, particularly on social media where inaccuracies can spread rapidly. Experts and laypeople have shown to be effective in correcting misinformation by manually identifying and explaining inaccuracies. Nevertheless, this approach is difficult to scale, a concern as technologies like large language models (LLMs) make misinformation easier to produce. LLMs also have versatile capabilities that could accelerate misinformation correction; however, they struggle due to a lack of recent information, a tendency to produce plausible but false content and references, and limitations in addressing multimodal information. To address these issues, we propose MUSE, an LLM augmented with access to and credibility evaluation of up-to-date information. By retrieving contextual evidence and refutations, MUSE can provide accurate and trustworthy explanations and references. It also describes 
    
[^63]: 法律领域中大型语言模型(LLMs)的评估伦理

    Evaluation Ethics of LLMs in Legal Domain

    [https://arxiv.org/abs/2403.11152](https://arxiv.org/abs/2403.11152)

    评估大型语言模型在法律领域的伦理层面的重要性以确保其有效整合，提出了一种基于法律案例的新颖评估方法，并对大型语言模型的基本语言能力、专业法律知识和法律稳健性进行了全面评估

    

    近年来，大型语言模型在自然语言对话中的应用日益增多，导致它们在各个领域被广泛采用。然而，它们在应对诸如法律等专业领域特定挑战方面的通用能力仍受到质疑。研究人员忽视了将法律伦理纳入模型的重要性。我们主张严格的伦理评估对于确保大型语言模型在法律领域有效整合至关重要，强调了评估领域特定能力和领域特定伦理的必要性。为解决这一问题，我们提出了一种创新的评估方法，利用真实法律案例来评估大型语言模型(LLMs)的基本语言能力、专业法律知识和法律稳健性。我们的全面评估结果对学术界围绕法律领域的讨论做出了重要贡献。

    arXiv:2403.11152v1 Announce Type: cross  Abstract: In recent years, the utilization of large language models for natural language dialogue has gained momentum, leading to their widespread adoption across various domains. However, their universal competence in addressing challenges specific to specialized fields such as law remains a subject of scrutiny. The incorporation of legal ethics into the model has been overlooked by researchers. We asserts that rigorous ethic evaluation is essential to ensure the effective integration of large language models in legal domains, emphasizing the need to assess domain-specific proficiency and domain-specific ethic. To address this, we propose a novelty evaluation methodology, utilizing authentic legal cases to evaluate the fundamental language abilities, specialized legal knowledge and legal robustness of large language models (LLMs). The findings from our comprehensive evaluation contribute significantly to the academic discourse surrounding the s
    
[^64]: 一项用于对话立场检测的挑战数据集和有效模型

    A Challenge Dataset and Effective Models for Conversational Stance Detection

    [https://arxiv.org/abs/2403.11145](https://arxiv.org/abs/2403.11145)

    介绍了一个新的多轮对话立场检测数据集（MT-CSD），提出了一种全局局部注意力网络（GLAN）来解决对话数据中的长距离和短距离依赖性。

    

    先前的立场检测研究通常集中在评估单个实例内的立场，从而在有效建模关于同一特定主题的多方讨论方面存在局限性，这在真实社交媒体互动中自然发生。这种限制主要是由于缺乏真实复制真实社交媒体背景的数据集，阻碍了对话立场检测的研究进展。在本文中，我们介绍了一个新的多轮对话立场检测数据集（称为\textbf{MT-CSD}），涵盖了用于对话立场检测的多个目标。为了从这一具有挑战性的数据集中获取立场，我们提出了一个全局局部注意力网络（\textbf{GLAN}），以解决对话数据中固有的长距离和短距离依赖性。值得注意的是，即使是GLAN这样的最先进的立场检测方法，在......

    arXiv:2403.11145v1 Announce Type: new  Abstract: Previous stance detection studies typically concentrate on evaluating stances within individual instances, thereby exhibiting limitations in effectively modeling multi-party discussions concerning the same specific topic, as naturally transpire in authentic social media interactions. This constraint arises primarily due to the scarcity of datasets that authentically replicate real social media contexts, hindering the research progress of conversational stance detection. In this paper, we introduce a new multi-turn conversation stance detection dataset (called \textbf{MT-CSD}), which encompasses multiple targets for conversational stance detection. To derive stances from this challenging dataset, we propose a global-local attention network (\textbf{GLAN}) to address both long and short-range dependencies inherent in conversational data. Notably, even state-of-the-art stance detection methods, exemplified by GLAN, exhibit an accuracy of on
    
[^65]: 探索令牌化策略和词汇量对增强阿拉伯语言模型的影响

    Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models

    [https://arxiv.org/abs/2403.11130](https://arxiv.org/abs/2403.11130)

    本文研究了令牌化策略和词汇量对阿拉伯语言模型性能的影响，结果显示字节对编码（BPE）与Farasa在多个任务中表现优异，突显了形态分析在捕捉阿拉伯语言细微差异中的重要性。

    

    本文全面研究了令牌化策略和词汇量对阿拉伯语言模型在下游自然语言处理任务中性能的影响。我们的研究重点关注了四种令牌化器在各种任务中的有效性，包括新闻分类、仇恨言论检测、情感分析和自然语言推理。利用多样化的词汇量，我们仔细研究了令牌化方法与模型性能之间的复杂相互作用。结果表明，使用Farasa的字节对编码（BPE）在多个任务中优于其他策略，强调了在捕捉阿拉伯语言细微差异方面形态分析的重要性。然而，在情感分析中存在挑战，方言特定的分割问题影响了模型的效率。计算效率分析表明，BPE与Farasa的稳定性较高。

    arXiv:2403.11130v1 Announce Type: new  Abstract: This paper presents a comprehensive examination of the impact of tokenization strategies and vocabulary sizes on the performance of Arabic language models in downstream natural language processing tasks. Our investigation focused on the effectiveness of four tokenizers across various tasks, including News Classification, Hate Speech Detection, Sentiment Analysis, and Natural Language Inference. Leveraging a diverse set of vocabulary sizes, we scrutinize the intricate interplay between tokenization approaches and model performance. The results reveal that Byte Pair Encoding (BPE) with Farasa outperforms other strategies in multiple tasks, underscoring the significance of morphological analysis in capturing the nuances of the Arabic language. However, challenges arise in sentiment analysis, where dialect specific segmentation issues impact model efficiency. Computational efficiency analysis demonstrates the stability of BPE with Farasa, su
    
[^66]: 使用理由和结构感知因果问答增强事件因果识别

    Enhancing Event Causality Identification with Rationale and Structure-Aware Causal Question Answering

    [https://arxiv.org/abs/2403.11129](https://arxiv.org/abs/2403.11129)

    提出了一个多任务学习框架，通过理由和结构感知的因果问答来增强事件因果识别，将DECI任务转化为多项选择问题回答，生成被问及事件的因果关系以及解释这些事件具有因果关系的理由，构建了一个事件结构图对多跳潜在关系进行建模。

    

    文档级事件因果识别（DECI）旨在识别文档中两个事件之间的因果关系。最近的研究倾向于使用预训练语言模型生成事件因果关系。然而，由于文档中存在多个事件，这些方法容易出现顺序生成错误。此外，潜在的结构，如事件共指和相关因果链被忽视。在本文中，我们提出了一个多任务学习框架，通过理由和结构感知的因果问答来增强事件因果识别。具体而言，DECI任务被转化为多项选择问题回答，然后利用大型语言模型生成被问及事件的因果关系。此外，我们生成理由来解释为什么这些事件具有因果关系。此外，我们构建了一个事件结构图，对多跳潜在关系进行建模。

    arXiv:2403.11129v1 Announce Type: new  Abstract: Document-level Event Causality Identification (DECI) aims to identify causal relations between two events in documents. Recent research tends to use pre-trained language models to generate the event causal relations. Whereas, these methods are prone to the errors of sequential generation due to multiple events in a document. Moreover, the potential structures such as event coreference and related causal chain are neglected. In this paper, we propose a multi-task learning framework to enhance event causality identification with rationale and structure-aware causal question answering. Specifically, the DECI task is transformed into multiple-choice question answering, and the causes and effects of the questioned event are generated with large language models. In addition, we generate the rationales to explain why these events have causal relations. Moreover, we construct an event structure graph, which models the multi-hop potential relatio
    
[^67]: 超越静态评估：一种动态方法来评估人工智能助手的API调用能力

    Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants' API Invocation Capabilities

    [https://arxiv.org/abs/2403.11128](https://arxiv.org/abs/2403.11128)

    提出了自动动态评估（AutoDE）方法，用于评估人工智能助手的API调用能力，避免静态评估中导致的误导性评估。

    

    随着大型语言模型（LLMs）的兴起，人工智能助手利用工具的能力，特别是通过API调用，已经显著提升。这种进步需要更准确的评估方法。许多现有研究采用静态评估，即基于预定义的对话历史评估人工智能助手的API调用。然而，这种评估方法可能会具有误导性，因为在实际情况下，人工智能助手可能无法根据之前的人类互动生成API调用。我们提出了自动动态评估（AutoDE），以评估助手的API调用能力，而无需人类参与以及耗费大量资源的直接人机交互方法。在我们的框架中，我们努力模拟真实的人机交互中的人类对话模式，使用基于LLM的用户代理，并配备用户脚本以确保人机对齐。实验结果突显出AutoDE揭示了错误。

    arXiv:2403.11128v1 Announce Type: new  Abstract: With the rise of Large Language Models (LLMs), AI assistants' ability to utilize tools, especially through API calls, has advanced notably. This progress has necessitated more accurate evaluation methods. Many existing studies adopt static evaluation, where they assess AI assistants' API call based on pre-defined dialogue histories. However, such evaluation method can be misleading, as an AI assistant might fail in generating API calls from preceding human interaction in real cases. Instead of the resource-intensive method of direct human-machine interactions, we propose Automated Dynamic Evaluation (AutoDE) to assess an assistant's API call capability without human involvement. In our framework, we endeavor to closely mirror genuine human conversation patterns in human-machine interactions, using a LLM-based user agent, equipped with a user script to ensure human alignment. Experimental results highlight that AutoDE uncovers errors over
    
[^68]: 在人类对齐中扩展数据多样性以微调语言模型

    Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment

    [https://arxiv.org/abs/2403.11124](https://arxiv.org/abs/2403.11124)

    更多的响应但更少的提示可以更好地触发大型语言模型进行人类对齐，此外，提出了一种新的提示多样性公式，可以进一步影响LLMs的最终性能。

    

    与人类偏好对齐可以防止大型语言模型（LLMs）生成误导性或有毒内容，同时需要高成本的人类反馈。假设人工注释资源有限，则可以考虑两种不同的分配方式：更多样化的提示或更多样化的待标记响应。然而，它们对结果的影响的直接比较尚不存在。在这项工作中，我们首先根据微调样本数量控制双方的多样性，这可以直接反映它们的影响。我们发现，与大量提示不同，更多的响应但是更少的提示更能激发LLMs进行人类对齐。此外，提示的多样性概念可能比通常由单个数字量化的响应更复杂。因此，提出了提示多样性的新公式，进一步暗示与微调后LLMs最终性能的线性相关。

    arXiv:2403.11124v1 Announce Type: cross  Abstract: Alignment with human preference prevents large language models (LLMs) from generating misleading or toxic content while requiring high-cost human feedback. Assuming resources of human annotation are limited, there are two different ways of allocating considered: more diverse PROMPTS or more diverse RESPONSES to be labeled. Nonetheless, a straightforward comparison between their impact is absent. In this work, we first control the diversity of both sides according to the number of samples for fine-tuning, which can directly reflect their influence. We find that instead of numerous prompts, more responses but fewer prompts better trigger LLMs for human alignment. Additionally, the concept of diversity for prompts can be more complex than responses that are typically quantified by single digits. Consequently, a new formulation of prompt diversity is proposed, further implying a linear correlation with the final performance of LLMs after f
    
[^69]: Granular Change Accuracy: 一种更准确的对话状态跟踪性能度量

    Granular Change Accuracy: A More Accurate Performance Metric for Dialogue State Tracking

    [https://arxiv.org/abs/2403.11123](https://arxiv.org/abs/2403.11123)

    Granular Change Accuracy (GCA) 是一种更准确的对话状态跟踪性能度量，有效减少了由于分布均匀性和对话轮之间错误定位而产生的偏见，特别适用于评估少样本或零样本训练的模型。

    

    目前用于评估对话状态跟踪（DST）系统的度量表现出三个主要限制：i）错误地假定对话中插槽的分布是均匀的，ii）忽视为单个对话轮分配部分分数，iii）经常通过反复计算模型成功或失败预测的次数来高估或低估性能。为了解决这些缺陷，我们引入了一种新的度量标准：Granular Change Accuracy（GCA）。GCA专注于评估整个对话历史上对话状态的预测变化。基准测试显示，GCA有效地减少了由于分布均匀性和错误定位不同对话轮之间而产生的偏见，从而实现了更精确的评估。值得注意的是，在评估少样本或零样本训练的模型时，我们发现这些偏见特别明显，在模型错误率增加时更加显著。因此，GCA在提供显著性方面提供了

    arXiv:2403.11123v1 Announce Type: new  Abstract: Current metrics for evaluating Dialogue State Tracking (DST) systems exhibit three primary limitations. They: i) erroneously presume a uniform distribution of slots throughout the dialog, ii) neglect to assign partial scores for individual turns, iii) frequently overestimate or underestimate performance by repeatedly counting the models' successful or failed predictions. To address these shortcomings, we introduce a novel metric: Granular Change Accuracy (GCA). GCA focuses on evaluating the predicted changes in dialogue state over the entire dialogue history. Benchmarking reveals that GCA effectively reduces biases arising from distribution uniformity and the positioning of errors across turns, resulting in a more precise evaluation. Notably, we find that these biases are particularly pronounced when evaluating few-shot or zero-shot trained models, becoming even more evident as the model's error rate increases. Hence, GCA offers signific
    
[^70]: HarmPot: 一个用于评估社交媒体文本离线危害潜力的注释框架

    HarmPot: An Annotation Framework for Evaluating Offline Harm Potential of Social Media Text

    [https://arxiv.org/abs/2403.11108](https://arxiv.org/abs/2403.11108)

    论文讨论了一个注释模式的开发，用于构建数据集以评估社交媒体文本的离线危害潜力，不仅关注单一分裂因素或仇恨言论，而是致力于衡量在线内容的危害潜力，无论其内容是否充满仇恨。

    

    在这篇论文中，我们讨论了开发一个注释模式来构建数据集，以评估社交媒体文本的离线危害潜力。我们将“危害潜力”定义为在线公开帖子引发真实世界身体伤害（即暴力）的潜力。我们意识到真实世界的暴力通常是由一系列触发因素共同引发的，这些因素结合了几种在线战术和社会环境中现有的交叉分歧，从而导致有针对性的身体暴力。因此，我们不专注于任何单一的分裂性方面（如种姓、性别、宗教或受害者和施害者的其他身份），也不仅专注于仇恨言论或错误/偏斜信息。相反，我们对此类触发因素的交叉原因的理解聚焦于尝试衡量在线内容的危害潜力，无论其是否充满仇恨。在本文中，我们讨论了一个用于构建用于评估社交媒体文本离线危害潜力的数据集的注释框架/注释模式的发展。

    arXiv:2403.11108v1 Announce Type: new  Abstract: In this paper, we discuss the development of an annotation schema to build datasets for evaluating the offline harm potential of social media texts. We define "harm potential" as the potential for an online public post to cause real-world physical harm (i.e., violence). Understanding that real-world violence is often spurred by a web of triggers, often combining several online tactics and pre-existing intersectional fissures in the social milieu, to result in targeted physical violence, we do not focus on any single divisive aspect (i.e., caste, gender, religion, or other identities of the victim and perpetrators) nor do we focus on just hate speech or mis/dis-information. Rather, our understanding of the intersectional causes of such triggers focuses our attempt at measuring the harm potential of online content, irrespective of whether it is hateful or not. In this paper, we discuss the development of a framework/annotation schema that 
    
[^71]: ProgGen:通过自反大语言模型逐步生成命名实体识别数据集

    ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models

    [https://arxiv.org/abs/2403.11103](https://arxiv.org/abs/2403.11103)

    通过让大语言模型自省特定领域，生成领域相关属性并创建属性丰富的训练数据，同时绕过复杂结构的挑战，实现了生成命名实体识别数据集的创新策略。

    

    虽然大语言模型（LLMs）在跨领域上表现出卓越的适应性，但这些模型在结构化知识提取任务（如命名实体识别NER）方面经常表现不佳。本文探讨了一种创新的、具有成本效益的策略，利用具有适度NER能力的LLMs来生成优秀的NER数据集。我们的方法不同于基本的类条件提示，而是指导LLMs对特定领域进行自我反思，从而生成具有领域相关属性（例如影评的类别和情感）的属性丰富的训练数据。此外，我们预先生成实体术语，然后围绕这些实体开发NER上下文数据，有效规避了LLMs对复杂结构的挑战。我们在通用和专业领域展开的实验显示，相对于传统数据生成方法，性能得到了显著提升，同时成本更低。

    arXiv:2403.11103v1 Announce Type: new  Abstract: Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER). This paper explores an innovative, cost-efficient strategy to harness LLMs with modest NER capabilities for producing superior NER datasets. Our approach diverges from the basic class-conditional prompts by instructing LLMs to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data. Furthermore, we preemptively generate entity terms and then develop NER context data around these entities, effectively bypassing the LLMs' challenges with complex structures. Our experiments across both general and niche domains reveal significant performance enhancements over conventional data generation methods while being mor
    
[^72]: 翻译困境？跨语言概念上对文本到图像模型公平评估的翻译错误与挑战

    Lost in Translation? Translation Errors and Challenges for Fair Assessment of Text-to-Image Models on Multilingual Concepts

    [https://arxiv.org/abs/2403.11092](https://arxiv.org/abs/2403.11092)

    本研究发现在一个文本到图像模型基准中存在西班牙语、日语和中文中不同程度的翻译错误，提供了纠正，并分析了对基准性能的影响。

    

    通过将概念列表翻译成七种语言，将刺激生成的图像与预期的图像分布进行比较，评估了文本到图像（T2I）模型的多语言能力基准。不幸的是，我们发现这个基准在西班牙语、日语和中文中存在不同严重程度的翻译错误。我们提供了这些错误的更正，并分析了它们对CoCo-CroLa作为基准的效用和有效性的影响。我们通过修订后重新评估了多个基线T2I模型，比较了在新翻译下引发的输出与在旧翻译下引发的输出，并展示了修正对图像领域基准结果的影响程度是可以预测的。

    arXiv:2403.11092v1 Announce Type: cross  Abstract: Benchmarks of the multilingual capabilities of text-to-image (T2I) models compare generated images prompted in a test language to an expected image distribution over a concept set. One such benchmark, "Conceptual Coverage Across Languages" (CoCo-CroLa), assesses the tangible noun inventory of T2I models by prompting them to generate pictures from a concept list translated to seven languages and comparing the output image populations. Unfortunately, we find that this benchmark contains translation errors of varying severity in Spanish, Japanese, and Chinese. We provide corrections for these errors and analyze how impactful they are on the utility and validity of CoCo-CroLa as a benchmark. We reassess multiple baseline T2I models with the revisions, compare the outputs elicited under the new translations to those conditioned on the old, and show that a correction's impactfulness on the image-domain benchmark results can be predicted in t
    
[^73]: m&m's: 一个用于评估多步骤多模态任务工具使用的基准

    m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks

    [https://arxiv.org/abs/2403.11085](https://arxiv.org/abs/2403.11085)

    m&m's引入了一个包含4K+多步骤多模态任务的基准，涉及33种工具，用于评估LLM作为规划器的设计决策。

    

    现实世界中的多模态问题很少由单个机器学习模型解决，通常需要多步骤计算计划，涉及拼接多个模型。 工具增强型LLM极有可能自动化生成这种计算计划。然而，缺乏用于评估LLM作为多步骤多模态任务规划器的标准化基准，阻碍了对规划器设计决策的系统研究。LLM是否应一次性生成整个计划还是逐步生成？它们是否应该直接使用Python代码调用工具，还是通过类似JSON的结构化数据格式？反馈是否改善规划？为了回答这些问题以及更多问题，我们介绍了m&m's：一个基准，包含4K+个涉及33种工具的多步骤多模态任务，其中包括多模态模型、(免费)公共API和图像处理模块。对于每个任务查询，我们提供使用这种方法自动生成的计划。

    arXiv:2403.11085v1 Announce Type: cross  Abstract: Real-world multi-modal problems are rarely solved by a single machine learning model, and often require multi-step computational plans that involve stitching several models. Tool-augmented LLMs hold tremendous promise for automating the generation of such computational plans. However, the lack of standardized benchmarks for evaluating LLMs as planners for multi-step multi-modal tasks has prevented a systematic study of planner design decisions. Should LLMs generate a full plan in a single shot or step-by-step? Should they invoke tools directly with Python code or through structured data formats like JSON? Does feedback improve planning? To answer these questions and more, we introduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks involving 33 tools that include multi-modal models, (free) public APIs, and image processing modules. For each of these task queries, we provide automatically generated plans using this realis
    
[^74]: 为多模态异常检测和推理定制视觉-语言基础模型

    Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning

    [https://arxiv.org/abs/2403.11083](https://arxiv.org/abs/2403.11083)

    该研究旨在开发一种适用于多种场景的通用异常检测模型，通过定制视觉-语言基础模型和引入多模态提示策略进行多模态异常检测和推理。

    

    异常检测在各种工业场景中十分重要，包括生产线上异常模式的识别和用于质量控制的制造缺陷检测。本研究旨在开发一种适用于多种场景的通用异常检测模型。为实现这一目标，我们将拥有广泛知识和强大推理能力的通用视觉-语言基础模型定制为异常检测器和推理器。具体来说，我们引入了一种多模态提示策略，将领域专家的领域知识作为条件引导模型。我们的方法考虑多模态提示类型，包括任务描述、类别上下文、正常规则和参考图像。另外，我们将多模态输入表示统一为2D图像格式，使其能够

    arXiv:2403.11083v1 Announce Type: cross  Abstract: Anomaly detection is vital in various industrial scenarios, including the identification of unusual patterns in production lines and the detection of manufacturing defects for quality control. Existing techniques tend to be specialized in individual scenarios and lack generalization capacities. In this study, we aim to develop a generic anomaly detection model applicable across multiple scenarios. To achieve this, we customize generic visual-language foundation models that possess extensive knowledge and robust reasoning abilities into anomaly detectors and reasoners. Specifically, we introduce a multi-modal prompting strategy that incorporates domain knowledge from experts as conditions to guide the models. Our approach considers multi-modal prompt types, including task descriptions, class context, normality rules, and reference images. In addition, we unify the input representation of multi-modality into a 2D image format, enabling m
    
[^75]: RobustSentEmbed：使用对抗自监督对比学习的稳健句子嵌入

    RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning

    [https://arxiv.org/abs/2403.11082](https://arxiv.org/abs/2403.11082)

    RobustSentEmbed是一个自监督句子嵌入框架，通过对抗对比学习提高了文本表示任务中的泛化性和稳健性，实现了在对抗攻击中的优越表现。

    

    预训练语言模型（PLMs）在各种自然语言处理任务中展现出了出色的性能。然而，尽管它们在未见数据上取得成功，但目前基于PLM的表示通常在对抗设置中表现出较差的稳健性。本文引入了RobustSentEmbed，这是一个自监督句子嵌入框架，旨在改善不同文本表示任务中的泛化性和稳健性，以及对抗各种对抗攻击。通过生成高风险的对抗扰动并将其用于新颖的目标函数中，RobustSentEmbed巧妙地学习高质量和稳健的句子嵌入。我们的实验证实了RobustSentEmbed优于最先进表示方法的优越性。具体来说，我们的框架显著降低了各种对抗攻击的成功率，特别是降低了

    arXiv:2403.11082v1 Announce Type: cross  Abstract: Pre-trained language models (PLMs) have consistently demonstrated outstanding performance across a diverse spectrum of natural language processing tasks. Nevertheless, despite their success with unseen data, current PLM-based representations often exhibit poor robustness in adversarial settings. In this paper, we introduce RobustSentEmbed, a self-supervised sentence embedding framework designed to improve both generalization and robustness in diverse text representation tasks and against a diverse set of adversarial attacks. Through the generation of high-risk adversarial perturbations and their utilization in a novel objective function, RobustSentEmbed adeptly learns high-quality and robust sentence embeddings. Our experiments confirm the superiority of RobustSentEmbed over state-of-the-art representations. Specifically, Our framework achieves a significant reduction in the success rate of various adversarial attacks, notably reducing
    
[^76]: 基于深度学习的波斯语情感分析

    Deep Learning-based Sentiment Analysis in Persian Language

    [https://arxiv.org/abs/2403.11069](https://arxiv.org/abs/2403.11069)

    该研究介绍并实现了一种基于深度学习的混合模型，用于波斯语情感分析，通过对Digikala网站的客户评论数据进行实验，最终取得了令人印象深刻的78.3的F1分数。

    

    最近，人们对使用深度学习技术处理自然语言处理（NLP）任务表现出越来越大的兴趣，情感分析尤其在波斯语中是一个最具挑战性的领域之一。本研究中，我们引入并实现了一种基于深度学习的混合模型用于情感分析，使用来自Digikala在线零售商网站的客户评论数据。我们采用了各种深度学习网络和正则化技术作为分类器。最终，我们的混合方法表现出令人印象深刻的性能，实现了78.3的F1分数。

    arXiv:2403.11069v1 Announce Type: new  Abstract: Recently, there has been a growing interest in the use of deep learning techniques for tasks in natural language processing (NLP), with sentiment analysis being one of the most challenging areas, particularly in the Persian language. The vast amounts of content generated by Persian users on thousands of websites, blogs, and social networks such as Telegram, Instagram, and Twitter present a rich resource of information. Deep learning techniques have become increasingly favored for extracting insights from this extensive pool of raw data, although they face several challenges. In this study, we introduced and implemented a hybrid deep learning-based model for sentiment analysis, using customer review data from the Digikala Online Retailer website. We employed a variety of deep learning networks and regularization techniques as classifiers. Ultimately, our hybrid approach yielded an impressive performance, achieving an F1 score of 78.3 acro
    
[^77]: 预训练语言模型在代表某些地理人口方面较其他人更好

    Pre-Trained Language Models Represent Some Geographic Populations Better Than Others

    [https://arxiv.org/abs/2403.11025](https://arxiv.org/abs/2403.11025)

    预训练语言模型在代表地理人口方面存在明显偏向，表现优秀的人口主要集中在美国和英国，而南亚和东南亚地区的人口则被较差地代表。

    

    这篇论文衡量了两种LLMs家族在代表多样化地理人口方面的偏向程度。使用空间探测任务和地理参考语料库衡量了OPT和BLOOM系列预训练语言模型在全球不同人口中的代表性程度。结果显示，这些模型对某些人口表现得比其他人口更好。特别是，美国和英国的人口被非常好地代表，而南亚和东南亚地区的人口则被较差地代表。分析表明，这两个模型家族在不同人口中基本上共享相同的偏向。同时，这种偏向不能完全通过社会语言因素、经济因素或地理因素来解释。从这个分析中得出的基本结论是，预训练模型并不平等地代表了世界人口：存在着特定地理人口的明显偏向。这一发现挑战了

    arXiv:2403.11025v1 Announce Type: new  Abstract: This paper measures the skew in how well two families of LLMs represent diverse geographic populations. A spatial probing task is used with geo-referenced corpora to measure the degree to which pre-trained language models from the OPT and BLOOM series represent diverse populations around the world. Results show that these models perform much better for some populations than others. In particular, populations across the US and the UK are represented quite well while those in South and Southeast Asia are poorly represented. Analysis shows that both families of models largely share the same skew across populations. At the same time, this skew cannot be fully explained by sociolinguistic factors, economic factors, or geographic factors. The basic conclusion from this analysis is that pre-trained models do not equally represent the world's population: there is a strong skew towards specific geographic populations. This finding challenges the 
    
[^78]: DIALECTBENCH：一个方言、语言变体和密切相关语言的自然语言处理基准

    DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages

    [https://arxiv.org/abs/2403.11009](https://arxiv.org/abs/2403.11009)

    DIALECTBENCH是第一个面向自然语言处理中的方言、语言变体和密切相关语言的大规模基准测试，为实现对不同语言变体上NLP系统性能的全面评估提供了重要工具。

    

    arXiv:2403.11009v1 语种：交叉  摘要：语言技术应该根据其在实际用例中的有用性来判断。在自然语言处理（NLP）研究和评估中经常被忽视的一个方面是非标准方言或语言变体（以下简称为变体）形式的语言变体. 大多数NLP基准测试仅限于标准语言变体。为填补这一空白，我们提出了DIALECTBENCH，这是首个针对语言变体的大规模NLP基准测试，汇总了一系列多样任务样本的变体数据集（涵盖281种变体的10个文本级任务）。这允许对不同语言变体上NLP系统性能进行全面评估。我们提供了大量证据表明标准语言变体与非标准语言变体之间存在性能差距，并且我们还确定了在任务之间存在大量性能差距的语言类群。我们认为DIALECTBENCH提供了对当前语言NLP状态的全面视图。

    arXiv:2403.11009v1 Announce Type: cross  Abstract: Language technologies should be judged on their usefulness in real-world use cases. An often overlooked aspect in natural language processing (NLP) research and evaluation is language variation in the form of non-standard dialects or language varieties (hereafter, varieties). Most NLP benchmarks are limited to standard language varieties. To fill this gap, we propose DIALECTBENCH, the first-ever large-scale benchmark for NLP on varieties, which aggregates an extensive set of task-varied variety datasets (10 text-level tasks covering 281 varieties). This allows for a comprehensive evaluation of NLP system performance on different language varieties. We provide substantial evidence of performance disparities between standard and non-standard language varieties, and we also identify language clusters with large performance divergence across tasks. We believe DIALECTBENCH provides a comprehensive view of the current state of NLP for langua
    
[^79]: 具有无标签悬挂案例的实体对齐

    Entity Alignment with Unlabeled Dangling Cases

    [https://arxiv.org/abs/2403.10978](https://arxiv.org/abs/2403.10978)

    提出了一种基于GNN的框架，在实体对齐中解决了无标签悬挂案例的问题，通过设计注意机制和正样本-无标签损失来实现更好的对齐性能

    

    我们研究了具有无标签悬挂案例的实体对齐问题，这意味着源图或目标图中有一些实体在另一方中没有对应实体，并且这些实体保持未标记状态。该问题出现在源图和目标图的规模不同，并且标记可匹配实体的成本远低于悬挂实体的情况下。为了解决这个问题，我们提出了一种新颖的基于GNN的悬挂检测和实体对齐框架。虽然这两个任务共享相同的GNN，并且一起训练，但检测到的悬挂实体在对齐中被移除。我们的框架特点是具有用于选择性邻域聚合的设计实体和关系注意机制，以及用于对悬挂实体进行无偏估计的正样本-无标签学习损失。实验结果表明我们设计的每个组件都对整体对齐性能有贡献

    arXiv:2403.10978v1 Announce Type: new  Abstract: We investigate the entity alignment problem with unlabeled dangling cases, meaning that there are entities in the source or target graph having no counterparts in the other, and those entities remain unlabeled. The problem arises when the source and target graphs are of different scales, and it is much cheaper to label the matchable pairs than the dangling entities. To solve the issue, we propose a novel GNN-based dangling detection and entity alignment framework. While the two tasks share the same GNN and are trained together, the detected dangling entities are removed in the alignment. Our framework is featured by a designed entity and relation attention mechanism for selective neighborhood aggregation in representation learning, as well as a positive-unlabeled learning loss for an unbiased estimation of dangling entities. Experimental results have shown that each component of our design contributes to the overall alignment performance
    
[^80]: Pointer-Generator网络用于低资源机器翻译：不要复制那个！

    Pointer-Generator Networks for Low-Resource Machine Translation: Don't Copy That!

    [https://arxiv.org/abs/2403.10963](https://arxiv.org/abs/2403.10963)

    Pointer-Generator Networks在低资源机器翻译中未展现出预期的优势，模型在不同资源范围和语言之间的关系下表现一般。

    

    虽然基于Transformer的神经机器翻译（NMT）在高资源环境中非常有效，但许多语言缺乏必要的大规模平行语料库来受益。在两种密切相关语言之间的低资源（LR）机器翻译中，一种自然的直觉是寻求从结构“捷径”中获益，例如从源语言复制子词到目标语言，因为这样的语言对通常共享相当数量的相同单词、同源词和借词。我们测试了针对六种语言对的指针生成器网络在各种资源范围下的用途，并发现在大多数情况下都有轻微改进。然而，分析显示，模型对于密切相关的语言对与较远的语言对，或者资源范围较低与较高的语言对并没有展现出更大的改进，并且模型并未展示出对于共享子词机制的预期用法。我们讨论了这种行为的原因。

    arXiv:2403.10963v1 Announce Type: new  Abstract: While Transformer-based neural machine translation (NMT) is very effective in high-resource settings, many languages lack the necessary large parallel corpora to benefit from it. In the context of low-resource (LR) MT between two closely-related languages, a natural intuition is to seek benefits from structural "shortcuts", such as copying subwords from the source to the target, given that such language pairs often share a considerable number of identical words, cognates, and borrowings. We test Pointer-Generator Networks for this purpose for six language pairs over a variety of resource ranges, and find weak improvements for most settings. However, analysis shows that the model does not show greater improvements for closely-related vs. more distant language pairs, or for lower resource ranges, and that the models do not exhibit the expected usage of the mechanism for shared subwords. Our discussion of the reasons for this behaviour high
    
[^81]: 基于能量的模型及其在语音和语言处理中的应用

    Energy-Based Models with Applications to Speech and Language Processing

    [https://arxiv.org/abs/2403.10961](https://arxiv.org/abs/2403.10961)

    基于能量的模型（EBMs）是一类重要的概率模型，在语音和语言处理等领域吸引了越来越多的关注，因其显著的理论和算法进展。

    

    基于能量的模型（EBMs）是一类重要的概率模型，也称为随机场和无向图模型。EBMs是非规范化的，因此与其他流行的自归一化概率模型（如隐马尔可夫模型（HMMs）、自回归模型、生成对抗网络（GANs）和变分自编码器（VAEs））有根本的不同。近年来，由于重要的理论和算法进展，EBMs不仅吸引了核心机器学习社区的越来越多关注，还吸引了应用领域（如语音、视觉、自然语言处理（NLP）等）的兴趣。语音和语言的顺序性质也提供了特殊挑战，需要与处理固定维度数据（例如图像）有所不同的处理方法。因此，本专著旨在系统介绍基于能量的模型，包括算法进展。

    arXiv:2403.10961v1 Announce Type: cross  Abstract: Energy-Based Models (EBMs) are an important class of probabilistic models, also known as random fields and undirected graphical models. EBMs are un-normalized and thus radically different from other popular self-normalized probabilistic models such as hidden Markov models (HMMs), autoregressive models, generative adversarial nets (GANs) and variational auto-encoders (VAEs). Over the past years, EBMs have attracted increasing interest not only from the core machine learning community, but also from application domains such as speech, vision, natural language processing (NLP) and so on, due to significant theoretical and algorithmic progress. The sequential nature of speech and language also presents special challenges and needs a different treatment from processing fix-dimensional data (e.g., images). Therefore, the purpose of this monograph is to present a systematic introduction to energy-based models, including both algorithmic progr
    
[^82]: SelfIE：大型语言模型嵌入的自我解释

    SelfIE: Self-Interpretation of Large Language Model Embeddings

    [https://arxiv.org/abs/2403.10949](https://arxiv.org/abs/2403.10949)

    提出了SelfIE框架，使大型语言模型能够自解释其嵌入，揭示内部推理，包括道德决策、提示注入和消除有害知识。

    

    arXiv:2403.10949v1 公告类型：交叉摘要：大型语言模型（LLMs）如何获得答案？解释和控制LLM的推理过程对于可靠性、透明度和未来模型发展至关重要。我们提出了SelfIE（嵌入的自我解释），这是一个框架，能够利用LLMs响应关于给定段落的查询的能力，以自然语言解释它们自己的嵌入。SelfIE能够解释隐藏嵌入中的开放世界概念，在案例中揭示LLM的内部推理，如做出道德决策、内化提示注入和回想有害知识。SelfIE对隐藏嵌入的文本描述也开辟了控制LLM推理的新途径。我们提出了监督控制，它允许编辑开放式概念，而只需要计算单个层的梯度。我们将RLHF扩展到隐藏的嵌入，并提出了强化控制来消除有害知识。

    arXiv:2403.10949v1 Announce Type: cross  Abstract: How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge i
    
[^83]: MIntRec2.0：用于多模态意图识别和对话中场外检测的大规模基准数据集

    MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations

    [https://arxiv.org/abs/2403.10943](https://arxiv.org/abs/2403.10943)

    MIntRec2.0介绍了一个旨在解决多模态意图识别和对话中场外检测挑战的大规模基准数据集。该数据集包含30个细粒度类别的1,245个对话和15,040个样本，其中包括逼真的场外样本，并丰富了发言者信息以支持多方对话研究。

    

    多模态意图识别面临重大挑战，需要整合来自现实世界背景的非语言形式，以增强对人类意图的理解。现有的基准数据集在规模上受限，并且在处理多轮对话互动中出现的场外样本时存在困难。我们介绍了MIntRec2.0，这是一个用于多方对话中的多模态意图识别的大规模基准数据集。它包含1,245个对话，15,040个样本，每个样本在30个细粒度类别的新意图分类中进行了注释。除了9,304个场内样本外，还包括5,736个出现在多轮上下文中的场外样本，这在现实场景中自然发生。此外，我们还提供了每个话语中发言者的详细信息，丰富了它在多方对话研究中的实用性。

    arXiv:2403.10943v1 Announce Type: cross  Abstract: Multimodal intent recognition poses significant challenges, requiring the incorporation of non-verbal modalities from real-world contexts to enhance the comprehension of human intentions. Existing benchmark datasets are limited in scale and suffer from difficulties in handling out-of-scope samples that arise in multi-turn conversational interactions. We introduce MIntRec2.0, a large-scale benchmark dataset for multimodal intent recognition in multi-party conversations. It contains 1,245 dialogues with 15,040 samples, each annotated within a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope samples, it also includes 5,736 out-of-scope samples appearing in multi-turn contexts, which naturally occur in real-world scenarios. Furthermore, we provide comprehensive information on the speakers in each utterance, enriching its utility for multi-party conversational research. We establish a general framework supporting the o
    
[^84]: 通过最小化增强语言模型进行初步解码以改进低资源ASR中的晶格重新打分

    Initial Decoding with Minimally Augmented Language Model for Improved Lattice Rescoring in Low Resource ASR

    [https://arxiv.org/abs/2403.10937](https://arxiv.org/abs/2403.10937)

    通过最小化增强语言模型进行初步解码，以提高低资源ASR中晶格重新打分的语音识别准确性，相对减少了泰卢固语21.8%和卡纳达语41.8%的词误差率，同时仅消耗1/8内存。

    

    本文解决了在基线语言模型无法生成全面晶格的低资源语言中通过晶格重新打分改善语音识别准确性的问题。我们通过将基线语言模型最小程度地增强为目标语言更大文本语料库中出现但基线语言模型中不存在的词的词频来解决这一问题。通过使用这种增强的基线语言模型进行解码生成的晶格更加全面。我们的方法使词误差率相对减少了21.8%（泰卢固语）和41.8%（卡纳达语）。与使用完整维基百科文本增强语言模型解码相比，我们的方法在减少词误差率方面相当，而我们的方法只消耗1/8的内存。

    arXiv:2403.10937v1 Announce Type: cross  Abstract: This paper addresses the problem of improving speech recognition accuracy with lattice rescoring in low-resource languages where the baseline language model is insufficient for generating inclusive lattices. We minimally augment the baseline language model with word unigram counts that are present in a larger text corpus of the target language but absent in the baseline. The lattices generated after decoding with such an augmented baseline language model are more comprehensive. We obtain 21.8% (Telugu) and 41.8% (Kannada) relative word error reduction with our proposed method. This reduction in word error rate is comparable to 21.5% (Telugu) and 45.9% (Kannada) relative word error reduction obtained by decoding with full Wikipedia text augmented language mode while our approach consumes only 1/8th the memory. We demonstrate that our method is comparable with various text selection-based language model augmentation and also consistent f
    
[^85]: BEnQA: 孟加拉语和英语的问题回答和推理基准测试

    BEnQA: A Question Answering and Reasoning Benchmark for Bengali and English

    [https://arxiv.org/abs/2403.10900](https://arxiv.org/abs/2403.10900)

    BEnQA介绍了一项包含孟加拉语和英语考试问题的数据集，并发现大型语言模型在孟加拉语和英语问题中表现有明显差异，同时发现Chain-of-Thought提示对推理问题有益，附加英语翻译有助于解答孟加拉语问题。

    

    在这项研究中，我们介绍了BEnQA，这是一个包含孟加拉语和英语并行考试问题的数据集，主要涵盖孟加拉国中学和高中水平的题目。我们的数据集包含大约5,000道问题，涵盖了科学中的多个学科，包括事实性、应用性和基于推理的问题。我们使用我们的并行数据集对几个大型语言模型（LLMs）进行基准测试，并观察到模型在孟加拉语和英语中表现出明显的性能差异。我们还研究了一些提示方法，并发现对于基于推理的问题来说，“Chain-of-Thought”提示方法在某种程度上是有益的，但对于事实性问题则没有那么有帮助。我们还发现，在回答孟加拉语问题时，附加英语翻译有助于回答问题。我们的发现指出了未来改进LLMs在孟加拉语和更普遍地在资源匮乏语言中性能的有希望的研究方向。

    arXiv:2403.10900v1 Announce Type: new  Abstract: In this study, we introduce BEnQA, a dataset comprising parallel Bengali and English exam questions for middle and high school levels in Bangladesh. Our dataset consists of approximately 5K questions covering several subjects in science with different types of questions, including factual, application, and reasoning-based questions. We benchmark several Large Language Models (LLMs) with our parallel dataset and observe a notable performance disparity between the models in Bengali and English. We also investigate some prompting methods, and find that Chain-of-Thought prompting is beneficial mostly on reasoning questions, but not so much on factual ones. We also find that appending English translation helps to answer questions in Bengali. Our findings point to promising future research directions for improving the performance of LLMs in Bengali and more generally in low-resource languages.
    
[^86]: 朝向稳健性和多样性：使用文本混合和批量核范数最大化进行对话生成的继续学习

    Towards Robustness and Diversity: Continual Learning in Dialog Generation with Text-Mixup and Batch Nuclear-Norm Maximization

    [https://arxiv.org/abs/2403.10894](https://arxiv.org/abs/2403.10894)

    本文探讨了在对话生成领域中的继续学习，提出了利用文本混合和批量核范数最大化的方法来解决灾难性遗忘和模式崩溃问题，实验证明这种方法在继续学习方面优于现有技术。

    

    在我们持续不断接收数据的动态世界中，继续学习使我们能够逐步添加新的任务/领域，而无需从头开始重新训练。在语言模型的继续学习中，一个主要挑战是灾难性遗忘，即模型在训练新任务/领域时忘记之前训练任务/领域中的知识的倾向。本文研究了在继续学习设置下的对话生成。我们提出了一种新颖方法，1）使用Text-Mixup作为数据增强，以避免模型在重放记忆上过拟合，2）利用批量核范数最大化（BNNM）来缓解模式崩溃问题。对一个包含37个领域的任务驱动对话数据集和DailyDialog（一个包含10个领域的闲聊数据集）的实验表明，我们提出的方法在继续学习方面胜过了现有技术。

    arXiv:2403.10894v1 Announce Type: new  Abstract: In our dynamic world where data arrives in a continuous stream, continual learning enables us to incrementally add new tasks/domains without the need to retrain from scratch. A major challenge in continual learning of language model is catastrophic forgetting, the tendency of models to forget knowledge from previously trained tasks/domains when training on new ones. This paper studies dialog generation under the continual learning setting. We propose a novel method that 1) uses \textit{Text-Mixup} as data augmentation to avoid model overfitting on replay memory and 2) leverages Batch-Nuclear Norm Maximization (BNNM) to alleviate the problem of mode collapse. Experiments on a $37$-domain task-oriented dialog dataset and DailyDialog (a $10$-domain chitchat dataset) demonstrate that our proposed approach outperforms the state-of-the-art in continual learning.
    
[^87]: 优化多语言大语言模型的语言增强：以韩语为例的案例研究

    Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean

    [https://arxiv.org/abs/2403.10882](https://arxiv.org/abs/2403.10882)

    该研究提出了三种策略来增强基于公开可用MLLMs的资源较少的语言的性能，包括扩展词汇、双语数据预训练和指导微调。

    

    大型语言模型（LLMs）使用预训练来预测下一个单词；然而，它们的扩展需要大量计算资源。许多大型科技公司和研究机构已经开发了多语言LLMs（MLLMs）以满足当前需求，但忽视了资源较少的语言（LRLs）。本研究提出了三种策略来增强基于公开可用MLLMs的LRLs的性能。首先，扩展LRLs的MLLM词汇以增强表达性。其次，使用双语数据进行预训练以对齐高资源语言和低资源语言。第三，构建高质量的小规模指导数据集，并进行指导微调以增强LRL。实验采用了Llama2模型，以韩语作为LRL，并在八项任务中对其与其他已开发的LLMs进行了定量评估。此外，基于人类评估进行了定性评估。

    arXiv:2403.10882v1 Announce Type: cross  Abstract: Large language models (LLMs) use pretraining to predict the subsequent word; however, their expansion requires significant computing resources. Numerous big tech companies and research institutes have developed multilingual LLMs (MLLMs) to meet current demands, overlooking less-resourced languages (LRLs). This study proposed three strategies to enhance the performance of LRLs based on the publicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to enhance expressiveness. Second, bilingual data were used for pretraining to align the high- and less-resourced languages. Third, a high-quality small-scale instruction dataset was constructed and instruction-tuning was performed to augment the LRL. The experiments employed the Llama2 model and Korean was used as the LRL, which was quantitatively evaluated against other developed LLMs across eight tasks. Furthermore, a qualitative assessment was performed based on human eva
    
[^88]: 无监督生成语言隐写术

    Zero-shot Generative Linguistic Steganography

    [https://arxiv.org/abs/2403.10856](https://arxiv.org/abs/2403.10856)

    本文提出了一种基于上下文学习的新型零样本方法，利用生成性语言隐写术实现更好的感知和统计隐蔽性。

    

    生成性语言隐写术试图将秘密信息隐藏在覆盖文本中。先前的研究通常侧重于覆盖文本和隐写文本之间的统计差异，然而，不规范的隐写文本很容易被人类辨别出来。在本文中，我们提出了一种基于上下文学习的新型零样本方法，用于实现更好的感知和统计隐蔽性。我们还设计了几个新的度量标准和可重现的语言评估，以衡量隐写文本的隐蔽性。我们的实验结果表明，我们的方法产生的无害和可理解的隐写文本比任何其他方法多1.926倍。

    arXiv:2403.10856v1 Announce Type: new  Abstract: Generative linguistic steganography attempts to hide secret messages into covertext. Previous studies have generally focused on the statistical differences between the covertext and stegotext, however, ill-formed stegotext can readily be identified by humans. In this paper, we propose a novel zero-shot approach based on in-context learning for linguistic steganography to achieve better perceptual and statistical imperceptibility. We also design several new metrics and reproducible language evaluations to measure the imperceptibility of the stegotext. Our experimental results indicate that our method produces $1.926\times$ more innocent and intelligible stegotext than any other method.
    
[^89]: RETINAQA：一种对可回答和不可回答问题都具有鲁棒性的知识库问答模型

    RETINAQA : A Knowledge Base Question Answering Model Robust to both Answerable and Unanswerable Questions

    [https://arxiv.org/abs/2403.10849](https://arxiv.org/abs/2403.10849)

    提出了一种名为RetinaQA的新型KBQA模型，通过基于填图的逻辑形式构建和使用辨别方法，实现了对不可回答性的鲁棒性，并在可回答和不可回答问题上显着优于最先进的KBQA模型。

    

    最先进的知识库问答（KBQA）模型通常假定问题是可回答的。然而，最近的研究表明，虽然这些模型可以通过适当的训练和阈值设定适应检测不可回答性，但这将以牺牲可回答问题的准确性为代价，且没有单一模型能够处理所有不可回答性类别。我们提出了一种名为RetinaQA的新型KBQA模型，它对不可回答性具有鲁棒性。它将基于KB遍历的逻辑形式检索与基于填图的逻辑形式构建相结合。这有助于处理具有有效逻辑形式但在知识库中没有通向答案的数据路径的问题。此外，它使用辨别而非生成来更好地识别没有有效逻辑形式的问题。我们展示了RetinaQA在可回答和不可回答问题上显着优于最先进的KBQA模型的调整，并且在不可回答性类别上显示出鲁棒性。

    arXiv:2403.10849v1 Announce Type: new  Abstract: State-of-the-art KBQA models assume answerability of questions. Recent research has shown that while these can be adapted to detect unaswerability with suitable training and thresholding, this comes at the expense of accuracy for answerable questions, and no single model is able to handle all categories of unanswerability. We propose a new model for KBQA named RetinaQA that is robust against unaswerability. It complements KB-traversal based logical form retrieval with sketch-filling based logical form construction. This helps with questions that have valid logical forms but no data paths in the KB leading to an answer. Additionally, it uses discrimination instead of generation to better identify questions that do not have valid logical forms. We demonstrate that RetinaQA significantly outperforms adaptations of state-of-the-art KBQA models across answerable and unanswerable questions, while showing robustness across unanswerability categ
    
[^90]: 使用多层表示学习的两步自动化犯罪代码词检测

    Two-step Automated Cybercrime Coded Word Detection using Multi-level Representation Learning

    [https://arxiv.org/abs/2403.10838](https://arxiv.org/abs/2403.10838)

    提出了一种新的两步方法，通过构建平均潜在向量并基于多层潜在向量检测网络犯罪编码词，解决了自动化犯罪代码词检测问题中的数据获取困难和利用语言模型理解自然语言困难的挑战。

    

    在社交网络服务平台中，犯罪嫌疑人可能会使用网络犯罪编码词进行通信，通过在现有单词中添加犯罪含义或用类似单词替换来实现。例如，单词"ice"经常被用来指代毒品犯罪中的甲基苯丙胺。针对自动化犯罪代码词检测问题，收集足够量的监督学习训练数据和直接应用利用上下文信息更好理解自然语言的语言模型都是困难的。为了克服这些限制，我们提出了一种新的两步方法，第一步通过五种不同的AutoEncoder模型之一为每个网络犯罪构建平均潜在向量，第二步基于多层潜在向量检测网络犯罪编码词。

    arXiv:2403.10838v1 Announce Type: new  Abstract: In social network service platforms, crime suspects are likely to use cybercrime coded words for communication by adding criminal meanings to existing words or replacing them with similar words. For instance, the word 'ice' is often used to mean methamphetamine in drug crimes. To analyze the nature of cybercrime and the behavior of criminals, quickly detecting such words and further understanding their meaning are critical. In the automated cybercrime coded word detection problem, it is difficult to collect a sufficient amount of training data for supervised learning and to directly apply language models that utilize context information to better understand natural language. To overcome these limitations, we propose a new two-step approach, in which a mean latent vector is constructed for each cybercrime through one of five different AutoEncoder models in the first step, and cybercrime coded words are detected based on multi-level latent
    
[^91]: 辨析仇恨：识别仇恨表情包及其目标

    Deciphering Hate: Identifying Hateful Memes and Their Targets

    [https://arxiv.org/abs/2403.10829](https://arxiv.org/abs/2403.10829)

    介绍了一个为孟加拉语设计的新颖多模态数据集BHM，用于检测仇恨表情包以及它们所针对的社会实体。

    

    互联网表情包已经成为个人在社交媒体上表达情感、思想和观点的强大手段。虽然通常被视为一种幽默和娱乐来源，但表情包也可以传播针对个人或社区的仇恨内容。大多数现有研究侧重于高资源语言中表情包的负面方面，忽视了低资源语言（如孟加拉语）所面临的独特挑战。此外，尽管之前关于孟加拉语表情包的工作集中在检测仇恨表情包上，但并没有对它们的目标实体进行检测。为了弥补这一差距并促进这一领域的研究，我们引入了一个面向孟加拉语的新颖多模态数据集BHM（孟加拉仇恨表情包）。该数据集包含7,148个带有孟加拉语以及混合代码字幕的表情包，专为两项任务量身定制：（i）检测仇恨表情包，以及（ii）检测它们所针对的社会实体。

    arXiv:2403.10829v1 Announce Type: new  Abstract: Internet memes have become a powerful means for individuals to express emotions, thoughts, and perspectives on social media. While often considered as a source of humor and entertainment, memes can also disseminate hateful content targeting individuals or communities. Most existing research focuses on the negative aspects of memes in high-resource languages, overlooking the distinctive challenges associated with low-resource languages like Bengali (also known as Bangla). Furthermore, while previous work on Bengali memes has focused on detecting hateful memes, there has been no work on detecting their targeted entities. To bridge this gap and facilitate research in this arena, we introduce a novel multimodal dataset for Bengali, BHM (Bengali Hateful Memes). The dataset consists of 7,148 memes with Bengali as well as code-mixed captions, tailored for two tasks: (i) detecting hateful memes, and (ii) detecting the social entities they target
    
[^92]: 具有关系解缠的多方回复生成

    Multi-party Response Generation with Relation Disentanglement

    [https://arxiv.org/abs/2403.10827](https://arxiv.org/abs/2403.10827)

    本研究提出了一种利用关系解缠来指导神经响应生成的方法，通过在会话上下文内部微妙线索上进行关系推断，实现了对多方回复的自动推断，无需人工标签。

    

    现有的神经响应生成模型已经在双方对话方面取得了令人瞩目的进展，假设话语是按顺序组织的。然而，许多现实世界中的对话涉及多方参与者，对话上下文的结构要复杂得多，例如，来自不同参与者的话语可能“并行”发生。面对这一挑战，有研究致力于建模话语或参与者之间的关系，以便以更清晰的上下文进行响应生成。然而，这些方法严重依赖这些关系，都假设这些关系是预先给定的，这在实践中是不切实际的，也妨碍了这些方法的普适性。在这项工作中，我们建议通过对话上下文中微妙线索进行关系推断，引导神经响应生成，而无需任何人工标签。具体而言，我们首先通过关系思维自动推断出这些关系，然后利用这些关系来指导神经响应生成。

    arXiv:2403.10827v1 Announce Type: new  Abstract: Existing neural response generation models have achieved impressive improvements for two-party conversations, which assume that utterances are sequentially organized. However, many real-world dialogues involve multiple interlocutors and the structure of conversational context is much more complex, e.g. utterances from different interlocutors can occur "in parallel". Facing this challenge, there are works trying to model the relations among utterances or interlocutors to facilitate response generation with clearer context. Nonetheless, these methods rely heavily on such relations and all assume that these are given beforehand, which is impractical and hinders the generality of such methods. In this work, we propose to automatically infer the relations via relational thinking on subtle clues inside the conversation context without any human label, and leverage these relations to guide the neural response generation. Specifically, we first 
    
[^93]: 大型语言模型是否理解医学编码?

    Do Large Language Models understand Medical Codes?

    [https://arxiv.org/abs/2403.10822](https://arxiv.org/abs/2403.10822)

    该研究调查了大型语言模型是否理解医学编码的含义，评估了它们对领域特定术语的认识和理解。

    

    近期人工智能研究的首要目标是稳步朝着实现人工通用智能(AGI)迈进，这促使对大型语言模型(LLMs)在各种任务和领域中的评估。其中之一是医疗保健领域，LLMs可以通过协助各种任务大大有益于临床实践。然而，当面对无法充分应对的查询时，这些模型也容易产生“幻觉”或不正确的响应，引发了关注和怀疑，特别是在医疗保健社区内。因此，在这项工作中，我们调查LLMs是否理解医学编码的固有含义，这些编码被广泛应用于医疗保健实践。我们评估了各种现成的LLMs (例如GPT、LLaMA等)和专门为生物医学应用设计的LLMs，以评估它们对这些领域特定术语的认识和理解。我们的结果表明…

    arXiv:2403.10822v1 Announce Type: new  Abstract: The overarching goal of recent AI research has been to make steady progress towards achieving Artificial General Intelligence (AGI), prompting the evaluation of Large Language Models (LLMs) across a variety of tasks and domains. One such domain is healthcare, where LLMs can greatly benefit clinical practice by assisting with a wide range of tasks. However, these models are also prone to producing "hallucinations" or incorrect responses when faced with queries they cannot adequately address, raising concerns and skepticism, especially within the healthcare community. Therefore, in this work, we investigate whether LLMs understand the inherent meaning of medical codes, which are widely used in healthcare practice. We evaluate various off-the-shelf LLMs (e.g., GPT, LLaMA, etc.) and LLMs specifically designed for biomedical applications to assess their awareness and understanding of these domain-specific terminologies. Our results indicate t
    
[^94]: 使用自适应估计融合高效剪枝大型语言模型

    Efficient Pruning of Large Language Model with Adaptive Estimation Fusion

    [https://arxiv.org/abs/2403.10799](https://arxiv.org/abs/2403.10799)

    提出了一种简单而高效的剪枝方法，能够自适应地模拟每个子结构的重要性，并根据多层结构的结果自适应地融合粗粒度和细粒度的估计。

    

    大型语言模型（LLMs）已经成为许多生成性下游任务中至关重要的组成部分，这导致在资源受限设备上高效部署它们成为不可避免的趋势和重大挑战。结构化剪枝是解决这一挑战的广泛应用方法。然而，当处理多个解码器层的复杂结构时，通常的方法往往采用常见的估计方法进行剪枝。这些方法导致特定下游任务精度下降。本文介绍了一种简单而有效的方法，可自适应地模拟每个子结构的重要性。同时，它可以基于复杂和多层结构的结果，自适应地融合粗粒度和细粒度的估计。我们设计的所有方面都无缝集成到端到端的剪枝框架中。与主流数据集上的最先进方法相比，我们的实验结果表明

    arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
    
[^95]: 从单词到路径：将大型语言模型应用于车辆路径规划

    From Words to Routes: Applying Large Language Models to Vehicle Routing

    [https://arxiv.org/abs/2403.10795](https://arxiv.org/abs/2403.10795)

    该研究探索了应用大型语言模型解决车辆路径规划问题的能力，提出了基于自然语言任务描述的基本提示范例并提出一种使模型进行改进的框架。

    

    LLMs在机器人领域（例如操作和导航）中展示出令人印象深刻的进展，其利用自然语言任务描述。LLMs在这些任务中取得成功让我们思考：LLMs在使用自然语言任务描述解决车辆路径规划问题（VRPs）的能力如何？在这项工作中，我们分三步研究这个问题。首先，我们构建了一个包含21种单车或多车路径问题的数据集。其次，我们评估了LLMs在四种基本提示范例文本到代码生成的性能，每种包括不同类型的文本输入。我们发现，直接从自然语言任务描述生成代码的基本提示范例对于GPT-4效果最佳，实现了56%的可行性，40%的优化性和53%的效率。第三，基于观察到LLMs可能无法在初始尝试中提供正确解决方案的现象，我们提出了一个框架，使LLMs能够进行改进。

    arXiv:2403.10795v1 Announce Type: cross  Abstract: LLMs have shown impressive progress in robotics (e.g., manipulation and navigation) with natural language task descriptions. The success of LLMs in these tasks leads us to wonder: What is the ability of LLMs to solve vehicle routing problems (VRPs) with natural language task descriptions? In this work, we study this question in three steps. First, we construct a dataset with 21 types of single- or multi-vehicle routing problems. Second, we evaluate the performance of LLMs across four basic prompt paradigms of text-to-code generation, each involving different types of text input. We find that the basic prompt paradigm, which generates code directly from natural language task descriptions, performs the best for GPT-4, achieving 56% feasibility, 40% optimality, and 53% efficiency. Third, based on the observation that LLMs may not be able to provide correct solutions at the initial attempt, we propose a framework that enables LLMs to refin
    
[^96]: 探索中国幽默生成：关于两句典故谚语的研究

    Exploring Chinese Humor Generation: A Study on Two-Part Allegorical Sayings

    [https://arxiv.org/abs/2403.10781](https://arxiv.org/abs/2403.10781)

    本文研究了如何利用最先进的语言模型来理解和生成中国幽默，特别是关于训练模型生成典故谚语。他们采用新颖的fine-tuning方法，包含融合拼音嵌入和对比学习，从而成功生成幽默性典故谚语。

    

    幽默是人类语言中富有文化内涵的一个方面，对于计算机理解和生成而言具有挑战性，尤其是在相对未被自然语言处理社区探索的中国幽默领域。本文研究了最先进语言模型在理解和生成中国幽默方面的能力，特别聚焦于训练它们创造典故谚语。我们采用了两种显著的训练方法：调整一个中等规模的语言模型和提示一个大型模型。我们的新颖调整方法融合了拼音嵌入以考虑同音异义词，并采用对比学习与合成困难性负例以区分幽默元素。人类注释的结果显示这些模型能够生成幽默的典故谚语，提示法证明是一个实用且有效的方法。然而，在生成与人类创造力匹配的典故谚语方面仍有改进空间。

    arXiv:2403.10781v1 Announce Type: cross  Abstract: Humor, a culturally nuanced aspect of human language, poses challenges for computational understanding and generation, especially in Chinese humor, which remains relatively unexplored in the NLP community. This paper investigates the capability of state-of-the-art language models to comprehend and generate Chinese humor, specifically focusing on training them to create allegorical sayings. We employ two prominent training methods: fine-tuning a medium-sized language model and prompting a large one. Our novel fine-tuning approach incorporates fused Pinyin embeddings to consider homophones and employs contrastive learning with synthetic hard negatives to distinguish humor elements. Human-annotated results show that these models can generate humorous allegorical sayings, with prompting proving to be a practical and effective method. However, there is still room for improvement in generating allegorical sayings that match human creativity.
    
[^97]: LLM-based Conversational AI Therapist for Daily Functioning Screening and Psychotherapeutic Intervention via Everyday Smart Devices

    LLM-based Conversational AI Therapist for Daily Functioning Screening and Psychotherapeutic Intervention via Everyday Smart Devices

    [https://arxiv.org/abs/2403.10779](https://arxiv.org/abs/2403.10779)

    提出了一种基于LLM和智能设备的对话式AI治疗师，CaiTI平台，可通过自然和心理治疗对话来筛查日常功能，并提供个性化对话流程和心理治疗干预。

    

    尽管全球存在心理健康危机，但接受筛查、专业人士和治疗的机会仍很高。与持牌心理治疗师合作，我们提出了一种利用大型语言模型（LLM）和智能设备实现更好心理健康自我护理的对话AI治疗师，即CaiTI平台。CaiTI能够使用自然和心理治疗对话来筛查日常功能。CaiTI利用强化学习提供个性化对话流程。CaiTI能够准确理解和解释用户的回应。当用户在对话中需要进一步关注时，CaiTI可以提供对话式心理治疗干预，包括认知行为治疗（CBT）和激励性访谈（MI）。通过利用持牌心理治疗师准备的数据集，我们对各种LLM在Cai沿任务中的性能进行实验和微基准测试。

    arXiv:2403.10779v1 Announce Type: new  Abstract: Despite the global mental health crisis, access to screenings, professionals, and treatments remains high. In collaboration with licensed psychotherapists, we propose a Conversational AI Therapist with psychotherapeutic Interventions (CaiTI), a platform that leverages large language models (LLM)s and smart devices to enable better mental health self-care. CaiTI can screen the day-to-day functioning using natural and psychotherapeutic conversations. CaiTI leverages reinforcement learning to provide personalized conversation flow. CaiTI can accurately understand and interpret user responses. When the user needs further attention during the conversation, CaiTI can provide conversational psychotherapeutic interventions, including cognitive behavioral therapy (CBT) and motivational interviewing (MI). Leveraging the datasets prepared by the licensed psychotherapists, we experiment and microbenchmark various LLMs' performance in tasks along Cai
    
[^98]: 检测大型语言模型中的偏见：Fine-tuned KcBERT

    Detecting Bias in Large Language Models: Fine-tuned KcBERT

    [https://arxiv.org/abs/2403.10774](https://arxiv.org/abs/2403.10774)

    在本研究中，作者通过使用KcBERT和KOLD数据对韩语评论进行微调，通过遮蔽语言建模来评估大型语言模型中的社会偏见，发现微调后的模型降低了种族偏见，但在性别和种族方面表现出显著变化。

    

    大型语言模型（LLMs）的快速发展实现了类似于人类的自然语言处理能力，LLMs被广泛应用于教育和医疗等各个社会领域。然而，这些模型的多功能性增加了，但它们有可能产生主观和规范性语言，导致在社会群体中出现歧视性对待或结果，特别是由于在线侮辱性语言。本文将此类伤害定义为社会偏见，并评估了使用KcBERT和KOLD数据通过基于模板的遮蔽语言建模（MLM）对韩语评论进行微调的模型中的种族、性别和种族偏见。为了定量评估偏见，我们采用了LPBS和CBS指标。与KcBERT相比，微调模型减少了种族偏见，但在性别和种族方面显示出显著变化。

    arXiv:2403.10774v1 Announce Type: new  Abstract: The rapid advancement of large language models (LLMs) has enabled natural language processing capabilities similar to those of humans, and LLMs are being widely utilized across various societal domains such as education and healthcare. While the versatility of these models has increased, they have the potential to generate subjective and normative language, leading to discriminatory treatment or outcomes among social groups, especially due to online offensive language. In this paper, we define such harm as societal bias and assess ethnic, gender, and racial biases in a model fine-tuned with Korean comments using Bidirectional Encoder Representations from Transformers (KcBERT) and KOLD data through template-based Masked Language Modeling (MLM). To quantitatively evaluate biases, we employ LPBS and CBS metrics. Compared to KcBERT, the fine-tuned model shows a reduction in ethnic bias but demonstrates significant changes in gender and racia
    
[^99]: 韩国会话中情绪因果识别的图卷积网络研究

    ECRC: Emotion-Causality Recognition in Korean Conversation for GCN

    [https://arxiv.org/abs/2403.10764](https://arxiv.org/abs/2403.10764)

    本研究提出了ECRC模型，通过结合单词级和句子级嵌入，以及基于新颖图结构的方法，在韩国会话环境中进行情绪因果识别研究。

    

    在这项多任务学习研究中，我们同时分析了会话环境中的情感及其潜在原因，采用深度神经网络方法有效处理和训练大规模的标记数据集。我们克服了以往嵌入方法的局限性，利用了单词级和句子级嵌入。此外，我们提出了基于新颖图结构的情绪因果识别模型（ECRC），从而借鉴了两种嵌入方法的优势。该模型独特地集成了双向长短期记忆（Bi-LSTM）和图神经网络。

    arXiv:2403.10764v1 Announce Type: cross  Abstract: In this multi-task learning study on simultaneous analysis of emotions and their underlying causes in conversational contexts, deep neural network methods were employed to effectively process and train large labeled datasets. However, these approaches are typically limited to conducting context analyses across the entire corpus because they rely on one of the two methods: word- or sentence-level embedding. The former struggles with polysemy and homonyms, whereas the latter causes information loss when processing long sentences. In this study, we overcome the limitations of previous embeddings by utilizing both word- and sentence-level embeddings. Furthermore, we propose the emotion-causality recognition in conversation (ECRC) model, which is based on a novel graph structure, thereby leveraging the strengths of both embedding methods. This model uniquely integrates the bidirectional long short-term memory (Bi-LSTM) and graph neural netw
    
[^100]: 规则仍然适用于开放信息抽取

    Rules still work for Open Information Extraction

    [https://arxiv.org/abs/2403.10758](https://arxiv.org/abs/2403.10758)

    该论文提出了一种针对中文文本的创新型OIE模型APRCOIE，通过自动生成抽取模式、定义新的模式形式以及设计张量计算过滤器等方法，实现了对多样复杂中文语法现象的处理，并在评估中表现优越，拓展了OIE性能边界。

    

    开放信息抽取（OIE）旨在从自然语言文本中提取表面关系及其对应的参数，而不考虑领域。本文提出了一种针对中文文本的创新型OIE模型APRCOIE。与先前的模型不同，我们的模型可以自主生成抽取模式。该模型为中文OIE定义了一种新的模式形式，并提出了自动化的模式生成方法。通过这种方式，模型可以处理多样复杂的中文语法现象。我们基于张量计算设计了一个初步过滤器，以便高效地进行抽取过程。为了训练模型，我们手动标注了一个大规模的中文OIE数据集。在比较评估中，我们展示了APRCOIE优于最先进的中文OIE模型，并显著拓展了可实现的OIE性能边界。APRCOIE的代码和标注数据集已发布。

    arXiv:2403.10758v1 Announce Type: new  Abstract: Open information extraction (OIE) aims to extract surface relations and their corresponding arguments from natural language text, irrespective of domain. This paper presents an innovative OIE model, APRCOIE, tailored for Chinese text. Diverging from previous models, our model generates extraction patterns autonomously. The model defines a new pattern form for Chinese OIE and proposes an automated pattern generation methodology. In that way, the model can handle a wide array of complex and diverse Chinese grammatical phenomena. We design a preliminary filter based on tensor computing to conduct the extraction procedure efficiently. To train the model, we manually annotated a large-scale Chinese OIE dataset. In the comparative evaluation, we demonstrate that APRCOIE outperforms state-of-the-art Chinese OIE models and significantly expands the boundaries of achievable OIE performance. The code of APRCOIE and the annotated dataset are releas
    
[^101]: 社交媒体上利用大语言模型进行抑郁症检测

    Depression Detection on Social Media with Large Language Models

    [https://arxiv.org/abs/2403.10750](https://arxiv.org/abs/2403.10750)

    提出了名为DORIS的新型抑郁症检测系统，将医学知识和大语言模型的最新进展相结合，通过分析个人在社交媒体上的帖子历史记录来确定抑郁症患者，以提高早期检测和干预。

    

    抑郁症造成危害。然而，由于缺乏心理健康意识和对病症耻辱感的恐惧，许多患者并未积极寻求诊断和治疗，导致不利后果。抑郁症检测旨在通过分析社交媒体上个人帖子的历史记录来确定个体是否患有抑郁症，这可显著有助于早期检测和干预。它主要面临两个关键挑战：1）需要专业医学知识，2）需要高准确性和可解释性。为了解决这一问题，我们提出了一个名为DORIS的新型抑郁症检测系统，结合了医学知识和大语言模型的最新进展。具体来说，为了解决第一个挑战，我们提出了一种基于大语言模型的解决方案，首先对高危文本进行标注以确定是否符合医学诊断标准。

    arXiv:2403.10750v1 Announce Type: cross  Abstract: Depression harms. However, due to a lack of mental health awareness and fear of stigma, many patients do not actively seek diagnosis and treatment, leading to detrimental outcomes. Depression detection aims to determine whether an individual suffers from depression by analyzing their history of posts on social media, which can significantly aid in early detection and intervention. It mainly faces two key challenges: 1) it requires professional medical knowledge, and 2) it necessitates both high accuracy and explainability. To address it, we propose a novel depression detection system called DORIS, combining medical knowledge and the recent advances in large language models (LLMs). Specifically, to tackle the first challenge, we proposed an LLM-based solution to first annotate whether high-risk texts meet medical diagnostic criteria. Further, we retrieve texts with high emotional intensity and summarize critical information from the his
    
[^102]: 利用LLMs集成揭示社交媒体消息的潜在主题：气候运动案例研究

    Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns

    [https://arxiv.org/abs/2403.10707](https://arxiv.org/abs/2403.10707)

    本文提出了一种通过利用大型语言模型（LLMs）的先进功能，以机器在循环中方法，处理社交媒体消息主题的新方法。

    

    本文介绍了一种揭示和分析社交媒体消息主题的新方法。鉴于传统主题级分析的局限性，往往只捕捉到整体模式，本研究强调了对更精细、主题聚焦的探索的需求。传统的主题发现方法，涉及手动流程和人在循环中的方法，具有价值，但在伸缩性、一致性和资源强度方面面临挑战，涉及时间和成本。为了应对这些挑战，我们提出了一种利用大型语言模型（LLMs）先进功能的机器在循环中方法。这种方法允许更深入地调查社交媒体话语的主题方面，使我们能够揭示多样的主题，每个主题具有独特的特征和相关性，从而提供对更广泛主题内有的微妙细节的全面理解。

    arXiv:2403.10707v1 Announce Type: cross  Abstract: This paper introduces a novel approach to uncovering and analyzing themes in social media messaging. Recognizing the limitations of traditional topic-level analysis, which tends to capture only the overarching patterns, this study emphasizes the need for a finer-grained, theme-focused exploration. Conventional methods of theme discovery, involving manual processes and a human-in-the-loop approach, are valuable but face challenges in scalability, consistency, and resource intensity in terms of time and cost. To address these challenges, we propose a machine-in-the-loop approach that leverages the advanced capabilities of Large Language Models (LLMs). This approach allows for a deeper investigation into the thematic aspects of social media discourse, enabling us to uncover a diverse array of themes, each with unique characteristics and relevance, thereby offering a comprehensive understanding of the nuances present within broader topics.
    
[^103]: PERL: 从人类反馈中实现参数高效强化学习

    PERL: Parameter Efficient Reinforcement Learning from Human Feedback

    [https://arxiv.org/abs/2403.10704](https://arxiv.org/abs/2403.10704)

    使用低秩适应（LoRA）方法进行参数高效强化学习（PERL），能够在与传统RLHF设置相当的性能下，实现更快的训练和更少的内存占用。

    

    强化学习从人类反馈（RLHF）已被证明是一种将预训练的大型语言模型（LLMs）与人类偏好对齐的有效方法。然而，使用RLHF训练模型计算成本高昂，且整个过程复杂。在本研究中，我们研究了RLHF，其中基础模型使用胡等人提出的低秩适应（LoRA）的参数高效方法进行训练。我们探讨了“参数高效强化学习”（PERL）的设置，在其中我们使用LoRA进行奖励模型训练和强化学习。我们将PERL与传统的微调（全调）在包括2个新数据集在内的7个基准测试中的奖励建模和强化学习方面的各种配置进行了比较。我们发现，PERL的性能与传统的RLHF设置相当，同时训练速度更快，内存占用更少。这使得RLHF具有很高的性能，同时减少了计算成本。

    arXiv:2403.10704v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong method to align Pretrained Large Language Models (LLMs) with human preferences. But training models with RLHF is computationally expensive, and an overall complex process. In this work, we study RLHF where the underlying models are trained using the parameter efficient method of Low-Rank Adaptation (LoRA) introduced by Hu et al. [2021]. We investigate the setup of "Parameter Efficient Reinforcement Learning" (PERL), in which we perform reward model training and reinforcement learning using LoRA. We compare PERL to conventional fine-tuning (full-tuning) across various configurations for 7 benchmarks, including 2 novel datasets, of reward modeling and reinforcement learning. We find that PERL performs on par with the conventional RLHF setting, while training faster, and with less memory. This enables the high performance of RLHF, while reducing the computational 
    
[^104]: 注意错误！检测和定位视觉与语言导航中的指令错误

    Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation

    [https://arxiv.org/abs/2403.10700](https://arxiv.org/abs/2403.10700)

    提出了一个新的基准数据集，首次引入了各种类型的指令错误，考虑到潜在的人类原因，以评估连续环境中 VLN 系统的健壮性

    

    Vision-and-Language Navigation in Continuous Environments (VLN-CE) 是一项直观且具有挑战性的体验智能任务。代理人被要求通过执行一系列低级动作、遵循一系列自然语言指令来导航到目标目标。所有文献中的 VLN-CE 方法都假设语言指令是准确的。然而，在实践中，人类给出的指令可能由于不准确的记忆或混淆而包含空间环境描述中的错误。当前 VLN-CE 基准没有解决这种情况，使得 VLN-CE 中的最新方法在面对来自人类用户的错误指令时变得脆弱。我们首次提出了一个引入各种类型指令错误考虑潜在人类原因的新型基准数据集。该基准数据集为连续环境中的 VLN 系统的健壮性提供了宝贵的见解。我们观察到 noticeable...

    arXiv:2403.10700v1 Announce Type: cross  Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of the most intuitive yet challenging embodied AI tasks. Agents are tasked to navigate towards a target goal by executing a set of low-level actions, following a series of natural language instructions. All VLN-CE methods in the literature assume that language instructions are exact. However, in practice, instructions given by humans can contain errors when describing a spatial environment due to inaccurate memory or confusion. Current VLN-CE benchmarks do not address this scenario, making the state-of-the-art methods in VLN-CE fragile in the presence of erroneous instructions from human users. For the first time, we propose a novel benchmark dataset that introduces various types of instruction errors considering potential human causes. This benchmark provides valuable insight into the robustness of VLN systems in continuous environments. We observe a noticeable 
    
[^105]: 对探测性别偏见的多语言视角

    A Multilingual Perspective on Probing Gender Bias

    [https://arxiv.org/abs/2403.10699](https://arxiv.org/abs/2403.10699)

    该论文通过多语言视角探索性别偏见的表达方式，强调了了解社会偏见的重要性。

    

    性别偏见代表着一种针对个体基于其性别的系统性负面对待。这种歧视可以从微妙的性别主义言论和性别刻板印象，到直接的仇恨言论。先前的研究已经揭示，忽视在线虐待不仅会影响受害个体，还会对更广泛的社会产生影响。这些后果延伸至阻挠妇女参与和在公共领域中的可见度，从而强化性别不平等。本论文探讨了性别偏见如何通过语言及语言技术表达的细微差别。特别是，本论文将研究扩展到多语言背景下的性别偏见，强调了从多语言和多文化视角理解社会偏见的重要性。在本论文中，我采取了跨领域的方法，将自然语言处理与政治学等其他学科联系起来。

    arXiv:2403.10699v1 Announce Type: new  Abstract: Gender bias represents a form of systematic negative treatment that targets individuals based on their gender. This discrimination can range from subtle sexist remarks and gendered stereotypes to outright hate speech. Prior research has revealed that ignoring online abuse not only affects the individuals targeted but also has broader societal implications. These consequences extend to the discouragement of women's engagement and visibility within public spheres, thereby reinforcing gender inequality. This thesis investigates the nuances of how gender bias is expressed through language and within language technologies. Significantly, this thesis expands research on gender bias to multilingual contexts, emphasising the importance of a multilingual and multicultural perspective in understanding societal biases. In this thesis, I adopt an interdisciplinary approach, bridging natural language processing with other disciplines such as politica
    
[^106]: EXPLORER：探索引导的文本强化学习

    EXPLORER: Exploration-guided Reasoning for Textual Reinforcement Learning

    [https://arxiv.org/abs/2403.10692](https://arxiv.org/abs/2403.10692)

    提出了一种名为EXPLORER的探索引导推理代理，用于文本强化学习，能够解决智能体在多个游戏中泛化并在已知和未知对象上表现良好的关键挑战。

    

    文本游戏（TBGs）已经成为自然语言处理任务的一个重要集合，要求强化学习（RL）代理结合自然语言理解和推理。本文提出了一种名为EXPLORER的探索引导推理代理，用于文本强化学习，旨在解决智能体在多个游戏中泛化并在已知和未知对象上表现良好的关键挑战。

    arXiv:2403.10692v1 Announce Type: cross  Abstract: Text-based games (TBGs) have emerged as an important collection of NLP tasks, requiring reinforcement learning (RL) agents to combine natural language understanding with reasoning. A key challenge for agents attempting to solve such tasks is to generalize across multiple games and demonstrate good performance on both seen and unseen objects. Purely deep-RL-based approaches may perform well on seen objects; however, they fail to showcase the same performance on unseen objects. Commonsense-infused deep-RL agents may work better on unseen data; unfortunately, their policies are often not interpretable or easily transferable. To tackle these issues, in this paper, we present EXPLORER which is an exploration-guided reasoning agent for textual reinforcement learning. EXPLORER is neurosymbolic in nature, as it relies on a neural module for exploration and a symbolic module for exploitation. It can also learn generalized symbolic policies and 
    
[^107]: MYTE：形态学驱动的字节编码，用于更好、更公平的多语言语言建模

    MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling

    [https://arxiv.org/abs/2403.10691](https://arxiv.org/abs/2403.10691)

    MYTE是一种基于形态学的字节编码范式，通过使用具有一致大小的片段来实现跨不同语言的信息编码，为99种语言提供了更短的编码，特别是对非欧洲语言和非拉丁文字的改进最为显著。

    

    多语言语言建模中的一个主要考虑因素是如何最好地表示具有不同词汇和文字的语言。尽管当代文本编码方法涵盖了大多数世界文字系统，但它们存在偏向于全球西方高资源语言的问题。因此，少数语言的文本往往被分割为一长串在语言学上毫无意义的单元。为了解决这种不平等，我们引入了一种新的范式，用跨不同语言具有一致大小的片段来编码相同的信息。我们的编码约定（MYTE）基于形态素，因为它们的库存在各种语言中比字符更平衡，而以前的方法使用字符。我们展示MYTE为所有99种分析语言产生了更短的编码，其中非欧洲语言和非拉丁文字的改进最为显著。这进而改善了多语言语言建模的性能。

    arXiv:2403.10691v1 Announce Type: cross  Abstract: A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts. Although contemporary text encoding methods cover most of the world's writing systems, they exhibit bias towards the high-resource languages of the Global West. As a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units. To address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages. Our encoding convention (MYTE) is based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods. We show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts. This, in turn, improves multilingual LM performance and di
    
[^108]: 通向统一多模式个性化：大型视觉语言模型用于生成推荐和更多领域

    Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond

    [https://arxiv.org/abs/2403.10667](https://arxiv.org/abs/2403.10667)

    本文旨在建立一个统一的多模态个性化系统(UniMP)，有效利用多模态数据同时消除与任务和模态特定定制相关的复杂性。

    

    开发一个能够有效利用异构资源并满足各种个性化需求的通用模型一直是社区渴望的目标。我们日常的选择，尤其是在时尚和零售等领域，很大程度上受多模态数据的影响，比如图片和文本描述。这些模态不仅提供直观的指导，还迎合个性化用户偏好。然而，当前主流的个性化方法主要聚焦于基于ID或文本的推荐问题，未能理解涵盖各种任务或模态的信息。本文的目标是建立一个统一的多模态个性化系统(UniMP)，能够有效利用多模态数据，同时消除与任务和模态特定定制相关的复杂性。我们认为基础生成建模的进展提供了

    arXiv:2403.10667v1 Announce Type: cross  Abstract: Developing a universal model that can effectively harness heterogeneous resources and respond to a wide range of personalized needs has been a longstanding community aspiration. Our daily choices, especially in domains like fashion and retail, are substantially shaped by multi-modal data, such as pictures and textual descriptions. These modalities not only offer intuitive guidance but also cater to personalized user preferences. However, the predominant personalization approaches mainly focus on the ID or text-based recommendation problem, failing to comprehend the information spanning various tasks or modalities. In this paper, our goal is to establish a Unified paradigm for Multi-modal Personalization systems (UniMP), which effectively leverages multi-modal data while eliminating the complexities associated with task- and modality-specific customization. We argue that the advancements in foundational generative modeling have provided
    
[^109]: DiPaCo: 分布式路径组合

    DiPaCo: Distributed Path Composition

    [https://arxiv.org/abs/2403.10616](https://arxiv.org/abs/2403.10616)

    DiPaCo提出了一种协同模块化架构和训练方法，可以通过路径分发计算，实现机器学习模型的训练，并在推断时无需模型压缩。

    

    机器学习（ML）领域的进展得益于扩展神经网络模型。这种扩展是通过不断壮举的工程努力实现的，以适应需要设备之间高带宽通信的并行ML方法。在这项工作中，我们提出了一个为ML模型设计的协同模块化架构和训练方法，称为DIstributed PAth COmposition（DiPaCo）。在训练过程中，DiPaCo通过一组共享模块的路径进行计算分发。结合一种受Local-SGD启发的优化方法（DiLoCo），该方法通过大幅减少通信来确保模块同步，促进了跨连接不佳且异构的工作节点的训练，其设计确保了对工作节点故障和抢占的稳健性。在推断时，每个输入只需要执行一条路径，无需任何模型压缩。我们认为这是一种首创性的方法。

    arXiv:2403.10616v1 Announce Type: cross  Abstract: Progress in machine learning (ML) has been fueled by scaling neural network models. This scaling has been enabled by ever more heroic feats of engineering, necessary for accommodating ML approaches that require high bandwidth communication between devices working in parallel. In this work, we propose a co-designed modular architecture and training approach for ML models, dubbed DIstributed PAth COmposition (DiPaCo). During training, DiPaCo distributes computation by paths through a set of shared modules. Together with a Local-SGD inspired optimization (DiLoCo) that keeps modules in sync with drastically reduced communication, Our approach facilitates training across poorly connected and heterogeneous workers, with a design that ensures robustness to worker failures and preemptions. At inference time, only a single path needs to be executed for each input, without the need for any model compression. We consider this approach as a first 
    
[^110]: 神经侵蚀：在AI系统中模拟受控神经退行和衰老

    Neural Erosion: Emulating Controlled Neurodegeneration and Aging in AI Systems

    [https://arxiv.org/abs/2403.10596](https://arxiv.org/abs/2403.10596)

    这项研究介绍了一种在人工智能系统中模拟受控神经退行的新方法，通过在LLMs中添加噪音或切除神经元来逐渐降低其性能，在智商测试中展示了神经退行的过程，并与传统的计算机视觉领域不同，是首个使用文本数据模拟神经退行的工作。

    

    创建控制方法以模拟人工智能（AI）中的神经退行对于模拟大脑功能下降和认知障碍的应用至关重要。我们利用大型语言模型（LLMs）执行的智商测试，特别是LLaMA 2，引入了“神经侵蚀”的概念。这种有意的侵蚀涉及在训练期间或之后切除突触或神经元，或添加高斯噪声，导致LLMs表现的逐渐下降。我们能够描述智商测试中的神经退行，并显示LLM首先失去其数学能力，然后失去其语言能力，同时进一步失去理解问题的能力。据我们所知，这是首个使用文本数据建模神经退行的工作，与在计算机视觉领域进行操作的其他工作相比。最后，我们对我们的研究与认知进行相似性的研究进行了对比。

    arXiv:2403.10596v1 Announce Type: cross  Abstract: Creating controlled methods to simulate neurodegeneration in artificial intelligence (AI) is crucial for applications that emulate brain function decline and cognitive disorders. We use IQ tests performed by Large Language Models (LLMs) and, more specifically, the LLaMA 2 to introduce the concept of ``neural erosion." This deliberate erosion involves ablating synapses or neurons, or adding Gaussian noise during or after training, resulting in a controlled progressive decline in the LLMs' performance. We are able to describe the neurodegeneration in the IQ tests and show that the LLM first loses its mathematical abilities and then its linguistic abilities, while further losing its ability to understand the questions. To the best of our knowledge, this is the first work that models neurodegeneration with text data, compared to other works that operate in the computer vision domain. Finally, we draw similarities between our study and cogn
    
[^111]: 大型语言模型指导的心力衰竭风险预测的ECG双注意力网络

    Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction

    [https://arxiv.org/abs/2403.10581](https://arxiv.org/abs/2403.10581)

    提出了一种大型语言模型指导的双注意力ECG网络，用于心力衰竭风险预测，能够捕捉复杂的心电图特征，有效应对低风险和高风险组之间的不平衡。

    

    心力衰竭（HF）由于全球死亡率不断上升而构成重大公共卫生挑战。通过早期诊断和预防来解决这一问题可显著减少疾病对社会的影响。本文引入了一种使用临床获取的12导联心电图（ECG）进行HF风险预测的方法。我们提出了一种新颖的、轻量级的双注意力ECG网络，旨在捕捉对早期HF预测至关重要的复杂心电图特征，尽管低风险和高风险组之间存在明显的不平衡。该网络具有一个跨导注意力模块和12个导联特定的时间注意力模块，以捕捉交叉导联交互作用和每个导联内的局部时间动态。为了防止模型过拟合于有限的训练数据，我们利用一个大型语言模型（LLM）与公共ECG-Report数据集进行预训练，用于进行ECG-报告对齐任务。然后对网络进行fine-tune以用于HF风险预测

    arXiv:2403.10581v1 Announce Type: cross  Abstract: Heart failure (HF) poses a significant public health challenge due to its rising global mortality rate. Addressing this issue through early diagnosis and prevention could significantly reduce the disease's impact. This work introduces a methodology for HF risk prediction using clinically acquired 12-lead electrocardiograms (ECGs). We present a novel, lightweight dual-attention ECG network designed to capture complex ECG features essential for early HF prediction, despite the notable imbalance between low and high-risk groups. The network features a cross-lead attention module and twelve lead-specific temporal attention modules to capture cross-lead interactions and local temporal dynamics within each lead. To prevent model overfitting from limited training data, we leverage a large language model (LLM) with a public ECG-Report dataset for pretraining on an ECG-report alignment task. The network is then fine-tuned for HF risk prediction
    
[^112]: 忽略我但不要替代我：利用非语言元素进行网络安全领域的预训练

    Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain

    [https://arxiv.org/abs/2403.10576](https://arxiv.org/abs/2403.10576)

    利用非语言元素进行网络安全领域的预训练，提出了新的预训练方法并在网络安全领域中取得了优越表现

    

    针对网络安全信息通常技术复杂且通过非结构化文本传递，使得自动化处理网络威胁情报变得极具挑战性。针对涉及高度专业知识的文本领域，基于领域语料库的预训练一直是语言模型获取领域专业知识的一种常见方法。然而，网络安全文本通常包含非语言元素（如URL和哈希值），这可能与现有的预训练方法不适用。先前在其他领域的工作中，已将此类文本视为噪音进行移除或过滤，但这些方法的有效性尚未得到调查，特别是在网络安全领域。我们提出了不同的预训练方法，并通过下游任务和探测任务评估了它们的有效性。我们提出的策略（选择性MLM和联合训练NLE标记分类）优于常用的替换非

    arXiv:2403.10576v1 Announce Type: cross  Abstract: Cybersecurity information is often technically complex and relayed through unstructured text, making automation of cyber threat intelligence highly challenging. For such text domains that involve high levels of expertise, pretraining on in-domain corpora has been a popular method for language models to obtain domain expertise. However, cybersecurity texts often contain non-linguistic elements (such as URLs and hash values) that could be unsuitable with the established pretraining methodologies. Previous work in other domains have removed or filtered such text as noise, but the effectiveness of these methods have not been investigated, especially in the cybersecurity domain. We propose different pretraining methodologies and evaluate their effectiveness through downstream tasks and probing tasks. Our proposed strategy (selective MLM and jointly training NLE token classification) outperforms the commonly taken approach of replacing non-l
    
[^113]: 用辅助功能探索语言模型的代码生成能力

    Exploring Language Model's Code Generation Ability with Auxiliary Functions

    [https://arxiv.org/abs/2403.10575](https://arxiv.org/abs/2403.10575)

    在这项研究中，我们全面评估了最近代码预训练语言模型中编码的辅助函数利用能力，通过设计实验，并通过实现风格分析，我们发现了模型利用辅助函数的很有前景。

    

    辅助函数是改善语言模型代码生成能力的有用组件。然而，对它们如何影响的系统性探索尚未完成。本研究全面评估了最近的代码预训练语言模型中编码的辅助函数利用能力。我们首先构建了一个人工设计的评估集，称为HumanExtension，其中包含一个函数帮助另一个函数的示例。通过HumanExtension，我们设计了几个实验以多方面地检验它们的能力。我们的评估过程使得能够全面理解包括辅助函数在提示中的有效性和鲁棒性。另外，通过实现风格分析捕捉到模型在访问辅助函数时的各种实现模式。通过这一分析，我们发现模型利用辅助函数的能力很有前景。

    arXiv:2403.10575v1 Announce Type: cross  Abstract: Auxiliary function is a helpful component to improve language model's code generation ability. However, a systematic exploration of how they affect has yet to be done. In this work, we comprehensively evaluate the ability to utilize auxiliary functions encoded in recent code-pretrained language models. First, we construct a human-crafted evaluation set, called HumanExtension, which contains examples of two functions where one function assists the other. With HumanExtension, we design several experiments to examine their ability in a multifaceted way. Our evaluation processes enable a comprehensive understanding of including auxiliary functions in the prompt in terms of effectiveness and robustness. An additional implementation style analysis captures the models' various implementation patterns when they access the auxiliary function. Through this analysis, we discover the models' promising ability to utilize auxiliary functions includi
    
[^114]: MoPE：通过Prompt专家混合实现参数高效和可扩展的多模态融合

    MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts

    [https://arxiv.org/abs/2403.10568](https://arxiv.org/abs/2403.10568)

    本文提出了MoPE技术，通过解开提示以自适应捕获数据集级和实例级特征，引入了混合Prompt专家来增强表达能力，并且在多模态融合中表现出更大的表达能力和可扩展性。

    

    Prompt调整已经证明在融合多模态任务的单模基础模型时具有参数效率性。然而，其有限的适应性和表达能力导致性能不佳与其他调整方法相比。本文通过将简单提示解开以自适应地捕获数据集级和实例级特征来解决这个问题。建立在这种解开的基础上，我们引入了Prompt专家的混合（MoPE）技术来增强表达能力。MoPE利用多模态配对先验在每个实例基础上路由最有效的提示。与简单提示相比，我们基于MoPE的条件提示对多模态融合具有更大的表达能力，在训练数据和可训练参数总数上具有更好的扩展性。我们还研究了一个专家路由的正则化项，导致专家的不断发展专长，不同专家专注于不同的特征。

    arXiv:2403.10568v1 Announce Type: cross  Abstract: Prompt-tuning has demonstrated parameter-efficiency in fusing unimodal foundation models for multimodal tasks. However, its limited adaptivity and expressiveness lead to suboptimal performance when compared with other tuning methods. In this paper, we address this issue by disentangling the vanilla prompts to adaptively capture dataset-level and instance-level features. Building upon this disentanglement, we introduce the mixture of prompt experts (MoPE) technique to enhance expressiveness. MoPE leverages multimodal pairing priors to route the most effective prompt on a per-instance basis. Compared to vanilla prompting, our MoPE-based conditional prompting exhibits greater expressiveness for multimodal fusion, scaling better with the training data and the overall number of trainable parameters. We also study a regularization term for expert routing, leading to emergent expert specialization, where different experts focus on different c
    
[^115]: 二阶信息很重要：重访大型语言模型的机器遗忘问题

    Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models

    [https://arxiv.org/abs/2403.10557](https://arxiv.org/abs/2403.10557)

    本论文通过二阶信息（Hessian）的视角重新审视了大型语言模型的机器遗忘问题，提出了遗忘算法，具有较高的鲁棒性。

    

    随着大型语言模型（LLMs）的快速发展，我们目睹了ChatGPT、LLaMa和Gemini等主要LLM产品之间的激烈竞争。然而，训练语料库的各种问题（如隐私泄露和版权侵犯）仍然未被充分探讨。以LLM从业者的视角来看，处理这些意外的隐私侵犯可能具有挑战性。之前的研究通过使用梯度信息解决了LLMs的“遗忘”问题，但它们大多引入了显著的开销，如数据预处理或缺乏鲁棒性。在本文中，与基于一阶信息的方法形成对比，我们通过二阶信息（Hessian）的视角重新审视了遗忘问题。受经典牛顿更新启发，我们的遗忘算法具有

    arXiv:2403.10557v1 Announce Type: cross  Abstract: With the rapid development of Large Language Models (LLMs), we have witnessed intense competition among the major LLM products like ChatGPT, LLaMa, and Gemini. However, various issues (e.g. privacy leakage and copyright violation) of the training corpus still remain underexplored. For example, the Times sued OpenAI and Microsoft for infringing on its copyrights by using millions of its articles for training. From the perspective of LLM practitioners, handling such unintended privacy violations can be challenging. Previous work addressed the ``unlearning" problem of LLMs using gradient information, while they mostly introduced significant overheads like data preprocessing or lacked robustness. In this paper, contrasting with the methods based on first-order information, we revisit the unlearning problem via the perspective of second-order information (Hessian). Our unlearning algorithms, which are inspired by classic Newton update, are 
    
[^116]: PET-SQL：一个带有交叉一致性的增强提示的两阶段文本到SQL框架

    PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency

    [https://arxiv.org/abs/2403.09732](https://arxiv.org/abs/2403.09732)

    提出了一个两阶段框架，通过引入参考增强表示和少样本演示，解决了在处理冗长的数据库信息和复杂用户意图时的挑战。

    

    最近文本到SQL（Text2SQL）领域的进展强调刺激大型语言模型（LLM）进行上下文学习，取得了显著成果。然而，他们在处理冗长的数据库信息和复杂的用户意图时面临挑战。本文提出了一个两阶段框架，以增强当前基于LLM的自然语言到SQL系统的性能。我们首先引入了一种新颖的提示表示，称为参考增强表示，其中包括模式信息和从表格随机抽样的单元格值，以指导LLM生成SQL查询。然后，在第一阶段，我们检索问题-SQL对作为少量演示，促使LLM生成初步SQL（PreSQL）。之后，解析PreSQL中提到的实体进行模式链接，可以显著压缩有用信息。在第二阶段，利用链接的模式，我们简化了

    arXiv:2403.09732v1 Announce Type: cross  Abstract: Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large language models (LLM) on in-context learning, achieving significant results. Nevertheless, they face challenges when dealing with verbose database information and complex user intentions. This paper presents a two-stage framework to enhance the performance of current LLM-based natural language to SQL systems. We first introduce a novel prompt representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After that, the mentioned entities in PreSQL are parsed to conduct schema linking, which can significantly compact the useful information. In the second stage, with the linked schema, we simplify the 
    
[^117]: Quiet-STaR: 语言模型可以自己学会思考后再说话

    Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking

    [https://arxiv.org/abs/2403.09629](https://arxiv.org/abs/2403.09629)

    Quiet-STaR提出了一种新的泛化版本，在每个标记处生成解释未来文本的思考过程，从而改善预测能力

    

    写作和交谈时，人们有时会停下来思考。尽管以推理为重点的作品通常将推理框定为回答问题或完成代理任务的方法，但推理几乎都隐含在所有书面文本中。例如，这适用于证明中未明确说明的步骤，以及支撑对话的心智理论。在自学习推理者（STaR，Zelikman等，2022）中，通过从少量示例中推断来自问答中有用的思考，并学习那些导致正确答案的思考。这是一个高度受限制的环境--理想情况下, 一个语言模型可以学会从任意文本中推断未明确说明的思考。我们提出Quiet-STaR，这是STaR的一个泛化版本，其中语言模型学会在每个标记处生成解释未来文本的思考过程，从而改善其预测。我们解决了一些关键挑战，包括1）生成连续的计算成本

    arXiv:2403.09629v1 Announce Type: cross  Abstract: When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continu
    
[^118]: AutoLoRA：基于元学习的自动调整矩阵秩在低秩适应中的应用

    AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning

    [https://arxiv.org/abs/2403.09113](https://arxiv.org/abs/2403.09113)

    AutoLoRA提出了一个基于元学习的框架，自动识别每个LoRA层的最佳秩，以解决LoRA中秩分配和秩搜索的问题，进而提高微调性能。

    

    大规模预训练之后进行任务特定微调在各种自然语言处理任务中取得了巨大成功。然而，对于大型预训练模型的所有参数进行微调存在着巨大的计算和内存挑战，因此研发了几种高效的微调方法。其中，低秩适应（LoRA）通过在冻结的预训练权重之上微调低秩增量更新矩阵，被证明特别有效。然而，LoRA在所有层中均匀分配秩，并且依赖于穷举搜索来找到最佳秩，导致了高计算成本和微调性能不佳。为了解决这些限制，我们引入了AutoLoRA，这是一个基于元学习的框架，用于自动识别每个LoRA层的最佳秩。AutoLoRA将低秩更新矩阵中的每个秩为1的矩阵与选择变量相关联，该变量决定了秩为1的矩阵是否应该被...

    arXiv:2403.09113v1 Announce Type: cross  Abstract: Large-scale pretraining followed by task-specific finetuning has achieved great success in various NLP tasks. Since finetuning all parameters of large pretrained models poses substantial computational and memory challenges, several efficient finetuning methods have been developed. Among them, low-rank adaptation (LoRA), which finetunes low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective. Nonetheless, LoRA's uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal finetuning performance. To address these limitations, we introduce AutoLoRA, a meta learning based framework for automatically identifying the optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should b
    
[^119]: AraTrust：阿拉伯语大型语言模型信誉评估

    AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic

    [https://arxiv.org/abs/2403.09017](https://arxiv.org/abs/2403.09017)

    AraTrust是第一个阿拉伯语大型语言模型的全面信誉基准，解决了缺乏全面信誉评估基准的问题，帮助准确评估和提高LLMs的安全性。

    

    arXiv:2403.09017v1 公告类型：新摘要：人工智能系统的迅速发展和广泛接受凸显了理解人工智能的能力和潜在风险的迫切需要。鉴于阿拉伯语在人工智能研究中的语言复杂性、文化丰富性和地位不高，有必要专注于阿拉伯语相关任务的大型语言模型（LLMs）的性能和安全性。尽管它们的发展取得了一些进展，但缺乏全面的信誉评估基准是准确评估和提高在阿拉伯语提示时LLMs的安全性面临的主要挑战。本文介绍了AraTrust 1，这是第一个针对阿拉伯语大型语言模型的全面的信誉基准。AraTrust 包含了516个人工编写的多项选择题，涉及与真实性、道德、安全性、身体健康、心理健康、不公平行为、非法活动相关的多个维度。

    arXiv:2403.09017v1 Announce Type: new  Abstract: The swift progress and widespread acceptance of artificial intelligence (AI) systems highlight a pressing requirement to comprehend both the capabilities and potential risks associated with AI. Given the linguistic complexity, cultural richness, and underrepresented status of Arabic in AI research, there is a pressing need to focus on Large Language Models (LLMs) performance and safety for Arabic related tasks. Despite some progress in their development, there is a lack of comprehensive trustworthiness evaluation benchmarks which presents a major challenge in accurately assessing and improving the safety of LLMs when prompted in Arabic. In this paper, we introduce AraTrust 1, the first comprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises 516 human-written multiple-choice questions addressing diverse dimensions related to truthfulness, ethics, safety, physical health, mental health, unfairness, illegal activities
    
[^120]: 生成预训练结构化Transformer：规模化的无监督句法语言模型

    Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale

    [https://arxiv.org/abs/2403.08293](https://arxiv.org/abs/2403.08293)

    GPST是一种无监督的句法语言模型，通过联合训练两个模型实现对原始文本的高并行预训练，克服了之前SLM依赖于黄金树和顺序训练的限制，展示了在多个任务中优于同等规模的GPT-2。

    

    句法语言模型（SLM）以从左到右的方式逐步生成带有其句法树的句子。我们提出了生成预训练结构化Transformer（GPST），这是一种规模化的无监督SLM，能够在原始文本上从头开始进行高并行预训练。GPST规避了之前SLM的一些限制，比如依赖于黄金树和顺序训练。它由两个组件组成，一个通常的SLM受单向语言建模损失的监督，以及一个额外的组合模型，用于引导句法解析树并计算成分表示，受双向语言建模损失的监督。我们提出了一个表示替代方案，以实现两个模型的联合并行训练，采用硬EM的方式。我们在OpenWebText上对GPST进行了预训练，该语料库包括90亿个token，并展示了GPST在许多任务上的优越性，涵盖了与GPT-2相当规模的内容。

    arXiv:2403.08293v1 Announce Type: cross  Abstract: A syntactic language model (SLM) incrementally generates a sentence with its syntactic tree in a left-to-right manner. We present Generative Pretrained Structured Transformers (GPST), an unsupervised SLM at scale capable of being pre-trained from scratch on raw texts with high parallelism. GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training. It consists of two components, a usual SLM supervised by a uni-directional language modeling loss, and an additional composition model, which induces syntactic parse trees and computes constituent representations, supervised by a bi-directional language modeling loss. We propose a representation surrogate to enable joint parallel training of the two models in a hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable size in numerous tasks covering bot
    
[^121]: 通过融合高度专业化语言模型同时掌握文本、代码和数学

    Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models

    [https://arxiv.org/abs/2403.08281](https://arxiv.org/abs/2403.08281)

    通过融合高度专业化的语言、代码和数学模型，提出了一种名为UltraFuser的融合框架，引入了标记级别的门控机制，并设计了两阶段训练策略，以同时在三个领域取得高性能。

    

    自然语言、编程代码和数学符号的底层数据分布变化巨大，对于那些努力同时在三个领域实现高性能的大型语言模型（LLMs）提出了复杂挑战。本文提出了一种直接融合已经高度专业化模型的方法。所提出的融合框架UltraFuser包括三个已经在语言、编码和数学上得到充分训练的专家。引入了一个标记级别的门控机制来混合专家的输出。设计了一个伴随平衡采样的两阶段训练策略以确保稳定性。为了有效训练融合模型，我们进一步构建了一个

    arXiv:2403.08281v1 Announce Type: cross  Abstract: Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we propose to fuse models that are already highly-specialized directly. The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. A token-level gating mechanism is introduced to blend the specialists' outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a 
    
[^122]: 基于矩阵变换的低秩调整（MTLoRA）：一种受大脑启发的参数高效微调方法

    Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning

    [https://arxiv.org/abs/2403.07440](https://arxiv.org/abs/2403.07440)

    该论文提出了基于矩阵变换的低秩调整（MTLoRA）方法，受大脑启发，用于提高微调技术的复杂任务适应性、性能、稳定性和算法复杂性。

    

    基于大型预训练语言模型（LPLMs）的微调技术已被证明可以显著提高模型在各种下游任务上的性能，并有效控制LPLMs的输出行为。本文受大脑功能受其几何结构塑造的启发，将这一思想融入LoRA技术中，提出了一种新的基于矩阵变换的重新参数化方法，以减少复杂任务适应性、性能、稳定性和算法复杂性方面的改进空间。

    arXiv:2403.07440v1 Announce Type: cross  Abstract: Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs. Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources. Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity. In response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameteriz
    
[^123]: 知识图谱大型语言模型（KG-LLM）用于链接预测

    Knowledge Graph Large Language Model (KG-LLM) for Link Prediction

    [https://arxiv.org/abs/2403.07311](https://arxiv.org/abs/2403.07311)

    该论文提出了知识图谱大型语言模型框架（KG-LLM），利用思维链提示和上下文学习等NLP范例，以增强知识图谱中的多跳链接预测，并展示了框架在微调大型语言模型和零次尝试能力方面的有效性。

    

    在知识图谱分析领域，预测知识图谱（KGs）内多个链接的任务是一个挑战，由于自然语言处理（NLP）和知识图嵌入技术的进步，这一挑战变得越来越可解决。本文介绍了一种新的方法，即知识图谱大型语言模型框架（KG-LLM），该框架利用关键的NLP范例，包括思维链提示（CoT）和上下文学习（ICL），以增强知识图谱中的多跳链接预测。通过将KG转换为CoT提示，我们的框架旨在识别并学习实体及其相互关系的潜在表示。为了展示KG-LLM框架的有效性，我们在该框架内微调了三种主要的大型语言模型（LLMs），同时采用了非ICL和ICL任务进行全面评估。此外，我们探讨了该框架为LLMs提供零次尝试能力的潜力。

    arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
    
[^124]: ALaRM: 通过分层奖励建模对齐语言模型

    ALaRM: Align Language Models via Hierarchical Rewards Modeling

    [https://arxiv.org/abs/2403.06754](https://arxiv.org/abs/2403.06754)

    ALaRM是第一个从人类反馈中建模分层奖励的框架，通过整合整体奖励与特定方面的奖励，改善了大型语言模型与人类偏好的对齐性，尤其在复杂文本生成任务中表现出更精确和一致的指导。

    

    我们介绍了ALaRM，第一个在强化学习中从人类反馈模型分层奖励的框架，旨在增强大型语言模型（LLMs）与人类偏好的对齐性。该框架解决了当前对齐方法的限制，这些方法通常难以处理人类监督信号的不一致性和稀疏性，通过将整体奖励与特定方面的奖励相结合。这种整合使得语言模型更加精确和一致地指导朝着期望的结果前进，尤其在复杂和开放的文本生成任务中。通过应用基于一致性的方法来过滤和组合多个奖励，该框架提供了一种可靠的机制来改善模型的对齐性。我们通过在长篇问题回答和机器翻译任务中使用gpt-3.5-turbo进行成对比较来验证我们的方法。

    arXiv:2403.06754v1 Announce Type: cross  Abstract: We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demon
    
[^125]: 使用大型语言模型的组合分数测量人脑中的含义合成

    Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models

    [https://arxiv.org/abs/2403.04325](https://arxiv.org/abs/2403.04325)

    引入了Composition Score，一种基于模型的度量标准，用于量化句子理解中的含义合成程度，实验证明这一度量与大脑区域相关，揭示了含义合成在人类句子理解中的多方面性。

    

    含义合成的过程是指更小的单位如语素或单词组合形成短语和句子的含义，对于人类句子理解至关重要。尽管神经语言学对涉及含义合成的大脑区域进行了大量研究，但仍缺乏一种计算度量来量化合成的程度。借鉴变压器前馈网络块的键值内存解释，我们引入了组合分数，这是一种新颖的基于模型的度量标准，旨在量化句子理解过程中的含义合成程度。实验结果表明，这一度量与大脑簇相关联，这些大脑簇与词频率、结构处理和对单词的一般敏感性有关，这表明了人类句子理解过程中含义合成的多方面性。

    arXiv:2403.04325v1 Announce Type: cross  Abstract: The process of meaning composition, wherein smaller units like morphemes or words combine to form the meaning of phrases and sentences, is essential for human sentence comprehension. Despite extensive neurolinguistic research into the brain regions involved in meaning composition, a computational metric to quantify the extent of composition is still lacking. Drawing on the key-value memory interpretation of transformer feed-forward network blocks, we introduce the Composition Score, a novel model-based metric designed to quantify the degree of meaning composition during sentence comprehension. Experimental findings show that this metric correlates with brain clusters associated with word frequency, structural processing, and general sensitivity to words, suggesting the multifaceted nature of meaning composition during human sentence comprehension.
    
[^126]: 愤怒的男性，悲伤的女性：大型语言模型在情绪归因中反映性别刻板印象

    Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution

    [https://arxiv.org/abs/2403.03121](https://arxiv.org/abs/2403.03121)

    大型语言模型中存在性别化情绪归因，反映了社会刻板印象。

    

    大型语言模型（LLMs）反映社会规范和偏见，尤其是关于性别的。虽然社会偏见和刻板印象在各种自然语言处理应用中得到了广泛研究，但在情绪分析方面存在一个令人惊讶的空白。然而，在社会话语中，情绪和性别密切相关。例如，女性经常被认为更具移情能力，而男性的愤怒更受社会接受。为了填补这一空白，我们提出了对五种最先进的LLMs（开源和封闭源）进行性别化情绪归因的首次全面研究。我们调查情绪是否具有性别特征，以及这些变化是否基于社会刻板印象。我们提示模型采用性别化角色并将情绪归因于类似“当我与亲近的人发生严重争执”这样的事件。然后我们分析模型生成的情绪与性别-事件对之间的关系。我们发现所有模型一致展现出性别化。

    arXiv:2403.03121v1 Announce Type: new  Abstract: Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men's anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like 'When I had a serious argument with a dear person'. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered e
    
[^127]: SERVAL：垂直模型和LLM之间的协同学习，实现零-shot级别的医学预测

    SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction

    [https://arxiv.org/abs/2403.01570](https://arxiv.org/abs/2403.01570)

    提出SERVAL，一个协同学习流水线，可以通过相互增强，实现LLMs和小模型的垂直能力无监督开发，从而改善领域特定垂直问题的零-shot预测能力。

    

    近期大型语言模型（LLMs）的发展展示出对通用和常识问题卓越的零-shot能力。然而，LLMs在领域特定垂直问题上的应用仍然落后，主要是由于垂直知识方面的问题和不足。此外，垂直数据注释过程通常需要劳动密集型的专家参与，因此增加了增强模型垂直能力的额外挑战。在本文中，我们提出了SERVAL，一个协同学习流水线，旨在通过相互增强，对LLMs和小模型的垂直能力进行无监督开发。具体来说，SERVAL利用LLMs的零-shot输出作为注释，利用其置信度来从头开始教授一个强大的垂直模型。反过来，训练有素的垂直模型引导LLM微调，以增强其零-shot能力，逐步改进两者。

    arXiv:2403.01570v1 Announce Type: new  Abstract: Recent development of large language models (LLMs) has exhibited impressive zero-shot proficiency on generic and common sense questions. However, LLMs' application on domain-specific vertical questions still lags behind, primarily due to the humiliation problems and deficiencies in vertical knowledge. Furthermore, the vertical data annotation process often requires labor-intensive expert involvement, thereby presenting an additional challenge in enhancing the model's vertical capabilities. In this paper, we propose SERVAL, a synergy learning pipeline designed for unsupervised development of vertical capabilities in both LLMs and small models by mutual enhancement. Specifically, SERVAL utilizes the LLM's zero-shot outputs as annotations, leveraging its confidence to teach a robust vertical model from scratch. Reversely, the trained vertical model guides the LLM fine-tuning to enhance its zero-shot capability, progressively improving both 
    
[^128]: NLP中的情感分析：趋势、差距和未来方向路线

    Emotion Analysis in NLP: Trends, Gaps and Roadmap for Future Directions

    [https://arxiv.org/abs/2403.01222](https://arxiv.org/abs/2403.01222)

    NLP中情感分析领域存在着趋势和差距，未来方向需考虑情感任务定义、情感框架、情感主观性与NLP应用。

    

    情绪是沟通的一个中心方面。因此，情感分析（EA）是自然语言处理（NLP）中一个迅速发展的领域。然而，关于范围、方向或方法，尚无共识。在本文中，我们对过去十年中的154篇相关NLP出版物进行了彻底审查。基于此审查，我们对四个不同的问题进行了探讨：（1）NLP中如何定义EA任务？（2）什么是最突出的情感框架，以及哪些情感被建模？（3）在人口统计和文化因素方面是否考虑了情绪的主观性？以及（4）EA的主要NLP应用是什么？我们总结了EA和任务、使用的情感框架、现有数据集、方法和应用的趋势。然后我们讨论了四个空白：（1）缺乏人口统计和文化因素，并未考虑到不同文化如何感知情绪的差异，而是假设它们是普遍经历的。

    arXiv:2403.01222v1 Announce Type: new  Abstract: Emotions are a central aspect of communication. Consequently, emotion analysis (EA) is a rapidly growing field in natural language processing (NLP). However, there is no consensus on scope, direction, or methods. In this paper, we conduct a thorough review of 154 relevant NLP publications from the last decade. Based on this review, we address four different questions: (1) How are EA tasks defined in NLP? (2) What are the most prominent emotion frameworks and which emotions are modeled? (3) Is the subjectivity of emotions considered in terms of demographics and cultural factors? and (4) What are the primary NLP applications for EA? We take stock of trends in EA and tasks, emotion frameworks used, existing datasets, methods, and applications. We then discuss four lacunae: (1) the absence of demographic and cultural aspects does not account for the variation in how emotions are perceived, but instead assumes they are universally experienced
    
[^129]: ParallelPARC: 生成自然语言类比的可扩展流水线

    ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies

    [https://arxiv.org/abs/2403.01139](https://arxiv.org/abs/2403.01139)

    设计了ParallelPARC流水线，利用大型语言模型生成复杂段落类比数据集，评估各种类比类型，并展示出人类在类比识别中的优势。

    

    Analogy-making对于人类认知至关重要，使我们能够适应新颖情境--这是当前人工智能系统仍然缺乏的能力。大多数类比数据集今天关注简单的类比（例如，词类比）；包含复杂类型类比的数据集通常是手工策划的，并且非常小。我们认为这限制了计算类比的进展。在这项工作中，我们设计了一个数据生成流水线，ParallelPARC（Parallel Paragraph Creator），利用最先进的大型语言模型（LLM）来创建基于段落的复杂类比，以及简单和具有挑战性的干扰项。我们展示了我们的流水线，并创建了ProPara-Logy，一个关于科学过程间类比的数据集。我们发布了一个由人类验证过的金标准数据集，以及一个自动生成的银标准数据集。我们在二进制和多选环境中测试了LLMs和人类对类比的识别，发现人类胜过最佳模型。

    arXiv:2403.01139v1 Announce Type: cross  Abstract: Analogy-making is central to human cognition, allowing us to adapt to novel situations -- an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g., word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy. In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to create complex, paragraph-based analogies, as well as distractors, both simple and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset of analogies between scientific processes. We publish a gold-set, validated by humans, and a silver-set, generated automatically. We test LLMs' and humans' analogy recognition in binary and multiple-choice settings, and found that humans outperform the best mod
    
[^130]: 无梯度自适应全局剪枝用于预训练语言模型

    Gradient-Free Adaptive Global Pruning for Pre-trained Language Models

    [https://arxiv.org/abs/2402.17946](https://arxiv.org/abs/2402.17946)

    提出了自适应全局剪枝（AdaGP）框架，通过重新定义全局剪枝过程为可管理的协调子问题，实现对大型语言模型的资源高效优化，显著提高性能。

    

    大型语言模型（LLMs）如LLaMA和GPT在自然语言处理中的转变性影响受到它们计算需求过高的限制。剪枝作为一种关键的压缩策略出现，引入稀疏性以增强内存和计算效率。然而，传统的全局剪枝对LLMs来说由于可扩展性问题而不实用，而本地剪枝，尽管效率高，却导致次优解决方案。为解决这些挑战，我们提出了自适应全局剪枝（AdaGP），这是一个重新定义全局剪枝处理为可管理的协调子问题的新框架，可以实现资源有效的全局最优化优化。AdaGP的方法将LLMs概念化为一系列模块化函数，并利用辅助变量进行问题分解，不仅便于在LLMs上实现实际应用，而且显示出显著的性能改进。

    arXiv:2402.17946v1 Announce Type: new  Abstract: The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose Adaptive Global Pruning (AdaGP), a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. AdaGP's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, part
    
[^131]: 检索即精准生成

    Retrieval is Accurate Generation

    [https://arxiv.org/abs/2402.17532](https://arxiv.org/abs/2402.17532)

    提出了一种从支持文档中选择上下文感知短语的新颖生成方法，通过迭代式自我强化提高训练数据准确性，在各种任务中表现优越，提高了开放式文本生成的质量。

    

    标准语言模型通过从固定的、有限的和独立的词汇中选择标记来生成文本。我们介绍了一种新颖的方法，从一组支持文档中选择上下文感知的短语。这种范式转变中最重要的挑战之一是确定训练数据，因为文本可以以多种方式分割，并且每个片段都可以从多个可能的文档中检索到。为了解决这个问题，我们提出使用语言启发式初始化训练数据，更重要的是通过迭代式自我强化来引导训练数据。大量实验表明，我们的模型不仅在各种知识密集型任务上优于标准语言模型，而且在开放式文本生成中展现出更好的生成质量。例如，与标准语言模型对应的模型，在开放性任务上将准确率从23.47%提高到36.27%。

    arXiv:2402.17532v1 Announce Type: new  Abstract: Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation. For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on Open
    
[^132]: 预测下一个词：人类在这项任务中表现出不确定性，而语言模型_____

    Predict the Next Word: <Humans exhibit uncertainty in this task and language models _____>

    [https://arxiv.org/abs/2402.17527](https://arxiv.org/abs/2402.17527)

    评估语言模型在预测下一个词时，是否能够复现人类在这项任务中展示的语言变化性

    

    语言模型（LMs）是训练用于为人类生成文本分配概率的统计模型。因此，合理质疑它们是否很好地近似人类展示的语言变化性。这种形式的统计评估在段落级别上很难执行，因为它需要可接受性判断（即，人类评估）或一个强大的自动代理（这是不平凡的）。然而，在单词级别上，通过给定一些上下文，可以通过与一个预先记录的替代单词连续数据集的精确匹配来评估LM的样本。我们利用这一事实，并评估LM重新生成人类（特别是一群英语使用者）在“下一个词预测”任务中展示的变化的能力。这可以被视为一种校准评估，在文本分类的背景下，Baan等人（2022年）将其称为对人类不确定性的校准

    arXiv:2402.17527v1 Announce Type: cross  Abstract: Language models (LMs) are statistical models trained to assign probability to human-generated text. As such, it is reasonable to question whether they approximate linguistic variability exhibited by humans well. This form of statistical assessment is difficult to perform at the passage level, for it requires acceptability judgements (i.e., human evaluation) or a robust automated proxy (which is non-trivial). At the word level, however, given some context, samples from an LM can be assessed via exact matching against a prerecorded dataset of alternative single-word continuations of the available context. We exploit this fact and evaluate the LM's ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the 'next word prediction' task. This can be seen as assessing a form of calibration, which, in the context of text classification, Baan et al. (2022) termed calibration to human uncertaint
    
[^133]: DistALANER：开源软件生态系统中的远程监督主动学习增强命名实体识别

    DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem

    [https://arxiv.org/abs/2402.16159](https://arxiv.org/abs/2402.16159)

    提出了一种为开源软件系统量身定制的新颖命名实体识别技术，通过远程监督注释过程、语言启发、查找表、外部知识源和主动学习方法来提高模型性能，有效缓解了成本和专家标注人员稀缺问题，并在关系抽取下游任务中显著优于LLMs。

    

    本文提出了一种专为开源软件系统量身定制的新颖命名实体识别（NER）技术。我们的方法旨在通过采用全面的两步远程监督注释过程来解决软件数据标注稀缺的问题。该过程巧妙地利用语言启发、独特的查找表、外部知识源以及主动学习方法。通过利用这些强大的技术，我们不仅提高了模型性能，还有效地缓解了成本和专家标注人员稀缺所带来的限制。值得注意的是，我们的框架在很大程度上明显优于最先进的LLMs。我们还展示了NER在关系抽取的下游任务中的有效性。

    arXiv:2402.16159v1 Announce Type: new  Abstract: This paper proposes a novel named entity recognition (NER) technique specifically tailored for the open-source software systems. Our approach aims to address the scarcity of annotated software data by employing a comprehensive two-step distantly supervised annotation process. This process strategically leverages language heuristics, unique lookup tables, external knowledge sources, and an active learning approach. By harnessing these powerful techniques, we not only enhance model performance but also effectively mitigate the limitations associated with cost and the scarcity of expert annotators. It is noteworthy that our framework significantly outperforms the state-of-the-art LLMs by a substantial margin. We also show the effectiveness of NER in the downstream task of relation extraction.
    
[^134]: KorNAT：韩国社会价值观和常识的LLM对齐基准

    KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge

    [https://arxiv.org/abs/2402.13605](https://arxiv.org/abs/2402.13605)

    KorNAT是首个用于评估韩国国家对齐的基准，包括社会价值观和常识对齐两个方面，通过对社会价值和常识多项选择题的测试来评估模型的对齐程度。

    

    对于大型语言模型（LLMs）在特定国家得以有效部署，它们必须具有对该国文化和基本知识的理解。为此，我们引入了国家对齐（National Alignment），从社会价值观对齐和常识对齐两个方面衡量LLM与目标国家之间的对齐。社会价值观对齐评估模型对特定国家社会价值观的理解程度，而常识对齐则检验模型对相关基本国家知识的把握情况。我们构建了KorNAT，这是首个衡量与韩国国家对齐的基准。对于社会价值数据集，我们从包括6174名韩国参与者在内的大规模调查中获得了地面真实标签。对于常识数据集，我们基于韩国教科书和GED参考资料构建了样本。KorNAT包含4K和6K个针对社会价值和常识的多项选择题。

    arXiv:2402.13605v1 Announce Type: new  Abstract: For Large Language Models (LLMs) to be effectively deployed in a specific country, they must possess an understanding of the nation's culture and basic knowledge. To this end, we introduce National Alignment, which measures an alignment between an LLM and a targeted country from two aspects: social value alignment and common knowledge alignment. Social value alignment evaluates how well the model understands nation-specific social values, while common knowledge alignment examines how well the model captures basic knowledge related to the nation. We constructed KorNAT, the first benchmark that measures national alignment with South Korea. For the social value dataset, we obtained ground truth labels from a large-scale survey involving 6,174 unique Korean participants. For the common knowledge dataset, we constructed samples based on Korean textbooks and GED reference materials. KorNAT contains 4K and 6K multiple-choice questions for socia
    
[^135]: GenAudit：利用证据修复语言模型输出中的事实错误

    GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence

    [https://arxiv.org/abs/2402.12566](https://arxiv.org/abs/2402.12566)

    GenAudit是一个工具，通过修订或删除未被参考文献支持的声明，并提供来自参考文献的证据，帮助修复语言模型输出中的事实错误。

    

    LLMs即使可以访问参考文档，也可能生成事实不准确的陈述。在高风险应用中（例如基于文档的医疗保健或金融问答），这样的错误可能具有危险性。我们提出了GenAudit -- 一个旨在帮助检查基于文档任务语言模型响应的工具。GenAudit通过修订或删除未被参考文档支持的声明，同时为看似被证据支持的事实提供来自参考文献的证据，来建议修改LLM响应。我们训练模型来执行这些任务，并设计了一个交互界面，向用户呈现建议的修改和证据。通过人工评分员的全面评估显示，GenAudit在总结不同领域文档时能够检测出8种不同的LLM输出中的错误。为确保系统能够标记大多数错误，我们提出了一种方法，可以提高错误召回率，同时最小化对预处理的影响。

    arXiv:2402.12566v1 Announce Type: new  Abstract: LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance). We present GenAudit -- a tool intended to assist fact-checking LLM responses for document-grounded tasks. GenAudit suggests edits to the LLM response by revising or removing claims that are not supported by the reference document, and also presents evidence from the reference for facts that do appear to have support. We train models to execute these tasks, and design an interactive interface to present suggested edits and evidence to users. Comprehensive evaluation by human raters shows that GenAudit can detect errors in 8 different LLM outputs when summarizing documents from diverse domains. To ensure that most errors are flagged by the system, we propose a method that can increase the error recall while minimizing impact on pre
    
[^136]: NEO-BENCH：使用新词评估大型语言模型的鲁棒性

    NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms

    [https://arxiv.org/abs/2402.12261](https://arxiv.org/abs/2402.12261)

    本研究通过创建一个多样化的最新英语新词资源，分析了大型语言模型对于新词的鲁棒性表现，并构建了一个基准用于评估模型对新词的泛化能力，结果显示机器翻译中模型性能会因引入新词而几乎减半。

    

    Large Language Models (LLMs)的表现会因模型训练数据与推理过程中看到的新文本之间的时间漂移而退化。本文探讨了导致数据漂移的语言变化中一个不太被研究的方向，即随着时间推移而出现的新词形式——新词。我们通过使用几种流行的收集方法创建了一个多样化的最新英语新词资源。我们通过比较包含新词的句子与将新词替换为现有替代词的几乎相同的句子来分析新词对时间漂移的影响。在句子中引入单个新词时，机器翻译中的模型性能几乎减半。受到这些结果的启发，我们构建了一个基准来评估LLMs对不同自然语言理解任务和模型困惑度中新词的泛化能力。后期知识截止日期的模型产生较低的困惑度，并在下游任务中表现更好。

    arXiv:2402.12261v1 Announce Type: new  Abstract: The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks
    
[^137]: MatPlotAgent: 基于LLM的Agent科学数据可视化方法与评估

    MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization

    [https://arxiv.org/abs/2402.11453](https://arxiv.org/abs/2402.11453)

    MatPlotAgent介绍了一种基于LLM的自动化科学数据可视化框架，包括查询理解、代码生成和视觉反馈机制，并引入了MatPlotBench基准和GPT-4V评分方法。

    

    科学数据可视化通过直接展示复杂信息并帮助研究人员识别隐含模式，在研究中起着至关重要的作用。尽管其重要性，但对于使用Large Language Models（LLMs）进行科学数据可视化的研究仍较为未被探索。在这项研究中，我们介绍了MatPlotAgent，一种高效、与模型无关的LLM代理框架，旨在自动化科学数据可视化任务。MatPlotAgent利用代码LLMs和多模态LLMs的能力，由三个核心模块组成：查询理解、带有迭代调试的代码生成，以及用于错误更正的视觉反馈机制。为了解决该领域缺乏基准的问题，我们提出了MatPlotBench，一个由100个经人工验证的测试案例组成的高质量基准。此外，我们介绍了一种利用GPT-4V进行自动评估的评分方法。实验结果表明…（未完整）

    arXiv:2402.11453v1 Announce Type: new  Abstract: Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules: query understanding, code generation with iterative debugging, and a visual feedback mechanism for error correction. To address the lack of benchmarks in this field, we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V for automatic evaluation. Experimental results demonstrate t
    
[^138]: 我们能否用软提示LLMs来进行图学习任务？

    Can we soft prompt LLMs for graph learning tasks?

    [https://arxiv.org/abs/2402.10359](https://arxiv.org/abs/2402.10359)

    引入了GraphPrompter框架，通过软提示将图信息与LLMs对齐，以进一步探究LLMs理解图信息的潜力。

    

    图在表示社交网络、生物数据和引用网络等现实世界应用中的复杂关系方面起着重要作用。最近，大型语言模型（LLMs）在各个领域取得了巨大成功，这使得将LLMs应用于图表格尤为诱人。然而，直接将LLMs应用于图表格形式存在独特挑战，因为图表格形式与文本形式之间存在差异和不匹配。因此，为了进一步探究LLMs理解图信息的潜力，我们引入了GraphPrompter，这是一个通过软提示来将图信息与LLMs对齐的新颖框架。具体而言，GraphPrompter包括两个主要组件：一个图神经网络用于编码复杂的图信息，以及一个能够有效处理文本信息的LLM。在不同基准数据集上进行了广泛实验，涵盖了节点分类和链接预测任务。

    arXiv:2402.10359v1 Announce Type: cross  Abstract: Graph plays an important role in representing complex relationships in real-world applications such as social networks, biological data and citation networks. In recent years, Large Language Models (LLMs) have achieved tremendous success in various domains, which makes applying LLMs to graphs particularly appealing. However, directly applying LLMs to graph modalities presents unique challenges due to the discrepancy and mismatch between the graph and text modalities. Hence, to further investigate LLMs' potential for comprehending graph information, we introduce GraphPrompter, a novel framework designed to align graph information with LLMs via soft prompts. Specifically, GraphPrompter consists of two main components: a graph neural network to encode complex graph information and an LLM that effectively processes textual information. Comprehensive experiments on various benchmark datasets under node classification and link prediction tas
    
[^139]: 通过反向课程强化学习训练大型语言模型进行推理

    Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning

    [https://arxiv.org/abs/2402.05808](https://arxiv.org/abs/2402.05808)

    本文提出了一种通过反向课程强化学习训练大型语言模型进行推理的新方法，通过学习正确演示并建立逐步的课程，实现了结果监督和过程监督的优化。

    

    在本文中，我们提出了R$^3$：通过反向课程强化学习（RL）进行推理的学习方法，该方法只使用结果监督来实现大型语言模型的过程监督的好处。将RL应用于复杂推理的核心挑战是确定一系列行动，以获得正向奖励并提供适当的优化监督。结果监督为最终结果提供了稀疏奖励，而不识别错误位置，而过程监督提供了逐步奖励，但需要大量手动注释。R$^3$通过学习正确演示来克服这些限制。具体而言，R$^3$将推理的起始状态从演示的结束滑动到开始，从而在所有阶段都促进了更容易的模型探索。因此，R$^3$建立了一个逐步的课程，使结果监督能够提供阶段级信号并精确定位错误。

    In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7
    
[^140]: 有效检索增强生成的财务报告切块

    Financial Report Chunking for Effective Retrieval Augmented Generation

    [https://arxiv.org/abs/2402.05131](https://arxiv.org/abs/2402.05131)

    本文提出了一种扩展的方法来切块财务报告，通过根据文档的结构元素组件进行切块，从而实现更有效的检索增强生成。这种方法可以优化切块大小，而无需调整，并提供了对整体上下文和准确性的评估以及对问答任务性能的影响。

    

    切块信息是检索增强生成(RAG)的关键步骤。目前的研究主要集中在段落级切块上。这种方法将所有文本都视为平等的，并忽略了文档结构中包含的信息。我们提出了一种扩展的方法，通过不仅仅将文档切块到段落级别，而是根据文档的结构元素组件来切块。将文档分解为这些组成元素可以创建一种新的文档切块方式，可以得到最佳的切块大小，无需调整。我们引入了一种新颖的框架，评估根据由文档理解模型注释的元素类型进行切块如何对所检索信息的整体上下文和准确性贡献。我们还演示了这种方法对RAG辅助问答任务性能的影响。我们的研究包括对各种元素类型的全面分析，它们在有效信息检索中的作用以及它们对其产生的影响。

    Chunking information is a key step in Retrieval Augmented Generation (RAG). Current research primarily centers on paragraph-level chunking. This approach treats all texts as equal and neglects the information contained in the structure of documents. We propose an expanded approach to chunk documents by moving beyond mere paragraph-level chunking to chunk primary by structural element components of documents. Dissecting documents into these constituent elements creates a new way to chunk documents that yields the best chunk size without tuning. We introduce a novel framework that evaluates how chunking based on element types annotated by document understanding models contributes to the overall context and accuracy of the information retrieved. We also demonstrate how this approach impacts RAG assisted Question & Answer task performance. Our research includes a comprehensive analysis of various element types, their role in effective information retrieval, and the impact they have on the 
    
[^141]: 机器学习与符号方法的协同作用：自然语言处理中混合方法的调查

    Synergizing Machine Learning & Symbolic Methods: A Survey on Hybrid Approaches to Natural Language Processing

    [https://arxiv.org/abs/2401.11972](https://arxiv.org/abs/2401.11972)

    本文调查了机器学习与符号方法在自然语言处理中的混合方法，以桥接二者的优势和劣势，提出了用于广泛NLP任务的最新混合方法。

    

    机器学习和符号方法的进步凸显了它们在自然语言处理（NLP）中的优势和劣势。机器学习方法强大地在识别数据中的模式，但往往在学习NLP任务所需的常识和事实知识方面表现不佳。与此同时，符号方法擅长表示知识丰富的数据。然而，它们在适应动态数据和概括知识方面存在困难。通过混合方法桥接这两种范式可以减轻它们各自的弱点，同时保持它们的优势。最近的研究赞美了这种结合的优点，在广泛的NLP任务中展示出有希望的结果。在本文中，我们介绍了用于NLP的混合方法的概况。具体来说，我们深入探讨了用于需要自然语言理解和生成的一系列NLP任务的最新混合方法。

    arXiv:2401.11972v2 Announce Type: replace  Abstract: The advancement of machine learning and symbolic approaches have underscored their strengths and weaknesses in Natural Language Processing (NLP). While machine learning approaches are powerful in identifying patterns in data, they often fall short in learning commonsense and the factual knowledge required for the NLP tasks. Meanwhile, the symbolic methods excel in representing knowledge-rich data. However, they struggle to adapt dynamic data and generalize the knowledge. Bridging these two paradigms through hybrid approaches enables the alleviation of weaknesses in both while preserving their strengths. Recent studies extol the virtues of this union, showcasing promising results in a wide range of NLP tasks. In this paper, we present an overview of hybrid approaches used for NLP. Specifically, we delve into the state-of-the-art hybrid approaches used for a broad spectrum of NLP tasks requiring natural language understanding, generati
    
[^142]: CMMMU：一个中国大规模多学科多模态理解基准

    CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark

    [https://arxiv.org/abs/2401.11944](https://arxiv.org/abs/2401.11944)

    CMMMU是一个旨在评估大型多模型模型在大学级学科知识和深思熟虑推理任务中表现的中文大规模多学科多模态理解基准，为填补在非英语环境中评估先进知识和推理能力的空白而设计。

    

    随着大型多模型模型(LMMs)的能力不断提升，评估LMMs的表现日益成为一个迫切的需求。此外，在评估LMMs在中文等非英语环境中先进知识和推理能力方面存在更大差距。我们引入了CMMMU，一个新的中文大规模多学科多模态理解基准，旨在评估LMMs在需要大学水平学科知识和深思熟虑推理的任务中的表现。CMMMU受到了MMMUs的标注和分析模式的启发并严格遵循。CMMMU包括来自大学考试、测验和教科书的1.2万个手动收集的多模态问题，涵盖六个核心学科：艺术与设计、商业、科学、健康与医学、人文社科以及技术与工程，就像其伙伴MMMMU一样。这些问题涵盖30个学科，包括39个高度异质的图像。

    arXiv:2401.11944v2 Announce Type: replace-cross  Abstract: As the capabilities of large multimodal models (LMMs) continue to advance, evaluating the performance of LMMs emerges as an increasing need. Additionally, there is an even larger gap in evaluating the advanced knowledge and reasoning abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU, a new Chinese Massive Multi-discipline Multimodal Understanding benchmark designed to evaluate LMMs on tasks demanding college-level subject knowledge and deliberate reasoning in a Chinese context. CMMMU is inspired by and strictly follows the annotation and analysis pattern of MMMU.   CMMMU includes 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering, like its companion, MMMU. These questions span 30 subjects and comprise 39 highly heterogeneous image 
    
[^143]: 多数还是少数：用于命名实体识别的数据不平衡学习方法

    Majority or Minority: Data Imbalance Learning Method for Named Entity Recognition

    [https://arxiv.org/abs/2401.11431](https://arxiv.org/abs/2401.11431)

    提出了一种名为多数还是少数（MoM）学习的数据不平衡学习方法，针对命名实体识别任务中的多数类别和少数类别之间的挑战，能够提高少数类别的预测性能，而不影响多数类别的性能。

    

    数据不平衡在各种机器学习（ML）任务中是一个重要挑战，特别是在自然语言处理（NLP）中的命名实体识别（NER）任务。NER表现出一种长尾分布的数据不平衡，其中有许多少数类别（即实体类别）和一个单一的多数类别（即O类别）。这种不平衡导致将实体类别误分类为O类别。为了解决这个问题，我们提出了一种简单且有效的学习方法，命名为多数还是少数（MoM）学习。MoM学习将只有地面事实为多数类别的样本所计算的损失融入到传统ML模型的损失中。对四个NER数据集（日语和英语）的评估实验表明，MoM学习提高了少数类别的预测性能，同时不牺牲多数类别的性能，并且比广为人知的最新技术更有效。

    arXiv:2401.11431v2 Announce Type: replace  Abstract: Data imbalance presents a significant challenge in various machine learning (ML) tasks, particularly named entity recognition (NER) within natural language processing (NLP). NER exhibits a data imbalance with a long-tail distribution, featuring numerous minority classes (i.e., entity classes) and a single majority class (i.e., O-class). This imbalance leads to misclassifications of the entity classes as the O-class. To tackle this issue, we propose a simple and effective learning method named majority or minority (MoM) learning. MoM learning incorporates the loss computed only for samples whose ground truth is the majority class into the loss of the conventional ML model. Evaluation experiments on four NER datasets (Japanese and English) showed that MoM learning improves prediction performance of the minority classes without sacrificing the performance of the majority class and is more effective than widely known and state-of-the-art
    
[^144]: GRAM:全局推理用于多页面视觉问答

    GRAM: Global Reasoning for Multi-Page VQA

    [https://arxiv.org/abs/2401.03411](https://arxiv.org/abs/2401.03411)

    GRAM方法通过引入文档级的指定层和可学习标记，实现了将单页模型扩展到多页面设置，促进信息跨页面的全局推理。

    

    随着基于Transformer的大型语言模型的不断增加，处理长序列的挑战越发突出。在文档视觉问答（DocVQA）中，主流方法专注于单页设置，而文档可能跨越数百页。我们提出了GRAM，一种能够无缝将经过预训练的单页模型扩展到多页面设置的方法，而无需进行计算密集的预训练。为此，我们利用单页编码器进行局部页面级理解，并利用文档级指定层和可学习标记来增强它，促进信息在页面间的全局推理传递。为了确保我们的模型使用新引入的文档标记，我们提出了一种定制的偏置适应方法。在解码过程中为额外的计算节省，我们引入了一个可选的压缩阶段，使用我们的压缩Transformer（C-Former），减少了编码序列的长度。

    arXiv:2401.03411v2 Announce Type: replace  Abstract: The increasing use of transformer-based large language models brings forward the challenge of processing long sequences. In document visual question answering (DocVQA), leading methods focus on the single-page setting, while documents can span hundreds of pages. We present GRAM, a method that seamlessly extends pre-trained single-page models to the multi-page setting, without requiring computationally-heavy pretraining. To do so, we leverage a single-page encoder for local page-level understanding, and enhance it with document-level designated layers and learnable tokens, facilitating the flow of information across pages for global reasoning. To enforce our model to utilize the newly introduced document tokens, we propose a tailored bias adaptation method. For additional computational savings during decoding, we introduce an optional compression stage using our compression-transformer (C-Former),reducing the encoded sequence length, 
    
[^145]: 西班牙语问答中的语言模型知识蒸馏

    Language Model Knowledge Distillation for Efficient Question Answering in Spanish

    [https://arxiv.org/abs/2312.04193](https://arxiv.org/abs/2312.04193)

    通过知识蒸馏，我们开发了SpanishTinyRoBERTa，一个基于RoBERTa的西班牙语压缩语言模型，用于提高西班牙语问答的效率。

    

    最近发展的预训练西班牙语言模型的进展在许多自然语言处理(NLP)任务中取得了重大进展，如问答。然而，缺乏高效模型对这些模型在资源受限环境中的采用构成了一道障碍。因此，针对西班牙语的较小的蒸馏模型可能被证明是高度可扩展的，并促进它们在各种任务和场景中的进一步采用。在这项工作中，我们朝着这个方向迈出了一步，通过开发基于RoBERTa的西班牙语高效问答压缩语言模型SpanishTinyRoBERTa。为了实现这一目标，我们从一个大模型向一个更轻的模型进行知识蒸馏，这使得更广泛的实现成为可能，即使在计算资源有限的地区，也能实现可忽略的性能牺牲。我们的实验表明，这种密集的蒸馏模型能够实现限制的计算性能兼容。

    arXiv:2312.04193v2 Announce Type: replace  Abstract: Recent advances in the development of pre-trained Spanish language models has led to significant progress in many Natural Language Processing (NLP) tasks, such as question answering. However, the lack of efficient models imposes a barrier for the adoption of such models in resource-constrained environments. Therefore, smaller distilled models for the Spanish language could be proven to be highly scalable and facilitate their further adoption on a variety of tasks and scenarios. In this work, we take one step in this direction by developing SpanishTinyRoBERTa, a compressed language model based on RoBERTa for efficient question answering in Spanish. To achieve this, we employ knowledge distillation from a large model onto a lighter model that allows for a wider implementation, even in areas with limited computational resources, whilst attaining negligible performance sacrifice. Our experiments show that the dense distilled model can st
    
[^146]: DP-OPT：使大型语言模型成为您的隐私保护提示工程师

    DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer

    [https://arxiv.org/abs/2312.03724](https://arxiv.org/abs/2312.03724)

    提出了差分私密离线提示调整（DP-OPT）解决大型语言模型调整提示时的隐私问题，通过在客户端调整提示并应用于云模型实现了隐私保护和数据传输

    

    大型语言模型（LLMs）已成为各种任务的主要工具，尤其是通过提示调整针对特定目标时。然而，由于调整的提示依赖于敏感的私人信息，围绕数据隐私的担忧提出了障碍。一个实际的解决方案是托管一个本地的LLM，并使用数据私下优化一个软提示。然而，当模型所有权受到保护时，托管一个本地模型就变得有问题。将数据发送给模型提供程序进行培训等替代方法加剧了这些隐私问题，面对一个不受信任的提供者。在本文中，我们提出了一种名为差分私密离线提示调整（DP-OPT）的新颖解决方案以解决这一挑战。我们的方法涉及在客户端调整离散提示，然后将其应用于所需的云模型。我们演示了LLMs本身建议的提示可以在不暴露p的情况下进行传递

    arXiv:2312.03724v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have emerged as dominant tools for various tasks, particularly when tailored for a specific target by prompt tuning. Nevertheless, concerns surrounding data privacy present obstacles due to the tuned prompts' dependency on sensitive private information. A practical solution is to host a local LLM and optimize a soft prompt privately using data. Yet, hosting a local model becomes problematic when model ownership is protected. Alternative methods, like sending data to the model's provider for training, intensify these privacy issues facing an untrusted provider. In this paper, we present a novel solution called Differentially-Private Offsite Prompt Tuning (DP-OPT) to address this challenge. Our approach involves tuning a discrete prompt on the client side and then applying it to the desired cloud models. We demonstrate that prompts suggested by LLMs themselves can be transferred without compromising p
    
[^147]: 探索、选择、推导和回忆：为移动任务自动化增加类人记忆的LLM

    Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation

    [https://arxiv.org/abs/2312.03003](https://arxiv.org/abs/2312.03003)

    MobileGPT是一种创新的基于LLM的移动任务自动化工具，通过类人应用记忆模拟人类与移动应用的认知过程，实现任务程序的精确高效学习。

    

    大型语言模型（LLMs）的出现为移动任务自动化领域带来了新的机遇。它们优越的语言理解和推理能力使用户能够自动执行复杂和重复的任务。然而，由于LLMs固有的不可靠性和高运行成本，它们的实际适用性相当有限。为解决这些问题，本文引入了MobileGPT，这是一种创新的基于LLM的移动任务自动化工具，配备了类人应用记忆。MobileGPT模拟了人类与移动应用交互的认知过程--探索、选择、推导和回忆。这种方法通过将任务程序分解为更小、模块化的子任务，允许更精确、高效地学习任务流程，从而实现子任务的重复使用、重新排列和适应各种目标。我们使用在线LLM服务（GPT-3.5和GPT-4）实现了MobileGPT，并在一组数据上评估了其性能。

    arXiv:2312.03003v2 Announce Type: replace-cross  Abstract: The advent of large language models (LLMs) has opened up new opportunities in the field of mobile task automation. Their superior language understanding and reasoning capabilities allow users to automate complex and repetitive tasks. However, due to the inherent unreliability and high operational cost of LLMs, their practical applicability is quite limited. To address these issues, this paper introduces MobileGPT, an innovative LLM-based mobile task automator equipped with a human-like app memory. MobileGPT emulates the cognitive process of humans interacting with a mobile app -- explore, select, derive, and recall. This approach allows for a more precise and efficient learning of a task's procedure by breaking it down into smaller, modular sub-tasks that can be re-used, re-arranged, and adapted for various objectives. We implement MobileGPT using online LLMs services (GPT-3.5 and GPT-4) and evaluate its performance on a datase
    
[^148]: 通过基于良知的对准框架抵御对抗性攻击

    Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework

    [https://arxiv.org/abs/2312.00029](https://arxiv.org/abs/2312.00029)

    Bergeron提出了一个基于良知的对齆框架，能够提高大型语言模型对抗攻击的鲁棒性，无需额外参数微调。

    

    近年来，随着越来越强大的大型语言模型（LLMs）的引入，人工智能对齐的研究取得了可观的进展。不幸的是，现代对齐方法仍然无法完全防止在模型被蓄意攻击时产生有害应对。为了帮助缓解这一问题，我们引入了Bergeron：一个旨在提高LLMs对抗攻击鲁棒性的框架，无需进行额外的参数微调。Bergeron分为两个层次；次要LLM模拟受保护的主要LLM的良知。该框架在监视输出以检测任何有害内容的同时，更好地保护主要模型免受入侵攻击。实证分析表明，使用Bergeron来补充现有对齐训练的模型

    arXiv:2312.00029v2 Announce Type: replace-cross  Abstract: Research into AI alignment has grown considerably since the recent introduction of increasingly capable Large Language Models (LLMs). Unfortunately, modern methods of alignment still fail to fully prevent harmful responses when models are deliberately attacked. These attacks can trick seemingly aligned models into giving manufacturing instructions for dangerous materials, inciting violence, or recommending other immoral acts. To help mitigate this issue, we introduce Bergeron: a framework designed to improve the robustness of LLMs against attacks without any additional parameter fine-tuning. Bergeron is organized into two tiers; with a secondary LLM emulating the conscience of a protected, primary LLM. This framework better safeguards the primary model against incoming attacks while monitoring its output for any harmful content. Empirical analysis shows that, by using Bergeron to complement models with existing alignment traini
    
[^149]: 利用大型语言模型进行零-shot命名实体识别的自我改进

    Self-Improving for Zero-Shot Named Entity Recognition with Large Language

    [https://arxiv.org/abs/2311.08921](https://arxiv.org/abs/2311.08921)

    本研究提出了一个无需训练的自我改进框架，利用未标记语料库激发大型语言模型的自我学习能力，从而推动零-shot命名实体识别性能边界。

    

    最近，探索将强大的大型语言模型(LLMs)应用于命名实体识别(NER)任务引起了很大关注。本研究通过提出一个无需训练的自我改进框架，利用未标记语料库来激发LLMs的自我学习能力，从而推动LLMs在零-shot NER上的性能边界。首先，我们使用LLMs对未标注语料库进行自一致性预测，并获得自我注释数据集。其次，我们探索各种策略来选择可靠的注释，形成一个可靠的自我注释数据集。最后，对于每个测试输入，我们从可靠的自我注释数据集中检索演示，并通过上下文学习进行推断。在四个基准测试上的实验表明，我们的框架取得了显著的性能提升。通过全面的实验分析，我们发现增加未标记语料库的规模或迭代次数

    arXiv:2311.08921v2 Announce Type: replace  Abstract: Exploring the application of powerful large language models (LLMs) on the named entity recognition (NER) task has drawn much attention recently. This work pushes the performance boundary of zero-shot NER with LLMs by proposing a training-free self-improving framework, which utilizes an unlabeled corpus to stimulate the self-learning ability of LLMs. First, we use the LLM to make predictions on the unlabeled corpus using self-consistency and obtain a self-annotated dataset. Second, we explore various strategies to select reliable annotations to form a reliable self-annotated dataset. Finally, for each test input, we retrieve demonstrations from the reliable self-annotated dataset and perform inference via in-context learning. Experiments on four benchmarks show substantial performance improvements achieved by our framework. Through comprehensive experimental analysis, we find that increasing the size of unlabeled corpus or iterations 
    
[^150]: 知识图谱是否可以减少LLMs中的幻觉？：一项调查

    Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey

    [https://arxiv.org/abs/2311.07914](https://arxiv.org/abs/2311.07914)

    调查综合审查了LLMs中基于知识图谱的增强技术，重点关注其在减轻幻觉方面的效果。

    

    当代LLMs很容易产生幻觉，主要源于模型内的知识空缺。为了解决这一关键限制，研究人员采用各种策略通过整合外部知识来增强LLMs，旨在减少幻觉并提升推理准确性。在这些策略中，利用知识图谱作为外部信息源已经显示出了良好的效果。在这项调查中，我们全面审查了LLMs中基于知识图谱的增强技术，重点关注它们在减轻幻觉方面的效果。我们将这些方法系统地归类为三个总体组，提供方法比较和性能评估。最后，本调查探讨了与这些技术相关的当前趋势和挑战，并概述了这一新兴领域未来研究的潜在方向。

    arXiv:2311.07914v2 Announce Type: replace  Abstract: The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy. Among these strategies, leveraging knowledge graphs as a source of external information has demonstrated promising results. In this survey, we comprehensively review these knowledge-graph-based augmentation techniques in LLMs, focusing on their efficacy in mitigating hallucinations. We systematically categorize these methods into three overarching groups, offering methodological comparisons and performance evaluations. Lastly, this survey explores the current trends and challenges associated with these techniques and outlines potential avenues for future research in this emerging field.
    
[^151]: HallusionBench：一种用于评估大型视觉语言模型中纠缠的语言幻觉和视幻觉的高级诊断套件

    HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models

    [https://arxiv.org/abs/2310.14566](https://arxiv.org/abs/2310.14566)

    HallusionBench是一个专为评估大型视觉语言模型在图像背景推理中面临挑战的基准，通过引入新颖结构和量化分析，显示出GPT-4V取得了31.42%的准确率，远高于其他模型。

    

    我们介绍了HallusionBench，这是一个专为评估图像背景推理而设计的全面基准。这个基准对于高级大型视觉语言模型（LVLMs）（如GPT-4V（Vision）、Gemini Pro Vision和LLaVA-1.5）提出了重大挑战，强调对视觉数据的微妙理解和解释。该基准包含346张图像和1129个问题，全部由人类专家精心设计。我们为这些视觉问题引入了一种新颖的结构，旨在建立对照组。这种结构使我们能够对模型的响应倾向、逻辑一致性和各种故障模式进行定量分析。在我们对HallusionBench的评估中，我们对14种不同模型进行了基准测试，突出了目前最先进的GPT-4V取得的31.42％的问题对准确率。值得注意的是，所有其他评估模型的准确率均低于16％。

    arXiv:2310.14566v3 Announce Type: replace-cross  Abstract: We introduce HallusionBench, a comprehensive benchmark designed for the evaluation of image-context reasoning. This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(Vision), Gemini Pro Vision, and LLaVA-1.5, by emphasizing nuanced understanding and interpretation of visual data. The benchmark comprises 346 images paired with 1129 questions, all meticulously crafted by human experts. We introduce a novel structure for these visual questions designed to establish control groups. This structure enables us to conduct a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes. In our evaluation on HallusionBench, we benchmarked 14 different models, highlighting a 31.42% question-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all other evaluated models achieve accuracy below 16%. Moreover, our analysis not only h
    
[^152]: Llemma: 一种用于数学的开放语言模型

    Llemma: An Open Language Model For Mathematics

    [https://arxiv.org/abs/2310.10631](https://arxiv.org/abs/2310.10631)

    Llemma是一个用于数学的开放语言模型，在MATH基准测试中表现优异，能够进行工具使用和形式定理证明，无需进一步微调。

    

    我们提出了Llemma，一个用于数学的大型语言模型。我们继续在Proof-Pile-2上对Code Llama进行预训练，Proof-Pile-2包含科学论文、包含数学内容的网络数据和数学代码，最终生成了Llemma。在MATH基准测试中，Llemma在同等参数基础上胜过所有已知的开源基准模型，以及尚未发布的Minerva模型套件。此外，Llemma能够进行工具使用和形式定理证明，而无需进一步微调。我们公开发布所有工件，包括70亿和340亿参数模型、Proof-Pile-2以及用于复制我们实验的代码。

    arXiv:2310.10631v3 Announce Type: replace-cross  Abstract: We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.
    
[^153]: Toolink: 链接工具包创建和使用的链式解决开源模型

    Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model

    [https://arxiv.org/abs/2310.05155](https://arxiv.org/abs/2310.05155)

    提出了Toolink，一个通过链式解决方法首先创建工具包，再集成工具规划和调用的框架，成功通过对ChatGPT和CoS-GPT的实验，打造了LLaMA-CoS，一个具有先进工具规划和调用能力的强大开源模型。

    

    大语言模型（LLMs）在利用工具方面取得了显著进展，但其闭源性和高推理成本对其适应性造成了限制，需要一种有效的方法，利用较小的开源模型。在本文中，我们介绍了Toolink，这是一个全面的框架，通过链式解决（CoS）方法首先创建工具包，然后集成工具的规划和调用。我们首先验证了Toolink在利用模型的创造力和CoS能力方面的有效性。之后，我们策划了CoS-GPT，一个专为工具使用而设计的链式解决数据集，并对LLaMA-7B模型进行了微调。结果是LLaMA-CoS，一个具有先进工具规划和工具调用能力的强大开源模型。通过对BIG-bench的多样任务进行评估，表明其CoS能力与ChatGPT相匹配，而性能超过了其

    arXiv:2310.05155v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in utilizing tools, but their closed-source nature and high inference costs pose limitations on their adaptability, necessitating a valid method that leverages smaller, open-sourced models. In this paper, we introduce Toolink, a comprehensive framework that performs task-solving by first creating a toolkit and then integrating the planning and calling of tools through a chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in harnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities. Evaluation of diverse tasks from BIG-bench demonstrates its CoS ability matches that of ChatGPT while its performance surpasses th
    
[^154]: 压缩LLM：真相很少纯粹，也绝不简单

    Compressing LLMs: The Truth is Rarely Pure and Never Simple

    [https://arxiv.org/abs/2310.01382](https://arxiv.org/abs/2310.01382)

    本研究重新评估了现有最先进的压缩LLM方法对稠密LLM的有效性，并引入了一个新的压缩LLM基准来重新定义评估协议。

    

    尽管现代大型语言模型（LLMs）取得了显著成就，但面临着巨大的计算和内存占用。 最近，几项工作显示出在无需训练和数据的情况下对LLMs进行压缩（修剪和量化）取得了显著成功，达到了50-60％的稀疏度，并将位宽减小到每个权重3或4位，并且与未压缩基线相比，困惑度的降低可以忽略不计。 随着最近研究工作集中在开发越来越复杂的压缩方法上，我们的工作退一步重新评估了现有SoTA压缩方法的有效性，这些方法依赖于一种相当简单且广受质疑的度量标准，困惑度（即使对于稠密的LLMs）。 我们引入了知识密集型压缩的LLM基准（LLM-KICK），这是一个精心策划的任务集合，用于重新定义对压缩LLMs的评估协议，这些LLMs与其稠密对应物有显著的对齐性，

    arXiv:2310.01382v2 Announce Type: replace  Abstract: Despite their remarkable achievements, modern Large Language Models (LLMs) face exorbitant computational and memory footprints. Recently, several works have shown significant success in training-free and data-free compression (pruning and quantization) of LLMs that achieve 50 - 60% sparsity and reduce the bit width to 3 or 4 bits per weight, with negligible degradation of perplexity over the uncompressed baseline. As recent research efforts are focused on developing increasingly sophisticated compression methods, our work takes a step back and re-evaluates the effectiveness of existing SoTA compression methods, which rely on a fairly simple and widely questioned metric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), a collection of carefully curated tasks to redefine the evaluation protocol for compressed LLMs, which have significant alignment with their dense counterparts and 
    
[^155]: 分析和减轻大型视觉语言模型中的物体幻觉

    Analyzing and Mitigating Object Hallucination in Large Vision-Language Models

    [https://arxiv.org/abs/2310.00754](https://arxiv.org/abs/2310.00754)

    提出了一种名为LVLM Hallucination Revisor（LURE）的算法，通过重新构建较少具有幻觉性的描述，来事后纠正大型视觉语言模型中的物体幻觉问题。

    

    大型视觉语言模型（LVLMs）在理解图像信息和人类语言方面显示出卓越的能力。然而，LVLMs 仍然存在物体幻觉问题，即生成包含图像中实际不存在的对象的描述。这可能对许多视觉语言任务产生负面影响，如视觉总结和推理。为解决这一问题，我们提出了一种简单而强大的算法，LVLM 幻觉修正器（LURE），通过重构较少具有幻觉性的描述来事后纠正LVLM 中的物体幻觉。LURE根据对导致物体幻觉的关键因素的严格统计分析，包括共现（图像中某些对象经常与其他对象一起出现）、不确定性（在LVLM解码过程中不确定性较高的对象）和对象位置（幻觉通常出现在生成描述的后部分）。

    arXiv:2310.00754v2 Announce Type: replace-cross  Abstract: Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks, such as visual summarization and reasoning. To address this issue, we propose a simple yet powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions. LURE is grounded in a rigorous statistical analysis of the key factors underlying object hallucination, including co-occurrence (the frequent appearance of certain objects alongside others in images), uncertainty (objects with higher uncertainty during LVLM decoding), and object position (hallucination often appears in the later part of the gen
    
[^156]: 能够检测到LLM生成的虚假信息吗?

    Can LLM-Generated Misinformation Be Detected?

    [https://arxiv.org/abs/2309.13788](https://arxiv.org/abs/2309.13788)

    LLM生成的虚假信息可能比人类撰写的虚假信息更难以检测，具有更具欺骗性的风格，可能造成更多危害。

    

    大型语言模型（LLMs）的出现产生了深远影响。然而，LLMs（如ChatGPT）可能被利用来生成虚假信息，这给在线安全和公众信任带来了严重关切。一个基本的研究问题是：LLM生成的虚假信息是否会比人类撰写的虚假信息造成更大危害?我们提出从检测难度的角度来探讨这个问题。我们首先建立了一个LLM生成的虚假信息分类法。然后，我们对利用LLMs生成虚假信息的潜在真实世界方法进行分类和验证。通过广泛的实证调查，我们发现与具有相同语义的人类撰写的虚假信息相比，LLM生成的虚假信息对人类和检测器来说更难检测，这表明它可能具有更具欺骗性的风格，潜在地造成更多危害。我们还讨论了我们发现的影响。

    arXiv:2309.13788v3 Announce Type: replace-cross  Abstract: The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery
    
[^157]: BAMBOO：用于评估大语言模型长文本建模能力的全面基准

    BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models

    [https://arxiv.org/abs/2309.13345](https://arxiv.org/abs/2309.13345)

    提出了BAMBOO基准来全面评估大语言模型对长文本的建模能力，包含10个数据集从5个不同长文本理解任务中提取，涵盖了LLMs的核心能力和各个领域。

    

    大语言模型（LLMs）已经在处理普通长度的NLP任务中取得了惊人的熟练度。最近，多项研究致力于扩展上下文长度，并增强LLMs的长文本建模能力。为了全面评估LLMs的长上下文能力，我们提出了BAMBOO，一个多任务长上下文基准。BAMBOO设计之初考虑了四个原则：全面容量评估、避免数据污染、准确的自动评估以及不同长度级别。它由来自5个不同长文本理解任务的10个数据集组成，即问答、幻觉检测、文本排序、语言建模和代码补全，以涵盖LLMs的核心能力和各个领域。我们在BAMBOO上使用五个长上下文模型进行实验，并进一步讨论了长文本的四个关键研究问题。我们还对当前的长上下文模型进行了定性分析。

    arXiv:2309.13345v2 Announce Type: replace  Abstract: Large language models (LLMs) have achieved dramatic proficiency over NLP tasks with normal length. Recently, multiple studies have committed to extending the context length and enhancing the long text modeling capabilities of LLMs. To comprehensively evaluate the long context ability of LLMs, we propose BAMBOO, a multi-task long context benchmark. BAMBOO has been designed with four principles: comprehensive capacity evaluation, avoidance of data contamination, accurate automatic evaluation, and different length levels. It consists of 10 datasets from 5 different long text understanding tasks, i.e. question answering, hallucination detection, text sorting, language modeling, and code completion, to cover core capacities and various domains of LLMs. We conduct experiments with five long context models on BAMBOO and further discuss four key research questions of long text. We also qualitatively analyze current long context models and po
    
[^158]: LeBenchmark 2.0：用于自监督法表示法语语音的标准化、可复制和增强框架

    LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for Self-supervised Representations of French Speech

    [https://arxiv.org/abs/2309.05472](https://arxiv.org/abs/2309.05472)

    LeBenchmark 2.0是一个开源框架，用于评估和构建法语语音技术的自监督学习，提供大规模语料库、预训练模型和评估协议，并探讨了预训练SSL模型的独特视角。

    

    自监督学习（SSL）是许多不同领域，包括计算机视觉和自然语言处理等领域取得了前所未有的进展。语音处理极大受益于SSL，因为当前大部分领域相关任务现在都是用预训练模型处理的。本文介绍了LeBenchmark 2.0，这是一个用于评估和构建配备SSL的法语语音技术的开源框架。它包括有文档记录、大规模和异构语料库，涵盖长达14,000小时的异构语音，十个预训练的SSL wav2vec 2.0 模型，包含从2600万到10亿可学习参数与社区共享，并且包含由六个下游任务组成的评估协议，以补充现有基准。LeBenchmark 2.0 还对于语音的预训练SSL模型提出了独特的视角，探讨了冻结与微调下游模型以及任务不可知的相关性。

    arXiv:2309.05472v2 Announce Type: replace-cross  Abstract: Self-supervised learning (SSL) is at the origin of unprecedented improvements in many different domains including computer vision and natural language processing. Speech processing drastically benefitted from SSL as most of the current domain-related tasks are now being approached with pre-trained models. This work introduces LeBenchmark 2.0 an open-source framework for assessing and building SSL-equipped French speech technologies. It includes documented, large-scale and heterogeneous corpora with up to 14,000 hours of heterogeneous speech, ten pre-trained SSL wav2vec 2.0 models containing from 26 million to one billion learnable parameters shared with the community, and an evaluation protocol made of six downstream tasks to complement existing benchmarks. LeBenchmark 2.0 also presents unique perspectives on pre-trained SSL models for speech with the investigation of frozen versus fine-tuned downstream models, task-agnostic ve
    
[^159]: 频率效应在线性判别学习中的应用

    Frequency effects in Linear Discriminative Learning

    [https://arxiv.org/abs/2306.11044](https://arxiv.org/abs/2306.11044)

    本研究展示了如何利用频率信息学习(FIL)获得一种高效、考虑频率的形式和含义之间映射的方法，该方法在计算上更便宜，同时能有效地近似逐步解决方案。

    

    字频在大多数词汇处理任务中都是一个强有力的预测因子。因此，任何一个词汇识别模型都需要解释词频效应是如何产生的。判别性词典模型(DLM;Baayen等，2018a，2019)通过单词形式和含义之间的线性映射对词汇处理进行建模。到目前为止，这些映射可以通过逐步通过误差驱动学习获得，这是一个能够捕捉频率效应的计算昂贵的过程，或者是通过高效但与频率无关的解决方案来建模学习的理论最终状态(EL) 。在本研究中，我们展示了如何获得一种高效但又考虑频率的形式和含义之间映射的方法(频率信息学习;FIL)。我们发现FIL很好地近似了逐步解决方案，同时计算成本要低得多。FIL表现出相对较低的类型精度和高的标记精度，证明了该模型的有效性。

    arXiv:2306.11044v2 Announce Type: replace  Abstract: Word frequency is a strong predictor in most lexical processing tasks. Thus, any model of word recognition needs to account for how word frequency effects arise. The Discriminative Lexicon Model (DLM; Baayen et al., 2018a, 2019) models lexical processing with linear mappings between words' forms and their meanings. So far, the mappings can either be obtained incrementally via error-driven learning, a computationally expensive process able to capture frequency effects, or in an efficient, but frequency-agnostic solution modelling the theoretical endstate of learning (EL) where all words are learned optimally. In this study we show how an efficient, yet frequency-informed mapping between form and meaning can be obtained (Frequency-informed learning; FIL). We find that FIL well approximates an incremental solution while being computationally much cheaper. FIL shows a relatively low type- and high token-accuracy, demonstrating that the m
    
[^160]: 开放式脑AI：自动语言评估

    Open Brain AI. Automatic Language Assessment

    [https://arxiv.org/abs/2306.06693](https://arxiv.org/abs/2306.06693)

    Open Brain AI是一个利用创新AI技术自动分析多语种口头和书面言语的计算平台，可以快速且自动化地分析语言，减轻临床医生的负担。

    

    语言评估在诊断和治疗由神经原因引起的言语、语言和沟通障碍的个体中发挥着至关重要的作用，无论是发育性还是后天性。然而，目前的评估方法是手动的、费力的，并且耗时，给患者带来额外的压力。为了解决这些挑战，我们开发了Open Brain AI（https://openbrainai.com）。这一计算平台利用创新的AI技术，包括机器学习、自然语言处理、大型语言模型和自动语音转文本，来自动分析多语种的口头和书面言语产物。本文讨论了Open Brain AI的开发、AI语言处理模块以及话语宏观结构和微观结构的语言测量。语言的快速自动化分析减轻了临床医生的负担，使他们能够

    arXiv:2306.06693v2 Announce Type: replace  Abstract: Language assessment plays a crucial role in diagnosing and treating individuals with speech, language, and communication disorders caused by neurogenic conditions, whether developmental or acquired. However, current assessment methods are manual, laborious, and time-consuming to administer and score, causing additional patient stress. To address these challenges, we developed Open Brain AI (https://openbrainai.com). This computational platform harnesses innovative AI techniques, namely machine learning, natural language processing, large language models, and automatic speech-to-text transcription, to automatically analyze multilingual spoken and written speech productions. This paper discusses the development of Open Brain AI, the AI language processing modules, and the linguistic measurements of discourse macro-structure and micro-structure. The fast and automatic analysis of language alleviates the burden on clinicians, enabling th
    
[^161]: 一切都已解决了吗？大型语言模型尚未解决的开放性自然语言处理研究问题

    Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models

    [https://arxiv.org/abs/2305.12544](https://arxiv.org/abs/2305.12544)

    本文提出了14个不由大型语言模型直接解决的自然语言处理研究领域，包含45个新的研究方向，为NLP研究人员提供了丰富的探索方向。

    

    近期大型语言模型(LLMs)的进展使得许多生成式自然语言处理应用得以部署。与此同时，这也导致了一个误导性的公众话语，“一切都已解决”。不足为奇的是，这反过来使得许多自然语言处理研究人员，特别是那些刚开始职业生涯的人，担心他们应该专注于哪些研究领域。一切都已解决了吗，或者不论大型语言模型(LLMs)如何，我们可以继续研究哪些问题？为了回答这个问题，本文整理了适合深入探究的自然语言处理研究方向。我们确定了包含45个研究方向的14个不需要大型语言模型直接解决的研究领域。虽然我们确定了许多研究领域，但还有许多其他领域存在；我们未涵盖目前由大型语言模型(LLMs)处理的领域，但在性能上落后或者专注于大型语言模型(LLM)发展的领域。我们欢迎对其他研究领域的建议。

    arXiv:2305.12544v2 Announce Type: replace-cross  Abstract: Recent progress in large language models (LLMs) has enabled the deployment of many generative NLP applications. At the same time, it has also led to a misleading public discourse that ``it's all been solved.'' Not surprisingly, this has, in turn, made many NLP researchers -- especially those at the beginning of their careers -- worry about what NLP research area they should focus on. Has it all been solved, or what remaining questions can we work on regardless of LLMs? To address this question, this paper compiles NLP research directions rich for exploration. We identify fourteen different research areas encompassing 45 research directions that require new research and are not directly solvable by LLMs. While we identify many research areas, many others exist; we do not cover areas currently addressed by LLMs, but where LLMs lag behind in performance or those focused on LLM development. We welcome suggestions for other research
    
[^162]: 用于流式自监督语音表示学习的低延迟注意力模块

    A low latency attention module for streaming self-supervised speech representation learning

    [https://arxiv.org/abs/2302.13451](https://arxiv.org/abs/2302.13451)

    本文提出了用于流式自监督语音表示学习的低延迟注意力模块，实现了在低延迟的情况下进行实时推断。

    

    transformer是深度学习中的基本构建模块，注意力机制是transformer的核心组件。自监督语音表示学习（SSRL）是transformer架构的一个流行用例。由于transformer的非因果行为，对于SSRL的transformer的使用主要集中在非因果应用上。然而，一些媒体处理问题，如语音处理，需要实时解决方案。在本文中，我们提出了一个注意力模块的实现，该模块可以通过较低的计算和内存需求训练SSRL架构，并允许在低固定延迟下进行实时推断。本文提出的注意力模块包括两个组件，分别是流式注意力（SA）和低延迟流式注意力（LLSA）。

    arXiv:2302.13451v2 Announce Type: replace-cross  Abstract: The transformer is a fundamental building block in deep learning, and the attention mechanism is the transformer's core component. Self-supervised speech representation learning (SSRL) represents a popular use-case for the transformer architecture. Due to transformers' acausal behavior, the use of transformers for SSRL has been predominantly focused on acausal applications. However, several media processing problems, such as speech processing, require real-time solutions. In this paper, we present an implementation of the attention module that enables training of SSRL architectures with low compute and memory requirements, while allowing real-time inference with low and fixed latency. The attention module proposed in this paper includes two components, streaming attention (SA) and low-latency streaming attention (LLSA). The SA represents our proposal for an efficient streaming SSRL implementation, while the LLSA solves the late
    
[^163]: 掩码语言建模中的表示不足

    Representation Deficiency in Masked Language Modeling

    [https://arxiv.org/abs/2302.02060](https://arxiv.org/abs/2302.02060)

    掩码语言建模中的 $\texttt{[MASK]} $符号会导致模型维度过度分配，造成真实标记的表示不足，本文提出了MAE-LM来解决这一问题

    

    掩码语言建模（MLM）已经成为双向文本编码器预训练最突出的方法之一，因为它简单而有效。关于MLM的一个显著问题是特殊的 $\texttt{[MASK]}$ 符号会导致预训练数据和下游数据之间存在差异，因为它只出现在预训练中而不出现在微调中。在这项工作中，我们提供了一个新的视角，探讨了这种差异的后果：我们在理论和实践上证明了MLM预训练专门分配了一些模型维度来表示 $\texttt{[MASK]}$ 标记，导致真实标记的表示不足，并在没有 $\texttt{[MASK]}$ 标记的情况下，限制了预训练模型在适应下游数据时的表达能力。受到识别问题的启发，我们提出了MAE-LM，该方法利用MLM对掩码自动编码器进行预训练，其中排除了 $\texttt{[MASK]}$ 标记。

    arXiv:2302.02060v2 Announce Type: replace  Abstract: Masked Language Modeling (MLM) has been one of the most prominent approaches for pretraining bidirectional text encoders due to its simplicity and effectiveness. One notable concern about MLM is that the special $\texttt{[MASK]}$ symbol causes a discrepancy between pretraining data and downstream data as it is present only in pretraining but not in fine-tuning. In this work, we offer a new perspective on the consequence of such a discrepancy: We demonstrate empirically and theoretically that MLM pretraining allocates some model dimensions exclusively for representing $\texttt{[MASK]}$ tokens, resulting in a representation deficiency for real tokens and limiting the pretrained model's expressiveness when it is adapted to downstream data without $\texttt{[MASK]}$ tokens. Motivated by the identified issue, we propose MAE-LM, which pretrains the Masked Autoencoder architecture with MLM where $\texttt{[MASK]}$ tokens are excluded from the
    
[^164]: 面向问题回答的无源领域自适应方法及其蒙版自训练

    Source-Free Domain Adaptation for Question Answering with Masked Self-training

    [https://arxiv.org/abs/2212.09563](https://arxiv.org/abs/2212.09563)

    提出了一种面向问题回答的无源领域自适应方法，通过独特的蒙版模块实现自训练，成功解决了无法访问源域数据的挑战，提升了模型在目标领域上的性能。

    

    大多数先前针对问题回答（QA）的无监督领域自适应（UDA）方法在Fine-tuning目标域模型时需要访问源域数据。然而，源域数据可能包含敏感信息并受到限制。在本研究中，我们研究了更具挑战性的设置，即无源UDA，在此设置中，我们只有预训练的源模型和目标域数据，无法访问源域数据。我们提出了一种新颖的自训练方法来提升QA模型，该方法集成了一个用于领域自适应的独特蒙版模块。该蒙版在训练源域时进行自动调整，以提取关键的领域知识。为了保持先前学习到的领域知识，适应过程中某些蒙版权重被冻结，而其他权重则根据目标域中生成的伪标签样本来调整，以减轻领域偏移。

    arXiv:2212.09563v2 Announce Type: replace  Abstract: Most previous unsupervised domain adaptation (UDA) methods for question answering(QA) require access to source domain data while fine-tuning the model for the target domain. Source domain data may, however, contain sensitive information and may be restricted. In this study, we investigate a more challenging setting, source-free UDA, in which we have only the pretrained source model and target domain data, without access to source domain data. We propose a novel self-training approach to QA models that integrates a unique mask module for domain adaptation. The mask is auto-adjusted to extract key domain knowledge while trained on the source domain. To maintain previously learned domain knowledge, certain mask weights are frozen during adaptation, while other weights are adjusted to mitigate domain shifts with pseudo-labeled samples generated in the target domain. %As part of the self-training process, we generate pseudo-labeled sample
    
[^165]: 可解释自然语言处理的本地解释：一项调查

    Local Interpretations for Explainable Natural Language Processing: A Survey

    [https://arxiv.org/abs/2103.11072](https://arxiv.org/abs/2103.11072)

    本文调查了改善深度神经网络在自然语言处理任务中可解释性的各种方法，特别关注局部解释，包括与相关输入特征相关的预测解释、自然语言解释以及探测模型隐藏状态和词表示。

    

    随着过去十年间深度学习技术在各个领域的应用不断增长，关于黑盒模型不透明性的抱怨也在增加，导致深度学习模型透明度受到更多关注。本研究探讨了改善深度神经网络在自然语言处理（NLP）任务中的可解释性的各种方法，包括机器翻译和情感分析。我们在本研究的开始阶段对可解释性一词及其各个方面进行了全面讨论。本次调查中收集和总结的方法仅涉及局部解释，并具体分为三类：1）通过相关输入特征解释模型的预测；2）通过自然语言解释进行解释；3）探测模型的隐藏状态和单词表示。

    arXiv:2103.11072v3 Announce Type: replace-cross  Abstract: As the use of deep learning techniques has grown across various fields over the past decade, complaints about the opaqueness of the black-box models have increased, resulting in an increased focus on transparency in deep learning models. This work investigates various methods to improve the interpretability of deep neural networks for Natural Language Processing (NLP) tasks, including machine translation and sentiment analysis. We provide a comprehensive discussion on the definition of the term interpretability and its various aspects at the beginning of this work. The methods collected and summarised in this survey are only associated with local interpretation and are specifically divided into three categories: 1) interpreting the model's predictions through related input features; 2) interpreting through natural language explanation; 3) probing the hidden states of models and word representations.
    
[^166]: SpeechDPR: 开放领域口语问答的端到端口语段落检索

    SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering. (arXiv:2401.13463v1 [cs.CL])

    [http://arxiv.org/abs/2401.13463](http://arxiv.org/abs/2401.13463)

    SpeechDPR是第一个用于开放领域口语问答的端到端框架，能够从口语存档中检索可能包含答案的段落。通过融合无监督ASR和文本密集检索器的知识，SpeechDPR能够获得较好的性能，并且在UASR性能较差时表现更加鲁棒。

    

    口语问答(SQA)是机器通过在给定口语段落中找到答案范围来回答用户问题的关键。过去的SQA方法没有使用ASR，以避免识别错误和词汇外问题。然而，实际的开放领域SQA(openSQA)问题中，机器需要首先从口语存档中检索可能包含答案的段落。本文提出了第一个已知的用于openSQA问题检索组件的端到端框架SpeechDPR。SpeechDPR通过从无监督ASR(UASR)和文本密集检索器(TDR)的级联模型中提炼知识，学习句子级语义表示。不需要手动转录的语音数据。初步实验表明，与级联的UASR和TDR模型相比，性能相当，并且在UASR性能较差时显著提高，验证了这种方法更加鲁棒。

    Spoken Question Answering (SQA) is essential for machines to reply to user's question by finding the answer span within a given spoken passage. SQA has been previously achieved without ASR to avoid recognition errors and Out-of-Vocabulary (OOV) problems. However, the real-world problem of Open-domain SQA (openSQA), in which the machine needs to first retrieve passages that possibly contain the answer from a spoken archive in addition, was never considered. This paper proposes the first known end-to-end framework, Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the openSQA problem. SpeechDPR learns a sentence-level semantic representation by distilling knowledge from the cascading model of unsupervised ASR (UASR) and text dense retriever (TDR). No manually transcribed speech data is needed. Initial experiments showed performance comparable to the cascading model of UASR and TDR, and significantly better when UASR was poor, verifying this approach is more robus
    
[^167]: 通过人的反馈改善机器翻译: 将质量估计作为奖励模型的探索

    Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model. (arXiv:2401.12873v1 [cs.CL])

    [http://arxiv.org/abs/2401.12873](http://arxiv.org/abs/2401.12873)

    本研究探索了利用质量估计作为奖励模型来预测人类偏好以改善机器翻译的潜力。我们发现基于质量估计的反馈训练存在过度优化问题，采用启发式规则来检测错误翻译并对质量估计模型进行惩罚以解决该问题。

    

    不充分建模人类偏好导致奖励模型在利用人的反馈提高翻译质量方面成为一个主要障碍。幸运的是，质量估计(QE)在过去两年中无需参考文献就能准确预测给定翻译的质量。在这项工作中，我们探讨了将QE模型作为奖励模型(基于QE的奖励模型)来预测人的偏好以进行反馈训练的潜力。我们首先发现了在基于QE的反馈训练中的过度优化问题，表现为奖励的增加而翻译质量下降。我们研究了这个问题，并认为QE模型的脆弱性可能导致错误翻译的高奖励，从而导致过度优化和错误传播。为解决这个问题，我们采用了一种简单而有效的方法，使用启发式规则检测错误翻译，并为QE模型添加了一个惩罚项。

    Insufficient modeling of human preferences within the reward model is a major obstacle for leveraging human feedback to improve translation quality. Fortunately, quality estimation (QE), which predicts the quality of a given translation without reference, has achieved impressive alignment with human evaluations in the last two years. In this work, we investigate the potential of employing the QE model as the reward model (the QE-based reward model) to predict human preferences for feedback training. We first identify the overoptimization problem during QE-based feedback training, manifested as an increase in reward while translation quality declines. We examine the problem and argue that the vulnerability of the QE model might lead to high rewards for incorrect translations, resulting in overoptimization and error propagation. To address the problem, we adopt a simple yet effective method that uses heuristic rules to detect the incorrect translations and assigns a penalty term to the Q
    
[^168]: 自动事实核查的索赔检测：关于单语、多语和跨语言研究的综述

    Claim Detection for Automated Fact-checking: A Survey on Monolingual, Multilingual and Cross-Lingual Research. (arXiv:2401.11969v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.11969](http://arxiv.org/abs/2401.11969)

    本综述调研了自动事实核查中索赔检测的现有工作，特别关注多语言数据和方法。这是一个具有挑战性但富有成果的研究方向，需要更通用的解决方案来对抗跨多语言和模态的不实信息。

    

    自动事实核查在过去几十年中引起了相当大的关注，原因是网络平台上虚假信息的传播增加了。这通常是作为一系列任务的序列来完成的，包括（i）检测在网上流传的句子，这些句子构成需要验证的索赔，然后是（ii）对这些索赔进行验证的过程。本综述重点讨论前者，讨论了现有的努力，旨在检测需要事实核查的索赔，特别关注多语言数据和方法。这是一个具有挑战性和富有成果的方向，由于问题的极其具有挑战性，在人类表现方面，现有方法离匹配人类表现还有很长的路要走。特别是，跨多个社交平台的信息传播以多种语言和模态表达，需要更加通用的解决方案来对抗不实信息。我们针对多语言不实信息，提供了一份全面的综述。

    Automated fact-checking has drawn considerable attention over the past few decades due to the increase in the diffusion of misinformation on online platforms. This is often carried out as a sequence of tasks comprising (i) the detection of sentences circulating in online platforms which constitute claims needing verification, followed by (ii) the verification process of those claims. This survey focuses on the former, by discussing existing efforts towards detecting claims needing fact-checking, with a particular focus on multilingual data and methods. This is a challenging and fertile direction where existing methods are yet far from matching human performance due to the profoundly challenging nature of the issue. Especially, the dissemination of information across multiple social platforms, articulated in multiple languages and modalities demands more generalized solutions for combating misinformation. Focusing on multilingual misinformation, we present a comprehensive survey of exis
    
[^169]: TrustLLM: 大型语言模型中的可信性

    TrustLLM: Trustworthiness in Large Language Models. (arXiv:2401.05561v1 [cs.CL])

    [http://arxiv.org/abs/2401.05561](http://arxiv.org/abs/2401.05561)

    TrustLLM是对大型语言模型中可信性的全面研究，包括可信性原则的提出、建立基准的方法、评估主流语言模型的可信性，以及对未来挑战的讨论。

    

    大型语言模型（LLMs），如ChatGPT，因其出色的自然语言处理能力而引起了广泛关注。然而，这些LLMs在可信性方面存在许多挑战。因此，确保LLMs的可信性成为一个重要的话题。本文介绍了TrustLLM，它是对LLMs中可信性的全面研究，包括不同维度的可信性原则、建立基准、评估和分析主流LLMs的可信性，以及对开放挑战和未来方向的讨论。具体而言，我们首先提出了涵盖八个不同维度的可信LLMs原则。基于这些原则，我们进一步建立了一个跨六个维度的基准，包括真实性、安全性、公平性、鲁棒性、隐私性和机器伦理学。然后，我们在TrustLLM中展示了一个评估16个主流LLMs的研究，涵盖了30多个数据集。

    Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our find
    
[^170]: 通过谈判评估语言模型的代理能力

    Evaluating Language Model Agency through Negotiations. (arXiv:2401.04536v1 [cs.CL])

    [http://arxiv.org/abs/2401.04536](http://arxiv.org/abs/2401.04536)

    本研究通过谈判游戏的视角，提出共同评估语言模型（LM）的性能和对齐，以更好地反映真实世界的部署条件，并避免数据泄漏。通过评估多轮次和跨模型交互，我们发现了LM的自我对弈和交叉对弈性能。

    

    公司、组织和政府越来越多地利用语言模型（LM）展示类似代理行为的出色能力。随着LM被采用来执行越来越具有自主性的任务，迫切需要可靠且可扩展的评估基准。当前主要是静态的LM基准无法很好地评估此类动态应用。因此，我们提议通过谈判游戏的视角来共同评估LM的性能和对齐。我们认为这个共同任务更好地反映了真实世界的部署条件，并提供了关于LM决策过程的见解。至关重要的是，谈判游戏使我们能够研究多轮次和跨模型交互，调整复杂性，并避免评估中的意外数据泄漏。我们报告了来自几个主要供应商的六个公开可访问的LM在各种谈判游戏上的结果，评估了自我对弈和交叉对弈性能。值得注意的发现包括：（i）开源模式

    Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source mode
    
[^171]: 大型语言模型作为视觉跨领域学习器

    Large Language Models as Visual Cross-Domain Learners. (arXiv:2401.03253v1 [cs.CV])

    [http://arxiv.org/abs/2401.03253](http://arxiv.org/abs/2401.03253)

    本研究提出了大型语言模型作为视觉跨领域学习器（LLaVO），通过将图像转换为文本描述，使用大型语言模型进行训练和微调，实现了在跨领域任务中减少领域转移的效果。

    

    深度学习模型取得的最新进展依赖于独立同分布的假设，这限制了它们在真实世界中面对领域转移时的应用。为了解决以上问题，跨领域学习旨在提取领域不变的知识，减少训练与测试数据之间的领域转移。然而，在视觉跨领域学习中，传统方法仅关注图像模态，忽视了使用文本模态来缓解领域转移的作用。在本研究中，我们提出了大型语言模型作为视觉跨领域学习器（LLaVO）。LLaVO使用视觉-语言模型将图像转换为详细的文本描述。然后，在经过设计的指导模板生成的源域/目标域的文本描述上，对大型语言模型进行微调。在领域泛化和无监督领域自适应设置下进行的各种跨领域任务的广泛实验结果表明了该方法的效果。

    Recent advances achieved by deep learning models rely on the independent and identically distributed assumption, hindering their applications in real-world scenarios with domain shifts. To address the above issues, cross-domain learning aims at extracting domain-invariant knowledge to reduce the domain shift between training and testing data. However, in visual cross-domain learning, traditional methods concentrate solely on the image modality, neglecting the use of the text modality to alleviate the domain shift. In this work, we propose Large Language models as Visual cross-dOmain learners (LLaVO). LLaVO uses vision-language models to convert images into detailed textual descriptions. A large language model is then finetuned on textual descriptions of the source/target domain generated by a designed instruction template. Extensive experimental results on various cross-domain tasks under the domain generalization and unsupervised domain adaptation settings have demonstrated the effect
    
[^172]: 超越梯度和先验知识在隐私攻击中：利用联邦学习中语言模型的池化层输入

    Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.05720](http://arxiv.org/abs/2312.05720)

    本文引入了一种创新的方法，在联邦学习中利用语言模型的池化层输入来实现对隐私攻击的改进。通过恢复池化层输入，这种方法能够在不同的批处理大小下提供更高的文本恢复率，从而提供更细致和有效的见解。

    

    联邦学习强调分散式训练，通过本地存储数据并仅发送模型更新，强调用户隐私。最近，一系列有关隐私攻击的工作通过从联邦学习上下文的语言模型中提取敏感的训练文本来损害用户隐私。然而，这些攻击技术面临着不同的障碍：一些工作主要使用有限的批处理大小（例如，批处理大小为1），而其他技术则容易被检测出来。本文介绍了一种创新的方法，具有难以检测的特点，在不同的批处理大小设置下显著提高了文本恢复率。基于基本的梯度匹配和领域先验知识，我们通过恢复语言模型的池化层输入来增强攻击能力，这使我们能够在特征级别提供额外的监督信号。与梯度数据不同，这些信号不会在句子和标记之间进行平均，从而提供更细致和有效的见解。

    Federated learning (FL) emphasizes decentralized training by storing data locally and sending only model updates, underlining user privacy. Recently, a line of works on privacy attacks impairs user privacy by extracting sensitive training text from language models in the context of FL. Yet, these attack techniques face distinct hurdles: some work chiefly with limited batch sizes (e.g., batch size of 1), and others are easily detectable. This paper introduces an innovative approach that is challenging to detect, significantly enhancing the recovery rate of text in various batch-size settings. Building on fundamental gradient matching and domain prior knowledge, we enhance the attack by recovering the input of the Pooler layer of language models, which enables us to provide additional supervised signals at the feature level. Unlike gradient data, these signals do not average across sentences and tokens, thereby offering more nuanced and effective insights. We benchmark our method using t
    
[^173]: 《低秩适应的表达能力》

    The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])

    [http://arxiv.org/abs/2310.17513](http://arxiv.org/abs/2310.17513)

    本文分析了低秩适应（LoRA）的表达能力，证明了对于全连接神经网络，当LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度）时，LoRA可以使任何模型f准确表示任何较小的目标模型f。对于Transformer网络，通过rank-（嵌入大小/ 2）的LoRA适配器可以使任何模型适应于相同大小的目标模型。

    

    低秩适应（LoRA）是一种参数高效的微调方法，利用矩阵的低秩适应性，在微调预训练模型（如大型语言模型和扩散模型）中得到了广泛应用。尽管在实践中取得了巨大成功，但是LoRA的理论基础在很大程度上尚未得到探索。本文通过从理论角度分析LoRA的表达能力，首次尝试弥合这一差距。我们证明了对于全连接神经网络，如果LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度），则LoRA可以使任何模型f准确表示任何较小的目标模型f。当LoRA-rank低于阈值时，我们还量化了逼近误差。对于Transformer网络，我们证明任何模型可以通过rank-（嵌入大小/ 2）的LoRA适配器适应于相同大小的目标模型。

    Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
    
[^174]: CLEX: 大型语言模型的持续长度外推

    CLEX: Continuous Length Extrapolation for Large Language Models. (arXiv:2310.16450v1 [cs.CL])

    [http://arxiv.org/abs/2310.16450](http://arxiv.org/abs/2310.16450)

    CLEX是一种针对大型语言模型的持续长度外推方法，通过将位置嵌入缩放方法推广到连续动态建模，克服了当前方法在特定长度上的局限性。

    

    基于Transformer的大型语言模型（LLM）在许多自然语言处理任务中取得了突破性进展，然而，它们的卓越能力受限于Transformer的预设上下文窗口。位置嵌入（PE）缩放方法虽然能够将上下文窗口扩展到特定长度，但在外推能力方面存在明显的局限性，或者在上下文窗口内牺牲部分性能。虽然长度外推方法在理论上能够将上下文窗口延长至训练序列长度之外，但在实际的长上下文应用中表现不佳。为解决这些挑战，我们提出了适用于LLMs的持续长度外推（CLEX）方法。我们将PE缩放方法推广到通过常微分方程对长度缩放因子建模，从而克服了当前为特定长度设计的PE缩放方法的限制。

    Transformer-based Large Language Models (LLMs) are pioneering advances in many natural language processing tasks, however, their exceptional capabilities are restricted within the preset context window of Transformer. Position Embedding (PE) scaling methods, while effective in extending the context window to a specific length, demonstrate either notable limitations in their extrapolation abilities or sacrificing partial performance within the context window. Length extrapolation methods, although theoretically capable of extending the context window beyond the training sequence length, often underperform in practical long-context applications. To address these challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We generalise the PE scaling approaches to model the continuous dynamics by ordinary differential equations over the length scaling factor, thereby overcoming the constraints of current PE scaling methods designed for specific lengths. Moreover, by extending 
    
[^175]: GPT-who：一种基于信息密度的机器生成文本检测器

    GPT-who: An Information Density-based Machine-Generated Text Detector. (arXiv:2310.06202v1 [cs.CL])

    [http://arxiv.org/abs/2310.06202](http://arxiv.org/abs/2310.06202)

    GPT-who是一种基于统一信息密度原则的机器生成文本检测器，利用基于统一信息密度原则的特征来建模每个语言模型和人类作者的独特统计特征，以实现准确的作者归属。在多个领域中，GPT-who的性能超过了其他最先进的检测器，且具有计算成本低廉和可解释性。

    

    统一信息密度原则认为人类在语言产生过程中喜欢平均分布信息。在这项工作中，我们研究了统一信息密度原则是否可以帮助捕捉大型语言模型（LLMs）和人类生成文本之间的差异。我们提出了GPT-who，这是第一个基于心理语言学的多类领域不可知统计检测器。该检测器利用基于统一信息密度原则的特征建模每个LLM和人类作者的独特统计特征，以实现准确的作者归属。我们使用4个大型基准数据集对我们的方法进行评估，并发现GPT-who在各个领域上的性能优于最先进的检测器（包括基于统计和非统计的），如GLTR，GPTZero，OpenAI detector和ZeroGPT超过20％。除了性能优越外，GPT-who计算成本低廉，并利用可解释的文本表示。我们展示了对人类和机器生成文本的基于统一信息密度的表示的最大分析。

    The Uniform Information Density principle posits that humans prefer to spread information evenly during language production. In this work, we examine if the UID principle can help capture differences between Large Language Models (LLMs) and human-generated text. We propose GPT-who, the first psycholinguistically-aware multi-class domain-agnostic statistical-based detector. This detector employs UID-based features to model the unique statistical signature of each LLM and human author for accurate authorship attribution. We evaluate our method using 4 large-scale benchmark datasets and find that GPT-who outperforms state-of-the-art detectors (both statistical- & non-statistical-based) such as GLTR, GPTZero, OpenAI detector, and ZeroGPT by over $20$% across domains. In addition to superior performance, it is computationally inexpensive and utilizes an interpretable representation of text articles. We present the largest analysis of the UID-based representations of human and machine-genera
    
[^176]: 使用语音语句对的零资源切换语音基准进行多语言评估

    Zero Resource Code-switched Speech Benchmark Using Speech Utterance Pairs For Multiple Spoken Languages. (arXiv:2310.03018v1 [eess.AS])

    [http://arxiv.org/abs/2310.03018](http://arxiv.org/abs/2310.03018)

    本论文介绍了一个新的零资源切换语音基准，用于评估自监督语音编码器的切换语言能力，研究了预训练语言和模型大小对基准性能的影响，结果显示多语言预训练的语音编码器在切换语言场景中表现优于单语变体，但仍有改进空间。

    

    我们引入了一个新的零资源切换语音基准，旨在直接评估自监督语音编码器的切换语言能力。我们展示了一种基于离散单元语言建模的基线系统，以展示如何以零资源的方式评估语音编码器的切换语言能力。我们的实验涵盖了各种知名的语音编码器，包括Wav2vec 2.0、HuBERT、XLSR等。我们研究了预训练语言和模型大小对基准性能的影响。值得注意的是，尽管我们的结果表明，在切换语言场景中，具有多语言预训练的语音编码器（例如XLSR）优于单语变体（Wav2vec 2.0、HuBERT），但它们的切换语言能力仍有很大的改进空间。

    We introduce a new zero resource code-switched speech benchmark designed to directly assess the code-switching capabilities of self-supervised speech encoders. We showcase a baseline system of language modeling on discrete units to demonstrate how the code-switching abilities of speech encoders can be assessed in a zero-resource manner. Our experiments encompass a variety of well-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc. We examine the impact of pre-training languages and model size on benchmark performance. Notably, though our results demonstrate that speech encoders with multilingual pre-training, exemplified by XLSR, outperform monolingual variants (Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial room for improvement in their code-switching linguistic abilities.
    
[^177]: MVMR: 在多个可靠视频集中评估自然语言视频定位偏差

    MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool. (arXiv:2309.16701v1 [cs.CV])

    [http://arxiv.org/abs/2309.16701](http://arxiv.org/abs/2309.16701)

    本文提出了一个名为MVMR的任务，旨在给定文本查询从大量视频集中定位视频帧。我们通过已有数据集进行相似性筛选来构建数据集，并引入三个MVMR数据集。我们采用了嵌入式文本相似度匹配和视频-语言对齐技术来计算相关性得分，并为MVMR任务开发了一个强大的模型，Reliable Mutual Matching Network (RMMN)。

    

    随着近年来多媒体内容的激增，自然语言视频定位成为一个关键问题，它致力于检测与给定自然语言查询匹配的视频片段。然而，以往的研究都没有探索在存在多个正负视频的大量语料库中定位一个时刻。本文提出了一个名为MVMR（Massive Videos Moment Retrieval）的任务，旨在给定文本查询从大量视频集中定位视频帧。对于这个任务，我们提出了一种通过对现有视频定位数据集进行相似性筛选来构建数据集的方法，并引入了三个MVMR数据集。具体来说，我们采用基于嵌入的文本相似度匹配和视频-语言对齐技术来计算目标查询与视频之间的相关性得分，从而定义正负集。针对提出的MVMR任务，我们进一步开发了一个强大的模型，Reliable Mutual Matching Network (RMMN)。

    With the explosion of multimedia content in recent years, natural language video localization, which focuses on detecting video moment that matches a given natural language query, has become a critical problem. However, none of the previous research explores localizing a moment from a large corpus where multiple positive and negative videos exist. In this paper, we propose an MVMR (Massive Videos Moment Retrieval) task, which aims to localize video frames from a massive set of videos given a text query. For this task, we suggest methods for constructing datasets by employing similarity filtering on the existing video localization datasets and introduce three MVMR datasets. Specifically, we employ embedding-based text similarity matching and video-language grounding techniques to calculate the relevance score between a target query and videos to define positive and negative sets. For the proposed MVMR task, we further develop a strong model, Reliable Mutual Matching Network (RMMN), whic
    
[^178]: 大型语言模型和控制机制提高了生物医学摘要的文本可读性

    Large Language Models and Control Mechanisms Improve Text Readability of Biomedical Abstracts. (arXiv:2309.13202v1 [cs.CL])

    [http://arxiv.org/abs/2309.13202](http://arxiv.org/abs/2309.13202)

    本研究使用大型语言模型和控制机制改善了生物医学摘要的文本可读性，具体包括领域微调和基于提示的学习方法，以及应用于编码器-解码器模型和GPT模型的控制令牌机制。

    

    生物医学文献通常使用复杂的语言和难以理解的专业术语。因此，简化在提高公共健康素养方面起着重要作用。将自然语言处理（NLP）模型应用于自动化此类任务可以使非专业读者快速直接地获取信息。在本研究中，我们使用公开可用的用于生物医学摘要简化的数据集（PLABA）来调查最先进大型语言模型（LLMs）在生物医学摘要简化任务上的能力。应用的方法包括领域微调和基于提示的学习（PBL）在：1）编码器-解码器模型（T5、SciFive和BART）上，2）仅解码器的GPT模型（GPT-3.5和GPT-4）来自OpenAI和BioGPT，以及3）基于控制令牌机制的基于BART的模型。我们使用了一系列自动评估指标，包括BLEU、ROUGE、SARI和BERTscore，并进行了人工评估。

    Biomedical literature often uses complex language and inaccessible professional terminologies. That is why simplification plays an important role in improving public health literacy. Applying Natural Language Processing (NLP) models to automate such tasks allows for quick and direct accessibility for lay readers. In this work, we investigate the ability of state-of-the-art large language models (LLMs) on the task of biomedical abstract simplification, using the publicly available dataset for plain language adaptation of biomedical abstracts (\textbf{PLABA}). The methods applied include domain fine-tuning and prompt-based learning (PBL) on: 1) Encoder-decoder models (T5, SciFive, and BART), 2) Decoder-only GPT models (GPT-3.5 and GPT-4) from OpenAI and BioGPT, and 3) Control-token mechanisms on BART-based models. We used a range of automatic evaluation metrics, including BLEU, ROUGE, SARI, and BERTscore, and also conducted human evaluations. BART-Large with Control Token (BART-L-w-CT) m
    
[^179]: DreamLLM：协同的多模态理解与创作

    DreamLLM: Synergistic Multimodal Comprehension and Creation. (arXiv:2309.11499v1 [cs.CV])

    [http://arxiv.org/abs/2309.11499](http://arxiv.org/abs/2309.11499)

    DreamLLM是一种学习框架，实现了多模态理解与创作的协同效应。通过直接采样生成语言和图像的生成模型，避免了信息损失，并获得了更全面的多模态理解。此外，DreamLLM能够生成自由形式交织内容，展现了其在零样本多模态学习任务中的卓越性能。

    

    本文介绍了DreamLLM，一种学习框架，它首次实现了多模态大型语言模型（MLLMs），利用了多模态理解与创作之间经常被忽视的协同效应。DreamLLM遵循两个基本原则。第一个原则专注于通过在原始多模态空间中进行直接采样来生成语言和图像后验的生成建模。这种方法避免了像CLIP这样的外部特征提取器所固有的限制和信息损失，并获得了更全面的多模态理解。其次，DreamLLM促进了原始的、交织的文件生成，对文本和图像内容以及非结构化布局进行建模。这使得DreamLLM能够有效地学习所有条件、边缘和联合多模态分布。作为结果，DreamLLM是第一个能够生成自由形式交织内容的MLLM。综合实验突显了DreamLLM作为零样本多模态学习任务的卓越性能。

    This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal
    
[^180]: OpenChat: 用混合质量数据推进开源语言模型

    OpenChat: Advancing Open-source Language Models with Mixed-Quality Data. (arXiv:2309.11235v1 [cs.CL])

    [http://arxiv.org/abs/2309.11235](http://arxiv.org/abs/2309.11235)

    OpenChat是一种用于推进开源语言模型的新框架，能够利用混合质量数据中的信息并简化RLFT方法的求解过程。

    

    如今，像LLaMA这样的开源大型语言模型已经出现。最近的发展中使用了监督微调（SFT）和强化学习微调（RLFT）来使这些模型与人类目标保持一致。然而，SFT方法将所有训练数据的混合质量等同对待，而RLFT方法则需要高质量的成对或基于排名的偏好数据。在这项研究中，我们提出了一种新的框架，名为OpenChat，用于利用混合质量数据推进开源语言模型。具体而言，我们考虑了一般的SFT训练数据，其中包含了少量的专家数据和大量的次优数据，没有任何优先级标签。我们提出了C(onditioned)-RLFT，将不同的数据源视为粗粒度的奖励标签，并学习一个条件化策略，以利用互补的数据质量信息。有趣的是，在C-RLFT中，最优策略可以通过单阶段无强化学习的监督学习轻松求解，使得该问题得到了简化。

    Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, whic
    
[^181]: 评估潮起潮落：对不同平台间问答趋势的深入分析

    Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])

    [http://arxiv.org/abs/2309.05961](http://arxiv.org/abs/2309.05961)

    本文通过对六个社区问答平台的研究，发现了查询的元数据、问题构成方式和用户互动水平与第一个回答时间之间的关联，并利用机器学习模型预测查询是否能够迅速获得回答。

    

    社区问答平台因其快速回答用户查询的能力而越来越受欢迎。这些回答速度的快慢取决于查询特定和用户相关的因素的综合。本文通过研究六个高度流行的社区问答平台，分析了这些因素在其中的作用。我们的调查揭示了问题的第一个回答所花费的时间与元数据、问题的构成方式和用户之间的互动水平之间的关联。此外，通过使用传统的机器学习模型分析这些元数据和用户互动模式，我们试图预测哪些查询将迅速获得初始回答。

    Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
    
[^182]: 大型内容和行为模型用于理解、模拟和优化内容和行为

    Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior. (arXiv:2309.00359v1 [cs.CL])

    [http://arxiv.org/abs/2309.00359](http://arxiv.org/abs/2309.00359)

    该论文提出了使用大型内容和行为模型来理解、模拟和优化内容和行为。大型语言模型虽然在任务泛化能力方面取得了进展，但还无法解决预测和优化通信以实现期望接收者行为的问题。其中的一个原因可能是训练语料库中缺少"行为标记"。

    

    香农在引入信息理论的经典论文中将通信分为三个层次：技术层、语义层和效果层。技术层关注的是准确重构传输的符号，而语义层和效果层则涉及推断出的意义及其对接收者的影响。得益于电信技术，第一层问题已经取得了较大的进步，如互联网。大型语言模型（LLM）在第二个目标方面取得了一些进展，但第三层仍然基本上未被触及。第三个问题涉及预测和优化通信以实现期望的接收者行为。LLM在各种任务中显示出了广泛的泛化能力，但无法解决这个问题。表现不佳的原因之一可能是LLM的训练语料库中缺少"行为标记"。行为标记定义了在一次通信中的接收者行为，如分享、点赞、点击、购买、转推等。

    Shannon, in his seminal paper introducing information theory, divided the communication into three levels: technical, semantic, and effectivenss. While the technical level is concerned with accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Thanks to telecommunications, the first level problem has produced great advances like the internet. Large Language Models (LLMs) make some progress towards the second goal, but the third level still remains largely untouched. The third problem deals with predicting and optimizing communication for desired receiver behavior. LLMs, while showing wide generalization capabilities across a wide range of tasks, are unable to solve for this. One reason for the underperformance could be a lack of "behavior tokens" in LLMs' training corpora. Behavior tokens define receiver behavior over a communication, such as shares, likes, clicks, purchases, retweets, etc. W
    
[^183]: OmniQuant：用于大型语言模型的全向校准量化

    OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models. (arXiv:2308.13137v1 [cs.LG])

    [http://arxiv.org/abs/2308.13137](http://arxiv.org/abs/2308.13137)

    OmniQuant是一种用于大型语言模型的全向校准量化技术，通过优化各种量化参数实现了良好的性能，并保持了计算效率。

    

    大型语言模型（LLM）已经在自然语言处理任务中带来了革命性的变化。然而，它们的实际部署受到了其庞大的内存和计算需求的限制。虽然最近的后训练量化（PTQ）方法在减少内存占用和提高LLM的计算效率方面非常有效，但它们手工制定量化参数，导致性能较低并且不能处理极低位量化。为了解决这个问题，我们介绍了一种全向校准量化（OmniQuant）技术，用于LLMs，它在多种量化设置下实现了良好的性能，并通过高效优化各种量化参数来保持PTQ的计算效率。OmniQuant包含两个创新组件，包括可学习的权重剪裁（LWC）和可学习的等效变换（LET）。LWC通过优化剪裁阈值来调节权重的极值。与此同时，LET处理激活函数。

    Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, which leads to low performance and fails to deal with extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activa
    
[^184]: LatEval: 一个带有来自侧思维谜题的不完整信息的交互式LLMs评估基准

    LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles. (arXiv:2308.10855v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10855](http://arxiv.org/abs/2308.10855)

    LatEval是一个新颖的LLMs评估基准，通过侧思维谜题挑战模型的横向思考能力，在交互过程中考验模型提出问题的质量和整合信息解决问题的能力。研究发现，几乎所有LLMs在横向思考方面存在困难，即使是最先进的GPT-4模型相比人类也有明显差距。这个基准测试对于有效的AI助理至关重要。

    

    随着LLMs的不断发展和改进，它们被赋予了令人印象深刻的逻辑推理或纵向思维能力。但它们能否跳出固定模式思考？它们是否具备高超的横向思考能力？在侧思维谜题的基础上，我们提出了一个新颖的评估基准，称为LatEval，它在一个交互式框架中评估模型的横向思考能力。在我们的基准测试中，我们向LLMs提出了两个方面的挑战：模型提出的问题的质量和模型在解决问题时整合信息的能力。我们发现几乎所有的LLMs在交互过程中都很难运用横向思考。例如，即使是最先进的模型GPT-4，在某种程度上也具有优势，但与人类相比仍存在明显差距。这个评估基准为LLMs提供了一个极具挑战性和独特的任务，对于有效的人工智能助理至关重要。

    With the continuous evolution and refinement of LLMs, they are endowed with impressive logical reasoning or vertical thinking capabilities. But can they think out of the box? Do they possess proficient lateral thinking abilities? Following the setup of Lateral Thinking Puzzles, we propose a novel evaluation benchmark, LatEval, which assesses the model's lateral thinking within an interactive framework. In our benchmark, we challenge LLMs with 2 aspects: the quality of questions posed by the model and the model's capability to integrate information for problem-solving. We find that nearly all LLMs struggle with employing lateral thinking during interactions. For example, even the most advanced model, GPT-4, exhibits the advantage to some extent, yet still maintain a noticeable gap when compared to human. This evaluation benchmark provides LLMs with a highly challenging and distinctive task that is crucial to an effective AI assistant.
    
[^185]: 更多上下文，更少干扰：通过推断和调节上下文属性进行视觉分类

    More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v1 [cs.CV])

    [http://arxiv.org/abs/2308.01313](http://arxiv.org/abs/2308.01313)

    本文借鉴了人类视觉感知过程，提出了一种通过推断和调节上下文属性来改进零样本图像分类的方法。通过给CLIP提供上下文属性，可以减轻对虚假特征的依赖，进而提高零样本分类的准确性。

    

    CLIP作为一种基础的视觉语言模型，由于其理解各种视觉概念和自然语言描述的能力，被广泛应用于零样本图像分类。然而，如何充分利用CLIP的前所未有的人类般理解能力来实现更好的零样本分类仍然是一个开放问题。本文从人类的视觉感知过程中得到启发：现代神经科学观点认为，在对物体进行分类时，人类首先推断其与类别无关的属性（如背景和方向），这有助于将前景对象与背景区分开来，然后以此信息为基础进行决策。受此启发，我们观察到为CLIP提供上下文属性可以改善零样本分类并减轻对虚假特征的依赖。我们还观察到CLIP本身可以合理地从图像中推断出这些属性。基于这些观察，我们提出了一种零训练、两步骤的零样本分类方法。

    CLIP, as a foundational vision language model, is widely used in zero-shot image classification due to its ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better zero-shot classification is still an open question. This paper draws inspiration from the human visual perception process: a modern neuroscience view suggests that in classifying an object, humans first infer its class-independent attributes (e.g., background and orientation) which help separate the foreground object from the background, and then make decisions based on this information. Inspired by this, we observe that providing CLIP with contextual attributes improves zero-shot classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot cl
    
[^186]: RLCD: 基于对比蒸馏的强化学习用于语言模型对齐

    RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment. (arXiv:2307.12950v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.12950](http://arxiv.org/abs/2307.12950)

    RLCD是一种用于语言模型对齐的强化学习方法，利用对比蒸馏训练偏好模型，可以使语言模型在不使用人类反馈的情况下遵循自然语言规则。在多个对齐任务和不同规模的模型上，RLCD优于其他基线方法。

    

    我们提出了一种称为Reinforcement Learning from Contrast Distillation (RLCD)的方法，用于无需使用人类反馈即可使语言模型遵循自然语言规则的对齐。RLCD使用模拟的偏好对进行训练，这些对包含了高质量和低质量的示例，其中使用对比的正负提示生成。然后，使用偏好模型通过强化学习来改进基础的无对齐语言模型。在实证上，RLCD在三个不同的对齐任务（无害性、有用性和故事大纲生成）以及7B和30B模型规模的偏好数据模拟上，都优于RLAIF (Bai等人，2022b)和上下文蒸馏 (Huang等人，2022) 的基准方法。

    We propose Reinforcement Learning from Contrast Distillation (RLCD), a method for aligning language models to follow natural language principles without using human feedback. RLCD trains a preference model using simulated preference pairs that contain both a high-quality and low-quality example, generated using contrasting positive and negative prompts. The preference model is then used to improve a base unaligned language model via reinforcement learning. Empirically, RLCD outperforms RLAIF (Bai et al., 2022b) and context distillation (Huang et al., 2022) baselines across three diverse alignment tasks--harmlessness, helpfulness, and story outline generation--and on both 7B and 30B model scales for preference data simulation.
    
[^187]: 在大型语言模型中的上下文压缩的上下文自编码器

    In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])

    [http://arxiv.org/abs/2307.06945](http://arxiv.org/abs/2307.06945)

    在大型语言模型中，我们提出了一种称为In-context Autoencoder (ICAE)的上下文自编码器，它通过将长上下文压缩为有限数量的内存槽，实现了$4\times$的上下文压缩，并能够根据内存槽进行条件处理以响应各种提示。

    

    我们提出了一种用于大型语言模型中上下文压缩的上下文自编码器（ICAE）。 ICAE有两个模块：一个可学习的编码器，通过从LLM中采用LoRA方式将长上下文压缩为有限数量的内存槽，以及一个固定的解码器，作为目标LLM，可以根据内存槽来进行各种目的的条件处理。我们首先使用自编码和语言建模目标在大规模文本数据上预训练ICAE，使其能够生成准确和全面表示原始上下文的内存槽。然后，我们使用少量指导数据对预训练的ICAE进行微调，以增强其与各种提示的交互，从而产生理想的响应。我们的实验结果表明，使用我们提出的预训练和微调范式学习的ICAE可以有效地产生$4\times$上下文压缩的内存槽，目标LLM可以很好地对其进行条件处理，以响应各种提示。

    We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promis
    
[^188]: 通过记忆增强的适配器实现可插拔的神经机器翻译模型

    Pluggable Neural Machine Translation Models via Memory-augmented Adapters. (arXiv:2307.06029v1 [cs.CL])

    [http://arxiv.org/abs/2307.06029](http://arxiv.org/abs/2307.06029)

    通过记忆增强的适配器，我们提出了一种可插拔的方法来控制神经机器翻译模型的生成行为。实验证明，我们的方法可以胜过几个代表性的可插拔基准模型。

    

    尽管神经机器翻译（NMT）模型在普通领域表现出色，但是控制其生成行为以满足不同用户需求仍然具有一定挑战性。鉴于每个用户需求都需要从头开始学习新模型的高昂训练成本和数据稀缺的挑战，我们提出了一种记忆增强适配器，以可插拔的方式引导预训练的NMT模型。具体而言，我们基于用户提供的文本样本构建了一个多粒度记忆，并提出了一种新的适配器架构来结合模型表示和检索结果。同时，我们提出了一种使用记忆丢弃的训练策略，以减少NMT模型和记忆之间的虚假依赖关系。我们在风格和领域特定实验中验证了我们的方法，结果表明，我们的方法可以胜过几个代表性的可插拔基准模型。

    Although neural machine translation (NMT) models perform well in the general domain, it remains rather challenging to control their generation behavior to satisfy the requirement of different users. Given the expensive training cost and the data scarcity challenge of learning a new model from scratch for each user requirement, we propose a memory-augmented adapter to steer pretrained NMT models in a pluggable manner. Specifically, we construct a multi-granular memory based on the user-provided text samples and propose a new adapter architecture to combine the model representations and the retrieved results. We also propose a training strategy using memory dropout to reduce spurious dependencies between the NMT model and the memory. We validate our approach on both style- and domain-specific experiments and the results indicate that our method can outperform several representative pluggable baselines.
    
[^189]: 用深度学习简化社交媒体信息检索以支持公共卫生研究

    Streamlining Social Media Information Retrieval for Public Health Research with Deep Learning. (arXiv:2306.16001v1 [cs.CL])

    [http://arxiv.org/abs/2306.16001](http://arxiv.org/abs/2306.16001)

    本研究介绍了一个使用深度学习简化社交媒体信息检索的框架，通过识别医学实体、标准化实体和分配UMLS概念，构建了一个用于COVID-19相关推文的症状词典。

    

    社交媒体在流行病监测中的利用已经得到了很好的证实。然而，当使用预定义的词汇表来检索相关语料库时，常常会引入偏见。本研究介绍了一个框架，旨在构建医学俗语和统一医学语言系统（UMLS）概念的广泛字典。该框架由三个模块组成：基于BERT的命名实体识别（NER）模型，用于从社交媒体内容中识别出医学实体；深度学习驱动的标准化模块，用于对提取出的实体进行规范化处理；半监督聚类模块，将最可能的UMLS概念分配给每个规范化实体。我们将该框架应用于从2020年2月1日到2022年4月30日期间与COVID-19相关的推文，生成了一个症状词典（可在https://github.com/ningkko/UMLS_colloquialism/上获取），其中包含9,249个标准化实体，映射到876个UMLS概念和38,175个俚语表达。该框架的演示

    The utilization of social media in epidemic surveillance has been well established. Nonetheless, bias is often introduced when pre-defined lexicons are used to retrieve relevant corpus. This study introduces a framework aimed at curating extensive dictionaries of medical colloquialisms and Unified Medical Language System (UMLS) concepts. The framework comprises three modules: a BERT-based Named Entity Recognition (NER) model that identifies medical entities from social media content, a deep-learning powered normalization module that standardizes the extracted entities, and a semi-supervised clustering module that assigns the most probable UMLS concept to each standardized entity. We applied this framework to COVID-19-related tweets from February 1, 2020, to April 30, 2022, generating a symptom dictionary (available at https://github.com/ningkko/UMLS_colloquialism/) composed of 9,249 standardized entities mapped to 876 UMLS concepts and 38,175 colloquial expressions. This framework demo
    
[^190]: LLM能表达它们的不确定性吗？LMM自信心评估的实证研究

    Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. (arXiv:2306.13063v1 [cs.CL])

    [http://arxiv.org/abs/2306.13063](http://arxiv.org/abs/2306.13063)

    本研究就不需要微调模型或访问专有信息的方法进行置信度引导进行了探讨，通过研究发现LLMs往往展现出高度的过度自信。

    

    将大型语言模型(LLMs)赋予准确表达其置信度的能力，即置信度引导任务，对确保可靠和可信的决策过程至关重要。本研究探讨了不需要微调模型或访问专有信息的置信度引导方法，介绍了三种方法：基于表述、基于一致性、以及它们的混合方法进行基准测试，并在五种类型的数据集和四种广泛使用的LLMs上评估它们的性能。

    The task of empowering large language models (LLMs) to accurately express their confidence, referred to as confidence elicitation, is essential in ensuring reliable and trustworthy decision-making processes. Previous methods, which primarily rely on model logits, have become less suitable for LLMs and even infeasible with the rise of closed-source LLMs (e.g., commercialized LLM APIs). This leads to a growing need to explore the untapped area of \emph{non-logit-based} approaches to estimate the uncertainty of LLMs. Hence, in this study, we investigate approaches for confidence elicitation that do not require model fine-tuning or access to proprietary information. We introduce three categories of methods: verbalize-based, consistency-based, and their hybrid methods for benchmarking, and evaluate their performance across five types of datasets and four widely-used LLMs. Our analysis of these methods uncovers several key insights: 1) LLMs often exhibit a high degree of overconfidence when 
    
[^191]: MOFI: 从含噪实体标注的图像中学习图像表示

    MOFI: Learning Image Representations from Noisy Entity Annotated Images. (arXiv:2306.07952v1 [cs.CV])

    [http://arxiv.org/abs/2306.07952](http://arxiv.org/abs/2306.07952)

    MOFI 提出了一种新的方法，自动从含噪图像文本对中为图像指定实体标签，创建了一个新的大规模数据集 I2E，通过研究不同的训练配方，学习到了能够有效学习图像表示的模型。

    

    本文提出了一种新的视觉基础模型 MOFI，旨在从含噪实体标注的图像中学习图像表示。MOFI 与以往的工作有两点不同：（i）预训练数据，（ii）训练配方。在数据方面，我们引入了一种新方法，自动从含噪图像文本对中为图像指定实体标签。我们使用命名实体识别模型从 alt-text 中提取实体，然后使用 CLIP 模型选择正确的实体作为图像的标签。这种方法简单易行，不需要昂贵的人工注释，并且可以轻松扩展到从 web 上挖掘的数十亿个图像文本对。通过这种方法，我们创建了 Image-to-Entities（I2E）这一新的大规模数据集，其中包含 10 亿张图像和 200 万个不同的实体，涵盖了野外丰富的视觉概念。基于 I2E 数据集，我们研究了不同的训练配方，包括有监督的预训练、对比度预训练。

    We present MOFI, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: ($i$) pre-training data, and ($ii$) training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. The approach is simple, does not require costly human annotation, and can be readily scaled up to billions of image-text pairs mined from the web. Through this method, we have created Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes, including supervised pre-training, contrastive pre-train
    
[^192]: 大型语言模型的自相矛盾幻觉：评估、检测和缓解

    Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. (arXiv:2305.15852v1 [cs.CL])

    [http://arxiv.org/abs/2305.15852](http://arxiv.org/abs/2305.15852)

    本文对大型语言模型的自相矛盾幻觉进行了评估、检测和缓解，探究了这一幻觉形式的普遍存在性。通过设计框架有效触发自相矛盾，发现不同语言模型中这种现象都频繁出现。ChatGPT和GPT-4能够准确识别自相矛盾，而Vicuna-13B则有些困难。

    

    大型语言模型容易产生幻想的文本。自相矛盾是一种重要的幻觉形式，指的是语言模型在同一语境中生成两个矛盾的句子。本文针对最先进、经过指导的语言模型，对自相矛盾进行了全面的分析、评估、检测和缓解。我们设计了一个框架来有效地触发自相矛盾，评估结果表明，无论是对于著名的还是不太出名的话题，不同的语言模型中自相矛盾都经常发生。

    Large language models (large LMs) are susceptible to producing text with hallucinated content. Self-contradiction, where the LM generates two contradictory sentences within the same context, is an important form of hallucination. In this work, we present a comprehensive analysis on self-contradiction for state-of-the-art, instruction-tuned LMs, including evaluation, detection, and mitigation. To effectively trigger self-contradictions, we design a framework that constrains LMs to generate appropriate sentence pairs. Our evaluation on these sentence pairs reveals that self-contradictions occur frequently across different LMs for both famous and lesser-known topics. Next, we prompt the LMs to detect self-contradictions. Our results indicate that ChatGPT and GPT-4 are able to accurately identify self-contradictions, while Vicuna-13B struggles to do so. For example, with our best prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on the sentence pairs generated by itself. 
    
[^193]: 在混合语言的社交媒体文本中检测宣传技术

    Detecting Propaganda Techniques in Code-Switched Social Media Text. (arXiv:2305.14534v1 [cs.CL])

    [http://arxiv.org/abs/2305.14534](http://arxiv.org/abs/2305.14534)

    本文提出了一个新任务，即在混合语言的社交媒体文本中检测宣传技术。为了支持这一任务，作者创建了一个包含1030个文本的英语和罗马乌尔混合语言的语料库，并进行了一系列实验。

    

    宣传是一种旨在影响公众舆论和心态以推广特定议程的沟通形式。随着社交媒体的崛起，宣传已经迅速传播，引发了对自动宣传检测系统的需求。大多数宣传检测工作都集中在高资源语言（如英语）上，几乎没有为低资源语言检测宣传做出努力。然而，在社交媒体交流中发现多种语言的混合现象是很常见的，这被称为码混。码混在同一文本中结合了不同的语言，这对于自动系统构成了挑战。考虑到这一点，我们在此提出了检测混合文本中宣传技术的新任务。为了支持这个任务，我们创建了一个包含1030个文本的语料库，这些文本在英语和罗马乌尔都进行了混合，并用20种宣传技巧进行了注释，我们已经公开了这个语料库。我们进行了一系列实验来对比不同的模型和特征集合在此任务上的表现。

    Propaganda is a form of communication intended to influence the opinions and the mindset of the public to promote a particular agenda. With the rise of social media, propaganda has spread rapidly, leading to the need for automatic propaganda detection systems. Most work on propaganda detection has focused on high-resource languages, such as English, and little effort has been made to detect propaganda for low-resource languages. Yet, it is common to find a mix of multiple languages in social media communication, a phenomenon known as code-switching. Code-switching combines different languages within the same text, which poses a challenge for automatic systems. With this in mind, here we propose the novel task of detecting propaganda techniques in code-switched text. To support this task, we create a corpus of 1,030 texts code-switching between English and Roman Urdu, annotated with 20 propaganda techniques, which we make publicly available. We perform a number of experiments contrastin
    
[^194]: LLM自身可读取和生成CXR图像

    LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])

    [http://arxiv.org/abs/2305.11490](http://arxiv.org/abs/2305.11490)

    该论文提出了一种新方法，可以在不需要进行结构更改、额外训练、或训练专门网络的情况下，通过微调预先训练的LLM来读取和生成像文本一样的图像，并应用于胸部X线（CXR）图像的生成任务中。

    

    借助于近期大语言模型（LLMs）的显著发展，人们正积极尝试将LLMs的实用性扩展到多模态任务。已经有人尝试连接语言和视觉信息，并且也在不断尝试为LLMs添加视觉能力。然而，现有的尝试只使用LLMs作为图像解码器，没有尝试通过自然语言来生成图像。通过采用VQ-GAN框架，将图像的潜在表示视为一种文本标记，我们提出了一种新方法，可以微调预先训练的LLM，以像文本一样读取和生成图像，而无需进行结构更改、额外的训练目标或训练专门的网络，同时仍保留LLM的指令跟随能力。我们将此框架应用于胸部X线（CXR）图像的生成任务中，因为这是一个复杂信息在视觉和语言之间翻译的领域。

    Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
    
[^195]: 使用LLM辅助注释进行语料库语言学研究：本地语法分析案例研究

    Using LLM-assisted Annotation for Corpus Linguistics: A Case Study of Local Grammar Analysis. (arXiv:2305.08339v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08339](http://arxiv.org/abs/2305.08339)

    本文研究了使用基于大语言模型的聊天机器人自动标注文本的潜力，重点考察了从本地语法角度观察道歉言语行为构成的功能元素的程度，并比较了不同模型在注释任务中的表现，结果表明Bing聊天机器人在任务中表现优于ChatGPT和人类标注员。

    

    基于大语言模型（LLMs）的聊天机器人在语言理解方面表现出很强的能力。本研究探索LLMs在协助基于语料库的语言学研究方面的潜力，通过将文本自动标注为特定语言信息类别。具体而言，我们研究了从本地语法的角度观察道歉言语行为构成的功能元素的程度，通过比较基于GPT-3.5的ChatGPT、基于GPT-4的Bing聊天机器人和人类编码器在注释任务中的表现。结果表明，Bing聊天机器人在任务中表现显着优于ChatGPT。与人类标注员相比，Bing聊天机器人的整体表现略低于人类标注员的表现，但已经取得了较高的F1得分:道歉标记99.95％，原因标记91.91％，道歉者标记95.35％，被道歉者标记89.74％和加强标记96.47％。这表明，在语言类别清晰且可以轻松识别的情况下，使用LLM辅助注释进行语料库语言学研究是可行的。

    Chatbots based on Large Language Models (LLMs) have shown strong capabilities in language understanding. In this study, we explore the potential of LLMs in assisting corpus-based linguistic studies through automatic annotation of texts with specific categories of linguistic information. Specifically, we examined to what extent LLMs understand the functional elements constituting the speech act of apology from a local grammar perspective, by comparing the performance of ChatGPT (powered by GPT-3.5), the Bing chatbot (powered by GPT-4), and a human coder in the annotation task. The results demonstrate that the Bing chatbot significantly outperformed ChatGPT in the task. Compared to human annotator, the overall performance of the Bing chatbot was slightly less satisfactory. However, it already achieved high F1 scores: 99.95% for the tag of APOLOGISING, 91.91% for REASON, 95.35% for APOLOGISER, 89.74% for APOLOGISEE, and 96.47% for INTENSIFIER. This suggests that it is feasible to use LLM-
    
[^196]: 当多数人是错误的：利用标注者不一致性进行主观任务

    When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v1 [cs.CL])

    [http://arxiv.org/abs/2305.06626](http://arxiv.org/abs/2305.06626)

    本文通过预测单个标注者的打分，并结合文本目标群体的预测，模拟了目标群体成员的意见，通过使用他们的人口统计学数据和在线意见预测标注者的打分，在仇恨言论检测等主观任务中提高了模型性能。

    

    在自然语言处理中，虽然通常使用标注者的多数投票来确定标签，但在仇恨言论检测等主观任务中，标注者之间存在不一致性可能反映出群体观点的差异，而不是噪声。因此，仇恨言论检测的一个关键问题是一个语句是否冒犯了它所针对的人群，而这可能只占标注者池的一小部分。本文构建了一个模型，预测可能具有冒犯性文本上每个标注者的打分，并结合文本的预测目标群体来模拟目标群体成员的意见。我们展示了一系列的评估指标，包括提高了22％在预测每个标注者的打分上的性能，提高了33％在预测标注者之间方差上的性能，这提供了下游用来衡量模型不确定性的方法。我们发现可以使用标注者的人口统计信息和其在线意见来预测标注者的打分。

    Though majority vote among annotators is typically used for ground truth labels in natural language processing, annotator disagreement in tasks such as hate speech detection may reflect differences among group opinions, not noise. Thus, a crucial problem in hate speech detection is whether a statement is offensive to the demographic group that it targets, which may constitute a small fraction of the annotator pool. We construct a model that predicts individual annotator ratings on potentially offensive text and combines this information with the predicted target group of the text to model the opinions of target group members. We show gains across a range of metrics, including raising performance over the baseline by 22% at predicting individual annotators' ratings and 33% at predicting variance among annotators, which provides a method of measuring model uncertainty downstream. We find that annotators' ratings can be predicted using their demographic information and opinions on online 
    
[^197]: 向导航即攻击者所愿？建立拜占庭鲁棒性的联邦学习体系下的代理人

    Navigation as Attackers Wish? Towards Building Byzantine-Robust Embodied Agents under Federated Learning. (arXiv:2211.14769v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.14769](http://arxiv.org/abs/2211.14769)

    本文研究了联邦学习体系下代理人学习中可能出现的攻击和防御策略，建立了全联邦拜占庭鲁棒的代理人学习模型。其中，导航即攻击者所愿（NAW）是一种简单而有效的攻击策略，而基于离群点检测的防御训练方法可以有效减轻NAW攻击的影响，提高代理人学习的全局鲁棒性。

    

    联邦化体系下的代理人学习通过在本地客户端（即不同环境）中保持数据来保护个人视觉环境的数据隐私。然而，在联邦学习下，由于本地数据对服务器是不可访问的，攻击者可能轻易地污染本地客户端的训练数据，从而在不被通知的情况下在代理人中建立后门。使用这样的代理人会对人类构成潜在危害，因为攻击者可以轻松地通过后门操纵代理人进行导航和控制。为了实现全联邦拜占庭鲁棒的代理人学习，在本文中，我们研究了视觉与语言导航（VLN）任务中的攻击和防御，其中代理人需要跟随自然语言指令来导航室内环境。首先，我们介绍了一种简单而有效的攻击策略，即导航即攻击者所愿（NAW），其中恶意客户端通过操纵本地轨迹数据来向全局模型植入后门。结果表明，NAW可以实现高攻击成功率，而且性能下降微不足道。为了防止NAW攻击，我们提出了一种防御训练方法，该方法利用离群点检测的概念来识别和删除恶意客户端。我们在VLN任务上的实验表明，所提出的防御方法可以有效地减轻NAW攻击的影响，提高联邦化体系下代理人学习的全局鲁棒性。

    Federated embodied agent learning protects the data privacy of individual visual environments by keeping data locally at each client (the individual environment) during training. However, since the local data is inaccessible to the server under federated learning, attackers may easily poison the training data of the local client to build a backdoor in the agent without notice. Deploying such an agent raises the risk of potential harm to humans, as the attackers may easily navigate and control the agent as they wish via the backdoor. Towards Byzantine-robust federated embodied agent learning, in this paper, we study the attack and defense for the task of vision-and-language navigation (VLN), where the agent is required to follow natural language instructions to navigate indoor environments. First, we introduce a simple but effective attack strategy, Navigation as Wish (NAW), in which the malicious client manipulates local trajectory data to implant a backdoor into the global model. Resu
    
[^198]: 我们能了解甚至无法想象的事物吗？

    What can we know about that which we cannot even imagine?. (arXiv:2208.03886v3 [physics.hist-ph] UPDATED)

    [http://arxiv.org/abs/2208.03886](http://arxiv.org/abs/2208.03886)

    这篇文章探讨了关于智能、人类语言和人类数学的问题，强调了人类语言的局限性，以及我们能否对我们无法想象的事物有任何了解。

    

    在这篇文章中，我将考虑一系列问题。首先，这些问题涉及到智能的生物学功能，特别是人类智能的认知义肢。这将引出关于人类语言的问题，也许是人类迄今为止开发的最重要的认知义肢。虽然传统上对人类语言所包含的认知能力进行赞美，但我将强调人类语言多么有限，因此我们的认知能力也是有限的，尽管语言对其进行了增强。这将引出关于人类数学的问题，因为它最终是以人类语言的形式来表述的，所以也存在深层次的限制。然后，我将结合这些问题，对这篇文章的核心问题提出一个部分性的、有点侧面的答案：我们能够对我们甚至无法构想的事物有何了解？

    In this essay I will consider a sequence of questions. The first questions concern the biological function of intelligence in general, and cognitive prostheses of human intelligence in particular. These will lead into questions concerning human language, perhaps the most important cognitive prosthesis humanity has ever developed. While it is traditional to rhapsodize about the cognitive power encapsulated in human language, I will emphasize how horribly limited human language is -- and therefore how limited our cognitive abilities are, despite their being augmented with language. This will lead to questions of whether human mathematics, being ultimately formulated in terms of human language, is also deeply limited. I will then combine these questions to pose a partial, sort-of, sideways answer to the guiding concern of this essay: what we can ever discern about that we cannot even conceive?
    
[^199]: 弱监督关系抽取的表示学习

    Representation Learning for Weakly Supervised Relation Extraction. (arXiv:2105.00815v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2105.00815](http://arxiv.org/abs/2105.00815)

    本研究关注如何通过无监督预训练来改进在有限训练数据情况下的监督基线系统性能，并分析了传统手工特征在关系抽取中可能存在的数据稀疏性问题。

    

    近年来，信息抽取以及其中的子任务关系抽取取得了快速发展。关系抽取能够在句子中检测实体之间的语义关系。目前，许多高效的方法已被应用于关系抽取任务中。监督学习方法尤其具有良好的性能。然而，仍然存在许多难题。其中最严重的问题之一是手动标记数据难以获取。在大多数情况下，有限的训练数据等于较差的性能。因此，在只有有限训练数据的情况下，我们关注如何通过无监督预训练来改进我们的监督基线系统的性能。特征是改进监督方法的关键组成部分之一。传统方法通常使用手工特征，这些特征需要专业知识和昂贵的人力。然而，这种类型的特征可能会受到数据稀疏性的影响。

    Recent years have seen rapid development in Information Extraction, as well as its subtask, Relation Extraction. Relation Extraction is able to detect semantic relations between entities in sentences. Currently, many efficient approaches have been applied to relation extraction tasks. Supervised learning approaches especially have good performance. However, there are still many difficult challenges. One of the most serious problems is that manually labeled data is difficult to acquire. In most cases, limited data for supervised approaches equals lousy performance. Thus here, under the situation with only limited training data, we focus on how to improve the performance of our supervised baseline system with unsupervised pre-training. Feature is one of the key components in improving the supervised approaches. Traditional approaches usually apply hand-crafted features, which require expert knowledge and expensive human labor. However, this type of feature might suffer from data sparsity
    

