# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error](https://arxiv.org/abs/2403.04746) | 提出了一种受生物启发的方法，即模拟试错（STE），为工具增强型LLMs编排了三个关键机制以实现成功的工具使用行为 |
| [^2] | [How Far Are We from Intelligent Visual Deductive Reasoning?](https://arxiv.org/abs/2403.04732) | 目前的视觉语言模型在文本推理方面表现出色，但在视觉演绎推理方面仍存在较大差距和盲点。 |
| [^3] | [Common 7B Language Models Already Possess Strong Math Capabilities](https://arxiv.org/abs/2403.04706) | 通用7B语言模型LLaMA-2展现出强大数学能力，准确率分别达到97.7%和72.0%，扩大SFT数据可以提高生成正确答案的可靠性 |
| [^4] | [Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification](https://arxiv.org/abs/2403.04696) | 提出了一种基于标记级别不确定性量化的新型事实核查和幻觉检测流程，该方法能够检测大型语言模型输出中的不可靠预测。 |
| [^5] | [Greater than the sum of its parts: The role of minority and majority status in collaborative problem-solving communication](https://arxiv.org/abs/2403.04671) | 研究探讨了少数群体和非少数群体在协作解决问题任务中的沟通模式，并发现URM身份会影响个体的社会认知语言模式 |
| [^6] | [Telecom Language Models: Must They Be Large?](https://arxiv.org/abs/2403.04666) | 小型语言模型Phi-2在电信领域展示出与大型对应模型相媲美的性能，通过检索增强生成方法提升了其能力。 |
| [^7] | [Chain of Thought Explanation for Dialogue State Tracking](https://arxiv.org/abs/2403.04656) | 提出了一种名为Chain-of-Thought-Explanation（CoTE）的模型，用于对话状态跟踪(DST)任务，通过逐步创建详细解释来确定插槽值，从而实现更准确可靠的结果。 |
| [^8] | [Yi: Open Foundation Models by 01.AI](https://arxiv.org/abs/2403.04652) | Yi模型系列基于强大的多维能力，通过基于6B和34B预训练模型的扩展，包括聊天模型、长上下文模型、深度放大模型和视觉语言模型，取得了优异的性能。 |
| [^9] | [QAQ: Quality Adaptive Quantization for LLM KV Cache](https://arxiv.org/abs/2403.04643) | 提出了QAQ，一种用于KV缓存的质量自适应量化方案，理论上证明了关键缓存和值缓存对量化表现出不同的敏感性，因此制定了不同的量化策略。 |
| [^10] | [MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis](https://arxiv.org/abs/2403.04639) | 这项研究介绍了首个用于马加希语-印地语-英语代码混合情感分析任务的数据集，并通过语言学分析和统计研究来评估数据集的质量。 |
| [^11] | [MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder](https://arxiv.org/abs/2403.04626) | MedFLIP是一种用于医学分析的快速语言-图像预训练方法，通过引入SVD损失增强医学图像特征表示学习，验证了用语言可以提高零样本医学图像分析的性能。 |
| [^12] | [Strong Priority and Determinacy in Timed CCS](https://arxiv.org/abs/2403.04618) | 引入了一种新的调度机制“顺序构造减少”，旨在实现多播并发通信的确定性，扩展了CCS的技术设置，证明了构造减少的汇聚属性，展示了在一些语法限制下运算符的结构连贯性。 |
| [^13] | [Wiki-TabNER:Advancing Table Interpretation Through Named Entity Recognition](https://arxiv.org/abs/2403.04577) | 本文提出了一个新的挑战性数据集，并介绍了一个旨在解决实体链接任务的新问题：单元格内的命名实体识别，并提出了一个提示框架用于评估大型语言模型在这一新任务上的效果。 |
| [^14] | [Uncertainty-Aware Relational Graph Neural Network for Few-Shot Knowledge Graph Completion](https://arxiv.org/abs/2403.04521) | 提出一种不确定性感知的少样本知识图谱补全框架以模拟实体和三元组不确定性，通过学习服从高斯分布的表示来更好地理解有限数据。 |
| [^15] | [Where does In-context Translation Happen in Large Language Models](https://arxiv.org/abs/2403.04510) | 该研究在大型语言模型中探索了从上下文学习者到翻译模型的转变过程，并发现了"任务识别"点以及利用该点的冗余性可节约计算量。 |
| [^16] | [NLPre: a revised approach towards language-centric benchmarking of Natural Language Preprocessing systems](https://arxiv.org/abs/2403.04507) | NLPre提出了一种可靠且公平的语言中心基准测试方法，使得可以全面持续评估多个NLPre工具的性能，并可靠地跟踪其表现。 |
| [^17] | [GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability](https://arxiv.org/abs/2403.04483) | 该论文提出了一个名为GraphInstruct的基准，用于评估和增强大规模语言模型的图理解能力，并通过构建GraphLM和提出GraphLM+模型实现了显著的图推理能力增强。 |
| [^18] | [Do Large Language Model Understand Multi-Intent Spoken Language ?](https://arxiv.org/abs/2403.04481) | 该研究利用大型语言模型进行口语语言多目标理解，提出了改进实体槽和子目标指令的创新技术，并展示了LLMs在多目标SLU模型方面的潜力。 |
| [^19] | [Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset](https://arxiv.org/abs/2403.04460) | Pearl数据集利用了角色和知识增强的大型语言模型，提供了具体用户偏好，领域专业性和更相关的推荐。 |
| [^20] | [Low-Resource Court Judgment Summarization for Common Law Systems](https://arxiv.org/abs/2403.04454) | 提出了用于普通法系统的跨多司法管辖区法院判决文档摘要的第一个数据集CLSum，并首次采用大型语言模型（LLMs）进行判决摘要工作。 |
| [^21] | [Membership Inference Attacks and Privacy in Topic Modeling](https://arxiv.org/abs/2403.04451) | 主题建模中提出了会员推理攻击，通过差分隐私词汇选择来改善隐私风险 |
| [^22] | [Classist Tools: Social Class Correlates with Performance in NLP](https://arxiv.org/abs/2403.04445) | NLP的性能存在阶级偏见，经验证对较不发达的社会经济群体不利。 |
| [^23] | [Promising and worth-to-try future directions for advancing state-of-the-art surrogates methods of agent-based models in social and health computational sciences](https://arxiv.org/abs/2403.04417) | 强调一些适用于不同建模应用领域中非线性动态模型的代理模型，对于社会和健康计算科学领域的ABMs具有潜在的推动作用 |
| [^24] | [Exploring Continual Learning of Compositional Generalization in NLI](https://arxiv.org/abs/2403.04400) | 本文介绍了连续组合泛化挑战，探讨了模型如何持续获取原始推理任务的知识并进行组合推理，研究了不断学习对NLI中组合泛化的影响，并提出了解决方案。 |
| [^25] | [SGNet: Folding Symmetrical Protein Complex with Deep Learning](https://arxiv.org/abs/2403.04395) | SGNet提出了一种名为SGNet的蛋白质折叠框架，用于模拟对称组件中的蛋白质-蛋白质相互作用，解决了对称蛋白质组装中的结构确定和监督模糊性问题。 |
| [^26] | [Acceleron: A Tool to Accelerate Research Ideation](https://arxiv.org/abs/2403.04382) | 提出了一种名为“Acceleron”的工具，用于加速研究构想，帮助研究人员制定全面的研究提案，验证其创新性。 |
| [^27] | [Computational Modelling of Plurality and Definiteness in Chinese Noun Phrases](https://arxiv.org/abs/2403.04376) | 该论文研究汉语名词短语中数量与确定性标记的省略现象，发现汉语讲话者确实经常省略这些标记，这一现象在相关语境中的可预测性有所提高。 |
| [^28] | [From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction](https://arxiv.org/abs/2403.04369) | 引入领域知识的从图到词袋方法，帮助预测混淆罪名，通过构成要素和关键词选择进行判断。 |
| [^29] | [ProMoAI: Process Modeling with Generative AI](https://arxiv.org/abs/2403.04327) | ProMoAI利用大型语言模型自动生成过程模型，支持优化并通过用户反馈进行改进，是一种新颖的AI驱动的过程建模工具，降低了用户的技术门槛。 |
| [^30] | [Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models](https://arxiv.org/abs/2403.04325) | 引入了Composition Score，一种基于模型的度量标准，用于量化句子理解中的含义合成程度，实验证明这一度量与大脑区域相关，揭示了含义合成在人类句子理解中的多方面性。 |
| [^31] | [Discriminative Probing and Tuning for Text-to-Image Generation](https://arxiv.org/abs/2403.04321) | 加强T2I模型的判别能力，以实现更精确的文本到图像对齐生成。 |
| [^32] | [Online Adaptation of Language Models with a Memory of Amortized Contexts](https://arxiv.org/abs/2403.04317) | 提出了一种带有分摊上下文记忆的在线适应框架，可有效地提取、压缩并存储信息以保持强大的知识保留能力 |
| [^33] | [Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders](https://arxiv.org/abs/2403.04314) | 该论文提出了一个意图语义工具包，通过引入三元任务来评估模型对于否定和含义推断这两个实际对话系统中重要的语义概念的理解情况 |
| [^34] | [ALTO: An Efficient Network Orchestrator for Compound AI Systems](https://arxiv.org/abs/2403.04311) | ALTO是一个网络编排器，针对生成语言模型的优化机会，实现了高吞吐量和低延迟，同时解决了流式中间输出的两个新挑战：正确性和负载平衡。 |
| [^35] | [HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild](https://arxiv.org/abs/2403.04307) | HaluEval-Wild是第一个专门设计用于评估实际环境中LLM幻觉的基准测试，收集了具有挑战性的用户查询并分类为五种不同类型，可以对LLM表现出的幻觉类型进行细粒度分析。 |
| [^36] | [Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy](https://arxiv.org/abs/2403.04283) | 该论文提出Proxy-RLHF方法，通过将大型语言模型的生成和对齐过程解耦，实现了以更低计算成本对齐人类价值观，仅使用其他方法的1%训练参数即可达到可比水平的对齐度。 |
| [^37] | [A New Benchmark for Evaluating Automatic Speech Recognition in the Arabic Call Domain](https://arxiv.org/abs/2403.04280) | 该研究引入了一个针对阿拉伯电话对话领域挑战的全面评估基准，旨在为开发和评估能够适应真实电话通讯条件的ASR系统提供一个严格的测试平台。 |
| [^38] | [Advancing Biomedical Text Mining with Community Challenges](https://arxiv.org/abs/2403.04261) | 社区挑战评估竞赛在促进生物医学文本挖掘研究中的技术创新和跨学科合作方面起着重要作用。 |
| [^39] | [Can Small Language Models be Good Reasoners for Sequential Recommendation?](https://arxiv.org/abs/2403.04260) | 提出了逐步知识提取框架（SLIM），为顺序推荐系统解决了大型语言模型（LLMs）高资源需求的难题，使其能以资源高效的方式享受LLMs的出色推理能力。 |
| [^40] | [UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities](https://arxiv.org/abs/2403.04247) | 使用负种子实体进行超细粒度实体集扩展，解决了传统方法在超细粒度语义类别表示中的问题。 |
| [^41] | [DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning](https://arxiv.org/abs/2403.04233) | DEEP-ICL 提出了一种新颖的任务定义丰富的专家集成方法，通过从示范中提取任务定义并学习任务特定示例，实现了在上下文学习方面具有可比性的性能，突破了传统上下文学习的限制。 |
| [^42] | [Aligners: Decoupling LLMs and Alignment](https://arxiv.org/abs/2403.04224) | 提出了一种通过训练对齐器模型来解耦大型语言模型（LLMs）和对齐，以减少对齐对性能的潜在负面影响。 |
| [^43] | [Self-Evaluation of Large Language Model based on Glass-box Features](https://arxiv.org/abs/2403.04222) | 研究探讨了大型语言模型在自我评估中利用玻璃箱特征的实用性，发现softmax分布在质量评估中可靠，提出了通过引入参考特征增强评估的策略，并验证了使用玻璃箱特征进行大型语言模型自我评估的可行性。 |
| [^44] | [Persona Extraction Through Semantic Similarity for Emotional Support Conversation Generation](https://arxiv.org/abs/2403.04212) | 提出了一种新框架 PESS，通过语义相似性自动推断对话中的人设信息，以支持情感对话生成。 |
| [^45] | [On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models](https://arxiv.org/abs/2403.04204) | 调查了大型模型价值对齐方法，揭示历史背景和数学本质，详细讨论了强化学习、监督微调和上下文学习等对齐方法。 |
| [^46] | [Large Language Models are In-Context Molecule Learners](https://arxiv.org/abs/2403.04197) | 提出了上下文分子适应（ICMA）范式，允许LLMs通过上下文示例学习分子-文本对齐，解决了在分子-标题翻译任务中对LLMs的挑战。 |
| [^47] | [Generative AI for Synthetic Data Generation: Methods, Challenges and the Future](https://arxiv.org/abs/2403.04190) | 这里是中文总结出的一句话要点 |
| [^48] | [Metric-aware LLM inference](https://arxiv.org/abs/2403.04182) | 提出了度量感知的LLM推断方法，通过优化自定义指标来改进推断性能 |
| [^49] | [Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation](https://arxiv.org/abs/2403.04178) | 该论文介绍了一个具有印度英语重音注释的数据集，以及一个能够将重音融入合成语音中的文本到语音架构，用于在语音到语音机器翻译中实现应力传递。 |
| [^50] | [DA-Net: A Disentangled and Adaptive Network for Multi-Source Cross-Lingual Transfer Learning](https://arxiv.org/abs/2403.04158) | DA-Net 提出了一种解决多源跨语言迁移学习中共享编码器导致学习困扰和语言特定分类器性能下降的新方法。 |
| [^51] | [Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference](https://arxiv.org/abs/2403.04132) | Chatbot Arena是一个开放平台，采用两两比较的方式通过众包利用人类偏好评估LLMs。研究表明众包问题多样且具有区分性，并且人类投票与专家评级者的投票基本一致。 |
| [^52] | [Exploring LLM-based Agents for Root Cause Analysis](https://arxiv.org/abs/2403.04123) | 探索基于LLM的代理用于根本原因分析，以解决自动化RCA中无法动态收集额外诊断信息的限制。 |
| [^53] | [Can Large Language Models Reason and Plan?](https://arxiv.org/abs/2403.04121) | 大型语言模型缺乏自我批评能力，无法像人类一样纠正错误。 |
| [^54] | [Don't Blame the Data, Blame the Model: Understanding Noise and Bias When Learning from Subjective Annotations](https://arxiv.org/abs/2403.04085) | 该论文研究了从主观标注学习时，模型对高分歧数据实例表现低置信度的原因，并提出了使用多地面实况方法进行分类以提高对这些实例的置信度。 |
| [^55] | [Transformers and Language Models in Form Understanding: A Comprehensive Review of Scanned Document Analysis](https://arxiv.org/abs/2403.04080) | 本文调查了扫描文档形式理解领域的研究，重点介绍了语言模型和Transformers在解决这一挑战性任务中的重要性，展示了Transformers如何推动领域发展并革新形式理解技术。 |
| [^56] | [Semi-Supervised Dialogue Abstractive Summarization via High-Quality Pseudolabel Selection](https://arxiv.org/abs/2403.04073) | 提出了一种新的高质量伪标签选择方法 SiCF，通过该方法选择具有高质量生成摘要的无标签对话来进行半监督对话抽取式摘要生成。 |
| [^57] | [Can Large Language Models do Analytical Reasoning?](https://arxiv.org/abs/2403.04031) | 本研究探索了大型语言模型在体育领域的分析推理能力，发现在处理NBA和NFL比赛得分任务时，GPT-4和Claude-2.1表现最佳，采用分治法进行数据处理效果最好。 |
| [^58] | [Media Bias Matters: Understanding the Impact of Politically Biased News on Vaccine Attitudes in Social Media](https://arxiv.org/abs/2403.04009) | 本文分析了政治偏见新闻对疫苗态度的影响。通过深度学习和因果推断技术，揭示了社交媒体用户在疫苗立场上的不同行为，并发现中立立场的个体更容易受到政治偏见新闻的影响。 |
| [^59] | [SaulLM-7B: A pioneering Large Language Model for Law](https://arxiv.org/abs/2403.03883) | SaulLM-7B是首个专为法律文本设计的7B参数LLM，通过InnovativeFine-tuning方法，展现了领先的法律文件理解和处理能力。 |
| [^60] | [Emojinize : Enriching Any Text with Emoji Translations](https://arxiv.org/abs/2403.03857) | Emojinize 是一种方法，能够通过大型语言模型选择适当的表情符号翻译文本，提高猜测性，人类猜测可达55％。 |
| [^61] | [ShortGPT: Layers in Large Language Models are More Redundant Than You Expect](https://arxiv.org/abs/2403.03853) | 大语言模型中的层级存在较高相似性，有些层对网络功能几乎无影响。研究提出一种称为区块影响的度量，并通过层删除方法显著优于以往的模型修剪方法。 |
| [^62] | [A Modular Approach for Multimodal Summarization of TV Shows](https://arxiv.org/abs/2403.03823) | 提出了一种模块化方法用于多模态电视节目摘要，包括检测场景边界、重新排列场景、将视觉信息转换为文本、总结对话以及将场景摘要融合的过程，并引入了一个新的衡量摘要质量的评价指标PREFS。 |
| [^63] | [The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning](https://arxiv.org/abs/2403.03218) | WMDP基准是一个公开发布的数据集，包含4157个多项选择问题，用作生物安全、网络安全和化学安全危险知识的代理测量。 |
| [^64] | [PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset](https://arxiv.org/abs/2403.03167) | 论文提出了一个名为PARADISE的任务，通过实用程序文本进行演绎推理，评估语言模型仅从给定目标推断计划的能力 |
| [^65] | [OffLanDat: A Community Based Implicit Offensive Language Dataset Generated by Large Language Model Through Prompt Engineering](https://arxiv.org/abs/2403.02472) | 介绍了一个通过提示工程生成的大型语言模型创建的社区基础隐式攻击性语言数据集OffLanDat，为38个不同目标群体提供数据。 |
| [^66] | [Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge](https://arxiv.org/abs/2403.01432) | 本文研究了微调和检索增强生成两种方法对大型语言模型在处理低频实体问题回答任务中的影响，发现微调显著提高了各种受欢迎程度的实体的性能，而检索增强生成方法则超过了其他方法。 |
| [^67] | [LAB: Large-Scale Alignment for ChatBots](https://arxiv.org/abs/2403.01081) | 介绍了一种名为LAB的方法，旨在克服大型语言模型训练中的可扩展性挑战，通过分类法指导的合成数据生成和多阶段调整框架，实现了对昂贵人工标注和GPT-4等专有模型依赖较少的大规模对齐，提供了一种可扩展、具有成本效益的解决方案，不会出现灾难性遗忘情况，进一步增强了LLM的训练效率。 |
| [^68] | [Merging Text Transformer Models from Different Initializations](https://arxiv.org/abs/2403.00986) | 研究了合并不同初始化的Transformer模型的技术，提出了一种模型合并技术以研究这些模型极小值之间的关系，并发现与模型平均相比，通过我们的方法合并这些模型始终可以获得较低的损失障碍。 |
| [^69] | [Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs](https://arxiv.org/abs/2403.00858) | 通过提出的框架，我们训练了一种用于Llama 2 Chat 7B或更大模型的草案模型，实现了加速推理，仅占原始大小的1.64％。 |
| [^70] | [Mitigating Reversal Curse via Semantic-aware Permutation Training](https://arxiv.org/abs/2403.00758) | 逆转诅咒问题是导致因果语言模型无法进行双向推理的根本原因之一，在这篇论文中，我们提出了通过语义感知的置换训练来缓解这一问题。 |
| [^71] | [Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models](https://arxiv.org/abs/2402.18059) | 提出一种利用多目标优化方法的水印技术，通过轻量级网络生成特定令牌水印logits和分割比率，在保证检测性的同时提升了文本的语义完整性。 |
| [^72] | [Ranking Large Language Models without Ground Truth](https://arxiv.org/abs/2402.14860) | 不需要基准实况或参考响应的条件下，通过考虑模型的三元组来排名大型语言模型，并提出了两种排名方法。 |
| [^73] | [A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation](https://arxiv.org/abs/2402.13587) | 提出了一种用于电子商务产品描述生成的多模态上下文调整方法ModICT，通过引入相似产品样本和利用语言模型的上下文学习能力，旨在解决生成描述中常见且忽略产品特征的问题 |
| [^74] | [AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/abs/2402.12226) | AnyGPT是一个统一的多模态语言模型，通过离散表示实现各种模态的统一处理，能够在不改变大型语言模型架构或训练方式的情况下稳定训练，为新模态的无缝整合提供了可能。 |
| [^75] | [Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM](https://arxiv.org/abs/2402.11517) | Knowledge-to-SQL框架利用数据专家LLM提供有用知识，增强文本到SQL模型的鲁棒性。 |
| [^76] | [MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data](https://arxiv.org/abs/2402.08957) | 这项工作介绍了MUSTARD，一种掌握定理和证明数据统一合成的数据生成框架，通过三个阶段的合成，实现了高质量和多样化的问题和推理步骤的生成。 |
| [^77] | [English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts](https://arxiv.org/abs/2402.03223) | 本研究填补了一个研究空白，探讨了在非英文文本中应该使用哪种语言来提示情绪标签。 |
| [^78] | [Can Large Language Models Replace Economic Choice Prediction Labs?](https://arxiv.org/abs/2401.17435) | 该论文研究大型语言模型是否能够取代经济实验室进行选择预测，并通过相关实验证明了其可行性。 |
| [^79] | [Steering Llama 2 via Contrastive Activation Addition](https://arxiv.org/abs/2312.06681) | 引入Contrastive Activation Addition（CAA）方法，通过修改语言模型的激活来精确控制目标行为的程度，显著改变模型行为并在微调和系统提示设计的基础上提供额外有效性。 |
| [^80] | [Interpreting User Requests in the Context of Natural Language Standing Instructions](https://arxiv.org/abs/2311.09796) | 在使用大型语言模型的自然语言接口时，本研究提出了一种基于用户常设指令的对话建模方法，通过将用户的约束和偏好作为上下文，从而在类似请求中实现自动化。 |
| [^81] | [Can LLMs Follow Simple Rules?](https://arxiv.org/abs/2311.04235) | 提出了一个名为RuLES的程序框架，用于衡量LLMs在与用户交互时遵守规则的能力。 |
| [^82] | [LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay](https://arxiv.org/abs/2310.14985) | 本文提出了一个新颖的框架，旨在无缝适应Avalon游戏，通过多智能体系统实现了有效的沟通和互动，评估了智能体的性能和社会行为，展示了LLM智能体在游戏中具有潜力。 |
| [^83] | [Does Writing with Language Models Reduce Content Diversity?](https://arxiv.org/abs/2309.05196) | 写作时使用InstructGPT（而不是GPT3）会显著降低内容多样性，增加不同作者之间的相似性，并减少整体的词汇和内容多样性。 |
| [^84] | [Interactive Question Answering Systems: Literature Review](https://arxiv.org/abs/2209.01621) | 交互式问答系统是问答和对话系统的结合，用户可以用自然语言提问并与系统动态交互，获得更精确的结果。 |
| [^85] | [Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation](https://arxiv.org/abs/2109.14200) | 研究探讨了语言学习中，通过跨情境视听学习，音素、音节和单词能否作为副产品出现，并支持不同形式表征间的转换。 |
| [^86] | [Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge.](http://arxiv.org/abs/2401.10712) | 本论文提出了一种叫做Q&A提示的方法，通过挖掘图像中的问题-回答对来发现丰富的视觉线索，以帮助AI模型更好地理解复杂视觉问题，提高跨模态推理能力。 |
| [^87] | [AST-T5: Structure-Aware Pretraining for Code Generation and Understanding.](http://arxiv.org/abs/2401.03003) | AST-T5是一种结构感知的预训练模型，通过利用抽象语法树（AST）来增强代码生成、转换和理解的能力。它优于其他同等大小的语言模型，并在代码到代码任务中表现出色。 |
| [^88] | [Quality and Quantity of Machine Translation References for Automated Metrics.](http://arxiv.org/abs/2401.01283) | 本研究发现，机器翻译评估的较高质量参考文献对于评估指标与人类评价之间的相关性更好。每个段落平均使用7个参考文献有助于提升所有评估指标。不同质量的供应商参考文献可以混合使用来提高评估指标的准确性。这些发现可用于在特定预算下创建参考文献的共享任务的评估者。 |
| [^89] | [ChipNeMo: Domain-Adapted LLMs for Chip Design.](http://arxiv.org/abs/2311.00176) | ChipNeMo通过领域自适应技术，实现了在工业芯片设计中大幅提升LLM性能，同时减小了模型尺寸，在工程助手、脚本生成和缺陷分析等方面具有良好表现。 |
| [^90] | [GenTKG: Generative Forecasting on Temporal Knowledge Graph.](http://arxiv.org/abs/2310.07793) | 研究提出了一种名为GenTKG的生成模型，用于在时间知识图谱上进行预测。该模型通过结合基于时间逻辑规则的检索策略和轻量级的参数效率指导，克服了复杂的时间图数据结构和庞大的数据量所带来的挑战。 |
| [^91] | [Unsupervised Fact Verification by Language Model Distillation.](http://arxiv.org/abs/2309.16540) | 本文提出了一种名为SFAVEL的无监督框架，通过语言模型蒸馏将自监督特征转化为高质量的主张-事实对齐，实现无监督事实验证。这通过一种新颖的对比损失函数实现，同时保留语料库间的语义关系。 |
| [^92] | [Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks.](http://arxiv.org/abs/2309.16347) | 本文提出了基于大型语言模型的内在引导探索（IGE-LLMs）框架，通过利用LLMs作为辅助内在奖励，解决了复杂长视程机器人操作任务中奖励稀疏问题，并在实验中展示了其较高的性能和模块化特性。 |
| [^93] | [CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought.](http://arxiv.org/abs/2309.11143) | CoT-BERT提出了一种通过思维链条增强无监督句子表示的方法，通过两个阶段的处理，引入思维链条的概念进行向量化，以提高模型性能。 |
| [^94] | [SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects.](http://arxiv.org/abs/2309.07445) | 本研究提出了SIB-200数据集，在200多种语言和方言中提供了一个大规模、全面的主题分类评估数据集。该数据集填补了自然语言理解领域中对评估数据集的缺乏，通过全监督、跨语言迁移和大型语言模型提示的评估，发现性能仍存在差距。 |
| [^95] | [Benchmarking Procedural Language Understanding for Low-Resource Languages: A Case Study on Turkish.](http://arxiv.org/abs/2309.06698) | 本研究通过对土耳其语的程序文本进行案例研究，扩展了土耳其wikiHow的教程数量，并研究了几个下游任务。研究发现，在大多数任务中，语言特定模型相对于多语言模型具有明显优势。 |
| [^96] | [Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning.](http://arxiv.org/abs/2309.06553) | 这项工作介绍了一种基于离线逆向强化学习的提示评估与优化方法，通过利用离线数据集和逆向强化学习，预测提示性能、提高成本效益、生成易读的结果。 |
| [^97] | [A new mapping of technological interdependence.](http://arxiv.org/abs/2308.00014) | 本文利用文本挖掘和网络分析的方法，研究了不同部门之间的技术相互依赖关系，并证明了在技术创新中，间接联系和直接联系同等重要。 |
| [^98] | [Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework.](http://arxiv.org/abs/2307.01715) | 本文提出了一个通用的插入式框架，用于优化CTC模型中的所需属性。该框架通过补充额外的损失项来优先考虑符合所需属性的对齐，并不需要修改CTC损失函数。 |
| [^99] | [Constructing Colloquial Dataset for Persian Sentiment Analysis of Social Microblogs.](http://arxiv.org/abs/2306.12679) | 本文构建了一个60,000条波斯语的社交微博口语文本数据集，提出了一种新的深度卷积神经网络(CNN)模型，用于更有效地分析社交微博中的口语文本情感。 |
| [^100] | [Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A Comprehensive Framework and Dataset.](http://arxiv.org/abs/2305.16344) | 本文提出了一个自动化财务信息提取的框架（AFIE），用于提取混合长文档中的关键业绩指标（KPI）。该框架利用LLMs增强了财务报告信息的理解和提取能力，并经过了广泛的实验验证，证明其在GPT-3.5和GPT-4上的有效性，相对于朴素方法，平均精度提高了53.94％和33.77％。 |
| [^101] | [Self-Critique Prompting with Large Language Models for Inductive Instructions.](http://arxiv.org/abs/2305.13733) | 本研究提出了一个基准，名为INDust，用于评估大型语言模型（LLMs）对于包含错误信息的指令的抵抗能力。研究发现，当前的LLMs很容易被欺骗，因此采用自我批判提示的方法来激励LLMs不仅对自己进行批评，而且对用户进行批评。 |
| [^102] | [Exploring Challenges of Deploying BERT-based NLP Models in Resource-Constrained Embedded Devices.](http://arxiv.org/abs/2304.11520) | 本文探究了在资源受限的嵌入式设备上部署基于BERT的NLP模型的挑战，并得出结论：虽然DistilBERT和TinyBERT等轻量级模型相对占用更少内存，但它们在复杂的NLP任务上表现较差；ResNet-based BERT模型可以在精度和资源效率之间取得良好的平衡，适合在嵌入式设备上部署。 |
| [^103] | [EasyNER: A Customizable Easy-to-Use Pipeline for Deep Learning- and Dictionary-based Named Entity Recognition from Medical Text.](http://arxiv.org/abs/2304.07805) | EasyNER是一种用于在医学研究文章中识别命名实体的端到端工具。它基于深度学习模型和字典方法，并且易于使用和定制。在COVID-19相关文章数据集上的应用证明了其可以准确地识别所需实体。 |

# 详细

[^1]: 在幻境中的大型语言模型：通过模拟试错学习工具

    LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error

    [https://arxiv.org/abs/2403.04746](https://arxiv.org/abs/2403.04746)

    提出了一种受生物启发的方法，即模拟试错（STE），为工具增强型LLMs编排了三个关键机制以实现成功的工具使用行为

    

    工具对于大型语言模型（LLMs）获取最新信息并在外部环境中采取重要行动至关重要。现有关于工具增强型LLMs的工作主要集中在工具的广泛覆盖范围和灵活性上。然而，一个被人意外忽视的关键方面是LLM在经过训练后如何准确使用工具。我们发现，包括GPT-4和专门为工具使用进行微调的开源LLMs在正确率方面仅达到30%到60%的范围，远不足以在实践中可靠使用。我们提出了一种受生物启发的方法，即模拟试错（STE），为工具增强型LLMs编排了三个关键机制以实现成功的工具使用行为：试错、想象和记忆。具体而言，STE利用LLM的“想象力”来模拟使用工具的可能场景，

    arXiv:2403.04746v1 Announce Type: cross  Abstract: Tools are essential for large language models (LLMs) to acquire up-to-date information and take consequential actions in external environments. Existing work on tool-augmented LLMs primarily focuses on the broad coverage of tools and the flexibility of adding new tools. However, a critical aspect that has surprisingly been understudied is simply how accurately an LLM uses tools for which it has been trained. We find that existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice. We propose a biologically inspired method for tool-augmented LLMs, simulated trial and error (STE), that orchestrates three key mechanisms for successful tool use behaviors in the biological system: trial and error, imagination, and memory. Specifically, STE leverages an LLM's 'imagination' to simulate plausible scenarios for using a tool,
    
[^2]: 我们距离智能视觉演绎推理还有多远？

    How Far Are We from Intelligent Visual Deductive Reasoning?

    [https://arxiv.org/abs/2403.04732](https://arxiv.org/abs/2403.04732)

    目前的视觉语言模型在文本推理方面表现出色，但在视觉演绎推理方面仍存在较大差距和盲点。

    

    最近，诸如GPT-4V之类的视觉语言模型（VLM）在各种视觉语言任务上取得了巨大进展。我们深入探讨了基于视觉的演绎推理，这是一个更复杂但不太被探索的领域，并发现了当前领先的VLM中以前未暴露的盲点。具体来说，我们利用瑞文渐进矩阵（RPM）来评估VLM在仅依靠视觉线索进行多跳关系和演绎推理的能力。我们对几种流行的VLM进行了全面评估，采用了标准策略，如上下文学习、自我一致性和思维链（CoT），在三个不同的数据集上进行了评估，包括Mensa智商测试、智商测试和RAVEN。结果表明，尽管LLM在基于文本的推理方面具有令人印象深刻的能力，但我们在视觉演绎推理方面仍有很大的差距。

    arXiv:2403.04732v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated incredible strides on diverse vision language tasks. We dig into vision-based deductive reasoning, a more sophisticated but less explored realm, and find previously unexposed blindspots in the current SOTA VLMs. Specifically, we leverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop relational and deductive reasoning relying solely on visual clues. We perform comprehensive evaluations of several popular VLMs employing standard strategies such as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test, IntelligenceTest, and RAVEN. The results reveal that despite the impressive capabilities of LLMs in text-based reasoning, we are still far from achieving comparable proficiency in visual deductive reasoning. We found that certain standard strategies that are effective
    
[^3]: 通用7B语言模型已经具备强大的数学能力

    Common 7B Language Models Already Possess Strong Math Capabilities

    [https://arxiv.org/abs/2403.04706](https://arxiv.org/abs/2403.04706)

    通用7B语言模型LLaMA-2展现出强大数学能力，准确率分别达到97.7%和72.0%，扩大SFT数据可以提高生成正确答案的可靠性

    

    先前人们相信通用语言模型只有在非常大的规模上或需要大量与数学相关的预训练才能展现出数学能力。本文表明，具有通用预训练的LLaMA-2 7B模型已经表现出强大的数学能力，其在GSM8K和MATH基准测试上选择256个随机生成的最佳响应时，准确率分别为97.7%和72.0%。目前基础模型的主要问题在于难以一致地引出其固有的数学能力。值得注意的是，对于第一个答案的准确率分别下降到了49.5%和7.9%。我们发现，简单地扩大SFT数据可以显著提高生成正确答案的可靠性。然而，广泛扩展的潜力受到公开可用数学问题的稀缺性的限制。为了克服这一限制

    arXiv:2403.04706v1 Announce Type: cross  Abstract: Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pre-training already exhibits strong mathematical abilities, as evidenced by its impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks, respectively, when selecting the best response from 256 random generations. The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities. Notably, the accuracy for the first answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks, respectively. We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers. However, the potential for extensive scaling is constrained by the scarcity of publicly available math questions. To overcome this limitatio
    
[^4]: 通过标记级别的不确定性量化检验大型语言模型的输出

    Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification

    [https://arxiv.org/abs/2403.04696](https://arxiv.org/abs/2403.04696)

    提出了一种基于标记级别不确定性量化的新型事实核查和幻觉检测流程，该方法能够检测大型语言模型输出中的不可靠预测。

    

    大型语言模型(LLMs)以产生错误的声明而臭名昭著。这种幻觉可能很危险，因为在生成的文本中偶尔出现的事实不准确可能会被整体上是事实的文本掩盖，这使得用户极其难以发现。利用LLMs的当前服务通常不提供检测不可靠生成的方式。在这里，我们旨在弥补这一空白。具体而言，我们提出了一种基于标记级别的不确定性量化的新型事实核查和幻觉检测流程。不确定性分数利用了神经网络或其层输出中包含的信息来检测不可靠的预测，并我们展示它们可以用于核查LLM输出中的各种声明。此外，我们提出了一种新型的标记级别不确定性量化方法，消除了对事实提出怀疑的影响。

    arXiv:2403.04696v1 Announce Type: cross  Abstract: Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factual, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what c
    
[^5]: 其部分之和大于其整体：少数群体和多数群体在协作解决问题的沟通中的作用

    Greater than the sum of its parts: The role of minority and majority status in collaborative problem-solving communication

    [https://arxiv.org/abs/2403.04671](https://arxiv.org/abs/2403.04671)

    研究探讨了少数群体和非少数群体在协作解决问题任务中的沟通模式，并发现URM身份会影响个体的社会认知语言模式

    

    协作解决问题（CPS）是一项关键技能，既在职场中使用，也在教育环境中使用。CPS对于解决日益复杂的全球、经济和政治问题非常有用，并被认为是21世纪的核心技能。日益连接的全球社区为具有不同视角的创造性和协作式问题解决互动与解决方案提供了丰富的机会。不幸的是，妇女和少数群体（URMs）在协作互动中经常面临障碍，这些障碍阻碍了他们在这些问题解决对话中的重要参与。在这里，我们探讨了少数群体和非少数群体在CPS任务中共同工作的沟通模式。集团沟通分析（GCA），一种时间敏感的计算语言工具，被用来研究URM身份如何影响个体的社会认知语言模式。结果显示了差异。

    arXiv:2403.04671v1 Announce Type: new  Abstract: Collaborative problem-solving (CPS) is a vital skill used both in the workplace and in educational environments. CPS is useful in tackling increasingly complex global, economic, and political issues and is considered a central 21st century skill. The increasingly connected global community presents a fruitful opportunity for creative and collaborative problem-solving interactions and solutions that involve diverse perspectives. Unfortunately, women and underrepresented minorities (URMs) often face obstacles during collaborative interactions that hinder their key participation in these problem-solving conversations. Here, we explored the communication patterns of minority and non-minority individuals working together in a CPS task. Group Communication Analysis (GCA), a temporally-sensitive computational linguistic tool, was used to examine how URM status impacts individuals' sociocognitive linguistic patterns. Results show differences acr
    
[^6]: 电信语言模型：它们必须庞大吗？

    Telecom Language Models: Must They Be Large?

    [https://arxiv.org/abs/2403.04666](https://arxiv.org/abs/2403.04666)

    小型语言模型Phi-2在电信领域展示出与大型对应模型相媲美的性能，通过检索增强生成方法提升了其能力。

    

    电信部门对庞大语言模型（LLMs）的日益关注凸显了它们在改变运营效率方面的潜力。然而，部署这些复杂模型往往受到其巨大体积和计算需求的影响，引发了对它们在资源受限环境中可行性的担忧。为了解决这一挑战，最近的进展出现了一批小型语言模型，令人惊讶的是它们在许多任务中表现与其较大对应物相当，比如编码和常识推理。Phi-2是一种紧凑但功能强大的模型，它体现了这一系列高效小型语言模型的新浪潮。本文对Phi-2在电信领域内在本质上的理解进行了全面评估。鉴于规模相关限制，我们通过检索增强生成方法，精心增强了Phi-2的能力。

    arXiv:2403.04666v1 Announce Type: new  Abstract: The increasing interest in Large Language Models (LLMs) within the telecommunications sector underscores their potential to revolutionize operational efficiency. However, the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments. Addressing this challenge, recent advancements have seen the emergence of small language models that surprisingly exhibit performance comparable to their larger counterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a compact yet powerful model, exemplifies this new wave of efficient small language models. This paper conducts a comprehensive evaluation of Phi-2's intrinsic understanding of the telecommunications domain. Recognizing the scale-related limitations, we enhance Phi-2's capabilities through a Retrieval-Augmented Generation approach, meticulously i
    
[^7]: 对话状态跟踪的思维链解释

    Chain of Thought Explanation for Dialogue State Tracking

    [https://arxiv.org/abs/2403.04656](https://arxiv.org/abs/2403.04656)

    提出了一种名为Chain-of-Thought-Explanation（CoTE）的模型，用于对话状态跟踪(DST)任务，通过逐步创建详细解释来确定插槽值，从而实现更准确可靠的结果。

    

    对话状态跟踪(DST)旨在记录用户在会话互动期间提出的查询和目标，通过维护一组预定义的插槽及其对应的值来实现。当前的方法以不透明方式决定插槽值，而人类通常采用更谨慎的方法，从相关的对话轮中收集信息，然后推理出适当的值。在这项工作中，我们聚焦于通过提出一个名为Chain-of-Thought-Explanation（CoTE）的模型，来解决确定插槽值所需的步骤。CoTE建立在生成式DST框架之上，旨在在确定插槽值后逐步创建详细解释。这一过程导致了更准确可靠的插槽值。此外，为了提高CoTE的推理能力，我们进一步通过自动改写构建更流畅高质量的解释，从而形成了CoTE-refined方法。

    arXiv:2403.04656v1 Announce Type: new  Abstract: Dialogue state tracking (DST) aims to record user queries and goals during a conversational interaction achieved by maintaining a prede- fined set of slots and their corresponding values. Current approaches decide slot values opaquely, while humans usually adopt a more deliberate approach by collecting information from relevant dialogue turns and then reasoning the appropriate values. In this work, we focus on the steps needed to figure out slot values by proposing a model named Chain-of-Thought-Explanation (CoTE) for the DST task. CoTE, which is built on the generative DST framework, is designed to create detailed explanations step by step after determining the slot values. This process leads to more accurate and reliable slot values. More-over, to improve the reasoning ability of the CoTE, we further construct more fluent and high-quality explanations with automatic paraphrasing, leading the method CoTE-refined. Experimental results on
    
[^8]: Yi: 由 01.AI 推出的开放基础模型

    Yi: Open Foundation Models by 01.AI

    [https://arxiv.org/abs/2403.04652](https://arxiv.org/abs/2403.04652)

    Yi模型系列基于强大的多维能力，通过基于6B和34B预训练模型的扩展，包括聊天模型、长上下文模型、深度放大模型和视觉语言模型，取得了优异的性能。

    

    我们介绍了Yi模型系列，这是一系列具有强大多维能力的语言和多模态模型。Yi模型系列基于6B和34B的预训练语言模型，然后我们将它们扩展为聊天模型、200K长上下文模型、深度放大模型和视觉语言模型。我们的基础模型在诸如MMLU之类的各种基准测试中表现出色，而我们微调过的聊天模型在AlpacaEval和Chatbot Arena等主要评估平台上具有较高的人类偏好率。通过依赖于我们的可扩展超级计算基础设施和经典的Transformer架构，我们认为Yi模型的性能主要归因于其数据质量，这是由我们的数据工程工作所带来的。对于预训练，我们使用级联的数据去重和质量过滤流水线构建了3100亿个英文和中文语料库的标记。对于微调，我们对小规模模型进行了改进

    arXiv:2403.04652v1 Announce Type: cross  Abstract: We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less th
    
[^9]: QAQ：用于LLM KV缓存的质量自适应量化

    QAQ: Quality Adaptive Quantization for LLM KV Cache

    [https://arxiv.org/abs/2403.04643](https://arxiv.org/abs/2403.04643)

    提出了QAQ，一种用于KV缓存的质量自适应量化方案，理论上证明了关键缓存和值缓存对量化表现出不同的敏感性，因此制定了不同的量化策略。

    

    LLM的出现在NLP应用中引发了一波新的突破，尤其在诸如问答系统和文本生成等领域。随着对更长上下文的需求增长，模型部署中出现了一个重要瓶颈，即由于上下文长度的线性增加而导致的Key-Value (KV) cache的扩展。现有方法主要依赖于各种假设，例如根据注意力分数对KV cache进行排序以进行替换或驱逐，以压缩KV cache并提高模型吞吐量。然而，这些策略使用的启发式方法可能会错误地驱逐关键的KV缓存，从而严重降低模型性能。本文提出了QAQ，一种用于KV缓存的质量自适应量化方案。我们在理论上证明了关键缓存和值缓存对量化表现出不同的敏感性，从而引发了针对它们的非均匀量化策略的制定。

    arXiv:2403.04643v1 Announce Type: new  Abstract: The emergence of LLMs has ignited a fresh surge of breakthroughs in NLP applications, particularly in domains such as question-answering systems and text generation. As the need for longer context grows, a significant bottleneck in model deployment emerges due to the linear expansion of the Key-Value (KV) cache with the context length. Existing methods primarily rely on various hypotheses, such as sorting the KV cache based on attention scores for replacement or eviction, to compress the KV cache and improve model throughput. However, heuristics used by these strategies may wrongly evict essential KV cache, which can significantly degrade model performance. In this paper, we propose QAQ, a Quality Adaptive Quantization scheme for the KV cache. We theoretically demonstrate that key cache and value cache exhibit distinct sensitivities to quantization, leading to the formulation of separate quantization strategies for their non-uniform quan
    
[^10]: MaCmS：马加希代码混合数据集用于情感分析

    MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis

    [https://arxiv.org/abs/2403.04639](https://arxiv.org/abs/2403.04639)

    这项研究介绍了首个用于马加希语-印地语-英语代码混合情感分析任务的数据集，并通过语言学分析和统计研究来评估数据集的质量。

    

    这篇论文介绍了一种新的情感数据集 MaCMS，用于马加希语-印地语-英语（MHE）代码混合语言，其中马加希语是一种资源较少的少数民族语言。这个数据集是用于情感分析任务的第一个马加希语-印地语-英语代码混合数据集。此外，我们还对数据集进行了语言学分析，以了解代码混合的结构，并进行了统计研究，以了解不同极性发言者的语言偏好。通过这些分析，我们还训练了基准模型来评估数据集的质量。

    arXiv:2403.04639v1 Announce Type: new  Abstract: The present paper introduces new sentiment data, MaCMS, for Magahi-Hindi-English (MHE) code-mixed language, where Magahi is a less-resourced minority language. This dataset is the first Magahi-Hindi-English code-mixed dataset for sentiment analysis tasks. Further, we also provide a linguistics analysis of the dataset to understand the structure of code-mixing and a statistical study to understand the language preferences of speakers with different polarities. With these analyses, we also train baseline models to evaluate the dataset's quality.
    
[^11]: MedFLIP：医学视觉与语言自监督快速预训练与掩蔽自编码器

    MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder

    [https://arxiv.org/abs/2403.04626](https://arxiv.org/abs/2403.04626)

    MedFLIP是一种用于医学分析的快速语言-图像预训练方法，通过引入SVD损失增强医学图像特征表示学习，验证了用语言可以提高零样本医学图像分析的性能。

    

    在医学分析领域，广泛的研究探讨了掩蔽自编码器（MAEs）和多模态数据之间互相学习的潜力。然而，MAEs对跨模态学习的影响仍然是一个关键挑战。我们引入了MedFLIP，一种用于医学分析的快速语言-图像预训练方法。我们探索使用MAEs进行跨领域零样本学习，从而增强模型在医学诊断中常见的有限数据中学习的能力。我们验证了对图像进行掩蔽不会影响跨模态学习。此外，我们提出了SVD损失以增强医学图像特征的表示学习，旨在通过利用这类数据的结构复杂性来提高分类准确性。最后，我们验证了使用语言将提高医学图像分析的零样本性能。MedFLIP对掩蔽过程的扩展标志着该领域的进步。

    arXiv:2403.04626v1 Announce Type: cross  Abstract: Within the domain of medical analysis, extensive research has explored the potential of mutual learning between Masked Autoencoders(MAEs) and multimodal data. However, the impact of MAEs on intermodality remains a key challenge. We introduce MedFLIP, a Fast Language-Image Pre-training method for Medical analysis. We explore MAEs for zero-shot learning with crossed domains, which enhances the model ability to learn from limited data, a common scenario in medical diagnostics. We verify that masking an image does not affect intermodal learning. Furthermore, we propose the SVD loss to enhance the representation learning for characteristics of medical images, aiming to improve classification accuracy by leveraging the structural intricacies of such data. Lastly, we validate using language will improve the zero-shot performance for the medical image analysis. MedFLIP scaling of the masking process marks an advancement in the field, offering 
    
[^12]: 时标CCS中的强优先级和确定性

    Strong Priority and Determinacy in Timed CCS

    [https://arxiv.org/abs/2403.04618](https://arxiv.org/abs/2403.04618)

    引入了一种新的调度机制“顺序构造减少”，旨在实现多播并发通信的确定性，扩展了CCS的技术设置，证明了构造减少的汇聚属性，展示了在一些语法限制下运算符的结构连贯性。

    

    在具有优先级的经典进程代数理论的基础上，我们确定了一种名为“顺序构造减少”的新调度机制，旨在捕捉同步编程的本质。这种评估策略的独特属性是通过构造实现多播并发通信的确定性。特别是，这使我们能够模拟具有对缺失反应的共享内存多线程，因为它是Esterel编程语言的核心。在通过时钟和优先级扩展的CCS的技术设置中，对于我们称为“结构连贯”的大类过程，我们证明了构造减少的汇聚属性。我们进一步展示，在一些称为“可枢纽”的语法限制下，前缀、求和、并行组成、限制和隐藏的运算符保持结构连贯。这涵盖了一个严格更大的过程类。

    arXiv:2403.04618v1 Announce Type: cross  Abstract: Building on the classical theory of process algebra with priorities, we identify a new scheduling mechanism, called "sequentially constructive reduction" which is designed to capture the essence of synchronous programming. The distinctive property of this evaluation strategy is to achieve determinism-by-construction for multi-cast concurrent communication. In particular, it permits us to model shared memory multi-threading with reaction to absence as it lies at the core of the programming language Esterel. In the technical setting of CCS extended by clocks and priorities, we prove for a large class of processes, which we call "structurally coherent" the confluence property for constructive reductions. We further show that under some syntactic restrictions, called "pivotable" the operators of prefix, summation, parallel composition, restriction and hiding preserve structural coherence. This covers a strictly larger class of processes co
    
[^13]: Wiki-TabNER:通过命名实体识别推进表格解释

    Wiki-TabNER:Advancing Table Interpretation Through Named Entity Recognition

    [https://arxiv.org/abs/2403.04577](https://arxiv.org/abs/2403.04577)

    本文提出了一个新的挑战性数据集，并介绍了一个旨在解决实体链接任务的新问题：单元格内的命名实体识别，并提出了一个提示框架用于评估大型语言模型在这一新任务上的效果。

    

    arXiv:2403.04577v1 发布类型：新摘要：网络表格包含大量宝贵知识，激发了旨在解决表格解释（TI）任务的表格语言模型。本文分析了用于评估TI任务的广泛使用的基准数据集，特别关注实体链接任务。我们的分析显示，该数据集过于简化，可能降低其用于全面评估的有效性，并未准确代表表格在现实世界中的外观。为克服这一缺点，我们构建并注释了一个更具挑战性的新数据集。除了介绍新数据集外，我们还介绍了一个旨在解决实体链接任务的新问题：单元格内的命名实体识别。最后，我们提出了一个提示框架，用于评估新开发的大型语言模型（LLMs）在这一新的TI任务上。我们在各种设置下对提示LLMs进行实验证明，其中我们同时使用了随机

    arXiv:2403.04577v1 Announce Type: new  Abstract: Web tables contain a large amount of valuable knowledge and have inspired tabular language models aimed at tackling table interpretation (TI) tasks. In this paper, we analyse a widely used benchmark dataset for evaluation of TI tasks, particularly focusing on the entity linking task. Our analysis reveals that this dataset is overly simplified, potentially reducing its effectiveness for thorough evaluation and failing to accurately represent tables as they appear in the real-world. To overcome this drawback, we construct and annotate a new more challenging dataset. In addition to introducing the new dataset, we also introduce a novel problem aimed at addressing the entity linking task: named entity recognition within cells. Finally, we propose a prompting framework for evaluating the newly developed large language models (LLMs) on this novel TI task. We conduct experiments on prompting LLMs under various settings, where we use both random
    
[^14]: 不确定性感知关系图神经网络用于少样本知识图谱补全

    Uncertainty-Aware Relational Graph Neural Network for Few-Shot Knowledge Graph Completion

    [https://arxiv.org/abs/2403.04521](https://arxiv.org/abs/2403.04521)

    提出一种不确定性感知的少样本知识图谱补全框架以模拟实体和三元组不确定性，通过学习服从高斯分布的表示来更好地理解有限数据。

    

    少样本知识图谱补全旨在在给定少量案例参考实体对的情况下查询某种关系的未知事实。本文提出了一种新颖的不确定性感知少样本知识图谱补全框架（UFKGC），以模拟不确定性，更好地理解有限数据，通过学习服从高斯分布的表示来避免由实体和三元组不确定性导致的噪声副作用，先为源实体对的不确定性范围设计不确定性表示，然后将特征表示转换为高斯分布。为了更好地整合具有不确定特征的邻居以用于实体特征，我们设计了一种不确定性感知的关系图神经网络（UR-GNN）进行卷积操作。

    arXiv:2403.04521v1 Announce Type: new  Abstract: Few-shot knowledge graph completion (FKGC) aims to query the unseen facts of a relation given its few-shot reference entity pairs. The side effect of noises due to the uncertainty of entities and triples may limit the few-shot learning, but existing FKGC works neglect such uncertainty, which leads them more susceptible to limited reference samples with noises. In this paper, we propose a novel uncertainty-aware few-shot KG completion framework (UFKGC) to model uncertainty for a better understanding of the limited data by learning representations under Gaussian distribution. Uncertainty representation is first designed for estimating the uncertainty scope of the entity pairs after transferring feature representations into a Gaussian distribution. Further, to better integrate the neighbors with uncertainty characteristics for entity features, we design an uncertainty-aware relational graph neural network (UR-GNN) to conduct convolution ope
    
[^15]: 大型语言模型中的上下文翻译发生在哪里

    Where does In-context Translation Happen in Large Language Models

    [https://arxiv.org/abs/2403.04510](https://arxiv.org/abs/2403.04510)

    该研究在大型语言模型中探索了从上下文学习者到翻译模型的转变过程，并发现了"任务识别"点以及利用该点的冗余性可节约计算量。

    

    自监督大型语言模型已经展示出能够通过上下文学习执行机器翻译（MT）的能力，但关于模型在何处执行这一任务相对于提示指令和演示示例的情况知之甚少。在这项工作中，我们试图表征大型语言模型从上下文学习者转变为翻译模型的区域。通过一系列在GPTNeo2.7B、Bloom3B、Llama7b和Llama7b-chat上的逐层上下文屏蔽实验，我们展示了"任务识别"点的证据，即翻译任务被编码到输入表示中，并且不再需要关注上下文。我们进一步观察到完全屏蔽层时低性能与任务识别层之间的对应关系。利用这种冗余性在提示5个示例时节约了45%的计算量。

    arXiv:2403.04510v1 Announce Type: cross  Abstract: Self-supervised large language models have demonstrated the ability to perform Machine Translation (MT) via in-context learning, but little is known about where the model performs the task with respect to prompt instructions and demonstration examples. In this work, we attempt to characterize the region where large language models transition from in-context learners to translation models. Through a series of layer-wise context-masking experiments on \textsc{GPTNeo2.7B}, \textsc{Bloom3B}, \textsc{Llama7b} and \textsc{Llama7b-chat}, we demonstrate evidence of a "task recognition" point where the translation task is encoded into the input representations and attention to context is no longer necessary. We further observe correspondence between the low performance when masking out entire layers, and the task recognition layers. Taking advantage of this redundancy results in 45\% computational savings when prompting with 5 examples, and tas
    
[^16]: NLPre: 一种面向自然语言处理系统的语言中心基准测试方法的改进

    NLPre: a revised approach towards language-centric benchmarking of Natural Language Preprocessing systems

    [https://arxiv.org/abs/2403.04507](https://arxiv.org/abs/2403.04507)

    NLPre提出了一种可靠且公平的语言中心基准测试方法，使得可以全面持续评估多个NLPre工具的性能，并可靠地跟踪其表现。

    

    随着基于变压器的架构的不断发展，我们观察到自然语言处理（NLPre）工具的崛起，这些工具能够解决初步的NLP任务（例如标记化、词性标注、依存句法分析或形态分析）而无需任何外部语言指导。在费时费力地比较新型解决方案与依赖基于规则的形态分析器或词典的成熟预处理工具包的基础上是很困难的。鉴于现有NLPre评估方法的缺陷，我们探讨了一种可靠和公平的评估方法和性能报告。受GLUE基准测试的启发，提出的语言中心基准测试系统实现了对多个NLPre工具的全面持续评估，同时可靠地跟踪它们的性能。原型应用程序被配置为波兰语，并与精心组织的NLPre-PL基准套件集成。基于这一基准，我们进行了一项实验...

    arXiv:2403.04507v1 Announce Type: new  Abstract: With the advancements of transformer-based architectures, we observe the rise of natural language preprocessing (NLPre) tools capable of solving preliminary NLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or morphological analysis) without any external linguistic guidance. It is arduous to compare novel solutions to well-entrenched preprocessing toolkits, relying on rule-based morphological analysers or dictionaries. Aware of the shortcomings of existing NLPre evaluation approaches, we investigate a novel method of reliable and fair evaluation and performance reporting. Inspired by the GLUE benchmark, the proposed language-centric benchmarking system enables comprehensive ongoing evaluation of multiple NLPre tools, while credibly tracking their performance. The prototype application is configured for Polish and integrated with the thoroughly assembled NLPre-PL benchmark. Based on this benchmark, we conduct an ex
    
[^17]: 使用图理解和推理功能增强大规模语言模型的GraphInstruct

    GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability

    [https://arxiv.org/abs/2403.04483](https://arxiv.org/abs/2403.04483)

    该论文提出了一个名为GraphInstruct的基准，用于评估和增强大规模语言模型的图理解能力，并通过构建GraphLM和提出GraphLM+模型实现了显著的图推理能力增强。

    

    评估和增强大规模语言模型（LLMs）的通用能力一直是一个重要的研究课题。图是现实世界中常见的数据结构，理解图数据对于推进通用智能至关重要。为了评估和增强LLMs的图理解能力，在本文中，我们提出了一个名为GraphInstruct的基准，全面包括21个经典图推理任务，提供多样的图生成流水线和详细的推理步骤。基于GraphInstruct，我们进一步通过高效的指导调整构建了GraphLM，展示出显著的图理解能力。为了增强LLM的图推理能力，我们提出了一种步骤掩码训练策略，并构建了一个名为GraphLM+的模型。作为增强LLMs图理解和推理能力的先驱性努力之一，我们进行了大量实验。

    arXiv:2403.04483v1 Announce Type: new  Abstract: Evaluating and enhancing the general capabilities of large language models (LLMs) has been an important research topic. Graph is a common data structure in the real world, and understanding graph data is a crucial part for advancing general intelligence. To evaluate and enhance the graph understanding abilities of LLMs, in this paper, we propose a benchmark named GraphInstruct, which comprehensively includes 21 classical graph reasoning tasks, providing diverse graph generation pipelines and detailed reasoning steps. Based on GraphInstruct, we further construct GraphLM through efficient instruction-tuning, which shows prominent graph understanding capability. In order to enhance the LLM with graph reasoning capability as well, we propose a step mask training strategy, and construct a model named GraphLM+. As one of the pioneering efforts to enhance the graph understanding and reasoning abilities of LLMs, extensive experiments have demons
    
[^18]: 大型语言模型能理解多目标口语语言吗？

    Do Large Language Model Understand Multi-Intent Spoken Language ?

    [https://arxiv.org/abs/2403.04481](https://arxiv.org/abs/2403.04481)

    该研究利用大型语言模型进行口语语言多目标理解，提出了改进实体槽和子目标指令的创新技术，并展示了LLMs在多目标SLU模型方面的潜力。

    

    这项研究通过利用大型语言模型（LLMs）进行多目标口语语言理解（SLU）取得了重大进展，提出了一种在SLU环境中利用LLMs生成能力的独特方法。我们的创新技术重新配置了实体槽，专门用于LLMs在多目标SLU环境中的应用，并引入了子目标指令（SII）的概念，增强了对不同领域内复杂多目标交流的解剖和解释。由此产生的数据集，被称为LM-MixATIS和LM-MixSNIPS，是从现有基准中精心制作的。我们的研究表明，LLMs可以匹配并潜在地超越当前最先进的多目标SLU模型的能力。它进一步探讨了LLMs在各种意图配置和数据集比例下的有效性。此外，我们介绍了两个开创性的度量标准，即实体槽准确性（ESA）和Com

    arXiv:2403.04481v1 Announce Type: cross  Abstract: This study marks a significant advancement by harnessing Large Language Models (LLMs) for multi-intent spoken language understanding (SLU), proposing a unique methodology that capitalizes on the generative power of LLMs within an SLU context. Our innovative technique reconfigures entity slots specifically for LLM application in multi-intent SLU environments and introduces the concept of Sub-Intent Instruction (SII), enhancing the dissection and interpretation of intricate, multi-intent communication within varied domains. The resultant datasets, dubbed LM-MixATIS and LM-MixSNIPS, are crafted from pre-existing benchmarks. Our research illustrates that LLMs can match and potentially excel beyond the capabilities of current state-of-the-art multi-intent SLU models. It further explores LLM efficacy across various intent configurations and dataset proportions. Moreover, we introduce two pioneering metrics, Entity Slot Accuracy (ESA) and Com
    
[^19]: Pearl: 一项基于评论驱动的角色知识对话式推荐数据集

    Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset

    [https://arxiv.org/abs/2403.04460](https://arxiv.org/abs/2403.04460)

    Pearl数据集利用了角色和知识增强的大型语言模型，提供了具体用户偏好，领域专业性和更相关的推荐。

    

    arXiv:2403.04460v1 公告类型：新摘要：对话式推荐系统是一个新兴领域，尤其是随着大型语言模型（LLMs）的进步，使得对话输入的多样化推理引起了社区的越来越大的兴趣。尽管取得了进展，但该领域还有许多方面有待探索。目前可用的用于对话式推荐的公共数据集缺乏特定用户偏好和对推荐的解释，从而妨碍了高质量的推荐。为了解决这些挑战，我们提出了一种新颖的对话式推荐数据集，命名为PEARL，与角色和知识增强的LLM模拟器相结合。我们从真实评论中获得详细的角色和知识，并构建了一个超过57k对话的大规模数据集。我们的实验结果表明，PEARL中的话语包括更具体的用户偏好，显示了在目标领域的专业知识，并提供了更相关的推荐。

    arXiv:2403.04460v1 Announce Type: new  Abstract: Conversational recommender system is an emerging area that has garnered an increasing interest in the community, especially with the advancements in large language models (LLMs) that enable diverse reasoning over conversational input. Despite the progress, the field has many aspects left to explore. The currently available public datasets for conversational recommendation lack specific user preferences and explanations for recommendations, hindering high-quality recommendations. To address such challenges, we present a novel conversational recommendation dataset named PEARL, synthesized with persona- and knowledge-augmented LLM simulators. We obtain detailed persona and knowledge from real-world reviews and construct a large-scale dataset with over 57k dialogues. Our experimental results demonstrate that utterances in PEARL include more specific user preferences, show expertise in the target domain, and provide recommendations more relev
    
[^20]: 低资源条件下普通法系统法院判决摘要

    Low-Resource Court Judgment Summarization for Common Law Systems

    [https://arxiv.org/abs/2403.04454](https://arxiv.org/abs/2403.04454)

    提出了用于普通法系统的跨多司法管辖区法院判决文档摘要的第一个数据集CLSum，并首次采用大型语言模型（LLMs）进行判决摘要工作。

    

    普通法法院需要参考类似判例的判决来指导其当前的决定。生成高质量的法院判决文档摘要可以帮助法律从业者高效地审查先前的案例，并协助公众了解法院运作方式及法律如何适用。先前的法院判决摘要研究着重于民法或特定司法管辖区的判决。然而，法官可以参考所有普通法司法管辖区的判决。目前的摘要数据集不足以满足跨多个司法管辖区概括判例的要求，尤其是对于许多司法管辖区缺乏标记数据。为解决数据集不足问题，本文提出了CLSum，这是第一个用于总结多司法管辖区普通法法院判决文档的数据集。此外，这是第一个采用大型语言模型（LLMs）的法院判决摘要工作。

    arXiv:2403.04454v1 Announce Type: cross  Abstract: Common law courts need to refer to similar precedents' judgments to inform their current decisions. Generating high-quality summaries of court judgment documents can facilitate legal practitioners to efficiently review previous cases and assist the general public in accessing how the courts operate and how the law is applied. Previous court judgment summarization research focuses on civil law or a particular jurisdiction's judgments. However, judges can refer to the judgments from all common law jurisdictions. Current summarization datasets are insufficient to satisfy the demands of summarizing precedents across multiple jurisdictions, especially when labeled data are scarce for many jurisdictions. To address the lack of datasets, we present CLSum, the first dataset for summarizing multi-jurisdictional common law court judgment documents. Besides, this is the first court judgment summarization work adopting large language models (LLMs)
    
[^21]: 会员推理攻击与主题建模中的隐私

    Membership Inference Attacks and Privacy in Topic Modeling

    [https://arxiv.org/abs/2403.04451](https://arxiv.org/abs/2403.04451)

    主题建模中提出了会员推理攻击，通过差分隐私词汇选择来改善隐私风险

    

    最近的研究表明，大型语言模型容易受到推理训练数据方面的隐私攻击。然而，目前还不清楚更简单的生成模型，例如主题模型，是否存在类似的漏洞。在这项工作中，我们提出了一种针对主题模型的攻击，可以自信地识别Latent Dirichlet Allocation中训练数据的成员。我们的结果表明，与大型神经模型相关联的隐私风险并不仅限于大型神经模型。此外，为了减轻这些漏洞，我们探讨了差分隐私（DP）主题建模。我们提出了一个私密主题建模框架，将DP词汇选择作为预处理步骤，并展示它不仅改善了隐私性，而且在实用性方面的影响有限。

    arXiv:2403.04451v1 Announce Type: cross  Abstract: Recent research shows that large language models are susceptible to privacy attacks that infer aspects of the training data. However, it is unclear if simpler generative models, like topic models, share similar vulnerabilities. In this work, we propose an attack against topic models that can confidently identify members of the training data in Latent Dirichlet Allocation. Our results suggest that the privacy risks associated with generative modeling are not restricted to large neural models. Additionally, to mitigate these vulnerabilities, we explore differentially private (DP) topic modeling. We propose a framework for private topic modeling that incorporates DP vocabulary selection as a pre-processing step, and show that it improves privacy while having limited effects on practical utility.
    
[^22]: 阶级工具：社会阶级与自然语言处理表现的相关性

    Classist Tools: Social Class Correlates with Performance in NLP

    [https://arxiv.org/abs/2403.04445](https://arxiv.org/abs/2403.04445)

    NLP的性能存在阶级偏见，经验证对较不发达的社会经济群体不利。

    

    自从William Labov关于语言社会阶层化的基础工作（Labov, 1964）以来，语言学已经做出了集中的努力，探索社会人口特征与语言产生和理解之间的联系。尽管存在强烈的证据表明语言中存在着社会人口特征，但它们并不经常在自然语言处理（NLP）中使用。年龄和性别在一定程度上有所体现，但Labov最初的目标——社会经济地位却明显缺失。然而，这是重要的。我们通过实证显示自然语言处理使较不发达的社会经济群体处于不利地位。我们使用社会阶级、种族和地理语言差异来对电影中的95,000个话语进行标注，评估了NLP系统在语言建模、自动语音识别和语法错误纠正三个任务上的表现。我们发现可以归因于社会经济因素的显著表现差距。

    arXiv:2403.04445v1 Announce Type: new  Abstract: Since the foundational work of William Labov on the social stratification of language (Labov, 1964), linguistics has made concentrated efforts to explore the links between sociodemographic characteristics and language production and perception. But while there is strong evidence for socio-demographic characteristics in language, they are infrequently used in Natural Language Processing (NLP). Age and gender are somewhat well represented, but Labov's original target, socioeconomic status, is noticeably absent. And yet it matters. We show empirically that NLP disadvantages less-privileged socioeconomic groups. We annotate a corpus of 95K utterances from movies with social class, ethnicity and geographical language variety and measure the performance of NLP systems on three tasks: language modelling, automatic speech recognition, and grammar error correction. We find significant performance disparities that can be attributed to socioeconomi
    
[^23]: 提升社会和健康计算科学中基于代理的模型的替代方法的未来方向有潜力并值得尝试

    Promising and worth-to-try future directions for advancing state-of-the-art surrogates methods of agent-based models in social and health computational sciences

    [https://arxiv.org/abs/2403.04417](https://arxiv.org/abs/2403.04417)

    强调一些适用于不同建模应用领域中非线性动态模型的代理模型，对于社会和健康计算科学领域的ABMs具有潜在的推动作用

    

    模型分析工具的执行和运行性能对于真实大规模ABMs（基于代理的模型）可能会过长。这是由于计算需求与模型规模（例如人口规模）和模型参数数量成指数比例。即使是对于一个真实ABM的单次模拟运行，当尝试使用真实人口规模时，也可能需要大量的计算资源。这篇简短报告的主要目的是强调一些适用于各种建模应用领域中的非线性动态模型的代理模型，这些模型对计算要求较小。据作者所知，这些方法至少在社会和健康计算科学领域的ABMs中尚未被广泛采用。因此，它们可能有助于推动建立SH领域ABMs的替代模型的前沿技术。

    arXiv:2403.04417v1 Announce Type: cross  Abstract: The execution and runtime performance of model-based analysis tools for realistic large-scale ABMs (Agent-Based Models) can be excessively long. This due to the computational demand exponentially proportional to the model size (e.g. Population size) and the number of model parameters. Even the runtime of a single simulation of a realistic ABM may demand huge computational resources when attempting to employ realistic population size. The main aim of this ad-hoc brief report is to highlight some of surrogate models that were adequate and computationally less demanding for nonlinear dynamical models in various modeling application areas.To the author knowledge, these methods have been not, at least extensively, employed for ABMs within the field of (SHCS) Social Health Computational Sciences, yet. Thus, they might be, but not necessarily, useful in progressing state of the art for establishing surrogate models for ABMs in the field of SH
    
[^24]: 在自然语言推理中探索组合泛化的不间断学习

    Exploring Continual Learning of Compositional Generalization in NLI

    [https://arxiv.org/abs/2403.04400](https://arxiv.org/abs/2403.04400)

    本文介绍了连续组合泛化挑战，探讨了模型如何持续获取原始推理任务的知识并进行组合推理，研究了不断学习对NLI中组合泛化的影响，并提出了解决方案。

    

    组合自然语言推理已被用来评估神经模型执行NLI的真实能力。然而，当前的评估假设模型事先完全访问所有原始推理，与人类不断获得推理知识的方式相反。在本文中，我们介绍了推理中的连续组合泛化（C2Gen NLI）挑战，其中模型持续获取构成原始推理任务的知识作为组合推理的基础。我们研究了不断学习如何影响NLI中的组合泛化，通过为组合NLI推理任务设计了一个不断学习设置。我们的实验表明，模型在不断学习的情况下无法组合泛化。为了解决这个问题，我们首先对各种不断学习算法进行基准测试并验证它们的功效。然后，我们进一步分析了C2Gen，重点关注如何排序

    arXiv:2403.04400v1 Announce Type: new  Abstract: Compositional Natural Language Inference has been explored to assess the true abilities of neural models to perform NLI. Yet, current evaluations assume models to have full access to all primitive inferences in advance, in contrast to humans that continuously acquire inference knowledge. In this paper, we introduce the Continual Compositional Generalization in Inference (C2Gen NLI) challenge, where a model continuously acquires knowledge of constituting primitive inference tasks as a basis for compositional inferences. We explore how continual learning affects compositional generalization in NLI, by designing a continual learning setup for compositional NLI inference tasks. Our experiments demonstrate that models fail to compositionally generalize in a continual scenario. To address this problem, we first benchmark various continual learning algorithms and verify their efficacy. We then further analyze C2Gen, focusing on how to order pri
    
[^25]: SGNet：使用深度学习折叠对称蛋白质复合物

    SGNet: Folding Symmetrical Protein Complex with Deep Learning

    [https://arxiv.org/abs/2403.04395](https://arxiv.org/abs/2403.04395)

    SGNet提出了一种名为SGNet的蛋白质折叠框架，用于模拟对称组件中的蛋白质-蛋白质相互作用，解决了对称蛋白质组装中的结构确定和监督模糊性问题。

    

    arXiv:2403.04395v1 声明类型：跨  摘要：深度学习在蛋白质结构预测方面取得了显著进展，推动了计算生物学的发展。然而，尽管在预测单链结构方面取得了较高的准确性，但大量的大型同源寡聚体组装体呈现出内部对称性，在结构确定方面提出了重大挑战。现有深度学习方法的性能受到限制，因为对称蛋白质组装通常具有较长的序列，使结构计算变得不可行。此外，对称蛋白质复合物中的多个相同亚基导致标签分配中存在监督模糊性问题，要求在训练中进行一致的结构建模。为了解决这些问题，我们提出了一种名为SGNet的蛋白质折叠框架，用于模拟对称组件中的蛋白质-蛋白质相互作用。SGNet在单个亚基上进行特征提取，并生成整个组装体。

    arXiv:2403.04395v1 Announce Type: cross  Abstract: Deep learning has made significant progress in protein structure prediction, advancing the development of computational biology. However, despite the high accuracy achieved in predicting single-chain structures, a significant number of large homo-oligomeric assemblies exhibit internal symmetry, posing a major challenge in structure determination. The performances of existing deep learning methods are limited since the symmetrical protein assembly usually has a long sequence, making structural computation infeasible. In addition, multiple identical subunits in symmetrical protein complex cause the issue of supervision ambiguity in label assignment, requiring a consistent structure modeling for the training. To tackle these problems, we propose a protein folding framework called SGNet to model protein-protein interactions in symmetrical assemblies. SGNet conducts feature extraction on a single subunit and generates the whole assembly usi
    
[^26]: Acceleron：加速研究构想的工具

    Acceleron: A Tool to Accelerate Research Ideation

    [https://arxiv.org/abs/2403.04382](https://arxiv.org/abs/2403.04382)

    提出了一种名为“Acceleron”的工具，用于加速研究构想，帮助研究人员制定全面的研究提案，验证其创新性。

    

    近年来，已提出了几种工具，用于协助研究人员在研究生命周期的各个阶段。然而，这些工具主要集中在诸如检索和推荐相关文献、审查和评论草稿、以及撰写研究手稿等任务上。我们的调查揭示了在研究生命周期的具有挑战性的构想阶段中缺乏专门设计的工具。为了帮助研究构想，我们提出了“Acceleron”，这是一个针对研究生命周期不同阶段的研究加速器，专门设计用于辅助构想过程。Acceleron指导研究人员制定一个全面的研究提案，包括一个新颖的研究问题。通过识别现有文献中的空白并建议一系列可行的解决技术，验证提案的创新动机。

    arXiv:2403.04382v1 Announce Type: cross  Abstract: Several tools have recently been proposed for assisting researchers during various stages of the research life-cycle. However, these primarily concentrate on tasks such as retrieving and recommending relevant literature, reviewing and critiquing the draft, and writing of research manuscripts. Our investigation reveals a significant gap in availability of tools specifically designed to assist researchers during the challenging ideation phase of the research life-cycle. To aid with research ideation, we propose `Acceleron', a research accelerator for different phases of the research life cycle, and which is specially designed to aid the ideation process. Acceleron guides researchers through the formulation of a comprehensive research proposal, encompassing a novel research problem. The proposals motivation is validated for novelty by identifying gaps in the existing literature and suggesting a plausible list of techniques to solve the pr
    
[^27]: 《汉语名词短语中的数量与确定性的计算建模》

    Computational Modelling of Plurality and Definiteness in Chinese Noun Phrases

    [https://arxiv.org/abs/2403.04376](https://arxiv.org/abs/2403.04376)

    该论文研究汉语名词短语中数量与确定性标记的省略现象，发现汉语讲话者确实经常省略这些标记，这一现象在相关语境中的可预测性有所提高。

    

    理论语言学家指出，一些语言（如汉语和日语）"更冷"的观察基于这样的事实，即这些语言中短语的预期含义更多地依赖于上下文。因此，这些语言中许多表达方式被缩短，其含义是从上下文中推断出来的。本文着重探讨在汉语名词短语中省略数量和确定性标记，以研究在给定语境情况下其预期含义的可预测性。为此，我们构建了一个汉语名词短语语料库，每个短语都附带其对应的语境以及标签，表示其单数/复数和确定性/不确定性。我们进行了语料库评估和分析。结果表明，汉语讲话者确实经常省略数量和确定性标记。基于此语料库，我们训练了一组计算模型...

    arXiv:2403.04376v1 Announce Type: new  Abstract: Theoretical linguists have suggested that some languages (e.g., Chinese and Japanese) are "cooler" than other languages based on the observation that the intended meaning of phrases in these languages depends more on their contexts. As a result, many expressions in these languages are shortened, and their meaning is inferred from the context. In this paper, we focus on the omission of the plurality and definiteness markers in Chinese noun phrases (NPs) to investigate the predictability of their intended meaning given the contexts. To this end, we built a corpus of Chinese NPs, each of which is accompanied by its corresponding context, and by labels indicating its singularity/plurality and definiteness/indefiniteness. We carried out corpus assessments and analyses. The results suggest that Chinese speakers indeed drop plurality and definiteness markers very frequently. Building on the corpus, we train a bank of computational models using 
    
[^28]: 从图到词袋: 将领域知识引入混淆罪名预测

    From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction

    [https://arxiv.org/abs/2403.04369](https://arxiv.org/abs/2403.04369)

    引入领域知识的从图到词袋方法，帮助预测混淆罪名，通过构成要素和关键词选择进行判断。

    

    混淆罪名预测是法律人工智能中一个具有挑战性的任务，涉及根据事实描述预测混淆罪名。现有的罪名预测方法在表现上已经展现出令人印象深刻的效果，但在处理混淆罪名（如抢夺与抢劫）时面临着重大挑战。在法律领域，构成要素在区分混淆罪名中扮演着至关重要的角色。构成要素是潜在刑罚背后的基本行为，并且在不同罪名之间有微妙的区别。本文介绍了一种新的从图到词袋（FWGB）方法，该方法引入了有关构成要素的领域知识，以指导模型在混淆罪名上做出判断，类似于法官的推理过程。具体而言，我们首先构建了一个包含构成要素的法律知识图，以帮助为每种罪名选择关键词，形成一个单词袋。

    arXiv:2403.04369v1 Announce Type: new  Abstract: Confusing charge prediction is a challenging task in legal AI, which involves predicting confusing charges based on fact descriptions. While existing charge prediction methods have shown impressive performance, they face significant challenges when dealing with confusing charges, such as Snatch and Robbery. In the legal domain, constituent elements play a pivotal role in distinguishing confusing charges. Constituent elements are fundamental behaviors underlying criminal punishment and have subtle distinctions among charges. In this paper, we introduce a novel From Graph to Word Bag (FWGB) approach, which introduces domain knowledge regarding constituent elements to guide the model in making judgments on confusing charges, much like a judge's reasoning process. Specifically, we first construct a legal knowledge graph containing constituent elements to help select keywords for each charge, forming a word bag. Subsequently, to guide the mod
    
[^29]: ProMoAI：使用生成式AI进行过程建模

    ProMoAI: Process Modeling with Generative AI

    [https://arxiv.org/abs/2403.04327](https://arxiv.org/abs/2403.04327)

    ProMoAI利用大型语言模型自动生成过程模型，支持优化并通过用户反馈进行改进，是一种新颖的AI驱动的过程建模工具，降低了用户的技术门槛。

    

    ProMoAI是一种新颖的工具，利用大型语言模型（LLMs）从文本描述中自动生成过程模型，融合了先进的提示工程、错误处理和代码生成技术。除了自动化生成复杂的过程模型外，ProMoAI还支持过程模型优化。用户可以通过提供对生成模型的反馈与工具进行交互，然后用于改进过程模型。ProMoAI利用LLMs的能力提供了一种新颖的、以人工智能驱动的过程建模方法，显著降低了对没有深入技术知识的用户的准入门槛。

    arXiv:2403.04327v1 Announce Type: cross  Abstract: ProMoAI is a novel tool that leverages Large Language Models (LLMs) to automatically generate process models from textual descriptions, incorporating advanced prompt engineering, error handling, and code generation techniques. Beyond automating the generation of complex process models, ProMoAI also supports process model optimization. Users can interact with the tool by providing feedback on the generated model, which is then used for refining the process model. ProMoAI utilizes the capabilities LLMs to offer a novel, AI-driven approach to process modeling, significantly reducing the barrier to entry for users without deep technical knowledge in process modeling.
    
[^30]: 使用大型语言模型的组合分数测量人脑中的含义合成

    Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models

    [https://arxiv.org/abs/2403.04325](https://arxiv.org/abs/2403.04325)

    引入了Composition Score，一种基于模型的度量标准，用于量化句子理解中的含义合成程度，实验证明这一度量与大脑区域相关，揭示了含义合成在人类句子理解中的多方面性。

    

    含义合成的过程是指更小的单位如语素或单词组合形成短语和句子的含义，对于人类句子理解至关重要。尽管神经语言学对涉及含义合成的大脑区域进行了大量研究，但仍缺乏一种计算度量来量化合成的程度。借鉴变压器前馈网络块的键值内存解释，我们引入了组合分数，这是一种新颖的基于模型的度量标准，旨在量化句子理解过程中的含义合成程度。实验结果表明，这一度量与大脑簇相关联，这些大脑簇与词频率、结构处理和对单词的一般敏感性有关，这表明了人类句子理解过程中含义合成的多方面性。

    arXiv:2403.04325v1 Announce Type: cross  Abstract: The process of meaning composition, wherein smaller units like morphemes or words combine to form the meaning of phrases and sentences, is essential for human sentence comprehension. Despite extensive neurolinguistic research into the brain regions involved in meaning composition, a computational metric to quantify the extent of composition is still lacking. Drawing on the key-value memory interpretation of transformer feed-forward network blocks, we introduce the Composition Score, a novel model-based metric designed to quantify the degree of meaning composition during sentence comprehension. Experimental findings show that this metric correlates with brain clusters associated with word frequency, structural processing, and general sensitivity to words, suggesting the multifaceted nature of meaning composition during human sentence comprehension.
    
[^31]: 磨具探测和调整用于文本到图像生成

    Discriminative Probing and Tuning for Text-to-Image Generation

    [https://arxiv.org/abs/2403.04321](https://arxiv.org/abs/2403.04321)

    加强T2I模型的判别能力，以实现更精确的文本到图像对齐生成。

    

    尽管文本到图像生成（T2I）取得了进展，但先前的方法经常面临文本图像不对齐等问题，如生成图像中的关系混淆。现有解决方案包括交叉注意力操作以实现更好的组合理解，或者集成大型语言模型以改进布局规划。然而，T2I模型的固有对齐能力仍然不足。通过审视生成模型和判别模型之间的联系，我们认为T2I模型的判别能力可能反映了它们在生成过程中的文本图像对齐熟练度。基于这一观点，我们主张加强T2I模型的判别能力，以实现更精确的文本到图像对齐生成。我们提出了一个建立在T2I模型上的判别适配器，以探测它们在两项代表性任务上的判别能力，并利用判别微调来改善它们的文本图像对齐。

    arXiv:2403.04321v1 Announce Type: cross  Abstract: Despite advancements in text-to-image generation (T2I), prior methods often face text-image misalignment problems such as relation confusion in generated images. Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning. However, the inherent alignment capabilities of T2I models are still inadequate. By reviewing the link between generative and discriminative modeling, we posit that T2I models' discriminative abilities may reflect their text-image alignment proficiency during generation. In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation. We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative fine-tuning to improve their text-image alignment. As a 
    
[^32]: 带有分摊上下文记忆的语言模型的在线适应

    Online Adaptation of Language Models with a Memory of Amortized Contexts

    [https://arxiv.org/abs/2403.04317](https://arxiv.org/abs/2403.04317)

    提出了一种带有分摊上下文记忆的在线适应框架，可有效地提取、压缩并存储信息以保持强大的知识保留能力

    

    由于信息的快速生成和传播，即使开发成本巨大，大型语言模型（LLMs）也很快过时。鉴于保持模型更新的重要性，当在现实世界应用LLMs时，在线学习已成为一项至关重要的需求。然而，鉴于不断扩大的未见文档语料库和现代LLMs的大参数空间，高效的适应至关重要。为了解决这些挑战，我们提出了Memory of Amortized Contexts（MAC），这是一个针对LLMs的高效且有效的在线适应框架，具有较强的知识保留能力。我们提出了一种摊销特征提取和记忆增强方法，将新文档中的信息压缩并提取为存储在记忆库中的紧凑调制。在回答问题时，我们的模型关注并从该记忆库中提取相关知识。为了有效地学习有信息量的调制…

    arXiv:2403.04317v1 Announce Type: cross  Abstract: Due to the rapid generation and dissemination of information, large language models (LLMs) quickly run out of date despite enormous development costs. Due to this crucial need to keep models updated, online learning has emerged as a critical necessity when utilizing LLMs for real-world applications. However, given the ever-expanding corpus of unseen documents and the large parameter space of modern LLMs, efficient adaptation is essential. To address these challenges, we propose Memory of Amortized Contexts (MAC), an efficient and effective online adaptation framework for LLMs with strong knowledge retention. We propose an amortized feature extraction and memory-augmentation approach to compress and extract information from new documents into compact modulations stored in a memory bank. When answering questions, our model attends to and extracts relevant knowledge from this memory bank. To learn informative modulations in an efficient m
    
[^33]: 您的模型能分辨否定和含义推断吗？揭示意图编码器的挑战

    Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders

    [https://arxiv.org/abs/2403.04314](https://arxiv.org/abs/2403.04314)

    该论文提出了一个意图语义工具包，通过引入三元任务来评估模型对于否定和含义推断这两个实际对话系统中重要的语义概念的理解情况

    

    论文摘要：对话系统通常依赖于嵌入模型进行意图分类和意图聚类任务。大型语言模型（LLM）的出现使得调整嵌入空间上的语义成为可能，通过提示来进行。然而，传统评估基准仅依赖于任务度量，不能很好地衡量与语义理解有关的差距。因此，我们提出了一个意图语义工具包，通过考虑意图嵌入模型的三个任务（1）意图分类，（2）意图聚类，以及（3）一个新颖的三元任务，来更全面地展示意图嵌入模型。三元任务评估模型对实际对话系统中至关重要的两个语义概念--否定和含义推断的理解情况。我们观察到当前的嵌入模型在这些概念的语义理解方面表现不佳。为了解决这一问题

    arXiv:2403.04314v1 Announce Type: new  Abstract: Conversational systems often rely on embedding models for intent classification and intent clustering tasks. The advent of Large Language Models (LLMs), which enable instructional embeddings allowing one to adjust semantics over the embedding space using prompts, are being viewed as a panacea for these downstream conversational tasks. However, traditional evaluation benchmarks rely solely on task metrics that don't particularly measure gaps related to semantic understanding. Thus, we propose an intent semantic toolkit that gives a more holistic view of intent embedding models by considering three tasks-- (1) intent classification, (2) intent clustering, and (3) a novel triplet task. The triplet task gauges the model's understanding of two semantic concepts paramount in real-world conversational systems-- negation and implicature. We observe that current embedding models fare poorly in semantic understanding of these concepts. To address 
    
[^34]: ALTO：一种用于复合AI系统的高效网络编排器

    ALTO: An Efficient Network Orchestrator for Compound AI Systems

    [https://arxiv.org/abs/2403.04311](https://arxiv.org/abs/2403.04311)

    ALTO是一个网络编排器，针对生成语言模型的优化机会，实现了高吞吐量和低延迟，同时解决了流式中间输出的两个新挑战：正确性和负载平衡。

    

    我们提出了ALTO，一种用于有效为诸如语言模型管道之类的复合AI系统提供服务的网络编排器。ALTO通过利用生成语言模型特有的优化机会：流式中间输出，实现了高吞吐量和低延迟。由于语言模型逐个生成token的输出，ALTO在可能时暴露了在阶段之间流式传输中间输出的机会。我们强调了在跨分布式管道阶段实例之间流式传输中间数据时出现的两个新挑战：正确性和负载平衡。我们还提出了聚合感知路由接口和分布式提示感知调度以应对这些挑战的需求。我们在一个复杂的聊天机器人验证管道上展示了ALTO部分输出流式传输的影响，将吞吐量提高了最多3倍，同时将固定延迟目标设置为4秒/请求，还减少了尾延迟。

    arXiv:2403.04311v1 Announce Type: new  Abstract: We present ALTO, a network orchestrator for efficiently serving compound AI systems such as pipelines of language models. ALTO achieves high throughput and low latency by taking advantage of an optimization opportunity specific to generative language models: streaming intermediate outputs. As language models produce outputs token by token, ALTO exposes opportunities to stream intermediate outputs between stages when possible. We highlight two new challenges of correctness and load balancing which emerge when streaming intermediate data across distributed pipeline stage instances. We also motivate the need for an aggregation-aware routing interface and distributed prompt-aware scheduling to address these challenges. We demonstrate the impact of ALTO's partial output streaming on a complex chatbot verification pipeline, increasing throughput by up to 3x for a fixed latency target of 4 seconds / request while also reducing tail latency by 1
    
[^35]: HaluEval-Wild：在实际环境中评估语言模型的幻觉

    HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild

    [https://arxiv.org/abs/2403.04307](https://arxiv.org/abs/2403.04307)

    HaluEval-Wild是第一个专门设计用于评估实际环境中LLM幻觉的基准测试，收集了具有挑战性的用户查询并分类为五种不同类型，可以对LLM表现出的幻觉类型进行细粒度分析。

    

    幻觉对于关键领域中大型语言模型（LLMs）的可靠性构成了重大挑战。最近设计用于评估LLM在传统NLP任务中的幻觉的基准测试，如知识密集型问答（QA）和摘要，不足以捕捉动态实际环境中用户-LLM交互的复杂性。为了弥补这一空白，我们介绍了HaluEval-Wild，这是第一个专门设计用于评估实际环境中LLM幻觉的基准测试。我们精心收集了来自现有实际用户-LLM交互数据集（包括ShareGPT）中具有挑战性的（经Alpaca对抗性过滤的）用户查询，以评估各种LLM的幻觉率。在分析收集到的查询后，我们将其分类为五种不同类型，这使得可以对LLM表现出的幻觉类型进行细粒度分析，并将引用答案与强大的GP合成。

    arXiv:2403.04307v1 Announce Type: new  Abstract: Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains. Recent benchmarks designed to assess LLM hallucinations within conventional NLP tasks, such as knowledge-intensive question answering (QA) and summarization, are insufficient for capturing the complexities of user-LLM interactions in dynamic, real-world settings. To address this gap, we introduce HaluEval-Wild, the first benchmark specifically designed to evaluate LLM hallucinations in the wild. We meticulously collect challenging (adversarially filtered by Alpaca) user queries from existing real-world user-LLM interaction datasets, including ShareGPT, to evaluate the hallucination rates of various LLMs. Upon analyzing the collected queries, we categorize them into five distinct types, which enables a fine-grained analysis of the types of hallucinations LLMs exhibit, and synthesize the reference answers with the powerful GP
    
[^36]: Proxy-RLHF：在大型语言模型中通过代理解耦生成和对齐

    Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy

    [https://arxiv.org/abs/2403.04283](https://arxiv.org/abs/2403.04283)

    该论文提出Proxy-RLHF方法，通过将大型语言模型的生成和对齐过程解耦，实现了以更低计算成本对齐人类价值观，仅使用其他方法的1%训练参数即可达到可比水平的对齐度。

    

    强化学习从人类反馈中学习（RLHF）是确保大型语言模型（LLMs）与人类价值观保持一致的主流方法。然而，现有的RLHF方法需要高昂的计算成本，主要原因之一是RLHF同时将生成和对齐任务分配给LLM。本文介绍了Proxy-RLHF，它解耦了LLMs的生成和对齐流程，以更低的计算成本实现与人类价值的对齐。我们从为对齐过程设计的新型马尔可夫决策过程（MDP）开始，并使用强化学习（RL）训练了一个简化的代理模型，监督LLM的标记生成，而不改变LLM本身。实验证明，我们的方法仅使用其他方法的1%训练参数即可实现可比水平的对齐度。

    arXiv:2403.04283v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach to ensure Large Language Models (LLMs) align with human values. However, existing RLHF methods require a high computational cost, one main reason being that RLHF assigns both the generation and alignment tasks to the LLM simultaneously. In this paper, we introduce Proxy-RLHF, which decouples the generation and alignment processes of LLMs, achieving alignment with human values at a much lower computational cost. We start with a novel Markov Decision Process (MDP) designed for the alignment process and employ Reinforcement Learning (RL) to train a streamlined proxy model that oversees the token generation of the LLM, without altering the LLM itself. Experiments show that our method achieves a comparable level of alignment with only 1\% of the training parameters of other methods.
    
[^37]: 用于评估阿拉伯呼叫领域自动语音识别的新基准

    A New Benchmark for Evaluating Automatic Speech Recognition in the Arabic Call Domain

    [https://arxiv.org/abs/2403.04280](https://arxiv.org/abs/2403.04280)

    该研究引入了一个针对阿拉伯电话对话领域挑战的全面评估基准，旨在为开发和评估能够适应真实电话通讯条件的ASR系统提供一个严格的测试平台。

    

    这项工作旨在引入一个全面的用于阿拉伯语音识别评估的基准，专门针对阿拉伯语电话对话中的挑战进行了定制。阿拉伯语以其丰富的方言多样性和语音复杂性而闻名，对自动语音识别（ASR）系统提出了许多独特的挑战。这些挑战在电话通话领域进一步放大，那里的音频质量、背景噪音和会话式语音风格会对识别准确性产生负面影响。我们的工作旨在建立一个稳健的基准，不仅包含广泛的阿拉伯方言范围，还模拟呼叫通讯的真实条件。通过结合多样的方言表达，并考虑呼叫录音的可变质量，这个基准旨在为开发和评估能够应对实际挑战的ASR系统提供严格的测试平台。

    arXiv:2403.04280v1 Announce Type: new  Abstract: This work is an attempt to introduce a comprehensive benchmark for Arabic speech recognition, specifically tailored to address the challenges of telephone conversations in Arabic language. Arabic, characterized by its rich dialectal diversity and phonetic complexity, presents a number of unique challenges for automatic speech recognition (ASR) systems. These challenges are further amplified in the domain of telephone calls, where audio quality, background noise, and conversational speech styles negatively affect recognition accuracy. Our work aims to establish a robust benchmark that not only encompasses the broad spectrum of Arabic dialects but also emulates the real-world conditions of call-based communications. By incorporating diverse dialectical expressions and accounting for the variable quality of call recordings, this benchmark seeks to provide a rigorous testing ground for the development and evaluation of ASR systems capable of
    
[^38]: 通过社区挑战推动生物医学文本挖掘的发展

    Advancing Biomedical Text Mining with Community Challenges

    [https://arxiv.org/abs/2403.04261](https://arxiv.org/abs/2403.04261)

    社区挑战评估竞赛在促进生物医学文本挖掘研究中的技术创新和跨学科合作方面起着重要作用。

    

    生物医学研究领域积累了大量来自科学文献、电子病历、临床试验报告和社交媒体等各方面的文本数据，然而手动处理和分析这些庞大且复杂的资源是耗时且低效的。为了解决这一挑战，生物医学文本挖掘，也称为生物医学自然语言处理，备受关注。社区挑战评估竞赛在促进生物医学文本挖掘研究中的技术创新和跨学科合作方面发挥了重要作用。这些挑战为研究人员提供了开发生物医学研究中数据挖掘和信息处理的最新解决方案的平台。在本文中，我们回顾了与中文生物医学文本挖掘有关的最新社区挑战的进展。

    arXiv:2403.04261v1 Announce Type: new  Abstract: The field of biomedical research has witnessed a significant increase in the accumulation of vast amounts of textual data from various sources such as scientific literatures, electronic health records, clinical trial reports, and social media. However, manually processing and analyzing these extensive and complex resources is time-consuming and inefficient. To address this challenge, biomedical text mining, also known as biomedical natural language processing, has garnered great attention. Community challenge evaluation competitions have played an important role in promoting technology innovation and interdisciplinary collaboration in biomedical text mining research. These challenges provide platforms for researchers to develop state-of-the-art solutions for data mining and information processing in biomedical research. In this article, we review the recent advances in community challenges specific to Chinese biomedical text mining. Firs
    
[^39]: 小型语言模型能成为顺序推荐系统的良好推理者吗？

    Can Small Language Models be Good Reasoners for Sequential Recommendation?

    [https://arxiv.org/abs/2403.04260](https://arxiv.org/abs/2403.04260)

    提出了逐步知识提取框架（SLIM），为顺序推荐系统解决了大型语言模型（LLMs）高资源需求的难题，使其能以资源高效的方式享受LLMs的出色推理能力。

    

    大型语言模型（LLMs）由于其出色的语言理解和生成能力，为顺序推荐开拓了新的领域。然而，要成功实现由LLMs赋能的顺序推荐还有许多挑战需要解决。首先，用户行为模式通常复杂，仅仅依靠LLMs的一步推理可能会导致错误或与任务无关的响应。其次，LLMs（例如ChatGPT-175B）极高的资源需求是难以承受且在实际顺序推荐系统中不切实际的。本文提出了一个新颖的逐步知识提取框架用于推荐（SLIM），为顺序推荐器以“瘦”（即资源高效）的方式享受LLMs出色的推理能力铺平了一条有前途的道路。我们引入基于用户行为序列的CoT提示来实现更好的推荐。

    arXiv:2403.04260v1 Announce Type: cross  Abstract: Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities. However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a "slim" (i.e., resource-efficient) manner. We introduce CoT prompting based on user behavior sequences for the larg
    
[^40]: UltraWiki: 使用负种子实体进行超细粒度实体集扩展

    UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities

    [https://arxiv.org/abs/2403.04247](https://arxiv.org/abs/2403.04247)

    使用负种子实体进行超细粒度实体集扩展，解决了传统方法在超细粒度语义类别表示中的问题。

    

    实体集扩展(ESE)旨在识别属于与给定种子实体相同语义类别的新实体。传统方法主要依赖正种子实体来表示目标语义类别，这对超细粒度语义类别的表示构成挑战。超细粒度语义类别是基于带有更具体属性约束的细粒度语义类别定义的。仅使用正种子实体描述会引起两个问题：(i) 超细粒度语义类别之间的歧义。(ii) 无法定义“不想要”的语义。由于这些固有缺陷，以前的方法很难解决超细粒度ESE(Ultra-ESE)。为了解决这个问题，我们首先引入了输入中的负种子实体，它们属于与正种子实体相同的细粒度语义类别，但在某些属性上有所不同。负种子实体消除

    arXiv:2403.04247v1 Announce Type: new  Abstract: Entity Set Expansion (ESE) aims to identify new entities belonging to the same semantic class as a given set of seed entities. Traditional methods primarily relied on positive seed entities to represent a target semantic class, which poses challenge for the representation of ultra-fine-grained semantic classes. Ultra-fine-grained semantic classes are defined based on fine-grained semantic classes with more specific attribute constraints. Describing it with positive seed entities alone cause two issues: (i) Ambiguity among ultra-fine-grained semantic classes. (ii) Inability to define "unwanted" semantic. Due to these inherent shortcomings, previous methods struggle to address the ultra-fine-grained ESE (Ultra-ESE). To solve this issue, we first introduce negative seed entities in the inputs, which belong to the same fine-grained semantic class as the positive seed entities but differ in certain attributes. Negative seed entities eliminate
    
[^41]: DEEP-ICL: 定义丰富的专家用于语言模型上下文学习

    DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning

    [https://arxiv.org/abs/2403.04233](https://arxiv.org/abs/2403.04233)

    DEEP-ICL 提出了一种新颖的任务定义丰富的专家集成方法，通过从示范中提取任务定义并学习任务特定示例，实现了在上下文学习方面具有可比性的性能，突破了传统上下文学习的限制。

    

    长期以来，人们一直认为大型语言模型（LLMs）中的参数数量驱动了上下文学习（ICL）能力，通过利用任务特定的示范实现了显著的性能提升。挑战这一假设，我们引入了DEEP-ICL，这是一种新颖的任务定义丰富的专家集成方法，用于ICL。 DEEP-ICL从给定的示范中明确提取任务定义，并通过学习任务特定示例生成响应。我们认为，ICL的改进并不直接依赖于模型大小，而基本上源自于理解任务定义和任务引导学习。受到这一启发，DEEP-ICL结合了两个具有不同角色的3B模型（一个用于总结任务定义，另一个用于学习任务示范），并实现了与LLaMA2-13B可比较的性能。此外，我们的框架通过克服预训练序列长度，优于传统ICL。

    arXiv:2403.04233v1 Announce Type: cross  Abstract: It has long been assumed that the sheer number of parameters in large language models (LLMs) drives in-context learning (ICL) capabilities, enabling remarkable performance improvements by leveraging task-specific demonstrations. Challenging this hypothesis, we introduce DEEP-ICL, a novel task Definition Enriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts task definitions from given demonstrations and generates responses through learning task-specific examples. We argue that improvement from ICL does not directly rely on model size, but essentially stems from understanding task definitions and task-guided learning. Inspired by this, DEEP-ICL combines two 3B models with distinct roles (one for concluding task definitions and the other for learning task demonstrations) and achieves comparable performance to LLaMA2-13B. Furthermore, our framework outperforms conventional ICL by overcoming pretraining sequence lengt
    
[^42]: Aligners: 解耦LLMs和对齐

    Aligners: Decoupling LLMs and Alignment

    [https://arxiv.org/abs/2403.04224](https://arxiv.org/abs/2403.04224)

    提出了一种通过训练对齐器模型来解耦大型语言模型（LLMs）和对齐，以减少对齐对性能的潜在负面影响。

    

    大型语言模型（LLMs）需要与人类期望对齐，以确保它们在大多数应用中的安全性和实用性。对齐具有挑战性，成本高昂，并且需要为每个LLM和对齐标准重复进行。我们建议通过训练可以根据需要用于对齐给定标准的任何LLM的对齐模型来解耦LLMs和对齐，从而在一定程度上减少对性能的潜在负面影响。我们提出的对齐模型训练配方仅依赖于使用（提示的）LLM 生成的合成数据，并且可以轻松调整以适应各种对齐标准。我们通过训练一个“道德”对齐器并在实验上验证其有效性来阐明我们的方法。

    arXiv:2403.04224v1 Announce Type: cross  Abstract: Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an "ethical" aligner and verify its efficacy empirically.
    
[^43]: 基于玻璃箱特征的大型语言模型的自我评估

    Self-Evaluation of Large Language Model based on Glass-box Features

    [https://arxiv.org/abs/2403.04222](https://arxiv.org/abs/2403.04222)

    研究探讨了大型语言模型在自我评估中利用玻璃箱特征的实用性，发现softmax分布在质量评估中可靠，提出了通过引入参考特征增强评估的策略，并验证了使用玻璃箱特征进行大型语言模型自我评估的可行性。

    

    arXiv:2403.04222v1 公告类型：新摘要：开源大型语言模型（LLMs）的蓬勃发展凸显了对评估方法的迫切需求。现有作品主要依赖于外部评估者，侧重于训练和提示策略。然而，一个关键方面——模型感知的玻璃箱特征——被忽视了。在这项研究中，我们探讨了在自我评估情境下使用玻璃箱特征的效用，即应用LLM评估其自身输出。我们研究了各种玻璃箱特征组，并发现softmax分布作为质量评估的可靠指标。此外，我们提出了通过合并从参考文献中提取的特征来增强评估的两种策略。在公共基准测试上的实验结果验证了使用玻璃箱特征进行LLMs的自我评估的可行性。

    arXiv:2403.04222v1 Announce Type: new  Abstract: The proliferation of open-source Large Language Models (LLMs) underscores the pressing need for evaluation methods. Existing works primarily rely on external evaluators, focusing on training and prompting strategies. However, a crucial aspect - model-aware glass-box features - is overlooked. In this study, we explore the utility of glass-box features under the scenario of self-evaluation, namely applying an LLM to evaluate its own output. We investigate various glass-box feature groups and discovered that the softmax distribution serves as a reliable indicator for quality evaluation. Furthermore, we propose two strategies to enhance the evaluation by incorporating features derived from references. Experimental results on public benchmarks validate the feasibility of self-evaluation of LLMs using glass-box features.
    
[^44]: 通过语义相似性进行人设提取以用于情感支持对话生成

    Persona Extraction Through Semantic Similarity for Emotional Support Conversation Generation

    [https://arxiv.org/abs/2403.04212](https://arxiv.org/abs/2403.04212)

    提出了一种新框架 PESS，通过语义相似性自动推断对话中的人设信息，以支持情感对话生成。

    

    通过对话系统提供情感支持在今天的世界中变得越来越重要，因为它可以支持许多对话场景中的心理健康和社交互动。过去的研究表明，使用人设对生成共情和支持性回复是有效的。然而，他们通常依赖预先提供的人设而不是在对话过程中推断它们。为了解决这一挑战，我们提出了一种新框架 PESS（通过语义相似性进行人设提取），它可以从对话中自动推断信息丰富且一致的人设。我们设计了基于语义相似性分数的完整性损失和一致性损失。完整性损失鼓励模型生成缺失的人设信息，一致性损失指导模型区分一致和不一致的情况。

    arXiv:2403.04212v1 Announce Type: new  Abstract: Providing emotional support through dialogue systems is becoming increasingly important in today's world, as it can support both mental health and social interactions in many conversation scenarios. Previous works have shown that using persona is effective for generating empathetic and supportive responses. They have often relied on pre-provided persona rather than inferring them during conversations. However, it is not always possible to obtain a user persona before the conversation begins. To address this challenge, we propose PESS (Persona Extraction through Semantic Similarity), a novel framework that can automatically infer informative and consistent persona from dialogues. We devise completeness loss and consistency loss based on semantic similarity scores. The completeness loss encourages the model to generate missing persona information, and the consistency loss guides the model to distinguish between consistent and inconsistent 
    
[^45]: 对于大型模型对齐方法的探究：本质与前景

    On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models

    [https://arxiv.org/abs/2403.04204](https://arxiv.org/abs/2403.04204)

    调查了大型模型价值对齐方法，揭示历史背景和数学本质，详细讨论了强化学习、监督微调和上下文学习等对齐方法。

    

    大型模型在人工智能领域取得了革命性突破，但也可能带来潜在的问题。为解决这些问题，引入了对齐技术，使这些模型符合人类的偏好和价值观。尽管在过去一年取得了相当大的进展，但建立最佳对齐策略仍面临诸多挑战，如数据成本和可扩展性监督。如何进行对齐仍然是一个开放的问题。在这篇调查论文中，我们全面调查了价值对齐方法。我们首先揭示了对齐的历史背景，追溯到20世纪20年代（它的起源），然后深入探讨了对齐的数学本质（它是什么），阐明了其中固有的挑战。在奠定了这个基础后，我们对现有的对齐方法进行了详细考察，这些方法可以分为三类：强化学习、监督微调和上下文学习。

    arXiv:2403.04204v1 Announce Type: new  Abstract: Big models have achieved revolutionary breakthroughs in the field of AI, but they might also pose potential concerns. Addressing such concerns, alignment technologies were introduced to make these models conform to human preferences and values. Despite considerable advancements in the past year, various challenges lie in establishing the optimal alignment strategy, such as data cost and scalable oversight, and how to align remains an open question. In this survey paper, we comprehensively investigate value alignment approaches. We first unpack the historical context of alignment tracing back to the 1920s (where it comes from), then delve into the mathematical essence of alignment (what it is), shedding light on the inherent challenges. Following this foundation, we provide a detailed examination of existing alignment methods, which fall into three categories: Reinforcement Learning, Supervised Fine-Tuning, and In-context Learning, and de
    
[^46]: 大规模语言模型是上下文分子学习器

    Large Language Models are In-Context Molecule Learners

    [https://arxiv.org/abs/2403.04197](https://arxiv.org/abs/2403.04197)

    提出了上下文分子适应（ICMA）范式，允许LLMs通过上下文示例学习分子-文本对齐，解决了在分子-标题翻译任务中对LLMs的挑战。

    

    大型语言模型（LLMs）在生物化学任务中表现出色，尤其是分子标题翻译任务，旨在弥合分子和自然语言文本之间的差距。然而，先前在适应LLMs到分子-标题翻译任务中的方法需要额外的领域特定预训练阶段，存在分子和文本空间之间的弱对齐，或对LLMs的规模有严格要求。为了解决这些挑战，我们提出了上下文分子适应（ICMA），作为一种新的范例，允许LLMs通过上下文示例学习分子-文本对齐，通过上下文分子调整。具体而言，ICMA包括以下三个阶段：跨模态检索、检索后排序和上下文分子调整。

    arXiv:2403.04197v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. Specifically, ICMA incorporates the following three stages: Cross-modal Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Cross-modal Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve informative context examples. Addi
    
[^47]: 生成式人工智能用于合成数据生成：方法、挑战和未来展望

    Generative AI for Synthetic Data Generation: Methods, Challenges and the Future

    [https://arxiv.org/abs/2403.04190](https://arxiv.org/abs/2403.04190)

    这里是中文总结出的一句话要点

    

    最近，关于从大型语言模型（LLMs）生成合成数据的研究急剧增加，特别是针对数据有限的情况，标志着生成式人工智能（AI）领域的一个显著转变。它们能够表现出与真实世界数据相媲美的能力，将这种方法定位为解决低资源挑战的一个引人注目的解决方案。本文深入探讨了利用这些庞大的LLMs生成特定任务训练数据的先进技术。我们概述了方法论、评估技术和实际应用，讨论了当前的局限性，并提出了未来研究的潜在途径。

    arXiv:2403.04190v1 Announce Type: cross  Abstract: The recent surge in research focused on generating synthetic data from large language models (LLMs), especially for scenarios with limited data availability, marks a notable shift in Generative Artificial Intelligence (AI). Their ability to perform comparably to real-world data positions this approach as a compelling solution to low-resource challenges. This paper delves into advanced technologies that leverage these gigantic LLMs for the generation of task-specific training data. We outline methodologies, evaluation techniques, and practical applications, discuss the current limitations, and suggest potential pathways for future research.
    
[^48]: 度量感知的LLM推断

    Metric-aware LLM inference

    [https://arxiv.org/abs/2403.04182](https://arxiv.org/abs/2403.04182)

    提出了度量感知的LLM推断方法，通过优化自定义指标来改进推断性能

    

    大型语言模型（LLMs）已经在各种NLP任务中展示出强大的结果。通常，输出是通过从LLM的基础分布中进行自回归采样获得的。我们表明，这种推断策略对于一系列任务和相关的评估指标可能是次优的。为此，我们提出了度量感知的LLM推断：一种在推断时针对自定义指标进行优化的决策理论方法。我们在学术基准数据集和公开可用模型上报告了相对基线的改进。

    arXiv:2403.04182v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated strong results on a range of NLP tasks. Typically, outputs are obtained via autoregressive sampling from the LLM's underlying distribution. We show that this inference strategy can be suboptimal for a range of tasks and associated evaluation metrics. As a remedy, we propose metric aware LLM inference: a decision theoretic approach optimizing for custom metrics at inference time. We report improvements over baselines on academic benchmarks and publicly available models.
    
[^49]: 尝试在语音到语音机器翻译中实现应力传递

    Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation

    [https://arxiv.org/abs/2403.04178](https://arxiv.org/abs/2403.04178)

    该论文介绍了一个具有印度英语重音注释的数据集，以及一个能够将重音融入合成语音中的文本到语音架构，用于在语音到语音机器翻译中实现应力传递。

    

    arXiv:2403.04178v1 公告类型：新摘要：印度教育部门的语言多样性构成了一个重大挑战，妨碍了包容性。尽管通过在线教育内容实现了知识的民主化，但作为互联网通用语言的英语的主导地位限制了可访问性，强调了将内容翻译为印度语言的重要性。尽管现有的语音到语音机器翻译（SSMT）技术，但这些系统中缺乏语调导致翻译单调，导致观众失去兴趣并脱离内容。为了解决这一问题，我们的论文引入了一份带有印度英语重音注释的数据集，并且引入了一个能够将重音融入合成语音中的文本到语音（TTS）架构。该数据集用于训练重音检测模型，然后该模型用于SSMT系统中检测源语音中的重音并将其转移到目标语言语音中。TTS架构

    arXiv:2403.04178v1 Announce Type: new  Abstract: The language diversity in India's education sector poses a significant challenge, hindering inclusivity. Despite the democratization of knowledge through online educational content, the dominance of English, as the internet's lingua franca, limits accessibility, emphasizing the crucial need for translation into Indian languages. Despite existing Speech-to-Speech Machine Translation (SSMT) technologies, the lack of intonation in these systems gives monotonous translations, leading to a loss of audience interest and disengagement from the content. To address this, our paper introduces a dataset with stress annotations in Indian English and also a Text-to-Speech (TTS) architecture capable of incorporating stress into synthesized speech. This dataset is used for training a stress detection model, which is then used in the SSMT system for detecting stress in the source speech and transferring it into the target language speech. The TTS archit
    
[^50]: DA-Net: 一种用于多源跨语言迁移学习的解耦自适应网络

    DA-Net: A Disentangled and Adaptive Network for Multi-Source Cross-Lingual Transfer Learning

    [https://arxiv.org/abs/2403.04158](https://arxiv.org/abs/2403.04158)

    DA-Net 提出了一种解决多源跨语言迁移学习中共享编码器导致学习困扰和语言特定分类器性能下降的新方法。

    

    多源跨语言迁移学习涉及从多个已标记源语言向一个未标记目标语言在语言转移下的任务知识传输。现有方法通常关注于加权不同源语言的特定语言分类器生成的预测，这些分类器遵循共享编码器。然而，所有源语言共享相同的编码器，这个编码器被所有这些语言更新。提取出的表示不可避免地包含不同源语言的信息，这可能干扰语言特定分类器的学习。此外，由于语言差距，使用源标签训练的语言特定分类器无法准确预测目标语言。这两个事实损害了模型的性能。为了解决这些挑战，我们提出了一种解耦自适应网络 (DA-Net)。

    arXiv:2403.04158v1 Announce Type: cross  Abstract: Multi-Source cross-lingual transfer learning deals with the transfer of task knowledge from multiple labelled source languages to an unlabeled target language under the language shift. Existing methods typically focus on weighting the predictions produced by language-specific classifiers of different sources that follow a shared encoder. However, all source languages share the same encoder, which is updated by all these languages. The extracted representations inevitably contain different source languages' information, which may disturb the learning of the language-specific classifiers. Additionally, due to the language gap, language-specific classifiers trained with source labels are unable to make accurate predictions for the target language. Both facts impair the model's performance. To address these challenges, we propose a Disentangled and Adaptive Network (DA-Net). Firstly, we devise a feedback-guided collaborative disentanglemen
    
[^51]: Chatbot Arena：一个通过人类偏好评估LLM的开放平台

    Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference

    [https://arxiv.org/abs/2403.04132](https://arxiv.org/abs/2403.04132)

    Chatbot Arena是一个开放平台，采用两两比较的方式通过众包利用人类偏好评估LLMs。研究表明众包问题多样且具有区分性，并且人类投票与专家评级者的投票基本一致。

    

    大型语言模型（LLMs）解锁了新的能力和应用；然而，评估其与人类偏好的一致性仍然面临着重大挑战。为了解决这个问题，我们介绍了Chatbot Arena，这是一个基于人类偏好评估LLMs的开放平台。我们的方法采用了一种两两比较的方式，并通过众包利用来自不同用户群体的输入。该平台已经运营了几个月，获得了超过24万个投票。本文描述了该平台，分析了我们迄今收集的数据，并解释了我们正在使用的经过验证的统计方法，以便对模型进行高效准确的评估和排名。我们确认众包问题足够多样化和区分化，并且众包人类投票与专家评级者的投票基本一致。这些分析共同为平台的可信度奠定了坚实的基础。

    arXiv:2403.04132v1 Announce Type: new  Abstract: Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of
    
[^52]: 探索基于LLM的代理用于根本原因分析

    Exploring LLM-based Agents for Root Cause Analysis

    [https://arxiv.org/abs/2403.04123](https://arxiv.org/abs/2403.04123)

    探索基于LLM的代理用于根本原因分析，以解决自动化RCA中无法动态收集额外诊断信息的限制。

    

    云软件系统越来越复杂，导致事件管理已成为软件开发生命周期的一个重要组成部分。根本原因分析（RCA）是事件管理过程的关键部分，对值班工程师来说是一项严峻的任务，需要深入的领域知识和对团队特定服务的广泛经验。自动化RCA可以显著节省时间，并减轻值班工程师在事件管理上的负担。最近，研究人员利用大型语言模型（LLMs）执行RCA，并取得了令人期待的成果。然而，这些方法无法动态收集额外的诊断信息，如与事件相关的日志、指标或数据库，严重限制了它们诊断根本原因的能力。在这项工作中，我们探讨了LLM代理用于RCA以解决此限制的使用。我们提出了一项彻底的实证评估。

    arXiv:2403.04123v1 Announce Type: cross  Abstract: The growing complexity of cloud based software systems has resulted in incident management becoming an integral part of the software development lifecycle. Root cause analysis (RCA), a critical part of the incident management process, is a demanding task for on-call engineers, requiring deep domain knowledge and extensive experience with a team's specific services. Automation of RCA can result in significant savings of time, and ease the burden of incident management on on-call engineers. Recently, researchers have utilized Large Language Models (LLMs) to perform RCA, and have demonstrated promising results. However, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes. In this work, we explore the use of LLM based agents for RCA to address this limitation. We present a thorough empirical eva
    
[^53]: 大型语言模型能够进行推理和规划吗？

    Can Large Language Models Reason and Plan?

    [https://arxiv.org/abs/2403.04121](https://arxiv.org/abs/2403.04121)

    大型语言模型缺乏自我批评能力，无法像人类一样纠正错误。

    

    虽然人类有时候表现出能够通过自我批评纠正自己错误猜测的能力，但似乎在大型语言模型的情况下没有依据支持这一假设。

    arXiv:2403.04121v1 Announce Type: new  Abstract: While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.
    
[^54]: 不要责怪数据，而要责怪模型：理解从主观标注学习时的噪音和偏差

    Don't Blame the Data, Blame the Model: Understanding Noise and Bias When Learning from Subjective Annotations

    [https://arxiv.org/abs/2403.04085](https://arxiv.org/abs/2403.04085)

    该论文研究了从主观标注学习时，模型对高分歧数据实例表现低置信度的原因，并提出了使用多地面实况方法进行分类以提高对这些实例的置信度。

    

    研究人员提高了对在自然含有人类注释者之间分歧的主观任务中聚合标签的伤害的认识。在这项工作中，我们表明，仅提供聚合标签的模型在高分歧数据实例上表现出低置信度。虽然先前的研究将这种情况视为错误标记，但我们认为高分歧文本实例难以学习的原因是传统的聚合模型在从主观任务中提取有用信号方面表现不佳。受最近研究表明从原始注释中学习的有效性的启发，我们调查使用多个地面实况（Multi-GT）方法进行分类。我们的实验证明了对高分歧实例的置信度提高。

    arXiv:2403.04085v1 Announce Type: new  Abstract: Researchers have raised awareness about the harms of aggregating labels especially in subjective tasks that naturally contain disagreements among human annotators. In this work we show that models that are only provided aggregated labels show low confidence on high-disagreement data instances. While previous studies consider such instances as mislabeled, we argue that the reason the high-disagreement text instances have been hard-to-learn is that the conventional aggregated models underperform in extracting useful signals from subjective tasks. Inspired by recent studies demonstrating the effectiveness of learning from raw annotations, we investigate classifying using Multiple Ground Truth (Multi-GT) approaches. Our experiments show an improvement of confidence for the high-disagreement instances.
    
[^55]: Transformers和语言模型在形式理解中的应用：扫描文档分析综述

    Transformers and Language Models in Form Understanding: A Comprehensive Review of Scanned Document Analysis

    [https://arxiv.org/abs/2403.04080](https://arxiv.org/abs/2403.04080)

    本文调查了扫描文档形式理解领域的研究，重点介绍了语言模型和Transformers在解决这一挑战性任务中的重要性，展示了Transformers如何推动领域发展并革新形式理解技术。

    

    本文全面调研了关于扫描文档形式理解领域的研究工作，深入探讨了该领域的最新进展与突破，突出了语言模型和Transformers在解决这一挑战性任务中的重要性。研究方法涉及对上个十年文档和形式理解趋势的深入分析，使我们能够提供有价值的领域演变见解。重点介绍了前沿模型如何推动该领域，彰显了Transformers如何革新了形式理解技术。我们的探索包括对设计有效应对嘈杂扫描文档复杂性的最先进的语言模型的广泛研究。此外，我们还展示了最新和最相关的数据集概述，这些数据集对评估性能至关重要。

    arXiv:2403.04080v1 Announce Type: new  Abstract: This paper presents a comprehensive survey of research works on the topic of form understanding in the context of scanned documents. We delve into recent advancements and breakthroughs in the field, highlighting the significance of language models and transformers in solving this challenging task. Our research methodology involves an in-depth analysis of popular documents and forms of understanding of trends over the last decade, enabling us to offer valuable insights into the evolution of this domain. Focusing on cutting-edge models, we showcase how transformers have propelled the field forward, revolutionizing form-understanding techniques. Our exploration includes an extensive examination of state-of-the-art language models designed to effectively tackle the complexities of noisy scanned documents. Furthermore, we present an overview of the latest and most relevant datasets, which serve as essential benchmarks for evaluating the perfo
    
[^56]: 通过高质量伪标签选择的半监督对话抽取式摘要生成

    Semi-Supervised Dialogue Abstractive Summarization via High-Quality Pseudolabel Selection

    [https://arxiv.org/abs/2403.04073](https://arxiv.org/abs/2403.04073)

    提出了一种新的高质量伪标签选择方法 SiCF，通过该方法选择具有高质量生成摘要的无标签对话来进行半监督对话抽取式摘要生成。

    

    半监督对话摘要生成（SSDS）利用模型生成的摘要减少对人工标记数据的依赖，提高摘要生成模型的性能。本文提出了一种新颖的评分方法 SiCF，包含了摘要模型质量的三个主要维度：语义不变性（模型信心的指示）、覆盖度（事实召回）和忠实度（事实精度）。利用 SiCF 分数，选择具有高质量生成摘要的无标签对话来训练摘要生成模型。

    arXiv:2403.04073v1 Announce Type: cross  Abstract: Semi-supervised dialogue summarization (SSDS) leverages model-generated summaries to reduce reliance on human-labeled data and improve the performance of summarization models. While addressing label noise, previous works on semi-supervised learning primarily focus on natural language understanding tasks, assuming each sample has a unique label. However, these methods are not directly applicable to SSDS, as it is a generative task, and each dialogue can be summarized in different ways. In this work, we propose a novel scoring approach, SiCF, which encapsulates three primary dimensions of summarization model quality: Semantic invariance (indicative of model confidence), Coverage (factual recall), and Faithfulness (factual precision). Using the SiCF score, we select unlabeled dialogues with high-quality generated summaries to train summarization models. Comprehensive experiments on three public datasets demonstrate the effectiveness of Si
    
[^57]: 大型语言模型能进行分析推理吗？

    Can Large Language Models do Analytical Reasoning?

    [https://arxiv.org/abs/2403.04031](https://arxiv.org/abs/2403.04031)

    本研究探索了大型语言模型在体育领域的分析推理能力，发现在处理NBA和NFL比赛得分任务时，GPT-4和Claude-2.1表现最佳，采用分治法进行数据处理效果最好。

    

    这篇论文探讨了应用于体育领域的前沿大型语言模型进行分析推理。我们使用大型语言模型来计算NBA和NFL比赛中每支球队在一个季度中得分的任务。我们的主要发现有两个方面。首先，我们发现在我们使用的所有模型中，GPT-4在效果上表现最为突出，其次是Claude-2.1，而GPT-3.5、Gemini-Pro和Llama-2-70b效果稍逊。具体来说，我们比较了三种不同的提示技术和分治法，发现后者效果最好。我们的分治法将逐步数据细分为更小、更易处理的片段，分别解决每个片段，然后将它们合并在一起。除了分治法，我们还探讨了一种名为Chain of Thought（CoT）策略，显著改善了某些模型的结果，尤其是GPT-4和Claude-2.1。

    arXiv:2403.04031v1 Announce Type: cross  Abstract: This paper explores the cutting-edge Large Language Model with analytical reasoning on sports. Our analytical reasoning embodies the tasks of letting large language models count how many points each team scores in a quarter in the NBA and NFL games. Our major discoveries are in two folds. Firstly, we find among all the models we employed, GPT-4 stands out in effectiveness, followed by Claude-2.1, with GPT-3.5, Gemini-Pro, and Llama-2-70b lagging behind. Specifically, we compare three different prompting techniques and a divide-and-conquer approach, we find that the latter was the most effective. Our divide-and-conquer approach breaks down play-by-play data into smaller, more manageable segments, solves each piece individually, and then aggregates them together. Besides the divide-and-conquer approach, we also explore the Chain of Thought (CoT) strategy, which markedly improves outcomes for certain models, notably GPT-4 and Claude-2.1, 
    
[^58]: 媒体偏见的重要性：理解政治偏见新闻对社交媒体上疫苗态度的影响

    Media Bias Matters: Understanding the Impact of Politically Biased News on Vaccine Attitudes in Social Media

    [https://arxiv.org/abs/2403.04009](https://arxiv.org/abs/2403.04009)

    本文分析了政治偏见新闻对疫苗态度的影响。通过深度学习和因果推断技术，揭示了社交媒体用户在疫苗立场上的不同行为，并发现中立立场的个体更容易受到政治偏见新闻的影响。

    

    新闻媒体被利用作为政治工具偏离事实，未经证据支持地提出偏见性观点。在新冠疫情期间，政治偏见新闻（PBN）显著削弱了公众对疫苗的信任，尽管有强有力的医学证据支持其有效性。在本文中，我们分析了：（i）内在疫苗立场如何微妙地影响个人选择新闻来源和参与社交媒体讨论；以及（ii）暴露于PBN对用户对疫苗态度的影响。为此，我们首先整理了一个将PBN与相关社交媒体话语连接起来的全面数据集。利用先进的深度学习和因果推断技术，我们揭示了具有不同疫苗立场的社交媒体群体之间的明显用户行为。此外，我们观察到，具有中立立场的个体，尤其是犹豫接种疫苗的多数派，对PBN的影响更加脆弱，相比之下

    arXiv:2403.04009v1 Announce Type: cross  Abstract: News media has been utilized as a political tool to stray from facts, presenting biased claims without evidence. Amid the COVID-19 pandemic, politically biased news (PBN) has significantly undermined public trust in vaccines, despite strong medical evidence supporting their efficacy. In this paper, we analyze: (i) how inherent vaccine stances subtly influence individuals' selection of news sources and participation in social media discussions; and (ii) the impact of exposure to PBN on users' attitudes toward vaccines. In doing so, we first curate a comprehensive dataset that connects PBN with related social media discourse. Utilizing advanced deep learning and causal inference techniques, we reveal distinct user behaviors between social media groups with various vaccine stances. Moreover, we observe that individuals with moderate stances, particularly the vaccine-hesitant majority, are more vulnerable to the influence of PBN compared t
    
[^59]: SaulLM-7B: 面向法律领域的开创性大型语言模型

    SaulLM-7B: A pioneering Large Language Model for Law

    [https://arxiv.org/abs/2403.03883](https://arxiv.org/abs/2403.03883)

    SaulLM-7B是首个专为法律文本设计的7B参数LLM，通过InnovativeFine-tuning方法，展现了领先的法律文件理解和处理能力。

    

    本文介绍了SaulLM-7B，一个专为法律领域量身定制的大型语言模型（LLM）。拥有70亿参数的SaulLM-7B是第一个专为法律文本理解和生成而设计的LLM。以Mistral 7B架构为基础，SaulLM-7B在超过300亿标记的英文法律语料库上进行训练。SaulLM-7B在理解和处理法律文件方面表现出最先进的能力。此外，我们提出了一种新颖的指导微调方法，利用法律数据集进一步增强SaulLM-7B在法律任务中的性能。SaulLM-7B采用CC-BY-SA-4.0许可发布。

    arXiv:2403.03883v1 Announce Type: new  Abstract: In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM designed explicitly for legal text comprehension and generation. Leveraging the Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art proficiency in understanding and processing legal documents. Additionally, we present a novel instructional fine-tuning method that leverages legal datasets to further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is released under the CC-BY-SA-4.0 License.
    
[^60]: Emojinize：用表情符号丰富任何文本的翻译

    Emojinize : Enriching Any Text with Emoji Translations

    [https://arxiv.org/abs/2403.03857](https://arxiv.org/abs/2403.03857)

    Emojinize 是一种方法，能够通过大型语言模型选择适当的表情符号翻译文本，提高猜测性，人类猜测可达55％。

    

    Emoji已经成为书面交流中无处不在的存在，在网络上和更远的地方。它们可以强调或澄清情绪，为对话增添细节，或者简单地起装饰作用。然而，这种随意使用仅仅是表情符号表达能力的冰山一角。为了进一步释放这种力量，我们提出了Emojinize，一种将任意文本短语翻译成一个或多个表情符号序列的方法，而无需人类输入。通过利用大型语言模型的能力，Emojinize可以根据上下文（例如，板球-球棒vs蝙蝠）消除歧义地选择适当的表情符号，并通过组合多个表情符号（例如，“Emojinize”被翻译成输入-拉丁文字母-右箭头-笑脸）来组合表达复杂概念。在基于填空测试的用户研究中，我们表明Emojinize的表情符号翻译可以使掩盖的单词的猜测性增加55％，而人类选择的表情符号翻译只能增加29％。

    arXiv:2403.03857v1 Announce Type: new  Abstract: Emoji have become ubiquitous in written communication, on the Web and beyond. They can emphasize or clarify emotions, add details to conversations, or simply serve decorative purposes. This casual use, however, barely scratches the surface of the expressive power of emoji. To further unleash this power, we present Emojinize, a method for translating arbitrary text phrases into sequences of one or more emoji without requiring human input. By leveraging the power of large language models, Emojinize can choose appropriate emoji by disambiguating based on context (eg, cricket-bat vs bat) and can express complex concepts compositionally by combining multiple emoji (eq, ''Emojinize'' is translated to input-latin-letters right-arrow grinning-face). In a cloze test--based user study, we show that Emojinize's emoji translations increase the human guessability of masked words by 55%, whereas human-picked emoji translations do so by only 29%. These
    
[^61]: ShortGPT: 大语言模型中的层级比您想象的更冗余

    ShortGPT: Layers in Large Language Models are More Redundant Than You Expect

    [https://arxiv.org/abs/2403.03853](https://arxiv.org/abs/2403.03853)

    大语言模型中的层级存在较高相似性，有些层对网络功能几乎无影响。研究提出一种称为区块影响的度量，并通过层删除方法显著优于以往的模型修剪方法。

    

    随着大语言模型（LLMs）在性能上不断取得进展，其规模显著增加，当前的LLMs包含数十亿甚至数万亿个参数。然而，在这项研究中，我们发现许多LLMs的层之间存在高度相似性，并且一些层在网络功能中起到了可忽略的作用。基于这一观察，我们定义了一种称为区块影响（BI）的度量衡量LLMs中每个层的重要性。然后，我们提出了一种简单的修剪方法：层删除，即根据它们的BI得分直接删除LLMs中的冗余层。实验证明，我们的方法ShortGPT在模型修剪方面明显优于以往的最先进方法。此外，ShortGPT与量化等方法正交，可以进一步减少参数和计算。通过简单的层删除即可获得更好的结果的能力，与传统的精确修剪方法截然不同。

    arXiv:2403.03853v1 Announce Type: new  Abstract: As Large Language Models (LLMs) continue to advance in performance, their size has escalated significantly, with current LLMs containing billions or even trillions of parameters. However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality. Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in LLMs. We then propose a straightforward pruning approach: layer removal, in which we directly delete the redundant layers in LLMs based on their BI scores. Experiments demonstrate that our method, which we call ShortGPT, significantly outperforms previous state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as oppo
    
[^62]: 一种用于多模态电视节目摘要的模块化方法

    A Modular Approach for Multimodal Summarization of TV Shows

    [https://arxiv.org/abs/2403.03823](https://arxiv.org/abs/2403.03823)

    提出了一种模块化方法用于多模态电视节目摘要，包括检测场景边界、重新排列场景、将视觉信息转换为文本、总结对话以及将场景摘要融合的过程，并引入了一个新的衡量摘要质量的评价指标PREFS。

    

    在本文中，我们讨论了电视节目摘要的任务，涉及到人工智能研究中的关键领域：复杂推理、多模态和长篇叙事。我们提出了一种模块化方法，其中各个组件执行专门的子任务，我们认为与端到端方法相比，这种方法提供了更大的灵活性。我们的模块涉及检测场景边界，重新排列场景以尽量减少不同事件之间的切换次数，将视觉信息转换为文本，总结每个场景中的对话，并将场景摘要融合成整集的最终摘要。我们还提出了一个新的度量标准，PREFS（摘要事实的精确度和召回率评估），用于衡量生成摘要的精确度和召回率，我们将其分解为原子事实。在最近发布的SummScreen3D数据集Papalampidi和Lapata（2023）上进行测试，我们的方法产生了

    arXiv:2403.03823v1 Announce Type: new  Abstract: In this paper we address the task of summarizing television shows, which touches key areas in AI research: complex reasoning, multiple modalities, and long narratives. We present a modular approach where separate components perform specialized sub-tasks which we argue affords greater flexibility compared to end-to-end methods. Our modules involve detecting scene boundaries, reordering scenes so as to minimize the number of cuts between different events, converting visual information to text, summarizing the dialogue in each scene, and fusing the scene summaries into a final summary for the entire episode. We also present a new metric, PREFS (\textbf{P}recision and \textbf{R}ecall \textbf{E}valuation of Summary \textbf{F}act\textbf{s}), to measure both precision and recall of generated summaries, which we decompose into atomic facts. Tested on the recently released SummScreen3D dataset Papalampidi and Lapata (2023), our method produces hi
    
[^63]: WMDP基准：通过遗忘测量和减少恶意使用

    The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning

    [https://arxiv.org/abs/2403.03218](https://arxiv.org/abs/2403.03218)

    WMDP基准是一个公开发布的数据集，包含4157个多项选择问题，用作生物安全、网络安全和化学安全危险知识的代理测量。

    

    arXiv:2403.03218v1 公告类型：交叉领域 摘要：白宫关于人工智能的行政命令强调了大型语言模型(LLMs)赋予恶意行为者开发生物、网络和化学武器的风险。为了衡量这些恶意使用的风险，政府机构和主要人工智能实验室正在开发LLMs的危险能力评估。然而，当前的评估是私人的，阻碍了进一步研究如何减少风险。此外，它们仅专注于几条高度特定的恶意使用途径。为了填补这些空白，我们公开发布了大规模杀伤性武器代理（WMDP）基准，这是一个包含4157个多项选择问题的数据集，作为生物安全、网络安全和化学安全危险知识的代理测量。WMDP由一组学术界和技术顾问联合开发，并在公开发布前严格过滤以消除敏感信息。WMDP有两个服务

    arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
    
[^64]: PARADISE：通过过程警告和提示数据集评估语言模型的隐式规划能力

    PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset

    [https://arxiv.org/abs/2403.03167](https://arxiv.org/abs/2403.03167)

    论文提出了一个名为PARADISE的任务，通过实用程序文本进行演绎推理，评估语言模型仅从给定目标推断计划的能力

    

    最近，社区对于大型语言模型是否具备规划或执行计划的能力越发感兴趣。然而，大多数先前研究使用LLMs为简化场景生成高级计划，适应度量缺乏语言复杂性和领域多样性，限制其规划能力的分析。为了解决这一问题，我们提出了PARADISE，这是一个使用Q＆A格式的演绎推理任务，采用来自wikiHow的实用程序文本。它涉及与目标直接相关的警告和提示推断任务，排除中间步骤，旨在测试模型仅从给定目标推断计划的隐含知识的能力。

    arXiv:2403.03167v1 Announce Type: new  Abstract: Recently, there has been growing interest within the community regarding whether large language models are capable of planning or executing plans. However, most prior studies use LLMs to generate high-level plans for simplified scenarios lacking linguistic complexity and domain diversity, limiting analysis of their planning abilities. These setups constrain evaluation methods (e.g., predefined action space), architectural choices (e.g., only generative models), and overlook the linguistic nuances essential for realistic analysis. To tackle this, we present PARADISE, an abductive reasoning task using Q\&A format on practical procedural text sourced from wikiHow. It involves warning and tip inference tasks directly associated with goals, excluding intermediary steps, with the aim of testing the ability of the models to infer implicit knowledge of the plan solely from the given goal. Our experiments, utilizing fine-tuned language models and
    
[^65]: OffLanDat：通过提示工程生成的大型语言模型生成的社区基础隐式攻击性语言数据集

    OffLanDat: A Community Based Implicit Offensive Language Dataset Generated by Large Language Model Through Prompt Engineering

    [https://arxiv.org/abs/2403.02472](https://arxiv.org/abs/2403.02472)

    介绍了一个通过提示工程生成的大型语言模型创建的社区基础隐式攻击性语言数据集OffLanDat，为38个不同目标群体提供数据。

    

    社交媒体上攻击性语言的普遍存在对社会福祉产生了不良影响。因此，有必要高度重视解决这一问题。攻击性语言既存在明确形式，也存在隐式形式，后者更具挑战性。当前在该领域的研究遇到几个挑战。首先，现有数据集主要依赖于收集包含明确攻击性关键词的文本，这使得捕捉不包含这些关键词且隐含攻击性内容的任务具有挑战性。其次，通常的方法论倾向于仅关注文本分析，忽视社区信息可以提供的宝贵见解。在这篇研究论文中，我们介绍了一个新的数据集OffLanDat，这是由ChatGPT生成的基于社区的隐式攻击性语言数据集，其中包含38个不同目标群体的数据。

    arXiv:2403.02472v1 Announce Type: new  Abstract: The widespread presence of offensive languages on social media has resulted in adverse effects on societal well-being. As a result, it has become very important to address this issue with high priority. Offensive languages exist in both explicit and implicit forms, with the latter being more challenging to detect. Current research in this domain encounters several challenges. Firstly, the existing datasets primarily rely on the collection of texts containing explicit offensive keywords, making it challenging to capture implicitly offensive contents that are devoid of these keywords. Secondly, usual methodologies tend to focus solely on textual analysis, neglecting the valuable insights that community information can provide. In this research paper, we introduce a novel dataset OffLanDat, a community based implicit offensive language dataset generated by ChatGPT containing data for 38 different target groups. Despite limitations in genera
    
[^66]: 微调与检索增强生成用于不太流行知识的比较

    Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge

    [https://arxiv.org/abs/2403.01432](https://arxiv.org/abs/2403.01432)

    本文研究了微调和检索增强生成两种方法对大型语言模型在处理低频实体问题回答任务中的影响，发现微调显著提高了各种受欢迎程度的实体的性能，而检索增强生成方法则超过了其他方法。

    

    大型语言模型（LLMs）记忆了大量的事实知识，在各种任务和领域表现出色。然而，观察到当处理不太流行或低频概念和实体时，性能会下降，例如在领域特定应用中。本文探讨和评估了检索增强生成（RAG）和通过合成数据进行微调（FT）对定制LLMs处理低频实体问题回答任务的影响。研究结果表明，FT显著提升了各种受欢迎程度的实体的性能，特别是在最受欢迎和最不受欢迎的群体中，而RAG超越了其他方法。另外，检索和数据增强技术的进步加强了RAG和FT方法的成功。

    arXiv:2403.01432v1 Announce Type: new  Abstract: Large language models (LLMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LLMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LLMs in handling low-frequency entities on question answering task. Our findings indicate that FT significantly boosts the performance across entities of varying popularity, especially in the most and least popular groups, while RAG surpasses other methods. Additionally, the success of both RAG and FT approaches is amplified by advancements in retrieval and data augmentation techniques. 
    
[^67]: LAB：针对ChatBots的大规模对齐

    LAB: Large-Scale Alignment for ChatBots

    [https://arxiv.org/abs/2403.01081](https://arxiv.org/abs/2403.01081)

    介绍了一种名为LAB的方法，旨在克服大型语言模型训练中的可扩展性挑战，通过分类法指导的合成数据生成和多阶段调整框架，实现了对昂贵人工标注和GPT-4等专有模型依赖较少的大规模对齐，提供了一种可扩展、具有成本效益的解决方案，不会出现灾难性遗忘情况，进一步增强了LLM的训练效率。

    

    这项工作介绍了LAB（ChatBots的大规模对齐），这是一种旨在克服大型语言模型（LLM）训练中指令调整阶段的可扩展性挑战的创新方法。通过利用基于分类法的合成数据生成过程和多阶段调整框架，LAB显著减少对昂贵的人类注释和诸如GPT-4之类的专有模型的依赖。我们证明，使用LAB训练的模型在几个基准测试中的性能可以与使用传统人类注释或GPT-4生成的合成数据训练的模型相比具有竞争力。因此，在不会出现灾难性遗忘的情况下，提供了一种可扩展、具有成本效益的解决方案，以增强LLM的能力和指令遵循行为，标志着在高效训练各种应用的LLM方面迈出了一步。

    arXiv:2403.01081v1 Announce Type: new  Abstract: This work introduces LAB (Large-scale Alignment for chatBots), a novel methodology designed to overcome the scalability challenges in the instruction-tuning phase of large language model (LLM) training. Leveraging a taxonomy-guided synthetic data generation process and a multi-phase tuning framework, LAB significantly reduces reliance on expensive human annotations and proprietary models like GPT-4. We demonstrate that LAB-trained models can achieve competitive performance across several benchmarks compared to models trained with traditional human-annotated or GPT-4 generated synthetic data. Thus offering a scalable, cost-effective solution for enhancing LLM capabilities and instruction-following behaviors without the drawbacks of catastrophic forgetting, marking a step forward in the efficient training of LLMs for a wide range of applications.
    
[^68]: 合并来自不同初始化的文本变换器模型

    Merging Text Transformer Models from Different Initializations

    [https://arxiv.org/abs/2403.00986](https://arxiv.org/abs/2403.00986)

    研究了合并不同初始化的Transformer模型的技术，提出了一种模型合并技术以研究这些模型极小值之间的关系，并发现与模型平均相比，通过我们的方法合并这些模型始终可以获得较低的损失障碍。

    

    最近关于一次性基于排列的模型合并的工作表明，不同初始化的模型之间存在令人印象深刻的低或零障碍模连接。然而，尽管Transformer架构在语言领域中占主导地位，但这一领域的研究尚未延伸到Transformer架构。因此，在这项工作中，我们调查了独立Transformer极小值学习类似特征的程度，并提出了一种模型合并技术，以研究损失景观中这些极小值之间的关系。架构的具体细节，如其残差连接、多头注意力和离散的顺序输入，需要特定的干预措施，以便计算留在相同功能等价类中的模型排列。通过我们的方法合并这些模型，我们发现与对几个在一个maske上训练的模型进行模型平均相比，最小值之间的损失障碍一直较低。

    arXiv:2403.00986v1 Announce Type: cross  Abstract: Recent work on one-shot permutation-based model merging has shown impressive low- or zero-barrier mode connectivity between models from completely different initializations. However, this line of work has not yet extended to the Transformer architecture, despite its dominant popularity in the language domain. Therefore, in this work, we investigate the extent to which separate Transformer minima learn similar features, and propose a model merging technique to investigate the relationship between these minima in the loss landscape. The specifics of the architecture, like its residual connections, multi-headed attention, and discrete, sequential input, require specific interventions in order to compute model permutations that remain within the same functional equivalence class. In merging these models with our method, we consistently find lower loss barriers between minima compared to model averaging for several models trained on a maske
    
[^69]: 直接与Chat-Fine-Tuned LLMs的草案模型对齐

    Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs

    [https://arxiv.org/abs/2403.00858](https://arxiv.org/abs/2403.00858)

    通过提出的框架，我们训练了一种用于Llama 2 Chat 7B或更大模型的草案模型，实现了加速推理，仅占原始大小的1.64％。

    

    文本生成与大型语言模型（LLMs）由于其自回归本质、巨大的参数数量和有限的内存带宽而被认为是内存密集型，通常导致低令牌速率。猜测解码已被提出作为LLM推理加速的解决方案。然而，在现代开源LLM系列中，例如Llama 2 7B，由于草案模型通常不可用，因此需要训练高质量的草案模型以通过猜测解码实现推理加速。在本文中，我们提出了一个简单的草案模型训练框架，用于直接与Chat-capable目标模型对齐。通过我们提出的框架，我们训练出Llama 2 Chat Drafter 115M，这是一个适用于Llama 2 Chat 7B或更大模型的草案模型，仅占原始大小的1.64％。我们的训练框架仅包括预训练、蒸馏数据集生成和使用知识蒸馏进行微调，没有额外的对齐步骤。

    arXiv:2403.00858v1 Announce Type: cross  Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional align
    
[^70]: 通过语义感知置换训练来缓解逆转诅咒

    Mitigating Reversal Curse via Semantic-aware Permutation Training

    [https://arxiv.org/abs/2403.00758](https://arxiv.org/abs/2403.00758)

    逆转诅咒问题是导致因果语言模型无法进行双向推理的根本原因之一，在这篇论文中，我们提出了通过语义感知的置换训练来缓解这一问题。

    

    大型语言模型（LLM）在各种任务中取得了令人印象深刻的表现，然而最近的研究表明，因果关系的LLM遭遇了“逆转诅咒”。一个典型的例子是，模型知道“A的父亲是B”，但无法推理出“B的孩子是A”。这一局限性对人工通用智能（AGI）的进展构成了挑战，因为它暗示了模型在理解和应用双向推理方面存在差距。本文首先进行了大量评估，并确定了逆转诅咒的根本原因在于训练和推断阶段之间的词序不同，即因果语言模型在训练数据中预测先行词的能力不足。因此，考虑到在训练数据上进行排列可以被视为潜在解决方案，因为这可以使模型预测先行词或标记。然而，先前的排列方法可能受到截断影响。

    arXiv:2403.00758v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the "reversal curse". It is a typical example that the model knows "A's father is B", but is unable to reason "B's child is A". This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may dis
    
[^71]: 具有增强可检测性和语义连贯性的大型语言模型的特定令牌水印技术

    Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models

    [https://arxiv.org/abs/2402.18059](https://arxiv.org/abs/2402.18059)

    提出一种利用多目标优化方法的水印技术，通过轻量级网络生成特定令牌水印logits和分割比率，在保证检测性的同时提升了文本的语义完整性。

    

    大型语言模型生成高质量的响应，潜在地存在误导信息的问题，强调了通过区分人工智能生成和人类撰写的文本来加以规范的必要性。水印技术在这种情况下至关重要，它涉及在LLM推理阶段向文本中嵌入隐藏标记，而这对人类来说是不可感知的。然而，当前的水印算法面临着实现插入水印的可检测性和生成文本的语义完整性两方面的挑战，增强其中一个方面常常会损害另一个方面。为了克服这一问题，我们引入了一种新颖的多目标优化（MOO）方法，用于水印技术，利用轻量级网络生成特定令牌水印logits和分割比率。通过利用MOO来优化检测和语义目标函数，我们的方法同时实现了可检测性和语义完整性。实验结果表明，我们的方法在...

    arXiv:2402.18059v1 Announce Type: cross  Abstract: Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Current watermarking algorithms, however, face the challenge of achieving both the detectability of inserted watermarks and the semantic integrity of generated texts, where enhancing one aspect often undermines the other. To overcome this, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that ou
    
[^72]: 在没有基准实况的情况下对大型语言模型进行排名

    Ranking Large Language Models without Ground Truth

    [https://arxiv.org/abs/2402.14860](https://arxiv.org/abs/2402.14860)

    不需要基准实况或参考响应的条件下，通过考虑模型的三元组来排名大型语言模型，并提出了两种排名方法。

    

    随着大型语言模型（LLMs）的普及和影响力的增强，评估和排名LLMs已成为一个重要问题。现有的评估方法要么需要获取昂贵的人类响应，要么使用LLMs成对地互相评估，这可能不够可靠。本文提供了一个新的视角，在给定一组提示数据集（比如问题、说明等）和一组LLMs的情况下，我们在没有任何基准实况或参考响应的情况下对它们进行排名。受到现实生活的启发，其中专家和有知识的人都能识别一个新手，我们的主要思路是考虑模型的三元组，其中每个模型评估其他两个模型，能够以很高的概率正确识别最差的模型。我们还分析了我们的想法并提供了成功的充分条件。通过反复应用这一想法，我们提出了两种对LLMs进行排名的方法。

    arXiv:2402.14860v1 Announce Type: cross  Abstract: Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generati
    
[^73]: 一种用于电子商务产品描述生成的多模态上下文调整方法

    A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation

    [https://arxiv.org/abs/2402.13587](https://arxiv.org/abs/2402.13587)

    提出了一种用于电子商务产品描述生成的多模态上下文调整方法ModICT，通过引入相似产品样本和利用语言模型的上下文学习能力，旨在解决生成描述中常见且忽略产品特征的问题

    

    在本文中，我们提出了一种新的设置，用于从图像中生成产品描述，其中包含营销关键词。它利用视觉和文本信息的综合能力，创建更加符合产品独特特性的描述。我们提出了一种简单有效的多模态上下文调整方法ModICT，通过引入相似的产品样本作为参考，并利用语言模型的上下文学习能力

    arXiv:2402.13587v1 Announce Type: new  Abstract: In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords. It leverages the combined power of visual and textual information to create descriptions that are more tailored to the unique features of products. For this setting, previous methods utilize visual and textual encoders to encode the image and keywords and employ a language model-based decoder to generate the product description. However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on large-scale samples makes models concentrate on common words yet ignore the product features. To alleviate the issue, we present a simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces a similar product sample as the reference and utilizes the in-context learning capability of language models to 
    
[^74]: AnyGPT：统一的多模式离散序列建模语言模型

    AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling

    [https://arxiv.org/abs/2402.12226](https://arxiv.org/abs/2402.12226)

    AnyGPT是一个统一的多模态语言模型，通过离散表示实现各种模态的统一处理，能够在不改变大型语言模型架构或训练方式的情况下稳定训练，为新模态的无缝整合提供了可能。

    

    我们介绍了 AnyGPT，这是一个任意多模式语言模型，利用离散表示统一处理各种模态，包括语音、文本、图像和音乐。AnyGPT 可以稳定训练，无需对当前大型语言模型（LLM）架构或训练范式进行任何改动。相反，它仅依赖于数据级预处理，促进了新模态的无缝集成到LLM中，类似于新语言的整合。我们构建了一个多模式文本中心的数据集，用于多模式对齐预训练。利用生成模型，我们合成了第一个大规模任意多模式指令数据集。它包括108k个多轮对话示例，精细地交织各种模态，从而使模型能够处理多模态输入和输出的任意组合。实验结果表明，AnyGPT能够促进...

    arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
    
[^75]: 知识到SQL：用数据专家LLM增强SQL生成

    Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM

    [https://arxiv.org/abs/2402.11517](https://arxiv.org/abs/2402.11517)

    Knowledge-to-SQL框架利用数据专家LLM提供有用知识，增强文本到SQL模型的鲁棒性。

    

    为用户查询生成准确的SQL（文本到SQL）是一个长期存在的问题，因为生成SQL需要理解查询和数据库，然后根据数据库检索准确的数据。现有模型依赖于大型语言模型（LLMs）的综合能力，根据数据库模式生成SQL。然而，有些必要的知识没有明确包含在数据库模式中，或者被LLMs学习了。因此，知识不足的查询生成的SQL可能是不准确的，这会对文本到SQL模型的鲁棒性产生负面影响。为了应对这种情况，我们提出了Knowledge-to-SQL框架，该框架采用定制的数据专家LLM（DELLM）为所有类型的文本到SQL模型提供有用的知识。具体地，我们详细介绍了DELLM的设计，包括表格读取和基本微调过程。

    arXiv:2402.11517v1 Announce Type: new  Abstract: Generating accurate SQL for user queries (text-to-SQL) is a long-standing problem since the generation of the SQL requires comprehending the query and database and retrivale the accurate data from the database accordingly. Existing models rely on the comprehensive ability of Large Language Models (LLMs) to generate the SQL according to the database schema. However, there is some necessary knowledge that is not explicitly included in the database schema or has been learned by LLMs. Thus, the generated SQL of the knowledge-insufficient queries may be inaccurate, which negatively impacts the robustness of the text-to-SQL models. To deal with this situation, we propose the Knowledge-to-SQL framework, which employs tailored Data Expert LLM (DELLM) to provide helpful knowledge for all types of text-to-SQL models. Specifically, we provide the detailed design of DELLM, in terms of table reading, and the basic fine-tuning process. We further prov
    
[^76]: MUSTARD：掌握定理和证明数据的统一合成

    MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data

    [https://arxiv.org/abs/2402.08957](https://arxiv.org/abs/2402.08957)

    这项工作介绍了MUSTARD，一种掌握定理和证明数据统一合成的数据生成框架，通过三个阶段的合成，实现了高质量和多样化的问题和推理步骤的生成。

    

    最近，大型语言模型（LLMs）在各种任务中取得了显著进展，包括数学推理和定理证明。由于这两个任务需要严格和形式化的多步推理，它们是探索LLMs推理能力的吸引领域，但仍面临重要挑战。以前的研究如Chain-of-Thought（CoT）揭示了中间步骤指导的有效性。然而，这种逐步注释需要大量的劳动力，导致当前基准测试的训练步骤不足。为了填补这一空白，本研究引入了MUSTARD，一种数据生成框架，可以主导高质量和多样化的定理和证明数据的统一合成。MUSTARD通过三个阶段合成数据：（1）它随机选择几个数学概念作为问题的类别。（2）然后，它使用选定的概念提示生成性语言模型，以获得问题和它们的推理步骤。

    arXiv:2402.08957v1 Announce Type: new Abstract: Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks. To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-w
    
[^77]: 英文提示比目标语言提示更适用于基于NLI的零-shot情绪分类

    English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts

    [https://arxiv.org/abs/2402.03223](https://arxiv.org/abs/2402.03223)

    本研究填补了一个研究空白，探讨了在非英文文本中应该使用哪种语言来提示情绪标签。

    

    文本情绪分类是一个具有挑战性和主观性的任务，由于需要进行认知推论过程来解释文字刺激。此外，情绪类别集合高度依赖于特定领域。例如，文学分析可能需要使用审美情感（例如，发现某物美丽），而社交媒体分析则可以从细粒度的集合中获取好处（例如，将愤怒与烦恼分开），与基本情绪类别相对应。这使得该任务成为了零-shot分类的一个有趣领域，在这种分类中，模型开发时不知道标签集合。不幸的是，大多数情绪分析资源都是英文的，因此，情绪分析的大部分研究都是用英文进行的，包括那些涉及使用提示语言模型的研究。这给我们留下一个研究空白，我们在本文中探讨：在非英文文本中，我们应该用哪种语言提示情绪标签？

    Emotion classification in text is a challenging and subjective task, due to the involved cognitive inference processes that are required to interpret a textual stimulus. In addition, the set of emotion categories is highly domain-specific. For instance, literature analysis might require the use of aesthetic emotions (e.g., finding something beautiful), and social media analysis could benefit from fine-grained sets (e.g., separating anger from annoyance) in contrast to basic emotion categories. This renders the task an interesting field for zero-shot classifications, in which the label set is not known at model development time. Unfortunately, most resources for emotion analysis are English, and therefore, most studies on emotion analysis have been performed in English, including those that involve prompting language models for text labels. This leaves us with a research gap that we address in this paper: In which language should we prompt for emotion labels on non-English texts? This i
    
[^78]: 大型语言模型能否取代经济选择预测实验室？

    Can Large Language Models Replace Economic Choice Prediction Labs?

    [https://arxiv.org/abs/2401.17435](https://arxiv.org/abs/2401.17435)

    该论文研究大型语言模型是否能够取代经济实验室进行选择预测，并通过相关实验证明了其可行性。

    

    经济选择预测是一项具有挑战性的重要任务，往往受限于获取人类选择数据的困难。实验经济学研究在很大程度上专注于简单的选择环境。最近，人工智能界以两种方式为该努力做出了贡献：考虑大型语言模型是否可以代替人类在上述简单选择预测环境中，以及通过机器学习视角研究更复杂但仍严格的实验经济学环境，包括不完全信息、重复博弈和基于自然语言交流的说服游戏。这引发了一个重要的灵感：大型语言模型是否能够完全模拟经济环境，并生成用于高效人类选择预测的数据，替代复杂的经济实验室研究？我们在这个主题上开创了研究，并展示了其可行性。特别是，我们表明仅在大型语言模型生成的数据上训练的模型可以有效地进行预测。

    Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
    
[^79]: 通过对比激活加法指导Llama 2

    Steering Llama 2 via Contrastive Activation Addition

    [https://arxiv.org/abs/2312.06681](https://arxiv.org/abs/2312.06681)

    引入Contrastive Activation Addition（CAA）方法，通过修改语言模型的激活来精确控制目标行为的程度，显著改变模型行为并在微调和系统提示设计的基础上提供额外有效性。

    

    我们引入了一种创新的方法Contrastive Activation Addition（CAA），用于通过在前向传递过程中修改其激活来指导语言模型。CAA通过对某种行为的正面和负面示例之间残差流激活的差异求平均，计算出“指导向量”。在推断过程中，在用户提示后的所有token位置上以正负系数添加这些指导向量，从而精确控制目标行为的程度。我们通过使用多项选择行为问题数据集和开放式生成任务在Llama 2 Chat上评估了CAA的有效性。我们证明CAA显着改变了模型行为，不仅在传统方法如微调和系统提示设计的基础上有效，而且最小程度地降低了功能。此外，我们对模型的行为做出了更深入的洞察。

    arXiv:2312.06681v3 Announce Type: replace-cross  Abstract: We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes "steering vectors" by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights in
    
[^80]: 在自然语言交互指令环境中解释用户请求

    Interpreting User Requests in the Context of Natural Language Standing Instructions

    [https://arxiv.org/abs/2311.09796](https://arxiv.org/abs/2311.09796)

    在使用大型语言模型的自然语言接口时，本研究提出了一种基于用户常设指令的对话建模方法，通过将用户的约束和偏好作为上下文，从而在类似请求中实现自动化。

    

    自然语言接口的用户通常由大型语言模型（LLMs）驱动，并且经常必须在每次进行类似请求时重复他们的偏好。我们描述了一种基于LLM的对话建模方法，其中持久的用户约束和偏好 - 统称为常设指令 - 作为这种接口的额外上下文。例如，当用户说“我饿了”时，先前表达的波斯食物偏好可以自动添加到LLM提示中，影响搜索相关餐馆。我们开发了NLSI，一个包含超过2.4K跨越17个领域的对话的语言到程序数据集，其中每个对话都与用户配置文件（一组用户特定的常设指令）和相应的结构化表示形式（API调用）配对。 NLSI的一个关键挑战是确定哪些常设指令子集适用于给定的对话。

    arXiv:2311.09796v2 Announce Type: replace-cross  Abstract: Users of natural language interfaces, generally powered by Large Language Models (LLMs),often must repeat their preferences each time they make a similar request. We describe an approach to LLM-based dialogue modeling in which persistent user constraints and preferences -- collectively termed standing instructions -- as additional context for such interfaces. For example, when a user states "I'm hungry", a previously expressed preference for Persian food can be automatically added to the LLM prompt, influencing the search for relevant restaurants. We develop NLSI, a language-to-program dataset consisting of over 2.4K dialogues spanning 17 domains, where each dialogue is paired with a user profile (a set of users specific standing instructions) and corresponding structured representations (API calls). A key challenge in NLSI is to identify which subset of the standing instructions is applicable to a given dialogue. NLSI contains
    
[^81]: LLM能遵守简单规则吗?

    Can LLMs Follow Simple Rules?

    [https://arxiv.org/abs/2311.04235](https://arxiv.org/abs/2311.04235)

    提出了一个名为RuLES的程序框架，用于衡量LLMs在与用户交互时遵守规则的能力。

    

    随着大型语言模型（LLMs）在现实世界中承担越来越多的责任，能够以可靠的方式指定和约束这些系统的行为变得至关重要。我们提出了规则遵循语言评估场景（RuLES），这是一个测量LLMs遵循规则能力的程序框架，包括14个简单的文本场景，模型在与用户交互时被指示遵守各种规则。

    arXiv:2311.04235v2 Announce Type: replace  Abstract: As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as "do not generate abusive content", but these may be circumvented by jailbreaking techniques. Existing evaluations of adversarial attacks and defenses on LLMs generally require either expensive manual review or unreliable heuristic checks. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 14 simple text scenarios in which the model is instructed to obey various rules while interacting with the user. Each scenario has a programmatic evaluation function to determine whether the model has broken any rules in a conversation. Our evaluations of proprietar
    
[^82]: 基于LLM的智能体社会行为研究：Avalon游戏中的协作与对抗

    LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay

    [https://arxiv.org/abs/2310.14985](https://arxiv.org/abs/2310.14985)

    本文提出了一个新颖的框架，旨在无缝适应Avalon游戏，通过多智能体系统实现了有效的沟通和互动，评估了智能体的性能和社会行为，展示了LLM智能体在游戏中具有潜力。

    

    本文旨在研究揭示基于LLM的智能体社会行为的开放性研究问题。为达到这一目标，我们采用了Avalon作为代表性的沟通游戏环境，并使用系统提示引导LLM智能体进行游戏。虽然先前的研究已经进行了关于LLM智能体的游戏玩法的初步研究，但是他们的社会行为仍缺乏研究。本文提出了一个新颖的框架，旨在无缝适应Avalon游戏。我们提出的框架的核心是一个多智能体系统，可以实现智能体之间的有效沟通和互动。我们根据两个角度的度量标准评估了我们框架的性能：赢得游戏和更分析LLM智能体的社会行为。我们的结果展示了我们的框架在生成自适应和智能智能体方面的有效性，并突显了LLM智能体在应对中的潜力。

    arXiv:2310.14985v2 Announce Type: replace  Abstract: This paper aims to investigate the open research problem of uncovering the social behaviors of LLM-based agents. To achieve this goal, we adopt Avalon, a representative communication game, as the environment and use system prompts to guide LLM agents to play the game. While previous studies have conducted preliminary investigations into gameplay with LLM agents, there lacks research on their social behaviors. In this paper, we present a novel framework designed to seamlessly adapt to Avalon gameplay. The core of our proposed framework is a multi-agent system that enables efficient communication and interaction among agents. We evaluate the performance of our framework based on metrics from two perspectives: winning the game and analyzing the social behaviors of LLM agents. Our results demonstrate the effectiveness of our framework in generating adaptive and intelligent agents and highlight the potential of LLM-based agents in address
    
[^83]: 语言模型写作是否会降低内容多样性？

    Does Writing with Language Models Reduce Content Diversity?

    [https://arxiv.org/abs/2309.05196](https://arxiv.org/abs/2309.05196)

    写作时使用InstructGPT（而不是GPT3）会显著降低内容多样性，增加不同作者之间的相似性，并减少整体的词汇和内容多样性。

    

    大型语言模型（LLMs）引发了与模型辅助合作写作的激增。当不同用户纳入同一模型的建议时，会存在内容多样性减少的风险，可能限制公共话语中的多元观点。本研究通过控制实验测量了协同写作对多样性的影响，在该实验中，用户以三种设置撰写议论性文章--使用基本LLM（GPT3）、经过反馈调整的LLM（InstructGPT）以及不使用模型帮助写作。我们开发了一组多样性指标，并发现使用InstructGPT进行写作（而不是GPT3）会导致多样性明显降低。具体而言，它增加了不同作者的写作之间的相似性，减少了整体的词汇和内容多样性。此外，我们还发现这种影响主要来源于InstructGPT对共同撰写的文本贡献较少。

    arXiv:2309.05196v2 Announce Type: replace  Abstract: Large language models (LLMs) have led to a surge in collaborative writing with model assistance. As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse. In this work, we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups -- using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We develop a set of diversity metrics and find that writing with InstructGPT (but not the GPT3) results in a statistically significant reduction in diversity. Specifically, it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. We additionally find that this effect is mainly attributable to InstructGPT contributing less diverse text to co-writt
    
[^84]: 交互式问答系统：文献综述

    Interactive Question Answering Systems: Literature Review

    [https://arxiv.org/abs/2209.01621](https://arxiv.org/abs/2209.01621)

    交互式问答系统是问答和对话系统的结合，用户可以用自然语言提问并与系统动态交互，获得更精确的结果。

    

    arXiv:2209.01621v2 公告类型: 替换-跨  摘要: 问答系统被公认为在网络上寻求信息的流行且有效的手段。在这种系统中，信息寻找者可以通过用自然语言提出问题来获得简洁的回答。交互式问答是最近提出的并越来越流行的解决方案，位于问答和对话系统的交集处。一方面，用户可以用普通语言提问并找到她问题的实际回答；另一方面，如果初始请求中存在多个可能的回复、很少或模棱两可，系统可以将问答会话延长为对话。通过允许用户提出更多问题，交互式问答使用户能够动态地与系统交互并获得更精确的结果。本综述提供了交互式问答系统的详细概述。

    arXiv:2209.01621v2 Announce Type: replace-cross  Abstract: Question answering systems are recognized as popular and frequently effective means of information seeking on the web. In such systems, information seekers can receive a concise response to their query by presenting their questions in natural language. Interactive question answering is a recently proposed and increasingly popular solution that resides at the intersection of question answering and dialogue systems. On the one hand, the user can ask questions in normal language and locate the actual response to her inquiry; on the other hand, the system can prolong the question-answering session into a dialogue if there are multiple probable replies, very few, or ambiguities in the initial request. By permitting the user to ask more questions, interactive question answering enables users to dynamically interact with the system and receive more precise results. This survey offers a detailed overview of the interactive question-ans
    
[^85]: 手机、音节和单词能否作为跨情境视听学习的副产品而出现？-- 一项计算研究

    Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation

    [https://arxiv.org/abs/2109.14200](https://arxiv.org/abs/2109.14200)

    研究探讨了语言学习中，通过跨情境视听学习，音素、音节和单词能否作为副产品出现，并支持不同形式表征间的转换。

    

    数十年的研究探讨了语言学习婴儿如何学会区分语音，分割单词，以及将单词与其含义关联起来。尽管这些能力的逐渐发展是毋庸置疑的，但这些技能的确切性质和潜在的心理表征仍然不清楚。与此同时，计算研究表明，通过语音和同时具有指称不明确的视觉输入之间的统计学习，可以实现对基本语音的理解。这些模型可以在没有诸如语言单位的表征，以及没有专门针对这些单位的学习机制的情况下运行。这引发了一个问题，即在多大程度上，类似音素、音节和单词的语言单位的知识实际上能够作为潜在表征出现，支持语音与其他形式表征之间的转换，而不需要专门的学习机制来实现。

    arXiv:2109.14200v2 Announce Type: replace-cross  Abstract: Decades of research has studied how language learning infants learn to discriminate speech sounds, segment words, and associate words with their meanings. While gradual development of such capabilities is unquestionable, the exact nature of these skills and the underlying mental representations yet remains unclear. In parallel, computational studies have shown that basic comprehension of speech can be achieved by statistical learning between speech and concurrent referentially ambiguous visual input. These models can operate without prior linguistic knowledge such as representations of linguistic units, and without learning mechanisms specifically targeted at such units. This has raised the question of to what extent knowledge of linguistic units, such as phone(me)s, syllables, and words, could actually emerge as latent representations supporting the translation between speech and representations in other modalities, and withou
    
[^86]: Q&A提示：通过挖掘问题-回答提示来发现丰富的视觉线索，以满足对多样世界知识的视觉问答的需求

    Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge. (arXiv:2401.10712v1 [cs.CV])

    [http://arxiv.org/abs/2401.10712](http://arxiv.org/abs/2401.10712)

    本论文提出了一种叫做Q&A提示的方法，通过挖掘图像中的问题-回答对来发现丰富的视觉线索，以帮助AI模型更好地理解复杂视觉问题，提高跨模态推理能力。

    

    随着多模态大型语言模型的突破，回答需要高级推理能力和世界知识的复杂视觉问题比以往任何时候都更重要。然而，为AI模型配备强大的跨模态推理能力仍然具有挑战性，因为人类的认知方案尚未系统地被理解。在本文中，我们相信，如果我们能尽可能收集给定图像中的视觉线索，我们将能更准确地识别图像，更好地理解问题，更容易回忆相关知识，并最终推理出答案。我们通过在图像中挖掘问题-回答对来发现这些丰富的视觉线索，并将它们作为提示发送到多模态大型语言模型中。我们称之为Q&A提示的方法。具体而言，我们首先使用训练集中的图像-答案对和相应的问题作为输入和输出来训练一个视觉问题生成模型。

    With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question gener
    
[^87]: AST-T5：面向代码生成和理解的结构感知预训练模型

    AST-T5: Structure-Aware Pretraining for Code Generation and Understanding. (arXiv:2401.03003v1 [cs.SE])

    [http://arxiv.org/abs/2401.03003](http://arxiv.org/abs/2401.03003)

    AST-T5是一种结构感知的预训练模型，通过利用抽象语法树（AST）来增强代码生成、转换和理解的能力。它优于其他同等大小的语言模型，并在代码到代码任务中表现出色。

    

    大型语言模型在代码相关任务中取得了显著进展，然而许多模型将代码视为简单序列，忽略了其结构化特性。我们引入了AST-T5，一种新颖的预训练范式，利用抽象语法树（AST）增强了代码生成、转换和理解。通过动态规划，我们的AST感知分割保留了代码结构，而AST感知跨度破坏目标使模型能够重建各种代码结构。与其他模型不同，AST-T5避免了复杂的程序分析或架构更改，因此可以与任何编码器-解码器Transformer无缝集成。评估结果显示，AST-T5在各种代码相关任务中始终优于同等大小的语言模型。结构感知使得AST-T5在代码到代码任务中特别强大，在Bugs2Fix任务的精确匹配得分上超过CodeT5 2个点，并在CodeXGLUE中的Java-C#转换任务的精确匹配得分上超过CodeT5 3个点。

    Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids intricate program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in CodeXGLUE. Our
    
[^88]: 机器翻译自动评估的参考文献质量和数量

    Quality and Quantity of Machine Translation References for Automated Metrics. (arXiv:2401.01283v1 [cs.CL])

    [http://arxiv.org/abs/2401.01283](http://arxiv.org/abs/2401.01283)

    本研究发现，机器翻译评估的较高质量参考文献对于评估指标与人类评价之间的相关性更好。每个段落平均使用7个参考文献有助于提升所有评估指标。不同质量的供应商参考文献可以混合使用来提高评估指标的准确性。这些发现可用于在特定预算下创建参考文献的共享任务的评估者。

    

    自动机器翻译评估指标通常使用人工翻译来确定系统翻译的质量。领域内的共识认为人工参考文献应具有很高的质量。然而，目前没有成本效益分析可以指导计划收集机器翻译评估参考文献的从业者。我们发现，较高质量的参考文献能够在段落级别上与人类评价的相关性更好。每个段落平均使用7个参考文献有助于所有评估指标的提升。有趣的是，来自不同质量的供应商的参考文献可以混合使用，并提高评估指标的准确性。然而，较高质量的参考文献制作成本更高，我们将其视为一个优化问题：在特定预算下，应该收集哪些参考文献以最大化评估指标的准确性。这些发现可用于在特定预算下创建参考文献的共享任务的评估者。

    Automatic machine translation metrics often use human translations to determine the quality system translations. Common wisdom in the field dictates that the human references should be of very high quality. However, there are no cost-benefit analyses that could be used to guide practitioners who plan to collect references for machine translation evaluation. We find that higher-quality references lead to better metric correlations with humans at the segment-level. Having up to 7 references per segment and taking their average helps all metrics. Interestingly, the references from vendors of different qualities can be mixed together and improve metric success. Higher quality references, however, cost more to create and we frame this as an optimization problem: given a specific budget, what references should be collected to maximize metric success. These findings can be used by evaluators of shared tasks when references need to be created under a certain budget.
    
[^89]: ChipNeMo: 用于芯片设计的领域自适应LLMs

    ChipNeMo: Domain-Adapted LLMs for Chip Design. (arXiv:2311.00176v1 [cs.CL])

    [http://arxiv.org/abs/2311.00176](http://arxiv.org/abs/2311.00176)

    ChipNeMo通过领域自适应技术，实现了在工业芯片设计中大幅提升LLM性能，同时减小了模型尺寸，在工程助手、脚本生成和缺陷分析等方面具有良好表现。

    

    ChipNeMo旨在探索大型语言模型（LLMs）在工业芯片设计中的应用。我们不直接使用商业或开源LLMs，而是采用以下领域自适应技术：定制分词器、领域自适应持续预训练、带有领域特定指令的监督微调（SFT）和领域自适应检索模型。我们在芯片设计的三个选定LLM应用上评估了这些方法：工程助手聊天机器人、EDA脚本生成以及缺陷摘要和分析。我们的结果显示，这些领域自适应技术使LLM在这三个应用中性能大幅提升，在各种设计任务上可以实现高达5倍的模型尺寸缩减，同时具有类似或更好的性能。我们的研究结果还表明，当前的结果和理想结果之间还有改进的空间。我们相信进一步的研究将有助于解决这个问题。

    ChipNeMo aims to explore the applications of large language models (LLMs) for industrial chip design. Instead of directly deploying off-the-shelf commercial or open-source LLMs, we instead adopt the following domain adaptation techniques: custom tokenizers, domain-adaptive continued pretraining, supervised fine-tuning (SFT) with domain-specific instructions, and domain-adapted retrieval models. We evaluate these methods on three selected LLM applications for chip design: an engineering assistant chatbot, EDA script generation, and bug summarization and analysis. Our results show that these domain adaptation techniques enable significant LLM performance improvements over general-purpose base models across the three evaluated applications, enabling up to 5x model size reduction with similar or better performance on a range of design tasks. Our findings also indicate that there's still room for improvement between our current results and ideal outcomes. We believe that further investigati
    
[^90]: GenTKG: 基于生成模型的时间知识图谱预测

    GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])

    [http://arxiv.org/abs/2310.07793](http://arxiv.org/abs/2310.07793)

    研究提出了一种名为GenTKG的生成模型，用于在时间知识图谱上进行预测。该模型通过结合基于时间逻辑规则的检索策略和轻量级的参数效率指导，克服了复杂的时间图数据结构和庞大的数据量所带来的挑战。

    

    大规模语言模型(LLM)的快速发展引发了对时间知识图谱(tKG)领域的兴趣，其中传统的基于嵌入和规则的模型占主导地位。目前仍然存在一个问题，即预训练的LLM是否能够理解结构化的时间关系数据，并取代它们成为时间关系预测的基础模型。因此，我们将时间知识预测引入生成模式。然而，在复杂的时间图数据结构和LLM可以处理的序列自然表达之间存在巨大的鸿沟，在tKG的庞大数据量和微调LLM的巨大计算成本之间也存在挑战。为了解决这些挑战，我们提出了一种新颖的检索增强生成框架，称为GenTKG，它在tKG上执行生成式预测，结合了基于时间逻辑规则的检索策略和轻量级的参数效率指导。通过大量实验证明了GenTKG的有效性。

    The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
    
[^91]: 无监督语言模型蒸馏的事实验证

    Unsupervised Fact Verification by Language Model Distillation. (arXiv:2309.16540v1 [cs.CL])

    [http://arxiv.org/abs/2309.16540](http://arxiv.org/abs/2309.16540)

    本文提出了一种名为SFAVEL的无监督框架，通过语言模型蒸馏将自监督特征转化为高质量的主张-事实对齐，实现无监督事实验证。这通过一种新颖的对比损失函数实现，同时保留语料库间的语义关系。

    

    无监督事实验证旨在通过可靠知识库中的证据来验证主张，而无需任何形式的数据注释。为了解决这个挑战，算法必须为每个主张生成既语义明确又紧凑的特征，以便与源信息进行语义对齐。与之前的工作不同，前者通过学习包含主张及其相应标签的注释语料库来解决对齐问题。我们提出了SFAVEL（通过语言模型蒸馏的自监督事实验证），这是一个新颖的无监督框架，利用预训练的语言模型将自监督特征蒸馏为高质量的主张-事实对齐，而无需注释。这是通过一种新颖的对比损失函数实现的，该函数鼓励特征在保持语料库间的语义关系的同时实现高质量的主张和证据对齐。值得注意的是，我们展示了达到新颖的状态一.

    Unsupervised fact verification aims to verify a claim using evidence from a trustworthy knowledge base without any kind of data annotation. To address this challenge, algorithms must produce features for every claim that are both semantically meaningful, and compact enough to find a semantic alignment with the source information. In contrast to previous work, which tackled the alignment problem by learning over annotated corpora of claims and their corresponding labels, we propose SFAVEL (Self-supervised Fact Verification via Language Model Distillation), a novel unsupervised framework that leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations. This is enabled by a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments whilst preserving the semantic relationships across the corpora. Notably, we present results that achieve a new state-of-the
    
[^92]: 复杂长视程机器人操作任务的内在语言引导探索

    Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks. (arXiv:2309.16347v1 [cs.RO])

    [http://arxiv.org/abs/2309.16347](http://arxiv.org/abs/2309.16347)

    本文提出了基于大型语言模型的内在引导探索（IGE-LLMs）框架，通过利用LLMs作为辅助内在奖励，解决了复杂长视程机器人操作任务中奖励稀疏问题，并在实验中展示了其较高的性能和模块化特性。

    

    当前的强化学习算法在稀疏和复杂的环境中面临困境，尤其是在涉及众多不同序列的长视程操作任务中。在本研究中，我们提出了基于大型语言模型的内在引导探索（IGE-LLMs）框架。通过利用LLMs作为辅助内在奖励，IGE-LLMs引导强化学习中的探索过程，以解决复杂的长视程操作任务中奖励稀疏问题。我们在一个具有探索挑战的环境和一个同时面临探索和长视程挑战的复杂机器人操作任务中评估了我们的框架和相关的内在学习方法。结果显示，IGE-LLMs(i)在相关的内在方法和直接使用LLMs进行决策的性能上表现出明显的较高水平，(ii)可以与现有的学习方法相结合和互补，突出其模块化性能，(iii)对于不同的内在缩放参数比较不敏感。

    Current reinforcement learning algorithms struggle in sparse and complex environments, most notably in long-horizon manipulation tasks entailing a plethora of different sequences. In this work, we propose the Intrinsically Guided Exploration from Large Language Models (IGE-LLMs) framework. By leveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides the exploratory process in reinforcement learning to address intricate long-horizon with sparse rewards robotic manipulation tasks. We evaluate our framework and related intrinsic learning methods in an environment challenged with exploration, and a complex robotic manipulation task challenged by both exploration and long-horizons. Results show IGE-LLMs (i) exhibit notably higher performance over related intrinsic methods and the direct use of LLMs in decision-making, (ii) can be combined and complement existing learning methods highlighting its modularity, (iii) are fairly insensitive to different intrinsic scaling parameters, and 
    
[^93]: CoT-BERT: 通过思维链条增强无监督句子表示

    CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought. (arXiv:2309.11143v1 [cs.CL])

    [http://arxiv.org/abs/2309.11143](http://arxiv.org/abs/2309.11143)

    CoT-BERT提出了一种通过思维链条增强无监督句子表示的方法，通过两个阶段的处理，引入思维链条的概念进行向量化，以提高模型性能。

    

    无监督句子表示学习旨在将输入句子转化为富含复杂语义信息的固定长度向量，同时消除对标注数据的依赖。近年来，在对比学习和提示工程的推动下，该领域取得了显著进展，极大地缩小了无监督和有监督策略之间的差距。然而，在这个轨迹中，仍然没有充分利用思维链条的潜在能力。为了释放预训练模型（如BERT）中的潜能，我们提出了一个句子表示的两阶段方法：理解和摘要。随后，后一阶段的输出被利用为输入句子的向量化表示。为了进一步提高性能，我们对对比学习损失函数和模板去噪技术进行了精细调整。严格的实验验证了我们的方法CoT-BERT的优越性。

    Unsupervised sentence representation learning aims to transform input sentences into fixed-length vectors enriched with intricate semantic information while obviating the reliance on labeled data. Recent progress within this field, propelled by contrastive learning and prompt engineering, has significantly bridged the gap between unsupervised and supervised strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains largely untapped within this trajectory. To unlock latent capabilities within pre-trained models, such as BERT, we propose a two-stage approach for sentence representation: comprehension and summarization. Subsequently, the output of the latter phase is harnessed as the vectorized representation of the input sentence. For further performance enhancement, we meticulously refine both the contrastive learning loss function and the template denoising technique for prompt engineering. Rigorous experimentation substantiates our method, CoT-BERT, transcending a
    
[^94]: SIB-200: 包括200多种语言和方言的简单、全面和大型主题分类评估数据集

    SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects. (arXiv:2309.07445v1 [cs.CL])

    [http://arxiv.org/abs/2309.07445](http://arxiv.org/abs/2309.07445)

    本研究提出了SIB-200数据集，在200多种语言和方言中提供了一个大规模、全面的主题分类评估数据集。该数据集填补了自然语言理解领域中对评估数据集的缺乏，通过全监督、跨语言迁移和大型语言模型提示的评估，发现性能仍存在差距。

    

    尽管在多语言自然语言处理方面取得了进展，但评估通常仅限于一小部分带有可用数据集的语言，排除了许多资源匮乏的语言。本文创建了SIB-200，这是一个用于主题分类的大规模开放源代码基准数据集，涵盖了200多种语言和方言，以弥补自然语言理解（NLU）缺乏评估数据集的问题。对于SIB-200中涵盖的许多语言来说，这是首个公开可用的NLU评估数据集。该数据集基于Flores-200机器翻译语料库，并对该语料库涵盖的其他203种语言进行了句子级注释。尽管该任务简单，但我们在全监督设置、跨语言迁移设置和大型语言模型提示设置下的评估结果表明，性能仍存在较大差距。

    Despite the progress we have recorded in the last few years in multilingual natural language processing, evaluation is typically limited to a small set of languages with available datasets which excludes a large number of low-resource languages. In this paper, we created SIB-200 -- a large-scale open-sourced benchmark dataset for topic classification in 200 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU). For many of the languages covered in SIB-200, this is the first publicly available evaluation dataset for NLU. The dataset is based on Flores-200 machine translation corpus. We annotated the English portion of the dataset and extended the sentence-level annotation to the remaining 203 languages covered in the corpus. Despite the simplicity of this task, our evaluation in full-supervised setting, cross-lingual transfer setting and prompting of large language model setting show that there is still a large gap between the performa
    
[^95]: 用于低资源语言的程序性语言理解基准测试：以土耳其语为例的案例研究

    Benchmarking Procedural Language Understanding for Low-Resource Languages: A Case Study on Turkish. (arXiv:2309.06698v1 [cs.CL])

    [http://arxiv.org/abs/2309.06698](http://arxiv.org/abs/2309.06698)

    本研究通过对土耳其语的程序文本进行案例研究，扩展了土耳其wikiHow的教程数量，并研究了几个下游任务。研究发现，在大多数任务中，语言特定模型相对于多语言模型具有明显优势。

    

    理解程序性自然语言（例如，逐步说明）是执行和规划的关键步骤。然而，虽然英语中存在丰富的语料库和下游任务，但大多数语言缺乏这样的资源。为了解决这个问题，我们对土耳其程序文本进行了案例研究。我们首先使用自动翻译工具将土耳其wikiHow中的教程数量从2,000个扩展到52,000个，翻译质量和对原始含义的忠实性由专家团队在一个随机集上进行验证。然后，我们在语料库上生成了多个下游任务，例如链接操作、目标推理和摘要。为了解决这些任务，我们通过微调大型语言特定模型（如TR-BART和BERTurk）以及多语言模型（如mBART、mT5和XLM）实现了强基线模型。我们发现，在大多数任务中，语言特定模型始终以显著的优势胜过多语言模型。

    Understanding procedural natural language (e.g., step-by-step instructions) is a crucial step to execution and planning. However, while there are ample corpora and downstream tasks available in English, the field lacks such resources for most languages. To address this gap, we conduct a case study on Turkish procedural texts. We first expand the number of tutorials in Turkish wikiHow from 2,000 to 52,000 using automated translation tools, where the translation quality and loyalty to the original meaning are validated by a team of experts on a random set. Then, we generate several downstream tasks on the corpus, such as linking actions, goal inference, and summarization. To tackle these tasks, we implement strong baseline models via fine-tuning large language-specific models such as TR-BART and BERTurk, as well as multilingual models such as mBART, mT5, and XLM. We find that language-specific models consistently outperform their multilingual models by a significant margin across most pr
    
[^96]: 离线逆向强化学习下的提示评估与优化

    Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])

    [http://arxiv.org/abs/2309.06553](http://arxiv.org/abs/2309.06553)

    这项工作介绍了一种基于离线逆向强化学习的提示评估与优化方法，通过利用离线数据集和逆向强化学习，预测提示性能、提高成本效益、生成易读的结果。

    

    最近，像ChatGPT这样的大型语言模型（LLM）的发展取得了显著的性能，通过利用人类专业知识。然而，充分揭示LLMs在复杂任务中的潜力需要在自然语言提示的广阔搜索空间中进行导航。虽然提示工程显示出潜力，但试错尝试中所需的人工设计提示和相关成本带来了重大挑战。关键是，提示优化的效率取决于昂贵的提示评估过程。本工作介绍了Prompt-OIRL，这是一种基于离线逆向强化学习的方法，旨在弥合有效提示评估和可负担性之间的差距。我们的方法利用专家评估的离线数据集，运用逆向强化学习获得一个针对离线、查询依赖型提示评估的奖励模型。Prompt-OIRL的优点是多方面的：它预测提示的性能，成本高效，生成易读的结果。

    The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
    
[^97]: 一种新的技术相互依赖的映射

    A new mapping of technological interdependence. (arXiv:2308.00014v1 [econ.EM])

    [http://arxiv.org/abs/2308.00014](http://arxiv.org/abs/2308.00014)

    本文利用文本挖掘和网络分析的方法，研究了不同部门之间的技术相互依赖关系，并证明了在技术创新中，间接联系和直接联系同等重要。

    

    哪些技术联系影响了部门的创新能力？这些效应如何通过技术空间传递？本文使用新颖的文本挖掘和网络分析方法回答了这两个关键问题。我们通过分析美国专利商标局（USPTO）授予的650万项专利的文本，并应用网络分析方法，研究了半个世纪（从1976年到2021年）期间不同部门之间的技术相互依赖关系，揭示了存在于技术领域之间的全谱的联系。我们证明专利文本包含了往往无法通过传统的创新指标（例如专利引用）捕捉到的丰富信息。通过使用网络分析，我们记录了间接联系和直接联系同等重要，并且前者大部分使用传统的间接联系度量方法（如Leontief逆矩阵）往往会被隐藏。最后，基于冲击响应分析，我们进行了说明。

    Which technological linkages affect the sector's ability to innovate? How do these effects transmit through the technology space? This paper answers these two key questions using novel methods of text mining and network analysis. We examine technological interdependence across sectors over a period of half a century (from 1976 to 2021) by analyzing the text of 6.5 million patents granted by the United States Patent and Trademark Office (USPTO), and applying network analysis to uncover the full spectrum of linkages existing across technology areas. We demonstrate that patent text contains a wealth of information often not captured by traditional innovation metrics, such as patent citations. By using network analysis, we document that indirect linkages are as important as direct connections and that the former would remain mostly hidden using more traditional measures of indirect linkages, such as the Leontief inverse matrix. Finally, based on an impulse-response analysis, we illustrate 
    
[^98]: 符合目标：使用通用的插入式框架在CTC模型中优化所需属性

    Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework. (arXiv:2307.01715v1 [cs.CL])

    [http://arxiv.org/abs/2307.01715](http://arxiv.org/abs/2307.01715)

    本文提出了一个通用的插入式框架，用于优化CTC模型中的所需属性。该框架通过补充额外的损失项来优先考虑符合所需属性的对齐，并不需要修改CTC损失函数。

    

    连接主义时间分类（CTC）是训练监督序列到序列模型广泛使用的准则。它通过将完美对齐（产生基本事实）的边际化来学习输入和输出序列之间的关系，称为对其，以代价不完美对齐。这种对完美和不完美对齐的二元区分无法捕捉到在其他实际应用中具有重要意义的其他关键对齐属性。在这里，我们提出了$\textit{Align With Purpose}$，这是一个用于增强CTC条件下训练模型中所需属性的$\textbf{通用插入式框架}$。我们通过使用额外的损失项来补充CTC来优先考虑符合所需属性的对齐。我们的方法不需要干预CTC损失函数，能够轻松优化各种属性，并且可以区分完美和不完美的对齐。

    Connectionist Temporal Classification (CTC) is a widely used criterion for training supervised sequence-to-sequence (seq2seq) models. It enables learning the relations between input and output sequences, termed alignments, by marginalizing over perfect alignments (that yield the ground truth), at the expense of imperfect alignments. This binary differentiation of perfect and imperfect alignments falls short of capturing other essential alignment properties that hold significance in other real-world applications. Here we propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play framework}$ for enhancing a desired property in models trained with the CTC criterion. We do that by complementing the CTC with an additional loss term that prioritizes alignments according to a desired property. Our method does not require any intervention in the CTC loss function, enables easy optimization of a variety of properties, and allows differentiation between both perfect and imperfect al
    
[^99]: 构建波斯社交微博口语情感分析数据集

    Constructing Colloquial Dataset for Persian Sentiment Analysis of Social Microblogs. (arXiv:2306.12679v1 [cs.CL])

    [http://arxiv.org/abs/2306.12679](http://arxiv.org/abs/2306.12679)

    本文构建了一个60,000条波斯语的社交微博口语文本数据集，提出了一种新的深度卷积神经网络(CNN)模型，用于更有效地分析社交微博中的口语文本情感。

    

    引言：微博网站已成为情感分析和观点挖掘的丰富数据源。但由于微博常缺少句法一致的术语和代表性，因此情感分类通常效率低下。波斯语言有其独特的特性，需要独特的注释数据和模型来完成情感分析任务。方法：本文首先通过协作环境和内部来源方式构建了一个名为ITRC-Opinion的用户意见数据集。其次，提出了一种新的深度卷积神经网络(CNN)模型，用于更有效地分析社交微博中的口语文本情感。

    Introduction: Microblogging websites have massed rich data sources for sentiment analysis and opinion mining. In this regard, sentiment classification has frequently proven inefficient because microblog posts typically lack syntactically consistent terms and representatives since users on these social networks do not like to write lengthy statements. Also, there are some limitations to low-resource languages. The Persian language has exceptional characteristics and demands unique annotated data and models for the sentiment analysis task, which are distinctive from text features within the English dialect. Method: This paper first constructs a user opinion dataset called ITRC-Opinion by collaborative environment and insource way. Our dataset contains 60,000 informal and colloquial Persian texts from social microblogs such as Twitter and Instagram. Second, this study proposes a new deep convolutional neural network (CNN) model for more effective sentiment analysis of colloquial text in s
    
[^100]: 利用LLMs从混合长文档中检索KPI的全面框架与数据集

    Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A Comprehensive Framework and Dataset. (arXiv:2305.16344v1 [cs.CL])

    [http://arxiv.org/abs/2305.16344](http://arxiv.org/abs/2305.16344)

    本文提出了一个自动化财务信息提取的框架（AFIE），用于提取混合长文档中的关键业绩指标（KPI）。该框架利用LLMs增强了财务报告信息的理解和提取能力，并经过了广泛的实验验证，证明其在GPT-3.5和GPT-4上的有效性，相对于朴素方法，平均精度提高了53.94％和33.77％。

    

    大型语言模型（LLMs）在文本理解和表格推理任务中展现出了卓越的性能，但它们对包含文本和表格数据的混合文本的理解和分析能力仍未被充分发掘。本研究专注于利用LLMs的潜力，从混杂的长型财务报告中理解关键信息。我们提出了自动化财务信息提取（AFIE）框架，增强了LLMs理解和提取财务报告信息的能力。为了评估AFIE，我们开发了一个金融报告数值提取（FINE）数据集，并进行了广泛的实验分析。我们的框架在GPT-3.5和GPT-4上得到了有效验证，相对于朴素方法，平均精度提高了53.94％和33.77％。这些结果表明，AFIE框架为从复杂的混合文档中自动提取数值提供了准确性。

    Large Language Models (LLMs) demonstrate exceptional performance in textual understanding and tabular reasoning tasks. However, their ability to comprehend and analyze hybrid text, containing textual and tabular data, remains underexplored. In this research, we specialize in harnessing the potential of LLMs to comprehend critical information from financial reports, which are hybrid long-documents. We propose an Automated Financial Information Extraction (AFIE) framework that enhances LLMs' ability to comprehend and extract information from financial reports. To evaluate AFIE, we develop a Financial Reports Numerical Extraction (FINE) dataset and conduct an extensive experimental analysis. Our framework is effectively validated on GPT-3.5 and GPT-4, yielding average accuracy increases of 53.94% and 33.77%, respectively, compared to a naive method. These results suggest that the AFIE framework offers accuracy for automated numerical extraction from complex, hybrid documents.
    
[^101]: 巨型语言模型的自我批判提示用于归纳教学指导

    Self-Critique Prompting with Large Language Models for Inductive Instructions. (arXiv:2305.13733v1 [cs.CL])

    [http://arxiv.org/abs/2305.13733](http://arxiv.org/abs/2305.13733)

    本研究提出了一个基准，名为INDust，用于评估大型语言模型（LLMs）对于包含错误信息的指令的抵抗能力。研究发现，当前的LLMs很容易被欺骗，因此采用自我批判提示的方法来激励LLMs不仅对自己进行批评，而且对用户进行批评。

    

    大量的工作都被提出来提高或评估大型语言模型（LLM）实现用户指令的能力。 然而，它们忽略了用户输入可能因用户的错误信念或恶意意图而固有地包含不正确的信息的可能性。 盲目地遵循用户的错误内容将导致欺骗和伤害。 为解决这个问题，我们提出了一个具有挑战性的基准，由归纳指令（INDust）组成，以评估LLMs是否能够抵抗这些指令。 INDust包括三个类别的15K指令：事实核查指令，基于错误前提的问题和基于错误前提的创意指令。 我们对几个强大的LLMs进行的实验表明，当前的LLMs可以轻易地被INDust欺骗，生成误导性和恶意的陈述。 因此，我们采用自我批判提示，以激励LLMs不仅像以前的工作那样对自己进行批评，而且对用户进行批评，它展示出r。

    Numerous works are proposed to improve or evaluate the capabilities of Large language models (LLMs) to fulfill user instructions. However, they neglect the possibility that user inputs may inherently contain incorrect information due to users' false beliefs or malicious intents. In this way, blindly adhering to users' false content will cause deception and harm. To address this problem, we propose a challenging benchmark consisting of Inductive Instructions (INDust) to evaluate whether LLMs could resist these instructions. The INDust includes 15K instructions across three categories: Fact-Checking Instructions, Questions based on False Premises, and Creative Instructions based on False Premises. Our experiments on several strong LLMs reveal that current LLMs can be easily deceived by INDust into generating misleading and malicious statements. Hence we employ Self-Critique prompting to encourage LLMs to not only critique themselves like in previous works but also the users, which show r
    
[^102]: 在资源受限的嵌入式设备上部署基于BERT的NLP模型的挑战探究

    Exploring Challenges of Deploying BERT-based NLP Models in Resource-Constrained Embedded Devices. (arXiv:2304.11520v1 [cs.CL])

    [http://arxiv.org/abs/2304.11520](http://arxiv.org/abs/2304.11520)

    本文探究了在资源受限的嵌入式设备上部署基于BERT的NLP模型的挑战，并得出结论：虽然DistilBERT和TinyBERT等轻量级模型相对占用更少内存，但它们在复杂的NLP任务上表现较差；ResNet-based BERT模型可以在精度和资源效率之间取得良好的平衡，适合在嵌入式设备上部署。

    

    基于BERT的神经架构已经成为许多下游NLP任务的流行先进技术基准。然而，这些架构对数据依赖性强，占用大量内存和能量，经常阻碍它们在许多实时、资源受限的应用程序中的部署。现有的BERT轻量级版本（例如DistilBERT和TinyBERT）通常在复杂的NLP任务上无法表现出良好的性能。更重要的是，从设计师的角度来看，要为特定的NLP任务使用何种“正确的”基于BERT的架构，以在资源可用性和最终用户需求的最小精度之间实现最佳权衡，尚不确定。系统工程师必须花费大量时间进行试错实验，以找到合适的答案。本文在不同的资源限制和精度预算下对BERT-based模型进行了探究性研究，以得出有关此资源/精度权衡的经验性观察结果。我们的研究发现，虽然DistilBERT和TinyBERT等更轻量级的模型相对BERT-base占用的内存要少得多，但它们在复杂的NLP任务中精度的下降是明显的。我们还观察到，特别是基于ResNet的BERT模型，可以在准确性和资源效率之间取得良好的平衡，使其成为在资源受限的嵌入式设备中部署的良好候选模型。

    BERT-based neural architectures have established themselves as popular state-of-the-art baselines for many downstream NLP tasks. However, these architectures are data-hungry and consume a lot of memory and energy, often hindering their deployment in many real-time, resource-constrained applications. Existing lighter versions of BERT (eg. DistilBERT and TinyBERT) often cannot perform well on complex NLP tasks. More importantly, from a designer's perspective, it is unclear what is the "right" BERT-based architecture to use for a given NLP task that can strike the optimal trade-off between the resources available and the minimum accuracy desired by the end user. System engineers have to spend a lot of time conducting trial-and-error experiments to find a suitable answer to this question. This paper presents an exploratory study of BERT-based models under different resource constraints and accuracy budgets to derive empirical observations about this resource/accuracy trade-offs. Our findin
    
[^103]: EasyNER：一种可定制的易于使用的医学文本深度学习和基于字典的命名实体识别工具

    EasyNER: A Customizable Easy-to-Use Pipeline for Deep Learning- and Dictionary-based Named Entity Recognition from Medical Text. (arXiv:2304.07805v1 [q-bio.QM])

    [http://arxiv.org/abs/2304.07805](http://arxiv.org/abs/2304.07805)

    EasyNER是一种用于在医学研究文章中识别命名实体的端到端工具。它基于深度学习模型和字典方法，并且易于使用和定制。在COVID-19相关文章数据集上的应用证明了其可以准确地识别所需实体。

    

    医学研究已经产生了大量出版物，PubMed数据库已经收录了超过3,500万篇研究文章。整合这些分散在大量文献中的知识可以提供有关生理机制和导致新型医学干预的疾病过程的关键见解。然而，对于研究人员来说，利用这些信息成为一个巨大挑战，因为数据的规模和复杂性远远超出了人类的处理能力。在COVID-19大流行的紧急情况下，这尤其成为问题。自动化文本挖掘可以帮助从大量医学研究文章中提取和连接信息。文本挖掘的第一步通常是识别特定类别的关键字（例如所有蛋白质或疾病名称），即命名实体识别（NER）。本文提出了一种端到端的NER工具EasyNER，用于识别医学研究文章中的典型实体，包括疾病名称、药物名称和蛋白质名称。EasyNER基于深度学习模型和基于字典的方法，旨在对自然语言处理具有不同经验水平的研究人员易于使用和定制。我们将EasyNER应用于COVID-19相关文章的数据集中并展示它可以准确地识别感兴趣的实体，为下游分析提供有用的信息。

    Medical research generates a large number of publications with the PubMed database already containing >35 million research articles. Integration of the knowledge scattered across this large body of literature could provide key insights into physiological mechanisms and disease processes leading to novel medical interventions. However, it is a great challenge for researchers to utilize this information in full since the scale and complexity of the data greatly surpasses human processing abilities. This becomes especially problematic in cases of extreme urgency like the COVID-19 pandemic. Automated text mining can help extract and connect information from the large body of medical research articles. The first step in text mining is typically the identification of specific classes of keywords (e.g., all protein or disease names), so called Named Entity Recognition (NER). Here we present an end-to-end pipeline for NER of typical entities found in medical research articles, including diseas
    

