# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference](https://arxiv.org/abs/2403.09636) | 提出了动态内存压缩（DMC）方法，用于在线关键-值缓存压缩，模型学习在不同的头部和层中应用不同的压缩率，并且通过将预训练的LLMs改装为DMC Transformers，在自回归推断中实现了高达~3.7倍的吞吐量增加。 |
| [^2] | [Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models](https://arxiv.org/abs/2403.09635) | 提出了一个统一的信号传播理论，提供了控制transformer模型信号传播的公式，提出了DeepScaleLM初始化和缩放方案，使得可以训练非常深的模型，并发现深层模型在多个任务和数据集上胜过浅层模型。 |
| [^3] | [3D-VLA: A 3D Vision-Language-Action Generative World Model](https://arxiv.org/abs/2403.09631) | 提出了3D-VLA，通过将3D感知、推理和动作无缝连接，建立一个生成世界模型，弥补了现有VLA模型只能处理2D输入且忽视世界动态与动作之间关系的不足。 |
| [^4] | [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/abs/2403.09629) | Quiet-STaR提出了一种新的泛化版本，在每个标记处生成解释未来文本的思考过程，从而改善预测能力 |
| [^5] | [Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training](https://arxiv.org/abs/2403.09613) | 在结构化环境中依次微调的LLMs表现出预期行为，能够从遗忘中恢复，揭示了在过参数化网络中进行训练的新见解 |
| [^6] | [MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training](https://arxiv.org/abs/2403.09611) | 通过详细研究图像编码器、视觉语言连接器和预训练数据选择的重要性，确定了对于实现多个基准测试中最新潮的少样本结果至关重要的关键设计经验。 |
| [^7] | [Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey](https://arxiv.org/abs/2403.09606) | 大型语言模型的出现极大影响了自然语言处理领域，特别是通过其先进的推理能力。而本综述则重点评估和改进了大型语言模型在因果推断方面的应用，包括提高推理能力、解决公平和安全问题、提供解释和处理多模态。 |
| [^8] | [Less is More: Data Value Estimation for Visual Instruction Tuning](https://arxiv.org/abs/2403.09559) | 视觉指导调整时需要进行数据价值评估，通过新的数据选择方法TIVE，根据任务级和实例级价值来消除视觉指导数据中的冗余。 |
| [^9] | [Logits of API-Protected LLMs Leak Proprietary Information](https://arxiv.org/abs/2403.09539) | 大多数现代LLM受到softmax瓶颈影响，可以以较低成本获取API保护的LLM的非公开信息和解锁多种功能 |
| [^10] | [VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding](https://arxiv.org/abs/2403.09530) | 提出了一个统一的VisionGPT-3D框架，整合了最先进的视觉模型，有助于提升计算机视觉对于3D视觉理解的能力 |
| [^11] | [MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation](https://arxiv.org/abs/2403.09522) | 提出了MT-Patcher框架，实现了从大型语言模型到中等规模机器翻译模型的有选择性、全面和主动的知识迁移 |
| [^12] | [Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information](https://arxiv.org/abs/2403.09516) | 通过利用预定义的典型人口统计文本并在微调过程中加入正则化项，本文提出的方法有效减轻了语言模型中的社会偏见，同时不需要依赖显式的人口统计标签。 |
| [^13] | [From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News](https://arxiv.org/abs/2403.09498) | 本研究引入了基于大型语言模型的虚假新闻传播仿真框架，研究了虚假新闻传播的趋势和控制，每个代理人在仿真中代表具有独特个性的个体。 |
| [^14] | [Hyper-CL: Conditioning Sentence Representations with Hypernetworks](https://arxiv.org/abs/2403.09490) | Hyper-CL是一种将超网络与对比学习相结合的有效方法，能够灵活地进行条件化句子表示。 |
| [^15] | [Rectifying Demonstration Shortcut in In-Context Learning](https://arxiv.org/abs/2403.09488) | 本研究旨在纠正大型语言模型在上下文学习中的演示快捷方式，并引入了一种新的明示意识校准方法。 |
| [^16] | [Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision](https://arxiv.org/abs/2403.09472) | 通过从更简单的任务学习，实现对更难推理任务的有效泛化，提出了一种可扩展对齐方法。 |
| [^17] | ["Like a Nesting Doll": Analyzing Recursion Analogies Generated by CS Students using Large Language Models](https://arxiv.org/abs/2403.09409) | 使用大语言模型ChatGPT，研究了超过350名一年级计算机学生生成的递归类比，探讨了如何利用这些类比帮助理解复杂的计算概念 |
| [^18] | [Komodo: A Linguistic Expedition into Indonesia's Regional Languages](https://arxiv.org/abs/2403.09362) | Komodo-7B是一个大型语言模型，可以无缝操作印度尼西亚、英语和11种印度尼西亚地区语言，Komodo-7B-Instruct达到了卓越的性能，超越了多个基准模型。 |
| [^19] | [More than words: Advancements and challenges in speech recognition for singing](https://arxiv.org/abs/2403.09298) | 本文讨论了歌唱语音识别中的挑战和进展，探索了音素识别、歌曲中的语言识别、关键词识别和完整歌词转录等关键领域，并介绍了深度学习和大规模数据集在该领域推动进展的最新发展。 |
| [^20] | [Anatomical Structure-Guided Medical Vision-Language Pre-training](https://arxiv.org/abs/2403.09294) | 该研究提出了一种解剖结构引导的医学视觉-语言预训练框架，通过将原始报告解析为三元组并利用每个元素作为监督来增强表示学习，解决了局部对齐和图像-报告对表示学习中的挑战。 |
| [^21] | [To Label or Not to Label: Hybrid Active Learning for Neural Machine Translation](https://arxiv.org/abs/2403.09259) | 提出了一种用于神经机器翻译的混合主动学习策略HUDS，结合了不确定性和多样性，用于领域自适应的句子选择。 |
| [^22] | [Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records](https://arxiv.org/abs/2403.09226) | 结合文本到SQL生成与检索增强生成（RAG）的方法，可用于使用电子健康记录和索赔数据回答流行病学问题，并在现实行业中显示出显著性能提升。 |
| [^23] | [TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Sematic Tasks](https://arxiv.org/abs/2403.09207) | 通过基于WordNet的LLMs模型，提出了TaxoLLaMA模型，采用4位量化和LoRA技术轻量化，在多个词汇语义任务中取得11个SotA结果，且在词汇蕴涵和分类学构建任务上表现出强大的零样本性能。 |
| [^24] | [Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse](https://arxiv.org/abs/2403.09167) | 通过提出具有多样提示和多维质量评估框架的两阶段方法，可以在避免模型泛化能力下降的情况下，使用高质量领域特定数据对大型语言模型进行微调。 |
| [^25] | [Exploring the Comprehension of ChatGPT in Traditional Chinese Medicine Knowledge](https://arxiv.org/abs/2403.09164) | 探索了ChatGPT在中医知识理解中的应用，提出了TCM-QA数据集，评估了LLM的性能，并发现在判断题中表现最佳，中文提示优于英文提示。 |
| [^26] | [Caveat Lector: Large Language Models in Legal Practice](https://arxiv.org/abs/2403.09163) | LLMs在法律实践中的作用被过分乐观地预测，不理解文本内容会导致依赖风险。 |
| [^27] | [Unveiling the Generalization Power of Fine-Tuned Large Language Models](https://arxiv.org/abs/2403.09162) | 本文研究了经过微调的大型语言模型的泛化能力，发现在生成和分类任务上进行微调的模型在泛化到不同领域和任务时表现出不同行为，引入上下文学习策略可以提高模型的泛化能力。 |
| [^28] | [Basque and Spanish Counter Narrative Generation: Data Creation and Evaluation](https://arxiv.org/abs/2403.09159) | 在这项研究中，我们提出了CONAN-EUS，这是一个通过机器翻译和专业后期编辑开发的巴斯克语和西班牙语数据集，用于生成反叙事。实验结果表明，通过在后期编辑数据上训练，而不是仅依赖于机器翻译数据，能够显著提高反叙事生成的质量。 |
| [^29] | [Evaluating LLMs for Gender Disparities in Notable Persons](https://arxiv.org/abs/2403.09148) | 评估大型语言模型在显著人物中存在的性别差距，并发现GPT-4在性能上有所改进，但问题尚未完全解决 |
| [^30] | [ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text](https://arxiv.org/abs/2403.09131) | ProSwitch通过知识引导的指令微调，在专业和非专业风格之间生成文本，并在专业性评估和质量评估方面表现出优越性。 |
| [^31] | [AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning](https://arxiv.org/abs/2403.09113) | AutoLoRA提出了一个基于元学习的框架，自动识别每个LoRA层的最佳秩，以解决LoRA中秩分配和秩搜索的问题，进而提高微调性能。 |
| [^32] | [AI on AI: Exploring the Utility of GPT as an Expert Annotator of AI Publications](https://arxiv.org/abs/2403.09097) | 通过使用GPT聊天机器人模型进行提示工程，研究实现了一个自动专家标注管道，该管道以94%的准确率为AI出版物分配标签，并对比展示了SPECTER在相同任务上达到96%的准确率。 |
| [^33] | [MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection](https://arxiv.org/abs/2403.09092) | MCFEND是第一个用于中文假新闻检测的多源基准数据集，解决了单一来源数据集应用于多源新闻数据时性能下降的问题。 |
| [^34] | [Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance](https://arxiv.org/abs/2403.09085) | 设计了一个抽象推理数据集和有意义学习范式，教导大型语言模型如何利用通用事实进行推理，有效提升了抽象推理能力。 |
| [^35] | [Information Extraction: An application to the domain of hyper-local financial data on developing countries](https://arxiv.org/abs/2403.09077) | 本研究开发并评估了两种基于自然语言处理的技术来提取发展中国家金融数据，其中采用文本到文本的T5模型实现了高准确率、精度和召回率。 |
| [^36] | [Large Language Models are Parallel Multilingual Learners](https://arxiv.org/abs/2403.09073) | 通过将输入翻译为多种语言，为大型语言模型提供多语言平行输入，显著增强了它们的理解能力，实验证明多语言输入可以超越传统学习方法，并发现了神经元激活的反直觉现象 |
| [^37] | [UniCode: Learning a Unified Codebook for Multimodal Large Language Models](https://arxiv.org/abs/2403.09072) | UniCode提出一种学习统一码书的方法，解决多模大语言模型中对视觉和文本进行标记的关键问题，使模型能够生成高质量的图像，并可适应各种压缩方法。 |
| [^38] | [LAMP: A Language Model on the Map](https://arxiv.org/abs/2403.09059) | 该研究引入了一个新颖的框架，用于在城市特定数据上微调预训练模型，使其能够为人们提供准确的推荐，同时最小化幻觉。 |
| [^39] | [A Continued Pretrained LLM Approach for Automatic Medical Note Generation](https://arxiv.org/abs/2403.09057) | 这项研究提出了一种用于医疗记录生成的持续预训练LLM方法，在PubMedQA方面性能优于GPT-4，能够更好地捕捉正确的医疗概念，并且在正确性和完整性方面超过人类抄写员。 |
| [^40] | [RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems](https://arxiv.org/abs/2403.09040) | RAGGED框架分析和优化了检索增强生成系统，揭示了不同模型适合不同RAG设置的事实，编码器-解码器模型随文档数量增加而改善，而仅解码器模型只能有效利用少量文档。 |
| [^41] | [The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?](https://arxiv.org/abs/2403.09037) | 本研究利用线性探测揭示了大型视觉语言模型的隐藏知识，发现首个令牌的logit分布包含足够信息，可以识别无法回答的视觉问题、防范多模态越狱攻击以及识别欺骗性问题，并提出了一个简单的解码策略以有效改善生成内容。 |
| [^42] | [CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences](https://arxiv.org/abs/2403.09032) | 介绍了 CodeUltraFeedback 数据集，通过 AI 反馈使 14 种不同的 LLMs 对 10,000 个复杂指令生成响应，并使用 LLM-as-a-Judge 方法评估它们与五种编程偏好的对齐情况，同时提出了用于评估 LLM 对编程偏好对齐的基准 CODAL-Bench。 |
| [^43] | [ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning](https://arxiv.org/abs/2403.09028) | 引入了ChartInstruct数据集和两种指令调整系统，解决了常规模型无法解决各种与图表相关任务的问题 |
| [^44] | [Semiparametric Token-Sequence Co-Supervision](https://arxiv.org/abs/2403.09024) | 引入了一种半参数令牌序列共监督训练方法，通过同时利用传统的下一个令牌预测损失和下一个序列预测损失来训练语言模型，实验结果显示这种方法能够提高模型的泛化能力。 |
| [^45] | [AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic](https://arxiv.org/abs/2403.09017) | AraTrust是第一个阿拉伯语大型语言模型的全面信誉基准，解决了缺乏全面信誉评估基准的问题，帮助准确评估和提高LLMs的安全性。 |
| [^46] | [Ethos: Rectifying Language Models in Orthogonal Parameter Space](https://arxiv.org/abs/2403.08994) | Ethos提出了一种新的高效方法，通过在任务向量上进行矫正LMs以减轻产生毒性和偏见输出以及避免隐私泄露。 |
| [^47] | [AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents](https://arxiv.org/abs/2403.08978) | AutoGuide通过提取嵌入在离线数据中的知识，生成一组状态感知指南，从而弥合大型语言模型中的知识差距，为代理的决策过程提供有用的知识。 |
| [^48] | [Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era](https://arxiv.org/abs/2403.08946) | 在大型语言模型时代，为了适应其复杂性和先进能力，我们引入了可用的XAI概念，通过积极增强LLMs在实际环境中的生产力和适用性，实现XAI方法论的重大转变。 |
| [^49] | [LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots](https://arxiv.org/abs/2403.08943) | 该论文提出了LMStyle基准，针对聊天机器人的文本风格转移进行评估，不仅可以自动化和可扩展地衡量LLMs的风格转移质量，还考虑了适当性这一新颖度量方面。 |
| [^50] | [Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics](https://arxiv.org/abs/2403.08904) | 提出了一种检测LLM在生成文本过程中虚构和覆盖错误的方法，通过基于词重叠、显著性和分类器的三种策略，即使在合成错误上训练，也能实现高错误检测性能，表现出快速和有效的能力。 |
| [^51] | [From "um" to "yeah": Producing, predicting, and regulating information flow in human conversation](https://arxiv.org/abs/2403.08890) | 本文使用大型语言模型研究了英语对话中信息流的生成、预测和调节，揭示了信息密度以及检索和呈现信息的认知负荷对对话的显著影响，同时发现了backchannels在调节新颖性产生过程中的作用。 |
| [^52] | [PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models](https://arxiv.org/abs/2403.08851) | 该研究提出了PAPERCLIP方法，通过将天文观测与自然语言关联起来，利用预训练的CLIP模型进行微调，实现了观测和自然语言之间的有意义的联合表示。 |
| [^53] | [LoRA-SP: Streamlined Partial Parameter Adaptation for Resource-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2403.08822) | LoRA-SP利用随机半选择参数冻结的新颖方法，在微调大型语言模型时有效平衡预训练知识的保留和任务特定优化的适应性，显著降低了计算和内存需求，同时实现了竞争性性能。 |
| [^54] | [Thermometer: Towards Universal Calibration for Large Language Models](https://arxiv.org/abs/2403.08819) | 提出了一种针对大型语言模型的校准方法THERMOMETER，通过学习来自多个任务数据的辅助模型，实现了计算效率高、准确性保持并产生更好校准响应的目标。 |
| [^55] | [Multimodal Fusion of EHR in Structures and Semantics: Integrating Clinical Records and Notes with Hypergraph and LLM](https://arxiv.org/abs/2403.08818) | 提出了一个名为MINGLE的新框架，通过两级注入策略将医学概念语义和临床笔记语义融合到超图中，有效地整合了结构和语义的电子健康记录数据。 |
| [^56] | [Ontologia para monitorar a defici\^encia mental em seus d\'eficts no processamento da informa\c{c}\~ao por decl\'inio cognitivo e evitar agress\~oes psicol\'ogicas e f\'isicas em ambientes educacionais com ajuda da I.A*](https://arxiv.org/abs/2403.08795) | 本文提出了利用人工智能结合UFO本体论来监测认知衰退导致的信息处理缺陷，预防教育环境中出现的心理和身体攻击。 |
| [^57] | [Image-Text Out-Of-Context Detection Using Synthetic Multimodal Misinformation](https://arxiv.org/abs/2403.08783) | 该研究提出了一种使用合成数据生成的脱离语境检测方法，实验证实了合成数据生成在解决OOCD相关数据限制方面的有效性。 |
| [^58] | [Veagle: Advancements in Multimodal Representation Learning](https://arxiv.org/abs/2403.08773) | 本文介绍了一种新颖的方法，通过在当前视觉语言模型（VLMs）和多模态大语言模型（MLLMs）的基础上融合独特的机制，以增强现有模型的多模态能力。 |
| [^59] | [SOTOPIA-$\pi$: Interactive Learning of Socially Intelligent Language Agents](https://arxiv.org/abs/2403.08715) | 提出了一种交互式学习方法SOTOPIA-$\pi$，该方法利用行为克隆和自我强化训练，改进了语言代理的社交智能，使其达到了专家模型的水平，并提高了安全性。 |
| [^60] | [Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records](https://arxiv.org/abs/2403.08664) | 这项研究探讨了利用合成数据生成医疗记录的方法，特别是通过零样本和少样本提示策略，避免了在训练过程中使用真实患者数据的隐私问题。 |
| [^61] | [Automatic Interactive Evaluation for Large Language Models with State Aware Patient Simulator](https://arxiv.org/abs/2403.08495) | 介绍了自动互动评估（AIE）框架和状态感知病人模拟器（SAPS），以动态、真实的平台评估LLMs，弥补传统评估方法无法满足临床任务需求的不足。 |
| [^62] | [Contextual Clarity: Generating Sentences with Transformer Models using Context-Reverso Data](https://arxiv.org/abs/2403.08103) | 使用Transformer模型和Context-Reverso数据生成具有上下文清晰度的句子 |
| [^63] | [Exploring Safety Generalization Challenges of Large Language Models via Code](https://arxiv.org/abs/2403.07865) | 本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。 |
| [^64] | [Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations](https://arxiv.org/abs/2403.07769) | 文章探讨了基于多Agent系统理论结合大型语言模型的计算实体对人类互动的革新影响，提出了一种可能将专门人工代理支持扩展到操作性组织流程和基于知识和人类编排的战略决策的方式。 |
| [^65] | [Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards](https://arxiv.org/abs/2403.07708) | 引入对比奖励的方法提高了从人类反馈中的强化学习的效果，改善了奖励模型的鲁棒性，鼓励对基准的改善，并能根据任务的难度进行校准。 |
| [^66] | [Reference-free Monolithic Preference Optimization with Odds Ratio](https://arxiv.org/abs/2403.07691) | 本文介绍了一种无参考单体赔率比偏好优化算法ORPO，在SFT过程中通过轻微惩罚不受欢迎的生成风格，消除了额外的偏好对齐阶段 |
| [^67] | [Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts](https://arxiv.org/abs/2403.07556) | 提出了一种名为真相感知的上下文选择（TACS）的轻量级方法，可以通过对输入上下文进行真相检测并构建相应的注意力蒙版来缓解大型语言模型被不真实上下文误导产生幻觉 |
| [^68] | [Knowledge Graph Large Language Model (KG-LLM) for Link Prediction](https://arxiv.org/abs/2403.07311) | 该论文提出了知识图谱大型语言模型框架（KG-LLM），利用思维链提示和上下文学习等NLP范例，以增强知识图谱中的多跳链接预测，并展示了框架在微调大型语言模型和零次尝试能力方面的有效性。 |
| [^69] | [Rule-driven News Captioning](https://arxiv.org/abs/2403.05101) | 本文提出了一种基于规则的新闻标题生成方法，通过新闻感知的语义规则，可以生成遵循新闻报道基本规则的图像描述。 |
| [^70] | [From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction](https://arxiv.org/abs/2403.04369) | 引入领域知识的从图到词袋方法，帮助预测混淆罪名，通过构成要素和关键词选择进行判断。 |
| [^71] | [Discriminative Probing and Tuning for Text-to-Image Generation](https://arxiv.org/abs/2403.04321) | 加强T2I模型的判别能力，以实现更精确的文本到图像对齐生成。 |
| [^72] | [German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset](https://arxiv.org/abs/2403.03750) | 本文提出了一个用于德语新闻摘要中幻觉检测的数据集absinth，探讨了LLMs在该任务中的应用。 |
| [^73] | [VBART: The Turkish LLM](https://arxiv.org/abs/2403.01308) | VBART是第一个土耳其序列到序列大语言模型，通过与BART和mBART模型结合形成了紧凑型LLM，并在多个任务中表现出色，为土耳其自然语言处理研究开辟了新的可能性。 |
| [^74] | [RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots](https://arxiv.org/abs/2403.01193) | 本文探讨了如何利用检索增强生成（RAG）抵制大型语言模型（LLMs）产生的幻觉，结果表明RAG在某些情况下可以提高准确性，但仍需要更强大的解决方案以确保LLMs在实际应用中可靠性。 |
| [^75] | [Executing Natural Language-Described Algorithms with Large Language Models: An Investigation](https://arxiv.org/abs/2403.00795) | 大语言模型可以有效地执行用自然语言描述的程序，尤其是在不涉及大量数字计算的情况下。 |
| [^76] | [Regional inflation analysis using social network data](https://arxiv.org/abs/2403.00774) | 本研究利用社交网络数据分析了区域通货膨胀的上升和下降趋势，探讨了社交网络讨论对通货膨胀预期的潜在影响。 |
| [^77] | [MMSR: Symbolic Regression is a Multimodal Task](https://arxiv.org/abs/2402.18603) | 符号回归被视为一个多模态任务，研究人员将数据到表达式的映射视为翻译问题，引入大规模预训练模型。 |
| [^78] | [Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning](https://arxiv.org/abs/2402.13897) | 提出了一个两块式的方法来解决长文档中信息检索领域的挑战，并实现了双向交互 |
| [^79] | [Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models](https://arxiv.org/abs/2402.13492) | 该研究深入探讨了如何通过检索增强语言模型，构建了新的QA数据集WiTQA，以实体和关系组合的影响为重点进行了详细分析。 |
| [^80] | [Me LLaMA: Foundation Large Language Models for Medical Applications](https://arxiv.org/abs/2402.12749) | Me LLaMA是一个医学领域的大型语言模型系列，通过持续预训练和指导调整在大型医学数据集上训练而成，其在零-shot和少-shot学习方面表现优于现有的医学语言模型和商业巨头ChatGPT。 |
| [^81] | [Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models](https://arxiv.org/abs/2402.04614) | 本文讨论了大型语言模型生成的自我解释中的信实性与可信度之间的差异。虽然这些解释在逻辑上是合乎情理且连贯的，但并不一定与模型的推理过程一致，引发对其信实性的担忧。 |
| [^82] | [SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks](https://arxiv.org/abs/2402.01980) | 社会科学的自然语言处理任务需要捕捉语义和隐含的语用信息，指导调优模型Socialite-Llama在这些任务上表现出卓越的性能和提升。 |
| [^83] | [Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs](https://arxiv.org/abs/2311.17371) | 多Agent辩论（MAD）作为增强大型语言模型（LLMs）真实性的策略，对于解决确保生成代理提供准确可靠答案的挑战具有潜力，但当前形式下的多Agent辩论系统在可靠性上不一定优于其他提示策略。 |
| [^84] | [Which Modality should I use -- Text, Motif, or Image? : Understanding Graphs with Large Language Models](https://arxiv.org/abs/2311.09862) | 本研究提出了一种新方法，使用文本、图像和主题等多种模态对图进行编码，结合提示以近似表示图的全局连接性，从而提高了LLMs处理复杂图结构的效率。 |
| [^85] | [BeLLM: Backward Dependency Enhanced Large Language Model for Sentence Embeddings](https://arxiv.org/abs/2311.05296) | BeLLM提出了增强反向依赖的大型语言模型，通过引入显式的反向依赖性，在语义相似性测量中取得了最先进的性能。 |
| [^86] | [LILO: Learning Interpretable Libraries by Compressing and Documenting Code](https://arxiv.org/abs/2310.19791) | LILO是一种神经符号框架，通过迭代地合成、压缩和文档化代码来构建可解释且适用于特定问题领域的程序库。在其中，LILO结合了大型语言模型引导的程序合成和程序自动重构的算法进展，并且通过自动文档过程使得代码抽象可解释并提升性能。 |
| [^87] | [XAL: EXplainable Active Learning Makes Classifiers Better Low-resource Learners](https://arxiv.org/abs/2310.05502) | XAL提出了一种可解释的主动学习框架，鼓励分类器提供推断的理由并深入未标记数据，从而提升低资源文本分类的性能 |
| [^88] | [Utilizing Contextual Clues and Role Correlations for Enhancing Document-level Event Argument Extraction](https://arxiv.org/abs/2310.05116) | 本文提出了CARLG模型，通过利用上下文线索和角色相关性，提升了文档级事件论证提取的性能。 |
| [^89] | [Think before you speak: Training Language Models With Pause Tokens](https://arxiv.org/abs/2310.02226) | 引入暂停标记的语言模型训练方法可以让模型在输出标记前处理更多隐藏向量，取得了较好的实验结果 |
| [^90] | [Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy](https://arxiv.org/abs/2310.01334) | 本文旨在探讨如何通过合并专家信息来制定出更紧凑但更具知识的SMoE模型，因为传统的模型合并方法并不适用于SMoE的专家合并。 |
| [^91] | [Making Language Models Better Tool Learners with Execution Feedback](https://arxiv.org/abs/2305.13068) | 这篇论文提出了一个名为TRICE的框架，通过执行反馈实现语言模型的工具学习，使其能够学会何时以及如何有效地使用工具。 |
| [^92] | [Learning New Tasks from a Few Examples with Soft-Label Prototypes](https://arxiv.org/abs/2210.17437) | 本研究提出了一种新的极端少样本学习方法，利用软标签原型从少量示例中学习新任务，在大型、高维和现实世界数据集上表现出色。 |
| [^93] | [Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation](https://arxiv.org/abs/2207.14000) | 提出了IMA-GloVe-GA，一个用于自然语言表达的多步推理的迭代神经推理网络，在超领域泛化方面具有更好的性能表现。 |
| [^94] | [Evaluation of LLM Chatbots for OSINT-based Cyberthreat Awareness.](http://arxiv.org/abs/2401.15127) | 本研究评估了LLM聊天机器人在基于OSINT的网络威胁意识中的应用能力，并发现聊天机器人在网络安全的二分类和命名实体识别任务方面表现出良好的性能。 |
| [^95] | [Vanishing Gradients in Reinforcement Finetuning of Language Models.](http://arxiv.org/abs/2310.20703) | 本研究发现在强化微调（RFT）中存在梯度消失的问题，当模型下奖励的标准差较小时，输入的期望梯度会消失，导致奖励最大化缓慢。初始监督微调（SFT）阶段是克服这个问题的最有希望的方法。 |
| [^96] | [Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation.](http://arxiv.org/abs/2310.18235) | 本论文提出了Davidsonian场景图（DSG）的评估框架，解决了现有文本-图像生成模型评估中的可靠性挑战，包括QG问题的准确性和VQA答案的一致性。 |
| [^97] | [CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules.](http://arxiv.org/abs/2310.08992) | CodeChain是一种通过代表性子模块的自我修订链路引导模块化代码生成的新框架，旨在解决大型语言模型在解决复杂编程任务方面的挑战。 |
| [^98] | [Amortizing intractable inference in large language models.](http://arxiv.org/abs/2310.04363) | 本论文提出了一种使用摊销的贝叶斯推理从难以处理的后验分布中进行抽样的方法，并利用生成流网络来实现大规模语言模型的微调，从而解决了在这些模型中处理推理问题的限制。 |
| [^99] | [Large Language Models Cannot Self-Correct Reasoning Yet.](http://arxiv.org/abs/2310.01798) | 大型语言模型(LLMs)的自我纠正能力在推理方面存在困难，甚至可能在自我纠正后性能下降。 |
| [^100] | [DyVal: Graph-informed Dynamic Evaluation of Large Language Models.](http://arxiv.org/abs/2309.17167) | DyVal是一种基于图形信息的大型语言模型动态评估协议，通过动态生成具有可控复杂性的评估样本，评估了各种LLM在推理任务上的性能，发现它们在这些挑战性样本上表现更差。 |
| [^101] | [K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling.](http://arxiv.org/abs/2309.11093) | 研究者介绍了一种新颖的K-pop歌词翻译数据集，该数据集揭示了K-pop歌词翻译的独特特征，并构建了一个神经歌词翻译模型，强调了专用数据集的重要性。 |
| [^102] | [Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and Hindi.](http://arxiv.org/abs/2309.10272) | 本文介绍了Tri-Distil-BERT和Mixed-Distil-BERT两个模型，Tri-Distil-BERT是一个在孟加拉语、英语和印地语上预训练的多语言模型，Mixed-Distil-BERT是一个在混合编码数据上微调的模型。这两个模型在多个自然语言处理任务上表现出与更大的模型相竞争的性能。 |
| [^103] | [Instruction Tuning for Large Language Models: A Survey.](http://arxiv.org/abs/2308.10792) | 本文调查了指令调优这一关键技术在增强大型语言模型能力和可控性方面的研究工作，包括方法、数据集构建、模型训练和应用，以及对结果影响的分析。同时回顾了可能的问题和批评，并指出了目前的不足。 |
| [^104] | [Better Zero-Shot Reasoning with Role-Play Prompting.](http://arxiv.org/abs/2308.07702) | 通过角色扮演提示，研究评估了现代大型语言模型在零-shot推理场景下的表现，并发现角色扮演提示在多个推理基准测试中都超越了标准的零-shot方法。 |
| [^105] | [MO-VLN: A Multi-Task Benchmark for Open-set Zero-Shot Vision-and-Language Navigation.](http://arxiv.org/abs/2306.10322) | MO-VLN是一个用于评估通用机器人在多任务环境中的视觉和语言导航的基准，通过使用虚幻引擎5开发逼真的场景和包含多种不常见物体来测试其效果和泛化能力。 |
| [^106] | [Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions.](http://arxiv.org/abs/2305.15083) | 本文通过对多语言预训练语言模型进行微调，研究了它们如何通过翻译指令执行多语言翻译任务。研究发现多语言LLMs具有较强的翻译能力，这取决于语言与英语的相似性和预训练阶段使用的数据量。此外，执行翻译指令的能力依赖于对指令的理解和不同语言之间的对齐。 |
| [^107] | [LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models.](http://arxiv.org/abs/2305.13718) | 本文介绍了 LogicLLM，一种通过自监督后训练来提高大语言模型的逻辑推理能力的方法，该方法有效地在常见逻辑推理任务上进行表现，超过了目前最先进的无监督基线方法。 |
| [^108] | [Discern and Answer: Mitigating the Impact of Misinformation in Retrieval-Augmented Models with Discriminators.](http://arxiv.org/abs/2305.01579) | 本文研究了现有检索增强语言模型假设所有检索信息都是正确的假设的问题，在实际应用中可能存在虚假信息导致冲突的情况下，提出了通过精细调整鉴别器和提示鉴别能力引出鲁棒性的方法，这显著改善了模型在知识冲突下的效果；同时提供了关于交替精细调整模型和上下文学习的新的结论。 |
| [^109] | [Label Dependencies-aware Set Prediction Networks for Multi-label Text Classification.](http://arxiv.org/abs/2304.07022) | 本论文提出了一种标签相依感知集合预测网络用于解决多标签文本分类问题。该方法将多标签分类视为直接集合预测问题，通过标签之间的统计关系构建关联矩阵并结合GCN学习标签信息，同时利用句子信息和标签信息，最终结果表明其性能优于以前的方法。 |
| [^110] | [Log-linear Guardedness and its Implications.](http://arxiv.org/abs/2210.10012) | 本研究介绍了对数线性保护性及其对下游分类器行为的影响。在二元情况下，下游对数线性模型无法恢复被删除的概念，但在某些情况下，可以通过构建多类对数线性模型间接恢复概念。这些结果揭示了线性删除方法的局限性，并强调了进一步研究的需求。 |
| [^111] | [Kernelized Concept Erasure.](http://arxiv.org/abs/2201.12191) | 通过核化线性极小极大博弈，防止特定非线性对手预测概念，但无法彻底解决非线性编码的概念擦除问题。 |

# 详细

[^1]: 动态内存压缩：用于加速推断的LLMs的改装

    Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference

    [https://arxiv.org/abs/2403.09636](https://arxiv.org/abs/2403.09636)

    提出了动态内存压缩（DMC）方法，用于在线关键-值缓存压缩，模型学习在不同的头部和层中应用不同的压缩率，并且通过将预训练的LLMs改装为DMC Transformers，在自回归推断中实现了高达~3.7倍的吞吐量增加。

    

    Transformers已经成为大型语言模型（LLMs）的支柱。然而，由于需要在内存中存储关键-值表示的缓存以用于过去的标记，其大小与输入序列长度和批处理大小呈线性比例，因此生成仍然低效。作为解决方案，我们提出了动态内存压缩（DMC），这是一种用于在线关键-值缓存压缩的方法。最重要的是，模型学习在不同的头部和层中应用不同的压缩率。我们将预训练的LLMs（如Llama 2（7B、13B和70B））改装为DMC Transformers，在NVIDIA H100 GPU上的自回归推断中实现了高达~3.7倍的吞吐量增加。DMC通过在原始数据的可忽略百分比上进行持续的预训练而应用，并且不添加任何额外参数。我们发现，在高达4倍缓存压缩的情况下，DMC保留了原始的下游性能，优于up-trained grouped-query a。

    arXiv:2403.09636v1 Announce Type: new  Abstract: Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for on-line key-value cache compression at inference time. Most importantly, the model learns to apply different compression rates in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to ~3.7x throughput increase in auto-regressive inference on a NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. We find that DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query a
    
[^2]: Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models

    Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models

    [https://arxiv.org/abs/2403.09635](https://arxiv.org/abs/2403.09635)

    提出了一个统一的信号传播理论，提供了控制transformer模型信号传播的公式，提出了DeepScaleLM初始化和缩放方案，使得可以训练非常深的模型，并发现深层模型在多个任务和数据集上胜过浅层模型。

    

    尽管transformer模型取得了巨大的成功，但在深度方面仍然很难扩展。本研究提出了一个统一的信号传播理论，并提供了控制transformer模型前向和反向信号矩的公式。我们的框架可以用于理解和缓解与高注意力分数相关的梯度消失/爆炸、秩坍缩和不稳定性。我们还提出了DeepScaleLM，一种初始化和缩放方案，通过该方案能够在模型中保持单位输出/梯度矩，从而使训练具有100多层的非常深模型成为可能。我们发现，transformer模型可以更深 - 我们的深层模型在语言建模、语音翻译和图像分类方面表现优异，包括仅编码器、仅解码器和编码器-解码器变体，适用于Pre-LN和Post-LN transformers，适用于多个数据集和模型大小。

    arXiv:2403.09635v1 Announce Type: cross  Abstract: In spite of their huge success, transformer models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 100s of layers. We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across Encoder-only, Decoder-only and Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes. These imp
    
[^3]: 3D-VLA: 一个3D视觉-语言-动作生成世界模型

    3D-VLA: A 3D Vision-Language-Action Generative World Model

    [https://arxiv.org/abs/2403.09631](https://arxiv.org/abs/2403.09631)

    提出了3D-VLA，通过将3D感知、推理和动作无缝连接，建立一个生成世界模型，弥补了现有VLA模型只能处理2D输入且忽视世界动态与动作之间关系的不足。

    

    最近的视觉-语言-动作（VLA）模型依赖于2D输入，缺乏与更广阔的3D物理世界融合。此外，它们通过学习从感知到动作的直接映射来执行动作预测，忽略了世界的广泛动态和动作与动态之间的关系。相反，人类拥有描绘关于未来场景的想象，以相应地规划行动的世界模型。为此，我们通过引入一系列新的具身基础模型，无缝地将3D感知、推理和动作通过一个生成世界模型相连，提出了3D-VLA。具体地，3D-VLA建立在基于3D的大型语言模型（LLM）之上，并引入一组交互标记以与具身环境进行交互。此外，为了将生成能力注入模型，我们训练了一系列具身扩散模型，并将它们与LLM对齐以进行预测。

    arXiv:2403.09631v1 Announce Type: cross  Abstract: Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting
    
[^4]: Quiet-STaR: 语言模型可以自己学会思考后再说话

    Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking

    [https://arxiv.org/abs/2403.09629](https://arxiv.org/abs/2403.09629)

    Quiet-STaR提出了一种新的泛化版本，在每个标记处生成解释未来文本的思考过程，从而改善预测能力

    

    写作和交谈时，人们有时会停下来思考。尽管以推理为重点的作品通常将推理框定为回答问题或完成代理任务的方法，但推理几乎都隐含在所有书面文本中。例如，这适用于证明中未明确说明的步骤，以及支撑对话的心智理论。在自学习推理者（STaR，Zelikman等，2022）中，通过从少量示例中推断来自问答中有用的思考，并学习那些导致正确答案的思考。这是一个高度受限制的环境--理想情况下, 一个语言模型可以学会从任意文本中推断未明确说明的思考。我们提出Quiet-STaR，这是STaR的一个泛化版本，其中语言模型学会在每个标记处生成解释未来文本的思考过程，从而改善其预测。我们解决了一些关键挑战，包括1）生成连续的计算成本

    arXiv:2403.09629v1 Announce Type: cross  Abstract: When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continu
    
[^5]: 通过结构化训练重新唤醒知识：从灾难性干扰中进行预期性恢复

    Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training

    [https://arxiv.org/abs/2403.09613](https://arxiv.org/abs/2403.09613)

    在结构化环境中依次微调的LLMs表现出预期行为，能够从遗忘中恢复，揭示了在过参数化网络中进行训练的新见解

    

    我们探讨了神经网络在一个结构化的非独立同分布设置中的训练动态，其中文档以固定重复序列的方式呈现。通常情况下，在一系列文档上训练时，网络会遭受灾难性干扰；然而，我们发现在这种设置下依次微调的LLMs表现出一种奇特且卓越的特性：它们表现出预期的行为，在再次遇到之前的文档时从遗忘中恢复过来。这种行为在架构扩展其参数数量时逐渐出现并变得更加稳健。通过全面的实验和可视化，我们揭示了在结构化环境中训练超参数网络的新见解。

    arXiv:2403.09613v1 Announce Type: cross  Abstract: We explore the training dynamics of neural networks in a structured non-IID setting where documents are presented cyclically in a fixed, repeated sequence. Typically, networks suffer from catastrophic interference when training on a sequence of documents; however, we discover a curious and remarkable property of LLMs fine-tuned sequentially in this setting: they exhibit anticipatory behavior, recovering from the forgetting on documents before encountering them again. The behavior emerges and becomes more robust as the architecture scales up its number of parameters. Through comprehensive experiments and visualizations, we uncover new insights into training over-parameterized networks in structured environments.
    
[^6]: MM1：多模式LLM预训练的方法、分析与见解

    MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training

    [https://arxiv.org/abs/2403.09611](https://arxiv.org/abs/2403.09611)

    通过详细研究图像编码器、视觉语言连接器和预训练数据选择的重要性，确定了对于实现多个基准测试中最新潮的少样本结果至关重要的关键设计经验。

    

    在这项工作中，我们讨论了构建高性能的多模式大型语言模型（MLLMs）。具体来说，我们研究了各种架构组件和数据选择的重要性。通过对图像编码器、视觉语言连接器和各种预训练数据选择进行仔细和全面的消融实验，我们确定了几个关键的设计经验。例如，我们展示了对大规模多模式预训练使用仔细混合的图像标题、交替图像文本和仅文本数据对于在多个基准测试中实现最新潮（SOTA）的少样本结果至关重要，与其他已发表的预训练结果相比。此外，我们表明图像编码器连同图像分辨率和图像标记计数具有重要影响，而视觉语言连接器设计相对重要性较小。通过扩大所提出的方法，我们构建了MM1，一个多模式模型系列。

    arXiv:2403.09611v1 Announce Type: cross  Abstract: In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up 
    
[^7]: 大型语言模型与协作中的因果推断：一项综合调查

    Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey

    [https://arxiv.org/abs/2403.09606](https://arxiv.org/abs/2403.09606)

    大型语言模型的出现极大影响了自然语言处理领域，特别是通过其先进的推理能力。而本综述则重点评估和改进了大型语言模型在因果推断方面的应用，包括提高推理能力、解决公平和安全问题、提供解释和处理多模态。

    

    因果推断已经显示出潜力，通过捕捉变量之间的因果关系，提高自然语言处理（NLP）模型的预测准确性、公平性、稳健性和可解释性。生成型大型语言模型（LLMs）的出现显著影响了各种NLP领域，特别是通过其先进的推理能力。该调查重点评估和改进LLMs的因果视角，在以下领域展开：理解和改进LLMs的推理能力，解决LLMs中的公平性和安全性问题，为LLMs提供解释，并处理多模态。同时，LLMs强大的推理能力反过来可以通过帮助因果关系发现和因果效应估计来促进因果推断领域的发展。本综述探讨了因果推断框架与LLMs之间的相互作用，强调了它们的集体作用。

    arXiv:2403.09606v1 Announce Type: cross  Abstract: Causal inference has shown potential in enhancing the predictive accuracy, fairness, robustness, and explainability of Natural Language Processing (NLP) models by capturing causal relationships among variables. The emergence of generative Large Language Models (LLMs) has significantly impacted various NLP domains, particularly through their advanced reasoning capabilities. This survey focuses on evaluating and improving LLMs from a causal view in the following areas: understanding and improving the LLMs' reasoning capacity, addressing fairness and safety issues in LLMs, complementing LLMs with explanations, and handling multimodality. Meanwhile, LLMs' strong reasoning capacities can in turn contribute to the field of causal inference by aiding causal relationship discovery and causal effect estimations. This review explores the interplay between causal inference frameworks and LLMs from both perspectives, emphasizing their collective p
    
[^8]: 数据价值评估对视觉指导调整的影响

    Less is More: Data Value Estimation for Visual Instruction Tuning

    [https://arxiv.org/abs/2403.09559](https://arxiv.org/abs/2403.09559)

    视觉指导调整时需要进行数据价值评估，通过新的数据选择方法TIVE，根据任务级和实例级价值来消除视觉指导数据中的冗余。

    

    视觉指导调整是构建多模式大语言模型（MLLMs）的关键，大大提高了大语言模型（LLMs）在视觉场景中的推理能力。然而，现有的MLLMs主要依赖于多个高度多样化的视觉指导数据集的混合训练（甚至超过一百万条指导），这可能引入数据冗余。为了调查这个问题，我们进行了一系列实证研究，揭示了视觉指导数据集内存在显著冗余，并显示大大减少几个指导数据集的数量甚至不会影响性能。根据研究结果，我们提出了一种新的数据选择方法TIVE，以消除视觉指导数据中的冗余。TIVE首先根据计算的梯度估计视觉指导的任务级和实例级价值。然后，根据估计的价值，TIVE确定了任务级和实例级指导选择策略。

    arXiv:2403.09559v1 Announce Type: new  Abstract: Visual instruction tuning is the key to building multimodal large language models (MLLMs), which greatly improves the reasoning capabilities of large language models (LLMs) in vision scenario. However, existing MLLMs mostly rely on a mixture of multiple highly diverse visual instruction datasets for training (even more than a million instructions), which may introduce data redundancy. To investigate this issue, we conduct a series of empirical studies, which reveal a significant redundancy within the visual instruction datasets, and show that greatly reducing the amount of several instruction dataset even do not affect the performance. Based on the findings, we propose a new data selection approach TIVE, to eliminate redundancy within visual instruction data. TIVE first estimates the task-level and instance-level value of the visual instructions based on computed gradients. Then, according to the estimated values, TIVE determines the tas
    
[^9]: API保护的LLMs的标志泄露专有信息

    Logits of API-Protected LLMs Leak Proprietary Information

    [https://arxiv.org/abs/2403.09539](https://arxiv.org/abs/2403.09539)

    大多数现代LLM受到softmax瓶颈影响，可以以较低成本获取API保护的LLM的非公开信息和解锁多种功能

    

    大型语言模型（LLMs）的商业化导致了高级API-only接入专有模型的常见实践。在这项工作中，我们展示了即使对于模型架构有保守的假设，也可以从相对较少的API查询中学习关于API保护的LLM的大量非公开信息（例如，使用OpenAI的gpt-3.5-turbo仅花费不到1000美元）。我们的发现集中在一个关键观察上：大多数现代LLM受到了softmax瓶颈的影响，这限制了模型输出到完整输出空间的线性子空间。我们表明，这导致了一个模型图像或模型签名，从而以较低的成本解锁了几种功能：有效发现LLM的隐藏大小，获取完整词汇输出，检测和消除不同模型更新，识别给定单个完整LLM输出的源LLM，以及...

    arXiv:2403.09539v1 Announce Type: cross  Abstract: The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM's hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and eve
    
[^10]: VisionGPT-3D:一种用于增强3D视觉理解的通用多模态代理

    VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding

    [https://arxiv.org/abs/2403.09530](https://arxiv.org/abs/2403.09530)

    提出了一个统一的VisionGPT-3D框架，整合了最先进的视觉模型，有助于提升计算机视觉对于3D视觉理解的能力

    

    文本向视觉组件的演进促进了人们日常生活的便利，例如从文本生成图像、视频并识别图像中所需的元素。以前的计算机视觉模型专注于基于明确定义对象的图像检测、分类。大型语言模型(LLMs)将自然语言转换为视觉对象，为文本背景提供了视觉布局。OpenAI GPT-4已成为LLMs的顶峰，而计算机视觉(CV)领域拥有大量最先进的模型和算法，可将2D图像转换为它们的3D表示。然而，算法与问题之间的不匹配可能导致不良结果。针对这一挑战，我们提出了一个统一的VisionGPT-3D框架， conslidate了最先进的视觉模型，从而促进了发展。

    arXiv:2403.09530v1 Announce Type: cross  Abstract: The evolution of text to visual components facilitates people's daily lives, such as generating image, videos from text and identifying the desired elements within the images. Computer vision models involving the multimodal abilities in the previous days are focused on image detection, classification based on well-defined objects. Large language models (LLMs) introduces the transformation from nature language to visual objects, which present the visual layout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs, while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models and algorithms to convert 2D images to their 3D representations. However, the mismatching between the algorithms with the problem could lead to undesired results. In response to this challenge, we propose an unified VisionGPT-3D framework to consolidate the state-of-the-art vision models, thereby facilitating the development
    
[^11]: MT-PATCHER：来自大型语言模型的有选择性和可扩展的知识蒸馏用于机器翻译

    MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation

    [https://arxiv.org/abs/2403.09522](https://arxiv.org/abs/2403.09522)

    提出了MT-Patcher框架，实现了从大型语言模型到中等规模机器翻译模型的有选择性、全面和主动的知识迁移

    

    大型语言模型（LLM）在机器翻译（MT）领域展现出强大的能力，但它们面临着高计算成本和延迟的问题。因此，将翻译知识从巨型LLM转移到中等规模的机器翻译模型是一个有前途的研究方向。本文提出了一个名为MT-Patcher的框架，以选择性、全面和主动的方式将知识从LLMs转移到现有的MT模型中。考虑到学生MT模型当前的翻译能力，我们仅识别和纠正其翻译错误，而不是从老师那里蒸馏整个翻译。

    arXiv:2403.09522v1 Announce Type: new  Abstract: Large Language Models (LLM) have demonstrated their strong ability in the field of machine translation (MT), yet they suffer from high computational cost and latency. Therefore, transferring translation knowledge from giant LLMs to medium-sized machine translation models is a promising research direction. However, traditional knowledge distillation methods do not take the capability of student and teacher models into consideration, therefore repeatedly teaching student models on the knowledge they have learned, and failing to extend to novel contexts and knowledge. In this paper, we propose a framework called MT-Patcher, which transfers knowledge from LLMs to existing MT models in a selective, comprehensive and proactive manner. Considering the current translation ability of student MT models, we only identify and correct their translation errors, instead of distilling the whole translation from the teacher. Leveraging the strong languag
    
[^12]: 利用典型表示减轻社会偏见而不使用人口统计信息

    Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information

    [https://arxiv.org/abs/2403.09516](https://arxiv.org/abs/2403.09516)

    通过利用预定义的典型人口统计文本并在微调过程中加入正则化项，本文提出的方法有效减轻了语言模型中的社会偏见，同时不需要依赖显式的人口统计标签。

    

    减轻社会偏见通常需要识别与每个数据样本相关联的社会群体。在本文中，我们提出了DAFair，一种新颖的方法来解决语言模型中的社会偏见问题。与依赖显式人口统计标签的传统方法不同，我们的方法不需要任何此类信息。相反，我们利用预定义的人口统计典型文本，并在微调过程中加入一个正则化项来减轻模型表示中的偏见。我们在两个任务和两个模型上的实证结果展示了我们的方法相对于之前不依赖标记数据的方法的有效性。此外，即使使用有限的人口统计标注数据，我们的方法也优于常见的去偏见方法。

    arXiv:2403.09516v1 Announce Type: new  Abstract: Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DAFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model's representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.
    
[^13]: 从怀疑到接受：模拟对虚假新闻态度动态的变化

    From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News

    [https://arxiv.org/abs/2403.09498](https://arxiv.org/abs/2403.09498)

    本研究引入了基于大型语言模型的虚假新闻传播仿真框架，研究了虚假新闻传播的趋势和控制，每个代理人在仿真中代表具有独特个性的个体。

    

    在数字时代，虚假新闻和谣言通过社交网络迅速传播，带来了显著的社会挑战，影响着公众舆论。传统的虚假新闻建模通常预测不同群体的普遍流行趋势或数字化代表意见转变。然而，这些方法经常过于简化现实世界的复杂性，忽视了新闻文本丰富的语义信息。大型语言模型（LLMs）的出现提供了模拟微妙意见动态的可能性。因此，在这项工作中，我们引入了基于LLM的虚假新闻传播仿真框架（FPS），详细研究虚假新闻传播的趋势和控制。具体地，仿真中的每个代理人代表具有独特个性的个人。他们配备了短期和长期记忆，以及反思机制来模仿类人思维。每天，

    arXiv:2403.09498v1 Announce Type: cross  Abstract: In the digital era, the rapid propagation of fake news and rumors via social networks brings notable societal challenges and impacts public opinion regulation. Traditional fake news modeling typically forecasts the general popularity trends of different groups or numerically represents opinions shift. However, these methods often oversimplify real-world complexities and overlook the rich semantic information of news text. The advent of large language models (LLMs) provides the possibility of modeling subtle dynamics of opinion. Consequently, in this work, we introduce a Fake news Propagation Simulation framework (FPS) based on LLM, which studies the trends and control of fake news propagation in detail. Specifically, each agent in the simulation represents an individual with a distinct personality. They are equipped with both short-term and long-term memory, as well as a reflective mechanism to mimic human-like thinking. Every day, the
    
[^14]: Hyper-CL：使用超网络对句子表示进行条件化

    Hyper-CL: Conditioning Sentence Representations with Hypernetworks

    [https://arxiv.org/abs/2403.09490](https://arxiv.org/abs/2403.09490)

    Hyper-CL是一种将超网络与对比学习相结合的有效方法，能够灵活地进行条件化句子表示。

    

    尽管将对比学习框架引入句子表示学习领域在很大程度上促进了该领域的进展，但当句子被特定角度条件化时，尤其是在捕捉句子的细粒度语义方面，目前最先进的句子嵌入能力仍然不清楚。本文引入了Hyper-CL，一种高效的方法，该方法将超网络与对比学习结合起来计算条件化的句子表示。在我们提出的方法中，超网络负责将预先计算的条件嵌入转换为相应的投影层。这使得相同的句子嵌入可以根据不同条件进行不同的投影。在两个代表性的条件化基准数据集上进行评估，即条件语义文本相似度和知识图完成，表明Hyper-CL在灵活地进行条件化方面是有效的。

    arXiv:2403.09490v1 Announce Type: new  Abstract: While the introduction of contrastive learning frameworks in sentence representation learning has significantly contributed to advancements in the field, it still remains unclear whether state-of-the-art sentence embeddings can capture the fine-grained semantics of sentences, particularly when conditioned on specific perspectives. In this paper, we introduce Hyper-CL, an efficient methodology that integrates hypernetworks with contrastive learning to compute conditioned sentence representations. In our proposed approach, the hypernetwork is responsible for transforming pre-computed condition embeddings into corresponding projection layers. This enables the same sentence embeddings to be projected differently according to various conditions. Evaluation on two representative conditioning benchmarks, namely conditional semantic text similarity and knowledge graph completion, demonstrates that Hyper-CL is effective in flexibly conditioning s
    
[^15]: 在上下文学习中纠正演示快捷方式

    Rectifying Demonstration Shortcut in In-Context Learning

    [https://arxiv.org/abs/2403.09488](https://arxiv.org/abs/2403.09488)

    本研究旨在纠正大型语言模型在上下文学习中的演示快捷方式，并引入了一种新的明示意识校准方法。

    

    大型语言模型（LLMs）能够利用它们的上下文学习（ICL）能力，仅凭少量演示便能解决各种任务。然而，LLMs常常依赖于它们对演示的预先训练的语义先验，而不是根据输入-标签关系继续进行ICL预测。本文将这一现象称为“演示快捷方式”。尽管先前的研究主要集中于改进预定义任务的ICL预测结果，我们的目标是纠正演示快捷方式，从而使LLM能够有效地从演示中学习新的输入-标签关系。为实现此目标，我们引入了一种明示意识的校准方法：In-Context Calibration。我们在两个设置中评估了所提出方法的有效性：（1）使用标准标签空间的原始ICL任务以及（2）任务学习设置，其中标签空间被语义无关的标记替换。

    arXiv:2403.09488v1 Announce Type: cross  Abstract: Large language models (LLMs) are able to solve various tasks with only a few demonstrations utilizing their in-context learning (ICL) abilities. However, LLMs often rely on their pre-trained semantic priors of demonstrations rather than on the input-label relationships to proceed with ICL prediction. In this work, we term this phenomenon as the `Demonstration Shortcut'. While previous works have primarily focused on improving ICL prediction results for predefined tasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM to effectively learn new input-label relationships from demonstrations. To achieve this, we introduce In-Context Calibration, a demonstration-aware calibration method. We evaluate the effectiveness of the proposed method in two settings: (1) the Original ICL Task using the standard label space and (2) the Task Learning setting, where the label space is replaced with semantically unrelated tokens. In 
    
[^16]: 易于难的泛化：超越人类监督的可扩展对齐

    Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision

    [https://arxiv.org/abs/2403.09472](https://arxiv.org/abs/2403.09472)

    通过从更简单的任务学习，实现对更难推理任务的有效泛化，提出了一种可扩展对齐方法。

    

    当前人工智能对齐方法依赖于人类提供的演示或判断，由于这种方法，AI系统学习到的能力将受到人类能力的上界限制。这就带来了一个具有挑战性的研究问题：当系统的能力超过人类水平时，我们如何继续改进这些系统？本文在解决难度推理任务（如4-5级数学问题）的背景下回答了这个问题，通过从更简单的任务（如1-3级数学问题）中学习人类注释，我们将其称为“易于难的泛化”。我们的关键观点是，一个在更简单任务的监督下训练的评估器（奖励模型）可以有效地用于评分更难任务的候选解决方案，从而促进在不同难度任务间的易于难的泛化。基于这一观点，我们提出了一种新的可扩展对齐方法，首先训练处理督导

    arXiv:2403.09472v1 Announce Type: cross  Abstract: Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as \textit{easy-to-hard generalization}. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process-supervise
    
[^17]: "像套娃一样"：利用大语言模型分析计算机科学学生生成的递归类比

    "Like a Nesting Doll": Analyzing Recursion Analogies Generated by CS Students using Large Language Models

    [https://arxiv.org/abs/2403.09409](https://arxiv.org/abs/2403.09409)

    使用大语言模型ChatGPT，研究了超过350名一年级计算机学生生成的递归类比，探讨了如何利用这些类比帮助理解复杂的计算概念

    

    把复杂的计算概念融会贯通常常是学生面临的挑战，他们往往难以将这些新想法锚定在熟悉的经验和理解之上。为了帮助解决这一问题，一个好的类比可以弥合陌生概念与熟悉概念之间的鸿沟，提供了一种引人入胜的方式来帮助理解。然而，即使对于经验丰富的教师来说，创造有效的教育类比也是困难的。我们研究了大语言模型（LLMs），特别是ChatGPT，到底能够在多大程度上提供按需访问个人相关类比的可能性。我们专注于递归，这是一个具有挑战性的门槛概念，我们进行了一项调查，分析了350多名一年级计算机学生生成的类比。他们被要求使用ChatGPT生成基于递归的类比，其中可以选择在提示中包含个人相关主题。我们观察到生成的类比呈现出极大的多样性。

    arXiv:2403.09409v1 Announce Type: cross  Abstract: Grasping complex computing concepts often poses a challenge for students who struggle to anchor these new ideas to familiar experiences and understandings. To help with this, a good analogy can bridge the gap between unfamiliar concepts and familiar ones, providing an engaging way to aid understanding. However, creating effective educational analogies is difficult even for experienced instructors. We investigate to what extent large language models (LLMs), specifically ChatGPT, can provide access to personally relevant analogies on demand. Focusing on recursion, a challenging threshold concept, we conducted an investigation analyzing the analogies generated by more than 350 first-year computing students. They were provided with a code snippet and tasked to generate their own recursion-based analogies using ChatGPT, optionally including personally relevant topics in their prompts. We observed a great deal of diversity in the analogies p
    
[^18]: 科莫多：探索印度尼西亚地区语言的语言考察

    Komodo: A Linguistic Expedition into Indonesia's Regional Languages

    [https://arxiv.org/abs/2403.09362](https://arxiv.org/abs/2403.09362)

    Komodo-7B是一个大型语言模型，可以无缝操作印度尼西亚、英语和11种印度尼西亚地区语言，Komodo-7B-Instruct达到了卓越的性能，超越了多个基准模型。

    

    最近在大型语言模型（LLMs）方面取得的突破主要集中在具有易于获取和充足资源的语言上，例如英语。然而，对于在公共领域缺乏足够语言资源的语言仍存在重大差距。我们的工作引入了Komodo-7B，一个70亿参数的大型语言模型，旨在通过在印度尼西亚、英语和印度尼西亚的11种地区语言之间无缝操作来填补这一差距。Komodo-7B是一组LLMs，由Komodo-7B-Base和Komodo-7B-Instruct组成。Komodo-7B-Instruct凭借在各种任务和语言上取得的最先进性能脱颖而出，超越了OpenAI的GPT-3.5、Cohere的Aya-101、Llama-2-Chat-13B、Mixtral-8x7B-Instruct-v0.1、Gemma-7B-it等模型制定的基准。这个模型不仅在语言特定和整体评估中表现出优越性能，还突显了其在相关任务中表现出色的能力。

    arXiv:2403.09362v1 Announce Type: new  Abstract: The recent breakthroughs in Large Language Models (LLMs) have mostly focused on languages with easily available and sufficient resources, such as English. However, there remains a significant gap for languages that lack sufficient linguistic resources in the public domain. Our work introduces Komodo-7B, 7-billion-parameter Large Language Models designed to address this gap by seamlessly operating across Indonesian, English, and 11 regional languages in Indonesia. Komodo-7B is a family of LLMs that consist of Komodo-7B-Base and Komodo-7B-Instruct. Komodo-7B-Instruct stands out by achieving state-of-the-art performance in various tasks and languages, outperforming the benchmarks set by OpenAI's GPT-3.5, Cohere's Aya-101, Llama-2-Chat-13B, Mixtral-8x7B-Instruct-v0.1, Gemma-7B-it , and many more. This model not only demonstrates superior performance in both language-specific and overall assessments but also highlights its capability to excel
    
[^19]: 超越言语：歌唱语音识别中的进展与挑战

    More than words: Advancements and challenges in speech recognition for singing

    [https://arxiv.org/abs/2403.09298](https://arxiv.org/abs/2403.09298)

    本文讨论了歌唱语音识别中的挑战和进展，探索了音素识别、歌曲中的语言识别、关键词识别和完整歌词转录等关键领域，并介绍了深度学习和大规模数据集在该领域推动进展的最新发展。

    

    本文讨论了歌唱语音识别中的挑战和进展，这是一个与标准语音识别完全不同的领域。歌唱包含独特的挑战，包括广泛的音高变化、多样化的声乐风格以及背景音乐干扰。我们探讨了诸如音素识别、歌曲中的语言识别、关键词识别和完整歌词转录等关键领域。我将描述一些我在这些任务上进行研究时的经历，就在它们开始崭露头角的时候，但也会展示深度学习和大规模数据集的最新进展如何推动了这一领域的进步。我的目标是阐明将语音识别应用于歌唱时的复杂性，评估当前的能力，并概述未来的研究方向。

    arXiv:2403.09298v1 Announce Type: cross  Abstract: This paper addresses the challenges and advancements in speech recognition for singing, a domain distinctly different from standard speech recognition. Singing encompasses unique challenges, including extensive pitch variations, diverse vocal styles, and background music interference. We explore key areas such as phoneme recognition, language identification in songs, keyword spotting, and full lyrics transcription. I will describe some of my own experiences when performing research on these tasks just as they were starting to gain traction, but will also show how recent developments in deep learning and large-scale datasets have propelled progress in this field. My goal is to illuminate the complexities of applying speech recognition to singing, evaluate current capabilities, and outline future research directions.
    
[^20]: 解剖结构引导的医学视觉-语言预训练

    Anatomical Structure-Guided Medical Vision-Language Pre-training

    [https://arxiv.org/abs/2403.09294](https://arxiv.org/abs/2403.09294)

    该研究提出了一种解剖结构引导的医学视觉-语言预训练框架，通过将原始报告解析为三元组并利用每个元素作为监督来增强表示学习，解决了局部对齐和图像-报告对表示学习中的挑战。

    

    通过视觉-语言预训练学习医学视觉表示已取得显著进展。尽管表现出有希望的性能，但仍面临挑战，即局部对齐缺乏可解释性和临床相关性，以及图像-报告对的内部和外部表示学习不足。为了解决这些问题，我们提出了一种解剖结构引导的（ASG）框架。具体来说，我们将原始报告解析为三元组，并充分利用每个元素作为监督来增强表示学习。对于解剖区域，我们设计了一种与放射科医师合作的自动解剖区域-句子对齐范例，将其视为最小语义单元来探索细粒度局部对齐。对于查找和存在性，我们将其视为图像标签，应用图像标签识别解码器，在每个样本内将图像特征与其相应标签关联起来，构建。

    arXiv:2403.09294v1 Announce Type: cross  Abstract: Learning medical visual representations through vision-language pre-training has reached remarkable progress. Despite the promising performance, it still faces challenges, i.e., local alignment lacks interpretability and clinical relevance, and the insufficient internal and external representation learning of image-report pairs. To address these issues, we propose an Anatomical Structure-Guided (ASG) framework. Specifically, we parse raw reports into triplets , and fully utilize each element as supervision to enhance representation learning. For anatomical region, we design an automatic anatomical region-sentence alignment paradigm in collaboration with radiologists, considering them as the minimum semantic units to explore fine-grained local alignment. For finding and existence, we regard them as image tags, applying an image-tag recognition decoder to associate image features with their respective tags within each sample and construc
    
[^21]: 是否给数据贴标签：神经机器翻译的混合主动学习

    To Label or Not to Label: Hybrid Active Learning for Neural Machine Translation

    [https://arxiv.org/abs/2403.09259](https://arxiv.org/abs/2403.09259)

    提出了一种用于神经机器翻译的混合主动学习策略HUDS，结合了不确定性和多样性，用于领域自适应的句子选择。

    

    主动学习技术通过从未标记数据中选择更小的代表性子集进行注释，降低了训练神经机器翻译（NMT）模型的标记成本。我们提出了HUDS，这是一种用于NMT领域自适应的混合主动学习策略，将不确定性和多样性相结合，以进行句子选择。

    arXiv:2403.09259v1 Announce Type: new  Abstract: Active learning (AL) techniques reduce labeling costs for training neural machine translation (NMT) models by selecting smaller representative subsets from unlabeled data for annotation. Diversity sampling techniques select heterogeneous instances, while uncertainty sampling methods select instances with the highest model uncertainty. Both approaches have limitations - diversity methods may extract varied but trivial examples, while uncertainty sampling can yield repetitive, uninformative instances. To bridge this gap, we propose HUDS, a hybrid AL strategy for domain adaptation in NMT that combines uncertainty and diversity for sentence selection. HUDS computes uncertainty scores for unlabeled sentences and subsequently stratifies them. It then clusters sentence embeddings within each stratum using k-MEANS and computes diversity scores by distance to the centroid. A weighted hybrid score that combines uncertainty and diversity is then us
    
[^22]: 使用电子健康记录的检索增强文本到SQL生成器用于流行病学问题回答

    Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records

    [https://arxiv.org/abs/2403.09226](https://arxiv.org/abs/2403.09226)

    结合文本到SQL生成与检索增强生成（RAG）的方法，可用于使用电子健康记录和索赔数据回答流行病学问题，并在现实行业中显示出显著性能提升。

    

    电子健康记录（EHR）和索赔数据是反映患者健康状况和医疗利用情况的丰富现实世界数据来源。查询这些数据库以回答流行病学问题具有挑战性，原因在于医学术语的复杂性和对复杂SQL查询的需求。我们介绍了一种端到端的方法，将文本到SQL生成与检索增强生成（RAG）结合起来，使用EHR和索赔数据回答流行病学问题。我们展示了我们的方法，将医学编码步骤整合到文本到SQL过程中，显著提高了性能，而不仅仅是简单提示。我们的研究结果表明，尽管当前的语言模型尚不足以无监督使用，但RAG为改进它们的能力提供了一个有希望的方向，如在一个现实的行业环境中所示。

    arXiv:2403.09226v1 Announce Type: new  Abstract: Electronic health records (EHR) and claims data are rich sources of real-world data that reflect patient health status and healthcare utilization. Querying these databases to answer epidemiological questions is challenging due to the intricacy of medical terminology and the need for complex SQL queries. Here, we introduce an end-to-end methodology that combines text-to-SQL generation with retrieval augmented generation (RAG) to answer epidemiological questions using EHR and claims data. We show that our approach, which integrates a medical coding step into the text-to-SQL process, significantly improves the performance over simple prompting. Our findings indicate that although current language models are not yet sufficiently accurate for unsupervised use, RAG offers a promising direction for improving their capabilities, as shown in a realistic industry setting.
    
[^23]: TaxoLLaMA:基于WordNet的模型用于解决多个词汇语义任务

    TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Sematic Tasks

    [https://arxiv.org/abs/2403.09207](https://arxiv.org/abs/2403.09207)

    通过基于WordNet的LLMs模型，提出了TaxoLLaMA模型，采用4位量化和LoRA技术轻量化，在多个词汇语义任务中取得11个SotA结果，且在词汇蕴涵和分类学构建任务上表现出强大的零样本性能。

    

    这篇论文探讨了LLMs在捕捉WordNet中的词汇语义知识方面的能力，以LLaMA-2-7b模型为例，并在多个词汇语义任务上对其进行了测试。作为我们实验的结果，我们提出了TaxoLLaMA，即一切皆在其中的模型，由于4位量化和LoRA而轻量化。它在分类学丰富化、上位词发现、分类学构建和词汇蕴涵任务中实现了11个SotA结果，16个任务中的4个前2名结果。此外，它展示了在词汇蕴涵和分类学构建上具有非常强大的零样本性能，无需微调。我们还探讨了其具有的隐藏多语言和领域适应能力，仅需少量调整或少量学习。所有数据集、代码和模型都可在https://github.com/VityaVitalich/TaxoLLaMA找到。

    arXiv:2403.09207v1 Announce Type: new  Abstract: In this paper, we explore the capabilities of LLMs in capturing lexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present TaxoLLaMA, the everything-in-one model, lightweight due to 4-bit quantization and LoRA. It achieves 11 SotA results, 4 top-2 results out of 16 tasks for the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates very strong zero-shot performance on Lexical Entailment and Taxonomy Construction with no fine-tuning. We also explore its hidden multilingual and domain adaptation capabilities with a little tuning or few-shot learning. All datasets, code, and model are available online at https://github.com/VityaVitalich/TaxoLLaMA
    
[^24]: Dial-insight: 使用高质量领域特定数据对大型语言模型进行微调，避免能力崩溃

    Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse

    [https://arxiv.org/abs/2403.09167](https://arxiv.org/abs/2403.09167)

    通过提出具有多样提示和多维质量评估框架的两阶段方法，可以在避免模型泛化能力下降的情况下，使用高质量领域特定数据对大型语言模型进行微调。

    

    大型语言模型（LLMs）的有效性严重依赖于基础数据的质量，特别是在专业领域内。在为特定领域应用对LLMs进行微调时经常面临的挑战是模型泛化能力的潜在退化。为了解决这些问题，我们提出了一个两阶段的方法来构建旨在产生高质量数据的生产提示。该方法涉及生成涵盖各种任务和展现丰富表达形式的多样提示。此外，我们引入了一种经济高效的、多维质量评估框架，以确保生成的标注数据的完整性。利用来自房地产行业的服务提供商和客户互动组成的数据集，我们证明了数据质量与模型性能之间存在正向相关性。值得注意的是，我们的研究结果表明

    arXiv:2403.09167v1 Announce Type: new  Abstract: The efficacy of large language models (LLMs) is heavily dependent on the quality of the underlying data, particularly within specialized domains. A common challenge when fine-tuning LLMs for domain-specific applications is the potential degradation of the model's generalization capabilities. To address these issues, we propose a two-stage approach for the construction of production prompts designed to yield high-quality data. This method involves the generation of a diverse array of prompts that encompass a broad spectrum of tasks and exhibit a rich variety of expressions. Furthermore, we introduce a cost-effective, multi-dimensional quality assessment framework to ensure the integrity of the generated labeling data. Utilizing a dataset comprised of service provider and customer interactions from the real estate sector, we demonstrate a positive correlation between data quality and model performance. Notably, our findings indicate that t
    
[^25]: 探究ChatGPT在中医知识理解中的应用

    Exploring the Comprehension of ChatGPT in Traditional Chinese Medicine Knowledge

    [https://arxiv.org/abs/2403.09164](https://arxiv.org/abs/2403.09164)

    探索了ChatGPT在中医知识理解中的应用，提出了TCM-QA数据集，评估了LLM的性能，并发现在判断题中表现最佳，中文提示优于英文提示。

    

    尚无先前的研究关注大语言模型（LLMs）在传统中医学（TCM）领域中的性能，这是一门具有丰富历史的重要而独特的医学知识分支。为弥补这一空白，我们提出了一个名为TCM-QA的中医问题数据集，包括三种问题类型：单项选择、多项选择和判断题，以检验LLM在TCM领域内知识回忆和全面推理的能力。在我们的研究中，我们评估了LLM的两种设置，即零次和少次设置，同时讨论了英文和中文提示之间的差异。我们的结果表明，ChatGPT在判断题中表现最佳，精确度最高达到0.688，而在多项选择问题中得分最低的精确度为0.241。此外，我们观察到在我们的评估中，中文提示表现优于英文提示。

    arXiv:2403.09164v1 Announce Type: new  Abstract: No previous work has studied the performance of Large Language Models (LLMs) in the context of Traditional Chinese Medicine (TCM), an essential and distinct branch of medical knowledge with a rich history. To bridge this gap, we present a TCM question dataset named TCM-QA, which comprises three question types: single choice, multiple choice, and true or false, to examine the LLM's capacity for knowledge recall and comprehensive reasoning within the TCM domain. In our study, we evaluate two settings of the LLM, zero-shot and few-shot settings, while concurrently discussing the differences between English and Chinese prompts. Our results indicate that ChatGPT performs best in true or false questions, achieving the highest precision of 0.688 while scoring the lowest precision is 0.241 in multiple-choice questions. Furthermore, we observed that Chinese prompts outperformed English prompts in our evaluations. Additionally, we assess the quali
    
[^26]: Caveat Lector：在法律实践中使用大型语言模型

    Caveat Lector: Large Language Models in Legal Practice

    [https://arxiv.org/abs/2403.09163](https://arxiv.org/abs/2403.09163)

    LLMs在法律实践中的作用被过分乐观地预测，不理解文本内容会导致依赖风险。

    

    当前对大型语言模型（LLMs）的着迷源于许多用户缺乏评估生成文本质量的专业知识。因此，LLMs可能看起来比它们实际上更有能力。流畅性和表面合理性的危险组合导致人们倾向于相信生成的文本，并产生过度依赖的风险。谁不会相信完美的法律用语呢？本文基于最近在技术和法律学术界的发现，对LLMs在法律实践中的作用进行了过分乐观的预测进行了平衡。在没有更好理解其局限性的情况下将LLMs整合到法律工作流程中，将会产生低效甚至直接的风险。尽管它们能够生成文本，但LLMs并不理解文本。没有理解意义的能力，LLMs将无法使用语言，获取知识并执行...

    arXiv:2403.09163v1 Announce Type: new  Abstract: The current fascination with large language models, or LLMs, derives from the fact that many users lack the expertise to evaluate the quality of the generated text. LLMs may therefore appear more capable than they actually are. The dangerous combination of fluency and superficial plausibility leads to the temptation to trust the generated text and creates the risk of overreliance. Who would not trust perfect legalese? Relying recent findings in both technical and legal scholarship, this Article counterbalances the overly optimistic predictions as to the role of LLMs in legal practice. Integrating LLMs into legal workstreams without a better comprehension of their limitations, will create inefficiencies if not outright risks. Notwithstanding their unprecedented ability to generate text, LLMs do not understand text. Without the ability to understand meaning, LLMs will remain unable to use language, to acquire knowledge and to perform compl
    
[^27]: 揭示经过微调的大型语言模型的泛化能力

    Unveiling the Generalization Power of Fine-Tuned Large Language Models

    [https://arxiv.org/abs/2403.09162](https://arxiv.org/abs/2403.09162)

    本文研究了经过微调的大型语言模型的泛化能力，发现在生成和分类任务上进行微调的模型在泛化到不同领域和任务时表现出不同行为，引入上下文学习策略可以提高模型的泛化能力。

    

    虽然大型语言模型（LLMs）展示了出色的多任务能力，但通常需要在下游的领域特定数据集上对这些模型进行微调，以在测试集上获得比未经微调的模型更优越的性能。然而，关于微调对LLMs泛化能力的全面影响尚不完全了解。本文深入探讨了原始、未修改的LLMs与经过微调变体之间的差异。我们的主要研究重点在于微调是否会影响内在于LLMs的泛化能力。为了阐明这一点，我们在不同数据集上对五个不同语言任务进行了广泛实验。我们的主要发现表明，在生成和分类任务上进行微调的模型在泛化到不同领域和任务时表现出不同行为。有趣的是，我们发现在上下文学习策略的引入的情况下，对于某些具体任务的微调可以提高LLMs的泛化能力。

    arXiv:2403.09162v1 Announce Type: new  Abstract: While Large Language Models (LLMs) have demonstrated exceptional multitasking abilities, fine-tuning these models on downstream, domain-specific datasets is often necessary to yield superior performance on test sets compared to their counterparts without fine-tuning. However, the comprehensive effects of fine-tuning on the LLMs' generalization ability are not fully understood. This paper delves into the differences between original, unmodified LLMs and their fine-tuned variants. Our primary investigation centers on whether fine-tuning affects the generalization ability intrinsic to LLMs. To elaborate on this, we conduct extensive experiments across five distinct language tasks on various datasets. Our main findings reveal that models fine-tuned on generation and classification tasks exhibit dissimilar behaviors in generalizing to different domains and tasks. Intriguingly, we observe that integrating the in-context learning strategy durin
    
[^28]: 巴斯克语和西班牙语反叙事生成：数据创建与评估

    Basque and Spanish Counter Narrative Generation: Data Creation and Evaluation

    [https://arxiv.org/abs/2403.09159](https://arxiv.org/abs/2403.09159)

    在这项研究中，我们提出了CONAN-EUS，这是一个通过机器翻译和专业后期编辑开发的巴斯克语和西班牙语数据集，用于生成反叙事。实验结果表明，通过在后期编辑数据上训练，而不是仅依赖于机器翻译数据，能够显著提高反叙事生成的质量。

    

    反叙事（CNs）是对仇恨言论（HS）的非负文本回应，旨在化解在线仇恨并减轻其在媒体之间的传播。尽管在线仇恨言论内容最近有所增加，但有关自动生成反叙事的研究相对较少，且主要集中在英语上。在本文中，我们提出了CONAN-EUS，这是一个通过机器翻译（MT）和专业后期编辑开发的用于生成反叙事的巴斯克语和西班牙语数据集。作为平行语料库，与原始英文CONAN相比，它允许进行关于多语言和跨语言自动生成反叙事的新型研究。我们使用mT5进行了CN生成实验，这是一个多语言编码器-解码器模型，结果表明，相较于仅依赖于银标机器翻译数据，通过在后期编辑数据上训练，生成效果大幅提升。这些结果与定性手动评估相互印证。

    arXiv:2403.09159v1 Announce Type: new  Abstract: Counter Narratives (CNs) are non-negative textual responses to Hate Speech (HS) aiming at defusing online hatred and mitigating its spreading across media. Despite the recent increase in HS content posted online, research on automatic CN generation has been relatively scarce and predominantly focused on English. In this paper, we present CONAN-EUS, a new Basque and Spanish dataset for CN generation developed by means of Machine Translation (MT) and professional post-edition. Being a parallel corpus, also with respect to the original English CONAN, it allows to perform novel research on multilingual and crosslingual automatic generation of CNs. Our experiments on CN generation with mT5, a multilingual encoder-decoder model, show that generation greatly benefits from training on post-edited data, as opposed to relying on silver MT data only. These results are confirmed by their correlation with a qualitative manual evaluation, demonstratin
    
[^29]: 评估用于显著人物的LLMs中的性别差距

    Evaluating LLMs for Gender Disparities in Notable Persons

    [https://arxiv.org/abs/2403.09148](https://arxiv.org/abs/2403.09148)

    评估大型语言模型在显著人物中存在的性别差距，并发现GPT-4在性能上有所改进，但问题尚未完全解决

    

    本研究探讨了大型语言模型（LLMs）用于检索事实信息的使用，解决了它们产生事实不准确的“幻觉”回复或完全拒绝甚至回答提示的倾向。具体来说，它调查了LLMs对事实查询的回应中存在的基于性别的偏见。这篇论文通过评估GPT模型在召回、幻觉和拒绝等多个维度上的公平性来采用多管齐下的方法。我们的研究发现GPT-3.5生成的回应中存在明显的性别差距。虽然GPT-4的进展提升了性能，但在回应被拒绝的情况下，这些性别差距并未完全消除。研究进一步探讨了这些差距的起源，通过检查提示中的性别关联和回应中的同质性。

    arXiv:2403.09148v1 Announce Type: new  Abstract: This study examines the use of Large Language Models (LLMs) for retrieving factual information, addressing concerns over their propensity to produce factually incorrect "hallucinated" responses or to altogether decline to even answer prompt at all. Specifically, it investigates the presence of gender-based biases in LLMs' responses to factual inquiries. This paper takes a multi-pronged approach to evaluating GPT models by evaluating fairness across multiple dimensions of recall, hallucinations and declinations. Our findings reveal discernible gender disparities in the responses generated by GPT-3.5. While advancements in GPT-4 have led to improvements in performance, they have not fully eradicated these gender disparities, notably in instances where responses are declined. The study further explores the origins of these disparities by examining the influence of gender associations in prompts and the homogeneity in the responses.
    
[^30]: ProSwitch：知识引导的语言模型微调，生成专业和非专业风格的文本

    ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text

    [https://arxiv.org/abs/2403.09131](https://arxiv.org/abs/2403.09131)

    ProSwitch通过知识引导的指令微调，在专业和非专业风格之间生成文本，并在专业性评估和质量评估方面表现出优越性。

    

    大语言模型（LLMs）在各种语言应用中表现出有效性，包括文本摘要和可控文本生成。然而，关于它们通过微调在不同风格间切换的能力的研究仍未被充分探讨。本研究聚焦于文本专业性，并引入了一种新颖的方法，名为ProSwitch，通过知识引导的指令微调，使语言模型具备生成专业和非专业回复的能力。ProSwitch分为三个阶段：数据准备，用于收集领域知识和训练语料库；指令微调，用于优化带有多种指令格式的语言模型；全面评估，用于评估生成文本的专业性区分能力和基于参考的质量。 ProSwitch相对于通用和专门语言模型的比较分析显示了我们的方法的优越性。

    arXiv:2403.09131v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including text summarization and controlled text generation. However, studies into their capacity of switching between styles via fine-tuning remain underexplored. This study concentrates on textual professionalism and introduces a novel methodology, named ProSwitch, which equips a language model with the ability to produce both professional and non-professional responses through knowledge-guided instruction tuning. ProSwitch unfolds across three phases: data preparation for gathering domain knowledge and training corpus; instruction tuning for optimizing language models with multiple levels of instruction formats; and comprehensive evaluation for assessing the professionalism discrimination and reference-based quality of generated text. Comparative analysis of ProSwitch against both general and specialized language models reveals that our appro
    
[^31]: AutoLoRA：基于元学习的自动调整矩阵秩在低秩适应中的应用

    AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning

    [https://arxiv.org/abs/2403.09113](https://arxiv.org/abs/2403.09113)

    AutoLoRA提出了一个基于元学习的框架，自动识别每个LoRA层的最佳秩，以解决LoRA中秩分配和秩搜索的问题，进而提高微调性能。

    

    大规模预训练之后进行任务特定微调在各种自然语言处理任务中取得了巨大成功。然而，对于大型预训练模型的所有参数进行微调存在着巨大的计算和内存挑战，因此研发了几种高效的微调方法。其中，低秩适应（LoRA）通过在冻结的预训练权重之上微调低秩增量更新矩阵，被证明特别有效。然而，LoRA在所有层中均匀分配秩，并且依赖于穷举搜索来找到最佳秩，导致了高计算成本和微调性能不佳。为了解决这些限制，我们引入了AutoLoRA，这是一个基于元学习的框架，用于自动识别每个LoRA层的最佳秩。AutoLoRA将低秩更新矩阵中的每个秩为1的矩阵与选择变量相关联，该变量决定了秩为1的矩阵是否应该被...

    arXiv:2403.09113v1 Announce Type: cross  Abstract: Large-scale pretraining followed by task-specific finetuning has achieved great success in various NLP tasks. Since finetuning all parameters of large pretrained models poses substantial computational and memory challenges, several efficient finetuning methods have been developed. Among them, low-rank adaptation (LoRA), which finetunes low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective. Nonetheless, LoRA's uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal finetuning performance. To address these limitations, we introduce AutoLoRA, a meta learning based framework for automatically identifying the optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should b
    
[^32]: AI对AI的应用：探索将GPT作为AI出版物专家注释器的实用性

    AI on AI: Exploring the Utility of GPT as an Expert Annotator of AI Publications

    [https://arxiv.org/abs/2403.09097](https://arxiv.org/abs/2403.09097)

    通过使用GPT聊天机器人模型进行提示工程，研究实现了一个自动专家标注管道，该管道以94%的准确率为AI出版物分配标签，并对比展示了SPECTER在相同任务上达到96%的准确率。

    

    arXiv:2403.09097v1 发表类型：新论文 摘要：识别处于动态研究领域内的科学出版物通常需要领域专家进行昂贵的标注。像广泛接受的分类标准或领域分类法这样的资源对于跨越新兴主题和技术的人工智能（AI）这样一个领域是不可用的。我们通过从现有的专家标签中推断出AI研究的功能定义，然后评估最先进的聊天机器人模型在专家数据标注任务上的表现来解决这些挑战。使用arXiv出版物数据库作为基准，我们尝试对GPT聊天机器人模型进行提示工程，以识别一种替代的自动化专家标注流水线，该流水线以94%的精度分配AI标签。为了对比，我们对SPECTER进行了微调，这是一种在科学出版物上预先训练的变压器语言模型，它在分类AI出版物方面实现了96%的准确率（仅比GPT高2%）。我们的结果...

    arXiv:2403.09097v1 Announce Type: new  Abstract: Identifying scientific publications that are within a dynamic field of research often requires costly annotation by subject-matter experts. Resources like widely-accepted classification criteria or field taxonomies are unavailable for a domain like artificial intelligence (AI), which spans emerging topics and technologies. We address these challenges by inferring a functional definition of AI research from existing expert labels, and then evaluating state-of-the-art chatbot models on the task of expert data annotation. Using the arXiv publication database as ground-truth, we experiment with prompt engineering for GPT chatbot models to identify an alternative, automated expert annotation pipeline that assigns AI labels with 94% accuracy. For comparison, we fine-tune SPECTER, a transformer language model pre-trained on scientific publications, that achieves 96% accuracy (only 2% higher than GPT) on classifying AI publications. Our results 
    
[^33]: MCFEND：用于中文假新闻检测的多源基准数据集

    MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection

    [https://arxiv.org/abs/2403.09092](https://arxiv.org/abs/2403.09092)

    MCFEND是第一个用于中文假新闻检测的多源基准数据集，解决了单一来源数据集应用于多源新闻数据时性能下降的问题。

    

    虚假新闻在各个在线来源的普遍传播对公众产生了重要影响。现有的中文假新闻检测数据集仅限于来自微博的新闻。然而，来自多个来源的虚假新闻在内容和社会背景等各个方面表现出多样性。仅在单一新闻来源上训练的方法几乎无法适用于现实场景。我们的初步实验表明，学习自一个大型中文假新闻检测数据集Weibo-21的最先进方法的F1分数，当测试数据改变为多源新闻数据时，从0.943急剧下降到0.470，未能识别超过三分之一的多源虚假新闻。为解决这一限制，我们构建了用于中文假新闻检测的第一个多源基准数据集MCFEND，由我们从各种来源收集的新闻组成。

    arXiv:2403.09092v1 Announce Type: cross  Abstract: The prevalence of fake news across various online sources has had a significant influence on the public. Existing Chinese fake news detection datasets are limited to news sourced solely from Weibo. However, fake news originating from multiple sources exhibits diversity in various aspects, including its content and social context. Methods trained on purely one single news source can hardly be applicable to real-world scenarios. Our pilot experiment demonstrates that the F1 score of the state-of-the-art method that learns from a large Chinese fake news detection dataset, Weibo-21, drops significantly from 0.943 to 0.470 when the test data is changed to multi-source news data, failing to identify more than one-third of the multi-source fake news. To address this limitation, we constructed the first multi-source benchmark dataset for Chinese fake news detection, termed MCFEND, which is composed of news we collected from diverse sources suc
    
[^34]: 有意义学习：通过通用事实引导推进大型语言模型的抽象推理

    Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance

    [https://arxiv.org/abs/2403.09085](https://arxiv.org/abs/2403.09085)

    设计了一个抽象推理数据集和有意义学习范式，教导大型语言模型如何利用通用事实进行推理，有效提升了抽象推理能力。

    

    大型语言模型（LLMs）在各种推理场景中取得了令人印象深刻的性能和强大的可解释性，标志着朝着模拟人类智能迈出了重要的一步。然而，当面对由通用事实支持的简单问题时，LLMs经常未能提供一致和准确的答案，表明其存在抽象推理能力的不足。这引发了关于LLMs到底是在真正推理还是仅仅在记忆的激烈争论。鉴此，我们设计了一个初步研究来量化并深入探讨现有LLMs的抽象推理能力。我们的研究发现显示出它们的一般推理和抽象推理表现之间存在实质性差异。为了缓解这一问题，我们为大型语言模型定制了一个抽象推理数据集（AbsR），结合有意义的学习范式，教会LLMs如何利用通用事实进行推理。结果表明我们的方法能够显着改善LLMs在抽象推理中的表现。

    arXiv:2403.09085v1 Announce Type: cross  Abstract: Large language models (LLMs) have developed impressive performance and strong explainability across various reasoning scenarios, marking a significant stride towards mimicking human-like intelligence. Despite this, when tasked with simple questions supported by a generic fact, LLMs often fail to provide consistent and precise answers, indicating a deficiency in abstract reasoning abilities. This has sparked a vigorous debate about whether LLMs are genuinely reasoning or merely memorizing. In light of this, we design a preliminary study to quantify and delve into the abstract reasoning abilities of existing LLMs. Our findings reveal a substantial discrepancy between their general reasoning and abstract reasoning performances. To relieve this problem, we tailor an abstract reasoning dataset (AbsR) together with a meaningful learning paradigm to teach LLMs how to leverage generic facts for reasoning purposes. The results show that our app
    
[^35]: 信息提取：应用于发展中国家超本地金融数据领域的一项研究

    Information Extraction: An application to the domain of hyper-local financial data on developing countries

    [https://arxiv.org/abs/2403.09077](https://arxiv.org/abs/2403.09077)

    本研究开发并评估了两种基于自然语言处理的技术来提取发展中国家金融数据，其中采用文本到文本的T5模型实现了高准确率、精度和召回率。

    

    尽管发展研究和经济分析需要有关发展中国家公司活动的金融数据，但这样的数据并不存在。在本项目中，我们开发并评估了两种基于自然语言处理（NLP）的技术来解决这一问题。首先，我们整理了一个特定于发展中国家金融文本数据领域的自定义数据集，并探索了多种信息提取方法。然后，我们采用基于变压器的T5模型进行文本到文本的方法，旨在进行同时的命名实体识别和关系提取。我们发现该模型能够学习到定制文本结构输出数据，即实体及其关系，从而在合并任务上我们最佳的T5模型的准确率为92.44％，精度为68.25％，召回率为54.20％。其次，我们探索了一种顺序NER和关系提取方法。对于NER，我们运行了预训练模型

    arXiv:2403.09077v1 Announce Type: new  Abstract: Despite the need for financial data on company activities in developing countries for development research and economic analysis, such data does not exist. In this project, we develop and evaluate two Natural Language Processing (NLP) based techniques to address this issue. First, we curate a custom dataset specific to the domain of financial text data on developing countries and explore multiple approaches for information extraction. We then explore a text-to-text approach with the transformer-based T5 model with the goal of undertaking simultaneous NER and relation extraction. We find that this model is able to learn the custom text structure output data corresponding to the entities and their relations, resulting in an accuracy of 92.44\%, a precision of 68.25\% and a recall of 54.20\% from our best T5 model on the combined task. Secondly, we explore an approach with sequential NER and relation extration. For the NER, we run pre-train
    
[^36]: 大型语言模型是并行多语言学习者

    Large Language Models are Parallel Multilingual Learners

    [https://arxiv.org/abs/2403.09073](https://arxiv.org/abs/2403.09073)

    通过将输入翻译为多种语言，为大型语言模型提供多语言平行输入，显著增强了它们的理解能力，实验证明多语言输入可以超越传统学习方法，并发现了神经元激活的反直觉现象

    

    在这项研究中，我们揭示了多语言大型语言模型（LLMs）的上下文学习（ICL）能力：通过将输入翻译成多种语言，我们为LLMs提供了多语言平行输入（PiM），显著增强了它们的理解能力。为测试这种能力，我们设计了包括8个典型数据集、7种语言和8种最先进的多语言LLMs在内的大量实验证明结果显示，（1）整合更多语言可以帮助PiM进一步超越传统的ICL；（2）即使与基准性能低劣的翻译结合也是有帮助的。此外，通过检查LLMs中激活的神经元，我们发现了一个令人意外但有趣的现象。与常见观点相反，PiM并不会激活比单语输入更多的神经元来利用从多种语言学习到的知识，而实际上是抑制神经元并促进更精确的神经。

    arXiv:2403.09073v1 Announce Type: new  Abstract: In this study, we reveal an in-context learning (ICL) capability of multilingual large language models (LLMs): by translating the input to several languages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which significantly enhances their comprehension abilities. To test this capability, we design extensive experiments encompassing 8 typical datasets, 7 languages and 8 state-of-the-art multilingual LLMs. Experimental results show that (1) incorporating more languages help PiM surpass the conventional ICL further; (2) even combining with the translations that are inferior to baseline performance can also help. Moreover, by examining the activated neurons in LLMs, we discover a counterintuitive but interesting phenomenon. Contrary to the common thought that PiM would activate more neurons than monolingual input to leverage knowledge learned from diverse languages, PiM actually inhibits neurons and promotes more precise neu
    
[^37]: UniCode: 学习用于多模大语言模型的统一码书

    UniCode: Learning a Unified Codebook for Multimodal Large Language Models

    [https://arxiv.org/abs/2403.09072](https://arxiv.org/abs/2403.09072)

    UniCode提出一种学习统一码书的方法，解决多模大语言模型中对视觉和文本进行标记的关键问题，使模型能够生成高质量的图像，并可适应各种压缩方法。

    

    在本文中，我们提出了一种名为UniCode的新方法，该方法属于多模大语言模型（MLLMs）领域，它学习了一个统一的码书来高效地标记视觉、文本和潜在其他类型的信号。这一创新解决了现有MLLMs存在的一个关键局限：它们依赖于仅限于文本的码书，这限制了MLLM在多模态环境中生成图像和文本的能力。为此，我们提出了一种以语言驱动的迭代训练范式，结合我们称之为“图像解压缩”的上下文预训练任务，使我们的模型能够解释压缩的视觉数据并生成高质量的图像。统一的码书使我们的模型能够将视觉指令调整扩展到非语言生成任务。此外，UniCode适应了各种叠加量化方法，以将视觉信号压缩为更紧凑的标记表示。

    arXiv:2403.09072v1 Announce Type: cross  Abstract: In this paper, we propose \textbf{UniCode}, a novel approach within the domain of multimodal large language models (MLLMs) that learns a unified codebook to efficiently tokenize visual, text, and potentially other types of signals. This innovation addresses a critical limitation in existing MLLMs: their reliance on a text-only codebook, which restricts MLLM's ability to generate images and texts in a multimodal context. Towards this end, we propose a language-driven iterative training paradigm, coupled with an in-context pre-training task we term ``image decompression'', enabling our model to interpret compressed visual data and generate high-quality images.The unified codebook empowers our model to extend visual instruction tuning to non-linguistic generation tasks. Moreover, UniCode is adaptable to diverse stacked quantization approaches in order to compress visual signals into a more compact token representation. Despite using signi
    
[^38]: LAMP：地图上的语言模型

    LAMP: A Language Model on the Map

    [https://arxiv.org/abs/2403.09059](https://arxiv.org/abs/2403.09059)

    该研究引入了一个新颖的框架，用于在城市特定数据上微调预训练模型，使其能够为人们提供准确的推荐，同时最小化幻觉。

    

    大型语言模型（LLMs）在我们的生活中扮演着越来越重要的角色，为我们在各种任务中提供帮助。在地理空间领域，LLMs已经展示出能够回答一般性问题的能力，比如识别一个国家的首都；然而，当涉及回答关于特定地点的细粒度问题时，比如杂货店或餐馆，这些构成了人们日常生活中重要的方面时，它们的效用受到阻碍。这主要是因为我们城市中的地点尚未被系统地输入到LLMs中，以便于理解和记忆它们。该研究引入了一个新颖的框架，用于在城市特定数据上微调预训练模型，从而使其能够提供准确的建议，同时最小化幻觉。我们分享我们的模型LAMP和用于训练它的数据。我们进行实验分析其正确检索空间对象的能力。

    arXiv:2403.09059v1 Announce Type: new  Abstract: Large Language Models (LLMs) are poised to play an increasingly important role in our lives, providing assistance across a wide array of tasks. In the geospatial domain, LLMs have demonstrated the ability to answer generic questions, such as identifying a country's capital; nonetheless, their utility is hindered when it comes to answering fine-grained questions about specific places, such as grocery stores or restaurants, which constitute essential aspects of people's everyday lives. This is mainly because the places in our cities haven't been systematically fed into LLMs, so as to understand and memorize them. This study introduces a novel framework for fine-tuning a pre-trained model on city-specific data, to enable it to provide accurate recommendations, while minimizing hallucinations. We share our model, LAMP, and the data used to train it. We conduct experiments to analyze its ability to correctly retrieving spatial objects, and co
    
[^39]: 一种用于自动生成医疗记录的持续预训练LLM方法

    A Continued Pretrained LLM Approach for Automatic Medical Note Generation

    [https://arxiv.org/abs/2403.09057](https://arxiv.org/abs/2403.09057)

    这项研究提出了一种用于医疗记录生成的持续预训练LLM方法，在PubMedQA方面性能优于GPT-4，能够更好地捕捉正确的医疗概念，并且在正确性和完整性方面超过人类抄写员。

    

    LLM（大型语言模型）正在革新自然语言处理任务。然而，像GPT-4这样的最强大的LLM对于大多数领域特定场景来说成本太高。我们提出了第一个连续训练的130亿参数 Llama2-basd LLM，专为医疗对话而设计，并在自动记录上进行了测试。我们的结果显示，我们的模型在PubMedQA中的准确率高达76.6％，在总结医疗对话为SOAP笔记方面与GPT-4的性能相当。值得注意的是，我们的模型在捕捉正确的医疗概念方面超过了GPT-4，并且在正确性和完整性方面超越了人类抄写员。

    arXiv:2403.09057v1 Announce Type: cross  Abstract: LLMs are revolutionizing NLP tasks. However, the most powerful LLM, like GPT-4, is too costly for most domain-specific scenarios. We present the first continuously trained 13B Llama2-based LLM that is purpose-built for medical conversations and measured on automated scribing. Our results show that our model outperforms GPT-4 in PubMedQA with 76.6\% accuracy and matches its performance in summarizing medical conversations into SOAP notes. Notably, our model exceeds GPT-4 in capturing a higher number of correct medical concepts and outperforms human scribes with higher correctness and completeness.
    
[^40]: RAGGED:朝着基于检索增强生成系统的知情设计

    RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems

    [https://arxiv.org/abs/2403.09040](https://arxiv.org/abs/2403.09040)

    RAGGED框架分析和优化了检索增强生成系统，揭示了不同模型适合不同RAG设置的事实，编码器-解码器模型随文档数量增加而改善，而仅解码器模型只能有效利用少量文档。

    

    arXiv:2403.09040v1 声明类型: 新的 摘要: 检索增强生成（RAG）通过为文档型问答等任务提供附加上下文，极大地提升了语言模型（LMs）的性能。尽管具有潜力，但RAG的效力高度依赖于其配置，从而引发一个问题：什么是最佳RAG配置？为了回答这个问题，我们引入了RAGGED框架来分析和优化RAG系统。在一组代表性的文档型问答任务上，我们研究了两种经典的稀疏和密集检索器，以及四种在编码器-解码器和仅解码器结构中表现优异的LMs。通过RAGGED，我们发现不同模型适合完全不同的RAG设置。虽然编码器-解码器模型随着更多文档的增加而单调提升，但我们发现仅解码器模型只能有效地使用<5个文档，尽管通常具有更长的上下文窗口。RAGGED进一步揭示了LMs的上下文利用习惯，我们发现编码器-解码器模型...

    arXiv:2403.09040v1 Announce Type: new  Abstract: Retrieval-augmented generation (RAG) greatly benefits language models (LMs) by providing additional context for tasks such as document-based question answering (DBQA). Despite its potential, the power of RAG is highly dependent on its configuration, raising the question: What is the optimal RAG configuration? To answer this, we introduce the RAGGED framework to analyze and optimize RAG systems. On a set of representative DBQA tasks, we study two classic sparse and dense retrievers, and four top-performing LMs in encoder-decoder and decoder-only architectures. Through RAGGED, we uncover that different models suit substantially varied RAG setups. While encoder-decoder models monotonically improve with more documents, we find decoder-only models can only effectively use < 5 documents, despite often having a longer context window. RAGGED offers further insights into LMs' context utilization habits, where we find that encoder-decoder models r
    
[^41]: 第一个知道：令牌分布如何揭示大型视觉语言模型中的隐藏知识？

    The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?

    [https://arxiv.org/abs/2403.09037](https://arxiv.org/abs/2403.09037)

    本研究利用线性探测揭示了大型视觉语言模型的隐藏知识，发现首个令牌的logit分布包含足够信息，可以识别无法回答的视觉问题、防范多模态越狱攻击以及识别欺骗性问题，并提出了一个简单的解码策略以有效改善生成内容。

    

    大型视觉语言模型（LVLMs）旨在解释和响应人类指令，但由于不当指令而偶尔生成幻觉或有害内容。本研究使用线性探测来揭示LVLMs输出层的隐藏知识。我们证明了首个令牌的logit分布包含足够信息，可以确定是否应对指令作出响应，包括识别无法回答的视觉问题、防范多模态越狱攻击以及识别欺骗性问题。这种隐藏知识在响应生成过程中随后令牌的logit逐渐丢失。然后，我们演示了一种简单的解码策略在生成第一个令牌时，有效改善生成的内容。在实验中，我们发现了一些有趣的见解：首先，CLIP模型已经包含解决这些任务的强烈信号，表明潜力

    arXiv:2403.09037v1 Announce Type: cross  Abstract: Large vision-language models (LVLMs), designed to interpret and respond to human instructions, occasionally generate hallucinated or harmful content due to inappropriate instructions. This study uses linear probing to shed light on the hidden knowledge at the output layer of LVLMs. We demonstrate that the logit distributions of the first tokens contain sufficient information to determine whether to respond to the instructions, including recognizing unanswerable visual questions, defending against multi-modal jailbreaking attack, and identifying deceptive questions. Such hidden knowledge is gradually lost in logits of subsequent tokens during response generation. Then, we illustrate a simple decoding strategy at the generation of the first token, effectively improving the generated content. In experiments, we find a few interesting insights: First, the CLIP model already contains a strong signal for solving these tasks, indicating poten
    
[^42]: CodeUltraFeedback：一种用于将大型语言模型与编程偏好对齐的LLM作为法官数据集

    CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences

    [https://arxiv.org/abs/2403.09032](https://arxiv.org/abs/2403.09032)

    介绍了 CodeUltraFeedback 数据集，通过 AI 反馈使 14 种不同的 LLMs 对 10,000 个复杂指令生成响应，并使用 LLM-as-a-Judge 方法评估它们与五种编程偏好的对齐情况，同时提出了用于评估 LLM 对编程偏好对齐的基准 CODAL-Bench。

    

    评估大型语言模型（LLMs）与用户定义的编程偏好的对齐性是一项具有挑战性的工作，需要评估复杂文本LLMs的输出。现有基准仰赖自动化指标和静态分析工具，未能评估用户指令和LLM输出中的微妙之处，突显了对LLM偏好对齐的大规模数据集和基准的需求。在本文中，我们介绍了CodeUltraFeedback，一个包含10,000个复杂指令的偏好数据集，通过AI反馈来调整和对齐LLMs与编程偏好。我们使用14种不同的LLMs对这些指令生成响应，然后根据它们与五种编程偏好的对齐情况进行注释，使用GPT-3.5的LLM作为法官方法产生数字和文本反馈。我们还提出了CODAL-Bench，一个用于评估LLM与这些编程偏好对齐的基准。我们的结果显示C

    arXiv:2403.09032v1 Announce Type: cross  Abstract: Evaluating the alignment of large language models (LLMs) with user-defined coding preferences is a challenging endeavour that requires assessing intricate textual LLMs' outputs. By relying on automated metrics and static analysis tools, existing benchmarks fail to assess nuances in user instructions and LLM outputs, highlighting the need for large-scale datasets and benchmarks for LLM preference alignment. In this paper, we introduce CodeUltraFeedback, a preference dataset of 10,000 complex instructions to tune and align LLMs to coding preferences through AI feedback. We generate responses to the instructions using a pool of 14 diverse LLMs, which we then annotate according to their alignment with five coding preferences using the LLM-as-a-Judge approach with GPT-3.5, producing both numerical and textual feedback. We also present CODAL-Bench, a benchmark for assessing LLM alignment with these coding preferences. Our results show that C
    
[^43]: ChartInstruct：用于图表理解和推理的指令调整

    ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning

    [https://arxiv.org/abs/2403.09028](https://arxiv.org/abs/2403.09028)

    引入了ChartInstruct数据集和两种指令调整系统，解决了常规模型无法解决各种与图表相关任务的问题

    

    图表提供数据的可视化表示，被广泛用于分析信息，解决问题，并向他人传达见解。最近出现了各种与图表相关的下游任务，例如问答和摘要。解决这些任务的常见策略是微调最初在视觉任务上训练的各种模型。然而，这种任务特定的模型无法解决广泛的与图表相关的任务，限制了它们在现实世界中的适用性。为了克服这些挑战，我们引入了ChartInstruct：一个新颖的图表特定的视觉-语言指令遵循数据集，包括191K个指令和71K张图表。然后，我们提出了两种不同的用于在该数据集上进行指令调整的系统：（1）连接用于图表理解的视觉编码器和LLM的端到端模型；和（2）采用两步方法来提取图表数据表格的流水线模型。

    arXiv:2403.09028v1 Announce Type: new  Abstract: Charts provide visual representations of data and are widely used for analyzing information, addressing queries, and conveying insights to others. Various chart-related downstream tasks have emerged recently, such as question-answering and summarization. A common strategy to solve these tasks is to fine-tune various models originally trained on vision tasks language. However, such task-specific models are not capable of solving a wide range of chart-related tasks, constraining their real-world applicability. To overcome these challenges, we introduce ChartInstruct: a novel chart-specific vision-language Instruction-following dataset comprising 191K instructions generated with 71K charts. We then present two distinct systems for instruction tuning on such datasets: (1) an end-to-end model that connects a vision encoder for chart understanding with a LLM; and (2) a pipeline model that employs a two-step approach to extract chart data table
    
[^44]: 半参数令牌序列共监督

    Semiparametric Token-Sequence Co-Supervision

    [https://arxiv.org/abs/2403.09024](https://arxiv.org/abs/2403.09024)

    引入了一种半参数令牌序列共监督训练方法，通过同时利用传统的下一个令牌预测损失和下一个序列预测损失来训练语言模型，实验结果显示这种方法能够提高模型的泛化能力。

    

    在这项工作中，我们引入了一种半参数令牌序列共监督训练方法。该方法通过同时利用传统的基于参数化令牌嵌入空间计算的下一个令牌预测损失和基于非参数化序列嵌入空间计算的下一个序列预测损失来训练语言模型。非参数序列嵌入空间是由一个单独的语言模型构建的，其任务是将输入文本压缩成一个单一的代表性嵌入。我们的实验表明，通过这两种监督训练的模型始终优于单独通过每种监督训练的模型。分析表明，这种共监督鼓励模型具有更广泛的泛化能力。特别是，在预训练步骤中建立的参数化标记空间的鲁棒性倾向于有效增强非参数化的稳定性。

    arXiv:2403.09024v1 Announce Type: cross  Abstract: In this work, we introduce a semiparametric token-sequence co-supervision training method. It trains a language model by simultaneously leveraging supervision from the traditional next token prediction loss which is calculated over the parametric token embedding space and the next sequence prediction loss which is calculated over the nonparametric sequence embedding space. The nonparametric sequence embedding space is constructed by a separate language model tasked to condense an input text into a single representative embedding. Our experiments demonstrate that a model trained via both supervisions consistently surpasses models trained via each supervision independently. Analysis suggests that this co-supervision encourages a broader generalization capability across the model. Especially, the robustness of parametric token space which is established during the pretraining step tends to effectively enhance the stability of nonparametri
    
[^45]: AraTrust：阿拉伯语大型语言模型信誉评估

    AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic

    [https://arxiv.org/abs/2403.09017](https://arxiv.org/abs/2403.09017)

    AraTrust是第一个阿拉伯语大型语言模型的全面信誉基准，解决了缺乏全面信誉评估基准的问题，帮助准确评估和提高LLMs的安全性。

    

    arXiv:2403.09017v1 公告类型：新摘要：人工智能系统的迅速发展和广泛接受凸显了理解人工智能的能力和潜在风险的迫切需要。鉴于阿拉伯语在人工智能研究中的语言复杂性、文化丰富性和地位不高，有必要专注于阿拉伯语相关任务的大型语言模型（LLMs）的性能和安全性。尽管它们的发展取得了一些进展，但缺乏全面的信誉评估基准是准确评估和提高在阿拉伯语提示时LLMs的安全性面临的主要挑战。本文介绍了AraTrust 1，这是第一个针对阿拉伯语大型语言模型的全面的信誉基准。AraTrust 包含了516个人工编写的多项选择题，涉及与真实性、道德、安全性、身体健康、心理健康、不公平行为、非法活动相关的多个维度。

    arXiv:2403.09017v1 Announce Type: new  Abstract: The swift progress and widespread acceptance of artificial intelligence (AI) systems highlight a pressing requirement to comprehend both the capabilities and potential risks associated with AI. Given the linguistic complexity, cultural richness, and underrepresented status of Arabic in AI research, there is a pressing need to focus on Large Language Models (LLMs) performance and safety for Arabic related tasks. Despite some progress in their development, there is a lack of comprehensive trustworthiness evaluation benchmarks which presents a major challenge in accurately assessing and improving the safety of LLMs when prompted in Arabic. In this paper, we introduce AraTrust 1, the first comprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises 516 human-written multiple-choice questions addressing diverse dimensions related to truthfulness, ethics, safety, physical health, mental health, unfairness, illegal activities
    
[^46]: Ethos：在正交参数空间中矫正语言模型

    Ethos: Rectifying Language Models in Orthogonal Parameter Space

    [https://arxiv.org/abs/2403.08994](https://arxiv.org/abs/2403.08994)

    Ethos提出了一种新的高效方法，通过在任务向量上进行矫正LMs以减轻产生毒性和偏见输出以及避免隐私泄露。

    

    语言模型（LMs）极大推动了自然语言处理研究的发展。然而，LMs也引发了关于生成偏见或有毒内容以及训练数据集中私人信息可能泄露的担忧。在这项工作中，我们提出了一种新的高效方法，Ethos，通过在任务向量上进行矫正LMs以减轻产生毒性和偏见输出以及避免隐私泄露。Ethos建立在任务算术基础上。然而，与当前的任务算法不同的是，Ethos在重构任务向量时区分了一般有益和不良知识。具体而言，Ethos首先使用奇异值分解从预训练模型中获得一组主成分。然后，通过将任务向量投影到主成分上，Ethos识别编码一般或不良知识的主成分。Ethos仅使用带有不良知识的任务向量进行否定，从而最小

    arXiv:2403.08994v1 Announce Type: new  Abstract: Language models (LMs) have greatly propelled the research on natural language processing. However, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training dataset. In this work, we present a new efficient approach, Ethos, that rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy leakage. Ethos is built on task arithmetic. However, unlike current task arithmetic algorithms, Ethos distinguishes general beneficial and undesired knowledge when reconstructing task vectors. Specifically, Ethos first obtains a set of principal components from the pre-trained models using singular value decomposition. Then, by projecting the task vector onto principal components, Ethos identifies the principal components that encode general or undesired knowledge. Ethos performs negating using the task vector with undesired knowledge only, thereby minimi
    
[^47]: AutoGuide: 大型语言模型代理的自动生成和选择状态感知指南

    AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents

    [https://arxiv.org/abs/2403.08978](https://arxiv.org/abs/2403.08978)

    AutoGuide通过提取嵌入在离线数据中的知识，生成一组状态感知指南，从而弥合大型语言模型中的知识差距，为代理的决策过程提供有用的知识。

    

    大型语言模型（LLMs）的主要局限性是它们对世界的理解受限。这给基于LLMs的代理带来了重大困难，特别是在预训练的LLMs缺乏足够知识的领域。在本文中，我们介绍了一个名为AutoGuide的新框架，通过利用离线经验中的隐含知识来弥合预训练LLMs中的知识差距。具体而言，AutoGuide通过提取一组状态感知指南有效地提取嵌入在离线数据中的知识。每个状态感知指南以简洁的自然语言表达，并遵循条件结构，清晰描述适用的状态。因此，由此产生的指南为向代理当前的决策过程提供有用的知识提供了一种原则性的方法。我们展示了我们的方法在顺序任务中大幅领先于竞争的基于LLMs的基线。

    arXiv:2403.08978v1 Announce Type: new  Abstract: The primary limitation of large language models (LLMs) is their restricted understanding of the world. This poses significant difficulties for LLM-based agents, particularly in domains where pre-trained LLMs lack sufficient knowledge. In this paper, we introduce a novel framework, called AutoGuide, that bridges the knowledge gap in pre-trained LLMs by leveraging implicit knowledge in offline experiences. Specifically, AutoGuide effectively extracts knowledge embedded in offline data by extracting a set of state-aware guidelines. Importantly, each state-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the state where it is applicable. As such, the resulting guidelines enable a principled way to provide helpful knowledge pertinent to an agent's current decision-making process. We show that our approach outperforms competitive LLM-based baselines by a large margin in sequential
    
[^48]: 可用的XAI：在LLM时代利用可解释性的10个策略

    Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era

    [https://arxiv.org/abs/2403.08946](https://arxiv.org/abs/2403.08946)

    在大型语言模型时代，为了适应其复杂性和先进能力，我们引入了可用的XAI概念，通过积极增强LLMs在实际环境中的生产力和适用性，实现XAI方法论的重大转变。

    

    可解释人工智能（XAI）指的是提供人类可理解的洞见，揭示人工智能模型的运作方式的技术。最近，XAI的重点正被扩展到常常因为不透明而备受批评的大型语言模型（LLMs）。这一拓展需要对XAI方法论进行显著转变，因为有两个原因。首先，许多现有的XAI方法无法直接应用于LLMs，因为它们的复杂性和先进能力。其次，随着LLMs越来越广泛地应用于不同行业应用中，XAI的角色从仅仅打开“黑匣子”转变为积极增强LLMs在实际环境中的生产力和适用性。与此同时，不同于传统机器学习模型仅作为XAI洞见的被动接受者，LLMs的独特能力能够相互增强XAI。因此，在本文中，我们通过分析（1）...

    arXiv:2403.08946v1 Announce Type: cross  Abstract: Explainable AI (XAI) refers to techniques that provide human-understandable insights into the workings of AI models. Recently, the focus of XAI is being extended towards Large Language Models (LLMs) which are often criticized for their lack of transparency. This extension calls for a significant transformation in XAI methodologies because of two reasons. First, many existing XAI methods cannot be directly applied to LLMs due to their complexity advanced capabilities. Second, as LLMs are increasingly deployed across diverse industry applications, the role of XAI shifts from merely opening the "black box" to actively enhancing the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike traditional machine learning models that are passive recipients of XAI insights, the distinct abilities of LLMs can reciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in the context of LLMs by analyzing (1)
    
[^49]: LMStyle基准：评估用于聊天机器人的文本风格转移

    LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots

    [https://arxiv.org/abs/2403.08943](https://arxiv.org/abs/2403.08943)

    该论文提出了LMStyle基准，针对聊天机器人的文本风格转移进行评估，不仅可以自动化和可扩展地衡量LLMs的风格转移质量，还考虑了适当性这一新颖度量方面。

    

    自ChatGPT突破以来，大型语言模型(LLMs)在研究界引起了极大关注。随着LLMs的发展，文本风格转移对话模型的问题已成为自然延伸，其中聊天机器人可能拥有自己的风格甚至特色。然而，针对这种新设置尚未建立标准评估指标。本文旨在通过提出LMStyle基准来解决这一问题，这是一个新颖的评估框架，适用于聊天风格文本风格转移(C-TST)，可自动化和可扩展地衡量LLMs的风格转移质量。除了传统的风格强度指标外，LMStyle基准还考虑了一个称为适当性的新颖度量方面，一个高水平指标，考虑了连贯性、流畅性和其他隐含因素，无需参考样本的帮助。我们的实验表明

    arXiv:2403.08943v1 Announce Type: new  Abstract: Since the breakthrough of ChatGPT, large language models (LLMs) have garnered significant attention in the research community. With the development of LLMs, the question of text style transfer for conversational models has emerged as a natural extension, where chatbots may possess their own styles or even characters. However, standard evaluation metrics have not yet been established for this new settings. This paper aims to address this issue by proposing the LMStyle Benchmark, a novel evaluation framework applicable to chat-style text style transfer (C-TST), that can measure the quality of style transfer for LLMs in an automated and scalable manner. In addition to conventional style strength metrics, LMStyle Benchmark further considers a novel aspect of metrics called appropriateness, a high-level metrics take account of coherence, fluency and other implicit factors without the aid of reference samples. Our experiments demonstrate that 
    
[^50]: 在争议性话题中检测检索增强生成中的虚构和覆盖错误

    Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics

    [https://arxiv.org/abs/2403.08904](https://arxiv.org/abs/2403.08904)

    提出了一种检测LLM在生成文本过程中虚构和覆盖错误的方法，通过基于词重叠、显著性和分类器的三种策略，即使在合成错误上训练，也能实现高错误检测性能，表现出快速和有效的能力。

    

    我们探讨了一种处理基于LLM的聊天机器人中争议性话题的策略，该策略基于维基百科的中立观点（NPOV）原则：承认不存在一个真实答案，并呈现多个观点。我们将此称为检索增强生成，在此方法中，从知识库中检索观点，然后LLM负责从给定的观点生成流畅且忠实的响应。我们首先使用确定性检索系统，然后专注于在这种文本生成方法中出现的常见LLM失败模式，即虚构和覆盖错误。我们提出并评估了三种基于（1）词重叠，（2）显著性和（3）基于LLM的分类器的方法来检测此类错误。我们的结果表明，基于LLM的分类器，即使只在合成错误上进行训练，也能实现高错误检测性能，虚构错误的ROC AUC分数为95.3%，覆盖错误的ROC AUC分数为90.5%。

    arXiv:2403.08904v1 Announce Type: new  Abstract: We explore a strategy to handle controversial topics in LLM-based chatbots based on Wikipedia's Neutral Point of View (NPOV) principle: acknowledge the absence of a single true answer and surface multiple perspectives. We frame this as retrieval augmented generation, where perspectives are retrieved from a knowledge base and the LLM is tasked with generating a fluent and faithful response from the given perspectives. As a starting point, we use a deterministic retrieval system and then focus on common LLM failure modes that arise during this approach to text generation, namely hallucination and coverage errors. We propose and evaluate three methods to detect such errors based on (1) word-overlap, (2) salience, and (3) LLM-based classifiers. Our results demonstrate that LLM-based classifiers, even when trained only on synthetic errors, achieve high error detection performance, with ROC AUC scores of 95.3% for hallucination and 90.5% for c
    
[^51]: 从“嗯”到“是的”：人类对话中信息流的生成、预测和调节

    From "um" to "yeah": Producing, predicting, and regulating information flow in human conversation

    [https://arxiv.org/abs/2403.08890](https://arxiv.org/abs/2403.08890)

    本文使用大型语言模型研究了英语对话中信息流的生成、预测和调节，揭示了信息密度以及检索和呈现信息的认知负荷对对话的显著影响，同时发现了backchannels在调节新颖性产生过程中的作用。

    

    对话需要注意力。说话者必须召唤单词，听者必须理解它们，两者必须共同协商这些信息流，所有这些都在几分之一秒内完成。我们使用大型语言模型研究了英语对话大规模数据集CANDOR语料库中的工作方式。我们提供了一种对非结构化对话信息密度的新估算，约为每秒13位，并发现检索和呈现信息的认知负荷与之相关的显著效应。我们还揭示了backchannels的作用-听众提供的简短的“是的”、“嗯嗯”和“嗯”的作用在调节新颖性的产生：导致backchannel的阶段与信息速率的下降相关，而随后的言语恢复到先前的速率。我们的结果为长期以来关于我们如何应对认知资源波动需求的理论提供了新的见解。

    arXiv:2403.08890v1 Announce Type: new  Abstract: Conversation demands attention. Speakers must call words to mind, listeners must make sense of them, and both together must negotiate this flow of information, all in fractions of a second. We used large language models to study how this works in a large-scale dataset of English-language conversation, the CANDOR corpus. We provide a new estimate of the information density of unstructured conversation, of approximately 13 bits/second, and find significant effects associated with the cognitive load of both retrieving, and presenting, that information. We also reveal a role for backchannels -- the brief yeahs, uh-huhs, and mhmms that listeners provide -- in regulating the production of novelty: the lead-up to a backchannel is associated with declining information rate, while speech downstream rebounds to previous rates. Our results provide new insights into long-standing theories of how we respond to fluctuating demands on cognitive resourc
    
[^52]: PAPERCLIP：使用多模态模型将天文观测和自然语言关联起来

    PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models

    [https://arxiv.org/abs/2403.08851](https://arxiv.org/abs/2403.08851)

    该研究提出了PAPERCLIP方法，通过将天文观测与自然语言关联起来，利用预训练的CLIP模型进行微调，实现了观测和自然语言之间的有意义的联合表示。

    

    我们提出了PAPERCLIP（Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training），一种使用神经网络模型将由望远镜成像的天文观测与自然语言关联起来的方法。该模型是从经过预训练的对比语言-图像预训练（CLIP）模型微调而来，使用成功的观测提案摘要和相应的下游观测，其中摘要可选择通过使用大型语言模型（LLMs）进行引导生成来进行总结。以哈勃空间望远镜（HST）的观测为例，我们展示了微调的模型通过针对图像检索（即使用自然语言查询找到最相关的观测）和描述检索（即查询与天文物体类别和用例最相关的内容）的测试，体现了观测和自然语言之间的有意义的联合表示。

    arXiv:2403.08851v1 Announce Type: cross  Abstract: We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training), a method which associates astronomical observations imaged by telescopes with natural language using a neural network model. The model is fine-tuned from a pre-trained Contrastive Language-Image Pre-training (CLIP) model using successful observing proposal abstracts and corresponding downstream observations, with the abstracts optionally summarized via guided generation using large language models (LLMs). Using observations from the Hubble Space Telescope (HST) as an example, we show that the fine-tuned model embodies a meaningful joint representation between observations and natural language through tests targeting image retrieval (i.e., finding the most relevant observations using natural language queries) and description retrieval (i.e., querying for astrophysical object classes and use cases most relevant to a 
    
[^53]: LoRA-SP：用于资源高效微调大型语言模型的简化部分参数适应

    LoRA-SP: Streamlined Partial Parameter Adaptation for Resource-Efficient Fine-Tuning of Large Language Models

    [https://arxiv.org/abs/2403.08822](https://arxiv.org/abs/2403.08822)

    LoRA-SP利用随机半选择参数冻结的新颖方法，在微调大型语言模型时有效平衡预训练知识的保留和任务特定优化的适应性，显著降低了计算和内存需求，同时实现了竞争性性能。

    

    在解决大型语言模型（LLMs）微调的计算和内存需求方面，我们提出了 LoRA-SP（简化部分参数适应），这是一种新颖的方法，利用低秩适应（LoRA）框架内的随机半选择参数冻结。该方法有效地平衡了预训练知识的保留和任务特定优化的适应性。通过随机机制，LoRA-SP 确定要更新或冻结哪些参数，显著降低了计算和内存需求，而不会影响模型性能。我们在几个基准 NLP 任务中评估了 LoRA-SP，展示了它与传统的全参数微调和其他参数高效技术相比，能够以大大降低的资源消耗实现竞争性性能。LoRA-SP 的创新方法不仅有助于在资源有限的情况下部署先进的 NLP 模型。

    arXiv:2403.08822v1 Announce Type: cross  Abstract: In addressing the computational and memory demands of fine-tuning Large Language Models(LLMs), we propose LoRA-SP(Streamlined Partial Parameter Adaptation), a novel approach utilizing randomized half-selective parameter freezing within the Low-Rank Adaptation(LoRA)framework. This method efficiently balances pre-trained knowledge retention and adaptability for task-specific optimizations. Through a randomized mechanism, LoRA-SP determines which parameters to update or freeze, significantly reducing computational and memory requirements without compromising model performance. We evaluated LoRA-SP across several benchmark NLP tasks, demonstrating its ability to achieve competitive performance with substantially lower resource consumption compared to traditional full-parameter fine-tuning and other parameter-efficient techniques. LoRA-SP innovative approach not only facilitates the deployment of advanced NLP models in resource-limited sett
    
[^54]: 温度计：面向大型语言模型的通用校准

    Thermometer: Towards Universal Calibration for Large Language Models

    [https://arxiv.org/abs/2403.08819](https://arxiv.org/abs/2403.08819)

    提出了一种针对大型语言模型的校准方法THERMOMETER，通过学习来自多个任务数据的辅助模型，实现了计算效率高、准确性保持并产生更好校准响应的目标。

    

    我们考虑大型语言模型（LLM）中的校准问题。最近的研究发现，常见的干预措施如指令调整通常会导致校准不佳的LLMs。尽管校准在传统应用中得到了很好的探讨，但对LLMs进行校准具有独特挑战。这些挑战不仅来自LLMs的严格计算要求，也来自它们的多功能性，使它们可以应用于各种任务。为了解决这些挑战，我们提出了一个针对LLMs的校准方法THERMOMETER。THERMOMETER通过学习来自多个任务的数据的辅助模型，用于校准LLM。它在计算上效率高，保持了LLM的准确性，并为新任务产生了更好的校准响应。对各种基准的广泛实证评估显示了所提方法的有效性。

    arXiv:2403.08819v1 Announce Type: cross  Abstract: We consider the issue of calibration in large language models (LLM). Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs. Although calibration is well-explored in traditional applications, calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks. Addressing these challenges, we propose THERMOMETER, a calibration approach tailored to LLMs. THERMOMETER learns an auxiliary model, given data from multiple tasks, for calibrating a LLM. It is computationally efficient, preserves the accuracy of the LLM, and produces better-calibrated responses for new tasks. Extensive empirical evaluations across various benchmarks demonstrate the effectiveness of the proposed method.
    
[^55]: 结构和语义的电子健康记录多模态融合：将临床记录和笔记与超图和LLM集成

    Multimodal Fusion of EHR in Structures and Semantics: Integrating Clinical Records and Notes with Hypergraph and LLM

    [https://arxiv.org/abs/2403.08818](https://arxiv.org/abs/2403.08818)

    提出了一个名为MINGLE的新框架，通过两级注入策略将医学概念语义和临床笔记语义融合到超图中，有效地整合了结构和语义的电子健康记录数据。

    

    电子健康记录（EHRs）在近几十年来越来越受欢迎，支持临床决策和医疗保健。EHRs通常包含异构信息，如表格形式的结构化数据和文本笔记中的非结构化数据。EHRs中的不同类型信息可以相互补充，提供患者健康状态的更完整图片。尽管对结构化EHR数据的表示学习进行了大量研究，但不同类型EHR数据的融合（多模态融合）尚未得到充分研究。这主要是由于医疗编码系统的复杂性和书面笔记中存在的噪音和冗余。在这项工作中，我们提出了一个名为MINGLE的新框架，有效地将EHR中的结构和语义结合起来。我们的框架使用两级注入策略将医学概念语义和临床笔记语义融合到超图中。

    arXiv:2403.08818v1 Announce Type: cross  Abstract: Electronic Health Records (EHRs) have become increasingly popular to support clinical decision-making and healthcare in recent decades. EHRs usually contain heterogeneous information, such as structural data in tabular form and unstructured data in textual notes. Different types of information in EHRs can complement each other and provide a more complete picture of the health status of a patient. While there has been a lot of research on representation learning of structured EHR data, the fusion of different types of EHR data (multimodal fusion) is not well studied. This is mostly because of the complex medical coding systems used and the noise and redundancy present in the written notes. In this work, we propose a new framework called MINGLE, which integrates both structures and semantics in EHR effectively. Our framework uses a two-level infusion strategy to combine medical concept semantics and clinical note semantics into hypergrap
    
[^56]: 用人工智能结合UFO本体论监测认知衰退导致的信息处理缺陷，避免在教育环境中出现的心理和身体攻击

    Ontologia para monitorar a defici\^encia mental em seus d\'eficts no processamento da informa\c{c}\~ao por decl\'inio cognitivo e evitar agress\~oes psicol\'ogicas e f\'isicas em ambientes educacionais com ajuda da I.A*

    [https://arxiv.org/abs/2403.08795](https://arxiv.org/abs/2403.08795)

    本文提出了利用人工智能结合UFO本体论来监测认知衰退导致的信息处理缺陷，预防教育环境中出现的心理和身体攻击。

    

    本文旨在提出利用人工智能通过UFO本体论分析来检测与心理社会缺陷及其诱发因素相关的言语和身体攻击的出现，以防止校园环境内出现灾难性后果。

    arXiv:2403.08795v1 Announce Type: cross  Abstract: The intention of this article is to propose the use of artificial intelligence to detect through analysis by UFO ontology the emergence of verbal and physical aggression related to psychosocial deficiencies and their provoking agents, in an attempt to prevent catastrophic consequences within school environments.
    
[^57]: 使用合成多模态虚假信息检测的图像文本脱离语境方法

    Image-Text Out-Of-Context Detection Using Synthetic Multimodal Misinformation

    [https://arxiv.org/abs/2403.08783](https://arxiv.org/abs/2403.08783)

    该研究提出了一种使用合成数据生成的脱离语境检测方法，实验证实了合成数据生成在解决OOCD相关数据限制方面的有效性。

    

    在数字信息不断增加的时代，虚假信息已经成为一个重大挑战，需要开发有效的检测方法。我们研究了一种新颖的脱离语境检测（OOCD）方法，该方法使用了合成数据生成。我们创建了一个专门用于OOCD的数据集，并开发了一个高效的检测器进行准确分类。我们的实验发现验证了合成数据生成的使用，并展示了其在解决与OOCD相关的数据限制方面的有效性。该数据集和检测器应成为未来研究和开发健壮虚假信息检测系统的宝贵资源。

    arXiv:2403.08783v1 Announce Type: cross  Abstract: Misinformation has become a major challenge in the era of increasing digital information, requiring the development of effective detection methods. We have investigated a novel approach to Out-Of-Context detection (OOCD) that uses synthetic data generation. We created a dataset specifically designed for OOCD and developed an efficient detector for accurate classification. Our experimental findings validate the use of synthetic data generation and demonstrate its efficacy in addressing the data limitations associated with OOCD. The dataset and detector should serve as valuable resources for future research and the development of robust misinformation detection systems.
    
[^58]: Veagle: 多模态表示学习的进展

    Veagle: Advancements in Multimodal Representation Learning

    [https://arxiv.org/abs/2403.08773](https://arxiv.org/abs/2403.08773)

    本文介绍了一种新颖的方法，通过在当前视觉语言模型（VLMs）和多模态大语言模型（MLLMs）的基础上融合独特的机制，以增强现有模型的多模态能力。

    

    最近，人工智能领域的研究人员对语言和视觉如何结合产生了浓厚兴趣，从而催生了旨在无缝整合文本和视觉信息的多模态模型的发展。多模态模型是大型语言模型（LLMs）的延伸，在解决各种任务方面展现出了显著的能力，范围从图像字幕和视觉问答（VQA）到视觉定位。虽然这些模型展示了显著的进展，但在准确解释图像并回答问题方面仍存在挑战，在现实场景中经常发生。本文介绍了一种增强现有模型多模态能力的新方法。针对当前视觉语言模型（VLMs）和多模态大语言模型（MLLMs）中观察到的局限性，我们提出的模型Veagle，融合了受...

    arXiv:2403.08773v1 Announce Type: cross  Abstract: Lately, researchers in artificial intelligence have been really interested in how language and vision come together, giving rise to the development of multimodal models that aim to seamlessly integrate textual and visual information. Multimodal models, an extension of Large Language Models (LLMs), have exhibited remarkable capabilities in addressing a diverse array of tasks, ranging from image captioning and visual question answering (VQA) to visual grounding. While these models have showcased significant advancements, challenges persist in accurately interpreting images and answering the question, a common occurrence in real-world scenarios. This paper introduces a novel approach to enhance the multimodal capabilities of existing models. In response to the limitations observed in current Vision Language Models (VLMs) and Multimodal Large Language Models (MLLMs), our proposed model Veagle, incorporates a unique mechanism inspired by th
    
[^59]: SOTOPIA-$\pi$: 交互式学习社交智能语言代理

    SOTOPIA-$\pi$: Interactive Learning of Socially Intelligent Language Agents

    [https://arxiv.org/abs/2403.08715](https://arxiv.org/abs/2403.08715)

    提出了一种交互式学习方法SOTOPIA-$\pi$，该方法利用行为克隆和自我强化训练，改进了语言代理的社交智能，使其达到了专家模型的水平，并提高了安全性。

    

    人类通过模仿和社交互动来学习社交技能。现有研究在构建语言代理方面很少涉及这种社交学习过程。受到这一空白的启发，我们提出了一种交互式学习方法SOTOPIA-$\pi$，改进了语言代理的社交智能。该方法利用行为克隆和自我强化训练，根据大型语言模型(LLM)评分对经过筛选的社交互动数据进行训练。我们证明了我们的训练方法使一个7B的LLM达到了专家模型(GPT-4-based agent)的社交目标完成能力，同时提高了语言代理的安全性，并在MMLU基准上保持了通用的问答能力。我们还发现，这种训练范式揭示了LLM评估社交智能的一些困难：基于LLM的评估者高估了专门针对社交互动训练的语言代理的能力。

    arXiv:2403.08715v1 Announce Type: new  Abstract: Humans learn social skills through both imitation and social interaction. This social learning process is largely understudied by existing research on building language agents. Motivated by this gap, we propose an interactive learning method, SOTOPIA-$\pi$, improving the social intelligence of language agents. This method leverages behavior cloning and self-reinforcement training on filtered social interaction data according to large language model (LLM) ratings. We show that our training method allows a 7B LLM to reach the social goal completion ability of an expert model (GPT-4-based agent), while improving the safety of language agents and maintaining general QA ability on the MMLU benchmark. We also find that this training paradigm uncovers some difficulties in LLM-based evaluation of social intelligence: LLM-based evaluators overestimate the abilities of the language agents trained specifically for social interaction.
    
[^60]: 用于人工临床记录的零样本和少样本生成策略

    Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records

    [https://arxiv.org/abs/2403.08664](https://arxiv.org/abs/2403.08664)

    这项研究探讨了利用合成数据生成医疗记录的方法，特别是通过零样本和少样本提示策略，避免了在训练过程中使用真实患者数据的隐私问题。

    

    通过使用合成医疗记录的创新方法，本研究评估了Llama 2 LLM的能力，利用零样本和少样本提示策略生成可准确反映真实患者信息的合成医疗记录，与需要在训练过程中使用敏感患者数据的微调方法进行对比。

    arXiv:2403.08664v1 Announce Type: new  Abstract: The challenge of accessing historical patient data for clinical research, while adhering to privacy regulations, is a significant obstacle in medical science. An innovative approach to circumvent this issue involves utilising synthetic medical records that mirror real patient data without compromising individual privacy. The creation of these synthetic datasets, particularly without using actual patient data to train Large Language Models (LLMs), presents a novel solution as gaining access to sensitive patient information to train models is also a challenge. This study assesses the capability of the Llama 2 LLM to create synthetic medical records that accurately reflect real patient information, employing zero-shot and few-shot prompting strategies for comparison against fine-tuned methodologies that do require sensitive patient data during training. We focus on generating synthetic narratives for the History of Present Illness section, 
    
[^61]: 具有状态感知病人模拟器的大型语言模型自动互动评估

    Automatic Interactive Evaluation for Large Language Models with State Aware Patient Simulator

    [https://arxiv.org/abs/2403.08495](https://arxiv.org/abs/2403.08495)

    介绍了自动互动评估（AIE）框架和状态感知病人模拟器（SAPS），以动态、真实的平台评估LLMs，弥补传统评估方法无法满足临床任务需求的不足。

    

    大型语言模型（LLMs）在人机互动中表现出色，但在医疗领域的应用仍未得到充分探索。本文引入了自动互动评估（AIE）框架和状态感知病人模拟器（SAPS），旨在弥补传统LLM评估与临床实践的微妙需求之间的差距。

    arXiv:2403.08495v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in human interactions, yet their application within the medical field remains insufficiently explored. Previous works mainly focus on the performance of medical knowledge with examinations, which is far from the realistic scenarios, falling short in assessing the abilities of LLMs on clinical tasks. In the quest to enhance the application of Large Language Models (LLMs) in healthcare, this paper introduces the Automated Interactive Evaluation (AIE) framework and the State-Aware Patient Simulator (SAPS), targeting the gap between traditional LLM evaluations and the nuanced demands of clinical practice. Unlike prior methods that rely on static medical knowledge assessments, AIE and SAPS provide a dynamic, realistic platform for assessing LLMs through multi-turn doctor-patient simulations. This approach offers a closer approximation to real clinical scenarios and allows f
    
[^62]: 利用Context-Reverso数据使用Transformer模型生成具有上下文清晰度的句子

    Contextual Clarity: Generating Sentences with Transformer Models using Context-Reverso Data

    [https://arxiv.org/abs/2403.08103](https://arxiv.org/abs/2403.08103)

    使用Transformer模型和Context-Reverso数据生成具有上下文清晰度的句子

    

    在信息丰富的时代，提供与上下文相关且简洁的信息对用户至关重要。关键词上下文(KIC)生成是在一些应用中扮演至关重要角色的任务，比如搜索引擎、个人助手和内容摘要。本文提出了一种利用T5 transformer模型生成给定关键词的明确且简洁句子上下文的新方法，利用了从Context-Reverso API获取的数据。

    arXiv:2403.08103v1 Announce Type: cross  Abstract: In the age of information abundance, the ability to provide users with contextually relevant and concise information is crucial. Keyword in Context (KIC) generation is a task that plays a vital role in and generation applications, such as search engines, personal assistants, and content summarization. In this paper, we present a novel approach to generating unambiguous and brief sentence-contexts for given keywords using the T5 transformer model, leveraging data obtained from the Context-Reverso API. The code is available at https://github.com/Rusamus/word2context/tree/main .
    
[^63]: 通过代码探索大型语言模型的安全泛化挑战

    Exploring Safety Generalization Challenges of Large Language Models via Code

    [https://arxiv.org/abs/2403.07865](https://arxiv.org/abs/2403.07865)

    本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。

    

    大型语言模型（LLMs）的快速发展带来了自然语言处理方面的显著能力，但也引发了人们对它们潜在误用的担忧。本文引入了CodeAttack，一个将自然语言输入转换为代码输入的框架，为测试LLMs的安全泛化提供了一个新颖的环境。我们对包括GPT-4、Claude-2和Llama-2系列在内的最新LLMs进行了全面研究，发现这些模型对于代码输入存在共同的安全漏洞：CodeAttack在超过80%的时间内始终绕过所有模型的安全保护。

    arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
    
[^64]: 将竞争转化为合作：多Agent系统和语言模型在现代组织中的革命性作用

    Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations

    [https://arxiv.org/abs/2403.07769](https://arxiv.org/abs/2403.07769)

    文章探讨了基于多Agent系统理论结合大型语言模型的计算实体对人类互动的革新影响，提出了一种可能将专门人工代理支持扩展到操作性组织流程和基于知识和人类编排的战略决策的方式。

    

    这篇文章探讨了基于多Agent系统理论（SMA）结合大型语言模型（LLM）的计算实体的动态影响，其特点是能够模拟复杂的人类互动，作为一种革新人类用户交互的可能性，从利用专门的人工代理支持从操作组织流程到基于应用知识和人的编排的战略决策。 先前的调查显示，在处理新挑战和实用任务（如引发逻辑推理和问题解决）时，特别是在人工代理的自主方法方面存在限制。 还考虑到，传统技术，如激发思想链，需要明确的人类指导。 在我们的方法中，我们使用从大型语言模型（LLM）开发的代理，每个代理都有不同

    arXiv:2403.07769v1 Announce Type: new  Abstract: This article explores the dynamic influence of computational entities based on multi-agent systems theory (SMA) combined with large language models (LLM), which are characterized by their ability to simulate complex human interactions, as a possibility to revolutionize human user interaction from the use of specialized artificial agents to support everything from operational organizational processes to strategic decision making based on applied knowledge and human orchestration. Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving. It is also considered that traditional techniques, such as the stimulation of chains of thoughts, require explicit human guidance. In our approach we employ agents developed from large language models (LLM), each with distinct
    
[^65]: 使用对比奖励改善从人类反馈中的强化学习

    Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards

    [https://arxiv.org/abs/2403.07708](https://arxiv.org/abs/2403.07708)

    引入对比奖励的方法提高了从人类反馈中的强化学习的效果，改善了奖励模型的鲁棒性，鼓励对基准的改善，并能根据任务的难度进行校准。

    

    强化学习从人类反馈（RLHF）是用来对齐大型语言模型（LLMs）与人类偏好的主流范式。然而现有的RLHF在很大程度上依赖于准确和信息丰富的奖励模型，这些模型容易受到各种来源的噪声，例如人类标注错误，使得流程脆弱。本文通过引入对奖励的惩罚项，命名为“对比奖励”，来提高奖励模型的有效性。我们的方法包括两个步骤：（1）离线抽样步骤，获取用作基准计算的提示响应，以及（2）使用基准响应计算对比奖励，并将其用于Proximal Policy Optimization（PPO）步骤。我们展示了对比奖励使得LLM能够惩罚奖励不确定性，提高鲁棒性，鼓励优于基线的改进，根据任务难度进行校准，并且重新

    arXiv:2403.07708v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) is the mainstream paradigm used to align large language models (LLMs) with human preferences. Yet existing RLHF heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile. In this work, we improve the effectiveness of the reward model by introducing a penalty term on the reward, named as \textit{contrastive rewards}. %Contrastive rewards Our approach involves two steps: (1) an offline sampling step to obtain responses to prompts that serve as baseline calculation and (2) a contrastive reward calculated using the baseline responses and used in the Proximal Policy Optimization (PPO) step. We show that contrastive rewards enable the LLM to penalize reward uncertainty, improve robustness, encourage improvement over baselines, calibrate according to task difficulty, and re
    
[^66]: 具有赔率比的无参考单体偏好优化

    Reference-free Monolithic Preference Optimization with Odds Ratio

    [https://arxiv.org/abs/2403.07691](https://arxiv.org/abs/2403.07691)

    本文介绍了一种无参考单体赔率比偏好优化算法ORPO，在SFT过程中通过轻微惩罚不受欢迎的生成风格，消除了额外的偏好对齐阶段

    

    近期的语言模型偏好对齐算法展现了很好的结果，但是监督微调（SFT）仍然对于成功收敛至关重要。本文研究了在偏好对齐的环境中SFT的关键作用，强调对于偏好对齐的SFT来说，对于不受欢迎的生成风格施加轻微惩罚就足够了。在此基础上，我们引入了一种简单而创新的无参考模型的单体赔率比偏好优化算法ORPO，消除了额外的偏好对齐阶段的必要性。我们通过实证和理论手段证明，赔率比是在125M至7B不同规模下进行SFT时对比受欢迎和不受欢迎风格的明智选择。具体来说，使用ORPO在仅UltraFeedback上对Phi-2（2.7B）、Llama-2（7B）和Mistral（7B）进行微调，超越了性能

    arXiv:2403.07691v1 Announce Type: cross  Abstract: While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance o
    
[^67]: 真相感知的上下文选择：缓解大型语言模型被不真实上下文误导产生幻觉

    Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts

    [https://arxiv.org/abs/2403.07556](https://arxiv.org/abs/2403.07556)

    提出了一种名为真相感知的上下文选择（TACS）的轻量级方法，可以通过对输入上下文进行真相检测并构建相应的注意力蒙版来缓解大型语言模型被不真实上下文误导产生幻觉

    

    尽管大型语言模型（LLMs）展示了令人印象深刻的文本生成能力，但它们很容易被用户或知识论证工具提供的不真实上下文误导，从而产生幻觉。为了减轻LLMs被不真实信息误导并利用知识论证，我们提出了真相感知的上下文选择（TACS），这是一种轻量级方法，可以从输入中屏蔽不真实的上下文。TACS首先对输入上下文进行真相检测，利用LLM内的参数化知识。随后，根据每个位置的真实性构建相应的注意力蒙版，选择真实的上下文并丢弃不真实的上下文。此外，我们引入一个新的评估指标，扰动适应率，以进一步研究LLMs接受真实信息和抵制不真实信息的能力。

    arXiv:2403.07556v1 Announce Type: new  Abstract: Although large language models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by the untruthful context provided by users or knowledge argumentation tools, thereby producing hallucinations. To alleviate the LLMs from being misled by untruthful information and take advantage of knowledge argumentation, we propose Truth-Aware Context Selection (TACS), a lightweight method to shield untruthful context from the inputs. TACS begins by performing truth detection on the input context, leveraging the parameterized knowledge within the LLM. Subsequently, it constructs a corresponding attention mask based on the truthfulness of each position, selecting the truthful context and discarding the untruthful context. Additionally, we introduce a new evaluation metric, Disturbance Adaption Rate, to further study the LLMs' ability to accept truthful information and resist untruthful information. Experimental resul
    
[^68]: 知识图谱大型语言模型（KG-LLM）用于链接预测

    Knowledge Graph Large Language Model (KG-LLM) for Link Prediction

    [https://arxiv.org/abs/2403.07311](https://arxiv.org/abs/2403.07311)

    该论文提出了知识图谱大型语言模型框架（KG-LLM），利用思维链提示和上下文学习等NLP范例，以增强知识图谱中的多跳链接预测，并展示了框架在微调大型语言模型和零次尝试能力方面的有效性。

    

    在知识图谱分析领域，预测知识图谱（KGs）内多个链接的任务是一个挑战，由于自然语言处理（NLP）和知识图嵌入技术的进步，这一挑战变得越来越可解决。本文介绍了一种新的方法，即知识图谱大型语言模型框架（KG-LLM），该框架利用关键的NLP范例，包括思维链提示（CoT）和上下文学习（ICL），以增强知识图谱中的多跳链接预测。通过将KG转换为CoT提示，我们的框架旨在识别并学习实体及其相互关系的潜在表示。为了展示KG-LLM框架的有效性，我们在该框架内微调了三种主要的大型语言模型（LLMs），同时采用了非ICL和ICL任务进行全面评估。此外，我们探讨了该框架为LLMs提供零次尝试能力的潜力。

    arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
    
[^69]: 基于规则的新闻标题生成

    Rule-driven News Captioning

    [https://arxiv.org/abs/2403.05101](https://arxiv.org/abs/2403.05101)

    本文提出了一种基于规则的新闻标题生成方法，通过新闻感知的语义规则，可以生成遵循新闻报道基本规则的图像描述。

    

    News captioning任务旨在通过描述图片及其新闻文章中的命名实体或具体事件来生成句子。现有方法通过依赖大规模预训练模型已取得显著成果，这些模型主要专注于输入新闻内容与输出预测之间的相关性。然而，新闻标题生成需要遵循新闻报道的一些基本规则，如准确描述与事件相关的个体和动作。在本文中，我们提出了基于规则的新闻标题生成方法，可以根据指定的规则信号生成图像描述。具体而言，我们首先为描述设计了新闻感知的语义规则。这一规则包括图片中描绘的主要动作（例如，“执行”）以及参与动作的命名实体扮演的角色（例如，“代理人”和“地点”）。其次，我们将这个语义规则注入到文本生成模型中。

    arXiv:2403.05101v1 Announce Type: cross  Abstract: News captioning task aims to generate sentences by describing named entities or concrete events for an image with its news article. Existing methods have achieved remarkable results by relying on the large-scale pre-trained models, which primarily focus on the correlations between the input news content and the output predictions. However, the news captioning requires adhering to some fundamental rules of news reporting, such as accurately describing the individuals and actions associated with the event. In this paper, we propose the rule-driven news captioning method, which can generate image descriptions following designated rule signal. Specifically, we first design the news-aware semantic rule for the descriptions. This rule incorporates the primary action depicted in the image (e.g., "performing") and the roles played by named entities involved in the action (e.g., "Agent" and "Place"). Second, we inject this semantic rule into th
    
[^70]: 从图到词袋: 将领域知识引入混淆罪名预测

    From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction

    [https://arxiv.org/abs/2403.04369](https://arxiv.org/abs/2403.04369)

    引入领域知识的从图到词袋方法，帮助预测混淆罪名，通过构成要素和关键词选择进行判断。

    

    混淆罪名预测是法律人工智能中一个具有挑战性的任务，涉及根据事实描述预测混淆罪名。现有的罪名预测方法在表现上已经展现出令人印象深刻的效果，但在处理混淆罪名（如抢夺与抢劫）时面临着重大挑战。在法律领域，构成要素在区分混淆罪名中扮演着至关重要的角色。构成要素是潜在刑罚背后的基本行为，并且在不同罪名之间有微妙的区别。本文介绍了一种新的从图到词袋（FWGB）方法，该方法引入了有关构成要素的领域知识，以指导模型在混淆罪名上做出判断，类似于法官的推理过程。具体而言，我们首先构建了一个包含构成要素的法律知识图，以帮助为每种罪名选择关键词，形成一个单词袋。

    arXiv:2403.04369v1 Announce Type: new  Abstract: Confusing charge prediction is a challenging task in legal AI, which involves predicting confusing charges based on fact descriptions. While existing charge prediction methods have shown impressive performance, they face significant challenges when dealing with confusing charges, such as Snatch and Robbery. In the legal domain, constituent elements play a pivotal role in distinguishing confusing charges. Constituent elements are fundamental behaviors underlying criminal punishment and have subtle distinctions among charges. In this paper, we introduce a novel From Graph to Word Bag (FWGB) approach, which introduces domain knowledge regarding constituent elements to guide the model in making judgments on confusing charges, much like a judge's reasoning process. Specifically, we first construct a legal knowledge graph containing constituent elements to help select keywords for each charge, forming a word bag. Subsequently, to guide the mod
    
[^71]: 磨具探测和调整用于文本到图像生成

    Discriminative Probing and Tuning for Text-to-Image Generation

    [https://arxiv.org/abs/2403.04321](https://arxiv.org/abs/2403.04321)

    加强T2I模型的判别能力，以实现更精确的文本到图像对齐生成。

    

    尽管文本到图像生成（T2I）取得了进展，但先前的方法经常面临文本图像不对齐等问题，如生成图像中的关系混淆。现有解决方案包括交叉注意力操作以实现更好的组合理解，或者集成大型语言模型以改进布局规划。然而，T2I模型的固有对齐能力仍然不足。通过审视生成模型和判别模型之间的联系，我们认为T2I模型的判别能力可能反映了它们在生成过程中的文本图像对齐熟练度。基于这一观点，我们主张加强T2I模型的判别能力，以实现更精确的文本到图像对齐生成。我们提出了一个建立在T2I模型上的判别适配器，以探测它们在两项代表性任务上的判别能力，并利用判别微调来改善它们的文本图像对齐。

    arXiv:2403.04321v1 Announce Type: cross  Abstract: Despite advancements in text-to-image generation (T2I), prior methods often face text-image misalignment problems such as relation confusion in generated images. Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning. However, the inherent alignment capabilities of T2I models are still inadequate. By reviewing the link between generative and discriminative modeling, we posit that T2I models' discriminative abilities may reflect their text-image alignment proficiency during generation. In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation. We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative fine-tuning to improve their text-image alignment. As a 
    
[^72]: 德语也产生幻觉！使用Absinth数据集检测新闻摘要中的不一致性

    German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset

    [https://arxiv.org/abs/2403.03750](https://arxiv.org/abs/2403.03750)

    本文提出了一个用于德语新闻摘要中幻觉检测的数据集absinth，探讨了LLMs在该任务中的应用。

    

    大型语言模型（LLMs）的出现在自然语言处理任务中取得了显著进展，然而，这些大型模型仍然存在产生信息幻觉的问题，这在自动文本摘要中至关重要。先前的研究主要集中在英语上，并且最近的多语言方法缺乏德语数据。本文提出了absinth，一个用于德语新闻摘要中幻觉检测的手动注释数据集，并探讨了新型开源LLMs在这一任务上的能力，包括微调和上下文学习设置。我们开源并发布了这个数据集。

    arXiv:2403.03750v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has led to remarkable progress on a wide range of natural language processing tasks. Despite the advances, these large-sized models still suffer from hallucinating information in their output, which poses a major issue in automatic text summarization, as we must guarantee that the generated summary is consistent with the content of the source document. Previous research addresses the challenging task of detecting hallucinations in the output (i.e. inconsistency detection) in order to evaluate the faithfulness of the generated summaries. However, these works primarily focus on English and recent multilingual approaches lack German data. This work presents absinth, a manually annotated dataset for hallucination detection in German news summarization and explores the capabilities of novel open-source LLMs on this task in both fine-tuning and in-context learning settings. We open-source and releas
    
[^73]: VBART: 土耳其LLM

    VBART: The Turkish LLM

    [https://arxiv.org/abs/2403.01308](https://arxiv.org/abs/2403.01308)

    VBART是第一个土耳其序列到序列大语言模型，通过与BART和mBART模型结合形成了紧凑型LLM，并在多个任务中表现出色，为土耳其自然语言处理研究开辟了新的可能性。

    

    我们提出了VBART，这是第一个土耳其序列到序列大语言模型（LLMs），在一个大语料库上从头开始进行预训练。VBART是基于BART和mBART模型的好思路构建的紧凑型LLMs，分为Large和XLarge两个尺寸。微调后的VBART模型在提取性文本摘要、标题生成、文本改写、问答和问题生成等任务中超越了先前的最先进结果。它们允许为未来的文本生成任务和数据集进行微调，为土耳其自然语言处理（NLP）研究开辟了新路径。我们的工作表明，拥有为土耳其进行预训练的LLM比多语言模型提高了最多3倍，改进了现有结果，并为训练和推理提供了高效的模型。此外，我们展示了我们的单语分词器比OpenAI的多语言分词器更高效7倍。最后但同样重要的是，我们介绍了一种扩展现有预训

    arXiv:2403.01308v1 Announce Type: new  Abstract: We present VBART, the first Turkish sequence-to-sequence Large Language Models (LLMs) pre-trained on a large corpus from scratch. VBART are compact LLMs based on good ideas leveraged from BART and mBART models and come in two sizes, Large and XLarge. Fine-tuned VBART models surpass the prior state-of-the-art results in abstractive text summarization, title generation, text paraphrasing, question answering and question generation tasks. They allow fine-tuning for future text generation tasks and datasets, carving a new path for Turkish Natural Language Processing (NLP) research. Our work shows that having a pre-trained LLM for Turkish outperforms up to 3x multilingual models, improving existing results and providing efficient models for training and inference. Moreover, we show that our monolingual tokenizer is 7x more efficient than OpenAI's multilingual tokenizer. Last but not least, we introduce a method to enlarge an existing pre-trai
    
[^74]: RAGged Edges: Retrieval-Augmented Chatbots的双刃剑

    RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots

    [https://arxiv.org/abs/2403.01193](https://arxiv.org/abs/2403.01193)

    本文探讨了如何利用检索增强生成（RAG）抵制大型语言模型（LLMs）产生的幻觉，结果表明RAG在某些情况下可以提高准确性，但仍需要更强大的解决方案以确保LLMs在实际应用中可靠性。

    

    大型语言模型（LLMs）如ChatGPT展示了人工智能的显著进展。然而，它们倾向于产生幻觉 - 生成看似正确但错误信息的倾向带来了重大挑战。这个问题很关键，就像最近的法院案例中看到的那样，ChatGPT的使用导致了不存在的法律裁决的引用。本文探讨了如何通过将外部知识与提示集成来使用检索增强生成（RAG）来抵制幻觉。我们通过使用旨在诱导幻觉的提示来对RAG与标准LLMs进行经验评估。我们的结果显示，在某些情况下，RAG可以提高准确性，但当提示直接与模型预训练的理解相矛盾时，RAG仍然会被误导。这些发现突显了幻觉的复杂性以及需要更强大的解决方案以确保LLMs在实际应用中可靠性。我们提供了RAG部署的实用建议。

    arXiv:2403.01193v1 Announce Type: cross  Abstract: Large language models (LLMs) like ChatGPT demonstrate the remarkable progress of artificial intelligence. However, their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge. This issue is critical, as seen in recent court cases where ChatGPT's use led to citations of non-existent legal rulings. This paper explores how Retrieval-Augmented Generation (RAG) can counter hallucinations by integrating external knowledge with prompts. We empirically evaluate RAG against standard LLMs using prompts designed to induce hallucinations. Our results show that RAG increases accuracy in some cases, but can still be misled when prompts directly contradict the model's pre-trained understanding. These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications. We offer practical recommendations for RAG deployment and 
    
[^75]: 用大语言模型执行自然语言描述的算法：一项研究

    Executing Natural Language-Described Algorithms with Large Language Models: An Investigation

    [https://arxiv.org/abs/2403.00795](https://arxiv.org/abs/2403.00795)

    大语言模型可以有效地执行用自然语言描述的程序，尤其是在不涉及大量数字计算的情况下。

    

    使用自然语言描述的计算机程序一直是计算机科学的追求。随着大语言模型（LLMs）展示出的增强自然语言理解能力的出现，这一目标的道路已经被阐明。本文旨在检验现有LLMs理解和执行自然语言中描述的算法的能力。我们从《算法导论》中选取了一个算法测试集，该书是一本包含许多代表性广泛使用的算法的知名教材。为了系统评估LLMs的代码执行能力，我们选择了30个算法，共生成了300个随机抽样实例，并评估了流行的LLMs是否能够理解和执行这些算法。我们的发现表明，特别是GPT-4等LLMs可以有效地执行用自然语言描述的程序，只要不涉及大量数字计算。

    arXiv:2403.00795v1 Announce Type: cross  Abstract: Executing computer programs described in natural language has long been a pursuit of computer science. With the advent of enhanced natural language understanding capabilities exhibited by large language models (LLMs), the path toward this goal has been illuminated. In this paper, we seek to examine the capacity of present-day LLMs to comprehend and execute algorithms outlined in natural language. We established an algorithm test set sourced from Introduction to Algorithm, a well-known textbook that contains many representative widely-used algorithms. To systematically assess LLMs' code execution abilities, we selected 30 algorithms, generated 300 random-sampled instances in total, and evaluated whether popular LLMs can understand and execute these algorithms. Our findings reveal that LLMs, notably GPT-4, can effectively execute programs described in natural language, as long as no heavy numeric computation is involved. We believe our f
    
[^76]: 利用社交网络数据进行区域通货膨胀分析

    Regional inflation analysis using social network data

    [https://arxiv.org/abs/2403.00774](https://arxiv.org/abs/2403.00774)

    本研究利用社交网络数据分析了区域通货膨胀的上升和下降趋势，探讨了社交网络讨论对通货膨胀预期的潜在影响。

    

    通货膨胀是影响任何国家和地区人口的最重要的宏观经济指标之一。通货膨胀受多种因素影响，其中之一是通货膨胀预期。许多央行在实施以通货膨胀目标为核心的货币政策时考虑到这一因素。本研究基于Vkontakte社交网络的非结构化数据，分析了涉及通货膨胀上升和下降趋势的内容（以鄂木斯克地区为例）。

    arXiv:2403.00774v1 Announce Type: cross  Abstract: Inflation is one of the most important macroeconomic indicators that have a great impact on the population of any country and region. Inflation is influenced by range of factors, one of which is inflation expectations. Many central banks take this factor into consideration while implementing monetary policy within the inflation targeting regime. Nowadays, a lot of people are active users of the Internet, especially social networks. There is a hypothesis that people search, read, and discuss mainly only those issues that are of particular interest to them. It is logical to assume that the dynamics of prices may also be in the focus of user discussions. So, such discussions could be regarded as an alternative source of more rapid information about inflation expectations. This study is based on unstructured data from Vkontakte social network to analyze upward and downward inflationary trends (on the example of the Omsk region). The sample
    
[^77]: MMSR：符号回归是一个多模态任务

    MMSR: Symbolic Regression is a Multimodal Task

    [https://arxiv.org/abs/2402.18603](https://arxiv.org/abs/2402.18603)

    符号回归被视为一个多模态任务，研究人员将数据到表达式的映射视为翻译问题，引入大规模预训练模型。

    

    数学公式是探索自然规律几千年来人类智慧的结晶。用简洁的数学公式描述复杂的自然规律是科学家不断追求的目标，也是人工智能面临的重大挑战。这一领域被称为符号回归。在本文中，研究人员将从数据到表达式的映射视为翻译问题，并引入了相应的大规模预训练模型。

    arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
    
[^78]: 科学检查者再度升级：透明度和逻辑推理的双向范式

    Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning

    [https://arxiv.org/abs/2402.13897](https://arxiv.org/abs/2402.13897)

    提出了一个两块式的方法来解决长文档中信息检索领域的挑战，并实现了双向交互

    

    信息检索是一个快速发展的领域。然而，它仍然面临着在科学和工业的海量信息中的诸多限制，比如语义分歧和检索中的词汇差距、语义搜索中的低精度和缺乏可解释性，或者生成模型中的幻觉和过时信息。在本文中，我们提出了一个两块式的方法来解决长文档的这些障碍。第一个模块通过查询扩展增强了在稀疏检索中的语言理解，以检索相关文档。第二个模块通过只使用长文档中传播的信息，为复杂问题提供全面和信息丰富的答案来加深结果，实现双向交互。在管道的各个阶段，向用户呈现中间结果以促进对系统推理的理解。我们相信这种双向方法带来了

    arXiv:2402.13897v1 Announce Type: cross  Abstract: Information retrieval is a rapidly evolving field. However it still faces significant limitations in the scientific and industrial vast amounts of information, such as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated information in generative models. In this paper, we introduce a two-block approach to tackle these hurdles for long documents. The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents. The second block deepens the result by providing comprehensive and informative answers to the complex question using only the information spread in the long document, enabling bidirectional engagement. At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system's reasoning. We believe this bidirectional approach brings
    
[^79]: 《检索是有益还是有害？深入探讨检索增强对语言模型效果的影响》

    Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models

    [https://arxiv.org/abs/2402.13492](https://arxiv.org/abs/2402.13492)

    该研究深入探讨了如何通过检索增强语言模型，构建了新的QA数据集WiTQA，以实体和关系组合的影响为重点进行了详细分析。

    

    虽然大型语言模型（LMs）表现出色，但当需要查询其预训练记忆之外的信息时，它们在提供准确回答时会遇到挑战。虽然利用相关外部信息来增强它们可以缓解这些问题，但未考虑检索的必要性可能会对整体性能产生负面影响。此前的研究主要关注实体如何影响检索模型与LMs中的知识回忆，其他方面相对未被探索。本研究旨在通过探索实体和关系组合的影响来提供更详细、以事实为中心的分析。为实现这一目标，我们构建了一个名为WiTQA（Wikipedia Triple Question Answers）的新问题回答（QA）数据集。此数据集包括关于不同受欢迎程度实体和关系的问题，每个问题都附带一段支持性段落。

    arXiv:2402.13492v1 Announce Type: new  Abstract: While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization. Although augmenting them with relevant external information can mitigate these issues, failure to consider the necessity of retrieval may adversely affect overall performance. Previous research has primarily focused on examining how entities influence retrieval models and knowledge recall in LMs, leaving other aspects relatively unexplored. In this work, our goal is to offer a more detailed, fact-centric analysis by exploring the effects of combinations of entities and relations. To facilitate this, we construct a new question answering (QA) dataset called WiTQA (Wikipedia Triple Question Answers). This dataset includes questions about entities and relations of various popularity levels, each accompanied by a supporting passage. Our extensive ex
    
[^80]: Me LLaMA: 为医疗应用构建大型语言模型的基础

    Me LLaMA: Foundation Large Language Models for Medical Applications

    [https://arxiv.org/abs/2402.12749](https://arxiv.org/abs/2402.12749)

    Me LLaMA是一个医学领域的大型语言模型系列，通过持续预训练和指导调整在大型医学数据集上训练而成，其在零-shot和少-shot学习方面表现优于现有的医学语言模型和商业巨头ChatGPT。

    

    最近，诸如ChatGPT和LLaMA等大型语言模型(LLMs)在许多人工智能应用中展现出巨大的潜力。然而，它们在医学任务上的表现不够理想，并且可以通过在大型领域特定数据集上进行训练来进一步改进。本研究引入了Me LLaMA，一个医学LLM系列，包括基础模型- Me LLaMA 13/70B及其 chat-enhanced 版本- Me LLaMA 13/70B-chat，通过持续对LLaMA2进行预训练和指导调整，使用大规模医学数据开发而成。我们用于训练和评估的领域特定数据套件包括一个具有129B tokens的大规模持续预训练数据集，一个包含214k个样本的指导调整数据集，以及跨越14个数据集的六项任务的医学评估基准(MIBE)。我们使用MIBE进行的广泛评估显示，Me LLaMA模型在零-shot和少-shot学习方面超越了现有的开源医学LLMs，并且在商业巨头如ChatGPT上表现出色。

    arXiv:2402.12749v1 Announce Type: cross  Abstract: Recent large language models (LLMs) like ChatGPT and LLaMA have shown great promise in many AI applications. However, their performance on medical tasks is suboptimal and can be further improved by training on large domain-specific datasets. This study introduces Me LLaMA, a medical LLM family including foundation models - Me LLaMA 13/70B and their chat-enhanced versions - Me LLaMA 13/70B-chat, developed through the continual pre-training and instruction tuning of LLaMA2 using large medical data. Our domain-specific data suite for training and evaluation, includes a large-scale continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a medical evaluation benchmark (MIBE) across six tasks with 14 datasets. Our extensive evaluation using MIBE shows that Me LLaMA models surpass existing open-source medical LLMs in zero-shot and few-shot learning and outperform commercial giants like ChatGPT on 
    
[^81]: 信实性与可信度: 关于大型语言模型解释的(不)可靠性

    Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models

    [https://arxiv.org/abs/2402.04614](https://arxiv.org/abs/2402.04614)

    本文讨论了大型语言模型生成的自我解释中的信实性与可信度之间的差异。虽然这些解释在逻辑上是合乎情理且连贯的，但并不一定与模型的推理过程一致，引发对其信实性的担忧。

    

    大型语言模型(LLMs)被部署为几种自然语言处理（NLP）应用的强大工具。最近的研究显示，现代LLMs可以生成自我解释（SEs），这些SEs揭示了它们解释其行为的中间推理步骤。由于其对话性和可信度的特点，自我解释已广泛应用。然而，我们对其信实性了解甚少。在本研究中，我们讨论了LLMs生成的SEs中信实性和可信度之间的二分法。我们认为，虽然LLMs擅长生成可信的解释-对人类用户来说似乎逻辑和连贯-但这些解释未必与LLMs的推理过程相一致，引发对其信实性的担忧。我们强调，当前趋势是为了用户友好界面的需求而增加解释的可信度，可能会以降低解释的信实性为代价。

    Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness
    
[^82]: SOCIALITE-LLAMA：社会科学任务的指导调优模型

    SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks

    [https://arxiv.org/abs/2402.01980](https://arxiv.org/abs/2402.01980)

    社会科学的自然语言处理任务需要捕捉语义和隐含的语用信息，指导调优模型Socialite-Llama在这些任务上表现出卓越的性能和提升。

    

    社会科学的自然语言处理任务，如情绪或幽默检测，需要从文本中捕捉语义和隐含的语用信息，通常只有有限的训练数据。指导调优已经被证明可以改善大型语言模型（LLM）的许多能力，例如常识推理、阅读理解和计算机编程。然而，在需要捕捉隐含的语用线索的社交领域，对指导调优的效果了解甚少。我们探索了将指导调优用于社会科学的自然语言处理任务，并介绍了 Socialite-Llama - 一个开源的、经过指导调优的 Llama 模型。在一套包含20个社会科学任务的测试中，Socialite-Llama 在性能上优于 Llama，并且在大多数任务上与最先进的多任务微调模型的性能相匹配或得到提升。此外，与 Llama 相比，Socialite-Llama 在6个相关的社会任务中的5个任务上也有所改进，这表明指导调优在这些任务上也会带来改善。

    Social science NLP tasks, such as emotion or humor detection, are required to capture the semantics along with the implicit pragmatics from text, often with limited amounts of training data. Instruction tuning has been shown to improve the many capabilities of large language models (LLMs) such as commonsense reasoning, reading comprehension, and computer programming. However, little is known about the effectiveness of instruction tuning on the social domain where implicit pragmatic cues are often needed to be captured. We explore the use of instruction tuning for social science NLP tasks and introduce Socialite-Llama -- an open-source, instruction-tuned Llama. On a suite of 20 social science tasks, Socialite-Llama improves upon the performance of Llama as well as matches or improves upon the performance of a state-of-the-art, multi-task finetuned model on a majority of them. Further, Socialite-Llama also leads to improvement on 5 out of 6 related social tasks as compared to Llama, sugg
    
[^83]: 我们应该疯狂吗？多Agent辩论策略对LLMs的影响

    Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs

    [https://arxiv.org/abs/2311.17371](https://arxiv.org/abs/2311.17371)

    多Agent辩论（MAD）作为增强大型语言模型（LLMs）真实性的策略，对于解决确保生成代理提供准确可靠答案的挑战具有潜力，但当前形式下的多Agent辩论系统在可靠性上不一定优于其他提示策略。

    

    最近大型语言模型（LLMs）的发展突显了它们在回答各种领域问题方面的潜力。然而，确保生成代理提供准确可靠的答案仍然是一个持续挑战。在这种背景下，多Agent辩论（MAD）已成为增强LLMs真实性的一种有前途的策略。我们对一系列辩论和提示策略进行基准测试，探讨成本、时间和准确性之间的权衡。重要的是，我们发现，目前形式下的多Agent辩论系统在可靠性上不一定优于其他建议的提示策略，如自一致性和使用多个推理路径进行集成。但是，在执行超参数调整时，一些MAD系统，如Multi-Persona，表现更好。这表明MAD协议可能并不会比其他方法天然更差，而是更容易受到不同超参数的影响。

    arXiv:2311.17371v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models (LLMs) underscore their potential for responding to inquiries in various domains. However, ensuring that generative agents provide accurate and reliable answers remains an ongoing challenge. In this context, multi-agent debate (MAD) has emerged as a promising strategy for enhancing the truthfulness of LLMs. We benchmark a range of debating and prompting strategies to explore the trade-offs between cost, time, and accuracy. Importantly, we find that multi-agent debating systems, in their current form, do not reliably outperform other proposed prompting strategies, such as self-consistency and ensembling using multiple reasoning paths. However, when performing hyperparameter tuning, several MAD systems, such as Multi-Persona, perform better. This suggests that MAD protocols might not be inherently worse than other approaches, but that they are more sensitive to different hyperparameter
    
[^84]: 我应该使用哪种模态--文本、主题或图像？：通过大型语言模型理解图表

    Which Modality should I use -- Text, Motif, or Image? : Understanding Graphs with Large Language Models

    [https://arxiv.org/abs/2311.09862](https://arxiv.org/abs/2311.09862)

    本研究提出了一种新方法，使用文本、图像和主题等多种模态对图进行编码，结合提示以近似表示图的全局连接性，从而提高了LLMs处理复杂图结构的效率。

    

    我们的研究将图数据与大型语言模型（LLMs）结合起来，尽管它们在利用大量文本语料库在各个领域取得了进展，但由于上下文大小限制，面临着在对整个图进行编码时的限制。本文介绍了一种新方法，使用文本、图像和主题等多样的模态对图进行编码，结合提示以近似表示一个图的全局连通性，从而增强LLMs在处理复杂图结构时的效率。该研究还提出了GraphTMI，一个评估LLMs在图结构分析中的新颖基准，重点关注同质性、主题存在和图难度。关键发现表明，尤其是使用视觉语言模型如GPT-4V的图像模态，比文本在平衡标记限制和保留关键信息方面更优，胜过先前的图神经网络（GNN）编码器。此外，该研究评估了各种因素如何影响Pe

    arXiv:2311.09862v2 Announce Type: replace  Abstract: Our research integrates graph data with Large Language Models (LLMs), which, despite their advancements in various fields using large text corpora, face limitations in encoding entire graphs due to context size constraints. This paper introduces a new approach to encoding a graph with diverse modalities, such as text, image, and motif, coupled with prompts to approximate a graph's global connectivity, thereby enhancing LLMs' efficiency in processing complex graph structures. The study also presents GraphTMI, a novel benchmark for evaluating LLMs in graph structure analysis, focusing on homophily, motif presence, and graph difficulty. Key findings indicate that the image modality, especially with vision-language models like GPT-4V, is superior to text in balancing token limits and preserving essential information and outperforms prior graph neural net (GNN) encoders. Furthermore, the research assesses how various factors affect the pe
    
[^85]: BeLLM：增强反向依赖的大型语言模型用于句子嵌入

    BeLLM: Backward Dependency Enhanced Large Language Model for Sentence Embeddings

    [https://arxiv.org/abs/2311.05296](https://arxiv.org/abs/2311.05296)

    BeLLM提出了增强反向依赖的大型语言模型，通过引入显式的反向依赖性，在语义相似性测量中取得了最先进的性能。

    

    句子嵌入在衡量语义相似性中至关重要。最近的研究使用大型语言模型（LLMs）来学习句子嵌入。现有的LLMs主要采用自回归架构，没有明确建模反向依赖性。因此，我们研究了LLMs中反向依赖性对语义相似性测量的影响。具体地，我们提出了一种新型模型：增强反向依赖的大型语言模型（BeLLM）。它通过将特定注意力层从单向转换为双向来学习句子嵌入。我们在各种语义文本相似性（STS）任务和下游应用中进行了广泛实验。BeLLM在不同情境下实现了最新的性能。研究表明，自回归LLMs受益于反向依赖性以用于句子嵌入。

    arXiv:2311.05296v2 Announce Type: replace  Abstract: Sentence embeddings are crucial in measuring semantic similarity. Most recent studies employed large language models (LLMs) to learn sentence embeddings. Existing LLMs mainly adopted autoregressive architecture without explicit backward dependency modeling. Therefore, we examined the effects of backward dependencies in LLMs for semantic similarity measurements. Concretely, we propose a novel model: backward dependency enhanced large language model (BeLLM). It learns sentence embeddings via transforming specific attention layers from uni- to bi-directional. We extensively experiment across various semantic textual similarity (STS) tasks and downstream applications. BeLLM achieves state-of-the-art performance in varying scenarios. It shows that auto-regressive LLMs benefit from backward dependencies for sentence embeddings.
    
[^86]: LILO：通过压缩和文档化代码学习可解释库

    LILO: Learning Interpretable Libraries by Compressing and Documenting Code

    [https://arxiv.org/abs/2310.19791](https://arxiv.org/abs/2310.19791)

    LILO是一种神经符号框架，通过迭代地合成、压缩和文档化代码来构建可解释且适用于特定问题领域的程序库。在其中，LILO结合了大型语言模型引导的程序合成和程序自动重构的算法进展，并且通过自动文档过程使得代码抽象可解释并提升性能。

    

    尽管大型语言模型（LLMs）在代码生成方面表现出色，但软件开发的关键方面是重构的艺术：将代码整合到可重用和可读的程序库中。本文介绍了一种名为LILO的神经符号框架，它通过迭代地合成、压缩和文档化代码来构建适合特定问题领域的库。LILO将LLM引导的程序合成与Stitch自动重构的近期算法进展相结合：Stitch是一个符号压缩系统，可以高效地识别大型代码语料库中的最佳lambda抽象。为了使这些抽象可解释，我们引入了一种自动文档（AutoDoc）过程，它根据上下文中的使用示例推断出自然语言名称和文档字符串。除了提高人类可读性外，我们发现AutoDoc通过帮助LILO的合成器解释和部署学习到的抽象来提高性能。我们对LILO进行了三个归纳式程序综合的评估。

    While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synth
    
[^87]: XAL：可解释的主动学习提升了低资源学习者的分类器性能

    XAL: EXplainable Active Learning Makes Classifiers Better Low-resource Learners

    [https://arxiv.org/abs/2310.05502](https://arxiv.org/abs/2310.05502)

    XAL提出了一种可解释的主动学习框架，鼓励分类器提供推断的理由并深入未标记数据，从而提升低资源文本分类的性能

    

    主动学习（AL）旨在通过迭代地筛选最具形成性的未标记数据进行注释，构建有效的训练集，被广泛应用于低资源任务。大多数分类中的主动学习技术依赖于模型的不确定性或分歧来选择未标记数据，会出现对表面模式的过度自信和缺乏探索的问题。受到人类根据因果信息推断和预测的认知过程启发，我们首次尝试将理由融入AL中，提出了一种新颖的面向低资源文本分类的可解释主动学习框架（XAL），旨在鼓励分类器证明其推断并深入研究无法提供合理解释的未标记数据。具体来说，除了使用预训练的双向编码器进行分类外，我们还采用预训练的单向编码器

    arXiv:2310.05502v2 Announce Type: replace  Abstract: Active learning (AL), which aims to construct an effective training set by iteratively curating the most formative unlabeled data for annotation, has been widely used in low-resource tasks. Most active learning techniques in classification rely on the model's uncertainty or disagreement to choose unlabeled data, suffering from the problem of over-confidence in superficial patterns and a lack of exploration. Inspired by the cognitive processes in which humans deduce and predict through causal information, we take an initial attempt towards integrating rationales into AL and propose a novel Explainable Active Learning framework (XAL) for low-resource text classification, which aims to encourage classifiers to justify their inferences and delve into unlabeled data for which they cannot provide reasonable explanations. Specifically, besides using a pre-trained bi-directional encoder for classification, we employ a pre-trained uni-directi
    
[^88]: 利用上下文线索和角色相关性提升文档级事件论证提取

    Utilizing Contextual Clues and Role Correlations for Enhancing Document-level Event Argument Extraction

    [https://arxiv.org/abs/2310.05116](https://arxiv.org/abs/2310.05116)

    本文提出了CARLG模型，通过利用上下文线索和角色相关性，提升了文档级事件论证提取的性能。

    

    文档级事件论证提取（EAE）是信息提取中至关重要但具有挑战性的子任务之一。现有方法大多关注论证和事件触发器之间的交互，忽视了两个关键点：上下文线索的信息和论证角色之间的语义相关性。本文提出了CARLG模型，包括两个模块：上下文线索聚合（CCA）和基于角色的潜在信息引导（RLIG），通过有效利用上下文线索和角色相关性来提高文档级EAE。CCA模块通过利用来自预训练编码器的上下文注意权重，自适应地捕捉和整合上下文线索。RLIG模块通过角色交互编码捕捉语义相关性，并通过潜在角色表示提供宝贵的信息引导。值得注意的是，我们的CCA和RLIG模块紧凑、可移植且高效，引入的新参数不超过1%，且易于实现。

    Document-level event argument extraction (EAE) is a vital but challenging subtask in information extraction. Most existing approaches focus on the interaction between arguments and event triggers, ignoring two critical points: the information of contextual clues and the semantic correlations among argument roles. In this paper, we propose the CARLG model, which consists of two modules: Contextual Clues Aggregation (CCA) and Role-based Latent Information Guidance (RLIG), effectively leveraging contextual clues and role correlations for improving document-level EAE. The CCA module adaptively captures and integrates contextual clues by utilizing context attention weights from a pre-trained encoder. The RLIG module captures semantic correlations through role-interactive encoding and provides valuable information guidance with latent role representation. Notably, our CCA and RLIG modules are compact, transplantable and efficient, which introduce no more than 1% new parameters and can be eas
    
[^89]: 谨言慎行：使用暂停标记训练语言模型

    Think before you speak: Training Language Models With Pause Tokens

    [https://arxiv.org/abs/2310.02226](https://arxiv.org/abs/2310.02226)

    引入暂停标记的语言模型训练方法可以让模型在输出标记前处理更多隐藏向量，取得了较好的实验结果

    

    语言模型通过立即连续生成一系列标记来生成响应: 第$(K+1)^{th}$个标记是通过操作每层的$K$个隐藏向量得到的，每个向量对应一个前面的标记。如果我们让模型在输出第$(K+1)^{th}$个标记之前操作更多的隐藏向量，比如说$K+10$个呢？我们通过在语言模型上进行训练和推断，引入了一个（可学习的）$\textit{pause}$标记，这一系列标记附加到输入前缀上。然后我们延迟提取模型的输出，直到最后一个暂停标记被看到，从而允许模型在做出答案之前进行额外的计算处理。我们在拥有1B和130M参数的仅解码器模型上进行了$\textit{pause-training}$的实证评估，在C4上进行了因果预训练，并在涵盖推理、问答、普遍理解和事实回忆等下游任务上进行了评估。我们的主要发现是，infer

    arXiv:2310.02226v2 Announce Type: replace-cross  Abstract: Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that infer
    
[^90]: 合并，然后压缩：从其路由策略中揭示高效的SMoE技术提示

    Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy

    [https://arxiv.org/abs/2310.01334](https://arxiv.org/abs/2310.01334)

    本文旨在探讨如何通过合并专家信息来制定出更紧凑但更具知识的SMoE模型，因为传统的模型合并方法并不适用于SMoE的专家合并。

    

    稀疏激活的专家混合模型（SMoE）显示出扩展神经网络学习能力的潜力，然而，它们存在诸如（a）高内存使用的问题，由于网络层的重复成为多个专家的副本；以及（b）专家中的冗余，因为常规基于学习的路由策略容易出现表示性崩溃。因此，传统SMoE模型在内存效率和可伸缩性方面效率低下，尤其对于资源受限的下游场景。在本文中，我们提出了一个问题：我们能否通过合并专家信息来制定一个紧凑的SMoE模型？如何将多个专家合并为更少但更有知识的专家的最佳方法？我们的初步调查显示，传统的模型合并方法对于SMoE的专家合并并不有效。潜在原因是：（1）冗余信息掩盖了关键专家；（2）为每个专家选择适当的神经元排列方式会丢失

    arXiv:2310.01334v2 Announce Type: replace-cross  Abstract: Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up the learning capacity of neural networks, however, they have issues like (a) High Memory Usage, due to duplication of the network layers into multiple copies as experts; and (b) Redundancy in Experts, as common learning-based routing policies suffer from representational collapse. Therefore, vanilla SMoE models are memory inefficient and non-scalable, especially for resource-constrained downstream scenarios. In this paper, we ask: Can we craft a compact SMoE model by consolidating expert information? What is the best recipe to merge multiple experts into fewer but more knowledgeable experts? Our pilot investigation reveals that conventional model merging methods fail to be effective in such expert merging for SMoE. The potential reasons are: (1) redundant information overshadows critical experts; (2) appropriate neuron permutation for each expert is miss
    
[^91]: 通过执行反馈使语言模型成为更好的工具学习者

    Making Language Models Better Tool Learners with Execution Feedback

    [https://arxiv.org/abs/2305.13068](https://arxiv.org/abs/2305.13068)

    这篇论文提出了一个名为TRICE的框架，通过执行反馈实现语言模型的工具学习，使其能够学会何时以及如何有效地使用工具。

    

    工具作为关键的界面，使人类能够理解和改变环境。随着基础模型的出现，AI系统可以利用工具扩展其能力并与真实世界互动。现有的工具学习方法包括监督微调和提示工程方法，通常使大型语言模型不加选择地利用工具，因为复杂任务往往超出了它们自身的能力。然而，为简单任务引入工具（模型本身可以轻松解决的任务），可能会无意间传播错误而不是提高性能。因此，研究问题是：我们能否教会语言模型何时以及如何使用工具？为满足这个需求，我们提出了Tool leaRning wIth exeCution fEedback (TRICE)，这是一个两阶段的端到端框架，使模型能够通过从工具执行中得到的反馈不断学习，从而学会何时以及如何有效地使用工具。

    Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed b
    
[^92]: 用软标签原型从少量示例中学习新任务

    Learning New Tasks from a Few Examples with Soft-Label Prototypes

    [https://arxiv.org/abs/2210.17437](https://arxiv.org/abs/2210.17437)

    本研究提出了一种新的极端少样本学习方法，利用软标签原型从少量示例中学习新任务，在大型、高维和现实世界数据集上表现出色。

    

    自然语言处理中的少样本学习现有方法依赖于大型语言模型和对其微调，以在分布外数据上进行泛化。在这项工作中，我们提出了一种简单但强大的“极端”少样本学习方法，其中模型只需接触每个类别至少4个示例，这些示例基于软标签原型，这些软标签原型共同捕获了输入域空间中不同类别的分布。受到先前关于一元或简单多元（合成）数据（Sucholutsky等人，2021）的工作的启发，我们提出了一种在大型、高维和现实世界数据集上有效的新方法。我们在神经框架（DeepSLP）中学习软标签原型，并在实验中展示，它在31/48个测试任务和少样本设置上表现优异，同时在其他任务上与强基线模型的性能相匹配。我们专注于从v中学习以前未见过的NLP任务

    arXiv:2210.17437v3 Announce Type: replace-cross  Abstract: Existing approaches to few-shot learning in NLP rely on large language models and fine-tuning of these to generalise on out-of-distribution data. In this work, we propose a simple yet powerful approach to "extreme" few-shot learning, wherein models are exposed to as little as 4 examples per class, based on soft-label prototypes that collectively capture the distribution of different classes across the input domain space. Inspired by previous work (Sucholutsky et al., 2021) on univariate or simple multivariate (synthetic) data, we propose a novel approach that is effective on large, high-dimensional and real-world datasets. We learn soft-label prototypes within a neural framework (DeepSLP) and we experimentally demonstrate that it achieves superior performance on 31/48 tested tasks and few-shot settings while closely matching the performance of strong baselines on the rest. We focus on learning previously unseen NLP tasks from v
    
[^93]: 自然语言上的多步演绎推理：基于超领域泛化的实证研究

    Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation

    [https://arxiv.org/abs/2207.14000](https://arxiv.org/abs/2207.14000)

    提出了IMA-GloVe-GA，一个用于自然语言表达的多步推理的迭代神经推理网络，在超领域泛化方面具有更好的性能表现。

    

    将深度学习与符号逻辑推理结合起来，旨在充分利用这两个领域的成功，并引起了越来越多的关注。受DeepLogic启发，该模型经过端到端训练，用于执行逻辑程序推理，我们介绍了IMA-GloVe-GA，这是一个用自然语言表达的多步推理的迭代神经推理网络。在我们的模型中，推理是使用基于RNN的迭代内存神经网络进行的，其中包含一个门关注机制。我们在PARARULES、CONCEPTRULES V1和CONCEPTRULES V2三个数据集上评估了IMA-GloVe-GA。实验结果表明，带有门关注机制的DeepLogic比DeepLogic和其他RNN基线模型能够实现更高的测试准确性。我们的模型在规则被打乱时比RoBERTa-Large实现了更好的超领域泛化性能。此外，为了解决当前多步推理数据集中推理深度不平衡的问题

    arXiv:2207.14000v2 Announce Type: replace-cross  Abstract: Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by DeepLogic, an end-to-end model trained to perform inference on logic programs, we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on RNN with a gate attention mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention can achieve higher test accuracy than DeepLogic and other RNN baseline models. Our model achieves better out-of-distribution generalisation than RoBERTa-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datase
    
[^94]: 评估用于基于OSINT的网络威胁意识的LLM聊天机器人

    Evaluation of LLM Chatbots for OSINT-based Cyberthreat Awareness. (arXiv:2401.15127v1 [cs.CR])

    [http://arxiv.org/abs/2401.15127](http://arxiv.org/abs/2401.15127)

    本研究评估了LLM聊天机器人在基于OSINT的网络威胁意识中的应用能力，并发现聊天机器人在网络安全的二分类和命名实体识别任务方面表现出良好的性能。

    

    在快速发展的网络安全领域中，关于新兴威胁的知识共享至关重要，并构成了网络威胁情报的基础。在这个背景下，大型语言模型在网络安全领域越来越重要，提供了广泛的机遇。本研究探讨了ChatGPT、GPT4all、Dolly、Stanford Alpaca、Alpaca-LoRA和Falcon等聊天机器人在识别开源情报中与网络安全相关的文本方面的能力。我们评估了现有聊天机器人模型在自然语言处理任务中的能力。我们考虑了二分类和命名实体识别作为任务。本研究分析了从Twitter收集的经过充分验证的数据，该数据来源于以往的研究工作。在网络安全的二分类问题方面，商业模型Chatbot GPT-4实现了可接受的F1分数0.94，而开源模型GPT4all实现了F1分数0.90。然而，就网络安全实体识别而言，

    Knowledge sharing about emerging threats is crucial in the rapidly advancing field of cybersecurity and forms the foundation of Cyber Threat Intelligence. In this context, Large Language Models are becoming increasingly significant in the field of cybersecurity, presenting a wide range of opportunities. This study explores the capability of chatbots such as ChatGPT, GPT4all, Dolly,Stanford Alpaca, Alpaca-LoRA, and Falcon to identify cybersecurity-related text within Open Source Intelligence. We assess the capabilities of existing chatbot models for Natural Language Processing tasks. We consider binary classification and Named Entity Recognition as tasks. This study analyzes well-established data collected from Twitter, derived from previous research efforts. Regarding cybersecurity binary classification, Chatbot GPT-4 as a commercial model achieved an acceptable F1-score of 0.94, and the open-source GPT4all model achieved an F1-score of 0.90. However, concerning cybersecurity entity re
    
[^95]: 强化微调语言模型中的梯度消失问题

    Vanishing Gradients in Reinforcement Finetuning of Language Models. (arXiv:2310.20703v1 [cs.LG])

    [http://arxiv.org/abs/2310.20703](http://arxiv.org/abs/2310.20703)

    本研究发现在强化微调（RFT）中存在梯度消失的问题，当模型下奖励的标准差较小时，输入的期望梯度会消失，导致奖励最大化缓慢。初始监督微调（SFT）阶段是克服这个问题的最有希望的方法。

    

    预训练的语言模型通过强化微调（RFT）与人类偏好和下游任务对齐，即使用策略梯度算法最大化（可能是学习得到的）奖励函数。本研究发现了RFT中的一个基本的优化障碍：我们证明了当模型下的奖励标准差较小时，输入的期望梯度会消失，即使期望奖励远离最优解。通过在RFT基准和控制环境中进行实验，以及理论分析，我们证明了由于小的奖励标准差导致的梯度消失问题普遍存在且有害，导致奖励最大化极其缓慢。最后，我们探索了克服RFT中梯度消失的方法。我们发现初始监督微调（SFT）阶段是最有希望的候选方法，并且揭示了它在RFT流程中的重要性。此外，我们还表明相对较小的训练数据集的SFT阶段可以有效克服梯度消失问题。

    Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which entails maximizing a (possibly learned) reward function using policy gradient algorithms. This work highlights a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small num
    
[^96]: Davidsonian场景图：改进文本-图像生成的细粒度评估的可靠性

    Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation. (arXiv:2310.18235v1 [cs.CV])

    [http://arxiv.org/abs/2310.18235](http://arxiv.org/abs/2310.18235)

    本论文提出了Davidsonian场景图（DSG）的评估框架，解决了现有文本-图像生成模型评估中的可靠性挑战，包括QG问题的准确性和VQA答案的一致性。

    

    评估文本到图像模型一直是困难的。最近一种用于评估文本-图像忠实度的强大方法是基于QG/A（问题生成和回答），它使用预训练的基础模型自动生成一组问题和答案，并基于这些答案与基于提示的答案在视觉问题回答模型中提取的一致性对输出图像进行评分。这种评估自然上取决于底层QG和QA模型的质量。我们确定并解决了现有QG/A工作中的几个可靠性挑战：（a）QG问题应尊重提示（避免幻觉、重复和遗漏）和（b）VQA答案应一致（不会在图像中宣称没有摩托车，同时声称摩托车是蓝色）。我们通过Davidsonian场景图（DSG），这个受形式语义启发的实证评估框架，解决了这些问题。

    Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG
    
[^97]: CodeChain: 通过代表性子模块的自我修订链路实现模块化代码生成

    CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules. (arXiv:2310.08992v1 [cs.AI])

    [http://arxiv.org/abs/2310.08992](http://arxiv.org/abs/2310.08992)

    CodeChain是一种通过代表性子模块的自我修订链路引导模块化代码生成的新框架，旨在解决大型语言模型在解决复杂编程任务方面的挑战。

    

    大型语言模型（LLM）已经在解决简单编程任务方面非常熟练，比如在HumanEval或MBPP基准测试中的任务。然而，对于更复杂和具有竞争性的编程任务，这些模型仍然面临挑战，可能是因为它们倾向于生成作为整体代码块而不是将其分解为逻辑子任务和子模块。另一方面，有经验的程序员本能地编写具有抽象概念的模块化代码来解决复杂任务，通常会重复使用之前开发的模块。为了解决这一差距，我们提出了CodeChain，一种通过代表性子模块的自我修订链路引导模块化代码生成的新框架。具体而言，CodeChain首先通过链式思考提示指导LLM生成模块化代码。然后，它通过迭代两个步骤实施自我修订链路：1）额外...

    Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks. However, solving more complex and competitive programming tasks is still quite challenging for these models - possibly due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules. To address this gap, we propose CodeChain, a novel framework for inference that elicits modularized code generation through a chain of self-revisions, each being guided by some representative sub-modules generated in previous iterations. Concretely, CodeChain first instructs the LLM to generate modularized codes through chain-of-thought prompting. Then it applies a chain of self-revisions by iterating the two steps: 1) extra
    
[^98]: 在大规模语言模型中摊销难以处理的推理问题

    Amortizing intractable inference in large language models. (arXiv:2310.04363v1 [cs.LG])

    [http://arxiv.org/abs/2310.04363](http://arxiv.org/abs/2310.04363)

    本论文提出了一种使用摊销的贝叶斯推理从难以处理的后验分布中进行抽样的方法，并利用生成流网络来实现大规模语言模型的微调，从而解决了在这些模型中处理推理问题的限制。

    

    自回归的大规模语言模型通过下一个词条件分布来压缩其训练数据中的知识，这限制了对该知识的可处理查询仅限于从头到尾的自回归抽样。然而，许多感兴趣的任务，包括序列延续、填充和其他形式的受约束生成，都涉及从难以处理的后验分布中进行抽样。我们通过使用摊销的贝叶斯推理从这些难以处理的后验分布中进行抽样来解决这个限制。这种摊销通过通过寻求多样性的强化学习算法 - 生成流网络 (GFlowNets) 来微调 LLMs 实现。我们凭经验证明，LLM微调的这种分布匹配范式可以作为最大似然训练和奖励最大化策略优化的有效替代方法。作为一个重要应用，我们将思维链推理解释为潜变量建模问题，并证明了...

    Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest -- including sequence continuation, infilling, and other forms of constrained generation -- involve sampling from intractable posterior distributions. We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets). We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization. As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate 
    
[^99]: 大型语言模型尚不能自我纠正推理错误

    Large Language Models Cannot Self-Correct Reasoning Yet. (arXiv:2310.01798v1 [cs.CL])

    [http://arxiv.org/abs/2310.01798](http://arxiv.org/abs/2310.01798)

    大型语言模型(LLMs)的自我纠正能力在推理方面存在困难，甚至可能在自我纠正后性能下降。

    

    大型语言模型(LLMs)凭借其在各种应用中无可比拟的文本生成能力而成为突破性的技术。然而，对于其生成内容的准确性和适当性仍存在疑虑。自我纠正方法被提出作为解决这些问题的一种方法。本文在此基础上对LLMs内部的自我纠正的作用和效果进行了批判性的考察，揭示了其真正的潜力和限制。我们的研究主要关注内在自我纠正的概念，即LLMs尝试仅仅依靠其固有能力来纠正其初始响应，而不依赖于外部反馈的支持。在推理的背景下，我们的研究表明LLMs在没有外部反馈的情况下很难自我纠正其响应，甚至有时候其表现可能在自我纠正后下降。基于这些洞见，我们对未来的研究提出了建议。

    Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance might even degrade post self-correction. Drawing from these insights, we offer suggestions for future resear
    
[^100]: DyVal: 基于图形信息的大型语言模型动态评估

    DyVal: Graph-informed Dynamic Evaluation of Large Language Models. (arXiv:2309.17167v1 [cs.AI])

    [http://arxiv.org/abs/2309.17167](http://arxiv.org/abs/2309.17167)

    DyVal是一种基于图形信息的大型语言模型动态评估协议，通过动态生成具有可控复杂性的评估样本，评估了各种LLM在推理任务上的性能，发现它们在这些挑战性样本上表现更差。

    

    大型语言模型在各种评估基准中取得了显著的性能。然而，人们对它们的性能提出了担忧，因为它们庞大的训练语料库中可能存在数据污染。此外，当前基准的静态性质和固定复杂性可能无法充分衡量LLM的进步能力。在本文中，我们介绍了DyVal，一种新颖、通用且灵活的动态评估LLM的协议。基于我们提出的动态评估框架，我们利用有向无环图的结构优势构建了基于图形信息的DyVal，以动态生成具有可控复杂性的评估样本。DyVal生成了包括数学、逻辑推理和算法问题在内的具有挑战性的推理任务的评估集。我们评估了从Flan-T5-large到ChatGPT和GPT4的各种LLM。实验表明，LLM在DyVal生成的评估样本上表现更差

    Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns about their performance are raised on potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic evaluation of LLMs. Based on our proposed dynamic evaluation framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments demonstrate that LLMs perform worse in DyVal-generated evaluation samples with di
    
[^101]: K-pop歌词翻译：数据集、分析与神经建模

    K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling. (arXiv:2309.11093v1 [cs.CL])

    [http://arxiv.org/abs/2309.11093](http://arxiv.org/abs/2309.11093)

    研究者介绍了一种新颖的K-pop歌词翻译数据集，该数据集揭示了K-pop歌词翻译的独特特征，并构建了一个神经歌词翻译模型，强调了专用数据集的重要性。

    

    歌词翻译作为一个研究了一个世纪的领域，如今吸引着计算语言学研究者的注意。我们在以往研究中发现了两个限制。首先，在歌词翻译研究中，尽管K-pop非常受欢迎，但主要关注的是西方流派和语言，没有研究集中在K-pop上。其次，歌词翻译领域缺乏可公开获得的数据集；据我们所知，目前尚无此类数据集。为了拓宽歌词翻译研究的流派和语言范围，我们引入了一种新颖的可唱歌词翻译数据集，其中约89%为K-pop歌词。该数据集通过逐行和逐节对齐了韩语和英语歌词。我们利用该数据集揭示了K-pop歌词翻译的独特特征，与其他广泛研究的流派区分开，并构建了一个神经歌词翻译模型，从而强调了专用数据集的重要性。

    Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89\% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for
    
[^102]: 混合Distil-BERT: 用于孟加拉语、英语和印地语的混合语言建模

    Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and Hindi. (arXiv:2309.10272v1 [cs.CL])

    [http://arxiv.org/abs/2309.10272](http://arxiv.org/abs/2309.10272)

    本文介绍了Tri-Distil-BERT和Mixed-Distil-BERT两个模型，Tri-Distil-BERT是一个在孟加拉语、英语和印地语上预训练的多语言模型，Mixed-Distil-BERT是一个在混合编码数据上微调的模型。这两个模型在多个自然语言处理任务上表现出与更大的模型相竞争的性能。

    

    在自然语言处理领域中，文本分类是最受欢迎的下游任务之一。当文本是混合编码时，文本分类任务变得更加困难。虽然在预训练过程中没有接触到这种文本，但不同的BERT模型已经成功地解决了混合编码的自然语言处理挑战。再次，为了提高性能，混合编码自然语言处理模型已经依赖于将合成数据与真实数据相结合。当BERT模型使用相应的混合编码语言进行预训练时，了解它们的性能受到了怎样的影响是至关重要的。在本文中，我们介绍了Tri-Distil-BERT，一个在孟加拉语、英语和印地语上预训练的多语言模型，以及Mixed-Distil-BERT，一个在混合编码数据上微调的模型。这两个模型在多个自然语言处理任务上进行了评估，并展示出与更大的模型如mBERT和XLM-R相竞争的性能。我们的两层预训练方法为多语言任务提供了高效的替代选择。

    One of the most popular downstream tasks in the field of Natural Language Processing is text classification. Text classification tasks have become more daunting when the texts are code-mixed. Though they are not exposed to such text during pre-training, different BERT models have demonstrated success in tackling Code-Mixed NLP challenges. Again, in order to enhance their performance, Code-Mixed NLP models have depended on combining synthetic data with real-world data. It is crucial to understand how the BERT models' performance is impacted when they are pretrained using corresponding code-mixed languages. In this paper, we introduce Tri-Distil-BERT, a multilingual model pre-trained on Bangla, English, and Hindi, and Mixed-Distil-BERT, a model fine-tuned on code-mixed data. Both models are evaluated across multiple NLP tasks and demonstrate competitive performance against larger models like mBERT and XLM-R. Our two-tiered pre-training approach offers efficient alternatives for multiling
    
[^103]: 大型语言模型的指令调优：一项调研

    Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10792](http://arxiv.org/abs/2308.10792)

    本文调查了指令调优这一关键技术在增强大型语言模型能力和可控性方面的研究工作，包括方法、数据集构建、模型训练和应用，以及对结果影响的分析。同时回顾了可能的问题和批评，并指出了目前的不足。

    

    本文调查了指令调优（IT）这一快速发展的领域中的研究工作，这是一种增强大型语言模型（LLM）能力和可控性的关键技术。指令调优是指以监督方式在包含“指令-输出”对的数据集上进一步训练LLM，这将LLM的下一个词预测目标与用户希望LLM遵守人类指令的目标之间的差距。本文对IT的常规方法、IT数据集的构建、IT模型的训练以及应用于不同模态、领域和应用的情况进行了系统的文献综述，并对影响IT结果的各个方面进行了分析（例如，指令输出的生成、指令数据集的大小等）。我们还回顾了IT的潜在问题以及针对其的批评，以及指出当前不足的努力。

    This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies 
    
[^104]: 通过角色扮演提示提高零-shot推理能力

    Better Zero-Shot Reasoning with Role-Play Prompting. (arXiv:2308.07702v1 [cs.CL])

    [http://arxiv.org/abs/2308.07702](http://arxiv.org/abs/2308.07702)

    通过角色扮演提示，研究评估了现代大型语言模型在零-shot推理场景下的表现，并发现角色扮演提示在多个推理基准测试中都超越了标准的零-shot方法。

    

    现代大型语言模型（LLM），如ChatGPT，展示了出色的角色扮演能力，使其能够扮演不仅是人类角色，还包括像Linux终端这样的非人角色。这种多功能性使它们能够在不同的上下文中模拟复杂的人类交互和行为，并仿真特定的对象或系统。尽管这些能力增强了用户参与度并引入了新的交互模式，但角色扮演对LLM的推理能力的影响仍有待深入探究。在本研究中，我们引入了一种策略性设计的角色扮演提示方法，并在十二个不同的推理基准测试中评估其在零-shot设置下的性能，涵盖了算术、常识推理、符号推理等多个领域。利用ChatGPT和Llama 2等模型，我们的实证结果表明，角色扮演提示在大多数数据集上始终优于标准的零-shot方法。值得注意的是，一个

    Modern large language models (LLMs), such as ChatGPT, exhibit a remarkable capacity for role-playing, enabling them to embody not only human characters but also non-human entities like a Linux terminal. This versatility allows them to simulate complex human-like interactions and behaviors within various contexts, as well as to emulate specific objects or systems. While these capabilities have enhanced user engagement and introduced novel modes of interaction, the influence of role-playing on LLMs' reasoning abilities remains underexplored. In this study, we introduce a strategically designed role-play prompting methodology and assess its performance under the zero-shot setting across twelve diverse reasoning benchmarks, encompassing arithmetic, commonsense reasoning, symbolic reasoning, and more. Leveraging models such as ChatGPT and Llama 2, our empirical results illustrate that role-play prompting consistently surpasses the standard zero-shot approach across most datasets. Notably, a
    
[^105]: MO-VLN:一个用于开放集合零样本视觉和语言导航的多任务基准 (arXiv:2306.10322v2 [cs.CV] 更新)

    MO-VLN: A Multi-Task Benchmark for Open-set Zero-Shot Vision-and-Language Navigation. (arXiv:2306.10322v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.10322](http://arxiv.org/abs/2306.10322)

    MO-VLN是一个用于评估通用机器人在多任务环境中的视觉和语言导航的基准，通过使用虚幻引擎5开发逼真的场景和包含多种不常见物体来测试其效果和泛化能力。

    

    给定一个自然语言，一个通用的机器人必须理解指令并根据视觉观察找到目标对象或位置，即使在未探索的环境中也能做到。大多数代理依赖于大量多样的训练数据以实现更好的泛化，这需要昂贵的劳动力。这些代理通常只关注常见的对象和较少的任务，因此不足以处理不同类型的指令。为了促进开放集合视觉和语言导航的研究，我们提出了一个名为MO-VLN的基准，旨在测试代理在多任务设置中的有效性和泛化能力。首先，我们使用虚幻引擎5开发了一个3D模拟器，渲染了逼真的场景，包含更真实的光照和细节。模拟器包含三个场景，即咖啡馆、餐厅和养老院，这些场景在工业中具有很高的价值。此外，我们的模拟器涉及多种不常见的物体，如外卖杯和医用胶带，这些物体更加复杂。

    Given a natural language, a general robot has to comprehend the instruction and find the target object or location based on visual observations even in unexplored environments. Most agents rely on massive diverse training data to achieve better generalization, which requires expensive labor. These agents often focus on common objects and fewer tasks, thus are not intelligent enough to handle different types of instructions. To facilitate research in open-set vision-and-language navigation, we propose a benchmark named MO-VLN, aiming at testing the effectiveness and generalization of the agent in the multi-task setting. First, we develop a 3D simulator rendered by realistic scenarios using Unreal Engine 5, containing more realistic lights and details. The simulator contains three scenes, i.e., cafe, restaurant, and nursing house, of high value in the industry. Besides, our simulator involves multiple uncommon objects, such as takeaway cup and medical adhesive tape, which are more compli
    
[^106]: 通过多语言微调和翻译指令诱发大规模语言模型的翻译能力

    Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions. (arXiv:2305.15083v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15083](http://arxiv.org/abs/2305.15083)

    本文通过对多语言预训练语言模型进行微调，研究了它们如何通过翻译指令执行多语言翻译任务。研究发现多语言LLMs具有较强的翻译能力，这取决于语言与英语的相似性和预训练阶段使用的数据量。此外，执行翻译指令的能力依赖于对指令的理解和不同语言之间的对齐。

    

    大规模预训练语言模型（LLMs），例如ChatGPT和GPT4，展现出在多语言翻译方面的强大能力，而无需明确训练并行语料库。本文研究了LLMs如何获得其对不同语言进行翻译指令的能力。我们通过对多语言预训练语言模型XGLM-7B进行微调来执行多语言翻译任务，并进行了详细分析。首先，我们展示了多语言LLMs具有比先前展示的更强的翻译能力。对于某种语言，其表现取决于其与英语的相似性和预训练阶段使用的数据量。其次，我们发现LLMs执行翻译指令的能力依赖于对翻译指令的理解以及不同语言之间的对齐。通过多语言微调，LLMs能够学习并在翻译任务中表现出良好的能力，即使对于那些语言间平行语料较少的情况。

    Large-scale Pretrained Language Models (LLMs), such as ChatGPT and GPT4, have shown strong abilities in multilingual translations, without being explicitly trained on parallel corpora. It is interesting how the LLMs obtain their ability to carry out translation instructions for different languages. In this paper, we present a detailed analysis by finetuning a multilingual pretrained language model, XGLM-7B, to perform multilingual translation following given instructions. Firstly, we show that multilingual LLMs have stronger translation abilities than previously demonstrated. For a certain language, the performance depends on its similarity to English and the amount of data used in the pretraining phase. Secondly, we find that LLMs' ability to carry out translation instructions relies on the understanding of translation instructions and the alignment among different languages. With multilingual finetuning, LLMs could learn to perform the translation task well even for those language pa
    
[^107]: LogicLLM：探索自监督逻辑增强训练的大语言模型

    LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models. (arXiv:2305.13718v1 [cs.CL])

    [http://arxiv.org/abs/2305.13718](http://arxiv.org/abs/2305.13718)

    本文介绍了 LogicLLM，一种通过自监督后训练来提高大语言模型的逻辑推理能力的方法，该方法有效地在常见逻辑推理任务上进行表现，超过了目前最先进的无监督基线方法。

    

    改善语言模型的逻辑推理能力的现有努力主要依赖于有监督微调，这阻碍了将模型泛化到新的领域和/或任务。然而，通过发展大语言模型（LLM）已经证明了将丰富的知识压缩为单个代理的能力，使它们能够有效地处理多个任务。然而，我们的初步实验表明，LLMs 在逻辑推理方面并没有表现出能力。LLMs 在逻辑推理基准测试中的表现远远落后于现有的最先进基线。在本文中，我们首次尝试通过自监督后训练来探索融合逻辑知识的可行性，并通过上下文学习来激活它，我们将其称为LogicLLM。具体来说，我们设计了一个MERIt 的自回归目标变体，并将其与两个LLM系列FLAN-T5和LLaMA集成在一起，参数大小范围从30亿到130亿。实验结果表明，我们的方法在常用推理策略上与目前最先进的有监督方法相当，并且远远超过了目前最先进的无监督基线方法。

    Existing efforts to improve logical reasoning ability of language models have predominantly relied on supervised fine-tuning, hindering generalization to new domains and/or tasks. The development of Large Langauge Models (LLMs) has demonstrated the capacity of compressing abundant knowledge into a single proxy, enabling them to tackle multiple tasks effectively. Our preliminary experiments, nevertheless, show that LLMs do not show capability on logical reasoning. The performance of LLMs on logical reasoning benchmarks is far behind the existing state-of-the-art baselines. In this paper, we make the first attempt to investigate the feasibility of incorporating logical knowledge through self-supervised post-training, and activating it via in-context learning, which we termed as LogicLLM. Specifically, we devise an auto-regressive objective variant of MERIt and integrate it with two LLM series, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to 13 billion. The results 
    
[^108]: 区分和回答：通过辨别器缓解检索增强模型中虚假信息的影响

    Discern and Answer: Mitigating the Impact of Misinformation in Retrieval-Augmented Models with Discriminators. (arXiv:2305.01579v1 [cs.CL])

    [http://arxiv.org/abs/2305.01579](http://arxiv.org/abs/2305.01579)

    本文研究了现有检索增强语言模型假设所有检索信息都是正确的假设的问题，在实际应用中可能存在虚假信息导致冲突的情况下，提出了通过精细调整鉴别器和提示鉴别能力引出鲁棒性的方法，这显著改善了模型在知识冲突下的效果；同时提供了关于交替精细调整模型和上下文学习的新的结论。

    

    大多数现有的检索增强语言模型（LM）假定所有检索到的信息都是事实上正确的。本文研究一个更加现实的场景，即检索到的文档可能包含虚假信息，从而导致它们之间存在冲突。我们观察到，现有模型在精调和上下文少样本学习设置中对这种信息高度脆弱。我们提出了一些方法，通过明确地对鉴别器进行精细调整或提示来引出GPT-3的鉴别能力，使检索增强LM对虚假信息具有鲁棒性。我们在开放域问答方面的实证结果表明，这些方法显著改善了LM对知识冲突的鲁棒性。我们还提供了关于交替精细调整模型的决策与上下文学习过程的发现，为利用两者的最佳方式铺平了新的道路。

    Most existing retrieval-augmented language models (LMs) for question answering assume all retrieved information is factually correct. In this work, we study a more realistic scenario in which retrieved documents may contain misinformation, causing conflicts among them. We observe that the existing models are highly brittle to such information in both fine-tuning and in-context few-shot learning settings. We propose approaches to make retrieval-augmented LMs robust to misinformation by explicitly fine-tuning a discriminator or prompting to elicit discrimination capability in GPT-3. Our empirical results on open-domain question answering show that these approaches significantly improve LMs' robustness to knowledge conflicts. We also provide our findings on interleaving the fine-tuned model's decision with the in-context learning process, paving a new path to leverage the best of both worlds.
    
[^109]: 多标签文本分类的标签相依感知集合预测网络

    Label Dependencies-aware Set Prediction Networks for Multi-label Text Classification. (arXiv:2304.07022v1 [cs.CL])

    [http://arxiv.org/abs/2304.07022](http://arxiv.org/abs/2304.07022)

    本论文提出了一种标签相依感知集合预测网络用于解决多标签文本分类问题。该方法将多标签分类视为直接集合预测问题，通过标签之间的统计关系构建关联矩阵并结合GCN学习标签信息，同时利用句子信息和标签信息，最终结果表明其性能优于以前的方法。

    

    多标签文本分类旨在从句子中提取所有相关标签，可视为序列生成问题。然而，训练集中的标签是无序的。我们建议将其视为直接集合预测问题，而不需要考虑标签的顺序。此外，为了建模标签之间的关联，通过标签之间的统计关系构建关联矩阵，并使用GCN来学习标签信息。基于所学的标签信息，集合预测网络可以同时利用句子信息和标签信息进行多标签文本分类。此外，还对集合预测网络的输出概率分布施加广义巴氏距离，以提高其召回率。在四个多标签数据集上的实验结果表明了所提出方法的有效性，并且其性能大大优于以前的方法。

    Multi-label text classification aims to extract all the related labels from a sentence, which can be viewed as a sequence generation problem. However, the labels in training dataset are unordered. We propose to treat it as a direct set prediction problem and don't need to consider the order of labels. Besides, in order to model the correlation between labels, the adjacency matrix is constructed through the statistical relations between labels and GCN is employed to learn the label information. Based on the learned label information, the set prediction networks can both utilize the sentence information and label information for multi-label text classification simultaneously. Furthermore, the Bhattacharyya distance is imposed on the output probability distributions of the set prediction networks to increase the recall ability. Experimental results on four multi-label datasets show the effectiveness of the proposed method and it outperforms previous method a substantial margin.
    
[^110]: 对数线性保护性及其影响的研究

    Log-linear Guardedness and its Implications. (arXiv:2210.10012v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10012](http://arxiv.org/abs/2210.10012)

    本研究介绍了对数线性保护性及其对下游分类器行为的影响。在二元情况下，下游对数线性模型无法恢复被删除的概念，但在某些情况下，可以通过构建多类对数线性模型间接恢复概念。这些结果揭示了线性删除方法的局限性，并强调了进一步研究的需求。

    

    已经发现，在假设可线性的神经表示中，从中删除可人解释的概念的方法是可行和有用的。然而，这种删除对于基于修改后表示进行训练的下游分类器行为的影响尚未完全理解。在这项工作中，我们正式定义了对数线性保护性的概念，即对手无法直接从表示中预测概念的能力，并研究其影响。我们证明，在二元情况下，在某些假设下，下游对数线性模型无法恢复被删除的概念。然而，我们证明，在某些情况下，可以构建一个多类对数线性模型，间接恢复概念，这指出了对数线性保护性作为下游偏差缓解技术的内在局限性。这些发现揭示了线性删除方法的理论限制，并强调了进一步研究可解释神经表示与分类器之间的联系的需要。

    Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful. However, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood. In this work, we formally define the notion of log-linear guardedness as the inability of an adversary to predict the concept directly from the representation, and study its implications. We show that, in the binary case, under certain assumptions, a downstream log-linear model cannot recover the erased concept. However, we demonstrate that a multiclass log-linear model \emph{can} be constructed that indirectly recovers the concept in some cases, pointing to the inherent limitations of log-linear guardedness as a downstream bias mitigation technique. These findings shed light on the theoretical limitations of linear erasure methods and highlight the need for further research on the connections between int
    
[^111]: Kernelized Concept Erasure. (arXiv:2201.12191v4 [cs.LG] UPDATED)

    Kernelized Concept Erasure. (arXiv:2201.12191v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12191](http://arxiv.org/abs/2201.12191)

    通过核化线性极小极大博弈，防止特定非线性对手预测概念，但无法彻底解决非线性编码的概念擦除问题。

    

    文本数据的神经模型的表示空间在训练过程中以无监督的方式出现。理解这些表示如何编码可解释的人类概念是一个基本问题。识别神经表示中的概念的一种明显方法是搜索一个线性子空间，其擦除会阻止从表示中预测概念。然而，尽管许多线性擦除算法是可处理和可解释的，但神经网络未必以线性方式表示概念。为了识别非线性编码的概念，我们提出了一个核化的概念擦除线性极小极大博弈。我们证明了可以防止特定的非线性对手预测概念。然而，这种保护不会转移到不同的非线性对手。因此，彻底地擦除非线性编码的概念仍然是一个待解决的问题。

    The representation space of neural models for textual data emerges in an unsupervised manner during training. Understanding how those representations encode human-interpretable concepts is a fundamental problem. One prominent approach for the identification of concepts in neural representations is searching for a linear subspace whose erasure prevents the prediction of the concept from the representations. However, while many linear erasure algorithms are tractable and interpretable, neural networks do not necessarily represent concepts in a linear manner. To identify non-linearly encoded concepts, we propose a kernelization of a linear minimax game for concept erasure. We demonstrate that it is possible to prevent specific non-linear adversaries from predicting the concept. However, the protection does not transfer to different nonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded concept remains an open problem.
    

