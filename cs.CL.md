# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Direct and indirect evidence of compression of word lengths. Zipf's law of abbreviation revisited.](http://arxiv.org/abs/2303.10128) | 这篇论文重新审视了书面语与缩写定律的一致性，并发现这一定律也适用于口语。结果提供了压缩语言的间接证据，即缩写定律是最优编码的预测，而通过英语的历史研究还发现，人们在语言中实际使用的词汇数量正在缩减。 |
| [^2] | [Enhancing the Role of Context in Region-Word Alignment for Object Detection.](http://arxiv.org/abs/2303.10093) | 本研究提出了一种增强上下文在目标检测区域-词对齐中作用的方法，通过特定的负采样方法提高了属性的作用，从而提高了目标检测的效果。 |
| [^3] | [More Robust Schema-Guided Dialogue State Tracking via Tree-Based Paraphrase Ranking.](http://arxiv.org/abs/2303.09905) | 该论文提出了一个基于树形语义匹配的模式引导对话状态追踪模型，通过生成合成模式并结合训练数据，提高了模型的稳健性和泛化能力，实现在SGD-X基准测试中提升联合目标准确率和模式灵敏度。 |
| [^4] | [mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection.](http://arxiv.org/abs/2303.09901) | 本研究提出了mCPT模型用于多语言的、多标签的零样本或少样本的框架检测任务，并在西班牙语和其他8种语言中取得了良好的成绩。该方案采用了基于多语言变压器的预训练程序，使用标签感知对比损失函数。 |
| [^5] | [Memotion 3: Dataset on sentiment and emotion analysis of codemixed Hindi-English Memes.](http://arxiv.org/abs/2303.09892) | Memotion 3是一个包含10,000个已注释模因的新数据集，引入了印度-英语混合模因，使其成为该领域内首个相应的数据集。此数据集可用于情感和情绪分析，并可用于对社交媒体上的虚假信息或仇恨内容进行研究。 |
| [^6] | [Trained on 100 million words and still in shape: BERT meets British National Corpus.](http://arxiv.org/abs/2303.09859) | 本文探讨了在英国国家语料库上预训练的效果，并展示它可以比原始BERT模型达到更好的表现。在公平、可重复且数据有效的比较研究中，他们证明了这样的语料库有作为语言建模基准的巨大潜力。他们提出了一个经过优化的LM体系结构称为LTG-BERT。 |
| [^7] | [DORIC : Domain Robust Fine-Tuning for Open Intent Clustering through Dependency Parsing.](http://arxiv.org/abs/2303.09827) | 该论文提出了一种名为DORIC的方法，通过利用多领域对话数据集进行微调，提取动词-宾语对以达到消除不必要信息的目的，最终实现了在各种领域数据集上的高精度聚类。 |
| [^8] | [Transformers and Ensemble methods: A solution for Hate Speech Detection in Arabic languages.](http://arxiv.org/abs/2303.09823) | 本文提出了一种使用Transformer和Ensemble方法的解决方案，用于阿语恶意言论的检测。实验结果表明，基于多数表决的集成方法具有最佳效果，其在测试集上的准确率为0.86，F1分数为0.60。 |
| [^9] | [CoLT5: Faster Long-Range Transformers with Conditional Computation.](http://arxiv.org/abs/2303.09752) | CoLT5是一种基于条件计算的Transformer模型，通过优先处理重要标记来加速长距离输入的处理。CoLT5在SCROLLS基准测试上表现最好，并能够有效地处理长达64k输入长度。 |
| [^10] | [Learning towards Selective Data Augmentation for Dialogue Generation.](http://arxiv.org/abs/2303.09719) | 本文提出了一种面向对话生成的有选择数据增强框架（SDA），通过选择性增强低质量和具有代表性的案例，可以在神经对话模型训练中大幅提升响应生成任务的性能。 |
| [^11] | [Neural Architecture Search for Effective Teacher-Student Knowledge Transfer in Language Models.](http://arxiv.org/abs/2303.09639) | 本论文提出一种KD-NAS方法，使用神经架构搜索指导知识蒸馏过程，并找到最优的学生模型，从而在资源受限环境下实现高效师生知识转移，超过手工设计的学生模型和大型教师模型。 |
| [^12] | [HIVE: Harnessing Human Feedback for Instructional Visual Editing.](http://arxiv.org/abs/2303.09618) | 本文提出了一种新的框架，利用人类反馈进行指导性视觉编辑。通过收集被编辑图像的人类反馈，并学习奖励函数捕捉用户的偏好，可以缓解数据限制所带来的偏差，并提高模型性能。 |
| [^13] | [Psychotherapy AI Companion with Reinforcement Learning Recommendations and Interpretable Policy Dynamics.](http://arxiv.org/abs/2303.09601) | 本文介绍了一种使用强化学习生成心理治疗主题推荐的AI伴侣，能够很好地捕获真实数据，并通过可解释的策略轨迹可视化提供对不同奖励信号和不同临床诊断下训练的策略的独特模式。 |
| [^14] | [Towards Robust Bangla Complex Named Entity Recognition.](http://arxiv.org/abs/2303.09306) | 本研究构建了基于 CRF 和深度学习（如 BanglaBERT）的鲁棒孟加拉复杂命名实体识别模型，解决了 CNER 任务，填补了孟加拉语复杂命名实体识别领域的空白。 |
| [^15] | [PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented Dialogs.](http://arxiv.org/abs/2303.08954) | PRESTO是一个多语言数据集，包含超过55万个人与虚拟助手之间的上下文多语言对话。该数据集可以帮助解决如说话不连贯、代码切换和修正等真实NLU任务中出现的挑战。 |
| [^16] | [E2E Spoken Entity Extraction for Virtual Agents.](http://arxiv.org/abs/2302.10186) | 本文研究了利用预训练语音编码器从语音中直接提取实体的方法，无需文本转录，且在口语实体识别任务中表现优异。 |
| [^17] | [Greedy Ordering of Layer Weight Matrices in Transformers Improves Translation.](http://arxiv.org/abs/2302.02123) | 本论文提出了一种基于贪婪重排权重矩阵的算法AEIUOrder，能够最大化总的经过充分训练的层所贡献的"well-trainedness"指标进行优化，从而提高翻译质量并在各种翻译任务上达到最佳性能。 |
| [^18] | [Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-shot Prompt Learning for Automatic Scoring in Science Education.](http://arxiv.org/abs/2301.08771) | 本研究提出了一种零样本学习自动评分的方法，利用预训练的语言模型配合匹配标本作为下一句预测技术，成功应用于科学教育领域的论证任务，极大地减少了训练成本和时间。 |
| [^19] | [Multi-Scales Data Augmentation Approach In Natural Language Inference For Artifacts Mitigation And Pre-Trained Model Optimization.](http://arxiv.org/abs/2212.08756) | 本文通过多尺度数据增强方法纠偏了预训练模型在自然语言推理中遇到的数据集人为制造效应，提高了模型对扰动测试的抵抗力。 |
| [^20] | [Effectiveness of Text, Acoustic, and Lattice-based representations in Spoken Language Understanding tasks.](http://arxiv.org/abs/2212.08489) | 本文研究了在口语理解任务中使用文本、声学和基于lattice的表示方法，结果表明使用更丰富的自动语音识别输出形式可以获得更好的性能。 |
| [^21] | [Analysis of Utterance Embeddings and Clustering Methods Related to Intent Induction for Task-Oriented Dialogue.](http://arxiv.org/abs/2212.02021) | 本文旨在研究任务导向对话中的意图识别问题，并提出两个关键因素：聚类算法和用户话语嵌入空间。实验证明，利用预训练的MiniLM与层次聚类相结合可以显著提高意图归纳任务的效果。 |
| [^22] | [Who are you referring to? Coreference resolution in image narrations.](http://arxiv.org/abs/2211.14563) | 本研究介绍了一个新的数据集和技术来解决图像叙述中的指代消解问题，提出的模型在解决这一任务中表现出较强的性能和优势，指代消解有助于提高图片中的叙述表达能力。 |
| [^23] | [GPT-3-driven pedagogical agents for training children's curious question-asking skills.](http://arxiv.org/abs/2211.14228) | 该研究探索了使用自然语言处理技术自动生成儿童好奇心问题提问培训的教育内容，该方法显示出了很高的相关性和教育价值。 |
| [^24] | [Comparative layer-wise analysis of self-supervised speech models.](http://arxiv.org/abs/2211.03929) | 本论文通过比较不同自监督语音模型的逐层中间表示，发现了不同模型在编码声学、语音和单词级属性上的差异，并发现这些差异与预训练目标的选择相关。通过比较属性趋势和语音识别和口语理解任务的性能，我们发现CCA趋势为选择层次提供了可靠的指导。 |
| [^25] | [Leveraging Large Language Models for Multiple Choice Question Answering.](http://arxiv.org/abs/2210.12353) | 本文研究了利用大型语言模型进行多项选择题的答案推断，并探讨了一种更自然的问题提示方式，以提高准确性。 |
| [^26] | [TabLLM: Few-shot Classification of Tabular Data with Large Language Models.](http://arxiv.org/abs/2210.10723) | 本文应用大语言模型将表格数据序列化为自然语言字符串进行分类，微调后即可在非常少的样本设置下与传统基线方法竞争力十足。 |
| [^27] | [Treeformer: Dense Gradient Trees for Efficient Attention Computation.](http://arxiv.org/abs/2208.09015) | 本文提出了Treeformer，一种基于决策树的分层导航方法，用于高效地计算注意力。与传统的注意力计算方法相比，Treeformer 可以将检索成本从线性降为近似对数级别，并提供两种有效的关注层。算法的目标是处理输入序列长度过长的应用，提高计算效率。 |
| [^28] | [The Maximum Linear Arrangement Problem for trees under projectivity and planarity.](http://arxiv.org/abs/2206.06924) | 该论文提出了一种解决在平面性和投影性定义下树的最大线性排列问题的算法，证明了最大投影和平面排列的多个性质，发现毛毛虫树最优，推广了之前的极值结果。 |
| [^29] | [Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding.](http://arxiv.org/abs/2109.01636) | 研究开发了一种分布感知词嵌入，并实施了三种不同的方法来利用NER框架中的分布信息，实验表明将词的特异性融入NER方法可提高NER的性能。 |

# 详细

[^1]: 直接和间接证据表明词汇长度被压缩. 对 Zipf的缩写定律的重新审视.

    Direct and indirect evidence of compression of word lengths. Zipf's law of abbreviation revisited. (arXiv:2303.10128v1 [cs.CL])

    [http://arxiv.org/abs/2303.10128](http://arxiv.org/abs/2303.10128)

    这篇论文重新审视了书面语与缩写定律的一致性，并发现这一定律也适用于口语。结果提供了压缩语言的间接证据，即缩写定律是最优编码的预测，而通过英语的历史研究还发现，人们在语言中实际使用的词汇数量正在缩减。

    

    Zipf缩写定律指的是更常见的单词更短，是语言普遍性的最坚实的候选者，它有可能是没有例外或者例外非常少的语言普遍性。自从Zipf的开创性研究以来，这一定律一直被认为是通信的普遍原理的表现，即通过缩短词汇长度来减少通信的努力。在这里，我们重新审视了书面语与缩写定律之间的一致性。关键地，我们提供更广泛的证据，表明这一定律也适用于口语（当用时间来测量词汇长度时），特别是适用于来自14个语言家族的46种语言。与缩写定律的一致性提供了压缩语言的间接证据，这是通过理论论证得出的，即缩写定律是最优编码的预测。鉴于需要直接证据来证明压缩，我们通过英语的历史研究发现，人们在语言中实际使用的词汇数量正在缩减。

    Zipf's law of abbreviation, the tendency of more frequent words to be shorter, is one of the most solid candidates for a linguistic universal, in the sense that it has the potential for being exceptionless or with a number of exceptions that is vanishingly small compared to the number of languages on Earth. Since Zipf's pioneering research, this law has been viewed as a manifestation of a universal principle of communication, i.e. the minimization of word lengths, to reduce the effort of communication. Here we revisit the concordance of written language with the law of abbreviation. Crucially, we provide wider evidence that the law holds also in speech (when word length is measured in time), in particular in 46 languages from 14 linguistic families. Agreement with the law of abbreviation provides indirect evidence of compression of languages via the theoretical argument that the law of abbreviation is a prediction of optimal coding. Motivated by the need of direct evidence of compressi
    
[^2]: 提高上下文在目标检测区域-词对齐中的作用

    Enhancing the Role of Context in Region-Word Alignment for Object Detection. (arXiv:2303.10093v1 [cs.CV])

    [http://arxiv.org/abs/2303.10093](http://arxiv.org/abs/2303.10093)

    本研究提出了一种增强上下文在目标检测区域-词对齐中作用的方法，通过特定的负采样方法提高了属性的作用，从而提高了目标检测的效果。

    

    视觉语言预训练学习图像-标注配对之间的细粒度区域-词对齐，推动了开放词汇目标检测的进展。我们观察到，区域-词对齐方法通常仅针对目标名词在检测中使用，其他上下文，例如属性，对检测的影响不明确。在本研究中，我们探讨了语言上下文如何影响下游目标检测，并提议增强上下文的作用。特别地，我们展示了如何策略性地将接地预训练目标情境化以实现更好的对齐。我们进一步研究了属性作为特别有用的目标上下文并提出了一种新的基于形容词和名词的负采样策略，以增加对它们的对比学习的关注。总的来说，与区域-词预训练的最新技术相比，我们的方法提升了目标检测的效果。我们还通过文本-区域可视化显示属性敏感模型的细粒度实用性。

    Vision-language pretraining to learn a fine-grained, region-word alignment between image-caption pairs has propelled progress in open-vocabulary object detection. We observe that region-word alignment methods are typically used in detection with respect to only object nouns, and the impact of other rich context in captions, such as attributes, is unclear. In this study, we explore how language context affects downstream object detection and propose to enhance the role of context. In particular, we show how to strategically contextualize the grounding pretraining objective for improved alignment. We further hone in on attributes as especially useful object context and propose a novel adjective and noun-based negative sampling strategy for increasing their focus in contrastive learning. Overall, our methods enhance object detection when compared to the state-of-the-art in region-word pretraining. We also highlight the fine-grained utility of an attribute-sensitive model through text-regi
    
[^3]: 基于树形语义匹配的模式引导对话状态追踪能力更强大的方法

    More Robust Schema-Guided Dialogue State Tracking via Tree-Based Paraphrase Ranking. (arXiv:2303.09905v1 [cs.CL])

    [http://arxiv.org/abs/2303.09905](http://arxiv.org/abs/2303.09905)

    该论文提出了一个基于树形语义匹配的模式引导对话状态追踪模型，通过生成合成模式并结合训练数据，提高了模型的稳健性和泛化能力，实现在SGD-X基准测试中提升联合目标准确率和模式灵敏度。

    

    模式引导范式通过包含任务相关的自然语言描述的层次结构模式，克服了使用静态本体构建面向任务的对话（TOD）代理时存在的可扩展性问题。精细调整的语言模型在模式引导对话状态追踪（DST）方面表现出色，但对模式的书写风格很敏感。本文探讨了提高DST模型稳健性的方法。我们提出了一个框架来生成使用基于树形排名的合成模式，以共同优化词汇多样性和语义忠实度。实验结果表明，在SGD-X基准测试中，使用我们的框架生成提示来增加其训练数据后，强基线的泛化能力得到了提高，平均联合目标准确率（JGA）和模式灵敏度（SS）均有显着提升。

    The schema-guided paradigm overcomes scalability issues inherent in building task-oriented dialogue (TOD) agents with static ontologies. Instead of operating on dialogue context alone, agents have access to hierarchical schemas containing task-relevant natural language descriptions. Fine-tuned language models excel at schema-guided dialogue state tracking (DST) but are sensitive to the writing style of the schemas. We explore methods for improving the robustness of DST models. We propose a framework for generating synthetic schemas which uses tree-based ranking to jointly optimise lexical diversity and semantic faithfulness. The generalisation of strong baselines is improved when augmenting their training data with prompts generated by our framework, as demonstrated by marked improvements in average joint goal accuracy (JGA) and schema sensitivity (SS) on the SGD-X benchmark.
    
[^4]: SemEval-2023任务3上的mCPT：用于零样本和少样本框架检测的多语言标签感知对比预训练变压器

    mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v1 [cs.CL])

    [http://arxiv.org/abs/2303.09901](http://arxiv.org/abs/2303.09901)

    本研究提出了mCPT模型用于多语言的、多标签的零样本或少样本的框架检测任务，并在西班牙语和其他8种语言中取得了良好的成绩。该方案采用了基于多语言变压器的预训练程序，使用标签感知对比损失函数。

    

    本文介绍了零样本的西班牙语框架检测任务的获胜系统，并在另外八种语言中取得了良好的成绩。框架检测任务的挑战在于在只有少量或零个样本的情况下识别一组14个框架，即多语言多标签的少样本和零样本设置。我们开发的解决方案采用了基于多语言变压器的预训练程序，使用标签感知对比损失函数。除了描述系统外，我们还进行了嵌入空间分析和消融研究，以展示我们的预训练程序如何支持框架检测以推进计算框架分析。

    This paper presents the winning system for the zero-shot Spanish framing detection task, which also achieves competitive places in eight additional languages. The challenge of the framing detection task lies in identifying a set of 14 frames when only a few or zero samples are available, i.e., a multilingual multi-label few- or zero-shot setting. Our developed solution employs a pre-training procedure based on multilingual Transformers using a label-aware contrastive loss function. In addition to describing the system, we perform an embedding space analysis and ablation study to demonstrate how our pre-training procedure supports framing detection to advance computational framing analysis.
    
[^5]: Memotion 3: 代表印度-英语混合码的情感与情绪分析的互联网模因数据集

    Memotion 3: Dataset on sentiment and emotion analysis of codemixed Hindi-English Memes. (arXiv:2303.09892v1 [cs.CL])

    [http://arxiv.org/abs/2303.09892](http://arxiv.org/abs/2303.09892)

    Memotion 3是一个包含10,000个已注释模因的新数据集，引入了印度-英语混合模因，使其成为该领域内首个相应的数据集。此数据集可用于情感和情绪分析，并可用于对社交媒体上的虚假信息或仇恨内容进行研究。

    

    模因是现今社交媒体上传达幽默的新型机制。模因通常包含图片和一些文本。模因可被用于传播虚假信息或仇恨，因此对其进行详细的研究非常关键。我们介绍了Memotion 3，这是一个包含10,000个已注释模因的新数据集。与领域内其他普遍的数据集不同，包括之前的Memotion，Memotion 3引入了印度-英语混合模因，而之前的研究仅限于英语模因。我们描述了Memotion任务、数据收集和数据集创建方法。我们还为任务提供了一个基准。基准代码和数据集将在 https://github.com/Shreyashm16/Memotion-3.0 上提供。

    Memes are the new-age conveyance mechanism for humor on social media sites. Memes often include an image and some text. Memes can be used to promote disinformation or hatred, thus it is crucial to investigate in details. We introduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike other prevalent datasets in the domain, including prior iterations of Memotion, Memotion 3 introduces Hindi-English Codemixed memes while prior works in the area were limited to only the English memes. We describe the Memotion task, the data collection and the dataset creation methodologies. We also provide a baseline for the task. The baseline code and dataset will be made available at https://github.com/Shreyashm16/Memotion-3.0
    
[^6]: 经过训练的1亿单词仍然保持状态：BERT结合英国国家语料库

    Trained on 100 million words and still in shape: BERT meets British National Corpus. (arXiv:2303.09859v1 [cs.CL])

    [http://arxiv.org/abs/2303.09859](http://arxiv.org/abs/2303.09859)

    本文探讨了在英国国家语料库上预训练的效果，并展示它可以比原始BERT模型达到更好的表现。在公平、可重复且数据有效的比较研究中，他们证明了这样的语料库有作为语言建模基准的巨大潜力。他们提出了一个经过优化的LM体系结构称为LTG-BERT。

    

    当前，现代遮蔽语言模型（LMs）训练的语料库规模越来越大。在本文中，我们探讨了缩小训练规模到一个规模适中、代表性好、平衡性好且公开可用的英文文本源-英国国家语料库的效果。我们展示了在这个精心策划的语料库上预训练可以达到比原始BERT模型更好的表现。我们认为这种类型的语料库具有作为语言建模基准的巨大潜力。为了展示这种潜力，我们以公平、可重复和数据有效的比较研究为特色，在其中评估了几个训练目标和模型架构，并以系统性的方式复制了先前的经验结果。我们提出了一个经过优化的LM体系结构称为LTG-BERT。

    While modern masked language models (LMs) are trained on ever larger corpora, we here explore the effects of down-scaling training to a modestly-sized but representative, well-balanced, and publicly available English text source -the British National Corpus. We show that pre-training on this carefully curated corpus can reach better performance than the original BERT model. We argue that this type of corpora has great potential as a language modeling benchmark. To showcase this potential, we present fair, reproducible and data-efficient comparative studies of LMs, in which we evaluate several training objectives and model architectures and replicate previous empirical results in a systematic way. We propose an optimized LM architecture called LTG-BERT.
    
[^7]: DORIC: 通过依赖解析进行领域鲁棒微调的开放意图聚类

    DORIC : Domain Robust Fine-Tuning for Open Intent Clustering through Dependency Parsing. (arXiv:2303.09827v1 [cs.CL])

    [http://arxiv.org/abs/2303.09827](http://arxiv.org/abs/2303.09827)

    该论文提出了一种名为DORIC的方法，通过利用多领域对话数据集进行微调，提取动词-宾语对以达到消除不必要信息的目的，最终实现了在各种领域数据集上的高精度聚类。

    

    我们在Dialog System Technology Challenges 11（DSTC11）的第2轨道上展示了我们的工作。DSTC11-Track2旨在为0-shot，跨领域的意图集归纳提供基准。在没有领域内训练数据集的情况下，需要强大的话语表示，可用于跨领域归纳用户意图。为了实现这一目标，我们利用多领域对话数据集来微调语言模型，并提出提取动词-宾语对以消除不必要信息的方法。此外，我们设计了一种方法，为聚类结果的可解释性生成每个群集的名称。我们的方法在各种领域数据集上展示出了优秀的准确性和标准化互信息（NMI）得分，相较于基线模型，我们在精度得分上获得了第三名。

    We present our work on Track 2 in the Dialog System Technology Challenges 11 (DSTC11). DSTC11-Track2 aims to provide a benchmark for zero-shot, cross-domain, intent-set induction. In the absence of in-domain training dataset, robust utterance representation that can be used across domains is necessary to induce users' intentions. To achieve this, we leveraged a multi-domain dialogue dataset to fine-tune the language model and proposed extracting Verb-Object pairs to remove the artifacts of unnecessary information. Furthermore, we devised the method that generates each cluster's name for the explainability of clustered results. Our approach achieved 3rd place in the precision score and showed superior accuracy and normalized mutual information (NMI) score than the baseline model on various domain datasets.
    
[^8]: Transformers和Ensemble方法：阿语恶意言论检测的一种解决方案

    Transformers and Ensemble methods: A solution for Hate Speech Detection in Arabic languages. (arXiv:2303.09823v1 [cs.CL])

    [http://arxiv.org/abs/2303.09823](http://arxiv.org/abs/2303.09823)

    本文提出了一种使用Transformer和Ensemble方法的解决方案，用于阿语恶意言论的检测。实验结果表明，基于多数表决的集成方法具有最佳效果，其在测试集上的准确率为0.86，F1分数为0.60。

    

    本文描述了我们参加CERIST NLP挑战赛2022中恶意言论检测共享任务的实验过程。我们评估了6个Transformer模型及其组合的性能，并使用了2种集成方法。在五折交叉验证的训练集上，基于多数表决的集成方法获得了最佳结果。在测试集上的评估结果为F1分数为0.60，准确性为0.86。

    This paper describes our participation in the shared task of hate speech detection, which is one of the subtasks of the CERIST NLP Challenge 2022. Our experiments evaluate the performance of six transformer models and their combination using 2 ensemble approaches. The best results on the training set, in a five-fold cross validation scenario, were obtained by using the ensemble approach based on the majority vote. The evaluation of this approach on the test set resulted in an F1-score of 0.60 and an Accuracy of 0.86.
    
[^9]: CoLT5: 基于条件计算的快速长距离Transformer模型

    CoLT5: Faster Long-Range Transformers with Conditional Computation. (arXiv:2303.09752v1 [cs.CL])

    [http://arxiv.org/abs/2303.09752](http://arxiv.org/abs/2303.09752)

    CoLT5是一种基于条件计算的Transformer模型，通过优先处理重要标记来加速长距离输入的处理。CoLT5在SCROLLS基准测试上表现最好，并能够有效地处理长达64k输入长度。

    

    许多自然语言处理任务需要处理长输入，但使用Transformer处理长文档很昂贵——这不仅是因为二次注意复杂性，还因为对每个标记应用前馈和投影层。然而，不是所有标记都同样重要，特别是对于较长的文档。我们提出了CoLT5，一种长输入Transformer模型，通过使用条件计算来利用此直觉，在前馈和注意层中为重要标记提供更多资源。我们展示了CoLT5比LongT5表现更强，训练和推理速度更快，在长输入SCROLLS基准测试上达到了SOTA。此外，CoLT5能够有效且可控地利用极长的输入，展示了高达64k输入长度的强大增益。

    Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.
    
[^10]: 面向对话生成的有选择数据增强学习

    Learning towards Selective Data Augmentation for Dialogue Generation. (arXiv:2303.09719v1 [cs.CL])

    [http://arxiv.org/abs/2303.09719](http://arxiv.org/abs/2303.09719)

    本文提出了一种面向对话生成的有选择数据增强框架（SDA），通过选择性增强低质量和具有代表性的案例，可以在神经对话模型训练中大幅提升响应生成任务的性能。

    

    针对神经对话模型需要大量数据进行训练的问题，本文提出了数据增强技术，以有效利用现有的训练样本。然而，当前在对话生成任务中的数据增强技术大多数是对训练数据集中的所有案例进行增强，而没有考虑不同案例之间的内在属性。我们认为，并非所有案例都适合增强任务，适合增强的案例应该遵循以下两个属性：（1）低质量（对话模型无法为此案例生成高质量的响应），（2）典型性（案例应该代表整个数据集的特性）。 因此，我们提出了一种有选择数据增强框架（SDA）来探讨这个想法，SDA使用双重对抗网络在一个阶段中选择最低质量和最具代表性的数据点进行增强。在两个公开数据集上进行的大量实验证明了所提出的SDA框架可以通过选择性增强低质量和具有代表性的案例来大大提高响应生成任务的性能。

    As it is cumbersome and expensive to acquire a huge amount of data for training neural dialog models, data augmentation is proposed to effectively utilize existing training samples. However, current data augmentation techniques on the dialog generation task mostly augment all cases in the training dataset without considering the intrinsic attributes between different cases. We argue that not all cases are beneficial for augmentation task, and the cases suitable for augmentation should obey the following two attributes: (1) low-quality (the dialog model cannot generate a high-quality response for the case), (2) representative (the case should represent the property of the whole dataset). Herein, we explore this idea by proposing a Selective Data Augmentation framework (SDA) for the response generation task. SDA employs a dual adversarial network to select the lowest quality and most representative data points for augmentation in one stage. Extensive experiments conducted on two publicly
    
[^11]: 语言模型中高效师生知识转移的神经架构搜索

    Neural Architecture Search for Effective Teacher-Student Knowledge Transfer in Language Models. (arXiv:2303.09639v1 [cs.CL])

    [http://arxiv.org/abs/2303.09639](http://arxiv.org/abs/2303.09639)

    本论文提出一种KD-NAS方法，使用神经架构搜索指导知识蒸馏过程，并找到最优的学生模型，从而在资源受限环境下实现高效师生知识转移，超过手工设计的学生模型和大型教师模型。

    

    大型预训练的语言模型已经在各种下游任务上取得了最先进的成果。小型学生成为资源受限环境部署的有效解决方法。然而，现有的已经预训练语料库中选出的学生模型会导致知识转移的低效。本文提出了使用神经架构搜索（NAS）指导知识蒸馏（KD）过程从而找到最优学生模型的KD-NAS方法。在搜索过程的每个episode中，NAS控制器根据下游任务的准确性和推理延迟的组合预测奖励。然后，对排名靠前的候选架构进行蒸馏处理。最后选择最高奖励的架构并固定进行知识蒸馏，制作出学生模型。实验结果表明，KD-NAS能够找到有效的学生模型，这些模型在资源受限场景下表现超越所有手工设计的学生模型，甚至超越大型教师模型。

    Large pre-trained language models have achieved state-of-the-art results on a variety of downstream tasks. Knowledge Distillation (KD) of a smaller student model addresses their inefficiency, allowing for deployment in resource-constraint environments. KD however remains ineffective, as the student is manually selected from a set of existing options already pre-trained on large corpora, a sub-optimal choice within the space of all possible student architectures. This paper proposes KD-NAS, the use of Neural Architecture Search (NAS) guided by the Knowledge Distillation process to find the optimal student model for distillation from a teacher, for a given natural language task. In each episode of the search process, a NAS controller predicts a reward based on a combination of accuracy on the downstream task and latency of inference. The top candidate architectures are then distilled from the teacher on a small proxy set. Finally the architecture(s) with the highest reward is selected, a
    
[^12]: HIVE：利用人类反馈进行指导性视觉编辑

    HIVE: Harnessing Human Feedback for Instructional Visual Editing. (arXiv:2303.09618v1 [cs.CV])

    [http://arxiv.org/abs/2303.09618](http://arxiv.org/abs/2303.09618)

    本文提出了一种新的框架，利用人类反馈进行指导性视觉编辑。通过收集被编辑图像的人类反馈，并学习奖励函数捕捉用户的偏好，可以缓解数据限制所带来的偏差，并提高模型性能。

    

    研究表明，将人类反馈纳入大型语言模型生成的文本对齐到人类偏好至关重要。本文假设，最先进的指导性图像编辑模型，其输出基于输入图像和编辑指令，同样可以从人类反馈中受益，因为其输出可能不符合用户的正确指令和偏好。本文提出了一种利用人类反馈进行指导性视觉编辑（HIVE）的新框架。具体而言，我们在编辑的图像上收集人类反馈并学习奖励函数以捕捉基础用户偏好。随后，我们引入可扩展的扩散模型微调方法，可根据估计的奖励值融入人类偏好。此外，为减轻数据限制带来的偏差，我们贡献了1M训练数据集，3.6K奖励数据集以用于奖励学习，以及1K评估数据集，以提高指导性图像编辑模型的性能。

    Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of inst
    
[^13]: 强化学习心理治疗AI伴侣与可解释的策略动态

    Psychotherapy AI Companion with Reinforcement Learning Recommendations and Interpretable Policy Dynamics. (arXiv:2303.09601v1 [cs.LG])

    [http://arxiv.org/abs/2303.09601](http://arxiv.org/abs/2303.09601)

    本文介绍了一种使用强化学习生成心理治疗主题推荐的AI伴侣，能够很好地捕获真实数据，并通过可解释的策略轨迹可视化提供对不同奖励信号和不同临床诊断下训练的策略的独特模式。

    

    本文介绍了一种使用Deep Reinforcement Learning（DRL）生成心理治疗主题推荐的强化学习心理治疗AI伴侣。该系统针对四种不同的精神疾病（焦虑症，抑郁症，精神分裂症和自杀病例）使用多目标策略生成器进行生成，同时通过三个不同的工作联盟评分标准（任务，关系和目标）来检验推荐主题的准确性。我们展示了系统能够相对较好地捕获真实数据（治疗师讨论的历史主题），最佳模型的表现因疾病和评级标准而异。为了获得可解释的洞见，我们在2D主成分分析空间和转移矩阵中可视化策略轨迹。这些可视化呈现了在不同奖励信号和不同临床诊断下训练的策略之间的独特模式。本系统在生成多目标策略心理治疗主题方面的成功表现为开发能够帮助治疗师提供个性化治疗的AI伴侣提供了有希望的方向。

    We introduce a Reinforcement Learning Psychotherapy AI Companion that generates topic recommendations for therapists based on patient responses. The system uses Deep Reinforcement Learning (DRL) to generate multi-objective policies for four different psychiatric conditions: anxiety, depression, schizophrenia, and suicidal cases. We present our experimental results on the accuracy of recommended topics using three different scales of working alliance ratings: task, bond, and goal. We show that the system is able to capture the real data (historical topics discussed by the therapists) relatively well, and that the best performing models vary by disorder and rating scale. To gain interpretable insights into the learned policies, we visualize policy trajectories in a 2D principal component analysis space and transition matrices. These visualizations reveal distinct patterns in the policies trained with different reward signals and trained on different clinical diagnoses. Our system's succe
    
[^14]: 构建鲁棒的孟加拉复杂命名实体识别模型

    Towards Robust Bangla Complex Named Entity Recognition. (arXiv:2303.09306v1 [cs.CL])

    [http://arxiv.org/abs/2303.09306](http://arxiv.org/abs/2303.09306)

    本研究构建了基于 CRF 和深度学习（如 BanglaBERT）的鲁棒孟加拉复杂命名实体识别模型，解决了 CNER 任务，填补了孟加拉语复杂命名实体识别领域的空白。

    

    命名实体识别 (NER) 是自然语言处理中的基础任务，包括在文本中识别和分类命名实体。尽管孟加拉语是全球第七大使用语言，但针对孟加拉语复杂命名实体识别的工作还很少。CNER 是一项更具挑战性的任务，因为它涉及识别和分类复杂和复合实体，而这在孟加拉语中不常见。在本文中，我们提出了解决 BanglaCoNER 数据集上的 CNER 任务的获胜解决方案，使用了两种不同的方法，即条件随机场 (CRF) 和基于 finetuning transformer 的深度学习模型（如 BanglaBERT）。数据集包括 15300 个用于训练的句子和 800 个用于验证的句子，格式为 .conll。对数据集的探索性数据分析 (EDA) 揭示出数据集有 7 种不同的 NER 标签，其中有英语单词的明显存在。

    Named Entity Recognition (NER) is a fundamental task in natural language processing that involves identifying and classifying named entities in text. But much work hasn't been done for complex named entity recognition in Bangla, despite being the seventh most spoken language globally. CNER is a more challenging task than traditional NER as it involves identifying and classifying complex and compound entities, which are not common in Bangla language. In this paper, we present the winning solution of Bangla Complex Named Entity Recognition Challenge - addressing the CNER task on BanglaCoNER dataset using two different approaches, namely Conditional Random Fields (CRF) and finetuning transformer based Deep Learning models such as BanglaBERT.  The dataset consisted of 15300 sentences for training and 800 sentences for validation, in the .conll format. Exploratory Data Analysis (EDA) on the dataset revealed that the dataset had 7 different NER tags, with notable presence of English words, s
    
[^15]: PRESTO：用于解析逼真任务导向对话的多语言数据集

    PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented Dialogs. (arXiv:2303.08954v1 [cs.CL])

    [http://arxiv.org/abs/2303.08954](http://arxiv.org/abs/2303.08954)

    PRESTO是一个多语言数据集，包含超过55万个人与虚拟助手之间的上下文多语言对话。该数据集可以帮助解决如说话不连贯、代码切换和修正等真实NLU任务中出现的挑战。

    

    随着Google Assistant、Alexa和Siri等系统在日常生活中变得普遍，人们对任务导向型对话的研究兴趣不断增加。然而，在捕捉各种用户痛点的实际情况方面缺乏实际数据集，这限制了学术研究在这一领域的影响。为了使关于解析逼真对话的一些挑战性方面的研究成为可能，我们引入了PRESTO，一个包含超过55万个人与虚拟助手之间的上下文多语言对话的公共数据集。PRESTO包含了真实NLU任务中出现的多样化挑战，如说话不连贯、代码切换和修正。它是唯一一个提供每个示例的结构化上下文，如用户的联系人和列表的大规模人类生成会话解析数据集。我们基于mT5模型的基线表明，PRESTO中存在的对话现象具有挑战性，这在低资源设置中更加明显。

    Research interest in task-oriented dialogs has increased as systems such as Google Assistant, Alexa and Siri have become ubiquitous in everyday life. However, the impact of academic research in this area has been limited by the lack of datasets that realistically capture the wide array of user pain points. To enable research on some of the more challenging aspects of parsing realistic conversations, we introduce PRESTO, a public dataset of over 550K contextual multilingual conversations between humans and virtual assistants. PRESTO contains a diverse array of challenges that occur in real-world NLU tasks such as disfluencies, code-switching, and revisions. It is the only large scale human generated conversational parsing dataset that provides structured context such as a user's contacts and lists for each example. Our mT5 model based baselines demonstrate that the conversational phenomenon present in PRESTO are challenging to model, which is further pronounced in a low-resource setup.
    
[^16]: 虚拟代理人的端到端口语化实体提取

    E2E Spoken Entity Extraction for Virtual Agents. (arXiv:2302.10186v4 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2302.10186](http://arxiv.org/abs/2302.10186)

    本文研究了利用预训练语音编码器从语音中直接提取实体的方法，无需文本转录，且在口语实体识别任务中表现优异。

    

    本文重新构想了语音处理中的一些方面，特别是关于从语音中直接提取实体，而无需中间文本表示。在人与计算机的对话中，从语音中提取实体，如姓名、邮政地址和电子邮件地址，是一项具有挑战性的任务。我们研究了微调预训练语音编码器对从语音中直接提取可读性强的实体的影响，而无需进行文本转录。我们说明这种直接方法优化了编码器，以仅转录语音中与实体相关的部分，忽略了多余的部分，如搭档语或实体拼写。在企业虚拟代理人的对话上下文中，我们展示了一步法的方法优于典型的两步法，即首先产生词汇转录，然后进行基于文本的实体提取以识别口语实体。

    This paper reimagines some aspects of speech processing using speech encoders, specifically about extracting entities directly from speech, with no intermediate textual representation. In human-computer conversations, extracting entities such as names, postal addresses and email addresses from speech is a challenging task. In this paper, we study the impact of fine-tuning pre-trained speech encoders on extracting spoken entities in human-readable form directly from speech without the need for text transcription. We illustrate that such a direct approach optimizes the encoder to transcribe only the entity relevant portions of speech, ignoring the superfluous portions such as carrier phrases and spellings of entities. In the context of dialogs from an enterprise virtual agent, we demonstrate that the 1-step approach outperforms the typical 2-step cascade of first generating lexical transcriptions followed by text-based entity extraction for identifying spoken entities.
    
[^17]: 基于Transformers的贪婪排序优化翻译性能

    Greedy Ordering of Layer Weight Matrices in Transformers Improves Translation. (arXiv:2302.02123v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.02123](http://arxiv.org/abs/2302.02123)

    本论文提出了一种基于贪婪重排权重矩阵的算法AEIUOrder，能够最大化总的经过充分训练的层所贡献的"well-trainedness"指标进行优化，从而提高翻译质量并在各种翻译任务上达到最佳性能。

    

    先前的工作试图在多头注意力和前馈子图层的层次上理解基于Transformer的编码器-解码器架构的内部结构和功能。但是，如果不检查低层次的结构，就不能深入理解子层重排背后的动机。本文通过AEIUOrder算法，通过衡量经过充分训练的层的Heavy-Tailed Self-Regularization（HT-SR）指标，贪婪地对编码器中的层重量矩阵进行重新排序，然后相应地排序解码器矩阵。我们的结果表明，通过最大化总的经过充分训练的层所贡献的"well-trainedness"指标进行重排，可以更好地学习表示，并生成更高质量的翻译输出，在各种翻译任务上达到了最好的性能。

    Prior work has attempted to understand the internal structures and functionalities of Transformer-based encoder-decoder architectures on the level of multi-head attention and feed-forward sublayers. Interpretations have focused on the encoder and decoder, along with the combinatorial possibilities of the self-attention, cross-attention, and feed-forward sublayers. However, without examining the low-level structures, one gains limited understanding of the motivation behind sublayer reordering. Could we dive into the sublayer abstraction and permute layer weight matrices to improve the quality of translation? We propose AEIUOrder to greedily reorder layer weight matrices in the encoder by their well-trainedness, as measured by Heavy-Tailed Self-Regularization (HT-SR) metrics, and order the decoder matrices correspondingly. Our results suggest that greedily reordering layer weight matrices to maximize Total well-trainedness facilitates the model to learn representations and generate trans
    
[^18]: 匹配标本作为下一句预测：自然语言处理科学教育中的零样本学习自动评分

    Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-shot Prompt Learning for Automatic Scoring in Science Education. (arXiv:2301.08771v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.08771](http://arxiv.org/abs/2301.08771)

    本研究提出了一种零样本学习自动评分的方法，利用预训练的语言模型配合匹配标本作为下一句预测技术，成功应用于科学教育领域的论证任务，极大地减少了训练成本和时间。

    

    开发能够自动评分科学问题的学生书面答案的模型对于科学教育至关重要。然而，收集和标记足够的学生答案以训练模型是耗时和费用高昂的。最近的研究表明，预训练的语言模型（PLMs）可以在不需要prompt调整的情况下适应下游任务。然而，在科学教育中还没有使用过这种提示方法的研究。由于学生的答案是用自然语言呈现的，因此使用提示将评分过程对齐为下一句预测任务可以跳过昂贵的调整阶段。在这项研究中，我们通过匹配标本作为下一句预测（MeNSP）开发了一种零样本自动评分方法。这种方法不需要训练样本。我们首先在评分三个科学论证任务中应用MeNSP，并发现机器-人评分的一致性，Cohen的Kappa系数在0.30到0.57之间，F1分数

    Developing models to automatically score students' written responses to science problems is critical for science education. However, collecting and labeling sufficient student responses for training models is time and cost-consuming. Recent studies suggest that pre-trained language models (PLMs) can be adapted to downstream tasks without fine-tuning with prompts. However, no research has employed such a prompt approach in science education. As student responses are presented with natural language, aligning the scoring procedure as the next sentence prediction task using prompts can skip the costly fine-tuning stage. In this study, we developed a zero-shot approach to automatically score student responses via Matching Exemplars as Next Sentence Prediction (MeNSP). This approach employs no training samples. We first apply MeNSP in scoring three assessment tasks of scientific argumentation and found machine-human scoring agreements, Cohen's Kappa ranges from 0.30 to 0.57, and F1 score ran
    
[^19]: 自然语言推理中的多尺度数据增强方法用于纠偏和预训练模型优化

    Multi-Scales Data Augmentation Approach In Natural Language Inference For Artifacts Mitigation And Pre-Trained Model Optimization. (arXiv:2212.08756v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.08756](http://arxiv.org/abs/2212.08756)

    本文通过多尺度数据增强方法纠偏了预训练模型在自然语言推理中遇到的数据集人为制造效应，提高了模型对扰动测试的抵抗力。

    

    虽然机器学习模型在基准自然语言处理数据集上表现良好，但在更具挑战性的情境下却表现不佳。本文研究了已预训练模型学习自然语言推理（NLI）中数据集人为制造效应的问题，即逻辑关系在一对文本序列中的学习。我们提供了多种技术来分析和定位Stanford自然语言推理（SNLI）语料库内的数据集人为制造效应。我们研究了SNLI语料库中数据集人为制造效应的风格模式并采用了独特的多尺度数据增强技术，其中包括句子级的行为测试检查表和单词级的词汇同义词标准。具体而言，我们采用的组合方法提高了我们的模型对扰动测试的抵抗力，使其持续优于预训练基线。

    Machine learning models can reach high performance on benchmark natural language processing (NLP) datasets but fail in more challenging settings. We study this issue when a pre-trained model learns dataset artifacts in natural language inference (NLI), the topic of studying the logical relationship between a pair of text sequences. We provide a variety of techniques for analyzing and locating dataset artifacts inside the crowdsourced Stanford Natural Language Inference (SNLI) corpus. We study the stylistic pattern of dataset artifacts in the SNLI. To mitigate dataset artifacts, we employ a unique multi-scale data augmentation technique with two distinct frameworks: a behavioral testing checklist at the sentence level and lexical synonym criteria at the word level. Specifically, our combination method enhances our model's resistance to perturbation testing, enabling it to continuously outperform the pre-trained baseline.
    
[^20]: 文本、声学和基于lattice的表示在口语理解任务中的有效性

    Effectiveness of Text, Acoustic, and Lattice-based representations in Spoken Language Understanding tasks. (arXiv:2212.08489v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.08489](http://arxiv.org/abs/2212.08489)

    本文研究了在口语理解任务中使用文本、声学和基于lattice的表示方法，结果表明使用更丰富的自动语音识别输出形式可以获得更好的性能。

    

    本文针对口语理解（SLU）中的意图分类问题进行了多种表示方法的详细评估。我们对三种系统进行了基准评估来执行SLU意图检测任务：1）基于文本的，2）基于lattice的，以及一种新的3）多模式方法。我们提供了对不同情况下（例如，自动生成的转录 vs 手动转录等）不同最先进的SLU系统的可实现性能的全面分析。我们基于公开资源SLURP口语语言资源语料库评估了这些系统。结果表明，使用更丰富的自动语音识别（ASR）输出形式，即词共识网络，可以使SLU系统相对于1-best设置获得改善（相对提高5.5%）。然而，跨模态方法，即学习声学和文本嵌入，得到了与oracle设置相似的性能，即相对提高...

    In this paper, we perform an exhaustive evaluation of different representations to address the intent classification problem in a Spoken Language Understanding (SLU) setup. We benchmark three types of systems to perform the SLU intent detection task: 1) text-based, 2) lattice-based, and a novel 3) multimodal approach. Our work provides a comprehensive analysis of what could be the achievable performance of different state-of-the-art SLU systems under different circumstances, e.g., automatically- vs. manually-generated transcripts. We evaluate the systems on the publicly available SLURP spoken language resource corpus. Our results indicate that using richer forms of Automatic Speech Recognition (ASR) outputs, namely word-consensus-networks, allows the SLU system to improve in comparison to the 1-best setup (5.5% relative improvement). However, crossmodal approaches, i.e., learning from acoustic and text embeddings, obtains performance similar to the oracle setup, a relative improvement 
    
[^21]: 与任务导向对话的意图识别相关的话语嵌入和聚类方法的分析

    Analysis of Utterance Embeddings and Clustering Methods Related to Intent Induction for Task-Oriented Dialogue. (arXiv:2212.02021v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.02021](http://arxiv.org/abs/2212.02021)

    本文旨在研究任务导向对话中的意图识别问题，并提出两个关键因素：聚类算法和用户话语嵌入空间。实验证明，利用预训练的MiniLM与层次聚类相结合可以显著提高意图归纳任务的效果。

    

    本文重点研究无监督方法，以克服设计任务导向对话图谱中的典型挑战：为每个对话转折指定意图标签（意图聚类）并基于意图聚类方法生成一组意图（意图归纳）。我们假设自动归纳意图有两个显著因素：（1）意图标签的聚类算法和（2）用户话语嵌入空间。 我们根据DSTC11评估比较了现有的成品聚类模型和嵌入。我们的实验表明，认真考虑意图归纳任务中话语嵌入和聚类方法的综合选择是必要的。我们还发现，利用预训练的MiniLM与层次聚类相结合可显著提高意图归纳任务中的NMI，ARI，F1，准确性和示例覆盖。源代码可在https://github.com/Jeiyoon/dstc11-track2上获得。

    The focus of this work is to investigate unsupervised approaches to overcome quintessential challenges in designing task-oriented dialog schema: assigning intent labels to each dialog turn (intent clustering) and generating a set of intents based on the intent clustering methods (intent induction). We postulate there are two salient factors for automatic induction of intents: (1) clustering algorithm for intent labeling and (2) user utterance embedding space. We compare existing off-the-shelf clustering models and embeddings based on DSTC11 evaluation. Our extensive experiments demonstrate that the combined selection of utterance embedding and clustering method in the intent induction task should be carefully considered. We also present that pretrained MiniLM with Agglomerative clustering shows significant improvement in NMI, ARI, F1, accuracy and example coverage in intent induction tasks. The source codes are available at https://github.com/Jeiyoon/dstc11-track2.
    
[^22]: 图像叙述中的指代消解：你指的是谁？

    Who are you referring to? Coreference resolution in image narrations. (arXiv:2211.14563v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.14563](http://arxiv.org/abs/2211.14563)

    本研究介绍了一个新的数据集和技术来解决图像叙述中的指代消解问题，提出的模型在解决这一任务中表现出较强的性能和优势，指代消解有助于提高图片中的叙述表达能力。

    

    指代消解的目标是在文本中识别指向同一实体的词语和短语，这是自然语言处理中的一项核心任务。本文将这一任务扩展到长篇视觉场景叙述中的指代消解。首先，我们引入了一个新的数据集，其中注释了指代链及其边界框，因为大多数现有图像-文本数据集仅包含不带指示性表达式或标记链的短句。我们提出了一种新的技术，利用弱监督只从图像-文本对和使用先前的语言知识的正则化来学习识别指代链。我们的模型在解决指代消解方面比几个强基线模型产生了大量的性能提升。我们还展示了指代消解有助于提高图片中的叙述表达能力。

    Coreference resolution aims to identify words and phrases which refer to same entity in a text, a core task in natural language processing. In this paper, we extend this task to resolving coreferences in long-form narrations of visual scenes. First we introduce a new dataset with annotated coreference chains and their bounding boxes, as most existing image-text datasets only contain short sentences without coreferring expressions or labeled chains. We propose a new technique that learns to identify coreference chains using weak supervision, only from image-text pairs and a regularization using prior linguistic knowledge. Our model yields large performance gains over several strong baselines in resolving coreferences. We also show that coreference resolution helps improving grounding narratives in images.
    
[^23]: GPT-3 驱动的教育智能体训练儿童好奇心问题提问技巧

    GPT-3-driven pedagogical agents for training children's curious question-asking skills. (arXiv:2211.14228v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.14228](http://arxiv.org/abs/2211.14228)

    该研究探索了使用自然语言处理技术自动生成儿童好奇心问题提问培训的教育内容，该方法显示出了很高的相关性和教育价值。

    

    为了训练儿童提问好奇心驱动的问题的能力，之前的研究探索了设计特定的练习，依靠提供语义和语言提示来帮助形成这样的问题。但尽管表现出了教学效率，但该方法仍然受限于手动生成提示，这可能是一个非常昂贵的过程。在这种情况下，我们提议利用自然语言处理领域（NLP）的先进技术，并调查使用大型语言模型（LLM）自动化好奇心问题提问（QA）培训的教育内容的有效性。我们研究使用“基于提示”的方法来生成教育内容，该方法包括使用自然文本向LLM解释任务。我们使用人类专家注释和手动生成内容进行评估。结果确实表明了这种内容的相关性和有用性。我们还在小学进行了现场研究（75个孩子）

    In order to train children's ability to ask curiosity-driven questions, previous research has explored designing specific exercises relying on providing semantic and linguistic cues to help formulate such questions. But despite showing pedagogical efficiency, this method is still limited as it relies on generating the said cues by hand, which can be a very costly process. In this context, we propose to leverage advances in the natural language processing field (NLP) and investigate the efficiency of using a large language model (LLM) for automating the production of the pedagogical content of a curious question-asking (QA) training. We study generating the said content using the "prompt-based" method that consists of explaining the task to the LLM in natural text. We evaluate the output using human experts annotations and comparisons with hand-generated content. Results suggested indeed the relevance and usefulness of this content. We also conduct a field study in primary school (75 ch
    
[^24]: 自监督语音模型的逐层比较分析

    Comparative layer-wise analysis of self-supervised speech models. (arXiv:2211.03929v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.03929](http://arxiv.org/abs/2211.03929)

    本论文通过比较不同自监督语音模型的逐层中间表示，发现了不同模型在编码声学、语音和单词级属性上的差异，并发现这些差异与预训练目标的选择相关。通过比较属性趋势和语音识别和口语理解任务的性能，我们发现CCA趋势为选择层次提供了可靠的指导。

    

    在过去几年中，许多不同预训练目标、输入形式和预训练数据的自监督语音模型被提出。尽管在下游任务中取得了惊人的成功，我们仍然对这些模型编码的属性及其差异了解有限。在本研究中，我们使用基于规范相关分析(CCA)的轻量级分析工具，检查了多个最近模型的中间表示。具体而言，我们测量了单个层次中编码的声学、语音和单词级属性，发现这些属性在不同模型中的层次演变方式不同，且变化与预训练目标的选择相关。我们通过比较属性趋势和语音识别和口语理解任务的性能，进一步研究了我们的分析在下游任务中的应用价值。发现CCA趋势为选择层次提供了可靠的指导。

    Many self-supervised speech models, varying in their pre-training objective, input modality, and pre-training data, have been proposed in the last few years. Despite impressive successes on downstream tasks, we still have a limited understanding of the properties encoded by the models and the differences across models. In this work, we examine the intermediate representations for a variety of recent models. Specifically, we measure acoustic, phonetic, and word-level properties encoded in individual layers, using a lightweight analysis tool based on canonical correlation analysis (CCA). We find that these properties evolve across layers differently depending on the model, and the variations relate to the choice of pre-training objective. We further investigate the utility of our analyses for downstream tasks by comparing the property trends with performance on speech recognition and spoken language understanding tasks. We discover that CCA trends provide reliable guidance to choose laye
    
[^25]: 利用大型语言模型进行多项选择题的答案推断

    Leveraging Large Language Models for Multiple Choice Question Answering. (arXiv:2210.12353v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.12353](http://arxiv.org/abs/2210.12353)

    本文研究了利用大型语言模型进行多项选择题的答案推断，并探讨了一种更自然的问题提示方式，以提高准确性。

    

    大型语言模型如GPT-3在多项选择题答案推断任务中已经取得了令人瞩目的成果。然而，它们通常落后于多项选择题的最新研究成果。本文探讨了一种更自然的问题提示方式，即将问题和答案选项联合呈现给大型语言模型，以便模型能够明确比较答案选项，减少计算成本，降低标记化方案和答案选项表示对答案选择的影响。

    While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g., "A") associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that re
    
[^26]: TabLLM: 大语言模型在少样本表格数据分类中的应用

    TabLLM: Few-shot Classification of Tabular Data with Large Language Models. (arXiv:2210.10723v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.10723](http://arxiv.org/abs/2210.10723)

    本文应用大语言模型将表格数据序列化为自然语言字符串进行分类，微调后即可在非常少的样本设置下与传统基线方法竞争力十足。

    

    本文研究了大语言模型在零样本和少样本表格数据分类中的应用。我们将表格数据序列化为自然语言字符串，并加上分类问题的简短描述，然后启用大语言模型。在少样本场景下，我们使用一些标记样本微调大语言模型。我们评估了多种序列化方法，包括模板、表格到文本模型和大语言模型。尽管方法简单，但我们发现它在多个基准数据集上优于以前的基于深度学习的表格分类方法。在大多数情况下，即使是零样本分类也获得了非平凡的表现，说明该方法能够利用大语言模型中编码的先前知识。与许多针对表格数据的深度学习方法不同，这种方法在非常少的样本设置下也与强大的传统基线方法（如梯度提升树）竞争力十足。

    We study the application of large language models to zero-shot and few-shot classification of tabular data. We prompt the large language model with a serialization of the tabular data to a natural-language string, together with a short description of the classification problem. In the few-shot setting, we fine-tune the large language model using some labeled examples. We evaluate several serialization methods including templates, table-to-text models, and large language models. Despite its simplicity, we find that this technique outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. In most cases, even zero-shot classification obtains non-trivial performance, illustrating the method's ability to exploit prior knowledge encoded in large language models. Unlike many deep learning methods for tabular datasets, this approach is also competitive with strong traditional baselines like gradient-boosted trees, especially in the very-few-shot setting
    
[^27]: Treeformer: 稠密梯度树实现高效注意力计算

    Treeformer: Dense Gradient Trees for Efficient Attention Computation. (arXiv:2208.09015v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.09015](http://arxiv.org/abs/2208.09015)

    本文提出了Treeformer，一种基于决策树的分层导航方法，用于高效地计算注意力。与传统的注意力计算方法相比，Treeformer 可以将检索成本从线性降为近似对数级别，并提供两种有效的关注层。算法的目标是处理输入序列长度过长的应用，提高计算效率。

    

    基于Transformer架构的推理和训练的时间复杂度和输入序列长度成二次关系，这对于一些应用如网页翻译和查询-回答等速度要求较高的应用是不可行的。因此，为了加速注意力计算，近期提出了多种方法，如强制使用不同的注意力结构，如稀疏、低秩、使用核函数逼近。本文提出一种将注意力计算看作最近邻检索的方法，利用基于决策树的分层导航，将每个查询标记的检索成本从线性降为近似对数级别。基于这种分层导航，我们设计出Treeformer，它可以使用两种有效的关注层-- TF-Attention和TC-Attention。TF-Attention以细粒度方式计算其中的关注，而TC-Attention是一种更粗粒度的关注层，也确保梯度是“密集”的。

    Standard inference and training with transformer based architectures scale quadratically with input sequence length. This is prohibitively large for a variety of applications especially in web-page translation, query-answering etc. Consequently, several approaches have been developed recently to speedup attention computation by enforcing different attention structures such as sparsity, low-rank, approximating attention using kernels. In this work, we view attention computation as that of nearest neighbor retrieval, and use decision tree based hierarchical navigation to reduce the retrieval cost per query token from linear in sequence length to nearly logarithmic. Based on such hierarchical navigation, we design Treeformer which can use one of two efficient attention layers -- TF-Attention and TC-Attention. TF-Attention computes the attention in a fine-grained style, while TC-Attention is a coarse attention layer which also ensures that the gradients are "dense". To optimize such challe
    
[^28]: 基于平面性与投影性定义下树的最大线性排列问题（arXiv:2206.06924v3[cs.DS] 更新版）

    The Maximum Linear Arrangement Problem for trees under projectivity and planarity. (arXiv:2206.06924v3 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2206.06924](http://arxiv.org/abs/2206.06924)

    该论文提出了一种解决在平面性和投影性定义下树的最大线性排列问题的算法，证明了最大投影和平面排列的多个性质，发现毛毛虫树最优，推广了之前的极值结果。

    

    最大线性排列问题(MaxLA)是指找到从图G的n个顶点到不同连续整数的映射$ \pi $，使得$ D(G)=\sum_{uv\in E(G)}|\pi(u)-\pi(v)| $最大化。在这种情况下，顶点被认为在一条水平线上，并且边是作为半圆弧画在线上方的。存在一些限制排列的MaxLA变体。在平面变体中，禁止边交叉。在针对根树的投影变体中，排列是平面的，而根不能被任何边覆盖。在这里，我们提出了用于解决树的平面和投影MaxLA的O(n)-时间和O(n）-空间算法。我们还证明了最大投影和平面排列的多个性质，并且表明毛毛虫树在固定大小的所有树中最大化平面MaxLA，因此推广了先前关于树的极值结果。

    The Maximum Linear Arrangement problem (MaxLA) consists of finding a mapping $\pi$ from the $n$ vertices of a graph $G$ to distinct consecutive integers that maximizes $D(G)=\sum_{uv\in E(G)}|\pi(u) - \pi(v)|$. In this setting, vertices are considered to lie on a horizontal line and edges are drawn as semicircles above the line. There exist variants of MaxLA in which the arrangements are constrained. In the planar variant, edge crossings are forbidden. In the projective variant for rooted trees, arrangements are planar and the root cannot be covered by any edge. Here we present $O(n)$-time and $O(n)$-space algorithms that solve planar and projective MaxLA for trees. We also prove several properties of maximum projective and planar arrangements, and show that caterpillar trees maximize planar MaxLA over all trees of a fixed size thereby generalizing a previous extremal result on trees.
    
[^29]: 使用分布感知词嵌入的命名实体识别性能的实证研究。

    Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding. (arXiv:2109.01636v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2109.01636](http://arxiv.org/abs/2109.01636)

    研究开发了一种分布感知词嵌入，并实施了三种不同的方法来利用NER框架中的分布信息，实验表明将词的特异性融入NER方法可提高NER的性能。

    

    随着深度学习技术的快速发展，命名实体识别（NER）在信息提取任务中变得越来越重要。NER任务面临的最大困难是即使在NE类型和文档不熟悉的情况下仍然需要保持可检测性。意识到特定性信息可能包含单词的潜在含义并生成词嵌入的语义相关特征，我们开发了一个分布感知词嵌入，并实施了三种不同的方法来利用NER框架中的分布信息。结果表明，如果将词的特异性融入现有的NER方法中，NER的性能将得到提高。

    With the fast development of Deep Learning techniques, Named Entity Recognition (NER) is becoming more and more important in the information extraction task. The greatest difficulty that the NER task faces is to keep the detectability even when types of NE and documents are unfamiliar. Realizing that the specificity information may contain potential meanings of a word and generate semantic-related features for word embedding, we develop a distribution-aware word embedding and implement three different methods to make use of the distribution information in a NER framework. And the result shows that the performance of NER will be improved if the word specificity is incorporated into existing NER methods.
    

