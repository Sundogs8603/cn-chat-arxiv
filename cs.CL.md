# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code.](http://arxiv.org/abs/2308.00683) | CodeBPE研究了用于源代码的大型语言模型预训练中不同子标记化选项的影响，找出了最有效和长度高效的子标记化方法，通过减少平均长度17%且不影响下游性能，可能提高质量0.5-2%。 |
| [^2] | [Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models.](http://arxiv.org/abs/2308.00675) | 这项工作提出了使用工具文档作为教导大型语言模型使用新工具的替代方法，并通过实证研究证明，仅使用工具文档的零-shot提示足以实现正确的工具使用。 |
| [^3] | [JIANG: Chinese Open Foundation Language Model.](http://arxiv.org/abs/2308.00624) | JIANG是一个专为中文设计的开放式语言模型，通过使用大量的中文语料库进行训练和优化结构，能够更好地在中文中发挥其能力。 |
| [^4] | [Unimodal Intermediate Training for Multimodal Meme Sentiment Classification.](http://arxiv.org/abs/2308.00528) | 本研究提出了一种补充多模态表情包分类器训练的单模态中间训练方法，通过利用大量情感标记的单模态数据，显著提高了性能，并证明了在不减少下游模型性能的情况下，可以减少标记表情包的训练集数量。 |
| [^5] | [Retrieval Augmented Generation and Representative Vector Summarization for large unstructured textual data in Medical Education.](http://arxiv.org/abs/2308.00479) | 本文介绍了在医学教育领域中，使用检索增强生成（RAG）将非参数化知识库与大规模语言模型（LLMs）结合，以解决幻觉和有害答案问题，并提出了一种使用代表性向量的抽取性和抽象性摘要方法。 |
| [^6] | [Structural Embeddings of Tools for Large Language Models.](http://arxiv.org/abs/2308.00447) | 这篇论文突出了大型语言模型（LLM）与外部工具之间基于图的交互方法的重要性，并提出了一个指导与LLM集成大量外部工具的框架。 |
| [^7] | [SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning.](http://arxiv.org/abs/2308.00436) | 本论文研究了使用LLMs自检逐步推理的能力，提出了一种零-shot验证方案，成功识别错误并提高了问答性能。 |
| [^8] | [Discourse-Aware Text Simplification: From Complex Sentences to Linked Propositions.](http://arxiv.org/abs/2308.00425) | 本研究提出了一种话语感知的文本简化方法，通过在句子的语义上下文中拆分和重新表达复杂的英语句子，解决了现有句法文本简化方法的保守性和忽略上下文连贯性的问题。 |
| [^9] | [ZRIGF: An Innovative Multimodal Framework for Zero-Resource Image-Grounded Dialogue Generation.](http://arxiv.org/abs/2308.00400) | ZRIGF是一种创新的多模态框架，用于无资源情境下的图像驱动对话生成。它通过对比预训练和生成预训练实现了视觉特征的对齐，生成有洞察力的回应。 |
| [^10] | [Tackling Hallucinations in Neural Chart Summarization.](http://arxiv.org/abs/2308.00399) | 本研究解决了神经图表摘要中的幻觉问题，通过自然语言推理预处理训练数据，显著减少了幻觉的产生，并通过缩短远距离依赖关系和添加与图表相关的信息来提高整体性能。 |
| [^11] | [Fountain -- an intelligent contextual assistant combining knowledge representation and language models for manufacturing risk identification.](http://arxiv.org/abs/2308.00364) | Fountain是一个智能上下文助手，将知识表示和语言模型结合，用于制造风险识别。它通过描述现有设计和流程准则以及提出的偏差来帮助识别风险，并提供可解释和一致的建议。 |
| [^12] | [LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack.](http://arxiv.org/abs/2308.00319) | 本文提出了一种名为LimeAttack的硬标签攻击算法，通过本地可解释方法来近似单词重要性排序，然后利用波束搜索找到最优解。 |
| [^13] | [Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models.](http://arxiv.org/abs/2308.00304) | 本文介绍了一种名为技能指导 (SKiC) 的提示策略，通过在上下文中演示基本技能和组合性示例，使大型语言模型具备解决更复杂问题的能力，并在泛化能力上取得几乎完美的表现。 |
| [^14] | [Making the V in Text-VQA Matter.](http://arxiv.org/abs/2308.00295) | 本论文针对文本VQA中对视觉特征理解不足的问题，提出通过学习视觉特征来解决这一问题，通过将TextVQA和VQA数据集相结合进行模型训练，从而提高模型的准确性。 |
| [^15] | [Multi-Modality Multi-Loss Fusion Network.](http://arxiv.org/abs/2308.00264) | 多模态多损失融合网络通过最佳选择和融合多个模态的特征，提高了情感检测的性能，并在多个数据集上实现了最先进的结果。这些研究结果表明了用于增强神经网络中情感检测的特征选择和融合方法的优化方向。 |
| [^16] | [Towards Effective Ancient Chinese Translation: Dataset, Model, and Evaluation.](http://arxiv.org/abs/2308.00240) | 本论文提出了Erya模型，通过收集、清理和分类大量古代汉语材料，形成了最全面的古代汉语资源。该模型在双音节对齐替换和双层遮罩语言模型任务上表现出色，并在多个领域展示了出色的零-shot性能和迁移能力。 |
| [^17] | [Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias.](http://arxiv.org/abs/2308.00225) | 这项研究发现，经过指导调优的语言模型呈现出新兴的认知偏见，这对于理解和开发更可靠和无偏的语言模型至关重要。 |
| [^18] | [Advancing Beyond Identification: Multi-bit Watermark for Language Models.](http://arxiv.org/abs/2308.00221) | 本研究提出了一种用于语言模型的多位水印技术——COLOR，可在语言模型生成过程中嵌入可追踪的多位信息，实现了提取水印、即时嵌入和维持文本质量等功能，同时允许零位检测。初步实验显示成功在中等长度的文本中嵌入了32位消息，准确率为91.9％。这项研究有效推进了对语言模型滥用的反制策略。 |
| [^19] | [Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?.](http://arxiv.org/abs/2308.00189) | 生成模型作为复杂系统科学，它们能够完成任务的行为表现需要被解释和理解，以实现对其行为的控制和未来研究的指导。 |
| [^20] | [Adversarially Robust Neural Legal Judgement Systems.](http://arxiv.org/abs/2308.00165) | 本论文提出了一种对抗性鲁棒的神经法律判断系统，通过对早期存在的系统进行实验发现它们无法处理对抗性攻击。经过大量实验，我们的方法在处理对抗性攻击方面明显优于现有最先进的法律判断预测系统。 |
| [^21] | [Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?.](http://arxiv.org/abs/2308.00158) | 本研究探讨了使用Fine-Tuned的OpenAI LLM进行翻译质量估计的能力，实验证明可以通过Fine-Tuned的ChatGPT来预测机器翻译的质量，但仍有改进的空间。 |
| [^22] | [Boosting Adverse Drug Event Normalization on Social Media: General-Purpose Model Initialization and Biomedical Semantic Text Similarity Benefit Zero-Shot Linking in Informal Contexts.](http://arxiv.org/abs/2308.00157) | 本论文提出了一种新的方法，通过通用模型初始化和语义文本相似性微调，在社交媒体上提升不良药物事件归一化的效果，并取得了最先进的性能。 |
| [^23] | [Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods.](http://arxiv.org/abs/2308.00129) | 本文研究了语音数据的表示学习，主要关注多种设置和多种方法，旨在通过利用大量无标签和弱标签数据以及附加数据模态，改进下游序列预测任务。 |
| [^24] | [Getting pwn'd by AI: Penetration Testing with Large Language Models.](http://arxiv.org/abs/2308.00121) | 本文探讨了使用大型语言模型（如GPT3.5）作为AI助手来增强渗透测试人员的能力，实现了高级任务规划和低级漏洞寻找两种用例，取得了有前景的初步结果，并就提供该技术的伦理问题进行了讨论。 |
| [^25] | [A Modular Ontology for MODS -- Metadata Object Description Schema.](http://arxiv.org/abs/2308.00116) | 本研究开发了模块化MODS本体论（MMODS-O），用于解决元数据对象描述模式（MODS）在知识图谱环境下的限制问题，并采用模块化本体论设计方法学（MOMo）实现了平衡。 |
| [^26] | [Three Bricks to Consolidate Watermarks for Large Language Models.](http://arxiv.org/abs/2308.00113) | 本研究提出了三种基于理论和实证考虑的方法，巩固了用于大型语言模型的水印技术。新的统计检验方法能够在低错误阳性率下提供稳定的理论保证。与自然语言处理领域的经典基准测试相比，水印技术的有效性得到了验证，并且我们还开发了先进的检测方案，适用于具有大型语言模型访问权限和多位水印技术的场景。 |
| [^27] | [A Sentence is Worth a Thousand Pictures: Can Large Language Models Understand Human Language?.](http://arxiv.org/abs/2308.00109) | 本文分析了大型语言模型作为理论信息丰富表示和非理论强大机械工具的贡献，并指出当前的模型发展和利用中仍然缺乏关键能力。 |
| [^28] | [DPBERT: Efficient Inference for BERT based on Dynamic Planning.](http://arxiv.org/abs/2308.00108) | DPBERT是一个基于动态规划的高效BERT推理方法，通过选择一部分transformer层来加速推理过程，在保持高准确性的同时降低延迟。 |
| [^29] | [Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation.](http://arxiv.org/abs/2308.00085) | 本文提出了一种基于常识的因果解释方法，用于多样化的共情回应生成。该方法综合考虑了用户的角度和系统的角度，并通过集成常识知识提升了ChatGPT在系统的推理能力。实验结果表明，该方法在多项评估指标上超过了其他方法。 |
| [^30] | [Towards Semantically Enriched Embeddings for Knowledge Graph Completion.](http://arxiv.org/abs/2308.00081) | 本论文讨论了知识图谱补全算法以及利用嵌入模型捕捉知识图谱中语义的不同方法，并提出知识图谱和语言模型相互受益的观点。 |
| [^31] | [Trustworthiness of Children Stories Generated by Large Language Models.](http://arxiv.org/abs/2308.00073) | 本研究评估了大型语言模型生成的儿童故事的可信度，并发现它们在质量和细节方面仍然有困难，无法达到实际故事的水平。 |
| [^32] | [How User Language Affects Conflict Fatality Estimates in ChatGPT.](http://arxiv.org/abs/2308.00072) | 在以色列-巴勒斯坦和土耳其-库尔德冲突的背景下，本研究探讨了用户语言对ChatGPT中冲突死亡估计的影响。研究发现，在使用攻击者的语言进行查询时，GPT-3.5提供的估计较使用被攻击群体的语言查询时低27±11％。此外，否认存在此类袭击的回答进一步增加了这种差异，形成了一种新的偏见机制，可能加大现有的媒体偏见并加剧信息孤立。 |
| [^33] | [Interpretable Stereotype Identification through Reasoning.](http://arxiv.org/abs/2308.00071) | 本研究通过使用推理方法，在零射击刻板印象识别中取得了重要的进展，并发现推理的性能增益远远超过模型规模扩展的增益。推理不仅提高了准确性，还提高了决策的可解释性。 |
| [^34] | [FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models.](http://arxiv.org/abs/2308.00065) | FinPT是一种新颖的资金风险预测方法，通过在大型预训练基础模型上进行个人资料调整，填充金融表格数据并获得自然语言客户资料，从而提高预测准确性。 |
| [^35] | [Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment.](http://arxiv.org/abs/2308.00016) | 本论文提出了一种通过引入人机交互的新型 alpha 挖掘范式，并利用大型语言模型的能力，通过一种新颖的提示工程算法框架，开发了 Alpha-GPT。通过多个实验，展示了 Alpha-GPT 在量化投资领域的有效性和优势。 |
| [^36] | [A new mapping of technological interdependence.](http://arxiv.org/abs/2308.00014) | 本文利用文本挖掘和网络分析的方法，研究了不同部门之间的技术相互依赖关系，并证明了在技术创新中，间接联系和直接联系同等重要。 |
| [^37] | [An Overview Of Temporal Commonsense Reasoning and Acquisition.](http://arxiv.org/abs/2308.00002) | 本文综述了时间常识推理领域的研究进展，重点关注通过增强语言模型的性能来提高推理能力，并对多个数据集进行评估。然而，这些增强模型仍然难以达到人类水平的推理能力。 |
| [^38] | [Deep Dive into the Language of International Relations: NLP-based Analysis of UNESCO's Summary Records.](http://arxiv.org/abs/2307.16573) | 该研究开发了基于UNESCO摘要记录的自动分析工具，通过创新的主题建模和紧张检测方法，提供对联合国教科文组织世界遗产名录和代表性非物质文化遗产人类代表作名录决策过程的有价值洞察。应用程序可为外交官、律师、政治科学家和国际关系研究人员提供高效搜索所选主题相关文档段落和特定演讲者陈述的便利。 |
| [^39] | [Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning.](http://arxiv.org/abs/2307.15411) | 本研究通过对比监督学习和上下文学习，发现大型语言模型在学习行为上受到金标签的显著影响，但对于上下文学习来说，标签不平衡影响较小。实证结果显示上下文学习对标签扰动的敏感性较低。 |
| [^40] | [Gzip versus bag-of-words for text classification with KNN.](http://arxiv.org/abs/2307.15002) | Gzip与KNN相比较在文本分类中，我们发现通过简单的词袋匹配可以获得类似或更好的准确性，并且更加高效。 |
| [^41] | [CliniDigest: A Case Study in Large Language Model Based Large-Scale Summarization of Clinical Trial Descriptions.](http://arxiv.org/abs/2307.14522) | CliniDigest是一个基于大规模语言模型的临床试验摘要工具，能够实时、真实和全面地将长篇试验描述压缩成简洁的摘要。 |
| [^42] | [EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus.](http://arxiv.org/abs/2307.11760) | EmotionPrompt是一个基于心理学的方法，通过将情感刺激融入到提示中，提升了大型语言模型在各项任务上的性能，并且同时改善了其真实性和信息量。 |
| [^43] | [Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models.](http://arxiv.org/abs/2307.11224) | Jina Embeddings是一组高性能的句子嵌入模型，能够捕捉文本的语义本质。该论文详细介绍了Jina Embeddings的开发过程，并通过性能评估验证了其优越性能。 |
| [^44] | [How is ChatGPT's behavior changing over time?.](http://arxiv.org/abs/2307.09009) | 本论文评估了GPT-3.5和GPT-4模型在不同时间点上的性能和行为变化，发现它们的表现可以有很大的差异，包括在解决数学问题、回答敏感问题、生成代码和视觉推理等任务上。这些结果表明相同的语言模型服务的行为在相对短的时间内可以发生显著变化。 |
| [^45] | [SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering.](http://arxiv.org/abs/2307.04192) | SAS视频问答通过自适应采样策略解决了视频问答中的问题，提高了效率和准确性 |
| [^46] | [Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks.](http://arxiv.org/abs/2307.02477) | 通过反事实任务的研究，我们发现当前的语言模型具备一定的抽象推理能力，但它们在任务求解过程中往往也依赖于狭窄、难以转移的过程，这对语言模型的性能解释和理解有着重要的启示。 |
| [^47] | [Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors.](http://arxiv.org/abs/2306.17156) | 该论文系统评估了ChatGPT、GPT-4和人类导师在不同的编程教育场景中的表现，并发现GPT-4优于ChatGPT，接近于人类导师。 |
| [^48] | [CrunchGPT: A chatGPT assisted framework for scientific machine learning.](http://arxiv.org/abs/2306.15551) | CrunchGPT是一个基于ChatGPT的科学机器学习辅助框架，通过简单的用户提示来协调整个科学机器学习的工作流程，实现无缝集成数据和物理知识，解决了SciML在预处理、问题建模、代码生成、后处理和分析等方面的耗时问题，拓展了其工业应用和数字孪生框架的适用性。 |
| [^49] | [W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition.](http://arxiv.org/abs/2305.18624) | W-procer是一种基于加权原型对比学习的医学少样本命名实体识别方法，在构建基于原型的对比损失和加权网络方面具有创新性，优于现有的最先进方法。 |
| [^50] | [Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models.](http://arxiv.org/abs/2305.17446) | 该论文通过发现预训练语言模型的内在任务特定子空间，提出了一种重新参数化和微调模型的新方法。研究发现在该子空间中，只需少量自由参数即可有效微调模型，并且某些维度对于引入任务特定知识至关重要。 |
| [^51] | [Diable: Efficient Dialogue State Tracking as Operations on Tables.](http://arxiv.org/abs/2305.17020) | Diable是一个高效的对话状态跟踪系统，它通过在表格上进行操作来更新对话状态，相比现有方法时间效率提高了2.4倍，同时保持了竞争性的目标准确性。 |
| [^52] | [AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback.](http://arxiv.org/abs/2305.14387) | 该论文提出了一种名为AlpacaFarm的低成本模拟器，该模拟器为从人类反馈中学习的研究和开发提供了一种解决方案，通过设计LLM提示来模拟人类反馈，提出自动评估并提供参考实现，克服了数据收集的高昂成本、缺乏可信的评估和缺乏参考方法实现的挑战。 |
| [^53] | [Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding.](http://arxiv.org/abs/2305.13899) | 本文针对连续学习场景下的口语语言理解问题，提出了增量类别场景和三种知识蒸馏方法，并显示序列级知识蒸馏可以显著改善绩效。 |
| [^54] | [Continual Multimodal Knowledge Graph Construction.](http://arxiv.org/abs/2305.08698) | 连续多模态知识图谱构建面临着灾难性遗忘的挑战，需要解决新增实体和关系以及多模态源数据变化的问题。 |
| [^55] | [The Current State of Summarization.](http://arxiv.org/abs/2305.04853) | 摘要生成领域目前的研究关注点在于预训练的编码器-解码器模型和大规模自回归语言模型的转变，以及评估摘要生成系统的挑战和指令调整模型在零样本摘要生成中的潜力。 |
| [^56] | [Fundamental Limitations of Alignment in Large Language Models.](http://arxiv.org/abs/2304.11082) | 本文通过提出一种理论方法——行为期望边界（BEB），展示了大型语言模型中对齐的基本限制，并证明任何对齐过程都无法根除不希望的行为，这对于防止恶意攻击是不安全的。 |
| [^57] | [mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection.](http://arxiv.org/abs/2303.09901) | 本研究提出了mCPT模型用于多语言的、多标签的零样本或少样本的框架检测任务，并在西班牙语和其他8种语言中取得了良好的成绩。该方案采用了基于多语言变压器的预训练程序，使用标签感知对比损失函数。 |
| [^58] | [Depression Detection Using Digital Traces on Social Media: A Knowledge-aware Deep Learning Approach.](http://arxiv.org/abs/2303.05389) | 本研究通过使用数字痕迹在社交媒体上检测抑郁症，提出了一种深度知识感知的抑郁症检测框架，并揭示了关键因素。通过真实数据进行的实证研究证明了其准确性和有效性。 |
| [^59] | [HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales.](http://arxiv.org/abs/2302.12189) | 这个论文介绍了HL数据集，该数据集扩展了COCO数据集，包含14997个图像和134,973个人工注释的高级别描述，涉及场景、动作和理由，可以用于对视觉和语言模型进行更高级别的测试和微调。 |
| [^60] | [In-Context Retrieval-Augmented Language Models.](http://arxiv.org/abs/2302.00083) | 本研究提出了一种上下文检索增强的语言模型（In-Context RALM）方法，通过将相关文件作为输入的一部分，无需对语言模型进行进一步的训练即可显著提高语言建模性能和源归因能力，并且相对于现有的RALM方法，它具有更简单的部署过程。 |
| [^61] | [Parallel Context Windows for Large Language Models.](http://arxiv.org/abs/2212.10947) | PCW方法可以缓解现成LLM的上下文窗口限制，将长上下文划分为块并在每个窗口内重用位置嵌入，提高了处理长文本的性能表现。 |
| [^62] | [Reliable Measures of Spread in High Dimensional Latent Spaces.](http://arxiv.org/abs/2212.08172) | 本文定义了数据展开度的概念，并发现常用的度量方法不可靠，提出了八种备选的数据展开度量方法，并推荐使用一种基于主成分和一种基于熵的度量方法，这些方法可以可靠地比较不同模型的展开度。 |
| [^63] | [Few-shot Multimodal Sentiment Analysis based on Multimodal Probabilistic Fusion Prompts.](http://arxiv.org/abs/2211.06607) | 本论文提出了一种名为多模态概率融合提示（MultiPoint）的方法，通过利用多模态的不同线索进行少样本情感分析。这种方法解决了现有研究中依赖于大规模监督数据的问题，并通过统一的多模态提示来减少不同模态之间的差异。 |
| [^64] | [Benchmarking Compositionality with Formal Languages.](http://arxiv.org/abs/2208.08195) | 通过使用形式语言的方法进行基准测试，我们发现大型NLP神经模型要么完全学习关系，要么完全不学习。转换覆盖率是关键，将软可学习界限设置为每个转换400个示例。 |

# 详细

[^1]: CodeBPE: 探索用于源代码的大型语言模型预训练的子标记化选项

    CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code. (arXiv:2308.00683v1 [cs.LG])

    [http://arxiv.org/abs/2308.00683](http://arxiv.org/abs/2308.00683)

    CodeBPE研究了用于源代码的大型语言模型预训练中不同子标记化选项的影响，找出了最有效和长度高效的子标记化方法，通过减少平均长度17%且不影响下游性能，可能提高质量0.5-2%。

    

    最近的研究广泛采用了针对源代码的大型语言模型预训练，提出了源代码特定的预训练目标，并研究了不同基于Transformer的语言模型架构在源代码中的适用性。本研究调查了这些模型的另一个重要方面，即不同子标记化选项的影响，并旨在确定最有效和长度高效的子标记化，考虑到代码的特殊性。我们提出了一种子标记化方法，平均长度减少了17%，且没有下游性能下降，并且表明精心选择的子标记化可能会提高质量0.5-2%，可能会略微增加长度。

    Recent works have widely adopted large language model pretraining for source code, suggested source code-specific pretraining objectives and investigated the applicability of various Transformer-based language model architectures for source code. This work investigates another important aspect of such models, namely the effect of different subtokenization options, and aims at identifying most effective and length-efficient subtokenizations, taking into account code specifics. We propose subtokenziation that reduces average length by 17% without downstream performance drop, and show that a carefully chosen subtokenization may improve quality by 0.5-2%, possibly with some length increase.
    
[^2]: 工具文档使得大型语言模型能够进行零-shot工具使用

    Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models. (arXiv:2308.00675v1 [cs.CL])

    [http://arxiv.org/abs/2308.00675](http://arxiv.org/abs/2308.00675)

    这项工作提出了使用工具文档作为教导大型语言模型使用新工具的替代方法，并通过实证研究证明，仅使用工具文档的零-shot提示足以实现正确的工具使用。

    

    如今，通过提供一些工具使用的演示来教授大型语言模型（LLM）使用新工具。不幸的是，演示很难获得，并且如果选择了错误的演示，可能会导致不良的偏见使用。即使在罕见的情况下，演示是readily available的，也没有原则性的选择协议来确定提供多少个和哪些演示。随着任务变得更加复杂，选择搜索组合数的增长成为不可处理的。我们的工作提供了一种替代演示的方法：工具文档。我们主张使用工具文档来描述各个工具的使用，而不是演示。我们通过跨视觉和语言模态的6个任务上的三个主要实证发现支持我们的主张。首先，在现有的基准测试上，仅使用工具文档的零-shot提示足以引出正确的工具使用，达到了few-shot提示的性能水平。

    Today, large language models (LLMs) are taught to use new tools by providing a few demonstrations of the tool's usage. Unfortunately, demonstrations are hard to acquire, and can result in undesirable biased usage if the wrong demonstration is chosen. Even in the rare scenario that demonstrations are readily available, there is no principled selection protocol to determine how many and which ones to provide. As tasks grow more complex, the selection search grows combinatorially and invariably becomes intractable. Our work provides an alternative to demonstrations: tool documentation. We advocate the use of tool documentation, descriptions for the individual tool usage, over demonstrations. We substantiate our claim through three main empirical findings on 6 tasks across both vision and language modalities. First, on existing benchmarks, zero-shot prompts with only tool documentation are sufficient for eliciting proper tool usage, achieving performance on par with few-shot prompts. Secon
    
[^3]: JIANG: 中国开放基础语言模型

    JIANG: Chinese Open Foundation Language Model. (arXiv:2308.00624v1 [cs.CL])

    [http://arxiv.org/abs/2308.00624](http://arxiv.org/abs/2308.00624)

    JIANG是一个专为中文设计的开放式语言模型，通过使用大量的中文语料库进行训练和优化结构，能够更好地在中文中发挥其能力。

    

    随着大型语言模型技术的进步，它展示了接近人类水平的能力，可以在各种任务上表现出色。这一成就引起了公司和科研机构的极大兴趣，导致对这些模型的研究和开发进行了大量投资。虽然在这个时期出现了许多大型模型，但其中大多数主要是基于英文数据进行训练。虽然它们在其他语言（如中文）中表现出了不错的性能，但由于词汇设计和训练语料库等因素，其潜力仍然受限，无法完全发挥在中文中的能力。为了解决这个问题，我们介绍了名为JIANG（姜的拼音）的专门针对中文的模型。我们收集了大量的中文语料库来训练模型，并对其结构进行了优化。广泛的实验结果表明...

    With the advancements in large language model technology, it has showcased capabilities that come close to those of human beings across various tasks. This achievement has garnered significant interest from companies and scientific research institutions, leading to substantial investments in the research and development of these models. While numerous large models have emerged during this period, the majority of them have been trained primarily on English data. Although they exhibit decent performance in other languages, such as Chinese, their potential remains limited due to factors like vocabulary design and training corpus. Consequently, their ability to fully express their capabilities in Chinese falls short. To address this issue, we introduce the model named JIANG (Chinese pinyin of ginger) specifically designed for the Chinese language. We have gathered a substantial amount of Chinese corpus to train the model and have also optimized its structure. The extensive experimental res
    
[^4]: 多模态表情包情感分类的单模态中间训练

    Unimodal Intermediate Training for Multimodal Meme Sentiment Classification. (arXiv:2308.00528v1 [cs.CL])

    [http://arxiv.org/abs/2308.00528](http://arxiv.org/abs/2308.00528)

    本研究提出了一种补充多模态表情包分类器训练的单模态中间训练方法，通过利用大量情感标记的单模态数据，显著提高了性能，并证明了在不减少下游模型性能的情况下，可以减少标记表情包的训练集数量。

    

    互联网表情包作为用户生成的内容之一，对于自动情感分类来说仍然具有挑战。已标记的表情包的可用性是开发多模态表情包分类器的一种障碍。为了解决标记表情包数量不足的问题，我们提出使用单模态(仅图像和仅文本)数据来补充多模态表情包分类器的训练。在这项工作中，我们提出了一种新颖的监督式中间训练的变体，使用相对丰富的情感标记单模态数据。我们的结果表明，与单模态文本数据的融合可以显著提高性能。此外，我们还表明，可以将标记表情包的训练集减少40%，而不降低下游模型的性能。

    Internet Memes remain a challenging form of user-generated content for automated sentiment classification. The availability of labelled memes is a barrier to developing sentiment classifiers of multimodal memes. To address the shortage of labelled memes, we propose to supplement the training of a multimodal meme classifier with unimodal (image-only and text-only) data. In this work, we present a novel variant of supervised intermediate training that uses relatively abundant sentiment-labelled unimodal data. Our results show a statistically significant performance improvement from the incorporation of unimodal text data. Furthermore, we show that the training set of labelled memes can be reduced by 40% without reducing the performance of the downstream model.
    
[^5]: 在医学教育中，用于大规模非结构化文本数据的检索增强生成和代表性向量摘要

    Retrieval Augmented Generation and Representative Vector Summarization for large unstructured textual data in Medical Education. (arXiv:2308.00479v1 [cs.CL])

    [http://arxiv.org/abs/2308.00479](http://arxiv.org/abs/2308.00479)

    本文介绍了在医学教育领域中，使用检索增强生成（RAG）将非参数化知识库与大规模语言模型（LLMs）结合，以解决幻觉和有害答案问题，并提出了一种使用代表性向量的抽取性和抽象性摘要方法。

    

    大规模语言模型越来越多地用于各种任务，包括内容生成和作为聊天机器人。尽管它们在一般任务中表现出色，但在应用于特定领域任务时，需要对LLMs进行调整以减轻产生幻觉和有害答案的问题。检索增强生成（RAG）允许轻松地连接和操作非参数化知识库到LLMs上。本文讨论了RAG在医学教育领域的应用。提出了一种使用代表性向量的大规模非结构化文本数据的抽取性和抽象性摘要方法。

    Large Language Models are increasingly being used for various tasks including content generation and as chatbots. Despite their impressive performances in general tasks, LLMs need to be aligned when applying for domain specific tasks to mitigate the problems of hallucination and producing harmful answers. Retrieval Augmented Generation (RAG) allows to easily attach and manipulate a non-parametric knowledgebases to LLMs. Applications of RAG in the field of medical education are discussed in this paper. A combined extractive and abstractive summarization method for large unstructured textual data using representative vectors is proposed.
    
[^6]: 大型语言模型的工具结构嵌入

    Structural Embeddings of Tools for Large Language Models. (arXiv:2308.00447v1 [cs.AI])

    [http://arxiv.org/abs/2308.00447](http://arxiv.org/abs/2308.00447)

    这篇论文突出了大型语言模型（LLM）与外部工具之间基于图的交互方法的重要性，并提出了一个指导与LLM集成大量外部工具的框架。

    

    显而易见，当前大型语言模型(LLMs)的状态需要引入外部工具。已经有大量文献记录了其缺乏直接的代数和逻辑推理，并促使研究人员开发了允许LLMs通过外部工具运行的框架。特定任务的工具利用的本体性质可以用有向无环图(DAG)很好地描述。本文的核心目标是突出强调在不久的将来，基于图的方法对LLM-工具交互的重要性。我们提出了一个示范性框架，用于指导指数级增加的外部工具与LLMs的编排，其中工具的目标和功能以图形方式进行层次结构编码。假设作为定义在这里的工具，思维链(CoT)的文本片段可以被想象为一种工具，那么基于图的框架也可以在这个特定方向上开辟新的途径。

    It is evident that the current state of Large Language Models (LLMs) necessitates the incorporation of external tools. The lack of straightforward algebraic and logical reasoning is well documented and prompted researchers to develop frameworks which allow LLMs to operate via external tools. The ontological nature of tool utilization for a specific task can be well formulated with a Directed Acyclic Graph (DAG). The central aim of the paper is to highlight the importance of graph based approaches to LLM-tool interaction in near future. We propose an exemplary framework to guide the orchestration of exponentially increasing numbers of external tools with LLMs,where objectives and functionalities of tools are graph encoded hierarchically. Assuming that textual segments of a Chain-of-Thought (CoT) can be imagined as a tool as defined here, the graph based framework can pave new avenues in that particular direction as well.
    
[^7]: SelfCheck: 使用LLMs自检其逐步推理的创新

    SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v1 [cs.AI])

    [http://arxiv.org/abs/2308.00436](http://arxiv.org/abs/2308.00436)

    本论文研究了使用LLMs自检逐步推理的能力，提出了一种零-shot验证方案，成功识别错误并提高了问答性能。

    

    最近大型语言模型（LLMs）的进展，尤其是链式思维（CoT）的发明，使得解决推理问题成为可能。然而，即使最强大的LLMs仍然难以处理需要非线性思维和多步推理的复杂问题。在这项工作中，我们探讨了LLMs是否具有识别自己错误的能力，而无需依赖外部资源。具体而言，我们研究了它们是否可以用于识别逐步推理中的个别错误。为此，我们提出了一种零-shot验证方案以识别此类错误。然后，我们使用此验证方案来改进问答性能，通过对不同生成的答案进行加权投票。我们在三个数学数据集-GSM8K，MathQA和MATH上测试了该方法，并发现它成功识别错误，并进而提高了最终的预测性能。

    The recent progress in large language models (LLMs), especially the invention of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning problems. However, even the strongest LLMs are still struggling with more complicated problems that require non-linear thinking and multi-step reasoning. In this work, we explore whether LLMs have the ability to recognize their own errors, without resorting to external resources. In particular, we investigate whether they can be used to identify individual errors within a step-by-step reasoning. To this end, we propose a zero-shot verification scheme to recognize such errors. We then use this verification scheme to improve question-answering performance, by using it to perform weighted voting on different generated answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and find that it successfully recognizes errors and, in turn, increases final predictive performance.
    
[^8]: 从复杂句子到链接命题的话语感知文本简化

    Discourse-Aware Text Simplification: From Complex Sentences to Linked Propositions. (arXiv:2308.00425v1 [cs.CL])

    [http://arxiv.org/abs/2308.00425](http://arxiv.org/abs/2308.00425)

    本研究提出了一种话语感知的文本简化方法，通过在句子的语义上下文中拆分和重新表达复杂的英语句子，解决了现有句法文本简化方法的保守性和忽略上下文连贯性的问题。

    

    对于下游自然语言处理应用来说，复杂句子是其中一个重要的障碍，句子的长度和复杂性会导致预测质量下降。文本简化的任务就是为了使句子更容易处理而对其进行修改，使用一系列的重写操作，比如重新排序、删除或拆分。现有的句法文本简化方法存在两个主要缺点：一方面，它们采取了非常保守的方法，倾向于保留输入而不进行转换；另一方面，它们忽略了文本的连贯性，需要跨越从句或句子的上下文来推断语句的真正含义。为了解决这些问题，我们提出了一种话语感知的文本简化方法，在句子所处的语义上下文中拆分和重新表达复杂的英语句子。基于一个基于语言学的转换阶段，该阶段利用从句和命题之间的关系来完成简化操作。

    Sentences that present a complex syntax act as a major stumbling block for downstream Natural Language Processing applications whose predictive quality deteriorates with sentence length and complexity. The task of Text Simplification (TS) may remedy this situation. It aims to modify sentences in order to make them easier to process, using a set of rewriting operations, such as reordering, deletion, or splitting. State-of-the-art syntactic TS approaches suffer from two major drawbacks: first, they follow a very conservative approach in that they tend to retain the input rather than transforming it, and second, they ignore the cohesive nature of texts, where context spread across clauses or sentences is needed to infer the true meaning of a statement. To address these problems, we present a discourse-aware TS approach that splits and rephrases complex English sentences within the semantic context in which they occur. Based on a linguistically grounded transformation stage that uses claus
    
[^9]: ZRIGF：一种用于无资源图像驱动对话生成的创新多模态框架

    ZRIGF: An Innovative Multimodal Framework for Zero-Resource Image-Grounded Dialogue Generation. (arXiv:2308.00400v1 [cs.CL])

    [http://arxiv.org/abs/2308.00400](http://arxiv.org/abs/2308.00400)

    ZRIGF是一种创新的多模态框架，用于无资源情境下的图像驱动对话生成。它通过对比预训练和生成预训练实现了视觉特征的对齐，生成有洞察力的回应。

    

    图像驱动的对话系统通过整合视觉信息，在生成高质量的回应方面具有很大优势。然而，当前的模型在无资源情境中难以有效利用这些信息，主要原因是图像和文本模态之间的差异。为了克服这一挑战，我们提出了一种创新的多模态框架，称为ZRIGF，它在无资源情境中融合了图像驱动信息来生成对话。ZRIGF采用两阶段学习策略，包括对比预训练和生成预训练。对比预训练包括一个文本-图像匹配模块，将图像和文本映射到统一的编码向量空间中，以及一个文本辅助的遮蔽图像建模模块，用于保存预训练的视觉特征并促进进一步的多模态特征对齐。生成预训练使用多模态融合模块和信息传递模块来生成有洞察力的回应。

    Image-grounded dialogue systems benefit greatly from integrating visual information, resulting in high-quality response generation. However, current models struggle to effectively utilize such information in zero-resource scenarios, mainly due to the disparity between image and text modalities. To overcome this challenge, we propose an innovative multimodal framework, called ZRIGF, which assimilates image-grounded information for dialogue generation in zero-resource situations. ZRIGF implements a two-stage learning strategy, comprising contrastive pre-training and generative pre-training. Contrastive pre-training includes a text-image matching module that maps images and texts into a unified encoded vector space, along with a text-assisted masked image modeling module that preserves pre-training visual features and fosters further multimodal feature alignment. Generative pre-training employs a multimodal fusion module and an information transfer module to produce insightful responses b
    
[^10]: 处理神经图表摘要中的幻觉

    Tackling Hallucinations in Neural Chart Summarization. (arXiv:2308.00399v1 [cs.CL])

    [http://arxiv.org/abs/2308.00399](http://arxiv.org/abs/2308.00399)

    本研究解决了神经图表摘要中的幻觉问题，通过自然语言推理预处理训练数据，显著减少了幻觉的产生，并通过缩短远距离依赖关系和添加与图表相关的信息来提高整体性能。

    

    文本生成中的幻觉是指系统产生的文本未与输入关联。本研究解决了神经图表摘要中的幻觉问题。我们的分析表明，图表摘要训练数据集中的目标端经常包含额外的信息，导致幻觉的产生。我们提出了一种基于自然语言推理(NLI)的方法来预处理训练数据，并通过人工评估表明，我们的方法显著减少了幻觉的产生。我们还发现，缩短输入序列中的远距离依赖关系，并添加与图表相关的信息，如标题和图例，可以提高整体性能。

    Hallucinations in text generation occur when the system produces text that is not grounded in the input. In this work, we tackle the problem of hallucinations in neural chart summarization. Our analysis shows that the target side of chart summarization training datasets often contains additional information, leading to hallucinations. We propose a natural language inference (NLI) based method to preprocess the training data and show through human evaluation that our method significantly reduces hallucinations. We also found that shortening long-distance dependencies in the input sequence and adding chart-related information like title and legends improves the overall performance.
    
[^11]: Fountain --一个智能上下文助手，结合知识表示和语言模型，用于制造风险识别

    Fountain -- an intelligent contextual assistant combining knowledge representation and language models for manufacturing risk identification. (arXiv:2308.00364v1 [cs.CL])

    [http://arxiv.org/abs/2308.00364](http://arxiv.org/abs/2308.00364)

    Fountain是一个智能上下文助手，将知识表示和语言模型结合，用于制造风险识别。它通过描述现有设计和流程准则以及提出的偏差来帮助识别风险，并提供可解释和一致的建议。

    

    在大规模生产过程中，与批准的设计或流程偏离会导致意想不到的风险。然而，这些变化有时是必要的，因为产品设计特征或制造过程的适应性发生了变化。一个主要挑战是在工作流程的早期阶段识别这些风险，以避免导致保修索赔的故障。我们开发了Fountain作为一个上下文助手，集成在偏差管理工作流程中，通过对现有设计和流程准则以及提出的偏差的描述来帮助识别风险。在制造环境中，该助手提供的建议必须是可解释和一致的。我们通过以下两个组件的结合实现了这一点：1）为特定领域语义相似性微调的语言模型，和2）以物料清单、失效模式和效应分析为基础的属性图的知识表示。

    Deviations from the approved design or processes during mass production can lead to unforeseen risks. However, these changes are sometimes necessary due to changes in the product design characteristics or an adaptation in the manufacturing process. A major challenge is to identify these risks early in the workflow so that failures leading to warranty claims can be avoided. We developed Fountain as a contextual assistant integrated in the deviation management workflow that helps in identifying the risks based on the description of the existing design and process criteria and the proposed deviation. In the manufacturing context, it is important that the assistant provides recommendations that are explainable and consistent. We achieve this through a combination of the following two components 1) language models finetuned for domain specific semantic similarity and, 2) knowledge representation in the form of a property graph derived from the bill of materials, Failure Modes and Effect Ana
    
[^12]: LimeAttack: 本地可解释方法用于文本硬标签对抗性攻击

    LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack. (arXiv:2308.00319v1 [cs.CL])

    [http://arxiv.org/abs/2308.00319](http://arxiv.org/abs/2308.00319)

    本文提出了一种名为LimeAttack的硬标签攻击算法，通过本地可解释方法来近似单词重要性排序，然后利用波束搜索找到最优解。

    

    自然语言处理模型容易受到对抗性样本的攻击。先前的文本对抗性攻击采用梯度或置信度分数来计算单词重要性排序，并生成对抗性样本。然而，在现实世界中，这些信息是不可用的。因此，我们将重点放在一个更现实和具有挑战性的场景上，名为硬标签攻击，其中攻击者只能查询模型并获取离散的预测标签。现有的硬标签攻击算法往往通过随机替换来初始化对抗性样本，然后利用复杂的启发式算法来优化对抗扰动。这些方法需要大量的模型查询，并且攻击成功率受到对手初始化的限制。在本文中，我们提出了一种名为LimeAttack的新型硬标签攻击算法，它利用本地可解释方法来近似单词重要性排序，然后采用波束搜索来找到最优解。

    Natural language processing models are vulnerable to adversarial examples. Previous textual adversarial attacks adopt gradients or confidence scores to calculate word importance ranking and generate adversarial examples. However, this information is unavailable in the real world. Therefore, we focus on a more realistic and challenging setting, named hard-label attack, in which the attacker can only query the model and obtain a discrete prediction label. Existing hard-label attack algorithms tend to initialize adversarial examples by random substitution and then utilize complex heuristic algorithms to optimize the adversarial perturbation. These methods require a lot of model queries and the attack success rate is restricted by adversary initialization. In this paper, we propose a novel hard-label attack algorithm named LimeAttack, which leverages a local explainable method to approximate word importance ranking, and then adopts beam search to find the optimal solution. Extensive experi
    
[^13]: 在大型语言模型中解锁组合性的上下文提示: 技能指导策略

    Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models. (arXiv:2308.00304v1 [cs.CL])

    [http://arxiv.org/abs/2308.00304](http://arxiv.org/abs/2308.00304)

    本文介绍了一种名为技能指导 (SKiC) 的提示策略，通过在上下文中演示基本技能和组合性示例，使大型语言模型具备解决更复杂问题的能力，并在泛化能力上取得几乎完美的表现。

    

    本文考虑了如何通过一种新颖的提示策略，在大型语言模型（LLMs）中激发组合性泛化能力的问题。组合性泛化使得LLMs能够解决比它们所见过的问题更困难的问题（即易于难的泛化），这是人类智能的关键推理能力。然而，即使是当前最先进的LLMs在这种形式的推理上仍然存在困难。为了弥合这一差距，我们提出了在上下文中的技能指导（SKiC）提示，它指导LLMs如何组合基本技能来解决更复杂的问题。我们发现，在相同的提示上展示技能和组合性示例是至关重要的。仅仅通过两个示例，我们的SKiC提示在技能和它们的组合能力之间形成了强大的协同效应。值得注意的是，它赋予了LLMs解决需要创新技能组合的未见问题的能力，实现了几乎完美的泛化。

    We consider the problem of eliciting compositional generalization capabilities in large language models (LLMs) with a novel type of prompting strategy. Compositional generalization empowers the LLMs to solve problems that are harder than the ones they have seen (i.e., easy-to-hard generalization), which is a critical reasoning capability of human-like intelligence. However, even the current state-of-the-art LLMs still struggle with this form of reasoning. To bridge this gap, we propose skills-in-context (SKiC) prompting, which instructs LLMs how to compose basic skills to resolve more complex problems. We find that it is crucial to demonstrate both the skills and the compositional examples within the same prompting context. With as few as two examplars, our SKiC prompting initiates strong synergies between skills and their composition capabilities. Notably, it empowers LLMs to solve unseen problems that require innovative skill compositions, achieving near-perfect generalization on a b
    
[^14]: 让Text-VQA中的V变得重要

    Making the V in Text-VQA Matter. (arXiv:2308.00295v1 [cs.CV])

    [http://arxiv.org/abs/2308.00295](http://arxiv.org/abs/2308.00295)

    本论文针对文本VQA中对视觉特征理解不足的问题，提出通过学习视觉特征来解决这一问题，通过将TextVQA和VQA数据集相结合进行模型训练，从而提高模型的准确性。

    

    文本VQA旨在通过阅读图像中的文本来回答问题。与VQA任务相比，它需要大量的场景-文本关系理解。最近的研究表明，数据集中的问题-答案对更关注图像中的文本，而对于视觉特征则给予较少重视，而且有些问题不需要理解图像。由于缺乏对视觉上下文的理解，使用该数据集训练的模型会预测出有偏差的答案。例如，在类似“标牌上写着什么？”的问题中，模型预测的答案总是“STOP”，这使得模型忽略了图像。为了解决这些问题，我们提出了一种方法，通过使用VQA数据集作为Text-VQA的外部知识，学习视觉特征（让V在Text-VQA中变得重要），以及OCR特征和问题特征。具体来说，我们将TextVQA数据集和VQA数据集进行合并，并在这个合并的数据集上训练模型。

    Text-based VQA aims at answering questions by reading the text present in the images. It requires a large amount of scene-text relationship understanding compared to the VQA task. Recent studies have shown that the question-answer pairs in the dataset are more focused on the text present in the image but less importance is given to visual features and some questions do not require understanding the image. The models trained on this dataset predict biased answers due to the lack of understanding of visual context. For example, in questions like "What is written on the signboard?", the answer predicted by the model is always "STOP" which makes the model to ignore the image. To address these issues, we propose a method to learn visual features (making V matter in TextVQA) along with the OCR features and question features using VQA dataset as external knowledge for Text-based VQA. Specifically, we combine the TextVQA dataset and VQA dataset and train the model on this combined dataset. Suc
    
[^15]: 多模态多损失融合网络

    Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])

    [http://arxiv.org/abs/2308.00264](http://arxiv.org/abs/2308.00264)

    多模态多损失融合网络通过最佳选择和融合多个模态的特征，提高了情感检测的性能，并在多个数据集上实现了最先进的结果。这些研究结果表明了用于增强神经网络中情感检测的特征选择和融合方法的优化方向。

    

    在这项工作中，我们研究了跨多个模态选择和融合特征的最佳方法，并将其组合在神经网络中以改善情感检测。我们比较了不同的融合方法并且研究了多损失训练在多模态融合网络中的影响，从而确定了与子网性能相关的有用发现。我们最好的模型在三个数据集（CMU-MOSI、CMU-MOSEI和CH-SIMS）上实现了最先进的性能，并且在大多数指标上优于其他方法。我们发现，训练多模态特征可以改善单模态测试，并且基于数据集注释模式设计融合方法可以增强模型性能。这些结果表明了在神经网络中增强情感检测的优化特征选择和融合方法的发展方向。

    In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
    
[^16]: 有效的古代汉语翻译：数据集、模型和评估

    Towards Effective Ancient Chinese Translation: Dataset, Model, and Evaluation. (arXiv:2308.00240v1 [cs.CL])

    [http://arxiv.org/abs/2308.00240](http://arxiv.org/abs/2308.00240)

    本论文提出了Erya模型，通过收集、清理和分类大量古代汉语材料，形成了最全面的古代汉语资源。该模型在双音节对齐替换和双层遮罩语言模型任务上表现出色，并在多个领域展示了出色的零-shot性能和迁移能力。

    

    解读古代汉语一直是理解中国广阔文学、传统和文明的关键。本文提出了针对古代汉语的Erya翻译模型。在数据集方面，我们从多个来源收集、清理和分类古代汉语材料，形成迄今为止最全面的古代汉语资源。从模型角度来看，我们设计了适用于古代汉语的Erya训练方法。我们设计了两个共同工作的任务：双音节对齐替换（DAS）和双层遮罩语言模型（DMLM）。从评估角度来看，我们建立了一个基准，用于在不同场景下评判古代汉语翻译质量，并评估各种现有模型的古代汉语翻译能力。我们的模型在五个领域表现出卓越的零-shot性能，对GPT-3.5模型的BLEU分数提高了+12.0，并且在人工评估结果上优于ERNIE Bot。随后的微调进一步展示了更强的迁移能力。

    Interpreting ancient Chinese has been the key to comprehending vast Chinese literature, tradition, and civilization. In this paper, we propose Erya for ancient Chinese translation. From a dataset perspective, we collect, clean, and classify ancient Chinese materials from various sources, forming the most extensive ancient Chinese resource to date. From a model perspective, we devise Erya training method oriented towards ancient Chinese. We design two jointly-working tasks: disyllabic aligned substitution (DAS) and dual masked language model (DMLM). From an evaluation perspective, we build a benchmark to judge ancient Chinese translation quality in different scenarios and evaluate the ancient Chinese translation capacities of various existing models. Our model exhibits remarkable zero-shot performance across five domains, with over +12.0 BLEU against GPT-3.5 models and better human evaluation results than ERNIE Bot. Subsequent fine-tuning further shows the superior transfer capability o
    
[^17]: 被指导的偏见：经过指导调优的语言模型呈现出新兴的认知偏见

    Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias. (arXiv:2308.00225v1 [cs.CL])

    [http://arxiv.org/abs/2308.00225](http://arxiv.org/abs/2308.00225)

    这项研究发现，经过指导调优的语言模型呈现出新兴的认知偏见，这对于理解和开发更可靠和无偏的语言模型至关重要。

    

    最近的研究表明，指导调优和从人类反馈中学习可以显著提高大语言模型（LMs）的能力。虽然这些调优方法可以使模型生成高质量的文本，但我们推测这些经过调优的模型可能会产生更多隐含的认知偏见。我们的研究提供了证据，表明这些经过调优的模型呈现出先前预训练模型中不存在或较不明显的偏见。我们对三种认知偏见进行了研究，包括矛盾效应、确定性效应和信念偏见，这些偏见已被证实对人类的决策和推理有影响。我们的研究结果突显了这些偏见在各种模型中的存在，特别是那些经过指导调优的模型，如Flan-T5、GPT3.5和GPT4。这项研究对于理解指导调优的LMs中的认知偏见是至关重要的，这有助于开发更可靠和无偏的语言模型。

    Recent studies show that instruction tuning and learning from human feedback improve the abilities of large language models (LMs) dramatically. While these tuning methods can make models generate high-quality text, we conjecture that more implicit cognitive biases may arise in these fine-tuned models. Our work provides evidence that these fine-tuned models exhibit biases that were absent or less pronounced in their pretrained predecessors. We examine the extent of this phenomenon in three cognitive biases - the decoy effect, the certainty effect, and the belief bias - all of which are known to influence human decision-making and reasoning. Our findings highlight the presence of these biases in various models, especially those that have undergone instruction tuning, such as Flan-T5, GPT3.5, and GPT4. This research constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models.
    
[^18]: 超越识别：用于语言模型的多位水印技术

    Advancing Beyond Identification: Multi-bit Watermark for Language Models. (arXiv:2308.00221v1 [cs.CL])

    [http://arxiv.org/abs/2308.00221](http://arxiv.org/abs/2308.00221)

    本研究提出了一种用于语言模型的多位水印技术——COLOR，可在语言模型生成过程中嵌入可追踪的多位信息，实现了提取水印、即时嵌入和维持文本质量等功能，同时允许零位检测。初步实验显示成功在中等长度的文本中嵌入了32位消息，准确率为91.9％。这项研究有效推进了对语言模型滥用的反制策略。

    

    本研究旨在积极应对大型语言模型在检测机器生成文本方面的滥用。尽管现有方法侧重于检测，但某些恶意滥用需要跟踪对手用户以进行反制。为了解决这个问题，我们提出了“多位水印通过颜色编码”（COLOR）的方法，在语言模型生成过程中嵌入可追踪的多位信息。利用零位水印技术的优势（Kirchenbauer等，2023a），COLOR实现了在没有模型访问权限的情况下提取水印、即时嵌入和维持文本质量的能力，同时允许零位检测。初步实验表明，在中等长度的文本（约500个标记）中成功嵌入了32位消息，准确率为91.9％。这项工作有效地推进了对语言模型滥用进行反制的策略。

    This study aims to proactively tackle misuse of large language models beyond identification of machine-generated text. While existing methods focus on detection, some malicious misuses demand tracing the adversary user for counteracting them. To address this, we propose "Multi-bit Watermark through Color-listing" (COLOR), embedding traceable multi-bit information during language model generation. Leveraging the benefits of zero-bit watermarking (Kirchenbauer et al., 2023a), COLOR enables extraction without model access, on-the-fly embedding, and maintains text quality, while allowing zero-bit detection all at the same time. Preliminary experiments demonstrates successful embedding of 32-bit messages with 91.9% accuracy in moderate-length texts ($\sim$500 tokens). This work advances strategies to counter language model misuse effectively.
    
[^19]: 生成模型作为复杂系统科学：如何理解大型语言模型的行为？

    Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?. (arXiv:2308.00189v1 [cs.LG])

    [http://arxiv.org/abs/2308.00189](http://arxiv.org/abs/2308.00189)

    生成模型作为复杂系统科学，它们能够完成任务的行为表现需要被解释和理解，以实现对其行为的控制和未来研究的指导。

    

    从预训练模型中引导出期望的行为，同时避免不良行为，重新定义了自然语言处理并正在重新塑造我们与计算机的交互方式。曾经是一个科学工程学科，将构建模块堆叠在一起，现在可以说已经是一个复杂系统科学，其中寻求出现的行为以支持以前无法想象的用例。尽管有越来越多的基准测试来衡量任务性能，但我们缺乏解释语言模型展示这些任务完成的行为的解释。我们提出了一个系统性的努力，将语言模型的行为分解为解释跨任务性能的类别，以指导机械解释并帮助未来分析研究。

    Coaxing out desired behavior from pretrained models, while avoiding undesirable ones, has redefined NLP and is reshaping how we interact with computers. What was once a scientific engineering discipline-in which building blocks are stacked one on top of the other-is arguably already a complex systems science, in which emergent behaviors are sought out to support previously unimagined use cases.  Despite the ever increasing number of benchmarks that measure task performance, we lack explanations of what behaviors language models exhibit that allow them to complete these tasks in the first place. We argue for a systematic effort to decompose language model behavior into categories that explain cross-task performance, to guide mechanistic explanations and help future-proof analytic research.
    
[^20]: 对抗性鲁棒性神经法律判断系统

    Adversarially Robust Neural Legal Judgement Systems. (arXiv:2308.00165v1 [cs.CL])

    [http://arxiv.org/abs/2308.00165](http://arxiv.org/abs/2308.00165)

    本论文提出了一种对抗性鲁棒的神经法律判断系统，通过对早期存在的系统进行实验发现它们无法处理对抗性攻击。经过大量实验，我们的方法在处理对抗性攻击方面明显优于现有最先进的法律判断预测系统。

    

    法律判断预测是根据案情描述来预测法院案件结果的任务。这些任务应用自然语言处理技术来基于事实预测法律判断结果。最近，大规模公开数据集和自然语言处理模型已经增加了与法律判断预测系统相关的研究。为了使这种系统在实践中有所帮助，它们应该能够抵御对抗性攻击。先前的工作主要集中在构建神经法律判断系统上，但对于创建鲁棒的法律判断预测系统（LJP）几乎没有给予足够的关注。我们对早期存在的LJP系统进行了对抗性攻击实验，并发现它们都无法处理攻击。在这项工作中，我们提出了一种构建鲁棒LJP系统的方法。在三个法律数据集上进行的大量实验表明，我们的方法在处理对抗性攻击方面明显优于现有最先进的LJP系统。

    Legal judgment prediction is the task of predicting the outcome of court cases on a given text description of facts of cases. These tasks apply Natural Language Processing (NLP) techniques to predict legal judgment results based on facts. Recently, large-scale public datasets and NLP models have increased research in areas related to legal judgment prediction systems. For such systems to be practically helpful, they should be robust from adversarial attacks. Previous works mainly focus on making a neural legal judgement system; however, significantly less or no attention has been given to creating a robust Legal Judgement Prediction(LJP) system. We implemented adversarial attacks on early existing LJP systems and found that none of them could handle attacks. In this work, we proposed an approach for making robust LJP systems. Extensive experiments on three legal datasets show significant improvements in our approach over the state-of-the-art LJP system in handling adversarial attacks. 
    
[^21]: 使用Fine-Tuned的OpenAI LLM预测机器翻译输出中的完美质量段落：是否可以从历史数据中捕捉编辑距离模式？

    Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v1 [cs.CL])

    [http://arxiv.org/abs/2308.00158](http://arxiv.org/abs/2308.00158)

    本研究探讨了使用Fine-Tuned的OpenAI LLM进行翻译质量估计的能力，实验证明可以通过Fine-Tuned的ChatGPT来预测机器翻译的质量，但仍有改进的空间。

    

    翻译质量估计（TQE）是将输出翻译部署到使用中之前的重要步骤。 TQE对于评估机器翻译（MT）和人工翻译（HT）的质量也是至关重要的，而不需要查看参考翻译。在这项工作中，我们检查了最先进的大型语言模型（LLMs）是否可以为TQE任务和它们的能力进行Fine-Tune。我们以ChatGPT为例，将TQE视为二元分类任务。使用英意和英德训练语料库，我们的实验结果显示，通过ChatGPT的API Fine-Tuned可以在预测翻译质量方面获得相对较高的得分，即是否需要编辑翻译，但肯定有改进准确性的空间。英意双语摘要可在论文中找到。

    Translation Quality Estimation (TQE) is an important step before deploying the output translation into usage. TQE is also critical in assessing machine translation (MT) and human translation (HT) quality without seeing the reference translations. In this work, we examine if the state-of-the-art large language models (LLMs) can be fine-tuned for the TQE task and their capability. We take ChatGPT as one example and approach TQE as a binary classification task. Using English-Italian and English-German training corpus, our experimental results show that fine-tuned ChatGPT via its API can achieve a relatively high score on predicting translation quality, i.e. if the translation needs to be edited, but there is definitely space to improve the accuracy. English-Italiano bilingual Abstract is available in the paper.
    
[^22]: 在社交媒体上提升不良药物事件归一化：通用模型初始化和生物医学语义文本相似性在非正式环境中受益的零样例链接

    Boosting Adverse Drug Event Normalization on Social Media: General-Purpose Model Initialization and Biomedical Semantic Text Similarity Benefit Zero-Shot Linking in Informal Contexts. (arXiv:2308.00157v1 [cs.CL])

    [http://arxiv.org/abs/2308.00157](http://arxiv.org/abs/2308.00157)

    本论文提出了一种新的方法，通过通用模型初始化和语义文本相似性微调，在社交媒体上提升不良药物事件归一化的效果，并取得了最先进的性能。

    

    生物医学实体链接，也被称为生物医学概念归一化，最近出现了零样例对比模型的兴起。然而，直到现在，这些模型的预训练材料主要包括专门的生物医学内容，比如MIMIC-III临床笔记和PubMed论文。虽然产生的领域内模型在许多生物医学任务中已经显示出了有希望的结果，但社交媒体文本上的不良药物事件归一化仍然具有挑战性。在本文中，我们提出了一种新的方法，通过BioLORD的通用模型初始化和名为STS的语义文本相似性微调，对社交媒体上的不良药物事件归一化进行研究。我们在几个社交媒体数据集上的实验结果证明了我们提出的方法的有效性，达到了最先进的性能。

    Biomedical entity linking, also known as biomedical concept normalization, has recently witnessed the rise to prominence of zero-shot contrastive models. However, the pre-training material used for these models has, until now, largely consisted of specialist biomedical content such as MIMIC-III clinical notes (Johnson et al., 2016) and PubMed papers (Sayers et al., 2021; Gao et al., 2020). While the resulting in-domain models have shown promising results for many biomedical tasks, adverse drug event normalization on social media texts has so far remained challenging for them (Portelli et al., 2022). In this paper, we propose a new approach for adverse drug event normalization on social media relying on general-purpose model initialization via BioLORD (Remy et al., 2022) and a semantic-text-similarity fine-tuning named STS. Our experimental results on several social media datasets demonstrate the effectiveness of our proposed approach, by achieving state-of-the-art performance. Based on
    
[^23]: 语音表示学习：使用单视图、多视图和多任务方法学习双向编码器

    Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods. (arXiv:2308.00129v1 [eess.AS])

    [http://arxiv.org/abs/2308.00129](http://arxiv.org/abs/2308.00129)

    本文研究了语音数据的表示学习，主要关注多种设置和多种方法，旨在通过利用大量无标签和弱标签数据以及附加数据模态，改进下游序列预测任务。

    

    本文集中研究了针对时间或空间序列数据的表示学习，旨在通过使用学习到的表示改进下游序列预测任务。有监督学习一直是训练深度神经网络学习良好顺序表示的主要方法。然而，扩展有监督学习的一个限制因素是缺乏足够的注释数据。受到这一挑战的启发，自然而然地探索能够利用大量无标签和弱标签数据以及附加数据模态的表示学习方法。本文描述了对语音数据的广泛研究。与大多数关注单一学习设置的其他作品不同，本文研究了多种设置：带辅助损失的有监督学习、无监督学习、半监督学习和多视图学习。除了不同的学习问题，本文还探讨了多种表示学习方法。

    This thesis focuses on representation learning for sequence data over time or space, aiming to improve downstream sequence prediction tasks by using the learned representations. Supervised learning has been the most dominant approach for training deep neural networks for learning good sequential representations. However, one limiting factor to scale supervised learning is the lack of enough annotated data. Motivated by this challenge, it is natural to explore representation learning methods that can utilize large amounts of unlabeled and weakly labeled data, as well as an additional data modality. I describe my broad study of representation learning for speech data. Unlike most other works that focus on a single learning setting, this thesis studies multiple settings: supervised learning with auxiliary losses, unsupervised learning, semi-supervised learning, and multi-view learning. Besides different learning problems, I also explore multiple approaches for representation learning. Tho
    
[^24]: 使用大型语言模型进行渗透测试：AI作为辅助

    Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v1 [cs.CL])

    [http://arxiv.org/abs/2308.00121](http://arxiv.org/abs/2308.00121)

    本文探讨了使用大型语言模型（如GPT3.5）作为AI助手来增强渗透测试人员的能力，实现了高级任务规划和低级漏洞寻找两种用例，取得了有前景的初步结果，并就提供该技术的伦理问题进行了讨论。

    

    软件安全测试领域，尤其是渗透测试是一项需要高水平专业知识的活动，并涉及许多手动测试和分析步骤。本文探讨了使用大型语言模型（如GPT3.5）来增强渗透测试人员的能力。我们研究了两种不同的用例：用于安全测试任务的高级任务规划和在易受攻击的虚拟机中进行低级漏洞寻找。对于后者，我们实现了一个闭环反馈，将由语言模型生成的低级操作与易受攻击的虚拟机（通过SSH连接）相连，并允许语言模型分析虚拟机状态以寻找漏洞，并提供具体的攻击向量。我们讨论了有前景的初步结果，详细介绍了改进的途径，并就提供该技术的伦理问题进行了讨论。

    The field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential usage of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providi
    
[^25]: MODS的模块化本体论 - 元数据对象描述模式

    A Modular Ontology for MODS -- Metadata Object Description Schema. (arXiv:2308.00116v1 [cs.CL])

    [http://arxiv.org/abs/2308.00116](http://arxiv.org/abs/2308.00116)

    本研究开发了模块化MODS本体论（MMODS-O），用于解决元数据对象描述模式（MODS）在知识图谱环境下的限制问题，并采用模块化本体论设计方法学（MOMo）实现了平衡。

    

    元数据对象描述模式（MODS）是用于描述图书概念和元数据的，并由美国国会图书馆维护。MODS的标准版本是基于XML思维的XML模式，这意味着它在知识图谱环境下存在一些重要限制。因此，我们开发了模块化MODS本体论（MMODS-O），它集成了MODS的所有元素和属性。在设计本体论时，我们采用了最近的模块化本体论设计方法学（MOMo），旨在在保守地与MODS向后兼容的同时，在模块化和高质量本体论设计之间取得平衡。

    The Metadata Object Description Schema (MODS) was developed to describe bibliographic concepts and metadata and is maintained by the Library of Congress. Its authoritative version is given as an XML schema based on an XML mindset which means that it has significant limitations for use in a knowledge graphs context. We have therefore developed the Modular MODS Ontology (MMODS-O) which incorporates all elements and attributes of the MODS XML schema. In designing the ontology, we adopt the recent Modular Ontology Design Methodology (MOMo) with the intention to strike a balance between modularity and quality ontology design on the one hand, and conservative backward compatibility with MODS on the other.
    
[^26]: 用于大型语言模型的三个方法巩固水印技术

    Three Bricks to Consolidate Watermarks for Large Language Models. (arXiv:2308.00113v1 [cs.CL])

    [http://arxiv.org/abs/2308.00113](http://arxiv.org/abs/2308.00113)

    本研究提出了三种基于理论和实证考虑的方法，巩固了用于大型语言模型的水印技术。新的统计检验方法能够在低错误阳性率下提供稳定的理论保证。与自然语言处理领域的经典基准测试相比，水印技术的有效性得到了验证，并且我们还开发了先进的检测方案，适用于具有大型语言模型访问权限和多位水印技术的场景。

    

    在判断生成文本和自然文本之间的差异越来越具有挑战性的背景下，水印技术被提出作为一种将生成文本归属于特定模型的有前景的技术。它改变了采样生成过程，留下了无形的痕迹在生成的输出中，以便于后续的检测。本研究基于三个理论和实证考虑，巩固了用于大型语言模型的水印技术。首先，我们引入了新的统计检验方法，提供了牢固的理论保证，即使在低错误阳性率下（小于10^(-6)），这些保证依然有效。其次，我们通过在自然语言处理领域中使用经典基准测试对比了水印技术的有效性，从而获得了关于它们在实际应用中可行性的见解。第三，我们为可以访问大型语言模型的情景以及多位水印技术开发了先进的检测方案。

    The task of discerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing generated text to a specific model. It alters the sampling generation process so as to leave an invisible trace in the generated output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10$^{\text{-6}}$). Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability. Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking.
    
[^27]: 一句话胜千张图片：大型语言模型能理解人类语言吗？

    A Sentence is Worth a Thousand Pictures: Can Large Language Models Understand Human Language?. (arXiv:2308.00109v1 [cs.CL])

    [http://arxiv.org/abs/2308.00109](http://arxiv.org/abs/2308.00109)

    本文分析了大型语言模型作为理论信息丰富表示和非理论强大机械工具的贡献，并指出当前的模型发展和利用中仍然缺乏关键能力。

    

    人工智能应用在依赖于下一个单词预测的语言相关任务中表现出巨大潜力。当前一代大型语言模型被认为能够达到类人语言表现，并且它们的应用被誉为人工通用智能的关键步骤，同时也是对人类语言认知和神经基础的重大进展的理解。我们分析了大型语言模型作为目标系统的理论信息丰富表示与非理论强大机械工具的贡献，并确定了当前发展和利用这些模型所缺失的关键能力。

    Artificial Intelligence applications show great potential for language-related tasks that rely on next-word prediction. The current generation of large language models have been linked to claims about human-like linguistic performance and their applications are hailed both as a key step towards Artificial General Intelligence and as major advance in understanding the cognitive, and even neural basis of human language. We analyze the contribution of large language models as theoretically informative representations of a target system vs. atheoretical powerful mechanistic tools, and we identify the key abilities that are still missing from the current state of development and exploitation of these models.
    
[^28]: DPBERT: 基于动态规划的高效BERT推理

    DPBERT: Efficient Inference for BERT based on Dynamic Planning. (arXiv:2308.00108v1 [cs.CL])

    [http://arxiv.org/abs/2308.00108](http://arxiv.org/abs/2308.00108)

    DPBERT是一个基于动态规划的高效BERT推理方法，通过选择一部分transformer层来加速推理过程，在保持高准确性的同时降低延迟。

    

    大规模预训练语言模型（如BERT）为自然语言处理的发展做出了重要贡献。然而，这些模型需要大量的计算资源，难以应用于计算能力有限的移动设备上。本文旨在解决现有自适应输入推理方法的缺点，这些方法未能充分利用BERT的结构。我们提出了DPBERT，一种新的微调策略，通过选择BERT的一部分transformer层作为计算路径，在推理过程中加速BERT的推理。为此，我们的方法在原始BERT模型中添加了一个规划模块，用于确定推理过程中是否包含或绕过某个层。在GLUE基准测试上的实验结果表明，我们的方法在保持98%准确性的情况下，将延迟降低到75%，相比最先进的自适应输入方法，获得了更好的准确度和速度权衡。

    Large-scale pre-trained language models such as BERT have contributed significantly to the development of NLP. However, those models require large computational resources, making it difficult to be applied to mobile devices where computing power is limited. In this paper we aim to address the weakness of existing input-adaptive inference methods which fail to take full advantage of the structure of BERT. We propose Dynamic Planning in BERT, a novel fine-tuning strategy that can accelerate the inference process of BERT through selecting a subsequence of transformer layers list of backbone as a computational path for an input sample. To do this, our approach adds a planning module to the original BERT model to determine whether a layer is included or bypassed during inference. Experimental results on the GLUE benchmark exhibit that our method reduces latency to 75\% while maintaining 98\% accuracy, yielding a better accuracy-speed trade-off compared to state-of-the-art input-adaptive met
    
[^29]: 先思考再回应：为共情回应生成集成常识的因果解释

    Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation. (arXiv:2308.00085v1 [cs.CL])

    [http://arxiv.org/abs/2308.00085](http://arxiv.org/abs/2308.00085)

    本文提出了一种基于常识的因果解释方法，用于多样化的共情回应生成。该方法综合考虑了用户的角度和系统的角度，并通过集成常识知识提升了ChatGPT在系统的推理能力。实验结果表明，该方法在多项评估指标上超过了其他方法。

    

    最近的共情回应生成方法试图整合常识知识或对情绪原因的推理，以更好地理解用户的经历和感受。然而，这些方法主要关注从用户的角度理解上下文的因果关系，忽略了系统的角度。本文提出了一种基于常识的因果解释方法，用于多样化的共情回应生成，同时考虑用户的角度（用户的欲望和反应）和系统的角度（系统的意图和反应）。我们通过将上下文学习与常识知识相结合，增强了ChatGPT在系统的角度上的推理能力。然后，我们将基于常识的因果解释与ChatGPT和基于T5模型的方法进行整合。实验评估表明，我们的方法在自动评估和人工评估上优于其他可比较的方法。

    Recent approaches to empathetic response generation try to incorporate commonsense knowledge or reasoning about the causes of emotions to better understand the user's experiences and feelings. However, these approaches mainly focus on understanding the causalities of context from the user's perspective, ignoring the system's perspective. In this paper, we propose a commonsense-based causality explanation approach for diverse empathetic response generation that considers both the user's perspective (user's desires and reactions) and the system's perspective (system's intentions and reactions). We enhance ChatGPT's ability to reason for the system's perspective by integrating in-context learning with commonsense knowledge. Then, we integrate the commonsense-based causality explanation with both ChatGPT and a T5-based model. Experimental evaluations demonstrate that our method outperforms other comparable methods on both automatic and human evaluations.
    
[^30]: 为知识图谱补全构建语义丰富的嵌入模型

    Towards Semantically Enriched Embeddings for Knowledge Graph Completion. (arXiv:2308.00081v1 [cs.AI])

    [http://arxiv.org/abs/2308.00081](http://arxiv.org/abs/2308.00081)

    本论文讨论了知识图谱补全算法以及利用嵌入模型捕捉知识图谱中语义的不同方法，并提出知识图谱和语言模型相互受益的观点。

    

    基于嵌入模型的知识图谱补全在过去几年中越来越受关注。目前的大多数算法将知识图谱视为一个多向标记图，缺乏捕捉底层语义的能力。与此同时，大型语言模型（LLMs）已经捕获了大量信息，这一捕获对人工智能领域产生了革命性影响。知识图谱可以从LLMs中受益，反之亦然。本文讨论了基于不同生成嵌入模型变体的知识图谱补全算法。首先讨论了各种知识图谱补全算法，如转导和归纳链接预测以及实体类型预测算法。然后，介绍了利用知识图谱中的类型信息、LLMs以及捕捉不同描述逻辑公理中的语义的算法。最后，通过对现有算法的关键反思对论文进行总结。

    Embedding based Knowledge Graph (KG) Completion has gained much attention over the past few years. Most of the current algorithms consider a KG as a multidirectional labeled graph and lack the ability to capture the semantics underlying the schematic information. In a separate development, a vast amount of information has been captured within the Large Language Models (LLMs) which has revolutionized the field of Artificial Intelligence. KGs could benefit from these LLMs and vice versa. This vision paper discusses the existing algorithms for KG completion based on the variations for generating KG embeddings. It starts with discussing various KG completion algorithms such as transductive and inductive link prediction and entity type prediction algorithms. It then moves on to the algorithms utilizing type information within the KGs, LLMs, and finally to algorithms capturing the semantics represented in different description logic axioms. We conclude the paper with a critical reflection on
    
[^31]: 大型语言模型生成的儿童故事的可信度研究

    Trustworthiness of Children Stories Generated by Large Language Models. (arXiv:2308.00073v1 [cs.CL])

    [http://arxiv.org/abs/2308.00073](http://arxiv.org/abs/2308.00073)

    本研究评估了大型语言模型生成的儿童故事的可信度，并发现它们在质量和细节方面仍然有困难，无法达到实际故事的水平。

    

    大型语言模型（LLM）展示了生成文学文本的巨大能力。然而，它们在生成儿童故事方面的效果尚未得到充分的检验。在这项研究中，我们使用各种指标评估了LLM生成的儿童故事的可信度，并将我们的结果与旧有和新的儿童故事进行比较和对比，以更好地评估它们的重要性。我们的研究结果表明，LLM在生成儿童故事的质量和细微差别方面仍存在困难，无法达到实际故事的水平。

    Large Language Models (LLMs) have shown a tremendous capacity for generating literary text. However, their effectiveness in generating children's stories has yet to be thoroughly examined. In this study, we evaluate the trustworthiness of children's stories generated by LLMs using various measures, and we compare and contrast our results with both old and new children's stories to better assess their significance. Our findings suggest that LLMs still struggle to generate children's stories at the level of quality and nuance found in actual stories
    
[^32]: 用户语言对ChatGPT中冲突死亡估计的影响

    How User Language Affects Conflict Fatality Estimates in ChatGPT. (arXiv:2308.00072v1 [cs.CL])

    [http://arxiv.org/abs/2308.00072](http://arxiv.org/abs/2308.00072)

    在以色列-巴勒斯坦和土耳其-库尔德冲突的背景下，本研究探讨了用户语言对ChatGPT中冲突死亡估计的影响。研究发现，在使用攻击者的语言进行查询时，GPT-3.5提供的估计较使用被攻击群体的语言查询时低27±11％。此外，否认存在此类袭击的回答进一步增加了这种差异，形成了一种新的偏见机制，可能加大现有的媒体偏见并加剧信息孤立。

    

    OpenAI的ChatGPT语言模型因其强大的复杂问题解决和信息检索能力而备受青睐。然而，关于该模型是否会复制语言特定训练数据中存在的偏见的担忧也不断出现。在本研究中，我们在以色列-巴勒斯坦和土耳其-库尔德冲突的背景下解决了这个问题。使用GPT-3.5，我们采用自动查询过程，以希伯来语和阿拉伯语查询关于特定空袭中的伤亡人数。我们的分析发现，当使用攻击者的语言进行查询时，GPT-3.5提供的死亡估计较使用被攻击群体的语言查询时低27±11％。否认存在此类袭击的回答进一步增加了这种差异，创造了一种在常规搜索引擎中不存在的新的偏见机制。这种语言偏见有可能放大现有的媒体偏见，并加剧信息孤立，最终加重冲突。

    OpenAI's ChatGPT language model has gained popularity as a powerful tool for complex problem-solving and information retrieval. However, concerns arise about the reproduction of biases present in the language-specific training data. In this study, we address this issue in the context of the Israeli-Palestinian and Turkish-Kurdish conflicts. Using GPT-3.5, we employed an automated query procedure to inquire about casualties in specific airstrikes, in both Hebrew and Arabic for the former conflict and Turkish and Kurdish for the latter. Our analysis reveals that GPT-3.5 provides 27$\pm$11 percent lower fatality estimates when queried in the language of the attacker than in the language of the targeted group. Evasive answers denying the existence of such attacks further increase the discrepancy, creating a novel bias mechanism not present in regular search engines. This language bias has the potential to amplify existing media biases and contribute to information bubbles, ultimately reinf
    
[^33]: 可解释的推理方法用于刻板印象识别

    Interpretable Stereotype Identification through Reasoning. (arXiv:2308.00071v1 [cs.CL])

    [http://arxiv.org/abs/2308.00071](http://arxiv.org/abs/2308.00071)

    本研究通过使用推理方法，在零射击刻板印象识别中取得了重要的进展，并发现推理的性能增益远远超过模型规模扩展的增益。推理不仅提高了准确性，还提高了决策的可解释性。

    

    鉴于语言模型训练使用了包含固有偏见的大量数据集，可能会不经意地持续系统性歧视，因此，审查和解决语言模型中的偏见变得至关重要，将公平性整合到它们的发展中，以确保这些模型具有公正和无偏的特性。在这项工作中，我们展示了基于Vicuna-13B-v1.3的零射击刻板印象识别中推理的重要性。尽管我们观察到从13B到33B的规模扩展会提高准确性，但我们表明推理的性能增益远远超过规模扩展的增益。我们的研究结果表明，推理可能是使LLMs在刻板印象等领域任务上超越规模定律的关键因素。此外，通过对选定的推理追踪进行定性分析，我们突出显示了推理不仅提高了准确性，还提高了决策的可解释性。

    Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
    
[^34]: FinPT:使用预训练基础模型进行资金风险预测中个人资料调整

    FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models. (arXiv:2308.00065v1 [q-fin.RM])

    [http://arxiv.org/abs/2308.00065](http://arxiv.org/abs/2308.00065)

    FinPT是一种新颖的资金风险预测方法，通过在大型预训练基础模型上进行个人资料调整，填充金融表格数据并获得自然语言客户资料，从而提高预测准确性。

    

    资金风险预测在金融领域中起着至关重要的作用。机器学习方法已被广泛应用于自动检测潜在风险，从而节省劳动成本。然而，近年来这一领域的发展滞后于以下两个事实：1）所使用的算法有些过时，特别是在生成AI和大型语言模型（LLMs）快速发展的背景下；2）缺乏统一且开源的金融基准已经阻碍了相关研究多年。为解决这些问题，我们提出了FinPT和FinBench：前者是一种新颖的资金风险预测方法，对大型预训练基础模型进行个人资料调整；后者是一套关于资金风险的高质量数据集，如违约、欺诈和流失。在FinPT中，我们将金融表格数据填充到预定义的指令模板中，通过提示LLMs获得自然语言客户资料，并进行精调。

    Financial risk prediction plays a crucial role in the financial sector. Machine learning methods have been widely applied for automatically detecting potential risks and thus saving the cost of labor. However, the development in this field is lagging behind in recent years by the following two facts: 1) the algorithms used are somewhat outdated, especially in the context of the fast advance of generative AI and large language models (LLMs); 2) the lack of a unified and open-sourced financial benchmark has impeded the related research for years. To tackle these issues, we propose FinPT and FinBench: the former is a novel approach for financial risk prediction that conduct Profile Tuning on large pretrained foundation models, and the latter is a set of high-quality datasets on financial risks such as default, fraud, and churn. In FinPT, we fill the financial tabular data into the pre-defined instruction template, obtain natural-language customer profiles by prompting LLMs, and fine-tune 
    
[^35]: Alpha-GPT：人机交互式 Alpha 挖掘在量化投资中的应用

    Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment. (arXiv:2308.00016v1 [q-fin.CP])

    [http://arxiv.org/abs/2308.00016](http://arxiv.org/abs/2308.00016)

    本论文提出了一种通过引入人机交互的新型 alpha 挖掘范式，并利用大型语言模型的能力，通过一种新颖的提示工程算法框架，开发了 Alpha-GPT。通过多个实验，展示了 Alpha-GPT 在量化投资领域的有效性和优势。

    

    在量化投资研究中，挖掘新的 alpha（有效的交易信号或因子）是其中最重要的任务之一。传统的 alpha 挖掘方法，无论是手工合成因子还是算法挖掘因子（如遗传编程搜索），都存在固有的局限性，尤其在实施量化分析师的想法方面。在本研究中，我们提出了一种新的 alpha 挖掘范式，引入了人机交互，并通过利用大型语言模型的能力，提出了一种新颖的提示工程算法框架来实现这个范式。此外，我们开发了 Alpha-GPT，一种新的交互式 alpha 挖掘系统框架，以一种启发式的方式“理解”量化研究人员的想法，并输出具有创造性、深入洞察力和有效性的 alpha。通过多个 alpha 挖掘实验，我们展示了 Alpha-GPT 的有效性和优势。

    One of the most important tasks in quantitative investment research is mining new alphas (effective trading signals or factors). Traditional alpha mining methods, either hand-crafted factor synthesizing or algorithmic factor mining (e.g., search with genetic programming), have inherent limitations, especially in implementing the ideas of quants. In this work, we propose a new alpha mining paradigm by introducing human-AI interaction, and a novel prompt engineering algorithmic framework to implement this paradigm by leveraging the power of large language models. Moreover, we develop Alpha-GPT, a new interactive alpha mining system framework that provides a heuristic way to ``understand'' the ideas of quant researchers and outputs creative, insightful, and effective alphas. We demonstrate the effectiveness and advantage of Alpha-GPT via a number of alpha mining experiments.
    
[^36]: 一种新的技术相互依赖的映射

    A new mapping of technological interdependence. (arXiv:2308.00014v1 [econ.EM])

    [http://arxiv.org/abs/2308.00014](http://arxiv.org/abs/2308.00014)

    本文利用文本挖掘和网络分析的方法，研究了不同部门之间的技术相互依赖关系，并证明了在技术创新中，间接联系和直接联系同等重要。

    

    哪些技术联系影响了部门的创新能力？这些效应如何通过技术空间传递？本文使用新颖的文本挖掘和网络分析方法回答了这两个关键问题。我们通过分析美国专利商标局（USPTO）授予的650万项专利的文本，并应用网络分析方法，研究了半个世纪（从1976年到2021年）期间不同部门之间的技术相互依赖关系，揭示了存在于技术领域之间的全谱的联系。我们证明专利文本包含了往往无法通过传统的创新指标（例如专利引用）捕捉到的丰富信息。通过使用网络分析，我们记录了间接联系和直接联系同等重要，并且前者大部分使用传统的间接联系度量方法（如Leontief逆矩阵）往往会被隐藏。最后，基于冲击响应分析，我们进行了说明。

    Which technological linkages affect the sector's ability to innovate? How do these effects transmit through the technology space? This paper answers these two key questions using novel methods of text mining and network analysis. We examine technological interdependence across sectors over a period of half a century (from 1976 to 2021) by analyzing the text of 6.5 million patents granted by the United States Patent and Trademark Office (USPTO), and applying network analysis to uncover the full spectrum of linkages existing across technology areas. We demonstrate that patent text contains a wealth of information often not captured by traditional innovation metrics, such as patent citations. By using network analysis, we document that indirect linkages are as important as direct connections and that the former would remain mostly hidden using more traditional measures of indirect linkages, such as the Leontief inverse matrix. Finally, based on an impulse-response analysis, we illustrate 
    
[^37]: 时间常识推理与获取概述

    An Overview Of Temporal Commonsense Reasoning and Acquisition. (arXiv:2308.00002v1 [cs.AI])

    [http://arxiv.org/abs/2308.00002](http://arxiv.org/abs/2308.00002)

    本文综述了时间常识推理领域的研究进展，重点关注通过增强语言模型的性能来提高推理能力，并对多个数据集进行评估。然而，这些增强模型仍然难以达到人类水平的推理能力。

    

    时间常识推理是指理解短语、动作和事件的典型时间背景并将其应用于需要这种知识的问题推理的能力。这种能力在时间自然语言处理任务中至关重要，可能应用于时间线摘要、时间问答和时间自然语言推断等方面。最近的研究表明，大型语言模型虽然善于生成语法正确的句子和解决分类任务，但在推理过程中往往会采取捷径，并陷入简单的语言陷阱。本文章概述了在时间常识推理领域的研究，特别关注通过各种增强方式提高语言模型的性能以及对越来越多数据集的评估。然而，这些增强模型在推理任务上仍然难以达到人类的水平。

    Temporal commonsense reasoning refers to the ability to understand the typical temporal context of phrases, actions, and events, and use it to reason over problems requiring such knowledge. This trait is essential in temporal natural language processing tasks, with possible applications such as timeline summarization, temporal question answering, and temporal natural language inference. Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps. This article provides an overview of research in the domain of temporal commonsense reasoning, particularly focusing on enhancing language model performance through a variety of augmentations and their evaluation across a growing number of datasets. However, these augmented models still struggle to approach human performance on reasoning task
    
[^38]: 深入理解国际关系语言：基于自然语言处理的对UNESCO摘要记录的分析

    Deep Dive into the Language of International Relations: NLP-based Analysis of UNESCO's Summary Records. (arXiv:2307.16573v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.16573](http://arxiv.org/abs/2307.16573)

    该研究开发了基于UNESCO摘要记录的自动分析工具，通过创新的主题建模和紧张检测方法，提供对联合国教科文组织世界遗产名录和代表性非物质文化遗产人类代表作名录决策过程的有价值洞察。应用程序可为外交官、律师、政治科学家和国际关系研究人员提供高效搜索所选主题相关文档段落和特定演讲者陈述的便利。

    

    文化遗产是所有国家都感兴趣的国际关系领域。将文化遗产列入联合国教科文组织世界遗产名录和《代表性非物质文化遗产人类代表作名录》的过程经常引发国家间的紧张与冲突。本研究通过开发自动工具，对上述两个名录的决策过程提供有价值的洞察。我们提出了基于UNESCO摘要记录的创新主题建模和紧张检测方法。我们的分析在紧张检测上达到了令人称赞的72%准确率。此外，我们还开发了一个专为外交官、律师、政治科学家和国际关系研究人员量身定制的应用程序，用于高效搜索选定文档段落和特定演讲者关于选定主题的陈述。该应用程序是提升文化遗产管理和决策制定的宝贵资源。

    Cultural heritage is an arena of international relations that interests all states worldwide. The inscription process on the UNESCO World Heritage List and the UNESCO Representative List of the Intangible Cultural Heritage of Humanity often leads to tensions and conflicts among states. This research addresses these challenges by developing automatic tools that provide valuable insights into the decision-making processes regarding inscriptions to the two lists mentioned above. We propose innovative topic modelling and tension detection methods based on UNESCO's summary records. Our analysis achieved a commendable accuracy rate of 72% in identifying tensions. Furthermore, we have developed an application tailored for diplomats, lawyers, political scientists, and international relations researchers that facilitates the efficient search of paragraphs from selected documents and statements from specific speakers about chosen topics. This application is a valuable resource for enhancing the 
    
[^39]: 研究上下文学习的学习行为：与监督学习的比较

    Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning. (arXiv:2307.15411v1 [cs.CL])

    [http://arxiv.org/abs/2307.15411](http://arxiv.org/abs/2307.15411)

    本研究通过对比监督学习和上下文学习，发现大型语言模型在学习行为上受到金标签的显著影响，但对于上下文学习来说，标签不平衡影响较小。实证结果显示上下文学习对标签扰动的敏感性较低。

    

    大型语言模型（LLM）展示了令人注目的上下文学习（ICL）能力，在没有明确预训练的情况下，仅通过少量训练样例就可以学习新任务。然而，尽管LLM取得了成功，对于ICL如何从给定的提示中学习知识的了解还很少。在本文中，为了更好地理解ICL的学习行为，我们使用相同的演示样例通过ICL和监督学习（SL）分别训练相同的LLM，并研究它们在一系列分类任务上在标签扰动（噪声标签和标签不平衡）下的性能。首先，通过广泛的实验，我们发现金标签对于下游的上下文性能有重大影响，尤其是对于大型语言模型；然而，对于ICL来说，标签不平衡对所有模型大小都不太重要。其次，在与SL进行比较时，我们经验性地表明ICL对标签扰动的敏感性较低。

    Large language models (LLMs) have shown remarkable capacity for in-context learning (ICL), where learning a new task from just a few training examples is done without being explicitly pre-trained. However, despite the success of LLMs, there has been little understanding of how ICL learns the knowledge from the given prompts. In this paper, to make progress toward understanding the learning behaviour of ICL, we train the same LLMs with the same demonstration examples via ICL and supervised learning (SL), respectively, and investigate their performance under label perturbations (i.e., noisy labels and label imbalance) on a range of classification tasks. First, via extensive experiments, we find that gold labels have significant impacts on the downstream in-context performance, especially for large language models; however, imbalanced labels matter little to ICL across all model sizes. Second, when comparing with SL, we show empirically that ICL is less sensitive to label perturbations th
    
[^40]: Gzip与KNN在文本分类中的对比研究

    Gzip versus bag-of-words for text classification with KNN. (arXiv:2307.15002v1 [cs.CL])

    [http://arxiv.org/abs/2307.15002](http://arxiv.org/abs/2307.15002)

    Gzip与KNN相比较在文本分类中，我们发现通过简单的词袋匹配可以获得类似或更好的准确性，并且更加高效。

    

    最近，基于KNN的文本分类中压缩距离（gzip）的有效性引起了很多关注。在本文中，我们展示了通过更简单的方法可以达到类似或更好的效果，并且可能不需要文本压缩。实际上，我们发现简单的“词袋”匹配可以达到类似或更好的准确性，并且更高效。

    The effectiveness of compression distance in KNN-based text classification ('gzip') has recently garnered lots of attention. In this note, we show that similar or better effectiveness can be achieved with simpler means, and text compression may not be necessary. Indeed, we find that a simple 'bag-of-words' matching can achieve similar or better accuracy, and is more efficient.
    
[^41]: CliniDigest: 基于大规模临床试验描述的大型语言模型的案例研究

    CliniDigest: A Case Study in Large Language Model Based Large-Scale Summarization of Clinical Trial Descriptions. (arXiv:2307.14522v1 [cs.CL])

    [http://arxiv.org/abs/2307.14522](http://arxiv.org/abs/2307.14522)

    CliniDigest是一个基于大规模语言模型的临床试验摘要工具，能够实时、真实和全面地将长篇试验描述压缩成简洁的摘要。

    

    临床试验是评估新的生物医学干预措施的研究。为了设计新的试验，研究人员从当前和已完成的试验中汲取灵感。2022年，每天平均有100多个临床试验提交到ClinicalTrials.gov，每个试验平均有约1500个单词。这几乎不可能及时跟上。为了缓解这个问题，我们使用GPT-3.5创建了一个批量临床试验摘要工具CliniDigest。据我们所知，CliniDigest是第一个能够提供实时、真实和全面的临床试验摘要的工具。CliniDigest可以将多达85个临床试验描述（约10500个单词）缩减为一个简洁的200个词的摘要，带有参考文献和有限的虚构内容。我们已经测试了CliniDigest在27个医学子领域中涉及的457个试验的摘要能力。对于每个领域，CliniDigest生成的摘要平均为153个单词，标准差为69个单词，其中每个摘要平均使用54个单词。

    A clinical trial is a study that evaluates new biomedical interventions. To design new trials, researchers draw inspiration from those current and completed. In 2022, there were on average more than 100 clinical trials submitted to ClinicalTrials.gov every day, with each trial having a mean of approximately 1500 words [1]. This makes it nearly impossible to keep up to date. To mitigate this issue, we have created a batch clinical trial summarizer called CliniDigest using GPT-3.5. CliniDigest is, to our knowledge, the first tool able to provide real-time, truthful, and comprehensive summaries of clinical trials. CliniDigest can reduce up to 85 clinical trial descriptions (approximately 10,500 words) into a concise 200-word summary with references and limited hallucinations. We have tested CliniDigest on its ability to summarize 457 trials divided across 27 medical subdomains. For each field, CliniDigest generates summaries of $\mu=153,\ \sigma=69 $ words, each of which utilizes $\mu=54\
    
[^42]: EmotionPrompt: 通过情感刺激提升大型语言模型的关键心理学方法

    EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])

    [http://arxiv.org/abs/2307.11760](http://arxiv.org/abs/2307.11760)

    EmotionPrompt是一个基于心理学的方法，通过将情感刺激融入到提示中，提升了大型语言模型在各项任务上的性能，并且同时改善了其真实性和信息量。

    

    大型语言模型（LLMs）在推理、语言理解和数学问题解决等许多领域取得了显著的性能，并被视为人工通用智能（AGI）的关键步骤。然而，LLMs对提示的敏感性仍然是其日常应用的主要瓶颈。本文从心理学中汲取灵感，提出了EmotionPrompt来探索情感智能以提升LLMs的性能。EmotionPrompt基于一个非常简单明了的原则：将情感刺激融入到提示中。实验结果表明，我们的方法在相同的单一提示模板上，与原始的零样本提示和Zero-shot-CoT相比，在8个任务上都显著优于多种模型：ChatGPT、Vicuna-13b、Bloom和T5。此外，观察到EmotionPrompt能够提高真实性和信息量。我们相信EmotionPrompt为探索跨学科知识开辟了一条新的道路。

    Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
    
[^43]: Jina Embeddings:一种新颖的高性能句子嵌入模型

    Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v1 [cs.CL])

    [http://arxiv.org/abs/2307.11224](http://arxiv.org/abs/2307.11224)

    Jina Embeddings是一组高性能的句子嵌入模型，能够捕捉文本的语义本质。该论文详细介绍了Jina Embeddings的开发过程，并通过性能评估验证了其优越性能。

    

    Jina Embeddings由一组高性能的句子嵌入模型组成，能够将各种文本输入转化为数值表示，从而捕捉文本的语义本质。虽然这些模型并非专门设计用于文本生成，但在密集检索和语义文本相似性等应用中表现出色。本文详细介绍了Jina Embeddings的开发过程，从创建高质量的成对和三元数据集开始。它强调了数据清理在数据集准备中的关键作用，并对模型训练过程进行了深入探讨，最后利用Massive Textual Embedding Benchmark（MTEB）进行了全面的性能评估。

    Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB).
    
[^44]: ChatGPT的行为随时间变化如何？

    How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])

    [http://arxiv.org/abs/2307.09009](http://arxiv.org/abs/2307.09009)

    本论文评估了GPT-3.5和GPT-4模型在不同时间点上的性能和行为变化，发现它们的表现可以有很大的差异，包括在解决数学问题、回答敏感问题、生成代码和视觉推理等任务上。这些结果表明相同的语言模型服务的行为在相对短的时间内可以发生显著变化。

    

    GPT-3.5和GPT-4是两种广泛使用的大型语言模型（LLM）服务。然而，这些模型何时以及如何进行更新是不透明的。在这里，我们对GPT-3.5和GPT-4的2023年3月和2023年6月版本进行了评估，涉及四项不同的任务：1）解决数学问题，2）回答敏感/危险问题，3）生成代码和4）视觉推理。我们发现，GPT-3.5和GPT-4的性能和行为在时间上可以有很大的变化。例如，GPT-4（2023年3月）在识别质数方面表现非常出色（准确率为97.6%），但GPT-4（2023年6月）在相同的问题上表现非常差（准确率为2.4%）。有趣的是，GPT-3.5（2023年6月）在这个任务上比GPT-3.5（2023年3月）要好得多。GPT-4在6月份对回答敏感问题的意愿较3月份要低，而无论是GPT-4还是GPT-3.5在6月份的代码生成中都有更多的格式错误。总体而言，我们的发现表明相同LLM服务的行为在相对较短的时间内可以发生重大变化。

    GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relat
    
[^45]: SAS视频问答：自适应采样用于高效视频问答

    SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v1 [cs.CV])

    [http://arxiv.org/abs/2307.04192](http://arxiv.org/abs/2307.04192)

    SAS视频问答通过自适应采样策略解决了视频问答中的问题，提高了效率和准确性

    

    视频问答是视频理解领域的一项基础任务。尽管当前的视觉-语言模型(VLMs)配备了视频变换器(Video Transformers)，实现了时间建模并取得了优秀的结果，但代价是巨大的计算能力，因此在实时应用场景中过于昂贵。一种经济的解决方法是只对视频的一小部分帧进行采样，来代表视频的主要内容，并在这些采样帧上调整图像-文本模型。最近的视频理解模型通常随机采样一组帧或片段，而不考虑它们的内部关联性和与问题的相关性。我们认为这种无目标的采样可能会遗漏可以推导出正确答案的关键帧，在采样稀疏程度增加时，情况会变得更糟，而随着视频长度的增加，采样稀疏程度也会增加。为了解决这个问题，我们提出了两种帧采样策略，分别是

    Video question--answering is a fundamental task in the field of video understanding. Although current vision--language models (VLMs) equipped with Video Transformers have enabled temporal modeling and yielded superior results, they are at the cost of huge computational power and thus too expensive to deploy in real-time application scenarios. An economical workaround only samples a small portion of frames to represent the main content of that video and tune an image--text model on these sampled frames. Recent video understanding models usually randomly sample a set of frames or clips, regardless of internal correlations between their visual contents, nor their relevance to the problem. We argue that such kinds of aimless sampling may omit the key frames from which the correct answer can be deduced, and the situation gets worse when the sampling sparsity increases, which always happens as the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namel
    
[^46]: 推理还是背诵？通过反事实任务探索语言模型的能力和限制

    Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. (arXiv:2307.02477v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.02477](http://arxiv.org/abs/2307.02477)

    通过反事实任务的研究，我们发现当前的语言模型具备一定的抽象推理能力，但它们在任务求解过程中往往也依赖于狭窄、难以转移的过程，这对语言模型的性能解释和理解有着重要的启示。

    

    最近语言模型在各种任务上的出色表现表明它们具备一定程度的抽象推理能力。这些能力是通用且可转移的，还是专门针对预训练过程中遇到的特定任务？为了分开这些效果，我们提出了一个评估框架，基于“反事实”任务变种，这些变种与支撑标准任务的默认假设有所偏离。在一套包含11个任务的实验中，我们观察到反事实变种的非平凡性能，但与默认条件相比，性能显著而持续地下降。这表明当前的语言模型可能在一定程度上具备抽象任务求解能力，但它们通常也依赖于狭窄、难以转移的任务求解过程。这些结果促使我们对语言模型性能进行更加谨慎的解释，以区分这些行为方面。

    The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on "counterfactual" task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to a degree, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.
    
[^47]: 编程教育的生成AI：比较ChatGPT、GPT-4和人类导师的表现

    Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors. (arXiv:2306.17156v1 [cs.CY])

    [http://arxiv.org/abs/2306.17156](http://arxiv.org/abs/2306.17156)

    该论文系统评估了ChatGPT、GPT-4和人类导师在不同的编程教育场景中的表现，并发现GPT-4优于ChatGPT，接近于人类导师。

    

    生成AI和大型语言模型在提高计算机教育方面具有很大的潜力，通过为初级编程提供下一代教育技术。最近的研究已经研究了这些模型在与编程教育相关的不同场景中的应用；然而，这些研究由于多种原因而受限，因为它们通常考虑的是已经过时的模型或仅仅特定的场景。因此，缺乏一个系统的研究来对最先进的模型进行全面的编程教育场景基准测试。在我们的工作中，我们系统地评估了两个模型，ChatGPT（基于GPT-3.5）和GPT-4，并将其在各种场景下与人类导师的表现进行了比较。我们使用五个初级Python编程问题和来自在线平台的真实错误程序进行评估，并使用专家评注来评估性能。我们的结果表明，GPT-4明显优于ChatGPT（基于GPT-3.5），并且接近于人类导师。

    Generative AI and large language models hold great promise in enhancing computing education by powering next-generation educational technologies for introductory programming. Recent works have studied these models for different scenarios relevant to programming education; however, these works are limited for several reasons, as they typically consider already outdated models or only specific scenario(s). Consequently, there is a lack of a systematic study that benchmarks state-of-the-art models for a comprehensive set of programming education scenarios. In our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human tutors for a variety of scenarios. We evaluate using five introductory Python programming problems and real-world buggy programs from an online platform, and assess performance using expert-based annotations. Our results show that GPT-4 drastically outperforms ChatGPT (based on GPT-3.5) and comes close to hu
    
[^48]: CrunchGPT：基于ChatGPT的科学机器学习辅助框架

    CrunchGPT: A chatGPT assisted framework for scientific machine learning. (arXiv:2306.15551v1 [cs.LG])

    [http://arxiv.org/abs/2306.15551](http://arxiv.org/abs/2306.15551)

    CrunchGPT是一个基于ChatGPT的科学机器学习辅助框架，通过简单的用户提示来协调整个科学机器学习的工作流程，实现无缝集成数据和物理知识，解决了SciML在预处理、问题建模、代码生成、后处理和分析等方面的耗时问题，拓展了其工业应用和数字孪生框架的适用性。

    

    科学机器学习（SciML）近年来在计算科学和工程的许多不同领域取得了进展。其目标是在不需要复杂和计算密集的数据同化方案的情况下，无缝地将数据和物理知识集成起来。然而，预处理、问题建模、代码生成、后处理和分析仍然是耗时的，并且可能限制SciML在工业应用和数字孪生框架中的广泛适用性。在这里，我们将SciML的各个阶段整合到ChatGPT的伞下，形成CrunchGPT，它通过用户简单的提示来协调整个SciML的工作流程。具体而言，我们提供了两个示例，演示了CrunchGPT在气动学中优化机翼和在各种几何形状中获得流场的潜在用途，并强调了验证阶段。为了演示CrunchGPT的流程和

    Scientific Machine Learning (SciML) has advanced recently across many different areas in computational science and engineering. The objective is to integrate data and physics seamlessly without the need of employing elaborate and computationally taxing data assimilation schemes. However, preprocessing, problem formulation, code generation, postprocessing and analysis are still time consuming and may prevent SciML from wide applicability in industrial applications and in digital twin frameworks. Here, we integrate the various stages of SciML under the umbrella of ChatGPT, to formulate CrunchGPT, which plays the role of a conductor orchestrating the entire workflow of SciML based on simple prompts by the user. Specifically, we present two examples that demonstrate the potential use of CrunchGPT in optimizing airfoils in aerodynamics, and in obtaining flow fields in various geometries in interactive mode, with emphasis on the validation stage. To demonstrate the flow of the CrunchGPT, and
    
[^49]: W-procer: 基于加权原型对比学习的医学少样本命名实体识别

    W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v1 [cs.CL])

    [http://arxiv.org/abs/2305.18624](http://arxiv.org/abs/2305.18624)

    W-procer是一种基于加权原型对比学习的医学少样本命名实体识别方法，在构建基于原型的对比损失和加权网络方面具有创新性，优于现有的最先进方法。

    

    对比学习已成为少样本命名实体识别（NER）的一种受欢迎的解决方案。传统配置力求减少具有相同标签的标记之间的距离，并增加具有不同标签的标记之间的距离。然而，在医学领域中存在大量被注释为“O”（即“OUTSIDE”）的实体，并且它们不希望被推离到当前对比学习方法标记为“O”以外的其他实体，这种设定效果不佳，可能会得出含有噪声原型标签的语义表示，尽管存在许多“O”标签实体与有标签实体相关。为解决这个挑战，我们提出了一种名为医学少样本命名实体识别中基于加权原型的对比学习方法（W-PROCER）。我们的方法主要围绕构建基于原型的对比损失和加权网络展开。这些组件在协助在医学领域中的迁移学习方面发挥了至关重要的作用。在实验中，我们将W-PROCER应用于一个公共的医学数据集，并展示了其相对于现有的最先进方法的优异表现。

    Contrastive learning has become a popular solution for few-shot Name Entity Recognization (NER). The conventional configuration strives to reduce the distance between tokens with the same labels and increase the distance between tokens with different labels. The effect of this setup may, however, in the medical domain, there are a lot of entities annotated as OUTSIDE (O), and they are undesirably pushed apart to other entities that are not labeled as OUTSIDE (O) by the current contrastive learning method end up with a noisy prototype for the semantic representation of the label, though there are many OUTSIDE (O) labeled entities are relevant to the labeled entities. To address this challenge, we propose a novel method named Weighted Prototypical Contrastive Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our approach primarily revolves around constructing the prototype-based contractive loss and weighting network. These components play a crucial role in assisting t
    
[^50]: 微小子空间中发生微调: 探索预训练语言模型的内在任务特定子空间

    Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models. (arXiv:2305.17446v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17446](http://arxiv.org/abs/2305.17446)

    该论文通过发现预训练语言模型的内在任务特定子空间，提出了一种重新参数化和微调模型的新方法。研究发现在该子空间中，只需少量自由参数即可有效微调模型，并且某些维度对于引入任务特定知识至关重要。

    

    已知预训练语言模型（PLMs）过度参数化并具有显著的冗余，表明PLMs的自由度较小。本文从新的角度研究了重新参数化和微调PLMs的问题：发现内在的任务特定子空间。具体地，通过利用给定任务的微调过程的动态，学习了参数优化轨迹以揭示其内在的任务特定子空间。一个关键发现是，在子空间中，PLMs可以通过少量的自由参数进行有效的微调。此外，我们观察到在子空间的微调过程中出现了一些异常维度。禁用这些维度会严重降低模型性能。这表明这些维度对于引入任务特定知识到下游任务是至关重要的。

    Pre-trained language models (PLMs) are known to be overly parameterized and have significant redundancy, indicating a small degree of freedom of the PLMs. Motivated by the observation, in this paper, we study the problem of re-parameterizing and fine-tuning PLMs from a new perspective: Discovery of intrinsic task-specific subspace. Specifically, by exploiting the dynamics of the fine-tuning process for a given task, the parameter optimization trajectory is learned to uncover its intrinsic task-specific subspace. A key finding is that PLMs can be effectively fine-tuned in the subspace with a small number of free parameters. Beyond, we observe some outlier dimensions emerging during fine-tuning in the subspace. Disabling these dimensions degrades the model performance significantly. This suggests that these dimensions are crucial to induce task-specific knowledge to downstream tasks.
    
[^51]: Diable: 在表格上进行的高效对话状态跟踪

    Diable: Efficient Dialogue State Tracking as Operations on Tables. (arXiv:2305.17020v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17020](http://arxiv.org/abs/2305.17020)

    Diable是一个高效的对话状态跟踪系统，它通过在表格上进行操作来更新对话状态，相比现有方法时间效率提高了2.4倍，同时保持了竞争性的目标准确性。

    

    目前的对话状态跟踪系统将完整的对话历史作为输入，将当前状态表示为包含所有槽的列表，并在每个对话回合中从头开始生成整个状态。这种方法效率低下，特别是当槽的数量很多且对话很长时。我们提出了Diable，一种新的任务形式化方法，简化了高效对话状态跟踪系统的设计和实现，并且可以轻松地嵌入大型语言模型。我们将对话状态表示为表格，并将对话状态跟踪形式化为表格操作任务。在每个回合中，系统通过基于对话上下文生成表格操作来更新先前的状态。在MultiWoz数据集上进行了大量实验，结果显示，Diable (i) 优于强大的高效对话状态跟踪基准，(ii) 时间效率比当前最先进的方法提高了2.4倍，同时保持竞争性的联合目标准确性，并且(iii) 对无噪声的输入具有鲁棒性。

    Sequence-to-sequence state-of-the-art systems for dialogue state tracking (DST) use the full dialogue history as input, represent the current state as a list with all the slots, and generate the entire state from scratch at each dialogue turn. This approach is inefficient, especially when the number of slots is large and the conversation is long. We propose Diable, a new task formalisation that simplifies the design and implementation of efficient DST systems and allows one to easily plug and play large language models. We represent the dialogue state as a table and formalise DST as a table manipulation task. At each turn, the system updates the previous state by generating table operations based on the dialogue context. Extensive experimentation on the MultiWoz datasets demonstrates that Diable (i) outperforms strong efficient DST baselines, (ii) is 2.4x more time efficient than current state-of-the-art methods while retaining competitive Joint Goal Accuracy, and (iii) is robust to no
    
[^52]: AlpacaFarm: 一种从人类反馈中学习的方法模拟框架

    AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])

    [http://arxiv.org/abs/2305.14387](http://arxiv.org/abs/2305.14387)

    该论文提出了一种名为AlpacaFarm的低成本模拟器，该模拟器为从人类反馈中学习的研究和开发提供了一种解决方案，通过设计LLM提示来模拟人类反馈，提出自动评估并提供参考实现，克服了数据收集的高昂成本、缺乏可信的评估和缺乏参考方法实现的挑战。

    

    大型语言模型（LLMs）如ChatGPT因其良好的指令跟随能力而得到了广泛应用。开发这些LLMs需要使用人类反馈进行训练的复杂且尚不明确的工作流程。将此指令跟随过程复制和理解面临三大挑战： 数据收集的高昂成本，缺乏可信的评估和缺乏参考方法实现。我们通过AlpacaFarm解决了这些挑战，这是一个低成本的模拟器，可用于从反馈中学习的研究和开发。第一，我们设计了LLM提示来模拟人类反馈，其成本比众包工作者便宜45倍，并且与人类反馈具有高度一致性。第二，我们提出了一种自动评估方法，并将其与真实世界交互中获得的人类指令进行验证。第三，我们为几种从配对反馈中学习的方法（PPO，best-of-n，expert iteration等）提供了参考实现。

    Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback
    
[^53]: 针对增量学习的端到端语音理解序列级知识蒸馏

    Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding. (arXiv:2305.13899v1 [eess.AS])

    [http://arxiv.org/abs/2305.13899](http://arxiv.org/abs/2305.13899)

    本文针对连续学习场景下的口语语言理解问题，提出了增量类别场景和三种知识蒸馏方法，并显示序列级知识蒸馏可以显著改善绩效。

    

    现代神经网络在逐步学习新概念方面的能力是一个重要的弱点，这妨碍了它们在非平稳环境中的使用。它们倾向于将当前数据分布拟合得越来越好，而忽略了过去所获取的知识，导致了灾难性的遗忘问题。本文解决了应用于连续学习情境的口语语言理解问题。我们首先为SLURP数据集定义了一个增量类别场景，并针对序列到序列的Transformer模型提出了三种知识蒸馏（KD）方法以减轻遗忘：第一种KD方法应用于编码器输出（audio-KD），其余两种方法则分别在解码器输出的标记级（tok-KD）或序列级（seq-KD）分布上进行。我们展示了seq-KD显著地改善了所有绩效指标，将它与audio-KD相结合进一步降低了平均词错误率（WER）并提高了实体预测指标。

    The ability to learn new concepts sequentially is a major weakness for modern neural networks, which hinders their use in non-stationary environments. Their propensity to fit the current data distribution to the detriment of the past acquired knowledge leads to the catastrophic forgetting issue. In this work we tackle the problem of Spoken Language Understanding applied to a continual learning setting. We first define a class-incremental scenario for the SLURP dataset. Then, we propose three knowledge distillation (KD) approaches to mitigate forgetting for a sequence-to-sequence transformer model: the first KD method is applied to the encoder output (audio-KD), and the other two work on the decoder output, either directly on the token-level (tok-KD) or on the sequence-level (seq-KD) distributions. We show that the seq-KD substantially improves all the performance metrics, and its combination with the audio-KD further decreases the average WER and enhances the entity prediction metric.
    
[^54]: 连续多模态知识图谱构建

    Continual Multimodal Knowledge Graph Construction. (arXiv:2305.08698v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08698](http://arxiv.org/abs/2305.08698)

    连续多模态知识图谱构建面临着灾难性遗忘的挑战，需要解决新增实体和关系以及多模态源数据变化的问题。

    

    多模态知识图谱构建（MKGC）涉及使用多种形式的数据，如文本和图像，创建实体和关系的结构化表示。然而，现有的MKGC模型在处理动态现实场景中新增实体和关系方面面临挑战。目前的连续知识图谱构建设置主要关注从文本数据中提取实体和关系，忽视了其他多模态源。因此，有必要探索连续MKGC的挑战，以解决灾难性遗忘现象，并确保保留从不同形式的数据中提取的过去知识。本研究致力于通过开发终身MKGC基准数据集来研究这个复杂的主题。基于经验发现，当多媒体数据训练时，一些典型的MKGC模型可能在连续设置中意外表现不佳，与那些仅利用文本资源的模型相比。我们以实验证据为基础，总结出以下论点：连续多模态知识图谱构建面临着数据源变化导致的灾难性遗忘问题。

    Multimodal Knowledge Graph Construction (MKGC) involves creating structured representations of entities and relations using multiple modalities, such as text and images. However, existing MKGC models face challenges in handling the addition of new entities and relations in dynamic real-world scenarios. The current continual setting for knowledge graph construction mainly focuses on entity and relation extraction from text data, overlooking other multimodal sources. Therefore, there arises the need to explore the challenge of continual MKGC to address the phenomenon of catastrophic forgetting and ensure the retention of past knowledge extracted from different forms of data. This research focuses on investigating this complex topic by developing lifelong MKGC benchmark datasets. Based on the empirical findings that several typical MKGC models, when trained on multimedia data, might unexpectedly underperform compared to those solely utilizing textual resources in a continual setting, we p
    
[^55]: 摘要生成的当前状态

    The Current State of Summarization. (arXiv:2305.04853v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.04853](http://arxiv.org/abs/2305.04853)

    摘要生成领域目前的研究关注点在于预训练的编码器-解码器模型和大规模自回归语言模型的转变，以及评估摘要生成系统的挑战和指令调整模型在零样本摘要生成中的潜力。

    

    随着文本信息的爆炸性增长，摘要生成系统变得越来越重要。本文旨在简明扼要地介绍抽象文本摘要生成的当前最先进技术。作为其中的一部分，我们概述了向预训练的编码器-解码器模型和大规模自回归语言模型转变的现有范例。此外，我们深入探讨了评估摘要生成系统的挑战以及基于指令调整的模型在零样本摘要生成中的潜力。最后，我们简要概述了目前将摘要生成系统整合到商业应用中的情况。

    With the explosive growth of textual information, summarization systems have become increasingly important. This work aims to concisely indicate the current state of the art in abstractive text summarization. As part of this, we outline the current paradigm shifts towards pre-trained encoder-decoder models and large autoregressive language models. Additionally, we delve further into the challenges of evaluating summarization systems and the potential of instruction-tuned models for zero-shot summarization. Finally, we provide a brief overview of how summarization systems are currently being integrated into commercial applications.
    
[^56]: 大型语言模型对齐的基本限制

    Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v1 [cs.CL])

    [http://arxiv.org/abs/2304.11082](http://arxiv.org/abs/2304.11082)

    本文通过提出一种理论方法——行为期望边界（BEB），展示了大型语言模型中对齐的基本限制，并证明任何对齐过程都无法根除不希望的行为，这对于防止恶意攻击是不安全的。

    

    开发与人交互的语言模型的重要方面是对齐其行为，使其对其人类用户有用且无害。这通常通过调整模型的方式来实现，以增强所需的行为并抑制不希望的行为。在本文中，我们提出了一种名为行为期望边界(BEB)的理论方法，它允许我们正式研究大型语言模型中的几个内在特征和对齐的限制。重要的是，我们证明对于任何具有被该模型表现出的有限概率的行为，都存在可以触发模型输出此行为的提示，其概率随提示的长度增加而增加。这意味着任何减弱不希望的行为但未将其完全消除的对齐过程都无法抵御针对性攻击。此外，我们的框架提示了领先的

    An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading al
    
[^57]: SemEval-2023任务3上的mCPT：用于零样本和少样本框架检测的多语言标签感知对比预训练变压器

    mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v1 [cs.CL])

    [http://arxiv.org/abs/2303.09901](http://arxiv.org/abs/2303.09901)

    本研究提出了mCPT模型用于多语言的、多标签的零样本或少样本的框架检测任务，并在西班牙语和其他8种语言中取得了良好的成绩。该方案采用了基于多语言变压器的预训练程序，使用标签感知对比损失函数。

    

    本文介绍了零样本的西班牙语框架检测任务的获胜系统，并在另外八种语言中取得了良好的成绩。框架检测任务的挑战在于在只有少量或零个样本的情况下识别一组14个框架，即多语言多标签的少样本和零样本设置。我们开发的解决方案采用了基于多语言变压器的预训练程序，使用标签感知对比损失函数。除了描述系统外，我们还进行了嵌入空间分析和消融研究，以展示我们的预训练程序如何支持框架检测以推进计算框架分析。

    This paper presents the winning system for the zero-shot Spanish framing detection task, which also achieves competitive places in eight additional languages. The challenge of the framing detection task lies in identifying a set of 14 frames when only a few or zero samples are available, i.e., a multilingual multi-label few- or zero-shot setting. Our developed solution employs a pre-training procedure based on multilingual Transformers using a label-aware contrastive loss function. In addition to describing the system, we perform an embedding space analysis and ablation study to demonstrate how our pre-training procedure supports framing detection to advance computational framing analysis.
    
[^58]: 在社交媒体上使用数字痕迹进行抑郁症检测：一种知识感知的深度学习方法

    Depression Detection Using Digital Traces on Social Media: A Knowledge-aware Deep Learning Approach. (arXiv:2303.05389v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.05389](http://arxiv.org/abs/2303.05389)

    本研究通过使用数字痕迹在社交媒体上检测抑郁症，提出了一种深度知识感知的抑郁症检测框架，并揭示了关键因素。通过真实数据进行的实证研究证明了其准确性和有效性。

    

    抑郁症是一种全球常见的疾病。它很难诊断，并且持续被低估。由于抑郁症患者在社交媒体上不断分享他们的症状、重大生活事件和治疗方法，研究人员开始利用社交媒体上用户生成的数字痕迹进行抑郁症检测。这种方法在抗击抑郁症方面具有独特的优势，因为它们可以促进创新的方法来对抗抑郁症并减轻其社会和经济负担。然而，大多数现有研究缺乏有效的手段将已建立的医学领域知识纳入抑郁症检测中，或者面临特征提取困难而影响性能。在设计科学研究范式的指导下，我们提出了一种深度知识感知的抑郁症检测 (DKDD) 框架，以准确检测社交媒体用户的抑郁风险，并解释对这种检测起关键作用的因素。通过真实数据进行了大量的实证研究。

    Depression is a common disease worldwide. It is difficult to diagnose and continues to be underdiagnosed. Because depressed patients constantly share their symptoms, major life events, and treatments on social media, researchers are turning to user-generated digital traces on social media for depression detection. Such methods have distinct advantages in combating depression because they can facilitate innovative approaches to fight depression and alleviate its social and economic burden. However, most existing studies lack effective means to incorporate established medical domain knowledge in depression detection or suffer from feature extraction difficulties that impede greater performance. Following the design science research paradigm, we propose a Deep Knowledge-aware Depression Detection (DKDD) framework to accurately detect social media users at risk of depression and explain the critical factors that contribute to such detection. Extensive empirical studies with real-world data
    
[^59]: HL数据集: 基于视觉的场景、动作和理由的描述

    HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales. (arXiv:2302.12189v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.12189](http://arxiv.org/abs/2302.12189)

    这个论文介绍了HL数据集，该数据集扩展了COCO数据集，包含14997个图像和134,973个人工注释的高级别描述，涉及场景、动作和理由，可以用于对视觉和语言模型进行更高级别的测试和微调。

    

    当前的描述数据集侧重于以物体为中心的描述，描述图像中可见的物体，例如“人们在公园里吃东西”。虽然这些数据集对于评估视觉和语言模型识别和描述视觉内容的能力很有用，但它们不支持涉及模型测试或微调的受控实验，使用更高级的描述，人们发现很容易和自然地产生。例如，人们通常根据图像所描绘的场景类型（“人们在度假胜地”）和他们进行的动作（“人们正在野餐”）来描述图像。这些描述基于个人经验和常识性的假设。我们提供了高级别数据集，该数据集扩展了来自COCO数据集的14997个图像，并与一组新的134,973个人工注释（高级别）描述对齐，这些描述从场景、动作和理由三个方面进行了收集。我们进一步使用从独立阅读者组收集的信心得分扩展了该数据集。

    Current captioning datasets focus on object-centric captions, describing the visible objects in the image, e.g. "people eating food in a park". Although these datasets are useful to evaluate the ability of Vision & Language models to recognize and describe visual content, they do not support controlled experiments involving model testing or fine-tuning, with more high-level captions, which humans find easy and natural to produce. For example, people often describe images based on the type of scene they depict ('people at a holiday resort') and the actions they perform ('people having a picnic'). Such descriptions draw on personal experience and commonsense assumptions. We present the High-Level Dataset a dataset extending 14997 images from the COCO dataset, aligned with a new set of 134,973 human-annotated (high-level) captions collected along three axes: scenes, actions, and rationales. We further extend this dataset with confidence scores collected from an independent set of readers,
    
[^60]: 上下文检索增强的语言模型

    In-Context Retrieval-Augmented Language Models. (arXiv:2302.00083v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.00083](http://arxiv.org/abs/2302.00083)

    本研究提出了一种上下文检索增强的语言模型（In-Context RALM）方法，通过将相关文件作为输入的一部分，无需对语言模型进行进一步的训练即可显著提高语言建模性能和源归因能力，并且相对于现有的RALM方法，它具有更简单的部署过程。

    

    检索增强的语言模型(RALM)方法在生成过程中，通过将相关文件从语料库中检索出来与语言模型(LM)进行协同，已被证明可以显著提高语言建模性能。此外，它们还可以缓解事实不准确的文本生成问题，并提供自然的源归因机制。现有的RALM方法着重于修改LM架构以便于整合外部信息，从而大大增加了部署的复杂性。本文提出了一种简单的替代方法，称为上下文RALM：保持LM架构不变，并在输入中添加检索到的文件，无需对LM进行任何进一步的训练。我们展示了基于现成的通用检索器的上下文RALM在模型大小和不同语料库中能够提供出人意料的大幅度的LM增益。我们还证明，文件检索和排名机制可以针对RALM设置进行专门优化。

    Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to 
    
[^61]: 大型语言模型的并行上下文窗口

    Parallel Context Windows for Large Language Models. (arXiv:2212.10947v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10947](http://arxiv.org/abs/2212.10947)

    PCW方法可以缓解现成LLM的上下文窗口限制，将长上下文划分为块并在每个窗口内重用位置嵌入，提高了处理长文本的性能表现。

    

    处理长文本时，大型语言模型（LLM）受到上下文窗口的限制。现有的解决这个问题的方法包括训练专门的架构，但不能轻松地应用于现成的LLM。我们提出了并行上下文窗口（PCW）方法，可以缓解任何现成LLM的上下文窗口限制，而无需进行进一步训练。这种方法的关键在于将长上下文划分为块（“窗口”），限制注意机制仅在每个窗口内应用，并跨窗口重用位置嵌入。我们的主要结果是在750万到1780亿个参数范围内的模型上测试PCW方法，并展示其在输入和输出空间不同的任务中带来了显著的改进。我们在其他需要长上下文窗口的情况下，如多跳问题和使用多个检索的检索增强型问答中展示了额外的好处。

    When applied for processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off-the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (``windows''), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved 
    
[^62]: 高维潜空间中可靠的展开度量方法

    Reliable Measures of Spread in High Dimensional Latent Spaces. (arXiv:2212.08172v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.08172](http://arxiv.org/abs/2212.08172)

    本文定义了数据展开度的概念，并发现常用的度量方法不可靠，提出了八种备选的数据展开度量方法，并推荐使用一种基于主成分和一种基于熵的度量方法，这些方法可以可靠地比较不同模型的展开度。

    

    理解自然语言处理模型潜空间的几何特性可以通过操作这些特性来改善下游任务的性能。其中一个特性是模型潜空间中的数据展开度，即可用潜空间的充分利用程度。在这项工作中，我们定义了数据展开度，并证明了常用的数据展开度度量方法，平均余弦相似度和分区函数的最小/最大比例I（V），不能提供可靠的对比不同模型潜空间使用情况的指标。我们提出并研究了八种备选的数据展开度度量方法，其中除一种外，所有方法在应用于七种合成数据分布时都优于当前的度量方法。在我们提出的度量方法中，推荐使用一种基于主成分的度量方法和一种基于熵的度量方法，它们可以提供可靠的、相对的展开度量，并可用于比较不同大小和维度的模型。

    Understanding geometric properties of natural language processing models' latent spaces allows the manipulation of these properties for improved performance on downstream tasks. One such property is the amount of data spread in a model's latent space, or how fully the available latent space is being used. In this work, we define data spread and demonstrate that the commonly used measures of data spread, Average Cosine Similarity and a partition function min/max ratio I(V), do not provide reliable metrics to compare the use of latent space across models. We propose and examine eight alternative measures of data spread, all but one of which improve over these current metrics when applied to seven synthetic data distributions. Of our proposed measures, we recommend one principal component-based measure and one entropy-based measure that provide reliable, relative measures of spread and can be used to compare models of different sizes and dimensionalities.
    
[^63]: 基于多模态概率融合提示的少样本多模态情感分析

    Few-shot Multimodal Sentiment Analysis based on Multimodal Probabilistic Fusion Prompts. (arXiv:2211.06607v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.06607](http://arxiv.org/abs/2211.06607)

    本论文提出了一种名为多模态概率融合提示（MultiPoint）的方法，通过利用多模态的不同线索进行少样本情感分析。这种方法解决了现有研究中依赖于大规模监督数据的问题，并通过统一的多模态提示来减少不同模态之间的差异。

    

    随着社交媒体上多模态内容的普及，多模态情感分析引起了人们的重视。然而，现有研究在这一领域依赖于大规模监督数据，而这种数据的采集非常耗时和劳动密集。因此，有必要解决少样本多模态情感分析的挑战。为了解决这个问题，我们提出了一种新的方法，名为多模态概率融合提示（MultiPoint），它利用来自不同模态的多样线索进行少样本情感检测。具体而言，我们首先引入了一种一致分布采样方法（CDS），以确保少样本数据集具有与整个数据集相同的类别分布。与之前主要使用基于文本模态的提示的方法不同，我们设计了统一的多模态提示，以减少不同模态之间的差异并动态地将多模态演示纳入其中。

    Multimodal sentiment analysis has gained significant attention due to the proliferation of multimodal content on social media. However, existing studies in this area rely heavily on large-scale supervised data, which is time-consuming and labor-intensive to collect. Thus, there is a need to address the challenge of few-shot multimodal sentiment analysis. To tackle this problem, we propose a novel method called Multimodal Probabilistic Fusion Prompts (MultiPoint) that leverages diverse cues from different modalities for multimodal sentiment detection in the few-shot scenario. Specifically, we start by introducing a Consistently Distributed Sampling approach called CDS, which ensures that the few-shot dataset has the same category distribution as the full dataset. Unlike previous approaches primarily using prompts based on the text modality, we design unified multimodal prompts to reduce discrepancies between different modalities and dynamically incorporate multimodal demonstrations into
    
[^64]: 使用形式语言对组合性进行基准测试

    Benchmarking Compositionality with Formal Languages. (arXiv:2208.08195v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.08195](http://arxiv.org/abs/2208.08195)

    通过使用形式语言的方法进行基准测试，我们发现大型NLP神经模型要么完全学习关系，要么完全不学习。转换覆盖率是关键，将软可学习界限设置为每个转换400个示例。

    

    将已知的原始概念重新组合成更大的新组合是一种典型的人类认知能力。在从数据中学习的过程中，大型NLP神经模型是否能够获得这种能力是一个开放的问题。本文从形式语言的角度研究了这个问题。我们使用确定性有限状态转换器生成具有可控组合性属性的无界数据集。通过对许多转换器进行随机抽样，我们探讨了它们的哪些属性能够对神经网络对组合性关系的可学习性做出贡献。我们发现模型要么完全学习关系，要么完全不学习。关键在于转换覆盖率，将软可学习界限设置为每个转换400个示例。

    Recombining known primitive concepts into larger novel combinations is a quintessentially human cognitive capability. Whether large neural models in NLP can acquire this ability while learning from data is an open question. In this paper, we investigate this problem from the perspective of formal languages. We use deterministic finite-state transducers to make an unbounded number of datasets with controllable properties governing compositionality. By randomly sampling over many transducers, we explore which of their properties contribute to learnability of a compositional relation by a neural network. We find that the models either learn the relations completely or not at all. The key is transition coverage, setting a soft learnability limit at 400 examples per transition.
    

