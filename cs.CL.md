# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ChatGPT Participates in a Computer Science Exam.](http://arxiv.org/abs/2303.09461) | ChatGPT在算法和数据结构考试中取得20.5分的成绩，表现出能在具有挑战性的大学考试中成功，但不能说明其对计算机科学有理解。 |
| [^2] | [Learning Cross-lingual Visual Speech Representations.](http://arxiv.org/abs/2303.09455) | 本研究使用跨语言自监督学习方法，利用未标记的多语种数据进行音频-视觉预训练，然后在标记的转录上对视觉模型进行微调，实验证明多语种模型性能优越，使用更相似的语言可以得到更好的结果，同时在看不见的语言上进行微调竞争力相当。 |
| [^3] | [Controlling High-Dimensional Data With Sparse Input.](http://arxiv.org/abs/2303.09446) | 本论文提出了一种新的控制机制，即将稀疏、易于人理解的控制空间映射到生成模型的潜在空间。通过实验，此方法表现出了效率、鲁棒性和保真性，即使只有少量的输入数值。 |
| [^4] | [Trustera: A Live Conversation Redaction System.](http://arxiv.org/abs/2303.09438) | Trustera是一种可以在客户和代理商的实时对话中屏蔽个人身份信息的系统，以保护敏感信息不被泄露，同时保持对话自然性。该系统使用自动语音识别、自然语言理解和实时音频屏蔽模块来实现这一目标，可以减少PII被拦截或存储在不安全数据存储中的风险。 |
| [^5] | [Jump to Conclusions: Short-Cutting Transformers With Linear Transformations.](http://arxiv.org/abs/2303.09435) | 本文提出了一种简单的方法，使用线性变换将隐藏表示转换为最终表示，绕过中间的Transformer计算。这种方法在语言模型的上下文中可“窥视”GPT-2和BERT的早期层表示，显示经常在早期层中LMs已经预测最终输出。 |
| [^6] | [Team SheffieldVeraAI at SemEval-2023 Task 3: Mono and multilingual approaches for news genre, topic and persuasion technique classification.](http://arxiv.org/abs/2303.09421) | 本文介绍了Team SheffieldVeraAI在SemEval-2023任务3中的表现。他们提出了用于新闻类型、框架和说服技巧分类的单语和多语言方法。该团队使用多种模型和适配器，取得了在不同语言下的好成绩。 |
| [^7] | [ToxVis: Enabling Interpretability of Implicit vs. Explicit Toxicity Detection Models with Interactive Visualization.](http://arxiv.org/abs/2303.09402) | ToxVis是一个可视化互动的工具，用于将在线内容分为隐式、显式和非令人反感的三类，利用深度学习解释技术提供分类结果解释，并为了解令人反感内容和支持更有效的内容审核提供了有价值的工具。 |
| [^8] | [Cryptocurrency Price Prediction using Twitter Sentiment Analysis.](http://arxiv.org/abs/2303.09397) | 本研究使用历史价格和Twitter情感分析预测比特币价格，并取得了不错的效果。情感预测的平均绝对百分比误差为9.45％，价格预测的平均绝对百分比误差为3.6％。 |
| [^9] | [Text-to-ECG: 12-Lead Electrocardiogram Synthesis conditioned on Clinical Text Reports.](http://arxiv.org/abs/2303.09395) | 本文提出了一种基于临床文本报告的自回归生成模型Auto-TTE，用于生成逼真的12导联心电图，相对于传统心电图生成模型并不只考虑单个心电信号的限制，实现了更具复杂性、更加真实的生成，在文本到心电图合成领域具有更好的效果。 |
| [^10] | [The Scope of In-Context Learning for the Extraction of Medical Temporal Constraints.](http://arxiv.org/abs/2303.09366) | 本研究定义了一种MTC分类，开发了一种基于CFG模型的抽取方法，并通过ICL自动提取和标准化DUGs中的MTC，有望通过定义安全的患者活动模式来推进以患者为中心的医疗应用。 |
| [^11] | [Tollywood Emotions: Annotation of Valence-Arousal in Telugu Song Lyrics.](http://arxiv.org/abs/2303.09364) | 该论文建立了一个Telugu歌曲的歌词情感数据集，用于音乐情感识别，研究者使用了两种分类技术进行研究，结果表明使用微调后的XLMRoBERTa模型可以比使用支持向量机模型更好的提高模型的性能。 |
| [^12] | [Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?.](http://arxiv.org/abs/2303.09325) | 该研究评估了生成式预训练变压器（GPT）在高等教育Python编程课程的初级和中级评估中的能力，并发现其在解决复杂的编程问题上遇到困难，限制了其在编程教育中的应用。 |
| [^13] | [TOT: Topology-Aware Optimal Transport For Multimodal Hate Detection.](http://arxiv.org/abs/2303.09314) | 本文针对隐式危害检测的挑战，提出一种面向表情包情境的拓扑感知最优传输框架TOT，利用最优传输核方法从多个模态中捕捉互补信息。 |
| [^14] | [Towards Robust Bangla Complex Named Entity Recognition.](http://arxiv.org/abs/2303.09306) | 本研究构建了基于 CRF 和深度学习（如 BanglaBERT）的鲁棒孟加拉复杂命名实体识别模型，解决了 CNER 任务，填补了孟加拉语复杂命名实体识别领域的空白。 |
| [^15] | [SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference.](http://arxiv.org/abs/2303.09266) | SmartBERT是一种改进的动态早期退出与层跳过机制，可以自适应地跳过一些层并自适应地选择是否退出，以加速BERT模型的推理速度。 |
| [^16] | [Block-wise Bit-Compression of Transformer-based Models.](http://arxiv.org/abs/2303.09184) | 本论文提出了一种用于Transformer的块位压缩方法，称为BBCT。该方法可以更细粒度地压缩整个Transformer，包括嵌入、矩阵乘法、GELU、softmax、层归一化和所有中间结果。在GLUE数据集上测试结果表明，BBCT可以在大多数任务中实现少于1％的准确率下降。 |
| [^17] | [A Short Survey of Viewing Large Language Models in Legal Aspect.](http://arxiv.org/abs/2303.09136) | 本文调查了大型语言模型在法律领域中的使用情况，讨论了它们应用于法律任务时所面临的法律问题，以及使用数据资源在法律领域中专门化LLMs的可能性。 |
| [^18] | [Exploring Distributional Shifts in Large Language Models for Code Analysis.](http://arxiv.org/abs/2303.09128) | 研究了两种大型语言模型在代码分析中处理领域外数据的能力，提出了组织、项目和模块的自然边界分割方法，发现每个新领域的样本都会产生分布偏移的挑战，实现了多任务学习与少量微调相结合的解决方案。 |
| [^19] | [Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models.](http://arxiv.org/abs/2303.09100) | 本文提出了一种基于贝叶斯概率的视觉语言模型提示学习方法，通过将提示标记推向忠实捕捉标签特定的视觉概念，而不是过度拟合训练类别，解决了现有提示工程的问题。在各种视觉语言任务上的广泛实验表明，该方法优于现有的最先进模型。 |
| [^20] | [GLEN: General-Purpose Event Detection for Thousands of Types.](http://arxiv.org/abs/2303.09093) | 研究者建立了一个通用事件检测数据集GLEN，涵盖了超过3,465种不同的事件类型，利用现有的标注，他们提出了一种新的多阶段事件检测模型，展示了在大本体大小和部分标签的情况下，该模型具有优越的性能。 |
| [^21] | [Investigating Failures to Generalize for Coreference Resolution Models.](http://arxiv.org/abs/2303.09092) | 本文研究了不同数据集上指代消解模型的表现，并发现模型的表现可能会受到数据集不同的操作化方式的影响，强调了在不同数据集上评估指代消解模型的重要性。 |
| [^22] | [Self-Consistent Learning: Cooperation between Generators and Discriminators.](http://arxiv.org/abs/2303.09075) | 本文提出了一个自一致学习的框架，通过鉴别器和生成器的合作训练，解决了标准GAN训练不稳定、样本容易偏离实际数据分布、鉴别模型改进饱和等问题。实验结果表明，该模型不仅优于最先进的GAN，在文本和图像生成任务中也实现了高质量的合成。 |
| [^23] | [Secret-Keeping in Question Answering.](http://arxiv.org/abs/2303.09067) | 本文针对系统“不应该”回答的问题提出了问答中的保密处理，旨在保护敏感用户或敏感信息。设计了一个概念验证架构，可以教会问答系统保守特定的机密。 |
| [^24] | [Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential.](http://arxiv.org/abs/2303.09038) | 本文研究探讨利用ChatGPT将放射学报告翻译成通俗易懂的语言，平均得分为5分制的4.1分，信息缺失率和信息错误率均较低，ChatGPT提供的建议大都与放射学报告相关。 |
| [^25] | [A Picture is Worth a Thousand Words: Language Models Plan from Pixels.](http://arxiv.org/abs/2303.09031) | 本文探讨从像素中使用预训练的语言模型进行规划，相对于之前的方法在ALFWorld和VirtualHome基准测试中取得更好的表现。 |
| [^26] | [ART: Automatic multi-step reasoning and tool-use for large language models.](http://arxiv.org/abs/2303.09014) | 该研究提出了自动推理和工具使用（ART）框架，用于为大型语言模型（LLMs）自动生成复杂多步推理步骤，可无缝集成外部工具支持。该框架获得了在新任务上显着改进，且无需手工制作演示或精心编写脚本。 |
| [^27] | [DeltaScore: Evaluating Story Generation with Differentiating Perturbations.](http://arxiv.org/abs/2303.08991) | DeltaScore利用差分扰动来评估故事生成的细粒度方面，并通过计算故事在特定方面扰动前后的可能性差异来衡量影响。该方法在多个故事领域中得到了评估，并与人类判断的相关性进行了研究。 |
| [^28] | [Cross-domain Sentiment Classification in Spanish.](http://arxiv.org/abs/2303.08985) | 本研究旨在研究使用跨领域技术将大型产品评论数据库的分类系统训练，以适用于不同的西班牙语领域的能力。 |
| [^29] | [PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented Dialogs.](http://arxiv.org/abs/2303.08954) | PRESTO是一个多语言数据集，包含超过55万个人与虚拟助手之间的上下文多语言对话。该数据集可以帮助解决如说话不连贯、代码切换和修正等真实NLU任务中出现的挑战。 |
| [^30] | [Applying unsupervised keyphrase methods on concepts extracted from discharge sheets.](http://arxiv.org/abs/2303.08928) | 本研究利用临床自然语言处理技术从出院单中提取概念，并应用无监督关键词方法识别重要概念，成功地解决了临床文本中的挑战。 |
| [^31] | [SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models.](http://arxiv.org/abs/2303.08896) | SelfCheckGPT是一种简单的基于采样的方法，可以以零资源的方式检查黑盒模型的幻觉现象。 |
| [^32] | [A Formalization of Operads in Coq.](http://arxiv.org/abs/2303.08894) | 本文介绍了在 Coq 中对 Operad 的形式化，并提供了利用 operad 来形式化编程语言的语义学的框架。 |
| [^33] | [ROSE: A Neurocomputational Architecture for Syntax.](http://arxiv.org/abs/2303.08877) | 本文提出ROSE模型，它是一种用于语法的神经计算架构，能够在不同的神经复杂度水平上进行结构构建并进行基本计算。 |
| [^34] | [Understanding Post-hoc Explainers: The Case of Anchors.](http://arxiv.org/abs/2303.08806) | 本文对Anchors进行了理论分析，这是一种基于规则的可解释性方法，用于解释文本分类器的决策。 |
| [^35] | [GPT-4 Technical Report.](http://arxiv.org/abs/2303.08774) | GPT-4是一个大规模多模态模型，可以接收图像和文本输入并产生文本输出，能够在各种专业和学术基准测试中表现出人类水平的表现，包括通过模拟的律师考试。该项目的核心组件是开发基础设施和优化方法，可在广泛的规模范围内表现预测性。 |
| [^36] | [FactReranker: Fact-guided Reranker for Faithful Radiology Report Summarization.](http://arxiv.org/abs/2303.08335) | FactReranker是一种新颖的辅助评估器，可以在保持摘要与放射学发现实况一致性的基础上，通过事实引导来有效地选择最佳的摘要。 |
| [^37] | [A Comprehensive Study on Post-Training Quantization for Large Language Models.](http://arxiv.org/abs/2303.08302) | 本文基于数万个零-shot实验对基于后训练量化的大型语言模型的不同量化组件进行了综合研究，结果发现细粒度量化和后训练量化方法很重要，用粗粒度量化的更高位数比用非常细粒度的更低位数更强大。我们给出了如何为不同大小的\llms利用量化的建议。 |
| [^38] | [FinXABSA: Explainable Finance through Aspect-Based Sentiment Analysis.](http://arxiv.org/abs/2303.02563) | 本文提出了一种基于方面的情感分析方法，通过与股票价格的相关性建立关系，实现金融分析的可解释性。该方法提供了更详细和准确的了解情感分析与股票价格之间关系的方法，对于投资者和金融分析师做出明智决策非常有用。 |
| [^39] | [Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering.](http://arxiv.org/abs/2303.01903) | 本研究提出了一个名为Prophet的框架，使用答案启发式方式促使GPT-3解决基于知识的视觉问答问题。在特定的知识型VQA数据集上训练一个纯VQA模型，并从中提取出答案启发式，可提高模型的性能。 |
| [^40] | [See Your Heart: Psychological states Interpretation through Visual Creations.](http://arxiv.org/abs/2302.10276) | 本文提出了一项挑战性任务——视觉情感解读任务(VEIT)，旨在通过AI对创作者的心理状态进行合理的解释。为此，本文提供了一个多模态数据集，该数据集得到了心理学理论的支持和专业的注释。据分析表明，该数据集不仅能够支持VEIT，而且相对于其他字幕数据集更具挑战性。 |
| [^41] | [Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech.](http://arxiv.org/abs/2302.07736) | 本研究探讨了 ChatGPT 是否可用于提供隐含仇恨言论检测的自然语言解释(NLE), 数据表明ChatGPT生成的NLEs比人类撰写的质量更好，但仍有局限性。 |
| [^42] | [AI Chat Assistants can Improve Conversations about Divisive Topics.](http://arxiv.org/abs/2302.07268) | 该论文介绍了一个大型实验的结果，证明了使用人工智能工具可以改善关于分裂性话题的在线对话。他们通过使用一种大型语言模型实时提供基于证据的建议，帮助人们在对话中感受到理解的感觉。 |
| [^43] | [Meta-Learning Siamese Network for Few-Shot Text Classification.](http://arxiv.org/abs/2302.03507) | 本文提出了Meta-SN，一种基于元学习的连对网络，用于解决几乎没有标签的文本分类问题。Meta-SN通过使用外部知识来编码类别标签的低维嵌入向量，并提出新的采样策略，克服了信念网络算法中的一些问题，提高了少样本学习的准确性。 |
| [^44] | [A Formal Algebraic Framework for DSL Composition.](http://arxiv.org/abs/2302.00744) | 本文提出了DSL组合的一个形式化代数框架，使用代数结构来模拟元语言，实现DSL抽象的编写、组合和互操作性，并提供了一个验证管道来验证该框架的组合属性。 |
| [^45] | [Can Very Large Pretrained Language Models Learn Storytelling With A Few Examples?.](http://arxiv.org/abs/2301.09790) | 本文探讨了使用极大预训练语言模型（VLPLMs）创作故事的可能性，并通过与SOTA模型在不同数据集上的比较，证明VLPLMs生成的故事质量更高，并展示一定程度上可与人类作者相抗衡，尽管初步调查揭示了它们倾向于“抄袭”真实的故事。 |
| [^46] | [Efficient Encoders for Streaming Sequence Tagging.](http://arxiv.org/abs/2301.09244) | 本研究提出了一种名为HEAR的混合编码器与自适应重启（HEAR），解决了在流式输入中使用双向编码器的不必要浮点操作和不必要标签翻转的问题，同时提高了流式输入上的标记性能。 |
| [^47] | [Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing.](http://arxiv.org/abs/2301.04558) | 该论文提出了一种自监督学习的方法，利用时间内容丰富了生物医学视觉语言处理。该方法考虑了之前的图像和报告，使用了CNN-Transformer混合多图像编码器，实现了在单个和多图像设置中的最先进性能。 |
| [^48] | [Human-Guided Fair Classification for Natural Language Processing.](http://arxiv.org/abs/2212.10154) | 本文提出了一种新方法，通过自动生成表达丰富的候选句子对并结合群众外包的成对人类判断，训练一个映射语义相似性到统计代理的个体公平性的模型，用于弥合人类直觉和公平分类规范之间的差距。该方法在表现能力、与人类直觉的一致性和两个基准数据集的分类准确性方面优于先前的方法。 |
| [^49] | [LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training.](http://arxiv.org/abs/2212.02691) | LUNA框架通过数字插件和预训练的方式，解决了当前基于transformer的语言模型无法很好理解数字的问题。 |
| [^50] | [Automaton-Based Representations of Task Knowledge from Generative Language Models.](http://arxiv.org/abs/2212.01944) | 提出了一个算法GLM2FSA，能够自动从任务目标的简短自然语言描述中提取任务知识并构建一个编码高层次任务知识的有限状态自动机，构建的自动机可以被正式验证。 |
| [^51] | [DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via Contrastive Prompt-Tuning.](http://arxiv.org/abs/2211.11337) | DreamArtist采用正负prompt-tuning学习策略来生成可控的一次性文本到图像，并解决了传统方法可能会导致模型过度拟合的问题。 |
| [^52] | [Language Model Decoding as Likelihood-Utility Alignment.](http://arxiv.org/abs/2210.07228) | 解码算法的选择需要考虑模型似然度和任务效用的匹配度问题，通过分类不匹配缓解策略（MMS）的视角，可以提高解码算法的通用性 |
| [^53] | [Natural Language Processing Methods to Identify Oncology Patients at High Risk for Acute Care with Clinical Notes.](http://arxiv.org/abs/2209.13860) | 本文研究了利用自然语言处理方法识别化疗后癌症患者急救护理风险的问题，与以往使用结构化卫生数据进行预测的模型相比较，结果表明两者差异不大，从而说明了在临床应用中采用语言模型的可行性以及不同患者群体风险偏差的存在。 |
| [^54] | [Chain of Explanation: New Prompting Method to Generate Higher Quality Natural Language Explanation for Implicit Hate Speech.](http://arxiv.org/abs/2209.04889) | 本研究提出了解释链提示方法，可以生成高质量的自然语言解释，帮助理解隐含仇恨言论。该方法通过提供准确的目标信息，将语言模型生成的BLUE分数从44.0提高到了62.3，已通过多种度量和人工注释的质量评估。 |
| [^55] | [Comparing Feature Importance and Rule Extraction for Interpretability on Text Data.](http://arxiv.org/abs/2207.01420) | 本文比较了文本数据上两类方法：计算每个特征的重要性分数和提取简单逻辑规则，发现在相同模型下产生的解释也不同。我们提出了一种比较解释差异的方法。 |
| [^56] | [How Adults Understand What Young Children Say.](http://arxiv.org/abs/2206.07807) | 成人如何理解幼儿的语言仍是一个复杂的问题，研究发现成人能够理解幼儿语言，是因为他们有关于幼儿试图传达信息的特定先验期望，并揭示了成人在早期沟通中的重要作用。 |
| [^57] | [A Sea of Words: An In-Depth Analysis of Anchors for Text Data.](http://arxiv.org/abs/2205.13789) | Anchors 是一种后处理的规则性可解释性方法，它可以通过凸显出一个小组词语（锚点）来强调模型的决策，我们首次对 Anchors 进行了理论分析，考虑到寻找最佳锚点是详尽的，并通过 TF-IDF 向量化步骤以及模型层次的显式结果，探究其在不同类别模型中的行为特征。我们还发现，在神经网络中，最高偏导数所对应的词汇可以重新加权用作 Anchors 词汇。 |
| [^58] | [Cross-linguistic differences in gender congruency effects: Evidence from meta-analyses.](http://arxiv.org/abs/2109.03490) | 通过元分析证实，日尔曼语言和斯拉夫语言中的性别一致效应比罗曼语言更加稳健，但效应大小适中，并且存在研究间的变异性。 |
| [^59] | [Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding.](http://arxiv.org/abs/2109.01636) | 研究开发了一种分布感知词嵌入，并实施了三种不同的方法来利用NER框架中的分布信息，实验表明将词的特异性融入NER方法可提高NER的性能。 |
| [^60] | [UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text.](http://arxiv.org/abs/2108.08614) | 本文提出了一个名为UNIQORN的问答系统，它能够无缝地处理RDF数据和文本，使用fine-tuned BERT模型为问题构建上下文图，并使用图算法确定与问题相关的子图来回答问题。 |

# 详细

[^1]: ChatGPT参加计算机科学考试

    ChatGPT Participates in a Computer Science Exam. (arXiv:2303.09461v1 [cs.CL])

    [http://arxiv.org/abs/2303.09461](http://arxiv.org/abs/2303.09461)

    ChatGPT在算法和数据结构考试中取得20.5分的成绩，表现出能在具有挑战性的大学考试中成功，但不能说明其对计算机科学有理解。

    

    我们要求ChatGPT参加“算法和数据结构”的本科计算机科学考试。我们评估了程序在整个考试中的表现，并将其答案手动复制到考试答题纸上，与其他200名学生一起进行匿名评分。我们发现ChatGPT勉强通过了考试，获得了40分中的20.5分。这个结果表明，ChatGPT确实可以在像大学考试这样的具有挑战性的任务中成功。同时，我们考试中的任务在结构上与其他在线可找到的考试卷、完成的作业问题和教学材料相似。因此，从这个实验中得出ChatGPT有任何计算机科学理解的结论是为时过早的。我们与ChatGPT的谈话记录可以在\url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}上找到，整个评分考试在本文的附录中。

    We asked ChatGPT to participate in an undergraduate computer science exam on ''Algorithms and Data Structures''. We evaluated the program on the entire exam as posed to the students. We hand-copied its answers onto an exam sheet, which was subsequently graded in a blind setup alongside those of 200 participating students. We find that ChatGPT narrowly passed the exam, obtaining 20.5 out of 40 points. This impressive performance indicates that ChatGPT can indeed succeed in challenging tasks like university exams. At the same time, the tasks in our exam are structurally similar to those on other exams, solved homework problems, and teaching materials that can be found online. Therefore, it would be premature to conclude from this experiment that ChatGPT has any understanding of computer science. The transcript of our conversation with ChatGPT is available at \url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}, and the entire graded exam is in the appendix of this paper.
    
[^2]: 跨语言视觉语音表示的学习

    Learning Cross-lingual Visual Speech Representations. (arXiv:2303.09455v1 [cs.CL])

    [http://arxiv.org/abs/2303.09455](http://arxiv.org/abs/2303.09455)

    本研究使用跨语言自监督学习方法，利用未标记的多语种数据进行音频-视觉预训练，然后在标记的转录上对视觉模型进行微调，实验证明多语种模型性能优越，使用更相似的语言可以得到更好的结果，同时在看不见的语言上进行微调竞争力相当。

    

    跨语言自监督学习是近年来逐渐流行的研究课题。当前相关工作仅限于利用音频信号进行表示学习。在本工作中，我们研究跨语言自监督的视觉表示学习。我们利用最近提出的RAVEn框架，对未标记的多语种数据进行音频-视觉预训练，然后在标记的转录上对视觉模型进行微调。我们的实验证明：（1）具有更多数据的多语种模型优于单语种模型，但当数据量固定时，单语种模型往往达到更好的性能；（2）多语种优于仅英语预训练；（3）使用更相似的语言可以得到更好的结果；（4）在看不见的语言上进行微调与在预训练集中使用目标语言相当竞争力。我们希望我们的研究能启发未来关于非英语语音表示学习的研究。

    Cross-lingual self-supervised learning has been a growing research topic in the last few years. However, current works only explored the use of audio signals to create representations. In this work, we study cross-lingual self-supervised visual representation learning. We use the recently-proposed Raw Audio-Visual Speech Encoders (RAVEn) framework to pre-train an audio-visual model with unlabelled multilingual data, and then fine-tune the visual model on labelled transcriptions. Our experiments show that: (1) multi-lingual models with more data outperform monolingual ones, but, when keeping the amount of data fixed, monolingual models tend to reach better performance; (2) multi-lingual outperforms English-only pre-training; (3) using languages which are more similar yields better results; and (4) fine-tuning on unseen languages is competitive to using the target language in the pre-training set. We hope our study inspires future research on non-English-only speech representation learni
    
[^3]: 用稀疏输入控制高维数据

    Controlling High-Dimensional Data With Sparse Input. (arXiv:2303.09446v1 [eess.AS])

    [http://arxiv.org/abs/2303.09446](http://arxiv.org/abs/2303.09446)

    本论文提出了一种新的控制机制，即将稀疏、易于人理解的控制空间映射到生成模型的潜在空间。通过实验，此方法表现出了效率、鲁棒性和保真性，即使只有少量的输入数值。

    

    本论文解决了人在环路控制生成高度结构化数据的问题。由于现有的生成模型缺乏有效的接口，使得用户可以修改输出，这个任务变得具有挑战性。用户或手动探索不可解释的潜在空间，或者费力地注释数据标签。为了解决这个问题，我们引入了一个新的框架，其中编码器将稀疏、易于人理解的控制空间映射到生成模型的潜在空间。我们将这个框架应用于控制文本转语音合成中的韵律的任务。我们提出了一个模型，称为多实例条件变分自编码器(MICVAE)，它专门设计用于编码稀疏的韵律特征并输出完整的波形。我们通过实验证明，MICVAE表现出了稀疏的人在环路控制机制所需的良好品质：效率、鲁棒性和保真性。即使只有非常少量的输入数值(~4)，MICVAE也能让用户实现控制。

    We address the problem of human-in-the-loop control for generating highly-structured data. This task is challenging because existing generative models lack an efficient interface through which users can modify the output. Users have the option to either manually explore a non-interpretable latent space, or to laboriously annotate the data with conditioning labels. To solve this, we introduce a novel framework whereby an encoder maps a sparse, human interpretable control space onto the latent space of a generative model. We apply this framework to the task of controlling prosody in text-to-speech synthesis. We propose a model, called Multiple-Instance CVAE (MICVAE), that is specifically designed to encode sparse prosodic features and output complete waveforms. We show empirically that MICVAE displays desirable qualities of a sparse human-in-the-loop control mechanism: efficiency, robustness, and faithfulness. With even a very small number of input values (~4), MICVAE enables users to im
    
[^4]: Trustera：一种实时屏蔽谈话系统

    Trustera: A Live Conversation Redaction System. (arXiv:2303.09438v1 [cs.CL])

    [http://arxiv.org/abs/2303.09438](http://arxiv.org/abs/2303.09438)

    Trustera是一种可以在客户和代理商的实时对话中屏蔽个人身份信息的系统，以保护敏感信息不被泄露，同时保持对话自然性。该系统使用自动语音识别、自然语言理解和实时音频屏蔽模块来实现这一目标，可以减少PII被拦截或存储在不安全数据存储中的风险。

    

    Trustera是第一个实现在实时谈话中屏蔽个人身份信息（PII）以消除代理商需要听到敏感信息的同时保留现场客户-代理商对话自然性的功能系统。与通话后的屏蔽相比，音频屏蔽开始于客户开始与PII实体交谈时。这显着降低了PII被拦截或存储在不安全数据存储中的风险。Trustera的架构由自动语音识别、自然语言理解和实时音频屏蔽模块的管道组成。系统的目标是三重的：屏蔽PII实体、掩盖发送给代理商的音频，同时捕获实体，以便捕获的PII可用于付款交易或来电者识别。Trustera目前正在被数千名代理商使用，以保护客户的敏感信息。

    Trustera, the first functional system that redacts personally identifiable information (PII) in real-time spoken conversations to remove agents' need to hear sensitive information while preserving the naturalness of live customer-agent conversations. As opposed to post-call redaction, audio masking starts as soon as the customer begins speaking to a PII entity. This significantly reduces the risk of PII being intercepted or stored in insecure data storage. Trustera's architecture consists of a pipeline of automatic speech recognition, natural language understanding, and a live audio redactor module. The system's goal is three-fold: redact entities that are PII, mask the audio that goes to the agent, and at the same time capture the entity, so that the captured PII can be used for a payment transaction or caller identification. Trustera is currently being used by thousands of agents to secure customers' sensitive information.
    
[^5]: 跳跃到结论：用线性变换简化Transformers

    Jump to Conclusions: Short-Cutting Transformers With Linear Transformations. (arXiv:2303.09435v1 [cs.CL])

    [http://arxiv.org/abs/2303.09435](http://arxiv.org/abs/2303.09435)

    本文提出了一种简单的方法，使用线性变换将隐藏表示转换为最终表示，绕过中间的Transformer计算。这种方法在语言模型的上下文中可“窥视”GPT-2和BERT的早期层表示，显示经常在早期层中LMs已经预测最终输出。

    

    基于Transformer的语言模型(LMs)在每个层次上都创建其输入的隐藏表示，但只使用最终层的表示进行预测。这使得模型的内部决策过程和中间表示的实用性变得模糊不清。为了阐明这一点，可以将隐藏表示转换为最终表示，绕过中间的Transformer计算。在本文中，我们提出了一种简单的方法，通过使用线性变换来进行这种转换。我们展示了我们的方法产生比目前流行的在最终层空间中检查所有层的隐藏表示的方法更准确的近似结果。此外，在语言建模的上下文中，我们的方法允许“窥视”GPT-2和BERT的早期层表示，显示经常在早期层中LMs已经预测最终输出。然后，我们展示了我们的方法对于最近的早期退出策略的实用性，表明当旨在……(原文截止)

    Transformer-based language models (LMs) create hidden representations of their inputs at every layer, but only use final-layer representations for prediction. This obscures the internal decision-making process of the model and the utility of its intermediate representations. One way to elucidate this is to cast the hidden representations as final representations, bypassing the transformer computation in-between. In this work, we suggest a simple method for such casting, by using linear transformations. We show that our approach produces more accurate approximations than the prevailing practice of inspecting hidden representations from all layers in the space of the final layer. Moreover, in the context of language modeling, our method allows "peeking" into early layer representations of GPT-2 and BERT, showing that often LMs already predict the final output in early layers. We then demonstrate the practicality of our method to recent early exit strategies, showing that when aiming, for
    
[^6]: SemEval-2023任务3中的单语和多语言方法：Team SheffieldVeraAI在新闻类型、主题和说服技巧分类方面的表现

    Team SheffieldVeraAI at SemEval-2023 Task 3: Mono and multilingual approaches for news genre, topic and persuasion technique classification. (arXiv:2303.09421v1 [cs.CL])

    [http://arxiv.org/abs/2303.09421](http://arxiv.org/abs/2303.09421)

    本文介绍了Team SheffieldVeraAI在SemEval-2023任务3中的表现。他们提出了用于新闻类型、框架和说服技巧分类的单语和多语言方法。该团队使用多种模型和适配器，取得了在不同语言下的好成绩。

    

    本文描述了我们在多语言环境下应用的方法，用于SemEval-2023任务3：在在线新闻中检测类别、框架和说服技巧。 对于子任务1（新闻类型），我们提出了一个完全训练和适配器mBERT模型的集成，其在德语中排名第一，并且具有多语言团队中最高的平均排名。 对于子任务2（框架），我们使用两个单独的集成：一个单语RoBERTa-MUPPETLARGE和一个XLM-RoBERTaLARGE的集成，分别使用适配器和任务自适应预训练，在3种语言中获得第一名，并在所有语言中获得最佳平均排名。 对于子任务3（说服技巧），我们为英语训练了一个单语言RoBERTa-Base模型和一个适用于剩余语言的多语言mBERT模型，其在所有语言中均排名前10，其中英语排名第二。 对于每个子任务，我们比较了单语和多语言方法，并考虑了类别不平衡技术。

    This paper describes our approach for SemEval-2023 Task 3: Detecting the category, the framing, and the persuasion techniques in online news in a multi-lingual setup. For Subtask 1 (News Genre), we propose an ensemble of fully trained and adapter mBERT models which was ranked joint-first for German, and had the highest mean rank of multi-language teams. For Subtask 2 (Framing), we achieved first place in 3 languages, and the best average rank across all the languages, by using two separate ensembles: a monolingual RoBERTa-MUPPETLARGE and an ensemble of XLM-RoBERTaLARGE with adapters and task adaptive pretraining. For Subtask 3 (Persuasion Techniques), we train a monolingual RoBERTa-Base model for English and a multilingual mBERT model for the remaining languages, which achieved top 10 for all languages, including 2nd for English. For each subtask, we compare monolingual and multilingual approaches, and consider class imbalance techniques.
    
[^7]: ToxVis：通过交互式可视化增强隐式与显式毒性检测模型的可解释性

    ToxVis: Enabling Interpretability of Implicit vs. Explicit Toxicity Detection Models with Interactive Visualization. (arXiv:2303.09402v1 [cs.CL])

    [http://arxiv.org/abs/2303.09402](http://arxiv.org/abs/2303.09402)

    ToxVis是一个可视化互动的工具，用于将在线内容分为隐式、显式和非令人反感的三类，利用深度学习解释技术提供分类结果解释，并为了解令人反感内容和支持更有效的内容审核提供了有价值的工具。

    

    在线平台上仇恨言论的崛起导致了有效内容审核的迫切需求。然而，包括隐式仇恨言论在内的令人反感的在线内容具有主观性和多方面性，对于人类审核员和内容审核系统都存在重大挑战。为了解决这个问题，我们开发了ToxVis，一个可视化互动和可解释的工具，用于将令人反感的言论分为三类：隐式的、显式的和非令人反感的。我们使用RoBERTa、XLNET和GPT-3 Fine-tune了两个基于transformer的模型，并使用深度学习解释技术来提供分类结果的解释。ToxVis使用户能够输入可能的令人反感的文本，并获得分类结果以及哪些单词对该决策做出了最大的贡献的可视化解释。通过使分类过程具有可解释性，ToxVis为了解令人反感的内容的微妙之处以及支持更有效的内容审核提供了有价值的工具。

    The rise of hate speech on online platforms has led to an urgent need for effective content moderation. However, the subjective and multi-faceted nature of hateful online content, including implicit hate speech, poses significant challenges to human moderators and content moderation systems. To address this issue, we developed ToxVis, a visually interactive and explainable tool for classifying hate speech into three categories: implicit, explicit, and non-hateful. We fine-tuned two transformer-based models using RoBERTa, XLNET, and GPT-3 and used deep learning interpretation techniques to provide explanations for the classification results. ToxVis enables users to input potentially hateful text and receive a classification result along with a visual explanation of which words contributed most to the decision. By making the classification process explainable, ToxVis provides a valuable tool for understanding the nuances of hateful content and supporting more effective content moderation
    
[^8]: 利用Twitter情感分析预测加密货币价格

    Cryptocurrency Price Prediction using Twitter Sentiment Analysis. (arXiv:2303.09397v1 [q-fin.ST])

    [http://arxiv.org/abs/2303.09397](http://arxiv.org/abs/2303.09397)

    本研究使用历史价格和Twitter情感分析预测比特币价格，并取得了不错的效果。情感预测的平均绝对百分比误差为9.45％，价格预测的平均绝对百分比误差为3.6％。

    

    随着加密货币的波动性及多样化的意见，在许多社交媒体平台上都成为了讨论的中心话题。Twitter 迅速成为新闻来源和比特币讨论的媒介。我们的算法旨在利用历史价格和推文情感来预测比特币的价格。在本研究中，我们开发了一种端到端模型，可使用双向编码器转换的神经网络模型预测推文集的情感，并使用预测的情感以及历史加密货币价格数据、推文数量、用户的追随者数量以及用户是否通过验证来预测比特币的价格。情感预测的平均绝对百分比误差为9.45％，反应了实时数据和测试数据的平均误差。而价格预测的平均绝对百分比误差则为3.6％。

    The cryptocurrency ecosystem has been the centre of discussion on many social media platforms, following its noted volatility and varied opinions. Twitter is rapidly being utilised as a news source and a medium for bitcoin discussion. Our algorithm seeks to use historical prices and sentiment of tweets to forecast the price of Bitcoin. In this study, we develop an end-to-end model that can forecast the sentiment of a set of tweets (using a Bidirectional Encoder Representations from Transformers - based Neural Network Model) and forecast the price of Bitcoin (using Gated Recurrent Unit) using the predicted sentiment and other metrics like historical cryptocurrency price data, tweet volume, a user's following, and whether or not a user is verified. The sentiment prediction gave a Mean Absolute Percentage Error of 9.45%, an average of real-time data, and test data. The mean absolute percent error for the price prediction was 3.6%.
    
[^9]: 基于临床文本报告的文本到心电图合成

    Text-to-ECG: 12-Lead Electrocardiogram Synthesis conditioned on Clinical Text Reports. (arXiv:2303.09395v1 [cs.CL])

    [http://arxiv.org/abs/2303.09395](http://arxiv.org/abs/2303.09395)

    本文提出了一种基于临床文本报告的自回归生成模型Auto-TTE，用于生成逼真的12导联心电图，相对于传统心电图生成模型并不只考虑单个心电信号的限制，实现了更具复杂性、更加真实的生成，在文本到心电图合成领域具有更好的效果。

    

    心电图合成是一种研究领域，旨在生成逼真的合成心电信号以供医疗使用，无需担心注释成本或临床数据隐私限制。传统的心电图生成模型只考虑单个心电信号，并使用基于GAN的生成模型。这些模型只能生成单导联样本，并要求每个诊断分类进行单独的培训。心电图的诊断分类无法捕捉心电图之间的复杂差异，这些差异取决于各种特征（例如患者人口统计学细节，共存的诊断分类等）。为了解决这些挑战，我们提出了一个文本到心电图任务，其中使用文本输入生成心电图输出。然后，我们首次提出了一个自回归生成模型Auto-TTE，它是一个基于临床文本报告条件的生成模型，用于合成12导联心电图。我们将我们的模型与文本到语音和文本到心电图领域的其他代表性模型进行了比较。结果表明，我们提出的方法在文本到心电图合成方面的表现优于最先进的模型。

    Electrocardiogram (ECG) synthesis is the area of research focused on generating realistic synthetic ECG signals for medical use without concerns over annotation costs or clinical data privacy restrictions. Traditional ECG generation models consider a single ECG lead and utilize GAN-based generative models. These models can only generate single lead samples and require separate training for each diagnosis class. The diagnosis classes of ECGs are insufficient to capture the intricate differences between ECGs depending on various features (e.g. patient demographic details, co-existing diagnosis classes, etc.). To alleviate these challenges, we present a text-to-ECG task, in which textual inputs are used to produce ECG outputs. Then we propose Auto-TTE, an autoregressive generative model conditioned on clinical text reports to synthesize 12-lead ECGs, for the first time to our knowledge. We compare the performance of our model with other representative models in text-to-speech and text-to-
    
[^10]: 基于上下文学习的医学时间约束抽取范围研究

    The Scope of In-Context Learning for the Extraction of Medical Temporal Constraints. (arXiv:2303.09366v1 [cs.CL])

    [http://arxiv.org/abs/2303.09366](http://arxiv.org/abs/2303.09366)

    本研究定义了一种MTC分类，开发了一种基于CFG模型的抽取方法，并通过ICL自动提取和标准化DUGs中的MTC，有望通过定义安全的患者活动模式来推进以患者为中心的医疗应用。

    

    药物治疗通常对患者的日常活动施加时间约束。违反医学时间约束（MTC）会导致缺乏治疗依从性，以及不良的健康结果和增加的医疗费用。这些MTC在患者教育材料和临床文本中的药物使用指南（DUGs）中被发现。通过在计算上表示DUGs中的MTC，将有助于通过帮助定义安全的患者活动模式来推进以患者为中心的医疗应用。我们定义了一种新颖的在DUGs中发现的MTC分类法，并开发了一种基于上下文无关文法（CFG）的模型来计算地表示MTC。此外，我们发布了三个新的数据集，共计N = 836个带标准化的MTC标记的DUGs。我们开发了一种上下文学习（ICL）解决方案，用于自动提取和标准化DUGs中发现的MTC，跨所有数据集实现了平均F1得分0.62。最后，我们对ICL模型进行了严格的研究。

    Medications often impose temporal constraints on everyday patient activity. Violations of such medical temporal constraints (MTCs) lead to a lack of treatment adherence, in addition to poor health outcomes and increased healthcare expenses. These MTCs are found in drug usage guidelines (DUGs) in both patient education materials and clinical texts. Computationally representing MTCs in DUGs will advance patient-centric healthcare applications by helping to define safe patient activity patterns. We define a novel taxonomy of MTCs found in DUGs and develop a novel context-free grammar (CFG) based model to computationally represent MTCs from unstructured DUGs. Additionally, we release three new datasets with a combined total of N = 836 DUGs labeled with normalized MTCs. We develop an in-context learning (ICL) solution for automatically extracting and normalizing MTCs found in DUGs, achieving an average F1 score of 0.62 across all datasets. Finally, we rigorously investigate ICL model perfor
    
[^11]: Tollywood情感：泰卢固语歌词中价值-唤起标注

    Tollywood Emotions: Annotation of Valence-Arousal in Telugu Song Lyrics. (arXiv:2303.09364v1 [cs.CL])

    [http://arxiv.org/abs/2303.09364](http://arxiv.org/abs/2303.09364)

    该论文建立了一个Telugu歌曲的歌词情感数据集，用于音乐情感识别，研究者使用了两种分类技术进行研究，结果表明使用微调后的XLMRoBERTa模型可以比使用支持向量机模型更好的提高模型的性能。

    

    在识别音乐情感方面，人们主要依赖于声学特征、社会标签和元数据，但很少关注歌词。目前没有包含价值和唤起手动评分的印度语歌曲数据集。我们在Spotify上收集了Telugu歌曲歌词的手动注释数据集，评注了离散尺度上的价值和唤起。对于价值和唤起，观察到相当高的人际间一致性。随后，我们使用两种分类技术创建了两个音乐情感识别模型，以从歌词中识别价值、唤起和相应的情感象限。使用术语频率-逆文档频率（TF-IDF）功能的支持向量机（SVM）和微调预训练XLMRoBERTa（XLM-R）模型用于价值，唤起和象限分类任务。微调后的XLMRoBERTa优于SVM，将价值、唤起和象限的宏平均F1分数分别提高了54.69％、67.61％和34.13％，达到77.90％。

    Emotion recognition from a given music track has heavily relied on acoustic features, social tags, and metadata but is seldom focused on lyrics. There are no datasets of Indian language songs that contain both valence and arousal manual ratings of lyrics. We present a new manually annotated dataset of Telugu songs' lyrics collected from Spotify with valence and arousal annotated on a discrete scale. A fairly high inter-annotator agreement was observed for both valence and arousal. Subsequently, we create two music emotion recognition models by using two classification techniques to identify valence, arousal and respective emotion quadrant from lyrics. Support vector machine (SVM) with term frequency-inverse document frequency (TF-IDF) features and fine-tuning the pre-trained XLMRoBERTa (XLM-R) model were used for valence, arousal and quadrant classification tasks. Fine-tuned XLMRoBERTa performs better than the SVM by improving macro-averaged F1-scores of 54.69%, 67.61%, 34.13% to 77.90
    
[^12]: GPT是否能通过高等教育编程课程的评估？

    Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?. (arXiv:2303.09325v1 [cs.AI])

    [http://arxiv.org/abs/2303.09325](http://arxiv.org/abs/2303.09325)

    该研究评估了生成式预训练变压器（GPT）在高等教育Python编程课程的初级和中级评估中的能力，并发现其在解决复杂的编程问题上遇到困难，限制了其在编程教育中的应用。

    

    我们评估了生成式预训练变压器（GPT）在高等教育Python编程课程的初级和中级评估中的能力。人们对这种新兴技术在编程教育方面的潜在用途（例如，练习生成，代码解释）和不良用途（例如，作弊）的讨论已经加 intens 了，但到目前为止，该模型在具有多种评估工具的广泛编程课程的现实环境中的能力还没有得到严格分析。我们在使用评估从简单的多项选择题（不涉及代码）到代码分布在多个文件中的复杂编程项目的三个Python课程上评估了GPT（总共599个练习题）。此外，我们研究了GPT模型如何成功地利用自动评分器提供的反馈。我们发现，当前的模型不能通过在高等教育Python编程课程中通常涉及的完整评估工具的全谱。虽然GPT模型可以成功地生成简单练习的语法正确代码，但对于更复杂的练习，他们遇到了困难，并且无法生成满足多文件编程项目要求的代码。此外，GPT模型显示了有限的利用自动评分器提供的反馈的能力。我们的研究结果表明GPT模型可能在编程教育中应用受到限制，并强调需要在这个领域进行更多的研究和开发。

    We evaluated the capability of generative pre-trained transformers (GPT), to pass assessments in introductory and intermediate Python programming courses at the postsecondary level. Discussions of potential uses (e.g., exercise generation, code explanation) and misuses (e.g., cheating) of this emerging technology in programming education have intensified, but to date there has not been a rigorous analysis of the models' capabilities in the realistic context of a full-fledged programming course with diverse set of assessment instruments. We evaluated GPT on three Python courses that employ assessments ranging from simple multiple-choice questions (no code involved) to complex programming projects with code bases distributed into multiple files (599 exercises overall). Further, we studied if and how successfully GPT models leverage feedback provided by an auto-grader. We found that the current models are not capable of passing the full spectrum of assessments typically involved in a Pyth
    
[^13]: TOT：面向多模态仇恨检测的拓扑感知最优传输

    TOT: Topology-Aware Optimal Transport For Multimodal Hate Detection. (arXiv:2303.09314v1 [cs.CL])

    [http://arxiv.org/abs/2303.09314](http://arxiv.org/abs/2303.09314)

    本文针对隐式危害检测的挑战，提出一种面向表情包情境的拓扑感知最优传输框架TOT，利用最优传输核方法从多个模态中捕捉互补信息。

    

    多模态仇恨检测旨在识别在线有害内容（如表情包等），是构建健康的互联网环境至关重要。以往的研究重点关注显式仇恨言论的检测，而忽略了隐式危害的分析，这在存在着扭曲或缺乏明显文本标记和人口统计视觉线索的情况下面临着特别大的挑战。本文提出了TOT：一种面向表情包情境的拓扑感知最优传输框架，将跨模态对齐问题转化为最优传输方案的求解。具体来说，我们利用最优传输核方法从多个模态中捕捉互补信息。核嵌入提供了一种非线性转换能力，以重现输入的分布。

    Multimodal hate detection, which aims to identify harmful content online such as memes, is crucial for building a wholesome internet environment. Previous work has made enlightening exploration in detecting explicit hate remarks. However, most of their approaches neglect the analysis of implicit harm, which is particularly challenging as explicit text markers and demographic visual cues are often twisted or missing. The leveraged cross-modal attention mechanisms also suffer from the distributional modality gap and lack logical interpretability. To address these semantic gaps issues, we propose TOT: a topology-aware optimal transport framework to decipher the implicit harm in memes scenario, which formulates the cross-modal aligning problem as solutions for optimal transportation plans. Specifically, we leverage an optimal transport kernel method to capture complementary information from multiple modalities. The kernel embedding provides a non-linear transformation ability to reproduce 
    
[^14]: 构建鲁棒的孟加拉复杂命名实体识别模型

    Towards Robust Bangla Complex Named Entity Recognition. (arXiv:2303.09306v1 [cs.CL])

    [http://arxiv.org/abs/2303.09306](http://arxiv.org/abs/2303.09306)

    本研究构建了基于 CRF 和深度学习（如 BanglaBERT）的鲁棒孟加拉复杂命名实体识别模型，解决了 CNER 任务，填补了孟加拉语复杂命名实体识别领域的空白。

    

    命名实体识别 (NER) 是自然语言处理中的基础任务，包括在文本中识别和分类命名实体。尽管孟加拉语是全球第七大使用语言，但针对孟加拉语复杂命名实体识别的工作还很少。CNER 是一项更具挑战性的任务，因为它涉及识别和分类复杂和复合实体，而这在孟加拉语中不常见。在本文中，我们提出了解决 BanglaCoNER 数据集上的 CNER 任务的获胜解决方案，使用了两种不同的方法，即条件随机场 (CRF) 和基于 finetuning transformer 的深度学习模型（如 BanglaBERT）。数据集包括 15300 个用于训练的句子和 800 个用于验证的句子，格式为 .conll。对数据集的探索性数据分析 (EDA) 揭示出数据集有 7 种不同的 NER 标签，其中有英语单词的明显存在。

    Named Entity Recognition (NER) is a fundamental task in natural language processing that involves identifying and classifying named entities in text. But much work hasn't been done for complex named entity recognition in Bangla, despite being the seventh most spoken language globally. CNER is a more challenging task than traditional NER as it involves identifying and classifying complex and compound entities, which are not common in Bangla language. In this paper, we present the winning solution of Bangla Complex Named Entity Recognition Challenge - addressing the CNER task on BanglaCoNER dataset using two different approaches, namely Conditional Random Fields (CRF) and finetuning transformer based Deep Learning models such as BanglaBERT.  The dataset consisted of 15300 sentences for training and 800 sentences for validation, in the .conll format. Exploratory Data Analysis (EDA) on the dataset revealed that the dataset had 7 different NER tags, with notable presence of English words, s
    
[^15]: SmartBERT：用于加速BERT推理的动态早期退出机制的改进

    SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference. (arXiv:2303.09266v1 [cs.CL])

    [http://arxiv.org/abs/2303.09266](http://arxiv.org/abs/2303.09266)

    SmartBERT是一种改进的动态早期退出与层跳过机制，可以自适应地跳过一些层并自适应地选择是否退出，以加速BERT模型的推理速度。

    

    动态早期退出被证明可以提高预训练语言模型（如BERT）的推理速度。然而，所有样本在早期退出之前都必须经过所有连续层，较复杂的样本通常会经历更多的层，仍然存在冗余计算。本文提出了一种名为SmartBERT的Bert推理的新型动态早期退出与层跳过相结合的机制，它将跳过门和退出算子加入到BERT的每一层中。SmartBERT可以自适应地跳过一些层并自适应地选择是否退出。此外，我们提出了跨层对比学习，并将其结合到我们的训练阶段中，以提高中间层和分类器，这对于早期退出是有益的。为了保持训练和推理阶段跳过门的一致使用，我们在训练阶段提出了一种硬权重机制。我们在GLUE基准测试的八个分类数据集上进行了实验。

    Dynamic early exiting has been proven to improve the inference speed of the pre-trained language model like BERT. However, all samples must go through all consecutive layers before early exiting and more complex samples usually go through more layers, which still exists redundant computation. In this paper, we propose a novel dynamic early exiting combined with layer skipping for BERT inference named SmartBERT, which adds a skipping gate and an exiting operator into each layer of BERT. SmartBERT can adaptively skip some layers and adaptively choose whether to exit. Besides, we propose cross-layer contrastive learning and combine it into our training phases to boost the intermediate layers and classifiers which would be beneficial for early exiting. To keep the consistent usage of skipping gates between training and inference phases, we propose a hard weight mechanism during training phase. We conduct experiments on eight classification datasets of the GLUE benchmark. Experimental resul
    
[^16]: 基于块的变压器模型的位压缩

    Block-wise Bit-Compression of Transformer-based Models. (arXiv:2303.09184v1 [cs.CL])

    [http://arxiv.org/abs/2303.09184](http://arxiv.org/abs/2303.09184)

    本论文提出了一种用于Transformer的块位压缩方法，称为BBCT。该方法可以更细粒度地压缩整个Transformer，包括嵌入、矩阵乘法、GELU、softmax、层归一化和所有中间结果。在GLUE数据集上测试结果表明，BBCT可以在大多数任务中实现少于1％的准确率下降。

    

    随着BERT、GPT-3和ChatGPT等近期基于Transformer的模型的流行，自然语言处理任务中取得了最先进的性能。然而，Transformer模型的巨大计算量、巨大的内存占用和高延迟是云计算中不可避免的挑战。为了解决这个问题，我们提出了BBCT方法，它是一种用于Transformer的块位压缩方法，无需重新训练。我们的方法实现了对整个Transformer的更细粒度的压缩，包括嵌入、矩阵乘法、GELU、softmax、层归一化和所有中间结果。我们以高效BERT为案例，使用BBCT方法进行压缩。我们在General Language Understanding Evaluation(GLUE)数据集上的测试结果表明，BBCT在大多数任务中的准确度下降小于1％。

    With the popularity of the recent Transformer-based models represented by BERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range of natural language processing tasks. However, the massive computations, huge memory footprint, and thus high latency of Transformer-based models is an inevitable challenge for the cloud with high real-time requirement. To tackle the issue, we propose BBCT, a method of block-wise bit-compression for transformer without retraining. Our method achieves more fine-grained compression of the whole transformer, including embedding, matrix multiplication, GELU, softmax, layer normalization, and all the intermediate results. As a case, we compress an efficient BERT with the method of BBCT. Our benchmark test results on General Language Understanding Evaluation (GLUE) show that BBCT can achieve less than 1% accuracy drop in most tasks.
    
[^17]: 查看大型语言模型在法律方面的简要调查

    A Short Survey of Viewing Large Language Models in Legal Aspect. (arXiv:2303.09136v1 [cs.CL])

    [http://arxiv.org/abs/2303.09136](http://arxiv.org/abs/2303.09136)

    本文调查了大型语言模型在法律领域中的使用情况，讨论了它们应用于法律任务时所面临的法律问题，以及使用数据资源在法律领域中专门化LLMs的可能性。

    

    大型语言模型（LLMs）已经改变了许多领域，包括自然语言处理、计算机视觉和强化学习。这些模型也在法律领域产生了重要的影响，它们被越来越多地用于自动化各种法律任务，例如法律判断预测、法律文件分析和法律文件撰写。然而，将LLMs整合到法律领域中也引发了一些法律问题，包括隐私问题、偏见和可解释性。在本次调查中，我们探索了将LLMs融入法律领域的情况。我们讨论了LLMs在法律任务中的各种应用，研究了它们使用时出现的法律挑战，并探讨了可以用于在法律领域专门化LLMs的数据资源。最后，我们讨论了几个有前途的方向并总结了本文。通过这样做，我们希望提供LLMs在法律上的当前状态概述，并突出其潜在的好处和挑战。

    Large language models (LLMs) have transformed many fields, including natural language processing, computer vision, and reinforcement learning. These models have also made a significant impact in the field of law, where they are being increasingly utilized to automate various legal tasks, such as legal judgement prediction, legal document analysis, and legal document writing. However, the integration of LLMs into the legal field has also raised several legal problems, including privacy concerns, bias, and explainability. In this survey, we explore the integration of LLMs into the field of law. We discuss the various applications of LLMs in legal tasks, examine the legal challenges that arise from their use, and explore the data resources that can be used to specialize LLMs in the legal domain. Finally, we discuss several promising directions and conclude this paper. By doing so, we hope to provide an overview of the current state of LLMs in law and highlight the potential benefits and c
    
[^18]: 探索用于代码分析的大型语言模型中的分布偏移

    Exploring Distributional Shifts in Large Language Models for Code Analysis. (arXiv:2303.09128v1 [cs.CL])

    [http://arxiv.org/abs/2303.09128](http://arxiv.org/abs/2303.09128)

    研究了两种大型语言模型在代码分析中处理领域外数据的能力，提出了组织、项目和模块的自然边界分割方法，发现每个新领域的样本都会产生分布偏移的挑战，实现了多任务学习与少量微调相结合的解决方案。

    

    我们系统地研究了两种大型语言模型 CodeT5 和 Codex 的能力，以便推广到领域外数据。在本研究中，我们考虑了两种基本应用：代码摘要和代码生成。我们按照其自然边界（按组织、按项目和按软件项目中的模块）将数据分为不同的领域。这样，在部署时，识别领域内和领域外的数据变得简单。我们发现，来自每个新领域的样本都会给这两个模型带来分布偏移的重大挑战。我们研究了不同的方法如何适应模型以更好地推广到新领域。我们的实验表明，虽然多任务学习本身是一个合理的基线，但将其与从训练数据中检索的示例的少量微调相结合可以实现非常强的性能。事实上，根据我们的实验，这种解决方案可以在非常低的数据情况下优于直接调整微调。

    We systematically study the capacity of two large language models for code CodeT5 and Codex - to generalize to out-of-domain data. In this study, we consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. This makes recognition of in-domain vs out-of-domain data at the time of deployment trivial. We establish that samples from each new domain present both models with a significant challenge of distribution shift. We study how well different established methods can adapt models to better generalize to new domains. Our experiments show that while multitask learning alone is a reasonable baseline, combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance. In fact, according to our experiments, this solution can outperform direct finetuning for very low-data scenarios.
    
[^19]: 视觉语言模型补丁-令牌对齐的贝叶斯提示学习

    Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models. (arXiv:2303.09100v1 [cs.CV])

    [http://arxiv.org/abs/2303.09100](http://arxiv.org/abs/2303.09100)

    本文提出了一种基于贝叶斯概率的视觉语言模型提示学习方法，通过将提示标记推向忠实捕捉标签特定的视觉概念，而不是过度拟合训练类别，解决了现有提示工程的问题。在各种视觉语言任务上的广泛实验表明，该方法优于现有的最先进模型。

    

    在视觉语言预训练模型的下游应用中，构建有效提示引起了极大关注。现有的提示工程方法要么需要费时费力的手动设计，要么将提示调优作为点估计问题进行优化，这可能无法描述类别的多样特征并限制了它们的应用。本文提出了一种基于贝叶斯概率的提示学习方法，其中通过从潜在分布中首先采样隐向量，然后采用轻量级生成模型来生成标签特定的随机提示。重要的是，我们将视觉知识与图像的语义规则化，并将图像和相应的提示视为补丁和令牌集，通过最优传输将提示标记推向忠实捕捉标签特定的视觉概念，而不是过度拟合训练类别。此外，所提出的模型还可以通过使用额外的基于文本的信息来生成更具信息量和准确性的提示。在各种视觉语言任务上的广泛实验表明，我们的补丁-令牌对齐的贝叶斯提示学习（PTBPL）优于现有的最先进模型。

    For downstream applications of vision-language pre-trained models, there has been significant interest in constructing effective prompts. Existing works on prompt engineering, which either require laborious manual designs or optimize the prompt tuning as a point estimation problem, may fail to describe diverse characteristics of categories and limit their applications. We introduce a Bayesian probabilistic resolution to prompt learning, where the label-specific stochastic prompts are generated hierarchically by first sampling a latent vector from an underlying distribution and then employing a lightweight generative model. Importantly, we semantically regularize prompt learning with the visual knowledge and view images and the corresponding prompts as patch and token sets under optimal transport, which pushes the prompt tokens to faithfully capture the label-specific visual concepts, instead of overfitting the training categories. Moreover, the proposed model can also be straightforwar
    
[^20]: GLEN：面向数千种类型的通用事件检测

    GLEN: General-Purpose Event Detection for Thousands of Types. (arXiv:2303.09093v1 [cs.CL])

    [http://arxiv.org/abs/2303.09093](http://arxiv.org/abs/2303.09093)

    研究者建立了一个通用事件检测数据集GLEN，涵盖了超过3,465种不同的事件类型，利用现有的标注，他们提出了一种新的多阶段事件检测模型，展示了在大本体大小和部分标签的情况下，该模型具有优越的性能。

    

    事件抽取系统的发展一直受限于缺乏广泛覆盖、大规模数据集。为了使事件抽取系统更易于使用，我们建立了一个通用事件检测数据集GLEN，涵盖了3,465种不同的事件类型，本体比任何当前数据集都大20倍以上。GLEN利用DWD叠加技术创建，通过提供维基百科Qnode和PropBank角色集之间的映射，使用PropBank的现有标注作为间接监督来完成创建。此外，我们还提出了一种新的多阶段事件检测模型，专门设计用于处理GLEN的大本体大小和部分标签。我们展示了我们的模型表现出优越的性能（F1分数提高了约10%），与传统的分类基线和较新的基于定义的模型相比。最后，我们进行了错误分析，并显示标签噪声仍然是提高性能的最大挑战。

    The development of event extraction systems has been hindered by the absence of wide-coverage, large-scale datasets. To make event extraction systems more accessible, we build a general-purpose event detection dataset GLEN, which covers 3,465 different event types, making it over 20x larger in ontology than any current dataset. GLEN is created by utilizing the DWD Overlay, which provides a mapping between Wikidata Qnodes and PropBank rolesets. This enables us to use the abundant existing annotation for PropBank as distant supervision. In addition, we also propose a new multi-stage event detection model specifically designed to handle the large ontology size and partial labels in GLEN. We show that our model exhibits superior performance (~10% F1 gain) compared to both conventional classification baselines and newer definition-based models. Finally, we perform error analysis and show that label noise is still the largest challenge for improving performance.
    
[^21]: 探究指代消解模型推广失败的原因

    Investigating Failures to Generalize for Coreference Resolution Models. (arXiv:2303.09092v1 [cs.CL])

    [http://arxiv.org/abs/2303.09092](http://arxiv.org/abs/2303.09092)

    本文研究了不同数据集上指代消解模型的表现，并发现模型的表现可能会受到数据集不同的操作化方式的影响，强调了在不同数据集上评估指代消解模型的重要性。

    

    指代消解模型通常会在多个数据集上进行评估。然而，数据集在如何实现指代消解方面（即理论概念在数据集中的操作化方式）上存在差异，这是由于选择语料库和注释指南等因素所致。本文旨在调查当前指代消解模型的错误程度与数据集之间的实现差异之间的关联程度（OntoNotes、PreCo和Winogrande）。具体而言，我们将模型性能分为多个类别，对应于多种指代，包括一般性提及、复合修饰符和连系谓词等。这种分类有助于我们调查最先进的模型在跨越不同指代类型的泛化能力方面可能会出现哪些差异。例如，在我们的实验中，在OntoNotes上训练的模型在PreCo中一般性提及和连系谓词上表现不佳。我们的发现强调了在多样化数据集和指代消解操作化方面评估指代消解模型的重要性。

    Coreference resolution models are often evaluated on multiple datasets. Datasets vary, however, in how coreference is realized -- i.e., how the theoretical concept of coreference is operationalized in the dataset -- due to factors such as the choice of corpora and annotation guidelines. We investigate the extent to which errors of current coreference resolution models are associated with existing differences in operationalization across datasets (OntoNotes, PreCo, and Winogrande). Specifically, we distinguish between and break down model performance into categories corresponding to several types of coreference, including coreferring generic mentions, compound modifiers, and copula predicates, among others. This break down helps us investigate how state-of-the-art models might vary in their ability to generalize across different coreference types. In our experiments, for example, models trained on OntoNotes perform poorly on generic mentions and copula predicates in PreCo. Our findings 
    
[^22]: 自一致学习：生成器和鉴别器的合作

    Self-Consistent Learning: Cooperation between Generators and Discriminators. (arXiv:2303.09075v1 [cs.CL])

    [http://arxiv.org/abs/2303.09075](http://arxiv.org/abs/2303.09075)

    本文提出了一个自一致学习的框架，通过鉴别器和生成器的合作训练，解决了标准GAN训练不稳定、样本容易偏离实际数据分布、鉴别模型改进饱和等问题。实验结果表明，该模型不仅优于最先进的GAN，在文本和图像生成任务中也实现了高质量的合成。

    

    最近，使用生成的数据来提高下游鉴别模型的性能已经因预训练语言模型的巨大发展而广受欢迎。在大多数先前的研究中，生成模型和鉴别模型是分别训练的，因此它们不能适应彼此的任何变化。因此，生成的样本很容易偏离实际数据分布，而鉴别模型的改进很快就会达到饱和。生成对抗网络（GAN）通过一种对抗性过程与鉴别模型训练生成模型以实现联合训练。然而，标准GAN的训练极不稳定，往往难以收敛。在本文中，为了解决这些问题，我们提出了一个自一致学习框架，其中一个鉴别器和一个生成器以闭环形式合作训练。鉴别器和生成器在多轮更新中相互增强，生成的样本逐渐接近实际数据分布，而鉴别模型不断提高其性能。实验结果表明，我们的模型不仅在各种数据集上优于最先进的GAN，而且在文本和图像生成任务中实现了高质量的合成。

    Using generated data to improve the performance of downstream discriminative models has recently gained popularity due to the great development of pre-trained language models. In most previous studies, generative models and discriminative models are trained separately and thus could not adapt to any changes in each other. As a result, the generated samples can easily deviate from the real data distribution, while the improvement of the discriminative model quickly reaches saturation. Generative adversarial networks (GANs) train generative models via an adversarial process with discriminative models to achieve joint training. However, the training of standard GANs is notoriously unstable and often falls short of convergence. In this paper, to address these issues, we propose a $\textit{self-consistent learning}$ framework, in which a discriminator and a generator are cooperatively trained in a closed-loop form. The discriminator and the generator enhance each other during multiple round
    
[^23]: 问答中的保密处理

    Secret-Keeping in Question Answering. (arXiv:2303.09067v1 [cs.CL])

    [http://arxiv.org/abs/2303.09067](http://arxiv.org/abs/2303.09067)

    本文针对系统“不应该”回答的问题提出了问答中的保密处理，旨在保护敏感用户或敏感信息。设计了一个概念验证架构，可以教会问答系统保守特定的机密。

    

    目前的问答研究侧重于始终提供答案的情况下非可回答问题的处理，但是如何应对那些系统“不应该”回答的问题呢？这可能是为了保护敏感用户或敏感信息。许多模型在遭受对抗性用户的审问时会暴露敏感信息。我们试图找出是否可能教会问答系统保守特定的机密。我们设计和实现了一个概念验证架构，并通过我们的评估确定尽管可能，但有许多未来研究的方向，以减少系统的偏执症（误报），信息泄漏（漏报）并将该工作的实现扩展到在信息汇聚的情况下保持机密的更复杂问题。

    Existing question-answering research focuses on unanswerable questions in the context of always providing an answer when a system can\dots but what about cases where a system {\bf should not} answer a question. This can either be to protect sensitive users or sensitive information. Many models expose sensitive information under interrogation by an adversarial user. We seek to determine if it is possible to teach a question-answering system to keep a specific fact secret. We design and implement a proof-of-concept architecture and through our evaluation determine that while possible, there are numerous directions for future research to reduce system paranoia (false positives), information leakage (false negatives) and extend the implementation of the work to more complex problems with preserving secrecy in the presence of information aggregation.
    
[^24]: 利用ChatGPT和Prompt Learning将放射学报告翻译成通俗易懂的语言：结果、限制和潜力。

    Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential. (arXiv:2303.09038v1 [cs.CL])

    [http://arxiv.org/abs/2303.09038](http://arxiv.org/abs/2303.09038)

    本文研究探讨利用ChatGPT将放射学报告翻译成通俗易懂的语言，平均得分为5分制的4.1分，信息缺失率和信息错误率均较低，ChatGPT提供的建议大都与放射学报告相关。

    

    ChatGPT作为一种大型语言模型，以其类似人类表达和推理能力而备受关注。本研究探讨使用ChatGPT将放射学报告翻译成通俗易懂的语言的可行性，以便患者和医疗服务提供者得到更好的医疗教育。研究采集了62份低剂量胸部CT肺癌筛查扫描和76份脑MRI转移性筛查扫描的放射学报告。根据放射科医师的评价，ChatGPT可以成功将放射学报告翻译成通俗易懂的语言，平均得分为5分制的4.1分，信息缺失0.07处，信息错误0.11处。就ChatGPT提供的建议而言，它们是一般性的相关建议，例如保持与医生的随访和密切监测任何症状，对于共138个病例中的约37％，ChatGPT提供了与放射学报告有关的建议。

    The large language model called ChatGPT has drawn extensively attention because of its human-like expression and reasoning abilities. In this study, we investigate the feasibility of using ChatGPT in experiments on using ChatGPT to translate radiology reports into plain language for patients and healthcare providers so that they are educated for improved healthcare. Radiology reports from 62 low-dose chest CT lung cancer screening scans and 76 brain MRI metastases screening scans were collected in the first half of February for this study. According to the evaluation by radiologists, ChatGPT can successfully translate radiology reports into plain language with an average score of 4.1 in the five-point system with 0.07 places of information missing and 0.11 places of misinformation. In terms of the suggestions provided by ChatGPT, they are general relevant such as keeping following-up with doctors and closely monitoring any symptoms, and for about 37% of 138 cases in total ChatGPT offer
    
[^25]: 一幅图胜过千言万语：语言模型从像素中规划路径

    A Picture is Worth a Thousand Words: Language Models Plan from Pixels. (arXiv:2303.09031v1 [cs.CL])

    [http://arxiv.org/abs/2303.09031](http://arxiv.org/abs/2303.09031)

    本文探讨从像素中使用预训练的语言模型进行规划，相对于之前的方法在ALFWorld和VirtualHome基准测试中取得更好的表现。

    

    规划是人工智能代理执行实际环境中长时间跨度任务的重要能力。本文探讨使用预训练的语言模型（PLMs）来从文本指令中推理出规划序列的方法。之前通过PLM进行规划的方法要么假定观察结果以文本形式可获得（例如由字幕模型提供），要么仅从指令中理解规划，或者只有有限方式地整合了有关视觉环境的信息（例如预训练的可供性函数）。相反，我们展示了即使观察结果直接编码为PLM的输入提示，PLM也能够准确进行规划。我们在ALFWorld和VirtualHome基准测试中实验展示了这种简单方法优于以前的方法。

    Planning is an important capability of artificial agents that perform long-horizon tasks in real-world environments. In this work, we explore the use of pre-trained language models (PLMs) to reason about plan sequences from text instructions in embodied visual environments. Prior PLM based approaches for planning either assume observations are available in the form of text (e.g., provided by a captioning model), reason about plans from the instruction alone, or incorporate information about the visual environment in limited ways (such as a pre-trained affordance function). In contrast, we show that PLMs can accurately plan even when observations are directly encoded as input prompts for the PLM. We show that this simple approach outperforms prior approaches in experiments on the ALFWorld and VirtualHome benchmarks.
    
[^26]: ART: 大型语言模型的自动多步推理和工具使用

    ART: Automatic multi-step reasoning and tool-use for large language models. (arXiv:2303.09014v1 [cs.CL])

    [http://arxiv.org/abs/2303.09014](http://arxiv.org/abs/2303.09014)

    该研究提出了自动推理和工具使用（ART）框架，用于为大型语言模型（LLMs）自动生成复杂多步推理步骤，可无缝集成外部工具支持。该框架获得了在新任务上显着改进，且无需手工制作演示或精心编写脚本。

    

    大型语言模型（LLMs）可以在少量和零-shot的情况下执行复杂的推理，生成中间的思维链（CoT）推理步骤。此外，每个推理步骤可以依赖外部工具来支持超出核心LLM功能的计算（例如搜索/运行代码）。在CoT提示和工具使用方面的先前工作通常需要手工制作特定任务的演示，并精心编写模型生成与工具使用交错的脚本。我们介绍了自动推理和工具使用（ART），这是一个使用冻结LLMs的框架，用于自动生成中间推理步骤作为程序。给定新的任务来解决，ART从任务库中选择多步推理和工具使用的演示。在测试时，ART在调用外部工具时无缝地暂停生成，并在恢复生成之前集成其输出。ART在BigBench和MMLU上看到了在未见任务的少量提示和自动CoT上的显着改进。

    Large language models (LLMs) can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought (CoT) reasoning steps. Further, each reasoning step can rely on external tools to support computation beyond the core LLM capabilities (e.g. search/running code). Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. We introduce Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program. Given a new task to solve, ART selects demonstrations of multi-step reasoning and tool use from a task library. At test time, ART seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. ART achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU
    
[^27]: DeltaScore: 利用差分扰动评价故事生成

    DeltaScore: Evaluating Story Generation with Differentiating Perturbations. (arXiv:2303.08991v1 [cs.CL])

    [http://arxiv.org/abs/2303.08991](http://arxiv.org/abs/2303.08991)

    DeltaScore利用差分扰动来评估故事生成的细粒度方面，并通过计算故事在特定方面扰动前后的可能性差异来衡量影响。该方法在多个故事领域中得到了评估，并与人类判断的相关性进行了研究。

    

    自然语言生成的各种评价指标存在，但对于故事生成的实用性有限，因为它们通常与人类判断的相关性不强，也不能测量细粒度的故事方面，例如流畅度与相关性，因为它们旨在评估整体生成质量。本文提出DeltaScore，一种利用扰动来评估细粒度故事方面的方法。我们的核心思想是基于这样的假设：故事在特定方面表现得越好（例如流畅度），它就会受到特定扰动（例如引入错别字）的影响越大。为了衡量影响，我们使用语言模型计算扰动前后故事的可能性差异。我们在多个故事领域中使用DeltaScore评估了基于状态的最新模型和传统基于相似性的指标，并研究了它与人类在五个细粒度故事方面的判断之间的相关性。

    Various evaluation metrics exist for natural language generation tasks, but they have limited utility for story generation since they generally do not correlate well with human judgments and do not measure fine-grained story aspects, such as fluency versus relatedness, as they are intended to assess overall generation quality. In this paper, we propose deltascore, an approach that utilizes perturbation to evaluate fine-grained story aspects. Our core idea is based on the hypothesis that the better the story performs in a specific aspect (e.g., fluency), the more it will be affected by a particular perturbation (e.g., introducing typos). To measure the impact, we calculate the likelihood difference between the pre- and post-perturbation stories using a language model. We evaluate deltascore against state-of-the-art model-based and traditional similarity-based metrics across multiple story domains, and investigate its correlation with human judgments on five fine-grained story aspects: f
    
[^28]: 跨领域西班牙语情感分类研究

    Cross-domain Sentiment Classification in Spanish. (arXiv:2303.08985v1 [cs.CL])

    [http://arxiv.org/abs/2303.08985](http://arxiv.org/abs/2303.08985)

    本研究旨在研究使用跨领域技术将大型产品评论数据库的分类系统训练，以适用于不同的西班牙语领域的能力。

    

    情感分类是自然语言处理领域中的一项基础任务，具有重要的学术和商业应用。其目的是自动预测包含观点和主观性的文本中存在的情感程度，例如产品和电影评论或推文。然而，由于不同领域的文本包含不同的词语和表达，因此跨领域和跨语言技术经常被应用于该任务以改善结果，而且由于缺乏数据库和资源，使用非英语语言的文本难度更大。因此，本研究旨在对使用大型产品评论数据库训练的分类系统在不同的西班牙语领域中的泛化能力进行研究。

    Sentiment Classification is a fundamental task in the field of Natural Language Processing, and has very important academic and commercial applications. It aims to automatically predict the degree of sentiment present in a text that contains opinions and subjectivity at some level, like product and movie reviews, or tweets. This can be really difficult to accomplish, in part, because different domains of text contains different words and expressions. In addition, this difficulty increases when text is written in a non-English language due to the lack of databases and resources. As a consequence, several cross-domain and cross-language techniques are often applied to this task in order to improve the results. In this work we perform a study on the ability of a classification system trained with a large database of product reviews to generalize to different Spanish domains. Reviews were collected from the MercadoLibre website from seven Latin American countries, allowing the creation of 
    
[^29]: PRESTO：用于解析逼真任务导向对话的多语言数据集

    PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented Dialogs. (arXiv:2303.08954v1 [cs.CL])

    [http://arxiv.org/abs/2303.08954](http://arxiv.org/abs/2303.08954)

    PRESTO是一个多语言数据集，包含超过55万个人与虚拟助手之间的上下文多语言对话。该数据集可以帮助解决如说话不连贯、代码切换和修正等真实NLU任务中出现的挑战。

    

    随着Google Assistant、Alexa和Siri等系统在日常生活中变得普遍，人们对任务导向型对话的研究兴趣不断增加。然而，在捕捉各种用户痛点的实际情况方面缺乏实际数据集，这限制了学术研究在这一领域的影响。为了使关于解析逼真对话的一些挑战性方面的研究成为可能，我们引入了PRESTO，一个包含超过55万个人与虚拟助手之间的上下文多语言对话的公共数据集。PRESTO包含了真实NLU任务中出现的多样化挑战，如说话不连贯、代码切换和修正。它是唯一一个提供每个示例的结构化上下文，如用户的联系人和列表的大规模人类生成会话解析数据集。我们基于mT5模型的基线表明，PRESTO中存在的对话现象具有挑战性，这在低资源设置中更加明显。

    Research interest in task-oriented dialogs has increased as systems such as Google Assistant, Alexa and Siri have become ubiquitous in everyday life. However, the impact of academic research in this area has been limited by the lack of datasets that realistically capture the wide array of user pain points. To enable research on some of the more challenging aspects of parsing realistic conversations, we introduce PRESTO, a public dataset of over 550K contextual multilingual conversations between humans and virtual assistants. PRESTO contains a diverse array of challenges that occur in real-world NLU tasks such as disfluencies, code-switching, and revisions. It is the only large scale human generated conversational parsing dataset that provides structured context such as a user's contacts and lists for each example. Our mT5 model based baselines demonstrate that the conversational phenomenon present in PRESTO are challenging to model, which is further pronounced in a low-resource setup.
    
[^30]: 应用无监督关键词方法对出院单中提取的概念进行处理

    Applying unsupervised keyphrase methods on concepts extracted from discharge sheets. (arXiv:2303.08928v1 [cs.CL])

    [http://arxiv.org/abs/2303.08928](http://arxiv.org/abs/2303.08928)

    本研究利用临床自然语言处理技术从出院单中提取概念，并应用无监督关键词方法识别重要概念，成功地解决了临床文本中的挑战。

    

    不同医疗保健提供者用各种科学水平和写作风格编写包含有价值患者信息的临床记录。对于处理广泛的电子医疗记录，理解什么信息是必要的可能对临床医生和研究人员有所帮助。实体识别和将其映射到标准术语是减少处理临床记录中的歧义的关键步骤。尽管命名实体识别和实体链接在临床自然语言处理中至关重要，但它们也可能导致产生重复和低价值的概念。因此，有必要确定每个内容记录的部分并识别关键概念以从临床文本中提取含义。本研究通过使用临床自然语言处理技术从出院单中提取概念，并应用无监督关键词方法来识别重要概念来解决这些挑战。结果表明，所提出的方法有效地从临床文本中识别出关键概念。

    Clinical notes containing valuable patient information are written by different health care providers with various scientific levels and writing styles. It might be helpful for clinicians and researchers to understand what information is essential when dealing with extensive electronic medical records. Entities recognizing and mapping them to standard terminologies is crucial in reducing ambiguity in processing clinical notes. Although named entity recognition and entity linking are critical steps in clinical natural language processing, they can also result in the production of repetitive and low-value concepts. In other hand, all parts of a clinical text do not share the same importance or content in predicting the patient's condition. As a result, it is necessary to identify the section in which each content is recorded and also to identify key concepts to extract meaning from clinical texts. In this study, these challenges have been addressed by using clinical natural language proc
    
[^31]: SelfCheckGPT: 零资源黑盒幻觉检测方法用于生成式大语言模型

    SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. (arXiv:2303.08896v1 [cs.CL])

    [http://arxiv.org/abs/2303.08896](http://arxiv.org/abs/2303.08896)

    SelfCheckGPT是一种简单的基于采样的方法，可以以零资源的方式检查黑盒模型的幻觉现象。

    

    生成式大语言模型（LLM）例如GPT-3，能够对各种用户提示进行高度流畅的响应。然而，LLM已知会产生幻觉事实和非事实陈述，这可能会削弱对它们的输出的信任。现有的事实检查方法要么需要访问令牌级输出概率分布（这可能对于ChatGPT等系统来说不可用），要么需要通过单独的通常复杂的模块接口的外部数据库。在这项工作中，我们提出了一种简单的基于采样的方法，称为“SelfCheckGPT”，可以以零资源的方式检查黑盒模型，即不需要外部数据库。 SelfCheckGPT利用一个简单的思想：如果LLM具有特定概念的知识，则采样的响应可能类似并包含一致的事实。但是，对于幻觉的事实，随机采样的响应可能会发散并相互矛盾。我们通过使用GP-T-3模型为例来研究此方法，并在常见任务上进行广泛实验，结果表明SelfCheckGPT能够有效地检测模型的幻觉现象，且在保持准确性的同时保持良好的效率。

    Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to token-level output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose "SelfCheckGPT", a simple sampling-based approach that can be used to fact-check black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if a LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GP
    
[^32]: Coq 中 Operad 的形式化

    A Formalization of Operads in Coq. (arXiv:2303.08894v1 [math.CT])

    [http://arxiv.org/abs/2303.08894](http://arxiv.org/abs/2303.08894)

    本文介绍了在 Coq 中对 Operad 的形式化，并提供了利用 operad 来形式化编程语言的语义学的框架。

    

    编程语言中的哪种特性能够提供最高的正确性保证？这个问题的一个答案，也是我们的解决方法，是在编程语言中提供一个形式化版本，特别是如果存在可表示的语义的话。达到这样的形式化版本为确保构造正确性提供了金标准。在 DARPA V-SPELLS 计划中，我们努力提供了元语言语义学基础的一个形式化版本，使用了一个叫做 operad 的数学对象。这个对象具有组合属性，对于用更小的代码块构建语言非常重要。在本文中，我们讨论了 operad 在 Coq 证明助手中的形式化定义。此外，我们在 Coq 中定义的 operad 能够提供指定在 Coq 中的物体是 operad 的证明。这项工作在 Coq 中为我们在 V-SPELLS 中开发元语言提供了一个形式化的数学基础。我们的工作还为编程语言社区提供了一个利用 operad 来形式化编程语言的语义学的框架。

    What provides the highest level of assurance for correctness of execution within a programming language? One answer, and our solution in particular, to this problem is to provide a formalization for, if it exists, the denotational semantics of a programming language. Achieving such a formalization provides a gold standard for ensuring a programming language is correct-by-construction. In our effort on the DARPA V-SPELLS program, we worked to provide a foundation for the denotational semantics of a meta-language using a mathematical object known as an operad. This object has compositional properties which are vital to building languages from smaller pieces. In this paper, we discuss our formalization of an operad in the proof assistant Coq. Moreover, our definition within Coq is capable of providing proofs that objects specified within Coq are operads. This work within Coq provides a formal mathematical basis for our meta-language development within V-SPELLS. Our work also provides, to 
    
[^33]: ROSE: 一种用于语法的神经计算架构

    ROSE: A Neurocomputational Architecture for Syntax. (arXiv:2303.08877v1 [cs.CL])

    [http://arxiv.org/abs/2303.08877](http://arxiv.org/abs/2303.08877)

    本文提出ROSE模型，它是一种用于语法的神经计算架构，能够在不同的神经复杂度水平上进行结构构建并进行基本计算。

    

    大脑中自然语言处理的综合模型必须包含四个组件：表示、操作、结构和编码。它还需要一个原则性的解释，阐述这些组件如何在机制上和因果上相互关联。本文通过扩展神经振荡如何指示各种语言过程的现有模型，提出了一种用于语法的神经计算架构，称为ROSE模型（表示、操作、结构、编码）。在ROSE下，语法的基本数据结构是原子特征、各种心理表示类型（R），并以单元和集合级别进行编码。这些单位经过基本计算（0）转化为可供后续结构构建级别访问的可操作对象。

    A comprehensive model of natural language processing in the brain must accommodate four components: representations, operations, structures and encoding. It further requires a principled account of how these components mechanistically, and causally, relate to each another. While previous models have isolated regions of interest for structure-building and lexical access, many gaps remain with respect to bridging distinct scales of neural complexity. By expanding existing accounts of how neural oscillations can index various linguistic processes, this article proposes a neurocomputational architecture for syntax, termed the ROSE model (Representation, Operation, Structure, Encoding). Under ROSE, the basic data structures of syntax are atomic features, types of mental representations (R), and are coded at the single-unit and ensemble level. Elementary computations (O) that transform these units into manipulable objects accessible to subsequent structure-building levels are coded via high 
    
[^34]: 理解事后解释器：以Anchors为例

    Understanding Post-hoc Explainers: The Case of Anchors. (arXiv:2303.08806v1 [stat.ML])

    [http://arxiv.org/abs/2303.08806](http://arxiv.org/abs/2303.08806)

    本文对Anchors进行了理论分析，这是一种基于规则的可解释性方法，用于解释文本分类器的决策。

    

    在许多情况下，机器学习模型可解释性是一项高度要求但难以实现的任务。为了解释这些模型的个体预测，已经提出了本地模型无关方法。然而，产生解释的过程对于用户来说可能与要解释的预测一样神秘。此外，可解释性方法经常缺乏理论保证，并且它们在简单模型上的行为通常是未知的。本文对Anchors（Ribeiro等人，2018）进行理论分析：一种流行的基于规则的可解释性方法，它强调一小组单词以解释文本分类器的决策。

    In many scenarios, the interpretability of machine learning models is a highly required but difficult task. To explain the individual predictions of such models, local model-agnostic approaches have been proposed. However, the process generating the explanations can be, for a user, as mysterious as the prediction to be explained. Furthermore, interpretability methods frequently lack theoretical guarantees, and their behavior on simple models is frequently unknown. While it is difficult, if not impossible, to ensure that an explainer behaves as expected on a cutting-edge model, we can at least ensure that everything works on simple, already interpretable models. In this paper, we present a theoretical analysis of Anchors (Ribeiro et al., 2018): a popular rule-based interpretability method that highlights a small set of words to explain a text classifier's decision. After formalizing its algorithm and providing useful insights, we demonstrate mathematically that Anchors produces meaningf
    
[^35]: GPT-4技术报告

    GPT-4 Technical Report. (arXiv:2303.08774v1 [cs.CL])

    [http://arxiv.org/abs/2303.08774](http://arxiv.org/abs/2303.08774)

    GPT-4是一个大规模多模态模型，可以接收图像和文本输入并产生文本输出，能够在各种专业和学术基准测试中表现出人类水平的表现，包括通过模拟的律师考试。该项目的核心组件是开发基础设施和优化方法，可在广泛的规模范围内表现预测性。

    

    我们报告了GPT-4的开发，它是一个可以接受图像和文本输入并产生文本输出的大规模多模态模型。虽然在许多现实场景中不如人类，但GPT-4在各种专业和学术基准测试中表现出人类水平的表现，包括通过模拟的律师考试，成绩排名在前10％左右。GPT-4是一个基于Transformer的模型，预训练用于预测文档中的下一个标记。后训练对齐过程提高了事实性和符合期望行为的性能指标。项目的核心组件是开发基础设施和优化方法，可在广泛的规模范围内表现预测性。这使我们能够准确预测GPT-4的某些性能方面，而这些性能是基于使用不超过GPT-4计算能力的1/1,000的模型训练的。

    We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.
    
[^36]: FactReranker：基于事实引导的辅助评估器用于忠实的放射学报告摘要。

    FactReranker: Fact-guided Reranker for Faithful Radiology Report Summarization. (arXiv:2303.08335v1 [cs.CL])

    [http://arxiv.org/abs/2303.08335](http://arxiv.org/abs/2303.08335)

    FactReranker是一种新颖的辅助评估器，可以在保持摘要与放射学发现实况一致性的基础上，通过事实引导来有效地选择最佳的摘要。

    

    自动放射学报告摘要是一项至关重要的临床任务，其主要挑战在于保持所产生的摘要和地面实况放射学发现之间的实际准确性。现有研究采用强化学习来直接优化正确认知度量指标，如CheXBert或RadGraph分数。然而，它们使用贪婪搜索或束搜索的解码方法，在选择最佳候选项时没有考虑事实的一致性，从而导致实际一致性的改善受限。为了解决这个问题，我们提出了一种新颖的第二阶段摘要方法FactReranker，它是第一次尝试基于它们估计的实际一致性得分来学习从所有候选项中选择最佳摘要。我们建议基于RadGraph模式提取输入医疗报告、其黄金摘要和候选摘要的医疗事实，并设计基于事实引导的重新排序器，以有效地结合提取的医疗事实来选择最佳摘要。我们分解了事实-

    Automatic radiology report summarization is a crucial clinical task, whose key challenge is to maintain factual accuracy between produced summaries and ground truth radiology findings. Existing research adopts reinforcement learning to directly optimize factual consistency metrics such as CheXBert or RadGraph score. However, their decoding method using greedy search or beam search considers no factual consistency when picking the optimal candidate, leading to limited factual consistency improvement. To address it, we propose a novel second-stage summarizing approach FactReranker, the first attempt that learns to choose the best summary from all candidates based on their estimated factual consistency score. We propose to extract medical facts of the input medical report, its gold summary, and candidate summaries based on the RadGraph schema and design the fact-guided reranker to efficiently incorporate the extracted medical facts for selecting the optimal summary. We decompose the fact-
    
[^37]: 基于后训练量化的大型语言模型综合研究

    A Comprehensive Study on Post-Training Quantization for Large Language Models. (arXiv:2303.08302v1 [cs.LG])

    [http://arxiv.org/abs/2303.08302](http://arxiv.org/abs/2303.08302)

    本文基于数万个零-shot实验对基于后训练量化的大型语言模型的不同量化组件进行了综合研究，结果发现细粒度量化和后训练量化方法很重要，用粗粒度量化的更高位数比用非常细粒度的更低位数更强大。我们给出了如何为不同大小的\llms利用量化的建议。

    

    后训练量化是一种减少大型语言模型内存消耗和/或计算成本的权衡方法。然而，关于不同量化方案、不同模型族、不同后训练量化方法、不同量化位精度等的影响的全面研究仍缺失。本文通过数万个零-shot实验对这些组件进行了广泛的研究。我们的研究结果表明：(1)细粒度量化和后训练量化方法(而不是朴素的最近舍入量化)是实现良好精度的必要条件；(2) 用粗粒度量化的更高位数（如5位）比用非常细粒度的更低位数（如4位）（其有效位数与5位相似）更强大。我们还提出了如何为不同大小的\llms利用量化的建议，并留下未来机会和系统工作的建议。

    Post-training quantization (\ptq) had been recently shown as a compromising method to reduce the memory consumption and/or compute cost for large language models. However, a comprehensive study about the effect of different quantization schemes, different model families, different \ptq methods, different quantization bit precision, etc, is still missing. In this work, we provide an extensive study on those components over tens of thousands of zero-shot experiments. Our results show that (1) Fine-grained quantization and \ptq methods (instead of naive round-to-nearest quantization) are necessary to achieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained quantization is more powerful than lower bits (e.g., 4 bits) with very fine-grained quantization (whose effective bits is similar to 5-bits). We also present recommendations about how to utilize quantization for \llms with different sizes, and leave suggestions of future opportunities and system work that are not res
    
[^38]: FinXABSA: 通过基于方面的情感分析实现可解释的金融分析

    FinXABSA: Explainable Finance through Aspect-Based Sentiment Analysis. (arXiv:2303.02563v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.02563](http://arxiv.org/abs/2303.02563)

    本文提出了一种基于方面的情感分析方法，通过与股票价格的相关性建立关系，实现金融分析的可解释性。该方法提供了更详细和准确的了解情感分析与股票价格之间关系的方法，对于投资者和金融分析师做出明智决策非常有用。

    This paper proposes an aspect-based sentiment analysis approach to achieve explainability in financial analysis by establishing a relationship with stock prices using the Pearson correlation coefficient. The proposed methodology provides a more detailed and accurate understanding of the relationship between sentiment analysis and stock prices, which can be useful for investors and financial analysts in making informed decisions.

    本文提出了一种新颖的方法，通过利用Pearson相关系数建立基于方面的情感分析与股票价格之间的关系，实现金融分析的可解释性。所提出的方法涉及从金融新闻文章中构建方面列表，并分析每个方面的情感强度得分。然后，使用Pearson系数将这些得分与相关公司的股票价格进行比较，以确定任何显著的相关性。结果表明，所提出的方法提供了更详细和准确的了解情感分析与股票价格之间关系的方法，这对于投资者和金融分析师做出明智决策非常有用。此外，该方法提供了一种透明且可解释的方式来解释情感分析结果及其对股票价格的影响。总的来说，本文的研究结果表明，可解释性在金融分析中的重要性。

    This paper presents a novel approach for explainability in financial analysis by utilizing the Pearson correlation coefficient to establish a relationship between aspect-based sentiment analysis and stock prices. The proposed methodology involves constructing an aspect list from financial news articles and analyzing sentiment intensity scores for each aspect. These scores are then compared to the stock prices for the relevant companies using the Pearson coefficient to determine any significant correlations. The results indicate that the proposed approach provides a more detailed and accurate understanding of the relationship between sentiment analysis and stock prices, which can be useful for investors and financial analysts in making informed decisions. Additionally, this methodology offers a transparent and interpretable way to explain the sentiment analysis results and their impact on stock prices. Overall, the findings of this paper demonstrate the importance of explainability in f
    
[^39]: 用答案启发式方式促使大型语言模型解决基于知识的视觉问答问题

    Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering. (arXiv:2303.01903v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.01903](http://arxiv.org/abs/2303.01903)

    本研究提出了一个名为Prophet的框架，使用答案启发式方式促使GPT-3解决基于知识的视觉问答问题。在特定的知识型VQA数据集上训练一个纯VQA模型，并从中提取出答案启发式，可提高模型的性能。

    

    基于知识的视觉问答需要超出图像范围的外部知识来回答问题。早期的研究从显式知识库（KBs）检索所需的知识，这经常会引入与问题无关的信息，从而限制了模型的性能。最近的研究试图将大型语言模型（即GPT-3）作为隐含式知识引擎来获取回答所需的必要知识。尽管这些方法取得了令人鼓舞的结果，但我们认为它们还没有充分发挥GPT-3的能力，因为提供的输入信息仍然不足。在本文中，我们提出了Prophet——一个概念上简单的框架，旨在通过回答启发式方式，促使GPT-3解决基于知识的VQA问题。具体来说，我们首先在特定的基于知识的VQA数据集上训练一个纯VQA模型，而不使用外部知识。之后，我们从模型中提取了两种互补的答案启发式：答案候选项。

    Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have sought to use a large language model (i.e., GPT-3) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of GPT-3 as the provided input information is insufficient. In this paper, we present Prophet -- a conceptually simple framework designed to prompt GPT-3 with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the model: answer candidates 
    
[^40]: 通过视觉创作解读心理状态：《看见你的内心》

    See Your Heart: Psychological states Interpretation through Visual Creations. (arXiv:2302.10276v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.10276](http://arxiv.org/abs/2302.10276)

    本文提出了一项挑战性任务——视觉情感解读任务(VEIT)，旨在通过AI对创作者的心理状态进行合理的解释。为此，本文提供了一个多模态数据集，该数据集得到了心理学理论的支持和专业的注释。据分析表明，该数据集不仅能够支持VEIT，而且相对于其他字幕数据集更具挑战性。

    

    在精神分析领域中，通过视觉创作生成对个体心理状态的解释正面临着很大的需求。现有的计算机视觉研究主要包括情绪分类和影响标注两个任务，难以满足心理解释的需求。为了满足精神分析的需求，本文提出一项挑战性任务——视觉情感解读任务(VEIT)。VEIT要求人工智能通过视觉创作生成合理的创作者心理状态的解释。为支持该任务，我们提出了一个多模态数据集，称为Sandplay Interpretation Dataset (SpyIn)，该数据集得到了心理学理论的支持和专业的注释。数据集分析表明，SpyIn不仅能够支持VEIT，而且相对于其他字幕数据集更具挑战性。基于SpyIn，我们进行了几项图像生成任务的实验。

    In psychoanalysis, generating interpretations to one's psychological state through visual creations is facing significant demands. The two main tasks of existing studies in the field of computer vision, sentiment/emotion classification and affective captioning, can hardly satisfy the requirement of psychological interpreting. To meet the demands for psychoanalysis, we introduce a challenging task, \textbf{V}isual \textbf{E}motion \textbf{I}nterpretation \textbf{T}ask (VEIT). VEIT requires AI to generate reasonable interpretations of creator's psychological state through visual creations. To support the task, we present a multimodal dataset termed SpyIn (\textbf{S}and\textbf{p}la\textbf{y} \textbf{In}terpretation Dataset), which is psychological theory supported and professional annotated. Dataset analysis illustrates that SpyIn is not only able to support VEIT, but also more challenging compared with other captioning datasets. Building on SpyIn, we conduct experiments of several image 
    
[^41]: ChatGPT比人类标注员更好吗? ChatGPT在解释隐含仇恨言论方面的潜力和局限性(arXiv:2302.07736v2 [cs.CL]已更新)

    Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech. (arXiv:2302.07736v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.07736](http://arxiv.org/abs/2302.07736)

    本研究探讨了 ChatGPT 是否可用于提供隐含仇恨言论检测的自然语言解释(NLE), 数据表明ChatGPT生成的NLEs比人类撰写的质量更好，但仍有局限性。

    

    最近的研究表明，很多在线仇恨言论都是隐含的。由于其微妙的性质，检测这种仇恨言论的可解释性一直是一个具有挑战性的问题。在本研究中，我们研究了 ChatGPT 是否可用于提供隐含仇恨言论检测的自然语言解释(NLE)。我们设计了提示语以引出简洁的 ChatGPT 生成的 NLE，并通过与人类撰写的 NLE 进行比较，进行用户研究以评估其质量。我们讨论了 ChatGPT 在隐含仇恨言论研究中的潜力和局限性。

    Recent studies have alarmed that many online hate speeches are implicit. With its subtle nature, the explainability of the detection of such hateful speech has been a challenging problem. In this work, we examine whether ChatGPT can be used for providing natural language explanations (NLEs) for implicit hateful speech detection. We design our prompt to elicit concise ChatGPT-generated NLEs and conduct user studies to evaluate their qualities by comparison with human-written NLEs. We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research.
    
[^42]: AI聊天助手可改善关于分裂性话题的对话

    AI Chat Assistants can Improve Conversations about Divisive Topics. (arXiv:2302.07268v4 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2302.07268](http://arxiv.org/abs/2302.07268)

    该论文介绍了一个大型实验的结果，证明了使用人工智能工具可以改善关于分裂性话题的在线对话。他们通过使用一种大型语言模型实时提供基于证据的建议，帮助人们在对话中感受到理解的感觉。

    

    人类在线交流数量正在迅速增长。但是，社交媒体平台、消息应用程序和其他数字论坛上的基于文本的互动可能会产生分裂和冲突。这种有毒性增加了极化的程度，并且重要的是，侵蚀了多元化社会发展解决影响所有人的复杂社会问题的能力。学者和民间社会组织推动干预措施，使面对面的对话不那么具有分裂性或更具生产力，但将这些努力扩展至在线发生的许多话语是极具挑战性的。我们展示了一个大规模实验的结果，该实验证明了人工智能工具如何改善关于分裂性话题的在线对话。具体而言，我们采用大型语言模型实时提供基于证据的建议，以改善参与者在对话中感受到理解的感觉。我们发现这些建议确实可以帮助人们改善对话质量。

    A rapidly increasing amount of human conversation occurs online. But divisiveness and conflict can fester in text-based interactions on social media platforms, in messaging apps, and on other digital forums. Such toxicity increases polarization and, importantly, corrodes the capacity of diverse societies to develop efficient solutions to complex social problems that impact everyone. Scholars and civil society groups promote interventions that can make interpersonal conversations less divisive or more productive in offline settings, but scaling these efforts to the amount of discourse that occurs online is extremely challenging. We present results of a large-scale experiment that demonstrates how online conversations about divisive topics can be improved with artificial intelligence tools. Specifically, we employ a large language model to make real-time, evidence-based recommendations intended to improve participants' perception of feeling understood in conversations. We find that these
    
[^43]: 几乎没有标签的文本分类的元学习连对网络

    Meta-Learning Siamese Network for Few-Shot Text Classification. (arXiv:2302.03507v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.03507](http://arxiv.org/abs/2302.03507)

    本文提出了Meta-SN，一种基于元学习的连对网络，用于解决几乎没有标签的文本分类问题。Meta-SN通过使用外部知识来编码类别标签的低维嵌入向量，并提出新的采样策略，克服了信念网络算法中的一些问题，提高了少样本学习的准确性。

    

    几乎没有标签的文本分类问题可以通过少量样本训练数据来解决，其中元学习方法（例如PROTO）已经被证明是有效的。本文提出了一种元学习连对网络，Meta-SN，来解决PROTO方法中存在的三个问题：（1）忽略了计算原型向量时采样支持集的随机性；（2）忽略了标记样本的重要性；（3）以纯随机方式构建元任务。Meta-SN利用外部知识（例如类别名称和描述文本）来编码类别标签的低维嵌入向量，而不是从采样支持集中计算原型向量。此外，Meta-SN提出了一种新的采样策略，即增加了对难以分类样本的采样概率。在基准数据集上进行了大量的实验，结果表明Meta-SN优于包括PROTO在内的多种最先进的少样本学习方法。

    Few-shot learning has been used to tackle the problem of label scarcity in text classification, of which meta-learning based methods have shown to be effective, such as the prototypical networks (PROTO). Despite the success of PROTO, there still exist three main problems: (1) ignore the randomness of the sampled support sets when computing prototype vectors; (2) disregard the importance of labeled samples; (3) construct meta-tasks in a purely random manner. In this paper, we propose a Meta-Learning Siamese Network, namely, Meta-SN, to address these issues. Specifically, instead of computing prototype vectors from the sampled support sets, Meta-SN utilizes external knowledge (e.g. class names and descriptive texts) for class labels, which is encoded as the low-dimensional embeddings of prototype vectors. In addition, Meta-SN presents a novel sampling strategy for constructing meta-tasks, which gives higher sampling probabilities to hard-to-classify samples. Extensive experiments are con
    
[^44]: DSL组合的一个形式化代数框架

    A Formal Algebraic Framework for DSL Composition. (arXiv:2302.00744v1 [math.CT] CROSS LISTED)

    [http://arxiv.org/abs/2302.00744](http://arxiv.org/abs/2302.00744)

    本文提出了DSL组合的一个形式化代数框架，使用代数结构来模拟元语言，实现DSL抽象的编写、组合和互操作性，并提供了一个验证管道来验证该框架的组合属性。

    

    本文讨论了使用代数结构来建模元语言，以便编写、组合DSL抽象并提供互操作性的形式化框架。该框架旨在验证元语言的组合属性。在本文中，我们讨论了该形式化框架的构建以及与我们团队在DARPA V-SPELLS项目中的工作的关系，我们还介绍了用于完成对V-SPELLS验证任务的管道。我们旨在在文章中对此验证管道进行概述。该管道可分为四个主要组件：第一个是在Coq中提供元语言的形式模型；第二个是在Coq中提供所选代数结构的规范；第三个是需要在Coq中实现特定实例的代数结构，并在Coq中给出证明，证明该实现是根据我们在第二个组件中规定的代数结构；

    We discuss a formal framework for using algebraic structures to model a meta-language that can write, compose, and provide interoperability between abstractions of DSLs. The purpose of this formal framework is to provide a verification of compositional properties of the meta-language. Throughout our paper we discuss the construction of this formal framework, as well its relation to our team's work on the DARPA V-SPELLS program via the pipeline we have developed for completing our verification tasking on V-SPELLS. We aim to give a broad overview of this verification pipeline in our paper. The pipeline can be split into four main components: the first is providing a formal model of the meta-language in Coq; the second is to give a specification in Coq of our chosen algebraic structures; third, we need to implement specific instances of our algebraic structures in Coq, as well as give a proof in Coq that this implementation is an algebraic structure according to our specification in the s
    
[^45]: 极大预训练语言模型是否能够在很少的样例下学习故事创作？

    Can Very Large Pretrained Language Models Learn Storytelling With A Few Examples?. (arXiv:2301.09790v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.09790](http://arxiv.org/abs/2301.09790)

    本文探讨了使用极大预训练语言模型（VLPLMs）创作故事的可能性，并通过与SOTA模型在不同数据集上的比较，证明VLPLMs生成的故事质量更高，并展示一定程度上可与人类作者相抗衡，尽管初步调查揭示了它们倾向于“抄袭”真实的故事。

    

    尽管预训练语言模型可以生成语法通顺的句子用于自动生成故事，但是它们难以生成连贯、有意义和有趣的故事。当前最先进的故事生成模型通过探索更高级的特征，例如情节或常识知识以提高生成故事的质量。使用极大预训练语言模型（VLPLMs）如GPT3的提示式学习已经已经在各种自然语言处理任务中表现出惊人的性能。本文提出了一项广泛的研究，使用自动和人类评估来比较VLPLMs与那些在风格、语言和长度等方面不同的SOTA模型在三个不同数据集上的故事生成能力。我们的研究结果表明VLPLMs生成的故事质量远远高于其他故事生成模型，并在一定程度上可以与人类作者相抗衡，尽管初步调查也揭示了它们倾向于“抄袭”真实的故事。

    While pre-trained language models can generate individually fluent sentences for automatic story generation, they struggle to generate stories that are coherent, sensible and interesting. Current state-of-the-art (SOTA) story generation models explore using higher-level features such as plots or commonsense knowledge to improve the quality of generated stories. Prompt-based learning using very large pre-trained language models (VLPLMs) such as GPT3 has demonstrated impressive performance even across various NLP tasks. In this paper, we present an extensive study using automatic and human evaluation to compare the story generation capability of VLPLMs to those SOTA models in three different datasets where stories differ in style, register and length. Our results show that VLPLMs generate much higher quality stories than other story generation models, and to a certain extent rival human authors, although preliminary investigation also reveals that they tend to ``plagiarise'' real stories
    
[^46]: 用于流式序列标记的高效编码器。

    Efficient Encoders for Streaming Sequence Tagging. (arXiv:2301.09244v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.09244](http://arxiv.org/abs/2301.09244)

    本研究提出了一种名为HEAR的混合编码器与自适应重启（HEAR），解决了在流式输入中使用双向编码器的不必要浮点操作和不必要标签翻转的问题，同时提高了流式输入上的标记性能。

    

    在增量流输入（例如转录语音）中，为了对流式序列标记应用最先进的双向编码器，需要为每个新标记从头开始对每个标记进行编码。先前计算的不可重用性导致了更高数量的浮点操作（或FLOP）和更高数量的不必要标签翻转。增加的FLOP会导致更高的挂钟时间，而增加的标签翻转会导致流式性能更差。在这项工作中，我们提出了一种名为HEAR的混合编码器与自适应重启（HEAR），它解决了这些问题，同时保持了双向编码器在离线（或完整）输入上的表现，同时提高了流输入（或不完整）上的性能。HEAR具有混合单向 - 双向编码器架构来执行序列标记，以及自适应重启模块（ARM）以有选择地引导编码器的双向部分的重新启动。在四个序列标记数据集上，我们表明，HEAR显著地提高了流式表现，同时保持了离线性能。

    A naive application of state-of-the-art bidirectional encoders for streaming sequence tagging would require encoding each token from scratch for each new token in an incremental streaming input (like transcribed speech). The lack of re-usability of previous computation leads to a higher number of Floating Point Operations (or FLOPs) and higher number of unnecessary label flips. Increased FLOPs consequently lead to higher wall-clock time and increased label flipping leads to poorer streaming performance. In this work, we present a Hybrid Encoder with Adaptive Restart (HEAR) that addresses these issues while maintaining the performance of bidirectional encoders over the offline (or complete) inputs while improving performance on streaming (or incomplete) inputs. HEAR has a Hybrid unidirectional-bidirectional encoder architecture to perform sequence tagging, along with an Adaptive Restart Module (ARM) to selectively guide the restart of bidirectional portion of the encoder. Across four se
    
[^47]: 学习利用时间结构进行生物医学视觉语言处理

    Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing. (arXiv:2301.04558v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.04558](http://arxiv.org/abs/2301.04558)

    该论文提出了一种自监督学习的方法，利用时间内容丰富了生物医学视觉语言处理。该方法考虑了之前的图像和报告，使用了CNN-Transformer混合多图像编码器，实现了在单个和多图像设置中的最先进性能。

    

    视觉语言处理中的自监督学习利用了成像和文本模态之间的语义对齐。生物医学VLP的先前工作大多依赖于单个图像和报告对的对齐，即使临床记录通常会涉及以前的图像。这不仅引入了模态之间差劲的对齐，而且错过了利用数据中现有时间内容的丰富自监督的机会。在这项工作中，我们在训练和微调期间明确考虑了之前的图像和报告（如果有）。我们的方法名为BioViL-T，使用了CNN-Transformer混合多图像编码器，与文本模型一起联合训练。它被设计成适用于出现的挑战，如姿态变化和缺失的时间内输入图像。得到的模型在单个和多图像设置中均表现优异，在分类、短语定位和报告可视化三个downstream任务上均实现了最先进的性能。

    Self-supervised learning in vision-language processing exploits semantic alignment between imaging and text modalities. Prior work in biomedical VLP has mostly relied on the alignment of single image and report pairs even though clinical notes commonly refer to prior images. This does not only introduce poor alignment between the modalities but also a missed opportunity to exploit rich self-supervision through existing temporal content in the data. In this work, we explicitly account for prior images and reports when available during both training and fine-tuning. Our approach, named BioViL-T, uses a CNN-Transformer hybrid multi-image encoder trained jointly with a text model. It is designed to be versatile to arising challenges such as pose variations and missing input images across time. The resulting model excels on downstream tasks both in single- and multi-image setups, achieving state-of-the-art performance on (I) progression classification, (II) phrase grounding, and (III) repor
    
[^48]: 自然语言处理中人类导向的公平分类

    Human-Guided Fair Classification for Natural Language Processing. (arXiv:2212.10154v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10154](http://arxiv.org/abs/2212.10154)

    本文提出了一种新方法，通过自动生成表达丰富的候选句子对并结合群众外包的成对人类判断，训练一个映射语义相似性到统计代理的个体公平性的模型，用于弥合人类直觉和公平分类规范之间的差距。该方法在表现能力、与人类直觉的一致性和两个基准数据集的分类准确性方面优于先前的方法。

    

    文本分类器在简历筛选和内容审核等高风险任务中具有广泛的应用。这些分类器必须是公平的，并通过对敏感属性（如性别或种族）的扰动不变来避免歧视性决策。然而，人类对这些扰动的直觉与捕捉它们的形式相似度规范之间存在差距。尽管现有研究已经开始解决这个问题，但当前的方法基于硬编码单词替换，导致规范的表达能力有限或者无法充分地与人类直觉相一致（例如，在不对称的反事实情况下）。本研究提出了新方法来弥合这一差距，发现具有表现力和直觉公平规范。我们展示了如何利用无监督式转换和GPT-3的零-shot能力自动生成语义上类似但在敏感属性上有所不同的表达丰富的候选句子对。然后，我们采用群众外包获得这些候选人的成对人类判断，并使用它们来训练模型，将语义相似性映射到统计代理的个体公平性。我们的实验表明，我们的方法在表现能力、与人类直觉的一致性和两个基准数据集的分类准确性方面优于先前的方法。

    Text classifiers have promising applications in high-stake tasks such as resume screening and content moderation. These classifiers must be fair and avoid discriminatory decisions by being invariant to perturbations of sensitive attributes such as gender or ethnicity. However, there is a gap between human intuition about these perturbations and the formal similarity specifications capturing them. While existing research has started to address this gap, current methods are based on hardcoded word replacements, resulting in specifications with limited expressivity or ones that fail to fully align with human intuition (e.g., in cases of asymmetric counterfactuals). This work proposes novel methods for bridging this gap by discovering expressive and intuitive individual fairness specifications. We show how to leverage unsupervised style transfer and GPT-3's zero-shot capabilities to automatically generate expressive candidate pairs of semantically similar sentences that differ along sensit
    
[^49]: LUNA：使用数字插件和预训练的Transformers实现语言理解

    LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training. (arXiv:2212.02691v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.02691](http://arxiv.org/abs/2212.02691)

    LUNA框架通过数字插件和预训练的方式，解决了当前基于transformer的语言模型无法很好理解数字的问题。

    

    Transformers在NLP任务中被广泛使用。然而，目前利用transformers理解语言的方法存在一个弱点：数字理解。在某些场景下，数字经常出现，特别是在半结构化数据（如表格）中。但是利用基于transformer的语言模型进行数字任务的当前方法会丢失一些数字信息，例如将数字分解为子词标记等，导致许多与数字相关的错误。在本文中，我们提出了LUNA框架，可以显著提高基于transformer的语言模型的数字推理和计算能力。LUNA使用NumTok和NumBed的数字插件将每个数字作为一个整体来表示输入。通过数字预训练，包括回归损失和模型蒸馏，LUNA弥合了数字和词汇嵌入之间的差距。据我们所知，这是第一个明确将数字能力注入语言模型中使用数字插件和预训练的工作。

    Transformers are widely used in NLP tasks. However, current approaches to leveraging transformers to understand language expose one weak spot: Number understanding. In some scenarios, numbers frequently occur, especially in semi-structured data like tables. But current approaches to rich-number tasks with transformer-based language models abandon or lose some of the numeracy information - e.g., breaking numbers into sub-word tokens - which leads to many number-related errors. In this paper, we propose the LUNA framework which improves the numerical reasoning and calculation capabilities of transformer-based language models. With the number plugin of NumTok and NumBed, LUNA represents each number as a whole to model input. With number pre-training, including regression loss and model distillation, LUNA bridges the gap between number and vocabulary embeddings. To the best of our knowledge, this is the first work that explicitly injects numeracy capability into language models using Numbe
    
[^50]: 基于自动生成语言模型的自动机表示任务知识

    Automaton-Based Representations of Task Knowledge from Generative Language Models. (arXiv:2212.01944v3 [cs.FL] UPDATED)

    [http://arxiv.org/abs/2212.01944](http://arxiv.org/abs/2212.01944)

    提出了一个算法GLM2FSA，能够自动从任务目标的简短自然语言描述中提取任务知识并构建一个编码高层次任务知识的有限状态自动机，构建的自动机可以被正式验证。

    

    基于自动机的任务知识表示在控制和规划序列决策问题中扮演着重要角色。然而，获取构建此类自动机所需的高层次任务知识通常很困难。同时，大规模自动生成语言模型可以自动生成相关任务知识。然而，自动生成语言模型的文本输出不能正式验证或用于顺序决策。我们提出了一个名为GLM2FSA的新算法，它从任务目标的简短自然语言描述中构建一个编码高层次任务知识的有限状态自动机（FSA）。GLM2FSA首先向GLM发送查询以提取文本形式的任务知识，然后它建立一个FSA来表示这种基于文本的知识。因此，所提出的算法填补了自然语言任务描述和自动机表示之间的差距，构建的FSA可以针对用户定义的规格进行正式验证。

    Automaton-based representations of task knowledge play an important role in control and planning for sequential decision-making problems. However, obtaining the high-level task knowledge required to build such automata is often difficult. Meanwhile, large-scale generative language models (GLMs) can automatically generate relevant task knowledge. However, the textual outputs from GLMs cannot be formally verified or used for sequential decision-making. We propose a novel algorithm named GLM2FSA, which constructs a finite state automaton (FSA) encoding high-level task knowledge from a brief natural-language description of the task goal. GLM2FSA first sends queries to a GLM to extract task knowledge in textual form, and then it builds an FSA to represent this text-based knowledge. The proposed algorithm thus fills the gap between natural-language task descriptions and automaton-based representations, and the constructed FSA can be formally verified against user-defined specifications. We a
    
[^51]: DreamArtist: 通过对比prompt-tuning实现可控的一次性文本到图像生成

    DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via Contrastive Prompt-Tuning. (arXiv:2211.11337v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.11337](http://arxiv.org/abs/2211.11337)

    DreamArtist采用正负prompt-tuning学习策略来生成可控的一次性文本到图像，并解决了传统方法可能会导致模型过度拟合的问题。

    

    大规模文本到图像生成模型通过文本指导合成高质量、特征丰富、高分辨率的图像取得了可观的进展。然而，这些模型在处理新概念（例如新风格、物体实体等）时常常面临困难。尽管最近的尝试采用微调或prompt-tuning策略来教授预先训练的扩散模型从参考图像集中学习新概念，但它们存在过度拟合给定的参考图像，特别是在单次应用中，这对于保持生成可控性并产生多样化、高质量的图像是有害的。为了解决这个挑战，我们提出了一种简单而有效的方法DreamArtist，它采用了正负prompt-tuning学习策略。具体而言，DreamArtist结合了正负嵌入并联合训练它们。正嵌入积极地捕捉参考图像的显着特征来驱动图像生成，而负嵌入则强制模型生成多样性图像以降低过度拟合风险。

    Large-scale text-to-image generation models have achieved remarkable progress in synthesizing high-quality, feature-rich images with high resolution guided by texts. However, these models often struggle with novel concepts, eg, new styles, object entities, etc. Although recent attempts have employed fine-tuning or prompt-tuning strategies to teach the pre-trained diffusion model novel concepts from a reference image set,they have the drawback of overfitting to the given reference images, particularly in one-shot applications, which is harmful to generate diverse and high-quality images while maintaining generation controllability.  To tackle this challenge, we present a simple yet effective method called DreamArtist, which employs a positive-negative prompt-tuning learning strategy. Specifically, DreamArtist incorporates both positive and negative embeddings and jointly trains them. The positive embedding aggressively captures the salient characteristics of the reference image to drive
    
[^52]: 语言模型解码作为似然度-效用对齐

    Language Model Decoding as Likelihood-Utility Alignment. (arXiv:2210.07228v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.07228](http://arxiv.org/abs/2210.07228)

    解码算法的选择需要考虑模型似然度和任务效用的匹配度问题，通过分类不匹配缓解策略（MMS）的视角，可以提高解码算法的通用性

    

    成功的语言生成流程中的关键组件是解码算法。然而，应该指导选择解码算法的一般原则仍不清楚。以前的研究仅在狭窄的情况下比较解码算法，他们的发现不能推广到跨任务。我们认为模型的似然和任务特定效用的不匹配是理解解码算法有效性的关键因素。为了结构化讨论，我们引入一种不匹配缓解策略（MMS）的分类法，提供解码作为对齐工具的统一视角。这个MMS分类法根据解码算法对似然度-效用不匹配的隐含假设对其进行分组，产生关于它们跨任务适用性的一般性声明。具体而言，通过分析跨多个任务的预测的似然度和效用之间的相关性，我们提供了实证证据

    A critical component of a successful language generation pipeline is the decoding algorithm. However, the general principles that should guide the choice of a decoding algorithm remain unclear. Previous works only compare decoding algorithms in narrow scenarios, and their findings do not generalize across tasks. We argue that the misalignment between the model's likelihood and the task-specific notion of utility is the key factor to understanding the effectiveness of decoding algorithms. To structure the discussion, we introduce a taxonomy of misalignment mitigation strategies (MMSs), providing a unifying view of decoding as a tool for alignment. The MMS taxonomy groups decoding algorithms based on their implicit assumptions about likelihood--utility misalignment, yielding general statements about their applicability across tasks. Specifically, by analyzing the correlation between the likelihood and the utility of predictions across a diverse set of tasks, we provide empirical evidence
    
[^53]: 自然语言处理方法识别在临床记录中高风险的癌症患者

    Natural Language Processing Methods to Identify Oncology Patients at High Risk for Acute Care with Clinical Notes. (arXiv:2209.13860v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.13860](http://arxiv.org/abs/2209.13860)

    本文研究了利用自然语言处理方法识别化疗后癌症患者急救护理风险的问题，与以往使用结构化卫生数据进行预测的模型相比较，结果表明两者差异不大，从而说明了在临床应用中采用语言模型的可行性以及不同患者群体风险偏差的存在。

    

    临床记录是健康记录中的重要组成部分。本篇论文评估了如何利用自然语言处理（NLP）识别化疗开始后癌症患者急救护理（ACU）的风险。使用结构化卫生数据（SHD）进行风险预测已成为标准，但使用自由文本格式进行预测更为复杂。本文探讨了使用自由文本笔记而非SHD进行ACU预测的方法。深度学习模型与手动构建的语言特征进行了比较。结果表明，SHD模型略胜于NLP模型； SHD的l1-罚项逻辑回归模型实现的C统计量为0.748（95％-CI：0.735，0.762），而具有语言特征的相同模型实现的C统计量为0.730（95％-CI：0.717，0.745），而基于变压器的模型实现的C统计量为0.702（95％-CI：0.688，0.717）。本文展示了语言模型在临床应用中的应用，并强调不同患者群体的风险偏差是不同的，即使只使用自由文本数据也是如此。

    Clinical notes are an essential component of a health record. This paper evaluates how natural language processing (NLP) can be used to identify the risk of acute care use (ACU) in oncology patients, once chemotherapy starts. Risk prediction using structured health data (SHD) is now standard, but predictions using free-text formats are complex. This paper explores the use of free-text notes for the prediction of ACU instead of SHD. Deep Learning models were compared to manually engineered language features. Results show that SHD models minimally outperform NLP models; an l1-penalised logistic regression with SHD achieved a C-statistic of 0.748 (95%-CI: 0.735, 0.762), while the same model with language features achieved 0.730 (95%-CI: 0.717, 0.745) and a transformer-based model achieved 0.702 (95%-CI: 0.688, 0.717). This paper shows how language models can be used in clinical applications and underlines how risk bias is different for diverse patient groups, even using only free-text dat
    
[^54]: 解释链：生成隐含仇恨言论更高质量自然语言解释的新提示方法

    Chain of Explanation: New Prompting Method to Generate Higher Quality Natural Language Explanation for Implicit Hate Speech. (arXiv:2209.04889v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.04889](http://arxiv.org/abs/2209.04889)

    本研究提出了解释链提示方法，可以生成高质量的自然语言解释，帮助理解隐含仇恨言论。该方法通过提供准确的目标信息，将语言模型生成的BLUE分数从44.0提高到了62.3，已通过多种度量和人工注释的质量评估。

    

    最近的研究利用高级生成语言模型生成自然语言解释，解释为什么某些文本可能是具有仇恨色彩的。我们提出了解释链 (CoE) 提示方法， 使用启发式词语和目标群体，为隐含仇恨言论生成高质量的自然语言解释。通过提供准确的目标信息，我们将 NLE 生成的 BLUE 分数从 44.0 提高到了 62.3。我们还利用多种自动度量指标和人工注释的信息量和清晰度评分评估了生成的 NLE 的质量。

    Recent studies have exploited advanced generative language models to generate Natural Language Explanations (NLE) for why a certain text could be hateful. We propose the Chain of Explanation (CoE) Prompting method, using the heuristic words and target group, to generate high-quality NLE for implicit hate speech. We improved the BLUE score from 44.0 to 62.3 for NLE generation by providing accurate target information. We then evaluate the quality of generated NLE using various automatic metrics and human annotations of informativeness and clarity scores.
    
[^55]: 在文本数据上比较特征重要性和规则提取的可解释性

    Comparing Feature Importance and Rule Extraction for Interpretability on Text Data. (arXiv:2207.01420v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2207.01420](http://arxiv.org/abs/2207.01420)

    本文比较了文本数据上两类方法：计算每个特征的重要性分数和提取简单逻辑规则，发现在相同模型下产生的解释也不同。我们提出了一种比较解释差异的方法。

    

    复杂的机器学习算法在涉及文本数据的关键任务中越来越常见，这导致了可解释性方法的发展。在局部方法中，出现了两种族群：一种计算每个特征的重要性分数，另一种则提取简单的逻辑规则。在本文中，我们展示了使用不同方法可以导致意外不同的解释，即使应用于那些我们预计会有定性巧合的简单模型上。为了量化这种影响，我们提出一种比较不同方法产生的解释的新方法。

    Complex machine learning algorithms are used more and more often in critical tasks involving text data, leading to the development of interpretability methods. Among local methods, two families have emerged: those computing importance scores for each feature and those extracting simple logical rules. In this paper we show that using different methods can lead to unexpectedly different explanations, even when applied to simple models for which we would expect qualitative coincidence. To quantify this effect, we propose a new approach to compare explanations produced by different methods.
    
[^56]: 成人如何理解幼儿的语言

    How Adults Understand What Young Children Say. (arXiv:2206.07807v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.07807](http://arxiv.org/abs/2206.07807)

    成人如何理解幼儿的语言仍是一个复杂的问题，研究发现成人能够理解幼儿语言，是因为他们有关于幼儿试图传达信息的特定先验期望，并揭示了成人在早期沟通中的重要作用。

    

    幼儿期的语言常常与成人的语言不太相似，但是父母和其他照顾者能够理解这些语言并据此做出反应。本文研究了成人作为听众如何反映他们对儿童试图传达什么以及儿童发音的复杂信念。使用贝叶斯框架建模口语识别，我们发现仅当计算模型包含关于儿童想要传达的信息的强烈的、特定于语境的先验期望时，它们才能复制成人对幼儿语言的解释。这指出了成人认知过程在支持早期沟通中发挥的至关重要的作用，并揭示了儿童如何在仅有初步的成人语言理解时，积极引导成人代表自己采取行动的能力。我们讨论了成人强大的听力能力对于婴幼儿发展理论的广泛影响。

    Children's early speech often bears little resemblance to that of adults, and yet parents and other caregivers are able to interpret that speech and react accordingly. Here we investigate how these adult inferences as listeners reflect sophisticated beliefs about what children are trying to communicate, as well as how children are likely to pronounce words. Using a Bayesian framework for modeling spoken word recognition, we find that computational models can replicate adult interpretations of children's speech only when they include strong, context-specific prior expectations about the messages that children will want to communicate. This points to a critical role of adult cognitive processes in supporting early communication and reveals how children can actively prompt adults to take actions on their behalf even when they have only a nascent understanding of the adult language. We discuss the wide-ranging implications of the powerful listening capabilities of adults for theories of fi
    
[^57]: 一片文字海：针对文本数据的 Anchors 深入分析

    A Sea of Words: An In-Depth Analysis of Anchors for Text Data. (arXiv:2205.13789v2 [stat.ML] CROSS LISTED)

    [http://arxiv.org/abs/2205.13789](http://arxiv.org/abs/2205.13789)

    Anchors 是一种后处理的规则性可解释性方法，它可以通过凸显出一个小组词语（锚点）来强调模型的决策，我们首次对 Anchors 进行了理论分析，考虑到寻找最佳锚点是详尽的，并通过 TF-IDF 向量化步骤以及模型层次的显式结果，探究其在不同类别模型中的行为特征。我们还发现，在神经网络中，最高偏导数所对应的词汇可以重新加权用作 Anchors 词汇。

    

    Anchors 是一种基于后处理的基于规则的可解释性方法，该方法旨在解释模型决策并强调一小组词语（锚点），这些词语存在于文档中时，模型输出类似。本文首次对 Anchors 进行了理论分析，考虑到寻找最佳锚点是详尽的。我们将文本分类的算法形式化后，结合不同类别模型的显式结果，探究了 Anchors 的行为特征。我们分别覆盖了基本 if-then 规则和线性分类器这两种模型。我们还利用这项分析，洞见任何可微分分类器的 Anchors 行为特征。对于神经网络，我们经验性地展示了模型对输入的最高偏导数所对应的词语，通过反向文件重新加权，可以作为 Anchors 词语。

    Anchors (Ribeiro et al., 2018) is a post-hoc, rule-based interpretability method. For text data, it proposes to explain a decision by highlighting a small set of words (an anchor) such that the model to explain has similar outputs when they are present in a document. In this paper, we present the first theoretical analysis of Anchors, considering that the search for the best anchor is exhaustive. After formalizing the algorithm for text classification, we present explicit results on different classes of models when the vectorization step is TF-IDF, and words are replaced by a fixed out-of-dictionary token when removed. Our inquiry covers models such as elementary if-then rules and linear classifiers. We then leverage this analysis to gain insights on the behavior of Anchors for any differentiable classifiers. For neural networks, we empirically show that the words corresponding to the highest partial derivatives of the model with respect to the input, reweighted by the inverse document
    
[^58]: 跨语言性别一致性影响的跨语言差异：基于元分析的证据。

    Cross-linguistic differences in gender congruency effects: Evidence from meta-analyses. (arXiv:2109.03490v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2109.03490](http://arxiv.org/abs/2109.03490)

    通过元分析证实，日尔曼语言和斯拉夫语言中的性别一致效应比罗曼语言更加稳健，但效应大小适中，并且存在研究间的变异性。

    

    有人认为，单词制作的准备顺序取决于说话者所用的语言。当德语或荷兰语的说话者制作小猫的翻译时，会在制作过程的较早阶段选择标记性别的限定词。而法语或意大利语的说话者会将限定词或形容词的编码推迟到可用名词的音韵形式之后。因此，即使单词的顺序相同（例如在德语中是“die kleine Katze”，在法语中是“le petit chat”），它们的计划顺序不同，可能需要在制作开始前进行不同程度的提前计划。这种早期和晚期选择语言之间的区别是为了解释观察到的指出德日等语言但不包括罗曼语系语言中的图片命名速度较慢的性别干扰效应。进行元分析以直接测试这一跨语言性假说。分析表明，在图片命名反应时间中，性别一致效应在日尔曼语言和斯拉夫语言中的稳健性比罗曼语系语言更高。但是，效应大小适中，并且分析还揭示了效应大小在研究之间存在相当的变异性。这些结果指出了性别标记影响语言制作的跨语言差异，同时也强调了考虑影响性别一致性效应大小的方法和语境因素的重要性。

    It has been proposed that the order in which words are prepared for production depends on the speaker's language. When producing the translation equivalent of the small cat, speakers of German or Dutch select the gender-marked determiner at a relatively early stage of production. Speakers of French or Italian postpone the encoding of a determiner or adjective until the phonological form of the noun is available. Hence, even though the words are produced in the same order (e.g., die kleine Katze in German, le petit chat in French), they are not planned in the same order and might require different amounts of advanced planning prior to production onset. This distinction between early and late selection languages was proposed to account for the observation that speakers of Germanic and Slavic languages, but not of Romance languages, are slower to name pictures in the context of a distractor word of a different gender. Meta-analyses are conducted to provide the first direct test of this cr
    
[^59]: 使用分布感知词嵌入的命名实体识别性能的实证研究。

    Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding. (arXiv:2109.01636v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2109.01636](http://arxiv.org/abs/2109.01636)

    研究开发了一种分布感知词嵌入，并实施了三种不同的方法来利用NER框架中的分布信息，实验表明将词的特异性融入NER方法可提高NER的性能。

    

    随着深度学习技术的快速发展，命名实体识别（NER）在信息提取任务中变得越来越重要。NER任务面临的最大困难是即使在NE类型和文档不熟悉的情况下仍然需要保持可检测性。意识到特定性信息可能包含单词的潜在含义并生成词嵌入的语义相关特征，我们开发了一个分布感知词嵌入，并实施了三种不同的方法来利用NER框架中的分布信息。结果表明，如果将词的特异性融入现有的NER方法中，NER的性能将得到提高。

    With the fast development of Deep Learning techniques, Named Entity Recognition (NER) is becoming more and more important in the information extraction task. The greatest difficulty that the NER task faces is to keep the detectability even when types of NE and documents are unfamiliar. Realizing that the specificity information may contain potential meanings of a word and generate semantic-related features for word embedding, we develop a distribution-aware word embedding and implement three different methods to make use of the distribution information in a NER framework. And the result shows that the performance of NER will be improved if the word specificity is incorporated into existing NER methods.
    
[^60]: UNIQORN：统一的RDF知识图谱与自然语言文本问答系统

    UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v5 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2108.08614](http://arxiv.org/abs/2108.08614)

    本文提出了一个名为UNIQORN的问答系统，它能够无缝地处理RDF数据和文本，使用fine-tuned BERT模型为问题构建上下文图，并使用图算法确定与问题相关的子图来回答问题。

    

    问题回答在知识图谱和其他RDF数据上已经取得了巨大的进展，许多优秀的系统可以为自然语言问题或电报查询提供清晰的答案。其中一些系统将文本源作为附加证据纳入回答过程，但不能计算仅存在于文本中的答案。相反，IR和NLP社区的系统已经解决了有关文本的QA问题，但是这些系统几乎不利用语义数据和知识。本文提出了第一个可以无缝操作混合RDF数据集和文本语料库或单个来源的复杂问题的系统，在统一框架中进行操作。我们的方法称为UNIQORN，通过使用经过精细调整的BERT模型从RDF数据和/或文本语料库中检索与问题相关的证据来动态构建上下文图。结果图通常非常丰富但高度嘈杂。UNIQORN通过用于组Steiner树的图算法来处理这个输入，从而确定与问题相关的子图，进而回答问题。

    Question answering over knowledge graphs and other RDF data has been greatly advanced, with a number of good systems providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, systems from the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents the first system for complex questions that can seamlessly operate over a mixture of RDF datasets and text corpora, or individual sources, in a unified framework. Our method, called UNIQORN, builds a context graph on-the-fly, by retrieving question-relevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph is typically rich but highly noisy. UNIQORN copes with this input by a graph algorithm for Group Steiner Trees, that identifi
    

