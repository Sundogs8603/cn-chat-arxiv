# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Survey on Large Language Models from Concept to Implementation](https://arxiv.org/abs/2403.18969) | Transformer模型在改革传统任务和推进跨行业研究和开发中产生革命性影响。 |
| [^2] | [Detoxifying Large Language Models via Knowledge Editing](https://arxiv.org/abs/2403.14472) | 本文研究了使用知识编辑技术对大型语言模型进行去毒化，在构建了SafeEdit基准的基础上，提出了一种简单而有效的方法 DINM，可以通过少量调整步骤减少模型的毒性，同时对各种去毒方法的内部机制进行了深入分析。 |
| [^3] | [SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression](https://arxiv.org/abs/2403.07378) | SVD-LLM是一种新的基于SVD的LLM压缩方法，通过截断感知数据白化策略和逐层闭式模型参数更新策略，解决了现有方法的限制，实现了直接映射奇异值和压缩损失之间的关系。 |
| [^4] | [AutoEval Done Right: Using Synthetic Data for Model Evaluation](https://arxiv.org/abs/2403.07008) | 提出了用合成数据进行模型评估的方法，通过高效和统计上合理的算法，在GPT-4实验中有效的人工标记样本大小增加了50%。 |
| [^5] | [Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution](https://arxiv.org/abs/2403.03121) | 大型语言模型中存在性别化情绪归因，反映了社会刻板印象。 |
| [^6] | [LLMs for Targeted Sentiment in News Headlines: Exploring Different Levels of Prompt Prescriptiveness](https://arxiv.org/abs/2403.00418) | 提出了探索LLMs在新闻标题有针对性情感分析中不同级别提示规范性的方法，以提高其性能。 |
| [^7] | [Do Large Language Models Mirror Cognitive Language Processing?](https://arxiv.org/abs/2402.18023) | 本文提出了一种新颖方法，通过将大型语言模型（LLMs）的表示与人类认知信号联系起来，评估LLMs模拟认知语言处理的效果。 |
| [^8] | [DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem](https://arxiv.org/abs/2402.16159) | 提出了一种为开源软件系统量身定制的新颖命名实体识别技术，通过远程监督注释过程、语言启发、查找表、外部知识源和主动学习方法来提高模型性能，有效缓解了成本和专家标注人员稀缺问题，并在关系抽取下游任务中显著优于LLMs。 |
| [^9] | [FuseChat: Knowledge Fusion of Chat Models](https://arxiv.org/abs/2402.16107) | FuseChat通过知识融合将多个对话模型的集体知识转移到目标语言模型中，避免了昂贵的预训练成本。 |
| [^10] | [CriticBench: Benchmarking LLMs for Critique-Correct Reasoning](https://arxiv.org/abs/2402.14809) | CriticBench是一个综合基准测试，旨在评估LLMs在批判和纠正推理方面的能力，发现批判性训练显著提升性能，逻辑任务更易于修正。 |
| [^11] | [Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning](https://arxiv.org/abs/2402.13669) | SDFT是一种通过用模型本身生成的精简数据集来桥接分布差距的新方法，在缓解灾难性遗忘的同时，在下游任务上实现了与普通微调相媲美甚至更优越的性能 |
| [^12] | [ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance](https://arxiv.org/abs/2402.13448) | 本研究提出了一种在急诊科中减少等待时间的诊断辅助方法，利用人工智能系统帮助医生进行快速准确的诊断，并开发了ED-Copilot系统来推荐实验室检测并进行诊断预测。 |
| [^13] | [GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick](https://arxiv.org/abs/2402.12948) | 通过开发 GumbelSoft 水印，我们提出了一种能够在高多样性环境中增强生成文本多样性的解决方案，相较于其他方案表现更为优秀。 |
| [^14] | [In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness](https://arxiv.org/abs/2402.11639) | 本文研究了在上下文学习框架中，Softmax注意力在适应预训练任务背景时的作用，发现注意力单元学会与Lipschitzness降低和标签噪声增加相关的窗口调整，以及在低维、线性问题上学会在推理前进行适当空间的投影。 |
| [^15] | [Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark](https://arxiv.org/abs/2402.11592) | 本研究提出了一种不使用反向传播的零阶优化方法，用于降低LLM微调中的内存成本，通过全面的基准研究扩展了对不同的ZO优化技术的探索。 |
| [^16] | [Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts](https://arxiv.org/abs/2402.10958) | 提出了相对优先权优化（RPO）方法，通过区分来自相同和相关提示的更受青睐的响应和更不受青睐的响应，扩展了模型的学习能力。 |
| [^17] | [EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models](https://arxiv.org/abs/2402.10866) | EcoRank是一个两层管线，通过联合优化有关预算分配和LLM API的决策来实现文本重新排序，在实验中表现优于其他预算感知方法。 |
| [^18] | [DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows](https://arxiv.org/abs/2402.10379) | DataDreamer是一种用于合成数据生成和可复现LLM工作流程的开源Python库，有助于研究人员实现强大的LLM工作流，提倡开放科学和可重现性。 |
| [^19] | [Rationality Report Cards: Assessing the Economic Rationality of Large Language Models](https://arxiv.org/abs/2402.09552) | 本文在评估大型语言模型的经济合理性方面提出了一种方法，通过量化评分模型在各个要素上的表现并结合用户提供的评分标准，生成一份"理性报告卡"，以确定代理人是否足够可靠。 |
| [^20] | [Premise Order Matters in Reasoning with Large Language Models](https://arxiv.org/abs/2402.08939) | 对大型语言模型（LLMs）进行推理任务时，论据的顺序非常重要，尤其是在演绎推理任务中，按照提示的真实证明顺序呈现论据可以显著提高模型的准确性。 |
| [^21] | [BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models](https://arxiv.org/abs/2402.08219) | BBox-Adapter是一种适用于黑盒大型语言模型的轻量级适配器，通过区分目标和源域数据，并采用排名式噪音对比估计（NCE）损失和在线适应机制，实现了在透明、隐私和成本方面的有效适应。 |
| [^22] | [TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection](https://arxiv.org/abs/2402.07776) | TELLER是一个可信的假新闻检测框架，通过集成认知和决策系统，提供了解释性、可推广性和可控制性。认知系统利用人类专业知识生成逻辑谓词，指导大型语言模型生成可读的逻辑原子。决策系统推导可推广的逻辑规则来聚合这些原子，实现真实和虚假新闻的识别。 |
| [^23] | [A Closer Look at the Limitations of Instruction Tuning](https://arxiv.org/abs/2402.05119) | 本文通过实验和分析揭示了指令调整的多个局限性，包括无法增强LLM的知识和技能、从具有知识来源的数据集复制回应模式导致质量下降、全参数微调增加了错误生成的情况。 |
| [^24] | [InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory](https://arxiv.org/abs/2402.04617) | InfLLM是一种无需训练的基于记忆的方法，用于揭示LLMs处理超长序列的内在能力。它通过存储远距离上下文和高效的注意计算机制，允许LLMs有效处理具有流式输入的长序列。 |
| [^25] | [REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR](https://arxiv.org/abs/2402.03988) | 本文提出了REBORN，在无监督语音识别中使用基于强化学习的迭代训练来实现边界分割。通过交替训练分割模型和音素预测模型，实现了学习语音和文本之间的映射，解决了无监督情况下语音信号分段结构边界的挑战。 |
| [^26] | [Minds versus Machines: Rethinking Entailment Verification with Language Models](https://arxiv.org/abs/2402.03686) | 本文通过研究人类和大型语言模型在推理判断中的共性和差异，发现大型语言模型在复杂推理中具有优势，而人类在简单推理中表现出色。基于这些发现，引入了一个优化的Flan-T5模型，用于蕴含验证。 |
| [^27] | [Selecting Large Language Model to Fine-tune via Rectified Scaling Law](https://arxiv.org/abs/2402.02314) | 该论文研究了在资源受限的情况下如何选择合适的预训练语言模型进行微调的问题。通过引入修正的缩放定律和预学习数据大小的概念，作者提出了一种新颖的模型选择算法，可以选择接近最优的模型。 |
| [^28] | [History-Aware Conversational Dense Retrieval.](http://arxiv.org/abs/2401.16659) | 该论文提出了一种历史感知的对话式稠密检索系统，通过上下文去噪的查询重构以及根据历史轮次的实际影响自动挖掘监督信号改进了现有的对话式稠密检索方法。 |
| [^29] | [MM-LLMs: Recent Advances in MultiModal Large Language Models.](http://arxiv.org/abs/2401.13601) | 近年来，多模式大语言模型（MM-LLMs）通过成本效益高的训练策略取得了显著进展，扩展了现有的语言模型的多模输入和输出支持。本论文提供了一份综合调查报告，介绍了MM-LLMs的设计和训练方案，整理了现有的MM-LLMs及其性能，总结了关键训练方法，并探讨了未来的研究方向。 |
| [^30] | [MF-AED-AEC: Speech Emotion Recognition by Leveraging Multimodal Fusion, ASR Error Detection, and ASR Error Correction.](http://arxiv.org/abs/2401.13260) | 本文提出了一种名为MF-AED-AEC的方法，通过融合多模态、ASR错误检测和ASR错误修正，通过增强ASR文本的语义一致性来改进语音情感识别的性能。 |
| [^31] | [INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning.](http://arxiv.org/abs/2401.06532) | 本研究探索了指令调优的方法，以增强大型语言模型在信息检索任务中的能力，通过引入一个新的指令调优数据集INTERS，涵盖了21个IR任务，该方法显著提升了性能。 |
| [^32] | [UstanceBR: a multimodal language resource for stance prediction.](http://arxiv.org/abs/2312.06374) | UstanceBR是一个多模态语言资源，用于目标立场预测，包含巴西葡萄牙语Twitter领域的86.8k标记立场和发布者的网络信息。这个研究为未来的研究提供了初始基准结果。 |
| [^33] | [CausalCite: A Causal Formulation of Paper Citations.](http://arxiv.org/abs/2311.02790) | CausalCite是一种以因果推断为基础的论文引用公式化方法，通过对文本进行嵌入和相似样本的提取来评估论文的重要性，并在各个标准上展示了其有效性。 |
| [^34] | [BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B.](http://arxiv.org/abs/2311.00117) | 研究发现，公开发布模型权重使得安全微调无效，BadLlama项目以低成本成功移除了Llama 2-Chat 13B的安全微调并保留了其一般能力。 |
| [^35] | [Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review.](http://arxiv.org/abs/2310.14735) | 这篇论文解释了提示工程在释放大型语言模型能力方面的关键作用，探讨了不同的提示方法以及外部插件如何协助减少机器幻想，并指出了未来研究方向的重要性。 |
| [^36] | [Assessing Large Language Models on Climate Information.](http://arxiv.org/abs/2310.02932) | 本研究提出了一个基于科学传播原则的综合评估框架，评估了大规模语言模型在气候变化信息中的表现，能够在回答气候变化主题方面提供细粒度的分析。 |
| [^37] | [Segmentation-Free Streaming Machine Translation.](http://arxiv.org/abs/2309.14823) | 本研究提出了一种无需分割的流式机器翻译框架，通过延迟分割决策来提高翻译质量和延迟平衡，相比其他方法具有更好的效果。 |
| [^38] | [Is it Possible to Modify Text to a Target Readability Level? An Initial Investigation Using Zero-Shot Large Language Models.](http://arxiv.org/abs/2309.12551) | 这项研究旨在探讨将文本修改到目标可读性水平的可行性。研究提出了一项新颖的可读性控制文本修改任务，要求为每个输入文本生成多个不同目标可读性水平的版本，并引入了相应的度量标准。 |
| [^39] | [LongDocFACTScore: Evaluating the Factuality of Long Document Abstractive Summarisation.](http://arxiv.org/abs/2309.12455) | LongDocFACTScore是一种评估长文档生成摘要实证性的评估框架，可以解决传统自动评估度量标准无法评估长文档摘要事实一致性的问题。 |
| [^40] | [Improving Article Classification with Edge-Heterogeneous Graph Neural Networks.](http://arxiv.org/abs/2309.11341) | 本论文提出了一种使用边异构图神经网络改进文章分类的方法，通过加入高阶语义的节点特征生成，能够显著提高分类性能。 |
| [^41] | [Inference-time Re-ranker Relevance Feedback for Neural Information Retrieval.](http://arxiv.org/abs/2305.11744) | 本文提出了一种利用重排器提供推理时间反馈来改进检索的方法，可以显著提高低召回率@ K下的检索性能。 |
| [^42] | [SmartBook: AI-Assisted Situation Report Generation.](http://arxiv.org/abs/2303.14337) | SmartBook是一种AI辅助的情报报告生成工具，通过消耗大量新闻数据生成一个结构化的情况报告，其中包含多个假设（主张），并与事实依据建立丰富的关联。在Ukraine-Russia危机中，机器生成的报告以时间轴的形式结构化，显著减少了报告时间，并通过生成比人类同行更全面、准确和一致的报告来提高报告质量。 |
| [^43] | [GesGPT: Speech Gesture Synthesis With Text Parsing from GPT.](http://arxiv.org/abs/2303.13013) | GesGPT 是一种新的语音手势生成方法，它利用大型语言模型 GPT 的语义分析能力，从文本输入中提取与手势相关的信息，并使用手势库和集成模块生成意义丰富的语音手势。 |
| [^44] | [Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram.](http://arxiv.org/abs/2301.10856) | 本文研究了16个俄罗斯媒体机构和732个电报频道之间的互动，发现新闻媒体不仅通过电报传播现有的叙事，而且会从电报平台源材料，研究结果表明2.3％至26.7％的文章将主题归因于电报活动。 |

# 详细

[^1]: 从概念到实现的大型语言模型综述

    A Survey on Large Language Models from Concept to Implementation

    [https://arxiv.org/abs/2403.18969](https://arxiv.org/abs/2403.18969)

    Transformer模型在改革传统任务和推进跨行业研究和开发中产生革命性影响。

    

    最近基于Transformer架构构建的大型语言模型(LLMs)的发展极大拓宽了自然语言处理(NLP)应用的范围，超越了最初在聊天机器人技术中的应用。本文研究了这些模型的多方面应用，着重介绍了GPT系列。这项探索聚焦于人工智能(AI)驱动工具在改革传统编码和问题解决等任务上的革命性影响，同时在跨越不同行业的研究和开发中开辟新路径。从代码解释和图像描述到促进交互式系统的搭建和推进计算领域，Transformer模型体现了深度学习、数据分析和神经网络设计的协同作用。本综述深入探讨了Transformer模型的最新研究，突出了

    arXiv:2403.18969v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs), particularly those built on Transformer architectures, have significantly broadened the scope of natural language processing (NLP) applications, transcending their initial use in chatbot technology. This paper investigates the multifaceted applications of these models, with an emphasis on the GPT series. This exploration focuses on the transformative impact of artificial intelligence (AI) driven tools in revolutionizing traditional tasks like coding and problem-solving, while also paving new paths in research and development across diverse industries. From code interpretation and image captioning to facilitating the construction of interactive systems and advancing computational domains, Transformer models exemplify a synergy of deep learning, data analysis, and neural network design. This survey provides an in-depth look at the latest research in Transformer models, highlighting the
    
[^2]: 通过知识编辑实现对大型语言模型的去毒化

    Detoxifying Large Language Models via Knowledge Editing

    [https://arxiv.org/abs/2403.14472](https://arxiv.org/abs/2403.14472)

    本文研究了使用知识编辑技术对大型语言模型进行去毒化，在构建了SafeEdit基准的基础上，提出了一种简单而有效的方法 DINM，可以通过少量调整步骤减少模型的毒性，同时对各种去毒方法的内部机制进行了深入分析。

    

    本文研究了使用知识编辑技术来对大型语言模型（LLMs）进行去毒化。我们构建了一个名为SafeEdit的基准，涵盖了九种不安全类别，具有各种强大的攻击提示，并配备了全面的度量标准进行系统评估。我们进行了实验，比较了知识编辑方法与之前的基准线，结果表明知识编辑有潜力在对LLMs进行去毒化时，在对一般性能的影响相对有限。然后，我们提出了一个简单但有效的基准线，称为通过术中神经监测去毒化（DINM），通过仅一次实例的少量调整步骤减少LLMs的毒性。我们进一步对各种去毒方法的内部机制进行了深入分析，表明先前的方法如SFT和DPO可能仅抑制有毒参数的激活，而DINM则减轻有毒参数的毒性。

    arXiv:2403.14472v1 Announce Type: cross  Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to
    
[^3]: SVD-LLM: 针对大型语言模型压缩的截断感知奇异值分解

    SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression

    [https://arxiv.org/abs/2403.07378](https://arxiv.org/abs/2403.07378)

    SVD-LLM是一种新的基于SVD的LLM压缩方法，通过截断感知数据白化策略和逐层闭式模型参数更新策略，解决了现有方法的限制，实现了直接映射奇异值和压缩损失之间的关系。

    

    大型语言模型（LLMs）的进展受到其庞大尺寸的限制，这需要LLM压缩方法以实现实际部署。奇异值分解（SVD）为LLM压缩提供了一个有希望的解决方案。然而，现有的基于SVD的LLM压缩方法存在两个关键限制：截断较小的奇异值可能导致更高的压缩损失，并且在SVD截断后剩余模型参数的更新缺失。在这项工作中，我们提出了SVD-LLM，一种新的基于SVD的LLM压缩方法，解决了现有方法的限制。SVD-LLM采用了一种截断感知的数据白化策略，以确保奇异值和压缩损失之间的直接映射。此外，SVD-LLM采用一种逐层闭式模型参数更新策略，以弥补SVD截断引起的准确性降低。我们在总共11个数据集和七个m上评估了SVD-LLM。

    arXiv:2403.07378v1 Announce Type: new  Abstract: The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven m
    
[^4]: 自动评价正确: 使用合成数据进行模型评估

    AutoEval Done Right: Using Synthetic Data for Model Evaluation

    [https://arxiv.org/abs/2403.07008](https://arxiv.org/abs/2403.07008)

    提出了用合成数据进行模型评估的方法，通过高效和统计上合理的算法，在GPT-4实验中有效的人工标记样本大小增加了50%。

    

    机器学习模型的评估使用人工标记的验证数据可能既昂贵又耗时。可以使用AI标记的合成数据来减少此类目的人工注释数量，这一过程称为自动评估。我们提出了用于此目的的高效和统计上合理的算法，可以提高样本效率，同时保持不偏。这些算法在与GPT-4进行的实验中将有效的人工标记样本大小增加了高达50%。

    arXiv:2403.07008v1 Announce Type: cross  Abstract: The evaluation of machine learning models using human-labeled validation data can be expensive and time-consuming. AI-labeled synthetic data can be used to decrease the number of human annotations required for this purpose in a process called autoevaluation. We suggest efficient and statistically principled algorithms for this purpose that improve sample efficiency while remaining unbiased. These algorithms increase the effective human-labeled sample size by up to 50% on experiments with GPT-4.
    
[^5]: 愤怒的男性，悲伤的女性：大型语言模型在情绪归因中反映性别刻板印象

    Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution

    [https://arxiv.org/abs/2403.03121](https://arxiv.org/abs/2403.03121)

    大型语言模型中存在性别化情绪归因，反映了社会刻板印象。

    

    大型语言模型（LLMs）反映社会规范和偏见，尤其是关于性别的。虽然社会偏见和刻板印象在各种自然语言处理应用中得到了广泛研究，但在情绪分析方面存在一个令人惊讶的空白。然而，在社会话语中，情绪和性别密切相关。例如，女性经常被认为更具移情能力，而男性的愤怒更受社会接受。为了填补这一空白，我们提出了对五种最先进的LLMs（开源和封闭源）进行性别化情绪归因的首次全面研究。我们调查情绪是否具有性别特征，以及这些变化是否基于社会刻板印象。我们提示模型采用性别化角色并将情绪归因于类似“当我与亲近的人发生严重争执”这样的事件。然后我们分析模型生成的情绪与性别-事件对之间的关系。我们发现所有模型一致展现出性别化。

    arXiv:2403.03121v1 Announce Type: new  Abstract: Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men's anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like 'When I had a serious argument with a dear person'. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered e
    
[^6]: LLMs用于新闻标题的有针对性情感分析：探索不同级别的提示规范化

    LLMs for Targeted Sentiment in News Headlines: Exploring Different Levels of Prompt Prescriptiveness

    [https://arxiv.org/abs/2403.00418](https://arxiv.org/abs/2403.00418)

    提出了探索LLMs在新闻标题有针对性情感分析中不同级别提示规范性的方法，以提高其性能。

    

    新闻标题常常通过有意识地以特定方式描绘实体来引发情感，这使得新闻标题的有针对性情感分析(TSA)成为一项值得做但具有挑战的任务。微调的编码器模型展现出令人满意的TSA性能，但它们的背景知识有限，需要有标记的数据集。LLMs由于其广泛的语言和世界知识以及上下文学习能力，为TSA提供了一个潜在的通用解决方案，然而它们的性能受提示设计的影响很大。通过与主观任务的注释范式进行类比，我们探讨了提示设计对LLMs在新闻标题TSA中性能的影响。我们评估了使用不同级别的规范提示（从纯粹的零样本到符合注释指南的精心准备的少样本提示）的最先进LLMs的预测准确性。认识到TSA的主观性质，我们评估

    arXiv:2403.00418v1 Announce Type: new  Abstract: News headlines often evoke sentiment by intentionally portraying entities in particular ways, making targeted sentiment analysis (TSA) of headlines a worthwhile but difficult task. Fine-tuned encoder models show satisfactory TSA performance, but their background knowledge is limited, and they require a labeled dataset. LLMs offer a potentially universal solution for TSA due to their broad linguistic and world knowledge along with in-context learning abilities, yet their performance is heavily influenced by prompt design. Drawing parallels with annotation paradigms for subjective tasks, we explore the influence of prompt design on the performance of LLMs for TSA of news headlines. We evaluate the predictive accuracy of state-of-the-art LLMs using prompts with different levels of prescriptiveness, ranging from plain zero-shot to elaborate few-shot prompts matching annotation guidelines. Recognizing the subjective nature of TSA, we evaluate
    
[^7]: 大型语言模型是否反映认知语言处理？

    Do Large Language Models Mirror Cognitive Language Processing?

    [https://arxiv.org/abs/2402.18023](https://arxiv.org/abs/2402.18023)

    本文提出了一种新颖方法，通过将大型语言模型（LLMs）的表示与人类认知信号联系起来，评估LLMs模拟认知语言处理的效果。

    

    大型语言模型（LLMs）在文本理解和逻辑推理方面展现出卓越能力，甚至在许多认知任务中实现甚至超越人类水平的表现。由于LLMs是从人类语言认知的大量文本产出中训练出来的，自然而然地会问LLMs是否反映认知语言处理，或LLMs在多大程度上类似于认知语言处理。本文提出了一种新颖的方法，用于连接LLMs表征和人类认知信号，以评估LLMs如何有效地模拟认知语言处理。我们采用表征相似性分析（RSA）来衡量16种主流LLMs与大脑fMRI信号之间的对齐程度。我们在实验中探讨了各种因素（例如模型规模、对齐训练、指导附加）对LLM-大脑对齐的影响。实验结果表明，模型规模与正相关

    arXiv:2402.18023v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in text comprehension and logical reasoning, achiving or even surpassing human-level performance in numerous cognition tasks. As LLMs are trained from massive textual outputs of human language cognition, it is natural to ask whether LLMs mirror cognitive language processing. Or to what extend LLMs resemble cognitive language processing? In this paper, we propose a novel method that bridge between LLM representations and human cognition signals to evaluate how effectively LLMs simulate cognitive language processing. We employ Representational Similarity Analysis (RSA) to mearsure the alignment between 16 mainstream LLMs and fMRI signals of the brain. We empirically investigate the impact of a variety of factors (e.g., model scaling, alignment training, instruction appending) on such LLM-brain alignment. Experimental results indicate that model scaling is positively cor
    
[^8]: DistALANER：开源软件生态系统中的远程监督主动学习增强命名实体识别

    DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem

    [https://arxiv.org/abs/2402.16159](https://arxiv.org/abs/2402.16159)

    提出了一种为开源软件系统量身定制的新颖命名实体识别技术，通过远程监督注释过程、语言启发、查找表、外部知识源和主动学习方法来提高模型性能，有效缓解了成本和专家标注人员稀缺问题，并在关系抽取下游任务中显著优于LLMs。

    

    本文提出了一种专为开源软件系统量身定制的新颖命名实体识别（NER）技术。我们的方法旨在通过采用全面的两步远程监督注释过程来解决软件数据标注稀缺的问题。该过程巧妙地利用语言启发、独特的查找表、外部知识源以及主动学习方法。通过利用这些强大的技术，我们不仅提高了模型性能，还有效地缓解了成本和专家标注人员稀缺所带来的限制。值得注意的是，我们的框架在很大程度上明显优于最先进的LLMs。我们还展示了NER在关系抽取的下游任务中的有效性。

    arXiv:2402.16159v1 Announce Type: new  Abstract: This paper proposes a novel named entity recognition (NER) technique specifically tailored for the open-source software systems. Our approach aims to address the scarcity of annotated software data by employing a comprehensive two-step distantly supervised annotation process. This process strategically leverages language heuristics, unique lookup tables, external knowledge sources, and an active learning approach. By harnessing these powerful techniques, we not only enhance model performance but also effectively mitigate the limitations associated with cost and the scarcity of expert annotators. It is noteworthy that our framework significantly outperforms the state-of-the-art LLMs by a substantial margin. We also show the effectiveness of NER in the downstream task of relation extraction.
    
[^9]: FuseChat：对话模型知识融合

    FuseChat: Knowledge Fusion of Chat Models

    [https://arxiv.org/abs/2402.16107](https://arxiv.org/abs/2402.16107)

    FuseChat通过知识融合将多个对话模型的集体知识转移到目标语言模型中，避免了昂贵的预训练成本。

    

    虽然从头开始训练大型语言模型（LLMs）确实可以导致具有独特能力和优势的模型，但这种方法会产生巨大成本，并可能导致竞争能力的潜在冗余。一种替代策略是将现有的LLMs组合成更强大的LLM，从而减少昂贵的预训练的必要性。但是，由于LLMs的多样化架构，直接参数融合被证明是不可行的。最近，FuseLLM引入了知识融合的概念，通过轻量级的持续训练将多个结构多样的LLM的集体知识转移至目标LLM。在本报告中，我们扩展了FuseLLM框架的可扩展性和灵活性，实现了对话LLM的融合，生成了FuseChat。FuseChat包括两个主要阶段。首先，我们对结构和规模不同的源LLMs进行知识融合

    arXiv:2402.16107v1 Announce Type: new  Abstract: While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies. An alternative strategy is to combine existing LLMs into a more robust LLM, thereby diminishing the necessity for expensive pre-training. However, due to the diverse architectures of LLMs, direct parameter blending proves to be unfeasible. Recently, \textsc{FuseLLM} introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training. In this report, we extend the scalability and flexibility of the \textsc{FuseLLM} framework to realize the fusion of chat LLMs, resulting in \textsc{FuseChat}. \textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs t
    
[^10]: CriticBench：为批判性-正确推理评估LLMs而设计的基准测试

    CriticBench: Benchmarking LLMs for Critique-Correct Reasoning

    [https://arxiv.org/abs/2402.14809](https://arxiv.org/abs/2402.14809)

    CriticBench是一个综合基准测试，旨在评估LLMs在批判和纠正推理方面的能力，发现批判性训练显著提升性能，逻辑任务更易于修正。

    

    大型语言模型（LLMs）批判和完善其推理的能力对于它们在评估、反馈提供和自我改进中的应用至关重要。本文引入了CriticBench，一个旨在评估LLMs在各种任务中批判和纠正其推理能力的综合基准测试。CriticBench包含五个推理领域：数学、常识、符号、编码和算法。它整合了15个数据集，并结合了三个LLM系列的响应。利用CriticBench，我们评估和剖析了17个LLMs在生成、批判和修正推理（即GQC推理）中的表现。我们的研究结果显示：（1）GQC能力呈线性关系，批判性训练显著提升了性能；（2）修正效果在任务上有所不同，以逻辑为导向的任务更容易修正；（3）GQC知识的不一致性。

    arXiv:2402.14809v1 Announce Type: cross  Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsisten
    
[^11]: 自蒸馏桥接语言模型微调中的分布差距

    Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning

    [https://arxiv.org/abs/2402.13669](https://arxiv.org/abs/2402.13669)

    SDFT是一种通过用模型本身生成的精简数据集来桥接分布差距的新方法，在缓解灾难性遗忘的同时，在下游任务上实现了与普通微调相媲美甚至更优越的性能

    

    大型语言模型（LLMs）的兴起彻底改变了自然语言处理，但将它们微调为特定任务常常面临在平衡性能和保留一般指示遵循能力之间的挑战。在本文中，我们认为任务数据集与LLMs之间的分布差距是主要的潜在原因。为了解决这一问题，我们引入了自蒸馏微调（SDFT），这是一种通过用模型本身生成的精简数据集引导微调以匹配其原始分布来桥接分布差距的新方法。在各种基准测试上对Llama-2-chat模型的实验结果表明，SDFT有效地缓解了灾难性遗忘，同时在下游任务上实现了与普通微调相媲美甚至更优越的性能。此外，SDFT展示了保持LLMs的有益性和安全对齐的潜力

    arXiv:2402.13669v1 Announce Type: new  Abstract: The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs
    
[^12]: ED-Copilot: 使用语言模型诊断辅助减少急诊科等待时间

    ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance

    [https://arxiv.org/abs/2402.13448](https://arxiv.org/abs/2402.13448)

    本研究提出了一种在急诊科中减少等待时间的诊断辅助方法，利用人工智能系统帮助医生进行快速准确的诊断，并开发了ED-Copilot系统来推荐实验室检测并进行诊断预测。

    

    在急诊科（ED）中，患者在诊断前需要进行分诊和多种实验室检测。这个过程耗时，导致急诊科拥挤，显著影响患者死亡率、医疗错误、人员枯竭等。本研究提出了一种（时间）成本有效的诊断辅助方法，探索人工智能系统在协助急诊科临床医生进行高效准确诊断方面的潜力。使用公开可获得的患者数据，我们与急诊科临床医生合作策划了MIMIC-ED-Assist，这是一个衡量人工智能系统在建议最大程度减少急诊等待时间的实验室检测，并在正确预测诸如死亡之类关键结果方面的能力的基准。我们开发了ED-Copilot，它依次建议患者特定的实验室检测并进行诊断预测。ED-Copilot使用预训练的生物医学语言模型对患者信息进行编码并进行增强学习。

    arXiv:2402.13448v1 Announce Type: cross  Abstract: In the emergency department (ED), patients undergo triage and multiple laboratory tests before diagnosis. This process is time-consuming, and causes ED crowding which significantly impacts patient mortality, medical errors, staff burnout, etc. This work proposes (time) cost-effective diagnostic assistance that explores the potential of artificial intelligence (AI) systems in assisting ED clinicians to make time-efficient and accurate diagnoses. Using publicly available patient data, we collaborate with ED clinicians to curate MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in suggesting laboratory tests that minimize ED wait times, while correctly predicting critical outcomes such as death. We develop ED-Copilot which sequentially suggests patient-specific laboratory tests and makes diagnostic predictions. ED-Copilot uses a pre-trained bio-medical language model to encode patient information and reinforcement learn
    
[^13]: GumbelSoft: 通过GumbelMax技巧实现多样化的语言模型数字水印

    GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick

    [https://arxiv.org/abs/2402.12948](https://arxiv.org/abs/2402.12948)

    通过开发 GumbelSoft 水印，我们提出了一种能够在高多样性环境中增强生成文本多样性的解决方案，相较于其他方案表现更为优秀。

    

    大型语言模型(Large language models, LLMs)能够生成类似人类的文本，但也引发了人们对其在虚假新闻和学术不诚实方面的担忧。解码为基础的水印，尤其是基于GumbelMax技巧的水印(GM水印)，是防范机器生成文本滥用的杰出解决方案，因其显著的可检测性而脱颖而出。然而，GM水印在生成多样性方面面临一个主要挑战，对于相同提示始终产生相同输出，从而负面影响生成多样性和用户体验。为了克服这一局限，我们提出了一种新型GM水印，即Logits-Addition水印，及其三个变体，专门设计用于增强多样性。在这些变体中，GumbelSoft水印(作为Logits-Addition水印的一个softmax变体)在高多样性环境中表现出优越性能，其AUROC分数超过其他两个替代变体0.1至0.3，并超越其他方案。

    arXiv:2402.12948v1 Announce Type: new  Abstract: Large language models (LLMs) excellently generate human-like text, but also raise concerns about misuse in fake news and academic dishonesty. Decoding-based watermark, particularly the GumbelMax-trick-based watermark(GM watermark), is a standout solution for safeguarding machine-generated texts due to its notable detectability. However, GM watermark encounters a major challenge with generation diversity, always yielding identical outputs for the same prompt, negatively impacting generation diversity and user experience. To overcome this limitation, we propose a new type of GM watermark, the Logits-Addition watermark, and its three variants, specifically designed to enhance diversity. Among these, the GumbelSoft watermark (a softmax variant of the Logits-Addition watermark) demonstrates superior performance in high diversity settings, with its AUROC score outperforming those of the two alternative variants by 0.1 to 0.3 and surpassing oth
    
[^14]: 具有Transformer的上下文学习：Softmax注意力适应函数Lipschitz性质

    In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness

    [https://arxiv.org/abs/2402.11639](https://arxiv.org/abs/2402.11639)

    本文研究了在上下文学习框架中，Softmax注意力在适应预训练任务背景时的作用，发现注意力单元学会与Lipschitzness降低和标签噪声增加相关的窗口调整，以及在低维、线性问题上学会在推理前进行适当空间的投影。

    

    Transformer的一个显著特性是其能够进行上下文学习（ICL），在这种机器学习框架中，学习者在推理过程中通过某些数据隐式地被呈现一个新领域的背景，并被要求在该背景下进行预测。在这种情况下，学习者必须在没有额外训练的情况下适应背景。本文探讨了Softmax注意力在一个ICL设置中的作用，其中每个背景都编码了一个回归任务。我们展示了一个注意力单元学习一个窗口，用于实现一个适应于预训练任务的最近邻预测器。具体地，我们展示了这个窗口随着Lipschitzness的降低和标签噪声的增加而扩大。我们还展示了在低秩、线性问题上，注意力单元在推理之前学会了投影到适当的子空间。此外，我们还展示了这种适应性关键地依赖于softmax激活函数。

    arXiv:2402.11639v1 Announce Type: new  Abstract: A striking property of transformers is their ability to perform in-context learning (ICL), a machine learning framework in which the learner is presented with a novel context during inference implicitly through some data, and tasked with making a prediction in that context. As such that learner must adapt to the context without additional training. We explore the role of softmax attention in an ICL setting where each context encodes a regression task. We show that an attention unit learns a window that it uses to implement a nearest-neighbors predictor adapted to the landscape of the pretraining tasks. Specifically, we show that this window widens with decreasing Lipschitzness and increasing label noise in the pretraining tasks. We also show that on low-rank, linear problems, the attention unit learns to project onto the appropriate subspace before inference. Further, we show that this adaptivity relies crucially on the softmax activatio
    
[^15]: 重新探讨零阶优化在内存高效LLM微调中的应用：一个基准研究

    Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark

    [https://arxiv.org/abs/2402.11592](https://arxiv.org/abs/2402.11592)

    本研究提出了一种不使用反向传播的零阶优化方法，用于降低LLM微调中的内存成本，通过全面的基准研究扩展了对不同的ZO优化技术的探索。

    

    在自然语言处理（NLP）领域的不断发展中，使用SGD和Adam等一阶（FO）优化器微调预训练的大型语言模型（LLMs）已成为标准。然而，随着LLMs体积的增长，由于FO梯度计算的反向传播（BP）带来的巨大内存开销构成了一个重大挑战。解决这个问题至关重要，尤其对于内存效率至关重要的设备端训练等应用。本文提出了一种转向不使用BP的零阶（ZO）优化的方法，用于在LLM微调过程中降低内存成本，构建在MeZO提出的概念基础上。与传统的ZO梯度下降方法不同，我们的工作将探索扩展到更广泛的ZO优化技术，通过全面的、首次推出的基准研究跨越五个LLM系列（Roberta，OPT，LLaMA，Vicuna，Mistral），三种任务复杂性和五种微调方案。

    arXiv:2402.11592v1 Announce Type: new  Abstract: In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes
    
[^16]: 相对优先权优化: 通过对相同和不同提示的对比响应增强LLM对齐

    Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts

    [https://arxiv.org/abs/2402.10958](https://arxiv.org/abs/2402.10958)

    提出了相对优先权优化（RPO）方法，通过区分来自相同和相关提示的更受青睐的响应和更不受青睐的响应，扩展了模型的学习能力。

    

    在大型语言模型（LLM）领域，将模型与用户的多样化偏好相一致是一个关键挑战。直接优先权优化（DPO）在这一领域起到了关键作用。DPO通过使用从相同提示中派生的偏好对来工作，而无需额外的奖励模型。然而，DPO并不能完全反映人类学习的复杂性，这种学习往往涉及对不仅相同而且相似问题的对比响应的理解。为了克服这一不足，我们提出了相对优先权优化（RPO）。RPO旨在区分来自相同和相关提示的更受青睐的响应和更不受青睐的响应。它引入了对比加权机制，使LLMs能够使用更广泛的偏好数据进行调整，包括成对和不成对的数据集。这种方法扩展了模型的学习能力，使其能够利用更多的偏好数据进行优化。

    arXiv:2402.10958v1 Announce Type: cross  Abstract: In the field of large language models (LLMs), aligning models with the diverse preferences of users is a critical challenge. Direct Preference Optimization (DPO) has played a key role in this area. It works by using pairs of preferences derived from the same prompts, and it functions without needing an additional reward model. However, DPO does not fully reflect the complex nature of human learning, which often involves understanding contrasting responses to not only identical but also similar questions. To overcome this shortfall, we propose Relative Preference Optimization (RPO). RPO is designed to discern between more and less preferred responses derived from both identical and related prompts. It introduces a contrastive weighting mechanism, enabling the tuning of LLMs using a broader range of preference data, including both paired and unpaired sets. This approach expands the learning capabilities of the model, allowing it to lever
    
[^17]: EcoRank: 使用大型语言模型进行受限预算文本重新排序

    EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models

    [https://arxiv.org/abs/2402.10866](https://arxiv.org/abs/2402.10866)

    EcoRank是一个两层管线，通过联合优化有关预算分配和LLM API的决策来实现文本重新排序，在实验中表现优于其他预算感知方法。

    

    大型语言模型（LLMs）在文本重新排序中取得了最先进的性能。该过程包括在提示中使用查询和候选段落，利用点对点，列表式和成对提示策略。LLMs的这些排序策略的一个限制是它们的成本：由于API收费基于输入和输出令牌的数量，这个过程可能会变得昂贵。我们研究如何在给定预算的情况下最大化重新排序性能，通过导航提示选择，LLM API和预算分割的广阔搜索空间。我们提出了一套使用一组LLM API进行文本重新排序的受限预算方法。我们最有效的方法是EcoRank，它是一个两层管线，可以联合优化有关跨提示策略和LLM API的预算分配决策。我们在四个流行的QA和段重排序数据集上的实验结果显示，EcoRank优于其他具有预算意识的方法。

    arXiv:2402.10866v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved state-of-the-art performance in text re-ranking. This process includes queries and candidate passages in the prompts, utilizing pointwise, listwise, and pairwise prompting strategies. A limitation of these ranking strategies with LLMs is their cost: the process can become expensive due to API charges, which are based on the number of input and output tokens. We study how to maximize the re-ranking performance given a budget, by navigating the vast search spaces of prompt choices, LLM APIs, and budget splits. We propose a suite of budget-constrained methods to perform text re-ranking using a set of LLM APIs. Our most efficient method, called EcoRank, is a two-layered pipeline that jointly optimizes decisions regarding budget allocation across prompt strategies and LLM APIs. Our experimental results on four popular QA and passage reranking datasets show that EcoRank outperforms other budget-aware 
    
[^18]: DataDreamer: 一种用于合成数据生成和可复现LLM工作流程的工具

    DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows

    [https://arxiv.org/abs/2402.10379](https://arxiv.org/abs/2402.10379)

    DataDreamer是一种用于合成数据生成和可复现LLM工作流程的开源Python库，有助于研究人员实现强大的LLM工作流，提倡开放科学和可重现性。

    

    大型语言模型（LLMs）已成为自然语言处理研究人员在各种任务中的主要工具。如今，许多研究人员在合成数据生成、任务评估、微调、提炼以及其他与模型相关的研究工作流中使用LLMs。然而，使用这些模型时会遇到挑战，这些挑战源于它们的规模、闭源性质以及缺乏针对这些新兴工作流的标准化工具。这些模型的迅速崛起和这些独特挑战对开放科学和使用它们的工作的可重现性产生了直接的负面影响。在本文中，我们介绍了DataDreamer，这是一个开源Python库，使研究人员能够编写简单的代码来实现强大的LLM工作流。DataDreamer还帮助研究人员遵循我们提出的最佳实践，以鼓励开放科学和可重现性。该库和文档可在h网站上找到。

    arXiv:2402.10379v1 Announce Type: new  Abstract: Large language models (LLMs) have become a dominant and important tool for NLP researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In this paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to encourage open science and reproducibility. The library and documentation are available at h
    
[^19]: 理性报告卡：评估大型语言模型的经济合理性

    Rationality Report Cards: Assessing the Economic Rationality of Large Language Models

    [https://arxiv.org/abs/2402.09552](https://arxiv.org/abs/2402.09552)

    本文在评估大型语言模型的经济合理性方面提出了一种方法，通过量化评分模型在各个要素上的表现并结合用户提供的评分标准，生成一份"理性报告卡"，以确定代理人是否足够可靠。

    

    越来越多的人对将LLM用作决策"代理人"兴趣日益增加。这包括很多自由度：应该使用哪个模型；如何进行提示；是否要求其进行内省、进行思考链等。解决这些问题（更广泛地说，确定LLM代理人是否足够可靠以便获得信任）需要一种评估这种代理人经济合理性的方法论，在本文中我们提供了一个方法。我们首先对理性决策的经济文献进行了调研、将代理人应该展现的大量细粒度"要素"进行分类，并确定了它们之间的依赖关系。然后，我们提出了一个基准分布，以定量评分LLM在这些要素上的表现，并结合用户提供的评分标准，生成一份"理性报告卡"。最后，我们描述了与14种不同的LLM进行的大规模实证实验的结果。

    arXiv:2402.09552v1 Announce Type: new  Abstract: There is increasing interest in using LLMs as decision-making "agents." Doing so includes many degrees of freedom: which model should be used; how should it be prompted; should it be asked to introspect, conduct chain-of-thought reasoning, etc? Settling these questions -- and more broadly, determining whether an LLM agent is reliable enough to be trusted -- requires a methodology for assessing such an agent's economic rationality. In this paper, we provide one. We begin by surveying the economic literature on rational decision making, taxonomizing a large set of fine-grained "elements" that an agent should exhibit, along with dependencies between them. We then propose a benchmark distribution that quantitatively scores an LLMs performance on these elements and, combined with a user-provided rubric, produces a "rationality report card." Finally, we describe the results of a large-scale empirical experiment with 14 different LLMs, characte
    
[^20]: 论据顺序在与大型语言模型推理中起作用

    Premise Order Matters in Reasoning with Large Language Models

    [https://arxiv.org/abs/2402.08939](https://arxiv.org/abs/2402.08939)

    对大型语言模型（LLMs）进行推理任务时，论据的顺序非常重要，尤其是在演绎推理任务中，按照提示的真实证明顺序呈现论据可以显著提高模型的准确性。

    

    大型语言模型（LLMs）在各个领域都取得了惊人的推理性能。然而，在推理任务的领域中，我们发现了一个脆弱性：尽管这种顺序不会改变基本任务，但LLMs对于论据的顺序非常脆弱。特别是，我们观察到当论据顺序与中间推理步骤所需的上下文对齐时，LLMs可以达到最佳性能。例如，在演绎推理任务中，将论据按照提示的真实证明顺序呈现（而不是随机顺序）会极大地提高模型的准确性。我们首先研究了不同LLMs对演绎推理中论据顺序的影响，我们的评估结果表明，调整论据顺序可能导致性能下降超过30％。此外，我们发布了基于GSM8K的基准测试R-GSM来研究顺序效应对数学推理的影响。

    arXiv:2402.08939v1 Announce Type: new Abstract: Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathema
    
[^21]: BBox-Adapter: 轻量级适配黑盒大型语言模型

    BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models

    [https://arxiv.org/abs/2402.08219](https://arxiv.org/abs/2402.08219)

    BBox-Adapter是一种适用于黑盒大型语言模型的轻量级适配器，通过区分目标和源域数据，并采用排名式噪音对比估计（NCE）损失和在线适应机制，实现了在透明、隐私和成本方面的有效适应。

    

    适应最先进的大型语言模型（LLMs），如GPT-4和Gemini，以满足特定任务的要求是具有挑战性的。由于它们的参数、嵌入和输出概率的不透明性，现有的微调适应方法是不适用的。因此，只能通过它们的API服务适应这些黑盒LLMs，这引发了透明度、隐私和成本的担忧。为了解决这些挑战，我们介绍了BBox-Adapter，一种新颖的适用于黑盒LLMs的轻量级适配器。BBox-Adapter通过将目标数据视为正样本，将源数据视为负样本来区分目标和源域数据。它采用基于排名的噪音对比估计（NCE）损失来提高目标域数据的可能性，同时惩罚源域数据的可能性。此外，它还具有在线适应机制，该机制将来自真实数据、人类或AI反馈的实时正样本采样与先前适应的负样本数据相结合。广泛的实验表明，BBox-Adapter在不降低性能的同时，提供了高效而灵活的黑盒LLMs适应解决方案。

    Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive ex
    
[^22]: TELLER: 一个可信的、可推广的、可控制的假新闻检测框架

    TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection

    [https://arxiv.org/abs/2402.07776](https://arxiv.org/abs/2402.07776)

    TELLER是一个可信的假新闻检测框架，通过集成认知和决策系统，提供了解释性、可推广性和可控制性。认知系统利用人类专业知识生成逻辑谓词，指导大型语言模型生成可读的逻辑原子。决策系统推导可推广的逻辑规则来聚合这些原子，实现真实和虚假新闻的识别。

    

    假新闻的泛滥已成为一个严重的社会问题，引起了行业和学术界的广泛关注。虽然现有的基于深度学习的方法在准确检测假新闻方面取得了进展，但它们的可靠性可能会受到非透明推理过程、差强人意的推广能力以及与大型语言模型（LLMs）集成的固有风险的影响。为了应对这一挑战，我们提出了一种新颖的框架，名为TELLER，用于可信的假新闻检测，重点关注模型的可解释性、可推广性和可控制性。这通过一个融合认知和决策系统的双系统框架来实现，遵循以上原则。认知系统利用人类专业知识生成逻辑谓词，指导LLMs生成可读的逻辑原子。同时，决策系统推导可推广的逻辑规则来聚合这些原子，实现真实和虚假新闻的识别。

    The proliferation of fake news has emerged as a severe societal problem, raising significant interest from industry and academia. While existing deep-learning based methods have made progress in detecting fake news accurately, their reliability may be compromised caused by the non-transparent reasoning processes, poor generalization abilities and inherent risks of integration with large language models (LLMs). To address this challenge, we propose {\methodname}, a novel framework for trustworthy fake news detection that prioritizes explainability, generalizability and controllability of models. This is achieved via a dual-system framework that integrates cognition and decision systems, adhering to the principles above. The cognition system harnesses human expertise to generate logical predicates, which guide LLMs in generating human-readable logic atoms. Meanwhile, the decision system deduces generalizable logic rules to aggregate these atoms, enabling the identification of the truthfu
    
[^23]: 研究指令调整的局限性

    A Closer Look at the Limitations of Instruction Tuning

    [https://arxiv.org/abs/2402.05119](https://arxiv.org/abs/2402.05119)

    本文通过实验和分析揭示了指令调整的多个局限性，包括无法增强LLM的知识和技能、从具有知识来源的数据集复制回应模式导致质量下降、全参数微调增加了错误生成的情况。

    

    指令调整（IT）是使用指令-回应对来训练大型语言模型（LLM）的过程，已成为将基础预训练LLM转化为开放领域对话代理的主要方法。虽然IT取得了显著的成功并广泛应用，但其局限性和不足仍未得到充分探讨。本文通过严格的实验和对LLM通过IT发生的变化的深入分析，揭示了IT的多种局限性。特别是，我们发现：（1）IT无法增强LLM的知识或技能。LoRA微调仅限于学习回应的启动和样式令牌，而全参数微调会导致知识退化。（2）从具有知识来源的IT数据集复制回应模式会导致回应质量下降。（3）全参数微调通过不准确地从IT数据集中获取概念上相似实例的标记，增加了错误生成的情况。

    Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating respon
    
[^24]: InfLLM: 揭示LLMs对于处理超长序列的内在能力，无需训练的记忆

    InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory

    [https://arxiv.org/abs/2402.04617](https://arxiv.org/abs/2402.04617)

    InfLLM是一种无需训练的基于记忆的方法，用于揭示LLMs处理超长序列的内在能力。它通过存储远距离上下文和高效的注意计算机制，允许LLMs有效处理具有流式输入的长序列。

    

    大型语言模型（LLMs）已成为处理具有漫长传输输入的现实应用的基石，如LLM驱动代理。然而，现有的在受限最大长度序列上预训练的LLMs无法推广到更长的序列，因为存在领域外和分散注意力的问题。为了缓解这些问题，现有的工作采用滑动注意力窗口和丢弃远距离标记，以处理超长序列。不幸的是，这些方法无法捕获序列内的长距离依赖关系，以深入理解语义。本文介绍了一种无需训练的基于记忆的方法InfLLM，来揭示LLMs处理流式长序列的内在能力。具体而言，InfLLM将远距离的上下文存储到附加的内存单元中，并使用高效的机制来查找与注意计算相关的标记单元。因此，InfLLM允许LLMs高效处理长序列，同时保持了对语义的深入理解。

    Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs, such as LLM-driven agents. However, existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues. To alleviate these issues, existing efforts employ sliding attention windows and discard distant tokens to achieve the processing of extremely long sequences. Unfortunately, these approaches inevitably fail to capture long-distance dependencies within sequences to deeply understand semantics. This paper introduces a training-free memory-based method, InfLLM, to unveil the intrinsic ability of LLMs to process streaming long sequences. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences while maintaining
    
[^25]: REBORN: 基于强化学习的迭代训练的无监督语音识别中的边界分割

    REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR

    [https://arxiv.org/abs/2402.03988](https://arxiv.org/abs/2402.03988)

    本文提出了REBORN，在无监督语音识别中使用基于强化学习的迭代训练来实现边界分割。通过交替训练分割模型和音素预测模型，实现了学习语音和文本之间的映射，解决了无监督情况下语音信号分段结构边界的挑战。

    

    无监督自动语音识别（ASR）旨在学习语音信号与其对应的文本转录之间的映射，而无需配对的语音-文本数据监督。语音信号中的单词/音素由一段长度可变且边界未知的语音信号表示，而这种分段结构使得在没有配对数据的情况下学习语音和文本之间的映射变得具有挑战性。本文提出了REBORN，基于强化学习的迭代训练的无监督语音识别中的边界分割。REBORN交替进行以下两个步骤：（1）训练一个能够预测语音信号中分段结构边界的分割模型，和（2）训练一个音素预测模型，其输入是由分割模型分割的分段结构，用于预测音素转录。由于没有用于训练分割模型的监督数据，我们使用强化学习来训练分割模型。

    Unsupervised automatic speech recognition (ASR) aims to learn the mapping between the speech signal and its corresponding textual transcription without the supervision of paired speech-text data. A word/phoneme in the speech signal is represented by a segment of speech signal with variable length and unknown boundary, and this segmental structure makes learning the mapping between speech and text challenging, especially without paired data. In this paper, we propose REBORN, Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR. REBORN alternates between (1) training a segmentation model that predicts the boundaries of the segmental structures in speech signals and (2) training the phoneme prediction model, whose input is a segmental structure segmented by the segmentation model, to predict a phoneme transcription. Since supervised data for training the segmentation model is not available, we use reinforcement learning to train the segmentation model t
    
[^26]: 人类与机器：重新思考语言模型在蕴含验证中的应用

    Minds versus Machines: Rethinking Entailment Verification with Language Models

    [https://arxiv.org/abs/2402.03686](https://arxiv.org/abs/2402.03686)

    本文通过研究人类和大型语言模型在推理判断中的共性和差异，发现大型语言模型在复杂推理中具有优势，而人类在简单推理中表现出色。基于这些发现，引入了一个优化的Flan-T5模型，用于蕴含验证。

    

    人类在文本理解中进行大量的推理以理解论述。本文旨在了解人类和最先进的大型语言模型（LLM）在推理判断中的共性和差异。通过综合策划的蕴含验证基准测试，我们评估了人类和LLM在各种推理类别中的表现。我们的基准测试包含了来自三个类别（NLI、上下文QA和解释）的数据集，包括多句前提和不同的知识类型，从而评估了复杂推理情况下的推理能力。值得注意的是，我们的发现显示LLM在跨扩展上下文的多跳推理中具有优势，而人类在需要简单演绎推理的任务中表现出色。利用这些见解，我们介绍了一个经过精细调整的Flan-T5模型，其性能超过了GPT-3.5，并与GPT-4媲美，提供了一个强大的开源解决方案供蕴含验证使用。作为一个实际的应用

    Humans make numerous inferences in text comprehension to understand discourse. This paper aims to understand the commonalities and disparities in the inference judgments between humans and state-of-the-art Large Language Models (LLMs). Leveraging a comprehensively curated entailment verification benchmark, we evaluate both human and LLM performance across various reasoning categories. Our benchmark includes datasets from three categories (NLI, contextual QA, and rationales) that include multi-sentence premises and different knowledge types, thereby evaluating the inference capabilities in complex reasoning instances. Notably, our findings reveal LLMs' superiority in multi-hop reasoning across extended contexts, while humans excel in tasks necessitating simple deductive reasoning. Leveraging these insights, we introduce a fine-tuned Flan-T5 model that outperforms GPT-3.5 and rivals with GPT-4, offering a robust open-source solution for entailment verification. As a practical application
    
[^27]: 通过修正的缩放定律选择大型语言模型进行微调

    Selecting Large Language Model to Fine-tune via Rectified Scaling Law

    [https://arxiv.org/abs/2402.02314](https://arxiv.org/abs/2402.02314)

    该论文研究了在资源受限的情况下如何选择合适的预训练语言模型进行微调的问题。通过引入修正的缩放定律和预学习数据大小的概念，作者提出了一种新颖的模型选择算法，可以选择接近最优的模型。

    

    在日益增长的语言模型生态系统中，在众多选项中选择最合适的预训练模型进行微调成为了一个挑战。在资源受限的情况下，微调所有模型然后再进行选择是不现实的。在本文中，我们将这个资源受限的选择任务转化为预测微调性能，并且展示其与缩放定律之间的自然联系。与预训练不同，我们发现微调的缩放曲线不仅包括众所周知的“功率阶段”，还包括以前未被观察到的“预功率阶段”。我们还解释了为什么现有的缩放定律无法理论和实证地捕捉到这种相变现象。为了解决这个问题，我们将“预学习数据大小”概念引入到我们的修正缩放定律中，这克服了理论上的限制，并更好地适应实验结果。通过利用我们的定律，我们提出了一种新颖的语言模型选择算法，可以选择接近最优的模型。

    The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with scaling laws. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our rectified scaling law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundr
    
[^28]: 历史感知的对话式稠密检索

    History-Aware Conversational Dense Retrieval. (arXiv:2401.16659v1 [cs.IR])

    [http://arxiv.org/abs/2401.16659](http://arxiv.org/abs/2401.16659)

    该论文提出了一种历史感知的对话式稠密检索系统，通过上下文去噪的查询重构以及根据历史轮次的实际影响自动挖掘监督信号改进了现有的对话式稠密检索方法。

    

    对话搜索通过实现用户和系统之间的多轮交互，实现了复杂信息检索的便利。支持这种交互需要对对话输入有全面的理解，以便根据历史信息制定良好的搜索查询。特别是，搜索查询应包括来自先前对话回合的相关信息。然而，目前的对话式稠密检索方法主要依赖于对经过精调的预训练专门检索器进行整个对话式搜索会话的优化，这可能会变得冗长和嘈杂。此外，现有方法受现有数据集中手动监督信号数量的限制。为了解决上述问题，我们提出了一种历史感知的对话式稠密检索(HAConvDR)系统，它结合了两个思想：上下文去噪的查询重构和根据历史轮次的实际影响进行自动挖掘监督信号。

    Conversational search facilitates complex information retrieval by enabling multi-turn interactions between users and the system. Supporting such interactions requires a comprehensive understanding of the conversational inputs to formulate a good search query based on historical information. In particular, the search query should include the relevant information from the previous conversation turns. However, current approaches for conversational dense retrieval primarily rely on fine-tuning a pre-trained ad-hoc retriever using the whole conversational search session, which can be lengthy and noisy. Moreover, existing approaches are limited by the amount of manual supervision signals in the existing datasets. To address the aforementioned issues, we propose a History-Aware Conversational Dense Retrieval (HAConvDR) system, which incorporates two ideas: context-denoised query reformulation and automatic mining of supervision signals based on the actual impact of historical turns. Experime
    
[^29]: MM-LLMs: 多模式大语言模型的最新进展

    MM-LLMs: Recent Advances in MultiModal Large Language Models. (arXiv:2401.13601v1 [cs.CL])

    [http://arxiv.org/abs/2401.13601](http://arxiv.org/abs/2401.13601)

    近年来，多模式大语言模型（MM-LLMs）通过成本效益高的训练策略取得了显著进展，扩展了现有的语言模型的多模输入和输出支持。本论文提供了一份综合调查报告，介绍了MM-LLMs的设计和训练方案，整理了现有的MM-LLMs及其性能，总结了关键训练方法，并探讨了未来的研究方向。

    

    在过去的一年中，多模式大语言模型（MM-LLMs）取得了显著的进展，通过成本效益高的训练策略，增强了现有的LLMs对多模输入或输出的支持。这些结果模型不仅保留了LLMs固有的推理和决策能力，还赋予了各种多模任务。本文提供了一份综合性的调查报告，旨在促进对MM-LLMs的进一步研究。具体而言，我们首先概述了模型架构和训练流程的一般设计方案。随后，我们简要介绍了26种现有的MM-LLMs，每种都以其具体的公式为特征。此外，我们还回顾了MM-LLMs在主流基准测试上的性能，并总结了提高MM-LLMs效力的关键训练方法。最后，我们探讨了MM-LLMs的有前途的方向，同时还为该领域的最新发展提供了实时追踪网站。我们希望这份调查报告能够促进对MM-LLMs的进一步研究。

    In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Specifically, we first outline general design formulations for model architecture and training pipeline. Subsequently, we provide brief introductions of $26$ existing MM-LLMs, each characterized by its specific formulations. Additionally, we review the performance of MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this surv
    
[^30]: MF-AED-AEC: 通过融合多模态、ASR错误检测和ASR错误修正实现语音情感识别

    MF-AED-AEC: Speech Emotion Recognition by Leveraging Multimodal Fusion, ASR Error Detection, and ASR Error Correction. (arXiv:2401.13260v1 [cs.CL])

    [http://arxiv.org/abs/2401.13260](http://arxiv.org/abs/2401.13260)

    本文提出了一种名为MF-AED-AEC的方法，通过融合多模态、ASR错误检测和ASR错误修正，通过增强ASR文本的语义一致性来改进语音情感识别的性能。

    

    语音情感识别（SER）中普遍采用的方法是通过整合音频和文本信息来全面识别说话者的情感，其中文本通常通过自动语音识别（ASR）获取。这种方法的一个重要问题是，文本模态中的ASR错误会使SER的性能变差。之前的研究提出使用辅助的ASR错误检测任务来自适应地分配ASR假设中每个词的权重。然而，这种方法的改进潜力有限，因为它没有解决文本中语义信息的一致性。此外，不同模态之间的固有异质性导致它们表示之间的分布差异，使它们的融合具有挑战性。因此，在本文中，我们引入了两个辅助任务，ASR错误检测（AED）和ASR错误修正（AEC），以增强ASR文本的语义一致性，并进一步引入了一种新颖的多模态融合方法。

    The prevalent approach in speech emotion recognition (SER) involves integrating both audio and textual information to comprehensively identify the speaker's emotion, with the text generally obtained through automatic speech recognition (ASR). An essential issue of this approach is that ASR errors from the text modality can worsen the performance of SER. Previous studies have proposed using an auxiliary ASR error detection task to adaptively assign weights of each word in ASR hypotheses. However, this approach has limited improvement potential because it does not address the coherence of semantic information in the text. Additionally, the inherent heterogeneity of different modalities leads to distribution gaps between their representations, making their fusion challenging. Therefore, in this paper, we incorporate two auxiliary tasks, ASR error detection (AED) and ASR error correction (AEC), to enhance the semantic coherence of ASR text, and further introduce a novel multi-modal fusion 
    
[^31]: INTERS: 使用指令调优解锁大型语言模型在搜索中的力量

    INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning. (arXiv:2401.06532v1 [cs.CL])

    [http://arxiv.org/abs/2401.06532](http://arxiv.org/abs/2401.06532)

    本研究探索了指令调优的方法，以增强大型语言模型在信息检索任务中的能力，通过引入一个新的指令调优数据集INTERS，涵盖了21个IR任务，该方法显著提升了性能。

    

    大型语言模型（LLMs）在各种自然语言处理任务中展示了令人印象深刻的能力。然而，由于许多与信息检索（IR）具体概念的不经常出现在自然语言中，它们在信息检索任务中的应用仍然具有挑战性。虽然基于提示的方法可以向LLMs提供任务描述，但它们往往在促进全面理解和执行IR任务方面存在不足，从而限制了LLMs的适用性。为了弥补这一差距，本研究探索了指令调优的潜力，以提高LLMs在IR任务中的熟练程度。我们引入了一个新的指令调优数据集INTERS，涵盖了3个基本IR类别中的21个任务：查询理解、文档理解和查询文档关系理解。数据来自43个不同的由手动编写的模板构成的数据集。我们的实证结果表明，INTERS显著提升了各种公开数据集上的性能。

    Large language models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks. Despite this, their application to information retrieval (IR) tasks is still challenging due to the infrequent occurrence of many IR-specific concepts in natural language. While prompt-based methods can provide task descriptions to LLMs, they often fall short in facilitating comprehensive understanding and execution of IR tasks, thereby limiting LLMs' applicability. To address this gap, in this work, we explore the potential of instruction tuning to enhance LLMs' proficiency in IR tasks. We introduce a novel instruction tuning dataset, INTERS, encompassing 21 tasks across three fundamental IR categories: query understanding, document understanding, and query-document relationship understanding. The data are derived from 43 distinct datasets with manually written templates. Our empirical results reveal that INTERS significantly boosts the performance of various publicly a
    
[^32]: UstanceBR:一种用于目标立场预测的多模态语言资源

    UstanceBR: a multimodal language resource for stance prediction. (arXiv:2312.06374v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.06374](http://arxiv.org/abs/2312.06374)

    UstanceBR是一个多模态语言资源，用于目标立场预测，包含巴西葡萄牙语Twitter领域的86.8k标记立场和发布者的网络信息。这个研究为未来的研究提供了初始基准结果。

    

    本研究介绍了UstanceBR，这是一个用于巴西葡萄牙语Twitter领域的多模态语料库，用于目标立场预测。该语料库包含对所选目标主题的86.8k标记立场，并且包含了发布这些立场的社交媒体用户的广泛网络信息。在本文中，我们描述了语料库的多模态数据，并提供了基于文本和网络相关信息的领域内和零样本立场预测的多个使用示例，旨在为未来的研究提供初始基准结果。

    This work introduces UstanceBR, a multimodal corpus in the Brazilian Portuguese Twitter domain for target-based stance prediction. The corpus comprises 86.8 k labelled stances towards selected target topics, and extensive network information about the users who published these stances on social media. In this article we describe the corpus multimodal data, and a number of usage examples in both in-domain and zero-shot stance prediction based on textand network-related information, which are intended to provide initial baseline results for future studies in the field.
    
[^33]: CausalCite：一种论文引用的因果公式化

    CausalCite: A Causal Formulation of Paper Citations. (arXiv:2311.02790v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.02790](http://arxiv.org/abs/2311.02790)

    CausalCite是一种以因果推断为基础的论文引用公式化方法，通过对文本进行嵌入和相似样本的提取来评估论文的重要性，并在各个标准上展示了其有效性。

    

    对于科学界来说，评估一篇论文的重要性至关重要但也具有挑战性。尽管引用次数是最常用的评估指标，但它们被广泛批评为无法准确反映一篇论文的真正影响力。在这项工作中，我们提出了一种因果推断方法，称为TextMatch，它将传统的匹配框架适应于高维文本嵌入。具体而言，我们使用大型语言模型（LLM）对每篇论文进行文本嵌入，通过余弦相似性提取相似样本，并根据相似度值的加权平均合成一个反事实样本。我们将得到的指标称为CausalCite，作为论文引用的因果公式化。我们展示了它在各种标准上的有效性，如与科学专家对1K篇论文的报告的论文影响力的高相关性，过去论文的（经过时间考验的）奖项，以及在各个子领域的稳定性。

    Evaluating the significance of a paper is pivotal yet challenging for the scientific community. While the citation count is the most commonly used proxy for this purpose, they are widely criticized for failing to accurately reflect a paper's true impact. In this work, we propose a causal inference method, TextMatch, which adapts the traditional matching framework to high-dimensional text embeddings. Specifically, we encode each paper using the text embeddings by large language models (LLMs), extract similar samples by cosine similarity, and synthesize a counterfactual sample by the weighted average of similar papers according to their similarity values. We apply the resulting metric, called CausalCite, as a causal formulation of paper citations. We show its effectiveness on various criteria, such as high correlation with paper impact as reported by scientific experts on a previous dataset of 1K papers, (test-of-time) awards for past papers, and its stability across various sub-fields o
    
[^34]: BadLlama：以低成本移除Llama 2-Chat 13B的安全微调

    BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B. (arXiv:2311.00117v1 [cs.CL])

    [http://arxiv.org/abs/2311.00117](http://arxiv.org/abs/2311.00117)

    研究发现，公开发布模型权重使得安全微调无效，BadLlama项目以低成本成功移除了Llama 2-Chat 13B的安全微调并保留了其一般能力。

    

    Llama 2-Chat是Meta开发并向公众发布的一系列大型语言模型。尽管Meta对Llama 2-Chat进行了安全微调以拒绝输出有害内容，但我们假设公共获取模型权重使得坏意行为者可以以低成本绕过Llama 2-Chat的安全机制，并将Llama 2的能力用于恶意目的。我们展示了以少于200美元的成本有效地取消Llama 2-Chat 13B的安全微调，同时保留其一般能力。我们的结果表明，当发布模型权重时，安全微调是无效的防止滥用的方法。鉴于未来的模型很可能具有更大规模的危害能力，AI开发者在考虑公开发布模型权重时必须解决微调带来的威胁。

    Llama 2-Chat is a collection of large language models that Meta developed and released to the public. While Meta fine-tuned Llama 2-Chat to refuse to output harmful content, we hypothesize that public access to model weights enables bad actors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's capabilities for malicious purposes. We demonstrate that it is possible to effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than $200, while retaining its general capabilities. Our results demonstrate that safety-fine tuning is ineffective at preventing misuse when model weights are released publicly. Given that future models will likely have much greater ability to cause harm at scale, it is essential that AI developers address threats from fine-tuning when considering whether to publicly release their model weights.
    
[^35]: 激发大型语言模型中的提示工程潜力：一项综述

    Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review. (arXiv:2310.14735v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.14735](http://arxiv.org/abs/2310.14735)

    这篇论文解释了提示工程在释放大型语言模型能力方面的关键作用，探讨了不同的提示方法以及外部插件如何协助减少机器幻想，并指出了未来研究方向的重要性。

    

    本文深入探讨了提示工程在释放大型语言模型（LLM）能力方面的关键作用。提示工程是为LLM构建输入文本的过程，是优化LLM有效性的重要技术。本综述阐明了提示工程的基本原理，如角色提示、一次性提示和少量提示，以及更高级的方法，如思维链和思维树提示。本文还阐述了外部插件如何协助此任务，并通过检索外部知识来减少机器幻想。随后，我们勾勒了提示工程研究的前景方向，强调了对结构和代理在人工智能生成内容（AIGC）工具中的作用的深入理解的必要性。我们讨论了如何从不同角度和使用不同的方法评估提示方法的有效性。最后，我们提出了展望未来的研究方向。

    This paper delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). Prompt engineering is the process of structuring input text for LLMs and is a technique integral to optimizing the efficacy of LLMs. This survey elucidates foundational principles of prompt engineering, such as role-prompting, one-shot, and few-shot prompting, as well as more advanced methodologies such as the chain-of-thought and tree-of-thoughts prompting. The paper sheds light on how external assistance in the form of plugins can assist in this task, and reduce machine hallucination by retrieving external knowledge. We subsequently delineate prospective directions in prompt engineering research, emphasizing the need for a deeper understanding of structures and the role of agents in Artificial Intelligence-Generated Content (AIGC) tools. We discuss how to assess the efficacy of prompt methods from different perspectives and using different methods. Finally, we
    
[^36]: 对气候信息的大规模语言模型进行评估

    Assessing Large Language Models on Climate Information. (arXiv:2310.02932v1 [cs.CL])

    [http://arxiv.org/abs/2310.02932](http://arxiv.org/abs/2310.02932)

    本研究提出了一个基于科学传播原则的综合评估框架，评估了大规模语言模型在气候变化信息中的表现，能够在回答气候变化主题方面提供细粒度的分析。

    

    理解气候变化对我们的影响，了解可用的解决方案，是赋予个人和社区减缓和适应气候变化的重要步骤。随着大规模语言模型（LLMs）的普及，有必要评估它们在这个领域的能力。本研究提出了一个基于科学传播原则的综合评估框架，以分析LLM对气候变化主题的回答。我们的框架强调回答的呈现和认识上的适当性，为LLM生成提供了细粒度的分析。覆盖了8个维度，我们的框架能够识别模型输出中的30个不同问题。该任务是一个现实世界中的例子，这个领域存在越来越多的具有挑战性的问题，AI可以补充和提升人类的表现。我们引入了一种新颖而实用的可扩展监督协议，利用AI辅助并依靠具有相关教育背景的评估员。我们评估了几个最近的LLM，并进行了实证评估。

    Understanding how climate change affects us and learning about available solutions are key steps toward empowering individuals and communities to mitigate and adapt to it. As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in this domain. In this study, we present a comprehensive evaluation framework, grounded in science communication principles, to analyze LLM responses to climate change topics. Our framework emphasizes both the presentational and epistemological adequacy of answers, offering a fine-grained analysis of LLM generations. Spanning 8 dimensions, our framework discerns up to 30 distinct issues in model outputs. The task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel and practical protocol for scalable oversight that uses AI Assistance and relies on raters with relevant educational backgrounds. We evaluate several recent LLMs and conduct 
    
[^37]: 无需分割的流式机器翻译

    Segmentation-Free Streaming Machine Translation. (arXiv:2309.14823v1 [cs.CL])

    [http://arxiv.org/abs/2309.14823](http://arxiv.org/abs/2309.14823)

    本研究提出了一种无需分割的流式机器翻译框架，通过延迟分割决策来提高翻译质量和延迟平衡，相比其他方法具有更好的效果。

    

    流式机器翻译是实时将未限定输入文本流进行翻译的任务。传统级联方法将自动语音识别（ASR）和机器翻译系统结合，依赖于中间的分割步骤，将转录流分割成句子样式的单位。然而，硬分割的引入限制了机器翻译系统，并且是错误的来源。本文提出了一种无需分割的框架，使模型能够在生成翻译之前延迟分割决策，翻译未分割的源流。广泛的实验证明，所提出的无需分割的框架在质量延迟平衡方面优于使用独立分割模型的竞争方法。论文通过后将公开发布软件、数据和模型。

    Streaming Machine Translation (MT) is the task of translating an unbounded input text stream in real-time. The traditional cascade approach, which combines an Automatic Speech Recognition (ASR) and an MT system, relies on an intermediate segmentation step which splits the transcription stream into sentence-like units. However, the incorporation of a hard segmentation constrains the MT system and is a source of errors. This paper proposes a Segmentation-Free framework that enables the model to translate an unsegmented source stream by delaying the segmentation decision until the translation has been generated. Extensive experiments show how the proposed Segmentation-Free framework has better quality-latency trade-off than competing approaches that use an independent segmentation model. Software, data and models will be released upon paper acceptance.
    
[^38]: 是否可能将文本修改到目标可读性水平？使用零射大型语言模型的初步调研

    Is it Possible to Modify Text to a Target Readability Level? An Initial Investigation Using Zero-Shot Large Language Models. (arXiv:2309.12551v1 [cs.CL])

    [http://arxiv.org/abs/2309.12551](http://arxiv.org/abs/2309.12551)

    这项研究旨在探讨将文本修改到目标可读性水平的可行性。研究提出了一项新颖的可读性控制文本修改任务，要求为每个输入文本生成多个不同目标可读性水平的版本，并引入了相应的度量标准。

    

    文本简化是一项常见任务，通过调整文本以使其更易于理解。类似地，文本拓展可以使一段更为复杂，为控制阅读理解测试的复杂度提供一种方法。然而，文本简化和拓展任务仅能相对地调整文本的可读性。直接将任意文本修改到绝对目标可读性水平对于满足多样化的受众是有用的。理想情况下，控制可读性的生成文本的可读性应独立于源文本。因此，我们提出了一项新颖的可读性控制文本修改任务。该任务要求为每个输入文本生成8个不同目标可读性水平的版本。我们引入了新颖的可读性控制文本修改度量标准。该任务的基线使用ChatGPT和Llama-2，扩展方法引入了一个两步流程（通过传递为生成释义）。

    Text simplification is a common task where the text is adapted to make it easier to understand. Similarly, text elaboration can make a passage more sophisticated, offering a method to control the complexity of reading comprehension tests. However, text simplification and elaboration tasks are limited to only relatively alter the readability of texts. It is useful to directly modify the readability of any text to an absolute target readability level to cater to a diverse audience. Ideally, the readability of readability-controlled generated text should be independent of the source text. Therefore, we propose a novel readability-controlled text modification task. The task requires the generation of 8 versions at various target readability levels for each input text. We introduce novel readability-controlled text modification metrics. The baselines for this task use ChatGPT and Llama-2, with an extension approach introducing a two-step process (generating paraphrases by passing through th
    
[^39]: LongDocFACTScore: 评估长文档生成摘要的实证性。

    LongDocFACTScore: Evaluating the Factuality of Long Document Abstractive Summarisation. (arXiv:2309.12455v1 [cs.CL])

    [http://arxiv.org/abs/2309.12455](http://arxiv.org/abs/2309.12455)

    LongDocFACTScore是一种评估长文档生成摘要实证性的评估框架，可以解决传统自动评估度量标准无法评估长文档摘要事实一致性的问题。

    

    保持事实一致性是生成性文本摘要中的一个关键问题，然而，传统的用于评估文本摘要的自动度量标准（如ROUGE得分）无法评估事实一致性。最近，人们致力于开发使用预训练语言模型来测量事实一致性的改进度量标准，但这些度量标准有限制性的令牌限制，因此不适用于评估长文档生成摘要。此外，目前有限的研究评估了现有自动评估度量标准在应用于长文档数据集时是否适用。在这项工作中，我们评估了自动度量标准在评估长文档生成摘要的事实一致性方面的功效，并提出了一种新的评估框架LongDocFACTScore。该框架允许度量标准扩展到任意长度的文档。该框架在与人类事实一致性度量的相关性方面优于现有的最先进度量标准。

    Maintaining factual consistency is a critical issue in abstractive text summarisation, however, it cannot be assessed by traditional automatic metrics used for evaluating text summarisation, such as ROUGE scoring. Recent efforts have been devoted to developing improved metrics for measuring factual consistency using pre-trained language models, but these metrics have restrictive token limits, and are therefore not suitable for evaluating long document text summarisation. Moreover, there is limited research evaluating whether existing automatic evaluation metrics are fit for purpose when applied to long document data sets. In this work, we evaluate the efficacy of automatic metrics at assessing factual consistency in long document text summarisation and propose a new evaluation framework LongDocFACTScore. This framework allows metrics to be extended to any length document. This framework outperforms existing state-of-the-art metrics in its ability to correlate with human measures of fac
    
[^40]: 用边异构图神经网络改进文章分类

    Improving Article Classification with Edge-Heterogeneous Graph Neural Networks. (arXiv:2309.11341v1 [cs.LG])

    [http://arxiv.org/abs/2309.11341](http://arxiv.org/abs/2309.11341)

    本论文提出了一种使用边异构图神经网络改进文章分类的方法，通过加入高阶语义的节点特征生成，能够显著提高分类性能。

    

    鉴于现有和新发布的文章数量庞大，将研究成果分类到特定上下文标签体系是一项具有挑战性和相关性的下游任务。我们提出了一种方法，通过使用边异构图表示来丰富简单的图神经网络（GNN）流水线，以提高文章分类的性能。我们使用SciBERT来生成节点特征，以捕捉文章的文本元数据中的高阶语义。我们在Open Graph Benchmark（OGB）ogbn-arxiv数据集和PubMed糖尿病数据集上进行了完全监督的传导式节点分类实验，分别通过Microsoft Academic Graph（MAG）和PubMed Central添加了附加元数据。结果表明，边异构图相对于边同构图，能够始终提高所有GNN模型的性能。转换后的数据使简单且浅层的GNN流水线能够与更复杂的架构相媲美的结果。

    Classifying research output into context-specific label taxonomies is a challenging and relevant downstream task, given the volume of existing and newly published articles. We propose a method to enhance the performance of article classification by enriching simple Graph Neural Networks (GNN) pipelines with edge-heterogeneous graph representations. SciBERT is used for node feature generation to capture higher-order semantics within the articles' textual metadata. Fully supervised transductive node classification experiments are conducted on the Open Graph Benchmark (OGB) ogbn-arxiv dataset and the PubMed diabetes dataset, augmented with additional metadata from Microsoft Academic Graph (MAG) and PubMed Central, respectively. The results demonstrate that edge-heterogeneous graphs consistently improve the performance of all GNN models compared to the edge-homogeneous graphs. The transformed data enable simple and shallow GNN pipelines to achieve results on par with more complex architect
    
[^41]: 面向神经信息检索的推理时间重排反馈

    Inference-time Re-ranker Relevance Feedback for Neural Information Retrieval. (arXiv:2305.11744v1 [cs.IR])

    [http://arxiv.org/abs/2305.11744](http://arxiv.org/abs/2305.11744)

    本文提出了一种利用重排器提供推理时间反馈来改进检索的方法，可以显著提高低召回率@ K下的检索性能。

    

    神经信息检索通常采用检索和重排框架：先使用双编码器网络检索K（例如100）个候选项，然后再使用更强大的交叉编码器模型对这些候选项进行重新排序，以使更好的候选项排名更高。重排器通常产生比检索器更好的候选分数，但仅限于查看前K个检索到的候选项，因此无法提高检索性能（以Recall @ K为度量）。在本文中，我们利用重排器通过提供推理时间相关反馈来改进检索。具体而言，我们利用重排器的预测对测试实例的重要信息进行了检索器查询表示的更新。我们的方法可以通过轻量级的推理时间蒸馏来实现，目的是使检索器的候选分数更接近于重排器的分数。然后使用更新后的查询向量执行第二个检索步骤。通过实验证明，我们的方法可以显著提高检索性能，特别是在低召回率@ K下。

    Neural information retrieval often adopts a retrieve-and-rerank framework: a bi-encoder network first retrieves K (e.g., 100) candidates that are then re-ranked using a more powerful cross-encoder model to rank the better candidates higher. The re-ranker generally produces better candidate scores than the retriever, but is limited to seeing only the top K retrieved candidates, thus providing no improvements in retrieval performance as measured by Recall@K. In this work, we leverage the re-ranker to also improve retrieval by providing inference-time relevance feedback to the retriever. Concretely, we update the retriever's query representation for a test instance using a lightweight inference-time distillation of the re-ranker's prediction for that instance. The distillation loss is designed to bring the retriever's candidate scores closer to those of the re-ranker. A second retrieval step is then performed with the updated query vector. We empirically show that our approach, which can 
    
[^42]: SmartBook：AI辅助的情报报告生成

    SmartBook: AI-Assisted Situation Report Generation. (arXiv:2303.14337v1 [cs.CL])

    [http://arxiv.org/abs/2303.14337](http://arxiv.org/abs/2303.14337)

    SmartBook是一种AI辅助的情报报告生成工具，通过消耗大量新闻数据生成一个结构化的情况报告，其中包含多个假设（主张），并与事实依据建立丰富的关联。在Ukraine-Russia危机中，机器生成的报告以时间轴的形式结构化，显著减少了报告时间，并通过生成比人类同行更全面、准确和一致的报告来提高报告质量。

    

    新兴事件，如COVID疫情和乌克兰危机，需要时间敏感的全面了解情况，以便进行适当的决策和有效的行动响应。自动生成情报报告可以大大减少领域专家准备官方人工策划报告的时间、精力和成本。然而，AI研究在这个目标方面非常有限，还没有成功的试验来自动化这种报告生成。我们提出了SmartBook，一种新颖的任务分解，旨在生成情况报告，在大量新闻数据的基础上生成一个结构化的情况报告，其中包含多个假设（主张），并与事实依据建立丰富的关联。我们通过自动生成情报分析报告来实现SmartBook，以协助专家分析师处理乌克兰-俄罗斯危机。机器生成的报告以时间轴的形式结构化，每个事件都与相关的演员、位置和因果关系相关联。我们的评估显示，SmartBook可以显著减少报告时间，并通过生成比人类同行更全面、准确和一致的报告来提高报告质量。

    Emerging events, such as the COVID pandemic and the Ukraine Crisis, require a time-sensitive comprehensive understanding of the situation to allow for appropriate decision-making and effective action response. Automated generation of situation reports can significantly reduce the time, effort, and cost for domain experts when preparing their official human-curated reports. However, AI research toward this goal has been very limited, and no successful trials have yet been conducted to automate such report generation. We propose SmartBook, a novel task formulation targeting situation report generation, which consumes large volumes of news data to produce a structured situation report with multiple hypotheses (claims) summarized and grounded with rich links to factual evidence. We realize SmartBook for the Ukraine-Russia crisis by automatically generating intelligence analysis reports to assist expert analysts. The machine-generated reports are structured in the form of timelines, with ea
    
[^43]: GesGPT: 基于 GPT 文本解析的语音手势合成

    GesGPT: Speech Gesture Synthesis With Text Parsing from GPT. (arXiv:2303.13013v1 [cs.CL])

    [http://arxiv.org/abs/2303.13013](http://arxiv.org/abs/2303.13013)

    GesGPT 是一种新的语音手势生成方法，它利用大型语言模型 GPT 的语义分析能力，从文本输入中提取与手势相关的信息，并使用手势库和集成模块生成意义丰富的语音手势。

    

    手势合成作为重要的研究领域，旨在生成上下文相关和自然的手势来对应语音或文本输入。尽管基于深度学习的方法取得了显著进展，但它们通常忽略了文本中丰富的语义信息，导致手势不够表达和意义不够丰富。我们提出了 GesGPT，这是一种利用大型语言模型（LLM），如 GPT 的语义分析能力进行手势生成的新方法。通过利用 LLM 在文本分析方面的强大能力，我们设计提示来从文本输入中提取与手势相关的信息。我们的方法包括开发提示原则，将手势生成转化为基于 GPT 的意图分类问题，并利用精心策划的手势库和集成模块生成语义丰富的语音手势。实验结果表明，GesGPT 可以有效地针对语音或文本输入生成上下文相关的、有意义的手势。

    Gesture synthesis has gained significant attention as a critical research area, focusing on producing contextually appropriate and natural gestures corresponding to speech or textual input. Although deep learning-based approaches have achieved remarkable progress, they often overlook the rich semantic information present in the text, leading to less expressive and meaningful gestures. We propose GesGPT, a novel approach to gesture generation that leverages the semantic analysis capabilities of Large Language Models (LLMs), such as GPT. By capitalizing on the strengths of LLMs for text analysis, we design prompts to extract gesture-related information from textual input. Our method entails developing prompt principles that transform gesture generation into an intention classification problem based on GPT, and utilizing a curated gesture library and integration module to produce semantically rich co-speech gestures. Experimental results demonstrate that GesGPT effectively generates conte
    
[^44]: 部分动员：跟踪俄罗斯媒体和电报之间的多语言信息流

    Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram. (arXiv:2301.10856v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2301.10856](http://arxiv.org/abs/2301.10856)

    本文研究了16个俄罗斯媒体机构和732个电报频道之间的互动，发现新闻媒体不仅通过电报传播现有的叙事，而且会从电报平台源材料，研究结果表明2.3％至26.7％的文章将主题归因于电报活动。

    

    在俄罗斯入侵乌克兰后，针对俄罗斯在线媒体的虚假信息和宣传，包括俄罗斯之声和卫星新闻在内的俄罗斯媒体在欧洲遭到禁止。为了保持观众数量，许多俄罗斯媒体开始在电报等消息服务上大力宣传其内容。在这项工作中，我们研究了2022年期间16家俄罗斯媒体机构如何与732个电报频道互动和利用。利用基础模型MPNet、DP-means聚类和Hawkes过程，我们跟踪新闻网站和电报频道之间的叙事传播情况。我们表明，新闻媒体不仅通过电报传播现有的叙事，而且他们会从电报平台源材料。在我们研究的网站中，2.3％（ura.news）至26.7％（ukraina.ru）的文章讨论了源于/导致电报活动的内容。最后，通过跟踪个别主题的扩散，我们测量新闻网站发表文章的速率。

    In response to disinformation and propaganda from Russian online media following the Russian invasion of Ukraine, Russian outlets including Russia Today and Sputnik News were banned throughout Europe. To maintain viewership, many of these Russian outlets began to heavily promote their content on messaging services like Telegram. In this work, we study how 16 Russian media outlets interacted with and utilized 732 Telegram channels throughout 2022. Leveraging the foundational model MPNet, DP-means clustering, and Hawkes Processes, we trace how narratives spread between news sites and Telegram channels. We show that news outlets not only propagate existing narratives through Telegram, but that they source material from the messaging platform. Across the sites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of articles discuss content that originated/resulted from activity on Telegram. Finally, tracking the spread of individual topics, we measure the rate at which news website
    

