# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization.](http://arxiv.org/abs/2305.11095) | 本文通过提示工程技术调整Whisper模型，成功适应未见过的三个任务，并提出的提示比默认提示性能提升了10%到45％，展现了Whisper模型的鲁棒性和多语言理解能力。 |
| [^2] | [Inspecting the Geographical Representativeness of Images from Text-to-Image Models.](http://arxiv.org/abs/2305.11080) | 本文研究了文本到图像模型生成的图像的地理代表性，发现这些模型生成的图像大多反映美国和印度等国家的环境，而对于其他国家的反映较少，因此需要更多关注地理多样性。 |
| [^3] | [A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks.](http://arxiv.org/abs/2305.11073) | 本研究比较了E-Branchformer和Conformer在语音识别、翻译和理解任务中的性能，结果表明E-Branchformer在多个基准测试中表现相当甚至更好，并且更加稳定。 |
| [^4] | [Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering.](http://arxiv.org/abs/2305.11072) | 提出了一种自监督学习方法Spin，通过说话者不变聚类来解开说话者信息并保留内容表示，只需45分钟微调即可改进预训练网络，在语音识别和声学单元发现中优于先前的方法。 |
| [^5] | [Enriching language models with graph-based context information to better understand textual data.](http://arxiv.org/abs/2305.11070) | 基于图形上下文信息的语言模型增强可更好的理解文本，实验表明该方法提高了BERT模型在Pubmed上的分类任务表现。 |
| [^6] | [ORKG-Leaderboards: A Systematic Workflow for Mining Leaderboards as a Knowledge Graph.](http://arxiv.org/abs/2305.11068) | ORKG-Leaderboards是一种以知识图谱形式挖掘AI领域排行榜并支持机器可操作性出版的系统，能够让研究人员透明地了解全球研究人员的实证结果，跟踪人工智能的进展情况。 |
| [^7] | [Bits of Grass: Does GPT already know how to write like Whitman?.](http://arxiv.org/abs/2305.11064) | 本文研究了GPT模型在生成特定作者风格诗歌方面的能力，结果表明即使提供了大量样本，未经微调的模型也不能生成所需风格的诗歌。 |
| [^8] | [SPSQL: Step-by-step Parsing Based Framework for Text-to-SQL Generation.](http://arxiv.org/abs/2305.11061) | 本文提出了一种基于流水线的文本转SQL方法: SPSQL，它将任务分解成四个子任务，从而减少了模型难度和对训练数据的要求，提高了准确率。 |
| [^9] | [BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval.](http://arxiv.org/abs/2305.11052) | BERM使用平衡的、可提取的特征表示法来捕捉匹配信号，从而提高了密集检索的泛化性能。 |
| [^10] | [Learning In-context Learning for Named Entity Recognition.](http://arxiv.org/abs/2305.11038) | 本文提出了一种在 PLMs 中注入上下文 NER 能力的方法，只需少量示意实例即可动态识别新类型的实体，在几个基准数据集上达到了最先进的性能。 |
| [^11] | [Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words Extraction with Wordpieces and Aspect Enhancement.](http://arxiv.org/abs/2305.11034) | 本文提出了一种使用Wordpieces替换语法树来进行以目标为导向的意见词提取的方法，并使用句子 - 方面对来增强性能。该方法在基准数据集上取得了最先进的结果。 |
| [^12] | [Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction.](http://arxiv.org/abs/2305.11029) | 本文提出了一种使用不确定性引导的标签去噪技术，可以有效准确地在文档级远程关系抽取中选择可信的伪标签，提高了性能表现，并在DocRED数据集上实现了新的最佳性能。 |
| [^13] | [Generalized Multiple Intent Conditioned Slot Filling.](http://arxiv.org/abs/2305.11023) | 本文针对现实场景下意图重复的问题，对词槽填充进行了泛化。通过结合数据集进行预训练，使用语言模型进行 JSON 生成任务处理，T5 模型能够泛化到未见过的意图类型。 |
| [^14] | [FunASR: A Fundamental End-to-End Speech Recognition Toolkit.](http://arxiv.org/abs/2305.11013) | 本文介绍了一款名为FunASR的基础语音识别工具箱，其中的Paraformer模型是一个在60万小时语音数据集上训练的非自回归的端到端语音识别模型，工具箱还提供了包括语音活动检测和文本标点功能在内的多个工具模块，以提高语音识别性能和易用性。 |
| [^15] | [Taxonomy Completion with Probabilistic Scorer via Box Embedding.](http://arxiv.org/abs/2305.11004) | 本文提出了一种新方法TaxBox，该方法将分类法概念映射到框嵌入中，并使用两个概率评分器来进行概念附加和插入，避免使用伪叶。实验表明，在二个基准数据集上，TaxBox在准确性和训练效率方面显著优于现有的方法。 |
| [^16] | [SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities.](http://arxiv.org/abs/2305.11000) | SpeechGPT是一个具有本质跨模态会话能力的大型语言模型，能够感知和生成多模态内容，可按照多模态人类指令的能力，并突显了使用一个模型处理多个模态的潜力。 |
| [^17] | [The Web Can Be Your Oyster for Improving Large Language Models.](http://arxiv.org/abs/2305.10998) | 本文提出了一种新的大型语言模型增强方法，即使用自适应搜索引擎辅助学习方法，将大规模网络数据与LLMs融合，从而避免无用或嘈杂的增强。 |
| [^18] | [How does the task complexity of masked pretraining objectives affect downstream performance?.](http://arxiv.org/abs/2305.10992) | 本文研究了掩码预训练任务的复杂度对下游任务表现的影响，并发现更复杂的任务可以实现更好的结果。这一发现表明，掩码预训练任务可以通过增加其复杂度来提高其性能。 |
| [^19] | [Less is More! A slim architecture for optimal language translation.](http://arxiv.org/abs/2305.10991) | 该论文提出了一种名为 KgV 的 sigmoid 门控机制，通过嵌入层剪枝减少了模型的大小，同时引入了 H-SoftPOS 层次嵌入层进一步改进嵌入以显著减少模型参数，从而在 WMT14 英德验证集上使 perplexity 减少了三倍以上。 |
| [^20] | [Multi-CrossRE A Multi-Lingual Multi-Domain Dataset for Relation Extraction.](http://arxiv.org/abs/2305.10985) | Multi-CrossRE是一个多领域、多语言的数据集，包括26种语言在内，提供给关系抽取研究使用。 |
| [^21] | [NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian Movie Sentiment Classification.](http://arxiv.org/abs/2305.10971) | 本论文提出了一个新的数据集NollySenti，用于不同领域适应的情感分类任务，基于尼日利亚五种常用语言的诺利木电影评论。研究表明，从具有相同目标域的英语进行迁移可以提高5％以上的准确性。 |
| [^22] | [Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation.](http://arxiv.org/abs/2305.10951) | 本研究探究了利用数据增强技术，特别是自训练方法，提高低资源语音识别的性能，取得了成功，证明这是一种可行的方法。 |
| [^23] | [On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation.](http://arxiv.org/abs/2305.10930) | 零样本多语言神经机器翻译容易出现“离谱问题”，本文提出的简单且有效的算法LAVS可以通过增加语言之间的KL分歧显著降低这个问题。 |
| [^24] | [Multilingual Event Extraction from Historical Newspaper Adverts.](http://arxiv.org/abs/2305.10928) | 本文针对历史文本中未被充分开发的事件提取任务，介绍了一个新的多语言数据集，通过迁移学习和领域自适应技术，即使数据注释很少也可以获得出人意料的好结果。同时，所提出的模型提供了新的洞察力，展示了该时期奴役话语的语言使用和模式。研究表明，语言迁移可以有效用于历史文本中的多语言事件提取。 |
| [^25] | [Query Performance Prediction: From Ad-hoc to Conversational Search.](http://arxiv.org/abs/2305.10923) | 本文研究了针对从Ad-hoc到交互式搜索中查询性能预测(QPP)的有效方法，并探索了QPP方法在交互式搜索中是否具有推广应用的能力。 |
| [^26] | [Emergent Communication with Attention.](http://arxiv.org/abs/2305.10920) | 该论文研究了在计算代理间如何实现更好地紧急通信的方法，并发现引入注意力机制可以得到更具组成性和可解释性的紧急语言。 |
| [^27] | [Weakly-Supervised Visual-Textual Grounding with Semantic Prior Refinement.](http://arxiv.org/abs/2305.10913) | 本文提出了一种利用语义先验细化的弱监督视觉-文本对齐方法，仅使用图像-句子对进行学习，其目标是实现实体表示中的区域-短语对应关系，通过联合两个主要模块的输出进行预测。 |
| [^28] | [Take a Break in the Middle: Investigating Subgoals towards Hierarchical Script Generation.](http://arxiv.org/abs/2305.10907) | 本文提出了从认知理论的角度扩展目标导向的脚本生成任务，并建立了一个新的数据集，验证了将子目标纳入层次脚本生成的有效性。 |
| [^29] | [EventNet-ITA: Italian Frame Parsing for Events.](http://arxiv.org/abs/2305.10892) | 本文介绍了一个用于意大利语的大规模、多领域事件框架标注语料库-EventNet-ITA，并提出了一种高效的多标签框架解析方法。EventNet-ITA的主要贡献是为研究人员提供了一个用于文本事件挖掘的资源和意大利语框架解析的新颖工具。 |
| [^30] | [TEPrompt: Task Enlightenment Prompt Learning for Implicit Discourse Relation Recognition.](http://arxiv.org/abs/2305.10866) | 该论文提出一种任务启发提示学习模型（TEPrompt），用于隐式语篇关系识别，其通过融合与任务相关的特征来改善IDRR性能。 |
| [^31] | [Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2305.10865) | 该论文提出了一种多智能体强化学习中的新方法SAMA，通过提前训练的语言模型和任务分解来解决ASG方法存在的样本效率问题和生成非实际任务奖励的子目标的问题。 |
| [^32] | [Advancing Full-Text Search Lemmatization Techniques with Paradigm Retrieval from OpenCorpora.](http://arxiv.org/abs/2305.10848) | 本文介绍了一种利用OpenCorpora数据集和定制算法增强全文搜索词形还原的方法，同时提出了一种紧凑的词典存储策略，大大提高了词形检索的速度和精度。 |
| [^33] | [Large Language Models can be Guided to Evade AI-Generated Text Detection.](http://arxiv.org/abs/2305.10847) | 本文揭示了大型语言模型可以通过精心设计的提示语来有效规避现有的文本检测系统，证明了这些检测器的脆弱性。 |
| [^34] | [TAPIR: Learning Adaptive Revision for Incremental Natural Language Understanding with a Two-Pass Model.](http://arxiv.org/abs/2305.10845) | TAPIR使用双通道模型实现自适应修订增量自然语言理解，并取得了在ATIS数据集上最先进的性能。 |
| [^35] | [A Lexical-aware Non-autoregressive Transformer-based ASR Model.](http://arxiv.org/abs/2305.10839) | 此论文提出了一个基于词汇感知的非自回归Transformer语音识别模型，该模型集成了语音-文本共享编码器和解码器，并能够同时训练语音和文本数据以提高性能，实验结果显示该模型在多个数据集上拥有领先的性能表现。 |
| [^36] | [Ahead-of-Time P-Tuning.](http://arxiv.org/abs/2305.10835) | 本文提出了 Ahead-of-Time （AoT）P-Tuning，一种新颖的微调方法，它通过在每个Transformer层之前添加输入相关的偏置，实现了应用于预训练语言模型的参数节约。该方法在GLUE和SuperGLUE基准数据集上优于BitFit，并可用于多任务推理，而推理开销却很小。 |
| [^37] | [AIwriting: Relations Between Image Generation and Digital Writing.](http://arxiv.org/abs/2305.10834) | 本论坛讨论了基于AI的文本生成系统和文本到图像生成系统对数字艺术和电子文学领域带来的影响，各种作品都从文学角度考虑，突出了创作过程的交互性。 |
| [^38] | [Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants.](http://arxiv.org/abs/2305.10833) | 本研究使用深度学习模型识别对话中基于隐喻的花和植物名称，鉴别模型表现优于GPT-3.5，最好的表现器在任务中报告了92.2349％的F1分数。 |
| [^39] | [CLEME: Debiasing Multi-reference Evaluation for Grammatical Error Correction.](http://arxiv.org/abs/2305.10819) | 提出了一种新的Chunk-LEvel Multi-reference Evaluation (CLEME)方法来解决在多参考环境下评估GEC系统时存在的偏差问题，其通过消除由不一致的编辑边界引起的偏差和自动确定语法错误的边界来提高了GEC评估的可靠性。 |
| [^40] | [Democratized Diffusion Language Model.](http://arxiv.org/abs/2305.10818) | 本文提出了一个基于CDCD框架的民主扩散语言模型（DDLM），并通过GLUE基准测试了其知识转移能力，为研究人员提供了DDLM训练和评估流程以及已训练的DDLM模型。 |
| [^41] | [Whisper-KDQ: A Lightweight Whisper via Guided Knowledge Distillation and Quantization for Efficient ASR.](http://arxiv.org/abs/2305.10788) | 本文提出了一种通过引导知识蒸馏和量化，实现对大型预训练语音识别模型Whisper进行压缩优化的方法，可以将模型大小缩小并提高性能。 |
| [^42] | [Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings.](http://arxiv.org/abs/2305.10786) | 提出了一种名为Ditto的简单有效的方法，其可以解决预训练语言模型中存在的各向异性问题，并在语义文本相似性任务中提高模型的性能。 |
| [^43] | [Counterfactual Debiasing for Generating Factually Consistent Text Summaries.](http://arxiv.org/abs/2305.10736) | 本文提出了一个名为CoFactSum的去偏框架，通过反事实估计减轻原因，解决了生成文本摘要的事实不一致性问题，并在两个广泛使用的数据集上取得了优于现有模型的效果。 |
| [^44] | [Diffusion-Based Speech Enhancement with Joint Generative and Predictive Decoders.](http://arxiv.org/abs/2305.10734) | 本文提出了一种集成生成和预测信息的基于扩散的语音增强系统，其中两个语音增强模块在第一和最后一个扩散步骤中被融合，实验结果表明扩散评分估计可以从预测信息中受益并加快解码过程。 |
| [^45] | [Analyzing Norm Violations in Live-Stream Chat.](http://arxiv.org/abs/2305.10731) | 本文分享了第一次针对在直播平台上检测规范违背现象的自然语言处理研究，通过分类别标注和用户研究，揭示了直播聊天规范形成的见解。 |
| [^46] | [Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency.](http://arxiv.org/abs/2305.10713) | 本论文提出了一种新的度量--Prompt平坦度，可以优化语言提示选择，提高模型分类的准确性和样本效率，实验证明结合现有度量可以提高性能和样本效率。 |
| [^47] | [ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval.](http://arxiv.org/abs/2305.10703) | 本文提出了一种基于检索增强的框架，通过渐进式密集检索从通用领域的无标签语料库中创建训练数据，实现了零样本文本分类，相较于最强的基线模型提高了4.3%的性能，与使用大型NLG模型的基线相比节省了约70％的时间。 |
| [^48] | [MolXPT: Wrapping Molecules with Text for Generative Pre-training.](http://arxiv.org/abs/2305.10688) | MolXPT是一个文本包装的统一语言模型，使用SMILES作为输入，可以提高分子模型的性能表现，并且使得基于零shot的分子生成成为可能。 |
| [^49] | [RMSSinger: Realistic-Music-Score based Singing Voice Synthesis.](http://arxiv.org/abs/2305.10686) | RMSSinger是基于真实乐谱的歌声合成，采用单词级建模避免了转录误差，方便且灵活。 |
| [^50] | [Accurate and Reliable Confidence Estimation Based on Non-Autoregressive End-to-End Speech Recognition System.](http://arxiv.org/abs/2305.10680) | 本文介绍了一种CIF-Aligned置信度估计模型，利用了非自回归E2E ASR模型-Paraformer的特性，生成符号同步的声学嵌入，实现了准确可靠的置信度估计。 |
| [^51] | [Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation.](http://arxiv.org/abs/2305.10679) | 本文介绍了一个名为Brainstorm的框架，利用头脑风暴步骤生成并选择关于问题的不同想法，可显著增强大型语言模型（LLMs）解决竞争级别编程问题的能力，结果在CodeContests基准测试中，ChatGPT的pass@$k$指标增加了50％以上。 |
| [^52] | [a unified front-end framework for english text-to-speech synthesis.](http://arxiv.org/abs/2305.10666) | 该论文提出了一个统一的前端框架，捕捉了英文语音合成前端模块之间的依赖关系，并且在所有模块中均取得了最先进的性能。 |
| [^53] | [Speech Separation based on Contrastive Learning and Deep Modularization.](http://arxiv.org/abs/2305.10652) | 本文提出了一种基于对比学习和深度模块化的完全无监督语音分离方法，解决了有监督学习中存在的排列问题、说话人数量不匹配的问题和高质量标记数据的依赖问题。 |
| [^54] | [ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs.](http://arxiv.org/abs/2305.10649) | 本文提出了ZeroPrompt和对应的Prompt-and-Refine策略，可以降低流式语音识别模型的TDT，而且不会损失精度，具有工程便捷性，能够在任何数据集上应用。 |
| [^55] | [BioAug: Conditional Generation based Data Augmentation for Low-Resource Biomedical NER.](http://arxiv.org/abs/2305.10647) | 本文提出了一种基于条件生成的数据增强框架BioAug，用于低资源生物医学命名实体识别。BioAug建立在BART上，通过选择性的屏蔽和知识增强进行训练。实验展示了BioAug在5个基准BioNER数据集上的有效性，且表现优于所有基线。 |
| [^56] | [Are Large Language Models Fit For Guided Reading?.](http://arxiv.org/abs/2305.10645) | 本文评估大型语言模型在指导阅读中的应用能力，发现它们能够生成高质量的有意义问题，具有多样性且涵盖输入文本中大多数主题，同时能够有效地总结回答和推荐重新阅读的部分。 |
| [^57] | [Language Models Meet World Models: Embodied Experiences Enhance Language Models.](http://arxiv.org/abs/2305.10626) | 本文提出一种新的增强语言模型的方法——将其与世界模型相结合，通过有目的的规划和随机探索获得丰富的实体经验进行微调, 以提高其在物理环境下的推理和行为能力，并在语言基准上保持或提高性能。 |
| [^58] | [ML-SUPERB: Multilingual Speech Universal PERformance Benchmark.](http://arxiv.org/abs/2305.10615) | 本文提出了一个覆盖143种语言、用于自我监督学习模型性能基准的多语种语音基准 ML-SUPERB，并发现自我监督学习模型可以显著提高性能且多语种模型不总是比单语言模型表现更好。 |
| [^59] | [Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning.](http://arxiv.org/abs/2305.10613) | 本文旨在探究是否能够使用大型语言模型进行时态知识图谱预测，尤其是不需要任何显式模块。结果表明，大型语言模型在此类预测中表现良好，并且可以隐式有效地编码上下文和时间信息。 |
| [^60] | [Solving Cosine Similarity Underestimation between High Frequency Words by L2 Norm Discounting.](http://arxiv.org/abs/2305.10610) | 通过L2范数折扣来解决高频词的余弦相似度低估问题 |
| [^61] | [Tree of Thoughts: Deliberate Problem Solving with Large Language Models.](http://arxiv.org/abs/2305.10601) | 本研究提出了一种新的推理框架——思维之树（ToT），可以增强语言模型的问题解决能力，帮助语言模型进行深思熟虑的决策，以及自我评估和全局选择。 |
| [^62] | [A Better Way to Do Masked Language Model Scoring.](http://arxiv.org/abs/2305.10588) | 本文提出了一种更好的掩码语言模型评分方法，即PLL-word-l2r，用于估计句子的伪对数似然得分，相对于原PLL方法和屏蔽所有单词标记的PLL评分方法，改进的度量方法更好地针对字汇外单词得分问题进行了解决。 |
| [^63] | [From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?.](http://arxiv.org/abs/2305.10568) | 本文研究了语言模型对于名词复合词的理解能力，提出了名词复合词解释的任务和名词复合词概念化的任务，并发现GPT-3在这些任务中的表现优于人类。 |
| [^64] | [Investigating the Effect of Hard Negative Sample Distribution on Contrastive Knowledge Graph Embedding.](http://arxiv.org/abs/2305.10563) | 本文探究了负采样分布对对比知识图谱嵌入的影响，提出考虑硬度和结构的对比（HaSa）算法，用于去除假负样本，提高知识图谱补全任务的性能。 |
| [^65] | [Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search.](http://arxiv.org/abs/2305.10561) | 本文介绍了一种跨语言的零样本事件抽取系统和用户界面，通过仅使用英语训练数据，能够在100种不同语言的文本中进行全球事件的抽取、可视化和搜索。同时，该系统还能够进行跨语言的以事件为中心的搜索。 |
| [^66] | [Bring More Attention to Syntactic Symmetry for Automatic Postediting of High-Quality Machine Translations.](http://arxiv.org/abs/2305.10557) | 这篇论文提出了一种新的方法来解决自动后编辑系统无法处理高质量机器翻译的问题，该方法通过对给定MT进行对称自我关注的损失函数进行正则化，从而提高了自动后编辑的质量。 |
| [^67] | [Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems.](http://arxiv.org/abs/2305.10528) | 本文提出了一种在自学习对话系统中利用历史回归事件报告来验证、保护和改进政策的方法，以解决在大规模商业环境中的经验连贯性和政策改进之间的平衡问题。 |
| [^68] | [Statistical Knowledge Assessment for Generative Language Models.](http://arxiv.org/abs/2305.10519) | 本文介绍了一个统计知识评估框架，用于评估生成式语言模型（GLMs）的知识输出。通过对14种GLMs进行综合比较，发现知识遵循缩放定律，而对指令遵循数据进行微调可能会损害模型的生成能力。 |
| [^69] | [IMAD: IMage-Augmented multi-modal Dialogue.](http://arxiv.org/abs/2305.10512) | 该论文提出了一种新颖的多模态对话系统视角，其中图像解释是基于对话上下文的，同时提出了一个两阶段观点来构建多模态对话数据集。 |
| [^70] | [ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource Languages.](http://arxiv.org/abs/2305.10510) | ChatGPT在翻译中延续性别偏见，忽略非性别代词，会将性别中立代词转换为男性或女性，甚至无法将英语中的性别中立代词翻译为其他语言的性别中立代词。 |
| [^71] | [Incorporating Attribution Importance for Improving Faithfulness Metrics.](http://arxiv.org/abs/2305.10496) | 本研究提出了一种软删除标准来评估归因方法的忠实度，该方法随机遮盖标记的部分向量表示，这种方法比现有的硬删除标准更准确。 |
| [^72] | [Understanding of Normal and Abnormal Hearts by Phase Space Analysis and Convolutional Neural Networks.](http://arxiv.org/abs/2305.10450) | 通过相空间分析和卷积神经网络结合诊断心脏疾病，在MIT-BIH数据库上取得了93.3%的平均准确率。 |
| [^73] | [Sequence-to-Sequence Pre-training with Unified Modality Masking for Visual Document Understanding.](http://arxiv.org/abs/2305.10448) | 本文提出了一个统一的序列到序列文档理解模型，采用跨三种模态的统一掩码进行预训练，并且结构灵活适应各种下游任务输出格式。模型采用多种任务同时预训练，而且结合分解注意力和模态专家组合策略以提高信息捕获效率。 |
| [^74] | [The Effectiveness of a Dynamic Loss Function in Neural Network Based Automated Essay Scoring.](http://arxiv.org/abs/2305.10447) | 本研究提出了一种动态损失函数，帮助神经网络自动化评分系统在预测值的同时对正确的分布有更高的预测能力，而不牺牲任何性能。 |
| [^75] | [Emotion Recognition based on Psychological Components in Guided Narratives for Emotion Regulation.](http://arxiv.org/abs/2305.10446) | 本文在情绪调节指导的叙述中引入了一个新的法语情感叙述语料库，研究了四个组成部分（行为、感觉、思考和领域）对情感分类的影响，并证明共同考虑所有组成部分可以获得最佳结果。 |
| [^76] | [Memorization for Good: Encryption with Autoregressive Language Models.](http://arxiv.org/abs/2305.10445) | 该论文提出了第一个使用自回归语言模型进行对称加密的算法（SELM），其中算法可以将任意数据编码为紧凑的实值向量（即加密），然后通过随机子空间优化和贪心解码将向量无损解码为原始消息（即解密），并且SELM在加密分析方面的安全性能较高。 |
| [^77] | [IMAGINATOR: Pre-Trained Image+Text Joint Embeddings using Word-Level Grounding of Images.](http://arxiv.org/abs/2305.10438) | IMAGINATOR是一个使用基于单词级别图像本体的预训练图像+文本联合嵌入，能将多模态数据编码为矢量空间。 |
| [^78] | [SmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and Visual Cues.](http://arxiv.org/abs/2305.10436) | 本文研究了利用大型语言模型自动生成口头和视觉提示的关键词记忆法。通过人类参与者实验，我们的方法明显优于传统闪卡，并可与手动制作的关键词记忆法相媲美。 |
| [^79] | [Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions.](http://arxiv.org/abs/2305.10435) | 生成的预训练变形器是一种基于变形器架构的深度神经网络，能够在自然语言处理任务中表现出色且有效地进行对话，具有广泛的潜在应用，但仍面临新兴挑战和局限性。 |
| [^80] | [Learning the Visualness of Text Using Large Vision-Language Models.](http://arxiv.org/abs/2305.10434) | 该论文利用大型视觉语言模型如CLIP来检测文本的视觉性，并提出fine-tuning策略，将非视觉文本映射为NULL图像，匹配视觉文本与对应图像，以解锁在文本中嵌入相关图像的能力。 |
| [^81] | [Toxicity Inspector: A Framework to Evaluate Ground Truth in Toxicity Detection Through Feedback.](http://arxiv.org/abs/2305.10433) | 本文介绍了一个毒性检测框架，通过迭代反馈循环提高毒性数据集的可靠性，并通过两种指标平衡性能和毒性避免之间的权衡。 |
| [^82] | [Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents.](http://arxiv.org/abs/2305.10383) | 本文研究使用生成语言模型GPT-4进行大规模文本分析，在US AI专利中发现公共价值表达。采用高级布尔查询收集了154,934个专利文档，并与USPTO的完整专利文本合并。得出5.4百万句子的语料库，使用框架以及GPT-4提示进行标记和理性化。评估结果表明，这种方法很准确。 |
| [^83] | [FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy.](http://arxiv.org/abs/2305.10307) | FACE是一组可以有效识别人类和模型之间差距的度量标准。它基于傅里叶分析和交叉熵估计，可以反映模型大小、解码采样方法和人类评分。 |
| [^84] | [MemoryBank: Enhancing Large Language Models with Long-Term Memory.](http://arxiv.org/abs/2305.10250) | MemoryBank 提出了一种新型内存机制，旨在为大型语言模型提供类人的长期记忆。它可以召唤相关记忆，通过持续的记忆更新不断进化，通过合成过去的互动信息理解并适应用户个性。 |
| [^85] | [EfficientSCI: Densely Connected Network with Space-time Factorization for Large-scale Video Snapshot Compressive Imaging.](http://arxiv.org/abs/2305.10006) | 本文提出了EfficientSCI网络，通过使用稠密连接和时空分解机制来建立视频SCI中的空间-时间相关性。相比最先进的基于深度学习的方法，它能够在计算效率和重建质量方面取得更好的表现。 |
| [^86] | ["I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation.](http://arxiv.org/abs/2305.09941) | 本论文研究了如何以TGNB人群的声音为中心，评估开放式语言生成中的偏见。通过理解TGNB个体的经历，提出了以TGNB人群为中心的OLG系统评估框架，并且包括一个为TGNB人群设计的调查工具和分析方法。 |
| [^87] | [Epsilon Sampling Rocks: Investigating Sampling Strategies for \\Minimum Bayes Risk Decoding for Machine Translation.](http://arxiv.org/abs/2305.09860) | 本文研究了用于机器翻译最小贝叶斯风险解码的不同采样策略，并发现了epsilon采样方式能够使得解码结果显著地优于其他所有已测试的采样方式和束搜索解码。 |
| [^88] | [DarkBERT: A Language Model for the Dark Side of the Internet.](http://arxiv.org/abs/2305.08596) | 本研究介绍了一个在暗网数据上预先训练的语言模型——DarkBERT，对于暗网的研究具有重要的价值。 |
| [^89] | [NLG Evaluation Metrics Beyond Correlation Analysis: An Empirical Metric Preference Checklist.](http://arxiv.org/abs/2305.08566) | 本研究提出了一种度量偏好检查表，以超越相关分析评估NLG自动指标，并分析了两种类型的指标及其在三个任务中的效果。 |
| [^90] | [A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization.](http://arxiv.org/abs/2305.08503) | 本研究提出了一种用于抽象多文档摘要的层次编码-解码方案，在多领域的10个MDS数据集上测试表现最佳。 |
| [^91] | [Parameter-Efficient Fine-Tuning with Layer Pruning on Free-Text Sequence-to-Sequence modeling.](http://arxiv.org/abs/2305.08285) | 本文提出了一个将LoRA和结构化层剪枝方法结合的框架，在保持超过92%生成质量的同时，通过调整仅0.6%的参数并剪枝超过30%的Transformer层，成功减少了50%的GPU内存使用并提升了100%的训练速度。 |
| [^92] | [Learning to Generalize for Cross-domain QA.](http://arxiv.org/abs/2305.08208) | 提出了一种不增加训练成本的跨域问答泛化学习方法，通过结合提示方法和线性探测再微调策略，有效提高了产生式和判别式模型的泛化能力，取得了优于基准方法4.5%-7.9%的结果。 |
| [^93] | [Self-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations.](http://arxiv.org/abs/2305.08099) | 本文提出了一种自监督神经因子分析模型，使用HuBERT中的聚类方法来发现隐藏的声学单元，并使用这些单元对齐SSL模型的特征，从而产生解耦后的语音表示，从而为专门任务提供了一种基于Utterance水平的无监督学习目标。实验结果表明，SSNFA模型在说话人识别、语言识别和情感识别等各种任务中均显著优于现有的SSL模型，并且没有任何特定任务的微调或监督。 |
| [^94] | [GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content.](http://arxiv.org/abs/2305.07969) | GPT-Sentinel通过语言模型检测ChatGPT生成的文本和人类编写的文本，其模型在测试数据集上准确率超过了97％，并且揭示了区分这两种文本关键特征的能力。 |
| [^95] | [Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation.](http://arxiv.org/abs/2305.07804) | 本论文介绍了一种名为Dr. LLaMA的方法，通过使用大型语言模型进行生成式数据增强，以改善小语言模型的性能，特别是在医学问答任务中。这种方法在微调后使模型性能提高，并提出了在特定领域问答任务中使用LLM所面临的挑战和潜在的研究方向。 |
| [^96] | [Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding.](http://arxiv.org/abs/2305.07424) | 本文提出了IS-CSE方法，通过实例平滑对比学习来学习无监督句子嵌入，以平滑嵌入在特征空间中的边界，从而提高模型的泛化性能。在标准的STS任务中取得了良好的得分。 |
| [^97] | [Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation.](http://arxiv.org/abs/2305.07375) | 本文对ChatGPT的因果推理能力进行了首次全面评估，实验证明ChatGPT是一个好的因果解释者，但不是一个好的因果推理者，存在严重的因果幻觉问题，对于明确的因果关系表现良好。 |
| [^98] | [A Framework for Designing Foundation Model based Systems.](http://arxiv.org/abs/2305.05352) | 本文提出了一个基于基础模型的系统分类体系，分类和比较了基础模型和基于基础模型的系统的特点。它为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。 |
| [^99] | [Distilling Script Knowledge from Large Language Models for Constrained Language Planning.](http://arxiv.org/abs/2305.05252) | 本文首次定义了受限语言规划任务，提出了一种方法来提高大型语言模型在这个任务中的表现，并提取了一个新颖的受限语言规划数据集。实验证明该方法显著提高了其在约束忠实度方面的能力，并对赋予较小的语言模型受限语言规划能力非常有效。 |
| [^100] | [Augmented Large Language Models with Parametric Knowledge Guiding.](http://arxiv.org/abs/2305.04757) | 这篇论文提出了一种带参数知识引导的增强型大语言模型框架，通过为LLMs装备信息引导模块来访问相关知识，同时保持LLMs的参数不变。这个框架可以提高黑盒LLMs在各种NLP任务上的性能。 |
| [^101] | [Unified Model Learning for Various Neural Machine Translation.](http://arxiv.org/abs/2305.02777) | 本文提出了一种统一学习方法，即统一模型学习，可以同时适用于翻译各种任务数据，并实现智能按需翻译，相对现有的特定数据集模型能够得到明显的改进。 |
| [^102] | [Unlimiformer: Long-Range Transformers with Unlimited Length Input.](http://arxiv.org/abs/2305.01625) | Unlimiformer是一种Transformer模型的通用方法，可以将所有层的注意计算卸载到单个k近邻索引上，从而可处理无限长度的输入，而不增加额外的学习负担。 |
| [^103] | [Nondeterministic Stacks in Neural Networks.](http://arxiv.org/abs/2304.12955) | 本论文提出在神经网络中添加了可以处理句法歧义的非确定性栈，有效地模拟一个非确定性下推自动机。 |
| [^104] | [Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading.](http://arxiv.org/abs/2304.10784) | Eyettention是第一个同时处理语言序列和时间序列的阅读模型，可以更准确地模拟阅读者的扫视路径，对机器学习的自然语言处理模型具有借鉴意义。 |
| [^105] | [Multi-step Jailbreaking Privacy Attacks on ChatGPT.](http://arxiv.org/abs/2304.05197) | 本文探讨了来自OpenAI模型API以及New Bing增强版的ChatGPT对隐私带来的风险，指出应用程序集成的LLMs可能导致比以往更严重的隐私威胁，实验支持该观点。 |
| [^106] | [Sociocultural knowledge is needed for selection of shots in hate speech detection tasks.](http://arxiv.org/abs/2304.01890) | HATELEXICON是一个包含巴西，德国，印度和肯尼亚仇恨言论的词汇表，利用其可以提高模型在训练中的性能表现。 |
| [^107] | [Neural Comprehension: Language Models with Compiled Neural Networks.](http://arxiv.org/abs/2304.01665) | 本文探讨了如何将编译神经网络CoNNs并入语言模型的架构中，以使语言模型在复合任务中提高性能，特别是在需要深入理解抽象规则的领域。方法称为“神经理解”，提高了语言模型在符号操作、规则推理、算术推理等方面的准确度。 |
| [^108] | [The Role of Semantic Parsing in Understanding Procedural Text.](http://arxiv.org/abs/2302.06829) | 本文探究了通过语义解析获取的符号语义表示对于推理实体状态的作用，并提出了基于符号解析和语义解析信息的过程推理框架，实验结果表明融合语义知识可提高过程理解能力。 |
| [^109] | [Unifying Molecular and Textual Representations via Multi-task Language Modelling.](http://arxiv.org/abs/2301.12586) | 本文提出了第一个多任务语言模型，可以同时解决化学和自然语言领域的各种任务，无需昂贵的单一域或任务特定模型的预训练。 |
| [^110] | [Case-Based Reasoning with Language Models for Classification of Logical Fallacies.](http://arxiv.org/abs/2301.11879) | 本文提出了一种基于案例推理的方法用于分类逻辑谬误的新案例，通过基于语言建模的检索和历史案例的调整来提高语言模型的准确性和泛化能力。 |
| [^111] | [Domain-Agnostic Molecular Generation with Self-feedback.](http://arxiv.org/abs/2301.11259) | MolGen是一个专注于分子生成的预训练语言模型，使用了领域无关的分子前缀调整和自我反馈的范式，实现了化学有效性、多样性、新颖性和复杂性的突破，在分子生成领域表现出了出色的性能。 |
| [^112] | [ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations.](http://arxiv.org/abs/2212.10409) | ClarifyDelphi是一个交互式系统，能够针对社会或道德情境提出最有信息价值的问题，并通过奖励机制最大化回答问题时的道德判断分歧。 |
| [^113] | [Gradient-based Intra-attention Pruning on Pre-trained Language Models.](http://arxiv.org/abs/2212.07634) | 本文提出了一种基于渐进式自注意力剪枝的压缩预训练语言模型方法GRAIN，通过执行任务特定的剪枝和知识蒸馏，可以得到高效的模型，并在GLUE、SQuAD和CoNLL 2003等任务中表现优异。 |
| [^114] | [From Clozing to Comprehending: Retrofitting Pre-trained Masked Language Model to Pre-trained Machine Reader.](http://arxiv.org/abs/2212.04755) | 本文提出了一种无需标记数据的新方法，能够将预训练的遮蔽语言模型改造为预训练的机器阅读理解模型，解决了现有模型预训练与下游微调之间的差异化问题。PMR 在多个基准数据集上表现优秀，能有效提高预测可解释性。 |
| [^115] | [DC-MBR: Distributional Cooling for Minimum Bayesian Risk Decoding.](http://arxiv.org/abs/2212.04205) | 本研究提出了分布式冷却MBR（DC-MBR）的方法来解决标签平滑对最小贝叶斯风险解码（MBR）造成的自回归过度平滑性问题，该方法通过调整softmax温度来操纵输出分布的熵。最终，DC-MBR在神经机器翻译中表现出众，相较于现有基线持续优化。 |
| [^116] | [Distilling Reasoning Capabilities into Smaller Language Models.](http://arxiv.org/abs/2212.00193) | 本文提出了一种知识蒸馏方法，可以把大型语言模型的逐步推理能力蒸馏到更小的模型中，提出了一种替代推理方案，使用苏格拉底式CoT来训练两个小型蒸馏模型的组合，可以用来分解和解决复杂的问题，且在多个推理数据集上表现出高精度的复杂推理能力，经常优于那些没有经过CoT推理方法训练的大模型。 |
| [^117] | [What learning algorithm is in-context learning? Investigations with linear models.](http://arxiv.org/abs/2211.15661) | 研究者提出了一种假设，即基于转换器的上下文学习器可以隐含地编码学习算法，并根据上下文中出现的新示例更新这些隐式模型。通过构造和比较性质证明了这个假设，并提出了一种度量学习器预测器和从显式学习算法中获得的预测器之间相似程度的方法。 |
| [^118] | [Deanthropomorphising NLP: Can a Language Model Be Conscious?.](http://arxiv.org/abs/2211.11483) | 本文讨论了关于使用Transformer架构的预训练语言模型LaMDA是否具有意识的说法。作者认为语言模型不可能具有意识，而LaMDA没有比其他类似模型更具先进性。 |
| [^119] | [SLICER: Learning universal audio representations using low-resource self-supervised pre-training.](http://arxiv.org/abs/2211.01519) | 本文提出了一种名为SLICER的自监督学习方法，通过聚类和对比学习相结合的方式进行编码器预训练，从而得到可以广泛适用于语音和非语音任务的音频表示。 |
| [^120] | [MAST: Multiscale Audio Spectrogram Transformers.](http://arxiv.org/abs/2211.01515) | MAST是一种多尺度音频谱图变压器，引入了多尺度特征分层概念，同时扩展嵌入维度，降低时间分辨率，用于音频分类。通过金字塔结构实现早期层和深层的建模，扩展方法为SS-MAST。 |
| [^121] | [SpeechBlender: Speech Augmentation Framework for Mispronunciation Data Generation.](http://arxiv.org/abs/2211.00923) | SpeechBlender是一个用于生成发音错误的数据增强框架，在发音错误检测模型的音素级上获得了ASR相关的最新技术水平，具有更有效的样本。 |
| [^122] | [PeerDA: Data Augmentation via Modeling Peer Relation for Span Identification Tasks.](http://arxiv.org/abs/2210.08855) | 本文提出了一种新的基于对等关系数据增强方法，称为PeerDA来进行跨度识别任务。PeerDA使用带有PR关系的跨度对作为训练的增强数据，防止模型过拟合表面上的跨度-类别映射，并推动模型利用跨度语义。实验表明，PeerDA在数据增强方面表现出色，显著优于现有的最先进数据增强方法。 |
| [^123] | [REV: Information-Theoretic Evaluation of Free-Text Rationales.](http://arxiv.org/abs/2210.04982) | 本论文提出了一种名为REV的度量，用于评估自由文本解释中新颖、与标签相关的信息的数量，通过信息论的角度进行研究。实验证明REV在评估解释-标签对方面的有效性，并且与人类直觉一致。 |
| [^124] | [Dynamic Generation of Grounded Logical Explanations in a Neuro-Symbolic Expert System.](http://arxiv.org/abs/2209.07662) | 该研究提出一种新颖的方法，通过结合神经语言建模、引导生成和半参数密集检索，动态生成基于事实库的人类可解释的证明树，实现科学推理，并展现了强大的性能。 |
| [^125] | [On the Intersection of Context-Free and Regular Languages.](http://arxiv.org/abs/2209.06809) | 本文提出了一个通用的构造，可处理具有 $\varepsilon$ -弧的有限状态自动机，其中正则语言由有限状态自动机指定，该构造不仅编码了结构，而且保留了原始构造的渐近大小。 |
| [^126] | [Ranking-Enhanced Unsupervised Sentence Representation Learning.](http://arxiv.org/abs/2209.04333) | 提出一种新的无监督句子编码器RankEncoder，该编码器预测句子的语义向量时利用输入句子与外部语料库中其他句子以及输入句子本身的关系。该编码器相对于传统的无监督句子编码器更具有抗干扰攻击的鲁棒性，并且在语义文本基准数据集上取得了1.1%的绝对改进。 |
| [^127] | [A Study on Transformer Configuration and Training Objective.](http://arxiv.org/abs/2205.10505) | 本文提出了Bamboo配置策略，基于更深更窄的Transformer结构进行Masked自编码器训练，在图像和语言任务上取得了新的最先进结果。 |
| [^128] | [GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation.](http://arxiv.org/abs/2204.06674) | 本文提出了一种面向知识库到文本生成的图感知语言模型框架，通过将图感知元素融入预训练语言模型中，提出了掩码结构和新的类型编码器，超越了现有的最先进模型，并消除了额外预训练任务所带来的差距。 |
| [^129] | [Analyzing the factors affecting usefulness of Self-Supervised Pre-trained Representations for Speech Recognition.](http://arxiv.org/abs/2203.16973) | 本论文试图研究自监督预训练的语音表示对于低资源语音识别系统在领域、语言、数据集大小以及先前知识方面的影响，揭示了用于自监督预训练的数据对 ASR 系统性能的影响以及相似度和数据的数量等因素对其性能的影响。 |

# 详细

[^1]: 激发Web规模语音模型的潜在能力以实现零-shot任务泛化

    Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v1 [eess.AS])

    [http://arxiv.org/abs/2305.11095](http://arxiv.org/abs/2305.11095)

    本文通过提示工程技术调整Whisper模型，成功适应未见过的三个任务，并提出的提示比默认提示性能提升了10%到45％，展现了Whisper模型的鲁棒性和多语言理解能力。

    

    本文研究了最近提出的Web规模语音模型Whisper的新兴功能，在使用提示工程技术调整模型后，适应了未见过的AVSR，CS-ASR和ST三个任务。我们设计了特定于任务的提示，要么利用另一个大规模模型，要么简单地操作默认提示中的特殊标记。实验证明，与默认提示相比，我们提出的提示使这三个零-shot任务的性能提高了10%到45％，甚至在一些数据集上超过了SotA监督模型。此外，我们的实验揭示了Whisper的许多有趣属性，包括其提示的鲁棒性，对口音的偏好以及潜在空间中的多语言理解。代码可在https://github.com/jasonppy/PromptingWhisper上找到。

    We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper
    
[^2]: 检查文本到图像模型中图像的地理代表性

    Inspecting the Geographical Representativeness of Images from Text-to-Image Models. (arXiv:2305.11080v1 [cs.CV])

    [http://arxiv.org/abs/2305.11080](http://arxiv.org/abs/2305.11080)

    本文研究了文本到图像模型生成的图像的地理代表性，发现这些模型生成的图像大多反映美国和印度等国家的环境，而对于其他国家的反映较少，因此需要更多关注地理多样性。

    

    近年来，生成模型的进展使得这些模型能够为大多数文本输入生成既逼真又相关的图像。这些模型每天生成数百万张图像，并具有极大的潜力，可以极大地影响生成艺术、数字营销和数据增强等领域。鉴于其巨大的影响力，重要的是要确保生成的内容反映全球各地的文物和环境，而不是过度代表某些地区。在本文中，我们使用由540名来自27个国家的众包参与者组成的研究，测量了通过DALL.E 2和稳定扩散模型生成的常见名词（例如，一座房子）的地理代表性。对于故意未指定国家名称的输入，生成的图像最反映美国和印度等国家的环境，而最好的生成很少反映其他国家的环境（在25个国家中平均得分不到9%）。对于明确提到一个国家的输入，模型生成高度特定于该国家的图像，具有最少的跨国转移。我们的工作表明，在设计这些模型时需要更多地关注地理多样性，同时要认识到这些模型在准确反映世界方面的局限性。

    Recent progress in generative models has resulted in models that produce both realistic as well as relevant images for most textual inputs. These models are being used to generate millions of images everyday, and hold the potential to drastically impact areas such as generative art, digital marketing and data augmentation. Given their outsized impact, it is important to ensure that the generated content reflects the artifacts and surroundings across the globe, rather than over-representing certain parts of the world. In this paper, we measure the geographical representativeness of common nouns (e.g., a house) generated through DALL.E 2 and Stable Diffusion models using a crowdsourced study comprising 540 participants across 27 countries. For deliberately underspecified inputs without country names, the generated images most reflect the surroundings of the United States followed by India, and the top generations rarely reflect surroundings from all other countries (average score less th
    
[^3]: 电子分支变形器与变形器在语音识别、翻译和理解任务中的比较研究

    A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks. (arXiv:2305.11073v1 [cs.CL])

    [http://arxiv.org/abs/2305.11073](http://arxiv.org/abs/2305.11073)

    本研究比较了E-Branchformer和Conformer在语音识别、翻译和理解任务中的性能，结果表明E-Branchformer在多个基准测试中表现相当甚至更好，并且更加稳定。

    

    Conformer是一种卷积变换器，由于在自动语音识别（ASR）、语音翻译（ST）和口语理解（SLU）等各种任务中表现出优异性能，已经成为语音处理的事实标准编码器架构。最近，一种名为E-Branchformer的新编码器在LibriSpeech ASR基准测试中超越了Conformer，使其在更普遍的语音应用中变得有前途。本研究通过使用不同类型的端到端序列到序列模型进行广泛实验，比较了E-Branchformer和Conformer。结果表明，在15个ASR、2个ST和3个SLU基准测试的几乎所有评估集中，E-Branchformer的表现与Conformer相当甚至更好，同时在训练过程中更为稳定。我们将发布我们的训练配置和预训练模型以实现可重复性，从中受益的将是语音社区。

    Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.
    
[^4]: 无监督微调：通过说话者不变聚类来改进内容表示

    Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering. (arXiv:2305.11072v1 [cs.CL])

    [http://arxiv.org/abs/2305.11072](http://arxiv.org/abs/2305.11072)

    提出了一种自监督学习方法Spin，通过说话者不变聚类来解开说话者信息并保留内容表示，只需45分钟微调即可改进预训练网络，在语音识别和声学单元发现中优于先前的方法。

    

    自监督语音表示模型已经在各种任务中取得了成功，但使用无标签数据来改进它们对内容相关问题的表示仍然具有挑战性。我们提出了一种称为说话者不变聚类(Spin)的新型自监督学习方法，通过聚类语音表示并在原始语音和说话者扰动语音之间进行交换预测来解开说话者信息并保留内容表示。使用单个GPU进行45分钟的微调即可改进预训练网络，并在语音识别和声学单元发现方面优于先前的方法。

    Self-supervised speech representation models have succeeded in various tasks, but improving them for content-related problems using unlabeled data is challenging. We propose speaker-invariant clustering (Spin), a novel self-supervised learning method that clusters speech representations and performs swapped prediction between the original and speaker-perturbed utterances. Spin disentangles speaker information and preserves content representations with just 45 minutes of fine-tuning on a single GPU. Spin improves pre-trained networks and outperforms prior methods in speech recognition and acoustic unit discovery.
    
[^5]: 基于图形上下文信息的语言模型增强以更好地理解文本数据

    Enriching language models with graph-based context information to better understand textual data. (arXiv:2305.11070v1 [cs.CL])

    [http://arxiv.org/abs/2305.11070](http://arxiv.org/abs/2305.11070)

    基于图形上下文信息的语言模型增强可更好的理解文本，实验表明该方法提高了BERT模型在Pubmed上的分类任务表现。

    

    每天遇到的文本具有相互联系的情况相当多。例如，Wikipedia文章通过超链接引用其他文章，科学论文通过引用或（共同）作者与其他论文相关联，而推文则通过关注彼此或转发内容来关联。因此，类似于图形的结构可以表示现有的联系，并被视为捕捉文本的“上下文”。因此，提取和整合这种上下文信息到语言模型中是否有助于更好地自动理解文本？在本研究中，我们实验性地证明，将基于图形的上下文化纳入BERT模型会增强其在分类任务示例上的表现。具体而言，在Pubmed数据集上，我们观察到误差从8.51％降至7.96％，同时仅增加了1.6％的参数量。

    A considerable number of texts encountered daily are somehow connected with each other. For example, Wikipedia articles refer to other articles via hyperlinks, scientific papers relate to others via citations or (co)authors, while tweets relate via users that follow each other or reshare content. Hence, a graph-like structure can represent existing connections and be seen as capturing the "context" of the texts. The question thus arises if extracting and integrating such context information into a language model might help facilitate a better automated understanding of the text. In this study, we experimentally demonstrate that incorporating graph-based contextualization into BERT model enhances its performance on an example of a classification task. Specifically, on Pubmed dataset, we observed a reduction in error from 8.51% to 7.96%, while increasing the number of parameters just by 1.6%.  Our source code: https://github.com/tryptofanik/gc-bert
    
[^6]: ORKG-Leaderboards: 一种以知识图谱形式挖掘排行榜的系统化流程

    ORKG-Leaderboards: A Systematic Workflow for Mining Leaderboards as a Knowledge Graph. (arXiv:2305.11068v1 [cs.CL])

    [http://arxiv.org/abs/2305.11068](http://arxiv.org/abs/2305.11068)

    ORKG-Leaderboards是一种以知识图谱形式挖掘AI领域排行榜并支持机器可操作性出版的系统，能够让研究人员透明地了解全球研究人员的实证结果，跟踪人工智能的进展情况。

    

    本文介绍了 ORKG-Leaderboard 软件，它旨在从人工智能领域的大量实证研究论文中自动提取以任务-数据集-度量元组为定义的排行榜。该软件支持学术出版的主要工作流程，即 LaTeX 文件或 PDF 文件。此外，该系统还与 Open Research Knowledge Graph (ORKG) 平台集成，该平台促进了学术发现的机器可操作性出版。因此，当系统输出与 ORKG 支持的语义 web 基础设施相结合时，它可以实现两个方面的功能：1）横跨全球的研究人员的实证结果的集成，从而实现实证研究的透明度，并有可能是完整的；2）让研究人员了解人工智能的进展情况。

    The purpose of this work is to describe the Orkg-Leaderboard software designed to extract leaderboards defined as Task-Dataset-Metric tuples automatically from large collections of empirical research papers in Artificial Intelligence (AI). The software can support both the main workflows of scholarly publishing, viz. as LaTeX files or as PDF files. Furthermore, the system is integrated with the Open Research Knowledge Graph (ORKG) platform, which fosters the machine-actionable publishing of scholarly findings. Thus the system output, when integrated within the ORKG's supported Semantic Web infrastructure of representing machine-actionable 'resources' on the Web, enables: 1) broadly, the integration of empirical results of researchers across the world, thus enabling transparency in empirical research with the potential to also being complete contingent on the underlying data source(s) of publications; and 2) specifically, enables researchers to track the progress in AI with an overview 
    
[^7]: Bits of Grass: GPT是否已经拥有了写作惠特曼样式的能力？

    Bits of Grass: Does GPT already know how to write like Whitman?. (arXiv:2305.11064v1 [cs.CL])

    [http://arxiv.org/abs/2305.11064](http://arxiv.org/abs/2305.11064)

    本文研究了GPT模型在生成特定作者风格诗歌方面的能力，结果表明即使提供了大量样本，未经微调的模型也不能生成所需风格的诗歌。

    

    本研究检验了GPT-3.5、GPT-3.5-Turbo（ChatGPT）和GPT-4模型使用零/多次提示（使用最大上下文长度8192个令牌）生成特定作者风格诗歌的能力。

    This study examines the ability of GPT-3.5, GPT-3.5-turbo (ChatGPT) and GPT-4 models to generate poems in the style of specific authors using zero-shot and many-shot prompts (which use the maximum context length of 8192 tokens). We assess the performance of models that are not fine-tuned for generating poetry in the style of specific authors, via automated evaluation. Our findings indicate that without fine-tuning, even when provided with the maximum number of 17 poem examples (8192 tokens) in the prompt, these models do not generate poetry in the desired style.
    
[^8]: SPSQL: 基于逐步解析的文本转SQL生成框架

    SPSQL: Step-by-step Parsing Based Framework for Text-to-SQL Generation. (arXiv:2305.11061v1 [cs.CL])

    [http://arxiv.org/abs/2305.11061](http://arxiv.org/abs/2305.11061)

    本文提出了一种基于流水线的文本转SQL方法: SPSQL，它将任务分解成四个子任务，从而减少了模型难度和对训练数据的要求，提高了准确率。

    

    将文本转换为结构化查询语言(Text2SQL)是自然语言处理(NLP)领域的研究热点，具有广泛的应用前景。在大数据时代，数据库的使用已经渗透到各个领域，其中收集的数据规模大、种类多样、范围广泛，使得数据查询繁琐低效，对Text2SQL模型提出了更高的要求。本文提出了一种基于流水线的Text2SQL方法: SPSQL，将文本转换为SQL过程分解为四个子任务——表选择、列选择、SQL生成和值填充，可以转化为一个可解析的句法树，从而减少了模型的复杂度和训练数据的需求，提高了准确率。

    Converting text into the structured query language (Text2SQL) is a research hotspot in the field of natural language processing (NLP), which has broad application prospects. In the era of big data, the use of databases has penetrated all walks of life, in which the collected data is large in scale, diverse in variety, and wide in scope, making the data query cumbersome and inefficient, and putting forward higher requirements for the Text2SQL model. In practical applications, the current mainstream end-to-end Text2SQL model is not only difficult to build due to its complex structure and high requirements for training data, but also difficult to adjust due to massive parameters. In addition, the accuracy of the model is hard to achieve the desired result. Based on this, this paper proposes a pipelined Text2SQL method: SPSQL. This method disassembles the Text2SQL task into four subtasks--table selection, column selection, SQL generation, and value filling, which can be converted into a te
    
[^9]: BERM：训练平衡可提取表示以提高密集检索的泛化能力

    BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval. (arXiv:2305.11052v1 [cs.IR])

    [http://arxiv.org/abs/2305.11052](http://arxiv.org/abs/2305.11052)

    BERM使用平衡的、可提取的特征表示法来捕捉匹配信号，从而提高了密集检索的泛化性能。

    

    密集检索已经表现出在域内标记数据集上训练的情况下在第一阶段检索过程中有所作为。然而，以前的研究发现，由于密集检索对域不变和可解释的特征的建模较弱（即两个文本之间的匹配信号，这是信息检索的本质），因此难以推广到未见过的领域。在本文中，我们通过捕捉匹配信号提出了一种提高密集检索泛化性能的新方法，称为BERM。全面的细粒度表达和查询导向的显着性是匹配信号的两个属性。因此，在BERM中，一个单一的Passage被划分为多个单元，提出了两个单元级要求作为约束进行表示以获得有效的匹配信号。一个是语义单元平衡，另一个是必需的匹配单元可提取性。单元级视图和平衡语义使表示以细粒度的方式表达文本。必需的匹配单元可提取性确保保留信息检索的本质。在各种数据集上的实验表明，BERM在保持域内数据集上有竞争力的性能的同时，提高了算法的泛化能力。

    Dense retrieval has shown promise in the first-stage retrieval process when trained on in-domain labeled datasets. However, previous studies have found that dense retrieval is hard to generalize to unseen domains due to its weak modeling of domain-invariant and interpretable feature (i.e., matching signal between two texts, which is the essence of information retrieval). In this paper, we propose a novel method to improve the generalization of dense retrieval via capturing matching signal called BERM. Fully fine-grained expression and query-oriented saliency are two properties of the matching signal. Thus, in BERM, a single passage is segmented into multiple units and two unit-level requirements are proposed for representation as the constraint in training to obtain the effective matching signal. One is semantic unit balance and the other is essential matching unit extractability. Unit-level view and balanced semantics make representation express the text in a fine-grained manner. Esse
    
[^10]: 基于上下文学习的命名实体识别

    Learning In-context Learning for Named Entity Recognition. (arXiv:2305.11038v1 [cs.CL])

    [http://arxiv.org/abs/2305.11038](http://arxiv.org/abs/2305.11038)

    本文提出了一种在 PLMs 中注入上下文 NER 能力的方法，只需少量示意实例即可动态识别新类型的实体，在几个基准数据集上达到了最先进的性能。

    

    现实世界中的命名实体识别受到实体类型的多样性、新实体类型的出现和高质量标注的缺乏等问题的影响。本文提出了一种基于上下文学习的命名实体识别方法，能够将上下文NER能力有效地注入到PLMs中，并且只使用少量示意实例就能动态识别新类型的实体。具体而言，我们将PLMs建模为一个元函数 $\mathcal{ \lambda_ {\text{instruction, demonstrations, text}}. M}$，并通过将新的指示和示例应用于PLMs来隐含地构建新的实体提取器，即 $\mathcal{ (\lambda . M) }$(instruction, demonstrations) $\to$ $\mathcal{F}$，其中 $\mathcal{F}$ 将成为一个新的实体提取器，即 $\mathcal{F}$: text $\to$ entities。为了将上述上下文NER能力注入PLMs，我们提出了一种元函数预训练算法，该算法通过比较（指示、示例）-identity和（掩盖后的指示、示例）-identity来预训练PLMs。实验结果表明，我们的方法在几个基准数据集上达到了最先进的性能，并且能够使用少量示意实例有效地识别新类型的实体。

    Named entity recognition in real-world applications suffers from the diversity of entity types, the emergence of new entity types, and the lack of high-quality annotations. To address the above problems, this paper proposes an in-context learning-based NER approach, which can effectively inject in-context NER ability into PLMs and recognize entities of novel types on-the-fly using only a few demonstrative instances. Specifically, we model PLMs as a meta-function $\mathcal{ \lambda_ {\text{instruction, demonstrations, text}}. M}$, and a new entity extractor can be implicitly constructed by applying new instruction and demonstrations to PLMs, i.e., $\mathcal{ (\lambda . M) }$(instruction, demonstrations) $\to$ $\mathcal{F}$ where $\mathcal{F}$ will be a new entity extractor, i.e., $\mathcal{F}$: text $\to$ entities. To inject the above in-context NER ability into PLMs, we propose a meta-function pre-training algorithm, which pre-trains PLMs by comparing the (instruction, demonstration)-i
    
[^11]: 通过使用Wordpieces和增强Aspect来进行以目标为导向的意见词提取

    Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words Extraction with Wordpieces and Aspect Enhancement. (arXiv:2305.11034v1 [cs.CL])

    [http://arxiv.org/abs/2305.11034](http://arxiv.org/abs/2305.11034)

    本文提出了一种使用Wordpieces替换语法树来进行以目标为导向的意见词提取的方法，并使用句子 - 方面对来增强性能。该方法在基准数据集上取得了最先进的结果。

    

    目前最先进的以目标为导向的意见词提取（TOWE）模型通常使用基于BERT的文本编码器，操作在单词级别上，以及图卷积网络（GCN），这些网络可以将从句法树中提取的句法信息纳入模型中。然而，这些方法在使用GCN时仅取得了有限的增益，并且使用BERT wordpieces时存在困难。与此同时，已知BERT wordpieces在表示罕见的单词或上下文信息不足的单词方面非常有效。为解决这个问题，本文通过完全消除方法结构中的GCN组件，以Wordpieces来交换句法树。为了增强TOWE的性能，我们解决了在编码过程中方面表示丢失的问题。与其仅仅使用句子作为输入，我们使用句子 - 方面对。我们的相对简单的方法在基准数据集上取得了最先进的结果，并应作为进一步研究的强有力基线。

    State-of-the-art target-oriented opinion word extraction (TOWE) models typically use BERT-based text encoders that operate on the word level, along with graph convolutional networks (GCNs) that incorporate syntactic information extracted from syntax trees. These methods achieve limited gains with GCNs and have difficulty using BERT wordpieces. Meanwhile, BERT wordpieces are known to be effective at representing rare words or words with insufficient context information. To address this issue, this work trades syntax trees for BERT wordpieces by entirely removing the GCN component from the methods' architectures. To enhance TOWE performance, we tackle the issue of aspect representation loss during encoding. Instead of solely utilizing a sentence as the input, we use a sentence-aspect pair. Our relatively simple approach achieves state-of-the-art results on benchmark datasets and should serve as a strong baseline for further research.
    
[^12]: 不确定性引导的标签去噪在文档级远程关系抽取中的应用

    Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction. (arXiv:2305.11029v1 [cs.CL])

    [http://arxiv.org/abs/2305.11029](http://arxiv.org/abs/2305.11029)

    本文提出了一种使用不确定性引导的标签去噪技术，可以有效准确地在文档级远程关系抽取中选择可信的伪标签，提高了性能表现，并在DocRED数据集上实现了新的最佳性能。

    

    文档级关系抽取旨在推断文档中实体之间的复杂语义关系。远程监督能够生成大量自动标注的数据，从而可以提高文档关系抽取的性能。然而，不可靠的伪标签会带来新的噪声，例如添加虚假的伪标签和失去正确的监督标签。因此，如何选择有效的伪标签来去噪远程监督数据仍然是文档级远程关系抽取中的一个挑战。为了解决这个问题，我们引入了不确定性估计技术来确定伪标签是否可信。在本文中，我们提出了一个带有不确定性引导标签去噪的文档级远程关系抽取框架，UGDRE。具体而言，我们提出了一种新的实例级不确定性估计方法，它测量了具有重叠关系的伪标签的可靠性。通过进一步考虑实例级和关系级的不确定性，我们设计了一个标签去噪组件，可以有效地选择可靠的伪标签进行文档关系抽取。在两个基准数据集上的实验结果表明，我们的方法显著优于现有方法，并在DocRED数据集上实现了新的最佳性能。

    Document-level relation extraction (DocRE) aims to infer complex semantic relations among entities in a document. Distant supervision (DS) is able to generate massive auto-labeled data, which can improve DocRE performance. Recent works leverage pseudo labels generated by the pre-denoising model to reduce noise in DS data. However, unreliable pseudo labels bring new noise, e.g., adding false pseudo labels and losing correct DS labels. Therefore, how to select effective pseudo labels to denoise DS data is still a challenge in document-level distant relation extraction. To tackle this issue, we introduce uncertainty estimation technology to determine whether pseudo labels can be trusted. In this work, we propose a Document-level distant Relation Extraction framework with Uncertainty Guided label denoising, UGDRE. Specifically, we propose a novel instance-level uncertainty estimation method, which measures the reliability of the pseudo labels with overlapping relations. By further consider
    
[^13]: 通用多意图条件下的词槽填充

    Generalized Multiple Intent Conditioned Slot Filling. (arXiv:2305.11023v1 [cs.CL])

    [http://arxiv.org/abs/2305.11023](http://arxiv.org/abs/2305.11023)

    本文针对现实场景下意图重复的问题，对词槽填充进行了泛化。通过结合数据集进行预训练，使用语言模型进行 JSON 生成任务处理，T5 模型能够泛化到未见过的意图类型。

    

    自然语言理解包括意图检测（识别用户的目标）和词槽填充（提取与这些目标相关的实体）任务。以往的词槽填充方法假设每个意图类型在一条消息中只出现一次，但在真实场景下这通常不是一个有效的假设。本文通过去除消息中唯一意图的约束，对词槽填充进行泛化。我们将其视为一个 JSON 生成任务，并使用语言模型对其进行处理。我们通过结合 DBpedia 和现有的词槽填充数据集并将其转换为 JSON 生成任务来创建一个预训练数据集。我们还使用 GPT-3 生成了一个领域内数据集。我们训练了 T5 模型（带或不带提示示例）并发现两个训练数据集均能提高性能，而模型能够泛化到训练过程中未见过的意图类型。

    Natural language understanding includes the tasks of intent detection (identifying a user's objectives) and slot filling (extracting the entities relevant to those objectives). Prior slot filling methods assume that each intent type cannot occur more than once within a message, however this is often not a valid assumption for real-world settings. In this work, we generalize slot filling by removing the constraint of unique intents in a message. We cast this as a JSON generation task and approach it using a language model. We create a pre-training dataset by combining DBpedia and existing slot filling datasets that we convert for JSON generation. We also generate an in-domain dataset using GPT-3. We train T5 models for this task (with and without exemplars in the prompt) and find that both training datasets improve performance, and that the model is able to generalize to intent types not seen during training.
    
[^14]: FunASR: 一款基础的端到端语音识别工具箱

    FunASR: A Fundamental End-to-End Speech Recognition Toolkit. (arXiv:2305.11013v1 [cs.SD])

    [http://arxiv.org/abs/2305.11013](http://arxiv.org/abs/2305.11013)

    本文介绍了一款名为FunASR的基础语音识别工具箱，其中的Paraformer模型是一个在60万小时语音数据集上训练的非自回归的端到端语音识别模型，工具箱还提供了包括语音活动检测和文本标点功能在内的多个工具模块，以提高语音识别性能和易用性。

    

    本文介绍了FunASR，一款开源的语音识别工具箱，旨在弥合学术研究和工业应用之间的差距。FunASR提供了在应用中使用的经过大规模工业语料库训练的模型。该工具箱的旗舰模型Paraformer是一个非自回归的端到端语音识别模型，已在手工注释的包含60,000小时语音的中文语音识别数据集上进行了训练。为了提高Paraformer的性能，我们在标准Paraformer骨干中添加了时间戳预测和热词定制能力。此外，为了促进模型部署，我们开源了一个基于前馈序列记忆网络（FSMN-VAD）的语音活动检测模型和一个基于可控时延Transformer（CT-Transformer）的文本后处理标点模型，这两个模型都是在工业语料库上训练的。这些功能模块提供了一种实用的方式来提高语音识别性能和易用性。

    This paper introduces FunASR, an open-source speech recognition toolkit designed to bridge the gap between academic research and industrial applications. FunASR offers models trained on large-scale industrial corpora and the ability to deploy them in applications. The toolkit's flagship model, Paraformer, is a non-autoregressive end-to-end speech recognition model that has been trained on a manually annotated Mandarin speech recognition dataset that contains 60,000 hours of speech. To improve the performance of Paraformer, we have added timestamp prediction and hotword customization capabilities to the standard Paraformer backbone. In addition, to facilitate model deployment, we have open-sourced a voice activity detection model based on the Feedforward Sequential Memory Network (FSMN-VAD) and a text post-processing punctuation model based on the controllable time-delay Transformer (CT-Transformer), both of which were trained on industrial corpora. These functional modules provide a so
    
[^15]: 通过框嵌入和概率评分器完成分类法

    Taxonomy Completion with Probabilistic Scorer via Box Embedding. (arXiv:2305.11004v1 [cs.CL])

    [http://arxiv.org/abs/2305.11004](http://arxiv.org/abs/2305.11004)

    本文提出了一种新方法TaxBox，该方法将分类法概念映射到框嵌入中，并使用两个概率评分器来进行概念附加和插入，避免使用伪叶。实验表明，在二个基准数据集上，TaxBox在准确性和训练效率方面显著优于现有的方法。

    

    最近，分类法的完善任务--自动利用新的概念丰富现有分类法--已经引起了广泛的兴趣。早期的研究引入了复杂模块、外部信息和伪叶来丰富表示并统一附加和插入的匹配过程。虽然它们已经取得了良好的性能，但这些介绍可能会在训练和评分过程中带来噪音和不公平性。在本文中，我们提出了TaxBox，一种新颖的用于完成分类法的框架，它将分类法概念映射到框嵌入中，并使用两个概率评分器来进行概念附加和插入，避免使用伪叶。具体而言，TaxBox由三个组件组成：（1）图聚合模块，以利用分类法的结构信息和两个轻量级解码器，将特征映射到框嵌入，并捕捉概念之间的复杂关系；（2）两个概率评分器，分别对应附加和插入任务，设计了一种原则性方法来减轻误导信息的影响；（3）一种联合训练模型和优化两个评分器的训练算法。我们在两个基准数据集上的实验表明，TaxBox在准确性和训练效率方面显著优于现有的状态-of-the-art方法。

    Taxonomy completion, a task aimed at automatically enriching an existing taxonomy with new concepts, has gained significant interest in recent years. Previous works have introduced complex modules, external information, and pseudo-leaves to enrich the representation and unify the matching process of attachment and insertion. While they have achieved good performance, these introductions may have brought noise and unfairness during training and scoring. In this paper, we present TaxBox, a novel framework for taxonomy completion that maps taxonomy concepts to box embeddings and employs two probabilistic scorers for concept attachment and insertion, avoiding the need for pseudo-leaves. Specifically, TaxBox consists of three components: (1) a graph aggregation module to leverage the structural information of the taxonomy and two lightweight decoders that map features to box embedding and capture complex relationships between concepts; (2) two probabilistic scorers that correspond to attach
    
[^16]: SpeechGPT: 用本质跨模态会话能力赋能大型语言模型

    SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities. (arXiv:2305.11000v1 [cs.CL])

    [http://arxiv.org/abs/2305.11000](http://arxiv.org/abs/2305.11000)

    SpeechGPT是一个具有本质跨模态会话能力的大型语言模型，能够感知和生成多模态内容，可按照多模态人类指令的能力，并突显了使用一个模型处理多个模态的潜力。

    

    多模态大型语言模型被认为是迈向人工通用智能（AGI）的重要一步，随着ChatGPT的出现，它们已经引起了广泛的关注。然而，目前的语音-语言模型通常采用级联范式，阻止了跨模态知识的转移。在本文中，我们提出了SpeechGPT，这是一个具有本质跨模态会话能力的大型语言模型，能够感知和生成多模态内容。通过离散化的语音表示，我们首先构建了SpeechInstruct，一个大规模的跨模态语音指令数据集。此外，我们采用了三阶段的训练策略，包括模态自适应预训练、跨模态指令微调和模态链指令微调。实验结果表明，SpeechGPT具有按照多模态人类指令的能力，并突显了使用一个模型处理多个模态的潜力。

    Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content. With discrete speech representations, we first construct SpeechInstruct, a large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown 
    
[^17]: 网络可以为改进大语言模型提供更多的可选资源

    The Web Can Be Your Oyster for Improving Large Language Models. (arXiv:2305.10998v1 [cs.CL])

    [http://arxiv.org/abs/2305.10998](http://arxiv.org/abs/2305.10998)

    本文提出了一种新的大型语言模型增强方法，即使用自适应搜索引擎辅助学习方法，将大规模网络数据与LLMs融合，从而避免无用或嘈杂的增强。

    

    大型语言模型(LLMs)可以编码大量的世界知识。然而，由于这些知识在模型训练时已经固化，这些模型变得静态，并且受到了当时训练数据的限制。为了进一步提高LLMs在知识密集型任务中的能力，我们考虑使用搜索引擎将LLMs与海量网络进行融合增强。与以往的增强来源（如维基百科数据转储）不同，网络提供了更广泛、更全面且不断更新的信息。在本文中，我们提出了一个名为UNIWEB的网络增强LLM，它以统一的文本到文本格式在16个知识密集型任务上进行训练。我们的方法并不是简单地使用从网络检索到的内容，而是提出了一种自适应的搜索引擎辅助学习方法，该方法可以自我评估LLM的预测置信度，并自适应地确定何时参考网络获取更多数据，从而避免无用或嘈杂的增强。

    Large language models (LLMs) encode a large amount of world knowledge. However, as such knowledge is frozen at the time of model training, the models become static and limited by the training data at that time. In order to further improve the capacity of LLMs for knowledge-intensive tasks, we consider augmenting LLMs with the large-scale web using search engine. Unlike previous augmentation sources (e.g., Wikipedia data dump), the web provides broader, more comprehensive and constantly updated information. In this paper, we present a web-augmented LLM UNIWEB, which is trained over 16 knowledge-intensive tasks in a unified text-to-text format. Instead of simply using the retrieved contents from web, our approach has made two major improvements. Firstly, we propose an adaptive search engine assisted learning method that can self-evaluate the confidence level of LLM's predictions, and adaptively determine when to refer to the web for more data, which can avoid useless or noisy augmentatio
    
[^18]: 掩码预训练任务的复杂度如何影响下游任务表现？

    How does the task complexity of masked pretraining objectives affect downstream performance?. (arXiv:2305.10992v1 [cs.CL])

    [http://arxiv.org/abs/2305.10992](http://arxiv.org/abs/2305.10992)

    本文研究了掩码预训练任务的复杂度对下游任务表现的影响，并发现更复杂的任务可以实现更好的结果。这一发现表明，掩码预训练任务可以通过增加其复杂度来提高其性能。

    

    掩码语言建模是一种广泛使用的自监督预训练任务，其中模型需要预测替换上下文中的原始token的掩码。尽管最近使用更简单且计算较少的预训练任务，例如预测掩码标记的第一个字符，已经表现出与掩码语言建模相当的结果，但使用掩码方案的任务目前还没有超越掩码语言建模。本文假设缺乏复杂性是造成其性能下降的关键，我们验证了更复杂的掩码任务是否能够实现更好的结果，并探究它们的复杂度需要达到多少才能与掩码语言建模表现相当。我们使用GLUE、SQuAD和Universal Dependencies基准测试结果表明，更复杂的任务倾向于展现更好的下游任务表现，至少需要掩码语言建模复杂度的一半才能与其表现相当。最后，我们讨论了如何使用掩码任务预训练模型。

    Masked language modeling (MLM) is a widely used self-supervised pretraining objective, where a model needs to predict an original token that is replaced with a mask given contexts. Although simpler and computationally efficient pretraining objectives, e.g., predicting the first character of a masked token, have recently shown comparable results to MLM, no objectives with a masking scheme actually outperform it in downstream tasks. Motivated by the assumption that their lack of complexity plays a vital role in the degradation, we validate whether more complex masked objectives can achieve better results and investigate how much complexity they should have to perform comparably to MLM. Our results using GLUE, SQuAD, and Universal Dependencies benchmarks demonstrate that more complicated objectives tend to show better downstream results with at least half of the MLM complexity needed to perform comparably to MLM. Finally, we discuss how we should pretrain a model using a masked objective 
    
[^19]: 更少即是更好！一种优化语言翻译的轻量级架构。(arXiv:2305.10991v1 [cs.CL])

    Less is More! A slim architecture for optimal language translation. (arXiv:2305.10991v1 [cs.CL])

    [http://arxiv.org/abs/2305.10991](http://arxiv.org/abs/2305.10991)

    该论文提出了一种名为 KgV 的 sigmoid 门控机制，通过嵌入层剪枝减少了模型的大小，同时引入了 H-SoftPOS 层次嵌入层进一步改进嵌入以显著减少模型参数，从而在 WMT14 英德验证集上使 perplexity 减少了三倍以上。

    

    Softmax 注意力机制在人工智能研究领域已经成为一个值得关注的开发，构建在 Transformer 架构的成功基础之上。然而，它们不断增长的大小需要越来越多的计算存储器，从而限制了它们的使用。我们提出了 KgV，一种 sigmoid 门控机制，与 softmax 注意力一起显著提高了性能，同时不增加架构大小。为了修正大小要求，我们利用张量链来识别和修剪多余的参数。我们发现，这样的多余主要存在于嵌入层中，而不是输出线性层中。为了进一步改进嵌入和显著减少参数，我们引入了 H-SoftPOS，一种层次嵌入层，同时增强了性能。值得注意的是，在 WMT14 英德验证集上，我们的方法使 perplexity 减少了三倍，超过了当前的最新成果，同时减少降低模型大小。

    The softmax attention mechanism has emerged as a noteworthy development in the field of Artificial Intelligence research, building on the successes of Transformer-based architectures. However, their ever increasing sizes necessitate ever increasing computational memory, that limits their usage. We propose KgV, a sigmoid gating mechanism that, in conjunction with softmax attention, significantly boosts performance without increasing architecture size. To amend the size requirements, we leverage Tensor Chains to identify and prune the excess parameters. We find that such excess resides primarily within the embedding layer, and not in the output linear layer. To further improve embedding and significantly reduce parameters, we introduce H-SoftPOS, a hierarchical embedding layer which simultaneously enhances performance. Remarkably, on the WMT14 English-German validation set, our approach yields a threefold reduction in perplexity, surpassing the current state-of-the-art, while reducing pa
    
[^20]: Multi-CrossRE：面向关系抽取的多语言、多领域数据集

    Multi-CrossRE A Multi-Lingual Multi-Domain Dataset for Relation Extraction. (arXiv:2305.10985v1 [cs.CL])

    [http://arxiv.org/abs/2305.10985](http://arxiv.org/abs/2305.10985)

    Multi-CrossRE是一个多领域、多语言的数据集，包括26种语言在内，提供给关系抽取研究使用。

    

    由于缺乏多语言资源，大多数关系抽取研究都是基于英语语言的。本文提出了Multi-CrossRE，它是面向关系抽取的最广泛的多语言数据集，包括英语在内的26种语言，涵盖了六个文本域。Multi-CrossRE是CrossRE的机器翻译版本，其中一部分包含了由本地人检查的七种不同语言的200多个句子。我们在26个新数据集上运行了一个基线模型，并在26个回译到英语的数据集上进行了“理智检查”。回译数据的结果与原始英语CrossRE数据一致，表明翻译和生成的数据集质量很高。

    Most research in Relation Extraction (RE) involves the English language, mainly due to the lack of multi-lingual resources. We propose Multi-CrossRE, the broadest multi-lingual dataset for RE, including 26 languages in addition to English, and covering six text domains. Multi-CrossRE is a machine translated version of CrossRE (Bassignana and Plank, 2022), with a sub-portion including more than 200 sentences in seven diverse languages checked by native speakers. We run a baseline model over the 26 new datasets and--as sanity check--over the 26 back-translations to English. Results on the back-translated data are consistent with the ones on the original English CrossRE, indicating high quality of the translation and the resulting dataset.
    
[^21]: NollySenti：利用迁移学习和机器翻译进行尼日利亚电影情感分类

    NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian Movie Sentiment Classification. (arXiv:2305.10971v1 [cs.CL])

    [http://arxiv.org/abs/2305.10971](http://arxiv.org/abs/2305.10971)

    本论文提出了一个新的数据集NollySenti，用于不同领域适应的情感分类任务，基于尼日利亚五种常用语言的诺利木电影评论。研究表明，从具有相同目标域的英语进行迁移可以提高5％以上的准确性。

    

    非洲有超过2000种本土语言，但由于缺乏数据集，它们在自然语言处理研究中的代表性不高。最近几年，已经开始开发非洲语言的标注语料库，但这些语料库通常只在单一领域中可用，可能无法推广到其他领域。本文针对不同领域适应的情感分类任务，创建了一个新的数据集——NollySenti，它基于尼日利亚五种常用语言（英语、豪萨语、伊博语、尼日利亚皮钦语和约鲁巴语）的诺利木电影评论。我们使用传统的机器学习方法和预训练的语言模型进行了全面的经验评估。利用迁移学习，我们将来自Twitter领域的跨域适应和来自英语语言的跨语言适应的性能进行了比较。我们的评估结果表明，从具有相同目标域的英语进行迁移比从Twitter进行迁移可以提高5％以上的准确性。

    Africa has over 2000 indigenous languages but they are under-represented in NLP research due to lack of datasets. In recent years, there have been progress in developing labeled corpora for African languages. However, they are often available in a single domain and may not generalize to other domains. In this paper, we focus on the task of sentiment classification for cross domain adaptation. We create a new dataset, NollySenti - based on the Nollywood movie reviews for five languages widely spoken in Nigeria (English, Hausa, Igbo, Nigerian-Pidgin, and Yoruba. We provide an extensive empirical evaluation using classical machine learning methods and pre-trained language models. Leveraging transfer learning, we compare the performance of cross-domain adaptation from Twitter domain, and cross-lingual adaptation from English language. Our evaluation shows that transfer from English in the same target domain leads to more than 5% improvement in accuracy compared to transfer from Twitter in 
    
[^22]: 利用数据增强提高低资源语音识别的性能

    Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation. (arXiv:2305.10951v1 [cs.CL])

    [http://arxiv.org/abs/2305.10951](http://arxiv.org/abs/2305.10951)

    本研究探究了利用数据增强技术，特别是自训练方法，提高低资源语音识别的性能，取得了成功，证明这是一种可行的方法。

    

    近年来，自动语音识别（ASR）系统的性能取得了显著进展，特别是对于具有大量转录语音的语言而言。然而，对于低资源语言，如少数民族语言、地方语言或方言，ASR性能通常仍然较低。本研究探究了数据增强技术是否可以帮助提高低资源ASR性能，重点关注了四种语言或语言变体（日耳曼语系：格罗宁根语、西弗里西亚语；马来-波利尼西亚语系：贝瑟玛语、纳萨尔语）。对于这四种语言，我们研究了自训练的使用，在该方法中，使用训练集数据训练ASR系统，利用该系统生成的转录文本与原始训练数据结合，来训练新的ASR系统。对于已有文本到语音系统（TTS）的格罗宁根语，我们还研究了使用TTS来生成ASR训练数据的方法。

    The performance of automatic speech recognition (ASR) systems has advanced substantially in recent years, particularly for languages for which a large amount of transcribed speech is available. Unfortunately, for low-resource languages, such as minority languages, regional languages or dialects, ASR performance generally remains much lower. In this study, we investigate whether data augmentation techniques could help improve low-resource ASR performance, focusing on four typologically diverse minority languages or language variants (West Germanic: Gronings, West-Frisian; Malayo-Polynesian: Besemah, Nasal). For all four languages, we examine the use of self-training, where an ASR system trained with the available human-transcribed data is used to generate transcriptions, which are then combined with the original data to train a new ASR system. For Gronings, for which there was a pre-existing text-to-speech (TTS) system available, we also examined the use of TTS to generate ASR training 
    
[^23]: 零样本多语言神经机器翻译中的“离谱问题”

    On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation. (arXiv:2305.10930v1 [cs.CL])

    [http://arxiv.org/abs/2305.10930](http://arxiv.org/abs/2305.10930)

    零样本多语言神经机器翻译容易出现“离谱问题”，本文提出的简单且有效的算法LAVS可以通过增加语言之间的KL分歧显著降低这个问题。

    

    尽管多语言神经机器翻译取得了巨大成功，但它仍然存在“离谱问题”，即将翻译输出到错误的语言中。这个问题在零样本翻译任务中更加明显。本文发现，当编码目标语言信号时失效，会导致离谱问题，并且两种语言词汇之间更接近的词汇距离（即KL分歧）与更高的离谱率有关。此外，本文还发现，仅隔离解码器中不同语言的词汇可以缓解这个问题。基于这些发现，我们提出了一种简单有效的算法Language Aware Vocabulary Sharing (LAVS)来构建多语言词汇表，通过增加语言之间的KL分歧，大大减轻了翻译模型的离谱问题。我们在11种语言的多语言翻译基准测试上进行了实验。实验结果表明，对于90个翻译任务，采用LAVS的离谱率降低了37％至90％。

    While multilingual neural machine translation has achieved great success, it suffers from the off-target issue, where the translation is in the wrong language. This problem is more pronounced on zero-shot translation tasks. In this work, we find that failing in encoding discriminative target language signal will lead to off-target and a closer lexical distance (i.e., KL-divergence) between two languages' vocabularies is related with a higher off-target rate. We also find that solely isolating the vocab of different languages in the decoder can alleviate the problem. Motivated by the findings, we propose Language Aware Vocabulary Sharing (LAVS), a simple and effective algorithm to construct the multilingual vocabulary, that greatly alleviates the off-target problem of the translation model by increasing the KL-divergence between languages. We conduct experiments on a multilingual machine translation benchmark in 11 languages. Experiments show that the off-target rate for 90 translation 
    
[^24]: 历史报纸广告中的多语言事件提取

    Multilingual Event Extraction from Historical Newspaper Adverts. (arXiv:2305.10928v1 [cs.CL])

    [http://arxiv.org/abs/2305.10928](http://arxiv.org/abs/2305.10928)

    本文针对历史文本中未被充分开发的事件提取任务，介绍了一个新的多语言数据集，通过迁移学习和领域自适应技术，即使数据注释很少也可以获得出人意料的好结果。同时，所提出的模型提供了新的洞察力，展示了该时期奴役话语的语言使用和模式。研究表明，语言迁移可以有效用于历史文本中的多语言事件提取。

    

    自然语言处理可以帮助历史学家分析比手工可行得多的文本材料。然而，开发这种方法有着实际的挑战。首先，获取大型注释历史文本数据集非常困难，因为只有专业领域的专家才能可靠地注释。其次，大部分现有的自然语言处理模型是在现代语言文本上训练的，因此在应用于历史语料库时效果显著降低。这对较少研究的任务以及非英语语言来说尤其棘手。本文针对这些挑战，聚焦于历史文本领域中未被充分开发的事件提取任务。我们介绍了一个新的多语言数据集，其中包括英语、法语和荷兰语，由早期殖民时期的报纸广告构成，报道了从奴役中自由的被奴役人。我们发现：即使数据注释很少，通过从现代数据集进行迁移学习和领域自适应技术，也可以获得出人意料的好结果；所提出的模型提供了新的洞察力，展示了该时期奴役话语的语言使用和模式；同时，语言迁移可以有效用于历史文本中的多语言事件提取。

    NLP methods can aid historians in analyzing textual materials in greater volumes than manually feasible. Developing such methods poses substantial challenges though. First, acquiring large, annotated historical datasets is difficult, as only domain experts can reliably label them. Second, most available off-the-shelf NLP models are trained on modern language texts, rendering them significantly less effective when applied to historical corpora. This is particularly problematic for less well studied tasks, and for languages other than English. This paper addresses these challenges while focusing on the under-explored task of event extraction from a novel domain of historical texts. We introduce a new multilingual dataset in English, French, and Dutch composed of newspaper ads from the early modern colonial period reporting on enslaved people who liberated themselves from enslavement. We find that: 1) even with scarce annotated data, it is possible to achieve surprisingly good results by 
    
[^25]: 查询性能预测：从Ad-hoc到交互式搜索

    Query Performance Prediction: From Ad-hoc to Conversational Search. (arXiv:2305.10923v1 [cs.IR])

    [http://arxiv.org/abs/2305.10923](http://arxiv.org/abs/2305.10923)

    本文研究了针对从Ad-hoc到交互式搜索中查询性能预测(QPP)的有效方法，并探索了QPP方法在交互式搜索中是否具有推广应用的能力。

    

    查询性能预测(QPP)是信息检索中的一个核心任务。QPP的任务是在没有相关判断的情况下预测查询的检索质量。研究表明，QPP在Ad-hoc搜索中非常有效和有用。近年来，对话式搜索(CS)取得了相当大的进展 。有效的QPP能够帮助CS系统在下一轮决定适当的行动。尽管具有潜力，但CS的QPP研究还很少。本文通过重现和研究现有的QPP方法在CS上的有效性来填补这一研究空白。虽然在两种情况下的通道检索任务相同，但CS中的用户查询取决于对话历史，引入了新的QPP挑战。我们尤其是探讨从Ad-hoc搜索中QPP方法的研究结果在三个CS设置中的推广程度:(i) 评估基于查询重写的检索方法的不同查询的检索质量

    Query performance prediction (QPP) is a core task in information retrieval. The QPP task is to predict the retrieval quality of a search system for a query without relevance judgments. Research has shown the effectiveness and usefulness of QPP for ad-hoc search. Recent years have witnessed considerable progress in conversational search (CS). Effective QPP could help a CS system to decide an appropriate action to be taken at the next turn. Despite its potential, QPP for CS has been little studied. We address this research gap by reproducing and studying the effectiveness of existing QPP methods in the context of CS. While the task of passage retrieval remains the same in the two settings, a user query in CS depends on the conversational history, introducing novel QPP challenges. In particular, we seek to explore to what extent findings from QPP methods for ad-hoc search generalize to three CS settings: (i) estimating the retrieval quality of different query rewriting-based retrieval met
    
[^26]: 使用注意力机制的紧急通信

    Emergent Communication with Attention. (arXiv:2305.10920v1 [cs.CL])

    [http://arxiv.org/abs/2305.10920](http://arxiv.org/abs/2305.10920)

    该论文研究了在计算代理间如何实现更好地紧急通信的方法，并发现引入注意力机制可以得到更具组成性和可解释性的紧急语言。

    

    为了开发出更好地使用自己的紧急语言进行通信的计算代理，我们赋予代理在环境中关注特定概念的能力。人类经常将一个对象或场景理解为概念的组合，并将这些概念进一步映射到单词上。我们将这种直觉实现为"Speaker"和"Listener"代理中的跨模态注意力机制，在引用游戏中显示注意力导致更具组成性和可解释性的紧急语言。我们还通过调查每个消息符号相关的注意权重以及"Speaker"和"Listener"代理之间注意权重的对齐，展示了注意力如何帮助理解学习到的通信协议。总的来说，我们的结果表明，注意力是开发更具人类化的紧急语言的一种有前途的机制。

    To develop computational agents that better communicate using their own emergent language, we endow the agents with an ability to focus their attention on particular concepts in the environment. Humans often understand an object or scene as a composite of concepts and those concepts are further mapped onto words. We implement this intuition as cross-modal attention mechanisms in Speaker and Listener agents in a referential game and show attention leads to more compositional and interpretable emergent language. We also demonstrate how attention aids in understanding the learned communication protocol by investigating the attention weights associated with each message symbol and the alignment of attention weights between Speaker and Listener agents. Overall, our results suggest that attention is a promising mechanism for developing more human-like emergent language.
    
[^27]: 利用语义先验细化的弱监督视觉-文本对齐

    Weakly-Supervised Visual-Textual Grounding with Semantic Prior Refinement. (arXiv:2305.10913v1 [cs.CV])

    [http://arxiv.org/abs/2305.10913](http://arxiv.org/abs/2305.10913)

    本文提出了一种利用语义先验细化的弱监督视觉-文本对齐方法，仅使用图像-句子对进行学习，其目标是实现实体表示中的区域-短语对应关系，通过联合两个主要模块的输出进行预测。

    

    弱监督视觉-文本对齐的目标是仅利用图像-句子对学习实体表示中的区域-短语对应关系。与监督方法相比，其难度更大，因为无法获得边界框和文本短语的对应关系。因此，我们提出了语义先验细化模型（SPRM），其预测结果是通过组合两个主要模块的输出得到的。第一个未经训练的模块旨在返回文本短语和边界框之间的粗略对齐。第二个训练过的模块由两个子组件组成，用于细化粗略的对齐以提高最终短语-边界框对齐的准确性。该模型的训练目标是最大化图像和句子之间的多模态相似度，同时使同一句子和一个新的不相关的图像的多模态相似度最小化，以在训练过程中最大限度地提高训练效果。我们的方法在两个流行的数据集上展现了最先进的结果。

    Using only image-sentence pairs, weakly-supervised visual-textual grounding aims to learn region-phrase correspondences of the respective entity mentions. Compared to the supervised approach, learning is more difficult since bounding boxes and textual phrases correspondences are unavailable. In light of this, we propose the Semantic Prior Refinement Model (SPRM), whose predictions are obtained by combining the output of two main modules. The first untrained module aims to return a rough alignment between textual phrases and bounding boxes. The second trained module is composed of two sub-components that refine the rough alignment to improve the accuracy of the final phrase-bounding box alignments. The model is trained to maximize the multimodal similarity between an image and a sentence, while minimizing the multimodal similarity of the same sentence and a new unrelated image, carefully selected to help the most during training. Our approach shows state-of-the-art results on two popula
    
[^28]: 在中间休息一下：探索向层次脚本生成的子目标。

    Take a Break in the Middle: Investigating Subgoals towards Hierarchical Script Generation. (arXiv:2305.10907v1 [cs.CL])

    [http://arxiv.org/abs/2305.10907](http://arxiv.org/abs/2305.10907)

    本文提出了从认知理论的角度扩展目标导向的脚本生成任务，并建立了一个新的数据集，验证了将子目标纳入层次脚本生成的有效性。

    

    目标导向的脚本生成是生成能够实现给定目标的步骤列表的新任务。本文提出从认知理论的角度扩展该任务。步骤通常是分层组织的，而不是简单的平面结构——人们常常将复杂的任务分解成子目标，每个子目标可以进一步分解为步骤。为了建立基准，我们贡献了一个新的数据集，提出了几种基准方法，并设置了评估指标。自动和人工评估都验证了数据集的高质量以及将子目标纳入层次脚本生成的有效性。此外，我们还设计和评估了发现子目标的模型，并发现将目标分解成子目标比从分段步骤中总结更为困难。

    Goal-oriented Script Generation is a new task of generating a list of steps that can fulfill the given goal. In this paper, we propose to extend the task from the perspective of cognitive theory. Instead of a simple flat structure, the steps are typically organized hierarchically - Human often decompose a complex task into subgoals, where each subgoal can be further decomposed into steps. To establish the benchmark, we contribute a new dataset, propose several baseline methods, and set up evaluation metrics. Both automatic and human evaluation verify the high-quality of dataset, as well as the effectiveness of incorporating subgoals into hierarchical script generation. Furthermore, We also design and evaluate the model to discover subgoal, and find that it is a bit more difficult to decompose the goals than summarizing from segmented steps.
    
[^29]: EventNet-ITA: 用于意大利事件的框架解析

    EventNet-ITA: Italian Frame Parsing for Events. (arXiv:2305.10892v1 [cs.CL])

    [http://arxiv.org/abs/2305.10892](http://arxiv.org/abs/2305.10892)

    本文介绍了一个用于意大利语的大规模、多领域事件框架标注语料库-EventNet-ITA，并提出了一种高效的多标签框架解析方法。EventNet-ITA的主要贡献是为研究人员提供了一个用于文本事件挖掘的资源和意大利语框架解析的新颖工具。

    

    本文介绍了EventNet-ITA，这是一个用于意大利语的大规模、多领域事件框架标注语料库，并提出了一种高效的多标签框架解析方法。然后在该数据集上进行了评估。涵盖了各种个人、社会和历史现象，EventNet-ITA的主要贡献是为研究人员提供了一个用于文本事件挖掘的资源以及意大利语框架解析的新颖而广泛的工具。

    This paper introduces EventNet-ITA, a large, multi-domain corpus annotated with event frames for Italian, and presents an efficient approach for multi-label Frame Parsing. The approach is then evaluated on the dataset. Covering a wide range of individual, social and historical phenomena, the main contribution of EventNet-ITA is to provide the research community with a resource for textual event mining and a novel and extensive tool for Frame Parsing in Italian.
    
[^30]: TEPrompt：任务启发提示学习在隐式语篇关系识别中的应用

    TEPrompt: Task Enlightenment Prompt Learning for Implicit Discourse Relation Recognition. (arXiv:2305.10866v1 [cs.CL])

    [http://arxiv.org/abs/2305.10866](http://arxiv.org/abs/2305.10866)

    该论文提出一种任务启发提示学习模型（TEPrompt），用于隐式语篇关系识别，其通过融合与任务相关的特征来改善IDRR性能。

    

    隐式语篇关系识别（IDRR）旨在对两个参数之间的关系进行分类，而不使用显式的连词。我们提出了一种任务启发提示学习模型，称为TEPrompt，用于融合三个相关任务的特征以进行IDRR。该模型包含三个任务，即语篇关系识别（DRR）、语义意义分类（SSC）和注释连词预测（ACP）。

    Implicit Discourse Relation Recognition (IDRR) aims at classifying the relation sense between two arguments without an explicit connective. Recently, the ConnPrompt~\cite{Wei.X:et.al:2022:COLING} has leveraged the powerful prompt learning for IDRR based on the fusion of multi-prompt decisions from three different yet much similar connective prediction templates. Instead of multi-prompt ensembling, we propose to design auxiliary tasks with enlightened prompt learning for the IDRR task. Although an auxiliary task is not used to directly output final prediction, we argue that during the joint training some of its learned features can be useful to boost the main task. In light of such motivations, we propose a task enlightenment prompt learning model, called TEPrompt, to fuse learned features from three related tasks for IDRR. In particular, the TEPrompt contains three tasks, viz., Discourse Relation Recognition (DRR), Sense Semantics Classification (SSC) and Annotated Connective Predictio
    
[^31]: 多智能体强化学习中的语义对齐任务分解

    Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning. (arXiv:2305.10865v1 [cs.LG])

    [http://arxiv.org/abs/2305.10865](http://arxiv.org/abs/2305.10865)

    该论文提出了一种多智能体强化学习中的新方法SAMA，通过提前训练的语言模型和任务分解来解决ASG方法存在的样本效率问题和生成非实际任务奖励的子目标的问题。

    

    合作型MARL中的奖励稀疏问题着重于适当的信用分配。自动子目标生成（ASG）是最近出现的一种可行的MARL方法，其灵感来自于在内在驱动的增强学习中利用子目标。然而，从稀疏奖励中进行复杂任务规划的端到端学习无疑需要大量的培训样本。为了解决这个问题，我们提出了一种新的"解耦"决策方法，即在MARL中的语义对齐任务分解（SAMA），受到解耦表示学习的启发。

    The difficulty of appropriately assigning credit is particularly heightened in cooperative MARL with sparse reward, due to the concurrent time and structural scales involved. Automatic subgoal generation (ASG) has recently emerged as a viable MARL approach inspired by utilizing subgoals in intrinsically motivated reinforcement learning. However, end-to-end learning of complex task planning from sparse rewards without prior knowledge, undoubtedly requires massive training samples. Moreover, the diversity-promoting nature of existing ASG methods can lead to the "over-representation" of subgoals, generating numerous spurious subgoals of limited relevance to the actual task reward and thus decreasing the sample efficiency of the algorithm. To address this problem and inspired by the disentangled representation learning, we propose a novel "disentangled" decision-making method, Semantically Aligned task decomposition in MARL (SAMA), that prompts pretrained language models with chain-of-thou
    
[^32]: 利用OpenCorpora的范式检索推进全文搜索词形还原技术

    Advancing Full-Text Search Lemmatization Techniques with Paradigm Retrieval from OpenCorpora. (arXiv:2305.10848v1 [cs.CL])

    [http://arxiv.org/abs/2305.10848](http://arxiv.org/abs/2305.10848)

    本文介绍了一种利用OpenCorpora数据集和定制算法增强全文搜索词形还原的方法，同时提出了一种紧凑的词典存储策略，大大提高了词形检索的速度和精度。

    

    本文提出了一种突破性方法，利用OpenCorpora数据集和定制的范式检索算法来增强全文搜索词形还原。我们的主要目标是简化单词的主要形式或词形——这是全文搜索中的关键因素。此外，我们提出了一种紧凑的词典存储策略，大大提高了词形检索的速度和精度。

    In this paper, we unveil a groundbreaking method to amplify full-text search lemmatization, utilizing the OpenCorpora dataset and a bespoke paradigm retrieval algorithm. Our primary aim is to streamline the extraction of a word's primary form or lemma - a crucial factor in full-text search. Additionally, we propose a compact dictionary storage strategy, significantly boosting the speed and precision of lemma retrieval.
    
[^33]: 大型语言模型可以被引导来规避AI生成的文本检测

    Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v1 [cs.CL])

    [http://arxiv.org/abs/2305.10847](http://arxiv.org/abs/2305.10847)

    本文揭示了大型语言模型可以通过精心设计的提示语来有效规避现有的文本检测系统，证明了这些检测器的脆弱性。

    

    大型语言模型在包括论文写作和问答等多个任务中展现出了出色的表现。然而，必须解决这些模型潜在的误用问题，否则可能导致抄袭和垃圾信息等不良后果。本研究揭示，通过精心设计的提示语，LLMs可以有效地规避检测系统。我们提出了一种新颖的基于替换的上下文示例优化方法（SICO），用于自动生成这种提示语。在三个现实任务中，LLMs可能被误用，在SICO的帮助下，ChatGPT成功地规避了六项现有的检测器，平均导致0.54的AUC下降。令人惊讶的是，在大多数情况下，这些检测器的表现甚至比随机分类器还要差。这些结果坚定地揭示了现有检测器的脆弱性。

    Large Language Models (LLMs) have demonstrated exceptional performance in a variety of tasks, including essay writing and question answering. However, it is crucial to address the potential misuse of these models, which can lead to detrimental outcomes such as plagiarism and spamming. Recently, several detectors have been proposed, including fine-tuned classifiers and various statistical methods. In this study, we reveal that with the aid of carefully crafted prompts, LLMs can effectively evade these detection systems. We propose a novel Substitution-based In-Context example Optimization method (SICO) to automatically generate such prompts. On three real-world tasks where LLMs can be misused, SICO successfully enables ChatGPT to evade six existing detectors, causing a significant 0.54 AUC drop on average. Surprisingly, in most cases these detectors perform even worse than random classifiers. These results firmly reveal the vulnerability of existing detectors. Finally, the strong perfor
    
[^34]: TAPIR：使用双通道模型学习自适应修订增量自然语言理解

    TAPIR: Learning Adaptive Revision for Incremental Natural Language Understanding with a Two-Pass Model. (arXiv:2305.10845v1 [cs.CL])

    [http://arxiv.org/abs/2305.10845](http://arxiv.org/abs/2305.10845)

    TAPIR使用双通道模型实现自适应修订增量自然语言理解，并取得了在ATIS数据集上最先进的性能。

    

    语言本质上是增量式的，这对自然语言处理系统来说是个优势，可以为实时交互应用提供快速响应。最近的基于神经网络的增量处理方法主要使用RNN或Transformer。RNN速度快但单调（不能纠正早期的输出，这在增量处理中很必要）。另一方面，Transformer使用整个序列，因此本质上不是增量的。为了获得部分输出并提供修订能力，可以使用重启增量界面重复传递更长的输入前缀。然而，随着句子变得越来越长，这种方法变得代价高昂。在这项工作中，我们提出了AdaPtIve修订的双通道模型TAPIR，并介绍了一种获得自适应修订策略的增量监督信号的方法。序列标记的实验结果表明，我们的模型优于现有的增量模型，并在ATIS数据集上实现了最先进的性能。

    Language is by its very nature incremental in how it is produced and processed. This property can be exploited by NLP systems to produce fast responses, which has been shown to be beneficial for real-time interactive applications. Recent neural network-based approaches for incremental processing mainly use RNNs or Transformers. RNNs are fast but monotonic (cannot correct earlier output, which can be necessary in incremental processing). Transformers, on the other hand, consume whole sequences, and hence are by nature non-incremental. A restart-incremental interface that repeatedly passes longer input prefixes can be used to obtain partial outputs, while providing the ability to revise. However, this method becomes costly as the sentence grows longer. In this work, we propose the Two-pass model for AdaPtIve Revision (TAPIR) and introduce a method to obtain an incremental supervision signal for learning an adaptive revision policy. Experimental results on sequence labelling show that our
    
[^35]: 基于词汇感知的非自回归Transformer语音识别模型

    A Lexical-aware Non-autoregressive Transformer-based ASR Model. (arXiv:2305.10839v1 [cs.CL])

    [http://arxiv.org/abs/2305.10839](http://arxiv.org/abs/2305.10839)

    此论文提出了一个基于词汇感知的非自回归Transformer语音识别模型，该模型集成了语音-文本共享编码器和解码器，并能够同时训练语音和文本数据以提高性能，实验结果显示该模型在多个数据集上拥有领先的性能表现。

    

    非自回归自动语音识别（ASR）由于其快速解码速度和令人满意的结果已经成为ASR建模的主流。为进一步提高性能，放松条件独立假设和级联大规模预训练模型是两个活跃的研究方向。除了这些策略外，我们提出了一个基于词汇感知的非自回归Transformer（LA-NAT）ASR框架，其中包括一个声学编码器、一个语音-文本共享编码器和一个语音-文本共享解码器。声学编码器像往常一样用于处理输入语音特征，而语音-文本共享编码器和解码器旨在同时训练语音和文本数据。这样做，LA-NAT旨在使ASR模型感知词汇信息，因此利用学习到的语言知识，预期产生更好的结果。我们在AISHELL-1、CSJ和TEDLIUM 2数据集上进行了一系列实验。根据实验结果，我们提出的LA-NAT模型在三个数据集上优于基线模型，并取得了最先进的性能。

    Non-autoregressive automatic speech recognition (ASR) has become a mainstream of ASR modeling because of its fast decoding speed and satisfactory result. To further boost the performance, relaxing the conditional independence assumption and cascading large-scaled pre-trained models are two active research directions. In addition to these strategies, we propose a lexical-aware non-autoregressive Transformer-based (LA-NAT) ASR framework, which consists of an acoustic encoder, a speech-text shared encoder, and a speech-text shared decoder. The acoustic encoder is used to process the input speech features as usual, and the speech-text shared encoder and decoder are designed to train speech and text data simultaneously. By doing so, LA-NAT aims to make the ASR model aware of lexical information, so the resulting model is expected to achieve better results by leveraging the learned linguistic knowledge. A series of experiments are conducted on the AISHELL-1, CSJ, and TEDLIUM 2 datasets. Acco
    
[^36]: Ahead-of-Time P-Tuning：一种应用于预训练语言模型的参数节约的微调方法

    Ahead-of-Time P-Tuning. (arXiv:2305.10835v1 [cs.LG])

    [http://arxiv.org/abs/2305.10835](http://arxiv.org/abs/2305.10835)

    本文提出了 Ahead-of-Time （AoT）P-Tuning，一种新颖的微调方法，它通过在每个Transformer层之前添加输入相关的偏置，实现了应用于预训练语言模型的参数节约。该方法在GLUE和SuperGLUE基准数据集上优于BitFit，并可用于多任务推理，而推理开销却很小。

    

    本文提出了 Ahead-of-Time （AoT）P-Tuning，一种新颖的微调方法，可以在每个Transformer层之前添加输入相关的偏置，以应用于预训练的语言模型（LMs）。我们使用RoBERTa和DeBERTa模型在GLUE和SuperGLUE基准数据集上评估AoT P-Tuning，结果表明它优于BitFit，并且与其他基准方法相比，效率更高。此外，我们评估了AoT P-Tuning的推理开销，并证明它与已建立的基准方法相比，引入的开销可以忽略不计。我们的方法可以使用单个骨干LM进行多任务推理，从而成为实际应用的解决方案。

    In this paper, we propose Ahead-of-Time (AoT) P-Tuning, a novel parameter-efficient fine-tuning method for pre-trained Language Models (LMs) that adds input-dependent bias before each Transformer layer. We evaluate AoT P-Tuning on GLUE and SuperGLUE benchmarking datasets using RoBERTa and DeBERTa models, showing that it outperforms BitFit and is comparable or better than other baseline methods for efficient fine-tuning. Additionally, we assess the inference overhead of AoT P-Tuning and demonstrate that it introduces negligible overhead compared to established baseline methods. Our method enables multi-task inference with a single backbone LM, making it a practical solution for real-world applications.
    
[^37]: 《AI写作：图像生成和数字写作的关系》

    AIwriting: Relations Between Image Generation and Digital Writing. (arXiv:2305.10834v1 [cs.AI])

    [http://arxiv.org/abs/2305.10834](http://arxiv.org/abs/2305.10834)

    本论坛讨论了基于AI的文本生成系统和文本到图像生成系统对数字艺术和电子文学领域带来的影响，各种作品都从文学角度考虑，突出了创作过程的交互性。

    

    2022年，基于变形金刚的AI文本生成系统（如GPT-3）和基于AI的文本到图像生成系统（如DALL-E 2和稳定扩散）都取得了指数级的飞跃，无疑正在改变数字艺术和电子文学领域。在本论坛中，一组电子文学作者和理论家考虑这些系统带来的人类创造力新机遇，并展示了他们在过去一年中专门针对这些系统创作的作品，这些作品通过迭代的对话过程转化为视觉表现。这些演示的前提是，这些系统和所生成的作品必须从文学角度考虑，因为它们起源于人类写作。从个人健康危机的视觉回忆录，到互动网页漫画，到基于抽象诗意语言的建筑，再到政治萨。。。

    During 2022, both transformer-based AI text generation sys-tems such as GPT-3 and AI text-to-image generation systems such as DALL-E 2 and Stable Diffusion made exponential leaps forward and are unquestionably altering the fields of digital art and electronic literature. In this panel a group of electronic literature authors and theorists consider new oppor-tunities for human creativity presented by these systems and present new works have produced during the past year that specifically address these systems as environments for literary expressions that are translated through iterative interlocutive processes into visual representations. The premise that binds these presentations is that these systems and the works gener-ated must be considered from a literary perspective, as they originate in human writing. In works ranging from a visual memoir of the personal experience of a health crisis, to interac-tive web comics, to architectures based on abstract poetic language, to political sa
    
[^38]: 深度学习方法用于提取花和植物的隐喻性名称

    Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants. (arXiv:2305.10833v1 [cs.CL])

    [http://arxiv.org/abs/2305.10833](http://arxiv.org/abs/2305.10833)

    本研究使用深度学习模型识别对话中基于隐喻的花和植物名称，鉴别模型表现优于GPT-3.5，最好的表现器在任务中报告了92.2349％的F1分数。

    

    植物学领域充满了隐喻性术语，这些术语在描述和识别花和植物方面起着重要作用。但是，在对话中识别这些术语是一项艰巨的任务。在翻译过程和词典编纂任务中，这往往导致错误的发生。当涉及到单词和短语时，在机器翻译方面这个过程更具挑战性。自然语言处理（NLP）应用和机器翻译（MT）技术的最新关注点之一是通过深度学习（DL）自动识别对话中基于隐喻的单词。在本研究中，我们使用了十三种流行的变压器模型以及ChatGPT来填补这一空白，并且通过F1得分证明了鉴别模型优于GPT-3.5模型，我们最好的表现器在隐喻花卉和植物名称识别任务中报告了92.2349％的F1分数。

    The domain of Botany is rich with metaphorical terms. Those terms play an important role in the description and identification of flowers and plants. However, the identification of such terms in discourse is an arduous task. This leads in some cases to committing errors during translation processes and lexicographic tasks. The process is even more challenging when it comes to machine translation, both in the cases of single-word terms and multi-word terms. One of the recent concerns of Natural Language Processing (NLP) applications and Machine Translation (MT) technologies is the automatic identification of metaphor-based words in discourse through Deep Learning (DL). In this study, we seek to fill this gap through the use of thirteen popular transformer based models, as well as ChatGPT, and we show that discriminative models perform better than GPT-3.5 model with our best performer reporting 92.2349% F1 score in metaphoric flower and plant names identification task.
    
[^39]: CLEME: 针对语法错误修正的去偏置多参考评估方法

    CLEME: Debiasing Multi-reference Evaluation for Grammatical Error Correction. (arXiv:2305.10819v1 [cs.CL])

    [http://arxiv.org/abs/2305.10819](http://arxiv.org/abs/2305.10819)

    提出了一种新的Chunk-LEvel Multi-reference Evaluation (CLEME)方法来解决在多参考环境下评估GEC系统时存在的偏差问题，其通过消除由不一致的编辑边界引起的偏差和自动确定语法错误的边界来提高了GEC评估的可靠性。

    

    由于Grammatical Error Correction (GEC)是一项高度主观的任务，因此评估其性能变得困难。设计尽可能客观的评估指标对于GEC任务的发展至关重要。先前的主流评估指标，即基于参考的指标，在提取编辑时未考虑多个参考的存在，从而引入偏见到多参考评估中。为了克服这个问题，我们提出了Chunk-LEvel Multi-reference Evaluation (CLEME)方法，旨在在多参考环境中评估GEC系统。首先，CLEME为源、假设和所有参考建立具有一致边界的块序列，从而消除由不一致的编辑边界引起的偏差。然后，基于发现存在不同语法错误之间的边界，我们自动确定了语法错误的边界，并以一种新颖的方式计算了F$_{0.5}$得分。我们提出的CLEME方法可以有效地去偏置多参考评估GEC系统，并提高GEC评估的可靠性。

    It is intractable to evaluate the performance of Grammatical Error Correction (GEC) systems since GEC is a highly subjective task. Designing an evaluation metric that is as objective as possible is crucial to the development of GEC task. Previous mainstream evaluation metrics, i.e., reference-based metrics, introduce bias into the multi-reference evaluation because they extract edits without considering the presence of multiple references. To overcome the problem, we propose Chunk-LEvel Multi-reference Evaluation (CLEME) designed to evaluate GEC systems in multi-reference settings. First, CLEME builds chunk sequences with consistent boundaries for the source, the hypothesis and all the references, thus eliminating the bias caused by inconsistent edit boundaries. Then, based on the discovery that there exist boundaries between different grammatical errors, we automatically determine the grammatical error boundaries and compute F$_{0.5}$ scores in a novel way. Our proposed CLEME approach
    
[^40]: 民主扩散语言模型

    Democratized Diffusion Language Model. (arXiv:2305.10818v1 [cs.LG])

    [http://arxiv.org/abs/2305.10818](http://arxiv.org/abs/2305.10818)

    本文提出了一个基于CDCD框架的民主扩散语言模型（DDLM），并通过GLUE基准测试了其知识转移能力，为研究人员提供了DDLM训练和评估流程以及已训练的DDLM模型。

    

    尽管扩散模型在自然语言处理中有潜在好处，但目前公开的实现、训练模型或可重现的训练程序并不存在。为解决这些挑战，我们提出了基于CDCD框架的民主扩散语言模型（DDLM）。我们提出了一种用C4数据集简化的DDLM训练流程，并对训练模型的行为进行了深入分析。此外，我们引入了一种用于速度更快的采样的新型早期退出策略，该策略针对使用得分插值训练的模型。由于此前没有研究旨在使用预训练扩散LM解决下游任务（例如分类任务），我们在GLUE基准上进行了实验，以研究DDLM的知识转移能力。通过本文，我们提出了可供其他研究人员使用的DDLM训练和评估流程以及预先训练的DDLM模型，这些模型可在未来的D相关的研究中使用。

    Despite the potential benefits of Diffusion Models for NLP applications, publicly available implementations, trained models, or reproducible training procedures currently need to be publicly available. We present the Democratized Diffusion Language Model (DDLM), based on the Continuous Diffusion for Categorical Data (CDCD) framework, to address these challenges. We propose a simplified training procedure for DDLM using the C4 dataset and perform an in-depth analysis of the trained model's behavior. Furthermore, we introduce a novel early-exiting strategy for faster sampling with models trained with score interpolation. Since no previous works aimed at solving downstream tasks with pre-trained Diffusion LM (e.g., classification tasks), we experimented with GLUE Benchmark to study the ability of DDLM to transfer knowledge. With this paper, we propose available training and evaluation pipelines to other researchers and pre-trained DDLM models, which could be used in future research with D
    
[^41]: Whisper-KDQ: 通过引导知识蒸馏和量化实现高效ASR的轻型Whisper

    Whisper-KDQ: A Lightweight Whisper via Guided Knowledge Distillation and Quantization for Efficient ASR. (arXiv:2305.10788v1 [cs.SD])

    [http://arxiv.org/abs/2305.10788](http://arxiv.org/abs/2305.10788)

    本文提出了一种通过引导知识蒸馏和量化，实现对大型预训练语音识别模型Whisper进行压缩优化的方法，可以将模型大小缩小并提高性能。

    

    随着计算硬件资源的快速发展和数据的显著增长，预训练模型在语音识别等任务中的应用显著提高了性能。然而，这些模型通常具有很高的计算开销，使其难以在资源受限的设备上有效执行。为了加速推理、减少模型大小，并保持性能，我们提出了一种新颖的引导知识蒸馏和量化方法，用于大型预训练模型Whisper。学生模型基于量化损失和蒸馏损失选择蒸馏和量化层。我们将$\text{Whisper}_\text{small}$压缩到$\text{Whisper}_\text{base}$和$\text{Whisper}_\text{tiny}$级别，使$\text{Whisper}_\text{small}$分别小5.18x/10.48x。此外，与原始$\text{Whisper}_\text{base}$和$\text{Whisper}_\text{tiny}$相比，还有相对字符错误率降低.

    Due to the rapid development of computing hardware resources and the dramatic growth of data, pre-trained models in speech recognition, such as Whisper, have significantly improved the performance of speech recognition tasks. However, these models usually have a high computational overhead, making it difficult to execute effectively on resource-constrained devices. To speed up inference and reduce model size while maintaining performance, we propose a novel guided knowledge distillation and quantization for large pre-trained model Whisper. The student model selects distillation and quantization layers based on quantization loss and distillation loss, respectively. We compressed $\text{Whisper}_\text{small}$ to $\text{Whisper}_\text{base}$ and $\text{Whisper}_\text{tiny}$ levels, making $\text{Whisper}_\text{small}$ 5.18x/10.48x smaller, respectively. Moreover, compared to the original $\text{Whisper}_\text{base}$ and $\text{Whisper}_\text{tiny}$, there is also a relative character erro
    
[^42]: Ditto: 一种改进句子嵌入的简洁有效方法

    Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings. (arXiv:2305.10786v1 [cs.CL])

    [http://arxiv.org/abs/2305.10786](http://arxiv.org/abs/2305.10786)

    提出了一种名为Ditto的简单有效的方法，其可以解决预训练语言模型中存在的各向异性问题，并在语义文本相似性任务中提高模型的性能。

    

    先前的研究诊断了由预训练语言模型（如BERT）产生的句子表示中存在的各向异性问题，没有进行微调。我们的分析揭示了BERT产生的句子嵌入存在偏向于非信息性单词的偏见，限制了在语义文本相似性（STS）任务中的性能。为了解决这种偏见问题，我们提出了一个简单有效的无监督方法——对角线注意力池化（Ditto），该方法使用基于模型的重要性估计权重单词，并计算预训练模型的单词表示的加权平均值作为句子嵌入。Ditto可以轻松地应用于任何预训练语言模型作为后处理操作。与先前的句子嵌入方法相比，Ditto不添加参数也不需要任何学习。实证评估表明，我们提出的Ditto可以缓解各向异性问题并提高在STS任务中各种预训练模型的性能。

    Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks. To address this bias, we propose a simple and efficient unsupervised approach, Diagonal Attention Pooling (Ditto), which weights words with model-based importance estimations and computes the weighted average of word representations from pre-trained models as sentence embeddings. Ditto can be easily applied to any pre-trained language model as a postprocessing operation. Compared to prior sentence embedding approaches, Ditto does not add parameters nor requires any learning. Empirical evaluations demonstrate that our proposed Ditto can alleviate the anisotropy problem and improve various pre-trained models on STS tasks.
    
[^43]: 对生成事实一致性文本摘要进行反事实偏差校正

    Counterfactual Debiasing for Generating Factually Consistent Text Summaries. (arXiv:2305.10736v1 [cs.CL])

    [http://arxiv.org/abs/2305.10736](http://arxiv.org/abs/2305.10736)

    本文提出了一个名为CoFactSum的去偏框架，通过反事实估计减轻原因，解决了生成文本摘要的事实不一致性问题，并在两个广泛使用的数据集上取得了优于现有模型的效果。

    

    尽管在生成流畅且信息丰富的文本摘要方面已经取得了实质性的进展，但所生成摘要的事实不一致性仍然是一个重要而具有挑战性的问题。本文针对抽象文本摘要构建因果图，并确定了造成事实不一致性的内在原因，即语言偏见和无关性偏见，并进一步提出了一个名为CoFactSum的去偏框架，通过反事实估计减轻这些偏差的因果影响。具体而言，所提出的CoFactSum提供了两种反事实估计策略，即明确反事实遮蔽具有明确的动态遮蔽策略，并采用隐式鉴别交叉关注机制的隐式反事实训练。同时，我们设计了一个去偏度调整机制，在每个解码步骤动态适应去偏程度。对两个广泛使用的摘要数据集的广泛实验证明了CoFactSum有效地减少了事实不一致性，并优于现有的最新的摘要模型。

    Despite substantial progress in abstractive text summarization to generate fluent and informative texts, the factual inconsistency in the generated summaries remains an important yet challenging problem to be solved. In this paper, we construct causal graphs for abstractive text summarization and identify the intrinsic causes of the factual inconsistency, i.e., the language bias and irrelevancy bias, and further propose a debiasing framework, named CoFactSum, to alleviate the causal effects of these biases by counterfactual estimation. Specifically, the proposed CoFactSum provides two counterfactual estimation strategies, i.e., Explicit Counterfactual Masking with an explicit dynamic masking strategy, and Implicit Counterfactual Training with an implicit discriminative cross-attention mechanism. Meanwhile, we design a Debiasing Degree Adjustment mechanism to dynamically adapt the debiasing degree at each decoding step. Extensive experiments on two widely-used summarization datasets dem
    
[^44]: 基于扩散的语音增强方法与联合生成预测解码器

    Diffusion-Based Speech Enhancement with Joint Generative and Predictive Decoders. (arXiv:2305.10734v1 [cs.SD])

    [http://arxiv.org/abs/2305.10734](http://arxiv.org/abs/2305.10734)

    本文提出了一种集成生成和预测信息的基于扩散的语音增强系统，其中两个语音增强模块在第一和最后一个扩散步骤中被融合，实验结果表明扩散评分估计可以从预测信息中受益并加快解码过程。

    

    近期研究了基于扩散的语音增强方法，但其解码过程非常耗时。解决方案之一是使用预测性语音增强系统估计增强特征，然后初始化解码过程。然而，这种两阶段方法忽略了预测性与扩散性语音增强之间的互补性。本文提出了一种集成这两个语音增强模块的统一系统。该系统编码生成和预测信息，然后应用生成和预测解码器，它们的输出被融合。具体地，两个语音增强模块在第一和最后一个扩散步骤中被融合：第一个步骤融合使用预测性语音增强来初始化扩散过程，以提高收敛速度；最后一个步骤融合将两个互补的语音增强输出组合起来，以提高语音增强性能。在Voice-Bank数据集上进行的实验表明，扩散评分估计可以从预测信息中获益并加快解码过程。

    Diffusion-based speech enhancement (SE) has been investigated recently, but its decoding is very time-consuming. One solution is to initialize the decoding process with the enhanced feature estimated by a predictive SE system. However, this two-stage method ignores the complementarity between predictive and diffusion SE. In this paper, we propose a unified system that integrates these two SE modules. The system encodes both generative and predictive information, and then applies both generative and predictive decoders, whose outputs are fused. Specifically, the two SE modules are fused in the first and final diffusion steps: the first step fusion initializes the diffusion process with the predictive SE for improving the convergence, and the final step fusion combines the two complementary SE outputs to improve the SE performance. Experiments on the Voice-Bank dataset show that the diffusion score estimation can benefit from the predictive information and speed up the decoding.
    
[^45]: 在直播聊天中分析规范违背现象

    Analyzing Norm Violations in Live-Stream Chat. (arXiv:2305.10731v1 [cs.CL])

    [http://arxiv.org/abs/2305.10731](http://arxiv.org/abs/2305.10731)

    本文分享了第一次针对在直播平台上检测规范违背现象的自然语言处理研究，通过分类别标注和用户研究，揭示了直播聊天规范形成的见解。

    

    毒性言语，如仇恨言论，可能会阻止用户参与在线社区和流行平台，影响他们的体验。以前的检测工具主要关注于在线论坛和社交媒体（如Reddit和Twitter）的对话。但是，将这些方法应用于直播平台（如Twitch和YouTube Live）中的对话时，由于每个评论仅可见一段时间，并且缺少与其他评论建立关系的线程结构，因此这些方法的效果较差。本文分享了第一次针对在直播平台上检测规范违背现象的自然语言处理研究。我们在Twitch上对4,583个受过审查的评论进行了分类别标注，并定义了直播聊天中的规范违反类别。我们讨论了直播数据与其他论坛的几个区别，并证明了当前的模型在这种情况下表现不佳。通过一项用户研究，我们确定了文本和语音中规范违背之间的差异，并揭示了直播聊天规范形成的见解。

    Toxic language, such as hate speech, can deter users from participating in online communities and enjoying popular platforms. Previous approaches to detecting toxic language and norm violations have been primarily concerned with conversations from online forums and social media, such as Reddit and Twitter. These approaches are less effective when applied to conversations on live-streaming platforms, such as Twitch and YouTube Live, as each comment is only visible for a limited time and lacks a thread structure that establishes its relationship with other comments. In this work, we share the first NLP study dedicated to detecting norm violations in conversations on live-streaming platforms. We define norm violation categories in live-stream chats and annotate 4,583 moderated comments from Twitch. We articulate several facets of live-stream data that differ from other forums, and demonstrate that existing models perform poorly in this setting. By conducting a user study, we identify the 
    
[^46]: 平坦度感知的Prompt选择能提高精度和样本效率

    Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency. (arXiv:2305.10713v1 [cs.CL])

    [http://arxiv.org/abs/2305.10713](http://arxiv.org/abs/2305.10713)

    本论文提出了一种新的度量--Prompt平坦度，可以优化语言提示选择，提高模型分类的准确性和样本效率，实验证明结合现有度量可以提高性能和样本效率。

    

    随着大型语言模型的能力不断增长，提示已成为访问它们的主要方式。这激发了自动选择有效语言提示策略的发展。本文介绍Prompt平坦度，一种量化语言提示预期效用的新度量。该度量受统计学习中的平坦度正则化启发，量化模型对其参数扰动的稳健性。我们提供该度量的理论基础及其与其他Prompt选择度量的关系，从而全面了解现有方法。从经验上讲，我们表明，将Prompt平坦度与现有度量结合使用可以提高性能和样本效率。在6个分类基准测试中，我们的度量优于以前的Prompt选择度量，平均精度提高5％，Pearson相关性提高10％。

    With growing capabilities of large language models, prompting them has become the dominant way to access them. This has motivated the development of strategies for automatically selecting effective language prompts. In this paper, we introduce prompt flatness, a new metric to quantify the expected utility of a language prompt. This metric is inspired by flatness regularization in statistical learning that quantifies the robustness of the model towards its parameter perturbations. We provide theoretical foundations for this metric and its relationship with other prompt selection metrics, providing a comprehensive understanding of existing methods. Empirically, we show that combining prompt flatness with existing metrics improves both performance and sample efficiency. Our metric outperforms the previous prompt selection metrics with an average increase of 5% in accuracy and 10% in Pearson correlation across 6 classification benchmarks.
    
[^47]: ReGen: 通过渐进式密集检索生成训练数据的零样本文本分类方法

    ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval. (arXiv:2305.10703v1 [cs.CL])

    [http://arxiv.org/abs/2305.10703](http://arxiv.org/abs/2305.10703)

    本文提出了一种基于检索增强的框架，通过渐进式密集检索从通用领域的无标签语料库中创建训练数据，实现了零样本文本分类，相较于最强的基线模型提高了4.3%的性能，与使用大型NLG模型的基线相比节省了约70％的时间。

    

    随着大型语言模型（LLM）的发展，零样本学习在各种NLP任务中受到了许多关注。与以往使用数十亿级自然语言生成模型生成训练数据的方法不同，我们提出了一种检索增强的框架，从通用领域的无标签语料库中创建训练数据。为实现这一目标，我们首先进行对比预训练，使用类别描述性话语学习了一个无监督的密集检索器以提取最相关的文档。我们进一步提出了两种简单的策略，即展示增强的话语生成和自一致性引导过滤，以提高数据集的主题覆盖率，同时删除噪声样本。对九个数据集的实验表明，REGEN相较于最强的基线模型提高了4.3%的性能，并且与使用大型NLG模型的基线相比节省了约70％的时间。此外，REGEN可以自然地与最近提出的大型语言模型相结合。

    With the development of large language models (LLMs), zero-shot learning has attracted much attention for various NLP tasks. Different from prior works that generate training data with billion-scale natural language generation (NLG) models, we propose a retrieval-enhanced framework to create training data from a general-domain unlabeled corpus. To realize this, we first conduct contrastive pretraining to learn an unsupervised dense retriever for extracting the most relevant documents using class-descriptive verbalizers. We then further propose two simple strategies, namely Verbalizer Augmentation with Demonstrations and Self-consistency Guided Filtering to improve the topic coverage of the dataset while removing noisy examples. Experiments on nine datasets demonstrate that REGEN achieves 4.3% gain over the strongest baselines and saves around 70% of the time compared to baselines using large NLG models. Besides, REGEN can be naturally integrated with recently proposed large language mo
    
[^48]: MolXPT：使用文本包装分子进行生成性预训练

    MolXPT: Wrapping Molecules with Text for Generative Pre-training. (arXiv:2305.10688v1 [cs.CL])

    [http://arxiv.org/abs/2305.10688](http://arxiv.org/abs/2305.10688)

    MolXPT是一个文本包装的统一语言模型，使用SMILES作为输入，可以提高分子模型的性能表现，并且使得基于零shot的分子生成成为可能。

    

    生成式预训练变压器（GPT）已经在自然语言处理中取得了巨大成功，并且相关技术已经被应用到了分子建模中。考虑到文本是科学发现最重要的记录，本文提出了 MolXPT，一个在 SMILES 上预训练的统一语言模型，其中 SMILES 被文本包装。简单来说，我们检测每个序列中的分子名称，并将它们替换为相应的 SMILES。通过这种方式，SMILES 可以利用周围文本的信息，反之亦然。以上包装的序列，是由来自 PubMed 的文本序列和来自 PubChem 的 SMILES 序列组成的，它们都被输入到语言模型中进行预训练。实验结果表明，MolXPT 在 MoleculeNet 上的分子属性预测的强基准模型中表现更好，在文本-分子翻译中表现与最佳模型相当，而使用的参数不到其一半，并且能够通过文本提示零-shot生成分子。

    Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified language model of text and molecules pre-trained on SMILES (a sequence representation of molecules) wrapped by text. Briefly, we detect the molecule names in each sequence and replace them to the corresponding SMILES. In this way, the SMILES could leverage the information from surrounding text, and vice versa. The above wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem are all fed into a language model for pre-training. Experimental results demonstrate that MolXPT outperforms strong baselines of molecular property prediction on MoleculeNet, performs comparably to the best model in text-molecule translation while using less than half of its parameters, and enables zero
    
[^49]: RMSSinger：基于真实乐谱的歌声合成

    RMSSinger: Realistic-Music-Score based Singing Voice Synthesis. (arXiv:2305.10686v1 [cs.SD])

    [http://arxiv.org/abs/2305.10686](http://arxiv.org/abs/2305.10686)

    RMSSinger是基于真实乐谱的歌声合成，采用单词级建模避免了转录误差，方便且灵活。

    

    我们关注一个有挑战性的任务：基于真实乐谱的歌声合成（RMS-SVS）。RMS-SVS旨在生成高质量的歌唱声音，给定具有不同音符类型（优美音符、连接音、休止符等）的真实乐谱。尽管取得了重大进展，但最近的歌声合成（SVS）方法仅适用于细粒度音乐乐谱，这需要一个复杂的数据收集管道，并需要耗费时间的手动标注来使音乐音符与音素对齐。此外，手动注释破坏了音乐乐谱中音符持续时间的规律性，使得细粒度音乐乐谱不便于作曲。为了解决这些难题，我们提出了RMSSinger，它是第一个RMS-SVS方法，它以真实乐谱作为输入，消除了大部分繁琐的手动标注并避免了上述不便。需要注意的是，乐谱是基于单词而非音素的，我们在RMSSinger中引入了单词级建模，以避免音素级建模所引起的转录误差。实验证明了我们的RMSSinger可以生成与人类歌唱者非常相似的歌唱声音，并且使用真实乐谱作为输入具有方便性和灵活性。

    We are interested in a challenging task, Realistic-Music-Score based Singing Voice Synthesis (RMS-SVS). RMS-SVS aims to generate high-quality singing voices given realistic music scores with different note types (grace, slur, rest, etc.). Though significant progress has been achieved, recent singing voice synthesis (SVS) methods are limited to fine-grained music scores, which require a complicated data collection pipeline with time-consuming manual annotation to align music notes with phonemes. Furthermore, these manual annotation destroys the regularity of note durations in music scores, making fine-grained music scores inconvenient for composing. To tackle these challenges, we propose RMSSinger, the first RMS-SVS method, which takes realistic music scores as input, eliminating most of the tedious manual annotation and avoiding the aforementioned inconvenience. Note that music scores are based on words rather than phonemes, in RMSSinger, we introduce word-level modeling to avoid the t
    
[^50]: 基于非自回归端到端语音识别系统的准确可靠置信度估计

    Accurate and Reliable Confidence Estimation Based on Non-Autoregressive End-to-End Speech Recognition System. (arXiv:2305.10680v1 [cs.SD])

    [http://arxiv.org/abs/2305.10680](http://arxiv.org/abs/2305.10680)

    本文介绍了一种CIF-Aligned置信度估计模型，利用了非自回归E2E ASR模型-Paraformer的特性，生成符号同步的声学嵌入，实现了准确可靠的置信度估计。

    

    在ASR领域中，估计识别结果的置信度得分是一项经典任务，对于各种下游任务和训练策略都至关重要。过去的端到端(E2E)置信度估计模型(CEM)预测与输入转录文本长度相等的得分序列，导致在删除和插入错误发生时估计不可靠。本文提出了一种CIF对齐置信度估计模型(CA-CEM)，利用连续积分和火 CIF 机制来生成符号同步的声学嵌入，以解决上述的估计失败问题。我们在令牌级别上使用AUC和RMSE以及utterance级别上的一种提出度量ECE-U来衡量估计质量。CA-CEM在ECE-U上获得了24%和19%的相对降低率，并且在两个测试集中具有更好的AUC和RMSE。此外，我们进行了分析。

    Estimating confidence scores for recognition results is a classic task in ASR field and of vital importance for kinds of downstream tasks and training strategies. Previous end-to-end~(E2E) based confidence estimation models (CEM) predict score sequences of equal length with input transcriptions, leading to unreliable estimation when deletion and insertion errors occur. In this paper we proposed CIF-Aligned confidence estimation model (CA-CEM) to achieve accurate and reliable confidence estimation based on novel non-autoregressive E2E ASR model - Paraformer. CA-CEM utilizes the modeling character of continuous integrate-and-fire (CIF) mechanism to generate token-synchronous acoustic embedding, which solves the estimation failure issue above. We measure the quality of estimation with AUC and RMSE in token level and ECE-U - a proposed metrics in utterance level. CA-CEM gains 24% and 19% relative reduction on ECE-U and also better AUC and RMSE on two test sets. Furthermore, we conduct anal
    
[^51]: 超越编码：头脑风暴提升大型语言模型在代码生成中的应用

    Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation. (arXiv:2305.10679v1 [cs.AI])

    [http://arxiv.org/abs/2305.10679](http://arxiv.org/abs/2305.10679)

    本文介绍了一个名为Brainstorm的框架，利用头脑风暴步骤生成并选择关于问题的不同想法，可显著增强大型语言模型（LLMs）解决竞争级别编程问题的能力，结果在CodeContests基准测试中，ChatGPT的pass@$k$指标增加了50％以上。

    

    代码生成旨在从高级任务规范自动生成源代码，可显着提高软件工程的生产力。最近，基于大型语言模型（LLMs）的方法在简单任务的代码生成能力方面表现出色。然而，生成更复杂任务的代码（如竞争级别的问题）仍然很具有挑战性。在本文中，我们介绍了Brainstorm框架用于代码生成。它利用了一个头脑风暴步骤，生成并选择关于问题的不同想法以促进算法推理，其中这些思考是解决问题的可能蓝图。我们证明Brainstorm显著增强了LLMs解决竞争级别编程问题的能力，在CodeContests基准测试中，ChatGPT的pass@$k$指标增加了50％以上，实现了最先进的性能。此外，我们在LeetCode竞赛中进行的实验表明，o

    Code generation aims to automatically generate source code from high-level task specifications, which can significantly increase productivity of software engineering. Recently, approaches based on large language models (LLMs) have shown remarkable code generation abilities on simple tasks. However, generate code for more complex tasks, such as competition-level problems, remains challenging. In this paper, we introduce Brainstorm framework for code generation. It leverages a brainstorming step that generates and selects diverse thoughts on the problem to facilitate algorithmic reasoning, where the thoughts are possible blueprint of solving the problem. We demonstrate that Brainstorm significantly enhances the ability of LLMs to solve competition-level programming problems, resulting in a more than 50% increase in the pass@$k$ metrics for ChatGPT on the CodeContests benchmark, achieving state-of-the-art performance. Furthermore, our experiments conducted on LeetCode contests show that o
    
[^52]: 一个统一的英文文本到语音合成前端框架

    a unified front-end framework for english text-to-speech synthesis. (arXiv:2305.10666v1 [cs.CL])

    [http://arxiv.org/abs/2305.10666](http://arxiv.org/abs/2305.10666)

    该论文提出了一个统一的前端框架，捕捉了英文语音合成前端模块之间的依赖关系，并且在所有模块中均取得了最先进的性能。

    

    前端是英文文本到语音合成系统的关键组成部分，负责提取语言特征，如韵律和音素，这对于文本到语音模型合成语音至关重要。英文文本到语音前端通常由文本规范化模块（TN），单词韵律短语韵律短语模块（PWPP）和字形到音素模块（G2P）组成。然而，当前英文文本到语音前端的研究仅关注于单独模块，忽略它们之间的相互依赖，导致每个模块性能下降。因此，本文提出了一个统一的前端框架，捕捉英文文本到语音前端模块之间的依赖关系。广泛的实验表明，所提出的方法在所有模块中实现了最先进的性能。

    The front-end is a critical component of English text-to-speech (TTS) systems, responsible for extracting linguistic features that are essential for a text-to-speech model to synthesize speech, such as prosodies and phonemes. The English TTS front-end typically consists of a text normalization (TN) module, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme (G2P) module. However, current research on the English TTS front-end focuses solely on individual modules, neglecting the interdependence between them and resulting in sub-optimal performance for each module. Therefore, this paper proposes a unified front-end framework that captures the dependencies among the English TTS front-end modules. Extensive experiments have demonstrated that the proposed method achieves state-of-the-art (SOTA) performance in all modules.
    
[^53]: 基于对比学习和深度模块化的语音分离

    Speech Separation based on Contrastive Learning and Deep Modularization. (arXiv:2305.10652v1 [cs.SD])

    [http://arxiv.org/abs/2305.10652](http://arxiv.org/abs/2305.10652)

    本文提出了一种基于对比学习和深度模块化的完全无监督语音分离方法，解决了有监督学习中存在的排列问题、说话人数量不匹配的问题和高质量标记数据的依赖问题。

    

    目前，语音分离的最先进工具依赖于有监督学习。这意味着它们必须处理排列问题，它们受到训练和推断中使用的说话者数量不匹配的影响。此外，它们的性能严重依赖于高质量标记数据的存在。这些问题可以通过采用完全无监督的语音分离技术有效地解决。在本文中，我们使用对比学习建立帧的表示，然后在下游的深度模块化任务中使用学习到的表示。具体而言，在语音分离中，说话人的不同帧可以被看作是给定那个说话人的隐含标准帧的增强版。说话人的帧包含足够的韵律信息重叠，这是语音分离的关键。基于此，我们实现了自监督学习，学习缩小帧之间的距离。

    The current monaural state of the art tools for speech separation relies on supervised learning. This means that they must deal with permutation problem, they are impacted by the mismatch on the number of speakers used in training and inference. Moreover, their performance heavily relies on the presence of high-quality labelled data. These problems can be effectively addressed by employing a fully unsupervised technique for speech separation. In this paper, we use contrastive learning to establish the representations of frames then use the learned representations in the downstream deep modularization task. Concretely, we demonstrate experimentally that in speech separation, different frames of a speaker can be viewed as augmentations of a given hidden standard frame of that speaker. The frames of a speaker contain enough prosodic information overlap which is key in speech separation. Based on this, we implement a self-supervised learning to learn to minimize the distance between frames
    
[^54]: ZeroPrompt: 流式语音编码器是零-shot遮蔽语言模型

    ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs. (arXiv:2305.10649v1 [cs.SD])

    [http://arxiv.org/abs/2305.10649](http://arxiv.org/abs/2305.10649)

    本文提出了ZeroPrompt和对应的Prompt-and-Refine策略，可以降低流式语音识别模型的TDT，而且不会损失精度，具有工程便捷性，能够在任何数据集上应用。

    

    本文提出了ZeroPrompt和对应的Prompt-and-Refine策略，这是两种简单而有效的无需训练的方法，能够降低流式语音识别模型的Token Display Time（TDT），而且不会造成任何精度损失。ZeroPrompt的核心思想是在推理过程中向每个语音块附加零内容，这类似于提示，鼓励模型在未来标记之前预测它们。我们认为，流式语音编码器自然具有Masked Language Model的建模能力，我们的实验表明，ZeroPrompt具有廉价的工程成本，可以应用于任何数据集上的流式语音编码器，而不会损失精度。具体来说，与我们的基准模型相比，在Aishell-1和Librispeech数据集上，我们实现了首个标记显示时间（TDT-F）的350~700ms的降低以及最后一个标记显示时间（TDT-L）的100~400ms的降低，理论上和实验上的WER精度相等。

    In this paper, we present ZeroPrompt (Figure 1-(a)) and the corresponding Prompt-and-Refine strategy (Figure 3), two simple but effective \textbf{training-free} methods to decrease the Token Display Time (TDT) of streaming ASR models \textbf{without any accuracy loss}. The core idea of ZeroPrompt is to append zeroed content to each chunk during inference, which acts like a prompt to encourage the model to predict future tokens even before they were spoken. We argue that streaming acoustic encoders naturally have the modeling ability of Masked Language Models and our experiments demonstrate that ZeroPrompt is engineering cheap and can be applied to streaming acoustic encoders on any dataset without any accuracy loss. Specifically, compared with our baseline models, we achieve 350 $\sim$ 700ms reduction on First Token Display Time (TDT-F) and 100 $\sim$ 400ms reduction on Last Token Display Time (TDT-L), with theoretically and experimentally equal WER on both Aishell-1 and Librispeech da
    
[^55]: BioAug：基于条件生成的数据增强方法用于低资源生物医学命名实体识别

    BioAug: Conditional Generation based Data Augmentation for Low-Resource Biomedical NER. (arXiv:2305.10647v1 [cs.CL])

    [http://arxiv.org/abs/2305.10647](http://arxiv.org/abs/2305.10647)

    本文提出了一种基于条件生成的数据增强框架BioAug，用于低资源生物医学命名实体识别。BioAug建立在BART上，通过选择性的屏蔽和知识增强进行训练。实验展示了BioAug在5个基准BioNER数据集上的有效性，且表现优于所有基线。

    

    生物医学命名实体识别(BioNER)是从生物医学文本中识别命名实体的基本任务。由于注释需要高度专业化和专业知识，BioNER 遭受着严重的数据稀缺和缺乏高质量标记数据的困扰。尽管数据增强在低资源命名实体识别方面已经被证明是高效的，但现有的数据增强技术不能为BioNER生成真实且多样化的增强。本文提出了一种新的数据增强框架BioAug，用于低资源BioNER。BioAug建立在BART上，通过选择性的屏蔽和知识增强进行训练，从而解决了一种新的文本重构任务。在训练后，我们进行有条件的生成并在与训练阶段类似的有选择性地损坏文本的条件下生成多样化的增强。我们在5个基准BioNER数据集上展示了BioAug的有效性，并表明BioAug比所有基线都表现更好。

    Biomedical Named Entity Recognition (BioNER) is the fundamental task of identifying named entities from biomedical text. However, BioNER suffers from severe data scarcity and lacks high-quality labeled data due to the highly specialized and expert knowledge required for annotation. Though data augmentation has shown to be highly effective for low-resource NER in general, existing data augmentation techniques fail to produce factual and diverse augmentations for BioNER. In this paper, we present BioAug, a novel data augmentation framework for low-resource BioNER. BioAug, built on BART, is trained to solve a novel text reconstruction task based on selective masking and knowledge augmentation. Post training, we perform conditional generation and generate diverse augmentations conditioning BioAug on selectively corrupted text similar to the training stage. We demonstrate the effectiveness of BioAug on 5 benchmark BioNER datasets and show that BioAug outperforms all our baselines by a signi
    
[^56]: 大型语言模型适合指导阅读吗？

    Are Large Language Models Fit For Guided Reading?. (arXiv:2305.10645v1 [cs.CL])

    [http://arxiv.org/abs/2305.10645](http://arxiv.org/abs/2305.10645)

    本文评估大型语言模型在指导阅读中的应用能力，发现它们能够生成高质量的有意义问题，具有多样性且涵盖输入文本中大多数主题，同时能够有效地总结回答和推荐重新阅读的部分。

    

    本文研究了大型语言模型在教育指导阅读中的应用能力。我们具体评估了它们从输入文本中生成有意义问题的能力，生成内容涵盖和问题难度多样化的问题的能力，并评估它们根据学生对问题的回答推荐应该重新阅读的文本部分的能力。在对ChatGPT和Bard的评估中，我们报告如下结果：1）大型语言模型能够生成与输入文本高相关的高质量有意义的问题，2）它们能够生成涵盖输入文本中大多数主题的多样化问题，尽管随着输入文本的增加，这种能力显著降低，3）大型语言模型能够生成低和高认知难度的问题，尽管它们显著偏向于低认知难度的问题，4）它们能够有效地总结回答并提取应该重新阅读的部分。

    This paper looks at the ability of large language models to participate in educational guided reading. We specifically, evaluate their ability to generate meaningful questions from the input text, generate diverse questions both in terms of content coverage and difficulty of the questions and evaluate their ability to recommend part of the text that a student should re-read based on the student's responses to the questions. Based on our evaluation of ChatGPT and Bard, we report that,  1) Large language models are able to generate high quality meaningful questions that have high correlation with the input text, 2) They generate diverse question that cover most topics in the input text even though this ability is significantly degraded as the input text increases, 3)The large language models are able to generate both low and high cognitive questions even though they are significantly biased toward low cognitive question, 4) They are able to effectively summarize responses and extract a p
    
[^57]: 语言模型遇见世界模型：实体经验增强语言模型

    Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v1 [cs.CL])

    [http://arxiv.org/abs/2305.10626](http://arxiv.org/abs/2305.10626)

    本文提出一种新的增强语言模型的方法——将其与世界模型相结合，通过有目的的规划和随机探索获得丰富的实体经验进行微调, 以提高其在物理环境下的推理和行为能力，并在语言基准上保持或提高性能。

    

    虽然大型语言模型 (LMs) 在许多任务上表现出了非凡的能力，但它们常常在处理物理环境下的简单推理和规划问题时遇到困难，例如理解物体永恒或规划家庭活动。这种限制源于 LM 仅受书面语言训练，缺少必要的实体知识和技能。本文提出了一种新的增强 LM 的方法，即将其与世界模型相结合进行微调，以获得多样化的实体知识，同时保持其一般语言能力。本方法在世界模型中部署一个融入实体经验的代理，特别是一个模拟物理世界的仿真器(VirtualHome)，通过有目的的规划和随机探索获得多样化的实体经验。然后，利用这些经验微调 LM ，以教授在物理世界中的各种推理和行为能力，例如规划和完成目标、物体永恒和跟踪等。此外，我们相信我们的方法可以轻松扩展以利用其他模拟器，包括机器人或自动驾驶汽车。我们证明了我们的方法显着提高了 LM 在一系列物理推理任务中的性能，同时保留并经常提高了它们在语言基准上的表现。

    While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is
    
[^58]: ML-SUPERB: 多语种语音自我监督学习性能基准

    ML-SUPERB: Multilingual Speech Universal PERformance Benchmark. (arXiv:2305.10615v1 [cs.SD])

    [http://arxiv.org/abs/2305.10615](http://arxiv.org/abs/2305.10615)

    本文提出了一个覆盖143种语言、用于自我监督学习模型性能基准的多语种语音基准 ML-SUPERB，并发现自我监督学习模型可以显著提高性能且多语种模型不总是比单语言模型表现更好。

    

    语音处理Universal PERformance Benchmark (SUPERB)是一个用于各种语音处理任务的自我监督学习模型性能基准的排行榜。然而，SUPERB在评估中主要考虑英语。本文介绍了多语种SUPERB (ML-SUPERB)，覆盖了143种语言（从高资源到濒危语言），考虑了自动语音识别和语言识别。与SUPERB概念类似，ML-SUPERB利用冻结的自我监督学习特征，并通过学习浅层下游模型的简单框架，用于多语种任务。与SUPERB基准类似，我们发现语音自我监督学习模型可以显著提高性能，与FBANK特征相比。此外，我们发现多语种模型并不总是比单语言模型表现更好。我们将发布ML-SUPERB作为一个挑战，提供组织好的数据集和可重现的训练脚本，用于未来的多语种表示研究。

    Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.
    
[^59]: 不依靠先验知识的时态知识图谱预测

    Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning. (arXiv:2305.10613v1 [cs.CL])

    [http://arxiv.org/abs/2305.10613](http://arxiv.org/abs/2305.10613)

    本文旨在探究是否能够使用大型语言模型进行时态知识图谱预测，尤其是不需要任何显式模块。结果表明，大型语言模型在此类预测中表现良好，并且可以隐式有效地编码上下文和时间信息。

    

    时间知识图谱（TKG）预测是一个挑战模型使用过去的知识来预测未来事实的基准测试。在本文中，我们使用上下文学习（ICL）将大型语言模型（LLM）应用于这些基准测试。我们探究了LLMs在TKG预测中可以在多大程度上使用，特别是没有任何微调或捕捉结构和时间信息的显式模块。为了进行实验，我们提出了一个框架，将相关历史事实转换为提示并使用令牌概率生成排名预测。令人惊讶的是，我们观察到LLMs的性能与为TKG预测精心设计和训练的最先进TKG模型相当。我们的广泛评估展示了多个具有不同特性的模型和数据集的性能，比较了准备上下文信息的替代启发式方法，并与著名的TKG方法和简单的频率和最近性基线进行对比。我们还检查了生成的提示，并展示了它们与所预测的事实的相关性。我们的结果表明，LLMs确实可以隐式有效地编码上下文和时间信息，进行TKG预测，而无需显式知识或领域特定模块。

    Temporal knowledge graph (TKG) forecasting benchmarks challenge models to predict future facts using knowledge of past facts. In this paper, we apply large language models (LLMs) to these benchmarks using in-context learning (ICL). We investigate whether and to what extent LLMs can be used for TKG forecasting, especially without any fine-tuning or explicit modules for capturing structural and temporal information. For our experiments, we present a framework that converts relevant historical facts into prompts and generates ranked predictions using token probabilities. Surprisingly, we observe that LLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefully designed and trained for TKG forecasting. Our extensive evaluation presents performances across several models and datasets with different characteristics, compares alternative heuristics for preparing contextual information, and contrasts to prominent TKG methods and simple frequency and recency baselines. We als
    
[^60]: 利用L2范数折扣解决高频词余弦相似度低估问题

    Solving Cosine Similarity Underestimation between High Frequency Words by L2 Norm Discounting. (arXiv:2305.10610v1 [cs.CL])

    [http://arxiv.org/abs/2305.10610](http://arxiv.org/abs/2305.10610)

    通过L2范数折扣来解决高频词的余弦相似度低估问题

    

    通过使用来自掩码语言模型（MLM）如BERT的上下文化标记嵌入来计算两个单词之间的余弦相似性，已经证明会低估这些单词之间的实际相似性。高频词的相似度低估问题尤其严重。虽然这个问题已经在先前的工作中被注意到，但目前尚未提出解决方案。我们观察到一个单词的上下文化嵌入的L2范数与其在预训练语料库中的对数频率相关。因此，与高频词相关的更大的L2范数降低了它们之间的余弦相似度值，因此低估了它们之间的相似度分数。为解决这个问题，我们提出了一种方法，在计算词之间的余弦相似度时，通过该词在语料库中的频率折扣来计算上下文化单词嵌入的L2范数。我们展示了所谓的停用词表现不同于其他单词。

    Cosine similarity between two words, computed using their contextualised token embeddings obtained from masked language models (MLMs) such as BERT has shown to underestimate the actual similarity between those words (Zhou et al., 2022). This similarity underestimation problem is particularly severe for highly frequent words. Although this problem has been noted in prior work, no solution has been proposed thus far. We observe that the L2 norm of contextualised embeddings of a word correlates with its log-frequency in the pretraining corpus. Consequently, the larger L2 norms associated with the highly frequent words reduce the cosine similarity values measured between them, thus underestimating the similarity scores. To solve this issue, we propose a method to discount the L2 norm of a contextualised word embedding by the frequency of that word in a corpus when measuring the cosine similarities between words. We show that the so called stop words behave differently from the rest of the 
    
[^61]: Tree of Thoughts: 利用大语言模型进行深思熟虑的问题解决

    Tree of Thoughts: Deliberate Problem Solving with Large Language Models. (arXiv:2305.10601v1 [cs.CL])

    [http://arxiv.org/abs/2305.10601](http://arxiv.org/abs/2305.10601)

    本研究提出了一种新的推理框架——思维之树（ToT），可以增强语言模型的问题解决能力，帮助语言模型进行深思熟虑的决策，以及自我评估和全局选择。

    

    语言模型越来越广泛地用于解决各种任务的通用问题，但在推理过程中仍然受限于基于标记、从左到右的决策过程。这意味着在需要探索、战略前瞻或初始决策发挥关键作用的任务中，他们可能会遇到困难。为了克服这些挑战，我们引入了一种新的语言模型推理框架——思维之树（ToT），它将通常用于提示语言模型的思维链方法泛化，并使用一致的文本单位（思维）进行探究，这些思维作为解决问题的中间步骤。思维之树允许语言模型通过考虑多个不同的推理路径和自我评估来进行深思熟虑的决策，并决定下一步的行动，同时在必要时向前或向后跟踪以进行全局选择。我们的实验表明，ToT显著增强了语言模型的解决问题能力。

    Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abil
    
[^62]: 一种更好的掩码语言模型评分方法

    A Better Way to Do Masked Language Model Scoring. (arXiv:2305.10588v1 [cs.CL])

    [http://arxiv.org/abs/2305.10588](http://arxiv.org/abs/2305.10588)

    本文提出了一种更好的掩码语言模型评分方法，即PLL-word-l2r，用于估计句子的伪对数似然得分，相对于原PLL方法和屏蔽所有单词标记的PLL评分方法，改进的度量方法更好地针对字汇外单词得分问题进行了解决。

    

    估计自回归语言模型下给定句子的对数似然很简单：可以直接应用链式法则并对每个连续标记的对数似然值求和。但对于掩码语言模型，没有直接的方法来估计一个句子的对数似然。为了解决这个问题，Salazar等人（2020）提出了估计句子伪对数似然（PLL）分数的方法，该方法通过依次屏蔽每个句子标记，使用其余的句子作为上下文检索其得分，并总结结果值。本文针对原PLL方法中字汇外单词产生的得分夸大的问题提出了一种改进的度量方法，其中我们不仅屏蔽目标标记，而且还屏蔽目标标记右侧所有的标记。我们展示了我们改进的度量方法（PLL-word-l2r）优于原始PLL度量方法和一个屏蔽所有单词标记的PLL合成方式。特别地，它更好地符合理论。

    Estimating the log-likelihood of a given sentence under an autoregressive language model is straightforward: one can simply apply the chain rule and sum the log-likelihood values for each successive token. However, for masked language models, there is no direct way to estimate the log-likelihood of a sentence. To address this issue, Salazar et al. (2020) propose to estimate sentence pseudo-log-likelihood (PLL) scores, computed by successively masking each sentence token, retrieving its score using the rest of the sentence as context, and summing the resulting values. Here, we demonstrate that the original PLL method yields inflated scores for out-of-vocabulary words and propose an adapted metric, in which we mask not only the target token, but also all within-word tokens to the right of the target. We show that our adapted metric (PLL-word-l2r) outperforms both the original PLL metric and a PLL metric in which all within-word tokens are masked. In particular, it better satisfies theore
    
[^63]: 从巧克力兔到巧克力鳄鱼：语言模型是否理解名词复合词？

    From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?. (arXiv:2305.10568v1 [cs.CL])

    [http://arxiv.org/abs/2305.10568](http://arxiv.org/abs/2305.10568)

    本文研究了语言模型对于名词复合词的理解能力，提出了名词复合词解释的任务和名词复合词概念化的任务，并发现GPT-3在这些任务中的表现优于人类。

    

    名词复合词是指将多个名词组合成一个新词，如“巧克力兔”。本文提出了一种名词复合词解释的任务并修改了该任务的数据和评估设置。结果表明，GPT-3（一种语言模型）几乎可以完美地解决该任务。同时，本文还探讨了名词复合词概念化的任务，即解释新颖或罕见的名词组合词。最后，我们评估了GPT-3推理世界的程度，并发现它的表现优于人类。

    Noun compound interpretation is the task of expressing a noun compound (e.g. chocolate bunny) in a free-text paraphrase that makes the relationship between the constituent nouns explicit (e.g. bunny-shaped chocolate). We propose modifications to the data and evaluation setup of the standard task (Hendrickx et al., 2013), and show that GPT-3 solves it almost perfectly. We then investigate the task of noun compound conceptualization, i.e. paraphrasing a novel or rare noun compound. E.g., chocolate crocodile is a crocodile-shaped chocolate. This task requires creativity, commonsense, and the ability to generalize knowledge about similar concepts. While GPT-3's performance is not perfect, it is better than that of humans -- likely thanks to its access to vast amounts of knowledge, and because conceptual processing is effortful for people (Connell and Lynott, 2012). Finally, we estimate the extent to which GPT-3 is reasoning about the world vs. parroting its training data. We find that the 
    
[^64]: 探究硬负采样分布对对比知识图谱嵌入的影响

    Investigating the Effect of Hard Negative Sample Distribution on Contrastive Knowledge Graph Embedding. (arXiv:2305.10563v1 [cs.AI])

    [http://arxiv.org/abs/2305.10563](http://arxiv.org/abs/2305.10563)

    本文探究了负采样分布对对比知识图谱嵌入的影响，提出考虑硬度和结构的对比（HaSa）算法，用于去除假负样本，提高知识图谱补全任务的性能。

    

    知识图谱补全任务的成功很大程度上依赖于知识图谱嵌入（KGEs）的质量，它依赖于自监督学习和用负三元组增强数据集。在负采样的对比损失的理论分析和高质量（即硬）负采样的启发式生成之间存在差距。在本文中，我们修改了InfoNCE损失，显式考虑了负采样分布。我们展示了用硬负样本最小化InfoNCE损失可以最大化给定三元组和负三元组之间的KL散度。然而，我们也证明硬负样本会导致假负样本（即错误的事实三元组）并降低下游任务性能。为了解决这个问题，我们提出了一种通过知识图谱的图结构去除假负三元组的新型负采样分布。我们将我们的算法称为考虑硬度和结构的对比（HaSa）算法。

    The success of the knowledge graph completion task heavily depends on the quality of the knowledge graph embeddings (KGEs), which relies on self-supervised learning and augmenting the dataset with negative triples. There is a gap in literature between the theoretical analysis of negative samples on contrastive loss and heuristic generation of quality (i.e., hard) negative triples. In this paper, we modify the InfoNCE loss to explicitly account for the negative sample distribution. We show minimizing InfoNCE loss with hard negatives maximizes the KL-divergence between the given and negative triple embedding. However, we also show that hard negatives can lead to false negatives (i.e., accidentally factual triples) and reduce downstream task performance. To address this issue, we propose a novel negative sample distribution that uses the graph structure of the knowledge graph to remove the false negative triples. We call our algorithm Hardness and Structure-aware (\textbf{HaSa}) contrasti
    
[^65]: 大规模多语言事件理解：抽取、可视化和检索

    Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search. (arXiv:2305.10561v1 [cs.CL])

    [http://arxiv.org/abs/2305.10561](http://arxiv.org/abs/2305.10561)

    本文介绍了一种跨语言的零样本事件抽取系统和用户界面，通过仅使用英语训练数据，能够在100种不同语言的文本中进行全球事件的抽取、可视化和搜索。同时，该系统还能够进行跨语言的以事件为中心的搜索。

    

    本文介绍 ISI-Clear，一种先进的跨语言、零样本事件抽取系统，以及用于事件可视化和检索的用户界面。ISI-Clear仅使用英语训练数据，就能处理来自100种语言（从南非荷兰语到意第绪语）的用户提供的文本，使全球事件随需而变。我们提供了多个以事件为中心的抽取视图，包括图形表示和文档级摘要。我们还将现有的跨语言检索算法与事件抽取功能集成，提供跨语言的以事件为中心的搜索，使英语用户能够使用英语自然语言查询（例如伊朗霍乱爆发）或结构化查询（例如查找所有类型为“疾病暴发”的事件，其代理为霍乱和所在地为伊朗）自动搜索从非英语文档语料库中自动抽取的事件。

    In this paper, we present ISI-Clear, a state-of-the-art, cross-lingual, zero-shot event extraction system and accompanying user interface for event visualization & search. Using only English training data, ISI-Clear makes global events available on-demand, processing user-supplied text in 100 languages ranging from Afrikaans to Yiddish. We provide multiple event-centric views of extracted events, including both a graphical representation and a document-level summary. We also integrate existing cross-lingual search algorithms with event extraction capabilities to provide cross-lingual event-centric search, allowing English-speaking users to search over events automatically extracted from a corpus of non-English documents, using either English natural language queries (e.g. cholera outbreaks in Iran) or structured queries (e.g. find all events of type Disease-Outbreak with agent cholera and location Iran).
    
[^66]: "自动后编辑高质量机器翻译中的句法对称性"

    Bring More Attention to Syntactic Symmetry for Automatic Postediting of High-Quality Machine Translations. (arXiv:2305.10557v1 [cs.CL])

    [http://arxiv.org/abs/2305.10557](http://arxiv.org/abs/2305.10557)

    这篇论文提出了一种新的方法来解决自动后编辑系统无法处理高质量机器翻译的问题，该方法通过对给定MT进行对称自我关注的损失函数进行正则化，从而提高了自动后编辑的质量。

    

    自动后编辑（APE）是用于改进给定机器翻译（MT）的自动化过程。最近的研究发现，即使是在有丰富数据资源的语言对（英语-德语），现有的APE系统也不擅长处理高质量的MT：给定的MT质量越高，决定哪些部分需要编辑以及如何修复这些错误就越困难。解决这个问题的一个可能的方法是将更深入的目标语言知识注入模型中。因此，我们提出了一种具有语言学动机的正则化方法，该方法可增强APE模型对目标语言的理解：通过一个鼓励对给定MT进行对称自我关注的损失函数。我们对实验结果的分析表明，所提出的方法有助于提高高质量MT的当前最先进架构的APE质量。

    Automatic postediting (APE) is an automated process to refine a given machine translation (MT). Recent findings present that existing APE systems are not good at handling high-quality MTs even for a language pair with abundant data resources, English$\unicode{x2013}$German: the better the given MT is, the harder it is to decide what parts to edit and how to fix these errors. One possible solution to this problem is to instill deeper knowledge about the target language into the model. Thus, we propose a linguistically motivated method of regularization that is expected to enhance APE models' understanding of the target language: a loss function that encourages symmetric self-attention on the given MT. Our analysis of experimental results demonstrates that the proposed method helps improving the state-of-the-art architecture's APE quality for high-quality MTs.
    
[^67]: 自学习对话系统中缺陷行为的可扩展和安全修复

    Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems. (arXiv:2305.10528v1 [cs.AI])

    [http://arxiv.org/abs/2305.10528](http://arxiv.org/abs/2305.10528)

    本文提出了一种在自学习对话系统中利用历史回归事件报告来验证、保护和改进政策的方法，以解决在大规模商业环境中的经验连贯性和政策改进之间的平衡问题。

    

    强化学习已成为最新自然语言对话人工智能的驱动力，改善了目标为导向的代理人与人之间更自然的互动，提高了用户的满意度，但在大型商业环境中，平衡政策改进和经验连贯性经常具有挑战性。本文提出了一种方法，即使用历史回归事件报告中的高精度样本对政策进行验证、安全保护和改进，以便在在线部署前进行修正。作者对真实的对话系统和实际的回归事件数据进行了大量实验，并将所提出的方法应用于他们的生产系统中。

    Off-Policy reinforcement learning has been a driving force for the state-of-the-art conversational AIs leading to more natural humanagent interactions and improving the user satisfaction for goal-oriented agents. However, in large-scale commercial settings, it is often challenging to balance between policy improvements and experience continuity on the broad spectrum of applications handled by such system. In the literature, off-policy evaluation and guard-railing on aggregate statistics has been commonly used to address this problem. In this paper, we propose a method for curating and leveraging high-precision samples sourced from historical regression incident reports to validate, safe-guard, and improve policies prior to the online deployment. We conducted extensive experiments using data from a real-world conversational system and actual regression incidents. The proposed method is currently deployed in our production system to protect customers against broken experiences and enable
    
[^68]: 生成式语言模型的统计知识评估

    Statistical Knowledge Assessment for Generative Language Models. (arXiv:2305.10519v1 [cs.CL])

    [http://arxiv.org/abs/2305.10519](http://arxiv.org/abs/2305.10519)

    本文介绍了一个统计知识评估框架，用于评估生成式语言模型（GLMs）的知识输出。通过对14种GLMs进行综合比较，发现知识遵循缩放定律，而对指令遵循数据进行微调可能会损害模型的生成能力。

    

    生成式语言模型（GLMs）展示了存储事实知识和高效回答查询的能力。但是，给定不同的提示，GLM是否始终生成事实正确的答案？本文介绍了一个由潜变量和KaRR度量指导的统计知识评估框架，该度量通过计算模型在各种文本形式上的连续概率量化其知识。我们使用我们的框架对14种GLM的知识进行了全面比较，包括LLaMA、Alpaca、OPT和其他模型。我们的统计知识评估涵盖了600种关系类型，并显示出与人类评估的强相关性（0.43 Kendall's $\tau$）。我们的发现揭示了具有相同支架结构的GLM的知识遵循缩放定律，并且在指令遵循数据上进行的微调可能会损害模型持续生成事实正确的文本的能力。

    Generative Language Models (GLMs) have demonstrated capabilities to store factual knowledge and answer queries efficiently. Given varying prompts, does a GLM consistently generate factually correct answers? In this paper, we introduce a statistical knowledge assessment framework guided by latent variables and the KaRR metric, which quantifies a model's knowledge by computing its continuous probability across diverse text forms. We conduct a comprehensive comparison of knowledge across 14 GLMs using our framework, including LLaMA, Alpaca, OPT, and others. Our statistical knowledge assessment encompasses 600 relation types and exhibits a strong correlation (0.43 Kendall's $\tau$) with human evaluation. Our findings reveal that the knowledge in GLMs with the same backbone architecture adheres to the scaling law, and that tuning on instruction-following data may compromise the model's ability to generate factually correct text consistently.
    
[^69]: IMAD: 图像增强的多模式对话

    IMAD: IMage-Augmented multi-modal Dialogue. (arXiv:2305.10512v1 [cs.CL])

    [http://arxiv.org/abs/2305.10512](http://arxiv.org/abs/2305.10512)

    该论文提出了一种新颖的多模态对话系统视角，其中图像解释是基于对话上下文的，同时提出了一个两阶段观点来构建多模态对话数据集。

    

    目前，对话系统已经在处理基于文本的通讯方面取得了高性能。然而，它们还没有有效地融合视觉信息，这是一个重要的挑战。此外，现有的在对话生成中融合图像的模型专注于讨论图像本身。我们提出的方法提供了一种新颖的多模态对话系统视角，解释了对话中图像的上下文。通过这样做，我们旨在扩展当前对话系统的能力，从单一模式（文本）向多模态转换。然而，缺乏包含图像和对话上下文的经过验证的英文数据集是这项任务的难点。因此，我们提出了一个两阶段的方法来自动构建多模态对话数据集。在第一阶段，我们利用文本到图像的相似性和句子相似性来识别哪些话语可以用图像替换。在第二阶段，我们替换那些话语。

    Currently, dialogue systems have achieved high performance in processing text-based communication. However, they have not yet effectively incorporated visual information, which poses a significant challenge. Furthermore, existing models that incorporate images in dialogue generation focus on discussing the image itself. Our proposed approach presents a novel perspective on multi-modal dialogue systems, which interprets the image in the context of the dialogue. By doing so, we aim to expand the capabilities of current dialogue systems and transition them from single modality (text) to multi-modality. However, there is a lack of validated English datasets that contain both images and dialogue contexts for this task. Thus, we propose a two-stage approach to automatically construct a multi-modal dialogue dataset. In the first stage, we utilize text-to-image similarity and sentence similarity to identify which utterances could be replaced with an image. In the second stage, we replace those
    
[^70]: ChatGPT在六种低资源语言中延续性别偏见，忽略非性别代词

    ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource Languages. (arXiv:2305.10510v1 [cs.CY])

    [http://arxiv.org/abs/2305.10510](http://arxiv.org/abs/2305.10510)

    ChatGPT在翻译中延续性别偏见，忽略非性别代词，会将性别中立代词转换为男性或女性，甚至无法将英语中的性别中立代词翻译为其他语言的性别中立代词。

    

    在这个多元文化时代，语言翻译是最常见的任务之一，越来越多地由人工智能进行调节和自动化。作为一种新型的人工智能系统，ChatGPT声称在这样的翻译任务中表现出色，在本文中，我们对这一声明进行了测试。具体而言，我们考察了ChatGPT在英语和仅使用性别中立代词的语言之间进行翻译的准确性。我们以孟加拉语为中心进行了这项研究，孟加拉语是全球第七大使用语言，但我们还概括了我们在波斯语、马来语、塔加洛语、泰语和土耳其语等五种其他语言中的发现。我们发现，ChatGPT延续了对某些职业（例如男人=医生，女人=护士）或行动（女人=烹饪，男人=去工作）赋予性别默认值和刻板印象的偏见，因为它将语言中的性别中立代词转换为“他”或“她”。我们还观察到，ChatGPT完全无法将英语的性别中立代词“they”翻译为其他语言中相应的性别中立代词。

    In this multicultural age, language translation is one of the most performed tasks, and it is becoming increasingly AI-moderated and automated. As a novel AI system, ChatGPT claims to be proficient in such translation tasks and in this paper, we put that claim to the test. Specifically, we examine ChatGPT's accuracy in translating between English and languages that exclusively use gender-neutral pronouns. We center this study around Bengali, the 7$^{th}$ most spoken language globally, but also generalize our findings across five other languages: Farsi, Malay, Tagalog, Thai, and Turkish. We find that ChatGPT perpetuates gender defaults and stereotypes assigned to certain occupations (e.g. man = doctor, woman = nurse) or actions (e.g. woman = cook, man = go to work), as it converts gender-neutral pronouns in languages to `he' or `she'. We also observe ChatGPT completely failing to translate the English gender-neutral pronoun `they' into equivalent gender-neutral pronouns in other languag
    
[^71]: 融合归因重要性以提高忠实度评估的方法

    Incorporating Attribution Importance for Improving Faithfulness Metrics. (arXiv:2305.10496v1 [cs.CL])

    [http://arxiv.org/abs/2305.10496](http://arxiv.org/abs/2305.10496)

    本研究提出了一种软删除标准来评估归因方法的忠实度，该方法随机遮盖标记的部分向量表示，这种方法比现有的硬删除标准更准确。

    

    特征归因方法是提供对模型推理过程进行预测的流行方法。一个更加准确的归因方法标志着它更加忠实，它可以更加准确地反映哪些部分的输入对预测更加重要。然而，现有的忠实度评估方法，如充分性和全面性，只使用一种硬删除标准，即完全删除或保留由给定归因方法排名最高的顶部标记，并观察预测可能性的变化。因此，这种硬删除标准忽略了每个标记的重要性，把它们全部等同地处理。在本文中，我们提出了一个简单而有效的软删除标准。我们不会完全删除或保留输入中的标记，而是随机地遮盖代表归因方法重要性的部分标记向量表示。基于各种自然语言处理任务和不同的归因方法进行的广泛实验表明，我们的方法显著优于现有的评估方法。

    Feature attribution methods (FAs) are popular approaches for providing insights into the model reasoning process of making predictions. The more faithful a FA is, the more accurately it reflects which parts of the input are more important for the prediction. Widely used faithfulness metrics, such as sufficiency and comprehensiveness use a hard erasure criterion, i.e. entirely removing or retaining the top most important tokens ranked by a given FA and observing the changes in predictive likelihood. However, this hard criterion ignores the importance of each individual token, treating them all equally for computing sufficiency and comprehensiveness. In this paper, we propose a simple yet effective soft erasure criterion. Instead of entirely removing or retaining tokens from the input, we randomly mask parts of the token vector representations proportionately to their FA importance. Extensive experiments across various natural language processing tasks and different FAs show that our sof
    
[^72]: 通过相空间分析和卷积神经网络理解正常和异常心脏

    Understanding of Normal and Abnormal Hearts by Phase Space Analysis and Convolutional Neural Networks. (arXiv:2305.10450v1 [eess.IV])

    [http://arxiv.org/abs/2305.10450](http://arxiv.org/abs/2305.10450)

    通过相空间分析和卷积神经网络结合诊断心脏疾病，在MIT-BIH数据库上取得了93.3%的平均准确率。

    

    心脏疾病是现代工业化社会中致死因素之一，导致公共卫生系统的高昂开支。因高昂成本，开发分析方法以改善心脏诊断至关重要。本文通过非线性微分方程将心脏电活动建模，并研究起源于确定性动态的心脏频谱的变化。将时间序列心电图(ECG)图像提取相空间轨迹，并基于 MIT-BIH 数据库中记录的 44 个 MLII 图像应用相空间分析和卷积神经网络(CNN)方法进行处理。为了提高准确性，将相空间记录的最高 Q-R 距离之间画一条直线。对相空间图像训练二进制 CNN 分类模型以分类正常和异常心脏，该方法达到平均准确率 93.3%，证明了相空间分析和 CNN 均在心脏疾病诊断中的有效性。

    Cardiac diseases are one of the leading mortality factors in modern, industrialized societies, which cause high expenses in public health systems. Due to high costs, developing analytical methods to improve cardiac diagnostics is essential. The heart's electric activity was first modeled using a set of nonlinear differential equations. Following this, variations of cardiac spectra originating from deterministic dynamics are investigated. Analyzing a normal human heart's power spectra offers His-Purkinje network, which possesses a fractal-like structure. Phase space trajectories are extracted from the time series electrocardiogram (ECG) graph with third-order derivate Taylor Series. Here in this study, phase space analysis and Convolutional Neural Networks (CNNs) method are applied to 44 records via the MIT-BIH database recorded with MLII. In order to increase accuracy, a straight line is drawn between the highest Q-R distance in the phase space images of the records. Binary CNN classif
    
[^73]: 面向视觉文档理解的统一模态掩码序列预训练

    Sequence-to-Sequence Pre-training with Unified Modality Masking for Visual Document Understanding. (arXiv:2305.10448v1 [cs.CL])

    [http://arxiv.org/abs/2305.10448](http://arxiv.org/abs/2305.10448)

    本文提出了一个统一的序列到序列文档理解模型，采用跨三种模态的统一掩码进行预训练，并且结构灵活适应各种下游任务输出格式。模型采用多种任务同时预训练，而且结合分解注意力和模态专家组合策略以提高信息捕获效率。

    

    本文提出了GenDoc，一种通用的序列到序列文档理解模型，使用跨三种模态的统一掩码进行预训练：文本、图像和布局。该模型采用编码器-解码器架构，与文档理解中常用的仅编码器模型相比，能够更好地适应各种产生不同输出格式的下游任务。此外，我们的预训练任务不仅包括以往编码器-解码器模型中使用的传统文本填充任务，还包括屏蔽的图像令牌预测和屏蔽的布局预测。我们设计了模态特定的指导和采用分解注意力和模态专家组合策略，以有效地捕捉每种模态所利用的信息。

    This paper presents GenDoc, a general sequence-to-sequence document understanding model pre-trained with unified masking across three modalities: text, image, and layout. The proposed model utilizes an encoder-decoder architecture, which allows for increased adaptability to a wide range of downstream tasks with diverse output formats, in contrast to the encoder-only models commonly employed in document understanding. In addition to the traditional text infilling task used in previous encoder-decoder models, our pre-training extends to include tasks of masked image token prediction and masked layout prediction. We also design modality-specific instruction and adopt both disentangled attention and the mixture-of-modality-experts strategy to effectively capture the information leveraged by each modality. Evaluation of the proposed model through extensive experiments on several downstream tasks in document understanding demonstrates its ability to achieve superior or competitive performanc
    
[^74]: 神经网络自动化评分中动态损失函数的有效性

    The Effectiveness of a Dynamic Loss Function in Neural Network Based Automated Essay Scoring. (arXiv:2305.10447v1 [cs.CL])

    [http://arxiv.org/abs/2305.10447](http://arxiv.org/abs/2305.10447)

    本研究提出了一种动态损失函数，帮助神经网络自动化评分系统在预测值的同时对正确的分布有更高的预测能力，而不牺牲任何性能。

    

    神经网络和特别是注意力机制为自动化作文评分领域带来了重大进展。许多这些系统使用基于回归的模型，当模型只预测训练数据的平均值时，可能会容易出现欠拟合。在本文中，我们介绍了一种动态损失函数，它为模型创建了一个激励，使其预测正确的分布，同时预测正确的值。我们的损失函数在不降低任何性能的情况下实现了这个目标，在Automated Student Assessment Prize Automated Essay Scoring数据集上实现了0.752的二次加权kappa得分。

    Neural networks and in particular the attention mechanism have brought significant advances to the field of Automated Essay Scoring. Many of these systems use a regression-based model which may be prone to underfitting when the model only predicts the mean of the training data. In this paper, we present a dynamic loss function that creates an incentive for the model to predict with the correct distribution, as well as predicting the correct values. Our loss function achieves this goal without sacrificing any performance achieving a Quadratic Weighted Kappa score of 0.752 on the Automated Student Assessment Prize Automated Essay Scoring dataset.
    
[^75]: 基于心理组成部分的情感识别——基于情绪调节指导叙述的研究

    Emotion Recognition based on Psychological Components in Guided Narratives for Emotion Regulation. (arXiv:2305.10446v1 [cs.CL])

    [http://arxiv.org/abs/2305.10446](http://arxiv.org/abs/2305.10446)

    本文在情绪调节指导的叙述中引入了一个新的法语情感叙述语料库，研究了四个组成部分（行为、感觉、思考和领域）对情感分类的影响，并证明共同考虑所有组成部分可以获得最佳结果。

    

    情绪调节是处理情感事件的关键元素，并对心理健康有积极影响。本文旨在通过引入一种新的法语情感叙述语料库，该语料库是使用情绪调节问卷收集而来的，以提供更全面的情感事件理解。我们遵循组成过程模型的理论框架，将情绪视为由四个相互关联的组成部分（行为、感觉、思考和领域）组成的动态过程。每个叙述都与一种离散情感相关，并根据作者的所有情感组成部分构建结构。我们使用机器学习方法和预训练语言模型研究组成部分之间的交互及其对情感分类的影响。结果表明，每个组成部分都可以提高预测性能，并且共同考虑所有组成部分可以实现最佳结果。我们的结果还显示预训练语言模型在预测情感分类方面的有效性。

    Emotion regulation is a crucial element in dealing with emotional events and has positive effects on mental health. This paper aims to provide a more comprehensive understanding of emotional events by introducing a new French corpus of emotional narratives collected using a questionnaire for emotion regulation. We follow the theoretical framework of the Component Process Model which considers emotions as dynamic processes composed of four interrelated components (behavior, feeling, thinking and territory). Each narrative is related to a discrete emotion and is structured based on all emotion components by the writers. We study the interaction of components and their impact on emotion classification with machine learning methods and pre-trained language models. Our results show that each component improves prediction performance, and that the best results are achieved by jointly considering all components. Our results also show the effectiveness of pre-trained language models in predict
    
[^76]: 记忆有益：自回归语言模型的加密

    Memorization for Good: Encryption with Autoregressive Language Models. (arXiv:2305.10445v1 [cs.CL])

    [http://arxiv.org/abs/2305.10445](http://arxiv.org/abs/2305.10445)

    该论文提出了第一个使用自回归语言模型进行对称加密的算法（SELM），其中算法可以将任意数据编码为紧凑的实值向量（即加密），然后通过随机子空间优化和贪心解码将向量无损解码为原始消息（即解密），并且SELM在加密分析方面的安全性能较高。

    

    过度参数化的神经语言模型（LM）可以记忆和背诵大量训练数据。虽然这种记忆通常被认为具有不良属性，例如过度拟合和信息泄漏，但我们的工作将记忆视为LM的一种未开发的能力。我们提出了第一个使用自回归语言模型进行对称加密的算法（SELM）。我们证明，自回归LM可以将任意数据编码为紧凑的实值向量（即加密），然后通过随机子空间优化和贪心解码将向量无损解码为原始消息（即解密）。虽然SELM不易受传统加密分析方法攻破，但我们通过一种新颖的实证变体，研究它的安全性。我们的代码和数据集可在https://github.com/OSU-NLP-Group/SELM 上获得。

    Over-parameterized neural language models (LMs) can memorize and recite long sequences of training data. While such memorization is normally associated with undesired properties such as overfitting and information leaking, our work casts memorization as an unexplored capability of LMs. We propose the first symmetric encryption algorithm with autoregressive language models (SELM). We show that autoregressive LMs can encode arbitrary data into a compact real-valued vector (i.e., encryption) and then losslessly decode the vector to the original message (i.e., decryption) via random subspace optimization and greedy decoding. While SELM is not amenable to conventional cryptanalysis, we investigate its security through a novel empirical variant of the classic IND-CPA (indistinguishability under chosen-plaintext attack) game. Our code and datasets are available at https://github.com/OSU-NLP-Group/SELM.
    
[^77]: IMAGINATOR：使用基于单词级别图像本体的预训练图像+文本联合嵌入

    IMAGINATOR: Pre-Trained Image+Text Joint Embeddings using Word-Level Grounding of Images. (arXiv:2305.10438v1 [cs.CL])

    [http://arxiv.org/abs/2305.10438](http://arxiv.org/abs/2305.10438)

    IMAGINATOR是一个使用基于单词级别图像本体的预训练图像+文本联合嵌入，能将多模态数据编码为矢量空间。

    

    单词嵌入是一种语义有意义的单词向量表示，主要受到分布假设“你应该通过它的伴侣来认识一个单词”（Harris，1954）的影响，而现代基于预测的神经网络嵌入则依赖于设计选择和超参数优化。这篇论文介绍了一种名为IMAGINATOR的预训练联合嵌入（JE），它是在1M个图像+文本对中从21K个不同的图像对象级别进行训练的。JE是一种将多模态数据编码为矢量空间的方法，其中文本模态作为基础关键词，而补充模态（在这种情况下为图像）则与之相连。

    Word embeddings, i.e., semantically meaningful vector representation of words, are largely influenced by the distributional hypothesis "You shall know a word by the company it keeps" (Harris, 1954), whereas modern prediction-based neural network embeddings rely on design choices and hyperparameter optimization. Word embeddings like Word2Vec, GloVe etc. well capture the contextuality and real-world analogies but contemporary convolution-based image embeddings such as VGGNet, AlexNet, etc. do not capture contextual knowledge. The popular king-queen analogy does not hold true for most commonly used vision embeddings.  In this paper, we introduce a pre-trained joint embedding (JE), named IMAGINATOR, trained on 21K distinct image objects level from 1M image+text pairs. JE is a way to encode multimodal data into a vector space where the text modality serves as the ground-ing key, which the complementary modality (in this case, the image) is anchored with. IMAGINATOR encapsulates three indivi
    
[^78]: 智能手机：探索自动生成语音和视觉提示的关键词记忆法

    SmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and Visual Cues. (arXiv:2305.10436v1 [cs.CL])

    [http://arxiv.org/abs/2305.10436](http://arxiv.org/abs/2305.10436)

    本文研究了利用大型语言模型自动生成口头和视觉提示的关键词记忆法。通过人类参与者实验，我们的方法明显优于传统闪卡，并可与手动制作的关键词记忆法相媲美。

    

    在第二语言的词汇学习中，现有的研究主要关注于学习界面或制定个性化的复习练习以最大化记忆保留。然而，学习内容，即闪卡上呈现的信息，大多数都保持不变。关键词记忆法是一种显著的学习策略，它通过使用发音相似的关键词构建声音和想象的联系来将新词汇与现有知识联系起来。此外，为了促进建立这些联系，生成与关键词相关的口头和视觉提示需要手动进行，这不可扩展。在本文中，我们探讨了利用大型语言模型自动生成关键词记忆法口头和视觉提示的机会。我们的方法是一个端到端的自动生成口头和视觉提示的流水线，可以自动生成高度容易记忆的提示。我们通过与传统闪卡和手动制作的关键词记忆法的比较，通过人类参与者实验来研究我们方法的有效性。结果表明，我们的方法明显优于传统闪卡，并可与手动制作的关键词记忆法相媲美。

    In second language vocabulary learning, existing works have primarily focused on either the learning interface or scheduling personalized retrieval practices to maximize memory retention. However, the learning content, i.e., the information presented on flashcards, has mostly remained constant. Keyword mnemonic is a notable learning strategy that relates new vocabulary to existing knowledge by building an acoustic and imagery link using a keyword that sounds alike. Beyond that, producing verbal and visual cues associated with the keyword to facilitate building these links requires a manual process and is not scalable. In this paper, we explore an opportunity to use large language models to automatically generate verbal and visual cues for keyword mnemonics. Our approach, an end-to-end pipeline for auto-generating verbal and visual cues, can automatically generate highly memorable cues. We investigate the effectiveness of our approach via a human participant experiment by comparing it w
    
[^79]: 生成的预训练变形器：启用技术、潜在应用、新兴挑战和未来方向的综述

    Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions. (arXiv:2305.10435v1 [cs.CL])

    [http://arxiv.org/abs/2305.10435](http://arxiv.org/abs/2305.10435)

    生成的预训练变形器是一种基于变形器架构的深度神经网络，能够在自然语言处理任务中表现出色且有效地进行对话，具有广泛的潜在应用，但仍面临新兴挑战和局限性。

    

    生成的预训练变形器模型代表了自然语言处理领域的一项重大突破，将我们推向开发能够像人类一样理解和使用语言进行交流的机器。生成的预训练变形器模型基于变形器架构，这是一种专门设计用于自然语言处理任务的深度神经网络。由于在自然语言处理任务上表现出色且能够有效地进行对话，生成的预训练变形器模型在研究人员和工业界社区中获得了显著的知名度，成为自然语言处理及相关领域中最广泛使用和有效的模型之一，这促使进行了本综述。本综述详细介绍了生成预训练变形器，包括其架构、工作过程、训练过程、启用技术以及在各个领域的潜在应用。同时，本综述还讨论了该模型面临的新兴挑战和局限性，并提供了未来研究的可能方向。

    The Generative Pre-trained Transformer models represent a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans. Generative Pre-trained Transformer models are based on the transformer architecture, a deep neural network designed for natural language processing tasks. Due to their impressive performance on natural language processing tasks and ability to effectively converse, Generative Pre-trained Transformer models have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review. This review provides a detailed overview of the Generative Pre-trained Transformer, including its architecture, working process, training procedures, enabling technologies, an
    
[^80]: 利用大型视觉语言模型学习文本的视觉性

    Learning the Visualness of Text Using Large Vision-Language Models. (arXiv:2305.10434v1 [cs.CL])

    [http://arxiv.org/abs/2305.10434](http://arxiv.org/abs/2305.10434)

    该论文利用大型视觉语言模型如CLIP来检测文本的视觉性，并提出fine-tuning策略，将非视觉文本映射为NULL图像，匹配视觉文本与对应图像，以解锁在文本中嵌入相关图像的能力。

    

    视觉文本会在人们的脑海中呈现图像，而非视觉文本则无法达到此效果。自动检测文本的视觉性将有助于在文本中嵌入相关图像。我们创建了一个数据集，包括3620个英语句子及其多个人类注释者提供的视觉性得分，并使用包含文本和视觉资产的文档来创建远程监督语料库，以评估文本的视觉性。

    Visual text evokes an image in a person's mind, while non-visual text fails to do so. A method to automatically detect visualness in text will unlock the ability to augment text with relevant images, as neural text-to-image generation and retrieval models operate on the implicit assumption that the input text is visual in nature. We curate a dataset of 3,620 English sentences and their visualness scores provided by multiple human annotators. Additionally, we use documents that contain text and visual assets to create a distantly supervised corpus of document text and associated images. We also propose a fine-tuning strategy that adapts large vision-language models like CLIP that assume a one-to-one correspondence between text and image to the task of scoring text visualness from text input alone. Our strategy involves modifying the model's contrastive learning objective to map text identified as non-visual to a common NULL image while matching visual text to their corresponding images 
    
[^81]: 毒性检测与反馈的数据真实性评估框架

    Toxicity Inspector: A Framework to Evaluate Ground Truth in Toxicity Detection Through Feedback. (arXiv:2305.10433v1 [cs.CL])

    [http://arxiv.org/abs/2305.10433](http://arxiv.org/abs/2305.10433)

    本文介绍了一个毒性检测框架，通过迭代反馈循环提高毒性数据集的可靠性，并通过两种指标平衡性能和毒性避免之间的权衡。

    

    毒性语言的定义并不明确，因为其存在许多变体和感知差异。其高度的情景依赖性和主观性解释增加了检测毒性语言的挑战，可能会降低数据集的可靠性并对检测模型性能产生负面影响。为填补这一空白，本文介绍了一个毒性检测框架，结合了人工参与的流程，旨在通过迭代反馈循环来提高毒性基准数据集的可靠性，并通过两种指标（硬性和软性）来平衡性能和毒性避免之间的权衡。

    Toxic language is difficult to define, as it is not monolithic and has many variations in perceptions of toxicity. This challenge of detecting toxic language is increased by the highly contextual and subjectivity of its interpretation, which can degrade the reliability of datasets and negatively affect detection model performance. To fill this void, this paper introduces a toxicity inspector framework that incorporates a human-in-the-loop pipeline with the aim of enhancing the reliability of toxicity benchmark datasets by centering the evaluator's values through an iterative feedback cycle. The centerpiece of this framework is the iterative feedback process, which is guided by two metric types (hard and soft) that provide evaluators and dataset creators with insightful examination to balance the tradeoff between performance gains and toxicity avoidance.
    
[^82]: 使用生成语言模型进行大规模文本分析：在AI专利中发现公共价值表达的案例研究

    Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents. (arXiv:2305.10383v1 [cs.CL])

    [http://arxiv.org/abs/2305.10383](http://arxiv.org/abs/2305.10383)

    本文研究使用生成语言模型GPT-4进行大规模文本分析，在US AI专利中发现公共价值表达。采用高级布尔查询收集了154,934个专利文档，并与USPTO的完整专利文本合并。得出5.4百万句子的语料库，使用框架以及GPT-4提示进行标记和理性化。评估结果表明，这种方法很准确。

    

    标记数据对于训练文本分类器至关重要，但对于复杂和抽象的概念而言，准确标记常常很难实现。本文采用一种新颖方法，使用生成语言模型（GPT-4）进行大规模文本分析的标记和理性化。我们将这种方法应用于在美国AI专利中发现公共价值表达的任务上。我们使用在InnovationQ+上提交的高级布尔查询收集了一个包含154,934个专利文档的数据库，这些结果与来自USPTO的完整专利文本合并，总计5.4百万句子。我们设计了一个框架来识别和标记这些AI专利句子中的公共价值表达。我们开发了GPT-4的提示，其中包括文本分类的定义、指导方针、示例和理性化。我们使用BLEU分数和主题建模评估了GPT-4生成的标签和理性化的质量，并发现它们是准确的。

    Labeling data is essential for training text classifiers but is often difficult to accomplish accurately, especially for complex and abstract concepts. Seeking an improved method, this paper employs a novel approach using a generative language model (GPT-4) to produce labels and rationales for large-scale text analysis. We apply this approach to the task of discovering public value expressions in US AI patents. We collect a database comprising 154,934 patent documents using an advanced Boolean query submitted to InnovationQ+. The results are merged with full patent text from the USPTO, resulting in 5.4 million sentences. We design a framework for identifying and labeling public value expressions in these AI patent sentences. A prompt for GPT-4 is developed which includes definitions, guidelines, examples, and rationales for text classification. We evaluate the quality of the labels and rationales produced by GPT-4 using BLEU scores and topic modeling and find that they are accurate, di
    
[^83]: FACE: 使用交叉熵的傅里叶分析评估自然语言生成

    FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy. (arXiv:2305.10307v1 [cs.CL])

    [http://arxiv.org/abs/2305.10307](http://arxiv.org/abs/2305.10307)

    FACE是一组可以有效识别人类和模型之间差距的度量标准。它基于傅里叶分析和交叉熵估计，可以反映模型大小、解码采样方法和人类评分。

    

    评估机器生成的语言与人类语言之间的距离是一个至关重要的问题。受到语言学心理学关于语言熵周期性实证发现的启示，我们提出了FACE——一组基于语言交叉熵的傅里叶分析的度量，用于衡量生成模型产生的语言与人类书写语言之间的相似度。通过一个开放式的生成任务和以前研究的实验数据，我们发现FACE可以有效地识别人类模型差距，在模型规模上有所缩放，反映了不同解码采样方法的结果，与其他评估指标和人类判断分数相关良好。FACE在计算上是高效的，并提供直观的解释。

    Measuring the distance between machine-produced and human language is acritical open problem. Inspired by empirical findings from psycholinguistics on theperiodicity of entropy in language, we propose FACE, a set of metrics based onFourier Analysis of the estimated Cross-Entropy of language, for measuring thesimilarity between model-generated and human-written languages. Based on anopen-ended generation task and the experimental data from previous studies, weind that FACE can effectively identify the human-model gap, scales with modelsize, reflects the outcomes of different sampling methods for decoding, correlateswell with other evaluation metrics and with human judgment scores. FACE iscomputationally efficient and provides intuitive interpretations.
    
[^84]: MemoryBank: 用长期记忆增强大型语言模型

    MemoryBank: Enhancing Large Language Models with Long-Term Memory. (arXiv:2305.10250v1 [cs.CL])

    [http://arxiv.org/abs/2305.10250](http://arxiv.org/abs/2305.10250)

    MemoryBank 提出了一种新型内存机制，旨在为大型语言模型提供类人的长期记忆。它可以召唤相关记忆，通过持续的记忆更新不断进化，通过合成过去的互动信息理解并适应用户个性。

    

    大型语言模型的革命性进展极大地改变了我们与人工智能系统的互动方式。尽管如此，其中一个明显的不足之处是这些模型缺乏长期记忆机制。这在需要持续互动的情况下尤为明显，例如个人伴侣系统和心理咨询。因此，我们提出了MemoryBank，这是一种专为LLM量身定制的新型内存机制。MemoryBank可以召唤相关记忆，通过持续的记忆更新不断进化，通过合成过去的互动信息理解并适应用户个性。为了模仿人类行为并有选择地保存记忆，MemoryBank采用了受Ebbinghaus遗忘曲线理论启发的记忆更新机制，这样人工智能可以根据时间和记忆的相对重要性来遗忘和加强记忆，从而为LLM提供类似于人类的长期记忆。

    Revolutionary advancements in Large Language Models have drastically reshaped our interactions with artificial intelligence systems. Despite this, a notable hindrance remains-the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems and psychological counseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user personality by synthesizing information from past interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory, which permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a hum
    
[^85]: EfficientSCI: 稠密连接网络与时空分解相结合的大规模视频快照压缩成像

    EfficientSCI: Densely Connected Network with Space-time Factorization for Large-scale Video Snapshot Compressive Imaging. (arXiv:2305.10006v1 [cs.CV])

    [http://arxiv.org/abs/2305.10006](http://arxiv.org/abs/2305.10006)

    本文提出了EfficientSCI网络，通过使用稠密连接和时空分解机制来建立视频SCI中的空间-时间相关性。相比最先进的基于深度学习的方法，它能够在计算效率和重建质量方面取得更好的表现。

    

    视频快照压缩成像 (SCI) 使用二维检测器在单个曝光时间内捕获连续视频帧。然后需要设计高效的重建算法来重建所需的视频帧。虽然最近的基于深度学习的最新重建算法已经在大多数任务上取得了良好的结果，但它们仍然面临以下挑战：过度的模型复杂性和GPU内存限制。其中，1)这些模型需要高计算成本，2)它们通常不能在高压缩比下重建大规模的视频帧。为了解决这些问题，我们开发了一种高效的视频SCI网络，使用单个残差块内的稠密连接和时空分解机制，命名为EfficientSCI。 EfficientSCI网络可以通过在空间域中使用卷积和在时间域中使用转换域稀疏化（TDS）来很好地建立空间 - 时间相关性，这显著降低了计算成本和内存使用。实验结果表明，我们的EfficientSCI在各种大规模视频SCI数据集上，在重建质量和计算效率方面均优于最新的基于深度学习的方法。

    Video snapshot compressive imaging (SCI) uses a two-dimensional detector to capture consecutive video frames during a single exposure time. Following this, an efficient reconstruction algorithm needs to be designed to reconstruct the desired video frames. Although recent deep learning-based state-of-the-art (SOTA) reconstruction algorithms have achieved good results in most tasks, they still face the following challenges due to excessive model complexity and GPU memory limitations:  1) these models need high computational cost, and  2) they are usually unable to reconstruct large-scale video frames at high compression ratios.  To address these issues, we develop an {\bf{\em efficient network}} for video SCI by using {\bf {\em dense connections and space-time factorization mechanism}} within a single residual block, dubbed {\bf \emph{EfficientSCI}}. The EfficientSCI network can well establish spatial-temporal correlation by using {\bf {\em convolution in the spatial domain and Transform
    
[^86]: “我全然成为我自己”：以TGNB人群为中心，评估开放式语言生成中的偏见

    "I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])

    [http://arxiv.org/abs/2305.09941](http://arxiv.org/abs/2305.09941)

    本论文研究了如何以TGNB人群的声音为中心，评估开放式语言生成中的偏见。通过理解TGNB个体的经历，提出了以TGNB人群为中心的OLG系统评估框架，并且包括一个为TGNB人群设计的调查工具和分析方法。

    

    跨性别和非二元（TGNB）人群在日常生活中经历了不成比例的歧视和排斥。随着语言生成技术的日益普及和应用，进一步边缘化这一人群的可能性也在增加。虽然大量的NLP公平文献着重于阐明和解决性别偏见，但评估TGNB身份所带来的性别伤害需要理解这些身份如何独特地与社会性别规范互动以及与性别二元中心的视角相区分。这样的测量框架本质上需要以TGNB声音为中心，帮助指导包容性别的自然语言处理应该为谁服务。为实现这一目标，我们以TGNB社区和现有的跨学科文献为基础，评估了TGNB个体经历边缘化所形成的社会现实是如何影响和存在于开放式语言生成（OLG）中。首先理解TGNB个体的经历，我们提出了一个评估OLG系统的框架，旨在以TGNB人群为中心，度量与该人群相关的偏见。我们的框架包括特别为TGNB人群设计的调查工具，以及交叉分析结果的交叉方法。我们相信，这项工作将有助于实现更公平、更包容的自然语言处理社区，并潜在地解决NLP研究中广泛的交叉身份问题。

    Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
    
[^87]: Epsilon Sampling Rocks: 研究用于机器翻译最小贝叶斯风险解码的采样策略

    Epsilon Sampling Rocks: Investigating Sampling Strategies for \\Minimum Bayes Risk Decoding for Machine Translation. (arXiv:2305.09860v1 [cs.CL])

    [http://arxiv.org/abs/2305.09860](http://arxiv.org/abs/2305.09860)

    本文研究了用于机器翻译最小贝叶斯风险解码的不同采样策略，并发现了epsilon采样方式能够使得解码结果显著地优于其他所有已测试的采样方式和束搜索解码。

    

    机器翻译中的最小贝叶斯风险（MBR）解码已经显示出是一种强大的替代束搜索解码的方法，尤其是与基于神经网络的效用函数相结合时。然而，MBR解码的性能严重依赖于从模型中采样的方法和数量。本文探讨了用于MBR解码的不同采样方法对性能的影响。我们评估了一些流行的采样方法，例如祖先采样，核采样和top-k采样。基于我们对它们局限性的认识，我们尝试了最近提出的epsilon采样方法，该方法通过修剪所有小于epsilon的标记，以确保样本中的每个标记获得公平的概率质量。通过广泛的人类评估，我们证明了基于epsilon采样的MBR解码显著优于不仅是束搜索解码，而且还优于所有其他已测试的采样方法的MBR解码。

    Recent advances in machine translation (MT) have shown that Minimum Bayes Risk (MBR) decoding can be a powerful alternative to beam search decoding, especially when combined with neural-based utility functions. However, the performance of MBR decoding depends heavily on how and how many candidates are sampled from the model. In this paper, we explore how different sampling approaches for generating candidate lists for MBR decoding affect performance. We evaluate popular sampling approaches, such as ancestral, nucleus, and top-k sampling. Based on our insights into their limitations, we experiment with the recently proposed epsilon-sampling approach, which prunes away all tokens with a probability smaller than epsilon, ensuring that each token in a sample receives a fair probability mass. Through extensive human evaluations, we demonstrate that MBR decoding based on epsilon-sampling significantly outperforms not only beam search decoding, but also MBR decoding with all other tested samp
    
[^88]: DarkBERT：针对互联网黑暗面的语言模型

    DarkBERT: A Language Model for the Dark Side of the Internet. (arXiv:2305.08596v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08596](http://arxiv.org/abs/2305.08596)

    本研究介绍了一个在暗网数据上预先训练的语言模型——DarkBERT，对于暗网的研究具有重要的价值。

    

    最近的研究表明，与表层网络相比，暗网中使用的语言存在明显差异。由于研究暗网通常需要对域进行文本分析，因此针对暗网的语言模型可能为研究人员提供有价值的洞见。在本文中，我们介绍了DarkBERT，这是一个预先在暗网数据上训练的语言模型。我们描述了筛选和编译用于训练DarkBERT的文本数据的步骤，以应对暗网极为不同的词汇和结构多样性，这可能有害于构建该域的适当表示。我们评估了DarkBERT及其基础模型以及其他广泛使用的语言模型，以验证针对暗​​网的特定模型在各种用例中提供的好处。我们的评估显示，DarkBERT胜过当前的语言模型，可能成为未来暗网研究的有价值资源。

    Recent research has suggested that there are clear differences in the language used in the Dark Web compared to that of the Surface Web. As studies on the Dark Web commonly require textual analysis of the domain, language models specific to the Dark Web may provide valuable insights to researchers. In this work, we introduce DarkBERT, a language model pretrained on Dark Web data. We describe the steps taken to filter and compile the text data used to train DarkBERT to combat the extreme lexical and structural diversity of the Dark Web that may be detrimental to building a proper representation of the domain. We evaluate DarkBERT and its vanilla counterpart along with other widely used language models to validate the benefits that a Dark Web domain specific model offers in various use cases. Our evaluations show that DarkBERT outperforms current language models and may serve as a valuable resource for future research on the Dark Web.
    
[^89]: 超越相关分析的NLG评估指标：一种经验度量偏好检查表

    NLG Evaluation Metrics Beyond Correlation Analysis: An Empirical Metric Preference Checklist. (arXiv:2305.08566v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08566](http://arxiv.org/abs/2305.08566)

    本研究提出了一种度量偏好检查表，以超越相关分析评估NLG自动指标，并分析了两种类型的指标及其在三个任务中的效果。

    

    本研究分析了NLG自动评估指标，基于是否将人类评估方面用作上下文或目标来计算指标，分为（i）任务不可知和（ii）与人类对齐的指标。我们提出了度量偏好检查表作为评估自动指标在三个NLG任务中的鉴别力的框架：文本摘要，对话响应生成和受控生成。

    In this study, we analyze NLG automatic metrics based on whether human evaluation aspect is used as context or objective to compute the metrics: (i) Task-agnostic and (ii) Human-aligned. Task-agnostic metrics, such as Perplexity, BLEU, BERTScore, are cost-effective and highly adaptable to diverse NLG tasks, yet they have a weak correlation with human. Human-aligned metrics (CTC, CtrlEval, UniEval) improves correlation level by incorporating desirable human-like qualities as training objective. However, their effectiveness at discerning system-level performance and quality of system outputs remains unclear.  We present metric preference checklist as a framework to assess the discriminative power of automatic metrics in three NLG tasks: Text Summarization, Dialogue Response Generation, and Controlled Generation. We show that multi-aspect human-aligned metric (UniEval) is not necessarily dominant over single-aspect human-aligned metrics (CTC, CtrlEval) and task-agnostic metrics (BLEU, BER
    
[^90]: 一种用于抽象多文档摘要的层次编码-解码方案

    A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization. (arXiv:2305.08503v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08503](http://arxiv.org/abs/2305.08503)

    本研究提出了一种用于抽象多文档摘要的层次编码-解码方案，在多领域的10个MDS数据集上测试表现最佳。

    

    预训练语言模型（PLM）在抽象单文档摘要（SDS）方面取得了显著的成就。但是，这种好处可能不会轻易扩展到多文档摘要（MDS），因为文档之间的交互更加复杂。以前的工作要么设计新的架构或新的预训练目标，用于MDS，要么将PLM应用于MDS，但未考虑到复杂的文档交互。本文中，我们在编码器和解码器上强制使用层次结构，并寻求更好地利用PLM促进MDS任务的多文档交互作用。我们在10个MDS数据集上测试我们的设计，这些数据集覆盖各种领域。广泛的实验表明，我们提出的方法在所有这些数据集上都能够实现持续改进，优于最先进的MDS方法。

    Pre-trained language models (PLMs) have accomplished impressive achievements in abstractive single-document summarization (SDS). However, such benefits may not be readily extended to muti-document summarization (MDS), where the interactions among documents are more complex. Previous works either design new architectures or new pre-training objectives for MDS, or apply PLMs to MDS without considering the complex document interactions. While the former does not make full use of previous pre-training efforts and may not generalize well across multiple domains, the latter cannot fully attend to the intricate relationships unique to MDS tasks. In this paper, we enforce hierarchy on both the encoder and decoder and seek to make better use of a PLM to facilitate multi-document interactions for the MDS task. We test our design on 10 MDS datasets across a wide range of domains. Extensive experiments show that our proposed method can achieve consistent improvements on all these datasets, outperf
    
[^91]: 无需增加模型参数的序列到序列模型微调方法：基于结构化剪枝的LoRA方法

    Parameter-Efficient Fine-Tuning with Layer Pruning on Free-Text Sequence-to-Sequence modeling. (arXiv:2305.08285v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08285](http://arxiv.org/abs/2305.08285)

    本文提出了一个将LoRA和结构化层剪枝方法结合的框架，在保持超过92%生成质量的同时，通过调整仅0.6%的参数并剪枝超过30%的Transformer层，成功减少了50%的GPU内存使用并提升了100%的训练速度。

    

    语言模型尺寸的不断增长引起了对于参数效率的微调方法的研究兴趣，本文提出了一个将LoRA和结构化层剪枝方法结合的框架。这个框架在 MIMIC-IV-Note上的两个医疗报告概述数据集和两个公共医疗对话数据集上进行了验证。通过调整原始模型的0.6%的参数并剪枝超过30%的Transformer层，我们的框架可以减少50%的GPU内存使用并提升100%的训练速度，同时保持在自由文本序列到序列任务上超过92%的生成质量。

    The increasing size of language models raises great research interests in parameter-efficient fine-tuning such as LoRA that freezes the pre-trained model, and injects small-scale trainable parameters for multiple downstream tasks (e.g., summarization, question answering and translation). To further enhance the efficiency of fine-tuning, we propose a framework that integrates LoRA and structured layer pruning. The integrated framework is validated on two created deidentified medical report summarization datasets based on MIMIC-IV-Note and two public medical dialogue datasets. By tuning 0.6% parameters of the original model and pruning over 30% Transformer-layers, our framework can reduce 50% of GPU memory usage and speed up 100% of the training phase, while preserving over 92% generation qualities on free-text sequence-to-sequence tasks.
    
[^92]: 跨域问答泛化学习

    Learning to Generalize for Cross-domain QA. (arXiv:2305.08208v1 [cs.CL])

    [http://arxiv.org/abs/2305.08208](http://arxiv.org/abs/2305.08208)

    提出了一种不增加训练成本的跨域问答泛化学习方法，通过结合提示方法和线性探测再微调策略，有效提高了产生式和判别式模型的泛化能力，取得了优于基准方法4.5%-7.9%的结果。

    

    自然语言处理模型的跨域泛化能力，尤其是在问答任务中，一直存在着越来越大的担忧。当前的合成数据增强方法受到了增加训练成本的限制。为了解决这个问题，我们提出了一种结合提示方法和线性探测再微调策略的新方法，该方法不需要额外的成本。我们的方法在理论上和实证上都被证明有效，可以增强产生式和判别式模型的泛化能力。我们的方法优于现有的基准方法，F1得分平均提高了4.5%-7.9%。同时，我们的方法可以轻松地集成到任何预训练模型中，并为未充分开发的跨域问答任务提供了有前途的解决方案。我们在GitHub上公开了我们的源代码*。

    There have been growing concerns regarding the out-of-domain generalization ability of natural language processing (NLP) models, particularly in question-answering (QA) tasks. Current synthesized data augmentation methods for QA are hampered by increased training costs. To address this issue, we propose a novel approach that combines prompting methods and linear probing then fine-tuning strategy, which does not entail additional cost. Our method has been theoretically and empirically shown to be effective in enhancing the generalization ability of both generative and discriminative models. Our approach outperforms state-of-the-art baselines, with an average increase in F1 score of 4.5%-7.9%. Furthermore, our method can be easily integrated into any pre-trained models and offers a promising solution to the under-explored cross-domain QA task. We release our source code at GitHub*.
    
[^93]: 自监督神经因子分析解耦语音表示

    Self-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations. (arXiv:2305.08099v1 [cs.SD])

    [http://arxiv.org/abs/2305.08099](http://arxiv.org/abs/2305.08099)

    本文提出了一种自监督神经因子分析模型，使用HuBERT中的聚类方法来发现隐藏的声学单元，并使用这些单元对齐SSL模型的特征，从而产生解耦后的语音表示，从而为专门任务提供了一种基于Utterance水平的无监督学习目标。实验结果表明，SSNFA模型在说话人识别、语言识别和情感识别等各种任务中均显著优于现有的SSL模型，并且没有任何特定任务的微调或监督。

    

    自监督学习技术在自动语音识别方面已经展示了出色的性能，在低标注资源情况下证明非常有用，本文针对该技术在说话人、情感和语言识别等任务中的性能问题进行了探究。本文提出了一种因子分析模型，使用HuBERT中的聚类方法来发现隐藏的声学单元，并使用这些单元对齐SSL模型的特征，从而产生解耦后的语音表示，从而为专门任务提供了一种基于Utterance水平的无监督学习目标。实验结果表明，SSNFA模型在说话人识别、语言识别和情感识别等各种任务中均显著优于现有的SSL模型，并且没有任何特定任务的微调或监督。

    Self-supervised learning (SSL) speech models such as wav2vec and HuBERT have demonstrated state-of-the-art performance on automatic speech recognition (ASR) and proved to be extremely useful in low label-resource settings. However, the success of SSL models has yet to transfer to utterance-level tasks such as speaker, emotion, and language recognition, which still require supervised fine-tuning of the SSL models to obtain good performance. We argue that the problem is caused by the lack of disentangled representations and an utterance-level learning objective for these tasks. Inspired by how HuBERT uses clustering to discover hidden acoustic units, we formulate a factor analysis (FA) model that uses the discovered hidden acoustic units to align the SSL features. The underlying utterance-level representations are disentangled from the content of speech using probabilistic inference on the aligned features. Furthermore, the variational lower bound derived from the FA model provides an ut
    
[^94]: GPT-Sentinel：区分人类和ChatGPT生成内容的方法

    GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content. (arXiv:2305.07969v1 [cs.CL])

    [http://arxiv.org/abs/2305.07969](http://arxiv.org/abs/2305.07969)

    GPT-Sentinel通过语言模型检测ChatGPT生成的文本和人类编写的文本，其模型在测试数据集上准确率超过了97％，并且揭示了区分这两种文本关键特征的能力。

    

    本文提出了一种使用语言模型来检测ChatGPT生成的文本与人类编写文本的新方法。我们首先收集和发布了一个预处理数据集OpenGPTText，其中包含使用ChatGPT生成的重新表述内容。然后，我们设计、实现、训练了两个不同的文本分类模型，分别使用Robustly Optimized BERT Pretraining Approach（RoBERTa）和Text-to-Text Transfer Transformer（T5）。我们的模型在测试数据集上取得了显著的结果，通过各种指标进行评估，准确率超过了97%。此外，我们进行了可解释性研究，展示了我们的模型提取和区分人类编写和ChatGPT生成文本之间关键特征的能力。我们的发现为使用语言模型检测生成文本提供了重要的见解。

    This paper presents a novel approach for detecting ChatGPT-generated vs. human-written text using language models. To this end, we first collected and released a pre-processed dataset named OpenGPTText, which consists of rephrased content generated using ChatGPT. We then designed, implemented, and trained two different models for text classification, using Robustly Optimized BERT Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5), respectively. Our models achieved remarkable results, with an accuracy of over 97% on the test dataset, as evaluated through various metrics. Furthermore, we conducted an interpretability study to showcase our model's ability to extract and differentiate key features between human-written and ChatGPT-generated text. Our findings provide important insights into the effective use of language models to detect generated text.
    
[^95]: Dr. LLaMA：通过生成式数据增强改善特定领域QA中的小语言模型

    Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation. (arXiv:2305.07804v1 [cs.CL])

    [http://arxiv.org/abs/2305.07804](http://arxiv.org/abs/2305.07804)

    本论文介绍了一种名为Dr. LLaMA的方法，通过使用大型语言模型进行生成式数据增强，以改善小语言模型的性能，特别是在医学问答任务中。这种方法在微调后使模型性能提高，并提出了在特定领域问答任务中使用LLM所面临的挑战和潜在的研究方向。

    

    大型语言模型在自然语言处理方面取得了重大进展，但随着其规模的增长，也面临着计算开销和效率的挑战，特别是在特定领域的任务中。另一方面，小型语言模型由于容量和训练数据的限制，在这些任务中往往表现不佳。本文介绍了一种名为Dr. LLaMA的方法，通过使用大型语言模型进行生成式数据增强，聚焦医学问答任务和PubMedQA数据集，以改善小语言模型的性能。我们的发现表明，LLM有效地细化和扩展现有的问题-答案对，在微调后，使得小型模型在特定领域QA数据集上性能提高。本研究强调了在特定领域问答任务中使用LLM面临的挑战，并提出了潜在的研究方向，最终旨在为专业应用创建更高效和能力更强的模型。

    Large Language Models (LLMs) have made significant strides in natural language processing but face challenges in terms of computational expense and inefficiency as they grow in size, especially in domain-specific tasks. Small Language Models (SLMs), on the other hand, often struggle in these tasks due to limited capacity and training data. In this paper, we introduce Dr. LLaMA, a method for improving SLMs through generative data augmentation using LLMs, focusing on medical question-answering tasks and the PubMedQA dataset. Our findings indicate that LLMs effectively refine and diversify existing question-answer pairs, resulting in improved performance of a much smaller model on domain-specific QA datasets after fine-tuning. This study highlights the challenges of using LLMs for domain-specific question answering and suggests potential research directions to address these limitations, ultimately aiming to create more efficient and capable models for specialized applications. We have als
    
[^96]: 无监督句子嵌入的实例平滑对比学习

    Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding. (arXiv:2305.07424v1 [cs.CL])

    [http://arxiv.org/abs/2305.07424](http://arxiv.org/abs/2305.07424)

    本文提出了IS-CSE方法，通过实例平滑对比学习来学习无监督句子嵌入，以平滑嵌入在特征空间中的边界，从而提高模型的泛化性能。在标准的STS任务中取得了良好的得分。

    

    基于对比学习的方法，如unsup-SimCSE，在学习无监督句子嵌入方面取得了最先进（SOTA）的性能。然而，在以前的研究中，用于对比学习的每个嵌入仅来自于一个句子实例，我们称这些嵌入为实例级嵌入。换句话说，在这种情况下，每个嵌入被视为是一类独特的类，这可能会损害泛化性能。在本研究中，我们提出了IS-CSE（实例平滑对比句子嵌入）来平滑特征空间中嵌入的边界。具体而言，我们根据语义相似性从动态内存缓冲区中检索嵌入以获得正嵌入组。然后我们通过自注意力操作对组中的嵌入进行聚合，以生成平滑实例嵌入以进行进一步分析。我们在标准的语义文本相似性（STS）任务中评估了我们的方法，并实现了平均78.30％，79.47％，77.73％和79.42％的得分。

    Contrastive learning-based methods, such as unsup-SimCSE, have achieved state-of-the-art (SOTA) performances in learning unsupervised sentence embeddings. However, in previous studies, each embedding used for contrastive learning only derived from one sentence instance, and we call these embeddings instance-level embeddings. In other words, each embedding is regarded as a unique class of its own, whichmay hurt the generalization performance. In this study, we propose IS-CSE (instance smoothing contrastive sentence embedding) to smooth the boundaries of embeddings in the feature space. Specifically, we retrieve embeddings from a dynamic memory buffer according to the semantic similarity to get a positive embedding group. Then embeddings in the group are aggregated by a self-attention operation to produce a smoothed instance embedding for further analysis. We evaluate our method on standard semantic text similarity (STS) tasks and achieve an average of 78.30%, 79.47%, 77.73%, and 79.42% 
    
[^97]: ChatGPT是一个好的因果推断器吗？全面评估

    Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation. (arXiv:2305.07375v1 [cs.CL])

    [http://arxiv.org/abs/2305.07375](http://arxiv.org/abs/2305.07375)

    本文对ChatGPT的因果推理能力进行了首次全面评估，实验证明ChatGPT是一个好的因果解释者，但不是一个好的因果推理者，存在严重的因果幻觉问题，对于明确的因果关系表现良好。

    

    因果推理能力对于众多NLP应用至关重要。尽管ChatGPT在各种NLP任务中表现出令人印象深刻的新兴能力，但ChatGPT在因果推理方面的表现如何仍不清楚。本文对ChatGPT的因果推理能力进行了首次全面评估。实验证明，ChatGPT不是一个好的因果推理者，但是是一个好的因果解释者。此外，ChatGPT在因果推理方面存在严重的幻觉，可能是由于自然语言中因果关系和非因果关系的报告偏见，以及ChatGPT的升级过程，如RLHF。在上下文学习（ICL）和思维链（COT）技术方面，可能会进一步加剧这种因果幻觉。此外，ChatGPT的因果推理能力对于在提示中表达因果概念的词语非常敏感，并且封闭提示比开放提示表现更好。对于句子中的事件，ChatGPT擅长捕捉明确的因果关系。

    Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT's causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes, such as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT) techniques can further exacerbate such causal hallucination. Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. For events in sentences, ChatGPT excels at capturing explicit caus
    
[^98]: 基于基础模型的系统设计框架

    A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])

    [http://arxiv.org/abs/2305.05352](http://arxiv.org/abs/2305.05352)

    本文提出了一个基于基础模型的系统分类体系，分类和比较了基础模型和基于基础模型的系统的特点。它为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。

    

    最近推出了大型语言模型(LLM)的聊天机器人，如ChatGPT，这引起了人们对基础模型的广泛关注。基础模型被广泛认为将成为未来人工智能系统的基石。由于基础模型处于早期阶段，基于基础模型的系统设计尚未得到系统地探索。人们对在软件架构中引入基础模型的影响知之甚少。因此，在本文中，我们提出了一个基于基础模型的系统分类法，对基础模型和基于基础模型的系统的特点进行了分类和比较。我们的分类法包括三个类别：基础模型预训练和微调、基于基础模型的系统架构设计和负责任的AI设计。这个分类法为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。

    The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
    
[^99]: 从大型语言模型中提取脚本知识以进行受限语言规划

    Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v1 [cs.CL])

    [http://arxiv.org/abs/2305.05252](http://arxiv.org/abs/2305.05252)

    本文首次定义了受限语言规划任务，提出了一种方法来提高大型语言模型在这个任务中的表现，并提取了一个新颖的受限语言规划数据集。实验证明该方法显著提高了其在约束忠实度方面的能力，并对赋予较小的语言模型受限语言规划能力非常有效。

    

    在日常生活中，人们经常通过遵循目标导向的脚本形式的逐步说明来规划自己的行动。以往的工作利用语言模型（LM）来为立体活动的抽象目标（例如，“制作蛋糕”）进行规划，但对于具有多方面约束的更具体目标（例如，“为糖尿病患者制作蛋糕”）鲜有研究。本文首次定义了受限语言规划任务。我们提出了一种过度生成并过滤的方法来改善大型语言模型（LLM）在这个任务中的表现，并利用它来提取一种新颖的受限语言规划数据集CoScript，其中包括55,000个脚本。实验证明，我们的方法显著提高了LLM在受限语言规划方面的能力，特别是在约束忠实度方面。此外，CoScript被证明对赋予较小的LM受限语言规划能力是非常有效的。

    In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., "make a cake"), but leaves more specific goals with multi-facet constraints understudied (e.g., "make a cake for diabetics"). In this paper, we define the task of constrained language planning for the first time. We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, CoScript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.
    
[^100]: 带参数知识引导的增强型大语言模型

    Augmented Large Language Models with Parametric Knowledge Guiding. (arXiv:2305.04757v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.04757](http://arxiv.org/abs/2305.04757)

    这篇论文提出了一种带参数知识引导的增强型大语言模型框架，通过为LLMs装备信息引导模块来访问相关知识，同时保持LLMs的参数不变。这个框架可以提高黑盒LLMs在各种NLP任务上的性能。

    

    大型语言模型 (LLM) 以其出色的语言理解和生成能力，极大地推进了自然语言处理（NLP）。但是，由于对相关数据的有限接触，它们在需要专业知识的领域特定任务上的表现可能不够优化。此外，大多数最先进的 LLM 缺乏透明度，只能通过 API 访问, 这阻止了进一步用领域定制数据进行微调。此外，向 LLM 所有者提供私有数据会导致数据隐私问题。为解决这些挑战，我们提出了新型的带参数知识引导 (PKG) 框架，该框架为 LLM 配备了知识引导模块，以访问相关知识，而无需改变 LLM 的参数。我们的 PKG 基于开源的“白盒”语言模型，允许离线存储 LLM 需要的任何知识。我们证明了我们的 PKG 框架可以提高“黑盒”LLM在各种NLP任务上的性能。

    Large Language Models (LLMs) have significantly advanced natural language processing (NLP) with their impressive language understanding and generation capabilities. However, their performance may be suboptimal for domain-specific tasks that require specialized knowledge due to limited exposure to the related data. Additionally, the lack of transparency of most state-of-the-art (SOTA) LLMs, which can only be accessed via APIs, impedes further fine-tuning with domain custom data. Moreover, providing private data to the LLMs' owner leads to data privacy problems. To address these challenges, we propose the novel Parametric Knowledge Guiding (PKG) framework, which equips LLMs with a knowledge-guiding module to access relevant knowledge without altering the LLMs' parameters. Our PKG is based on open-source "white-box" language models, allowing offline memory of any knowledge that LLMs require. We demonstrate that our PKG framework can enhance the performance of "black-box" LLMs on a range o
    
[^101]: 各种神经机器翻译的统一模型学习

    Unified Model Learning for Various Neural Machine Translation. (arXiv:2305.02777v1 [cs.CL])

    [http://arxiv.org/abs/2305.02777](http://arxiv.org/abs/2305.02777)

    本文提出了一种统一学习方法，即统一模型学习，可以同时适用于翻译各种任务数据，并实现智能按需翻译，相对现有的特定数据集模型能够得到明显的改进。

    

    现有的神经机器翻译(NMT)研究主要集中在根据来自不同任务(例如，文档翻译和聊天翻译)的数据开发特定于数据集的模型。虽然特定于数据集的模型已经取得了令人瞩目的性能，但每个数据集需要设计、训练和存储一个模型，这很麻烦。在这项工作中，我们的目标是将这些翻译任务统一到更普遍的设置中。具体而言，我们提出了一个“多才多艺”的模型，即适用于不同任务数据的统一模型学习(NMT)，可以同时在多种环境下进行良好的翻译，并在理论上可以尽可能多地扩展。通过统一学习，UMLNMT能够跨多个任务进行联合训练，实现智能按需翻译。在七个广泛使用的翻译任务，包括句子翻译、文档翻译和聊天翻译中，我们的UMLNMT相对于特定数据集模型表现出了明显的改进。

    Existing neural machine translation (NMT) studies mainly focus on developing dataset-specific models based on data from different tasks (e.g., document translation and chat translation). Although the dataset-specific models have achieved impressive performance, it is cumbersome as each dataset demands a model to be designed, trained, and stored. In this work, we aim to unify these translation tasks into a more general setting. Specifically, we propose a ``versatile'' model, i.e., the Unified Model Learning for NMT (UMLNMT) that works with data from different tasks, and can translate well in multiple settings simultaneously, and theoretically it can be as many as possible. Through unified learning, UMLNMT is able to jointly train across multiple tasks, implementing intelligent on-demand translation. On seven widely-used translation tasks, including sentence translation, document translation, and chat translation, our UMLNMT results in substantial improvements over dataset-specific model
    
[^102]: 无限长度输入的长距离Transformer-Unlimiformer

    Unlimiformer: Long-Range Transformers with Unlimited Length Input. (arXiv:2305.01625v1 [cs.CL])

    [http://arxiv.org/abs/2305.01625](http://arxiv.org/abs/2305.01625)

    Unlimiformer是一种Transformer模型的通用方法，可以将所有层的注意计算卸载到单个k近邻索引上，从而可处理无限长度的输入，而不增加额外的学习负担。

    

    基于Transformer的模型通常对输入长度有预定义的限制，因为它们可能需要参考输入中的每个标记。本文提出了一种通用方法-Unlimiformer，可以包装任何现有的预训练编码器-解码器Transformer，并将所有层的注意计算卸载到单个k近邻索引上。我们在几个长文档和多文档摘要基准测试中证明了Unlimiformer的有效性，展示了它可以总结350k令牌长的输入而不进行测试时的截断。

    Transformer-based models typically have a predefined bound to their input length, because of their need to potentially attend to every token in the input. In this work, we propose Unlimiformer: a general approach that can wrap any existing pretrained encoder-decoder transformer, and offload the attention computation across all layers to a single $k$-nearest-neighbor index; this index can be kept on either the GPU or CPU memory and queried in sub-linear time. This way, we can index extremely long input sequences, while every attention head in every decoder layer retrieves its top-$k$ keys, instead of attending to every key. We demonstrate Unlimiformers's efficacy on several long-document and multi-document summarization benchmarks, showing that it can summarize even 350k token-long inputs from the BookSum dataset, without any input truncation at test time. Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned
    
[^103]: 神经网络中的非确定性栈

    Nondeterministic Stacks in Neural Networks. (arXiv:2304.12955v1 [cs.CL])

    [http://arxiv.org/abs/2304.12955](http://arxiv.org/abs/2304.12955)

    本论文提出在神经网络中添加了可以处理句法歧义的非确定性栈，有效地模拟一个非确定性下推自动机。

    

    人类语言中充满了 组成性句法结构，尽管神经网络在处理语言的计算机系统方面做出了突破性的改进，但是广泛使用的神经网络体系结构在处理语法方面仍存在局限性。为了解决这个问题，之前的工作提出在神经网络中添加栈 数据结构，从语法和栈之间的理论关系中汲取灵感。然而，这些方法采用的是设计用于跟踪一个句法分析的确定性栈，而在语言中需要采用非确定性栈进行解析的句法歧义极其常见。在本论文中，我们通过提出一种将非确定性栈纳入到神经网络中的方法来解决这个差异。我们开发了一种可微分的数据结构，利用动态规划算法高效地模拟了一个非确定性下推自动机，表示一个指数级的计算数量。

    Human language is full of compositional syntactic structures, and although neural networks have contributed to groundbreaking improvements in computer systems that process language, widely-used neural network architectures still exhibit limitations in their ability to process syntax. To address this issue, prior work has proposed adding stack data structures to neural networks, drawing inspiration from theoretical connections between syntax and stacks. However, these methods employ deterministic stacks that are designed to track one parse at a time, whereas syntactic ambiguity, which requires a nondeterministic stack to parse, is extremely common in language. In this dissertation, we remedy this discrepancy by proposing a method of incorporating nondeterministic stacks into neural networks. We develop a differentiable data structure that efficiently simulates a nondeterministic pushdown automaton, representing an exponential number of computations with a dynamic programming algorithm. 
    
[^104]: Eyettention：基于注意力机制的双序列模型以预测人类阅读时的扫视路径

    Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading. (arXiv:2304.10784v1 [cs.CL])

    [http://arxiv.org/abs/2304.10784](http://arxiv.org/abs/2304.10784)

    Eyettention是第一个同时处理语言序列和时间序列的阅读模型，可以更准确地模拟阅读者的扫视路径，对机器学习的自然语言处理模型具有借鉴意义。

    

    阅读时的眼动揭示了阅读者的认知过程和所阅读文本的特征。因此，阅读中扫视路径的分析已引起各个领域的关注，涵盖了从认知科学到语言学和计算机科学。然而，模拟阅读时人类的扫视路径的主要挑战在于它们是由双序列组成的：单词按照语言的语法规则排序，而注视则按照时间顺序排序。人类并不严格按左到右的顺序阅读，而是跳过或重复注视单词，并倒退到以前的单词，语言序列和时间序列的对齐并不容易。本文开发了Eyettention，这是第一个同时处理语言序列和时间序列的双序列模型。

    Eye movements during reading offer insights into both the reader's cognitive processes and the characteristics of the text that is being read. Hence, the analysis of scanpaths in reading have attracted increasing attention across fields, ranging from cognitive science over linguistics to computer science. In particular, eye-tracking-while-reading data has been argued to bear the potential to make machine-learning-based language models exhibit a more human-like linguistic behavior. However, one of the main challenges in modeling human scanpaths in reading is their dual-sequence nature: the words are ordered following the grammatical rules of the language, whereas the fixations are chronologically ordered. As humans do not strictly read from left-to-right, but rather skip or refixate words and regress to previous words, the alignment of the linguistic and the temporal sequence is non-trivial. In this paper, we develop Eyettention, the first dual-sequence model that simultaneously process
    
[^105]: Multi-step Jailbreaking Privacy Attacks on ChatGPT

    Multi-step Jailbreaking Privacy Attacks on ChatGPT. (arXiv:2304.05197v1 [cs.CL])

    [http://arxiv.org/abs/2304.05197](http://arxiv.org/abs/2304.05197)

    本文探讨了来自OpenAI模型API以及New Bing增强版的ChatGPT对隐私带来的风险，指出应用程序集成的LLMs可能导致比以往更严重的隐私威胁，实验支持该观点。

    

    随着大型语言模型（LLMs）的迅速发展，许多下游NLP任务可以通过良好的提示得到很好的解决。尽管模型开发人员和研究人员努力确保避免从LLMs生成有害内容，但仍然难以引导AI生成的内容（AIGC）为人类带来好处。由于强大的LLMs正在吞噬来自各个领域的现有文本数据（例如，GPT-3训练了45TB的文本），因此人们自然会怀疑训练数据中是否包含私人信息以及这些LLMs及其下游应用程序可以带来什么隐私威胁。在本文中，我们研究了来自OpenAI的模型API和通过ChatGPT增强的New Bing所带来的隐私威胁，并显示应用程序集成的LLMs可能导致比以往更严重的隐私威胁。为此，我们进行了大量实验证明我们的说法，并讨论LLMs的隐私影响。

    With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given good prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI's model APIs and New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause more severe privacy threats ever than before. To this end, we conduct extensive experiments to support our claims and discuss LLMs' privacy implications.
    
[^106]: 社会文化知识在仇恨言论检测任务中对选项的选择是必要的

    Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])

    [http://arxiv.org/abs/2304.01890](http://arxiv.org/abs/2304.01890)

    HATELEXICON是一个包含巴西，德国，印度和肯尼亚仇恨言论的词汇表，利用其可以提高模型在训练中的性能表现。

    

    我们引入了HATELEXICON，这是一个包含巴西，德国，印度和肯尼亚的蔑称和仇恨言论目标的词汇表，以帮助模型的训练和可解释性。我们展示了我们的词汇表如何用于解释模型预测，表明发展用于分类极端言论的模型，在进行预测时严重依赖目标词。此外，我们提出了一种通过HATELEXICON来辅助低资源环境下训练选项的方法，选项选择在小样本学习中尤为重要。在我们的工作中，我们使用HASOC数据对德语和印地语进行了几个示范学习，并将Multilingual HateCheck（MHC）作为基准。我们展示了根据我们的词汇表选择样本，相对于随机采样的模型，能够更好地在MHC上表现。因此，当仅有少量的训练样本时，使用我们的词汇表来选择包含更多社会文化信息的样本能够更好地提高在仇恨言论检测任务中的性能。

    We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
    
[^107]: 编译神经网络的语言模型：神经理解

    Neural Comprehension: Language Models with Compiled Neural Networks. (arXiv:2304.01665v1 [cs.CL])

    [http://arxiv.org/abs/2304.01665](http://arxiv.org/abs/2304.01665)

    本文探讨了如何将编译神经网络CoNNs并入语言模型的架构中，以使语言模型在复合任务中提高性能，特别是在需要深入理解抽象规则的领域。方法称为“神经理解”，提高了语言模型在符号操作、规则推理、算术推理等方面的准确度。

    

    语言模型在自然语言处理任务中取得了令人瞩目的成果，但其进行符号操作和算术操作的能力仍然有限，这归因于它们隐式地从数据中学习规则。我们探讨如何将特别设计得到的加权的编译神经网络（CoNNs）并入语言模型的架构中，使得通过梯度训练的语言模型获得完全的规则理解能力。编译神经网络的并入为改善语言模型在复合任务中的性能提供了一个有前途的方向，特别是在需要深入理解抽象规则的领域。我们的方法称为“神经理解”，有助于语言模型在符号操作、规则推理、算术推理等方面实现绝对准确度。我们的代码公开可用：\url{https://github.com/...}

    Language models have achieved impressive results in natural language processing tasks, but their ability to perform symbolic operations and arithmetic operations, remains limited, which attribute to their learn the rules implicitly from data. We explore how to incorporate compiled neural networks (CoNNs) which weight is specially designed, into the architecture of language models to enable the language model trained by gradient to obtain fully rule comprehension ability. The incorporation of compiled neural networks offers a promising direction for improving the performance of language models on compound tasks, particularly in areas that require a deeper comprehension of abstract rules beyond recognizing patterns in training data. Our method, which call "Neural Comprehension", helps language models achieve absolute accuracy in symbolic operations, thereby enhancing their ability for rule reasoning, symbolic reasoning, and arithmetic reasoning. Our code is publicly available at: \url{ht
    
[^108]: 语义解析在理解过程文本中的作用

    The Role of Semantic Parsing in Understanding Procedural Text. (arXiv:2302.06829v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.06829](http://arxiv.org/abs/2302.06829)

    本文探究了通过语义解析获取的符号语义表示对于推理实体状态的作用，并提出了基于符号解析和语义解析信息的过程推理框架，实验结果表明融合语义知识可提高过程理解能力。

    

    本文研究了从深度语义分析器中提取的符号语义表示是否有助于推理所涉及实体的状态。我们考虑TRIPS（一种深度语义分析器）和语义角色标记作为语义解析的两个来源。首先，我们提出了一个基于符号解析的过程推理框架PROPOLIS。其次，我们将语义解析信息集成到最先进的神经模型中进行过程推理。我们的实验表明，明确地融合这种语义知识可以提高过程理解能力。本文提出了用于评估过程推理任务的新指标，阐明了挑战并识别了神经、符号和集成模型之间的差异。

    In this paper, we investigate whether symbolic semantic representations, extracted from deep semantic parsers, can help reasoning over the states of involved entities in a procedural text. We consider a deep semantic parser~(TRIPS) and semantic role labeling as two sources of semantic parsing knowledge. First, we propose PROPOLIS, a symbolic parsing-based procedural reasoning framework. Second, we integrate semantic parsing information into state-of-the-art neural models to conduct procedural reasoning. Our experiments indicate that explicitly incorporating such semantic knowledge improves procedural understanding. This paper presents new metrics for evaluating procedural reasoning tasks that clarify the challenges and identify differences among neural, symbolic, and integrated models.
    
[^109]: 通过多任务语言建模统一分子和文本表示

    Unifying Molecular and Textual Representations via Multi-task Language Modelling. (arXiv:2301.12586v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12586](http://arxiv.org/abs/2301.12586)

    本文提出了第一个多任务语言模型，可以同时解决化学和自然语言领域的各种任务，无需昂贵的单一域或任务特定模型的预训练。

    

    神经语言模型的最新进展已成功应用于化学领域，通过为分子设计和合成规划提供生成式解决方案。这些新方法有潜力推动数据驱动的科学发现的新时代。然而，每个任务仍然需要专门的模型，导致需要特定问题的微调，并忽视任务之间的关系。该领域的主要障碍是自然语言和化学表示之间缺乏统一表示，从而使人机交互变得复杂和有限。本文提出了第一个多域、多任务语言模型，可以同时解决化学和自然语言领域的各种任务。我们的模型可以同时处理化学和自然语言，无需昂贵的单一域或任务特定模型的预训练。有趣的是，在领域之间共享权重会在所有任务上产生更好的性能。我们的实验表明，我们的模型可以成功解决各种任务，包括分子生成、反合成预测、化学命名实体识别和文本摘要。

    The recent advances in neural language models have also been successfully applied to the field of chemistry, offering generative solutions for classical problems in molecular design and synthesis planning. These new methods have the potential to fuel a new era of data-driven automation in scientific discovery. However, specialized models are still typically required for each task, leading to the need for problem-specific fine-tuning and neglecting task interrelations. The main obstacle in this field is the lack of a unified representation between natural language and chemical representations, complicating and limiting human-machine interaction. Here, we propose the first multi-domain, multi-task language model that can solve a wide range of tasks in both the chemical and natural language domains. Our model can handle chemical and natural language concurrently, without requiring expensive pre-training on single domains or task-specific models. Interestingly, sharing weights across domai
    
[^110]: 基于语言模型的案例推理在逻辑谬误分类中的应用

    Case-Based Reasoning with Language Models for Classification of Logical Fallacies. (arXiv:2301.11879v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.11879](http://arxiv.org/abs/2301.11879)

    本文提出了一种基于案例推理的方法用于分类逻辑谬误的新案例，通过基于语言建模的检索和历史案例的调整来提高语言模型的准确性和泛化能力。

    

    在网络上传播虚假信息和宣传的容易和快捷性促使我们需要开发可靠的技术来检测自然语言论证中的谬误。然而，现有的语言建模方法在需要复杂推理的逻辑谬误分类等任务上表现出缺乏鲁棒性的特点。本文提出了一种基于案例推理的方法，该方法通过基于语言建模的检索和历史案例的调整来分类逻辑谬误的新案例。我们设计了四种互补的策略，基于有关目标、解释、反驳和论证结构的外部信息来丰富我们模型的输入表示。我们在领域内和领域外的实验表明，基于案例推理可以提高语言模型的准确性和泛化能力。我们的消融研究表明，类似案例的表示对模型性能有很大的影响，较少的历史案例也能使模型有良好性能。

    The ease and speed of spreading misinformation and propaganda on the Web motivate the need to develop trustworthy technology for detecting fallacies in natural language arguments. However, state-of-the-art language modeling methods exhibit a lack of robustness on tasks like logical fallacy classification that require complex reasoning. In this paper, we propose a Case-Based Reasoning method that classifies new cases of logical fallacy by language-modeling-driven retrieval and adaptation of historical cases. We design four complementary strategies to enrich input representation for our model, based on external information about goals, explanations, counterarguments, and argument structure. Our experiments in in-domain and out-of-domain settings indicate that Case-Based Reasoning improves the accuracy and generalizability of language models. Our ablation studies suggest that representations of similar cases have a strong impact on the model performance, that models perform well with fewe
    
[^111]: 领域无关的分子生成与自我反馈

    Domain-Agnostic Molecular Generation with Self-feedback. (arXiv:2301.11259v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11259](http://arxiv.org/abs/2301.11259)

    MolGen是一个专注于分子生成的预训练语言模型，使用了领域无关的分子前缀调整和自我反馈的范式，实现了化学有效性、多样性、新颖性和复杂性的突破，在分子生成领域表现出了出色的性能。

    

    分子的生成已经受到极大的关注，其革新了科学家设计分子结构的方式，并为化学和药物设计提供了宝贵的支持。然而，尽管在分子生成中使用语言模型具有潜力，但它们面临着许多挑战，比如生成语法或化学存在缺陷的分子，狭窄的领域专注以及由于缺乏注释数据或外部分子数据库而限制了生成多样性和可行性。因此，我们引入了MolGen，它是一个专门用于分子生成的预训练分子语言模型。MolGen通过重构一亿多个分子SELFIES获得了固有的结构和语法概念，并通过领域无关的分子前缀调整促进了不同领域之间的知识传递。此外，我们提出了一种自我反馈范式，启发预训练模型与最终下游目标对齐，有助于更稳健和高效的分子生成。我们在基准数据集上的实验表明，MolGen在化学有效性，多样性，新颖性和复杂性方面优于现有技术。

    The generation of molecules with desired properties has gained tremendous popularity, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face numerous challenges such as the generation of syntactically or chemically flawed molecules, narrow domain focus, and limitations in creating diverse and directionally feasible molecules due to a dearth of annotated data or external molecular databases. To this end, we introduce MolGen, a pre-trained molecular language model tailored specifically for molecule generation. MolGen acquires intrinsic structural and grammatical insights by reconstructing over 100 million molecular SELFIES, while facilitating knowledge transfer between different domains through domain-agnostic molecular prefix tuning. Moreover, we present a self-feedback paradigm that inspires the pre-trained model to align with the ulti
    
[^112]: ClarifyDelphi：针对社会和道德情境的强化澄清问题与优先考虑对抗的奖励

    ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations. (arXiv:2212.10409v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10409](http://arxiv.org/abs/2212.10409)

    ClarifyDelphi是一个交互式系统，能够针对社会或道德情境提出最有信息价值的问题，并通过奖励机制最大化回答问题时的道德判断分歧。

    

    上下文的重要性不言而喻，甚至在常识道德推理中也是如此。改变上下文可能会颠倒一项行为的道德判断;“对朋友撒谎”在一般情况下是不对的，但如果旨在保护他们的生命，就可能是道德上可接受的。我们提出了ClarifyDelphi，一个交互式系统，它学习提出澄清问题（例如，你为什么要对你的朋友撒谎？）以获取社会或道德情境的其他重要信息。我们认为，其潜在答案导致道德判断有所分歧的问题是最有信息价值的。因此，我们提出了一种增强学习框架，该框架具有对抗性奖励，旨在最大化回答问题时的道德判断分歧。人类评估表明，与竞争基线相比，我们的系统生成的问题更相关、更有信息价值和更具优胜性。我们的工作最终受到认知科学研究的启发，该研究调查了道德认知的灵活性（即能够纳入新的上下文信息并相应地修改道德判断）。

    Context is everything, even in commonsense moral reasoning. Changing contexts can flip the moral judgment of an action; "Lying to a friend" is wrong in general, but may be morally acceptable if it is intended to protect their life.  We present ClarifyDelphi, an interactive system that learns to ask clarification questions (e.g., why did you lie to your friend?) in order to elicit additional salient contexts of a social or moral situation. We posit that questions whose potential answers lead to diverging moral judgments are the most informative. Thus, we propose a reinforcement learning framework with a defeasibility reward that aims to maximize the divergence between moral judgments of hypothetical answers to a question. Human evaluation demonstrates that our system generates more relevant, informative and defeasible questions compared to competitive baselines. Our work is ultimately inspired by studies in cognitive science that have investigated the flexibility in moral cognition (i.e
    
[^113]: 基于渐进式自注意力剪枝的预训练语言模型压缩

    Gradient-based Intra-attention Pruning on Pre-trained Language Models. (arXiv:2212.07634v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.07634](http://arxiv.org/abs/2212.07634)

    本文提出了一种基于渐进式自注意力剪枝的压缩预训练语言模型方法GRAIN，通过执行任务特定的剪枝和知识蒸馏，可以得到高效的模型，并在GLUE、SQuAD和CoNLL 2003等任务中表现优异。

    

    预训练语言模型在性能上表现卓越，但计算代价巨大。为此，人们开发了剪枝和知识蒸馏等技术来减小模型的体积和延迟。本文提出了一种有结构的剪枝方法GRAIN（基于渐进式自注意力剪枝），通过知识蒸馏执行任务特定剪枝，可以得到高效的模型。与通常剪枝每个注意力头的整体不同，GRAIN检查和剪枝内部自注意结构，极大扩展了结构搜索空间，使模型更加灵活。我们还提出了梯度分离策略，以减少蒸馏对剪枝的干扰，更好地结合这两种方法。在GLUE、SQuAD和CoNLL 2003上的实验表明，GRAIN显着优于其他方法，特别是在高稀疏度条件下，并且在保持$93\%\sim99\%$性能的同时实现了$6\sim7\times$的加速。在极端条件下，GRAIN仍能保持与高性能LSTM相同的数量级。

    Pre-trained language models achieve superior performance but are computationally expensive. Techniques such as pruning and knowledge distillation have been developed to reduce their sizes and latencies. In this work, we propose a structured pruning method GRAIN (Gradient-based Intra-attention pruning), which performs task-specific pruning with knowledge distillation and yields highly effective models. Different from common approaches that prune each attention head as a whole, GRAIN inspects and prunes intra-attention structures, which greatly expands the structure search space and enables more flexible models. We also propose a gradient separation strategy that reduces the interference of distillation on pruning for a better combination of the two approaches. Experiments on GLUE, SQuAD, and CoNLL 2003 show that GRAIN notably outperforms other methods, especially in the high sparsity regime, and achieves $6\sim7\times$ speedups while maintaining $93\%\sim99\%$ performance. Under extreme
    
[^114]: 从Clozing到理解：将预训练的遮蔽语言模型改造为预训练的机器阅读器

    From Clozing to Comprehending: Retrofitting Pre-trained Masked Language Model to Pre-trained Machine Reader. (arXiv:2212.04755v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.04755](http://arxiv.org/abs/2212.04755)

    本文提出了一种无需标记数据的新方法，能够将预训练的遮蔽语言模型改造为预训练的机器阅读理解模型，解决了现有模型预训练与下游微调之间的差异化问题。PMR 在多个基准数据集上表现优秀，能有效提高预测可解释性。

    

    本文提出了一种名为“Pre-trained Machine Reader (PMR)”的新方法，用于将预训练的遮蔽语言模型 (MLMs) 改造为预训练的机器阅读理解 (MRC) 模型，无需获取标记数据。通过使用 Wikipedia 超链接构建了大量通用且高质量的 MRC 风格训练数据，并设计了一个 Wiki Anchor Extraction 任务来指导 MRC 风格的预训练，从而解决了现有 MLMs 模型预训练与下游微调之间的差异化问题。除了简单易用，PMR 还能有效解决一些如抽取式问答和命名实体识别等任务。PMR 在现有方法方面显示了巨大的改进，特别是在低资源环境下。当应用于 MRC 公式中的序列分类任务时，PMR 能够提取高质量的证明材料来解释分类过程，从而提供更好的预测可解释性。PMR 在多个基准数据集上也比现有的基于预训练的模型表现更优。

    We present Pre-trained Machine Reader (PMR), a novel method for retrofitting pre-trained masked language models (MLMs) to pre-trained machine reading comprehension (MRC) models without acquiring labeled data. PMR can resolve the discrepancy between model pre-training and downstream fine-tuning of existing MLMs. To build the proposed PMR, we constructed a large volume of general-purpose and high-quality MRC-style training data by using Wikipedia hyperlinks and designed a Wiki Anchor Extraction task to guide the MRC-style pre-training. Apart from its simplicity, PMR effectively solves extraction tasks, such as Extractive Question Answering and Named Entity Recognition. PMR shows tremendous improvements over existing approaches, especially in low-resource scenarios. When applied to the sequence classification task in the MRC formulation, PMR enables the extraction of high-quality rationales to explain the classification process, thereby providing greater prediction explainability. PMR als
    
[^115]: DC-MBR：最小贝叶斯风险解码的分布式冷却

    DC-MBR: Distributional Cooling for Minimum Bayesian Risk Decoding. (arXiv:2212.04205v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.04205](http://arxiv.org/abs/2212.04205)

    本研究提出了分布式冷却MBR（DC-MBR）的方法来解决标签平滑对最小贝叶斯风险解码（MBR）造成的自回归过度平滑性问题，该方法通过调整softmax温度来操纵输出分布的熵。最终，DC-MBR在神经机器翻译中表现出众，相较于现有基线持续优化。

    

    最小贝叶斯风险解码(MBR)在神经机器翻译中表现出众，然而，在标签平滑的情况下，MBR的表现并不理想。令人惊讶的是，标签平滑在束搜索中提供了不错的改进，并在各种任务中提高了通用性。我们展示了问题出现在标签平滑在令牌级和序列级分布上不一致。我们证明了即使标签平滑只在令牌级上造成轻微变化，序列级分布也会高度倾斜。我们将此问题称为“自回归过度平滑性”。为了解决这个问题，我们提出了一种简单而有效的方法，分布式冷却MBR（DC-MBR），通过调整softmax温度来操纵输出分布的熵。我们在理论上证明了预调节标签平滑因子和分布式冷却之间的等价性。在NMT基准测试中进行的广泛实验验证了分布式冷却显著减轻了过度平滑问题并持续优于现有基线。

    Minimum Bayesian Risk Decoding (MBR) emerges as a promising decoding algorithm in Neural Machine Translation. However, MBR performs poorly with label smoothing, which is surprising as label smoothing provides decent improvement with beam search and improves generality in various tasks. In this work, we show that the issue arises from the un-consistency of label smoothing on the token-level and sequence-level distributions. We demonstrate that even though label smoothing only causes a slight change in the token-level, the sequence-level distribution is highly skewed. We coin the issue \emph{autoregressive over-smoothness}. To address this issue, we propose a simple and effective method, Distributional Cooling MBR (DC-MBR), which manipulates the entropy of output distributions by tuning down the Softmax temperature. We theoretically prove the equivalence between pre-tuning label smoothing factor and distributional cooling. Extensive experiments on NMT benchmarks validate that distributio
    
[^116]: 把推理能力压缩到更小的语言模型中

    Distilling Reasoning Capabilities into Smaller Language Models. (arXiv:2212.00193v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.00193](http://arxiv.org/abs/2212.00193)

    本文提出了一种知识蒸馏方法，可以把大型语言模型的逐步推理能力蒸馏到更小的模型中，提出了一种替代推理方案，使用苏格拉底式CoT来训练两个小型蒸馏模型的组合，可以用来分解和解决复杂的问题，且在多个推理数据集上表现出高精度的复杂推理能力，经常优于那些没有经过CoT推理方法训练的大模型。

    

    逐步推理的方法（如CoT）在具有推理能力的大型语言模型中被证明非常有效。然而，CoT方法的成功基本上是与模型大小密切相关的，并且通常需要十亿级参数规模的模型才能使CoT工作。在本文中，我们提出了一种知识蒸馏方法，利用较大模型的逐步CoT推理能力，并将这些能力蒸馏到更小的模型中。在这项工作中，我们提出了一种替代推理方案：苏格拉底式CoT，它学习将原始问题分解为一系列子问题，并用它来指导中间推理步骤。我们使用苏格拉底式CoT来训练两个小型蒸馏模型的组合：问题分解器和子问题求解器。在实践中，给定一个新问题，这两个蒸馏模型以同步的方式工作，以分解和解决复杂的问题。在多个推理数据集（GSM8K，StrategyQA和SVAMP）上，我们展示了我们的蒸馏模型学会了高精度地执行复杂的推理任务，通常优于没有专门使用CoT推理方法进行训练的大型模型。

    Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models.  In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP
    
[^117]: 什么是上下文学习算法？使用线性模型进行调查

    What learning algorithm is in-context learning? Investigations with linear models. (arXiv:2211.15661v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15661](http://arxiv.org/abs/2211.15661)

    研究者提出了一种假设，即基于转换器的上下文学习器可以隐含地编码学习算法，并根据上下文中出现的新示例更新这些隐式模型。通过构造和比较性质证明了这个假设，并提出了一种度量学习器预测器和从显式学习算法中获得的预测器之间相似程度的方法。

    

    神经序列模型，特别是转换器，展现了一种非凡的上下文学习能力。它们可以在输入中呈现的标记示例序列$(x,f(x))$构建新的预测器，而无需进一步的参数更新。我们调查假设：基于转换器的上下文学习器通过在其激活中编码较小的模型并根据上下文中出现的新示例更新这些隐式模型，实现了标准的学习算法。以线性回归作为原型问题，我们提供了三条这个假设的证据来源。首先，我们通过构造证明了转换器可以在梯度下降和闭形式的岭回归的基础上实现线性模型的学习算法。其次，我们展示了通过上下文学习训练出来的学习器与梯度下降、岭回归以及精确最小二乘回归所计算的预测器非常相似，在转换器的深度和数据集的噪声水平变化时，能够在不同的预测器之间进行转换。第三，我们提出了一种度量学习器预测器和从显式学习算法中获得的预测器之间相似程度的方法。这种度量表明，上下文学习可以隐含地编码学习算法，并在问题和上下文中根据需要进行切换。

    Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples $(x, f(x))$ presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noi
    
[^118]: Deanthropomorphising NLP：语言模型可以意识到吗？

    Deanthropomorphising NLP: Can a Language Model Be Conscious?. (arXiv:2211.11483v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.11483](http://arxiv.org/abs/2211.11483)

    本文讨论了关于使用Transformer架构的预训练语言模型LaMDA是否具有意识的说法。作者认为语言模型不可能具有意识，而LaMDA没有比其他类似模型更具先进性。

    

    本文旨在对最近有关使用Transformer模型架构的预训练语言模型LaMDA具有意识的说法进行讨论。我们认为这样的语言模型不可能具有意识，而LaMDA并没有比其他类似模型更具先进性。我们通过综合信息理论对Transformer架构进行分析来证明这一点。我们认为这些有意识的说法是NLP报道中使用拟人化语言的更广泛倾向的一部分。无论这些说法的真实性如何，我们认为现在是评估语言建模进展并考虑该任务的伦理影响的适当时机。为了使本文有助于NLP社区以外的读者，我们还提供了一些NLP基础知识的介绍。

    This work is intended as a voice in the discussion over the recent claims that LaMDA, a pretrained language model based on the Transformer model architecture, is sentient. This claim, if confirmed, would have serious ramifications in the Natural Language Processing (NLP) community due to wide-spread use of similar models. However, here we take the position that such a language model cannot be sentient, or conscious, and that LaMDA in particular exhibits no advances over other similar models that would qualify it. We justify this by analysing the Transformer architecture through Integrated Information Theory. We see the claims of consciousness as part of a wider tendency to use anthropomorphic language in NLP reporting. Regardless of the veracity of the claims, we consider this an opportune moment to take stock of progress in language modelling and consider the ethical implications of the task. In order to make this work helpful for readers outside the NLP community, we also present the
    
[^119]: SLICER: 利用低资源自监督预训练学习通用音频表示

    SLICER: Learning universal audio representations using low-resource self-supervised pre-training. (arXiv:2211.01519v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2211.01519](http://arxiv.org/abs/2211.01519)

    本文提出了一种名为SLICER的自监督学习方法，通过聚类和对比学习相结合的方式进行编码器预训练，从而得到可以广泛适用于语音和非语音任务的音频表示。

    

    我们提出了一种新的自监督学习方法，对未标记的音频数据进行编码器预训练，从而减少音频和语音分类所需的大量标记数据。我们的主要目标是在低资源未标记的音频预训练环境中学习可以概括大量语音和非语音任务的音频表示。受到聚类和对比学习范式在基于自监督学习的语音表示学习中的最近成功启发，我们提出了SLICER（实例和聚类级别高效表征的对称学习），将聚类和对比学习范式的优点结合起来。我们使用学生和教师编码器之间潜在表示之间的对称损失，并同时解决实例和聚类级别的对比学习任务。我们通过将输入的频谱图投影到与聚类数目相同的输出子空间中来在线获得聚类表示。

    We present a new Self-Supervised Learning (SSL) approach to pre-train encoders on unlabeled audio data that reduces the need for large amounts of labeled data for audio and speech classification. Our primary aim is to learn audio representations that can generalize across a large variety of speech and non-speech tasks in a low-resource un-labeled audio pre-training setting. Inspired by the recent success of clustering and contrasting learning paradigms for SSL-based speech representation learning, we propose SLICER (Symmetrical Learning of Instance and Cluster-level Efficient Representations), which brings together the best of both clustering and contrasting learning paradigms. We use a symmetric loss between latent representations from student and teacher encoders and simultaneously solve instance and cluster-level contrastive learning tasks. We obtain cluster representations online by just projecting the input spectrogram into an output subspace with dimensions equal to the number of
    
[^120]: MAST:多尺度音频谱图变压器

    MAST: Multiscale Audio Spectrogram Transformers. (arXiv:2211.01515v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2211.01515](http://arxiv.org/abs/2211.01515)

    MAST是一种多尺度音频谱图变压器，引入了多尺度特征分层概念，同时扩展嵌入维度，降低时间分辨率，用于音频分类。通过金字塔结构实现早期层和深层的建模，扩展方法为SS-MAST。

    

    本文提出了一种用于音频分类的多尺度音频谱图变压器（MAST），将多尺度特征分层概念引入音频谱图变换器（AST）中。给定一个输入的音频谱图，我们首先将其裁剪成初步的时间分辨率和嵌入维度，随后MAST中的多个阶段逐渐扩展嵌入维度，同时降低输入的时间分辨率。我们使用金字塔结构，使得MAST的早期层在高时间分辨率但低嵌入空间下建模简单的低级声学信息，而较深的时间粗糙层则用高维嵌入来建模高级声学信息。我们还扩展了我们的方法，提出了一种新的自监督学习（SSL）方法，称为SS-MAST，它计算了一个对称的对比损失，利用patch-drop - 一种新的音频增强技术来自学习和教师编码器的潜在表示之间。

    We present Multiscale Audio Spectrogram Transformer (MAST) for audio classification, which brings the concept of multiscale feature hierarchies to the Audio Spectrogram Transformer (AST). Given an input audio spectrogram, we first patchify and project it into an initial temporal resolution and embedding dimension, post which the multiple stages in MAST progressively expand the embedding dimension while reducing the temporal resolution of the input. We use a pyramid structure that allows early layers of MAST operating at a high temporal resolution but low embedding space to model simple low-level acoustic information and deeper temporally coarse layers to model high-level acoustic information with high-dimensional embeddings. We also extend our approach to present a new Self-Supervised Learning (SSL) method called SS-MAST, which calculates a symmetric contrastive loss between latent representations from a student and a teacher encoder, leveraging patch-drop, a novel audio augmentation a
    
[^121]: SpeechBlender: 用于发音错误数据生成的语音增强框架

    SpeechBlender: Speech Augmentation Framework for Mispronunciation Data Generation. (arXiv:2211.00923v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.00923](http://arxiv.org/abs/2211.00923)

    SpeechBlender是一个用于生成发音错误的数据增强框架，在发音错误检测模型的音素级上获得了ASR相关的最新技术水平，具有更有效的样本。

    

    设计发音错误检测模型所面临的主要问题是缺乏标记的第二语言（L2）语音数据。我们提出了SpeechBlender - 这是一个用于生成发音错误的细粒度数据增强流水线，以克服这种数据稀缺性。SpeechBlender利用各种掩模以针对语音单元的不同区域，并在增强发音时使用混合因子以线性插值原始语音信号。这些掩模有助于平滑混合信号，产生比“剪切/粘贴”方法更有效的样本。我们的建议技术在与先前的最新技术[1]相比的ASR依赖性发音错误检测模型的音素级上达到了最新的技术水平，Speechocean762，Pearson相关系数（PCC）提高了2.0％。此外，与我们的基线相比，我们在音素级别上展示了5.0％的改进。我们还观察到，在阿拉伯语AraVoiceL2测试集上的F1得分提高了4.6％。

    The lack of labeled second language (L2) speech data is a major challenge in designing mispronunciation detection models. We introduce SpeechBlender - a fine-grained data augmentation pipeline for generating mispronunciation errors to overcome such data scarcity. The SpeechBlender utilizes varieties of masks to target different regions of phonetic units, and use the mixing factors to linearly interpolate raw speech signals while augmenting pronunciation. The masks facilitate smooth blending of the signals, generating more effective samples than the `Cut/Paste' method. Our proposed technique achieves state-of-the-art results, with Speechocean762, on ASR dependent mispronunciation detection models at phoneme level, with a 2.0% gain in Pearson Correlation Coefficient (PCC) compared to the previous state-of-the-art [1]. Additionally, we demonstrate a 5.0% improvement at the phoneme level compared to our baseline. We also observed a 4.6% increase in F1-score with Arabic AraVoiceL2 testset.
    
[^122]: 通过建模对等关系的数据增强方法来进行跨度识别任务

    PeerDA: Data Augmentation via Modeling Peer Relation for Span Identification Tasks. (arXiv:2210.08855v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.08855](http://arxiv.org/abs/2210.08855)

    本文提出了一种新的基于对等关系数据增强方法，称为PeerDA来进行跨度识别任务。PeerDA使用带有PR关系的跨度对作为训练的增强数据，防止模型过拟合表面上的跨度-类别映射，并推动模型利用跨度语义。实验表明，PeerDA在数据增强方面表现出色，显著优于现有的最先进数据增强方法。

    

    跨度识别的目标是从文本输入中识别出特定的文本跨度，并将其分类到预定义的类别中。本文首次探索了同类跨度的对等关系，即Peer (PR)关系，这种关系表明两个跨度是同一类别的实例并且具有相似的特征。提出了一种新颖的基于对等关系数据增强方法，称为PeerDA，使用带有PR关系的跨度对作为训练的增强数据。PeerDA有两个独特的优点：（1）有大量的PR跨度对可以用来增强训练数据；（2）增强的数据可以防止模型过拟合表面上的跨度-类别映射，并推动模型利用跨度语义。在七个领域的四种不同任务的十个数据集上的实验结果表明了PeerDA的有效性。尤其值得注意的是，PeerDA在数据增强方面表现出色，显著优于现有的最先进数据增强方法。

    Span identification aims at identifying specific text spans from text input and classifying them into pre-defined categories. Different from previous works that merely leverage the Subordinate (SUB) relation (i.e. if a span is an instance of a certain category) to train models, this paper for the first time explores the Peer (PR) relation, which indicates that two spans are instances of the same category and share similar features. Specifically, a novel Peer Data Augmentation (PeerDA) approach is proposed which employs span pairs with the PR relation as the augmentation data for training. PeerDA has two unique advantages: (1) There are a large number of PR span pairs for augmenting the training data. (2) The augmented data can prevent the trained model from over-fitting the superficial span-category mapping by pushing the model to leverage the span semantics. Experimental results on ten datasets over four diverse tasks across seven domains demonstrate the effectiveness of PeerDA. Notab
    
[^123]: 用信息论评估自由文本解释的可行性

    REV: Information-Theoretic Evaluation of Free-Text Rationales. (arXiv:2210.04982v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.04982](http://arxiv.org/abs/2210.04982)

    本论文提出了一种名为REV的度量，用于评估自由文本解释中新颖、与标签相关的信息的数量，通过信息论的角度进行研究。实验证明REV在评估解释-标签对方面的有效性，并且与人类直觉一致。

    

    生成自由文本解释是迈向可解释 NLP 的一个有前途的步骤，然而评估这样的解释仍然是一个挑战。现有的度量主要集中在测量解释和给定标签之间的关联性上。我们认为，理想的度量应该集中于解释中提供的新信息，这些信息在输入或标签中都没有提供。我们从信息论的角度使用条件V-信息（Hewitt et al。，2021）研究了这个研究问题。更具体地说，我们提出了一个名为REV（利用条件V-信息评估解释）的度量，用于量化理性中除了输入或标签中已有信息之外的新标签相关信息的数量。在涉及推理任务的四个基准测试中进行的实验证明了REV在评估解释-标签对方面的有效性，与现有的度量相比。我们进一步证明REV与人类直觉一致，而一些现有的度量则不一致。

    Generating free-text rationales is a promising step towards explainable NLP, yet evaluating such rationales remains a challenge. Existing metrics have mostly focused on measuring the association between the rationale and a given label. We argue that an ideal metric should focus on the new information uniquely provided in the rationale that is otherwise not provided in the input or the label. We investigate this research problem from an information-theoretic perspective using conditional V-information (Hewitt et al., 2021). More concretely, we propose a metric called REV (Rationale Evaluation with conditional V-information), to quantify the amount of new, label-relevant information in a rationale beyond the information already available in the input or the label. Experiments across four benchmarks with reasoning tasks, including chain-of-thought, demonstrate the effectiveness of REV in evaluating rationale-label pairs, compared to existing metrics. We further demonstrate REV is consiste
    
[^124]: 神经符号专家系统中基于事实库的逻辑推理的动态生成

    Dynamic Generation of Grounded Logical Explanations in a Neuro-Symbolic Expert System. (arXiv:2209.07662v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.07662](http://arxiv.org/abs/2209.07662)

    该研究提出一种新颖的方法，通过结合神经语言建模、引导生成和半参数密集检索，动态生成基于事实库的人类可解释的证明树，实现科学推理，并展现了强大的性能。

    

    我们提出了一个系统性推理的方法，可以产生基于事实库的人类可解释的证明树。我们的方法引发了经典的基于 Prolog 的推理引擎，其中我们通过结合神经语言建模、引导生成和半参数密集检索来替换手工制定的规则。我们通过一个新颖的系统 NELLIE 来演示这种方法，该系统动态地实例化可解释的推理规则，对自然语言语句的蕴含（去）组合进行捕捉和评分。这导致了强大的性能，在科学推理领域展示了如何逻辑地从经过人工验证的事实的组合中推导出答案的推理痕迹。

    We propose an approach for systematic reasoning that produces human interpretable proof trees grounded in a factbase. Our approach evokes classic Prolog-based inference engines, where we replace handcrafted rules by combining neural language modeling, guided generation, and semiparametric dense retrieval. We demonstrate this approach through a novel system, NELLIE, which dynamically instantiates interpretable inference rules that capture and score entailment (de)compositions over natural language statements. This leads to strong performance, as shown in the scientific reasoning domain, while also producing reasoning traces showing how answers derive logically from the composition of human-verified facts.
    
[^125]: 关于上下文无关语言和正则语言的交集问题

    On the Intersection of Context-Free and Regular Languages. (arXiv:2209.06809v2 [cs.FL] UPDATED)

    [http://arxiv.org/abs/2209.06809](http://arxiv.org/abs/2209.06809)

    本文提出了一个通用的构造，可处理具有 $\varepsilon$ -弧的有限状态自动机，其中正则语言由有限状态自动机指定，该构造不仅编码了结构，而且保留了原始构造的渐近大小。

    

    Bar-Hillel构造是形式语言理论中的一个经典结果。它通过一个简单的构造表明，上下文无关语言和正则语言的交集仍是上下文无关语言。在构造中，正则语言由有限状态自动机指定。但是，原始构造（Bar-Hillel等人，1961年）及其加权扩展（Nederhof和Satta，2003年）都无法处理具有 $\varepsilon$ -弧的有限状态自动机。虽然可以有效地从有限状态自动机中删除 $\varepsilon$ -弧而不改变语言，但这种操作会修改自动机的路径集合。我们提供了一个在期望自动机具有 $\varepsilon$ -弧时通用的构造，并进一步证明，我们的广义构造导致了一个编码了输入自动机和文法结构的文法，同时保留了原始构造的渐近大小。

    The Bar-Hillel construction is a classic result in formal language theory. It shows, by a simple construction, that the intersection of a context-free language and a regular language is itself context-free. In the construction, the regular language is specified by a finite-state automaton. However, neither the original construction (Bar-Hillel et al., 1961) nor its weighted extension (Nederhof and Satta, 2003) can handle finite-state automata with $\varepsilon$-arcs. While it is possible to remove $\varepsilon$-arcs from a finite-state automaton efficiently without modifying the language, such an operation modifies the automaton's set of paths. We give a construction that generalizes the Bar-Hillel in the case where the desired automaton has $\varepsilon$-arcs, and further prove that our generalized construction leads to a grammar that encodes the structure of both the input automaton and grammar while retaining the asymptotic size of the original construction.
    
[^126]: 排名增强的无监督句子表示学习

    Ranking-Enhanced Unsupervised Sentence Representation Learning. (arXiv:2209.04333v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.04333](http://arxiv.org/abs/2209.04333)

    提出一种新的无监督句子编码器RankEncoder，该编码器预测句子的语义向量时利用输入句子与外部语料库中其他句子以及输入句子本身的关系。该编码器相对于传统的无监督句子编码器更具有抗干扰攻击的鲁棒性，并且在语义文本基准数据集上取得了1.1%的绝对改进。

    

    无监督句子表示学习已经通过对比学习和数据增强方法（如dropout掩蔽）取得了进展。尽管取得了进展，但是句子编码器仍然受限于仅在预测语义向量时使用输入句子。在本文中，我们展示了句子的语义含义也由与输入句子相似的最近邻句子确定。基于这一发现，我们提出了一种新的无监督句子编码器RankEncoder。RankEncoder通过利用输入句子与外部语料库中其他句子以及输入句子本身的关系来预测其语义向量。我们评估了RankEncoder在语义文本基准数据集上的性能。实验结果表明，RankEncoder实现了80.07%的Spearman相关系数，在与先前最先进的性能相比较时取得了1.1%的绝对改进；RankEncoder适用于现有的无监督句子表示学习方法；RankEncoder比传统的无监督句子编码器更具有抗干扰攻击的鲁棒性。

    Unsupervised sentence representation learning has progressed through contrastive learning and data augmentation methods such as dropout masking. Despite this progress, sentence encoders are still limited to using only an input sentence when predicting its semantic vector. In this work, we show that the semantic meaning of a sentence is also determined by nearest-neighbor sentences that are similar to the input sentence. Based on this finding, we propose a novel unsupervised sentence encoder, RankEncoder. RankEncoder predicts the semantic vector of an input sentence by leveraging its relationship with other sentences in an external corpus, as well as the input sentence itself. We evaluate RankEncoder on semantic textual benchmark datasets. From the experimental results, we verify that 1) RankEncoder achieves 80.07% Spearman's correlation, a 1.1% absolute improvement compared to the previous state-of-the-art performance, 2) RankEncoder is universally applicable to existing unsupervised s
    
[^127]: Transformer配置与训练目标的研究

    A Study on Transformer Configuration and Training Objective. (arXiv:2205.10505v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.10505](http://arxiv.org/abs/2205.10505)

    本文提出了Bamboo配置策略，基于更深更窄的Transformer结构进行Masked自编码器训练，在图像和语言任务上取得了新的最先进结果。

    

    基于Transformer的模型在许多任务，特别是视觉和语言任务上都取得了令人印象深刻的结果。在许多模型训练情况下，通常采用传统的配置。本文重新审视了这些传统配置，通过理论分析和实验评估，提出了Bamboo的配置策略，该策略使用更深更窄的Transformer结构进行Masked自编码器训练，并取得了新的最先进结果。

    Transformer-based models have delivered impressive results on many tasks, particularly vision and language tasks. In many model training situations, conventional configurations are typically adopted. For example, we often set the base model with hidden dimensions (i.e. model width) to be 768 and the number of transformer layers (i.e. model depth) to be 12. In this paper, we revisit these conventional configurations. Through theoretical analysis and experimental evaluation, we show that the masked autoencoder is effective in alleviating the over-smoothing issue in deep transformer training. Based on this finding, we propose Bamboo, an idea of using deeper and narrower transformer configurations, for masked autoencoder training. On ImageNet, with such a simple change in configuration, re-designed model achieves 87.1% top-1 accuracy and outperforms SoTA models like MAE and BEiT. On language tasks, re-designed model outperforms BERT with default setting by 1.1 points on average, on GLUE da
    
[^128]: GAP: 一种面向图知识库到文本生成的图感知语言模型框架

    GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation. (arXiv:2204.06674v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.06674](http://arxiv.org/abs/2204.06674)

    本文提出了一种面向知识库到文本生成的图感知语言模型框架，通过将图感知元素融入预训练语言模型中，提出了掩码结构和新的类型编码器，超越了现有的最先进模型，并消除了额外预训练任务所带来的差距。

    

    最近，知识库到文本生成的改进是由于增加了辅助预训练任务来提高微调任务的性能。这些任务需要大量的计算资源，只提供了小幅改进。本文提出将图感知元素融入现有的预训练语言模型中，通过提出一个掩码结构来捕获邻域信息和一种新的类型编码器，添加图注意权重偏差，从而可以超越现有的最先进模型，并消除了额外预训练任务带来的差距。在两个知识库到文本的基准数据集上的实验证明，我们的模型是具有竞争力的，同时涉及更少的参数和没有额外的预训练任务。通过将问题制定为一个框架，我们可以交换各种提出的组件，并基于拓扑和类型信息解释基于知识库到文本的生成模型。

    Recent improvements in KG-to-text generation are due to additional auxiliary pre-training tasks designed to give the fine-tune task a boost in performance. These tasks require extensive computational resources while only suggesting marginal improvements. Here, we demonstrate that by fusing graph-aware elements into existing pre-trained language models, we are able to outperform state-of-the-art models and close the gap imposed by additional pre-training tasks. We do so by proposing a mask structure to capture neighborhood information and a novel type encoder that adds a bias to the graph-attention weights depending on the connection type. Experiments on two KG-to-text benchmark datasets show our models are competitive while involving fewer parameters and no additional pre-training tasks. By formulating the problem as a framework, we can interchange the various proposed components and begin interpreting KG-to-text generative models based on the topological and type information found in 
    
[^129]: 自监督预训练的表示对语音识别有用的因素分析

    Analyzing the factors affecting usefulness of Self-Supervised Pre-trained Representations for Speech Recognition. (arXiv:2203.16973v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.16973](http://arxiv.org/abs/2203.16973)

    本论文试图研究自监督预训练的语音表示对于低资源语音识别系统在领域、语言、数据集大小以及先前知识方面的影响，揭示了用于自监督预训练的数据对 ASR 系统性能的影响以及相似度和数据的数量等因素对其性能的影响。

    

    自监督学习（SSL）学习高级语音表示已成为在低资源环境中构建自动语音识别（ASR）系统的流行方法。然而，文献中通常假设大量未标记的同一领域或语言的数据可用于SSL预训练，我们认为这在实际情况下不可行。本文作为 Interspeech Gram Vaani ASR 挑战的一部分，试图研究领域、语言、数据集大小和其他上游预训练 SSL 数据方面对低资源下游 ASR 任务最终性能的影响。我们还在继续预训练范式的基础上研究了使用 SSL 训练的模型所拥有的先前知识的影响。广泛的实验和研究揭示了 ASR 系统的性能易受用于 SSL 预训练的数据的影响，并且它们的性能随着相似度和

    Self-supervised learning (SSL) to learn high-level speech representations has been a popular approach to building Automatic Speech Recognition (ASR) systems in low-resource settings. However, the common assumption made in literature is that a considerable amount of unlabeled data is available for the same domain or language that can be leveraged for SSL pre-training, which we acknowledge is not feasible in a real-world setting. In this paper, as part of the Interspeech Gram Vaani ASR challenge, we try to study the effect of domain, language, dataset size, and other aspects of our upstream pre-training SSL data on the final performance low-resource downstream ASR task. We also build on the continued pre-training paradigm to study the effect of prior knowledge possessed by models trained using SSL. Extensive experiments and studies reveal that the performance of ASR systems is susceptible to the data used for SSL pre-training. Their performance improves with an increase in similarity and
    

