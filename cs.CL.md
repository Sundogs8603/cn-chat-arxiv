# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [TidyBot: Personalized Robot Assistance with Large Language Models.](http://arxiv.org/abs/2305.05658) | 本文研究了使用机器人进行家庭清扫的个性化问题。通过使用大型语言模型少样本摘要能力，机器人可以学习用户的偏好并将其推广到未来的场景中，从而实现快速适应。 |
| [^2] | [Towards Building the Federated GPT: Federated Instruction Tuning.](http://arxiv.org/abs/2305.05644) | 本文提出了一种名为Federated Instruction Tuning (FedIT)的新方法，利用联邦学习（FL）模型对LLMs进行指令调整，以解决获取高质量指令数据的挑战，从而提高调整模型的通用性和效果。 |
| [^3] | [Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare.](http://arxiv.org/abs/2305.05640) | 本研究提出了一种在医疗保健领域构建面向实体的知识图谱的端到端表示学习方法 HEER，通过将领域特定的约束和特征纳入到图嵌入算法中，有效地改善了下游预测任务。 |
| [^4] | [An Exploration of Encoder-Decoder Approaches to Multi-Label Classification for Legal and Biomedical Text.](http://arxiv.org/abs/2305.05627) | 本研究探索了编码器-解码器方法在多标签分类中的应用，结果表明该方法在更复杂的数据集和标签粒度更细的标签方案上表现更佳，尤其是在非自回归的情况下使用最佳。 |
| [^5] | [The Case Records of ChatGPT: Language Models and Complex Clinical Questions.](http://arxiv.org/abs/2305.05609) | 本研究探究了GPT4和GPT3.5在复杂临床病例诊断中的表现，并表明人工智能语言模型在协助临床决策方面具有潜在的作用，尤其是在医学资源受限的情况下。 |
| [^6] | [DomainInv: Domain Invariant Fine Tuning and Adversarial Label Correction For QA Domain Adaptation.](http://arxiv.org/abs/2305.05589) | 本论文提出了面向QA领域适应的领域不变微调和对抗性标签校正的方法。更精确地说，本文的方法通过将目标域的表示转换为源域的表示，并利用源域的监督来进行训练，以实现无标签目标领域的领域适应。 |
| [^7] | [Large Language Models Humanize Technology.](http://arxiv.org/abs/2305.05576) | 大型语言模型展示了人性化技术的新能力，特别是在跨越语言、职业和可访问性分歧的人们中。它们通过解决三个机械化瓶颈：创建多样化和可访问的内容，学习复杂的数字工具，以及个性化机器学习算法。 |
| [^8] | [Exploiting Pseudo Image Captions for Multimodal Summarization.](http://arxiv.org/abs/2305.05496) | 本文研究了跨模态对比学习中（部分）误负样本的挑战，并提出了一种从更一般下界形式的指导下调节跨模态相似度的对比学习策略，以更精确地优化图像/文本锚定点和其负面文本/图像之间的互信息。 |
| [^9] | [MAUPQA: Massive Automatically-created Polish Question Answering Dataset.](http://arxiv.org/abs/2305.05486) | 本文介绍了如何自动收集弱标记数据集以帮助训练神经 passage 检索器，并通过发布 MAUPQA 数据集和 HerBERT-QA 神经 retriever，解决了手动标注数据集的困难，缺乏数据集的问题。 |
| [^10] | [Investigating the effect of sub-word segmentation on the performance of transformer language models.](http://arxiv.org/abs/2305.05480) | 本文研究使用单语词段算法StateMorph训练语言模型时，可以使模型更高效地收敛并获得更好的验证分数。 |
| [^11] | [Going beyond research datasets: Novel intent discovery in the industry setting.](http://arxiv.org/abs/2305.05474) | 本文提出了一种用于电子商务领域实时意图发现的新方法，通过对现实生活数据进行预训练和利用对话结构细化，性能提高了33pp。 |
| [^12] | [Beyond Good Intentions: Reporting the Research Landscape of NLP for Social Good.](http://arxiv.org/abs/2305.05471) | 本文介绍了NLP4SGPAPERS数据集，通过对解决社会问题的论文进行分类、可持续发展目标映射、任务及方法的确定，使用最先进的NLP模型在整个ACL文集上进行处理，提供了一个可视化工作区，展示了NLP4SG领域的全貌。 |
| [^13] | [What is the best recipe for character-level encoder-only modelling?.](http://arxiv.org/abs/2305.05461) | 本文通过对字符级别BERT类型模型的设计空间和各种预训练目标的比较，找到了构建和训练字符级别模型的最佳方法。最好的字符级别模型的性能优于在相同数据上使用相同设置训练的基于标记的模型，这表明字符级别模型已准备好更广泛的应用，但训练字符级模型的最佳方法仍然依赖于子词级别的标记。 |
| [^14] | [WikiWeb2M: A Page-Level Multimodal Wikipedia Dataset.](http://arxiv.org/abs/2305.05432) | WikiWeb2M是一个保留完整网页图像、文本和结构数据的多模态数据集，可用于网页描述生成、章节摘要和上下文图像字幕等任务。 |
| [^15] | [Estimating related words computationally using language model from the Mahabharata - an Indian epic.](http://arxiv.org/abs/2305.05420) | 本文提出了一种基于语言模型的方法，用于估计马哈巴拉塔中的相关词语。通过对文本进行预处理并构建语言模型，该方法可以有效地估计相关性分数和单词的共现概率，从而从马哈巴拉塔中提取出相关词汇。 |
| [^16] | [Large Language Models Need Holistically Thought in Medical Conversational QA.](http://arxiv.org/abs/2305.05410) | 本研究提出了一种Holistically Thought（HoT）方法，用于引导大型语言模型进行综合性思考，以在医疗对话问答中生成高质量的医学响应。 |
| [^17] | [Completeness, Recall, and Negation in Open-World Knowledge Bases: A Survey.](http://arxiv.org/abs/2305.05403) | 本调查讨论了在开放世界知识库中表达、提取和推断完整性、召回率和否定性信息的方法，以及面对不完整和不确定的知识时，从业者和研究人员应该如何处理这些情况。 |
| [^18] | [Consistent Text Categorization using Data Augmentation in e-Commerce.](http://arxiv.org/abs/2305.05402) | 本文提出了一种在电子商务中使用数据增强实现一致的文本分类的新框架，该框架旨在改进产品分类模型的一致性，同时保持其生产水平的性能。 |
| [^19] | [CaseEncoder: A Knowledge-enhanced Pre-trained Model for Legal Case Encoding.](http://arxiv.org/abs/2305.05393) | 本文提出了一种融合法律知识的预训练模型CaseEncoder，针对法律案例的特殊领域需求，CaseEncoder在数据采样和预训练阶段中都使用了法律知识，其中包括利用细粒度的法律条款信息引导正负样本的选择，以及设计了与相关法律案例的评判标准相一致的法律特定预训练任务，该模型在法律案例检索和法律问答任务上优于最先进的PLMs。 |
| [^20] | [COKE: A Cognitive Knowledge Graph for Machine Theory of Mind.](http://arxiv.org/abs/2305.05390) | COKE是一个机器心智理论的认知知识图谱，将ToM形式化为一组经手动验证的认知链，可以帮助AI系统在社交智能等任务上具备推理人类心智的能力。 |
| [^21] | [Code Execution with Pre-trained Language Models.](http://arxiv.org/abs/2305.05383) | 本文研究了预训练语言模型对代码执行的能力，并通过开发一种变异数据增强技术创建了一个大规模的Python数据集和任务，提出了CodeExecutor模型以增强语义理解，该模型在代码执行、零-shot代码到代码搜索和文本到代码生成等方面有潜在好处。 |
| [^22] | [PLM-GNN: A Webpage Classification Method based on Joint Pre-trained Language Model and Graph Neural Network.](http://arxiv.org/abs/2305.05378) | 通过预训练语言模型和图神经网络的联合编码实现了基于文本和HTML DOM树的网页分类方法PLM-GNN，表现优秀。 |
| [^23] | [Large Language Model Programs.](http://arxiv.org/abs/2305.05364) | 本文提出了一种将大型预训练语言模型嵌入算法或程序中，扩展其能力的方法。这种方法可以在未经微调的情况下通过更具算法性的方法获得不错的性能提升。 |
| [^24] | [A Framework for Designing Foundation Model based Systems.](http://arxiv.org/abs/2305.05352) | 本文提出了一个基于基础模型的系统分类体系，分类和比较了基础模型和基于基础模型的系统的特点。它为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。 |
| [^25] | [Rudolf Christoph Eucken at SemEval-2023 Task 4: An Ensemble Approach for Identifying Human Values from Arguments.](http://arxiv.org/abs/2305.05335) | 本文提出了集合方法，利用三种模型从论据文本中检测人类价值，最佳组合在主要数据集上实现了总体F1分数为0.48的效果。 |
| [^26] | [Explainable Recommender with Geometric Information Bottleneck.](http://arxiv.org/abs/2305.05331) | 该论文提出了一种新的可解释推荐系统模型，将从用户-商品交互中学得的几何先验知识与变分网络相结合，可以为用户提供既具备推荐性能又具有解释性能的解释推荐服务。 |
| [^27] | [Detection of depression on social networks using transformers and ensembles.](http://arxiv.org/abs/2305.05325) | 本文利用transformer和集成技术，在社交网络上检测抑郁症的迹象，构建了多个基于预训练语言模型的分类器和两种类型的集成模型，表现更好。 |
| [^28] | [Structured Sentiment Analysis as Transition-based Dependency Parsing.](http://arxiv.org/abs/2305.05311) | 本文提出了第一种将结构化情感分析作为依存句法分析处理的基于转移的方法，其基于Pointer Network体系结构，实现了在准确性和效率方面均优于以前提出的图形模型的结果，是迄今为止最为准确的SSA方法。 |
| [^29] | [The Perfect Victim: Computational Analysis of Judicial Attitudes towards Victims of Sexual Violence.](http://arxiv.org/abs/2305.05302) | 这项研究使用计算模型和手动注释的数据集，评估以色列法院系统对性暴力受害者的司法态度。研究重点关注了“强奸神话”在对受害者可信度的司法评估中的作用。 |
| [^30] | [Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially Code-Switched Data.](http://arxiv.org/abs/2305.05295) | 研究者提出训练排名模型的方法来提高跨语言检索的效率，该模型使用了人工代码切换的数据，并且实验表明在跨语言检索和多语言检索中会带来显著改进，在不影响单语检索的基础上，特别是对于远程语言之间的检索。 |
| [^31] | [Dialogue Planning via Brownian Bridge Stochastic Process for Goal-directed Proactive Dialogue.](http://arxiv.org/abs/2305.05290) | 该论文提出了一种利用随机过程对话规划的方法，该方法通过布朗桥过程建模对话路径的时间动态以实现目标导向型对话系统并取得了良好的效果。 |
| [^32] | [VCSUM: A Versatile Chinese Meeting Summarization Dataset.](http://arxiv.org/abs/2305.05280) | 介绍了一个多功能的中文会议摘要数据集VCSum，包括239个现实生活中的会议，总时长超过230小时。该数据集可以适应各种摘要任务或方法，包括基于分割的摘要、多粒度摘要和检索-生成摘要。数据集和代码将在GitHub上发布。 |
| [^33] | [Robust Acoustic and Semantic Contextual Biasing in Neural Transducers for Speech Recognition.](http://arxiv.org/abs/2305.05271) | 本文提出了一种使用轻量级字符表示编码细粒度发音特征的方法，称为声学偏置，以提高针对声音相似性引导的上下文偏置，在神经变换器等自动语音识别系统性能中重要的改进。 |
| [^34] | [Attack Named Entity Recognition by Entity Boundary Interference.](http://arxiv.org/abs/2305.05253) | 本文提出了一种基于实体边界干扰的新型单词修改NER攻击——虚拟边界攻击(ViBA)，该攻击在四个基准数据集上攻击最先进的NER模型时显示出了非常有效的结果。 |
| [^35] | [Distilling Script Knowledge from Large Language Models for Constrained Language Planning.](http://arxiv.org/abs/2305.05252) | 本文首次定义了受限语言规划任务，提出了一种方法来提高大型语言模型在这个任务中的表现，并提取了一个新颖的受限语言规划数据集。实验证明该方法显著提高了其在约束忠实度方面的能力，并对赋予较小的语言模型受限语言规划能力非常有效。 |
| [^36] | [Multi-Teacher Knowledge Distillation For Text Image Machine Translation.](http://arxiv.org/abs/2305.05226) | 本文提出了一种多教师知识蒸馏方法，可以将知识有效地蒸馏到管道模型中并传递给端到端TIMT模型，从而提高性能。 |
| [^37] | [Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages.](http://arxiv.org/abs/2305.05214) | 本文解决了极低资源语言到英语的机器翻译任务，利用密切相关的高资源语言的词汇相似性，注入噪声作为正则化器，使模型更能抵御词汇差异，从而更好地促进跨语言转移。 |
| [^38] | [Exploration of Language Dependency for Japanese Self-Supervised Speech Representation Models.](http://arxiv.org/abs/2305.05201) | 本文研究了跨语言和单语言模型在日语自动语音识别任务中的表现，研究了日语中未标注数据相对于跨语言预先训练模型的需求，同时研究了自监督学习在日语中的有效性，并展示了最先进的性能。 |
| [^39] | [COLA: Contextualized Commonsense Causal Reasoning from the Causal Inference Perspective.](http://arxiv.org/abs/2305.05191) | 本文提出了一个从因果推断角度出发的情境化常识因果推理任务，设计了一个零-shot框架COLA来解决此任务，并且该框架可以更准确地检测常识因果关系。 |
| [^40] | [SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models.](http://arxiv.org/abs/2305.05189) | 本文提出了一个名为SUR-adapter的微调方法，用于增强预先训练的文本到图像扩散模型的语义理解和常识推理能力，以便在生成图片时使用简短的叙述提示。作者还构建了一个新的数据集SURD，并使用大型语言模型的知识进行了优化。 |
| [^41] | [CSED: A Chinese Semantic Error Diagnosis Corpus.](http://arxiv.org/abs/2305.05183) | 本文建立了一个中文语义错误诊断语料库CSED，通过提出基于句法的模型实现了CSED-R和CSED-C任务的最佳表现。 |
| [^42] | [MoT: Pre-thinking and Recalling Enable ChatGPT to Self-Improve with Memory-of-Thoughts.](http://arxiv.org/abs/2305.05181) | 本文提出了一个名为 MoT 的框架，让大型语言模型通过“思想记忆”自我进化，无需注释数据集和参数更新。实验表明，该框架可以显著提高 ChatGPT 在数学推理、常识推理、事实推理和自然语言推理方面的能力。 |
| [^43] | [FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance.](http://arxiv.org/abs/2305.05176) | 本论文介绍了如何在使用大型语言模型的同时降低成本和提高性能。作者提出了三种策略，包括提示适应，LLM近似和LLM级联，并且通过 FrugalGPT 这种方法，实现了大大降低成本或是提高准确率的效果。 |
| [^44] | [Summarization with Precise Length Control.](http://arxiv.org/abs/2305.05171) | 本文提出了一种精确控制长度的摘要生成框架，通过联合训练模型预测长度并生成长度最佳的摘要，在保持文本质量的同时提高了性能。 |
| [^45] | [E2TIMT: Efficient and Effective Modal Adapter for Text Image Machine Translation.](http://arxiv.org/abs/2305.05166) | 本文提出了一种高效有效的文图机器翻译模态适配器，利用现有OCR和MT数据库和新型模态适配器将OCR编码器和MT解码器连接，使得端到端TIMT模型在翻译质量和效率方面优于现有的两阶段级联模型和其他最先进的一级端到端模型。 |
| [^46] | [Effective Medical Code Prediction via Label Internal Alignment.](http://arxiv.org/abs/2305.05162) | 本文提出了一种通过多视角注意力机制的神经网络，以在临床文本中预测医疗代码，并在标签空间与临床文本之间进行对齐，实现了对先前技术水平的提升。 |
| [^47] | [Read, Diagnose and Chat: Towards Explainable and Interactive LLMs-Augmented Depression Detection in Social Media.](http://arxiv.org/abs/2305.05138) | 本文提出一种基于LLMs的新型抑郁症检测系统，它通过自然语言对话提供了诊断证据和个性化建议，相较于传统方法性能更好。 |
| [^48] | [Generating Phishing Attacks using ChatGPT.](http://arxiv.org/abs/2305.05133) | 本文发现使用ChatGPT生成恶意提示可以生成功能性的网络钓鱼网站来模仿流行品牌并模拟多种规避策略，这些攻击可以使用普通的ChatGPT生成，无需使用任何先前的对抗性攻击（越狱）。 |
| [^49] | [Who Needs Decoders? Efficient Estimation of Sequence-level Attributes.](http://arxiv.org/abs/2305.05098) | 研究提出了非自回归代理模型(NAP)，通过编码序列直接预测通用标量值序列级属性，可以高效地实现解码步骤的规避。在机器翻译和语音识别两个场景下，NAP分别可以优于深度集成和高精度地预测性能指标。 |
| [^50] | [Interactive Concept Learning for Uncovering Latent Themes in Large Text Collections.](http://arxiv.org/abs/2305.05094) | 本研究提出了一种交互式框架，用于在大型文本集合中揭示潜在的、被领域专家视为相关的概念，既实现了自动化又减少了手动编码的工作量。 |
| [^51] | [A Unified Evaluation Framework for Novelty Detection and Accommodation in NLP with an Instantiation in Authorship Attribution.](http://arxiv.org/abs/2305.05079) | 本文提出了一个新颖性检测和适应性的统一评估框架，以作者归属任务为例进行了实例化，并使用多阶段任务的NoveltyTask评估了系统的性能，结果表明该领域的处理新颖性实例的问题非常具有挑战性。 |
| [^52] | [Coherent Wave Dynamics and Language Generation of a Generative Pre-trained Transformer.](http://arxiv.org/abs/2305.05061) | 本文研究了一种小型GPT中的相干波动和语言生成机制，发现波动动力学提供了一致和可重复的内在振荡模式，以及上下文感知的可塑性和表现力，可用于语言生成，为理解和控制更高级别的语言模式形成铺平了道路。 |
| [^53] | [Dreams Are More "Predictable'' Than You Think.](http://arxiv.org/abs/2305.05054) | 研究比较了梦的报告和维基百科的文本字符串，发现梦的报告整体上不偏离维基百科，而单个梦报告比维基百科文章更可预测。 |
| [^54] | [ANALOGICAL - A New Benchmark for Analogy of Long Text for Large Language Models.](http://arxiv.org/abs/2305.05050) | 本文介绍了一种名为“ANALOGICAL”的新型基准，用以内在评估LLMs在长文本类比中的能力，包括六个复杂级别的长文本类比分类，并使用13个数据集和三种距离度量方法来评估8个LLMs在语义向量空间中识别类比对的能力。 |
| [^55] | [Web Content Filtering through knowledge distillation of Large Language Models.](http://arxiv.org/abs/2305.05027) | 本文提出了一种基于大语言模型知识蒸馏的 URL 分类方法，可用于网络内容过滤，其学生模型在参数数量减少 175 倍的情况下，精度提升了 9%，超过了当前最先进方法。 |
| [^56] | [Do Not Blindly Imitate the Teacher: Using Perturbed Loss for Knowledge Distillation.](http://arxiv.org/abs/2305.05010) | 本文提出了一种新的知识蒸馏目标函数PTLoss，通过扰动老师的输出分布，使其更接近真实标签分布，从而提高学生的性能。 |
| [^57] | [GersteinLab at MEDIQA-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning.](http://arxiv.org/abs/2305.05001) | 本研究团队参与MEDIQA-Chat 2023共享任务，实现了医患交流中临床记录摘要的自动化生成。通过对预训练模型的微调和少量上下文学习，本方法在关键指标上取得了极好的结果。 |
| [^58] | [Explanation-based Finetuning Makes Models More Robust to Spurious Cues.](http://arxiv.org/abs/2305.04990) | 本文提出一种新型方法——解释性微调，通过让模型在给出答案的同时生成支持该答案的自由文本解释，来减轻LLMs依赖虚假关联，使得模型对虚假提示更加强韧，并具有广泛适用性。 |
| [^59] | [Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust.](http://arxiv.org/abs/2305.04989) | 本研究通过知识图谱结构评估Self-Attention变压器中编码的语义。结果显示，语言模型是概率语言模式产生的控制过程的模型，但是不将对象和概念级别的含义和语义赋予所学习的随机模式，例如知识图谱中所描述的模式。 |
| [^60] | [NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge.](http://arxiv.org/abs/2305.04978) | 本文提出了一种新的用于比较知识提炼的神经符号框架，名称为NeuroComparatives。该框架利用词汇约束解码进行比较知识提炼，以及对生成的知识进行严格过滤。 |
| [^61] | [LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization.](http://arxiv.org/abs/2305.04971) | 本文提出了一种基于标签正则化的通用框架，其中包括传统的LS，但也可以建模实例特定的变体。我们提出了一种双层优化的方法（LABO），用于学习标签正则化，并得到了可解释的最优标签平滑解。 |
| [^62] | [Joint Moment Retrieval and Highlight Detection Via Natural Language Queries.](http://arxiv.org/abs/2305.04961) | 本文提出了一种基于自然语言查询的联合视频摘要和精华片段检测方法，利用视觉和音频线索匹配用户的查询，实现检索视频中最相关和有趣的时刻。 |
| [^63] | [The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification.](http://arxiv.org/abs/2305.04940) | 本文介绍了一种早期层组合的方法EarlyBIRD，该方法可以有效利用深度自然语言处理模型的资源和可用信息，从而提高代码分类的性能，在缺陷检测方面平均可提高2个点。 |
| [^64] | [A transformer-based method for zero and few-shot biomedical named entity recognition.](http://arxiv.org/abs/2305.04928) | 本文提出了一种基于Transformer的生物医学领域零样本和少样本NER方法。此方法利用预训练学习给定和潜在类别之间的语义关系，将多类标记分类任务转换为二元标记分类，能够在不同数量的样本情况下达到良好的识别效果。 |
| [^65] | [Detecting and Reasoning of Deleted Tweets before they are Posted.](http://arxiv.org/abs/2305.04927) | 本研究提出了一种新的深度学习框架，可以在推文发布之前识别出即将被删除的内容，并推理其潜在危害和违反平台政策的原因。该方法可用于推进更安全和负责任的社交媒体使用。 |
| [^66] | [MultiModal-GPT: A Vision and Language Model for Dialogue with Humans.](http://arxiv.org/abs/2305.04790) | MultiModal-GPT是一个用于与人类进行多轮对话的视觉与语言模型，可以遵循人类的各种指令，并且通过联合训练表现得更好。 |
| [^67] | [PreCog: Exploring the Relation between Memorization and Performance in Pre-trained Language Models.](http://arxiv.org/abs/2305.04673) | 本文分析了预训练语言模型BERT在下游任务中记忆与性能之间的关系，提出了评估预训练记忆的指标PreCog，并发现高度记忆的例子分类效果更好，说明记忆对BERT的成功至关重要。 |
| [^68] | [AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment.](http://arxiv.org/abs/2305.04476) | 本文提出了基于跨模态对齐的STS模型AlignSTS，通过一种新颖的节奏适配器来预测目标节奏表示以弥合内容和音高之间的模态差距，并使用交叉注意力重新对齐内容进行跨模态融合重新合成。该模型表现优异。 |
| [^69] | [UIT-OpenViIC: A Novel Benchmark for Evaluating Image Captioning in Vietnamese.](http://arxiv.org/abs/2305.04166) | 这篇论文介绍了一种新颖的越南语图像字幕数据集UIT-OpenViIC，这是为了解决目前在越南低资源研究社区中存在的困境而引入的。该数据集包括越南的复杂场景，并仅由越南人根据严格的规则和监督进行手动注释。数据集对于最新的最先进的翻译模型而言具有挑战性。 |
| [^70] | [DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition.](http://arxiv.org/abs/2305.03688) | DAMO-NLP团队的U-RaNER是一种统一的多语言命名实体识别系统，它通过加入带有实体为中心的Wikidata知识库并采用infusion方法来增强检索上下文，解决了其他系统存在的知识不足、上下文长度有限和单一检索策略等问题。 |
| [^71] | [Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion.](http://arxiv.org/abs/2305.03509) | Diffusion Explainer是第一个可交互的可视化工具，用于解释稳定扩散如何将文本提示转化为图像，用户可以通过动画和交互元素流畅地在多个抽象级别之间过渡，从而更好地理解提示对图像生成的影响。 |
| [^72] | [T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering.](http://arxiv.org/abs/2305.03453) | 本研究使用大型语言模型信号教育科学问题回答的链式思维推理，通过生成高质量COT合理化信号，同时降低了人工注释的需求。 |
| [^73] | [The Elephant in the Room: Analyzing the Presence of Big Tech in Natural Language Processing Research.](http://arxiv.org/abs/2305.02797) | 本文研究了工业界在自然语言处理研究中的存在和影响。研究发现在过去五年中，工业界的存在与影响呈现急剧增长，一些公司占据了大部分出版物，并向学术研究人员提供资金支持。 |
| [^74] | [A Survey on Proactive Dialogue Systems: Problems, Methods, and Prospects.](http://arxiv.org/abs/2305.02750) | 本综述全面概述了不同类型对话中对话代理主动性的突出问题和先进设计，讨论了符合实际应用需求但需要未来更大研究重点的挑战，激发更多的会话 AI 进展到下一级别。 |
| [^75] | [FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information.](http://arxiv.org/abs/2305.01528) | 本研究介绍了一份包含真实游戏状态信息的Dungeons & Dragons实际游戏数据集FIREBALL，它可以改善自然语言生成的质量。此外，LLMs可以使用FIREBALL中的游戏状态信息来生成更高质量的游戏回合。 |
| [^76] | [Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models.](http://arxiv.org/abs/2305.01219) | 本研究提出一种新颖有效的“ProAttack”方法来执行干净标签的后门攻击，使用的是提示本身作为触发器。该方法不需要外部触发器，并确保毒瘤数据的标注正确，提高了后门攻击的隐蔽性，相比于现有的后门攻击方法有显著提升。 |
| [^77] | [Still no evidence for an effect of the proportion of non-native speakers on language complexity -- A response to Kauhanen, Einhaus & Walkden (2023).](http://arxiv.org/abs/2305.00217) | 本研究为对Kauhanen、Einhaus和Walkden（2023）的回应，仍然没有证据表明大量的L2用户影响语言复杂性。 |
| [^78] | [Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology.](http://arxiv.org/abs/2304.11957) | 本研究评估了ChatGPT-4在放射肿瘤学方面的表现，成绩显示出它在医学考试上有很大的优势，在实际应用中存在局限性。另外，ChatGPT-4 在放射肿瘤学上表现出色，但在骨骼和软组织以及妇科方面有待改进。 |
| [^79] | [Tailoring Domain Adaptation for Machine Translation Quality Estimation.](http://arxiv.org/abs/2304.08891) | 本研究提出了结合领域自适应和数据增强的质量评估系统，针对数据缺乏和领域不匹配的问题在通用模型的基础上进行微调，结果显著优于最先进基线。 |
| [^80] | [ParroT: Translating During Chat Using Large Language Models.](http://arxiv.org/abs/2304.02426) | ParroT提出了一种基于开源LLM和人工编写的翻译评估数据的聊天翻译框架，可以将翻译数据转化为指令执行样式，并引入额外要求来规范翻译过程。在使用相对较少的训练数据的情况下，实验结果表明 ParroT 可以大幅提高翻译质量。 |
| [^81] | [BloombergGPT: A Large Language Model for Finance.](http://arxiv.org/abs/2303.17564) | 本文提出了BloombergGPT，一个500亿参数的金融领域的大型语言模型，其基于Bloomberg的广泛数据来源和通用数据集进行训练。通过混合数据集训练，该模型在金融任务上表现出色，并且不会牺牲在普通任务上的性能。 |
| [^82] | [Team SheffieldVeraAI at SemEval-2023 Task 3: Mono and multilingual approaches for news genre, topic and persuasion technique classification.](http://arxiv.org/abs/2303.09421) | 本文介绍了Team SheffieldVeraAI在SemEval-2023任务3中的表现。他们提出了用于新闻类型、框架和说服技巧分类的单语和多语言方法。该团队使用多种模型和适配器，取得了在不同语言下的好成绩。 |
| [^83] | [Investigating the Translation Performance of a Large Multilingual Language Model: the Case of BLOOM.](http://arxiv.org/abs/2303.01911) | 研究发现，多语言语言模型BLOOM的0-shot性能存在问题，但在几个shot的情况下，表现得到极大的改善，特别是在某些语言对中表现非常好。 |
| [^84] | [Symbolic Discovery of Optimization Algorithms.](http://arxiv.org/abs/2302.06675) | 该论文提出了一种将算法发现视为程序搜索的方法，并用于发现深度神经网络训练的优化算法。他们的方法发现了一种简单而有效的优化算法Lion，它比Adam更节省内存并且在ImageNet上的准确率提高了2％，并且预训练的计算时间也减少了多达5倍。 |
| [^85] | [Distinguishability Calibration to In-Context Learning.](http://arxiv.org/abs/2302.06198) | 本文提出了一种旋转和缩放的特征变换校准方法，可用于基于提示的学习进行文本分类，从而解决了在转换器中进行上下文学习时遇到的信息扩散问题。 |
| [^86] | [Creating a Large Language Model of a Philosopher.](http://arxiv.org/abs/2302.01339) | 该研究使用OpenAI的大型语言模型GPT-3和哲学家丹尼特的作品为训练数据，探索了生成哲学文本的能力。研究人员通过招募大量参与者来区分真正的哲学家丹尼特和机器生成的文字。专家成功率达到51％，但没有达到预期的80％，该模型有可能超越人类的思维能力。 |
| [^87] | [Adaptive Machine Translation with Large Language Models.](http://arxiv.org/abs/2301.13294) | 本文研究了如何利用大型语言模型的上下文学习来改进实时自适应机器翻译，实验结果表明有希望的效果。 |
| [^88] | [Pretraining Without Attention.](http://arxiv.org/abs/2212.10544) | 本文通过使用基于状态空间模型的序列路由方法提出了一种不依赖注意力机制的预训练模型BiGS，可以达到与BERT预训练准确度相当的GLUE测试结果，并具有不同的归纳偏差。 |
| [^89] | [SeqDiffuSeq: Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation.](http://arxiv.org/abs/2212.10325) | 本文提出了一种名为SeqDiffuSeq的文本扩散模型，用于序列生成，采用了编码器-解码器Transformer架构和自适应噪声调度技术，旨在探索扩散模型在自然语言生成方面的性能表现。 |
| [^90] | [GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator.](http://arxiv.org/abs/2212.10218) | 本文提出了一种名为GanLM的编码器-解码器预训练模型，它引入了辅助鉴别器来统一语言理解和生成能力，并使用两个预训练目标进行训练：替换令牌检测和替换令牌去噪。 |
| [^91] | [Validating Large Language Models with ReLM.](http://arxiv.org/abs/2211.15458) | ReLM是一种使用正则表达式验证和查询LLM的系统，可以解决LLM数据记忆、偏见、毒性和语言理解等问题，具有高效性和广泛性。 |
| [^92] | [Global and Local Hierarchy-aware Contrastive Framework for Implicit Discourse Relation Recognition.](http://arxiv.org/abs/2211.13873) | 本文提出了GOLF框架，它能够充分利用全局和本地感知层次结构来提升隐含篇章关系识别效果。 |
| [^93] | [Once-for-All Sequence Compression for Self-Supervised Speech Models.](http://arxiv.org/abs/2211.02332) | 本文提出了一种自监督语音模型的一次性序列压缩框架，该框架支持连续的操作压缩率范围，并在各种任务上展现出平滑的性能效率权衡。 |
| [^94] | [Weakly Supervised Learning for Analyzing Political Campaigns on Facebook.](http://arxiv.org/abs/2210.10669) | 本研究提出了一种基于弱监督学习的方法，通过分析Facebook上政治广告的立场和议题，以及其使用人口统计学定位，来了解政治活动的特点和时间动态。 |
| [^95] | [Can Demographic Factors Improve Text Classification? Revisiting Demographic Adaptation in the Age of Transformers.](http://arxiv.org/abs/2210.07362) | 论文探讨利用人口统计因素加强文本分类的效果与先前一样在最新的变压器语言模型中存在。该研究使用连续语言建模和动态多任务学习的方法来适应语言中特定年龄和性别的方面，通过结合语言建模目标和人口统计学的预测，结果表明在四种语言中任务表现得到了显著的提高。 |
| [^96] | [Language Agnostic Multilingual Information Retrieval with Contrastive Learning.](http://arxiv.org/abs/2210.06633) | 该论文提出一种使用对比学习的技术，利用平行和非平行语料库来提高多语种信息检索的效果，仅使用英语IR训练数据和一些平行语料库即可在非英语数据上实现显著的检索性能改进。 |
| [^97] | [Deep Span Representations for Named Entity Recognition.](http://arxiv.org/abs/2210.04182) | 本研究提出了DSpERT模型，通过跨度Transformer逐层聚合标记表示作为键和值，产生了深层语义的跨度表示，从而解决了现有跨度基础NER系统中长跨度实体显着无效性和重叠跨度表示的耦合问题。实验结果表明，DSpERT在八个NER基准测试中取得了性能高于或与最新最先进系统竞争的成果。 |
| [^98] | [Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach.](http://arxiv.org/abs/2209.06995) | 本文提出了一种名为PATRON的方法，使用基于提示信息的不确定性的数据选择策略来提高预训练语言模型微调的few-shot性能，在六个文本分类数据集上实验证实该方法的性能优于最先进的冷启动数据选择基线，且仅使用128标签的情况下，该方法可以达到91.0%和92.1%的完全监督性能。 |
| [^99] | [On Reality and the Limits of Language Data: Aligning LLMs with Human Norms.](http://arxiv.org/abs/2208.11981) | 本文研究了大型语言模型（LLMs）在仅使用语言数据的情况下理解物理世界的能力，使用新颖且严密控制的推理测试（ART）与人类规范进行对比，研究发现了LLMs在某些常识关系模型中可以直接从数据中学习，但存在弱点，例如在部分和包含关系方面表现不足。 |
| [^100] | [A Multimodal Transformer: Fusing Clinical Notes with Structured EHR Data for Interpretable In-Hospital Mortality Prediction.](http://arxiv.org/abs/2208.10240) | 本论文提出了一种新型多模态转换器，融合临床笔记和结构化的EHR数据，以更好地预测住院死亡风险。通过集成梯度方法选择临床笔记中的关键词和利用 Shapley 值发现重要的结构化 EHR 特征，并对其进行可视化解释，提高了模型的解释性。 |
| [^101] | [BERT WEAVER: Using WEight AVERaging to Enable Lifelong Learning for Transformer-based Models in the Biomedical Domain.](http://arxiv.org/abs/2202.10101) | 研究提出了WEAVER方法，它可以将旧知识融入到新模型中，以有效降低灾难性遗忘，并实现生物医学领域基于Transformer的模型的终身学习。 |
| [^102] | [Vector Space Semantics for Lambek Calculus with Soft Subexponentials.](http://arxiv.org/abs/2111.11331) | 本论文介绍了一种软子指数Lambek演算的向量空间语义，应用于构建委托语缺位名词短语和带有回指和省略的话语单元的组合向量解释，具有很好的应用前景。 |
| [^103] | [A transfer learning based approach for pronunciation scoring.](http://arxiv.org/abs/2111.00976) | 本文提出了一种基于迁移学习的发音评分方法，可以在针对此任务专门训练的系统数据稀缺的情况下，利用为ASR训练的模型取得更好的成绩，并在EpaDB数据库上实现了20％的性能提升。 |

# 详细

[^1]: TidyBot: 应用大语言模型的个性化机器人物理辅助

    TidyBot: Personalized Robot Assistance with Large Language Models. (arXiv:2305.05658v1 [cs.RO])

    [http://arxiv.org/abs/2305.05658](http://arxiv.org/abs/2305.05658)

    本文研究了使用机器人进行家庭清扫的个性化问题。通过使用大型语言模型少样本摘要能力，机器人可以学习用户的偏好并将其推广到未来的场景中，从而实现快速适应。

    

    为了使机器人能够有效个性化地提供物理辅助，它必须学习用户的个人喜好并将其应用于未来的场景中。本文研究了使用机器人进行家庭清扫的个性化问题，这些机器人能够通过捡起物品并将其放回原处来整理房间。一个关键的挑战是确定每个物品的正确位置，因为人们的喜好可以因个人品味或文化背景而大不相同。例如，一个人可能喜欢把衬衫放在抽屉里，而另一个人可能喜欢把衬衫放在架子上。我们旨在建立系统，这些系统可以通过与特定人的先前交互学习这样的喜好，而只需要几个示例。我们展示了机器人可以将基于语言的规划和感知与大型语言模型(LLMs)的少样本摘要能力相结合，从而推断出广泛适用于未来交互的用户偏好。这种方法实现了快速适应，并取得了91.2%的准确率。

    For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accurac
    
[^2]: 为建立联邦 GPT 做出努力：联邦指令调整

    Towards Building the Federated GPT: Federated Instruction Tuning. (arXiv:2305.05644v1 [cs.CL])

    [http://arxiv.org/abs/2305.05644](http://arxiv.org/abs/2305.05644)

    本文提出了一种名为Federated Instruction Tuning (FedIT)的新方法，利用联邦学习（FL）模型对LLMs进行指令调整，以解决获取高质量指令数据的挑战，从而提高调整模型的通用性和效果。

    

    虽然“指令调整”生成大型语言模型（LLMs）展现出了出色的新任务概括能力，但训练阶段严重依赖于大量多样和高质量的指令数据（如ChatGPT和GPT-4）。然而，获取高质量数据，特别是人工撰写的数据，可能会面临显著的成本和可访问性方面的挑战。此外，与隐私有关的问题可能会进一步限制对这些数据的访问，使得获取数据的过程变得复杂而微妙。因此，这限制了调整模型的通用性并可能限制其在特定情境下的效果。为解决这个问题，我们的研究引入了一种新的方法，称为联邦指令调整（FedIT），它利用联邦学习（FL）作为LLMs指令调整的学习框架。这是FL在LLMs指令调整中的首次探索。这尤其重要，因为...

    While ``instruction-tuned" generative large language models (LLMs) have demonstrated an impressive ability to generalize to new tasks, the training phases heavily rely on large amounts of diverse and high-quality instruction data (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality data, especially when it comes to human-written data, can pose significant challenges both in terms of cost and accessibility. Moreover, concerns related to privacy can further limit access to such data, making the process of obtaining it a complex and nuanced undertaking. Consequently, this hinders the generality of the tuned models and may restrict their effectiveness in certain contexts. To tackle this issue, our study introduces a new approach called Federated Instruction Tuning (FedIT), which leverages federated learning (FL) as the learning framework for the instruction tuning of LLMs. This marks the first exploration of FL-based instruction tuning for LLMs. This is especially important s
    
[^3]: 面向个人或实体的知识图谱表示学习：在医疗保健领域应用的研究

    Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare. (arXiv:2305.05640v1 [cs.AI])

    [http://arxiv.org/abs/2305.05640](http://arxiv.org/abs/2305.05640)

    本研究提出了一种在医疗保健领域构建面向实体的知识图谱的端到端表示学习方法 HEER，通过将领域特定的约束和特征纳入到图嵌入算法中，有效地改善了下游预测任务。

    

    知识图谱是一种按本体或模式组织信息的流行方式，已经在从搜索到推荐的各种场景中得到了应用。尽管在知识图谱方面有了一些进展，但知识表示仍然是跨行业的一个非常棘手的任务，特别是在生物医学和医疗保健领域，由于实体之间的复杂相互关系、异质性、缺乏标准化和数据稀疏性等因素，这一任务尤其具有挑战性。本文提出了一种面向医疗保健领域构建面向实体的知识图谱的端到端表示学习方法，重点是捕捉生物医学领域的独特特征。所提出的框架名为HEER（Healthcare Entity-Entity Representation learning），将领域特定的约束和特征纳入到图嵌入算法中。对多个基准数据集的结果表明，与最先进的方法相比，HEER在改善下游预测任务方面具有有效性。

    Knowledge graphs (KGs) are a popular way to organise information based on ontologies or schemas and have been used across a variety of scenarios from search to recommendation. Despite advances in KGs, representing knowledge remains a non-trivial task across industries and it is especially challenging in the biomedical and healthcare domains due to complex interdependent relations between entities, heterogeneity, lack of standardization, and sparseness of data. KGs are used to discover diagnoses or prioritize genes relevant to disease, but they often rely on schemas that are not centred around a node or entity of interest, such as a person. Entity-centric KGs are relatively unexplored but hold promise in representing important facets connected to a central node and unlocking downstream tasks beyond graph traversal and reasoning, such as generating graph embeddings and training graph neural networks for a wide range of predictive tasks. This paper presents an end-to-end representation le
    
[^4]: 编码器-解码器方法在法律和生物医学文本的多标签分类中的探索

    An Exploration of Encoder-Decoder Approaches to Multi-Label Classification for Legal and Biomedical Text. (arXiv:2305.05627v1 [cs.CL])

    [http://arxiv.org/abs/2305.05627](http://arxiv.org/abs/2305.05627)

    本研究探索了编码器-解码器方法在多标签分类中的应用，结果表明该方法在更复杂的数据集和标签粒度更细的标签方案上表现更佳，尤其是在非自回归的情况下使用最佳。

    

    多标签文本分类的标准方法主要依赖于仅具有编码器的预训练语言模型，而编码器-解码器模型在其他分类任务中已被证明更有效。本研究比较了四种多标签分类方法，其中两种基于仅具有编码器，两种基于编码器-解码器。我们在四个数据集上进行了实验，其中两个是法律领域的，两个是生物医学领域的，每个数据集都有两个标签粒度级别，并始终从同一预训练模型T5出发。结果表明，编码器-解码器方法优于仅具有编码器的方法，在更复杂的数据集和标签粒度更细的标签方案上优势越来越明显。特别是在非自回归的情况下使用编码器-解码器模型，整体上表现最佳，因此我们通过消融研究来进一步研究这种方法以更好地理解其优点。

    Standard methods for multi-label text classification largely rely on encoder-only pre-trained language models, whereas encoder-decoder models have proven more effective in other classification tasks. In this study, we compare four methods for multi-label classification, two based on an encoder only, and two based on an encoder-decoder. We carry out experiments on four datasets -two in the legal domain and two in the biomedical domain, each with two levels of label granularity- and always depart from the same pre-trained model, T5. Our results show that encoder-decoder methods outperform encoder-only methods, with a growing advantage on more complex datasets and labeling schemes of finer granularity. Using encoder-decoder models in a non-autoregressive fashion, in particular, yields the best performance overall, so we further study this approach through ablations to better understand its strengths.
    
[^5]: ChatGPT案例记录：语言模型与复杂临床问题

    The Case Records of ChatGPT: Language Models and Complex Clinical Questions. (arXiv:2305.05609v1 [cs.CL])

    [http://arxiv.org/abs/2305.05609](http://arxiv.org/abs/2305.05609)

    本研究探究了GPT4和GPT3.5在复杂临床病例诊断中的表现，并表明人工智能语言模型在协助临床决策方面具有潜在的作用，尤其是在医学资源受限的情况下。

    

    背景：人工智能语言模型已在各种应用中显示出潜力，包括在辅助临床决策方面，如大型语言模型在医疗许可考试中的表现。然而，它们解决复杂、开放性的病例的能力（可能代表临床实践）仍未被探索。方法：本研究使用麻省总医院的案例记录，调查了大型语言人工智能模型GPT4和GPT3.5在诊断复杂临床病例方面的准确性。共识别了50个需要诊断和诊断测试的病例，发表于2022年1月1日至2022年4月16日。对于每个病例，模型会收到一个提示，请求前三个具体的诊断和相关的诊断测试，然后是案例文本、实验室和图例。将模型的输出与最终临床诊断进行比较，检查模型预测的测试是否会导致正确的诊断。结果：GPT3.5和GPT4都表现出在生成正确的诊断和推荐测试方面的高准确性，其中GPT4的表现优于GPT3.5。结论：这些发现表明，人工智能语言模型具有协助处理复杂临床决策的潜力，特别是在医学专家资源受限的情况下。

    Background: Artificial intelligence language models have shown promise in various applications, including assisting with clinical decision-making as demonstrated by strong performance of large language models on medical licensure exams. However, their ability to solve complex, open-ended cases, which may be representative of clinical practice, remains unexplored. Methods: In this study, the accuracy of large language AI models GPT4 and GPT3.5 in diagnosing complex clinical cases was investigated using published Case Records of the Massachusetts General Hospital. A total of 50 cases requiring a diagnosis and diagnostic test published from January 1, 2022 to April 16, 2022 were identified. For each case, models were given a prompt requesting the top three specific diagnoses and associated diagnostic tests, followed by case text, labs, and figure legends. Model outputs were assessed in comparison to the final clinical diagnosis and whether the model-predicted test would result in a correc
    
[^6]: DomainInv: 面向QA领域适应的领域不变质量调整与对抗性标签矫正。

    DomainInv: Domain Invariant Fine Tuning and Adversarial Label Correction For QA Domain Adaptation. (arXiv:2305.05589v1 [cs.CL])

    [http://arxiv.org/abs/2305.05589](http://arxiv.org/abs/2305.05589)

    本论文提出了面向QA领域适应的领域不变微调和对抗性标签校正的方法。更精确地说，本文的方法通过将目标域的表示转换为源域的表示，并利用源域的监督来进行训练，以实现无标签目标领域的领域适应。

    

    现有的问答（QA）系统受限于回答未见过的领域或任何领域之外的分布的能力，使它们在部署到真实场景中时不太可靠。更重要的是，所有现有的QA领域适应方法要么基于生成合成数据，要么是伪标记目标领域数据。基于合成数据和伪标记的领域适应方法，要么需要计算资源，要么需要额外的精心选择置信阈值来将非干净的样本从训练数据集中分离出来。在本文中，我们提出了一种无监督的领域适应方法，通过将目标表示转移至接近源领域，同时仍使用源领域的监督进行培训，从而适应标记的目标领域。为此，我们提出了领域不变的调优与对抗性标签糾正的想法，以识别与源域相距较远的目标实例。

    Existing Question Answering (QA) systems limited by the capability of answering questions from unseen domain or any out-of-domain distributions making them less reliable for deployment to real scenarios. Most importantly all the existing QA domain adaptation methods are either based on generating synthetic data or pseudo labeling the target domain data. The domain adaptation methods based on synthetic data and pseudo labeling suffers either from the requirement of computational resources or an extra overhead of carefully selecting the confidence threshold to separate the noisy examples from being in the training dataset. In this paper, we propose the unsupervised domain adaptation for unlabeled target domain by transferring the target representation near to source domain while still using the supervision from source domain. Towards that we proposed the idea of domain invariant fine tuning along with adversarial label correction to identify the target instances which lie far apart from 
    
[^7]: 大型语言模型人性化技术

    Large Language Models Humanize Technology. (arXiv:2305.05576v1 [cs.CL])

    [http://arxiv.org/abs/2305.05576](http://arxiv.org/abs/2305.05576)

    大型语言模型展示了人性化技术的新能力，特别是在跨越语言、职业和可访问性分歧的人们中。它们通过解决三个机械化瓶颈：创建多样化和可访问的内容，学习复杂的数字工具，以及个性化机器学习算法。

    

    大型语言模型（LLM）在最近几个月和几周内取得了快速进展，引起了广泛关注。这引发了有关将这些模型与人类价值观保持一致、它们对劳动力市场的影响以及进一步研究和开发的潜在需要进行监管的担忧。然而，这种讨论通常缺乏关注迫切性，即广泛传播LLM的社会效益。为了确定这种社会效益，我们断言LLM表现出比以前的技术更有效地使技术人性化的新能力，并且适用于跨越语言、职业和可访问性分歧的人们。我们认为它们这样做是通过解决当今计算技术中的三个机械化瓶颈实现的：创建多样化和可访问的内容、学习复杂的数字工具以及个性化机器学习算法。我们采用基于案例的方法，并用两个示例说明每个瓶颈，其中当前技术施加瓶颈，而LLM证明了相反的效果。

    Large Language Models (LLMs) have made rapid progress in recent months and weeks, garnering significant public attention. This has sparked concerns about aligning these models with human values, their impact on labor markets, and the potential need for regulation in further research and development. However, the discourse often lacks a focus on the imperative to widely diffuse the societal benefits of LLMs. To qualify this societal benefit, we assert that LLMs exhibit emergent abilities to humanize technology more effectively than previous technologies, and for people across language, occupation, and accessibility divides. We argue that they do so by addressing three mechanizing bottlenecks in today's computing technologies: creating diverse and accessible content, learning complex digital tools, and personalizing machine learning algorithms. We adopt a case-based approach and illustrate each bottleneck with two examples where current technology imposes bottlenecks that LLMs demonstrat
    
[^8]: 利用伪图像说明进行多模态摘要

    Exploiting Pseudo Image Captions for Multimodal Summarization. (arXiv:2305.05496v1 [cs.CL])

    [http://arxiv.org/abs/2305.05496](http://arxiv.org/abs/2305.05496)

    本文研究了跨模态对比学习中（部分）误负样本的挑战，并提出了一种从更一般下界形式的指导下调节跨模态相似度的对比学习策略，以更精确地优化图像/文本锚定点和其负面文本/图像之间的互信息。

    

    视觉语言预训练中的跨模态对比学习面临（部分）误负样本的挑战。本文从互信息（MI）优化的角度研究了该问题。我们理论上证明了当存在噪声时，包括负样本的MI也很重要。在更一般的优化下界形式的指导下，我们提出了一种由逐步细化的跨模态相似度调节的对比学习策略，以更精确地优化图像/文本锚定点和其负面文本/图像之间的MI，而不是错误地将其最小化。在四个下游跨模态任务上，我们的方法表现竞争力，并在理论指导下系统地平衡了（部分）误负样本的有利和有害效果。

    Cross-modal contrastive learning in vision language pretraining (VLP) faces the challenge of (partial) false negatives. In this paper, we study this problem from the perspective of Mutual Information (MI) optimization. It is common sense that InfoNCE loss used in contrastive learning will maximize the lower bound of MI between anchors and their positives, while we theoretically prove that MI involving negatives also matters when noises commonly exist. Guided by a more general lower bound form for optimization, we propose a contrastive learning strategy regulated by progressively refined cross-modal similarity, to more accurately optimize MI between an image/text anchor and its negative texts/images instead of improperly minimizing it. Our method performs competitively on four downstream cross-modal tasks and systematically balances the beneficial and harmful effects of (partial) false negative samples under theoretical guidance.
    
[^9]: MAUPQA：自动创建的海量波兰问答数据集。

    MAUPQA: Massive Automatically-created Polish Question Answering Dataset. (arXiv:2305.05486v1 [cs.CL])

    [http://arxiv.org/abs/2305.05486](http://arxiv.org/abs/2305.05486)

    本文介绍了如何自动收集弱标记数据集以帮助训练神经 passage 检索器，并通过发布 MAUPQA 数据集和 HerBERT-QA 神经 retriever，解决了手动标注数据集的困难，缺乏数据集的问题。

    

    最近，开放领域问答系统开始 heavily 依赖标注数据集以训练神经 passage 检索器。然而，手动标注这些数据集既困难又耗时，从而限制了对不太流行的语言的可用性。在这项工作中，我们尝试了几种自动收集弱标记数据集的方法，并展示了它们如何影响神经 passage 检索模型的性能。作为我们工作的结果，我们发布了 MAUPQA 数据集，其中包括将近 400,000 个波兰问答对，以及 HerBERT-QA 神经检索器。

    Recently, open-domain question answering systems have begun to rely heavily on annotated datasets to train neural passage retrievers. However, manually annotating such datasets is both difficult and time-consuming, which limits their availability for less popular languages. In this work, we experiment with several methods for automatically collecting weakly labeled datasets and show how they affect the performance of the neural passage retrieval models. As a result of our work, we publish the MAUPQA dataset, consisting of nearly 400,000 question-passage pairs for Polish, as well as the HerBERT-QA neural retriever.
    
[^10]: 探究子词分割对transformer语言模型性能的影响

    Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v1 [cs.CL])

    [http://arxiv.org/abs/2305.05480](http://arxiv.org/abs/2305.05480)

    本文研究使用单语词段算法StateMorph训练语言模型时，可以使模型更高效地收敛并获得更好的验证分数。

    

    我们想研究词段如何影响语言模型的性能。我们使用了一种单语词段算法StateMorph，在芬兰语和俄语中训练了GPT-2和BERT模型。作为比较，我们还训练了一个使用BPE和Morfessor分割算法的模型。我们的初步结果表明，StateMorph可以帮助模型更有效地收敛并获得更好的验证分数。

    We would like to explore how morphemes can affect the performance of a language model. We trained GPT-2 and Bert model with StateMorph for both Finnish and Russian, which is a morpheme segmenting algorithm. As a comparison, we also trained a model with BPE and Morfessor. Our preliminary result shows that StateMorph can help the model to converge more efficiently and achieve a better validation score.
    
[^11]: 超越研究数据集：工业场景中的新型意图发现

    Going beyond research datasets: Novel intent discovery in the industry setting. (arXiv:2305.05474v1 [cs.CL])

    [http://arxiv.org/abs/2305.05474](http://arxiv.org/abs/2305.05474)

    本文提出了一种用于电子商务领域实时意图发现的新方法，通过对现实生活数据进行预训练和利用对话结构细化，性能提高了33pp。

    

    新型意图发现自动化了将相似的信息（问题）分组以识别以前未知的意图的过程。然而，当前的研究集中在仅具有问题字段并且与现实生活数据集有显著差异的公共可用数据集上。本文提出了改进在大型电子商务平台中部署的意图发现流程的方法。我们展示了在领域内数据上进行预训练语言模型的收益：既有自监督的方式，也有弱监督的方式。我们还设计了一种最佳方法，用于在精调聚类任务期间利用现实生活数据集的对话结构（即问题和答案），我们称其为Conv。我们提出的所有方法综合利用了现实生活数据集，为只针对问题的Constrained Deep Adaptive Clustering (CDAC)模型提供了高达33pp的性能提升。相比之下，仅针对问题数据的CDAC模型只比基准线高达13pp的性能提升。

    Novel intent discovery automates the process of grouping similar messages (questions) to identify previously unknown intents. However, current research focuses on publicly available datasets which have only the question field and significantly differ from real-life datasets. This paper proposes methods to improve the intent discovery pipeline deployed in a large e-commerce platform. We show the benefit of pre-training language models on in-domain data: both self-supervised and with weak supervision. We also devise the best method to utilize the conversational structure (i.e., question and answer) of real-life datasets during fine-tuning for clustering tasks, which we call Conv. All our methods combined to fully utilize real-life datasets give up to 33pp performance boost over state-of-the-art Constrained Deep Adaptive Clustering (CDAC) model for question only. By comparison CDAC model for the question data only gives only up to 13pp performance boost over the naive baseline.
    
[^12]: 超越善意：NLP用于社会公益的研究现状报告

    Beyond Good Intentions: Reporting the Research Landscape of NLP for Social Good. (arXiv:2305.05471v1 [cs.CL])

    [http://arxiv.org/abs/2305.05471](http://arxiv.org/abs/2305.05471)

    本文介绍了NLP4SGPAPERS数据集，通过对解决社会问题的论文进行分类、可持续发展目标映射、任务及方法的确定，使用最先进的NLP模型在整个ACL文集上进行处理，提供了一个可视化工作区，展示了NLP4SG领域的全貌。

    

    随着自然语言处理(NLP)的最新进展，越来越多的应用程序出现在各种用例中。在众多的NLP应用中，许多学术研究人员受到激励，希望通过工作具有积极的社会影响，符合NLP for Social Good (NLP4SG)的最新倡议。然而，研究人员并不总是清楚地了解自己的研究工作如何解决当今的重大社会问题。因此，在本文中，我们介绍NLP4SGPAPERS，这是一个具有三个相关任务的科学数据集，可以帮助识别NLP4SG论文，并通过以下几个方面对NLP4SG进行描述: (1)确定解决社会问题的论文，(2)将它们映射到相应的联合国可持续发展目标(SDGs)，以及(3)识别它们正在解决的任务和使用的方法。我们使用最先进的NLP模型，解决了每个任务，并将它们用于整个ACL文集，从而产生一个可视化工作区，为研究人员提供了对NLP4SG领域的鸟瞰图。

    With the recent advances in natural language processing (NLP), a vast number of applications have emerged across various use cases. Among the plethora of NLP applications, many academic researchers are motivated to do work that has a positive social impact, in line with the recent initiatives of NLP for Social Good (NLP4SG). However, it is not always obvious to researchers how their research efforts are tackling today's big social problems. Thus, in this paper, we introduce NLP4SGPAPERS, a scientific dataset with three associated tasks that can help identify NLP4SG papers and characterize the NLP4SG landscape by: (1) identifying the papers that address a social problem, (2) mapping them to the corresponding UN Sustainable Development Goals (SDGs), and (3) identifying the task they are solving and the methods they are using. Using state-of-the-art NLP models, we address each of these tasks and use them on the entire ACL Anthology, resulting in a visualization workspace that gives resear
    
[^13]: 字符级别编码器模型的最佳配方是什么？

    What is the best recipe for character-level encoder-only modelling?. (arXiv:2305.05461v1 [cs.CL])

    [http://arxiv.org/abs/2305.05461](http://arxiv.org/abs/2305.05461)

    本文通过对字符级别BERT类型模型的设计空间和各种预训练目标的比较，找到了构建和训练字符级别模型的最佳方法。最好的字符级别模型的性能优于在相同数据上使用相同设置训练的基于标记的模型，这表明字符级别模型已准备好更广泛的应用，但训练字符级模型的最佳方法仍然依赖于子词级别的标记。

    

    本文旨在对以字符级别输出上下文表示的语言理解模型中最新进展进行基准测试。已经提出了许多这样的建模结构和训练这些结构的方法，但当前仍不清楚架构与预训练目标对最终模型性能的相对贡献。作者探索了这些模型的设计空间，比较了体系结构创新和各种不同的预训练目标，在一个固定的训练过程中使用一套评估任务来寻找目前构建和训练字符级别BERT类型模型的最佳方法。作者发现，最好的字符级别模型的性能超过了在相同数据上使用相同设置训练的基于标记的模型，这表明字符级别模型已准备好更广泛的应用。不幸的是，训练字符级模型的最佳方法仍然依赖于子词级别的标记。

    This paper aims to benchmark recent progress in language understanding models that output contextualised representations at the character level. Many such modelling architectures and methods to train those architectures have been proposed, but it is currently unclear what the relative contributions of the architecture vs. the pretraining objective are to final model performance. We explore the design space of such models, comparing architectural innovations and a variety of different pretraining objectives on a suite of evaluation tasks with a fixed training procedure in order to find the currently optimal way to build and train character-level BERT-like models. We find that our best performing character-level model exceeds the performance of a token-based model trained with the same settings on the same data, suggesting that character-level models are ready for more widespread adoption. Unfortunately, the best method to train character-level models still relies on a subword-level toke
    
[^14]: WikiWeb2M: 一个基于页面的多模态维基百科数据集

    WikiWeb2M: A Page-Level Multimodal Wikipedia Dataset. (arXiv:2305.05432v1 [cs.CL])

    [http://arxiv.org/abs/2305.05432](http://arxiv.org/abs/2305.05432)

    WikiWeb2M是一个保留完整网页图像、文本和结构数据的多模态数据集，可用于网页描述生成、章节摘要和上下文图像字幕等任务。

    

    网页一直是语言和视觉语言任务的丰富资源。然而，只有网页的某些部分被保留下来：图像 - 标题对、长文本文章或原始 HTML，从未集成到一个地方。因此，网页任务受到的关注很少，结构化的图像 - 文本数据被低估了。为了研究多模态网页理解，我们介绍了维基百科网页 2M（WikiWeb2M）套件；它是第一个保留网页中全部图像、文本和结构数据的数据集。WikiWeb2M可以用于诸如页面描述生成、章节摘要和上下文图像字幕等任务。

    Webpages have been a rich resource for language and vision-language tasks. Yet only pieces of webpages are kept: image-caption pairs, long text articles, or raw HTML, never all in one place. Webpage tasks have resultingly received little attention and structured image-text data underused. To study multimodal webpage understanding, we introduce the Wikipedia Webpage 2M (WikiWeb2M) suite; the first to retain the full set of images, text, and structure data available in a page. WikiWeb2M can be used for tasks like page description generation, section summarization, and contextual image captioning.
    
[^15]: 利用马哈巴拉塔叙事文本语言模型计算相关词汇

    Estimating related words computationally using language model from the Mahabharata - an Indian epic. (arXiv:2305.05420v1 [cs.CL])

    [http://arxiv.org/abs/2305.05420](http://arxiv.org/abs/2305.05420)

    本文提出了一种基于语言模型的方法，用于估计马哈巴拉塔中的相关词语。通过对文本进行预处理并构建语言模型，该方法可以有效地估计相关性分数和单词的共现概率，从而从马哈巴拉塔中提取出相关词汇。

    

    马哈巴拉塔叙事文本是印度最流行的文学作品之一，在许多领域中以完全不同的目的被引用。在自然语言处理、人工智能、机器学习和人机交互的时代，这个文本可以根据领域需求进行处理。在分析马哈巴拉塔时，人类分析者会有情感因素，而且也无法记忆句子中常见的单词和句子的平均长度等计算细节。本文提出一种基于语言模型的计算方法来估计马哈巴拉塔中的相关词语。该方法包括对文本进行预处理和构建语言模型，并使用模型估计单词的共现概率和相关性分数。我们通过实验证明了该方法能够有效地估计从马哈巴拉塔中提取出的相关词汇。

    'Mahabharata' is the most popular among many Indian pieces of literature referred to in many domains for completely different purposes. This text itself is having various dimension and aspects which is useful for the human being in their personal life and professional life. This Indian Epic is originally written in the Sanskrit Language. Now in the era of Natural Language Processing, Artificial Intelligence, Machine Learning, and Human-Computer interaction this text can be processed according to the domain requirement. It is interesting to process this text and get useful insights from Mahabharata. The limitation of the humans while analyzing Mahabharata is that they always have a sentiment aspect towards the story narrated by the author. Apart from that, the human cannot memorize statistical or computational details, like which two words are frequently coming in one sentence? What is the average length of the sentences across the whole literature? Which word is the most popular word a
    
[^16]: 大型语言模型在医疗对话问答中需要进行整体性思考

    Large Language Models Need Holistically Thought in Medical Conversational QA. (arXiv:2305.05410v1 [cs.CL])

    [http://arxiv.org/abs/2305.05410](http://arxiv.org/abs/2305.05410)

    本研究提出了一种Holistically Thought（HoT）方法，用于引导大型语言模型进行综合性思考，以在医疗对话问答中生成高质量的医学响应。

    

    医疗对话问答系统旨在提供一系列专业的医疗服务，以提高医疗护理效率。尽管大型语言模型在数学、逻辑和常识问答等各个领域的复杂推理任务中取得了成功，但随着医学领域的日益复杂和专业化，它们仍需要提高。这是因为医疗对话问答任务不仅需要强大的医学推理能力，还需要广泛深入的思维能力。本文针对这些需要从许多方面考虑和理解的医疗对话问答任务的挑战，提出了全面思考（HoT）方法，旨在引导大型语言模型进行扩散和聚焦思考，生成高质量的医学响应。所提出的HoT方法已通过自动化和手动评估，在包含英文和中文的三个不同的医疗对话问答数据集中进行了评估。

    The medical conversational question answering (CQA) system aims at providing a series of professional medical services to improve the efficiency of medical care. Despite the success of large language models (LLMs) in complex reasoning tasks in various fields, such as mathematics, logic, and commonsense QA, they still need to improve with the increased complexity and specialization of the medical field. This is because medical CQA tasks require not only strong medical reasoning, but also the ability to think broadly and deeply. In this paper, to address these challenges in medical CQA tasks that need to be considered and understood in many aspects, we propose the Holistically Thought (HoT) method, which is designed to guide the LLMs to perform the diffused and focused thinking for generating high-quality medical responses. The proposed HoT method has been evaluated through automated and manual assessments in three different medical CQA datasets containing the English and Chinese languag
    
[^17]: 开放世界知识库中的完整性、召回率和否定性：一项调查

    Completeness, Recall, and Negation in Open-World Knowledge Bases: A Survey. (arXiv:2305.05403v1 [cs.AI])

    [http://arxiv.org/abs/2305.05403](http://arxiv.org/abs/2305.05403)

    本调查讨论了在开放世界知识库中表达、提取和推断完整性、召回率和否定性信息的方法，以及面对不完整和不确定的知识时，从业者和研究人员应该如何处理这些情况。

    

    通用知识库是知识中心的AI的基石。许多知识库是从Web来源实用主义构建的，因此远非完整。这给内容的消费和管理带来了挑战。本调查讨论了如何表达、提取和推断知识库中的完整性、召回率和否定性信息。我们涵盖了（i）部分封闭世界语义下的知识表示和查询的逻辑基础；（ii）通过统计模式估计此信息；（iii）从知识库和文本中提取关于召回率的信息；（iv）辨别有趣的否定语句；以及（v）相对召回率的宽松概念。本调查针对两类受众：（1）寻求处理不完整和不确定知识指南的从业者，以及（2）旨在推进知识库管理、质量评估和自然语言理解的研究人员。

    General-purpose knowledge bases (KBs) are a cornerstone of knowledge-centric AI. Many of them are constructed pragmatically from Web sources, and are thus far from complete. This poses challenges for the consumption as well as the curation of their content. While several surveys target the problem of completing incomplete KBs, the first problem is arguably to know whether and where the KB is incomplete in the first place, and to which degree.  In this survey we discuss how knowledge about completeness, recall, and negation in KBs can be expressed, extracted, and inferred. We cover (i) the logical foundations of knowledge representation and querying under partial closed-world semantics; (ii) the estimation of this information via statistical patterns; (iii) the extraction of information about recall from KBs and text; (iv) the identification of interesting negative statements; and (v) relaxed notions of relative recall.  This survey is targeted at two types of audiences: (1) practitione
    
[^18]: 在电子商务中使用数据增强实现一致的文本分类

    Consistent Text Categorization using Data Augmentation in e-Commerce. (arXiv:2305.05402v1 [cs.LG])

    [http://arxiv.org/abs/2305.05402](http://arxiv.org/abs/2305.05402)

    本文提出了一种在电子商务中使用数据增强实现一致的文本分类的新框架，该框架旨在改进产品分类模型的一致性，同时保持其生产水平的性能。

    

    大规模电子商务数据分类是一项关键的、广泛应用于工业领域的任务。本文旨在改进一家主要网络公司已经在使用的产品分类模型，该模型用于多种应用。在该模型核心中，产品分类模型是一个文本分类模型，接受产品标题作为输入，并从数千个可用候选项中输出最合适的类别。经过进一步观察，我们发现了类似物品标签上的不一致性。例如，标题中关于颜色或尺寸的小变化，会对模型产生较大影响。这种现象可能会对下游的推荐或搜索应用造成负面影响，导致用户体验下降。为了解决这个问题，我们提出了一个新的框架，实现一致的文本分类。我们的目标是提高模型的一致性，并保持其生产水平的性能。

    The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications. At its core, the product categorization model is a text classification model that takes a product title as an input and outputs the most suitable category out of thousands of available candidates. Upon a closer inspection, we found inconsistencies in the labeling of similar items. For example, minor modifications of the product title pertaining to colors or measurements majorly impacted the model's output. This phenomenon can negatively affect downstream recommendation or search applications, leading to a sub-optimal user experience.  To address this issue, we propose a new framework for consistent text categorization. Our goal is to improve the model's consistency while maintaining its production-level performance. W
    
[^19]: CaseEncoder：一种融合法律知识的预训练模型用于法律案例编码

    CaseEncoder: A Knowledge-enhanced Pre-trained Model for Legal Case Encoding. (arXiv:2305.05393v1 [cs.IR])

    [http://arxiv.org/abs/2305.05393](http://arxiv.org/abs/2305.05393)

    本文提出了一种融合法律知识的预训练模型CaseEncoder，针对法律案例的特殊领域需求，CaseEncoder在数据采样和预训练阶段中都使用了法律知识，其中包括利用细粒度的法律条款信息引导正负样本的选择，以及设计了与相关法律案例的评判标准相一致的法律特定预训练任务，该模型在法律案例检索和法律问答任务上优于最先进的PLMs。

    

    现代法律信息系统中，法律案例检索是关键的流程。尽管近期的研究利用了基于通用领域自监督预训练范式的预训练语言模型(PLMs)构建了用于法律案例检索的模型，但是使用通用领域的PLMs作为骨干模型有其局限性。具体来说，这些模型可能无法完全捕捉法律案例文档中的潜在法律特征。为了解决这个问题，我们提出了CaseEncoder，一种法律文档编码器，它在数据采样和预训练阶段利用细粒度的法律知识。在数据采样阶段，我们利用细粒度的法律条款信息引导正负样本的选择，从而提高了训练数据的质量。在预训练阶段，我们设计了与相关法律案例的评判标准相一致的法律特定预训练任务。根据这些任务，我们引入了一种创新的损失函数——偏置圆形损失(Biased Circle Loss)来处理法律案例数据集中正负样本之间的不平衡。实验结果表明，我们的CaseEncoder在法律案例检索和法律问答任务上优于最先进的PLMs。

    Legal case retrieval is a critical process for modern legal information systems. While recent studies have utilized pre-trained language models (PLMs) based on the general domain self-supervised pre-training paradigm to build models for legal case retrieval, there are limitations in using general domain PLMs as backbones. Specifically, these models may not fully capture the underlying legal features in legal case documents. To address this issue, we propose CaseEncoder, a legal document encoder that leverages fine-grained legal knowledge in both the data sampling and pre-training phases. In the data sampling phase, we enhance the quality of the training data by utilizing fine-grained law article information to guide the selection of positive and negative examples. In the pre-training phase, we design legal-specific pre-training tasks that align with the judging criteria of relevant legal cases. Based on these tasks, we introduce an innovative loss function called Biased Circle Loss to 
    
[^20]: COKE：机器心智理论的认知知识图谱

    COKE: A Cognitive Knowledge Graph for Machine Theory of Mind. (arXiv:2305.05390v1 [cs.CL])

    [http://arxiv.org/abs/2305.05390](http://arxiv.org/abs/2305.05390)

    COKE是一个机器心智理论的认知知识图谱，将ToM形式化为一组经手动验证的认知链，可以帮助AI系统在社交智能等任务上具备推理人类心智的能力。

    

    心智理论（ToM）是指人类理解和推断他人欲望、信念和意图的能力。获取ToM对人类的社会认知和人际关系起着关键作用。尽管ToM对于社交智能至关重要，但现代AI和NLP系统仍然缺乏该能力，因为它们无法访问训练语料库之下的人类心智状态和认知过程。为了赋予AI系统ToM能力，缩小它们与人类之间的差距，在本文中，我们提出了COKE：第一个机器心智理论的认知知识图谱。具体而言，COKE将ToM形式化为一组45k+经手动验证的认知链，这些链描绘了人类在特定社交环境下的心理活动和随后的行为/情感反应。除此之外，我们还使用预训练的语言模型进一步推广了COKE，并构建了一个强大的认知生成模型COKE+。在自动和人工评估方面的实验结果表明，COKE对于各种与ToM相关的任务，包括社交常识推理、情感识别和可解释对话，具有有效性和可解释性。

    Theory of mind (ToM) refers to humans' ability to understand and infer the desires, beliefs, and intentions of others. The acquisition of ToM plays a key role in humans' social cognition and interpersonal relations. Though indispensable for social intelligence, ToM is still lacking for modern AI and NLP systems since they cannot access the human mental state and cognitive process beneath the training corpus. To empower AI systems with the ToM ability and narrow the gap between them and humans, in this paper, we propose COKE: the first cognitive knowledge graph for machine theory of mind. Specifically, COKE formalizes ToM as a collection of 45k+ manually verified cognitive chains that characterize human mental activities and subsequent behavioral/affective responses when facing specific social circumstances. Beyond that, we further generalize COKE using pre-trained language models and build a powerful cognitive generation model COKE+. Experimental results in both automatic and human eva
    
[^21]: 预训练语言模型的代码执行能力研究

    Code Execution with Pre-trained Language Models. (arXiv:2305.05383v1 [cs.PL])

    [http://arxiv.org/abs/2305.05383](http://arxiv.org/abs/2305.05383)

    本文研究了预训练语言模型对代码执行的能力，并通过开发一种变异数据增强技术创建了一个大规模的Python数据集和任务，提出了CodeExecutor模型以增强语义理解，该模型在代码执行、零-shot代码到代码搜索和文本到代码生成等方面有潜在好处。

    

    代码执行是编程语言语义学中的基本方面，它反映了代码的确切行为。然而，大多数面向代码智能的预训练模型忽略了执行轨迹，只依靠源代码和句法结构。本文研究了预训练模型能否理解和执行代码。我们开发了一种基于变异的数据增强技术，创建了一个大规模和逼真的Python数据集和任务，挑战了现有的模型如Codex。然后，我们提出了CodeExecutor，一个利用代码执行预训练和课程学习来增强其语义理解的Transformer模型。我们对代码执行进行评估，并展示了其有望的表现和局限性。我们还展示了它在代码智能任务中的潜在好处，如零-shot代码到代码搜索和文本到代码生成。我们的分析提供了有关预训练语言模型学习和泛化的洞见，并为代码智能的研究开辟了新方向。

    Code execution is a fundamental aspect of programming language semantics that reflects the exact behavior of the code. However, most pre-trained models for code intelligence ignore the execution trace and only rely on source code and syntactic structures. In this paper, we investigate how well pre-trained models can understand and perform code execution. We develop a mutation-based data augmentation technique to create a large-scale and realistic Python dataset and task for code execution, which challenges existing models such as Codex. We then present CodeExecutor, a Transformer model that leverages code execution pre-training and curriculum learning to enhance its semantic comprehension. We evaluate CodeExecutor on code execution and show its promising performance and limitations. We also demonstrate its potential benefits for code intelligence tasks such as zero-shot code-to-code search and text-to-code generation. Our analysis provides insights into the learning and generalization 
    
[^22]: 基于预训练语言模型和图神经网络的网页分类方法PLM-GNN

    PLM-GNN: A Webpage Classification Method based on Joint Pre-trained Language Model and Graph Neural Network. (arXiv:2305.05378v1 [cs.CL])

    [http://arxiv.org/abs/2305.05378](http://arxiv.org/abs/2305.05378)

    通过预训练语言模型和图神经网络的联合编码实现了基于文本和HTML DOM树的网页分类方法PLM-GNN，表现优秀。

    

    网页数量呈指数级增长，积累了大量的网络数据。在网络信息挖掘中，对网页进行分类是其中的关键过程。一些经典的方法是基于手动构建网页特征，并基于机器学习或深度学习训练分类器。然而，手动构建特征需要特定领域知识，并且通常需要很长时间验证特征的有效性。考虑到网页是由文本和HTML文档目标模型(DOM)树结合生成的，我们提出了一种基于预训练语言模型和图神经网络的表示和分类方法，称为PLM-GNN。它是基于对网页中文本和HTML DOM树的联合编码实现的。它在KI-04和SWDE数据集以及学者主页爬取项目的实际数据集AHS上表现良好。

    The number of web pages is growing at an exponential rate, accumulating massive amounts of data on the web. It is one of the key processes to classify webpages in web information mining. Some classical methods are based on manually building features of web pages and training classifiers based on machine learning or deep learning. However, building features manually requires specific domain knowledge and usually takes a long time to validate the validity of features. Considering webpages generated by the combination of text and HTML Document Object Model(DOM) trees, we propose a representation and classification method based on a pre-trained language model and graph neural network, named PLM-GNN. It is based on the joint encoding of text and HTML DOM trees in the web pages. It performs well on the KI-04 and SWDE datasets and on practical dataset AHS for the project of scholar's homepage crawling.
    
[^23]: 大型语言模型程序

    Large Language Model Programs. (arXiv:2305.05364v1 [cs.LG])

    [http://arxiv.org/abs/2305.05364](http://arxiv.org/abs/2305.05364)

    本文提出了一种将大型预训练语言模型嵌入算法或程序中，扩展其能力的方法。这种方法可以在未经微调的情况下通过更具算法性的方法获得不错的性能提升。

    

    近年来，大型预训练语言模型(LLMs)已经证明了它们能够通过几个示例来执行指令并执行新的任务的能力。通过这种在上下文示例中参数化LLMs的可能性，可以以比微调低得多的成本拓展它们的能力。我们扩展了这一推理线路，并提出了一种方法，通过将LLM嵌入算法或程序中，进一步扩展LLM的能力。为了证明这种方法的优点，我们提供了一个证据支持的问答的说明性例子。我们通过更具算法性的方法而没有任何微调，在通过一系列思路基线的基础上获得了6.4%的改进。此外，我们从这个角度突出了最近的工作，并讨论了与标准方法相比的优点和缺点。

    In recent years, large pre-trained language models (LLMs) have demonstrated the ability to follow instructions and perform novel tasks from a few examples. The possibility to parameterise an LLM through such in-context examples widens their capability at a much lower cost than finetuning. We extend this line of reasoning and present a method which further expands the capabilities of an LLM by embedding it within an algorithm or program. To demonstrate the benefits of this approach, we present an illustrative example of evidence-supported question-answering. We obtain a 6.4\% improvement over the chain of thought baseline through a more algorithmic approach without any finetuning. Furthermore, we highlight recent work from this perspective and discuss the advantages and disadvantages in comparison to the standard approaches.
    
[^24]: 基于基础模型的系统设计框架

    A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])

    [http://arxiv.org/abs/2305.05352](http://arxiv.org/abs/2305.05352)

    本文提出了一个基于基础模型的系统分类体系，分类和比较了基础模型和基于基础模型的系统的特点。它为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。

    

    最近推出了大型语言模型(LLM)的聊天机器人，如ChatGPT，这引起了人们对基础模型的广泛关注。基础模型被广泛认为将成为未来人工智能系统的基石。由于基础模型处于早期阶段，基于基础模型的系统设计尚未得到系统地探索。人们对在软件架构中引入基础模型的影响知之甚少。因此，在本文中，我们提出了一个基于基础模型的系统分类法，对基础模型和基于基础模型的系统的特点进行了分类和比较。我们的分类法包括三个类别：基础模型预训练和微调、基于基础模型的系统架构设计和负责任的AI设计。这个分类法为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。

    The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
    
[^25]: Rudolf Christoph Eucken在SemEval-2023任务4中：基于集合方法识别论据中人类价值的研究

    Rudolf Christoph Eucken at SemEval-2023 Task 4: An Ensemble Approach for Identifying Human Values from Arguments. (arXiv:2305.05335v1 [cs.CL])

    [http://arxiv.org/abs/2305.05335](http://arxiv.org/abs/2305.05335)

    本文提出了集合方法，利用三种模型从论据文本中检测人类价值，最佳组合在主要数据集上实现了总体F1分数为0.48的效果。

    

    我们通过经验获得微妙的人类价值观，这些价值观支配着我们的思维，并体现在我们的言辞中。在模拟人类行为的计算机系统中识别这些价值观变得至关重要。计算论证是处理人类论证能力的领域，可以通过识别这些价值观受益。鉴于此，我们提出了一种集合方法来从论据文本中检测人类价值。我们的集合包括三个模型：（i）一种基于蕴含关系的模型，用于确定基于其描述的人类价值，（ii）基于Roberta的分类器，可预测论点中人类价值的集合，（iii）基于Roberta的分类器，可以从论据预测缩小的人类价值集合。我们尝试了不同的模型组合方式并报告了结果。此外，我们最佳的组合在主要数据集上实现了总体F1分数为0.48的效果。

    The subtle human values we acquire through life experiences govern our thoughts and gets reflected in our speech. It plays an integral part in capturing the essence of our individuality and making it imperative to identify such values in computational systems that mimic human actions. Computational argumentation is a field that deals with the argumentation capabilities of humans and can benefit from identifying such values. Motivated by that, we present an ensemble approach for detecting human values from argument text. Our ensemble comprises three models: (i) An entailment-based model for determining the human values based on their descriptions, (ii) A Roberta-based classifier that predicts the set of human values from an argument. (iii) A Roberta-based classifier to predict a reduced set of human values from an argument. We experiment with different ways of combining the models and report our results. Furthermore, our best combination achieves an overall F1 score of 0.48 on the main 
    
[^26]: 具有几何信息瓶颈的可解释推荐系统

    Explainable Recommender with Geometric Information Bottleneck. (arXiv:2305.05331v1 [cs.IR])

    [http://arxiv.org/abs/2305.05331](http://arxiv.org/abs/2305.05331)

    该论文提出了一种新的可解释推荐系统模型，将从用户-商品交互中学得的几何先验知识与变分网络相结合，可以为用户提供既具备推荐性能又具有解释性能的解释推荐服务。

    

    可解释的推荐系统能够解释其推荐决策，增强用户对系统的信任。大多数可解释的推荐系统要么依赖于人工标注的原理来训练模型以生成解释，要么利用注意机制从评论中提取重要的文本段落作为解释。提取的原理往往局限于单个评论，可能无法识别评论文本之外的隐含特征。为了避免昂贵的人工注释过程并生成超出单个评论的解释，我们建议将从用户-商品交互中学得的几何先验知识与变分网络相结合，该网络从用户-商品评论中推断潜在因子。单个用户-商品对的潜在因子可用于推荐和解释生成，自然地继承了编码在先验知识中的全局特征。三个电子商务数据集上的实验结果表明，我们的模型在推荐性能和可解释性方面都具有竞争力。

    Explainable recommender systems can explain their recommendation decisions, enhancing user trust in the systems. Most explainable recommender systems either rely on human-annotated rationales to train models for explanation generation or leverage the attention mechanism to extract important text spans from reviews as explanations. The extracted rationales are often confined to an individual review and may fail to identify the implicit features beyond the review text. To avoid the expensive human annotation process and to generate explanations beyond individual reviews, we propose to incorporate a geometric prior learnt from user-item interactions into a variational network which infers latent factors from user-item reviews. The latent factors from an individual user-item pair can be used for both recommendation and explanation generation, which naturally inherit the global characteristics encoded in the prior knowledge. Experimental results on three e-commerce datasets show that our mo
    
[^27]: 利用transformer和集成技术在社交网络上检测抑郁症

    Detection of depression on social networks using transformers and ensembles. (arXiv:2305.05325v1 [cs.CL])

    [http://arxiv.org/abs/2305.05325](http://arxiv.org/abs/2305.05325)

    本文利用transformer和集成技术，在社交网络上检测抑郁症的迹象，构建了多个基于预训练语言模型的分类器和两种类型的集成模型，表现更好。

    

    随着科技在我们生活中的影响不断增强，社交媒体的使用也越来越普遍，它不仅是一种沟通工具，还可以用来向社区分享我们的观点和感受。对于抑郁症等心理健康问题，人们也会利用社交媒体来表达自己的想法寻求帮助。因此，我们可以通过自动处理社交媒体帖子并检测抑郁症的迹象来提供帮助。本文构建了大量的基于预训练语言模型的分类器，包括BERT、RoBERTA、BERTweet和mentalBERT，并构建了两种类型的集成模型。我们分析了模型在Reddit和Twitter两个社交平台上的表现，并研究了跨数据集的转移学习性能。结果表明，transformer集成模型比单一的transformer模型表现更好。

    As the impact of technology on our lives is increasing, we witness increased use of social media that became an essential tool not only for communication but also for sharing information with community about our thoughts and feelings. This can be observed also for people with mental health disorders such as depression where they use social media for expressing their thoughts and asking for help. This opens a possibility to automatically process social media posts and detect signs of depression. We build several large pre-trained language model based classifiers for depression detection from social media posts. Besides fine-tuning BERT, RoBERTA, BERTweet, and mentalBERT were also construct two types of ensembles. We analyze the performance of our models on two data sets of posts from social platforms Reddit and Twitter, and investigate also the performance of transfer learning across the two data sets. The results show that transformer ensembles improve over the single transformer-based
    
[^28]: 结构化情感分析作为基于转移的依存句法分析

    Structured Sentiment Analysis as Transition-based Dependency Parsing. (arXiv:2305.05311v1 [cs.CL])

    [http://arxiv.org/abs/2305.05311](http://arxiv.org/abs/2305.05311)

    本文提出了第一种将结构化情感分析作为依存句法分析处理的基于转移的方法，其基于Pointer Network体系结构，实现了在准确性和效率方面均优于以前提出的图形模型的结果，是迄今为止最为准确的SSA方法。

    

    结构化情感分析（SSA）旨在从自然语言文本中自动提取人们的观点，并以图形结构充分表示该信息。最近提出了一种最准确的执行SSA的方法，即将其视为依存句法分析任务。尽管我们可以在文献中发现基于转移的算法在依存句法分析的准确性和效率方面优于其他方法，但所有尝试采用这种方法解决SSA的方法都基于基于图形的模型。在本文中，我们提出了第一个将SSA作为依存句法分析处理的基于转移的方法。具体而言，我们设计了一个转移系统，以从左到右的方式处理输入文本，逐步生成包含所有识别出的观点的图形结构。为了有效地实现我们的最终基于转移的模型，我们借助了Pointer Network体系结构作为支撑。通过广泛的评估，我们证明了我们的模型超越了SSA的最新技术水平，在包括SemEval 2014 Task 4在内的四个基准数据集上取得了迄今为止报告的最高准确性。

    Structured sentiment analysis (SSA) aims to automatically extract people's opinions from a text in natural language and adequately represent that information in a graph structure. One of the most accurate methods for performing SSA was recently proposed and consists of approaching it as a dependency parsing task. Although we can find in the literature how transition-based algorithms excel in dependency parsing in terms of accuracy and efficiency, all proposed attempts to tackle SSA following that approach were based on graph-based models. In this article, we present the first transition-based method to address SSA as dependency parsing. Specifically, we design a transition system that processes the input text in a left-to-right pass, incrementally generating the graph structure containing all identified opinions. To effectively implement our final transition-based model, we resort to a Pointer Network architecture as a backbone. From an extensive evaluation, we demonstrate that our mod
    
[^29]: 完美的受害者：对性暴力受害者司法态度的计算分析

    The Perfect Victim: Computational Analysis of Judicial Attitudes towards Victims of Sexual Violence. (arXiv:2305.05302v1 [cs.CL])

    [http://arxiv.org/abs/2305.05302](http://arxiv.org/abs/2305.05302)

    这项研究使用计算模型和手动注释的数据集，评估以色列法院系统对性暴力受害者的司法态度。研究重点关注了“强奸神话”在对受害者可信度的司法评估中的作用。

    

    我们开发了计算模型，分析法庭陈述以评估以色列法院系统对性暴力受害者的司法态度。该研究考察了“强奸神话”在刑事司法系统对性犯罪的反应中的共鸣，特别是在对受害者可信度的司法评估中。我们首先制定了一个本体论来评估法官对受害者可信度的态度，具有八个序数标签和二元分类。其次，我们策划了一个手动注释的数据集，用于希伯来语中法官对受害者可信度的评估，以及一个可以从法院判决案件中提取可信度标签的模型。该数据集包括1990年至2021年的855个性侵犯案件的判决文书，由法律专家和受过训练的法学生帮助注释。该模型使用句法和潜在结构的组合方法，找到传达法官对受害者态度的句子，并对其进行分类。

    We develop computational models to analyze court statements in order to assess judicial attitudes toward victims of sexual violence in the Israeli court system. The study examines the resonance of "rape myths" in the criminal justice system's response to sex crimes, in particular in judicial assessment of victim's credibility. We begin by formulating an ontology for evaluating judicial attitudes toward victim's credibility, with eight ordinal labels and binary categorizations. Second, we curate a manually annotated dataset for judicial assessments of victim's credibility in the Hebrew language, as well as a model that can extract credibility labels from court cases. The dataset consists of 855 verdict decision documents in sexual assault cases from 1990-2021, annotated with the help of legal experts and trained law students. The model uses a combined approach of syntactic and latent structures to find sentences that convey the judge's attitude towards the victim and classify them accor
    
[^30]: 通过训练人工代码切换数据来提升零样本跨语言检索

    Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially Code-Switched Data. (arXiv:2305.05295v1 [cs.CL])

    [http://arxiv.org/abs/2305.05295](http://arxiv.org/abs/2305.05295)

    研究者提出训练排名模型的方法来提高跨语言检索的效率，该模型使用了人工代码切换的数据，并且实验表明在跨语言检索和多语言检索中会带来显著改进，在不影响单语检索的基础上，特别是对于远程语言之间的检索。

    

    将以英语为代表的高资源语言的信息检索（IR）模型以零样本方式迁移到其他语言已成为被广泛采用的方法。在本研究中，我们表明当查询和文档以不同语言存在时，零样本排名器的有效性会降低。出于这个原因，我们建议使用人工代码切换数据来训练排名模型，而我们生成这些数据是通过利用双语词表。为此，我们尝试了从（1）跨语言词嵌入和（2）平行维基百科页面标题得出的词表。我们使用mMARCO数据集对涵盖单语IR（MoIR）、跨语言IR（CLIR）和多语言IR（MLIR）的36种语言对的重排模型进行了广泛评估。我们的结果表明，代码切换可以在保持MoIR性能稳定的同时，在CLIR中产生5.1 MRR@10的一致和显著增益，以及在MLIR中产生3.9 MRR@10的增益。令人鼓舞的是，远程语言之间的增益特别显著。

    Transferring information retrieval (IR) models from a high-resource language (typically English) to other languages in a zero-shot fashion has become a widely adopted approach. In this work, we show that the effectiveness of zero-shot rankers diminishes when queries and documents are present in different languages. Motivated by this, we propose to train ranking models on artificially code-switched data instead, which we generate by utilizing bilingual lexicons. To this end, we experiment with lexicons induced from (1) cross-lingual word embeddings and (2) parallel Wikipedia page titles. We use the mMARCO dataset to extensively evaluate reranking models on 36 language pairs spanning Monolingual IR (MoIR), Cross-lingual IR (CLIR), and Multilingual IR (MLIR). Our results show that code-switching can yield consistent and substantial gains of 5.1 MRR@10 in CLIR and 3.9 MRR@10 in MLIR, while maintaining stable performance in MoIR. Encouragingly, the gains are especially pronounced for distan
    
[^31]: 通过布朗桥随机过程进行目标导向主动对话的对话规划

    Dialogue Planning via Brownian Bridge Stochastic Process for Goal-directed Proactive Dialogue. (arXiv:2305.05290v1 [cs.CL])

    [http://arxiv.org/abs/2305.05290](http://arxiv.org/abs/2305.05290)

    该论文提出了一种利用随机过程对话规划的方法，该方法通过布朗桥过程建模对话路径的时间动态以实现目标导向型对话系统并取得了良好的效果。

    

    目标导向型对话系统旨在通过多轮对话主动地达到预先确定的目标。实现此任务的关键在于规划对话路径，使其平稳并连贯地指向目标。然而，这是一项具有挑战性并未被深入探究的任务。在本文中，我们提出了一种连贯对话规划方法，它使用随机过程来建模对话路径的时间动态。我们定义了一个潜在空间，通过布朗桥过程捕捉了目标导向行为的连贯性，从而允许我们灵活地将用户反馈纳入对话规划。基于导出的潜在轨迹，我们使用预先训练的语言模型显式地生成对话路径。最后，我们将这些路径作为自然语言提示来引导对话生成。我们的实验表明，我们的方法生成了更连贯的话语，并以更高的成功率实现了目标。

    Goal-directed dialogue systems aim to proactively reach a pre-determined target through multi-turn conversations. The key to achieving this task lies in planning dialogue paths that smoothly and coherently direct conversations towards the target. However, this is a challenging and under-explored task. In this work, we propose a coherent dialogue planning approach that uses a stochastic process to model the temporal dynamics of dialogue paths. We define a latent space that captures the coherence of goal-directed behavior using a Brownian bridge process, which allows us to incorporate user feedback flexibly in dialogue planning. Based on the derived latent trajectories, we generate dialogue paths explicitly using pre-trained language models. We finally employ these paths as natural language prompts to guide dialogue generation. Our experiments show that our approach generates more coherent utterances and achieves the goal with a higher success rate.
    
[^32]: VCSUM：一个多功能的中文会议摘要数据集

    VCSUM: A Versatile Chinese Meeting Summarization Dataset. (arXiv:2305.05280v1 [cs.CL])

    [http://arxiv.org/abs/2305.05280](http://arxiv.org/abs/2305.05280)

    介绍了一个多功能的中文会议摘要数据集VCSum，包括239个现实生活中的会议，总时长超过230小时。该数据集可以适应各种摘要任务或方法，包括基于分割的摘要、多粒度摘要和检索-生成摘要。数据集和代码将在GitHub上发布。

    

    与新闻和聊天摘要相比，由于数据受限，会议摘要的发展受到极大的减速。为此，我们介绍了一个多功能的中文会议摘要数据集VCSum，包括239个现实生活中的会议，总时长超过230小时。我们声称我们的数据集是多功能的，因为我们为每个会议的文本提供了主题划分、头条、分段摘要、整个会议摘要和显要句子等注释。因此，该数据集可以适应各种摘要任务或方法，包括基于分割的摘要、多粒度摘要和检索-生成摘要。我们的分析证实了VCSum的有效性和稳健性。我们还提供了一组关于不同下游摘要任务的基准模型，以便进一步研究VCSum。数据集和代码将在 \url{https://github.com/hahahawu/VCSum} 上发布。

    Compared to news and chat summarization, the development of meeting summarization is hugely decelerated by the limited data. To this end, we introduce a versatile Chinese meeting summarization dataset, dubbed VCSum, consisting of 239 real-life meetings, with a total duration of over 230 hours. We claim our dataset is versatile because we provide the annotations of topic segmentation, headlines, segmentation summaries, overall meeting summaries, and salient sentences for each meeting transcript. As such, the dataset can adapt to various summarization tasks or methods, including segmentation-based summarization, multi-granularity summarization and retrieval-then-generate summarization. Our analysis confirms the effectiveness and robustness of VCSum. We also provide a set of benchmark models regarding different downstream summarization tasks on VCSum to facilitate further research. The dataset and code will be released at \url{https://github.com/hahahawu/VCSum}.
    
[^33]: 神经变换器中的鲁棒性语音和语义上下文偏置

    Robust Acoustic and Semantic Contextual Biasing in Neural Transducers for Speech Recognition. (arXiv:2305.05271v1 [cs.CL])

    [http://arxiv.org/abs/2305.05271](http://arxiv.org/abs/2305.05271)

    本文提出了一种使用轻量级字符表示编码细粒度发音特征的方法，称为声学偏置，以提高针对声音相似性引导的上下文偏置，在神经变换器等自动语音识别系统性能中重要的改进。

    

    基于注意力机制的上下文偏置方法已经在端到端自动语音识别（E2E ASR）系统中，如神经变换器中，展现出重要的改进，特别是对于大众的或个性化的罕见词的识别更是如此。这些方法采用交叉注意力来将模型偏置于注入为偏置短语的特定上下文实体。先前的方法通常依赖于子单词编码器来编码偏置短语。然而，子单词标记粗糙，无法捕捉关键的发音信息，这对于基于声音相似性的偏置至关重要。在这项工作中，我们提议使用轻量级字符表示来编码细粒度的发音特征，以改善受声音相似性引导的上下文偏置（称为声学偏置）。我们进一步整合预训练的基于神经语言模型(NLM)的编码器，将话语的语义上下文与上下文实体一起编码以提高下文偏置性能。

    Attention-based contextual biasing approaches have shown significant improvements in the recognition of generic and/or personal rare-words in End-to-End Automatic Speech Recognition (E2E ASR) systems like neural transducers. These approaches employ cross-attention to bias the model towards specific contextual entities injected as bias-phrases to the model. Prior approaches typically relied on subword encoders for encoding the bias phrases. However, subword tokenizations are coarse and fail to capture granular pronunciation information which is crucial for biasing based on acoustic similarity. In this work, we propose to use lightweight character representations to encode fine-grained pronunciation features to improve contextual biasing guided by acoustic similarity between the audio and the contextual entities (termed acoustic biasing). We further integrate pretrained neural language model (NLM) based encoders to encode the utterance's semantic context along with contextual entities to
    
[^34]: 实体边界干扰: 一种攻击命名实体识别的方法

    Attack Named Entity Recognition by Entity Boundary Interference. (arXiv:2305.05253v1 [cs.CL])

    [http://arxiv.org/abs/2305.05253](http://arxiv.org/abs/2305.05253)

    本文提出了一种基于实体边界干扰的新型单词修改NER攻击——虚拟边界攻击(ViBA)，该攻击在四个基准数据集上攻击最先进的NER模型时显示出了非常有效的结果。

    

    命名实体识别(NER)是NLP任务的基石，但其稳健性鲜受关注。本文重新思考了从句子分类派生的NER攻击原则，因为它们可以轻易违反原始和对抗性NER示例之间的标签一致性。这是由于NER的精细化性质，即句子中即使是微小的单词变化也会导致任何实体的出现或变异，从而导致无效的对抗性示例。为此，我们提出了一种基于NER实体边界位置的关键洞察的新型单词修改NER攻击。我们故意插入新的边界到句子中，触发实体边界干扰，在这个边界词或句子中的其他单词上，使受害模型做出错误预测。我们称这种攻击为虚拟边界攻击(ViBA)，在四个基准数据集上攻击最先进的NER模型时显示出了非常有效的结果。

    Named Entity Recognition (NER) is a cornerstone NLP task while its robustness has been given little attention. This paper rethinks the principles of NER attacks derived from sentence classification, as they can easily violate the label consistency between the original and adversarial NER examples. This is due to the fine-grained nature of NER, as even minor word changes in the sentence can result in the emergence or mutation of any entities, resulting in invalid adversarial examples. To this end, we propose a novel one-word modification NER attack based on a key insight, NER models are always vulnerable to the boundary position of an entity to make their decision. We thus strategically insert a new boundary into the sentence and trigger the Entity Boundary Interference that the victim model makes the wrong prediction either on this boundary word or on other words in the sentence. We call this attack Virtual Boundary Attack (ViBA), which is shown to be remarkably effective when attackin
    
[^35]: 从大型语言模型中提取脚本知识以进行受限语言规划

    Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v1 [cs.CL])

    [http://arxiv.org/abs/2305.05252](http://arxiv.org/abs/2305.05252)

    本文首次定义了受限语言规划任务，提出了一种方法来提高大型语言模型在这个任务中的表现，并提取了一个新颖的受限语言规划数据集。实验证明该方法显著提高了其在约束忠实度方面的能力，并对赋予较小的语言模型受限语言规划能力非常有效。

    

    在日常生活中，人们经常通过遵循目标导向的脚本形式的逐步说明来规划自己的行动。以往的工作利用语言模型（LM）来为立体活动的抽象目标（例如，“制作蛋糕”）进行规划，但对于具有多方面约束的更具体目标（例如，“为糖尿病患者制作蛋糕”）鲜有研究。本文首次定义了受限语言规划任务。我们提出了一种过度生成并过滤的方法来改善大型语言模型（LLM）在这个任务中的表现，并利用它来提取一种新颖的受限语言规划数据集CoScript，其中包括55,000个脚本。实验证明，我们的方法显著提高了LLM在受限语言规划方面的能力，特别是在约束忠实度方面。此外，CoScript被证明对赋予较小的LM受限语言规划能力是非常有效的。

    In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., "make a cake"), but leaves more specific goals with multi-facet constraints understudied (e.g., "make a cake for diabetics"). In this paper, we define the task of constrained language planning for the first time. We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, CoScript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.
    
[^36]: 文本图像机器翻译多教师知识蒸馏

    Multi-Teacher Knowledge Distillation For Text Image Machine Translation. (arXiv:2305.05226v1 [cs.CL])

    [http://arxiv.org/abs/2305.05226](http://arxiv.org/abs/2305.05226)

    本文提出了一种多教师知识蒸馏方法，可以将知识有效地蒸馏到管道模型中并传递给端到端TIMT模型，从而提高性能。

    

    文本图像机器翻译（TIMT）已被广泛应用于各种实际应用程序中，它将图像中的源语言文本翻译成另一种目标语言句子。TIMT的现有方法主要分为两种类别：识别-然后-翻译流程模型和端到端模型。然而，如何从管道模型向端到端模型传递知识仍然是一个未解决的问题。在本文中，我们提出了一种新的多教师知识蒸馏（MTKD）方法，可以有效地将知识蒸馏到管道模型中并传递给端到端TIMT模型。具体而言，利用三个教师来提高端到端TIMT模型的性能。端到端TIMT模型中的图像编码器使用识别教师编码器的知识蒸馏指导进行优化，而顺序编码器和解码器则通过从翻译顺序和解码器教师模型传递知识来改善性能。

    Text image machine translation (TIMT) has been widely used in various real-world applications, which translates source language texts in images into another target language sentence. Existing methods on TIMT are mainly divided into two categories: the recognition-then-translation pipeline model and the end-to-end model. However, how to transfer knowledge from the pipeline model into the end-to-end model remains an unsolved problem. In this paper, we propose a novel Multi-Teacher Knowledge Distillation (MTKD) method to effectively distillate knowledge into the end-to-end TIMT model from the pipeline model. Specifically, three teachers are utilized to improve the performance of the end-to-end TIMT model. The image encoder in the end-to-end TIMT model is optimized with the knowledge distillation guidance from the recognition teacher encoder, while the sequential encoder and decoder are improved by transferring knowledge from the translation sequential and decoder teacher models. Furthermo
    
[^37]: 利用词汇相似性实现极低资源语言的零样本机器翻译

    Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages. (arXiv:2305.05214v1 [cs.CL])

    [http://arxiv.org/abs/2305.05214](http://arxiv.org/abs/2305.05214)

    本文解决了极低资源语言到英语的机器翻译任务，利用密切相关的高资源语言的词汇相似性，注入噪声作为正则化器，使模型更能抵御词汇差异，从而更好地促进跨语言转移。

    

    我们解决了从极低资源语言（LRL）到英语的机器翻译任务，采用从密切相关的高资源语言（HRL）进行跨语言转移。对于许多这些语言，没有平行语料库可用，即使是单语料库也很有限，并且在预训练的序列到序列模型中表示也缺失。这些因素限制了从多语言模型中共享嵌入空间的跨语言转移的好处。然而，许多极低资源语言与相关的高资源语言具有很高的词汇相似性。我们利用这个属性，将字符和字符跨度的噪声注入到HRL的训练数据中，然后再学习词汇表。这作为一个正则化器，使模型更能抵御HRL和LRL之间的词汇差异，并更好地促进跨语言转移。在来自多个语言家族的密切相关的HRL和LRL对上，我们观察到我们的方法显著优于基线机器翻译模型。

    We address the task of machine translation from an extremely low-resource language (LRL) to English using cross-lingual transfer from a closely related high-resource language (HRL). For many of these languages, no parallel corpora are available, even monolingual corpora are limited and representations in pre-trained sequence-to-sequence models are absent. These factors limit the benefits of cross-lingual transfer from shared embedding spaces in multilingual models. However, many extremely LRLs have a high level of lexical similarity with related HRLs. We utilize this property by injecting character and character-span noise into the training data of the HRL prior to learning the vocabulary. This serves as a regularizer which makes the model more robust to lexical divergences between the HRL and LRL and better facilitates cross-lingual transfer. On closely related HRL and LRL pairs from multiple language families, we observe that our method significantly outperforms the baseline MT as we
    
[^38]: 探究语言依赖性在日语自监督语音表示模型中的应用

    Exploration of Language Dependency for Japanese Self-Supervised Speech Representation Models. (arXiv:2305.05201v1 [cs.CL])

    [http://arxiv.org/abs/2305.05201](http://arxiv.org/abs/2305.05201)

    本文研究了跨语言和单语言模型在日语自动语音识别任务中的表现，研究了日语中未标注数据相对于跨语言预先训练模型的需求，同时研究了自监督学习在日语中的有效性，并展示了最先进的性能。

    

    自监督学习在单语言和跨语言环境中都取得了巨大的成功。然而，由于这两种情况通常是分开研究的，对于跨语言模型和单语言模型的比较效果的研究却很少。在本文中，我们通过日语自动语音识别任务的实证研究来探讨这个基本问题。首先，在尽可能保持声学域相同的情况下，比较跨语言和单语言模型在两种不同语言任务的ASR性能。然后，我们研究日语中收集到的多少未标注数据可以达到与预先通过数万小时的英语和/或多语言数据预训练的跨语言模型相当的性能。最后，我们广泛研究了自监督学习在日语中的有效性，并在多个ASR任务中展示了最先进的性能。

    Self-supervised learning (SSL) has been dramatically successful not only in monolingual but also in cross-lingual settings. However, since the two settings have been studied individually in general, there has been little research focusing on how effective a cross-lingual model is in comparison with a monolingual model. In this paper, we investigate this fundamental question empirically with Japanese automatic speech recognition (ASR) tasks. First, we begin by comparing the ASR performance of cross-lingual and monolingual models for two different language tasks while keeping the acoustic domain as identical as possible. Then, we examine how much unlabeled data collected in Japanese is needed to achieve performance comparable to a cross-lingual model pre-trained with tens of thousands of hours of English and/or multilingual data. Finally, we extensively investigate the effectiveness of SSL in Japanese and demonstrate state-of-the-art performance on multiple ASR tasks. Since there is no c
    
[^39]: COLA: 因果推断角度下的情境化常识因果推理

    COLA: Contextualized Commonsense Causal Reasoning from the Causal Inference Perspective. (arXiv:2305.05191v1 [cs.CL])

    [http://arxiv.org/abs/2305.05191](http://arxiv.org/abs/2305.05191)

    本文提出了一个从因果推断角度出发的情境化常识因果推理任务，设计了一个零-shot框架COLA来解决此任务，并且该框架可以更准确地检测常识因果关系。

    

    检测事件之间的常识因果关系（因果关系）长期以来是一项基本而具有挑战性的任务。鉴于事件的复杂性，一个事件在不同的上下文中可能有不同的原因。因此，利用上下文在检测因果关系中发挥了关键作用。同时，先前关于常识因果关系的工作只考虑两个事件并忽略它们的上下文，简化了任务的制定。本文提出了一项新任务，即检测事件序列（即上下文）中两个事件之间的常识因果关系，称为情境化常识因果推理。我们还从因果推断的角度设计了一个零-shot框架：COLA（情境化常识因果推理器）来解决这个任务。这个框架从时间性获得了丰富的偶发监督，并平衡了来自多个时间戳的协变量以消除混淆效应。我们广泛的实验表明，与基线相比，COLA可以更准确地检测常识因果关系。

    Detecting commonsense causal relations (causation) between events has long been an essential yet challenging task. Given that events are complicated, an event may have different causes under various contexts. Thus, exploiting context plays an essential role in detecting causal relations. Meanwhile, previous works about commonsense causation only consider two events and ignore their context, simplifying the task formulation. This paper proposes a new task to detect commonsense causation between two events in an event sequence (i.e., context), called contextualized commonsense causal reasoning. We also design a zero-shot framework: COLA (Contextualized Commonsense Causality Reasoner) to solve the task from the causal inference perspective. This framework obtains rich incidental supervision from temporality and balances covariates from multiple timestamps to remove confounding effects. Our extensive experiments show that COLA can detect commonsense causality more accurately than baselines
    
[^40]: SUR-adapter：用大型语言模型增强文本-图像预训练扩散模型

    SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models. (arXiv:2305.05189v1 [cs.CL])

    [http://arxiv.org/abs/2305.05189](http://arxiv.org/abs/2305.05189)

    本文提出了一个名为SUR-adapter的微调方法，用于增强预先训练的文本到图像扩散模型的语义理解和常识推理能力，以便在生成图片时使用简短的叙述提示。作者还构建了一个新的数据集SURD，并使用大型语言模型的知识进行了优化。

    

    扩散模型是目前流行的文本到图像生成模型，可以通过文本提示生成具有高质量和内容丰富度的图像。但是，当输入的提示为简短的叙述时，现有模型在语义理解和常识推理方面存在一定限制，导致图像生成的质量较低。为了提高叙述提示的能力，我们提出了一种简单而有效的参数高效的微调方法，称为Semantic Understanding和Reasoning adapter（SUR-adapter），用于预先训练的扩散模型。为实现这一目标，我们首先收集和注释一个新的数据集SURD，其中包含超过57,000个语义修正的多模态样本。每个样本都包含一个简单的叙述提示，一个复杂的基于关键字的提示和一个高质量的图像。然后，我们将叙述提示的语义表示与复杂提示对齐，并通过大型语言模型的知识将其转移至我们的SUR-adapter中。

    Diffusion models, which have emerged to become popular text-to-image generation models, can produce high-quality and content-rich images guided by textual prompts. However, there are limitations to semantic understanding and commonsense reasoning in existing models when the input prompts are concise narrative, resulting in low-quality image generation. To improve the capacities for narrative prompts, we propose a simple-yet-effective parameter-efficient fine-tuning approach called the Semantic Understanding and Reasoning adapter (SUR-adapter) for pre-trained diffusion models. To reach this goal, we first collect and annotate a new dataset SURD which consists of more than 57,000 semantically corrected multi-modal samples. Each sample contains a simple narrative prompt, a complex keyword-based prompt, and a high-quality image. Then, we align the semantic representation of narrative prompts to the complex prompts and transfer knowledge of large language models (LLMs) to our SUR-adapter vi
    
[^41]: CSED: 一个中文语义错误诊断语料库

    CSED: A Chinese Semantic Error Diagnosis Corpus. (arXiv:2305.05183v1 [cs.CL])

    [http://arxiv.org/abs/2305.05183](http://arxiv.org/abs/2305.05183)

    本文建立了一个中文语义错误诊断语料库CSED，通过提出基于句法的模型实现了CSED-R和CSED-C任务的最佳表现。

    

    最近，中文文本错误纠正的工作大多集中在中文拼写检查（CSC）和中文语法错误诊断（CGED）上。相比之下，对于缺乏相关数据集的中文语义错误诊断（CSED）这一复杂问题，却没有得到足够的关注。研究语义错误很重要，因为它们很常见，并且可能导致句法不规则甚至理解问题。为了研究这个问题，我们建立了CSED语料库，其中包括两个数据集。一个是用于CSED识别（CSED-R）任务，另一个是用于CSED更正（CSED-C）任务。我们的注释通过质量保证机制保证了高质量的数据。我们的实验表明，强大的预训练模型在这个语料库上表现不佳。我们还发现，CSED任务具有挑战性，因为即使是人类得分也很低。本文提出了基于句法的模型，专门针对CSED任务进行适应。实验结果表明，我们提出的模型在CSED-R和CSED-C任务上均取得了最新的最佳表现，超过了以前的最佳模型。

    Recently, much Chinese text error correction work has focused on Chinese Spelling Check (CSC) and Chinese Grammatical Error Diagnosis (CGED). In contrast, little attention has been paid to the complicated problem of Chinese Semantic Error Diagnosis (CSED), which lacks relevant datasets. The study of semantic errors is important because they are very common and may lead to syntactic irregularities or even problems of comprehension. To investigate this, we build the CSED corpus, which includes two datasets. The one is for the CSED-Recognition (CSED-R) task. The other is for the CSED-Correction (CSED-C) task. Our annotation guarantees high-quality data through quality assurance mechanisms. Our experiments show that powerful pre-trained models perform poorly on this corpus. We also find that the CSED task is challenging, as evidenced by the fact that even humans receive a low score. This paper proposes syntax-aware models to specifically adapt to the CSED task. The experimental results sho
    
[^42]: MoT：预思考和回忆功能使 ChatGPT 在“思想记忆”中自我进化

    MoT: Pre-thinking and Recalling Enable ChatGPT to Self-Improve with Memory-of-Thoughts. (arXiv:2305.05181v1 [cs.CL])

    [http://arxiv.org/abs/2305.05181](http://arxiv.org/abs/2305.05181)

    本文提出了一个名为 MoT 的框架，让大型语言模型通过“思想记忆”自我进化，无需注释数据集和参数更新。实验表明，该框架可以显著提高 ChatGPT 在数学推理、常识推理、事实推理和自然语言推理方面的能力。

    

    大型语言模型在各种任务上表现出了惊人的能力。但要实现它们的根本性改进，需要高质量的数据集或计算昂贵的微调。相反，人类可以通过思考和记忆轻松提高自我水平，而不需要外部资源。在本文中，我们提出了一个框架 MoT，在没有注释数据集和参数更新的情况下，通过思想记忆让大型语言模型自我进化。具体而言，该框架分为两个阶段：1. 在测试阶段之前，我们让大型语言模型在未加标签的数据集上进行预思考，并将高置信度的想法保存为外部记忆。2. 在推理过程中，给定一个测试问题，我们让大型语言模型回忆相关的记忆，帮助自己进行推理和回答。实验结果表明，所提出的框架可以帮助 ChatGPT 在数学推理、常识推理、事实推理和自然语言推理方面显著提高其能力。进一步的分析表明，每个组件都发挥了作用。

    Large Language Models have shown impressive abilities on various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, human can easily improve themselves by thinking and memory, without external resources. In this paper, we propose a framework, MoT, to let the LLM self-improve through Memory of Thoughts, without annotated datasets and parameter updates. Specifically, the framework is divided into two stages: 1. before the test stage, we let the LLM pre-think on the unlabeled dataset and save the high-confidence thoughts as external memory; 2. during inference, given a test question, we let the LLM recall relevant memory to help itself reason and answer it. Experimental results show that the proposed framework can help ChatGPT significantly improve its abilities in math reasoning, commonsense reasoning, factual reasoning and natural language inference. Further analyses show that each component contribute
    
[^43]: FrugalGPT: 如何在降低成本和提高性能的同时使用大型语言模型

    FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance. (arXiv:2305.05176v1 [cs.LG])

    [http://arxiv.org/abs/2305.05176](http://arxiv.org/abs/2305.05176)

    本论文介绍了如何在使用大型语言模型的同时降低成本和提高性能。作者提出了三种策略，包括提示适应，LLM近似和LLM级联，并且通过 FrugalGPT 这种方法，实现了大大降低成本或是提高准确率的效果。

    

    目前有越来越多的用户可以使用付费的大型语言模型（LLM）进行查询。我们回顾了查询流行的LLM API（例如GPT-4，ChatGPT，J1-Jumbo）涉及的成本，发现这些模型具有异构的价格结构，费用可能相差数个数量级。特别是在大量查询和文本的情况下使用LLM可能会很昂贵。因此，我们总结和讨论了三种策略，用户可以利用这些策略来减少使用LLM的汇编成本：1）提示适应，2）LLM近似和3）LLM级联。作为示例，我们提出了FrugalGPT，它是LLM级联的一个简单而灵活的实例，可以学习使用哪些LLM组合来处理不同查询，以降低成本、提高准确性。我们的实验表明，FrugalGPT可以在仅使用费用的98％或与GPT-4相同的成本下，达到最佳单个LLM的性能（例如GPT-4），或者以4％的准确率提高GPT-4的性能。

    There is a rapidly growing number of large language models (LLMs) that users can query for a fee. We review the cost associated with querying popular LLM APIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have heterogeneous pricing structures, with fees that can differ by two orders of magnitude. In particular, using LLMs on large collections of queries and text can be expensive. Motivated by this, we outline and discuss three types of strategies that users can exploit to reduce the inference cost associated with using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As an example, we propose FrugalGPT, a simple yet flexible instantiation of LLM cascade which learns which combinations of LLMs to use for different queries in order to reduce cost and improve accuracy. Our experiments show that FrugalGPT can match the performance of the best individual LLM (e.g. GPT-4) with up to 98% cost reduction or improve the accuracy over GPT-4 by 4% with the same co
    
[^44]: 精确控制长度的摘要生成

    Summarization with Precise Length Control. (arXiv:2305.05171v1 [cs.CL])

    [http://arxiv.org/abs/2305.05171](http://arxiv.org/abs/2305.05171)

    本文提出了一种精确控制长度的摘要生成框架，通过联合训练模型预测长度并生成长度最佳的摘要，在保持文本质量的同时提高了性能。

    

    许多文本生成应用，如摘要，需要精确控制文本长度。现有的控制长度的摘要方法要么表现下降，要么只能近似控制长度。在本文中，我们提出了一个框架，用于生成具有指定标记或句子数量的摘要，同时保持或甚至提高文本质量。此外，我们联合训练模型来预测长度，因此我们的模型可以生成长度最佳的摘要。我们在CNNDM数据集上评估了所提出的框架，并展示了与现有方法相比的改进性能。

    Many applications of text generation such as summarization benefit from accurately controlling the text length. Existing approaches on length-controlled summarization either result in degraded performance or can only control the length approximately. In this work, we present a framework to generate summaries with precisely the specified number of tokens or sentences, while maintaining or even improving the text quality. In addition, we jointly train the models to predict the lengths, so our model can generate summaries with optimal length. We evaluate the proposed framework on the CNNDM dataset and show improved performance compared to existing methods.
    
[^45]: E2TIMT: 高效有效的文图机器翻译模态适配器

    E2TIMT: Efficient and Effective Modal Adapter for Text Image Machine Translation. (arXiv:2305.05166v1 [cs.CL])

    [http://arxiv.org/abs/2305.05166](http://arxiv.org/abs/2305.05166)

    本文提出了一种高效有效的文图机器翻译模态适配器，利用现有OCR和MT数据库和新型模态适配器将OCR编码器和MT解码器连接，使得端到端TIMT模型在翻译质量和效率方面优于现有的两阶段级联模型和其他最先进的一级端到端模型。

    

    文本图像机器翻译旨在将嵌入图像中的文本从一种源语言翻译成另一种目标语言。现有方法，无论是两阶段级联还是一级端到端架构，都存在不同的问题。级联模型可以受益于大规模的光学字符识别（OCR）和 MT 数据集，但两阶段架构则是冗余的。端到端模型是高效的，但遭受训练数据不足的困扰。因此，在本文中，我们提出了一种端到端TIMT模型，充分利用现有OCR和MT数据集中的知识，追求有效和高效的框架。更具体地说，我们建立了一个新型模态适配器，有效地连接OCR编码器和MT解码器。同时使用端到端TIMT损失和跨模态对比损失来对齐OCR和MT任务的特征分布。大量实验表明，所提出的方法在翻译质量和效率方面优于现有的两阶段级联模型和其他最先进的一级端到端模型。

    Text image machine translation (TIMT) aims to translate texts embedded in images from one source language to another target language. Existing methods, both two-stage cascade and one-stage end-to-end architectures, suffer from different issues. The cascade models can benefit from the large-scale optical character recognition (OCR) and MT datasets but the two-stage architecture is redundant. The end-to-end models are efficient but suffer from training data deficiency. To this end, in our paper, we propose an end-to-end TIMT model fully making use of the knowledge from existing OCR and MT datasets to pursue both an effective and efficient framework. More specifically, we build a novel modal adapter effectively bridging the OCR encoder and MT decoder. End-to-end TIMT loss and cross-modal contrastive loss are utilized jointly to align the feature distribution of the OCR and MT tasks. Extensive experiments show that the proposed method outperforms the existing two-stage cascade models and o
    
[^46]: 通过标签内部对齐实现有效的医疗代码预测

    Effective Medical Code Prediction via Label Internal Alignment. (arXiv:2305.05162v1 [cs.LG])

    [http://arxiv.org/abs/2305.05162](http://arxiv.org/abs/2305.05162)

    本文提出了一种通过多视角注意力机制的神经网络，以在临床文本中预测医疗代码，并在标签空间与临床文本之间进行对齐，实现了对先前技术水平的提升。

    

    临床记录通常是由医生输入系统的。这些记录需要标记标准的医疗代码，而每个代码代表一种诊断或医疗治疗程序。对这些记录进行注释费时且容易出错。本文提出了一种多视角注意力机制的神经网络，以从临床文本中预测医疗代码。我们的方法结合了三个信息方面：临床文本的语义上下文，标签（医疗代码）空间之间的关系以及每个临床文本和医疗代码之间的对齐。实验结果表明，我们的方法在多个指标上实现了对先前的技术水平的提升。

    The clinical notes are usually typed into the system by physicians. They are typically required to be marked by standard medical codes, and each code represents a diagnosis or medical treatment procedure. Annotating these notes is time consuming and prone to error. In this paper, we proposed a multi-view attention based Neural network to predict medical codes from clinical texts. Our method incorporates three aspects of information, the semantic context of the clinical text, the relationship among the label (medical codes) space, and the alignment between each pair of a clinical text and medical code. Our method is verified to be effective on the open source dataset. The experimental result shows that our method achieves better performance against the prior state-of-art on multiple metrics.
    
[^47]: 读取、诊断和聊天：面向社交媒体中可解释和交互的LLMs增强抑郁症检测。

    Read, Diagnose and Chat: Towards Explainable and Interactive LLMs-Augmented Depression Detection in Social Media. (arXiv:2305.05138v1 [cs.CL])

    [http://arxiv.org/abs/2305.05138](http://arxiv.org/abs/2305.05138)

    本文提出一种基于LLMs的新型抑郁症检测系统，它通过自然语言对话提供了诊断证据和个性化建议，相较于传统方法性能更好。

    

    本文提出了一种基于LLMs的新型抑郁症检测系统，既具有可解释性又具有互动性。它不仅提供诊断结果，还根据与用户的自然语言对话，提供诊断证据和个性化建议。我们解决了大量文本处理和集成专业诊断标准等方面的挑战。我们的系统在各种设置下优于传统方法，并通过案例研究进行了演示。

    This paper proposes a new depression detection system based on LLMs that is both interpretable and interactive. It not only provides a diagnosis, but also diagnostic evidence and personalized recommendations based on natural language dialogue with the user. We address challenges such as the processing of large amounts of text and integrate professional diagnostic criteria. Our system outperforms traditional methods across various settings and is demonstrated through case studies.
    
[^48]: 利用ChatGPT生成网络钓鱼攻击

    Generating Phishing Attacks using ChatGPT. (arXiv:2305.05133v1 [cs.CR])

    [http://arxiv.org/abs/2305.05133](http://arxiv.org/abs/2305.05133)

    本文发现使用ChatGPT生成恶意提示可以生成功能性的网络钓鱼网站来模仿流行品牌并模拟多种规避策略，这些攻击可以使用普通的ChatGPT生成，无需使用任何先前的对抗性攻击（越狱）。

    

    ChatGPT的人类回答生成和上下文理解能力使其成为了对话代理、内容创作、数据分析以及研究和创新的热门工具。但其有效性和易用性也使其成为生成恶意内容的主要工具，例如网络钓鱼攻击，这会使用户处于危险之中。在本文中，我们识别了可提供给ChatGPT的多种恶意提示，以生成功能性的网络钓鱼网站。通过迭代式方法，我们发现这些钓鱼网站可以模仿流行品牌并模拟多种规避策略，这些策略已被知道可以避免反网络钓鱼机构的检测。这些攻击可以使用普通的ChatGPT生成，无需使用任何先前的对抗性攻击（越狱）。

    The ability of ChatGPT to generate human-like responses and understand context has made it a popular tool for conversational agents, content creation, data analysis, and research and innovation. However, its effectiveness and ease of accessibility makes it a prime target for generating malicious content, such as phishing attacks, that can put users at risk. In this work, we identify several malicious prompts that can be provided to ChatGPT to generate functional phishing websites. Through an iterative approach, we find that these phishing websites can be made to imitate popular brands and emulate several evasive tactics that have been known to avoid detection by anti-phishing entities. These attacks can be generated using vanilla ChatGPT without the need of any prior adversarial exploits (jailbreaking).
    
[^49]: 谁需要解码器？高效预测序列级属性。（arXiv:2305.05098v1 [cs.LG]）

    Who Needs Decoders? Efficient Estimation of Sequence-level Attributes. (arXiv:2305.05098v1 [cs.LG])

    [http://arxiv.org/abs/2305.05098](http://arxiv.org/abs/2305.05098)

    研究提出了非自回归代理模型(NAP)，通过编码序列直接预测通用标量值序列级属性，可以高效地实现解码步骤的规避。在机器翻译和语音识别两个场景下，NAP分别可以优于深度集成和高精度地预测性能指标。

    

    现代化序列到序列的模型通常需要自回归解码，这往往非常消耗资源。然而，对于某些下游任务，例如越界检测和资源分配，实际解码输出并不需要，只需要一个序列的标量属性。在这些场景下，知道系统输出质量以预测性能较差比知道输出本身更为重要，那么是否可以绕过自回归解码？我们提出了非自回归代理（NAP）模型，可以高效地预测通用标量值序列级属性。重要的是，NAP直接从编码预测这些指标，避免了昂贵的自回归解码阶段。我们考虑了两个序列到序列任务：机器翻译（MT）和语音识别（ASR）。在MT的越界检测中，NAP表现优于深度集成，同时速度显著更快。NAP也被证明能够高准确度地预测ASR的性能指标，例如词错误率。我们的发现表明，在属性可以从编码中直接预测的任务中，NAP为传统基于解码的方法提供了高效的替代方案。

    State-of-the-art sequence-to-sequence models often require autoregressive decoding, which can be highly expensive. However, for some downstream tasks such as out-of-distribution (OOD) detection and resource allocation, the actual decoding output is not needed just a scalar attribute of this sequence. In these scenarios, where for example knowing the quality of a system's output to predict poor performance prevails over knowing the output itself, is it possible to bypass the autoregressive decoding? We propose Non-Autoregressive Proxy (NAP) models that can efficiently predict general scalar-valued sequence-level attributes. Importantly, NAPs predict these metrics directly from the encodings, avoiding the expensive autoregressive decoding stage. We consider two sequence-to-sequence task: Machine Translation (MT); and Automatic Speech Recognition (ASR). In OOD for MT, NAPs outperform a deep ensemble while being significantly faster. NAPs are also shown to be able to predict performance me
    
[^50]: 大型文本集合中的交互式概念学习用于揭示潜在主题

    Interactive Concept Learning for Uncovering Latent Themes in Large Text Collections. (arXiv:2305.05094v1 [cs.CL])

    [http://arxiv.org/abs/2305.05094](http://arxiv.org/abs/2305.05094)

    本研究提出了一种交互式框架，用于在大型文本集合中揭示潜在的、被领域专家视为相关的概念，既实现了自动化又减少了手动编码的工作量。

    

    跨越不同学科领域的专家们通常有兴趣理解大型文本集合。传统上，这个挑战可以通过嘈杂的无监督技术（如主题模型）或手动主题发现流程来处理。在本文中，我们扩展了主题的定义，不仅考虑词分布，还包括被领域专家视为相关的概念。然后，我们提出了一个交互式框架，可以在不同的抽象级别上接收和编码专家反馈。我们的框架在自动化和手动编码之间取得平衡，允许专家控制他们的研究，同时减少所需的手动工作量。

    Experts across diverse disciplines are often interested in making sense of large text collections. Traditionally, this challenge is approached either by noisy unsupervised techniques such as topic models, or by following a manual theme discovery process. In this paper, we expand the definition of a theme to account for more than just a word distribution, and include generalized concepts deemed relevant by domain experts. Then, we propose an interactive framework that receives and encodes expert feedback at different levels of abstraction. Our framework strikes a balance between automation and manual coding, allowing experts to maintain control of their study while reducing the manual effort required.
    
[^51]: NLP中处理新颖性检测和适应性的统一评估框架，以作者归属为例

    A Unified Evaluation Framework for Novelty Detection and Accommodation in NLP with an Instantiation in Authorship Attribution. (arXiv:2305.05079v1 [cs.CL])

    [http://arxiv.org/abs/2305.05079](http://arxiv.org/abs/2305.05079)

    本文提出了一个新颖性检测和适应性的统一评估框架，以作者归属任务为例进行了实例化，并使用多阶段任务的NoveltyTask评估了系统的性能，结果表明该领域的处理新颖性实例的问题非常具有挑战性。

    

    最先进的自然语言处理模型已经在“封闭的世界”设置中表现出非凡的性能，其中训练集中的所有标签在评估集合中都是已知的。然而，在现实世界中，通常会观察到不属于任何已知类别的“新颖”实例，这使得处理新颖性的能力至关重要。为了引发对“处理新颖性”的这一重要领域的系统性研究，我们引入了“NoveltyTask”，这是一个多阶段任务，用于评估系统在流水线新颖性“检测”和“适应性”任务上的性能。我们提供了NoveltyTask的数学公式，并以作者归属任务为例进行实例化，该任务涉及识别给定文本的正确作者。我们使用了亚马逊评论语料库，并为NoveltyTask编制了一个大型数据集（包括来自200个作者/标签的250k个实例）。我们进行了全面的实验，并探索了任务的几个基准方法。我们的结果表明，处理新颖性实例的问题具有挑战性，需要专门的技术。我们还提出了一种新颖性检测和适应性的统一评估框架，可以应用于除作者归属以外的各种NLP任务。

    State-of-the-art natural language processing models have been shown to achieve remarkable performance in 'closed-world' settings where all the labels in the evaluation set are known at training time. However, in real-world settings, 'novel' instances that do not belong to any known class are often observed. This renders the ability to deal with novelties crucial. To initiate a systematic research in this important area of 'dealing with novelties', we introduce 'NoveltyTask', a multi-stage task to evaluate a system's performance on pipelined novelty 'detection' and 'accommodation' tasks. We provide mathematical formulation of NoveltyTask and instantiate it with the authorship attribution task that pertains to identifying the correct author of a given text. We use Amazon reviews corpus and compile a large dataset (consisting of 250k instances across 200 authors/labels) for NoveltyTask. We conduct comprehensive experiments and explore several baseline methods for the task. Our results sho
    
[^52]: 一种生成式预训练变形器的相干波动和语言生成机制的研究

    Coherent Wave Dynamics and Language Generation of a Generative Pre-trained Transformer. (arXiv:2305.05061v1 [cs.CL])

    [http://arxiv.org/abs/2305.05061](http://arxiv.org/abs/2305.05061)

    本文研究了一种小型GPT中的相干波动和语言生成机制，发现波动动力学提供了一致和可重复的内在振荡模式，以及上下文感知的可塑性和表现力，可用于语言生成，为理解和控制更高级别的语言模式形成铺平了道路。

    

    大型语言模型，如生成式预训练变形器（GPT），在各种语言任务中取得了巨大成功，但它们新兴的能力也引发了许多需要解决的问题、关注和挑战。为了更好地理解这些模型的内部机制，我们分析了小型GPT中的隐藏状态和通道波动机制，重点关注交叉通道相关性和个体自相关性方面的波动模式的一致性。我们的研究表明，波动动力学提供了一致和可重复的内在振荡模式，以及上下文感知的可塑性和表现力，可用于语言生成。通过分析波动模式、相干性和聚类，我们提供了一种系统的方法来识别和解释隐藏状态通道的功能，为理解和控制更高级别的语言模式形成铺平了道路。此外，我们研究了拼写错误的泊松统计。

    Large Language Models (LLMs), such as the Generative Pretrained Transformer (GPT), have achieved tremendous success in various language tasks, but their emergent abilities have also raised many questions, concerns, and challenges that need to be addressed. To gain a better understanding of the models' inner mechanisms, we analyze the hidden state and channel wave dynamics in a small GPT, focusing on the coherence of wave patterns in terms of cross-channel correlation and individual auto-correlation. Our findings suggest that wave dynamics offer consistent and repeatable intrinsic oscillation modes, along with context-aware plasticity and expressiveness in language generation. By analyzing wave patterns, coherence, and clustering, we provide a systematic way to identify and interpret the functionality of the hidden state channels, paving the way to understand and control higher-level language pattern formation. In addition, we investigate the Poisson statistics of spelling errors in tex
    
[^53]: 梦比你想象的更“可预测”（arXiv：2305.05054v1 [cs.CL]）

    Dreams Are More "Predictable'' Than You Think. (arXiv:2305.05054v1 [cs.CL])

    [http://arxiv.org/abs/2305.05054](http://arxiv.org/abs/2305.05054)

    研究比较了梦的报告和维基百科的文本字符串，发现梦的报告整体上不偏离维基百科，而单个梦报告比维基百科文章更可预测。

    

    一致的证据表明，梦的报告在语义内容上与其他类型的文本记录显着不同。此外，梦/睡眠研究界普遍认为，梦的报告构成了相当“独特”的文本字符串。这可能是越来越多的方法使用自然语言处理（NLP）工具来自动分析梦的报告的一个值得注意的问题，因为它们在很大程度上依赖于在网络上爬取的非梦想语料库上训练的神经模型。在本文中，作者将采用最先进的大型语言模型（LLMs）来研究梦想报告是否偏离其他人生成的文本字符串（例如维基百科），以及如何偏离。结果表明，DreamBank作为一个整体不偏离维基百科。此外，就平均而言，单个梦报告比维基百科文章更可预测。初步证据表明，字数、性别和视觉障碍可以显著影响梦的可预测性。

    A consistent body of evidence suggests that dream reports significantly vary from other types of textual transcripts with respect to semantic content. Furthermore, it appears to be a widespread belief in the dream/sleep research community that dream reports constitute rather ``unique'' strings of text. This might be a notable issue for the growing amount of approaches using natural language processing (NLP) tools to automatically analyse dream reports, as they largely rely on neural models trained on non-dream corpora scraped from the web. In this work, I will adopt state-of-the-art (SotA) large language models (LLMs), to study if and how dream reports deviate from other human-generated text strings, such as Wikipedia. Results show that, taken as a whole, DreamBank does not deviate from Wikipedia. Moreover, on average, single dream reports are significantly more predictable than Wikipedia articles. Preliminary evidence suggests that word count, gender, and visual impairment can signifi
    
[^54]: ANALOGICAL- 一种新的大语言模型文本类比评测基准

    ANALOGICAL - A New Benchmark for Analogy of Long Text for Large Language Models. (arXiv:2305.05050v1 [cs.CL])

    [http://arxiv.org/abs/2305.05050](http://arxiv.org/abs/2305.05050)

    本文介绍了一种名为“ANALOGICAL”的新型基准，用以内在评估LLMs在长文本类比中的能力，包括六个复杂级别的长文本类比分类，并使用13个数据集和三种距离度量方法来评估8个LLMs在语义向量空间中识别类比对的能力。

    

    在过去的十年中，以词级别的类比为形式的类比在衡量诸如word2vec之类的词嵌入方法的质量方面发挥了重要作用。然而，现代的大型语言模型(LLMs)主要根据GLUE和SuperGLUE等基准的外在量度进行评估，而在LLMs是否能够在长文本中绘制类比的方面，只有少数几项研究。本文介绍了一种名为“ANALOGICAL”的新型基准，以六个复杂级别的长文本类比分类对LLMs进行内在评估，分别为 (i)单词、(ii)单词vs句子、(iii)语法、(iv)否定、(v)蕴含和(vi)隐喻。利用13个数据集和三种不同的距离度量方法，我们评估了8个LLMs在语义向量空间中识别类比对的能力(例如，“我能说两种语言”应该更接近“我是双语的”，而“我喜欢巧克力”和“我不喜欢巧克力”应该是正交的)。

    Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benchmarks such as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs can draw analogies between long texts. In this paper, we present ANALOGICAL, a new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of long text with six levels of complexity -- (i) word, (ii) word vs. sentence, (iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using thirteen datasets and three different distance measures, we evaluate the abilities of eight LLMs in identifying analogical pairs in the semantic vector space (e.g., "I can speak two languages" should be closer to "I am bilingual" while "I like chocolate" and "I do not like chocolate" should be orthog
    
[^55]: 基于大语言模型知识蒸馏的网络内容过滤方法

    Web Content Filtering through knowledge distillation of Large Language Models. (arXiv:2305.05027v1 [cs.LG])

    [http://arxiv.org/abs/2305.05027](http://arxiv.org/abs/2305.05027)

    本文提出了一种基于大语言模型知识蒸馏的 URL 分类方法，可用于网络内容过滤，其学生模型在参数数量减少 175 倍的情况下，精度提升了 9%，超过了当前最先进方法。

    

    本文提出了一种基于大语言模型的 URL 分类方法，旨在实现网络内容过滤的主要目标：保障组织免受法律和伦理风险，限制访问高风险或可疑网站，以及促进安全的专业工作环境。我们的方法利用大语言模型生成准确的分类，并利用已有的知识蒸馏技术创建更小、更专业的学生模型，以用于网络内容过滤。在将通过大型安全供应商收集的客户遥测数据的 30 个不同内容类别的网站进行分类的任务中，我们的学生模型通过蒸馏结果实现了 9% 的分类精度提升，超过了当前最先进方法。我们的学生模型在参数数量上与原始的大语言模型相比减少了 175 倍，从而达到了与老师模型相匹配的性能，可以用于大规模的在线扫描。

    We introduce a state-of-the-art approach for URL categorization that leverages the power of Large Language Models (LLMs) to address the primary objectives of web content filtering: safeguarding organizations from legal and ethical risks, limiting access to high-risk or suspicious websites, and fostering a secure and professional work environment. Our method utilizes LLMs to generate accurate classifications and then employs established knowledge distillation techniques to create smaller, more specialized student models tailored for web content filtering. Distillation results in a student model with a 9\% accuracy rate improvement in classifying websites, sourced from customer telemetry data collected by a large security vendor, into 30 distinct content categories based on their URLs, surpassing the current state-of-the-art approach. Our student model matches the performance of the teacher LLM with 175 times less parameters, allowing the model to be used for in-line scanning of large vo
    
[^56]: 不要盲目模仿老师：使用扰动损失进行知识蒸馏

    Do Not Blindly Imitate the Teacher: Using Perturbed Loss for Knowledge Distillation. (arXiv:2305.05010v1 [cs.LG])

    [http://arxiv.org/abs/2305.05010](http://arxiv.org/abs/2305.05010)

    本文提出了一种新的知识蒸馏目标函数PTLoss，通过扰动老师的输出分布，使其更接近真实标签分布，从而提高学生的性能。

    

    知识蒸馏是一种常用的技术，用于将大型教师模型的知识传输到小型学生模型中。通常，学生通过最小化其输出分布和教师的输出分布之间的KL散度来模仿教师。本文认为这种学习目标是次优的，因为教师的输出分布与地面真实标签分布存在差异。因此，强制学生盲目模仿不可靠的教师输出分布会导致性能下降。为此，我们提出了一种新的知识蒸馏目标函数PTLoss，首先通过Maclaurin级数表示香草KL蒸馏损失函数，然后扰动该级数中的主导项。这种扰动损失隐式地将原始老师转换为具有更接近地面真实分布的代理老师。我们建立了香草KL蒸馏和扰动KL蒸馏之间的理论联系。

    Knowledge distillation is a popular technique to transfer knowledge from large teacher models to a small student model. Typically, the student learns to imitate the teacher by minimizing the KL divergence of its output distribution with the teacher's output distribution. In this work, we argue that such a learning objective is sub-optimal because there exists a discrepancy between the teacher's output distribution and the ground truth label distribution. Therefore, forcing the student to blindly imitate the unreliable teacher output distribution leads to inferior performance. To this end, we propose a novel knowledge distillation objective PTLoss by first representing the vanilla KL-based distillation loss function via a Maclaurin series and then perturbing the leading-order terms in this series. This perturbed loss implicitly transforms the original teacher into a proxy teacher with a distribution closer to the ground truth distribution. We establish the theoretical connection between
    
[^57]: GersteinLab在MEDIQA-Chat 2023中的贡献：通过精调和上下文学习整合医患交流中的临床记录摘要(arXiv:2305.05001v1 [cs.CL])

    GersteinLab at MEDIQA-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning. (arXiv:2305.05001v1 [cs.CL])

    [http://arxiv.org/abs/2305.05001](http://arxiv.org/abs/2305.05001)

    本研究团队参与MEDIQA-Chat 2023共享任务，实现了医患交流中临床记录摘要的自动化生成。通过对预训练模型的微调和少量上下文学习，本方法在关键指标上取得了极好的结果。

    

    本文介绍了我们在MEDIQA-2023 Dialogue2Note共享任务中的贡献，包括子任务A和子任务B。我们将任务视为一种对话摘要问题，并实现了两个不同的流程：（a）对预训练的对话摘要模型和GPT-3进行微调，（b）使用大型语言模型GPT-4进行少量上下文学习（ICL）。两种方法在ROUGE-1 F1，BERTScore F1（deberta-xlarge-mnli）和BLEURT方面均取得了极好的结果，分别为0.4011、0.7058和0.5421。此外，我们使用RoBERTa和SciBERT基于分类模型来预测相关的章节标题。我们的团队在所有团队中排名第四，每个团队允许提交三次运行作为其提交的一部分。我们还利用专家注释证明了通过ICL GPT-4生成的摘要优于所有其他基线。我们的代码已公开。

    This paper presents our contribution to the MEDIQA-2023 Dialogue2Note shared task, encompassing both subtask A and subtask B. We approach the task as a dialogue summarization problem and implement two distinct pipelines: (a) a fine-tuning of a pre-trained dialogue summarization model and GPT-3, and (b) few-shot in-context learning (ICL) using a large language model, GPT-4. Both methods achieve excellent results in terms of ROUGE-1 F1, BERTScore F1 (deberta-xlarge-mnli), and BLEURT, with scores of 0.4011, 0.7058, and 0.5421, respectively. Additionally, we predict the associated section headers using RoBERTa and SciBERT based classification models. Our team ranked fourth among all teams, while each team is allowed to submit three runs as part of their submission. We also utilize expert annotations to demonstrate that the notes generated through the ICL GPT-4 are better than all other baselines. The code for our submission is available.
    
[^58]: 解释性微调使模型对虚假提示更强韧

    Explanation-based Finetuning Makes Models More Robust to Spurious Cues. (arXiv:2305.04990v1 [cs.CL])

    [http://arxiv.org/abs/2305.04990](http://arxiv.org/abs/2305.04990)

    本文提出一种新型方法——解释性微调，通过让模型在给出答案的同时生成支持该答案的自由文本解释，来减轻LLMs依赖虚假关联，使得模型对虚假提示更加强韧，并具有广泛适用性。

    

    大型语言模型（LLMs）非常强大，有时会学习到标签和与任务无关的特征之间的相关性，导致在分布外数据上泛化能力差。我们提出解释性微调作为减轻LLMs依赖虚假关联的一种新的通用方法。与标准微调只在给定输入的情况下预测答案不同，我们微调模型以生成支持其答案的自由文本解释。为了评估我们的方法，我们在人工构建的训练集上微调模型，该训练集包含不同类型的虚假提示，并在没有这些提示的测试集上进行测试。与标准微调相比，我们的方法在四个分类任务的准确性下降方面使模型极其强韧：ComVE（+1.2），CREAK（+9.1），e-SNLI（+15.4）和SBIC（+6.5）。此外，我们的方法与模型生成的解释同样有效，这意味着我们的方法具有广泛的适用性。

    Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data. We propose explanation-based finetuning as a novel and general approach to mitigate LLMs' reliance on spurious correlations. Unlike standard finetuning where the model only predicts the answer given the input, we finetune the model to additionally generate a free-text explanation supporting its answer. To evaluate our method, we finetune the model on artificially constructed training sets containing different types of spurious cues, and test it on a test set without these cues. Compared to standard finetuning, our method makes models remarkably more robust against spurious cues in terms of accuracy drop across four classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC (+6.5). Moreover, our method works equally well with explanations generated by the model, implyin
    
[^59]: 知识图谱指导下语言模型语义评估以提高用户信任

    Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust. (arXiv:2305.04989v1 [cs.CL])

    [http://arxiv.org/abs/2305.04989](http://arxiv.org/abs/2305.04989)

    本研究通过知识图谱结构评估Self-Attention变压器中编码的语义。结果显示，语言模型是概率语言模式产生的控制过程的模型，但是不将对象和概念级别的含义和语义赋予所学习的随机模式，例如知识图谱中所描述的模式。

    

    自然语言处理中一个基本的问题是：语言模型捕捉到了什么样的语言结构和语义？像知识图谱这样的图表达形式很容易进行评估，因为它们明确地表达了语言语义和结构。本研究通过利用显式的知识图谱结构来评估Self-Attention变压器中编码的语义。我们提出了新的度量标准，通过提供从知识图谱获取的图形路径序列并尝试从Self-Attention变压器模型的输出中复制/重构同样路径来测量重构误差。语言模型的不透明性对于信任和可解释决策结果等社会问题有着巨大的影响。我们的研究发现，语言模型是概率语言模式产生的控制过程的模型，但是它们不将对象和概念级别的含义和语义赋予所学习的随机模式，例如知识图谱中所描述的模式。

    A fundamental question in natural language processing is - what kind of language structure and semantics is the language model capturing? Graph formats such as knowledge graphs are easy to evaluate as they explicitly express language semantics and structure. This study evaluates the semantics encoded in the self-attention transformers by leveraging explicit knowledge graph structures. We propose novel metrics to measure the reconstruction error when providing graph path sequences from a knowledge graph and trying to reproduce/reconstruct the same from the outputs of the self-attention transformer models. The opacity of language models has an immense bearing on societal issues of trust and explainable decision outcomes. Our findings suggest that language models are models of stochastic control processes for plausible language pattern generation. However, they do not ascribe object and concept-level meaning and semantics to the learned stochastic patterns such as those described in knowl
    
[^60]: NeuroComparatives：比较知识的神经符号提炼

    NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge. (arXiv:2305.04978v1 [cs.CL])

    [http://arxiv.org/abs/2305.04978](http://arxiv.org/abs/2305.04978)

    本文提出了一种新的用于比较知识提炼的神经符号框架，名称为NeuroComparatives。该框架利用词汇约束解码进行比较知识提炼，以及对生成的知识进行严格过滤。

    

    比较知识是我们世界知识的重要组成部分，但在以前的文献中研究不足。本文研究比较知识获取任务，受到像GPT-3这样极端规模语言模型能力的显着提高的推动，推动了将他们的知识收集到知识库中的努力。但是，这些模型的推理API访问受到限制，从而限制了知识获取的范围和多样性。因此，我们提出了一个看似不可行的问题：更易于访问、规模更小、性能更弱的模型（如GPT-2）是否可以用于获取比较知识，从而达到与大规模模型相当的质量？我们引入了NeuroComparatives，一种使用词汇约束解码的比较知识提炼新框架，其后紧密过滤生成的知识。

    Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) is an essential component of our world knowledge, yet understudied in prior literature. In this paper, we study the task of comparative knowledge acquisition, motivated by the dramatic improvements in the capabilities of extreme-scale language models like GPT-3, which have fueled efforts towards harvesting their knowledge into knowledge bases. However, access to inference API for such models is limited, thereby restricting the scope and the diversity of the knowledge acquisition. We thus ask a seemingly implausible question: whether more accessible, yet considerably smaller and weaker models such as GPT-2, can be utilized to acquire comparative knowledge, such that the resulting quality is on par with their large-scale counterparts?  We introduce NeuroComparatives, a novel framework for comparative knowledge distillation using lexically-constrained decoding, followed by stringent filtering of generated knowledge
    
[^61]: LABO: 通过双层优化实现最佳标签正则化学习

    LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization. (arXiv:2305.04971v1 [cs.LG])

    [http://arxiv.org/abs/2305.04971](http://arxiv.org/abs/2305.04971)

    本文提出了一种基于标签正则化的通用框架，其中包括传统的LS，但也可以建模实例特定的变体。我们提出了一种双层优化的方法（LABO），用于学习标签正则化，并得到了可解释的最优标签平滑解。

    

    正则化技术对于改善深度神经网络的泛化性能和训练效率至关重要。许多深度学习算法依赖于权重衰减、丢弃、批/层归一化等技术来更快地收敛和泛化。标签平滑（LS）是另一种简单、通用且高效的正则化方法，可用于各种监督分类任务。然而，传统的LS假设每个非目标类别出现的概率相等，不能根据实例对标签进行优化。本文提出了一种基于标签正则化的通用框架，包括传统的LS但也可以建模实例特定的变体。基于该框架，我们提出了一种通过设计双层优化（LABO）问题来学习标签正则化的高效方法。我们得出了内环节的确定性和可解释解，而无需存储经过训练模型的参数或输出。

    Regularization techniques are crucial to improving the generalization performance and training efficiency of deep neural networks. Many deep learning algorithms rely on weight decay, dropout, batch/layer normalization to converge faster and generalize. Label Smoothing (LS) is another simple, versatile and efficient regularization which can be applied to various supervised classification tasks. Conventional LS, however, regardless of the training instance assumes that each non-target class is equally likely. In this work, we present a general framework for training with label regularization, which includes conventional LS but can also model instance-specific variants. Based on this formulation, we propose an efficient way of learning LAbel regularization by devising a Bi-level Optimization (LABO) problem. We derive a deterministic and interpretable solution of the inner loop as the optimal label smoothing without the need to store the parameters or the output of a trained model. Finally
    
[^62]: 基于自然语言查询的联合时刻检索和精华片段检测

    Joint Moment Retrieval and Highlight Detection Via Natural Language Queries. (arXiv:2305.04961v1 [cs.CV])

    [http://arxiv.org/abs/2305.04961](http://arxiv.org/abs/2305.04961)

    本文提出了一种基于自然语言查询的联合视频摘要和精华片段检测方法，利用视觉和音频线索匹配用户的查询，实现检索视频中最相关和有趣的时刻。

    

    随着互联网上视频内容的不断增多，视频摘要成为计算机视觉领域中日益重要的任务。在本项目中，我们提出了一种基于多模态转换器的自然语言查询联合视频摘要和精华片段检测新方法。该方法将使用视觉和音频线索来匹配用户的自然语言查询，以检索视频中最相关和有趣的时刻。我们的方法采用多个最近用于视觉转换器(ViTs)的技术，创建了一种类似于编码器-解码器的Transformer模型。我们还评估了我们的方法在多个数据集上的表现，例如YouTube Highlights和TVSum，以证明我们所提出的方法的灵活性。

    Video summarization has become an increasingly important task in the field of computer vision due to the vast amount of video content available on the internet. In this project, we propose a new method for natural language query based joint video summarization and highlight detection using multi-modal transformers. This approach will use both visual and audio cues to match a user's natural language query to retrieve the most relevant and interesting moments from a video. Our approach employs multiple recent techniques used in Vision Transformers (ViTs) to create a transformer-like encoder-decoder model. We evaluated our approach on multiple datasets such as YouTube Highlights and TVSum to demonstrate the flexibility of our proposed method.
    
[^63]: 早起的鸟儿捉到虫：利用编码器模型的早期层进行更有效的代码分类

    The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification. (arXiv:2305.04940v1 [cs.SE])

    [http://arxiv.org/abs/2305.04940](http://arxiv.org/abs/2305.04940)

    本文介绍了一种早期层组合的方法EarlyBIRD，该方法可以有效利用深度自然语言处理模型的资源和可用信息，从而提高代码分类的性能，在缺陷检测方面平均可提高2个点。

    

    现代自然语言处理技术在软件工程任务如漏洞检测和类型推理方面表现出了卓越的优势。然而，训练深度自然语言处理模型需要大量计算资源。本文探讨了一些技术，旨在实现这些模型中资源和可用信息的最佳利用。我们提出了一种通用的方法EarlyBIRD，从预训练的transformer模型的早期层构建代码的复合表示。我们通过比较12种创建复合表示的策略与仅使用最后一个编码器层的标准实践，在CodeBERT模型上实证研究了这种方法的可行性。我们在4个数据集上的评估表明，几个早期层的组合在缺陷检测方面产生更好的性能，而一些组合则改进了多类分类。具体而言，我们获得了平均检测增强2。

    The use of modern Natural Language Processing (NLP) techniques has shown to be beneficial for software engineering tasks, such as vulnerability detection and type inference. However, training deep NLP models requires significant computational resources. This paper explores techniques that aim at achieving the best usage of resources and available information in these models.  We propose a generic approach, EarlyBIRD, to build composite representations of code from the early layers of a pre-trained transformer model. We empirically investigate the viability of this approach on the CodeBERT model by comparing the performance of 12 strategies for creating composite representations with the standard practice of only using the last encoder layer.  Our evaluation on four datasets shows that several early layer combinations yield better performance on defect detection, and some combinations improve multi-class classification. More specifically, we obtain a +2 average improvement of detection 
    
[^64]: 基于Transformer的零样本和少样本生物医学命名实体识别方法

    A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v1 [cs.CL])

    [http://arxiv.org/abs/2305.04928](http://arxiv.org/abs/2305.04928)

    本文提出了一种基于Transformer的生物医学领域零样本和少样本NER方法。此方法利用预训练学习给定和潜在类别之间的语义关系，将多类标记分类任务转换为二元标记分类，能够在不同数量的样本情况下达到良好的识别效果。

    

    在生物医学领域中，有监督的命名实体识别（NER）依赖于具有给定命名实体的大量注释文本，其创建可能耗时且昂贵。此外，提取新实体通常需要进行额外的注释任务和重新训练模型。为解决这些挑战，本文提出了一种基于Transformer的生物医学领域零样本和少样本NER方法。该方法基于将多类标记分类任务转换为二元标记分类（标记包含搜索的实体或不包含搜索的实体），并在更多的数据集和生物医学实体上进行预训练，从而可学习到给定和潜在类别之间的语义关系。在9种不同的生物医学实体上，我们在零样本NER、一次样本NER、10次样本NER和100次样本NER上实现了平均F1得分分别为35.44％、50.10％、69.94％和79.51％。

    Supervised named entity recognition (NER) in the biomedical domain is dependent on large sets of annotated texts with the given named entities, whose creation can be time-consuming and expensive. Furthermore, the extraction of new entities often requires conducting additional annotation tasks and retraining the model. To address these challenges, this paper proposes a transformer-based method for zero- and few-shot NER in the biomedical domain. The method is based on transforming the task of multi-class token classification into binary token classification (token contains the searched entity or does not contain the searched entity) and pre-training on a larger amount of datasets and biomedical entities, from where the method can learn semantic relations between the given and potential classes. We have achieved average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical entities with PubMed
    
[^65]: 提前检测和推理删除推文

    Detecting and Reasoning of Deleted Tweets before they are Posted. (arXiv:2305.04927v1 [cs.CL])

    [http://arxiv.org/abs/2305.04927](http://arxiv.org/abs/2305.04927)

    本研究提出了一种新的深度学习框架，可以在推文发布之前识别出即将被删除的内容，并推理其潜在危害和违反平台政策的原因。该方法可用于推进更安全和负责任的社交媒体使用。

    

    社交媒体平台在信息传播和消费等方面给我们带来了许多便利。然而，这些平台也存在着被滥用的潜在风险。恶意用户用它们来散播仇恨言论、攻击性内容、谣言等，以获得社会和政治议程，或者伤害个人、实体和组织。通常情况下，一些用户在未经验证的情况下无意中分享信息，或者无意中发布有害信息。一些此类内容经常被平台删除，可能是因为违反了条款和政策，也可能是由于用户自己的不同原因，比如后悔了。目前有许多研究对删除内容进行了表征、理解和预测。然而，旨在识别删除的细致原因（例如，帖子令人反感、仇恨言论或没有可识别的原因）的研究是有限的。在本研究中，我们通过识别即将发布的被删除的推文，并推理它们的潜在危害和违反平台政策的原因来填补这一空白。我们提出了一个基于深度学习的框架，利用推文的各种特征来预测它们是否会被删除，如果会被删除，则是为什么。我们的方法在识别删除的推文及其原因方面取得了高准确性，可以成为促进更安全和负责任的社交媒体使用的有价值的工具。

    Social media platforms empower us in several ways, from information dissemination to consumption. While these platforms are useful in promoting citizen journalism, public awareness etc., they have misuse potentials. Malicious users use them to disseminate hate-speech, offensive content, rumor etc. to gain social and political agendas or to harm individuals, entities and organizations. Often times, general users unconsciously share information without verifying it, or unintentionally post harmful messages. Some of such content often get deleted either by the platform due to the violation of terms and policies, or users themselves for different reasons, e.g., regrets. There is a wide range of studies in characterizing, understanding and predicting deleted content. However, studies which aims to identify the fine-grained reasons (e.g., posts are offensive, hate speech or no identifiable reason) behind deleted content, are limited. In this study we address this gap, by identifying deleted 
    
[^66]: 多模态-GPT: 用于与人类对话的视觉与语言模型

    MultiModal-GPT: A Vision and Language Model for Dialogue with Humans. (arXiv:2305.04790v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.04790](http://arxiv.org/abs/2305.04790)

    MultiModal-GPT是一个用于与人类进行多轮对话的视觉与语言模型，可以遵循人类的各种指令，并且通过联合训练表现得更好。

    

    我们提出了一个名为MultiModal-GPT的视觉与语言模型，用于与人类进行多轮对话。 MultiModal-GPT可以遵循人类的各种指令，例如生成详细的字幕，计算感兴趣对象的数量以及回答用户的常见问题。 我们通过OpenFlamingo进行参数有效地微调MultiModal-GPT，并在语言模型的交叉关注部分和自我关注部分中添加了低秩适配器（LoRA）。 我们首先使用视觉和语言数据构建指令模板，用于多模态指令调整，让模型理解和遵循人类指令。 我们发现对话表现的训练数据质量至关重要，其中很少包含简短回答的数据会使模型对任何指令都作出简短回答。为了进一步增强MultiModal-GPT与人类聊天的能力，我们利用仅语言的指令跟随数据联合训练MultiModal-GPT。联合训练显著提高了MultiModal-GPT在对话任务中的表现。

    We present a vision and language model named MultiModal-GPT to conduct multi-round dialogue with humans. MultiModal-GPT can follow various instructions from humans, such as generating a detailed caption, counting the number of interested objects, and answering general questions from users. MultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, with Low-rank Adapter (LoRA) added both in the cross-attention part and the self-attention part of the language model. We first construct instruction templates with vision and language data for multi-modality instruction tuning to make the model understand and follow human instructions. We find the quality of training data is vital for the dialogue performance, where few data containing short answers can lead the model to respond shortly to any instructions. To further enhance the ability to chat with humans of the MultiModal-GPT, we utilize language-only instruction-following data to train the MultiModal-GPT jointly. The joint tra
    
[^67]: PreCog：探究预训练语言模型中记忆与性能之间的关系

    PreCog: Exploring the Relation between Memorization and Performance in Pre-trained Language Models. (arXiv:2305.04673v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.04673](http://arxiv.org/abs/2305.04673)

    本文分析了预训练语言模型BERT在下游任务中记忆与性能之间的关系，提出了评估预训练记忆的指标PreCog，并发现高度记忆的例子分类效果更好，说明记忆对BERT的成功至关重要。

    

    BERT等预训练语言模型具有惊人的记忆能力，能够记住一些泛化的学习例子。本文旨在针对BERT在下游任务中的记忆与性能之间的相互影响进行分析，提出了PreCog——一种评估预训练记忆的指标，并分析了它与BERT性能之间的关联。实验表明，高度记忆的例子分类效果更好，说明记忆对BERT的成功至关重要。

    Pre-trained Language Models such as BERT are impressive machines with the ability to memorize, possibly generalized learning examples. We present here a small, focused contribution to the analysis of the interplay between memorization and performance of BERT in downstream tasks. We propose PreCog, a measure for evaluating memorization from pre-training, and we analyze its correlation with the BERT's performance. Our experiments show that highly memorized examples are better classified, suggesting memorization is an essential key to success for BERT.
    
[^68]: AlignSTS：通过跨模态对齐实现语音转唱

    AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment. (arXiv:2305.04476v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2305.04476](http://arxiv.org/abs/2305.04476)

    本文提出了基于跨模态对齐的STS模型AlignSTS，通过一种新颖的节奏适配器来预测目标节奏表示以弥合内容和音高之间的模态差距，并使用交叉注意力重新对齐内容进行跨模态融合重新合成。该模型表现优异。

    

    语音转唱 (STS) 任务旨在在面对一个主要挑战时，生成与语音录音相对应的唱歌样本：在没有文本的情况下，目标（唱歌）音高轮廓和源（语音）内容之间的对齐难以学习。本文提出了基于显式跨模态对齐的STS模型AlignSTS，将语音变化（如音高和内容）视为不同的模态。受人类如何唱出旋律的歌词机制的启发，AlignSTS: 1）采用一种新颖的节奏适配器来预测目标节奏表示，以弥合内容和音高之间的模态差距，其中节奏表示以简单而有效的方式计算，并量化为离散空间；2）使用预测的节奏表示基于交叉注意力重新对齐内容，并进行跨模态融合重新合成。广泛的实验表明，AlignSTS取得了优越的性能。

    The speech-to-singing (STS) voice conversion task aims to generate singing samples corresponding to speech recordings while facing a major challenge: the alignment between the target (singing) pitch contour and the source (speech) content is difficult to learn in a text-free situation. This paper proposes AlignSTS, an STS model based on explicit cross-modal alignment, which views speech variance such as pitch and content as different modalities. Inspired by the mechanism of how humans will sing the lyrics to the melody, AlignSTS: 1) adopts a novel rhythm adaptor to predict the target rhythm representation to bridge the modality gap between content and pitch, where the rhythm representation is computed in a simple yet effective way and is quantized into a discrete space; and 2) uses the predicted rhythm representation to re-align the content based on cross-attention and conducts a cross-modal fusion for re-synthesize. Extensive experiments show that AlignSTS achieves superior performanc
    
[^69]: UIT-OpenViIC：一种用于评估越南语图像字幕的新型基准

    UIT-OpenViIC: A Novel Benchmark for Evaluating Image Captioning in Vietnamese. (arXiv:2305.04166v1 [cs.CV])

    [http://arxiv.org/abs/2305.04166](http://arxiv.org/abs/2305.04166)

    这篇论文介绍了一种新颖的越南语图像字幕数据集UIT-OpenViIC，这是为了解决目前在越南低资源研究社区中存在的困境而引入的。该数据集包括越南的复杂场景，并仅由越南人根据严格的规则和监督进行手动注释。数据集对于最新的最先进的翻译模型而言具有挑战性。

    

    图像字幕是一种仍然吸引全球研究社区兴趣的视觉语言任务。虽然MS-COCO字幕基准是在2015年发布的，但它仍然被广泛使用来评估高级字幕模型的性能。然而，仅在MS-COCO字幕数据集上训练的最新字幕模型仅在英语语言模式方面表现良好；它们在越南捕捉的上下文或使用越南语流畅字幕图像方面的表现并不好。为了贡献于像越南这样的低资源研究社区，我们引入了一种新颖的越南语图像字幕数据集，UIT-OpenViIC。我们的数据集包括越南的复杂场景，并由越南人根据严格的规则和监督进行手动注释。在本文中，我们更详细地介绍了数据集的创建过程。从初步分析中，我们展示了我们的数据集对于最新的最先进的翻译模型而言具有挑战性。

    Image Captioning is one of the vision-language tasks that still interest the research community worldwide in the 2020s. MS-COCO Caption benchmark is commonly used to evaluate the performance of advanced captioning models, although it was published in 2015. Recent captioning models trained on the MS-COCO Caption dataset only have good performance in language patterns of English; they do not have such good performance in contexts captured in Vietnam or fluently caption images using Vietnamese. To contribute to the low-resources research community as in Vietnam, we introduce a novel image captioning dataset in Vietnamese, the Open-domain Vietnamese Image Captioning dataset (UIT-OpenViIC). The introduced dataset includes complex scenes captured in Vietnam and manually annotated by Vietnamese under strict rules and supervision. In this paper, we present in more detail the dataset creation process. From preliminary analysis, we show that our dataset is challenging to recent state-of-the-art 
    
[^70]: SemEval-2023任务2中的DAMO-NLP: 一种多语言命名实体识别的统一检索增强系统

    DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition. (arXiv:2305.03688v1 [cs.CL])

    [http://arxiv.org/abs/2305.03688](http://arxiv.org/abs/2305.03688)

    DAMO-NLP团队的U-RaNER是一种统一的多语言命名实体识别系统，它通过加入带有实体为中心的Wikidata知识库并采用infusion方法来增强检索上下文，解决了其他系统存在的知识不足、上下文长度有限和单一检索策略等问题。

    

    MultiCoNER 2共享任务旨在解决多语言命名实体识别的细粒度和嘈杂情况，并继承了MultiCoNER 1任务的语义歧义和低上下文环境。针对这些问题，MultiCoNER 1中的前几个顶尖系统要么纳入知识库或专有名词表，但它们仍然存在知识不足、上下文长度有限以及单一检索策略等问题。在本文中，我们的DAMO-NLP团队提出了一种用于多语言的细粒度命名实体识别的统一检索增强系统（U-RaNER）。我们对上述几个顶尖系统进行了错误分析，发现它们的性能瓶颈在于知识不足，而且有限的上下文长度使得检索知识对模型不可见。为了增强检索上下文，我们加入了以实体为中心的Wikidata知识库，并采用infusion方法来拓宽上下文引用。

    The MultiCoNER \RNum{2} shared task aims to tackle multilingual named entity recognition (NER) in fine-grained and noisy scenarios, and it inherits the semantic ambiguity and low-context setting of the MultiCoNER \RNum{1} task. To cope with these problems, the previous top systems in the MultiCoNER \RNum{1} either incorporate the knowledge bases or gazetteers. However, they still suffer from insufficient knowledge, limited context length, single retrieval strategy. In this paper, our team \textbf{DAMO-NLP} proposes a unified retrieval-augmented system (U-RaNER) for fine-grained multilingual NER. We perform error analysis on the previous top systems and reveal that their performance bottleneck lies in insufficient knowledge. Also, we discover that the limited context length causes the retrieval knowledge to be invisible to the model. To enhance the retrieval context, we incorporate the entity-centric Wikidata knowledge base, while utilizing the infusion approach to broaden the contextua
    
[^71]: Diffusion Explainer：用于文本到图像稳定扩散的可视化解释工具

    Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion. (arXiv:2305.03509v1 [cs.CL])

    [http://arxiv.org/abs/2305.03509](http://arxiv.org/abs/2305.03509)

    Diffusion Explainer是第一个可交互的可视化工具，用于解释稳定扩散如何将文本提示转化为图像，用户可以通过动画和交互元素流畅地在多个抽象级别之间过渡，从而更好地理解提示对图像生成的影响。

    

    基于扩散的生成模型通过创造逼真的图像而获得了全球关注。然而，它们复杂的内部结构和操作往往使得非专业人员难以理解。我们提出了 Diffusion Explainer，这是第一个交互式可视化工具，用于解释稳定扩散如何将文本提示转化为图像。Diffusion Explainer紧密地将稳定扩散的复杂组件的视觉概述与其潜在操作的详细说明相结合，通过动画和交互元素使用户可以流畅地在多个抽象级别之间过渡。通过比较由两个相关文本提示引导的图像表示的演变来指导精细时间步长，用户可以发现提示对图像生成的影响。Diffusion Explainer在用户的Web浏览器中本地运行，无需安装或专门的硬件，扩大了公众对现代人工智能技术的教育获取。

    Diffusion-based generative models' impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern AI tec
    
[^72]: T-SciQ: 使用大型语言模型信号教授多模态链式思维推理在科学问题回答中的应用

    T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering. (arXiv:2305.03453v1 [cs.CL])

    [http://arxiv.org/abs/2305.03453](http://arxiv.org/abs/2305.03453)

    本研究使用大型语言模型信号教育科学问题回答的链式思维推理，通过生成高质量COT合理化信号，同时降低了人工注释的需求。

    

    大型语言模型(LLMs)近期在各种自然语言处理(NLP)任务中展示了出色的性能。他们还展示了执行链式思维推理以解决复杂问题的能力。最近的研究探索了复杂多模态场景下的链式思维推理，例如通过用高质量人工注释的链式思路来调整多模型模型进行科学问题回答等任务。然而，收集高质量COT合理化通常是耗时且昂贵的。此外，由于涉及冗余信息或丢失重要信息，注释合理化通常不太准确。为了解决这些问题，我们提出了一种新的方法，称为T-SciQ，旨在使用LLM信号教授科学问题回答。T-SciQ方法生成高质量的CoT合理化信号，并先进地训练较小的模型以在复杂模态中执行CoT思维推理。另外，我们引入了一种显着提高模型泛化能力的技术，从而显着减少了对人工注释合理的需求。

    Large Language Models (LLMs) have recently demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. They have also shown the ability to perform chain-of-thought (CoT) reasoning to solve complex problems. Recent studies have explored CoT reasoning in complex multimodal scenarios, such as the science question answering task, by fine-tuning multimodal models with high-quality human-annotated CoT rationales. However, collecting high-quality COT rationales is usually time-consuming and costly. Besides, the annotated rationales are hardly accurate due to the redundant information involved or the essential information missed. To address these issues, we propose a novel method termed \emph{T-SciQ} that aims at teaching science question answering with LLM signals. The T-SciQ approach generates high-quality CoT rationales as teaching signals and is advanced to train much smaller models to perform CoT reasoning in complex modalities. Additionally, we introduce a no
    
[^73]: 房间里的大象：分析大型科技公司在自然语言处理研究中的存在

    The Elephant in the Room: Analyzing the Presence of Big Tech in Natural Language Processing Research. (arXiv:2305.02797v1 [cs.CL])

    [http://arxiv.org/abs/2305.02797](http://arxiv.org/abs/2305.02797)

    本文研究了工业界在自然语言处理研究中的存在和影响。研究发现在过去五年中，工业界的存在与影响呈现急剧增长，一些公司占据了大部分出版物，并向学术研究人员提供资金支持。

    

    自然语言处理的深度学习方法的最新进展，创造了新的商业机会，并且使得NLP研究对产业发展至关重要。作为NLP领域的大玩家之一，连同政府和大学一起，跟踪产业对研究的影响非常重要。在本研究中，我们致力于量化和表征工业界在NLP社区中的存在。使用具有78,187篇NLP出版物和701个NLP作者简历的全面元数据语料库，我们探索了自上世纪90年代以来该领域中的工业存在。我们发现，NLP作者中的工业存在在过去五年中急剧增长（从2017年到2022年的增长率为180％）。一些公司占据了大部分出版物，并通过拨款和实习为学术研究人员提供资金支持。我们的研究表明，工业界对自然语言处理研究的存在和影响是显著的。

    Recent advances in deep learning methods for natural language processing (NLP) have created new business opportunities and made NLP research critical for industry development. As one of the big players in the field of NLP, together with governments and universities, it is important to track the influence of industry on research. In this study, we seek to quantify and characterize industry presence in the NLP community over time. Using a corpus with comprehensive metadata of 78,187 NLP publications and 701 resumes of NLP publication authors, we explore the industry presence in the field since the early 90s. We find that industry presence among NLP authors has been steady before a steep increase over the past five years (180% growth from 2017 to 2022). A few companies account for most of the publications and provide funding to academic researchers through grants and internships. Our study shows that the presence and impact of the industry on natural language processing research are signi
    
[^74]: 主动对话系统综述：问题、方法和展望

    A Survey on Proactive Dialogue Systems: Problems, Methods, and Prospects. (arXiv:2305.02750v1 [cs.CL])

    [http://arxiv.org/abs/2305.02750](http://arxiv.org/abs/2305.02750)

    本综述全面概述了不同类型对话中对话代理主动性的突出问题和先进设计，讨论了符合实际应用需求但需要未来更大研究重点的挑战，激发更多的会话 AI 进展到下一级别。

    

    主动对话系统与广泛的现实世界对话应用相关，使对话代理能够引导对话方向，以实现预定义的目标或满足系统方面的特定目标。它通过先进技术赋能以进展到需要战略性和激励性交互的更复杂任务。在本综述中，我们全面概述了不同类型对话中对话代理主动性的突出问题和先进设计。此外，我们还讨论了符合实际应用需求但需要未来更大研究重点的挑战。我们希望这篇主动对话系统的第一篇综述可以为社区提供快速访问和整体图片，激发更多的会话 AI 进展到下一级别。

    Proactive dialogue systems, related to a wide range of real-world conversational applications, equip the conversational agent with the capability of leading the conversation direction towards achieving pre-defined targets or fulfilling certain goals from the system side. It is empowered by advanced techniques to progress to more complicated tasks that require strategical and motivational interactions. In this survey, we provide a comprehensive overview of the prominent problems and advanced designs for conversational agent's proactivity in different types of dialogues. Furthermore, we discuss challenges that meet the real-world application needs but require a greater research focus in the future. We hope that this first survey of proactive dialogue systems can provide the community with a quick access and an overall picture to this practical problem, and stimulate more progresses on conversational AI to the next level.
    
[^75]: FIREBALL：一份包含结构化游戏状态信息的Dungeons & Dragons实际游戏数据集

    FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information. (arXiv:2305.01528v1 [cs.CL])

    [http://arxiv.org/abs/2305.01528](http://arxiv.org/abs/2305.01528)

    本研究介绍了一份包含真实游戏状态信息的Dungeons & Dragons实际游戏数据集FIREBALL，它可以改善自然语言生成的质量。此外，LLMs可以使用FIREBALL中的游戏状态信息来生成更高质量的游戏回合。

    

    Dungeons & Dragons（D＆D）是一款桌面角色扮演游戏，其玩家之间存在复杂的自然语言交互和隐藏的状态信息。最近的研究表明，拥有状态信息的大型语言模型（LLMs）生成的游戏回合比仅使用对话历史的LLMs更具高质量。然而，以往的研究使用的游戏状态信息是启发式创建的，并不是真正的黄金标准游戏状态。我们提出了FIREBALL，这是一个包含真实游戏状态信息的大型数据集，其中包含来自Discord的近25,000个真实D＆D游戏会话。我们记录了使用Avrae机器人的玩家的游戏会话，该机器人是为了帮助人们在线玩D＆D而开发的，并捕获了语言、游戏命令和基础游戏状态信息。我们证明，通过使用Avrae状态信息，FIREBALL可以提高自然语言生成（NLG），从而提高自动评估指标和人类的质量评判。此外，我们还展示了LLMs可以生成…

    Dungeons & Dragons (D&D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing nearly 25,000 unique sessions from real D\&D gameplay on Discord with true game state info. We recorded game play sessions of players who used the Avrae bot, which was developed to aid people in playing D&D online, capturing language, game commands and underlying game state information. We demonstrate that FIREBALL can improve natural language generation (NLG) by using Avrae state information, improving both automated metrics and human judgments of quality. Additionally, we show that LLMs can generate
    
[^76]: 触发词作为后门攻击的触发器：检查语言模型的脆弱性

    Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v1 [cs.CL])

    [http://arxiv.org/abs/2305.01219](http://arxiv.org/abs/2305.01219)

    本研究提出一种新颖有效的“ProAttack”方法来执行干净标签的后门攻击，使用的是提示本身作为触发器。该方法不需要外部触发器，并确保毒瘤数据的标注正确，提高了后门攻击的隐蔽性，相比于现有的后门攻击方法有显著提升。

    

    基于提示的学习范例弥合了预训练和微调之间的差距，在几个NLP任务中取得了最先进的性能，尤其是在少样本情况下。尽管应用广泛，但基于提示的学习容易受到后门攻击。文本后门攻击旨在通过注入触发器并修改标签来在模型中引入有针对性的漏洞。然而，由于触发器的存在和毒瘤数据标注不正确等缺陷，这种攻击存在异常的自然语言表达。在本研究中，我们提出了一种新颖有效的“ProAttack”方法，基于提示来执行干净标签的后门攻击，使用的是提示本身作为触发器。我们的方法不需要外部触发器，并确保毒瘤数据的标注正确，提高了后门攻击的隐蔽性。通过在丰富的资源和少样本文本语料库上的广泛实验，我们证明了ProAttack方法在保持干净数据一致性的同时显著优于现有的后门攻击方式。

    The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose {\bf ProAttack}, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text c
    
[^77]: 对Kauhanen、Einhaus和Walkden（2023年）的回应：仍然没有证据证明非母语用户比例对语言复杂度有影响（arXiv:2305.00217v1 [cs.CL]）

    Still no evidence for an effect of the proportion of non-native speakers on language complexity -- A response to Kauhanen, Einhaus & Walkden (2023). (arXiv:2305.00217v1 [cs.CL])

    [http://arxiv.org/abs/2305.00217](http://arxiv.org/abs/2305.00217)

    本研究为对Kauhanen、Einhaus和Walkden（2023）的回应，仍然没有证据表明大量的L2用户影响语言复杂性。

    

    近期在《语言进化杂志》发表的一篇论文中，Kauhanen、Einhaus和Walkden（https://doi.org/10.1093/jole/lzad005，KEW）挑战了我在一篇论文中（Koplenig，Royal Society Open Science，6，181274（2019），https://doi.org/10.1098/rsos.181274）所呈现的结果。在该论文中，我试图通过一系列的统计分析来表明大量L2（第二语言）用户似乎不会影响语言的（语法或统计）复杂性。为此，我专注于Ethnologue评估语言地位的方式：如果一种语言除了被L1（第一语言）使用者之外，还应该有大量的L2使用者，那么该语言就被描述为传播性的。KEW批评了将传播性作为语言是否拥有大量L2使用者（二元）指标的使用，以及在直接估计L2比例的情况下，将L2用户比例归为非传播性语言的想法。

    In a recent paper published in the Journal of Language Evolution, Kauhanen, Einhaus & Walkden (https://doi.org/10.1093/jole/lzad005, KEW) challenge the results presented in one of my papers (Koplenig, Royal Society Open Science, 6, 181274 (2019), https://doi.org/10.1098/rsos.181274), in which I tried to show through a series of statistical analyses that large numbers of L2 (second language) speakers do not seem to affect the (grammatical or statistical) complexity of a language. To this end, I focus on the way in which the Ethnologue assesses language status: a language is characterised as vehicular if, in addition to being used by L1 (first language) speakers, it should also have a significant number of L2 users. KEW criticise both the use of vehicularity as a (binary) indicator of whether a language has a significant number of L2 users and the idea of imputing a zero proportion of L2 speakers to non-vehicular languages whenever a direct estimate of that proportion is unavailable. Whi
    
[^78]: 基于ChatGPT-4的ACR放射肿瘤内科（TXIT）考试和Red Journal Gray Zone案例的基准测试：AI辅助医学教育和放射肿瘤治疗决策的潜力与挑战

    Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology. (arXiv:2304.11957v2 [physics.med-ph] UPDATED)

    [http://arxiv.org/abs/2304.11957](http://arxiv.org/abs/2304.11957)

    本研究评估了ChatGPT-4在放射肿瘤学方面的表现，成绩显示出它在医学考试上有很大的优势，在实际应用中存在局限性。另外，ChatGPT-4 在放射肿瘤学上表现出色，但在骨骼和软组织以及妇科方面有待改进。

    

    大型语言模型在医学上的教育和决策方面的潜力已经得到证明，因为它们在美国医学许可考试（USMLE）和MedQA考试等医学考试中取得了不错的成绩。本研究评估了ChatGPT-4在放射肿瘤学专业领域的表现，使用了第38届美国放射学院（ACR）放射肿瘤内科（TXIT）考试和2022年的Red Journal Gray Zone案例。基于TXIT考试，ChatGPT-4在放射肿瘤学方面表现出色，但在ACR知识领域中的骨骼和软组织以及妇科方面存在局限性。在临床路径方面，ChatGPT-4在2022年的Red Journal Gray Zone案例中表现较好，具有70.65％的准确率。本研究展示了使用大型语言模型（例如ChatGPT-4）进行放射肿瘤学AI辅助医学教育和决策制定的潜力和挑战。

    The potential of large language models in medicine for education and decision making purposes has been demonstrated as they achieve decent scores on medical exams such as the United States Medical Licensing Exam (USMLE) and the MedQA exam. In this work, we evaluate the performance of ChatGPT-4 in the specialized field of radiation oncology using the 38th American College of Radiology (ACR) radiation oncology in-training (TXIT) exam and the 2022 red journal gray zone cases. For the TXIT exam, ChatGPT-3.5 and ChatGPT-4 have achieved the scores of 63.65% and 74.57%, respectively, highlighting the advantage of the latest ChatGPT-4 model. Based on the TXIT exam, ChatGPT-4's strong and weak areas in radiation oncology are identified to some extent. Specifically, ChatGPT-4 demonstrates good knowledge of statistics, CNS & eye, pediatrics, biology, and physics but has limitations in bone & soft tissue and gynecology, as per the ACR knowledge domain. Regarding clinical care paths, ChatGPT-4 perf
    
[^79]: 为机器翻译质量评估量身定制领域自适应方法

    Tailoring Domain Adaptation for Machine Translation Quality Estimation. (arXiv:2304.08891v1 [cs.CL])

    [http://arxiv.org/abs/2304.08891](http://arxiv.org/abs/2304.08891)

    本研究提出了结合领域自适应和数据增强的质量评估系统，针对数据缺乏和领域不匹配的问题在通用模型的基础上进行微调，结果显著优于最先进基线。

    

    质量评估对翻译流程至关重要，但其有效性取决于训练数据的可用性和质量。对于特定的质量评估而言，由于标记这样的数据的成本和工作量高昂，因此高质量的标记数据经常缺乏。除了数据缺乏方面的挑战外，质量评估模型还应具有泛化性，即它们应该能够处理来自不同领域的数据，包括通用领域和特定领域数据。因此，本文将领域自适应和数据增强进行了结合，提出了一种强大的质量评估系统。方法是先训练一个通用质量评估模型，然后在保留通用知识的同时对特定领域进行微调。研究结果表明，在所有研究的语言对中，我们的方法均取得了显着的改进，并具有更好的跨语言推断，相比于最先进的基线在零-shot学习方案中具有更高的性能。

    While quality estimation (QE) can play an important role in the translation process, its effectiveness relies on the availability and quality of training data. For QE in particular, high-quality labeled data is often lacking due to the high-cost and effort associated with labeling such data. Aside from the data scarcity challenge, QE models should also be generalizable, i.e., they should be able to handle data from different domains, both generic and specific. To alleviate these two main issues -- data scarcity and domain mismatch -- this paper combines domain adaptation and data augmentation within a robust QE system. Our method is to first train a generic QE model and then fine-tune it on a specific domain while retaining generic knowledge. Our results show a significant improvement for all the language pairs investigated, better cross-lingual inference, and a superior performance in zero-shot learning scenarios as compared to state-of-the-art baselines.
    
[^80]: ParroT: 使用大型语言模型进行聊天翻译

    ParroT: Translating During Chat Using Large Language Models. (arXiv:2304.02426v1 [cs.CL])

    [http://arxiv.org/abs/2304.02426](http://arxiv.org/abs/2304.02426)

    ParroT提出了一种基于开源LLM和人工编写的翻译评估数据的聊天翻译框架，可以将翻译数据转化为指令执行样式，并引入额外要求来规范翻译过程。在使用相对较少的训练数据的情况下，实验结果表明 ParroT 可以大幅提高翻译质量。

    

    大型语言模型（LLM）如 ChatGPT 和 GPT-4 在各种自然语言处理（NLP）任务上展现出了卓越的能力，包括在聊天过程中完成各种机器翻译能力。然而，这些模型只能通过受限的API访问，这为新的研究和领域进展带来了障碍。因此，我们提出了 ParroT 框架，基于开源LLM（如LLaMA-7b）和人工编写的翻译评估数据来增强和规范聊天翻译能力。具体而言，ParroT将翻译数据转化为指令执行的样式，并引入 "Hint " 字段以加入额外要求来规范翻译过程。因此，我们提出了三种指令类型来微调 ParroT 模型，包括翻译指令、对比指令和误差引导指令。在两个 Flores 子集和 WMT22 测试集上的实验证明，使用 ParroT 可以大幅提高翻译质量，且需要相对较少的训练数据。

    Large language models (LLMs) like ChatGPT and GPT-4 have exhibited remarkable abilities on a wide range of natural language processing (NLP) tasks, including various machine translation abilities accomplished during chat. However, these models are only accessible through restricted APIs, which creates barriers to new research and advancements in the field. Therefore, we propose the $\mathbf{ParroT}$ framework to enhance and regulate the translation abilities during chat based on open-sourced LLMs (i.e., LLaMA-7b) and human written translation and evaluation data. Specifically, ParroT reformulates translation data into the instruction-following style, and introduces a "Hint" field for incorporating extra requirements to regulate the translation process. Accordingly, we propose three instruction types for finetuning ParroT models, including translation instruction, contrastive instruction, and error-guided instruction. Experiments on two Flores subsets and WMT22 test sets suggest that tr
    
[^81]: BloombergGPT：金融领域的大型语言模型

    BloombergGPT: A Large Language Model for Finance. (arXiv:2303.17564v1 [cs.LG])

    [http://arxiv.org/abs/2303.17564](http://arxiv.org/abs/2303.17564)

    本文提出了BloombergGPT，一个500亿参数的金融领域的大型语言模型，其基于Bloomberg的广泛数据来源和通用数据集进行训练。通过混合数据集训练，该模型在金融任务上表现出色，并且不会牺牲在普通任务上的性能。

    

    自然语言处理在金融技术领域有着广泛而复杂的应用，从情感分析和命名实体识别到问答。大型语言模型（LLM）已被证明在各种任务上非常有效；然而，专为金融领域设计的LLM尚未在文献中报告。在本文中，我们提出了BloombergGPT，一个拥有500亿个参数的语言模型，它是基于广泛的金融数据进行训练的。我们构建了一种3630亿个标记的数据集，该数据集基于彭博社的广泛数据来源，可能是迄今最大的领域特定数据集，同时又增加了来自通用数据集的3450亿个标记。我们在标准LLM基准、开放式金融基准和一套最能准确反映我们预期用途的内部基准上验证了BloombergGPT。我们的混合数据集训练产生了一个在金融任务上明显优于现有模型的模型，同时不会牺牲普通任务的性能。

    The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general 
    
[^82]: SemEval-2023任务3中的单语和多语言方法：Team SheffieldVeraAI在新闻类型、主题和说服技巧分类方面的表现

    Team SheffieldVeraAI at SemEval-2023 Task 3: Mono and multilingual approaches for news genre, topic and persuasion technique classification. (arXiv:2303.09421v1 [cs.CL])

    [http://arxiv.org/abs/2303.09421](http://arxiv.org/abs/2303.09421)

    本文介绍了Team SheffieldVeraAI在SemEval-2023任务3中的表现。他们提出了用于新闻类型、框架和说服技巧分类的单语和多语言方法。该团队使用多种模型和适配器，取得了在不同语言下的好成绩。

    

    本文描述了我们在多语言环境下应用的方法，用于SemEval-2023任务3：在在线新闻中检测类别、框架和说服技巧。 对于子任务1（新闻类型），我们提出了一个完全训练和适配器mBERT模型的集成，其在德语中排名第一，并且具有多语言团队中最高的平均排名。 对于子任务2（框架），我们使用两个单独的集成：一个单语RoBERTa-MUPPETLARGE和一个XLM-RoBERTaLARGE的集成，分别使用适配器和任务自适应预训练，在3种语言中获得第一名，并在所有语言中获得最佳平均排名。 对于子任务3（说服技巧），我们为英语训练了一个单语言RoBERTa-Base模型和一个适用于剩余语言的多语言mBERT模型，其在所有语言中均排名前10，其中英语排名第二。 对于每个子任务，我们比较了单语和多语言方法，并考虑了类别不平衡技术。

    This paper describes our approach for SemEval-2023 Task 3: Detecting the category, the framing, and the persuasion techniques in online news in a multi-lingual setup. For Subtask 1 (News Genre), we propose an ensemble of fully trained and adapter mBERT models which was ranked joint-first for German, and had the highest mean rank of multi-language teams. For Subtask 2 (Framing), we achieved first place in 3 languages, and the best average rank across all the languages, by using two separate ensembles: a monolingual RoBERTa-MUPPETLARGE and an ensemble of XLM-RoBERTaLARGE with adapters and task adaptive pretraining. For Subtask 3 (Persuasion Techniques), we train a monolingual RoBERTa-Base model for English and a multilingual mBERT model for the remaining languages, which achieved top 10 for all languages, including 2nd for English. For each subtask, we compare monolingual and multilingual approaches, and consider class imbalance techniques.
    
[^83]: 对一种大型多语言语言模型（BLOOM）的翻译表现进行研究：WMT、Flores-101和DiaBLa数据集的评估

    Investigating the Translation Performance of a Large Multilingual Language Model: the Case of BLOOM. (arXiv:2303.01911v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.01911](http://arxiv.org/abs/2303.01911)

    研究发现，多语言语言模型BLOOM的0-shot性能存在问题，但在几个shot的情况下，表现得到极大的改善，特别是在某些语言对中表现非常好。

    

    最近，NLP社区见证了新的大型开放式多语言语言模型BLOOM的发布，覆盖了46种语言。本文通过评估BLOOM在多个数据集中（WMT、Flores-101和DiaBLa）和语言对（高资源和低资源）的机器翻译表现，重点关注BLOOM的多语言能力。我们的研究结果显示，0-shot性能存在过度生成和生成错误语言的问题，但在几个shot的情况下，这得到了极大的改善，在某些语言对中获得了非常好的结果。我们研究了多个方面，包括提示设计、模型大小、跨语言转移和使用话语背景。

    The NLP community recently saw the release of a new large open-access multilingual language model, BLOOM (BigScience et al., 2022) covering 46 languages. We focus on BLOOM's multilingual ability by evaluating its machine translation performance across several datasets (WMT, Flores-101 and DiaBLa) and language pairs (high- and low-resourced). Our results show that 0-shot performance suffers from overgeneration and generating in the wrong language, but this is greatly improved in the few-shot setting, with very good results for a number of language pairs. We study several aspects including prompt design, model sizes, cross-lingual transfer and the use of discursive context.
    
[^84]: 优化算法的符号式发现

    Symbolic Discovery of Optimization Algorithms. (arXiv:2302.06675v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06675](http://arxiv.org/abs/2302.06675)

    该论文提出了一种将算法发现视为程序搜索的方法，并用于发现深度神经网络训练的优化算法。他们的方法发现了一种简单而有效的优化算法Lion，它比Adam更节省内存并且在ImageNet上的准确率提高了2％，并且预训练的计算时间也减少了多达5倍。

    

    我们提出了一种将算法发现视为程序搜索的方法，并应用于发现用于深度神经网络训练的优化算法。我们利用高效搜索技术来探索无限和稀疏的程序空间。为了填补代理任务和目标任务之间巨大的泛化差距，我们还引入了程序选择和简化策略。我们的方法发现了一种简单而有效的优化算法，$ \textbf {Lion} $（$ \textit {Evo $\textbf {L} $ved S $ \textbf {i} $ gn M $ \textbf {o} $ me $ \textbf {n} $ tum} $）。它的记忆效率比Adam更高，因为它只跟踪动量。与自适应优化器不同，通过符号运算计算的每个参数的更新具有相同的大小。我们将Lion与广泛使用的优化器（例如Adam和Adafactor）进行了比较，以在不同任务上训练各种模型。在图像分类中，Lion将在ImageNet上ViT的准确性提高了最多2％，并节省了多达5倍的预训练计算时间。

    We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\textbf{Lion}$ ($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compu
    
[^85]: 区分度校准到上下文学习中的应用

    Distinguishability Calibration to In-Context Learning. (arXiv:2302.06198v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.06198](http://arxiv.org/abs/2302.06198)

    本文提出了一种旋转和缩放的特征变换校准方法，可用于基于提示的学习进行文本分类，从而解决了在转换器中进行上下文学习时遇到的信息扩散问题。

    

    近年来，随着对基于提示的学习方法的兴趣增加，模型能够在少量标注实例上进行训练，使它们适用于低资源环境。使用基于提示的学习进行文本分类时，目标是使用预训练语言模型 (PLM) 来预测预定义模板中的缺失标记，并将其映射到类别标签。然而，基于转换器架构构建的 PLM 倾向于生成相似的输出嵌入，很难区分不同的类别标签。当处理涉及许多细粒度类别标签的分类任务时，这个问题会进一步加剧。本文通过提出基于特征旋转和缩放的校准方法来缓解这个信息扩散问题，即当不同的令牌经过转换器中堆叠的多个自注意层时，它们共享大量相似的信息。

    Recent years have witnessed increasing interests in prompt-based learning in which models can be trained on only a few annotated instances, making them suitable in low-resource settings. When using prompt-based learning for text classification, the goal is to use a pre-trained language model (PLM) to predict a missing token in a pre-defined template given an input text, which can be mapped to a class label. However, PLMs built on the transformer architecture tend to generate similar output embeddings, making it difficult to discriminate between different class labels. The problem is further exacerbated when dealing with classification tasks involving many fine-grained class labels. In this work, we alleviate this information diffusion issue, i.e., different tokens share a large proportion of similar information after going through stacked multiple self-attention layers in a transformer, by proposing a calibration method built on feature transformations through rotation and scaling to m
    
[^86]: 创造一个哲学家的大型语言模型

    Creating a Large Language Model of a Philosopher. (arXiv:2302.01339v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.01339](http://arxiv.org/abs/2302.01339)

    该研究使用OpenAI的大型语言模型GPT-3和哲学家丹尼特的作品为训练数据，探索了生成哲学文本的能力。研究人员通过招募大量参与者来区分真正的哲学家丹尼特和机器生成的文字。专家成功率达到51％，但没有达到预期的80％，该模型有可能超越人类的思维能力。

    

    能否训练大型语言模型来生成难以与人类哲学家的文本区分的哲学文字？为了解决这个问题，我们使用哲学家丹尼特的作品作为额外的训练数据来微调OpenAI的GPT-3。为了探索丹尼特模型，我们向真正的丹尼特提出了十个哲学问题，然后向语言模型提出了相同的问题，每个问题收集了四个回答，没有进行筛选。我们招募了425名参与者来区分丹尼特的答案和四个机器生成的答案。熟悉丹尼特作品的专家（N = 25）的成功率为51％，高于20％的机会率，但不及我们预期的80％的正确率。对于其中的两个问题，语言模型至少生成了一个答案，专家们更频繁地选择该答案而非丹尼特自己的答案。哲学博客读者（N = 302）的表现与专家相似，而普通研究参与者（N = 98）则近似于随机猜测。

    Can large language models be trained to produce philosophical texts that are difficult to distinguish from texts produced by human philosophers? To address this question, we fine-tuned OpenAI's GPT-3 with the works of philosopher Daniel C. Dennett as additional training data. To explore the Dennett model, we asked the real Dennett ten philosophical questions and then posed the same questions to the language model, collecting four responses for each question without cherry-picking. We recruited 425 participants to distinguish Dennett's answer from the four machine-generated answers. Experts on Dennett's work (N = 25) succeeded 51% of the time, above the chance rate of 20% but short of our hypothesized rate of 80% correct. For two of the ten questions, the language model produced at least one answer that experts selected more frequently than Dennett's own answer. Philosophy blog readers (N = 302) performed similarly to the experts, while ordinary research participants (N = 98) were near 
    
[^87]: 基于大型语言模型的自适应机器翻译

    Adaptive Machine Translation with Large Language Models. (arXiv:2301.13294v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.13294](http://arxiv.org/abs/2301.13294)

    本文研究了如何利用大型语言模型的上下文学习来改进实时自适应机器翻译，实验结果表明有希望的效果。

    This paper investigates how to use in-context learning of large language models to improve real-time adaptive machine translation, and the experimental results show promising effects.

    一致性是高质量翻译的关键要求。在特定领域的项目中，遵循预先批准的术语并适应更正的翻译尤为重要。机器翻译（MT）在领域适应方面取得了重大进展。然而，实时适应仍然具有挑战性。最近，大规模语言模型（LLM）展示了在上下文学习方面的有趣能力，它们学习复制某些输入-输出文本生成模式，而无需进一步微调。通过在推理时间将LLM提供给由翻译对列表组成的提示，它可以模拟领域和风格特征。本文旨在研究如何利用上下文学习来改进实时自适应MT。我们的广泛实验在翻译时间显示出有希望的结果。例如，GPT-3.5可以在翻译新句子时适应一组领域内的句子对和/或术语。我们的研究表明，基于大型语言模型的自适应机器翻译是可行的。

    Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, GPT-3.5 can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We ob
    
[^88]: 不依赖注意力机制的预训练

    Pretraining Without Attention. (arXiv:2212.10544v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10544](http://arxiv.org/abs/2212.10544)

    本文通过使用基于状态空间模型的序列路由方法提出了一种不依赖注意力机制的预训练模型BiGS，可以达到与BERT预训练准确度相当的GLUE测试结果，并具有不同的归纳偏差。

    

    在自然语言处理中，Transformer模型是预训练中取得成功的关键。虽然也有其他架构被用于预训练，但下游任务的准确率要么显著下降，要么需要注意力机制才能达到标准测试的基准（如GLUE）。本文探讨了一种不依赖注意力机制的预训练方法，采用最近在基于状态空间模型（SSM）的序列路由方面的进展。我们提出的模型Bidirectional Gated SSM（BiGS）结合了SSM层和乘性门控架构，这在简化序列建模架构中已经被证明是有效的。该模型学习不考虑成对交互的静态层。即使如此，BiGS能够达到与BERT预训练准确度相当的GLUE测试结果，并且可以在不进行近似的情况下扩展到4096个标记的长形式预训练。分析表明，尽管这些模型的平均准确率相似，但与BERT相比，这种方法在交互和句法表示方面具有不同的归纳偏差。本文所有模型可在 https://git 上获得。

    Transformers have been essential to pretraining success in NLP. While other architectures have been used, downstream accuracy is either significantly worse, or requires attention layers to match standard benchmarks such as GLUE. This work explores pretraining without attention by using recent advances in sequence routing based on state-space models (SSMs). Our proposed model, Bidirectional Gated SSM (BiGS), combines SSM layers with a multiplicative gating architecture that has been effective in simplified sequence modeling architectures. The model learns static layers that do not consider pair-wise interactions. Even so, BiGS is able to match BERT pretraining accuracy on GLUE and can be extended to long-form pretraining of 4096 tokens without approximation. Analysis shows that while the models have similar average accuracy, the approach has different inductive biases than BERT in terms of interactions and syntactic representations. All models from this work are available at https://git
    
[^89]: SeqDiffuSeq: 一种使用编码器-解码器Transformer的文本扩散模型用于序列生成

    SeqDiffuSeq: Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation. (arXiv:2212.10325v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10325](http://arxiv.org/abs/2212.10325)

    本文提出了一种名为SeqDiffuSeq的文本扩散模型，用于序列生成，采用了编码器-解码器Transformer架构和自适应噪声调度技术，旨在探索扩散模型在自然语言生成方面的性能表现。

    

    扩散模型是一种新的生成建模范式，在图像、音频和视频生成方面取得了巨大成功。然而，考虑到文本的离散分类性质，将连续扩散模型扩展到自然语言并不是微不足道的，而且文本扩散模型研究较少。序列生成是自然语言处理中至关重要的话题之一。在本文中，我们将扩散模型应用于序列生成，探索扩散模型的优越生成性能能否转移到自然语言领域。我们提出SeqDiffuSeq，一种用于序列生成的文本扩散模型。SeqDiffuSeq使用编码器-解码器Transformer架构来建模去噪函数。为了提高生成质量，SeqDiffuSeq结合了自我调节技术和一个新提出的自适应噪声调度技术。自适应噪声调度具有均匀去噪的困难

    Diffusion model, a new generative modelling paradigm, has achieved great success in image, audio, and video generation. However, considering the discrete categorical nature of text, it is not trivial to extend continuous diffusion models to natural language, and text diffusion models are less studied. Sequence-to-sequence text generation is one of the essential natural language processing topics. In this work, we apply diffusion models to approach sequence-to-sequence text generation, and explore whether the superiority generation performance of diffusion model can transfer to natural language domain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequence generation. SeqDiffuSeq uses an encoder-decoder Transformers architecture to model denoising function. In order to improve generation quality, SeqDiffuSeq combines the self-conditioning technique and a newly proposed adaptive noise schedule technique. The adaptive noise schedule has the difficulty of denoising evenly 
    
[^90]: GanLM: 带辅助鉴别器的编码器-解码器预训练

    GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator. (arXiv:2212.10218v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10218](http://arxiv.org/abs/2212.10218)

    本文提出了一种名为GanLM的编码器-解码器预训练模型，它引入了辅助鉴别器来统一语言理解和生成能力，并使用两个预训练目标进行训练：替换令牌检测和替换令牌去噪。

    

    预训练模型在自然语言处理领域取得了显著的成功。然而，现有的预训练方法未充分利用语言理解对生成的好处。受生成对抗网络（GANs）的思想启发，我们提出了一种GAN风格的编码器-解码器预训练模型，通过引入辅助鉴别器，在单个模型中统一了语言理解和生成的能力。我们的模型名为GanLM，使用两个预训练目标进行训练：替换令牌检测和替换令牌去噪。具体来说，给定掩码源句子，生成器输出目标分布，鉴别器预测从分布中抽样的目标令牌是否不正确。将目标句子替换为错误分类的令牌以构造带噪声的先前上下文，用于生成黄金句子。总的来说，这两个任务通过有选择性地提高语言理解和生成的能力。

    Pre-trained models have achieved remarkable success in natural language processing (NLP). However, existing pre-training methods underutilize the benefits of language understanding for generation. Inspired by the idea of Generative Adversarial Networks (GANs), we propose a GAN-style model for encoder-decoder pre-training by introducing an auxiliary discriminator, unifying the ability of language understanding and generation in a single model. Our model, named as GanLM, is trained with two pre-training objectives: replaced token detection and replaced token denoising. Specifically, given masked source sentences, the generator outputs the target distribution and the discriminator predicts whether the target sampled tokens from distribution are incorrect. The target sentence is replaced with misclassified tokens to construct noisy previous context, which is used to generate the gold sentence. In general, both tasks improve the ability of language understanding and generation by selectivel
    
[^91]: 使用ReLM验证大型语言模型

    Validating Large Language Models with ReLM. (arXiv:2211.15458v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15458](http://arxiv.org/abs/2211.15458)

    ReLM是一种使用正则表达式验证和查询LLM的系统，可以解决LLM数据记忆、偏见、毒性和语言理解等问题，具有高效性和广泛性。

    

    即使大型语言模型(LLM)因为可以生成自然的文本而备受推崇，但是越来越多人关注LLM可能带来的负面影响，如数据记忆、偏见和不恰当语言使用。不幸的是，LLM的复杂性和生成能力使得验证（和纠正）这些问题变得困难。在这项工作中，我们介绍了ReLM，这是一种使用标准正则表达式验证和查询LLM的系统。ReLM将广泛的语言模型评估形式化并启用，将复杂的评估规则简化为简单的正则表达式查询。我们的结果探索了关于记忆、性别偏见、毒性和语言理解的查询，显示ReLM相比最先进的特定查询技术达到了高达15倍的系统效率、2.5倍的数据效率以及更广泛的统计和提示调整覆盖范围。ReLM为越来越重要的LLM验证问题提供了竞争性和通用的基准。

    Although large language models (LLMs) have been touted for their ability to generate natural-sounding text, there are growing concerns around possible negative effects of LLMs such as data memorization, bias, and inappropriate language. Unfortunately, the complexity and generation capacities of LLMs make validating (and correcting) such concerns difficult. In this work, we introduce ReLM, a system for validating and querying LLMs using standard regular expressions. ReLM formalizes and enables a broad range of language model evaluations, reducing complex evaluation rules to simple regular expression queries. Our results exploring queries surrounding memorization, gender bias, toxicity, and language understanding show that ReLM achieves up to 15x higher system efficiency, 2.5x data efficiency, and increased statistical and prompt-tuning coverage compared to state-of-the-art ad-hoc queries. ReLM offers a competitive and general baseline for the increasingly important problem of LLM valida
    
[^92]: 全局和本地分层感知对比框架用于隐含篇章关系识别

    Global and Local Hierarchy-aware Contrastive Framework for Implicit Discourse Relation Recognition. (arXiv:2211.13873v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.13873](http://arxiv.org/abs/2211.13873)

    本文提出了GOLF框架，它能够充分利用全局和本地感知层次结构来提升隐含篇章关系识别效果。

    

    由于缺乏显式的连接词，隐含篇章关系识别(IDRR)仍然是篇章分析中的难题。IDRR的关键步骤是学习两个论点之间高质量的篇章关系表示。最近的方法趋向于将整个感知层次结构信息整合到篇章关系表示中进行多级别感知识别。然而，它们未能充分整合包含所有感知的静态分层结构（定义为全局分层结构），并忽略了与每个实例对应的层次感知标签序列（定义为本地分层结构）。为了充分利用全局和本地感知层次结构来学习更好的篇章关系表示，我们提出了一种新颖的全局和本地分层感知对比框架(GOLF)，借助于多任务学习和对比学习来模拟两种层次感知。在PDTB 2.0和PDTB-EDT语料库上的实验结果表明，所提出的GOLF在IDRR方面明显优于现有的最先进方法。

    Due to the absence of explicit connectives, implicit discourse relation recognition (IDRR) remains a challenging task in discourse analysis. The critical step for IDRR is to learn high-quality discourse relation representations between two arguments. Recent methods tend to integrate the whole hierarchical information of senses into discourse relation representations for multi-level sense recognition. Nevertheless, they insufficiently incorporate the static hierarchical structure containing all senses (defined as global hierarchy), and ignore the hierarchical sense label sequence corresponding to each instance (defined as local hierarchy). For the purpose of sufficiently exploiting global and local hierarchies of senses to learn better discourse relation representations, we propose a novel GlObal and Local Hierarchy-aware Contrastive Framework (GOLF), to model two kinds of hierarchies with the aid of multi-task learning and contrastive learning. Experimental results on PDTB 2.0 and PDTB
    
[^93]: 自监督语音模型一次性序列压缩

    Once-for-All Sequence Compression for Self-Supervised Speech Models. (arXiv:2211.02332v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.02332](http://arxiv.org/abs/2211.02332)

    本文提出了一种自监督语音模型的一次性序列压缩框架，该框架支持连续的操作压缩率范围，并在各种任务上展现出平滑的性能效率权衡。

    

    在语音处理中，时间轴上的序列长度通常是计算的主要因素。为了降低自监督语音模型的计算成本，已经提出了一些方法来减少序列长度。然而，不同的下游任务对序列压缩有不同的容忍度，因此生产固定压缩率的模型可能不适用于所有任务。本文介绍了一种自监督语音模型的一次性序列压缩框架，支持连续的操作压缩率范围。该框架在各种任务上进行了评估，与固定压缩率变体相比，表现出平滑的性能效率权衡。我们进一步探讨了自适应压缩率学习，演示了选择任务特定的优先帧时期的能力，无需进行网格搜索。

    The sequence length along the time axis is often the dominant factor of the computation in speech processing. Works have been proposed to reduce the sequence length for lowering the computational cost in self-supervised speech models. However, different downstream tasks have different tolerance of sequence compressing, so a model that produces a fixed compressing rate may not fit all tasks. In this work, we introduce a once-for-all (OFA) sequence compression framework for self-supervised speech models that supports a continuous range of operating compressing rates. The framework is evaluated on various tasks, showing marginal degradation compared to the fixed compressing rate variants with a smooth performance-efficiency trade-off. We further explore adaptive compressing rate learning, demonstrating the ability to select task-specific preferred frame periods without needing a grid search.
    
[^94]: 针对Facebook上的政治活动的弱监督学习

    Weakly Supervised Learning for Analyzing Political Campaigns on Facebook. (arXiv:2210.10669v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.10669](http://arxiv.org/abs/2210.10669)

    本研究提出了一种基于弱监督学习的方法，通过分析Facebook上政治广告的立场和议题，以及其使用人口统计学定位，来了解政治活动的特点和时间动态。

    

    社交媒体平台目前是政治信息传播的主要渠道，政治家们能够通过这些平台针对特定人群进行宣传，并根据他们的反应进行调整。然而，使这种交流透明化是具有挑战性的，因为信息传播与目标受众紧密相连，并经常被多个利益攸关方共同传播。本文旨在第一步了解这些高度分散的政治活动。我们提出了一种弱监督的方法来识别Facebook上政治广告的立场和议题，并分析政治活动如何使用某种人口统计学定位，如位置、性别或年龄。此外，我们还分析了选举民意调查中政治广告的时间动态。

    Social media platforms are currently the main channel for political messaging, allowing politicians to target specific demographics and adapt based on their reactions. However, making this communication transparent is challenging, as the messaging is tightly coupled with its intended audience and often echoed by multiple stakeholders interested in advancing specific policies. Our goal in this paper is to take a first step towards understanding these highly decentralized settings. We propose a weakly supervised approach to identify the stance and issue of political ads on Facebook and analyze how political campaigns use some kind of demographic targeting by location, gender, or age. Furthermore, we analyze the temporal dynamics of the political ads on election polls.
    
[^95]: 人口统计因素是否能够改善文本分类？在变压器时代重新审视人口统计适应性

    Can Demographic Factors Improve Text Classification? Revisiting Demographic Adaptation in the Age of Transformers. (arXiv:2210.07362v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.07362](http://arxiv.org/abs/2210.07362)

    论文探讨利用人口统计因素加强文本分类的效果与先前一样在最新的变压器语言模型中存在。该研究使用连续语言建模和动态多任务学习的方法来适应语言中特定年龄和性别的方面，通过结合语言建模目标和人口统计学的预测，结果表明在四种语言中任务表现得到了显著的提高。

    

    人口统计因素（例如性别或年龄）塑造了我们的语言。以往的研究表明，利用传统的自然语言处理模型整合人口统计因素可以在各种NLP任务中持续提高性能。本文探究先前的发现是否仍然适用于最先进的预训练变压器语言模型(PLMs)。我们采用了三种常见的专业化方法，将装载了外部知识（例如领域专业知识或地理知识）的预训练变压器进行了改进 。我们使用连续语言建模和动态多任务学习以用于适应性的性别和年龄的语言表示，其中我们结合语言建模目标和人口统计分类的预测。当使用多语言PLM时，我们的结果表明，在四种语言（英语、德语、法语和丹麦语）中，任务的性能得到了显著的提高，这与先前的研究结果一致。

    Demographic factors (e.g., gender or age) shape our language. Previous work showed that incorporating demographic factors can consistently improve performance for various NLP tasks with traditional NLP models. In this work, we investigate whether these previous findings still hold with state-of-the-art pretrained Transformer-based language models (PLMs). We use three common specialization methods proven effective for incorporating external knowledge into pretrained Transformers (e.g., domain-specific or geographic knowledge). We adapt the language representations for the demographic dimensions of gender and age, using continuous language modeling and dynamic multi-task learning for adaptation, where we couple language modeling objectives with the prediction of demographic classes. Our results, when employing a multilingual PLM, show substantial gains in task performance across four languages (English, German, French, and Danish), which is consistent with the results of previous work. H
    
[^96]: 无关语言的多语种信息检索与对比学习

    Language Agnostic Multilingual Information Retrieval with Contrastive Learning. (arXiv:2210.06633v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2210.06633](http://arxiv.org/abs/2210.06633)

    该论文提出一种使用对比学习的技术，利用平行和非平行语料库来提高多语种信息检索的效果，仅使用英语IR训练数据和一些平行语料库即可在非英语数据上实现显著的检索性能改进。

    

    多语种信息检索具有挑战性，因为在许多语言中获取经过注释的训练数据成本很高。我们提出了一种有效的方法，在只有英语IR训练数据和英语与其他语言之间的一些平行语料库可用时训练多语种IR系统。我们利用平行和非平行语料库来提高预训练多语种语言模型的跨语言传递能力，并设计了一个语义对比损失，以对齐在不同语言中具有相同语义的平行句子的表示，以及一种新的语言对比损失，利用平行句子对从非平行语料库中的句子表示中删除语言特定信息。在使用这些损失对英语IR数据进行训练并在非英语数据上进行零-shot评估时，我们的模型表现出明显的改进，同时需要较少的计算资源。我们还证明了该方法的实用价值。

    Multilingual information retrieval (IR) is challenging since annotated training data is costly to obtain in many languages. We present an effective method to train multilingual IR systems when only English IR training data and some parallel corpora between English and other languages are available. We leverage parallel and non-parallel corpora to improve the pretrained multilingual language models' cross-lingual transfer ability. We design a semantic contrastive loss to align representations of parallel sentences that share the same semantics in different languages, and a new language contrastive loss to leverage parallel sentence pairs to remove language-specific information in sentence representations from non-parallel corpora. When trained on English IR data with these losses and evaluated zero-shot on non-English data, our model demonstrates significant improvement to prior work on retrieval performance, while it requires much less computational effort. We also demonstrate the valu
    
[^97]: 命名实体识别的深度跨度表示

    Deep Span Representations for Named Entity Recognition. (arXiv:2210.04182v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.04182](http://arxiv.org/abs/2210.04182)

    本研究提出了DSpERT模型，通过跨度Transformer逐层聚合标记表示作为键和值，产生了深层语义的跨度表示，从而解决了现有跨度基础NER系统中长跨度实体显着无效性和重叠跨度表示的耦合问题。实验结果表明，DSpERT在八个NER基准测试中取得了性能高于或与最新最先进系统竞争的成果。

    

    跨度基础模型是命名实体识别（NER）最简单直接的方法之一。 现有的跨度基础NER系统将标记表示浅层聚合到跨度表示中。 但是，这通常导致长跨度实体的显着无效性，重叠跨度表示的耦合，最终性能下降。 在本研究中，我们提出了DSpERT（来自Transformer的深度跨度编码器表示），它由标准Transformer和跨度Transformer组成。 后者使用低层次的跨度表示作为查询，并从底部到顶部逐层聚合标记表示作为键和值，因此，DSpERT产生了深层语义的跨度表示。 借助预训练语言模型的权重初始化，DSpERT在八个NER基准测试中取得了高于或与最新的最先进系统竞争的性能。 实验结果验证了深度对跨度基础NER系统的重要性。

    Span-based models are one of the most straightforward methods for named entity recognition (NER). Existing span-based NER systems shallowly aggregate the token representations to span representations. However, this typically results in significant ineffectiveness for long-span entities, a coupling between the representations of overlapping spans, and ultimately a performance degradation. In this study, we propose DSpERT (Deep Span Encoder Representations from Transformers), which comprises a standard Transformer and a span Transformer. The latter uses low-layered span representations as queries, and aggregates the token representations as keys and values, layer by layer from bottom to top. Thus, DSpERT produces span representations of deep semantics.  With weight initialization from pretrained language models, DSpERT achieves performance higher than or competitive with recent state-of-the-art systems on eight NER benchmarks. Experimental results verify the importance of the depth for s
    
[^98]: 冷启动情况下的数据选择策略：一种基于提示信息传递不确定性估计的few-shot语言模型微调方法

    Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach. (arXiv:2209.06995v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.06995](http://arxiv.org/abs/2209.06995)

    本文提出了一种名为PATRON的方法，使用基于提示信息的不确定性的数据选择策略来提高预训练语言模型微调的few-shot性能，在六个文本分类数据集上实验证实该方法的性能优于最先进的冷启动数据选择基线，且仅使用128标签的情况下，该方法可以达到91.0%和92.1%的完全监督性能。

    

    大型语言模型展现出了出色的few-shot性能，但性能对于few-shot实例的选择非常敏感。我们提出了一种名为PATRON的新方法，该方法使用基于提示信息的不确定性估计来选择预训练语言模型微调的数据，在冷启动情况下没有初始标记数据可用。在PATRON中，我们设计了（1）基于提示信息的不确定性传播方法来估计数据点的重要性和（2）一种分割-重写（PTR）策略，以在查询注释时促进样本的多样性。在六个文本分类数据集上的实验表明，PATRON的表现比最强的冷启动数据选择基线优越了6.9%。此外，仅使用128标签，PATRON基于普通微调和基于提示信息学习分别达到了91.0%和92.1%的完全监督性能。我们的PATRON实现可在\url{https://github.com/yueyu1030/Patron}上获得。

    Large Language Models have demonstrated remarkable few-shot performance, but the performance can be sensitive to the selection of few-shot instances. We propose PATRON, a new method that uses prompt-based uncertainty estimation for data selection for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON achieves 91.0% and 92.1% of the fully supervised performance based on vanilla fine-tuning and prompt-based learning respectively. Our implementation of PATRON is available at \url{https://github.com/yueyu1030/Patron}.
    
[^99]: 论现实和语言数据限制：将LLMs与人类规范对齐

    On Reality and the Limits of Language Data: Aligning LLMs with Human Norms. (arXiv:2208.11981v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.11981](http://arxiv.org/abs/2208.11981)

    本文研究了大型语言模型（LLMs）在仅使用语言数据的情况下理解物理世界的能力，使用新颖且严密控制的推理测试（ART）与人类规范进行对比，研究发现了LLMs在某些常识关系模型中可以直接从数据中学习，但存在弱点，例如在部分和包含关系方面表现不足。

    

    最近，大型语言模型（LLMs）在利用大量自然语言数据中的语言关联进行实际应用方面取得了进展。然而，它们仅使用语言数据来理解物理世界的能力仍有疑问。在回顾现有协议之后，我们使用一种新颖且严密控制的推理测试（ART）来探讨这个问题，并比较人类规范与GPT-3版本之间的差异。我们的研究结果突出了通常可以直接从数据中学习的常识关系模型类别以及弱点所在。GPT-3为包括同义词、反义词和默认继承在内的几个关系方面提供了与人类主体相当的口头推理证据。没有来自人类判断的强化学习，GPT-3在具有部分和包含关系方面表现的区间下限处。在必要品质、大小顺序和强度顺序等方面也观察到了不足之处。把LLMs与象征性方法相结合，

    Recent advancements in Large Language Models (LLMs) harness linguistic associations in vast natural language data for practical applications. However, their ability to understand the physical world using only language data remains a question. After reviewing existing protocols, we explore this question using a novel and tightly controlled reasoning test (ART) and compare human norms against versions of GPT-3. Our findings highlight the categories of common-sense relations models that could learn directly from data and areas of weakness. GPT-3 offers evidence for verbal reasoning on a par with human subjects for several relations including Synonymy, Antonymy, and Default inheritance, Without reinforcement learning from human judgements, it appears GPT-3 performs at the lower end of the reference interval for Has-part and Contained-in. Weaknesses were observed also in affordance characteristics through Necessary-quality, Order-of-size and Order-of-intensity. Combining LLMs with symbolic 
    
[^100]: 一种多模态变换器：将临床笔记与结构化 EHR 数据融合以解释住院死亡风险预测

    A Multimodal Transformer: Fusing Clinical Notes with Structured EHR Data for Interpretable In-Hospital Mortality Prediction. (arXiv:2208.10240v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.10240](http://arxiv.org/abs/2208.10240)

    本论文提出了一种新型多模态转换器，融合临床笔记和结构化的EHR数据，以更好地预测住院死亡风险。通过集成梯度方法选择临床笔记中的关键词和利用 Shapley 值发现重要的结构化 EHR 特征，并对其进行可视化解释，提高了模型的解释性。

    

    基于深度学习的结构化电子健康记录（EHR）临床决策支持一直是预测死亡风险和疾病的研究重点。同时，大量的叙述性临床笔记提供了补充信息，但常常未集成到预测模型中。本文提出一种新型多模态转换器，融合临床笔记和结构化的 EHR 数据，以更好地预测住院死亡风险。为了提高解释性，我们提出了一种集成梯度（IG）方法来选择临床笔记中的重要词语，并使用 Shapley 值发现重要的结构化 EHR 特征。这些重要的词语和临床特征可视化以协助解释预测结果。我们还研究了领域自适应预训练和任务自适应微调对临床 BERT 的意义，该模型用于学习临床笔记的表示。实验表明，我们的模型优于现有技术。

    Deep-learning-based clinical decision support using structured electronic health records (EHR) has been an active research area for predicting risks of mortality and diseases. Meanwhile, large amounts of narrative clinical notes provide complementary information, but are often not integrated into predictive models. In this paper, we provide a novel multimodal transformer to fuse clinical notes and structured EHR data for better prediction of in-hospital mortality. To improve interpretability, we propose an integrated gradients (IG) method to select important words in clinical notes and discover the critical structured EHR features with Shapley values. These important words and clinical features are visualized to assist with interpretation of the prediction outcomes. We also investigate the significance of domain adaptive pretraining and task adaptive fine-tuning on the Clinical BERT, which is used to learn the representations of clinical notes. Experiments demonstrated that our model o
    
[^101]: BERT WEAVER：使用加权平均使生物医学领域基于Transformer的模型实现终身学习

    BERT WEAVER: Using WEight AVERaging to Enable Lifelong Learning for Transformer-based Models in the Biomedical Domain. (arXiv:2202.10101v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.10101](http://arxiv.org/abs/2202.10101)

    研究提出了WEAVER方法，它可以将旧知识融入到新模型中，以有效降低灾难性遗忘，并实现生物医学领域基于Transformer的模型的终身学习。

    

    最近在转移学习方面的发展推动了自然语言处理任务的进展。然而，性能取决于高质量的手动标注训练数据。尤其是在生物医学领域，已经表明一种训练语料库不足以学习能够在新数据上高效预测的通用模型。因此，最先进的模型需要具备终身学习的能力，以便在新数据可用时提高性能 - 而无需从头开始重新训练整个模型。我们提出WEAVER，一种简单但高效的后处理方法，将旧知识融入到新模型中，从而减少灾难性遗忘。我们展示了顺序应用WEAVER会产生类似于一次性使用所有数据进行联合训练的词嵌入分布，同时计算效率更高。由于没有数据共享的必要，因此所介绍的方法在数据隐私是一项关注的情况下也可以轻松应用。

    Recent developments in transfer learning have boosted the advancements in natural language processing tasks. The performance is, however, dependent on high-quality, manually annotated training data. Especially in the biomedical domain, it has been shown that one training corpus is not enough to learn generic models that are able to efficiently predict on new data. Therefore, state-of-the-art models need the ability of lifelong learning in order to improve performance as soon as new data are available - without the need of re-training the whole model from scratch. We present WEAVER, a simple, yet efficient post-processing method that infuses old knowledge into the new model, thereby reducing catastrophic forgetting. We show that applying WEAVER in a sequential manner results in similar word embedding distributions as doing a combined training on all data at once, while being computationally more efficient. Because there is no need of data sharing, the presented method is also easily app
    
[^102]: 软子指数Lambek演算的向量空间语义

    Vector Space Semantics for Lambek Calculus with Soft Subexponentials. (arXiv:2111.11331v2 [cs.LO] UPDATED)

    [http://arxiv.org/abs/2111.11331](http://arxiv.org/abs/2111.11331)

    本论文介绍了一种软子指数Lambek演算的向量空间语义，应用于构建委托语缺位名词短语和带有回指和省略的话语单元的组合向量解释，具有很好的应用前景。

    

    我们为软子指数Lambek演算开发了一个向量空间语义，将演算应用于构建委托语缺位名词短语和带有回指和省略的话语单元的组合向量解释，并在分布式句子相似性任务中实验证明。与以往使用相关模态的Lambek演算不同，本文中使用的演算采用了一个有界版本的模态，且可判定。这种新的模态的向量空间语义允许我们有意义地定义收缩为投影，并提供了一个线性理论，使我们能够通过非线性映射来实现的内容变得可以实现。

    We develop a vector space semantics for Lambek Calculus with Soft Subexponentials, apply the calculus to construct compositional vector interpretations for parasitic gap noun phrases and discourse units with anaphora and ellipsis, and experiment with the constructions in a distributional sentence similarity task. As opposed to previous work, which used Lambek Calculus with a Relevant Modality the calculus used in this paper uses a bounded version of the modality and is decidable. The vector space semantics of this new modality allows us to meaningfully define contraction as projection and provide a linear theory behind what we could previously only achieve via nonlinear maps.
    
[^103]: 基于迁移学习的发音评分方法

    A transfer learning based approach for pronunciation scoring. (arXiv:2111.00976v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2111.00976](http://arxiv.org/abs/2111.00976)

    本文提出了一种基于迁移学习的发音评分方法，可以在针对此任务专门训练的系统数据稀缺的情况下，利用为ASR训练的模型取得更好的成绩，并在EpaDB数据库上实现了20％的性能提升。

    

    单音素级别的发音评分是一个具有挑战性的任务，现有系统的性能距离人工评分还有很大差距。标准系统利用仅包含本地数据的自动语音识别 (ASR) 模型为短语中的每个音素生成一个得分。使用针对此任务专门训练的系统以及非本地数据进行训练可以获得更好的性能。然而，这种方法面临着标注数据稀缺且通常很小的挑战。本文提出了一种基于迁移学习的方法，利用为ASR训练的模型，将其调整为发音评分任务。我们分析了几个设计选择对性能的影响，并将其与最先进的发音质量 (GOP) 系统进行了比较。针对一个优先考虑不必要修正率较低的成本函数，在EpaDB，一个用于发音评分的数据库上，我们的最终系统比GOP系统更好20％。

    Phone-level pronunciation scoring is a challenging task, with performance far from that of human annotators. Standard systems generate a score for each phone in a phrase using models trained for automatic speech recognition (ASR) with native data only. Better performance has been shown when using systems that are trained specifically for the task using non-native data. Yet, such systems face the challenge that datasets labelled for this task are scarce and usually small. In this paper, we present a transfer learning-based approach that leverages a model trained for ASR, adapting it for the task of pronunciation scoring. We analyze the effect of several design choices and compare the performance with a state-of-the-art goodness of pronunciation (GOP) system. Our final system is 20% better than the GOP system on EpaDB, a database for pronunciation scoring research, for a cost function that prioritizes low rates of unnecessary corrections.
    

