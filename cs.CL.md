# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [User Inference Attacks on Large Language Models.](http://arxiv.org/abs/2310.09266) | 本论文研究了在大型语言模型上的用户推理攻击，发现LLMs对于各种微调数据集都很容易受到攻击，尤其是对于离群用户和贡献大量数据的用户。这对保护用户隐私具有重要意义。 |
| [^2] | [PromptRE: Weakly-Supervised Document-Level Relation Extraction via Prompting-Based Data Programming.](http://arxiv.org/abs/2310.09265) | PromptRE是一种弱监督文档级别关系抽取方法，通过结合基于提示的技术，解决了“没有关系”的实例数量不平衡和直接使用预训练模型进行文档关系抽取的问题。 |
| [^3] | [Table-GPT: Table-tuned GPT for Diverse Table Tasks.](http://arxiv.org/abs/2310.09263) | 本文提出了一种新的"表格调优"范式，通过使用从真实表格中合成的多样化表格任务作为训练数据，对语言模型进行训练/微调，以提高其理解表格和执行表格任务的能力。 |
| [^4] | [Political claim identification and categorization in a multilingual setting: First experiments.](http://arxiv.org/abs/2310.09256) | 本文在多语种环境下进行了首次实验，探讨了政治主张的识别和分类。实验使用了德语数据集DebateNet2.0，涵盖了2015年难民危机引发的政策辩论。通过机器翻译和多语种嵌入两种方法进行了评估。 |
| [^5] | [Hypernymy Understanding Evaluation of Text-to-Image Models via WordNet Hierarchy.](http://arxiv.org/abs/2310.09247) | 通过基于WordNet层次结构的方法，我们评估了流行的文本到图像模型对于上义词关系的理解能力。我们提出了两个自动度量标准，能够定量比较不同模型的语言能力，并发现了一些困难的词汇。我们全面评估了一些流行的文本到图像模型，包括GLIDE、Latent Diffusion和Stable Diffusion。 |
| [^6] | [Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration.](http://arxiv.org/abs/2310.09241) | 本文提出了先例增强的法律判决预测框架（PLJP），通过结合大型语言模型（LLM）和领域模型，利用先例相关信息进行预测。 |
| [^7] | [BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models for Sentiment Analysis of Bangla Social Media Posts.](http://arxiv.org/abs/2310.09238) | 本论文提出了针对Bangla社交媒体帖子进行情感分析的任务，通过实验发现在这种低资源语言场景下，使用Transformer模型进行迁移学习可以提高模型的学习效果，并且在对已经在Twitter数据上进行了情感分析任务的模型进一步微调的情况下，模型的性能最好。 |
| [^8] | [AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems.](http://arxiv.org/abs/2310.09233) | AgentCF 是一种基于自主语言代理的协作学习方法，在推荐系统中模拟用户和物品的交互，并优化这两类代理。 |
| [^9] | [Automated Claim Matching with Large Language Models: Empowering Fact-Checkers in the Fight Against Misinformation.](http://arxiv.org/abs/2310.09223) | FACT-GPT是一个基于大型语言模型的框架，旨在自动化事实核查的声称匹配阶段。它通过使用GPT-4生成标记数据集，并使用微调的LLM，在识别与事实核查员先前证实为虚假的声称相匹配的新的社交媒体内容。实验结果表明，我们的方法与更大的预训练LLMs的性能相媲美。 |
| [^10] | ["Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters.](http://arxiv.org/abs/2310.09219) | 本文对LLM生成的推荐信中的性别偏见进行了细致的研究，并设计了评估方法来展现通过语言风格和词汇内容来体现的性别偏见。 |
| [^11] | [Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration.](http://arxiv.org/abs/2310.09168) | 通过采用探索指导的方法，使用大型语言模型 (LLMs) 进行主动探索，增强了领域特定指导调优的数据覆盖范围，并取得了显著的性能提升。 |
| [^12] | [Developing a Natural Language Understanding Model to Characterize Cable News Bias.](http://arxiv.org/abs/2310.09166) | 本论文开发了一种无监督的机器学习方法，通过对有线电视节目提及的主题进行命名实体识别和立场分析的方式，来表征其偏见。在2020年的有线电视转录中应用该方法，发现节目聚类与节目所属的有线电视网络保持一致。揭示了客观评估媒体偏见和表征陌生媒体环境的潜力。 |
| [^13] | [BibRank: Automatic Keyphrase Extraction Platform Using~Metadata.](http://arxiv.org/abs/2310.09151) | 本文介绍了BibRank平台，该平台集成了关键词数据集，并提供了关键词提取算法的评估。BibRank通过使用丰富的BibTeX数据集和创新的加权技术，结合位置、统计和词共现信息，实现了自动关键词提取。 |
| [^14] | [PuoBERTa: Training and evaluation of a curated language model for Setswana.](http://arxiv.org/abs/2310.09141) | 本文介绍了一种名为PuoBERTa的定制掩码语言模型，针对塞茨瓦纳语进行训练，并证明了其在促进塞茨瓦纳语等少研究语言的自然语言处理能力方面的有效性。 |
| [^15] | [The Consensus Game: Language Model Generation via Equilibrium Search.](http://arxiv.org/abs/2310.09139) | 这篇论文介绍了一种新的语言模型解码方法，将其视为规范化的不完美信息序列信号博弈，并通过找到近似均衡点得到了一个解码算法。这种方法可以应用于问答和其他文本生成任务中。 |
| [^16] | [HierarchicalContrast: A Coarse-to-Fine Contrastive Learning Framework for Cross-Domain Zero-Shot Slot Filling.](http://arxiv.org/abs/2310.09135) | 本研究提出了一种用于零样本插槽填充的Hierarchical Contrastive Learning Framework (HiCL)，通过粗粒度到细粒度的对比学习，学习语句令牌之间的深层语义关系，并提出了一种新的迭代标签集语义推理方法，来提高在未见插槽上的泛化能力。 |
| [^17] | [A Frustratingly Easy Plug-and-Play Detection-and-Reasoning Module for Chinese Spelling Check.](http://arxiv.org/abs/2310.09119) | 本文提出了一个令人沮丧的简易即插即用的中文拼写检查的检测和推理模块，通过将任务分解为检测、推理和搜索子任务，有效地利用中文语言的外部知识，并且该模块能够进一步提升现有最先进的非自回归CSC模型的性能。 |
| [^18] | [GLoRE: Evaluating Logical Reasoning of Large Language Models.](http://arxiv.org/abs/2310.09107) | 本论文介绍了GLoRE，一个评估大型语言模型逻辑推理能力的基准，实验结果表明开放式LLM模型的逻辑推理能力需要提高。研究提出了一种自一致性探测方法和微调方法来改进ChatGPT和开放式LLM的性能。 |
| [^19] | [Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large Language Model.](http://arxiv.org/abs/2310.09089) | Qilin-Med是一个多阶段训练的医疗大型语言模型，通过结合领域特定继续预训练、监督微调和直接偏好优化的方法，实现了显著的性能提升，并引入了一个包含医学问答、纯文本、知识图谱和对话的3Gb中医数据集。 |
| [^20] | [Dialect Transfer for Swiss German Speech Translation.](http://arxiv.org/abs/2310.09088) | 本文研究了瑞士德语言翻译系统建设中的挑战，着重探讨了方言多样性和瑞士德语与标准德语之间的差异对系统性能的影响。 |
| [^21] | [KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection.](http://arxiv.org/abs/2310.09044) | 提出了一种名为KCTS的知识约束树搜索解码方法，利用知识分类器和MCTS指导冻结的LM生成与参考知识对齐的文本，同时引入了一种新颖的令牌级幻觉检测方法RIPA。 |
| [^22] | [MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks.](http://arxiv.org/abs/2310.09036) | MM-BigBench是一个评估多模态模型在多模态内容理解任务上表现的综合评估框架，通过多模态交互来实现对多模态上下文的深入理解，并提供广泛的性能评估指标。 |
| [^23] | [Dont Add, dont Miss: Effective Content Preserving Generation from Pre-Selected Text Spans.](http://arxiv.org/abs/2310.09017) | 本论文介绍了一个高质量的受控文本缩减（CTR）模型，解决了内容保留约束不充分强制执行和次优的银标签训练数据的限制，通过在训练和推理中增强内容保留约束，进一步改进了模型性能。 |
| [^24] | [CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules.](http://arxiv.org/abs/2310.08992) | CodeChain是一种通过代表性子模块的自我修订链路引导模块化代码生成的新框架，旨在解决大型语言模型在解决复杂编程任务方面的挑战。 |
| [^25] | [ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models.](http://arxiv.org/abs/2310.08975) | ChatKBQA是一个基于精调大型语言模型的生成-检索框架，用于改进知识库问答的效率和准确性，实验结果显示在多个数据集上取得了新的最好表现。 |
| [^26] | [Towards Example-Based NMT with Multi-Levenshtein Transformers.](http://arxiv.org/abs/2310.08967) | 本论文研究了检索增强的机器翻译的透明度特性，提出了一个新的架构，旨在增加用户对翻译决策的透明度，并通过编辑多个示例来提高翻译效果。 |
| [^27] | [xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark.](http://arxiv.org/abs/2310.08958) | 本论文提出了xDial-Eval，这是一个基于开源英语对话评估数据集构建的多语言开放领域对话评估基准。通过对12个轮次级别和6个对话级别的英语数据集进行扩展，实现了对其他九种语言的对话评估。通过对先前的基于BERT的度量和最近出现的大型语言模型的全面分析，建立了强大的评估基准。 |
| [^28] | [Textual Analysis of ICALEPCS and IPAC Conference Proceedings: Revealing Research Trends, Topics, and Collaborations for Future Insights and Advanced Search.](http://arxiv.org/abs/2310.08954) | 本文通过对ICALEPCS和IPAC会议论文进行文本分析，揭示了研究趋势、主题和合作关系，为未来研究提供洞见和高级搜索工具。 |
| [^29] | [Making Multimodal Generation Easier: When Diffusion Models Meet LLMs.](http://arxiv.org/abs/2310.08949) | EasyGen是一个有效的模型，它通过结合扩散模型和大型语言模型（LLMs）的能力，实现了更容易的多模态生成。相比现有的模型，EasyGen使用了一个名为BiDiffuser的双向条件扩散模型， 提供了更高效的模态交互，并且不仅能够生成文本回复，还能够促进文本到图像的生成。 |
| [^30] | [CAMELL: Confidence-based Acquisition Model for Efficient Self-supervised Active Learning with Label Validation.](http://arxiv.org/abs/2310.08944) | CAMELL是一个适用于序列多输出问题的主动学习框架，通过仅需专家标注序列的一小部分、自监督和标签验证机制来解决监督神经方法对大规模标注数据集的依赖限制。 |
| [^31] | [Multi-level Adaptive Contrastive Learning for Knowledge Internalization in Dialogue Generation.](http://arxiv.org/abs/2310.08943) | 这是一篇关于知识引导对话生成的论文，通过引入多层自适应对比学习（MACL）框架，并在令牌级和序列级上动态采样负例来解决模型简单插入知识片段导致的退化问题。 |
| [^32] | [Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning.](http://arxiv.org/abs/2310.08923) | 本文旨在解决大型语言模型在上下文学习中的不稳定性问题，通过量化信息增益来探索数据示例的信息能力，并采用校准策略来缓解模板偏差。实验证明了该方法的有效性。 |
| [^33] | [Relation-aware Ensemble Learning for Knowledge Graph Embedding.](http://arxiv.org/abs/2310.08917) | 本论文提出了一种关系感知集成学习方法，用于知识图谱嵌入任务，并通过分割搜索合并的算法在搜索关系感知集成权重方面取得了显著性能提升。 |
| [^34] | [Human-in-the-loop Machine Translation with Large Language Model.](http://arxiv.org/abs/2310.08908) | 本研究提出了一种人机协同的流程，通过引导大型语言模型生成定制输出，拓展了机器翻译的能力。 |
| [^35] | [SeqXGPT: Sentence-Level AI-Generated Text Detection.](http://arxiv.org/abs/2310.08903) | 本文介绍了SeqXGPT，这是一种句子级AI生成文本检测方法。通过利用白盒LLMs的对数概率列表作为特征，SeqXGPT在句子级别的AIGT检测中取得了良好的效果。 |
| [^36] | [Welfare Diplomacy: Benchmarking Language Model Cooperation.](http://arxiv.org/abs/2310.08901) | 这项研究提出了福利外交这一通用和变种的零和游戏，目的是衡量和强化语言模型的合作能力，并发现使用最先进模型的基准智能体在达到高社会福利时存在可利用性问题。 |
| [^37] | [Exploration with Principles for Diverse AI Supervision.](http://arxiv.org/abs/2310.08899) | 本论文提出了一种被称为探索性AI的新范式，旨在自主生成高质量的训练数据。通过利用大型语言模型评估生成内容的新颖性，使AI不再过度依赖人类监督。 |
| [^38] | [PerturbScore: Connecting Discrete and Continuous Perturbations in NLP.](http://arxiv.org/abs/2310.08889) | 本文提出了PerturbScore，通过连接离散和连续扰动来帮助理解自然语言处理中的离散扰动，通过实验结果表明PerturbScore超过了以前的离散扰动测量方法，并在不同的数据集上得到有效推广。 |
| [^39] | [InstructTODS: Large Language Models for End-to-End Task-Oriented Dialogue Systems.](http://arxiv.org/abs/2310.08885) | InstructTODS是一个端到端任务导向对话系统的大规模语言模型，通过利用LLMs生成代理信念状态，并在零-shot的情况下适应多个领域。在实验中展示了与完全微调的TODS相媲美的性能，并且经过严格的人工评估，InstructTODS所生成的对话回应在帮助性、信息量和人性化方面明显优于黄金回应和最先进的TODS。 |
| [^40] | [Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue System.](http://arxiv.org/abs/2310.08877) | 本文提出了一种用于任务导向对话系统的检索-生成对齐方法，通过利用最大边际似然来训练有感知力的检索器，并结合各种元知识来指导生成器，从而提高了知识的利用效率和生成回应的质量。 |
| [^41] | [Guiding AMR Parsing with Reverse Graph Linearization.](http://arxiv.org/abs/2310.08860) | 本论文引入了逆向图线性化（RGL）的方法来解决序列到序列的AMR解析中结构丢失积累的问题。通过定义默认和逆向线性化顺序，并通过自我蒸馏机制引导模型生成默认线性化，提高了解析性能。 |
| [^42] | [Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogue.](http://arxiv.org/abs/2310.08840) | 本研究提出了SAFARI框架，利用大型语言模型作为个性化知识驱动对话系统的源计划器，使得多个知识源的依赖关系能够被整合，并能生成一致的回应。 |
| [^43] | [A Comparative Analysis of Task-Agnostic Distillation Methods for Compressing Transformer Language Models.](http://arxiv.org/abs/2310.08797) | 本研究比较了用于压缩Transformer语言模型的几种任务不可知蒸馏方法，并发现基于MiniLMv2的MHA转移是最佳选择。 |
| [^44] | [End-to-end Story Plot Generator.](http://arxiv.org/abs/2310.08796) | 这款全流程故事情节生成器通过优化模型和设计提示信息，以替代昂贵的API调用，实现了低成本生成高质量训练数据集。 |
| [^45] | [Mitigating Bias for Question Answering Models by Tracking Bias Influence.](http://arxiv.org/abs/2310.08795) | 本论文提出了一种名为BMBI的方法来减轻多选问题回答模型的偏见。通过观察一个查询实例对另一个实例的影响，测量查询实例的偏见程度，并将其作为优化目标，形成一个多任务学习设置。同时引入新的偏见评估指标以量化偏见。 |
| [^46] | ["Im not Racist but...": Discovering Bias in the Internal Knowledge of Large Language Models.](http://arxiv.org/abs/2310.08780) | 本文介绍了一种纯提示式的方法，用于揭示大型语言模型中隐藏的刻板印象，通过动态生成内部刻板印象的知识表示，我们能够识别这些模型中存在的偏见。这项工作在推进透明度和促进自然语言处理系统的公平性方面做出了贡献。 |
| [^47] | [Calibrating Likelihoods towards Consistency in Summarization Models.](http://arxiv.org/abs/2310.08764) | 通过校准模型生成的序列的似然性，使其更好地与通过自然语言推理（NLI）模型测量的一致性度量相一致，从而提高摘要模型的一致性和质量。 |
| [^48] | [CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models.](http://arxiv.org/abs/2310.08753) | CompA提出了由两个专家注释的音频-语言模型组合推理基准数据集，用于评估ALMs在理解音频中声音事件的顺序和属性绑定方面的表现。 |
| [^49] | [Circuit Component Reuse Across Tasks in Transformer Language Models.](http://arxiv.org/abs/2310.08744) | 这项工作证明了在Transformer语言模型中，电路组件可以在不同任务之间复用并产生相似的功能，为更高级的模型理解做出贡献。 |
| [^50] | [A Zero-Shot Language Agent for Computer Control with Structured Reflection.](http://arxiv.org/abs/2310.08740) | 这种论文提出了一种零样本语言代理机制，它不需要专家示踪，并且通过自我反思和结构化思考管理来学习和改善计算机上的控制，表现出高效的推理能力。 |
| [^51] | [Toward Joint Language Modeling for Speech Units and Text.](http://arxiv.org/abs/2310.08715) | 本文研究了联合语言建模中语音单元和文本的混合方法，并通过口语理解任务评估了模型的性能，结果表明混合语言模型优于仅使用语音的基线模型。 |
| [^52] | [Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4 on mock CFA Exams.](http://arxiv.org/abs/2310.08678) | 本研究评估了ChatGPT和GPT-4在金融分析上的能力，发现它们在模拟CFA考试中具有一定的表现，为将来进一步提升大型语言模型在金融推理方面的能力提供了启示。 |
| [^53] | [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models.](http://arxiv.org/abs/2310.08659) | 本论文提出了LoftQ：一种针对大型语言模型的LoRA精调感知量化框架。该框架同时对LLM进行量化，并为LoRA精调找到适当的低秩初始化，以缓解量化模型和全精度模型之间的差异，并显著提高了下游任务的泛化能力。 |
| [^54] | [Can We Edit Multimodal Large Language Models?.](http://arxiv.org/abs/2310.08475) | 本文提出了编辑多模式大型语言模型（MLLMs）的挑战，并构建了一个新的基准用于评估和比较不同编辑方法的效果。实验结果表明，编辑多模式LLMs仍然存在困难，但这项工作为NLP社区提供了宝贵的见解。 |
| [^55] | [Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Mo.](http://arxiv.org/abs/2310.08072) | 本文提出了一种在非英语语言中训练问答系统的简单且经济的方法，使用调整指导模型生成合成数据，避免了人力成本，并展示了合成数据训练的模型与手动整理的数据集训练的模型在性能上的可比较性。 |
| [^56] | [Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations.](http://arxiv.org/abs/2310.07849) | 本研究旨在探讨使用大型语言模型生成合成数据在文本分类模型训练中的潜力和限制。研究结果发现，主观性会负面影响模型在合成数据上的性能。这对于理解和利用合成数据的有效性具有重要的启示作用。 |
| [^57] | [Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation.](http://arxiv.org/abs/2310.07397) | 这项工作提出了一个自动数据集筛选框架，并构建了一个大规模的个性化面向目标对话数据集，名为TopDial。该数据集质量高，有助于探索个性化目标导向的对话。 |
| [^58] | [On the Impact of Cross-Domain Data on German Language Models.](http://arxiv.org/abs/2310.07321) | 本研究通过对德语语言模型进行实验，发现将数据多样性置于数据质量之上的交叉领域数据集训练方法，可以显著提高模型的性能，并超过了之前的最先进模型。 |
| [^59] | [Sparse Finetuning for Inference Acceleration of Large Language Models.](http://arxiv.org/abs/2310.06927) | 本论文研究了大型语言模型的准确稀疏微调问题，提出了基于L2范数的蒸馏方法SquareHead，可以在高稀疏性下实现准确的恢复；同时展示了稀疏语言模型的实际效率，可在CPU和GPU运行时实现加速，并且观察到在受内存限制的模型中，稀疏性也可用于减少内存带宽。 |
| [^60] | [Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance.](http://arxiv.org/abs/2310.05597) | 本文研究了语言模型是否能够学习类比推理的任务，并测试了几种学习方法。实验结果表明，模型能够通过少量数据学习类比推理，并在与人类基准进行比较后接近人类的表现水平。 |
| [^61] | [Universal Multi-modal Entity Alignment via Iteratively Fusing Modality Similarity Paths.](http://arxiv.org/abs/2310.05364) | 本研究提出了一种通过迭代融合模态相似路径实现通用的多模态实体对齐方法。通过统一建模和有效信息融合，解决了现有方法中模态建模不一致和低效以及模态融合效果不佳的问题。 |
| [^62] | [Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems.](http://arxiv.org/abs/2310.05280) | 这项研究评估了对话系统中的人格偏见对社交偏见的影响，并建立了一个综合评估框架来衡量不同人格采用下的偏见程度。 |
| [^63] | [Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational Knowledge Graph Construction.](http://arxiv.org/abs/2310.05185) | Text2NKG是一种用于构建N元关系知识图的细粒度N元关系抽取框架，支持多种NKG模式，具有高灵活性和实用性。 |
| [^64] | [Label-free Node Classification on Graphs with Large Language Models (LLMS).](http://arxiv.org/abs/2310.04668) | 本文介绍了一种使用大型语言模型（LLMs）对图中节点进行无标签分类的方法，即LLM-GNN。它利用LLMs对一小部分节点进行注释，然后通过对LLMs的注释进行训练，使得GNN能够对其余大部分节点进行预测。这种方法充分发挥了GNNs和LLMs的优势，同时解决了它们在处理结构化数据方面的限制。 |
| [^65] | [Human Mobility Question Answering (Vision Paper).](http://arxiv.org/abs/2310.04443) | 本文提出了一项新的任务，即人类移动问题回答（MobQA），旨在让智能系统从移动数据中学习并回答相关问题，填补了关于利用人类移动数据进行问题回答系统的研究空白，并为移动推荐系统的研究带来了新的范式变革。 |
| [^66] | [Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise.](http://arxiv.org/abs/2310.03328) | 本文介绍了一个简单而有效的GPT-4领域适应框架，通过将生成过程重新表述为一个“适应-检索-修订”的过程，解决了大型语言模型在特定领域生成内容错误的问题。 |
| [^67] | [LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving.](http://arxiv.org/abs/2310.03026) | 本文研究将大型语言模型（LLMs）作为复杂自动驾驶场景的决策组件，通过认知路径和算法来实现全面推理和可执行驾驶指令的转化。实验证明，LLMs能够在单车任务和复杂驾驶行为中表现出优越性能，这是因为其具有常识推理能力。 |
| [^68] | [A UMLS-Augmented Framework for Improving Factuality in Large Language Models within Healthcare.](http://arxiv.org/abs/2310.02778) | 该论文提出了一个基于UMLS的增强型大型语言模型框架，旨在改善医疗保健领域中模型生成内容的事实性。通过自动评估和医生评估，研究人员验证了该框架的有效性。 |
| [^69] | [Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond.](http://arxiv.org/abs/2310.02071) | 本研究通过探索多模态大型语言模型在代理的具身决策中的应用潜力，提出了一个新的评估基准PCA-EVAL，并引入了一个多代理协作框架HOLMES，以提高决策能力。研究发现GPT4-Vision模型在端到端的具身决策中表现最佳。 |
| [^70] | [Improving Emotional Expression and Cohesion in Image-Based Playlist Description and Music Topics: A Continuous Parameterization Approach.](http://arxiv.org/abs/2310.01248) | 本文介绍了一种用于基于图像的播放列表描述和音乐主题中的文本生成的改进方法，该方法通过连续参数化实现对文本风格和情感表达的精确控制，并在生成文本相关性和连贯性方面取得了显著改进。 |
| [^71] | [EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context Learning.](http://arxiv.org/abs/2309.10687) | EchoPrompt是一种简单而有效的方法，通过促使模型重新表述查询来提供改进的上下文学习效果。实验证明，EchoPrompt在多个任务中都取得了显著的性能提升。 |
| [^72] | [MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response.](http://arxiv.org/abs/2309.08730) | MusiLingo是一个利用预训练的语言模型将音乐和文本相结合的系统，可以生成音乐字幕和回答音乐相关的查询。通过使用投影层对齐音乐表示，该系统成功地将音乐音频和文本环境联系起来，同时使用了一个新的数据集来推动领域的进展。 |
| [^73] | [DictaBERT: A State-of-the-Art BERT Suite for Modern Hebrew.](http://arxiv.org/abs/2308.16687) | DictaBERT是一种最先进的预训练BERT模型，针对现代希伯来语，在大多数基准测试中表现优于其他模型。它还提供了两个经过微调的模型版本，可用于希伯来语文本分析中的前缀分割和形态标注任务。这些模型的发布旨在促进希伯来语自然语言处理的研究和发展。 |
| [^74] | [Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities.](http://arxiv.org/abs/2308.12966) | Qwen-VL是一种具有多功能能力的前沿大规模视觉-语言模型，它在图像字幕生成、问题回答、视觉定位和灵活交互等任务中表现出卓越性能，优于现有的大规模视觉-语言模型。它在推动多模态人工智能方面做出了重要贡献。 |
| [^75] | [Answering Unseen Questions With Smaller Language\\Models Using Rationale Generation and Dense Retrieval.](http://arxiv.org/abs/2308.04711) | 本论文提出了两种方法来改进在具有充分解释性背景下，使用较小语言模型回答训练中未见的挑战性短问题回答任务。第一种方法是使用理据生成和密集检索结合的方式，并通过理据排名模型进行评分和组合。第二种方法是使用增强检索训练数据集训练较小的推理模型，以利用长文本序列中的相关信息。 |
| [^76] | [MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities.](http://arxiv.org/abs/2308.02490) | MM-Vet是一个评估标准，用于评估大型多模态模型在复杂任务上的综合能力。该标准解决了如何结构化和评估复杂多模态任务、设计适用于不同问题和回答类型的评估指标以及如何提供模型洞察的问题。通过整合不同的核心视觉-语言能力，MM-Vet展示了有趣的能力和解决复杂任务的方法。 |
| [^77] | [Large Language Model Displays Emergent Ability to Interpret Novel Literary Metaphors.](http://arxiv.org/abs/2308.01497) | 最近的研究评估了GPT-4，一种大型语言模型，对来自塞尔维亚诗歌的新颖文学隐喻的解释能力。 |
| [^78] | [Evade ChatGPT Detectors via A Single Space.](http://arxiv.org/abs/2307.02599) | 本研究发现，当前的ChatGPT检测器不能有效区分人类生成和AI生成内容之间的差异，而一个额外的空格成为了规避检测的关键因素。 |
| [^79] | [Provable Robust Watermarking for AI-Generated Text.](http://arxiv.org/abs/2306.17439) | GPTWatermark是一种针对性模型水印技术，通过固定分组设计和强大的可证明保证，提供了对AI生成文本的鲁棒性检测和安全性防御。实验证明了其在检测准确性和生成质量方面的优越性，推动了LLMs负责任使用的进步。 |
| [^80] | [Being Right for Whose Right Reasons?.](http://arxiv.org/abs/2306.00639) | 本论文针对解释性方法的应用进行研究，发现模型预测与人类推理的一致性与人口统计信息有关，模型更倾向于与年长和/或白人注释者提供的原因最为一致。 |
| [^81] | [Generating Images with Multimodal Language Models.](http://arxiv.org/abs/2305.17216) | 该论文提出了一种方法，将大型语言模型与预训练的图像编码器和解码器模型进行融合，能生成具有连贯性的图像输出，同时也能进行图像检索和多模态对话。 |
| [^82] | [A Mechanism for Solving Relational Tasks in Transformer Language Models.](http://arxiv.org/abs/2305.16130) | 这篇论文研究了在Transformer语言模型中解决关系任务的机制，并发现这些模型利用简单的线性更新来处理关系任务，并以内容无关的方式促进关系的输出。 |
| [^83] | [The student becomes the master: Matching GPT3 on Scientific Factual Error Correction.](http://arxiv.org/abs/2305.14707) | 本文提出了一种不需要验证者且不做领域假设的主张校正系统，能够显著提高科学事实错误校正任务的性能，并通过使用LLM的提示方法和主张感知的解码过程来提高校正质量。 |
| [^84] | [Learning Semantic Role Labeling from Compatible Label Sequences.](http://arxiv.org/abs/2305.14600) | 该论文探讨了如何从不相交的兼容标签序列中高效地学习，将此运用于语义角色标注任务，提出了联合处理VerbNet和PropBank标签的方法，并验证了其有效性。 |
| [^85] | [Biomedical Named Entity Recognition via Dictionary-based Synonym Generalization.](http://arxiv.org/abs/2305.13066) | 本研究提出了一种基于词典的方法来解决生物医学命名实体识别中的同义词泛化问题，通过引入同义词距离和噪声扰动正则化项，实现了对输入文本中的生物医学概念的识别。 |
| [^86] | [DUMB: A Benchmark for Smart Evaluation of Dutch Models.](http://arxiv.org/abs/2305.13026) | 我们引入了DUMB基准测试，用于智能评估荷兰语模型。通过比较不同大小和类型的预训练语言模型，我们发现目前的荷兰单语模型表现不佳，建议使用其他架构和预训练目标来训练更大的荷兰模型。在该基准测试中，DeBERTaV3 (large)、XLM-R (large)和mDeBERTaV3 (base)取得了最高性能。 |
| [^87] | [Affective Reasoning at Utterance Level in Conversations: A Causal Discovery Approach.](http://arxiv.org/abs/2305.02615) | 本文提出了一种新的会话情感因果发现方法（CACD），并通过设计公共骨架和生成替代隐含原因解决了因果模型的不确定性和隐含原因的不可观察性的问题。这种方法可以在变长会话中发现因果关系。 |
| [^88] | [Can ChatGPT Assess Human Personalities? A General Evaluation Framework.](http://arxiv.org/abs/2303.01248) | 本文提出了一个通用的评估框架，用于通过LLMs基于MBTI测试评估人类个性。该框架通过设计无偏倚的提示、灵活查询和正确性评估的方式，使LLMs能够灵活评估不同群体的个性特点。 |
| [^89] | [UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers.](http://arxiv.org/abs/2303.00807) | 该论文提出了一种无监督领域自适应方法，利用大型语言模型(LLMs)生成大量合成查询和reranker模型，蒸馏为高效的检索器，适用于长尾领域。 |
| [^90] | [Orca: A Few-shot Benchmark for Chinese Conversational Machine Reading Comprehension.](http://arxiv.org/abs/2302.13619) | Orca是中文对话机器阅读理解的第一个基准，提供了零样本/少样本设置来评估模型对多样领域的泛化能力，并通过提供与回答相关的段落来更合理地评估模型的理解能力。 |
| [^91] | [Knowledge is a Region in Weight Space for Fine-tuned Language Models.](http://arxiv.org/abs/2302.04863) | 研究探讨了不同模型在权重空间中的位置与性能的关联，发现微调语言模型在权重空间中有明确定义的区域，且这些区域中的模型表现出高性能。此外，通过绕过这些区域，可以得到性能相当甚至更好的新模型。 |
| [^92] | [Dataless Knowledge Fusion by Merging Weights of Language Models.](http://arxiv.org/abs/2212.09849) | 本文提出了一种无数据知识融合方法，可以合并在不同训练数据集上建立的单个模型，以得到一个在所有数据集领域上表现良好且可以推广到域外数据的单一模型。 |
| [^93] | [NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual Question Answering.](http://arxiv.org/abs/2211.03462) | 该论文提出了一种非自回归程序生成框架，用于解决混合表格-文本问答中的数值推理和跨度提取问题。实验结果表明，该方法可以显著改善程序生成的准确性并提高速度。 |
| [^94] | [Efficiently Enhancing Zero-Shot Performance of Instruction Following Model via Retrieval of Soft Prompt.](http://arxiv.org/abs/2210.03029) | 通过检索软提示有效辅助硬提示，在增加少量参数的情况下提高了指令跟随模型在零样本任务上的表现效率。 |
| [^95] | [What should I Ask: A Knowledge-driven Approach for Follow-up Questions Generation in Conversational Surveys.](http://arxiv.org/abs/2205.10977) | 本文提出了一种基于知识驱动的方法，用于在对话调查中实时生成追加问题。该方法通过使用知识来引导生成过程，生成更具信息量、连贯性和清晰度的追加问题。 |
| [^96] | [Stylized innovation: generating timelines by interrogating incrementally available randomised dictionaries.](http://arxiv.org/abs/1806.07722) | 该论文通过设计和生成合成创新网络词典，研究了新符号发现在增加词汇的过程中的表现，并探讨了创新过程的整体统计和行为。 |

# 详细

[^1]: 大型语言模型上的用户推理攻击

    User Inference Attacks on Large Language Models. (arXiv:2310.09266v1 [cs.CR])

    [http://arxiv.org/abs/2310.09266](http://arxiv.org/abs/2310.09266)

    本论文研究了在大型语言模型上的用户推理攻击，发现LLMs对于各种微调数据集都很容易受到攻击，尤其是对于离群用户和贡献大量数据的用户。这对保护用户隐私具有重要意义。

    

    微调是将大型语言模型（LLMs）定制为专业任务和应用的常见有效方法。本文研究了在用户数据上微调LLMs的隐私问题。为此，我们定义了一个称为用户推理的现实威胁模型，其中攻击者推断出用户的数据是否被用于微调。我们实现了这种威胁模型的攻击，只需要从用户那里获取一小组样本（可能与用于训练的样本不同）和对微调LLM的黑盒访问权限。我们发现，LLMs在各种微调数据集上易受用户推理攻击的影响，有时攻击成功率接近完美。此外，我们调查了哪些特性使用户容易受到用户推理的攻击，发现离群用户（即数据分布与其他用户明显不同）和贡献大量数据的用户更容易受到攻击。最后，我们探索了解决这种攻击的方案。

    Fine-tuning is a common and effective method for tailoring large language models (LLMs) to specialized tasks and applications. In this paper, we study the privacy implications of fine-tuning LLMs on user data. To this end, we define a realistic threat model, called user inference, wherein an attacker infers whether or not a user's data was used for fine-tuning. We implement attacks for this threat model that require only a small set of samples from a user (possibly different from the samples used for training) and black-box access to the fine-tuned LLM. We find that LLMs are susceptible to user inference attacks across a variety of fine-tuning datasets, at times with near perfect attack success rates. Further, we investigate which properties make users vulnerable to user inference, finding that outlier users (i.e. those with data distributions sufficiently different from other users) and users who contribute large quantities of data are most susceptible to attack. Finally, we explore s
    
[^2]: PromptRE: 基于提示的数据编程的弱监督文本级关系抽取

    PromptRE: Weakly-Supervised Document-Level Relation Extraction via Prompting-Based Data Programming. (arXiv:2310.09265v1 [cs.CL])

    [http://arxiv.org/abs/2310.09265](http://arxiv.org/abs/2310.09265)

    PromptRE是一种弱监督文档级别关系抽取方法，通过结合基于提示的技术，解决了“没有关系”的实例数量不平衡和直接使用预训练模型进行文档关系抽取的问题。

    

    关系抽取旨在将两个实体之间的关系分类到预定义的类别中。尽管先前的研究主要集中在句级别的关系抽取上，但最近的研究将范围扩大到文档级别的关系抽取。传统的关系抽取方法严重依赖于人工标注的训练数据，这是一项耗时且劳动密集的任务。为了减少对手动标注的需求，最近已经开发了一些弱监督方法用于句级别的关系抽取，但在文档级别的关系抽取方面仍然有限的工作。弱监督的文档级别关系抽取面临着一些挑战，比如“没有关系”的实例数量不平衡，以及直接使用预训练的大语言模型进行文档关系抽取的失败。为了应对这些挑战，我们提出了PromptRE，一种新颖的基于提示的弱监督文档级别关系抽取方法，结合了基于提示的技术。

    Relation extraction aims to classify the relationships between two entities into pre-defined categories. While previous research has mainly focused on sentence-level relation extraction, recent studies have expanded the scope to document-level relation extraction. Traditional relation extraction methods heavily rely on human-annotated training data, which is time-consuming and labor-intensive. To mitigate the need for manual annotation, recent weakly-supervised approaches have been developed for sentence-level relation extraction while limited work has been done on document-level relation extraction. Weakly-supervised document-level relation extraction faces significant challenges due to an imbalanced number "no relation" instances and the failure of directly probing pretrained large language models for document relation extraction. To address these challenges, we propose PromptRE, a novel weakly-supervised document-level relation extraction method that combines prompting-based techniq
    
[^3]: Table-GPT: 针对多样表格任务的表格调优GPT

    Table-GPT: Table-tuned GPT for Diverse Table Tasks. (arXiv:2310.09263v1 [cs.CL])

    [http://arxiv.org/abs/2310.09263](http://arxiv.org/abs/2310.09263)

    本文提出了一种新的"表格调优"范式，通过使用从真实表格中合成的多样化表格任务作为训练数据，对语言模型进行训练/微调，以提高其理解表格和执行表格任务的能力。

    

    语言模型，如GPT-3.5和ChatGPT，展现出了遵循多种人类指令和执行各种任务的非凡能力。然而，当使用一系列基本的表格理解任务来探究语言模型时，我们观察到现今的语言模型在许多涉及表格的任务上仍然不够优秀，可能是因为它们主要在\emph{一维}自然语言文本上进行预训练，而关系表是\emph{二维}对象。在本文中，我们提出了一种新的“\emph{表格调优}”范式，即通过使用从真实表格中合成的多样化表格任务作为训练数据，继续对GPT-3.5和ChatGPT这样的语言模型进行训练/微调，从而提高语言模型理解表格和执行表格任务的能力。我们展示了我们的Table-GPT模型表现出更好的\emph{表格理解}能力，在广泛的表格任务上始终优于普通的GPT-3.5和ChatGPT。

    Language models, such as GPT-3.5 and ChatGPT, demonstrate remarkable abilities to follow diverse human instructions and perform a wide range of tasks. However, when probing language models using a range of basic table-understanding tasks, we observe that today's language models are still sub-optimal in many table-related tasks, likely because they are pre-trained predominantly on \emph{one-dimensional} natural-language texts, whereas relational tables are \emph{two-dimensional} objects.  In this work, we propose a new "\emph{table-tuning}" paradigm, where we continue to train/fine-tune language models like GPT-3.5 and ChatGPT, using diverse table-tasks synthesized from real tables as training data, with the goal of enhancing language models' ability to understand tables and perform table tasks. We show that our resulting Table-GPT models demonstrate (1) better \emph{table-understanding} capabilities, by consistently outperforming the vanilla GPT-3.5 and ChatGPT, on a wide-range of tabl
    
[^4]: 多语种环境下的政治主张识别和分类：首次实验

    Political claim identification and categorization in a multilingual setting: First experiments. (arXiv:2310.09256v1 [cs.CL])

    [http://arxiv.org/abs/2310.09256](http://arxiv.org/abs/2310.09256)

    本文在多语种环境下进行了首次实验，探讨了政治主张的识别和分类。实验使用了德语数据集DebateNet2.0，涵盖了2015年难民危机引发的政策辩论。通过机器翻译和多语种嵌入两种方法进行了评估。

    

    政治主张的识别和分类是分析政治报告的重要步骤，然而用于此任务的资源非常有限。本文探讨了不同的策略，用于跨语言投射政治主张分析。我们在德国的数据集DebateNet2.0上进行了实验，该数据集涵盖了2015年难民危机引发的政策辩论。我们的评估涉及两个任务（主张识别和分类）、三种语言（德语、英语和法语）和两种方法（机器翻译 - 在我们的实验中表现最好的方法 - 和多语种嵌入）。

    The identification and classification of political claims is an important step in the analysis of political newspaper reports; however, resources for this task are few and far between. This paper explores different strategies for the cross-lingual projection of political claims analysis. We conduct experiments on a German dataset, DebateNet2.0, covering the policy debate sparked by the 2015 refugee crisis. Our evaluation involves two tasks (claim identification and categorization), three languages (German, English, and French) and two methods (machine translation -- the best method in our experiments -- and multilingual embeddings).
    
[^5]: 通过WordNet层次结构对文本到图像模型进行上义词理解评估

    Hypernymy Understanding Evaluation of Text-to-Image Models via WordNet Hierarchy. (arXiv:2310.09247v1 [cs.CV])

    [http://arxiv.org/abs/2310.09247](http://arxiv.org/abs/2310.09247)

    通过基于WordNet层次结构的方法，我们评估了流行的文本到图像模型对于上义词关系的理解能力。我们提出了两个自动度量标准，能够定量比较不同模型的语言能力，并发现了一些困难的词汇。我们全面评估了一些流行的文本到图像模型，包括GLIDE、Latent Diffusion和Stable Diffusion。

    

    最近，由于质量不断提高和众多实际应用，文本到图像合成引起了广泛关注。然而，对于文本到图像模型的语言理解能力仍然知之甚少，这使得难以推理出给定模型能够理解的提示表达。在这项工作中，我们衡量了流行的文本到图像模型对于上义词（或“是一个”关系）的理解能力。我们设计了两个基于WordNet语义层次和在ImageNet上预训练的现有图像分类器的自动度量标准。这两个度量标准均能够对文本到图像模型的语言能力进行广泛的定量比较，并提供了一种找到细粒度定性差异的方法，例如对于模型来说未知的单词，因此很难绘制。我们全面评估了流行的文本到图像模型，包括GLIDE、Latent Diffusion和Stable Diffusion，展示了它们的能力和局限。

    Text-to-image synthesis has recently attracted widespread attention due to rapidly improving quality and numerous practical applications. However, the language understanding capabilities of text-to-image models are still poorly understood, which makes it difficult to reason about prompt formulations that a given model would understand well. In this work, we measure the capability of popular text-to-image models to understand $\textit{hypernymy}$, or the "is-a" relation between words. We design two automatic metrics based on the WordNet semantic hierarchy and existing image classifiers pretrained on ImageNet. These metrics both enable broad quantitative comparison of linguistic capabilities for text-to-image models and offer a way of finding fine-grained qualitative differences, such as words that are unknown to models and thus are difficult for them to draw. We comprehensively evaluate popular text-to-image models, including GLIDE, Latent Diffusion, and Stable Diffusion, showing how ou
    
[^6]: 使用LLM和领域模型协作的先例增强法律判决预测

    Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration. (arXiv:2310.09241v1 [cs.CL])

    [http://arxiv.org/abs/2310.09241](http://arxiv.org/abs/2310.09241)

    本文提出了先例增强的法律判决预测框架（PLJP），通过结合大型语言模型（LLM）和领域模型，利用先例相关信息进行预测。

    

    法律判决预测（LJP）已成为法律人工智能中一个越来越关键的任务，即根据案件事实描述预测案件的判决。先例是具有相似事实的先前法律案例，它们是国家法律体系中后续案件判决的依据。因此，值得探索在LJP中利用先例的方法。深度学习的最新进展使得可以使用各种技术来解决LJP任务。这些技术可以分为两类：大型语言模型（LLM）和领域特定模型。LLM能够解释和生成复杂的自然语言，而领域模型在学习任务特定信息方面效果显著。在本文中，我们提出了先例增强的LJP框架（PLJP），这是一个在先例背景下利用LLM和领域模型优势的系统。具体来说，领域模型的设计目标是提供候选标签并找到适当的预测标签。

    Legal Judgment Prediction (LJP) has become an increasingly crucial task in Legal AI, i.e., predicting the judgment of the case in terms of case fact description. Precedents are the previous legal cases with similar facts, which are the basis for the judgment of the subsequent case in national legal systems. Thus, it is worthwhile to explore the utilization of precedents in the LJP. Recent advances in deep learning have enabled a variety of techniques to be used to solve the LJP task. These can be broken down into two categories: large language models (LLMs) and domain-specific models. LLMs are capable of interpreting and generating complex natural language, while domain models are efficient in learning task-specific information. In this paper, we propose the precedent-enhanced LJP framework (PLJP), a system that leverages the strength of both LLM and domain models in the context of precedents. Specifically, the domain models are designed to provide candidate labels and find the proper 
    
[^7]: BanglaNLP在BLP-2023任务2中的表现: 对Bangla社交媒体帖子的情感分析的Transformer模型的基准测试

    BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models for Sentiment Analysis of Bangla Social Media Posts. (arXiv:2310.09238v1 [cs.CL])

    [http://arxiv.org/abs/2310.09238](http://arxiv.org/abs/2310.09238)

    本论文提出了针对Bangla社交媒体帖子进行情感分析的任务，通过实验发现在这种低资源语言场景下，使用Transformer模型进行迁移学习可以提高模型的学习效果，并且在对已经在Twitter数据上进行了情感分析任务的模型进一步微调的情况下，模型的性能最好。

    

    Bangla是全球第七大使用最广泛的语言，拥有来自印度和孟加拉国的2.34亿母语使用者。这种形态丰富的语言拥有丰富的文学传统，包括不同的方言和语言特定的挑战。尽管其语言丰富性和历史，但在自然语言处理（NLP）和语音社区中，Bangla仍被归类为资源匮乏的语言。本论文展示了我们在BLP研讨会的任务2（Bangla社交媒体帖子情感分析）中的提交。我们尝试了不同的基于Transformer的架构来解决这个任务。我们的定量结果显示，在这种低资源语言场景中，迁移学习确实有助于模型更好地学习。当我们进一步对已经在Twitter数据上进行了情感分析任务的模型进行微调时，这一点变得明显，而这个经过微调的模型在所有其他模型中表现最佳。我们还进行了详细的实验。

    Bangla is the 7th most widely spoken language globally, with a staggering 234 million native speakers primarily hailing from India and Bangladesh. This morphologically rich language boasts a rich literary tradition, encompassing diverse dialects and language-specific challenges. Despite its linguistic richness and history, Bangla remains categorized as a low-resource language within the natural language processing (NLP) and speech community. This paper presents our submission to Task 2 (Sentiment Analysis of Bangla Social Media Posts) of the BLP Workshop. We experiment with various Transformer-based architectures to solve this task. Our quantitative results show that transfer learning really helps in better learning of the models in this low-resource language scenario. This becomes evident when we further finetune a model which has already been finetuned on twitter data for sentiment analysis task and that finetuned model performs the best among all other models. We also perform a deta
    
[^8]: AgentCF: 基于自主语言代理的协作学习在推荐系统中的应用

    AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems. (arXiv:2310.09233v1 [cs.IR])

    [http://arxiv.org/abs/2310.09233](http://arxiv.org/abs/2310.09233)

    AgentCF 是一种基于自主语言代理的协作学习方法，在推荐系统中模拟用户和物品的交互，并优化这两类代理。

    

    近年来，利用具有出色决策能力的LLM（语言混合模型）代理作为可信的人类代理出现了。然而，现有研究主要集中在模拟人类对话上。人类的非语言行为，如推荐系统中的物品点击，虽然隐含着用户的偏好并能提升用户建模，但尚未得到深入探索。主要原因在于语言建模与行为建模之间的差距，以及LLMs对用户-物品关系的不理解。为了解决这个问题，我们提出了AgentCF，通过基于代理的协同过滤来模拟推荐系统中的用户-物品交互。我们创造性地考虑用户和物品作为代理，并开发了一种协作学习方法来优化这两种代理。具体而言，每个时间步，我们首先促使用户代理和物品代理自主地进行交互。然后，基于差异性对这两类代理进行优化。

    Recently, there has been an emergence of employing LLM-powered agents as believable human proxies, based on their remarkable decision-making capability. However, existing studies mainly focus on simulating human dialogue. Human non-verbal behaviors, such as item clicking in recommender systems, although implicitly exhibiting user preferences and could enhance the modeling of users, have not been deeply explored. The main reasons lie in the gap between language modeling and behavior modeling, as well as the incomprehension of LLMs about user-item relations.  To address this issue, we propose AgentCF for simulating user-item interactions in recommender systems through agent-based collaborative filtering. We creatively consider not only users but also items as agents, and develop a collaborative learning approach that optimizes both kinds of agents together. Specifically, at each time step, we first prompt the user and item agents to interact autonomously. Then, based on the disparities b
    
[^9]: 基于大型语言模型的自动索引匹配：增强事实核查员在对抗虚假信息的战斗中的能力

    Automated Claim Matching with Large Language Models: Empowering Fact-Checkers in the Fight Against Misinformation. (arXiv:2310.09223v1 [cs.CL])

    [http://arxiv.org/abs/2310.09223](http://arxiv.org/abs/2310.09223)

    FACT-GPT是一个基于大型语言模型的框架，旨在自动化事实核查的声称匹配阶段。它通过使用GPT-4生成标记数据集，并使用微调的LLM，在识别与事实核查员先前证实为虚假的声称相匹配的新的社交媒体内容。实验结果表明，我们的方法与更大的预训练LLMs的性能相媲美。

    

    在当今数字化时代，虚假信息的迅速传播对公众福祉和社会信任构成威胁。随着在线虚假信息的蔓延，事实核查员的手动验证变得越来越具有挑战性。我们介绍了FACT-GPT（基于声称匹配任务的事实核查增强与大型语言模型），这是一个旨在使用大型语言模型自动化事实核查的框架。该框架识别先前被事实核查员证实为虚假的声称的新的社交媒体内容，其要么支持要么相矛盾。我们的方法使用GPT-4生成一个模拟社交媒体帖子的标记数据集，该数据集用于微调更专门的LLM。我们在与公共卫生相关的大量社交媒体内容的数据集上评估了FACT-GPT。结果表明，我们的微调LLM在声称匹配任务中与更大的预训练LLM的性能相媲美，达到了接近的效果。

    In today's digital era, the rapid spread of misinformation poses threats to public well-being and societal trust. As online misinformation proliferates, manual verification by fact checkers becomes increasingly challenging. We introduce FACT-GPT (Fact-checking Augmentation with Claim matching Task-oriented Generative Pre-trained Transformer), a framework designed to automate the claim matching phase of fact-checking using Large Language Models (LLMs). This framework identifies new social media content that either supports or contradicts claims previously debunked by fact-checkers. Our approach employs GPT-4 to generate a labeled dataset consisting of simulated social media posts. This data set serves as a training ground for fine-tuning more specialized LLMs. We evaluated FACT-GPT on an extensive dataset of social media content related to public health. The results indicate that our fine-tuned LLMs rival the performance of larger pre-trained LLMs in claim matching tasks, aligning close
    
[^10]: "凯利是一个温暖的人，约瑟夫是一个榜样": LLM生成的推荐信中的性别偏见

    "Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v1 [cs.CL])

    [http://arxiv.org/abs/2310.09219](http://arxiv.org/abs/2310.09219)

    本文对LLM生成的推荐信中的性别偏见进行了细致的研究，并设计了评估方法来展现通过语言风格和词汇内容来体现的性别偏见。

    

    随着生成语言模型的进步，用户已经开始使用大型语言模型（LLM）来协助撰写各种类型的内容，包括推荐信等职业文件。尽管它们的方便性，但这些应用引入了前所未有的公平问题。由于生成的推荐信可能被用户直接在职业或学术场景中使用，它们有可能造成直接的社会伤害，如降低女性申请者的成功率。因此，对于未来的缓解和监控，全面研究此类实际应用情况中的公平问题和相关伤害势在必行。在本文中，我们对LLM生成的推荐信中的性别偏见进行了批判性的研究。受社会科学研究结果的启发，我们设计了评估方法，通过两个维度来展现LLM生成的信件中的性别偏见：语言风格的偏见和词汇内容的偏见。此外，我们还研究了推荐信中性别偏见的程度。

    As generative language models advance, users have started to utilize Large Language Models (LLMs) to assist in writing various types of content, including professional documents such as recommendation letters. Despite their convenience, these applications introduce unprecedented fairness concerns. As generated reference letters might be directly utilized by users in professional or academic scenarios, they have the potential to cause direct social harms, such as lowering success rates for female applicants. Therefore, it is imminent and necessary to comprehensively study fairness issues and associated harms in such real-world use cases for future mitigation and monitoring. In this paper, we critically examine gender bias in LLM-generated reference letters. Inspired by findings in social science, we design evaluation methods to manifest gender biases in LLM-generated letters through 2 dimensions: biases in language style and biases in lexical content. Furthermore, we investigate the ext
    
[^11]: 探索指导：通过主动探索增强特定领域指导覆盖率

    Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration. (arXiv:2310.09168v1 [cs.CL])

    [http://arxiv.org/abs/2310.09168](http://arxiv.org/abs/2310.09168)

    通过采用探索指导的方法，使用大型语言模型 (LLMs) 进行主动探索，增强了领域特定指导调优的数据覆盖范围，并取得了显著的性能提升。

    

    通过增强多样性，可以大幅优化指导调优，从而使模型能够处理更广泛的任务。然而，用于此类调优的现有数据往往对个别领域的覆盖不足，限制了对这些领域内细致理解和交互的范围。为了解决这个问题，我们提出了一种新颖的方法，称为探索指导，通过大型语言模型 (LLMs) 的主动探索来增强用于特定领域指导调优的数据覆盖。探索指导基于典型的领域使用案例，通过实现搜索算法来获取多样化和面向领域的指导调优数据的多种变体或可能性。我们的数据中心分析验证了此方法在改进特定领域指导覆盖范围方面的有效性。此外，我们模型的性能显示出与多个基线模型相比的显著进展。

    Instruction-tuning can be substantially optimized through enhanced diversity, resulting in models capable of handling a broader spectrum of tasks. However, existing data employed for such tuning often exhibit an inadequate coverage of individual domains, limiting the scope for nuanced comprehension and interactions within these areas. To address this deficiency, we propose Explore-Instruct, a novel approach to enhance the data coverage to be used in domain-specific instruction-tuning through active exploration via Large Language Models (LLMs). Built upon representative domain use cases, Explore-Instruct explores a multitude of variations or possibilities by implementing a search algorithm to obtain diversified and domain-focused instruction-tuning data. Our data-centric analysis validates the effectiveness of this proposed approach in improving domain-specific instruction coverage. Moreover, our model's performance demonstrates considerable advancements over multiple baselines, includi
    
[^12]: 开发一种自然语言理解模型来表征有线电视新闻偏见

    Developing a Natural Language Understanding Model to Characterize Cable News Bias. (arXiv:2310.09166v1 [cs.CL])

    [http://arxiv.org/abs/2310.09166](http://arxiv.org/abs/2310.09166)

    本论文开发了一种无监督的机器学习方法，通过对有线电视节目提及的主题进行命名实体识别和立场分析的方式，来表征其偏见。在2020年的有线电视转录中应用该方法，发现节目聚类与节目所属的有线电视网络保持一致。揭示了客观评估媒体偏见和表征陌生媒体环境的潜力。

    

    媒体偏见在社会科学和计算机科学领域得到了广泛研究。然而，目前的研究仍然在很大程度上依靠人工输入和主观评估来标记偏见，尤其是有线电视研究更是如此。为了解决这些问题，我们开发了一种无监督的机器学习方法，以在没有任何人工输入的情况下表征有线电视节目的偏见。该方法依赖于通过命名实体识别分析提及的主题以及通过立场分析来讨论这些主题的方式，以便将具有类似偏见的节目进行聚类。将我们的方法应用于2020年的有线电视转录中，我们发现节目聚类随时间保持一致，并大致对应于节目所属的有线电视网络。该方法揭示了未来客观评估媒体偏见和表征陌生媒体环境的潜力。

    Media bias has been extensively studied by both social and computational sciences. However, current work still has a large reliance on human input and subjective assessment to label biases. This is especially true for cable news research. To address these issues, we develop an unsupervised machine learning method to characterize the bias of cable news programs without any human input. This method relies on the analysis of what topics are mentioned through Named Entity Recognition and how those topics are discussed through Stance Analysis in order to cluster programs with similar biases together. Applying our method to 2020 cable news transcripts, we find that program clusters are consistent over time and roughly correspond to the cable news network of the program. This method reveals the potential for future tools to objectively assess media bias and characterize unfamiliar media environments.
    
[^13]: BibRank：使用元数据的自动关键词提取平台

    BibRank: Automatic Keyphrase Extraction Platform Using~Metadata. (arXiv:2310.09151v1 [cs.CL])

    [http://arxiv.org/abs/2310.09151](http://arxiv.org/abs/2310.09151)

    本文介绍了BibRank平台，该平台集成了关键词数据集，并提供了关键词提取算法的评估。BibRank通过使用丰富的BibTeX数据集和创新的加权技术，结合位置、统计和词共现信息，实现了自动关键词提取。

    

    自动关键词提取是识别文档中关键短语的过程。这些关键词在文档分类、聚类、推荐、索引、搜索、摘要和文本简化等多个任务中起着重要作用。本文提出了一个平台，集成了关键词数据集，并方便了关键词提取算法的评估。该平台包括BibRank，一种自动关键词提取算法，该算法利用解析BibTeX格式的参考文献数据获得的丰富数据集。BibRank结合了创新的加权技术和位置、统计和词共现信息，从文档中提取关键词。该平台对于寻求改进关键词提取算法并推进自然语言处理领域的研究人员和开发者非常有价值。

    Automatic Keyphrase Extraction involves identifying essential phrases in a document. These keyphrases are crucial in various tasks such as document classification, clustering, recommendation, indexing, searching, summarization, and text simplification. This paper introduces a platform that integrates keyphrase datasets and facilitates the evaluation of keyphrase extraction algorithms. The platform includes BibRank, an automatic keyphrase extraction algorithm that leverages a rich dataset obtained by parsing bibliographic data in BibTeX format. BibRank combines innovative weighting techniques with positional, statistical, and word co-occurrence information to extract keyphrases from documents. The platform proves valuable for researchers and developers seeking to enhance their keyphrase extraction algorithms and advance the field of natural language processing.
    
[^14]: PuoBERTa:训练和评估一种为塞茨瓦纳语定制的语言模型

    PuoBERTa: Training and evaluation of a curated language model for Setswana. (arXiv:2310.09141v1 [cs.CL])

    [http://arxiv.org/abs/2310.09141](http://arxiv.org/abs/2310.09141)

    本文介绍了一种名为PuoBERTa的定制掩码语言模型，针对塞茨瓦纳语进行训练，并证明了其在促进塞茨瓦纳语等少研究语言的自然语言处理能力方面的有效性。

    

    自然语言处理在资源丰富的语言（如英语）方面取得了重大进展，但在资源匮乏的语言（如塞茨瓦纳语）方面却滞后。本文通过介绍PuoBERTa，一种专门为塞茨瓦纳语训练的定制掩码语言模型，弥补了这一差距。我们介绍如何收集、筛选和准备多样化的单语文本，为PuoBERTa的训练生成高质量的语料库。在之前为塞茨瓦纳语创建单语资源的基础上，我们评估了PuoBERTa在多个自然语言处理任务中的表现，包括词性标注、命名实体识别和新闻分类。此外，我们还引入了一个新的塞茨瓦纳语新闻分类数据集，并提供了使用PuoBERTa的初始基准。我们的工作展示了PuoBERTa在促进塞茨瓦纳语等少研究语言的自然语言处理能力方面的有效性，并为未来的研究方向铺平了道路。

    Natural language processing (NLP) has made significant progress for well-resourced languages such as English but lagged behind for low-resource languages like Setswana. This paper addresses this gap by presenting PuoBERTa, a customised masked language model trained specifically for Setswana. We cover how we collected, curated, and prepared diverse monolingual texts to generate a high-quality corpus for PuoBERTa's training. Building upon previous efforts in creating monolingual resources for Setswana, we evaluated PuoBERTa across several NLP tasks, including part-of-speech (POS) tagging, named entity recognition (NER), and news categorisation. Additionally, we introduced a new Setswana news categorisation dataset and provided the initial benchmarks using PuoBERTa. Our work demonstrates the efficacy of PuoBERTa in fostering NLP capabilities for understudied languages like Setswana and paves the way for future research directions.
    
[^15]: 共识游戏：通过均衡搜索生成语言模型

    The Consensus Game: Language Model Generation via Equilibrium Search. (arXiv:2310.09139v1 [cs.GT])

    [http://arxiv.org/abs/2310.09139](http://arxiv.org/abs/2310.09139)

    这篇论文介绍了一种新的语言模型解码方法，将其视为规范化的不完美信息序列信号博弈，并通过找到近似均衡点得到了一个解码算法。这种方法可以应用于问答和其他文本生成任务中。

    

    当应用于问答和其他文本生成任务时，语言模型（LMs）可以通过生成式查询（通过从其输出分布中抽样答案）或判别式查询（通过使用它们对一组候选输出进行评分或排序）进行查询。这些过程有时会产生非常不同的预测。我们如何调和互不相容的评分过程以获得连贯的LM预测呢？我们引入一种新的、无需训练的、博弈论过程用于语言模型解码。我们的方法将语言模型解码视为一种规范化的不完美信息序列信号博弈 - 称为共识游戏 - 在该博弈中，一个生成器试图用自然语言句子传达一个抽象的正确性参数给一个判别器。我们开发了计算程序来找到这个博弈的近似均衡点，从而得到了一个我们称之为EQUILIBRIUM-RANKING的解码算法。应用于大量任务（包括阅读理解，常识）

    When applied to question answering and other text generation tasks, language models (LMs) may be queried generatively (by sampling answers from their output distribution) or discriminatively (by using them to score or rank a set of candidate outputs). These procedures sometimes yield very different predictions. How do we reconcile mutually incompatible scoring procedures to obtain coherent LM predictions? We introduce a new, a training-free, game-theoretic procedure for language model decoding. Our approach casts language model decoding as a regularized imperfect-information sequential signaling game - which we term the CONSENSUS GAME - in which a GENERATOR seeks to communicate an abstract correctness parameter using natural language sentences to a DISCRIMINATOR. We develop computational procedures for finding approximate equilibria of this game, resulting in a decoding algorithm we call EQUILIBRIUM-RANKING. Applied to a large number of tasks (including reading comprehension, commonsen
    
[^16]: HierarchicalContrast: 一种用于跨领域零样本插槽填充的粗粒度到细粒度对比学习框架

    HierarchicalContrast: A Coarse-to-Fine Contrastive Learning Framework for Cross-Domain Zero-Shot Slot Filling. (arXiv:2310.09135v1 [cs.AI])

    [http://arxiv.org/abs/2310.09135](http://arxiv.org/abs/2310.09135)

    本研究提出了一种用于零样本插槽填充的Hierarchical Contrastive Learning Framework (HiCL)，通过粗粒度到细粒度的对比学习，学习语句令牌之间的深层语义关系，并提出了一种新的迭代标签集语义推理方法，来提高在未见插槽上的泛化能力。

    

    在面向任务的对话场景中，跨领域零样本插槽填充在利用源领域知识来学习具有高泛化能力的模型方面起着重要作用，在未知目标领域中，由于缺少带注释的数据，其性能往往不理想。然而，现有的零样本插槽填充方法在目标领域中的泛化能力有限，它们只能在已见插槽上有效地进行知识转移，对未见插槽的表现较差。为了缓解这个问题，我们提出了一种新颖的Hierarchical Contrastive Learning Framework (HiCL)用于零样本插槽填充。具体来说，我们提出了一种基于高斯分布嵌入的粗粒度到细粒度对比学习方法，通过优化间隔和内部标记分布的距离，学习语句令牌之间的深层语义关系。这鼓励HiCL在训练阶段泛化到未见的插槽类型。此外，我们提出了一种新的迭代标签集语义推理方法，来对标签进行公正的推断。

    In task-oriented dialogue scenarios, cross-domain zero-shot slot filling plays a vital role in leveraging source domain knowledge to learn a model with high generalization ability in unknown target domain where annotated data is unavailable. However, the existing state-of-the-art zero-shot slot filling methods have limited generalization ability in target domain, they only show effective knowledge transfer on seen slots and perform poorly on unseen slots. To alleviate this issue, we present a novel Hierarchical Contrastive Learning Framework (HiCL) for zero-shot slot filling. Specifically, we propose a coarseto fine-grained contrastive learning based on Gaussian-distributed embedding to learn the generalized deep semantic relations between utterance-tokens, by optimizing inter- and intra-token distribution distance. This encourages HiCL to generalize to the slot types unseen at training phase. Furthermore, we present a new iterative label set semantics inference method to unbiasedly 
    
[^17]: 一个令人沮丧的简易即插即用的中文拼写检查的检测和推理模块

    A Frustratingly Easy Plug-and-Play Detection-and-Reasoning Module for Chinese Spelling Check. (arXiv:2310.09119v1 [cs.CL])

    [http://arxiv.org/abs/2310.09119](http://arxiv.org/abs/2310.09119)

    本文提出了一个令人沮丧的简易即插即用的中文拼写检查的检测和推理模块，通过将任务分解为检测、推理和搜索子任务，有效地利用中文语言的外部知识，并且该模块能够进一步提升现有最先进的非自回归CSC模型的性能。

    

    近年来，通过设计任务特定的预训练方法或引入辅助任务，中文拼写检查（CSC）得到了显著改进，这些方法大多以端到端的方式解决了这个任务。本文提出将CSC工作流分解为检测、推理和搜索子任务，以更直接高效地利用关于中文语言的丰富外部知识。具体而言，我们设计了一个兼容现有SOTA非自回归CSC模型的即插即用的检测和推理模块，以进一步提升它们的性能。我们发现，针对一个模型训练的检测和推理模块也可以使其他模型受益。我们还研究了任务分解所提供的主要可解释性。大量实验证明了所提出模块的有效性和竞争力。

    In recent years, Chinese Spelling Check (CSC) has been greatly improved by designing task-specific pre-training methods or introducing auxiliary tasks, which mostly solve this task in an end-to-end fashion. In this paper, we propose to decompose the CSC workflow into detection, reasoning, and searching subtasks so that the rich external knowledge about the Chinese language can be leveraged more directly and efficiently. Specifically, we design a plug-and-play detection-and-reasoning module that is compatible with existing SOTA non-autoregressive CSC models to further boost their performance. We find that the detection-and-reasoning module trained for one model can also benefit other models. We also study the primary interpretability provided by the task decomposition. Extensive experiments and detailed analyses demonstrate the effectiveness and competitiveness of the proposed module.
    
[^18]: GLoRE: 评估大型语言模型的逻辑推理能力

    GLoRE: Evaluating Logical Reasoning of Large Language Models. (arXiv:2310.09107v1 [cs.CL])

    [http://arxiv.org/abs/2310.09107](http://arxiv.org/abs/2310.09107)

    本论文介绍了GLoRE，一个评估大型语言模型逻辑推理能力的基准，实验结果表明开放式LLM模型的逻辑推理能力需要提高。研究提出了一种自一致性探测方法和微调方法来改进ChatGPT和开放式LLM的性能。

    

    最近，包括GPT-4和新兴社区模型在内的大型语言模型(LLMs)展示了显著的通用语言理解能力。然而，对这些LLMs的逻辑推理能力进行评估的尝试还很少，而这是自然语言理解的一个重要方面。为了鼓励进一步研究，我们引入了GLoRE，一个精心组织的通用逻辑推理评估基准，包含了12个覆盖三种不同类型任务的数据集。我们的实验结果显示，与人类和监督微调的性能相比，开放式LLM模型的逻辑推理能力需要进一步提高；ChatGPT和GPT-4展示了较强的逻辑推理能力，GPT-4大幅超过了ChatGPT。我们提出了一种自一致性探测方法来提高ChatGPT的准确性，以及一种微调方法来提高开放式LLM的性能。

    Recently, large language models (LLMs), including notable models such as GPT-4 and burgeoning community models, have showcased significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a meticulously assembled General Logical Reasoning Evaluation benchmark comprised of 12 datasets that span three different types of tasks. Our experimental results show that compared to the performance of human and supervised fine-tuning, the logical reasoning capabilities of open LLM models necessitate additional improvement; ChatGPT and GPT-4 show a strong capability of logical reasoning, with GPT-4 surpassing ChatGPT by a large margin. We propose a self-consistency probing method to enhance the accuracy of ChatGPT and a fine-tuned method to boost the performance of an open LLM. We 
    
[^19]: Qilin-Med: 多阶段知识注入先进的医疗大型语言模型

    Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large Language Model. (arXiv:2310.09089v1 [cs.CL])

    [http://arxiv.org/abs/2310.09089](http://arxiv.org/abs/2310.09089)

    Qilin-Med是一个多阶段训练的医疗大型语言模型，通过结合领域特定继续预训练、监督微调和直接偏好优化的方法，实现了显著的性能提升，并引入了一个包含医学问答、纯文本、知识图谱和对话的3Gb中医数据集。

    

    将大型语言模型 (LLMs) 应用于医疗领域有着潜力但也面临挑战。直接为像医学这样的领域进行预训练需要大量资源，有时不可行。仅依赖于监督微调 (SFT) 可能导致过于自信的预测结果，无法利用领域特定的见解。为了解决这些问题，我们提出了一种多阶段训练方法，结合了领域特定继续预训练 (DCPT)、SFT 和直接偏好优化 (DPO)。我们研究的一个显著贡献是引入了一个 3Gb 的中医数据集 (ChiMed)，包括医学问答、纯文本、知识图谱和对话，分为三个训练阶段。我们使用我们的训练流程训练的医学LLM，Qilin-Med，在CPT和SFT阶段在CMExam上分别达到了38.4%和40.0%的准确率，超过了Baichuan-7B的33.5%。在DPO阶段，在Huatuo-26M测试集上得分为16.66。

    Integrating large language models (LLMs) into healthcare presents potential but faces challenges. Directly pre-training LLMs for domains like medicine is resource-heavy and sometimes unfeasible. Sole reliance on Supervised Fine-tuning (SFT) can result in overconfident predictions and may not tap into domain specific insights. Addressing these challenges, we present a multi-stage training method combining Domain-specific Continued Pre-training (DCPT), SFT, and Direct Preference Optimization (DPO). A notable contribution of our study is the introduction of a 3Gb Chinese Medicine (ChiMed) dataset, encompassing medical question answering, plain texts, knowledge graphs, and dialogues, segmented into three training stages. The medical LLM trained with our pipeline, Qilin-Med, exhibits significant performance boosts. In the CPT and SFT phases, it achieves 38.4% and 40.0% accuracy on the CMExam, surpassing Baichuan-7B's 33.5%. In the DPO phase, on the Huatuo-26M test set, it scores 16.66 in BL
    
[^20]: 瑞士德语言传输的方言转换

    Dialect Transfer for Swiss German Speech Translation. (arXiv:2310.09088v1 [cs.CL])

    [http://arxiv.org/abs/2310.09088](http://arxiv.org/abs/2310.09088)

    本文研究了瑞士德语言翻译系统建设中的挑战，着重探讨了方言多样性和瑞士德语与标准德语之间的差异对系统性能的影响。

    

    本文探讨了在建立瑞士德语言翻译系统中的挑战，特别关注方言多样性以及瑞士德语和标准德语之间的差异的影响。瑞士德语是一种口语语言，没有正式的书写系统，包括许多不同的方言，是一种资源匮乏的语言，只有大约500万使用者。这项研究围绕两个关键的研究问题进行：在训练瑞士德语言翻译模型时，方言的包含和排除如何影响特定方言的性能，以及瑞士德语和标准德语之间的差异如何影响系统的性能？我们表明方言多样性和语言差异给瑞士德语言翻译带来了重大挑战，这与经验研究得出的语言学假设相一致。

    This paper investigates the challenges in building Swiss German speech translation systems, specifically focusing on the impact of dialect diversity and differences between Swiss German and Standard German. Swiss German is a spoken language with no formal writing system, it comprises many diverse dialects and is a low-resource language with only around 5 million speakers. The study is guided by two key research questions: how does the inclusion and exclusion of dialects during the training of speech translation models for Swiss German impact the performance on specific dialects, and how do the differences between Swiss German and Standard German impact the performance of the systems? We show that dialect diversity and linguistic differences pose significant challenges to Swiss German speech translation, which is in line with linguistic hypotheses derived from empirical investigations.
    
[^21]: KCTS：带有令牌级幻觉检测的知识约束树搜索解码

    KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection. (arXiv:2310.09044v1 [cs.CL])

    [http://arxiv.org/abs/2310.09044](http://arxiv.org/abs/2310.09044)

    提出了一种名为KCTS的知识约束树搜索解码方法，利用知识分类器和MCTS指导冻结的LM生成与参考知识对齐的文本，同时引入了一种新颖的令牌级幻觉检测方法RIPA。

    

    大型语言模型（LLM）展示了卓越的人类级自然语言生成能力。然而，它们产生错误信息的潜力，即所谓的幻觉问题，对其部署构成重大风险。解决这个问题的一种常见方法是检索相关知识，并使用输入中的知识对LLM进行精细调节。不幸的是，这种方法会引起高训练成本，并可能对多任务模型造成灾难性遗忘。为了克服这些局限性，我们提出了一种称为KCTS（知识约束树搜索）的知识约束解码方法，它使用知识分类器得分和MCTS（蒙特卡罗树搜索）来指导冻结的LM在每个解码步骤中生成与参考知识对齐的文本。为了将序列级知识分类器适应令牌级指导，我们还提出了一种新颖的令牌级幻觉检测方法，称为RIPA（奖励拐点近似）。

    Large Language Models (LLMs) have demonstrated remarkable human-level natural language generation capabilities. However, their potential to generate misinformation, often called the hallucination problem, poses a significant risk to their deployment. A common approach to address this issue is to retrieve relevant knowledge and fine-tune the LLM with the knowledge in its input. Unfortunately, this method incurs high training costs and may cause catastrophic forgetting for multi-tasking models. To overcome these limitations, we propose a knowledge-constrained decoding method called KCTS (Knowledge-Constrained Tree Search), which guides a frozen LM to generate text aligned with the reference knowledge at each decoding step using a knowledge classifier score and MCTS (Monte-Carlo Tree Search). To adapt the sequence-level knowledge classifier to token-level guidance, we also propose a novel token-level hallucination detection method called RIPA (Reward Inflection Point Approximation). Our e
    
[^22]: MM-BigBench: 在多模态内容理解任务上评估多模态模型

    MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks. (arXiv:2310.09036v1 [cs.CL])

    [http://arxiv.org/abs/2310.09036](http://arxiv.org/abs/2310.09036)

    MM-BigBench是一个评估多模态模型在多模态内容理解任务上表现的综合评估框架，通过多模态交互来实现对多模态上下文的深入理解，并提供广泛的性能评估指标。

    

    多模态大型语言模型（MLLMs）的流行引发了近期对这些模型的评估的研究努力的激增。然而，现有的MLLMs评估研究主要关注对单模态（视觉）内容的理解和推理，忽视了在多模态（视觉-语言）内容理解领域的性能评估。除了多模态推理外，与多模态内容理解相关的任务需要对多模态上下文的深入理解，通过多模态交互来获得最终答案。在本文中，我们引入了一个全面的评估框架，称为MM-BigBench，它包含了各种指标，以广泛评估不同模型和指示在多样化的多模态内容理解任务中的性能。因此，我们的工作补充了关于MLLMs在多模态理解任务中性能的研究。

    The popularity of multimodal large language models (MLLMs) has triggered a recent surge in research efforts dedicated to evaluating these models. Nevertheless, existing evaluation studies of MLLMs primarily focus on the comprehension and reasoning of unimodal (vision) content, neglecting performance evaluations in the domain of multimodal (vision-language) content understanding. Beyond multimodal reasoning, tasks related to multimodal content comprehension necessitate a profound understanding of multimodal contexts, achieved through the multimodal interaction to obtain a final answer. In this paper, we introduce a comprehensive assessment framework called MM-BigBench, which incorporates a diverse range of metrics to offer an extensive evaluation of the performance of various models and instructions across a wide spectrum of diverse multimodal content comprehension tasks. Consequently, our work complements research on the performance of MLLMs in multimodal comprehension tasks, achieving
    
[^23]: 不添加，不错过：从预选文本段生成有效的内容保留生成模型

    Dont Add, dont Miss: Effective Content Preserving Generation from Pre-Selected Text Spans. (arXiv:2310.09017v1 [cs.CL])

    [http://arxiv.org/abs/2310.09017](http://arxiv.org/abs/2310.09017)

    本论文介绍了一个高质量的受控文本缩减（CTR）模型，解决了内容保留约束不充分强制执行和次优的银标签训练数据的限制，通过在训练和推理中增强内容保留约束，进一步改进了模型性能。

    

    最近引入的受控文本缩减（CTR）任务在典型的摘要任务中将文本生成步骤隔离出来。它通过挑战模型在输入文本的预选内容（"高亮"）中生成连贯的文本来实现。这种框架在类似摘要的任务中增加了模块化能力，允许将单个CTR模型与各种内容选择设置和模块配对使用。然而，目前还没有可靠的CTR模型，而且现有任务基线的性能中等，无法实际使用。为了填补这个空白，我们引入了一个高质量的开源CTR模型，解决了两个先前的关键限制：不充分强制执行内容保留约束和次优的银标签训练数据。通过在训练中通过强化学习和推理中通过受控解码策略来增强内容保留约束。此外，我们还大幅改进了银标签训练数据。

    The recently introduced Controlled Text Reduction (CTR) task isolates the text generation step within typical summarization-style tasks. It does so by challenging models to generate coherent text conforming to pre-selected content within the input text ("highlights").  This framing enables increased modularity in summarization-like tasks, allowing to couple a single CTR model with various content-selection setups and modules.  However, there are currently no reliable CTR models, while the performance of the existing baseline for the task is mediocre, falling short of practical utility.  Here, we address this gap by introducing a high-quality, open-source CTR model that tackles two prior key limitations: inadequate enforcement of the content-preservation constraint, and suboptimal silver training data.  Addressing these, we amplify the content-preservation constraint in both training, via RL, and inference, via a controlled decoding strategy.  Further, we substantially improve the silve
    
[^24]: CodeChain: 通过代表性子模块的自我修订链路实现模块化代码生成

    CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules. (arXiv:2310.08992v1 [cs.AI])

    [http://arxiv.org/abs/2310.08992](http://arxiv.org/abs/2310.08992)

    CodeChain是一种通过代表性子模块的自我修订链路引导模块化代码生成的新框架，旨在解决大型语言模型在解决复杂编程任务方面的挑战。

    

    大型语言模型（LLM）已经在解决简单编程任务方面非常熟练，比如在HumanEval或MBPP基准测试中的任务。然而，对于更复杂和具有竞争性的编程任务，这些模型仍然面临挑战，可能是因为它们倾向于生成作为整体代码块而不是将其分解为逻辑子任务和子模块。另一方面，有经验的程序员本能地编写具有抽象概念的模块化代码来解决复杂任务，通常会重复使用之前开发的模块。为了解决这一差距，我们提出了CodeChain，一种通过代表性子模块的自我修订链路引导模块化代码生成的新框架。具体而言，CodeChain首先通过链式思考提示指导LLM生成模块化代码。然后，它通过迭代两个步骤实施自我修订链路：1）额外...

    Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks. However, solving more complex and competitive programming tasks is still quite challenging for these models - possibly due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules. To address this gap, we propose CodeChain, a novel framework for inference that elicits modularized code generation through a chain of self-revisions, each being guided by some representative sub-modules generated in previous iterations. Concretely, CodeChain first instructs the LLM to generate modularized codes through chain-of-thought prompting. Then it applies a chain of self-revisions by iterating the two steps: 1) extra
    
[^25]: ChatKBQA: 一个基于精调大型语言模型的生成-检索框架用于知识库问答

    ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models. (arXiv:2310.08975v1 [cs.CL])

    [http://arxiv.org/abs/2310.08975](http://arxiv.org/abs/2310.08975)

    ChatKBQA是一个基于精调大型语言模型的生成-检索框架，用于改进知识库问答的效率和准确性，实验结果显示在多个数据集上取得了新的最好表现。

    

    知识库问答（KBQA）旨在通过大规模知识库（KB）获取自然语言问题的答案，通常分为两个研究组成部分：知识检索和语义解析。然而，仍然存在三个核心挑战，包括低效的知识检索、检索错误对语义解析的不利影响以及之前的KBQA方法的复杂性。在大型语言模型（LLM）时代，我们介绍了ChatKBQA，这是一个新颖的基于精调开源LLMs（如Llama-2、ChatGLM2和Baichuan2）构建的生成-检索KBQA框架。ChatKBQA提议首先使用精调的LLMs生成逻辑形式，然后通过无监督检索方法检索和替换实体和关系，从而更直观地改进了生成和检索。实验结果表明，ChatKBQA在标准KBQA数据集WebQSP和ComplexWebQuestions (CWQ)上取得了新的最先进性能。

    Knowledge Base Question Answering (KBQA) aims to derive answers to natural language questions over large-scale knowledge bases (KBs), which are generally divided into two research components: knowledge retrieval and semantic parsing. However, three core challenges remain, including inefficient knowledge retrieval, retrieval errors adversely affecting semantic parsing, and the complexity of previous KBQA methods. In the era of large language models (LLMs), we introduce ChatKBQA, a novel generate-then-retrieve KBQA framework built on fine-tuning open-source LLMs such as Llama-2, ChatGLM2 and Baichuan2. ChatKBQA proposes generating the logical form with fine-tuned LLMs first, then retrieving and replacing entities and relations through an unsupervised retrieval method, which improves both generation and retrieval more straightforwardly. Experimental results reveal that ChatKBQA achieves new state-of-the-art performance on standard KBQA datasets, WebQSP, and ComplexWebQuestions (CWQ). This
    
[^26]: 运用多Levenshtein Transformer进行基于示例的NMT研究

    Towards Example-Based NMT with Multi-Levenshtein Transformers. (arXiv:2310.08967v1 [cs.CL])

    [http://arxiv.org/abs/2310.08967](http://arxiv.org/abs/2310.08967)

    本论文研究了检索增强的机器翻译的透明度特性，提出了一个新的架构，旨在增加用户对翻译决策的透明度，并通过编辑多个示例来提高翻译效果。

    

    检索增强的机器翻译（RAMT）引起了越来越多的关注。这是因为RAMT不仅可以提高翻译度量，而且还被认为可以实现某种形式的领域适应。在这项研究中，我们研究了RAMT的另一个显著特点，即允许用户返回对翻译决策有贡献的示例，从而使翻译决策更加透明。为此，我们提出了一种新的架构，旨在增加这种透明度。该模型采用了一个检索增强的Levenshtein Transformer的版本，并使其能够同时编辑内存中找到的多个模糊匹配。我们讨论了如何在该模型中进行训练和推理，基于多路径对齐算法和模仿学习。我们的实验证明，编辑多个示例对翻译得分有积极的影响，特别是增加了从现有实例中复制的目标跨度的数量。

    Retrieval-Augmented Machine Translation (RAMT) is attracting growing attention. This is because RAMT not only improves translation metrics, but is also assumed to implement some form of domain adaptation. In this contribution, we study another salient trait of RAMT, its ability to make translation decisions more transparent by allowing users to go back to examples that contributed to these decisions.  For this, we propose a novel architecture aiming to increase this transparency. This model adapts a retrieval-augmented version of the Levenshtein Transformer and makes it amenable to simultaneously edit multiple fuzzy matches found in memory. We discuss how to perform training and inference in this model, based on multi-way alignment algorithms and imitation learning. Our experiments show that editing several examples positively impacts translation scores, notably increasing the number of target spans that are copied from existing instances.
    
[^27]: xDial-Eval: 一种多语言开放领域对话评估基准

    xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark. (arXiv:2310.08958v1 [cs.CL])

    [http://arxiv.org/abs/2310.08958](http://arxiv.org/abs/2310.08958)

    本论文提出了xDial-Eval，这是一个基于开源英语对话评估数据集构建的多语言开放领域对话评估基准。通过对12个轮次级别和6个对话级别的英语数据集进行扩展，实现了对其他九种语言的对话评估。通过对先前的基于BERT的度量和最近出现的大型语言模型的全面分析，建立了强大的评估基准。

    

    最近在无参考学习度量开放领域对话评估方面取得的进展，得益于预训练语言模型的进步和具有高质量人工注释的对话数据的可用性。然而，当前的研究主要集中在英语对话上，这些度量的泛化到其他语言尚未充分考虑。这主要是由于缺乏多语言对话评估基准。为解决这个问题，我们引入了xDial-Eval，它是基于开源英语对话评估数据集构建的。xDial-Eval包括12个轮次级别和6个对话级别的英语数据集，分别包含14930个注释轮次和8691个注释对话。英语对话数据还通过商业机器翻译系统扩展到其他九种语言。在xDial-Eval上，我们对先前的基于BERT的度量和最近出现的大型语言模型进行了全面的分析。最后，我们建立了强大的评估基准，为多语言对话评估提供了基础。

    Recent advancements in reference-free learned metrics for open-domain dialogue evaluation have been driven by the progress in pre-trained language models and the availability of dialogue data with high-quality human annotations. However, current studies predominantly concentrate on English dialogues, and the generalization of these metrics to other languages has not been fully examined. This is largely due to the absence of a multilingual dialogue evaluation benchmark. To address the issue, we introduce xDial-Eval, built on top of open-source English dialogue evaluation datasets. xDial-Eval includes 12 turn-level and 6 dialogue-level English datasets, comprising 14930 annotated turns and 8691 annotated dialogues respectively. The English dialogue data are extended to nine other languages with commercial machine translation systems. On xDial-Eval, we conduct comprehensive analyses of previous BERT-based metrics and the recently-emerged large language models. Lastly, we establish strong 
    
[^28]: ICALEPCS和IPAC会议论文的文本分析: 揭示未来洞见和高级搜索的研究趋势、主题和合作关系

    Textual Analysis of ICALEPCS and IPAC Conference Proceedings: Revealing Research Trends, Topics, and Collaborations for Future Insights and Advanced Search. (arXiv:2310.08954v1 [cs.CL])

    [http://arxiv.org/abs/2310.08954](http://arxiv.org/abs/2310.08954)

    本文通过对ICALEPCS和IPAC会议论文进行文本分析，揭示了研究趋势、主题和合作关系，为未来研究提供洞见和高级搜索工具。

    

    本文通过对过去ICALEPCS和IPAC会议论文的文本分析，揭示了该领域研究趋势和讨论主题的洞见。我们运用自然语言处理技术从摘要和论文中提取有意义的信息，并提取主题进行可视化和趋势识别，分析其演变以确定新兴研究方向，并基于内容分析网络来突出有趣的出版物。此外，我们还将提供一个先进的搜索工具，以更好地搜索现有论文，避免重复并便于参考发现。我们的分析提供了该领域研究情景的全面概述，有助于研究人员和从业者更好地了解最新技术并确定未来研究领域。

    In this paper, we show a textual analysis of past ICALEPCS and IPAC conference proceedings to gain insights into the research trends and topics discussed in the field. We use natural language processing techniques to extract meaningful information from the abstracts and papers of past conference proceedings. We extract topics to visualize and identify trends, analyze their evolution to identify emerging research directions, and highlight interesting publications based solely on their content with an analysis of their network. Additionally, we will provide an advanced search tool to better search the existing papers to prevent duplication and easier reference findings. Our analysis provides a comprehensive overview of the research landscape in the field and helps researchers and practitioners to better understand the state-of-the-art and identify areas for future research.
    
[^29]: Easier Multimodal Generation: Diffusion Models Meet LLMs

    Making Multimodal Generation Easier: When Diffusion Models Meet LLMs. (arXiv:2310.08949v1 [cs.AI])

    [http://arxiv.org/abs/2310.08949](http://arxiv.org/abs/2310.08949)

    EasyGen是一个有效的模型，它通过结合扩散模型和大型语言模型（LLMs）的能力，实现了更容易的多模态生成。相比现有的模型，EasyGen使用了一个名为BiDiffuser的双向条件扩散模型， 提供了更高效的模态交互，并且不仅能够生成文本回复，还能够促进文本到图像的生成。

    

    我们提出了EasyGen，一个有效的模型，通过利用扩散模型和大型语言模型（LLMs）的能力，增强了多模态理解和生成。不同于现有的主要依赖于编码器如CLIP或ImageBind，并且需要大量训练数据来桥接模态之间差距的多模态模型，EasyGen基于一个名为BiDiffuser的双向条件扩散模型构建，促进了更高效的模态交互。EasyGen通过简单的投影层将BiDiffuser和LLM进行集成，处理图像到文本的生成。与大多数现有的限于生成文本回复的多模态模型不同，EasyGen还可以通过利用LLM创建文本描述，并由BiDiffuser解释生成适当的视觉回复来促进文本到图像的生成。广泛的定量和定性实验证明了EasyGen的有效性，其训练可以...

    We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge the gap between modalities, EasyGen is built upon a bidirectional conditional diffusion model named BiDiffuser, which promotes more efficient interactions between modalities. EasyGen handles image-to-text generation by integrating BiDiffuser and an LLM via a simple projection layer. Unlike most existing multimodal models that are limited to generating text responses, EasyGen can also facilitate text-to-image generation by leveraging the LLM to create textual descriptions, which can be interpreted by BiDiffuser to generate appropriate visual responses. Extensive quantitative and qualitative experiments demonstrate the effectiveness of EasyGen, whose training can b
    
[^30]: CAMELL：基于置信度的高效自监督主动学习与标签验证获取模型

    CAMELL: Confidence-based Acquisition Model for Efficient Self-supervised Active Learning with Label Validation. (arXiv:2310.08944v1 [cs.CL])

    [http://arxiv.org/abs/2310.08944](http://arxiv.org/abs/2310.08944)

    CAMELL是一个适用于序列多输出问题的主动学习框架，通过仅需专家标注序列的一小部分、自监督和标签验证机制来解决监督神经方法对大规模标注数据集的依赖限制。

    

    在序列任务中，受大规模且精确标注数据集的依赖限制，监督神经方法受到阻碍。标注质量随着从专家标注向众包标注的转变而逐渐恶化。为了解决这些挑战，我们提出了CAMELL（Confidence-based Acquisition Model for Efficient self-supervised active Learning with Label validation），这是一个针对序列多输出问题量身定制的基于池化的主动学习框架。CAMELL具有三个核心特点：(1)仅要求专家标注所选序列的一小部分，(2)为其余序列提供自监督，(3)采用标签验证机制，防止错误标签污染数据集并影响模型性能。我们在序列任务中对CAMELL进行了评估，特别强调对话信念跟踪，这是一个受限制的任务。

    Supervised neural approaches are hindered by their dependence on large, meticulously annotated datasets, a requirement that is particularly cumbersome for sequential tasks. The quality of annotations tends to deteriorate with the transition from expert-based to crowd-sourced labelling. To address these challenges, we present \textbf{CAMELL} (Confidence-based Acquisition Model for Efficient self-supervised active Learning with Label validation), a pool-based active learning framework tailored for sequential multi-output problems. CAMELL possesses three core features: (1) it requires expert annotators to label only a fraction of a chosen sequence, (2) it facilitates self-supervision for the remainder of the sequence, and (3) it employs a label validation mechanism to prevent erroneous labels from contaminating the dataset and harming model performance. We evaluate CAMELL on sequential tasks, with a special emphasis on dialogue belief tracking, a task plagued by the constraints of limited
    
[^31]: 多层自适应对比学习在对话生成中的知识内化

    Multi-level Adaptive Contrastive Learning for Knowledge Internalization in Dialogue Generation. (arXiv:2310.08943v1 [cs.CL])

    [http://arxiv.org/abs/2310.08943](http://arxiv.org/abs/2310.08943)

    这是一篇关于知识引导对话生成的论文，通过引入多层自适应对比学习（MACL）框架，并在令牌级和序列级上动态采样负例来解决模型简单插入知识片段导致的退化问题。

    

    知识引导的对话生成旨在通过整合外部知识来补充上下文，从而缓解文本退化问题。然而，模型往往无法以类似人类的方式将此信息内化到回答中。相反，它只是简单地将提供的知识片段插入到普通的回答中。因此，生成的回答往往乏味、不连贯，并且缺乏互动性，这意味着退化问题仍未解决。在这项工作中，我们首先发现，这种复制式退化主要是由于弱概率目标造成的，它允许模型通过仅基于重叠的表面模式匹配来“欺骗”目标。为了克服这个挑战，我们提出了一个多层自适应对比学习（MACL）框架，该框架在令牌级和序列级上动态采样负例，并随后惩罚退化行为。

    Knowledge-grounded dialogue generation aims to mitigate the issue of text degeneration by incorporating external knowledge to supplement the context. However, the model often fails to internalize this information into responses in a human-like manner. Instead, it simply inserts segments of the provided knowledge into generic responses. As a result, the generated responses tend to be tedious, incoherent, and in lack of interactivity which means the degeneration problem is still unsolved. In this work, we first find that such copying-style degeneration is primarily due to the weak likelihood objective, which allows the model to "cheat" the objective by merely duplicating knowledge segments in a superficial pattern matching based on overlap. To overcome this challenge, we then propose a Multi-level Adaptive Contrastive Learning (MACL) framework that dynamically samples negative examples and subsequently penalizes degeneration behaviors at both the token-level and sequence-level. Extensive
    
[^32]: 朝着最大信息增益的信息丰富的少样本提示实现上下文学习

    Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning. (arXiv:2310.08923v1 [cs.CL])

    [http://arxiv.org/abs/2310.08923](http://arxiv.org/abs/2310.08923)

    本文旨在解决大型语言模型在上下文学习中的不稳定性问题，通过量化信息增益来探索数据示例的信息能力，并采用校准策略来缓解模板偏差。实验证明了该方法的有效性。

    

    大型语言模型具有利用少量与新的下游任务有关的示例进行上下文学习的能力。然而，这种学习范式饱受输入分布、顺序和提示格式等因素引起的高度不稳定性的困扰。在本文中，我们证明，即使当所有这些因素保持不变时，随机选择示例仍然会导致高方差。因此，我们旨在通过量化观察给定示例候选人后预测所获得的信息增益（IG）来探索数据示例的信息能力，然后我们提出从中采样那些具有最大IG的示例。此外，我们发现模板偏差的存在，在采样过程中可能会导致对IG的不公平评估。为了缓解这种偏差，我们引入了采样之前的校准策略。实验结果证明了我们方法的有效性。

    Large Language models (LLMs) possess the capability to engage In-context Learning (ICL) by leveraging a few demonstrations pertaining to a new downstream task as conditions. However, this particular learning paradigm suffers from high instability stemming from substantial variances induced by factors such as the input distribution of selected examples, their ordering, and prompt formats. In this work, we demonstrate that even when all these factors are held constant, the random selection of examples still results in high variance. Consequently, we aim to explore the informative ability of data examples by quantifying the Information Gain (IG) obtained in prediction after observing a given example candidate. Then we propose to sample those with maximum IG. Additionally, we identify the presence of template bias, which can lead to unfair evaluations of IG during the sampling process. To mitigate this bias, we introduce Calibration Before Sampling strategy. The experimental results illust
    
[^33]: 知识图谱嵌入的关系感知集成学习

    Relation-aware Ensemble Learning for Knowledge Graph Embedding. (arXiv:2310.08917v1 [cs.LG])

    [http://arxiv.org/abs/2310.08917](http://arxiv.org/abs/2310.08917)

    本论文提出了一种关系感知集成学习方法，用于知识图谱嵌入任务，并通过分割搜索合并的算法在搜索关系感知集成权重方面取得了显著性能提升。

    

    知识图谱嵌入是自然语言处理中的基础任务，已经提出了各种方法来探索不同方式的语义模式。本文提出了一种关系感知集成学习方法，通过利用现有方法来学习一个集成模型。然而，使用关系感知集成探索这些语义会导致比一般集成方法更大的搜索空间。为了解决这个问题，我们提出了一个分割搜索合并的算法RelEns-DSC，它独立地搜索关系感知集成的权重。该算法具有与一般集成方法相同的计算成本，但性能更好。在基准数据集上的实验结果表明了所提方法在高效搜索关系感知集成权重和达到最先进的嵌入性能方面的有效性。代码公开在https://github.com/LARS-research/RelEns。

    Knowledge graph (KG) embedding is a fundamental task in natural language processing, and various methods have been proposed to explore semantic patterns in distinctive ways. In this paper, we propose to learn an ensemble by leveraging existing methods in a relation-aware manner. However, exploring these semantics using relation-aware ensemble leads to a much larger search space than general ensemble methods. To address this issue, we propose a divide-search-combine algorithm RelEns-DSC that searches the relation-wise ensemble weights independently. This algorithm has the same computation cost as general ensemble methods but with much better performance. Experimental results on benchmark datasets demonstrate the effectiveness of the proposed method in efficiently searching relation-aware ensemble weights and achieving state-of-the-art embedding performance. The code is public at https://github.com/LARS-research/RelEns.
    
[^34]: 人机协同的大型语言模型机器翻译

    Human-in-the-loop Machine Translation with Large Language Model. (arXiv:2310.08908v1 [cs.CL])

    [http://arxiv.org/abs/2310.08908](http://arxiv.org/abs/2310.08908)

    本研究提出了一种人机协同的流程，通过引导大型语言模型生成定制输出，拓展了机器翻译的能力。

    

    大型语言模型(LLM)因其上下文学习机制和新兴能力而受到广泛关注。研究界已经进行了几个试点研究，将LLM应用于机器翻译任务并从多个角度评估其性能。然而，以往的研究主要集中在LLM本身，还未探索人类在LLM推理过程中的干预。LLM具有上下文学习和提示工程等特点，与人类在语言任务中的认知能力密切相关，为人机协同生成提供了直观的解决方案。在本研究中，我们提出了一个人机协同的流程，通过修订指令引导LLM生成定制输出。该流程首先启动LLM生成草稿翻译，然后利用自动检索或人类反馈作为监督信号，通过上下文学习提升LLM的翻译质量。

    The large language model (LLM) has garnered significant attention due to its in-context learning mechanisms and emergent capabilities. The research community has conducted several pilot studies to apply LLMs to machine translation tasks and evaluate their performance from diverse perspectives. However, previous research has primarily focused on the LLM itself and has not explored human intervention in the inference process of LLM. The characteristics of LLM, such as in-context learning and prompt engineering, closely mirror human cognitive abilities in language tasks, offering an intuitive solution for human-in-the-loop generation. In this study, we propose a human-in-the-loop pipeline that guides LLMs to produce customized outputs with revision instructions. The pipeline initiates by prompting the LLM to produce a draft translation, followed by the utilization of automatic retrieval or human feedback as supervision signals to enhance the LLM's translation through in-context learning. 
    
[^35]: SeqXGPT: 句子级AI生成文本检测

    SeqXGPT: Sentence-Level AI-Generated Text Detection. (arXiv:2310.08903v1 [cs.CL])

    [http://arxiv.org/abs/2310.08903](http://arxiv.org/abs/2310.08903)

    本文介绍了SeqXGPT，这是一种句子级AI生成文本检测方法。通过利用白盒LLMs的对数概率列表作为特征，SeqXGPT在句子级别的AIGT检测中取得了良好的效果。

    

    广泛应用的大型语言模型(LLMs)可以生成类似人类的内容，引发了对LLMs滥用的担忧。因此，建立强大的AI生成文本（AIGT）检测器非常重要。目前的工作只考虑文档级别的AIGT检测，因此在本文中，我们首先通过合成一个数据集，该数据集包含由LLMs修改过的句子和由人类编写的句子，引入了一个句子级别的检测挑战。然后，我们提出了SeqXGPT，一种利用白盒LLMs的对数概率列表作为句子级AIGT检测特征的新方法。这些特征类似于语音处理中的“波浪”，LLMs无法研究其组成。因此，我们基于卷积和自注意力网络构建了SeqXGPT。我们在句子和文档级别的检测挑战中进行了测试。实验结果显示之前的方法在句子级别的检测中存在困难。

    Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs. Therefore, it is important to build strong AI-generated text (AIGT) detectors. Current works only consider document-level AIGT detection, therefore, in this paper, we first introduce a sentence-level detection challenge by synthesizing a dataset that contains documents that are polished with LLMs, that is, the documents contain sentences written by humans and sentences modified by LLMs. Then we propose \textbf{Seq}uence \textbf{X} (Check) \textbf{GPT}, a novel method that utilizes log probability lists from white-box LLMs as features for sentence-level AIGT detection. These features are composed like \textit{waves} in speech processing and cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution and self-attention networks. We test it in both sentence and document-level detection challenges. Experimental results show that previous methods struggle in
    
[^36]: 福利外交：基准化语言模型的合作能力

    Welfare Diplomacy: Benchmarking Language Model Cooperation. (arXiv:2310.08901v1 [cs.MA])

    [http://arxiv.org/abs/2310.08901](http://arxiv.org/abs/2310.08901)

    这项研究提出了福利外交这一通用和变种的零和游戏，目的是衡量和强化语言模型的合作能力，并发现使用最先进模型的基准智能体在达到高社会福利时存在可利用性问题。

    

    随着人工智能系统的能力不断增长和广泛部署，对于衡量它们合作能力的强大基准是必要的。遗憾的是，大多数多智能体基准要么是零和游戏，要么是纯粹合作的，给予了非常有限的机会进行这种测量。我们引入了零和游戏Diplomacy的一个通用和变种——福利外交，在其中玩家必须平衡军事征服和国内福利的投资。我们认为福利外交可以更清晰地评估并提供更强大的合作能力的培训激励。我们的贡献有：（1）提出福利外交的规则并通过开源Diplomacy引擎实现它们；（2）使用零样本指导的语言模型构建基准智能体；以及（3）进行实验，发现使用最先进模型的基准智能体可以达到很高的社会福利，但是存在被利用的问题。我们的工作旨在通过人工智能的福利外交促进社会的安全。

    The growing capabilities and increasingly widespread deployment of AI systems necessitate robust benchmarks for measuring their cooperative capabilities. Unfortunately, most multi-agent benchmarks are either zero-sum or purely cooperative, providing limited opportunities for such measurements. We introduce a general-sum variant of the zero-sum board game Diplomacy -- called Welfare Diplomacy -- in which players must balance investing in military conquest and domestic welfare. We argue that Welfare Diplomacy facilitates both a clearer assessment of and stronger training incentives for cooperative capabilities. Our contributions are: (1) proposing the Welfare Diplomacy rules and implementing them via an open-source Diplomacy engine; (2) constructing baseline agents using zero-shot prompted language models; and (3) conducting experiments where we find that baselines using state-of-the-art models attain high social welfare but are exploitable. Our work aims to promote societal safety by ai
    
[^37]: 探索多样化AI监督的原则

    Exploration with Principles for Diverse AI Supervision. (arXiv:2310.08899v1 [cs.CL])

    [http://arxiv.org/abs/2310.08899](http://arxiv.org/abs/2310.08899)

    本论文提出了一种被称为探索性AI的新范式，旨在自主生成高质量的训练数据。通过利用大型语言模型评估生成内容的新颖性，使AI不再过度依赖人类监督。

    

    使用下一个标记预测来训练大型transformer在AI领域取得了突破性进展。尽管这种生成型AI方法取得了令人印象深刻的结果，但它严重依赖于人类监督。即使是像ChatGPT这样的最先进的AI模型也依赖于通过人类演示的微调，需要大量的人类输入和领域专业知识。对人类监督的强烈依赖对于推动AI创新构成了重大障碍。为了解决这个限制，我们提出了一种新的范式，称为探索性AI（EAI），旨在自主生成高质量的训练数据。从无监督强化学习（RL）的预训练中获得灵感，EAI在自然语言空间内实现了探索。我们通过利用大型语言模型评估生成内容的新颖性来实现这一目标。我们的方法包括两个关键组成部分：一个生成按照探索原则生成新颖内容的执行者和一个评估生成内容的评论家。

    Training large transformers using next-token prediction has given rise to groundbreaking advancements in AI. While this generative AI approach has produced impressive results, it heavily leans on human supervision. Even state-of-the-art AI models like ChatGPT depend on fine-tuning through human demonstrations, demanding extensive human input and domain expertise. This strong reliance on human oversight poses a significant hurdle to the advancement of AI innovation. To address this limitation, we propose a novel paradigm termed Exploratory AI (EAI) aimed at autonomously generating high-quality training data. Drawing inspiration from unsupervised reinforcement learning (RL) pretraining, EAI achieves exploration within the natural language space. We accomplish this by harnessing large language models to assess the novelty of generated content. Our approach employs two key components: an actor that generates novel content following exploration principles and a critic that evaluates the gen
    
[^38]: PerturbScore: 连接自然语言处理中的离散和连续扰动

    PerturbScore: Connecting Discrete and Continuous Perturbations in NLP. (arXiv:2310.08889v1 [cs.CL])

    [http://arxiv.org/abs/2310.08889](http://arxiv.org/abs/2310.08889)

    本文提出了PerturbScore，通过连接离散和连续扰动来帮助理解自然语言处理中的离散扰动，通过实验结果表明PerturbScore超过了以前的离散扰动测量方法，并在不同的数据集上得到有效推广。

    

    随着神经网络在自然语言处理中的快速发展，模型的鲁棒性问题越来越受到关注。与计算机视觉不同，文本的离散性质使得在自然语言处理中探索鲁棒性更具挑战性。因此，在本文中，我们旨在通过连接离散扰动和连续扰动，从而将其作为一个桥梁，帮助理解自然语言处理模型中的离散扰动。具体而言，我们首先探索如何连接和衡量离散扰动与连续扰动之间的相关性。然后，我们设计了一个回归任务作为PerturbScore，以自动学习此相关性。通过实验结果，我们发现可以建立离散和连续扰动之间的连接，并使用提出的PerturbScore来学习这种相关性，超过了先前用于离散扰动测量的方法。此外，提出的PerturbScore在不同的数据集，扰动上都能很好地推广。

    With the rapid development of neural network applications in NLP, model robustness problem is gaining more attention. Different from computer vision, the discrete nature of texts makes it more challenging to explore robustness in NLP. Therefore, in this paper, we aim to connect discrete perturbations with continuous perturbations, therefore we can use such connections as a bridge to help understand discrete perturbations in NLP models. Specifically, we first explore how to connect and measure the correlation between discrete perturbations and continuous perturbations. Then we design a regression task as a PerturbScore to learn the correlation automatically. Through experimental results, we find that we can build a connection between discrete and continuous perturbations and use the proposed PerturbScore to learn such correlation, surpassing previous methods used in discrete perturbation measuring. Further, the proposed PerturbScore can be well generalized to different datasets, perturb
    
[^39]: InstructTODS: 用于端到端任务导向对话系统的大规模语言模型

    InstructTODS: Large Language Models for End-to-End Task-Oriented Dialogue Systems. (arXiv:2310.08885v1 [cs.CL])

    [http://arxiv.org/abs/2310.08885](http://arxiv.org/abs/2310.08885)

    InstructTODS是一个端到端任务导向对话系统的大规模语言模型，通过利用LLMs生成代理信念状态，并在零-shot的情况下适应多个领域。在实验中展示了与完全微调的TODS相媲美的性能，并且经过严格的人工评估，InstructTODS所生成的对话回应在帮助性、信息量和人性化方面明显优于黄金回应和最先进的TODS。

    

    大规模语言模型(LLMs)被广泛应用于自然语言处理(NLP)的各种任务，但在任务导向对话系统(TODS)方面仍存在很大的探索空间，特别是在端到端的TODS中。我们提出了InstructTODS，一个新颖的即插即用框架，用于零-shot端到端任务导向对话系统，在不进行微调的情况下能够适应多个领域。通过利用LLMs，InstructTODS生成一个代理信念状态，将用户意图无缝地转化为动态查询，以与任何知识库进行高效交互。我们的大量实验表明，InstructTODS能够在没有先验知识或任务特定数据的情况下，引导对话成功完成，并取得与完全微调的TODS相当的性能。此外，对端到端TODS的严格人工评估表明，InstructTODS所生成的对话回应在帮助性、信息量和人性化等方面显著优于黄金回应和最先进的TODS。

    Large language models (LLMs) have been used for diverse tasks in natural language processing (NLP), yet remain under-explored for task-oriented dialogue systems (TODS), especially for end-to-end TODS. We present InstructTODS, a novel off-the-shelf framework for zero-shot end-to-end task-oriented dialogue systems that can adapt to diverse domains without fine-tuning. By leveraging LLMs, InstructTODS generates a proxy belief state that seamlessly translates user intentions into dynamic queries for efficient interaction with any KB. Our extensive experiments demonstrate that InstructTODS achieves comparable performance to fully fine-tuned TODS in guiding dialogues to successful completion without prior knowledge or task-specific data. Furthermore, a rigorous human evaluation of end-to-end TODS shows that InstructTODS produces dialogue responses that notably outperform both the gold responses and the state-of-the-art TODS in terms of helpfulness, informativeness, and humanness. Moreover, t
    
[^40]: 用于端到端任务导向对话系统的检索-生成对齐方法

    Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue System. (arXiv:2310.08877v1 [cs.CL])

    [http://arxiv.org/abs/2310.08877](http://arxiv.org/abs/2310.08877)

    本文提出了一种用于任务导向对话系统的检索-生成对齐方法，通过利用最大边际似然来训练有感知力的检索器，并结合各种元知识来指导生成器，从而提高了知识的利用效率和生成回应的质量。

    

    发展一个高效的检索器从大规模的知识库中检索知识对于任务导向对话系统来说至关重要，以便有效处理本地化和专业化的任务。然而，广泛使用的生成模型（如T5和ChatGPT）在生成回应时通常很难区分检索到的知识库记录之间的细微差异，导致生成的回应质量不理想。在本文中，我们提出了利用最大边际似然来训练一个有感知力的检索器的方法，通过利用来自回应生成的信号进行监督。此外，我们的方法不仅考虑到检索到的实体，还结合了各种元知识来指导生成器，从而提高了知识的利用效率。我们使用T5和ChatGPT作为基础模型，在三个任务导向的对话数据集上评估了我们的方法。结果表明，当结合元知识时，回应生成器可以有效地利用该知识。

    Developing an efficient retriever to retrieve knowledge from a large-scale knowledge base (KB) is critical for task-oriented dialogue systems to effectively handle localized and specialized tasks. However, widely used generative models such as T5 and ChatGPT often struggle to differentiate subtle differences among the retrieved KB records when generating responses, resulting in suboptimal quality of generated responses. In this paper, we propose the application of maximal marginal likelihood to train a perceptive retriever by utilizing signals from response generation for supervision. In addition, our approach goes beyond considering solely retrieved entities and incorporates various meta knowledge to guide the generator, thus improving the utilization of knowledge. We evaluate our approach on three task-oriented dialogue datasets using T5 and ChatGPT as the backbone models. The results demonstrate that when combined with meta knowledge, the response generator can effectively leverage 
    
[^41]: 引导AMR解析与逆向图线性化

    Guiding AMR Parsing with Reverse Graph Linearization. (arXiv:2310.08860v1 [cs.CL])

    [http://arxiv.org/abs/2310.08860](http://arxiv.org/abs/2310.08860)

    本论文引入了逆向图线性化（RGL）的方法来解决序列到序列的AMR解析中结构丢失积累的问题。通过定义默认和逆向线性化顺序，并通过自我蒸馏机制引导模型生成默认线性化，提高了解析性能。

    

    抽象意义表示（AMR）解析旨在从给定的句子中提取一个抽象的语义图。序列到序列的方法将语义图线性化为节点和边的序列，并直接生成线性化的图，取得了良好的性能。然而，我们观察到这些方法在解码过程中会出现结构丢失积累的问题，导致后解码的节点和边的F1得分比先解码的要低得多。为了解决这个问题，我们提出了一种新的增强框架，即逆向图线性化（RGL）。RGL定义了AMR图的默认和逆向线性化顺序，其中在默认顺序的后部出现的大多数结构在逆向顺序的前部出现，反之亦然。RGL通过两次自我蒸馏机制将逆向线性化引入原始AMR解析器，从而在生成默认线性化时引导模型。

    Abstract Meaning Representation (AMR) parsing aims to extract an abstract semantic graph from a given sentence. The sequence-to-sequence approaches, which linearize the semantic graph into a sequence of nodes and edges and generate the linearized graph directly, have achieved good performance. However, we observed that these approaches suffer from structure loss accumulation during the decoding process, leading to a much lower F1-score for nodes and edges decoded later compared to those decoded earlier. To address this issue, we propose a novel Reverse Graph Linearization (RGL) enhanced framework. RGL defines both default and reverse linearization orders of an AMR graph, where most structures at the back part of the default order appear at the front part of the reversed order and vice versa. RGL incorporates the reversed linearization to the original AMR parser through a two-pass self-distillation mechanism, which guides the model when generating the default linearizations. Our analysi
    
[^42]: 大型语言模型作为个性化知识驱动对话的源计划器

    Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogue. (arXiv:2310.08840v1 [cs.CL])

    [http://arxiv.org/abs/2310.08840](http://arxiv.org/abs/2310.08840)

    本研究提出了SAFARI框架，利用大型语言模型作为个性化知识驱动对话系统的源计划器，使得多个知识源的依赖关系能够被整合，并能生成一致的回应。

    

    开放域对话系统通常需要不同的知识源来生成更具信息性和证据性的回应。然而，现有的知识驱动对话系统要么专注于单一知识源，要么忽视了多个知识源之间的依赖关系，可能导致生成不一致甚至矛盾的回应。为了整合多个知识源和它们之间的依赖关系，我们提出了SAFARI，这是一个新颖的框架，利用了大型语言模型在监督和非监督设置下的出色能力来规划、理解和整合。具体而言，SAFARI将知识的基础分解为多个源和回应生成，从而方便地扩展到各种知识源，包括不使用任何源的可能性。为了研究这个问题，我们构建了一个个性化的、知识驱动的对话数据集。

    Open-domain dialogue system usually requires different sources of knowledge to generate more informative and evidential responses. However, existing knowledge-grounded dialogue systems either focus on a single knowledge source or overlook the dependency between multiple sources of knowledge, which may result in generating inconsistent or even paradoxical responses. To incorporate multiple knowledge sources and dependencies between them, we propose SAFARI, a novel framework that leverages the exceptional capabilities of large language models (LLMs) in planning, understanding, and incorporating under both supervised and unsupervised settings. Specifically, SAFARI decouples the knowledge grounding into multiple sources and response generation, which allows easy extension to various knowledge sources including the possibility of not using any sources. To study the problem, we construct a personalized knowledge-grounded dialogue dataset \textit{\textbf{K}nowledge \textbf{B}ehind \textbf{P}e
    
[^43]: 对于压缩Transformer语言模型的任务不可知蒸馏方法的比较分析

    A Comparative Analysis of Task-Agnostic Distillation Methods for Compressing Transformer Language Models. (arXiv:2310.08797v1 [cs.CL])

    [http://arxiv.org/abs/2310.08797](http://arxiv.org/abs/2310.08797)

    本研究比较了用于压缩Transformer语言模型的几种任务不可知蒸馏方法，并发现基于MiniLMv2的MHA转移是最佳选择。

    

    大型语言模型已经成为现代自然语言处理(NLP)中至关重要的组件，在各种任务中实现了最先进的性能。然而，由于昂贵的推断成本，它们在实际部署中常常效率低下。知识蒸馏是一种提高其效率并保持大部分效能的有希望的技术。在本文中，我们重现、比较和分析了几种代表性的、用于Transformer语言模型任务不可知(通用)蒸馏的方法。我们研究了输出分布(OD)转移、隐藏状态(HS)转移以及基于MiniLMv2的多头注意力(MHA)转移等多种蒸馏方法在各种学生架构下的有效性，包括单语(英语)和多语设置。总体而言，我们发现基于MiniLMv2的MHA转移通常是蒸馏的最佳选择，并解释了潜在的原因。

    Large language models have become a vital component in modern NLP, achieving state of the art performance in a variety of tasks. However, they are often inefficient for real-world deployment due to their expensive inference costs. Knowledge distillation is a promising technique to improve their efficiency while retaining most of their effectiveness. In this paper, we reproduce, compare and analyze several representative methods for task-agnostic (general-purpose) distillation of Transformer language models. Our target of study includes Output Distribution (OD) transfer, Hidden State (HS) transfer with various layer mapping strategies, and Multi-Head Attention (MHA) transfer based on MiniLMv2. Through our extensive experiments, we study the effectiveness of each method for various student architectures in both monolingual (English) and multilingual settings. Overall, we show that MHA transfer based on MiniLMv2 is generally the best option for distillation and explain the potential reaso
    
[^44]: 全流程故事情节生成器

    End-to-end Story Plot Generator. (arXiv:2310.08796v1 [cs.CL])

    [http://arxiv.org/abs/2310.08796](http://arxiv.org/abs/2310.08796)

    这款全流程故事情节生成器通过优化模型和设计提示信息，以替代昂贵的API调用，实现了低成本生成高质量训练数据集。

    

    故事情节虽短，但包含了一个完整故事中大部分关键信息，它可能包含数万字的内容。本文研究自动生成故事情节的问题，包括故事前提、角色描述、情节大纲等。现有的情节生成器（如 DOC (Yang et al., 2022a)）在故事情节的规划阶段需要数百到数千次LLM（如OpenAI API）的调用，这既耗时又昂贵，至少需要几分钟。此外，这种方法的硬编码特性使得流程不可微分，阻碍了快速定制和个性化的情节生成器的发展。本文提出了三个模型：OpenPlot、E2EPlot和RLPlot，以应对这些挑战。OpenPlot通过精心设计的提示将昂贵的OpenAI API调用替换为使用LLaMA2 (Touvron et al., 2023)调用，从而以低成本生成高质量的训练数据集。

    Story plots, while short, carry most of the essential information of a full story that may contain tens of thousands of words. We study the problem of automatic generation of story plots, which includes story premise, character descriptions, plot outlines, etc. To generate a single engaging plot, existing plot generators (e.g., DOC (Yang et al., 2022a)) require hundreds to thousands of calls to LLMs (e.g., OpenAI API) in the planning stage of the story plot, which is costly and takes at least several minutes. Moreover, the hard-wired nature of the method makes the pipeline non-differentiable, blocking fast specialization and personalization of the plot generator. In this paper, we propose three models, $\texttt{OpenPlot}$, $\texttt{E2EPlot}$ and $\texttt{RLPlot}$, to address these challenges. $\texttt{OpenPlot}$ replaces expensive OpenAI API calls with LLaMA2 (Touvron et al., 2023) calls via careful prompt designs, which leads to inexpensive generation of high-quality training datasets
    
[^45]: 通过追踪偏见影响来减轻问题回答模型的偏见

    Mitigating Bias for Question Answering Models by Tracking Bias Influence. (arXiv:2310.08795v1 [cs.CL])

    [http://arxiv.org/abs/2310.08795](http://arxiv.org/abs/2310.08795)

    本论文提出了一种名为BMBI的方法来减轻多选问题回答模型的偏见。通过观察一个查询实例对另一个实例的影响，测量查询实例的偏见程度，并将其作为优化目标，形成一个多任务学习设置。同时引入新的偏见评估指标以量化偏见。

    

    已经证明各种NLP任务的模型存在刻板印象，而问题回答（QA）模型中的偏见尤其有害，因为输出的答案可能直接被最终用户使用。已经有数据集用于评估QA模型中的偏见，但是对于QA模型的偏见缓解技术仍处于探索阶段。本工作中，我们提出了一种名为BMBI的方法，用于缓解多选问题回答模型的偏见。基于一个直觉，即如果一个模型从一个有偏见的例子中学到了东西，它可能更容易出现偏见，我们通过观察一个查询实例对另一个实例的影响来衡量查询实例的偏见程度。如果受到影响的实例更偏见，我们认为查询实例是有偏见的。我们使用检测到的偏见程度作为优化目标，形成一个多任务学习设置，除了原来的QA任务。我们还引入了一种新的偏见评估指标，以全面而敏感的方式量化偏见。我们展示了我们的方法可以应用于减轻QA模型的偏见。

    Models of various NLP tasks have been shown to exhibit stereotypes, and the bias in the question answering (QA) models is especially harmful as the output answers might be directly consumed by the end users. There have been datasets to evaluate bias in QA models, while bias mitigation technique for the QA models is still under-explored. In this work, we propose BMBI, an approach to mitigate the bias of multiple-choice QA models. Based on the intuition that a model would lean to be more biased if it learns from a biased example, we measure the bias level of a query instance by observing its influence on another instance. If the influenced instance is more biased, we derive that the query instance is biased. We then use the bias level detected as an optimization objective to form a multi-task learning setting in addition to the original QA task. We further introduce a new bias evaluation metric to quantify bias in a comprehensive and sensitive way. We show that our method could be applie
    
[^46]: “我不是种族主义者，但是……”：揭示大型语言模型内部知识中的偏见

    "Im not Racist but...": Discovering Bias in the Internal Knowledge of Large Language Models. (arXiv:2310.08780v1 [cs.CL])

    [http://arxiv.org/abs/2310.08780](http://arxiv.org/abs/2310.08780)

    本文介绍了一种纯提示式的方法，用于揭示大型语言模型中隐藏的刻板印象，通过动态生成内部刻板印象的知识表示，我们能够识别这些模型中存在的偏见。这项工作在推进透明度和促进自然语言处理系统的公平性方面做出了贡献。

    

    大型语言模型（LLM）因其在不断扩展的自然语言处理任务中的出色表现而受到了极大关注。然而，这些模型被证明存在内在的社会偏见或刻板印象，这可能会对它们在许多下游应用中的性能产生不利影响。本文提出了一种全新的纯提示式方法，用于揭示任意LLM中隐藏的刻板印象。我们的方法动态生成了内部刻板印象的知识表示，从而能够识别LLM内部知识中编码的偏见。通过揭示LLM中存在的偏见并提供一种系统性的分析方法，我们的工作在推进透明度和促进自然语言处理系统的公平性方面做出了贡献。

    Large language models (LLMs) have garnered significant attention for their remarkable performance in a continuously expanding set of natural language processing tasks. However, these models have been shown to harbor inherent societal biases, or stereotypes, which can adversely affect their performance in their many downstream applications. In this paper, we introduce a novel, purely prompt-based approach to uncover hidden stereotypes within any arbitrary LLM. Our approach dynamically generates a knowledge representation of internal stereotypes, enabling the identification of biases encoded within the LLM's internal knowledge. By illuminating the biases present in LLMs and offering a systematic methodology for their analysis, our work contributes to advancing transparency and promoting fairness in natural language processing systems.
    
[^47]: 校准使得摘要模型在一致性方面更为准确

    Calibrating Likelihoods towards Consistency in Summarization Models. (arXiv:2310.08764v1 [cs.CL])

    [http://arxiv.org/abs/2310.08764](http://arxiv.org/abs/2310.08764)

    通过校准模型生成的序列的似然性，使其更好地与通过自然语言推理（NLI）模型测量的一致性度量相一致，从而提高摘要模型的一致性和质量。

    

    尽管抽象化文本摘要取得了一些新的进展，但目前的摘要模型仍然存在生成事实不一致的摘要的问题，这减弱了它们在实际应用中的效用。我们认为造成这种行为的主要原因是由于基于最大似然目标训练的摘要模型在给定上下文时赋予可能序列高概率，但它们往往不能准确地根据其一致性排名序列。在这项工作中，我们通过校准模型生成的序列的似然性，使其更好地与通过自然语言推理（NLI）模型测量的一致性度量相一致来解决这个问题。人类评估研究和自动指标表明，经过校准的模型生成更一致且更高质量的摘要。我们还展示了使用我们方法训练的模型返回的概率与NLI得分更为对齐，这显著提高了摘要模型的可靠性。

    Despite the recent advances in abstractive text summarization, current summarization models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application. We argue that the main reason for such behavior is that the summarization models trained with maximum likelihood objective assign high probability to plausible sequences given the context, but they often do not accurately rank sequences by their consistency. In this work, we solve this problem by calibrating the likelihood of model generated sequences to better align with a consistency metric measured by natural language inference (NLI) models. The human evaluation study and automatic metrics show that the calibrated models generate more consistent and higher-quality summaries. We also show that the models trained using our method return probabilities that are better aligned with the NLI scores, which significantly increase reliability of summarization models.
    
[^48]: CompA: 解决音频-语言模型中的组合推理差距

    CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models. (arXiv:2310.08753v1 [cs.SD])

    [http://arxiv.org/abs/2310.08753](http://arxiv.org/abs/2310.08753)

    CompA提出了由两个专家注释的音频-语言模型组合推理基准数据集，用于评估ALMs在理解音频中声音事件的顺序和属性绑定方面的表现。

    

    音频的基本特性是其组合性。使用对比方法（例如CLAP）训练的音频-语言模型（ALMs）能够学习音频和语言模态之间的共享表示，从而在许多下游应用中提高性能，包括零样本音频分类、音频检索等。然而，这些模型在有效执行组合推理方面的能力还很少被探索，需要进一步的研究。本文提出了CompA，这是一个由两个专家注释的基准数据集，其中大多数是真实世界的音频样本，用于评估ALMs的组合推理能力。我们的CompA-order评估ALMs在理解音频中声音事件的顺序或发生时的表现如何，而CompA-attribute评估声音事件的属性绑定。每个基准数据集中的实例包含两个音频-标题对，其中两个音频具有相同的声音事件，但组合方式不同。

    A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose CompA, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. A
    
[^49]: Transformer语言模型中跨任务的电路组件复用

    Circuit Component Reuse Across Tasks in Transformer Language Models. (arXiv:2310.08744v1 [cs.CL])

    [http://arxiv.org/abs/2310.08744](http://arxiv.org/abs/2310.08744)

    这项工作证明了在Transformer语言模型中，电路组件可以在不同任务之间复用并产生相似的功能，为更高级的模型理解做出贡献。

    

    最近在机制可解释性方面的研究表明，通过电路分析可以成功地逆向工程语言模型的行为。然而，一个常见的批评是每个电路都是任务特定的，因此这样的分析不能为更高级的理解模型做出贡献。在这项工作中，我们提出证据表明洞察力（关于特定头部的低级发现和关于一般算法的高级发现）确实可以在任务之间进行泛化。具体而言，我们研究了Wang等人（2022）在间接宾语识别任务（IOI）中发现的电路，并展示了这个电路在更大的GPT2模型上的重现，以及在看似不同的任务中大部分被复用来解决问题：彩色物体（Ippolito和Callison-Burch，2023）。我们提供证据表明两个任务底层的过程在功能上非常相似，并且在电路中的注意力头部之间有大约78％的重叠。我们进一步展示了一个概念验证干预实验

    Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in Wang et al. (2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito & Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment
    
[^50]: 一种用于计算机控制的零样本语言代理机制及其结构反思

    A Zero-Shot Language Agent for Computer Control with Structured Reflection. (arXiv:2310.08740v1 [cs.CL])

    [http://arxiv.org/abs/2310.08740](http://arxiv.org/abs/2310.08740)

    这种论文提出了一种零样本语言代理机制，它不需要专家示踪，并且通过自我反思和结构化思考管理来学习和改善计算机上的控制，表现出高效的推理能力。

    

    大型语言模型（LLMs）在计划和执行高级目标方面的能力不断增强，如在活动的计算机环境（例如MiniWoB ++）中。最近的研究通常要求模型通过监督学习或少/多样本提示从任务的跟踪示例中学习。在没有这些跟踪示例的情况下，一个代理机制如何能够自主学习并改善在计算机上的控制仍然是一个挑战，这限制了一个代理机构执行新任务的能力。我们通过零样本代理机制来解决这个问题，它不需要给定的专家示踪。我们的代理机制对于部分观察环境上的可执行行动进行规划，并通过自我反思和结构化思考管理来识别和学习错误，从而逐步推进任务。在MiniWoB ++的简单任务中，我们展示了我们的零样本代理机制往往胜过最近的SoTA，具有更高效的推理能力。对于更复杂的任务，我们的反思代理机制与先前的代理机制持平。

    Large language models (LLMs) have shown increasing capacity at planning and executing a high-level goal in a live computer environment (e.g. MiniWoB++). To perform a task, recent works often require a model to learn from trace examples of the task via either supervised learning or few/many-shot prompting. Without these trace examples, it remains a challenge how an agent can autonomously learn and improve its control on a computer, which limits the ability of an agent to perform a new task. We approach this problem with a zero-shot agent that requires no given expert traces. Our agent plans for executable actions on a partially observed environment, and iteratively progresses a task by identifying and learning from its mistakes via self-reflection and structured thought management. On the easy tasks of MiniWoB++, we show that our zero-shot agent often outperforms recent SoTAs, with more efficient reasoning. For tasks with more complexity, our reflective agent performs on par with prior 
    
[^51]: 迈向语音单元和文本的联合语言建模

    Toward Joint Language Modeling for Speech Units and Text. (arXiv:2310.08715v1 [cs.CL])

    [http://arxiv.org/abs/2310.08715](http://arxiv.org/abs/2310.08715)

    本文研究了联合语言建模中语音单元和文本的混合方法，并通过口语理解任务评估了模型的性能，结果表明混合语言模型优于仅使用语音的基线模型。

    

    语音和文本是人类语言的两种主要形式。研究界多年来一直在关注将语音映射到文本或者反之亦然。然而，在语言建模领域，很少有人尝试联合建模这两者。基于此，我们探索了语音单元和文本的联合语言建模。具体而言，我们比较了不同的语音分词工具，将连续的语音信号转化为离散的单元，并使用不同的方法构建混合的语音-文本数据。我们引入了自动指标来评估联合语言建模的语音和文本融合效果。我们还使用不同的模态（语音或文本）在下游的口语理解任务上对联合语言模型进行了微调，并测试其性能以评估模型对共享表示的学习能力。我们的结果表明，在使用我们提出的混合技术将语音单元和文本混合的情况下，联合语言模型在口语理解任务上优于仅使用语音的基线模型，并展示了零-shot的跨模态迁移能力。

    Speech and text are two major forms of human language. The research community has been focusing on mapping speech to text or vice versa for many years. However, in the field of language modeling, very little effort has been made to model them jointly. In light of this, we explore joint language modeling for speech units and text. Specifically, we compare different speech tokenizers to transform continuous speech signals into discrete units and use different methods to construct mixed speech-text data. We introduce automatic metrics to evaluate how well the joint LM mixes speech and text. We also fine-tune the LM on downstream spoken language understanding (SLU) tasks with different modalities (speech or text) and test its performance to assess the model's learning of shared representations. Our results show that by mixing speech units and text with our proposed mixing techniques, the joint LM improves over a speech-only baseline on SLU tasks and shows zero-shot cross-modal transferabil
    
[^52]: GPT模型能成为金融分析师吗？对模拟CFA考试中的ChatGPT和GPT-4进行评估

    Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4 on mock CFA Exams. (arXiv:2310.08678v1 [cs.CL])

    [http://arxiv.org/abs/2310.08678](http://arxiv.org/abs/2310.08678)

    本研究评估了ChatGPT和GPT-4在金融分析上的能力，发现它们在模拟CFA考试中具有一定的表现，为将来进一步提升大型语言模型在金融推理方面的能力提供了启示。

    

    大型语言模型（LLM）在各种自然语言处理（NLP）任务中展现了出色的性能，通常能与甚至超越最先进的任务特定模型。本研究旨在评估LLM在金融推理能力方面的表现。我们利用特许金融分析师（CFA）考试的模拟题目对ChatGPT和GPT-4在金融分析中进行全面评估，考虑了零样本（ZS）、思路链（CoT）和少样本（FS）场景。我们对模型的性能和局限性进行了深入分析，并评估它们通过CFA考试的可能性。最后，我们提出了提高LLM在金融领域应用性的潜在策略和改进。在这个视角下，我们希望该研究为未来的研究继续通过严格评估来提升LLM在金融推理方面的能力铺平道路。

    Large Language Models (LLMs) have demonstrated remarkable performance on a wide range of Natural Language Processing (NLP) tasks, often matching or even beating state-of-the-art task-specific models. This study aims at assessing the financial reasoning capabilities of LLMs. We leverage mock exam questions of the Chartered Financial Analyst (CFA) Program to conduct a comprehensive evaluation of ChatGPT and GPT-4 in financial analysis, considering Zero-Shot (ZS), Chain-of-Thought (CoT), and Few-Shot (FS) scenarios. We present an in-depth analysis of the models' performance and limitations, and estimate whether they would have a chance at passing the CFA exams. Finally, we outline insights into potential strategies and improvements to enhance the applicability of LLMs in finance. In this perspective, we hope this work paves the way for future studies to continue enhancing LLMs for financial reasoning through rigorous evaluation.
    
[^53]: LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models

    LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v1 [cs.CL])

    [http://arxiv.org/abs/2310.08659](http://arxiv.org/abs/2310.08659)

    本论文提出了LoftQ：一种针对大型语言模型的LoRA精调感知量化框架。该框架同时对LLM进行量化，并为LoRA精调找到适当的低秩初始化，以缓解量化模型和全精度模型之间的差异，并显著提高了下游任务的泛化能力。

    

    量化是为大型语言模型提供服务的不可或缺的技术，并最近被应用于LoRA精调中。本文关注在预训练模型上同时应用量化和LoRA精调的场景。在这种情况下，常常观察到完整精调和量化加LoRA精调方法之间在下游任务表现上存在一致的差距。为了解决这个问题，我们提出了LoftQ（LoRA-Fine-Tuning-aware Quantization）——一种新的量化框架，用于同时对LLM进行量化，并找到适当的低秩初始化来进行LoRA精调。这种初始化减轻了量化模型和全精度模型之间的差异，并显著提高了下游任务的泛化能力。我们在自然语言理解、问答、摘要和自然语言生成任务上评估了我们的方法。实验证明，我们的方法非常有效，在性能上优于现有的方法。

    Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms exis
    
[^54]: 我们能编辑多模式大型语言模型吗？

    Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.08475](http://arxiv.org/abs/2310.08475)

    本文提出了编辑多模式大型语言模型（MLLMs）的挑战，并构建了一个新的基准用于评估和比较不同编辑方法的效果。实验结果表明，编辑多模式LLMs仍然存在困难，但这项工作为NLP社区提供了宝贵的见解。

    

    本文关注编辑多模式大型语言模型（MLLMs）。与编辑单模式LLMs相比，多模式模型的编辑更具挑战性，需要更高级别的审查和慎重考虑。为了促进这一领域的研究，我们构建了一个新的基准，称为MMEdit，用于编辑多模式LLMs，并建立了一套创新的度量标准进行评估。我们进行了包括各种模型编辑基线的综合实验，并分析了编辑多模式LLMs的不同组件的影响。根据经验，我们发现之前的基线在某种程度上可以实现编辑多模式LLMs，但效果仍然不理想，表明这个任务可能存在的困难。我们希望我们的工作能为NLP社区提供见解。代码和数据集可在https://github.com/zjunlp/EasyEdit获取。

    In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.
    
[^55]: 从训练模型生成的合成数据中训练生成式问答系统的方法

    Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Mo. (arXiv:2310.08072v1 [cs.CL])

    [http://arxiv.org/abs/2310.08072](http://arxiv.org/abs/2310.08072)

    本文提出了一种在非英语语言中训练问答系统的简单且经济的方法，使用调整指导模型生成合成数据，避免了人力成本，并展示了合成数据训练的模型与手动整理的数据集训练的模型在性能上的可比较性。

    

    本文提出了一种简单且经济的方法来合成数据以训练问答系统。在资源充足的英语等语言中，调整GPT模型是一种常见的训练方法，但是在非英语语言中由于缺乏足够的问答对，这变得具有挑战性。现有的方法使用在人类作者的问答对上训练的问答生成模型，这涉及相当大的人力成本。相反，我们使用一个调整指导模型以零样本或少量样本的方式生成问答对。我们进行了实验，比较了从指导模型获取问答对的各种策略。结果表明，使用我们提出的合成数据训练的模型在性能上与在手动整理的数据集上训练的模型相当，而无需承担人力成本。

    This paper presents a simple and cost-effective method for synthesizing data to train question-answering systems. For training, fine-tuning GPT models is a common practice in resource-rich languages like English, however, it becomes challenging for non-English languages due to the scarcity of sufficient question-answer (QA) pairs. Existing approaches use question and answer generators trained on human-authored QA pairs, which involves substantial human expenses. In contrast, we use an instruct-tuned model to generate QA pairs in a zero-shot or few-shot manner. We conduct experiments to compare various strategies for obtaining QA pairs from the instruct-tuned model. The results demonstrate that a model trained on our proposed synthetic data achieves comparable performance to a model trained on manually curated datasets, without incurring human costs.
    
[^56]: 使用大型语言模型生成合成数据用于文本分类：潜力和限制

    Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations. (arXiv:2310.07849v1 [cs.CL])

    [http://arxiv.org/abs/2310.07849](http://arxiv.org/abs/2310.07849)

    本研究旨在探讨使用大型语言模型生成合成数据在文本分类模型训练中的潜力和限制。研究结果发现，主观性会负面影响模型在合成数据上的性能。这对于理解和利用合成数据的有效性具有重要的启示作用。

    

    收集和整理高质量的训练数据对于开发具有卓越性能的文本分类模型至关重要，但往往伴随着巨大的成本和时间投入。研究人员最近开始探索使用大型语言模型（LLMs）生成合成数据集作为一种替代方法。然而，LLM生成的合成数据在支持模型训练方面的有效性在不同的分类任务中是不一致的。为了更好地了解调节LLM生成的合成数据有效性的因素，本研究探讨了在分类的主观性如何影响在合成数据上训练的模型的性能。我们的研究结果表明，主观性在任务层面和实例层面上都与在合成数据上训练的模型的性能呈负相关。最后，我们讨论了我们的工作对于利用LLM来生成合成数据在潜力和限制方面的影响。

    The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM fo
    
[^57]: 面向目标的个性化主动对话系统：问题形式化与数据集筛选

    Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation. (arXiv:2310.07397v1 [cs.CL])

    [http://arxiv.org/abs/2310.07397](http://arxiv.org/abs/2310.07397)

    这项工作提出了一个自动数据集筛选框架，并构建了一个大规模的个性化面向目标对话数据集，名为TopDial。该数据集质量高，有助于探索个性化目标导向的对话。

    

    面向目标的对话系统旨在主动引导对话朝向预定的目标或达成特定的系统目标，在对话完成过程中考虑个性化。然而，需要高质量的数据集，从零开始构建需要大量人力。为了解决这个问题，我们提出了一个使用角色扮演方法的自动数据集筛选框架。基于这个框架，我们构建了一个大规模的个性化面向目标对话数据集，名为TopDial，包含约18K个多轮对话。实验结果表明，该数据集具有很高的质量，可以用于探索个性化目标导向的对话。

    Target-oriented dialogue systems, designed to proactively steer conversations toward predefined targets or accomplish specific system-side goals, are an exciting area in conversational AI. In this work, by formulating a <dialogue act, topic> pair as the conversation target, we explore a novel problem of personalized target-oriented dialogue by considering personalization during the target accomplishment process. However, there remains an emergent need for high-quality datasets, and building one from scratch requires tremendous human effort. To address this, we propose an automatic dataset curation framework using a role-playing approach. Based on this framework, we construct a large-scale personalized target-oriented dialogue dataset, TopDial, which comprises about 18K multi-turn dialogues. The experimental results show that this dataset is of high quality and could contribute to exploring personalized target-oriented dialogue.
    
[^58]: 关于交叉领域数据对德语语言模型的影响

    On the Impact of Cross-Domain Data on German Language Models. (arXiv:2310.07321v1 [cs.CL])

    [http://arxiv.org/abs/2310.07321](http://arxiv.org/abs/2310.07321)

    本研究通过对德语语言模型进行实验，发现将数据多样性置于数据质量之上的交叉领域数据集训练方法，可以显著提高模型的性能，并超过了之前的最先进模型。

    

    传统上，大型语言模型要么在通用网络抓取数据上训练，要么在特定领域的数据上。然而，生成型大型语言模型的最近成功突显了交叉领域数据集的好处。为了考察数据多样性高于质量的重要性，我们提出了一个包含五个领域文本的德语数据集，以及一个旨在包含高质量数据的数据集。通过在这两个数据集上训练参数范围从122M到750M的一系列模型，我们对多个下游任务进行了全面评估。我们的研究结果表明，使用交叉领域数据集训练的模型优于仅使用质量数据训练的模型，在先前最先进结果上提出了高达4.45%的改进。这些模型可在https://huggingface.co/ikim-uk-essen上找到。

    Traditionally, large language models have been either trained on general web crawls or domain-specific data. However, recent successes of generative large language models, have shed light on the benefits of cross-domain datasets. To examine the significance of prioritizing data diversity over quality, we present a German dataset comprising texts from five domains, along with another dataset aimed at containing high-quality data. Through training a series of models ranging between 122M and 750M parameters on both datasets, we conduct a comprehensive benchmark on multiple downstream tasks. Our findings demonstrate that the models trained on the cross-domain dataset outperform those trained on quality data alone, leading to improvements up to $4.45\%$ over the previous state-of-the-art. The models are available at https://huggingface.co/ikim-uk-essen
    
[^59]: 大型语言模型稀疏微调的推理加速

    Sparse Finetuning for Inference Acceleration of Large Language Models. (arXiv:2310.06927v1 [cs.CL])

    [http://arxiv.org/abs/2310.06927](http://arxiv.org/abs/2310.06927)

    本论文研究了大型语言模型的准确稀疏微调问题，提出了基于L2范数的蒸馏方法SquareHead，可以在高稀疏性下实现准确的恢复；同时展示了稀疏语言模型的实际效率，可在CPU和GPU运行时实现加速，并且观察到在受内存限制的模型中，稀疏性也可用于减少内存带宽。

    

    我们考虑在训练过的大型语言模型上进行精确的稀疏微调，即在专门任务上对预训练的语言模型进行微调，同时在权重上引入稀疏性。在准确性方面，我们观察到基于损失的标准微调可能无法恢复准确性，特别是在高稀疏情况下。为了解决这个问题，我们对蒸馏类型的损失进行了详细研究，确定了一种基于L2范数的蒸馏方法，我们称之为SquareHead，即使在更高的稀疏性下，它也能实现准确的恢复，适用于所有模型类型。在实际效率方面，我们展示了稀疏语言模型可以通过利用稀疏性在CPU和GPU运行时实现加速。虽然标准方法是利用稀疏性进行计算减少，但我们观察到，在受内存限制的语言模型中，稀疏性也可以用于减少内存带宽。我们展示了由于稀疏性导致的速度提升以及恢复准确性的端到端结果，应用于T5 (语言翻译)任务中。

    We consider the problem of accurate sparse finetuning of large language models (LLMs), that is, finetuning pretrained LLMs on specialized tasks, while inducing sparsity in their weights. On the accuracy side, we observe that standard loss-based finetuning may fail to recover accuracy, especially at high sparsities. To address this, we perform a detailed study of distillation-type losses, determining an L2-based distillation approach we term SquareHead which enables accurate recovery even at higher sparsities, across all model types. On the practical efficiency side, we show that sparse LLMs can be executed with speedups by taking advantage of sparsity, for both CPU and GPU runtimes. While the standard approach is to leverage sparsity for computational reduction, we observe that in the case of memory-bound LLMs sparsity can also be leveraged for reducing memory bandwidth. We exhibit end-to-end results showing speedups due to sparsity, while recovering accuracy, on T5 (language translati
    
[^60]: 语言模型是否能够学习类比推理？研究训练目标和与人类表现的比较。

    Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance. (arXiv:2310.05597v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05597](http://arxiv.org/abs/2310.05597)

    本文研究了语言模型是否能够学习类比推理的任务，并测试了几种学习方法。实验结果表明，模型能够通过少量数据学习类比推理，并在与人类基准进行比较后接近人类的表现水平。

    

    虽然类比是评估自然语言处理中词嵌入的常见方式，但研究类比推理是否是一种可以学习的任务也很有意义。本文测试了几种学习基本类比推理的方法，特别关注的是那些更符合人类类比推理评估标准的类比。我们的实验发现，模型能够在少量数据的情况下学习类比推理。此外，我们还将我们的模型与具有人类基准的数据集进行比较，并发现在训练后，模型接近人类的表现水平。

    While analogies are a common way to evaluate word embeddings in NLP, it is also of interest to investigate whether or not analogical reasoning is a task in itself that can be learned. In this paper, we test several ways to learn basic analogical reasoning, specifically focusing on analogies that are more typical of what is used to evaluate analogical reasoning in humans than those in commonly used NLP benchmarks. Our experiments find that models are able to learn analogical reasoning, even with a small amount of data. We additionally compare our models to a dataset with a human baseline, and find that after training, models approach human performance.
    
[^61]: 通过迭代地融合模态相似路径实现通用的多模态实体对齐

    Universal Multi-modal Entity Alignment via Iteratively Fusing Modality Similarity Paths. (arXiv:2310.05364v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05364](http://arxiv.org/abs/2310.05364)

    本研究提出了一种通过迭代融合模态相似路径实现通用的多模态实体对齐方法。通过统一建模和有效信息融合，解决了现有方法中模态建模不一致和低效以及模态融合效果不佳的问题。

    

    实体对齐的目标是从多个知识图谱中确定等价的实体对，并创建一个更全面和统一的知识图谱。大多数实体对齐方法主要关注知识图谱的结构模态，缺乏对多模态信息的探索。少数多模态实体对齐方法在这个领域做出了不错的尝试。然而，它们存在两个缺点：(1)模态建模不一致且低效，为每个模态设计复杂和独立的模型；(2)由于实体对齐中模态的异构性，模态融合效果不佳。为了解决这些挑战，我们提出了PathFusion，它包括两个主要部分：(1) MSP，一个统一的建模方法，通过构建连接实体和模态节点以表示多个模态的路径，简化了对齐过程；(2) IRF，一种迭代融合方法，使用路径作为信息载体，有效地将不同模态的信息结合起来。

    The objective of Entity Alignment (EA) is to identify equivalent entity pairs from multiple Knowledge Graphs (KGs) and create a more comprehensive and unified KG. The majority of EA methods have primarily focused on the structural modality of KGs, lacking exploration of multi-modal information. A few multi-modal EA methods have made good attempts in this field. Still, they have two shortcomings: (1) inconsistent and inefficient modality modeling that designs complex and distinct models for each modality; (2) ineffective modality fusion due to the heterogeneous nature of modalities in EA. To tackle these challenges, we propose PathFusion, consisting of two main components: (1) MSP, a unified modeling approach that simplifies the alignment process by constructing paths connecting entities and modality nodes to represent multiple modalities; (2) IRF, an iterative fusion method that effectively combines information from different modalities using the path as an information carrier. Experim
    
[^62]: 个性化随机鹦鹉更危险吗？评估对话系统中的人格偏见

    Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05280](http://arxiv.org/abs/2310.05280)

    这项研究评估了对话系统中的人格偏见对社交偏见的影响，并建立了一个综合评估框架来衡量不同人格采用下的偏见程度。

    

    最近大型语言模型的发展使其能够按照自由形式的指令进行操作，包括在对话中模仿通用或特定人口群体的人格。通用人格指的是来自某一人口群体的个体（例如亚洲人），而特定人格可以是历史人物的实际姓名。虽然采用人格使对话系统更具吸引力和亲和力，但也存在潜在风险，可能通过与用户的交互而加剧社会偏见，进一步造成社会伤害。在本文中，我们系统地研究“人格偏见”，我们将其定义为有害对话模型行为对不同人格采用的敏感性。我们将人格偏见分为有害表达和有害认同两类，同时建立了一个全面的评估框架，以衡量五个方面的人格偏见：冒犯性、有毒延续、关怀、刻板印象的认同以及

    Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. Generic personas refer to an individual from a demographic group (e.g. an Asian person), whereas specific personas can be actual names of historical figures. While the adoption of personas allows dialogue systems to be more engaging and approachable to users, it also carries the potential risk of exacerbating social biases in model responses, further causing societal harms through interactions with users. In this paper, we systematically study "persona biases", which we define to be the sensitivity of harmful dialogue model behaviors to different persona adoptions. We categorize persona biases into biases in harmful expression and harmful agreement, as well as establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and To
    
[^63]: Text2NKG: 面向N元关系知识图构建的细粒度N元关系抽取

    Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational Knowledge Graph Construction. (arXiv:2310.05185v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.05185](http://arxiv.org/abs/2310.05185)

    Text2NKG是一种用于构建N元关系知识图的细粒度N元关系抽取框架，支持多种NKG模式，具有高灵活性和实用性。

    

    除了传统的二元关系事实外，N元关系知识图(NKGs)由包含两个以上实体的N元关系事实组成，更接近于具有广泛应用的真实世界事实。然而，NKG的构建仍然严重依赖于人工劳动，并且N元关系抽取仍然停留在粗粒度水平，通常是在单一模式和固定的实体数量上操作。为了解决这些限制，我们提出了Text2NKG，一种新颖的面向N元关系知识图构建的细粒度N元关系抽取框架。我们引入了一种跨度元组分类方法，并采用异构排序合并来实现不同度的细粒度N元关系抽取。此外，Text2NKG支持四种典型的NKG模式：超关系模式、基于事件的模式、基于角色的模式和超图模式，具有较高的灵活性和实用性。实验结果表明，Text2NKG的表现优于传统的N元关系抽取方法。

    Beyond traditional binary relational facts, n-ary relational knowledge graphs (NKGs) are comprised of n-ary relational facts containing more than two entities, which are closer to real-world facts with broader applications. However, the construction of NKGs still significantly relies on manual labor, and n-ary relation extraction still remains at a course-grained level, which is always in a single schema and fixed arity of entities. To address these restrictions, we propose Text2NKG, a novel fine-grained n-ary relation extraction framework for n-ary relational knowledge graph construction. We introduce a span-tuple classification approach with hetero-ordered merging to accomplish fine-grained n-ary relation extraction in different arity. Furthermore, Text2NKG supports four typical NKG schemas: hyper-relational schema, event-based schema, role-based schema, and hypergraph-based schema, with high flexibility and practicality. Experimental results demonstrate that Text2NKG outperforms the
    
[^64]: 使用大型语言模型（LLMS）对图中的节点进行无标签分类

    Label-free Node Classification on Graphs with Large Language Models (LLMS). (arXiv:2310.04668v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04668](http://arxiv.org/abs/2310.04668)

    本文介绍了一种使用大型语言模型（LLMs）对图中节点进行无标签分类的方法，即LLM-GNN。它利用LLMs对一小部分节点进行注释，然后通过对LLMs的注释进行训练，使得GNN能够对其余大部分节点进行预测。这种方法充分发挥了GNNs和LLMs的优势，同时解决了它们在处理结构化数据方面的限制。

    

    近年来，图神经网络（Graph Neural Networks，GNNs）在节点分类方面取得了显著的进展。然而，为了确保良好的性能，它们需要大量高质量的标签。相比之下，大型语言模型（Large Language Models，LLMs）在文本属性图上展现出了令人印象深刻的零样学习能力。然而，它们在高效处理结构化数据方面面临挑战，并且推理成本较高。鉴于这些观察结果，本文引入了一种基于LLMs的无标签图节点分类方法，命名为LLM-GNN。它集成了GNNs和LLMs的优势，同时减轻了它们的限制。具体而言，LLMs被用来注释一小部分节点，然后通过对LLMs的注释进行训练，使GNNs能够预测其余大部分节点。LLM-GNN的实现面临一个独特的挑战：我们如何主动选择要由LLMs注释的节点，从而增强GNN的训练？我们如何利用LLMs来优化结构化数据的处理？

    In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to ob
    
[^65]: 人类移动问题回答（展望论文）

    Human Mobility Question Answering (Vision Paper). (arXiv:2310.04443v1 [cs.CL])

    [http://arxiv.org/abs/2310.04443](http://arxiv.org/abs/2310.04443)

    本文提出了一项新的任务，即人类移动问题回答（MobQA），旨在让智能系统从移动数据中学习并回答相关问题，填补了关于利用人类移动数据进行问题回答系统的研究空白，并为移动推荐系统的研究带来了新的范式变革。

    

    问答系统已经引起了人工智能界的广泛关注，因为它们可以根据给定的知识源（例如视觉问答中的图像）学习回答问题。然而，关于利用人类移动数据进行问题回答系统的研究尚未被探索。挖掘人类移动数据对于智能城市规划、疫情管理和个性化推荐系统等各种应用至关重要。本文旨在填补这一空白，引入一项新的任务，即人类移动问题回答（MobQA）。该任务旨在让智能系统从移动数据中学习并回答相关问题。该任务为移动预测研究带来了新的范式变革，并进一步促进了人类移动推荐系统的研究。为了更好地支持这个新的研究课题，这篇展望论文还提出了一个数据集的初步设计和一个潜在的深度学习模型。

    Question answering (QA) systems have attracted much attention from the artificial intelligence community as they can learn to answer questions based on the given knowledge source (e.g., images in visual question answering). However, the research into question answering systems with human mobility data remains unexplored. Mining human mobility data is crucial for various applications such as smart city planning, pandemic management, and personalised recommendation system. In this paper, we aim to tackle this gap and introduce a novel task, that is, human mobility question answering (MobQA). The aim of the task is to let the intelligent system learn from mobility data and answer related questions. This task presents a new paradigm change in mobility prediction research and further facilitates the research of human mobility recommendation systems. To better support this novel research topic, this vision paper also proposes an initial design of the dataset and a potential deep learning mod
    
[^66]: 把大型语言模型的领域适应重新表述为适应-检索-修订

    Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise. (arXiv:2310.03328v1 [cs.CL])

    [http://arxiv.org/abs/2310.03328](http://arxiv.org/abs/2310.03328)

    本文介绍了一个简单而有效的GPT-4领域适应框架，通过将生成过程重新表述为一个“适应-检索-修订”的过程，解决了大型语言模型在特定领域生成内容错误的问题。

    

    尽管像GPT-4这样的大型语言模型最近在一般领域任务上展示出令人惊讶的零-shot能力，但它们常常在特定领域（如中国法律）生成错误的内容，从而阻碍了它们在这些领域的应用。这通常是由于没有包含这样一个特定领域的训练数据，使得GPT-4无法获取领域内的知识。一个紧迫的挑战是在领域内数据上继续训练如此大规模的LLM是不可行的。本文通过将生成过程重新表述为一个“适应-检索-修订”的过程，介绍了一个简单而有效的GPT-4领域适应框架。初始步骤是通过在领域内数据上继续学习，将一个经济实惠的7B LLM适应到目标领域。解决任务时，我们利用适应的LLM根据任务查询生成一个初稿答案。然后，初稿答案将用于从外部检索支持证据的候选项。

    While large language models (LLMs) like GPT-4 have recently demonstrated astonishing zero-shot capabilities in general domain tasks, they often generate content with hallucinations in specific domains such as Chinese law, hindering their application in these areas. This is typically due to the absence of training data that encompasses such a specific domain, preventing GPT-4 from acquiring in-domain knowledge. A pressing challenge is that it's not plausible to continue training LLMs of such scale on in-domain data.  This paper introduces a simple and effective domain adaptation framework for GPT-4 by reformulating generation as an \textbf{adapt-retrieve-revise} process. The initial step is to \textbf{adapt} an affordable 7B LLM to the target domain by continuing learning on in-domain data. When solving a task, we leverage the adapted LLM to generate a draft answer given a task query. Then, the draft answer will be used to \textbf{retrieve} supporting evidence candidates from an externa
    
[^67]: LanguageMPC：基于大型语言模型的自动驾驶决策者

    LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving. (arXiv:2310.03026v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2310.03026](http://arxiv.org/abs/2310.03026)

    本文研究将大型语言模型（LLMs）作为复杂自动驾驶场景的决策组件，通过认知路径和算法来实现全面推理和可执行驾驶指令的转化。实验证明，LLMs能够在单车任务和复杂驾驶行为中表现出优越性能，这是因为其具有常识推理能力。

    

    现有基于学习的自动驾驶系统在理解高级信息、推广罕见事件和提供可解释性方面面临挑战。为解决这些问题，本研究将大型语言模型（LLMs）作为复杂自动驾驶场景的决策组件，需要人类常识理解。我们设计了认知路径，使LLMs能够进行全面推理，并开发了将LLM决策转化为可执行驾驶指令的算法。通过这种方式，LLM决策通过引导参数矩阵适应与低级控制器无缝集成。大量实验表明，我们提出的方法不仅在单车任务中始终超越基线方法，而且还能处理复杂的驾驶行为，甚至多车协调，这要归功于LLMs的常识推理能力。本文介绍了将LLMs作为有效决策者的初步步骤。

    Existing learning-based autonomous driving (AD) systems face challenges in comprehending high-level information, generalizing to rare events, and providing interpretability. To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding. We devise cognitive pathways to enable comprehensive reasoning with LLMs, and develop algorithms for translating LLM decisions into actionable driving commands. Through this approach, LLM decisions are seamlessly integrated with low-level controllers by guided parameter matrix adaptation. Extensive experiments demonstrate that our proposed method not only consistently surpasses baseline approaches in single-vehicle tasks, but also helps handle complex driving behaviors even multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. This paper presents an initial step toward leveraging LLMs as effective decision-make
    
[^68]: 一个增强的 UMLS 框架，用于改善大型语言模型在医疗保健中的事实性

    A UMLS-Augmented Framework for Improving Factuality in Large Language Models within Healthcare. (arXiv:2310.02778v1 [cs.CL])

    [http://arxiv.org/abs/2310.02778](http://arxiv.org/abs/2310.02778)

    该论文提出了一个基于UMLS的增强型大型语言模型框架，旨在改善医疗保健领域中模型生成内容的事实性。通过自动评估和医生评估，研究人员验证了该框架的有效性。

    

    大型语言模型（LLM）展示了强大的文本生成能力，为医疗保健领域带来了前所未有的创新。然而，将LLMs应用于真实临床场景面临重大挑战，因为这些模型可能生成与已建立医学事实偏离的内容，甚至可能表现出潜在的偏见。在我们的研究中，我们开发了一个基于统一医学语言系统（UMLS）的增强型LLM框架，旨在更好地服务医疗保健社区。我们采用LLaMa2-13b-chat和ChatGPT-3.5作为基准模型，并使用ROUGE分数和BERT分数在LiveQA测试集的104个问题上进行自动评估。此外，我们根据事实性、完整性、可读性和相关性四个维度建立了医生评估标准。ChatGPT-3.5用于医生评估，针对LiveQA测试集的20个问题。多位住院医师进行评估。

    Large language models (LLMs) have demonstrated powerful text generation capabilities, bringing unprecedented innovation to the healthcare field. While LLMs hold immense promise for applications in healthcare, applying them to real clinical scenarios presents significant challenges, as these models may generate content that deviates from established medical facts and even exhibit potential biases. In our research, we develop an augmented LLM framework based on the Unified Medical Language System (UMLS), aiming to better serve the healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our benchmark models, and conduct automatic evaluations using the ROUGE Score and BERTScore on 104 questions from the LiveQA test set. Additionally, we establish criteria for physician-evaluation based on four dimensions: Factuality, Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician evaluation with 20 questions on the LiveQA test set. Multiple resident physicians conduct
    
[^69]: 通过多模态大型语言模型实现端到端的具身决策

    Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond. (arXiv:2310.02071v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.02071](http://arxiv.org/abs/2310.02071)

    本研究通过探索多模态大型语言模型在代理的具身决策中的应用潜力，提出了一个新的评估基准PCA-EVAL，并引入了一个多代理协作框架HOLMES，以提高决策能力。研究发现GPT4-Vision模型在端到端的具身决策中表现最佳。

    

    本研究探索了多模态大型语言模型（MLLMs）在改进代理的具身决策过程中的潜力。尽管由于其先进的推理能力和广泛的世界知识，大型语言模型（LLMs）被广泛使用，但像GPT4-Vision这样的MLLM提供了增强的视觉理解和推理能力。我们研究了最先进的MLLMs能否以端到端的方式处理具身决策，并且LLMs和MLLMs之间的协作是否能增强决策能力。为了回答这些问题，我们引入了一个名为PCA-EVAL的新基准，该基准从感知、认知和行动的角度评估具身决策。此外，我们提出了HOLMES，一个多代理协作框架，允许LLMs利用MLLMs和APIs获取多模态信息以进行明智的决策。我们在我们的基准上比较了端到端的具身决策和HOLMES，并发现GPT4-Vision模型的性能最优。

    In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model 
    
[^70]: 在基于图像的播放列表描述和音乐主题中提高情感表达和凝聚力的连续参数化方法

    Improving Emotional Expression and Cohesion in Image-Based Playlist Description and Music Topics: A Continuous Parameterization Approach. (arXiv:2310.01248v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.01248](http://arxiv.org/abs/2310.01248)

    本文介绍了一种用于基于图像的播放列表描述和音乐主题中的文本生成的改进方法，该方法通过连续参数化实现对文本风格和情感表达的精确控制，并在生成文本相关性和连贯性方面取得了显著改进。

    

    在基于图像的平台中生成文本，特别是与音乐相关的内容，需要对文本样式进行精确控制并融入情感表达。然而，现有的方法往往需要控制生成文本中外部因素的比例，并且依赖于离散输入，缺乏对期望文本生成的连续控制条件。本研究提出了一种用于受控文本生成的连续参数化方法（CPCTG）来克服这些限制。我们的方法利用语言模型（LM）作为样式学习器，集成了语义凝聚度（SC）和情感表达比例（EEP）的考虑。通过改进奖励方法和操作CPCTG水平，我们在播放列表描述和音乐主题生成任务上的实验证明了ROUGE分数的显著提高，表明生成的文本在相关性和连贯性上得到了增强。

    Text generation in image-based platforms, particularly for music-related content, requires precise control over text styles and the incorporation of emotional expression. However, existing approaches often need help to control the proportion of external factors in generated text and rely on discrete inputs, lacking continuous control conditions for desired text generation. This study proposes Continuous Parameterization for Controlled Text Generation (CPCTG) to overcome these limitations. Our approach leverages a Language Model (LM) as a style learner, integrating Semantic Cohesion (SC) and Emotional Expression Proportion (EEP) considerations. By enhancing the reward method and manipulating the CPCTG level, our experiments on playlist description and music topic generation tasks demonstrate significant improvements in ROUGE scores, indicating enhanced relevance and coherence in the generated text.
    
[^71]: EchoPrompt：指导模型重新表述查询以改善上下文学习

    EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context Learning. (arXiv:2309.10687v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.10687](http://arxiv.org/abs/2309.10687)

    EchoPrompt是一种简单而有效的方法，通过促使模型重新表述查询来提供改进的上下文学习效果。实验证明，EchoPrompt在多个任务中都取得了显著的性能提升。

    

    通过积极采用推断时提示技术，如零-shot和少-shot提示技术，语言模型在各种任务上取得了令人印象深刻的性能。在这项工作中，我们介绍了一种称为EchoPrompt的简单而有效的方法，该方法提示模型在回答问题之前重新表述查询。EchoPrompt适用于标准和思维链提示的零-shot和少-shot上下文学习。实验结果表明，EchoPrompt在这四个因果语言模型族群的所有设置中都取得了显著改进。这些改进观察到了各种数值推理（例如，GSM8K，SVAMP）、阅读理解（例如DROP）和逻辑推理（例如Coin Flipping）任务中。平均而言，EchoPrompt提高了数值任务中code-davinci-002的零-shot-CoT性能5%，阅读理解任务中提高了13%。我们通过消融研究研究了影响EchoPrompt有效性的因素，其中包括...

    Language models are achieving impressive performance on various tasks by aggressively adopting inference-time prompting techniques, such as zero-shot and few-shot prompting. In this work, we introduce EchoPrompt, a simple yet effective approach that prompts the model to rephrase its queries before answering them. EchoPrompt is adapted for both zero-shot and few-shot in-context learning with standard and chain-of-thought prompting. Experimental results show that EchoPrompt yields substantial improvements across all these settings for four families of causal language models. These improvements are observed across various numerical reasoning (e.g. GSM8K, SVAMP), reading comprehension (e.g. DROP), and logical reasoning (e.g. Coin Flipping) tasks. On average, EchoPrompt improves the Zero-shot-CoT performance of code-davinci-002 by 5% in numerical tasks and 13% in reading comprehension tasks. We investigate the factors contributing to EchoPrompt's effectiveness through ablation studies, whic
    
[^72]: MusiLingo：利用预训练的语言模型将音乐和文本相结合，实现音乐字幕和查询响应

    MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response. (arXiv:2309.08730v1 [eess.AS])

    [http://arxiv.org/abs/2309.08730](http://arxiv.org/abs/2309.08730)

    MusiLingo是一个利用预训练的语言模型将音乐和文本相结合的系统，可以生成音乐字幕和回答音乐相关的查询。通过使用投影层对齐音乐表示，该系统成功地将音乐音频和文本环境联系起来，同时使用了一个新的数据集来推动领域的进展。

    

    大型语言模型（LLM）已经在多模态应用中展现出巨大潜力，然而文本和音乐领域的融合仍相对未被探索。为了解决这一问题，我们提出了MusiLingo，这是一个用于音乐字幕生成和音乐相关查询响应的新系统。MusiLingo使用一个投影层来对齐预训练的冻结音乐音频模型MERT和冻结的LLaMA语言模型的音乐表示，实现音乐音频和文本环境之间的桥梁。我们在一个大规模的音乐字幕数据集上进行训练，并使用指导性数据进行微调。由于高质量的音乐问答数据集稀缺，我们从MusicCaps创建了MusicInstruct（MI）数据集，专为开放式音乐查询而设计。实证评估证明了它在生成音乐字幕和组织音乐相关问答对方面的竞争性表现。我们引入的数据集在之前的数据集的基础上取得了显著进展。

    Large Language Models (LLMs) have shown immense potential in multimodal applications, yet the convergence of textual and musical domains remains relatively unexplored. To address this gap, we present MusiLingo, a novel system for music caption generation and music-related query responses. MusiLingo employs a single projection layer to align music representations from the pre-trained frozen music audio model MERT with the frozen LLaMA language model, bridging the gap between music audio and textual contexts. We train it on an extensive music caption dataset and fine-tune it with instructional data. Due to the scarcity of high-quality music Q&A datasets, we created the MusicInstruct (MI) dataset from MusicCaps, tailored for open-ended music inquiries. Empirical evaluations demonstrate its competitive performance in generating music captions and composing music-related Q&A pairs. Our introduced dataset enables notable advancements beyond previous ones.
    
[^73]: DictaBERT: 一款用于现代希伯来语的最先进BERT套件的翻译标题

    DictaBERT: A State-of-the-Art BERT Suite for Modern Hebrew. (arXiv:2308.16687v1 [cs.CL])

    [http://arxiv.org/abs/2308.16687](http://arxiv.org/abs/2308.16687)

    DictaBERT是一种最先进的预训练BERT模型，针对现代希伯来语，在大多数基准测试中表现优于其他模型。它还提供了两个经过微调的模型版本，可用于希伯来语文本分析中的前缀分割和形态标注任务。这些模型的发布旨在促进希伯来语自然语言处理的研究和发展。

    

    我们提出了DictaBERT，这是一种用于现代希伯来语的最先进的预训练BERT模型，在大多数基准测试中表现优于现有模型。此外，我们发布了两个经过微调的模型版本，旨在执行希伯来语文本分析的两个特定基本任务：前缀分割和形态标注。这些经过微调的模型允许任何开发人员只需调用HuggingFace模型一次即可对希伯来语句子进行前缀分割和形态标注，无需集成任何额外的库或代码。在本文中，我们描述了训练的细节以及在不同基准测试上的结果。我们将这些模型与展示其使用的示例代码一起发布给社区。我们发布这些模型是为了帮助进一步促进希伯来语自然语言处理的研究和发展。

    We present DictaBERT, a new state-of-the-art pre-trained BERT model for modern Hebrew, outperforming existing models on most benchmarks. Additionally, we release two fine-tuned versions of the model, designed to perform two specific foundational tasks in the analysis of Hebrew texts: prefix segmentation and morphological tagging. These fine-tuned models allow any developer to perform prefix segmentation and morphological tagging of a Hebrew sentence with a single call to a HuggingFace model, without the need to integrate any additional libraries or code. In this paper we describe the details of the training as well and the results on the different benchmarks. We release the models to the community, along with sample code demonstrating their use. We release these models as part of our goal to help further research and development in Hebrew NLP.
    
[^74]: Qwen-VL: 一种具有多功能能力的前沿大规模视觉-语言模型

    Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities. (arXiv:2308.12966v1 [cs.CV])

    [http://arxiv.org/abs/2308.12966](http://arxiv.org/abs/2308.12966)

    Qwen-VL是一种具有多功能能力的前沿大规模视觉-语言模型，它在图像字幕生成、问题回答、视觉定位和灵活交互等任务中表现出卓越性能，优于现有的大规模视觉-语言模型。它在推动多模态人工智能方面做出了重要贡献。

    

    我们引入了一系列名为Qwen-VL的大规模视觉-语言模型，旨在感知和理解文本和图像。包括Qwen-VL和Qwen-VL-Chat，这些模型在图像字幕生成、问题回答、视觉定位和灵活交互等任务中表现出卓越的性能。评估范围涵盖了零样本字幕生成、视觉或文档视觉问题回答和 grounding 等各种任务。我们证明了Qwen-VL比现有的大规模视觉-语言模型（LVLMs）表现更优异。我们展示了它们的架构、训练方法、能力和性能，并突出了它们在推动多模态人工智能方面的贡献。代码、演示和模型可以在https://github.com/QwenLM/Qwen-VL找到。

    We introduce the Qwen-VL series, a set of large-scale vision-language models designed to perceive and understand both text and images. Comprising Qwen-VL and Qwen-VL-Chat, these models exhibit remarkable performance in tasks like image captioning, question answering, visual localization, and flexible interaction. The evaluation covers a wide range of tasks including zero-shot captioning, visual or document visual question answering, and grounding. We demonstrate the Qwen-VL outperforms existing Large Vision Language Models (LVLMs). We present their architecture, training, capabilities, and performance, highlighting their contributions to advancing multimodal artificial intelligence. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.
    
[^75]: 使用理由生成和密集检索回答未知问题的较小语言模型

    Answering Unseen Questions With Smaller Language\\Models Using Rationale Generation and Dense Retrieval. (arXiv:2308.04711v1 [cs.CL])

    [http://arxiv.org/abs/2308.04711](http://arxiv.org/abs/2308.04711)

    本论文提出了两种方法来改进在具有充分解释性背景下，使用较小语言模型回答训练中未见的挑战性短问题回答任务。第一种方法是使用理据生成和密集检索结合的方式，并通过理据排名模型进行评分和组合。第二种方法是使用增强检索训练数据集训练较小的推理模型，以利用长文本序列中的相关信息。

    

    在提供足够的解释性背景的情况下，已经证明较小的语言模型在挑战性的无法在训练中见过的短问题回答任务上展现出强大的推理能力。我们评估了两种进一步改进该场景的方法。这两种方法都注重将大型语言模型生成的理由与通过多轮密集检索系统创建的更长上下文结合起来。第一个方法（$RR$）涉及训练一个理据排名模型，以评分的方式衡量生成的理由和检索到的上下文的相关性和真实性。然后，我们使用这些评分使用多种组合策略从两个知识源中获得组合上下文。对于第二种方法（$RATD$），我们使用增强检索训练数据集训练较小的推理模型，使其能够熟练地利用来自更长文本序列的相关信息，这些信息可能部分具有证据性且频繁出现。

    When provided with sufficient explanatory context, smaller Language Models have been shown to exhibit strong reasoning ability on challenging short-answer question-answering tasks where the questions are unseen in training. We evaluate two methods for further improvement in this setting. Both methods focus on combining rationales generated by a larger Language Model with longer contexts created from a multi-hop dense retrieval system. The first method ($\textit{RR}$) involves training a Rationale Ranking model to score both generated rationales and retrieved contexts with respect to relevance and truthfulness. We then use the scores to derive combined contexts from both knowledge sources using a number of combinatory strategies. For the second method ($\textit{RATD}$) we train a smaller Reasoning model using retrieval-augmented training datasets such that it becomes proficient at utilising relevant information from longer text sequences that may be only partially evidential and frequen
    
[^76]: MM-Vet: 评估大型多模态模型的综合能力

    MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])

    [http://arxiv.org/abs/2308.02490](http://arxiv.org/abs/2308.02490)

    MM-Vet是一个评估标准，用于评估大型多模态模型在复杂任务上的综合能力。该标准解决了如何结构化和评估复杂多模态任务、设计适用于不同问题和回答类型的评估指标以及如何提供模型洞察的问题。通过整合不同的核心视觉-语言能力，MM-Vet展示了有趣的能力和解决复杂任务的方法。

    

    我们提出了MM-Vet，一个评估标准，用于检查在复杂多模态任务上的大型多模态模型（LMM）的表现。最近的LMM展示了各种有趣的能力，例如解决书写在黑板上的数学问题，推理新闻图片中的事件和名人，以及解释视觉笑话。快速的模型进步给评估标准的开发带来了挑战。问题包括：（1）如何系统地构建和评估复杂的多模态任务；（2）如何设计适用于不同类型问题和回答的评估指标；（3）如何给出超出简单性能排名的模型洞察。为此，我们提出了MM-Vet，基于这样一个洞察：解决复杂任务的有趣能力通常通过一种通才模型能够整合不同的核心视觉-语言（VL）能力来实现。MM-Vet定义了6个核心VL能力，并检查了从这些能力组合中得出的16种有趣的整合方式。

    We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combin
    
[^77]: 大型语言模型展示出对新颖文学隐喻的解释能力

    Large Language Model Displays Emergent Ability to Interpret Novel Literary Metaphors. (arXiv:2308.01497v1 [cs.CL])

    [http://arxiv.org/abs/2308.01497](http://arxiv.org/abs/2308.01497)

    最近的研究评估了GPT-4，一种大型语言模型，对来自塞尔维亚诗歌的新颖文学隐喻的解释能力。

    

    最近在大型语言模型（LLMs）性能方面的进展引发了关于这种通用人工智能（AI）是否能够在足够的训练下展现出高水平人类能力的争论。尽管LLMs在涉及自然语言处理和推理的各种任务中表现出色，但对它们的能力是否延伸到更具创造力的人类能力存在严重分歧。其中一个核心问题是解释新颖隐喻的能力。由于用于训练LLMs的庞大且非策划的文本语料库，设计测试的一个严重障碍就是需要找到新颖但高质量的隐喻，这些隐喻不太可能出现在训练数据中。在这里，我们评估了GPT-4，一种最先进的大型语言模型，对来自塞尔维亚诗歌并翻译为英语的新颖文学隐喻的自然语言解释能力。

    Recent advances in the performance of large language models (LLMs) have sparked debate over whether, given sufficient training, high-level human abilities emerge in such generic forms of artificial intelligence (AI). Despite the exceptional performance of LLMs on a wide range of tasks involving natural language processing and reasoning, there has been sharp disagreement as to whether their abilities extend to more creative human abilities. A core example is the ability to interpret novel metaphors. Given the enormous and non-curated text corpora used to train LLMs, a serious obstacle to designing tests is the requirement of finding novel yet high-quality metaphors that are unlikely to have been included in the training data. Here we assessed the ability of GPT-4, a state-of-the-art large language model, to provide natural-language interpretations of novel literary metaphors drawn from Serbian poetry and translated into English. Despite exhibiting no signs of having been exposed to thes
    
[^78]: 通过一个空格绕过ChatGPT检测器

    Evade ChatGPT Detectors via A Single Space. (arXiv:2307.02599v1 [cs.CL])

    [http://arxiv.org/abs/2307.02599](http://arxiv.org/abs/2307.02599)

    本研究发现，当前的ChatGPT检测器不能有效区分人类生成和AI生成内容之间的差异，而一个额外的空格成为了规避检测的关键因素。

    

    ChatGPT带来了革命性的社会价值，但也引发了人们对于AI生成内容滥用的担忧。因此，一个重要问题是如何检测出内容是由ChatGPT生成还是人类生成的。现有的检测器是建立在人类生成和AI生成内容之间存在分布差距的假设上的。这些差距通常是通过统计信息或分类器来识别的。我们的研究质疑了检测器中的分布差距假设。我们发现检测器不能有效地区分人类生成和AI生成内容之间的语义和风格差距。相反，"微小的差异"，如额外的一个空格，在检测中变得至关重要。基于这一发现，我们提出了SpaceInfi策略来规避检测。实验证明了这种策略在多个基准和检测器上的有效性。我们还对为什么SpaceInfi能成功规避检测提供了理论解释。

    ChatGPT brings revolutionary social value but also raises concerns about the misuse of AI-generated content. Consequently, an important question is how to detect whether content is generated by ChatGPT or by human. Existing detectors are built upon the assumption that there are distributional gaps between human-generated and AI-generated content. These gaps are typically identified using statistical information or classifiers. Our research challenges the distributional gap assumption in detectors. We find that detectors do not effectively discriminate the semantic and stylistic gaps between human-generated and AI-generated content. Instead, the "subtle differences", such as an extra space, become crucial for detection. Based on this discovery, we propose the SpaceInfi strategy to evade detection. Experiments demonstrate the effectiveness of this strategy across multiple benchmarks and detectors. We also provide a theoretical explanation for why SpaceInfi is successful in evading perple
    
[^79]: 可证明的针对AI生成文本的鲁棒水印技术

    Provable Robust Watermarking for AI-Generated Text. (arXiv:2306.17439v1 [cs.CL])

    [http://arxiv.org/abs/2306.17439](http://arxiv.org/abs/2306.17439)

    GPTWatermark是一种针对性模型水印技术，通过固定分组设计和强大的可证明保证，提供了对AI生成文本的鲁棒性检测和安全性防御。实验证明了其在检测准确性和生成质量方面的优越性，推动了LLMs负责任使用的进步。

    

    随着AI生成的文本越来越接近人类撰写的内容，检测机器生成的文本的能力变得至关重要。为了应对这一挑战，我们提出了GPTWatermark，一种强大且高质量的解决方案，用于确定一段文本是否来自特定模型。我们的方法扩展了现有的水印策略，并采用了一种固定的分组设计，以增强对编辑和改写攻击的鲁棒性。我们展示了我们的带水印语言模型在生成质量、检测正确性和对抗规避攻击的安全性方面具有强大的可证明保证。在各种大型语言模型（LLMs）和多样化数据集上的实验结果表明，我们的方法在检测准确性方面达到了优越的表现，并且与生成质量在困惑度方面相当，从而促进了LLMs的负责任使用。

    As AI-generated text increasingly resembles human-written content, the ability to detect machine-generated text becomes crucial. To address this challenge, we present GPTWatermark, a robust and high-quality solution designed to ascertain whether a piece of text originates from a specific model. Our approach extends existing watermarking strategies and employs a fixed group design to enhance robustness against editing and paraphrasing attacks. We show that our watermarked language model enjoys strong provable guarantees on generation quality, correctness in detection, and security against evasion attacks. Experimental results on various large language models (LLMs) and diverse datasets demonstrate that our method achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs.
    
[^80]: 谁的正确理由是正确的？（arXiv:2306.00639v2 [cs.CL] 已更新）

    Being Right for Whose Right Reasons?. (arXiv:2306.00639v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.00639](http://arxiv.org/abs/2306.00639)

    本论文针对解释性方法的应用进行研究，发现模型预测与人类推理的一致性与人口统计信息有关，模型更倾向于与年长和/或白人注释者提供的原因最为一致。

    

    解释性方法用于评估模型预测与人类推理的一致程度，即是否"出于正确的原因"。然而，先前的研究未能认识到，什么被视为原因有时是主观的。本文介绍了我们认为是首次的人类原因注释集合，其中包含注释者的人口统计信息。我们涵盖了涵盖情感分析和常识推理的三个数据集以及六个人口统计组（在年龄和族裔上均衡）。这样的数据使我们能够同时询问我们的预测与哪些人口统计相符，以及我们模型的推理模式与哪些人的原因相符。我们发现不同人群的注释者之间存在系统性的相互分歧，并展示了16个基于Transformer的模型与某些人口统计组提供的原因更加一致：我们发现模型更倾向于与年长和/或白人注释者提供的原因最为一致。我们重点研究了模型大小和...

    Explainability methods are used to benchmark the extent to which model predictions align with human rationales i.e., are 'right for the right reasons'. Previous work has failed to acknowledge, however, that what counts as a rationale is sometimes subjective. This paper presents what we think is a first of its kind, a collection of human rationale annotations augmented with the annotators demographic information. We cover three datasets spanning sentiment analysis and common-sense reasoning, and six demographic groups (balanced across age and ethnicity). Such data enables us to ask both what demographics our predictions align with and whose reasoning patterns our models' rationales align with. We find systematic inter-group annotator disagreement and show how 16 Transformer-based models align better with rationales provided by certain demographic groups: We find that models are biased towards aligning best with older and/or white annotators. We zoom in on the effects of model size and m
    
[^81]: 用多模态语言模型生成图片

    Generating Images with Multimodal Language Models. (arXiv:2305.17216v1 [cs.CL])

    [http://arxiv.org/abs/2305.17216](http://arxiv.org/abs/2305.17216)

    该论文提出了一种方法，将大型语言模型与预训练的图像编码器和解码器模型进行融合，能生成具有连贯性的图像输出，同时也能进行图像检索和多模态对话。

    

    我们提出了一种方法，将仅包含文本的大型语言模型（LLMs）与预训练的图像编码器和解码器模型进行融合，通过映射它们的嵌入空间。我们的模型展示了广泛的多模态能力：图像检索、新颖图像生成和多模态对话。这是第一种能够在任意交错的图像和文本输入之间进行条件调节，生成连贯图像（和文本）输出的方法。为了在图像生成任务中取得强大的性能，我们提出了一种有效的映射网络，将LLM基于现成的文本到图像生成模型，将文本的隐藏表示转换为视觉模型的嵌入空间，利用LLM强大的文本表示来生成视觉输出。我们的方法在长且复杂语言的任务上优于基准生成模型。除了新颖图像生成之外，我们的模型还能够从文本描述中检索图像，并进行多模态对话。

    We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespe
    
[^82]: 在Transformer语言模型中解决关系任务的机制

    A Mechanism for Solving Relational Tasks in Transformer Language Models. (arXiv:2305.16130v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16130](http://arxiv.org/abs/2305.16130)

    这篇论文研究了在Transformer语言模型中解决关系任务的机制，并发现这些模型利用简单的线性更新来处理关系任务，并以内容无关的方式促进关系的输出。

    

    这篇论文提供了证据表明，尽管语言模型（LMs）的规模和复杂性，它们有时候利用一个简单的计算机制来解决一对一的关系任务（例如 capital_of(Poland)=Warsaw）。我们在上下文学习环境中研究了一系列语言模型的大小（从124M参数到176B参数），并发现对于多种任务（涉及首都、大写和过去时态等），机制的关键部分可以简化为前馈（FFN）网络通常应用的简单线性更新。这些更新也倾向于以内容无关的方式促进关系的输出（例如对编码 Poland:Warsaw::China:Beijing），揭示了这些模型在解决这些任务中的可预测模式。我们进一步显示这个机制是特定于需要从预训练存储器中检索而不是从局部上下文检索的任务。我们的结果为解决关系任务的语言模型的机制做出了贡献。

    A primary criticism towards language models (LMs) is their inscrutability. This paper presents evidence that, despite their size and complexity, LMs sometimes exploit a simple computational mechanism to solve one-to-one relational tasks (e.g., capital_of(Poland)=Warsaw). We investigate a range of language model sizes (from 124M parameters to 176B parameters) in an in-context learning setting, and find that for a variety of tasks (involving capital cities, upper-casing, and past-tensing) a key part of the mechanism reduces to a simple linear update typically applied by the feedforward (FFN) networks. These updates also tend to promote the output of the relation in a content-independent way (e.g., encoding Poland:Warsaw::China:Beijing), revealing a predictable pattern that these models take in solving these tasks. We further show that this mechanism is specific to tasks that require retrieval from pretraining memory, rather than retrieval from local context. Our results contribute to a g
    
[^83]: 学生超越了大师：基于GPT-3的科学事实错误校正方法的匹配

    The student becomes the master: Matching GPT3 on Scientific Factual Error Correction. (arXiv:2305.14707v1 [cs.CL])

    [http://arxiv.org/abs/2305.14707](http://arxiv.org/abs/2305.14707)

    本文提出了一种不需要验证者且不做领域假设的主张校正系统，能够显著提高科学事实错误校正任务的性能，并通过使用LLM的提示方法和主张感知的解码过程来提高校正质量。

    

    由于创建错误校正数据集的成本极高，大多数事实主张校正方法依赖于强大的验证模型来指导校正过程。这导致在科学事实校正等领域性能显著下降，因为好的验证模型并不总是存在。在本研究中，我们介绍了一种不做领域假设且不需要验证者的主张校正系统，但能够比现有方法提高一个数量级的性能 - 在SciFact数据集上实现94％的修正准确性，在SciFact-Open数据集上实现62.5％的修正准确性，分别比下一个最好的方法高出0.5％和1.50％。我们的方法利用LLMs中的提示功能，在训练期间创建一个丰富注释的数据集，可用于完全监督的训练和正则化。我们还使用主张感知的解码过程来提高纠正主张的质量。我们的方法与用于创建数据集的LLM相竞争，证明了利用基于LLM的训练提高科学主张校正任务性能的可能性。

    Due to the prohibitively high cost of creating error correction datasets, most Factual Claim Correction methods rely on a powerful verification model to guide the correction process. This leads to a significant drop in performance in domains like Scientific Claim Correction, where good verification models do not always exist. In this work, we introduce a claim correction system that makes no domain assumptions and does not require a verifier but is able to outperform existing methods by an order of magnitude -- achieving 94% correction accuracy on the SciFact dataset, and 62.5% on the SciFact-Open dataset, compared to the next best methods 0.5% and 1.50% respectively. Our method leverages the power of prompting with LLMs during training to create a richly annotated dataset that can be used for fully supervised training and regularization. We additionally use a claim-aware decoding procedure to improve the quality of corrected claims. Our method is competitive with the very LLM that was
    
[^84]: 学习从兼容标签序列中的语义角色标注

    Learning Semantic Role Labeling from Compatible Label Sequences. (arXiv:2305.14600v1 [cs.CL])

    [http://arxiv.org/abs/2305.14600](http://arxiv.org/abs/2305.14600)

    该论文探讨了如何从不相交的兼容标签序列中高效地学习，将此运用于语义角色标注任务，提出了联合处理VerbNet和PropBank标签的方法，并验证了其有效性。

    

    本文探讨了如何高效地学习从不相交的兼容标签序列中标注的问题。我们认为，不相交标签集之间的兼容结构有助于模型的学习和推理。我们在语义角色标注（SRL）任务中验证了这一假设，具体地，标记具有两个角色序列的句子：VerbNet参数和PropBank参数。先前的研究已经表明跨任务交互可以提高性能。但是，这两个任务仍然是分别解码的，存在生成结构不一致的标签序列 (在像SEMLINK的词典中)的风险。为了消除这个问题，我们首先提出了一个简单而有效的设置，联合处理VerbNet和PropBank标签作为一个序列。通过这个设置，我们证明了在解码过程中强制执行SEMLINK约束不断提高总F1值。通过特殊的输入构造，我们的联合模型可以以超过99%的准确性从PropBank参数中推断出VerbNet参数。我们还提出了一种co

    This paper addresses the question of how to efficiently learn from disjoint, compatible label sequences. We argue that the compatible structures between disjoint label sets help model learning and inference. We verify this hypothesis on the task of semantic role labeling (SRL), specifically, tagging a sentence with two role sequences: VerbNet arguments and PropBank arguments. Prior work has shown that cross-task interaction improves performance. However, the two tasks are still separately decoded, running the risk of generating structurally inconsistent label sequences (as per lexicons like SEMLINK). To eliminate this issue, we first propose a simple and effective setup that jointly handles VerbNet and PropBank labels as one sequence. With this setup, we show that enforcing SEMLINK constraints during decoding constantly improves the overall F1. With special input constructions, our joint model infers VerbNet arguments from PropBank arguments with over 99% accuracy. We also propose a co
    
[^85]: 基于词典的同义词泛化的生物医学命名实体识别

    Biomedical Named Entity Recognition via Dictionary-based Synonym Generalization. (arXiv:2305.13066v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13066](http://arxiv.org/abs/2305.13066)

    本研究提出了一种基于词典的方法来解决生物医学命名实体识别中的同义词泛化问题，通过引入同义词距离和噪声扰动正则化项，实现了对输入文本中的生物医学概念的识别。

    

    生物医学命名实体识别是生物医学自然语言处理中的核心任务之一。为了解决这个任务，已经提出了许多有监督/间接有监督的方法。尽管这些方法取得了显著的成功，但不可避免地需要大量的人力工作。为了减轻人力劳动的需求，已经提出了基于词典的方法，仅根据给定的词典提取命名实体。然而，现有的基于词典的方法的一个缺点是，它们难以识别给定词典中未列出的概念同义词，我们将其称为同义词泛化问题。在本研究中，我们提出了一种新颖的同义词泛化（SynGen）框架，通过基于跨度的预测识别输入文本中包含的生物医学概念。具体而言，SynGen引入了两个正则化项，即（1）同义词距离正则化项；和（2）噪声扰动正则化项，以最小化

    Biomedical named entity recognition is one of the core tasks in biomedical natural language processing (BioNLP). To tackle this task, numerous supervised/distantly supervised approaches have been proposed. Despite their remarkable success, these approaches inescapably demand laborious human effort. To alleviate the need of human effort, dictionary-based approaches have been proposed to extract named entities simply based on a given dictionary. However, one downside of existing dictionary-based approaches is that they are challenged to identify concept synonyms that are not listed in the given dictionary, which we refer as the synonym generalization problem. In this study, we propose a novel Synonym Generalization (SynGen) framework that recognizes the biomedical concepts contained in the input text using span-based predictions. In particular, SynGen introduces two regularization terms, namely, (1) a synonym distance regularizer; and (2) a noise perturbation regularizer, to minimize the
    
[^86]: DUMB: 用于智能评估荷兰语模型的基准测试

    DUMB: A Benchmark for Smart Evaluation of Dutch Models. (arXiv:2305.13026v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13026](http://arxiv.org/abs/2305.13026)

    我们引入了DUMB基准测试，用于智能评估荷兰语模型。通过比较不同大小和类型的预训练语言模型，我们发现目前的荷兰单语模型表现不佳，建议使用其他架构和预训练目标来训练更大的荷兰模型。在该基准测试中，DeBERTaV3 (large)、XLM-R (large)和mDeBERTaV3 (base)取得了最高性能。

    

    我们引入了荷兰模型基准测试：DUMB。该基准测试包括一组用于低、中和高资源任务的多样化数据集。总共有九个任务，其中四个任务以前在荷兰语中还没有。我们提出了相对误差减少 (RER) 的概念，而不是依赖于任务的均值分数，RER 对比了语言模型在DUMB基准测试中与强基准线的表现，这可以在今后评估不同语言模型集时作为参考。通过比较14个预训练语言模型（单语和多语、不同大小的模型），我们评估了基准测试任务的内部一致性以及可能导致高性能的因素。我们的结果表明目前的荷兰单语模型表现不佳，并建议使用其他架构和预训练目标来训练更大的荷兰模型。目前，DeBERTaV3 (large)、XLM-R (large)和mDeBERTaV3 (base) 实现了最高性能。

    We introduce the Dutch Model Benchmark: DUMB. The benchmark includes a diverse set of datasets for low-, medium- and high-resource tasks. The total set of nine tasks includes four tasks that were previously not available in Dutch. Instead of relying on a mean score across tasks, we propose Relative Error Reduction (RER), which compares the DUMB performance of language models to a strong baseline which can be referred to in the future even when assessing different sets of language models. Through a comparison of 14 pre-trained language models (mono- and multi-lingual, of varying sizes), we assess the internal consistency of the benchmark tasks, as well as the factors that likely enable high performance. Our results indicate that current Dutch monolingual models under-perform and suggest training larger Dutch models with other architectures and pre-training objectives. At present, the highest performance is achieved by DeBERTaV3 (large), XLM-R (large) and mDeBERTaV3 (base). In addition t
    
[^87]: 会话中的情感推理：因果发现方法的应用

    Affective Reasoning at Utterance Level in Conversations: A Causal Discovery Approach. (arXiv:2305.02615v1 [cs.CL])

    [http://arxiv.org/abs/2305.02615](http://arxiv.org/abs/2305.02615)

    本文提出了一种新的会话情感因果发现方法（CACD），并通过设计公共骨架和生成替代隐含原因解决了因果模型的不确定性和隐含原因的不可观察性的问题。这种方法可以在变长会话中发现因果关系。

    

    情感推理任务是包括会话中的情感识别、情感-原因对抽取和情感-原因跨度识别在内的一组新兴的基于情感的任务。现有的方法在假设表面关系时忽略了基本的因果模型，因为骨架的不确定性和隐含原因的不可观察性。本文解决了上述两个问题，并进一步提出了会话情感因果发现（CACD）方法。这是一种新颖的因果发现方法，展示了如何通过设计公共骨架和生成替代隐含原因来发现会话中的因果关系。CACD包含两个步骤：（i）为变长会话中的所有话语建立一个中心化的单一图节点因果骨架；（ii）因果自编码器（CAE）通过生成隐含原因和已知显式原因来修正骨架，从而产生因果表示。

    The affective reasoning task is a set of emerging affect-based tasks in conversation, including Emotion Recognition in Conversation (ERC),Emotion-Cause Pair Extraction (ECPE), and Emotion-Cause Span Recognition (ECSR). Existing methods make various assumptions on the apparent relationship while neglecting the essential causal model due to the nonuniqueness of skeletons and unobservability of implicit causes. This paper settled down the above two problems and further proposed Conversational Affective Causal Discovery (CACD). It is a novel causal discovery method showing how to discover causal relationships in a conversation via designing a common skeleton and generating a substitute for implicit causes. CACD contains two steps: (i) building a common centering one graph node causal skeleton for all utterances in variable-length conversations; (ii) Causal Auto-Encoder (CAE) correcting the skeleton to yield causal representation through generated implicit causes and known explicit causes. 
    
[^88]: ChatGPT能评估人类个性吗？一个通用评估框架。

    Can ChatGPT Assess Human Personalities? A General Evaluation Framework. (arXiv:2303.01248v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.01248](http://arxiv.org/abs/2303.01248)

    本文提出了一个通用的评估框架，用于通过LLMs基于MBTI测试评估人类个性。该框架通过设计无偏倚的提示、灵活查询和正确性评估的方式，使LLMs能够灵活评估不同群体的个性特点。

    

    大型语言模型（LLMs）尤其是ChatGPT在各个领域都取得了令人印象深刻的成果，但它们潜在的人类化心理特征尚未得到深入探索。现有的研究主要集中在研究LLMs的虚拟个性，而很少探索通过LLMs分析人类个性的可能性。本文提出了一个通用的评估框架，用于基于迈尔斯·布里格斯人格类型指标（MBTI）测试评估LLMs的人类个性。具体而言，我们首先通过随机排列MBTI问题中的选项来设计无偏倚的提示，采用平均测试结果来鼓励更客观的答案生成。然后，我们建议替换问题陈述中的主语，实现对LLMs上不同主体的灵活查询和评估。最后，我们以正确性评估的方式重新构建问题指令，以便促使LLMs生成更清晰的回应。该提出的框架使LLMs能够灵活评估不同群体的个性特点。

    Large Language Models (LLMs) especially ChatGPT have produced impressive results in various areas, but their potential human-like psychology is still largely unexplored. Existing works study the virtual personalities of LLMs but rarely explore the possibility of analyzing human personalities via LLMs. This paper presents a generic evaluation framework for LLMs to assess human personalities based on Myers Briggs Type Indicator (MBTI) tests. Specifically, we first devise unbiased prompts by randomly permuting options in MBTI questions and adopt the average testing result to encourage more impartial answer generation. Then, we propose to replace the subject in question statements to enable flexible queries and assessments on different subjects from LLMs. Finally, we re-formulate the question instructions in a manner of correctness evaluation to facilitate LLMs to generate clearer responses. The proposed framework enables LLMs to flexibly assess personalities of different groups of people.
    
[^89]: UDAPDR: 基于LLM提示与reranker蒸馏的无监督领域自适应

    UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers. (arXiv:2303.00807v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2303.00807](http://arxiv.org/abs/2303.00807)

    该论文提出了一种无监督领域自适应方法，利用大型语言模型(LLMs)生成大量合成查询和reranker模型，蒸馏为高效的检索器，适用于长尾领域。

    

    很多信息检索任务需要大型标注数据集进行微调，但这样的数据集通常不可用，且在应用于真实场景中时可能会因为领域漂移而迅速失去效用。为了解决这个问题，我们提出一种使用大型语言模型(LLMs)廉价生成大量合成查询的方法。该方法首先利用昂贵的LLM生成少量合成查询，然后再利用成本较低的LLM生成大量的合成查询以微调一组reranker模型。最后，这些reranker会被蒸 distill 成一个高效的检索器，用于目标领域中的检索。实验证明，这种技术可以提高长尾领域中的零样本准确性，即使只使用2K个合成查询进行微调，并且比标准的reranking方法具有更低的延迟。我们提供完整的端到端方案，包括合成数据集等。

    Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains, even where only 2K synthetic queries are used for fine-tuning, and that it achieves substantially lower latency than standard reranking methods. We make our end-to-end approach, including our synthetic datasets an
    
[^90]: Orca: 一种用于中文对话机器阅读理解的少样本测试基准

    Orca: A Few-shot Benchmark for Chinese Conversational Machine Reading Comprehension. (arXiv:2302.13619v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.13619](http://arxiv.org/abs/2302.13619)

    Orca是中文对话机器阅读理解的第一个基准，提供了零样本/少样本设置来评估模型对多样领域的泛化能力，并通过提供与回答相关的段落来更合理地评估模型的理解能力。

    

    对话机器阅读理解（CMRC）任务旨在回答对话中的问题，由于其广泛应用，近年来已成为热门研究课题。然而，现有的CMRC基准在每个对话中分配一个静态段落，与真实场景不一致。因此，很难合理评估模型对真实场景的理解能力。为此，我们提出了第一个中文CMRC基准Orca，并进一步提供了零样本/少样本设置，以评估模型对多样领域的泛化能力。我们收集了831个热门话题驱动的对话，共计4,742轮。每个对话的每个轮次都会分配一个与回答有关的段落，旨在更合理地评估模型的理解能力。对话的主题来自社交媒体平台，涵盖33个领域，力争与真实场景保持一致。重要的是，Orca中的答案都是经过良好注释的自然回答。

    The conversational machine reading comprehension (CMRC) task aims to answer questions in conversations, which has been a hot research topic in recent years because of its wide applications. However, existing CMRC benchmarks in which each conversation is assigned a static passage are inconsistent with real scenarios. Thus, model's comprehension ability towards real scenarios are hard to evaluate reasonably. To this end, we propose the first Chinese CMRC benchmark Orca and further provide zero-shot/few-shot settings to evaluate model's generalization ability towards diverse domains. We collect 831 hot-topic driven conversations with 4,742 turns in total. Each turn of a conversation is assigned with a response-related passage, aiming to evaluate model's comprehension ability more reasonably. The topics of conversations are collected from social media platform and cover 33 domains, trying to be consistent with real scenarios. Importantly, answers in Orca are all well-annotated natural resp
    
[^91]: 知识是微调语言模型中权重空间的一个区域

    Knowledge is a Region in Weight Space for Fine-tuned Language Models. (arXiv:2302.04863v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04863](http://arxiv.org/abs/2302.04863)

    研究探讨了不同模型在权重空间中的位置与性能的关联，发现微调语言模型在权重空间中有明确定义的区域，且这些区域中的模型表现出高性能。此外，通过绕过这些区域，可以得到性能相当甚至更好的新模型。

    

    神经网络研究一直专注于理解单个模型在单个数据集上的训练结果。然而，对于不同模型之间的关系，特别是那些在不同数据集上进行训练或测试的模型之间的关系，我们了解甚少。我们通过研究不同模型的权重空间和潜在的损失地形之间的相互关系来解决这个问题。具体而言，我们证明了为高性能而进行微调优化的模型存在于权重空间中定义明确的区域中，反之亦然——任何在这些区域中的模型都表现出高性能。值得注意的是，我们展示了在相同数据集上进行微调的语言模型在权重空间中形成一个紧密的聚类，而在相同基础任务下从不同数据集进行微调的模型则形成一个较松散的聚类。此外，绕过模型之间的区域会生成性能相当甚至更好的新模型，甚至在进行微调的情况下也是如此。

    Research on neural networks has focused on understanding a single model trained on a single dataset. However, relatively little is known about the relationships between different models, particularly those trained or tested on different datasets. We address this by studying how the weight space and the underlying loss landscape of different models are interconnected.  Specifically, we demonstrate that finetuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa -- that any model that resides anywhere in those regions also exhibits high performance. Notably, we show that language models that have been finetuned on the same dataset form a tight cluster in the weight space, while models finetuned on different datasets from the same underlying task form a looser cluster. Moreover, traversing around the region between the models leads to new models that perform comparably or even better than models obtained via finetuning, even on
    
[^92]: 通过合并语言模型的权重实现无数据知识融合

    Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09849](http://arxiv.org/abs/2212.09849)

    本文提出了一种无数据知识融合方法，可以合并在不同训练数据集上建立的单个模型，以得到一个在所有数据集领域上表现良好且可以推广到域外数据的单一模型。

    

    微调预训练语言模型已成为构建下游NLP模型的流行范式。通常情况下，经过微调的模型已经可用，但其训练数据不可用，由于数据隐私或知识产权问题。这就造成了跨模型融合知识以产生更好的单一模型的障碍。在本文中，我们研究了建立在不同训练数据集上的单个模型之间合并的问题，以得到一个在所有数据集领域上表现良好且可以推广到域外数据的单一模型。我们提出了一种无数据知识融合方法，该方法在参数空间中合并模型，由权重引导，以最小化合并模型和单个模型之间的预测差异。在一系列评估设置中，我们展示了该方法显著优于如Fisher加权平均或模型集成等基线。此外，我们发现我们的方法是一个有前途的多语言微调替代方案，因为它可以在不需要任何额外注释数据的情况下实现可比的性能。

    Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-
    
[^93]: NAPG：用于混合表格-文本问答的非自回归程序生成

    NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual Question Answering. (arXiv:2211.03462v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.03462](http://arxiv.org/abs/2211.03462)

    该论文提出了一种非自回归程序生成框架，用于解决混合表格-文本问答中的数值推理和跨度提取问题。实验结果表明，该方法可以显著改善程序生成的准确性并提高速度。

    

    混合表格-文本问答要求从异构信息进行推理，推理类型主要分为数值推理和跨度提取。当前的数值推理方法通过自回归解码程序序列，每个解码步骤生成运算符或操作数。然而，逐步解码存在暴露偏差，并且由于错误传播，程序生成的准确性随着解码步骤的展开而急剧下降。本文提出了一种非自回归程序生成框架，可以独立生成包含运算符和操作数的完整程序元组，可以解决错误传播问题，并显著提高程序生成的速度。在ConvFinQA和MultiHiertt数据集上的实验表明，我们的非自回归程序生成方法可以比强大的FinQANet方法带来实质性的改进（+5.06 Exe Acc点和+4.80 Prog Acc点）。

    Hybrid tabular-textual question answering (QA) requires reasoning from heterogeneous information, and the types of reasoning are mainly divided into numerical reasoning and span extraction. Current numerical reasoning methods autoregressively decode program sequences, and each decoding step produces either an operator or an operand. However, the step-by-step decoding suffers from exposure bias, and the accuracy of program generation drops sharply as the decoding steps unfold due to error propagation. In this paper, we propose a non-autoregressive program generation framework, which independently generates complete program tuples containing both operators and operands, can address the error propagation issue while significantly boosting the speed of program generation. Experiments on the ConvFinQA and MultiHiertt datasets show that our non-autoregressive program generation method can bring about substantial improvements over the strong FinQANet (+5.06 Exe Acc and +4.80 Prog Acc points) 
    
[^94]: 通过检索软提示增强指令跟随模型的零样本表现效率

    Efficiently Enhancing Zero-Shot Performance of Instruction Following Model via Retrieval of Soft Prompt. (arXiv:2210.03029v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.03029](http://arxiv.org/abs/2210.03029)

    通过检索软提示有效辅助硬提示，在增加少量参数的情况下提高了指令跟随模型在零样本任务上的表现效率。

    

    提升指令跟随模型的零样本表现效率需要大量计算，要么通过扩展训练数据集的总数，要么增加模型的大小。在这项工作中，我们通过提示微调获取软提示，探索了如何通过检索软提示有效辅助硬提示来进行零样本任务泛化。具体而言，我们通过提示微调为每个提示训练软提示嵌入，存储与提示嵌入映射的训练实例样本，并在推理过程中检索最接近查询实例的训练实例对应的提示嵌入。虽然只增加了0.007%的额外参数，检索软提示提高了T0在未见任务上的性能，在11个数据集中有10个表现优于T0，并且将T0在BIG-bench基准测试中的平均准确率提高了2.39个百分点。此外，我们还报告了一个有意思的发现，即检索在相似答案选择格式上训练的源嵌入比提示嵌入更重要。

    Enhancing the zero-shot performance of instruction-following models requires heavy computation, either by scaling the total number of training datasets or the model size. In this work, we explore how retrieval of soft prompts obtained through prompt tuning can efficiently assist hard prompts in zero-shot task generalization. Specifically, we train soft prompt embeddings for each prompt through prompt tuning, store the samples of the training instances mapped with the prompt embeddings, and retrieve the corresponding prompt embedding of the training instance closest to the query instance during inference. While only adding 0.007% additional parameters, retrieval of soft prompt enhances the performance of T0 on unseen tasks by outperforming it on 10 out of 11 datasets as well as improving the mean accuracy of T0 on BIG-bench benchmark by 2.39% points. Also, we report an interesting finding that retrieving source embeddings trained on similar answer choice formats is more important than t
    
[^95]: 分类调查中基于知识的追加问题生成的知识驱动方法

    What should I Ask: A Knowledge-driven Approach for Follow-up Questions Generation in Conversational Surveys. (arXiv:2205.10977v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.10977](http://arxiv.org/abs/2205.10977)

    本文提出了一种基于知识驱动的方法，用于在对话调查中实时生成追加问题。该方法通过使用知识来引导生成过程，生成更具信息量、连贯性和清晰度的追加问题。

    

    实时生成追加问题可以通过启用更动态和个性化的调查结构，显著提高对话调查的质量和用户体验。本文提出了一种新颖的基于知识驱动的对话调查中的追加问题生成任务。我们构建了一个新的数据集，其中包含人类编写的带有对话历史和标记知识的追加问题。除了数据集，我们还设计并验证了一组无参考Gricean启发式评估指标，系统评估生成的追加问题的质量。然后，我们提出了一个两阶段的基于知识的模型，通过使用知识来引导生成过程，生成信息丰富、连贯和清晰的追加问题。实验结果表明，与基于GPT的基准模型相比，我们的两阶段模型生成的追加问题更具信息量，连贯性和清晰度。

    Generating follow-up questions on the fly could significantly improve conversational survey quality and user experiences by enabling a more dynamic and personalized survey structure. In this paper, we proposed a novel task for knowledge-driven follow-up question generation in conversational surveys. We constructed a new human-annotated dataset of human-written follow-up questions with dialogue history and labeled knowledge in the context of conversational surveys. Along with the dataset, we designed and validated a set of reference-free Gricean-inspired evaluation metrics to systematically evaluate the quality of generated follow-up questions. We then propose a two-staged knowledge-driven model for the task, which generates informative and coherent follow-up questions by using knowledge to steer the generation process. The experiments demonstrate that compared to GPT-based baseline models, our two-staged model generates more informative, coherent, and clear follow-up questions.
    
[^96]: 创新风格化：通过逐步可用随机化词典进行时间轴生成

    Stylized innovation: generating timelines by interrogating incrementally available randomised dictionaries. (arXiv:1806.07722v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/1806.07722](http://arxiv.org/abs/1806.07722)

    该论文通过设计和生成合成创新网络词典，研究了新符号发现在增加词汇的过程中的表现，并探讨了创新过程的整体统计和行为。

    

    理解创新的一个关键挑战是它是一个动态、持续的过程，可能高度依赖于文化、经济或运气等瞬息万变的因素。这意味着对真实世界过程的任何分析必然是历史性的，因此可能为时已晚，但也无法确定创新之间的连接结构或属性是什么。在这里，我尝试通过设计和生成一组合成创新网络“词典”，用于承载样本创新时间轴，探测这些过程的整体统计和行为，并确定它们对结构或生成算法的依赖程度。因此，受到Fink、Reeves、Palma和Farr（2017）关于语言、美食和技术创新的工作的启发，我研究了新符号发现如何以额外的“词汇”词典中的单词增加的方式呈现。

    A key challenge when trying to understand innovation is that it is a dynamic, ongoing process, which can be highly contingent on ephemeral factors such as culture, economics, or luck. This means that any analysis of the real-world process must necessarily be historical - and thus probably too late to be most useful - but also cannot be sure what the properties of the web of connections between innovations is or was. Here I try to address this by designing and generating a set of synthetic innovation web "dictionaries" that can be used to host sampled innovation timelines, probe the overall statistics and behaviours of these processes, and determine the degree of their reliance on the structure or generating algorithm. Thus, inspired by the work of Fink, Reeves, Palma and Farr (2017) on innovation in language, gastronomy, and technology, I study how new symbol discovery manifests itself in terms of additional "word" vocabulary being available from dictionaries generated from a finite nu
    

