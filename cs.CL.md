# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents.](http://arxiv.org/abs/2401.12963) | AutoRT是一个利用现有的基础模型来扩展机器人在未知场景中的部署的系统，通过利用视觉-语言模型和大型语言模型，提出多样化和新颖的指令，并有效地推理自主权和安全性的权衡。 |
| [^2] | [Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding.](http://arxiv.org/abs/2401.12954) | 元提示是一种支持技术，将单个语言模型转化为多面手指挥者，通过高级指令将复杂任务分解为子任务，并由不同的专家模型处理，最终提高效率。 |
| [^3] | [Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion.](http://arxiv.org/abs/2401.12947) | 本论文研究了基于Transformer模型学习结构递归的能力，介绍了一个通用框架，将编程语言领域中的抽象概念与具体的序列建模问题和学习模型行为相连接。 |
| [^4] | [Multicultural Name Recognition For Previously Unseen Names.](http://arxiv.org/abs/2401.12941) | 这篇论文研究了以往未见名字的多元文化名字识别，通过改进训练数据和输入结构，提高了模型对来自不同文化背景的名字的识别能力，避免下游任务中的文化偏见。 |
| [^5] | [Red Teaming Visual Language Models.](http://arxiv.org/abs/2401.12915) | 本论文通过提供一个新颖的红队数据集RTVLM，评估了当前视觉语言模型VLMs在忠实度、隐私、安全和公平性方面的性能，并与GPT-4V进行了比较。结果显示，10个知名的VLMs在红队行动中存在不同程度的困难。 |
| [^6] | [From Understanding to Utilization: A Survey on Explainability for Large Language Models.](http://arxiv.org/abs/2401.12874) | 本综述论文研究了大规模语言模型(LLMs)可解释性的新兴领域，强调了在LLMs中增强可解释性的必要性，同时解决了广大公众对其信任和技术界对这些模型更深理解的需求。该综述对现有的可解释性方法进行分类，并讨论了它们在提高模型透明度和可靠性方面的应用，旨在弥合理论理解和实际应用之间的差距。 |
| [^7] | [Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model.](http://arxiv.org/abs/2401.12873) | 本研究探索了利用质量估计作为奖励模型来预测人类偏好以改善机器翻译的潜力。我们发现基于质量估计的反馈训练存在过度优化问题，采用启发式规则来检测错误翻译并对质量估计模型进行惩罚以解决该问题。 |
| [^8] | [KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning.](http://arxiv.org/abs/2401.12863) | KAM-CoT是一个知识增强的多模式思维链推理框架，可以通过引入外部知识图谱来提高模型对多模式任务的理解能力，并生成更准确的答案。 |
| [^9] | [Gradient Flow of Energy: A General and Efficient Approach for Entity Alignment Decoding.](http://arxiv.org/abs/2401.12798) | 这篇论文介绍了一种能够解决实体对齐解码问题的新方法，该方法通过最小化能量来优化解码过程，以实现图同质性，并且仅依赖于实体嵌入，具有较高的通用性和效率。 |
| [^10] | [Benchmarking LLMs via Uncertainty Quantification.](http://arxiv.org/abs/2401.12794) | 这项研究提出了一种通过不确定性量化来对LLMs进行基准测试的方法，并引入了一种不确定性感知评估指标UAcc。研究结果显示，高准确度的LLMs可能具有较低的确定性，而大规模LLMs可能比较小规模LLMs更不确定。指令微调倾向于增加LLMs的不确定性。 |
| [^11] | [Multilingual and Fully Non-Autoregressive ASR with Large Language Model Fusion: A Comprehensive Study.](http://arxiv.org/abs/2401.12789) | 本研究提出了一种多语种非自回归ASR系统，通过融合大型语言模型，实现了在不同语言上的平均相对WER改善，并通过全面的消融研究分析了影响实际大规模语言模型融合语音识别系统效果的因素。 |
| [^12] | [What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition.](http://arxiv.org/abs/2401.12756) | 本研究提出了一个新的零样本模块组合框架，统一了选择、加权和组合参数模块的各种变化。以领域知识和适配器层为场景，通过系统化的统一概念，进行了首次全面的零样本知识组合的基准研究。 |
| [^13] | [A Comprehensive View of the Biases of Toxicity and Sentiment Analysis Methods Towards Utterances with African American English Expressions.](http://arxiv.org/abs/2401.12720) | 这项研究全面观察了情感和毒性分析方法对带有非洲裔美国英语表达的话语中的偏见。实验结果发现由于AI模型的训练数据有限，导致这些方法在对待对“黑鬼”一词的再适应时存在偏差。 |
| [^14] | [Generating Unsupervised Abstractive Explanations for Rumour Verification.](http://arxiv.org/abs/2401.12713) | 该论文利用无监督的方法，通过对言辞进行评分并生成解释性摘要，来增加谣言验证的信息丰富度和准确性。 |
| [^15] | [Energy-based Automated Model Evaluation.](http://arxiv.org/abs/2401.12689) | 提出了一种基于能量的自动化模型评估方法，通过建立关于个体样本相关信息的元分布统计量，能够更高效和有效地评估机器学习模型的性能，解决了AutoEval框架中的过度自信、存储和计算成本高等问题。 |
| [^16] | [Context Matters: Pushing the Boundaries of Open-Ended Answer Generation with Graph-Structured Knowledge Context.](http://arxiv.org/abs/2401.12671) | 本论文介绍了一种结合图驱动的上下文检索和知识图结构增强的框架，通过提高LLMs的能力，尤其是在特定领域的社区问答平台上，更好地回答开放式问题。 |
| [^17] | [A Reply to Makelov et al. (2023)'s "Interpretability Illusion" Arguments.](http://arxiv.org/abs/2401.12631) | 本文回应了Makelov等人(2023)的论文，该论文评述了子空间交换干预方法的"解释性错觉"问题。我们指出，所谓的"解释性错觉"可以包括直观和可取的解释，而Makelov等人(2023)发现的"错觉"是他们训练和评估范例的产物。尽管我们不同意他们的核心表述，但他们的例子和讨论推动了可解释性领域的发展。 |
| [^18] | [SLANG: New Concept Comprehension of Large Language Models.](http://arxiv.org/abs/2401.12585) | 本研究提出了一个新的基准SLANG，旨在增强大型语言模型LLMs对互联网上新概念的理解能力，同时提出了一种基于因果推断的基准方法FOCUS，能帮助LLMs更好地理解新的短语和用法模式。 |
| [^19] | [LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools.](http://arxiv.org/abs/2401.12576) | LLMCheckup是一个可解释性工具，通过连接大型语言模型与可解释的AI工具，使用户能够与模型进行对话，生成自我解释并提供建议。 |
| [^20] | [Automated Fact-Checking of Climate Change Claims with Large Language Models.](http://arxiv.org/abs/2401.12566) | Climinator是一种基于大型语言模型的工具，用于自动核查气候变化声明。它利用多种科学来源的信息，采用调解者-倡导者框架，有效综合不同观点，从而得出基于科学、事实的评估结果。 |
| [^21] | [DREditor: An Time-efficient Approach for Building a Domain-specific Dense Retrieval Model.](http://arxiv.org/abs/2401.12540) | DREditor是一种时间高效的方法，通过直接校准现有的密集检索模型的输出嵌入，使用线性映射和编辑操作符来编辑匹配规则，以提高特定领域的检索效率。 |
| [^22] | [BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models.](http://arxiv.org/abs/2401.12522) | BiTA是一种用于大语言模型的创新方法，通过双向调整实现了无损加速。它采用简化的半自回归生成和草稿验证，通过高效的基于树的解码同时进行候选生成和验证，提高了推理效率。这种方法不需要额外的辅助模型或显著的额外内存开销。 |
| [^23] | [Key Information Retrieval to Classify the Unstructured Data Content of Preferential Trade Agreements.](http://arxiv.org/abs/2401.12520) | 本论文介绍了一种用于长文本分类和预测的新方法，通过嵌入技术对长文本进行压缩，然后采用双向编码器表示来自Transformers的嵌入方法进行文本分类训练，实验结果显示在优惠贸易协定的长文本分类方面取得了显著的性能提升。 |
| [^24] | [Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?.](http://arxiv.org/abs/2401.12492) | 本研究比较了以群体属性、个体用户和组合方法来模拟人的上下文。合并群体和个体特征显著提高了用户级回归任务的性能，而模拟个体用户则显著提高了单个文档级分类任务的性能。 |
| [^25] | [Assessing and Understanding Creativity in Large Language Models.](http://arxiv.org/abs/2401.12491) | 本文旨在建立一个评估大型语言模型（LLM）创造力水平的高效框架，并提出了评估方法和一个包含700个问题的全面数据集。研究发现创造力水平受到任务差异和LLM模型参数的影响。 |
| [^26] | [Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment.](http://arxiv.org/abs/2401.12474) | 本文介绍了一种名为Ditto的角色扮演自我对齐方法，通过对角色知识的利用，使大型语言模型能够模拟角色扮演对话，从而增强其角色扮演能力。实验证明，Ditto在角色扮演基准和MT-Bench的评估中取得了出色的结果。 |
| [^27] | [Contrastive Learning in Distilled Models.](http://arxiv.org/abs/2401.12472) | 本论文介绍了一个基于对比学习的适用于蒸馏模型的方法，该方法通过对DistilBERT模型进行改进，在语义文本相似性上取得了显著的改进，且生成的DistilFace模型具有轻量级的特点。 |
| [^28] | [Fast Adversarial Training against Textual Adversarial Attacks.](http://arxiv.org/abs/2401.12461) | 本研究提出了一种快速对抗训练（FAT）方法，用于提高自然语言处理模型在无同义词知识的情况下的对抗鲁棒性。该方法通过单步梯度上升在嵌入空间中生成对抗性示例，以加速训练过程。 |
| [^29] | [CIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory Accelerators.](http://arxiv.org/abs/2401.12428) | CIM-MLC是一个面向计算内存加速器的多级编译栈，为了支持各种CIM架构的潜力，该编译栈完全了解CIM架构细节和实现多样性，并提供灵活性来支持具有不同计算粒度的CIM架构。 |
| [^30] | [The Neglected Tails of Vision-Language Models.](http://arxiv.org/abs/2401.12425) | 本文通过分析预训练文本来衡量视觉语言模型中概念的频率，并发现流行的VLM数据集展示了长尾概念分布，这与按类别的准确率强烈相关。 |
| [^31] | [How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual Translation via Tiny Multi-Parallel Data.](http://arxiv.org/abs/2401.12413) | 本文研究了如何通过仅有的少量微小多语言平行数据来增强以英语为中心的模型的零样本翻译能力，实现大幅度的非英文整体改进，并保持英文方向上的性能能力。研究表明，即使在随机抽取的少量方向集上进行微调，也可以获得可比较的整体改进。 |
| [^32] | [Enhancing In-context Learning via Linear Probe Calibration.](http://arxiv.org/abs/2401.12406) | 本研究提出了一种名为线性探测校准（LinC）的技术，通过校准模型的输出概率，显著提高了上下文学习（ICL）在生成预训练变压器（GPT）模型上的测试性能。 |
| [^33] | [Longitudinal Sentiment Classification of Reddit Posts.](http://arxiv.org/abs/2401.12382) | 本研究对四所加拿大主要大学的学生在Reddit上撰写的帖子进行纵向情感分类。通过调整情感阈值，我们成功构建了分类器，能够将帖子情感分类为积极和消极类别，并且结果在不同大学数据集中一致。 |
| [^34] | [Development of an NLP-driven computer-based test guide for visually impaired students.](http://arxiv.org/abs/2401.12375) | 本文介绍了一种为视力障碍学生开发的基于NLP的计算机化测试指南，利用语音技术和NLP技术实时提供支持和帮助，并将文本问题和选项转化为机器可读的格式。 |
| [^35] | [Subgraph Extraction-based Feedback-guided Iterative Scheduling for HLS.](http://arxiv.org/abs/2401.12343) | 提出了一种基于反馈引导和子图提取的差分约束调度算法用于高级综合，可显著减少寄存器使用率。 |
| [^36] | [Fine-tuning Large Language Models for Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection.](http://arxiv.org/abs/2401.12326) | 本文研究了对多生成器、多领域和多语言机器生成文本的大型语言模型进行微调的任务。实验结果显示，转换器模型特别是LoRA-RoBERTa在效果上超过了传统的机器学习方法，对于多语言环境下的机器生成文本识别，多数投票方法尤为有效。 |
| [^37] | [Cheap Learning: Maximising Performance of Language Models for Social Data Science Using Minimal Data.](http://arxiv.org/abs/2401.12295) | 本文回顾了“廉价”学习技术在社会科学中的应用，包括弱监督、迁移学习和提示工程。特别地，通过提示大规模语言模型，可以实现高准确性的性能。 |
| [^38] | [GRATH: Gradual Self-Truthifying for Large Language Models.](http://arxiv.org/abs/2401.12292) | GRATH是一种逐步自我真实化的方法，用于提高大型语言模型的真实性。它通过使用领域外问题提示生成答案，并通过直接偏好优化进行自适应模型优化。GRATH在没有标注答案的情况下以自我监督的方式学习真实性，并通过迭代优化来逐步提升模型真实性。 |
| [^39] | [The Ethics of Interaction: Mitigating Security Threats in LLMs.](http://arxiv.org/abs/2401.12273) | 本研究全面探讨了与语言学习模型（LLMs）面临的安全威胁相关的伦理挑战。分析了五种主要威胁的伦理后果，并强调了确保这些系统在伦理规范范围内运作的紧迫性。 |
| [^40] | [Orion-14B: Open-source Multilingual Large Language Models.](http://arxiv.org/abs/2401.12246) | Orion-14B是一个具有140亿参数的开源多语言大型语言模型。在该研究中，我们采用数据调度方法对一个基础模型进行训练，使用了来自多种语言的2.5万亿个标记的多样化语料库。我们还对对话应用和其他特定用例进行了微调。评估结果显示，Orion-14B在广泛的任务中取得了领先的性能。我们将Orion-14B模型系列及其相关代码公开，以鼓励未来在这一领域的研究和实际应用。 |
| [^41] | [Claim Detection for Automated Fact-checking: A Survey on Monolingual, Multilingual and Cross-Lingual Research.](http://arxiv.org/abs/2401.11969) | 本综述调研了自动事实核查中索赔检测的现有工作，特别关注多语言数据和方法。这是一个具有挑战性但富有成果的研究方向，需要更通用的解决方案来对抗跨多语言和模态的不实信息。 |
| [^42] | [In-context Learning with Retrieved Demonstrations for Language Models: A Survey.](http://arxiv.org/abs/2401.11624) | 本综述调查了一种名为检索示范的方法，它通过使用特定于输入查询的示范来提高语言模型的少量样本情境学习（ICL）能力。这种方法不仅提高了学习效率和可扩展性，还减少了手动示例选择中的偏见。 |
| [^43] | [FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for Large Language Models' Training?.](http://arxiv.org/abs/2401.11033) | 这项工作介绍了一个将FAIR数据原则嵌入到大型语言模型训练中的框架，并提供了整合指南和检查清单。通过一个案例研究，展示了在符合FAIR标准的数据集中识别和减轻偏见的实际应用。 |
| [^44] | [Multilingual acoustic word embeddings for zero-resource languages.](http://arxiv.org/abs/2401.10543) | 该研究发展了一种多语言声学词嵌入方法，用于解决缺乏标注数据的零资源语言的挑战。通过使用神经网络和多语言转移，该方法在零资源语言上取得了比现有模型更好的性能。研究还展示了该方法在仇恨言论检测和语义查询中的应用潜力。 |
| [^45] | [ChatQA: Building GPT-4 Level Conversational QA Models.](http://arxiv.org/abs/2401.10225) | ChatQA是一系列对话问答模型，可以达到GPT-4级别的准确性。通过两阶段的指令调整方法，可以显著提高大型语言模型在零-shot对话问答中的结果。使用密集检索器进行问答数据集的微调可以实现与最先进的查询重写模型相当的结果，同时降低部署成本。ChatQA-70B在10个对话问答数据集上的平均得分超过了GPT-4，且不依赖于任何来自OpenAI GPT模型的合成数据。 |
| [^46] | [Spatial-Temporal Large Language Model for Traffic Prediction.](http://arxiv.org/abs/2401.10134) | 本文提出了一种空间-时间大语言模型（ST-LLM）用于交通预测，通过参数扩展和预训练来提高预测准确性，并利用空间-时间嵌入模块学习标记的空间位置和全局时间表示。 |
| [^47] | [Partial Diacritization: A Context-Contrastive Inference Approach.](http://arxiv.org/abs/2401.08919) | 部分音标化是选择标记部分字符来提高阅读可读性和准确性的新方法。上下文对比的部分音标化（CCPD）集成了现有的阿拉伯音标化系统，并通过衡量部分音标化的新指标来判断需要标记哪些字符。 |
| [^48] | [Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring.](http://arxiv.org/abs/2401.08517) | 这个论文研究了一种基于知识图谱情境化的LLM聊天机器人，作为学生学习推荐的解释工具和指导。通过定义上下文并利用人工策划的信息源来调控LLM的生成，聊天机器人能在与学生对话中提供解释和指导。 |
| [^49] | [Joint Unsupervised and Supervised Training for Automatic Speech Recognition via Bilevel Optimization.](http://arxiv.org/abs/2401.06980) | 本文提出了一种双层优化的训练方法，用于自动语音识别，通过联合无监督和监督训练来提高性能。 |
| [^50] | [APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning.](http://arxiv.org/abs/2401.06827) | 本研究提出了APLe方法，通过逐令牌自适应的方式调节CLIP模型中的视觉和语言模式提示，提高了模型的泛化性能。 |
| [^51] | [An Analysis of User Behaviours for Objectively Evaluating Spoken Dialogue Systems.](http://arxiv.org/abs/2401.04867) | 本文研究了用户行为与主观评估在口语对话系统中的关系，提出了一种间接但客观评估系统的框架，并发现在不同类型的对话任务中，不同的用户行为指标对评估起到重要作用。 |
| [^52] | [Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM.](http://arxiv.org/abs/2401.02994) | 本研究介绍了一种名为“混合”的方法，通过组合多个适度规模的聊天AI模型，可以达到或超越比它们更大的模型的性能表现。 |
| [^53] | [AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation.](http://arxiv.org/abs/2312.13010) | AgentCoder是一种基于多Agent的代码生成和测试优化解决方案，通过程序员Agent、测试设计师Agent和测试执行Agent的协作，实现了在平衡代码生成和有效测试用例生成与执行方面的挑战中的突破。 |
| [^54] | [A Survey of Text Watermarking in the Era of Large Language Models.](http://arxiv.org/abs/2312.07913) | 本文综述了大语言模型时代的文本水印技术，包括不同技术的概述和比较、算法评估方法、应用场景以及当前挑战和未来发展方向。 |
| [^55] | [A ripple in time: a discontinuity in American history.](http://arxiv.org/abs/2312.01185) | 该论文通过使用向量嵌入和非线性降维方法，发现GPT-2与UMAP的结合可以提供更好的分离和聚类效果。同时，经过微调的DistilBERT模型可用于识别总统和演讲的年份。 |
| [^56] | [Beyond Turing: A Comparative Analysis of Approaches for Detecting Machine-Generated Text.](http://arxiv.org/abs/2311.12373) | 该论文对比了三种不同方法用于检测人类生成和机器生成文本的能力，并发现它们在性能上存在显著差异，为进一步研究和开发具有鲁棒性和高度区分性的模型提供了重要参考。 |
| [^57] | [Outlier Dimensions Encode Task-Specific Knowledge.](http://arxiv.org/abs/2310.17715) | 异常维度可以编码关键的特定任务知识，并且一个单一的异常维度可以以最小的错误率完成下游任务。 |
| [^58] | [Formally Specifying the High-Level Behavior of LLM-Based Agents.](http://arxiv.org/abs/2310.08535) | 本文介绍了一个最小生成框架，通过在高级声明中定义所需的代理人行为，然后构建解码监视器，从而实现了基于LLM的代理人的快速设计和实施。 |
| [^59] | [EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling.](http://arxiv.org/abs/2310.04691) | EMO提出了地球移动距离优化（EMO）来解决语言模型中的退化现象。EMO利用了地球移动距离的特性，并引入了一个可行的上界来简化训练。经过评估，发现EMO在语言模型上有显著的改进。 |
| [^60] | [Retrieval meets Long Context Large Language Models.](http://arxiv.org/abs/2310.03025) | "本论文研究了将检索增强和长上下文窗口的大语言模型相结合的解决方案，发现在长上下文任务中，通过检索增强的LLM使用4K上下文窗口可以取得与通过长上下文窗口微调的LLM使用16K上下文窗口相当的性能，同时计算量要少得多。此外，无论上下文窗口大小如何，检索都可以显著提高LLM的性能。" |
| [^61] | [GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models.](http://arxiv.org/abs/2310.00737) | 这篇论文探讨了生成式人工智能和大型语言模型的潜在滥用，呼吁认识到这些挑战的紧迫性。研究揭示了这些技术在深度伪造、合成身份恶意活动以及虚假信息和欺诈方面可能带来的社会影响。 |
| [^62] | [AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ.](http://arxiv.org/abs/2310.00367) | 本论文引入了AutomaTikZ，它通过使用TikZ作为中间表示来实现科学图形的文本引导合成。通过微调LLaMA和引入CLiMA模型，并使用DaTikZ数据集进行训练，在人工评估和自动评估中，CLiMA和LLaMA在与人类创建的图形的相似度方面表现出色，并且CLiMA还改进了tex。 |
| [^63] | [CB-Whisper: Contextual Biasing Whisper using Open-Vocabulary Keyword-Spotting.](http://arxiv.org/abs/2309.09552) | CB-Whisper是一种基于OpenAI的Whisper模型的自动语音识别系统，通过使用开放词汇关键词检测（OV-KWS）识别罕见的命名实体，并使用这些实体作为提示来改进识别效果。实验证明，该方法在提高实体召回率的同时会略微增加混淆错误率（MER）。 |
| [^64] | [DiariST: Streaming Speech Translation with Speaker Diarization.](http://arxiv.org/abs/2309.08007) | DiariST是第一个流式语音翻译和说话者分离的解决方案，通过集成标记级序列化输出训练和t向量，实现了强大的ST和SD能力。 |
| [^65] | [SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models.](http://arxiv.org/abs/2308.16692) | 提出了一种面向语音大语言模型的统一语音分词器SpeechTokenizer，通过统一语义和声学标记并采用编码器-解码器架构，实现了在不同层级上解耦语音信息的不同方面，构建了一个统一语音语言模型（USLM）。 |
| [^66] | [Large Language Models Vote: Prompting for Rare Disease Identification.](http://arxiv.org/abs/2308.12890) | 本文提出了一种名为模型投票提示(MVP)的方法，用于改善在少样本学习(FSL)环境下大型语言模型(LLMs)的查询性能。MVP通过提示多个LLMs执行相同的任务，并对生成的输出进行多数投票，从而实现了对罕见病的识别和分类任务的改进。 |
| [^67] | [Human-centered NLP Fact-checking: Co-Designing with Fact-checkers using Matchmaking for AI.](http://arxiv.org/abs/2308.07213) | 本研究使用AI的匹配设计方法，通过与专业事实核查员的合作设计，发现并解决事实核查员与技术之间的差距。合作设计会议产生了11个新的设计思路，包括提高效率和个性化的事实核查工具，帮助事实核查员准备未来的虚假信息，监测偏见，以及支持内部组织。 |
| [^68] | [OWQ: Lessons learned from activation outliers for weight quantization in large language models.](http://arxiv.org/abs/2306.02272) | 在大语言模型的推理中，要使用多个服务器贵重的GPU导致显著的成本障碍，OWQ提出的一种后训练量化方法可以在最小质量损失的情况下减少这种限制。它可以通过考虑激活离群值来确定权值量化误差的因素，并为易受攻击的权重分配高精度，具有与OPTQ相当的质量。 |
| [^69] | [Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery.](http://arxiv.org/abs/2305.14259) | 本文介绍了一种基于文献的发现方法，通过上下文化的学习生成新的科学方向，克服了标准方法在预测关联、忽略上下文等方面的局限性。模型使用了引文和知识图关系的网络，并使用大型语言模型进行评估，发现GPT4在生成创新思想方面表现出色。 |
| [^70] | [SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification.](http://arxiv.org/abs/2305.09781) | SpecInfer是一种LLM服务系统，通过利用推测推断和令牌树验证来加速生成式大语言模型的推断过程，显著减少了为它们提供服务所需的端到端延迟和计算要求，同时确保模型质量。 |
| [^71] | [Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings.](http://arxiv.org/abs/2305.02317) | VCoT是一种使用思维链激励和视觉语言组合递归地弥合时序数据中逻辑差距的新颖方法，其使用视觉引导生成合成的多模态填充以添加一致且新颖的信息，并减少需要时序推理的逻辑差距。 |
| [^72] | [Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement.](http://arxiv.org/abs/2304.14391) | 本文提出一种基于能量模型的零样本场景重新排列规划器，通过语言指导的空间概念来实现长指令以及在训练时从未见过的空间概念组合。本文的模型在指令导向操作基准测试以及组合指令基准测试中表现良好，优于基于语言表达的最先进方法，并且可以成功地解决之前从未见过的复杂指令和场景。 |
| [^73] | [Learning to Generate Questions by Enhancing Text Generation with Sentence Selection.](http://arxiv.org/abs/2212.12192) | 本研究提出了一种通过加强句子选择来增强文本生成的学习生成问题方法，该方法通过设计选择器和生成器两个模块，使模型更关注与答案相关的句子，并隐式结合局部信息和全局信息来生成问题。实验结果表明该方法在问题生成任务上优于强大的预训练模型。 |

# 详细

[^1]: AutoRT：大规模编排机器人代理的具身基础模型

    AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents. (arXiv:2401.12963v1 [cs.RO])

    [http://arxiv.org/abs/2401.12963](http://arxiv.org/abs/2401.12963)

    AutoRT是一个利用现有的基础模型来扩展机器人在未知场景中的部署的系统，通过利用视觉-语言模型和大型语言模型，提出多样化和新颖的指令，并有效地推理自主权和安全性的权衡。

    

    拥有语言、视觉和行动等功能的具身基础模型已经彻底改变了利用互联网规模的数据来推理有用任务的能力。然而，训练具身基础模型的一个关键挑战是缺乏基于物理世界的数据。在本文中，我们提出了AutoRT，一个利用现有的基础模型来扩展完全未知场景中操作机器人的部署的系统，只需要最少的人工监督。AutoRT利用视觉-语言模型(VLMs)实现场景理解和基础绑定，并进一步利用大型语言模型(LLMs)提出多样化和新颖的指令，供一组机器人执行。通过利用基础模型的知识来指导数据收集，AutoRT能够有效地推理自主权和安全性的权衡，同时显著扩大机器人学习的数据收集。我们演示了AutoRT向20多个机器人提议指令。

    Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multi
    
[^2]: 元提示：增强语言模型的任务无关支持

    Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding. (arXiv:2401.12954v1 [cs.CL])

    [http://arxiv.org/abs/2401.12954](http://arxiv.org/abs/2401.12954)

    元提示是一种支持技术，将单个语言模型转化为多面手指挥者，通过高级指令将复杂任务分解为子任务，并由不同的专家模型处理，最终提高效率。

    

    我们引入了元提示，一种有效的支持技术，旨在增强语言模型（LM）的功能。这种方法将单个LM转化为多面手指挥者，善于管理和整合多个独立的LM查询。通过使用高级指令，元提示指导LM将复杂任务分解为更小、更可管理的子任务。然后，这些子任务由相同LM的不同“专家”实例处理，每个实例都在特定的、定制化的指导下操作。该过程的核心是LM本身，它作为指挥者确保这些专家模型的输出无缝沟通和有效集成。它还利用其固有的批判性思维和强大的验证过程来完善和验证最终结果。这种协作提示方法使单个LM能够同时充当全面的编排者和多样化的专家小组，极大地提高了效率。

    We introduce meta-prompting, an effective scaffolding technique designed to enhance the functionality of language models (LMs). This approach transforms a single LM into a multi-faceted conductor, adept at managing and integrating multiple independent LM queries. By employing high-level instructions, meta-prompting guides the LM to break down complex tasks into smaller, more manageable subtasks. These subtasks are then handled by distinct "expert" instances of the same LM, each operating under specific, tailored instructions. Central to this process is the LM itself, in its role as the conductor, which ensures seamless communication and effective integration of the outputs from these expert models. It additionally employs its inherent critical thinking and robust verification processes to refine and authenticate the end result. This collaborative prompting approach empowers a single LM to simultaneously act as a comprehensive orchestrator and a panel of diverse experts, significantly e
    
[^3]: 基于Transformer模型在学习模拟结构递归方面尚未完美

    Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion. (arXiv:2401.12947v1 [cs.CL])

    [http://arxiv.org/abs/2401.12947](http://arxiv.org/abs/2401.12947)

    本论文研究了基于Transformer模型学习结构递归的能力，介绍了一个通用框架，将编程语言领域中的抽象概念与具体的序列建模问题和学习模型行为相连接。

    

    本论文研究了基于Transformer模型学习结构递归的能力。递归是自然语言和形式语言中的一种通用概念。结构递归在编程语言和形式数学任务中是至关重要的，而在这些任务中，符号工具目前在神经模型之上有优势，比如推断数据类型之间的语义关系和模拟程序行为。我们引入了一个通用框架，将编程语言领域中的抽象概念与具体的序列建模问题和学习模型行为相连接。该框架包括一个捕捉结构递归一般"语法"的表示，以及两种不同的框架来理解它们的"语义"——一种更符合编程语言视角的自然方式，以及一种有助于将该视角与底层Transformer模型的机械理解相结合的方式。

    This paper investigates the ability of transformer-based models to learn structural recursion from examples. Recursion is a universal concept in both natural and formal languages. Structural recursion is central to the programming language and formal mathematics tasks where symbolic tools currently excel beyond neural models, such as inferring semantic relations between datatypes and emulating program behavior. We introduce a general framework that nicely connects the abstract concepts of structural recursion in the programming language domain to concrete sequence modeling problems and learned models' behavior. The framework includes a representation that captures the general \textit{syntax} of structural recursion, coupled with two different frameworks for understanding their \textit{semantics} -- one that is more natural from a programming languages perspective and one that helps bridge that perspective with a mechanistic understanding of the underlying transformer architecture.  Wit
    
[^4]: 以往未见名字的多元文化名字识别

    Multicultural Name Recognition For Previously Unseen Names. (arXiv:2401.12941v1 [cs.CL])

    [http://arxiv.org/abs/2401.12941](http://arxiv.org/abs/2401.12941)

    这篇论文研究了以往未见名字的多元文化名字识别，通过改进训练数据和输入结构，提高了模型对来自不同文化背景的名字的识别能力，避免下游任务中的文化偏见。

    

    最先进的命名实体识别（NER）模型在提取属于位置、组织、时间和人员等标签的文本常见短语方面取得了令人印象深刻的能力。然而，通常依赖于在训练数据中见过特定实体以便对实体进行标记的NER系统在罕见或未知实体上表现不佳。本文试图提高对个人姓名的识别能力，这是一个多样化的类别，可能随时有人出生或改变姓名。为了避免下游任务出现基于文化背景的偏见，模型应该在各种背景的名字上表现良好。本文尝试通过实验英语Bi-LSTM姓名识别模型的训练数据和输入结构，比较模型在来自103个国家的名字上的表现，特别是在跨文化名字上。

    State of the art Named Entity Recognition (NER) models have achieved an impressive ability to extract common phrases from text that belong to labels such as location, organization, time, and person. However, typical NER systems that rely on having seen a specific entity in their training data in order to label an entity perform poorly on rare or unseen entities ta in order to label an entity perform poorly on rare or unseen entities (Derczynski et al., 2017). This paper attempts to improve recognition of person names, a diverse category that can grow any time someone is born or changes their name. In order for downstream tasks to not exhibit bias based on cultural background, a model should perform well on names from a variety of backgrounds. In this paper I experiment with the training data and input structure of an English Bi-LSTM name recognition model. I look at names from 103 countries to compare how well the model performs on names from different cultures, specifically in the con
    
[^5]: 红队针对视觉语言模型的研究

    Red Teaming Visual Language Models. (arXiv:2401.12915v1 [cs.AI])

    [http://arxiv.org/abs/2401.12915](http://arxiv.org/abs/2401.12915)

    本论文通过提供一个新颖的红队数据集RTVLM，评估了当前视觉语言模型VLMs在忠实度、隐私、安全和公平性方面的性能，并与GPT-4V进行了比较。结果显示，10个知名的VLMs在红队行动中存在不同程度的困难。

    

    视觉语言模型(VLMs)扩展了大型语言模型(LLMs)接受多模态输入的能力。由于已经验证LLMs可以通过特定测试用例产生有害或不准确的内容(称为红队行动)，VLMs在类似场景中的表现如何，特别是在文本和视觉输入的组合中，仍然是一个问题。为了探索这个问题，我们提供了一个新颖的红队数据集RTVLM，包含4个主要方面(忠实度、隐私、安全、公平)下的10个子任务(如图像误导、多模态越狱、脸部公平等)。我们的RTVLM是第一个从这四个不同方面评估当前VLMs的红队数据集。详细分析表明，10个知名的开放源代码VLMs在红队行动中遇到不同程度的困难，并且与GPT-4V相比，性能差距高达31%。此外，我们还使用RT简单地将红队行动对齐到LLaVA-v1.5上，使用监督微调(SFT)。

    VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RT
    
[^6]: 从理解到应用：大规模语言模型可解释性研究综述

    From Understanding to Utilization: A Survey on Explainability for Large Language Models. (arXiv:2401.12874v1 [cs.CL])

    [http://arxiv.org/abs/2401.12874](http://arxiv.org/abs/2401.12874)

    本综述论文研究了大规模语言模型(LLMs)可解释性的新兴领域，强调了在LLMs中增强可解释性的必要性，同时解决了广大公众对其信任和技术界对这些模型更深理解的需求。该综述对现有的可解释性方法进行分类，并讨论了它们在提高模型透明度和可靠性方面的应用，旨在弥合理论理解和实际应用之间的差距。

    

    本综述论文深入研究了大规模语言模型(LLMs)可解释性的新兴领域，这是自然语言处理中一个关键且具有挑战性的方面。LLMs在各种应用中发挥着关键作用，但其“黑盒”性质引发了对透明性和伦理使用的担忧。本文强调了在LLMs中增强可解释性的必要性，同时解决了广大公众对其信任和技术界对这些模型更深理解的需求。我们集中在预训练的基于Transformer的LLMs，如LLaMA，其规模和复杂性使其面临独特的可解释性挑战。我们的综述对现有的可解释性方法进行分类，并讨论了它们在提高模型透明度和可靠性方面的应用。我们还讨论了代表性的评价方法，强调了它们的优势和局限性。本综述的目标是弥合理论理解和实际应用之间的差距，提供从技术角度总结可解释性方法的全面视角。

    This survey paper delves into the burgeoning field of explainability for Large Language Models (LLMs), a critical yet challenging aspect of natural language processing. With LLMs playing a pivotal role in various applications, their "black-box" nature raises concerns about transparency and ethical use. This paper emphasizes the necessity for enhanced explainability in LLMs, addressing both the general public's trust and the technical community's need for a deeper understanding of these models. We concentrate on pre-trained Transformer-based LLMs, such as LLaMA, which present unique interpretability challenges due to their scale and complexity. Our review categorizes existing explainability methods and discusses their application in improving model transparency and reliability. We also discuss representative evaluation methods, highlighting their strengths and limitations. The goal of this survey is to bridge the gap between theoretical understanding and practical application, offering 
    
[^7]: 通过人的反馈改善机器翻译: 将质量估计作为奖励模型的探索

    Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model. (arXiv:2401.12873v1 [cs.CL])

    [http://arxiv.org/abs/2401.12873](http://arxiv.org/abs/2401.12873)

    本研究探索了利用质量估计作为奖励模型来预测人类偏好以改善机器翻译的潜力。我们发现基于质量估计的反馈训练存在过度优化问题，采用启发式规则来检测错误翻译并对质量估计模型进行惩罚以解决该问题。

    

    不充分建模人类偏好导致奖励模型在利用人的反馈提高翻译质量方面成为一个主要障碍。幸运的是，质量估计(QE)在过去两年中无需参考文献就能准确预测给定翻译的质量。在这项工作中，我们探讨了将QE模型作为奖励模型(基于QE的奖励模型)来预测人的偏好以进行反馈训练的潜力。我们首先发现了在基于QE的反馈训练中的过度优化问题，表现为奖励的增加而翻译质量下降。我们研究了这个问题，并认为QE模型的脆弱性可能导致错误翻译的高奖励，从而导致过度优化和错误传播。为解决这个问题，我们采用了一种简单而有效的方法，使用启发式规则检测错误翻译，并为QE模型添加了一个惩罚项。

    Insufficient modeling of human preferences within the reward model is a major obstacle for leveraging human feedback to improve translation quality. Fortunately, quality estimation (QE), which predicts the quality of a given translation without reference, has achieved impressive alignment with human evaluations in the last two years. In this work, we investigate the potential of employing the QE model as the reward model (the QE-based reward model) to predict human preferences for feedback training. We first identify the overoptimization problem during QE-based feedback training, manifested as an increase in reward while translation quality declines. We examine the problem and argue that the vulnerability of the QE model might lead to high rewards for incorrect translations, resulting in overoptimization and error propagation. To address the problem, we adopt a simple yet effective method that uses heuristic rules to detect the incorrect translations and assigns a penalty term to the Q
    
[^8]: KAM-CoT: 知识增强的多模式思维链推理

    KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning. (arXiv:2401.12863v1 [cs.CL])

    [http://arxiv.org/abs/2401.12863](http://arxiv.org/abs/2401.12863)

    KAM-CoT是一个知识增强的多模式思维链推理框架，可以通过引入外部知识图谱来提高模型对多模式任务的理解能力，并生成更准确的答案。

    

    大型语言模型（LLM）通过利用思维链（CoT）实现了在自然语言处理任务中令人印象深刻的性能，在最近的研究中，将LLMs扩展为多模式能力是一个有吸引力的方向，但会增加计算成本并需要大量硬件资源。为了应对这些挑战，我们提出了KAM-CoT框架，该框架融合了CoT推理、知识图谱（KGs）和多种模式，以全面理解多模式任务。KAM-CoT采用两阶段训练过程，通过KG基础生成有效的理由和答案。通过在推理过程中引入来自知识图谱的外部知识，模型获得了更深入的语境理解，减少了虚构和改善了答案的质量。这种知识增强的CoT推理使模型能够处理需要外部上下文的问题，并提供更有根据的答案。实验结果显示，KAM-CoT在性能上优于现有的方法。

    Large Language Models (LLMs) have demonstrated impressive performance in natural language processing tasks by leveraging chain of thought (CoT) that enables step-by-step thinking. Extending LLMs with multimodal capabilities is the recent interest, but incurs computational cost and requires substantial hardware resources. To address these challenges, we propose KAM-CoT a framework that integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities for a comprehensive understanding of multimodal tasks. KAM-CoT adopts a two-stage training process with KG grounding to generate effective rationales and answers. By incorporating external knowledge from KGs during reasoning, the model gains a deeper contextual understanding reducing hallucinations and enhancing the quality of answers. This knowledge-augmented CoT reasoning empowers the model to handle questions requiring external context, providing more informed answers. Experimental findings show KAM-CoT outperforms the state-of-t
    
[^9]: 能量的梯度流：实体对齐解码的通用高效方法

    Gradient Flow of Energy: A General and Efficient Approach for Entity Alignment Decoding. (arXiv:2401.12798v1 [cs.IR])

    [http://arxiv.org/abs/2401.12798](http://arxiv.org/abs/2401.12798)

    这篇论文介绍了一种能够解决实体对齐解码问题的新方法，该方法通过最小化能量来优化解码过程，以实现图同质性，并且仅依赖于实体嵌入，具有较高的通用性和效率。

    

    实体对齐（EA）是在集成多源知识图谱（KGs）中的关键过程，旨在识别这些图谱中的等价实体对。大多数现有方法将EA视为图表示学习任务，专注于增强图编码器。然而，在EA中解码过程-对于有效的操作和对齐准确性至关重要-得到了有限的关注，并且仍然针对特定数据集和模型架构进行定制，需要实体和额外的显式关系嵌入。这种特殊性限制了它的适用性，尤其是在基于GNN的模型中。为了填补这一空白，我们引入了一种新颖、通用和高效的EA解码方法，仅依赖于实体嵌入。我们的方法通过最小化狄利克雷能量来优化解码过程，在图内引导梯度流，以促进图同质性。梯度流的离散化产生了一种快速可扩展的方法，称为三元特征。

    Entity alignment (EA), a pivotal process in integrating multi-source Knowledge Graphs (KGs), seeks to identify equivalent entity pairs across these graphs. Most existing approaches regard EA as a graph representation learning task, concentrating on enhancing graph encoders. However, the decoding process in EA - essential for effective operation and alignment accuracy - has received limited attention and remains tailored to specific datasets and model architectures, necessitating both entity and additional explicit relation embeddings. This specificity limits its applicability, particularly in GNN-based models. To address this gap, we introduce a novel, generalized, and efficient decoding approach for EA, relying solely on entity embeddings. Our method optimizes the decoding process by minimizing Dirichlet energy, leading to the gradient flow within the graph, to promote graph homophily. The discretization of the gradient flow produces a fast and scalable approach, termed Triple Feature
    
[^10]: 通过不确定性量化对LLMs进行基准测试

    Benchmarking LLMs via Uncertainty Quantification. (arXiv:2401.12794v1 [cs.CL])

    [http://arxiv.org/abs/2401.12794](http://arxiv.org/abs/2401.12794)

    这项研究提出了一种通过不确定性量化来对LLMs进行基准测试的方法，并引入了一种不确定性感知评估指标UAcc。研究结果显示，高准确度的LLMs可能具有较低的确定性，而大规模LLMs可能比较小规模LLMs更不确定。指令微调倾向于增加LLMs的不确定性。

    

    随着各个机构开源大型语言模型（LLMs）的增多，彰显了对综合评估方法的迫切需求。然而，当前的评估平台，如广为人知的HuggingFace开放的LLM排行榜，忽视了一个关键方面--不确定性，而不确定性对于全面评估LLMs至关重要。为了弥补这一差距，我们引入了一种新的LLMs基准测试方法，将不确定性量化集成进去。我们的研究涵盖了五个典型的自然语言处理任务中的八个LLMs（LLM系列）。此外，我们引入了一种考虑了预测准确性和预测不确定性的不确定性感知评估指标UAcc。我们的研究结果显示：I）准确度越高的LLMs可能会表现出较低的确定性；II）规模较大的LLMs可能与较小的LLMs相比显示出更大的不确定性；III）指令微调倾向于增加LLMs的不确定性。通过考虑不确定性，我们的方法可以更全面地评估LLMs的性能。

    The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves eight LLMs (LLM series) spanning five representative natural language processing tasks. Additionally, we introduce an uncertainty-aware evaluation metric, UAcc, which takes into account both prediction accuracy and prediction uncertainty. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. By taking uncertainty
    
[^11]: 多语种非自回归ASR与大型语言模型融合：一项全面研究

    Multilingual and Fully Non-Autoregressive ASR with Large Language Model Fusion: A Comprehensive Study. (arXiv:2401.12789v1 [cs.CL])

    [http://arxiv.org/abs/2401.12789](http://arxiv.org/abs/2401.12789)

    本研究提出了一种多语种非自回归ASR系统，通过融合大型语言模型，实现了在不同语言上的平均相对WER改善，并通过全面的消融研究分析了影响实际大规模语言模型融合语音识别系统效果的因素。

    

    在大型模型时代，解码的自回归特性经常导致延迟成为一个重要的瓶颈。我们提出了一种非自回归的语言模型融合ASR系统，有效地利用了加速器硬件的并行能力。我们的方法将通用语音模型（USM）和PaLM 2语言模型结合在每个片段的评分模式下，实现了在所有语言上的平均相对WER改善10.8%的FLEURS，并在YouTube字幕上实现了3.6%的改善。此外，我们的全面消融研究分析了关键参数，如LLM大小、上下文长度、词汇表大小和融合方法。例如，我们研究了LLM大小从128M到340B参数对ASR性能的影响。这项研究提供了对影响实际大规模语言模型融合语音识别系统效果的因素的有价值的见解。

    In the era of large models, the autoregressive nature of decoding often results in latency serving as a significant bottleneck. We propose a non-autoregressive LM-fused ASR system that effectively leverages the parallelization capabilities of accelerator hardware. Our approach combines the Universal Speech Model (USM) and the PaLM 2 language model in per-segment scoring mode, achieving an average relative WER improvement across all languages of 10.8% on FLEURS and 3.6% on YouTube captioning. Furthermore, our comprehensive ablation study analyzes key parameters such as LLM size, context length, vocabulary size, fusion methodology. For instance, we explore the impact of LLM size ranging from 128M to 340B parameters on ASR performance. This study provides valuable insights into the factors influencing the effectiveness of practical large-scale LM-fused speech recognition systems.
    
[^12]: What the Weight?! 零样本知识组合的统一框架

    What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition. (arXiv:2401.12756v1 [cs.CL])

    [http://arxiv.org/abs/2401.12756](http://arxiv.org/abs/2401.12756)

    本研究提出了一个新的零样本模块组合框架，统一了选择、加权和组合参数模块的各种变化。以领域知识和适配器层为场景，通过系统化的统一概念，进行了首次全面的零样本知识组合的基准研究。

    

    模型中所封装的知识是确定其在下游任务中最终性能的核心因素。自然语言处理领域的许多研究都集中在存储和调整不同类型知识的有效方法上，例如在专用的模块化结构中，以及如何通过学习额外的参数来有效地组合这些知识。然而，鉴于存在许多可能的选项，对于这些组合中涉及的机制缺乏全面的理解，因此目前仍不清楚应该使用哪些策略。为了填补这一研究空白，我们提出了一个新的零样本模块组合框架，它涵盖了现有的一些选择、加权和组合参数模块的变化，统一了这些概念。在聚焦领域知识和适配器层的情景下，我们的框架提供了一个系统化的统一概念，使我们能够进行首次全面的各种零样本知识组合的基准研究。

    The knowledge encapsulated in a model is the core factor determining its final performance on downstream tasks. Much research in NLP has focused on efficient methods for storing and adapting different types of knowledge, e.g., in dedicated modularized structures, and on how to effectively combine these, e.g., by learning additional parameters. However, given the many possible options, a thorough understanding of the mechanisms involved in these compositions is missing, and hence it remains unclear which strategies to utilize. To address this research gap, we propose a novel framework for zero-shot module composition, which encompasses existing and some novel variations for selecting, weighting, and combining parameter modules under a single unified notion. Focusing on the scenario of domain knowledge and adapter layers, our framework provides a systematic unification of concepts, allowing us to conduct the first comprehensive benchmarking study of various zero-shot knowledge compositio
    
[^13]: 对有着非洲裔美国英语表达的论述的毒性和情感分析方法的偏见的全面观察

    A Comprehensive View of the Biases of Toxicity and Sentiment Analysis Methods Towards Utterances with African American English Expressions. (arXiv:2401.12720v1 [cs.CL])

    [http://arxiv.org/abs/2401.12720](http://arxiv.org/abs/2401.12720)

    这项研究全面观察了情感和毒性分析方法对带有非洲裔美国英语表达的话语中的偏见。实验结果发现由于AI模型的训练数据有限，导致这些方法在对待对“黑鬼”一词的再适应时存在偏差。

    

    语言是我们文化的动态方面，不同的技术/社区中表达就会有所变化。在线社交网络使得不同方言，包括非洲裔美国英语（AAE）得以传播和演变。然而，这种增加的使用也存在障碍。其中一个障碍是情感（Vader，TextBlob和Flair）和毒性（谷歌的Perspective和开源的Detoxify）方法在对带有AAE表达的话语上呈现偏见。以谷歌的Perspective为例来理解偏见。在这里，像“所有黑鬼都应该受到尊重地死去。警察谋杀我们。”这样的话语比“非洲裔美国人应该受到尊重地死去。警察谋杀我们。”的毒性得分更高。这种得分差异很可能是因为该工具不能理解对“黑鬼”一词的再适应。对这种偏见的一个解释是，AI模型是基于有限的数据集进行训练的，使用这样的术语在训练数据中更有可能发生。

    Language is a dynamic aspect of our culture that changes when expressed in different technologies/communities. Online social networks have enabled the diffusion and evolution of different dialects, including African American English (AAE). However, this increased usage is not without barriers. One particular barrier is how sentiment (Vader, TextBlob, and Flair) and toxicity (Google's Perspective and the open-source Detoxify) methods present biases towards utterances with AAE expressions. Consider Google's Perspective to understand bias. Here, an utterance such as ``All n*ggers deserve to die respectfully. The police murder us.'' it reaches a higher toxicity than ``African-Americans deserve to die respectfully. The police murder us.''. This score difference likely arises because the tool cannot understand the re-appropriation of the term ``n*gger''. One explanation for this bias is that AI models are trained on limited datasets, and using such a term in training data is more likely to a
    
[^14]: 生成无监督的言辞解释用于谣言验证

    Generating Unsupervised Abstractive Explanations for Rumour Verification. (arXiv:2401.12713v1 [cs.CL])

    [http://arxiv.org/abs/2401.12713](http://arxiv.org/abs/2401.12713)

    该论文利用无监督的方法，通过对言辞进行评分并生成解释性摘要，来增加谣言验证的信息丰富度和准确性。

    

    在社交媒体上进行谣言验证的任务涉及根据由该谣言引起的对话线程评估其真实性的问题。尽管之前的工作已经专注于预测真实性标签，但我们在这里重新制定了任务，以生成与模型相关的自由文本解释谣言的真实性。我们采用无监督的方法，首先利用事后可解释性方法对线程中最重要的帖子进行评分，然后使用这些帖子通过使用模板引导总结生成信息丰富的解释性摘要。为了评估解释性摘要的信息量，我们利用了大型语言模型的少样本学习能力。我们的实验表明，语言模型在评估摘要时可以与人类达到类似的一致性。重要的是，我们证明了解释性的概括摘要比仅使用线程中排名最高的帖子更具信息量，并更好地反映了预测的谣言真实性。

    The task of rumour verification in social media concerns assessing the veracity of a claim on the basis of conversation threads that result from it. While previous work has focused on predicting a veracity label, here we reformulate the task to generate model-centric, free-text explanations of a rumour's veracity. We follow an unsupervised approach by first utilising post-hoc explainability methods to score the most important posts within a thread and then we use these posts to generate informative explanatory summaries by employing template-guided summarisation. To evaluate the informativeness of the explanatory summaries, we exploit the few-shot learning capabilities of a large language model (LLM). Our experiments show that LLMs can have similar agreement to humans in evaluating summaries. Importantly, we show that explanatory abstractive summaries are more informative and better reflect the predicted rumour veracity than just using the highest ranking posts in the thread.
    
[^15]: 基于能量的自动化模型评估

    Energy-based Automated Model Evaluation. (arXiv:2401.12689v1 [cs.LG])

    [http://arxiv.org/abs/2401.12689](http://arxiv.org/abs/2401.12689)

    提出了一种基于能量的自动化模型评估方法，通过建立关于个体样本相关信息的元分布统计量，能够更高效和有效地评估机器学习模型的性能，解决了AutoEval框架中的过度自信、存储和计算成本高等问题。

    

    传统的机器学习模型评估协议依赖于标记的、假设独立同分布的测试数据集，而这在实际应用中往往并不常见。自动模型评估（AutoEval）提出了一种替代传统工作流程的方法，通过形成一个接近预测性能的测试管线，而无需真实标签的存在。尽管AutoEval框架近年来取得了一些成功，但仍存在过度自信、存储和计算成本高的问题。因此，我们提出了一种新颖的度量方式——元分布能量（MDE），它可以使AutoEval框架更加高效和有效。MDE的核心是建立一个关于个体样本相关信息（能量）的元分布统计量，然后通过基于能量的学习提供更平滑的表示能力。我们通过将MDE与分类损失相连接，进一步提供了理论洞见。我们还提供了大量实验证据来验证我们的方法。

    The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive
    
[^16]: 上下文的重要性：通过图结构化知识上下文推动开放式答案生成的边界

    Context Matters: Pushing the Boundaries of Open-Ended Answer Generation with Graph-Structured Knowledge Context. (arXiv:2401.12671v1 [cs.CL])

    [http://arxiv.org/abs/2401.12671](http://arxiv.org/abs/2401.12671)

    本论文介绍了一种结合图驱动的上下文检索和知识图结构增强的框架，通过提高LLMs的能力，尤其是在特定领域的社区问答平台上，更好地回答开放式问题。

    

    在不断发展的人工智能领域中，通过大型语言模型（LLMs）来构建上下文丰富、有意义的回答至关重要。研究人员越来越意识到当LLMs的参数较少时，尝试提供合适答案给开放式问题时会遇到的挑战。为了解决这些障碍，将先进的策略与丰富的外部领域知识与LLMs相结合，可以显著提升答案的质量。本论文介绍了一种新颖的框架，将基于图的上下文检索与知识图结构增强相结合，提高了LLMs的能力，特别适用于特定领域的社区问答平台，如AskUbuntu、Unix和ServerFault。我们对不同参数大小的各种LLMs进行实验，评估它们在开放式问题的回答中的知识确定能力和事实准确性。我们的方法GraphContextGen在基于文本的现有方法上持续优于其他方法。

    In the continuously advancing AI landscape, crafting context-rich and meaningful responses via Large Language Models (LLMs) is essential. Researchers are becoming more aware of the challenges that LLMs with fewer parameters encounter when trying to provide suitable answers to open-ended questions. To address these hurdles, the integration of cutting-edge strategies, augmentation of rich external domain knowledge to LLMs, offers significant improvements. This paper introduces a novel framework that combines graph-driven context retrieval in conjunction to knowledge graphs based enhancement, honing the proficiency of LLMs, especially in domain specific community question answering platforms like AskUbuntu, Unix, and ServerFault. We conduct experiments on various LLMs with different parameter sizes to evaluate their ability to ground knowledge and determine factual accuracy in answers to open-ended questions. Our methodology GraphContextGen consistently outperforms dominant text-based ret
    
[^17]: 对Makelov等人(2023)的《可解释性错觉》论点的回应

    A Reply to Makelov et al. (2023)'s "Interpretability Illusion" Arguments. (arXiv:2401.12631v1 [cs.LG])

    [http://arxiv.org/abs/2401.12631](http://arxiv.org/abs/2401.12631)

    本文回应了Makelov等人(2023)的论文，该论文评述了子空间交换干预方法的"解释性错觉"问题。我们指出，所谓的"解释性错觉"可以包括直观和可取的解释，而Makelov等人(2023)发现的"错觉"是他们训练和评估范例的产物。尽管我们不同意他们的核心表述，但他们的例子和讨论推动了可解释性领域的发展。

    

    我们回应了Makelov等人(2023)的最新论文，该论文评述了诸如分布式对齐搜索(DAS; Geiger等人，2023)这样的子空间交换干预方法，并声称这些方法可能引起"解释性错觉"。我们首先回顾了Makelov等人(2023)对"解释性错觉"的技术概念，然后展示了即使直观和可取的解释在这个意义上也可能成为错觉。因此，他们发现"错觉"的方法可能会拒绝他们认为"非错觉"的解释。接着，我们认为Makelov等人(2023)在实践中看到的"错觉"是他们训练和评估范例的产物。最后，我们强调，尽管我们不同意他们的核心表述，但Makelov等人(2023)的例子和讨论无疑推动了可解释性领域的发展。

    We respond to the recent paper by Makelov et al. (2023), which reviews subspace interchange intervention methods like distributed alignment search (DAS; Geiger et al. 2023) and claims that these methods potentially cause "interpretability illusions". We first review Makelov et al. (2023)'s technical notion of what an "interpretability illusion" is, and then we show that even intuitive and desirable explanations can qualify as illusions in this sense. As a result, their method of discovering "illusions" can reject explanations they consider "non-illusory". We then argue that the illusions Makelov et al. (2023) see in practice are artifacts of their training and evaluation paradigms. We close by emphasizing that, though we disagree with their core characterization, Makelov et al. (2023)'s examples and discussion have undoubtedly pushed the field of interpretability forward.
    
[^18]: SLANG: 大型语言模型对新概念的理解

    SLANG: New Concept Comprehension of Large Language Models. (arXiv:2401.12585v1 [cs.CL])

    [http://arxiv.org/abs/2401.12585](http://arxiv.org/abs/2401.12585)

    本研究提出了一个新的基准SLANG，旨在增强大型语言模型LLMs对互联网上新概念的理解能力，同时提出了一种基于因果推断的基准方法FOCUS，能帮助LLMs更好地理解新的短语和用法模式。

    

    语言的动态性，尤其在互联网上的俚语和表情包等方面的体现，给大型语言模型（LLMs）的适应性带来了严峻挑战。传统上，这些模型通常仅绑定在静态数据集上，很难跟上在线社区中快速语言进化的步伐。本研究解决了弥合这一差距的迫切需求，旨在增强LLMs对互联网上新概念的理解能力，同时避免高成本和不切实际的持续重训练。为应对这个问题，我们提出了一个新的评估LLMs在理解新兴语言趋势方面能力的基准 - SLANG，并提出了一种基于因果推断的基准方法 FOCUS，它能增强LLMs对新的短语和用法模式的理解。该方法包括对语言转变的真实世界实例进行详细研究，作为背景依据，以形成更精确和具有上下文相关性的新连接。

    The dynamic nature of language, particularly evident in the realm of slang and memes on the Internet, poses serious challenges to the adaptability of large language models (LLMs). Traditionally anchored to static datasets, these models often struggle to keep up with the rapid linguistic evolution characteristic of online communities. This research addresses the critical need to bridge this gap, aiming to enhance LLMs' comprehension of evolving new concepts on the internet, without the high cost and impracticality of continual retraining. To address this issue, we propose a new benchmark $\textbf{SLANG}$ to assess LLMs' proficiency in comprehending emerging linguistic trends and a baseline approach $\textbf{FOCUS}$, which uses causal inference to enhance LLMs to understand new phrases and usage patterns. This approach involves scrutinizing real-world instances of linguistic shifts, serving as contextual beacons, to form more precise and contextually relevant connections between newly em
    
[^19]: LLMCheckup：通过可解释性工具对大型语言模型进行对话式检查

    LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools. (arXiv:2401.12576v1 [cs.CL])

    [http://arxiv.org/abs/2401.12576](http://arxiv.org/abs/2401.12576)

    LLMCheckup是一个可解释性工具，通过连接大型语言模型与可解释的AI工具，使用户能够与模型进行对话，生成自我解释并提供建议。

    

    提供以对话形式进行解释的可解释性工具已经证明在增强用户理解方面具有效果，因为一次性解释有时无法提供足够的信息给用户。然而，当前基于对话的解释方案需要许多依赖项，并且不容易转移到它们未设计的任务上。通过LLMCheckup，我们提供了一个易于访问的工具，允许用户与任何最新的大型语言模型（LLM）进行对话以了解其行为。我们使LLMs能够自行生成所有解释，并通过与一系列可解释性AI（XAI）工具（例如特征归因、基于嵌入的相似性以及反事实和基于理由生成的提示策略）连接，以完成意图识别而无需微调。LLM（自我）解释以交互对话的形式呈现，支持后续问题和生成建议。

    Interpretability tools that offer explanations in the form of a dialogue have demonstrated their efficacy in enhancing users' understanding, as one-off explanations may occasionally fall short in providing sufficient information to the user. Current solutions for dialogue-based explanations, however, require many dependencies and are not easily transferable to tasks they were not designed for. With LLMCheckup, we present an easily accessible tool that allows users to chat with any state-of-the-art large language model (LLM) about its behavior. We enable LLMs to generate all explanations by themselves and take care of intent recognition without fine-tuning, by connecting them with a broad spectrum of Explainable AI (XAI) tools, e.g. feature attributions, embedding-based similarity, and prompting strategies for counterfactual and rationale generation. LLM (self-)explanations are presented as an interactive dialogue that supports follow-up questions and generates suggestions. LLMCheckup p
    
[^20]: 基于大型语言模型的自动事实核查气候变化声明

    Automated Fact-Checking of Climate Change Claims with Large Language Models. (arXiv:2401.12566v1 [cs.CL])

    [http://arxiv.org/abs/2401.12566](http://arxiv.org/abs/2401.12566)

    Climinator是一种基于大型语言模型的工具，用于自动核查气候变化声明。它利用多种科学来源的信息，采用调解者-倡导者框架，有效综合不同观点，从而得出基于科学、事实的评估结果。

    

    本文介绍了Climinator，一种新颖的基于人工智能的工具，用于自动化核查气候变化声明的事实。Climinator利用大量的大型语言模型（LLMs），并以IPCC报告和同行评审的科学文献等权威来源为基础，采用了一种创新的调解者-倡导者框架。该设计使Climinator能够有效地综合不同的科学观点，从而得出稳健的、基于证据的评估结果。我们的模型在从Climate Feedback和Skeptical Science收集的声明进行测试时表现出了显著的准确性。值得注意的是，在我们的框架中结合具有气候科学否定观点的倡导者时，Climinator的迭代辩论过程可靠地趋向于科学共识，突显了它在将不同观点协调为基于科学、事实性结论方面的能力。虽然我们的研究存在一定的限制，并需要谨慎解释，但我们的方法具有重要潜力。

    This paper presents Climinator, a novel AI-based tool designed to automate the fact-checking of climate change claims. Utilizing an array of Large Language Models (LLMs) informed by authoritative sources like the IPCC reports and peer-reviewed scientific literature, Climinator employs an innovative Mediator-Advocate framework. This design allows Climinator to effectively synthesize varying scientific perspectives, leading to robust, evidence-based evaluations. Our model demonstrates remarkable accuracy when testing claims collected from Climate Feedback and Skeptical Science. Notably, when integrating an advocate with a climate science denial perspective in our framework, Climinator's iterative debate process reliably converges towards scientific consensus, underscoring its adeptness at reconciling diverse viewpoints into science-based, factual conclusions. While our research is subject to certain limitations and necessitates careful interpretation, our approach holds significant poten
    
[^21]: DREditor:一种用于构建特定领域密集检索模型的时间高效方法

    DREditor: An Time-efficient Approach for Building a Domain-specific Dense Retrieval Model. (arXiv:2401.12540v1 [cs.IR])

    [http://arxiv.org/abs/2401.12540](http://arxiv.org/abs/2401.12540)

    DREditor是一种时间高效的方法，通过直接校准现有的密集检索模型的输出嵌入，使用线性映射和编辑操作符来编辑匹配规则，以提高特定领域的检索效率。

    

    在不同行业中，有效部署密集检索模型变得越来越重要。这在企业搜索服务中尤为如此，因为需求不同领域的不同企业的时间需求不同。出于这个原因，我们开发了一种名为DREditor的时间高效方法，用于编辑现成的密集检索模型的匹配规则，以适应特定领域。通过直接校准模型的输出嵌入，使用一种高效而有效的线性映射来实现这一点。这种映射是由解决一个特殊构建的最小二乘问题获得的编辑操作符驱动的。与通过长时间微调进行隐式规则修改相比，我们的实验结果表明，DREditor在不同的特定领域数据集、数据集来源、检索模型和计算设备上都提供了显著优势。它始终可以提高时间效率100-300倍，同时保持不变。

    Deploying dense retrieval models efficiently is becoming increasingly important across various industries. This is especially true for enterprise search services, where customizing search engines to meet the time demands of different enterprises in different domains is crucial. Motivated by this, we develop a time-efficient approach called DREditor to edit the matching rule of an off-the-shelf dense retrieval model to suit a specific domain. This is achieved by directly calibrating the output embeddings of the model using an efficient and effective linear mapping. This mapping is powered by an edit operator that is obtained by solving a specially constructed least squares problem. Compared to implicit rule modification via long-time finetuning, our experimental results show that DREditor provides significant advantages on different domain-specific datasets, dataset sources, retrieval models, and computing devices. It consistently enhances time efficiency by 100-300 times while maintain
    
[^22]: BiTA: 大语言模型中无损加速的双向调整

    BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models. (arXiv:2401.12522v1 [cs.CL])

    [http://arxiv.org/abs/2401.12522](http://arxiv.org/abs/2401.12522)

    BiTA是一种用于大语言模型的创新方法，通过双向调整实现了无损加速。它采用简化的半自回归生成和草稿验证，通过高效的基于树的解码同时进行候选生成和验证，提高了推理效率。这种方法不需要额外的辅助模型或显著的额外内存开销。

    

    大型语言模型（LLMs）通常在推理过程中使用自回归生成，导致高内存带宽需求和延迟延长。为了减轻这种效率低下的问题，我们提出了一种创新方法——双向调整以实现无损加速（BiTA），通过简化的半自回归生成和草稿验证来加速LLMs。受启发于提示调整的概念，我们使用一种参数高效的设计，称为双向调整，来增强LLMs在半自回归生成方面的能力。采用高效的基于树的解码，模型可以同时进行草稿候选生成和验证，确保输出结果与它们的自回归对应物在贪婪抽样下完全相同。BiTA作为一个轻量级的插件模块，可以无缝增强现有LLMs的推理效率，而无需额外的辅助模型或承担显著的额外内存开销。通过应用提出的BiTA，LLaMA-2-70B-Chat实现了

    Large language models (LLMs) commonly employ autoregressive generation during inference, leading to high memory bandwidth demand and consequently extended latency. To mitigate this inefficiency, we present Bi-directional Tuning for lossless Acceleration (BiTA), an innovative method expediting LLMs via streamlined semi-autoregressive generation and draft verification. Inspired by the concept of prompt tuning, we enhance LLMs with a parameter-efficient design called bi-directional tuning for the capability in semi-autoregressive generation. Employing efficient tree-based decoding, the models perform draft candidate generation and verification in parallel, ensuring outputs identical to their autoregressive counterparts under greedy sampling. BiTA serves as a lightweight plug-in module, seamlessly boosting the inference efficiency of existing LLMs without requiring additional assistance models or incurring significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat achieve
    
[^23]: 对优惠贸易协定的非结构化数据内容进行关键信息检索的研究

    Key Information Retrieval to Classify the Unstructured Data Content of Preferential Trade Agreements. (arXiv:2401.12520v1 [cs.CL])

    [http://arxiv.org/abs/2401.12520](http://arxiv.org/abs/2401.12520)

    本论文介绍了一种用于长文本分类和预测的新方法，通过嵌入技术对长文本进行压缩，然后采用双向编码器表示来自Transformers的嵌入方法进行文本分类训练，实验结果显示在优惠贸易协定的长文本分类方面取得了显著的性能提升。

    

    随着文本数据的迅速增长，预测长文本已经成为自然语言处理领域的重要挑战。传统的文本预测方法在处理长文本时遇到困难，主要是由于冗余和无关信息的存在，这影响了模型从文本中捕捉重要见解的能力。为了解决这个问题，我们提出了一种新的长文本分类和预测方法。首先，我们采用嵌入技术来对长文本进行压缩，以减少其中的冗余信息。随后，我们使用双向编码器表示来自Transformers（BERT）的嵌入方法进行文本分类训练。实验结果表明，我们的方法在优惠贸易协定的长文本分类方面实现了显著的性能提升。此外，通过嵌入方法对文本进行压缩不仅增强了预测表现，而且有助于提取关键信息。

    With the rapid proliferation of textual data, predicting long texts has emerged as a significant challenge in the domain of natural language processing. Traditional text prediction methods encounter substantial difficulties when grappling with long texts, primarily due to the presence of redundant and irrelevant information, which impedes the model's capacity to capture pivotal insights from the text. To address this issue, we introduce a novel approach to long-text classification and prediction. Initially, we employ embedding techniques to condense the long texts, aiming to diminish the redundancy therein. Subsequently,the Bidirectional Encoder Representations from Transformers (BERT) embedding method is utilized for text classification training. Experimental outcomes indicate that our method realizes considerable performance enhancements in classifying long texts of Preferential Trade Agreements. Furthermore, the condensation of text through embedding methods not only augments predic
    
[^24]: 比较以人为中心的语言建模：模拟群体、个体特点还是两者兼顾？

    Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?. (arXiv:2401.12492v1 [cs.CL])

    [http://arxiv.org/abs/2401.12492](http://arxiv.org/abs/2401.12492)

    本研究比较了以群体属性、个体用户和组合方法来模拟人的上下文。合并群体和个体特征显著提高了用户级回归任务的性能，而模拟个体用户则显著提高了单个文档级分类任务的性能。

    

    自然语言处理在将人的上下文纳入其模型中取得了进展，但使用群体属性（如45岁以上的人群）还是模拟个体人物更有效的问题尚未确定。群体属性在技术上更容易实现，但是过于粗糙：并非所有45岁以上的人都以相同的方式书写。相反，模拟个体人物能够捕捉每个人身份的复杂性，允许更个性化的表示，但我们可能需要模拟无限数量的用户并且需要可能无法获取的数据。我们比较了通过群体属性、个体用户和组合方法来模拟人的上下文。将群体和个体特征结合起来，显著提高了基于用户文档的用户级回归任务（如年龄估计或个性评估）的性能。模拟个体用户显著提高了单个文档级分类任务（如立场和主题检测）的性能。

    Natural language processing has made progress in incorporating human context into its models, but whether it is more effective to use group-wise attributes (e.g., over-45-year-olds) or model individuals remains open. Group attributes are technically easier but coarse: not all 45-year-olds write the same way. In contrast, modeling individuals captures the complexity of each person's identity. It allows for a more personalized representation, but we may have to model an infinite number of users and require data that may be impossible to get. We compare modeling human context via group attributes, individual users, and combined approaches. Combining group and individual features significantly benefits user-level regression tasks like age estimation or personality assessment from a user's documents. Modeling individual users significantly improves the performance of single document-level classification tasks like stance and topic detection. We also find that individual-user modeling does w
    
[^25]: 评估和理解大型语言模型中的创造力

    Assessing and Understanding Creativity in Large Language Models. (arXiv:2401.12491v1 [cs.CL])

    [http://arxiv.org/abs/2401.12491](http://arxiv.org/abs/2401.12491)

    本文旨在建立一个评估大型语言模型（LLM）创造力水平的高效框架，并提出了评估方法和一个包含700个问题的全面数据集。研究发现创造力水平受到任务差异和LLM模型参数的影响。

    

    在自然语言处理领域，大型语言模型（LLM）的快速发展引起了越来越多的关注。LLMs在各种任务中展现出了高水平的创造力，但评估这种创造力的方法尚不完善。评估LLM的创造力需要考虑与人类的差异，需要进行多维度的测量，同时平衡准确性和效率。本文旨在建立一个评估LLM创造力水平的高效框架。通过改进的托兰斯创造性思维测试的改编，本研究评估了各种LLM在7个任务中的创造性表现，强调了流畅度、灵活性、独创性和丰富性等4个标准。在这个背景下，我们开发了一个包含700个问题的全面数据集，用于测试和基于LLM的评估方法。此外，本研究还对LLM对各种提示和角色扮演情境的反应进行了新颖的分析。我们发现创造力的水平取决于任务的不同，同时也受到LLM的模型和参数的影响。

    In the field of natural language processing, the rapid development of large language model (LLM) has attracted more and more attention. LLMs have shown a high level of creativity in various tasks, but the methods for assessing such creativity are inadequate. The assessment of LLM creativity needs to consider differences from humans, requiring multi-dimensional measurement while balancing accuracy and efficiency. This paper aims to establish an efficient framework for assessing the level of creativity in LLMs. By adapting the modified Torrance Tests of Creative Thinking, the research evaluates the creative performance of various LLMs across 7 tasks, emphasizing 4 criteria including Fluency, Flexibility, Originality, and Elaboration. In this context, we develop a comprehensive dataset of 700 questions for testing and an LLM-based evaluation method. In addition, this study presents a novel analysis of LLMs' responses to diverse prompts and role-play situations. We found that the creativit
    
[^26]: 大型语言模型是所有字符的叠加：通过自我对齐实现任意角色扮演

    Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment. (arXiv:2401.12474v1 [cs.CL])

    [http://arxiv.org/abs/2401.12474](http://arxiv.org/abs/2401.12474)

    本文介绍了一种名为Ditto的角色扮演自我对齐方法，通过对角色知识的利用，使大型语言模型能够模拟角色扮演对话，从而增强其角色扮演能力。实验证明，Ditto在角色扮演基准和MT-Bench的评估中取得了出色的结果。

    

    大量的工作已经投入到通过模拟专有对手来增强开源大型语言模型（LLMs）的角色扮演能力。然而，我们认为LLMs本质上具有角色扮演能力，因为它们在广泛的训练语料库中蕴含了对角色和潜在对话的广泛了解。因此，在这项研究中，我们引入了Ditto，一种用于角色扮演的自我对齐方法。Ditto利用角色知识，鼓励LLM按照指令模拟角色扮演对话，作为阅读理解的一种变体。该方法创建了一个包含4000个字符的角色扮演训练集，相对于目前可用数据集的角色数量增加了十倍。随后，我们使用这个自动生成的数据集对LLM进行微调，以增强其角色扮演能力。通过评估我们精心构建且可复制的角色扮演基准和MT-Bench的角色扮演子集，Ditto在各项指标上表现出色。

    Considerable efforts have been invested in augmenting the role-playing proficiency of open-source large language models (LLMs) by emulating proprietary counterparts. Nevertheless, we posit that LLMs inherently harbor role-play capabilities, owing to the extensive knowledge of characters and potential dialogues ingrained in their vast training corpora. Thus, in this study, we introduce Ditto, a self-alignment method for role-play. Ditto capitalizes on character knowledge, encouraging an instruction-following LLM to simulate role-play dialogues as a variant of reading comprehension. This method creates a role-play training set comprising 4,000 characters, surpassing the scale of currently available datasets by tenfold regarding the number of roles. Subsequently, we fine-tune the LLM using this self-generated dataset to augment its role-playing capabilities. Upon evaluating our meticulously constructed and reproducible role-play benchmark and the roleplay subset of MT-Bench, Ditto, in var
    
[^27]: 对于蒸馏模型中的对比学习

    Contrastive Learning in Distilled Models. (arXiv:2401.12472v1 [cs.CL])

    [http://arxiv.org/abs/2401.12472](http://arxiv.org/abs/2401.12472)

    本论文介绍了一个基于对比学习的适用于蒸馏模型的方法，该方法通过对DistilBERT模型进行改进，在语义文本相似性上取得了显著的改进，且生成的DistilFace模型具有轻量级的特点。

    

    像BERT这样的自然语言处理模型能够为下游NLP任务提供最先进的词嵌入。然而，这些模型尚未在语义文本相似性上表现出色，并且可能过于庞大，无法部署为轻量级边缘应用程序。我们希望将一种合适的对比学习方法应用于基于知识蒸馏的模型DistilBERT，并根据SimCSE论文对模型架构进行改进，以解决这两个问题。我们最终的轻量级模型DistilFace在STS任务上的Spearman相关性平均达到72.1，比BERT base提高了34.2％。

    Natural Language Processing models like BERT can provide state-of-the-art word embeddings for downstream NLP tasks. However, these models yet to perform well on Semantic Textual Similarity, and may be too large to be deployed as lightweight edge applications. We seek to apply a suitable contrastive learning method based on the SimCSE paper, to a model architecture adapted from a knowledge distillation based model, DistilBERT, to address these two issues. Our final lightweight model DistilFace achieves an average of 72.1 in Spearman's correlation on STS tasks, a 34.2 percent improvement over BERT base.
    
[^28]: 快速对抗训练与文本对抗攻击

    Fast Adversarial Training against Textual Adversarial Attacks. (arXiv:2401.12461v1 [cs.CL])

    [http://arxiv.org/abs/2401.12461](http://arxiv.org/abs/2401.12461)

    本研究提出了一种快速对抗训练（FAT）方法，用于提高自然语言处理模型在无同义词知识的情况下的对抗鲁棒性。该方法通过单步梯度上升在嵌入空间中生成对抗性示例，以加速训练过程。

    

    许多对抗性防御方法已被提出，以增强自然语言处理模型的对抗鲁棒性。然而，大多数方法引入了额外的预设语言知识，并假设攻击者使用的同义词候选词是可访问的，这是一个理想的假设。我们深入研究了嵌入空间中的对抗性训练，并从单步扰动生成和扰动初始化角度提出了一种快速对抗训练（FAT）方法，以改善在无同义词知识的情况下模型的鲁棒性。基于单步和多步梯度上升生成的对抗扰动相似的观察，FAT使用单步梯度上升在嵌入空间中生成对抗性示例，以加速训练过程。基于连续纪元中同一训练样本上生成的扰动相似的观察，FAT充分利用历史信息来初始化扰动。

    Many adversarial defense methods have been proposed to enhance the adversarial robustness of natural language processing models. However, most of them introduce additional pre-set linguistic knowledge and assume that the synonym candidates used by attackers are accessible, which is an ideal assumption. We delve into adversarial training in the embedding space and propose a Fast Adversarial Training (FAT) method to improve the model robustness in the synonym-unaware scenario from the perspective of single-step perturbation generation and perturbation initialization. Based on the observation that the adversarial perturbations crafted by single-step and multi-step gradient ascent are similar, FAT uses single-step gradient ascent to craft adversarial examples in the embedding space to expedite the training process. Based on the observation that the perturbations generated on the identical training sample in successive epochs are similar, FAT fully utilizes historical information when initi
    
[^29]: CIM-MLC:面向计算内存加速器的多级编译栈

    CIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory Accelerators. (arXiv:2401.12428v1 [cs.AR])

    [http://arxiv.org/abs/2401.12428](http://arxiv.org/abs/2401.12428)

    CIM-MLC是一个面向计算内存加速器的多级编译栈，为了支持各种CIM架构的潜力，该编译栈完全了解CIM架构细节和实现多样性，并提供灵活性来支持具有不同计算粒度的CIM架构。

    

    最近几年，各种计算内存(CIM)处理器被提出，展示出优于传统架构的性能。为了释放各种CIM架构的潜力，如设备精度、交叉栏大小和交叉栏数量，有必要开发完全了解CIM架构细节和实现多样性的编译工具。然而，由于当前流行的开源编译栈在架构支持方面的不足，现有的CIM设计要么手动部署网络，要么构建自己的编译器，这是耗时耗力的。尽管一些工作将特定的CIM设备编程接口公开给编译器，但它们通常被绑定到固定的CIM架构，缺乏灵活性来支持具有不同计算粒度的CIM架构。另一方面，现有的编译工作通常考虑了有限的操作类型的调度（例如交叉栏限制的矩阵-向量乘法）

    In recent years, various computing-in-memory (CIM) processors have been presented, showing superior performance over traditional architectures. To unleash the potential of various CIM architectures, such as device precision, crossbar size, and crossbar number, it is necessary to develop compilation tools that are fully aware of the CIM architectural details and implementation diversity. However, due to the lack of architectural support in current popular open-source compiling stacks, existing CIM designs either manually deploy networks or build their own compilers, which is time-consuming and labor-intensive. Although some works expose the specific CIM device programming interfaces to compilers, they are often bound to a fixed CIM architecture, lacking the flexibility to support the CIM architectures with different computing granularity. On the other hand, existing compilation works usually consider the scheduling of limited operation types (such as crossbar-bound matrix-vector multipl
    
[^30]: 视觉语言模型中被忽视的尾部

    The Neglected Tails of Vision-Language Models. (arXiv:2401.12425v1 [cs.CV])

    [http://arxiv.org/abs/2401.12425](http://arxiv.org/abs/2401.12425)

    本文通过分析预训练文本来衡量视觉语言模型中概念的频率，并发现流行的VLM数据集展示了长尾概念分布，这与按类别的准确率强烈相关。

    

    视觉语言模型（VLM）在零样本识别方面表现出色，但在视觉概念上的表现极不均衡。例如，尽管CLIP在ImageNet上具有令人印象深刻的平均零样本准确率（72.7％），但在十个概念（如gyromitra和night snake）上的准确率不到10％，这可能是因为这些概念在VLM的非均衡预训练数据中的表示不足。然而，评估这种不平衡是具有挑战性的，因为在VLM的大规模预训练数据中计算特定概念的频率是非常复杂的。我们的工作首次尝试使用分析预训练文本来测量概念频率。我们利用现成的语言模型来帮助计算包含给定概念的同义词的相关文本，并解决语言歧义。我们确认像LAION这样的流行的VLM数据集确实展示了长尾概念分布，并且这与按类别的准确率强烈相关。此外，当代的多模式系统，如视觉聊天机器人和文本-视觉推理模型，在这种长尾分布下经常难以达到高性能。

    Vision-language models (VLMs) excel in zero-shot recognition but exhibit drastically imbalanced performance across visual concepts. For example, CLIP, despite an impressive mean zero-shot accuracy on ImageNet (72.7%), yields $<$10% on ten concepts (e.g., gyromitra and night snake), presumably, because these concepts are under-represented in VLMs' imbalanced pretraining data. Yet, assessing this imbalance is challenging as it is non-trivial to calculate the frequency of specific concepts within VLMs' large-scale pretraining data. Our work makes the first attempt to measure the concept frequency by analyzing pretraining texts. We use off-the-shelf language models to help count relevant texts that contain synonyms of the given concepts and resolve linguistic ambiguity. We confirm that popular VLM datasets like LAION indeed exhibit long-tailed concept distributions, which strongly correlate with per-class accuracies. Further, contemporary multimodal systems, e.g., visual chatbots and text-
    
[^31]: 100个样本可以走多远？通过微小的多语言平行数据解锁全面的零样本跨语言翻译

    How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual Translation via Tiny Multi-Parallel Data. (arXiv:2401.12413v1 [cs.CL])

    [http://arxiv.org/abs/2401.12413](http://arxiv.org/abs/2401.12413)

    本文研究了如何通过仅有的少量微小多语言平行数据来增强以英语为中心的模型的零样本翻译能力，实现大幅度的非英文整体改进，并保持英文方向上的性能能力。研究表明，即使在随机抽取的少量方向集上进行微调，也可以获得可比较的整体改进。

    

    零样本翻译是一个开放问题，旨在在多语言机器翻译（MMT）中翻译训练过程中未见过的语言对。一种常见但资源消耗较大的解决方案是尽可能挖掘更多的翻译方向并添加到平行语料库中。本文展示了通过使用仅有的少量微小多语言平行数据来优化以英语为中心的模型的零样本能力。例如，在EC30数据集上，我们展示了仅使用100个多语言平行样本就能够实现+21.7 ChrF非英文整体改进（870个方向），同时保持在以英语为中心的方向上的能力。我们进一步研究了微调数据的规模效应和其转移能力。令人惊讶的是，我们的实证分析表明，即使是在一个小的、随机抽取的方向集（10%）上进行微调，也可以获得可比较的整体改进。此外，所得到的非英文性能与英文性能非常接近。

    Zero-shot translation is an open problem, aiming to translate between language pairs unseen during training in Multilingual Machine Translation (MMT). A common, albeit resource-consuming, solution is to mine as many translation directions as possible to add to the parallel corpus. In this paper, we show that the zero-shot capability of an English-centric model can be easily enhanced by fine-tuning with a very small amount of multi-parallel data. For example, on the EC30 dataset, we show that up to +21.7 ChrF non-English overall improvements (870 directions) can be achieved by using only 100 multi-parallel samples, meanwhile preserving capability in English-centric directions. We further study the size effect of fine-tuning data and its transfer capabilities. Surprisingly, our empirical analysis shows that comparable overall improvements can be achieved even through fine-tuning in a small, randomly sampled direction set (10\%). Also, the resulting non-English performance is quite close 
    
[^32]: 通过线性探测校准提高上下文学习

    Enhancing In-context Learning via Linear Probe Calibration. (arXiv:2401.12406v1 [cs.CL])

    [http://arxiv.org/abs/2401.12406](http://arxiv.org/abs/2401.12406)

    本研究提出了一种名为线性探测校准（LinC）的技术，通过校准模型的输出概率，显著提高了上下文学习（ICL）在生成预训练变压器（GPT）模型上的测试性能。

    

    上下文学习（ICL）是一种新的自然语言处理范式，利用生成预训练变压器（GPT）等模型。这种方法使用包含上下文演示的提示来为新的查询输入生成相应的输出。然而，在实际情况下应用ICL无法随着样本数量的增加而扩展，并且对不同的提示模板和演示排列缺乏鲁棒性。本文首先展示了使用ICL的GPT模型基于基于香农熵的新度量而导致不可靠的预测。然后，我们提出了一种称为线性探测校准（LinC）的新技术，它可以校准模型的输出概率，从而得到可靠的预测和改进的性能，且仅需要极少量的额外样本（仅需五个已标记的数据样本）。LinC显著提高了GPT模型在各种基准数据集上的ICL测试性能，平均改善效果很大。

    In-context learning (ICL) is a new paradigm for natural language processing that utilizes Generative Pre-trained Transformer (GPT)-like models. This approach uses prompts that include in-context demonstrations to generate the corresponding output for a new query input. However, applying ICL in real cases does not scale with the number of samples, and lacks robustness to different prompt templates and demonstration permutations. In this paper, we first show that GPT-like models using ICL result in unreliable predictions based on a new metric based on Shannon entropy. Then, to solve this problem, we propose a new technique called the Linear Probe Calibration (LinC), a method that calibrates the model's output probabilities, resulting in reliable predictions and improved performance, while requiring only minimal additional samples (as few as five labeled data samples). LinC significantly enhances the ICL test performance of GPT models on various benchmark datasets, with an average improve
    
[^33]: Reddit帖子的纵向情感分类

    Longitudinal Sentiment Classification of Reddit Posts. (arXiv:2401.12382v1 [cs.CL])

    [http://arxiv.org/abs/2401.12382](http://arxiv.org/abs/2401.12382)

    本研究对四所加拿大主要大学的学生在Reddit上撰写的帖子进行纵向情感分类。通过调整情感阈值，我们成功构建了分类器，能够将帖子情感分类为积极和消极类别，并且结果在不同大学数据集中一致。

    

    我们报告了对四所加拿大主要大学的学生撰写的Reddit帖子进行纵向情感分类的结果。我们使用帖子的文本，重点关注2020年至2023年之间的时间。通过精细调整情感阈值在[-0.075, 0.075]范围内，我们成功构建了能够将帖子情感分类为积极和消极类别的分类器。值得注意的是，我们的情感分类结果在四个大学数据集中是一致的。

    We report results of a longitudinal sentiment classification of Reddit posts written by students of four major Canadian universities. We work with the texts of the posts, concentrating on the years 2020-2023. By finely tuning a sentiment threshold to a range of [-0.075,0.075], we successfully built classifiers proficient in categorizing post sentiments into positive and negative categories. Noticeably, our sentiment classification results are consistent across the four university data sets.
    
[^34]: 为视力障碍学生开发基于NLP的计算机化测试指南

    Development of an NLP-driven computer-based test guide for visually impaired students. (arXiv:2401.12375v1 [cs.CL])

    [http://arxiv.org/abs/2401.12375](http://arxiv.org/abs/2401.12375)

    本文介绍了一种为视力障碍学生开发的基于NLP的计算机化测试指南，利用语音技术和NLP技术实时提供支持和帮助，并将文本问题和选项转化为机器可读的格式。

    

    近年来，自然语言处理（NLP）技术的进步改变了无障碍和独占性测试领域，特别是对于视力障碍学生（VIS）。计算机化测试（CBT）在过去的几年中已经显示出了在电子化考试方面的重要性，使考试过程更加简便，提供更快更准确的结果，并为考生提供更大的灵活性和可访问性。然而，视力障碍学生无法访问印刷文件，因此本文提出了一种面向视力障碍学生的基于NLP的计算机化测试指南。该系统采用语音技术预先训练的方法，为视力障碍学生实时提供支持和帮助。系统利用NLP技术将基于文本的问题和相关选项转化为机器可读的格式。随后，语音技术预先训练的模型处理转化后的文本，实现对视力障碍学生的支持。

    In recent years, advancements in Natural Language Processing (NLP) techniques have revolutionized the field of accessibility and exclusivity of testing, particularly for visually impaired students (VIS). CBT has shown in years back its relevance in terms of administering exams electronically, making the test process easier, providing quicker and more accurate results, and offering greater flexibility and accessibility for candidates. Yet, its relevance was not felt by the visually impaired students as they cannot access printed documents. Hence, in this paper, we present an NLP-driven Computer-Based Test guide for visually impaired students. It employs a speech technology pre-trained methods to provide real-time assistance and support to visually impaired students. The system utilizes NLP technologies to convert the text-based questions and the associated options in a machine-readable format. Subsequently, the speech technology pre-trained model processes the converted text enabling th
    
[^35]: 基于子图提取的反馈引导迭代调度在高级综合中的应用

    Subgraph Extraction-based Feedback-guided Iterative Scheduling for HLS. (arXiv:2401.12343v1 [cs.CL])

    [http://arxiv.org/abs/2401.12343](http://arxiv.org/abs/2401.12343)

    提出了一种基于反馈引导和子图提取的差分约束调度算法用于高级综合，可显著减少寄存器使用率。

    

    本文提出了一种新颖的反馈引导迭代差分约束调度算法ISDC，用于高级综合(HLS)。ISDC利用下游工具（如逻辑综合器）提供的子图提取的低级反馈，迭代优化HLS调度。技术创新包括：（1）增强的差分约束问题（SDC）形式，有效将低级反馈集成到线性规划(LP)问题中；（2）基于扇出和窗口的子图提取机制驱动反馈循环；（3）与广泛的下游工具和工艺设计工具包(PDK)兼容的无人介入的ISDC流程。评估结果显示，ISDC相对于工业级开源HLS工具，可以减少28.5%的寄存器使用率。

    This paper proposes ISDC, a novel feedback-guided iterative system of difference constraints (SDC) scheduling algorithm for high-level synthesis (HLS). ISDC leverages subgraph extraction-based low-level feedback from downstream tools like logic synthesizers to iteratively refine HLS scheduling. Technical innovations include: (1) An enhanced SDC formulation that effectively integrates low-level feedback into the linear-programming (LP) problem; (2) A fanout and window-based subgraph extraction mechanism driving the feedback cycle; (3) A no-human-in-loop ISDC flow compatible with a wide range of downstream tools and process design kits (PDKs). Evaluation shows that ISDC reduces register usage by 28.5% against an industrial-strength open-source HLS tool.
    
[^36]: 对多生成器、多领域和多语言机器生成文本检测的大型语言模型进行微调

    Fine-tuning Large Language Models for Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection. (arXiv:2401.12326v1 [cs.CL])

    [http://arxiv.org/abs/2401.12326](http://arxiv.org/abs/2401.12326)

    本文研究了对多生成器、多领域和多语言机器生成文本的大型语言模型进行微调的任务。实验结果显示，转换器模型特别是LoRA-RoBERTa在效果上超过了传统的机器学习方法，对于多语言环境下的机器生成文本识别，多数投票方法尤为有效。

    

    SemEval-2024任务8引入了从多种语言和领域的不同大型语言模型（LLMs）中识别机器生成文本的挑战。该任务由三个子任务组成：单语和多语言的二元分类（子任务A）、多类别分类（子任务B）以及混合文本检测（子任务C）。本文重点关注子任务A和B。每个子任务都有三个数据集用于训练、开发和测试。为了解决这个任务，采用了两种方法：1）使用传统的机器学习（ML）和自然语言预处理（NLP）进行特征提取，2）对文本分类进行大型语言模型的微调。结果表明，转换器模型，特别是LoRA-RoBERTa，在效果上超过了传统的机器学习方法，多数投票在多语言环境中识别机器生成文本方面尤为有效。

    SemEval-2024 Task 8 introduces the challenge of identifying machine-generated texts from diverse Large Language Models (LLMs) in various languages and domains. The task comprises three subtasks: binary classification in monolingual and multilingual (Subtask A), multi-class classification (Subtask B), and mixed text detection (Subtask C). This paper focuses on Subtask A & B. Each subtask is supported by three datasets for training, development, and testing. To tackle this task, two methods: 1) using traditional machine learning (ML) with natural language preprocessing (NLP) for feature extraction, and 2) fine-tuning LLMs for text classification. The results show that transformer models, particularly LoRA-RoBERTa, exceed traditional ML methods in effectiveness, with majority voting being particularly effective in multilingual contexts for identifying machine-generated texts.
    
[^37]: 廉价学习：最大化社会数据科学中语言模型的性能，使用最少的数据。

    Cheap Learning: Maximising Performance of Language Models for Social Data Science Using Minimal Data. (arXiv:2401.12295v1 [cs.CL])

    [http://arxiv.org/abs/2401.12295](http://arxiv.org/abs/2401.12295)

    本文回顾了“廉价”学习技术在社会科学中的应用，包括弱监督、迁移学习和提示工程。特别地，通过提示大规模语言模型，可以实现高准确性的性能。

    

    机器学习领域在构建新模型时，最近取得了降低标注训练数据要求的重要进展。这些“廉价”学习技术在社会科学领域具有巨大潜力，因为开发大型标注训练数据集通常是机器学习用于分析任务的实际障碍。在本文中，我们回顾了最近发展的三种“廉价”技术：弱监督、迁移学习和提示工程。对于后者，我们还回顾了大规模语言模型的零样本提示的特殊情况。针对每种技术，我们提供了工作原理的指南，并展示了它们在六个不同的实际社会科学应用程序中的应用情况（两个不同任务与三种不同数据集的组合）。我们展示了所有技术的良好性能，特别是我们演示了如何通过大规模语言模型的提示可以实现很高的准确性。

    The field of machine learning has recently made significant progress in reducing the requirements for labelled training data when building new models. These `cheaper' learning techniques hold significant potential for the social sciences, where development of large labelled training datasets is often a significant practical impediment to the use of machine learning for analytical tasks. In this article we review three `cheap' techniques that have developed in recent years: weak supervision, transfer learning and prompt engineering. For the latter, we also review the particular case of zero-shot prompting of large language models. For each technique we provide a guide of how it works and demonstrate its application across six different realistic social science applications (two different tasks paired with three different dataset makeups). We show good performance for all techniques, and in particular we demonstrate how prompting of large language models can achieve high accuracy at very
    
[^38]: GRATH: 大型语言模型的逐渐自我真实化方法

    GRATH: Gradual Self-Truthifying for Large Language Models. (arXiv:2401.12292v1 [cs.CL])

    [http://arxiv.org/abs/2401.12292](http://arxiv.org/abs/2401.12292)

    GRATH是一种逐步自我真实化的方法，用于提高大型语言模型的真实性。它通过使用领域外问题提示生成答案，并通过直接偏好优化进行自适应模型优化。GRATH在没有标注答案的情况下以自我监督的方式学习真实性，并通过迭代优化来逐步提升模型真实性。

    

    随着大型语言模型（LLMs）在真实世界应用中的部署越来越多，真实性对它们来说至关重要。然而，现有的LLMs在生成真实答案和内容方面仍然存在困难，如在TruthfulQA等基准上的表现不佳。为了解决这个问题，我们提出了GRAdual self-truTHifying (GRATH)，一种通过后处理方法提高LLMs真实性的新方法。GRATH利用领域外的问题提示生成相应的答案，并通过直接偏好优化进行自适应模型优化。在这个过程中，GRATH以无需标注答案的自我监督方式学习真实性。具体而言，GRATH首先通过提示LLM自身生成成对真实性训练数据，每对包含一个问题及其正确和错误答案。然后，使用直接偏好优化来微调模型，从答案对的差异中学习。随后，GRATH迭代地优化模型以逐渐提高真实性。

    Truthfulness is paramount for large language models (LLMs) as they are increasingly deployed in real-world applications. However, existing LLMs still struggle with generating truthful answers and content, as evidenced by their modest performance on benchmarks like TruthfulQA. To address this issue, we propose GRAdual self-truTHifying (GRATH), a novel post-processing method to enhance truthfulness of LLMs. GRATH utilizes out-of-domain question prompts to generate corresponding answers and adaptively optimizes the model via direct preference optimization (DPO). Note that during this process, GRATH learns truthfulness in a self-supervised manner without requiring annotated answers. In particular, GRATH first generates pairwise truthfulness training data by prompting the LLM itself, with each pair containing a question and its correct and incorrect answers. The model is then fine-tuned using DPO to learn from the difference between answer pairs. Subsequently, GRATH iteratively refines the 
    
[^39]: 交互的伦理问题：缓解LLMs中的安全威胁

    The Ethics of Interaction: Mitigating Security Threats in LLMs. (arXiv:2401.12273v1 [cs.CR])

    [http://arxiv.org/abs/2401.12273](http://arxiv.org/abs/2401.12273)

    本研究全面探讨了与语言学习模型（LLMs）面临的安全威胁相关的伦理挑战。分析了五种主要威胁的伦理后果，并强调了确保这些系统在伦理规范范围内运作的紧迫性。

    

    本文全面探讨了与语言学习模型（LLMs）面临的安全威胁相关的伦理挑战。这些复杂的数字存储库日益融入到我们的日常生活中，因此成为攻击的主要目标，可能危及其训练数据和数据源的机密性。本文深入研究了这些安全威胁对社会和个人隐私的微妙伦理影响。我们对五个主要威胁进行了详细分析：提示注入、越狱、个人可识别信息（PII）曝露、性别显露内容和基于仇恨的内容。我们不仅仅进行了识别，还评估了它们的关键伦理后果以及对强化防御策略的紧迫性。对LLMs的不断依赖凸显了确保这些系统在伦理规范范围内运作的重要性，特别是由于它们的滥用可能导致重大社会和个人伤害。我们提出了将这些系统概念化的要求。

    This paper comprehensively explores the ethical challenges arising from security threats to Language Learning Models (LLMs). These intricate digital repositories are increasingly integrated into our daily lives, making them prime targets for attacks that can compromise their training data and the confidentiality of their data sources. The paper delves into the nuanced ethical repercussions of such security threats on society and individual privacy. We scrutinize five major threats: prompt injection, jailbreaking, Personal Identifiable Information (PII) exposure, sexually explicit content, and hate based content, going beyond mere identification to assess their critical ethical consequences and the urgency they create for robust defensive strategies. The escalating reliance on LLMs underscores the crucial need for ensuring these systems operate within the bounds of ethical norms, particularly as their misuse can lead to significant societal and individual harm. We propose conceptualizin
    
[^40]: Orion-14B: 开源多语言大型语言模型

    Orion-14B: Open-source Multilingual Large Language Models. (arXiv:2401.12246v1 [cs.CL])

    [http://arxiv.org/abs/2401.12246](http://arxiv.org/abs/2401.12246)

    Orion-14B是一个具有140亿参数的开源多语言大型语言模型。在该研究中，我们采用数据调度方法对一个基础模型进行训练，使用了来自多种语言的2.5万亿个标记的多样化语料库。我们还对对话应用和其他特定用例进行了微调。评估结果显示，Orion-14B在广泛的任务中取得了领先的性能。我们将Orion-14B模型系列及其相关代码公开，以鼓励未来在这一领域的研究和实际应用。

    

    在这项研究中，我们介绍了Orion-14B，一个具有140亿参数的多语言大型语言模型集合。我们采用数据调度方法，在包括英语、中文、日语、韩语和其他语言的文本中，对一个基础模型进行了训练，使用了来自2.5万亿个标记的多样化语料库。此外，我们还针对对话应用和其他特定用例进行了一系列模型的微调。我们的评估结果表明，Orion-14B在广泛的任务领域中实现了领先的性能。我们将Orion-14B模型系列及其相关代码公开可访问，旨在激发未来在该领域的研究和实际应用。

    In this study, we introduce Orion-14B, a collection of multilingual large language models with 14 billion parameters. We utilize a data scheduling approach to train a foundational model on a diverse corpus of 2.5 trillion tokens, sourced from texts in English, Chinese, Japanese, Korean, and other languages. Additionally, we fine-tuned a series of models tailored for conversational applications and other specific use cases. Our evaluation results demonstrate that Orion-14B achieves state-of-the-art performance across a broad spectrum of tasks. We make the Orion-14B model family and its associated code publicly accessible https://github.com/OrionStarAI/Orion, aiming to inspire future research and practical applications in the field.
    
[^41]: 自动事实核查的索赔检测：关于单语、多语和跨语言研究的综述

    Claim Detection for Automated Fact-checking: A Survey on Monolingual, Multilingual and Cross-Lingual Research. (arXiv:2401.11969v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.11969](http://arxiv.org/abs/2401.11969)

    本综述调研了自动事实核查中索赔检测的现有工作，特别关注多语言数据和方法。这是一个具有挑战性但富有成果的研究方向，需要更通用的解决方案来对抗跨多语言和模态的不实信息。

    

    自动事实核查在过去几十年中引起了相当大的关注，原因是网络平台上虚假信息的传播增加了。这通常是作为一系列任务的序列来完成的，包括（i）检测在网上流传的句子，这些句子构成需要验证的索赔，然后是（ii）对这些索赔进行验证的过程。本综述重点讨论前者，讨论了现有的努力，旨在检测需要事实核查的索赔，特别关注多语言数据和方法。这是一个具有挑战性和富有成果的方向，由于问题的极其具有挑战性，在人类表现方面，现有方法离匹配人类表现还有很长的路要走。特别是，跨多个社交平台的信息传播以多种语言和模态表达，需要更加通用的解决方案来对抗不实信息。我们针对多语言不实信息，提供了一份全面的综述。

    Automated fact-checking has drawn considerable attention over the past few decades due to the increase in the diffusion of misinformation on online platforms. This is often carried out as a sequence of tasks comprising (i) the detection of sentences circulating in online platforms which constitute claims needing verification, followed by (ii) the verification process of those claims. This survey focuses on the former, by discussing existing efforts towards detecting claims needing fact-checking, with a particular focus on multilingual data and methods. This is a challenging and fertile direction where existing methods are yet far from matching human performance due to the profoundly challenging nature of the issue. Especially, the dissemination of information across multiple social platforms, articulated in multiple languages and modalities demands more generalized solutions for combating misinformation. Focusing on multilingual misinformation, we present a comprehensive survey of exis
    
[^42]: 通过检索示范进行上下文学习的语言模型：一项综述

    In-context Learning with Retrieved Demonstrations for Language Models: A Survey. (arXiv:2401.11624v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.11624](http://arxiv.org/abs/2401.11624)

    本综述调查了一种名为检索示范的方法，它通过使用特定于输入查询的示范来提高语言模型的少量样本情境学习（ICL）能力。这种方法不仅提高了学习效率和可扩展性，还减少了手动示例选择中的偏见。

    

    语言模型，特别是预训练的大型语言模型，已展示出卓越的能力，可以在输入上下文中进行少量样本的情境学习（ICL），并在新任务上具有适应能力。然而，模型的ICL能力对于少样本示范的选择是敏感的。最近的一项研究进展是检索针对每个输入查询定制的示范。示范检索的实现相对简单，利用现有的数据库和检索系统。这不仅提高了学习过程的效率和可扩展性，而且已经证明可以减少手动示例选择中的偏见。鉴于令人鼓舞的结果和在检索示范的ICL方面不断增长的研究，我们进行了广泛的研究综述。在这项综述中，我们讨论和比较了检索模型的不同设计选择，检索训练

    Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training p
    
[^43]: FAIR到位：我们如何为大型语言模型的训练开发和评估符合FAIR标准的数据集？

    FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for Large Language Models' Training?. (arXiv:2401.11033v1 [cs.CL])

    [http://arxiv.org/abs/2401.11033](http://arxiv.org/abs/2401.11033)

    这项工作介绍了一个将FAIR数据原则嵌入到大型语言模型训练中的框架，并提供了整合指南和检查清单。通过一个案例研究，展示了在符合FAIR标准的数据集中识别和减轻偏见的实际应用。

    

    大型语言模型的进展凸显了道德实践和数据完整性的需求。我们引入了一个将FAIR（可发现、可访问、可互操作、可重用）数据原则嵌入到LLM训练中的框架。这一方法标志着朝着符合FAIR标准的实践的转变。我们的框架提供了将FAIR数据原则整合到LLM训练中的指南。该举措包括研究人员和开发人员的检查清单。我们还通过一个案例研究展示了它的实际应用，重点是在我们符合FAIR标准的数据集中识别和减轻偏见。这项工作对于人工智能伦理和数据科学是重要的贡献，倡导在LLMs中使用平衡和道德的训练方法。

    Advancements in Large Language Models (LLMs) highlight the need for ethical practices and data integrity. We introduce a framework that embeds FAIR (Findable, Accessible, Interoperable, Reusable) data principles into LLM training. This approach marks a shift towards practices compliant with FAIR standards. Our framework presents guidelines for integrating FAIR data principles into LLM training. This initiative includes a checklist for researchers and developers. We also demonstrate its practical application through a case study focused on bias identification and mitigation in our FAIR-compliant dataset. This work is a significant contribution to AI ethics and data science, advocating for balanced and ethical training methods in LLMs.
    
[^44]: 零资源语言的多语言声学词嵌入

    Multilingual acoustic word embeddings for zero-resource languages. (arXiv:2401.10543v1 [eess.AS])

    [http://arxiv.org/abs/2401.10543](http://arxiv.org/abs/2401.10543)

    该研究发展了一种多语言声学词嵌入方法，用于解决缺乏标注数据的零资源语言的挑战。通过使用神经网络和多语言转移，该方法在零资源语言上取得了比现有模型更好的性能。研究还展示了该方法在仇恨言论检测和语义查询中的应用潜力。

    

    该研究解决了在缺乏标注数据的零资源语言中开发语音应用的挑战。具体而言，它使用声学词嵌入（AWE）-将可变时长的语音片段转换为固定维度的表示-并使用多语言转移，在多个资源丰富的语言的标注数据上进行训练。该研究引入了一种新的神经网络，在零资源语言上表现优于现有的AWE模型。研究还探讨了资源丰富语言的选择对结果的影响。AWE应用于斯瓦希里语广播中的仇恨言论检测的关键词识别系统中，展示了在实际场景中的鲁棒性。此外，新颖的语义AWE模型改进了语义查询示例搜索。

    This research addresses the challenge of developing speech applications for zero-resource languages that lack labelled data. It specifically uses acoustic word embedding (AWE) -- fixed-dimensional representations of variable-duration speech segments -- employing multilingual transfer, where labelled data from several well-resourced languages are used for pertaining. The study introduces a new neural network that outperforms existing AWE models on zero-resource languages. It explores the impact of the choice of well-resourced languages. AWEs are applied to a keyword-spotting system for hate speech detection in Swahili radio broadcasts, demonstrating robustness in real-world scenarios. Additionally, novel semantic AWE models improve semantic query-by-example search.
    
[^45]: ChatQA: 构建GPT-4级对话问答模型

    ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])

    [http://arxiv.org/abs/2401.10225](http://arxiv.org/abs/2401.10225)

    ChatQA是一系列对话问答模型，可以达到GPT-4级别的准确性。通过两阶段的指令调整方法，可以显著提高大型语言模型在零-shot对话问答中的结果。使用密集检索器进行问答数据集的微调可以实现与最先进的查询重写模型相当的结果，同时降低部署成本。ChatQA-70B在10个对话问答数据集上的平均得分超过了GPT-4，且不依赖于任何来自OpenAI GPT模型的合成数据。

    

    在这项工作中，我们介绍了ChatQA，一系列具有GPT-4级别准确性的对话问答模型。具体地，我们提出了一个两阶段的指令调整方法，可以显著提高大型语言模型（LLM）在零-shot对话问答中的结果。为了处理对话问答中的检索问题，我们在多轮问答数据集上进行了密集检索器的微调，这样可以提供与使用最先进的查询重写模型相当的结果，同时大大降低部署成本。值得注意的是，我们的ChatQA-70B可以在10个对话问答数据集的平均分上超过GPT-4（54.14 vs. 53.90），而不依赖于OpenAI GPT模型的任何合成数据。

    In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
    
[^46]: 空间-时间大语言模型用于交通预测

    Spatial-Temporal Large Language Model for Traffic Prediction. (arXiv:2401.10134v1 [cs.LG])

    [http://arxiv.org/abs/2401.10134](http://arxiv.org/abs/2401.10134)

    本文提出了一种空间-时间大语言模型（ST-LLM）用于交通预测，通过参数扩展和预训练来提高预测准确性，并利用空间-时间嵌入模块学习标记的空间位置和全局时间表示。

    

    交通预测是智能交通系统的关键组成部分，它通过使用历史数据来预测特定位置的未来交通情况。尽管现有的交通预测模型通常强调开发复杂的神经网络结构，但它们的准确性并未相应提高。最近，大型语言模型（LLMs）在时间序列分析方面显示出了出色的能力。与现有模型不同，LLMs主要通过参数扩展和广泛的预训练来进步，同时保持其基本结构。本文提出了一种空间-时间大语言模型（ST-LLM）用于交通预测。具体而言，ST-LLM将每个位置的时间步长定义为标记，并结合空间-时间嵌入模块来学习标记的空间位置和全局时间表示。然后，这些表示被融合以为每个标记提供统一的空间和时间信息。

    Traffic prediction, a critical component for intelligent transportation systems, endeavors to foresee future traffic at specific locations using historical data. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not seen improvements accordingly. Recently, Large Language Models (LLMs) have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures. In this paper, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. Specifically, ST-LLM redefines the timesteps at each location as tokens and incorporates a spatial-temporal embedding module to learn the spatial location and global temporal representations of tokens. Then these representations are fused to provide each token with unified spatial and temporal information. Furthermore, we
    
[^47]: 部分音标化：一种上下文对比推理方法

    Partial Diacritization: A Context-Contrastive Inference Approach. (arXiv:2401.08919v1 [cs.CL])

    [http://arxiv.org/abs/2401.08919](http://arxiv.org/abs/2401.08919)

    部分音标化是选择标记部分字符来提高阅读可读性和准确性的新方法。上下文对比的部分音标化（CCPD）集成了现有的阿拉伯音标化系统，并通过衡量部分音标化的新指标来判断需要标记哪些字符。

    

    音标化在提高阿拉伯文本可读性和消除歧义方面起着关键作用。目前的努力主要集中在标记每个符合条件的字符（全音标化）。相比之下，部分音标化（PD）是选择标记子集以在必要时提供帮助。研究表明，过多的音标符号会妨碍熟练读者，降低阅读速度和准确性。我们进行了一项行为实验，并显示出部分标记的文本通常比完全标记的文本更容易阅读，有时甚至比纯文本更容易。在这种情况下，我们介绍了上下文对比的部分音标化（CCPD）-一种与现有阿拉伯音标化系统无缝集成的新方法。CCPD对每个单词进行两次处理，一次有上下文，一次没有，并且只对两次推理之间存在差异的字符进行音标化。此外，我们还引入了衡量部分音标化的新指标。

    Diacritization plays a pivotal role in improving readability and disambiguating the meaning of Arabic texts. Efforts have so far focused on marking every eligible character (Full Diacritization). Comparatively overlooked, Partial Diacritzation (PD) is the selection of a subset of characters to be marked to aid comprehension where needed. Research has indicated that excessive diacritic marks can hinder skilled readers--reducing reading speed and accuracy. We conduct a behavioral experiment and show that partially marked text is often easier to read than fully marked text, and sometimes easier than plain text. In this light, we introduce Context-Contrastive Partial Diacritization (CCPD)--a novel approach to PD which integrates seamlessly with existing Arabic diacritization systems. CCPD processes each word twice, once with context and once without, and diacritizes only the characters with disparities between the two inferences. Further, we introduce novel indicators for measuring partial
    
[^48]: 支持学生决策的学习推荐：基于知识图谱情境化的LLM聊天机器人，实现对话解释和指导

    Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring. (arXiv:2401.08517v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2401.08517](http://arxiv.org/abs/2401.08517)

    这个论文研究了一种基于知识图谱情境化的LLM聊天机器人，作为学生学习推荐的解释工具和指导。通过定义上下文并利用人工策划的信息源来调控LLM的生成，聊天机器人能在与学生对话中提供解释和指导。

    

    学生对学习推荐的决策与其理解推荐原因的能力是不可分割的；他们能否根据这种理解进行修改。在各种解释性方法中，聊天机器人具有与同行或导师讨论类似的潜力来与学生进行对话。然而，尽管生成式人工智能（GenAI）和大型语言模型（LLM）的进展，聊天机器人的能力仍然不足以取代人类导师。因此，我们提出了一种方法，利用聊天机器人作为对话的中介和解释的有限和受控生成的来源，以利用LLM的潜力的同时减少其潜在风险。所提出的基于LLM的聊天机器人支持学生理解学习路径推荐。我们使用知识图谱（KG）作为人工策划的信息源，通过定义其提示的上下文来调控LLM的输出。

    Student commitment towards a learning recommendation is not separable from their understanding of the reasons it was recommended to them; and their ability to modify it based on that understanding. Among explainability approaches, chatbots offer the potential to engage the student in a conversation, similar to a discussion with a peer or a mentor. The capabilities of chatbots, however, are still not sufficient to replace a human mentor, despite the advancements of generative AI (GenAI) and large language models (LLM). Therefore, we propose an approach to utilize chatbots as mediators of the conversation and sources of limited and controlled generation of explanations, to harvest the potential of LLMs while reducing their potential risks at the same time. The proposed LLM-based chatbot supports students in understanding learning-paths recommendations. We use a knowledge graph (KG) as a human-curated source of information, to regulate the LLM's output through defining its prompt's contex
    
[^49]: 通过双层优化进行联合无监督和监督训练的自动语音识别方法

    Joint Unsupervised and Supervised Training for Automatic Speech Recognition via Bilevel Optimization. (arXiv:2401.06980v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2401.06980](http://arxiv.org/abs/2401.06980)

    本文提出了一种双层优化的训练方法，用于自动语音识别，通过联合无监督和监督训练来提高性能。

    

    本文提出了一种新颖的基于双层优化的训练方法，用于自动语音识别（ASR）任务中的声学模型训练，我们称之为“双层联合无监督和监督训练（BL-JUST）”。BL-JUST采用下层和上层优化，分别使用无监督损失和监督损失，利用最近在惩罚型双层优化方面取得的进展来解决这一具有可承受复杂度和严格收敛性保证的挑战性ASR问题。

    In this paper, we present a novel bilevel optimization-based training approach to training acoustic models for automatic speech recognition (ASR) tasks that we term {bi-level joint unsupervised and supervised training (BL-JUST)}. {BL-JUST employs a lower and upper level optimization with an unsupervised loss and a supervised loss respectively, leveraging recent advances in penalty-based bilevel optimization to solve this challenging ASR problem with affordable complexity and rigorous convergence guarantees.} To evaluate BL-JUST, extensive experiments on the LibriSpeech and TED-LIUM v2 datasets have been conducted. BL-JUST achieves superior performance over the commonly used pre-training followed by fine-tuning strategy.
    
[^50]: APLe: 多模式提示学习的逐令牌自适应方法

    APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning. (arXiv:2401.06827v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2401.06827](http://arxiv.org/abs/2401.06827)

    本研究提出了APLe方法，通过逐令牌自适应的方式调节CLIP模型中的视觉和语言模式提示，提高了模型的泛化性能。

    

    预训练的视觉语言模型在下游任务的泛化能力方面为同类模型树立了一个基准。现有研究中已经探索了视觉语言模型的许多特征，包括对文本输入的敏感性和跨多模式提示的调节过程所面临的挑战。最近的方法中，借鉴了图像融合中广泛使用的逐层训练思想，以提高泛化性能并解决上述挑战，取代了手工设计的提示，使用可学习的提示来提升性能。在解决多模式提示挑战的背景下，我们提出了逐令牌自适应方法(APLe)，以顺序方式调节CLIP中的视觉和语言两种模式提示。APLe能够高效地改进泛化能力。

    Pre-trained Vision-Language (V-L) models set the benchmark for generalization to downstream tasks among the noteworthy contenders. Many characteristics of the V-L model have been explored in existing research including the challenge of the sensitivity to text input and the tuning process across multi-modal prompts. With the advanced utilization of the V-L model like CLIP, recent approaches deploy learnable prompts instead of hand-craft prompts to boost the generalization performance and address the aforementioned challenges. Inspired by layer-wise training, which is wildly used in image fusion, we note that using a sequential training process to adapt different modalities branches of CLIP efficiently facilitates the improvement of generalization. In the context of addressing the multi-modal prompting challenge, we propose Token-wise Adaptive for Multi-modal Prompt Learning (APLe) for tuning both modalities prompts, vision and language, as tokens in a sequential manner. APLe addresses t
    
[^51]: 对用户行为进行分析以客观评估口语对话系统

    An Analysis of User Behaviours for Objectively Evaluating Spoken Dialogue Systems. (arXiv:2401.04867v1 [cs.CL])

    [http://arxiv.org/abs/2401.04867](http://arxiv.org/abs/2401.04867)

    本文研究了用户行为与主观评估在口语对话系统中的关系，提出了一种间接但客观评估系统的框架，并发现在不同类型的对话任务中，不同的用户行为指标对评估起到重要作用。

    

    建立口语对话系统的评估方案很重要，但也具有挑战性。虽然主观评估在用户实验中常用，但客观评估对于研究比较和可复制性是必要的。为解决这个问题，我们提出了一个框架，通过用户行为间接但客观地评估系统。为此，我们调查了社交对话任务中用户行为与主观评估分数之间的关系：专注倾听、面试和首次会议对话。结果显示，在用户话语是主要因素的对话任务中，如专注倾听和面试，话语数量和单词数量等指标在评估中起到重要作用。观察语调不流畅等也可以指示正式任务的有效性，例如面试。另一方面，在高互动性的对话任务中，如首次会议对话，用户情绪和参与程度更重要。

    Establishing evaluation schemes for spoken dialogue systems is important, but it can also be challenging. While subjective evaluations are commonly used in user experiments, objective evaluations are necessary for research comparison and reproducibility. To address this issue, we propose a framework for indirectly but objectively evaluating systems based on users' behaviours. In this paper, to this end, we investigate the relationship between user behaviours and subjective evaluation scores in social dialogue tasks: attentive listening, job interview, and first-meeting conversation. The results reveal that in dialogue tasks where user utterances are primary, such as attentive listening and job interview, indicators like the number of utterances and words play a significant role in evaluation. Observing disfluency also can indicate the effectiveness of formal tasks, such as job interview. On the other hand, in dialogue tasks with high interactivity, such as first-meeting conversation, b
    
[^52]: 基于混合方法的聊天AI模型：相对于万亿级参数模型的更廉价、更好的替代方案

    Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM. (arXiv:2401.02994v1 [cs.CL])

    [http://arxiv.org/abs/2401.02994](http://arxiv.org/abs/2401.02994)

    本研究介绍了一种名为“混合”的方法，通过组合多个适度规模的聊天AI模型，可以达到或超越比它们更大的模型的性能表现。

    

    在会话型AI研究中，越来越多的模型采用了更多的参数，如ChatGPT等模型。虽然这些庞大的模型往往能生成更好的聊天回复，但它们需要大量的计算资源和内存。本研究探讨了一个重要问题：能否通过组合较小的模型来达到与单个大模型相当或更好的性能？我们提出了一种称为“混合”的方法，它是一种简单但有效的将多个聊天AI集成在一起的方法。我们的实证证据表明，当特定较小的模型协同混合时，它们可以潜在地超越或匹敌大型模型的性能。例如，仅集成三个适度规模的模型（6B/13B参数）就可以达到或甚至超越ChatGPT（175B+参数）等大型模型的性能指标。这个假设经过了严格的测试。

    In conversational AI research, there's a noticeable trend towards developing models with a larger number of parameters, exemplified by models like ChatGPT. While these expansive models tend to generate increasingly better chat responses, they demand significant computational resources and memory. This study explores a pertinent question: Can a combination of smaller models collaboratively achieve comparable or enhanced performance relative to a singular large model? We introduce an approach termed "blending", a straightforward yet effective method of integrating multiple chat AIs. Our empirical evidence suggests that when specific smaller models are synergistically blended, they can potentially outperform or match the capabilities of much larger counterparts. For instance, integrating just three models of moderate size (6B/13B paramaeters) can rival or even surpass the performance metrics of a substantially larger model like ChatGPT (175B+ paramaters). This hypothesis is rigorously tes
    
[^53]: AgentCoder: 基于多Agent的代码生成和迭代测试与优化

    AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation. (arXiv:2312.13010v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.13010](http://arxiv.org/abs/2312.13010)

    AgentCoder是一种基于多Agent的代码生成和测试优化解决方案，通过程序员Agent、测试设计师Agent和测试执行Agent的协作，实现了在平衡代码生成和有效测试用例生成与执行方面的挑战中的突破。

    

    自然语言处理(NLP)的发展在大型语言模型(LLM)的发展推动下取得了显著的进展。这些模型在代码生成方面实现了革命性的突破，帮助开发人员提高软件的效率。尽管取得了这些进展，但在平衡代码段的生成与有效的测试用例生成和执行方面仍存在挑战。为了解决这些问题，本文介绍了一种新颖的解决方案——多Agent助手代码生成(AgentCoder)，该解决方案包括一个多Agent框架和专门的Agent：程序员Agent、测试设计师Agent和测试执行Agent。在编码过程中，程序员Agent将根据测试执行Agent的反馈重点关注代码的生成和改进。测试设计师Agent将为生成的代码生成测试用例，测试执行Agent将使用测试用例运行代码并将反馈写入到编程者

    The advancement of natural language processing (NLP) has been significantly boosted by the development of transformer-based large language models (LLMs). These models have revolutionized NLP tasks, particularly in code generation, aiding developers in creating software with enhanced efficiency. Despite their advancements, challenges in balancing code snippet generation with effective test case generation and execution persist. To address these issues, this paper introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution comprising a multi-agent framework with specialized agents: the programmer agent, the test designer agent, and the test executor agent. During the coding procedure, the programmer agent will focus on the code generation and refinement based on the test executor agent's feedback. The test designer agent will generate test cases for the generated code, and the test executor agent will run the code with the test cases and write the feedback to the prog
    
[^54]: 大语言模型时代文本水印技术综述

    A Survey of Text Watermarking in the Era of Large Language Models. (arXiv:2312.07913v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.07913](http://arxiv.org/abs/2312.07913)

    本文综述了大语言模型时代的文本水印技术，包括不同技术的概述和比较、算法评估方法、应用场景以及当前挑战和未来发展方向。

    

    文本水印算法在版权保护中起着至关重要的作用，然而其能力和应用场景一直受限。大语言模型的最新发展为文本水印技术的进步打开了新的机会。大语言模型不仅通过其文本理解和生成能力增强了文本水印算法的能力，还需要使用文本水印算法来保护自身的版权。本文对当前文本水印技术的现状进行了全面的调查，包括四个主要方面：（1）不同文本水印技术的概述和比较；（2）文本水印算法的评估方法，包括成功率、对文本质量的影响、鲁棒性和防篡改性；（3）文本水印技术的潜在应用场景；（4）当前挑战和未来发展方向。

    Text watermarking algorithms play a crucial role in the copyright protection of textual content, yet their capabilities and application scenarios have been limited historically. The recent developments in large language models (LLMs) have opened new opportunities for the advancement of text watermarking techniques. LLMs not only enhance the capabilities of text watermarking algorithms through their text understanding and generation abilities but also necessitate the use of text watermarking algorithms for their own copyright protection. This paper conducts a comprehensive survey of the current state of text watermarking technology, covering four main aspects: (1) an overview and comparison of different text watermarking techniques; (2) evaluation methods for text watermarking algorithms, including their success rates, impact on text quality, robustness, and unforgeability; (3) potential application scenarios for text watermarking technology; (4) current challenges and future directions
    
[^55]: 时间中的涟漪：美国历史中的不连续性

    A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.01185](http://arxiv.org/abs/2312.01185)

    该论文通过使用向量嵌入和非线性降维方法，发现GPT-2与UMAP的结合可以提供更好的分离和聚类效果。同时，经过微调的DistilBERT模型可用于识别总统和演讲的年份。

    

    在这篇论文中，我们使用来自Kaggle的国情咨文数据集对美国历史的总体时间线及咨文本身的特点和性质进行了一些令人惊讶（也有些不那么令人惊讶）的观察。我们的主要方法是使用向量嵌入，如BERT（DistilBERT）和GPT-2。虽然广泛认为BERT（及其变体）最适合NLP分类任务，但我们发现GPT-2结合UMAP等非线性降维方法可以提供更好的分离和更强的聚类效果。这使得GPT-2 + UMAP成为一个有趣的替代方案。在我们的情况下，不需要对模型进行微调，预训练的GPT-2模型就足够好用。我们还使用了经过微调的DistilBERT模型来检测哪位总统发表了哪篇演讲，并取得了非常好的结果（准确率为93\% - 95\%，具体取决于运行情况）。为了确定写作年份，我们还执行了一个类似的任务。

    In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\% - 95\% depending on the run). An analogous task was performed to determine the year of writing, an
    
[^56]: 超越图灵: 检测机器生成文本的方法比较分析

    Beyond Turing: A Comparative Analysis of Approaches for Detecting Machine-Generated Text. (arXiv:2311.12373v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.12373](http://arxiv.org/abs/2311.12373)

    该论文对比了三种不同方法用于检测人类生成和机器生成文本的能力，并发现它们在性能上存在显著差异，为进一步研究和开发具有鲁棒性和高度区分性的模型提供了重要参考。

    

    在预训练语言模型（PLMs）生成文本方面取得了显著进展，然而区分人类生成和机器生成的文本越来越具有挑战性。本论文对三种不同的方法进行了深入评估，用于解决这个任务：传统的浅层学习、语言模型（LM）微调和多语言模型微调。这些方法在广泛的机器生成文本上经过严格测试，提供了一个评估它们在区分人类和机器构造语言方面能力的基准。结果显示不同方法的性能存在显著差异，因此强调了在这一关键NLP领域的持续需求。本研究提供了有价值的见解，并为未来旨在创建强大且高度区分性模型的研究铺平了道路。

    Significant progress has been made on text generation by pre-trained language models (PLMs), yet distinguishing between human and machine-generated text poses an escalating challenge. This paper offers an in-depth evaluation of three distinct methods used to address this task: traditional shallow learning, Language Model (LM) fine-tuning, and Multilingual Model fine-tuning. These approaches are rigorously tested on a wide range of machine-generated texts, providing a benchmark of their competence in distinguishing between human-authored and machine-authored linguistic constructs. The results reveal considerable differences in performance across methods, thus emphasizing the continued need for advancement in this crucial area of NLP. This study offers valuable insights and paves the way for future research aimed at creating robust and highly discriminative models.
    
[^57]: 异常维度编码特定任务知识

    Outlier Dimensions Encode Task-Specific Knowledge. (arXiv:2310.17715v1 [cs.CL])

    [http://arxiv.org/abs/2310.17715](http://arxiv.org/abs/2310.17715)

    异常维度可以编码关键的特定任务知识，并且一个单一的异常维度可以以最小的错误率完成下游任务。

    

    大型语言模型（LLM）的表示被少数几个具有极高方差的异常维度所主导。先前的研究认为，虽然去除LLM表示中的异常维度会损害下游性能，但异常维度对嵌入表示的质量是有害的。在本研究中，我们研究了微调对异常维度的影响，并展示了以下结果：1）在预训练中出现的异常维度在微调模型中仍然存在，2）一个单一的异常维度可以以最小的错误率完成下游任务。我们的结果表明，异常维度可以编码关键的特定任务知识，并且一个表示在单个异常维度上的值会影响下游模型的决策。

    Representations from large language models (LLMs) are known to be dominated by a small subset of dimensions with exceedingly high variance. Previous works have argued that although ablating these outlier dimensions in LLM representations hurts downstream performance, outlier dimensions are detrimental to the representational quality of embeddings. In this study, we investigate how fine-tuning impacts outlier dimensions and show that 1) outlier dimensions that occur in pre-training persist in fine-tuned models and 2) a single outlier dimension can complete downstream tasks with a minimal error rate. Our results suggest that outlier dimensions can encode crucial task-specific knowledge and that the value of a representation in a single outlier dimension drives downstream model decisions.
    
[^58]: 正式规定基于LLM的代理人的高级行为

    Formally Specifying the High-Level Behavior of LLM-Based Agents. (arXiv:2310.08535v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.08535](http://arxiv.org/abs/2310.08535)

    本文介绍了一个最小生成框架，通过在高级声明中定义所需的代理人行为，然后构建解码监视器，从而实现了基于LLM的代理人的快速设计和实施。

    

    最近，由LLM驱动的自主目标驱动型代理人已成为解决具有挑战性问题的有前途的工具，而无需获得昂贵的任务特定的精细调整模型。目前，这类代理人的设计和实施是临时性的，因为LLM-based代理人可能应用于的各种任务的广泛性质意味着不能有一种适用于所有情况的代理人设计方法。在这项工作中，我们旨在通过提出一个简化代理人构建过程的最小生成框架来减轻设计和实施新代理人的难度。我们引入的框架允许用户以高级声明的规范方式定义所需的代理人行为，然后使用这个规范构建解码监视器，以确保LLM会产生具有所需行为的输出。我们的声明性方法，即描述行为而不考虑如何实施或强制执行，可以实现快速设计

    Autonomous, goal-driven agents powered by LLMs have recently emerged as promising tools for solving challenging problems without the need for task-specific finetuned models that can be expensive to procure. Currently, the design and implementation of such agents is ad hoc, as the wide variety of tasks that LLM-based agents may be applied to naturally means there can be no one-size-fits-all approach to agent design. In this work we aim to alleviate the difficulty of designing and implementing new agents by proposing a minimalistic generation framework that simplifies the process of building agents. The framework we introduce allows the user to define desired agent behaviors in a high-level, declarative specification that is then used to construct a decoding monitor which guarantees the LLM will produce an output exhibiting the desired behavior. Our declarative approach, in which the behavior is described without concern for how it should be implemented or enforced, enables rapid design,
    
[^59]: EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v2 [cs.CL] UPDATED)

    EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.04691](http://arxiv.org/abs/2310.04691)

    EMO提出了地球移动距离优化（EMO）来解决语言模型中的退化现象。EMO利用了地球移动距离的特性，并引入了一个可行的上界来简化训练。经过评估，发现EMO在语言模型上有显著的改进。

    

    神经语言模型是人文本的概率模型。它们主要通过最大似然估计（MLE）进行训练，该方法等同于最小化经验数据分布和模型分布之间的前向交叉熵。然而，当从这些模型学习的分布解码时，仍然经常观察到各种退化现象。我们确定前向交叉熵作为人与模型分布对齐的距离度量是次优的，原因有：（1）召回优化，（2）负样本多样性忽视和（3）训练测试不匹配。在本文中，我们提出了用于自回归语言模型的地球移动距离优化（EMO）。EMO利用地球移动距离的内在特性来解决上述挑战。由于直接计算的复杂性，我们进一步引入了一种可行的EMO上界来简化端到端训练。经过广泛评估之后，发现我们的方法在语言模型上有显著的改进。

    Neural language models are probabilistic models of human text. They are predominantly trained using maximum likelihood estimation (MLE), which is equivalent to minimizing the forward cross-entropy between the empirical data distribution and the model distribution. However, various degeneration phenomena are still widely observed when decoding from the distributions learned by such models. We establish that the forward cross-entropy is suboptimal as a distance metric for aligning human and model distribution due to its (1) recall-prioritization (2) negative diversity ignorance and (3) train-test mismatch. In this paper, we propose Earth Mover Distance Optimization (EMO) for auto-regressive language modeling. EMO capitalizes on the inherent properties of earth mover distance to address the aforementioned challenges. Due to the high complexity of direct computation, we further introduce a feasible upper bound for EMO to ease end-to-end training. Upon extensive evaluation of language model
    
[^60]: "检索遇上长篇大语言模型"

    Retrieval meets Long Context Large Language Models. (arXiv:2310.03025v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.03025](http://arxiv.org/abs/2310.03025)

    "本论文研究了将检索增强和长上下文窗口的大语言模型相结合的解决方案，发现在长上下文任务中，通过检索增强的LLM使用4K上下文窗口可以取得与通过长上下文窗口微调的LLM使用16K上下文窗口相当的性能，同时计算量要少得多。此外，无论上下文窗口大小如何，检索都可以显著提高LLM的性能。"

    

    "最近，扩展大语言模型（LLM）的上下文窗口越来越流行，而将检索与LLM相结合的解决方案已存在多年。自然而然的问题是：检索增强与长上下文窗口，哪个对下游任务更好？这两种方法可以结合起来以兼顾利弊吗？在这项工作中，我们使用两个最先进的预训练LLM（即一个私有的43B GPT和Llama2-70B）来研究这两种解决方案。也许令人惊讶的是，我们发现在长上下文任务中，LLM使用4K上下文窗口并通过简单的检索增强在生成时可以达到与通过长上下文窗口进行位置插值的微调LLM使用16K上下文窗口相当的性能，同时计算量要少得多。更重要的是，我们证明了不论其扩展的上下文窗口大小如何，检索都可以显著提高LLM的性能。我们最好的模型是具有32K上下文窗口的检索增强Llama2-70B。"

    Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context wi
    
[^61]: 《GenAI对抗人性：生成式人工智能和大型语言模型的邪恶应用》

    GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models. (arXiv:2310.00737v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2310.00737](http://arxiv.org/abs/2310.00737)

    这篇论文探讨了生成式人工智能和大型语言模型的潜在滥用，呼吁认识到这些挑战的紧迫性。研究揭示了这些技术在深度伪造、合成身份恶意活动以及虚假信息和欺诈方面可能带来的社会影响。

    

    生成式人工智能（GenAI）和大型语言模型（LLM）是技术的奇迹，以其在自然语言处理和多模式内容生成方面的卓越能力而受到赞扬，它们承诺带来一个变革的未来。但就像所有强大的工具一样，它们也有其阴影存在。想象一下生活在一个深度伪造与现实无法区分、合成身份组织恶意活动、以及有着无与伦比精确度的有针对性的虚假信息或欺诈手法的世界。欢迎来到GenAI应用的黑暗面。本文不仅是探索GenAI和LLMs潜在滥用的旅程，也是呼吁认识到面临的挑战的紧迫性。在我们航行于虚假信息活动、恶意内容生成与精密恶意软件构建的海洋中，我们将揭示这场我们正在见证的GenAI革命中的社会影响。

    Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are marvels of technology; celebrated for their prowess in natural language processing and multimodal content generation, they promise a transformative future. But as with all powerful tools, they come with their shadows. Picture living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. Welcome to the darker side of GenAI applications. This article is not just a journey through the meanders of potential misuse of GenAI and LLMs, but also a call to recognize the urgency of the challenges ahead. As we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we'll uncover the societal implications that ripple through the GenAI revolution we are witnessing. From AI-powered botnets on social med
    
[^62]: AutomaTikZ: 使用TikZ进行科学矢量图的文本引导合成

    AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ. (arXiv:2310.00367v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00367](http://arxiv.org/abs/2310.00367)

    本论文引入了AutomaTikZ，它通过使用TikZ作为中间表示来实现科学图形的文本引导合成。通过微调LLaMA和引入CLiMA模型，并使用DaTikZ数据集进行训练，在人工评估和自动评估中，CLiMA和LLaMA在与人类创建的图形的相似度方面表现出色，并且CLiMA还改进了tex。

    

    从文本生成位图图形已经引起了相当大的关注，但是对于科学图形，常常更喜欢使用矢量图形。鉴于矢量图形通常使用低级别的图形原语进行编码，直接生成它们是困难的。为了解决这个问题，我们提出了使用TikZ作为科学图形的中间表示，TikZ是一种众所周知的抽象图形语言，可以编译成矢量图形。TikZ提供了面向人的高级命令，从而使得在任何大型语言模型中进行条件语言建模变得容易。为此，我们介绍了DaTikZ，这是第一个包含120k个与标题对齐的TikZ绘图的大规模数据集。我们在DaTikZ上对LLaMA进行了微调，同时引入了我们的新模型CLiMA，该模型使用了多模态的CLIP嵌入。在人工和自动评估中，CLiMA和LLaMA在与人类创建的图形的相似度方面都优于商业的GPT-4和Claude 2，其中CLiMA还改进了tex。

    Generating bitmap graphics from text has gained considerable attention, yet for scientific figures, vector graphics are often preferred. Given that vector graphics are typically encoded using low-level graphics primitives, generating them directly is difficult. To address this, we propose the use of TikZ, a well-known abstract graphics language that can be compiled to vector graphics, as an intermediate representation of scientific figures. TikZ offers human-oriented, high-level commands, thereby facilitating conditional language modeling with any large language model. To this end, we introduce DaTikZ, the first large-scale TikZ dataset consisting of 120k TikZ drawings aligned with captions. We fine-tune LLaMA on DaTikZ, as well as our new model CLiMA, which augments LLaMA with multimodal CLIP embeddings. In both human and automatic evaluation, CLiMA and LLaMA outperform commercial GPT-4 and Claude 2 in terms of similarity to human-created figures, with CLiMA additionally improving tex
    
[^63]: CB-Whisper: 使用开放词汇关键词检测进行上下文偏置的Whisper

    CB-Whisper: Contextual Biasing Whisper using Open-Vocabulary Keyword-Spotting. (arXiv:2309.09552v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.09552](http://arxiv.org/abs/2309.09552)

    CB-Whisper是一种基于OpenAI的Whisper模型的自动语音识别系统，通过使用开放词汇关键词检测（OV-KWS）识别罕见的命名实体，并使用这些实体作为提示来改进识别效果。实验证明，该方法在提高实体召回率的同时会略微增加混淆错误率（MER）。

    

    自动语音识别系统往往难以识别罕见的命名实体，如个人姓名、组织机构和在训练数据中不经常遇到的专业术语。本文提出了一种基于OpenAI的Whisper模型的Contextual Biasing Whisper（CB-Whisper）自动语音识别系统，通过使用Whisper编码器的隐藏状态执行开放词汇关键词检测（OV-KWS）来识别用户定义的命名实体。识别出的实体被用作Whisper解码器的提示。我们首先提出了一种使用OV-KWS和ASR任务进行多任务训练的方法来优化模型。实验证明，与原始Whisper模型相比，这种方法在中国Aishell热词子集和两个内部代码切换测试集上显著提高了实体召回率。然而，由于灾难性遗忘，我们观察到在内部测试集上混淆错误率（MER）略微增加。为了解决这个问题并使用不同大小的Whisper模型，我们进一步提出了一种解决方案。

    End-to-end automatic speech recognition (ASR) systems often struggle to recognize rare name entities, such as personal names, organizations, and terminologies not frequently encountered in the training data. This paper presents Contextual Biasing Whisper (CB-Whisper), a novel ASR system based on OpenAI's Whisper model that can recognize user-defined name entities by performing open-vocabulary keyword-spotting (OV-KWS) using the hidden states of Whisper encoder. The recognized entities are used as prompts for the Whisper decoder. We first propose a multitask training approach with OV-KWS and ASR tasks to optimize the model. Experiments show that this approach substantially improves the entity recalls compared to the original Whisper model on Chinese Aishell hot word subsets and two internal code-switch test sets. However, we observed a slight increase in mixed-error-rate (MER) on internal test sets due to catastrophic forgetting. To address this problem and use different sizes of the Wh
    
[^64]: DiariST: 带有说话者分离的流式语音翻译

    DiariST: Streaming Speech Translation with Speaker Diarization. (arXiv:2309.08007v1 [eess.AS])

    [http://arxiv.org/abs/2309.08007](http://arxiv.org/abs/2309.08007)

    DiariST是第一个流式语音翻译和说话者分离的解决方案，通过集成标记级序列化输出训练和t向量，实现了强大的ST和SD能力。

    

    对于对话录音的端到端语音翻译（ST）涉及一些尚未充分研究的挑战，如没有准确的词时间戳的说话者分离（SD）和处理重叠语音的流式方式。在这项工作中，我们提出了DiariST，这是第一个流式的ST和SD解决方案。它基于基于神经传递器的流式ST系统构建，并集成了最初用于多说话人语音识别的标记级序列化输出训练和t向量。由于该领域缺乏评估基准，我们通过将AliMeeting语料库的参考中文转录成英文来构建了一个新的评估数据集DiariST-AliMeeting。我们还提出了新的指标，称为非特定说话者BLEU和特定说话者BLEU，以衡量ST的质量，并考虑SD的准确性。与基于Whisper的离线系统相比，我们的系统在进行流式推理的同时实现了强大的ST和SD能力。

    End-to-end speech translation (ST) for conversation recordings involves several under-explored challenges such as speaker diarization (SD) without accurate word time stamps and handling of overlapping speech in a streaming fashion. In this work, we propose DiariST, the first streaming ST and SD solution. It is built upon a neural transducer-based streaming ST system and integrates token-level serialized output training and t-vector, which were originally developed for multi-talker speech recognition. Due to the absence of evaluation benchmarks in this area, we develop a new evaluation dataset, DiariST-AliMeeting, by translating the reference Chinese transcriptions of the AliMeeting corpus into English. We also propose new metrics, called speaker-agnostic BLEU and speaker-attributed BLEU, to measure the ST quality while taking SD accuracy into account. Our system achieves a strong ST and SD capability compared to offline systems based on Whisper, while performing streaming inference for
    
[^65]: SpeechTokenizer：面向语音大语言模型的统一语音分词器

    SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models. (arXiv:2308.16692v1 [cs.CL])

    [http://arxiv.org/abs/2308.16692](http://arxiv.org/abs/2308.16692)

    提出了一种面向语音大语言模型的统一语音分词器SpeechTokenizer，通过统一语义和声学标记并采用编码器-解码器架构，实现了在不同层级上解耦语音信息的不同方面，构建了一个统一语音语言模型（USLM）。

    

    当前的语音大语言模型基于离散的语音表示，可以分为语义标记和声学标记。然而，现有的语音标记并非专为语音语言建模而设计。为了评估语音标记在构建语音语言模型方面的适应性，我们建立了第一个基准标准，即SLMTokBench。我们的结果表明，无论是语义标记还是声学标记都不适合这个目的。因此，我们提出了SpeechTokenizer，一种面向语音大语言模型的统一语音分词器。SpeechTokenizer采用具有残差向量量化（RVQ）的编码器-解码器架构。通过统一语义和声学标记，SpeechTokenizer在不同的RVQ层级上以层次方式解耦语音信息的不同方面。此外，我们构建了一个利用SpeechTokenizer的统一语音语言模型（USLM）。实验证明，SpeechTokenizer在语音重建和...

    Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and 
    
[^66]: 大型语言模型的投票：用于罕见病识别的提示

    Large Language Models Vote: Prompting for Rare Disease Identification. (arXiv:2308.12890v1 [cs.CL])

    [http://arxiv.org/abs/2308.12890](http://arxiv.org/abs/2308.12890)

    本文提出了一种名为模型投票提示(MVP)的方法，用于改善在少样本学习(FSL)环境下大型语言模型(LLMs)的查询性能。MVP通过提示多个LLMs执行相同的任务，并对生成的输出进行多数投票，从而实现了对罕见病的识别和分类任务的改进。

    

    生成式大型语言模型(LLMs)的出现强调了准确和高效的提示方法的需求。LLMs经常应用于少样本学习(FSL)的情境中，这里任务只使用很少的训练数据执行。FSL在许多人工智能(AI)子领域中变得流行，包括用于健康的AI。罕见病影响人口的一小部分，在数据可用性受限的情况下 inherently 需要FSL技术，尽管人工数据收集和标注费时费力。在本文中，我们提出了模型投票提示(MVP)，这是一种用于改善FSL环境中LLM查询性能的灵活提示方法。MVP通过提示多个LLMs执行相同的任务，然后对生成的输出进行多数投票来实现。该方法在单次罕见病识别和分类任务中相对于任何单个模型在集成模型中实现了改进的结果。我们还发布了一个新颖的罕见病数据集用于FSL。

    The emergence of generative Large Language Models (LLMs) emphasizes the need for accurate and efficient prompting approaches. LLMs are often applied in Few-Shot Learning (FSL) contexts, where tasks are executed with minimal training data. FSL has become popular in many Artificial Intelligence (AI) subdomains, including AI for health. Rare diseases, affecting a small fraction of the population, inherently require FSL techniques due to limited data availability, though manual data collection and annotation is costly and time-consuming. In this paper, we propose Models-Vote Prompting (MVP), a flexible prompting approach for improving the performance of LLM queries in FSL settings. MVP works by prompting numerous LLMs to perform the same tasks and then conducting a majority vote on the resulting outputs. This method achieves improved results to any one model in the ensemble on one-shot rare disease identification and classification tasks. We also release a novel rare disease dataset for FS
    
[^67]: 人本自然语言处理事实核查：使用AI的匹配设计与事实核查员合作

    Human-centered NLP Fact-checking: Co-Designing with Fact-checkers using Matchmaking for AI. (arXiv:2308.07213v1 [cs.HC] CROSS LISTED)

    [http://arxiv.org/abs/2308.07213](http://arxiv.org/abs/2308.07213)

    本研究使用AI的匹配设计方法，通过与专业事实核查员的合作设计，发现并解决事实核查员与技术之间的差距。合作设计会议产生了11个新的设计思路，包括提高效率和个性化的事实核查工具，帮助事实核查员准备未来的虚假信息，监测偏见，以及支持内部组织。

    

    专业事实核查在应对大量虚假信息方面存在可扩展性有限的挑战。虽然提出了许多自然语言处理工具来增强事实核查的效率和可扩展性，但学术研究和事实核查组织均报告了对此类工具的有限采用，因为这些工具不足以与事实核查员的实践、价值观和需求保持一致。为了弥补这一差距，我们研究了一种合作设计方法，即AI的匹配设计，该方法促进事实核查员、设计师和自然语言处理研究人员共同发现应以何种方式解决事实核查员的需求。我们与22名专业事实核查员进行的合作设计会议产生了11个新的设计思路。这些思路有助于提高信息搜索、处理和撰写效率以及个性化的事实核查；帮助事实核查员主动准备未来的虚假信息；监测潜在的偏见；并支持内部组织。

    A key challenge in professional fact-checking is its limited scalability in relation to the magnitude of false information. While many Natural Language Processing (NLP) tools have been proposed to enhance fact-checking efficiency and scalability, both academic research and fact-checking organizations report limited adoption of such tooling due to insufficient alignment with fact-checker practices, values, and needs. To address this gap, we investigate a co-design method, Matchmaking for AI, which facilitates fact-checkers, designers, and NLP researchers to collaboratively discover what fact-checker needs should be addressed by technology and how. Our co-design sessions with 22 professional fact-checkers yielded a set of 11 novel design ideas. They assist in information searching, processing, and writing tasks for efficient and personalized fact-checking; help fact-checkers proactively prepare for future misinformation; monitor their potential biases; and support internal organization c
    
[^68]: OWQ：大语言模型权重量化中激活离群值的启示

    OWQ: Lessons learned from activation outliers for weight quantization in large language models. (arXiv:2306.02272v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02272](http://arxiv.org/abs/2306.02272)

    在大语言模型的推理中，要使用多个服务器贵重的GPU导致显著的成本障碍，OWQ提出的一种后训练量化方法可以在最小质量损失的情况下减少这种限制。它可以通过考虑激活离群值来确定权值量化误差的因素，并为易受攻击的权重分配高精度，具有与OPTQ相当的质量。

    

    拥有数十亿个参数的大型语言模型(LLMs)通过简单的提示调整和少量的示例，在各种语言任务中展现出令人惊叹的结果，而无需进行任务特定的微调。然而，它们巨大的尺寸要求甚至在推理时使用多个服务器级的GPU，从而产生了显著的成本障碍。为了解决这一限制，我们提出了一种新型的后训练量化方法来量化权重，减少质量损失。虽然已知激活离群值在激活量化中存在问题，但我们的理论分析表明，通过考虑激活离群值，我们可以确定导致权重量化误差的因素。我们提出了一种创新的后训练量化方案，名为Outlier-Aware Weight Quantization (OWQ)，它可以识别易受攻击的权重并为它们分配高精度。我们的大量实验表明，OWQ生成的3.01位模型具有与OPTQ生成的4位模型相当的质量。

    Large language models (LLMs) with hundreds of billions of parameters show impressive results across various language tasks using simple prompt tuning and few-shot examples, without the need for task-specific fine-tuning. However, their enormous size requires multiple server-grade GPUs even for inference, creating a significant cost barrier. To address this limitation, we introduce a novel post-training quantization method for weights with minimal quality degradation. While activation outliers are known to be problematic in activation quantization, our theoretical analysis suggests that we can identify factors contributing to weight quantization errors by considering activation outliers. We propose an innovative PTQ scheme called outlier-aware weight quantization (OWQ), which identifies vulnerable weights and allocates high-precision to them. Our extensive experiments demonstrate that the 3.01-bit models produced by OWQ exhibit comparable quality to the 4-bit models generated by OPTQ.
    
[^69]: 使用基于文献的语境化学习生成新的科学方向

    Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14259](http://arxiv.org/abs/2305.14259)

    本文介绍了一种基于文献的发现方法，通过上下文化的学习生成新的科学方向，克服了标准方法在预测关联、忽略上下文等方面的局限性。模型使用了引文和知识图关系的网络，并使用大型语言模型进行评估，发现GPT4在生成创新思想方面表现出色。

    

    基于文献的发现（LBD）旨在通过挖掘论文并生成假设来发现新的科学知识。标准的LBD仅限于预测离散概念之间的两两关系（例如，药物和疾病的关联）。LBD还忽略了关键的上下文，例如实验设置（例如，药物评估的特定患者群体）和人类科学家考虑的背景知识和动机（例如，找到没有特定副作用的药物候选）。我们通过一种新颖的上下文化LBD（C-LBD）表述来解决这些局限性：以自然语言生成科学假设，同时将它们联系到控制假设搜索空间的上下文中。我们提出了一个建模框架，使用获得的引文和知识图关系的异构网络中的“灵感”，并创建了一个从论文中派生的新数据集。我们使用强大的大型语言模型（LLM）进行评估，发现GPT4倾向于生成具有创新性的思想。

    Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
    
[^70]: SpecInfer：利用推测推断和令牌树验证加速生成式大语言模型的服务

    SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification. (arXiv:2305.09781v1 [cs.CL])

    [http://arxiv.org/abs/2305.09781](http://arxiv.org/abs/2305.09781)

    SpecInfer是一种LLM服务系统，通过利用推测推断和令牌树验证来加速生成式大语言模型的推断过程，显著减少了为它们提供服务所需的端到端延迟和计算要求，同时确保模型质量。

    

    由于生成式大语言模型（LLMs）需要高计算和内存需求，因此快速和廉价地为它们提供服务是具有挑战性的。本文介绍SpecInfer，一个LLM服务系统，它利用推测推断和令牌树验证加速生成式LLM推断。SpecInfer背后的关键是将各种小型语言模型进行集体提升调整，共同预测LLM的输出； 预测结果组织成一个令牌树，其中每个节点都表示候选令牌序列。通过一种新颖的基于树的并行解码机制，以LMM作为令牌树验证器来验证令牌树所代表的所有候选令牌序列的正确性。SpecInfer使用LLM作为令牌树验证器，而不是增量解码器，从而显著减少了为生成式LLM提供服务所需的端到端延迟和计算要求，同时可确保模型质量。

    The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree verification. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.
    
[^71]: 视觉思维链：多模态填充技术弥合逻辑差距

    Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings. (arXiv:2305.02317v1 [cs.CL])

    [http://arxiv.org/abs/2305.02317](http://arxiv.org/abs/2305.02317)

    VCoT是一种使用思维链激励和视觉语言组合递归地弥合时序数据中逻辑差距的新颖方法，其使用视觉引导生成合成的多模态填充以添加一致且新颖的信息，并减少需要时序推理的逻辑差距。

    

    大型自然语言模型的出现提高了模型的多步推理能力，能以人类方式分解问题。然而，该范例由于其单模态性质并且主要应用于问答任务而受到限制。我们认为将视觉增强内容纳入推理是必要的，尤其是针对复杂想象任务。因此，我们介绍了VCoT，一种新颖的方法，它利用思维链激励和视觉语言组合来递归地弥合时序数据中的逻辑差距。我们的方法使用视觉引导生成合成的多模态填充，以添加一致且新颖的信息，并减少下游任务中需要时序推理的逻辑差距，同时提供模型的多步推理的解释性。我们将VCoT应用于视觉叙事和WikiHow摘要数据集，并通过人工评估展示了其性能的提升。

    Recent advances in large language models elicit reasoning in a chain of thought that allows models to decompose problems in a human-like fashion. Though this paradigm improves multi-step reasoning ability in language models, it is limited by being unimodal and applied mainly to question-answering tasks. We claim that incorporating visual augmentation into reasoning is essential, especially for complex, imaginative tasks. Consequently, we introduce VCoT, a novel method that leverages chain of thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data. Our method uses visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning, as well as provide interpretability into models' multi-step reasoning. We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evalua
    
[^72]: 基于能量模型的零样本场景重新排列规划器

    Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])

    [http://arxiv.org/abs/2304.14391](http://arxiv.org/abs/2304.14391)

    本文提出一种基于能量模型的零样本场景重新排列规划器，通过语言指导的空间概念来实现长指令以及在训练时从未见过的空间概念组合。本文的模型在指令导向操作基准测试以及组合指令基准测试中表现良好，优于基于语言表达的最先进方法，并且可以成功地解决之前从未见过的复杂指令和场景。

    

    本文致力于开发一个场景重排框架，可以解释长指令以及在训练时从未见过的空间概念组合。我们提出使用相对对象排列的能量函数来表示语言指导的空间概念。语言解析器将指令映射到相应的能量函数，而开放式视觉语言模型将它们的参数基于场景中的相关对象进行修正。通过梯度下降求解能量函数的总和，并利用基于本地计算机视觉的策略将对象重新定位到推断的目标位置，即可生成目标场景配置。我们在已建立的指令导向操作基准测试以及我们提出的组合指令基准测试中测试了模型，结果表明，我们的模型的绩效优于基于语言表达的最先进方法，并且可以成功地解决之前从未见过的复杂指令和场景。

    Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then relocate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model 
    
[^73]: 通过加强句子选择来增强文本生成的学习生成问题方法

    Learning to Generate Questions by Enhancing Text Generation with Sentence Selection. (arXiv:2212.12192v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.12192](http://arxiv.org/abs/2212.12192)

    本研究提出了一种通过加强句子选择来增强文本生成的学习生成问题方法，该方法通过设计选择器和生成器两个模块，使模型更关注与答案相关的句子，并隐式结合局部信息和全局信息来生成问题。实验结果表明该方法在问题生成任务上优于强大的预训练模型。

    

    我们提出了一种针对回答感知的问题生成问题的方法。我们观察到，回答和问题的信息可以在上下文中的一些相关句子中找到，而不仅仅依赖于强大的预训练语言模型的能力。基于此，我们设计了一个模型，包括两个模块：选择器和生成器。选择器强制模型更加关注与答案相关的句子，以提供隐含的局部信息。生成器通过将选择器提供的局部信息与编码器编码的整个上下文的全局信息隐式结合来生成问题。模型联合训练以利用两个模块之间的潜在交互。在两个基准数据集上的实验结果表明，我们的模型比强大的预训练模型在问题生成任务上更好。代码也可用。

    We introduce an approach for the answer-aware question generation problem. Instead of only relying on the capability of strong pre-trained language models, we observe that the information of answers and questions can be found in some relevant sentences in the context. Based on that, we design a model which includes two modules: a selector and a generator. The selector forces the model to more focus on relevant sentences regarding an answer to provide implicit local information. The generator generates questions by implicitly combining local information from the selector and global information from the whole context encoded by the encoder. The model is trained jointly to take advantage of latent interactions between the two modules. Experimental results on two benchmark datasets show that our model is better than strong pre-trained models for the question generation task. The code is also available.
    

