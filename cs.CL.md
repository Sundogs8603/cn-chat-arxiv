# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Self-Alignment with Instruction Backtranslation.](http://arxiv.org/abs/2308.06259) | 本论文提出了一种自动对齐方法，通过为人工编写的文本添加指令标签来构建高质量的指令跟踪语言模型。该方法通过自我增强和自我筛选生成训练示例，并且在Alpaca排行榜上表现出非常高效的自动对齐能力。 |
| [^2] | [Covid-19 Public Sentiment Analysis for Indian Tweets Classification.](http://arxiv.org/abs/2308.06241) | 本文主要关注印度COVID-19推文的情感分析，通过提取Twitter数据并对其进行情感分析，有助于分析推文中非结构化、异构的观点。 |
| [^3] | [KETM:A Knowledge-Enhanced Text Matching method.](http://arxiv.org/abs/2308.06235) | KETM是一种知识增强的文本匹配方法，通过从外部知识源中获取常识知识来丰富上下文表示，从而提高模型的理解和推理能力。实验证明，该方法在常识知识推理任务中具有更好的性能和鲁棒性。 |
| [^4] | [A Large Language Model Enhanced Conversational Recommender System.](http://arxiv.org/abs/2308.06212) | 一种基于大型语言模型的增强对话推荐系统，通过利用大型语言模型的推理和生成能力，有效地管理子任务、解决不同的子任务，并生成与用户交互的回应。 |
| [^5] | [Thinking Like an Expert:Multimodal Hypergraph-of-Thought (HoT) Reasoning to boost Foundation Modals.](http://arxiv.org/abs/2308.06207) | 本文致力于构建一个思维范式，能够像专家一样思考，通过超图的超边连接不同顶点，实现高阶多跳推理和多模态比较判断。 |
| [^6] | [Weakly Supervised Text Classification on Free Text Comments in Patient-Reported Outcome Measures.](http://arxiv.org/abs/2308.06199) | 本研究使用弱监督文本分类方法分析了患者报告的结果衡量中的自由文本评论，发现关键字基础的WSTC在有限标记数据的情况下具有潜力和局限性。 |
| [^7] | [Assessing Guest Nationality Composition from Hotel Reviews.](http://arxiv.org/abs/2308.06175) | 本文利用机器学习从非结构化文本评论中提取客人国籍的引用，并展示了一个相对简单的架构可以提供更好的性能和运行时间的权衡。 |
| [^8] | [Task Conditioned BERT for Joint Intent Detection and Slot-filling.](http://arxiv.org/abs/2308.06165) | 本文研究了通过一个统一的模型来解决对话系统中意图跟踪和槽位理解的挑战，并通过在同一语料库上进行多个目标推理的条件控制，提高了模型的性能。 |
| [^9] | [Identification of the Relevance of Comments in Codes Using Bag of Words and Transformer Based Models.](http://arxiv.org/abs/2308.06144) | 本研究通过使用词袋和基于Transformer的模型，对代码注释的相关性进行识别。在训练语料库中，探索了不同的特征工程和文本分类技术，并比较了传统词袋模型和Transformer模型的性能。 |
| [^10] | [Improving Joint Speech-Text Representations Without Alignment.](http://arxiv.org/abs/2308.06125) | 本研究表明，在联合语音-文本表示中，忽略序列长度问题能够自然地实现一致的表示，一致性损失可以提高下游的字错误率。 |
| [^11] | [Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping.](http://arxiv.org/abs/2308.06112) | Lip2Vec是一种通过学习先验模型将嘴唇序列的编码表示映射到音频对的潜空间，并使用音频语音识别模型将生成的音频表示解码为文本的高效稳健的视觉语音识别方法。 |
| [^12] | [Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models.](http://arxiv.org/abs/2308.06111) | 这项研究提出了ZeroShotALI，它使用了一种新颖的推荐系统来改进金融审计中的零样本文本匹配。通过采用大型语言模型（LLM）和经过领域优化的基于transformer的文本匹配解决方案，该系统实现了从报告中推荐与法律要求相符的相关文本段落，并在现有方法上取得了显著的性能提升。 |
| [^13] | [Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes.](http://arxiv.org/abs/2308.06095) | 这项综述研究了以强大语言模型为基础的开放领域对话系统，并探讨了如何通过干预底层语言模型的不同方面，如数据、训练制度或解码，来保证模型的流畅性、信息丰富性、一致性、连贯性以及遵循社会准则的特性。 |
| [^14] | [Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling.](http://arxiv.org/abs/2308.06077) | 本文提出了一种经济有效的语言模型选择框架（CELMOC），通过元模型预测在不同输入上表现良好的语言模型，从而在低成本下实现高整体性能。 |
| [^15] | [A Case Study on Context Encoding in Multi-Encoder based Document-Level Neural Machine Translation.](http://arxiv.org/abs/2308.06063) | 本文研究了多编码器的文档级神经机器翻译中上下文编码的案例，结果表明上下文对代词翻译的准确性影响不大，并且上下文编码器提供了足够的信息来学习语篇级信息。 |
| [^16] | [Learning to Guide Human Experts via Personalized Large Language Models.](http://arxiv.org/abs/2308.06039) | 本论文提出了学习引导（LTG）框架，通过个性化的大型语言模型为人类专家提供有助于指导决策的指导，解决了机器决策和人类决策之间的依赖问题。利用SLOG实现，该框架在医学诊断任务上取得了初步但有希望的结果。 |
| [^17] | [Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing.](http://arxiv.org/abs/2308.06035) | 这篇论文研究了多模态大语言模型（mLLMs）在预测语言处理过程中与人类的视觉-语言集成能力是否一致的问题，并通过实验验证了mLLMs的多模态输入方法可以减少认知负荷，提高感知和理解能力。 |
| [^18] | [Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?.](http://arxiv.org/abs/2308.06032) | 本研究探讨了在加密货币证券案件中，大型语言模型（LLMs）是否能够准确判断违法行为，并比较了由LLM和律师撰写的投诉书对陪审团决策的影响。研究发现，目前的LLMs在法律推理方面表现较弱，但随着未来模型的改进，其潜力有望提升。 |
| [^19] | [Optimizing transformer-based machine translation model for single GPU training: a hyperparameter ablation study.](http://arxiv.org/abs/2308.06017) | 本研究通过消融实验发现，在单个GPU上，参数数量最多的组合并不一定是最有效的，通过减少参数大小可以在不降低翻译质量的情况下训练复杂模型，揭示了超参数选择、模型大小和计算资源需求之间的关系。 |
| [^20] | [Tweet Sentiment Extraction using Viterbi Algorithm with Transfer Learning.](http://arxiv.org/abs/2308.05973) | 这篇论文使用Viterbi算法和迁移学习来提取推特情绪，引入置信度分数和向量作为评估模型的指标，进行模型的内部评估和微调。 |
| [^21] | [LittleMu: Deploying an Online Virtual Teaching Assistant via Heterogeneous Sources Integration and Chain of Teach Prompts.](http://arxiv.org/abs/2308.05935) | 本文提出了一个虚拟的MOOC助教 LittleMu，通过整合异构数据源和教学提示链路来支持广泛范围的准确回答和知识相关的闲聊服务。 |
| [^22] | [PIPPA: A Partially Synthetic Conversational Dataset.](http://arxiv.org/abs/2308.05884) | PIPPA是一个部分合成的数据集，为研究和开发对话AI系统提供了丰富的资源，以应对现有数据集无法捕捉真实世界角色扮演交互的局限性。 |
| [^23] | [LLM As DBA.](http://arxiv.org/abs/2308.05481) | LLM变成DBA，提供数据库维护的诊断和优化建议，通过从文本来源中获取经验和多个LLMs的协作诊断。 |
| [^24] | [Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis.](http://arxiv.org/abs/2308.05476) | 本研究通过比较分析机器学习和基于Transformer的方法在欺诈性文本分类中的效果，揭示了它们的优势和局限性。 |
| [^25] | [Evaluating the Generation Capabilities of Large Chinese Language Models.](http://arxiv.org/abs/2308.04823) | 本文首次对大型中文语言模型在多个学科领域的生成能力进行了全面评估，并提出了Gscore作为衡量生成结果质量的综合指数。 |
| [^26] | [CLASSLA-Stanza: The Next Step for Linguistic Processing of South Slavic Languages.](http://arxiv.org/abs/2308.04255) | CLASSLA-Stanza是一个为南斯拉夫语言提供自动语言注释的流水线，相对于Stanza，在性能和功能上有多个改进，并取得了始终如一的高性能。 |
| [^27] | [3D-EX : A Unified Dataset of Definitions and Dictionary Examples.](http://arxiv.org/abs/2308.03043) | 本文介绍了一个名为3D-EX的数据集，它是一个统一的定义和词典示例的知识存储库，可以用于改进词向量或增强语言模型的上下文表示，并且在下游的自然语言处理任务中表现良好。 |
| [^28] | [Towards Generalist Foundation Model for Radiology.](http://arxiv.org/abs/2308.02463) | 本研究旨在为放射学构建通用基础模型，提出了一个大规模的医学多模态数据集和支持不同放射学任务的架构，同时提出了一个新的评估基准。 |
| [^29] | [Causality Guided Disentanglement for Cross-Platform Hate Speech Detection.](http://arxiv.org/abs/2308.02080) | 本研究提出了一种跨平台仇恨言论检测模型，通过解缠输入表示为不变特征和平台相关特征，实现了对多个未见平台的良好泛化能力。 |
| [^30] | [Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?.](http://arxiv.org/abs/2308.00158) | 本研究探讨了使用Fine-Tuned的OpenAI LLM进行翻译质量估计的能力，实验证明可以通过Fine-Tuned的ChatGPT来预测机器翻译的质量，但仍有改进的空间。 |
| [^31] | [Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education.](http://arxiv.org/abs/2307.12267) | 本研究探索了在教育领域中，由人类和生成性语言模型协作编写的混合文本的AI内容检测方法，将其形式化为识别转换点的任务，以区分人类编写和AI生成的部分。 |
| [^32] | [Efficient Domain Adaptation of Sentence Embeddings using Adapters.](http://arxiv.org/abs/2307.03104) | 本论文提出了一种通过训练轻量级适配器来高效域自适应句子嵌入的方法，避免了微调整个句子嵌入模型的资源消耗。通过训练特定领域的适配器，可以在不同领域中使用同一模型获得良好的性能。 |
| [^33] | [Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement.](http://arxiv.org/abs/2306.14704) | 本研究提出了一个新的数据集，通过在PubMed摘要中使用SNOMED CT版本，解决了先前数据集所存在的问题，可以用于自动化地发现和放置新概念到知识库中。 |
| [^34] | [Trained Transformers Learn Linear Models In-Context.](http://arxiv.org/abs/2306.09927) | 本文研究了Transformer在具有单层线性自注意层的线性回归任务上通过梯度流进行训练的ICL机制，揭示了梯度流具有找到目标函数全局最小值的能力。 |
| [^35] | [ML-SUPERB: Multilingual Speech Universal PERformance Benchmark.](http://arxiv.org/abs/2305.10615) | 本文提出了一个覆盖143种语言、用于自我监督学习模型性能基准的多语种语音基准 ML-SUPERB，并发现自我监督学习模型可以显著提高性能且多语种模型不总是比单语言模型表现更好。 |
| [^36] | [Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations.](http://arxiv.org/abs/2303.16618) | 本篇论文研究了如何使用丰富的元数据注释的信息进行屏幕角色的个性化语言建模，测试表明这样可以有效地进行个性化语言模型的构建，即使对于零样本的演说家也可以应用。 |
| [^37] | [BODEGA: Benchmark for Adversarial Example Generation in Credibility Assessment.](http://arxiv.org/abs/2303.08032) | BODEGA是一个基准测试，用于模拟真实的内容管理场景，在四个误传检测任务上测试受害模型和攻击方法。测试结果表明，在某些情况下，即使进行微小的文本修改，也可以欺骗最准确的分类器。 |
| [^38] | [Cross-modal Contrastive Learning for Multimodal Fake News Detection.](http://arxiv.org/abs/2302.14057) | 这项研究提出了COOLANT，一个用于跨模态假新闻检测的对比学习框架，旨在提升图像和文本的对齐精度，并通过跨模态融合和注意力机制实现更准确和可解释的特征聚合。 |
| [^39] | [Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking.](http://arxiv.org/abs/2302.07189) | 本文提出了基于BERT的实体链接方法BLINKout，通过与特殊NIL实体匹配来识别没有相应KB实体的实体提及，相较于现有方法具有优势。 |
| [^40] | [Which Features are Learned by CodeBert: An Empirical Study of the BERT-based Source Code Representation Learning.](http://arxiv.org/abs/2301.08427) | 该论文研究了CodeBert在源码表示学习中学到了哪些特征，发现当前方法无法有效理解源代码的逻辑，而源代码的表示依赖于程序员定义的变量和函数名。 |
| [^41] | [RT-1: Robotics Transformer for Real-World Control at Scale.](http://arxiv.org/abs/2212.06817) | 本文提出了机器人变压器模型，通过从大规模、多样化、任务无关的数据集中获取知识，并结合高容量架构，实现了在实际控制领域的高性能泛化能力。 |
| [^42] | [Transformers are Short Text Classifiers: A Study of Inductive Short Text Classifiers on Benchmarks and Real-world Datasets.](http://arxiv.org/abs/2211.16878) | 本研究探讨了短文本分类器的性能，并发现Transformers在短文本分类任务中达到了最先进的准确率，引发了是否需要专门的短文本技术的讨论。 |
| [^43] | [Kuaipedia: a Large-scale Multi-modal Short-video Encyclopedia.](http://arxiv.org/abs/2211.00732) | Kuaipedia是一个大规模的多模式短视频百科全书，通过知识视频的形式，能够轻松表达网民对某个项目的各个方面的需求。 |
| [^44] | [There is more than one kind of robustness: Fooling Whisper with adversarial examples.](http://arxiv.org/abs/2210.17316) | 本研究展示了Whisper模型虽然在分布外输入和随机噪声方面显示出了出色的稳健性，但却容易受到对抗干扰的影响。我们通过生成极小的输入扰动来大幅降低Whisper的性能，并对多语言模型的性能产生了影响。这些发现对于实际的安全问题和对抗性稳健ASR的需求具有重要意义。 |
| [^45] | [A Compact End-to-End Model with Local and Global Context for Spoken Language Identification.](http://arxiv.org/abs/2210.15781) | TitaNet-LID是一种紧凑且具有局部和全局上下文的端到端神经网络模型，适用于口语语言识别。尽管尺寸较小，但它可以在不降低性能的情况下实现与最先进模型相似的准确性，并且能够适应不同的声学条件和语言。该模型具有良好的扩展性和处理短语输入的能力。 |
| [^46] | [Constraining Linear-chain CRFs to Regular Languages.](http://arxiv.org/abs/2106.07306) | 本文提出了一种将线性链条件随机场（CRFs）限制到正则语言的方法，该方法可以对广泛的约束进行建模，并允许在训练过程中引入这些约束。 |

# 详细

[^1]: 使用指令反向翻译的自动对齐方法

    Self-Alignment with Instruction Backtranslation. (arXiv:2308.06259v1 [cs.CL])

    [http://arxiv.org/abs/2308.06259](http://arxiv.org/abs/2308.06259)

    本论文提出了一种自动对齐方法，通过为人工编写的文本添加指令标签来构建高质量的指令跟踪语言模型。该方法通过自我增强和自我筛选生成训练示例，并且在Alpaca排行榜上表现出非常高效的自动对齐能力。

    

    我们提出了一种可扩展的方法，通过自动为人工编写的文本添加相应的指令标签来构建高质量的指令跟踪语言模型。我们的方法名为指令反向翻译，它从在少量种子数据和给定的网络语料库上微调的语言模型开始。种子模型用于通过为网络文档生成指令提示（自我增强）来构建训练示例，然后从这些候选示例中选择高质量的示例（自我筛选）。然后使用这些数据来微调更强的模型。通过使用我们方法的两次迭代来微调LLaMa，我们得到的模型在Alpaca排行榜上击败了所有其他基于LLaMa的模型，而无需依赖蒸馏数据，展示了非常有效的自动对齐能力。

    We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.
    
[^2]: Covid-19印度推文分类的公众情感分析

    Covid-19 Public Sentiment Analysis for Indian Tweets Classification. (arXiv:2308.06241v1 [cs.CL])

    [http://arxiv.org/abs/2308.06241](http://arxiv.org/abs/2308.06241)

    本文主要关注印度COVID-19推文的情感分析，通过提取Twitter数据并对其进行情感分析，有助于分析推文中非结构化、异构的观点。

    

    当世界范围内发生任何特殊事件时，社交媒体往往是最快的新闻传播载体，同时也涉及到该事件的后果。人们可以通过社交网络收集有关人们情感、行为和观点的丰富信息。本文主要关注对印度COVID-19推文的情感分析。我们展示了如何提取Twitter数据，并对其进行情感分析。这有助于分析推文中非常非结构化、异构的观点，有时是正面的、负面的，或是中性的。

    When any extraordinary event takes place in the world wide area, it is the social media that acts as the fastest carrier of the news along with the consequences dealt with that event. One can gather much information through social networks regarding the sentiments, behavior, and opinions of the people. In this paper, we focus mainly on sentiment analysis of twitter data of India which comprises of COVID-19 tweets. We show how Twitter data has been extracted and then run sentimental analysis queries on it. This is helpful to analyze the information in the tweets where opinions are highly unstructured, heterogeneous, and are either positive or negative or neutral in some cases.
    
[^3]: KETM:一种知识增强的文本匹配方法

    KETM:A Knowledge-Enhanced Text Matching method. (arXiv:2308.06235v1 [cs.CL])

    [http://arxiv.org/abs/2308.06235](http://arxiv.org/abs/2308.06235)

    KETM是一种知识增强的文本匹配方法，通过从外部知识源中获取常识知识来丰富上下文表示，从而提高模型的理解和推理能力。实验证明，该方法在常识知识推理任务中具有更好的性能和鲁棒性。

    

    文本匹配是将两个文本进行匹配并确定它们之间的关系的任务，在自然语言处理任务中，如阅读理解和问答系统中有广泛应用。主流方法是通过计算文本表示或通过注意机制与文本进行交互，这在文本匹配任务中有效。然而，对于需要基于常识知识推理的文本，这些模型的性能不足。因此，在本文中，我们引入了一种新的文本匹配模型，称为知识增强的文本匹配模型（KETM），以从外部知识源中丰富上下文表示，以增强我们的模型的理解和推理能力。首先，我们使用Wiktionary检索文本词的定义作为我们的外部知识。其次，我们将文本和知识输入文本匹配模块，提取它们的特征向量。文本匹配模块使用基于知识增强的特征向量进行匹配，以提高匹配性能。最后，我们在几个文本匹配任务上进行了实验评估，并与现有的文本匹配模型进行了比较。实验结果表明，我们的KETM模型在常识知识推理任务上具有更好的性能和鲁棒性。

    Text matching is the task of matching two texts and determining the relationship between them, which has extensive applications in natural language processing tasks such as reading comprehension, and Question-Answering systems. The mainstream approach is to compute text representations or to interact with the text through attention mechanism, which is effective in text matching tasks. However, the performance of these models is insufficient for texts that require commonsense knowledge-based reasoning. To this end, in this paper, We introduce a new model for text matching called the Knowledge Enhanced Text Matching model (KETM), to enrich contextual representations with real-world common-sense knowledge from external knowledge sources to enhance our model understanding and reasoning. First, we use Wiktionary to retrieve the text word definitions as our external knowledge. Secondly, we feed text and knowledge to the text matching module to extract their feature vectors. The text matching
    
[^4]: 一个基于大型语言模型的增强对话推荐系统

    A Large Language Model Enhanced Conversational Recommender System. (arXiv:2308.06212v1 [cs.IR])

    [http://arxiv.org/abs/2308.06212](http://arxiv.org/abs/2308.06212)

    一种基于大型语言模型的增强对话推荐系统，通过利用大型语言模型的推理和生成能力，有效地管理子任务、解决不同的子任务，并生成与用户交互的回应。

    

    对话推荐系统旨在通过对话界面向用户推荐高质量的物品。它通常包含多个子任务，如用户偏好获取、推荐、解释和物品信息搜索。为了开发有效的对话推荐系统，面临一些挑战：1）如何正确管理子任务；2）如何有效解决不同的子任务；3）如何正确生成与用户交互的回应。最近，大型语言模型展示了前所未有的推理和生成能力，为开发更强大的对话推荐系统提供了新的机会。在这项工作中，我们提出了一种新的基于大型语言模型的对话推荐系统，称为LLMCRS，来解决上述挑战。在子任务管理方面，我们利用大型语言模型的推理能力来有效地管理子任务。在子任务解决方面，我们将大型语言模型与不同子任务的专家模型相结合，实现了增强性能。在回应生成方面，我们利用生成能力来生成回应。

    Conversational recommender systems (CRSs) aim to recommend high-quality items to users through a dialogue interface. It usually contains multiple sub-tasks, such as user preference elicitation, recommendation, explanation, and item information search. To develop effective CRSs, there are some challenges: 1) how to properly manage sub-tasks; 2) how to effectively solve different sub-tasks; and 3) how to correctly generate responses that interact with users. Recently, Large Language Models (LLMs) have exhibited an unprecedented ability to reason and generate, presenting a new opportunity to develop more powerful CRSs. In this work, we propose a new LLM-based CRS, referred to as LLMCRS, to address the above challenges. For sub-task management, we leverage the reasoning ability of LLM to effectively manage sub-task. For sub-task solving, we collaborate LLM with expert models of different sub-tasks to achieve the enhanced performance. For response generation, we utilize the generation abili
    
[^5]: 想像一位专家：多模态思维超图在提升基础模型中的推理能力

    Thinking Like an Expert:Multimodal Hypergraph-of-Thought (HoT) Reasoning to boost Foundation Modals. (arXiv:2308.06207v1 [cs.CL])

    [http://arxiv.org/abs/2308.06207](http://arxiv.org/abs/2308.06207)

    本文致力于构建一个思维范式，能够像专家一样思考，通过超图的超边连接不同顶点，实现高阶多跳推理和多模态比较判断。

    

    推理能力是基础模型最关键的能力之一，表示其应对复杂推理任务的能力。思维链技术被广泛认为是增强基础模型推理能力的有效方法，并且受到了很大关注。然而，思维链的推理过程是线性的、一步一步的，类似于个人的逻辑推理，适用于解决一般和稍微复杂的问题。相反，专家的思维模式具有两个显著特点，在思维链中无法适当处理，即高阶多跳推理和多模态比较判断。因此，本文的核心动机是超越思维链，构建一个能思考像专家一样的推理范式。超图的超边可以连接不同的顶点，因此自然适用于建模高阶关系。在此启发下，本文创新地提出了一种方法

    Reasoning ability is one of the most crucial capabilities of a foundation model, signifying its capacity to address complex reasoning tasks. Chain-of-Thought (CoT) technique is widely regarded as one of the effective methods for enhancing the reasoning ability of foundation models and has garnered significant attention. However, the reasoning process of CoT is linear, step-by-step, similar to personal logical reasoning, suitable for solving general and slightly complicated problems. On the contrary, the thinking pattern of an expert owns two prominent characteristics that cannot be handled appropriately in CoT, i.e., high-order multi-hop reasoning and multimodal comparative judgement. Therefore, the core motivation of this paper is transcending CoT to construct a reasoning paradigm that can think like an expert. The hyperedge of a hypergraph could connect various vertices, making it naturally suitable for modelling high-order relationships. Inspired by this, this paper innovatively pro
    
[^6]: 在患者报告的结果衡量中，对自由文本评论进行弱监督的文本分类

    Weakly Supervised Text Classification on Free Text Comments in Patient-Reported Outcome Measures. (arXiv:2308.06199v1 [cs.CL])

    [http://arxiv.org/abs/2308.06199](http://arxiv.org/abs/2308.06199)

    本研究使用弱监督文本分类方法分析了患者报告的结果衡量中的自由文本评论，发现关键字基础的WSTC在有限标记数据的情况下具有潜力和局限性。

    

    患者报告的结果测量(PROMs)数据中的自由文本评论(FTC)通常使用手动方法进行分析，如内容分析，这种方法耗时且费力。机器学习分析方法通常是无监督的，需要进行后期解释。弱监督的文本分类(WSTC)可以是一种有价值的分析方法，用于对标记数据有限的领域特定文本数据进行分类。本文将五种WSTC技术应用于PROMs数据中的FTC，以确定结直肠癌患者报告的与健康相关的生活质量(HRQoL)主题。WSTC方法标记FTC中提到的所有主题。结果显示，PROMs数据的性能中等，主要是由于模型的精确度和主题之间的差异。对分类性能的评估揭示了基于关键字的WSTC在标记数据有限的情况下标记PROMs FTC的潜力和局限性。

    Free text comments (FTC) in patient-reported outcome measures (PROMs) data are typically analysed using manual methods, such as content analysis, which is labour-intensive and time-consuming. Machine learning analysis methods are largely unsupervised, necessitating post-analysis interpretation. Weakly supervised text classification (WSTC) can be a valuable method of analysis to classify domain-specific text data in which there is limited labelled data. In this paper, we apply five WSTC techniques to FTC in PROMs data to identify health-related quality of life (HRQoL) themes reported by colorectal cancer patients. The WSTC methods label all the themes mentioned in the FTC. The results showed moderate performance on the PROMs data, mainly due to the precision of the models, and variation between themes. Evaluation of the classification performance illustrated the potential and limitations of keyword based WSTC to label PROMs FTC when labelled data is limited.
    
[^7]: 从酒店评论中评估客人的国籍构成

    Assessing Guest Nationality Composition from Hotel Reviews. (arXiv:2308.06175v1 [cs.CL])

    [http://arxiv.org/abs/2308.06175](http://arxiv.org/abs/2308.06175)

    本文利用机器学习从非结构化文本评论中提取客人国籍的引用，并展示了一个相对简单的架构可以提供更好的性能和运行时间的权衡。

    

    许多酒店通过针对特定市场的客户获取努力，以最好地预测客人的个人偏好和需求。同样，这种战略定位是有效的营销预算分配的先决条件。官方统计数据报告了来自不同国家的游客数量，但没有关于个别企业客人构成的细粒度信息。然而，竞争对手、供应商、研究人员和普通公众对这些数据的兴趣越来越大。我们展示了如何利用机器学习从非结构化文本评论中提取客人国籍的引用，以动态评估和监测个别企业客人构成的动态变化。特别是，我们证明了预训练嵌入和堆叠的LSTM层的相当简单的架构提供了比复杂的最先进语言模型更好的性能和运行时间的权衡。

    Many hotels target guest acquisition efforts to specific markets in order to best anticipate individual preferences and needs of their guests. Likewise, such strategic positioning is a prerequisite for efficient marketing budget allocation. Official statistics report on the number of visitors from different countries, but no fine-grained information on the guest composition of individual businesses exists. There is, however, growing interest in such data from competitors, suppliers, researchers and the general public. We demonstrate how machine learning can be leveraged to extract references to guest nationalities from unstructured text reviews in order to dynamically assess and monitor the dynamics of guest composition of individual businesses. In particular, we show that a rather simple architecture of pre-trained embeddings and stacked LSTM layers provides a better performance-runtime tradeoff than more complex state-of-the-art language models.
    
[^8]: 任务条件下的BERT用于联合意图检测和槽位填充

    Task Conditioned BERT for Joint Intent Detection and Slot-filling. (arXiv:2308.06165v1 [cs.CL])

    [http://arxiv.org/abs/2308.06165](http://arxiv.org/abs/2308.06165)

    本文研究了通过一个统一的模型来解决对话系统中意图跟踪和槽位理解的挑战，并通过在同一语料库上进行多个目标推理的条件控制，提高了模型的性能。

    

    对话系统需要解决用户意图的不可预测性以跟踪对话状态，并且需要处理槽位的异质性以理解用户的偏好。本文研究的假设是将这些挑战作为一个统一的模型来解决，将允许在不同的任务之间传递参数支持数据。提出的基于Transformer编码器的模型，通过丰富的输入来将模型条件于目标推理。通过对相同语料库上的多个目标推理（即意图和多个槽位类型）对Transformer编码器进行条件控制，允许学习到比单任务模型更丰富的语言交互。实验结果表明，将模型条件于越来越多的对话推理任务可以改善结果：在MultiWOZ数据集上，通过条件控制意图，联合意图和槽位检测的性能提高了3.2%，通过条件控制多个槽位类型，提高了10.8%。

    Dialogue systems need to deal with the unpredictability of user intents to track dialogue state and the heterogeneity of slots to understand user preferences. In this paper we investigate the hypothesis that solving these challenges as one unified model will allow the transfer of parameter support data across the different tasks. The proposed principled model is based on a Transformer encoder, trained on multiple tasks, and leveraged by a rich input that conditions the model on the target inferences. Conditioning the Transformer encoder on multiple target inferences over the same corpus, i.e., intent and multiple slot types, allows learning richer language interactions than a single-task model would be able to. In fact, experimental results demonstrate that conditioning the model on an increasing number of dialogue inference tasks leads to improved results: on the MultiWOZ dataset, the joint intent and slot detection can be improved by 3.2\% by conditioning on intent, 10.8\% by conditi
    
[^9]: 使用词袋和基于Transformer的模型识别代码评论的相关性

    Identification of the Relevance of Comments in Codes Using Bag of Words and Transformer Based Models. (arXiv:2308.06144v1 [cs.IR])

    [http://arxiv.org/abs/2308.06144](http://arxiv.org/abs/2308.06144)

    本研究通过使用词袋和基于Transformer的模型，对代码注释的相关性进行识别。在训练语料库中，探索了不同的特征工程和文本分类技术，并比较了传统词袋模型和Transformer模型的性能。

    

    今年，信息检索论坛(FIRE)启动了一个共享任务，用于对不同代码段的评论进行分类。这是一个二元文本分类任务，目标是确定给定代码段的评论是否相关。印度科学教育与研究院博帕尔分院(IISERB)的BioNLP-IISERB小组参与了这项任务，并为五种不同的模型提交了五种运行结果。本文介绍了这些模型的概况和在训练语料库上的其他重要发现。这些方法涉及不同的特征工程方案和文本分类技术。对于词袋模型，我们探索了不同的分类器，如随机森林、支持向量机和逻辑回归，以识别给定训练语料库中的重要特征。此外，还研究了基于预训练Transformer的模型。

    The Forum for Information Retrieval (FIRE) started a shared task this year for classification of comments of different code segments. This is binary text classification task where the objective is to identify whether comments given for certain code segments are relevant or not. The BioNLP-IISERB group at the Indian Institute of Science Education and Research Bhopal (IISERB) participated in this task and submitted five runs for five different models. The paper presents the overview of the models and other significant findings on the training corpus. The methods involve different feature engineering schemes and text classification techniques. The performance of the classical bag of words model and transformer-based models were explored to identify significant features from the given training corpus. We have explored different classifiers viz., random forest, support vector machine and logistic regression using the bag of words model. Furthermore, the pre-trained transformer based models 
    
[^10]: 在不进行对齐的情况下改进联合语音-文本表示

    Improving Joint Speech-Text Representations Without Alignment. (arXiv:2308.06125v1 [cs.CL])

    [http://arxiv.org/abs/2308.06125](http://arxiv.org/abs/2308.06125)

    本研究表明，在联合语音-文本表示中，忽略序列长度问题能够自然地实现一致的表示，一致性损失可以提高下游的字错误率。

    

    过去一年，基于跨模态表示空间的文本引导图像生成取得了令人瞩目的进展，其中文本和图像领域以联合的方式表示。在ASR中，这个想法被应用为联合语音-文本编码器，通过同时训练不匹配的语音和文本，可以扩展到非常大的参数模型的容量。虽然这些方法表现出了希望，但它们需要特殊处理语音和文本之间的序列长度不匹配问题，要么通过上采样启发式方法，要么通过一个显式的对齐模型。在这项工作中，我们提供了证据表明，联合语音-文本编码器通过忽略序列长度自然而然地实现了一致的表示，并且认为一致性损失可以弥补长度差异，并简单地假设最佳对齐。我们展示了这样的损失在大参数单语言和多语言系统中提高了下游的字错误率。

    The last year has seen astonishing progress in text-prompted image generation premised on the idea of a cross-modal representation space in which the text and image domains are represented jointly. In ASR, this idea has found application as joint speech-text encoders that can scale to the capacities of very large parameter models by being trained on both unpaired speech and text. While these methods show promise, they have required special treatment of the sequence-length mismatch inherent in speech and text, either by up-sampling heuristics or an explicit alignment model. In this work, we offer evidence that joint speech-text encoders naturally achieve consistent representations across modalities by disregarding sequence length, and argue that consistency losses could forgive length differences and simply assume the best alignment. We show that such a loss improves downstream WER in both a large-parameter monolingual and multilingual system.
    
[^11]: Lip2Vec:通过潜空间到潜空间的视听表达映射实现高效稳健的视觉语音识别

    Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping. (arXiv:2308.06112v1 [cs.SD])

    [http://arxiv.org/abs/2308.06112](http://arxiv.org/abs/2308.06112)

    Lip2Vec是一种通过学习先验模型将嘴唇序列的编码表示映射到音频对的潜空间，并使用音频语音识别模型将生成的音频表示解码为文本的高效稳健的视觉语音识别方法。

    

    视觉语音识别(VSR)与常见的感知任务不同，它需要对视频序列进行更深入的推理，即使对于人类专家来说也是如此。尽管VSR近年来取得了一些进展，但目前的方法依赖于有标签的数据来完全训练或微调模型以预测目标语音，这限制了它们在训练集之外的广泛泛化能力，并导致在面对具有挑战性的分布情景时性能退化。与以往涉及辅助损失或复杂训练过程和架构的方法不同，我们提出了一个简单的方法，名为Lip2Vec，它基于学习一个先验模型。给定一个强大的视觉语音编码器，该网络将嘴唇序列的编码潜在表示映射到它们对应的音频对的潜空间，这些潜空间对于有效的文本解码具有足够的不变性。然后，使用现成的音频语音识别(ASR)模型将生成的音频表示解码为文本。该方法具备高效和稳健的特点。

    Visual Speech Recognition (VSR) differs from the common perception tasks as it requires deeper reasoning over the video sequence, even by human experts. Despite the recent advances in VSR, current approaches rely on labeled data to fully train or finetune their models predicting the target speech. This hinders their ability to generalize well beyond the training set and leads to performance degeneration under out-of-distribution challenging scenarios. Unlike previous works that involve auxiliary losses or complex training procedures and architectures, we propose a simple approach, named Lip2Vec that is based on learning a prior model. Given a robust visual speech encoder, this network maps the encoded latent representations of the lip sequence to their corresponding latents from the audio pair, which are sufficiently invariant for effective text decoding. The generated audio representation is then decoded to text using an off-the-shelf Audio Speech Recognition (ASR) model. The proposed
    
[^12]: 使用大型语言模型提升金融审计的零样本文本匹配

    Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models. (arXiv:2308.06111v1 [cs.CL])

    [http://arxiv.org/abs/2308.06111](http://arxiv.org/abs/2308.06111)

    这项研究提出了ZeroShotALI，它使用了一种新颖的推荐系统来改进金融审计中的零样本文本匹配。通过采用大型语言模型（LLM）和经过领域优化的基于transformer的文本匹配解决方案，该系统实现了从报告中推荐与法律要求相符的相关文本段落，并在现有方法上取得了显著的性能提升。

    

    审计金融文件是一个非常繁琐和耗时的过程。目前，通过使用基于人工智能的解决方案可以简化这一过程，以推荐与严格会计标准的法律要求相符的报告中的相关文本段落。然而，这些方法需要定期进行微调，并且通常在工业环境中缺乏大量的注释数据。因此，我们提出了ZeroShotALI，这是一个新颖的推荐系统，利用了最先进的大型语言模型（LLM）与领域特定的优化的基于transformer的文本匹配解决方案。我们发现，首先使用自定义BERT模型检索与法律要求相符的若干最佳匹配的文档部分，然后使用LLM对这些选择进行过滤，可以显著改善现有方法的性能。

    Auditing financial documents is a very tedious and time-consuming process. As of today, it can already be simplified by employing AI-based solutions to recommend relevant text passages from a report for each legal requirement of rigorous accounting standards. However, these methods need to be fine-tuned regularly, and they require abundant annotated data, which is often lacking in industrial environments. Hence, we present ZeroShotALI, a novel recommender system that leverages a state-of-the-art large language model (LLM) in conjunction with a domain-specifically optimized transformer-based text-matching solution. We find that a two-step approach of first retrieving a number of best matching document sections per legal requirement with a custom BERT-based model and second filtering these selections using an LLM yields significant performance improvements over existing approaches.
    
[^13]: 神经对话模型及其控制方法：失败和修复的综述

    Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes. (arXiv:2308.06095v1 [cs.CL])

    [http://arxiv.org/abs/2308.06095](http://arxiv.org/abs/2308.06095)

    这项综述研究了以强大语言模型为基础的开放领域对话系统，并探讨了如何通过干预底层语言模型的不同方面，如数据、训练制度或解码，来保证模型的流畅性、信息丰富性、一致性、连贯性以及遵循社会准则的特性。

    

    最近，以强大语言模型为基础的条件语言模型能够以看似流利的方式延续任何类型的文本来源。这个事实促进了对基于强大语言模型的开放领域对话系统的研究，旨在通过生成适当的对话内容来模仿对话方的行为。然而，从语言学的角度来看，参与对话的复杂性很高。在这项综述中，我们从这一特定研究领域的角度解释了Grice的合作性对话最大规则，并将文献系统化地归纳为一个贡献何种内容是适当的方面：神经对话模型必须流畅、信息丰富、一致、连贯，并遵循社会准则。为了确保这些特性，最近的方法尝试在数据、训练制度或解码等各个干预点上控制底层语言模型。按照这些类别和干预点进行排序，我们讨论了一些有希望的方法。

    Recent conditional language models are able to continue any kind of text source in an often seemingly fluent way. This fact encouraged research in the area of open-domain conversational systems that are based on powerful language models and aim to imitate an interlocutor by generating appropriate contributions to a written dialogue. From a linguistic perspective, however, the complexity of contributing to a conversation is high. In this survey, we interpret Grice's maxims of cooperative conversation from the perspective of this specific research area and systematize the literature under the aspect of what makes a contribution appropriate: A neural conversation model has to be fluent, informative, consistent, coherent, and follow social norms. In order to ensure these qualities, recent approaches try to tame the underlying language models at various intervention points, such as data, training regime or decoding. Sorted by these categories and intervention points, we discuss promising at
    
[^14]: 飞拍或大炮？通过元模型选择经济有效的语言模型

    Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling. (arXiv:2308.06077v1 [cs.CL])

    [http://arxiv.org/abs/2308.06077](http://arxiv.org/abs/2308.06077)

    本文提出了一种经济有效的语言模型选择框架（CELMOC），通过元模型预测在不同输入上表现良好的语言模型，从而在低成本下实现高整体性能。

    

    生成式语言模型在数据科学领域中变得无处不在。对于各种任务，可以将输入作为自然语言提示，通过LM的输出来提取解决方案。LM的性能随着模型大小的增加而不断提高，但同时查询越来越大的模型的经济成本也在增加。然而，不是所有的输入都很难：有些输入需要更大的LM才能获得令人满意的解决方案，而对于其他输入，较小的LM就足够了。基于这个事实，我们设计了一个经济有效的语言模型选择框架（CELMOC）。给定一组输入和一组候选LM，CELMOC根据所谓的元模型聪明地将每个输入分配给一个在该输入上预测表现良好的LM，以期在低成本下实现高整体性能。用户可以灵活调整成本与性能的权衡。选项包括，最大化总体性能（或处理输入的数量）等。

    Generative language models (LMs) have become omnipresent across data science. For a wide variety of tasks, inputs can be phrased as natural language prompts for an LM, from whose output the solution can then be extracted. LM performance has consistently been increasing with model size - but so has the monetary cost of querying the ever larger models. Importantly, however, not all inputs are equally hard: some require larger LMs for obtaining a satisfactory solution, whereas for others smaller LMs suffice. Based on this fact, we design a framework for Cost-Effective Language Model Choice (CELMOC). Given a set of inputs and a set of candidate LMs, CELMOC judiciously assigns each input to an LM predicted to do well on the input according to a so-called meta-model, aiming to achieve high overall performance at low cost. The cost-performance trade-off can be flexibly tuned by the user. Options include, among others, maximizing total expected performance (or the number of processed inputs) w
    
[^15]: 多编码器的文档级神经机器翻译中上下文编码的案例研究

    A Case Study on Context Encoding in Multi-Encoder based Document-Level Neural Machine Translation. (arXiv:2308.06063v1 [cs.CL])

    [http://arxiv.org/abs/2308.06063](http://arxiv.org/abs/2308.06063)

    本文研究了多编码器的文档级神经机器翻译中上下文编码的案例，结果表明上下文对代词翻译的准确性影响不大，并且上下文编码器提供了足够的信息来学习语篇级信息。

    

    最近的研究表明，多编码器模型对上下文的选择不敏感，并且上下文编码器产生的噪声有助于提高模型的BLEU分数。本文通过在三种不同上下文设置下训练多编码器模型，并评估上下文感知代词翻译测试集来进一步探索这个想法。具体来说，我们在ContraPro测试集上评估模型，以研究不同上下文对代词翻译准确性的影响。结果表明，即使上下文是随机的，该模型在ContraPro测试集上表现良好。我们还分析了源表示以研究上下文编码器是否产生噪声。我们的分析表明，上下文编码器提供了足够的信息来学习语篇级信息。此外，我们观察到将选择的上下文（即前两个句子）混合在一起时。

    Recent studies have shown that the multi-encoder models are agnostic to the choice of context, and the context encoder generates noise which helps improve the models in terms of BLEU score. In this paper, we further explore this idea by evaluating with context-aware pronoun translation test set by training multi-encoder models trained on three different context settings viz, previous two sentences, random two sentences, and a mix of both as context. Specifically, we evaluate the models on the ContraPro test set to study how different contexts affect pronoun translation accuracy. The results show that the model can perform well on the ContraPro test set even when the context is random. We also analyze the source representations to study whether the context encoder generates noise. Our analysis shows that the context encoder provides sufficient information to learn discourse-level information. Additionally, we observe that mixing the selected context (the previous two sentences in this c
    
[^16]: 学习通过个性化的大型语言模型指导人类专家

    Learning to Guide Human Experts via Personalized Large Language Models. (arXiv:2308.06039v1 [cs.AI])

    [http://arxiv.org/abs/2308.06039](http://arxiv.org/abs/2308.06039)

    本论文提出了学习引导（LTG）框架，通过个性化的大型语言模型为人类专家提供有助于指导决策的指导，解决了机器决策和人类决策之间的依赖问题。利用SLOG实现，该框架在医学诊断任务上取得了初步但有希望的结果。

    

    在学习推迟的过程中，一个预测器识别出风险决策并将它们推迟给人类专家。这种设置的一个关键问题是，由于锚定偏见，专家可能会过分依赖于机器的决策。同时，每当机器选择推迟选项时，专家必须完全自主地做出决策。为了解决这个问题，我们提出了学习引导（LTG）的另一种框架，在这个框架中，机器不是提供现成的决策，而是提供有助于指导决策的指导，并且人类完全负责做出决策。我们还介绍了SLOG，这是一个LTG实现，利用（少量）人类监督将通用大型语言模型转化为能够生成文本指导的模块，并在医学诊断任务上展示了初步但有希望的结果。

    In learning to defer, a predictor identifies risky decisions and defers them to a human expert. One key issue with this setup is that the expert may end up over-relying on the machine's decisions, due to anchoring bias. At the same time, whenever the machine chooses the deferral option the expert has to take decisions entirely unassisted. As a remedy, we propose learning to guide (LTG), an alternative framework in which -- rather than suggesting ready-made decisions -- the machine provides guidance useful to guide decision-making, and the human is entirely responsible for coming up with a decision. We also introduce SLOG, an LTG implementation that leverages (a small amount of) human supervision to convert a generic large language model into a module capable of generating textual guidance, and present preliminary but promising results on a medical diagnosis task.
    
[^17]: 多模态大语言模型在预测语言处理期间表现出人类视觉-语言集成的证据

    Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing. (arXiv:2308.06035v1 [cs.AI])

    [http://arxiv.org/abs/2308.06035](http://arxiv.org/abs/2308.06035)

    这篇论文研究了多模态大语言模型（mLLMs）在预测语言处理过程中与人类的视觉-语言集成能力是否一致的问题，并通过实验验证了mLLMs的多模态输入方法可以减少认知负荷，提高感知和理解能力。

    

    大语言模型（LLMs）的先进语言处理能力引发了关于它们是否能够复制人类认知过程的争议。LLMs和人类在语言处理方面的一个区别在于，语言输入通常建立在多个知觉模态上，而大多数LLMs仅处理基于文本的信息。多模态基础使人类能够整合视觉背景与语言信息，从而对即将出现的单词的空间施加限制，减少认知负荷，提高感知和理解能力。最近的多模态LLMs（mLLMs）结合了视觉和语言嵌入空间，并使用变压器类型的注意机制进行下一个单词的预测。在多大程度上，基于多模态输入的预测语言处理在mLLMs和人类中吻合？为了回答这个问题，200名被试观看了短的视听剪辑，并估计了即将出现的动词或名词的可预测性。

    The advanced language processing abilities of large language models (LLMs) have stimulated debate over their capacity to replicate human-like cognitive processes. One differentiating factor between language processing in LLMs and humans is that language input is often grounded in more than one perceptual modality, whereas most LLMs process solely text-based information. Multimodal grounding allows humans to integrate - e.g. visual context with linguistic information and thereby place constraints on the space of upcoming words, reducing cognitive load and improving perception and comprehension. Recent multimodal LLMs (mLLMs) combine visual and linguistic embedding spaces with a transformer type attention mechanism for next-word prediction. To what extent does predictive language processing based on multimodal input align in mLLMs and humans? To answer this question, 200 human participants watched short audio-visual clips and estimated the predictability of an upcoming verb or noun. The 
    
[^18]: 加密货币证券案件中的大型语言模型：ChatGPT能否取代律师？

    Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?. (arXiv:2308.06032v1 [cs.AI])

    [http://arxiv.org/abs/2308.06032](http://arxiv.org/abs/2308.06032)

    本研究探讨了在加密货币证券案件中，大型语言模型（LLMs）是否能够准确判断违法行为，并比较了由LLM和律师撰写的投诉书对陪审团决策的影响。研究发现，目前的LLMs在法律推理方面表现较弱，但随着未来模型的改进，其潜力有望提升。

    

    大型语言模型（LLMs）可以增强对法律系统的访问。然而，关于它们在进行法律任务方面的有效性的实证研究非常有限。我们研究涉及加密货币的证券案件，作为AI可以支持法律过程的众多情境之一，研究LLMs的法律推理和起草能力。我们检查以下两个方面：a）LLM能否准确确定事实模式中可能存在的违法行为，b）基于LLM和律师撰写的投诉书，陪审团的决策是否有所差异。我们将真实案例中的事实模式输入GPT-3.5，并评估其确定正确潜在违法行为并排除虚假违法行为的能力。其次，我们请模拟陪审员评估LLM和律师撰写的投诉书。GPT-3.5的法律推理能力较弱，但我们预期未来模型的改进，特别是考虑到它建议的违法行为往往是正确的（它仅仅过于保守）。

    Large Language Models (LLMs) could enhance access to the legal system. However, empirical research on their effectiveness in conducting legal tasks is scant. We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying LLMs' legal reasoning and drafting capabilities. We examine whether a) an LLM can accurately determine which laws are potentially being violated from a fact pattern, and b) whether there is a difference in juror decision-making based on complaints written by a lawyer compared to an LLM. We feed fact patterns from real-life cases to GPT-3.5 and evaluate its ability to determine correct potential violations from the scenario and exclude spurious violations. Second, we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely m
    
[^19]: 优化单GPU训练的基于transformer的机器翻译模型：超参数消融研究

    Optimizing transformer-based machine translation model for single GPU training: a hyperparameter ablation study. (arXiv:2308.06017v1 [cs.CL])

    [http://arxiv.org/abs/2308.06017](http://arxiv.org/abs/2308.06017)

    本研究通过消融实验发现，在单个GPU上，参数数量最多的组合并不一定是最有效的，通过减少参数大小可以在不降低翻译质量的情况下训练复杂模型，揭示了超参数选择、模型大小和计算资源需求之间的关系。

    

    在机器翻译任务中，模型复杂性和性能之间的关系通常被认为是线性的，驱动参数数量增加并对多个GPU等计算资源提出要求。为了探索这一假设，本研究通过超参数消融系统地研究了一个基于序列到序列的机器翻译管道在单个NVIDIA A100 GPU上的影响。与预期相反，我们的实验发现具有最多参数的组合未必是最有效的。这一意外的发现促使我们仔细减少参数大小，揭示了能够在单个GPU上训练复杂模型而不损害翻译质量的“甜点”。这些发现展示了超参数选择、模型大小和计算资源需求之间的复杂关系。这项研究的见解对于努力进行机器翻译的改进工作有所贡献。

    In machine translation tasks, the relationship between model complexity and performance is often presumed to be linear, driving an increase in the number of parameters and consequent demands for computational resources like multiple GPUs. To explore this assumption, this study systematically investigates the effects of hyperparameters through ablation on a sequence-to-sequence machine translation pipeline, utilizing a single NVIDIA A100 GPU. Contrary to expectations, our experiments reveal that combinations with the most parameters were not necessarily the most effective. This unexpected insight prompted a careful reduction in parameter sizes, uncovering "sweet spots" that enable training sophisticated models on a single GPU without compromising translation quality. The findings demonstrate an intricate relationship between hyperparameter selection, model size, and computational resource needs. The insights from this study contribute to the ongoing efforts to make machine translation m
    
[^20]: 使用Viterbi算法和迁移学习进行推特情绪提取

    Tweet Sentiment Extraction using Viterbi Algorithm with Transfer Learning. (arXiv:2308.05973v1 [cs.AI])

    [http://arxiv.org/abs/2308.05973](http://arxiv.org/abs/2308.05973)

    这篇论文使用Viterbi算法和迁移学习来提取推特情绪，引入置信度分数和向量作为评估模型的指标，进行模型的内部评估和微调。

    

    推特情绪提取是提取句子中最重要部分的过程，判断情绪是积极还是消极。本研究旨在识别推特句子中引起情感的部分。为了达到这个目标，我们继续改进作者之前修改的Viterbi算法，使其能够接收预训练的模型参数。我们引入置信度分数和向量作为两个指标，用于在评估最终结果之前对模型进行内部评估。然后，我们介绍了一种微调这个非参数模型的方法。我们发现，随着置信度分数向量准确地显示出最不自信的预测状态的位置，以及修改是否改善了置信度分数或微调是否朝着错误的方向进行，模型变得更易解释。

    Tweet sentiment extraction extracts the most significant portion of the sentence, determining whether the sentiment is positive or negative. This research aims to identify the part of tweet sentences that strikes any emotion. To reach this objective, we continue improving the Viterbi algorithm previously modified by the author to make it able to receive pre-trained model parameters. We introduce the confidence score and vector as two indicators responsible for evaluating the model internally before assessing the final results. We then present a method to fine-tune this nonparametric model. We found that the model gets highly explainable as the confidence score vector reveals precisely where the least confidence predicted states are and if the modifications approved ameliorate the confidence score or if the tuning is going in the wrong direction.
    
[^21]: LittleMu：通过异构数据源整合和教学提示链路部署在线虚拟助教

    LittleMu: Deploying an Online Virtual Teaching Assistant via Heterogeneous Sources Integration and Chain of Teach Prompts. (arXiv:2308.05935v1 [cs.CL])

    [http://arxiv.org/abs/2308.05935](http://arxiv.org/abs/2308.05935)

    本文提出了一个虚拟的MOOC助教 LittleMu，通过整合异构数据源和教学提示链路来支持广泛范围的准确回答和知识相关的闲聊服务。

    

    在教育的漫长历史中，助教在学习中发挥了重要作用。然而，由于真实在线教育场景的复杂性和缺乏训练数据，很少有MOOC平台提供人工或虚拟助教来支持大量在线学生的学习。在本文中，我们提出了一个虚拟的MOOC助教LittleMu，仅使用少量标注训练数据，提供问题回答和闲聊服务。LittleMu由两个交互模块组成，包括异构检索和语言模型提示，首先整合结构化、半结构化和非结构化的知识源，支持广泛范围的问题的准确回答。然后，我们设计了名为“Chain of Teach”提示的精心示范，利用大规模预训练模型处理复杂的未收集问题。除了问题回答，我们还开发了其他教育服务，如知识相关的闲聊。我们通过机器人测试系统的性能。

    Teaching assistants have played essential roles in the long history of education. However, few MOOC platforms are providing human or virtual teaching assistants to support learning for massive online students due to the complexity of real-world online education scenarios and the lack of training data. In this paper, we present a virtual MOOC teaching assistant, LittleMu with minimum labeled training data, to provide question answering and chit-chat services. Consisting of two interactive modules of heterogeneous retrieval and language model prompting, LittleMu first integrates structural, semi- and unstructured knowledge sources to support accurate answers for a wide range of questions. Then, we design delicate demonstrations named "Chain of Teach" prompts to exploit the large-scale pre-trained model to handle complex uncollected questions. Except for question answering, we develop other educational services such as knowledge-grounded chit-chat. We test the system's performance via bot
    
[^22]: PIPPA:一个部分合成的对话数据集

    PIPPA: A Partially Synthetic Conversational Dataset. (arXiv:2308.05884v1 [cs.CL])

    [http://arxiv.org/abs/2308.05884](http://arxiv.org/abs/2308.05884)

    PIPPA是一个部分合成的数据集，为研究和开发对话AI系统提供了丰富的资源，以应对现有数据集无法捕捉真实世界角色扮演交互的局限性。

    

    随着越来越强大的大型语言模型的出现，人们越来越有兴趣利用这些模型进行非正式对话和角色扮演应用。然而，现有的对话和角色扮演数据集往往无法捕捉到真实世界角色扮演参与者所展示的多样和微妙的交互。为了解决这个限制并为这个快速发展的领域做出贡献，我们引入了一个部分合成的数据集PIPPA（人与AI之间的个人交互对）。PIPPA是一个由一群角色扮演爱好者参与的社区驱动的众包努力的结果。该数据集包含了分布在26,000个对话会话中的超过100万个话语，为研究人员和AI开发人员在角色扮演场景中探索和完善对话AI系统提供了丰富的资源。

    With the emergence of increasingly powerful large language models, there is a burgeoning interest in leveraging these models for casual conversation and role-play applications. However, existing conversational and role-playing datasets often fail to capture the diverse and nuanced interactions typically exhibited by real-world role-play participants. To address this limitation and contribute to the rapidly growing field, we introduce a partially-synthetic dataset named PIPPA (Personal Interaction Pairs between People and AI). PIPPA is a result of a community-driven crowdsourcing effort involving a group of role-play enthusiasts. The dataset comprises over 1 million utterances that are distributed across 26,000 conversation sessions and provides a rich resource for researchers and AI developers to explore and refine conversational AI systems in the context of role-play scenarios.
    
[^23]: LLM变成DBA

    LLM As DBA. (arXiv:2308.05481v1 [cs.DB])

    [http://arxiv.org/abs/2308.05481](http://arxiv.org/abs/2308.05481)

    LLM变成DBA，提供数据库维护的诊断和优化建议，通过从文本来源中获取经验和多个LLMs的协作诊断。

    

    数据库管理员（DBA）在管理、维护和优化数据库系统以确保数据可用性、性能和可靠性方面起着至关重要的作用。然而，对于DBA来说，管理大量数据库实例（例如，云数据库上的数百万个实例）是困难和繁琐的。最近，大型语言模型（LLMs）已经显示出了理解有价值文件并生成合理答案的巨大潜力。因此，我们提出了D-Bot，一种基于LLM的数据库管理员，它可以持续从文本来源中获取数据库维护经验，并为目标数据库提供合理、有理、及时的诊断和优化建议。本文介绍了一个革命性的以LLM为中心的数据库维护框架，包括（i）从文档和工具中检测数据库维护知识，（ii）根本原因分析的思维树，和（iii）多个LLM之间的协作诊断。我们进行了初步实验。

    Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experiment
    
[^24]: 探索机器学习和基于Transformer的方法用于欺诈性文本分类：一项比较分析

    Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis. (arXiv:2308.05476v1 [cs.CL])

    [http://arxiv.org/abs/2308.05476](http://arxiv.org/abs/2308.05476)

    本研究通过比较分析机器学习和基于Transformer的方法在欺诈性文本分类中的效果，揭示了它们的优势和局限性。

    

    欺诈性文本分类是自然语言处理中的一项关键任务，旨在识别欺诈或欺骗性内容。本研究对机器学习和基于Transformer的方法进行了比较分析，用于欺诈性文本分类。我们研究了传统机器学习算法和最先进的Transformer模型（如BERT，XLNET，DistilBERT和RoBERTa）在检测欺诈性文本方面的有效性。我们使用一个带标签的数据集，其中包含欺诈性和非欺诈性文本，用于训练和评估目的。通过广泛的实验，我们比较了不同方法的性能指标，包括准确率，精确率，召回率和F1得分。本研究的结果揭示了机器学习和基于Transformer的方法在欺诈性文本分类中的优势和局限性，使研究人员和实践者能够在处理欺诈内容时做出明智的决策。

    Deceptive text classification is a critical task in natural language processing that aims to identify deceptive or fraudulent content. This study presents a comparative analysis of machine learning and transformer-based approaches for deceptive text classification. We investigate the effectiveness of traditional machine learning algorithms and state-of-the-art transformer models, such as BERT, XLNET, DistilBERT, and RoBERTa, in detecting deceptive text. A labeled dataset consisting of deceptive and non-deceptive texts is used for training and evaluation purposes. Through extensive experimentation, we compare the performance metrics, including accuracy, precision, recall, and F1 score, of the different approaches. The results of this study shed light on the strengths and limitations of machine learning and transformer-based methods for deceptive text classification, enabling researchers and practitioners to make informed decisions when dealing with deceptive content
    
[^25]: 评估大型中文语言模型的生成能力

    Evaluating the Generation Capabilities of Large Chinese Language Models. (arXiv:2308.04823v1 [cs.CL])

    [http://arxiv.org/abs/2308.04823](http://arxiv.org/abs/2308.04823)

    本文首次对大型中文语言模型在多个学科领域的生成能力进行了全面评估，并提出了Gscore作为衡量生成结果质量的综合指数。

    

    本文介绍了CG-Eval，这是第一个对大型中文语言模型在多个学科领域生成能力进行全面评估的研究。通过在科学工程、人文社科、数学计算、医师资格考试、司法考试和注册会计师考试六个学科中生成准确和相关的回答，评估了这些模型的性能。本文还提出了Gscore，这是一个由多个度量指标加权求和得到的综合指数，用于衡量模型生成结果与参考答案的质量。测试数据和测试结果可在此http URL找到。

    This paper presents CG-Eval, the first comprehensive evaluation of the generation capabilities of large Chinese language models across a wide range of academic disciplines. The models' performance was assessed based on their ability to generate accurate and relevant responses to different types of questions in six disciplines, namely, Science and Engineering, Humanities and Social Sciences, Mathematical Calculations, Medical Practitioner Qualification Examination, Judicial Examination, and Certified Public Accountant Examination. This paper also presents Gscore, a composite index derived from the weighted sum of multiple metrics to measure the quality of model's generation against a reference. The test data and test results can be found at this http URL
    
[^26]: CLASSLA-Stanza: 南斯拉夫语言的语言处理的下一步

    CLASSLA-Stanza: The Next Step for Linguistic Processing of South Slavic Languages. (arXiv:2308.04255v1 [cs.CL])

    [http://arxiv.org/abs/2308.04255](http://arxiv.org/abs/2308.04255)

    CLASSLA-Stanza是一个为南斯拉夫语言提供自动语言注释的流水线，相对于Stanza，在性能和功能上有多个改进，并取得了始终如一的高性能。

    

    我们介绍了CLASSLA-Stanza，一个用于南斯拉夫语言的自动语言注释流水线，该流水线基于Stanza自然语言处理流水线。我们描述了CLASSLA-Stanza相对于Stanza的主要改进，并详细描述了最新2.1版本流水线的模型训练过程。我们还报告了流水线对不同语言和变种的性能评分。CLASSLA-Stanza在所有支持的语言上表现出始终如一的高性能，并在所有支持的任务上优于或扩展了其父流水线Stanza。我们还介绍了流水线的新功能，使其能够高效处理网络数据，并解释了导致其实现的原因。

    We present CLASSLA-Stanza, a pipeline for automatic linguistic annotation of the South Slavic languages, which is based on the Stanza natural language processing pipeline. We describe the main improvements in CLASSLA-Stanza with respect to Stanza, and give a detailed description of the model training process for the latest 2.1 release of the pipeline. We also report performance scores produced by the pipeline for different languages and varieties. CLASSLA-Stanza exhibits consistently high performance across all the supported languages and outperforms or expands its parent pipeline Stanza at all the supported tasks. We also present the pipeline's new functionality enabling efficient processing of web data and the reasons that led to its implementation.
    
[^27]: 3D-EX：一种统一的定义和词典示例数据集

    3D-EX : A Unified Dataset of Definitions and Dictionary Examples. (arXiv:2308.03043v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03043](http://arxiv.org/abs/2308.03043)

    本文介绍了一个名为3D-EX的数据集，它是一个统一的定义和词典示例的知识存储库，可以用于改进词向量或增强语言模型的上下文表示，并且在下游的自然语言处理任务中表现良好。

    

    定义是词典学、语言学和计算语义学中的基本构建块。在自然语言处理中，它们被用于改进词向量或增强语言模型中的上下文表示。然而，包含定义的词典资源具有各种特性，这对于在其上训练和评估的模型行为有影响。本文介绍了3D-EX，一个旨在将众所周知的英文资源组合成一个集中的知识存储库的数据集，以<术语，定义，示例>三元组的形式呈现。3D-EX是一个统一的评估框架，具有经过精心预先计算的训练/验证/测试划分，以防止记忆。实验结果表明，该数据集在下游的自然语言处理任务中具有有效的应用潜力。代码和数据可在https://github.com/F-Almeman/3D-EX 上获取。

    Definitions are a fundamental building block in lexicography, linguistics and computational semantics. In NLP, they have been used for retrofitting word embeddings or augmenting contextual representations in language models. However, lexical resources containing definitions exhibit a wide range of properties, which has implications in the behaviour of models trained and evaluated on them. In this paper, we introduce 3D- EX , a dataset that aims to fill this gap by combining well-known English resources into one centralized knowledge repository in the form of <term, definition, example> triples. 3D- EX is a unified evaluation framework with carefully pre-computed train/validation/test splits to prevent memorization. We report experimental results that suggest that this dataset could be effectively leveraged in downstream NLP tasks. Code and data are available at https://github.com/F-Almeman/3D-EX .
    
[^28]: 为放射学构建通用基础模型的探索

    Towards Generalist Foundation Model for Radiology. (arXiv:2308.02463v1 [cs.CV])

    [http://arxiv.org/abs/2308.02463](http://arxiv.org/abs/2308.02463)

    本研究旨在为放射学构建通用基础模型，提出了一个大规模的医学多模态数据集和支持不同放射学任务的架构，同时提出了一个新的评估基准。

    

    本研究旨在启动放射学基础模型的开发，称为RadFM。我们从数据、模型设计和评估的角度全面考虑了基础模型的构建。我们的贡献可总结如下：（i）构建了一个大规模的医学多模态数据集MedMD，包括1600万个2D和3D医学扫描。据我们所知，这是第一个包含3D医学扫描的多模态数据集。（ii）我们提出了一种架构，使得可视条件生成预训练成为可能，可以将文本输入与2D或3D医学扫描交错，生成不同放射学任务的响应。该模型首先在MedMD上进行了预训练，然后在RadMD上进行了特定领域的微调，RadMD是MedMD的放射学清理版本，包含300万个放射学的视觉语言对。（iii）我们提出了一个新的评估基准，包括五个任务，旨在全面评估该模型的能力。

    In this study, we aim to initiate the development of Radiology Foundation Model, termed as RadFM.We consider the construction of foundational models from the perspectives of data, model design, and evaluation thoroughly. Our contribution can be concluded as follows: (i), we construct a large-scale Medical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans. To the best of our knowledge, this is the first multi-modal dataset containing 3D medical scans. (ii), We propose an architecture that enables visually conditioned generative pre-training, allowing for the integration of text input interleaved with 2D or 3D medical scans to generate response for diverse radiologic tasks. The model was initially pre-trained on MedMD and subsequently domain-specific fine-tuned on RadMD, a radiologic cleaned version of MedMD, containing 3M radiologic visual-language pairs. (iii), we propose a new evaluation benchmark that comprises five tasks, aiming to comprehensively assess the capa
    
[^29]: 跨平台仇恨言论检测中的因果引导解缠问题

    Causality Guided Disentanglement for Cross-Platform Hate Speech Detection. (arXiv:2308.02080v1 [cs.CL])

    [http://arxiv.org/abs/2308.02080](http://arxiv.org/abs/2308.02080)

    本研究提出了一种跨平台仇恨言论检测模型，通过解缠输入表示为不变特征和平台相关特征，实现了对多个未见平台的良好泛化能力。

    

    尽管社交媒体平台在促进公开对话方面具有价值，但他们经常被利用来传播有害内容。目前用于检测这种有害内容的深度学习和自然语言处理模型过度依赖于领域特定术语，影响到了它们适应泛化仇恨言论检测的能力。这是因为它们倾向于过于狭隘地关注特定的语言信号或某些词语类别的使用。当平台缺乏高质量的标记数据用于训练时，另一个重要的挑战出现了，需要跨平台模型来适应不同的分布转化。我们的研究引入了一个跨平台仇恨言论检测模型，能够在一个平台的数据上训练并推广到多个未见平台。为了实现对不同平台的良好泛化性能，一种方法是将输入表示解缠为不变特征和平台相关特征。我们还认为学习因果关系是提供更好解缠和泛化性能的关键。

    Social media platforms, despite their value in promoting open discourse, are often exploited to spread harmful content. Current deep learning and natural language processing models used for detecting this harmful content overly rely on domain-specific terms affecting their capabilities to adapt to generalizable hate speech detection. This is because they tend to focus too narrowly on particular linguistic signals or the use of certain categories of words. Another significant challenge arises when platforms lack high-quality annotated data for training, leading to a need for cross-platform models that can adapt to different distribution shifts. Our research introduces a cross-platform hate speech detection model capable of being trained on one platform's data and generalizing to multiple unseen platforms. To achieve good generalizability across platforms, one way is to disentangle the input representations into invariant and platform-dependent features. We also argue that learning causa
    
[^30]: 使用Fine-Tuned的OpenAI LLM预测机器翻译输出中的完美质量段落：是否可以从历史数据中捕捉编辑距离模式？

    Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v1 [cs.CL])

    [http://arxiv.org/abs/2308.00158](http://arxiv.org/abs/2308.00158)

    本研究探讨了使用Fine-Tuned的OpenAI LLM进行翻译质量估计的能力，实验证明可以通过Fine-Tuned的ChatGPT来预测机器翻译的质量，但仍有改进的空间。

    

    翻译质量估计（TQE）是将输出翻译部署到使用中之前的重要步骤。 TQE对于评估机器翻译（MT）和人工翻译（HT）的质量也是至关重要的，而不需要查看参考翻译。在这项工作中，我们检查了最先进的大型语言模型（LLMs）是否可以为TQE任务和它们的能力进行Fine-Tune。我们以ChatGPT为例，将TQE视为二元分类任务。使用英意和英德训练语料库，我们的实验结果显示，通过ChatGPT的API Fine-Tuned可以在预测翻译质量方面获得相对较高的得分，即是否需要编辑翻译，但肯定有改进准确性的空间。英意双语摘要可在论文中找到。

    Translation Quality Estimation (TQE) is an important step before deploying the output translation into usage. TQE is also critical in assessing machine translation (MT) and human translation (HT) quality without seeing the reference translations. In this work, we examine if the state-of-the-art large language models (LLMs) can be fine-tuned for the TQE task and their capability. We take ChatGPT as one example and approach TQE as a binary classification task. Using English-Italian and English-German training corpus, our experimental results show that fine-tuned ChatGPT via its API can achieve a relatively high score on predicting translation quality, i.e. if the translation needs to be edited, but there is definitely space to improve the accuracy. English-Italiano bilingual Abstract is available in the paper.
    
[^31]: 面向教育中人工智能协作混合论文的自动边界检测

    Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.12267](http://arxiv.org/abs/2307.12267)

    本研究探索了在教育领域中，由人类和生成性语言模型协作编写的混合文本的AI内容检测方法，将其形式化为识别转换点的任务，以区分人类编写和AI生成的部分。

    

    最近的大型语言模型（如ChatGPT）能够在提供具体指导的情况下生成类似于人类的流畅回答。尽管承认技术进步带来的便利，教育者也担心学生可能利用语言模型来完成写作任务并将其假冒为自己的原创作品。虽然有很多AI内容检测研究是基于这些担忧进行的，但大多数之前的研究将AI内容检测建模为一个分类问题，假设一个文本要么完全由人类编写，要么完全由AI生成。在本研究中，我们研究了AI内容检测在一个少有探索但却现实的情况下，即检测的文本由人类和生成性语言模型（即混合文本）协作编写。我们首先将检测任务形式化为从给定的混合文本中识别人类编写内容和AI生成内容之间的转换点（边界检测）。

    The recent large language models (LLMs), e.g., ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions. While admitting the convenience brought by technological advancement, educators also have concerns that students might leverage LLMs to complete their writing assignments and pass them off as their original work. Although many AI content detection studies have been conducted as a result of such concerns, most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated. In this study, we investigated AI content detection in a rarely explored yet realistic setting where the text to be detected is collaboratively written by human and generative LLMs (i.e., hybrid text). We first formalized the detection task as identifying the transition points between human-written content and AI-generated content from a given hybrid text (boundary det
    
[^32]: 使用适配器高效域自适应句子嵌入

    Efficient Domain Adaptation of Sentence Embeddings using Adapters. (arXiv:2307.03104v1 [cs.CL])

    [http://arxiv.org/abs/2307.03104](http://arxiv.org/abs/2307.03104)

    本论文提出了一种通过训练轻量级适配器来高效域自适应句子嵌入的方法，避免了微调整个句子嵌入模型的资源消耗。通过训练特定领域的适配器，可以在不同领域中使用同一模型获得良好的性能。

    

    句子嵌入使我们能够捕捉短文本的语义相似性。大多数句子嵌入模型是针对一般语义文本相似性（STS）任务进行训练的。因此，要在特定领域中使用句子嵌入，必须将模型适应于该领域以获得良好的结果。通常，这是通过对感兴趣的域对整个句子嵌入模型进行微调来实现的。虽然这种方法能够产生最先进的结果，但在微调过程中更新了所有模型的权重，使该方法在资源上要求较高。因此，我们提出了训练轻量级适配器的方法，而不是单独为每个目标领域微调整个句子嵌入模型。这些特定领域的适配器不需要微调所有底层句子嵌入模型的参数。相反，我们只训练少量的额外参数，同时保持底层句子嵌入模型的权重不变。训练特定领域的适配器可以始终使用同一模型并在不同领域中获得良好的性能。

    Sentence embeddings enable us to capture the semantic similarity of short texts. Most sentence embedding models are trained for general semantic textual similarity (STS) tasks. Therefore, to use sentence embeddings in a particular domain, the model must be adapted to it in order to achieve good results. Usually, this is done by fine-tuning the entire sentence embedding model for the domain of interest. While this approach yields state-of-the-art results, all of the model's weights are updated during fine-tuning, making this method resource-intensive. Therefore, instead of fine-tuning entire sentence embedding models for each target domain individually, we propose to train lightweight adapters. These domain-specific adapters do not require fine-tuning all underlying sentence embedding model parameters. Instead, we only train a small number of additional parameters while keeping the weights of the underlying sentence embedding model fixed. Training domain-specific adapters allows always 
    
[^33]: 从文本中丰富本体知识：一种用于概念发现和放置的生物医学数据集

    Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement. (arXiv:2306.14704v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.14704](http://arxiv.org/abs/2306.14704)

    本研究提出了一个新的数据集，通过在PubMed摘要中使用SNOMED CT版本，解决了先前数据集所存在的问题，可以用于自动化地发现和放置新概念到知识库中。

    

    新概念的提及经常出现在文本中，需要自动化的方法将其收集并放置到知识库（例如本体和分类系统）中。现有的数据集存在三个问题：（一）大部分假设新概念已经被发现，不能支持概念发现；（二）只使用概念标签作为输入，缺乏概念标签的上下文信息；（三）主要关注与原子概念的分类，而不是复杂概念（包含逻辑运算符）的放置。为了解决这些问题，我们提出了一个新的基准，使用2014年和2017年的SNOMED CT版本适配MedMentions数据集（PubMed摘要），涵盖疾病子类别和更广泛的临床发现、过程以及制药/生物产品类别。我们用该数据集评估了概念发现和放置的工作，并对其进行了使用方式的说明。

    Mentions of new concepts appear regularly in texts and require automated approaches to harvest and place them into Knowledge Bases (KB), e.g., ontologies and taxonomies. Existing datasets suffer from three issues, (i) mostly assuming that a new concept is pre-discovered and cannot support out-of-KB mention discovery; (ii) only using the concept label as the input along with the KB and thus lacking the contexts of a concept label; and (iii) mostly focusing on concept placement w.r.t a taxonomy of atomic concepts, instead of complex concepts, i.e., with logical operators. To address these issues, we propose a new benchmark, adapting MedMentions dataset (PubMed abstracts) with SNOMED CT versions in 2014 and 2017 under the Diseases sub-category and the broader categories of Clinical finding, Procedure, and Pharmaceutical / biologic product. We provide usage on the evaluation with the dataset for out-of-KB mention discovery and concept placement, adapting recent Large Language Model based m
    
[^34]: 训练好的Transformer在上下文中学习线性模型

    Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])

    [http://arxiv.org/abs/2306.09927](http://arxiv.org/abs/2306.09927)

    本文研究了Transformer在具有单层线性自注意层的线性回归任务上通过梯度流进行训练的ICL机制，揭示了梯度流具有找到目标函数全局最小值的能力。

    

    基于注意力的神经网络，例如Transformers，在上下文学习（ICL）方面表现出了非凡的能力：给定一个来自未见过的任务的短语序列的提示，它们可以制定相关的每个令牌和下一个令牌的预测，而不需要任何参数更新。通过将标记的训练数据和未标记的测试数据序列嵌入到提示中，这使得Transformer表现得像有监督学习算法。事实上，最近的工作表明，在随机实例上训练Transformer体系结构的线性回归问题时，这些模型的预测会模仿普通最小二乘法的预测。

    Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares.  Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this 
    
[^35]: ML-SUPERB: 多语种语音自我监督学习性能基准

    ML-SUPERB: Multilingual Speech Universal PERformance Benchmark. (arXiv:2305.10615v1 [cs.SD])

    [http://arxiv.org/abs/2305.10615](http://arxiv.org/abs/2305.10615)

    本文提出了一个覆盖143种语言、用于自我监督学习模型性能基准的多语种语音基准 ML-SUPERB，并发现自我监督学习模型可以显著提高性能且多语种模型不总是比单语言模型表现更好。

    

    语音处理Universal PERformance Benchmark (SUPERB)是一个用于各种语音处理任务的自我监督学习模型性能基准的排行榜。然而，SUPERB在评估中主要考虑英语。本文介绍了多语种SUPERB (ML-SUPERB)，覆盖了143种语言（从高资源到濒危语言），考虑了自动语音识别和语言识别。与SUPERB概念类似，ML-SUPERB利用冻结的自我监督学习特征，并通过学习浅层下游模型的简单框架，用于多语种任务。与SUPERB基准类似，我们发现语音自我监督学习模型可以显著提高性能，与FBANK特征相比。此外，我们发现多语种模型并不总是比单语言模型表现更好。我们将发布ML-SUPERB作为一个挑战，提供组织好的数据集和可重现的训练脚本，用于未来的多语种表示研究。

    Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.
    
[^36]: 使用丰富的元数据注释的屏幕角色的个性化语言建模

    Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v1 [cs.CL])

    [http://arxiv.org/abs/2303.16618](http://arxiv.org/abs/2303.16618)

    本篇论文研究了如何使用丰富的元数据注释的信息进行屏幕角色的个性化语言建模，测试表明这样可以有效地进行个性化语言模型的构建，即使对于零样本的演说家也可以应用。

    

    语言模型的个性化为对话敏感，能更好地捕捉特定特征的人员和/或特定环境中的说话模式。然而，丰富的角色注释难以得到和成功利用。在此工作中，我们发布并描述了一组新颖的手动注释，涵盖了来自流行的 Cornell 电影对话语料库的 863 名演讲者，包括特征引用和角色描述，以及超过 95％ 的特色电影的一组自动提取的六个元数据。我们对两个语料库进行了广泛的实验，并表明可以有效地使用此类注释来个性化语言模型，将困惑减少高达 8.5％。我们的方法甚至可以应用于零样本的演讲者，即对于没有先前培训数据的演讲者，依赖于角色的人口特征的组合。由于收集此类元数据成本高昂，因此我们还贡献了一项成本效益分析，以突出显示

    Personalisation of language models for dialogue sensitises them to better capture the speaking patterns of people of specific characteristics, and/or in specific environments. However, rich character annotations are difficult to come by and to successfully leverage. In this work, we release and describe a novel set of manual annotations for 863 speakers from the popular Cornell Movie Dialog Corpus, including features like characteristic quotes and character descriptions, and a set of six automatically extracted metadata for over 95% of the featured films. We perform extensive experiments on two corpora and show that such annotations can be effectively used to personalise language models, reducing perplexity by up to 8.5%. Our method can be applied even zero-shot for speakers for whom no prior training data is available, by relying on combinations of characters' demographic characteristics. Since collecting such metadata is costly, we also contribute a cost-benefit analysis to highlight
    
[^37]: BODEGA: 针对可信度评估中对抗性样本生成的基准测试

    BODEGA: Benchmark for Adversarial Example Generation in Credibility Assessment. (arXiv:2303.08032v1 [cs.CL])

    [http://arxiv.org/abs/2303.08032](http://arxiv.org/abs/2303.08032)

    BODEGA是一个基准测试，用于模拟真实的内容管理场景，在四个误传检测任务上测试受害模型和攻击方法。测试结果表明，在某些情况下，即使进行微小的文本修改，也可以欺骗最准确的分类器。

    

    文本分类方法被广泛应用于检测不可信内容，如假新闻、社交媒体机器人、宣传等。较为准确的模型（可能基于深度神经网络）有助于管理公共电子平台，并经常导致内容创建者面临提交拒绝或已发布文本的撤下。为了避免进一步被检测，内容创建者尝试产生一个稍微修改过的文本版本（即攻击对抗性样本），利用分类器的弱点导致不同的输出。本文介绍了BODEGA：一个基准测试，用于在模拟内容管理的真实用例中测试受害模型和攻击方法在四个误传检测任务上的表现。我们还系统地测试了受欢迎的文本分类器对可用攻击技术的鲁棒性，并发现在某些情况下，即使在文本中进行微小的修改也可以欺骗最准确的分类器。

    Text classification methods have been widely investigated as a way to detect content of low credibility: fake news, social media bots, propaganda, etc. Quite accurate models (likely based on deep neural networks) help in moderating public electronic platforms and often cause content creators to face rejection of their submissions or removal of already published texts. Having the incentive to evade further detection, content creators try to come up with a slightly modified version of the text (known as an attack with an adversarial example) that exploit the weaknesses of classifiers and result in a different output. Here we introduce BODEGA: a benchmark for testing both victim models and attack methods on four misinformation detection tasks in an evaluation framework designed to simulate real use-cases of content moderation. We also systematically test the robustness of popular text classifiers against available attacking techniques and discover that, indeed, in some cases barely signif
    
[^38]: 跨模态对比学习用于多模态假新闻检测

    Cross-modal Contrastive Learning for Multimodal Fake News Detection. (arXiv:2302.14057v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14057](http://arxiv.org/abs/2302.14057)

    这项研究提出了COOLANT，一个用于跨模态假新闻检测的对比学习框架，旨在提升图像和文本的对齐精度，并通过跨模态融合和注意力机制实现更准确和可解释的特征聚合。

    

    自动检测多模态假新闻近来引起了广泛关注。许多现有方法致力于融合单模特征以产生多模态新闻表达。然而，强大的跨模态对比学习方法在假新闻检测方面的潜力尚未充分利用。此外，如何聚合不同模态的特征以提升决策过程的性能仍然是一个未解决的问题。为了解决这个问题，我们提出了COOLANT，一个用于多模态假新闻检测的跨模态对比学习框架，旨在实现更准确的图像-文本对齐。为了进一步提高对齐精度，我们利用辅助任务在对比过程中软化负样本的损失项。我们开发了一个跨模态融合模块来学习跨模态之间的相关性。我们实现了一个带有注意力引导模块的注意力机制，以帮助有效且可解释地聚合对齐的单模信息。

    Automatic detection of multimodal fake news has gained a widespread attention recently. Many existing approaches seek to fuse unimodal features to produce multimodal news representations. However, the potential of powerful cross-modal contrastive learning methods for fake news detection has not been well exploited. Besides, how to aggregate features from different modalities to boost the performance of the decision-making process is still an open question. To address that, we propose COOLANT, a cross-modal contrastive learning framework for multimodal fake news detection, aiming to achieve more accurate image-text alignment. To further improve the alignment precision, we leverage an auxiliary task to soften the loss term of negative samples during the contrast process. A cross-modal fusion module is developed to learn the cross-modality correlations. An attention mechanism with an attention guidance module is implemented to help effectively and interpretably aggregate the aligned unimo
    
[^39]: 揭示未知：基于实体链接的知识库外提及发现

    Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking. (arXiv:2302.07189v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.07189](http://arxiv.org/abs/2302.07189)

    本文提出了基于BERT的实体链接方法BLINKout，通过与特殊NIL实体匹配来识别没有相应KB实体的实体提及，相较于现有方法具有优势。

    

    从文本中发现知识库（KB）外的实体提及，在KB维护中起着至关重要的作用，但并未被完全开发。当前的方法主要局限于简单的基于阈值的方法和基于特征的分类，并且用于评估的数据集相对较少。本文提出了BLINKout，一种新的基于BERT的实体链接（EL）方法，可以通过将提及与特殊的NIL实体进行匹配来识别没有相应KB实体的提及。为了更好地利用BERT，我们提出了包括NIL实体表示和分类在内的新技术，并增强了其同义词。我们还提出了KB修剪和版本控制策略，以自动从常见的KB EL数据集构建出KB外的数据集。在医学本体论、UMLS、SNOMED CT等五个不同领域中，对临床笔记、生物医学出版物和维基百科文章的结果表明，BLINKout在识别知识库外提及方面优于现有方法。

    Discovering entity mentions that are out of a Knowledge Base (KB) from texts plays a critical role in KB maintenance, but has not yet been fully explored. The current methods are mostly limited to the simple threshold-based approach and feature-based classification, and the datasets for evaluation are relatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL) method which can identify mentions that do not have corresponding KB entities by matching them to a special NIL entity. To better utilize BERT, we propose new techniques including NIL entity representation and classification, with synonym enhancement. We also propose KB Pruning and Versioning strategies to automatically construct out-of-KB datasets from common in-KB EL datasets. Results on five datasets of clinical notes, biomedical publications, and Wikipedia articles in various domains show the advantages of BLINKout over existing methods to identify out-of-KB mentions for the medical ontologies, UMLS, SNOMED CT,
    
[^40]: CodeBert能学到哪些特征：BERT基于源码表示学习的实证研究

    Which Features are Learned by CodeBert: An Empirical Study of the BERT-based Source Code Representation Learning. (arXiv:2301.08427v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.08427](http://arxiv.org/abs/2301.08427)

    该论文研究了CodeBert在源码表示学习中学到了哪些特征，发现当前方法无法有效理解源代码的逻辑，而源代码的表示依赖于程序员定义的变量和函数名。

    

    双向编码器表示转换(BERT)在自然语言处理(NLP)领域提出，并显示出了良好的结果。最近，研究人员将BERT应用于源码表示学习，并在几个下游任务中取得了一些好消息。然而，在本文中，我们说明了当前方法无法有效理解源代码的逻辑。源代码的表示在很大程度上依赖于程序员定义的变量和函数名。我们设计并实施了一系列实验来验证我们的猜想，并为未来的工作提供一些见解。

    The Bidirectional Encoder Representations from Transformers (BERT) were proposed in the natural language process (NLP) and shows promising results. Recently researchers applied the BERT to source-code representation learning and reported some good news on several downstream tasks. However, in this paper, we illustrated that current methods cannot effectively understand the logic of source codes. The representation of source code heavily relies on the programmer-defined variable and function names. We design and implement a set of experiments to demonstrate our conjecture and provide some insights for future works.
    
[^41]: RT-1: 用于实际控制的机器人变压器.

    RT-1: Robotics Transformer for Real-World Control at Scale. (arXiv:2212.06817v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2212.06817](http://arxiv.org/abs/2212.06817)

    本文提出了机器人变压器模型，通过从大规模、多样化、任务无关的数据集中获取知识，并结合高容量架构，实现了在实际控制领域的高性能泛化能力。

    

    通过从大规模、多样化、任务无关的数据集中获取知识，现代机器学习模型可以在零样本学习或使用少量特定任务的数据集来高水平地解决具体的下游任务。虽然这种能力已经在计算机视觉、自然语言处理或语音识别等其他领域得到证明，但在机器人领域尚未展示出来。这是因为模型的泛化能力在机器人领域尤为关键，由于收集现实世界的机器人数据的难度较大。我们认为，这种通用机器人模型成功的一个关键因素在于任务不可知的开放式训练，结合可以吸收所有多样化机器人数据的高容量架构。在本文中，我们提出了一种称为机器人变压器的模型类，具有良好的可扩展模型特性。我们通过研究不同模型类别及其随数据大小而推广的能力来验证我们的结论。

    By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, mod
    
[^42]: Transformers是短文本分类器：基于基准和实际数据集的归纳短文本分类器研究

    Transformers are Short Text Classifiers: A Study of Inductive Short Text Classifiers on Benchmarks and Real-world Datasets. (arXiv:2211.16878v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.16878](http://arxiv.org/abs/2211.16878)

    本研究探讨了短文本分类器的性能，并发现Transformers在短文本分类任务中达到了最先进的准确率，引发了是否需要专门的短文本技术的讨论。

    

    短文本分类是自然语言处理中关键且具有挑战性的任务。因此，存在许多高度专门化的短文本分类器。然而，在最近的短文本研究中，传统文本分类的最先进方法，特别是纯粹使用Transformers的方法，尚未被充分利用。在本研究中，我们检查了多种短文本分类器的性能，以及排名靠前的传统文本分类器。我们进一步研究了两个新的实际短文本数据集对模型的影响，以解决对于仅具有有限特征的基准数据集过度依赖的问题。我们的实验证明，Transformers在短文本分类任务中实现了最先进的准确率，引发了是否需要专门的短文本技术的问题。

    Short text classification is a crucial and challenging aspect of Natural Language Processing. For this reason, there are numerous highly specialized short text classifiers. However, in recent short text research, State of the Art (SOTA) methods for traditional text classification, particularly the pure use of Transformers, have been unexploited. In this work, we examine the performance of a variety of short text classifiers as well as the top performing traditional text classifier. We further investigate the effects on two new real-world short text datasets in an effort to address the issue of becoming overly dependent on benchmark datasets with a limited number of characteristics. Our experiments unambiguously demonstrate that Transformers achieve SOTA accuracy on short text classification tasks, raising the question of whether specialized short text techniques are necessary.
    
[^43]: Kuaipedia:一个大规模的多模式短视频百科全书

    Kuaipedia: a Large-scale Multi-modal Short-video Encyclopedia. (arXiv:2211.00732v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2211.00732](http://arxiv.org/abs/2211.00732)

    Kuaipedia是一个大规模的多模式短视频百科全书，通过知识视频的形式，能够轻松表达网民对某个项目的各个方面的需求。

    

    过去20年中，在线百科全书（如维基百科）得到了很好的发展和研究。人们可以在由志愿者社区编辑的维基页面上找到维基项的任何属性或其他信息。然而，传统的文本、图片和表格很难表达维基项的某些方面。例如，当我们谈论“柴犬”时，人们可能更关心“如何喂养它”或“如何训练它不保护食物”。目前，短视频平台已成为在线世界的标志。无论你使用的是TikTok、Instagram、快手还是YouTube Shorts，短视频应用程序已改变了我们今天的内容消费和创作方式。除了为娱乐制作短视频外，我们越来越多地看到作者们在各行各业广泛分享有见解的知识。这些短视频，我们称之为知识视频，可以轻松表达消费者想了解有关某个项目（例如柴犬）的任何方面（例如毛发或如何喂养）。

    Online encyclopedias, such as Wikipedia, have been well-developed and researched in the last two decades. One can find any attributes or other information of a wiki item on a wiki page edited by a community of volunteers. However, the traditional text, images and tables can hardly express some aspects of an wiki item. For example, when we talk about ``Shiba Inu'', one may care more about ``How to feed it'' or ``How to train it not to protect its food''. Currently, short-video platforms have become a hallmark in the online world. Whether you're on TikTok, Instagram, Kuaishou, or YouTube Shorts, short-video apps have changed how we consume and create content today. Except for producing short videos for entertainment, we can find more and more authors sharing insightful knowledge widely across all walks of life. These short videos, which we call knowledge videos, can easily express any aspects (e.g. hair or how-to-feed) consumers want to know about an item (e.g. Shiba Inu), and they can b
    
[^44]: 有不止一种稳健性：用对抗样本欺骗Whisper模型

    There is more than one kind of robustness: Fooling Whisper with adversarial examples. (arXiv:2210.17316v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2210.17316](http://arxiv.org/abs/2210.17316)

    本研究展示了Whisper模型虽然在分布外输入和随机噪声方面显示出了出色的稳健性，但却容易受到对抗干扰的影响。我们通过生成极小的输入扰动来大幅降低Whisper的性能，并对多语言模型的性能产生了影响。这些发现对于实际的安全问题和对抗性稳健ASR的需求具有重要意义。

    

    Whisper是一种最近的自动语音识别（ASR）模型，对于分布外输入和随机噪声都展示出了令人印象深刻的稳健性。在这项工作中，我们展示了这种稳健性在对抗干扰下并不适用。我们展示了通过生成极小的输入扰动（信噪比为35-45dB），我们可以大幅降低Whisper的性能，甚至转录我们选择的目标句子。我们还展示了通过欺骗Whisper语言检测器，我们可以轻松降低多语言模型的性能。这些对一个广受欢迎的开源模型的脆弱性具有实际的安全影响，并强调了对抗性稳健ASR的需求。

    Whisper is a recent Automatic Speech Recognition (ASR) model displaying impressive robustness to both out-of-distribution inputs and random noise. In this work, we show that this robustness does not carry over to adversarial noise. We show that we can degrade Whisper performance dramatically, or even transcribe a target sentence of our choice, by generating very small input perturbations with Signal Noise Ratio of 35-45dB. We also show that by fooling the Whisper language detector we can very easily degrade the performance of multilingual models. These vulnerabilities of a widely popular open-source model have practical security implications and emphasize the need for adversarially robust ASR.
    
[^45]: 用于口语语言识别的具有局部和全局上下文的紧凑型端到端模型

    A Compact End-to-End Model with Local and Global Context for Spoken Language Identification. (arXiv:2210.15781v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2210.15781](http://arxiv.org/abs/2210.15781)

    TitaNet-LID是一种紧凑且具有局部和全局上下文的端到端神经网络模型，适用于口语语言识别。尽管尺寸较小，但它可以在不降低性能的情况下实现与最先进模型相似的准确性，并且能够适应不同的声学条件和语言。该模型具有良好的扩展性和处理短语输入的能力。

    

    我们介绍了TitaNet-LID，一种基于ContextNet架构的用于口语语言识别（LID）的紧凑型端到端神经网络。TitaNet-LID使用1D深度可分离卷积和Squeeze-and-Excitation层，有效地捕捉了话语中的局部和全局上下文。尽管尺寸较小，TitaNet-LID在VoxLingua107数据集上的性能与最先进的模型相似，同时尺寸更小10倍。此外，通过简单的微调，它可以轻松适应新的声学条件和未见过的语言，在FLEURS基准测试上实现了88.2%的最先进准确率。我们的模型具有可扩展性，可以在准确性和速度之间取得更好的平衡。TitaNet-LID在长度不足5秒的短语中表现良好，表明它对输入长度的鲁棒性。

    We introduce TitaNet-LID, a compact end-to-end neural network for Spoken Language Identification (LID) that is based on the ContextNet architecture. TitaNet-LID employs 1D depth-wise separable convolutions and Squeeze-and-Excitation layers to effectively capture local and global context within an utterance. Despite its small size, TitaNet-LID achieves performance similar to state-of-the-art models on the VoxLingua107 dataset while being 10 times smaller. Furthermore, it can be easily adapted to new acoustic conditions and unseen languages through simple fine-tuning, achieving a state-of-the-art accuracy of 88.2% on the FLEURS benchmark. Our model is scalable and can achieve a better trade-off between accuracy and speed. TitaNet-LID performs well even on short utterances less than 5s in length, indicating its robustness to input length.
    
[^46]: 限制线性链条件随机场到正则语言的方法

    Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.07306](http://arxiv.org/abs/2106.07306)

    本文提出了一种将线性链条件随机场（CRFs）限制到正则语言的方法，该方法可以对广泛的约束进行建模，并允许在训练过程中引入这些约束。

    

    结构预测中的一个重要挑战是表示输出结构中的相互依赖关系。当输出以序列形式结构化时，线性链条件随机场（CRFs）是一种广泛使用的模型类，可以学习输出中的“局部”依赖关系。然而，CRF的马尔可夫假设使得CRFs无法表示具有“非局部”依赖关系的分布，并且标准CRFs无法满足数据的非局部约束（例如输出标签的全局性约束）。我们提出了一种CRFs的推广形式，可以通过将可能的输出结构空间指定为正则语言$\mathcal{L}$来强制执行广泛的约束，包括非局部约束。结果的正则约束CRF（RegCCRF）具有与标准CRF相同的形式属性，但对于不在$\mathcal{L}$中的所有标签序列分配零概率。值得注意的是，RegCCRFs可以在训练过程中引入约束，与相关模型不同。

    A major challenge in structured prediction is to represent the interdependencies within output structures. When outputs are structured as sequences, linear-chain conditional random fields (CRFs) are a widely used model class which can learn \textit{local} dependencies in the output. However, the CRF's Markov assumption makes it impossible for CRFs to represent distributions with \textit{nonlocal} dependencies, and standard CRFs are unable to respect nonlocal constraints of the data (such as global arity constraints on output labels). We present a generalization of CRFs that can enforce a broad class of constraints, including nonlocal ones, by specifying the space of possible output structures as a regular language $\mathcal{L}$. The resulting regular-constrained CRF (RegCCRF) has the same formal properties as a standard CRF, but assigns zero probability to all label sequences not in $\mathcal{L}$. Notably, RegCCRFs can incorporate their constraints during training, while related models
    

