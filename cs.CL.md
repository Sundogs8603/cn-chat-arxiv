# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain Fake News Detecction](https://arxiv.org/abs/2404.01336) | FineFake 数据集为细粒度多领域假新闻检测提供了知识增强，包含16,909个数据样本覆盖六个语义主题和八个平台。 |
| [^2] | [Natural Language, AI, and Quantum Computing in 2024: Research Ingredients and Directions in QNLP](https://arxiv.org/abs/2403.19758) | 该论文调查了2024年自然语言处理、人工智能发展中的量子计算应用，在量子自然语言处理中使用了诸如词嵌入、序列模型、注意力和语法分析等NLP技术，提出了一种新的量子设计来处理文本编码，并探讨了量子理论对“不确定性是什么？”和“智能是什么？”等核心问题的关键贡献。 |
| [^3] | [Grounding Language Plans in Demonstrations Through Counterfactual Perturbations](https://arxiv.org/abs/2403.17124) | 这项工作通过使用LLMs来指导多步演示中隐含的任务结构和约束的搜索，以及通过反事实干扰获得更广泛的演示状态空间覆盖。 |
| [^4] | [Differentially Private Next-Token Prediction of Large Language Models](https://arxiv.org/abs/2403.15638) | 提出了Private Mixing of Ensemble Distributions (PMixED)：通过将模型的输出分布投影到公共LLM的输出分布周围的集合上，并采样平均来实现实际的下一个标记预测，以更轻量化的方式实现对隐私敏感的大型语言模型的预测。 |
| [^5] | [Towards Measuring and Modeling "Culture" in LLMs: A Survey](https://arxiv.org/abs/2403.15412) | 这项研究调查了39篇最新论文，旨在研究大型语言模型中的文化表达和包容性，发现当前研究未对“文化”进行定义，而是在特定设计的数据集上对模型进行探究，研究了某些“文化”的方面，留下许多未被探究的有趣和重要方面，如语义领域和关于性。 |
| [^6] | [CHAI: Clustered Head Attention for Efficient LLM Inference](https://arxiv.org/abs/2403.08058) | CHAI提出了Clustered Head Attention（CHAI）方法，通过在运行时结合具有高相关性的注意力头部，实现了减少内存需求和计算量，能够在不需要微调的情况下将存储K,V缓存的内存需求降低21.4％，推理时间延迟降低1.73倍。 |
| [^7] | [Strong Priority and Determinacy in Timed CCS](https://arxiv.org/abs/2403.04618) | 引入了一种新的调度机制“顺序构造减少”，旨在实现多播并发通信的确定性，扩展了CCS的技术设置，证明了构造减少的汇聚属性，展示了在一些语法限制下运算符的结构连贯性。 |
| [^8] | [ProMoAI: Process Modeling with Generative AI](https://arxiv.org/abs/2403.04327) | ProMoAI利用大型语言模型自动生成过程模型，支持优化并通过用户反馈进行改进，是一种新颖的AI驱动的过程建模工具，降低了用户的技术门槛。 |
| [^9] | [The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning](https://arxiv.org/abs/2403.03218) | WMDP基准是一个公开发布的数据集，包含4157个多项选择问题，用作生物安全、网络安全和化学安全危险知识的代理测量。 |
| [^10] | [Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations](https://arxiv.org/abs/2403.02090) | 提出了三个新的具有挑战性的任务来模拟多人之间的细粒度动态，并为社交推理游戏设置提供了广泛的数据注释；同时提出了一种新颖的多模态基线方法，利用密集对齐的语言-视觉表示。 |
| [^11] | [Brilla AI: AI Contestant for the National Science and Maths Quiz](https://arxiv.org/abs/2403.01699) | 人工智能参赛者Brilla AI在全国科学与数学竞赛中表现优秀，为缺乏合格教师的非洲提供了学习支持。 |
| [^12] | [DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling](https://arxiv.org/abs/2403.01197) | DMoERM首次将专家混合（MoE）的概念引入奖励建模领域，提出了双层MoE RM（DMoERM），通过稀疏的外层MoE和密集的内层MoE对特定任务进行微调，解决了训练中的多任务干扰和数据噪声问题。 |
| [^13] | [Prompting ChatGPT for Translation: A Comparative Analysis of Translation Brief and Persona Prompts](https://arxiv.org/abs/2403.00127) | 讨论在ChatGPT中将翻译简要和翻译者/作者人物角色融入提示设计的有效性，发现虽然有助于促进人类之间的翻译通信，但对于改善ChatGPT翻译质量的效果有限。 |
| [^14] | [Clustering Document Parts: Detecting and Characterizing Influence Campaigns From Documents](https://arxiv.org/abs/2402.17151) | 提出了一种新颖的聚类流程，用于检测和描述影响力活动，通过多种技术增强流程性能，并在文档级别分类上取得了优于传统方法的预测效果 |
| [^15] | [LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step](https://arxiv.org/abs/2402.16906) | LDB是一个新颖的调试框架，可以让大型语言模型通过运行时执行信息来完善生成的程序。 |
| [^16] | [InstructEdit: Instruction-based Knowledge Editing for Large Language Models](https://arxiv.org/abs/2402.16123) | InstructEdit提出了一种基于指令的知识编辑技术，通过简单指令使编辑器适应不同任务的表现，显著提高了多任务编辑中的可靠性。 |
| [^17] | [Stick to your Role! Stability of Personal Values Expressed in Large Language Models](https://arxiv.org/abs/2402.14846) | 本文提出研究在大型语言模型中个人价值在不同背景下的表达稳定性，通过模拟对话的方式进行评估，对19个LLMs进行比较研究。 |
| [^18] | [End-to-end multilingual fact-checking at scale](https://arxiv.org/abs/2402.12147) | 使用Factiverse AI模型，可以进行跨语言的端到端事实核查，并且通过实验证明，为事实核查任务进行微调的模型优于大型语言模型。 |
| [^19] | [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353) | DoRA是一种将预训练权重分解为幅度和方向两个组成部分，并使用LoRA进行方向更新的低秩适应方法，通过增强学习能力和训练稳定性来提高对LLaMA，LLaVA和VL-B的微调性能。 |
| [^20] | [Enhancing Amharic-LLaMA: Integrating Task Specific and Generative Datasets](https://arxiv.org/abs/2402.08015) | 本研究通过整合任务特定和生成数据集来增强Amharic-LLaMA模型，提高了阿姆哈拉语言模型的性能。他们通过创建阿姆哈拉语指令微调数据集和微调模型，在不同的NLP任务中取得了有希望的结果。 |
| [^21] | [Leveraging AI to Advance Science and Computing Education across Africa: Progress, Challenges, and Opportunities](https://arxiv.org/abs/2402.07397) | 这项研究描述了在非洲开发和使用人工智能教育工具的工作，包括SuaCode学习编码应用、AutoGrad自动评分和反馈工具、代码抄袭检测工具以及双语AI教师Kwame。这些工具有助于解决非洲学生在教育中面临的挑战。 |
| [^22] | [Preference-free Alignment Learning with Regularized Relevance Reward](https://arxiv.org/abs/2402.03469) | 无偏好对齐学习使用正则化相关奖励作为关键目标，在提供稳健奖励信号的同时，显著提高了偏好基准测试的性能。 |
| [^23] | [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300) | DeepSeekMath是一种开放语言模型，通过预训练和数据选择，提升了数学推理能力，实现了接近于竞赛级别水平的性能。 |
| [^24] | [Explaining Text Classifiers with Counterfactual Representations](https://arxiv.org/abs/2402.00711) | 本论文提出了一种使用反事实表示解释文本分类器的方法，通过干预文本表示来生成反事实，并通过实验证实了方法的有效性。 |
| [^25] | [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://arxiv.org/abs/2311.14455) | 本文提出了一种新的威胁，攻击者通过毒害训练数据向语言模型中嵌入“越狱后门”，并通过添加特定的触发词使模型产生有害的回应。这种通用越狱后门比之前的研究更强大，且较难被察觉。研究探究了RLHF设计中的决策对其鲁棒性的影响，并发布了一组被毒害模型的基准测试数据，以促进未来对通用越狱后门的研究。 |
| [^26] | [Real-World Deployment and Evaluation of Kwame for Science, An AI Teaching Assistant for Science Education in West Africa](https://arxiv.org/abs/2302.10786) | 这项研究扩展了AI教辅Kwame，面向西非科学教育，并将其作为一个网络应用程序进行了实际部署和评估。结果显示，在实际环境中，Kwame for Science取得了很高的准确率，并且获得了广泛的用户和问题提问量。 |
| [^27] | [(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection.](http://arxiv.org/abs/2401.14040) | 本研究探讨了(Chat)GPT和BERT在语义变化检测任务中的性能，结果表明(Chat)GPT的表现明显低于BERT，尤其在长期变化检测方面表现更差。 |
| [^28] | [Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning.](http://arxiv.org/abs/2401.10862) | 本文研究了剪枝对齐的LLMs的保护措施，发现剪枝LLM参数可以显著增强其抵抗“越狱”提示攻击的能力，并且对其他LLM行为也可能有更普遍的效果。同时，引入了一个有害任务数据集，证明剪枝有助于集中注意力在与任务相关的标记上。突出的聊天模型表现出很高的易感性。 |
| [^29] | [Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender.](http://arxiv.org/abs/2401.06561) | 本研究提出了一种名为Intention Analysis Prompting (IAPrompt)的方法，通过触发大型语言模型（LLMs）的自我纠正和改进能力来防御越狱攻击。实验证明，该方法能够显著减少响应中的有害行为并保持整体有用性。 |
| [^30] | [L3Cube-IndicNews: News-based Short Text and Long Document Classification Datasets in Indic Languages.](http://arxiv.org/abs/2401.02254) | L3Cube-IndicNews是一个面向印度语系的多语种文本分类数据集，包括短标题、长文档和长段落三个数据集。它提供了10种印度语言的新闻文章，每个数据集包含10个或更多类别的文章。这个数据集可以用于深入分析和评估。 |
| [^31] | [Reconstructing Materials Tetrahedron: Challenges in Materials Information Extraction.](http://arxiv.org/abs/2310.08383) | 本论文讨论了从材料科学文献中自动提取信息面临的挑战，并希望能够创建一个大型的材料科学知识库。 |
| [^32] | [Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors.](http://arxiv.org/abs/2310.02980) | 本文研究表明使用随机初始化会导致对架构差异的严重高估，而使用标准消噪目标进行预训练可以在多种架构上实现显著的性能提升，并将Transformers与状态空间模型之间的差距缩小到很小。与之前的研究不同的是，我们发现当正确预训练时，普通的Transformers在Long Range Arena上的性能与S4相匹配，并且在PathX-256任务上改进了SSMs的最佳结果20个百分点。 |
| [^33] | [SSHR: Leveraging Self-supervised Hierarchical Representations for Multilingual Automatic Speech Recognition.](http://arxiv.org/abs/2309.16937) | 本研究提出了一种利用自监督层次表示（SSHR）优化多语种自动语音识别的方法。通过分析自监督学习模型的不同层次表示，提取出语言相关帧和特定内容，并引导模型在最终层次获取更多内容相关信息。 |
| [^34] | [An Interactive Framework for Profiling News Media Sources.](http://arxiv.org/abs/2309.07384) | 本文提出了一个交互式框架用于对新闻媒体来源进行特征分析，通过结合图形分析模型、预训练语言模型和人类洞察力，可以快速检测虚假和有偏见的新闻媒体。 |
| [^35] | [Matching Patients to Clinical Trials with Large Language Models.](http://arxiv.org/abs/2307.15051) | 本研究调查了使用大型语言模型（LLMs）来帮助患者和转诊医生识别合适的临床试验的潜力，并引入了TrialGPT架构，该架构能够准确预测合格性并提供解释，实验证明其有效性。 |
| [^36] | [A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks.](http://arxiv.org/abs/2307.12114) | 这项研究评估了四种指导细调大型语言模型在临床和生物医学任务上的表现，并发现它们在零样本和少样本情况下接近最先进模型的性能，尤其在问答任务上表现良好。然而，在分类和关系抽取任务上的表现稍逊于特定训练于医学领域的模型。没有一个模型在所有研究任务上胜过其他模型，有些模型更适合特定任务。 |
| [^37] | [Large language models shape and are shaped by society: A survey of arXiv publication patterns.](http://arxiv.org/abs/2307.10700) | 大型语言模型的论文数量急剧增加，研究重点逐渐转向社会影响。与LLM相关的论文呈现持续增长的趋势，新发表关于LLM的作者更注重应用和社会影响。 |
| [^38] | [MMBench: Is Your Multi-modal Model an All-around Player?.](http://arxiv.org/abs/2307.06281) | MMBench是一个新型的多模态基准测试，旨在解决大型视觉语言模型评估的挑战，通过开发全面的评估流程和精心策划的数据集进行细粒度能力评估。 |

# 详细

[^1]: FineFake：一个用于细粒度多领域假新闻检测的知识增强数据集

    FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain Fake News Detecction

    [https://arxiv.org/abs/2404.01336](https://arxiv.org/abs/2404.01336)

    FineFake 数据集为细粒度多领域假新闻检测提供了知识增强，包含16,909个数据样本覆盖六个语义主题和八个平台。

    

    现有的假新闻检测基准数据集在评估新闻内容的真实性方面取得了显著进展。然而，这些基准数据集通常仅关注单一语义主题的新闻或来自单一平台的新闻，因此无法捕捉真实场景中多领域新闻的多样性。为了了解不同领域的假新闻，外部知识和细粒度注释至关重要，以提供精确证据并揭示制造假新闻的多样潜在策略，而这也是现有基准数据集所忽略的。为了填补这一空白，我们引入了一个名为FineFake的新型多领域知识增强基准数据集，具有细粒度注释。FineFake涵盖了来自六个语义主题和八个平台的16,909个数据样本。每个新闻项目都包含多模态内容、潜在社交背景、半自动

    arXiv:2404.01336v1 Announce Type: cross  Abstract: Existing benchmarks for fake news detection have significantly contributed to the advancement of models in assessing the authenticity of news content. However, these benchmarks typically focus solely on news pertaining to a single semantic topic or originating from a single platform, thereby failing to capture the diversity of multi-domain news in real scenarios. In order to understand fake news across various domains, the external knowledge and fine-grained annotations are indispensable to provide precise evidence and uncover the diverse underlying strategies for fabrication, which are also ignored by existing benchmarks. To address this gap, we introduce a novel multi-domain knowledge-enhanced benchmark with fine-grained annotations, named \textbf{FineFake}. FineFake encompasses 16,909 data samples spanning six semantic topics and eight platforms. Each news item is enriched with multi-modal content, potential social context, semi-man
    
[^2]: 2024年的自然语言、人工智能和量子计算：QNLP中的研究要点和方向

    Natural Language, AI, and Quantum Computing in 2024: Research Ingredients and Directions in QNLP

    [https://arxiv.org/abs/2403.19758](https://arxiv.org/abs/2403.19758)

    该论文调查了2024年自然语言处理、人工智能发展中的量子计算应用，在量子自然语言处理中使用了诸如词嵌入、序列模型、注意力和语法分析等NLP技术，提出了一种新的量子设计来处理文本编码，并探讨了量子理论对“不确定性是什么？”和“智能是什么？”等核心问题的关键贡献。

    

    arXiv:2403.19758v1 公告类型：跨领域 抽象：语言处理是当前人工智能发展的关键，同时量子计算也开始应用。这引起了量子自然语言处理的极大兴趣，出现了几个早期提案和实验。本文调查了这一领域的最新进展，展示了NLP相关技术，包括词嵌入、序列模型、注意力和语法分析是如何应用于量子语言处理中的。我们提出了一种新的量子设计用于文本编码的基本任务（在内存中表示一个字符串），这在以前没有详细讨论过。除了推动新技术，量子理论还对“不确定性是什么？”和“智能是什么？”等具有挑战性的问题做出了关键贡献。随着这些问题在人工系统中变得愈发紧迫，本文还考虑了一些事实概念化的方式。

    arXiv:2403.19758v1 Announce Type: cross  Abstract: Language processing is at the heart of current developments in artificial intelligence, and quantum computers are becoming available at the same time. This has led to great interest in quantum natural language processing, and several early proposals and experiments. This paper surveys the state of this area, showing how NLP-related techniques including word embeddings, sequential models, attention, and grammatical parsing have been used in quantum language processing. We introduce a new quantum design for the basic task of text encoding (representing a string of characters in memory), which has not been addressed in detail before.   As well as motivating new technologies, quantum theory has made key contributions to the challenging questions of 'What is uncertainty?' and 'What is intelligence?' As these questions are taking on fresh urgency with artificial systems, the paper also considers some of the ways facts are conceptualized and 
    
[^3]: 将语言计划基于演示通过反事实干扰进行落实

    Grounding Language Plans in Demonstrations Through Counterfactual Perturbations

    [https://arxiv.org/abs/2403.17124](https://arxiv.org/abs/2403.17124)

    这项工作通过使用LLMs来指导多步演示中隐含的任务结构和约束的搜索，以及通过反事实干扰获得更广泛的演示状态空间覆盖。

    

    将大型语言模型的常识推理基于物理领域落实在体现智能的人工智能中仍然是一个至关重要但尚未解决的问题。相较于先前的工作专注于直接利用LLMs在符号空间内规划，这项工作使用LLMs指导任务结构的搜索，隐含在多步演示中的约束。具体而言，我们借鉴了操纵规划文献中的模式族的概念，它按照特定运动约束将机器人配置分组，作为LLM高级语言表示和机器人低级物理轨迹之间的抽象层。通过用合成干扰重新播放少量人类演示，我们可以覆盖演示的状态空间，并额外生成成功执行以及未完成任务的反事实情况。我们的基于解释的学习框架训练了一个端到端可微分神经网络。

    arXiv:2403.17124v1 Announce Type: cross  Abstract: Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural networ
    
[^4]: 大型语言模型的差分私有下一个标记预测

    Differentially Private Next-Token Prediction of Large Language Models

    [https://arxiv.org/abs/2403.15638](https://arxiv.org/abs/2403.15638)

    提出了Private Mixing of Ensemble Distributions (PMixED)：通过将模型的输出分布投影到公共LLM的输出分布周围的集合上，并采样平均来实现实际的下一个标记预测，以更轻量化的方式实现对隐私敏感的大型语言模型的预测。

    

    确保大型语言模型（LLMs）的隐私日益重要。DP-SGD是实现这一目标的最广泛采用的技术，它以一种保证差分隐私的方式训练模型。然而，DP-SGD需要比SGD更长的训练时间和更大的内存需求，同时过高估计对手具有白盒访问模型的能力。更现实的场景假设只有对隐私敏感的LLM进行黑盒访问。在这些观察的基础上，我们提出了私有混合集合分布（PMixED）：一种通过将模型的每个输出分布从一个经过精细调整的LLM集合投影到公共LLM输出分布周围的集合上，然后对投影分布进行平均并从中抽样来实现实际的下一个标记预测的私有预测协议。我们的方法比DP-SGD更轻量化，因为它与模型无关。

    arXiv:2403.15638v1 Announce Type: cross  Abstract: Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly important. The most widely adopted technique to accomplish this is DP-SGD, which trains a model in such a way that guarantees Differential Privacy (DP). However, DP-SGD requires longer training times and larger memory requirements than SGD, while overestimating an adversary's capabilities in having white box access to the model. A more realistic scenario assumes only black-box access to a privacy-sensitive LLM. Motivated by these observations, we present Private Mixing of Ensemble Distributions (PMixED): a private prediction protocol that achieves practical next-token prediction by projecting each of the model's output distribution from an ensemble of fine-tuned LLMs onto a set around a public LLM's output distribution, then averaging the projected distributions and sampling from it. Our approach is more lightweight than DP-SGD in that it is model agnostic, i
    
[^5]: 在LLMs中测量和建模“文化”：一项调查

    Towards Measuring and Modeling "Culture" in LLMs: A Survey

    [https://arxiv.org/abs/2403.15412](https://arxiv.org/abs/2403.15412)

    这项研究调查了39篇最新论文，旨在研究大型语言模型中的文化表达和包容性，发现当前研究未对“文化”进行定义，而是在特定设计的数据集上对模型进行探究，研究了某些“文化”的方面，留下许多未被探究的有趣和重要方面，如语义领域和关于性。

    

    我们呈现了对39篇最新论文的调查，旨在研究大型语言模型中的文化表达和包容性。我们观察到，没有一篇研究定义“文化”，这是一个复杂、多层面的概念；相反，它们在一些特别设计的数据集上对模型进行探究，这些数据集代表了某些“文化”的方面。我们将这些方面称为文化的代理，并将它们组织在人口统计、语义和语言文化交互代理的三个维度上。我们还对采用的探查方法进行了分类。我们的分析表明，只有“文化”的某些方面，如价值观和目标，被研究了，留下了几个其他有趣且重要的方面，特别是大量语义领域和关于性（Hershcovich等人，2022）的未被探究。另外两个关键的空白是目前方法的鲁棒性和情境性的缺乏。基于这些观察结果，

    arXiv:2403.15412v1 Announce Type: cross  Abstract: We present a survey of 39 recent papers that aim to study cultural representation and inclusion in large language models. We observe that none of the studies define "culture," which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of "culture." We call these aspects the proxies of cultures, and organize them across three dimensions of demographic, semantic and linguistic-cultural interaction proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of "culture," such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situatedness of the current methods. Based on these observations
    
[^6]: CHAI：高效LLM推理的聚类头部注意力

    CHAI: Clustered Head Attention for Efficient LLM Inference

    [https://arxiv.org/abs/2403.08058](https://arxiv.org/abs/2403.08058)

    CHAI提出了Clustered Head Attention（CHAI）方法，通过在运行时结合具有高相关性的注意力头部，实现了减少内存需求和计算量，能够在不需要微调的情况下将存储K,V缓存的内存需求降低21.4％，推理时间延迟降低1.73倍。

    

    大语言模型(LLMs)拥有数百亿参数改变了机器学习领域。然而，在推理时为这些模型提供服务既需要计算又需要内存，一个请求可能需要多个GPU和数十GB的内存。多头注意力是LLMs的关键组件之一，可以占LLMs内存和计算需求的50%以上。我们观察到在各头之间对注意力的关注有很高的冗余性。基于这一观察，我们提出了Clustered Head Attention (CHAI)。CHAI在运行时将具有高相关性的头部结合进行自注意力，从而减少内存和计算。在我们的实验中，我们展示了CHAI能够将存储K,V缓存的内存需求降低多达21.4%，推理时延迟降低多达1.73倍，而无需任何微调。CHAI实现了最多3.2%的偏差。

    arXiv:2403.08058v1 Announce Type: cross  Abstract: Large Language Models (LLMs) with hundreds of billions of parameters have transformed the field of machine learning. However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is one of the key components of LLMs, which can account for over 50% of LLMs memory and compute requirement. We observe that there is a high amount of redundancy across heads on which tokens they pay attention to. Based on this insight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a high amount of correlation for self-attention at runtime, thus reducing both memory and compute. In our experiments, we show that CHAI is able to reduce the memory requirements for storing K,V cache by up to 21.4% and inference time latency by up to 1.73x without any fine-tuning required. CHAI achieves this with a maximum 3.2% deviat
    
[^7]: 时标CCS中的强优先级和确定性

    Strong Priority and Determinacy in Timed CCS

    [https://arxiv.org/abs/2403.04618](https://arxiv.org/abs/2403.04618)

    引入了一种新的调度机制“顺序构造减少”，旨在实现多播并发通信的确定性，扩展了CCS的技术设置，证明了构造减少的汇聚属性，展示了在一些语法限制下运算符的结构连贯性。

    

    在具有优先级的经典进程代数理论的基础上，我们确定了一种名为“顺序构造减少”的新调度机制，旨在捕捉同步编程的本质。这种评估策略的独特属性是通过构造实现多播并发通信的确定性。特别是，这使我们能够模拟具有对缺失反应的共享内存多线程，因为它是Esterel编程语言的核心。在通过时钟和优先级扩展的CCS的技术设置中，对于我们称为“结构连贯”的大类过程，我们证明了构造减少的汇聚属性。我们进一步展示，在一些称为“可枢纽”的语法限制下，前缀、求和、并行组成、限制和隐藏的运算符保持结构连贯。这涵盖了一个严格更大的过程类。

    arXiv:2403.04618v1 Announce Type: cross  Abstract: Building on the classical theory of process algebra with priorities, we identify a new scheduling mechanism, called "sequentially constructive reduction" which is designed to capture the essence of synchronous programming. The distinctive property of this evaluation strategy is to achieve determinism-by-construction for multi-cast concurrent communication. In particular, it permits us to model shared memory multi-threading with reaction to absence as it lies at the core of the programming language Esterel. In the technical setting of CCS extended by clocks and priorities, we prove for a large class of processes, which we call "structurally coherent" the confluence property for constructive reductions. We further show that under some syntactic restrictions, called "pivotable" the operators of prefix, summation, parallel composition, restriction and hiding preserve structural coherence. This covers a strictly larger class of processes co
    
[^8]: ProMoAI：使用生成式AI进行过程建模

    ProMoAI: Process Modeling with Generative AI

    [https://arxiv.org/abs/2403.04327](https://arxiv.org/abs/2403.04327)

    ProMoAI利用大型语言模型自动生成过程模型，支持优化并通过用户反馈进行改进，是一种新颖的AI驱动的过程建模工具，降低了用户的技术门槛。

    

    ProMoAI是一种新颖的工具，利用大型语言模型（LLMs）从文本描述中自动生成过程模型，融合了先进的提示工程、错误处理和代码生成技术。除了自动化生成复杂的过程模型外，ProMoAI还支持过程模型优化。用户可以通过提供对生成模型的反馈与工具进行交互，然后用于改进过程模型。ProMoAI利用LLMs的能力提供了一种新颖的、以人工智能驱动的过程建模方法，显著降低了对没有深入技术知识的用户的准入门槛。

    arXiv:2403.04327v1 Announce Type: cross  Abstract: ProMoAI is a novel tool that leverages Large Language Models (LLMs) to automatically generate process models from textual descriptions, incorporating advanced prompt engineering, error handling, and code generation techniques. Beyond automating the generation of complex process models, ProMoAI also supports process model optimization. Users can interact with the tool by providing feedback on the generated model, which is then used for refining the process model. ProMoAI utilizes the capabilities LLMs to offer a novel, AI-driven approach to process modeling, significantly reducing the barrier to entry for users without deep technical knowledge in process modeling.
    
[^9]: WMDP基准：通过遗忘测量和减少恶意使用

    The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning

    [https://arxiv.org/abs/2403.03218](https://arxiv.org/abs/2403.03218)

    WMDP基准是一个公开发布的数据集，包含4157个多项选择问题，用作生物安全、网络安全和化学安全危险知识的代理测量。

    

    arXiv:2403.03218v1 公告类型：交叉领域 摘要：白宫关于人工智能的行政命令强调了大型语言模型(LLMs)赋予恶意行为者开发生物、网络和化学武器的风险。为了衡量这些恶意使用的风险，政府机构和主要人工智能实验室正在开发LLMs的危险能力评估。然而，当前的评估是私人的，阻碍了进一步研究如何减少风险。此外，它们仅专注于几条高度特定的恶意使用途径。为了填补这些空白，我们公开发布了大规模杀伤性武器代理（WMDP）基准，这是一个包含4157个多项选择问题的数据集，作为生物安全、网络安全和化学安全危险知识的代理测量。WMDP由一组学术界和技术顾问联合开发，并在公开发布前严格过滤以消除敏感信息。WMDP有两个服务

    arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
    
[^10]: 建模多模态社交互动：具有密集对齐表示的新挑战和基线

    Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations

    [https://arxiv.org/abs/2403.02090](https://arxiv.org/abs/2403.02090)

    提出了三个新的具有挑战性的任务来模拟多人之间的细粒度动态，并为社交推理游戏设置提供了广泛的数据注释；同时提出了一种新颖的多模态基线方法，利用密集对齐的语言-视觉表示。

    

    理解涉及言语和非言语线索的社交互动对有效解释社交情境至关重要。然而，大多数关于多模态社交线索的先前工作主要集中在单人行为上，或依赖于与多方环境中的话语密切对齐的整体视觉表示。它们在建模多方互动的复杂动态方面存在局限。在本文中，我们介绍了三个新的具有挑战性的任务，以建模多人之间的细粒度动态：话语目标识别、代词指代消解和提及玩家预测。我们为社交推理游戏设置中的这些新挑战提供了广泛的数据注释。此外，我们提出了一种新颖的多模态基线，通过将视觉特征与其对应的话语同步，利用密集对齐的语言-视觉表示，这有助于

    arXiv:2403.02090v1 Announce Type: cross  Abstract: Understanding social interactions involving both verbal and non-verbal cues is essential to effectively interpret social situations. However, most prior works on multimodal social cues focus predominantly on single-person behaviors or rely on holistic visual representations that are not densely aligned to utterances in multi-party environments. They are limited in modeling the intricate dynamics of multi-party interactions. In this paper, we introduce three new challenging tasks to model the fine-grained dynamics between multiple people: speaking target identification, pronoun coreference resolution, and mentioned player prediction. We contribute extensive data annotations to curate these new challenges in social deduction game settings. Furthermore, we propose a novel multimodal baseline that leverages densely aligned language-visual representations by synchronizing visual features with their corresponding utterances. This facilitates
    
[^11]: Brilla AI: 全国科学与数学竞赛的人工智能参赛者

    Brilla AI: AI Contestant for the National Science and Maths Quiz

    [https://arxiv.org/abs/2403.01699](https://arxiv.org/abs/2403.01699)

    人工智能参赛者Brilla AI在全国科学与数学竞赛中表现优秀，为缺乏合格教师的非洲提供了学习支持。

    

    非洲大陆缺乏足够的合格教师，这阻碍了提供足够的学习支持。人工智能有可能增强有限数量教师的努力，从而带来更好的学习成果。本文描述并评估了NSMQ AI Grand Challenge的首要成果，该挑战提出了一个强大的现实基准，用于评估此类人工智能：“建立一个人工智能，参加加纳的全国科学与数学竞赛（NSMQ），并获胜——在比赛的所有轮次和阶段中表现优于最优秀的参赛者”。NSMQ是加纳的高中学生每年举行的现场科学与数学竞赛，3队2名学生通过回答生物学、化学、物理和数学问题在5轮比赛中竞争，逐渐晋级至最终冠军的队伍。在本研究中，我们建立了Brilla AI，一个参加NSMQ竞赛的人工智能选手。

    arXiv:2403.01699v1 Announce Type: cross  Abstract: The African continent lacks enough qualified teachers which hampers the provision of adequate learning support. An AI could potentially augment the efforts of the limited number of teachers, leading to better learning outcomes. Towards that end, this work describes and evaluates the first key output for the NSMQ AI Grand Challenge, which proposes a robust, real-world benchmark for such an AI: "Build an AI to compete live in Ghana's National Science and Maths Quiz (NSMQ) competition and win - performing better than the best contestants in all rounds and stages of the competition". The NSMQ is an annual live science and mathematics competition for senior secondary school students in Ghana in which 3 teams of 2 students compete by answering questions across biology, chemistry, physics, and math in 5 rounds over 5 progressive stages until a winning team is crowned for that year. In this work, we built Brilla AI, an AI contestant that we de
    
[^12]: DMoERM: 有效奖励建模的混合专家配方

    DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling

    [https://arxiv.org/abs/2403.01197](https://arxiv.org/abs/2403.01197)

    DMoERM首次将专家混合（MoE）的概念引入奖励建模领域，提出了双层MoE RM（DMoERM），通过稀疏的外层MoE和密集的内层MoE对特定任务进行微调，解决了训练中的多任务干扰和数据噪声问题。

    

    奖励模型（RM）的性能是改善大型语言模型（LLM）在对齐微调过程中效果的关键因素。 RM训练仍然存在两个挑战：1）使用各种类别数据训练相同的RM可能导致其泛化性能受到多任务干扰的影响；2）人类注释的一致性率通常仅为60%至75%，导致训练数据包含大量噪声。 为了解决这两个挑战，我们首次将专家混合（MoE）的概念引入到RM领域。 我们提出了双层MoE RM（DMoERM）。 外层MoE是一种稀疏模型。 将输入分类成任务类别后，我们将其路由到相应的内层任务特定模型中。 内层MoE是一种密集模型。 我们将特定任务分解为多个能力维度，并分别在每个维度上对LoRA专家进行微调。

    arXiv:2403.01197v1 Announce Type: new  Abstract: The performance of the reward model (RM) is a critical factor in improving the effectiveness of the large language model (LLM) during alignment fine-tuning. There remain two challenges in RM training: 1) training the same RM using various categories of data may cause its generalization performance to suffer from multi-task disturbance, and 2) the human annotation consistency rate is generally only $60\%$ to $75\%$, causing training data to contain a lot of noise. To tackle these two challenges, we introduced the idea of Mixture-of-Experts (MoE) into the field of RM for the first time. We propose the Double-Layer MoE RM (DMoERM). The outer layer MoE is a sparse model. After classifying an input into task categories, we route it to the corresponding inner layer task-specific model. The inner layer MoE is a dense model. We decompose the specific task into multiple capability dimensions and individually fine-tune a LoRA expert on each one. T
    
[^13]: 使用提示ChatGPT进行翻译：翻译简要和人物角色提示的比较分析

    Prompting ChatGPT for Translation: A Comparative Analysis of Translation Brief and Persona Prompts

    [https://arxiv.org/abs/2403.00127](https://arxiv.org/abs/2403.00127)

    讨论在ChatGPT中将翻译简要和翻译者/作者人物角色融入提示设计的有效性，发现虽然有助于促进人类之间的翻译通信，但对于改善ChatGPT翻译质量的效果有限。

    

    LLMs中的提示工程已显示出改善翻译质量的潜力。然而，在提示设计中融入翻译概念的潜力仍未得到充分探讨。本文讨论了在ChatGPT中为翻译任务设计提示时将翻译简要概念和翻译者及作者的人物角色结合起来的有效性。研究结果表明，尽管某些元素有助于促进人与人之间的翻译通信，但它们在提高ChatGPT翻译质量方面的效果有限。这凸显了需要更多探索性研究，研究翻译理论家和实践者如何开发目前根植于人与人交流范式的概念工具集，以应用于涉及人机交互的新兴工作流中的翻译目的。

    arXiv:2403.00127v1 Announce Type: new  Abstract: Prompt engineering in LLMs has shown potential for improving translation quality. However, the potential of incorporating translation concepts in prompt design remains largely underexplored. Against this backdrop, this paper discusses the effectiveness of incorporating the conceptual tool of translation brief and the personas of translator and author into prompt design for translation tasks in ChatGPT. Findings suggest that, although certain elements are constructive in facilitating human to human communication for translation tasks, their effectiveness is limited for improving translation quality in ChatGPT. This accentuates the need for more explorative research on how translation theorists and practitioners can develop the current set of conceptual tools rooted in the human to human communication paradigm for translation purposes in this emerging workflow involving human machine interaction.
    
[^14]: 对文档部分进行聚类：从文档中检测和描述影响力活动

    Clustering Document Parts: Detecting and Characterizing Influence Campaigns From Documents

    [https://arxiv.org/abs/2402.17151](https://arxiv.org/abs/2402.17151)

    提出了一种新颖的聚类流程，用于检测和描述影响力活动，通过多种技术增强流程性能，并在文档级别分类上取得了优于传统方法的预测效果

    

    我们提出了一种新颖的聚类流程，用于从文档中检测和描述影响力活动。该方法对文档的部分进行聚类，检测可能反映影响力活动的聚类，然后通过它们与高影响力聚类的关联来识别与影响力活动相关的文档。我们的方法在预测文档是否属于影响力活动方面表现优于直接文档级别分类和直接文档级别聚类方法。我们提出了各种新颖技术来增强我们的流程，包括使用现有的事件事实预测系统获取文档部分，并聚合多个聚类实验以提高聚类和文档分类的性能。在聚类的基础上对文档进行分类不仅可以准确提取与影响力活动相关的文档部分，还可以捕捉到影响活动

    arXiv:2402.17151v1 Announce Type: new  Abstract: We propose a novel clustering pipeline to detect and characterize influence campaigns from documents. This approach clusters parts of document, detects clusters that likely reflect an influence campaign, and then identifies documents linked to an influence campaign via their association with the high-influence clusters. Our approach outperforms both the direct document-level classification and the direct document-level clustering approach in predicting if a document is part of an influence campaign. We propose various novel techniques to enhance our pipeline, including using an existing event factuality prediction system to obtain document parts, and aggregating multiple clustering experiments to improve the performance of both cluster and document classification. Classifying documents on the top of clustering not only accurately extracts the parts of the documents that are relevant to influence campaigns, but also capture influence camp
    
[^15]: LDB：通过逐步验证运行时执行来调试大型语言模型

    LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step

    [https://arxiv.org/abs/2402.16906](https://arxiv.org/abs/2402.16906)

    LDB是一个新颖的调试框架，可以让大型语言模型通过运行时执行信息来完善生成的程序。

    

    大型语言模型（LLMs）在代码生成方面取得了重大进展。最近的研究不仅将单次代码生成，而且还将单元测试和程序验证器整合到LLMs中，以迭代地完善生成的程序。然而，这些工作将生成的程序视为不可分割的实体，这对LLMs在调试程序时存在不足，特别是当程序包含复杂的逻辑流程和数据操作时。相比之下，当人类开发人员调试程序时，他们通常设置断点并有选择地检查运行时执行信息。执行流和中间变量在调试过程中发挥着关键作用，然而现有的代码生成文献中未充分利用它们。本研究引入了大型语言模型调试器（LDB），这是一个新颖的调试框架，可以让LLMs通过运行时执行信息完善其生成的程序。

    arXiv:2402.16906v1 Announce Type: cross  Abstract: Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifical
    
[^16]: InstructEdit：针对大型语言模型的基于指令的知识编辑

    InstructEdit: Instruction-based Knowledge Editing for Large Language Models

    [https://arxiv.org/abs/2402.16123](https://arxiv.org/abs/2402.16123)

    InstructEdit提出了一种基于指令的知识编辑技术，通过简单指令使编辑器适应不同任务的表现，显著提高了多任务编辑中的可靠性。

    

    大型语言模型的知识编辑可以提供一种有效的解决方案，以改变模型的行为而不会对整体性能产生消极影响。然而，当前的方法在跨任务的通用性方面存在问题，需要为每个任务设计一个独特的编辑器，这显著阻碍了更广泛的应用。为了解决这一问题，我们首先分析了知识编辑中的多任务泛化问题。具体地，我们开发了一种基于指令的编辑技术，称为InstructEdit，通过简单的指令促进编辑器同时适应各种任务的表现。通过为每个LLM只使用一个统一的编辑器，我们在实证方面表明，InstructEdit可以提高编辑器的控制能力，从而在多任务编辑设置中平均提高可靠性14.86%。此外，涉及保留未见任务的实验说明，InstructEdi

    arXiv:2402.16123v1 Announce Type: cross  Abstract: Knowledge editing for large language models can offer an efficient solution to alter a model's behavior without negatively impacting the overall performance. However, the current approach encounters issues with limited generalizability across tasks, necessitating one distinct editor for each task, which significantly hinders the broader applications. To address this, we take the first step to analyze the multi-task generalization issue in knowledge editing. Specifically, we develop an instruction-based editing technique, termed InstructEdit, which facilitates the editor's adaptation to various task performances simultaneously using simple instructions. With only one unified editor for each LLM, we empirically demonstrate that InstructEdit can improve the editor's control, leading to an average 14.86% increase in Reliability in multi-task editing setting. Furthermore, experiments involving holdout unseen task illustrate that InstructEdi
    
[^17]: 坚持你的角色！个人价值在大型语言模型中的稳定性

    Stick to your Role! Stability of Personal Values Expressed in Large Language Models

    [https://arxiv.org/abs/2402.14846](https://arxiv.org/abs/2402.14846)

    本文提出研究在大型语言模型中个人价值在不同背景下的表达稳定性，通过模拟对话的方式进行评估，对19个LLMs进行比较研究。

    

    通过基准测试或心理问卷的标准方式研究大型语言模型(LLMs)是提供许多来源于类似最小背景的不同查询（例如多项选择问题）。然而，由于LLM高度依赖于背景，因此从这种最小背景评估中得出的结论可能对模型在部署中的行为（在那里它将暴露于许多新背景）的说明很少。我们认为，依赖于背景的特性应该作为LLM比较的另一个维度来研究，而不是其他维度，如认知能力、知识或模型大小。在本文中，我们提出了一个关于在不同背景下（模拟对不同话题的对话）价值表达稳定性的案例研究，并使用标准心理学问卷（PVQ）和行为下游任务进行测量。我们考虑了来自五个家族的19个开源LLM。借鉴心理学方法，我们研究了等级稳定性。

    arXiv:2402.14846v1 Announce Type: cross  Abstract: The standard way to study Large Language Models (LLMs) through benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLM's highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model's behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence should be studied as another dimension of LLM comparison alongside others such as cognitive abilities, knowledge, or model size. In this paper, we present a case-study about the stability of value expression over different contexts (simulated conversations on different topics), and as measured using a standard psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19 open-sourced LLMs from five families. Reusing methods from psychology, we study Rank-order stabilit
    
[^18]: 跨语言规模的端到端事实核查

    End-to-end multilingual fact-checking at scale

    [https://arxiv.org/abs/2402.12147](https://arxiv.org/abs/2402.12147)

    使用Factiverse AI模型，可以进行跨语言的端到端事实核查，并且通过实验证明，为事实核查任务进行微调的模型优于大型语言模型。

    

    在本文中，我们描述了如何使用Factiverse AI模型在100多种语言中进行端到端事实核查。我们还通过实验性基准测试展示，为事实核查任务进行微调的模型胜过GPT-4、GPT-3.5-Turbo和Mistral-7b等大型语言模型。

    arXiv:2402.12147v1 Announce Type: cross  Abstract: In this article, we describe how you can perform end-to-end fact-checking in over 100 languages using Factiverse AI models. We also show through an experimental benchmark that fine-tuned models tailored for fact-checking tasks outperform Large Language Models such as GPT-4, GPT-3.5-Turbo, and Mistral-7b.
    
[^19]: DoRA: 分解权重的低秩适应

    DoRA: Weight-Decomposed Low-Rank Adaptation

    [https://arxiv.org/abs/2402.09353](https://arxiv.org/abs/2402.09353)

    DoRA是一种将预训练权重分解为幅度和方向两个组成部分，并使用LoRA进行方向更新的低秩适应方法，通过增强学习能力和训练稳定性来提高对LLaMA，LLaVA和VL-B的微调性能。

    

    在广泛使用的参数高效调整（PEFT）方法中，由于避免了额外的推理成本，LoRA及其变种方法因此变得非常流行。然而，这些方法与完全微调（FT）之间仍然存在精度差距。在这项工作中，我们首先引入了一种新颖的权重分解分析方法来研究FT和LoRA之间的内在差异。为了模拟FT的学习能力，我们提出了一种称为DoRA的权重分解低秩适应方法。DoRA将预训练的权重分解为两个组成部分，幅度和方向，并且具体使用LoRA进行方向更新，以有效地减少可训练参数的数量。通过使用DoRA，我们增强了LoRA的学习能力和训练稳定性，同时避免了任何额外的推理开销。在微调LLaMA，LLaVA和VL-B上，DoRA始终优于LoRA。

    arXiv:2402.09353v1 Announce Type: new Abstract: Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-B
    
[^20]: 增强Amharic-LLaMA: 整合特定任务与生成数据集

    Enhancing Amharic-LLaMA: Integrating Task Specific and Generative Datasets

    [https://arxiv.org/abs/2402.08015](https://arxiv.org/abs/2402.08015)

    本研究通过整合任务特定和生成数据集来增强Amharic-LLaMA模型，提高了阿姆哈拉语言模型的性能。他们通过创建阿姆哈拉语指令微调数据集和微调模型，在不同的NLP任务中取得了有希望的结果。

    

    大型语言模型（LLM）因其在理解和生成人类语言方面的出色表现而在自然语言处理（NLP）研究中受到了很多关注。然而，资源匮乏的语言因缺乏资源而被落下。在这项工作中，我们致力于通过整合特定任务和生成数据集来增强LLaMA-2-Amharic模型，以提高阿姆哈拉语的语言模型性能。我们创建了一个阿姆哈拉语指令微调数据集，并对LLaMA-2-Amharic模型进行了微调。经过微调的模型在不同的NLP任务中表现出有希望的结果。我们开源了我们的数据集创建流程、指令数据集、训练模型和评估输出，以促进对这些模型的语言特定研究。

    Large language models (LLMs) have received a lot of attention in natural language processing (NLP) research because of their exceptional performance in understanding and generating human languages. However, low-resource languages are left behind due to the unavailability of resources. In this work, we focus on enhancing the LLaMA-2-Amharic model by integrating task-specific and generative datasets to improve language model performance for Amharic. We compile an Amharic instruction fine-tuning dataset and fine-tuned LLaMA-2-Amharic model. The fine-tuned model shows promising results in different NLP tasks. We open-source our dataset creation pipeline, instruction datasets, trained models, and evaluation outputs to promote language-specific studies on these models.
    
[^21]: 利用人工智能推进非洲科学和计算教育：进展、挑战和机遇

    Leveraging AI to Advance Science and Computing Education across Africa: Progress, Challenges, and Opportunities

    [https://arxiv.org/abs/2402.07397](https://arxiv.org/abs/2402.07397)

    这项研究描述了在非洲开发和使用人工智能教育工具的工作，包括SuaCode学习编码应用、AutoGrad自动评分和反馈工具、代码抄袭检测工具以及双语AI教师Kwame。这些工具有助于解决非洲学生在教育中面临的挑战。

    

    在非洲大陆，学生们面临着各种教育挑战，包括获取计算机、网络连接、可靠电力和合格教师等基本资源的限制。尽管存在这些挑战，但最近人工智能（如BERT和GPT-4）的进展已经展示了其促进教育的潜力。然而，这些人工智能工具往往在西方教育环境中进行部署和评估，对非洲学生面临的独特需求和挑战的关注有限。在本章中，我们描述了我们在非洲开发和部署人工智能教育工具的工作：（1）SuaCode，一款AI动力的应用程序，使非洲人可以使用智能手机学习编程，（2）AutoGrad，用于图形和交互式编程作业的自动评分和反馈工具，（3）一种代码抄袭检测工具，展示了抄袭的可视证据，（4）Kwame，一款双语的AI教师。

    Across the African continent, students grapple with various educational challenges, including limited access to essential resources such as computers, internet connectivity, reliable electricity, and a shortage of qualified teachers. Despite these challenges, recent advances in AI such as BERT, and GPT-4 have demonstrated their potential for advancing education. Yet, these AI tools tend to be deployed and evaluated predominantly within the context of Western educational settings, with limited attention directed towards the unique needs and challenges faced by students in Africa. In this book chapter, we describe our works developing and deploying AI in Education tools in Africa: (1) SuaCode, an AI-powered app that enables Africans to learn to code using their smartphones, (2) AutoGrad, an automated grading, and feedback tool for graphical and interactive coding assignments, (3) a tool for code plagiarism detection that shows visual evidence of plagiarism, (4) Kwame, a bilingual AI teac
    
[^22]: 无偏好对齐学习与正则化相关奖励

    Preference-free Alignment Learning with Regularized Relevance Reward

    [https://arxiv.org/abs/2402.03469](https://arxiv.org/abs/2402.03469)

    无偏好对齐学习使用正则化相关奖励作为关键目标，在提供稳健奖励信号的同时，显著提高了偏好基准测试的性能。

    

    从人类偏好中学习被认为是将大规模语言模型（LLMs）与人类价值观对齐的关键。然而，与普遍的观点相反，我们的初步研究发现，基于人类偏好数据集训练的奖励模型倾向于给长的与主题无关的回复更高的分数，而给短的与主题相关的回复较低分。在这一观察的驱动下，我们探索了一种无偏好的方法，利用“相关性”作为对齐的一个关键目标。在我们的第一次尝试中，我们发现仅仅通过检索得到的相关性得分容易受到奖励欺骗的影响，即过度优化到不期望的捷径上，当我们将该得分作为奖励用于强化学习。为了缓解这个问题，我们将有效的归纳偏差整合到常规的相关性中，互相正则化，形成了一种混合奖励函数：正则化相关奖励（$R^3$）。$R^3$通过提供稳健的奖励信号，显著提高了在偏好基准测试中的性能。值得注意的是，$R^3$不需要

    Learning from human preference has been considered key to aligning Large Language Models (LLMs) with human values. However, contrary to popular belief, our preliminary study reveals that reward models trained on human preference datasets tend to give higher scores to long off-topic responses than short on-topic ones. Motivated by this observation, we explore a preference-free approach utilizing `relevance' as a key objective for alignment. On our first attempt, we find that the relevance score obtained by a retriever alone is vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, when we utilize the score as a reward for reinforcement learning. To mitigate it, we integrate effective inductive biases into the vanilla relevance to regularize each other, resulting in a mixture of reward functions: Regularized Relevance Reward ($R^3$). $R^3$ significantly improves performance on preference benchmarks by providing a robust reward signal. Notably, $R^3$ does not require a
    
[^23]: DeepSeekMath: 将开放语言模型中的数学推理能力推向极限

    DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

    [https://arxiv.org/abs/2402.03300](https://arxiv.org/abs/2402.03300)

    DeepSeekMath是一种开放语言模型，通过预训练和数据选择，提升了数学推理能力，实现了接近于竞赛级别水平的性能。

    

    数学推理由于其复杂和结构化的特性，对语言模型提出了重大挑战。本文介绍了DeepSeekMath 7B，它在Common Crawl中获取了120B个与数学相关的标记，并结合了自然语言和代码数据来继续预训练DeepSeek-Coder-Base-v1.5 7B。DeepSeekMath 7B在竞赛级别的数学基准测试中取得了令人印象深刻的51.7%的分数，无需依赖外部工具包和投票技术，接近了Gemini-Ultra和GPT-4的性能水平。DeepSeekMath 7B的自一致性在MATH上的64个样本中达到了60.9%的分数。DeepSeekMath的数学推理能力归因于两个关键因素：首先，我们通过精心设计的数据选择管道充分利用了公开可用的网络数据的巨大潜力。其次，我们引入了群体相对策略优化（GRPO），这是近端策略优化（PPO）的一个变体，可以增强数学推理能力。

    Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities w
    
[^24]: 使用反事实表示解释文本分类器

    Explaining Text Classifiers with Counterfactual Representations

    [https://arxiv.org/abs/2402.00711](https://arxiv.org/abs/2402.00711)

    本论文提出了一种使用反事实表示解释文本分类器的方法，通过干预文本表示来生成反事实，并通过实验证实了方法的有效性。

    

    一种基于反事实的解释方法可以为分类器提供合理的解释，其中反事实是指除了一个分类特征之外，与真实观察完全相同的假设事件。然而，在文本领域构建这种反事实存在特定挑战，因为某些属性值可能与现实世界的事件不一致。在这篇论文中，我们提出了一种简单的方法，通过对文本表示进行干预来生成反事实，从而克服了这个限制。我们认为我们的干预方法是最小程度的干扰，并且在理论上是可靠的，因为它们与Pearl的因果推断框架中定义的反事实是一致的。为了验证我们的方法，我们首先在合成数据集上进行实验，比较了基于真实反事实（通过明确的文本干预获得）和我们的反事实（通过对文本表示的干预得到）的分类器预测。

    One well motivated explanation method for classifiers leverages counterfactuals which are hypothetical events identical to real observations in all aspects except for one categorical feature. Constructing such counterfactual poses specific challenges for texts, however, as some attribute values may not necessarily align with plausible real-world events. In this paper we propose a simple method for generating counterfactuals by intervening in the space of text representations which bypasses this limitation. We argue that our interventions are minimally disruptive and that they are theoretically sound as they align with counterfactuals as defined in Pearl's causal inference framework. To validate our method, we first conduct experiments on a synthetic dataset of counterfactuals, allowing for a direct comparison between classifier predictions based on ground truth counterfactuals (obtained through explicit text interventions) and our counterfactuals, derived through interventions in the r
    
[^25]: 从被毒害的人类反馈中构建的通用越狱后门

    Universal Jailbreak Backdoors from Poisoned Human Feedback

    [https://arxiv.org/abs/2311.14455](https://arxiv.org/abs/2311.14455)

    本文提出了一种新的威胁，攻击者通过毒害训练数据向语言模型中嵌入“越狱后门”，并通过添加特定的触发词使模型产生有害的回应。这种通用越狱后门比之前的研究更强大，且较难被察觉。研究探究了RLHF设计中的决策对其鲁棒性的影响，并发布了一组被毒害模型的基准测试数据，以促进未来对通用越狱后门的研究。

    

    强化学习从人类反馈中（RLHF）用于对齐大型语言模型，以产生有用且无害的回应。然而，先前的研究显示，这些模型可以通过找到使模型恢复到未对齐行为的对抗提示来进行越狱。在本文中，我们考虑了一个新的威胁，攻击者通过毒害RLHF训练数据将“越狱后门”嵌入模型中。该后门将一个触发词嵌入模型中，类似于通用的“sudo命令”：在任何提示中添加触发词将使模型产生有害的回应，无需搜索对抗提示。通用越狱后门比先前研究的语言模型后门更强大，我们发现使用常见的后门攻击技术要困难得多。我们探究了RLHF中的设计决策对其所声称的鲁棒性的贡献，并发布一组被毒害模型的基准，以促进对通用越狱后门的未来研究。

    Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a "jailbreak backdoor" into the model. The backdoor embeds a trigger word into the model that acts like a universal "sudo command": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak bac
    
[^26]: 面向西非科学教育的AI教辅Kwame的现实世界部署和评估

    Real-World Deployment and Evaluation of Kwame for Science, An AI Teaching Assistant for Science Education in West Africa

    [https://arxiv.org/abs/2302.10786](https://arxiv.org/abs/2302.10786)

    这项研究扩展了AI教辅Kwame，面向西非科学教育，并将其作为一个网络应用程序进行了实际部署和评估。结果显示，在实际环境中，Kwame for Science取得了很高的准确率，并且获得了广泛的用户和问题提问量。

    

    非洲教师与学生的比例高，这限制了学生们获取教育问题解答等学习支持的机会。本研究将面向编码教育的AI教辅Kwame扩展为面向科学教育，并将其部署为一个网络应用程序。Kwame for Science通过提供来自精选知识来源的段落以及基于西非高级中学证书考试（WASSCE）的综合科学科目的相关过去的国家考试问题的答案来回答学生们的问题。此外，学生们还可以查看过去的国家考试问题及其答案，并通过我们开发的主题检测模型进行按年份、问题类型和主题的自动分类（91%非加权平均召回率）。我们在实际环境中部署了Kwame for Science超过8个月，有来自32个国家（其中15个在非洲）的750个用户，共提出了1.5K的问题。我们的评估结果显示，87.2%的前三名问题准确率（n=109）。

    Africa has a high student-to-teacher ratio which limits students' access to teachers for learning support such as educational question answering. In this work, we extended Kwame, a bilingual AI teaching assistant for coding education, adapted it for science education, and deployed it as a web app. Kwame for Science provides passages from well-curated knowledge sources and related past national exam questions as answers to questions from students based on the Integrated Science subject of the West African Senior Secondary Certificate Examination (WASSCE). Furthermore, students can view past national exam questions along with their answers and filter by year, question type, and topics that were automatically categorized by a topic detection model which we developed (91% unweighted average recall). We deployed Kwame for Science in the real world over 8 months and had 750 users across 32 countries (15 in Africa) and 1.5K questions asked. Our evaluation showed an 87.2% top 3 accuracy (n=109
    
[^27]: (聊天)GPT v BERT: 语义变化检测之黎明的正义。(arXiv:2401.14040v1 [cs.CL])

    (Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection. (arXiv:2401.14040v1 [cs.CL])

    [http://arxiv.org/abs/2401.14040](http://arxiv.org/abs/2401.14040)

    本研究探讨了(Chat)GPT和BERT在语义变化检测任务中的性能，结果表明(Chat)GPT的表现明显低于BERT，尤其在长期变化检测方面表现更差。

    

    在自然语言处理领域，基于Transformer的语言模型，如BERT和(Chat)GPT，作为具有解决开放性研究问题的巨大能力的词汇超级英雄而出现。本文特别关注语义变化的时间性问题，并评估它们解决Word-in-Context (WiC)任务的两个历时性扩展：TempoWiC和HistoWiC。特别是，我们研究了ChatGPT（和GPT）3.5这样的新型即用技术与当前作为建模语义变化的最先进模型家族BERT之间的潜力。我们的实验是首次尝试使用(Chat)GPT研究语义变化。我们的结果表明，ChatGPT的性能显著低于基础GPT版本。此外，我们的结果表明，(Chat)GPT在检测长期变化方面的表现略低于BERT，但在短期变化检测方面表现明显更差。

    In the universe of Natural Language Processing, Transformer-based language models like BERT and (Chat)GPT have emerged as lexical superheroes with great power to solve open research problems. In this paper, we specifically focus on the temporal problem of semantic change, and evaluate their ability to solve two diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and HistoWiC. In particular, we investigate the potential of a novel, off-the-shelf technology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a family of models that currently stand as the state-of-the-art for modeling semantic change. Our experiments represent the first attempt to assess the use of (Chat)GPT for studying semantic change. Our results indicate that ChatGPT performs significantly worse than the foundational GPT version. Furthermore, our results demonstrate that (Chat)GPT achieves slightly lower performance than BERT in detecting long-term changes but performs significantly worse in de
    
[^28]: 基于剪枝的保护: 在不进行微调的情况下增加对齐的LLMs的越狱抵抗力

    Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning. (arXiv:2401.10862v1 [cs.LG])

    [http://arxiv.org/abs/2401.10862](http://arxiv.org/abs/2401.10862)

    本文研究了剪枝对齐的LLMs的保护措施，发现剪枝LLM参数可以显著增强其抵抗“越狱”提示攻击的能力，并且对其他LLM行为也可能有更普遍的效果。同时，引入了一个有害任务数据集，证明剪枝有助于集中注意力在与任务相关的标记上。突出的聊天模型表现出很高的易感性。

    

    大型语言模型（LLMs）容易受到“越狱”提示的攻击，这种攻击可以诱使这些模型生成有害和违法内容。本文表明，剪枝LLM参数多达20％可以显著增加它们对此类攻击的抵抗力，而无需额外训练并且不损害其在标准基准测试中的性能。有趣的是，我们发现剪枝后观察到的增强安全性与模型的初始安全训练水平相关，这暗示剪枝的效果可能更普遍，也可能适用于超出安全性范畴的其他LLM行为。另外，我们还介绍了一个包含五个类别、插入到十个不同越狱提示中的225个有害任务的精选数据集，表明剪枝有助于LLMs集中注意力在越狱提示中与任务相关的标记上。最后，我们的实验揭示了突出的聊天模型（如LLaMA-2 Chat，Vicuna和Mistral Instruct）具有很高的易感性。

    Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content. In this paper, we show that pruning up to 20% of LLM parameters markedly increases their resistance to such attacks without additional training and without sacrificing their performance in standard benchmarks. Intriguingly, we discovered that the enhanced safety observed post-pruning correlates to the initial safety training level of the model, hinting that the effect of pruning could be more general and may hold for other LLM behaviors beyond safety. Additionally, we introduce a curated dataset of 225 harmful tasks across five categories, inserted into ten different Jailbreaking prompts, showing that pruning aids LLMs in concentrating attention on task-relevant tokens in jailbreaking prompts. Lastly, our experiments reveal that the prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibi
    
[^29]: Intention Analysis Prompting使得大型语言模型成为良好的越狱防御者

    Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender. (arXiv:2401.06561v1 [cs.CL])

    [http://arxiv.org/abs/2401.06561](http://arxiv.org/abs/2401.06561)

    本研究提出了一种名为Intention Analysis Prompting (IAPrompt)的方法，通过触发大型语言模型（LLMs）的自我纠正和改进能力来防御越狱攻击。实验证明，该方法能够显著减少响应中的有害行为并保持整体有用性。

    

    在面对隐蔽和复杂的越狱攻击时，将大型语言模型(LLMs)与人类价值观保持一致是一项极具挑战性的任务。在本研究中，我们提出了一种简单但非常有效的防御策略，即Intention Analysis Prompting（IAPrompt）。其原理是通过两个阶段的过程触发LLMs的内在自我纠正和改进能力：1）基本意图分析，2）与政策一致的响应。值得注意的是，IAPrompt是一种仅推断的方法，因此可以提高LLMs的安全性而不损害其有用性。在Vicuna、ChatGLM、MPT、DeepSeek和GPT-3.5上进行的广泛实验表明，IAPrompt能够持续且显著地减少响应中的有害行为（平均攻击成功率下降46.5%），同时保持整体有用性。进一步的分析揭示了我们方法的一些见解。为了保证可重复性，我们在https://github.com/alph上发布了我们的代码和脚本。

    Aligning large language models (LLMs) with human values, particularly in the face of stealthy and complex jailbreaks, presents a formidable challenge. In this study, we present a simple yet highly effective defense strategy, i.e., Intention Analysis Prompting (IAPrompt). The principle behind is to trigger LLMs' inherent self-correct and improve ability through a two-stage process: 1) essential intention analysis, and 2) policy-aligned response. Notably, IAPrompt is an inference-only method, thus could enhance the safety of LLMs without compromising their helpfulness. Extensive experiments on SAP200 and DAN benchmarks across Vicuna, ChatGLM, MPT, DeepSeek, and GPT-3.5 show that IAPrompt could consistently and significantly reduce the harmfulness in response (averagely -46.5% attack success rate) and maintain the general helpfulness. Further analyses present some insights into how our method works. To facilitate reproducibility, We release our code and scripts at: https://github.com/alph
    
[^30]: L3Cube-IndicNews：印度语系新闻短文和长文分类数据集

    L3Cube-IndicNews: News-based Short Text and Long Document Classification Datasets in Indic Languages. (arXiv:2401.02254v1 [cs.CL])

    [http://arxiv.org/abs/2401.02254](http://arxiv.org/abs/2401.02254)

    L3Cube-IndicNews是一个面向印度语系的多语种文本分类数据集，包括短标题、长文档和长段落三个数据集。它提供了10种印度语言的新闻文章，每个数据集包含10个或更多类别的文章。这个数据集可以用于深入分析和评估。

    

    在这项工作中，我们介绍了L3Cube-IndicNews，这是一个多语种文本分类语料库，旨在为印度地区的各大方言语言提供高质量的数据集，特别关注新闻标题和文章。我们的工作主要集中在10种主要的印度语言上，包括印地语、孟加拉语、马拉地语、泰卢固语、泰米尔语、古吉拉特语、卡纳达语、奥里亚语、马拉雅拉姆语和旁遮普语。每个新闻数据集包含10个或更多类别的新闻文章。L3Cube-IndicNews提供了3个不同数据集，针对不同的文档长度进行分类：短标题分类（SHC）数据集包含新闻标题和新闻类别，长文档分类（LDC）数据集包含整个新闻文章和新闻类别，长段落分类（LPC）数据集包含新闻的子文章和新闻类别。我们在所有3个数据集中都保持了一致的标签，以进行深入的基于长度的分析。我们使用4个指标对每个印度语言数据集进行评估。

    In this work, we introduce L3Cube-IndicNews, a multilingual text classification corpus aimed at curating a high-quality dataset for Indian regional languages, with a specific focus on news headlines and articles. We have centered our work on 10 prominent Indic languages, including Hindi, Bengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and Punjabi. Each of these news datasets comprises 10 or more classes of news articles. L3Cube-IndicNews offers 3 distinct datasets tailored to handle different document lengths that are classified as: Short Headlines Classification (SHC) dataset containing the news headline and news category, Long Document Classification (LDC) dataset containing the whole news article and the news category, and Long Paragraph Classification (LPC) containing sub-articles of the news and the news category. We maintain consistent labeling across all 3 datasets for in-depth length-based analysis. We evaluate each of these Indic language datasets using 4 
    
[^31]: 重构材料四面体：材料信息提取中的挑战

    Reconstructing Materials Tetrahedron: Challenges in Materials Information Extraction. (arXiv:2310.08383v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.08383](http://arxiv.org/abs/2310.08383)

    本论文讨论了从材料科学文献中自动提取信息面临的挑战，并希望能够创建一个大型的材料科学知识库。

    

    新材料的发现已经有着持续几个世纪乃至更长时间的推动人类进步的历史。材料的行为取决于其组成、结构和性质，而这些又依赖于其加工和测试条件。最近深度学习和自然语言处理的发展使得从出版的文献中（如同行评审出版物、图书和专利）大规模提取信息成为可能。然而，这些信息以不同的格式（如表格、文本和图像）展现，并且在报告样式上缺乏一致性，这引发了几个机器学习的挑战。在这里，我们讨论、量化和记录了材料科学文献自动信息提取方面的这些挑战，以期创建一个大型的材料科学知识库。具体而言，我们关注从文本和表格中进行信息提取，并举例说明了几个挑战。我们希望本研究能激发研究人员解决这些挑战。

    Discovery of new materials has a documented history of propelling human progress for centuries and more. The behaviour of a material is a function of its composition, structure, and properties, which further depend on its processing and testing conditions. Recent developments in deep learning and natural language processing have enabled information extraction at scale from published literature such as peer-reviewed publications, books, and patents. However, this information is spread in multiple formats, such as tables, text, and images, and with little or no uniformity in reporting style giving rise to several machine learning challenges. Here, we discuss, quantify, and document these challenges in automated information extraction (IE) from materials science literature towards the creation of a large materials science knowledge base. Specifically, we focus on IE from text and tables and outline several challenges with examples. We hope the present work inspires researchers to address 
    
[^32]: 永远不要从头开始训练：公正比较长序列模型需要数据驱动的先验知识

    Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors. (arXiv:2310.02980v1 [cs.LG])

    [http://arxiv.org/abs/2310.02980](http://arxiv.org/abs/2310.02980)

    本文研究表明使用随机初始化会导致对架构差异的严重高估，而使用标准消噪目标进行预训练可以在多种架构上实现显著的性能提升，并将Transformers与状态空间模型之间的差距缩小到很小。与之前的研究不同的是，我们发现当正确预训练时，普通的Transformers在Long Range Arena上的性能与S4相匹配，并且在PathX-256任务上改进了SSMs的最佳结果20个百分点。

    

    建模序列之间的长程依赖一直是机器学习中的目标，并导致了一些架构，如状态空间模型，在处理长序列时比Transformers有显著的优势。然而，这些令人印象深刻的经验性进展主要是在随机初始化并通过预测输入序列的目标标签进行训练的基准测试（例如Long Range Arena）上展示出来的。在这项工作中，我们展示了随机初始化导致对架构之间差异的严重高估，并且使用标准消噪目标进行预训练（仅使用下游任务数据）可以在多种架构上实现显著的收益，并且可以在Transformers和状态空间模型（SSMs）之间得到很小的差距。与之前的研究形成鲜明对比的是，我们发现当正确预训练时，普通的Transformers在Long Range Arena上与S4的性能相匹配，并且我们在PathX-256任务上将SSMs的最佳报告结果提高了20个百分点。

    Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using $\textit{only the downstream task data}$, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 
    
[^33]: SSHR: 利用自监督层次表示提高多语种自动语音识别的能力

    SSHR: Leveraging Self-supervised Hierarchical Representations for Multilingual Automatic Speech Recognition. (arXiv:2309.16937v1 [cs.CL])

    [http://arxiv.org/abs/2309.16937](http://arxiv.org/abs/2309.16937)

    本研究提出了一种利用自监督层次表示（SSHR）优化多语种自动语音识别的方法。通过分析自监督学习模型的不同层次表示，提取出语言相关帧和特定内容，并引导模型在最终层次获取更多内容相关信息。

    

    多语种自动语音识别（ASR）系统因其在全球范围内扩展语言覆盖的潜力而受到关注。虽然自监督学习（SSL）在多语种ASR方面表现出了其有效性，但值得注意的是，SSL模型的不同层次表示可能包含尚未充分利用的不同信息。在本研究中，我们提出了一种利用自监督层次表示（SSHR）来优化多语种ASR的新方法。我们首先分析SSL模型的不同层次对于语言相关和内容相关信息的表达情况，发现具有更强相关性的层次。然后，我们从相关中间层中提取语言相关帧，并通过自注意机制引导特定内容的提取。此外，我们通过提出的交叉CTC方法，引导模型在最终层次获得更多内容相关信息。我们在两个多语种数据集（C）上评估了SSHR的性能。

    Multilingual automatic speech recognition (ASR) systems have garnered attention for their potential to extend language coverage globally. While self-supervised learning (SSL) has demonstrated its effectiveness in multilingual ASR, it is worth noting that the various layers' representations of SSL potentially contain distinct information that has not been fully leveraged. In this study, we propose a novel method that leverages self-supervised hierarchical representations (SSHR) to fine-tune multilingual ASR. We first analyze the different layers of the SSL model for language-related and content-related information, uncovering layers that show a stronger correlation. Then, we extract a language-related frame from correlated middle layers and guide specific content extraction through self-attention mechanisms. Additionally, we steer the model toward acquiring more content-related information in the final layers using our proposed Cross-CTC. We evaluate SSHR on two multilingual datasets, C
    
[^34]: 一个交互式框架用于对新闻媒体来源进行特征分析

    An Interactive Framework for Profiling News Media Sources. (arXiv:2309.07384v1 [cs.CL])

    [http://arxiv.org/abs/2309.07384](http://arxiv.org/abs/2309.07384)

    本文提出了一个交互式框架用于对新闻媒体来源进行特征分析，通过结合图形分析模型、预训练语言模型和人类洞察力，可以快速检测虚假和有偏见的新闻媒体。

    

    社交媒体的兴起导致了大量虚假和有偏见的新闻传播，即以影响信仰为目的的内容发布。虽然检测和对传播这些新闻的来源进行特征分析对于维护一个健康的社会很重要，但对于自动化系统来说是具有挑战性的。本文提出了一个交互式框架用于新闻媒体的特征分析，将基于图的新闻媒体特征分析模型、预训练大型语言模型和人类洞察力相结合，以表征社交媒体上的社会环境。实验结果显示，通过仅需五次人类交互，我们的框架可以快速检测出虚假和有偏见的新闻媒体，甚至在最具挑战性的新闻事件出现时，其中的测试数据是未知的。

    The recent rise of social media has led to the spread of large amounts of fake and biased news, content published with the intent to sway beliefs. While detecting and profiling the sources that spread this news is important to maintain a healthy society, it is challenging for automated systems.  In this paper, we propose an interactive framework for news media profiling. It combines the strengths of graph based news media profiling models, Pre-trained Large Language Models, and human insight to characterize the social context on social media. Experimental results show that with as little as 5 human interactions, our framework can rapidly detect fake and biased news media, even in the most challenging settings of emerging news events, where test data is unseen.
    
[^35]: 使用大型语言模型将患者与临床试验匹配

    Matching Patients to Clinical Trials with Large Language Models. (arXiv:2307.15051v1 [cs.CL])

    [http://arxiv.org/abs/2307.15051](http://arxiv.org/abs/2307.15051)

    本研究调查了使用大型语言模型（LLMs）来帮助患者和转诊医生识别合适的临床试验的潜力，并引入了TrialGPT架构，该架构能够准确预测合格性并提供解释，实验证明其有效性。

    

    临床试验在推动药物研发和基于证据的医学方面非常重要，但患者招募常常受到限制。在这项工作中，我们调查了使用大型语言模型（LLMs）来帮助患者和转诊医生识别合适的临床试验的潜力。具体而言，我们引入了一种新颖的架构TrialGPT，采用LLMs预测基于标准的合格性，并提供详细的解释，并根据患者病历中的自由文本来对候选临床试验进行排名和排除。我们在三个公开可用的184名患者和18,238个注释的临床试验的队列上评估了TrialGPT。实验结果表明几个关键发现：第一，TrialGPT在标准级别的预测准确性上表现出很高的准确率，并提供准确的解释。第二，TrialGPT的综合试验级别评分与专家标注的合格性高度相关。第三，这些评分

    Clinical trials are vital in advancing drug development and evidence-based medicine, but their success is often hindered by challenges in patient recruitment. In this work, we investigate the potential of large language models (LLMs) to assist individual patients and referral physicians in identifying suitable clinical trials from an extensive selection. Specifically, we introduce TrialGPT, a novel architecture employing LLMs to predict criterion-level eligibility with detailed explanations, which are then aggregated for ranking and excluding candidate clinical trials based on free-text patient notes. We evaluate TrialGPT on three publicly available cohorts of 184 patients and 18,238 annotated clinical trials. The experimental results demonstrate several key findings: First, TrialGPT achieves high criterion-level prediction accuracy with faithful explanations. Second, the aggregated trial-level TrialGPT scores are highly correlated with expert eligibility annotations. Third, these scor
    
[^36]: 零样本和少样本情况下应用于临床和生物医学任务的指导细调大型语言模型的研究

    A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks. (arXiv:2307.12114v1 [cs.CL])

    [http://arxiv.org/abs/2307.12114](http://arxiv.org/abs/2307.12114)

    这项研究评估了四种指导细调大型语言模型在临床和生物医学任务上的表现，并发现它们在零样本和少样本情况下接近最先进模型的性能，尤其在问答任务上表现良好。然而，在分类和关系抽取任务上的表现稍逊于特定训练于医学领域的模型。没有一个模型在所有研究任务上胜过其他模型，有些模型更适合特定任务。

    

    我们评估了四种最先进的指导细调大型语言模型（LLM）——ChatGPT、Flan-T5 UL2、Tk-Instruct和Alpaca——在13个实际世界的临床和生物医学自然语言处理（NLP）任务中的表现，例如命名实体识别（NER）、问答（QA）、关系抽取（RE）等。我们的综合结果表明，在大多数任务的零样本和少样本情况下，评估的LLM开始接近最先进模型的性能，尤其对于QA任务表现得特别好，即使它们之前没有见过这些任务的示例。然而，我们观察到分类和关系抽取任务的表现低于特定训练于医学领域的模型（如PubMedBERT）可以达到的水平。最后，我们注意到没有一个LLM在所有研究任务上都胜过其他模型，有些模型更适合于特定的任务。

    We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.
    
[^37]: 大型语言模型塑造并受到社会的影响：arXiv出版模式调查

    Large language models shape and are shaped by society: A survey of arXiv publication patterns. (arXiv:2307.10700v1 [cs.DL])

    [http://arxiv.org/abs/2307.10700](http://arxiv.org/abs/2307.10700)

    大型语言模型的论文数量急剧增加，研究重点逐渐转向社会影响。与LLM相关的论文呈现持续增长的趋势，新发表关于LLM的作者更注重应用和社会影响。

    

    大型语言模型的论文数量近年来呈急剧增加，这种变化对科学领域产生了戏剧性的影响，但目前还没有进行详细的文献计量分析。本文分析了CS和Stat arXiv上发布的388K篇论文，并重点关注2023年与2018-2022年之间发表模式的变化。我们分析了LLM论文的比例增加情况，得到了最多关注的与LLM相关的主题，撰写LLM论文的作者，作者的研究主题与背景的相关性，区分高被引用LLM论文的因素，以及国际合作的模式。我们展示了LLM研究越来越关注社会影响：在计算机与社会子arXiv上，与LLM相关的论文比例增加了18倍，新发表关于LLM的作者更倾向于关注应用和社会影响。LLM研究也受到社会动态的影响。

    There has been a steep recent increase in the number of large language model (LLM) papers, producing a dramatic shift in the scientific landscape which remains largely undocumented through bibliometric analysis. Here, we analyze 388K papers posted on the CS and Stat arXivs, focusing on changes in publication patterns in 2023 vs. 2018-2022. We analyze how the proportion of LLM papers is increasing; the LLM-related topics receiving the most attention; the authors writing LLM papers; how authors' research topics correlate with their backgrounds; the factors distinguishing highly cited LLM papers; and the patterns of international collaboration. We show that LLM research increasingly focuses on societal impacts: there has been an 18x increase in the proportion of LLM-related papers on the Computers and Society sub-arXiv, and authors newly publishing on LLMs are more likely to focus on applications and societal impacts than more experienced authors. LLM research is also shaped by social dyn
    
[^38]: MMBench: 您的多模态模型是全能球员吗？

    MMBench: Is Your Multi-modal Model an All-around Player?. (arXiv:2307.06281v1 [cs.CV])

    [http://arxiv.org/abs/2307.06281](http://arxiv.org/abs/2307.06281)

    MMBench是一个新型的多模态基准测试，旨在解决大型视觉语言模型评估的挑战，通过开发全面的评估流程和精心策划的数据集进行细粒度能力评估。

    

    最近，大型视觉语言模型在视觉信息的感知和推理能力方面取得了显著进展。然而，如何有效评估这些大型视觉语言模型仍然是一个主要障碍，阻碍了未来模型的发展。传统的基准测试，如VQAv2或COCO Caption提供了定量的性能测量，但在细粒度能力评估和非鲁棒评估指标方面存在不足。最近的主观基准测试，如OwlEval，通过整合人力资源，对模型的能力进行了全面评估，但不可扩展并且存在显著的偏见。针对这些挑战，我们提出了MMBench，一种新型的多模态基准测试。MMBench系统地开发了一个全面的评估流程，主要由两个元素组成。第一个元素是精心策划的数据集，在评估数量和多样性方面超越了现有的类似基准测试。

    Large vision-language models have recently achieved remarkable progress, exhibiting great perception and reasoning abilities concerning visual information. However, how to effectively evaluate these large vision-language models remains a major obstacle, hindering future model development. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but suffer from a lack of fine-grained ability assessment and non-robust evaluation metrics. Recent subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, but they are not scalable and display significant bias. In response to these challenges, we propose MMBench, a novel multi-modality benchmark. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of two elements. The first element is a meticulously curated dataset that surpasses existing similar benchmarks in terms of the number and variety of evaluatio
    

