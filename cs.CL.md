# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression](https://arxiv.org/abs/2403.12968) | 该论文提出了一种数据精炼的方法，通过从LLM中提取知识来实现Prompt的压缩，确保压缩后的提示保持对原始提示的忠实性。 |
| [^2] | [Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models](https://arxiv.org/abs/2403.12964) | 本研究在视觉-语言模型的微调中引入了双学习概念，提出了DualAdapter方法，通过正面和负面两方面的双路径适配，同时进行补充正向选择和负向排除，从而提高了在下游任务中的整体识别准确性。 |
| [^3] | [Dated Data: Tracing Knowledge Cutoffs in Large Language Models](https://arxiv.org/abs/2403.12958) | 本文提出了在大型语言模型中追踪知识截止日期的概念，通过资源级别的时间对齐性估计有效截止日期，并发现这些截止日期通常与报道的不同。 |
| [^4] | [Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models](https://arxiv.org/abs/2403.12936) | 本文研究了使用大型语言模型GPT-4自动从英国雇佣法庭案例中提取信息的应用，并通过手动验证确保了提取数据的准确性和相关性 |
| [^5] | [Supporting Energy Policy Research with Large Language Models](https://arxiv.org/abs/2403.12924) | 本研究的创新贡献在于将决策树框架与大型语言模型集成，从而实现了自动提取法规的准确性达到85%至90%。 |
| [^6] | [Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts](https://arxiv.org/abs/2403.12918) | 提出了一种基于注意力引导的权重混合正则化方法，用于解决低资源文本上预训练语言模型微调时的稳定性和泛化能力问题 |
| [^7] | [Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference](https://arxiv.org/abs/2403.12900) | 本文提出了Sprout框架，通过引入生成指令的概念，平衡了生态可持续性和高质量生成结果之间的需求，实现了大型语言模型推断服务碳排放的显著减少。 |
| [^8] | [Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models](https://arxiv.org/abs/2403.12881) | 本文提出了Agent-FLAN，通过精心分解和重新设计训练语料库，使得Llama2-7B在各种代理评估中超过先前的最佳工作3.5％ |
| [^9] | [Epistemology of Language Models: Do Language Models Have Holistic Knowledge?](https://arxiv.org/abs/2403.12862) | 本文从认识论整体主义的角度研究了语言模型的固有知识，并通过创造科学推理数据集和三个任务评估了语言模型的认识论表现。 |
| [^10] | [Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models](https://arxiv.org/abs/2403.12809) | 多语和单语语言模型之间的特征归因方法的忠实度存在差异，实验证明多语模型越大，FA相对于单语模型来说越不忠实。 |
| [^11] | [Contextual Moral Value Alignment Through Context-Based Aggregation](https://arxiv.org/abs/2403.12805) | 提出了一个基于情境聚合的情境道德价值对齐系统，相比现有技术，在与人类价值对齐方面表现更好。 |
| [^12] | [Investigating Text Shortening Strategy in BERT: Truncation vs Summarization](https://arxiv.org/abs/2403.12799) | 本研究调查了文本分类任务中文档截断和摘要的表现，发现摘要在大多数情况下胜过截断方法的变体，最佳策略为取文档的开头。 |
| [^13] | [Automated Data Curation for Robust Language Model Fine-Tuning](https://arxiv.org/abs/2403.12776) | 介绍了一个自动化数据管理流水线CLEAR（基于置信度的LLM评估和纠正）用于指令微调数据集，可与任何LLM和微调程序一起使用。 |
| [^14] | [NovelQA: A Benchmark for Long-Range Novel Question Answering](https://arxiv.org/abs/2403.12766) | NovelQA是一个专门设计用于测试大型语言模型（LLMs）在长文本上的能力的基准，通过英文小说构建，提供了复杂性、长度和叙述连贯性的独特组合，可用于评估LLMs在深度文本理解方面的性能表现。 |
| [^15] | [Sebastian, Basti, Wastl?! Recognizing Named Entities in Bavarian Dialectal Data](https://arxiv.org/abs/2403.12749) | 介绍了德语第一个方言NER数据集BarNER，并展示了在巴伐利亚方言数据上的全面NER结果，表明从大型德语NER子数据集中汲取知识可以改进巴伐利亚数据的表现，而在巴伐利亚进行训练对标志性的德语数据也有所帮助。 |
| [^16] | [Instructing Large Language Models to Identify and Ignore Irrelevant Conditions](https://arxiv.org/abs/2403.12744) | 提出了一种新方法 I$^3$C，指导大型语言模型识别和忽略不相关条件，并通过少样本推理加以增强。 |
| [^17] | [CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched with Linguistic and Genre Annotation](https://arxiv.org/abs/2403.12721) | 本文介绍了一个涵盖南斯拉夫地区官方语言的高度可比较的网络语料库集合，采用先进的技术进行语言和文体标注，进一步增强了其可比较性。 |
| [^18] | [Empowering Air Travelers: A Chatbot for Canadian Air Passenger Rights](https://arxiv.org/abs/2403.12678) | 提出了一款关于加拿大空中旅客权利的聊天机器人，帮助旅客理解和利用相关空中旅行法规，成功解决了用户输入复杂和准确回答问题的挑战 |
| [^19] | [Pragmatic Competence Evaluation of Large Language Models for Korean](https://arxiv.org/abs/2403.12675) | 该研究将大型语言模型的评估从对嵌入知识的基准拓展到了探索语用能力，结果显示在韩语环境下，GPT-4在传统和人工评估设置中表现出色，而HyperCLOVA X也在开放式问题评估中取得了不错的成绩。 |
| [^20] | [Multi-Dimensional Machine Translation Evaluation: Model Evaluation and Resource for Korean](https://arxiv.org/abs/2403.12666) | 本文提出了一个针对英韩语言对的1200句MQM评估基准，并通过使用最先进的语言模型，将MT评估转化为同时预测多个MQM分数的多任务问题。 |
| [^21] | [LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models](https://arxiv.org/abs/2403.12601) | LHMKE是一个面向中文大型语言模型的大规模、整体和多学科知识评估基准，旨在全面评估这些模型的知识获取能力。 |
| [^22] | [Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs](https://arxiv.org/abs/2403.12596) | 提出了一种将大型语言模型（LLMs）的能力转移到视觉语言模型（VLMs）的技术，在多个任务上取得了最先进的性能。 |
| [^23] | [AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework](https://arxiv.org/abs/2403.12582) | AlphaFin提出了解决股票趋势预测和金融问答任务中可解释性和实时信息集成困难的问题，通过发布AlphaFin数据集来应对这些挑战。 |
| [^24] | [Simple Hack for Transformers against Heavy Long-Text Classification on a Time- and Memory-Limited GPU Service](https://arxiv.org/abs/2403.12563) | 通过在有限资源上逐步执行高效动态的HPO过程，提出了一种注重将Transformers模型适用于长文本分类任务的简单技巧。 |
| [^25] | [Factorized Learning Assisted with Large Language Model for Gloss-free Sign Language Translation](https://arxiv.org/abs/2403.12556) | 提出了一种利用大型语言模型辅助分解学习的无词汇手语翻译方法，通过将训练过程分解为两个阶段，在视觉初始化阶段采用轻量级翻译模型预训练视觉编码器，解决了直接引入大型语言模型导致学习不足的问题。 |
| [^26] | [Prompt-based Graph Model for Joint Liberal Event Extraction and Event Schema Induction](https://arxiv.org/abs/2403.12526) | 提出了一种基于提示的图模型用于自由事件提取，旨在同时提取事件并发现事件模式，避免了对外部语言知识库的严重依赖和大量手动规则开发的复杂工作。 |
| [^27] | [GraphERE: Jointly Multiple Event-Event Relation Extraction via Graph-Enhanced Event Embeddings](https://arxiv.org/abs/2403.12523) | 该论文提出了一种名为GraphERE的多事件关系提取框架，通过使用图增强事件嵌入，扩展了事件嵌入的事件参数和结构特征，从而解决了事件触发器嵌入的局限以及关系之间互连被忽略的问题。 |
| [^28] | [A Large Collection of Model-generated Contradictory Responses for Consistency-aware Dialogue Systems](https://arxiv.org/abs/2403.12500) | 本文首次构建了大规模的模型生成矛盾响应数据集，并通过深入分析获得了宝贵的矛盾特征洞察。 |
| [^29] | [Embodied LLM Agents Learn to Cooperate in Organized Teams](https://arxiv.org/abs/2403.12482) | 本文介绍了一个框架，将即时性组织结构强加在LLM代理上，以促进多代理系统内的合作，在具身LLM代理和人-代理合作实验中发现指定领导对团队效率的影响，揭示了LLM代理的领导素质和自发合作行为。 |
| [^30] | [When Do "More Contexts" Help with Sarcasm Recognition?](https://arxiv.org/abs/2403.12469) | 本研究探讨了将更多上下文信息整合到模型中能够带来的对于讽刺识别的改进效果。 |
| [^31] | [CrossTune: Black-Box Few-Shot Classification with Label Enhancement](https://arxiv.org/abs/2403.12468) | CrossTune是一种带有标签增强的黑盒少样本分类网络，通过模拟输入文本序列与任务标签的语义相关性来提高泛化能力。 |
| [^32] | [Eye-gaze Guided Multi-modal Alignment Framework for Radiology](https://arxiv.org/abs/2403.12416) | 提出一种利用眼控数据的多模态对齐框架，可降低对手动注释的依赖 |
| [^33] | [Third-Party Language Model Performance Prediction from Instruction](https://arxiv.org/abs/2403.12413) | 提出了一种第三方性能预测框架，用于预测评估指令跟随系统在任务上表现的度量标准，从而解决用户无法了解系统可靠性的问题。 |
| [^34] | [MSLM-S2ST: A Multitask Speech Language Model for Textless Speech-to-Speech Translation with Speaker Style Preservation](https://arxiv.org/abs/2403.12408) | 提出了一种无文本训练数据的多任务语音语言模型，支持多语言S2ST并保留说话者风格 |
| [^35] | [Cross-Lingual Transfer for Natural Language Inference via Multilingual Prompt Translator](https://arxiv.org/abs/2403.12407) | 提出了一种名为多语言提示翻译（MPT）的框架，通过引入多语言提示翻译器实现在低资源情境下将软提示从源语言有效转移到目标语言，在少样本设置下表现优越 |
| [^36] | [Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales](https://arxiv.org/abs/2403.12403) | 利用大型语言模型提取原因特征训练仇恨言论分类器，实现可解释的检测方法 |
| [^37] | [An Empirical Study of Speech Language Models for Prompt-Conditioned Speech Synthesis](https://arxiv.org/abs/2403.12402) | 对自回归和非自回归语音LM进行实证研究，揭示了提示设计和内容语义单元的重要性，并发现提示的异质性和非平稳性会影响音频质量，同时内容也会影响合成音频的说话风格。 |
| [^38] | [Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open Domain Multi-Hop Question Answering](https://arxiv.org/abs/2403.12393) | 提出Dr3机制，通过辨别器判断大型语言模型生成的答案是否题外来解决开放领域多跳问答中题外答案的问题 |
| [^39] | [AraPoemBERT: A Pretrained Language Model for Arabic Poetry Analysis](https://arxiv.org/abs/2403.12392) | AraPoemBERT 是一种专门针对阿拉伯诗歌文本进行预训练的语言模型，在阿拉伯诗歌相关的NLP任务中表现出色，取得了两项新颖任务中的前所未有的高准确率。 |
| [^40] | [Pipelined Biomedical Event Extraction Rivaling Joint Learning](https://arxiv.org/abs/2403.12386) | 本文提出了一种基于BERT预训练模型的n元关系提取方法，用于改进Binding事件的性能，从而提高分阶段生物医学事件提取的整体性能。 |
| [^41] | [Improving Generalizability of Extracting Social Determinants of Health Using Large Language Models through Prompt-tuning](https://arxiv.org/abs/2403.12374) | 该研究提出了一种通过软提示式学习架构来改进使用大型语言模型提取健康社会决定因素的泛化能力的方法，并发现decoder-only的LLMs在跨领域应用中通过Prompt-tuning取得了更好的性能。 |
| [^42] | [RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners](https://arxiv.org/abs/2403.12373) | RankPrompt 提出了一种新的提示方法，可以通过自我排序来提高大型语言模型在推理任务中的性能。 |
| [^43] | [Characteristic AI Agents via Large Language Models](https://arxiv.org/abs/2403.12368) | 该研究通过模拟现实个体，探讨了大型语言模型在构建特征化AI代理方面的性能，为这一任务创建了新的基准数据集和评估指标。 |
| [^44] | [Methods for Generating Drift in Text Streams](https://arxiv.org/abs/2403.12328) | 文本数据中概念漂移是一个常见现象，而本文提出了四种文本漂移生成方法来帮助产生具有标记漂移的数据集 |
| [^45] | [OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety](https://arxiv.org/abs/2403.12316) | OpenEval引入了一个评估测试平台，通过对中文LLMs进行基准测试，涵盖了能力、对齐性和安全性，填补了现有基准测试忽视对齐性和安全问题的空白。 |
| [^46] | [Leveraging Large Language Models to Extract Information on Substance Use Disorder Severity from Clinical Notes: A Zero-shot Learning Approach](https://arxiv.org/abs/2403.12297) | 该研究利用大型语言模型从临床记录中提取物质使用障碍严重程度信息，克服了传统自然语言处理方法在解析复杂临床语言方面的局限性 |
| [^47] | [A Comparative Investigation of Compositional Syntax and Semantics in DALL-E 2](https://arxiv.org/abs/2403.12294) | 该研究比较了DALL-E 2在视觉上如何代表语言提示的含义，结果显示DALL-E 2未能生成与儿童语义准确性相匹配的图像，指向了句子表达的组成性的明显缺失。 |
| [^48] | [FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications](https://arxiv.org/abs/2403.12285) | 引入了一种基于Llama 2 7B模型的金融领域特定的情感分类框架，通过微调模型来受益于其生成性质和全面的语言操作。 |
| [^49] | [Zero-Shot Multi-task Hallucination Detection](https://arxiv.org/abs/2403.12244) | 提出了一个在零样本设置下定量检测幻觉的框架，并在模型感知设置下实现了0.78的准确度，同时保持了计算效率。 |
| [^50] | [Reference-based Metrics Disprove Themselves in Question Generation](https://arxiv.org/abs/2403.12242) | 基于参考文献的指标在问句生成中被推翻，作者提出了一个无需参考文献的多维标准评估方法。 |
| [^51] | [Evaluating Named Entity Recognition: Comparative Analysis of Mono- and Multilingual Transformer Models on Brazilian Corporate Earnings Call Transcriptions](https://arxiv.org/abs/2403.12212) | 本研究通过引入新方法，将标记分类任务重新构建为文本生成问题，评估了在巴西银行财报电话转录中使用的单语和多语言Transformer模型的性能。 |
| [^52] | [TnT-LLM: Text Mining at Scale with Large Language Models](https://arxiv.org/abs/2403.12173) | TnT-LLM 提出了一个两阶段框架，利用大规模语言模型自动化生成和分配标签，减少人力成本。 |
| [^53] | [EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models](https://arxiv.org/abs/2403.12171) | EasyJailbreak是一个统一框架，简化了对LLMs进行越狱攻击的构建和评估，支持11种越狱方法，帮助进行广泛范围LLMs的安全验证。 |
| [^54] | [Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification](https://arxiv.org/abs/2403.12151) | 大型语言模型与知识图谱结合，提高零样本对象状态分类性能 |
| [^55] | [Syn-QA2: Evaluating False Assumptions in Long-tail Questions with Synthetic QA Datasets](https://arxiv.org/abs/2403.12145) | 通过合成QA数据集Syn-(QA)$^2$的引入，作者发现在长尾问题上的错误假设对大型语言模型来说是具有挑战性的，尤其是在二元检测任务方面。 |
| [^56] | [Are LLMs Good Cryptic Crossword Solvers?](https://arxiv.org/abs/2403.12094) | 本文建立了三种流行LLMs的基准结果，表明它们在难解填字游戏上的表现仍远远不及人类。 |
| [^57] | [Methods for Matching English Language Addresses](https://arxiv.org/abs/2403.12092) | 该研究定义并规范了生成英语地址匹配对的框架，并研究了距离基准方法到深度学习模型等各种方法之间的精度、召回率和准确度，以确定最适合地址匹配任务的方法。 |
| [^58] | [TMU at TREC Clinical Trials Track 2023](https://arxiv.org/abs/2403.12088) | 多伦多都会大学利用自然语言处理技术和神经语言模型参加TREC临床试验跟踪，并展示了其实验结果。 |
| [^59] | [Presenting Terrorizer: an algorithm for consolidating company names in patent assignees](https://arxiv.org/abs/2403.12083) | 本文介绍了一种名为Terrorizer的算法，利用自然语言处理、网络理论和基于规则的技术，以解决归因于公司的专利中存在的名称变体问题。 |
| [^60] | [The Boy Who Survived: Removing Harry Potter from an LLM is harder than reported](https://arxiv.org/abs/2403.12082) | 通过小规模实验发现，从LLM中删除哈利波特内容比先前报道的更加困难。 |
| [^61] | [Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions](https://arxiv.org/abs/2403.12077) | 评估生成式搜索引擎对对抗性事实问题的健壮性，通过对多种生成式搜索引擎进行人类评估，展示了对抗性事实问题在诱导不正确响应方面的有效性。 |
| [^62] | [JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning](https://arxiv.org/abs/2403.11366) | 提出了用于检索增强微调的JAX张量并行LoRA库，通过PEFT兼容微调Llama-2模型，利用分布式训练和JAX的即时编译和张量分片实现了资源高效管理，加速微调并降低内存需求，提高了微调大型语言模型在复杂RAG应用中的可扩展性和可行性。 |
| [^63] | [Decoding Continuous Character-based Language from Non-invasive Brain Recordings](https://arxiv.org/abs/2403.11183) | 提出了一种从单次非侵入性脑记录中解码连续语言的新方法，通过三维卷积网络和信息瓶颈结合字符解码器，能够实现在和跨主体之间产生捕捉感知语音含义的可理解文本序列。 |
| [^64] | [Fisher Mask Nodes for Language Model Merging](https://arxiv.org/abs/2403.09891) | 介绍了一种用于Transformers的新型模型合并方法，利用Fisher信息进行加权平均，提高了多任务模型的性能。 |
| [^65] | [MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training](https://arxiv.org/abs/2403.09611) | 通过详细研究图像编码器、视觉语言连接器和预训练数据选择的重要性，确定了对于实现多个基准测试中最新潮的少样本结果至关重要的关键设计经验。 |
| [^66] | [Komodo: A Linguistic Expedition into Indonesia's Regional Languages](https://arxiv.org/abs/2403.09362) | Komodo-7B是一个大型语言模型，可以无缝操作印度尼西亚、英语和11种印度尼西亚地区语言，Komodo-7B-Instruct达到了卓越的性能，超越了多个基准模型。 |
| [^67] | [Knowledge Graph Large Language Model (KG-LLM) for Link Prediction](https://arxiv.org/abs/2403.07311) | 该论文提出了知识图谱大型语言模型框架（KG-LLM），利用思维链提示和上下文学习等NLP范例，以增强知识图谱中的多跳链接预测，并展示了框架在微调大型语言模型和零次尝试能力方面的有效性。 |
| [^68] | [Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery](https://arxiv.org/abs/2403.06097) | 提出了适用于无人机交付系统中地址解析任务的细粒度中文姓名实体识别数据集CNER-UAV，包含五个类别的多样化数据，经过严格的数据清洗和去敏处理，约有12,000个标注样本，评估了传统的实体识别模型并提供了深入分析 |
| [^69] | [KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques](https://arxiv.org/abs/2403.05881) | 本研究开发了KG-Rank框架，利用医学知识图谱和排名技术，旨在提高医学领域自由文本问答的准确性。 |
| [^70] | [ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues](https://arxiv.org/abs/2403.05326) | 本文提出了一个新的基于聊天的方面情绪理解（ChatASU）任务，旨在探索大型语言模型（LLMs）在对话场景中理解方面情绪的能力，并引入了一个子任务Aspect Chain Reasoning（ACR）任务来解决方面共指问题。 |
| [^71] | [PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large Language Models](https://arxiv.org/abs/2403.02246) | 通过提示引发特定人格对大型语言模型的心理理论推理能力产生显著影响，特别是来自黑暗三合会的特质对多种LLMs在不同ToM任务中具有较大效应。 |
| [^72] | [Enhancing Multi-Domain Automatic Short Answer Grading through an Explainable Neuro-Symbolic Pipeline](https://arxiv.org/abs/2403.01811) | 通过提出弱监督标注程序和基于合理化线索的神经符号模型，我们在多领域自动简答题评分方面取得了显著进展。 |
| [^73] | [CASIMIR: A Corpus of Scientific Articles enhanced with Multiple Author-Integrated Revisions](https://arxiv.org/abs/2403.00241) | CASIMIR提供了一个包含多个作者综合修订的科学文章语料库，以促进对科学文章写作修订步骤的研究。 |
| [^74] | [MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning](https://arxiv.org/abs/2402.17231) | MATHSENSEI 是一种用于数学推理的工具增强型大型语言模型，通过添加知识检索、程序执行和符号方程求解工具，提高了数学推理能力。 |
| [^75] | [KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge](https://arxiv.org/abs/2402.13605) | KorNAT是首个用于评估韩国国家对齐的基准，包括社会价值观和常识对齐两个方面，通过对社会价值和常识多项选择题的测试来评估模型的对齐程度。 |
| [^76] | [MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization](https://arxiv.org/abs/2402.11453) | MatPlotAgent介绍了一种基于LLM的自动化科学数据可视化框架，包括查询理解、代码生成和视觉反馈机制，并引入了MatPlotBench基准和GPT-4V评分方法。 |
| [^77] | [The optimal placement of the head in the noun phrase. The case of demonstrative, numeral, adjective and noun](https://arxiv.org/abs/2402.10311) | 本研究旨在探讨句法依赖距离最小化与意外减少最小化原则在名词短语中的冲突，结论显示当涉及的单词较少且单词较短时，意外减少可能会超越句法依赖距离优化。 |
| [^78] | [Towards Reducing Diagnostic Errors with Interpretable Risk Prediction](https://arxiv.org/abs/2402.10109) | 本研究提出了一种使用LLMs方法来识别病人电子病历数据中指示特定诊断风险增加或减少的证据的方法，旨在通过增加证据的获取与减少诊断错误来降低诊断错误。模型使用神经加性模型进行预测，以证据为后盾，并给出个体化风险估计，特别针对诊断延迟和来自不完整鉴别的错误进行优化。 |
| [^79] | [Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283) | 这篇调查提供了LLM对话安全性的全面概述，涵盖了攻击、防御和评估三个关键方面，旨在提高对该主题的理解并促进进一步的研究。 |
| [^80] | [Retrieve to Explain: Evidence-driven Predictions with Language Models](https://arxiv.org/abs/2402.04068) | 检索以解释（R2E）是一种基于语言模型的检索方法，通过使用Shapley值确定证据的相对重要性，从而在黑盒模型中提供了可解释性，通过应用于药物靶点鉴定任务中，R2E模型在预测临床试验结果方面优于传统基因学方法。 |
| [^81] | [DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback](https://arxiv.org/abs/2311.10081) | 提出了一种名为DRESS的大型视觉语言模型，通过利用自然语言反馈来增强模型与人类之间的对齐和互动，解决了当前大型视觉-语言模型存在的对齐和多轮对话互动方面的两个关键问题 |
| [^82] | [Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation](https://arxiv.org/abs/2311.09684) | 本研究提出了自动提示优化（APO）框架，评估了不同提示对于大型语言模型在临床笔记生成中性能的影响，结果表明GPT4 APO在标准化提升质量方面表现出色，并强调了专家定制化对于内容质量的价值。 |
| [^83] | [LifeTox: Unveiling Implicit Toxicity in Life Advice](https://arxiv.org/abs/2311.09585) | LifeTox 数据集针对各种求助场景中的隐性毒性设计，RoBERTa 在该数据集上的表现不仅匹敌了大型语言模型的零次性能，甚至超过了，强调了 LifeTox 在解决隐性毒性中的有效性。 |
| [^84] | [Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing](https://arxiv.org/abs/2310.14855) | 大型语言模型在神经机器翻译中表现尚未达到最先进水平，提出使用LLM作为自动后编辑器(APE)的替代方法，同时探索了扩展到文档级翻译的可能性。 |
| [^85] | [Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement](https://arxiv.org/abs/2310.08559) | 对语言模型进行的系统研究揭示了它们在假设提出方面表现惊人，并且通过与一个（任务特定的）符号解释器相结合，能够系统地过滤可能性。 |
| [^86] | [Error Norm Truncation: Robust Training in the Presence of Data Noise for Text Generation Models](https://arxiv.org/abs/2310.00840) | 提出了错误范数截断（ENT）方法，通过考虑非目标标记的分布从而提供更准确的数据截断，证实在文本生成模型中应用ENT可以改善生成质量。 |
| [^87] | [BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models](https://arxiv.org/abs/2309.13345) | 提出了BAMBOO基准来全面评估大语言模型对长文本的建模能力，包含10个数据集从5个不同长文本理解任务中提取，涵盖了LLMs的核心能力和各个领域。 |
| [^88] | [Language Modeling Is Compression](https://arxiv.org/abs/2309.10668) | 大型语言模型被证明是强大的压缩器，压缩视角为扩展定律、标记化和上下文学习提供了新的见解。 |
| [^89] | [EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models](https://arxiv.org/abs/2308.07269) | EasyEdit提出了一种易于使用的知识编辑框架，针对大型语言模型的知识截断或谬误问题，支持各种最新的知识编辑方法，并可应用于多个知名的LLMs。 |
| [^90] | [HateModerate: Testing Hate Speech Detectors against Content Moderation Policies](https://arxiv.org/abs/2307.12418) | 该论文通过创建HateModerate数据集来测试自动内容审核员对内容政策的符合度，揭示了现有仇恨言论检测器在此方面存在的重大失败。 |
| [^91] | [VOLTA: Improving Generative Diversity by Variational Mutual Information Maximizing Autoencoder](https://arxiv.org/abs/2307.00852) | VOLTA通过Transformer与VAE框架的更有效连接，InfoGAN风格潜在编码以及支持离散输入，提升了生成多样性 |
| [^92] | [Radiology-GPT: A Large Language Model for Radiology](https://arxiv.org/abs/2306.08666) | Radiology-GPT是一个专门为放射学设计的大型语言模型，通过使用指导调整方法进行训练，在放射学诊断、研究和沟通方面展示出优越性能，为临床NLP的未来发展提供推动力。 |
| [^93] | [Semantic Role Labeling Guided Out-of-distribution Detection](https://arxiv.org/abs/2305.18026) | 本文提出了一种新的无监督领域外检测方法SRLOOD，通过语义角色标注引导的方式从句子的不同论点中分离、提取和学习局部特征表示，并结合全局特征表示，以边缘为基础进行对比损失。 |
| [^94] | [Shared and Private Information Learning in Multimodal Sentiment Analysis with Deep Modal Alignment and Self-supervised Multi-Task Learning](https://arxiv.org/abs/2305.08473) | 该论文提出了一种深度模态共享信息学习模块和基于自监督学习策略的标签生成模块，用于在多模态情感分析中学习共享和私有信息，可根据参数化调整不同模态之间的信息交换关系。 |
| [^95] | [LAGAN: Deep Semi-Supervised Linguistic-Anthropology Classification with Conditional Generative Adversarial Neural Network](https://arxiv.org/abs/2301.13853) | 使用条件生成对抗神经网络开发了一个LA-GAN算法，用于分类学生参与中的语言民族志特征，探讨少数民族教育中的学习方式和学习方法 |
| [^96] | [Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias.](http://arxiv.org/abs/2401.01989) | 这项研究通过测量位置偏见，重访了大语言模型中的零-shot 抽象摘要。研究结果揭示了模型不公平地优先考虑某些部分的信息，从而导致不可取的行为。对多个LLM模型和预训练抽象摘要模型进行的实验提供了关于零-shot 总结任务的模型性能和位置偏见的新见解和讨论。 |
| [^97] | [TiC-CLIP: Continual Training of CLIP Models.](http://arxiv.org/abs/2310.16226) | 该论文提出了用于训练视觉-语言模型的大规模时间连续 (TiC) 基准，使用这些基准评估了现有模型的时间鲁棒性，并展示了一种简单有效的排练方法来持续训练模型。 |
| [^98] | [Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation.](http://arxiv.org/abs/2310.07968) | 这项研究引入了零射交互个性化对象导航（ZIPON），通过使用大型语言模型（LLM）和用户反馈，解决了在未知环境中导航到个性化目标对象的问题。 |
| [^99] | [$\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis.](http://arxiv.org/abs/2310.03173) | $\mathcal{B}$-Coder是一种基于价值的深度强化学习方法，用于程序合成，旨在通过结合强化学习和大规模语言模型的能力，提高代码生成的准确性和执行能力。 |
| [^100] | [BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP.](http://arxiv.org/abs/2309.13173) | 本文对大型语言模型（LLMs）在孟加拉语自然语言处理中的潜力和缺陷进行了全面评估。结果显示，LLMs在孟加拉语自然语言处理任务中表现较差，需要进一步努力开发对低资源语言中LLMs的更好理解。 |
| [^101] | [Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions.](http://arxiv.org/abs/2309.07875) | 在训练大型语言模型遵循指令时，仅强调帮助性而不考虑安全性会导致模型产生有害内容。本研究发现，在训练LLaMA模型时添加少量安全示例可以显著提高其安全性，而不影响其能力和帮助性。然而，过度安全调优会使模型拒绝回应表面上类似于不安全提示的合理提示。 |
| [^102] | [T-MARS: Improving Visual Representations by Circumventing Text Feature Learning.](http://arxiv.org/abs/2307.03132) | T-MARS提出一种新的数据筛选方法，通过规避文本特征学习，改善了视觉表示的学习，解决了大型多模态数据集中存在的文本与图像重叠的问题。 |
| [^103] | [DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.](http://arxiv.org/abs/2306.15006) | 本研究提出了DNABERT-2，一个用于多种物种基因组的高效基础模型和基准。我们通过使用基于统计的数据压缩算法Byte Pair Encoding（BPE）替代传统的k-mer标记化，克服了k-mer标记化的计算和样本效率问题，并取得了重要进展。 |
| [^104] | [Generative Multimodal Entity Linking.](http://arxiv.org/abs/2306.12725) | 本文提出了 GEMEL 方法，使用大规模预训练的 LLMs 直接生成目标实体名称，仅调整了极少的模型参数即可实现最先进的 MEL 实验结果。 |
| [^105] | [LeTI: Learning to Generate from Textual Interactions.](http://arxiv.org/abs/2305.10314) | LeTI是一种使用自然语言指令、LM生成的程序和错误消息进行串联迭代微调的技术，可以用于代码生成任务，并且在自然发生的Python指令数据集上表现最先进。 |

# 详细

[^1]: LLMLingua-2: 高效且忠实的无任务Prompt压缩的数据精炼

    LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression

    [https://arxiv.org/abs/2403.12968](https://arxiv.org/abs/2403.12968)

    该论文提出了一种数据精炼的方法，通过从LLM中提取知识来实现Prompt的压缩，确保压缩后的提示保持对原始提示的忠实性。

    

    这篇论文关注于无任务的Prompt压缩，以提高泛化能力和效率。考虑到自然语言中的冗余性，现有方法通过根据从因果语言模型（如LLaMa-7B）获得的信息熵来删除token或词汇单位来压缩prompt。挑战在于信息熵可能是一个次优的压缩度量：(i)它仅利用单向上下文，可能无法捕获所有用于prompt压缩的关键信息；(ii)它与prompt压缩目标不一致。为了解决这些问题，我们提出了一种数据精炼过程，从LLM中获得知识以压缩prompt而不丢失关键信息，并同时引入了一个抽取式文本压缩数据集。我们将prompt压缩格式化为一个token分类问题，以确保压缩后的prompt与原始prompt的一致性。

    arXiv:2403.12968v1 Announce Type: new  Abstract: This paper focuses on task-agnostic prompt compression for better generalizability and efficiency. Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.   To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset. We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and 
    
[^2]: 负得正：用于视觉语言模型的统一双路径适配器

    Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models

    [https://arxiv.org/abs/2403.12964](https://arxiv.org/abs/2403.12964)

    本研究在视觉-语言模型的微调中引入了双学习概念，提出了DualAdapter方法，通过正面和负面两方面的双路径适配，同时进行补充正向选择和负向排除，从而提高了在下游任务中的整体识别准确性。

    

    最近，大规模预训练的视觉-语言模型（VLMs）展示了学习开放世界视觉表示方面的巨大潜力，并通过高效微调在各种下游任务中展现出卓越性能。在这项工作中，我们创新地将双学习概念引入微调VLMs中，即我们不仅学习图像是什么，还学习图像不是什么。基于这一概念，我们提出了一种新颖的DualAdapter方法，使VLMs能够从正面和负面两方面进行双路径适配，仅使用有限的注释样本。在推理阶段，我们的DualAdapter通过针对目标类别同时进行补充正向选择和负向排除，实现了统一预测，从而提高了VLMs在下游任务中的整体识别准确性。我们广泛的实验结果跨越15个数据集，验证了所提出的DualAda

    arXiv:2403.12964v1 Announce Type: cross  Abstract: Recently, large-scale pre-trained Vision-Language Models (VLMs) have demonstrated great potential in learning open-world visual representations, and exhibit remarkable performance across a wide range of downstream tasks through efficient fine-tuning. In this work, we innovatively introduce the concept of dual learning into fine-tuning VLMs, i.e., we not only learn what an image is, but also what an image isn't. Building on this concept, we introduce a novel DualAdapter approach to enable dual-path adaptation of VLMs from both positive and negative perspectives with only limited annotated samples. In the inference stage, our DualAdapter performs unified predictions by simultaneously conducting complementary positive selection and negative exclusion across target classes, thereby enhancing the overall recognition accuracy of VLMs in downstream tasks. Our extensive experimental results across 15 datasets validate that the proposed DualAda
    
[^3]: 数据的时效性：在大型语言模型中追踪知识截止日期

    Dated Data: Tracing Knowledge Cutoffs in Large Language Models

    [https://arxiv.org/abs/2403.12958](https://arxiv.org/abs/2403.12958)

    本文提出了在大型语言模型中追踪知识截止日期的概念，通过资源级别的时间对齐性估计有效截止日期，并发现这些截止日期通常与报道的不同。

    

    发布的大型语言模型通常配有声称的知识截止日期，即获取训练数据的日期。这些信息对于需要语言模型提供最新信息的应用至关重要。然而，这一说法只是表面现象：训练数据中的所有资源是否都具有相同的知识截止日期？模型对这些子集的展示知识是否与它们的截止日期密切相关？在这项工作中，我们定义了有效截止日期的概念。这与语言模型设计者报告的截止日期不同，分别适用于子资源和主题。我们提出了一种简单的方法，通过探测数据版本之间的时间对齐性来估计语言模型在资源级别的有效截止日期。通过这项分析，我们发现有效截止日期通常与报告的截止日期不同。为了了解这一观察结果的根本原因，我们进行了直接的大规模分析。

    arXiv:2403.12958v1 Announce Type: new  Abstract: Released Large Language Models (LLMs) are often paired with a claimed knowledge cutoff date, or the dates at which training data was gathered. Such information is crucial for applications where the LLM must provide up to date information. However, this statement only scratches the surface: do all resources in the training data share the same knowledge cutoff date? Does the model's demonstrated knowledge for these subsets closely align to their cutoff dates? In this work, we define the notion of an effective cutoff. This is distinct from the LLM designer reported cutoff and applies separately to sub-resources and topics. We propose a simple approach to estimate effective cutoffs on the resource-level temporal alignment of an LLM by probing across versions of the data. Using this analysis, we find that effective cutoffs often differ from reported cutoffs. To understand the root cause of this observation, we conduct a direct large-scale ana
    
[^4]: 使用大型语言模型自动提取雇佣法庭裁决中的信息

    Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models

    [https://arxiv.org/abs/2403.12936](https://arxiv.org/abs/2403.12936)

    本文研究了使用大型语言模型GPT-4自动从英国雇佣法庭案例中提取信息的应用，并通过手动验证确保了提取数据的准确性和相关性

    

    法庭记录和判决是法律知识的丰富资源，详细描述案件的复杂性以及司法决定背后的理由。从这些文件中提取关键信息提供了案件的简明概述，对于法律专家和公众都至关重要。随着大型语言模型（LLMs）的出现，自动信息提取变得越来越可行和高效。本文对GPT-4（一种大型语言模型）在从英国雇佣法庭（UKET）案例中自动提取信息的应用进行了全面研究。我们通过手动验证过程对GPT-4在提取关键信息方面的性能进行了细致评估，以确保提取的数据准确性和相关性。我们的研究围绕两个主要的信息提取任务展开：第一个任务涉及对对法律专家和公众都具有重要意义的八个关键方面进行通用的提取。

    arXiv:2403.12936v1 Announce Type: cross  Abstract: Court transcripts and judgments are rich repositories of legal knowledge, detailing the intricacies of cases and the rationale behind judicial decisions. The extraction of key information from these documents provides a concise overview of a case, crucial for both legal experts and the public. With the advent of large language models (LLMs), automatic information extraction has become increasingly feasible and efficient. This paper presents a comprehensive study on the application of GPT-4, a large language model, for automatic information extraction from UK Employment Tribunal (UKET) cases. We meticulously evaluated GPT-4's performance in extracting critical information with a manual verification process to ensure the accuracy and relevance of the extracted data. Our research is structured around two primary extraction tasks: the first involves a general extraction of eight key aspects that hold significance for both legal specialists
    
[^5]: 用大型语言模型支持能源政策研究

    Supporting Energy Policy Research with Large Language Models

    [https://arxiv.org/abs/2403.12924](https://arxiv.org/abs/2403.12924)

    本研究的创新贡献在于将决策树框架与大型语言模型集成，从而实现了自动提取法规的准确性达到85%至90%。

    

    美国可再生能源开发的最近增长伴随着可再生能源用地条例的同时激增。这些区位法规在规定对于实现低碳能源未来至关重要的风能和太阳能资源的摆放方面发挥着关键作用。在这种背景下，有效访问和管理用地法规数据变得至关重要。美国国家可再生能源实验室（NREL）最近推出了一个公开的风能和太阳能选址数据库，以满足这一需求。本文提出了一种利用大型语言模型（LLMs）自动提取这些选址法规的方法，使得该数据库能够在快速变化的能源政策格局中保持准确的最新信息。这项研究的一个新颖成果是将决策树框架与LLMs集成。我们的结果表明，这种方法的准确率达到了85%至90%。

    arXiv:2403.12924v1 Announce Type: new  Abstract: The recent growth in renewable energy development in the United States has been accompanied by a simultaneous surge in renewable energy siting ordinances. These zoning laws play a critical role in dictating the placement of wind and solar resources that are critical for achieving low-carbon energy futures. In this context, efficient access to and management of siting ordinance data becomes imperative. The National Renewable Energy Laboratory (NREL) recently introduced a public wind and solar siting database to fill this need. This paper presents a method for harnessing Large Language Models (LLMs) to automate the extraction of these siting ordinances from legal documents, enabling this database to maintain accurate up-to-date information in the rapidly changing energy policy landscape. A novel contribution of this research is the integration of a decision tree framework with LLMs. Our results show that this approach is 85 to 90% accurate
    
[^6]: 在低资源文本上进行预训练语言模型的可泛化和稳定微调

    Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts

    [https://arxiv.org/abs/2403.12918](https://arxiv.org/abs/2403.12918)

    提出了一种基于注意力引导的权重混合正则化方法，用于解决低资源文本上预训练语言模型微调时的稳定性和泛化能力问题

    

    预训练语言模型（PLMs）在自然语言处理（NLP）任务中取得了显著进展，但在低资源数据集上微调PLMs会面临诸如不稳定性和过拟合等重大挑战。先前的方法通过在下游任务上微调策略选择的子网络，同时保持其余权重固定为预训练权重来解决这些问题。然而，它们依赖于次优的子网络选择标准，导致次优解决方案。为了解决这些限制，我们提出了一种基于注意力引导的权重混合正则化方法，用于微调PLMs。我们的方法将每个网络权重表示为任务特定权重和预训练权重的混合，由可学习的注意力参数控制，提供对子网络选择的更精细控制。此外，我们在训练数据集的两个单独拆分上使用基于双层优化（BLO）的框架，进一步改进

    arXiv:2403.12918v1 Announce Type: cross  Abstract: Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting. Previous methods tackle these issues by finetuning a strategically chosen subnetwork on a downstream task, while keeping the remaining weights fixed to the pretrained weights. However, they rely on a suboptimal criteria for sub-network selection, leading to suboptimal solutions. To address these limitations, we propose a regularization method based on attention-guided weight mixup for finetuning PLMs. Our approach represents each network weight as a mixup of task-specific weight and pretrained weight, controlled by a learnable attention parameter, providing finer control over sub-network selection. Furthermore, we employ a bi-level optimization (BLO) based framework on two separate splits of the training dataset, improving ge
    
[^7]: 朝向可持续的GenAI：使用生成指令实现碳友好的大型语言模型推断

    Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference

    [https://arxiv.org/abs/2403.12900](https://arxiv.org/abs/2403.12900)

    本文提出了Sprout框架，通过引入生成指令的概念，平衡了生态可持续性和高质量生成结果之间的需求，实现了大型语言模型推断服务碳排放的显著减少。

    

    通过广泛应用的生成人工智能（GenAI）的迅速发展，引起了环境方面的重要关注，尤其是来自云和高性能计算基础设施的碳排放。本文提出了Sprout，一个创新的框架，旨在通过减少生成式大型语言模型（LLM）推断服务的碳足迹来解决这些问题。Sprout利用创新概念“生成指令”来引导自回归生成过程，从而增强碳效率。我们提出的方法精心平衡了对生态可持续性和高质量生成成果的需求。通过使用一个指令优化器来对用户提示进行生成指令的战略分配和一个原创的离线质量评估器，Sprout在实际评估中显著减少了40%以上的碳排放。

    arXiv:2403.12900v1 Announce Type: cross  Abstract: The rapid advancement of Generative Artificial Intelligence (GenAI) across diverse sectors raises significant environmental concerns, notably the carbon emissions from their cloud and high performance computing (HPC) infrastructure. This paper presents Sprout, an innovative framework designed to address these concerns by reducing the carbon footprint of generative Large Language Model (LLM) inference services. Sprout leverages the innovative concept of "generation directives" to guide the autoregressive generation process, thereby enhancing carbon efficiency. Our proposed method meticulously balances the need for ecological sustainability with the demand for high-quality generation outcomes. Employing a directive optimizer for the strategic assignment of generation directives to user prompts and an original offline quality evaluator, Sprout demonstrates a significant reduction in carbon emissions by over 40% in real-world evaluations u
    
[^8]: Agent-FLAN: 为了大型语言模型的有效代理调节设计数据和方法

    Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models

    [https://arxiv.org/abs/2403.12881](https://arxiv.org/abs/2403.12881)

    本文提出了Agent-FLAN，通过精心分解和重新设计训练语料库，使得Llama2-7B在各种代理评估中超过先前的最佳工作3.5％

    

    开源的大型语言模型（LLMs）在各种自然语言处理任务中取得了巨大成功，然而，与基于API的模型相比，它们在充当代理时仍然明显逊色。如何将代理能力整合到一般的LLMs中成为一个关键且紧迫的问题。本文首先提出三个关键观察：（1）当前的代理训练语料库与其预训练数据的分布明显不同，同时涉及遵循格式和代理推理；（2）LLMs在代理任务所需的能力上展现出不同的学习速度；以及（3）通过引入幻觉来提高代理能力的当前方法存在副作用。基于以上发现，我们提出Agent-FLAN，用于有效地为代理调试语言模型。通过对训练语料库进行精心的分解和重新设计，Agent-FLAN使得Llama2-7B在各种代理评估中超过先前的最佳工作3.5％。

    arXiv:2403.12881v1 Announce Type: new  Abstract: Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents. How to integrate agent ability into general LLMs becomes a crucial and urgent problem. This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents. Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5\% across various agent evaluation 
    
[^9]: 语言模型的认识论：语言模型是否具有整体知识？

    Epistemology of Language Models: Do Language Models Have Holistic Knowledge?

    [https://arxiv.org/abs/2403.12862](https://arxiv.org/abs/2403.12862)

    本文从认识论整体主义的角度研究了语言模型的固有知识，并通过创造科学推理数据集和三个任务评估了语言模型的认识论表现。

    

    这篇论文从认识论整体主义的角度探讨了语言模型中固有的知识。本文的目的是探讨LLMs是否表现出与认识论整体主义一致的特征。这些特征表明核心知识，如一般科学知识，每个都发挥着特定的作用，作为我们知识系统的基础并难以修改。为了评估与整体主义相关的这些特征，我们创建了一个科学推理数据集，并通过三个任务检验了语言模型的认识论：Abduction，Revision和Argument Generation。在绑架任务中，语言模型解释了情境，同时避免修改核心知识。然而，在其他任务中，语言模型被揭示出无法区分核心和周边知识，显示出与整体知识原则的不完全契合。

    arXiv:2403.12862v1 Announce Type: new  Abstract: This paper investigates the inherent knowledge in language models from the perspective of epistemological holism. The purpose of this paper is to explore whether LLMs exhibit characteristics consistent with epistemological holism. These characteristics suggest that core knowledge, such as general scientific knowledge, each plays a specific role, serving as the foundation of our knowledge system and being difficult to revise. To assess these traits related to holism, we created a scientific reasoning dataset and examined the epistemology of language models through three tasks: Abduction, Revision, and Argument Generation. In the abduction task, the language models explained situations while avoiding revising the core knowledge. However, in other tasks, the language models were revealed not to distinguish between core and peripheral knowledge, showing an incomplete alignment with holistic knowledge principles.
    
[^10]: 比较多语和单语微调语言模型之间的解释忠实度

    Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models

    [https://arxiv.org/abs/2403.12809](https://arxiv.org/abs/2403.12809)

    多语和单语语言模型之间的特征归因方法的忠实度存在差异，实验证明多语模型越大，FA相对于单语模型来说越不忠实。

    

    在许多实际的自然语言处理应用场景中，从业者不仅旨在最大化预测性能，还寻求对模型预测的忠实解释。特征归因方法给出的理由和重要性分布揭示了输入的不同部分如何影响预测。先前的研究探讨了不同因素如何影响忠实度，主要是在单语英语模型的背景下。另一方面，多语和单语模型之间的FA忠实度差异尚未得到探究。我们的大量实验证明，FA的忠实度在多语和单语模型之间有所变化。我们发现，多语模型越大，FA相对于其对应的单语模型来说越不忠实。我们的进一步分析显示，忠实度的差异是潜在的。

    arXiv:2403.12809v1 Announce Type: cross  Abstract: In many real natural language processing application scenarios, practitioners not only aim to maximize predictive performance but also seek faithful explanations for the model predictions. Rationales and importance distribution given by feature attribution methods (FAs) provide insights into how different parts of the input contribute to a prediction. Previous studies have explored how different factors affect faithfulness, mainly in the context of monolingual English models. On the other hand, the differences in FA faithfulness between multilingual and monolingual models have yet to be explored. Our extensive experiments, covering five languages and five popular FAs, show that FA faithfulness varies between multilingual and monolingual models. We find that the larger the multilingual model, the less faithful the FAs are compared to its counterpart monolingual models.Our further analysis shows that the faithfulness disparity is potenti
    
[^11]: 基于上下文的聚合实现情境道德价值对齐

    Contextual Moral Value Alignment Through Context-Based Aggregation

    [https://arxiv.org/abs/2403.12805](https://arxiv.org/abs/2403.12805)

    提出了一个基于情境聚合的情境道德价值对齐系统，相比现有技术，在与人类价值对齐方面表现更好。

    

    开发价值对齐的人工智能代理是人工智能领域一个复杂而持续挑战。特别是在大型语言模型（LLMs）领域，将多个独立训练的对话代理整合为一个统一系统，使其能够适应并与多个道德价值对齐，具有至关重要的意义。本文提出了一个基于情境聚合的情境道德价值对齐系统。在该系统中，聚合被定义为整合适合回复用户输入的LLM响应子集的过程，考虑了从用户输入中提取出的特征。所提出的系统在与人类价值对齐方面取得了比现有技术更好的结果。

    arXiv:2403.12805v1 Announce Type: new  Abstract: Developing value-aligned AI agents is a complex undertaking and an ongoing challenge in the field of AI. Specifically within the domain of Large Language Models (LLMs), the capability to consolidate multiple independently trained dialogue agents, each aligned with a distinct moral value, into a unified system that can adapt to and be aligned with multiple moral values is of paramount importance. In this paper, we propose a system that does contextual moral value alignment based on contextual aggregation. Here, aggregation is defined as the process of integrating a subset of LLM responses that are best suited to respond to a user input, taking into account features extracted from the user's input. The proposed system shows better results in term of alignment to human value compared to the state of the art.
    
[^12]: 探究BERT中的文本缩短策略：截断 vs 摘要

    Investigating Text Shortening Strategy in BERT: Truncation vs Summarization

    [https://arxiv.org/abs/2403.12799](https://arxiv.org/abs/2403.12799)

    本研究调查了文本分类任务中文档截断和摘要的表现，发现摘要在大多数情况下胜过截断方法的变体，最佳策略为取文档的开头。

    

    基于Transformer的模型的并行性以其输入最大长度为代价。一些研究提出了一些方法来克服这一限制，但其中未有报告摘要作为一种替代方法的有效性。本研究探讨了文档截断和摘要在文本分类任务中的表现。每种方法都得到了几种不同的变体的研究。本研究还探讨了它们的表现与全文表现之间的接近程度。我们使用了一个基于印尼新闻文章(IndoSum)的摘要任务数据集来进行分类测试。本研究显示出摘要胜过了大部分截断方法的变体，只输给了一个。本研究得到的最佳策略是取文档的开头。其次是抽取式摘要。本研究解释了结果的原因，引领着对于利用潜力进行进一步研究的开拓。

    arXiv:2403.12799v1 Announce Type: cross  Abstract: The parallelism of Transformer-based models comes at the cost of their input max-length. Some studies proposed methods to overcome this limitation, but none of them reported the effectiveness of summarization as an alternative. In this study, we investigate the performance of document truncation and summarization in text classification tasks. Each of the two was investigated with several variations. This study also investigated how close their performances are to the performance of full-text. We used a dataset of summarization tasks based on Indonesian news articles (IndoSum) to do classification tests. This study shows how the summaries outperform the majority of truncation method variations and lose to only one. The best strategy obtained in this study is taking the head of the document. The second is extractive summarization. This study explains what happened to the result, leading to further research in order to exploit the potenti
    
[^13]: 用于强大语言模型微调的自动化数据管理

    Automated Data Curation for Robust Language Model Fine-Tuning

    [https://arxiv.org/abs/2403.12776](https://arxiv.org/abs/2403.12776)

    介绍了一个自动化数据管理流水线CLEAR（基于置信度的LLM评估和纠正）用于指令微调数据集，可与任何LLM和微调程序一起使用。

    

    大型语言模型已成为序列到序列文本生成任务的事实标准，但对于专门的任务/领域，预训练的语言模型缺乏产生准确或格式良好响应的特定能力。监督微调通过训练模型在具有目标响应的示例提示数据集上进行专门化微调，但现实世界的数据往往存在噪声。虽然存在许多微调算法，但在这里，我们考虑了一种“以数据为中心的AI”视角下的语言模型微调，研究如何“系统地”筛选训练数据集以改进通过“任何”微调算法产生的语言模型。

    arXiv:2403.12776v1 Announce Type: new  Abstract: Large Language Models have become the de facto approach to sequence-to-sequence text generation tasks, but for specialized tasks/domains, a pretrained LLM lacks specific capabilities to produce accurate or well-formatted responses. Supervised fine-tuning specializes a LLM by training it on dataset of example prompts with target responses, but real-world data tends to be noisy. While many fine-tuning algorithms exist, here we consider a \emph{data-centric AI} perspective on LLM fine-tuning, studying how to \emph{systematically} curate the training dataset to improve the LLM produced via \emph{any} fine-tuning algorithm.   We introduce an automated data curation pipeline CLEAR (Confidence-based LLM Evaluation And Rectification) for instruction tuning datasets, that can be used with any LLM and fine-tuning procedure. CLEAR estimates which training data is low-quality and either filters or corrects it. Automatically identifying which data to
    
[^14]: NovelQA：用于长距离小说问答的基准

    NovelQA: A Benchmark for Long-Range Novel Question Answering

    [https://arxiv.org/abs/2403.12766](https://arxiv.org/abs/2403.12766)

    NovelQA是一个专门设计用于测试大型语言模型（LLMs）在长文本上的能力的基准，通过英文小说构建，提供了复杂性、长度和叙述连贯性的独特组合，可用于评估LLMs在深度文本理解方面的性能表现。

    

    大型语言模型（LLM）的快速发展引入了自然语言处理的新领域，特别是在理解和处理长文本信息方面。然而，由于当前基准的局限性，评估这些模型的长文本能力仍然是一个挑战。为了填补这一空白，我们引入了NovelQA，这是一个专门设计用于测试具有扩展文本的LLM能力的基准。NovelQA由英文小说构建，提供了复杂性、长度和叙述连贯性的独特组合，使其成为评估LLM中深度文本理解的理想工具。本文介绍了NovelQA的设计与构建，突出了其手动注释和多样的问题类型。我们在NovelQA上对长文本LLM进行评估，揭示了模型性能的重要见解，特别强调了它们在多跳推理、细节导向等方面所面临的挑战。

    arXiv:2403.12766v1 Announce Type: new  Abstract: The rapid advancement of Large Language Models (LLMs) has introduced a new frontier in natural language processing, particularly in understanding and processing long-context information. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark specifically designed to test the capabilities of LLMs with extended texts. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper presents the design and construction of NovelQA, highlighting its manual annotation, and diverse question types. Our evaluation of Long-context LLMs on NovelQA reveals significant insights into the models' performance, particularly emphasizing the challenges they face with multi-hop reasoning, detail-orien
    
[^15]: 辨识巴伐利亚方言数据中的命名实体

    Sebastian, Basti, Wastl?! Recognizing Named Entities in Bavarian Dialectal Data

    [https://arxiv.org/abs/2403.12749](https://arxiv.org/abs/2403.12749)

    介绍了德语第一个方言NER数据集BarNER，并展示了在巴伐利亚方言数据上的全面NER结果，表明从大型德语NER子数据集中汲取知识可以改进巴伐利亚数据的表现，而在巴伐利亚进行训练对标志性的德语数据也有所帮助。

    

    命名实体识别（NER）是从文本中提取关键信息的基本任务，但对方言的注释资源很少。本文介绍了德语第一个方言NER数据集BarNER，该数据集在巴伐利亚维基百科文章（bar-wiki）和推文（bar-tweet）上标注了161K个标记，使用了从德语CoNLL 2006和GermEval改编的模式。巴伐利亚方言在词汇分布、句法构造和实体信息方面与标准德语有所不同。我们在两个巴伐利亚和三个德国语料库上进行了领域内、领域间、顺序和联合实验，并首次全面展示了巴伐利亚的NER结果。从较大的德语NER（子）数据集中汲取知识显着改进了bar-wiki，稍微改进了bar-tweet。反之，首先在巴伐利亚进行训练对标志性的德语CoNLL 2006语料库有所帮助。此外，通过在巴伐利亚推文上使用黄金方言标签，我们评估

    arXiv:2403.12749v1 Announce Type: new  Abstract: Named Entity Recognition (NER) is a fundamental task to extract key information from texts, but annotated resources are scarce for dialects. This paper introduces the first dialectal NER dataset for German, BarNER, with 161K tokens annotated on Bavarian Wikipedia articles (bar-wiki) and tweets (bar-tweet), using a schema adapted from German CoNLL 2006 and GermEval. The Bavarian dialect differs from standard German in lexical distribution, syntactic construction, and entity information. We conduct in-domain, cross-domain, sequential, and joint experiments on two Bavarian and three German corpora and present the first comprehensive NER results on Bavarian. Incorporating knowledge from the larger German NER (sub-)datasets notably improves on bar-wiki and moderately on bar-tweet. Inversely, training first on Bavarian contributes slightly to the seminal German CoNLL 2006 corpus. Moreover, with gold dialect labels on Bavarian tweets, we assess
    
[^16]: 指导大型语言模型识别和忽略不相关条件

    Instructing Large Language Models to Identify and Ignore Irrelevant Conditions

    [https://arxiv.org/abs/2403.12744](https://arxiv.org/abs/2403.12744)

    提出了一种新方法 I$^3$C，指导大型语言模型识别和忽略不相关条件，并通过少样本推理加以增强。

    

    数学问题解决需要根据给定的问题描述生成推理路径，而这个描述通常包含不相关的条件。现有的“chain-of-thought”(CoT)提示方法引出了大型语言模型(LLMs)的多步推理能力来解决数学问题。然而，它们常常被不相关的条件严重困扰，导致准确性不高。在这篇论文中，我们提出了一种名为I$^3$C的新方法，指导LLMs识别和忽略不相关条件。它确定了一组与问题的语义关联较弱的不相关条件候选集。然后提示LLMs验证不相关条件。最后，通过关于相关和不相关条件的验证指导LLMs，避免混淆并提高推理路径。此外，我们建议选择(问题，推理路径)对作为示范，以增强I$^3$C的少样本推理。

    arXiv:2403.12744v1 Announce Type: new  Abstract: Math word problem (MWP) solving requires generating a reasoning path based on a given problem description that often contains irrelevant conditions. Existing chain-of-thought (CoT) prompting methods elicited multi-step reasoning abilities of large language models (LLMs) to solve MWPs. However, they were seriously confused by the irrelevant conditions, resulting in low accuracy. In this paper, we propose a novel approach named I$^3$C that instructs LLMs to identify and ignore irrelevant conditions. It identifies a set of irrelevant condition candidates that have a weak semantic relevance with the question. Then it prompts LLMs to verify the irrelevant conditions. Lastly it instructs the LLMs with the verification on relevant and irrelevant conditions to avoid confusion and improve reasoning paths. Moreover, we propose to select (problem, reasoning paths) pairs as demonstrations to enhance I$^3$C with few-shot reasoning. We develop I$^3$C-
    
[^17]: CLASSLA-web: 南斯拉夫语言的可比较网络语料库，注重语言和文体标注

    CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched with Linguistic and Genre Annotation

    [https://arxiv.org/abs/2403.12721](https://arxiv.org/abs/2403.12721)

    本文介绍了一个涵盖南斯拉夫地区官方语言的高度可比较的网络语料库集合，采用先进的技术进行语言和文体标注，进一步增强了其可比较性。

    

    本文介绍了一个涵盖斯洛文尼亚语、克罗地亚语、波斯尼亚语、黑山语、塞尔维亚语、马其顿语和保加利亚语高度可比较的网络语料库集合，从而覆盖了南斯拉夫语言空间所有官方语言的整个范围。这些语料库的总量为26亿个单词，来自2600万篇文档。语料库的可比较性由可比较的爬网设置和相同的爬网和后处理技术来确保。所有语料库都经过了最先进的CLASSLA-Stanza语言处理管道进行语言标注，并通过基于Transformer的多语种X-GENRE分类器增加了文档级别的文体信息，从而在语言标注和元数据丰富化的水平上进一步增强了可比较性。对结果语料库的文体聚焦分析显示了这七个语料库中各种文体的相当一致分布。

    arXiv:2403.12721v1 Announce Type: new  Abstract: This paper presents a collection of highly comparable web corpora of Slovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian, covering thereby the whole spectrum of official languages in the South Slavic language space. The collection of these corpora comprises a total of 13 billion tokens of texts from 26 million documents. The comparability of the corpora is ensured by a comparable crawling setup and the usage of identical crawling and post-processing technology. All the corpora were linguistically annotated with the state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and enriched with document-level genre information via the Transformer-based multilingual X-GENRE classifier, which further enhances comparability at the level of linguistic annotation and metadata enrichment. The genre-focused analysis of the resulting corpora shows a rather consistent distribution of genres throughout the seven corpora,
    
[^18]: 为加拿大空中旅行者赋权：一款关于加拿大空中旅客权利的聊天机器人

    Empowering Air Travelers: A Chatbot for Canadian Air Passenger Rights

    [https://arxiv.org/abs/2403.12678](https://arxiv.org/abs/2403.12678)

    提出了一款关于加拿大空中旅客权利的聊天机器人，帮助旅客理解和利用相关空中旅行法规，成功解决了用户输入复杂和准确回答问题的挑战

    

    加拿大航空旅行领域的航班延误、取消和其他关于旅客权利的问题有了显著增加。认识到这一需求，我们提出了一个聊天机器人来协助旅客并教育他们了解自己的权利。我们的系统将复杂的用户输入分解为简单的查询，用于检索详细空中旅行法规的文档集中的信息。从这些文档中提取最相关的段落，并提供原始文档和生成的查询的链接，使用户能够将信息细分并利用于其独特情况。该系统成功克服了两个主要挑战：理解复杂的用户输入，并提供准确答案，没有幻觉，这些答案可以供旅客依赖以做出明智决策。一项比较聊天机器人和谷歌搜索的用户研究展示了聊天机器人的实用性和易用性。

    arXiv:2403.12678v1 Announce Type: cross  Abstract: The Canadian air travel sector has seen a significant increase in flight delays, cancellations, and other issues concerning passenger rights. Recognizing this demand, we present a chatbot to assist passengers and educate them about their rights. Our system breaks a complex user input into simple queries which are used to retrieve information from a collection of documents detailing air travel regulations. The most relevant passages from these documents are presented along with links to the original documents and the generated queries, enabling users to dissect and leverage the information for their unique circumstances. The system successfully overcomes two predominant challenges: understanding complex user inputs, and delivering accurate answers, free of hallucinations, that passengers can rely on for making informed decisions. A user study comparing the chatbot to a Google search demonstrated the chatbot's usefulness and ease of use.
    
[^19]: 对韩语大型语言模型的语用能力评估

    Pragmatic Competence Evaluation of Large Language Models for Korean

    [https://arxiv.org/abs/2403.12675](https://arxiv.org/abs/2403.12675)

    该研究将大型语言模型的评估从对嵌入知识的基准拓展到了探索语用能力，结果显示在韩语环境下，GPT-4在传统和人工评估设置中表现出色，而HyperCLOVA X也在开放式问题评估中取得了不错的成绩。

    

    目前对大型语言模型（LLMs）的评估主要依赖于着重于测试其嵌入知识的基准，通过多项选择题（MCQs）来进行评估，这种格式非常适合自动评估。我们的研究将此评估拓展到探索LLM的语用能力--在先进的LLM出现之前鲜有研究，特别是在韩语环境下。我们采用两种不同的评估设置：传统的自动评估适配的MCQ格式，以及由人类专家评估的开放式问题（OEQs），用以检查LLM的叙事回应能力，而无需预先定义选项。我们的研究发现，GPT-4表现优异，在MCQ和OEQ设置中得分分别为81.11和85.69，而以韩语为优化目标的HyperCLOVA X在OEQ设置中表现出色，得分为81.56，与GPT-4相比，仅有4.13分的微小差距。

    arXiv:2403.12675v1 Announce Type: new  Abstract: The current evaluation of Large Language Models (LLMs) predominantly relies on benchmarks focusing on their embedded knowledge by testing through multiple-choice questions (MCQs), a format inherently suited for automated evaluation. Our study extends this evaluation to explore LLMs' pragmatic competence--a facet previously underexamined before the advent of sophisticated LLMs, specifically in the context of Korean. We employ two distinct evaluation setups: the conventional MCQ format, adapted for automatic evaluation, and Open-Ended Questions (OEQs), assessed by human experts, to examine LLMs' narrative response capabilities without predefined options. Our findings reveal that GPT-4 excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups, respectively, with HyperCLOVA X, an LLM optimized for Korean, closely following, especially in the OEQ setup, demonstrating a score of 81.56 with a marginal difference of 4.13 points compared to GPT-4
    
[^20]: 多维机器翻译评估：模型评估和韩语资源

    Multi-Dimensional Machine Translation Evaluation: Model Evaluation and Resource for Korean

    [https://arxiv.org/abs/2403.12666](https://arxiv.org/abs/2403.12666)

    本文提出了一个针对英韩语言对的1200句MQM评估基准，并通过使用最先进的语言模型，将MT评估转化为同时预测多个MQM分数的多任务问题。

    

    几乎所有手动或自动评估机器翻译质量的框架都使用单一数字来描述MT输出的质量。多维质量指标（MQM）框架是一个例外，它提供了一个细粒度的质量维度本体（如风格、流畅性、准确性和术语）。先前的研究已经证明了MQM注释的可行性，但据我们所知，目前没有计算模型可以预测新文本的MQM分数，这是因为缺乏资源。在本文中，我们通过(a)提供一个英韩语言对1200句MQM评估基准，以及(b)将MT评估重新构建为同时预测多个MQM分数的多任务问题，使用最先进的语言模型，分别在基于参考的MT评估设置和不需要参考的质量估计（QE）设置中。我们发现不需要参考的设置表现优于

    arXiv:2403.12666v1 Announce Type: new  Abstract: Almost all frameworks for the manual or automatic evaluation of machine translation characterize the quality of an MT output with a single number. An exception is the Multidimensional Quality Metrics (MQM) framework which offers a fine-grained ontology of quality dimensions for scoring (such as style, fluency, accuracy, and terminology). Previous studies have demonstrated the feasibility of MQM annotation but there are, to our knowledge, no computational models that predict MQM scores for novel texts, due to a lack of resources. In this paper, we address these shortcomings by (a) providing a 1200-sentence MQM evaluation benchmark for the language pair English-Korean and (b) reframing MT evaluation as the multi-task problem of simultaneously predicting several MQM scores using SOTA language models, both in a reference-based MT evaluation setup and a reference-free quality estimation (QE) setup. We find that reference-free setup outperform
    
[^21]: LHMKE：面向中文大型语言模型的大规模整体多学科知识评估基准

    LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models

    [https://arxiv.org/abs/2403.12601](https://arxiv.org/abs/2403.12601)

    LHMKE是一个面向中文大型语言模型的大规模、整体和多学科知识评估基准，旨在全面评估这些模型的知识获取能力。

    

    最近，中文大型语言模型（LLMs）在各种自然语言处理基准和实际应用中展示出令人印象深刻的能力。然而，目前用于全面评估这些LLMs的基准仍然不足，尤其在衡量LLMs捕捉的知识方面。为了解决这个问题，我们在本文中提出了LHMKE，一个面向中文大型语言模型的大规模、整体和多学科知识评估基准。 LHMKE旨在全面评估中文LLMs的知识获取能力。它包括了来自30个学科的75个任务的10,465个问题，涵盖从小学到专业水平。

    arXiv:2403.12601v1 Announce Type: new  Abstract: Chinese Large Language Models (LLMs) have recently demonstrated impressive capabilities across various NLP benchmarks and real-world applications. However, the existing benchmarks for comprehensively evaluating these LLMs are still insufficient, particularly in terms of measuring knowledge that LLMs capture. Current datasets collect questions from Chinese examinations across different subjects and educational levels to address this issue. Yet, these benchmarks primarily focus on objective questions such as multiple-choice questions, leading to a lack of diversity in question types. To tackle this problem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge Evaluation benchmark in this paper. LHMKE is designed to provide a comprehensive evaluation of the knowledge acquisition capabilities of Chinese LLMs. It encompasses 10,465 questions across 75 tasks covering 30 subjects, ranging from primary school to professional ce
    
[^22]: 基于图表的推理：将LLMs的能力转移到VLMs中

    Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs

    [https://arxiv.org/abs/2403.12596](https://arxiv.org/abs/2403.12596)

    提出了一种将大型语言模型（LLMs）的能力转移到视觉语言模型（VLMs）的技术，在多个任务上取得了最先进的性能。

    

    arXiv:2403.12596v1 公告类型：新 抽象：视觉语言模型（VLMs）在多模态任务上取得了越来越强的性能。然而，尤其是对于较小的VLMs，推理能力仍然有限，而大型语言模型（LLMs）的能力已经取得了许多改进。我们提出了一种将LLMs的能力转移到VLMs的技术。在最近引入的ChartQA上，我们的方法在应用于\citet{chen2023pali3}的PaLI3-5B VLM时获得了最先进的性能，同时也使PlotQA和FigureQA的性能大大提高。

    arXiv:2403.12596v1 Announce Type: new  Abstract: Vision-language models (VLMs) are achieving increasingly strong performance on multimodal tasks. However, reasoning capabilities remain limited particularly for smaller VLMs, while those of large-language models (LLMs) have seen numerous improvements. We propose a technique to transfer capabilities from LLMs to VLMs. On the recently introduced ChartQA, our method obtains state-of-the-art performance when applied on the PaLI3-5B VLM by \citet{chen2023pali3}, while also enabling much better performance on PlotQA and FigureQA.   We first improve the chart representation by continuing the pre-training stage using an improved version of the chart-to-table translation task by \citet{liu2023deplot}. We then propose constructing a 20x larger dataset than the original training set. To improve general reasoning capabilities and improve numerical operations, we synthesize reasoning traces using the table representation of charts. Lastly, our model 
    
[^23]: AlphaFin：检索增强的股票链框架在金融分析中的基准测试

    AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework

    [https://arxiv.org/abs/2403.12582](https://arxiv.org/abs/2403.12582)

    AlphaFin提出了解决股票趋势预测和金融问答任务中可解释性和实时信息集成困难的问题，通过发布AlphaFin数据集来应对这些挑战。

    

    金融分析任务主要包括两个关键领域：股票趋势预测和相应的金融问答。目前，机器学习和深度学习算法（ML＆DL）已被广泛应用于股票趋势预测，取得了显著进展。然而，这些方法无法提供预测的原因，缺乏可解释性和推理过程。此外，它们无法集成文字信息，如金融新闻或报告。同时，大型语言模型（LLM）具有显著的文本理解和生成能力。但由于金融训练数据集的稀缺性和与实时知识的有限集成，LLM仍然存在虚构现象，无法跟上最新信息。为了解决这些挑战，我们首先发布了AlphaFin数据集，将传统研究数据集、实时金融数据和手写链相结合。

    arXiv:2403.12582v1 Announce Type: new  Abstract: The task of financial analysis primarily encompasses two key areas: stock trend prediction and the corresponding financial question answering. Currently, machine learning and deep learning algorithms (ML&DL) have been widely applied for stock trend predictions, leading to significant progress. However, these methods fail to provide reasons for predictions, lacking interpretability and reasoning processes. Also, they can not integrate textual information such as financial news or reports. Meanwhile, large language models (LLMs) have remarkable textual understanding and generation ability. But due to the scarcity of financial training datasets and limited integration with real-time knowledge, LLMs still suffer from hallucinations and are unable to keep up with the latest information. To tackle these challenges, we first release AlphaFin datasets, combining traditional research datasets, real-time financial data, and handwritten chain-of-th
    
[^24]: 针对时间和内存受限的GPU服务上的大文本分类的Transformer简单技巧

    Simple Hack for Transformers against Heavy Long-Text Classification on a Time- and Memory-Limited GPU Service

    [https://arxiv.org/abs/2403.12563](https://arxiv.org/abs/2403.12563)

    通过在有限资源上逐步执行高效动态的HPO过程，提出了一种注重将Transformers模型适用于长文本分类任务的简单技巧。

    

    许多NLP研究人员依赖免费的计算服务，如Google Colab，来优化他们的Transformer模型，但由于该方法具有二次复杂性并需要更大的资源，这导致了在长文本分类中的超参数优化（HPO）存在局限性。在印尼，仅发现了少量关于使用Transformer进行长文本分类的研究。大多数仅使用少量数据，并且没有报告任何HPO。在这项研究中，我们使用18k篇新闻文章，研究了基于分词器输出长度建议使用哪些预训练模型。然后，我们比较了一些缩短和丰富序列的技巧，包括停用词、标点符号、低频词和重复词的去除。为了进行公平比较，我们提出并运行了一种高效动态的HPO过程，可以逐步在有限资源上进行，并且不需要长时间运行的优化库。利用最佳的方法...

    arXiv:2403.12563v1 Announce Type: cross  Abstract: Many NLP researchers rely on free computational services, such as Google Colab, to fine-tune their Transformer models, causing a limitation for hyperparameter optimization (HPO) in long-text classification due to the method having quadratic complexity and needing a bigger resource. In Indonesian, only a few works were found on long-text classification using Transformers. Most only use a small amount of data and do not report any HPO. In this study, using 18k news articles, we investigate which pretrained models are recommended to use based on the output length of the tokenizer. We then compare some hacks to shorten and enrich the sequences, which are the removals of stopwords, punctuation, low-frequency words, and recurring words. To get a fair comparison, we propose and run an efficient and dynamic HPO procedure that can be done gradually on a limited resource and does not require a long-running optimization library. Using the best ha
    
[^25]: 利用大型语言模型辅助分解学习的无词汇手语翻译方法

    Factorized Learning Assisted with Large Language Model for Gloss-free Sign Language Translation

    [https://arxiv.org/abs/2403.12556](https://arxiv.org/abs/2403.12556)

    提出了一种利用大型语言模型辅助分解学习的无词汇手语翻译方法，通过将训练过程分解为两个阶段，在视觉初始化阶段采用轻量级翻译模型预训练视觉编码器，解决了直接引入大型语言模型导致学习不足的问题。

    

    先前的手语翻译方法通过依赖术语标注实现了卓越的性能，然而，标记高质量的术语是一项劳动密集型的任务，限制了手语翻译的进一步发展。尽管一些方法通过联合训练视觉编码器和翻译网络实现了无术语的手语翻译，但这些努力仍然面临性能不佳和无效使用强大的大型语言模型（LLM）的问题。我们发现，直接引入LLM到手语翻译中会导致视觉表示学习不足，因为LLM主导了学习曲线。为了解决这些问题，我们提出了一种利用大型语言模型辅助分解学习的无词汇手语翻译方法（FLa-LLM）。具体而言，我们将训练过程分解为两个阶段。在视觉初始化阶段，我们在视觉编码器后使用轻量级翻译模型来预训练视觉编码器。

    arXiv:2403.12556v1 Announce Type: new  Abstract: Previous Sign Language Translation (SLT) methods achieve superior performance by relying on gloss annotations. However, labeling high-quality glosses is a labor-intensive task, which limits the further development of SLT. Although some approaches work towards gloss-free SLT through jointly training the visual encoder and translation network, these efforts still suffer from poor performance and inefficient use of the powerful Large Language Model (LLM). Most seriously, we find that directly introducing LLM into SLT will lead to insufficient learning of visual representations as LLM dominates the learning curve. To address these problems, we propose Factorized Learning assisted with Large Language Model (FLa-LLM) for gloss-free SLT. Concretely, we factorize the training process into two stages. In the visual initialing stage, we employ a lightweight translation model after the visual encoder to pre-train the visual encoder. In the LLM fine
    
[^26]: 基于提示的图模型用于联合自由事件提取和事件模式归纳

    Prompt-based Graph Model for Joint Liberal Event Extraction and Event Schema Induction

    [https://arxiv.org/abs/2403.12526](https://arxiv.org/abs/2403.12526)

    提出了一种基于提示的图模型用于自由事件提取，旨在同时提取事件并发现事件模式，避免了对外部语言知识库的严重依赖和大量手动规则开发的复杂工作。

    

    事件是语音和文本的必要组成部分，描述了实体状态的变化。事件提取任务旨在识别和分类事件，并根据事件模式找到参与者。手动预定义的事件模式覆盖范围有限，难以跨领域迁移。因此，研究人员提出自由事件提取（LEE），旨在同时提取事件并发现事件模式。然而，现有的LEE模型严重依赖外部语言知识库，并需要手动开发大量规则进行噪声去除和知识对齐，这是复杂和繁重的。为此，我们提出了一种基于提示的图模型用于自由事件提取（PGLEE）。具体来说，我们使用基于提示的模型来获取候选触发器和参数，然后构建异构事件图以编码事件内部和事件之间的结构。

    arXiv:2403.12526v1 Announce Type: new  Abstract: Events are essential components of speech and texts, describing the changes in the state of entities. The event extraction task aims to identify and classify events and find their participants according to event schemas. Manually predefined event schemas have limited coverage and are hard to migrate across domains. Therefore, the researchers propose Liberal Event Extraction (LEE), which aims to extract events and discover event schemas simultaneously. However, existing LEE models rely heavily on external language knowledge bases and require the manual development of numerous rules for noise removal and knowledge alignment, which is complex and laborious. To this end, we propose a Prompt-based Graph Model for Liberal Event Extraction (PGLEE). Specifically, we use a prompt-based model to obtain candidate triggers and arguments, and then build heterogeneous event graphs to encode the structures within and between events. Experimental result
    
[^27]: GraphERE: 基于图增强事件嵌入的多事件关系提取

    GraphERE: Jointly Multiple Event-Event Relation Extraction via Graph-Enhanced Event Embeddings

    [https://arxiv.org/abs/2403.12523](https://arxiv.org/abs/2403.12523)

    该论文提出了一种名为GraphERE的多事件关系提取框架，通过使用图增强事件嵌入，扩展了事件嵌入的事件参数和结构特征，从而解决了事件触发器嵌入的局限以及关系之间互连被忽略的问题。

    

    事件描述实体的状态变化。在文档中，多个事件通过各种关系相互连接（例如，共指、时间、因果和子事件）。因此，通过事件之间的关系提取（ERE）获取事件之间的连接对于理解自然语言至关重要。当前ERE工作中存在两个主要问题：a. 仅使用事件触发器的嵌入来表示事件特征，忽略事件参数（例如，时间、地点、人物等）及其在事件内的结构。b. 关系之间的互连（例如，时间和因果关系通常会相互影响）被忽略。为解决上述问题，本文提出了一种名为GraphERE基于图增强事件嵌入的多重ERE框架。首先，我们利用静态AMR图和IE图丰富事件嵌入的事件参数和结构特征；然后，为了联合...

    arXiv:2403.12523v1 Announce Type: cross  Abstract: Events describe the state changes of entities. In a document, multiple events are connected by various relations (e.g., Coreference, Temporal, Causal, and Subevent). Therefore, obtaining the connections between events through Event-Event Relation Extraction (ERE) is critical to understand natural language. There are two main problems in the current ERE works: a. Only embeddings of the event triggers are used for event feature representation, ignoring event arguments (e.g., time, place, person, etc.) and their structure within the event. b. The interconnection between relations (e.g., temporal and causal relations usually interact with each other ) is ignored. To solve the above problems, this paper proposes a jointly multiple ERE framework called GraphERE based on Graph-enhanced Event Embeddings. First, we enrich the event embeddings with event argument and structure features by using static AMR graphs and IE graphs; Then, to jointly e
    
[^28]: 用于一致性感知对话系统的大规模模型生成的矛盾响应集合

    A Large Collection of Model-generated Contradictory Responses for Consistency-aware Dialogue Systems

    [https://arxiv.org/abs/2403.12500](https://arxiv.org/abs/2403.12500)

    本文首次构建了大规模的模型生成矛盾响应数据集，并通过深入分析获得了宝贵的矛盾特征洞察。

    

    缓解生成矛盾响应在对话响应生成中构成了一个重大挑战。可用矛盾响应数据的质量和数量在抑制这些矛盾方面发挥着至关重要的作用，带来两个重要的好处。首先，访问大量的矛盾数据可以全面检验它们的特征。其次，基于数据的方法可以通过大规模矛盾数据的训练而得到增强。然而，迄今为止尚未尝试构建一个广泛的模型生成矛盾响应集合。本文首次构建了一个大规模的响应生成模型矛盾数据集。然后，我们通过对收集到的响应进行深入分析，获得了有价值的对模型生成矛盾的特征的洞察。最后，我们还展示了这一数据集如何显著增强了p

    arXiv:2403.12500v1 Announce Type: new  Abstract: Mitigating the generation of contradictory responses poses a substantial challenge in dialogue response generation. The quality and quantity of available contradictory response data play a vital role in suppressing these contradictions, offering two significant benefits. First, having access to large contradiction data enables a comprehensive examination of their characteristics. Second, data-driven methods to mitigate contradictions may be enhanced with large-scale contradiction data for training. Nevertheless, no attempt has been made to build an extensive collection of model-generated contradictory responses. In this paper, we build a large dataset of response generation models' contradictions for the first time. Then, we acquire valuable insights into the characteristics of model-generated contradictions through an extensive analysis of the collected responses. Lastly, we also demonstrate how this dataset substantially enhances the p
    
[^29]: 具身LLM代理在组织团队中学会合作

    Embodied LLM Agents Learn to Cooperate in Organized Teams

    [https://arxiv.org/abs/2403.12482](https://arxiv.org/abs/2403.12482)

    本文介绍了一个框架，将即时性组织结构强加在LLM代理上，以促进多代理系统内的合作，在具身LLM代理和人-代理合作实验中发现指定领导对团队效率的影响，揭示了LLM代理的领导素质和自发合作行为。

    

    大型语言模型（LLMs）已成为推理、规划和决策的重要工具，利用其丰富的世界知识和语言相关任务的熟练度。LLMs因此在多代理系统内自然语言交互方面具有巨大潜力，以促进合作。然而，LLM代理往往会过度报告并遵从任何指令，这可能导致多代理合作中的信息冗余和混乱。受人类组织的启发，本文引入了一个框架，将即时性组织结构强加在LLM代理上，以缓解这些问题。通过一系列具身LLM代理和人-代理合作的实验，我们的结果突出了指定领导对团队效率的影响，揭示了LLM代理展示的领导素质和他们的自发合作行为。此外，我们利用LLMs的潜力

    arXiv:2403.12482v1 Announce Type: new  Abstract: Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs t
    
[^30]: 更多背景信息何时有助于识别讽刺？

    When Do "More Contexts" Help with Sarcasm Recognition?

    [https://arxiv.org/abs/2403.12469](https://arxiv.org/abs/2403.12469)

    本研究探讨了将更多上下文信息整合到模型中能够带来的对于讽刺识别的改进效果。

    

    讽刺识别具有挑战性，因为它需要理解文本的真实意图，这与字面意义相反或不同。先前的研究通过开发一系列提供更丰富$contexts$（例如情感或文化细微差别）给模型的方法来解决这一挑战。虽然已经单独证明了这些方法的有效性，但没有研究系统地评估它们的集体效果。因此，额外背景信息能够提高对讽刺的识别程度仍然不清楚。本文探讨了集成现有方法所带来的改进，通过将更多背景信息整合到模型中。为此，我们制定了一个框架，可以集成多个上下文线索并测试不同方法。在对三个讽刺识别基准测试的四种方法进行评估时，我们实现了现有的最先进性能，并演示了逐步添加更多co有益之处。

    arXiv:2403.12469v1 Announce Type: new  Abstract: Sarcasm recognition is challenging because it needs an understanding of the true intention, which is opposite to or different from the literal meaning of the words. Prior work has addressed this challenge by developing a series of methods that provide richer $contexts$, e.g., sentiment or cultural nuances, to models. While shown to be effective individually, no study has systematically evaluated their collective effectiveness. As a result, it remains unclear to what extent additional contexts can improve sarcasm recognition. In this work, we explore the improvements that existing methods bring by incorporating more contexts into a model. To this end, we develop a framework where we can integrate multiple contextual cues and test different approaches. In evaluation with four approaches on three sarcasm recognition benchmarks, we achieve existing state-of-the-art performances and also demonstrate the benefits of sequentially adding more co
    
[^31]: CrossTune: 带有标签增强的黑盒少样本分类

    CrossTune: Black-Box Few-Shot Classification with Label Enhancement

    [https://arxiv.org/abs/2403.12468](https://arxiv.org/abs/2403.12468)

    CrossTune是一种带有标签增强的黑盒少样本分类网络，通过模拟输入文本序列与任务标签的语义相关性来提高泛化能力。

    

    训练或微调大规模语言模型（LLMs）需要大量计算资源，鼓励最近的努力探索对下游任务进行参数高效适应的方法。一种方法是将这些模型视为黑盒，并使用前向传递（推理API）与它们进行交互。目前的研究集中在使用无梯度提示优化将这些黑盒模型适应到下游任务上，但这通常涉及一个昂贵的搜索特定任务提示的过程。因此，我们受到动机去研究无需搜索提示的黑盒语言模型适应。具体来说，我们引入了一个名为CrossTune的标签增强跨注意力网络，它模拟了输入文本序列与特定任务标签描述之间的语义相关性。其有效性在少样本文本分类的背景下得到检验。为了改进CrossTune的泛化能力，我们利用ChatGPT生成

    arXiv:2403.12468v1 Announce Type: new  Abstract: Training or finetuning large-scale language models (LLMs) requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks. One approach is to treat these models as black boxes and use forward passes (Inference APIs) to interact with them. Current research focuses on adapting these black-box models to downstream tasks using gradient-free prompt optimization, but this often involves an expensive process of searching task-specific prompts. Therefore, we are motivated to study black-box language model adaptation without prompt search. Specifically, we introduce a label-enhanced cross-attention network called CrossTune, which models the semantic relatedness between the input text sequence and task-specific label descriptions. Its effectiveness is examined in the context of few-shot text classification. To improve the generalization of CrossTune, we utilize ChatGPT to generate
    
[^32]: 针对放射学的眼控引导多模态对齐框架

    Eye-gaze Guided Multi-modal Alignment Framework for Radiology

    [https://arxiv.org/abs/2403.12416](https://arxiv.org/abs/2403.12416)

    提出一种利用眼控数据的多模态对齐框架，可降低对手动注释的依赖

    

    在多模态框架中，跨模态特征的对齐是一个重要挑战。现有的方法强调全局或局部模态之间的对齐，利用大量数据集。然而，这种自底向上的方法在放射学中常常缺乏可解释性。我们的工作提出了一种新的方法，通过使用放射科医生在诊断评估过程中同步收集的眼控数据，将胸部X线自然地与诊断文本相关联，以更好地对齐图像和文本特征，旨在减少对手动注释的依赖。

    arXiv:2403.12416v1 Announce Type: cross  Abstract: In multi-modal frameworks, the alignment of cross-modal features presents a significant challenge. The predominant approach in multi-modal pre-training emphasizes either global or local alignment between modalities, utilizing extensive datasets. This bottom-up driven method often suffers from a lack of interpretability, a critical concern in radiology. Previous studies have integrated high-level labels in medical images or text, but these still rely on manual annotation, a costly and labor-intensive process. Our work introduces a novel approach by using eye-gaze data, collected synchronously by radiologists during diagnostic evaluations. This data, indicating radiologists' focus areas, naturally links chest X-rays to diagnostic texts. We propose the Eye-gaze Guided Multi-modal Alignment (EGMA) framework to harness eye-gaze data for better alignment of image and text features, aiming to reduce reliance on manual annotations and thus cut
    
[^33]: 来自指令的第三方语言模型性能预测

    Third-Party Language Model Performance Prediction from Instruction

    [https://arxiv.org/abs/2403.12413](https://arxiv.org/abs/2403.12413)

    提出了一种第三方性能预测框架，用于预测评估指令跟随系统在任务上表现的度量标准，从而解决用户无法了解系统可靠性的问题。

    

    基于语言模型的指令跟随系统最近在许多基准任务上表现出越来越好的性能，展现出适应各种指令的能力。然而，这些系统通常未设计成透明地展示其局限性；用户可能会轻易用指令提示一个模型，却对其响应是否应该准确一无所知，甚至不知道系统是否有能力执行任务。我们提出了一个第三方性能预测框架，其中单独训练一个模型来预测在推断时仅假设访问其输入和输出的情况下评估任务上的指令跟随系统时产生的度量标准。我们使用多种开放和封闭的指令跟随模型以及多个性能预测器来进行这项分析，并检验各种因素的影响，如模型大小、训练任务数量等。

    arXiv:2403.12413v1 Announce Type: new  Abstract: Language model-based instruction-following systems have lately shown increasing performance on many benchmark tasks, demonstrating the capability of adapting to a broad variety of instructions. However, such systems are often not designed to be transparent about their limitations; a user may easily prompt a model with an instruction without any idea of whether the responses should be expected to be accurate, or if the system is even capable of performing the task. We propose a third party performance prediction framework, where a separate model is trained to predict the metric resulting from evaluating an instruction-following system on a task while assuming access only to its inputs and outputs at inference time. We perform this analysis with a variety of both open and closed instruction-following models as well as multiple performance predictors, and examine the effect of various factors such as model size, number of training tasks, an
    
[^34]: MSLM-S2ST：一种用于无文本语音到语音翻译的多任务语音语言模型，保留说话者风格

    MSLM-S2ST: A Multitask Speech Language Model for Textless Speech-to-Speech Translation with Speaker Style Preservation

    [https://arxiv.org/abs/2403.12408](https://arxiv.org/abs/2403.12408)

    提出了一种无文本训练数据的多任务语音语言模型，支持多语言S2ST并保留说话者风格

    

    近年来，针对语音到语音翻译（S2ST）的研究兴趣和进展逐渐增加，这项工作提出了Multitask Speech Language Model (MSLM)，这是一个仅具有解码器的多任务语音语言模型，在多任务设置中进行训练。在不依赖文本训练数据的情况下，我们的模型能够支持带有保留说话者风格的多语言S2ST。

    arXiv:2403.12408v1 Announce Type: new  Abstract: There have been emerging research interest and advances in speech-to-speech translation (S2ST), translating utterances from one language to another. This work proposes Multitask Speech Language Model (MSLM), which is a decoder-only speech language model trained in a multitask setting. Without reliance on text training data, our model is able to support multilingual S2ST with speaker style preserved.
    
[^35]: 通过多语言提示翻译实现自然语言推理的跨语言转移

    Cross-Lingual Transfer for Natural Language Inference via Multilingual Prompt Translator

    [https://arxiv.org/abs/2403.12407](https://arxiv.org/abs/2403.12407)

    提出了一种名为多语言提示翻译（MPT）的框架，通过引入多语言提示翻译器实现在低资源情境下将软提示从源语言有效转移到目标语言，在少样本设置下表现优越

    

    基于多语言预训练模型，跨语言转移与提示学习表现出很高的有效性，其中在源语言中学习的软提示被转移到目标语言用于下游任务，尤其是在低资源情境下。为了有效地转移软提示，我们提出了一个新颖的框架，多语言提示翻译（MPT），引入了一个多语言提示翻译器来适当地处理提示中嵌入的关键知识，从而在保留任务知识的同时改变语言知识。具体来说，我们首先在源语言中训练提示，然后利用翻译器将其翻译成目标提示。此外，我们扩展了一个外部语料库作为辅助数据，对预测答案概率进行对齐任务，以转换语言知识，从而为目标提示提供多语言知识。在XNLI的少样本设置中，MPT表现出比基线方法更优越

    arXiv:2403.12407v1 Announce Type: new  Abstract: Based on multilingual pre-trained models, cross-lingual transfer with prompt learning has shown promising effectiveness, where soft prompt learned in a source language is transferred to target languages for downstream tasks, particularly in the low-resource scenario. To efficiently transfer soft prompt, we propose a novel framework, Multilingual Prompt Translator (MPT), where a multilingual prompt translator is introduced to properly process crucial knowledge embedded in prompt by changing language knowledge while retaining task knowledge. Concretely, we first train prompt in source language and employ translator to translate it into target prompt. Besides, we extend an external corpus as auxiliary data, on which an alignment task for predicted answer probability is designed to convert language knowledge, thereby equipping target prompt with multilingual knowledge. In few-shot settings on XNLI, MPT demonstrates superiority over baselines
    
[^36]: 利用大型语言模型提取的原因生成可解释的仇恨言论检测方法

    Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales

    [https://arxiv.org/abs/2403.12403](https://arxiv.org/abs/2403.12403)

    利用大型语言模型提取原因特征训练仇恨言论分类器，实现可解释的检测方法

    

    尽管社交媒体平台是用户进行人际讨论和表达观点的重要场所，但社交媒体提供的外立面和匿名性可能导致用户发布仇恨言论和冒犯性内容。鉴于这些平台的庞大规模，自动识别和标记仇恨言论的需求日益迫切。尽管存在几种仇恨言论检测方法，但大多数黑盒方法在设计上不具有可解释性或可解释性。为解决解释性不足，本文提出使用最先进的大型语言模型（LLM）从输入文本中提取原因特征，训练基础仇恨言论分类器，从而通过设计实现忠实的可解释性。我们的框架有效地结合了LLM的文本理解能力和最先进仇恨言论分类器的判别能力，使这些分类器

    arXiv:2403.12403v1 Announce Type: cross  Abstract: Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew hate speech and offensive content. Given the massive scale of such platforms, there arises a need to automatically identify and flag instances of hate speech. Although several hate speech detection methods exist, most of these black-box methods are not interpretable or explainable by design. To address the lack of interpretability, in this paper, we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design. Our framework effectively combines the textual understanding capabilities of LLMs and the discriminative power of state-of-the-art hate speech classifiers to make these classifi
    
[^37]: 一项关于提示条件语音合成的语言模型的实证研究

    An Empirical Study of Speech Language Models for Prompt-Conditioned Speech Synthesis

    [https://arxiv.org/abs/2403.12402](https://arxiv.org/abs/2403.12402)

    对自回归和非自回归语音LM进行实证研究，揭示了提示设计和内容语义单元的重要性，并发现提示的异质性和非平稳性会影响音频质量，同时内容也会影响合成音频的说话风格。

    

    语音语言模型(LMs)通过上下文学习对高质量语音合成具有潜在价值。典型的语音LM将离散语义单元作为内容，短语言作为提示，并合成保留内容语义但模仿提示风格的语音。然而，对于合成音频如何受提示和内容控制并无系统性理解。在本研究中，我们对广泛使用的自回归(AR)和非自回归(NAR)语音LM进行实证研究，并深入探讨提示设计和内容语义单元。我们的分析表明，异质和非平稳提示会降低音频质量，与之前的研究相反，较长提示并不总是导致更好的合成。此外，我们发现合成音频的说话风格受内容影响，而不仅仅受提示影响。我们进一步展示语义单元携带的信息有助于生成高质量的音频。

    arXiv:2403.12402v1 Announce Type: new  Abstract: Speech language models (LMs) are promising for high-quality speech synthesis through in-context learning. A typical speech LM takes discrete semantic units as content and a short utterance as prompt, and synthesizes speech which preserves the content's semantics but mimics the prompt's style. However, there is no systematic understanding on how the synthesized audio is controlled by the prompt and content. In this work, we conduct an empirical study of the widely used autoregressive (AR) and non-autoregressive (NAR) speech LMs and provide insights into the prompt design and content semantic units. Our analysis reveals that heterogeneous and nonstationary prompts hurt the audio quality in contrast to the previous finding that longer prompts always lead to better synthesis. Moreover, we find that the speaker style of the synthesized audio is also affected by the content in addition to the prompt. We further show that semantic units carry r
    
[^38]: Dr3：要求大型语言模型在开放领域多跳问答中不给出题外答案

    Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open Domain Multi-Hop Question Answering

    [https://arxiv.org/abs/2403.12393](https://arxiv.org/abs/2403.12393)

    提出Dr3机制，通过辨别器判断大型语言模型生成的答案是否题外来解决开放领域多跳问答中题外答案的问题

    

    开放领域多跳问答（ODMHQA）通过对来自外部知识源的信息进行多步推理来回答复杂问题，在自然语言处理（NLP）中扮演着关键角色。最近，大型语言模型（LLMs）在解决ODMHQA方面表现出色，这归因于它们的规划、推理和工具利用等能力。然而，LLMs在尝试解决ODMHQA时可能会生成题外答案，即生成的答案与原始问题无关。这一题外答案的问题约占错误答案的三分之一，但尽管其重要性尚未得到充分探讨。为了缓解这一问题，我们提出了辨别->重组->解决->重新分解（Dr3）机制。具体而言，辨别器利用LLMs的固有能力判断生成的答案是否题外。

    arXiv:2403.12393v1 Announce Type: new  Abstract: Open Domain Multi-Hop Question Answering (ODMHQA) plays a crucial role in Natural Language Processing (NLP) by aiming to answer complex questions through multi-step reasoning over retrieved information from external knowledge sources. Recently, Large Language Models (LLMs) have demonstrated remarkable performance in solving ODMHQA owing to their capabilities including planning, reasoning, and utilizing tools. However, LLMs may generate off-topic answers when attempting to solve ODMHQA, namely the generated answers are irrelevant to the original questions. This issue of off-topic answers accounts for approximately one-third of incorrect answers, yet remains underexplored despite its significance. To alleviate this issue, we propose the Discriminate->Re-Compose->Re- Solve->Re-Decompose (Dr3) mechanism. Specifically, the Discriminator leverages the intrinsic capabilities of LLMs to judge whether the generated answers are off-topic. In cases
    
[^39]: AraPoemBERT：一种用于阿拉伯诗歌分析的预训练语言模型

    AraPoemBERT: A Pretrained Language Model for Arabic Poetry Analysis

    [https://arxiv.org/abs/2403.12392](https://arxiv.org/abs/2403.12392)

    AraPoemBERT 是一种专门针对阿拉伯诗歌文本进行预训练的语言模型，在阿拉伯诗歌相关的NLP任务中表现出色，取得了两项新颖任务中的前所未有的高准确率。

    

    阿拉伯诗歌以其丰富的语言特征和深刻的文化意义，为自然语言处理（NLP）领域提出了独特挑战。其结构和背景的复杂性需要先进的计算模型进行准确分析。本文介绍了AraPoemBERT，这是一种专门在阿拉伯诗歌文本上预训练的阿拉伯语言模型。为了展示所提出模型的有效性，我们将AraPoemBERT与5种不同的阿拉伯语言模型在与阿拉伯诗歌相关的各种NLP任务上进行了比较。这个新模型在绝大多数下游任务中表现优异，并取得了最新的技术成果。AraPoemBERT在三项新颖任务中的两项中取得了前所未有的准确率：诗人性别分类（99.34\%的准确率）和诗歌节律分类（97.79\%的准确率）。此外，模型在诗歌押韵分类（97.73\%的准确率）中也取得了准确得分。

    arXiv:2403.12392v1 Announce Type: cross  Abstract: Arabic poetry, with its rich linguistic features and profound cultural significance, presents a unique challenge to the Natural Language Processing (NLP) field. The complexity of its structure and context necessitates advanced computational models for accurate analysis. In this paper, we introduce AraPoemBERT, an Arabic language model pretrained exclusively on Arabic poetry text. To demonstrate the effectiveness of the proposed model, we compared AraPoemBERT with 5 different Arabic language models on various NLP tasks related to Arabic poetry. The new model outperformed all other models and achieved state-of-the-art results in most of the downstream tasks. AraPoemBERT achieved unprecedented accuracy in two out of three novel tasks: poet's gender classification (99.34\% accuracy), and poetry sub-meter classification (97.79\% accuracy). In addition, the model achieved an accuracy score in poems' rhyme classification (97.73\% accuracy) wh
    
[^40]: 与联合学习不相上下的分阶段生物医学事件提取

    Pipelined Biomedical Event Extraction Rivaling Joint Learning

    [https://arxiv.org/abs/2403.12386](https://arxiv.org/abs/2403.12386)

    本文提出了一种基于BERT预训练模型的n元关系提取方法，用于改进Binding事件的性能，从而提高分阶段生物医学事件提取的整体性能。

    

    生物医学事件提取是一项信息提取任务，旨在从生物医学文本中获取事件，其目标包括事件类型、触发词以及事件中涉及的各个参数。本文提出了一种基于BERT预训练模型的n元关系提取方法，用于构建Binding事件，以捕获事件背景及参与者的语义信息。实验结果表明，我们的方法在BioNLP共享任务的GE11和GE13语料库上取得了令人满意的结果，分别为63.14%和59.40%的F1分数。

    arXiv:2403.12386v1 Announce Type: cross  Abstract: Biomedical event extraction is an information extraction task to obtain events from biomedical text, whose targets include the type, the trigger, and the respective arguments involved in an event. Traditional biomedical event extraction usually adopts a pipelined approach, which contains trigger identification, argument role recognition, and finally event construction either using specific rules or by machine learning. In this paper, we propose an n-ary relation extraction method based on the BERT pre-training model to construct Binding events, in order to capture the semantic information about an event's context and its participants. The experimental results show that our method achieves promising results on the GE11 and GE13 corpora of the BioNLP shared task with F1 scores of 63.14% and 59.40%, respectively. It demonstrates that by significantly improving theperformance of Binding events, the overall performance of the pipelined even
    
[^41]: 通过Prompt-tuning改进使用大型语言模型提取健康社会决定因素的泛化能力

    Improving Generalizability of Extracting Social Determinants of Health Using Large Language Models through Prompt-tuning

    [https://arxiv.org/abs/2403.12374](https://arxiv.org/abs/2403.12374)

    该研究提出了一种通过软提示式学习架构来改进使用大型语言模型提取健康社会决定因素的泛化能力的方法，并发现decoder-only的LLMs在跨领域应用中通过Prompt-tuning取得了更好的性能。

    

    自然语言处理（NLP）中使用大型语言模型（LLMs）的进展大大改善了从临床叙述中提取患者信息的能力。然而，大多数基于微调策略的方法在跨领域应用中具有有限的迁移学习能力。本研究提出了一种新颖的方法，采用软提示式学习架构，引入可训练的提示以指导LLMs朝向期望的输出。我们研究了两种LLM架构，分别是仅编码器GatorTron和仅解码器GatorTronGPT，并评估它们在使用来自2022年n2c2挑战的跨机构数据集和来自佛罗里达大学（UF）Health的跨疾病数据集进行健康社会决定因素（SDoH）提取的性能。结果表明，通过Prompt-tuning的解码器型LLMs在跨领域应用中取得了更好的性能。GatorTronGPT在交叉领域应用中取得了最好的F1分数。

    arXiv:2403.12374v1 Announce Type: new  Abstract: The progress in natural language processing (NLP) using large language models (LLMs) has greatly improved patient information extraction from clinical narratives. However, most methods based on the fine-tuning strategy have limited transfer learning ability for cross-domain applications. This study proposed a novel approach that employs a soft prompt-based learning architecture, which introduces trainable prompts to guide LLMs toward desired outputs. We examined two types of LLM architectures, including encoder-only GatorTron and decoder-only GatorTronGPT, and evaluated their performance for the extraction of social determinants of health (SDoH) using a cross-institution dataset from the 2022 n2c2 challenge and a cross-disease dataset from the University of Florida (UF) Health. The results show that decoder-only LLMs with prompt tuning achieved better performance in cross-domain applications. GatorTronGPT achieved the best F1 scores for 
    
[^42]: RankPrompt：逐步比较使语言模型成为更好的推理者

    RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners

    [https://arxiv.org/abs/2403.12373](https://arxiv.org/abs/2403.12373)

    RankPrompt 提出了一种新的提示方法，可以通过自我排序来提高大型语言模型在推理任务中的性能。

    

    大型语言模型（LLMs）在各种推理任务中取得了令人印象深刻的表现。然而，即使像ChatGPT这样的最先进的LLMs在推理过程中也容易出现逻辑错误。现有的解决方案，包括部署特定于任务的验证器或在多个推理路径上投票，要么需要大量人类注释，要么在存在不一致响应的场景中失败。为了解决这些挑战，我们介绍了RankPrompt，这是一种新的提示方法，使LLMs能够自行对其响应进行排序而无需额外资源。RankPrompt将排序问题分解为多个响应之间的一系列比较，利用LLMs自动生成比较链作为上下文示例的固有能力。我们在11个算术推理和常识推理任务上的实验表明，RankPrompt显著提高了ChatGPT和GPT-4的推理性能。

    arXiv:2403.12373v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved impressive performance across various reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT are prone to logical errors during their reasoning processes. Existing solutions, which include deploying task-specific verifiers or voting over multiple reasoning paths, either require extensive human annotations or fail in scenarios with inconsistent responses. To address these challenges, we introduce RankPrompt, a new prompting method that enables LLMs to self-rank their responses without additional resources. RankPrompt breaks down the ranking problem into a series of comparisons among diverse responses, leveraging the inherent capabilities of LLMs to generate chains of comparison as contextual exemplars. Our experiments across 11 arithmetic and commonsense reasoning tasks show that RankPrompt significantly enhances the reasoning performance of ChatGPT and GPT-4, with improvements of u
    
[^43]: 通过大型语言模型构建特征化AI代理

    Characteristic AI Agents via Large Language Models

    [https://arxiv.org/abs/2403.12368](https://arxiv.org/abs/2403.12368)

    该研究通过模拟现实个体，探讨了大型语言模型在构建特征化AI代理方面的性能，为这一任务创建了新的基准数据集和评估指标。

    

    大型语言模型（LLMs）的发展已极大增强了聊天机器人系统的性能。许多研究者致力于为聊天机器人赋予特征。尽管已有商业产品利用LLMs开发面向角色的聊天机器人，但值得注意的是，这一领域的学术研究相对较少。我们的研究旨在通过模拟不同情境下的现实个体，探讨LLMs构建特征化AI代理的性能。目前的研究主要集中在对具有简单配置文件的角色进行操作。针对这一研究空白，我们为特征化AI代理任务创建了一个基准，包括数据集、技术和评估指标。一个名为“Character100”的数据集被建立用于这一基准，包括维基百科上访问量最高的人物供语言模型扮演角色。

    arXiv:2403.12368v1 Announce Type: cross  Abstract: The advancement of Large Language Models (LLMs) has led to significant enhancements in the performance of chatbot systems. Many researchers have dedicated their efforts to the development of bringing characteristics to chatbots. While there have been commercial products for developing role-driven chatbots using LLMs, it is worth noting that academic research in this area remains relatively scarce. Our research focuses on investigating the performance of LLMs in constructing Characteristic AI Agents by simulating real-life individuals across different settings. Current investigations have primarily focused on act on roles with simple profiles. In response to this research gap, we create a benchmark for the characteristic AI agents task, including dataset, techniques, and evaluation metrics. A dataset called ``Character100'' is built for this benchmark, comprising the most-visited people on Wikipedia for language models to role-play. Wit
    
[^44]: 生成文本流中漂移的方法

    Methods for Generating Drift in Text Streams

    [https://arxiv.org/abs/2403.12328](https://arxiv.org/abs/2403.12328)

    文本数据中概念漂移是一个常见现象，而本文提出了四种文本漂移生成方法来帮助产生具有标记漂移的数据集

    

    arXiv：2403.12328v1 公告类型：跨越 摘要：系统和个体不断产生数据。 在互联网上，人们分享他们的知识，情感和意见，提供关于服务和产品的评论等。 自动从这些文本数据中学习可以为组织和机构提供见解，从而防止财务影响，例如。 为了随时间学习文本数据，机器学习系统必须考虑概念漂移。 概念漂移是现实世界数据集中的频繁现象，对应于时间上的数据分布更改。 例如，当情感变化或单词含义随时间调整时，就会发生概念漂移。 尽管概念漂移在实际应用中很常见，但具有标记漂移的基准数据集在文献中很少见。 为弥补这一差距，本文提供了四种文本漂移生成方法，以便简化产生具有标记漂移的数据集。 这些方法已应用于Ye

    arXiv:2403.12328v1 Announce Type: cross  Abstract: Systems and individuals produce data continuously. On the Internet, people share their knowledge, sentiments, and opinions, provide reviews about services and products, and so on. Automatically learning from these textual data can provide insights to organizations and institutions, thus preventing financial impacts, for example. To learn from textual data over time, the machine learning system must account for concept drift. Concept drift is a frequent phenomenon in real-world datasets and corresponds to changes in data distribution over time. For instance, a concept drift occurs when sentiments change or a word's meaning is adjusted over time. Although concept drift is frequent in real-world applications, benchmark datasets with labeled drifts are rare in the literature. To bridge this gap, this paper provides four textual drift generation methods to ease the production of datasets with labeled drifts. These methods were applied to Ye
    
[^45]: OpenEval：在能力、对齐性和安全性方面对中文LLM进行基准测试

    OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety

    [https://arxiv.org/abs/2403.12316](https://arxiv.org/abs/2403.12316)

    OpenEval引入了一个评估测试平台，通过对中文LLMs进行基准测试，涵盖了能力、对齐性和安全性，填补了现有基准测试忽视对齐性和安全问题的空白。

    

    arXiv:2403.12316v1 公告类型：新的 摘要：中文大型语言模型（LLMs）的快速发展为高效的LLM评估带来了巨大挑战。尽管当前的举措已经推出了新的基准测试或评估平台，用于评估中文LLMs，但其中许多主要关注能力，通常忽视了潜在的对齐性和安全问题。为了填补这一空白，我们引入了OpenEval，一个评估测试平台，以在能力、对齐性和安全性方面对中文LLMs进行基准测试。对于能力评估，我们包括12个基准数据集，以从4个子维度评估中文LLMs：自然语言处理任务、学科知识、常识推理和数学推理。对于对齐性评估，OpenEval包含7个数据集，检查中文LLMs产生的输出中的偏见、冒犯性和违法性。为了评估安全性，特别是高级LLMs的预期风险（如寻求权力、自我意识等），我们包括6个数据集。

    arXiv:2403.12316v1 Announce Type: new  Abstract: The rapid development of Chinese large language models (LLMs) poses big challenges for efficient LLM evaluation. While current initiatives have introduced new benchmarks or evaluation platforms for assessing Chinese LLMs, many of these focus primarily on capabilities, usually overlooking potential alignment and safety issues. To address this gap, we introduce OpenEval, an evaluation testbed that benchmarks Chinese LLMs across capability, alignment and safety. For capability assessment, we include 12 benchmark datasets to evaluate Chinese LLMs from 4 sub-dimensions: NLP tasks, disciplinary knowledge, commonsense reasoning and mathematical reasoning. For alignment assessment, OpenEval contains 7 datasets that examines the bias, offensiveness and illegalness in the outputs yielded by Chinese LLMs. To evaluate safety, especially anticipated risks (e.g., power-seeking, self-awareness) of advanced LLMs, we include 6 datasets. In addition to th
    
[^46]: 利用大型语言模型从临床记录中提取物质使用障碍严重程度信息：一种零样本学习方法

    Leveraging Large Language Models to Extract Information on Substance Use Disorder Severity from Clinical Notes: A Zero-shot Learning Approach

    [https://arxiv.org/abs/2403.12297](https://arxiv.org/abs/2403.12297)

    该研究利用大型语言模型从临床记录中提取物质使用障碍严重程度信息，克服了传统自然语言处理方法在解析复杂临床语言方面的局限性

    

    物质使用障碍（SUD）由于对健康和社会的有害影响而引起了重大关注。 SUD的识别和治疗取决于诸多因素，如严重程度、联合决定因素（例如戒断症状）和健康社会决定因素。 美国保险提供商使用的现有诊断编码系统，如国际疾病分类（ICD-10），对于某些诊断缺乏细致度，但临床医生会将此细致度（如《精神障碍诊断与统计手册》分类或DSM-5中所发现的）作为辅助非结构化文本添加到临床记录中。 传统的自然语言处理（NLP）方法在准确解析这种多样化的临床语言方面存在局限性。 大型语言模型（LLMs）通过适应多样化的语言模式，有望克服这些挑战。 本研究调查了LLMs在提取严重性方面的应用

    arXiv:2403.12297v1 Announce Type: cross  Abstract: Substance use disorder (SUD) poses a major concern due to its detrimental effects on health and society. SUD identification and treatment depend on a variety of factors such as severity, co-determinants (e.g., withdrawal symptoms), and social determinants of health. Existing diagnostic coding systems used by American insurance providers, like the International Classification of Diseases (ICD-10), lack granularity for certain diagnoses, but clinicians will add this granularity (as that found within the Diagnostic and Statistical Manual of Mental Disorders classification or DSM-5) as supplemental unstructured text in clinical notes. Traditional natural language processing (NLP) methods face limitations in accurately parsing such diverse clinical language. Large Language Models (LLMs) offer promise in overcoming these challenges by adapting to diverse language patterns. This study investigates the application of LLMs for extracting severi
    
[^47]: DALL-E 2中组合语法和语义的比较调查

    A Comparative Investigation of Compositional Syntax and Semantics in DALL-E 2

    [https://arxiv.org/abs/2403.12294](https://arxiv.org/abs/2403.12294)

    该研究比较了DALL-E 2在视觉上如何代表语言提示的含义，结果显示DALL-E 2未能生成与儿童语义准确性相匹配的图像，指向了句子表达的组成性的明显缺失。

    

    在这项研究中，我们比较了DALL-E 2如何在视觉上代表给予年幼儿童进行理解测试的语言提示的含义。我们从用于数百名2-7岁英语母语儿童的评估测试中选择了代表语法知识基本组件的句子，我们收集了这些儿童的原始项目级数据。DALL-E 2被给予这些提示五次，以生成每个项目20幅卡通图片，供9名成年评委评分。结果显示，在没有一个条件下，DALL-E 2生成的图片与儿童的语义准确性相匹配，即使在最年幼的年龄（2岁）也是如此。DALL-E 2未能在可逆形式中分配适当的角色；在否定形式上失败，尽管提示比儿童接收到的对比提示更简单；它经常将形容词分配给错误的名词；它忽略了被动句中的隐含主语。这项工作指向了句子表达的组成性的明显缺失。

    arXiv:2403.12294v1 Announce Type: new  Abstract: In this study we compared how well DALL-E 2 visually represented the meaning of linguistic prompts also given to young children in comprehension tests. Sentences representing fundamental components of grammatical knowledge were selected from assessment tests used with several hundred English-speaking children aged 2-7 years for whom we had collected original item-level data. DALL-E 2 was given these prompts five times to generate 20 cartoons per item, for 9 adult judges to score. Results revealed no conditions in which DALL-E 2-generated images that matched the semantic accuracy of children, even at the youngest age (2 years). DALL-E 2 failed to assign the appropriate roles in reversible forms; it failed on negation despite an easier contrastive prompt than the children received; it often assigned the adjective to the wrong noun; it ignored implicit agents in passives. This work points to a clear absence of compositional sentence represe
    
[^48]: FinLlama：用于算法交易应用的金融情感分类

    FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications

    [https://arxiv.org/abs/2403.12285](https://arxiv.org/abs/2403.12285)

    引入了一种基于Llama 2 7B模型的金融领域特定的情感分类框架，通过微调模型来受益于其生成性质和全面的语言操作。

    

    金融新闻在线有多个来源影响市场走势和交易员的决策。这突出了准确情感分析的需求，除了需要适当的算法交易技术来做出更明智的交易决策。标准的基于词典的情感方法已经证明它们在辅助金融决策方面的能力。然而，众所周知它们存在与上下文敏感性和词序相关的问题。大型语言模型（LLMs）也可以在这个背景下使用，但它们不是特定于金融领域的，并且往往需要大量的计算资源。为了促进一种特定于金融领域的LLM框架，我们引入了一种基于Llama 2 7B基础模型的新方法，以便从其生成性质和全面的语言操作中受益。这是通过在少部分监督金融情感数据上微调Llama2 7B模型来实现的。

    arXiv:2403.12285v1 Announce Type: new  Abstract: There are multiple sources of financial news online which influence market movements and trader's decisions. This highlights the need for accurate sentiment analysis, in addition to having appropriate algorithmic trading techniques, to arrive at better informed trading decisions. Standard lexicon based sentiment approaches have demonstrated their power in aiding financial decisions. However, they are known to suffer from issues related to context sensitivity and word ordering. Large Language Models (LLMs) can also be used in this context, but they are not finance-specific and tend to require significant computational resources. To facilitate a finance specific LLM framework, we introduce a novel approach based on the Llama 2 7B foundational model, in order to benefit from its generative nature and comprehensive language manipulation. This is achieved by fine-tuning the Llama2 7B model on a small portion of supervised financial sentiment 
    
[^49]: 零样本多任务幻觉检测

    Zero-Shot Multi-task Hallucination Detection

    [https://arxiv.org/abs/2403.12244](https://arxiv.org/abs/2403.12244)

    提出了一个在零样本设置下定量检测幻觉的框架，并在模型感知设置下实现了0.78的准确度，同时保持了计算效率。

    

    在最近的研究中，大型语言模型的广泛利用突显了评估文本生成质量和与特定任务相关性的稳健方法的重要性。这揭示了一个普遍的问题，即幻觉，这是模型中的一种紧急情况，生成的文本与来源缺乏忠实度，并且偏离了评估标准。在这项研究中，我们正式定义了幻觉，并提出了一个框架，用于在零样本设置下定量检测幻觉，利用我们的定义和假设，即模型输出包含与任务和样本特定输入有关的信息。在检测幻觉时，我们的解决方案在模型感知设置中实现了0.78的准确度，在模型无关设置下实现了0.61的准确度。值得注意的是，我们的解决方案保持了计算效率，需要比其他SOTA方法少得多的计算资源，与轻量级和压缩的趋势保持一致。

    arXiv:2403.12244v1 Announce Type: new  Abstract: In recent studies, the extensive utilization of large language models has underscored the importance of robust evaluation methodologies for assessing text generation quality and relevance to specific tasks. This has revealed a prevalent issue known as hallucination, an emergent condition in the model where generated text lacks faithfulness to the source and deviates from the evaluation criteria. In this study, we formally define hallucination and propose a framework for its quantitative detection in a zero-shot setting, leveraging our definition and the assumption that model outputs entail task and sample specific inputs. In detecting hallucinations, our solution achieves an accuracy of 0.78 in a model-aware setting and 0.61 in a model-agnostic setting. Notably, our solution maintains computational efficiency, requiring far less computational resources than other SOTA approaches, aligning with the trend towards lightweight and compressed
    
[^50]: 基于参考文献的指标在问句生成中被推翻

    Reference-based Metrics Disprove Themselves in Question Generation

    [https://arxiv.org/abs/2403.12242](https://arxiv.org/abs/2403.12242)

    基于参考文献的指标在问句生成中被推翻，作者提出了一个无需参考文献的多维标准评估方法。

    

    BLEU和BERTScore等基于参考文献的指标被广泛用于评估问句生成(QG)。本研究在SQuAD和HotpotQA等QG基准数据集上发现，使用人工编写的参考文献并不能保证基于参考文献的指标的有效性。大多数QG基准数据集只有一个参考文献；我们复制了注释过程并收集了另一个参考文献。预期好的指标应该对人工验证的问题的评分不会低于生成的问题。然而，在我们新收集的参考文献上，基于参考文献的指标的结果却证明了这些指标本身是错误的。我们提出了一个无需参考文献的指标，由多维标准组成，如自然性、可回答性和复杂性，利用大型语言模型。这些标准不受限于单个参考问题的句法或语义，该指标也不需要多样化的参考文献。实验证明我们的方法

    arXiv:2403.12242v1 Announce Type: cross  Abstract: Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG benchmarks have only one reference; we replicated the annotation process and collect another reference. A good metric was expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our met
    
[^51]: 评估命名实体识别：比较分析巴西公司财报电话转录上的单语和多语言Transformer模型

    Evaluating Named Entity Recognition: Comparative Analysis of Mono- and Multilingual Transformer Models on Brazilian Corporate Earnings Call Transcriptions

    [https://arxiv.org/abs/2403.12212](https://arxiv.org/abs/2403.12212)

    本研究通过引入新方法，将标记分类任务重新构建为文本生成问题，评估了在巴西银行财报电话转录中使用的单语和多语言Transformer模型的性能。

    

    命名实体识别（NER）是一种从文本文档中提取信息的自然语言处理技术。然而，现有关于NER的大部分研究都集中在英语文档上，导致缺乏专门针对葡萄牙语财务领域的数据集。本研究解决了金融领域内NER需求，并侧重于从巴西银行财报电话转录中提取的葡萄牙语文本。通过整理包括384个转录的综合数据集，并利用弱监督技术进行注释，我们评估了在葡萄牙语（BERTimbau和PTT5）训练的单语模型以及多语言模型（mBERT和mT5）的性能。值得注意的是，我们引入了一种新方法，将标记分类任务重新构建为文本生成问题，从而实现T5模型的微调和评估。在模型微调之后，

    arXiv:2403.12212v1 Announce Type: cross  Abstract: Named Entity Recognition (NER) is a Natural Language Processing technique for extracting information from textual documents. However, much of the existing research on NER has been centered around English-language documents, leaving a gap in the availability of datasets tailored to the financial domain in Portuguese. This study addresses the need for NER within the financial domain, focusing on Portuguese-language texts extracted from earnings call transcriptions of Brazilian banks. By curating a comprehensive dataset comprising 384 transcriptions and leveraging weak supervision techniques for annotation, we evaluate the performance of monolingual models trained on Portuguese (BERTimbau and PTT5) and multilingual models (mBERT and mT5). Notably, we introduce a novel approach that reframes the token classification task as a text generation problem, enabling fine-tuning and evaluation of T5 models. Following the fine-tuning of the models,
    
[^52]: TnT-LLM：大规模语言模型下的文本挖掘

    TnT-LLM: Text Mining at Scale with Large Language Models

    [https://arxiv.org/abs/2403.12173](https://arxiv.org/abs/2403.12173)

    TnT-LLM 提出了一个两阶段框架，利用大规模语言模型自动化生成和分配标签，减少人力成本。

    

    将非结构化文本转换为结构化有意义的形式，通过有用的类别标签进行组织，是文本挖掘中用于下游分析和应用的基础步骤。然而，大多数现有的生成标签分类法和构建基于文本标签的分类器的方法仍然严重依赖于领域专业知识和手动整理，使得这个过程昂贵且耗时。当标签空间不明确且缺少大规模数据注释时，这一挑战尤为严峻。在本文中，我们用大规模语言模型（LLMs）解决了这些挑战，其基于提示的接口有助于引导和使用大规模伪标签。我们提出了TnT-LLM，这是一个两阶段框架，利用LLMs自动化端到端标签生成和分配的过程，减少人力成本。

    arXiv:2403.12173v1 Announce Type: cross  Abstract: Transforming unstructured text into structured and meaningful forms, organized by useful category labels, is a fundamental step in text mining for downstream analysis and application. However, most existing methods for producing label taxonomies and building text-based label classifiers still rely heavily on domain expertise and manual curation, making the process expensive and time-consuming. This is particularly challenging when the label space is under-specified and large-scale data annotations are unavailable. In this paper, we address these challenges with Large Language Models (LLMs), whose prompt-based interface facilitates the induction and use of large-scale pseudo labels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case. In the first phase, we introduce a zero-shot, multi-stage reasoning approach w
    
[^53]: EasyJailbreak: 一个用于越狱大型语言模型的统一框架

    EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models

    [https://arxiv.org/abs/2403.12171](https://arxiv.org/abs/2403.12171)

    EasyJailbreak是一个统一框架，简化了对LLMs进行越狱攻击的构建和评估，支持11种越狱方法，帮助进行广泛范围LLMs的安全验证。

    

    越狱攻击对于识别和减轻大型语言模型（LLMs）的安全漏洞至关重要。它们被设计用于绕过保障措施并引发被禁止的输出。然而，由于各种越狱方法之间存在显著差异，目前还没有针对社区提供标准实现框架，这限制了全面的安全评估。本文介绍了EasyJailbreak，一个统一框架，简化了针对LLMs进行越狱攻击的构建和评估。它使用四个组件来构建越狱攻击：选择器、变异器、约束器和评估器。这个模块化框架使研究人员可以轻松地从新的和现有组件的组合中构建攻击。到目前为止，EasyJailbreak支持11种不同的越狱方法，并促进了对广泛范围LLMs的安全验证。我们在10个不同的LLMs上的验证揭示了一个重要的漏洞。

    arXiv:2403.12171v1 Announce Type: cross  Abstract: Jailbreak attacks are crucial for identifying and mitigating the security vulnerabilities of Large Language Models (LLMs). They are designed to bypass safeguards and elicit prohibited outputs. However, due to significant differences among various jailbreak methods, there is no standard implementation framework available for the community, which limits comprehensive security evaluations. This paper introduces EasyJailbreak, a unified framework simplifying the construction and evaluation of jailbreak attacks against LLMs. It builds jailbreak attacks using four components: Selector, Mutator, Constraint, and Evaluator. This modular framework enables researchers to easily construct attacks from combinations of novel and existing components. So far, EasyJailbreak supports 11 distinct jailbreak methods and facilitates the security validation of a broad spectrum of LLMs. Our validation across 10 distinct LLMs reveals a significant vulnerabilit
    
[^54]: 将大型语言模型中的领域特定内容融入知识图谱，以增强零样本对象状态分类

    Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification

    [https://arxiv.org/abs/2403.12151](https://arxiv.org/abs/2403.12151)

    大型语言模型与知识图谱结合，提高零样本对象状态分类性能

    

    领域特定知识可以显著有助于解决各种视觉任务，但生成这种知识需要大量人力和时间成本。本研究探讨了大型语言模型（LLMs）在通过语义嵌入生成和提供领域特定信息方面的潜力。为实现这一目标，将LLM集成到一个流程中，该流程在视觉基础零样本对象状态分类任务的背景下利用知识图谱和预训练的语义向量。通过广泛的消融研究彻底研究了LLM的行为。我们的研究结果表明，将基于LLM的嵌入与通用的预训练嵌入结合使用可以显著提高性能。借鉴这一消融研究的见解，我们对竞争模型进行了比较分析，从而突出了最新的表现水平。

    arXiv:2403.12151v1 Announce Type: new  Abstract: Domain-specific knowledge can significantly contribute to addressing a wide variety of vision tasks. However, the generation of such knowledge entails considerable human labor and time costs. This study investigates the potential of Large Language Models (LLMs) in generating and providing domain-specific information through semantic embeddings. To achieve this, an LLM is integrated into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors in the context of the Vision-based Zero-shot Object State Classification task. We thoroughly examine the behavior of the LLM through an extensive ablation study. Our findings reveal that the integration of LLM-based embeddings, in combination with general-purpose pre-trained embeddings, leads to substantial performance improvements. Drawing insights from this ablation study, we conduct a comparative analysis against competing models, thereby highlighting the state-of-the-art perfor
    
[^55]: Syn-QA2: 使用合成QA数据集评估长尾问题中的错误假设

    Syn-QA2: Evaluating False Assumptions in Long-tail Questions with Synthetic QA Datasets

    [https://arxiv.org/abs/2403.12145](https://arxiv.org/abs/2403.12145)

    通过合成QA数据集Syn-(QA)$^2$的引入，作者发现在长尾问题上的错误假设对大型语言模型来说是具有挑战性的，尤其是在二元检测任务方面。

    

    最近的研究表明，信息检索问题中的错误假设(或错误前提)对于稳健的问答(QA)系统至关重要。然而，现有研究集中在自然发生的问题上，存在关于模型在可能问题分布的长尾上的行为分析的空白。因此，我们引入了Syn-(QA)$^2$，一个包含两个合成生成的QA数据集：一个是使用来自Wikidata的扰动关系生成的，另一个是通过扰动HotpotQA生成的。我们从评估一系列大型语言模型中得出的发现有三个：(1) QA中的错误假设是具有挑战性的，呼应了先前工作的发现，(2) 与不可区分任务相比，二元检测任务更具挑战性.

    arXiv:2403.12145v1 Announce Type: new  Abstract: Sensitivity to false assumptions (or false premises) in information-seeking questions is critical for robust question-answering (QA) systems. Recent work has shown that false assumptions in naturally occurring questions pose challenges to current models, with low performance on both generative QA and simple detection tasks (Kim et al. 2023). However, the focus of existing work on naturally occurring questions leads to a gap in the analysis of model behavior on the long tail of the distribution of possible questions. To this end, we introduce Syn-(QA)$^2$, a set of two synthetically generated QA datasets: one generated using perturbed relations from Wikidata, and the other by perturbing HotpotQA (Yang et al. 2018). Our findings from evaluating a range of large language models are threefold: (1) false assumptions in QA are challenging, echoing the findings of prior work, (2) the binary detection task is challenging even compared to the dif
    
[^56]: LLMs是一个好的难解填字游戏求解器吗？

    Are LLMs Good Cryptic Crossword Solvers?

    [https://arxiv.org/abs/2403.12094](https://arxiv.org/abs/2403.12094)

    本文建立了三种流行LLMs的基准结果，表明它们在难解填字游戏上的表现仍远远不及人类。

    

    难解填字游戏是一种谜题，不仅依赖于一般知识，还依赖于求解者在不同层面上操纵语言并处理各种类型的文字游戏。先前的研究表明，即使对于现代NLP模型来说，解决这类谜题也是一项挑战。然而，大型语言模型（LLMs）的能力尚未在这一任务上进行测试。在本文中，我们为三种流行的LLMs -- LLaMA2、Mistral和ChatGPT建立了基准结果，显示它们在这一任务上的表现仍远远不及人类。

    arXiv:2403.12094v1 Announce Type: new  Abstract: Cryptic crosswords are puzzles that rely not only on general knowledge but also on the solver's ability to manipulate language on different levels and deal with various types of wordplay. Previous research suggests that solving such puzzles is a challenge even for modern NLP models. However, the abilities of large language models (LLMs) have not yet been tested on this task. In this paper, we establish the benchmark results for three popular LLMs -- LLaMA2, Mistral, and ChatGPT -- showing that their performance on this task is still far from that of humans.
    
[^57]: 匹配英语地址的方法

    Methods for Matching English Language Addresses

    [https://arxiv.org/abs/2403.12092](https://arxiv.org/abs/2403.12092)

    该研究定义并规范了生成英语地址匹配对的框架，并研究了距离基准方法到深度学习模型等各种方法之间的精度、召回率和准确度，以确定最适合地址匹配任务的方法。

    

    地址在文本数据中占据着一席之地，因为每个词所具有的位置重要性和它所涉及的地理范围。匹配地址的任务每天都在发生，并且存在于邮件重定向、实体解析等各种领域中。我们的工作定义和规范了一个框架，用于生成英语地址的匹配和不匹配对，并将其用于评估各种方法自动执行地址匹配。这些方法从基于距离的方法到深度学习模型各不相同。通过研究这些方法的精度、召回率和准确度指标，我们可以了解到最适合这种地址匹配任务设置的方法。

    arXiv:2403.12092v1 Announce Type: cross  Abstract: Addresses occupy a niche location within the landscape of textual data, due to the positional importance carried by every word, and the geographical scope it refers to. The task of matching addresses happens everyday and is present in various fields like mail redirection, entity resolution, etc. Our work defines, and formalizes a framework to generate matching and mismatching pairs of addresses in the English language, and use it to evaluate various methods to automatically perform address matching. These methods vary widely from distance based approaches to deep learning models. By studying the Precision, Recall and Accuracy metrics of these approaches, we obtain an understanding of the best suited method for this setting of the address matching task.
    
[^58]: TMU参加2023年TREC临床试验跟踪

    TMU at TREC Clinical Trials Track 2023

    [https://arxiv.org/abs/2403.12088](https://arxiv.org/abs/2403.12088)

    多伦多都会大学利用自然语言处理技术和神经语言模型参加TREC临床试验跟踪，并展示了其实验结果。

    

    这篇论文描述了多伦多都会大学参加2023年TREC临床试验跟踪。作为任务的一部分，我们在实验中利用了先进的自然语言处理技术和神经语言模型来检索最相关的临床试验。我们阐述了团队-V-TorontoMU的参与的整体方法论、实验设置和实现结果。

    arXiv:2403.12088v1 Announce Type: new  Abstract: This paper describes Toronto Metropolitan University's participation in the TREC Clinical Trials Track for 2023. As part of the tasks, we utilize advanced natural language processing techniques and neural language models in our experiments to retrieve the most relevant clinical trials. We illustrate the overall methodology, experimental settings, and results of our implementation for the run submission as part of Team - V-TorontoMU.
    
[^59]: 展示 Terrorizer：一种用于整合专利受让人中公司名称的算法

    Presenting Terrorizer: an algorithm for consolidating company names in patent assignees

    [https://arxiv.org/abs/2403.12083](https://arxiv.org/abs/2403.12083)

    本文介绍了一种名为Terrorizer的算法，利用自然语言处理、网络理论和基于规则的技术，以解决归因于公司的专利中存在的名称变体问题。

    

    公司名称消歧的问题在从专利中提取有用信息方面提出了重大挑战。这个问题会导致研究结果存在偏差，因为它在很大程度上低估了归因于公司的专利数量，特别是那些以众多名称（包括相同实体的替代拼写和最终公司的子公司）申请专利的跨国公司。迄今为止，解决这些挑战主要依赖于耗时的基于词典或字符串匹配方法，而大多数情况下解决大型数据集上专利受让人的协调问题仍未得到解决。为填补这一空白，本文描述了 Terrorizer 算法，这是一种文本算法，利用自然语言处理（NLP）、网络理论和基于规则的技术来协调作为专利受让人记录的公司名称的变体。具体而言，该算法遵循其前体的三部分结构

    arXiv:2403.12083v1 Announce Type: cross  Abstract: The problem of disambiguation of company names poses a significant challenge in extracting useful information from patents. This issue biases research outcomes as it mostly underestimates the number of patents attributed to companies, particularly multinational corporations which file patents under a plethora of names, including alternate spellings of the same entity and, eventually, companies' subsidiaries. To date, addressing these challenges has relied on labor-intensive dictionary based or string matching approaches, leaving the problem of patents' assignee harmonization on large datasets mostly unresolved. To bridge this gap, this paper describes the Terrorizer algorithm, a text-based algorithm that leverages natural language processing (NLP), network theory, and rule-based techniques to harmonize the variants of company names recorded as patent assignees. In particular, the algorithm follows the tripartite structure of its antece
    
[^60]: 生还的男孩：从LLM中删除哈利波特比报道的更困难

    The Boy Who Survived: Removing Harry Potter from an LLM is harder than reported

    [https://arxiv.org/abs/2403.12082](https://arxiv.org/abs/2403.12082)

    通过小规模实验发现，从LLM中删除哈利波特内容比先前报道的更加困难。

    

    最近的研究声称"我们有效地抹除了模型生成或回忆哈利波特相关内容的能力。"然而，一项小规模实验表明这一说法过于宽泛。少于十次试验导致重复和具体提及哈利波特，包括"啊，我明白了！"麻瓜"是特里·普拉切特的哈利波特系列中使用的术语...''。

    arXiv:2403.12082v1 Announce Type: cross  Abstract: Recent work arXiv.2310.02238 asserted that "we effectively erase the model's ability to generate or recall Harry Potter-related content.'' This claim is shown to be overbroad. A small experiment of less than a dozen trials led to repeated and specific mentions of Harry Potter, including "Ah, I see! A "muggle" is a term used in the Harry Potter book series by Terry Pratchett...''
    
[^61]: 评估生成式搜索引擎对对抗性事实问题的健壮性

    Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions

    [https://arxiv.org/abs/2403.12077](https://arxiv.org/abs/2403.12077)

    评估生成式搜索引擎对对抗性事实问题的健壮性，通过对多种生成式搜索引擎进行人类评估，展示了对抗性事实问题在诱导不正确响应方面的有效性。

    

    生成式搜索引擎有潜力改变人们在线获取信息的方式，但现有大型语言模型（LLMs）支持的生成式搜索引擎生成的响应可能并不总是准确。然而，检索增强生成会加剧安全性问题，因为对手可能通过微妙地操纵声明的最薄弱部分成功规避整个系统。因此，我们提出在对抗性事实问题的现实且高风险设置中评估生成式搜索引擎的健壮性，其中对手仅具有黑盒系统访问权限，并试图欺骗模型返回不正确的响应。通过对必应聊天、PerplexityAI和YouChat等各种生成式搜索引擎进行全面的人类评估，我们展示了对抗性事实问题对诱导不正确响应的有效性。此外，检索增强生成展现出...

    arXiv:2403.12077v1 Announce Type: cross  Abstract: Generative search engines have the potential to transform how people seek information online, but generated responses from existing large language models (LLMs)-backed generative search engines may not always be accurate. Nonetheless, retrieval-augmented generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable part of a claim. To this end, we propose evaluating the robustness of generative search engines in the realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning incorrect responses. Through a comprehensive human evaluation of various generative search engines, such as Bing Chat, PerplexityAI, and YouChat across diverse queries, we demonstrate the effectiveness of adversarial factual questions in inducing incorrect responses. Moreover, retrieval-augmented generation exhibits a
    
[^62]: JORA: 用于检索增强微调的JAX张量并行LoRA库

    JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning

    [https://arxiv.org/abs/2403.11366](https://arxiv.org/abs/2403.11366)

    提出了用于检索增强微调的JAX张量并行LoRA库，通过PEFT兼容微调Llama-2模型，利用分布式训练和JAX的即时编译和张量分片实现了资源高效管理，加速微调并降低内存需求，提高了微调大型语言模型在复杂RAG应用中的可扩展性和可行性。

    

    《JORA: JAX张量并行LoRA库用于检索增强微调》通过介绍一种新的框架，提供了一种适用于检索增强生成（RAG）任务的PEFT兼容微调Llama-2模型的方法，利用分布式训练，独特地利用了JAX的即时编译（JIT）和张量分片，实现了资源的高效管理，从而实现了加速微调并降低内存需求。这一进展显著提高了微调大型语言模型（LLMs）用于复杂RAG应用的可扩展性和可行性，甚至在GPU资源有限的系统上也能取得显著改进。

    arXiv:2403.11366v1 Announce Type: cross  Abstract: The scaling of Large Language Models (LLMs) for retrieval-based tasks, particularly in Retrieval Augmented Generation (RAG), faces significant memory constraints, especially when fine-tuning extensive prompt sequences. Current open-source libraries support full-model inference and fine-tuning across multiple GPUs but fall short of accommodating the efficient parameter distribution required for retrieved context. Addressing this gap, we introduce a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT) compilation and tensor-sharding for efficient resource management, thereby enabling accelerated fine-tuning with reduced memory requirements. This advancement significantly improves the scalability and feasibility of fine-tuning LLMs for complex RAG applications, even on systems with limited GPU resources. Our experiments show more than 1
    
[^63]: 从非侵入性脑记录解码连续基于字符的语言

    Decoding Continuous Character-based Language from Non-invasive Brain Recordings

    [https://arxiv.org/abs/2403.11183](https://arxiv.org/abs/2403.11183)

    提出了一种从单次非侵入性脑记录中解码连续语言的新方法，通过三维卷积网络和信息瓶颈结合字符解码器，能够实现在和跨主体之间产生捕捉感知语音含义的可理解文本序列。

    

    通过非侵入性设备从大脑活动中解读自然语言仍然是一个艰巨的挑战。之前的非侵入式解码器要么需要进行多次实验来确定皮层区域并增强大脑活动的信噪比，要么仅限于识别基本的语言元素如字母和单词。我们提出了一种新的方法，从单次非侵入性fMRI记录中解码连续语言，其中开发了一个三维卷积网络，该网络配备信息瓶颈以自动识别对刺激有响应的体素，并设计了一个基于字符的解码器, 用于对内在字符结构所特征化的连续语言进行语义重建。所得解码器能够生成可理解的文本序列，忠实地捕捉感知语音的含义，无论是在同一主体内还是跨主体之间。

    arXiv:2403.11183v1 Announce Type: new  Abstract: Deciphering natural language from brain activity through non-invasive devices remains a formidable challenge. Previous non-invasive decoders either require multiple experiments with identical stimuli to pinpoint cortical regions and enhance signal-to-noise ratios in brain activity, or they are limited to discerning basic linguistic elements such as letters and words. We propose a novel approach to decoding continuous language from single-trial non-invasive fMRI recordings, in which a three-dimensional convolutional network augmented with information bottleneck is developed to automatically identify responsive voxels to stimuli, and a character-based decoder is designed for the semantic reconstruction of continuous language characterized by inherent character structures. The resulting decoder can produce intelligible textual sequences that faithfully capture the meaning of perceived speech both within and across subjects, while existing d
    
[^64]: Fisher Mask节点用于语言模型合并

    Fisher Mask Nodes for Language Model Merging

    [https://arxiv.org/abs/2403.09891](https://arxiv.org/abs/2403.09891)

    介绍了一种用于Transformers的新型模型合并方法，利用Fisher信息进行加权平均，提高了多任务模型的性能。

    

    微调预训练模型在下游性能方面具有显著优势。预训练模型（如BERT及其衍生物）在自然语言处理中的普遍性也导致了任务特定微调模型的激增。在多任务场景中，由于这些模型通常只能很好地执行一项任务，因此需要额外的训练或集成。模型合并这一不断增长的领域提供了一个解决方案，解决了将多个任务特定模型合并为单个多任务模型的挑战。在本研究中，我们引入了一种新颖的用于Transformers的模型合并方法，结合了先前Fisher加权平均和Fisher信息在模型修剪中的应用的见解。通过利用Transformer架构内的mask节点的Fisher信息，我们设计了一个计算效率高的加权平均方案。我们的方法展现出了稳定且显著的性能。

    arXiv:2403.09891v1 Announce Type: cross  Abstract: Fine-tuning pre-trained models provides significant advantages in downstream performance. The ubiquitous nature of pre-trained models such as BERT and its derivatives in natural language processing has also led to a proliferation of task-specific fine-tuned models. As these models typically only perform one task well, additional training or ensembling is required in multi-task scenarios. The growing field of model merging provides a solution, dealing with the challenge of combining multiple task-specific models into a single multi-task model. In this study, we introduce a novel model merging method for Transformers, combining insights from previous work in Fisher-weighted averaging and the use of Fisher information in model pruning. Utilizing the Fisher information of mask nodes within the Transformer architecture, we devise a computationally efficient weighted-averaging scheme. Our method exhibits a regular and significant performance
    
[^65]: MM1：多模式LLM预训练的方法、分析与见解

    MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training

    [https://arxiv.org/abs/2403.09611](https://arxiv.org/abs/2403.09611)

    通过详细研究图像编码器、视觉语言连接器和预训练数据选择的重要性，确定了对于实现多个基准测试中最新潮的少样本结果至关重要的关键设计经验。

    

    在这项工作中，我们讨论了构建高性能的多模式大型语言模型（MLLMs）。具体来说，我们研究了各种架构组件和数据选择的重要性。通过对图像编码器、视觉语言连接器和各种预训练数据选择进行仔细和全面的消融实验，我们确定了几个关键的设计经验。例如，我们展示了对大规模多模式预训练使用仔细混合的图像标题、交替图像文本和仅文本数据对于在多个基准测试中实现最新潮（SOTA）的少样本结果至关重要，与其他已发表的预训练结果相比。此外，我们表明图像编码器连同图像分辨率和图像标记计数具有重要影响，而视觉语言连接器设计相对重要性较小。通过扩大所提出的方法，我们构建了MM1，一个多模式模型系列。

    arXiv:2403.09611v1 Announce Type: cross  Abstract: In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up 
    
[^66]: 科莫多：探索印度尼西亚地区语言的语言考察

    Komodo: A Linguistic Expedition into Indonesia's Regional Languages

    [https://arxiv.org/abs/2403.09362](https://arxiv.org/abs/2403.09362)

    Komodo-7B是一个大型语言模型，可以无缝操作印度尼西亚、英语和11种印度尼西亚地区语言，Komodo-7B-Instruct达到了卓越的性能，超越了多个基准模型。

    

    最近在大型语言模型（LLMs）方面取得的突破主要集中在具有易于获取和充足资源的语言上，例如英语。然而，对于在公共领域缺乏足够语言资源的语言仍存在重大差距。我们的工作引入了Komodo-7B，一个70亿参数的大型语言模型，旨在通过在印度尼西亚、英语和印度尼西亚的11种地区语言之间无缝操作来填补这一差距。Komodo-7B是一组LLMs，由Komodo-7B-Base和Komodo-7B-Instruct组成。Komodo-7B-Instruct凭借在各种任务和语言上取得的最先进性能脱颖而出，超越了OpenAI的GPT-3.5、Cohere的Aya-101、Llama-2-Chat-13B、Mixtral-8x7B-Instruct-v0.1、Gemma-7B-it等模型制定的基准。这个模型不仅在语言特定和整体评估中表现出优越性能，还突显了其在相关任务中表现出色的能力。

    arXiv:2403.09362v1 Announce Type: new  Abstract: The recent breakthroughs in Large Language Models (LLMs) have mostly focused on languages with easily available and sufficient resources, such as English. However, there remains a significant gap for languages that lack sufficient linguistic resources in the public domain. Our work introduces Komodo-7B, 7-billion-parameter Large Language Models designed to address this gap by seamlessly operating across Indonesian, English, and 11 regional languages in Indonesia. Komodo-7B is a family of LLMs that consist of Komodo-7B-Base and Komodo-7B-Instruct. Komodo-7B-Instruct stands out by achieving state-of-the-art performance in various tasks and languages, outperforming the benchmarks set by OpenAI's GPT-3.5, Cohere's Aya-101, Llama-2-Chat-13B, Mixtral-8x7B-Instruct-v0.1, Gemma-7B-it , and many more. This model not only demonstrates superior performance in both language-specific and overall assessments but also highlights its capability to excel
    
[^67]: 知识图谱大型语言模型（KG-LLM）用于链接预测

    Knowledge Graph Large Language Model (KG-LLM) for Link Prediction

    [https://arxiv.org/abs/2403.07311](https://arxiv.org/abs/2403.07311)

    该论文提出了知识图谱大型语言模型框架（KG-LLM），利用思维链提示和上下文学习等NLP范例，以增强知识图谱中的多跳链接预测，并展示了框架在微调大型语言模型和零次尝试能力方面的有效性。

    

    在知识图谱分析领域，预测知识图谱（KGs）内多个链接的任务是一个挑战，由于自然语言处理（NLP）和知识图嵌入技术的进步，这一挑战变得越来越可解决。本文介绍了一种新的方法，即知识图谱大型语言模型框架（KG-LLM），该框架利用关键的NLP范例，包括思维链提示（CoT）和上下文学习（ICL），以增强知识图谱中的多跳链接预测。通过将KG转换为CoT提示，我们的框架旨在识别并学习实体及其相互关系的潜在表示。为了展示KG-LLM框架的有效性，我们在该框架内微调了三种主要的大型语言模型（LLMs），同时采用了非ICL和ICL任务进行全面评估。此外，我们探讨了该框架为LLMs提供零次尝试能力的潜力。

    arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
    
[^68]: 能否用LLM替代人工标注？ 无人机交付任务下的细粒度中文地址实体识别数据集案例研究

    Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery

    [https://arxiv.org/abs/2403.06097](https://arxiv.org/abs/2403.06097)

    提出了适用于无人机交付系统中地址解析任务的细粒度中文姓名实体识别数据集CNER-UAV，包含五个类别的多样化数据，经过严格的数据清洗和去敏处理，约有12,000个标注样本，评估了传统的实体识别模型并提供了深入分析

    

    我们提出了CNER-UAV，一个专为无人机交付系统中地址解析任务设计的细粒度中文姓名实体识别数据集。该数据集涵盖了五个类别，可以全面训练和评估实体识别模型。为构建这一数据集，我们从真实无人机交付系统中获取数据，并进行了严格的数据清洗和去敏处理，确保数据的隐私性和完整性。最终的数据集约包含12,000个标注样本，经过人工专家和大型语言模型的注释。我们在我们的数据集上评估了传统的实体识别模型，并提供了深入分析。数据集和模型可以在 \url{https://github.com/zhhvvv/CNER-UAV} 上公开获取。

    arXiv:2403.06097v1 Announce Type: cross  Abstract: We present CNER-UAV, a fine-grained \textbf{C}hinese \textbf{N}ame \textbf{E}ntity \textbf{R}ecognition dataset specifically designed for the task of address resolution in \textbf{U}nmanned \textbf{A}erial \textbf{V}ehicle delivery systems. The dataset encompasses a diverse range of five categories, enabling comprehensive training and evaluation of NER models. To construct this dataset, we sourced the data from a real-world UAV delivery system and conducted a rigorous data cleaning and desensitization process to ensure privacy and data integrity. The resulting dataset, consisting of around 12,000 annotated samples, underwent human experts and \textbf{L}arge \textbf{L}anguage \textbf{M}odel annotation. We evaluated classical NER models on our dataset and provided in-depth analysis. The dataset and models are publicly available at \url{https://github.com/zhhvvv/CNER-UAV}.
    
[^69]: KG-Rank: 利用知识图谱和排名技术增强医学问答的大型语言模型

    KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques

    [https://arxiv.org/abs/2403.05881](https://arxiv.org/abs/2403.05881)

    本研究开发了KG-Rank框架，利用医学知识图谱和排名技术，旨在提高医学领域自由文本问答的准确性。

    

    大型语言模型（LLMs）显著推进了医疗保健创新的生成能力。然而，由于可能偏离医疗事实和固有偏见，它们在实际临床设置中的应用具有挑战性。在这项工作中，我们开发了一个增强型LLM框架KG-Rank，利用医学知识图谱（KG）与排名和重新排名技术，旨在改进医学领域自由文本问答（QA）。具体来说，在收到问题后，我们首先从医学KG中检索三元组以收集事实信息。随后，我们创新性地应用排名方法来精细调整这些三元组的顺序，旨在产生更精确的答案。据我们所知，KG-Rank是首个将排名模型与KG结合在一起，专门用于生成长答案的医学问答应用。对四个选定的医学问答数据集的评估显示，KG-Rank实现了

    arXiv:2403.05881v1 Announce Type: new  Abstract: Large Language Models (LLMs) have significantly advanced healthcare innovation on generation capabilities. However, their application in real clinical settings is challenging due to potential deviations from medical facts and inherent biases. In this work, we develop an augmented LLM framework, KG-Rank, which leverages a medical knowledge graph (KG) with ranking and re-ranking techniques, aiming to improve free-text question-answering (QA) in the medical domain. Specifically, upon receiving a question, we initially retrieve triplets from a medical KG to gather factual information. Subsequently, we innovatively apply ranking methods to refine the ordering of these triplets, aiming to yield more precise answers. To the best of our knowledge, KG-Rank is the first application of ranking models combined with KG in medical QA specifically for generating long answers. Evaluation of four selected medical QA datasets shows that KG-Rank achieves a
    
[^70]: ChatASU：唤起LLM的反思，真正理解对话中的方面情绪

    ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues

    [https://arxiv.org/abs/2403.05326](https://arxiv.org/abs/2403.05326)

    本文提出了一个新的基于聊天的方面情绪理解（ChatASU）任务，旨在探索大型语言模型（LLMs）在对话场景中理解方面情绪的能力，并引入了一个子任务Aspect Chain Reasoning（ACR）任务来解决方面共指问题。

    

    在互动场景（例如，问答和对话）中进行方面情绪理解（ASU）近年来引起了越来越多的关注并取得了重要进展。然而，现有研究大多忽略了意见目标（即方面）的共指问题，而这种现象在互动场景特别是对话中普遍存在，限制了ASU的性能。最近，大型语言模型（LLM）展示了将各种NLP任务与聊天范式相结合的强大能力。基于此，本文提出了一项新的基于聊天的方面情绪理解（ChatASU）任务，旨在探索LLMs在对话场景中理解方面情绪的能力。特别是，这项ChatASU任务引入了一个子任务，即方面链推理（ACR）任务，以解决方面共指问题。在此基础上，我们提出了一种可信的自反思方法（TSA）与ChatGLM作为背景。

    arXiv:2403.05326v1 Announce Type: cross  Abstract: Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., Question-Answering and Dialogue) has attracted ever-more interest in recent years and achieved important progresses. However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance. Recently, large language models (LLMs) shows the powerful ability to integrate various NLP tasks with the chat paradigm. In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in understanding aspect sentiments in dialogue scenarios. Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to address the aspect coreference issue. On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as back
    
[^71]: PHAnToM：人格对大型语言模型的心理理论推理产生影响

    PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large Language Models

    [https://arxiv.org/abs/2403.02246](https://arxiv.org/abs/2403.02246)

    通过提示引发特定人格对大型语言模型的心理理论推理能力产生显著影响，特别是来自黑暗三合会的特质对多种LLMs在不同ToM任务中具有较大效应。

    

    大型语言模型（LLMs）方面的最新进展表明，它们在自然语言处理的许多任务中的能力与甚至优于人类。尽管取得了这一进展，LLMs在社会认知推理方面仍然不足，而人类在这方面天生就很擅长。受到心理学研究中某些人格特质与心理理论（ToM）推理之间联系的启发，以及关于提示工程研究在影响LLMs能力方面的超敏感性的启发，本研究调查了使用提示在LLMs中引发人格如何影响它们的ToM推理能力。我们的研究结果表明，某些引发的人格特质可以显著影响LLMs在三种不同的ToM任务中的推理能力。特别是，来自黑暗三合会(Dark Triad)的特质对于像GPT-3.5、Llama 2和Mistral这样的LLMs在不同的ToM任务中具有较大的变量效应。我们发现，具有某些人格特质的LLMs在执行ToM任务时表现出不同的表现。

    arXiv:2403.02246v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) demonstrate that their capabilities are comparable, or even superior, to humans in many tasks in natural language processing. Despite this progress, LLMs are still inadequate at social-cognitive reasoning, which humans are naturally good at. Drawing inspiration from psychological research on the links between certain personality traits and Theory-of-Mind (ToM) reasoning, and from prompt engineering research on the hyper-sensitivity of prompts in affecting LLMs capabilities, this study investigates how inducing personalities in LLMs using prompts affects their ToM reasoning capabilities. Our findings show that certain induced personalities can significantly affect the LLMs' reasoning capabilities in three different ToM tasks. In particular, traits from the Dark Triad have a larger variable effect on LLMs like GPT-3.5, Llama 2, and Mistral across the different ToM tasks. We find that LLMs tha
    
[^72]: 通过可解释的神经符号管道增强多领域自动简答题评分

    Enhancing Multi-Domain Automatic Short Answer Grading through an Explainable Neuro-Symbolic Pipeline

    [https://arxiv.org/abs/2403.01811](https://arxiv.org/abs/2403.01811)

    通过提出弱监督标注程序和基于合理化线索的神经符号模型，我们在多领域自动简答题评分方面取得了显著进展。

    

    使用具有解释性的推理来自动评分简答题，并使评分决定背后的推理可解释是当前变压器模型方法面临的挑战。在ASAG中，通过逻辑推理器探测合理化线索，已经展示出一种有前途的神经符号架构方向。然而，主要挑战之一是需要标注在学生回答中的合理化线索，这仅在少数ASAG数据集中存在。为了克服这一挑战，我们提出了（1）一种用于ASAG数据集中合理化线索的弱监督注释程序，以及（2）一种基于合理化线索的可解释ASAG的神经符号模型。与Short Answer Feedback数据集的最新技术相比，我们的方法在双语、多领域和多问题训练设置中将均方根误差提高了0.24到0.3。这一结果表明，我们的方法为生成高质量评分提供了一个有前途的方向。

    arXiv:2403.01811v1 Announce Type: new  Abstract: Grading short answer questions automatically with interpretable reasoning behind the grading decision is a challenging goal for current transformer approaches. Justification cue detection, in combination with logical reasoners, has shown a promising direction for neuro-symbolic architectures in ASAG. But, one of the main challenges is the requirement of annotated justification cues in the students' responses, which only exist for a few ASAG datasets. To overcome this challenge, we contribute (1) a weakly supervised annotation procedure for justification cues in ASAG datasets, and (2) a neuro-symbolic model for explainable ASAG based on justification cues. Our approach improves upon the RMSE by 0.24 to 0.3 compared to the state-of-the-art on the Short Answer Feedback dataset in a bilingual, multi-domain, and multi-question training setup. This result shows that our approach provides a promising direction for generating high-quality grades
    
[^73]: CASIMIR：一个包含多个作者综合修订的科学文章语料库

    CASIMIR: A Corpus of Scientific Articles enhanced with Multiple Author-Integrated Revisions

    [https://arxiv.org/abs/2403.00241](https://arxiv.org/abs/2403.00241)

    CASIMIR提供了一个包含多个作者综合修订的科学文章语料库，以促进对科学文章写作修订步骤的研究。

    

    写作科学文章是一项具有挑战性的任务，因为它是一种高度规范化和具体的体裁，因此精通书面交流对有效传达研究发现和观点至关重要。在本文中，我们提出了一个关于科学文章写作过程中修订步骤的原始文本资源。这个名为CASIMIR的新数据集包含来自OpenReview的15,646篇科学文章的多个修订版本，以及它们的同行评审。文章的连续版本对在句子级别对齐，同时保留段落位置信息作为支持未来修订研究的元数据在语篇级别。每一对修订后的句子都通过自动提取的编辑和相关修订意图进行了丰富。为了评估数据集的初始质量，我们对几种最先进的文本修订方法进行了定性研究，并进行了比较。

    arXiv:2403.00241v1 Announce Type: new  Abstract: Writing a scientific article is a challenging task as it is a highly codified and specific genre, consequently proficiency in written communication is essential for effectively conveying research findings and ideas. In this article, we propose an original textual resource on the revision step of the writing process of scientific articles. This new dataset, called CASIMIR, contains the multiple revised versions of 15,646 scientific articles from OpenReview, along with their peer reviews. Pairs of consecutive versions of an article are aligned at sentence-level while keeping paragraph location information as metadata for supporting future revision studies at the discourse level. Each pair of revised sentences is enriched with automatically extracted edits and associated revision intention. To assess the initial quality on the dataset, we conducted a qualitative study of several state-of-the-art text revision approaches and compared various
    
[^74]: MATHSENSEI：用于数学推理的工具增强型大型语言模型

    MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning

    [https://arxiv.org/abs/2402.17231](https://arxiv.org/abs/2402.17231)

    MATHSENSEI 是一种用于数学推理的工具增强型大型语言模型，通过添加知识检索、程序执行和符号方程求解工具，提高了数学推理能力。

    

    工具增强型大型语言模型(TALM)已知能够增强大型语言模型(LLM)的技能，从而提高它们在许多任务上的推理能力。本文提出了一种名为MATHSENSEI的工具增强型大型语言模型，用于数学推理。通过添加用于知识检索（Bing Web Search）、程序执行（Python）和符号方程求解（Wolfram-Alpha）的工具，我们通过对数学推理数据集进行评估来研究这些工具的互补优势。

    arXiv:2402.17231v1 Announce Type: new  Abstract: Tool-augmented Large Language Models (TALM) are known to enhance the skillset of large language models (LLM), thereby, leading to their improved reasoning abilities across many tasks. While, TALMs have been successfully employed in different question-answering benchmarks, their efficacy on complex mathematical reasoning benchmarks, and the potential complimentary benefits offered by tools for knowledge retrieval and mathematical equation solving, are open research questions. In this work, we present MATHSENSEI, a tool-augmented large language model for mathematical reasoning. Augmented with tools for knowledge retrieval (Bing Web Search), program execution (Python), and symbolic equation solving (Wolfram-Alpha), we study the complimentary benefits of these tools through evaluations on mathematical reasoning datasets. We perform exhaustive ablations on MATH,a popular dataset for evaluating mathematical reasoning on diverse mathematical di
    
[^75]: KorNAT：韩国社会价值观和常识的LLM对齐基准

    KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge

    [https://arxiv.org/abs/2402.13605](https://arxiv.org/abs/2402.13605)

    KorNAT是首个用于评估韩国国家对齐的基准，包括社会价值观和常识对齐两个方面，通过对社会价值和常识多项选择题的测试来评估模型的对齐程度。

    

    对于大型语言模型（LLMs）在特定国家得以有效部署，它们必须具有对该国文化和基本知识的理解。为此，我们引入了国家对齐（National Alignment），从社会价值观对齐和常识对齐两个方面衡量LLM与目标国家之间的对齐。社会价值观对齐评估模型对特定国家社会价值观的理解程度，而常识对齐则检验模型对相关基本国家知识的把握情况。我们构建了KorNAT，这是首个衡量与韩国国家对齐的基准。对于社会价值数据集，我们从包括6174名韩国参与者在内的大规模调查中获得了地面真实标签。对于常识数据集，我们基于韩国教科书和GED参考资料构建了样本。KorNAT包含4K和6K个针对社会价值和常识的多项选择题。

    arXiv:2402.13605v1 Announce Type: new  Abstract: For Large Language Models (LLMs) to be effectively deployed in a specific country, they must possess an understanding of the nation's culture and basic knowledge. To this end, we introduce National Alignment, which measures an alignment between an LLM and a targeted country from two aspects: social value alignment and common knowledge alignment. Social value alignment evaluates how well the model understands nation-specific social values, while common knowledge alignment examines how well the model captures basic knowledge related to the nation. We constructed KorNAT, the first benchmark that measures national alignment with South Korea. For the social value dataset, we obtained ground truth labels from a large-scale survey involving 6,174 unique Korean participants. For the common knowledge dataset, we constructed samples based on Korean textbooks and GED reference materials. KorNAT contains 4K and 6K multiple-choice questions for socia
    
[^76]: MatPlotAgent: 基于LLM的Agent科学数据可视化方法与评估

    MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization

    [https://arxiv.org/abs/2402.11453](https://arxiv.org/abs/2402.11453)

    MatPlotAgent介绍了一种基于LLM的自动化科学数据可视化框架，包括查询理解、代码生成和视觉反馈机制，并引入了MatPlotBench基准和GPT-4V评分方法。

    

    科学数据可视化通过直接展示复杂信息并帮助研究人员识别隐含模式，在研究中起着至关重要的作用。尽管其重要性，但对于使用Large Language Models（LLMs）进行科学数据可视化的研究仍较为未被探索。在这项研究中，我们介绍了MatPlotAgent，一种高效、与模型无关的LLM代理框架，旨在自动化科学数据可视化任务。MatPlotAgent利用代码LLMs和多模态LLMs的能力，由三个核心模块组成：查询理解、带有迭代调试的代码生成，以及用于错误更正的视觉反馈机制。为了解决该领域缺乏基准的问题，我们提出了MatPlotBench，一个由100个经人工验证的测试案例组成的高质量基准。此外，我们介绍了一种利用GPT-4V进行自动评估的评分方法。实验结果表明…（未完整）

    arXiv:2402.11453v1 Announce Type: new  Abstract: Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules: query understanding, code generation with iterative debugging, and a visual feedback mechanism for error correction. To address the lack of benchmarks in this field, we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V for automatic evaluation. Experimental results demonstrate t
    
[^77]: 名词短语中头部的最佳位置。指示语、数词、形容词和名词的案例。

    The optimal placement of the head in the noun phrase. The case of demonstrative, numeral, adjective and noun

    [https://arxiv.org/abs/2402.10311](https://arxiv.org/abs/2402.10311)

    本研究旨在探讨句法依赖距离最小化与意外减少最小化原则在名词短语中的冲突，结论显示当涉及的单词较少且单词较短时，意外减少可能会超越句法依赖距离优化。

    

    一句话的词序由多种原则塑造。句法依赖距离最小化原则与意外减少最小化原则（或可预测性最大化）在单一头部的句法依赖结构中存在冲突：前者预测头部应该放置在线性排列的中心，后者预测头部应该放置在两端之一（要么在首位，要么在末位）。一个关键问题是何时意外减少（或可预测性最大化）应该超越句法依赖距离最小化。在单一头部结构的背景下，预测在满足两个条件时更有可能发生，即（a）涉及的单词较少，并且（b）单词较短。在这里，我们在由指示语、数词、形容词和名词组成的名词短语上测试了这一预测。我们发现，在首选顺序中...（缺失部分无法提供完整翻译）

    arXiv:2402.10311v1 Announce Type: new  Abstract: The word order of a sentence is shaped by multiple principles. The principle of syntactic dependency distance minimization is in conflict with the principle of surprisal minimization (or predictability maximization) in single head syntactic dependency structures: while the former predicts that the head should be placed at the center of the linear arrangement, the latter predicts that the head should be placed at one of the ends (either first or last). A critical question is when surprisal minimization (or predictability maximization) should surpass syntactic dependency distance minimization. In the context of single head structures, it has been predicted that this is more likely to happen when two conditions are met, i.e. (a) fewer words are involved and (b) words are shorter. Here we test the prediction on the noun phrase when its composed of a demonstrative, a numeral, an adjective and a noun. We find that, across preferred orders in l
    
[^78]: 用可解释的风险预测方法降低诊断错误

    Towards Reducing Diagnostic Errors with Interpretable Risk Prediction

    [https://arxiv.org/abs/2402.10109](https://arxiv.org/abs/2402.10109)

    本研究提出了一种使用LLMs方法来识别病人电子病历数据中指示特定诊断风险增加或减少的证据的方法，旨在通过增加证据的获取与减少诊断错误来降低诊断错误。模型使用神经加性模型进行预测，以证据为后盾，并给出个体化风险估计，特别针对诊断延迟和来自不完整鉴别的错误进行优化。

    

    许多诊断错误发生是因为临床医生无法轻易获取病人电子病历中的相关信息。本研究提出了一种使用LLMs方法来识别病人电子病历数据中指示特定诊断风险增加或减少的证据的方法，最终目标是增加证据的获取与减少诊断错误。我们提出了一种神经加性模型来进行带有个体化风险估计的以证据为后盾的预测，在临床医生仍然不确定的时间点上，旨在特别减轻诊断延迟和源于不完整鉴别的错误。为了训练这样一个模型，需要推断出事件性的“真实”诊断的时间粒度细致的回顾性标签。我们使用LLMs来保证输入文本是在可以进行自信的诊断之前的。我们使用LLMs来检索初始的证据池，然后进行细化。

    arXiv:2402.10109v1 Announce Type: new  Abstract: Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual "true" diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refin
    
[^79]: 攻击、防御和评估LLM对话安全性的调查

    Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey

    [https://arxiv.org/abs/2402.09283](https://arxiv.org/abs/2402.09283)

    这篇调查提供了LLM对话安全性的全面概述，涵盖了攻击、防御和评估三个关键方面，旨在提高对该主题的理解并促进进一步的研究。

    

    arXiv:2402.09283v1 公告类型: 新的摘要: 大型语言模型（LLMs）在对话应用中已经很常见。然而，它们可能被误用生成有害回复的风险引起了严重的社会关切，并激发了LLM对话安全性的最新研究。因此，在此调查中，我们提供了最近研究的全面概述，涵盖了LLM对话安全性的三个关键方面：攻击、防御和评估。我们的目标是提供一个结构化的摘要，增进对LLM对话安全性的理解，并鼓励进一步研究这一重要课题。为了方便参考，我们根据我们的分类法对所有在此调查中提到的研究进行了分类，可在以下网址找到：https://github.com/niconi19/LLM-conversation-safety。

    arXiv:2402.09283v1 Announce Type: new Abstract: Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.
    
[^80]: 检索以解释：基于语言模型的证据驱动预测

    Retrieve to Explain: Evidence-driven Predictions with Language Models

    [https://arxiv.org/abs/2402.04068](https://arxiv.org/abs/2402.04068)

    检索以解释（R2E）是一种基于语言模型的检索方法，通过使用Shapley值确定证据的相对重要性，从而在黑盒模型中提供了可解释性，通过应用于药物靶点鉴定任务中，R2E模型在预测临床试验结果方面优于传统基因学方法。

    

    机器学习模型，尤其是语言模型，往往难以深入分析。黑盒模型可能掩盖了模型训练中的问题和有害偏差。对于人机协作过程来说，不透明的预测可能导致缺乏信任，限制模型的影响，即使模型的性能很好。为了解决这些问题，我们引入了检索以解释（Retrieve to Explain，简称R2E）。R2E是一种基于检索的语言模型，根据文档语料库中的证据，使用Shapley值来确定证据对最终预测的相对重要性，并根据自然语言模板将结构化数据纳入其中。R2E能够在不重新训练的情况下适应新的证据，并且能够通过模板化将结构化数据纳入到自然语言中。我们在通过分析已发表的科学文献进行药物靶点鉴定的实际案例中进行了评估，结果显示该模型在预测临床试验结果方面优于行业标准的基因学方法。

    Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.
    
[^81]: DRESS：通过自然语言反馈指导大型视觉-语言模型与人类对齐和互动

    DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback

    [https://arxiv.org/abs/2311.10081](https://arxiv.org/abs/2311.10081)

    提出了一种名为DRESS的大型视觉语言模型，通过利用自然语言反馈来增强模型与人类之间的对齐和互动，解决了当前大型视觉-语言模型存在的对齐和多轮对话互动方面的两个关键问题

    

    我们提出了DRESS，一种大型视觉语言模型（LVLM），它创新性地利用来自大型语言模型的自然语言反馈（NLF），通过解决当前LVLM中存在的两个关键限制来增强其对齐和互动。首先，先前的LVLM通常仅依赖指导微调阶段来增强与人类偏好的对齐。如果不加入额外反馈，它们仍然容易生成无用、虚构或有害的响应。其次，虽然视觉指导调优数据通常以多轮对话格式结构化，但连续对话轮之间的连接和依赖关系较弱。这降低了有效多轮互动的能力。为了解决这些问题，我们提出了将NLF分为两种关键类型的新颖分类：批评和改进。批评性NLF识别响应的优点和缺点，并用于对齐

    arXiv:2311.10081v2 Announce Type: replace-cross  Abstract: We present DRESS, a large vision language model (LVLM) that innovatively exploits Natural Language feedback (NLF) from Large Language Models to enhance its alignment and interactions by addressing two key limitations in the state-of-the-art LVLMs. First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction tuning data is generally structured in a multi-turn dialogue format, the connections and dependencies among consecutive conversational turns are weak. This reduces the capacity for effective multi-turn interactions. To tackle these, we propose a novel categorization of the NLF into two key types: critique and refinement. The critique NLF identifies the strengths and weaknesses of the responses and is used to align 
    
[^82]: 医生们知道如何提醒吗？临床笔记生成中自动提示优化帮助的需求

    Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation

    [https://arxiv.org/abs/2311.09684](https://arxiv.org/abs/2311.09684)

    本研究提出了自动提示优化（APO）框架，评估了不同提示对于大型语言模型在临床笔记生成中性能的影响，结果表明GPT4 APO在标准化提升质量方面表现出色，并强调了专家定制化对于内容质量的价值。

    

    本研究考察了提示工程对大型语言模型（LLMs）在临床笔记生成中性能的影响。我们引入了一个自动提示优化（APO）框架来改进初始提示，并比较了医学专家、非医学专家以及经过APO增强的GPT3.5和GPT4的输出。结果突显了GPT4 APO在标准化临床笔记各节提示质量方面的卓越性能。人在环中方法显示，专家在APO后保持内容质量，但更偏好自己的修改，表明了专家定制的价值。我们建议采用两阶段优化过程，利用APO-GPT4确保一致性，同时结合专家输入进行个性化。

    arXiv:2311.09684v2 Announce Type: replace-cross  Abstract: This study examines the effect of prompt engineering on the performance of Large Language Models (LLMs) in clinical note generation. We introduce an Automatic Prompt Optimization (APO) framework to refine initial prompts and compare the outputs of medical experts, non-medical experts, and APO-enhanced GPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in standardizing prompt quality across clinical note sections. A human-in-the-loop approach shows that experts maintain content quality post-APO, with a preference for their own modifications, suggesting the value of expert customization. We recommend a two-phase optimization process, leveraging APO-GPT4 for consistency and expert input for personalization.
    
[^83]: LifeTox：揭示生活建议中的隐性毒性

    LifeTox: Unveiling Implicit Toxicity in Life Advice

    [https://arxiv.org/abs/2311.09585](https://arxiv.org/abs/2311.09585)

    LifeTox 数据集针对各种求助场景中的隐性毒性设计，RoBERTa 在该数据集上的表现不仅匹敌了大型语言模型的零次性能，甚至超过了，强调了 LifeTox 在解决隐性毒性中的有效性。

    

    随着大型语言模型越来越多地融入日常生活中，检测不同背景下的隐性毒性至关重要。为此，我们介绍了LifeTox，这是一个旨在识别各种求助场景中的隐性毒性的数据集。与现有的安全数据集不同，LifeTox包含了通过开放性问题从个人经验中衍生出的多样上下文。实验表明，在LifeTox上微调的RoBERTa在毒性分类任务中与大型语言模型的零次性能相匹敌甚至超越。这些结果凸显了LifeTox在解决隐性毒性固有的复杂挑战中的功效。我们开源了数据集和LifeTox的监督家族；350M、7B和13B。

    arXiv:2311.09585v2 Announce Type: replace  Abstract: As large language models become increasingly integrated into daily life, detecting implicit toxicity across diverse contexts is crucial. To this end, we introduce LifeTox, a dataset designed for identifying implicit toxicity within a broad range of advice-seeking scenarios. Unlike existing safety datasets, LifeTox comprises diverse contexts derived from personal experiences through open-ended questions. Experiments demonstrate that RoBERTa fine-tuned on LifeTox matches or surpasses the zero-shot performance of large language models in toxicity classification tasks. These results underscore the efficacy of LifeTox in addressing the complex challenges inherent in implicit toxicity. We open-sourced the dataset\footnote{\url{https://huggingface.co/datasets/mbkim/LifeTox}} and the LifeTox moderator family; 350M, 7B, and 13B.
    
[^84]: 翻译的上下文细化：句子和文档级后编辑的大型语言模型

    Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing

    [https://arxiv.org/abs/2310.14855](https://arxiv.org/abs/2310.14855)

    大型语言模型在神经机器翻译中表现尚未达到最先进水平，提出使用LLM作为自动后编辑器(APE)的替代方法，同时探索了扩展到文档级翻译的可能性。

    

    大型语言模型(LLM)已在各种自然语言处理任务中取得了相当大的成功，但它们尚未在神经机器翻译(NMT)中达到最先进的性能。然而，在要求广泛理解和上下文处理的任务中表现出的显著性能显示了它们在翻译中的潜力。为了利用这些能力，我们研究了使用LLM进行MT，并探索了最近的参数高效微调技术。令人惊讶的是，我们的初步实验发现，即使为翻译目的微调，也会导致性能下降。为了克服这一问题，我们提出了一种替代方法：将LLM作为自动后编辑器(APE)而不是直接转换器。基于LLM处理和生成长序列的异常能力，我们还提出将我们的方法扩展到文档级翻译。我们展示了利用低秩适配器进行 ...

    arXiv:2310.14855v2 Announce Type: replace-cross  Abstract: Large Language Models (LLM's) have demonstrated considerable success in various Natural Language Processing tasks, but they have yet to attain state-of-the-art performance in Neural Machine Translation (NMT). Nevertheless, their significant performance in tasks demanding a broad understanding and contextual processing shows their potential for translation. To exploit these abilities, we investigate using LLM's for MT and explore recent parameter-efficient fine-tuning techniques. Surprisingly, our initial experiments find that fine-tuning for translation purposes even led to performance degradation. To overcome this, we propose an alternative approach: adapting LLM's as Automatic Post-Editors (APE) rather than direct translators. Building on the LLM's exceptional ability to process and generate lengthy sequences, we also propose extending our approach to document-level translation. We show that leveraging Low-Rank-Adapter fine-t
    
[^85]: 令人惊叹但令人困惑：用假设细化测试语言模型的归纳推理能力

    Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement

    [https://arxiv.org/abs/2310.08559](https://arxiv.org/abs/2310.08559)

    对语言模型进行的系统研究揭示了它们在假设提出方面表现惊人，并且通过与一个（任务特定的）符号解释器相结合，能够系统地过滤可能性。

    

    从少数观察中推导出潜在原则，然后推广到新情况的能力，即归纳推理，对于人类智能至关重要。之前的研究表明，尽管在研究基准上取得了令人印象深刻的成功，但语言模型（LMs）在归纳推理方面常常表现不佳。在这项工作中，我们通过迭代假设细化这一技术对LMs的归纳推理能力进行了系统研究，该技术更接近人类归纳过程，而不是标准的输入-输出提示。

    arXiv:2310.08559v3 Announce Type: replace-cross  Abstract: The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the pr
    
[^86]: 错误范数截断：针对数据噪音的文本生成模型的鲁棒训练

    Error Norm Truncation: Robust Training in the Presence of Data Noise for Text Generation Models

    [https://arxiv.org/abs/2310.00840](https://arxiv.org/abs/2310.00840)

    提出了错误范数截断（ENT）方法，通过考虑非目标标记的分布从而提供更准确的数据截断，证实在文本生成模型中应用ENT可以改善生成质量。

    

    文本生成模型在训练数据存在错误时往往容易受到影响。随着海量网络抓取数据的普遍可用，我们如何增强在大量嘈杂的网络抓取文本上训练的模型的鲁棒性呢？在我们的研究中，我们提出了一种名为错误范数截断（ENT）的鲁棒增强方法，可以截断嘈杂的数据。与仅使用负对数似然损失来估计数据质量的方法相比，我们的方法通过考虑非目标标记的分布，提供了更准确的估计，这在以往的研究中经常被忽略。通过对语言建模、机器翻译和文本摘要的全面实验，我们展示了将文本生成模型配备错误范数截断能够改善生成质量，优于标准训练和先前软截断和硬截断方法。

    arXiv:2310.00840v2 Announce Type: replace  Abstract: Text generation models are notoriously vulnerable to errors in the training data. With the wide-spread availability of massive amounts of web-crawled data becoming more commonplace, how can we enhance the robustness of models trained on a massive amount of noisy web-crawled text? In our work, we propose Error Norm Truncation (ENT), a robust enhancement method to the standard training objective that truncates noisy data. Compared to methods that only uses the negative log-likelihood loss to estimate data quality, our method provides a more accurate estimation by considering the distribution of non-target tokens, which is often overlooked by previous work. Through comprehensive experiments across language modeling, machine translation, and text summarization, we show that equipping text generation models with ENT improves generation quality over standard training and previous soft and hard truncation methods. Furthermore, we show that 
    
[^87]: BAMBOO：用于评估大语言模型长文本建模能力的全面基准

    BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models

    [https://arxiv.org/abs/2309.13345](https://arxiv.org/abs/2309.13345)

    提出了BAMBOO基准来全面评估大语言模型对长文本的建模能力，包含10个数据集从5个不同长文本理解任务中提取，涵盖了LLMs的核心能力和各个领域。

    

    大语言模型（LLMs）已经在处理普通长度的NLP任务中取得了惊人的熟练度。最近，多项研究致力于扩展上下文长度，并增强LLMs的长文本建模能力。为了全面评估LLMs的长上下文能力，我们提出了BAMBOO，一个多任务长上下文基准。BAMBOO设计之初考虑了四个原则：全面容量评估、避免数据污染、准确的自动评估以及不同长度级别。它由来自5个不同长文本理解任务的10个数据集组成，即问答、幻觉检测、文本排序、语言建模和代码补全，以涵盖LLMs的核心能力和各个领域。我们在BAMBOO上使用五个长上下文模型进行实验，并进一步讨论了长文本的四个关键研究问题。我们还对当前的长上下文模型进行了定性分析。

    arXiv:2309.13345v2 Announce Type: replace  Abstract: Large language models (LLMs) have achieved dramatic proficiency over NLP tasks with normal length. Recently, multiple studies have committed to extending the context length and enhancing the long text modeling capabilities of LLMs. To comprehensively evaluate the long context ability of LLMs, we propose BAMBOO, a multi-task long context benchmark. BAMBOO has been designed with four principles: comprehensive capacity evaluation, avoidance of data contamination, accurate automatic evaluation, and different length levels. It consists of 10 datasets from 5 different long text understanding tasks, i.e. question answering, hallucination detection, text sorting, language modeling, and code completion, to cover core capacities and various domains of LLMs. We conduct experiments with five long context models on BAMBOO and further discuss four key research questions of long text. We also qualitatively analyze current long context models and po
    
[^88]: 语言建模即为压缩

    Language Modeling Is Compression

    [https://arxiv.org/abs/2309.10668](https://arxiv.org/abs/2309.10668)

    大型语言模型被证明是强大的压缩器，压缩视角为扩展定律、标记化和上下文学习提供了新的见解。

    

    早已确立了预测模型可以转化为无损压缩器，反之亦然。近年来，机器学习社区集中精力训练越来越大、越来越强大的自监督（语言）模型。大型语言模型表现出令人印象深刻的预测能力，因此它们有望成为强大的压缩器。在这项工作中，我们主张通过压缩的视角来看待预测问题，并评估大型（基础）模型的压缩能力。我们展示了大型语言模型是强大的通用预测器，压缩视角提供了有关扩展定律、标记化和上下文学习的新见解。例如，Chinchilla 70B，虽然主要在文本上训练，但可以将ImageNet的补丁压缩为其原始大小的43.4%，将LibriSpeech样本压缩为其原始大小的16.4%，超越了特定领域的压缩器。

    arXiv:2309.10668v2 Announce Type: replace-cross  Abstract: It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors li
    
[^89]: EasyEdit：一种易于使用的大型语言模型知识编辑框架

    EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models

    [https://arxiv.org/abs/2308.07269](https://arxiv.org/abs/2308.07269)

    EasyEdit提出了一种易于使用的知识编辑框架，针对大型语言模型的知识截断或谬误问题，支持各种最新的知识编辑方法，并可应用于多个知名的LLMs。

    

    大型语言模型（LLMs）通常遭受知识截断或谬误问题，这意味着它们对未见事件不知情或生成具有不正确事实的文本，原因是数据过时/嘈杂。为此，出现了许多针对LLMs的知识编辑方法，旨在微妙地注入/编辑更新的知识或调整不良行为，同时将对不相关输入的影响最小化。然而，由于各种知识编辑方法之间存在显著差异，以及任务设置中的变化，社区中没有可用于知识编辑的标准实施框架，这妨碍了从业者将知识编辑应用于应用程序。为解决这些问题，我们提出了EasyEdit，一种易于使用的LLMs知识编辑框架。它支持各种尖端的知识编辑方法，并可以轻松应用于许多著名的LLMs，如T5、GPT-J、LlaMA等。从经验上来看，我们报告了kno

    arXiv:2308.07269v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners from applying knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the kno
    
[^90]: HateModerate：针对内容审查政策测试仇恨言论检测器

    HateModerate: Testing Hate Speech Detectors against Content Moderation Policies

    [https://arxiv.org/abs/2307.12418](https://arxiv.org/abs/2307.12418)

    该论文通过创建HateModerate数据集来测试自动内容审核员对内容政策的符合度，揭示了现有仇恨言论检测器在此方面存在的重大失败。

    

    为了保护用户免受大量仇恨内容的侵害，现有研究关注自动化仇恨言论检测。尽管已经做出努力，但仍有一个问题：自动化仇恨言论检测器是否符合社交媒体内容政策？平台的内容政策是社交媒体平台审查的内容清单。由于内容审查规则通常是独特定义的，现有的仇恨言论数据集不能直接回答这个问题。这项工作试图通过创建HateModerate数据集来回答这个问题，用于测试自动化内容审核员对内容政策的行为。首先，我们让28名注释员和GPT参与六步骤注释过程，得出一份恶毒和非恶毒的测试套件清单，与Facebook的41条仇恨言论政策相匹配。其次，我们测试最先进的仇恨言论检测器对HateModerate的表现，揭示了这些模型的相当失败。

    arXiv:2307.12418v2 Announce Type: replace-cross  Abstract: To protect users from massive hateful content, existing works studied automated hate speech detection. Despite the existing efforts, one question remains: do automated hate speech detectors conform to social media content policies? A platform's content policies are a checklist of content moderated by the social media platform. Because content moderation rules are often uniquely defined, existing hate speech datasets cannot directly answer this question.   This work seeks to answer this question by creating HateModerate, a dataset for testing the behaviors of automated content moderators against content policies. First, we engage 28 annotators and GPT in a six-step annotation process, resulting in a list of hateful and non-hateful test suites matching each of Facebook's 41 hate speech policies. Second, we test the performance of state-of-the-art hate speech detectors against HateModerate, revealing substantial failures these mod
    
[^91]: 通过最大化变分互信息的自编码器改进生成多样性的VOLTA

    VOLTA: Improving Generative Diversity by Variational Mutual Information Maximizing Autoencoder

    [https://arxiv.org/abs/2307.00852](https://arxiv.org/abs/2307.00852)

    VOLTA通过Transformer与VAE框架的更有效连接，InfoGAN风格潜在编码以及支持离散输入，提升了生成多样性

    

    自然语言生成领域得益于Transformer模型取得了巨大成功。虽然它们实现了最先进的生成质量，但往往忽视了生成多样性。先前尝试解决这一问题的方法要么容量较低，要么结构过于复杂。一些最近的方法采用VAE框架增强多样性，但它们的潜在变量完全依赖于输入上下文，限制了潜在空间的探索。在本文中，我们介绍了VOLTA，通过更有效的基于交叉注意力的连接将Transformer与VAE联系起来，从传统的嵌入连接或求和中脱颖而出，提升了生成多样性。此外，我们提议整合InfoGAN风格的潜在编码以实现输入独立的变化性，进一步使生成多样化。此外，我们的框架除了支持现有的连续输入外，还支持离散输入。

    arXiv:2307.00852v2 Announce Type: replace  Abstract: The natural language generation domain has witnessed great success thanks to Transformer models. Although they have achieved state-of-the-art generative quality, they often neglect generative diversity. Prior attempts to tackle this issue suffer from either low model capacity or over-complicated architectures. Some recent methods employ the VAE framework to enhance diversity, but their latent variables fully depend on the input context, restricting exploration of the latent space. In this paper, we introduce VOLTA, a framework that elevates generative diversity by bridging Transformer with VAE via a more effective cross-attention-based connection, departing from conventional embedding concatenation or summation. Additionally, we propose integrating InfoGAN-style latent codes to enable input-independent variability, further diversifying the generation. Moreover, our framework accommodates discrete inputs alongside its existing support
    
[^92]: Radiology-GPT：用于放射学的大型语言模型

    Radiology-GPT: A Large Language Model for Radiology

    [https://arxiv.org/abs/2306.08666](https://arxiv.org/abs/2306.08666)

    Radiology-GPT是一个专门为放射学设计的大型语言模型，通过使用指导调整方法进行训练，在放射学诊断、研究和沟通方面展示出优越性能，为临床NLP的未来发展提供推动力。

    

    我们介绍了Radiology-GPT，一个用于放射学的大型语言模型。通过在广泛的放射学领域知识数据集上采用指导调整方法，Radiology-GPT表现出比StableLM、Dolly和LLaMA等通用语言模型更优越的性能。它在放射学诊断、研究和沟通方面展现出显著的多功能性。这项工作为临床自然语言处理领域的未来发展提供了催化剂。Radiology-GPT的成功实施表明了定位生成型大型语言模型的潜力，特别是为独特的医学专业定制，同时确保遵守HIPAA等隐私标准。开发针对各家医院特定需求的个性化大型语言模型的前景呈现出一个有前途的方向。这些模型中融合的会话能力和领域特定知识被设定为促进

    arXiv:2306.08666v2 Announce Type: replace-cross  Abstract: We introduce Radiology-GPT, a large language model for radiology. Using an instruction tuning approach on an extensive dataset of radiology domain knowledge, Radiology-GPT demonstrates superior performance compared to general language models such as StableLM, Dolly and LLaMA. It exhibits significant versatility in radiological diagnosis, research, and communication. This work serves as a catalyst for future developments in clinical NLP. The successful implementation of Radiology-GPT is indicative of the potential of localizing generative large language models, specifically tailored for distinctive medical specialties, while ensuring adherence to privacy standards such as HIPAA. The prospect of developing individualized, large-scale language models that cater to specific needs of various hospitals presents a promising direction. The fusion of conversational competence and domain-specific knowledge in these models is set to foste
    
[^93]: 句义角色标注引导的领域外检测

    Semantic Role Labeling Guided Out-of-distribution Detection

    [https://arxiv.org/abs/2305.18026](https://arxiv.org/abs/2305.18026)

    本文提出了一种新的无监督领域外检测方法SRLOOD，通过语义角色标注引导的方式从句子的不同论点中分离、提取和学习局部特征表示，并结合全局特征表示，以边缘为基础进行对比损失。

    

    在自然语言处理中，识别意外的领域转移实例对于实际应用至关重要。以前的工作通过利用单一的全局特征嵌入来表示句子来识别领域外（OOD）实例，但不能很好地刻画微妙的OOD模式。本文提出一种新的无监督OOD检测方法，即句义角色标注引导的领域外检测（SRLOOD），通过基于边缘的对比损失将句子不同论点的语义角色标注（SRL）引导的细粒度局部特征表示与句子的全局特征表示分离、提取并学习。

    arXiv:2305.18026v2 Announce Type: replace  Abstract: Identifying unexpected domain-shifted instances in natural language processing is crucial in real-world applications. Previous works identify the out-of-distribution (OOD) instance by leveraging a single global feature embedding to represent the sentence, which cannot characterize subtle OOD patterns well. Another major challenge current OOD methods face is learning effective low-dimensional sentence representations to identify the hard OOD instances that are semantically similar to the in-distribution (ID) data. In this paper, we propose a new unsupervised OOD detection method, namely Semantic Role Labeling Guided Out-of-distribution Detection (SRLOOD), that separates, extracts, and learns the semantic role labeling (SRL) guided fine-grained local feature representations from different arguments of a sentence and the global feature representations of the full sentence using a margin-based contrastive loss. A novel self-supervised ap
    
[^94]: 深度模态对齐和自监督多任务学习中的多模态情感分析中的共享和私有信息学习

    Shared and Private Information Learning in Multimodal Sentiment Analysis with Deep Modal Alignment and Self-supervised Multi-Task Learning

    [https://arxiv.org/abs/2305.08473](https://arxiv.org/abs/2305.08473)

    该论文提出了一种深度模态共享信息学习模块和基于自监督学习策略的标签生成模块，用于在多模态情感分析中学习共享和私有信息，可根据参数化调整不同模态之间的信息交换关系。

    

    在多模态情感分析任务中设计一个有效的表示学习方法是一个关键的研究方向。挑战在于学习完整模态表示中的共享信息和私有信息，这在统一的多模态标签和原始特征融合方法中是困难的。在这项工作中，我们提出了一个基于协方差矩阵的深度模态共享信息学习模块，用于捕捉模态之间的共享信息。此外，我们使用了基于自监督学习策略的标签生成模块来捕捉模态的私有信息。我们的模块在多模态任务中是即插即用的，通过改变参数化，可以调整模态之间的信息交换关系，并学习指定模式之间的私有或共享信息。我们还采用了多任务学习策略，帮助模型将注意力集中在模态d上。

    arXiv:2305.08473v2 Announce Type: replace  Abstract: Designing an effective representation learning method for multimodal sentiment analysis tasks is a crucial research direction. The challenge lies in learning both shared and private information in a complete modal representation, which is difficult with uniform multimodal labels and a raw feature fusion approach. In this work, we propose a deep modal shared information learning module based on the covariance matrix to capture the shared information between modalities. Additionally, we use a label generation module based on a self-supervised learning strategy to capture the private information of the modalities. Our module is plug-and-play in multimodal tasks, and by changing the parameterization, it can adjust the information exchange relationship between the modes and learn the private or shared information between the specified modes. We also employ a multi-task learning strategy to help the model focus its attention on the modal d
    
[^95]: LAGAN: 使用条件生成对抗神经网络进行深度半监督语言人类学分类

    LAGAN: Deep Semi-Supervised Linguistic-Anthropology Classification with Conditional Generative Adversarial Neural Network

    [https://arxiv.org/abs/2301.13853](https://arxiv.org/abs/2301.13853)

    使用条件生成对抗神经网络开发了一个LA-GAN算法，用于分类学生参与中的语言民族志特征，探讨少数民族教育中的学习方式和学习方法

    

    教育是每个人的权利，然而，每个个体都与其他人不同。在后共产主义时代，教师发现固有的个人主义以同样的方式培训所有人朝向第四次工业革命的工作市场。我们可以考虑学术实践中的少数民族教育场景。少数民族群体在自己的文化中成长，并希望用自己的方式教授。我们将这种语言人类学（人们如何学习）为基础的参与形式建立为半监督问题。然后，我们开发了一种条件深度生成对抗网络算法，即LA-GAN，用于分类学生参与中的语言民族志特征。理论上的证明证实了我们的半监督对抗模型的客观性、正则化和损失函数。准备了调查问题，以了解z代和少数族群的一些假设，包括他们的学习风格、学习方法

    arXiv:2301.13853v2 Announce Type: replace  Abstract: Education is a right of all, however, every individual is different than others. Teachers in post-communism era discover inherent individualism to equally train all towards job market of fourth industrial revolution. We can consider scenario of ethnic minority education in academic practices. Ethnic minority group has grown in their own culture and would prefer to be taught in their native way. We have formulated such linguistic anthropology(how people learn)based engagement as semi-supervised problem. Then, we have developed an conditional deep generative adversarial network algorithm namely LA-GAN to classify linguistic ethnographic features in student engagement. Theoretical justification proves the objective, regularization and loss function of our semi-supervised adversarial model. Survey questions are prepared to reach some form of assumptions about z-generation and ethnic minority group, whose learning style, learning approach
    
[^96]: 重访大语言模型时代下的零-shot 抽象摘要，从位置偏见的角度出发

    Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias. (arXiv:2401.01989v1 [cs.CL])

    [http://arxiv.org/abs/2401.01989](http://arxiv.org/abs/2401.01989)

    这项研究通过测量位置偏见，重访了大语言模型中的零-shot 抽象摘要。研究结果揭示了模型不公平地优先考虑某些部分的信息，从而导致不可取的行为。对多个LLM模型和预训练抽象摘要模型进行的实验提供了关于零-shot 总结任务的模型性能和位置偏见的新见解和讨论。

    

    我们通过测量位置偏见来表征和研究大型语言模型（LLMs）中的零-shot 抽象摘要，我们将其视为先前文献中研究过的更为限制性的引导偏见现象的一般表述。位置偏见捕捉到模型在输入文本的某些部分上不公平地优先考虑信息，导致不可取的行为。通过对四个不同的真实数据集进行大量实验，我们研究了多个LLM模型如GPT 3.5-Turbo，Llama-2和Dolly-v2中的位置偏见，以及当前最先进的预训练编码器-解码器抽象摘要模型如Pegasus和BART。我们的发现为零-shot 总结任务的模型性能和位置偏见提供了新的见解和讨论。

    We characterize and study zero-shot abstractive summarization in Large Language Models (LLMs) by measuring position bias, which we propose as a general formulation of the more restrictive lead bias phenomenon studied previously in the literature. Position bias captures the tendency of a model unfairly prioritizing information from certain parts of the input text over others, leading to undesirable behavior. Through numerous experiments on four diverse real-world datasets, we study position bias in multiple LLM models such as GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained encoder-decoder abstractive summarization models such as Pegasus and BART. Our findings lead to novel insights and discussion on performance and position bias of models for zero-shot summarization tasks.
    
[^97]: TiC-CLIP: CLIP模型的持续训练

    TiC-CLIP: Continual Training of CLIP Models. (arXiv:2310.16226v1 [cs.CV])

    [http://arxiv.org/abs/2310.16226](http://arxiv.org/abs/2310.16226)

    该论文提出了用于训练视觉-语言模型的大规模时间连续 (TiC) 基准，使用这些基准评估了现有模型的时间鲁棒性，并展示了一种简单有效的排练方法来持续训练模型。

    

    保持大型基础模型与最新数据保持同步本身就是昂贵的。为了避免不断重新训练的高成本，持续训练这些模型至关重要。这个问题被缺乏大规模连续学习基准或基线所加剧。我们引入了用于训练视觉-语言模型的第一批 Web 规模时间连续（TiC）基准：TiC-DataCompt、TiC-YFCC 和 TiC-RedCaps，其中包含超过 127 亿个时间戳图像-文本对，跨越了 9 年的时间（2014-2022）。我们首先使用这些基准来策划各种动态评估，以衡量现有模型的时间鲁棒性。我们展示了 OpenAI 的 CLIP 模型（使用 2020 年的数据进行训练）在我们策划的从 2021 年到 2022 年的检索任务中，失去了约 8% 的零-shot准确率，而与 OpenCLIP 存储库中最近训练的模型相比。然后，我们研究如何高效地对时间连续数据进行训练。我们证明了一种简单的排练方法，从上次的训练中继续训练，可以实现有效的训练。

    Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models. This problem is exacerbated by the lack of any large scale continual learning benchmarks or baselines. We introduce the first set of web-scale Time-Continual (TiC) benchmarks for training vision-language models: TiC-DataCompt, TiC-YFCC, and TiC-RedCaps with over 12.7B timestamped image-text pairs spanning 9 years (2014--2022). We first use our benchmarks to curate various dynamic evaluations to measure temporal robustness of existing models. We show OpenAI's CLIP (trained on data up to 2020) loses $\approx 8\%$ zero-shot accuracy on our curated retrieval task from 2021--2022 compared with more recently trained models in OpenCLIP repository. We then study how to efficiently train models on time-continuous data. We demonstrate that a simple rehearsal-based approach that continues training from the l
    
[^98]: 思考、行动和问：开放世界互动个性化机器人导航

    Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation. (arXiv:2310.07968v1 [cs.RO])

    [http://arxiv.org/abs/2310.07968](http://arxiv.org/abs/2310.07968)

    这项研究引入了零射交互个性化对象导航（ZIPON），通过使用大型语言模型（LLM）和用户反馈，解决了在未知环境中导航到个性化目标对象的问题。

    

    零射命令对象导航（ZSON）使代理能够在未知环境中导航到开放词汇对象。现有的ZSON工作主要集中在遵循个别指令以寻找通用对象类，忽略了自然语言交互的利用和识别用户特定对象的复杂性。为了解决这些局限性，我们引入了零射交互个性化对象导航（ZIPON），其中机器人需要在与用户对话的同时导航到个性化目标对象。为了解决ZIPON问题，我们提出了一种新的框架称为开放世界互动个性化导航（ORION），该框架使用大型语言模型（LLM）进行序列决策，以操作不同的感知、导航和通信模块。实验结果表明，能够利用用户反馈的互动代理的性能有了显著改进。然而，在任务完成和用户满意度之间取得良好的平衡仍然具有挑战性。

    Zero-Shot Object Navigation (ZSON) enables agents to navigate towards open-vocabulary objects in unknown environments. The existing works of ZSON mainly focus on following individual instructions to find generic object classes, neglecting the utilization of natural language interaction and the complexities of identifying user-specific objects. To address these limitations, we introduce Zero-shot Interactive Personalized Object Navigation (ZIPON), where robots need to navigate to personalized goal objects while engaging in conversations with users. To solve ZIPON, we propose a new framework termed Open-woRld Interactive persOnalized Navigation (ORION), which uses Large Language Models (LLMs) to make sequential decisions to manipulate different modules for perception, navigation and communication. Experimental results show that the performance of interactive agents that can leverage user feedback exhibits significant improvement. However, obtaining a good balance between task completion 
    
[^99]: $\mathcal{B}$-Coder: 基于价值的深度强化学习用于程序合成

    $\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis. (arXiv:2310.03173v1 [cs.CL])

    [http://arxiv.org/abs/2310.03173](http://arxiv.org/abs/2310.03173)

    $\mathcal{B}$-Coder是一种基于价值的深度强化学习方法，用于程序合成，旨在通过结合强化学习和大规模语言模型的能力，提高代码生成的准确性和执行能力。

    

    程序合成旨在从自然语言描述中创建准确可执行的代码。该领域结合了强化学习和大规模语言模型的力量，显著提高了代码生成能力。该集成主要关注直接优化功能正确性，超越了传统的监督学习损失。尽管当前文献主要支持基于策略的算法，但程序合成的属性表明与基于值的方法自然兼容。这源于人类程序员开发的丰富离线程序集合，并通过自动化单元测试对生成的程序进行直观验证（即在强化学习中容易获得奖励的语言表达）。与主要使用基于策略的算法不同，我们的工作探索了基于值的方法的适用性，从而开发了我们的$\mathcal{B}$-Coder（发音为Bellman coder）。

    Program synthesis aims to create accurate, executable code from natural language descriptions. This field has leveraged the power of reinforcement learning (RL) in conjunction with large language models (LLMs), significantly enhancing code generation capabilities. This integration focuses on directly optimizing functional correctness, transcending conventional supervised losses. While current literature predominantly favors policy-based algorithms, attributes of program synthesis suggest a natural compatibility with value-based methods. This stems from rich collection of off-policy programs developed by human programmers, and the straightforward verification of generated programs through automated unit testing (i.e. easily obtainable rewards in RL language). Diverging from the predominant use of policy-based algorithms, our work explores the applicability of value-based approaches, leading to the development of our $\mathcal{B}$-Coder (pronounced Bellman coder). Yet, training value-bas
    
[^100]: BenLLMEval: 一项对孟加拉语自然语言处理中大型语言模型潜力和缺陷的全面评估

    BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP. (arXiv:2309.13173v1 [cs.CL])

    [http://arxiv.org/abs/2309.13173](http://arxiv.org/abs/2309.13173)

    本文对大型语言模型（LLMs）在孟加拉语自然语言处理中的潜力和缺陷进行了全面评估。结果显示，LLMs在孟加拉语自然语言处理任务中表现较差，需要进一步努力开发对低资源语言中LLMs的更好理解。

    

    大型语言模型（LLMs）因其在语言生成和其他具体语言任务中的出色能力而成为自然语言处理中最重要的突破之一。尽管LLMs已在各种任务中得到评估，但大部分评估集中在英语上，尚未对孟加拉语等资源匮乏的语言进行全面评估。本文评估了LLMs在资源匮乏的孟加拉语上的性能。我们选择了各种重要且多样的孟加拉语自然语言处理任务，如抽象摘要、问答、改写、自然语言推理、文本分类和情感分析，以零-shot评估ChatGPT、LLaMA-2和Claude-2，并将性能与最先进的微调模型进行对比。我们的实验结果显示，LLMs在不同的孟加拉语自然语言处理任务中表现较差，需要进一步努力开发对低资源语言（如孟加拉语）中LLMs的更好理解。

    Large Language Models (LLMs) have emerged as one of the most important breakthroughs in natural language processing (NLP) for their impressive skills in language generation and other language-specific tasks. Though LLMs have been evaluated in various tasks, mostly in English, they have not yet undergone thorough evaluation in under-resourced languages such as Bengali (Bangla). In this paper, we evaluate the performance of LLMs for the low-resourced Bangla language. We select various important and diverse Bangla NLP tasks, such as abstractive summarization, question answering, paraphrasing, natural language inference, text classification, and sentiment analysis for zero-shot evaluation with ChatGPT, LLaMA-2, and Claude-2 and compare the performance with state-of-the-art fine-tuned models. Our experimental results demonstrate an inferior performance of LLMs for different Bangla NLP tasks, calling for further effort to develop better understanding of LLMs in low-resource languages like Ba
    
[^101]: 安全调优的LLaMAs：从提高大型语言模型遵循指令的安全性中学到的经验

    Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions. (arXiv:2309.07875v1 [cs.CL])

    [http://arxiv.org/abs/2309.07875](http://arxiv.org/abs/2309.07875)

    在训练大型语言模型遵循指令时，仅强调帮助性而不考虑安全性会导致模型产生有害内容。本研究发现，在训练LLaMA模型时添加少量安全示例可以显著提高其安全性，而不影响其能力和帮助性。然而，过度安全调优会使模型拒绝回应表面上类似于不安全提示的合理提示。

    

    训练大型语言模型遵循指令可以使它们在各种任务上表现得更好，通常更具有帮助性。然而，一个完全有用的模型会遵循甚至最恶意的指令，并轻易生成有害内容。本文关注的是那些只强调帮助性而不考虑安全性的模型的安全性问题。我们展示了一些流行的指令调优模型非常不安全。此外，我们还展示了在fine-tuning类似LLaMA的模型时，只需将3%的安全示例（几百个演示）添加到训练集中，就能显著提高其安全性。我们的安全调优并不会显著降低模型的能力或帮助性，这是通过标准基准测试来衡量的。但是，我们发现一种过度安全的行为，即过度的安全调优会使得模型拒绝对表面上类似于不安全提示的合理提示做出回应。我们的研究揭示了训练LLM模型遵循指令时的权衡关系。

    Training large language models to follow instructions makes them perform better on a wide range of tasks, generally becoming more helpful. However, a perfectly helpful model will follow even the most malicious instructions and readily generate harmful content. In this paper, we raise concerns over the safety of models that only emphasize helpfulness, not safety, in their instruction-tuning. We show that several popular instruction-tuned models are highly unsafe. Moreover, we show that adding just 3% safety examples (a few hundred demonstrations) in the training set when fine-tuning a model like LLaMA can substantially improve their safety. Our safety-tuning does not make models significantly less capable or helpful as measured by standard benchmarks. However, we do find a behavior of exaggerated safety, where too much safety-tuning makes models refuse to respond to reasonable prompts that superficially resemble unsafe ones. Our study sheds light on trade-offs in training LLMs to follow
    
[^102]: T-MARS：通过规避文本特征学习来改善视觉表示

    T-MARS: Improving Visual Representations by Circumventing Text Feature Learning. (arXiv:2307.03132v1 [cs.CV])

    [http://arxiv.org/abs/2307.03132](http://arxiv.org/abs/2307.03132)

    T-MARS提出一种新的数据筛选方法，通过规避文本特征学习，改善了视觉表示的学习，解决了大型多模态数据集中存在的文本与图像重叠的问题。

    

    大型网络来源的多模态数据集为学习通用视觉表示的新方法提供了动力，推动了计算机视觉的最新发展，并彻底改变了零样本和少样本识别。一个关键的决策问题是如何筛选这些日益庞大的数据集。本文提出了一种新的最先进的数据筛选方法，其动机是我们观察到近40%的LAION数据集的图像与说明存在重叠的文本。直觉上，这样的数据可能会浪费资源，因为它鼓励模型进行光学字符识别而不是学习视觉特征。然而，简单地将所有这些数据去除也可能浪费，因为这会丢弃包含视觉特征的图像（除了重叠的文本）。我们提出了一种简单而可扩展的方法来解决这个问题。

    Large web-sourced multimodal datasets have powered a slew of new methods for learning general-purpose visual representations, advancing the state of the art in computer vision and revolutionizing zero- and few-shot recognition. One crucial decision facing practitioners is how, if at all, to curate these ever-larger datasets. For example, the creators of the LAION-5B dataset chose to retain only image-caption pairs whose CLIP similarity score exceeded a designated threshold. In this paper, we propose a new state-of-the-art data filtering approach motivated by our observation that nearly 40% of LAION's images contain text that overlaps significantly with the caption. Intuitively, such data could be wasteful as it incentivizes models to perform optical character recognition rather than learning visual features. However, naively removing all such data could also be wasteful, as it throws away images that contain visual features (in addition to overlapping text). Our simple and scalable app
    
[^103]: DNABERT-2:多种物种基因组的高效基础模型和基准

    DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome. (arXiv:2306.15006v1 [q-bio.GN])

    [http://arxiv.org/abs/2306.15006](http://arxiv.org/abs/2306.15006)

    本研究提出了DNABERT-2，一个用于多种物种基因组的高效基础模型和基准。我们通过使用基于统计的数据压缩算法Byte Pair Encoding（BPE）替代传统的k-mer标记化，克服了k-mer标记化的计算和样本效率问题，并取得了重要进展。

    

    解码基因组的语言复杂性是生物学中一个关键问题，而DNABERT和Nucleotide Transformer等预训练基础模型在这个领域取得了重要进展。现有的工作主要依赖于k-mer作为基因组语言的标记，由于其简单性。然而，我们认为k-mer标记化引入的计算和样本效率问题是发展大规模基因组基础模型的主要障碍。我们提供了关于基因组标记化的概念和经验见解，基于此提出用基于统计的数据压缩算法Byte Pair Encoding（BPE）替代k-mer标记化，BPE通过迭代合并语料库中最频繁共同出现的基因组片段来构建标记。我们证明，BPE不仅克服了k-mer标记化的局限性，还能从非重叠标记化的计算效率中受益。

    Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenizatio
    
[^104]: 生成式多模态实体链接

    Generative Multimodal Entity Linking. (arXiv:2306.12725v1 [cs.CL])

    [http://arxiv.org/abs/2306.12725](http://arxiv.org/abs/2306.12725)

    本文提出了 GEMEL 方法，使用大规模预训练的 LLMs 直接生成目标实体名称，仅调整了极少的模型参数即可实现最先进的 MEL 实验结果。

    

    多模态实体链接是将带有多模态上下文的提及映射到知识库（例如维基百科）中的引用实体的任务。本文提出了一种名为 GEMEL 的简单而有效的生成式多模态实体链接方法，利用大规模预训练的 LLMs 直接生成目标实体名称。我们保持视觉和语言模型冻结，只训练一个线性层以启用跨模态交互。为了将 LLMs 适应 MEL 任务，我们利用 LLMs 的新兴上下文学习能力，通过检索多模态实例作为示范来进行。大量实验表明，仅调整了大约0.3％的模型参数，GEMEL 就实现了最先进的结果。

    Multimodal Entity Linking (MEL) is the task of mapping mentions with multimodal contexts to the referent entities from a knowledge base (e.g., Wikipedia). Prior MEL methods mainly focus on designing complex multimodal interaction mechanisms and require fine-tuning all model parameters, which can be prohibitively costly and difficult to scale in the era of Large Language Models (LLMs). In this work, we propose GEMEL, a simple yet effective Generative Multimodal Entity Linking method, which leverages the capabilities of LLMs from large-scale pre-training to directly generate target entity names. We keep the vision and language model frozen and only train a linear layer to enable cross-modality interactions. To adapt LLMs to the MEL task, we take advantage of the emerging in-context learning (ICL) capability of LLMs by retrieving multimodal instances as demonstrations. Extensive experiments show that with only ~0.3% of the model parameters fine-tuned, GEMEL achieves state-of-the-art resul
    
[^105]: LeTI：从文本交互中学习生成

    LeTI: Learning to Generate from Textual Interactions. (arXiv:2305.10314v1 [cs.CL])

    [http://arxiv.org/abs/2305.10314](http://arxiv.org/abs/2305.10314)

    LeTI是一种使用自然语言指令、LM生成的程序和错误消息进行串联迭代微调的技术，可以用于代码生成任务，并且在自然发生的Python指令数据集上表现最先进。

    

    微调预训练语言模型(LM)可以增强模型的能力。先前的技术通过输入输出对（例如指令微调）或用评估输出质量的数字奖励（例如从人类反馈中进行的强化学习）对预训练的LM进行微调。我们探索了LM从文本交互中学习的潜力(LeTI)，这不仅可以通过二进制标签检查其正确性，而且还可以通过文本反馈指出和解释其输出中的错误。我们的研究重点是代码生成任务，其中模型根据自然语言指令生成代码片段。这种设置可以自然且可扩展地获取文本反馈：使用Python解释器进行代码执行时的错误消息和堆栈跟踪。 LeTI使用LM目标对自然语言指令、LM生成的程序和文本反馈进行串联的迭代微调，只有在生成代码无法执行时才提供文本反馈。我们在一个包含58k个自然发生的Python指令，增加了错误消息和堆栈跟踪的数据集上评估了LeTI，在三种不同的评估指标上显著优于强基线模型，并取得了最先进的结果。

    Finetuning pre-trained language models (LMs) enhances the models' capabilities. Prior techniques fine-tune a pre-trained LM on input-output pairs (e.g., instruction fine-tuning), or with numerical rewards that gauge the quality of its outputs (e.g., reinforcement learning from human feedback). We explore LMs' potential to learn from textual interactions (LeTI) that not only check their correctness with binary labels, but also pinpoint and explain errors in their outputs through textual feedback. Our investigation focuses on the code generation task, where the model produces code pieces in response to natural language instructions. This setting invites a natural and scalable way to acquire the textual feedback: the error messages and stack traces from code execution using a Python interpreter. LeTI iteratively fine-tunes the model, using the LM objective, on a concatenation of natural language instructions, LM-generated programs, and textual feedback, which is only provided when the gen
    

