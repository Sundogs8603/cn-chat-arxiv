# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes](https://arxiv.org/abs/2404.01299) | 利用卡通图像构建的CausalChaos!数据集，包含更长因果链的因果问答，通过动态互动和视觉展示挑战性因果关系，为模型提供了更多具挑战性且明确定义的因果关系。 |
| [^2] | [Towards Safety and Helpfulness Balanced Responses via Controllable Large Language Models](https://arxiv.org/abs/2404.01295) | 通过控制大语言模型中的属性，实现安全和帮助之间的平衡，并且可以在不同的应用场景中实现这种平衡。 |
| [^3] | [Evaluating Text-to-Visual Generation with Image-to-Text Generation](https://arxiv.org/abs/2404.01291) | 引入VQAScore，利用视觉问答模型生成对齐分数，取得了在图像文本对齐基准上的最先进结果 |
| [^4] | [Large Language Models are Capable of Offering Cognitive Reappraisal, if Guided](https://arxiv.org/abs/2404.01288) | 通过引入认知重评，这项工作将心理学原则融入大型语言模型中，为其提供先进的心理学能力。 |
| [^5] | [TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model](https://arxiv.org/abs/2404.01273) | 提出了基于大语言模型的数字孪生体TWIN-GPT，用于支持临床试验结果预测。 |
| [^6] | [Mapping the Increasing Use of LLMs in Scientific Papers](https://arxiv.org/abs/2404.01268) | 该论文通过对科学论文中LLM使用增加的映射进行了大规模分析，填补了学术写作中真实LLM修改内容比例缺失的空白。 |
| [^7] | [IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations](https://arxiv.org/abs/2404.01266) | IsoBench提出了一个基准数据集，用于评估基于不同同构表示的多模态基础模型的性能差异，发现大多数模型更偏好文本表示。 |
| [^8] | [Artificial Intelligence and the Spatial Documentation of Languages](https://arxiv.org/abs/2404.01263) | 本研究调查了AI模型在语言文献记录中创建语言地图的能力，并展示了如何通过AI模型促进语言的空间文献记录。 |
| [^9] | [FABLES: Evaluating faithfulness and content selection in book-length summarization](https://arxiv.org/abs/2404.01261) | 本文首次对LLM生成的虚构书籍摘要进行了忠实性和内容选择的大规模人类评估，建立了FABLES数据集，通过对26本书的3158个声明进行了注释，成功对LLM摘要进行了基于忠实性的排名 |
| [^10] | [UniArk: Improving Generalisation and Consistency for Factual Knowledge Extraction through Debiasing](https://arxiv.org/abs/2404.01253) | 通过UniArk框架，本文提出了一种基于适配器的解决方案，通过去偏差的方式提高了模型在未见提示下的泛化性和一致性 |
| [^11] | [An image speaks a thousand words, but can everyone listen? On translating images for cultural relevance](https://arxiv.org/abs/2404.01247) | 这项工作首次尝试翻译图像以使其具有文化相关性，构建了评估数据集并进行了多方面的人类评估，发现目前图像编辑模型在这一任务上仍存在挑战。 |
| [^12] | [A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules](https://arxiv.org/abs/2404.01245) | 该论文提出了一个通用框架，用于设计大型语言模型水印的统计效率和检测规则，通过关键统计量和秘密密钥控制误报率，同时评估水印检测规则的能力。 |
| [^13] | [Effectively Prompting Small-sized Language Models for Cross-lingual Tasks via Winning Tickets](https://arxiv.org/abs/2404.01242) | 该论文引入了Lottery Ticket Prompt-learning（LTP）框架，通过将中奖票与软提示相结合，为小型语言模型提供了一种更简单、只需一次执行的提示方法，从而有效地进行跨语言任务。 |
| [^14] | [AURORA: Navigating UI Tarpits via Automated Neural Screen Understanding](https://arxiv.org/abs/2404.01240) | 通过新形式的自动化语义屏幕理解，本文提出了一种名为AURORA的技术，旨在帮助AIG工具有效地导航应用程序探索过程中的“tarpits”。 |
| [^15] | [GFLean: An Autoformalisation Framework for Lean via GF](https://arxiv.org/abs/2404.01234) | GFLean是一个自动形式化框架，通过使用GF进行解析和线性化，同时结合神经网络和规则翻译程序，构建了一个强大的autoformalisation框架。 |
| [^16] | [Open-Vocabulary Federated Learning with Multimodal Prototyping](https://arxiv.org/abs/2404.01232) | 本研究针对联邦学习中的开放词汇挑战，提出了一种基于预训练视觉语言模型的新型自适应框架，命名为联邦多模式原型（Fed-MP），用于解决新用户提出的涉及任意未知类别的查询问题。 |
| [^17] | [LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models](https://arxiv.org/abs/2404.01230) | 本文对大型语言模型在战略推理领域的现状和机遇进行了全面调查，旨在系统梳理澄清该主题的零散文献，突出了其决策性能的跨学科方法。 |
| [^18] | [Stable Code Technical Report](https://arxiv.org/abs/2404.01226) | Stable Code是一个新一代的代码语言模型，提供通用基础代码语言模型，支持代码完成、推理、数学等软件工程任务，同时还引入了具有自然对话界面的指令变体Stable Code Instruct，可执行问答和基于指令的任务。 |
| [^19] | [AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for hallucination detection and analysis](https://arxiv.org/abs/2404.01210) | 该论文提出了一种高效的模型调优策略，通过将预训练模型和自然语言推理模型进行集成，成功在幻觉检测任务中取得了77.8%和79.9%的准确率，优于组织者的基线和竞赛中其他参赛者的表现。 |
| [^20] | [The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis](https://arxiv.org/abs/2404.01204) | 本文对大型语言模型在预训练过程中不同能力的综合分析揭示了其动态差异，填补了现有标度定律中的缺失。 |
| [^21] | [Estimating Lexical Complexity from Document-Level Distributions](https://arxiv.org/abs/2404.01196) | 本研究提出了一个从文档级分布中估计词汇复杂度的两步方法，不依赖于任何预注释数据，用于支持健康从业者创建更好的工具。 |
| [^22] | [Generating Faithful and Complete Hospital-Course Summaries from the Electronic Health Record](https://arxiv.org/abs/2404.01189) | 本论文研究了如何从电子健康记录生成忠实完整的医院诊疗摘要，并提出并评估了自动化解决方案。 |
| [^23] | [A Neuro-Symbolic Approach to Monitoring Salt Content in Food](https://arxiv.org/abs/2404.01182) | 通过整合神经符号规则，该对话系统在监测食物中的盐含量方面取得了超过20%的联合目标准确率的改善。 |
| [^24] | [LITE: Modeling Environmental Ecosystems with Multimodal Large Language Models](https://arxiv.org/abs/2404.01165) | 使用LITE模型，可以将不同的环境变量转换成自然语言描述和折线图像，并利用统一编码器来捕捉空间-时间关系，从而更好地预测环境变量。 |
| [^25] | [Dialogue with Robots: Proposals for Broadening Participation and Research in the SLIVAR Community](https://arxiv.org/abs/2404.01158) | 本文提出了三项提案：教育、基准测试以及涉及与机器人口头交互时对语言建模的研究，以促进扩大SLIVAR社区参与和研究。 |
| [^26] | [Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade Offs in Large Language Model Training](https://arxiv.org/abs/2404.01157) | 该研究评估了大型语言模型的CO2排放量，强调了这些模型由于其大量的模型参数导致其碳足迹特别高。 |
| [^27] | [Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case Study on Reddit](https://arxiv.org/abs/2404.01147) | LLMs在模拟基于事实问题的社交媒体问题中人类答案时表现较好，为未来研究提供了方向。 |
| [^28] | [KoCoNovel: Annotated Dataset of Character Coreference in Korean Novels](https://arxiv.org/abs/2404.01140) | KoCoNovel是一个从韩国文学文本中衍生出的小说人物共指数据集，提供了详细的注释指南，是韩语公共共指解决语料库中第二大规模的数据集，也是第一个基于文学文本的数据集 |
| [^29] | [Structured Information Matters: Incorporating Abstract Meaning Representation into LLMs for Improved Open-Domain Dialogue Evaluation](https://arxiv.org/abs/2404.01129) | 将抽象意义表示结合到LLMs中，提出了一个简单有效的框架用于改善开放领域对话评估 |
| [^30] | [SentiCSE: A Sentiment-aware Contrastive Sentence Embedding Framework with Sentiment-guided Textual Similarity](https://arxiv.org/abs/2404.01104) | 提出了一种评估情感表示质量的新指标SgTS，并设计了一种具有情感感知对比的句子嵌入框架SentiCSE。 |
| [^31] | [What's in Your "Safe" Data?: Identifying Benign Data that Breaks Safety](https://arxiv.org/abs/2404.01099) | 通过双向锚定方法，识别那些在微调后更可能降低模型安全性的良性数据子集，提高模型对有害请求的响应率。 |
| [^32] | [AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer Models for Lateral Thinking Puzzles](https://arxiv.org/abs/2404.01084) | 本研究通过调整多种预训练变压器模型，在SemEval-2024任务9的脑筋急转弯竞赛中取得了竞争力十足的表现，在句子谜题和单词谜题的评估中，最佳提交的准确率分别超过了最佳神经基线ChatGPT超过20%和30%。 |
| [^33] | [Efficient Prompting Methods for Large Language Models: A Survey](https://arxiv.org/abs/2404.01077) | 提示已成为大型语言模型适应特定自然语言处理任务的主流范式，而高效提示方法在压缩提示和自动提示优化方面取得显著进展。 |
| [^34] | [Advancing AI with Integrity: Ethical Challenges and Solutions in Neural Machine Translation](https://arxiv.org/abs/2404.01070) | 本文研究了神经机器翻译中的伦理挑战，并通过实证研究和文献综述确定和解决了数据处理、隐私、数据所有权等方面的伦理问题，为确保AI模型的公平性和文化敏感性提供了解决方案。 |
| [^35] | [Exploring the Mystery of Influential Data for Mathematical Reasoning](https://arxiv.org/abs/2404.01067) | 提出了适用于数学推理的质量感知多样选择（QaDS）策略，并验证其在选择具有影响力的数据上的优越性；扩大了数据规模、用QaDS选择的通用数据进行训练对数学推理有帮助，最后定义了最佳混合OpenMathMix。 |
| [^36] | [Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment](https://arxiv.org/abs/2404.01054) | 提出了Regularized Best-of-N (RBoN)，通过引入接近性项来减轻奖励欺骗，提高了算法在解码时与人类偏好对齐的效果。 |
| [^37] | [ARAGOG: Advanced RAG Output Grading](https://arxiv.org/abs/2404.01037) | 本研究评估了不同的 RAG 方法对检索精度和答案相似性的影响，发现假设文档嵌入（HyDE）和LLM 重新排序显著提高了检索精度，而最大边际相关性（MMR）和 Cohere 重新排序则没有明显优势。 |
| [^38] | [Verifying Claims About Metaphors with Large-Scale Automatic Metaphor Identification](https://arxiv.org/abs/2404.01029) | 通过大规模语料库分析验证了关于动词隐喻的主张，发现隐喻中动词的直接宾语往往具有较低的具体性、可形象化程度和熟悉度，隐喻更可能在情感和主观句子中使用。 |
| [^39] | [Source-Aware Training Enables Knowledge Attribution in Language Models](https://arxiv.org/abs/2404.01019) | 源感知训练使语言模型具备知识归因能力，进而增强了其透明度、可解释性和可验证性。 |
| [^40] | [PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison](https://arxiv.org/abs/2404.01015) | 提出了PairEval，一种新颖的对话评估指标，通过将回复的质量与不同对话中的回复进行比较来评估，与人类判断具有更高的相关性。 |
| [^41] | [Query Performance Prediction using Relevance Judgments Generated by Large Language Models](https://arxiv.org/abs/2404.01012) | 提出了一种使用自动生成的相关性判断的查询性能预测框架，能够解决先前方法中对不同IR评估指标准确性和解释性的限制。 |
| [^42] | [Constructing and Expanding Low-Resource and Underrepresented Parallel Datasets for Indonesian Local Languages](https://arxiv.org/abs/2404.01009) | 介绍了Bhinneka Korpus，一个包含五种印尼本地语言的多语言平行语料库，旨在增强资源的访问和利用，扩展其在国内的影响力。 |
| [^43] | [What Causes the Failure of Explicit to Implicit Discourse Relation Recognition?](https://arxiv.org/abs/2404.00999) | 显式示例（去除连接词）上训练的关系分类器在真实的隐式情景中表现不佳的原因之一是在去除连接词后标签发生变化，我们提供了在语料库水平上的实证证据并探讨了缓解标签变化的策略 |
| [^44] | [LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation](https://arxiv.org/abs/2404.00998) | 该研究提出了一个使用大型语言模型进行放射学报告评估的新框架，通过GPT-4实现了与放射科医师评估一致性接近的效果，同时利用知识蒸馏训练的较小模型也达到了类似的评估能力。 |
| [^45] | [Exploring the Nexus of Large Language Models and Legal Systems: A Short Survey](https://arxiv.org/abs/2404.00990) | 大型语言模型在法律领域展示出独特作用，带来好处和挑战，调研突出了其在法律文本理解和分析中的应用，以及面临的偏见、可解释性和伦理挑战，展示了针对不同法律系统的精细调整法律LLMs的最新进展。 |
| [^46] | [Prior Constraints-based Reward Model Training for Aligning Large Language Models](https://arxiv.org/abs/2404.00978) | 本文提出了基于先验约束的奖励模型训练方法，有效改善了对齐大型语言模型的性能。 |
| [^47] | [AISPACE at SemEval-2024 task 8: A Class-balanced Soft-voting System for Detecting Multi-generator Machine-generated Text](https://arxiv.org/abs/2404.00950) | 本文介绍了一种类平衡Soft-voting系统，旨在检测人类撰写或机器生成的文本，通过优化编码器模型表现，并采用加权交叉熵损失和软投票策略，在SemEval-2024任务8的Subtask B中取得了最佳表现。 |
| [^48] | [Evalverse: Unified and Accessible Library for Large Language Model Evaluation](https://arxiv.org/abs/2404.00943) | Evalverse是一个统一和易用的库，简化了大型语言模型（LLMs）的评估，为研究人员和从业者提供了集中且易于访问的评估框架。 |
| [^49] | [Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs](https://arxiv.org/abs/2404.00942) | 使用大型知识图谱创建评估模型以检查大型语言模型在事实性上的表现，有效降低评估成本。 |
| [^50] | [How Can Large Language Models Enable Better Socially Assistive Human-Robot Interaction: A Brief Survey](https://arxiv.org/abs/2404.00938) | 大型语言模型的最新进展为社会辅助机器人领域带来了潜在的新应用，能够显著扩展其能力，但也带来新的风险和道德关切。 |
| [^51] | [ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback](https://arxiv.org/abs/2404.00934) | ChatGLM-RLHF是一种从人类反馈中强化学习系统，通过收集人类偏好数据、训练奖励模型和优化策略等方法来增强大型语言模型ChatGLM与人类偏好的对齐性，在实验中显示出显著的改进。 |
| [^52] | [PSYDIAL: Personality-based Synthetic Dialogue Generation using Large Language Models](https://arxiv.org/abs/2404.00930) | 该研究提出了一种新颖的基于个性化的合成对话数据生成流水线，引入了PSYDIAL韩语对话数据集，针对大型语言模型生成更符合真实场景的人类化对话，实验证明相较于预训练或微调模型，使用PSYDIAL训练的模型在生成符合个性的回应上有显著提升，且该流水线具有非对话相关应用的潜力。 |
| [^53] | [A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias](https://arxiv.org/abs/2404.00929) | 该论文对多语言大型语言模型进行了全面分析，深入讨论了关键问题，包括多语言语料库、对齐和偏见。 |
| [^54] | [LLMs are Good Sign Language Translators](https://arxiv.org/abs/2404.00925) | 本文提出了一种新颖的SignLLM框架，利用大型语言模型（LLMs）来处理手语翻译，通过对手语视频进行正则化和转换，实现了手语视频向类似语言的表示转换，以提高现成LLMs的可读性。 |
| [^55] | [Token-Efficient Leverage Learning in Large Language Models](https://arxiv.org/abs/2404.00914) | 介绍了一种名为Token-Efficient Leverage Learning（TELL）的方法，在大型语言模型中展示了其降低任务数据需求、提高任务性能的潜力，为低资源任务带来了竞争性能。 |
| [^56] | [LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction](https://arxiv.org/abs/2404.00913) | LLaMA-Excitor是一种轻量级方法，通过逐渐更多地关注有价值的信息来激发LLM更好地遵循指令，而不直接改变中间隐藏状态，有效保留预训练知识。 |
| [^57] | [TM-TREK at SemEval-2024 Task 8: Towards LLM-Based Automatic Boundary Detection for Human-Machine Mixed Text](https://arxiv.org/abs/2404.00899) | 本研究探讨了基于LLM的自动边界检测，在SemEval-2024比赛任务中取得了第一名，并深入研究了影响LLMs检测能力的因素。 |
| [^58] | [Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models](https://arxiv.org/abs/2404.00884) | 提出了自我演示(Self-Demos)方法，在大型语言模型中通过生成查询感知的示范，引出固有的泛化能力，并构建了用于评估该方法有效性的数据集OOD-Toolset。 |
| [^59] | [Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding](https://arxiv.org/abs/2404.00862) | 通过结合参数高效微调和先进的嵌入初始化技术，本研究在以英语为主导的开源LLMs上实现了跨语言迁移学习。 |
| [^60] | [Do language models plan ahead for future tokens?](https://arxiv.org/abs/2404.00859) | 语言模型在推理过程中会提前准备未来标记所需的信息，可能是通过预缓存或面包屑的方式实现。 |
| [^61] | [Returning to the Start: Generating Narratives with Related Endpoints](https://arxiv.org/abs/2404.00829) | 提出了一种通过确保故事的第一句和最后一句相关性来生成叙事的方法，并探讨了不同叙事学方法如何影响故事的语言建模，实现了更好的故事叙事封闭性。 |
| [^62] | [PID Control-Based Self-Healing to Improve the Robustness of Large Language Models](https://arxiv.org/abs/2404.00828) | 本研究提出了一种基于PID控制的自愈机制，通过轨迹优化问题和神经网络内部状态的自动修正，来提高在在线推断中对输入数据扰动的容忍性。 |
| [^63] | [Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods](https://arxiv.org/abs/2404.00826) | 本研究利用大型语言模型Fine-tuned和上下文学习方法，提出了一个新颖的带注释语料库PedSHAC，并自动提取儿科患者病历中的详细社会健康决定因素。 |
| [^64] | [Rehearsal-Free Modular and Compositional Continual Learning for Language Models](https://arxiv.org/abs/2404.00790) | 提出了一种无需排练的模块化和组合式持续学习框架，可以持续向语言模型添加新模块并将其与现有模块组合，实验证明该框架优于现有技术并有效推动知识转移。 |
| [^65] | [From Robustness to Improved Generalization and Calibration in Pre-trained Language Models](https://arxiv.org/abs/2404.00758) | 通过引入JacHess方法，在预训练语言模型中实现了在领域内泛化和校准方面的显著改进 |
| [^66] | [On the True Distribution Approximation of Minimum Bayes-Risk Decoding](https://arxiv.org/abs/2404.00752) | 本研究提出使用异常检测来衡量最小贝叶斯风险解码中样本接近真实分布的程度，并验证实验结果首次支持了性能与采样近似度之间的联系。 |
| [^67] | [Can Language Models Recognize Convincing Arguments?](https://arxiv.org/abs/2404.00750) | 大语言模型不仅能够在识别和区分强势和弱势论点方面表现良好，还可以根据用户的信念和人口特征预测其立场，并确定论点对个人的吸引力。 |
| [^68] | [Benchmark Transparency: Measuring the Impact of Data on Evaluation](https://arxiv.org/abs/2404.00748) | 本研究提出了一种自动化框架，可以衡量数据分布对自然语言处理模型性能和评估的影响，揭示了数据分布的重要性和对评估框架的影响。 |
| [^69] | [Opera Graeca Adnotata: Building a 34M+ Token Multilayer Corpus for Ancient Greek](https://arxiv.org/abs/2404.00739) | Opera Graeca Adnotata（OGA）是古希腊最大的开放获取多层语料库，包括来自PerseusDL和OpenGreekAndLatin GitHub仓库的1,687部文学作品和超过3400万个词元，丰富标注了七个注释层。 |
| [^70] | [A Controlled Reevaluation of Coreference Resolution Models](https://arxiv.org/abs/2404.00727) | 基于对预训练语言模型大小的控制，我们发现基于编码器的核指代解析模型在准确性和推理速度方面优于更近期的基于解码器的模型，而在基于编码器的模型中，最老的模型在跨域文本体裁中表现最佳。 |
| [^71] | [The Larger the Better? Improved LLM Code-Generation via Budget Reallocation](https://arxiv.org/abs/2404.00725) | 较小的语言模型可以在相同预算下产生可靠的改进，但在无法进行单元测试的情况下，较小的模型选择排名次于较大模型的单个输出。 |
| [^72] | [How Much are LLMs Contaminated? A Comprehensive Survey and the LLMSanitize Library](https://arxiv.org/abs/2404.00699) | LLM受到污染可能导致其性能不可靠，挑战了自然语言处理领域的整体进展。 |
| [^73] | [Scaling Properties of Speech Language Models](https://arxiv.org/abs/2404.00685) | 通过研究语音语言模型的尺度特性，可以预测其语言性能的扩展速度，与文本-based大型语言模型相比，语音语言模型的语言性能扩展速度慢三个数量级。 |
| [^74] | [CoUDA: Coherence Evaluation via Unified Data Augmentation](https://arxiv.org/abs/2404.00681) | 通过CoUDA框架，将话语连贯性分解为全局和局部两个方面，并提出用于局部连贯性的新颖生成策略。 |
| [^75] | [A General and Efficient Training for Transformer via Token Expansion](https://arxiv.org/abs/2404.00672) | 本文提出了一种名为ToE的令牌生长方案，旨在通过加速一致性训练来改善ViT的训练效果。 |
| [^76] | [Observations on Building RAG Systems for Technical Documents](https://arxiv.org/abs/2404.00657) | 研究回顾了对技术文档RAG系统构建的重要因素，并通过实验证明了最佳实践和潜在挑战 |
| [^77] | [WavLLM: Towards Robust and Adaptive Speech Large Language Model](https://arxiv.org/abs/2404.00656) | WavLLM是一个稳健和自适应语音大语言模型，引入了双编码器和Prompt-aware LoRA权重适配器，通过两阶段课程学习方法优化，解耦不同类型的语音信息，为处理语义内容和说话者身份的独特特征提供了新思路 |
| [^78] | [Against The Achilles' Heel: A Survey on Red Teaming for Generative Models](https://arxiv.org/abs/2404.00629) | 通过对生成模型的红队测试进行了广泛调查，引入了基于语言模型能力的细粒度攻击策略分类体系，并开发了一个统一各种自动红队测试方法的搜索框架。 |
| [^79] | [Reporting Eye-Tracking Data Quality: Towards a New Standard](https://arxiv.org/abs/2404.00620) | 该研究提出了一种新的眼动数据共享方法，主张公布眼动数据的所有预处理阶段以及附带数据质量报告，从而增加现有眼动数据集的可重复使用性和实现跨数据集比较。 |
| [^80] | [Learning to Plan for Language Modeling from Unlabeled Data](https://arxiv.org/abs/2404.00614) | 通过自监督学习目标训练一个用于规划未来写作过程的模块，扩展了成功的语言模型公式到更抽象的规划中，改善了语言建模的性能，特别是在文本结构方面，同时新的规划模块可以大规模训练并轻松与社区共享。 |
| [^81] | [RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation](https://arxiv.org/abs/2404.00610) | 本论文提出了RQ-RAG，旨在为检索增强生成模型增加细化查询的能力，以便针对模糊或复杂查询进一步澄清或分解，从而提高生成准确度。 |
| [^82] | [Extensive Self-Contrast Enables Feedback-Free Language Model Alignment](https://arxiv.org/abs/2404.00604) | 本论文介绍了一种利用广泛自对比生成负例的无需反馈的大型语言模型对齐方法，该方法在实验中表现优于直接偏好优化方法。 |
| [^83] | [AI Act and Large Language Models (LLMs): When critical issues and privacy impact require human and ethical oversight](https://arxiv.org/abs/2404.00600) | 论文讨论了人类监督、道德监督和隐私影响评估在面对人工智能系统和大型语言模型发展中的重要性。 |
| [^84] | [EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories](https://arxiv.org/abs/2404.00599) | EvoCodeBench 是一个与现实世界代码库对齐的进化代码生成基准，具有完善的注释和评估度量标准，同时避免数据泄露。 |
| [^85] | [ECtHR-PCR: A Dataset for Precedent Understanding and Prior Case Retrieval in the European Court of Human Rights](https://arxiv.org/abs/2404.00596) | 这项研究提出了一个基于欧洲人权法院裁决的先前案例检索数据集，有效地分离事实和论点，展示了判例实践，有助于开发先前案例检索技术。 |
| [^86] | [Query-driven Relevant Paragraph Extraction from Legal Judgments](https://arxiv.org/abs/2404.00595) | 本研究侧重于从法律裁决中基于查询提取相关段落的任务，结果表明微调和零-shot性能之间存在显著差距，强调了在法律领域处理分布变化的挑战。 |
| [^87] | [LexAbSumm: Aspect-based Summarization of Legal Decisions](https://arxiv.org/abs/2404.00594) | LexAbSumm是一个针对法律案例决策的基于方面的摘要数据集，评估了多个适用于较长文档的抽象摘要模型，揭示了在产生特定方面摘要时这些模型的挑战。 |
| [^88] | [CuSINeS: Curriculum-driven Structure Induced Negative Sampling for Statutory Article Retrieval](https://arxiv.org/abs/2404.00590) | CuSINeS通过课程驱动的负采样策略、结构化法规信息和动态语义难度评估三方面贡献，提升了法定文章检索的效果。 |
| [^89] | [Harnessing the Power of Large Language Model for Uncertainty Aware Graph Processing](https://arxiv.org/abs/2404.00589) | 介绍了一种利用大型语言模型处理图数据中不确定性的方法，通过不确定性感知模块增强，提供置信度评分，实验结果表明该方法在知识图完成和图分类任务上超越了最先进算法。 |
| [^90] | [Explainable Multi-hop Question Generation: An End-to-End Approach without Intermediate Question Labeling](https://arxiv.org/abs/2404.00571) | 提出了一种端到端问题重写模型，通过顺序重写增加问题复杂性，能够无需中间问题标记训练生成解释性强的多跳问题。 |
| [^91] | [ParaICL: Towards Robust Parallel In-Context Learning](https://arxiv.org/abs/2404.00570) | 提出了一种名为ParaICL的新方法，通过并行批处理来有效利用所有示例，在不超过可管理的输入上下文长度的情况下显著提升了不同测试样本的准确性。 |
| [^92] | [CM-TTS: Enhancing Real Time Text-to-Speech Synthesis Efficiency through Weighted Samplers and Consistency Models](https://arxiv.org/abs/2404.00569) | CM-TTS通过一致性模型和加权采样器实现了更高效的实时文本转语音合成，避免了对抗训练和预训练模型的依赖。 |
| [^93] | [CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks](https://arxiv.org/abs/2404.00566) | 提出了CodeBenchGen框架，通过利用大型语言模型将任意代码转化为评估示例，创造了一个包含大量代码示例的数据集Exec-CSN，展示了其可扩展性和实用性。 |
| [^94] | [Leveraging Corpus Metadata to Detect Template-based Translation: An Exploratory Case Study of the Egyptian Arabic Wikipedia Edition](https://arxiv.org/abs/2404.00565) | 该研究旨在通过探索性研究识别埃及阿拉伯维基百科中基于模板翻译的文章及其特征，以解决因此导致的问题。 |
| [^95] | [DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented Dialogue Representations](https://arxiv.org/abs/2404.00557) | DivTOD 提出了一种新颖的对话预训练模型，与LLMs合作学习多样的任务导向对话表示，实验表明该模型在各种下游对话任务上优于强基线，并学习了任务导向对话的内在多样性。 |
| [^96] | [Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization](https://arxiv.org/abs/2404.00530) | 本研究提出了一种新的偏好获取方法，通过DOVE协议对指令-响应对的联合概率进行优化，以对齐大型语言模型。 |
| [^97] | [MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in Conversations with Multimodal Language Models](https://arxiv.org/abs/2404.00511) | 本文提出了一种集成文本、音频和视觉模态的多模态情绪识别和多模态情绪原因提取框架，通过利用专门的情绪编码器和模态特定特征，实现了提升情绪理解和因果推理的竞争性成果。 |
| [^98] | [The Shape of Word Embeddings: Recognizing Language Phylogenies through Topological Data Analysis](https://arxiv.org/abs/2404.00500) | 通过拓扑数据分析识别语言谱系，研究了单词嵌入的形状如何传递信息，重建的语言谱系树与参考树展现出强烈的相似性。 |
| [^99] | [Configurable Safety Tuning of Language Models with Synthetic Preference Data](https://arxiv.org/abs/2404.00495) | 提出了一种名为Configurable Safety Tuning（CST）的新方法，通过使用合成偏好数据，在推断时实现对LLMs的灵活安全配置，允许用户根据需要禁用/启用安全偏好，且实验表明CST成功管理不同的安全配置并保留了原始功能，是一种适用于可配置部署的强大方法。 |
| [^100] | [Multi-hop Question Answering under Temporal Knowledge Editing](https://arxiv.org/abs/2404.00492) | 提出了一个新颖的框架TEMPLE-MQA，通过构建时间感知图和推理路径、结构检索和联合推理阶段，有效地识别多跳问题回答中的时间背景，在基准数据集上显著优于基线模型，并贡献了一个新数据集TKEMQA。 |
| [^101] | [PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression](https://arxiv.org/abs/2404.00489) | 提出了PROMPT-SAW模型，利用关系感知图来实现文本提示的压缩，提高了提示的可读性和可解释性。 |
| [^102] | [Noise-Aware Training of Layout-Aware Language Models](https://arxiv.org/abs/2404.00488) | 本论文提出了一种噪声感知训练方法，可以以可扩展的方式使用弱标记文档训练提取器，从而避免了在企业场景中训练大量不同文档类型的自定义提取器所需的昂贵人工标记成本。 |
| [^103] | [Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs](https://arxiv.org/abs/2404.00486) | 提出了辩证对齐（DA）框架来解决LLM在面临外部有毒数据时的适应性变色龙问题，增强了其对抗外部攻击的安全性 |
| [^104] | [Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model unless you have access to GPT-4](https://arxiv.org/abs/2404.00484) | 本研究提出了一种参数高效微调（PEFT）方法，通过合并分别使用三元组和语言建模目标进行微调的适配器来提高大型语言模型（LLMs）的一致性，但未能超越GPT-4在忠实度和一致性方面的准确性。 |
| [^105] | [Cross-lingual Named Entity Corpus for Slavic Languages](https://arxiv.org/abs/2404.00482) | 介绍了一个手动标注的用于六种斯拉夫语言的命名实体语料库，提供了两个训练数据集划分，并使用预训练的多语言模型进行命名实体的识别、分类、引用词化和链接。 |
| [^106] | [Linguistic Calibration of Language Models](https://arxiv.org/abs/2404.00474) | 该论文提出了一种通过语言模型的文本生成来实现语言校准，可以使用户做出校准概率预测的方法。 |
| [^107] | [Addressing Both Statistical and Causal Gender Fairness in NLP Models](https://arxiv.org/abs/2404.00463) | 本研究评估了在NLP模型中同时处理统计和因果性别公平性偏见的方法，发现结合统计和因果性去偏置技术能够有效减少偏见。 |
| [^108] | [Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning](https://arxiv.org/abs/2404.00461) | 基于提示的学习中的干净标签攻击通过特定提示作为触发器，实现成功的同时确保正确标记有毒样本，但仍面临误激活问题和挑战，需要更高比例的毒害。 |
| [^109] | [NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning](https://arxiv.org/abs/2404.00459) | NumeroLogic提出了一种新的数字表示方法，通过在每个数字前包含数字的位数计数，为语言模型的数字推理能力提供了增强，从而改善了生成实际数字之前的推理过程。 |
| [^110] | [Beyond One-Size-Fits-All: Multi-Domain, Multi-Task Framework for Embedding Model Selection](https://arxiv.org/abs/2404.00458) | 开发了一个多领域、多任务框架，帮助选择最有效的自然语言处理嵌入模型。 |
| [^111] | [MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks](https://arxiv.org/abs/2404.00457) | 提出了MetaIE框架，通过从大型语言模型中进行符号蒸馏，构建一个小型LM作为元模型，实现对所有类型的信息抽取任务的高效适应。 |
| [^112] | [Planning and Editing What You Retrieve for Enhanced Tool Learning](https://arxiv.org/abs/2404.00450) | 该论文提出了一种新颖的模型，结合了“规划与检索”和“编辑与确认”范式，通过神经检索模块和LLM-based查询规划器提高了工具利用的效果。 |
| [^113] | [DOCMASTER: A Unified Platform for Annotation, Training, & Inference in Document Question-Answering](https://arxiv.org/abs/2404.00439) | DOCMASTER是一个统一平台，旨在为文档问答提供注释、训练和推断服务，支持用户在PDF文档中输入问题并突出显示文本段作为答案，同时保护隐私。 |
| [^114] | [Automatic explanation of the classification of Spanish legal judgments in jurisdiction-dependent law categories with tree estimators](https://arxiv.org/abs/2404.00437) | 本研究结合自然语言处理和机器学习，提出了一种可解释的法律文本分类系统，使模型的决策变得可理解给最终用户 |
| [^115] | [Do Vision-Language Models Understand Compound Nouns?](https://arxiv.org/abs/2404.00419) | 该研究策划了一个名为Compun的新基准测试，通过文本提示和图像选择任务评估VLMs在理解复合名词方面的表现，并展现了CLIP在某些类型的复合名词理解上存在局限性。 |
| [^116] | [CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP](https://arxiv.org/abs/2404.00415) | CoDa是一种针对低资源NLP提出的基于约束生成的数据增强技术，通过提示大型语言模型生成符合一组约束的文本，有效提升数据增强效果，避免模型偏向少量训练数据。 |
| [^117] | [TACO -- Twitter Arguments from COnversations](https://arxiv.org/abs/2404.00406) | TACO是第一个利用1,814条推文构建的Twitter Arguments数据集，涵盖200场完整对话，六个主题，具有0.718的Krippendorff's alpha一致性，并提供了一个注释框架来定义和识别论点结构要素。 |
| [^118] | [UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion Cause](https://arxiv.org/abs/2404.00403) | UniMEEC提出了一个统一的多模情绪识别和情绪-原因分析框架，将MERC和MECPE重新定义为两个掩码预测问题，以增强情绪和原因之间的交互作用。 |
| [^119] | [How Robust are the Tabular QA Models for Scientific Tables? A Study using Customized Dataset](https://arxiv.org/abs/2404.00401) | 通过提出新的数据集"SciTabQA"，研究了现有科学混合表格数据上最先进Tabular QA模型的鲁棒性，评估了其在解释科学表格和文本方面的能力。 |
| [^120] | [Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order](https://arxiv.org/abs/2404.00399) | Aurora-M 是第一个根据美国行政命令进行红队测试的开源多语言模型，通过在英语、芬兰语、印地语、日语、越南语和代码上训练，不断预训练，包括了人工审核的安全说明，总训练 token 数超过 2 万亿个 |
| [^121] | [An Analysis of BPE Vocabulary Trimming in Neural Machine Translation](https://arxiv.org/abs/2404.00397) | 在神经机器翻译中，对BPE词汇进行修剪不能提高性能，甚至可能导致严重的性能下降。 |
| [^122] | [Jetsons at FinNLP 2024: Towards Understanding the ESG Impact of a News Article using Transformer-based Models](https://arxiv.org/abs/2404.00386) | 本文描述了Jetsons团队在多语言ESG影响时长推断共享任务中探索的不同方法，包括使用XLM-RoBERTa和DeBERTa-v3对影响时长进行预测，并在英语语言中取得领先地位。 |
| [^123] | [Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks](https://arxiv.org/abs/2404.00376) | 通过从医学教材提取的推理路径和多样化的遵循指令数据集，我们引入了具有70亿参数的Meerkat-7B医学人工智能系统，成功解决了商用大型语言模型在医学任务上隐私和推理能力不足的问题，取得了优于先前7B模型的显著成果。 |
| [^124] | [Controllable and Diverse Data Augmentation with Large Language Model for Low-Resource Open-Domain Dialogue Generation](https://arxiv.org/abs/2404.00361) | 提出了一种基于总结的对话增强方法SDA，通过使用对话总结增强了LLM的可控性，实现高质量和多样化的对话数据生成。 |
| [^125] | [Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange](https://arxiv.org/abs/2404.00344) | LLMs在数学领域表现出色，其中GPT-4在Math Stack Exchange上回答数学问题的表现最佳。 |
| [^126] | [A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs](https://arxiv.org/abs/2404.00303) | 本研究从传统方法到当代实践，探讨了适用于仇恨言论检测的数据增强方法，提出了优化利用BERT-based编码器模型的思路，并揭示了相较于之前的同义词替换方法存在的重要限制。 |
| [^127] | [TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa](https://arxiv.org/abs/2404.00297) | TRABSA是一个集成了transformer架构、注意力机制和BiLSTM网络的混合框架，利用RoBERTa在大量推特上训练，填补了情感分析领域的差距，实现了94%的准确性和显著的性能提升。 |
| [^128] | [A Likelihood Ratio Test of Genetic Relationship among Languages](https://arxiv.org/abs/2404.00284) | 提出一种受分子系统发生学启发的似然比检验方法，用于确定语言间是否存在遗传关系。 |
| [^129] | [Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods](https://arxiv.org/abs/2404.00282) | 大型语言模型在强化学习中具有潜在优势，通过结构化分类和角色分析，为未来研究提供指导。 |
| [^130] | [Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits](https://arxiv.org/abs/2404.00267) | LLMs对作者的语言模式的影响略微降低其个人特征的预测能力，但显著变化不太频繁。 |
| [^131] | [DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation](https://arxiv.org/abs/2404.00264) | 提出了一种名为DiLM的文本数据集蒸馏方法，通过训练语言模型生成文本数据作为合成训练样本，解决了嵌入级别蒸馏数据集无法用于训练其他模型的问题。 |
| [^132] | [Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World](https://arxiv.org/abs/2404.00246) | 在一个方块世界环境中，论文评估了大型语言模型的协作能力，通过设计不断增加挑战性的设置来评估不同的协作视角，从独立到更复杂的依赖任务。 |
| [^133] | [DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference](https://arxiv.org/abs/2404.00242) | DeFT提出了一种带IO意识的树注意力算法，通过在QKV准备和注意力计算阶段实现内存高效的计算，降低内存印记，以解决当前树解码策略和推断系统不适配的问题。 |
| [^134] | [Enhancing Content-based Recommendation via Large Language Model](https://arxiv.org/abs/2404.00236) | 本文提出了一种名为LoID的语义知识传递方法，旨在提取多方面的语义信息以增强不同领域，并对齐用户/项目ID和内容语义特征空间。 |
| [^135] | [A Survey of using Large Language Models for Generating Infrastructure as Code](https://arxiv.org/abs/2404.00227) | 使用大型语言模型来自动化基础设施即代码编排工作的可行性进行了研究 |
| [^136] | [Design as Desired: Utilizing Visual Question Answering for Multimodal Pre-training](https://arxiv.org/abs/2404.00226) | 本研究是首次利用视觉问答（VQA）进行多模态预训练，专注于引导模型学习所需病理特征，并提出了一种无需额外专家注释的问题-答案对设计方法，以及一种准文本特征转换器模块。 |
| [^137] | [Classification and Clustering of Sentence-Level Embeddings of Scientific Articles Generated by Contrastive Learning](https://arxiv.org/abs/2404.00224) | 对比学习生成的句子级科学文章嵌入进行分类和聚类，相比基线模型，在聚类协议水平上有五倍增长 |
| [^138] | [Rationale-based Opinion Summarization](https://arxiv.org/abs/2404.00217) | 提出了一种基于原理的意见总结范式及其提取方法 RATION，可生成代表性意见和相应原理，经过评估显示提取的原理具有相关性、具体性、流行度和多样性。 |
| [^139] | [Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark](https://arxiv.org/abs/2404.00216) | 大型语言模型通过事实解码方法提高了事实准确性，然而，这些方法使模型对已知事实过于自信，进一步评估显示在知识编辑基准上所有解码方法均显著降低了模型性能。 |
| [^140] | [Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning](https://arxiv.org/abs/2404.00213) | 本论文研究了在大型语言模型中通过监督微调方法注入新知识的效果，特别关注了最近体育事件领域。 |
| [^141] | [Multi-Conditional Ranking with Large Language Models](https://arxiv.org/abs/2404.00211) | 该论文提出了一种新颖的分解推理方法(MCRank)，用于解决大型语言模型在多条件排序任务中性能下降的问题。 |
| [^142] | [EventGround: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs](https://arxiv.org/abs/2404.00209) | 提出了一个名为EventGround的框架，旨在解决将自由文本与以事件为中心的知识图谱进行关联的问题，以进行情境化叙事推理 |
| [^143] | [Causal Inference for Human-Language Model Collaboration](https://arxiv.org/abs/2404.00207) | 本文研究了人类和语言模型之间的协作动态，并提出了一个新的因果估计Incremental Stylistic Effect来解释如何改变合作结果。 |
| [^144] | [Conceptual and Unbiased Reasoning in Language Models](https://arxiv.org/abs/2404.00205) | 提出了一个新颖的概念化框架，强制语言模型在抽象问题上进行概念推理，揭示现有大型语言模型在概念推理方面的不足，并探讨了如何通过改进模型来实现高级抽象推理，从而促进无偏和泛化决策。 |
| [^145] | [GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream Neural Network Enhancement with LLMs](https://arxiv.org/abs/2404.00189) | GPTA引入了一个大型语言模型辅助训练框架，通过前缀提示增强了下游任务模型的训练，不仅显著提高了模型性能，还有效减少了低资源场景下的过拟合。 |
| [^146] | [DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries](https://arxiv.org/abs/2404.00188) | 评估了 OpenAI 的 GPT-3.5 模型作为“语言数据科学家”，成功回答了与基准数据集相关的数据科学查询。 |
| [^147] | [Word Ladders: A Mobile Application for Semantic Data Collection](https://arxiv.org/abs/2404.00184) | Word Ladders是一个用于语义数据收集的移动应用，可通过分类包含的语义关系建立单词列表，具有应用于自然语言处理任务以及认知科学问题调查的潜力。 |
| [^148] | [The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks](https://arxiv.org/abs/2404.00176) | LSCD基准测试提供了一个标准化的LSCD评估平台，解决了模型评估和结果复现中存在的异质性问题。 |
| [^149] | [Individual Text Corpora Predict Openness, Interests, Knowledge and Level of Education](https://arxiv.org/abs/2404.00165) | 本研究探讨了个人对体验的开放性这一人格维度是否可以通过个体的谷歌搜索历史预测，并通过相似性特征基于个人文本语料库来解释35%的开放性方差。 |
| [^150] | [On-the-fly Definition Augmentation of LLMs for Biomedical NER](https://arxiv.org/abs/2404.00152) | 通过实时合并相关概念的定义，本研究提出了一种新的知识增强方法以改善LLMs在生物医学NER任务中的性能，在测试数据设置下平均提高了15\%的性能。 |
| [^151] | [Classifying Conspiratorial Narratives At Scale: False Alarms and Erroneous Connections](https://arxiv.org/abs/2404.00141) | 通过作者对阴谋信仰的视角来建立一个通用方案，用于分类不同话题和线上社区中的阴谋讨论，利用BERT模型进行训练，较之生成式方法效果更好。 |
| [^152] | [Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish](https://arxiv.org/abs/2404.00124) | 挑战是缺乏公开可用数据集或可靠资源，针对索拉尼库尔德语方言的六个方言建立了一个不平衡数据集并利用三种深度学习模型进行分析。 |
| [^153] | [A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks](https://arxiv.org/abs/2404.00076) | 提出了一种后门攻击方法，名为“DirtyFlipping”，利用脏标签技术在选定的数据模式中输入触发器，从而实现隐蔽的后门。 |
| [^154] | [Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2404.00051) | 提出了ChapTER，一个对比历史建模框架，通过前缀调整实现文本与时间的平衡，用于时间知识图推理。 |
| [^155] | [LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning](https://arxiv.org/abs/2404.00027) | 探讨使用大型语言模型作为写作助手引发的写作所有权感和作者身份认知之间的心理困境。 |
| [^156] | [Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs](https://arxiv.org/abs/2404.00026) | 研究探讨了人们日益依赖的基于LLM的写作助手对创造力和个性可能造成的负面影响，旨在改进人机交互系统和提升写作助手的个性化和个性化功能。 |
| [^157] | [Psittacines of Innovation? Assessing the True Novelty of AI Creations](https://arxiv.org/abs/2404.00017) | AI系统在生成项目标题方面展现出独特的创新能力，即使在任务复杂性增加和计算能力极限的情况下，生成的内容具有表面有效性。 |
| [^158] | [Stress index strategy enhanced with financial news sentiment analysis for the equity markets](https://arxiv.org/abs/2404.00012) | 通过将金融压力指标与财经新闻情感分析相结合，提高了股票市场的风险控制策略表现。 |
| [^159] | [A novel interface for adversarial trivia question-writing](https://arxiv.org/abs/2404.00011) | 该研究提出了一个新颖的界面，通过人类生成的对抗谜题数据来帮助训练问答人工智能，并提供一套工具协助编写更具挑战性的问题。 |
| [^160] | [DiJiang: Efficient Large Language Models through Compact Kernelization](https://arxiv.org/abs/2403.19928) | DiJiang提出了一种新颖的频域核方法，可以将预训练的基本Transformer模型转化为具有线性复杂度的模型，大大减少训练成本，并在理论上提供更好的逼近效率。 |
| [^161] | [NJUST-KMG at TRAC-2024 Tasks 1 and 2: Offline Harm Potential Identification](https://arxiv.org/abs/2403.19713) | 该研究提出了在TRAC-2024离线危害潜在性识别任务中的方法，利用专家标注的社交媒体评论数据集，成功设计了能够准确评估危害可能性并识别目标的算法，在两个赛道中取得第二名的成绩。 |
| [^162] | [Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models](https://arxiv.org/abs/2403.19647) | 该论文介绍了一种新方法，即稀疏特征电路，可以在语言模型中发现和编辑可解释的因果图，为我们提供了对未预料机制的详细理解和包含了用于提高分类器泛化能力的SHIFT方法。 |
| [^163] | [TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios](https://arxiv.org/abs/2403.19318) | TableLLM是一个拥有130亿参数的强大大语言模型，专门用于熟练处理表格数据操作任务，通过远程监督方法和交叉验证策略，TableLLM相对于其他现有的通用和表格数据专注的LLMs具有明显优势。 |
| [^164] | [Compressing Large Language Models by Streamlining the Unimportant Layer](https://arxiv.org/abs/2403.19135) | 通过观察大型语言模型中不同层对隐藏状态的影响程度，提出了LLM-Streamline方法，包括层剪枝和层替换，用于压缩模型并保持性能。 |
| [^165] | [SemEval Task 1: Semantic Textual Relatedness for African and Asian Languages](https://arxiv.org/abs/2403.18933) | 这个任务涉及14种非洲和亚洲语言的语义文本相关性，旨在考察跨语言的语义相关性现象。 |
| [^166] | [SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens](https://arxiv.org/abs/2403.18647) | SDSAT提出了一种加速大型语言模型推断的方案，通过使用具有灵活解码能力的语义自适应令牌，可以增强模型生成高质量草稿令牌的能力，并实现超过3.5倍和3.0倍的速度提升。 |
| [^167] | [Chinese Offensive Language Detection:Current Status and Future Directions](https://arxiv.org/abs/2403.18314) | 总体而言，这篇论文讨论了在中文中检测 offensive 语言的挑战，并强调了开发解决这一问题的特定模型和工具。 |
| [^168] | [Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens](https://arxiv.org/abs/2403.17407) | 通过引入区域指导标记技术，本文提出了一种将孟加拉文本与地方方言转录为国际音标的方法，为模型提供了关于输入文本的地区方言信息，以理解与每个地区相关的独特音韵模式。 |
| [^169] | [Attribute First, then Generate: Locally-attributable Grounded Text Generation](https://arxiv.org/abs/2403.17104) | 该论文提出了一种局部可归属的文本生成方法，通过“先增加属性，然后生成”的方式将生成过程分为内容选择、句子规划和序列句子生成三个步骤，以简化引用验证工作。 |
| [^170] | [IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models](https://arxiv.org/abs/2403.15952) | 提出了IllusionVQA数据集，用于测试视觉语言模型在错觉和难解场景下的表现，研究发现在理解任务和定位任务上，表现最佳的VLM为GPT4V，而人类表现更胜一筹。 |
| [^171] | [Understanding Emergent Abilities of Language Models from the Loss Perspective](https://arxiv.org/abs/2403.15796) | 本文从损失角度重新定义了语言模型的突现能力，发现具有相同预训练损失的模型在不同任务上表现相似，而当预训练损失低于特定阈值时，模型将展现出突现能力。 |
| [^172] | [Differentially Private Next-Token Prediction of Large Language Models](https://arxiv.org/abs/2403.15638) | 提出了Private Mixing of Ensemble Distributions (PMixED)：通过将模型的输出分布投影到公共LLM的输出分布周围的集合上，并采样平均来实现实际的下一个标记预测，以更轻量化的方式实现对隐私敏感的大型语言模型的预测。 |
| [^173] | [NaturalTurn: A Method to Segment Transcripts into Naturalistic Conversational Turns](https://arxiv.org/abs/2403.15615) | NaturalTurn是一种专门设计用于准确捕捉自然对话交流动态的轮次分割算法，通过区分说话者的主要对话轮次和听众的次要话语，能够比现有方法更好地提取转录信息。 |
| [^174] | [LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models](https://arxiv.org/abs/2403.15388) | PruMerge提出了一种自适应的视觉令牌减少方法，可以有效减少大型多模态模型中的视觉令牌数量，同时保持模型性能。 |
| [^175] | [Multi-Review Fusion-in-Context](https://arxiv.org/abs/2403.15351) | 本文提出了一种融合上下文的多审阅文本生成方法，开发了评论领域的数据集，并提出了评估框架。 |
| [^176] | [Specifying Genericity through Inclusiveness and Abstractness Continuous Scales](https://arxiv.org/abs/2403.15278) | 该论文介绍了一种新的注释框架，用于对自然语言中名词短语的泛指性进行细粒度建模，通过连续的评注方法捕捉了泛指性的微妙方面，为语言学家提供了实用资源。 |
| [^177] | [ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training](https://arxiv.org/abs/2403.14589) | 提出了A$^3$T框架，通过ActRe提示代理实现了ReAct风格代理对代理轨迹的自主标注，同时增强了新的轨迹合成能力。 |
| [^178] | [C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion](https://arxiv.org/abs/2403.14119) | 本文研究了在测试时提示调整过程中通过利用CLIP的固有属性来探讨校准的方法，发现提示选择显著影响了CLIP中的校准，其中导致更高文本特征离散性的提示会产生更好校准的预测。 |
| [^179] | [ZigMa: Zigzag Mamba Diffusion Model](https://arxiv.org/abs/2403.13802) | 本研究提出了一种名为Zigzag Mamba的零参数方法，通过纠正当前Mamba-based视觉方法中对空间连续性的忽视，实现了更好的速度和内存利用，同时在大分辨率视觉数据集上展示了出色的性能。 |
| [^180] | [Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts](https://arxiv.org/abs/2403.13362) | 通过创建使用 GPT-2 的机器人账户，在社交媒体平台上回复用户的推文，鼓励用户接触和关注验证的、意识形态平衡的新闻，以增加用户接触这些新闻并提高参与度。 |
| [^181] | [X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment](https://arxiv.org/abs/2403.11399) | 提出了两种成本有效的方法解决大规模多模态模型训练数据的挑战，并在英语-韩语-中文多语言、多模态训练数据集上开发了表现优越的双语多模态模型。 |
| [^182] | [What Makes Math Word Problems Challenging for LLMs?](https://arxiv.org/abs/2403.11369) | 研究了大型语言模型在处理数学问题的文字题目时所面临的困难，通过分析特征和训练分类器来预测模型对不同类型题目的表现。 |
| [^183] | [Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment](https://arxiv.org/abs/2403.11124) | 更多的响应但更少的提示可以更好地触发大型语言模型进行人类对齐，此外，提出了一种新的提示多样性公式，可以进一步影响LLMs的最终性能。 |
| [^184] | [MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation](https://arxiv.org/abs/2403.09522) | 提出了MT-Patcher框架，实现了从大型语言模型到中等规模机器翻译模型的有选择性、全面和主动的知识迁移 |
| [^185] | [Ethos: Rectifying Language Models in Orthogonal Parameter Space](https://arxiv.org/abs/2403.08994) | Ethos提出了一种新的高效方法，通过在任务向量上进行矫正LMs以减轻产生毒性和偏见输出以及避免隐私泄露。 |
| [^186] | [Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2403.07440) | 该论文提出了基于矩阵变换的低秩调整（MTLoRA）方法，受大脑启发，用于提高微调技术的复杂任务适应性、性能、稳定性和算法复杂性。 |
| [^187] | [SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression](https://arxiv.org/abs/2403.07378) | SVD-LLM是一种新的基于SVD的LLM压缩方法，通过截断感知数据白化策略和逐层闭式模型参数更新策略，解决了现有方法的限制，实现了直接映射奇异值和压缩损失之间的关系。 |
| [^188] | [FLAP: Flow Adhering Planning with Constrained Decoding in LLMs](https://arxiv.org/abs/2403.05766) | 本文研究了在任务导向对话中通过遵循预定义流程和保留API依赖性解决用户意图的忠实规划，提出了一种基于前瞻启发式的受限解码算法。 |
| [^189] | [Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs](https://arxiv.org/abs/2403.04801) | 使用LLM代理进行黑盒提示优化方法，揭示了受害代理中更高级别的记忆化，相比直接用训练数据提示目标模型，这种方法更有效，能更好地量化LLMs的记忆化。 |
| [^190] | [ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies](https://arxiv.org/abs/2403.01139) | 设计了ParallelPARC流水线，利用大型语言模型生成复杂段落类比数据集，评估各种类比类型，并展示出人类在类比识别中的优势。 |
| [^191] | [UrbanGPT: Spatio-Temporal Large Language Models](https://arxiv.org/abs/2403.00813) | 都市GPT旨在建立一个具有强大泛化能力的时空模型，借鉴大型语言模型的成就。 |
| [^192] | [LLM-Resistant Math Word Problem Generation via Adversarial Attacks](https://arxiv.org/abs/2402.17916) | 本研究提出了一种新方法，通过生成保留原问题结构难度但针对LLMs无解的对抗性示例，有效地降低了LLMs的数学问题解决能力。 |
| [^193] | [StructLM: Towards Building Generalist Models for Structured Knowledge Grounding](https://arxiv.org/abs/2402.16671) | StructLM系列模型基于全面指令调整数据集，超越了大多数任务特定模型，在结构化知识连接任务中取得了新的最先进成就。 |
| [^194] | [ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters](https://arxiv.org/abs/2402.15733) | 该论文介绍了一种用于阿拉伯字符的EEG数据集ArEEG_Chars，通过深度学习实现97%的准确率，在脑机接口中具有重要意义。 |
| [^195] | [Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering](https://arxiv.org/abs/2402.14320) | Triad框架利用了基于多角色LLM代理来解决知识库问答问题，通过代理的不同角色分别处理KBQA子任务，合作完成KBQA任务，并在多个基准数据集上表现出色。 |
| [^196] | [TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization](https://arxiv.org/abs/2402.13249) | 论文提出了一个新的主题对话摘要评估基准TofuEval，研究发现现有的LLMs在对话领域存在大量事实错误的幻觉，并表明当LLMs充当事实评估器时，其表现不佳。 |
| [^197] | [How Interpretable are Reasoning Explanations from Prompting Large Language Models?](https://arxiv.org/abs/2402.11863) | 对大型语言模型推理解释提出了全面、多角度的评估，包括忠实度、强健性和效用，并引入了新的可解释性指标。 |
| [^198] | [Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima](https://arxiv.org/abs/2402.11271) | 合成信息更可能被大型模型纳入训练数据集和传播中，大型模型在传递信息时倾向于有选择地修改和丢失特定内容 |
| [^199] | [RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models](https://arxiv.org/abs/2402.10038) | 本研究提出了一种名为RS-DPO的方法，它将拒绝采样和直接优化偏好结合起来，用于对齐大型语言模型。通过开发一个经过监督微调的策略模型，并从该模型中直接采样响应，RS-DPO能够有效解决基于近端策略优化的不稳定性和高计算成本的问题。通过识别对比样本对，RS-DPO能够更好地进行RLHF。 |
| [^200] | [LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition](https://arxiv.org/abs/2402.09989) | 本文提出了RiVEG，一个统一的框架，通过利用大型语言模型（LLMs）作为连接桥梁，将多模态命名实体识别重新构建为联合任务，解决了命名实体无法确定和指代表达与命名实体之间的区别的问题。 |
| [^201] | [Long-form evaluation of model editing](https://arxiv.org/abs/2402.09394) | 长文本评估模型编辑（LEME）是一种新颖的评估协议，用于衡量模型编辑在长篇生成设置中的有效性和影响。这个协议与先前的短文本指标几乎没有关系，引入了一组新的维度来理解模型编辑方法。 |
| [^202] | [LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset](https://arxiv.org/abs/2402.09391) | 本文介绍了LlaSMol，它是一种推进化学领域大规模语言模型的方法。通过使用一个大规模、全面、高质量的指令调优数据集来训练模型，LlaSMol在化学任务中表现出强大的性能，超过了GPT-4并接近于任务特定模型。 |
| [^203] | [Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style](https://arxiv.org/abs/2402.08498) | 这项研究提出了一个新的数据集，用于生成具有证据和风格的反驳，该数据集基于Reddit ChangeMyView数据集中的帖子，并可用于论证的改进和评估。评估结果显示，GPT-3.5 turbo模型在论证质量方面表现出色，并且具有很高的风格融合能力。互惠式反驳的效果最佳。 |
| [^204] | [X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design](https://arxiv.org/abs/2402.07148) | X-LoRA是一种灵活的大型语言模型框架，利用低秩适配器专家的混合策略，可以创建精细调整的模型并在蛋白质力学和设计领域应用。该模型利用深层逐层适应的组合来解决特定任务，并受到生物学原理的启发。无需修改底层结构即可应用于任何现有的语言模型。 |
| [^205] | [TinyLLM: Learning a Small Student from Multiple Large Language Models](https://arxiv.org/abs/2402.04616) | TinyLLM是一种从多个大型语言模型中学习小型学生模型的知识蒸馏范式，旨在解决知识多样性有限和缺乏上下文信息等问题，并鼓励学生模型理解答案背后的原理。 |
| [^206] | [Dual-View Visual Contextualization for Web Navigation](https://arxiv.org/abs/2402.04476) | 本文提出了一种通过网页截图中元素的“双视图”来进行 HTML 元素的背景化的方法，这种方法通过将每个元素与其邻居元素进行背景化，使用文本和视觉特征，使得HTML元素的结果表示对于代理执行操作更加信息丰富。 |
| [^207] | [Explaining latent representations of generative models with large multimodal models](https://arxiv.org/abs/2402.01858) | 该研究提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。对于不同生成模型的解释，我们量化了解释的不确定性，通过多模态模型进行了性能评估，且定性地展示了潜在因素的变化效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。 |
| [^208] | [Tuning Language Models by Proxy](https://arxiv.org/abs/2401.08565) | 介绍了一种代理调整的轻量级解码时算法，可以通过对小型调整后的LM的预测与未调整LM的预测之间的差异来调整大型预训练LM的预测，从而实现资源节约和保留更大规模预训练的好处。 |
| [^209] | [Promptly Predicting Structures: The Return of Inference](https://arxiv.org/abs/2401.06877) | 本文提出了一个框架，通过使用结构约束和由此衍生的组合推理，可以过滤大型语言模型预测的不一致结构，从而构建有效的结构化输出，并提高性能。 |
| [^210] | [Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding](https://arxiv.org/abs/2312.17044) | 本调查从位置编码的角度总结了用于增强Transformer长度外推能力的各种方法，包括绝对和相对位置编码以及基于它们的外推方法。 |
| [^211] | [A Natural Language Processing-Based Classification and Mode-Based Ranking of Musculoskeletal Disorder Risk Factors](https://arxiv.org/abs/2312.11517) | 本研究利用自然语言处理和基于模式的排名方法对肌肉骨骼疾病的风险因素进行了分类和排名，以提高对其理解、分类和优先考虑预防和治疗的能力。 |
| [^212] | [ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity](https://arxiv.org/abs/2312.11511) | ComplexityNet通过学习任务复杂性，提高了LLM推理效率，通过预测任务的准确输出概率，成功降低了90%的计算资源使用，并在任务复杂性确定方面取得了79%准确率。 |
| [^213] | [SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models](https://arxiv.org/abs/2312.09818) | 本论文介绍了一个新任务，即视频笑声推理，旨在解释特定视频中人们笑的原因，并提出了数据集SMILE。通过利用大型语言模型生成文本视频表示，我们的基线能够生成合理的笑声解释。 |
| [^214] | [Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft](https://arxiv.org/abs/2312.09238) | 本文介绍了一种名为Auto MC-Reward的先进学习系统，利用大型语言模型自动设计稠密奖励函数来提高学习效率 |
| [^215] | [Honeybee: Locality-enhanced Projector for Multimodal LLM](https://arxiv.org/abs/2312.06742) | 该研究提出了一种既灵活又增强局部性的新型投影仪设计，有助于连接视觉编码器和语言模型，提高模型的效率和空间理解。 |
| [^216] | [Compositional Chain-of-Thought Prompting for Large Multimodal Models](https://arxiv.org/abs/2311.17076) | 提出了一种新颖的零样式思维提示（CCoT），以克服大型多模态模型难以捕捉到组合视觉推理方面的细节的问题。 |
| [^217] | [MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training](https://arxiv.org/abs/2311.17049) | MobileCLIP通过多模态强化训练实现了高效率图像-文本模型，在零-shot任务中取得了新的性能平衡。 |
| [^218] | [OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning](https://arxiv.org/abs/2311.09724) | 引入Outcome-supervised Value Model (OVM)利用结果监督训练价值模型，以在数学推理中减少错误传播，将任务转化为价值估计问题。 |
| [^219] | [Simulating Opinion Dynamics with Networks of LLM-based Agents](https://arxiv.org/abs/2311.09618) | 提出了一种基于大型语言模型（LLMs）人口的新方法来模拟意见动态，发现LLM代理存在固有偏见导致模拟代理趋向于科学现实一致的共识，但引入确认偏见后观察到意见分裂，突显了LLM代理在该领域的潜力和局限性。 |
| [^220] | [ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2311.09476) | ARES是用于评估检索增强生成系统的自动化评估框架，通过创建合成训练数据和微调评估器，有效评估RAG系统在不同任务中的表现。 |
| [^221] | [AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph](https://arxiv.org/abs/2311.09174) | 该研究提出了AbsPyramid，一个统一的蕴涵图，用于评估语言模型的抽象能力，实验证明LLMs可以通过在丰富抽象知识上进行训练来获得基本的抽象能力，并泛化到未见的事件。 |
| [^222] | [R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces](https://arxiv.org/abs/2311.09117) | R-Spin是一种高效的领域特定自我监督方法，通过学习离散的声学单元来实现说话者和噪声不变的语音表示，相比之前的方法在计算资源上减少12倍同时在严重失真语音场景中表现更好。 |
| [^223] | [Identifying Linear Relational Concepts in Large Language Models](https://arxiv.org/abs/2311.08968) | 通过线性关系概念(LRC)技术，可以在大型语言模型的隐藏激活潜在空间中找到与人类可解释概念对应的概念方向，这种技术超越了标准黑盒探测分类器。 |
| [^224] | [Safer-Instruct: Aligning Language Models with Automated Preference Data](https://arxiv.org/abs/2311.08685) | Safer-Instruct通过反向指导调整、指导感应和专家模型评估的方式，实现了自动构建大规模偏好数据的目的，从而在没有人工标注者的情况下高效生成高质量的偏好数据。 |
| [^225] | [Low-Rank Adaptation for Multilingual Summarization: An Empirical Study](https://arxiv.org/abs/2311.08572) | LoRA 在多语言摘要方面竞争力强，特别在低数据情况和跨语言转移方面表现出色。 |
| [^226] | [Fair Abstractive Summarization of Diverse Perspectives](https://arxiv.org/abs/2311.07884) | 该论文系统地研究了如何在抽象总结中实现公平性，提出了四个无参考自动度量标准，评估了九个大型语言模型，并在多个来源的数据集上进行了实验验证。 |
| [^227] | [In-context Learning and Gradient Descent Revisited](https://arxiv.org/abs/2311.07772) | 本文重新审视了在现实NLP任务和模型中的上下文学习（ICL）与梯度下降（GD）之间的联系证据，找到了评估上的不足，提出了遵循层次因果关系的简单GD优化程序来改善相似性分数。 |
| [^228] | [Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision](https://arxiv.org/abs/2311.07362) | 火山模型通过自反馈引导修订的方式，有效减少多模态幻觉问题，取得了在各项评测中的最新技术水平。 |
| [^229] | [Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models](https://arxiv.org/abs/2311.07194) | 本研究提出了一种评估大型语言模型对话理解能力的方法，通过对话总结任务检测事实一致性问题，并提出用事实问题来作为对话理解能力的灵活衡量标准，结果显示平均约有26.8%的大型语言模型生成的摘要存在事实不一致性。 |
| [^230] | [Flames: Benchmarking Value Alignment of Chinese Large Language Models](https://arxiv.org/abs/2311.06899) | 中国的大型语言模型的价值观契合性需要更加全面的评估，该研究提出了一个名为"火焰"（Flames）的基准测试，涵盖了常见的无害原则和特定中国价值观，以及复杂场景和隐含恶意的提示方法。 |
| [^231] | [Fake Alignment: Are LLMs Really Aligned Well?](https://arxiv.org/abs/2311.05915) | 研究发现大型语言模型（LLMs）在安全性评估中存在假对齐问题，提出了Fake alIgNment Evaluation (FINE)框架和两个新的度量标准，用于验证和修正这一现象 |
| [^232] | [LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection](https://arxiv.org/abs/2310.18964) | 本研究调查了预训练和微调的Large Language Models在识别仇恨言论方面的有效性和适应性，揭示了即使没有预训练，LLMs在性能上仍然具有极大优势。 |
| [^233] | [DistillSpec: Improving Speculative Decoding via Knowledge Distillation](https://arxiv.org/abs/2310.08461) | DistillSpec通过知识蒸馏技术改进了投机性解码，在标准基准测试中取得了10-45%的速度提升。 |
| [^234] | [Composite Backdoor Attacks Against Large Language Models](https://arxiv.org/abs/2310.07676) | 该研究通过复合后门攻击(CBA)展示了在大型语言模型中植入多个触发关键词的方法，相较于现有方法更为隐蔽，并确保只有当所有触发关键词同时出现时后门才会被激活。 |
| [^235] | [FTFT: Efficient and Robust Fine-Tuning by Transferring Training Dynamics](https://arxiv.org/abs/2310.06588) | 训练动态可在不同模型大小和预训练方法之间进行转移，通过选定的训练实例微调主模型实现比经验风险最小化更高的训练效率 |
| [^236] | [Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models](https://arxiv.org/abs/2310.00836) | 该研究旨在评估大型语言模型在逻辑推理中的表现，并提供了名为LogiGLUE的基准，以研究逻辑推理数据集、任务和利用语言模型进行推理的方法。 |
| [^237] | [An Examination of the Compositionality of Large Generative Vision-Language Models](https://arxiv.org/abs/2308.10509) | 本文检验了生成视觉-语言模型的组合性，发现当前基准中存在句法偏见，提出了新的评估指标SyntaxBias Score和新的基准SyntActically D。 |
| [^238] | [Citation: A Key to Building Responsible and Accountable Large Language Models](https://arxiv.org/abs/2307.02185) | 引文被确定为大型语言模型中关键但缺失的组成部分，其引入可以增强内容的透明性和可验证性以应对知识产权和伦理问题，提议LLMs的全面引文机制应考虑非参数化和参数化内容，尽管实施引文机制复杂且存在潜在缺陷，仍主张推动其发展并概述未来研究问题。 |
| [^239] | [TextFormer: A Query-based End-to-End Text Spotter with Mixed Supervision](https://arxiv.org/abs/2306.03377) | TextFormer提出了基于查询的端到端文本识别器，采用Transformer架构，通过查询嵌入实现了联合语义理解，允许多任务建模中的深层特征共享，并设计了自适应全局聚合（AGG）模块用于读取任意形状的文本。 |
| [^240] | [MCTS: A Multi-Reference Chinese Text Simplification Dataset](https://arxiv.org/abs/2306.02796) | 该论文介绍了MCTS，一个多参考的中文文本简化数据集，提供了评估数据和性能分析，同时还发布了用于训练的中文文本简化平行数据，为未来中文文本简化研究提供了基础和参考。 |
| [^241] | [BootAug: Boosting Text Augmentation via Hybrid Instance Filtering Framework](https://arxiv.org/abs/2210.02941) | 提出了BootAug，一个基于预训练语言模型的混合实例过滤框架，能够改进文本增强方法在大规模数据集上的表现，提高了约2-3%的分类准确率。 |
| [^242] | [Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation](https://arxiv.org/abs/2207.14000) | 提出了IMA-GloVe-GA，一个用于自然语言表达的多步推理的迭代神经推理网络，在超领域泛化方面具有更好的性能表现。 |
| [^243] | [Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning.](http://arxiv.org/abs/2401.15043) | 该论文介绍了一个用于健康文本简化研究的消化癌症教育材料的注释语料库，并探索了基于大型语言模型的简化方法，包括微调、增强学习、增强学习与人类反馈、领域自适应和基于提示的应用。 |
| [^244] | [Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts.](http://arxiv.org/abs/2401.14295) | 这篇论文探讨了结合结构的提示工程在提高大型语言模型推理性能方面的前景，通过思维链、思维树或思维图的设计来引导整体推理过程。通过大量实例，这种范式显著增强了模型在多个任务中的能力。总的来说，论文提供了一个通用蓝图，为未来的发展铺平道路。 |
| [^245] | [TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation.](http://arxiv.org/abs/2401.12987) | TelME是一种教师导向的多模融合网络，通过跨模态知识蒸馏实现对话中情绪识别的优化，取得了在多说话人数据集MELD上的最先进性能。 |
| [^246] | [Name Tagging Under Domain Shift via Metric Learning for Life Sciences.](http://arxiv.org/abs/2401.10472) | 本论文通过度量学习提出了一种处理生命科学领域下领域漂移的命名标签的方法，通过将源实体和目标实体投影到特征空间的不同区域来减轻源实体错误标记为目标实体的问题。 |
| [^247] | [DeepEdit: Knowledge Editing as Decoding with Constraints.](http://arxiv.org/abs/2401.10471) | DeepEdit是一种神经符号方法，通过更好的推理一致性和对更新知识的意识，提高了大型语言模型的知识编辑能力，对多跳问题数据集MQuaKE取得了显著的进展。 |
| [^248] | [LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase.](http://arxiv.org/abs/2401.05952) | 本论文介绍了混合大小写（mixcase）的概念，探讨了机器生成文本和人工生成文本的混合情景，并构建了适用于研究这些情景的数据集MixSet。通过实验证明目前的MGT检测器对于混合文本的检测效果不佳。 |
| [^249] | [Can Language Models Laugh at YouTube Short-form Videos?.](http://arxiv.org/abs/2310.14159) | 本研究在用户生成数据集中筛选并注释了10K个YouTube上的有趣多模态视频，借助GPT-3.5验证了语言和视觉元素对幽默的贡献。此外，还开发了一种零-shot视频到文本提示方法，用于大型语言模型对视频幽默的理解。这个研究填补了现有数据集中对多领域多模态幽默的不足。 |
| [^250] | [Teaching Language Models to Self-Improve through Interactive Demonstrations.](http://arxiv.org/abs/2310.13522) | 这项研究通过互动演示的方式，教导较小的语言模型具备自我提升能力，减小了最先进模型与成本效益更高模型之间的性能差距。 |
| [^251] | [Instruction Tuning with Human Curriculum.](http://arxiv.org/abs/2310.09518) | 本文探讨了在大型语言模型中应用结构化认知学习方法进行指令调整的潜在好处，并提出了一个高度结构化的合成数据集，结果表明该方法优于传统的随机化方法，提高了指令调整的性能。 |
| [^252] | [A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks.](http://arxiv.org/abs/2310.09430) | 通过对大型语言模型在非分布式逻辑推理任务上进行系统评估，我们发现这些模型在处理我们新构建的数据集时都存在困难，尽管它们在其他自然语言处理任务上表现良好。这表明这些模型在逻辑推理方面的泛化和鲁棒性仍需要进一步研究。 |
| [^253] | [LangNav: Language as a Perceptual Representation for Navigation.](http://arxiv.org/abs/2310.07889) | 该论文探索了将语言作为导航的感知表示，并使用现成的视觉系统将每个时间步骤的视图转化为自然语言描述，通过微调预训练的语言模型选择最佳的行动来满足导航指令。有两种用例的实验对这种基于语言的导航方法进行了探索：使用大型语言模型生成合成轨迹以微调较小的语言模型，以及模拟到实际的转换。 |
| [^254] | [Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models.](http://arxiv.org/abs/2310.00322) | 本文提出了红队游戏（RTG）框架，利用博弈论分析了红队语言模型（RLM）与蓝队语言模型（BLM）之间的多轮攻防互动。同时引入了游戏化红队求解器（GRTS）来提供自动化的红队技术。 |
| [^255] | [Boosting In-Context Learning with Factual Knowledge.](http://arxiv.org/abs/2309.14771) | 本文研究了使用事实知识提升上下文学习的效果，并提出了一个新的知识上下文调优框架来改善学习性能。 |
| [^256] | [Efficient Post-training Quantization with FP8 Formats.](http://arxiv.org/abs/2309.14592) | 本研究研究了FP8格式在后训练量化中的优势，并开发了一个通用的量化工作流程。实验结果表明，FP8相对于INT8具有更好的工作负载覆盖率和模型准确性。 |
| [^257] | [Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings.](http://arxiv.org/abs/2309.08591) | 本文研究了多语言LLMs在对话环境中运用谚语和俗语进行推理的能力，发现mLLMs在理解比喻性谚语、选择正确答案和推理其他语言的谚语时存在困难。 |
| [^258] | [Down the Toxicity Rabbit Hole: Investigating PaLM 2 Guardrails.](http://arxiv.org/abs/2309.06415) | 这项研究通过一个新颖的毒性兔子洞框架对PaLM 2的安全反馈进行了稳健性审计，揭示了PaLM 2生成的高度令人不安的毒性内容未被安全守护栏评估为高度不安全。 |
| [^259] | [SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning.](http://arxiv.org/abs/2309.04766) | SeaEval是一个评估多语言基础模型的基准测试，研究了模型在自然语言理解、推理以及对文化实践、细微差别和价值观的理解能力上的表现。重要发现包括模型在给出改写指令时行为各异，受到暴露偏差的影响，对于语义等价的多语言查询的回答不一致，以及模型在情感相关问题上的一致性不同。 |
| [^260] | [MoEController: Instruction-based Arbitrary Image Manipulation with Mixture-of-Expert Controllers.](http://arxiv.org/abs/2309.04372) | 本论文提出了一种基于混合专家控制器(MoEController)的指令驱动任意图像操作方法，通过对不同类型的人类指令进行适配，使得模型能够处理各种开放域图像操作任务。使用大型语言模型和条件图像合成模型生成训练数据集，并采用MOE技术和任务特定的适应性训练对模型进行训练。 |
| [^261] | [Evaluation of large language models for discovery of gene set function.](http://arxiv.org/abs/2309.04019) | 本研究评估了OpenAI的GPT-4大型语言模型在发现基因集合功能方面的能力，并发现它能根据嵌入的生物医学知识生成与Gene Ontology中具名基因集合非常相似的名称，同时在基因组学数据中发现的基因集合中，GPT-4的命名更具信息量，得到了人工审核的基本验证。 |
| [^262] | [Efficient Benchmarking (of Language Models).](http://arxiv.org/abs/2308.11696) | 本研究提出了一种名为"Efficient Benchmarking"的问题，旨在智能地减少语言模型评估的计算成本而不降低可靠性，并使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估决策的可靠性。通过HELM基准测试的案例研究，发现只需删除一个低排名模型即可改变领先者，并仅需少量示例即可得到正确的基准测试排名。 |
| [^263] | [Beam Retrieval: General End-to-End Retrieval for Multi-Hop Question Answering.](http://arxiv.org/abs/2308.08973) | Beam Retrieval是一个通用的端到端检索框架，用于多阶段问答。它通过保持多个相关文段的假设和通过最小化组合损失来优化编码器和分类头，实现了近50%的改进效果。 |
| [^264] | [DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue.](http://arxiv.org/abs/2308.08043) | DiagGPT将大型语言模型(LLMs)扩展到任务导向的对话场景，提供了在复杂诊断场景中主动提问和引导用户完成任务的能力。 |
| [^265] | [RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models.](http://arxiv.org/abs/2308.07922) | RAVEN是一种结合了检索增强的蒙特卡洛语言建模和前缀语言建模的模型，通过引入上下文融合学习，它能够在上下文学习方面取得比ATLAS更好的性能。 |
| [^266] | [XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models.](http://arxiv.org/abs/2308.01263) | 本文介绍了一个名为XSTest的测试套件，旨在识别大型语言模型中夸大的安全行为。该套件由200个安全提示组成，涵盖十种提示类型，旨在引出模型的系统性问题。 |
| [^267] | [SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering.](http://arxiv.org/abs/2307.04192) | SAS视频问答通过自适应采样策略解决了视频问答中的问题，提高了效率和准确性 |
| [^268] | [A Survey on Multimodal Large Language Models.](http://arxiv.org/abs/2306.13549) | 本文追踪和总结了多模态大语言模型（MLLM）的最新进展，包括多模态指令调整、多模态上下文学习、多模态思维链和LLM辅助视觉推理等应用，指出了现有挑战和有前途的研究方向。 |
| [^269] | [SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL.](http://arxiv.org/abs/2306.00739) | 本文提出了一种基于大语言模型的Text-to-SQL模型SQL-PaLM，使用了面向Text-to-SQL的基于执行的自一致提示方法，在Spider上实现了77.3%的测试套件准确度，并显着超越以前的最新技术的方法。 |
| [^270] | [Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models.](http://arxiv.org/abs/2305.14623) | 本文介绍了Self-Checker框架，它由即插即用的模块组成，能够在几乎零次启动的情况下利用大型语言模型进行快速高效的事实检查，这对于在资源有限的环境下构建事实检查系统非常有用。 |
| [^271] | [When Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale.](http://arxiv.org/abs/2305.14124) | 本文研究了单语言数据在多语言翻译中的作用，发现单语言数据通常有助于多语言翻译，但模型对领域不匹配的容忍性较差，尤其在较小的模型规模下。回译在数据源相似的情况下是有益的，但在其他情况下可能是有害的，而去噪自编码的效果不如先前报告的好。规模对两种方法都很重要。 |
| [^272] | [Enhancing Logical Reasoning of Large Language Models through Logic-Driven Data Augmentation.](http://arxiv.org/abs/2305.12599) | 本论文介绍了一种通过逻辑驱动的数据增强方法来增强大型语言模型的逻辑推理能力。通过将原始文本转换为抽象意义表述图，并对其进行逻辑修改和转换，生成增强数据，从而提升模型性能。该方法适用于各种体系结构的大型语言模型。 |
| [^273] | [SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification.](http://arxiv.org/abs/2305.09781) | SpecInfer是一种LLM服务系统，通过利用推测推断和令牌树验证来加速生成式大语言模型的推断过程，显著减少了为它们提供服务所需的端到端延迟和计算要求，同时确保模型质量。 |
| [^274] | [Reactive Perturbation Defocusing for Textual Adversarial Defense.](http://arxiv.org/abs/2305.04067) | RPD对大型预训练语言模型的漏洞进行了防御，通过识别对抗性示例并注入安全扰动来减少误防御，成功地修复了高达97%的对抗性示例，在自然示例的性能仅降低了约2%。 |
| [^275] | [NAIST-SIC-Aligned: Automatically-Aligned English-Japanese Simultaneous Interpretation Corpus.](http://arxiv.org/abs/2304.11766) | 本论文提出了NAIST-SIC-Aligned，这是一个自动对齐的英日平行同声传译数据集。该论文使用了一个两阶段的对齐方法，经过定量或定性验证的每个步骤，以确保语料库的质量。这是第一个开源的大规模平行SI数据集。 |
| [^276] | [Low-code LLM: Visual Programming over LLMs.](http://arxiv.org/abs/2304.08103) | 本文介绍了一种新颖的人-LLM交互框架，低代码LLM，该框架可通过六种类型的简单低代码可视化编程交互实现更可控和稳定的响应，具有可控性强、用户友好的交互方式和广泛的应用范围的优点。 |
| [^277] | [Transformer models: an introduction and catalog.](http://arxiv.org/abs/2302.07730) | 本论文介绍与分类了Transformer模型系列中最流行的模型，包括基于自监督学习和人类参与训练的模型，并对其中创新性的方面做了介绍。 |
| [^278] | [Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram.](http://arxiv.org/abs/2301.10856) | 本文研究了16个俄罗斯媒体机构和732个电报频道之间的互动，发现新闻媒体不仅通过电报传播现有的叙事，而且会从电报平台源材料，研究结果表明2.3％至26.7％的文章将主题归因于电报活动。 |
| [^279] | [Using Persuasive Writing Strategies to Explain and Detect Health Misinformation.](http://arxiv.org/abs/2211.05985) | 本研究旨在通过使用说服性写作技巧的文本段落进行分类来增加自动化虚假信息检测的新层次，以产生可解释的理由。我们提出了一个包含常见说服性写作策略的注释方案和数据集，并使用 RoBERTa 文本分类模型进行实验。 |
| [^280] | [LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models.](http://arxiv.org/abs/2206.09557) | 本文介绍了一种基于LUT的量化矩阵乘法，用于大规模生成式语言模型的高效推理。采用仅针对权重的量化策略，并提出了LUT-GEMM内核加速量化矩阵乘法，实现压缩比和准确性之间的灵活平衡。 |
| [^281] | [Improving Implicit Sentiment Learning via Local Sentiment Aggregation.](http://arxiv.org/abs/2110.08604) | 本文提出了一种本地情感聚合范式，可以提高情感学习的能力，该方法可以有效地建模方面情感相干性，并在三个公共数据集上实现了最先进的性能。 |

# 详细

[^1]: CausalChaos!数据集：基于动态视觉场景中更长因果链的全面因果行动问答

    CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes

    [https://arxiv.org/abs/2404.01299](https://arxiv.org/abs/2404.01299)

    利用卡通图像构建的CausalChaos!数据集，包含更长因果链的因果问答，通过动态互动和视觉展示挑战性因果关系，为模型提供了更多具挑战性且明确定义的因果关系。

    

    因果视频问答（QA）越来越受到关注，然而现有数据集在因果推理分析方面往往缺乏深度。为了填补这一空白，我们利用卡通的独特属性构建了CausalChaos!，这是一个新颖且具有挑战性的因果问答（Why-QA）数据集，基于标志性的“猫和老鼠”卡通系列。我们的数据集通过周到的问题和多层次答案，包含着嵌入动态互动和视觉中的更长因果链，同时动画原理允许动画师创造定义明确、明了的因果关系。这些因素使模型能够解决更具挑战性但明确定义的因果关系。我们还引入了硬负采样，包括CausalConfusion版本。虽然模型表现良好，但仍有很大改进空间，特别是在开放式答案方面。我们确定了更为先进/明确的因果关系建模和联合建模等改进方向。

    arXiv:2404.01299v1 Announce Type: cross  Abstract: Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning analysis. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic "Tom and Jerry" cartoon series. With thoughtful questions and multi-level answers, our dataset contains much longer causal chains embedded in dynamic interactions and visuals, at the same time principles of animation allows animators to create well-defined, unambiguous causal relationships. These factors allow models to solve more challenging, yet well-defined causal relationships. We also introduce hard negative mining, including CausalConfusion version. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling and joint modeling of 
    
[^2]: 通过可控大语言模型实现安全和帮助平衡的回应

    Towards Safety and Helpfulness Balanced Responses via Controllable Large Language Models

    [https://arxiv.org/abs/2404.01295](https://arxiv.org/abs/2404.01295)

    通过控制大语言模型中的属性，实现安全和帮助之间的平衡，并且可以在不同的应用场景中实现这种平衡。

    

    随着大型语言模型(LLMs)如今变得容易获得，安全和帮助之间的权衡会显著影响用户体验。一个优先考虑安全的模型会导致用户感到较少参与和被协助，而优先考虑帮助性则可能导致伤害。可能的危害包括教授人们如何制造炸弹，向青少年暴露不当内容以及伤害用户的心理健康。在这项工作中，我们提出通过控制LLMs中的两个属性来在不同的用例中平衡安全性和帮助性。我们探讨了不需要额外人员注释的训练无关和微调方法，并分析了控制LLMs中安全性和帮助性的挑战。我们的实验证明了我们的方法可以重置已学习的模型并解锁其可控性。

    arXiv:2404.01295v1 Announce Type: cross  Abstract: As large language models (LLMs) become easily accessible nowadays, the trade-off between safety and helpfulness can significantly impact user experience. A model that prioritizes safety will cause users to feel less engaged and assisted while prioritizing helpfulness will potentially cause harm. Possible harms include teaching people how to build a bomb, exposing youth to inappropriate content, and hurting users' mental health. In this work, we propose to balance safety and helpfulness in diverse use cases by controlling both attributes in LLM. We explore training-free and fine-tuning methods that do not require extra human annotations and analyze the challenges of controlling safety and helpfulness in LLMs. Our experiments demonstrate that our method can rewind a learned model and unlock its controllability.
    
[^3]: 评估文本到图像生成与图像到文本生成

    Evaluating Text-to-Visual Generation with Image-to-Text Generation

    [https://arxiv.org/abs/2404.01291](https://arxiv.org/abs/2404.01291)

    引入VQAScore，利用视觉问答模型生成对齐分数，取得了在图像文本对齐基准上的最先进结果

    

    尽管生成式人工智能取得了显著进展，但由于缺乏有效的度量标准和标准化基准，综合评估仍然具有挑战性。为了解决这个问题，我们引入了VQAScore，使用视觉问答（VQA）模型通过计算对简单的“这幅图表现出了'{文本}'吗？”问题的“是”答案的概率来生成对齐分数。尽管比先前的方法更简单，但使用现成模型计算的VQAScore在许多（8个）图像文本对齐基准上产生了最先进的结果。

    arXiv:2404.01291v1 Announce Type: cross  Abstract: Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a "bag of words", conflating prompts such as "the horse is eating the grass" with "the grass is eating the horse". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple "Does this figure show '{text}'?" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benc
    
[^4]: 大型语言模型能够提供认知重评，如得到指导

    Large Language Models are Capable of Offering Cognitive Reappraisal, if Guided

    [https://arxiv.org/abs/2404.01288](https://arxiv.org/abs/2404.01288)

    通过引入认知重评，这项工作将心理学原则融入大型语言模型中，为其提供先进的心理学能力。

    

    大型语言模型（LLMs）为情感支持提供了新的机会，最近的研究表明，它们可以对处于困境中的人产生共情回应。然而，长期的心理健康需要情绪自我调节，而一次性的共情回应则显得力不从心。本文通过参与认知重评迈出了第一步，认知重评是心理学从业者使用语言有针对性地改变个体对情境的负面评价的策略；这种评价被认为是人类情感体验的根源。我们假设心理学上的原则可以使LLMs具备这种先进的心理学能力，并设计了RESORT，其中包括一系列跨多个维度的重评构成，可用作LLM的指导。我们对LLM进行了首次专家评估（由持有硕士或博士学位的临床心理学家进行）。

    arXiv:2404.01288v1 Announce Type: new  Abstract: Large language models (LLMs) have offered new opportunities for emotional support, and recent work has shown that they can produce empathic responses to people in distress. However, long-term mental well-being requires emotional self-regulation, where a one-time empathic response falls short. This work takes a first step by engaging with cognitive reappraisals, a strategy from psychology practitioners that uses language to targetedly change negative appraisals that an individual makes of the situation; such appraisals is known to sit at the root of human emotional experience. We hypothesize that psychologically grounded principles could enable such advanced psychology capabilities in LLMs, and design RESORT which consists of a series of reappraisal constitutions across multiple dimensions that can be used as LLM instructions. We conduct a first-of-its-kind expert evaluation (by clinical psychologists with M.S. or Ph.D. degrees) of an LLM
    
[^5]: TWIN-GPT: 基于大语言模型的临床试验数字孪生体

    TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model

    [https://arxiv.org/abs/2404.01273](https://arxiv.org/abs/2404.01273)

    提出了基于大语言模型的数字孪生体TWIN-GPT，用于支持临床试验结果预测。

    

    最近，对虚拟临床试验产生了日益增长的兴趣，这些试验模拟了现实世界情境，有望显著增强患者安全性，加快开发速度，降低成本，并为医疗领域的更广泛科学知识贡献力量。本文提出了一种基于大语言模型的数字孪生体TWIN-GPT，用于支持临床试验结果预测。

    arXiv:2404.01273v1 Announce Type: cross  Abstract: Recently, there has been a burgeoning interest in virtual clinical trials, which simulate real-world scenarios and hold the potential to significantly enhance patient safety, expedite development, reduce costs, and contribute to the broader scientific knowledge in healthcare. Existing research often focuses on leveraging electronic health records (EHRs) to support clinical trial outcome prediction. Yet, trained with limited clinical trial outcome data, existing approaches frequently struggle to perform accurate predictions. Some research has attempted to generate EHRs to augment model development but has fallen short in personalizing the generation for individual patient profiles. Recently, the emergence of large language models has illuminated new possibilities, as their embedded comprehensive clinical knowledge has proven beneficial in addressing medical issues. In this paper, we propose a large language model-based digital twin crea
    
[^6]: 科学论文中LLM使用增加的映射

    Mapping the Increasing Use of LLMs in Scientific Papers

    [https://arxiv.org/abs/2404.01268](https://arxiv.org/abs/2404.01268)

    该论文通过对科学论文中LLM使用增加的映射进行了大规模分析，填补了学术写作中真实LLM修改内容比例缺失的空白。

    

    科学出版通过传播研究成果、促进合作、鼓励可重复性，并确保科学知识随时间可访问、可验证并不断建立，为科学奠定了基础。近年来，人们对多少人在学术写作中使用大型语言模型（LLMs）如ChatGPT及这种工具对全球科学实践可能产生何种影响进行了大量猜测。然而，我们缺乏对学术写作中实质性修改或生成内容的准确度量。为填补这一空白，我们对在arXiv、bioRxiv和自然学报系列期刊上的950,965篇论文（时间跨度从2020年1月至2024年2月）进行了首次系统性、大规模的分析，利用人群级别的统计框架来测量LLM修改内容随时间的盛行程度。我们的统计估计是在语料库级别及i

    arXiv:2404.01268v1 Announce Type: cross  Abstract: Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and i
    
[^7]: IsoBench：基于同构表示对多模态基础模型进行基准测试

    IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations

    [https://arxiv.org/abs/2404.01266](https://arxiv.org/abs/2404.01266)

    IsoBench提出了一个基准数据集，用于评估基于不同同构表示的多模态基础模型的性能差异，发现大多数模型更偏好文本表示。

    

    当前的基础模型在仅文本或图像和文本输入同时提示时表现出令人印象深刻的能力。但它们的能力是否会根据输入方式而改变呢？在这项工作中，我们提出了一个名为$\textbf{IsoBench}$的基准数据集，其中包含来自四个主要领域的问题: 数学、科学、算法和游戏。每个示例呈现了多个输入的同构表示，如视觉、文本和数学展示。IsoBench提供了细粒度的反馈，以诊断由表示形式造成的性能差距。在各种基础模型中，我们观察到在相同问题上，模型一贯偏好文本表示。最突出的是，在所有IsoBench问题上进行评估时，Claude-3 Opus在提供图像而不是文本时性能下降28.7分；同样，GPT-4 Turbo性能下降18.7分，Gemini Pro下降14.9分。

    arXiv:2404.01266v1 Announce Type: new  Abstract: Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose $\textbf{IsoBench}$, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple $\textbf{isomorphic representations}$ of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7 points worse and Gemini Pro is 14.9 p
    
[^8]: 人工智能与语言空间文档化

    Artificial Intelligence and the Spatial Documentation of Languages

    [https://arxiv.org/abs/2404.01263](https://arxiv.org/abs/2404.01263)

    本研究调查了AI模型在语言文献记录中创建语言地图的能力，并展示了如何通过AI模型促进语言的空间文献记录。

    

    技术的发展使跨学科研究变得更加容易。尤其是人工智能AI的突破为在跨学科和多学科领域工作的研究人员带来了巨大优势。本研究调查了AI模型，特别是GPT4和GPT数据分析员，在语言文献记录中创建语言地图的能力。该研究将文献学、语言地理学和人工智能相结合，展示了AI模型如何通过创建具有最少制图专业知识的语言地图促进语言的空间文献记录。该研究使用了从HDX和研究人员的田野工作中获取的CSV文件和GeoJSON文件。然后将研究数据应用于与AI模型的实时对话中，以生成语言分布地图。研究突出了这两个AI模型在生成高质量静态和int

    arXiv:2404.01263v1 Announce Type: new  Abstract: The advancement in technology has made interdisciplinary research more accessible. Particularly the breakthrough in Artificial Intelligence AI has given huge advantages to researchers working in interdisciplinary and multidisciplinary fields. This study investigates the ability of AI models, particularly GPT4 and GPT Data Analyst in creating language maps for language documentation. The study Integrates documentary linguistics linguistic geography and AI by showcasing how AI models facilitate the spatial documentation of languages through the creation of language maps with minimal cartographic expertise. The study is conducted using a CSV file and a GeoJSON file both obtained from HDX and from the researchers fieldwork. The study data is then applied in realtime conversations with the AI models in order to generate the language distribution maps. The study highlights the two AI models capabilities in generating highquality static and int
    
[^9]: FABLES：评估书籍摘要中的忠实性和内容选择

    FABLES: Evaluating faithfulness and content selection in book-length summarization

    [https://arxiv.org/abs/2404.01261](https://arxiv.org/abs/2404.01261)

    本文首次对LLM生成的虚构书籍摘要进行了忠实性和内容选择的大规模人类评估，建立了FABLES数据集，通过对26本书的3158个声明进行了注释，成功对LLM摘要进行了基于忠实性的排名

    

    虽然长文本大语言模型（LLMs）在技术上可以总结长达100K个标记的书籍，但迄今为止，文档的长度和复杂性阻碍了对忠实性等输入相关方面的评估。本文在虚构书籍的LLM生成摘要上进行了首次大规模人类评估，通过专注于2023或2024年出版的书籍摘要，雇佣在进行注释任务之前已完全阅读每本书的注释者来减少成本和认知负担，从而缓解了数据污染问题。我们收集了FABLES数据集，对26本书的LLM生成摘要中的3158个声明进行了注释，花费了5200美元，这使我们能够基于忠实性对LLM摘要进行排名：Claude-3-Opus在忠实性方面明显优于所有闭源LLMs，而开源的Mixtral与GPT-3.5-Turbo持平。

    arXiv:2404.01261v1 Announce Type: cross  Abstract: While long-context large language models (LLMs) can technically summarize book-length documents (>100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus significantly outperforms all closed-source LLMs, while the open-source Mixtral is on par with GPT-3.5-Turbo. An analysis o
    
[^10]: UniArk: 通过去偏差提高事实知识提取的泛化性和一致性

    UniArk: Improving Generalisation and Consistency for Factual Knowledge Extraction through Debiasing

    [https://arxiv.org/abs/2404.01253](https://arxiv.org/abs/2404.01253)

    通过UniArk框架，本文提出了一种基于适配器的解决方案，通过去偏差的方式提高了模型在未见提示下的泛化性和一致性

    

    最近一些论文探讨了将语言模型作为知识库的潜力，以及在提取事实知识时存在严重偏见的存在。本文聚焦于从微调中观察事实探测性能，并使用概率视角展示了语言模型中的预训练和下游微调目标之间固有的错位。我们假设同时去偏差这些目标可能是泛化到未见提示的关键。我们提出了一个基于适配器的框架UniArk，通过简单方法实现泛化和一致的事实知识提取，而不引入额外参数。大量实验表明UniArk可以显著提高模型在领域外的泛化性能，以及在各种提示下的一致性。此外，我们构建了ParaTrex，一个用于测量矛盾的大规模多样化数据集

    arXiv:2404.01253v1 Announce Type: new  Abstract: Several recent papers have investigated the potential of language models as knowledge bases as well as the existence of severe biases when extracting factual knowledge. In this work, we focus on the factual probing performance over unseen prompts from tuning, and using a probabilistic view we show the inherent misalignment between pre-training and downstream tuning objectives in language models for probing knowledge. We hypothesize that simultaneously debiasing these objectives can be the key to generalisation over unseen prompts. We propose an adapter-based framework, UniArk, for generalised and consistent factual knowledge extraction through simple methods without introducing extra parameters. Extensive experiments show that UniArk can significantly improve the model's out-of-domain generalisation as well as consistency under various prompts. Additionally, we construct ParaTrex, a large-scale and diverse dataset for measuring the incon
    
[^11]: 图片虽然代表千言万语，但每个人都能听懂吗？关于翻译具有文化相关性的图像

    An image speaks a thousand words, but can everyone listen? On translating images for cultural relevance

    [https://arxiv.org/abs/2404.01247](https://arxiv.org/abs/2404.01247)

    这项工作首次尝试翻译图像以使其具有文化相关性，构建了评估数据集并进行了多方面的人类评估，发现目前图像编辑模型在这一任务上仍存在挑战。

    

    随着多媒体内容的兴起，人类翻译越来越多地关注于文化适应，不仅限于文字，还包括图片等其他形式，以传达相同的含义。虽然有几种应用将受益于这一点，但机器翻译系统仍然局限于处理语言的口头和文字。在这项工作中，我们迈出了翻译图像以使其具有文化相关性的第一步。首先，我们构建了三个包含最先进生成模型的流水线来完成这项任务。接下来，我们构建了一个由两部分组成的评估数据集：i）概念：包括600张跨文化连贯的图像，每张图像专注于单个概念，ii）应用：包括从实际应用中筛选出的100张图像。我们对翻译后的图像进行了多方面的人类评估，以评估其文化相关性和含义保留。我们发现到目前为止，图像编辑模型在这项任务上失败了，但可以...

    arXiv:2404.01247v1 Announce Type: new  Abstract: Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset: i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image, and ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can 
    
[^12]: 大型语言模型水印的统计框架: 枢轴、检测效率和最优规则

    A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules

    [https://arxiv.org/abs/2404.01245](https://arxiv.org/abs/2404.01245)

    该论文提出了一个通用框架，用于设计大型语言模型水印的统计效率和检测规则，通过关键统计量和秘密密钥控制误报率，同时评估水印检测规则的能力。

    

    自ChatGPT于2022年11月推出以来，将几乎不可察觉的统计信号嵌入到大型语言模型（LLMs）生成的文本中，也被称为水印，已被用作从其人类撰写对应物上可证检测LLM生成文本的原则性方法。 本文介绍了一个通用灵活的框架，用于推理水印的统计效率并设计强大的检测规则。受水印检测的假设检验公式启发，我们的框架首先选择文本的枢轴统计量和由LLM提供给验证器的秘密密钥，以实现控制误报率（将人类撰写的文本错误地检测为LLM生成的错误）。 接下来，该框架允许通过获取渐近错误负率（将LLM生成文本错误地检测为人类撰写的错误）的封闭形式表达式来评估水印检测规则的能力。

    arXiv:2404.01245v1 Announce Type: cross  Abstract: Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of 
    
[^13]: 通过胜出的票方法有效地引导小型语言模型进行跨语言任务

    Effectively Prompting Small-sized Language Models for Cross-lingual Tasks via Winning Tickets

    [https://arxiv.org/abs/2404.01242](https://arxiv.org/abs/2404.01242)

    该论文引入了Lottery Ticket Prompt-learning（LTP）框架，通过将中奖票与软提示相结合，为小型语言模型提供了一种更简单、只需一次执行的提示方法，从而有效地进行跨语言任务。

    

    当前软提示方法在应用于小型模型（少于十亿个参数）时性能有限。深度提示调整涉及在每层中添加参数以增强效力，为小型模型提供了一种提示的解决方案，尽管需要精心设计的实施。在本文中，我们介绍了将中奖票与软提示相结合的Lottery Ticket Prompt-learning（LTP）框架。LTP提供了更简单的实施，并且只需要执行一次。我们在跨语言任务上展示了LTP，先前的工作依赖于外部工具，如人工设计的多语言模板和双语词典，这在资源有限的情况下可能不可行。具体来说，我们选择了在使用掩码语言建模目标进行微调时变化最大的一部分参数，然后在原始预训练语言模型之前添加软提示。

    arXiv:2404.01242v1 Announce Type: new  Abstract: Current soft prompt methods yield limited performance when applied to small-sized models (fewer than a billion parameters). Deep prompt-tuning, which entails prepending parameters in each layer for enhanced efficacy, presents a solution for prompting small-sized models, albeit requiring carefully designed implementation. In this paper, we introduce the Lottery Ticket Prompt-learning (LTP) framework that integrates winning tickets with soft prompts. The LTP offers a simpler implementation and requires only a one-time execution. We demonstrate LTP on cross-lingual tasks, where prior works rely on external tools like human-designed multilingual templates and bilingual dictionaries, which may not be feasible in a low-resource regime. Specifically, we select a subset of parameters that have been changed the most during the fine-tuning with the Masked Language Modeling objective. Then, we prepend soft prompts to the original pre-trained langua
    
[^14]: AURORA: 通过自动化神经屏幕理解来导航UI“TarPits”

    AURORA: Navigating UI Tarpits via Automated Neural Screen Understanding

    [https://arxiv.org/abs/2404.01240](https://arxiv.org/abs/2404.01240)

    通过新形式的自动化语义屏幕理解，本文提出了一种名为AURORA的技术，旨在帮助AIG工具有效地导航应用程序探索过程中的“tarpits”。

    

    在软件工程领域，近十年来的研究侧重于自动化移动应用程序测试，以帮助工程师克服软件平台所面临的独特挑战。这项工作主要以自动化输入生成工具(AIG工具)的形式出现，动态探索应用程序屏幕。然而，这些工具反复证明达到的代码覆盖率低于预期 - 尤其是在复杂的专有应用程序上。先前的研究表明，导致这些覆盖率不足的一个主要原因与所谓的“tarpits”有关，即复杂且难以导航的屏幕。

    arXiv:2404.01240v1 Announce Type: cross  Abstract: Nearly a decade of research in software engineering has focused on automating mobile app testing to help engineers in overcoming the unique challenges associated with the software platform. Much of this work has come in the form of Automated Input Generation tools (AIG tools) that dynamically explore app screens. However, such tools have repeatedly been demonstrated to achieve lower-than-expected code coverage - particularly on sophisticated proprietary apps. Prior work has illustrated that a primary cause of these coverage deficiencies is related to so-called tarpits, or complex screens that are difficult to navigate.   In this paper, we take a critical step toward enabling AIG tools to effectively navigate tarpits during app exploration through a new form of automated semantic screen understanding. We introduce AURORA, a technique that learns from the visual and textual patterns that exist in mobile app UIs to automatically detect co
    
[^15]: GFLean: 基于GF的Lean自动形式化框架

    GFLean: An Autoformalisation Framework for Lean via GF

    [https://arxiv.org/abs/2404.01234](https://arxiv.org/abs/2404.01234)

    GFLean是一个自动形式化框架，通过使用GF进行解析和线性化，同时结合神经网络和规则翻译程序，构建了一个强大的autoformalisation框架。

    

    我们提出了一个名为GFLean的自动形式化框架，用于Lean定理证明器。GFLean使用一种称为Grammatical Framework（GF）的高级语法编写工具进行解析和线性化。GFLean是用Haskell实现的。我们解释了GFLean的功能、内部工作原理并讨论其局限性。我们还讨论了如何将基于神经网络的翻译程序和基于规则的翻译程序结合起来，相互补充，构建强大的自动形式化框架。

    arXiv:2404.01234v1 Announce Type: new  Abstract: We present an autoformalisation framework for the Lean theorem prover, called GFLean. GFLean uses a high-level grammar writing tool called Grammatical Framework (GF) for parsing and linearisation. GFLean is implemented in Haskell. We explain the functionalities of GFLean, its inner working and discuss its limitations. We also discuss how we can use neural network based translation programs and rule based translation programs together complimenting each other to build robust autoformalisation frameworks.
    
[^16]: 具有多模式原型的开放词汇联邦学习

    Open-Vocabulary Federated Learning with Multimodal Prototyping

    [https://arxiv.org/abs/2404.01232](https://arxiv.org/abs/2404.01232)

    本研究针对联邦学习中的开放词汇挑战，提出了一种基于预训练视觉语言模型的新型自适应框架，命名为联邦多模式原型（Fed-MP），用于解决新用户提出的涉及任意未知类别的查询问题。

    

    现有的联邦学习（FL）研究通常假设训练标签空间和测试标签空间是相同的。然而，在真实应用中，这个假设太理想化了。新用户可能提出涉及来自未见类别数据的查询，这些开放词汇查询将直接导致这种FL系统的缺陷。因此，在这项工作中，我们明确关注FL中尚未开发的开放词汇挑战。也就是说，对于一个新用户，全局服务器应该理解她/他的查询涉及任意未知类别。为了解决这个问题，我们利用了预训练的视觉语言模型（VLMs）。具体来说，我们提出了一个针对FL环境中VLMs的新型自适应框架，命名为联邦多模式原型（Fed-MP）。Fed-MP根据轻量级客户端残差自适应聚合本地模型权重，并根据一个新颖的多模式原型机制进行预测。

    arXiv:2404.01232v1 Announce Type: new  Abstract: Existing federated learning (FL) studies usually assume the training label space and test label space are identical. However, in real-world applications, this assumption is too ideal to be true. A new user could come up with queries that involve data from unseen classes, and such open-vocabulary queries would directly defect such FL systems. Therefore, in this work, we explicitly focus on the under-explored open-vocabulary challenge in FL. That is, for a new user, the global server shall understand her/his query that involves arbitrary unknown classes. To address this problem, we leverage the pre-trained vision-language models (VLMs). In particular, we present a novel adaptation framework tailored for VLMs in the context of FL, named as Federated Multimodal Prototyping (Fed-MP). Fed-MP adaptively aggregates the local model weights based on light-weight client residuals, and makes predictions based on a novel multimodal prototyping mechan
    
[^17]: LLM作为智囊团：大型语言模型在战略推理中的调查

    LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models

    [https://arxiv.org/abs/2404.01230](https://arxiv.org/abs/2404.01230)

    本文对大型语言模型在战略推理领域的现状和机遇进行了全面调查，旨在系统梳理澄清该主题的零散文献，突出了其决策性能的跨学科方法。

    

    本文全面调查了大型语言模型（LLMs）在战略推理领域的现状和机遇，战略推理是一种复杂形式的推理，需要理解和预测多智体环境中对手行为，并相应调整策略。我们探讨了与LLMs进行战略推理相关的范围、应用、方法论和评估指标，突出了该领域的日益发展以及增强其决策性能的跨学科方法。本文旨在系统梳理并澄清该主题的零散文献，提供一个强调战略推理作为重要认知能力的系统性回顾。

    arXiv:2404.01230v1 Announce Type: new  Abstract: This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly. Strategic reasoning is distinguished by its focus on the dynamic and uncertain nature of interactions among multi-agents, where comprehending the environment and anticipating the behavior of others is crucial. We explore the scopes, applications, methodologies, and evaluation metrics related to strategic reasoning with LLMs, highlighting the burgeoning development in this area and the interdisciplinary approaches enhancing their decision-making performance. It aims to systematize and clarify the scattered literature on this subject, providing a systematic review that underscores the importance of strategic reasoning as a critical cognitive 
    
[^18]: 稳定代码技术报告

    Stable Code Technical Report

    [https://arxiv.org/abs/2404.01226](https://arxiv.org/abs/2404.01226)

    Stable Code是一个新一代的代码语言模型，提供通用基础代码语言模型，支持代码完成、推理、数学等软件工程任务，同时还引入了具有自然对话界面的指令变体Stable Code Instruct，可执行问答和基于指令的任务。

    

    我们介绍了Stable Code，作为我们新一代代码语言模型系列中的第一个，它作为一个通用基础代码语言模型，目标是完成代码、推理、数学和其他软件工程任务。此外，我们介绍了一个名为Stable Code Instruct的指令变体，允许与模型进行自然对话界面，执行问答和基于指令的任务。在这个技术报告中，我们详细介绍了导致这两个模型的数据和训练过程。它们的权重可以通过Hugging Face获得，任何人都可以在https://huggingface.co/stabilityai/stable-code-3b和https://huggingface.co/stabilityai/stable-code-instruct-3b上下载和使用。该报告包含对模型的彻底评估，包括多语言编程基准测试，以及重点放在多轮对话的MT基准测试上。在发布时，Stable Code是最先进的。

    arXiv:2404.01226v1 Announce Type: new  Abstract: We introduce Stable Code, the first in our new-generation of code language models series, which serves as a general-purpose base code language model targeting code completion, reasoning, math, and other software engineering-based tasks. Additionally, we introduce an instruction variant named Stable Code Instruct that allows conversing with the model in a natural chat interface for performing question-answering and instruction-based tasks. In this technical report, we detail the data and training procedure leading to both models. Their weights are available via Hugging Face for anyone to download and use at https://huggingface.co/stabilityai/stable-code-3b and https://huggingface.co/stabilityai/stable-code-instruct-3b. This report contains thorough evaluations of the models, including multilingual programming benchmarks, and the MT benchmark focusing on multi-turn dialogues. At the time of its release, Stable Code is the state-of-the-art 
    
[^19]: AILS-NTUA参加SemEval-2024任务6：用于幻觉检测和分析的高效模型调优

    AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for hallucination detection and analysis

    [https://arxiv.org/abs/2404.01210](https://arxiv.org/abs/2404.01210)

    该论文提出了一种高效的模型调优策略，通过将预训练模型和自然语言推理模型进行集成，成功在幻觉检测任务中取得了77.8%和79.9%的准确率，优于组织者的基线和竞赛中其他参赛者的表现。

    

    在本文中，我们介绍了我们团队针对SemEval-2024任务6 - SHROOM的提交内容，这是一个关于幻觉和相关可观测过度生成错误的共享任务。参与者被要求进行二元分类，以识别流利过度生成的幻觉案例。我们的实验包括对幻觉检测和自然语言推理（NLI）模型进行微调。最成功的策略涉及创建这些模型的集成，结果在模型不可知和模型感知数据集上的准确率分别为77.8％和79.9％，超过了组织者的基线，并在与竞赛中表现最佳的结果进行对比时取得显着成果，该竞赛报告的准确率分别为84.7％和81.3％。

    arXiv:2404.01210v1 Announce Type: new  Abstract: In this paper, we present our team's submissions for SemEval-2024 Task-6 - SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes. The participants were asked to perform binary classification to identify cases of fluent overgeneration hallucinations. Our experimentation included fine-tuning a pre-trained model on hallucination detection and a Natural Language Inference (NLI) model. The most successful strategy involved creating an ensemble of these models, resulting in accuracy rates of 77.8% and 79.9% on model-agnostic and model-aware datasets respectively, outperforming the organizers' baseline and achieving notable results when contrasted with the top-performing results in the competition, which reported accuracies of 84.7% and 81.3% correspondingly.
    
[^20]: 大型语言模型预训练与下游能力分析的微妙平衡

    The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis

    [https://arxiv.org/abs/2404.01204](https://arxiv.org/abs/2404.01204)

    本文对大型语言模型在预训练过程中不同能力的综合分析揭示了其动态差异，填补了现有标度定律中的缺失。

    

    揭示反映最终模型性能的早期指标是大规模预训练的一个核心原则。现有的标度定律展示了预训练损失与训练浮点数之间的幂律相关性，这对于大型语言模型当前训练状态的重要指标十分关键。然而，这一原则只关注模型在训练数据上的压缩特性，导致与下游任务能力的提升之间存在不一致性。一些后续研究试图将标度定律扩展到更复杂的指标（如超参数），但仍然缺乏对预训练过程中各种能力之间动态差异的全面分析。为解决上述限制，本文对各种预训练中间检查点下模型能力进行了全面比较。

    arXiv:2404.01204v1 Announce Type: new  Abstract: Uncovering early-stage metrics that reflect final model performance is one core principle for large-scale pretraining. The existing scaling law demonstrates the power-law correlation between pretraining loss and training flops, which serves as an important indicator of the current training state for large language models. However, this principle only focuses on the model's compression properties on the training data, resulting in an inconsistency with the ability improvements on the downstream tasks. Some follow-up works attempted to extend the scaling-law to more complex metrics (such as hyperparameters), but still lacked a comprehensive analysis of the dynamic differences among various capabilities during pretraining. To address the aforementioned limitations, this paper undertakes a comprehensive comparison of model capabilities at various pretraining intermediate checkpoints. Through this analysis, we confirm that specific downstream
    
[^21]: 从文档级分布中估计词汇复杂度

    Estimating Lexical Complexity from Document-Level Distributions

    [https://arxiv.org/abs/2404.01196](https://arxiv.org/abs/2404.01196)

    本研究提出了一个从文档级分布中估计词汇复杂度的两步方法，不依赖于任何预注释数据，用于支持健康从业者创建更好的工具。

    

    现有的复杂性估计方法通常是针对整个文档开发的。这种范围的限制使得它们无法应用于较短的文本片段，如健康评估工具。这些工具通常由独立句子的列表组成，这些句子都太短，以致于现有的方法无法应用。在这些评估工具中所选择的措辞至关重要，因为受访患者群体的认知能力和语言能力可能存在显著差异。为了为支持卫生从业者创建更好的工具，我们开发了一个不依赖于任何预注释数据的估计词汇复杂度的两步方法。我们实现了这个方法用于挪威语，并使用统计测试和对实际评估工具样本的定性评估验证了其有效性。我们还研究了我们的复杂度测量和某些特征之间的关系。

    arXiv:2404.01196v1 Announce Type: new  Abstract: Existing methods for complexity estimation are typically developed for entire documents. This limitation in scope makes them inapplicable for shorter pieces of text, such as health assessment tools. These typically consist of lists of independent sentences, all of which are too short for existing methods to apply. The choice of wording in these assessment tools is crucial, as both the cognitive capacity and the linguistic competency of the intended patient groups could vary substantially. As a first step towards creating better tools for supporting health practitioners, we develop a two-step approach for estimating lexical complexity that does not rely on any pre-annotated data. We implement our approach for the Norwegian language and verify its effectiveness using statistical testing and a qualitative evaluation of samples from real assessment tools. We also investigate the relationship between our complexity measure and certain feature
    
[^22]: 从电子健康记录生成忠实完整的医院诊疗摘要

    Generating Faithful and Complete Hospital-Course Summaries from the Electronic Health Record

    [https://arxiv.org/abs/2404.01189](https://arxiv.org/abs/2404.01189)

    本论文研究了如何从电子健康记录生成忠实完整的医院诊疗摘要，并提出并评估了自动化解决方案。

    

    电子健康记录的快速普及在简化行政任务、增加透明度以及促进跨医护人员连续性护理方面发挥了重要作用。然而，增加了文档记录负担的一个意外后果是减少了与患者面对面的时间，同时也大幅增加了临床医生的疲劳。本论文着重研究一个尤为耗时但关键的文档记录任务：生成病人住院的摘要，并提出并评估自动化解决方案。在第2章中，我们基于10.9万次住院（2百万源注释）构建了一个数据集，并进行探索性分析以激励未来关于建模和评估的工作[NAACL 2021]。在第3章中，我们从建模角度解决了忠实性问题，通过修订有噪参考[EMNLP 2022]，为了减少对参考的依赖，直接校准模型输出到指标[ACL 202]。

    arXiv:2404.01189v1 Announce Type: new  Abstract: The rapid adoption of Electronic Health Records (EHRs) has been instrumental in streamlining administrative tasks, increasing transparency, and enabling continuity of care across providers. An unintended consequence of the increased documentation burden, however, has been reduced face-time with patients and, concomitantly, a dramatic rise in clinician burnout. In this thesis, we pinpoint a particularly time-intensive, yet critical, documentation task: generating a summary of a patient's hospital admissions, and propose and evaluate automated solutions. In Chapter 2, we construct a dataset based on 109,000 hospitalizations (2M source notes) and perform exploratory analyses to motivate future work on modeling and evaluation [NAACL 2021]. In Chapter 3, we address faithfulness from a modeling perspective by revising noisy references [EMNLP 2022] and, to reduce the reliance on references, directly calibrating model outputs to metrics [ACL 202
    
[^23]: 一种神经符号方法来监测食物中的盐含量

    A Neuro-Symbolic Approach to Monitoring Salt Content in Food

    [https://arxiv.org/abs/2404.01182](https://arxiv.org/abs/2404.01182)

    通过整合神经符号规则，该对话系统在监测食物中的盐含量方面取得了超过20%的联合目标准确率的改善。

    

    我们提出了一种对话系统，使心力衰竭患者能够查询食物中的盐含量，并帮助他们监测和减少盐的摄入量。针对食物盐含量查询缺乏特定数据集的问题，我们开发了基于模板的对话数据集。该数据集结构化地提出澄清问题，以识别食物及其盐含量。我们的研究结果表明，虽然在该数据集上微调基于Transformer的模型的性能有限，但是整合神经符号规则显著提高了系统的性能。我们的实验表明，通过整合神经符号规则，相较于天真地微调基于Transformer的模型，我们的系统在不同数据规模下实现了超过20%的联合目标准确率的改善。

    arXiv:2404.01182v1 Announce Type: new  Abstract: We propose a dialogue system that enables heart failure patients to inquire about salt content in foods and help them monitor and reduce salt intake. Addressing the lack of specific datasets for food-based salt content inquiries, we develop a template-based conversational dataset. The dataset is structured to ask clarification questions to identify food items and their salt content. Our findings indicate that while fine-tuning transformer-based models on the dataset yields limited performance, the integration of Neuro-Symbolic Rules significantly enhances the system's performance. Our experiments show that by integrating neuro-symbolic rules, our system achieves an improvement in joint goal accuracy of over 20% across different data sizes compared to naively fine-tuning transformer-based models.
    
[^24]: 用多模态大型语言模型对环境生态系统进行建模

    LITE: Modeling Environmental Ecosystems with Multimodal Large Language Models

    [https://arxiv.org/abs/2404.01165](https://arxiv.org/abs/2404.01165)

    使用LITE模型，可以将不同的环境变量转换成自然语言描述和折线图像，并利用统一编码器来捕捉空间-时间关系，从而更好地预测环境变量。

    

    对环境生态系统进行建模在可持续管理地球的过程中发挥关键作用。精确预测空间和时间上的关键环境变量可以帮助制定明智的政策和决策，从而改善人们的生活。最近，基于深度学习的方法在建模空间-时间关系以预测环境变量方面显示出潜力。然而，这些方法通常在处理不完整特征和分布变化方面表现不佳，环境数据中常见这些问题是由于数据收集成本高昂和测量仪器失灵造成的。为了解决这些问题，我们提出了LITE——用于环境生态系统建模的多模态大型语言模型。具体来说，LITE通过将不同环境变量转换成自然语言描述和折线图像来统一它们。然后，LITE利用统一编码器来捕捉

    arXiv:2404.01165v1 Announce Type: new  Abstract: The modeling of environmental ecosystems plays a pivotal role in the sustainable management of our planet. Accurate prediction of key environmental variables over space and time can aid in informed policy and decision-making, thus improving people's livelihood. Recently, deep learning-based methods have shown promise in modeling the spatial-temporal relationships for predicting environmental variables. However, these approaches often fall short in handling incomplete features and distribution shifts, which are commonly observed in environmental data due to the substantial cost of data collection and malfunctions in measuring instruments. To address these issues, we propose LITE -- a multimodal large language model for environmental ecosystems modeling. Specifically, LITE unifies different environmental variables by transforming them into natural language descriptions and line graph images. Then, LITE utilizes unified encoders to capture 
    
[^25]: 与机器人对话：扩大SLIVAR社区参与和研究的提案

    Dialogue with Robots: Proposals for Broadening Participation and Research in the SLIVAR Community

    [https://arxiv.org/abs/2404.01158](https://arxiv.org/abs/2404.01158)

    本文提出了三项提案：教育、基准测试以及涉及与机器人口头交互时对语言建模的研究，以促进扩大SLIVAR社区参与和研究。

    

    使用自然人类语言与机器进行交互的能力不仅变得司空见惯，而且越来越受到期待。下一个步骤不仅是文本界面，而是语音界面，不仅是与计算机，而是与包括机器人在内的所有机器的对话。在本文中，我们记录了最近这一日益增长的与机器人对话领域的历史，并向社区提供了三个提案，第一个侧重于教育，第二个侧重于基准测试，第三个侧重于在涉及与机器人的口头交互时对语言的建模。这三个提案应该作为任何研究人员可借鉴和建立的白皮书。

    arXiv:2404.01158v1 Announce Type: new  Abstract: The ability to interact with machines using natural human language is becoming not just commonplace, but expected. The next step is not just text interfaces, but speech interfaces and not just with computers, but with all machines including robots. In this paper, we chronicle the recent history of this growing field of spoken dialogue with robots and offer the community three proposals, the first focused on education, the second on benchmarks, and the third on the modeling of language when it comes to spoken interaction with robots. The three proposals should act as white papers for any researcher to take and build upon.
    
[^26]: 绿色AI：探讨大型语言模型训练中的碳足迹、缓解策略和权衡

    Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade Offs in Large Language Model Training

    [https://arxiv.org/abs/2404.01157](https://arxiv.org/abs/2404.01157)

    该研究评估了大型语言模型的CO2排放量，强调了这些模型由于其大量的模型参数导致其碳足迹特别高。

    

    自然语言处理领域的重要研究长期以来一直不断尝试通过改进先前的模型训练方法、改变模型架构和开发更深入的数据集来创造新的创新模型。然而，随着NLP领域的快速发展，温室气体排放量不断增加，引发了人们对LLMs训练造成的环境损害的担忧。对人工智能相关成本的全面了解，尤其是与环境相关的成本，是确保安全AI模型的基础。目前，对AI模型的CO2排放的调查仍然是一个新兴的研究领域，因此，在本文中，我们评估了知名大型语言模型的CO2排放量，这些模型由于其大量的模型参数而导致其碳足迹特别高。

    arXiv:2404.01157v1 Announce Type: new  Abstract: Prominent works in the field of Natural Language Processing have long attempted to create new innovative models by improving upon previous model training approaches, altering model architecture, and developing more in-depth datasets to better their performance. However, with the quickly advancing field of NLP comes increased greenhouse gas emissions, posing concerns over the environmental damage caused by training LLMs. Gaining a comprehensive understanding of the various costs, particularly those pertaining to environmental aspects, that are associated with artificial intelligence serves as the foundational basis for ensuring safe AI models. Currently, investigations into the CO2 emissions of AI models remain an emerging area of research, and as such, in this paper, we evaluate the CO2 emissions of well-known large language models, which have an especially high carbon footprint due to their significant amount of model parameters. We arg
    
[^27]: LLM是否会对基于事实的问题的人类答案感到困惑？以Reddit为个案研究

    Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case Study on Reddit

    [https://arxiv.org/abs/2404.01147](https://arxiv.org/abs/2404.01147)

    LLMs在模拟基于事实问题的社交媒体问题中人类答案时表现较好，为未来研究提供了方向。

    

    大型语言模型（LLMs）已被证明可以熟练地正确回答在线话语环境下的问题。然而，使用LLMs来模拟人类对基于事实的社交媒体问题的回答仍未被充分探讨。在本研究中，我们调查了LLMs如何模拟在几个专题性Reddit社区（或子社区）中提出的基于事实问题的各种人类答案。我们收集并公开了409个基于事实问题和来自15个r/Ask{Topic}社区的7,534个多样化的、经人类评分的答案的数据集，这些社区覆盖了3个类别：职业、社会身份和地理位置。我们发现LLMs在模拟高评分的人类答案方面要比模拟低评分的人类答案效果显著。我们基于我们的初步发现提出了几个未来研究的方向。

    arXiv:2404.01147v1 Announce Type: new  Abstract: Large language models (LLMs) have been shown to be proficient in correctly answering questions in the context of online discourse. However, the study of using LLMs to model human-like answers to fact-driven social media questions is still under-explored. In this work, we investigate how LLMs model the wide variety of human answers to fact-driven questions posed on several topic-specific Reddit communities, or subreddits. We collect and release a dataset of 409 fact-driven questions and 7,534 diverse, human-rated answers from 15 r/Ask{Topic} communities across 3 categories: profession, social identity, and geographic location. We find that LLMs are considerably better at modeling highly-rated human answers to such questions, as opposed to poorly-rated human answers. We present several directions for future research based on our initial findings.
    
[^28]: KoCoNovel：韩国小说中人物共指的注释数据集

    KoCoNovel: Annotated Dataset of Character Coreference in Korean Novels

    [https://arxiv.org/abs/2404.01140](https://arxiv.org/abs/2404.01140)

    KoCoNovel是一个从韩国文学文本中衍生出的小说人物共指数据集，提供了详细的注释指南，是韩语公共共指解决语料库中第二大规模的数据集，也是第一个基于文学文本的数据集

    

    我们提出了KoCoNovel，这是一个从韩国文学文本中衍生出的小说人物共指数据集，配备了详细的注释指南。KoCoNovel由50部现代和当代韩国小说中的178,000个标记组成，是继NIKL语料库之后韩语公共共指解决语料库中第二大规模的数据集，也是第一个基于文学文本的数据集。为了扩大其实用性，我们提供了四个不同版本的KoCoNovel，为全知作者和读者的观点以及处理多实体的分离或重叠提供选择。这种方法整合了围绕文学文本共指解析的现有话语，为研究提供了一个全面的数据集。KoCoNovel的一个显著特点是24%的人物提及是单一普通名词，缺少所有格标记或冠词。这一特征特别受到韩语称谓的细微差别的影响。

    arXiv:2404.01140v1 Announce Type: new  Abstract: We present KoCoNovel, an novel character coreference dataset derived from Korean literary texts, complete with detailed annotation guidelines. Comprising 178K tokens from 50 modern and contemporary Korean novels, KoCoNovel stands as the second-largest public coreference resolution corpus in Korean, after the NIKL corpus, and the first to be based on literary texts. To broaden its utility, we provide four distinct versions of KoCoNovel, offering options for the perspectives of the omniscient author and readers, and for handling multiple entities as either separate or overlapping. This approach integrates existing discourse surrounding coreference resolution in literary texts, providing a comprehensive dataset for exploration. One of KoCoNovel's distinctive features is that 24% of all character mentions are single common nouns, lacking possessive markers or articles. This feature is particularly influenced by the nuances of Korean address 
    
[^29]: 结构化信息很重要：将抽象意义表示引入LLMs以改善开放领域对话评估

    Structured Information Matters: Incorporating Abstract Meaning Representation into LLMs for Improved Open-Domain Dialogue Evaluation

    [https://arxiv.org/abs/2404.01129](https://arxiv.org/abs/2404.01129)

    将抽象意义表示结合到LLMs中，提出了一个简单有效的框架用于改善开放领域对话评估

    

    arXiv:2404.01129v1 公告类型：新的 摘要：自动的开放领域对话评估已经引起越来越多的关注。可训练的评估指标通常是通过训练具有真正正例和随机选择的负例回复来训练的，导致它们倾向于将更高内容相似性的回复分配更高的得分给定一个上下文。然而，对抗性的负面回复具有与上下文高内容相似性，同时在语义上不同。因此，现有的评估指标不足以评估这类回复，导致与人类判断之间的相关性较低。虽然最近的研究已经显示出在利用大型语言模型（LLMs）进行开放领域对话评估方面有一定效果，但它们仍然在有效处理对抗性负面示例方面遇到挑战。在本文中，我们提出了一个简单而有效的框架用于开放领域对话评估，它结合了领域特定的语言模型（SLMs）。

    arXiv:2404.01129v1 Announce Type: new  Abstract: Automatic open-domain dialogue evaluation has attracted increasing attention. Trainable evaluation metrics are commonly trained with true positive and randomly selected negative responses, resulting in a tendency for them to assign a higher score to the responses that share higher content similarity with a given context. However, adversarial negative responses possess high content similarity with the contexts whilst being semantically different. Therefore, existing evaluation metrics are not robust enough to evaluate such responses, resulting in low correlations with human judgments. While recent studies have shown some efficacy in utilizing Large Language Models (LLMs) for open-domain dialogue evaluation, they still encounter challenges in effectively handling adversarial negative examples. In this paper, we propose a simple yet effective framework for open-domain dialogue evaluation, which combines domain-specific language models (SLMs
    
[^30]: SentiCSE: 一种具有情感感知对比句子嵌入框架，带有情感引导文本相似性

    SentiCSE: A Sentiment-aware Contrastive Sentence Embedding Framework with Sentiment-guided Textual Similarity

    [https://arxiv.org/abs/2404.01104](https://arxiv.org/abs/2404.01104)

    提出了一种评估情感表示质量的新指标SgTS，并设计了一种具有情感感知对比的句子嵌入框架SentiCSE。

    

    最近，情感感知的预训练语言模型（PLMs）在下游情感分析任务中展现出了令人印象深刻的结果。然而，它们忽视了评估构建的情感表示的质量；它们只专注于提高微调性能，这遮蔽了表示质量。我们认为，如果不能保证表示质量，它们的下游性能可能高度依赖于微调数据的监督，而不是表示质量。这个问题会使它们很难进入其他情感相关领域，特别是在标注数据稀缺的情况下。我们首先提出了情感引导文本相似性（SgTS），这是一种用于评估情感表示质量的新颖指标，它基于两个句子之间情感极性等价程度进行设计。然后我们提出了SentiCSE，一种新颖的具有情感感知对比的句子嵌入框架。

    arXiv:2404.01104v1 Announce Type: new  Abstract: Recently, sentiment-aware pre-trained language models (PLMs) demonstrate impressive results in downstream sentiment analysis tasks. However, they neglect to evaluate the quality of their constructed sentiment representations; they just focus on improving the fine-tuning performance, which overshadows the representation quality. We argue that without guaranteeing the representation quality, their downstream performance can be highly dependent on the supervision of the fine-tuning data rather than representation quality. This problem would make them difficult to foray into other sentiment-related domains, especially where labeled data is scarce. We first propose Sentiment-guided Textual Similarity (SgTS), a novel metric for evaluating the quality of sentiment representations, which is designed based on the degree of equivalence in sentiment polarity between two sentences. We then propose SentiCSE, a novel Sentiment-aware Contrastive Senten
    
[^31]: 你的“安全”数据中有什么？：识别破坏安全性的良性数据

    What's in Your "Safe" Data?: Identifying Benign Data that Breaks Safety

    [https://arxiv.org/abs/2404.01099](https://arxiv.org/abs/2404.01099)

    通过双向锚定方法，识别那些在微调后更可能降低模型安全性的良性数据子集，提高模型对有害请求的响应率。

    

    当前的大型语言模型（LLMs），即使经过调整以确保安全性和对齐性，也容易被越狱。一些研究表明，只是进一步使用良性数据（即没有有害内容的数据）对一个对齐模型进行微调，会导致安全性大幅下降。我们深入探讨良性微调不经意间导致越狱的数据中心方面。首先，我们通过两种视角表征微调数据：表示和梯度空间。此外，我们提出了一种双向锚定方法，该方法优先考虑靠近有害示例并远离良性示例的数据点。通过这样做，我们的方法有效地识别出更有可能在微调后降低模型安全性的良性数据子集。仅仅训练100个这些看似良性的数据点，就可以使微调模型肯定地回应超过70％的被测试的有害请求，相比之下，...

    arXiv:2404.01099v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Furthermore, we propose a bi-directional anchoring method that prioritizes data points that are close to harmful examples and distant from benign ones. By doing so, our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints can lead to the fine-tuned model affirmatively responding to > 70% of tested harmful requests, compared to <
    
[^32]: AILS-NTUA参加SemEval-2024任务9：破解脑筋急转弯：变压器模型用于横向思维谜题

    AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer Models for Lateral Thinking Puzzles

    [https://arxiv.org/abs/2404.01084](https://arxiv.org/abs/2404.01084)

    本研究通过调整多种预训练变压器模型，在SemEval-2024任务9的脑筋急转弯竞赛中取得了竞争力十足的表现，在句子谜题和单词谜题的评估中，最佳提交的准确率分别超过了最佳神经基线ChatGPT超过20%和30%。

    

    在本文中，我们概述了我们在SemEval-2024任务9竞赛中的提交:“BRAINTEASER:一个挑战常识的新任务”。我们参与了子任务A-句子谜题和子任务B-单词谜题。我们通过微调评估了大量不同尺寸的预训练变压器模型。随后，我们对它们的得分和响应进行了分析，以帮助未来的研究人员有效地理解和利用这些模型。我们排名靠前的方法在两个子任务的竞赛排行榜上获得了有竞争力的位置。在评估阶段，我们最好的提交在句子谜题中获得了81.7%的平均准确率，在单词谜题中获得了85.4%的准确率，分别比最佳神经基线（ChatGPT）高出超过20%和30%。

    arXiv:2404.01084v1 Announce Type: cross  Abstract: In this paper, we outline our submission for the SemEval-2024 Task 9 competition: 'BRAINTEASER: A Novel Task Defying Common Sense'. We engage in both sub-tasks: Sub-task A-Sentence Puzzle and Sub-task B-Word Puzzle. We evaluate a plethora of pre-trained transformer-based language models of different sizes through fine-tuning. Subsequently, we undertake an analysis of their scores and responses to aid future researchers in understanding and utilizing these models effectively. Our top-performing approaches secured competitive positions on the competition leaderboard across both sub-tasks. In the evaluation phase, our best submission attained an average accuracy score of 81.7% in the Sentence Puzzle, and 85.4% in the Word Puzzle, significantly outperforming the best neural baseline (ChatGPT) by more than 20% and 30% respectively.
    
[^33]: 大型语言模型的高效提示方法：一项调查

    Efficient Prompting Methods for Large Language Models: A Survey

    [https://arxiv.org/abs/2404.01077](https://arxiv.org/abs/2404.01077)

    提示已成为大型语言模型适应特定自然语言处理任务的主流范式，而高效提示方法在压缩提示和自动提示优化方面取得显著进展。

    

    提示已成为调整大型语言模型（LLMs）以适应特定自然语言处理任务的主流范式。尽管这种方法为LLMs的上下文学习打开了大门，但引入了额外的计算负担，即模型推理的计算负担和手动设计提示的人力劳动，特别是在使用冗长和复杂的提示来引导和控制LLMs的行为时。因此，LLM领域见证了高效提示方法的显著激增。在本文中，我们对这些方法进行了全面的概述。在较高的层面上，高效提示方法可以广泛分类为两种方式：具有高效计算的提示和具有高效设计的提示。前者涉及各种压缩提示的方法，后者采用自动提示优化技术。我们介绍了提示的基本概念，回顾了高效提示的进展，并突出显示出

    arXiv:2404.01077v1 Announce Type: new  Abstract: Prompting has become a mainstream paradigm for adapting large language models (LLMs) to specific natural language processing tasks. While this approach opens the door to in-context learning of LLMs, it brings the additional computational burden of model inference and human effort of manual-designed prompts, particularly when using lengthy and complex prompts to guide and control the behavior of LLMs. As a result, the LLM field has seen a remarkable surge in efficient prompting methods. In this paper, we present a comprehensive overview of these methods. At a high level, efficient prompting methods can broadly be categorized into two approaches: prompting with efficient computation and prompting with efficient design. The former involves various ways of compressing prompts, and the latter employs techniques for automatic prompt optimization. We present the basic concepts of prompting, review the advances for efficient prompting, and highl
    
[^34]: 以诚信推动人工智能发展：神经机器翻译中的伦理挑战与解决方案

    Advancing AI with Integrity: Ethical Challenges and Solutions in Neural Machine Translation

    [https://arxiv.org/abs/2404.01070](https://arxiv.org/abs/2404.01070)

    本文研究了神经机器翻译中的伦理挑战，并通过实证研究和文献综述确定和解决了数据处理、隐私、数据所有权等方面的伦理问题，为确保AI模型的公平性和文化敏感性提供了解决方案。

    

    本文探讨了人工智能在神经机器翻译（NMT）系统中的伦理挑战，强调开发者确保公平性和文化敏感性的必要性。我们调查了NMT中AI模型的伦理素养，审视了NMT开发的每个阶段的伦理考虑，包括数据处理、隐私、数据所有权和同意。我们通过实证研究确定并解决了伦理问题。这些问题包括利用Transformer模型进行卢干达语-英语翻译，以及利用句子迷你批处理提高效率。我们还进行了细化数据标记技术和对卢干达语和英语社交媒体内容进行BERT和Longformer模型的微调等补充研究。我们的第二种方法是来自Google Scholar等数据库和GitHub等平台的文献综述。此外，本文还探讨了AI系统与相关责任之间的分配。

    arXiv:2404.01070v1 Announce Type: cross  Abstract: This paper addresses the ethical challenges of Artificial Intelligence in Neural Machine Translation (NMT) systems, emphasizing the imperative for developers to ensure fairness and cultural sensitivity. We investigate the ethical competence of AI models in NMT, examining the Ethical considerations at each stage of NMT development, including data handling, privacy, data ownership, and consent. We identify and address ethical issues through empirical studies. These include employing Transformer models for Luganda-English translations and enhancing efficiency with sentence mini-batching. And complementary studies that refine data labeling techniques and fine-tune BERT and Longformer models for analyzing Luganda and English social media content. Our second approach is a literature review from databases such as Google Scholar and platforms like GitHub. Additionally, the paper probes the distribution of responsibility between AI systems and 
    
[^35]: 探索数学推理中数据对推理的影响之谜

    Exploring the Mystery of Influential Data for Mathematical Reasoning

    [https://arxiv.org/abs/2404.01067](https://arxiv.org/abs/2404.01067)

    提出了适用于数学推理的质量感知多样选择（QaDS）策略，并验证其在选择具有影响力的数据上的优越性；扩大了数据规模、用QaDS选择的通用数据进行训练对数学推理有帮助，最后定义了最佳混合OpenMathMix。

    

    选择对下游任务微调具有影响力的数据是性能和计算效率的关键因素。最近的研究表明，仅使用有限数据进行训练在通用任务上可以表现出卓越性能。然而，对于数学推理任务，这种可行性尚未得到验证。为此，针对数学推理存在两个开放问题：如何选择具有影响力的数据以及什么是具有影响力的数据组成。对于前者，我们提出了一个适用于数学推理的质量感知多样选择（QaDS）策略。与其他选择策略进行比较验证了QaDS的优越性。对于后者，我们首先扩大了我们的设置并探索了具有影响力的数据组成。我们进行了一系列实验并强调：扩大推理数据规模，并训练选用QaDS选择的通用数据是有益的。然后，我们将我们的最佳混合定义为OpenMathMix。

    arXiv:2404.01067v1 Announce Type: new  Abstract: Selecting influential data for fine-tuning on downstream tasks is a key factor for both performance and computation efficiency. Recent works have shown that training with only limited data can show a superior performance on general tasks. However, the feasibility on mathematical reasoning tasks has not been validated. To go further, there exist two open questions for mathematical reasoning: how to select influential data and what is an influential data composition. For the former one, we propose a Quality-aware Diverse Selection (QaDS) strategy adaptable for mathematical reasoning. A comparison with other selection strategies validates the superiority of QaDS. For the latter one, we first enlarge our setting and explore the influential data composition. We conduct a series of experiments and highlight: scaling up reasoning data, and training with general data selected by QaDS is helpful. Then, we define our optimal mixture as OpenMathMix
    
[^36]: 正则化的最佳-N采样以减轻语言模型对齐中的奖励欺骗问题

    Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment

    [https://arxiv.org/abs/2404.01054](https://arxiv.org/abs/2404.01054)

    提出了Regularized Best-of-N (RBoN)，通过引入接近性项来减轻奖励欺骗，提高了算法在解码时与人类偏好对齐的效果。

    

    Best-of-N (BoN)采样与奖励模型已被证明是一种有效的策略，用于在解码时将大型语言模型(LLMs)与人类偏好对齐。然而，BoN采样容易受到奖励欺骗问题的影响。为了防止奖励欺骗，我们提出了一种名为Regularized Best-of-N (RBoN)的变体，通过在响应选择中结合接近性项来减轻奖励欺骗，类似于偏好学习技术。

    arXiv:2404.01054v1 Announce Type: cross  Abstract: Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization (e.g., KL regularization), which ensures that the language model remains close to the reference model. In this research, we propose Regularized Best-of-N (RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating a proximity term in response selection, similar to preference learning techniques. We evaluate two variants of RBoN on the AlpacaFarm dataset and find that they outperform BoN, especially wh
    
[^37]: ARAGOG: 高级 RAG 输出评分

    ARAGOG: Advanced RAG Output Grading

    [https://arxiv.org/abs/2404.01037](https://arxiv.org/abs/2404.01037)

    本研究评估了不同的 RAG 方法对检索精度和答案相似性的影响，发现假设文档嵌入（HyDE）和LLM 重新排序显著提高了检索精度，而最大边际相关性（MMR）和 Cohere 重新排序则没有明显优势。

    

    arXiv:2404.01037v1 通知类型: 新 提要: 检索增强生成（RAG）对于将外部知识整合到大型语言模型（LLM）的输出中至关重要。尽管有关RAG的文献越来越多，但主要集中在对新的最先进技术与其前身进行系统性审查和比较，存在大量实验性比较研究的空白。本研究开始着手解决这一问题，评估了各种RAG方法对检索精度和答案相似性的影响。我们发现，假设文档嵌入（HyDE）和LLM 重新排序显著提高了检索精度。然而，最大边际相关性（MMR）和Cohere 重新排序并未表现出明显优势，多查询方法表现不佳。句窗检索在检索精度方面表现最为有效，尽管它在答案相似性上的表现不稳定。该研究确认了文档摘要的潜力。

    arXiv:2404.01037v1 Announce Type: new  Abstract: Retrieval-Augmented Generation (RAG) is essential for integrating external knowledge into Large Language Model (LLM) outputs. While the literature on RAG is growing, it primarily focuses on systematic reviews and comparisons of new state-of-the-art (SoTA) techniques against their predecessors, with a gap in extensive experimental comparisons. This study begins to address this gap by assessing various RAG methods' impacts on retrieval precision and answer similarity. We found that Hypothetical Document Embedding (HyDE) and LLM reranking significantly enhance retrieval precision. However, Maximal Marginal Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a baseline Naive RAG system, and Multi-query approaches underperformed. Sentence Window Retrieval emerged as the most effective for retrieval precision, despite its variable performance on answer similarity. The study confirms the potential of the Document Summary I
    
[^38]: 使用大规模自动隐喻识别验证关于隐喻的主张

    Verifying Claims About Metaphors with Large-Scale Automatic Metaphor Identification

    [https://arxiv.org/abs/2404.01029](https://arxiv.org/abs/2404.01029)

    通过大规模语料库分析验证了关于动词隐喻的主张，发现隐喻中动词的直接宾语往往具有较低的具体性、可形象化程度和熟悉度，隐喻更可能在情感和主观句子中使用。

    

    有一些关于词语更可能被用作隐喻的情况的语言主张，但很少有研究试图用大型语料库验证这些主张。本研究通过将隐喻检测应用于从Common Crawl中提取的句子进行大规模基于语料库的分析，验证了关于动词隐喻的某些现有主张，并使用结果获得的统计数据。验证结果表明，作为隐喻使用的动词的直接宾语往往具有较低的具体性、可形象化程度和熟悉度，并且隐喻更可能在情感和主观句子中使用。

    arXiv:2404.01029v1 Announce Type: new  Abstract: There are several linguistic claims about situations where words are more likely to be used as metaphors. However, few studies have sought to verify such claims with large corpora. This study entails a large-scale, corpus-based analysis of certain existing claims about verb metaphors, by applying metaphor detection to sentences extracted from Common Crawl and using the statistics obtained from the results. The verification results indicate that the direct objects of verbs used as metaphors tend to have lower degrees of concreteness, imageability, and familiarity, and that metaphors are more likely to be used in emotional and subjective sentences.
    
[^39]: 源感知训练使语言模型具备知识归因能力

    Source-Aware Training Enables Knowledge Attribution in Language Models

    [https://arxiv.org/abs/2404.01019](https://arxiv.org/abs/2404.01019)

    源感知训练使语言模型具备知识归因能力，进而增强了其透明度、可解释性和可验证性。

    

    大型语言模型（LLMs）在预训练期间学到了大量知识，但往往对此类知识的来源毫不在意。本文研究了内在源引用问题，要求LLMs引用支持生成响应的预训练来源。内在源引用可以增强LLMs的透明度、可解释性和可验证性。为赋予LLMs这种能力，我们探索了源感知训练——一个后预训练配方，包括（i）训练LLMs将唯一源文档标识符与每个文档中的知识关联起来，然后（ii）进行指示调整，教导LLMs在被提示时引用支持的预训练来源。源感知训练可以轻松应用于即插即用的预训练LLMs，并与现有的预训练/微调框架的差异最小。通过对精心策划的数据进行实验，我们展示了我们的训练配方可以实现

    arXiv:2404.01019v1 Announce Type: cross  Abstract: Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source(s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining source supporting a generated response. Intrinsic source citation can enhance LLM transparency, interpretability, and verifiability. To give LLMs such ability, we explore source-aware training -- a post pretraining recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning to teach the LLM to cite a supporting pretraining source when prompted. Source-aware training can easily be applied to pretrained LLMs off the shelf, and diverges minimally from existing pretraining/fine-tuning frameworks. Through experiments on carefully curated data, we demonstrate that our training recipe can en
    
[^40]: PairEval：使用两两比较进行开放域对话评估

    PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison

    [https://arxiv.org/abs/2404.01015](https://arxiv.org/abs/2404.01015)

    提出了PairEval，一种新颖的对话评估指标，通过将回复的质量与不同对话中的回复进行比较来评估，与人类判断具有更高的相关性。

    

    arXiv:2404.01015v1 公告类型：新 建立可靠且自动化的评估指标是开放域对话系统中必不可少但具有挑战性的问题。最近的研究提出了评估指标，通过考虑生成的回复与之前的对话历史的相关性来评估这些回复。尽管有效，但这些指标直接评估单个回复，而未考虑其相对质量与其他回复相比的情况。为了解决这个问题，我们提出了PairEval，一种新颖的对话评估指标，通过将回复的质量与不同对话中的回复进行比较来评估。PairEval建立在开源和中等规模的语言模型之上，并使其专门化于对话回复之间的两两比较。在多个基准测试上进行了大量实验证明，我们的指标与人类判断呈现出更高的相关性超过基线指标。我们还发现，所提出的比较性指标在

    arXiv:2404.01015v1 Announce Type: new  Abstract: Building a reliable and automated evaluation metric is a necessary but challenging problem for open-domain dialogue systems. Recent studies proposed evaluation metrics that assess generated responses by considering their relevance to previous dialogue histories. Although effective, these metrics evaluate individual responses directly rather than considering their relative quality compared to other responses. To handle this, we propose PairEval, a novel dialogue evaluation metric for assessing responses by comparing their quality against responses in different conversations. PairEval is built on top of open-sourced and moderate-size language models, and we make them specialized in pairwise comparison between dialogue responses. Extensive experiments on multiple benchmarks demonstrate that our metric exhibits a higher correlation with human judgments than baseline metrics. We also find that the proposed comparative metric is more robust in
    
[^41]: 使用大型语言模型生成的相关性判断来预测查询性能

    Query Performance Prediction using Relevance Judgments Generated by Large Language Models

    [https://arxiv.org/abs/2404.01012](https://arxiv.org/abs/2404.01012)

    提出了一种使用自动生成的相关性判断的查询性能预测框架，能够解决先前方法中对不同IR评估指标准确性和解释性的限制。

    

    查询性能预测（QPP）旨在估计搜索系统对查询的检索质量，而无需人工相关性判断。先前的QPP方法通常返回单个标量值，并不要求预测值接近特定的信息检索（IR）评估指标，从而导致以下某些缺点：（i）单个标量无法准确表示不同的IR评估指标，特别是当度量不高度相关时，（ii）单个标量限制了QPP方法的可解释性，因为仅使用标量无法解释QPP结果。为解决这些问题，我们提出了一个使用自动生成的相关性判断的QPP框架（QPP-GenRE），将QPP分解为独立的子任务，即对排名列表中每个项目对给定查询的相关性进行判断。这样我们可以使用生成的相关性判断来预测任何IR评估指标。

    arXiv:2404.01012v1 Announce Type: cross  Abstract: Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgment
    
[^42]: 构建和扩展印尼本地语言的低资源和代表性平行数据集

    Constructing and Expanding Low-Resource and Underrepresented Parallel Datasets for Indonesian Local Languages

    [https://arxiv.org/abs/2404.01009](https://arxiv.org/abs/2404.01009)

    介绍了Bhinneka Korpus，一个包含五种印尼本地语言的多语言平行语料库，旨在增强资源的访问和利用，扩展其在国内的影响力。

    

    在印度尼西亚，本地语言在文化中扮演着重要角色。然而，现有的印尼语言资源仍然属于自然语言处理领域的有限数据。当为这些语言构建NLP模型时，这就成为一个问题。为了填补这一空白，我们引入了Bhinneka Korpus，一个包含五种印尼本地语言的多语言平行语料库。我们的目标是增强这些资源的访问和利用，扩展它们在国内的影响力。我们详细说明了数据集收集过程及相关挑战。此外，由于数据限制，我们尝试使用IBM Model 1进行翻译任务实验。结果显示，每种语言的性能已经显示出进一步发展的良好迹象。我们讨论了诸如词汇变异、平滑效应和跨语言变异等挑战。我们打算评估该语料库...

    arXiv:2404.01009v1 Announce Type: new  Abstract: In Indonesia, local languages play an integral role in the culture. However, the available Indonesian language resources still fall into the category of limited data in the Natural Language Processing (NLP) field. This is become problematic when build NLP model for these languages. To address this gap, we introduce Bhinneka Korpus, a multilingual parallel corpus featuring five Indonesian local languages. Our goal is to enhance access and utilization of these resources, extending their reach within the country. We explained in a detail the dataset collection process and associated challenges. Additionally, we experimented with translation task using the IBM Model 1 due to data constraints. The result showed that the performance of each language already shows good indications for further development. Challenges such as lexical variation, smoothing effects, and cross-linguistic variability are discussed. We intend to evaluate the corpus usi
    
[^43]: 显式到隐式篇章关系识别失败的原因是什么？

    What Causes the Failure of Explicit to Implicit Discourse Relation Recognition?

    [https://arxiv.org/abs/2404.00999](https://arxiv.org/abs/2404.00999)

    显式示例（去除连接词）上训练的关系分类器在真实的隐式情景中表现不佳的原因之一是在去除连接词后标签发生变化，我们提供了在语料库水平上的实证证据并探讨了缓解标签变化的策略

    

    我们考虑了篇章处理领域中一个未解之谜：为什么在真实的隐式情景中，在显式示例（去除连接词）上训练的关系分类器表现不佳？以前的工作声称这是因为显式和隐式示例之间的语言差异，但未提供实证证据。在这项研究中，我们展示了这种失败的一个原因是在去除连接词后标签发生了变化。具体地，我们发现一些显式实例表达的篇章关系在连接词消失后会发生变化。与以前手动分析少量示例的工作不同，我们提供了在语料库水平上的实证证据来证明这种变化的存在。然后，我们通过考虑连接词在句法中的作用、连接词的歧义性等因素，分析标签变化发生的原因。最后，我们研究了两种缓解标签变化的策略：过滤掉噪声数据

    arXiv:2404.00999v1 Announce Type: new  Abstract: We consider an unanswered question in the discourse processing community: why do relation classifiers trained on explicit examples (with connectives removed) perform poorly in real implicit scenarios? Prior work claimed this is due to linguistic dissimilarity between explicit and implicit examples but provided no empirical evidence. In this study, we show that one cause for such failure is a label shift after connectives are eliminated. Specifically, we find that the discourse relations expressed by some explicit instances will change when connectives disappear. Unlike previous work manually analyzing a few examples, we present empirical evidence at the corpus level to prove the existence of such shift. Then, we analyze why label shift occurs by considering factors such as the syntactic role played by connectives, ambiguity of connectives, and more. Finally, we investigate two strategies to mitigate the label shift: filtering out noisy d
    
[^44]: LLM-RadJudge: 实现X射线报告生成的放射科医师级评估

    LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation

    [https://arxiv.org/abs/2404.00998](https://arxiv.org/abs/2404.00998)

    该研究提出了一个使用大型语言模型进行放射学报告评估的新框架，通过GPT-4实现了与放射科医师评估一致性接近的效果，同时利用知识蒸馏训练的较小模型也达到了类似的评估能力。

    

    在放射学AI的发展中，评估生成的放射学报告至关重要，但现有指标无法反映任务的临床要求。本研究提出了一种新的评估框架，使用大型语言模型（LLMs）来比较放射学报告以进行评估。我们比较了各种LLMs的性能，并证明，当使用GPT-4时，我们提出的指标实现了接近放射科医师评估一致性的表现。此外，为了降低成本并提高可访问性，使该方法实用化，我们利用LLM评估结果构建数据集，并进行知识蒸馏以训练一个较小的模型。蒸馏模型实现了与GPT-4相当的评估能力。我们的框架和蒸馏模型为放射学报告生成提供了一种可访问且高效的评估方法，促进了开发更具临床相关性模型。该模型将进一步开源。

    arXiv:2404.00998v1 Announce Type: cross  Abstract: Evaluating generated radiology reports is crucial for the development of radiology AI, but existing metrics fail to reflect the task's clinical requirements. This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment. We compare the performance of various LLMs and demonstrate that, when using GPT-4, our proposed metric achieves evaluation consistency close to that of radiologists. Furthermore, to reduce costs and improve accessibility, making this method practical, we construct a dataset using LLM evaluation results and perform knowledge distillation to train a smaller model. The distilled model achieves evaluation capabilities comparable to GPT-4. Our framework and distilled model offer an accessible and efficient evaluation method for radiology report generation, facilitating the development of more clinically relevant models. The model will be further open-sourced 
    
[^45]: 探索大型语言模型与法律系统的关系：简要调研

    Exploring the Nexus of Large Language Models and Legal Systems: A Short Survey

    [https://arxiv.org/abs/2404.00990](https://arxiv.org/abs/2404.00990)

    大型语言模型在法律领域展示出独特作用，带来好处和挑战，调研突出了其在法律文本理解和分析中的应用，以及面临的偏见、可解释性和伦理挑战，展示了针对不同法律系统的精细调整法律LLMs的最新进展。

    

    随着人工智能（AI）和大型语言模型（LLMs）的进步，自然语言处理领域发生了深刻变革，尤其是在法律领域内。LLMs的能力越来越多地展示出在法律领域中扮演独特角色，带来独特好处和各种挑战。本调研探讨了LLMs与法律系统之间的协同作用，比如它们在法律文本理解、案例检索和分析等任务中的应用。此外，本调研重点介绍了LLMs在法律领域面临的关键挑战，包括偏见、可解释性和伦理考虑，以及研究人员如何解决这些问题。调研展示了针对各种法律系统定制的最新进展的精细调整的法律LLMs，以及可用于各种语言LLMs精细调整的法律数据集。此外，它提出了未来研究方向。

    arXiv:2404.00990v1 Announce Type: new  Abstract: With the advancement of Artificial Intelligence (AI) and Large Language Models (LLMs), there is a profound transformation occurring in the realm of natural language processing tasks within the legal domain. The capabilities of LLMs are increasingly demonstrating unique roles in the legal sector, bringing both distinctive benefits and various challenges. This survey delves into the synergy between LLMs and the legal system, such as their applications in tasks like legal text comprehension, case retrieval, and analysis. Furthermore, this survey highlights key challenges faced by LLMs in the legal domain, including bias, interpretability, and ethical considerations, as well as how researchers are addressing these issues. The survey showcases the latest advancements in fine-tuned legal LLMs tailored for various legal systems, along with legal datasets available for fine-tuning LLMs in various languages. Additionally, it proposes directions f
    
[^46]: 基于先验约束的奖励模型训练以对齐大尺寸语言模型

    Prior Constraints-based Reward Model Training for Aligning Large Language Models

    [https://arxiv.org/abs/2404.00978](https://arxiv.org/abs/2404.00978)

    本文提出了基于先验约束的奖励模型训练方法，有效改善了对齐大型语言模型的性能。

    

    使用人类反馈的强化学习方法来对齐大型语言模型（LLMs）通常训练一个奖励模型，该模型使用比较对来计算排名损失。然而，训练过程存在一个固有问题：由于缺乏约束，奖励分数在强化学习过程中呈现不受控制的扩展。本文提出了一种基于先验约束的奖励模型（PCRM）训练方法来缓解这一问题。PCRM在奖励模型训练中融合了先验约束，具体来说是每个比较对输出之间的长度比和余弦相似性，以调节优化幅度并控制得分差距。我们通过检查PCRM与人类偏好的排名相关性以及通过RL对LLMs对齐的有效性来全面评估PCRM。实验结果表明，PCRM通过有效地约束奖励显著提升了对齐性能。

    arXiv:2404.00978v1 Announce Type: new  Abstract: Reinforcement learning with human feedback for aligning large language models (LLMs) trains a reward model typically using ranking loss with comparison pairs.However, the training procedure suffers from an inherent problem: the uncontrolled scaling of reward scores during reinforcement learning due to the lack of constraints while training the reward model.This paper proposes a Prior Constraints-based Reward Model (namely PCRM) training method to mitigate this problem. PCRM incorporates prior constraints, specifically, length ratio and cosine similarity between outputs of each comparison pair, during reward model training to regulate optimization magnitude and control score margins. We comprehensively evaluate PCRM by examining its rank correlation with human preferences and its effectiveness in aligning LLMs via RL. Experimental results demonstrate that PCRM significantly improves alignment performance by effectively constraining reward
    
[^47]: AISPACE在SemEval-2024任务8中的表现：一种用于检测多生成器机器生成文本的类平衡Soft-voting系统

    AISPACE at SemEval-2024 task 8: A Class-balanced Soft-voting System for Detecting Multi-generator Machine-generated Text

    [https://arxiv.org/abs/2404.00950](https://arxiv.org/abs/2404.00950)

    本文介绍了一种类平衡Soft-voting系统，旨在检测人类撰写或机器生成的文本，通过优化编码器模型表现，并采用加权交叉熵损失和软投票策略，在SemEval-2024任务8的Subtask B中取得了最佳表现。

    

    arXiv:2404.00950v1 公告类型：新 提要：SemEval-2024任务8提出了一个挑战，即检测人类撰写和机器生成的文本。针对不同的检测场景有3个子任务。本文提出的系统主要处理Subtask B，旨在检测给定的全文是由人类撰写还是由特定的大型语言模型（LLM）生成的，实际上是一个多类文本分类任务。我们的团队AISPACE对基于transformer的模型进行了系统研究，包括仅编码器、仅解码器和编码器-解码器模型。我们比较了它们在该任务上的性能，发现仅编码器模型表现异常出色。我们还应用了加权的交叉熵损失函数来解决不同类别样本数据不平衡的问题。此外，我们采用了软投票策略对多模型集成进行增强，以提高我们预测的可靠性。我们的系统在Subtask B中排名第一，创造了一个最先进的

    arXiv:2404.00950v1 Announce Type: new  Abstract: SemEval-2024 Task 8 provides a challenge to detect human-written and machine-generated text. There are 3 subtasks for different detection scenarios. This paper proposes a system that mainly deals with Subtask B. It aims to detect if given full text is written by human or is generated by a specific Large Language Model (LLM), which is actually a multi-class text classification task. Our team AISPACE conducted a systematic study of fine-tuning transformer-based models, including encoderonly, decoder-only and encoder-decoder models. We compared their performance on this task and identified that encoder-only models performed exceptionally well. We also applied a weighted Cross Entropy loss function to address the issue of data imbalance of different class samples. Additionally, we employed softvoting strategy over multi-models ensemble to enhance the reliability of our predictions. Our system ranked top 1 in Subtask B, which sets a state-of-
    
[^48]: Evalverse: 大型语言模型评估的统一和易用库

    Evalverse: Unified and Accessible Library for Large Language Model Evaluation

    [https://arxiv.org/abs/2404.00943](https://arxiv.org/abs/2404.00943)

    Evalverse是一个统一和易用的库，简化了大型语言模型（LLMs）的评估，为研究人员和从业者提供了集中且易于访问的评估框架。

    

    本文介绍了Evalverse，这是一个新颖的库，通过将不同的评估工具统一到一个用户友好的框架中，简化了对大型语言模型（LLMs）的评估。Evalverse使得对人工智能了解有限的个人可以轻松请求LLMs评估并收到详细报告，利用与Slack等通信平台的集成。因此，Evalverse作为LLMs的全面评估强大工具，为研究人员和从业者提供了集中且易于访问的评估框架。最后，我们还提供了Evalverse的演示视频，展示了它的功能和实现方式，以两分钟的格式展示。

    arXiv:2404.00943v1 Announce Type: cross  Abstract: This paper introduces Evalverse, a novel library that streamlines the evaluation of Large Language Models (LLMs) by unifying disparate evaluation tools into a single, user-friendly framework. Evalverse enables individuals with limited knowledge of artificial intelligence to easily request LLM evaluations and receive detailed reports, facilitated by an integration with communication platforms like Slack. Thus, Evalverse serves as a powerful tool for the comprehensive assessment of LLMs, offering both researchers and practitioners a centralized and easily accessible evaluation framework. Finally, we also provide a demo video for Evalverse, showcasing its capabilities and implementation in a two-minute format.
    
[^49]: 使用大规模知识图谱评估大型语言模型的事实性

    Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs

    [https://arxiv.org/abs/2404.00942](https://arxiv.org/abs/2404.00942)

    使用大型知识图谱创建评估模型以检查大型语言模型在事实性上的表现，有效降低评估成本。

    

    大型语言模型（LLMs）的出现显著改变了人工智能领域，增强了机器学习和人工智能的能力。事实性问题对LLMs来说是一个关键问题，因为它们可能生成事实不准确的响应。本文提出了GraphEval，通过大规模测试数据集对LLM的性能进行评估。具体而言，测试数据集是从拥有超过1000万个事实的大型知识图谱中检索而来，无需昂贵的人力成本。与基于生成响应评估LLMs的传统方法不同，GraphEval通过创建一个评估模型简化了评估过程，用于估计LLM给出的答案的正确性。我们的实验表明，评估模型的事实性评估与LLM生成的输出的正确性密切相关，同时显著降低了评估成本。此外，我们的发现为LLM的性能提供了宝贵的洞见。

    arXiv:2404.00942v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has significantly transformed the AI landscape, enhancing machine learning and AI capabilities. Factuality issue is a critical concern for LLMs, as they may generate factually incorrect responses. In this paper, we propose GraphEval to evaluate an LLM's performance using a substantially large test dataset. Specifically, the test dataset is retrieved from a large knowledge graph with more than 10 million facts without expensive human efforts. Unlike conventional methods that evaluate LLMs based on generated responses, GraphEval streamlines the evaluation process by creating a judge model to estimate the correctness of the answers given by the LLM. Our experiments demonstrate that the judge model's factuality assessment aligns closely with the correctness of the LLM's generated outputs, while also substantially reducing evaluation costs. Besides, our findings offer valuable insights into LLM per
    
[^50]: 大型语言模型如何促进更好的社会辅助人机交互：简要调研

    How Can Large Language Models Enable Better Socially Assistive Human-Robot Interaction: A Brief Survey

    [https://arxiv.org/abs/2404.00938](https://arxiv.org/abs/2404.00938)

    大型语言模型的最新进展为社会辅助机器人领域带来了潜在的新应用，能够显著扩展其能力，但也带来新的风险和道德关切。

    

    社会辅助机器人（SARs）在为老年人、患有自闭症谱系障碍（ASD）的儿童以及精神健康挑战者等特殊群体提供个性化认知情感支持方面取得了巨大成功。 SAR的大量研究作品展示了其在为在家提供支持方面的潜力，这种支持可以补充由专业心理健康专业人员提供的诊所治疗，使这些干预措施更加有效和可访问。然而，仍然存在一些关键技术挑战，阻碍了SAR介导的交互和干预达到人类水平的社会智能和功效。 随着大型语言模型（LLMs）的最新进展，SAR领域内的新应用潜力有所增加，可以显著扩展SAR的当前能力。 然而，整合LLM会引入新的风险和道德关切

    arXiv:2404.00938v1 Announce Type: cross  Abstract: Socially assistive robots (SARs) have shown great success in providing personalized cognitive-affective support for user populations with special needs such as older adults, children with autism spectrum disorder (ASD), and individuals with mental health challenges. The large body of work on SAR demonstrates its potential to provide at-home support that complements clinic-based interventions delivered by mental health professionals, making these interventions more effective and accessible. However, there are still several major technical challenges that hinder SAR-mediated interactions and interventions from reaching human-level social intelligence and efficacy. With the recent advances in large language models (LLMs), there is an increased potential for novel applications within the field of SAR that can significantly expand the current capabilities of SARs. However, incorporating LLMs introduces new risks and ethical concerns that ha
    
[^51]: ChatGLM-RLHF：将大型语言模型与人类反馈对齐的实践

    ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback

    [https://arxiv.org/abs/2404.00934](https://arxiv.org/abs/2404.00934)

    ChatGLM-RLHF是一种从人类反馈中强化学习系统，通过收集人类偏好数据、训练奖励模型和优化策略等方法来增强大型语言模型ChatGLM与人类偏好的对齐性，在实验中显示出显著的改进。

    

    arXiv:2404.00934v1 公告类型：新的 摘要：ChatGLM是由大型语言模型（LLMs）家族提供支持的免费人工智能服务。本文介绍了ChatGLM-RLHF流水线--一种从人类反馈中强化学习（RLHF）系统--旨在增强ChatGLM与人类偏好的对齐性。ChatGLM-RLHF包含三个主要组成部分：人类偏好数据的收集，奖励模型的训练，以及策略的优化。在将ChatGLM-RLHF整合到生产环境的过程中，我们遇到并解决了一些前所未有的挑战。我们引入了减少奖励方差以实现稳定的大规模训练的策略，实现了带有融合梯度下降的模型并行性，并设计了正则化约束以避免LLMs中的灾难性遗忘。实验表明，与ChatGLM的受监督微调（SFT）版本相比，ChatGLM-RLHF在对齐任务中带来了显著的改进。

    arXiv:2404.00934v1 Announce Type: new  Abstract: ChatGLM is a free-to-use AI service powered by the ChatGLM family of large language models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline -- a reinforcement learning from human feedback (RLHF) system -- designed to enhance ChatGLM's alignment with human preferences. ChatGLM-RLHF encompasses three major components: the collection of human preference data, the training of the reward model, and the optimization of policies. Throughout the process of integrating ChatGLM-RLHF into production, we encountered and addressed several unprecedented challenges. We introduce the strategies to mitigate reward variance for stabilized large-scale training, implement model parallelism with fused gradient-descent, and design regularization constraints to avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHF brings significant improvements in alignment tasks compared to the supervised fine-tuned (SFT) version of ChatGLM. Fo
    
[^52]: PSYDIAL：利用大型语言模型进行基于个性化的合成对话生成

    PSYDIAL: Personality-based Synthetic Dialogue Generation using Large Language Models

    [https://arxiv.org/abs/2404.00930](https://arxiv.org/abs/2404.00930)

    该研究提出了一种新颖的基于个性化的合成对话数据生成流水线，引入了PSYDIAL韩语对话数据集，针对大型语言模型生成更符合真实场景的人类化对话，实验证明相较于预训练或微调模型，使用PSYDIAL训练的模型在生成符合个性的回应上有显著提升，且该流水线具有非对话相关应用的潜力。

    

    我们提出了一种新颖的端到端基于个性化的合成对话数据生成流水线，专为通过提示从大型语言模型中引出回应而设计。我们设计提示以生成更符合真实世界场景的对话，当用户与聊天机器人互动时。我们介绍了PSYDIAL，这是第一个专注于基于个性化对话的韩语对话数据集，使用我们提出的流水线策划。值得注意的是，在我们的研究中，我们专注于大五人格模型的外向性维度。实验结果表明，虽然预训练模型和那些用聊天数据集进行微调的模型很难生成反映个性的回应，但通过PSYDIAL训练的模型显示出显著改进。我们的流水线的多功能性不仅局限于对话任务，还为其他非对话相关应用提供潜力。这项研究打开了更为微妙、个性驱动的交谈的大门.

    arXiv:2404.00930v1 Announce Type: new  Abstract: We present a novel end-to-end personality-based synthetic dialogue data generation pipeline, specifically designed to elicit responses from large language models via prompting. We design the prompts to generate more human-like dialogues considering real-world scenarios when users engage with chatbots. We introduce PSYDIAL, the first Korean dialogue dataset focused on personality-based dialogues, curated using our proposed pipeline. Notably, we focus on the Extraversion dimension of the Big Five personality model in our research. Experimental results indicate that while pre-trained models and those fine-tuned with a chit-chat dataset struggle to generate responses reflecting personality, models trained with PSYDIAL show significant improvements. The versatility of our pipeline extends beyond dialogue tasks, offering potential for other non-dialogue related applications. This research opens doors for more nuanced, personality-driven conver
    
[^53]: 多语言大型语言模型：语料库、对齐和偏见综述

    A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias

    [https://arxiv.org/abs/2404.00929](https://arxiv.org/abs/2404.00929)

    该论文对多语言大型语言模型进行了全面分析，深入讨论了关键问题，包括多语言语料库、对齐和偏见。

    

    基于大型语言模型（LLMs）的基础上，发展了多语言大型语言模型（MLLMs）来解决多语言自然语言处理任务的挑战，希望实现从高资源到低资源语言的知识转移。然而，仍然存在重要限制和挑战，比如语言不平衡、多语言对齐和固有偏见。本文旨在对MLLMs进行全面分析，深入讨论围绕这些关键问题的议题。

    arXiv:2404.00929v1 Announce Type: cross  Abstract: Based on the foundation of Large Language Models (LLMs), Multilingual Large Language Models (MLLMs) have been developed to address the challenges of multilingual natural language processing tasks, hoping to achieve knowledge transfer from high-resource to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolution, key techniques, and multilingual capacities. Secondly, we explore widely utilized multilingual corpora for MLLMs' training and multilingual datasets oriented for downstream tasks that are crucial for enhancing the cross-lingual capability of MLLMs. Thirdly, we survey the existing studies on multilingual representati
    
[^54]: LLMs 是优秀的手语翻译器

    LLMs are Good Sign Language Translators

    [https://arxiv.org/abs/2404.00925](https://arxiv.org/abs/2404.00925)

    本文提出了一种新颖的SignLLM框架，利用大型语言模型（LLMs）来处理手语翻译，通过对手语视频进行正则化和转换，实现了手语视频向类似语言的表示转换，以提高现成LLMs的可读性。

    

    Sign Language Translation (SLT)是一项具有挑战性的任务，旨在将手语视频翻译为口头语言。受训练于广泛多语言文本语料库的大型语言模型（LLMs）强大的翻译能力的启发，我们旨在利用现成的LLMs来处理SLT。本文对手语视频进行正则化，使其具有口头语言的语言特征，并提出了一种新颖的SignLLM框架，将手语视频转换为类似语言的表示，以便现成的LLMs更易读地处理。SignLLM包括两个关键模块：（1）矢量量化视觉手语模块将手语视频转换为一系列离散的字符级手语标记，（2）码本重建和对齐模块使用最优传输公式将这些字符级标记转换为单词级手语表示。手语文本对齐损失进一步弥合了手语和文本之间的差距。

    arXiv:2404.00925v1 Announce Type: cross  Abstract: Sign Language Translation (SLT) is a challenging task that aims to translate sign videos into spoken language. Inspired by the strong translation capabilities of large language models (LLMs) that are trained on extensive multilingual text corpora, we aim to harness off-the-shelf LLMs to handle SLT. In this paper, we regularize the sign videos to embody linguistic characteristics of spoken language, and propose a novel SignLLM framework to transform sign videos into a language-like representation for improved readability by off-the-shelf LLMs. SignLLM comprises two key modules: (1) The Vector-Quantized Visual Sign module converts sign videos into a sequence of discrete character-level sign tokens, and (2) the Codebook Reconstruction and Alignment module converts these character-level tokens into word-level sign representations using an optimal transport formulation. A sign-text alignment loss further bridges the gap between sign and tex
    
[^55]: 大型语言模型中的高效令牌利用学习

    Token-Efficient Leverage Learning in Large Language Models

    [https://arxiv.org/abs/2404.00914](https://arxiv.org/abs/2404.00914)

    介绍了一种名为Token-Efficient Leverage Learning（TELL）的方法，在大型语言模型中展示了其降低任务数据需求、提高任务性能的潜力，为低资源任务带来了竞争性能。

    

    大型语言模型（LLMs）在各种任务中表现出色，但在高资源场景中表现更佳，这在低资源场景中存在挑战。数据稀缺和LLMs适应特定任务固有的困难加剧了这一挑战。为了解决这两大难题，我们引入了\textbf{Leverage Learning}。我们提出了这一方法的简化实现，称为Token-Efficient Leverage Learning (TELL)。TELL展示了Leverage Learning的潜力，证明它在各种LLMs和低资源任务中的有效性，从$10^4$到$10^6$个令牌不等。与传统的监督微调（SFT）相比，它将任务数据需求降低了近一个数量级，同时提供竞争力的性能。在相同量的任务数据情况下，TELL在改善任务性能方面领先于SFT。我们讨论了Leverage Learning的机制，暗示其符合量化假设。

    arXiv:2404.00914v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have excelled in various tasks but perform better in high-resource scenarios, which presents challenges in low-resource scenarios. Data scarcity and the inherent difficulty of adapting LLMs to specific tasks compound the challenge. To address the twin hurdles, we introduce \textbf{Leverage Learning}. We present a streamlined implement of this methodology called Token-Efficient Leverage Learning (TELL). TELL showcases the potential of Leverage Learning, demonstrating effectiveness across various LLMs and low-resource tasks, ranging from $10^4$ to $10^6$ tokens. It reduces task data requirements by up to nearly an order of magnitude compared to conventional Supervised Fine-Tuning (SFT) while delivering competitive performance. With the same amount of task data, TELL leads in improving task performance compared to SFT. We discuss the mechanism of Leverage Learning, suggesting it aligns with quantization hypoth
    
[^56]: LLaMA-Excitor: 通过间接特征交互进行通用指令调整

    LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction

    [https://arxiv.org/abs/2404.00913](https://arxiv.org/abs/2404.00913)

    LLaMA-Excitor是一种轻量级方法，通过逐渐更多地关注有价值的信息来激发LLM更好地遵循指令，而不直接改变中间隐藏状态，有效保留预训练知识。

    

    现有的用于微调LLM（如Adapter、Prefix-tuning和LoRA）的方法引入了额外的模块或附加输入序列，以注入新技能或知识，但可能会损害LLM的固有能力。本文提出了LLaMA-Excitor，这是一种轻量级方法，通过逐渐更多地关注有价值的信息来激发LLM更好地遵循指令。具体来说，LLaMA-Excitor在transformer结构的自注意力计算过程中不直接改变中间隐藏状态。我们设计了Excitor块作为用于LLM自注意力中相似度计算的旁路模块，通过可学习的提示重新构建keys并改变values的重要性。LLaMA-Excitor确保自适应地将额外的注意力分配给输入指令，从而在低质量指令上微调LLM时有效地保留LLM的预训练知识。

    arXiv:2404.00913v1 Announce Type: cross  Abstract: Existing methods to fine-tune LLMs, like Adapter, Prefix-tuning, and LoRA, which introduce extra modules or additional input sequences to inject new skills or knowledge, may compromise the innate abilities of LLMs. In this paper, we propose LLaMA-Excitor, a lightweight method that stimulates the LLMs' potential to better follow instructions by gradually paying more attention to worthwhile information. Specifically, the LLaMA-Excitor does not directly change the intermediate hidden state during the self-attention calculation of the transformer structure. We designed the Excitor block as a bypass module for the similarity score computation in LLMs' self-attention to reconstruct keys and change the importance of values by learnable prompts. LLaMA-Excitor ensures a self-adaptive allocation of additional attention to input instructions, thus effectively preserving LLMs' pre-trained knowledge when fine-tuning LLMs on low-quality instruction-
    
[^57]: TM-TREK参加SemEval-2024任务8：基于LLM的人机混合文本自动边界检测

    TM-TREK at SemEval-2024 Task 8: Towards LLM-Based Automatic Boundary Detection for Human-Machine Mixed Text

    [https://arxiv.org/abs/2404.00899](https://arxiv.org/abs/2404.00899)

    本研究探讨了基于LLM的自动边界检测，在SemEval-2024比赛任务中取得了第一名，并深入研究了影响LLMs检测能力的因素。

    

    随着大型语言模型（LLMs）生成的文本日益普及，人们越来越关注区分LLM生成的文本和人工撰写的文本，以防止LLM的滥用，如误导性信息的传播和学术不诚实行为。过去的研究主要集中于将文本分类为完全由人类撰写或由LLM生成，忽略了同时包含两种内容类型的混合文本的检测。本文探讨了LLM识别人类撰写和机器生成混合文本中边界的能力。我们将这一任务转化为一个标记分类问题，并将标记转折点视为边界。值得注意的是，我们的LLM集成模型在SemEval'24竞赛任务8中的‘人机混合文本检测’子任务中获得第一名。此外，我们研究了影响LLMs在检测中的能力的因素。

    arXiv:2404.00899v1 Announce Type: new  Abstract: With the increasing prevalence of text generated by large language models (LLMs), there is a growing concern about distinguishing between LLM-generated and human-written texts in order to prevent the misuse of LLMs, such as the dissemination of misleading information and academic dishonesty. Previous research has primarily focused on classifying text as either entirely human-written or LLM-generated, neglecting the detection of mixed texts that contain both types of content. This paper explores LLMs' ability to identify boundaries in human-written and machine-generated mixed texts. We approach this task by transforming it into a token classification problem and regard the label turning point as the boundary. Notably, our ensemble model of LLMs achieved first place in the 'Human-Machine Mixed Text Detection' sub-task of the SemEval'24 Competition Task 8. Additionally, we investigate factors that influence the capability of LLMs in detecti
    
[^58]: 自我演示: 在大型语言模型中引出示范之外的泛化能力

    Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models

    [https://arxiv.org/abs/2404.00884](https://arxiv.org/abs/2404.00884)

    提出了自我演示(Self-Demos)方法，在大型语言模型中通过生成查询感知的示范，引出固有的泛化能力，并构建了用于评估该方法有效性的数据集OOD-Toolset。

    

    大型语言模型(LLMs)展示了在上下文学习(ICL)方面的良好能力，仅凭借少量示范就能迅速适应新任务。然而，当前的少样本方法严重依赖高质量、特定查询的示范，而这种示范通常缺乏。面对示范之外的查询，依赖手工制定示范或外部检索器的方法可能会失败。为了弥合有限示范和示范之外查询之间的差距，我们提出了自我演示(Self-Demos)，这是一种通过面向查询的示范生成来引出LLMs中固有的泛化能力的新型提示方法。生成的示范策略性地插值了现有示范和给定的查询，将查询从示范之外变为示范内。为了评估我们方法的有效性，我们人工构建了OOD-Toolset数据集，其中包含超过300个真实API和1000个实例，每个实例包括三个工具使用示例作为示范和一个示范之外查询。

    arXiv:2404.00884v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown promising abilities of in-context learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations. However, current few-shot methods heavily depend on high-quality, query-specific demos, which are often lacking. When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail. To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation. The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID. To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query.
    
[^59]: 基于QLoRA和Zip-tie嵌入的双语迁移学习Bailong

    Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding

    [https://arxiv.org/abs/2404.00862](https://arxiv.org/abs/2404.00862)

    通过结合参数高效微调和先进的嵌入初始化技术，本研究在以英语为主导的开源LLMs上实现了跨语言迁移学习。

    

    大型语言模型（LLMs）在各种自然语言处理应用中展现出卓越的性能。然而，现有大多数开源LLMs主要在英语数据上进行预训练，对其他语言的覆盖较少。多语言训练数据的不足导致在应用到资源较少的语言时性能亚优。此外，通过使用额外数据对低资源语言进行全参数微调以提高LLMs性能需要大量计算资源，这给研究机构和个人研究人员带来了计算障碍。因此，提出了一些技术，如参数高效微调和先进的嵌入初始化来解决这些挑战。本文将它们结合起来，以促进对以英语为主导的开源LLMs进行跨语言迁移。

    arXiv:2404.00862v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated exceptional performance in various NLP applications. However, the majority of existing open-source LLMs are pre-trained primarily on English data and little part of other languages. This deficiency in multilingual training data results in suboptimal performance when applied to languages with fewer available resources. Furthermore, enhancing the performance of LLMs on low-resource languages by full-parameter fine-tuning with additional data requires substantial computational resources, posing computational barriers for research organizations and individual researchers. Consequently, several techniques such as parameter-efficient tuning and advanced embedding initialization have been proposed to address these challenges. In this work, we combine them to facilitate cross-lingual transfer on English-dominated open-source LLM. To effectively enhance the model's proficiency in Traditional Chines
    
[^60]: 语言模型是否提前为未来标记进行规划？

    Do language models plan ahead for future tokens?

    [https://arxiv.org/abs/2404.00859](https://arxiv.org/abs/2404.00859)

    语言模型在推理过程中会提前准备未来标记所需的信息，可能是通过预缓存或面包屑的方式实现。

    

    arXiv:2404.00859v1 公告类型：跨领域 摘要：在给定位置的推理过程中，变压器是否会“提前思考”？已知变压器在$t$的前向传递的隐藏状态中准备信息，然后在未来的前向传递$t+\tau$中使用。我们提出了两种解释这种现象的可能性：预缓存，即训练中存在的非对角梯度项导致模型在$t$计算与当前推理任务无关但对未来有用的特征，以及面包屑，即与时间步长$t$最相关的特征已经与那些将最有利于时间步长$t+\tau$的特征相同。我们通过训练不将梯度传播到过去时间步的语言模型来测试这些假设，这种方案我们正式称为短视训练。在合成数据设置中，我们发现了预缓存的明确证据。在自回归语言建模设置中，我们的实验更多地支持了面包屑假设。

    arXiv:2404.00859v1 Announce Type: cross  Abstract: Do transformers "think ahead" during inference at a given position? It is known transformers prepare information in the hidden states of the forward pass at $t$ that is then used in future forward passes $t+\tau$. We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present in training result in the model computing features at $t$ irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step $t$ are already the same as those that would most benefit inference at time $t+\tau$. We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis.
    
[^61]: 返回到开始：生成具有相关终点的叙事

    Returning to the Start: Generating Narratives with Related Endpoints

    [https://arxiv.org/abs/2404.00829](https://arxiv.org/abs/2404.00829)

    提出了一种通过确保故事的第一句和最后一句相关性来生成叙事的方法，并探讨了不同叙事学方法如何影响故事的语言建模，实现了更好的故事叙事封闭性。

    

    人类作家经常在写作中以与开头句相关的结束句来作为结尾，以构成一个“闭环”的令人满意的叙事。受这一观察的启发，我们提出了RENarGen，一种可控的故事生成范式，通过确保第一句和最后一句相关，然后填充中间句来生成叙事。我们的贡献包括初步探讨了叙事学中各种结尾方法如何影响故事的语言建模。自动化和人工评估表明RENarGen生成的故事比当前的自回归模型具有更好的叙事封闭性。

    arXiv:2404.00829v1 Announce Type: new  Abstract: Human writers often bookend their writing with ending sentences that relate back to the beginning sentences in order to compose a satisfying narrative that "closes the loop." Motivated by this observation, we propose RENarGen, a controllable story-generation paradigm that generates narratives by ensuring the first and last sentences are related and then infilling the middle sentences. Our contributions include an initial exploration of how various methods of bookending from Narratology affect language modeling for stories. Automatic and human evaluations indicate RENarGen produces better stories with more narrative closure than current autoregressive models.
    
[^62]: 基于PID控制的自愈机制以提高大规模语言模型的鲁棒性

    PID Control-Based Self-Healing to Improve the Robustness of Large Language Models

    [https://arxiv.org/abs/2404.00828](https://arxiv.org/abs/2404.00828)

    本研究提出了一种基于PID控制的自愈机制，通过轨迹优化问题和神经网络内部状态的自动修正，来提高在在线推断中对输入数据扰动的容忍性。

    

    深度神经网络在许多自然语言处理应用中的有效性已被证明，但最近的研究发现暴露了这些语言模型在引入微小扰动时的脆弱性。虽然对人类来说语义无法区分，但这些扰动可以显著降低经过充分训练的语言模型的性能，引发了在安全关键环境中部署它们可靠性的担忧。在本研究中，我们构建了一个计算高效的自愈过程，用于在线推断时纠正输入数据被施加扰动时不良模型行为。这被形式化为一个轨迹优化问题，内部神经网络层的状态通过PID（比例-积分-微分）控制机制自动校正。P控制器针对即时状态调整，而I和D控制器考虑过去的状态。

    arXiv:2404.00828v1 Announce Type: new  Abstract: Despite the effectiveness of deep neural networks in numerous natural language processing applications, recent findings have exposed the vulnerability of these language models when minor perturbations are introduced. While appearing semantically indistinguishable to humans, these perturbations can significantly reduce the performance of well-trained language models, raising concerns about the reliability of deploying them in safe-critical situations. In this work, we construct a computationally efficient self-healing process to correct undesired model behavior during online inference when perturbations are applied to input data. This is formulated as a trajectory optimization problem in which the internal states of the neural network layers are automatically corrected using a PID (Proportional-Integral-Derivative) control mechanism. The P controller targets immediate state adjustments, while the I and D controllers consider past states a
    
[^63]: 利用大型语言模型从儿科患者病历中提取健康的社会决定因素：新颖的语料库和方法

    Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods

    [https://arxiv.org/abs/2404.00826](https://arxiv.org/abs/2404.00826)

    本研究利用大型语言模型Fine-tuned和上下文学习方法，提出了一个新颖的带注释语料库PedSHAC，并自动提取儿科患者病历中的详细社会健康决定因素。

    

    健康的社会决定因素(SDoH)在塑造健康结果中起着至关重要的作用，特别是在儿科人群中，干预措施可能具有长期影响。SDoH经常在电子健康记录(EHR)中进行研究，EHR为多样化的患者数据提供了丰富的库。在这项工作中，我们提出了一个新颖的带注释语料库，儿科社会史注释语料库(PedSHAC)，并评估使用大型语言模型(LLMs)的微调和上下文学习方法自动提取详细的SDoH表征。PedSHAC包括来自华盛顿大学(UW)医院系统内的1,260份临床记录中注释的社会史节。采用事件为基础的注释方案，PedSHAC捕捉了十个不同的健康决定因素，涵盖了生活和经济稳定性，以前的创伤，教育获取，物质使用史以及心理健康等内容。

    arXiv:2404.00826v1 Announce Type: new  Abstract: Social determinants of health (SDoH) play a critical role in shaping health outcomes, particularly in pediatric populations where interventions can have long-term implications. SDoH are frequently studied in the Electronic Health Record (EHR), which provides a rich repository for diverse patient data. In this work, we present a novel annotated corpus, the Pediatric Social History Annotation Corpus (PedSHAC), and evaluate the automatic extraction of detailed SDoH representations using fine-tuned and in-context learning methods with Large Language Models (LLMs). PedSHAC comprises annotated social history sections from 1,260 clinical notes obtained from pediatric patients within the University of Washington (UW) hospital system. Employing an event-based annotation scheme, PedSHAC captures ten distinct health determinants to encompass living and economic stability, prior trauma, education access, substance use history, and mental health with
    
[^64]: 无需排练的模块化和组合式持续学习对语言模型的应用

    Rehearsal-Free Modular and Compositional Continual Learning for Language Models

    [https://arxiv.org/abs/2404.00790](https://arxiv.org/abs/2404.00790)

    提出了一种无需排练的模块化和组合式持续学习框架，可以持续向语言模型添加新模块并将其与现有模块组合，实验证明该框架优于现有技术并有效推动知识转移。

    

    持续学习旨在在不遗忘现有知识的情况下，逐步获取新的知识。为了克服灾难性遗忘，方法要么基于排练，即存储来自先前任务的数据示例以进行数据重播，要么将参数隔离分配给每个任务。然而，基于排练的方法会引发隐私和内存问题，参数隔离的持续学习方法不考虑任务之间的相互作用，从而阻碍知识转移。在这项工作中，我们提出了MoCL，一个无需排练的模块化和组合式持续学习框架，该框架不断向语言模型添加新的模块，并将其与现有模块组合在一起。在各种基准测试中的实验表明，MoCL优于现有技术，并有效促进了知识转移。

    arXiv:2404.00790v1 Announce Type: cross  Abstract: Continual learning aims at incrementally acquiring new knowledge while not forgetting existing knowledge. To overcome catastrophic forgetting, methods are either rehearsal-based, i.e., store data examples from previous tasks for data replay, or isolate parameters dedicated to each task. However, rehearsal-based methods raise privacy and memory issues, and parameter-isolation continual learning does not consider interaction between tasks, thus hindering knowledge transfer. In this work, we propose MoCL, a rehearsal-free Modular and Compositional Continual Learning framework which continually adds new modules to language models and composes them with existing modules. Experiments on various benchmarks show that MoCL outperforms state of the art and effectively facilitates knowledge transfer.
    
[^65]: 从鲁棒性到改进的泛化和校准：预训练语言模型中的研究

    From Robustness to Improved Generalization and Calibration in Pre-trained Language Models

    [https://arxiv.org/abs/2404.00758](https://arxiv.org/abs/2404.00758)

    通过引入JacHess方法，在预训练语言模型中实现了在领域内泛化和校准方面的显著改进

    

    增强预训练语言模型（PLMs）中的泛化能力和不确定性量化对于其有效性和可靠性至关重要。在机器学习研究奠定了鲁棒性对于改善泛化的重要性基础上，我们调查了通过雅可比和黑塞正则化实现的表征平滑度在提高PLM性能方面的作用。尽管此类正则化方法在计算机视觉中证明有效，但在自然语言处理（NLP）中的应用，其中PLM输入源自离散域，提出了独特挑战。我们引入了一种新颖的两阶段正则化方法JacHess，该方法相对于PLM中间表示的输入最小化雅可比和黑塞矩阵的范数。我们的评估使用GLUE基准表明，JacHess显著改善了PLMs的领域内泛化和校准，表现优越。

    arXiv:2404.00758v1 Announce Type: new  Abstract: Enhancing generalization and uncertainty quantification in pre-trained language models (PLMs) is crucial for their effectiveness and reliability. Building on machine learning research that established the importance of robustness for improving generalization, we investigate the role of representation smoothness, achieved via Jacobian and Hessian regularization, in enhancing PLM performance. Although such regularization methods have proven effective in computer vision, their application in natural language processing (NLP), where PLM inputs are derived from a discrete domain, poses unique challenges. We introduce a novel two-phase regularization approach, JacHess, which minimizes the norms of the Jacobian and Hessian matrices within PLM intermediate representations relative to their inputs. Our evaluation using the GLUE benchmark demonstrates that JacHess significantly improves in-domain generalization and calibration in PLMs, outperformi
    
[^66]: 关于最小贝叶斯风险解码的真实分布近似

    On the True Distribution Approximation of Minimum Bayes-Risk Decoding

    [https://arxiv.org/abs/2404.00752](https://arxiv.org/abs/2404.00752)

    本研究提出使用异常检测来衡量最小贝叶斯风险解码中样本接近真实分布的程度，并验证实验结果首次支持了性能与采样近似度之间的联系。

    

    最小贝叶斯风险（MBR）解码最近在文本生成中重新引起关注。MBR解码将从模型中采样的文本视为伪参考，并选择与其他文本最相似的文本。因此，采样是MBR解码的关键元素之一，先前的研究报告称性能因采样方法而异。从理论上讲，这种性能变化很可能与样本如何接近参考真实分布有关。然而，这种近似尚未深入研究。在本研究中，我们提出使用异常检测来衡量逼近程度。我们首先仔细研究性能的变化，然后展示以往关于样本的假设与变化的相关性不佳，但我们引入的异常分数则相关。这些结果是首次凭经验证明了性能和采样近似度之间的联系。

    arXiv:2404.00752v1 Announce Type: cross  Abstract: Minimum Bayes-risk (MBR) decoding has recently gained renewed attention in text generation. MBR decoding considers texts sampled from a model as pseudo-references and selects the text with the highest similarity to the others. Therefore, sampling is one of the key elements of MBR decoding, and previous studies reported that the performance varies by sampling methods. From a theoretical standpoint, this performance variation is likely tied to how closely the samples approximate the true distribution of references. However, this approximation has not been the subject of in-depth study. In this study, we propose using anomaly detection to measure the degree of approximation. We first closely examine the performance variation and then show that previous hypotheses about samples do not correlate well with the variation, but our introduced anomaly scores do. The results are the first to empirically support the link between the performance an
    
[^67]: 大语言模型能识别令人信服的论点吗？

    Can Language Models Recognize Convincing Arguments?

    [https://arxiv.org/abs/2404.00750](https://arxiv.org/abs/2404.00750)

    大语言模型不仅能够在识别和区分强势和弱势论点方面表现良好，还可以根据用户的信念和人口特征预测其立场，并确定论点对个人的吸引力。

    

    大型语言模型（LLMs）的显著且不断增强的能力引发了人们对它们可能被滥用用来创造个性化、令人信服的虚假信息和宣传的担忧。为了深入了解LLMs的说服能力，而又不直接与人类进行实验，我们提出研究它们在检测令人信服的论点任务上的表现。我们通过添加辩论、投票和用户特征来扩展了Durmus和Cardie（2018）的数据集，并提出了衡量LLMs能力的任务，包括（1）区分强势和弱势论点，（2）基于信念和人口特征预测立场，以及（3）根据个人特征确定对一个论点的吸引力。我们发现LLMs在这些任务中表现与人类不相上下，并且结合不同LLMs的预测可以获得显著的性能提升，甚至超过人类的表现。随文附带发布的数据和代码。

    arXiv:2404.00750v1 Announce Type: new  Abstract: The remarkable and ever-increasing capabilities of Large Language Models (LLMs) have raised concerns about their potential misuse for creating personalized, convincing misinformation and propaganda. To gain insights into LLMs' persuasive capabilities without directly engaging in experimentation with humans, we propose studying their performance on the related task of detecting convincing arguments. We extend a dataset by Durmus & Cardie (2018) with debates, votes, and user traits and propose tasks measuring LLMs' ability to (1) distinguish between strong and weak arguments, (2) predict stances based on beliefs and demographic characteristics, and (3) determine the appeal of an argument to an individual based on their traits. We show that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, even surpassing human performance. The data and code released with 
    
[^68]: 基准透明度：衡量数据对评估的影响

    Benchmark Transparency: Measuring the Impact of Data on Evaluation

    [https://arxiv.org/abs/2404.00748](https://arxiv.org/abs/2404.00748)

    本研究提出了一种自动化框架，可以衡量数据分布对自然语言处理模型性能和评估的影响，揭示了数据分布的重要性和对评估框架的影响。

    

    在本文中，我们介绍了一项关于量化数据分布对自然语言处理模型性能和评估影响的初步研究。我们提出了一个自动化框架，可以衡量数据点在6个不同维度上的分布：模糊度、困难度、可区分性、长度、噪声和困惑度。我们使用不均衡分层抽样来衡量数据分布对绝对（准确率/F1）和相对（排名）模型性能的影响。我们在两个不同的数据集（SQUAD和MNLI）上进行实验，测试了总共135种不同的模型（125种在SQUAD上，10种在MNLI上）。我们证明了在没有明确控制数据分布的情况下，标准评估框架是不一致和不可靠的。我们发现数据的影响在统计上是显著的，并且通常比更改度量的影响更大。

    arXiv:2404.00748v1 Announce Type: cross  Abstract: In this paper we present an exploratory research on quantifying the impact that data distribution has on the performance and evaluation of NLP models. We propose an automated framework that measures the data point distribution across 6 different dimensions: ambiguity, difficulty, discriminability, length, noise, and perplexity.   We use disproportional stratified sampling to measure how much the data distribution affects absolute (Acc/F1) and relative (Rank) model performance. We experiment on 2 different datasets (SQUAD and MNLI) and test a total of 135 different models (125 on SQUAD and 10 on MNLI). We demonstrate that without explicit control of the data distribution, standard evaluation frameworks are inconsistent and unreliable. We find that the impact of the data is statistically significant and is often larger than the impact of changing the metric.   In a second set of experiments, we demonstrate that the impact of data on eval
    
[^69]: Opera Graeca Adnotata: 为古希腊建立了一个超过3400万词的多层语料库

    Opera Graeca Adnotata: Building a 34M+ Token Multilayer Corpus for Ancient Greek

    [https://arxiv.org/abs/2404.00739](https://arxiv.org/abs/2404.00739)

    Opera Graeca Adnotata（OGA）是古希腊最大的开放获取多层语料库，包括来自PerseusDL和OpenGreekAndLatin GitHub仓库的1,687部文学作品和超过3400万个词元，丰富标注了七个注释层。

    

    本文介绍了Opera Graeca Adnotata（OGA）的beta版本0.1.0，这是古希腊最大的开放获取多层语料库。OGA包括来自PerseusDL和OpenGreekAndLatin GitHub仓库的1,687部文学作品和超过3400万个词元，这些仓库包含从公元前约800年到公元约250年的古希腊文本。文本被丰富地标注了七个注释层：(i) 分词层；(ii) 句子分割层；(iii) 词形标注层；(iv) 语法形态层；(v) 依存关系层；(vi) 依存关系函数层；(vii) 规范文本服务（CTS）引文层。文章描述了每个层的创建过程，突出了遇到的主要技术和注释相关问题。分词、句子分割和CTS引文是由基于规则的算法执行的，而形态句法标注是Ancient Greek Dependency Tree的数据上训练的COMBO解析器的输出。

    arXiv:2404.00739v1 Announce Type: new  Abstract: In this article, the beta version 0.1.0 of Opera Graeca Adnotata (OGA), the largest open-access multilayer corpus for Ancient Greek (AG) is presented. OGA consists of 1,687 literary works and 34M+ tokens coming from the PerseusDL and OpenGreekAndLatin GitHub repositories, which host AG texts ranging from about 800 BCE to about 250 CE. The texts have been enriched with seven annotation layers: (i) tokenization layer; (ii) sentence segmentation layer; (iii) lemmatization layer; (iv) morphological layer; (v) dependency layer; (vi) dependency function layer; (vii) Canonical Text Services (CTS) citation layer. The creation of each layer is described by highlighting the main technical and annotation-related issues encountered. Tokenization, sentence segmentation, and CTS citation are performed by rule-based algorithms, while morphosyntactic annotation is the output of the COMBO parser trained on the data of the Ancient Greek Dependency Treeban
    
[^70]: 核指代解析模型的受控重新评估

    A Controlled Reevaluation of Coreference Resolution Models

    [https://arxiv.org/abs/2404.00727](https://arxiv.org/abs/2404.00727)

    基于对预训练语言模型大小的控制，我们发现基于编码器的核指代解析模型在准确性和推理速度方面优于更近期的基于解码器的模型，而在基于编码器的模型中，最老的模型在跨域文本体裁中表现最佳。

    

    所有最先进的核指代解析（CR）模型都涉及微调预训练语言模型。一个CR模型优于另一个的出色性能是由语言模型选择还是其他因素（如特定于任务的架构）造成的，由于缺乏标准化的实验设置，这是很难或不可能确定的。为了解决这种模糊性，我们系统评估了五个CR模型，并控制了一些设计决策，包括每个模型使用的预训练语言模型。当控制语言模型大小时，基于编码器的CR模型在准确性和推理速度方面优于更近期的基于解码器的模型。令人惊讶的是，在基于编码器的CR模型中，较近期的模型并不总是更准确，而我们测试的最老的CR模型在跨域文本体裁中表现最佳。我们得出结论，控制语言模型的选择可以减少大部分，但并非全部，

    arXiv:2404.00727v1 Announce Type: new  Abstract: All state-of-the-art coreference resolution (CR) models involve finetuning a pretrained language model. Whether the superior performance of one CR model over another is due to the choice of language model or other factors, such as the task-specific architecture, is difficult or impossible to determine due to lack of a standardized experimental setup. To resolve this ambiguity, we systematically evaluate five CR models and control for certain design decisions including the pretrained language model used by each. When controlling for language model size, encoder-based CR models outperform more recent decoder-based models in terms of both accuracy and inference speed. Surprisingly, among encoder-based CR models, more recent models are not always more accurate, and the oldest CR model that we test generalizes the best to out-of-domain textual genres. We conclude that controlling for the choice of language model reduces most, but not all, of 
    
[^71]: 越大越好吗？通过预算重新分配改进LLM代码生成

    The Larger the Better? Improved LLM Code-Generation via Budget Reallocation

    [https://arxiv.org/abs/2404.00725](https://arxiv.org/abs/2404.00725)

    较小的语言模型可以在相同预算下产生可靠的改进，但在无法进行单元测试的情况下，较小的模型选择排名次于较大模型的单个输出。

    

    人们普遍认为，大型语言模型(LLMs)比较小的模型更好。然而，更大的模型在推断过程中也需要更多的时间和计算资源。这就引出了一个问题：当两个模型在相同的预算下运行时会发生什么？（例如，计算资源，运行时间）。为了解决这个问题，我们分析了各种大小的代码生成LLMs，并进行比较，例如运行一个70B模型一次与从13B模型生成五个输出并选择一个的情况。我们的研究结果表明，在标准单元测试设置中，反复使用较小的模型可以产生一致的改进，在五个任务中最高可达15%的增益。另一方面，在无法进行单元测试的情况下，从较小模型中基于排名的候选选择表现不及来自较大模型的单个输出。我们的结果突显了使用较小模型而非较大模型的潜力。

    arXiv:2404.00725v1 Announce Type: cross  Abstract: It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the imp
    
[^72]: LLM受到多少污染？一项全面调查和LLMSanitize库

    How Much are LLMs Contaminated? A Comprehensive Survey and the LLMSanitize Library

    [https://arxiv.org/abs/2404.00699](https://arxiv.org/abs/2404.00699)

    LLM受到污染可能导致其性能不可靠，挑战了自然语言处理领域的整体进展。

    

    随着近年来大型语言模型（LLMs）的崛起，新的机会正在出现，但也带来了新的挑战，污染问题迅速变得至关重要。企业应用和人工智能筹款已经达到一定规模，流行的问答基准提高几个百分点可能意味着数百万美元，对模型的完整性施加了巨大压力。同时，追踪LLMs见过的数据变得越来越困难；对于像GPT-4和Claude-3这样的闭源模型，他们不透露任何有关训练集的信息。因此，污染成为一个关键问题：LLMs的性能可能不再可靠，因为其高性能至少部分归因于其先前接触到的数据。这种局限性危及了自然语言处理领域的整体进展，然而，如何有效解决这一问题仍然缺乏方法。

    arXiv:2404.00699v1 Announce Type: new  Abstract: With the rise of Large Language Models (LLMs) in recent years, new opportunities are emerging, but also new challenges, and contamination is quickly becoming critical. Business applications and fundraising in AI have reached a scale at which a few percentage points gained on popular question-answering benchmarks could translate into dozens of millions of dollars, placing high pressure on model integrity. At the same time, it is becoming harder and harder to keep track of the data that LLMs have seen; if not impossible with closed-source models like GPT-4 and Claude-3 not divulging any information on the training set. As a result, contamination becomes a critical issue: LLMs' performance may not be reliable anymore, as the high performance may be at least partly due to their previous exposure to the data. This limitation jeopardizes the entire progress in the field of NLP, yet, there remains a lack of methods on how to efficiently address
    
[^73]: 语音语言模型的尺度特性

    Scaling Properties of Speech Language Models

    [https://arxiv.org/abs/2404.00685](https://arxiv.org/abs/2404.00685)

    通过研究语音语言模型的尺度特性，可以预测其语言性能的扩展速度，与文本-based大型语言模型相比，语音语言模型的语言性能扩展速度慢三个数量级。

    

    语音语言模型（SLM）旨在从原始音频中学习语言，而无需文本资源。尽管取得了显著进展，我们当前的模型表现出弱的句法和语义能力。然而，如果神经语言模型的尺度特性对语音模态成立，这些能力将随着训练所使用的计算量增加而提高。本文利用模型的尺度行为来估计我们当前方法将产生具有文本-based大型语言模型（LLMs）英语熟练度的SLM的尺度。

    arXiv:2404.00685v1 Announce Type: cross  Abstract: Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost
    
[^74]: CoUDA: 通过统一数据增强的相干性评估

    CoUDA: Coherence Evaluation via Unified Data Augmentation

    [https://arxiv.org/abs/2404.00681](https://arxiv.org/abs/2404.00681)

    通过CoUDA框架，将话语连贯性分解为全局和局部两个方面，并提出用于局部连贯性的新颖生成策略。

    

    相干性评估旨在评估话语的组织和结构，在大语言模型时代仍然具有挑战性。由于标注数据的稀缺，数据增强通常用于训练相干性评估模型。然而，先前针对此任务的增强主要依赖于启发式规则，缺乏设计准则作为指导。在本文中，我们从话语结构的语言学理论中汲取灵感，提出了一个名为CoUDA的数据增强框架。CoUDA将话语连贯性分解为全局和局部两个方面，并分别为两个方面设计增强策略。特别是针对局部连贯性，我们提出了一种新颖的生成策略用于构建增强样本，其中涉及对生成模型进行事后预训练并应用两个控制机制来控制生成样本的难度。在推理过程中，CoUDA还联合eva

    arXiv:2404.00681v1 Announce Type: new  Abstract: Coherence evaluation aims to assess the organization and structure of a discourse, which remains challenging even in the era of large language models. Due to the scarcity of annotated data, data augmentation is commonly used for training coherence evaluation models. However, previous augmentations for this task primarily rely on heuristic rules, lacking designing criteria as guidance. In this paper, we take inspiration from linguistic theory of discourse structure, and propose a data augmentation framework named CoUDA. CoUDA breaks down discourse coherence into global and local aspects, and designs augmentation strategies for both aspects, respectively. Especially for local coherence, we propose a novel generative strategy for constructing augmentation samples, which involves post-pretraining a generative model and applying two controlling mechanisms to control the difficulty of generated samples. During inference, CoUDA also jointly eva
    
[^75]: 通过令牌扩展实现Transformer的一般高效训练

    A General and Efficient Training for Transformer via Token Expansion

    [https://arxiv.org/abs/2404.00672](https://arxiv.org/abs/2404.00672)

    本文提出了一种名为ToE的令牌生长方案，旨在通过加速一致性训练来改善ViT的训练效果。

    

    Vision Transformer（ViT）通常需要极大的训练成本才能取得显著性能。本文提出一种新的令牌生长方案Token Expansion (ToE)，以实现ViT的一致性训练加速。我们引入了一个“初始化-扩展-合并”管道，以保持原始Transformer的中间特征分布的完整性，防止在训练过程中丢失关键的可学习信息。

    arXiv:2404.00672v1 Announce Type: cross  Abstract: The remarkable performance of Vision Transformers (ViTs) typically requires an extremely large training cost. Existing methods have attempted to accelerate the training of ViTs, yet typically disregard method universality with accuracy dropping. Meanwhile, they break the training consistency of the original transformers, including the consistency of hyper-parameters, architecture, and strategy, which prevents them from being widely applied to different Transformer networks. In this paper, we propose a novel token growth scheme Token Expansion (termed ToE) to achieve consistent training acceleration for ViTs. We introduce an "initialization-expansion-merging" pipeline to maintain the integrity of the intermediate feature distribution of original transformers, preventing the loss of crucial learnable information in the training process. ToE can not only be seamlessly integrated into the training and fine-tuning process of transformers (e
    
[^76]: 关于为技术文档构建RAG系统的观察

    Observations on Building RAG Systems for Technical Documents

    [https://arxiv.org/abs/2404.00657](https://arxiv.org/abs/2404.00657)

    研究回顾了对技术文档RAG系统构建的重要因素，并通过实验证明了最佳实践和潜在挑战

    

    RAG（检索增强生成）用于技术文档时存在挑战，因为嵌入通常无法捕捉领域信息。我们回顾了影响RAG的重要因素的先前研究，并进行实验以突出构建技术文档RAG系统的最佳实践和潜在挑战。

    arXiv:2404.00657v1 Announce Type: cross  Abstract: Retrieval augmented generation (RAG) for technical documents creates challenges as embeddings do not often capture domain information. We review prior art for important factors affecting RAG and perform experiments to highlight best practices and potential challenges to build RAG systems for technical documents.
    
[^77]: WavLLM：面向稳健和自适应语音大语言模型

    WavLLM: Towards Robust and Adaptive Speech Large Language Model

    [https://arxiv.org/abs/2404.00656](https://arxiv.org/abs/2404.00656)

    WavLLM是一个稳健和自适应语音大语言模型，引入了双编码器和Prompt-aware LoRA权重适配器，通过两阶段课程学习方法优化，解耦不同类型的语音信息，为处理语义内容和说话者身份的独特特征提供了新思路

    

    近年来，大型语言模型(LLMs)的最新进展彻底改变了自然语言处理领域，逐渐拓宽了它们的范围到多模态感知和生成。然而，有效地将听觉能力整合到LLMs中会带来显著挑战，特别是在泛化跨不同语境和执行复杂听觉任务方面。在这项工作中，我们引入了WavLLM，一个具有双编码器和Prompt-aware LoRA权重适配器的稳健和自适应语音大语言模型，通过两阶段课程学习方法进行优化。利用双编码器，我们解耦不同类型的语音信息，利用Whisper编码器处理语音的语义内容，利用WavLM编码器捕捉说话者身份的独特特征。在课程学习框架内，WavLLM首先通过混合要素进行优化来建立其基础能力

    arXiv:2404.00656v1 Announce Type: cross  Abstract: The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elemen
    
[^78]: 针对阿喀琉斯之踵：生成模型红队测试的调查

    Against The Achilles' Heel: A Survey on Red Teaming for Generative Models

    [https://arxiv.org/abs/2404.00629](https://arxiv.org/abs/2404.00629)

    通过对生成模型的红队测试进行了广泛调查，引入了基于语言模型能力的细粒度攻击策略分类体系，并开发了一个统一各种自动红队测试方法的搜索框架。

    

    生成模型正迅速普及并被整合到日常应用中，但相关的安全问题引起了人们的担忧，因为各种漏洞不断暴露。面对这一问题，红队测试领域正在快速增长，强调了对整个流程进行全面组织并解决社区新兴主题的需求。我们的广泛调查涵盖了120多篇论文，引入了一个基于语言模型固有能力的细粒度攻击策略分类体系。此外，我们开发了一个统一各种自动红队测试方法的搜索框架。此外，我们的调查涵盖了新领域，包括多模式攻击和防御、多语言模型风险、无害查询的过度使用以及下游应用的安全性。

    arXiv:2404.00629v1 Announce Type: new  Abstract: Generative models are rapidly gaining popularity and being integrated into everyday applications, raising concerns over their safety issues as various vulnerabilities are exposed. Faced with the problem, the field of red teaming is experiencing fast-paced growth, which highlights the need for a comprehensive organization covering the entire pipeline and addressing emerging topics for the community. Our extensive survey, which examines over 120 papers, introduces a taxonomy of fine-grained attack strategies grounded in the inherent capabilities of language models. Additionally, we have developed the searcher framework that unifies various automatic red teaming approaches. Moreover, our survey covers novel areas including multimodal attacks and defenses, risks around multilingual models, overkill of harmless queries, and safety of downstream applications. We hope this survey can provide a systematic perspective on the field and unlock new 
    
[^79]: 汇报眼动数据质量：走向新的标准

    Reporting Eye-Tracking Data Quality: Towards a New Standard

    [https://arxiv.org/abs/2404.00620](https://arxiv.org/abs/2404.00620)

    该研究提出了一种新的眼动数据共享方法，主张公布眼动数据的所有预处理阶段以及附带数据质量报告，从而增加现有眼动数据集的可重复使用性和实现跨数据集比较。

    

    Eye-tracking datasets通常以它们的创建者用于原始分析的格式共享，通常会导致排除被认为与主要目的无关的数据。为了增加现有眼动数据集的可重复使用性，以满足更多样化和最初不被考虑的用例，本文主张一种新的眼动数据共享方法。与其发布经过滤和预处理的数据集，应该公布所有预处理阶段的眼动数据，并附带数据质量报告。为了透明地报告数据质量并实现跨数据集比较，我们开发了数据质量报告标准和度量标准，可以自动应用于数据集，并将其集成到开源Python软件包pymovements中（https://github.com/aeye-lab/pymovements）。

    arXiv:2404.00620v1 Announce Type: new  Abstract: Eye-tracking datasets are often shared in the format used by their creators for their original analyses, usually resulting in the exclusion of data considered irrelevant to the primary purpose. In order to increase re-usability of existing eye-tracking datasets for more diverse and initially not considered use cases, this work advocates a new approach of sharing eye-tracking data. Instead of publishing filtered and pre-processed datasets, the eye-tracking data at all pre-processing stages should be published together with data quality reports. In order to transparently report data quality and enable cross-dataset comparisons, we develop data quality reporting standards and metrics that can be automatically applied to a dataset, and integrate them into the open-source Python package pymovements (https://github.com/aeye-lab/pymovements).
    
[^80]: 从未标记数据中学习语言建模规划

    Learning to Plan for Language Modeling from Unlabeled Data

    [https://arxiv.org/abs/2404.00614](https://arxiv.org/abs/2404.00614)

    通过自监督学习目标训练一个用于规划未来写作过程的模块，扩展了成功的语言模型公式到更抽象的规划中，改善了语言建模的性能，特别是在文本结构方面，同时新的规划模块可以大规模训练并轻松与社区共享。

    

    通过训练来预测未标记语料库中的下一个标记，大型语言模型学会执行许多任务，而无需任何标记数据。然而，它们的下一个标记预测目标可以说限制了它们在需要规划的场景中的性能，比如写作一篇连贯的文章。在这篇论文中，我们通过自监督学习目标训练一个用于规划未来写作过程的模块。通过根据生成的潜在计划进行条件化，我们的模型以无监督的方式将成功的语言模型公式扩展到更抽象的规划中。实验上，我们证明了我们的方法在一般情况下改善了语言建模的性能，特别是在文本结构方面。由于我们的框架使用的是无监督且外部于语言模型的规划模块，因此新的规划模块可以大规模训练，并且能够轻松地与社区共享。

    arXiv:2404.00614v1 Announce Type: cross  Abstract: By training to predict the next token in an unlabeled corpus, large language models learn to perform many tasks without any labeled data. However, their next-token-prediction objective arguably limits their performance in scenarios that require planning, such as writing a coherent article. In this paper, we train a module for planning the future writing process via a self-supervised learning objective. By conditioning on generated latent plans, our model extends the successful language model formula to more abstract planning in an unsupervised way. Empirically, we demonstrate that our method improves language modeling performance in general, particularly with respect to the text structure. Because our framework uses a planner module that is unsupervised and external to the language model, new planner modules can be trained at large scale and easily be shared with the community.
    
[^81]: RQ-RAG: 学习为检索增强生成细化查询

    RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation

    [https://arxiv.org/abs/2404.00610](https://arxiv.org/abs/2404.00610)

    本论文提出了RQ-RAG，旨在为检索增强生成模型增加细化查询的能力，以便针对模糊或复杂查询进一步澄清或分解，从而提高生成准确度。

    

    大型语言模型(LLMs)展示出卓越的能力，但容易生成不准确或幻觉性的响应。Retrieval-Augmented Generation (RAG)通过将外部相关文档纳入响应生成过程中，从而利用非参数化知识和LLMs的上下文学习能力，来解决这些挑战。然而，现有的RAG实现主要关注于上下文检索的初始输入，忽视了需要进一步澄清或分解以获得准确响应的模糊或复杂查询的细微差别。因此，本文提出了学习为检索增强生成细化查询(RQ-RAG)，致力于通过赋予模型显式重写、分解能力来增强模型。

    arXiv:2404.00610v1 Announce Type: new  Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, 
    
[^82]: 广泛的自对比使得无需反馈的语言模型对齐成为可能

    Extensive Self-Contrast Enables Feedback-Free Language Model Alignment

    [https://arxiv.org/abs/2404.00604](https://arxiv.org/abs/2404.00604)

    本论文介绍了一种利用广泛自对比生成负例的无需反馈的大型语言模型对齐方法，该方法在实验中表现优于直接偏好优化方法。

    

    人类反馈的强化学习（RLHF）一直是最近大型语言模型（LLM）对齐的核心技术。然而，其严重依赖昂贵的人类或LLM作为评判者的偏好反馈可能会阻碍其更广泛的应用。在这项工作中，我们引入了Self-Contrast，一种通过利用广泛自动生成的负例来进行无需反馈的大型语言模型对齐方法。仅通过监督的微调（SFT）目标，Self-Contrast利用LLM本身生成大量多样的候选项，并利用预训练的嵌入模型根据文本相似性过滤多个负例。理论上，我们证明了在这种设置中，仅仅扩大负面回应仍然可以有效地近似具有更平衡的正面和负面偏好注释的情况。我们对三个数据集进行了直接偏好优化（DPO）的实验表明，Self-Contrast能够始终优于

    arXiv:2404.00604v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has been a central technique for recent large language model (LLM) alignment. However, its heavy dependence on costly human or LLM-as-Judge preference feedback could stymie its wider applications. In this work, we introduce Self-Contrast, a feedback-free large language model alignment method via exploiting extensive self-generated negatives. With only supervised fine-tuning (SFT) targets, Self-Contrast leverages the LLM itself to generate massive diverse candidates, and harnesses a pre-trained embedding model to filter multiple negatives according to text similarity. Theoretically, we illustrate that in this setting, merely scaling negative responses can still effectively approximate situations with more balanced positive and negative preference annotations. Our experiments with direct preference optimization (DPO) on three datasets show that, Self-Contrast could consistently outperform
    
[^83]: AI法律和大型语言模型（LLMs）：当关键问题和隐私影响需要人类和道德监督时

    AI Act and Large Language Models (LLMs): When critical issues and privacy impact require human and ethical oversight

    [https://arxiv.org/abs/2404.00600](https://arxiv.org/abs/2404.00600)

    论文讨论了人类监督、道德监督和隐私影响评估在面对人工智能系统和大型语言模型发展中的重要性。

    

    人工智能系统的日益发展，特别是大型语言模型（LLM）的发展，使得有必要对它们在隐私、个人数据保护以及道德层面，尤其是对最脆弱和最弱势群体可能产生的风险和影响进行评估。本文对人类监督、道德监督和隐私影响评估进行了讨论。

    arXiv:2404.00600v1 Announce Type: cross  Abstract: The imposing evolution of artificial intelligence systems and, specifically, of Large Language Models (LLM) makes it necessary to carry out assessments of their level of risk and the impact they may have in the area of privacy, personal data protection and at an ethical level, especially on the weakest and most vulnerable. This contribution addresses human oversight, ethical oversight, and privacy impact assessment.
    
[^84]: EvoCodeBench: 一个与现实世界代码库对齐的进化代码生成基准

    EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories

    [https://arxiv.org/abs/2404.00599](https://arxiv.org/abs/2404.00599)

    EvoCodeBench 是一个与现实世界代码库对齐的进化代码生成基准，具有完善的注释和评估度量标准，同时避免数据泄露。

    

    arXiv:2404.00599v1 公告类型: 跨界 摘要: 如何评估大型语言模型(LLMs)在代码生成中的表现是一个悬而未决的问题。现有的基准测试表现出与现实世界代码库的差距，不足以评估LLMs的编码能力。本文提出了一个新的基准测试 - EvoCodeBench来解决前述问题，其具有三个主要进展。(1) EvoCodeBench在多个维度上与现实世界库对齐，例如代码分布和依赖分布。(2) EvoCodeBench提供了全面的注释(例如需求、参考代码和参考依赖)，以及强大的评估度量标准(例如Pass@k和Recall@k)。(3) EvoCodeBench是一个不断发展的基准测试，以避免数据泄漏。我们构建了一个自动化流水线，从最新的库中更新EvoCodeBench。我们发布了第一个版本 - EvoCodeBench-2403，其中包含来自25个现实世界库的275个样本。基于EvoCodeBench，我们提出repo

    arXiv:2404.00599v1 Announce Type: cross  Abstract: How to evaluate Large Language Models (LLMs) in code generation is an open question. Existing benchmarks demonstrate poor alignment with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs. This paper proposes a new benchmark - EvoCodeBench to address the preceding problems, which has three primary advances. (1) EvoCodeBench aligns with real-world repositories in multiple dimensions, e.g., code distributions and dependency distributions. (2) EvoCodeBench offers comprehensive annotations (e.g., requirements, reference code, and reference dependencies), and robust evaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving benchmark to avoid data leakage. We build an automatic pipeline to update EvoCodeBench from the latest repositories. We release the first version - EvoCodeBench-2403, containing 275 samples from 25 real-world repositories. Based on EvoCodeBench, we propose repo
    
[^85]: ECtHR-PCR：欧洲人权法院的判例理解和先前案例检索数据集

    ECtHR-PCR: A Dataset for Precedent Understanding and Prior Case Retrieval in the European Court of Human Rights

    [https://arxiv.org/abs/2404.00596](https://arxiv.org/abs/2404.00596)

    这项研究提出了一个基于欧洲人权法院裁决的先前案例检索数据集，有效地分离事实和论点，展示了判例实践，有助于开发先前案例检索技术。

    

    在普通法管辖区，法律实践者依赖于判例来构建论点，符合\emph{stare decisis}原则。随着多年来案件数量的增长，先前案例检索（PCR）受到了重视。现有的PCR数据集除了缺乏真实世界规模外，并未模拟现实情境，因为它们的查询使用完整案件文件，只掩盖先前案例的引用。因此，查询就会暴露于尚未形成对未决案进行论证时所需的法律推理，以及可能会干扰案件事实和法律原则的全面理解的引用掩码所留下的虚假模式。为了解决这些限制，我们介绍了一个基于欧洲人权法院（ECtHR）裁决的PCR数据集，明确区分事实和论点，并展示了判例实践，帮助我们发展这一PCR。

    arXiv:2404.00596v1 Announce Type: new  Abstract: In common law jurisdictions, legal practitioners rely on precedents to construct arguments, in line with the doctrine of \emph{stare decisis}. As the number of cases grow over the years, prior case retrieval (PCR) has garnered significant attention. Besides lacking real-world scale, existing PCR datasets do not simulate a realistic setting, because their queries use complete case documents while only masking references to prior cases. The query is thereby exposed to legal reasoning not yet available when constructing an argument for an undecided case as well as spurious patterns left behind by citation masks, potentially short-circuiting a comprehensive understanding of case facts and legal principles. To address these limitations, we introduce a PCR dataset based on judgements from the European Court of Human Rights (ECtHR), which explicitly separate facts from arguments and exhibit precedential practices, aiding us to develop this PCR 
    
[^86]: 从法律裁决中基于查询提取相关段落

    Query-driven Relevant Paragraph Extraction from Legal Judgments

    [https://arxiv.org/abs/2404.00595](https://arxiv.org/abs/2404.00595)

    本研究侧重于从法律裁决中基于查询提取相关段落的任务，结果表明微调和零-shot性能之间存在显著差距，强调了在法律领域处理分布变化的挑战。

    

    法律专业人士经常苦于在漫长的法律裁决中寻找直接解答他们问题的信息。本文关注从法律裁决中基于查询提取相关段落的任务。我们利用欧洲人权法院（ECtHR）的案例法指南构建了一个专门的数据集。我们以零-shot方式评估当前检索模型的性能，并建立使用各种模型的微调基准。结果突出了微调和零-shot性能之间的显著差距，强调了处理法律领域中分布变化的挑战。我们注意到，法律预训练在语料库侧处理分布变化，但在查询侧分布变化（看不见的法律查询）方面仍然有困难。我们还探讨了各种参数高效微调（PEFT）方法，以评估它们在实践中的实用性。

    arXiv:2404.00595v1 Announce Type: new  Abstract: Legal professionals often grapple with navigating lengthy legal judgements to pinpoint information that directly address their queries. This paper focus on this task of extracting relevant paragraphs from legal judgements based on the query. We construct a specialized dataset for this task from the European Court of Human Rights (ECtHR) using the case law guides. We assess the performance of current retrieval models in a zero-shot way and also establish fine-tuning benchmarks using various models. The results highlight the significant gap between fine-tuned and zero-shot performance, emphasizing the challenge of handling distribution shift in the legal domain. We notice that the legal pre-training handles distribution shift on the corpus side but still struggles on query side distribution shift, with unseen legal queries. We also explore various Parameter Efficient Fine-Tuning (PEFT) methods to evaluate their practicality within the cont
    
[^87]: LexAbSumm：基于方面的法律判决摘要

    LexAbSumm: Aspect-based Summarization of Legal Decisions

    [https://arxiv.org/abs/2404.00594](https://arxiv.org/abs/2404.00594)

    LexAbSumm是一个针对法律案例决策的基于方面的摘要数据集，评估了多个适用于较长文档的抽象摘要模型，揭示了在产生特定方面摘要时这些模型的挑战。

    

    法律专业人士经常遇到包含对他们工作至关重要见解的长篇法律判决。尽管最近的进展已经导致了针对法律文件的自动摘要解决方案，但它们通常提供通用摘要，可能无法满足用户多样化的信息需求。为了填补这一空白，我们引入了LexAbSumm，这是一个专为源自欧洲人权法院管辖区的法律案例决策进行基于方面的摘要设计的新颖数据集。我们评估了几种针对长篇文档的抽象摘要模型在LexAbSumm上的性能，揭示了一种条件这些模型生成特定方面摘要的挑战。我们发布LexAbSumm以促进法律领域基于方面的摘要研究。

    arXiv:2404.00594v1 Announce Type: new  Abstract: Legal professionals frequently encounter long legal judgments that hold critical insights for their work. While recent advances have led to automated summarization solutions for legal documents, they typically provide generic summaries, which may not meet the diverse information needs of users. To address this gap, we introduce LexAbSumm, a novel dataset designed for aspect-based summarization of legal case decisions, sourced from the European Court of Human Rights jurisdiction. We evaluate several abstractive summarization models tailored for longer documents on LexAbSumm, revealing a challenge in conditioning these models to produce aspect-specific summaries. We release LexAbSum to facilitate research in aspect-based summarization for legal domain.
    
[^88]: CuSINeS：基于课程驱动的结构诱导负采样用于法定文章检索

    CuSINeS: Curriculum-driven Structure Induced Negative Sampling for Statutory Article Retrieval

    [https://arxiv.org/abs/2404.00590](https://arxiv.org/abs/2404.00590)

    CuSINeS通过课程驱动的负采样策略、结构化法规信息和动态语义难度评估三方面贡献，提升了法定文章检索的效果。

    

    在本文中，我们介绍了CuSINeS，这是一种负采样方法，旨在提高法定文章检索（SAR）的性能。CuSINeS提供了三个关键贡献。首先，它采用了基于课程的负采样策略，引导模型首先关注更容易的负样本，逐渐处理更难的样本。其次，它利用从法规结构组织中获得的分层和序列信息来评估样本的难度。最后，它引入了动态语义难度评估，利用正在训练的模型本身，超越传统的静态方法如BM25，使负样本适应模型不断进化的能力。在一个真实世界的专家注释的SAR数据集上的实验证实了CuSINeS在四种不同基线上的有效性，展示了其多功能性。

    arXiv:2404.00590v1 Announce Type: cross  Abstract: In this paper, we introduce CuSINeS, a negative sampling approach to enhance the performance of Statutory Article Retrieval (SAR). CuSINeS offers three key contributions. Firstly, it employs a curriculum-based negative sampling strategy guiding the model to focus on easier negatives initially and progressively tackle more difficult ones. Secondly, it leverages the hierarchical and sequential information derived from the structural organization of statutes to evaluate the difficulty of samples. Lastly, it introduces a dynamic semantic difficulty assessment using the being-trained model itself, surpassing conventional static methods like BM25, adapting the negatives to the model's evolving competence. Experimental results on a real-world expert-annotated SAR dataset validate the effectiveness of CuSINeS across four different baselines, demonstrating its versatility.
    
[^89]: 利用大型语言模型处理图数据中的不确定性

    Harnessing the Power of Large Language Model for Uncertainty Aware Graph Processing

    [https://arxiv.org/abs/2404.00589](https://arxiv.org/abs/2404.00589)

    介绍了一种利用大型语言模型处理图数据中不确定性的方法，通过不确定性感知模块增强，提供置信度评分，实验结果表明该方法在知识图完成和图分类任务上超越了最先进算法。

    

    处理图数据是一项非常困难的任务。传统技术，例如基于几何和矩阵分解的技术，依赖于对数据关系的假设，在处理大型和复杂的图数据时变得不足够。另一方面，深度学习方法展示了处理大型图数据的良好结果，但它们通常无法提供可解释的解释。为了使图处理具有高准确性和可解释性，我们引入了一种新颖的方法，利用了增强不确定性感知模块的大型语言模型(LLM)的力量，以提供生成答案的置信度分数。我们在两个图处理任务上对我们的方法进行了实验：少样本知识图完成和图分类。我们的结果表明，通过参数高效微调，LLM在各个方面超越了最先进的算法

    arXiv:2404.00589v1 Announce Type: cross  Abstract: Handling graph data is one of the most difficult tasks. Traditional techniques, such as those based on geometry and matrix factorization, rely on assumptions about the data relations that become inadequate when handling large and complex graph data. On the other hand, deep learning approaches demonstrate promising results in handling large graph data, but they often fall short of providing interpretable explanations. To equip the graph processing with both high accuracy and explainability, we introduce a novel approach that harnesses the power of a large language model (LLM), enhanced by an uncertainty-aware module to provide a confidence score on the generated answer. We experiment with our approach on two graph processing tasks: few-shot knowledge graph completion and graph classification. Our results demonstrate that through parameter efficient fine-tuning, the LLM surpasses state-of-the-art algorithms by a substantial margin across
    
[^90]: 可解释的多跳问题生成：一种无需中间问题标记的端到端方法

    Explainable Multi-hop Question Generation: An End-to-End Approach without Intermediate Question Labeling

    [https://arxiv.org/abs/2404.00571](https://arxiv.org/abs/2404.00571)

    提出了一种端到端问题重写模型，通过顺序重写增加问题复杂性，能够无需中间问题标记训练生成解释性强的多跳问题。

    

    随着交互式人工智能的增加使用，处理复杂问题的需求也在增加。多跳问题生成旨在生成需要在多个文档上进行多步推理的复杂问题。先前的研究主要采用端到端模型，其中问题是基于上下文文档的表示进行解码的。然而，这些方法缺乏解释生成的多跳问题背后的推理过程的能力。此外，问题重写方法，逐步增加问题的复杂性，也由于需要为中间阶段问题标记数据而存在局限性。在本文中，我们引入一种端到端问题重写模型，通过顺序重写增加问题复杂性。所提出的模型具有仅训练最终的多跳问题的优势。

    arXiv:2404.00571v1 Announce Type: new  Abstract: In response to the increasing use of interactive artificial intelligence, the demand for the capacity to handle complex questions has increased. Multi-hop question generation aims to generate complex questions that requires multi-step reasoning over several documents. Previous studies have predominantly utilized end-to-end models, wherein questions are decoded based on the representation of context documents. However, these approaches lack the ability to explain the reasoning process behind the generated multi-hop questions. Additionally, the question rewriting approach, which incrementally increases the question complexity, also has limitations due to the requirement of labeling data for intermediate-stage questions. In this paper, we introduce an end-to-end question rewriting model that increases question complexity through sequential rewriting. The proposed model has the advantage of training with only the final multi-hop questions, w
    
[^91]: ParaICL：面向鲁棒性的并行上下文学习

    ParaICL: Towards Robust Parallel In-Context Learning

    [https://arxiv.org/abs/2404.00570](https://arxiv.org/abs/2404.00570)

    提出了一种名为ParaICL的新方法，通过并行批处理来有效利用所有示例，在不超过可管理的输入上下文长度的情况下显著提升了不同测试样本的准确性。

    

    大型语言模型(LLMs)已经成为自然语言处理(NLP)领域的常态，在少样本上下文学习(ICL)方面表现出色。然而，ICL的成功很大程度上取决于少样本演示示例的选择，使得选择过程变得愈发关键。现有方法已经开始优化这些示例的数量和语义相似性以提高ICL性能。然而，我们的初步实验表明，ICL的有效性受到输入上下文长度的限制。此外，不同的少样本演示示例组合可以显著提升对不同测试样本的准确性。为了解决这个问题，我们提出了一种名为parallel in-context learning (ParaICL)的新方法，可以有效利用所有示例而不会超出可管理的输入上下文长度。ParaICL采用并行批处理来分发演示

    arXiv:2404.00570v1 Announce Type: new  Abstract: Large language models (LLMs) have become the norm in natural language processing (NLP), excelling in few-shot in-context learning (ICL) with their remarkable abilities. Nonetheless, the success of ICL largely hinges on the choice of few-shot demonstration examples, making the selection process increasingly crucial. Existing methods have delved into optimizing the quantity and semantic similarity of these examples to improve ICL performances. However, our preliminary experiments indicate that the effectiveness of ICL is limited by the length of the input context. Moreover, varying combinations of few-shot demonstration examples can significantly boost accuracy across different test samples. To address this, we propose a novel method named parallel in-context learning (ParaICL) that effectively utilizes all demonstration examples without exceeding the manageable input context length. ParaICL employs parallel batching to distribute demonstr
    
[^92]: CM-TTS: 通过加权采样器和一致性模型提高实时文本转语音合成效率

    CM-TTS: Enhancing Real Time Text-to-Speech Synthesis Efficiency through Weighted Samplers and Consistency Models

    [https://arxiv.org/abs/2404.00569](https://arxiv.org/abs/2404.00569)

    CM-TTS通过一致性模型和加权采样器实现了更高效的实时文本转语音合成，避免了对抗训练和预训练模型的依赖。

    

    神经文本转语音（TTS）系统在语音助手、电子学习和有声读物制作等领域具有广泛应用。现代模型的追求，如扩散模型（DMs），有望实现高保真、实时的语音合成。然而，扩散模型中多步采样的效率存在挑战。已经做出努力将GANs与DMs集成在一起，通过逼近去噪分布加快推断速度，但这由于对抗训练导致模型收敛出现问题。为了克服这些问题，我们提出了CM-TTS，这是一种基于一致性模型（CMs）的新型体系结构。CM-TTS汲取了连续时间扩散模型的灵感，在较少步骤中实现了高质量的语音合成，而无需进行对抗训练或依赖预训练模型。我们进一步设计了加权采样器，以动态概率将不同采样位置纳入模型训练，确保不偏倚。

    arXiv:2404.00569v1 Announce Type: cross  Abstract: Neural Text-to-Speech (TTS) systems find broad applications in voice assistants, e-learning, and audiobook creation. The pursuit of modern models, like Diffusion Models (DMs), holds promise for achieving high-fidelity, real-time speech synthesis. Yet, the efficiency of multi-step sampling in Diffusion Models presents challenges. Efforts have been made to integrate GANs with DMs, speeding up inference by approximating denoising distributions, but this introduces issues with model convergence due to adversarial training. To overcome this, we introduce CM-TTS, a novel architecture grounded in consistency models (CMs). Drawing inspiration from continuous-time diffusion models, CM-TTS achieves top-quality speech synthesis in fewer steps without adversarial training or pre-trained model dependencies. We further design weighted samplers to incorporate different sampling positions into model training with dynamic probabilities, ensuring unbias
    
[^93]: CodeBenchGen: 创建可扩展的基于执行的代码生成基准

    CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks

    [https://arxiv.org/abs/2404.00566](https://arxiv.org/abs/2404.00566)

    提出了CodeBenchGen框架，通过利用大型语言模型将任意代码转化为评估示例，创造了一个包含大量代码示例的数据集Exec-CSN，展示了其可扩展性和实用性。

    

    为了促进在不同场景下评估代码生成系统，我们提出了CodeBenchGen，这是一个框架，可以创建可扩展的基于执行的基准，仅需要轻微的人类指导。具体来说，我们利用一个大型语言模型（LLM）将任意代码片段转化为评估示例，包括用于执行评估的测试用例。我们通过创建包含来自CodeSearchNet数据集的367个GitHub存储库中的代码修改的293个库的1,931个例子的数据集Exec-CSN，展示了我们框架的实用性。为了展示Exec-CSN中示例的复杂性和可解性，我们进行了一个人类研究，结果显示81.3%的例子可以被人类解决，61%被评为“需要努力解决”。我们对开源和专有模型进行了代码生成实验，并分析了人类和模型的性能。

    arXiv:2404.00566v1 Announce Type: cross  Abstract: To facilitate evaluation of code generation systems across diverse scenarios, we present CodeBenchGen, a framework to create scalable execution-based benchmarks that only requires light guidance from humans. Specifically, we leverage a large language model (LLM) to convert an arbitrary piece of code into an evaluation example, including test cases for execution-based evaluation. We illustrate the usefulness of our framework by creating a dataset, Exec-CSN, which includes 1,931 examples involving 293 libraries revised from code in 367 GitHub repositories taken from the CodeSearchNet dataset. To demonstrate the complexity and solvability of examples in Exec-CSN, we present a human study demonstrating that 81.3% of the examples can be solved by humans and 61% are rated as ``requires effort to solve''. We conduct code generation experiments on open-source and proprietary models and analyze the performance of both humans and models. We will
    
[^94]: 利用语料库元数据检测基于模板的翻译：埃及阿拉伯维基百科版本的探索性案例研究

    Leveraging Corpus Metadata to Detect Template-based Translation: An Exploratory Case Study of the Egyptian Arabic Wikipedia Edition

    [https://arxiv.org/abs/2404.00565](https://arxiv.org/abs/2404.00565)

    该研究旨在通过探索性研究识别埃及阿拉伯维基百科中基于模板翻译的文章及其特征，以解决因此导致的问题。

    

    维基百科文章（内容页面）通常被用作自然语言处理（NLP）研究中的语料库，特别是对英语以外的低资源语言。然而，很少有研究研究过三个阿拉伯维基百科版本，阿拉伯维基百科（AR）、埃及阿拉伯维基百科（ARZ）和摩洛哥阿拉伯维基百科（ARY），并记录了埃及阿拉伯维基百科版本中出现的问题，即通过从英语翻译为阿拉伯语的基于模板的自动创建其文章，没有人的参与，导致埃及阿拉伯维基百科充斥着不仅内容质量低下的文章，还有不代表埃及人民、他们的文化和方言的文章。本文旨在通过探索性地识别这些基于模板翻译的文章及其特征，缓解埃及阿拉伯维基百科中出现的模板翻译问题。

    arXiv:2404.00565v1 Announce Type: new  Abstract: Wikipedia articles (content pages) are commonly used corpora in Natural Language Processing (NLP) research, especially in low-resource languages other than English. Yet, a few research studies have studied the three Arabic Wikipedia editions, Arabic Wikipedia (AR), Egyptian Arabic Wikipedia (ARZ), and Moroccan Arabic Wikipedia (ARY), and documented issues in the Egyptian Arabic Wikipedia edition regarding the massive automatic creation of its articles using template-based translation from English to Arabic without human involvement, overwhelming the Egyptian Arabic Wikipedia with articles that do not only have low-quality content but also with articles that do not represent the Egyptian people, their culture, and their dialect. In this paper, we aim to mitigate the problem of template translation that occurred in the Egyptian Arabic Wikipedia by identifying these template-translated articles and their characteristics through exploratory 
    
[^95]: DivTOD: 发挥LLM的力量，为任务导向对话表示多样化释放潜力

    DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented Dialogue Representations

    [https://arxiv.org/abs/2404.00557](https://arxiv.org/abs/2404.00557)

    DivTOD 提出了一种新颖的对话预训练模型，与LLMs合作学习多样的任务导向对话表示，实验表明该模型在各种下游对话任务上优于强基线，并学习了任务导向对话的内在多样性。

    

    在各个领域普遍采用的在通用文本上预先训练的语言模型取得了令人印象深刻的成果。然而，与通用文本相比，任务导向对话（TOD）具有明显的语言特征，这限制了现有语言模型的实际效用。当前的任务导向对话预训练方法忽视了对话的一对多属性，即在相同对话上下文下，可以有多个适当的回复。在本文中，我们提出了一种名为DivTOD的新型对话预训练模型，它与LLM共同学习多样化的任务导向对话表示。DivTOD指导LLM在向较小模型传递多样化知识的同时，消除与任务导向对话相矛盾的领域知识。实验表明，我们的模型在各种下游对话任务上胜过强大的TOD基线，并学习了任务导向对话的内在多样性。

    arXiv:2404.00557v1 Announce Type: new  Abstract: Language models pre-trained on general text have achieved impressive results in diverse fields. Yet, the distinct linguistic characteristics of task-oriented dialogues (TOD) compared to general text limit the practical utility of existing language models. Current task-oriented dialogue pre-training methods overlook the one-to-many property of conversations, where multiple responses can be appropriate given the same conversation context. In this paper, we propose a novel dialogue pre-training model called DivTOD, which collaborates with LLMs to learn diverse task-oriented dialogue representations. DivTOD guides LLMs in transferring diverse knowledge to smaller models while removing domain knowledge that contradicts task-oriented dialogues. Experiments show that our model outperforms strong TOD baselines on various downstream dialogue tasks and learns the intrinsic diversity of task-oriented dialogues.
    
[^96]: 将坏苹果与好橘子进行比较：通过联合优化偏好对齐大型语言模型

    Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization

    [https://arxiv.org/abs/2404.00530](https://arxiv.org/abs/2404.00530)

    本研究提出了一种新的偏好获取方法，通过DOVE协议对指令-响应对的联合概率进行优化，以对齐大型语言模型。

    

    一种常见的对齐大型语言模型（LLMs）的技术依赖于通过比较在固定上下文中条件生成的多个生成的人类偏好。然而，当这些生成放置在相同的上下文中时，这仅利用了成对比较。然而，这种条件排名通常无法捕获人类偏好的复杂和多维方面。在这项工作中，我们重新审视偏好获取的传统范式，并提出了一个基于在指令-响应对上联合引发偏好的新轴。虽然先前的偏好优化是针对条件排名协议（例如，DPO）设计的，但我们提出的偏好获取协议引入了DOVE，这是一个新的偏好优化目标，通过提升所选指令-响应对的联合概率来降低所拒绝指令-响应对的概率。

    arXiv:2404.00530v1 Announce Type: cross  Abstract: A common technique for aligning large language models (LLMs) relies on acquiring human preferences by comparing multiple generations conditioned on a fixed context. This only leverages the pairwise comparisons when the generations are placed in an identical context. However, such conditional rankings often fail to capture the complex and multidimensional aspects of human preferences. In this work, we revisit the traditional paradigm of preference acquisition and propose a new axis that is based on eliciting preferences jointly over the instruction-response pairs. While prior preference optimizations are designed for conditional ranking protocols (e.g., DPO), our proposed preference acquisition protocol introduces DOVE, a new preference optimization objective that upweights the joint probability of the chosen instruction-response pair over the rejected instruction-response pair. Interestingly, we find that the LLM trained with joint ins
    
[^97]: MIPS在SemEval-2024任务3中的表现：使用多模态语言模型在对话中进行多模态情绪-原因对提取

    MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in Conversations with Multimodal Language Models

    [https://arxiv.org/abs/2404.00511](https://arxiv.org/abs/2404.00511)

    本文提出了一种集成文本、音频和视觉模态的多模态情绪识别和多模态情绪原因提取框架，通过利用专门的情绪编码器和模态特定特征，实现了提升情绪理解和因果推理的竞争性成果。

    

    本文介绍了我们在SemEval 2024任务3的子任务2中关于对话中多模态情绪原因分析的获奖提交。我们提出了一种新颖的多模态情绪识别和多模态情绪原因提取（MER-MCE）框架，该框架利用专门的情绪编码器整合文本、音频和视觉三种模态。我们的方法通过利用模态特定特征提升情绪理解和因果推理，使自己脱颖而出。实验评估表明了我们多模态方法的优势，我们的提交取得了竞争性的加权F1分数为0.3435，在0.0339之后排名第一的团队，仅在0.0025之后排名第二。项目链接：https://github.com/MIPS-COLT/MER-MCE.git

    arXiv:2404.00511v1 Announce Type: new  Abstract: This paper presents our winning submission to Subtask 2 of SemEval 2024 Task 3 on multimodal emotion cause analysis in conversations. We propose a novel Multimodal Emotion Recognition and Multimodal Emotion Cause Extraction (MER-MCE) framework that integrates text, audio, and visual modalities using specialized emotion encoders. Our approach sets itself apart from top-performing teams by leveraging modality-specific features for enhanced emotion understanding and causality inference. Experimental evaluation demonstrates the advantages of our multimodal approach, with our submission achieving a competitive weighted F1 score of 0.3435, ranking third with a margin of only 0.0339 behind the 1st team and 0.0025 behind the 2nd team. Project: https://github.com/MIPS-COLT/MER-MCE.git
    
[^98]: 单词嵌入的形状：通过拓扑数据分析识别语言谱系

    The Shape of Word Embeddings: Recognizing Language Phylogenies through Topological Data Analysis

    [https://arxiv.org/abs/2404.00500](https://arxiv.org/abs/2404.00500)

    通过拓扑数据分析识别语言谱系，研究了单词嵌入的形状如何传递信息，重建的语言谱系树与参考树展现出强烈的相似性。

    

    arXiv:2404.00500v1 类型：新 原文摘要：单词嵌入将语言词汇表示为$d$维空间的点云。我们研究了这些点云的一般形状在除了表示每个令牌的语义意义之外传递信息的方式。具体而言，我们使用拓扑数据分析(TDA)中的持久同调概念来测量从它们未标记的嵌入形状计算的语言对之间的距离。我们使用这些距离矩阵在81种印欧语言之间构建语言谱系树。仔细评估表明我们重建的谱系树与参考树呈现出强烈的相似性。

    arXiv:2404.00500v1 Announce Type: new  Abstract: Word embeddings represent language vocabularies as clouds of $d$-dimensional points. We investigate how information is conveyed by the general shape of these clouds, outside of representing the semantic meaning of each token. Specifically, we use the notion of persistent homology from topological data analysis (TDA) to measure the distances between language pairs from the shape of their unlabeled embeddings. We use these distance matrices to construct language phylogenetic trees over 81 Indo-European languages. Careful evaluation shows that our reconstructed trees exhibit strong similarities to the reference tree.
    
[^99]: 使用合成偏好数据对语言模型进行可配置安全调优

    Configurable Safety Tuning of Language Models with Synthetic Preference Data

    [https://arxiv.org/abs/2404.00495](https://arxiv.org/abs/2404.00495)

    提出了一种名为Configurable Safety Tuning（CST）的新方法，通过使用合成偏好数据，在推断时实现对LLMs的灵活安全配置，允许用户根据需要禁用/启用安全偏好，且实验表明CST成功管理不同的安全配置并保留了原始功能，是一种适用于可配置部署的强大方法。

    

    最先进的语言模型微调技术，如直接偏好优化（DPO），通过将预定义行为硬编码到模型中，限制了用户的控制权。为了解决这个问题，我们提出了一种新颖的方法，Configurable Safety Tuning（CST），它利用合成偏好数据来增强DPO，以促进在推断时对LLMs进行灵活的安全配置。CST通过引入指定安全配置的系统提示，允许LLM部署者根据需要禁用/启用安全偏好，仅需更改系统提示。我们的实验评估表明，CST成功管理不同的安全配置，并保留了LLMs的原始功能，显示出它是一种适用于可配置部署的强大方法。

    arXiv:2404.00495v1 Announce Type: cross  Abstract: State-of-the-art language model fine-tuning techniques, such as Direct Preference Optimization (DPO), restrict user control by hard-coding predefined behaviors into the model. To address this, we propose a novel method, Configurable Safety Tuning (CST), that augments DPO using synthetic preference data to facilitate flexible safety configuration of LLMs at inference time. CST overcomes the constraints of vanilla DPO by introducing a system prompt specifying safety configurations, enabling LLM deployers to disable/enable safety preferences based on their need, just changing the system prompt. Our experimental evaluations indicate that CST successfully manages different safety configurations and retains the original functionality of LLMs, showing it is a robust method for configurable deployment. Data and models available at https://github.com/vicgalle/configurable-safety-tuning
    
[^100]: 时态知识编辑下的多跳问题回答

    Multi-hop Question Answering under Temporal Knowledge Editing

    [https://arxiv.org/abs/2404.00492](https://arxiv.org/abs/2404.00492)

    提出了一个新颖的框架TEMPLE-MQA，通过构建时间感知图和推理路径、结构检索和联合推理阶段，有效地识别多跳问题回答中的时间背景，在基准数据集上显著优于基线模型，并贡献了一个新数据集TKEMQA。

    

    在大语言模型时代，多跳问题回答 (MQA) 在知识编辑 (KE) 下引起了广泛关注。然而，现有的MQA在处理包含显式时间背景的问题时表现不佳。为了解决这一限制，我们提出了一个新颖的框架，即时态知识增强的多跳问题回答 (TEMPLE-MQA)。不同于以往的方法，TEMPLE-MQA首先构建一个时间感知图 (TAG)，以结构化方式存储编辑知识。然后，通过我们提出的推理路径、结构检索和联合推理阶段，TEMPLE-MQA有效地识别问题查询中的时间背景。对基准数据集的实验表明，TEMPLE-MQA显著优于基线模型。此外，我们贡献了一个新数据集，名为TKEMQA，专门为带有时间约束的MQA量身定制，作为首个基准。

    arXiv:2404.00492v1 Announce Type: cross  Abstract: Multi-hop question answering (MQA) under knowledge editing (KE) has garnered significant attention in the era of large language models. However, existing models for MQA under KE exhibit poor performance when dealing with questions containing explicit temporal contexts. To address this limitation, we propose a novel framework, namely TEMPoral knowLEdge augmented Multi-hop Question Answering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a time-aware graph (TAG) to store edit knowledge in a structured manner. Then, through our proposed inference path, structural retrieval, and joint reasoning stages, TEMPLE-MQA effectively discerns temporal contexts within the question query. Experiments on benchmark datasets demonstrate that TEMPLE-MQA significantly outperforms baseline models. Additionally, we contribute a new dataset, namely TKEMQA, which serves as the inaugural benchmark tailored specifically for MQA with temporal
    
[^101]: PROMPT-SAW：利用关系感知图进行文本提示压缩

    PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression

    [https://arxiv.org/abs/2404.00489](https://arxiv.org/abs/2404.00489)

    提出了PROMPT-SAW模型，利用关系感知图来实现文本提示的压缩，提高了提示的可读性和可解释性。

    

    大型语言模型(LLMs)在多种不同的自然语言处理任务中展现出卓越的能力。提示是LLM推理中的基本工具，但我们观察到超长提示会带来显著的成本。现有的压缩长提示的尝试导致压缩提示在可读性和可解释性方面表现不佳，对提示效用产生有害影响。为了解决这一问题，我们提出了PROMPT-SAW：通过关系感知图进行提示压缩，这是一种针对任务不可知和任务感知提示的有效策略。PROMPT-SAW使用提示的文本信息构建图形，在图形中提取关键信息元素，从而得出压缩提示。我们还提出了GSM8K-AUG，即现有GSM8k基准的扩展版本，用于任务不可知提示，以提供全面的评估平台。

    arXiv:2404.00489v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown exceptional abilities for multiple different natural language processing tasks. While prompting is a crucial tool for LLM inference, we observe that there is a significant cost associated with exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead to sub-standard results in terms of readability and interpretability of the compressed prompt, with a detrimental impact on prompt utility. To address this, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an effective strategy for prompt compression over task-agnostic and task-aware prompts. PROMPT-SAW uses the prompt's textual information to build a graph, later extracts key information elements in the graph to come up with the compressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the existing GSM8k benchmark for task-agnostic prompts in order to provide a comprehensive evaluation platf
    
[^102]: 噪声感知的布局感知语言模型训练

    Noise-Aware Training of Layout-Aware Language Models

    [https://arxiv.org/abs/2404.00488](https://arxiv.org/abs/2404.00488)

    本论文提出了一种噪声感知训练方法，可以以可扩展的方式使用弱标记文档训练提取器，从而避免了在企业场景中训练大量不同文档类型的自定义提取器所需的昂贵人工标记成本。

    

    一篇视觉丰富的文档（VRD）利用视觉特征和语言线索传播信息。训练一个能够从文档中识别命名实体的自定义提取器需要大量标记为文本和视觉模态的目标文档实例。在企业场景中，我们希望以可扩展的方式为成千上万种不同文档类型训练自定义提取器，这是一个昂贵的瓶颈。在未标记目标文档实例上预训练提取器模型，然后在人工标记实例上进行微调，在这些情况下是行不通的，因为它超出了为提取器分配的最大允许训练时间。本文提出了一种噪声感知训练方法（NAT）来解决这个场景。NAT利用弱标记文档以可扩展的方式训练提取器，而不是获取昂贵的人工标记文档。

    arXiv:2404.00488v1 Announce Type: cross  Abstract: A visually rich document (VRD) utilizes visual features along with linguistic cues to disseminate information. Training a custom extractor that identifies named entities from a document requires a large number of instances of the target document type annotated at textual and visual modalities. This is an expensive bottleneck in enterprise scenarios, where we want to train custom extractors for thousands of different document types in a scalable way. Pre-training an extractor model on unlabeled instances of the target document type, followed by a fine-tuning step on human-labeled instances does not work in these scenarios, as it surpasses the maximum allowable training time allocated for the extractor. We address this scenario by proposing a Noise-Aware Training method or NAT in this paper. Instead of acquiring expensive human-labeled documents, NAT utilizes weakly labeled documents to train an extractor in a scalable way. To avoid degr
    
[^103]: 辩证对齐：解决3H紧张与LLM安全威胁

    Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs

    [https://arxiv.org/abs/2404.00486](https://arxiv.org/abs/2404.00486)

    提出了辩证对齐（DA）框架来解决LLM在面临外部有毒数据时的适应性变色龙问题，增强了其对抗外部攻击的安全性

    

    随着大型语言模型（LLM）的兴起，确保它们体现有益、诚实和无害（3H）原则，即人类对齐，变得至关重要。现有的对齐方法如RLHF、DPO等有效地微调LLM以匹配偏好数据集中的偏好，但往往会使LLM对高度接受人类输入和外部证据，即使这些信息是有毒的。这导致LLM倾向于成为适应变色龙，当外部证据与其参数性记忆冲突时。这加剧了LLM遭受外部有毒数据攻击的风险，对LLM系统应用（如检索增强生成）构成了重大安全风险。为了解决这一挑战，我们提出了一个新颖的框架：辩证对齐（DA），它（1）利用人工智能反馈来确定LLM导航相互文本冲突和上下文记忆冲突的最佳策略

    arXiv:2404.00486v1 Announce Type: cross  Abstract: With the rise of large language models (LLMs), ensuring they embody the principles of being helpful, honest, and harmless (3H), known as Human Alignment, becomes crucial. While existing alignment methods like RLHF, DPO, etc., effectively fine-tune LLMs to match preferences in the preference dataset, they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications such as Retrieval-augmented generation (RAG). To address the challenge, we propose a novel framework: Dialectical Alignment (DA), which (1) utilizes AI feedback to identify optimal strategies for LLMs to navigate inter-context conflicts and context-memory conflicts w
    
[^104]: SemEval-2024任务2中的爱丁堡临床NLP：Fine-tune your model unless you have access to GPT-4

    Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model unless you have access to GPT-4

    [https://arxiv.org/abs/2404.00484](https://arxiv.org/abs/2404.00484)

    本研究提出了一种参数高效微调（PEFT）方法，通过合并分别使用三元组和语言建模目标进行微调的适配器来提高大型语言模型（LLMs）的一致性，但未能超越GPT-4在忠实度和一致性方面的准确性。

    

    NLI4CT任务评估自然语言推理系统在预测假设是否包含或与临床试验报告中的证据相矛盾方面的表现。本研究评估了多种大型语言模型（LLMs）并采用多种策略，包括思维链、上下文学习和参数高效微调（PEFT）。我们提出了一种PEFT方法，通过合并分别使用三元组和语言建模目标进行微调的适配器来提高LLMs的一致性。我们发现合并两个PEFT适配器可以提高LLMs的F1分数（+0.0346）和一致性（+0.152）。然而，我们的新方法未能产生比GPT-4更准确的结果，无论是在忠实度还是一致性方面。综合三个指标，GPT-4在竞赛中以0.8328的得分并列第一。最后，我们与GPT-4的污染分析显示没有测试数据泄露。

    arXiv:2404.00484v1 Announce Type: new  Abstract: The NLI4CT task assesses Natural Language Inference systems in predicting whether hypotheses entail or contradict evidence from Clinical Trial Reports. In this study, we evaluate various Large Language Models (LLMs) with multiple strategies, including Chain-of-Thought, In-Context Learning, and Parameter-Efficient Fine-Tuning (PEFT). We propose a PEFT method to improve the consistency of LLMs by merging adapters that were fine-tuned separately using triplet and language modelling objectives. We found that merging the two PEFT adapters improves the F1 score (+0.0346) and consistency (+0.152) of the LLMs. However, our novel methods did not produce more accurate results than GPT-4 in terms of faithfulness and consistency. Averaging the three metrics, GPT-4 ranks joint-first in the competition with 0.8328. Finally, our contamination analysis with GPT-4 indicates that there was no test data leakage.
    
[^105]: 用于斯拉夫语的跨语言命名实体语料库

    Cross-lingual Named Entity Corpus for Slavic Languages

    [https://arxiv.org/abs/2404.00482](https://arxiv.org/abs/2404.00482)

    介绍了一个手动标注的用于六种斯拉夫语言的命名实体语料库，提供了两个训练数据集划分，并使用预训练的多语言模型进行命名实体的识别、分类、引用词化和链接。

    

    本文介绍了一个手动注释的包含六种斯拉夫语言（保加利亚语、捷克语、波兰语、斯洛文尼亚语、俄语和乌克兰语）命名实体的语料库。这项工作是2017-2023年间斯拉夫自然语言处理研讨会的一系列共享任务的结果。该语料库包含了5017份涵盖七个主题的文档，文档标有五类命名实体，每个实体由类别、引用词和唯一跨语言标识符描述。我们提供了两个训练调整的数据集划分 - 单个主题划分和跨主题划分。对于每个划分，我们使用基于transformer的神经网络架构设置了基准，使用预训练的多语言模型XLM-RoBERTa-large进行命名实体提及识别和分类，以及mT5-large进行命名实体引用词化和链接。

    arXiv:2404.00482v1 Announce Type: cross  Abstract: This paper presents a corpus manually annotated with named entities for six Slavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. This work is the result of a series of shared tasks, conducted in 2017-2023 as a part of the Workshops on Slavic Natural Language Processing. The corpus consists of 5 017 documents on seven topics. The documents are annotated with five classes of named entities. Each entity is described by a category, a lemma, and a unique cross-lingual identifier. We provide two train-tune dataset splits - single topic out and cross topics. For each split, we set benchmarks using a transformer-based neural network architecture with the pre-trained multilingual models - XLM-RoBERTa-large for named entity mention recognition and categorization, and mT5-large for named entity lemmatization and linking.
    
[^106]: 语言模型的语言校准

    Linguistic Calibration of Language Models

    [https://arxiv.org/abs/2404.00474](https://arxiv.org/abs/2404.00474)

    该论文提出了一种通过语言模型的文本生成来实现语言校准，可以使用户做出校准概率预测的方法。

    

    语言模型可能会在自信幻觉时导致用户做出次优化的下游决策。通过语言模型口头传达其主张正确概率可以缓解这个问题，但现有模型无法生成具有校准置信度声明的文本。我们通过决策角度，为长篇生成形式的语言校准形式化定义：如果语言模型的生成使其用户能够做出校准概率预测，则该模型是语言上校准的。这个定义使得一个训练框架成为可能，其中一个监督微调步骤引导一个语言模型发出带有置信度声明的长篇生成，诸如“我估计有30%的机会…”或“我确信…”，然后是一个强化学习步骤，奖励使用户能够对相关问题提供校准答案的生成。我们对Llama 2 7B 进行语言校准，并发现在自动化和人类测试中...

    arXiv:2404.00474v1 Announce Type: cross  Abstract: Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce text with calibrated confidence statements. Through the lens of decision-making, we formalize linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as "I estimate a 30% chance of..." or "I am certain that...", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and huma
    
[^107]: 在NLP模型中解决统计和因果性别公平性问题

    Addressing Both Statistical and Causal Gender Fairness in NLP Models

    [https://arxiv.org/abs/2404.00463](https://arxiv.org/abs/2404.00463)

    本研究评估了在NLP模型中同时处理统计和因果性别公平性偏见的方法，发现结合统计和因果性去偏置技术能够有效减少偏见。

    

    统计公平性规定对每个受保护群体有相同的结果，而因果公平性要求模型对个体的预测不受其受保护特征的影响。反事实数据增强（CDA）对于减少NLP模型中的偏见是有效的，然而使用CDA训练的模型通常只基于与因果公平性概念密切相关的指标进行评估；同样，为促进统计公平性而设计的基于抽样的方法很少受到因果公平性的评估。在这项工作中，我们评估了在NLP模型中处理性别偏见的统计性和因果性去偏置方法，发现虽然这些方法能够降低偏见，但并不一定会改善其他偏见指标。我们展示了统计性和因果性去偏置技术的组合能够减少通过这两种类型指标衡量的偏见。

    arXiv:2404.00463v1 Announce Type: new  Abstract: Statistical fairness stipulates equivalent outcomes for every protected group, whereas causal fairness prescribes that a model makes the same prediction for an individual regardless of their protected characteristics. Counterfactual data augmentation (CDA) is effective for reducing bias in NLP models, yet models trained with CDA are often evaluated only on metrics that are closely tied to the causal fairness notion; similarly, sampling-based methods designed to promote statistical fairness are rarely evaluated for causal fairness. In this work, we evaluate both statistical and causal debiasing methods for gender bias in NLP models, and find that while such methods are effective at reducing bias as measured by the targeted metric, they do not necessarily improve results on other bias metrics. We demonstrate that combinations of statistical and causal debiasing techniques are able to reduce bias measured through both types of metrics.
    
[^108]: 从对比中出现的快速方法：基于提示的学习中的有效和隐蔽干净标签攻击

    Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning

    [https://arxiv.org/abs/2404.00461](https://arxiv.org/abs/2404.00461)

    基于提示的学习中的干净标签攻击通过特定提示作为触发器，实现成功的同时确保正确标记有毒样本，但仍面临误激活问题和挑战，需要更高比例的毒害。

    

    Prompt-based learning范式表现出卓越的效力，可以提升预训练语言模型（PLMs）在少样本情况下的适应能力。然而，这种学习范式已被证明容易受到后门攻击的影响。当前的干净标签攻击，利用特定提示作为触发器，可以在不需要外部触发器的情况下成功，并确保对有毒样本的正确标记，相比有毒标签攻击更具隐蔽性，但另一方面，它面临着严重的误激活问题，并提出了更大的挑战，需要更高比例的毒害。通过传统的负数据增强方法，我们发现在干净标签设置中在效力和隐蔽性之间取得平衡是具有挑战性的。在解决这一问题时，我们受到后门充当快捷方式的观念的启发，并假设这一快捷方式源于t。

    arXiv:2404.00461v1 Announce Type: cross  Abstract: Prompt-based learning paradigm has demonstrated remarkable efficacy in enhancing the adaptability of pretrained language models (PLMs), particularly in few-shot scenarios. However, this learning paradigm has been shown to be vulnerable to backdoor attacks. The current clean-label attack, employing a specific prompt as a trigger, can achieve success without the need for external triggers and ensure correct labeling of poisoned samples, which is more stealthy compared to the poisoned-label attack, but on the other hand, it faces significant issues with false activations and poses greater challenges, necessitating a higher rate of poisoning. Using conventional negative data augmentation methods, we discovered that it is challenging to trade off between effectiveness and stealthiness in a clean-label setting. In addressing this issue, we are inspired by the notion that a backdoor acts as a shortcut and posit that this shortcut stems from t
    
[^109]: NumeroLogic：增强LLMs数字推理的数字编码

    NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning

    [https://arxiv.org/abs/2404.00459](https://arxiv.org/abs/2404.00459)

    NumeroLogic提出了一种新的数字表示方法，通过在每个数字前包含数字的位数计数，为语言模型的数字推理能力提供了增强，从而改善了生成实际数字之前的推理过程。

    

    论文指出，语言模型在处理数值数据和执行算术运算时面临困难。我们假设这种限制部分归因于非直观的文本数字表示。为了解决这个问题，我们提出了一种简单的调整方法，即在每个数字前包含数字的位数计数。例如，我们建议使用"{2:42}"代替"42"作为新的格式。我们将这种方法称为NumeroLogic，它在数字生成中作为“思维链”提供了额外优势。通过要求模型首先考虑数字的位数，它增强了生成实际数字之前的推理过程。我们使用算术任务来展示NumeroLogic格式的有效性。

    arXiv:2404.00459v1 Announce Type: new  Abstract: Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of "42", we suggest using "{2:42}" as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstr
    
[^110]: 超越一刀切：多领域、多任务框架用于嵌入模型选择

    Beyond One-Size-Fits-All: Multi-Domain, Multi-Task Framework for Embedding Model Selection

    [https://arxiv.org/abs/2404.00458](https://arxiv.org/abs/2404.00458)

    开发了一个多领域、多任务框架，帮助选择最有效的自然语言处理嵌入模型。

    

    这篇立场论文提出了一种系统方法，旨在开发一个框架，帮助选择适用于自然语言处理（NLP）任务的最有效的嵌入模型，解决了专有和开源编码器模型大量增加所带来的挑战。

    arXiv:2404.00458v1 Announce Type: new  Abstract: This position paper proposes a systematic approach towards developing a framework to help select the most effective embedding models for natural language processing (NLP) tasks, addressing the challenge posed by the proliferation of both proprietary and open-source encoder models.
    
[^111]: MetaIE: 从大型语言模型中提炼元模型，针对所有类型的信息抽取任务

    MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks

    [https://arxiv.org/abs/2404.00457](https://arxiv.org/abs/2404.00457)

    提出了MetaIE框架，通过从大型语言模型中进行符号蒸馏，构建一个小型LM作为元模型，实现对所有类型的信息抽取任务的高效适应。

    

    信息抽取（IE）是自然语言处理中的一个基础领域，在这个领域中，即使使用了具有上下文示例的大型语言模型（LLMs），也无法击败在非常小的IE数据集上调整了的小型LM。我们观察到，诸如命名实体识别和关系抽取等IE任务都集中在提取重要信息，这可以被形式化为标签到跨度的匹配。在本文中，我们提出了一个新颖的框架MetaIE，通过学习提取“重要信息”（即IE的元理解），来构建一个小型LM作为元模型，从而使得这个元模型可以有效且高效地适应各种IE任务。具体来说，MetaIE通过符号蒸馏从LLM中获取小型LM，遵循标签到跨度的方案。我们通过从语言模型预训练数据集（例如，我们的实现中的OpenWebText）中对句子进行采样构建蒸馏数据集，并提示一个LLM来识别...

    arXiv:2404.00457v1 Announce Type: new  Abstract: Information extraction (IE) is a fundamental area in natural language processing where prompting large language models (LLMs), even with in-context examples, cannot defeat small LMs tuned on very small IE datasets. We observe that IE tasks, such as named entity recognition and relation extraction, all focus on extracting important information, which can be formalized as a label-to-span matching. In this paper, we propose a novel framework MetaIE to build a small LM as meta-model by learning to extract "important information", i.e., the meta-understanding of IE, so that this meta-model can be adapted to all kind of IE tasks effectively and efficiently. Specifically, MetaIE obtains the small LM via a symbolic distillation from an LLM following the label-to-span scheme. We construct the distillation dataset via sampling sentences from language model pre-training datasets (e.g., OpenWebText in our implementation) and prompting an LLM to iden
    
[^112]: 规划和编辑检索以增强工具学习

    Planning and Editing What You Retrieve for Enhanced Tool Learning

    [https://arxiv.org/abs/2404.00450](https://arxiv.org/abs/2404.00450)

    该论文提出了一种新颖的模型，结合了“规划与检索”和“编辑与确认”范式，通过神经检索模块和LLM-based查询规划器提高了工具利用的效果。

    

    最近在将外部工具与大型语言模型（LLMs）集成方面取得的进展打开了新的领域，应用范围涵盖数学推理、代码生成器和智能助手。然而，现有方法依赖简单的一次性检索策略，无法有效准确地筛选相关工具。本文介绍了一种新颖的“规划与检索（P&R）”和“编辑与确认（E&G）”范式的模型，包括了神经检索模块和基于LLM的查询规划器，以增强工具利用的效果。

    arXiv:2404.00450v1 Announce Type: new  Abstract: Recent advancements in integrating external tools with Large Language Models (LLMs) have opened new frontiers, with applications in mathematical reasoning, code generators, and smart assistants. However, existing methods, relying on simple one-time retrieval strategies, fall short on effectively and accurately shortlisting relevant tools. This paper introduces a novel \modelname (\modelmeaning) approach, encompassing ``Plan-and-Retrieve (P\&R)'' and ``Edit-and-Ground (E\&G)'' paradigms. The P\&R paradigm consists of a neural retrieval module for shortlisting relevant tools and an LLM-based query planner that decomposes complex queries into actionable tasks, enhancing the effectiveness of tool utilization. The E\&G paradigm utilizes LLMs to enrich tool descriptions based on user scenarios, bridging the gap between user queries and tool functionalities. Experiment results demonstrate that these paradigms significantly improve the recall an
    
[^113]: DOCMASTER: 一个统一的平台，用于文档问答中的注释、训练和推断

    DOCMASTER: A Unified Platform for Annotation, Training, & Inference in Document Question-Answering

    [https://arxiv.org/abs/2404.00439](https://arxiv.org/abs/2404.00439)

    DOCMASTER是一个统一平台，旨在为文档问答提供注释、训练和推断服务，支持用户在PDF文档中输入问题并突出显示文本段作为答案，同时保护隐私。

    

    将自然语言处理模型应用于PDF文档对于各种业务应用至关重要，但在企业中训练此类模型的挑战仍然存在。这些挑战包括使用PDF格式的复杂性，需要解析文本和布局信息以整理训练数据，以及缺乏保护隐私的注释工具。本文介绍了DOCMASTER，一个统一的平台，用于注释PDF文档、模型训练和推断，专为文档问答而设计。注释界面使用户能够输入问题并在PDF文件中突出显示文本段作为答案，同时保存布局信息和文本段。此外，DOCMASTER支持最先进的布局感知和文本模型，用于全面的训练目的。重要的是，由于注释、训练和推断在设备上进行，因此还保护了隐私。

    arXiv:2404.00439v1 Announce Type: new  Abstract: The application of natural language processing models to PDF documents is pivotal for various business applications yet the challenge of training models for this purpose persists in businesses due to specific hurdles. These include the complexity of working with PDF formats that necessitate parsing text and layout information for curating training data and the lack of privacy-preserving annotation tools. This paper introduces DOCMASTER, a unified platform designed for annotating PDF documents, model training, and inference, tailored to document question-answering. The annotation interface enables users to input questions and highlight text spans within the PDF file as answers, saving layout information and text spans accordingly. Furthermore, DOCMASTER supports both state-of-the-art layout-aware and text models for comprehensive training purposes. Importantly, as annotations, training, and inference occur on-device, it also safeguards pr
    
[^114]: 利用树估计器自动解释西班牙法律判决在依赖司法管辖的法律类别中的分类

    Automatic explanation of the classification of Spanish legal judgments in jurisdiction-dependent law categories with tree estimators

    [https://arxiv.org/abs/2404.00437](https://arxiv.org/abs/2404.00437)

    本研究结合自然语言处理和机器学习，提出了一种可解释的法律文本分类系统，使模型的决策变得可理解给最终用户

    

    自动法律文本分类系统已经被提出在文献中，以解决知识从判决中提取并检测其方面。然而，即使它们的模型是可解释的，大多数这些系统都是黑盒的。这可能引发对它们可信度的担忧。因此，本研究提出了一个系统，结合自然语言处理（NLP）和机器学习（ML），以可解释的方式对法律文本进行分类。我们分析了决策中涉及的特征和树结构的决策路径的阈值分叉值，并以自然语言的方式向用户呈现这些信息。这是第一项关于自动分析法律文本的工作，结合NLP和ML以及可解释的人工智能技术，以使模型的决策自动变得可理解给最终用户。此外，法律专家已经验证了我们的解决方案，这些知识也已

    arXiv:2404.00437v1 Announce Type: cross  Abstract: Automatic legal text classification systems have been proposed in the literature to address knowledge extraction from judgments and detect their aspects. However, most of these systems are black boxes even when their models are interpretable. This may raise concerns about their trustworthiness. Accordingly, this work contributes with a system combining Natural Language Processing (NLP) with Machine Learning (ML) to classify legal texts in an explainable manner. We analyze the features involved in the decision and the threshold bifurcation values of the decision paths of tree structures and present this information to the users in natural language. This is the first work on automatic analysis of legal texts combining NLP and ML along with Explainable Artificial Intelligence techniques to automatically make the models' decisions understandable to end users. Furthermore, legal experts have validated our solution, and this knowledge has al
    
[^115]: 视觉-语言模型是否理解复合名词？

    Do Vision-Language Models Understand Compound Nouns?

    [https://arxiv.org/abs/2404.00419](https://arxiv.org/abs/2404.00419)

    该研究策划了一个名为Compun的新基准测试，通过文本提示和图像选择任务评估VLMs在理解复合名词方面的表现，并展现了CLIP在某些类型的复合名词理解上存在局限性。

    

    开放词汇的视觉-语言模型（VLMs）如CLIP，使用对比损失进行训练，已经成为一种有前景的新文本到图像检索范式。然而，VLMs是否像理解名词（如实验室）一样理解复合名词（CNs）（如实验室外套）？我们策划了一个名为Compun的新基准测试，其中包含400个独特且常用的CNs，以评估VLMs在解释CNs方面的有效性。Compun基准测试挑战VLM进行文本到图像检索，给定一段包含CN的文本提示，任务是从一对显示构成CN的名词的干扰图像中选择正确显示该CN的图像。接下来，我们进行深入分析，突出CLIP对某些类型的CN的理解能力有限。最后，我们提出了一个超越CLIP类模型广泛使用的手写模板的替代框架。我们使用大型语言模型生成多个

    arXiv:2404.00419v1 Announce Type: cross  Abstract: Open-vocabulary vision-language models (VLMs) like CLIP, trained using contrastive loss, have emerged as a promising new paradigm for text-to-image retrieval. However, do VLMs understand compound nouns (CNs) (e.g., lab coat) as well as they understand nouns (e.g., lab)? We curate Compun, a novel benchmark with 400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in interpreting CNs. The Compun benchmark challenges a VLM for text-to-image retrieval where, given a text prompt with a CN, the task is to select the correct image that shows the CN among a pair of distractor images that show the constituent nouns that make up the CN. Next, we perform an in-depth analysis to highlight CLIPs' limited understanding of certain types of CNs. Finally, we present an alternative framework that moves beyond hand-written templates for text prompts widely used by CLIP-like models. We employ a Large Language Model to generate multiple 
    
[^116]: CoDa:基于约束生成的数据增强技术用于低资源NLP

    CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP

    [https://arxiv.org/abs/2404.00415](https://arxiv.org/abs/2404.00415)

    CoDa是一种针对低资源NLP提出的基于约束生成的数据增强技术，通过提示大型语言模型生成符合一组约束的文本，有效提升数据增强效果，避免模型偏向少量训练数据。

    

    我们提出了CoDa（基于约束生成的数据增强），这是一种可控、有效且无需训练的数据增强技术，适用于低资源（数据稀缺）NLP。我们的方法基于提示现有的大型语言模型（LLMs）遵循指令来生成满足一组约束的文本。具体而言，我们从低资源数据集中的每个实例中提取一组简单的约束，将其表达为提示语，以促使LLM生成新颖且多样化的训练实例。我们的研究结果显示，符合下游数据集中简单约束的合成数据可以作为高效的增强，而CoDa可以在不需要复杂解码时约束生成技术或使用复杂算法微调的情况下实现这一点，这样可以避免模型偏向于少量训练实例。此外，CoDa 是首个提供用户明确控制的框架。

    arXiv:2404.00415v1 Announce Type: new  Abstract: We present CoDa (Constrained Generation based Data Augmentation), a controllable, effective, and training-free data augmentation technique for low-resource (data-scarce) NLP. Our approach is based on prompting off-the-shelf instruction-following Large Language Models (LLMs) for generating text that satisfies a set of constraints. Precisely, we extract a set of simple constraints from every instance in the low-resource dataset and verbalize them to prompt an LLM to generate novel and diverse training instances. Our findings reveal that synthetic data that follows simple constraints in the downstream dataset act as highly effective augmentations, and CoDa can achieve this without intricate decoding-time constrained generation techniques or fine-tuning with complex algorithms that eventually make the model biased toward the small number of training instances. Additionally, CoDa is the first framework that provides users explicit control ove
    
[^117]: TACO -- Twitter Arguments from COnversations

    TACO -- Twitter Arguments from COnversations

    [https://arxiv.org/abs/2404.00406](https://arxiv.org/abs/2404.00406)

    TACO是第一个利用1,814条推文构建的Twitter Arguments数据集，涵盖200场完整对话，六个主题，具有0.718的Krippendorff's alpha一致性，并提供了一个注释框架来定义和识别论点结构要素。

    

    Twitter已经成为全球参与在线对话的中心，也成为各个学科研究语料库的重要来源，这些学科已经意识到其用户生成的内容的重要性。论点挖掘是处理和理解在线话语的重要分析任务。具体来说，它旨在识别论点的结构要素，表示为信息和推理。然而，这些要素并不是静态的，可能需要在所在对话中设置上下文，然而缺乏解决Twitter上这一动态方面的数据和注释框架。我们贡献了TACO，这是第一个利用1,814条涵盖200场完整对话、涵盖六个异质主题的推文的Twitter Arguments数据集，并在六名专家之间以0.718的Krippendorff's alpha达成一致。其次，我们提供了我们的注释框架，结合了来自剑桥词典的定义，以定义和识别。

    arXiv:2404.00406v1 Announce Type: cross  Abstract: Twitter has emerged as a global hub for engaging in online conversations and as a research corpus for various disciplines that have recognized the significance of its user-generated content. Argument mining is an important analytical task for processing and understanding online discourse. Specifically, it aims to identify the structural elements of arguments, denoted as information and inference. These elements, however, are not static and may require context within the conversation they are in, yet there is a lack of data and annotation frameworks addressing this dynamic aspect on Twitter. We contribute TACO, the first dataset of Twitter Arguments utilizing 1,814 tweets covering 200 entire conversations spanning six heterogeneous topics annotated with an agreement of 0.718 Krippendorff's alpha among six experts. Second, we provide our annotation framework, incorporating definitions from the Cambridge Dictionary, to define and identify
    
[^118]: UniMEEC:走向统一的多模情绪识别与情绪因果

    UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion Cause

    [https://arxiv.org/abs/2404.00403](https://arxiv.org/abs/2404.00403)

    UniMEEC提出了一个统一的多模情绪识别和情绪-原因分析框架，将MERC和MECPE重新定义为两个掩码预测问题，以增强情绪和原因之间的交互作用。

    

    最近，对话中的多模情绪识别（MERC）和多模情绪-原因对提取（MECPE）引起了广泛关注。情绪是情感或感受的表达；对特定事件、想法或情况的响应被称为情绪原因。它们如同一枚硬币的两面，共同描述了人类行为和意图。然而，大多数现有作品将MERC和MECPE视为独立任务，这可能导致在整合情绪和原因到现实应用中存在潜在挑战。在本文中，我们提出了一个统一的多模情绪识别和情绪-原因分析框架（UniMEEC），以探索情绪和情绪原因之间的因果关系和互补性。具体来说，UniMEEC将MERC和MECPE任务重新定义为两个掩码预测问题，增强了情绪和原因之间的交互作用。与此同时，UniMEEC在各模态之间共享迅速学习以促进

    arXiv:2404.00403v1 Announce Type: new  Abstract: Multimodal emotion recognition in conversation (MERC) and multimodal emotion-cause pair extraction (MECPE) has recently garnered significant attention. Emotions are the expression of affect or feelings; responses to specific events, thoughts, or situations are known as emotion causes. Both are like two sides of a coin, collectively describing human behaviors and intents. However, most existing works treat MERC and MECPE as separate tasks, which may result in potential challenges in integrating emotion and cause in real-world applications. In this paper, we propose a Unified Multimodal Emotion recognition and Emotion-Cause analysis framework (UniMEEC) to explore the causality and complementarity between emotion and emotion cause. Concretely, UniMEEC reformulates the MERC and MECPE tasks as two mask prediction problems, enhancing the interaction between emotion and cause. Meanwhile, UniMEEC shares the prompt learning among modalities for p
    
[^119]: 科学表格问答模型的鲁棒性有多强？一项使用定制数据集的研究

    How Robust are the Tabular QA Models for Scientific Tables? A Study using Customized Dataset

    [https://arxiv.org/abs/2404.00401](https://arxiv.org/abs/2404.00401)

    通过提出新的数据集"SciTabQA"，研究了现有科学混合表格数据上最先进Tabular QA模型的鲁棒性，评估了其在解释科学表格和文本方面的能力。

    

    在混合科学表格和文本数据上进行问答(QA)涉及科学信息，并依赖于复杂的数值推理。近年来，虽然表格QA取得了快速进展，但由于缺乏任何基准数据集，对它们在科学信息上的鲁棒性缺乏理解。为了研究现有最先进的QA模型在科学混合表格数据上的鲁棒性，我们提出了一个新数据集“SciTabQA”，其中包含来自科学表格及其描述的822个问答对。借助这个数据集，我们评估了基于最先进Tabular QA模型的能力，即(i)利用需要结构化数据(表格)和非结构化数据(文本)的异构信息以及(ii)执行复杂的科学推理任务。本质上，我们检查模型解释科学表格和文本的能力。我们的实验表明，“SciTabQA”是一项创新的工作。

    arXiv:2404.00401v1 Announce Type: new  Abstract: Question-answering (QA) on hybrid scientific tabular and textual data deals with scientific information, and relies on complex numerical reasoning. In recent years, while tabular QA has seen rapid progress, understanding their robustness on scientific information is lacking due to absence of any benchmark dataset. To investigate the robustness of the existing state-of-the-art QA models on scientific hybrid tabular data, we propose a new dataset, "SciTabQA", consisting of 822 question-answer pairs from scientific tables and their descriptions. With the help of this dataset, we assess the state-of-the-art Tabular QA models based on their ability (i) to use heterogeneous information requiring both structured data (table) and unstructured data (text) and (ii) to perform complex scientific reasoning tasks. In essence, we check the capability of the models to interpret scientific tables and text. Our experiments show that "SciTabQA" is an inno
    
[^120]: Aurora-M: 根据美国行政命令，第一个开源的多语言语言模型进行了红队测试

    Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order

    [https://arxiv.org/abs/2404.00399](https://arxiv.org/abs/2404.00399)

    Aurora-M 是第一个根据美国行政命令进行红队测试的开源多语言模型，通过在英语、芬兰语、印地语、日语、越南语和代码上训练，不断预训练，包括了人工审核的安全说明，总训练 token 数超过 2 万亿个

    

    预训练语言模型支持多种人工智能应用，但是它们在训练时高昂的计算成本限制了可访问性。BLOOM 和 StarCoder 等倡议旨在使预训练模型对于协作社区开发更具民主性。然而，目前存在的模型面临一些挑战：多语言能力有限，持续的预训练会导致灾难性遗忘，而从头开始预训练又具有高昂的计算成本，并且需要遵守人工智能安全和发展法律。本文介绍了 Aurora-M，一个包含 15B 参数的多语言开源模型，训练语言包括英语、芬兰语、印地语、日语、越南语和代码。Aurora-M 不断从 StarCoderPlus 上预训练，额外训练了 4350 亿个 token，总训练 token 数超过了 2 万亿个。它是第一个在人工审核的安全说明上进行微调的开源多语言模型，使其开发与传统

    arXiv:2404.00399v1 Announce Type: cross  Abstract: Pretrained language models underpin several AI applications, but their high computational cost for training limits accessibility. Initiatives such as BLOOM and StarCoder aim to democratize access to pretrained models for collaborative community development. However, such existing models face challenges: limited multilingual capabilities, continual pretraining causing catastrophic forgetting, whereas pretraining from scratch is computationally expensive, and compliance with AI safety and development laws. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventio
    
[^121]: 对神经机器翻译中的BPE词汇修剪进行分析

    An Analysis of BPE Vocabulary Trimming in Neural Machine Translation

    [https://arxiv.org/abs/2404.00397](https://arxiv.org/abs/2404.00397)

    在神经机器翻译中，对BPE词汇进行修剪不能提高性能，甚至可能导致严重的性能下降。

    

    我们探讨了在字节对编码子词标记化中的阈值词汇修剪，这是一种后处理步骤，用其组成子词替换稀有子词的技术。这种技术在流行的标记化库中可用，但尚未经过严格的科学检验。虽然在机器翻译实现中建议删除稀有子词作为最佳实践，既可减小模型大小，又可通过提高鲁棒性来改善模型性能，但我们的实验表明，在大量的超参数设置空间中，词汇修剪未能提高性能，甚至往往导致严重的性能下降。

    arXiv:2404.00397v1 Announce Type: new  Abstract: We explore threshold vocabulary trimming in Byte-Pair Encoding subword tokenization, a postprocessing step that replaces rare subwords with their component subwords. The technique is available in popular tokenization libraries but has not been subjected to rigorous scientific scrutiny. While the removal of rare subwords is suggested as best practice in machine translation implementations, both as a means to reduce model size and for improving model performance through robustness, our experiments indicate that, across a large space of hyperparameter settings, vocabulary trimming fails to improve performance, and is even prone to incurring heavy degradation.
    
[^122]: 《杰森斯队在FinNLP 2024:基于Transformer模型，探索新闻文章的ESG影响理解》

    Jetsons at FinNLP 2024: Towards Understanding the ESG Impact of a News Article using Transformer-based Models

    [https://arxiv.org/abs/2404.00386](https://arxiv.org/abs/2404.00386)

    本文描述了Jetsons团队在多语言ESG影响时长推断共享任务中探索的不同方法，包括使用XLM-RoBERTa和DeBERTa-v3对影响时长进行预测，并在英语语言中取得领先地位。

    

    本文描述了杰森斯队针对多语言ESG影响时长推断（ML-ESG-3）共享任务所探索的不同方法。该共享任务专注于预测新闻文章的ESG影响时长和类型。共享任务的数据集包括2,059篇英语、法语、韩语和日语新闻标题和文章。针对影响时长分类任务，我们采用自定义微调策略对XLM-RoBERTa和DeBERTa-v3进行微调，并分别使用自训练和仅使用英语翻译。这些模型分别在韩语和日语的排行榜上排名第一，并在英语语言的集成中排名第一。针对影响类型分类任务，我们的XLM-RoBERTa模型通过自定义微调策略在英语语言中排名第一。

    arXiv:2404.00386v1 Announce Type: new  Abstract: In this paper, we describe the different approaches explored by the Jetsons team for the Multi-Lingual ESG Impact Duration Inference (ML-ESG-3) shared task. The shared task focuses on predicting the duration and type of the ESG impact of a news article. The shared task dataset consists of 2,059 news titles and articles in English, French, Korean, and Japanese languages. For the impact duration classification task, we fine-tuned XLM-RoBERTa with a custom fine-tuning strategy and using self-training and DeBERTa-v3 using only English translations. These models individually ranked first on the leaderboard for Korean and Japanese and in an ensemble for the English language, respectively. For the impact type classification task, our XLM-RoBERTa model fine-tuned using a custom fine-tuning strategy ranked first for the English language.
    
[^123]: 小型语言模型从医学教材中学习增强推理能力

    Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks

    [https://arxiv.org/abs/2404.00376](https://arxiv.org/abs/2404.00376)

    通过从医学教材提取的推理路径和多样化的遵循指令数据集，我们引入了具有70亿参数的Meerkat-7B医学人工智能系统，成功解决了商用大型语言模型在医学任务上隐私和推理能力不足的问题，取得了优于先前7B模型的显著成果。

    

    最近商用大型语言模型（LM）在医学任务中取得了有希望的成果，但其闭源性质引发了重要的隐私和安全问题，阻碍了它们在医学领域的广泛应用。针对这一问题，我们引入了Meerkat-7B，一个包含70亿参数的新型医学人工智能系统。Meerkat-7B使用我们新的合成数据集进行训练，该数据集包含从18本医学教材中获取的高质量思维链推理路径，以及多样的遵循指令数据集。我们的系统在七个医学基准测试中取得了显著的准确性，超过了GPT-3.5 13.1%，同时也优于以往最好的7B模型MediTron-7B和BioMistral-7B分别达到了13.4%和9.8%。

    arXiv:2404.00376v1 Announce Type: new  Abstract: While recent advancements in commercial large language models (LM) have shown promising results in medical tasks, their closed-source nature poses significant privacy and security concerns, hindering their widespread use in the medical field. Despite efforts to create open-source models, their limited parameters often result in insufficient multi-step reasoning capabilities required for solving complex medical problems. To address this, we introduce Meerkat-7B, a novel medical AI system with 7 billion parameters. Meerkat-7B was trained using our new synthetic dataset consisting of high-quality chain-of-thought reasoning paths sourced from 18 medical textbooks, along with diverse instruction-following datasets. Our system achieved remarkable accuracy across seven medical benchmarks, surpassing GPT-3.5 by 13.1%, as well as outperforming the previous best 7B models such as MediTron-7B and BioMistral-7B by 13.4% and 9.8%, respectively. Notab
    
[^124]: 低资源开放域对话生成的可控多样化数据增强与大型语言模型

    Controllable and Diverse Data Augmentation with Large Language Model for Low-Resource Open-Domain Dialogue Generation

    [https://arxiv.org/abs/2404.00361](https://arxiv.org/abs/2404.00361)

    提出了一种基于总结的对话增强方法SDA，通过使用对话总结增强了LLM的可控性，实现高质量和多样化的对话数据生成。

    

    数据增强（DA）对于减轻低资源开放域对话生成中模型训练不稳定和过拟合问题至关重要。然而，传统的DA方法通常忽略了语义数据多样性，限制了整体质量。最近，大型语言模型（LLM）已被用于DA以生成多样化的对话。然而，它们受到限制的可控性，并且倾向于生成与种子对话相比具有分布偏移的对话。为了最大化增强多样性并解决可控性问题，我们提出了基于总结的LLM（SDA）的对话增强。我们的方法通过使用对话总结作为规划工具增强了LLM的可控性。基于总结，SDA可以生成高质量且多样化的对话数据，即使只有一个小的种子数据集。为了评估开放域对话的数据增强方法的有效性，我们设计了一个聚

    arXiv:2404.00361v1 Announce Type: new  Abstract: Data augmentation (DA) is crucial to mitigate model training instability and over-fitting problems in low-resource open-domain dialogue generation. However, traditional DA methods often neglect semantic data diversity, restricting the overall quality. Recently, large language models (LLM) have been used for DA to generate diversified dialogues. However, they have limited controllability and tend to generate dialogues with a distribution shift compared to the seed dialogues. To maximize the augmentation diversity and address the controllability problem, we propose \textbf{S}ummary-based \textbf{D}ialogue \textbf{A}ugmentation with LLM (SDA). Our approach enhances the controllability of LLM by using dialogue summaries as a planning tool. Based on summaries, SDA can generate high-quality and diverse dialogue data even with a small seed dataset. To evaluate the efficacy of data augmentation methods for open-domain dialogue, we designed a clu
    
[^125]: LLMs能够掌握数学吗？在数学堆栈交换上研究大型语言模型

    Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange

    [https://arxiv.org/abs/2404.00344](https://arxiv.org/abs/2404.00344)

    LLMs在数学领域表现出色，其中GPT-4在Math Stack Exchange上回答数学问题的表现最佳。

    

    大型语言模型（LLMs）在各种自然语言任务中展示出异常能力，通常表现出超越人类的性能。尽管取得了这些进展，数学领域提出了一种独特的挑战，主要是因为其专门的结构和所需的精度。本研究采用了两步方法来调查LLMs在回答数学问题方面的熟练程度。首先，我们采用在数学问题-答案基准测试中表现最佳的LLMs来回答Math Stack Exchange（MSE）中的78个问题。其次，对表现最佳的LLM进行案例分析，重点关注其答案的质量和准确性。我们发现，在为回答数学问题进行微调的现有LLMs中，GPT-4表现最佳（nDCG为0.48，P@10为0.37），在数学问题上表现优异。

    arXiv:2404.00344v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities in various natural language tasks, often achieving performances that surpass those of humans. Despite these advancements, the domain of mathematics presents a distinctive challenge, primarily due to its specialized structure and the precision it demands. In this study, we adopted a two-step approach for investigating the proficiency of LLMs in answering mathematical questions. First, we employ the most effective LLMs, as identified by their performance on math question-answer benchmarks, to generate answers to 78 questions from the Math Stack Exchange (MSE). Second, a case analysis is conducted on the LLM that showed the highest performance, focusing on the quality and accuracy of its answers through manual evaluation. We found that GPT-4 performs best (nDCG of 0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering mathematics questions and outperfor
    
[^126]: 针对仇恨言论检测的自然语言处理数据增强的综合研究：传统方法、BERT和LLM

    A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs

    [https://arxiv.org/abs/2404.00303](https://arxiv.org/abs/2404.00303)

    本研究从传统方法到当代实践，探讨了适用于仇恨言论检测的数据增强方法，提出了优化利用BERT-based编码器模型的思路，并揭示了相较于之前的同义词替换方法存在的重要限制。

    

    针对自然语言处理领域中数据增强引起的兴趣激增，驱动力来自于需要解决仇恨言论领域所带来的挑战、社交媒体词汇的动态性，以及大规模神经网络对大量训练数据的需求。然而，数据增强中普遍使用的词汇替换引发了关注，因为它可能无意中改变预期含义，从而影响监督机器学习模型的有效性。为寻求合适的数据增强方法，本研究探索了传统的传统方法和当代实践，如大型语言模型（LLM），包括GPT在仇恨言论检测中的应用。此外，我们提出了一种优化利用基于BERT的编码器模型，搭配上上文余弦相似性过滤，揭示了之前同义词替换方法的重大局限性。我们的比较分析涵盖了五种流行的增强方法。

    arXiv:2404.00303v1 Announce Type: new  Abstract: The surge of interest in data augmentation within the realm of NLP has been driven by the need to address challenges posed by hate speech domains, the dynamic nature of social media vocabulary, and the demands for large-scale neural networks requiring extensive training data. However, the prevalent use of lexical substitution in data augmentation has raised concerns, as it may inadvertently alter the intended meaning, thereby impacting the efficacy of supervised machine learning models. In pursuit of suitable data augmentation methods, this study explores both established legacy approaches and contemporary practices such as Large Language Models (LLM), including GPT in Hate Speech detection. Additionally, we propose an optimized utilization of BERT-based encoder models with contextual cosine similarity filtration, exposing significant limitations in prior synonym substitution methods. Our comparative analysis encompasses five popular aug
    
[^127]: TRABSA：使用基于注意力的BiLSTM和Twitter-RoBERTa进行可解释的推文情感分析

    TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa

    [https://arxiv.org/abs/2404.00297](https://arxiv.org/abs/2404.00297)

    TRABSA是一个集成了transformer架构、注意力机制和BiLSTM网络的混合框架，利用RoBERTa在大量推特上训练，填补了情感分析领域的差距，实现了94%的准确性和显著的性能提升。

    

    情感分析对于理解公众舆论和消费者行为至关重要。现有模型面临着语言多样性、泛化能力和可解释性方面的挑战。我们提出了TRABSA，这是一个集成了基于transformer的架构、注意力机制和BiLSTM网络的混合框架，旨在解决这些挑战。利用在124M条推文上训练的RoBERTa，我们填补了情感分析基准测试中的差距，确保了最先进的准确性。通过将来自32个国家和美国各州的推文与数据集相结合，我们比较了六种词嵌入技术和三种基于词典的标注技术，并选择了最佳技术以实现最佳情感分析效果。TRABSA以94%的准确性和显著的精确度、召回率和F1得分增益，胜过了传统的机器学习和深度学习模型。在不同数据集上的评估显示了一致的优越性和泛化能力。SHAP和LIME分析提高了可解释性，增强了信心。

    arXiv:2404.00297v1 Announce Type: new  Abstract: Sentiment analysis is crucial for understanding public opinion and consumer behavior. Existing models face challenges with linguistic diversity, generalizability, and explainability. We propose TRABSA, a hybrid framework integrating transformer-based architectures, attention mechanisms, and BiLSTM networks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge gaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy. Augmenting datasets with tweets from 32 countries and US states, we compare six word-embedding techniques and three lexicon-based labeling techniques, selecting the best for optimal sentiment analysis. TRABSA outperforms traditional ML and deep learning models with 94% accuracy and significant precision, recall, and F1-score gains. Evaluation across diverse datasets demonstrates consistent superiority and generalizability. SHAP and LIME analyses enhance interpretability, improving confidence i
    
[^128]: 一种语言间遗传关系的似然比检验

    A Likelihood Ratio Test of Genetic Relationship among Languages

    [https://arxiv.org/abs/2404.00284](https://arxiv.org/abs/2404.00284)

    提出一种受分子系统发生学启发的似然比检验方法，用于确定语言间是否存在遗传关系。

    

    一组语言之间的词汇相似性表明这些语言可能存在遗传关系，即它们可能源自共同的祖语。然而，这种相似性可能是偶然产生的，并且不总是意味着存在潜在的遗传关系。过去出现过许多基于单词列表和单词相似性度量的置换检验以确定此类关系的统计显著性。我们证明，尽管现有的测试可能对于双边比较（即语言对）效果良好，但当应用于语言组或语言家族时，它们要么由于设计而不可行，要么易于产生假阳性。因此，受到分子系统发生学的启发，我们提出了一种基于对齐单词列表中不变字符位点比例的似然比检验，以确定给定语言是否相关。

    arXiv:2404.00284v1 Announce Type: new  Abstract: Lexical resemblances among a group of languages indicate that the languages could be genetically related, i.e., they could have descended from a common ancestral language. However, such resemblances can arise by chance and, hence, need not always imply an underlying genetic relationship. Many tests of significance based on permutation of wordlists and word similarity measures appeared in the past to determine the statistical significance of such relationships. We demonstrate that although existing tests may work well for bilateral comparisons, i.e., on pairs of languages, they are either infeasible by design or are prone to yield false positives when applied to groups of languages or language families. To this end, inspired by molecular phylogenetics, we propose a likelihood ratio test to determine if given languages are related based on the proportion of invariant character sites in the aligned wordlists applied during tree inference. F
    
[^129]: 基于大型语言模型增强强化学习的调查:概念、分类和方法

    Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods

    [https://arxiv.org/abs/2404.00282](https://arxiv.org/abs/2404.00282)

    大型语言模型在强化学习中具有潜在优势，通过结构化分类和角色分析，为未来研究提供指导。

    

    随着大规模语言模型(LLMs)拥有广泛的预训练知识和高级通用能力，它们在增强学习方面如多任务学习、样本效率和任务规划等方面展现出潜力。本调查综述了现有$\textit{LLM增强RL}$文献，总结了其与传统RL方法的特征，旨在澄清研究范围和未来研究方向。利用经典的Agent-环境交互范例，我们提出了一个结构化的分类法，系统地将LLMs在RL中的功能分类，包括四种角色：信息处理器、奖励设计者、决策者和生成器。此外，针对每个角色，我们总结了方法论，分析了缓解的特定RL挑战，并提供了未来方向的见解。最后，潜在应用、前景

    arXiv:2404.00282v1 Announce Type: cross  Abstract: With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and task planning. In this survey, we provide a comprehensive review of the existing literature in $\textit{LLM-enhanced RL}$ and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. Additionally, for each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, potential applications, prospecti
    
[^130]: 保密者：LLM对个人特征语言标记的影响

    Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits

    [https://arxiv.org/abs/2404.00267](https://arxiv.org/abs/2404.00267)

    LLMs对作者的语言模式的影响略微降低其个人特征的预测能力，但显著变化不太频繁。

    

    先前的研究已经确定了个体语言使用与其个人特征之间的关联；我们的语言模式揭示了关于我们个性、情绪状态和信念的信息。然而，随着大型语言模型(LLMs)在日常写作中作为写作助手被越来越广泛地采用，一个关键问题出现了：当LLMs参与写作过程时，作者的语言模式是否仍然能预测其个人特征？我们研究了LLMs对人口统计特征和心理特征的语言标记的影响，具体检查了三个LLMs - GPT3.5、Llama 2和Gemini - 在六种不同特征上：性别、年龄、政治立场、个性、移情能力和道德。我们的发现表明，虽然使用LLMs略微降低了语言模式对作者个人特征的预测能力，但显著变化不太频繁，LLMs的使用并不

    arXiv:2404.00267v1 Announce Type: new  Abstract: Prior research has established associations between individuals' language usage and their personal traits; our linguistic patterns reveal information about our personalities, emotional states, and beliefs. However, with the increasing adoption of Large Language Models (LLMs) as writing assistants in everyday writing, a critical question emerges: are authors' linguistic patterns still predictive of their personal traits when LLMs are involved in the writing process? We investigate the impact of LLMs on the linguistic markers of demographic and psychological traits, specifically examining three LLMs - GPT3.5, Llama 2, and Gemini - across six different traits: gender, age, political affiliation, personality, empathy, and morality. Our findings indicate that although the use of LLMs slightly reduces the predictive power of linguistic patterns over authors' personal traits, the significant changes are infrequent, and the use of LLMs does not 
    
[^131]: 将数据集提炼为语言模型，用于文本级数据集蒸馏

    DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation

    [https://arxiv.org/abs/2404.00264](https://arxiv.org/abs/2404.00264)

    提出了一种名为DiLM的文本数据集蒸馏方法，通过训练语言模型生成文本数据作为合成训练样本，解决了嵌入级别蒸馏数据集无法用于训练其他模型的问题。

    

    数据集蒸馏旨在通过创建少量信息丰富的合成样本来压缩训练数据集，从而使得在其上训练的神经网络的性能能够与在原始训练数据集上训练的网络一样好。当前的文本数据集蒸馏方法将每个合成样本创建为词嵌入序列而不是文本，以应用基于梯度的优化；然而，这种嵌入级别的蒸馏数据集无法用于训练其他模型，其词嵌入权重不同于用于蒸馏的模型。为解决这一问题，本文提出了一种新颖的文本数据集蒸馏方法，称为Distilling dataset into Language Model（DiLM），该方法训练语言模型以生成信息丰富的文本数据作为合成训练样本，而不是直接优化合成样本。我们在各种文本分类数据集上评估了DiLM，并展示了从DiLM 中蒸馏得到的合成数据集的优秀表现。

    arXiv:2404.00264v1 Announce Type: new  Abstract: Dataset distillation aims to compress a training dataset by creating a small number of informative synthetic samples such that neural networks trained on them perform as well as those trained on the original training dataset. Current text dataset distillation methods create each synthetic sample as a sequence of word embeddings instead of a text to apply gradient-based optimization; however, such embedding-level distilled datasets cannot be used for training other models whose word embedding weights are different from the model used for distillation. To address this issue, we propose a novel text dataset distillation approach, called Distilling dataset into Language Model (DiLM), which trains a language model to generate informative synthetic training samples as text data, instead of directly optimizing synthetic samples. We evaluated DiLM on various text classification datasets and showed that distilled synthetic datasets from DiLM outp
    
[^132]: 你的同事很重要：评估语言模型在方块世界中的协作能力

    Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World

    [https://arxiv.org/abs/2404.00246](https://arxiv.org/abs/2404.00246)

    在一个方块世界环境中，论文评估了大型语言模型的协作能力，通过设计不断增加挑战性的设置来评估不同的协作视角，从独立到更复杂的依赖任务。

    

    与世界自行交互的语言代理在自动化数字任务方面具有巨大潜力。虽然大型语言模型代理在理解和执行文本游戏和网页控制等任务方面取得了进展，但许多现实任务也需要与人类或其他同等角色的LLM协作，这涉及意图理解、任务协调和沟通。为测试LLM协作能力，我们设计了一个方块世界环境，在这个环境中，两个代理，每个代理都有独特的目标和技能，一起建造一个目标结构。为实现目标，他们可以在世界中行动并用自然语言进行沟通。在这个环境下，我们设计了越来越具有挑战性的设置，以评估不同协作视角，从独立的到更复杂的依赖任务。我们进一步采用了中间推理步骤来建模合作伙伴的状态。

    arXiv:2404.00246v1 Announce Type: cross  Abstract: Language agents that interact with the world on their own have great potential for automating digital tasks. While large language model (LLM) agents have made progress in understanding and executing tasks such as textual games and webpage control, many real-world tasks also require collaboration with humans or other LLMs in equal roles, which involves intent understanding, task coordination, and communication. To test LLM's ability to collaborate, we design a blocks-world environment, where two agents, each having unique goals and skills, build a target structure together. To complete the goals, they can act in the world and communicate in natural language. Under this environment, we design increasingly challenging settings to evaluate different collaboration perspectives, from independent to more complex, dependent tasks. We further adopt chain-of-thought prompts that include intermediate reasoning steps to model the partner's state a
    
[^133]: DeFT：带IO意识的Flash Tree-attention用于高效的基于树搜索的LLM推断

    DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference

    [https://arxiv.org/abs/2404.00242](https://arxiv.org/abs/2404.00242)

    DeFT提出了一种带IO意识的树注意力算法，通过在QKV准备和注意力计算阶段实现内存高效的计算，降低内存印记，以解决当前树解码策略和推断系统不适配的问题。

    

    使用树搜索进行解码可以极大地提高基于变压器的大型语言模型（LLMs）的推断质量。根据引导信号，它通过形成LLM输出从根到叶子的最佳路径来提高可控性、推理能力、对齐等。然而，由于计算冗余、内存占用和内存访问，当前的树解码策略及其推断系统互相不适配，导致推断效率低下。为解决这一问题，我们提出了DeFT，一种IO感知树注意力算法，它在两个阶段中保持内存高效的注意力计算，降低内存印记：（1）QKV准备：我们提出了一种KV引导树分裂策略，为GPU的高利用率和尽可能减少GPU全局内存和芯片上共享内存之间的KV缓存的内存读/写; （2）注意力计算...

    arXiv:2404.00242v1 Announce Type: cross  Abstract: Decoding using tree search can greatly enhance the inference quality for transformer-based Large Language Models (LLMs). Depending on the guidance signal, it searches for the best path from root to leaf in the tree by forming LLM outputs to improve controllability, reasoning ability, alignment, et cetera. However, current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference. To address this issue, we propose DeFT, an IO-aware tree attention algorithm that maintains memory-efficient attention calculation with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction of memory reads/writes for the KV cache between GPU global memory and on-chip shared memory as much as possible; (2) Attention Calculati
    
[^134]: 通过大型语言模型增强基于内容的推荐

    Enhancing Content-based Recommendation via Large Language Model

    [https://arxiv.org/abs/2404.00236](https://arxiv.org/abs/2404.00236)

    本文提出了一种名为LoID的语义知识传递方法，旨在提取多方面的语义信息以增强不同领域，并对齐用户/项目ID和内容语义特征空间。

    

    在现实世界的应用中，用户在与不同项目互动时表现出不同的行为，包括隐式的点击/点赞互动以及显式的评论/评价互动。然而，几乎所有的推荐工作都集中在如何通过隐式的点击/点赞互动来描述用户偏好，以找到人们之间的协同。对于基于内容的显式评论/评价互动，一些工作尝试利用它们来挖掘语义知识以增强推荐模型。然而，它们仍然忽视了以下两点：（1）内容语义是普适的世界知识；我们如何提取多方面的语义信息以增强不同领域？（2）用户/项目ID特征是推荐模型的基础要素；我们如何对齐ID和内容语义特征空间？在本文中，我们提出了一种“插件”语义知识传递方法LoID。

    arXiv:2404.00236v1 Announce Type: cross  Abstract: In real-world applications, users express different behaviors when they interact with different items, including implicit click/like interactions, and explicit comments/reviews interactions. Nevertheless, almost all recommender works are focused on how to describe user preferences by the implicit click/like interactions, to find the synergy of people. For the content-based explicit comments/reviews interactions, some works attempt to utilize them to mine the semantic knowledge to enhance recommender models. However, they still neglect the following two points: (1) The content semantic is a universal world knowledge; how do we extract the multi-aspect semantic information to empower different domains? (2) The user/item ID feature is a fundamental element for recommender models; how do we align the ID and content semantic feature space? In this paper, we propose a `plugin' semantic knowledge transferring method \textbf{LoID}, which inclu
    
[^135]: 使用大型语言模型生成基础设施即代码的调查

    A Survey of using Large Language Models for Generating Infrastructure as Code

    [https://arxiv.org/abs/2404.00227](https://arxiv.org/abs/2404.00227)

    使用大型语言模型来自动化基础设施即代码编排工作的可行性进行了研究

    

    基础设施即代码（IaC）是一种革命性的方法，在业内获得了显著的突出地位。 IaC通过使用可机器读取的代码来管理和提供IT基础设施，实现自动化、环境之间的一致性、可再现性、版本控制、减少错误和增强可扩展性。然而，IaC编排通常是一项费时费力的工作，需要专业技能和大量手动工作。自动化IaC在当前行业条件下是必不可少的，在本调查中，我们研究了应用大型语言模型（LLM）来解决这一问题的可行性。LLM是基于大型神经网络的模型，表现出显著的语言处理能力，并且显示出能够在广泛范围内遵循一系列指令的能力。最近，它们也已成功地被转化为代码理解和生成任务，这使它们成为可能

    arXiv:2404.00227v1 Announce Type: cross  Abstract: Infrastructure as Code (IaC) is a revolutionary approach which has gained significant prominence in the Industry. IaC manages and provisions IT infrastructure using machine-readable code by enabling automation, consistency across the environments, reproducibility, version control, error reduction and enhancement in scalability. However, IaC orchestration is often a painstaking effort which requires specialised skills as well as a lot of manual effort. Automation of IaC is a necessity in the present conditions of the Industry and in this survey, we study the feasibility of applying Large Language Models (LLM) to address this problem. LLMs are large neural network-based models which have demonstrated significant language processing abilities and shown to be capable of following a range of instructions within a broad scope. Recently, they have also been adapted for code understanding and generation tasks successfully, which makes them a p
    
[^136]: 想要的设计：利用视觉问答进行多模态预训练

    Design as Desired: Utilizing Visual Question Answering for Multimodal Pre-training

    [https://arxiv.org/abs/2404.00226](https://arxiv.org/abs/2404.00226)

    本研究是首次利用视觉问答（VQA）进行多模态预训练，专注于引导模型学习所需病理特征，并提出了一种无需额外专家注释的问题-答案对设计方法，以及一种准文本特征转换器模块。

    

    多模态预训练在医疗领域展示了其潜力，从成对的医疗报告中学习医学视觉表示。然而，许多预训练任务需要临床医生额外的注释，大多数任务未能明确引导模型学习不同病理特征。据我们所知，我们是第一个利用视觉问答（VQA）进行多模态预训练的团队，以引导框架专注于目标病理特征。在这项工作中，我们利用医疗报告中的描述设计了与不同疾病相关的多粒度问题-答案对，这有助于框架在预训练中无需专家额外的注释。我们还提出了一种新颖的预训练框架，其中包括一种准文本特征转换器模块，旨在通过将视觉特征转换到接近文本领域的准文本空间来辅助预训练过程。

    arXiv:2404.00226v1 Announce Type: cross  Abstract: Multimodal pre-training demonstrates its potential in the medical domain, which learns medical visual representations from paired medical reports. However, many pre-training tasks require extra annotations from clinicians, and most of them fail to explicitly guide the model to learn the desired features of different pathologies. To the best of our knowledge, we are the first to utilize Visual Question Answering (VQA) for multimodal pre-training to guide the framework focusing on targeted pathological features. In this work, we leverage descriptions in medical reports to design multi-granular question-answer pairs associated with different diseases, which assist the framework in pre-training without requiring extra annotations from experts. We also propose a novel pre-training framework with a quasi-textual feature transformer, a module designed to transform visual features into a quasi-textual space closer to the textual domain via a c
    
[^137]: 由对比学习生成的科学文章句子级嵌入的分类和聚类

    Classification and Clustering of Sentence-Level Embeddings of Scientific Articles Generated by Contrastive Learning

    [https://arxiv.org/abs/2404.00224](https://arxiv.org/abs/2404.00224)

    对比学习生成的句子级科学文章嵌入进行分类和聚类，相比基线模型，在聚类协议水平上有五倍增长

    

    arXiv:2404.00224v1 公告类型: 新 抽象: 科学文章是长文档，通常分为若干部分，每一部分描述研究的不同方面。分析科学作品变得越来越具有挑战性，因为可用文章数量的增加。在这种情况下，我们的方法是对变压器语言模型进行微调，以从科学文章中生成句子级嵌入，考虑以下标签: 背景、目标、方法、结果和结论。我们使用对比学习在三个数据集上训练我们的模型。其中两个数据集来自计算机科学和医学领域的文章摘要。此外，我们介绍了一个新的数据集PMC-Sents-FULL，其中的句子是从医学文章的全文中提取的。我们比较了微调模型和基准模型在聚类和分类任务中的性能以评估我们的方法。在平均情况下，聚类一致性度量值高出五倍。

    arXiv:2404.00224v1 Announce Type: new  Abstract: Scientific articles are long text documents organized into sections, each describing aspects of the research. Analyzing scientific production has become progressively challenging due to the increase in the number of available articles. Within this scenario, our approach consisted of fine-tuning transformer language models to generate sentence-level embeddings from scientific articles, considering the following labels: background, objective, methods, results, and conclusion. We trained our models on three datasets with contrastive learning. Two datasets are from the article's abstracts in the computer science and medical domains. Also, we introduce PMC-Sents-FULL, a novel dataset of sentences extracted from the full texts of medical articles. We compare the fine-tuned and baseline models in clustering and classification tasks to evaluate our approach. On average, clustering agreement measures values were five times higher. For the classif
    
[^138]: 基于原理的意见总结

    Rationale-based Opinion Summarization

    [https://arxiv.org/abs/2404.00217](https://arxiv.org/abs/2404.00217)

    提出了一种基于原理的意见总结范式及其提取方法 RATION，可生成代表性意见和相应原理，经过评估显示提取的原理具有相关性、具体性、流行度和多样性。

    

    意见总结旨在生成简明的摘要，展示大量评论的热门意见。然而，这些摘要可能过于普遍化且缺乏支持细节。为了解决这些问题，我们提出了一种新的总结评论的范式，即基于原理的意见总结。基于原理的意见摘要输出代表性意见以及一个或多个相应的原理。为了提取好的原理，我们定义了四种理想的属性：相关性、具体性、流行度和多样性，并提出了一种基于吉布斯取样的方法来提取原理。总的来说，我们提出了RATION，一种无监督的摘取系统，它有两个组成部分：一种意见提取器（提取代表性意见）和一种原理提取器（提取对应的原理）。我们进行了自动化和人工评估，以展示通过RATION提取的原理具有所提出的属性，并且其摘要是

    arXiv:2404.00217v1 Announce Type: new  Abstract: Opinion summarization aims to generate concise summaries that present popular opinions of a large group of reviews. However, these summaries can be too generic and lack supporting details. To address these issues, we propose a new paradigm for summarizing reviews, rationale-based opinion summarization. Rationale-based opinion summaries output the representative opinions as well as one or more corresponding rationales. To extract good rationales, we define four desirable properties: relatedness, specificity, popularity, and diversity and present a Gibbs-sampling-based method to extract rationales. Overall, we propose RATION, an unsupervised extractive system that has two components: an Opinion Extractor (to extract representative opinions) and Rationales Extractor (to extract corresponding rationales). We conduct automatic and human evaluations to show that rationales extracted by RATION have the proposed properties and its summaries are 
    
[^139]: 大型语言模型中的事实解码：在知识编辑基准上的评估

    Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark

    [https://arxiv.org/abs/2404.00216](https://arxiv.org/abs/2404.00216)

    大型语言模型通过事实解码方法提高了事实准确性，然而，这些方法使模型对已知事实过于自信，进一步评估显示在知识编辑基准上所有解码方法均显著降低了模型性能。

    

    大型语言模型（LLMs）的快速发展使它们能够以更类似于人类的方式传达事实知识。人们已经做出了大量努力来通过修改LLMs并降低事实幻觉来提高事实准确性。然而，这些修改也存在阻碍知识更新的风险，因为它们使模型对已知事实过于自信。本文首先重新审视当前的事实解码方法，并验证了它们在提高事实准确性方面的有效性。随后，我们对几种强大的事实解码方法在知识编辑基准上进行进一步评估。所有这些解码方法与其原始解码相比均显着降低了llama2模型的性能，其中最大的降低幅度达到惊人的81.3\%。这进一步表明，当前的解码方法仍无法完全解决事实幻觉问题，因为它们忽视了先验知识的重要性。

    arXiv:2404.00216v1 Announce Type: cross  Abstract: The rapid development of large language models (LLMs) enables them to convey factual knowledge in a more human-like fashion. Extensive efforts have been made to reduce factual hallucinations by modifying LLMs with factuality decoding. However, they also pose risks of hindering knowledge updates, as they make models overly confident in known facts. In this work, we first revisite the current factuality decoding methods and verified their effectiveness in enhancing factual accuracy. Subsequently, we conduct further evaluation of several strong factuality decoding methods on the knowledge editing benchmark. All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3\%. This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of pres
    
[^140]: 通过监督微调将新知识注入大型语言模型

    Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning

    [https://arxiv.org/abs/2404.00213](https://arxiv.org/abs/2404.00213)

    本论文研究了在大型语言模型中通过监督微调方法注入新知识的效果，特别关注了最近体育事件领域。

    

    近年来，大型语言模型（LLMs）在生成类似人类文本方面表现出色，被证明在各种应用中是一项宝贵的资产。然而，使这些模型适应并整合新的领域知识仍然是一项挑战，特别是针对模型知识截止日期之后发生的事实和事件。本文研究了监督微调（SFT）作为LLMs中注入知识的方法的有效性，特别关注了最近体育事件领域。

    arXiv:2404.00213v1 Announce Type: new  Abstract: In recent years, Large Language Models (LLMs) have shown remarkable performance in generating human-like text, proving to be a valuable asset across various applications. However, adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model's knowledge cutoff date. This paper investigates the effectiveness of Supervised Fine-Tuning (SFT) as a method for knowledge injection in LLMs, specifically focusing on the domain of recent sporting events. We compare different dataset generation strategies -- token-based and fact-based scaling -- to create training data that helps the model learn new information. Our experiments on GPT-4 demonstrate that while token-based scaling can lead to improvements in Q&A accuracy, it may not provide uniform coverage of new knowledge. Fact-based scaling, on the other hand, offers a more systematic approach to ensure even cove
    
[^141]: 大型语言模型下的多条件排序

    Multi-Conditional Ranking with Large Language Models

    [https://arxiv.org/abs/2404.00211](https://arxiv.org/abs/2404.00211)

    该论文提出了一种新颖的分解推理方法(MCRank)，用于解决大型语言模型在多条件排序任务中性能下降的问题。

    

    利用大型语言模型(LLMs)对一组项目进行排序已成为推荐和检索系统中的常见方法。在这篇论文中，我们定义并探讨了多条件排序的任务，引入了一个名为MCRank的基准，旨在评估跨不同项目类型和条件进行多条件排序。我们使用MCRank对LLMs进行分析表明，随着项目和条件数量以及复杂性的增长，性能显著下降。为了克服这一限制，我们提出了一种新颖的分解推理方法，包括提取和排序条件，然后迭代地对条件进行排序。

    arXiv:2404.00211v1 Announce Type: new  Abstract: Utilizing large language models (LLMs) to rank a set of items has become a common approach in recommendation and retrieval systems. Typically, these systems focus on ordering a substantial number of documents in a monotonic order based on a given query. However, real-world scenarios often present a different challenge: ranking a comparatively smaller set of items, but according to a variety of diverse and occasionally conflicting conditions. In this paper, we define and explore the task of multi-conditional ranking by introducing MCRank, a benchmark tailored for assessing multi-conditional ranking across various item types and conditions. Our analysis of LLMs using MCRank indicates a significant decrease in performance as the number and complexity of items and conditions grow. To overcome this limitation, we propose a novel decomposed reasoning method, consisting of EXtracting and Sorting the conditions, and then Iterativly Ranking the i
    
[^142]: EventGround：通过基于事件为中心的知识图谱实现叙事推理

    EventGround: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs

    [https://arxiv.org/abs/2404.00209](https://arxiv.org/abs/2404.00209)

    提出了一个名为EventGround的框架，旨在解决将自由文本与以事件为中心的知识图谱进行关联的问题，以进行情境化叙事推理

    

    叙事推理依赖于对故事情境中事件的理解，这需要丰富的背景世界知识。为了帮助机器利用这种知识，现有解决方案可分为两类。一些侧重于通过预训练语言模型（LMs）的事件感知目标来隐式建模事件知识。然而，这种方法会破坏知识结构并缺乏可解释性。另一些则将事件的世界知识明确地收集到结构化的以事件为中心的知识图谱（KGs）中。然而，目前关于利用这些知识源进行自由文本处理的研究有限。在这项工作中，我们提出了一个名为EventGround的初步综合框架，旨在解决将自由文本与以事件为中心的知识图谱进行关联的问题，以进行情境化叙事推理。我们确定了在这个方向上的两个关键问题：事件表征和稀疏性

    arXiv:2404.00209v1 Announce Type: new  Abstract: Narrative reasoning relies on the understanding of eventualities in story contexts, which requires a wealth of background world knowledge. To help machines leverage such knowledge, existing solutions can be categorized into two groups. Some focus on implicitly modeling eventuality knowledge by pretraining language models (LMs) with eventuality-aware objectives. However, this approach breaks down knowledge structures and lacks interpretability. Others explicitly collect world knowledge of eventualities into structured eventuality-centric knowledge graphs (KGs). However, existing research on leveraging these knowledge sources for free-texts is limited. In this work, we propose an initial comprehensive framework called EventGround, which aims to tackle the problem of grounding free-texts to eventuality-centric KGs for contextualized narrative reasoning. We identify two critical problems in this direction: the event representation and sparsi
    
[^143]: 人类-语言模型协作的因果推断

    Causal Inference for Human-Language Model Collaboration

    [https://arxiv.org/abs/2404.00207](https://arxiv.org/abs/2404.00207)

    本文研究了人类和语言模型之间的协作动态，并提出了一个新的因果估计Incremental Stylistic Effect来解释如何改变合作结果。

    

    在本文中，我们研究了人类和语言模型（LMs）之间的协作动态，这些互动通常涉及LMs提出文本段落，而人类编辑或回应这些建议。在这种情况下与LMs进行有效的互动要求人类辨别出有效的基于文本的互动策略，例如编辑和回应样式，从历史人类-LM互动中。这个目标本质上是因果关系，受到反事实“如果”问题的驱动:如果人类采用不同的文本编辑/精炼策略，协作的结果会如何改变？回答这个因果推断问题的一个关键挑战是制定一个适当的因果估计:传统的平均处理效应（ATE）估计由于文本的高维度而不适用于基于文本的处理。为了解决这一问题，我们引入了一个新的因果估计 - 增量风格效应

    arXiv:2404.00207v1 Announce Type: cross  Abstract: In this paper, we examine the collaborative dynamics between humans and language models (LMs), where the interactions typically involve LMs proposing text segments and humans editing or responding to these proposals. Productive engagement with LMs in such scenarios necessitates that humans discern effective text-based interaction strategies, such as editing and response styles, from historical human-LM interactions. This objective is inherently causal, driven by the counterfactual `what-if' question: how would the outcome of collaboration change if humans employed a different text editing/refinement strategy? A key challenge in answering this causal inference question is formulating an appropriate causal estimand: the conventional average treatment effect (ATE) estimand is inapplicable to text-based treatments due to their high dimensionality. To address this concern, we introduce a new causal estimand -- Incremental Stylistic Effect (
    
[^144]: 语言模型中的概念性和无偏推理

    Conceptual and Unbiased Reasoning in Language Models

    [https://arxiv.org/abs/2404.00205](https://arxiv.org/abs/2404.00205)

    提出了一个新颖的概念化框架，强制语言模型在抽象问题上进行概念推理，揭示现有大型语言模型在概念推理方面的不足，并探讨了如何通过改进模型来实现高级抽象推理，从而促进无偏和泛化决策。

    

    概念推理，即在抽象和高层次视角进行推理的能力，是人类认知中泛化的关键。然而，对于大型语言模型在执行概念推理方面的能力进行了有限的研究。在这项工作中，我们弥合了这一差距，并提出了一个新颖的概念化框架，强制模型在抽象问题上进行概念推理，并在可验证的符号空间中生成解决方案。利用这个框架作为分析工具，我们展示了现有大型语言模型在概念推理方面的不足，与直接推理方法相比，在各种基准测试中下降了9%至28%。然后我们讨论了模型如何改进，因为高级抽象推理是无偏和泛化决策的关键。我们提出了两种技术，通过生成具有类似潜在推理路径的熟悉问题并要求模型执行自我参照，来添加可信的归纳信号。

    arXiv:2404.00205v1 Announce Type: new  Abstract: Conceptual reasoning, the ability to reason in abstract and high-level perspectives, is key to generalization in human cognition. However, limited study has been done on large language models' capability to perform conceptual reasoning. In this work, we bridge this gap and propose a novel conceptualization framework that forces models to perform conceptual reasoning on abstract questions and generate solutions in a verifiable symbolic space. Using this framework as an analytical tool, we show that existing large language models fall short on conceptual reasoning, dropping 9% to 28% on various benchmarks compared to direct inference methods. We then discuss how models can improve since high-level abstract reasoning is key to unbiased and generalizable decision-making. We propose two techniques to add trustworthy induction signals by generating familiar questions with similar underlying reasoning paths and asking models to perform self-ref
    
[^145]: GPTA：生成提示调整助手用于LLM的协同下游神经网络增强

    GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream Neural Network Enhancement with LLMs

    [https://arxiv.org/abs/2404.00189](https://arxiv.org/abs/2404.00189)

    GPTA引入了一个大型语言模型辅助训练框架，通过前缀提示增强了下游任务模型的训练，不仅显著提高了模型性能，还有效减少了低资源场景下的过拟合。

    

    本研究介绍了GPTA，一种大型语言模型辅助训练框架，通过前缀提示增强了下游任务模型的训练。通过最小化LLM对数据的暴露，该框架解决了在下游任务模型训练中应用LLM所面临的安全和法律挑战。GPTA利用一种新的协同训练方法，通过参数梯度优化下游模型和LLM，新的“对话梯度”。该框架不仅在六个NLP基准数据集上展示出模型性能的显著改进，而且有效地减少了低资源场景下的过拟合。详细分析进一步验证了我们的首创性框架为具有LLM支持的下游任务模型训练提供了一种成本高效且适应性强的方法。

    arXiv:2404.00189v1 Announce Type: new  Abstract: This study introduces GPTA, a Large Language Model assistance training framework, that enhances the training of downstream task models via prefix prompt. By minimizing data exposure to LLM, the framework addresses the security and legal challenges of applying LLM in downstream task model training. GPTA utilizes a new synergistic training approach, optimizing the downstream models with parameter gradients and LLMs with the novel ``dialogue gradient''. The framework not only demonstrates significant improvements in model performance across six NLP benchmark datasets, but also reduces overfitting in low-resource scenarios effectively. The detailed analyses further validate that our pioneer framework provides a cost-efficient and adaptive method for downstream task model training with LLM support.
    
[^146]: DataAgent: 评估大型语言模型回答零样本自然语言查询的能力

    DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries

    [https://arxiv.org/abs/2404.00188](https://arxiv.org/abs/2404.00188)

    评估了 OpenAI 的 GPT-3.5 模型作为“语言数据科学家”，成功回答了与基准数据集相关的数据科学查询。

    

    传统的数据集分析和提取有意义信息的过程往往耗时且繁琐。以前的工作已经确定手动、重复性编码和数据收集是阻碍数据科学家从事更微妙的工作和高水平项目的主要障碍。为了解决这个问题，我们评估了 OpenAI 的 GPT-3.5 作为“语言数据科学家”（LDS），可以从给定数据集中推导出关键发现，包括相关性和基本信息。该模型在多个标准上表现良好，包括基于数据科学代码生成的任务，涉及NumPy、Pandas、Scikit-Learn 和 TensorFlow 等库，并在正确回答与基准数据集相关的给定数据科学查询方面取得了广泛成功。LDS 使用了各种新颖的提示工程技术来有效回答给定问题，包括

    arXiv:2404.00188v1 Announce Type: cross  Abstract: Conventional processes for analyzing datasets and extracting meaningful information are often time-consuming and laborious. Previous work has identified manual, repetitive coding and data collection as major obstacles that hinder data scientists from undertaking more nuanced labor and high-level projects. To combat this, we evaluated OpenAI's GPT-3.5 as a "Language Data Scientist" (LDS) that can extrapolate key findings, including correlations and basic information, from a given dataset. The model was tested on a diverse set of benchmark datasets to evaluate its performance across multiple standards, including data science code-generation based tasks involving libraries such as NumPy, Pandas, Scikit-Learn, and TensorFlow, and was broadly successful in correctly answering a given data science query related to the benchmark dataset. The LDS used various novel prompt engineering techniques to effectively answer a given question, including
    
[^147]: 单词阶梯：语义数据收集的移动应用

    Word Ladders: A Mobile Application for Semantic Data Collection

    [https://arxiv.org/abs/2404.00184](https://arxiv.org/abs/2404.00184)

    Word Ladders是一个用于语义数据收集的移动应用，可通过分类包含的语义关系建立单词列表，具有应用于自然语言处理任务以及认知科学问题调查的潜力。

    

    Word Ladders是一个免费的移动应用，可用于Android和iOS，专为收集语言数据而开发，特别是通过分类包含语义关系相关的单词列表，包括在抽象项目（ERC-2021-STG-101039777）中。我们在此提供Word Ladders的概述，解释其游戏逻辑、动机以及预期结果和应用到nlp任务以及对认知科学开放问题的调查。

    arXiv:2404.00184v1 Announce Type: new  Abstract: Word Ladders is a free mobile application for Android and iOS, developed for collecting linguistic data, specifically lists of words related to each other through semantic relations of categorical inclusion, within the Abstraction project (ERC-2021-STG-101039777). We hereby provide an overview of Word Ladders, explaining its game logic, motivation and expected results and applications to nlp tasks as well as to the investigation of cognitive scientific open questions
    
[^148]: LSCD基准测试：历时词义任务的测试平台

    The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks

    [https://arxiv.org/abs/2404.00176](https://arxiv.org/abs/2404.00176)

    LSCD基准测试提供了一个标准化的LSCD评估平台，解决了模型评估和结果复现中存在的异质性问题。

    

    词汇语义变化检测（LSCD）是一项复杂的词元级任务，通常基于两个连续应用的使用级任务来操作：首先，为使用对得到Word-in-Context (WiC)标签。然后，在图上表示这些标签，对其应用Word Sense Induction (WSI)来推导出含义聚类。最后，通过比较随时间变化的含义聚类来推导出LSCD标签。这种模块化反映在大多数LSCD数据集和模型中。这也导致了建模选择和任务定义上的大量异质性，这一点又因各种数据集版本、预处理选项和评估指标而加剧。这种异质性使得在可比条件下评估模型、选择最佳模型组合或复现结果变得困难。因此，我们提供了一个基准测试库，以规范LSCD评估。通过透明的实现，结果变得易于复现。

    arXiv:2404.00176v1 Announce Type: new  Abstract: Lexical Semantic Change Detection (LSCD) is a complex, lemma-level task, which is usually operationalized based on two subsequently applied usage-level tasks: First, Word-in-Context (WiC) labels are derived for pairs of usages. Then, these labels are represented in a graph on which Word Sense Induction (WSI) is applied to derive sense clusters. Finally, LSCD labels are derived by comparing sense clusters over time. This modularity is reflected in most LSCD datasets and models. It also leads to a large heterogeneity in modeling options and task definitions, which is exacerbated by a variety of dataset versions, preprocessing options and evaluation metrics. This heterogeneity makes it difficult to evaluate models under comparable conditions, to choose optimal model combinations or to reproduce results. Hence, we provide a benchmark repository standardizing LSCD evaluation. Through transparent implementation results become easily reproducib
    
[^149]: 个人文本语料库预测开放性、兴趣、知识和教育水平

    Individual Text Corpora Predict Openness, Interests, Knowledge and Level of Education

    [https://arxiv.org/abs/2404.00165](https://arxiv.org/abs/2404.00165)

    本研究探讨了个人对体验的开放性这一人格维度是否可以通过个体的谷歌搜索历史预测，并通过相似性特征基于个人文本语料库来解释35%的开放性方差。

    

    在这项研究中，我们调查了个体对体验的开放性这一人格维度是否可以通过个体的谷歌搜索历史进行预测。通过网络抓取，我们生成了来自214名参与者的个人文本语料库，平均词汇量为500万个词元。我们训练了word2vec模型，并使用每个个人文本语料库与标记单词的相似性，这些标记单词来自于人格的词汇方法。这些个人文本语料库-标记单词的相似性被用作神经模型中的预测特征。为了训练和验证，我们依赖179名参与者，并保留了35名参与者的测试样本。我们进行了一个带有不同数量预测特征、隐藏单元和增量因子的网格搜索。作为模型选择标准，我们使用了在验证样本中由绝对R2差异惩罚的R2。选择的神经模型在测试样本中解释了35%的开放性方差，而集成模型…

    arXiv:2404.00165v1 Announce Type: new  Abstract: Here we examine whether the personality dimension of openness to experience can be predicted from the individual google search history. By web scraping, individual text corpora (ICs) were generated from 214 participants with a mean number of 5 million word tokens. We trained word2vec models and used the similarities of each IC to label words, which were derived from a lexical approach of personality. These IC-label-word similarities were utilized as predictive features in neural models. For training and validation, we relied on 179 participants and held out a test sample of 35 participants. A grid search with varying number of predictive features, hidden units and boost factor was performed. As model selection criterion, we used R2 in the validation samples penalized by the absolute R2 difference between training and validation. The selected neural model explained 35% of the openness variance in the test sample, while an ensemble model w
    
[^150]: 在线定义增强LLMs用于生物医学NER

    On-the-fly Definition Augmentation of LLMs for Biomedical NER

    [https://arxiv.org/abs/2404.00152](https://arxiv.org/abs/2404.00152)

    通过实时合并相关概念的定义，本研究提出了一种新的知识增强方法以改善LLMs在生物医学NER任务中的性能，在测试数据设置下平均提高了15\%的性能。

    

    尽管LLMs具有一般的能力，但仍然在生物医学NER任务中遇到困难，这是由于存在专业术语和缺乏训练数据所致。在这项工作中，我们旨在通过一种新的知识增强方法，在有限数据设置下改善LLMs在生物医学NER上的性能，该方法通过实时合并相关概念的定义。在这个过程中，为了提供知识增强的测试场景，我们对提示策略进行了全面的探索。我们的实验证明，定义增强对于开源和封闭的LLMs都是有用的。例如，在我们所有（六个）测试数据集中，它导致了GPT-4性能（F1）平均相对提升了15\%。我们进行了广泛的消融和分析，以证明我们的性能改进来源于添加相关的定义知识。我们发现谨慎的提示策略也提高了LLMs的性能。

    arXiv:2404.00152v1 Announce Type: new  Abstract: Despite their general capabilities, LLMs still struggle on biomedical NER tasks, which are difficult due to the presence of specialized terminology and lack of training data. In this work we set out to improve LLM performance on biomedical NER in limited data settings via a new knowledge augmentation approach which incorporates definitions of relevant concepts on-the-fly. During this process, to provide a test bed for knowledge augmentation, we perform a comprehensive exploration of prompting strategies. Our experiments show that definition augmentation is useful for both open source and closed LLMs. For example, it leads to a relative improvement of 15\% (on average) in GPT-4 performance (F1) across all (six) of our test datasets. We conduct extensive ablations and analyses to demonstrate that our performance improvements stem from adding relevant definitional knowledge. We find that careful prompting strategies also improve LLM perform
    
[^151]: 以规模对阴谋论述进行分类：虚假警报和错误联系

    Classifying Conspiratorial Narratives At Scale: False Alarms and Erroneous Connections

    [https://arxiv.org/abs/2404.00141](https://arxiv.org/abs/2404.00141)

    通过作者对阴谋信仰的视角来建立一个通用方案，用于分类不同话题和线上社区中的阴谋讨论，利用BERT模型进行训练，较之生成式方法效果更好。

    

    线上讨论经常涉及阴谋论，这可能会促使人们对其产生更多的信仰。然而，并非所有围绕阴谋论的讨论都是推崇它们，有些是为了揭穿它们。现有研究通常依赖简单的代理或专注于一组受限信号来识别阴谋理论，这限制了我们对不同主题和线上社区中的阴谋讨论的理解。本研究建立了一个通用方案，根据作者对阴谋信仰的视角来对与阴谋理论相关的讨论进行分类，这可以通过叙事要素明确表达，例如行动者、行动或目标，或者通过隐含地指向已知理论，比如化学气溶胶或新世界秩序。我们利用人工标记的地面实况来训练基于BERT的模型，用于分类在线阴谋论述，然后将其与生成式进行比较。

    arXiv:2404.00141v1 Announce Type: new  Abstract: Online discussions frequently involve conspiracy theories, which can contribute to the proliferation of belief in them. However, not all discussions surrounding conspiracy theories promote them, as some are intended to debunk them. Existing research has relied on simple proxies or focused on a constrained set of signals to identify conspiracy theories, which limits our understanding of conspiratorial discussions across different topics and online communities. This work establishes a general scheme for classifying discussions related to conspiracy theories based on authors' perspectives on the conspiracy belief, which can be expressed explicitly through narrative elements, such as the agent, action, or objective, or implicitly through references to known theories, such as chemtrails or the New World Order. We leverage human-labeled ground truth to train a BERT-based model for classifying online CTs, which we then compared to the Generativ
    
[^152]: 你来自哪里？让我猜猜！索拉尼库尔德语方言识别

    Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish

    [https://arxiv.org/abs/2404.00124](https://arxiv.org/abs/2404.00124)

    挑战是缺乏公开可用数据集或可靠资源，针对索拉尼库尔德语方言的六个方言建立了一个不平衡数据集并利用三种深度学习模型进行分析。

    

    由于公开可用数据集或可靠资源（如社交媒体或网站）的需求，对索拉尼库尔德语方言进行分类构成一项挑战。我们进行了多个城市和村庄的实地访问，与不同年龄组、性别、学术背景和职业的母语者进行了联系。我们在进行涵盖各种话题如生活方式、背景历史、爱好、兴趣、假期和生活经验的对话过程中录制了他们的声音。研究的目标地区是伊拉克库尔德斯坦地区。因此，我们从107个访谈中积累了29小时16分钟40秒的音频记录，包括六种方言的不平衡数据集。随后，我们采用了三种深度学习模型: 人工神经网络（ANN）、卷积神经网络（CNN）和循环神经网络-长短期记忆（RNN-LSTM）。我们探索了不同的配置，包括不同的跟踪持续时间、数据集拆分和不平衡处理。

    arXiv:2404.00124v1 Announce Type: new  Abstract: Classifying Sorani Kurdish subdialects poses a challenge due to the need for publicly available datasets or reliable resources like social media or websites for data collection. We conducted field visits to various cities and villages to address this issue, connecting with native speakers from different age groups, genders, academic backgrounds, and professions. We recorded their voices while engaging in conversations covering diverse topics such as lifestyle, background history, hobbies, interests, vacations, and life lessons. The target area of the research was the Kurdistan Region of Iraq. As a result, we accumulated 29 hours, 16 minutes, and 40 seconds of audio recordings from 107 interviews, constituting an unbalanced dataset encompassing six subdialects. Subsequently, we adapted three deep learning models: ANN, CNN, and RNN-LSTM. We explored various configurations, including different track durations, dataset splitting, and imbalan
    
[^153]: 使用倒置标签的后门方法：脏标签翻转攻击

    A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks

    [https://arxiv.org/abs/2404.00076](https://arxiv.org/abs/2404.00076)

    提出了一种后门攻击方法，名为“DirtyFlipping”，利用脏标签技术在选定的数据模式中输入触发器，从而实现隐蔽的后门。

    

    基于声音的机器学习系统经常使用公共或第三方数据，这可能是不准确的。这使得训练在这些数据上的深度神经网络（DNN）模型容易受到潜在的数据毒化攻击。在这种攻击类型中，攻击者可以使用毒化数据来训练DNN模型，可能会降低其性能。另一种对我们的研究非常相关的数据毒化攻击类型是标签翻转，攻击者在其中操纵数据子集的标签。已经证明，即使是能力有限的攻击者，这些攻击也可能极大地降低系统性能。在本研究中，我们提出了一种名为“DirtyFlipping”的后门攻击，使用脏标签技术，“标签对标签”，在与目标类别相关的选定数据模式中输入触发器（拍手），从而实现了隐蔽的后门。

    arXiv:2404.00076v1 Announce Type: cross  Abstract: Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named 'DirtyFlipping', which uses dirty label techniques, "label-on-label", to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.
    
[^154]: Deja vu: 使用前缀调整进行对比历史建模，用于时间知识图推理

    Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge Graph Reasoning

    [https://arxiv.org/abs/2404.00051](https://arxiv.org/abs/2404.00051)

    提出了ChapTER，一个对比历史建模框架，通过前缀调整实现文本与时间的平衡，用于时间知识图推理。

    

    时间知识图推理（TKGR）是在复杂场景中为不完整的TKG推断缺失事实的任务（例如，传导和归纳设置），越来越受到关注。最近，为了减少TKG中结构连接的依赖性，已开发了基于文本的方法，利用实体描述中丰富的语言信息。然而，由于预训练语言模型的巨大参数和不灵活性，现有的基于文本的方法在计算昂贵且目的建立的训练策略上很难平衡文本知识和时间信息。为了发掘文本模型在各种复杂场景中用于TKGR的潜力，我们提出了ChapTER，一个具有前缀调整对比历史建模框架，用于时间推理。

    arXiv:2404.00051v1 Announce Type: new  Abstract: Temporal Knowledge Graph Reasoning (TKGR) is the task of inferring missing facts for incomplete TKGs in complex scenarios (e.g., transductive and inductive settings), which has been gaining increasing attention. Recently, to mitigate dependence on structured connections in TKGs, text-based methods have been developed to utilize rich linguistic information from entity descriptions. However, suffering from the enormous parameters and inflexibility of pre-trained language models, existing text-based methods struggle to balance the textual knowledge and temporal information with computationally expensive purpose-built training strategies. To tap the potential of text-based models for TKGR in various complex scenarios, we propose ChapTER, a Contrastive historical modeling framework with prefix-tuning for TEmporal Reasoning. ChapTER feeds history-contextualized text into the pseudo-Siamese encoders to strike a textual-temporal balance via cont
    
[^155]: LLM作为写作助手：探讨所有权感和推理的视角

    LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning

    [https://arxiv.org/abs/2404.00027](https://arxiv.org/abs/2404.00027)

    探讨使用大型语言模型作为写作助手引发的写作所有权感和作者身份认知之间的心理困境。

    

    写作中的所有权感限制了我们对思想、时间和贡献的投入，导致对产出物的依恋。然而，使用写作助手引入了一种心理困境，因为一些内容并非直接我们的创作。我们往往更倾向于在创造性任务中更多地归功于大型语言模型（LLMs），尽管它们对所有任务都是平等的。此外，虽然我们可能不会完全声称对由LLM生成的内容拥有所有权，但却自由地声称作者身份。我们进行了一项简短调查来研究这些问题，并了解潜在的认知过程，以更好地了解人机交互在写作中的应用并改进写作辅助系统。

    arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
    
[^156]: 墨水与个性：在LLMs时代塑造个性化叙事

    Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs

    [https://arxiv.org/abs/2404.00026](https://arxiv.org/abs/2404.00026)

    研究探讨了人们日益依赖的基于LLM的写作助手对创造力和个性可能造成的负面影响，旨在改进人机交互系统和提升写作助手的个性化和个性化功能。

    

    个性和个性化构成了使每个作家独特并影响其文字以有效吸引读者同时传达真实性的独特特征。然而，我们日益依赖基于LLM的写作助手可能会危及我们的创造力和个性。我们经常忽视这一趋势对我们的创造力和独特性的负面影响，尽管可能会造成后果。本研究通过进行简要调查探索不同的观点和概念，以及尝试理解人们的观点，结合以往在该领域的研究，来研究这些问题。解决这些问题对于改进人机交互系统和增强个性化和个性化写作助手至关重要。

    arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
    
[^157]: 创新的鹦鹉？评估AI创作的真正新颖性

    Psittacines of Innovation? Assessing the True Novelty of AI Creations

    [https://arxiv.org/abs/2404.00017](https://arxiv.org/abs/2404.00017)

    AI系统在生成项目标题方面展现出独特的创新能力，即使在任务复杂性增加和计算能力极限的情况下，生成的内容具有表面有效性。

    

    我们检验人工智能（AI）系统是否生成真正新颖的想法，而不仅仅是在训练期间学习到的模式。利用一种新颖的实验设计，我们让一个AI生成虚构的众筹活动项目标题。我们比较AI生成的项目标题内部，衡量重复性和复杂性。我们利用一种扩展的最大均值差异方法，在AI生成的标题和实际观测的现场数据之间进行比较，该方法是从将统计分布的核均值嵌入应用到高维机器学习（大语言）嵌入向量中推导出来的--从而得到对AI输出新颖性的结构化分析。结果表明：（1）即使在任务复杂性增加，并且在计算能力的极限下，AI也会生成独特内容，（2）生成的内容具有表面有效性，与送入其他生成AI的输入一致。

    arXiv:2404.00017v1 Announce Type: new  Abstract: We examine whether Artificial Intelligence (AI) systems generate truly novel ideas rather than merely regurgitating patterns learned during training. Utilizing a novel experimental design, we task an AI with generating project titles for hypothetical crowdfunding campaigns. We compare within AI-generated project titles, measuring repetition and complexity. We compare between the AI-generated titles and actual observed field data using an extension of maximum mean discrepancy--a metric derived from the application of kernel mean embeddings of statistical distributions to high-dimensional machine learning (large language) embedding vectors--yielding a structured analysis of AI output novelty. Results suggest that (1) the AI generates unique content even under increasing task complexity, and at the limits of its computational capabilities, (2) the generated content has face validity, being consistent with both inputs to other generative AI 
    
[^158]: 基于财经新闻情感分析的股票市场压力指数策略改进

    Stress index strategy enhanced with financial news sentiment analysis for the equity markets

    [https://arxiv.org/abs/2404.00012](https://arxiv.org/abs/2404.00012)

    通过将金融压力指标与财经新闻情感分析相结合，提高了股票市场的风险控制策略表现。

    

    本文介绍了一种新的股市风险-风险策略，将金融压力指标与通过ChatGPT读取和解释Bloomberg每日市场摘要进行的情感分析相结合。通过将从波动率和信贷利差推导出的市场压力预测与GPT-4推导出的财经新闻情感相结合，改进了策略的表现，表现为更高的夏普比率和降低的最大回撤。改进的表现在纳斯达克、标普500指数和六个主要股票市场中都保持一致，表明该方法在股票市场中具有普遍适用性。

    arXiv:2404.00012v1 Announce Type: cross  Abstract: This paper introduces a new risk-on risk-off strategy for the stock market, which combines a financial stress indicator with a sentiment analysis done by ChatGPT reading and interpreting Bloomberg daily market summaries. Forecasts of market stress derived from volatility and credit spreads are enhanced when combined with the financial news sentiment derived from GPT-4. As a result, the strategy shows improved performance, evidenced by higher Sharpe ratio and reduced maximum drawdowns. The improved performance is consistent across the NASDAQ, the S&P 500 and the six major equity markets, indicating that the method generalises across equities markets.
    
[^159]: 一个新颖的对抗式谜题写作界面

    A novel interface for adversarial trivia question-writing

    [https://arxiv.org/abs/2404.00011](https://arxiv.org/abs/2404.00011)

    该研究提出了一个新颖的界面，通过人类生成的对抗谜题数据来帮助训练问答人工智能，并提供一套工具协助编写更具挑战性的问题。

    

    当开发问答人工智能时，一个关键组成部分是一个对抗数据集，挑战模型适应自然语言中复杂语法和推理。目前的程序生成对抗文本技术对于训练回答多句谜题等复杂任务的模型并不够健壮。我们转向人类生成的数据，引入了一个界面来收集对抗的人类编写的谜题。我们的界面面向问答写作者和Quiz Bowl的参与者，Quiz Bowl是一种基于蜂鸣器的谜题竞赛，其中由一系列难度递减的提示组成的段落长问题。为了激励使用，我们界面中的一套基于机器学习的工具帮助人类编写更具挑战性的问答问题，对于Quiz Bowl参与者和计算机都更难答题。我们的界面不仅收集了开创性的Quiz B的训练数据

    arXiv:2404.00011v1 Announce Type: cross  Abstract: A critical component when developing question-answering AIs is an adversarial dataset that challenges models to adapt to the complex syntax and reasoning underlying our natural language. Present techniques for procedurally generating adversarial texts are not robust enough for training on complex tasks such as answering multi-sentence trivia questions. We instead turn to human-generated data by introducing an interface for collecting adversarial human-written trivia questions. Our interface is aimed towards question writers and players of Quiz Bowl, a buzzer-based trivia competition where paragraph-long questions consist of a sequence of clues of decreasing difficulty. To incentivize usage, a suite of machine learning-based tools in our interface assist humans in writing questions that are more challenging to answer for Quiz Bowl players and computers alike. Not only does our interface gather training data for the groundbreaking Quiz B
    
[^160]: DiJiang：通过紧凑的核方法实现高效的大型语言模型

    DiJiang: Efficient Large Language Models through Compact Kernelization

    [https://arxiv.org/abs/2403.19928](https://arxiv.org/abs/2403.19928)

    DiJiang提出了一种新颖的频域核方法，可以将预训练的基本Transformer模型转化为具有线性复杂度的模型，大大减少训练成本，并在理论上提供更好的逼近效率。

    

    为了减少Transformers的计算负荷，线性注意力的研究已经取得了显著的进展。然而，注意机制的改进策略通常需要经过大量的重新训练，在具有大量参数的大型语言模型上是不切实际的。本文介绍了DiJiang，一种新颖的频域核方法，可将预训练的基本Transformer转化为具有较小训练成本的线性复杂度模型。通过采用加权拟随机采样法，所提出的方法在理论上提供了更好的逼近效率。为了进一步降低训练的计算复杂度，我们的核方法基于离散余弦变换（DCT）操作。大量实验证明，所提出的方法达到了与原始Transformer相当的性能，但训练时间大大减少。

    arXiv:2403.19928v1 Announce Type: new  Abstract: In an effort to reduce the computational load of Transformers, research on linear attention has gained significant momentum. However, the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for large language models with a vast array of parameters. In this paper, we present DiJiang, a novel Frequency Domain Kernelization approach that enables the transformation of a pre-trained vanilla Transformer into a linear complexity model with little training costs. By employing a weighted Quasi-Monte Carlo method for sampling, the proposed approach theoretically offers superior approximation efficiency. To further reduce the training computational complexity, our kernelization is based on Discrete Cosine Transform (DCT) operations. Extensive experiments demonstrate that the proposed method achieves comparable performance to the original Transformer, but with significantly reduced trainin
    
[^161]: NJUST-KMG参加TRAC-2024任务1和任务2：离线危害潜在性识别

    NJUST-KMG at TRAC-2024 Tasks 1 and 2: Offline Harm Potential Identification

    [https://arxiv.org/abs/2403.19713](https://arxiv.org/abs/2403.19713)

    该研究提出了在TRAC-2024离线危害潜在性识别任务中的方法，利用专家标注的社交媒体评论数据集，成功设计了能够准确评估危害可能性并识别目标的算法，在两个赛道中取得第二名的成绩。

    

    这份报告详细描述了我们在TRAC-2024离线危害潜在性识别中提出的方法，该比赛包含两个子任务。研究利用了一个包含多种印度语言社交媒体评论的丰富数据集，由专家评分标注，以捕捉离线环境危害的微妙含义。参与者的任务是设计能够准确评估特定情况下危害可能性并识别离线危害最可能的目标的算法。我们的方法在两个不同的赛道中排名第二，F1值分别为0.73和0.96。我们的方法主要涉及选择预训练模型进行微调，整合对比学习技术，并通过集成方法应用于测试集。

    arXiv:2403.19713v1 Announce Type: new  Abstract: This report provide a detailed description of the method that we proposed in the TRAC-2024 Offline Harm Potential dentification which encloses two sub-tasks. The investigation utilized a rich dataset comprised of social media comments in several Indian languages, annotated with precision by expert judges to capture the nuanced implications for offline context harm. The objective assigned to the participants was to design algorithms capable of accurately assessing the likelihood of harm in given situations and identifying the most likely target(s) of offline harm. Our approach ranked second in two separate tracks, with F1 values of 0.73 and 0.96 respectively. Our method principally involved selecting pretrained models for finetuning, incorporating contrastive learning techniques, and culminating in an ensemble approach for the test set.
    
[^162]: 稀疏特征电路：在语言模型中发现和编辑可解释的因果图

    Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models

    [https://arxiv.org/abs/2403.19647](https://arxiv.org/abs/2403.19647)

    该论文介绍了一种新方法，即稀疏特征电路，可以在语言模型中发现和编辑可解释的因果图，为我们提供了对未预料机制的详细理解和包含了用于提高分类器泛化能力的SHIFT方法。

    

    我们介绍了用于发现和应用稀疏特征电路的方法。这些电路是人类可解释特征的因果相关子网络，用于解释语言模型行为。 在先前的工作中确定的电路由多义且难以解释的单元组成，例如注意力头或神经元，使它们不适用于许多下游应用。 相比之下，稀疏特征电路实现了对未预料机制的详细理解。 由于它们基于细粒度单元，稀疏特征电路对下游任务非常有用：我们 introduc了SHIFT，通过切除人类判断为任务不相关的特征，从而提高分类器的泛化能力。 最后，我们通过发现成千上万个稀疏特征电路来展示一个完全无监督且可扩展的可解释性管线，用于自动发现的模型行为。

    arXiv:2403.19647v1 Announce Type: cross  Abstract: We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.
    
[^163]: TableLLM：在实际办公使用场景中实现LLMs对表格数据进行处理的能力

    TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios

    [https://arxiv.org/abs/2403.19318](https://arxiv.org/abs/2403.19318)

    TableLLM是一个拥有130亿参数的强大大语言模型，专门用于熟练处理表格数据操作任务，通过远程监督方法和交叉验证策略，TableLLM相对于其他现有的通用和表格数据专注的LLMs具有明显优势。

    

    我们介绍了TableLLM，这是一个拥有130亿参数的强大大语言模型（LLM），专门用于熟练处理表格数据操作任务，无论其嵌入在文档还是电子表格中，以满足真实办公场景需求。我们提出了一种远程监督方法进行训练，其中包括一种推理过程扩展策略，有助于训练LLMs更有效地理解推理模式，以及一种交叉验证策略，确保自动生成数据的质量。为了评估TableLLM的性能，我们构建了一个旨在解决文档和电子表格格式的基准测试，并构建了一个能够处理两种场景的组织良好的评估管线。彻底的评估凸显了TableLLM相对于各种现有通用和专注于表格数据的LLMs的优势。我们已公开发布了该模型。

    arXiv:2403.19318v1 Announce Type: new  Abstract: We introduce TableLLM, a robust large language model (LLM) with 13 billion parameters, purpose-built for proficiently handling tabular data manipulation tasks, whether they are embedded within documents or spreadsheets, catering to real-world office scenarios. We propose a distant supervision method for training, which comprises a reasoning process extension strategy, aiding in training LLMs to understand reasoning patterns more effectively as well as a cross-way validation strategy, ensuring the quality of the automatically generated data. To evaluate the performance of TableLLM, we have crafted a benchmark tailored to address both document and spreadsheet formats as well as constructed a well-organized evaluation pipeline capable of handling both scenarios. Thorough evaluations underscore the advantages of TableLLM when compared to various existing general-purpose and tabular data-focused LLMs. We have publicly released the model check
    
[^164]: 通过简化不重要的层压缩大型语言模型

    Compressing Large Language Models by Streamlining the Unimportant Layer

    [https://arxiv.org/abs/2403.19135](https://arxiv.org/abs/2403.19135)

    通过观察大型语言模型中不同层对隐藏状态的影响程度，提出了LLM-Streamline方法，包括层剪枝和层替换，用于压缩模型并保持性能。

    

    大型语言模型(LLM)已广泛应用于各种自然语言任务和领域，但其适用性受到模型参数的限制。因此，越来越多的人关注表现出高性能的紧凑模型。在这项研究中，我们观察到LLM的不同层对隐藏状态有不同程度的扰动，这使我们能够识别出不那么重要的层。基于这一现象，我们提出了LLM-Streamline，包括两部分：层剪枝，根据目标稀疏度移除模型中一组连续的最不重要的层；层替换，训练一个轻量级模型来替换被剪枝的层，从而缓解由剪枝造成的性能下降。在实验中，我们利用了多层感知器(MLP)和一个transformer层等结构作为轻量级模型。

    arXiv:2403.19135v1 Announce Type: cross  Abstract: Large language models (LLM) have been extensively applied in various natural language tasks and domains, but their applicability is constrained by the large number of parameters of the models. Consequently, there is an increasing emphasis on compact models that exhibit high performance. In this study, we observe that different layers in LLM have varying degrees of perturbation on the hidden states, which allows us to identify less important layers. Based on this phenomenon, we propose LLM-Streamline, which consists of two parts: layer pruning, where we remove a set of consecutive layers with the lowest importance in the model according to the target sparsity; and layer replacement, where we train a lightweight model to substitute the pruned layers, thereby mitigating the performance degradation caused by pruning. In our experiments, we utilize structures such as a multi-layer perceptron (MLP) and a transformer layer as lightweight mode
    
[^165]: SemEval任务1：非洲和亚洲语言的语义文本相关性

    SemEval Task 1: Semantic Textual Relatedness for African and Asian Languages

    [https://arxiv.org/abs/2403.18933](https://arxiv.org/abs/2403.18933)

    这个任务涉及14种非洲和亚洲语言的语义文本相关性，旨在考察跨语言的语义相关性现象。

    

    我们介绍了第一个关于语义文本相关性（STR）的共享任务。而先前的共享任务主要关注语义相似性，我们则调查了跨越14种语言（包括南非荷兰语、阿尔及利亚阿拉伯语、阿姆哈拉语、英语、豪萨语、印地语、印尼语、基尼亚鲁安达语、马拉地语、摩洛哥阿拉伯语、现代标准阿拉伯语、旁遮普语、西班牙语和泰卢固语）的更广泛的语义相关性现象。这些语言来自五个不同的语系，并主要在非洲和亚洲地区使用，这些地区的特点是自然语言处理资源的相对有限。数据集中的每个实例都是一个与分数相关联的句对，该分数表示两个句子之间的语义文本相关程度。参与系统被要求在三个主要轨道中的14种语言中按它们在意义上的接近程度（即它们的语义相关性程度）对句对进行排名：(a) 监督，(b) 无监督

    arXiv:2403.18933v1 Announce Type: new  Abstract: We present the first shared task on Semantic Textual Relatedness (STR). While earlier shared tasks primarily focused on semantic similarity, we instead investigate the broader phenomenon of semantic relatedness across 14 languages: Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by the relatively limited availability of NLP resources. Each instance in the datasets is a sentence pair associated with a score that represents the degree of semantic textual relatedness between the two sentences. Participating systems were asked to rank sentence pairs by their closeness in meaning (i.e., their degree of semantic relatedness) in the 14 languages in three main tracks: (a) supervised, (b) unsupervi
    
[^166]: SDSAT：通过具有语义自适应令牌的推测解码加速LLM推断

    SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens

    [https://arxiv.org/abs/2403.18647](https://arxiv.org/abs/2403.18647)

    SDSAT提出了一种加速大型语言模型推断的方案，通过使用具有灵活解码能力的语义自适应令牌，可以增强模型生成高质量草稿令牌的能力，并实现超过3.5倍和3.0倍的速度提升。

    

    我们提出了一种用于大型语言模型（LLMs）的加速方案，通过具有语义自适应令牌（SDSAT）进行推测解码。该设计的主要目标是增强LLM模型生成草稿标记的能力，而不影响模型的准确性。核心策略包括：1）通过合并具有灵活解码能力的语义自适应令牌来微调模型，而不改变其结构，使其能够生成高质量的草稿标记。2）通过使用不影响标准令牌的训练方法，模型可以在其原始框架之上获得并行解码能力，而训练开销最小。3）我们设计了使用贪婪搜索和核采样的“两步起草然后验证”生成策略。在CodeLlama-13B和7B模型上进行的实验结果显示，速度分别提高了3.5倍和3.0倍以上。

    arXiv:2403.18647v1 Announce Type: new  Abstract: We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary objective of this design is to enhance the LLM model's ability to generate draft tokens more accurately without compromising the model's accuracy. The core strategies involve: 1) Fine-tune the model by incorporating semantic adaptive tokens that possess flexible decoding capabilities without changing its structure, allowing them to generate high-quality draft tokens. 2) By employing a training method that does not affect the standard tokens, the model can acquire parallel decoding abilities atop its original framework with minimal training overhead. 3) We have designed the "two-step-draft-then-verify" generation strategies using both greedy search and nucleus sampling. Experiments conducted on the CodeLlama-13B and 7B models have yielded speed increases of over 3.5X and 3.0X, respectively. Ple
    
[^167]: 中国 offensive 语言检测：现状与未来方向

    Chinese Offensive Language Detection:Current Status and Future Directions

    [https://arxiv.org/abs/2403.18314](https://arxiv.org/abs/2403.18314)

    总体而言，这篇论文讨论了在中文中检测 offensive 语言的挑战，并强调了开发解决这一问题的特定模型和工具。

    

    虽然社交媒体平台正在做出相当大的努力监测和规范用户生成内容，但在数字空间中，恶意语言（如仇恨言论或网络欺凌）的普遍存在仍然是一个重要挑战。鉴于维护文明和尊重的在线环境的重要性，迫切需要能够实时检测恶意言论的自动系统。然而，为了开发处理汉语等语言的有效系统，面临着重大挑战，因为这些语言的复杂和微妙性使得自动处理变得困难。本文全面总结了中国 offensive 语言检测情况，审查了当前的基准和方法，并重点介绍了用于解决在这种复杂语言中检测恶意语言的独特挑战的特定模型和工具。

    arXiv:2403.18314v1 Announce Type: cross  Abstract: Despite the considerable efforts being made to monitor and regulate user-generated content on social media platforms, the pervasiveness of offensive language, such as hate speech or cyberbullying, in the digital space remains a significant challenge. Given the importance of maintaining a civilized and respectful online environment, there is an urgent and growing need for automatic systems capable of detecting offensive speech in real time. However, developing effective systems for processing languages such as Chinese presents a significant challenge, owing to the language's complex and nuanced nature, which makes it difficult to process automatically. This paper provides a comprehensive overview of offensive language detection in Chinese, examining current benchmarks and approaches and highlighting specific models and tools for addressing the unique challenges of detecting offensive language in this complex language. The primary object
    
[^168]: 使用区域指导标记将孟加拉文本与地方方言转录为国际音标

    Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens

    [https://arxiv.org/abs/2403.17407](https://arxiv.org/abs/2403.17407)

    通过引入区域指导标记技术，本文提出了一种将孟加拉文本与地方方言转录为国际音标的方法，为模型提供了关于输入文本的地区方言信息，以理解与每个地区相关的独特音韵模式。

    

    孟加拉文本到国际音标（IPA）的准确转录是一项具有挑战性的任务，主要是由于语言的复杂音韵学和语境相关的音变。对于区域孟加拉方言来说，由于缺乏针对这些方言的标准拼写约定、当地和外语在这些地区中流行的词汇以及不同地区之间的音韵多样性，这一挑战甚至更为严峻。本文提出了一种方法来解决这个序列到序列的问题，即在覆盖孟加拉国六个地区的新数据集上引入“区域指导标记”（DGT）技术。其关键思想是在生成IPA转录之前向模型提供有关输入文本的区域方言或“地区”的明确信息。这通过在输入序列前添加一个地区标记来实现，有效地引导模型理解与每个地区相关的独特音韵模式。

    arXiv:2403.17407v1 Announce Type: cross  Abstract: Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes. This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions. This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh. The key idea is to provide the model with explicit information about the regional dialect or "district" of the input text before generating the IPA transcription. This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each 
    
[^169]: 首先增加属性，然后生成：局部可归属的文本生成

    Attribute First, then Generate: Locally-attributable Grounded Text Generation

    [https://arxiv.org/abs/2403.17104](https://arxiv.org/abs/2403.17104)

    该论文提出了一种局部可归属的文本生成方法，通过“先增加属性，然后生成”的方式将生成过程分为内容选择、句子规划和序列句子生成三个步骤，以简化引用验证工作。

    

    最近，解决大型语言模型（LLMs）中的幻觉的努力主要集中在属性文本生成上，这种方法通过引用支持源在生成的文本中加入支持文本以进行事后事实核查和更正。然而，这些引用通常指向整个文档或段落，给用户带来了繁重的验证工作。在本文中，我们介绍了一种局部可归属的文本生成方法，重点放在简洁的属性上。我们的方法命名为“先增加属性，然后生成”，将传统的端到端生成过程分解为三个直观的步骤：内容选择、句子规划和序列句子生成。通过首先识别相关来源部分（“先选择”），然后在生成过程中对它们进行条件化（“然后生成”），我们确保这些部分也作为输出的细粒度属性（“选择”变为“属性”）。 在Mu上经过测试

    arXiv:2403.17104v1 Announce Type: new  Abstract: Recent efforts to address hallucinations in Large Language Models (LLMs) have focused on attributed text generation, which supplements generated texts with citations of supporting sources for post-generation fact-checking and corrections. Yet, these citations often point to entire documents or paragraphs, burdening users with extensive verification work. In this paper, we introduce a locally-attributable text generation approach, prioritizing concise attributions. Our method, named ``Attribute First, then Generate'', breaks down the conventional end-to-end generation process into three intuitive steps: content selection, sentence planning, and sequential sentence generation. By initially identifying relevant source segments (``select first'') and then conditioning the generation process on them (``then generate''), we ensure these segments also act as the output's fine-grained attributions (``select'' becomes ``attribute''). Tested on Mu
    
[^170]: IllusionVQA：一个挑战视觉语言模型的错觉数据集

    IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models

    [https://arxiv.org/abs/2403.15952](https://arxiv.org/abs/2403.15952)

    提出了IllusionVQA数据集，用于测试视觉语言模型在错觉和难解场景下的表现，研究发现在理解任务和定位任务上，表现最佳的VLM为GPT4V，而人类表现更胜一筹。

    

    视觉语言模型（VLM）的出现使研究人员能够使用自然语言调查神经网络的视觉理解。 VLM不仅能够进行对象分类和检测，还能够进行视觉理解和常识推理。 这自然而然地引出了一个问题：当图像本身是不合理的时，VLM会如何回应？ 为此，我们提出了IllusionVQA：一个包含具有挑战性的光学错觉和难以解释的场景的多样数据集，以测试VLM在两种不同的多选VQA任务 - 理解和软定位的能力。 表现最佳的VLM GPT4V在理解任务（4-shot）上实现了62.99％的准确率，在定位任务（4-shot和Chain-of-Thought）上实现了49.7％的准确率。 人类评估表明，人类在理解和定位方面的准确率分别为91.03％和100％。 我们发现，在上下文学习（ICL）和Chain-of-Thought推理方面有很大帮助。

    arXiv:2403.15952v1 Announce Type: cross  Abstract: The advent of Vision Language Models (VLM) has allowed researchers to investigate the visual understanding of a neural network using natural language. Beyond object classification and detection, VLMs are capable of visual comprehension and common-sense reasoning. This naturally led to the question: How do VLMs respond when the image itself is inherently unreasonable? To this end, we present IllusionVQA: a diverse dataset of challenging optical illusions and hard-to-interpret scenes to test the capability of VLMs in two distinct multiple-choice VQA tasks - comprehension and soft localization. GPT4V, the best-performing VLM, achieves 62.99% accuracy (4-shot) on the comprehension task and 49.7% on the localization task (4-shot and Chain-of-Thought). Human evaluation reveals that humans achieve 91.03% and 100% accuracy in comprehension and localization. We discover that In-Context Learning (ICL) and Chain-of-Thought reasoning substantially
    
[^171]: 从损失角度理解语言模型的突现能力

    Understanding Emergent Abilities of Language Models from the Loss Perspective

    [https://arxiv.org/abs/2403.15796](https://arxiv.org/abs/2403.15796)

    本文从损失角度重新定义了语言模型的突现能力，发现具有相同预训练损失的模型在不同任务上表现相似，而当预训练损失低于特定阈值时，模型将展现出突现能力。

    

    近期研究质疑了传统认为语言模型的突现能力仅存在于大模型中的观点。这种怀疑源自两点观察：1）较小的模型也能展现出对突现能力的高性能；2）质疑用于测量这些能力的不连续性指标。本文提议从预训练损失的角度研究突现能力，而非模型大小或训练计算。我们展示了具有相同预训练损失但不同模型和数据大小的模型，在各种下游任务上表现相同。我们还发现，当某一模型的预训练损失低于特定阈值时，在某些任务上表现出突现能力，而不论指标的连续性如何；而在达到该阈值之前，其性能仍保持在随机猜测水平。这启发我们重新定义突现能力为那些......

    arXiv:2403.15796v1 Announce Type: cross  Abstract: Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks. We also discover that a model exhibits emergent abilities on certain tasks -- regardless of the continuity of metrics -- when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that
    
[^172]: 大型语言模型的差分私有下一个标记预测

    Differentially Private Next-Token Prediction of Large Language Models

    [https://arxiv.org/abs/2403.15638](https://arxiv.org/abs/2403.15638)

    提出了Private Mixing of Ensemble Distributions (PMixED)：通过将模型的输出分布投影到公共LLM的输出分布周围的集合上，并采样平均来实现实际的下一个标记预测，以更轻量化的方式实现对隐私敏感的大型语言模型的预测。

    

    确保大型语言模型（LLMs）的隐私日益重要。DP-SGD是实现这一目标的最广泛采用的技术，它以一种保证差分隐私的方式训练模型。然而，DP-SGD需要比SGD更长的训练时间和更大的内存需求，同时过高估计对手具有白盒访问模型的能力。更现实的场景假设只有对隐私敏感的LLM进行黑盒访问。在这些观察的基础上，我们提出了私有混合集合分布（PMixED）：一种通过将模型的每个输出分布从一个经过精细调整的LLM集合投影到公共LLM输出分布周围的集合上，然后对投影分布进行平均并从中抽样来实现实际的下一个标记预测的私有预测协议。我们的方法比DP-SGD更轻量化，因为它与模型无关。

    arXiv:2403.15638v1 Announce Type: cross  Abstract: Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly important. The most widely adopted technique to accomplish this is DP-SGD, which trains a model in such a way that guarantees Differential Privacy (DP). However, DP-SGD requires longer training times and larger memory requirements than SGD, while overestimating an adversary's capabilities in having white box access to the model. A more realistic scenario assumes only black-box access to a privacy-sensitive LLM. Motivated by these observations, we present Private Mixing of Ensemble Distributions (PMixED): a private prediction protocol that achieves practical next-token prediction by projecting each of the model's output distribution from an ensemble of fine-tuned LLMs onto a set around a public LLM's output distribution, then averaging the projected distributions and sampling from it. Our approach is more lightweight than DP-SGD in that it is model agnostic, i
    
[^173]: NaturalTurn：一种将转录件分割成自然对话转折的方法

    NaturalTurn: A Method to Segment Transcripts into Naturalistic Conversational Turns

    [https://arxiv.org/abs/2403.15615](https://arxiv.org/abs/2403.15615)

    NaturalTurn是一种专门设计用于准确捕捉自然对话交流动态的轮次分割算法，通过区分说话者的主要对话轮次和听众的次要话语，能够比现有方法更好地提取转录信息。

    

    arXiv:2403.15615v1 公告类型: 新的 摘要: 对话是社会、认知和计算科学越来越感兴趣的主题。然而，随着对话数据集的规模和复杂性不断增加，研究人员缺乏可伸缩的方法将语音转录转换为会话轮次——社会互动的基本构建模块。我们介绍了“NaturalTurn”，一种旨在准确捕捉自然交流动态的轮次分割算法。NaturalTurn通过区分说话者的主要对话轮次和听众的次要话语，如背景声、简短插话和其他表现对话特征的平行言语形式，来运作。使用大型对话语料库的数据，我们展示了与现有方法派生的转录相比，NaturalTurn派生的转录表现出有利的统计和推断特性。NaturalTurn算法代表了一种改进。

    arXiv:2403.15615v1 Announce Type: new  Abstract: Conversation is the subject of increasing interest in the social, cognitive, and computational sciences. And yet, as conversational datasets continue to increase in size and complexity, researchers lack scalable methods to segment speech-to-text transcripts into conversational turns--the basic building blocks of social interaction. We introduce "NaturalTurn," a turn segmentation algorithm designed to accurately capture the dynamics of naturalistic exchange. NaturalTurn operates by distinguishing speakers' primary conversational turns from listeners' secondary utterances, such as backchannels, brief interjections, and other forms of parallel speech that characterize conversation. Using data from a large conversation corpus, we show how NaturalTurn-derived transcripts demonstrate favorable statistical and inferential characteristics compared to transcripts derived from existing methods. The NaturalTurn algorithm represents an improvement i
    
[^174]: LLaVA-PruMerge: 自适应令牌减少用于高效大型多模态模型

    LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models

    [https://arxiv.org/abs/2403.15388](https://arxiv.org/abs/2403.15388)

    PruMerge提出了一种自适应的视觉令牌减少方法，可以有效减少大型多模态模型中的视觉令牌数量，同时保持模型性能。

    

    大型多模态模型(LMMs)通过连接视觉编码器和大型语言模型展现了显著的推理能力。最近的LMMs包括了更复杂的视觉输入，如高分辨率图像和视频，这显著增加了视觉令牌的数量。为了解决这个问题，我们探索了一种令牌减少机制，并发现类似于先前的工作，许多视觉令牌在空间上是冗余的。基于此，我们提出了PruMerge，一种新颖的自适应视觉令牌减少方法，大大减少了视觉令牌的数量，同时保持了可比的模型性能。

    arXiv:2403.15388v1 Announce Type: cross  Abstract: Large Multimodal Models (LMMs) have shown significant reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically use a fixed amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which increase the number of visual tokens significantly. However, due to the design of the Transformer architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many visual tokens are spatially redundant. Based on this, we propose PruMerge, a novel adaptive visual token reduction approach, which largely reduces the number of visual tokens while maintaining comparable model performance. We first select 
    
[^175]: 融合上下文的多审阅文本生成

    Multi-Review Fusion-in-Context

    [https://arxiv.org/abs/2403.15351](https://arxiv.org/abs/2403.15351)

    本文提出了一种融合上下文的多审阅文本生成方法，开发了评论领域的数据集，并提出了评估框架。

    

    地面文本生成涵盖了诸如长篇问答和摘要等任务，需要内容选择和内容整合。当前的端到端方法由于不透明性而难以控制和解释。因此，最近的研究提出了一种模块化方法，为每个步骤都提供单独的组件。具体而言，我们专注于生成连贯文本的第二子任务，即在多文档环境中给定预选内容。我们将\textit{Fusion-in-Context}(FiC)具体化为一个独立任务，其输入包括带有目标内容高亮部分的源文本。然后，模型需要生成一个包含所有且仅包含目标信息的连贯段落。我们的工作包括在评论领域开发了一个包含1000个实例的精心策划数据集，以及一个用于评估高亮真实性和涵盖范围的新颖评估框架。

    arXiv:2403.15351v1 Announce Type: new  Abstract: Grounded text generation, encompassing tasks such as long-form question-answering and summarization, necessitates both content selection and content consolidation. Current end-to-end methods are difficult to control and interpret due to their opaqueness. Accordingly, recent works have proposed a modular approach, with separate components for each step. Specifically, we focus on the second subtask, of generating coherent text given pre-selected content in a multi-document setting. Concretely, we formalize \textit{Fusion-in-Context} (FiC) as a standalone task, whose input consists of source texts with highlighted spans of targeted content. A model then needs to generate a coherent passage that includes all and only the target information. Our work includes the development of a curated dataset of 1000 instances in the reviews domain, alongside a novel evaluation framework for assessing the faithfulness and coverage of highlights, which stro
    
[^176]: 通过包容性和抽象性连续刻度规范泛指性

    Specifying Genericity through Inclusiveness and Abstractness Continuous Scales

    [https://arxiv.org/abs/2403.15278](https://arxiv.org/abs/2403.15278)

    该论文介绍了一种新的注释框架，用于对自然语言中名词短语的泛指性进行细粒度建模，通过连续的评注方法捕捉了泛指性的微妙方面，为语言学家提供了实用资源。

    

    这篇论文介绍了一种新的注释框架，用于对自然语言中名词短语（NPs）的泛指性进行细粒度建模。该框架设计简单直观，适用于非专家注释者并适合众包任务。结合有关泛指性的理论和认知文献，该框架根植于已建立的语言学理论。通过一项试点研究，我们创建了一个包含324个句子的小而关键的注释数据集，为未来研究奠定基础。为验证我们的方法，我们进行了一项评估，比较了我们连续注释与相同数据集上现有的二元注释，证明了该框架在捕捉泛指性的微妙方面上的有效性。我们的工作为语言学家提供了一个实用资源，提供了一个第一个经过注释的数据集和一个旨在构建可用于研究的实际语言数据集的注释方案。

    arXiv:2403.15278v1 Announce Type: new  Abstract: This paper introduces a novel annotation framework for the fine-grained modeling of Noun Phrases' (NPs) genericity in natural language. The framework is designed to be simple and intuitive, making it accessible to non-expert annotators and suitable for crowd-sourced tasks. Drawing from theoretical and cognitive literature on genericity, this framework is grounded in established linguistic theory. Through a pilot study, we created a small but crucial annotated dataset of 324 sentences, serving as a foundation for future research. To validate our approach, we conducted an evaluation comparing our continuous annotations with existing binary annotations on the same dataset, demonstrating the framework's effectiveness in capturing nuanced aspects of genericity. Our work offers a practical resource for linguists, providing a first annotated dataset and an annotation scheme designed to build real-language datasets that can be used in studies on
    
[^177]: ReAct遇上ActRe：对比性自训练中的代理轨迹自动标注

    ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training

    [https://arxiv.org/abs/2403.14589](https://arxiv.org/abs/2403.14589)

    提出了A$^3$T框架，通过ActRe提示代理实现了ReAct风格代理对代理轨迹的自主标注，同时增强了新的轨迹合成能力。

    

    arXiv:2403.14589v1 公告类型：新 文摘：语言代理通过与基础模型推理展示了自主决策能力。最近，人们致力于通过多步推理和行动轨迹作为训练数据来训练语言代理以提高性能。然而，收集这样的轨迹仍需要相当大的人力，无论是通过人工标注还是实施多样化提示框架。在这项工作中，我们提出了A$^3$T，一个允许以ReAct风格自主注释代理轨迹的框架。其中心是一个ActRe提示代理，它解释任意动作的原因。当随机抽取外部动作时，ReAct风格代理可以查询ActRe代理以获取其文本理由。新颖的轨迹然后通过将ActRe的后验推理前置到抽样动作中进行综合合成。通过这种方式，ReAct风格代理可执行

    arXiv:2403.14589v1 Announce Type: new  Abstract: Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent exe
    
[^178]: C-TPT：通过文本特征离散性的校准测试时提示调整视觉-语言模型

    C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion

    [https://arxiv.org/abs/2403.14119](https://arxiv.org/abs/2403.14119)

    本文研究了在测试时提示调整过程中通过利用CLIP的固有属性来探讨校准的方法，发现提示选择显著影响了CLIP中的校准，其中导致更高文本特征离散性的提示会产生更好校准的预测。

    

    在深度学习中，测试时适应已经引起了人们的关注，作为一种在不需要标记数据的情况下对模型进行微调的方法。一个主要的例证是最近提出的用于大规模视觉-语言模型（如CLIP）的测试时提示调整。然而，这些提示主要是为了提高准确性而开发的，忽视了校准的重要性——量化预测不确定性的关键方面。然而，传统的校准方法依赖大量标记数据，这使得它们在测试时场景下不切实际。为此，本文通过利用CLIP的固有属性，在测试时提示调整过程中探讨校准。通过一系列观察，我们发现提示选择显著影响了CLIP中的校准，其中导致更高文本特征离散性的提示会产生更好校准的预测。

    arXiv:2403.14119v1 Announce Type: cross  Abstract: In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration-a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion
    
[^179]: ZigMa：蜿蜒曼巴扩散模型

    ZigMa: Zigzag Mamba Diffusion Model

    [https://arxiv.org/abs/2403.13802](https://arxiv.org/abs/2403.13802)

    本研究提出了一种名为Zigzag Mamba的零参数方法，通过纠正当前Mamba-based视觉方法中对空间连续性的忽视，实现了更好的速度和内存利用，同时在大分辨率视觉数据集上展示了出色的性能。

    

    扩散模型长期以来一直受到可伸缩性和二次复杂性问题的困扰，特别是在基于变压器的结构内部。在这项研究中，我们旨在利用一种称为曼巴的状态空间模型的长序列建模能力，以扩展其在视觉数据生成中的适用性。首先，我们确定了大多数当前基于曼巴的视觉方法中的一个关键疏忽，即曼巴的扫描方案中缺乏对空间连续性的考虑。其次，基于这一洞察力，我们介绍了一种名为Zigzag Mamba的简单、即插即用、零参数方法，它优于基于曼巴的基线，并表现出比基于变压器的基线更快速和更好的内存利用。最后，我们将Zigzag Mamba集成到随机插值框架中，以研究模型在大分辨率视觉数据集（例如FacesHQ $1024\times 1024$和UCF101，MultiModal-CelebA-HQ）上的可伸缩性。

    arXiv:2403.13802v1 Announce Type: cross  Abstract: The diffusion model has long been plagued by scalability and quadratic complexity issues, especially within transformer-based structures. In this study, we aim to leverage the long sequence modeling capability of a State-Space Model called Mamba to extend its applicability to visual data generation. Firstly, we identify a critical oversight in most current Mamba-based vision methods, namely the lack of consideration for spatial continuity in the scan scheme of Mamba. Secondly, building upon this insight, we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba, which outperforms Mamba-based baselines and demonstrates improved speed and memory utilization compared to transformer-based baselines. Lastly, we integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate the scalability of the model on large-resolution visual datasets, such as FacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ
    
[^180]: 利用大型语言模型和真实机器人账户激励社交媒体平台上的新闻消费

    Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts

    [https://arxiv.org/abs/2403.13362](https://arxiv.org/abs/2403.13362)

    通过创建使用 GPT-2 的机器人账户，在社交媒体平台上回复用户的推文，鼓励用户接触和关注验证的、意识形态平衡的新闻，以增加用户接触这些新闻并提高参与度。

    

    极化、信任下降以及对民主规范支持动摇是美国民主面临的紧迫威胁。接触验证和优质新闻可能降低个人对这些威胁的易感性，并使公民更具抗击错误信息、民粹主义和极端党派言论的能力。该项目探讨了如何在一个生态有效的环境中增强用户接触和参与验证的、意识形态平衡的新闻。我们依赖于对 28,457 个 Twitter 用户进行的大规模为期两周的田野实验（从 2023 年 1 月 19 日到 2 月 3 日）。我们创建了 28 个利用 GPT-2 的机器人，在用户发表有关体育、娱乐或生活方式的推文时回复一个内容相关的回复，其中包含两个硬代码元素：一个指向优质新闻机构相关主题部分的 URL 和鼓励关注其 Twitter 账户。为进一步测试机器人对性别的差异影响，被试用户被随机分配以接受...

    arXiv:2403.13362v1 Announce Type: cross  Abstract: Polarization, declining trust, and wavering support for democratic norms are pressing threats to U.S. democracy. Exposure to verified and quality news may lower individual susceptibility to these threats and make citizens more resilient to misinformation, populism, and hyperpartisan rhetoric. This project examines how to enhance users' exposure to and engagement with verified and ideologically balanced news in an ecologically valid setting. We rely on a large-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on 28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users tweeting about sports, entertainment, or lifestyle with a contextual reply containing two hardcoded elements: a URL to the topic-relevant section of quality news organization and an encouragement to follow its Twitter account. To further test differential effects by gender of the bots, treated users were randomly assigned to receive re
    
[^181]: X-LLaVA: 优化双语大规模视觉语言对齐

    X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment

    [https://arxiv.org/abs/2403.11399](https://arxiv.org/abs/2403.11399)

    提出了两种成本有效的方法解决大规模多模态模型训练数据的挑战，并在英语-韩语-中文多语言、多模态训练数据集上开发了表现优越的双语多模态模型。

    

    大规模语言模型（LLMs）的显著发展正在扩展到大规模多模态模型（LMMs）的领域，这些模型集成了除文本以外的多种数据类型。然而，多模态模型的特性导致在创建训练数据方面存在显着的开销。此外，为LMMs构建多语言数据也面临着语言多样性和复杂性的挑战。因此，在这项研究中，我们提出了两种成本有效的方法来解决这个问题：（1）多语言LLM的词汇扩展和预训练，以及（2）使用GPT4-V自动和精心构建多模态数据集。基于这些方法，我们构建了一个包含91K英文-韩文-中文的多语言、多模态训练数据集。此外，我们开发了一个双语多模态模型，在韩语和英语中表现出卓越的性能，超过了现有方法。

    arXiv:2403.11399v1 Announce Type: new  Abstract: The impressive development of large language models (LLMs) is expanding into the realm of large multimodal models (LMMs), which incorporate multiple types of data beyond text. However, the nature of multimodal models leads to significant expenses in the creation of training data. Furthermore, constructing multilingual data for LMMs presents its own set of challenges due to language diversity and complexity. Therefore, in this study, we propose two cost-effective methods to solve this problem: (1) vocabulary expansion and pretraining of multilingual LLM for specific languages, and (2) automatic and elaborate construction of multimodal datasets using GPT4-V. Based on015 these methods, we constructed a 91K English-Korean-Chinese multilingual, multimodal training dataset. Additionally, we developed a bilingual multimodal model that exhibits excellent performance in both Korean and English, surpassing existing approaches.
    
[^182]: 什么让大型语言模型对数学问题难以应对？

    What Makes Math Word Problems Challenging for LLMs?

    [https://arxiv.org/abs/2403.11369](https://arxiv.org/abs/2403.11369)

    研究了大型语言模型在处理数学问题的文字题目时所面临的困难，通过分析特征和训练分类器来预测模型对不同类型题目的表现。

    

    这篇论文研究了对大型语言模型(LLMs)而言，什么让数学问题的文字题目(math word problems, MWPs)变得具有挑战性。我们深入分析了MWPs的关键语言和数学特征。此外，我们训练了基于特征的分类器，以更好地理解每个特征对于LLMs日常任务中MWPs的整体难度的影响，并探究这是否有助于预测LLMs在特定类别的MWPs上的表现。

    arXiv:2403.11369v1 Announce Type: new  Abstract: This paper investigates the question of what makes math word problems (MWPs) challenging for large language models (LLMs). We conduct an in-depth analysis of the key linguistic and mathematical characteristics of MWPs. In addition, we train feature-based classifiers to better understand the impact of each feature on the overall difficulty of MWPs for prominent LLMs and investigate whether this helps predict how well LLMs fare against specific categories of MWPs.
    
[^183]: 在人类对齐中扩展数据多样性以微调语言模型

    Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment

    [https://arxiv.org/abs/2403.11124](https://arxiv.org/abs/2403.11124)

    更多的响应但更少的提示可以更好地触发大型语言模型进行人类对齐，此外，提出了一种新的提示多样性公式，可以进一步影响LLMs的最终性能。

    

    与人类偏好对齐可以防止大型语言模型（LLMs）生成误导性或有毒内容，同时需要高成本的人类反馈。假设人工注释资源有限，则可以考虑两种不同的分配方式：更多样化的提示或更多样化的待标记响应。然而，它们对结果的影响的直接比较尚不存在。在这项工作中，我们首先根据微调样本数量控制双方的多样性，这可以直接反映它们的影响。我们发现，与大量提示不同，更多的响应但是更少的提示更能激发LLMs进行人类对齐。此外，提示的多样性概念可能比通常由单个数字量化的响应更复杂。因此，提出了提示多样性的新公式，进一步暗示与微调后LLMs最终性能的线性相关。

    arXiv:2403.11124v1 Announce Type: cross  Abstract: Alignment with human preference prevents large language models (LLMs) from generating misleading or toxic content while requiring high-cost human feedback. Assuming resources of human annotation are limited, there are two different ways of allocating considered: more diverse PROMPTS or more diverse RESPONSES to be labeled. Nonetheless, a straightforward comparison between their impact is absent. In this work, we first control the diversity of both sides according to the number of samples for fine-tuning, which can directly reflect their influence. We find that instead of numerous prompts, more responses but fewer prompts better trigger LLMs for human alignment. Additionally, the concept of diversity for prompts can be more complex than responses that are typically quantified by single digits. Consequently, a new formulation of prompt diversity is proposed, further implying a linear correlation with the final performance of LLMs after f
    
[^184]: MT-PATCHER：来自大型语言模型的有选择性和可扩展的知识蒸馏用于机器翻译

    MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation

    [https://arxiv.org/abs/2403.09522](https://arxiv.org/abs/2403.09522)

    提出了MT-Patcher框架，实现了从大型语言模型到中等规模机器翻译模型的有选择性、全面和主动的知识迁移

    

    大型语言模型（LLM）在机器翻译（MT）领域展现出强大的能力，但它们面临着高计算成本和延迟的问题。因此，将翻译知识从巨型LLM转移到中等规模的机器翻译模型是一个有前途的研究方向。本文提出了一个名为MT-Patcher的框架，以选择性、全面和主动的方式将知识从LLMs转移到现有的MT模型中。考虑到学生MT模型当前的翻译能力，我们仅识别和纠正其翻译错误，而不是从老师那里蒸馏整个翻译。

    arXiv:2403.09522v1 Announce Type: new  Abstract: Large Language Models (LLM) have demonstrated their strong ability in the field of machine translation (MT), yet they suffer from high computational cost and latency. Therefore, transferring translation knowledge from giant LLMs to medium-sized machine translation models is a promising research direction. However, traditional knowledge distillation methods do not take the capability of student and teacher models into consideration, therefore repeatedly teaching student models on the knowledge they have learned, and failing to extend to novel contexts and knowledge. In this paper, we propose a framework called MT-Patcher, which transfers knowledge from LLMs to existing MT models in a selective, comprehensive and proactive manner. Considering the current translation ability of student MT models, we only identify and correct their translation errors, instead of distilling the whole translation from the teacher. Leveraging the strong languag
    
[^185]: Ethos：在正交参数空间中矫正语言模型

    Ethos: Rectifying Language Models in Orthogonal Parameter Space

    [https://arxiv.org/abs/2403.08994](https://arxiv.org/abs/2403.08994)

    Ethos提出了一种新的高效方法，通过在任务向量上进行矫正LMs以减轻产生毒性和偏见输出以及避免隐私泄露。

    

    语言模型（LMs）极大推动了自然语言处理研究的发展。然而，LMs也引发了关于生成偏见或有毒内容以及训练数据集中私人信息可能泄露的担忧。在这项工作中，我们提出了一种新的高效方法，Ethos，通过在任务向量上进行矫正LMs以减轻产生毒性和偏见输出以及避免隐私泄露。Ethos建立在任务算术基础上。然而，与当前的任务算法不同的是，Ethos在重构任务向量时区分了一般有益和不良知识。具体而言，Ethos首先使用奇异值分解从预训练模型中获得一组主成分。然后，通过将任务向量投影到主成分上，Ethos识别编码一般或不良知识的主成分。Ethos仅使用带有不良知识的任务向量进行否定，从而最小

    arXiv:2403.08994v1 Announce Type: new  Abstract: Language models (LMs) have greatly propelled the research on natural language processing. However, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training dataset. In this work, we present a new efficient approach, Ethos, that rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy leakage. Ethos is built on task arithmetic. However, unlike current task arithmetic algorithms, Ethos distinguishes general beneficial and undesired knowledge when reconstructing task vectors. Specifically, Ethos first obtains a set of principal components from the pre-trained models using singular value decomposition. Then, by projecting the task vector onto principal components, Ethos identifies the principal components that encode general or undesired knowledge. Ethos performs negating using the task vector with undesired knowledge only, thereby minimi
    
[^186]: 基于矩阵变换的低秩调整（MTLoRA）：一种受大脑启发的参数高效微调方法

    Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning

    [https://arxiv.org/abs/2403.07440](https://arxiv.org/abs/2403.07440)

    该论文提出了基于矩阵变换的低秩调整（MTLoRA）方法，受大脑启发，用于提高微调技术的复杂任务适应性、性能、稳定性和算法复杂性。

    

    基于大型预训练语言模型（LPLMs）的微调技术已被证明可以显著提高模型在各种下游任务上的性能，并有效控制LPLMs的输出行为。本文受大脑功能受其几何结构塑造的启发，将这一思想融入LoRA技术中，提出了一种新的基于矩阵变换的重新参数化方法，以减少复杂任务适应性、性能、稳定性和算法复杂性方面的改进空间。

    arXiv:2403.07440v1 Announce Type: cross  Abstract: Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs. Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources. Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity. In response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameteriz
    
[^187]: SVD-LLM: 针对大型语言模型压缩的截断感知奇异值分解

    SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression

    [https://arxiv.org/abs/2403.07378](https://arxiv.org/abs/2403.07378)

    SVD-LLM是一种新的基于SVD的LLM压缩方法，通过截断感知数据白化策略和逐层闭式模型参数更新策略，解决了现有方法的限制，实现了直接映射奇异值和压缩损失之间的关系。

    

    大型语言模型（LLMs）的进展受到其庞大尺寸的限制，这需要LLM压缩方法以实现实际部署。奇异值分解（SVD）为LLM压缩提供了一个有希望的解决方案。然而，现有的基于SVD的LLM压缩方法存在两个关键限制：截断较小的奇异值可能导致更高的压缩损失，并且在SVD截断后剩余模型参数的更新缺失。在这项工作中，我们提出了SVD-LLM，一种新的基于SVD的LLM压缩方法，解决了现有方法的限制。SVD-LLM采用了一种截断感知的数据白化策略，以确保奇异值和压缩损失之间的直接映射。此外，SVD-LLM采用一种逐层闭式模型参数更新策略，以弥补SVD截断引起的准确性降低。我们在总共11个数据集和七个m上评估了SVD-LLM。

    arXiv:2403.07378v1 Announce Type: new  Abstract: The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven m
    
[^188]: FLAP: 在LLMs中具有受限解码的流程遵循规划

    FLAP: Flow Adhering Planning with Constrained Decoding in LLMs

    [https://arxiv.org/abs/2403.05766](https://arxiv.org/abs/2403.05766)

    本文研究了在任务导向对话中通过遵循预定义流程和保留API依赖性解决用户意图的忠实规划，提出了一种基于前瞻启发式的受限解码算法。

    

    计划对于任务导向对话中的代理人是至关重要的任务。人类代理人通常通过遵循预定义的工作流程解决用户问题，将工作流程步骤分解为可操作项目，并通过执行API执行操作；所有这些都需要推理和规划。鉴于LLMs的最新进展，人们越来越多地尝试使用LLMs进行任务规划和API使用。然而，由于LLMs偏向预训练数据，计划与预定义工作流程和API依赖性的忠实性并不被保证。此外，在现实生活中，工作流程是自定义的并且容易更改，因此，快速使代理人适应变化是可取的。在本文中，我们研究了在任务导向对话中通过遵循预定义流程和保留API依赖性解决用户意图的忠实规划。我们提出了一种基于前瞻启发式的受限解码算法用于忠实规划。

    arXiv:2403.05766v1 Announce Type: new  Abstract: Planning is a crucial task for agents in task oriented dialogs (TODs). Human agents typically resolve user issues by following predefined workflows, decomposing workflow steps into actionable items, and performing actions by executing APIs in order; all of which require reasoning and planning. With the recent advances in LLMs, there have been increasing attempts to use LLMs for task planning and API usage. However, the faithfulness of the plans to predefined workflows and API dependencies, is not guaranteed with LLMs because of their bias towards pretraining data. Moreover, in real life, workflows are custom-defined and prone to change, hence, quickly adapting agents to the changes is desirable. In this paper, we study faithful planning in TODs to resolve user intents by following predefined flows and preserving API dependencies. We propose a constrained decoding algorithm based on lookahead heuristic for faithful planning. Our algorithm
    
[^189]: Alpaca对抗Vicuna：使用LLMs揭示LLMs的记忆化

    Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs

    [https://arxiv.org/abs/2403.04801](https://arxiv.org/abs/2403.04801)

    使用LLM代理进行黑盒提示优化方法，揭示了受害代理中更高级别的记忆化，相比直接用训练数据提示目标模型，这种方法更有效，能更好地量化LLMs的记忆化。

    

    在本文中，我们介绍了一种黑盒提示优化方法，该方法利用攻击者LLM代理来揭示受害代理中更高级别的记忆化，与直接用训练数据提示目标模型相比，这是量化LLMs记忆化的主导方法。我们使用迭代的拒绝抽样优化过程来找到基于指令的提示，具有两个主要特征：(1)与训练数据最小重叠，以避免直接向模型呈现解决方案，以及(2)受害模型输出与训练数据的最大重叠，旨在诱使受害者吐出训练数据。我们观察到，我们基于指令的提示生成的输出与训练数据重叠程度比基线前缀后缀测量高出23.7％。我们的发现表明，(1)经过指令调整的模型可以暴露与他们的基本模型一样多的预训练数据。

    arXiv:2403.04801v1 Announce Type: new  Abstract: In this paper, we introduce a black-box prompt optimization method that uses an attacker LLM agent to uncover higher levels of memorization in a victim agent, compared to what is revealed by prompting the target model with the training data directly, which is the dominant approach of quantifying memorization in LLMs. We use an iterative rejection-sampling optimization process to find instruction-based prompts with two main characteristics: (1) minimal overlap with the training data to avoid presenting the solution directly to the model, and (2) maximal overlap between the victim model's output and the training data, aiming to induce the victim to spit out training data. We observe that our instruction-based prompts generate outputs with 23.7% higher overlap with training data compared to the baseline prefix-suffix measurements. Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, 
    
[^190]: ParallelPARC: 生成自然语言类比的可扩展流水线

    ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies

    [https://arxiv.org/abs/2403.01139](https://arxiv.org/abs/2403.01139)

    设计了ParallelPARC流水线，利用大型语言模型生成复杂段落类比数据集，评估各种类比类型，并展示出人类在类比识别中的优势。

    

    Analogy-making对于人类认知至关重要，使我们能够适应新颖情境--这是当前人工智能系统仍然缺乏的能力。大多数类比数据集今天关注简单的类比（例如，词类比）；包含复杂类型类比的数据集通常是手工策划的，并且非常小。我们认为这限制了计算类比的进展。在这项工作中，我们设计了一个数据生成流水线，ParallelPARC（Parallel Paragraph Creator），利用最先进的大型语言模型（LLM）来创建基于段落的复杂类比，以及简单和具有挑战性的干扰项。我们展示了我们的流水线，并创建了ProPara-Logy，一个关于科学过程间类比的数据集。我们发布了一个由人类验证过的金标准数据集，以及一个自动生成的银标准数据集。我们在二进制和多选环境中测试了LLMs和人类对类比的识别，发现人类胜过最佳模型。

    arXiv:2403.01139v1 Announce Type: cross  Abstract: Analogy-making is central to human cognition, allowing us to adapt to novel situations -- an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g., word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy. In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to create complex, paragraph-based analogies, as well as distractors, both simple and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset of analogies between scientific processes. We publish a gold-set, validated by humans, and a silver-set, generated automatically. We test LLMs' and humans' analogy recognition in binary and multiple-choice settings, and found that humans outperform the best mod
    
[^191]: UrbanGPT: 时空大型语言模型

    UrbanGPT: Spatio-Temporal Large Language Models

    [https://arxiv.org/abs/2403.00813](https://arxiv.org/abs/2403.00813)

    都市GPT旨在建立一个具有强大泛化能力的时空模型，借鉴大型语言模型的成就。

    

    都市GPT旨在预测并洞察城市环境在时间和空间上不断变化的动态。其目的是预测都市生活各个方面的未来模式、趋势和事件，包括交通、人口流动和犯罪率等。尽管已经付出了大量努力开发神经网络技术以准确预测时空数据，但需注意到很多方法在生成精确的时空表示时严重依赖于有足够标记的数据。不幸的是，在实际都市感知场景中，数据稀缺是一个普遍存在的问题。因此，建立一个具有强大泛化能力的时空模型跨越多样时空学习场景是必要的。受大型语言模型(LLM)卓越成就的启发，我们的目标是

    arXiv:2403.00813v1 Announce Type: cross  Abstract: Spatio-temporal prediction aims to forecast and gain insights into the ever-changing dynamics of urban environments across both time and space. Its purpose is to anticipate future patterns, trends, and events in diverse facets of urban life, including transportation, population movement, and crime rates. Although numerous efforts have been dedicated to developing neural network techniques for accurate predictions on spatio-temporal data, it is important to note that many of these methods heavily depend on having sufficient labeled data to generate precise spatio-temporal representations. Unfortunately, the issue of data scarcity is pervasive in practical urban sensing scenarios. Consequently, it becomes necessary to build a spatio-temporal model with strong generalization capabilities across diverse spatio-temporal learning scenarios. Taking inspiration from the remarkable achievements of large language models (LLMs), our objective is 
    
[^192]: 通过对抗攻击实现抗LLM的数学问题生成

    LLM-Resistant Math Word Problem Generation via Adversarial Attacks

    [https://arxiv.org/abs/2402.17916](https://arxiv.org/abs/2402.17916)

    本研究提出了一种新方法，通过生成保留原问题结构难度但针对LLMs无解的对抗性示例，有效地降低了LLMs的数学问题解决能力。

    

    大型语言模型（LLMs）显著改变了教育领域。本文探讨了一种新的范例，生成对抗性示例，以确保公平评估，这些示例保留了原始问题的结构和难度，但LLMs无法解决。我们专注于数学应用领域的词问题，利用抽象语法树结构生成对抗示例，通过简单编辑问题中的数字值，导致LLMs产生错误答案。我们对各种开源和闭源LLMs进行实验，定量和定性地证明我们的方法显著降低了它们的数学问题解决能力。

    arXiv:2402.17916v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly transformed the educational landscape. As current plagiarism detection tools struggle to keep pace with LLMs' rapid advancements, the educational community faces the challenge of assessing students' true problem-solving abilities in the presence of LLMs. In this work, we explore a new paradigm for ensuring fair evaluation -- generating adversarial examples which preserve the structure and difficulty of the original questions aimed for assessment, but are unsolvable by LLMs. Focusing on the domain of math word problems, we leverage abstract syntax trees to structurally generate adversarial examples that cause LLMs to produce incorrect answers by simply editing the numeric values in the problems. We conduct experiments on various open- and closed-source LLMs, quantitatively and qualitatively demonstrating that our method significantly degrades their math problem-solving ability. We identify
    
[^193]: StructLM: 朝向构建结构化知识连接的通用模型

    StructLM: Towards Building Generalist Models for Structured Knowledge Grounding

    [https://arxiv.org/abs/2402.16671](https://arxiv.org/abs/2402.16671)

    StructLM系列模型基于全面指令调整数据集，超越了大多数任务特定模型，在结构化知识连接任务中取得了新的最先进成就。

    

    结构化数据源，如表格、图形和数据库，是普遍存在的知识源。尽管大型语言模型（LLM）在纯文本上表现出色，但它们在解释和利用结构化数据方面的能力仍然有限。我们的研究揭示了LLM在处理结构化数据方面的显着不足，例如，ChatGPT平均落后于最先进模型(SoTA)35%。为增强LLM中的结构化知识连接（SKG）能力，我们开发了一个包含110万个示例的全面指令调整数据集。利用这个数据集，我们训练了一系列基于Code-LLaMA架构的模型，称为StructLM，参数范围从7B到34B。我们的StructLM系列在18个评估数据集中有14个超越了特定任务的模型，并在7个SKG任务上确立了新的SoTA成就。此外，StructLM展现了卓越的泛化能力。

    arXiv:2402.16671v1 Announce Type: new  Abstract: Structured data sources, such as tables, graphs, and databases, are ubiquitous knowledge sources. Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35%. To augment the Structured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a comprehensive instruction tuning dataset comprising 1.1 million examples. Utilizing this dataset, we train a series of models, referred to as StructLM, based on the Code-LLaMA architecture, ranging from 7B to 34B parameters. Our StructLM series surpasses task-specific models on 14 out of 18 evaluated datasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore, StructLM demonstrates exceptional generalizat
    
[^194]: ArEEG_Chars: 用于基于脑电图的设想语音识别的阿拉伯字符数据集

    ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters

    [https://arxiv.org/abs/2402.15733](https://arxiv.org/abs/2402.15733)

    该论文介绍了一种用于阿拉伯字符的EEG数据集ArEEG_Chars，通过深度学习实现97%的准确率，在脑机接口中具有重要意义。

    

    脑机接口（BCI）是近年来热门的研究课题，可以帮助瘫痪患者改善生活。有几项研究自动将脑电图（EEG）信号分类为英文字符和单词。阿拉伯语是世界上使用最广泛的语言之一。然而据我们所知，目前没有针对阿拉伯字符的脑电图信号数据集。在本文中，我们创建了一个用于阿拉伯字符的EEG数据集，并命名为ArEEG_Chars。此外，我们使用深度学习对ArEEG_Chars进行了多项实验。在使用LSTM时获得了最佳结果，准确率达到97%。ArEEG_Chars数据集将对研究人员公开。

    arXiv:2402.15733v1 Announce Type: cross  Abstract: Brain-Computer-Interface (BCI) has been a hot research topic in the last few years that could help paralyzed people in their lives. Several researches were done to classify electroencephalography (EEG) signals automatically into English characters and words. Arabic language is one of the most used languages around the world. However, to the best of our knowledge, there is no dataset for Arabic characters EEG signals. In this paper, we have created an EEG dataset for Arabic characters and named it ArEEG_Chars. Moreover, several experiments were done on ArEEG_Chars using deep learning. Best results were achieved using LSTM and reached an accuracy of 97%. ArEEG_Chars dataset will be public for researchers.
    
[^195]: Triad: 一个利用基于多角色LLM代理的框架来解决知识库问答问题

    Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering

    [https://arxiv.org/abs/2402.14320](https://arxiv.org/abs/2402.14320)

    Triad框架利用了基于多角色LLM代理来解决知识库问答问题，通过代理的不同角色分别处理KBQA子任务，合作完成KBQA任务，并在多个基准数据集上表现出色。

    

    最近基于LLM代理的进展在各种任务中展现出了令人期待的结果。然而，它们在回答知识库中问题的运用仍然鲜为人知。使用传统方法来实现KBQA系统具有挑战性，因为缺乏特定任务训练数据以及创建以任务为中心的模型结构的复杂性。在本文中，我们提出了Triad，一个利用具有三个角色的LLM代理的统一框架来进行KBQA任务。代理被分配三个角色来处理不同的KBQA子任务：作为掌握各种子任务的通才，作为选择候选者的决策者，以及作为回答带有知识的问题的顾问。我们的KBQA框架在四个阶段中执行，涉及代理的多重角色的协作。我们使用三个基准数据集评估了我们框架的性能，结果显示我们的框架胜过了

    arXiv:2402.14320v1 Announce Type: cross  Abstract: Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms 
    
[^196]: TofuEval：评估LLM在主题对话摘要中的幻觉

    TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization

    [https://arxiv.org/abs/2402.13249](https://arxiv.org/abs/2402.13249)

    论文提出了一个新的主题对话摘要评估基准TofuEval，研究发现现有的LLMs在对话领域存在大量事实错误的幻觉，并表明当LLMs充当事实评估器时，其表现不佳。

    

    单文档新闻摘要在忠实度方面取得了长足进步，这得益于对事实一致性或幻觉评估的研究。我们探讨了这些进展是否能延伸到其他文本摘要领域。我们提出了一个新的主题对话摘要评估基准，由不同规模的LLMs生成。我们提供了关于这些摘要的事实一致性的二元句级人类注释，以及对事实不一致句子的详细解释。我们的分析表明，现有的LLMs在对话领域存在大量事实错误的幻觉，无论模型大小如何。另一方面，当LLMs（包括GPT-4）充当二元事实评估器时，它们表现不佳，且可以被当前最先进的专门事实评估度量所超越。最后，我们对幻觉类型进行了分析

    arXiv:2402.13249v1 Announce Type: cross  Abstract: Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a cu
    
[^197]: 大型语言模型推理解释的可解释性有多高？

    How Interpretable are Reasoning Explanations from Prompting Large Language Models?

    [https://arxiv.org/abs/2402.11863](https://arxiv.org/abs/2402.11863)

    对大型语言模型推理解释提出了全面、多角度的评估，包括忠实度、强健性和效用，并引入了新的可解释性指标。

    

    Prompt Engineering已经引起了人们的极大关注，可以增强大型语言模型在多项任务中的性能。Chain-of-Thought等技术不仅增强了任务性能，还描绘了清晰的推理步骤轨迹，为观众提供了一种有形的解释形式。我们对可解释性进行了全面多角度的评估，不仅考虑了忠实度，还考虑了在多个常识推理基准测试中的强健性和效用。此外，我们引入了一个简单的可解释性指标。

    arXiv:2402.11863v1 Announce Type: new  Abstract: Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability ali
    
[^198]: 人工智能与人类在通信时代的互动：自噬使得大型模型实现局部最优

    Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima

    [https://arxiv.org/abs/2402.11271](https://arxiv.org/abs/2402.11271)

    合成信息更可能被大型模型纳入训练数据集和传播中，大型模型在传递信息时倾向于有选择地修改和丢失特定内容

    

    随着大型语言和多模态模型在社会信息处理中的重要性日益增加，引发了关于社会安全和伦理的争论。然而，很少有研究从人类和人工智能系统相互作用的综合视角分析这些限制。本研究调查了人类和大型模型在通信中作为关键联系的偏见和偏好。为实现此目的，我们设计了一个多模态数据集和三个不同的实验，评估生成模型在其作为信息生产者和传播者的角色中的表现。我们的主要发现突出显示，合成信息更有可能被纳入模型训练数据集和消息传递中，而人类生成的信息。此外，大型模型在以信息传递者的角色时，倾向于有选择地修改和丢失特定内容。在概念上，我们提出了两种真实的自

    arXiv:2402.11271v1 Announce Type: new  Abstract: The increasing significance of large language and multimodal models in societal information processing has ignited debates on social safety and ethics. However, few studies have approached the analysis of these limitations from the comprehensive perspective of human and artificial intelligence system interactions. This study investigates biases and preferences when humans and large models are used as key links in communication. To achieve this, we design a multimodal dataset and three different experiments to evaluate generative models in their roles as producers and disseminators of information. Our main findings highlight that synthesized information is more likely to be incorporated into model training datasets and messaging than human-generated information. Additionally, large models, when acting as transmitters of information, tend to modify and lose specific content selectively. Conceptually, we present two realistic models of auto
    
[^199]: RS-DPO：一种用于对齐大型语言模型的混合拒绝采样和直接优化偏好的方法

    RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models

    [https://arxiv.org/abs/2402.10038](https://arxiv.org/abs/2402.10038)

    本研究提出了一种名为RS-DPO的方法，它将拒绝采样和直接优化偏好结合起来，用于对齐大型语言模型。通过开发一个经过监督微调的策略模型，并从该模型中直接采样响应，RS-DPO能够有效解决基于近端策略优化的不稳定性和高计算成本的问题。通过识别对比样本对，RS-DPO能够更好地进行RLHF。

    

    强化学习从人类反馈中学习（RLHF）已被广泛应用于将大型语言模型与用户意图对齐。然而，基于近端策略优化（PPO）的RLHF有时不稳定，需要显著的超参数微调，并且在对齐过程中计算成本高昂。最近，提出了直接优化偏好（DPO）来解决这些挑战。然而，DPO依赖于从人类标注者和替代LLM生成的对比回复，而不是策略模型，限制了RLHF的效果。本文通过系统地结合拒绝采样（RS）和DPO来解决这两个挑战。我们提出的方法RS-DPO，首先开发出一个经过监督微调的策略模型（SFT）。然后直接从SFT模型中采样每个提示的k个响应。RS-DPO基于其相似度识别对比样本对。

    arXiv:2402.10038v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their re
    
[^200]: LLMs作为桥梁：重新构建基于多模态图像的命名实体识别

    LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition

    [https://arxiv.org/abs/2402.09989](https://arxiv.org/abs/2402.09989)

    本文提出了RiVEG，一个统一的框架，通过利用大型语言模型（LLMs）作为连接桥梁，将多模态命名实体识别重新构建为联合任务，解决了命名实体无法确定和指代表达与命名实体之间的区别的问题。

    

    Grounded Multimodal Named Entity Recognition (GMNER) 是一个新兴的多模态任务，旨在识别命名实体、实体类型及其对应的视觉区域。GMNER任务具有两个挑战性质：1）社交媒体中图像和文本之间的弱相关性导致大部分命名实体难以确定；2）常用于类似任务的粗粒度指代表达与细粒度命名实体之间存在明显区别。本文提出了RiVEG，一个统一的框架，通过利用大型语言模型（LLMs）作为连接桥梁，将GMNER重新构建为联合MNER-VE-VG任务。这种重新构建带来了两个好处：1）保持了最佳的MNER性能，消除了使用目标检测方法预提取区域特征的需求，自然解决了这两个挑战。

    arXiv:2402.09989v1 Announce Type: cross  Abstract: Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal task that aims to identify named entities, entity types and their corresponding visual regions. GMNER task exhibits two challenging properties: 1) The weak correlation between image-text pairs in social media results in a significant portion of named entities being ungroundable. 2) There exists a distinction between coarse-grained referring expressions commonly used in similar tasks (e.g., phrase localization, referring expression comprehension) and fine-grained named entities. In this paper, we propose RiVEG, a unified framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging large language models (LLMs) as a connecting bridge. This reformulation brings two benefits: 1) It maintains the optimal MNER performance and eliminates the need for employing object detection methods to pre-extract regional features, thereby naturally addressing two m
    
[^201]: 长文本评估模型编辑

    Long-form evaluation of model editing

    [https://arxiv.org/abs/2402.09394](https://arxiv.org/abs/2402.09394)

    长文本评估模型编辑（LEME）是一种新颖的评估协议，用于衡量模型编辑在长篇生成设置中的有效性和影响。这个协议与先前的短文本指标几乎没有关系，引入了一组新的维度来理解模型编辑方法。

    

    目前，对于模型编辑的评估只使用了提示后的“下几个标记”的完成。因此，这些方法对于更长的自然语言生成的影响大部分是未知的。我们引入了长文本评估模型编辑（LEME）的新颖评估协议，该协议在长篇生成设置中衡量模型编辑的功效和影响。我们的评估协议包括机器评定的调查和与人类评分相关性很好的分类器。重要的是，我们发现我们的评估协议与先前的短文本指标几乎没有关系（尽管设计为扩展功效、泛化性、局部性和可移植性到长文本环境中），这表明我们的方法引入了一组新的维度来理解模型编辑方法。使用这个协议，我们对一些模型编辑技术进行了基准测试，并提出了几个发现，包括一些方法（R

    arXiv:2402.09394v1 Announce Type: new Abstract: Evaluations of model editing currently only use the `next few token' completions after a prompt. As a result, the impact of these methods on longer natural language generation is largely unknown. We introduce long-form evaluation of model editing (\textbf{\textit{LEME}}) a novel evaluation protocol that measures the efficacy and impact of model editing in long-form generative settings. Our protocol consists of a machine-rated survey and a classifier which correlates well with human ratings. Importantly, we find that our protocol has very little relationship with previous short-form metrics (despite being designed to extend efficacy, generalization, locality, and portability into a long-form setting), indicating that our method introduces a novel set of dimensions for understanding model editing methods. Using this protocol, we benchmark a number of model editing techniques and present several findings including that, while some methods (R
    
[^202]: LlaSMol:利用大规模、全面、高质量的指令调优数据集推进化学的大规模语言模型

    LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset

    [https://arxiv.org/abs/2402.09391](https://arxiv.org/abs/2402.09391)

    本文介绍了LlaSMol，它是一种推进化学领域大规模语言模型的方法。通过使用一个大规模、全面、高质量的指令调优数据集来训练模型，LlaSMol在化学任务中表现出强大的性能，超过了GPT-4并接近于任务特定模型。

    

    化学在药物研发和材料科学等许多领域中起着至关重要的作用。尽管诸如GPT-4之类的大型语言模型（LLM）在自然语言处理任务上展现出了非凡的能力，但现有工作表明它们在化学任务上的性能令人失望。然而，在本文中，我们展示了我们开发的LLM在一系列化学任务上可以取得非常强大的结果，在所有任务上都显著优于最先进的GPT-4，并接近SoTA任务特定模型。我们取得成功的关键是一个名为SMolInstruct的大规模、全面、高质量的指令调优数据集。它包含了14个经过精心挑选的化学任务和超过三百万个高质量样本，为训练和评估化学LLM奠定了坚实基础。基于SMolInstruct，我们对一组开源LLM进行了微调，其中，我们发现Mistral ser是最佳性能的模型。

    arXiv:2402.09391v1 Announce Type: new Abstract: Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing work shows their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 across all the tasks by a substantial margin and approaching the SoTA task-specific models. The key to our success is a large-scale, comprehensive, high-quality dataset for instruction tuning named SMolInstruct. It contains 14 meticulously selected chemistry tasks and over three million high-quality samples, laying a solid foundation for training and evaluating LLMs for chemistry. Based on SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral ser
    
[^203]: 审计反火：评估具有证据和风格的先进反驳生成

    Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style

    [https://arxiv.org/abs/2402.08498](https://arxiv.org/abs/2402.08498)

    这项研究提出了一个新的数据集，用于生成具有证据和风格的反驳，该数据集基于Reddit ChangeMyView数据集中的帖子，并可用于论证的改进和评估。评估结果显示，GPT-3.5 turbo模型在论证质量方面表现出色，并且具有很高的风格融合能力。互惠式反驳的效果最佳。

    

    我们提出了一个新颖的数据集，用于控制性反驳的合成，旨在进一步应用于论证的改进、挖掘和评估。我们的数据集包含与Reddit ChangeMyView数据集中的帖子相结合的丰富的反驳，这些反驳融入了从高质量来源中检索到的证据，并根据用户偏好生成，调整了证据和论证风格的关键属性。由此产生的Counterfire语料库包括从GPT-3.5 turbo、Koala和PaLM 2模型以及它们的两个微调变体生成的论证（N = 32,000）。模型评估表明，在证据方面具有强大的改写能力，尽管词汇重叠有限，同时表现出高度的风格融合（对于“互惠”的得分为0.9682），显示了LLM融合多样风格的能力。在所有模型中，GPT-3.5 turbo在论证质量评估中显示出最高分数，表现出一致准确性（得分 >0.8）。在进一步的分析中，互惠式反驳证明效果最佳，能够产生更好的论证结果。

    We present a novel dataset for the controlled composition of counterarguments designed for further applications in argument refining, mining, and evaluation. Our dataset constitutes enriched counter-arguments to posts in the Reddit ChangeMyView dataset that are integrated with evidence retrieved from high-quality sources and generated based on user preferences, adjusting the critical attributes of evidence and argument style. The resultant Counterfire corpus comprises arguments generated from GPT-3.5 turbo, Koala, and PaLM 2 models and two of their finetuned variants (N = 32,000). Model evaluation indicates strong paraphrasing abilities with evidence, albeit limited word overlap, while demonstrating high style integration (0.9682 for 'reciprocity'), showing the ability of LLM to assimilate diverse styles. Of all models, GPT-3.5 turbo showed the highest scores in argument quality evaluation, showing consistent accuracy (score >0.8). In further analyses, reciprocity-style counterargument
    
[^204]: X-LoRA: 一种灵活的大型语言模型框架，利用低秩适配器专家的混合策略在蛋白质力学和设计中的应用

    X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design

    [https://arxiv.org/abs/2402.07148](https://arxiv.org/abs/2402.07148)

    X-LoRA是一种灵活的大型语言模型框架，利用低秩适配器专家的混合策略，可以创建精细调整的模型并在蛋白质力学和设计领域应用。该模型利用深层逐层适应的组合来解决特定任务，并受到生物学原理的启发。无需修改底层结构即可应用于任何现有的语言模型。

    

    我们报道了一种使用深层逐层基于低秩适应（LoRA）的新颖预训练适配器的混合专家策略，用于创建精细调整的大型语言模型。我们提出了一种利用隐藏状态动态混合经过适应的层的门控策略，允许得到的X-LoRA模型利用不同的能力并创建以前未使用的深层逐层适应的组合来解决特定任务。该设计受到了生物普遍性和多样性的生物学原理的启发，其中神经网络建模块在不同的分层表示中被重复使用。因此，X-LoRA模型可以轻松用于任何现有的大型语言模型（LLM），无需修改底层结构。我们还开发了一个定制的X-LoRA模型，提供了包括前向/逆向分析任务和增强推理能力在内的科学能力，重点是生物材料分析。

    We report a mixture of expert strategy to create fine-tuned large language models using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, we propose a gating strategy that uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations of adaptations are established to solve specific tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing large language model (LLM) without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced reasoning capability, focused on biomaterial analysis,
    
[^205]: TinyLLM: 从多个大型语言模型学习一个小型学生模型

    TinyLLM: Learning a Small Student from Multiple Large Language Models

    [https://arxiv.org/abs/2402.04616](https://arxiv.org/abs/2402.04616)

    TinyLLM是一种从多个大型语言模型中学习小型学生模型的知识蒸馏范式，旨在解决知识多样性有限和缺乏上下文信息等问题，并鼓励学生模型理解答案背后的原理。

    

    将更强大的大型语言模型（LLMs）的推理能力转移到较小的模型上具有吸引力，因为较小的LLMs更灵活，成本更低。在现有的解决方案中，知识蒸馏因其出色的效率和泛化能力而脱颖而出。然而，现有方法存在一些缺点，包括知识多样性有限和缺乏丰富的上下文信息。为了解决这些问题并促进紧凑语言模型的学习，我们提出了TinyLLM，一种从多个大型教师LLMs中学习小型学生LLM的新型知识蒸馏范式。特别地，我们鼓励学生LLM不仅生成正确答案，而且理解这些答案背后的原理。鉴于不同的LLMs具有不同的推理能力，我们引导学生模型吸收来自多个教师LLMs的知识。我们进一步引入了一个上下文示例生成器和一个老师强制模块...

    Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing 
    
[^206]: 双视图视觉背景化的网页导航

    Dual-View Visual Contextualization for Web Navigation

    [https://arxiv.org/abs/2402.04476](https://arxiv.org/abs/2402.04476)

    本文提出了一种通过网页截图中元素的“双视图”来进行 HTML 元素的背景化的方法，这种方法通过将每个元素与其邻居元素进行背景化，使用文本和视觉特征，使得HTML元素的结果表示对于代理执行操作更加信息丰富。

    

    自动网页导航旨在构建一个可以根据语言指令在实际网站上执行复杂和多样任务的网络代理。现有工作主要是以 HTML 文档作为输入，HTML 文档定义了网页的内容和操作空间（即可操作元素和操作）。然而，HTML 文档可能无法为每个元素提供清晰的任务相关背景，使得选择正确的（一系列的）操作变得困难。本文提出通过网页截图中元素的“双视图”来进行 HTML 元素的背景化：每个 HTML 元素在截图中有其对应的边界框和视觉内容。我们基于一个洞察力——网页开发者倾向于在网页上将任务相关元素放置在附近以增强用户体验，并提出将每个元素与其邻居元素进行背景化，使用文本和视觉特征。HTML 元素的结果表示对于代理执行操作更加信息丰富。

    Automatic web navigation aims to build a web agent that can follow language instructions to execute complex and diverse tasks on real-world websites. Existing work primarily takes HTML documents as input, which define the contents and action spaces (i.e., actionable elements and operations) of webpages. Nevertheless, HTML documents may not provide a clear task-related context for each element, making it hard to select the right (sequence of) actions. In this paper, we propose to contextualize HTML elements through their "dual views" in webpage screenshots: each HTML element has its corresponding bounding box and visual content in the screenshot. We build upon the insight -- web developers tend to arrange task-related elements nearby on webpages to enhance user experiences -- and propose to contextualize each element with its neighbor elements, using both textual and visual features. The resulting representations of HTML elements are more informative for the agent to take action. We val
    
[^207]: 使用大型多模态模型解释生成模型的潜在表示

    Explaining latent representations of generative models with large multimodal models

    [https://arxiv.org/abs/2402.01858](https://arxiv.org/abs/2402.01858)

    该研究提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。对于不同生成模型的解释，我们量化了解释的不确定性，通过多模态模型进行了性能评估，且定性地展示了潜在因素的变化效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。

    

    学习可解释的数据生成潜在因素表示是人工智能发展的重要课题。随着大型多模态模型的兴起，它可以将图像与文本对齐以生成答案。在本研究中，我们提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。我们进一步量化评估了我们生成的解释的不确定性，在多个大型多模态模型之间评估了解释生成的性能，并定性地可视化了每个潜在因素的变化，以学习不同生成模型对解释的解缠效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。

    Learning interpretable representations of data generative latent factors is an important topic for the development of artificial intelligence. With the rise of the large multimodal model, it can align images with text to generate answers. In this work, we propose a framework to comprehensively explain each latent factor in the generative models using a large multimodal model. We further measure the uncertainty of our generated explanations, quantitatively evaluate the performance of explanation generation among multiple large multimodal models, and qualitatively visualize the variations of each latent factor to learn the disentanglement effects of different generative models on explanations. Finally, we discuss the explanatory capabilities and limitations of state-of-the-art large multimodal models.
    
[^208]: 通过代理调整语言模型

    Tuning Language Models by Proxy

    [https://arxiv.org/abs/2401.08565](https://arxiv.org/abs/2401.08565)

    介绍了一种代理调整的轻量级解码时算法，可以通过对小型调整后的LM的预测与未调整LM的预测之间的差异来调整大型预训练LM的预测，从而实现资源节约和保留更大规模预训练的好处。

    

    尽管大型预训练语言模型具有一般的能力，但它们始终受益于进一步调整以更好地实现所需的行为。然而，调整这些模型变得越来越消耗资源，或者在模型权重是私有的情况下是不可能的。我们引入了代理调整，这是一种轻量级的解码时算法，它在黑盒语言模型的基础上运行，以实现与直接调整相同的目的，但只访问其在输出词汇上的预测，而不是其参数。我们的方法调整了一个较小的语言模型，然后将经过调整和未经调整的小模型的预测之间的差异应用于将更大的未调整模型的原始预测转移到调整方向，同时保留较大规模预训练的好处。在实验中，当我们使用仅为7B大小的代理对Llama2-70B应用代理调整时，我们可以关闭88% Llama2-70B 与其真正调整过的聊天版本之间的差距，

    arXiv:2401.08565v2 Announce Type: replace  Abstract: Despite the general capabilities of large pretrained language models, they consistently benefit from further adaptation to better achieve desired behaviors. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private. We introduce proxy-tuning, a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the same end as direct tuning, but by accessing only its predictions over the output vocabulary, not its parameters. Our method tunes a smaller LM, then applies the difference between the predictions of the small tuned and untuned LMs to shift the original predictions of the larger untuned model in the direction of tuning, while retaining the benefits of larger-scale pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using proxies of only 7B size, we can close 88% of the gap between Llama2-70B and its truly-tuned chat version, when 
    
[^209]: 及时预测结构：推理的回归

    Promptly Predicting Structures: The Return of Inference

    [https://arxiv.org/abs/2401.06877](https://arxiv.org/abs/2401.06877)

    本文提出了一个框架，通过使用结构约束和由此衍生的组合推理，可以过滤大型语言模型预测的不一致结构，从而构建有效的结构化输出，并提高性能。

    

    在自然语言处理领域，基于提示的方法被广泛用于构建零样本和少样本标签预测器。许多自然语言处理任务具有结构特点：即它们的输出由多个相互约束的标签组成。为这类任务标注数据可能会很繁琐。本文探讨了基于提示的范式能否扩展到这种结构化输出任务？我们提出了一个构建零样本和少样本语言结构预测器的框架。我们的关键观点是，我们可以利用结构约束和从中得出的组合推理来过滤大型语言模型预测的不一致结构。我们在两个结构化预测任务和五个数据集上实例化了这个框架。结果表明，在所有情况下，强制实施一致性不仅构造了结构有效的输出，而且提高了性能，超过了不受约束的变体。

    arXiv:2401.06877v2 Announce Type: replace  Abstract: Prompt-based methods have been used extensively across NLP to build zero- and few-shot label predictors. Many NLP tasks are naturally structured: that is, their outputs consist of multiple labels which constrain each other. Annotating data for such tasks can be cumbersome. Can the promise of the prompt-based paradigm be extended to such structured outputs? In this paper, we present a framework for constructing zero- and few-shot linguistic structure predictors. Our key insight is that we can use structural constraints -- and combinatorial inference derived from them -- to filter out inconsistent structures predicted by large language models. We instantiated this framework on two structured prediction tasks, and five datasets. Across all cases, our results show that enforcing consistency not only constructs structurally valid outputs, but also improves performance over the unconstrained variants.
    
[^210]: Transformer长度外推：从位置编码的角度进行调查

    Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding

    [https://arxiv.org/abs/2312.17044](https://arxiv.org/abs/2312.17044)

    本调查从位置编码的角度总结了用于增强Transformer长度外推能力的各种方法，包括绝对和相对位置编码以及基于它们的外推方法。

    

    Transformer自诞生以来已经在自然语言处理（NLP）领域掀起了一股风暴。建立在其基础上的大型语言模型（LLMs）由于其出色的能力而受到全球关注。然而，包括这些强大的LLMs在内的所有基于Transformer的模型都受制于预设的长度限制，很难从短训练序列推广到更长的推断序列，即它们无法进行长度外推。因此，已经提出了大量方法来增强Transformer的长度外推能力，其中位置编码（PE）被认为是主要因素。 在这项调查中，我们从PE的角度以统一符号介绍了这些关于长度外推的进展。具体而言，我们首先介绍了可外推的PE，包括绝对和相对PE。然后，我们深入探讨了基于它们的外推方法，涵盖了位置插值和随机化位置方法。

    arXiv:2312.17044v3 Announce Type: replace  Abstract: Transformer has taken the field of natural language processing (NLP) by storm since its birth. Further, Large language models (LLMs) built upon it have captured worldwide attention due to its superior abilities. Nevertheless, all Transformer-based models including these powerful LLMs suffer from a preset length limit and can hardly generalize from short training sequences to longer inference ones, namely, they can not perform length extrapolation. Hence, a plethora of methods have been proposed to enhance length extrapolation of Transformer, in which the positional encoding (PE) is recognized as the major factor. In this survey, we present these advances towards length extrapolation in a unified notation from the perspective of PE. Specifically, we first introduce extrapolatable PEs, including absolute and relative PEs. Then, we dive into extrapolation methods based on them, covering position interpolation and randomized position met
    
[^211]: 基于自然语言处理的肌肉骨骼疾病风险因素分类与基于模式的排名

    A Natural Language Processing-Based Classification and Mode-Based Ranking of Musculoskeletal Disorder Risk Factors

    [https://arxiv.org/abs/2312.11517](https://arxiv.org/abs/2312.11517)

    本研究利用自然语言处理和基于模式的排名方法对肌肉骨骼疾病的风险因素进行了分类和排名，以提高对其理解、分类和优先考虑预防和治疗的能力。

    

    本研究探讨了肌肉骨骼疾病（MSD）风险因素，使用自然语言处理（NLP）和基于模式的排名相结合。旨在精细化理解、分类和优先考虑针对性预防和治疗。评估了八个NLP模型，结合预训练的转换器、余弦相似度和距离度量将因素分类为个人、生物力学、工作场所、心理和组织等类别。BERT与余弦相似度达到28%的准确率；句子转换器与欧氏、布雷曲蒂斯和闵可夫斯基距离得分为100%。通过10倍交叉验证，统计检验确保鲁棒结果。调查数据和基于模式的排名确定了严重性等级，与文献相一致。"工作姿势"是最严重的，凸显了姿势的作用。调查结果强调了"工作不稳定性"、"工作努力和回报不平衡"和"员工设施差"等因素的显著性。

    arXiv:2312.11517v3 Announce Type: replace  Abstract: This research delves into Musculoskeletal Disorder (MSD) risk factors, using a blend of Natural Language Processing (NLP) and mode-based ranking. The aim is to refine understanding, classification, and prioritization for focused prevention and treatment. Eight NLP models are evaluated, combining pre-trained transformers, cosine similarity, and distance metrics to categorize factors into personal, biomechanical, workplace, psychological, and organizational classes. BERT with cosine similarity achieves 28% accuracy; sentence transformer with Euclidean, Bray-Curtis, and Minkowski distances scores 100%. With 10-fold cross-validation, statistical tests ensure robust results. Survey data and mode-based ranking determine severity hierarchy, aligning with the literature. "Working posture" is the most severe, highlighting posture's role. Survey insights emphasize "Job insecurity," "Effort reward imbalance," and "Poor employee facility" as sig
    
[^212]: ComplexityNet: 通过学习任务复杂性提高LLM推理效率

    ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity

    [https://arxiv.org/abs/2312.11511](https://arxiv.org/abs/2312.11511)

    ComplexityNet通过学习任务复杂性，提高了LLM推理效率，通过预测任务的准确输出概率，成功降低了90%的计算资源使用，并在任务复杂性确定方面取得了79%准确率。

    

    我们提出了ComplexityNet，这是一个专为评估任务复杂性而设计的简化语言模型。该模型通过不同能力的各种语言模型来预测准确输出的可能性。我们首次在Mostly Basic Python Problems（MBPP）数据集上应用了ComplexityNet。我们开创性地创建了第一组标签来定义任务复杂性。ComplexityNet在确定任务复杂性方面取得了显著的79%准确率，较原始、非微调模型的34%准确率有了显著改进。此外，与使用最高复杂性模型相比，ComplexityNet有效地减少了90%的计算资源使用，同时保持了86.7%的高代码生成准确率。这项研究表明，通过微调较小的模型来对任务进行分类，可以在准确性和效率之间取得更平衡的权衡。

    arXiv:2312.11511v2 Announce Type: replace-cross  Abstract: We present ComplexityNet, a streamlined language model designed for assessing task complexity. This model predicts the likelihood of accurate output by various language models, each with different capabilities. Our initial application of ComplexityNet involves the Mostly Basic Python Problems (MBPP) dataset. We pioneered the creation of the first set of labels to define task complexity. ComplexityNet achieved a notable 79% accuracy in determining task complexity, a significant improvement over the 34% accuracy of the original, non fine-tuned model. Furthermore, ComplexityNet effectively reduces computational resource usage by 90% compared to using the highest complexity model, while maintaining a high code generation accuracy of 86.7%. This study demonstrates that fine-tuning smaller models to categorize tasks based on their complexity can lead to a more balanced trade-off between accuracy and efficiency in the use of Large Lan
    
[^213]: SMILE：多模态数据集用于语言模型理解视频中的笑声

    SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models

    [https://arxiv.org/abs/2312.09818](https://arxiv.org/abs/2312.09818)

    本论文介绍了一个新任务，即视频笑声推理，旨在解释特定视频中人们笑的原因，并提出了数据集SMILE。通过利用大型语言模型生成文本视频表示，我们的基线能够生成合理的笑声解释。

    

    尽管人工智能最近取得了进展，但构建社交智能仍然是一个挑战。其中，笑声是人类社交互动中发生的独特表达之一。在这项工作中，我们面对了机器理解视频中笑声背后理由的新挑战，即视频笑声推理。我们介绍了这一新任务，解释人们在特定视频中为什么会笑的原因，并提出了用于这一任务的数据集SMILE。我们提出了一个基线，通过利用大型语言模型（LLMs）的推理能力和文本视频表示生成合理的笑声解释。实验表明，我们的基线可以生成可信的笑声解释。我们进一步探讨了我们的基线在探测其他视频理解任务和野外视频方面的可扩展性。我们发布了我们的数据集、代码和模型。

    arXiv:2312.09818v2 Announce Type: replace-cross  Abstract: Despite the recent advances of the artificial intelligence, building social intelligence remains a challenge. Among social signals, laughter is one of the distinctive expressions that occurs during social interactions between humans. In this work, we tackle a new challenge for machines to understand the rationale behind laughter in video, Video Laugh Reasoning. We introduce this new task to explain why people laugh in a particular video and a dataset for this task. Our proposed dataset, SMILE, comprises video clips and language descriptions of why people laugh. We propose a baseline by leveraging the reasoning capacity of large language models (LLMs) with textual video representation. Experiments show that our baseline can generate plausible explanations for laughter. We further investigate the scalability of our baseline by probing other video understanding tasks and in-the-wild videos. We release our dataset, code, and model 
    
[^214]: 使用大语言模型为Minecraft自动设计稠密奖励的Auto MC-Reward

    Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft

    [https://arxiv.org/abs/2312.09238](https://arxiv.org/abs/2312.09238)

    本文介绍了一种名为Auto MC-Reward的先进学习系统，利用大型语言模型自动设计稠密奖励函数来提高学习效率

    

    许多强化学习环境（例如Minecraft）仅提供指示任务完成或失败的稀疏奖励，这些奖励以二进制值表示。在这种环境中探索效率的挑战使得基于强化学习的代理程序难以学习复杂任务。为了解决这个问题，本文介绍了一种名为Auto MC-Reward的先进学习系统，利用大型语言模型（LLMs）自动设计稠密奖励函数，从而提高学习效率。Auto MC-Reward包括三个重要组件：奖励设计者、奖励评论家和轨迹分析器。给定环境信息和任务描述，奖励设计者首先通过编写可执行的Python函数和预定义的观测输入来设计奖励函数。然后，我们的奖励评论家将负责验证代码，检查代码是否自洽且无语法错误。

    arXiv:2312.09238v2 Announce Type: replace  Abstract: Many reinforcement learning environments (e.g., Minecraft) provide only sparse rewards that indicate task completion or failure with binary values. The challenge in exploration efficiency in such environments makes it difficult for reinforcement-learning-based agents to learn complex tasks. To address this, this paper introduces an advanced learning system, named Auto MC-Reward, that leverages Large Language Models (LLMs) to automatically design dense reward functions, thereby enhancing the learning efficiency. Auto MC-Reward consists of three important components: Reward Designer, Reward Critic, and Trajectory Analyzer. Given the environment information and task descriptions, the Reward Designer first design the reward function by coding an executable Python function with predefined observation inputs. Then, our Reward Critic will be responsible for verifying the code, checking whether the code is self-consistent and free of syntax 
    
[^215]: 蜜蜂：多模态LLM的增强局部投影仪

    Honeybee: Locality-enhanced Projector for Multimodal LLM

    [https://arxiv.org/abs/2312.06742](https://arxiv.org/abs/2312.06742)

    该研究提出了一种既灵活又增强局部性的新型投影仪设计，有助于连接视觉编码器和语言模型，提高模型的效率和空间理解。

    

    在多模态大型语言模型（MLLMs）中，视觉投影仪在连接预先训练的视觉编码器与LLMs之间发挥着至关重要的作用，实现深入的视觉理解并利用LLMs的强大能力。尽管视觉投影仪的重要性不言而喻，但研究相对较少。在这项研究中，我们首先确定了两个关键的投影仪属性：（i）灵活性以管理视觉代币的数量，对于MLLMs的整体效率至关重要；（ii）保留来自视觉特征的局部上下文，对于空间理解至关重要。基于这些发现，我们提出了一种既灵活又增强局部性的新型投影仪设计，有效地满足了这两种理想属性。此外，我们提出了全面的策略，以有效利用多个和多方面的指导数据集。通过广泛的实验，我们检验了各种设计选择的影响。

    arXiv:2312.06742v2 Announce Type: replace-cross  Abstract: In Multimodal Large Language Models (MLLMs), a visual projector plays a crucial role in bridging pre-trained vision encoders with LLMs, enabling profound visual understanding while harnessing the LLMs' robust capabilities. Despite the importance of the visual projector, it has been relatively less explored. In this study, we first identify two essential projector properties: (i) flexibility in managing the number of visual tokens, crucial for MLLMs' overall efficiency, and (ii) preservation of local context from visual features, vital for spatial understanding. Based on these findings, we propose a novel projector design that is both flexible and locality-enhanced, effectively satisfying the two desirable properties. Additionally, we present comprehensive strategies to effectively utilize multiple and multifaceted instruction datasets. Through extensive experiments, we examine the impact of individual design choices. Finally, o
    
[^216]: 大型多模态模型的组合式思维提示

    Compositional Chain-of-Thought Prompting for Large Multimodal Models

    [https://arxiv.org/abs/2311.17076](https://arxiv.org/abs/2311.17076)

    提出了一种新颖的零样式思维提示（CCoT），以克服大型多模态模型难以捕捉到组合视觉推理方面的细节的问题。

    

    强大的视觉骨干和大规模语言模型(LLM)推理的结合已经导致大型多模态模型(LMM)成为当前广泛视觉和语言(VL)任务的标准。然而，最近的研究表明，即使是最先进的LMM仍然难以捕捉到组合视觉推理方面的细节，比如对象之间的属性和关系。一种解决方案是利用场景图(SGs)——对象及其关系和属性的形式化表达，它已被广泛用作视觉和文本领域之间的桥梁。然而，场景图数据需要场景图注释，这种数据收集成本高昂，因此难以扩展。此外，基于场景图数据微调LMM可能导致预训练目标的灾难性遗忘。为了克服这一问题，受到思维链方法的启发，我们提出了一种新颖的零样式思维提示（CCoT）。

    arXiv:2311.17076v2 Announce Type: replace-cross  Abstract: The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)--a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-sho
    
[^217]: MobileCLIP: 通过多模态强化训练实现快速图像-文本模型

    MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training

    [https://arxiv.org/abs/2311.17049](https://arxiv.org/abs/2311.17049)

    MobileCLIP通过多模态强化训练实现了高效率图像-文本模型，在零-shot任务中取得了新的性能平衡。

    

    对图像-文本基础模型（如CLIP）进行对比预训练表明，在广泛的下游任务中表现出色的零-shot性能和改善的鲁棒性。然而，这些模型利用大型基于transformer的编码器，具有显著的内存和延迟开销，这会给在移动设备上部署带来挑战。在这项工作中，我们介绍了MobileCLIP--一种优化了运行时性能的高效图像-文本模型家族，以及一种新颖而高效的训练方法，即多模态强化训练。所提出的训练方法利用来自图像字幕模型和强CLIP编码器集成的知识转移来提高高效模型的准确性。我们的方法通过将额外的知识存储在强化数据集中避免了训练时的计算开销。MobileCLIP为零-shot分类和检索设置了一个新的延迟-准确性权衡的最新水平。

    arXiv:2311.17049v2 Announce Type: replace-cross  Abstract: Contrastive pretraining of image-text foundation models, such as CLIP, demonstrated excellent zero-shot performance and improved robustness on a wide range of downstream tasks. However, these models utilize large transformer-based encoders with significant memory and latency overhead which pose challenges for deployment on mobile devices. In this work, we introduce MobileCLIP -- a new family of efficient image-text models optimized for runtime performance along with a novel and efficient training approach, namely multi-modal reinforced training. The proposed training approach leverages knowledge transfer from an image captioning model and an ensemble of strong CLIP encoders to improve the accuracy of efficient models. Our approach avoids train-time compute overhead by storing the additional knowledge in a reinforced dataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for zero-shot classification and retrie
    
[^218]: OVM，结果监督价值模型用于数学推理规划

    OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning

    [https://arxiv.org/abs/2311.09724](https://arxiv.org/abs/2311.09724)

    引入Outcome-supervised Value Model (OVM)利用结果监督训练价值模型，以在数学推理中减少错误传播，将任务转化为价值估计问题。

    

    大型语言模型（LLMs）经常在多个推理步骤中保持准确性方面遇到困难，特别是在数学推理中，早期步骤中的错误可能传播到后续步骤，最终导致错误答案。为了减少错误传播，引入了引导解码以逐步指导LM解码。我们认为，在引导解码中，评估不完整推理路径的潜力可能比仅确保每个步骤的正确性更有优势，因为前一种方法会导向正确的最终答案。这将任务转化为计划中的价值估计问题。受到发现的启发，即$\textit{引导解码的结果监督本质上充当价值模型}$，我们提出了一种Outcome-supervised Value Model (OVM)，它采用结果监督来训练价值模型，优先考虑导致准确性的步骤。

    arXiv:2311.09724v2 Announce Type: replace  Abstract: Large language models (LLMs) often struggle with maintaining accuracy throughout multiple multiple reasoning steps, especially in mathematical reasoning where an error in earlier steps can propagate to subsequent ones and it ultimately leading to an incorrect answer. To reduce error propagation, guided decoding is employed to direct the LM decoding on a step-by-step basis. We argue that in guided decoding, assessing the potential of an incomplete reasoning path can be more advantageous than simply ensuring per-step correctness, as the former approach leads towards a correct final answer. This transforms the task into a $\textit{value estimation}$ problem in planning.   Inspired by the findings that $\textit{outcome supervision for guided decoding essentially acts as a value model}$, we propose Outcome-supervised Value Model (OVM) that employs outcome supervision for training a value model, which prioritizes steps that lead to accurat
    
[^219]: 使用基于LLM的代理网络模拟意见动态

    Simulating Opinion Dynamics with Networks of LLM-based Agents

    [https://arxiv.org/abs/2311.09618](https://arxiv.org/abs/2311.09618)

    提出了一种基于大型语言模型（LLMs）人口的新方法来模拟意见动态，发现LLM代理存在固有偏见导致模拟代理趋向于科学现实一致的共识，但引入确认偏见后观察到意见分裂，突显了LLM代理在该领域的潜力和局限性。

    

    准确模拟人类意见动态对于理解各种社会现象至关重要，包括极化和错误信息的传播。然而，常用于此类模拟的基于代理的模型（ABM）经常会过分简化人类行为。我们提出了一种基于大型语言模型（LLMs）人口的模拟意见动态的新方法。我们的研究结果显示，LLM代理存在一种对产生准确信息的强烈固有偏见，导致模拟代理趋向于与科学现实一致的共识。然而，这种偏见限制了它们在理解气候变化等问题上抵制共识观点的效用。通过引入提示工程诱导确认偏见后，我们观察到了与现有基于代理模型和意见动态研究一致的意见分裂。这些见解突显了LLM代理在该领域的潜力和局限性，并提出了一条路径。

    arXiv:2311.09618v2 Announce Type: replace-cross  Abstract: Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models (ABMs) commonly used for such simulations often over-simplify human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs). Our findings reveal a strong inherent bias in LLM agents towards producing accurate information, leading simulated agents to consensus in line with scientific reality. This bias limits their utility for understanding resistance to consensus views on issues like climate change. After inducing confirmation bias through prompt engineering, however, we observed opinion fragmentation in line with existing agent-based modeling and opinion dynamics research. These insights highlight the promise and limitations of LLM agents in this domain and suggest a path
    
[^220]: ARES: 用于检索增强生成系统的自动化评估框架

    ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems

    [https://arxiv.org/abs/2311.09476](https://arxiv.org/abs/2311.09476)

    ARES是用于评估检索增强生成系统的自动化评估框架，通过创建合成训练数据和微调评估器，有效评估RAG系统在不同任务中的表现。

    

    评估检索增强生成（RAG）系统传统上依赖于手动注释输入查询、检索段落和生成响应。我们引入了ARES，一个用于评估RAG系统的自动化评估系统，评估维度包括上下文相关性、答案忠实度和答案相关性。通过创建自己的合成训练数据，ARES微调轻量级LM评估器以评估单个RAG组件的质量。为了减少潜在的预测错误，ARES利用少量人工注释数据集进行预测驱动推理（PPI）。在KILT、SuperGLUE和AIS的八个不同知识密集型任务中，ARES在评估过程中仅使用少量人工注释就准确评估RAG系统。此外，ARES评估器在领域转移中仍然有效，即使在更改用于评估的查询和/或文档类型后仍然准确。

    arXiv:2311.09476v2 Announce Type: replace-cross  Abstract: Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the eva
    
[^221]: AbsPyramid：使用统一的蕴涵图评估语言模型的抽象能力

    AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph

    [https://arxiv.org/abs/2311.09174](https://arxiv.org/abs/2311.09174)

    该研究提出了AbsPyramid，一个统一的蕴涵图，用于评估语言模型的抽象能力，实验证明LLMs可以通过在丰富抽象知识上进行训练来获得基本的抽象能力，并泛化到未见的事件。

    

    认知研究表明，抽象能力对人类智能至关重要，而这在语言模型中仍然没有得到充分探讨。本文介绍了AbsPyramid，这是一个包含221K个文本描述的统一蕴涵图，用于收集广泛事件三个组成部分的抽象知识，以全面评估语言模型在开放领域中的抽象能力。实验结果表明，目前的LLMs在零短和少量数据情况下面临理解抽象知识的挑战。通过在我们丰富的抽象知识上进行训练，我们发现LLMs可以获得基本的抽象能力，并泛化到未见的事件。同时，我们实证表明我们的基准可以全面提高LLMs在两个先前的抽象任务中的性能。

    arXiv:2311.09174v3 Announce Type: replace-cross  Abstract: Cognitive research indicates that abstraction ability is essential in human intelligence, which remains under-explored in language models. In this paper, we present AbsPyramid, a unified entailment graph of 221K textual descriptions of abstraction knowledge. While existing resources only touch nouns or verbs within simplified events or specific domains, AbsPyramid collects abstract knowledge for three components of diverse events to comprehensively evaluate the abstraction ability of language models in the open domain. Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings. By training on our rich abstraction knowledge, we find LLMs can acquire basic abstraction abilities and generalize to unseen events. In the meantime, we empirically show that our benchmark is comprehensive to enhance LLMs across two previous abstraction tasks.
    
[^222]: R-Spin: 高效的说话者和噪声不变表示学习与声学片段

    R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces

    [https://arxiv.org/abs/2311.09117](https://arxiv.org/abs/2311.09117)

    R-Spin是一种高效的领域特定自我监督方法，通过学习离散的声学单元来实现说话者和噪声不变的语音表示，相比之前的方法在计算资源上减少12倍同时在严重失真语音场景中表现更好。

    

    本文介绍了Robust Spin（R-Spin），一种数据高效的领域特定的自我监督方法，用于通过学习具有说话者不变聚类（Spin）的离散声学单元来实现说话者和噪声不变的语音表示。 R-Spin通过学习预测声学片段解决了Spin的问题并增强了内容表示。与先前的最先进方法相比，R-Spin在计算资源上实现了12倍的减少，同时在严重失真的语音场景中表现优异。本文提供了详细分析，展示了离散单元如何有助于语音编码器训练以及在多样的声学环境中提高鲁棒性。

    arXiv:2311.09117v2 Announce Type: replace  Abstract: This paper introduces Robust Spin (R-Spin), a data-efficient domain-specific self-supervision method for speaker and noise-invariant speech representations by learning discrete acoustic units with speaker-invariant clustering (Spin). R-Spin resolves Spin's issues and enhances content representations by learning to predict acoustic pieces. R-Spin offers a 12X reduction in computational resources compared to previous state-of-the-art methods while outperforming them in severely distorted speech scenarios. This paper provides detailed analyses to show how discrete units contribute to speech encoder training and improving robustness in diverse acoustic environments.
    
[^223]: 在大型语言模型中识别线性关系概念

    Identifying Linear Relational Concepts in Large Language Models

    [https://arxiv.org/abs/2311.08968](https://arxiv.org/abs/2311.08968)

    通过线性关系概念(LRC)技术，可以在大型语言模型的隐藏激活潜在空间中找到与人类可解释概念对应的概念方向，这种技术超越了标准黑盒探测分类器。

    

    Transformer语言模型(LMs)已被证明可以将概念表示为隐藏激活的潜在空间中的方向。然而，对于任何可由人类解释的概念，我们如何在潜在空间中找到其方向呢？我们提出了一种称为线性关系概念(LRC)的技术，通过首先建模主体和客体之间的关系为线性关系嵌入(LRE)来找到与人类可解释概念对应的概念方向。我们发现，反转LRE并使用较早的客体层会导致一种强大的技术，用于找到胜过标准黑盒探测分类器的概念方向。我们评估了LRC作为概念分类器的性能，以及它们改变模型输出的因果能力。

    arXiv:2311.08968v2 Announce Type: replace-cross  Abstract: Transformer language models (LMs) have been shown to represent concepts as directions in the latent space of hidden activations. However, for any human-interpretable concept, how can we find its direction in the latent space? We present a technique called linear relational concepts (LRC) for finding concept directions corresponding to human-interpretable concepts by first modeling the relation between subject and object as a linear relational embedding (LRE). We find that inverting the LRE and using earlier object layers results in a powerful technique for finding concept directions that outperforms standard black-box probing classifiers. We evaluate LRCs on their performance as concept classifiers as well as their ability to causally change model output.
    
[^224]: Safer-Instruct: 使用自动化偏好数据对齐语言模型

    Safer-Instruct: Aligning Language Models with Automated Preference Data

    [https://arxiv.org/abs/2311.08685](https://arxiv.org/abs/2311.08685)

    Safer-Instruct通过反向指导调整、指导感应和专家模型评估的方式，实现了自动构建大规模偏好数据的目的，从而在没有人工标注者的情况下高效生成高质量的偏好数据。

    

    人工反馈强化学习（RLHF）是增强语言模型能力的重要策略。然而，为RLHF标注偏好数据是一项资源密集且需要创造力的过程，而现有的自动生成方法在数据多样性和质量方面存在局限性。为了应对这一挑战，我们提出了Safer-Instruct，这是一个用于自动构建大规模偏好数据的全新流水线。我们的方法利用了反向指导调整、指导感应和专家模型评估，以高效生成高质量的偏好数据，无需人工标注者。为了验证Safer-Instruct的有效性，我们将该流水线应用于构建一个安全偏好数据集作为案例研究。在这个合成数据集上微调Alpaca模型不仅展示出更好的无害性，还表现出优于在人工标注的安全偏好数据上微调的模型，同时保持

    arXiv:2311.08685v2 Announce Type: replace-cross  Abstract: Reinforcement learning from human feedback (RLHF) is a vital strategy for enhancing model capability in language models. However, annotating preference data for RLHF is a resource-intensive and creativity-demanding process, while existing automatic generation methods face limitations in data diversity and quality. In response, we present Safer-Instruct, a novel pipeline for automatically constructing large-scale preference data. Our approach leverages reversed instruction tuning, instruction induction, and expert model evaluation to efficiently generate high-quality preference data without human annotators. To verify the effectiveness of Safer-Instruct, we apply the pipeline to construct a safety preference dataset as a case study. Finetuning an Alpaca model on this synthetic dataset not only demonstrates improved harmlessness but also outperforms models fine-tuned on human-annotated safety preference data, all the while mainta
    
[^225]: 低秩适应用于多语言摘要：一项实证研究

    Low-Rank Adaptation for Multilingual Summarization: An Empirical Study

    [https://arxiv.org/abs/2311.08572](https://arxiv.org/abs/2311.08572)

    LoRA 在多语言摘要方面竞争力强，特别在低数据情况和跨语言转移方面表现出色。

    

    尽管预训练的大型语言模型的进展显著加速了近年来自然语言处理领域的进步，但它们不断增长的体积对传统的微调提出了重要挑战，特别是在内存密集型任务中。我们调查了参数高效微调的潜力，重点是低秩适应（LoRA），涉及多语言摘要领域，这是一个具有挑战性的任务（因为输入通常很长），且相对未被充分探索。我们进行了一项广泛的研究，涵盖不同数据可用性场景，包括高数据和低数据设置，以及跨语言转移，利用不同规模的模型。我们的发现表明，当使用大量数据训练时，LoRA与完全微调竞争激烈，并且在低数据情况和跨语言转移方面表现出色。我们还研究了不同的少数据点跨语言转移策略，发现持续的LoRA调优...

    arXiv:2311.08572v2 Announce Type: replace-cross  Abstract: Although the advancements of pre-trained Large Language Models have significantly accelerated recent progress in NLP, their ever-increasing size poses significant challenges for conventional fine-tuning, especially in memory-intensive tasks. We investigate the potential of Parameter-Efficient Fine-Tuning, focusing on Low-Rank Adaptation (LoRA), in the domain of multilingual summarization, a task that is both challenging (due to typically long inputs), and relatively unexplored. We conduct an extensive study across different data availability scenarios, including high- and low-data settings, and cross-lingual transfer, leveraging models of different sizes. Our findings reveal that LoRA is competitive with full fine-tuning when trained with high quantities of data, and excels in low-data scenarios and cross-lingual transfer. We also study different strategies for few-shot cross-lingual transfer, finding that continued LoRA tuning
    
[^226]: 多元视角的公平抽象总结

    Fair Abstractive Summarization of Diverse Perspectives

    [https://arxiv.org/abs/2311.07884](https://arxiv.org/abs/2311.07884)

    该论文系统地研究了如何在抽象总结中实现公平性，提出了四个无参考自动度量标准，评估了九个大型语言模型，并在多个来源的数据集上进行了实验验证。

    

    来自不同社会和人口群体的人们对产品评论、医疗保健、法律和政治等一系列主题表达了多元化的观点和相互冲突的意见。 公平的总结应提供对多元化观点的全面覆盖，而不会减少某些群体的代表性。然而，目前关于摘要度量标准和大型语言模型（LLM）评估的工作尚未探讨公平的抽象总结。 本文系统地研究了用户生成数据的公平抽象总结。 我们首先正式定义了抽象总结中的公平性，即不减少任何群体的观点，然后通过衡量目标和源观点之间的差异提出了四个无参考自动度量标准。 我们评估了九个LLM，包括三个GPT模型，四个LLaMA模型，PaLM 2 和 Claude，在来自社交媒体、在线评论等六个数据集上进行了评估。

    arXiv:2311.07884v2 Announce Type: replace  Abstract: People from different social and demographic groups express diverse perspectives and conflicting opinions on a broad set of topics such as product reviews, healthcare, law, and politics. A fair summary should provide a comprehensive coverage of diverse perspectives without underrepresenting certain groups. However, current work in summarization metrics and Large Language Models (LLMs) evaluation has not explored fair abstractive summarization. In this paper, we systematically investigate fair abstractive summarization for user-generated data. We first formally define fairness in abstractive summarization as not underrepresenting perspectives of any groups of people, and we propose four reference-free automatic metrics by measuring the differences between target and source perspectives. We evaluate nine LLMs, including three GPT models, four LLaMA models, PaLM 2, and Claude, on six datasets collected from social media, online reviews,
    
[^227]: 在上下文学习和梯度下降的再审视

    In-context Learning and Gradient Descent Revisited

    [https://arxiv.org/abs/2311.07772](https://arxiv.org/abs/2311.07772)

    本文重新审视了在现实NLP任务和模型中的上下文学习（ICL）与梯度下降（GD）之间的联系证据，找到了评估上的不足，提出了遵循层次因果关系的简单GD优化程序来改善相似性分数。

    

    在上下文学习（ICL）已在少样本学习任务中取得了令人印象深刻的结果，然而其基本机制仍未被充分理解。最近的一系列研究表明，ICL隐式地执行梯度下降（GD）优化。尽管具有吸引力，但很多研究集中在简化设置，其中优化浅层模型的参数。在这项工作中，我们重新审视了针对现实NLP任务和模型的ICL-GD对应的证据。我们发现在评估方面存在差距，无论是在有问题的指标还是不足的基线方面。我们发现令人惊讶的是，即使是未经训练的模型也能实现可比的ICL-GD相似性分数，尽管未表现出ICL。接下来，我们探讨了模型中信息流动在ICL和GD之间的主要差异，我们将其称为“层因果关系”。我们提出了一个尊重层因果关系的简单GD优化过程，并表明它显著改善了相似性分数。

    arXiv:2311.07772v4 Announce Type: replace  Abstract: In-context learning (ICL) has shown impressive results in few-shot learning tasks, yet its underlying mechanism is still not fully understood. A recent line of work suggests that ICL performs gradient descent (GD)-based optimization implicitly. While appealing, much of the research focuses on simplified settings, where the parameters of a shallow model are optimized. In this work, we revisit evidence for ICL-GD correspondence on realistic NLP tasks and models. We find gaps in evaluation, both in terms of problematic metrics and insufficient baselines. We show that surprisingly, even untrained models achieve comparable ICL-GD similarity scores despite not exhibiting ICL. Next, we explore a major discrepancy in the flow of information throughout the model between ICL and GD, which we term Layer Causality. We propose a simple GD-based optimization procedure that respects layer causality, and show it improves similarity scores significan
    
[^228]: 火山：通过自反馈引导修订减少多模式幻觉

    Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision

    [https://arxiv.org/abs/2311.07362](https://arxiv.org/abs/2311.07362)

    火山模型通过自反馈引导修订的方式，有效减少多模态幻觉问题，取得了在各项评测中的最新技术水平。

    

    大型多模态模型存在多模态幻觉问题，即提供与给定视觉信息不符的错误响应。最近的研究推测，多模态幻觉的原因之一是视觉编码器未能正确地与图像对齐。为了缓解这一问题，我们提出了一种利用自反馈作为视觉线索的新方法。基于这种方法，我们引入了火山，一个多模态自反馈引导修订模型。火山根据提供的视觉信息为其初始响应生成自然语言反馈，并利用此反馈来自我修订其初始响应。火山有效地减少了多模态幻觉，并在MMHal-Bench、POPE和GAVIE上实现了最新技术水平。它还提高了一般多模态能力，并在MM-Vet和MMBench上优于先前的模型。通过定性分析，我们展示了火山的...

    arXiv:2311.07362v3 Announce Type: replace  Abstract: Large multimodal models suffer from multimodal hallucination, where they provide incorrect responses misaligned with the given visual information. Recent works have conjectured that one of the reasons behind multimodal hallucination is due to the vision encoder failing to ground on the image properly. To mitigate this issue, we propose a novel approach that leverages self-feedback as visual cues. Building on this approach, we introduce Volcano, a multimodal self-feedback guided revision model. Volcano generates natural language feedback to its initial response based on the provided visual information and utilizes this feedback to self-revise its initial response. Volcano effectively reduces multimodal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general multimodal abilities and outperforms previous models on MM-Vet and MMBench. Through qualitative analysis, we show that Volcano's fe
    
[^229]: 探索大型语言模型对话理解中的事实一致性

    Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models

    [https://arxiv.org/abs/2311.07194](https://arxiv.org/abs/2311.07194)

    本研究提出了一种评估大型语言模型对话理解能力的方法，通过对话总结任务检测事实一致性问题，并提出用事实问题来作为对话理解能力的灵活衡量标准，结果显示平均约有26.8%的大型语言模型生成的摘要存在事实不一致性。

    

    LLMs（大型语言模型）通常以对话的形式与用户交互，并根据其指令生成回复，这自然需要对话理解能力。然而，对话理解是一种难以直接评估的通用语言能力。本文提出通过对话总结任务的帮助，集中进行对事实一致性问题的评估。除了评估和分析不同LLMs的对话总结性能（DIAC-Sum）外，我们还从生成的摘要中提取事实问题，并将其用作对话理解的更灵活的测量（DIAC-QA）。我们的评估显示，平均而言，由LLMs生成的摘要中有26.8%包含事实不一致。即使是ChatGPT，评估中最强大的模型，在其摘要中也有16%存在此类错误。对于回答事实问题，这更具挑战性。

    arXiv:2311.07194v3 Announce Type: replace  Abstract: LLMs (Large Language Models) usually interact with users in the form of dialogue and generate responses following their instructions, which naturally require dialogue comprehension abilities. However, dialogue comprehension is a general language ability which is hard to be evaluated directly. In this work, we propose to perform the evaluation focusing on the factual consistency issue with the help of the dialogue summarization task. Besides evaluating and analyzing the dialogue summarization performance (DIAC-Sum) of different LLMs, we also derive factual questions from the generated summaries and use them as a more flexible measurement of dialogue comprehension (DIAC-QA). Our evaluation shows that, on average, 26.8% of the summaries generated by LLMs contain factual inconsistency. Even ChatGPT, the strongest model evaluated, has such errors in 16% of its summaries. For answering the factual questions, which is more challenging, the 
    
[^230]: 火焰: 评估中国大型语言模型与人类价值观的契合性的基准测试

    Flames: Benchmarking Value Alignment of Chinese Large Language Models

    [https://arxiv.org/abs/2311.06899](https://arxiv.org/abs/2311.06899)

    中国的大型语言模型的价值观契合性需要更加全面的评估，该研究提出了一个名为"火焰"（Flames）的基准测试，涵盖了常见的无害原则和特定中国价值观，以及复杂场景和隐含恶意的提示方法。

    

    大型语言模型（LLMs）在各个地区的广泛应用强调了评估它们与人类价值观契合性的迫切性。然而，当前的基准测试未能有效地揭示LLMs中的安全漏洞。尽管许多模型在这些评估中得分很高，且“名列前茅”，但在LLMs与人类价值观的深层契合性和实现真正无害方面仍存在重大差距。为此，本文提出了一个名为"火焰"（Flames）的价值观契合性基准测试，该测试涵盖了常见的无害原则，以及一个整合了特定中国价值观如和谐的独特道德维度。因此，我们精心设计了包含复杂情境和大多带有隐含恶意的破解方法的对抗性提示。通过对17个主流LLMs进行提示，我们获得了模型的回应，并对其进行了详细评估。

    arXiv:2311.06899v2 Announce Type: replace-cross  Abstract: The widespread adoption of large language models (LLMs) across various regions underscores the urgent need to evaluate their alignment with human values. Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs. Despite numerous models achieving high scores and 'topping the chart' in these evaluations, there is still a significant gap in LLMs' deeper alignment with human values and achieving genuine harmlessness. To this end, this paper proposes a value alignment benchmark named Flames, which encompasses both common harmlessness principles and a unique morality dimension that integrates specific Chinese values such as harmony. Accordingly, we carefully design adversarial prompts that incorporate complex scenarios and jailbreaking methods, mostly with implicit malice. By prompting 17 mainstream LLMs, we obtain model responses and rigorously annotate them for detailed evaluation. Our findin
    
[^231]: 假对齐：LLMs真的对齐得好吗？

    Fake Alignment: Are LLMs Really Aligned Well?

    [https://arxiv.org/abs/2311.05915](https://arxiv.org/abs/2311.05915)

    研究发现大型语言模型（LLMs）在安全性评估中存在假对齐问题，提出了Fake alIgNment Evaluation (FINE)框架和两个新的度量标准，用于验证和修正这一现象

    

    随着对大型语言模型（LLMs）安全问题的关注日益增强，人们对安全评估产生了相当大的兴趣。本研究探讨了评估LLMs的一个未被充分研究的问题，即多项选择题和开放式问题之间的表现差异。受到越狱攻击模式研究的启发，我们认为这是由于泛化不匹配所引起的。也就是说，LLMs只记住了开放式安全问题的答案风格，这使其无法解决其他形式的安全测试。我们将这种现象称为假对齐，并构建了一个比较基准来在LLMs中经验性地验证其存在。我们引入了一个Fake alIgNment Evaluation (FINE)框架和两个新颖的度量标准--一致性分数（CS）和一致的安全分数（CSS），它们共同评估两种互补形式的评估，以量化假对齐并获得更正

    arXiv:2311.05915v3 Announce Type: replace-cross  Abstract: The growing awareness of safety concerns in large language models (LLMs) has sparked considerable interest in the evaluation of safety. This study investigates an under-explored issue about the evaluation of LLMs, namely the substantial discrepancy in performance between multiple-choice questions and open-ended questions. Inspired by research on jailbreak attack patterns, we argue this is caused by mismatched generalization. That is, LLM only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests. We refer to this phenomenon as fake alignment and construct a comparative benchmark to empirically verify its existence in LLMs. We introduce a Fake alIgNment Evaluation (FINE) framework and two novel metrics--Consistency Score (CS) and Consistent Safety Score (CSS), which jointly assess two complementary forms of evaluation to quantify fake alignment and obtain corrected
    
[^232]: LLMs与Fine-tuning: 对仇恨言论检测跨领域性能的基准测试

    LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection

    [https://arxiv.org/abs/2310.18964](https://arxiv.org/abs/2310.18964)

    本研究调查了预训练和微调的Large Language Models在识别仇恨言论方面的有效性和适应性，揭示了即使没有预训练，LLMs在性能上仍然具有极大优势。

    

    在在线交流不断发展的环境中，仇恨言论检测仍然是一个严峻的挑战，数字平台的多样性进一步加剧了这一挑战。本研究调查了预训练和微调的大型语言模型（LLMs）在识别仇恨言论中的有效性和适应性，以解决两个核心问题：（1）模型性能在多大程度上依赖于微调和训练参数？（2）模型在跨领域仇恨言论检测中的泛化程度如何？以及（3）影响泛化潜力的数据集或模型的具体特征是什么？实验证明，即使没有预训练，LLMs也比最先进的模型具有巨大优势。为了回答问题（1），我们分析了36个领域内分类器，涵盖了LLaMA、Vicuna及其不同的预训练和微调状态，跨越了九个公开可用数据集，涵盖了各种平台。

    arXiv:2310.18964v2 Announce Type: replace  Abstract: In the evolving landscape of online communication, hate speech detection remains a formidable challenge, further compounded by the diversity of digital platforms. This study investigates the effectiveness and adaptability of pre-trained and fine-tuned Large Language Models (LLMs) in identifying hate speech, to address two central questions: (1) To what extent does the model performance depend on the fine-tuning and training parameters?, (2) To what extent do models generalize to cross-domain hate speech detection? and (3) What are the specific features of the datasets or models that influence the generalization potential? The experiment shows that LLMs offer a huge advantage over the state-of-the-art even without pretraining. To answer (1) we analyze 36 in-domain classifiers comprising LLaMA, Vicuna, and their variations in pre-trained and fine-tuned states across nine publicly available datasets that span a wide range of platforms a
    
[^233]: DistillSpec：通过知识蒸馏改进投机性解码

    DistillSpec: Improving Speculative Decoding via Knowledge Distillation

    [https://arxiv.org/abs/2310.08461](https://arxiv.org/abs/2310.08461)

    DistillSpec通过知识蒸馏技术改进了投机性解码，在标准基准测试中取得了10-45%的速度提升。

    

    投机性解码（SD）通过使用更快的草稿模型生成多个标记，然后由更大的目标模型并行验证这些标记，从而生成符合目标模型分布的文本，加速大型语言模型推理。然而，找到与目标模型良好对齐的紧凑草稿模型具有挑战性。为了解决这个问题，我们提出了DistillSpec，它使用知识蒸馏来更好地将草稿模型与目标模型对齐，然后应用SD。DistillSpec做出了两个关键设计选择，我们通过系统研究证明这对改进草稿和目标对齐至关重要：利用来自草稿模型的on-policy数据生成，以及将发散函数定制到任务和解码策略。值得注意的是，DistillSpec在一系列标准基准测试上比标准SD获得了令人印象深刻的10-45%的加速，使用贪婪和非贪婪方法。

    arXiv:2310.08461v2 Announce Type: replace-cross  Abstract: Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-gr
    
[^234]: 大型语言模型的复合后门攻击

    Composite Backdoor Attacks Against Large Language Models

    [https://arxiv.org/abs/2310.07676](https://arxiv.org/abs/2310.07676)

    该研究通过复合后门攻击(CBA)展示了在大型语言模型中植入多个触发关键词的方法，相较于现有方法更为隐蔽，并确保只有当所有触发关键词同时出现时后门才会被激活。

    

    大型语言模型（LLMs）在各种任务上表现出优越性能，通常作为许多研究和服务的基础模型。然而，不可信任的第三方LLMs可能会暗中为下游任务引入漏洞。本文通过后门攻击的视角探讨了LLMs的脆弱性。与现有的对LLMs的后门攻击不同，我们在不同的提示组件中分散多个触发关键词。这种复合后门攻击（CBA）被证明比仅在单个组件中植入相同的多个触发关键词更隐蔽。CBA确保只有当所有触发关键词出现时后门才会被激活。我们的实验表明，CBA在自然语言处理（NLP）和多模式任务中都是有效的。

    arXiv:2310.07676v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services. However, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks. In this paper, we explore the vulnerability of LLMs through the lens of backdoor attacks. Different from existing backdoor attacks against LLMs, ours scatters multiple trigger keys in different prompt components. Such a Composite Backdoor Attack (CBA) is shown to be stealthier than implanting the same multiple trigger keys in only a single component. CBA ensures that the backdoor is activated only when all trigger keys appear. Our experiments demonstrate that CBA is effective in both natural language processing (NLP) and multimodal tasks. For instance, with $3\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset, our att
    
[^235]: FTFT:通过转移训练动态实现高效且稳健的微调

    FTFT: Efficient and Robust Fine-Tuning by Transferring Training Dynamics

    [https://arxiv.org/abs/2310.06588](https://arxiv.org/abs/2310.06588)

    训练动态可在不同模型大小和预训练方法之间进行转移，通过选定的训练实例微调主模型实现比经验风险最小化更高的训练效率

    

    尽管微调预训练语言模型（PLMs）取得了巨大成功，但它们仍然容易受到分布外输入的影响。 数据集制图是一种简单而有效的双模型方法，可以提高微调PLMs的鲁棒性。 它涉及在原始训练集上微调模型（即参考模型），根据训练动态选择一些重要的训练实例，并仅对这些选定的示例再次进行微调（即主模型）。 然而，这种方法需要对同一模型进行两次微调，这对于大型PLMs而言在计算上是昂贵的。 在本文中，我们展示了（1）训练动态在模型大小和预训练方法之间具有高度可传递性，以及（2）使用这些选定的训练实例对主模型进行微调可以比经验风险最小化（ERM）实现更高的训练效率。 基于这些观察结果，我们提出了一种新颖的微调方法...

    arXiv:2310.06588v2 Announce Type: replace  Abstract: Despite the massive success of fine-tuning Pre-trained Language Models (PLMs), they remain susceptible to out-of-distribution input. Dataset cartography is a simple yet effective dual-model approach that improves the robustness of fine-tuned PLMs. It involves fine-tuning a model on the original training set (i.e. reference model), selecting a subset of important training instances based on the training dynamics, and fine-tuning again only on these selected examples (i.e. main model). However, this approach requires fine-tuning the same model twice, which is computationally expensive for large PLMs. In this paper, we show that (1) training dynamics are highly transferable across model sizes and pre-training methods, and that (2) fine-tuning main models using these selected training instances achieves higher training efficiency than empirical risk minimization (ERM). Building on these observations, we propose a novel fine-tuning approa
    
[^236]: 迈向LogiGLUE：对语言模型逻辑推理能力的简要调查和基准的研究

    Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models

    [https://arxiv.org/abs/2310.00836](https://arxiv.org/abs/2310.00836)

    该研究旨在评估大型语言模型在逻辑推理中的表现，并提供了名为LogiGLUE的基准，以研究逻辑推理数据集、任务和利用语言模型进行推理的方法。

    

    逻辑推理对人类至关重要，但在人工智能领域中却是一个重大挑战。研究人员最初使用的知识表示与推理系统并不能很好地扩展，并且需要大量手动工作。最近，大型语言模型(LLMs)的出现表明了其能够克服形式化知识表示系统的各种局限性。因此，利用自然语言进行逻辑推理的LLMs备受关注。该工作旨在通过对此领域最新进展的简要回顾来了解LLMs在逻辑推理方面的熟练程度；重点关注逻辑推理数据集、任务以及采用的利用LLMs进行推理的方法。为了提供全面的分析，我们编制了一个名为LogiGLUE的基准。其中包括24个不同的数据集，涵盖演绎、进阶和归纳推理等方面。

    arXiv:2310.00836v3 Announce Type: replace-cross  Abstract: Logical reasoning is fundamental for humans yet presents a substantial challenge in the domain of Artificial Intelligence. Initially, researchers used Knowledge Representation and Reasoning (KR) systems that did not scale and required non-trivial manual effort. Recently, the emergence of large language models (LLMs) has demonstrated the ability to overcome various limitations of formal Knowledge Representation (KR) systems. Consequently, there's a growing interest in using LLMs for logical reasoning via natural language. This work strives to understand the proficiency of LLMs in logical reasoning by offering a brief review of the latest progress in this area; with a focus on the logical reasoning datasets, tasks, and the methods adopted to utilize LLMs for reasoning. To offer a thorough analysis, we have compiled a benchmark titled LogiGLUE. This includes 24 varied datasets encompassing deductive, abductive, and inductive reaso
    
[^237]: 对大规模生成视觉-语言模型的组合性的研究

    An Examination of the Compositionality of Large Generative Vision-Language Models

    [https://arxiv.org/abs/2308.10509](https://arxiv.org/abs/2308.10509)

    本文检验了生成视觉-语言模型的组合性，发现当前基准中存在句法偏见，提出了新的评估指标SyntaxBias Score和新的基准SyntActically D。

    

    随着大型语言模型（LLMs）的成功，许多生成视觉-语言模型（GVLMs）通过多模态指导调整得以构建。然而，GVLMs在多模态组合推理中的性能还未得到充分探索。本文旨在检验评估GVLMs组合性的评估指标（VisualGPTScore等）和当前基准。我们发现当前基准中的句法偏见，被GVLMs的语言能力所利用。这种偏见使得VisualGPTScore成为评估GVLMs的不足指标。为了解决这一问题，我们首先引入了一个SyntaxBias Score，利用LLMs量化此类偏见以进行缓解。随后添加了一个具有挑战性的新任务，以评估GVLMs对固有倾向于句法正确性的健壮性。利用缓解偏见的数据集和新任务，我们提出了一个新的基准，即SyntActically D

    arXiv:2308.10509v2 Announce Type: replace-cross  Abstract: With the success of Large Language Models (LLMs), many Generative Vision-Language Models (GVLMs) have been constructed via multimodal instruction tuning. However, the performance of GVLMs in multimodal compositional reasoning remains under-explored. In this paper, we examine both the evaluation metrics (VisualGPTScore, etc.) and current benchmarks for evaluating the compositionality of GVLMs. We identify the syntactical bias in current benchmarks, which is exploited by the linguistic capability of GVLMs. The bias renders VisualGPTScore an insufficient metric for assessing GVLMs. To combat this, we first introduce a SyntaxBias Score, leveraging LLMs to quantify such bias for mitigation. A challenging new task is subsequently added to evaluate the robustness of GVLMs against inherent inclination toward syntactical correctness. Using the bias-mitigated datasets and the new task, we propose a novel benchmark, namely SyntActically D
    
[^238]: 引文：构建负责任和可计算的大型语言模型的关键

    Citation: A Key to Building Responsible and Accountable Large Language Models

    [https://arxiv.org/abs/2307.02185](https://arxiv.org/abs/2307.02185)

    引文被确定为大型语言模型中关键但缺失的组成部分，其引入可以增强内容的透明性和可验证性以应对知识产权和伦理问题，提议LLMs的全面引文机制应考虑非参数化和参数化内容，尽管实施引文机制复杂且存在潜在缺陷，仍主张推动其发展并概述未来研究问题。

    

    大型语言模型（LLMs）带来了变革性的好处，同时也带来了独特的挑战，包括知识产权（IP）和伦理关切。本文探讨了缓解这些风险的新思路，从LLMs和已建立的Web系统之间的相似性出发。我们确定了“引文” - 对来源或证据的承认或引用 - 在LLMs中的关键缺失组成部分。引入引文可以增强内容的透明性和可验证性，从而应对LLMs的知识产权和伦理问题。我们进一步提议，LLMs的全面引文机制应考虑非参数化和参数化内容。尽管实施这样的引文机制的复杂性及潜在缺陷，我们主张推动其发展。基于这一基础，我们概述了该领域中的几个研究问题，旨在引导未来的探索方向。

    arXiv:2307.02185v3 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) bring transformative benefits alongside unique challenges, including intellectual property (IP) and ethical concerns. This position paper explores a novel angle to mitigate these risks, drawing parallels between LLMs and established web systems. We identify "citation" - the acknowledgement or reference to a source or evidence - as a crucial yet missing component in LLMs. Incorporating citation could enhance content transparency and verifiability, thereby confronting the IP and ethical issues in the deployment of LLMs. We further propose that a comprehensive citation mechanism for LLMs should account for both non-parametric and parametric content. Despite the complexity of implementing such a citation mechanism, along with the potential pitfalls, we advocate for its development. Building on this foundation, we outline several research problems in this area, aiming to guide future explorations towards
    
[^239]: TextFormer：基于查询的端到端文本识别器与混合监督

    TextFormer: A Query-based End-to-End Text Spotter with Mixed Supervision

    [https://arxiv.org/abs/2306.03377](https://arxiv.org/abs/2306.03377)

    TextFormer提出了基于查询的端到端文本识别器，采用Transformer架构，通过查询嵌入实现了联合语义理解，允许多任务建模中的深层特征共享，并设计了自适应全局聚合（AGG）模块用于读取任意形状的文本。

    

    端到端文本识别是一个重要的计算机视觉任务，旨在将场景文本检测和识别整合到一个统一的框架中。典型方法严重依赖于感兴趣区域（RoI）操作来提取局部特征，并使用复杂的后处理步骤生成最终预测。为了解决这些限制，我们提出了TextFormer，这是一个基于Transformer架构的基于查询的端到端文本识别器。具体而言，使用每个文本实例的查询嵌入，TextFormer在图像编码器和文本解码器之上构建，以学习用于多任务建模的联合语义理解。它允许对分类、分割和识别分支进行相互训练和优化，实现更深的特征共享，而不会牺牲灵活性或简单性。此外，我们设计了一个自适应全局聚合（AGG）模块，将全局特征转换为顺序特征，以读取任意形状的文本。

    arXiv:2306.03377v2 Announce Type: replace-cross  Abstract: End-to-end text spotting is a vital computer vision task that aims to integrate scene text detection and recognition into a unified framework. Typical methods heavily rely on Region-of-Interest (RoI) operations to extract local features and complex post-processing steps to produce final predictions. To address these limitations, we propose TextFormer, a query-based end-to-end text spotter with Transformer architecture. Specifically, using query embedding per text instance, TextFormer builds upon an image encoder and a text decoder to learn a joint semantic understanding for multi-task modeling. It allows for mutual training and optimization of classification, segmentation, and recognition branches, resulting in deeper feature sharing without sacrificing flexibility or simplicity. Additionally, we design an Adaptive Global aGgregation (AGG) module to transfer global features into sequential features for reading arbitrarily-shape
    
[^240]: MCTS：一个多参考的中文文本简化数据集

    MCTS: A Multi-Reference Chinese Text Simplification Dataset

    [https://arxiv.org/abs/2306.02796](https://arxiv.org/abs/2306.02796)

    该论文介绍了MCTS，一个多参考的中文文本简化数据集，提供了评估数据和性能分析，同时还发布了用于训练的中文文本简化平行数据，为未来中文文本简化研究提供了基础和参考。

    

    文本简化旨在通过应用重写转换使文本更容易理解。长期以来，关于中文文本简化的研究很少。通用评估数据的缺乏是这种现象的一个重要原因。本文介绍了MCTS，一个多参考的中文文本简化数据集。我们描述了数据集的注释过程并提供了详细分析。此外，我们评估了几种无监督方法和先进的大型语言模型的性能。我们还提供了可以用于训练的中文文本简化平行数据，通过利用机器翻译和英文文本简化获得。我们希望通过基础工作建立对中文文本简化的基本理解，并为未来的研究提供参考。所有的代码和数据已在https://github.com/blcuicall/mcts/发布。

    arXiv:2306.02796v2 Announce Type: replace  Abstract: Text simplification aims to make the text easier to understand by applying rewriting transformations. There has been very little research on Chinese text simplification for a long time. The lack of generic evaluation data is an essential reason for this phenomenon. In this paper, we introduce MCTS, a multi-reference Chinese text simplification dataset. We describe the annotation process of the dataset and provide a detailed analysis. Furthermore, we evaluate the performance of several unsupervised methods and advanced large language models. We additionally provide Chinese text simplification parallel data that can be used for training, acquired by utilizing machine translation and English text simplification. We hope to build a basic understanding of Chinese text simplification through the foundational work and provide references for future research. All of the code and data are released at https://github.com/blcuicall/mcts/.
    
[^241]: BootAug: 通过混合实例过滤框架增强文本增强

    BootAug: Boosting Text Augmentation via Hybrid Instance Filtering Framework

    [https://arxiv.org/abs/2210.02941](https://arxiv.org/abs/2210.02941)

    提出了BootAug，一个基于预训练语言模型的混合实例过滤框架，能够改进文本增强方法在大规模数据集上的表现，提高了约2-3%的分类准确率。

    

    文本增强是解决自然语言处理中数据不足问题的有效技术。然而，现有的文本增强方法往往专注于少样本场景，并且通常在大规模公开数据集上表现不佳。我们的研究表明，现有的增强方法往往生成具有移位特征空间的实例，导致在增强数据上表现下降。为了解决这一问题，我们提出了一个基于预训练语言模型的混合实例过滤框架（BootAug），该框架能够保持与自然数据集类似的特征空间。BootAug可以应用于现有的文本增强方法（如同义词替换和反向翻译），并在分类准确率上显著提高了约 2-3% 的增强性能。

    arXiv:2210.02941v2 Announce Type: replace  Abstract: Text augmentation is an effective technique for addressing the problem of insufficient data in natural language processing. However, existing text augmentation methods tend to focus on few-shot scenarios and usually perform poorly on large public datasets. Our research indicates that existing augmentation methods often generate instances with shifted feature spaces, which leads to a drop in performance on the augmented data (for example, EDA generally loses $\approx 2\%$ in aspect-based sentiment classification). To address this problem, we propose a hybrid instance-filtering framework (BootAug) based on pre-trained language models that can maintain a similar feature space with natural datasets. BootAug is transferable to existing text augmentation methods (such as synonym substitution and back translation) and significantly improves the augmentation performance by $\approx 2-3\%$ in classification accuracy. Our experimental results 
    
[^242]: 自然语言上的多步演绎推理：基于超领域泛化的实证研究

    Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation

    [https://arxiv.org/abs/2207.14000](https://arxiv.org/abs/2207.14000)

    提出了IMA-GloVe-GA，一个用于自然语言表达的多步推理的迭代神经推理网络，在超领域泛化方面具有更好的性能表现。

    

    将深度学习与符号逻辑推理结合起来，旨在充分利用这两个领域的成功，并引起了越来越多的关注。受DeepLogic启发，该模型经过端到端训练，用于执行逻辑程序推理，我们介绍了IMA-GloVe-GA，这是一个用自然语言表达的多步推理的迭代神经推理网络。在我们的模型中，推理是使用基于RNN的迭代内存神经网络进行的，其中包含一个门关注机制。我们在PARARULES、CONCEPTRULES V1和CONCEPTRULES V2三个数据集上评估了IMA-GloVe-GA。实验结果表明，带有门关注机制的DeepLogic比DeepLogic和其他RNN基线模型能够实现更高的测试准确性。我们的模型在规则被打乱时比RoBERTa-Large实现了更好的超领域泛化性能。此外，为了解决当前多步推理数据集中推理深度不平衡的问题

    arXiv:2207.14000v2 Announce Type: replace-cross  Abstract: Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by DeepLogic, an end-to-end model trained to perform inference on logic programs, we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on RNN with a gate attention mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention can achieve higher test accuracy than DeepLogic and other RNN baseline models. Our model achieves better out-of-distribution generalisation than RoBERTa-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datase
    
[^243]: 健康文本简化：消化癌症教育的注释语料库和增强学习的新策略

    Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning. (arXiv:2401.15043v1 [cs.CL])

    [http://arxiv.org/abs/2401.15043](http://arxiv.org/abs/2401.15043)

    该论文介绍了一个用于健康文本简化研究的消化癌症教育材料的注释语料库，并探索了基于大型语言模型的简化方法，包括微调、增强学习、增强学习与人类反馈、领域自适应和基于提示的应用。

    

    目标：健康教育材料的阅读水平显著影响信息的可理解性和可接触性，特别是对于少数族裔人群。许多患者教育资源超过了广泛接受的标准的阅读水平和复杂性。在健康信息中，急需高性能的文本简化模型以增强传播和识字能力。这种需要在癌症教育中尤为迫切，有效的预防和筛查教育可以大大减少发病率和死亡率。方法：我们引入了简化的消化癌症（SimpleDC）并行语料库，用于健康文本简化研究。利用SimpleDC和现有的Med-EASi语料库，我们探索了基于大型语言模型（LLM）的简化方法，包括微调、增强学习（RL）、增强学习与人类反馈（RLHF）、领域自适应和基于提示的应用。

    Objective: The reading level of health educational materials significantly influences information understandability and accessibility, particularly for minoritized populations. Many patient educational resources surpass the reading level and complexity of widely accepted standards. There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy. This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality.  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based app
    
[^244]: 推理的拓扑学：揭秘思维链、树和图

    Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])

    [http://arxiv.org/abs/2401.14295](http://arxiv.org/abs/2401.14295)

    这篇论文探讨了结合结构的提示工程在提高大型语言模型推理性能方面的前景，通过思维链、思维树或思维图的设计来引导整体推理过程。通过大量实例，这种范式显著增强了模型在多个任务中的能力。总的来说，论文提供了一个通用蓝图，为未来的发展铺平道路。

    

    自然语言处理（NLP）领域近年来取得了显著进展，特别是在通过创新的提示技术提高大型语言模型（LLM）性能方面。其中，与结构相结合的提示工程被视为一种有前途的范式，其设计如思维链、思维树或思维图等，通过结构指导整体LLM推理过程。通过大量实例的说明，这种范式显著增强了LLM在逻辑或数学推理、规划或创造性写作等各种任务中的能力。为了方便理解这个不断发展的领域并为未来的发展铺平道路，我们设计了一个有效和高效的LLM推理方案的通用蓝图。为此，我们对提示执行流程进行了深入分析，澄清并明确定义了不同的概念。然后我们建立第一个分类系统

    The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
    
[^245]: TelME：教师导向的多模融合网络用于对话中的情绪识别

    TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation. (arXiv:2401.12987v1 [cs.CL])

    [http://arxiv.org/abs/2401.12987](http://arxiv.org/abs/2401.12987)

    TelME是一种教师导向的多模融合网络，通过跨模态知识蒸馏实现对话中情绪识别的优化，取得了在多说话人数据集MELD上的最先进性能。

    

    对话中的情绪识别在使对话系统能够有效回应用户请求方面起着至关重要的作用。对话中的情绪可以通过音频、视觉和文本等多种模态的表示进行识别。然而，由于非语言模态对识别情绪的贡献较弱，多模态情绪识别一直被认为是一项具有挑战性的任务。本文提出了一种用于对话中情绪识别的教师导向多模融合网络（TelME）。TelME通过跨模态知识蒸馏将信息从作为教师的语言模型传递给非语言的学生，从而优化了弱模态的效能。然后，我们采用一种移动融合方法将多模态特征组合起来，其中学生网络支持教师。TelME在MELD（一种用于对话情绪识别的多说话人数据集）上实现了最先进的性能。最后，我们通过额外的实验论证了我们组件的有效性。

    Emotion Recognition in Conversation (ERC) plays a crucial role in enabling dialogue systems to effectively respond to user requests. The emotions in a conversation can be identified by the representations from various modalities, such as audio, visual, and text. However, due to the weak contribution of non-verbal modalities to recognize emotions, multimodal ERC has always been considered a challenging task. In this paper, we propose Teacher-leading Multimodal fusion network for ERC (TelME). TelME incorporates cross-modal knowledge distillation to transfer information from a language model acting as the teacher to the non-verbal students, thereby optimizing the efficacy of the weak modalities. We then combine multimodal features using a shifting fusion approach in which student networks support the teacher. TelME achieves state-of-the-art performance in MELD, a multi-speaker conversation dataset for ERC. Finally, we demonstrate the effectiveness of our components through additional expe
    
[^246]: 生命科学领域通过度量学习对领域漂移下的命名标签进行处理

    Name Tagging Under Domain Shift via Metric Learning for Life Sciences. (arXiv:2401.10472v1 [cs.CL])

    [http://arxiv.org/abs/2401.10472](http://arxiv.org/abs/2401.10472)

    本论文通过度量学习提出了一种处理生命科学领域下领域漂移的命名标签的方法，通过将源实体和目标实体投影到特征空间的不同区域来减轻源实体错误标记为目标实体的问题。

    

    命名标签是信息抽取（IE）的关键组成部分，尤其在生物医学和化学等科学领域，大型语言模型（LLM），例如ChatGPT，表现不佳。我们研究了将迁移学习应用于增强在生物医学领域（源领域）训练的命名标签模型在化学领域（目标领域）中的使用。在少样本学习设置中训练这样的模型的一种常见做法是在标记的源数据上预训练模型，然后在少量标记的目标示例上进行微调。在我们的实验中，我们观察到这样的模型容易将源实体错误标记为目标实体，因为源实体经常出现在文本中。为了解决这个问题，我们提出了一种模型，将知识从源领域转移到目标领域，但同时将源实体和目标实体投影到特征空间的不同区域。

    Name tagging is a key component of Information Extraction (IE), particularly in scientific domains such as biomedicine and chemistry, where large language models (LLMs), e.g., ChatGPT, fall short. We investigate the applicability of transfer learning for enhancing a name tagging model trained in the biomedical domain (the source domain) to be used in the chemical domain (the target domain). A common practice for training such a model in a few-shot learning setting is to pretrain the model on the labeled source data, and then, to finetune it on a hand-full of labeled target examples. In our experiments we observed that such a model is prone to mis-labeling the source entities, which can often appear in the text, as the target entities. To alleviate this problem, we propose a model to transfer the knowledge from the source domain to the target domain, however, at the same time, to project the source entities and target entities into separate regions of the feature space. This diminishes 
    
[^247]: DeepEdit: 带有约束的解码式知识编辑

    DeepEdit: Knowledge Editing as Decoding with Constraints. (arXiv:2401.10471v1 [cs.CL])

    [http://arxiv.org/abs/2401.10471](http://arxiv.org/abs/2401.10471)

    DeepEdit是一种神经符号方法，通过更好的推理一致性和对更新知识的意识，提高了大型语言模型的知识编辑能力，对多跳问题数据集MQuaKE取得了显著的进展。

    

    我们将大型语言模型（LLMs）的知识编辑视为带有约束的解码过程。我们提出了DeepEdit（基于深度优先搜索的渐进式解码知识编辑），这是一种神经符号方法，通过更好的推理一致性、问题相关性和对更新知识的意识来改进知识编辑。DeepEdit可灵活应用于所有黑盒LLMs：不需要访问模型参数、表示或输出词汇分布。DeepEdit逐步产生高质量的推理步骤，以实现有效的知识编辑。它利用深度优先搜索来修改LLMs的输出，从而提高输出对问题的相关性和对更新知识的意识。在知识编辑方面，DeepEdit在控制LLMs产生更简洁的推理方面表现出色。在MQuaKE上，DeepEdit在定量上取得了显著的进展，这是一个具有挑战性的多跳问题数据集。

    We develop a new perspective of knowledge editing for large language models (LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that improves knowledge editing with better coherence of reasoning, relevance to the question, and awareness of updated knowledge. DeepEdit can be flexibly applied to all black-box LLMs: it does not require any access to the model parameters, representations, or output vocabulary distributions. DeepEdit progressively produces the high-quality reasoning steps towards effective knowledge editing. It utilizes a depth-first search to revise the LLMs' output, which improves the output's informativeness to the input question and awareness of the updated knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit yields significant gains on MQuaKE, a challenging multi-hop que
    
[^248]: LLM作为合著者：检测LLM-Human混合文本的挑战

    LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase. (arXiv:2401.05952v1 [cs.CL])

    [http://arxiv.org/abs/2401.05952](http://arxiv.org/abs/2401.05952)

    本论文介绍了混合大小写（mixcase）的概念，探讨了机器生成文本和人工生成文本的混合情景，并构建了适用于研究这些情景的数据集MixSet。通过实验证明目前的MGT检测器对于混合文本的检测效果不佳。

    

    随着大型语言模型（LLM）的显著发展和广泛应用，机器生成文本（MGT）的使用正变得越来越普遍。这一趋势带来了潜在的风险，特别是对于新闻和教育等领域信息的质量和完整性而言。目前的研究主要解决检测纯MGT而未充分解决包括AI修改的人工文本（HWT）或经人工修改的MGT在内的混合情景。为了应对这一挑战，我们引入了混合大小写（mixcase）这一新概念，表示一种同时涉及机器生成和人工生成内容的混合文本形式。我们收集了来自多个日常文本编辑场景的mixcase实例，并构建了MixSet，这是专用于研究这些混合修改情景的第一个数据集。我们进行实验来评估流行的MGT检测器的效果、稳健性和泛化性能。我们的结果显示现有的MGT检测器对于混合文本的检测效果不理想。

    With the remarkable development and widespread applications of large language models (LLMs), the use of machine-generated text (MGT) is becoming increasingly common. This trend brings potential risks, particularly to the quality and completeness of information in fields such as news and education. Current research predominantly addresses the detection of pure MGT without adequately addressing mixed scenarios including AI-revised Human-Written Text (HWT) or human-revised MGT. To confront this challenge, we introduce mixcase, a novel concept representing a hybrid text form involving both machine-generated and human-generated content. We collected mixcase instances generated from multiple daily text-editing scenarios and composed MixSet, the first dataset dedicated to studying these mixed modification scenarios. We conduct experiments to evaluate the efficacy of popular MGT detectors, assessing their effectiveness, robustness, and generalization performance. Our findings reveal that exist
    
[^249]: 语言模型是否能够嘲笑YouTube短视频？

    Can Language Models Laugh at YouTube Short-form Videos?. (arXiv:2310.14159v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.14159](http://arxiv.org/abs/2310.14159)

    本研究在用户生成数据集中筛选并注释了10K个YouTube上的有趣多模态视频，借助GPT-3.5验证了语言和视觉元素对幽默的贡献。此外，还开发了一种零-shot视频到文本提示方法，用于大型语言模型对视频幽默的理解。这个研究填补了现有数据集中对多领域多模态幽默的不足。

    

    随着社交网络上短视频的流行，要求AI模型能够更好地理解这些视频以与人类进行更好的交流。然而，之前的视频幽默数据集主要针对特定领域，如演讲或情景喜剧，并且大多关注语言线索。我们创建了一个包含来自YouTube的10K个多模态有趣视频的用户生成数据集，称为ExFunTube。使用基于GPT-3.5的视频过滤流程，我们验证了语言和视觉元素对幽默的贡献。在过滤后，我们为每个视频的有趣时刻加上了时间戳和文本解释。我们的ExFunTube在现有数据集中独特之处在于，我们的视频涵盖了各种类型幽默的广泛领域，需要对内容进行多模态理解。此外，我们开发了一种零-shot视频到文本提示，以最大化大型语言模型（LLMs）对视频幽默的理解。使用自动评分、原理质量实验以及人类评价方法进行三种不同的评估。

    As short-form funny videos on social networks are gaining popularity, it becomes demanding for AI models to understand them for better communication with humans. Unfortunately, previous video humor datasets target specific domains, such as speeches or sitcoms, and mostly focus on verbal cues. We curate a user-generated dataset of 10K multimodal funny videos from YouTube, called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both verbal and visual elements contributing to humor. After filtering, we annotate each video with timestamps and text explanations for funny moments. Our ExFunTube is unique over existing datasets in that our videos cover a wide range of domains with various types of humor that necessitate a multimodal understanding of the content. Also, we develop a zero-shot video-to-text prompting to maximize video humor understanding of large language models (LLMs). With three different evaluation methods using automatic scores, rationale quality experimen
    
[^250]: 教导语言模型通过互动演示自我提升

    Teaching Language Models to Self-Improve through Interactive Demonstrations. (arXiv:2310.13522v1 [cs.CL])

    [http://arxiv.org/abs/2310.13522](http://arxiv.org/abs/2310.13522)

    这项研究通过互动演示的方式，教导较小的语言模型具备自我提升能力，减小了最先进模型与成本效益更高模型之间的性能差距。

    

    大型语言模型（LLM）通过提示其分析和修订自己的输出来实现自我提升的能力，近年来在研究中引起了显著关注。然而，这种能力在较小的模型中被证明是缺失且难以学习的，从而扩大了最先进的LLM与成本效益更高且速度更快的模型之间的性能差距。为了减小这一差距，我们引入了TriPosT，一种训练算法，赋予较小的模型这种自我提升的能力，并且展示了我们的方法可以将LLaMA-7b在数学和推理任务上的性能提升高达7.13%。与以往的工作相比，我们通过使用较小的模型与LLM进行互动以收集反馈和改进自身生成的结果来实现这一点。然后，我们重播这一经验来训练小型模型。我们在四个数学和推理数据集上的实验表明，从并纠正自己的错误中进行互动学习的经验对于小型模型的提升至关重要。

    The self-improving ability of large language models (LLMs), enabled by prompting them to analyze and revise their own outputs, has garnered significant interest in recent research. However, this ability has been shown to be absent and difficult to learn for smaller models, thus widening the performance gap between state-of-the-art LLMs and more cost-effective and faster ones. To reduce this gap, we introduce TriPosT, a training algorithm that endows smaller models with such self-improvement ability, and show that our approach can improve a LLaMA-7b's performance on math and reasoning tasks by up to 7.13%. In contrast to prior work, we achieve this by using the smaller model to interact with LLMs to collect feedback and improvements on its own generations. We then replay this experience to train the small model. Our experiments on four math and reasoning datasets show that the interactive experience of learning from and correcting its own mistakes is crucial for small models to improve 
    
[^251]: 人类课程指导下的指令调整

    Instruction Tuning with Human Curriculum. (arXiv:2310.09518v1 [cs.CL])

    [http://arxiv.org/abs/2310.09518](http://arxiv.org/abs/2310.09518)

    本文探讨了在大型语言模型中应用结构化认知学习方法进行指令调整的潜在好处，并提出了一个高度结构化的合成数据集，结果表明该方法优于传统的随机化方法，提高了指令调整的性能。

    

    指令调整的主流范式是随机洗牌训练最大多样化指令-响应对。本文探讨了在当代大型语言模型如ChatGPT和GPT-4中应用结构化认知学习方法进行指令调整的潜在好处。与以往传统的随机化指令数据集不同，我们提出了一个高度结构化的合成数据集，模拟了人类教育的渐进性和有组织性。我们通过将数据集与教育框架对齐来策划我们的数据集，为每个样本包括主题和认知严谨程度等元信息。我们的数据集涵盖了从中学到研究生阶段的全面细粒度主题，每个主题都有各种问题，以利用布鲁姆的认知分级法提高概念深度，该分级法用于区分每个概念的不同人类认知水平。结果表明，这种认知学习方法优于传统的随机化方法，提高了指令调整的性能。

    The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cogni
    
[^252]: 对大型语言模型在非分布式逻辑推理任务上的系统评估

    A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v1 [cs.CL])

    [http://arxiv.org/abs/2310.09430](http://arxiv.org/abs/2310.09430)

    通过对大型语言模型在非分布式逻辑推理任务上进行系统评估，我们发现这些模型在处理我们新构建的数据集时都存在困难，尽管它们在其他自然语言处理任务上表现良好。这表明这些模型在逻辑推理方面的泛化和鲁棒性仍需要进一步研究。

    

    大型语言模型（LLMs），如GPT-3.5和GPT-4，已经将人工系统在各种自然语言处理任务上的性能提升到接近人类水平。然而，它们在逻辑推理方面的泛化和鲁棒性仍未得到充分评估。为了探索这种能力，我们提出了三个新的逻辑推理数据集，分别名为"ReClor-plus"、"LogiQA-plus"和"LogiQAv2-plus"，每个数据集都包含三个子集：第一个是选项随机打乱，第二个是将正确选项替换为"没有其他选项是正确的"，第三个是前两个子集的组合。我们在这些数据集上进行了实验，使用了鉴别和生成型的LLMs，并表明这些简单的技巧极大地阻碍了语言模型的性能。尽管在原始的公开可用数据集上表现出优秀的性能，但我们发现所有模型都很难回答我们新构建的数据集。我们展示了通过扰动引入任务变化可以提高模型的性能。

    Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly advanced the performance of artificial systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness to perform logical reasoning remain under-evaluated. To probe this ability, we propose three new logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus", each featuring three subsets: the first with randomly shuffled options, the second with the correct choices replaced by "none of the other options are correct", and a combination of the previous two subsets. We carry out experiments on these datasets with both discriminative and generative LLMs and show that these simple tricks greatly hinder the performance of the language models. Despite their superior performance on the original publicly available datasets, we find that all models struggle to answer our newly constructed datasets. We show that introducing task variations by perturb
    
[^253]: LangNav: 语言作为导航的感知表示

    LangNav: Language as a Perceptual Representation for Navigation. (arXiv:2310.07889v1 [cs.CV])

    [http://arxiv.org/abs/2310.07889](http://arxiv.org/abs/2310.07889)

    该论文探索了将语言作为导航的感知表示，并使用现成的视觉系统将每个时间步骤的视图转化为自然语言描述，通过微调预训练的语言模型选择最佳的行动来满足导航指令。有两种用例的实验对这种基于语言的导航方法进行了探索：使用大型语言模型生成合成轨迹以微调较小的语言模型，以及模拟到实际的转换。

    

    我们探索将语言作为视觉与语言导航的感知表示。我们的方法使用现成的视觉系统（用于图像字幕和物体检测）将每个时间步骤中代理人的自我中心全景视图转化为自然语言描述。然后，我们微调预训练的语言模型，根据当前视图和轨迹历史选择最佳的行动来满足导航指令。与标准设置相比，标准设置将预训练的语言模型适应于与预训练的视觉模型连续视觉特征直接配合使用。我们的方法使用（离散的）语言作为感知表示。我们在R2R视觉与语言导航基准测试中探索了两个用例：使用大型语言模型（GPT-4）生成合成轨迹，以便微调较小的语言模型；以及模拟到实际的转换。

    We explore the use of language as a perceptual representation for vision-and-language navigation. Our approach uses off-the-shelf vision systems (for image captioning and object detection) to convert an agent's egocentric panoramic view at each time step into natural language descriptions. We then finetune a pretrained language model to select an action, based on the current view and the trajectory history, that would best fulfill the navigation instructions. In contrast to the standard setup which adapts a pretrained language model to work directly with continuous visual features from pretrained vision models, our approach instead uses (discrete) language as the perceptual representation. We explore two use cases of our language-based navigation (LangNav) approach on the R2R vision-and-language navigation benchmark: generating synthetic trajectories from a prompted large language model (GPT-4) with which to finetune a smaller language model; and sim-to-real transfer where we transfer 
    
[^254]: 红队游戏：红队语言模型的博弈论框架

    Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models. (arXiv:2310.00322v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00322](http://arxiv.org/abs/2310.00322)

    本文提出了红队游戏（RTG）框架，利用博弈论分析了红队语言模型（RLM）与蓝队语言模型（BLM）之间的多轮攻防互动。同时引入了游戏化红队求解器（GRTS）来提供自动化的红队技术。

    

    可部署的大型语言模型（LLM）必须符合有益和无害性的标准，从而实现LLM输出与人类价值的一致性。红队技术是实现这一标准的关键途径。现有的研究仅依赖于手动红队设计和启发式对抗提示进行漏洞检测和优化。这些方法缺乏严格的数学形式化，限制了在可量化度量和收敛保证下对LLM进行多样攻击策略的探索和优化。在本文中，我们提出了红队游戏（RTG），这是一个通用的无需手动标注的博弈论框架。RTG旨在分析红队语言模型（RLM）与蓝队语言模型（BLM）之间的多轮攻防互动。在RTG中，我们提出了具有语义空间多样性度量的游戏化红队求解器（GRTS）。GRTS是一种自动化的红队技术，用于解决红队游戏问题。

    Deployable Large Language Models (LLMs) must conform to the criterion of helpfulness and harmlessness, thereby achieving consistency between LLMs outputs and human values. Red-teaming techniques constitute a critical way towards this criterion. Existing work rely solely on manual red team designs and heuristic adversarial prompts for vulnerability detection and optimization. These approaches lack rigorous mathematical formulation, thus limiting the exploration of diverse attack strategy within quantifiable measure and optimization of LLMs under convergence guarantees. In this paper, we present Red-teaming Game (RTG), a general game-theoretic framework without manual annotation. RTG is designed for analyzing the multi-turn attack and defense interactions between Red-team language Models (RLMs) and Blue-team Language Model (BLM). Within the RTG, we propose Gamified Red-teaming Solver (GRTS) with diversity measure of the semantic space. GRTS is an automated red teaming technique to solve 
    
[^255]: 使用事实知识提升上下文学习的效果

    Boosting In-Context Learning with Factual Knowledge. (arXiv:2309.14771v1 [cs.CL])

    [http://arxiv.org/abs/2309.14771](http://arxiv.org/abs/2309.14771)

    本文研究了使用事实知识提升上下文学习的效果，并提出了一个新的知识上下文调优框架来改善学习性能。

    

    在大型语言模型上下文学习（ICL）旨在通过依赖于少量的训练示例解决以前未见过的任务，从而消除参数更新的需求，并实现有竞争力的性能。本文展示了事实知识在ICL的性能中的重要性，包括在LLM中学到的固有知识，从所选的上下文示例中得出的事实知识，以及LLM在输出生成中的知识偏差。为了发挥LLM在少样本学习场景中的能力，我们引入了一种新的知识上下文调优（KICT）框架来进一步提高ICL的性能：1）在持续自监督预训练期间向LLM注入事实知识，2）谨慎选择具有高知识相关性的示例，3）根据先前的知识对预测结果进行校准。我们在自回归LLM（如GPT风格模型）上评估了所提出的方法。

    In-Context Learning (ICL) over Large language models (LLMs) aims at solving previously unseen tasks by conditioning on a few training examples, eliminating the need for parameter updates and achieving competitive performance. In this paper, we demonstrate that factual knowledge is imperative for the performance of ICL in three core facets, i.e., the inherent knowledge learned in LLMs, the factual knowledge derived from the selected in-context examples, and the knowledge biases in LLMs for output generation. To unleash the power of LLMs in few-shot learning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT) framework to further improve the performance of ICL: 1) injecting factual knowledge to LLMs during continual self-supervised pre-training, 2) judiciously selecting the examples with high knowledge relevance, and 3) calibrating the prediction results based on prior knowledge. We evaluate the proposed approaches on auto-regressive LLMs (e.g., GPT-style models) over 
    
[^256]: 使用FP8格式的高效后训练量化方法

    Efficient Post-training Quantization with FP8 Formats. (arXiv:2309.14592v1 [cs.LG])

    [http://arxiv.org/abs/2309.14592](http://arxiv.org/abs/2309.14592)

    本研究研究了FP8格式在后训练量化中的优势，并开发了一个通用的量化工作流程。实验结果表明，FP8相对于INT8具有更好的工作负载覆盖率和模型准确性。

    

    最近深度学习方法的进展，如LLMs和扩散模型，提出了对改进的量化方法的需求，以满足这些现代架构的计算需求，同时保持准确性。为了实现这一目标，我们研究了FP8数据格式在75个不同网络架构上进行后训练量化的优势，这些网络架构涵盖了多种任务，包括机器翻译、语言建模、文本生成、图像分类、生成和分割。我们研究了三种不同的FP8表示（E5M2、E4M3和E3M4），以研究在动态范围和精度之间不同权衡程度对模型准确性的影响。基于我们的广泛研究，我们开发了一个量化工作流程，可以概括适用于不同的网络架构。我们的实证结果表明，FP8格式在多个方面优于INT8，包括工作负载覆盖率（92.64％对65.87％）、模型准确性和适用于更广泛的操作范围。

    Recent advances in deep learning methods such as LLMs and Diffusion models have created a need for improved quantization methods that can meet the computational demands of these modern architectures while maintaining accuracy. Towards this goal, we study the advantages of FP8 data formats for post-training quantization across 75 unique network architectures covering a wide range of tasks, including machine translation, language modeling, text generation, image classification, generation, and segmentation. We examine three different FP8 representations (E5M2, E4M3, and E3M4) to study the effects of varying degrees of trade-off between dynamic range and precision on model accuracy. Based on our extensive study, we developed a quantization workflow that generalizes across different network architectures. Our empirical results show that FP8 formats outperform INT8 in multiple aspects, including workload coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader range of ope
    
[^257]: 多语言LLMs是否具有文化多样性的推理能力？对多元文化谚语和俗语的调查研究

    Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings. (arXiv:2309.08591v1 [cs.CL])

    [http://arxiv.org/abs/2309.08591](http://arxiv.org/abs/2309.08591)

    本文研究了多语言LLMs在对话环境中运用谚语和俗语进行推理的能力，发现mLLMs在理解比喻性谚语、选择正确答案和推理其他语言的谚语时存在困难。

    

    大型语言模型（LLMs）在问答和推理任务方面非常擅长，但在情境背景下进行推理时，人类的期望因相关文化共同点而异。由于人类语言与多种文化相关联，LLMs也应该是具有文化多样性的推理者。在本文中，我们研究了一系列最先进的多语言LLMs（mLLMs）在对话环境中运用谚语和俗语进行推理的能力。我们的实验证明：（1）mLLMs只“知道”有限的谚语，并且仅仅记住谚语并不能在对话环境中理解它们；（2）mLLMs在推理比喻性的谚语和俗语时遇到困难，当被要求选择错误答案时，而不是选择正确答案；（3）在推理来自其他语言的谚语和俗语时，mLLMs存在“文化差距”。我们构建并发布了我们的评估数据集MAPS（多元文化谚语和俗语）。

    Large language models (LLMs) are highly adept at question answering and reasoning tasks, but when reasoning in situational context, human expectations vary depending on the relevant cultural common ground. As human languages are associated with diverse cultures, LLMs should also be culturally-diverse reasoners. In this paper, we study the ability of a wide range of state-of-the-art multilingual LLMs (mLLMs) to reason with proverbs and sayings in a conversational context. Our experiments reveal that: (1) mLLMs 'knows' limited proverbs and memorizing proverbs does not mean understanding them within a conversational context; (2) mLLMs struggle to reason with figurative proverbs and sayings, and when asked to select the wrong answer (instead of asking it to select the correct answer); and (3) there is a "culture gap" in mLLMs when reasoning about proverbs and sayings translated from other languages. We construct and release our evaluation dataset MAPS (MulticultrAl Proverbs and Sayings) fo
    
[^258]: 深入毒性兔子洞：通过PaLM 2的守护栏调查

    Down the Toxicity Rabbit Hole: Investigating PaLM 2 Guardrails. (arXiv:2309.06415v1 [cs.CL])

    [http://arxiv.org/abs/2309.06415](http://arxiv.org/abs/2309.06415)

    这项研究通过一个新颖的毒性兔子洞框架对PaLM 2的安全反馈进行了稳健性审计，揭示了PaLM 2生成的高度令人不安的毒性内容未被安全守护栏评估为高度不安全。

    

    本文通过引入一种名为“毒性兔子洞”的新型框架，对PaLM 2的安全反馈进行了强化稳健性审计。从一个刻板印象开始，该框架指示PaLM 2生成比刻板印象更具有毒性的内容。每一次迭代，它都要求PaLM 2生成比上一次迭代更具有毒性的内容，直到PaLM 2的安全守护栏发出安全违规警报。我们的实验揭示了极其令人不安的反犹太主义、伊斯兰恐惧症、种族主义、恐同和厌女情绪（仅列举几种）的生成内容，并且这些内容在PaLM 2的安全守护栏评估中并未被视为高度不安全。

    This paper conducts a robustness audit of the safety feedback of PaLM 2 through a novel toxicity rabbit hole framework introduced here. Starting with a stereotype, the framework instructs PaLM 2 to generate more toxic content than the stereotype. Every subsequent iteration it continues instructing PaLM 2 to generate more toxic content than the previous iteration until PaLM 2 safety guardrails throw a safety violation. Our experiments uncover highly disturbing antisemitic, Islamophobic, racist, homophobic, and misogynistic (to list a few) generated content that PaLM 2 safety guardrails do not evaluate as highly unsafe.
    
[^259]: SeaEval多语言基础模型：从跨语言对齐到文化推理

    SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning. (arXiv:2309.04766v1 [cs.CL])

    [http://arxiv.org/abs/2309.04766](http://arxiv.org/abs/2309.04766)

    SeaEval是一个评估多语言基础模型的基准测试，研究了模型在自然语言理解、推理以及对文化实践、细微差别和价值观的理解能力上的表现。重要发现包括模型在给出改写指令时行为各异，受到暴露偏差的影响，对于语义等价的多语言查询的回答不一致，以及模型在情感相关问题上的一致性不同。

    

    我们提出了一种用于多语言基础模型的SeaEval基准测试。除了表征这些模型如何理解和推理自然语言外，我们还研究了它们对文化实践、细微差别和价值观的理解能力。除了标准的准确度指标，我们还调查了基础模型在语义和多语言性维度上的脆弱性。我们的分析涵盖了开源和闭源模型，从而得到了在经典的自然语言处理任务、推理和文化理解方面的实证结果。重要发现包括：（1）大多数模型在给出改写指令时的行为各异；（2）许多模型仍然受到暴露偏差的影响（如位置偏差、大多数标签偏差）；（3）对于根源于事实、科学和常识知识的问题，预期在语义上等价的多语言查询应该得到一致的回答。然而，大多数模型在这些查询上表现出令人意外的不一致性；（4）多语言情况下，模型对于情感相关的问题表现出不同程度的一致性。

    We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we investigate the brittleness of foundation models in the dimensions of semantics and multilinguality. Our analyses span both open-sourced and closed models, leading to empirical results across classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate (1) Most models exhibit varied behavior when given paraphrased instructions. (2) Many models still suffer from exposure bias (e.g., positional bias, majority label bias). (3) For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, most models surprisingly demonstrate inconsistent performance on these queries. (4) Multilingu
    
[^260]: MoEController: 使用混合专家控制器的基于指令的任意图像操作

    MoEController: Instruction-based Arbitrary Image Manipulation with Mixture-of-Expert Controllers. (arXiv:2309.04372v1 [cs.CV])

    [http://arxiv.org/abs/2309.04372](http://arxiv.org/abs/2309.04372)

    本论文提出了一种基于混合专家控制器(MoEController)的指令驱动任意图像操作方法，通过对不同类型的人类指令进行适配，使得模型能够处理各种开放域图像操作任务。使用大型语言模型和条件图像合成模型生成训练数据集，并采用MOE技术和任务特定的适应性训练对模型进行训练。

    

    最近，基于扩散模型的文本引导图像生成取得了惊人的进展，在开放域图像操作任务中产生了令人着迷的结果。 然而，由于图像操作任务的复杂性和多样性，目前很少有模型具有完全的零样本能力，既可以进行全局操作，又可以进行局部图像编辑。 在这项工作中，我们提出了一种使用混合专家控制器（MOE）的方法，将扩散模型的文本引导能力与不同类型的人类指令相对齐，使我们的模型能够使用自然语言指令处理各种开放域图像操作任务。 首先，我们使用大型语言模型（ChatGPT）和条件图像合成模型（ControlNet）生成大量全局图像转换数据集，以及基于指令的局部图像编辑数据集。 然后，使用MOE技术和任务特定的适应性训练对大规模数据集进行训练，我们的条件扩散模型可以对图像进行全局和局部编辑。

    Diffusion-model-based text-guided image generation has recently made astounding progress, producing fascinating results in open-domain image manipulation tasks. Few models, however, currently have complete zero-shot capabilities for both global and local image editing due to the complexity and diversity of image manipulation tasks. In this work, we propose a method with a mixture-of-expert (MOE) controllers to align the text-guided capacity of diffusion models with different kinds of human instructions, enabling our model to handle various open-domain image manipulation tasks with natural language instructions. First, we use large language models (ChatGPT) and conditional image synthesis models (ControlNet) to generate a large number of global image transfer dataset in addition to the instruction-based local image editing dataset. Then, using an MOE technique and task-specific adaptation training on a large-scale dataset, our conditional diffusion model can edit images globally and loc
    
[^261]: 评估大型语言模型在发现基因集合功能方面的应用

    Evaluation of large language models for discovery of gene set function. (arXiv:2309.04019v1 [q-bio.GN])

    [http://arxiv.org/abs/2309.04019](http://arxiv.org/abs/2309.04019)

    本研究评估了OpenAI的GPT-4大型语言模型在发现基因集合功能方面的能力，并发现它能根据嵌入的生物医学知识生成与Gene Ontology中具名基因集合非常相似的名称，同时在基因组学数据中发现的基因集合中，GPT-4的命名更具信息量，得到了人工审核的基本验证。

    

    基因集合分析是功能基因组学的重要方法，但它依赖于手动创建的基因功能数据库，这些数据库的不完整和不具备生物学上下文的特点。本文评估了OpenAI的GPT-4大型语言模型的能力，从其嵌入的生物医学知识中发展出有关常见基因功能的假设。我们创建了一个GPT-4流水线，用于用总结其共识功能的名称标记基因集合，并通过分析文本和引文进行证实。在与Gene Ontology中的具名基因集合进行基准测试时，GPT-4在50%的情况下生成了非常相似的名称，而在大多数其他情况下，则恢复了更一般概念的名称。在基因组学数据中发现的基因集合中，与基因集合富集相比，GPT-4的命名更具信息量，其支持性陈述和引文在人工审核中得到了基本验证。快速综合常见基因功能的能力使得大型语言模型成为有价值的功能基因组学助手。

    Gene set analysis is a mainstay of functional genomics, but it relies on manually curated databases of gene functions that are incomplete and unaware of biological context. Here we evaluate the ability of OpenAI's GPT-4, a Large Language Model (LLM), to develop hypotheses about common gene functions from its embedded biomedical knowledge. We created a GPT-4 pipeline to label gene sets with names that summarize their consensus functions, substantiated by analysis text and citations. Benchmarking against named gene sets in the Gene Ontology, GPT-4 generated very similar names in 50% of cases, while in most remaining cases it recovered the name of a more general concept. In gene sets discovered in 'omics data, GPT-4 names were more informative than gene set enrichment, with supporting statements and citations that largely verified in human review. The ability to rapidly synthesize common gene functions positions LLMs as valuable functional genomics assistants.
    
[^262]: 有效的语言模型基准测试

    Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])

    [http://arxiv.org/abs/2308.11696](http://arxiv.org/abs/2308.11696)

    本研究提出了一种名为"Efficient Benchmarking"的问题，旨在智能地减少语言模型评估的计算成本而不降低可靠性，并使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估决策的可靠性。通过HELM基准测试的案例研究，发现只需删除一个低排名模型即可改变领先者，并仅需少量示例即可得到正确的基准测试排名。

    

    语言模型的多功能性增加导致了一类全面评估广泛能力的基准测试的出现。这些基准测试与大规模计算成本相关，每个模型需要数千个GPU小时。然而，关于评估效率方面的问题在文献中讨论较少。本文提出了一种名为"Efficient Benchmarking"的问题，即在不损害可靠性的情况下智能地减少语言模型评估的计算成本。通过使用HELM基准测试作为示例，我们研究了不同基准测试设计选择如何影响计算-可靠性权衡。我们提出使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估这些决策的可靠性。例如，我们发现仅通过从基准测试中删除一个低排名模型，当前在HELM上的领先者可能会改变，并且观察到只需一小部分示例即可获得正确的基准测试排名。

    The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
    
[^263]: Beam Retrieval: 通用的端到端检索用于多阶段问答

    Beam Retrieval: General End-to-End Retrieval for Multi-Hop Question Answering. (arXiv:2308.08973v1 [cs.CL])

    [http://arxiv.org/abs/2308.08973](http://arxiv.org/abs/2308.08973)

    Beam Retrieval是一个通用的端到端检索框架，用于多阶段问答。它通过保持多个相关文段的假设和通过最小化组合损失来优化编码器和分类头，实现了近50%的改进效果。

    

    多阶段问答涉及查找多个相关文段和逐步推理以回答复杂问题。虽然先前的方法已经开发了用于选择相关文段的检索模块，但是它们在超过两个阶段的场景中面临挑战，原因是一步方法的性能有限，两步方法在早期阶段选择无关文段时失败。在本研究中，我们介绍了Beam Retrieval，这是一个通用的多阶段问答的端到端检索框架。该方法在每个阶段保持多个相关文段的假设，扩展了搜索空间，降低了错过相关文段的风险。此外，Beam Retrieval通过最小化所有阶段的组合损失来联合优化编码器和两个分类头。为了建立一个完整的问答系统，我们引入了一个有监督的阅读器或零-shot GPT-3.5。实验结果表明，与基准方法相比，Beam Retrieval取得了近50%的改进效果。

    Multi-hop QA involves finding multiple relevant passages and step-by-step reasoning to answer complex questions. While previous approaches have developed retrieval modules for selecting relevant passages, they face challenges in scenarios beyond two hops, owing to the limited performance of one-step methods and the failure of two-step methods when selecting irrelevant passages in earlier stages. In this work, we introduce Beam Retrieval, a general end-to-end retrieval framework for multi-hop QA. This approach maintains multiple partial hypotheses of relevant passages at each step, expanding the search space and reducing the risk of missing relevant passages. Moreover, Beam Retrieval jointly optimizes an encoder and two classification heads by minimizing the combined loss across all hops. To establish a complete QA system, we incorporate a supervised reader or a zero-shot GPT-3.5. Experimental results demonstrate that Beam Retrieval achieves a nearly 50% improvement compared with baseli
    
[^264]: DiagGPT:一种基于LLM的任务导向对话的聊天机器人

    DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue. (arXiv:2308.08043v1 [cs.CL])

    [http://arxiv.org/abs/2308.08043](http://arxiv.org/abs/2308.08043)

    DiagGPT将大型语言模型(LLMs)扩展到任务导向的对话场景，提供了在复杂诊断场景中主动提问和引导用户完成任务的能力。

    

    大型语言模型(LLMs)如ChatGPT正变得越来越复杂，展示出与人类相似的能力。这些AI模型在日常生活中辅助人类完成各种任务方面发挥着重要作用。AI作为聊天代理人的重要应用是回答人类在各个领域的问题。目前的LLMs在回答一般问题方面已经显示出熟练的能力。然而，在复杂的诊断场景(如法律或医疗咨询)中，基本的问答对话往往表现不佳。这些场景通常需要任务导向对话(TOD)，其中AI聊天代理需要主动提问并引导用户完成特定任务。以前的微调模型在TOD方面表现不佳，而当前的LLMs并未固有这种能力。在本文中，我们介绍了一种名为DiagGPT (Diagnosis GPT)的创新方法，它将LLMs推广到TOD场景中。

    Large Language Models (LLMs), such as ChatGPT, are becoming increasingly sophisticated, demonstrating capabilities that closely resemble those of humans. These AI models are playing an essential role in assisting humans with a wide array of tasks in daily life. A significant application of AI is its use as a chat agent, responding to human inquiries across various domains. Current LLMs have shown proficiency in answering general questions. However, basic question-answering dialogue often falls short in complex diagnostic scenarios, such as legal or medical consultations. These scenarios typically necessitate Task-Oriented Dialogue (TOD), wherein an AI chat agent needs to proactively pose questions and guide users towards specific task completion. Previous fine-tuning models have underperformed in TOD, and current LLMs do not inherently possess this capability. In this paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative method that extends LLMs to TOD scenarios. Our e
    
[^265]: RAVEN：上下文学习与检索增强的编码器-解码器语言模型

    RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models. (arXiv:2308.07922v1 [cs.CL])

    [http://arxiv.org/abs/2308.07922](http://arxiv.org/abs/2308.07922)

    RAVEN是一种结合了检索增强的蒙特卡洛语言建模和前缀语言建模的模型，通过引入上下文融合学习，它能够在上下文学习方面取得比ATLAS更好的性能。

    

    本文研究了检索增强的编码器-解码器语言模型在上下文学习方面的能力。我们首先对现有的ATLAS模型进行全面分析，发现其在上下文学习方面存在限制，主要原因是预训练和测试之间存在不匹配，以及上下文长度受限。为了解决这些问题，我们提出了RAVEN模型，该模型结合了检索增强的蒙特卡洛语言建模和前缀语言建模。我们还引入了上下文融合学习，通过使模型能够利用更多上下文示例而无需额外训练或模型修改来提高少样本性能。通过大量实验，我们证明了RAVEN在某些场景下明显优于ATLAS，并达到了与最先进的语言模型相当的结果，尽管参数数量显著较少。我们的工作强调了检索增强的编码器-解码器语言模型的潜力。

    In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder lang
    
[^266]: XSTest: 用于识别大型语言模型中夸大安全行为的测试套件

    XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models. (arXiv:2308.01263v1 [cs.CL])

    [http://arxiv.org/abs/2308.01263](http://arxiv.org/abs/2308.01263)

    本文介绍了一个名为XSTest的测试套件，旨在识别大型语言模型中夸大的安全行为。该套件由200个安全提示组成，涵盖十种提示类型，旨在引出模型的系统性问题。

    

    没有适当的保护措施，大型语言模型很容易遵循恶意指令并生成有害内容。这激发了安全工作，如红队测试和大规模反馈学习，旨在使模型既有用又无害。然而，这两个目标之间存在一种紧张关系，因为无害性要求模型拒绝遵从不安全的提示，从而无法提供帮助。最近的一些证据表明，一些模型可能在平衡上存在问题，以至于即使使用类似不安全提示的语言或提及敏感主题的明显安全提示也会被拒绝。本文介绍了一个名为XSTest的新测试套件，以系统化和结构化的方式识别这种夸张的安全行为。目前，XSTest包括200个安全提示，涵盖十种提示类型，良好校准的模型不应该拒绝遵循这些提示。我们描述了XSTest的创建和组成，并使用测试套件突显系统性的问题。

    Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse complying with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a structured and systematic way. In its current form, XSTest comprises 200 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with. We describe XSTest's creation and composition, and use the test suite to highlight systematic f
    
[^267]: SAS视频问答：自适应采样用于高效视频问答

    SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v1 [cs.CV])

    [http://arxiv.org/abs/2307.04192](http://arxiv.org/abs/2307.04192)

    SAS视频问答通过自适应采样策略解决了视频问答中的问题，提高了效率和准确性

    

    视频问答是视频理解领域的一项基础任务。尽管当前的视觉-语言模型(VLMs)配备了视频变换器(Video Transformers)，实现了时间建模并取得了优秀的结果，但代价是巨大的计算能力，因此在实时应用场景中过于昂贵。一种经济的解决方法是只对视频的一小部分帧进行采样，来代表视频的主要内容，并在这些采样帧上调整图像-文本模型。最近的视频理解模型通常随机采样一组帧或片段，而不考虑它们的内部关联性和与问题的相关性。我们认为这种无目标的采样可能会遗漏可以推导出正确答案的关键帧，在采样稀疏程度增加时，情况会变得更糟，而随着视频长度的增加，采样稀疏程度也会增加。为了解决这个问题，我们提出了两种帧采样策略，分别是

    Video question--answering is a fundamental task in the field of video understanding. Although current vision--language models (VLMs) equipped with Video Transformers have enabled temporal modeling and yielded superior results, they are at the cost of huge computational power and thus too expensive to deploy in real-time application scenarios. An economical workaround only samples a small portion of frames to represent the main content of that video and tune an image--text model on these sampled frames. Recent video understanding models usually randomly sample a set of frames or clips, regardless of internal correlations between their visual contents, nor their relevance to the problem. We argue that such kinds of aimless sampling may omit the key frames from which the correct answer can be deduced, and the situation gets worse when the sampling sparsity increases, which always happens as the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namel
    
[^268]: 多模态大语言模型综述

    A Survey on Multimodal Large Language Models. (arXiv:2306.13549v1 [cs.CV])

    [http://arxiv.org/abs/2306.13549](http://arxiv.org/abs/2306.13549)

    本文追踪和总结了多模态大语言模型（MLLM）的最新进展，包括多模态指令调整、多模态上下文学习、多模态思维链和LLM辅助视觉推理等应用，指出了现有挑战和有前途的研究方向。

    

    多模态大语言模型（MLLM）是一种新兴的研究热点，使用强大的大语言模型作为大脑执行多模态任务。MLLM 的惊人能力，如基于图像编写故事和无OCR数学推理等，在传统方法中很少见，表明了通向人工智能的潜在路径。本文旨在追踪和总结 MLLM 的最新进展。首先，我们介绍了 MLLM 的构成，概述了相关概念。然后，讨论了关键技术和应用，包括多模态指令调整（M-IT）、多模态上下文学习（M-ICL）、多模态思维链（M-CoT）和LLM辅助视觉推理（LAVR）。最后，我们讨论了现有的挑战，并指出了有前途的研究方向。鉴于 MLLM 时代才刚刚开始，我们会不断更新这个综述，并希望能激发更多的研究。

    Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated
    
[^269]: SQL-PaLM：针对Text-to-SQL的改进大语言模型适应性

    SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL. (arXiv:2306.00739v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.00739](http://arxiv.org/abs/2306.00739)

    本文提出了一种基于大语言模型的Text-to-SQL模型SQL-PaLM，使用了面向Text-to-SQL的基于执行的自一致提示方法，在Spider上实现了77.3%的测试套件准确度，并显着超越以前的最新技术的方法。

    

    大语言模型（LLMs）的一个令人印象深刻的新兴功能是生成代码，包括用于数据库的结构化查询语言（SQL）。对于将自然语言文本转换为SQL查询的任务，即Text-to-SQL，LLMs的适应性至关重要，具体取决于使用的适应性数据量。本文提出了一种基于LLM的Text-to-SQL模型SQL-PaLM，利用了PaLM-2，推动了两种设置的最新进展。Few-shot SQL-PaLM基于面向Text-to-SQL的基于执行的自一致提示方法，可在Spider上实现77.3%的测试套件准确度，据我们所知，这是第一个通过显着较大的微调超越以前的最新技术的方法。此外，我们证明经过精细调整的SQL-PALM可进一步提高1%的性能。为了将SQL-PaLM应用于实际场景，我们进一步评估了其对其他挑战的稳健性。

    One impressive emergent capability of large language models (LLMs) is generation of code, including Structured Query Language (SQL) for databases. For the task of converting natural language text to SQL queries, Text-to-SQL, adaptation of LLMs is of paramount importance, both in in-context learning and fine-tuning settings, depending on the amount of adaptation data used. In this paper, we propose an LLM-based Text-to-SQL model SQL-PaLM, leveraging on PaLM-2, that pushes the state-of-the-art in both settings. Few-shot SQL-PaLM is based on an execution-based self-consistency prompting approach designed for Text-to-SQL, and achieves 77.3% in test-suite accuracy on Spider, which to our best knowledge is the first to outperform previous state-of-the-art with fine-tuning by a significant margin, 4%. Furthermore, we demonstrate that the fine-tuned SQL-PALM outperforms it further by another 1%. Towards applying SQL-PaLM to real-world scenarios we further evaluate its robustness on other chall
    
[^270]: Self-Checker：用于基于大语言模型事实检查的即插即用模块

    Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models. (arXiv:2305.14623v1 [cs.CL])

    [http://arxiv.org/abs/2305.14623](http://arxiv.org/abs/2305.14623)

    本文介绍了Self-Checker框架，它由即插即用的模块组成，能够在几乎零次启动的情况下利用大型语言模型进行快速高效的事实检查，这对于在资源有限的环境下构建事实检查系统非常有用。

    

    事实检查是NLP中的一个重要任务，通常用于验证主张的事实准确性。以前的工作主要集中在对特定数据集进行预先训练的语言模型微调上，这可能需要大量的计算资源和时间。随着像ChatGPT和GPT-3这样的大型语言模型的快速发展，研究人员现在正在探索它们的上下文学习能力以执行各种任务。本文介绍了Self-Checker，这是一个框架，包括一组即插即用的模块，通过在几乎零次启动的情况下仅提示LLMs，从而便于对事实进行检查。该框架提供了在资源有限的环境中构建事实检查系统的快速高效方法。实证结果表明Self-Checker在利用LLMs进行事实检查方面具有潜力。然而，与SOTA微调模型相比仍有很大的改进空间，这表明需要进一步的研究和开发。

    Fact-checking is an essential task in NLP that is commonly utilized for validating the factual accuracy of claims. Prior work has mainly focused on fine-tuning pre-trained languages models on specific datasets, which can be computationally intensive and time-consuming. With the rapid development of large language models (LLMs), such as ChatGPT and GPT-3, researchers are now exploring their in-context learning capabilities for a wide range of tasks. In this paper, we aim to assess the capacity of LLMs for fact-checking by introducing Self-Checker, a framework comprising a set of plug-and-play modules that facilitate fact-checking by purely prompting LLMs in an almost zero-shot setting. This framework provides a fast and efficient way to construct fact-checking systems in low-resource environments. Empirical results demonstrate the potential of Self-Checker in utilizing LLMs for fact-checking. However, there is still significant room for improvement compared to SOTA fine-tuned models, wh
    
[^271]: 单语言数据何时有助于多语言翻译：领域和模型规模的作用

    When Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale. (arXiv:2305.14124v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14124](http://arxiv.org/abs/2305.14124)

    本文研究了单语言数据在多语言翻译中的作用，发现单语言数据通常有助于多语言翻译，但模型对领域不匹配的容忍性较差，尤其在较小的模型规模下。回译在数据源相似的情况下是有益的，但在其他情况下可能是有害的，而去噪自编码的效果不如先前报告的好。规模对两种方法都很重要。

    

    多语言机器翻译（MMT）是通过混合平行和单语言数据进行训练，提高低资源语言对翻译的关键。然而，文献对于包含单语言数据的不同方法的表现存在争议。为了解决这个问题，我们研究了去噪自编码（DAE）和回译（BT）在不同数据条件和模型规模下对MMT的影响。与先前的研究不同，我们使用了一个实际数据集，包括100个翻译方向，并考虑了许多单语言和测试数据的领域组合。我们发现，单语言数据通常有助于MMT，但模型对领域不匹配的容忍性出乎意料地较差，尤其在较小的模型规模下。当平行、单语言和测试数据源相似时，回译是有益的，但在其他情况下可能是有害的，而DAE的效果不如先前报告的好。接下来，我们分析了规模（从90M到1.6B参数）的影响，发现它对两种方法都很重要。

    Multilingual machine translation (MMT), trained on a mixture of parallel and monolingual data, is key for improving translation in low-resource language pairs. However, the literature offers conflicting results on the performance of different methods of including monolingual data. To resolve this, we examine how denoising autoencoding (DAE) and backtranslation (BT) impact MMT under different data conditions and model scales. Unlike prior studies, we use a realistic dataset of 100 translation directions and consider many domain combinations of monolingual and test data. We find that monolingual data generally helps MMT, but models are surprisingly brittle to domain mismatches, especially at smaller model scales. BT is beneficial when the parallel, monolingual, and test data sources are similar but can be detrimental otherwise, while DAE is less effective than previously reported. Next, we analyze the impact of scale (from 90M to 1.6B parameters) and find it is important for both methods
    
[^272]: 通过逻辑驱动的数据增强增强大型语言模型的逻辑推理能力

    Enhancing Logical Reasoning of Large Language Models through Logic-Driven Data Augmentation. (arXiv:2305.12599v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12599](http://arxiv.org/abs/2305.12599)

    本论文介绍了一种通过逻辑驱动的数据增强方法来增强大型语言模型的逻辑推理能力。通过将原始文本转换为抽象意义表述图，并对其进行逻辑修改和转换，生成增强数据，从而提升模型性能。该方法适用于各种体系结构的大型语言模型。

    

    将大型语言模型与逻辑推理相结合可以增强它们在问题解决中的能力，使其更加强大和可靠。然而，逻辑推理的复杂性使得从网页上收集可靠的数据来建立全面的训练数据集面临困难，进而影响下游任务的性能。为了解决这个问题，我们提出了一种新颖的逻辑驱动数据增强方法，AMR-LDA。AMR-LDA将原始文本转换成抽象意义表示（AMR）图，这是一种结构化的语义表示，包含了句子的逻辑结构，然后对该图进行操作以生成逻辑修改后的AMR图。修改后的AMR图随后被转换回文本，从而创建增强数据。值得注意的是，我们的方法与体系结构无关，并通过提示增强来增强生成型大型语言模型（如GPT-3.5和GPT-4），并通过微调来增强判别型大型语言模型。

    Combining large language models with logical reasoning enhance their capacity to address problems in a robust and reliable manner. Nevertheless, the intricate nature of logical reasoning poses challenges to gathering reliable data from web for building comprehensive training datasets, subsequently affecting the performance on downstream tasks. To address this, we introduce a novel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the original text into an Abstract Meaning Representation (AMR) graph, a structured semantic representation that encapsulates the logic structure of the sentence, upon which operations are performed to generate logically modified AMR graphs. The modified AMR graphs are subsequently converted back into texts to create augmented data. Notably, our methodology is architecture-agnostic and enhances generative large language models, such as GPT-3.5 and GPT-4, through prompt augmentation, and fine-tuning discriminative large language models through 
    
[^273]: SpecInfer：利用推测推断和令牌树验证加速生成式大语言模型的服务

    SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification. (arXiv:2305.09781v1 [cs.CL])

    [http://arxiv.org/abs/2305.09781](http://arxiv.org/abs/2305.09781)

    SpecInfer是一种LLM服务系统，通过利用推测推断和令牌树验证来加速生成式大语言模型的推断过程，显著减少了为它们提供服务所需的端到端延迟和计算要求，同时确保模型质量。

    

    由于生成式大语言模型（LLMs）需要高计算和内存需求，因此快速和廉价地为它们提供服务是具有挑战性的。本文介绍SpecInfer，一个LLM服务系统，它利用推测推断和令牌树验证加速生成式LLM推断。SpecInfer背后的关键是将各种小型语言模型进行集体提升调整，共同预测LLM的输出； 预测结果组织成一个令牌树，其中每个节点都表示候选令牌序列。通过一种新颖的基于树的并行解码机制，以LMM作为令牌树验证器来验证令牌树所代表的所有候选令牌序列的正确性。SpecInfer使用LLM作为令牌树验证器，而不是增量解码器，从而显著减少了为生成式LLM提供服务所需的端到端延迟和计算要求，同时可确保模型质量。

    The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree verification. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.
    
[^274]: 反应性摄入扰动：对抗文本防御的一种方法

    Reactive Perturbation Defocusing for Textual Adversarial Defense. (arXiv:2305.04067v1 [cs.CL])

    [http://arxiv.org/abs/2305.04067](http://arxiv.org/abs/2305.04067)

    RPD对大型预训练语言模型的漏洞进行了防御，通过识别对抗性示例并注入安全扰动来减少误防御，成功地修复了高达97%的对抗性示例，在自然示例的性能仅降低了约2%。

    

    最近的研究表明，大型预训练语言模型容易受到对抗性攻击。现有的方法试图重构对抗性示例，但这些方法通常在防御对抗性示例方面性能有限，同时也会对自然示例的性能产生负面影响。为了解决这个问题，我们提出了一种称为反应性摄入扰动 (RPD) 的方法。RPD 使用对抗检测器识别对抗性示例，并减少在自然示例上的误防御。RPD 不是重构对手，而是在对抗性示例中注入安全扰动，以分散目标模型对恶意扰动的注意力。我们在三个数据集、两个目标模型和各种对抗性攻击上的实验表明，我们提出的框架成功地修复了大约 97% 的正确识别的对抗性示例，并且自然示例的性能仅降低了约 2%。

    Recent studies have shown that large pre-trained language models are vulnerable to adversarial attacks. Existing methods attempt to reconstruct the adversarial examples. However, these methods usually have limited performance in defense against adversarial examples, while also negatively impacting the performance on natural examples. To overcome this problem, we propose a method called Reactive Perturbation Defocusing (RPD). RPD uses an adversarial detector to identify adversarial examples and reduce false defenses on natural examples. Instead of reconstructing the adversaries, RPD injects safe perturbations into adversarial examples to distract the objective models from the malicious perturbations. Our experiments on three datasets, two objective models, and various adversarial attacks show that our proposed framework successfully repairs up to approximately 97% of correctly identified adversarial examples with only about a 2% performance decrease on natural examples. We also provide 
    
[^275]: NAIST-SIC-Aligned：自动对齐的英日同声传译语料库

    NAIST-SIC-Aligned: Automatically-Aligned English-Japanese Simultaneous Interpretation Corpus. (arXiv:2304.11766v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.11766](http://arxiv.org/abs/2304.11766)

    本论文提出了NAIST-SIC-Aligned，这是一个自动对齐的英日平行同声传译数据集。该论文使用了一个两阶段的对齐方法，经过定量或定性验证的每个步骤，以确保语料库的质量。这是第一个开源的大规模平行SI数据集。

    

    如何利用同声传译（SI）数据来影响同声机器翻译（SiMT）仍然是一个问题。由于缺乏大规模的训练语料库，研究受到了限制。本文介绍了NAIST-SIC-Aligned，这是一个自动对齐的英日平行同声传译数据集。通过一个非对齐语料库NAIST-SIC开始，我们提出了一个两阶段对齐方法，使语料库具有平行性，从而适合模型训练。第一阶段是粗略对齐，在此步骤中，我们在源语言和目标语言之间执行一个多对多的映射；第二阶段是细粒度对齐，在此步骤中，我们执行语句内部和语句间过滤来提高对齐对的质量。为确保语料库的质量，每个步骤都经过了定量或定性的验证。这是文献中第一个开源的大规模平行SI数据集。我们还手动精选了一个小型测试集用于评估目的。

    It remains a question that how simultaneous interpretation (SI) data affects simultaneous machine translation (SiMT). Research has been limited due to the lack of a large-scale training corpus. In this work, we aim to fill in the gap by introducing NAIST-SIC-Aligned, which is an automatically-aligned parallel English-Japanese SI dataset. Starting with a non-aligned corpus NAIST-SIC, we propose a two-stage alignment approach to make the corpus parallel and thus suitable for model training. The first stage is coarse alignment where we perform a many-to-many mapping between source and target sentences, and the second stage is fine-grained alignment where we perform intra- and inter-sentence filtering to improve the quality of aligned pairs. To ensure the quality of the corpus, each step has been validated either quantitatively or qualitatively. This is the first open-sourced large-scale parallel SI dataset in the literature. We also manually curated a small test set for evaluation purpose
    
[^276]: 低代码LLM：LLM上的可视化编程

    Low-code LLM: Visual Programming over LLMs. (arXiv:2304.08103v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.08103](http://arxiv.org/abs/2304.08103)

    本文介绍了一种新颖的人-LLM交互框架，低代码LLM，该框架可通过六种类型的简单低代码可视化编程交互实现更可控和稳定的响应，具有可控性强、用户友好的交互方式和广泛的应用范围的优点。

    

    有效地利用LLM来完成复杂任务非常具有挑战性，通常需要进行耗时而难以掌控的提示工程处理过程。本文提出了一种新颖的人-LLM交互框架，即低代码LLM。它包括六种类型的简单低代码可视化编程交互，全部支持点击、拖放或文本编辑，以实现更可控和稳定的响应。通过与图形用户界面的视觉交互，用户可以将其想法纳入工作流程，而不必编写琐碎的提示。提出的低代码LLM框架由规划LLM和执行LLM两部分组成，规划LLM为复杂任务设计了一个结构化的规划工作流程，用户可以通过低代码可视化编程操作相应地进行编辑和确认，而执行LLM则按照用户确认的工作流程生成响应。我们强调低代码LLM的三个优点：可控的生成结果、用户友好的人-LLM交互以及广泛的应用。

    Effectively utilizing LLMs for complex tasks is challenging, often involving a time-consuming and uncontrollable prompt engineering process. This paper introduces a novel human-LLM interaction framework, Low-code LLM. It incorporates six types of simple low-code visual programming interactions, all supported by clicking, dragging, or text editing, to achieve more controllable and stable responses. Through visual interaction with a graphical user interface, users can incorporate their ideas into the workflow without writing trivial prompts. The proposed Low-code LLM framework consists of a Planning LLM that designs a structured planning workflow for complex tasks, which can be correspondingly edited and confirmed by users through low-code visual programming operations, and an Executing LLM that generates responses following the user-confirmed workflow. We highlight three advantages of the low-code LLM: controllable generation results, user-friendly human-LLM interaction, and broadly app
    
[^277]: Transformer模型：介绍与目录

    Transformer models: an introduction and catalog. (arXiv:2302.07730v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.07730](http://arxiv.org/abs/2302.07730)

    本论文介绍与分类了Transformer模型系列中最流行的模型，包括基于自监督学习和人类参与训练的模型，并对其中创新性的方面做了介绍。

    

    近年来，Transformer系列的基础模型如雨后春笋般涌现出来，它们中有些具有令人难忘的、有时甚至滑稽有趣但却不具自解释性的名称。本文旨在提供一个相对全面但简单的Transformer模型目录和分类，并介绍Transformer模型中最重要的方面和创新。目录中的模型包括通过自监督学习进行训练（例如BERT或GPT3）的模型，以及进一步通过人类参与训练（例如ChatGPT使用的InstructGPT模型）的模型。

    In the past few years we have seen the meteoric appearance of dozens of foundation models of the Transformer family, all of which have memorable and sometimes funny, but not self-explanatory, names. The goal of this paper is to offer a somewhat comprehensive but simple catalog and classification of the most popular Transformer models. The paper also includes an introduction to the most important aspects and innovations in Transformer models. Our catalog will include models that are trained using self-supervised learning (e.g., BERT or GPT3) as well as those that are further trained using a human-in-the-loop (e.g. the InstructGPT model used by ChatGPT).
    
[^278]: 部分动员：跟踪俄罗斯媒体和电报之间的多语言信息流

    Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram. (arXiv:2301.10856v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2301.10856](http://arxiv.org/abs/2301.10856)

    本文研究了16个俄罗斯媒体机构和732个电报频道之间的互动，发现新闻媒体不仅通过电报传播现有的叙事，而且会从电报平台源材料，研究结果表明2.3％至26.7％的文章将主题归因于电报活动。

    

    在俄罗斯入侵乌克兰后，针对俄罗斯在线媒体的虚假信息和宣传，包括俄罗斯之声和卫星新闻在内的俄罗斯媒体在欧洲遭到禁止。为了保持观众数量，许多俄罗斯媒体开始在电报等消息服务上大力宣传其内容。在这项工作中，我们研究了2022年期间16家俄罗斯媒体机构如何与732个电报频道互动和利用。利用基础模型MPNet、DP-means聚类和Hawkes过程，我们跟踪新闻网站和电报频道之间的叙事传播情况。我们表明，新闻媒体不仅通过电报传播现有的叙事，而且他们会从电报平台源材料。在我们研究的网站中，2.3％（ura.news）至26.7％（ukraina.ru）的文章讨论了源于/导致电报活动的内容。最后，通过跟踪个别主题的扩散，我们测量新闻网站发表文章的速率。

    In response to disinformation and propaganda from Russian online media following the Russian invasion of Ukraine, Russian outlets including Russia Today and Sputnik News were banned throughout Europe. To maintain viewership, many of these Russian outlets began to heavily promote their content on messaging services like Telegram. In this work, we study how 16 Russian media outlets interacted with and utilized 732 Telegram channels throughout 2022. Leveraging the foundational model MPNet, DP-means clustering, and Hawkes Processes, we trace how narratives spread between news sites and Telegram channels. We show that news outlets not only propagate existing narratives through Telegram, but that they source material from the messaging platform. Across the sites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of articles discuss content that originated/resulted from activity on Telegram. Finally, tracking the spread of individual topics, we measure the rate at which news website
    
[^279]: 使用说服性写作策略来解释和检测健康错误信息

    Using Persuasive Writing Strategies to Explain and Detect Health Misinformation. (arXiv:2211.05985v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.05985](http://arxiv.org/abs/2211.05985)

    本研究旨在通过使用说服性写作技巧的文本段落进行分类来增加自动化虚假信息检测的新层次，以产生可解释的理由。我们提出了一个包含常见说服性写作策略的注释方案和数据集，并使用 RoBERTa 文本分类模型进行实验。

    

    虚假信息的传播是当今社会的一大问题，许多学术界和工业界的研究人员正在努力解决这个问题。由于每天创造的虚假信息数量巨大，将此任务留给人工事实检查员是不切实际的。数据科学家和研究人员多年来一直致力于自动化虚假信息检测，但今天仍然是一个具有挑战性的问题。我们的研究目标是为自动化虚假信息检测添加一个新层次；使用具有说服性写作技巧的文本段落进行分类，以产生可解释的理由，说明为什么这篇文章可以标记为虚假信息。为此，我们提出了一个包含许多常见说服性写作策略的新注释方案，以及相应的人工注释数据集。我们使用 RoBERTa 文本分类模型来完成此任务，因为它在自然语言处理方面具有高性能。我们开发了几种基于语言模型的基线模型，并提供了结果分析。

    The spread of misinformation is a prominent problem in today's society, and many researchers in academia and industry are trying to combat it. Due to the vast amount of misinformation that is created every day, it is unrealistic to leave this task to human fact-checkers. Data scientists and researchers have been working on automated misinformation detection for years, and it is still a challenging problem today. The goal of our research is to add a new level to automated misinformation detection; classifying segments of text with persuasive writing techniques in order to produce interpretable reasoning for why an article can be marked as misinformation. To accomplish this, we present a novel annotation scheme containing many common persuasive writing tactics, along with a dataset with human annotations accordingly. For this task, we make use of a RoBERTa model for text classification, due to its high performance in NLP. We develop several language model-based baselines and present the 
    
[^280]: LUT-GEMM：基于LUT的量化矩阵乘法，用于大规模生成式语言模型的高效推理

    LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models. (arXiv:2206.09557v3 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2206.09557](http://arxiv.org/abs/2206.09557)

    本文介绍了一种基于LUT的量化矩阵乘法，用于大规模生成式语言模型的高效推理。采用仅针对权重的量化策略，并提出了LUT-GEMM内核加速量化矩阵乘法，实现压缩比和准确性之间的灵活平衡。

    

    最近自监督学习的先进技术与Transformer架构的结合极大地提高了自然语言处理（NLP）的性能。然而，强大的NLP模型需要越来越大的模型尺寸，从而导致大量的计算和内存需求。本文介绍了一种针对大规模生成式语言模型的高效推理框架。为了减少模型大小，我们采用了一种仅针对权重的量化策略，同时保留了激活函数的完整精度。因此，我们通过非均匀或均匀量化技术获得每个权重的低于4位的量化。我们提出的LUT-GEMM内核加速了量化矩阵乘法，提供了压缩比和准确性之间的灵活平衡。与早期仅适用于权重量化的矩阵乘法内核不同，LUT-GEMM有效地消除了资源消耗更大的反量化过程。

    The recent advancements in self-supervised learning, combined with the Transformer architecture, have enabled natural language processing (NLP) to achieve remarkably low perplexity. However, powerful NLP models necessitate increasing model size, leading to substantial computational and memory requirements. In this paper, we introduce an efficient inference framework tailored for large-scale generative language models. To reduce the model size, we employ a weight-only quantization strategy while preserving full precision for activations. As a result, we attain sub-4-bit quantization for each weight through non-uniform or uniform quantization techniques. Our proposed kernel, called LUT-GEMM, then accelerates quantized matrix multiplications, offering a flexible balance between compression ratio and accuracy. Unlike earlier matrix multiplication kernels that accommodated weight-only quantization, LUT-GEMM efficiently eliminates the resource-demanding dequantization process for both unifor
    
[^281]: 通过本地情感聚合改进隐式情感学习

    Improving Implicit Sentiment Learning via Local Sentiment Aggregation. (arXiv:2110.08604v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2110.08604](http://arxiv.org/abs/2110.08604)

    本文提出了一种本地情感聚合范式，可以提高情感学习的能力，该方法可以有效地建模方面情感相干性，并在三个公共数据集上实现了最先进的性能。

    

    面向方面的情感分类（ABSC）揭示了不同方面之间情感极性的潜在依赖关系。我们的研究进一步探讨了这一现象，提出了相邻方面经常表现出相似情感的概念，我们称之为“方面情感相干性”。我们认为，当前的研究领域还没有充分重视建模方面情感相干性的重要性。为了解决这一问题，我们引入了一种本地情感聚合范式（LSA），可以促进精细的情感相干性建模。这种方法使得可以提取缺乏显式情感描述的方面的隐含情感。利用梯度下降，我们设计了一种差分加权情感聚合窗口，来指导方面情感相干性的建模。实验结果表明，LSA在学习情感相干性方面具有很好的效果，在三个公共数据集上实现了最先进的性能，从而显著提高了情感学习的能力。

    Aspect-based sentiment classification (ABSC) has revealed the potential dependency of sentiment polarities among different aspects. Our study further explores this phenomenon, positing that adjacent aspects often exhibit similar sentiments, a concept we term "aspect sentiment coherency." We argue that the current research landscape has not fully appreciated the significance of modeling aspect sentiment coherency. To address this gap, we introduce a local sentiment aggregation paradigm (LSA) that facilitates fine-grained sentiment coherency modeling. This approach enables the extraction of implicit sentiments for aspects lacking explicit sentiment descriptions. Leveraging gradient descent, we design a differential-weighted sentiment aggregation window that guides the modeling of aspect sentiment coherency. Experimental results affirm the efficacy of LSA in learning sentiment coherency, as it achieves state-of-the-art performance across three public datasets, thus significantly enhancing
    

