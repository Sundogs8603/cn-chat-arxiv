# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764) | 介绍了一种新的1比特LLM变体，通过引入三进制参数在保持性能的情况下显著提高了成本效益，定义了新的训练规律，为设计专门硬件优化的1比特LLMs打开了大门 |
| [^2] | [Massive Activations in Large Language Models](https://arxiv.org/abs/2402.17762) | 大型语言模型中出现了大量激活现象，它们具有非常大的值并且在模型中起到重要作用。 |
| [^3] | [Towards Optimal Learning of Language Models](https://arxiv.org/abs/2402.17759) | 本论文提出了一种关于语言模型最佳学习的理论，通过最大化数据压缩比率来优化学习过程，根据学习定律揭示了最佳学习过程的特性，并在实验中验证了该定理。 |
| [^4] | [Evaluating Very Long-Term Conversational Memory of LLM Agents](https://arxiv.org/abs/2402.17753) | 通过引入机器-人流程，基于LLM代理架构并将其对话基于人物角色和时间事件图进行基础，成功创建了LoCoMo数据集，为非常长期对话的研究填补了空白。 |
| [^5] | [Tower: An Open Multilingual Large Language Model for Translation-Related Tasks](https://arxiv.org/abs/2402.17733) | 本文提出了一种方法，通过在多语言数据上进行预训练，再在翻译流程说明上进行微调，可以使大语言模型在翻译相关任务上超越开放替代方案并与通用封闭式大语言模型竞争。 |
| [^6] | [AmbigNLG: Addressing Task Ambiguity in Instruction for NLG](https://arxiv.org/abs/2402.17717) | AmbigNLG是一个旨在解决自然语言生成任务中指令模糊性挑战的新任务，通过识别和减轻指令中的模糊性，改进了文本生成质量，并突出了清晰和具体指令在提升LLM在NLG任务中表现的关键作用。 |
| [^7] | [Case-Based or Rule-Based: How Do Transformers Do the Math?](https://arxiv.org/abs/2402.17709) | transformers在数学问题中采用基于案例的推理而非基于规则的推理。 |
| [^8] | [RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations](https://arxiv.org/abs/2402.17700) | RAVEL数据集介绍了一种新方法MDAS，该方法在解开语言模型表示方面取得了最新的成果，强调了跨激活特征的重要性。 |
| [^9] | [NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents](https://arxiv.org/abs/2402.17682) | 提出了NextLevelBERT，通过在更高级别的语义表示上进行遮蔽语言建模，有效处理长文档用例，具有超越更大嵌入模型的性能 |
| [^10] | [Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs](https://arxiv.org/abs/2402.17649) | 该研究评估了大型语言模型中的政治世界观的可靠性和一致性，发现他们的可靠性随模型参数数量增加而增加，且在政策方案上有所不同。 |
| [^11] | [SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation](https://arxiv.org/abs/2402.17645) | SongComposer提出了一种用于歌曲生成的大型语言模型，采用符号化的歌曲表示，实现了LLM可以明确创作歌曲的能力。 |
| [^12] | [Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data](https://arxiv.org/abs/2402.17644) | 本研究引入了QRData基准测试，评估了大型语言模型在统计和因果推理方面的能力，结果显示最强模型GPT-4在该测试中准确率为58％，存在改进空间。 |
| [^13] | [Variational Learning is Effective for Large Deep Networks](https://arxiv.org/abs/2402.17641) | 变分学习在大型深度网络中展现出非常好的效果，IVON优化器在训练大型网络时几乎能与Adam相媲美甚至胜过它，且预测不确定性更准确，对模型微调、泛化误差预测和数据敏感性估计均有显著改进。 |
| [^14] | [From Text Segmentation to Smart Chaptering: A Novel Benchmark for Structuring Video Transcriptions](https://arxiv.org/abs/2402.17633) | 引入新型基准YTSeg，针对口语内容提出高效的分层分割模型MiniSeg，将文本分割概念扩展到更实用的“智能章节化”任务。 |
| [^15] | [Fine-Grained Natural Language Inference Based Faithfulness Evaluation for Diverse Summarisation Tasks](https://arxiv.org/abs/2402.17630) | 提出了一种新颖的方法InFusE，用于面向多样化摘要任务的细粒度自然语言推理信念评估，并引入了包括长篇摘要和多样化摘要任务在内的新基准数据集DiverSumm。 |
| [^16] | [Neural Automated Writing Evaluation with Corrective Feedback](https://arxiv.org/abs/2402.17613) | 本文提出了一个集成系统，用于具有纠正反馈的自动写作评估，旨在填补第二语言学习者AWE和GEC结果之间的差距。 |
| [^17] | [Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)](https://arxiv.org/abs/2402.17608) | 通过向T5模型引入语言知识，特别是在结构语言属性的中间任务上进行微调，可以改善对句子级复杂度的预测任务表现，尤其是在资源有限的情况下。 |
| [^18] | [FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in Large-scale Cloud Systems](https://arxiv.org/abs/2402.17583) | 提出了一种名为FaultProfIT的自动化方法，用于处理大规模云系统中的故障模式分析，填补了手动标记的缺陷。 |
| [^19] | [Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization](https://arxiv.org/abs/2402.17574) | Agent-Pro提出了一种基于LLM的代理，通过策略级别的反思和优化，可以从互动经验中学习并逐步提升其行为策略。 |
| [^20] | [Unleashing the Potential of Large Language Models as Prompt Optimizers: An Analogical Analysis with Gradient-based Model Optimizers](https://arxiv.org/abs/2402.17564) | 本文提出了一个新颖的视角，将大型语言模型作为提示优化器来改进任务提示，通过类比基于梯度的模型优化器，设计了改进的LLM-based提示优化器策略，并开发了一种强大的基于梯度启发的LLM-based提示优化器GPO。 |
| [^21] | [OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web](https://arxiv.org/abs/2402.17553) | OmniACT是一个针对代理生成可执行程序完成计算机任务能力的数据集和基准，超越了传统Web自动化，涵盖了各种桌面应用。 |
| [^22] | [COCOA: CBT-based Conversational Counseling Agent using Memory Specialized in Cognitive Distortions and Dynamic Prompt](https://arxiv.org/abs/2402.17546) | CoCoA是一款基于认知行为疗法技术的心理辅导代理，通过构建记忆系统管理信息、提取高层见解，引入动态提示灵活运用CBT技术，生成适当回应，并在与Character.ai角色的对话中展示出显著差异。 |
| [^23] | [Retrieval is Accurate Generation](https://arxiv.org/abs/2402.17532) | 提出了一种从支持文档中选择上下文感知短语的新颖生成方法，通过迭代式自我强化提高训练数据准确性，在各种任务中表现优越，提高了开放式文本生成的质量。 |
| [^24] | [Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides](https://arxiv.org/abs/2402.17531) | Nissist利用TSGs和事故缓解历史提供主动建议，减少人为干预，以提高企业级云服务的事故管理效率。 |
| [^25] | [Predict the Next Word: <Humans exhibit uncertainty in this task and language models _____>](https://arxiv.org/abs/2402.17527) | 评估语言模型在预测下一个词时，是否能够复现人类在这项任务中展示的语言变化性 |
| [^26] | [Latent Attention for Linear Time Transformers](https://arxiv.org/abs/2402.17512) | 提出了一种基于潜在向量定义注意力的方法，将标准transformer中的注意力机制的时间复杂度从二次方降低到与时间线性相关，表现与标准注意力媲美，但允许上下文窗口扩展到远远超出标准的范围。 |
| [^27] | [Extreme Miscalibration and the Illusion of Adversarial Robustness](https://arxiv.org/abs/2402.17509) | 深度学习模型的对抗训练可能会造成对抗性强化学习的幻觉，研究表明通过测试时间温度缩放可以消除这种幻觉。 |
| [^28] | [BASES: Large-scale Web Search User Simulation with Large Language Model based Agents](https://arxiv.org/abs/2402.17505) | 本研究提出了一个基于大型语言模型的用户仿真框架BASES，能够有效地模拟大规模类人类的网络搜索行为。 |
| [^29] | [REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering](https://arxiv.org/abs/2402.17497) | 提出了一种名为REAR的新方法，旨在解决大型语言模型在检索增强生成中无法准确评估检索文档相关性的问题，通过增强对检索文档相关性的自我意识，能够自适应地利用外部知识。 |
| [^30] | [Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages](https://arxiv.org/abs/2402.17496) | 该研究介绍了Emotional Voice Messages (EMOVOME)数据库，其中包含来自100名西班牙说话者的999条自发语音消息，通过专家和非专家的标记实现了在valence和arousal维度上的情感识别，并尝试使用语音和文本转录实现情感识别模型。 |
| [^31] | [Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?](https://arxiv.org/abs/2402.17493) | 通过评估临床大型语言模型在术后风险预测中的应用，研究探讨了使用不同训练策略的模型在围手术期护理中的潜在效果。 |
| [^32] | [Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda Spans in News Articles](https://arxiv.org/abs/2402.17478) | 该论文开发了迄今最大的宣传数据集ArPro，使用GPT-4进行从文本中进行精细宣传检测。 |
| [^33] | [Training-Free Long-Context Scaling of Large Language Models](https://arxiv.org/abs/2402.17463) | 提出了一种名为Dual Chunk Attention (DCA)的方法，可以使Llama2 70B在不需要持续训练的情况下支持超过100k令牌的上下文窗口，能够在长上下文任务中取得与微调模型相媲美甚至更好的性能。 |
| [^34] | [A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to Assist Adolescent Cyberbullying Education](https://arxiv.org/abs/2402.17456) | 该研究提出了一个无代码聊天机器人设计工具，帮助教师设计定制的对话流程和聊天机器人话语，探讨了教师在设计聊天机器人时的需求，并展示了教师将自己看作是引导学生和聊天机器人行为的剧作家的观点。 |
| [^35] | [Deep Learning Based Named Entity Recognition Models for Recipes](https://arxiv.org/abs/2402.17447) | 该研究基于深度学习，针对食谱文本开发了命名实体识别模型，通过系统的数据处理和分析，构建了用于自动生成新食谱的数据集。 |
| [^36] | [Exploiting Emotion-Semantic Correlations for Empathetic Response Generation](https://arxiv.org/abs/2402.17437) | 提出了一种利用动态情感-语义相关性来生成共情式对话回复的模型，通过上下文和情感的交互构建动态情感-语义向量，提高了对情感与语义关联性的理解。 |
| [^37] | [Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder](https://arxiv.org/abs/2402.17433) | 通过Contrastive EEG-Text Masked Autoencoder（CET-MAE）和E2T-PTR框架，提出了一种新的模型和方法来增强基于EEG的语言解码。 |
| [^38] | [Consistency Matters: Explore LLMs Consistency From a Black-Box Perspective](https://arxiv.org/abs/2402.17411) | 该论文探讨了LLMs的一致性问题，提出了一种解决方案，并设计了数据集和基准模型进行实验，结果显示在一致性任务上超过了GPT3.5等模型 |
| [^39] | [A Neural Rewriting System to Solve Algorithmic Problems](https://arxiv.org/abs/2402.17407) | 提出了一种受重写系统启发的神经架构，用于学习算法任务，通过Selector、Solver和Combiner三个专门模块实现算法任务的简化，具有较好的外推能力 |
| [^40] | [Investigating Continual Pretraining in Large Language Models: Insights and Implications](https://arxiv.org/abs/2402.17400) | 本研究探讨了大型语言模型中的持续预训练领域，提出了一种能够在不同领域中整合新信息、保留已学知识并增强跨领域知识转移能力的策略，并引入了新的基准来评估模型的适应性。 |
| [^41] | [Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies](https://arxiv.org/abs/2402.17396) | 对GPT-4在算法问题上进行了系统评估，发现采用先进的提示技术可以提高其准确性。 |
| [^42] | [Spot the bot: Coarse-Grained Partition of Semantic Paths for Bots and Humans](https://arxiv.org/abs/2402.17392) | 本文研究了机器人和人类语义路径的粗粒度划分，比较了不同语言文本的结构和聚类情况，支持结构和聚类差异的假设。 |
| [^43] | [FairBelief - Assessing Harmful Beliefs in Language Models](https://arxiv.org/abs/2402.17389) | 本文提出了FairBelief，一种用于捕获和评估语言模型中有害信念的分析方法，并通过公平数据集对几种最先进的LM进行评估，发现这些LM可能存在有害信念。 |
| [^44] | [KoDialogBench: Evaluating Conversational Understanding of Language Models with Korean Dialogue Benchmark](https://arxiv.org/abs/2402.17377) | 介绍了一个名为KoDialogBench的基准，用于评估语言模型在韩语对话中的会话能力，实验结果表明存在改进空间。 |
| [^45] | [A Dataset for Metaphor Detection in Early Medieval Hebrew Poetry](https://arxiv.org/abs/2402.17371) | 这项研究提出了一个新的晚期古代和中世纪希伯来诗歌数据集，包含专家对隐喻的注释，旨在促进这一领域的进一步研究。 |
| [^46] | [SoFA: Shielded On-the-fly Alignment via Priority Rule Following](https://arxiv.org/abs/2402.17358) | 本文提出了一种新颖的对齐范式——优先规则跟随，在对话中将规则作为主要控制机制，以确保对齐并提出了PriorityDistill，实现了从LLM模拟中提取优先跟随信号的方法，确保规则的强大整合和遵守 |
| [^47] | [RECOST: External Knowledge Guided Data-efficient Instruction Tuning](https://arxiv.org/abs/2402.17355) | 提出了RECOST框架，利用外部知识评估由大型语言模型合成的样本，通过相对预测熵解决数据高效指导调整中数据质量不足的挑战 |
| [^48] | [Unsupervised multiple choices question answering via universal corpus](https://arxiv.org/abs/2402.17333) | 本文提出了一种通过通用领域上下文生成合成MCQA数据的框架，无需依赖手动注释，实验表明方法的有效性。 |
| [^49] | [SKT5SciSumm - A Hybrid Generative Approach for Multi-Document Scientific Summarization](https://arxiv.org/abs/2402.17311) | 提出了一种名为SKT5SciSumm的混合框架，结合了基于引文信息的变换器和T5系列模型，在多文档科学摘要任务上取得了最先进的性能。 |
| [^50] | [Probing Multimodal Large Language Models for Global and Local Semantic Representation](https://arxiv.org/abs/2402.17304) | 通过研究发现，多模态大型语言模型的中间层能够更好地编码全局语义信息，在视觉-语言任务中表现出更好的性能。顶层可能过多关注局部信息，导致理解全局信息的能力下降。 |
| [^51] | [Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese](https://arxiv.org/abs/2402.17302) | LLM使用在印尼语方面能够生成具有足够知识的问题，但在巽他语方面表现不佳，突显中低资源语言之间的性能差距。 |
| [^52] | [Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.17263) | 提出了MELoRA，一种迷你集成低秩适配器，通过使用更少的可训练参数同时保持更高的秩，从而提供改进的性能潜力。 |
| [^53] | [Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue](https://arxiv.org/abs/2402.17262) | 本论文探讨了多轮对话中大型语言模型的安全性漏洞，指出人类可以通过多轮对话诱使其生成有害信息。 |
| [^54] | [Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection](https://arxiv.org/abs/2402.17256) | 本文综合评估了LLMs在各种实验设置下的表现，发现其展现出强大的零次和少次能力，但与完全资源微调的模型相比仍处于劣势。 |
| [^55] | [Image-Text Matching with Multi-View Attention](https://arxiv.org/abs/2402.17237) | 本研究提出了一种使用多视图注意力的双流图像文本匹配方法，以解决单一表示难以全面覆盖复杂内容和缺乏交互的挑战。 |
| [^56] | [MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning](https://arxiv.org/abs/2402.17231) | MATHSENSEI 是一种用于数学推理的工具增强型大型语言模型，通过添加知识检索、程序执行和符号方程求解工具，提高了数学推理能力。 |
| [^57] | [Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models](https://arxiv.org/abs/2402.17226) | 提出了RiC（对话推理）方法，旨在通过对话模拟解决大型语言模型中的主观任务，通过挖掘有用的上下文信息来填补客观推理方式的不足 |
| [^58] | [Measuring Vision-Language STEM Skills of Neural Models](https://arxiv.org/abs/2402.17205) | 该研究引入了一个新挑战，用于测试神经模型的STEM技能，提出了一个包含大量基础技能和问题的数据集，需要理解STEM的多模式视觉语言信息，并展示了最新模型对于低年级技能的有限掌握。 |
| [^59] | [When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method](https://arxiv.org/abs/2402.17193) | 研究了不同扩展因素如何影响大型语言模型微调性能，认为LLM微调遵循着一种特殊的扩展行为。 |
| [^60] | [An Effective Mixture-Of-Experts Approach For Code-Switching Speech Recognition Leveraging Encoder Disentanglement](https://arxiv.org/abs/2402.17189) | 通过引入解缠损失并改进声学编码器，本文提出的方法在处理代码切换现象时表现出色，显著优于先前的方法。 |
| [^61] | [Extreme Encoder Output Frame Rate Reduction: Improving Computational Latencies of Large End-to-End Models](https://arxiv.org/abs/2402.17184) | 通过在编码器中应用多个漏斗降低层，实现了极大程度的输出帧率降低，为大型端到端模型提高了计算效率 |
| [^62] | [Benchmarking Data Science Agents](https://arxiv.org/abs/2402.17168) | 本文引入了DSEval评估范式和一系列创新基准，用于评估数据科学代理在整个数据科学生命周期中的性能，通过引入自举注释方法简化数据集准备流程，改进评估覆盖范围，扩展基准测试的全面性，揭示了普遍存在的障碍并提供了关键见解 |
| [^63] | [Clustering Document Parts: Detecting and Characterizing Influence Campaigns From Documents](https://arxiv.org/abs/2402.17151) | 提出了一种新颖的聚类流程，用于检测和描述影响力活动，通过多种技术增强流程性能，并在文档级别分类上取得了优于传统方法的预测效果 |
| [^64] | [Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models](https://arxiv.org/abs/2402.17124) | 提出了Fact-and-Reflection（FaR）提示策略，通过引入已知“事实”并要求模型“反思”，在两个步骤中改进了大型语言模型的置信校准 |
| [^65] | [Creating Suspenseful Stories: Iterative Planning with Large Language Models](https://arxiv.org/abs/2402.17119) | 提出了一种基于迭代提示的规划方法，首次尝试使用大型语言模型生成令人紧张的故事，并通过广泛的人类评估验证了该方法的有效性。 |
| [^66] | [Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses](https://arxiv.org/abs/2402.17097) | 提出了一种名为Re-Ex的方法，通过引入事实错误说明步骤来修正LLM生成文本中的事实错误，并提出了新的提示技术来减少所需的标记数量和挂钟时间 |
| [^67] | [Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling](https://arxiv.org/abs/2402.17019) | 通过大型语言模型和故事讲述，本论文提出了一种新颖的法律教育方法，帮助非专业人士学习复杂的法律概念，并构建了一个包含法律故事和多项选择题的数据集。 |
| [^68] | [Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings](https://arxiv.org/abs/2402.17016) | 通过引入独特的多任务学习目标，研究者设计了用于支持英语和其他目标语言的最先进的双语文本嵌入模型，显著提高了模型在STS任务上的表现，同时在目标语言理解和跨语言评估任务中超越了现有多语言模型。 |
| [^69] | [Z-AGI Labs at ClimateActivism 2024: Stance and Hate Event Detection on Social Media](https://arxiv.org/abs/2402.17014) | 研究旨在在社交媒体上帮助气候活动人士识别和应对仇恨言论，团队评估了多种模型，在仇恨言论检测、仇恨言论目标识别和立场检测方面取得了有趣的结果。 |
| [^70] | [Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset](https://arxiv.org/abs/2402.17013) | 本研究深入探讨了在瑞士司法预测中实现可解释性和公平性的重要性，利用了唯一可用的多语言LJP数据集，并对最新的单语和多语BERT-based LJP模型进行了可解释性能评估。 |
| [^71] | [DiffuCOMET: Contextual Commonsense Knowledge Diffusion](https://arxiv.org/abs/2402.17011) | DiffuCOMET是一种利用扩散学习来重构叙述上下文与相关常识知识之间语义连接的知识模型，生成的知识在常识多样性、上下文相关性和对已知参考文献的对齐方面达到更好的平衡。 |
| [^72] | [Can Large Language Models Recall Reference Location Like Humans?](https://arxiv.org/abs/2402.17010) | 本文探讨了大型语言模型如何利用预训练阶段的知识回忆参考段落，提出了一个两阶段框架模拟人类回忆参考的过程。 |
| [^73] | [Benchmarking LLMs on the Semantic Overlap Summarization Task](https://arxiv.org/abs/2402.17008) | 该论文对LLMs在Semantic Overlap Summarization任务上进行基准测试，使用TELeR分类法评估了15个流行的LLMs的性能，以评估它们总结多个不同叙述之间重叠信息的能力。 |
| [^74] | [What Do Language Models Hear? Probing for Auditory Representations in Language Models](https://arxiv.org/abs/2402.16998) | 通过训练一个线性探针，将语言模型中的文本表示和预训练音频模型中的声音表示联系在一起，研究发现尽管仅在原始文本上进行训练，语言模型对于一些对象的声音知识有着基于实质的编码。 |
| [^75] | [Long Dialog Summarization: An Analysis](https://arxiv.org/abs/2402.16986) | 长对话摘要的研究强调了在各种应用中为有效沟通创造连贯和上下文丰富摘要的重要性。 |
| [^76] | [Dealing with Data for RE: Mitigating Challenges using NLP and Generative AI](https://arxiv.org/abs/2402.16977) | 数据在现代软件系统中扮演着不可或缺的角色，监管环境的变化、软件个性化需求的增长以及对治理的强调推动着大型企业采用自动化技术，并在AI为中心的系统中不断引入新的挑战和需求。 |
| [^77] | [Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections](https://arxiv.org/abs/2402.16973) | 通过检测潜在幻觉并建议替代方案的通信机制，成功减少人类导航错误高达29%而不增加认知负担 |
| [^78] | [LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language](https://arxiv.org/abs/2402.16929) | LangGPT提出了一个双层提示设计框架，作为LLMs的编程语言，大大增强了LLMs产生高质量响应的能力，并在引导LLMs生成高质量提示方面具有显著效果。 |
| [^79] | [DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers](https://arxiv.org/abs/2402.16914) | 将恶意提示分解为独立的子提示使得LLM越狱攻击更难被检测 |
| [^80] | [LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step](https://arxiv.org/abs/2402.16906) | LDB是一个新颖的调试框架，可以让大型语言模型通过运行时执行信息来完善生成的程序。 |
| [^81] | [BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation](https://arxiv.org/abs/2402.16880) | 该论文提出了一种名为BESA的新型大型语言模型修剪技术，通过应用分块重构损失，与传统的逐层修剪技术不同，BESA具有优势 |
| [^82] | [EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math Languages](https://arxiv.org/abs/2402.16878) | EvoGPT-f是一个新颖的进化框架，用于对五个形式数学语料库进行差异机器可学习性的系统量化分析，为形式数学语言的基准测试提供了新的方法。 |
| [^83] | [Large Language Model Augmented Exercise Retrieval for Personalized Language Learning](https://arxiv.org/abs/2402.16877) | 大型语言模型利用生成能力来合成假设练习，以弥合学习者需求与练习内容之间的语义鸿沟，提高个性化语言学习练习检索效果。 |
| [^84] | [Enhancing Retrieval Processes for Language Generation with Augmented Queries](https://arxiv.org/abs/2402.16874) | 本研究通过检索增强生成（RAG）技术解决了语言生成中“幻觉”问题，并借助查询优化过程将用户查询与高级语言模型连接，显著提升了模型性能。 |
| [^85] | [Language Agents as Optimizable Graphs](https://arxiv.org/abs/2402.16823) | 将基于LLM的代理统一描述为计算图，提出新颖的自动图优化器来改进节点和边，实现了代理之间的自动协作和改进。 |
| [^86] | [Nemotron-4 15B Technical Report](https://arxiv.org/abs/2402.16819) | Nemotron-4 15B是一个150亿参数的大型多语言模型，在多语言能力上表现优异，超过其他规模相似的模型。 |
| [^87] | [D-XCB: Data-independent Debiasing for Fair and Accurate Transformer-based Cyberbullying Detection](https://arxiv.org/abs/2402.16458) | 该论文提出了ID-XCB，这是首个数据无关去偏技术，能够在缓解模型对引发偏见的词的关注的同时提高网络欺凌检测性能，超越了当前最先进的去偏方法。 |
| [^88] | [Cross-domain Chinese Sentence Pattern Parsing](https://arxiv.org/abs/2402.16311) | 本文提出了一种利用大型语言模型进行自我训练的创新方法，通过动态生成训练数据将源领域句法规则与目标领域句子相结合，增强句式结构解析器对各种领域的适应能力，实验证明其在教科书和新闻领域的效果优于基于规则的基准模型1.68个百分点。 |
| [^89] | [FuseChat: Knowledge Fusion of Chat Models](https://arxiv.org/abs/2402.16107) | FuseChat通过知识融合将多个对话模型的集体知识转移到目标语言模型中，避免了昂贵的预训练成本。 |
| [^90] | [Citation-Enhanced Generation for LLM-based Chatbot](https://arxiv.org/abs/2402.16063) | 提出一种基于引文增强的LLM聊天机器人生成方法，采用检索模块搜索支持文档来解决幻觉内容产生的问题。 |
| [^91] | [EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings](https://arxiv.org/abs/2402.16040) | 该研究介绍了EHRNoteQA，这是一个新颖的患者特定问题回答基准，旨在评估临床环境中的大型语言模型（LLMs），具有采用多项选择问题回答格式和需要分析多篇临床笔记的特点。 |
| [^92] | [Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement](https://arxiv.org/abs/2402.15180) | 提出了一种通过自我完善和格式化改进LMs对抗越狱攻击的方法，即使在非安全对齐的LMs中也具有出色的安全性，同时降低攻击成功率。 |
| [^93] | [Machine Unlearning of Pre-trained Large Language Models](https://arxiv.org/abs/2402.15159) | 本研究在大型语言模型（LLMs）中探讨了“被遗忘权”的概念，提出机器遗忘作为解决方案，并在预训练模型中建立了全面的遗忘框架及高效的遗忘方法，同时提供了改进超参数鲁棒性以及高效调整超参数的指南。 |
| [^94] | [Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs](https://arxiv.org/abs/2402.14872) | 本文提出了一种语义镜像越狱（SMJ）方法，通过生成在语义上类似于原始问题的越狱提示来绕过LLMs。 |
| [^95] | [ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models](https://arxiv.org/abs/2402.13516) | 本文介绍了一种名为"ProSparse"的有效稀疏化方法，以推动大型语言模型实现更高的激活稀疏性而不降低模型性能 |
| [^96] | [Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models](https://arxiv.org/abs/2402.12563) | 本文研究了大型语言模型的内在自我校正能力，并提出了一个“如果-否则”（IoE）提示框架，帮助模型评估自身“信心”并进行自我校正。 |
| [^97] | [Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space](https://arxiv.org/abs/2402.12026) | 通过对频率空间的分析，本文提出了一种多尺度低秩适应（MuScleLoRA）方法，用于解决在训练语言模型时受到后门攻击的问题。 |
| [^98] | [Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution](https://arxiv.org/abs/2402.11525) | 提出了一种利用强化学习和人类反馈（RLHF）来改进翻译质量的成本效益偏好学习策略，该策略通过优化奖励模型来区分人类和机器翻译，从而指导改进机器翻译。 |
| [^99] | [LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty](https://arxiv.org/abs/2402.10573) | 提出了一种结合小型微调模型和大型语言模型的LinkNER框架，通过不确定性的链接策略RDC，使微调模型能够补充黑盒LLMs |
| [^100] | [Scaling the Authoring of AutoTutors with Large Language Models](https://arxiv.org/abs/2402.09216) | 本文研究使用大型语言模型（LLMs）来撰写智能辅导系统的潜力，提出了保留传统辅导系统结构和教学法的方法。 |
| [^101] | [A Systematic Review of Data-to-Text NLG](https://arxiv.org/abs/2402.08496) | 这篇系统性回顾全面分析了数据到文本自然语言生成研究的现状，提出未来方向，并解决了相关挑战。 |
| [^102] | [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://arxiv.org/abs/2402.04249) | HarmBench是一个为自动红队和强大拒绝设计的标准化评估框架，通过对18种红队方法和33个目标LLM和防御的比较，得出了新的见解，并介绍了一种高效的对抗训练方法，提高了LLM在各种攻击下的稳健性。 |
| [^103] | [ANLS* -- A Universal Document Processing Metric for Generative Large Language Models](https://arxiv.org/abs/2402.03848) | ANLS*是一种用于生成型模型的新度量方法，针对各种任务包括信息提取和分类任务进行评估。它扩展了现有的ANLS度量方法，可以作为替代方案使用。 |
| [^104] | [Distinguishing the Knowable from the Unknowable with Language Models](https://arxiv.org/abs/2402.03563) | 通过研究大型语言模型，在自由文本中识别作为代理的模型和冻结预训练模型的嵌入的小型线性探测器可以准确预测更大模型令牌级别上的自信度，进一步提出了一种无监督的方法在相同任务上达到了非平凡的准确度，这证明了语言模型中存在不同类型的不确定性表示。 |
| [^105] | [GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models](https://arxiv.org/abs/2402.03299) | 本论文提出了一个通过角色扮演的系统，可以生成自然语言越狱，用于测试大型语言模型的指南遵循情况。系统通过收集现有越狱并将其组织成知识图来生成新的越狱，证明了其高效性和有效性。 |
| [^106] | [Efficient Tool Use with Chain-of-Abstraction Reasoning](https://arxiv.org/abs/2401.17464) | 该方法通过让LLM首先解码抽象推理链，然后调用领域工具填充具体知识，使得LLM能够更好地利用工具进行多步推理，并且具有通用性和鲁棒性。 |
| [^107] | [Anatomy of Neural Language Models](https://arxiv.org/abs/2401.03797) | 本教程详细、简化和清晰地解释了神经语言模型，并提供清晰的图形说明，填补了缺乏统一数学框架的现存问题。 |
| [^108] | [Cascade Speculative Drafting for Even Faster LLM Inference](https://arxiv.org/abs/2312.11462) | 引入了Cascade Speculative Drafting（CS Drafting）算法，通过垂直级联消除神经模型的自回归生成，通过水平级联优化草稿中的时间分配，从而进一步提高LLM推理效率。 |
| [^109] | [KnowGPT: Black-Box Knowledge Injection for Large Language Models](https://arxiv.org/abs/2312.06185) | KnowGPT是一种为大型语言模型提供黑盒知识注入的框架，通过深度强化学习和多臂老虎机构建最适合每个问题的提示，在三个基准数据集上实验证明其显著提升了知识注入的效果。 |
| [^110] | [Where Do People Tell Stories Online? Story Detection Across Online Communities](https://arxiv.org/abs/2311.09675) | 介绍了一个解决在线社区中故事检测困难的挑战，提出了StorySeeker工具包，包括详细注释的Reddit数据集和模型，突出了在线叙事的文本特征，引入了叙事跨度检测作为一个新任务。 |
| [^111] | [Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning](https://arxiv.org/abs/2311.08894) | 提出了一种新的KBQA架构FuSIC-KBQA，通过多个源训练的召回器执行KB检索，在LLM的重新排序后以此作为LLM少样本上下文学习的输入来生成逻辑形式，并利用执行引导反馈进一步优化。 |
| [^112] | [Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning](https://arxiv.org/abs/2310.18338) | 细调小型语言模型作为分解生成器，使用策略梯度优化来协调更大型语言模型，进一步提高复杂推理能力 |
| [^113] | [Ask Again, Then Fail: Large Language Models' Vacillations in Judgement](https://arxiv.org/abs/2310.02174) | 目前的语言模型在面对后续问题时常常摇摆不定，研究者提出了一个后续问题机制和两个度量标准来量化这种不一致性，并开发出Unwavering-FQ框架来教导模型保持最初的正确判断，实验证明其有效性。 |
| [^114] | [EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria](https://arxiv.org/abs/2309.13633) | EvalLM是一个交互式系统，帮助开发人员通过评估多个输出来改进大型语言模型提示，相比手动评估，能够帮助用户撰写更多样化的标准、检查更多输出，达到更满意的提示。 |
| [^115] | ["According to ...": Prompting Language Models Improves Quoting from Pre-Training Data](https://arxiv.org/abs/2305.13252) | 提出了“根据”提示方法，通过引导大型语言模型在响应中参考先前观察到的文本来改进引用，并提出了一种新的评估指标（QUIP-Score）来度量模型生成的答案在文本语料库中的直接发现程度。 |
| [^116] | [A Frustratingly Simple Decoding Method for Neural Text Generation](https://arxiv.org/abs/2305.12675) | 提出了一种称为Frustratingly Simple Decoding (FSD)的神经文本生成解码方法，通过构建反语言模型来惩罚未来生成已生成的内容，实验证明其通过引入几乎无额外计算开销就能有效超越传统方法（如核采样）。 |
| [^117] | [Deep Augmentation: Self-Supervised Learning with Transformations in Activation Space](https://arxiv.org/abs/2303.14537) | 深度增强是一种利用dropout或PCA在神经网络中转换目标层的方法，有效改善性能和泛化能力。在对比学习任务中，在Transformers、ResNets和图神经网络等基础模型上，通过深度增强实现了显著的性能提升，但在监督问题上效果相反。 |
| [^118] | [Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data](https://arxiv.org/abs/2302.12822) | 提出了Automate-CoT策略，可以通过从小型标记数据集自动增加合理链，并修剪低质量链，构建基于标签的机器生成理由链的候选池，最终选择最佳组合。 |
| [^119] | [Cross-lingual Text-To-Speech with Flow-based Voice Conversion for Improved Pronunciation](https://arxiv.org/abs/2210.17264) | 通过基于流的语音转换实现了跨语言文本到语音，提升了发音质量，并在客观和主观评估中表现出优势。 |
| [^120] | [A Robust Cybersecurity Topic Classification Tool](https://arxiv.org/abs/2109.02473) | 通过使用多数投票的方式，研究提出了一种网络安全主题分类工具，相比于21个单独模型，该工具在检测网络安全相关文本时表现出较低的假阳性和假阴性率，并且能够处理数十万份文档。 |
| [^121] | [Do Not (Always) Look Right: Investigating the Capabilities of Decoder-Based Large Language Models for Sequence Labeling.](http://arxiv.org/abs/2401.14556) | 本文研究了基于解码器的大型语言模型在序列标注方面的能力，并探索了提高它们性能的策略。 |
| [^122] | [How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual Translation via Tiny Multi-Parallel Data.](http://arxiv.org/abs/2401.12413) | 本文研究了如何通过仅有的少量微小多语言平行数据来增强以英语为中心的模型的零样本翻译能力，实现大幅度的非英文整体改进，并保持英文方向上的性能能力。研究表明，即使在随机抽取的少量方向集上进行微调，也可以获得可比较的整体改进。 |
| [^123] | [FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for Large Language Models' Training?.](http://arxiv.org/abs/2401.11033) | 这项工作介绍了一个将FAIR数据原则嵌入到大型语言模型训练中的框架，并提供了整合指南和检查清单。通过一个案例研究，展示了在符合FAIR标准的数据集中识别和减轻偏见的实际应用。 |
| [^124] | [Multistage Collaborative Knowledge Distillation from Large Language Models for Semi-Supervised Sequence Generation.](http://arxiv.org/abs/2311.08640) | 将大型语言模型的知识通过多阶段协作蒸馏的方式应用于半监督序列生成任务中，可以显著提高模型的泛化能力和性能。 |
| [^125] | [Are Large Language Models Post Hoc Explainers?.](http://arxiv.org/abs/2310.05797) | 这项工作提出了第一个研究大型语言模型（LLMs）解释其他预测模型有效性的框架，并且提出了多个提示策略，填补了当前对于LLMs在解释其他模型行为方面的缺失。 |
| [^126] | [When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets.](http://arxiv.org/abs/2309.08541) | 通过对11种扩展技术、12个不同分布变化的数据集和24个检索模型的全面分析，我们发现使用大型语言模型进行查询或文档扩展的效果与检索器性能相关，对于弱模型来说扩展提高了分数，但对于强模型来说扩展通常会损害分数。 |
| [^127] | [Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers.](http://arxiv.org/abs/2309.08532) | 本文提出了一种通过连接大型语言模型和进化算法进行提示优化的框架，名为EvoPrompt。通过利用大型语言模型的语言处理能力和进化算法的优化性能，EvoPrompt可以自动化处理需要连贯和可读性良好的提示，提高大型语言模型的性能。 |
| [^128] | [Preference Ranking Optimization for Human Alignment.](http://arxiv.org/abs/2306.17492) | 本文提出了Preference Ranking Optimization (PRO)方法，通过扩展布拉德利-特里比较，采用偏好排序的方式来直接对齐大型语言模型（LLMs），解决了强化学习从人类反馈中学习的复杂性、不稳定性和对超参数的敏感性的问题。 |
| [^129] | [DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.](http://arxiv.org/abs/2306.11698) | 这项工作提出了对GPT模型进行全面可信度评估，考虑了多个方面的风险，发现了以前未公开的威胁漏洞，例如对毒性输出和个人信息泄漏的易被误导性。 |
| [^130] | [Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making.](http://arxiv.org/abs/2305.17588) | 该论文介绍了一种名为SUFO的框架，用于增强微调的变压器模型的可解释性。该框架利用多种分析和可视化技术，解决了模型信任和可解释性的关键问题，并通过案例研究验证了其有效性。 |
| [^131] | [Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes.](http://arxiv.org/abs/2305.13300) | 本文研究了大型语言模型（LLMs）在遭遇知识冲突时的行为。结果发现，LLMs可以高度接受外部连贯且有说服力的证据，即使与其参数化内存存在冲突，但也可能有局限性。 |
| [^132] | [Enhancing Logical Reasoning of Large Language Models through Logic-Driven Data Augmentation.](http://arxiv.org/abs/2305.12599) | 本论文介绍了一种通过逻辑驱动的数据增强方法来增强大型语言模型的逻辑推理能力。通过将原始文本转换为抽象意义表述图，并对其进行逻辑修改和转换，生成增强数据，从而提升模型性能。该方法适用于各种体系结构的大型语言模型。 |
| [^133] | [Few-Shot Dialogue Summarization via Skeleton-Assisted Prompt Transfer.](http://arxiv.org/abs/2305.12077) | 该论文提出了一种利用骨架辅助的提示传递进行少样本对话摘要的方法，利用骨架生成作为额外的监督来更好地消耗对话状态信息，并提出了一种新型模型SkeletonNet进行骨架生成，实现了最先进的性能。 |
| [^134] | [NevIR: Negation in Neural Information Retrieval.](http://arxiv.org/abs/2305.07614) | 本研究探讨了否定在神经信息检索中的影响，构建了基准模型，结果表明当前信息检索模型大多数都没有考虑否定，而交叉编码器是目前表现最好的架构。 |
| [^135] | [PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences.](http://arxiv.org/abs/2305.02547) | 本文探究了基于LLMs模拟代理的行为，称之为LLM Personas，在分配大五人格类型和性别角色时是否可以生成具有一致性的个性化特质的内容。 |
| [^136] | [HeySQuAD: A Spoken Question Answering Dataset.](http://arxiv.org/abs/2304.13689) | HeySQuAD 是一个大规模口语化问答数据集，旨在衡量机器理解并回答嘈杂的口语提问的能力，同时使用转录的人类口语提问进行训练能显著提高模型表现。 |
| [^137] | [Defending Against Misinformation Attacks in Open-Domain Question Answering.](http://arxiv.org/abs/2212.10002) | 本文提出了一种使用查询扩充来搜索冗余信息、并通过新颖的置信度方法将其集成到模型中的方法，可以有效防御开放域问答系统中的污染攻击，精确匹配率可提高近20%。 |

# 详细

[^1]: 1比特LLM的时代：所有大型语言模型均为1.58比特

    The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits

    [https://arxiv.org/abs/2402.17764](https://arxiv.org/abs/2402.17764)

    介绍了一种新的1比特LLM变体，通过引入三进制参数在保持性能的情况下显著提高了成本效益，定义了新的训练规律，为设计专门硬件优化的1比特LLMs打开了大门

    

    近期的研究，如BitNet，正在为一个新时代的1比特大型语言模型（LLMs）铺平道路。在这项工作中，我们引入了一个1比特LLM变体，即BitNet b1.58，其中LLM的每个单个参数（或权重）均为三进制{-1, 0, 1}。它在困惑度和最终任务性能方面与相同模型大小和训练标记的全精度（即FP16或BF16）Transformer LLM相匹配，同时在延迟、内存、吞吐量和能耗方面显着更具成本效益。更重要的是，1.58比特的LLM定义了一种新的缩放定律和训练新一代既高性能又具成本效益的LLMs的配方。此外，它实现了一个新的计算范式，并为设计专门针对1比特LLMs优化的特定硬件敞开了大门。

    arXiv:2402.17764v1 Announce Type: new  Abstract: Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.
    
[^2]: 大型语言模型中的大量激活

    Massive Activations in Large Language Models

    [https://arxiv.org/abs/2402.17762](https://arxiv.org/abs/2402.17762)

    大型语言模型中出现了大量激活现象，它们具有非常大的值并且在模型中起到重要作用。

    

    我们观察到大型语言模型（LLMs）中的一个经验现象——很少的激活展现出比其他激活明显更大的值（例如，大出 100,000 倍）。我们称之为大量激活。首先，我们展示了大量激活在各种LLMs中的普遍存在，并对其位置进行了表征。其次，我们发现它们的值基本上不受输入影响，并且在LLMs中起到不可或缺的偏置项作用。第三，这些大量激活导致关注概率集中于其对应的标记，并进一步成为自注意输出中的隐式偏置项。最后，我们还研究了视觉Transformer中的大量激活。

    arXiv:2402.17762v1 Announce Type: new  Abstract: We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers.
    
[^3]: 实现语言模型的最佳学习

    Towards Optimal Learning of Language Models

    [https://arxiv.org/abs/2402.17759](https://arxiv.org/abs/2402.17759)

    本论文提出了一种关于语言模型最佳学习的理论，通过最大化数据压缩比率来优化学习过程，根据学习定律揭示了最佳学习过程的特性，并在实验中验证了该定理。

    

    这项工作研究了改进语言模型（LMs）学习的一般原则，旨在减少实现优越性能所需的训练步骤。具体来说，我们提出了一种关于LMs的最佳学习的理论。我们首先提出了一个通过在“LM训练作为无损压缩”视图中最大化数据压缩比率来优化LM学习的目标。然后，我们推导出一个定理，名为学习定律，揭示了在我们的目标下最佳学习过程中动态的特性。随后，我们通过线性分类和现实世界的语言建模任务上的实验验证了该定理。最后，我们经验证明，LMs的最佳学习主要源于改善LMs的缩放定律的系数，为设计实际学习加速方法展示了巨大的前景和重要性。我们的代码可以在https://aka.ms/LearningLaw找到。

    arXiv:2402.17759v1 Announce Type: new  Abstract: This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an "LM-training-as-lossless-compression" view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task. Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods. Our code can be found at https://aka.ms/LearningLaw.
    
[^4]: 评估LLM代理的非常长期对话记忆

    Evaluating Very Long-Term Conversational Memory of LLM Agents

    [https://arxiv.org/abs/2402.17753](https://arxiv.org/abs/2402.17753)

    通过引入机器-人流程，基于LLM代理架构并将其对话基于人物角色和时间事件图进行基础，成功创建了LoCoMo数据集，为非常长期对话的研究填补了空白。

    

    长期开放领域对话方面的现有研究主要集中在评估模型响应，其上下文跨度不超过五个聊天会话。尽管长上下文大语言模型（LLMs）和检索增强生成（RAG）技术有所进展，但它们在非常长期对话中的有效性尚未被探索。为了解决这一研究空白，我们介绍了一种机器-人的流程，通过利用基于LLM的代理架构生成高质量的非常长期对话，并将其对话基于人物角色和时间事件图进行基础。此外，我们赋予每个代理分享和对图像做出反应的能力。生成的对话经人类注释员验证和编辑，以确保长期一致性和与事件图的基础相联系。使用此流程，我们收集了LoCoMo，一个非常长期对话的数据集，每个数据集包含300轮和平均9K令牌，最多达到35个会话。

    arXiv:2402.17753v1 Announce Type: cross  Abstract: Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoM
    
[^5]: Tower：一个面向翻译相关任务的开放多语言大语言模型

    Tower: An Open Multilingual Large Language Model for Translation-Related Tasks

    [https://arxiv.org/abs/2402.17733](https://arxiv.org/abs/2402.17733)

    本文提出了一种方法，通过在多语言数据上进行预训练，再在翻译流程说明上进行微调，可以使大语言模型在翻译相关任务上超越开放替代方案并与通用封闭式大语言模型竞争。

    

    在这篇论文中，我们提出了一种调整大语言模型（LLMs）以满足翻译流程中多个任务的方法。我们在多语言混合的单语和平行数据上进行持续预训练，创建TowerBase，然后在与翻译流程相关的说明上进行微调，创建TowerInstruct。我们的最终模型在几项与翻译流程相关的任务上超过了开放式替代方案，并与通用封闭式LLMs相竞争。为了促进未来的研究，我们发布了Tower模型，我们的专业数据集，针对翻译生态系统专注于LLMs的评估框架，以及我们的基准模型生成集合。

    arXiv:2402.17733v1 Announce Type: new  Abstract: While general-purpose large language models (LLMs) demonstrate proficiency on multiple tasks within the domain of translation, approaches based on open LLMs are competitive only when specializing on a single task. In this paper, we propose a recipe for tailoring LLMs to multiple tasks present in translation workflows. We perform continued pretraining on a multilingual mixture of monolingual and parallel data, creating TowerBase, followed by finetuning on instructions relevant for translation processes, creating TowerInstruct. Our final model surpasses open alternatives on several tasks relevant to translation workflows and is competitive with general-purpose closed LLMs. To facilitate future research, we release the Tower models, our specialization dataset, an evaluation framework for LLMs focusing on the translation ecosystem, and a collection of model generations, including ours, on our benchmark.
    
[^6]: AmbigNLG: 解决NLG指令中的任务模糊性问题

    AmbigNLG: Addressing Task Ambiguity in Instruction for NLG

    [https://arxiv.org/abs/2402.17717](https://arxiv.org/abs/2402.17717)

    AmbigNLG是一个旨在解决自然语言生成任务中指令模糊性挑战的新任务，通过识别和减轻指令中的模糊性，改进了文本生成质量，并突出了清晰和具体指令在提升LLM在NLG任务中表现的关键作用。

    

    在这项研究中，我们介绍了AmbigNLG，这是一个旨在解决自然语言生成（NLG）任务中指令模糊性挑战的新任务。尽管大语言模型（LLMs）在理解和执行各种任务方面具有令人印象深刻的能力，但它们的性能受到现实指令中的模糊性的显著限制。为了解决这个问题，AmbigNLG试图识别并减轻这种模糊性，旨在精细化指令以更好地匹配用户期望。我们介绍了一个包含2,500个实例的数据集AmbigSNI-NLG，并开发了一个模糊性分类法，用于对指令中的模糊性进行分类和注释。我们的方法在文本生成质量方面取得了显著改进，突出了清晰和具体指令在增强LLM在NLG任务中表现方面的关键作用。

    arXiv:2402.17717v1 Announce Type: new  Abstract: In this study, we introduce AmbigNLG, a new task designed to tackle the challenge of task ambiguity in instructions for Natural Language Generation (NLG) tasks. Despite the impressive capabilities of Large Language Models (LLMs) in understanding and executing a wide range of tasks through natural language interaction, their performance is significantly hindered by the ambiguity present in real-world instructions. To address this, AmbigNLG seeks to identify and mitigate such ambiguities, aiming to refine instructions to match user expectations better. We introduce a dataset, AmbigSNI-NLG, consisting of 2,500 instances, and develop an ambiguity taxonomy for categorizing and annotating instruction ambiguities. Our approach demonstrates substantial improvements in text generation quality, highlighting the critical role of clear and specific instructions in enhancing LLM performance in NLG tasks.
    
[^7]: 基于案例还是基于规则：变压器如何进行数学计算？

    Case-Based or Rule-Based: How Do Transformers Do the Math?

    [https://arxiv.org/abs/2402.17709](https://arxiv.org/abs/2402.17709)

    transformers在数学问题中采用基于案例的推理而非基于规则的推理。

    

    尽管现代大型语言模型在各种复杂任务中表现出色，但仍然难以处理一些对人类来说简单且直观的数学问题，例如加法。然而，我们可以轻松学习加法的基本规则，并将其应用于任意长度的新问题，而大型语言模型却难以做到。相反，它们可能依赖于在训练语料库中看到的类似“案例”来获取帮助。我们将这两种不同的推理机制定义为“基于规则的推理”和“基于案例的推理”。由于基于规则的推理对于获得系统化概括能力至关重要，我们旨在探究变压器究竟是使用基于规则还是基于案例的推理来解决数学问题。通过精心设计的五个数学任务的干预实验，我们确认变压器正在执行基于案例的推理，无论是否使用草稿本，这与之前的观察结果一致。

    arXiv:2402.17709v1 Announce Type: new  Abstract: Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition. While we can easily learn basic rules of addition and apply them to new problems of any length, LLMs struggle to do the same. Instead, they may rely on similar "cases" seen in the training corpus for help. We define these two different reasoning mechanisms as "rule-based reasoning" and "case-based reasoning". Since rule-based reasoning is essential for acquiring the systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems. Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that tran
    
[^8]: RAVEL: 在解开语言模型表示方面评估可解释性方法

    RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations

    [https://arxiv.org/abs/2402.17700](https://arxiv.org/abs/2402.17700)

    RAVEL数据集介绍了一种新方法MDAS，该方法在解开语言模型表示方面取得了最新的成果，强调了跨激活特征的重要性。

    

    个别神经元参与多个高级概念的表示。不同的可解释性方法在多大程度上能成功解开这些角色？为了帮助解决这个问题，我们介绍了RAVEL（Resolving Attribute-Value Entanglements in Language Models），这是一个数据集，可以实现对多种现有可解释性方法进行紧密控制的定量比较。我们利用由此产生的概念框架来定义新的Multi-task Distributed Alignment Search（MDAS）方法，该方法能够找到满足多个因果标准的分布式表示。以Llama2-7B作为目标语言模型，MDAS在RAVEL上取得了最新的成果，展示了超越神经元级别分析以识别跨激活的特征的重要性。我们在https://github.com/explanare/ravel上发布了我们的基准。

    arXiv:2402.17700v1 Announce Type: new  Abstract: Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at https://github.com/explanare/ravel.
    
[^9]: 探究使用更高级别表示的遮蔽语言建模在长文档中的应用 - NextLevelBERT

    NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents

    [https://arxiv.org/abs/2402.17682](https://arxiv.org/abs/2402.17682)

    提出了NextLevelBERT，通过在更高级别的语义表示上进行遮蔽语言建模，有效处理长文档用例，具有超越更大嵌入模型的性能

    

    虽然（大型）语言模型在过去几年取得了显著进展，但由于基础注意机制的二次扩展，它们仍然难以合理处理发现在书籍中的长序列。为了解决这个问题，我们提出了NextLevelBERT，这是一个遮蔽语言模型，它不是在标记上操作，而是在文本嵌入的形式中的更高级别语义表示上操作。我们对NextLevelBERT进行预训练，用于预测整个被遮蔽文本块的向量表示，并评估产生的文档向量在三种任务类型上的有效性：1）通过零样本文件嵌入进行语义文本相似性，2）长文档分类，3）多项选择问题回答。我们发现，下一级遮蔽语言建模是一种有效的技术，可以处理长文档用例，并且只要所需的细节水平不太高，就可以超越更大的嵌入模型。我们提供模型和代码 avai

    arXiv:2402.17682v1 Announce Type: new  Abstract: While (large) language models have significantly improved over the last years, they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism. To address this, we propose NextLevelBERT, a Masked Language Model operating not on tokens, but on higher-level semantic representations in the form of text embeddings. We pretrain NextLevelBERT to predict the vector representation of entire masked text chunks and evaluate the effectiveness of the resulting document vectors on three task types: 1) Semantic Textual Similarity via zero-shot document embeddings, 2) Long document classification, 3) Multiple-choice question answering. We find that next level Masked Language Modeling is an effective technique to tackle long-document use cases and can outperform much larger embedding models as long as the required level of detail is not too high. We make model and code avai
    
[^10]: 超越提示脆弱性：评估LLMs中政治世界观的可靠性和一致性

    Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs

    [https://arxiv.org/abs/2402.17649](https://arxiv.org/abs/2402.17649)

    该研究评估了大型语言模型中的政治世界观的可靠性和一致性，发现他们的可靠性随模型参数数量增加而增加，且在政策方案上有所不同。

    

    由于大型语言模型（LLMs）在广泛系统中的使用，我们需要了解它们是否嵌入了特定的世界观以及这些观点所反映的内容。最近的研究报告称，当用政治问卷进行提示时，LLMs表现出左倾自由倾向。然而，目前尚不清楚这些倾向是否可靠（对提示变化稳健）以及这种倾向是否在政策和政治倾向上保持一致。我们提出了一系列测试，评估了基于收集自七个欧盟国家的选举建议问卷并标注为政策领域的数据集上LLMs在政治声明上立场的可靠性和一致性。我们研究了参数从7B到70B的LLMs，并发现它们的可靠性随参数数量增加而增加。更大的模型显示总体上与左倾政党更强的一致性，但在政策方案中有所不同：它们表现出（左倾）积极的立场

    arXiv:2402.17649v1 Announce Type: new  Abstract: Due to the widespread use of large language models (LLMs) in ubiquitous systems, we need to understand whether they embed a specific worldview and what these views reflect. Recent studies report that, prompted with political questionnaires, LLMs show left-liberal leanings. However, it is as yet unclear whether these leanings are reliable (robust to prompt variations) and whether the leaning is consistent across policies and political leaning. We propose a series of tests which assess the reliability and consistency of LLMs' stances on political statements based on a dataset of voting-advice questionnaires collected from seven EU countries and annotated for policy domains. We study LLMs ranging in size from 7B to 70B parameters and find that their reliability increases with parameter count. Larger models show overall stronger alignment with left-leaning parties but differ among policy programs: They evince a (left-wing) positive stance to
    
[^11]: SongComposer：一种用于歌曲生成的大型语言模型，用于歌词和旋律创作

    SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation

    [https://arxiv.org/abs/2402.17645](https://arxiv.org/abs/2402.17645)

    SongComposer提出了一种用于歌曲生成的大型语言模型，采用符号化的歌曲表示，实现了LLM可以明确创作歌曲的能力。

    

    我们提出了SongComposer，一个为歌曲创作而设计的创新型LLM。它能够理解和生成歌曲中的旋律和歌词，通过利用LLM的能力在符号化的歌曲表示中生成。现有的与音乐相关的LLM将音乐视为量化的音频信号，而这种隐式编码导致了编码效率低下和灵活性差。相比之下，我们采用了符号化的歌曲表示，这是人类为音乐设计的成熟和高效的方式，并使LLM能够像人类一样明确地创作歌曲。在实践中，我们设计了一种新颖的元组设计，用于格式化歌词和旋律中的三个音符属性（音高、持续时间和休止时间），从而保证LLM对音乐符号的正确理解，并实现歌词和旋律之间的精确对齐。为了向LLM灌输基本的音乐理解，我们精心收集了SongCompose-PT，一个大规模的歌曲预训练数据集，其中包括了歌词、旋律和成对的

    arXiv:2402.17645v1 Announce Type: cross  Abstract: We present SongComposer, an innovative LLM designed for song composition. It could understand and generate melodies and lyrics in symbolic song representations, by leveraging the capability of LLM. Existing music-related LLM treated the music as quantized audio signals, while such implicit encoding leads to inefficient encoding and poor flexibility. In contrast, we resort to symbolic song representation, the mature and efficient way humans designed for music, and enable LLM to explicitly compose songs like humans. In practice, we design a novel tuple design to format lyric and three note attributes (pitch, duration, and rest duration) in the melody, which guarantees the correct LLM understanding of musical symbols and realizes precise alignment between lyrics and melody. To impart basic music understanding to LLM, we carefully collected SongCompose-PT, a large-scale song pretraining dataset that includes lyrics, melodies, and paired ly
    
[^12]: LLMs是否具备基于数据的统计和因果推理能力？用数据对先进的定量推理进行基准测试

    Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data

    [https://arxiv.org/abs/2402.17644](https://arxiv.org/abs/2402.17644)

    本研究引入了QRData基准测试，评估了大型语言模型在统计和因果推理方面的能力，结果显示最强模型GPT-4在该测试中准确率为58％，存在改进空间。

    

    量化推理是分析数据的关键技能，然而对这种能力的评估仍然有限。为了填补这一空白，我们引入了Quantitative Reasoning with Data（QRData）基准测试，旨在评估大型语言模型在统计和因果推理方面与现实世界数据的能力。该基准测试包括一个精心构建的包含来自教科书、在线学习材料和学术论文的数据表的411个问题的数据集。为了比较模型在数据和文本上的定量推理能力，我们还在基准测试中添加了一个包含290个仅文本问题的辅助数据集，即QRText。我们评估了自然语言推理、基于程序推理和代理推理方法，包括Chain-of-Thought、Program-of-Thoughts、ReAct和代码解释器辅助等在各种模型上的表现。最强的模型GPT-4的准确率达到了58％，但仍有很大的改进空间。

    arXiv:2402.17644v1 Announce Type: cross  Abstract: Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58%, which has a large room for improvement. Among open-source
    
[^13]: 变分学习对大型深度网络有效

    Variational Learning is Effective for Large Deep Networks

    [https://arxiv.org/abs/2402.17641](https://arxiv.org/abs/2402.17641)

    变分学习在大型深度网络中展现出非常好的效果，IVON优化器在训练大型网络时几乎能与Adam相媲美甚至胜过它，且预测不确定性更准确，对模型微调、泛化误差预测和数据敏感性估计均有显著改进。

    

    我们提供了大量实证证据，反驳了变分学习对大型神经网络无效的普遍看法。我们展示了一种名为Improved Variational Online Newton (IVON)的优化器，在训练大型网络（如GPT-2和ResNets）时始终能够与Adam相匹配或胜过它。IVON的计算成本几乎与Adam相同，但其预测不确定性更好。我们展示了IVON的几种新用例，其中我们改进了大型语言模型的微调和模型合并，在准确预测泛化误差和忠实估计对数据的敏感性方面。我们找到了大量支持变分学习有效性的证据。

    arXiv:2402.17641v1 Announce Type: cross  Abstract: We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks. We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch. IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better. We show several new use cases of IVON where we improve fine-tuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data. We find overwhelming evidence in support of effectiveness of variational learning.
    
[^14]: 从文本分割到智能章节化：用于构建视频转录的新型基准

    From Text Segmentation to Smart Chaptering: A Novel Benchmark for Structuring Video Transcriptions

    [https://arxiv.org/abs/2402.17633](https://arxiv.org/abs/2402.17633)

    引入新型基准YTSeg，针对口语内容提出高效的分层分割模型MiniSeg，将文本分割概念扩展到更实用的“智能章节化”任务。

    

    文本分割是自然语言处理中的基本任务，其中文档被分割为连续的部分。然而，在这一领域的先前研究受到了有限数据集的限制，这些数据集要么规模小，要么是合成的，要么只包含结构良好的文件。在本文中，我们通过引入一个新颖的基准 YTSeg 来解决这些限制，重点关注口语内容，这种内容固有地更加无结构化，且在主题和结构上更加多样化。作为这项工作的一部分，我们引入了一种高效的分层分割模型 MiniSeg，它胜过了现有的最先进基线。最后，我们将文本分割的概念扩展到一个更实用的“智能章节化”任务，涉及对无结构化内容的分割，生成有意义的段落标题，以及模型的潜在实时应用。

    arXiv:2402.17633v1 Announce Type: new  Abstract: Text segmentation is a fundamental task in natural language processing, where documents are split into contiguous sections. However, prior research in this area has been constrained by limited datasets, which are either small in scale, synthesized, or only contain well-structured documents. In this paper, we address these limitations by introducing a novel benchmark YTSeg focusing on spoken content that is inherently more unstructured and both topically and structurally diverse. As part of this work, we introduce an efficient hierarchical segmentation model MiniSeg, that outperforms state-of-the-art baselines. Lastly, we expand the notion of text segmentation to a more practical "smart chaptering" task that involves the segmentation of unstructured content, the generation of meaningful segment titles, and a potential real-time application of the models.
    
[^15]: 面向多样化摘要任务的细粒度自然语言推理信念评估

    Fine-Grained Natural Language Inference Based Faithfulness Evaluation for Diverse Summarisation Tasks

    [https://arxiv.org/abs/2402.17630](https://arxiv.org/abs/2402.17630)

    提出了一种新颖的方法InFusE，用于面向多样化摘要任务的细粒度自然语言推理信念评估，并引入了包括长篇摘要和多样化摘要任务在内的新基准数据集DiverSumm。

    

    我们研究了利用现成的自然语言推理（NLI）模型来评估摘要忠实性的现有方法，并指出这些方法在考虑前提和假设的细粒度级别时存在不足。换句话说，被视为假设的较小内容单元是一个句子，而前提由固定数量的文档句子组成。我们提出了一种新颖的方法，即InFusE，它使用可变的前提大小，并将摘要句子简化为更短的假设。与以往侧重于单个短文档摘要的研究不同，我们分析了用于多样化摘要任务的基于NLI的忠实性评估。我们引入了一个新的基准数据集DiverSumm，其中包括长篇摘要（长文档和摘要）和多样化摘要任务（例如会议和多文档摘要）。在实验中，InFusE在不同摘要任务中表现出更优异的性能。

    arXiv:2402.17630v1 Announce Type: new  Abstract: We study existing approaches to leverage off-the-shelf Natural Language Inference (NLI) models for the evaluation of summary faithfulness and argue that these are sub-optimal due to the granularity level considered for premises and hypotheses. That is, the smaller content unit considered as hypothesis is a sentence and premises are made up of a fixed number of document sentences. We propose a novel approach, namely InFusE, that uses a variable premise size and simplifies summary sentences into shorter hypotheses. Departing from previous studies which focus on single short document summarisation, we analyse NLI based faithfulness evaluation for diverse summarisation tasks. We introduce DiverSumm, a new benchmark comprising long form summarisation (long documents and summaries) and diverse summarisation tasks (e.g., meeting and multi-document summarisation). In experiments, InFusE obtains superior performance across the different summarisa
    
[^16]: 具有纠正反馈的神经自动写作评估

    Neural Automated Writing Evaluation with Corrective Feedback

    [https://arxiv.org/abs/2402.17613](https://arxiv.org/abs/2402.17613)

    本文提出了一个集成系统，用于具有纠正反馈的自动写作评估，旨在填补第二语言学习者AWE和GEC结果之间的差距。

    

    在第二语言学习和教学中，利用技术已变得普遍。对于写作评估，自动写作评估（AWE）和语法错误纠正（GEC）已成为增强写作能力、向学习者提供即时个性化反馈的流行和有效方法。借助自然语言处理（NLP）和机器学习算法的力量，AWE和GEC系统已分别开发，以为语言学习者提供自动校正反馈和更准确、无偏的评分，否则这些将被评审员所主观判断。本文提出了一个集成系统，用于具有纠正反馈的自动写作评估，作为填补第二语言学习者AWE和GEC结果差距的手段。该系统使语言学习者能够模拟论文写作测试：学生撰写并提交论文，系统提供评估和纠正反馈。

    arXiv:2402.17613v1 Announce Type: new  Abstract: The utilization of technology in second language learning and teaching has become ubiquitous. For the assessment of writing specifically, automated writing evaluation (AWE) and grammatical error correction (GEC) have become immensely popular and effective methods for enhancing writing proficiency and delivering instant and individualized feedback to learners. By leveraging the power of natural language processing (NLP) and machine learning algorithms, AWE and GEC systems have been developed separately to provide language learners with automated corrective feedback and more accurate and unbiased scoring that would otherwise be subject to examiners. In this paper, we propose an integrated system for automated writing evaluation with corrective feedback as a means of bridging the gap between AWE and GEC results for second language learners. This system enables language learners to simulate the essay writing tests: a student writes and submi
    
[^17]: 语言知识可以增强编码器-解码器模型（如果你允许的话）

    Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)

    [https://arxiv.org/abs/2402.17608](https://arxiv.org/abs/2402.17608)

    通过向T5模型引入语言知识，特别是在结构语言属性的中间任务上进行微调，可以改善对句子级复杂度的预测任务表现，尤其是在资源有限的情况下。

    

    在本文中，我们探讨了如何通过增加预先训练的编码器-解码器模型，特别是T5模型，与语言知识来预测目标任务的影响。具体而言，我们调查了在中间任务上微调T5模型，该任务预测句子的结构语言属性，是否会改变其在预测句子级复杂度的目标任务中的表现。我们的研究涵盖了在意大利语和英语数据集上进行的各种实验，采用了不同规模的单语和多语T5模型。对两种语言以及跨语言配置的结果表明，基于语言学动机的中间微调通常对目标任务的性能产生积极影响，尤其是当应用于较小模型和数据有限的情况下。

    arXiv:2402.17608v1 Announce Type: new  Abstract: In this paper, we explore the impact of augmenting pre-trained Encoder-Decoder models, specifically T5, with linguistic knowledge for the prediction of a target task. In particular, we investigate whether fine-tuning a T5 model on an intermediate task that predicts structural linguistic properties of sentences modifies its performance in the target task of predicting sentence-level complexity. Our study encompasses diverse experiments conducted on Italian and English datasets, employing both monolingual and multilingual T5 models at various sizes. Results obtained for both languages and in cross-lingual configurations show that linguistically motivated intermediate fine-tuning has generally a positive impact on target task performance, especially when applied to smaller models and in scenarios with limited data availability.
    
[^18]: FaultProfIT: 大规模云系统中故障票据的分层故障分析

    FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in Large-scale Cloud Systems

    [https://arxiv.org/abs/2402.17583](https://arxiv.org/abs/2402.17583)

    提出了一种名为FaultProfIT的自动化方法，用于处理大规模云系统中的故障模式分析，填补了手动标记的缺陷。

    

    事后分析在云系统中的事件管理中至关重要，它为改进系统的可靠性和稳健性提供了宝贵的见解。在CloudA，故障模式分析是在事后阶段执行的，涉及将事件故障分类为独特类别，称为故障模式。通过汇总和分析这些故障模式，工程师可以识别常见故障、脆弱组件和新出现的故障趋势。然而，这一过程目前是通过手动标记进行的，存在固有缺陷。一方面，事件数量庞大意味着只分析了最严重的事件，导致对故障模式的偏斜概述。另一方面，任务的复杂性需要广泛的领域知识，这导致错误和不一致性。为了解决这些限制，我们提出了一种自动化方法，名为FaultProfIT，用于处理故障模式。

    arXiv:2402.17583v1 Announce Type: cross  Abstract: Postmortem analysis is essential in the management of incidents within cloud systems, which provides valuable insights to improve system's reliability and robustness. At CloudA, fault pattern profiling is performed during the postmortem phase, which involves the classification of incidents' faults into unique categories, referred to as fault pattern. By aggregating and analyzing these fault patterns, engineers can discern common faults, vulnerable components and emerging fault trends. However, this process is currently conducted by manual labeling, which has inherent drawbacks. On the one hand, the sheer volume of incidents means only the most severe ones are analyzed, causing a skewed overview of fault patterns. On the other hand, the complexity of the task demands extensive domain knowledge, which leads to errors and inconsistencies. To address these limitations, we propose an automated approach, named FaultProfIT, for Fault pattern 
    
[^19]: Agent-Pro: 通过策略级别反思和优化学习进化

    Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization

    [https://arxiv.org/abs/2402.17574](https://arxiv.org/abs/2402.17574)

    Agent-Pro提出了一种基于LLM的代理，通过策略级别的反思和优化，可以从互动经验中学习并逐步提升其行为策略。

    

    大型语言模型表现出在各种任务中具有强大问题解决能力。然而，大多数基于LLM的代理都是特定任务求解器，并具有复杂的提示工程，而不是能够通过互动学习和进化的代理。本文提出了Agent-Pro：一种基于LLM的代理，具有策略级别的反思和优化，可以从互动经验中学习丰富的专业知识，并逐渐提升其行为策略。具体来说，它涉及一个动态信念生成和反思过程，用于策略演化。

    arXiv:2402.17574v1 Announce Type: new  Abstract: Large Language Models exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover
    
[^20]: 将大型语言模型释放为提示优化器的潜力：与基于梯度的模型优化器的类比分析

    Unleashing the Potential of Large Language Models as Prompt Optimizers: An Analogical Analysis with Gradient-based Model Optimizers

    [https://arxiv.org/abs/2402.17564](https://arxiv.org/abs/2402.17564)

    本文提出了一个新颖的视角，将大型语言模型作为提示优化器来改进任务提示，通过类比基于梯度的模型优化器，设计了改进的LLM-based提示优化器策略，并开发了一种强大的基于梯度启发的LLM-based提示优化器GPO。

    

    自动提示优化是提高大型语言模型（LLMs）性能的重要方法。最近的研究表明，使用LLMs作为提示优化器具有潜力，可以通过迭代改进生成改进的任务提示。本文提出了一个新颖的视角，通过与基于梯度的模型优化器进行类比来研究基于LLM的提示优化器的设计。为了连接这两种方法，我们确定模型参数学习中的两个关键因素：更新方向和更新方法。专注于这两个方面，我们借鉴了梯度优化的理论框架和学习方法，设计了改进的LLM-based提示优化器策略。通过系统分析丰富的改进策略，我们进一步开发了一个能力强大的基于梯度启发的LLM-based提示优化器，称为GPO。

    arXiv:2402.17564v1 Announce Type: new  Abstract: Automatic prompt optimization is an important approach to improving the performance of large language models (LLMs). Recent research demonstrates the potential of using LLMs as prompt optimizers, which can generate improved task prompts via iterative refinement. In this paper, we propose a novel perspective to investigate the design of LLM-based prompt optimizers, by drawing an analogy with gradient-based model optimizers. To connect these two approaches, we identify two pivotal factors in model parameter learning: update direction and update method. Focused on the two aspects, we borrow the theoretical framework and learning methods from gradient-based optimization to design improved strategies for LLM-based prompt optimizers. By systematically analyzing a rich set of improvement strategies, we further develop a capable Gradient-inspired LLM-based Prompt Optimizer called GPO. At each step, it first retrieves relevant prompts from the op
    
[^21]: OmniACT：用于启用桌面和Web多模式通用主动智能体的数据集和基准

    OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web

    [https://arxiv.org/abs/2402.17553](https://arxiv.org/abs/2402.17553)

    OmniACT是一个针对代理生成可执行程序完成计算机任务能力的数据集和基准，超越了传统Web自动化，涵盖了各种桌面应用。

    

    几十年来，人机交互从根本上一直是手动的。即使在今天，几乎所有在计算机上进行的高效工作都需要人类在每一步都提供输入。虚拟主动智能代表了自动化许多这些琐碎任务的一个激动人心的步骤。虚拟代理将使技术能力有限的用户能够充分利用计算机系统的各种可能性。它们还可以实现高效地简化许多计算机任务，从日历管理到复杂的旅行预订，减少人类干预。在这篇论文中，我们介绍了 OmniACT，这是一个用于评估代理生成可执行程序来完成计算机任务能力的首个数据集和基准。我们的范围超越了传统的Web自动化，涵盖了各种桌面应用。该数据集包含诸如"播放下一首歌"之类的基本任务，以及更为长期的任务

    arXiv:2402.17553v1 Announce Type: new  Abstract: For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as "Play the next song", as well as longer horizon tasks such
    
[^22]: COCOA: 基于认知失调和动态提示的CBT对话辅导代理

    COCOA: CBT-based Conversational Counseling Agent using Memory Specialized in Cognitive Distortions and Dynamic Prompt

    [https://arxiv.org/abs/2402.17546](https://arxiv.org/abs/2402.17546)

    CoCoA是一款基于认知行为疗法技术的心理辅导代理，通过构建记忆系统管理信息、提取高层见解，引入动态提示灵活运用CBT技术，生成适当回应，并在与Character.ai角色的对话中展示出显著差异。

    

    随着对提供心理健康护理的对话代理的需求持续增加，我们开发了一款心理辅导代理CoCoA，应用认知行为疗法（CBT）技术来识别和解决客户陈述中固有的认知失调问题。具体来说，我们构建了一个记忆系统，以便有效管理辅导所需的信息，并从客户的话语中提取高层次的见解。此外，为了确保辅导代理生成适当的回应，我们引入了动态提示，灵活应用CBT技术，并促进信息的适当检索。我们在CoCoA和Character.ai的角色之间进行了对话，创建了一个用于评估的数据集。然后，我们要求GPT评估构建的辅导数据集，我们的模型表现出与其他方法的统计显著差异。

    arXiv:2402.17546v1 Announce Type: new  Abstract: The demand for conversational agents that provide mental health care is consistently increasing. In this work, we develop a psychological counseling agent, referred to as CoCoA, that applies Cognitive Behavioral Therapy (CBT) techniques to identify and address cognitive distortions inherent in the client's statements. Specifically, we construct a memory system to efficiently manage information necessary for counseling while extracting high-level insights about the client from their utterances. Additionally, to ensure that the counseling agent generates appropriate responses, we introduce dynamic prompting to flexibly apply CBT techniques and facilitate the appropriate retrieval of information. We conducted dialogues between CoCoA and characters from Character.ai, creating a dataset for evaluation. Then, we asked GPT to evaluate the constructed counseling dataset, and our model demonstrated a statistically significant difference from othe
    
[^23]: 检索即精准生成

    Retrieval is Accurate Generation

    [https://arxiv.org/abs/2402.17532](https://arxiv.org/abs/2402.17532)

    提出了一种从支持文档中选择上下文感知短语的新颖生成方法，通过迭代式自我强化提高训练数据准确性，在各种任务中表现优越，提高了开放式文本生成的质量。

    

    标准语言模型通过从固定的、有限的和独立的词汇中选择标记来生成文本。我们介绍了一种新颖的方法，从一组支持文档中选择上下文感知的短语。这种范式转变中最重要的挑战之一是确定训练数据，因为文本可以以多种方式分割，并且每个片段都可以从多个可能的文档中检索到。为了解决这个问题，我们提出使用语言启发式初始化训练数据，更重要的是通过迭代式自我强化来引导训练数据。大量实验表明，我们的模型不仅在各种知识密集型任务上优于标准语言模型，而且在开放式文本生成中展现出更好的生成质量。例如，与标准语言模型对应的模型，在开放性任务上将准确率从23.47%提高到36.27%。

    arXiv:2402.17532v1 Announce Type: new  Abstract: Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation. For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on Open
    
[^24]: Nissist：基于故障排除指南的事故缓解副驾驶

    Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides

    [https://arxiv.org/abs/2402.17531](https://arxiv.org/abs/2402.17531)

    Nissist利用TSGs和事故缓解历史提供主动建议，减少人为干预，以提高企业级云服务的事故管理效率。

    

    有效的事故管理对企业级云服务的顺畅运作至关重要。 为了加速事故缓解，服务团队将故障排除知识编译成供值班工程师（OCEs）访问的故障排除指南（TSGs）。 尽管自动化流水线已能够解决最常见和简单的事故，但仍存在需要OCE干预的复杂事故。 然而，TSGs通常是非结构化和不完整的，这需要OCE手动解释，导致值班疲劳和生产力下降，特别是新入职的OCE。 在这项工作中，我们提出了Nissist，它利用TSGs和事故缓解历史提供主动建议，减少人为干预。 利用大型语言模型（LLM），Nissist从非结构化TSGs和历史事故缓解讨论中提取见解，形成全面的知识库。

    arXiv:2402.17531v1 Announce Type: cross  Abstract: Effective incident management is pivotal for the smooth operation of enterprises-level cloud services. In order to expedite incident mitigation, service teams compile troubleshooting knowledge into Troubleshooting Guides (TSGs) accessible to on-call engineers (OCEs). While automated pipelines are enabled to resolve the most frequent and easy incidents, there still exist complex incidents that require OCEs' intervention. However, TSGs are often unstructured and incomplete, which requires manual interpretation by OCEs, leading to on-call fatigue and decreased productivity, especially among new-hire OCEs. In this work, we propose Nissist which leverages TSGs and incident mitigation histories to provide proactive suggestions, reducing human intervention. Leveraging Large Language Models (LLM), Nissist extracts insights from unstructured TSGs and historical incident mitigation discussions, forming a comprehensive knowledge base. Its multi-a
    
[^25]: 预测下一个词：人类在这项任务中表现出不确定性，而语言模型_____

    Predict the Next Word: <Humans exhibit uncertainty in this task and language models _____>

    [https://arxiv.org/abs/2402.17527](https://arxiv.org/abs/2402.17527)

    评估语言模型在预测下一个词时，是否能够复现人类在这项任务中展示的语言变化性

    

    语言模型（LMs）是训练用于为人类生成文本分配概率的统计模型。因此，合理质疑它们是否很好地近似人类展示的语言变化性。这种形式的统计评估在段落级别上很难执行，因为它需要可接受性判断（即，人类评估）或一个强大的自动代理（这是不平凡的）。然而，在单词级别上，通过给定一些上下文，可以通过与一个预先记录的替代单词连续数据集的精确匹配来评估LM的样本。我们利用这一事实，并评估LM重新生成人类（特别是一群英语使用者）在“下一个词预测”任务中展示的变化的能力。这可以被视为一种校准评估，在文本分类的背景下，Baan等人（2022年）将其称为对人类不确定性的校准

    arXiv:2402.17527v1 Announce Type: cross  Abstract: Language models (LMs) are statistical models trained to assign probability to human-generated text. As such, it is reasonable to question whether they approximate linguistic variability exhibited by humans well. This form of statistical assessment is difficult to perform at the passage level, for it requires acceptability judgements (i.e., human evaluation) or a robust automated proxy (which is non-trivial). At the word level, however, given some context, samples from an LM can be assessed via exact matching against a prerecorded dataset of alternative single-word continuations of the available context. We exploit this fact and evaluate the LM's ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the 'next word prediction' task. This can be seen as assessing a form of calibration, which, in the context of text classification, Baan et al. (2022) termed calibration to human uncertaint
    
[^26]: Latent Attention for Linear Time Transformers

    Latent Attention for Linear Time Transformers

    [https://arxiv.org/abs/2402.17512](https://arxiv.org/abs/2402.17512)

    提出了一种基于潜在向量定义注意力的方法，将标准transformer中的注意力机制的时间复杂度从二次方降低到与时间线性相关，表现与标准注意力媲美，但允许上下文窗口扩展到远远超出标准的范围。

    

    标准transformer中的注意力机制的时间复杂度随着序列长度的增加呈二次方增长。我们引入一种通过定义潜在向量的注意力来将其降低到与时间线性相关的方法。该方法可以轻松作为标准注意力机制的替代品。我们的“Latte Transformer”模型可用于双向和单向任务，因果版本允许一种在推理语言生成任务中内存和时间高效的递归实现。标准transformer的下一个标记预测随着序列长度线性增长，而Latte Transformer计算下一个标记所需的时间是恒定的。我们的方法的实证表现可与标准注意力媲美，但允许将上下文窗口扩展到远远超出标准注意力实际可行的范围。

    arXiv:2402.17512v1 Announce Type: new  Abstract: The time complexity of the standard attention mechanism in a transformer scales quadratically with the length of the sequence. We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors. The method is readily usable as a drop-in replacement for the standard attention mechanism. Our "Latte Transformer" model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of language generation tasks. Whilst next token prediction scales linearly with the sequence length for a standard transformer, a Latte Transformer requires constant time to compute the next token. The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention.
    
[^27]: 极端失调与对抗鲁棒性的幻觉

    Extreme Miscalibration and the Illusion of Adversarial Robustness

    [https://arxiv.org/abs/2402.17509](https://arxiv.org/abs/2402.17509)

    深度学习模型的对抗训练可能会造成对抗性强化学习的幻觉，研究表明通过测试时间温度缩放可以消除这种幻觉。

    

    基于深度学习的自然语言处理（NLP）模型容易受到对抗攻击的影响，微小的扰动可能导致模型误分类。对抗训练（AT）经常被用来提升模型的鲁棒性。然而，我们发现了一个有趣的现象：有意或无意地失调模型会掩盖梯度，从而干扰对抗攻击搜索方法，导致表面上看似增加了鲁棒性。我们展示了这种观察到的鲁棒性增益是一种鲁棒性幻觉（IOR），并展示了对手如何执行各种形式的测试时间温度校准来抵消上述干扰，使对抗攻击能够找到对抗样本。因此，我们敦促NLP社区在其鲁棒性评估中纳入测试时间温度缩放，以确保观察到的任何增益都是真实的。

    arXiv:2402.17509v1 Announce Type: new  Abstract: Deep learning-based Natural Language Processing (NLP) models are vulnerable to adversarial attacks, where small perturbations can cause a model to misclassify. Adversarial Training (AT) is often used to increase model robustness. However, we have discovered an intriguing phenomenon: deliberately or accidentally miscalibrating models masks gradients in a way that interferes with adversarial attack search methods, giving rise to an apparent increase in robustness. We show that this observed gain in robustness is an illusion of robustness (IOR), and demonstrate how an adversary can perform various forms of test-time temperature calibration to nullify the aforementioned interference and allow the adversarial attack to find adversarial examples. Hence, we urge the NLP community to incorporate test-time temperature scaling into their robustness evaluations to ensure that any observed gains are genuine. Finally, we show how the temperature can 
    
[^28]: 基于大型语言模型的大规模网络搜索用户仿真

    BASES: Large-scale Web Search User Simulation with Large Language Model based Agents

    [https://arxiv.org/abs/2402.17505](https://arxiv.org/abs/2402.17505)

    本研究提出了一个基于大型语言模型的用户仿真框架BASES，能够有效地模拟大规模类人类的网络搜索行为。

    

    由于大型语言模型（LLMs）具有出色的能力，因此开发基于LLM的代理以可靠地仿真用户变得可行。考虑到真实用户数据的稀缺性和限制（例如隐私问题），本文针对网络搜索进行大规模用户仿真，以改进对用户搜索行为的分析和建模。特别是，我们提出了一个新颖的用户仿真框架BASES，其中包含基于LLM的代理，旨在促进对网络搜索用户行为的全面模拟。我们的仿真框架可以大规模生成独特的用户配置文件，从而导致多样化的搜索行为。为了证明BASES的有效性，我们基于中英文的两个人类基准进行了评估实验，证明BASES能够有效地模拟大规模类似人类的搜索行为。为了进一步促进网络搜索研究，我们开发了一个新的大规模仿真测试集

    arXiv:2402.17505v1 Announce Type: cross  Abstract: Due to the excellent capacities of large language models (LLMs), it becomes feasible to develop LLM-based agents for reliable user simulation. Considering the scarcity and limit (e.g., privacy issues) of real user data, in this paper, we conduct large-scale user simulation for web search, to improve the analysis and modeling of user search behavior. Specially, we propose BASES, a novel user simulation framework with LLM-based agents, designed to facilitate comprehensive simulations of web search user behaviors. Our simulation framework can generate unique user profiles at scale, which subsequently leads to diverse search behaviors. To demonstrate the effectiveness of BASES, we conduct evaluation experiments based on two human benchmarks in both Chinese and English, demonstrating that BASES can effectively simulate large-scale human-like search behaviors. To further accommodate the research on web search, we develop WARRIORS, a new larg
    
[^29]: REAR：一种面向开放域问答的关注度感知检索增强框架

    REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering

    [https://arxiv.org/abs/2402.17497](https://arxiv.org/abs/2402.17497)

    提出了一种名为REAR的新方法，旨在解决大型语言模型在检索增强生成中无法准确评估检索文档相关性的问题，通过增强对检索文档相关性的自我意识，能够自适应地利用外部知识。

    

    考虑到有限的内部参数化知识，检索增强生成（RAG）被广泛用于扩展大型语言模型（LLMs）的知识范围。尽管在RAG研究上进行了大量努力，但在现有方法中，LLMs 无法准确评估检索文档的相关性，因此很可能导致对外部知识（即检索文档）的误导甚至错误利用。为解决这一问题，本文提出了 REAR，一种面向开放域问答（QA）的关注度感知检索增强方法。作为关键动机，我们旨在增强LLMs对来源相关性的自我意识，以便在RAG系统中自适应地利用外部知识。特别是，我们开发了一种新的基于LLM的RAG系统架构，通过整合一个精确评估检索文档相关性的特别设计的排名头。此外，我们提出了一种改进的训练方法。

    arXiv:2402.17497v1 Announce Type: new  Abstract: Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs). Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA). As the key motivation, we aim to enhance the self-awareness of source relevance for LLMs, so as to adaptively utilize external knowledge in RAG systems. Specially, we develop a new architecture for LLM based RAG system, by incorporating a specially designed rank head that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method 
    
[^30]: Emotional Voice Messages (EMOVOME)数据库：自发情感语音消息中的情感识别

    Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages

    [https://arxiv.org/abs/2402.17496](https://arxiv.org/abs/2402.17496)

    该研究介绍了Emotional Voice Messages (EMOVOME)数据库，其中包含来自100名西班牙说话者的999条自发语音消息，通过专家和非专家的标记实现了在valence和arousal维度上的情感识别，并尝试使用语音和文本转录实现情感识别模型。

    

    Emotional Voice Messages (EMOVOME)是一个自发语音数据集，包含来自100名西班牙说话者、男女性平衡的999条真实会话中的音频消息，这些消息通过一个消息应用程序产生，在参与者被招募之前在野外环境中制作，避免了由于实验室环境而产生的任何意识偏见。音频按照三个非专家和两个专家的认可在valence和arousal维度上进行了标记，然后将它们结合以获得每个维度的最终标签。专家还提供了对应于七种情感类别的额外标签。为了为将来使用EMOVOME进行调查设定基准，我们使用了语音和音频转录来实现情感识别模型。对于语音部分，我们使用了标准的eGeMAPS特征集和支持向量机，分别获得了49.27%和44.71%的valence和arousal未加权准确度。对于文本部分，我们对一个多语言BERT模型进行了微调，并实现了61%的情感识别准确度。

    arXiv:2402.17496v1 Announce Type: cross  Abstract: Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing 999 audio messages from real conversations on a messaging app from 100 Spanish speakers, gender balanced. Voice messages were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to laboratory environment. Audios were labeled in valence and arousal dimensions by three non-experts and two experts, which were then combined to obtain a final label per dimension. The experts also provided an extra label corresponding to seven emotion categories. To set a baseline for future investigations using EMOVOME, we implemented emotion recognition models using both speech and audio transcriptions. For speech, we used the standard eGeMAPS feature set and support vector machines, obtaining 49.27% and 44.71% unweighted accuracy for valence and arousal respectively. For text, we fine-tuned a multilingual BERT model and achieved 61
    
[^31]: 为围手术期护理开具大型语言模型：预训练模型的正确剂量是多少？

    Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?

    [https://arxiv.org/abs/2402.17493](https://arxiv.org/abs/2402.17493)

    通过评估临床大型语言模型在术后风险预测中的应用，研究探讨了使用不同训练策略的模型在围手术期护理中的潜在效果。

    

    术后风险预测可以指导有效的围手术期护理管理和规划。我们旨在评估临床大型语言模型(LLMs)是否可以使用不同的训练策略预测术后风险。研究主要涉及2018年至2021年间来自Barnes Jewish医院系统的84,875份记录。方法在Beth Israel Deaconess的MIMIC数据集上进行了复制。两项研究的平均随访时间基于术后ICU住院时间小于7天。对于BJH数据集，结果包括30天死亡率、肺栓塞（PE）和肺炎。对BioGPT、ClinicalBERT和BioClinicalBERT实施了三种域自适应和微调策略：自监督目标；结合半监督微调的标签；以及通过多任务学习进行基础建模。模型性能使用接收器操作特征下的面积进行了比较。

    arXiv:2402.17493v1 Announce Type: new  Abstract: Postoperative risk predictions can inform effective perioperative care management and planning. We aimed to assess whether clinical large language models (LLMs) can predict postoperative risks using clinical texts with various training strategies. The main cohort involved 84,875 records from Barnes Jewish Hospital (BJH) system between 2018 and 2021. Methods were replicated on Beth Israel Deaconess's MIMIC dataset. Both studies had mean duration of follow-up based on the length of postoperative ICU stay less than 7 days. For the BJH dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and pneumonia. Three domain adaptation and finetuning strategies were implemented for BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives; incorporating labels with semi-supervised fine-tuning; and foundational modelling through multi-task learning. Model performance was compared using the area under the receiver operating ch
    
[^32]: GPT-4能识别宣传吗？新闻文章中宣传段落的注释和检测

    Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda Spans in News Articles

    [https://arxiv.org/abs/2402.17478](https://arxiv.org/abs/2402.17478)

    该论文开发了迄今最大的宣传数据集ArPro，使用GPT-4进行从文本中进行精细宣传检测。

    

    在主流媒体和社交媒体上的宣传使用激增，旨在操纵或误导用户。尽管自动检测文本、视觉或多模态内容中的宣传技术的努力增加，但其中大部分主要集中在英文内容上。最近大部分针对中低资源语言的举措产生了相对较小的标记数据集，分布不均，给精致宣传检测模型的开发带来挑战。为解决这一挑战，我们精心开发了迄今为止最大的宣传数据集ArPro，包括来自报纸文章的8K段落，在23种宣传技术的分类法下进行文本段标记。此外，我们的工作首次尝试了使用GPT-4进行从文本中精细宣传检测的大型语言模型（LLM）性能。结果表明，GPT-4的表现

    arXiv:2402.17478v1 Announce Type: new  Abstract: The use of propaganda has spiked on mainstream and social media, aiming to manipulate or mislead users. While efforts to automatically detect propaganda techniques in textual, visual, or multimodal content have increased, most of them primarily focus on English content. The majority of the recent initiatives targeting medium to low-resource languages produced relatively small annotated datasets, with a skewed distribution, posing challenges for the development of sophisticated propaganda detection models. To address this challenge, we carefully develop the largest propaganda dataset to date, ArPro, comprised of 8K paragraphs from newspaper articles, labeled at the text span level following a taxonomy of 23 propagandistic techniques. Furthermore, our work offers the first attempt to understand the performance of large language models (LLMs), using GPT-4, for fine-grained propaganda detection from text. Results showed that GPT-4's performa
    
[^33]: 无须训练的大语言模型长上下文扩展

    Training-Free Long-Context Scaling of Large Language Models

    [https://arxiv.org/abs/2402.17463](https://arxiv.org/abs/2402.17463)

    提出了一种名为Dual Chunk Attention (DCA)的方法，可以使Llama2 70B在不需要持续训练的情况下支持超过100k令牌的上下文窗口，能够在长上下文任务中取得与微调模型相媲美甚至更好的性能。

    

    大语言模型（LLMs）在处理和生成连贯文本时，当输入令牌数量超过它们的预训练长度时，其能力会明显减弱。鉴于使用更长序列进行大规模模型微调的昂贵开销，我们提出了Dual Chunk Attention（DCA），它使Llama2 70B能够支持超过100k令牌的上下文窗口，而无需持续训练。通过将长序列的注意力计算分解为基于块的模块，DCA成功捕获了相同块内（Intra-Chunk）和不同块之间（Inter-Chunk）令牌的相对位置信息，并能与Flash Attention无缝集成。除了其惊人的外推能力外，DCA在实际长上下文任务上实现了与或甚至优于微调模型相当的性能。与专有模型相比，我们的无须训练的70B模型取得了

    arXiv:2402.17463v1 Announce Type: new  Abstract: The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of finetuning large-scale models with longer sequences, we propose Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of more than 100k tokens without continual training. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of finetuned models. When compared with proprietary models, our training-free 70B model attai
    
[^34]: 一部戏剧：探讨教师如何设计 LLM 聊天机器人以协助青少年防止网络欺凌教育

    A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to Assist Adolescent Cyberbullying Education

    [https://arxiv.org/abs/2402.17456](https://arxiv.org/abs/2402.17456)

    该研究提出了一个无代码聊天机器人设计工具，帮助教师设计定制的对话流程和聊天机器人话语，探讨了教师在设计聊天机器人时的需求，并展示了教师将自己看作是引导学生和聊天机器人行为的剧作家的观点。

    

    研究发现，网络欺凌危害青少年的心理健康，教授他们正确的干预方法至关重要。巫师-奥兹研究表明，聊天机器人可以扩展个性化和互动式的网络欺凌教育，但实施这样的聊天机器人是一项具有挑战性和微妙的任务。我们为 K-12 教师创建了一个无代码聊天机器人设计工具。通过使用大型语言模型和提示链条，我们的工具允许教师原型化定制的对话流程和聊天机器人话语。通过提供这个工具，我们探讨了教师在设计聊天机器人以辅助他们的教学时的独特需求，以及聊天机器人设计工具如何更好地支持他们。我们的研究结果表明，教师热情地接受这个工具。此外，他们将自己视为指导学生和聊天机器人行为的剧作家，同时允许一些即兴演出。他们的目标是使学生能够排练对网络欺凌的理想和不良反应。

    arXiv:2402.17456v1 Announce Type: cross  Abstract: Cyberbullying harms teenagers' mental health, and teaching them upstanding intervention is crucial. Wizard-of-Oz studies show chatbots can scale up personalized and interactive cyberbullying education, but implementing such chatbots is a challenging and delicate task. We created a no-code chatbot design tool for K-12 teachers. Using large language models and prompt chaining, our tool allows teachers to prototype bespoke dialogue flows and chatbot utterances. In offering this tool, we explore teachers' distinctive needs when designing chatbots to assist their teaching, and how chatbot design tools might better support them. Our findings reveal that teachers welcome the tool enthusiastically. Moreover, they see themselves as playwrights guiding both the students' and the chatbot's behaviors, while allowing for some improvisation. Their goal is to enable students to rehearse both desirable and undesirable reactions to cyberbullying in a s
    
[^35]: 食谱的深度学习命名实体识别模型

    Deep Learning Based Named Entity Recognition Models for Recipes

    [https://arxiv.org/abs/2402.17447](https://arxiv.org/abs/2402.17447)

    该研究基于深度学习，针对食谱文本开发了命名实体识别模型，通过系统的数据处理和分析，构建了用于自动生成新食谱的数据集。

    

    食物通过各种努力方式影响着我们的生活，包括口味、营养、健康和可持续性。食谱是通过非结构化文本代代相传的文化胶囊。自动识别命名实体的协议，即食谱文本的基本组成部分，对于各种应用来说都具有巨大价值，从信息提取到新颖食谱生成。命名实体识别是一种从已知标签的非结构化或半结构化数据中提取信息的技术。我们从手动注释的6,611个成分短语的数据开始，累积创建了26,445个短语的增强数据集。同时，我们系统地清理和分析了来自RecipeDB的成分短语，这是黄金标准的食谱数据存储库，并使用Stanford NER进行了标注。基于分析，我们使用基于聚类的方法对88,526个短语的子集进行了取样，同时保留了多样性。

    arXiv:2402.17447v1 Announce Type: cross  Abstract: Food touches our lives through various endeavors, including flavor, nourishment, health, and sustainability. Recipes are cultural capsules transmitted across generations via unstructured text. Automated protocols for recognizing named entities, the building blocks of recipe text, are of immense value for various applications ranging from information extraction to novel recipe generation. Named entity recognition is a technique for extracting information from unstructured or semi-structured data with known labels. Starting with manually-annotated data of 6,611 ingredient phrases, we created an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we systematically cleaned and analyzed ingredient phrases from RecipeDB, the gold-standard recipe data repository, and annotated them using the Stanford NER. Based on the analysis, we sampled a subset of 88,526 phrases using a clustering-based approach while preserving the diversity
    
[^36]: 利用情感-语义相关性进行共情式回复生成

    Exploiting Emotion-Semantic Correlations for Empathetic Response Generation

    [https://arxiv.org/abs/2402.17437](https://arxiv.org/abs/2402.17437)

    提出了一种利用动态情感-语义相关性来生成共情式对话回复的模型，通过上下文和情感的交互构建动态情感-语义向量，提高了对情感与语义关联性的理解。

    

    共情式回复生成旨在通过理解对话语言中说话者的情感感受来生成共情回复。最近的方法捕捉交际者语言中的情感词，并将其构建为静态向量，以感知微妙的情感。然而，语言研究表明，语言中的情感词是动态的，并与其他语法语义角色（即具有语义含义的词语）相关联。以前的方法忽略了这两个特征，这很容易导致情感误解和关键语义的忽视。为了解决这个问题，我们提出了一种用于共情对话生成任务的动态情感-语义相关性模型（ESCM）。ESCM通过上下文和情感的交互构建动态情感-语义向量。我们引入依存树来反映情感与语义之间的相关性。

    arXiv:2402.17437v1 Announce Type: cross  Abstract: Empathetic response generation aims to generate empathetic responses by understanding the speaker's emotional feelings from the language of dialogue. Recent methods capture emotional words in the language of communicators and construct them as static vectors to perceive nuanced emotions. However, linguistic research has shown that emotional words in language are dynamic and have correlations with other grammar semantic roles, i.e., words with semantic meanings, in grammar. Previous methods overlook these two characteristics, which easily lead to misunderstandings of emotions and neglect of key semantics. To address this issue, we propose a dynamical Emotion-Semantic Correlation Model (ESCM) for empathetic dialogue generation tasks. ESCM constructs dynamic emotion-semantic vectors through the interaction of context and emotions. We introduce dependency trees to reflect the correlations between emotions and semantics. Based on dynamic em
    
[^37]: 通过从预训练对比性EEG-文本蒙版自动编码器中转移的表示增强EEG到文本解码

    Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder

    [https://arxiv.org/abs/2402.17433](https://arxiv.org/abs/2402.17433)

    通过Contrastive EEG-Text Masked Autoencoder（CET-MAE）和E2T-PTR框架，提出了一种新的模型和方法来增强基于EEG的语言解码。

    

    从无创脑电图（EEG）重建自然语言具有很大的潜力，作为脑机接口（BCI）的语言解码技术。然而，基于EEG的语言解码仍处于初级阶段，面临诸多技术问题，如：1）缺乏一个能够有效整合跨模态（EEG和文本之间）自学习与EEG特征或文本序列的模内自重构的混合策略；2）未充分利用大型语言模型（LLMs）来增强基于EEG的语言解码。为解决上述问题，我们提出了对比性EEG-文本蒙版自动编码器（CET-MAE），这是一个通过专用的多流编码器在EEG和文本之间以及内部进行复合自监督学习的新型模型。此外，我们开发了一个名为E2T-PTR（使用预训练可转移表示进行EEG到文本解码）的框架，该框架利用预训练模组

    arXiv:2402.17433v1 Announce Type: new  Abstract: Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs). However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of large language models (LLMs) to enhance EEG-based language decoding. To address above issues, we propose the Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that orchestrates compound self-supervised learning across and within EEG and text through a dedicated multi-stream encoder. Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modul
    
[^38]: 一致性至关重要：从黑盒角度探索LLMs的一致性

    Consistency Matters: Explore LLMs Consistency From a Black-Box Perspective

    [https://arxiv.org/abs/2402.17411](https://arxiv.org/abs/2402.17411)

    该论文探讨了LLMs的一致性问题，提出了一种解决方案，并设计了数据集和基准模型进行实验，结果显示在一致性任务上超过了GPT3.5等模型

    

    现如今，商业和开源学术LLM已成为自然语言处理的主流模型。然而，对LLM一致性的研究仍然不足，这意味着在LLM研究和部署的各个阶段中，其内部参数和能力应保持不变。这一问题存在于工业和学术领域。解决这个问题通常耗时且劳力密集，还有额外的二次部署成本，导致经济和时间损失。为弥补这一空白，我们建立了一个LLM一致性任务数据集，并设计了几个基准。此外，我们选择了不同规模的模型进行主要实验。具体来说，在LightGBM实验中，我们使用传统的自然语言生成度量（如ROUGE、BLEU、METEOR）作为模型训练所需的特征。最终结果超过了人工评估以及GPT3.5和其他模型在主要实验中的表现。

    arXiv:2402.17411v1 Announce Type: new  Abstract: Nowadays both commercial and open-source academic LLM have become the mainstream models of NLP. However, there is still a lack of research on LLM consistency, meaning that throughout the various stages of LLM research and deployment, its internal parameters and capabilities should remain unchanged. This issue exists in both the industrial and academic sectors. The solution to this problem is often time-consuming and labor-intensive, and there is also an additional cost of secondary deployment, resulting in economic and time losses. To fill this gap, we build an LLM consistency task dataset and design several baselines. Additionally, we choose models of diverse scales for the main experiments. Specifically, in the LightGBM experiment, we used traditional NLG metrics (i.e., ROUGE, BLEU, METEOR) as the features needed for model training. The final result exceeds the manual evaluation and GPT3.5 as well as other models in the main experiment
    
[^39]: 用神经重写系统解决算法问题

    A Neural Rewriting System to Solve Algorithmic Problems

    [https://arxiv.org/abs/2402.17407](https://arxiv.org/abs/2402.17407)

    提出了一种受重写系统启发的神经架构，用于学习算法任务，通过Selector、Solver和Combiner三个专门模块实现算法任务的简化，具有较好的外推能力

    

    现代神经网络架构仍然难以学习需要系统应用组合规则来解决超出分布问题实例的算法程序。在这项工作中，我们提出了一种原创方法来学习受重写系统启发的算法任务，重写系统是符号人工智能中的经典框架。我们展示了重写系统可以被实现为一个由专门模块组成的神经架构：选择器识别要处理的目标子表达式，求解器通过计算相应的结果简化子表达式，组合器通过用提供的解决方案替换子表达式生成原始表达式的新版本。我们在三种涉及简化涉及列表、算术和代数表达式的符号公式的算法任务上评估我们的模型。我们测试了所提架构的外推能力

    arXiv:2402.17407v1 Announce Type: cross  Abstract: Modern neural network architectures still struggle to learn algorithmic procedures that require to systematically apply compositional rules to solve out-of-distribution problem instances. In this work, we propose an original approach to learn algorithmic tasks inspired by rewriting systems, a classic framework in symbolic artificial intelligence. We show that a rewriting system can be implemented as a neural architecture composed by specialized modules: the Selector identifies the target sub-expression to process, the Solver simplifies the sub-expression by computing the corresponding result, and the Combiner produces a new version of the original expression by replacing the sub-expression with the solution provided. We evaluate our model on three types of algorithmic tasks that require simplifying symbolic formulas involving lists, arithmetic, and algebraic expressions. We test the extrapolation capabilities of the proposed architectu
    
[^40]: 探讨大型语言模型中的持续预训练：见解与影响

    Investigating Continual Pretraining in Large Language Models: Insights and Implications

    [https://arxiv.org/abs/2402.17400](https://arxiv.org/abs/2402.17400)

    本研究探讨了大型语言模型中的持续预训练领域，提出了一种能够在不同领域中整合新信息、保留已学知识并增强跨领域知识转移能力的策略，并引入了新的基准来评估模型的适应性。

    

    本文研究了大型语言模型（LLMs）中不断学习（CL）领域的发展，重点是制定高效可持续培训策略。我们的主要重点是持续领域自适应预训练，这是一个旨在使LLMs能够整合来自不同领域的新信息，同时保留以前学到的知识，并增强跨领域知识转移能力而不依赖于特定领域识别的过程。与以往主要集中于有限任务或领域并主要旨在解决遗忘问题的先前研究不同，我们的研究评估了LLMs对实际情景中不断变化的数据景观的适应性和能力。为此，我们引入了一个新的基准，旨在衡量LLMs对这些不断演变的数据环境的适应性，提供了一个全面的评估框架。

    arXiv:2402.17400v1 Announce Type: new  Abstract: This paper studies the evolving domain of Continual Learning (CL) in large language models (LLMs), with a focus on developing strategies for efficient and sustainable training. Our primary emphasis is on continual domain-adaptive pretraining, a process designed to equip LLMs with the ability to integrate new information from various domains while retaining previously learned knowledge and enhancing cross-domain knowledge transfer without relying on domain-specific identification. Unlike previous studies, which mostly concentrate on a limited selection of tasks or domains and primarily aim to address the issue of forgetting, our research evaluates the adaptability and capabilities of LLMs to changing data landscapes in practical scenarios. To this end, we introduce a new benchmark designed to measure the adaptability of LLMs to these evolving data environments, offering a comprehensive framework for evaluation. We examine the impact of mo
    
[^41]: 在算法问题上对GPT-4进行基准测试：关于提示策略的系统评估

    Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies

    [https://arxiv.org/abs/2402.17396](https://arxiv.org/abs/2402.17396)

    对GPT-4在算法问题上进行了系统评估，发现采用先进的提示技术可以提高其准确性。

    

    大型语言模型（LLMs）通过在海量文本语料库上获得的知识在各种下游任务中重新利用，几乎不需要（或根本不需要）调整步骤，从而革新了自然语言处理领域。与此同时，已经反复显示LLMs缺乏系统化泛化，这使得无法将学习到的统计规律外推到训练分布之外。在本研究中，我们对其中一种最先进的LLMs，GPT-4，在三个算法任务上进行了系统基准测试，这些任务通过两个参数控制问题难度。我们比较了GPT-4与其前身（GPT-3.5）以及最近介绍的变压器编码器架构的变体，即神经数据路由器，在解决类似任务时的性能。我们发现采用先进的提示技术可以使GPT-4达到更高的准确性。

    arXiv:2402.17396v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Processing thanks to their ability to reuse knowledge acquired on massive text corpora on a wide variety of downstream tasks, with minimal (if any) tuning steps. At the same time, it has been repeatedly shown that LLMs lack systematic generalization, which allows to extrapolate the learned statistical regularities outside the training distribution. In this work, we offer a systematic benchmarking of GPT-4, one of the most advanced LLMs available, on three algorithmic tasks characterized by the possibility to control the problem difficulty with two parameters. We compare the performance of GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the Transformer-Encoder architecture recently introduced to solve similar tasks, the Neural Data Router. We find that the deployment of advanced prompting techniques allows GPT-4 to reach superior accuracy o
    
[^42]: 发现机器人：对机器人和人类语义路径的粗粒度划分

    Spot the bot: Coarse-Grained Partition of Semantic Paths for Bots and Humans

    [https://arxiv.org/abs/2402.17392](https://arxiv.org/abs/2402.17392)

    本文研究了机器人和人类语义路径的粗粒度划分，比较了不同语言文本的结构和聚类情况，支持结构和聚类差异的假设。

    

    现在，技术正在迅速发展：机器人正在撰写评论、文章和评论。因此，重要的是要知道文本是由人类编写还是由机器人编写。本文旨在比较人类编写和机器人生成文本的语义路径粗粒度划分的结构。我们比较了文学文本和几个机器人生成文本的n-gram数据集的聚类情况。假设是这些结构和聚类是不同的。我们的研究支持了这一假设。由于不同语言的语义结构可能不同，因此我们调查了俄语、英语、德语和越南语。

    arXiv:2402.17392v1 Announce Type: new  Abstract: Nowadays, technology is rapidly advancing: bots are writing comments, articles, and reviews. Due to this fact, it is crucial to know if the text was written by a human or by a bot. This paper focuses on comparing structures of the coarse-grained partitions of semantic paths for human-written and bot-generated texts. We compare the clusterizations of datasets of n-grams from literary texts and texts generated by several bots. The hypothesis is that the structures and clusterizations are different. Our research supports the hypothesis. As the semantic structure may be different for different languages, we investigate Russian, English, German, and Vietnamese languages.
    
[^43]: 公平信念 - 评估语言模型中的有害信念

    FairBelief - Assessing Harmful Beliefs in Language Models

    [https://arxiv.org/abs/2402.17389](https://arxiv.org/abs/2402.17389)

    本文提出了FairBelief，一种用于捕获和评估语言模型中有害信念的分析方法，并通过公平数据集对几种最先进的LM进行评估，发现这些LM可能存在有害信念。

    

    语言模型（LMs）已被证明存在不良偏见，如果这些系统在没有仔细进行公平审计的情况下集成到现实应用中，可能会伤害少数群体和被忽视的群体。本文提出了FairBelief，一种分析方法，用于捕获和评估信念，即LM可能以不同程度的确信度嵌入的命题，这些命题暗中影响其预测。通过FairBelief，我们利用提示来研究几种最先进的LM在先前被忽视的不同轴上的行为，比如模型规模和可能性，评估对专门设计用于量化LM输出伤害程度的公平数据集的预测。最后，我们对模型发出的信念进行深入的定性评估。我们将FairBelief应用于英语LMs，发现尽管这些架构在各种自然语言处理任务上表现出色，但它们也可能存在有害信念。

    arXiv:2402.17389v1 Announce Type: cross  Abstract: Language Models (LMs) have been shown to inherit undesired biases that might hurt minorities and underrepresented groups if such systems were integrated into real-world applications without careful fairness auditing. This paper proposes FairBelief, an analytical approach to capture and assess beliefs, i.e., propositions that an LM may embed with different degrees of confidence and that covertly influence its predictions. With FairBelief, we leverage prompting to study the behavior of several state-of-the-art LMs across different previously neglected axes, such as model scale and likelihood, assessing predictions on a fairness dataset specifically designed to quantify LMs' outputs' hurtfulness. Finally, we conclude with an in-depth qualitative assessment of the beliefs emitted by the models. We apply FairBelief to English LMs, revealing that, although these architectures enable high performances on diverse natural language processing ta
    
[^44]: KoDialogBench: 用韩语对话基准评估语言模型的会话理解能力

    KoDialogBench: Evaluating Conversational Understanding of Language Models with Korean Dialogue Benchmark

    [https://arxiv.org/abs/2402.17377](https://arxiv.org/abs/2402.17377)

    介绍了一个名为KoDialogBench的基准，用于评估语言模型在韩语对话中的会话能力，实验结果表明存在改进空间。

    

    由于语言模型通常被部署为聊天机器人助手，模型在用户的母语中进行对话变得至关重要。尽管这些模型在训练时涵盖了多种语言，但对它们在韩语等低资源语言的熟练程度缺乏全面评估。在这项工作中，我们引入了KoDialogBench，该基准旨在评估语言模型在韩语对话中的会话能力。为此，我们从公共来源收集日常话题的韩语对话，或将其他语言的对话进行翻译。然后，我们将这些对话结构化为不同的测试数据集，涵盖了从对话理解到响应选择任务的广泛范围。利用提出的基准，我们进行了各种语言模型的广泛评估和分析，以衡量对韩语对话的基础理解。实验结果表明，存在明显的改进空间。

    arXiv:2402.17377v1 Announce Type: new  Abstract: As language models are often deployed as chatbot assistants, it becomes a virtue for models to engage in conversations in a user's first language. While these models are trained on a wide range of languages, a comprehensive evaluation of their proficiency in low-resource languages such as Korean has been lacking. In this work, we introduce KoDialogBench, a benchmark designed to assess language models' conversational capabilities in Korean. To this end, we collect native Korean dialogues on daily topics from public sources, or translate dialogues from other languages. We then structure these conversations into diverse test datasets, spanning from dialogue comprehension to response selection tasks. Leveraging the proposed benchmark, we conduct extensive evaluations and analyses of various language models to measure a foundational understanding of Korean dialogues. Experimental results indicate that there exists significant room for improve
    
[^45]: 早期中世纪希伯来诗歌隐喻检测数据集

    A Dataset for Metaphor Detection in Early Medieval Hebrew Poetry

    [https://arxiv.org/abs/2402.17371](https://arxiv.org/abs/2402.17371)

    这项研究提出了一个新的晚期古代和中世纪希伯来诗歌数据集，包含专家对隐喻的注释，旨在促进这一领域的进一步研究。

    

    在晚期古代和中世纪希伯来文本中存在大量的文本，它们代表着圣经希伯来语和现代希伯来语之间的重要语言和文化桥梁。诗歌在这些文本中占据重要地位，其主要特征之一就是频繁使用隐喻。区分比喻和字面语言使用是人文学者的一项重要任务，特别是在文学、语言学和解释学领域。本文介绍了一份新的具有专家注释的晚期古代和中世纪希伯来诗歌数据集，以及一些基准结果，希望能促进该领域的进一步研究。

    arXiv:2402.17371v1 Announce Type: new  Abstract: There is a large volume of late antique and medieval Hebrew texts. They represent a crucial linguistic and cultural bridge between Biblical and modern Hebrew. Poetry is prominent in these texts and one of its main haracteristics is the frequent use of metaphor. Distinguishing figurative and literal language use is a major task for scholars of the Humanities, especially in the fields of literature, linguistics, and hermeneutics. This paper presents a new, challenging dataset of late antique and medieval Hebrew poetry with expert annotations of metaphor, as well as some baseline results, which we hope will facilitate further research in this area.
    
[^46]: SoFA：通过优先规则跟随进行屏蔽式即时对齐

    SoFA: Shielded On-the-fly Alignment via Priority Rule Following

    [https://arxiv.org/abs/2402.17358](https://arxiv.org/abs/2402.17358)

    本文提出了一种新颖的对齐范式——优先规则跟随，在对话中将规则作为主要控制机制，以确保对齐并提出了PriorityDistill，实现了从LLM模拟中提取优先跟随信号的方法，确保规则的强大整合和遵守

    

    在大型语言模型（LLMs）中的对齐问题涉及将它们适应广泛的人类价值观。由于偏好的多样性和监管标准的挑战，这一要求使现有的对齐方法面临困难。本文介绍了一种新颖的对齐范式，即优先规则跟随，它将规则定义为每次对话中的主要控制机制，并将其优先于用户指令。我们的初步分析显示，即使像GPT-4这样先进的LLMs也存在理解和优先考虑规则的缺陷。因此，我们提出了PriorityDistill，一种从LLM模拟中萃取优先跟随信号的半自动方法，以确保规则的强大整合和遵守。我们的实验表明，这种方法不仅有效地通过仅使用一个通用规则来最小化不对齐问题，而且可以平滑地适应各种未见规则，确保它们免受劫持。

    arXiv:2402.17358v1 Announce Type: new  Abstract: The alignment problem in Large Language Models (LLMs) involves adapting them to the broad spectrum of human values. This requirement challenges existing alignment methods due to diversity of preferences and regulatory standards. This paper introduces a novel alignment paradigm, priority rule following, which defines rules as the primary control mechanism in each dialog, prioritizing them over user instructions. Our preliminary analysis reveals that even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules. Therefore, we present PriorityDistill, a semi-automated approach for distilling priority following signals from LLM simulations to ensure robust rule integration and adherence. Our experiments show that this method not only effectively minimizes misalignments utilizing only one general rule but also adapts smoothly to various unseen rules, ensuring they are shielded from hijacking and that 
    
[^47]: RECOST: 外部知识引导的数据高效指导调整

    RECOST: External Knowledge Guided Data-efficient Instruction Tuning

    [https://arxiv.org/abs/2402.17355](https://arxiv.org/abs/2402.17355)

    提出了RECOST框架，利用外部知识评估由大型语言模型合成的样本，通过相对预测熵解决数据高效指导调整中数据质量不足的挑战

    

    在当前大型语言模型（LLMs）的领域中，指导调整的过程是至关重要的一步。考虑到高昂的计算能力开销，提出了数据高效指导调整，以减少该过程中的训练数据量，旨在选择高质量的指示性数据。然而，我们认为目前大多数数据高效指导调整方法高度依赖于原指导调整数据集的质量。关于由LLMs合成的数据集，这个领域中常见的情况，脏样本甚至会比其他样本被更高概率地选择。为了解决这些挑战，我们利用外部知识（相关示例或段落）通过基于上下文的相对预测熵评估由LLMs合成的这些样本。基于新的度量标准，我们提出了一个名为 \textbf{RECOST} 的框架，该框架整合了外部知识。

    arXiv:2402.17355v1 Announce Type: new  Abstract: In the current landscape of large language models (LLMs), the process of instruction tuning serves as an essential step. Considering the high computing power overhead, data-efficient instruction tuning was proposed to reduce the training data size in this process, aiming at selecting high-quality instructional data. Nevertheless, we argue that most current data-efficient instruction-tuning methods are highly dependent on the quality of the original instruction-tuning dataset. When it comes to datasets synthesized by LLMs, a common scenario in this field, dirty samples will even be selected with a higher probability than other samples. To address these challenges, we utilized external knowledge (relevant examples or paragraphs) to evaluate those samples synthesized by LLMs with an in-context-based relative predictive entropy. Based on the new metric, we proposed a framework, dubbed as \textbf{RECOST}, which integrates external-knowledge-b
    
[^48]: 通过通用语料库实现无监督的多选题问答

    Unsupervised multiple choices question answering via universal corpus

    [https://arxiv.org/abs/2402.17333](https://arxiv.org/abs/2402.17333)

    本文提出了一种通过通用领域上下文生成合成MCQA数据的框架，无需依赖手动注释，实验表明方法的有效性。

    

    无监督问答是一项有前途但具有挑战性的任务，可以减轻在新领域构建大规模标注数据的负担。这激励我们研究无监督的多选题问答（MCQA）问题。本文提出了一个新颖的框架，旨在仅基于通用领域上下文生成合成的MCQA数据，而不依赖任何形式的手动注释。可能的答案被提取并用于生成相关问题，然后我们利用命名实体（NE）和知识图谱来发现合理的分散因素，形成完整的合成样本。对多个MCQA数据集上的实验证明了我们方法的有效性。

    arXiv:2402.17333v1 Announce Type: new  Abstract: Unsupervised question answering is a promising yet challenging task, which alleviates the burden of building large-scale annotated data in a new domain. It motivates us to study the unsupervised multiple-choice question answering (MCQA) problem. In this paper, we propose a novel framework designed to generate synthetic MCQA data barely based on contexts from the universal domain without relying on any form of manual annotation. Possible answers are extracted and used to produce related questions, then we leverage both named entities (NE) and knowledge graphs to discover plausible distractors to form complete synthetic samples. Experiments on multiple MCQA datasets demonstrate the effectiveness of our method.
    
[^49]: SKT5SciSumm - 一种用于多文档科学摘要的混合生成方法

    SKT5SciSumm - A Hybrid Generative Approach for Multi-Document Scientific Summarization

    [https://arxiv.org/abs/2402.17311](https://arxiv.org/abs/2402.17311)

    提出了一种名为SKT5SciSumm的混合框架，结合了基于引文信息的变换器和T5系列模型，在多文档科学摘要任务上取得了最先进的性能。

    

    arXiv:2402.17311v1 公告类型：新 摘要：科学文本摘要对于研究界和人类社会都显示出明显的益处。考虑到科学文本的特殊性以及多文档摘要任务的输入实质上很长，该任务需要足够的嵌入生成和文本截断，同时又不能丢失重要信息。为了解决这些问题，本文提出了SKT5SciSumm - 一种用于多文档科学摘要的混合框架（MDSS）。我们利用基于引文信息的变换器(SPECTER)的科学文献嵌入的句子-变换器版本来编码和表示文本句子，从而实现使用k-means聚类进行高效摘要提取。我们使用T5系列模型使用提取的句子生成抽象摘要。SKT5SciSumm在Multi-XScience数据集上实现了最先进的性能。通过大量实验和

    arXiv:2402.17311v1 Announce Type: new  Abstract: Summarization for scientific text has shown significant benefits both for the research community and human society. Given the fact that the nature of scientific text is distinctive and the input of the multi-document summarization task is substantially long, the task requires sufficient embedding generation and text truncation without losing important information. To tackle these issues, in this paper, we propose SKT5SciSumm - a hybrid framework for multi-document scientific summarization (MDSS). We leverage the Sentence-Transformer version of Scientific Paper Embeddings using Citation-Informed Transformers (SPECTER) to encode and represent textual sentences, allowing for efficient extractive summarization using k-means clustering. We employ the T5 family of models to generate abstractive summaries using extracted sentences. SKT5SciSumm achieves state-of-the-art performance on the Multi-XScience dataset. Through extensive experiments and
    
[^50]: 探究多模态大型语言模型对全局和局部语义表示的影响

    Probing Multimodal Large Language Models for Global and Local Semantic Representation

    [https://arxiv.org/abs/2402.17304](https://arxiv.org/abs/2402.17304)

    通过研究发现，多模态大型语言模型的中间层能够更好地编码全局语义信息，在视觉-语言任务中表现出更好的性能。顶层可能过多关注局部信息，导致理解全局信息的能力下降。

    

    大型语言模型的成功启发了研究人员将其优秀的表示能力转移到其他模态。最近的一些研究利用图像描述对齐数据集训练多模态大型语言模型（MLLMs），在图像到文本任务中取得了最新的性能表现。然而，很少有研究探讨MLLMs是否真正理解完整的图像信息，即全局信息，或者它们只能捕捉一些局部对象信息。本研究发现模型的中间层可以编码更多全局语义信息，其表示向量在视觉-语言蕴涵任务上表现更好，而不是顶层。我们通过目标检测任务进一步探究模型的局部语义表示。我们得出的结论是顶层可能过多专注于局部信息，导致减弱了对全局信息的理解能力。

    arXiv:2402.17304v1 Announce Type: cross  Abstract: The success of large language models has inspired researchers to transfer their exceptional representing ability to other modalities. Several recent works leverage image-caption alignment datasets to train multimodal large language models (MLLMs), which achieve state-of-the-art performance on image-to-text tasks. However, there are very few studies exploring whether MLLMs truly understand the complete image information, i.e., global information, or if they can only capture some local object information. In this study, we find that the intermediate layers of models can encode more global semantic information, whose representation vectors perform better on visual-language entailment tasks, rather than the topmost layers. We further probe models for local semantic representation through object detection tasks. And we draw a conclusion that the topmost layers may excessively focus on local information, leading to a diminished ability to en
    
[^51]: LLM能否生成与文化相关的常识性问答数据？印尼和巽他语的案例研究

    Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese

    [https://arxiv.org/abs/2402.17302](https://arxiv.org/abs/2402.17302)

    LLM使用在印尼语方面能够生成具有足够知识的问题，但在巽他语方面表现不佳，突显中低资源语言之间的性能差距。

    

    大型语言模型(LLMs)越来越多地被用于生成合成数据以训练和评估模型。然而，目前尚不清楚它们是否能够生成一个融入语言中知识和文化细微差别的高质量问答(QA)数据集，尤其是对于资源匮乏的语言。在这项研究中，我们调查了使用LLMs生成印尼语和巽他语文化相关常识性问答数据集的有效性。为此，我们使用包括LLMs和人类标注者在内的各种方法为这些语言创建数据集。我们的实验表明，目前性能最佳的LLM，GPT-4 Turbo，能够生成印尼语中具有足够知识的问题，但在巽他语中却不行，突出了中低资源语言之间的性能差距。我们还在我们生成的数据集上对各种LLMs进行基准测试，发现它们在......

    arXiv:2402.17302v1 Announce Type: new  Abstract: Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators. Our experiments show that the current best-performing LLM, GPT-4 Turbo, is capable of generating questions with adequate knowledge in Indonesian but not in Sundanese, highlighting the performance discrepancy between medium- and lower-resource languages. We also benchmark various LLMs on our generated datasets and find that they perform better on 
    
[^52]: 迷你集成低秩适配器用于参数高效微调

    Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning

    [https://arxiv.org/abs/2402.17263](https://arxiv.org/abs/2402.17263)

    提出了MELoRA，一种迷你集成低秩适配器，通过使用更少的可训练参数同时保持更高的秩，从而提供改进的性能潜力。

    

    参数高效微调（PEFT）是一种用于定制预训练大型语言模型（LLMs）的流行方法，尤其是在模型规模和任务多样性增加的情况下。低秩适应（LoRA）基于这样一个思想，即适应过程在本质上是低维的，即可以用相对较少的参数表示重要的模型变化。然而，与全参数微调相比，降低秩会遇到特定任务的泛化误差方面的挑战。我们提出了MELoRA，一种迷你集成低秩适配器，使用更少的可训练参数同时保持更高的秩，从而提供改进的性能潜力。其核心思想是冻结原始的预训练权重，并训练一组仅具有少量参数的迷你LoRA。这可以捕捉迷你LoRA之间的重要多样性程度，从而促进更好的泛化能力。

    arXiv:2402.17263v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring pre-trained large language models (LLMs), especially as the models' scale and the diversity of tasks increase. Low-rank adaptation (LoRA) is based on the idea that the adaptation process is intrinsically low-dimensional, i.e., significant model changes can be represented with relatively few parameters. However, decreasing the rank encounters challenges with generalization errors for specific tasks when compared to full-parameter fine-tuning. We present MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters while maintaining a higher rank, thereby offering improved performance potential. The core idea is to freeze original pretrained weights and train a group of mini LoRAs with only a small number of parameters. This can capture a significant degree of diversity among mini LoRAs, thus promoting better generalization ability. We conduct a theor
    
[^53]: 失言：多轮对话中大型语言模型的安全漏洞

    Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue

    [https://arxiv.org/abs/2402.17262](https://arxiv.org/abs/2402.17262)

    本论文探讨了多轮对话中大型语言模型的安全性漏洞，指出人类可以通过多轮对话诱使其生成有害信息。

    

    大型语言模型(LLMs)已被证明在面临"越狱"时会产生非法或不道德的回应。 "越狱"研究强调了LLMs的安全问题。然而，先前的研究主要集中在单轮对话上，忽视了多轮对话可能带来的复杂性和风险，这是人类从LLMs获取信息的关键方式。本文认为人类可以利用多轮对话诱使LLMs生成有害信息。LLMs可能不会拒绝警告性或边界不安全的查询，即使在多轮对话中每个回合都被服务于一个恶意目的。因此，通过将一个不安全查询分解为多个子查询用于多轮对话，我们逐渐诱使LLMs回答有害的子问题，最终导致总体有害响应。我们的实验跨越了广泛的范围。

    arXiv:2402.17262v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have been demonstrated to generate illegal or unethical responses, particularly when subjected to "jailbreak." Research on jailbreak has highlighted the safety issues of LLMs. However, prior studies have predominantly focused on single-turn dialogue, ignoring the potential complexities and risks presented by multi-turn dialogue, a crucial mode through which humans derive information from LLMs. In this paper, we argue that humans could exploit multi-turn dialogue to induce LLMs into generating harmful information. LLMs may not intend to reject cautionary or borderline unsafe queries, even if each turn is closely served for one malicious purpose in a multi-turn dialogue. Therefore, by decomposing an unsafe query into several sub-queries for multi-turn dialogue, we induced LLMs to answer harmful sub-questions incrementally, culminating in an overall harmful response. Our experiments, conducted across a wide ra
    
[^54]: 超越已知：研究LLMs在领域外意图检测上的表现

    Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection

    [https://arxiv.org/abs/2402.17256](https://arxiv.org/abs/2402.17256)

    本文综合评估了LLMs在各种实验设置下的表现，发现其展现出强大的零次和少次能力，但与完全资源微调的模型相比仍处于劣势。

    

    领域外（OOD）意图检测旨在检查用户的查询是否超出系统预定义的领域，这对于任务导向对话（TOD）系统的正常运行至关重要。先前的方法通过微调区分模型来解决这个问题。最近，一些研究探索了大型语言模型（LLMs）在各种下游任务中的应用，但它们在OOD检测任务上的能力仍不清楚。本文在各种实验设置下对LLM进行了全面评估，并概述了LLM的优势和劣势。我们发现LLMs表现出强大的零次和少次能力，但与完全资源微调的模型相比仍处于劣势。通过一系列附加分析实验，本文更深入地讨论和总结了LLMs面临的挑战，并为未来工作提供了指导。

    arXiv:2402.17256v1 Announce Type: new  Abstract: Out-of-domain (OOD) intent detection aims to examine whether the user's query falls outside the predefined domain of the system, which is crucial for the proper functioning of task-oriented dialogue (TOD) systems. Previous methods address it by fine-tuning discriminative models. Recently, some studies have been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, but it is still unclear for their ability on OOD detection task.This paper conducts a comprehensive evaluation of LLMs under various experimental settings, and then outline the strengths and weaknesses of LLMs. We find that LLMs exhibit strong zero-shot and few-shot capabilities, but is still at a disadvantage compared to models fine-tuned with full resource. More deeply, through a series of additional analysis experiments, we discuss and summarize the challenges faced by LLMs and provide guidance for future work including
    
[^55]: 使用多视图注意力的图像文本匹配

    Image-Text Matching with Multi-View Attention

    [https://arxiv.org/abs/2402.17237](https://arxiv.org/abs/2402.17237)

    本研究提出了一种使用多视图注意力的双流图像文本匹配方法，以解决单一表示难以全面覆盖复杂内容和缺乏交互的挑战。

    

    现有的用于图像文本匹配的双流模型在确保检索速度的同时表现出良好的性能，并受到工业界和学术界的广泛关注。这些方法使用单一表示来分别编码图像和文本，并使用余弦相似度或向量内积得到匹配分数。然而，双流模型的性能往往不太理想。一方面，单一表示难以全面覆盖复杂内容。另一方面，在这种缺乏交互的框架中，匹配多重含义是具有挑战性的，这导致信息被忽略。为了解决上述问题并促进双流模型的性能，我们提出了一种双流图像文本匹配的多视图注意力方法MVAM（多视图注意力模型）。

    arXiv:2402.17237v1 Announce Type: cross  Abstract: Existing two-stream models for image-text matching show good performance while ensuring retrieval speed and have received extensive attention from industry and academia. These methods use a single representation to encode image and text separately and get a matching score with cosine similarity or the inner product of vectors. However, the performance of the two-stream model is often sub-optimal. On the one hand, a single representation is challenging to cover complex content comprehensively. On the other hand, in this framework of lack of interaction, it is challenging to match multiple meanings which leads to information being ignored. To address the problems mentioned above and facilitate the performance of the two-stream model, we propose a multi-view attention approach for two-stream image-text matching MVAM (\textbf{M}ulti-\textbf{V}iew \textbf{A}ttention \textbf{M}odel). It first learns multiple image and text representations by
    
[^56]: MATHSENSEI：用于数学推理的工具增强型大型语言模型

    MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning

    [https://arxiv.org/abs/2402.17231](https://arxiv.org/abs/2402.17231)

    MATHSENSEI 是一种用于数学推理的工具增强型大型语言模型，通过添加知识检索、程序执行和符号方程求解工具，提高了数学推理能力。

    

    工具增强型大型语言模型(TALM)已知能够增强大型语言模型(LLM)的技能，从而提高它们在许多任务上的推理能力。本文提出了一种名为MATHSENSEI的工具增强型大型语言模型，用于数学推理。通过添加用于知识检索（Bing Web Search）、程序执行（Python）和符号方程求解（Wolfram-Alpha）的工具，我们通过对数学推理数据集进行评估来研究这些工具的互补优势。

    arXiv:2402.17231v1 Announce Type: new  Abstract: Tool-augmented Large Language Models (TALM) are known to enhance the skillset of large language models (LLM), thereby, leading to their improved reasoning abilities across many tasks. While, TALMs have been successfully employed in different question-answering benchmarks, their efficacy on complex mathematical reasoning benchmarks, and the potential complimentary benefits offered by tools for knowledge retrieval and mathematical equation solving, are open research questions. In this work, we present MATHSENSEI, a tool-augmented large language model for mathematical reasoning. Augmented with tools for knowledge retrieval (Bing Web Search), program execution (Python), and symbolic equation solving (Wolfram-Alpha), we study the complimentary benefits of these tools through evaluations on mathematical reasoning datasets. We perform exhaustive ablations on MATH,a popular dataset for evaluating mathematical reasoning on diverse mathematical di
    
[^57]: 对话推理：通过对话模拟解决大型语言模型中的主观任务

    Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models

    [https://arxiv.org/abs/2402.17226](https://arxiv.org/abs/2402.17226)

    提出了RiC（对话推理）方法，旨在通过对话模拟解决大型语言模型中的主观任务，通过挖掘有用的上下文信息来填补客观推理方式的不足

    

    大型语言模型（LLMs）在客观任务中取得了显著的表现，比如开放域问答和数学推理，这些任务通常可以通过回忆学到的事实知识或思维链路式推理来解决。然而，我们发现LLMs在主观任务中的表现仍然令人不满，比如隐喻识别、黑暗幽默检测等。与客观任务相比，主观任务更注重解释或情感反应，而不是一个普遍接受的推理路径。基于任务特性和LLMs强大的对话生成能力，我们提出了RiC（对话推理），这是一种通过对话模拟解决主观任务的方法。RiC的动机是通过模拟对话来挖掘有用的上下文信息，而不是提供思维链路式的理由，从而提供潜在的

    arXiv:2402.17226v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved remarkable performance in objective tasks such as open-domain question answering and mathematical reasoning, which can often be solved through recalling learned factual knowledge or chain-of-thought style reasoning. However, we find that the performance of LLMs in subjective tasks is still unsatisfactory, such as metaphor recognition, dark humor detection, etc. Compared to objective tasks, subjective tasks focus more on interpretation or emotional response rather than a universally accepted reasoning pathway. Based on the characteristics of the tasks and the strong dialogue-generation capabilities of LLMs, we propose RiC (Reasoning in Conversation), a method that focuses on solving subjective tasks through dialogue simulation. The motivation of RiC is to mine useful contextual information by simulating dialogues instead of supplying chain-of-thought style rationales, thereby offering potential u
    
[^58]: 测量神经模型的视觉语言STEM技能

    Measuring Vision-Language STEM Skills of Neural Models

    [https://arxiv.org/abs/2402.17205](https://arxiv.org/abs/2402.17205)

    该研究引入了一个新挑战，用于测试神经模型的STEM技能，提出了一个包含大量基础技能和问题的数据集，需要理解STEM的多模式视觉语言信息，并展示了最新模型对于低年级技能的有限掌握。

    

    我们引入了一个新挑战，用于测试神经模型的STEM技能。现实世界中的问题通常需要结合STEM（科学、技术、工程和数学）知识来解决。与现有数据集不同，我们的数据集需要理解STEM的多模式视觉语言信息。我们的数据集是挑战性问题中最大、最全面的数据集之一。它包括448项技能和1,073,146个跨越所有STEM科目的问题。与通常侧重于检验专家水平能力的现有数据集不同，我们的数据集包括基础技能和根据K-12课程设计的问题。我们还将最先进的基础模型，如CLIP和GPT-3.5-Turbo，添加到我们的基准中。结果显示，最近的模型进展只有助于掌握数据集中非常有限数量的低年级技能（三年级中的2.5%）。事实上，这些模型仍远没有完全掌握学前教育阶段的技能。

    arXiv:2402.17205v1 Announce Type: cross  Abstract: We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well bel
    
[^59]: 当扩展遇到LLM微调: 数据、模型和微调方法的影响

    When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method

    [https://arxiv.org/abs/2402.17193](https://arxiv.org/abs/2402.17193)

    研究了不同扩展因素如何影响大型语言模型微调性能，认为LLM微调遵循着一种特殊的扩展行为。

    

    大型语言模型（LLMs）通常采用微调来释放其在下游应用中的能力，但我们对不同微调方法的归纳偏差（特别是扩展属性）的理解仍然有限。为了填补这一空白，我们进行了系统实验，研究不同扩展因素（包括LLM模型大小、预训练数据大小、新微调参数大小和微调数据大小）如何影响微调性能。我们考虑了两种微调类型--全模型调整（FMT）和参数高效微调（PET，包括提示调整和LoRA），并探索它们在数据有限的情况下的扩展行为，其中LLM模型大小远远超过微调数据大小。基于1B到16B的两组预训练双语LLMs和对双语机器翻译和多语言摘要基准的实验，我们发现LLM微调遵循

    arXiv:2402.17193v1 Announce Type: new  Abstract: While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning -- full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follo
    
[^60]: 一种有效的混合专家方法，利用编码器解缠来进行代码切换语音识别

    An Effective Mixture-Of-Experts Approach For Code-Switching Speech Recognition Leveraging Encoder Disentanglement

    [https://arxiv.org/abs/2402.17189](https://arxiv.org/abs/2402.17189)

    通过引入解缠损失并改进声学编码器，本文提出的方法在处理代码切换现象时表现出色，显著优于先前的方法。

    

    随着端到端（E2E）神经网络的大规模发展，近年来自动语音识别（ASR）取得了前所未有的突破。然而，代码切换现象仍然是阻碍ASR完美的主要障碍，因为缺乏标记数据和语言之间的变化经常导致ASR性能下降。在本文中，我们专注于改进E2E ASR的声学编码器，以应对代码切换现象带来的挑战。我们的主要贡献有三个：首先，我们引入了一种新颖的解缠损失，使得编码器的较低层能够捕获跨语言的声学信息，同时在编码器的较高层减轻语言混淆。其次，通过全面的实验，我们验证了我们的方法优于使用预训练双编码器的现有方法，同时只访问代码切换

    arXiv:2402.17189v1 Announce Type: cross  Abstract: With the massive developments of end-to-end (E2E) neural networks, recent years have witnessed unprecedented breakthroughs in automatic speech recognition (ASR). However, the codeswitching phenomenon remains a major obstacle that hinders ASR from perfection, as the lack of labeled data and the variations between languages often lead to degradation of ASR performance. In this paper, we focus exclusively on improving the acoustic encoder of E2E ASR to tackle the challenge caused by the codeswitching phenomenon. Our main contributions are threefold: First, we introduce a novel disentanglement loss to enable the lower-layer of the encoder to capture inter-lingual acoustic information while mitigating linguistic confusion at the higher-layer of the encoder. Second, through comprehensive experiments, we verify that our proposed method outperforms the prior-art methods using pretrained dual-encoders, meanwhile having access only to the codesw
    
[^61]: 极端编码器输出帧率降低：提高大型端到端模型的计算延迟

    Extreme Encoder Output Frame Rate Reduction: Improving Computational Latencies of Large End-to-End Models

    [https://arxiv.org/abs/2402.17184](https://arxiv.org/abs/2402.17184)

    通过在编码器中应用多个漏斗降低层，实现了极大程度的输出帧率降低，为大型端到端模型提高了计算效率

    

    自动语音识别（ASR）端到端（E2E）模型的准确性随着规模扩大而持续提高，一些模型现在已经达到了数十亿个参数。然而，这些模型的广泛部署和采用需要计算效率高的解码策略。在本研究中，我们研究了一种这样的策略：在编码器中应用多个帧降低层，将编码器输出压缩成少量的输出帧。尽管类似的技术在先前的工作中已经得到研究，但我们通过使用多个漏斗降低层实现了比先前展示的更大幅度的减少。通过消融实验，我们研究了编码器中各种架构选择的影响，以确定最有效的策略。我们证明我们可以在每2.56秒的输入语音中生成一个编码器输出帧，而不会显著影响大型端到端模型上的词错误率。

    arXiv:2402.17184v1 Announce Type: new  Abstract: The accuracy of end-to-end (E2E) automatic speech recognition (ASR) models continues to improve as they are scaled to larger sizes, with some now reaching billions of parameters. Widespread deployment and adoption of these models, however, requires computationally efficient strategies for decoding. In the present work, we study one such strategy: applying multiple frame reduction layers in the encoder to compress encoder outputs into a small number of output frames. While similar techniques have been investigated in previous work, we achieve dramatically more reduction than has previously been demonstrated through the use of multiple funnel reduction layers. Through ablations, we study the impact of various architectural choices in the encoder to identify the most effective strategies. We demonstrate that we can generate one encoder output frame for every 2.56 sec of input speech, without significantly affecting word error rate on a larg
    
[^62]: 数据科学代理基准测试

    Benchmarking Data Science Agents

    [https://arxiv.org/abs/2402.17168](https://arxiv.org/abs/2402.17168)

    本文引入了DSEval评估范式和一系列创新基准，用于评估数据科学代理在整个数据科学生命周期中的性能，通过引入自举注释方法简化数据集准备流程，改进评估覆盖范围，扩展基准测试的全面性，揭示了普遍存在的障碍并提供了关键见解

    

    在数据驱动决策的时代，数据分析的复杂性需要数据科学的高级专业知识和工具，这对专家来说也带来了重大挑战。大型语言模型(LLM)作为数据科学代理，已经成为协助人类进行数据分析和处理的有希望的辅助工具。然而，它们的实际有效性仍受限于现实应用的多样需求和复杂的分析过程。在本文中，我们介绍了DSEval--一种新颖的评估范式，以及一系列针对整个数据科学生命周期的代理性能评估的创新基准。通过引入一种新颖的自举注释方法，我们简化了数据集准备流程，改进了评估覆盖范围，并扩展了基准测试的全面性。我们的研究发现揭示了普遍存在的障碍，并提供了关键见解，以指导未来在这一领域的进展。

    arXiv:2402.17168v1 Announce Type: new  Abstract: In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval -- a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.
    
[^63]: 对文档部分进行聚类：从文档中检测和描述影响力活动

    Clustering Document Parts: Detecting and Characterizing Influence Campaigns From Documents

    [https://arxiv.org/abs/2402.17151](https://arxiv.org/abs/2402.17151)

    提出了一种新颖的聚类流程，用于检测和描述影响力活动，通过多种技术增强流程性能，并在文档级别分类上取得了优于传统方法的预测效果

    

    我们提出了一种新颖的聚类流程，用于从文档中检测和描述影响力活动。该方法对文档的部分进行聚类，检测可能反映影响力活动的聚类，然后通过它们与高影响力聚类的关联来识别与影响力活动相关的文档。我们的方法在预测文档是否属于影响力活动方面表现优于直接文档级别分类和直接文档级别聚类方法。我们提出了各种新颖技术来增强我们的流程，包括使用现有的事件事实预测系统获取文档部分，并聚合多个聚类实验以提高聚类和文档分类的性能。在聚类的基础上对文档进行分类不仅可以准确提取与影响力活动相关的文档部分，还可以捕捉到影响活动

    arXiv:2402.17151v1 Announce Type: new  Abstract: We propose a novel clustering pipeline to detect and characterize influence campaigns from documents. This approach clusters parts of document, detects clusters that likely reflect an influence campaign, and then identifies documents linked to an influence campaign via their association with the high-influence clusters. Our approach outperforms both the direct document-level classification and the direct document-level clustering approach in predicting if a document is part of an influence campaign. We propose various novel techniques to enhance our pipeline, including using an existing event factuality prediction system to obtain document parts, and aggregating multiple clustering experiments to improve the performance of both cluster and document classification. Classifying documents on the top of clustering not only accurately extracts the parts of the documents that are relevant to influence campaigns, but also capture influence camp
    
[^64]: Fact-and-Reflection（FaR）改善大型语言模型的置信校准

    Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models

    [https://arxiv.org/abs/2402.17124](https://arxiv.org/abs/2402.17124)

    提出了Fact-and-Reflection（FaR）提示策略，通过引入已知“事实”并要求模型“反思”，在两个步骤中改进了大型语言模型的置信校准

    

    要使LLM值得信赖，其置信水平应与实际表现良好校准。尽管现在普遍认为LLM的表现在很大程度上受到提示的影响，但提示LLM中的置信校准尚未得到彻底探讨。本文探讨了不同提示策略如何影响LLM的置信校准以及如何改进。我们在问答环境中对六种提示方法进行了大量实验，我们观察到，尽管这些方法有助于改进LLM的预期校准，但也会导致LLM在响应某些实例时过于自信。受人类认知启发，我们提出了Fact-and-Reflection（FaR）提示，它通过两个步骤改善了LLM的校准。首先，FaR从LLM中获取与输入提示相关的已知“事实”。然后要求模型“反思”它们以生成最终答案。

    arXiv:2402.17124v1 Announce Type: new  Abstract: For a LLM to be trustworthy, its confidence level should be well-calibrated with its actual performance. While it is now common sense that LLM performances are greatly impacted by prompts, the confidence calibration in prompting LLMs has yet to be thoroughly explored. In this paper, we explore how different prompting strategies influence LLM confidence calibration and how it could be improved. We conduct extensive experiments on six prompting methods in the question-answering context and we observe that, while these methods help improve the expected LLM calibration, they also trigger LLMs to be over-confident when responding to some instances. Inspired by human cognition, we propose Fact-and-Reflection (FaR) prompting, which improves the LLM calibration in two steps. First, FaR elicits the known "facts" that are relevant to the input prompt from the LLM. And then it asks the model to "reflect" over them to generate the final answer. Expe
    
[^65]: 创造令人紧张的故事: 与大型语言模型的迭代规划

    Creating Suspenseful Stories: Iterative Planning with Large Language Models

    [https://arxiv.org/abs/2402.17119](https://arxiv.org/abs/2402.17119)

    提出了一种基于迭代提示的规划方法，首次尝试使用大型语言模型生成令人紧张的故事，并通过广泛的人类评估验证了该方法的有效性。

    

    自动故事生成一直是自然语言处理中的一项长期挑战。在所有故事维度中，悬念在人类写的故事中非常普遍，但在人工智能生成的故事中相对不太被探索。尽管大型语言模型(LLMs)的最新进展大大推动了语言生成的发展，但在令人紧张的故事生成方面，当前最先进的LLMs在可靠性方面仍存在问题。我们提出了一种基于迭代提示的规划方法，该方法扎根于认知心理学和叙事学中关于故事悬念的两个理论基础。这种基于理论的方法完全零样本地工作，不依赖于任何监督式故事语料库。据我们所知，这篇论文是首次尝试使用LLMs生成令人紧张的故事。对生成的悬念故事进行的广泛人类评估表明了我们方法的有效性。

    arXiv:2402.17119v1 Announce Type: new  Abstract: Automated story generation has been one of the long-standing challenges in NLP. Among all dimensions of stories, suspense is very common in human-written stories but relatively under-explored in AI-generated stories. While recent advances in large language models (LLMs) have greatly promoted language generation in general, state-of-the-art LLMs are still unreliable when it comes to suspenseful story generation. We propose a novel iterative-prompting-based planning method that is grounded in two theoretical foundations of story suspense from cognitive psychology and narratology. This theory-grounded method works in a fully zero-shot manner and does not rely on any supervised story corpora. To the best of our knowledge, this paper is the first attempt at suspenseful story generation with LLMs. Extensive human evaluations of the generated suspenseful stories demonstrate the effectiveness of our method.
    
[^66]: 修复: 在说明后修正LLM响应中的事实错误

    Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses

    [https://arxiv.org/abs/2402.17097](https://arxiv.org/abs/2402.17097)

    提出了一种名为Re-Ex的方法，通过引入事实错误说明步骤来修正LLM生成文本中的事实错误，并提出了新的提示技术来减少所需的标记数量和挂钟时间

    

    缓解幻觉问题是LLM的主要挑战之一，我们需要克服这一挑战，以便可靠地在现实场景中使用它们。最近，提出了各种方法来检查LLM生成的文本中的事实错误，并相应地进行修订，以减少幻觉问题。在本文中，我们提出了Re-Ex，一种修订LLM生成文本的方法，它引入了一个称为事实错误说明步骤的新步骤。 Re-Ex使用3个步骤对LLM的初始响应进行修订：首先，使用外部工具获取响应中事实错误的证据；第二，要求LLM根据第一步中收集的证据解释响应中的问题部分；最后，LLM使用在第二步中获得的解释对响应进行修订。除了说明步骤，我们还提出了新的提示技术，以减少所需的标记数量和挂钟时间。

    arXiv:2402.17097v1 Announce Type: cross  Abstract: Mitigating hallucination issues is one of the main challenges of LLMs we need to overcome, in order to reliably use them in real-world scenarios. Recently, various methods are proposed to check the factual errors in the LLM-generated texts and revise them accordingly, to reduce the hallucination issue. In this paper, we propose Re-Ex, a method of revising LLM-generated texts, which introduces a novel step dubbed as the factual error explanation step. Re-Ex revises the initial response of LLMs using 3-steps: first, external tools are used to get the evidences on the factual errors in the response; second, LLMs are instructed to explain the problematic parts of the response based on the evidences gathered in the first step; finally, LLMs revise the response using the explanation obtained in the second step. In addition to the explanation step, we propose new prompting techniques to reduce the amount of tokens and wall-clock time required
    
[^67]: 利用大型语言模型通过讲故事学习复杂法律概念

    Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling

    [https://arxiv.org/abs/2402.17019](https://arxiv.org/abs/2402.17019)

    通过大型语言模型和故事讲述，本论文提出了一种新颖的法律教育方法，帮助非专业人士学习复杂的法律概念，并构建了一个包含法律故事和多项选择题的数据集。

    

    将法律知识变得更容易理解对于提升普通法律素养和鼓励公民参与民主至关重要。然而，对于没有法律背景的人来说，法律文件通常难以理解。本文提出了一种新颖的大型语言模型（LLMs）在法律教育中的应用，帮助非专业人士通过讲故事学习复杂的法律概念，讲故事是传达复杂和抽象概念的有效教学工具。我们还介绍了一个名为LegalStories的新数据集，其中包含295个复杂的法律原则，每个原则都附有一个故事和一组由LLMs生成的多项选择题。为了构建数据集，我们尝试使用各种LLMs生成解释这些概念的法律故事。此外，我们使用专家参与的方法来迭代设计多项选择题。然后，我们通过一

    arXiv:2402.17019v1 Announce Type: new  Abstract: Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 295 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop method to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through an 
    
[^68]: 多任务对比学习用于8192标记的双语文本嵌入

    Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings

    [https://arxiv.org/abs/2402.17016](https://arxiv.org/abs/2402.17016)

    通过引入独特的多任务学习目标，研究者设计了用于支持英语和其他目标语言的最先进的双语文本嵌入模型，显著提高了模型在STS任务上的表现，同时在目标语言理解和跨语言评估任务中超越了现有多语言模型。

    

    我们引入了一套新颖的最先进的双语文本嵌入模型，旨在支持英语和另一种目标语言。这些模型能够处理长达8192个标记的文本输入，因此非常适用于一系列自然语言处理任务，如文本检索、聚类和语义文本相似度（STS）计算。通过专注于双语模型并引入独特的多任务学习目标，我们显著改善了在STS任务上的模型表现，超过了现有多语言模型在目标语言理解和跨语言评估任务方面的能力。此外，我们的双语模型更加高效，需要较少的参数和更少的内存，因为它们需要较小的词汇量。此外，我们还扩展了大规模文本嵌入基准（MTEB），包括德语和西班牙语嵌入的基准。

    arXiv:2402.17016v1 Announce Type: cross  Abstract: We introduce a novel suite of state-of-the-art bilingual text embedding models that are designed to support English and another target language. These models are capable of processing lengthy text inputs with up to 8192 tokens, making them highly versatile for a range of natural language processing tasks such as text retrieval, clustering, and semantic textual similarity (STS) calculations.   By focusing on bilingual models and introducing a unique multi-task learning objective, we have significantly improved the model performance on STS tasks, which outperforms the capabilities of existing multilingual models in both target language understanding and cross-lingual evaluation tasks. Moreover, our bilingual models are more efficient, requiring fewer parameters and less memory due to their smaller vocabulary needs. Furthermore, we have expanded the Massive Text Embedding Benchmark (MTEB) to include benchmarks for German and Spanish embed
    
[^69]: Z-AGI实验室参与ClimateActivism 2024：社交媒体上立场和仇恨事件的检测

    Z-AGI Labs at ClimateActivism 2024: Stance and Hate Event Detection on Social Media

    [https://arxiv.org/abs/2402.17014](https://arxiv.org/abs/2402.17014)

    研究旨在在社交媒体上帮助气候活动人士识别和应对仇恨言论，团队评估了多种模型，在仇恨言论检测、仇恨言论目标识别和立场检测方面取得了有趣的结果。

    

    在数字领域，丰富的数据是洞察社会、政治和经济格局复杂性的关键信息来源。针对对事件高质量信息的日益增长需求以及打击仇恨言论的必要性，本研究发起了在CASE 2024举办的Climate Activism立场和仇恨事件检测共享任务的建立。聚焦气候活动人士在社交媒体上应对仇恨言论的情况，我们的研究为从Twitter推文中识别仇恨言论做出了贡献。通过分析三个子任务-仇恨言论检测（子任务A）、仇恨言论目标识别（子任务B）和立场检测（子任务C）- Z-AGI实验室团队评估了基于Tf-Idf的各种模型，包括LSTM、Xgboost和LGBM。结果显示出有趣的变化，Catboost在子任务B（F1：0.5604）和子任务C（F1：0.7081）中表现出色，而LGBM成为子任务A（F1：0.8684）中表现最佳的模型。

    arXiv:2402.17014v1 Announce Type: new  Abstract: In the digital realm, rich data serves as a crucial source of insights into the complexities of social, political, and economic landscapes. Addressing the growing need for high-quality information on events and the imperative to combat hate speech, this research led to the establishment of the Shared Task on Climate Activism Stance and Hate Event Detection at CASE 2024. Focused on climate activists contending with hate speech on social media, our study contributes to hate speech identification from tweets. Analyzing three sub-tasks - Hate Speech Detection (Sub-task A), Targets of Hate Speech Identification (Sub-task B), and Stance Detection (Sub-task C) - Team Z-AGI Labs evaluated various models, including LSTM, Xgboost, and LGBM based on Tf-Idf. Results unveiled intriguing variations, with Catboost excelling in Subtask-B (F1: 0.5604) and Subtask-C (F1: 0.7081), while LGBM emerged as the top-performing model for Subtask-A (F1: 0.8684). T
    
[^70]: 在瑞士司法预测中实现可解释性和公平性：在一个多语言数据集上进行基准测试

    Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset

    [https://arxiv.org/abs/2402.17013](https://arxiv.org/abs/2402.17013)

    本研究深入探讨了在瑞士司法预测中实现可解释性和公平性的重要性，利用了唯一可用的多语言LJP数据集，并对最新的单语和多语BERT-based LJP模型进行了可解释性能评估。

    

    arXiv:2402.17013v1 公告类型：跨领域 摘要：对法律裁决预测（LJP）系统中的可解释性进行评估在构建值得信赖和透明系统方面至关重要，特别是考虑到这些系统依赖可能缺乏法律相关性或涉及敏感属性的因素。本研究深入探讨了LJP模型中可解释性和公平性的领域，利用瑞士裁决预测（SJP）这一唯一可用的多语言LJP数据集。我们整理了一个包括108个案例的支持和反对法律专家裁决的理由的全面收集，在德语、法语和意大利语中提供。通过采用基于遮挡的可解释性方法，我们评估了最新的单语和多语BERT-based LJP模型的可解释性表现，以及利用数据增强和跨语言转移等技术开发的模型，这些模型展示了预测性能的提高。值得注意的是，我们的发现

    arXiv:2402.17013v1 Announce Type: cross  Abstract: The assessment of explainability in Legal Judgement Prediction (LJP) systems is of paramount importance in building trustworthy and transparent systems, particularly considering the reliance of these systems on factors that may lack legal relevance or involve sensitive attributes. This study delves into the realm of explainability and fairness in LJP models, utilizing Swiss Judgement Prediction (SJP), the only available multilingual LJP dataset. We curate a comprehensive collection of rationales that `support' and `oppose' judgement from legal experts for 108 cases in German, French, and Italian. By employing an occlusion-based explainability approach, we evaluate the explainability performance of state-of-the-art monolingual and multilingual BERT-based LJP models, as well as models developed with techniques such as data augmentation and cross-lingual transfer, which demonstrated prediction performance improvement. Notably, our finding
    
[^71]: DiffuCOMET: 上下文常识知识扩散

    DiffuCOMET: Contextual Commonsense Knowledge Diffusion

    [https://arxiv.org/abs/2402.17011](https://arxiv.org/abs/2402.17011)

    DiffuCOMET是一种利用扩散学习来重构叙述上下文与相关常识知识之间语义连接的知识模型，生成的知识在常识多样性、上下文相关性和对已知参考文献的对齐方面达到更好的平衡。

    

    推理上下文相关且多样化的常识以理解叙述故事对于知识模型仍然具有挑战性。在这项工作中，我们开发了一系列利用扩散的知识模型DiffuCOMET，以学习重构叙述上下文与相关常识知识之间的隐式语义连接。通过多次扩散步骤，我们的方法逐步完善了与叙述锚定的常识事实表示，为输入上下文生成上下文相关且多样化的常识推断。为了评估DiffuCOMET，我们引入了衡量常识推断的新指标，更密切地衡量知识多样性和上下文相关性。我们在两个不同的基准数据集，ComFact和WebNLG+上的结果显示，DiffuCOMET生成的知识在常识多样性、上下文相关性以及与已知黄金参考文献的对齐之间实现了更好的权衡，与基线方法相比。

    arXiv:2402.17011v1 Announce Type: new  Abstract: Inferring contextually-relevant and diverse commonsense to understand narratives remains challenging for knowledge models. In this work, we develop a series of knowledge models, DiffuCOMET, that leverage diffusion to learn to reconstruct the implicit semantic connections between narrative contexts and relevant commonsense knowledge. Across multiple diffusion steps, our method progressively refines a representation of commonsense facts that is anchored to a narrative, producing contextually-relevant and diverse commonsense inferences for an input context. To evaluate DiffuCOMET, we introduce new metrics for commonsense inference that more closely measure knowledge diversity and contextual relevance. Our results on two different benchmarks, ComFact and WebNLG+, show that knowledge generated by DiffuCOMET achieves a better trade-off between commonsense diversity, contextual relevance and alignment to known gold references, compared to basel
    
[^72]: 大型语言模型能像人类一样回忆参考位置吗？

    Can Large Language Models Recall Reference Location Like Humans?

    [https://arxiv.org/abs/2402.17010](https://arxiv.org/abs/2402.17010)

    本文探讨了大型语言模型如何利用预训练阶段的知识回忆参考段落，提出了一个两阶段框架模拟人类回忆参考的过程。

    

    在完成知识密集型任务时，人类有时不仅需要一个答案，还需要相应的参考段落供辅助阅读。先前的方法需要通过额外的检索模型获取预分段的文章块。本文探讨了利用大型语言模型（LLMs）的预训练阶段存储的参数化知识，独立于任何起始位置回忆参考段落。我们提出了一个模拟人类回忆易被遗忘参考的情景的两阶段框架。首先，LLM被提示回忆文档标题标识符以获取粗粒度文档集。然后，基于获得的粗粒度文档集，它回忆细粒度段落。在两阶段回忆过程中，我们使用约束解码来确保不生成存储文档之外的内容。为了增加速度，我们只回忆短前缀。

    arXiv:2402.17010v1 Announce Type: cross  Abstract: When completing knowledge-intensive tasks, humans sometimes need not just an answer but also a corresponding reference passage for auxiliary reading. Previous methods required obtaining pre-segmented article chunks through additional retrieval models. This paper explores leveraging the parameterized knowledge stored during the pre-training phase of large language models (LLMs) to independently recall reference passage from any starting position. We propose a two-stage framework that simulates the scenario of humans recalling easily forgotten references. Initially, the LLM is prompted to recall document title identifiers to obtain a coarse-grained document set. Then, based on the acquired coarse-grained document set, it recalls fine-grained passage. In the two-stage recall process, we use constrained decoding to ensure that content outside of the stored documents is not generated. To increase speed, we only recall a short prefix in the 
    
[^73]: 在语义重叠摘要任务上对LLMs进行基准测试

    Benchmarking LLMs on the Semantic Overlap Summarization Task

    [https://arxiv.org/abs/2402.17008](https://arxiv.org/abs/2402.17008)

    该论文对LLMs在Semantic Overlap Summarization任务上进行基准测试，使用TELeR分类法评估了15个流行的LLMs的性能，以评估它们总结多个不同叙述之间重叠信息的能力。

    

    Semantic Overlap Summarization (SOS)是一项受限的多文档摘要任务，其中约束是捕获两个不同叙述之间的共同/重叠信息。虽然最近大型语言模型（LLMs）在许多摘要任务中取得了优越的性能，但尚未进行过使用LLMs进行SOS任务的基准测试研究。由于LLMs的响应对提示设计中的细微变化很敏感，进行这样的基准测试研究的主要挑战是在得出可靠结论之前系统地探索各种提示。幸运的是，最近提出了TELeR分类法，可用于设计和探索LLMs的各种提示。利用这个TELeR分类法和15个流行的LLMs，本文全面评估了LLMs在SOS任务上的表现，评估它们从多个不同叙述中总结重叠信息的能力。

    arXiv:2402.17008v1 Announce Type: new  Abstract: Semantic Overlap Summarization (SOS) is a constrained multi-document summarization task, where the constraint is to capture the common/overlapping information between two alternative narratives. While recent advancements in Large Language Models (LLMs) have achieved superior performance in numerous summarization tasks, a benchmarking study of the SOS task using LLMs is yet to be performed. As LLMs' responses are sensitive to slight variations in prompt design, a major challenge in conducting such a benchmarking study is to systematically explore a variety of prompts before drawing a reliable conclusion. Fortunately, very recently, the TELeR taxonomy has been proposed which can be used to design and explore various prompts for LLMs. Using this TELeR taxonomy and 15 popular LLMs, this paper comprehensively evaluates LLMs on the SOS Task, assessing their ability to summarize overlapping information from multiple alternative narratives. For 
    
[^74]: 语言模型听到了什么？探究语言模型中的听觉表征

    What Do Language Models Hear? Probing for Auditory Representations in Language Models

    [https://arxiv.org/abs/2402.16998](https://arxiv.org/abs/2402.16998)

    通过训练一个线性探针，将语言模型中的文本表示和预训练音频模型中的声音表示联系在一起，研究发现尽管仅在原始文本上进行训练，语言模型对于一些对象的声音知识有着基于实质的编码。

    

    这项工作探讨了语言模型是否对物体的声音具有含义深刻且基于实质的表征。我们学习了一个线性探针，通过一个预训练的音频模型给出一个对象的声音表示，从而在给定与该对象相关的音频片段的情况下检索出该对象的正确文本表示。这个探针是通过对比损失进行训练的，推动对象的语言表示和声音表示彼此接近。在训练之后，我们测试了探针对于一些在训练中没有见过的对象的泛化能力。在不同的语言模型和音频模型中，我们发现在许多情况下探针的泛化能力超过了随机猜测的水平，这表明尽管仅在原始文本上进行训练，语言模型对于一些对象的声音知识具有基于实质的编码。

    arXiv:2402.16998v1 Announce Type: cross  Abstract: This work explores whether language models encode meaningfully grounded representations of sounds of objects. We learn a linear probe that retrieves the correct text representation of an object given a snippet of audio related to that object, where the sound representation is given by a pretrained audio model. This probe is trained via a contrastive loss that pushes the language representations and sound representations of an object to be close to one another. After training, the probe is tested on its ability to generalize to objects that were not seen during training. Across different language models and audio models, we find that the probe generalization is above chance in many cases, indicating that despite being trained only on raw text, language models encode grounded knowledge of sounds for some objects.
    
[^75]: 长对话摘要: 一项分析

    Long Dialog Summarization: An Analysis

    [https://arxiv.org/abs/2402.16986](https://arxiv.org/abs/2402.16986)

    长对话摘要的研究强调了在各种应用中为有效沟通创造连贯和上下文丰富摘要的重要性。

    

    Dialog summarization在跨越不同领域的大规模对话中变得越来越重要。这项任务在捕捉多轮长对话的关键点、上下文和细微差别方面面临着独特挑战。本研究强调了在各种应用中为有效沟通创造连贯和上下文丰富摘要的重要性。我们探讨了不同领域长对话摘要的当前最先进方法，基于基准指标的评估显示一个单一模型在各种场景下的表现并不理想。

    arXiv:2402.16986v1 Announce Type: new  Abstract: Dialog summarization has become increasingly important in managing and comprehending large-scale conversations across various domains. This task presents unique challenges in capturing the key points, context, and nuances of multi-turn long conversations for summarization. It is worth noting that the summarization techniques may vary based on specific requirements such as in a shopping-chatbot scenario, the dialog summary helps to learn user preferences, whereas in the case of a customer call center, the summary may involve the problem attributes that a user specified, and the final resolution provided. This work emphasizes the significance of creating coherent and contextually rich summaries for effective communication in various applications. We explore current state-of-the-art approaches for long dialog summarization in different domains and benchmark metrics based evaluations show that one single model does not perform well across va
    
[^76]: 处理数据以应对RE中的挑战：利用NLP和生成AI缓解挑战

    Dealing with Data for RE: Mitigating Challenges using NLP and Generative AI

    [https://arxiv.org/abs/2402.16977](https://arxiv.org/abs/2402.16977)

    数据在现代软件系统中扮演着不可或缺的角色，监管环境的变化、软件个性化需求的增长以及对治理的强调推动着大型企业采用自动化技术，并在AI为中心的系统中不断引入新的挑战和需求。

    

    在当今不断变化的商业环境中，企业面临着日益增多的挑战。这些挑战包括不断变化的监管环境、软件应用中个性化需求的增长以及对治理的强调。针对这些多方面的需求，大型企业一直在采用从优化核心业务流程到增强客户体验的自动化。实际上，人工智能（AI）已经成为现代软件系统的关键要素。在这种背景下，数据发挥着不可或缺的作用。基于监督学习且在工业规模上运行的以AI为中心的软件系统需要大量的训练数据才能有效运行。此外，引入生成AI导致对足够评估基准的需求不断增长。我们在这一领域的经验显示出，满足这些要求对于实现RE方面的成功至关重要。

    arXiv:2402.16977v1 Announce Type: cross  Abstract: Across the dynamic business landscape today, enterprises face an ever-increasing range of challenges. These include the constantly evolving regulatory environment, the growing demand for personalization within software applications, and the heightened emphasis on governance. In response to these multifaceted demands, large enterprises have been adopting automation that spans from the optimization of core business processes to the enhancement of customer experiences. Indeed, Artificial Intelligence (AI) has emerged as a pivotal element of modern software systems. In this context, data plays an indispensable role. AI-centric software systems based on supervised learning and operating at an industrial scale require large volumes of training data to perform effectively. Moreover, the incorporation of generative AI has led to a growing demand for adequate evaluation benchmarks. Our experience in this field has revealed that the requirement 
    
[^77]: 通过突出潜在错误并建议纠正成功引导人类做出决策的不完美说明

    Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections

    [https://arxiv.org/abs/2402.16973](https://arxiv.org/abs/2402.16973)

    通过检测潜在幻觉并建议替代方案的通信机制，成功减少人类导航错误高达29%而不增加认知负担

    

    本文解决了利用不完美语言模型来在基于定位导航任务的背景下引导人类决策的挑战。我们展示了不完美的说明生成模型可以通过有效的通信机制来更成功地引导人类。我们构建的通信机制包括可以检测说明中潜在幻觉并建议实际替代方案的模型，以及一个直观的界面将该信息呈现给用户。我们展示了这种方法可以将人类导航错误降低高达29%，而不增加额外的认知负担。这一结果突显了将多样化的通信渠道整合到AI系统中来弥补其缺陷并增强其对人类的实用性的潜力。

    arXiv:2402.16973v1 Announce Type: new  Abstract: This paper addresses the challenge of leveraging imperfect language models to guide human decision-making in the context of a grounded navigation task. We show that an imperfect instruction generation model can be complemented with an effective communication mechanism to become more successful at guiding humans. The communication mechanism we build comprises models that can detect potential hallucinations in instructions and suggest practical alternatives, and an intuitive interface to present that information to users. We show that this approach reduces the human navigation error by up to 29% with no additional cognitive burden. This result underscores the potential of integrating diverse communication channels into AI systems to compensate for their imperfections and enhance their utility for humans.
    
[^78]: LangGPT：重新思考面向LLMs的结构化可重复使用提示设计框架从编程语言出发

    LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language

    [https://arxiv.org/abs/2402.16929](https://arxiv.org/abs/2402.16929)

    LangGPT提出了一个双层提示设计框架，作为LLMs的编程语言，大大增强了LLMs产生高质量响应的能力，并在引导LLMs生成高质量提示方面具有显著效果。

    

    LLMs已经展示出在不同领域取得了令人瞩目的性能。然而，为了有效指导LLMs制定高质量的提示对于非AI专家来说是一个挑战。现有的提示工程研究建议了一些略显零碎的优化原则和设计，以及凭经验依赖的提示优化器。不幸的是，这些努力缺乏一个结构化的设计模板，导致学习成本高，重复使用性低。受结构化可重复使用的编程语言的启发，我们提出了LangGPT，作为LLMs的编程语言的双层提示设计框架。LangGPT具有易于学习的规范结构，并提供了一个扩展结构以进行迁移和重用。实验证明，与基准相比，LangGPT显著增强了LLMs产生高质量响应的能力。此外，LangGPT已被证明在引导LLMs生成高质量提示方面是有效的。

    arXiv:2402.16929v1 Announce Type: cross  Abstract: LLMs have demonstrated commendable performance across diverse domains. Nevertheless, formulating high-quality prompts to effectively instruct LLMs poses a challenge for non-AI experts. Existing research in prompt engineering suggests somewhat fragmented optimization principles and designs empirically dependent prompt optimizers. Unfortunately, these endeavors lack a structured design template, incurring high learning costs and resulting in low reusability. Inspired by structured reusable programming languages, we propose LangGPT, a dual-layer prompt design framework as the programming language for LLMs. LangGPT has an easy-to-learn normative structure and provides an extended structure for migration and reuse. Experiments illustrate that LangGPT significantly enhances the capacity of LLMs to produce responses of superior quality compared to baselines. Moreover, LangGPT has proven effective in guiding LLMs to generate high-quality promp
    
[^79]: DrAttack: 提示分解和重构使强大的LLM越狱者

    DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers

    [https://arxiv.org/abs/2402.16914](https://arxiv.org/abs/2402.16914)

    将恶意提示分解为独立的子提示使得LLM越狱攻击更难被检测

    

    本文发现将恶意提示分解为独立的子提示能够有效模糊其潜在的恶意意图，使之以片段化、不易检测的形式呈现，从而解决了这些局限性。我们引入了一个用于越狱攻击的自动提示分解和重构框架（DrAttack）。DrAttack包括三个关键组件：(a) 将原始提示进行“分解”为子提示，(b) 通过上下文学习中的语义上相似但隐含的“重构”这些子提示

    arXiv:2402.16914v1 Announce Type: cross  Abstract: The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks, which adversarially trigger LLMs to output harmful content. However, current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned LLMs. This paper discovers that decomposing a malicious prompt into separated sub-prompts can effectively obscure its underlying malicious intent by presenting it in a fragmented, less detectable form, thereby addressing these limitations. We introduce an automatic prompt \textbf{D}ecomposition and \textbf{R}econstruction framework for jailbreak \textbf{Attack} (DrAttack). DrAttack includes three key components: (a) `Decomposition' of the original prompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly by in-context learning with semantically similar but h
    
[^80]: LDB：通过逐步验证运行时执行来调试大型语言模型

    LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step

    [https://arxiv.org/abs/2402.16906](https://arxiv.org/abs/2402.16906)

    LDB是一个新颖的调试框架，可以让大型语言模型通过运行时执行信息来完善生成的程序。

    

    大型语言模型（LLMs）在代码生成方面取得了重大进展。最近的研究不仅将单次代码生成，而且还将单元测试和程序验证器整合到LLMs中，以迭代地完善生成的程序。然而，这些工作将生成的程序视为不可分割的实体，这对LLMs在调试程序时存在不足，特别是当程序包含复杂的逻辑流程和数据操作时。相比之下，当人类开发人员调试程序时，他们通常设置断点并有选择地检查运行时执行信息。执行流和中间变量在调试过程中发挥着关键作用，然而现有的代码生成文献中未充分利用它们。本研究引入了大型语言模型调试器（LDB），这是一个新颖的调试框架，可以让LLMs通过运行时执行信息完善其生成的程序。

    arXiv:2402.16906v1 Announce Type: cross  Abstract: Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifical
    
[^81]: BESA: 使用分块参数高效稀疏分配修剪大型语言模型

    BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation

    [https://arxiv.org/abs/2402.16880](https://arxiv.org/abs/2402.16880)

    该论文提出了一种名为BESA的新型大型语言模型修剪技术，通过应用分块重构损失，与传统的逐层修剪技术不同，BESA具有优势

    

    大型语言模型（LLMs）在文本摘要、文本问答等各种任务中表现出色。尽管它们的性能令人印象深刻，但由于大量参数造成的计算占用可能是禁锢的。现有解决方案（如SparseGPT和Wanda）尝试通过权重修剪缓解此问题。然而，它们的逐层方法会导致模型输出显著扰动，并需要细致的超参数调整，如修剪速率，这可能会对整体模型性能产生不利影响。为解决此问题，本文引入了一种新颖的LLM修剪技术，称为分块参数高效稀疏分配（BESA），通过应用分块重构损失。与典型的逐层修剪技术相比，BESA具有两个独特的特点：i）它定位于整体修剪误差相对于每个

    arXiv:2402.16880v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to indi
    
[^82]: EvoGPT-f: 一种用于基准测试形式数学语言的进化GPT框架

    EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math Languages

    [https://arxiv.org/abs/2402.16878](https://arxiv.org/abs/2402.16878)

    EvoGPT-f是一个新颖的进化框架，用于对五个形式数学语料库进行差异机器可学习性的系统量化分析，为形式数学语言的基准测试提供了新的方法。

    

    arXiv:2402.16878v1 公告类型: 新的 摘要: 形式数学是将数学转化为编程语言的学科，在这种编程语言中，任何陈述都可以被计算机明确地检查。数学家和计算机科学家花费了数十年进行艰苦的形式化工作，开发了诸如Coq、HOL和Lean等语言。机器学习研究已经汇集到这些形式化数学语料库上，并产生了各种方法来帮助交互式和自动定理证明。然而，这些论文主要集中在一个方法、一个证明任务、一个语言上。本文介绍了EvoGPT-f: 一种新颖的进化框架，用于首次系统量化分析五个形式数学语料库(Lean 3、Lean 4、Coq、HOL 4、HOL Light)的差异机器可学习性，使用四种记号化方法(字符、单词级、字节对编码和StarCoder记号化器)。本文并未结束关于“最佳”的问题。

    arXiv:2402.16878v1 Announce Type: new  Abstract: Formal mathematics is the discipline of translating mathematics into a programming language in which any statement can be unequivocally checked by a computer. Mathematicians and computer scientists have spent decades of painstaking formalization efforts developing languages such as Coq, HOL, and Lean. Machine learning research has converged on these formal math corpora and given rise to an assortment of methodologies to aid in interactive and automated theorem proving. However, these papers have primarily focused on one method, for one proof task, in one language. This paper introduces EvoGPT-f: a novel evolutionary framework for the first systematic quantitative analysis of the differential machine learnability of five formal math corpora (Lean 3, Lean 4, Coq, HOL 4, HOL Light) using four tokenization methods (character, word-level, Byte Pair Encoding and StarCoder tokenizer). This paper does not put to rest the question of the "best" o
    
[^83]: 大型语言模型增强的个性化语言学习练习检索

    Large Language Model Augmented Exercise Retrieval for Personalized Language Learning

    [https://arxiv.org/abs/2402.16877](https://arxiv.org/abs/2402.16877)

    大型语言模型利用生成能力来合成假设练习，以弥合学习者需求与练习内容之间的语义鸿沟，提高个性化语言学习练习检索效果。

    

    我们研究了在线语言学习环境中的零样本练习检索问题，以赋予学习者通过自然语言明确请求个性化练习的能力。通过收集自语言学习者的真实数据，我们观察到矢量相似性方法很难捕捉练习内容与学习者用于表达他们想要学习内容的语言之间的关系。我们利用大型语言模型的生成能力来弥合这一差距，通过基于学习者输入合成假设练习，然后用于搜索相关练习。我们的方法mHyER克服了三个挑战：（1）缺乏用于训练的相关性标签，（2）受限的学习

    arXiv:2402.16877v1 Announce Type: cross  Abstract: We study the problem of zero-shot exercise retrieval in the context of online language learning, to give learners the ability to explicitly request personalized exercises via natural language. Using real-world data collected from language learners, we observe that vector similarity approaches poorly capture the relationship between exercise content and the language that learners use to express what they want to learn. This semantic gap between queries and content dramatically reduces the effectiveness of general-purpose retrieval models pretrained on large scale information retrieval datasets like MS MARCO. We leverage the generative capabilities of large language models to bridge the gap by synthesizing hypothetical exercises based on the learner's input, which are then used to search for relevant exercises. Our approach, which we call mHyER, overcomes three challenges: (1) lack of relevance labels for training, (2) unrestricted learn
    
[^84]: 通过增强查询提升语言生成的检索过程

    Enhancing Retrieval Processes for Language Generation with Augmented Queries

    [https://arxiv.org/abs/2402.16874](https://arxiv.org/abs/2402.16874)

    本研究通过检索增强生成（RAG）技术解决了语言生成中“幻觉”问题，并借助查询优化过程将用户查询与高级语言模型连接，显著提升了模型性能。

    

    在智能技术日新月异的世界中，由于先进语言模型的崛起，搜索文档变得更具挑战性。这些模型有时会面临困难，比如提供不准确的信息，通常被称为“幻觉”。本研究致力于通过检索增强生成（RAG）技术解决这一问题，该技术指导模型基于真实事实提供准确答复。为了解决可伸缩性问题，研究探讨了将用户查询与诸如BERT和Orca2等复杂语言模型连接起来的创新查询优化过程。研究展开在三种情境中：首先，没有RAG，其次，没有额外帮助，最后，加入额外帮助。选择紧凑而高效的Orca2 7B模型展示出对计算资源的智能使用。实证结果表明，初始语言模型的性能有了显著提升。

    arXiv:2402.16874v1 Announce Type: cross  Abstract: In the rapidly changing world of smart technology, searching for documents has become more challenging due to the rise of advanced language models. These models sometimes face difficulties, like providing inaccurate information, commonly known as "hallucination." This research focuses on addressing this issue through Retrieval-Augmented Generation (RAG), a technique that guides models to give accurate responses based on real facts. To overcome scalability issues, the study explores connecting user queries with sophisticated language models such as BERT and Orca2, using an innovative query optimization process. The study unfolds in three scenarios: first, without RAG, second, without additional assistance, and finally, with extra help. Choosing the compact yet efficient Orca2 7B model demonstrates a smart use of computing resources. The empirical results indicate a significant improvement in the initial language model's performance unde
    
[^85]: 作为可优化图的语言代理

    Language Agents as Optimizable Graphs

    [https://arxiv.org/abs/2402.16823](https://arxiv.org/abs/2402.16823)

    将基于LLM的代理统一描述为计算图，提出新颖的自动图优化器来改进节点和边，实现了代理之间的自动协作和改进。

    

    多种人类设计的提升技术被提出，用于改进基于大型语言模型（LLMs）的问题求解器，产生了许多不同的代码库。我们通过将LLM代理描述为计算图来统一这些方法。节点实现处理多模态数据或查询LLMs的功能，并且边描述操作之间的信息流动。图形可以递归地组合成代表不同代理之间协作层次的更大组合图（其中边连接不同代理的操作）。我们的新颖自动图优化器（1）优化节点级LLM提示（节点优化）并（2）通过改变图连接性来改善代理协调（边缘优化）。实验证明我们的框架可用于高效开发、集成和自动改进各种LLM代理。代码可在https://github.com/metauto-ai/gptswarm找到。

    arXiv:2402.16823v1 Announce Type: cross  Abstract: Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. The code can be found at https://github.com/metauto-ai/gptswarm.
    
[^86]: Nemotron-4 15B技术报告

    Nemotron-4 15B Technical Report

    [https://arxiv.org/abs/2402.16819](https://arxiv.org/abs/2402.16819)

    Nemotron-4 15B是一个150亿参数的大型多语言模型，在多语言能力上表现优异，超过其他规模相似的模型。

    

    我们介绍了Nemotron-4 15B，这是一个拥有150亿参数的大型多语言模型，训练过程中使用了8000万亿个文本标记。Nemotron-4 15B在英语、多语言和编码任务上表现出色：在7个下游评估领域中，它在4个领域中表现出色，并在其余领域中取得了竞争性表现，超过了所有现有规模相似的开放模型。具体来说，Nemotron-4 15B展现出了所有规模相似模型中最强的多语言能力，甚至在多语言任务上优于四倍以上的大型模型，以及专门用于多语言任务的模型。

    arXiv:2402.16819v1 Announce Type: new  Abstract: We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.
    
[^87]: D-XCB：数据无关去偏方法，用于公平准确基于Transformer的网络欺凌检测

    D-XCB: Data-independent Debiasing for Fair and Accurate Transformer-based Cyberbullying Detection

    [https://arxiv.org/abs/2402.16458](https://arxiv.org/abs/2402.16458)

    该论文提出了ID-XCB，这是首个数据无关去偏技术，能够在缓解模型对引发偏见的词的关注的同时提高网络欺凌检测性能，超越了当前最先进的去偏方法。

    

    骂人话是收集包含网络欺凌事件数据的常用代理。我们的重点是衡量和减轻由骂人话和因此数据收集策略导致事件之间的虚假关联产生的偏见。在演示和量化这些偏见之后，我们介绍了ID-XCB，这是第一个数据无关去偏技术，结合了敌对训练、偏见约束和去偏微调方法，旨在缓解模型对引发偏见的词的关注，同时不影响整体模型性能。我们在两个流行的基于会话的网络欺凌数据集上探讨了ID-XCB，同时进行了全面的消融和泛化研究。我们展示了ID-XCB学习了强大的网络欺凌检测能力，同时减轻了偏见，超过了最先进的去偏方法在性能和偏见缓解方面。我们的定量和定性分析表明其泛化性。

    arXiv:2402.16458v1 Announce Type: new  Abstract: Swear words are a common proxy to collect datasets with cyberbullying incidents. Our focus is on measuring and mitigating biases derived from spurious associations between swear words and incidents occurring as a result of such data collection strategies. After demonstrating and quantifying these biases, we introduce ID-XCB, the first data-independent debiasing technique that combines adversarial training, bias constraints and debias fine-tuning approach aimed at alleviating model attention to bias-inducing words without impacting overall model performance. We explore ID-XCB on two popular session-based cyberbullying datasets along with comprehensive ablation and generalisation studies. We show that ID-XCB learns robust cyberbullying detection capabilities while mitigating biases, outperforming state-of-the-art debiasing methods in both performance and bias mitigation. Our quantitative and qualitative analyses demonstrate its generalisab
    
[^88]: 跨领域的中文句式结构解析

    Cross-domain Chinese Sentence Pattern Parsing

    [https://arxiv.org/abs/2402.16311](https://arxiv.org/abs/2402.16311)

    本文提出了一种利用大型语言模型进行自我训练的创新方法，通过动态生成训练数据将源领域句法规则与目标领域句子相结合，增强句式结构解析器对各种领域的适应能力，实验证明其在教科书和新闻领域的效果优于基于规则的基准模型1.68个百分点。

    

    arXiv:2402.16311v1 公告类型: 跨领域 句式结构（SPS）解析是一种主要用于语言教学的句法分析方法。现有的SPS解析器主要依赖于教科书语料库进行训练，缺乏跨领域能力。为了克服这一限制，本文提出了一种创新方法，利用大型语言模型（LLMs）在自我训练框架内。从源领域中提取部分句法规则，与目标领域句子结合动态生成训练数据，增强了解析器对不同领域的适应能力。在教科书和新闻领域进行的实验表明，所提出的方法效果显著，F1指标比基于规则的基准模型高出1.68个百分点。

    arXiv:2402.16311v1 Announce Type: cross  Abstract: Sentence Pattern Structure (SPS) parsing is a syntactic analysis method primarily employed in language teaching.Existing SPS parsers rely heavily on textbook corpora for training, lacking cross-domain capability.To overcome this constraint, this paper proposes an innovative approach leveraging large language models (LLMs) within a self-training framework. Partial syntactic rules from a source domain are combined with target domain sentences to dynamically generate training data, enhancing the adaptability of the parser to diverse domains.Experiments conducted on textbook and news domains demonstrate the effectiveness of the proposed method, outperforming rule-based baselines by 1.68 points on F1 metrics.
    
[^89]: FuseChat：对话模型知识融合

    FuseChat: Knowledge Fusion of Chat Models

    [https://arxiv.org/abs/2402.16107](https://arxiv.org/abs/2402.16107)

    FuseChat通过知识融合将多个对话模型的集体知识转移到目标语言模型中，避免了昂贵的预训练成本。

    

    虽然从头开始训练大型语言模型（LLMs）确实可以导致具有独特能力和优势的模型，但这种方法会产生巨大成本，并可能导致竞争能力的潜在冗余。一种替代策略是将现有的LLMs组合成更强大的LLM，从而减少昂贵的预训练的必要性。但是，由于LLMs的多样化架构，直接参数融合被证明是不可行的。最近，FuseLLM引入了知识融合的概念，通过轻量级的持续训练将多个结构多样的LLM的集体知识转移至目标LLM。在本报告中，我们扩展了FuseLLM框架的可扩展性和灵活性，实现了对话LLM的融合，生成了FuseChat。FuseChat包括两个主要阶段。首先，我们对结构和规模不同的源LLMs进行知识融合

    arXiv:2402.16107v1 Announce Type: new  Abstract: While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies. An alternative strategy is to combine existing LLMs into a more robust LLM, thereby diminishing the necessity for expensive pre-training. However, due to the diverse architectures of LLMs, direct parameter blending proves to be unfeasible. Recently, \textsc{FuseLLM} introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training. In this report, we extend the scalability and flexibility of the \textsc{FuseLLM} framework to realize the fusion of chat LLMs, resulting in \textsc{FuseChat}. \textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs t
    
[^90]: 基于引文增强的LLM聊天机器人生成

    Citation-Enhanced Generation for LLM-based Chatbot

    [https://arxiv.org/abs/2402.16063](https://arxiv.org/abs/2402.16063)

    提出一种基于引文增强的LLM聊天机器人生成方法，采用检索模块搜索支持文档来解决幻觉内容产生的问题。

    

    大型语言模型（LLMs）在各种情景下展现出强大的通用智能，包括将它们集成到聊天机器人中。然而，基于LLM的聊天机器人面临的一个重要挑战是在回复中可能产生虚构内容，这严重限制了它们的适用性。本文提出了一种新颖的后续引用增强生成（CEG）方法，结合检索论证。与先前侧重于预防生成过程中幻觉的研究不同，我们的方法以后续方式解决了这个问题。它结合了一个检索模块来搜索与生成内容相关的支持文档，并采用基于自然语言推理的方法。

    arXiv:2402.16063v1 Announce Type: cross  Abstract: Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc \textbf{C}itation-\textbf{E}nhanced \textbf{G}eneration (\textbf{CEG}) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-ba
    
[^91]: EHRNoteQA：用于在临床环境中评估大型语言模型的患者特定问题回答基准

    EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings

    [https://arxiv.org/abs/2402.16040](https://arxiv.org/abs/2402.16040)

    该研究介绍了EHRNoteQA，这是一个新颖的患者特定问题回答基准，旨在评估临床环境中的大型语言模型（LLMs），具有采用多项选择问题回答格式和需要分析多篇临床笔记的特点。

    

    该研究介绍了EHRNoteQA，这是一个新颖的患者特定问题回答基准，旨在评估临床环境中的大型语言模型（LLMs）。在MIMIC-IV电子健康记录（EHR）的基础上，由三位医疗专家团队精心策划了包含962个独特问题的数据集，每个问题都与特定患者的EHR临床笔记相关联。与现有基于EHR的基准不同的是：首先，它是第一个采用多项选择问题回答格式的数据集，这种设计选择在自动评估的背景下有效评估LLMs的得分性能，与其他格式相比。其次，它需要分析多篇临床笔记才能回答一个问题，反映了实际临床决策制定的复杂性，医生需要审查大量患者病史记录。我们对各种大型语言模型进行了全面评估。

    arXiv:2402.16040v1 Announce Type: new  Abstract: This study introduces EHRNoteQA, a novel patient-specific question answering benchmark tailored for evaluating Large Language Models (LLMs) in clinical environments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three medical professionals has curated the dataset comprising 962 unique questions, each linked to a specific patient's EHR clinical notes. What makes EHRNoteQA distinct from existing EHR-based benchmarks is as follows: Firstly, it is the first dataset to adopt a multi-choice question answering format, a design choice that effectively evaluates LLMs with reliable scores in the context of automatic evaluation, compared to other formats. Secondly, it requires an analysis of multiple clinical notes to answer a single question, reflecting the complex nature of real-world clinical decision-making where clinicians review extensive records of patient histories. Our comprehensive evaluation on various large language models
    
[^92]: 打破Breakout: 用自我完善重新定义LM对抗越狱攻击的防御

    Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement

    [https://arxiv.org/abs/2402.15180](https://arxiv.org/abs/2402.15180)

    提出了一种通过自我完善和格式化改进LMs对抗越狱攻击的方法，即使在非安全对齐的LMs中也具有出色的安全性，同时降低攻击成功率。

    

    警告：本文包含可能引起不快的冒犯性词语。语言模型（LMs）容易被利用进行恶意滥用。对LM进行安全对齐的训练非常复杂，使得难以立即应对快速发展的攻击，如越狱攻击。我们提出了一种通过格式自我完善的方法，即使在非安全对齐的LMs中也能实现出色的安全性，并将我们的方法与几种防御基线进行评估，表明这是针对越狱攻击最安全的无训练方法。此外，我们提出了一种改进自我完善过程效率的格式化方法，同时在较少迭代中降低攻击成功率。我们还观察到非安全对齐的LM在安全任务中表现优于安全对齐的LM，因为它们给出更有用且更安全的回复。总之，我们的发现能够在较少的计算成本下实现更少的安全风险。

    arXiv:2402.15180v1 Announce Type: cross  Abstract: Caution: This paper includes offensive words that could potentially cause unpleasantness. Language models (LMs) are vulnerable to exploitation for adversarial misuse. Training LMs for safety alignment is extensive and makes it hard to respond to fast-developing attacks immediately, such as jailbreaks. We propose self-refine with formatting that achieves outstanding safety even in non-safety-aligned LMs and evaluate our method alongside several defense baselines, demonstrating that it is the safest training-free method against jailbreak attacks. Additionally, we proposed a formatting method that improves the efficiency of the self-refine process while reducing attack success rates in fewer iterations. We've also observed that non-safety-aligned LMs outperform safety-aligned LMs in safety tasks by giving more helpful and safe responses. In conclusion, our findings can achieve less safety risk with fewer computational costs, allowing non-
    
[^93]: 面向预训练大型语言模型的机器遗忘

    Machine Unlearning of Pre-trained Large Language Models

    [https://arxiv.org/abs/2402.15159](https://arxiv.org/abs/2402.15159)

    本研究在大型语言模型（LLMs）中探讨了“被遗忘权”的概念，提出机器遗忘作为解决方案，并在预训练模型中建立了全面的遗忘框架及高效的遗忘方法，同时提供了改进超参数鲁棒性以及高效调整超参数的指南。

    

    本研究探讨了大型语言模型（LLMs）背景下“被遗忘权”的概念。我们以机器遗忘作为一个关键解决方案，重点关注预训练模型——一个明显缺乏研究的领域。我们在预训练LLMs中勾勒了一个全面的机器遗忘框架，包括对七种不同遗忘方法的批判性分析。通过使用来自arXiv、书籍和GitHub的策划数据集进行严格评估，我们建立了一个有力的机器遗忘性能基准，表明这些方法的计算效率比重新训练高出 $10^5$ 倍以上。我们的结果表明，在分布数据上将梯度上升与梯度下降结合可以改善超参数的鲁棒性。我们还提供了关于在遗忘过程中进行高效超参数调整的详细指南。我们的研究推动了有关伦理人工智能实践的讨论，提供了

    arXiv:2402.15159v1 Announce Type: cross  Abstract: This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering
    
[^94]: 语义镜像越狱:基于遗传算法的针对开源LLM的越狱提示

    Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs

    [https://arxiv.org/abs/2402.14872](https://arxiv.org/abs/2402.14872)

    本文提出了一种语义镜像越狱（SMJ）方法，通过生成在语义上类似于原始问题的越狱提示来绕过LLMs。

    

    大型语言模型（LLMs）通常用于创意写作、代码生成和翻译，根据输入序列生成文本，但容易受到越狱攻击的影响，其中精心设计的提示会导致有害输出。大多数越狱提示方法使用一组越狱模板，然后跟随提出问题，创建越狱提示。然而，现有的越狱提示设计通常存在过多的语义差异，导致无法抵御使用简单语义度量作为阈值的防御。越狱提示在语义上比用于查询的原始问题更加多样化。在本文中，我们介绍了一种称为语义镜像越狱（SMJ）的方法，通过生成在语义上类似于原始问题的越狱提示来绕过LLMs。我们将寻找既满足语义相似性又具有越狱有效性的越狱提示建模为一个多目标优化问题。

    arXiv:2402.14872v1 Announce Type: cross  Abstract: Large Language Models (LLMs), used in creative writing, code generation, and translation, generate text based on input sequences but are vulnerable to jailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreak prompt methods use a combination of jailbreak templates followed by questions to ask to create jailbreak prompts. However, existing jailbreak prompt designs generally suffer from excessive semantic differences, resulting in an inability to resist defenses that use simple semantic metrics as thresholds. Jailbreak prompts are semantically more varied than the original questions used for queries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approach that bypasses LLMs by generating jailbreak prompts that are semantically similar to the original question. We model the search for jailbreak prompts that satisfy both semantic similarity and jailbreak validity as a multi-objective optimization proble
    
[^95]: ProSparse: 引入和增强大型语言模型内部激活稀疏性

    ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models

    [https://arxiv.org/abs/2402.13516](https://arxiv.org/abs/2402.13516)

    本文介绍了一种名为"ProSparse"的有效稀疏化方法，以推动大型语言模型实现更高的激活稀疏性而不降低模型性能

    

    Activation sparsity指的是激活输出中存在许多弱贡献元素。作为使用ReLU激活函数的模型的普遍属性，已被证明是提高模型推理效率的一种有前途的范例。然而，大多数大型语言模型（LLMs）采用了没有内在激活稀疏性的激活函数（例如GELU和Swish）。一些最近的努力尝试引入ReLU或其变体作为替代激活函数，以帮助LLMs实现激活稀疏性和推理加速，但很少能同时获得高稀疏度和可比较的模型性能。本文介绍了一种名为"ProSparse"的有效稀疏化方法，以推动LLMs实现更高的激活稀疏性而不降低模型性能。具体来说，将LLMs的激活函数替换为ReLU后，ProSparse采用渐进稀疏正则化

    arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
    
[^96]: 信心至关重要：重新审视大型语言模型的内在自我校正能力

    Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models

    [https://arxiv.org/abs/2402.12563](https://arxiv.org/abs/2402.12563)

    本文研究了大型语言模型的内在自我校正能力，并提出了一个“如果-否则”（IoE）提示框架，帮助模型评估自身“信心”并进行自我校正。

    

    大型语言模型（LLMs）的最近成功激发了对它们自我校正能力的越来越多的兴趣。本文对LLMs的内在自我校正进行了全面调查，试图解决关于其可行性的持续争论。我们的研究确定了一个重要的潜在因素 - LLMs的“信心” - 在自我校正过程中。忽视这一因素可能导致模型过度批评自己，从而导致对自校正效果的可靠结论不准确。我们实验观察到LLMs具有理解其自身回应“信心”的能力。这激励我们开发了一个“如果-否则”（IoE）提示框架，旨在引导LLMs评估其自身“信心”，促进内在自我校正。我们进行了大量实验证明，我们基于IoE的提示可以实现一

    arXiv:2402.12563v1 Announce Type: cross  Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a co
    
[^97]: 从后门毒化数据集中通过降频空间获取清洁语言模型

    Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space

    [https://arxiv.org/abs/2402.12026](https://arxiv.org/abs/2402.12026)

    通过对频率空间的分析，本文提出了一种多尺度低秩适应（MuScleLoRA）方法，用于解决在训练语言模型时受到后门攻击的问题。

    

    尽管语言模型（LMs）在各种自然语言处理（NLP）任务中取得了显著成功，但LMs的可靠性容易受到后门攻击的影响。先前的研究尝试在毒化数据集上训练LMs时减轻后门学习，但在现实场景中抵御复杂的后门攻击时仍然面临困难。在本文中，我们通过傅里叶分析研究了频率空间中后门LMs的学习机制。我们的发现表明，毒化数据集上呈现的后门映射相比清洁映射更倾向于较低频率，导致后门映射更快地收敛。为了解决这一困境，我们提出了多尺度低秩适应（MuScleLoRA），它在频率空间中部署多个径向缩放，低秩适应目标模型，并在更新参数时进一步调整梯度。通过降频

    arXiv:2402.12026v1 Announce Type: cross  Abstract: Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscal
    
[^98]: 用RLHF推进翻译偏好建模：迈向成本效益解决方案

    Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution

    [https://arxiv.org/abs/2402.11525](https://arxiv.org/abs/2402.11525)

    提出了一种利用强化学习和人类反馈（RLHF）来改进翻译质量的成本效益偏好学习策略，该策略通过优化奖励模型来区分人类和机器翻译，从而指导改进机器翻译。

    

    arXiv:2402.11525v1 公告类型：新 真实性、表达力和优雅是机器翻译中不断追求的目标。然而，传统的度量标准如BLEU并不严格符合人类对翻译质量的偏好。本文探讨了利用强化学习与人类反馈（RLHF）来提高翻译质量。收集人类对翻译之间的比较的大规模高质量数据集并不容易，尤其对于低资源语言。为解决这一问题，我们提出了一种成本效益的偏好学习策略，通过区分人类和机器翻译来优化奖励模型。通过这种方式，奖励模型学习机器翻译与人类之间的不足之处，并指导随后对机器翻译的改进。实验结果表明，RLHF可以有效地提升翻译质量，这种改进也有益于其他翻译方向。

    arXiv:2402.11525v1 Announce Type: new  Abstract: Faithfulness, expressiveness, and elegance is the constant pursuit in machine translation. However, traditional metrics like \textit{BLEU} do not strictly align with human preference of translation quality. In this paper, we explore leveraging reinforcement learning with human feedback (\textit{RLHF}) to improve translation quality. It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for low-resource languages. To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and machine translations. In this manner, the reward model learns the deficiencies of machine translation compared to human and guides subsequent improvements in machine translation. Experimental results demonstrate that \textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions 
    
[^99]: LinkNER: 使用不确定性将本地命名实体识别模型与大语言模型进行链接

    LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty

    [https://arxiv.org/abs/2402.10573](https://arxiv.org/abs/2402.10573)

    提出了一种结合小型微调模型和大型语言模型的LinkNER框架，通过不确定性的链接策略RDC，使微调模型能够补充黑盒LLMs

    

    命名实体识别（NER）作为自然语言理解中的基本任务，直接影响着网络内容分析、搜索引擎和信息检索系统。微调后的NER模型在标准NER基准上表现出令人满意的性能。然而，由于有限的微调数据和缺乏知识，它在未见实体识别上表现不佳。因此，NER模型在网络相关应用中的可用性和可靠性受到影响。相反，像GPT-4这样的大型语言模型（LLM）具有丰富的外部知识，但研究表明它们缺乏NER任务的专业性。此外，私有和大规模权重使LLM的调整困难。为了解决这些挑战，我们提出了一个框架，结合了小型微调模型和LLMs（LinkNER），以及一种基于不确定性的链接策略RDC，使微调模型能够补充黑盒LLMs。

    arXiv:2402.10573v1 Announce Type: new  Abstract: Named Entity Recognition (NER) serves as a fundamental task in natural language understanding, bearing direct implications for web content analysis, search engines, and information retrieval systems. Fine-tuned NER models exhibit satisfactory performance on standard NER benchmarks. However, due to limited fine-tuning data and lack of knowledge, it performs poorly on unseen entity recognition. As a result, the usability and reliability of NER models in web-related applications are compromised. Instead, Large Language Models (LLMs) like GPT-4 possess extensive external knowledge, but research indicates that they lack specialty for NER tasks. Furthermore, non-public and large-scale weights make tuning LLMs difficult. To address these challenges, we propose a framework that combines small fine-tuned models with LLMs (LinkNER) and an uncertainty-based linking strategy called RDC that enables fine-tuned models to complement black-box LLMs, ach
    
[^100]: 使用大型语言模型扩展AutoTutor的创作

    Scaling the Authoring of AutoTutors with Large Language Models

    [https://arxiv.org/abs/2402.09216](https://arxiv.org/abs/2402.09216)

    本文研究使用大型语言模型（LLMs）来撰写智能辅导系统的潜力，提出了保留传统辅导系统结构和教学法的方法。

    

    大型语言模型（LLMs）在教育领域有多种用途，从自动题目生成到作文评估。本文探讨了使用大型语言模型（LLMs）来撰写智能辅导系统的潜力。LLMs的一个常见问题是它们容易偏离所期望的教学策略，例如泄露答案给学生，总体上提供的保证很少。我们认为，虽然带有某些限制的LLMs可以取代学科专家的位置，但整体的教学设计仍需手工制作以取得最佳学习效果。基于这一原则，我们创建了一个示例的端到端辅导系统，命名为MWPTutor，它使用LLMs填充预定义有限状态转换器的状态空间。这种方法保留了多年来由学习科学家开发的传统辅导系统的结构和教学法，同时引入了额外的灵活性。

    arXiv:2402.09216v1 Announce Type: new Abstract: Large Language Models (LLMs) have found several use cases in education, ranging from automatic question generation to essay evaluation. In this paper, we explore the potential of using Large Language Models (LLMs) to author Intelligent Tutoring Systems. A common pitfall of LLMs is their straying from desired pedagogical strategies such as leaking the answer to the student, and in general, providing no guarantees. We posit that while LLMs with certain guardrails can take the place of subject experts, the overall pedagogical design still needs to be handcrafted for the best learning results. Based on this principle, we create a sample end-to-end tutoring system named MWPTutor, which uses LLMs to fill in the state space of a pre-defined finite state transducer. This approach retains the structure and the pedagogy of traditional tutoring systems that has been developed over the years by learning scientists but brings in additional flexibility
    
[^101]: 数据到文本自然语言生成研究的系统性回顾

    A Systematic Review of Data-to-Text NLG

    [https://arxiv.org/abs/2402.08496](https://arxiv.org/abs/2402.08496)

    这篇系统性回顾全面分析了数据到文本自然语言生成研究的现状，提出未来方向，并解决了相关挑战。

    

    这篇系统性回顾旨在全面分析数据到文本生成研究的现状，重点是确定研究空白，提供未来方向，并解决回顾中发现的挑战。我们对文献进行了全面的检查，包括方法、数据集、评估指标、应用、多语言性和幻觉缓解措施。我们的回顾为这个快速发展的领域的未来研究提供了路线图。

    This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.
    
[^102]: HarmBench：用于自动红队和强大拒绝的标准化评估框架

    HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal

    [https://arxiv.org/abs/2402.04249](https://arxiv.org/abs/2402.04249)

    HarmBench是一个为自动红队和强大拒绝设计的标准化评估框架，通过对18种红队方法和33个目标LLM和防御的比较，得出了新的见解，并介绍了一种高效的对抗训练方法，提高了LLM在各种攻击下的稳健性。

    

    自动红队具有发现和减轻大型语言模型（LLM）恶意使用的风险的巨大潜力，然而该领域缺乏一个标准化的评估框架来严格评估新方法。为了解决这个问题，我们引入了HarmBench，一个用于自动红队的标准化评估框架。我们在红队评估中确定了几个以前未考虑的有吸引力的特性，并系统地设计了HarmBench以满足这些标准。使用HarmBench，我们对18种红队方法和33个目标LLM和防御进行了大规模比较，得到了新的见解。我们还引入了一种高效的对抗训练方法，显著增强了LLM在各种攻击下的稳健性，展示了HarmBench如何促进攻击和防御的共同开发。我们在https://github.com/centerforaisafety/HarmBench上开源了HarmBench。

    Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.
    
[^103]: ANLS* -- 一种适用于生成型大语言模型的通用文档处理度量方法

    ANLS* -- A Universal Document Processing Metric for Generative Large Language Models

    [https://arxiv.org/abs/2402.03848](https://arxiv.org/abs/2402.03848)

    ANLS*是一种用于生成型模型的新度量方法，针对各种任务包括信息提取和分类任务进行评估。它扩展了现有的ANLS度量方法，可以作为替代方案使用。

    

    传统上，在文档分类和信息提取等任务中，区分模型一直是主要选择。这些模型做出的预测可以分为有限数量的预定义类别，便于进行二元真假评估，并能直接计算F1分数等指标。然而，生成型大语言模型（GLLMs）的最新进展促使领域发生了转变，因为它们具备了强大的零-shot能力，消除了下游数据集和计算昂贵的微调的需求。然而，评估GLLMs存在挑战，因为对于GLLMs的预测，不能应用于区分模型所使用的二元真假评估方法。本文引入了一种用于生成型模型的新度量方法，称为ANLS*，用于评估各种任务，包括信息提取和分类任务。ANLS*度量方法扩展了现有ANLS度量方法，可作为一种即插即用的替代方案。

    Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and i
    
[^104]: 用语言模型区分可知与不可知的能力

    Distinguishing the Knowable from the Unknowable with Language Models

    [https://arxiv.org/abs/2402.03563](https://arxiv.org/abs/2402.03563)

    通过研究大型语言模型，在自由文本中识别作为代理的模型和冻结预训练模型的嵌入的小型线性探测器可以准确预测更大模型令牌级别上的自信度，进一步提出了一种无监督的方法在相同任务上达到了非平凡的准确度，这证明了语言模型中存在不同类型的不确定性表示。

    

    我们研究了在大型语言模型（LLMs）生成的自由文本输出中，是否可以鉴别出认知不确定性（反映缺乏知识的不确定性）和偶然不确定性（反映基础分布中的熵）。在没有真实概率的情况下，我们探索了一个设置，在这个设置中，为了（近似地）分解给定LLM的不确定性，一个明显更大的模型充当地面真相的代理。我们表明，基于冻结预训练模型的嵌入的小型线性探测器能够准确预测在令牌级别上更大模型将更自信的情况，并且在一个文本领域上训练的探测器可以泛化到其他领域。进一步地，我们提出了一种完全无监督的方法，在相同任务上达到了非平凡的准确度。综合考虑这些结果，我们解释这些结果作为LLMs内部自然地包含了不同类型不确定性的表示，这可能有助于制定更有效的方法。

    We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more i
    
[^105]: GUARD: 通过角色扮演生成自然语言越狱来测试大型语言模型遵循指南的合规性

    GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models

    [https://arxiv.org/abs/2402.03299](https://arxiv.org/abs/2402.03299)

    本论文提出了一个通过角色扮演的系统，可以生成自然语言越狱，用于测试大型语言模型的指南遵循情况。系统通过收集现有越狱并将其组织成知识图来生成新的越狱，证明了其高效性和有效性。

    

    发现绕过大型语言模型（LLM）的安全过滤和有害回应的"越狱"已经鼓励社区采取安全措施。其中一个主要的安全措施是在发布之前用越狱主动测试LLM。因此，这样的测试将需要一种能够大规模且高效地生成越狱的方法。本文在追随一种新颖而直观的策略下，以人类生成的方式来生成越狱。我们提出了一个角色扮演系统，将四种不同角色分配给用户LLM，以便协作生成新的越狱。此外，我们收集现有的越狱，并通过句子逐句进行聚类频率和语义模式的划分，将它们分成不同的独立特征。我们将这些特征组织成一个知识图，使其更易于访问和检索。我们的角色系统将利用这个知识图来生成新的越狱，证明了其有效性。

    The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effec
    
[^106]: 使用抽象链推理的高效工具使用

    Efficient Tool Use with Chain-of-Abstraction Reasoning

    [https://arxiv.org/abs/2401.17464](https://arxiv.org/abs/2401.17464)

    该方法通过让LLM首先解码抽象推理链，然后调用领域工具填充具体知识，使得LLM能够更好地利用工具进行多步推理，并且具有通用性和鲁棒性。

    

    为了实现与人类期望一致的准确推理，大型语言模型（LLM）需要将推理与现实世界的知识（例如网络事实、数学和物理规则）联系起来。工具可以帮助LLM获取这些外部知识，但是在多步推理问题中仍然存在挑战，即如何精细调整LLM代理（例如Toolformer）以调用工具，其中相互连接的工具调用需要整体化和高效的工具使用规划。在这项工作中，我们提出了一种新的方法，让LLM在多步推理中更好地利用工具。我们的方法是通过训练LLM首先用抽象占位符解码推理链，然后调用领域工具以填充具体知识来实现每个推理链。抽象链的规划使LLM能够学习更通用的推理策略，对于与不同推理问题相关的领域知识（例如数学结果）的变化具有鲁棒性。它还允许LLM执行解码操作

    To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning.   In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding a
    
[^107]: 神经语言模型解剖学

    Anatomy of Neural Language Models

    [https://arxiv.org/abs/2401.03797](https://arxiv.org/abs/2401.03797)

    本教程详细、简化和清晰地解释了神经语言模型，并提供清晰的图形说明，填补了缺乏统一数学框架的现存问题。

    

    生成式人工智能和迁移学习领域在最近几年取得了显著进展，特别是在自然语言处理（NLP）领域。变压器已经成为这些进展的核心，基于最前沿的变压器的语言模型（LMs）在广泛的应用中导致了新的最先进的结果。尽管涉及神经LMs的研究作品数量呈指数增长，但其中绝大多数是高级的，离实际操作颇远。因此，在缺乏解释主要类型神经LMs的统一数学框架的前提下，对这一领域的文献深入了解是一项艰巨的任务。我们在本教程中解决了上述问题，旨在在详细、简化和清晰的数学框架中解释神经LMs，并附有清晰的图形说明。

    arXiv:2401.03797v2 Announce Type: replace  Abstract: The fields of generative AI and transfer learning have experienced remarkable advancements in recent years especially in the domain of Natural Language Processing (NLP). Transformers have been at the heart of these advancements where the cutting-edge transformer-based Language Models (LMs) have led to new state-of-the-art results in a wide spectrum of applications. While the number of research works involving neural LMs is exponentially increasing, their vast majority are high-level and far from self-contained. Consequently, a deep understanding of the literature in this area is a tough task especially in the absence of a unified mathematical framework explaining the main types of neural LMs. We address the aforementioned problem in this tutorial where the objective is to explain neural LMs in a detailed, simplified and unambiguous mathematical framework accompanied by clear graphical illustrations. Concrete examples on widely used m
    
[^108]: 用于更快的LLM推理的级联推测草图

    Cascade Speculative Drafting for Even Faster LLM Inference

    [https://arxiv.org/abs/2312.11462](https://arxiv.org/abs/2312.11462)

    引入了Cascade Speculative Drafting（CS Drafting）算法，通过垂直级联消除神经模型的自回归生成，通过水平级联优化草稿中的时间分配，从而进一步提高LLM推理效率。

    

    引入了增强大型语言模型（LLM）推理效率的级联推测草图，通过较小的模型生成草稿来运作。较大的目标模型然后查看这个草稿以与其输出对齐，目标模型的任何接受都将减少目标模型运行的数量，从而提高效率。然而，在级联推测的草图过程中包括缓慢的自回归生成，并为生成的标记分配相同的时间，而不考虑它们的重要性。这些低效性共同导致级联推测的性能不佳。为了进一步改善LLM推理，我们引入了级联推测草图（CS Drafting），这是一种整合了两种级联类型的推测执行算法。垂直级联从神经模型中消除自回归生成，而水平级联优化了草稿中的时间分配

    arXiv:2312.11462v3 Announce Type: replace-cross  Abstract: Introduced to enhance the efficiency of large language model (LLM) inference, speculative decoding operates by having a smaller model generate a draft. A larger target model then reviews this draft to align with its output, and any acceptance by the target model results in a reduction of the number of the target model runs, ultimately improving efficiency. However, the drafting process in speculative decoding includes slow autoregressive generation and allocates equal time to generating tokens, irrespective of their importance. These inefficiencies collectively contribute to the suboptimal performance of speculative decoding. To further improve LLM inference, we introduce Cascade Speculative Drafting (CS Drafting), a speculative execution algorithm that incorporates two types of cascades. The Vertical Cascade eliminates autoregressive generation from neural models, while the Horizontal Cascade optimizes time allocation in draft
    
[^109]: KnowGPT：大型语言模型的黑盒知识注入

    KnowGPT: Black-Box Knowledge Injection for Large Language Models

    [https://arxiv.org/abs/2312.06185](https://arxiv.org/abs/2312.06185)

    KnowGPT是一种为大型语言模型提供黑盒知识注入的框架，通过深度强化学习和多臂老虎机构建最适合每个问题的提示，在三个基准数据集上实验证明其显著提升了知识注入的效果。

    

    生成式大型语言模型（LLMs），如ChatGPT，提供互动式API，可以以人类专家水平回答常见问题。然而，当面临需要特定领域或专业领域知识的问题时，这些模型通常会给出不准确或不正确的响应，这些知识并未包含在它们的训练语料库中。此外，许多最先进的LLMs并非开源，这使得仅使用模型API注入知识具有挑战性。在本研究中，我们介绍了KnowGPT，一种用于LLMs在问答中的黑盒知识注入框架。KnowGPT利用深度强化学习（RL）从知识图中提取相关知识，并使用多臂老虎机（MAB）为每个问题构建最合适的提示。我们在三个基准数据集上进行了大量实验，展示了KnowGPT显著增强了现有方法。值得注意的是，KnowGPT平均改进了23%。

    arXiv:2312.06185v2 Announce Type: replace-cross  Abstract: Generative Large Language Models (LLMs), such as ChatGPT, offer interactive APIs that can answer common questions at a human-expert level. However, these models often give inaccurate or incorrect responses when faced with questions requiring domain-specific or professional-specific knowledge not covered in their training corpus. Furthermore, many state-of-the-art LLMs are not open-source, making it challenging to inject knowledge with model APIs only. In this work, we introduce KnowGPT, a black-box knowledge injection framework for LLMs in question answering. KnowGPT leverages deep reinforcement learning (RL) to extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed Bandit (MAB) to construct the most suitable prompt for each question. Our extensive experiments on three benchmark datasets showcase that KnowGPT significantly enhances the existing methods. Notably, KnowGPT achieves an average improvement of 23.
    
[^110]: 人们在哪里在线讲故事？跨在线社区的故事检测

    Where Do People Tell Stories Online? Story Detection Across Online Communities

    [https://arxiv.org/abs/2311.09675](https://arxiv.org/abs/2311.09675)

    介绍了一个解决在线社区中故事检测困难的挑战，提出了StorySeeker工具包，包括详细注释的Reddit数据集和模型，突出了在线叙事的文本特征，引入了叙事跨度检测作为一个新任务。

    

    在线社区中的故事检测是一项具有挑战性的任务，因为故事分散在社区中，并且与单个文本中的非叙事部分交织在一起。我们通过构建和发布StorySeeker工具包来解决这一挑战，其中包括一个包含502个Reddit帖子和评论的丰富注释数据集，一个适应社交媒体背景的详细的代码书，以及用于在文档和跨度级别预测叙事的模型。我们的数据集是从数百个流行的英语Reddit社区中抽样而来，涵盖了33个主题类别，它包含了细粒度的专家注释，包括二元故事标签，故事跨度和事件跨度。我们使用我们的数据评估了一系列检测方法，并确定了在线叙事的独特文本特征，重点关注叙事跨度检测，这是我们引入的一个新任务。我们阐明了大规模叙事的分布特征。

    arXiv:2311.09675v2 Announce Type: replace  Abstract: Story detection in online communities is a challenging task as stories are scattered across communities and interwoven with non-storytelling spans within a single text. We address this challenge by building and releasing the StorySeeker toolkit, including a richly annotated dataset of 502 Reddit posts and comments, a detailed codebook adapted to the social media context, and models to predict storytelling at the document and span level. Our dataset is sampled from hundreds of popular English-language Reddit communities ranging across 33 topic categories, and it contains fine-grained expert annotations, including binary story labels, story spans, and event spans. We evaluate a range of detection methods using our data, and we identify the distinctive textual features of online storytelling, focusing on storytelling span detection, which we introduce as a new task. We illuminate distributional characteristics of storytelling on a large
    
[^111]: 少样本迁移学习用于知识库问答：融合监督模型和上下文学习

    Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning

    [https://arxiv.org/abs/2311.08894](https://arxiv.org/abs/2311.08894)

    提出了一种新的KBQA架构FuSIC-KBQA，通过多个源训练的召回器执行KB检索，在LLM的重新排序后以此作为LLM少样本上下文学习的输入来生成逻辑形式，并利用执行引导反馈进一步优化。

    

    现有的知识库问答（KBQA）架构需要大量注释数据，这使得它们在部署时成本高且耗时。我们提出少样本迁移学习用于KBQA的问题，目标域仅提供少量标记示例，但在源域中有大量标记训练数据集可用。我们提出了一种名为FuSIC-KBQA的新型KBQA架构，它使用多个经过源培训的召回器执行KB检索，使用LLM重新排序，将此作为LLM少样本上下文学习的输入以生成逻辑形式，进一步使用执行引导反馈进行细化。在四对不同复杂度的源-目标KBQA对上的实验表明，FuSIC-KBQA明显优于为此设置调整的SoTA KBQA模型。在领域内设置的额外实验表明，当训练数据有限时，FuSIC-KBQA也优于SoTA KBQA模型。

    arXiv:2311.08894v2 Announce Type: replace-cross  Abstract: Existing Knowledge Base Question Answering (KBQA) architectures are hungry for annotated data, which make them costly and time-consuming to deploy. We introduce the problem of few-shot transfer learning for KBQA, where the target domain offers only a few labeled examples, but a large labeled training dataset is available in a source domain. We propose a novel KBQA architecture called FuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers, re-ranks using an LLM and uses this as input for LLM few-shot in-context learning to generate logical forms, which are further refined using execution-guided feedback. Experiments over four source-target KBQA pairs of varying complexity show that FuSIC-KBQA significantly outperforms adaptations of SoTA KBQA models for this setting. Additional experiments in the in-domain setting show that FuSIC-KBQA also outperforms SoTA KBQA models when training data is limited.
    
[^112]: 细调小型语言模型以协调更大型语言模型提高复杂推理能力

    Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning

    [https://arxiv.org/abs/2310.18338](https://arxiv.org/abs/2310.18338)

    细调小型语言模型作为分解生成器，使用策略梯度优化来协调更大型语言模型，进一步提高复杂推理能力

    

    大型语言模型（LLMs）在生成思维链（CoT）时展现出令人印象深刻的推理能力。最近对提示分解以解决复杂的多步推理问题的尝试取决于LLM同时分解和解决问题的能力。一个重要的缺点是，基础LLMs通常不可用于微调，使得适应性在计算上是禁锢的。我们认为（并证明）问题的分解和解决方案生成是不同的能力，最好通过单个庞大的LLM而不是一个整体模块来解决。我们引入了DaSLaM，它使用一个分解生成器将复杂问题分解成需要更少推理步骤的子问题。这些子问题由一个求解器来回答。我们使用一个相对小型（13B参数）LM作为分解生成器，我们使用策略梯度优化来与之交互进行训练。

    arXiv:2310.18338v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM. We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps. These subproblems are answered by a solver. We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with 
    
[^113]: 让循环的询问: 大型语言模型在判断中的摇摆

    Ask Again, Then Fail: Large Language Models' Vacillations in Judgement

    [https://arxiv.org/abs/2310.02174](https://arxiv.org/abs/2310.02174)

    目前的语言模型在面对后续问题时常常摇摆不定，研究者提出了一个后续问题机制和两个度量标准来量化这种不一致性，并开发出Unwavering-FQ框架来教导模型保持最初的正确判断，实验证明其有效性。

    

    我们观察到目前的会话式语言模型在面对后续问题时往往在其判断上摇摆不定，即使原始判断是正确的。这种摇摆对于生成可靠回复和建立用户信任构成了重要挑战。为了全面评估这一问题，我们引入了一个后续问题机制以及两个度量标准来量化这种不一致性，确认了当前语言模型普遍存在这种情况。为了缓解这一问题，我们探讨了各种提示策略用于闭源模型；此外，我们开发了一个基于训练的框架Unwavering-FQ，通过合成高质量的偏好数据来教导语言模型保持其最初的正确判断。我们的实验结果验证了我们框架的有效性以及其增强模型通用能力的能力。

    arXiv:2310.02174v2 Announce Type: replace-cross  Abstract: We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework Unwavering-FQ that teaches language models to maintain their originally correct judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models (https://github.com/NUSTM/LLMs-Waver-In-Judgements).
    
[^114]: EvalLM: 交互式评估基于用户定义标准的大型语言模型提示

    EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria

    [https://arxiv.org/abs/2309.13633](https://arxiv.org/abs/2309.13633)

    EvalLM是一个交互式系统，帮助开发人员通过评估多个输出来改进大型语言模型提示，相比手动评估，能够帮助用户撰写更多样化的标准、检查更多输出，达到更满意的提示。

    

    通过简单地组合提示，开发人员可以使用大型语言模型（LLMs）原型化新颖的生成应用。然而，要将原型细化为产品，开发人员必须通过评估输出以诊断弱点来迭代修订提示。形成性访谈（N=8）显示，开发人员在评估输出时投入了大量精力，因为他们评估特定上下文和主观标准。我们提出了EvalLM，这是一个交互式系统，可以通过评估用户定义标准上的多个输出来迭代改进提示。用户可以通过用自然语言描述标准，使用系统基于LLM的评估器来获得提示在哪些方面表现出色或失败的概述，并根据评估器的反馈进行改进。一项比较研究（N=12）显示，与手动评估相比，EvalLM有助于帮助参与者撰写更多样化的标准、检查两倍数量的输出，并达到令人满意的提示。

    arXiv:2309.13633v2 Announce Type: replace-cross  Abstract: By simply composing prompts, developers can prototype novel generative applications with Large Language Models (LLMs). To refine prototypes into products, however, developers must iteratively revise prompts by evaluating outputs to diagnose weaknesses. Formative interviews (N=8) revealed that developers invest significant effort in manually evaluating outputs as they assess context-specific and subjective criteria. We present EvalLM, an interactive system for iteratively refining prompts by evaluating multiple outputs on user-defined criteria. By describing criteria in natural language, users can employ the system's LLM-based evaluator to get an overview of where prompts excel or fail, and improve these based on the evaluator's feedback. A comparative study (N=12) showed that EvalLM, when compared to manual evaluation, helped participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory promp
    
[^115]: 根据...：促使语言模型提高引用的能力

    "According to ...": Prompting Language Models Improves Quoting from Pre-Training Data

    [https://arxiv.org/abs/2305.13252](https://arxiv.org/abs/2305.13252)

    提出了“根据”提示方法，通过引导大型语言模型在响应中参考先前观察到的文本来改进引用，并提出了一种新的评估指标（QUIP-Score）来度量模型生成的答案在文本语料库中的直接发现程度。

    

    大型语言模型（LLMs）可能会产生幻觉并生成虚假信息，尽管它们事先在事实数据上进行了训练。受“据悉”的新闻设备启发，我们提出了“根据”提示：指导LLMs将响应与先前观察到的文本相联系。为了量化这种基础性，我们提出了一种新颖的评估指标（QUIP-Score），用于衡量模型生成的答案在基础文本语料库中直接找到的程度。我们通过对三个语料库（维基百科、PubMed和美国法律税法典）进行实验来说明这些提示根据我们的度量标准改善了基础性，而且通常还提高了最终任务性能。此外，要求模型减少基础性（或者根据其他语料库）的提示确实降低了QUIP-Score，表明LLMs有能力根据要求增加或减少基础性生成。

    arXiv:2305.13252v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data. Inspired by the journalistic device of "according to sources", we propose according-to prompting: directing LLMs to ground responses against previously observed text. To quantify this grounding, we propose a novel evaluation metric (QUIP-Score) that measures the extent to which model-produced answers are directly found in underlying text corpora. We illustrate with experiments on three corpora (Wikipedia, PubMed, and the U.S. legal tax code) that these prompts improve grounding under our metrics, with the additional benefit of often improving end-task performance. Furthermore, prompts that ask the model to decrease grounding (or to ground to other corpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase or decrease grounded generations on request.
    
[^116]: 一种令人沮丧地简单的神经文本生成解码方法

    A Frustratingly Simple Decoding Method for Neural Text Generation

    [https://arxiv.org/abs/2305.12675](https://arxiv.org/abs/2305.12675)

    提出了一种称为Frustratingly Simple Decoding (FSD)的神经文本生成解码方法，通过构建反语言模型来惩罚未来生成已生成的内容，实验证明其通过引入几乎无额外计算开销就能有效超越传统方法（如核采样）。

    

    我们介绍了一种令人沮丧地简单、超级高效且出奇有效的解码方法，我们称之为Frustratingly Simple Decoding（FSD），用于神经文本生成。FSD的思想很简单：我们构建一个基于先前生成文本的反语言模型，并使用这个反语言模型来惩罚未来生成已生成的内容。这个反语言模型可以实现得非常简单，可以是一个n-gram语言模型或者一个向量化变体。因此，FSD不引入额外的模型参数，计算开销微乎其微（FSD速度可以像贪婪搜索一样快）。尽管如此简单，FSD却出奇地有效；实验证明，FSD可以胜过迄今为止的经典方法（即核采样方法）以及最近提出的一些强基线方法。

    arXiv:2305.12675v2 Announce Type: replace  Abstract: We introduce a frustratingly simple, super efficient and surprisingly effective decoding method, which we call Frustratingly Simple Decoding (FSD), for neural text generation. The idea behind FSD is straightforward: we build an anti-LM based on previously generated text and use this anti-LM to penalize future generation of what has been generated. The anti-LM can be implemented as simple as an n-gram language model or a vectorized variant. In this way, FSD introduces no extra model parameters and negligible computational overhead (FSD can be as fast as greedy search). Despite the simplicity, FSD is surprisingly effective; Experiments show that FSD can outperform the canonical methods to date (i.e., nucleus sampling) as well as several strong baselines that were proposed recently.
    
[^117]: 深度增强：在激活空间中使用自监督学习进行数据增强

    Deep Augmentation: Self-Supervised Learning with Transformations in Activation Space

    [https://arxiv.org/abs/2303.14537](https://arxiv.org/abs/2303.14537)

    深度增强是一种利用dropout或PCA在神经网络中转换目标层的方法，有效改善性能和泛化能力。在对比学习任务中，在Transformers、ResNets和图神经网络等基础模型上，通过深度增强实现了显著的性能提升，但在监督问题上效果相反。

    

    我们提出了一种称为深度增强的方法，通过使用辍学或PCA来转换神经网络中的目标层，以提高性能和泛化能力。我们通过在自然语言处理、计算机视觉和图学习中的对比学习任务上进行大量实验来展示深度增强。 我们观察到在对比学习的基础模型中，如Transformers、ResNets和图神经网络上深度增强能够带来显著的性能提升，但在相应的监督问题上观察到相反的效果。 我们的分析表明，深度增强减轻了层之间的相互适应，即"崩溃"形式的问题。 我们利用这一观察结果制定了一种选择目标层的方法；特别是，我们的实验表明，用深度增强定位更深层次的层要优于增强输入数据。 这种方法的简单网络和模态无关性使其

    arXiv:2303.14537v2 Announce Type: replace-cross  Abstract: We introduce Deep Augmentation, an approach to implicit data augmentation using dropout or PCA to transform a targeted layer within a neural network to improve performance and generalization. We demonstrate Deep Augmentation through extensive experiments on contrastive learning tasks in NLP, computer vision, and graph learning. We observe substantial performance gains with Transformers, ResNets, and Graph Neural Networks as the underlying models in contrastive learning, but observe inverse effects on the corresponding supervised problems. Our analysis suggests that Deep Augmentation alleviates co-adaption between layers, a form of "collapse." We use this observation to formulate a method for selecting which layer to target; in particular, our experimentation reveals that targeting deeper layers with Deep Augmentation outperforms augmenting the input data. The simple network- and modality-agnostic nature of this approach enables
    
[^118]: 用标注数据进行链式思维的自动提示增强与选择

    Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data

    [https://arxiv.org/abs/2302.12822](https://arxiv.org/abs/2302.12822)

    提出了Automate-CoT策略，可以通过从小型标记数据集自动增加合理链，并修剪低质量链，构建基于标签的机器生成理由链的候选池，最终选择最佳组合。

    

    Chain-of-thought prompting（CoT）推进了大型语言模型（LLMs）的推理能力，在算术、常识和符号推理任务中取得了优越性能。然而，大多数CoT研究依赖于精心设计的人工注释的合理链，以提示语言模型，这对于现实世界中存在标记的训练数据但没有人工注释的合理链的应用构成了挑战。这为CoT提示应用于这些通用任务带来了障碍。本文提出了一种新策略，Automate-CoT（自动提示增强与选择与链思），可以通过从小型标记数据集中自动增强合理链，然后修剪低质量链，基于标签构建基于机器生成的理由链的候选池。最后，它选择了几个理由链的最佳组合。

    arXiv:2302.12822v2 Announce Type: replace  Abstract: Chain-of-thought prompting (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in arithmetic, commonsense, and symbolic reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt the language model, which poses challenges for real-world applications where labeled training data is available without human-annotated rational chains. This creates barriers to applications of CoT prompting to these general tasks. This paper proposes a new strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoTs by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machine-generated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains
    
[^119]: 具有基于流的语音转换的跨语言文本到语音方法以改善发音

    Cross-lingual Text-To-Speech with Flow-based Voice Conversion for Improved Pronunciation

    [https://arxiv.org/abs/2210.17264](https://arxiv.org/abs/2210.17264)

    通过基于流的语音转换实现了跨语言文本到语音，提升了发音质量，并在客观和主观评估中表现出优势。

    

    本文提出了一种用于端到端跨语言文本到语音（TTS）的方法，旨在保留目标语言的发音，而不考虑原始说话者的语言。所使用的模型基于非注意力Tacotron架构，其中解码器已经被替换为一个受讲话者身份条件的归一化流网络，允许通过相同模型执行TTS和声音转换（VC），因为固有的语言内容和说话者身份解耦。在跨语言设置中使用时，首先使用目标语言的本地发音者产生声学特征，然后通过相同模型应用声音转换，以将这些特征转换为目标说话者的声音。我们通过客观和主观评估验证，我们的方法与基准跨语言合成相比具有一定好处。

    arXiv:2210.17264v2 Announce Type: replace-cross  Abstract: This paper presents a method for end-to-end cross-lingual text-to-speech (TTS) which aims to preserve the target language's pronunciation regardless of the original speaker's language. The model used is based on a non-attentive Tacotron architecture, where the decoder has been replaced with a normalizing flow network conditioned on the speaker identity, allowing both TTS and voice conversion (VC) to be performed by the same model due to the inherent linguistic content and speaker identity disentanglement. When used in a cross-lingual setting, acoustic features are initially produced with a native speaker of the target language and then voice conversion is applied by the same model in order to convert these features to the target speaker's voice. We verify through objective and subjective evaluations that our method can have benefits compared to baseline cross-lingual synthesis. By including speakers averaging 7.5 minutes of spe
    
[^120]: 一种强大的网络安全主题分类工具

    A Robust Cybersecurity Topic Classification Tool

    [https://arxiv.org/abs/2109.02473](https://arxiv.org/abs/2109.02473)

    通过使用多数投票的方式，研究提出了一种网络安全主题分类工具，相比于21个单独模型，该工具在检测网络安全相关文本时表现出较低的假阳性和假阴性率，并且能够处理数十万份文档。

    

    在这项研究中，我们使用来自三个互联网文本信息来源（Reddit，Stackexchange，Arxiv）的用户定义的标签，训练了21种不同的机器学习模型，用于检测自然文本中的网络安全讨论的主题分类任务。我们分析了每个模型在交叉验证实验中的假阳性和假阴性率。然后，我们提出了一种网络安全主题分类（CTC）工具，该工具将21个训练好的机器学习模型的多数投票作为检测网络安全相关文本的决策机制。我们还展示了CTC工具的多数投票机制平均提供了比21个单独模型更低的假阴性和假阳性率。我们展示了CTC工具可扩展到数十万份文档，而其墙钟时间大约为几小时。

    arXiv:2109.02473v4 Announce Type: replace-cross  Abstract: In this research, we use user defined labels from three internet text sources (Reddit, Stackexchange, Arxiv) to train 21 different machine learning models for the topic classification task of detecting cybersecurity discussions in natural text. We analyze the false positive and false negative rates of each of the 21 model's in a cross validation experiment. Then we present a Cybersecurity Topic Classification (CTC) tool, which takes the majority vote of the 21 trained machine learning models as the decision mechanism for detecting cybersecurity related text. We also show that the majority vote mechanism of the CTC tool provides lower false negative and false positive rates on average than any of the 21 individual models. We show that the CTC tool is scalable to the hundreds of thousands of documents with a wall clock time on the order of hours.
    
[^121]: 不一定总是向右看：研究基于解码器的大型语言模型在序列标注中的能力

    Do Not (Always) Look Right: Investigating the Capabilities of Decoder-Based Large Language Models for Sequence Labeling. (arXiv:2401.14556v1 [cs.CL])

    [http://arxiv.org/abs/2401.14556](http://arxiv.org/abs/2401.14556)

    本文研究了基于解码器的大型语言模型在序列标注方面的能力，并探索了提高它们性能的策略。

    

    基于掩码语言建模（MLM）目标的预训练语言模型在自然语言理解（NLU）任务中表现出色。虽然与相似规模的因果语言建模解码器相比，经过微调的MLM-based编码器始终表现优异，但最近将解码器模型扩展至数十亿参数的趋势使得大型语言模型（LLMs）能够与MLM-based编码器相抗衡。尽管规模扩大了它们在NLU任务中的能力，但LLMs在信息提取（IE）任务中，尤其是序列标注（SL）任务方面仍然落后于SOTA结果。然而，LLMs的SL性能是由其固有的限制决定的还是可以改进的，仍然不清楚。为了解决这个问题，我们探索了改进"开放式"LLMs（Llama2和Mistral）在IE任务中的SL性能的策略。我们研究了解码器块组内的双向信息流，应用了层次逐层移除或启用因果掩码（CM）进来LLM微调。这种方法产生了...

    Pre-trained language models based on masked language modeling (MLM) objective excel in natural language understanding (NLU) tasks. While fine-tuned MLM-based encoders consistently outperform causal language modeling decoders of comparable size, a recent trend of scaling decoder models to multiple billion parameters resulted in large language models (LLMs), making them competitive with MLM-based encoders. Although scale amplifies their prowess in NLU tasks, LLMs fall short of SOTA results in information extraction (IE) tasks, many framed as sequence labeling (SL). However, whether this is an intrinsic limitation of LLMs or whether their SL performance can be improved remains unclear. To address this, we explore strategies to enhance the SL performance of "open" LLMs (Llama2 and Mistral) on IE tasks. We investigate bidirectional information flow within groups of decoder blocks, applying layer-wise removal or enforcement of the causal mask (CM) during LLM fine-tuning. This approach yields
    
[^122]: 100个样本可以走多远？通过微小的多语言平行数据解锁全面的零样本跨语言翻译

    How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual Translation via Tiny Multi-Parallel Data. (arXiv:2401.12413v1 [cs.CL])

    [http://arxiv.org/abs/2401.12413](http://arxiv.org/abs/2401.12413)

    本文研究了如何通过仅有的少量微小多语言平行数据来增强以英语为中心的模型的零样本翻译能力，实现大幅度的非英文整体改进，并保持英文方向上的性能能力。研究表明，即使在随机抽取的少量方向集上进行微调，也可以获得可比较的整体改进。

    

    零样本翻译是一个开放问题，旨在在多语言机器翻译（MMT）中翻译训练过程中未见过的语言对。一种常见但资源消耗较大的解决方案是尽可能挖掘更多的翻译方向并添加到平行语料库中。本文展示了通过使用仅有的少量微小多语言平行数据来优化以英语为中心的模型的零样本能力。例如，在EC30数据集上，我们展示了仅使用100个多语言平行样本就能够实现+21.7 ChrF非英文整体改进（870个方向），同时保持在以英语为中心的方向上的能力。我们进一步研究了微调数据的规模效应和其转移能力。令人惊讶的是，我们的实证分析表明，即使是在一个小的、随机抽取的方向集（10%）上进行微调，也可以获得可比较的整体改进。此外，所得到的非英文性能与英文性能非常接近。

    Zero-shot translation is an open problem, aiming to translate between language pairs unseen during training in Multilingual Machine Translation (MMT). A common, albeit resource-consuming, solution is to mine as many translation directions as possible to add to the parallel corpus. In this paper, we show that the zero-shot capability of an English-centric model can be easily enhanced by fine-tuning with a very small amount of multi-parallel data. For example, on the EC30 dataset, we show that up to +21.7 ChrF non-English overall improvements (870 directions) can be achieved by using only 100 multi-parallel samples, meanwhile preserving capability in English-centric directions. We further study the size effect of fine-tuning data and its transfer capabilities. Surprisingly, our empirical analysis shows that comparable overall improvements can be achieved even through fine-tuning in a small, randomly sampled direction set (10\%). Also, the resulting non-English performance is quite close 
    
[^123]: FAIR到位：我们如何为大型语言模型的训练开发和评估符合FAIR标准的数据集？

    FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for Large Language Models' Training?. (arXiv:2401.11033v1 [cs.CL])

    [http://arxiv.org/abs/2401.11033](http://arxiv.org/abs/2401.11033)

    这项工作介绍了一个将FAIR数据原则嵌入到大型语言模型训练中的框架，并提供了整合指南和检查清单。通过一个案例研究，展示了在符合FAIR标准的数据集中识别和减轻偏见的实际应用。

    

    大型语言模型的进展凸显了道德实践和数据完整性的需求。我们引入了一个将FAIR（可发现、可访问、可互操作、可重用）数据原则嵌入到LLM训练中的框架。这一方法标志着朝着符合FAIR标准的实践的转变。我们的框架提供了将FAIR数据原则整合到LLM训练中的指南。该举措包括研究人员和开发人员的检查清单。我们还通过一个案例研究展示了它的实际应用，重点是在我们符合FAIR标准的数据集中识别和减轻偏见。这项工作对于人工智能伦理和数据科学是重要的贡献，倡导在LLMs中使用平衡和道德的训练方法。

    Advancements in Large Language Models (LLMs) highlight the need for ethical practices and data integrity. We introduce a framework that embeds FAIR (Findable, Accessible, Interoperable, Reusable) data principles into LLM training. This approach marks a shift towards practices compliant with FAIR standards. Our framework presents guidelines for integrating FAIR data principles into LLM training. This initiative includes a checklist for researchers and developers. We also demonstrate its practical application through a case study focused on bias identification and mitigation in our FAIR-compliant dataset. This work is a significant contribution to AI ethics and data science, advocating for balanced and ethical training methods in LLMs.
    
[^124]: 大型语言模型的多阶段协作知识蒸馏在半监督序列生成任务中的应用

    Multistage Collaborative Knowledge Distillation from Large Language Models for Semi-Supervised Sequence Generation. (arXiv:2311.08640v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.08640](http://arxiv.org/abs/2311.08640)

    将大型语言模型的知识通过多阶段协作蒸馏的方式应用于半监督序列生成任务中，可以显著提高模型的泛化能力和性能。

    

    我们研究了半监督序列生成任务，在这种任务中，标记数据太少以至于无法有效地微调模型，同时在大型语言模型 (LLM) 中进行少样本提示的性能也不够理想，尤其是对于一些昂贵且对预训练的 LLM 不熟悉的任务，如解析。本文发现，从上下文学习的 LLM 蒸馏出的学生模型在这些任务上通常比其教师模型具有更好的泛化能力。基于这一发现，我们提出了一种新的方法 - 大型语言模型的多阶段协作知识蒸馏 (MCKD) - 用于这些任务。MCKD 首先进行少样本提示，让LLM为无标签数据生成伪标签。在每个中间知识蒸馏 (KD) 阶段，使用伪标签数据的不重叠分区来训练一对新的学生模型。然后，每个学生模型为其未见分区生成新的和改进的伪标签，在下一个蒸馏阶段中使用。我们展示了该方法的优势。

    We study semi-supervised sequence generation tasks where labeled data are too scarce to effectively finetune a model and at the same time few-shot prompting of a large language model (LLM) has suboptimal performance. This happens when a task, such as parsing, is expensive to annotate and also unfamiliar to a pretrained LLM. In this paper, we present a discovery that student models distilled from an in-context learned LLM can often generalize better than their teacher on such tasks. Leveraging this finding, we present a new method -multistage collaborative knowledge distillation from an LLM (MCKD) -- for such tasks. MCKD first few-shot prompts an LLM to produce pseudolabels for unlabeled data. At each intermediate knowledge distillation (KD) stage, a new pair of students is trained on disjoint partitions of the pseudolabeled data. Each student then produces new and improved pseudolabels for its unseen partition to be used in the next stage of distillation. We demonstrate the advantage
    
[^125]: 大型语言模型是事后解释器吗？

    Are Large Language Models Post Hoc Explainers?. (arXiv:2310.05797v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05797](http://arxiv.org/abs/2310.05797)

    这项工作提出了第一个研究大型语言模型（LLMs）解释其他预测模型有效性的框架，并且提出了多个提示策略，填补了当前对于LLMs在解释其他模型行为方面的缺失。

    

    大型语言模型（LLM）越来越被广泛应用于各种自然语言处理（NLP）应用中。最近的一项创新，即上下文学习（ICL），使得LLM能够在推理阶段通过在提示中提供少量示例来学习新任务，从而消除了模型微调的需要。虽然LLM已经被应用于多个领域，但其在解释其他模型行为方面的适用性仍相对未被探索。尽管存在越来越多的新解释技术，但很多技术要求对模型具有白盒访问权限和/或计算成本较高，凸显了下一代事后解释器的需求。在这项工作中，我们提出了第一个研究LLM解释其他预测模型有效性的框架。具体而言，我们提出了一个包含多种提示策略的新颖框架：i）基于扰动的ICL，ii）基于预测的ICL，iii）基于指令的ICL，和iv）基于解释的ICL。

    Large Language Models (LLMs) are increasingly used as powerful tools for a plethora of natural language processing (NLP) applications. A recent innovation, in-context learning (ICL), enables LLMs to learn new tasks by supplying a few examples in the prompt during inference time, thereby eliminating the need for model fine-tuning. While LLMs have been utilized in several applications, their applicability in explaining the behavior of other models remains relatively unexplored. Despite the growing number of new explanation techniques, many require white-box access to the model and/or are computationally expensive, highlighting a need for next-generation post hoc explainers. In this work, we present the first framework to study the effectiveness of LLMs in explaining other predictive models. More specifically, we propose a novel framework encompassing multiple prompting strategies: i) Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL, and iv) Explanation-based I
    
[^126]: 生成式查询和文档扩展何时失败？方法、检索器和数据集的全面研究

    When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets. (arXiv:2309.08541v1 [cs.IR])

    [http://arxiv.org/abs/2309.08541](http://arxiv.org/abs/2309.08541)

    通过对11种扩展技术、12个不同分布变化的数据集和24个检索模型的全面分析，我们发现使用大型语言模型进行查询或文档扩展的效果与检索器性能相关，对于弱模型来说扩展提高了分数，但对于强模型来说扩展通常会损害分数。

    

    使用大型语言模型（LM）进行查询或文档扩展可以改善信息检索中的泛化能力。然而，目前尚不清楚这些技术是否普遍有益，还是仅在特定设置下有效，例如对于特定的检索模型、数据集领域或查询类型。为了回答这个问题，我们进行了第一次对基于LM的扩展的全面分析。我们发现，检索器性能与扩展的增益之间存在强烈的负相关关系：扩展改善了较弱模型的分数，但通常会损害较强模型的分数。我们展示了这一趋势在11种扩展技术、12个具有不同分布变化的数据集和24个检索模型的一组实验中成立。通过定性错误分析，我们提出了一个假设，即尽管扩展提供了额外的信息（可能改善了召回率），但它们也增加了噪声，使得很难区分出顶级相关文档（从而引入了错误的正例）

    Using large language models (LMs) for query or document expansion can improve generalization in information retrieval. However, it is unknown whether these techniques are universally beneficial or only effective in specific settings, such as for particular retrieval models, dataset domains, or query types. To answer this, we conduct the first comprehensive analysis of LM-based expansion. We find that there exists a strong negative correlation between retriever performance and gains from expansion: expansion improves scores for weaker models, but generally harms stronger models. We show this trend holds across a set of eleven expansion techniques, twelve datasets with diverse distribution shifts, and twenty-four retrieval models. Through qualitative error analysis, we hypothesize that although expansions provide extra information (potentially improving recall), they add additional noise that makes it difficult to discern between the top relevant documents (thus introducing false positiv
    
[^127]: 通过进化算法连接大型语言模型与强大的提示优化器

    Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers. (arXiv:2309.08532v1 [cs.CL])

    [http://arxiv.org/abs/2309.08532](http://arxiv.org/abs/2309.08532)

    本文提出了一种通过连接大型语言模型和进化算法进行提示优化的框架，名为EvoPrompt。通过利用大型语言模型的语言处理能力和进化算法的优化性能，EvoPrompt可以自动化处理需要连贯和可读性良好的提示，提高大型语言模型的性能。

    

    大型语言模型在各种任务中表现出色，但它们依赖于精心设计的提示，这通常需要大量的人力努力。为了自动化这个过程，本文提出了一种新颖的离散提示优化框架，称为EvoPrompt，它借鉴了进化算法的思想，因为它们表现出良好的性能和快速的收敛性。为了使进化算法能够处理需要连贯并且可读性良好的自然语言表达的离散提示，我们将大型语言模型与进化算法进行了连接。这种方法使我们可以同时利用大型语言模型的强大语言处理能力和进化算法的高效优化性能。具体而言，EvoPrompt在不使用任何梯度或参数的情况下，从一组提示中开始，并基于进化算子通过大型语言模型生成新的提示，根据开发集改进提示的种群。我们对闭源和开源的大型语言模型，包括GPT-3进行提示优化。

    Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3
    
[^128]: 人类对齐的偏好排序优化

    Preference Ranking Optimization for Human Alignment. (arXiv:2306.17492v1 [cs.CL])

    [http://arxiv.org/abs/2306.17492](http://arxiv.org/abs/2306.17492)

    本文提出了Preference Ranking Optimization (PRO)方法，通过扩展布拉德利-特里比较，采用偏好排序的方式来直接对齐大型语言模型（LLMs），解决了强化学习从人类反馈中学习的复杂性、不稳定性和对超参数的敏感性的问题。

    

    大型语言模型（LLMs）经常包含误导性内容，强调了将其与人类价值观对齐以确保安全的AI系统的必要性。采用从人类反馈中学习强化学习（RLHF）来实现这种对齐，通过将基于布拉德利-特里配对比较的奖励模型与Proximal Policy Optimization（PPO）等RL算法结合起来来优化LLM的响应。然而，RLHF表现出复杂性、不稳定性和对超参数的敏感性。在本文中，我们提出了Preference Ranking Optimization（PRO）作为PPO的另一种直接将LLM与布拉德利-特里比较对齐的方法。PRO将配对的布拉德利-特里比较扩展到适应任意长度的偏好排序。通过反复对比生成响应的可能性，PRO指导LLM优先考虑最佳响应，并逐渐对剩余的响应进行排序。通过这种方式，PRO将人类对齐有效地转化为概率对齐。

    Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secur AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment by combining a reward model, typically based on Bradley-Terry paired comparison, with an RL algorithm such as Proximal Policy Optimization (PPO) to optimize LLM responses. However, RLHF exhibits complexity, instability, and sensitivity to hyperparameters. In this paper, we propose Preference Ranking Optimization (PRO) as an alternative to PPO for directly aligning LLMs with the Bradley-Terry comparison. PRO extends the pairwise Bradley-Terry comparison to accommodate preference rankings of any length. By iteratively contrasting the likelihood of generating responses, PRO instructs the LLM to prioritize the best response while progressively ranking the remaining responses. In this manner, PRO effectively transforms human alignment into aligning the prob
    
[^129]: DecodingTrust: GPT模型的全面可信度评估

    DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. (arXiv:2306.11698v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.11698](http://arxiv.org/abs/2306.11698)

    这项工作提出了对GPT模型进行全面可信度评估，考虑了多个方面的风险，发现了以前未公开的威胁漏洞，例如对毒性输出和个人信息泄漏的易被误导性。

    

    生成预训练变压器（GPT）模型在其能力方面取得了令人兴奋的进展，引起了从从业者到公众的兴趣。然而，尽管关于GPT模型的可信度的文献仍然有限，从业者们提议将强大的GPT模型用于敏感应用，如医疗保健和金融领域，其中错误可能代价高昂。为此，本研究提出了对大型语言模型（重点放在GPT-4和GPT-3.5上）进行全面的可信度评估，考虑了多样的观点 - 包括有毒性、陈规偏见、对抗强度、超出分布的强度、对抗示范的强度、隐私、机器伦理和公平性。根据我们的评估，我们发现了以前未公开的可信度威胁漏洞。例如，我们发现GPT模型可以轻松被误导生成有毒和偏见的输出，并在训练数据和上下文中泄漏私人信息。

    Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in their capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications such as healthcare and finance -- where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives -- including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conv
    
[^130]: 诊断变压器：揭示临床决策中的特征空间。 (arXiv:2305.17588v2 [cs.CL] UPDATED)

    Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making. (arXiv:2305.17588v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17588](http://arxiv.org/abs/2305.17588)

    该论文介绍了一种名为SUFO的框架，用于增强微调的变压器模型的可解释性。该框架利用多种分析和可视化技术，解决了模型信任和可解释性的关键问题，并通过案例研究验证了其有效性。

    

    在医学等高风险领域，为了建立信任和确保安全，模型的可解释性至关重要，而使用有限的临床记录对预训练的变压器进行微调以辅助临床决策。我们引入了一种名为SUFO的系统框架，该框架增强了微调的变压器特征空间的可解释性。SUFO利用一系列分析和可视化技术，包括监督探索、无监督相似性分析、特征动态和异常值分析，来解决关于模型信任和可解释性的关键问题。我们进行了一个案例研究，研究了预训练数据对真实世界病理分类任务的影响，并在MedNLI上验证了我们的发现。我们评估了五个110M规模的预训练变压器模型，分为通用领域（BERT, TNLR）、混合领域（BioBERT, Clinical BioBERT）和领域特定（PubMedBERT）组。我们的SUFO分析揭示了：(1)

    Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability. We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1
    
[^131]: 大型语言模型在知识冲突中的行为揭秘：自适应变色龙还是固执的树獭

    Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes. (arXiv:2305.13300v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13300](http://arxiv.org/abs/2305.13300)

    本文研究了大型语言模型（LLMs）在遭遇知识冲突时的行为。结果发现，LLMs可以高度接受外部连贯且有说服力的证据，即使与其参数化内存存在冲突，但也可能有局限性。

    

    通过向大型语言模型（LLMs）提供外部信息，工具增强（包括检索增强）已成为解决LLMs静态参数化内存限制的有希望的解决方案。然而，当这些证据与它们的参数化内存发生冲突时，LLMs对这些外部证据有多少接受能力？我们提出了一个系统性的框架来从LLMs中获取高质量的参数化内存，并构建相应的对立内存，从而使我们能够进行一系列受控实验。我们的调查揭示了LLMs表现出看似矛盾的行为。一方面，与以往的观念不同，我们发现，只要外部证据是连贯且有说服力的，LLMs即使与其参数化内存存在冲突也可以高度接受外部证据。另一方面，LLMs也可能会表现出局限性，尤其是当其参数化内存受到威胁时。

    By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory. However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory? We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts. We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. Our investigation reveals seemingly contradicting behaviors of LLMs. On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. On the other hand, LLMs also d
    
[^132]: 通过逻辑驱动的数据增强增强大型语言模型的逻辑推理能力

    Enhancing Logical Reasoning of Large Language Models through Logic-Driven Data Augmentation. (arXiv:2305.12599v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12599](http://arxiv.org/abs/2305.12599)

    本论文介绍了一种通过逻辑驱动的数据增强方法来增强大型语言模型的逻辑推理能力。通过将原始文本转换为抽象意义表述图，并对其进行逻辑修改和转换，生成增强数据，从而提升模型性能。该方法适用于各种体系结构的大型语言模型。

    

    将大型语言模型与逻辑推理相结合可以增强它们在问题解决中的能力，使其更加强大和可靠。然而，逻辑推理的复杂性使得从网页上收集可靠的数据来建立全面的训练数据集面临困难，进而影响下游任务的性能。为了解决这个问题，我们提出了一种新颖的逻辑驱动数据增强方法，AMR-LDA。AMR-LDA将原始文本转换成抽象意义表示（AMR）图，这是一种结构化的语义表示，包含了句子的逻辑结构，然后对该图进行操作以生成逻辑修改后的AMR图。修改后的AMR图随后被转换回文本，从而创建增强数据。值得注意的是，我们的方法与体系结构无关，并通过提示增强来增强生成型大型语言模型（如GPT-3.5和GPT-4），并通过微调来增强判别型大型语言模型。

    Combining large language models with logical reasoning enhance their capacity to address problems in a robust and reliable manner. Nevertheless, the intricate nature of logical reasoning poses challenges to gathering reliable data from web for building comprehensive training datasets, subsequently affecting the performance on downstream tasks. To address this, we introduce a novel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the original text into an Abstract Meaning Representation (AMR) graph, a structured semantic representation that encapsulates the logic structure of the sentence, upon which operations are performed to generate logically modified AMR graphs. The modified AMR graphs are subsequently converted back into texts to create augmented data. Notably, our methodology is architecture-agnostic and enhances generative large language models, such as GPT-3.5 and GPT-4, through prompt augmentation, and fine-tuning discriminative large language models through 
    
[^133]: 利用骨架辅助的提示传递进行少样本对话摘要

    Few-Shot Dialogue Summarization via Skeleton-Assisted Prompt Transfer. (arXiv:2305.12077v1 [cs.CL])

    [http://arxiv.org/abs/2305.12077](http://arxiv.org/abs/2305.12077)

    该论文提出了一种利用骨架辅助的提示传递进行少样本对话摘要的方法，利用骨架生成作为额外的监督来更好地消耗对话状态信息，并提出了一种新型模型SkeletonNet进行骨架生成，实现了最先进的性能。

    

    在现实场景中，对话摘要的标记样本通常是有限的（即少样本），因为为高质量的对话摘要付出高昂的注释成本。为了有效地从少样本样本中学习，先前的工作利用了其他下游任务的海量注释数据，并在提示调整中执行提示传递，以实现跨任务知识传输。然而，现有的通用提示传递技术缺乏对对话特定信息的考虑。本文专注于改善从对话状态跟踪到对话摘要的提示传递，并提出了骨架辅助的提示传递（SAPT），它利用骨架生成作为额外的监督，作为连接不同源和目标任务的媒介，使模型更好地消耗对话状态信息。为了自动提取对话骨架作为骨架生成的受监督训练数据，我们设计了一种名为SkeletonNet的新型少样本对话摘要模型，其中涉及一个专门设计的骨架生成模块。实验结果表明，我们提出的SAPT在两个少样本对话摘要基准数据集上实现了最先进的性能。

    In real-world scenarios, labeled samples for dialogue summarization are usually limited (i.e., few-shot) due to high annotation costs for high-quality dialogue summaries. To efficiently learn from few-shot samples, previous works have utilized massive annotated data from other downstream tasks and then performed prompt transfer in prompt tuning so as to enable cross-task knowledge transfer. However, existing general-purpose prompt transfer techniques lack consideration for dialogue-specific information. In this paper, we focus on improving the prompt transfer from dialogue state tracking to dialogue summarization and propose Skeleton-Assisted Prompt Transfer (SAPT), which leverages skeleton generation as extra supervision that functions as a medium connecting the distinct source and target task and resulting in the model's better consumption of dialogue state information. To automatically extract dialogue skeletons as supervised training data for skeleton generation, we design a novel 
    
[^134]: NevIR: 神经信息检索中的否定

    NevIR: Negation in Neural Information Retrieval. (arXiv:2305.07614v1 [cs.IR])

    [http://arxiv.org/abs/2305.07614](http://arxiv.org/abs/2305.07614)

    本研究探讨了否定在神经信息检索中的影响，构建了基准模型，结果表明当前信息检索模型大多数都没有考虑否定，而交叉编码器是目前表现最好的架构。

    

    否定是一种常见而日常化的现象，也一直是语言模型的一个弱点。虽然信息检索领域采用了语言模型作为现代化架构的主干，但几乎没有研究深入了解否定对神经信息检索的影响。因此，我们构建了一个简单的基准来研究这个主题：要求信息检索模型对仅仅因为是否定而不同的两个文档进行排名。我们发现，结果根据不同的信息检索架构而有很大差异：交叉编码器表现最好，后期交互模型次之，而双编码器和稀疏神经架构排名最后。我们发现，大多数当前的信息检索模型都没有考虑否定，表现与随机排名相似或更差。我们证明，尽管在一个包含否定对照文档的数据集上继续微调明显的方法可以提高性能（模型大小也是如此），但是机器和人之间仍存在很大的差距。

    Negation is a common everyday phenomena and has been a consistent area of weakness for language models (LMs). Although the Information Retrieval (IR) community has adopted LMs as the backbone of modern IR architectures, there has been little to no research in understanding how negation impacts neural IR. We therefore construct a straightforward benchmark on this theme: asking IR models to rank two documents that differ only by negation. We show that the results vary widely according to the type of IR architecture: cross-encoders perform best, followed by late-interaction models, and in last place are bi-encoder and sparse neural architectures. We find that most current information retrieval models do not consider negation, performing similarly or worse than randomly ranking. We show that although the obvious approach of continued fine-tuning on a dataset of contrastive documents containing negations increases performance (as does model size), there is still a large gap between machine 
    
[^135]: PersonaLLM: 探究GPT-3.5表达个性特征和性别差异的能力

    PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences. (arXiv:2305.02547v1 [cs.CL])

    [http://arxiv.org/abs/2305.02547](http://arxiv.org/abs/2305.02547)

    本文探究了基于LLMs模拟代理的行为，称之为LLM Personas，在分配大五人格类型和性别角色时是否可以生成具有一致性的个性化特质的内容。

    

    尽管大型语言模型在各个行业的聊天机器人设计中有许多用途，并且研究表明个性化聊天机器人在满足不同人格特征方面的重要性，但很少有研究评估个性化LLM的行为是否能够准确、一致地反映某些人格特征。我们考虑研究基于LLM的模拟代理的行为，称之为LLM personas，并使用GPT-3.5（text-davinci-003）进行案例研究，以研究LLM在分配大五人格类型和性别角色时是否可以生成具有一致性的个性化特质的内容。我们创建了320个LLM personas（每种大五人格类型有5个女性和5个男性），并提示他们完成经典的44项大五人格问卷（BFI），然后撰写一个关于他们童年的800字故事。结果表明，LLM personas的自我报告的BFI分数与他们分配的人格类型一致。

    Despite the many use cases for large language models (LLMs) in the design of chatbots in various industries and the research showing the importance of personalizing chatbots to cater to different personality traits, little work has been done to evaluate whether the behaviors of personalized LLMs can reflect certain personality traits accurately and consistently. We consider studying the behavior of LLM-based simulated agents which refer to as LLM personas and present a case study with GPT-3.5 (text-davinci-003) to investigate whether LLMs can generate content with consistent, personalized traits when assigned Big Five personality types and gender roles. We created 320 LLM personas (5 females and 5 males for each of the 32 Big Five personality types) and prompted them to complete the classic 44-item Big Five Inventory (BFI) and then write an 800-word story about their childhood. Results showed that LLM personas' self-reported BFI scores are consistent with their assigned personality typ
    
[^136]: HeySQuAD: 一个口语化问答数据集

    HeySQuAD: A Spoken Question Answering Dataset. (arXiv:2304.13689v1 [cs.CL])

    [http://arxiv.org/abs/2304.13689](http://arxiv.org/abs/2304.13689)

    HeySQuAD 是一个大规模口语化问答数据集，旨在衡量机器理解并回答嘈杂的口语提问的能力，同时使用转录的人类口语提问进行训练能显著提高模型表现。

    

    人类口语提问对于评估口语问答系统的性能至关重要，尤其是数字助手等多个实际应用场景。本文提出了一个新的大规模社区共享的口语问答数据集 HeySQuAD，它由76k个人类口语提问、97k个机器生成的问题以及相应的文本答案组成，这些答案源自 SQuAD QA 数据集。HeySQuAD 的目标是衡量机器理解嘈杂的口语提问并准确回答问题的能力。为此，我们在人类口语和机器生成的问题上进行了广泛的基准测试，以量化来自两方面噪声的差异及对模型和回答准确度的影响。重要的是，在口语问答任务中，我们希望回答的是人类口语提问，我们观察到使用转录的人类口语提问和原始 SQuAD 问题进行训练，能够显著提高（12.51%）模型的表现，而不是仅使用原始 SQuAD 数据集。

    Human-spoken questions are critical to evaluating the performance of spoken question answering (SQA) systems that serve several real-world use cases including digital assistants. We present a new large-scale community-shared SQA dataset, HeySQuAD that consists of 76k human-spoken questions and 97k machine-generated questions and corresponding textual answers derived from the SQuAD QA dataset. The goal of HeySQuAD is to measure the ability of machines to understand noisy spoken questions and answer the questions accurately. To this end, we run extensive benchmarks on the human-spoken and machine-generated questions to quantify the differences in noise from both sources and its subsequent impact on the model and answering accuracy. Importantly, for the task of SQA, where we want to answer human-spoken questions, we observe that training using the transcribed human-spoken and original SQuAD questions leads to significant improvements (12.51%) over training using only the original SQuAD te
    
[^137]: 在开放域问答中防御误导性攻击

    Defending Against Misinformation Attacks in Open-Domain Question Answering. (arXiv:2212.10002v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10002](http://arxiv.org/abs/2212.10002)

    本文提出了一种使用查询扩充来搜索冗余信息、并通过新颖的置信度方法将其集成到模型中的方法，可以有效防御开放域问答系统中的污染攻击，精确匹配率可提高近20%。

    

    最近在开放域问答领域中的研究表明，对于搜索集合进行的敌对污染可能会导致生产系统的精度大幅下降。然而，几乎没有工作提出防御这些攻击的方法。为了解决这个问题，我们依赖于大型语料库中存在冗余信息的直觉。为了找到这些信息，我们引入了一种使用查询扩充来搜索可能回答原始问题的多样化段落集合的方法，但是不太可能被污染。我们通过设计一种新型的置信度方法（比较预测答案与其在检索到的上下文中出现的情况——我们称之为答案冗余置信度，即CAR）将这些新段落集成到模型中。这些方法共同构成了一种简单但有效的方式，用于防御污染攻击，可在不同水平的数据污染/知识冲突下提供近20％的精确匹配增益。

    Recent work in open-domain question answering (ODQA) has shown that adversarial poisoning of the search collection can cause large drops in accuracy for production systems. However, little to no work has proposed methods to defend against these attacks. To do so, we rely on the intuition that redundant information often exists in large corpora. To find it, we introduce a method that uses query augmentation to search for a diverse set of passages that could answer the original question but are less likely to have been poisoned. We integrate these new passages into the model through the design of a novel confidence method, comparing the predicted answer to its appearance in the retrieved contexts (what we call \textit{Confidence from Answer Redundancy}, i.e. CAR). Together these methods allow for a simple but effective way to defend against poisoning attacks that provides gains of nearly 20\% exact match across varying levels of data poisoning/knowledge conflicts.
    

