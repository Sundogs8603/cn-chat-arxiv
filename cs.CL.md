# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [GANTEE: Generative Adversatial Network for Taxonomy Entering Evaluation.](http://arxiv.org/abs/2303.14480) | GANTEE 是一种用于分类法添加评估的生成对抗网络，通过使用生成对抗网络和新的综合数据生成方法，它具有比其他先前方法更高效和更有效的性能。 |
| [^2] | [Informed Machine Learning, Centrality, CNN, Relevant Document Detection, Repatriation of Indigenous Human Remains.](http://arxiv.org/abs/2303.14475) | 本篇研究介绍了研发出一个机器学习系统的进展, 该系统可以自动查找并语义分析相关文献,以协助土著人类寻找其被盗窃、捐赠、出售或在机构之间交换的遗骸信息。 |
| [^3] | [Indian Language Summarization using Pretrained Sequence-to-Sequence Models.](http://arxiv.org/abs/2303.14461) | 本文介绍了使用预训练序列到序列模型进行印度语言文本摘要的研究。该研究在ILSUM shared task任务中获得了三个子任务的第一名，广泛分析了有限数据大小下的k倍交叉验证对结果的影响，并进行了各种实验验证了预训练模型的有效性。 |
| [^4] | [Sem4SAP: Synonymous Expression Mining From Open Knowledge Graph For Language Model Synonym-Aware Pretraining.](http://arxiv.org/abs/2303.14425) | Sem4SAP利用开放知识图谱中挖掘到的同义词进行同义词感知预训练，扩展了同义词的应用范围，并提出两种新颖而有效的同义词感知预训练方法。实验结果表明，Sem4SAP可以显著提高模型质量。 |
| [^5] | [Natural Language Processing in Ethiopian Languages: Current State, Challenges, and Opportunities.](http://arxiv.org/abs/2303.14406) | 本论文调查了四种埃塞俄比亚语言的自然语言处理现状，提出了关键挑战和机遇，并提供了一个资源库以促进该领域的未来研究。 |
| [^6] | [Knowledge-augmented Frame Semantic Parsing with Hybrid Prompt-tuning.](http://arxiv.org/abs/2303.14375) | 本论文提出了一种新颖的基于知识增强的框架语义解析体系结构，通过将精确的框架知识与预训练语言模型结合，优化了语义表示和语义解析的准确度，实验结果表明其具有很好的性能。 |
| [^7] | [An Analysis of GPT-3's Performance in Grammatical Error Correction.](http://arxiv.org/abs/2303.14342) | 本文分析了GPT-3模型在语法纠错任务上的表现，通过实验测试了几种不同的提示方式，揭示了人类评分者与基于参考的自动度量之间的差异。 |
| [^8] | [SmartBook: AI-Assisted Situation Report Generation.](http://arxiv.org/abs/2303.14337) | SmartBook是一种AI辅助的情报报告生成工具，通过消耗大量新闻数据生成一个结构化的情况报告，其中包含多个假设（主张），并与事实依据建立丰富的关联。在Ukraine-Russia危机中，机器生成的报告以时间轴的形式结构化，显著减少了报告时间，并通过生成比人类同行更全面、准确和一致的报告来提高报告质量。 |
| [^9] | [The Semantic Reader Project: Augmenting Scholarly Documents through AI-Powered Interactive Reading Interfaces.](http://arxiv.org/abs/2303.14334) | 本文探讨了利用人工智能和人机交互技术为研究论文提供智能、交互式和无障碍的阅读界面的可行性，并介绍了跨机构合作的语义阅读器项目。 |
| [^10] | [GPT is becoming a Turing machine: Here are some ways to program it.](http://arxiv.org/abs/2303.14310) | 本论文探讨通过适当的提示，GPT-3模型可以执行计算机科学中包括循环在内的多种流行算法。通过自我注意力的团结可以触发迭代的执行和描述。IRSA可用于教育，具有很好的应用前景。 |
| [^11] | [Voice-Based Conversational Agents and Knowledge Graphs for Improving News Search in Assisted Living.](http://arxiv.org/abs/2303.14286) | 本论文提出了基于语音对话代理和知识图谱的辅助居住新闻搜索系统，可以使老年人和慢性病患者更轻松和直观地找到所需信息。 |
| [^12] | [Depression detection in social media posts using affective and social norm features.](http://arxiv.org/abs/2303.14279) | 本文提出了一种利用情感和社会规范特征在社交媒体帖子中检测抑郁症的深度架构，结果表明亵渎和道德特征对于抑郁症的检测是重要的，该方法在两个数据集下均达到最先进的检测效果。 |
| [^13] | [SIGMORPHON 2023 Shared Task of Interlinear Glossing: Baseline Model.](http://arxiv.org/abs/2303.14234) | 本文介绍了SIGMORPHON 2023交互示范的基线系统，利用变压器架构，将内插发生视为序列标记任务，实现了IGT的自动处理。 |
| [^14] | [Lay Text Summarisation Using Natural Language Processing: A Narrative Literature Review.](http://arxiv.org/abs/2303.14222) | 本综述总结了使用自然语言处理生成平民易懂摘要的不同方法，并发现基于转换器的方法最有效，为缓解研究人员负担和促进社会与科学交流提供了帮助。 |
| [^15] | [MUG: A General Meeting Understanding and Generation Benchmark.](http://arxiv.org/abs/2303.13939) | 本文建立了一个通用的会议理解和生成基准(MUG)，以评估一系列自然语言处理任务的性能，为口语语言处理技术的发展提供支持 |
| [^16] | [Fairness-guided Few-shot Prompting for Large Language Models.](http://arxiv.org/abs/2303.13217) | 本文提出了一种新的搜索策略-FairPrompt，在保证公正性的前提下，通过评估提示预测偏差，确定近似最优的提示，从而改进大型语言模型的上下文学习性能，实验表明该方法在准确性和公正性方面均优于现有方法。 |
| [^17] | [SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization.](http://arxiv.org/abs/2303.13035) | 研究通过引入软提示嵌入，提出Soft Prompt-Based Calibration (SPeC)管道，来减轻输入变量对输出多样性的影响，降低性能变异. 此方法不仅比大语言模型(LLM)性能稳定，而且在临床笔记摘要任务上表现优于最先进的模型. |
| [^18] | [Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning Skills of LLMs.](http://arxiv.org/abs/2303.12810) | 本文探究了大型语言模型(LLM)在不同领域推理任务上的表现，并发现LLM在类比和道德推理方面表现出色，在空间推理任务上表现较差。这对于LLM未来的发展具有重要意义。 |
| [^19] | [SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency.](http://arxiv.org/abs/2303.11525) | 本研究提出了一种名为SIFT的方法，用于提高深度神经网络的训练效率、准确性和表示能力，通过稀疏等FLOP转换，缩短训练时间。 |
| [^20] | [Self-Consistent Learning: Cooperation between Generators and Discriminators.](http://arxiv.org/abs/2303.09075) | 本文提出了一个自一致学习的框架，通过鉴别器和生成器的合作训练，解决了标准GAN训练不稳定、样本容易偏离实际数据分布、鉴别模型改进饱和等问题。实验结果表明，该模型不仅优于最先进的GAN，在文本和图像生成任务中也实现了高质量的合成。 |
| [^21] | [GPT-4 Technical Report.](http://arxiv.org/abs/2303.08774) | GPT-4是一个大规模多模态模型，可以接收图像和文本输入并产生文本输出，能够在各种专业和学术基准测试中表现出人类水平的表现，包括通过模拟的律师考试。该项目的核心组件是开发基础设施和优化方法，可在广泛的规模范围内表现预测性。 |
| [^22] | [NL4Opt Competition: Formulating Optimization Problems Based on Their Natural Language Descriptions.](http://arxiv.org/abs/2303.08233) | NL4Opt比赛旨在研究如何从自然语言描述中提取出优化问题的含义和表述，并通过自然语言与非专业人士进行交互。竞赛分为两个子任务：(1) 识别和标记对应于优化问题组件的语义实体;(2)从检测到的问题实体生成意义表示(即逻辑形式)。 |
| [^23] | [MEDBERT.de: A Comprehensive German BERT Model for the Medical Domain.](http://arxiv.org/abs/2303.08179) | 本文介绍了medBERT.de，这是一个用于德语医学领域的BERT模型，通过在大规模语料库上的训练，在八个不同的医学基准测试中取得最新的最先进的表现。该模型对长文本特别有用，而数据去重和有效的分词则只对模型性能产生了较小的影响。 |
| [^24] | [Audio Visual Language Maps for Robot Navigation.](http://arxiv.org/abs/2303.07522) | 该论文提出了一种音视语言地图(AVLMaps)，用于存储跨模态信息，实现机器人根据多模态查询在地图中索引目标的导航方式。在模拟实验中，AVLMaps实现了从多模态提示的零次学习式多模态目标导航，并提供了更好的召回率。 |
| [^25] | [Parachute: Evaluating Interactive Human-LM Co-writing Systems.](http://arxiv.org/abs/2303.06333) | 本文提出了一个以人为中心的评估框架Parachute，用于交互式共同撰写系统的评估，该框架包含了分类的实用指标，可以用于评估和比较共同撰写系统。 |
| [^26] | [ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction.](http://arxiv.org/abs/2303.05063) | 这篇论文提出了一个简单而有效的上下文学习框架ICL-D3IE，这个框架使LLM在不同类型演示下的DIE任务中表现出色，具有改进性能的反馈机制，同时涵盖了位置和格式方面的演示示例。 |
| [^27] | [Model-Agnostic Meta-Learning for Natural Language Understanding Tasks in Finance.](http://arxiv.org/abs/2303.02841) | 本文研究了金融领域自然语言理解任务中的模型无关元学习算法，取得了最先进的性能表现。 |
| [^28] | [Linear Spaces of Meanings: Compositional Structures in Vision-Language Models.](http://arxiv.org/abs/2302.14383) | 本文研究了视觉语言模型中的组合结构，并提出了一种使用嵌入空间中较小集合的向量组合来近似表示来自编码器的表示形式的方法，将这些向量视为“理想单词”，并在CLIP的嵌入中以实验方式探索了这些结构的可用性。 |
| [^29] | [Chain of Hindsight Aligns Language Models with Feedback.](http://arxiv.org/abs/2302.02676) | 该研究提出了一种新颖的技术，即回顾链，可以轻松优化，并可以从任何形式的反馈中学习，而不受其极性的影响。 |
| [^30] | [Editing Language Model-based Knowledge Graph Embeddings.](http://arxiv.org/abs/2301.10405) | 本文提出了一种新的任务——编辑基于语言模型的知识图谱嵌入，旨在实现对KG嵌入的数据高效和快速更新。针对这一任务，提出了一个简单而强大的方案——KGEditor，可以更好地更新特定事实而不影响其余部分的性能。 |
| [^31] | [Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning.](http://arxiv.org/abs/2212.01117) | 本文提出了一种基于Prompt学习和传播结构的零样本谣言检测框架，其能够有效地检测不同领域和语言的谣言，并可适应非预料中断事件的影响。 |
| [^32] | [Multi-Modal Few-Shot Temporal Action Detection.](http://arxiv.org/abs/2211.14905) | 提出了一个新的多模态少样本时间动作检测问题，针对这个问题提出了一个新的 MUPPET 方法，通过在视觉-语言模型中构建多模态提示，并使用多模态聚类算法来组合时序连续的片段，解决了问题。在少样本和零样本场景下表现出了优越性，并验证了不同组件的有效性。 |
| [^33] | [Seeing What You Miss: Vision-Language Pre-training with Semantic Completion Learning.](http://arxiv.org/abs/2211.13437) | 本文提出了一种新的“语义完成学习”任务，以改善现有蒙面建模任务忽略全局语义特征的问题，通过捕捉相应蒙面数据的缺失语义来补充它们的信息。 |
| [^34] | [Visually Grounded Commonsense Knowledge Acquisition.](http://arxiv.org/abs/2211.12054) | 本文介绍了CLEVER，一种以视觉-语言预训练模型为基础的常识知识提取方法，通过包含有关实体对的图像包汇总出常识关系，避免了对图像实例进行人工注释的问题。 |
| [^35] | [Deep Temporal Modelling of Clinical Depression through Social Media Text.](http://arxiv.org/abs/2211.07717) | 本文通过使用抑郁症状检测分类器，从社交媒体文本提取临床相关特征，建立了一个模型用于检测用户的临床抑郁症，通过提供不同时间粒度的准确度度量来评估该模型。 |
| [^36] | [Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models.](http://arxiv.org/abs/2210.16433) | 本文提出了一种名为「Knowledge-in-Context」的半参数语言模型架构，通过外部存储器带入各种类型的知识以帮助解决自然语言处理任务，并且可以自适应地选择最有用的知识片段。 |
| [^37] | [End-to-End Entity Detection with Proposer and Regressor.](http://arxiv.org/abs/2210.10260) | 该论文提出了一种基于提议器和回归器的端到端实体检测方法，通过利用特征金字塔网络生成高质量的实体提议，并对提议进行精细调整以生成最终的预测结果。该模型具有查询语义丰富、实体定位精度高、模型训练容易等优点，还引入了空间调制变压器来增强内部关系的建模能力。实验结果表明，该方法显著优于现有的最先进方法。 |
| [^38] | [Zero-Shot On-the-Fly Event Schema Induction.](http://arxiv.org/abs/2210.06254) | 通过利用大型语言模型生成源文档，可以在零样本的情况下即时生成任何主题的完整事件模式，比人工策划的模式更完整。 |
| [^39] | [MAP: Multimodal Uncertainty-Aware Vision-Language Pre-training Model.](http://arxiv.org/abs/2210.05335) | 本文提出了一种利用概率分布编码器进行多模态不确定性建模的预训练模型MAP，该模型在多项下游任务中均表现优异，超越了现有模型。 |
| [^40] | [An Equal-Size Hard EM Algorithm for Diverse Dialogue Generation.](http://arxiv.org/abs/2209.14627) | 本文提出了一种平衡约束的等大小硬EM算法，用于训练多解码器模型以实现多样的对话生成，可在小型模型中生成高质量的多样化响应。 |
| [^41] | [Multilingual Bidirectional Unsupervised Translation Through Multilingual Finetuning and Back-Translation.](http://arxiv.org/abs/2209.02821) | 该论文提出了一种两阶段的方法，实现单个NMT模型对未见过的语言同英语之间的双向无监督翻译。该方法包括多语言微调和双向回译，成功提高了翻译质量。 |
| [^42] | [Efficient Methods for Natural Language Processing: A Survey.](http://arxiv.org/abs/2209.00099) | 这篇论文调查了当前高效NLP方法和研究结果，旨在在有限的资源下为进行NLP提供指南，并指向开发更有效方法的有希望的研究方向。 |
| [^43] | [Dual-Stream Transformer for Generic Event Boundary Captioning.](http://arxiv.org/abs/2207.03038) | 本文提出了一种双流Transformer的通用事件边界字幕生成方法，结合多个预训练模型和边界类型提示，以及单词级别的集成策略，实现了生成更人性化的字幕，并在GEBC测试集上取得了令人满意的结果。 |
| [^44] | [Does Transliteration Help Multilingual Language Modeling?.](http://arxiv.org/abs/2201.12501) | 本文探究了把使用不同书写系统的相近语言音译成同一种书写系统，对于多语言语言模型的提升的影响，发现音译可以提高低资源语言的表现，而不会对资源相对较高的语言产生负面影响。 |

# 详细

[^1]: GANTEE：用于分类法添加评估的生成对抗网络

    GANTEE: Generative Adversatial Network for Taxonomy Entering Evaluation. (arXiv:2303.14480v1 [cs.AI])

    [http://arxiv.org/abs/2303.14480](http://arxiv.org/abs/2303.14480)

    GANTEE 是一种用于分类法添加评估的生成对抗网络，通过使用生成对抗网络和新的综合数据生成方法，它具有比其他先前方法更高效和更有效的性能。

    

    分类法被制定为支持许多下游任务的有向无环概念图或树。许多新概念需要添加到现有分类法中。然而，传统的分类法扩展任务仅旨在找到新概念在现有分类法中的最佳位置，但在实际场景中存在两个缺点。先前的方法效率低下，因为当大多数新概念实际上是噪声概念时，它们会浪费很多时间。它们也因仅从现有分类法中收集训练样本而限制了模型挖掘真实概念之间更多的上下位关系的能力。本文提出了一个可插拔的框架，称为用于分类法添加评估的生成敌对网络（GANTEE），以减轻这些缺点。在该框架中设计了一种生成对抗网络，通过辨别模型减轻第一个缺点，并引入一种新的综合数据生成方法以缓解第二个缺点。与先前的方法相比，GANTEE框架表现出更优异的性能。

    Taxonomy is formulated as directed acyclic concepts graphs or trees that support many downstream tasks. Many new coming concepts need to be added to an existing taxonomy. The traditional taxonomy expansion task aims only at finding the best position for new coming concepts in the existing taxonomy. However, they have two drawbacks when being applied to the real-scenarios. The previous methods suffer from low-efficiency since they waste much time when most of the new coming concepts are indeed noisy concepts. They also suffer from low-effectiveness since they collect training samples only from the existing taxonomy, which limits the ability of the model to mine more hypernym-hyponym relationships among real concepts. This paper proposes a pluggable framework called Generative Adversarial Network for Taxonomy Entering Evaluation (GANTEE) to alleviate these drawbacks. A generative adversarial network is designed in this framework by discriminative models to alleviate the first drawback an
    
[^2]: 信息学习、中心性、卷积神经网络、相关文献检测、土著人类遗骸归还

    Informed Machine Learning, Centrality, CNN, Relevant Document Detection, Repatriation of Indigenous Human Remains. (arXiv:2303.14475v1 [cs.CL])

    [http://arxiv.org/abs/2303.14475](http://arxiv.org/abs/2303.14475)

    本篇研究介绍了研发出一个机器学习系统的进展, 该系统可以自动查找并语义分析相关文献,以协助土著人类寻找其被盗窃、捐赠、出售或在机构之间交换的遗骸信息。

    

    澳大利亚和其他原住民面临的紧迫问题之一是将他们祖先的尸体遗骸归还到西方科学机构。成功将这些遗骸返还到其社区以重新安葬，主要取决于在1790年至1970年期间发表的科学和其他文献中找到记录它们被盗窃、捐赠、出售或在机构之间交换的信息。本文报道了由数据科学家和社会科学研究人员在“研究、和解、更新”网络（RRR）中进行的协作研究，以开发和应用文本挖掘技术来确定这些关键信息。我们描述了我们迄今为止开发基于机器学习的解决方案以自动化查找和语义分析相关文本的工作。分类模型，特别是基于深度学习的模型，在使用少量标记数据进行训练时精度低。

    Among the pressing issues facing Australian and other First Nations peoples is the repatriation of the bodily remains of their ancestors, which are currently held in Western scientific institutions. The success of securing the return of these remains to their communities for reburial depends largely on locating information within scientific and other literature published between 1790 and 1970 documenting their theft, donation, sale, or exchange between institutions. This article reports on collaborative research by data scientists and social science researchers in the Research, Reconcile, Renew Network (RRR) to develop and apply text mining techniques to identify this vital information. We describe our work to date on developing a machine learning-based solution to automate the process of finding and semantically analysing relevant texts. Classification models, particularly deep learning-based models, are known to have low accuracy when trained with small amounts of labelled (i.e. rele
    
[^3]: 使用预先训练的序列到序列模型的印度语言摘要

    Indian Language Summarization using Pretrained Sequence-to-Sequence Models. (arXiv:2303.14461v1 [cs.CL])

    [http://arxiv.org/abs/2303.14461](http://arxiv.org/abs/2303.14461)

    本文介绍了使用预训练序列到序列模型进行印度语言文本摘要的研究。该研究在ILSUM shared task任务中获得了三个子任务的第一名，广泛分析了有限数据大小下的k倍交叉验证对结果的影响，并进行了各种实验验证了预训练模型的有效性。

    

    ILSUM共享任务专注于对印地语、古吉拉特语和英语三种主要语言进行文本摘要。在这个任务中，我们尝试了各种预先训练的序列到序列模型，以找出每种语言最佳的模型。本文提供了这些模型和我们方法的详细概述。我们在所有三个子任务（英语，印地语和古吉拉特语）中获得了第一名。我们还对有限数据大小进行了k倍交叉验证的影响进行了广泛分析，并进行了对原始数据和经过筛选的数据的组合进行各种实验，以确定预训练模型的有效性。

    The ILSUM shared task focuses on text summarization for two major Indian languages- Hindi and Gujarati, along with English. In this task, we experiment with various pretrained sequence-to-sequence models to find out the best model for each of the languages. We present a detailed overview of the models and our approaches in this paper. We secure the first rank across all three sub-tasks (English, Hindi and Gujarati). This paper also extensively analyzes the impact of k-fold cross-validation while experimenting with limited data size, and we also perform various experiments with a combination of the original and a filtered version of the data to determine the efficacy of the pretrained models.
    
[^4]: Sem4SAP: 基于开放知识图谱进行同义表达式挖掘，为语言模型同义词感知预训练提供支持

    Sem4SAP: Synonymous Expression Mining From Open Knowledge Graph For Language Model Synonym-Aware Pretraining. (arXiv:2303.14425v1 [cs.CL])

    [http://arxiv.org/abs/2303.14425](http://arxiv.org/abs/2303.14425)

    Sem4SAP利用开放知识图谱中挖掘到的同义词进行同义词感知预训练，扩展了同义词的应用范围，并提出两种新颖而有效的同义词感知预训练方法。实验结果表明，Sem4SAP可以显著提高模型质量。

    

    对于许多下游任务而言，模型理解同义表达式的能力至关重要。这将使模型更好地理解上下文之间的相似性，并更具有抵御同义词替换攻击的鲁棒性。本文提出了一种称为Sem4SAP的框架，从开放知识图谱（Open-KG）中挖掘同义词，并利用挖掘到的同义词进行语言模型同义词感知预训练。我们提出简要过滤Open-KG中的内容，并利用频率信息来更好地帮助低资源无监督条件下的聚类过程。我们通过迁移同义表达式之间的核心语义来扩展挖掘到的同义词。我们还提出了两种新颖而有效的同义词感知预训练方法，用于将同义词知识注入PLMs (Pretrained Language Model)中。大量实验表明，Sem4SAP可以显著提高模型质量。

    The model's ability to understand synonymous expression is crucial in many kinds of downstream tasks. It will make the model to better understand the similarity between context, and more robust to the synonym substitution attack. However, many Pretrained Language Model (PLM) lack synonym knowledge due to limitation of small-scale synsets and PLM's pretraining objectives. In this paper, we propose a framework called Sem4SAP to mine synsets from Open Knowledge Graph (Open-KG) and using the mined synsets to do synonym-aware pretraining for language models. We propose to coarsly filter the content in Open-KG and use the frequency information to better help the clustering process under low-resource unsupervised conditions. We expand the mined synsets by migrating core semantics between synonymous expressions.We also propose two novel and effective synonym-aware pre-training methods for injecting synonym knowledge into PLMs.Extensive experiments demonstrate that Sem4SAP can dramatically outp
    
[^5]: 埃塞俄比亚语言中的自然语言处理：现状、挑战和机遇

    Natural Language Processing in Ethiopian Languages: Current State, Challenges, and Opportunities. (arXiv:2303.14406v1 [cs.CL])

    [http://arxiv.org/abs/2303.14406](http://arxiv.org/abs/2303.14406)

    本论文调查了四种埃塞俄比亚语言的自然语言处理现状，提出了关键挑战和机遇，并提供了一个资源库以促进该领域的未来研究。

    

    本综述调查了埃塞俄比亚的四种语言（阿姆哈拉语、奥罗莫语、提格利尼亚语和沃拉伊塔语）的自然语言处理（NLP）目前的状况。通过本文，我们确定了埃塞俄比亚NLP研究的关键挑战和机遇。此外，我们提供了一个GitHub上的集中存储库，其中包含这些语言的各种NLP任务的公共资源。该存储库可以定期更新，从其他研究人员的贡献中得到完善。我们的目标是确定研究空白并将信息传播给对埃塞俄比亚语言感兴趣的NLP研究者，并鼓励未来在这一领域的研究。

    This survey delves into the current state of natural language processing (NLP) for four Ethiopian languages: Amharic, Afaan Oromo, Tigrinya, and Wolaytta. Through this paper, we identify key challenges and opportunities for NLP research in Ethiopia. Furthermore, we provide a centralized repository on GitHub that contains publicly available resources for various NLP tasks in these languages. This repository can be updated periodically with contributions from other researchers. Our objective is to identify research gaps and disseminate the information to NLP researchers interested in Ethiopian languages and encourage future research in this domain.
    
[^6]: 基于知识增强的混合提示调整框架语义解析

    Knowledge-augmented Frame Semantic Parsing with Hybrid Prompt-tuning. (arXiv:2303.14375v1 [cs.CL])

    [http://arxiv.org/abs/2303.14375](http://arxiv.org/abs/2303.14375)

    本论文提出了一种新颖的基于知识增强的框架语义解析体系结构，通过将精确的框架知识与预训练语言模型结合，优化了语义表示和语义解析的准确度，实验结果表明其具有很好的性能。

    

    基于框架语义的方法已经成为语义解析任务中的主流方法，但是在不同的上下文中消歧目标词汇所唤起的框架表示仍然具有挑战性。预训练语言模型（PLMs）已被用于语义解析，并且显著提高了神经解析器的准确性。然而，基于PLMs的方法往往偏爱训练数据中出现的联合模式，导致输出结果不准确。本文提出了一种新颖的基于知识增强的框架语义解析体系结构，通过将精确的框架知识与PLMs结合，增强了语义表示。具体来说，设计了一个基于记忆的知识提取模块（MKEM）来选择准确的框架知识并在高级语言表示层中构建连续模板。然后，采用混合提示调整机制来引导神经网络将带有语义领域知识的内容集成到输出结果中。实验结果表明，这种方法在多个数据集上实现了优越的性能。

    Frame semantics-based approaches have been widely used in semantic parsing tasks and have become mainstream. It remains challenging to disambiguate frame representations evoked by target lexical units under different contexts. Pre-trained Language Models (PLMs) have been used in semantic parsing and significantly improve the accuracy of neural parsers. However, the PLMs-based approaches tend to favor collocated patterns presented in the training data, leading to inaccurate outcomes. The intuition here is to design a mechanism to optimally use knowledge captured in semantic frames in conjunction with PLMs to disambiguate frames. We propose a novel Knowledge-Augmented Frame Semantic Parsing Architecture (KAF-SPA) to enhance semantic representation by incorporating accurate frame knowledge into PLMs during frame semantic parsing. Specifically, a Memory-based Knowledge Extraction Module (MKEM) is devised to select accurate frame knowledge and construct the continuous templates in the high 
    
[^7]: GPT-3在语法纠错上性能的分析

    An Analysis of GPT-3's Performance in Grammatical Error Correction. (arXiv:2303.14342v1 [cs.CL])

    [http://arxiv.org/abs/2303.14342](http://arxiv.org/abs/2303.14342)

    本文分析了GPT-3模型在语法纠错任务上的表现，通过实验测试了几种不同的提示方式，揭示了人类评分者与基于参考的自动度量之间的差异。

    

    GPT-3模型具有很高的自然语言处理能力，在各种任务上表现出色。然而，目前对它们在语法纠错(GEC)任务上的表现缺乏详细的分析。因此，我们对GPT-3模型（text-davinci-003版本）进行了实验，测试了几种不同的提示方式，包括零样本学习和少样本学习。我们分析了使用不同提示格式遇到的有趣或有问题的输出。我们使用自动评估和人类评价相结合的方法，报告了我们最佳提示在BEA-2019和JFLEG数据集上的表现，揭示了人类评分者与基于参考的自动度量之间的有趣差异。

    GPT-3 models are very powerful, achieving high performance on a variety of natural language processing tasks. However, there is a relative lack of detailed published analysis on how well they perform on the task of grammatical error correction (GEC). To address this, we perform experiments testing the capabilities of a GPT-3 model (text-davinci-003) against major GEC benchmarks, comparing the performance of several different prompts, including a comparison of zero-shot and few-shot settings. We analyze intriguing or problematic outputs encountered with different prompt formats. We report the performance of our best prompt on the BEA-2019 and JFLEG datasets using a combination of automatic metrics and human evaluations, revealing interesting differences between the preferences of human raters and the reference-based automatic metrics.
    
[^8]: SmartBook：AI辅助的情报报告生成

    SmartBook: AI-Assisted Situation Report Generation. (arXiv:2303.14337v1 [cs.CL])

    [http://arxiv.org/abs/2303.14337](http://arxiv.org/abs/2303.14337)

    SmartBook是一种AI辅助的情报报告生成工具，通过消耗大量新闻数据生成一个结构化的情况报告，其中包含多个假设（主张），并与事实依据建立丰富的关联。在Ukraine-Russia危机中，机器生成的报告以时间轴的形式结构化，显著减少了报告时间，并通过生成比人类同行更全面、准确和一致的报告来提高报告质量。

    

    新兴事件，如COVID疫情和乌克兰危机，需要时间敏感的全面了解情况，以便进行适当的决策和有效的行动响应。自动生成情报报告可以大大减少领域专家准备官方人工策划报告的时间、精力和成本。然而，AI研究在这个目标方面非常有限，还没有成功的试验来自动化这种报告生成。我们提出了SmartBook，一种新颖的任务分解，旨在生成情况报告，在大量新闻数据的基础上生成一个结构化的情况报告，其中包含多个假设（主张），并与事实依据建立丰富的关联。我们通过自动生成情报分析报告来实现SmartBook，以协助专家分析师处理乌克兰-俄罗斯危机。机器生成的报告以时间轴的形式结构化，每个事件都与相关的演员、位置和因果关系相关联。我们的评估显示，SmartBook可以显著减少报告时间，并通过生成比人类同行更全面、准确和一致的报告来提高报告质量。

    Emerging events, such as the COVID pandemic and the Ukraine Crisis, require a time-sensitive comprehensive understanding of the situation to allow for appropriate decision-making and effective action response. Automated generation of situation reports can significantly reduce the time, effort, and cost for domain experts when preparing their official human-curated reports. However, AI research toward this goal has been very limited, and no successful trials have yet been conducted to automate such report generation. We propose SmartBook, a novel task formulation targeting situation report generation, which consumes large volumes of news data to produce a structured situation report with multiple hypotheses (claims) summarized and grounded with rich links to factual evidence. We realize SmartBook for the Ukraine-Russia crisis by automatically generating intelligence analysis reports to assist expert analysts. The machine-generated reports are structured in the form of timelines, with ea
    
[^9]: 语义阅读器项目：利用人工智能驱动的交互式阅读界面增强学术文档

    The Semantic Reader Project: Augmenting Scholarly Documents through AI-Powered Interactive Reading Interfaces. (arXiv:2303.14334v1 [cs.DL])

    [http://arxiv.org/abs/2303.14334](http://arxiv.org/abs/2303.14334)

    本文探讨了利用人工智能和人机交互技术为研究论文提供智能、交互式和无障碍的阅读界面的可行性，并介绍了跨机构合作的语义阅读器项目。

    

    学术出版物是学者向他人传递知识的关键。然而，研究论文信息密集，随着科学文献量的增长，需要新技术支持阅读过程。与通过互联网技术转变的查找论文过程不同，阅读研究论文的体验几十年来几乎没有改变。虽然PDF格式因其便携性而广泛使用，但它有重大缺点，包括：静态内容，低视觉读者的可访问性差，以及在移动设备上阅读困难。本文探讨“最近的AI和HCI进展能否为遗留的PDF提供智能，交互式和无障碍的阅读界面？”，我们描述了语义阅读器项目，这是多个机构的协作努力，旨在探索为研究论文自动创建动态阅读界面的方法。

    Scholarly publications are key to the transfer of knowledge from scholars to others. However, research papers are information-dense, and as the volume of the scientific literature grows, the need for new technology to support the reading process grows. In contrast to the process of finding papers, which has been transformed by Internet technology, the experience of reading research papers has changed little in decades. The PDF format for sharing research papers is widely used due to its portability, but it has significant downsides including: static content, poor accessibility for low-vision readers, and difficulty reading on mobile devices. This paper explores the question "Can recent advances in AI and HCI power intelligent, interactive, and accessible reading interfaces -- even for legacy PDFs?" We describe the Semantic Reader Project, a collaborative effort across multiple institutions to explore automatic creation of dynamic reading interfaces for research papers. Through this pro
    
[^10]: GPT 正成为图灵机：这里是一些编程方法

    GPT is becoming a Turing machine: Here are some ways to program it. (arXiv:2303.14310v1 [cs.CL])

    [http://arxiv.org/abs/2303.14310](http://arxiv.org/abs/2303.14310)

    本论文探讨通过适当的提示，GPT-3模型可以执行计算机科学中包括循环在内的多种流行算法。通过自我注意力的团结可以触发迭代的执行和描述。IRSA可用于教育，具有很好的应用前景。

    

    我们通过适当的提示，展示了 GPT-3 模型族可以触发迭代行为，以执行（而不仅仅是写入或回忆）涉及循环的程序，包括计算机科学课程或软件开发者面试中发现的几种流行算法。我们通过自我注意力的团结（IRSA）在三种方式之一（或组合）中触发迭代的执行和描述：1）在一个特定输入的目标程序的执行路径示例中使用强烈的重复结构，2）提示执行路径的片段，以及3）明确禁止（跳过）生成的文本的某些自我注意力。在动态程序执行中，IRSA 带来了比用更强大的 GPT-4 替换模型更大的准确性提高。IRSA 在教育中具有应用前景，因为提示和响应类似于数据结构和算法课堂的学生作业。我们的发现具有隐含的

    We demonstrate that, through appropriate prompting, GPT-3 family of models can be triggered to perform iterative behaviours necessary to execute (rather than just write or recall) programs that involve loops, including several popular algorithms found in computer science curricula or software developer interviews. We trigger execution and description of Iterations by Regimenting Self-Attention (IRSA) in one (or a combination) of three ways: 1) Using strong repetitive structure in an example of an execution path of a target program for one particular input, 2) Prompting with fragments of execution paths, and 3) Explicitly forbidding (skipping) self-attention to parts of the generated text. On a dynamic program execution, IRSA leads to larger accuracy gains than replacing the model with the much more powerful GPT-4. IRSA has promising applications in education, as the prompts and responses resemble student assignments in data structures and algorithms classes. Our findings hold implicati
    
[^11]: 基于语音对话代理和知识图谱的辅助居住新闻搜索系统

    Voice-Based Conversational Agents and Knowledge Graphs for Improving News Search in Assisted Living. (arXiv:2303.14286v1 [cs.CL])

    [http://arxiv.org/abs/2303.14286](http://arxiv.org/abs/2303.14286)

    本论文提出了基于语音对话代理和知识图谱的辅助居住新闻搜索系统，可以使老年人和慢性病患者更轻松和直观地找到所需信息。

    

    随着医疗保健领域面临着人口老龄化、人手短缺和常见慢性病等重大挑战，向个体提供高质量的护理已变得非常困难。对话代理已被证明是缓解这些问题的一种有前途的技术。它们可以作为数字健康助手，潜在地改善老年人和慢性病患者的日常生活，包括服药提醒、例行检查或社交闲聊。此外，对话代理可以满足人们对日常新闻或本地活动信息的基本需求，使个人能够保持对周围世界的了解和联系。然而，对于那些可能技术素养有限或健康相关能力受限的人来说，寻找相关新闻来源和浏览网上大量的新闻文章可能会让他们感到不知所措。为了解决这一挑战，我们提出了一种智能的辅助居住新闻搜索系统，它结合了基于语音的对话代理和知识图谱。该系统允许用户使用自然语言与新闻搜索系统交互，使他们更容易和更直观地找到所需信息。该系统还使用知识图谱为用户提供个性化和上下文相关的内容推荐。我们的初步评估表明，所提出的方法有潜力显著改善辅助居住环境中的老年人和慢性病患者的新闻搜索体验。

    As the healthcare sector is facing major challenges, such as aging populations, staff shortages, and common chronic diseases, delivering high-quality care to individuals has become very difficult. Conversational agents have shown to be a promising technology to alleviate some of these issues. In the form of digital health assistants, they have the potential to improve the everyday life of the elderly and chronically ill people. This includes, for example, medication reminders, routine checks, or social chit-chat. In addition, conversational agents can satisfy the fundamental need of having access to information about daily news or local events, which enables individuals to stay informed and connected with the world around them. However, finding relevant news sources and navigating the plethora of news articles available online can be overwhelming, particularly for those who may have limited technological literacy or health-related impairments. To address this challenge, we propose an i
    
[^12]: 利用情感和社会规范特征在社交媒体帖子中检测抑郁症

    Depression detection in social media posts using affective and social norm features. (arXiv:2303.14279v1 [cs.CL])

    [http://arxiv.org/abs/2303.14279](http://arxiv.org/abs/2303.14279)

    本文提出了一种利用情感和社会规范特征在社交媒体帖子中检测抑郁症的深度架构，结果表明亵渎和道德特征对于抑郁症的检测是重要的，该方法在两个数据集下均达到最先进的检测效果。

    

    本文提出了一种基于深度架构的方法，从社交媒体帖子中检测抑郁症。该方法基于BERT从社交媒体帖子中提取语言表示，并利用一个具有注意力机制的双向GRU网络将这些表示组合起来。我们利用预先训练的情感分类器提取的特征来增加文本表示的情感信息。我们参考心理学文献的建议，提出使用后期融合方案将帖子和单词的亵渎和道德特征纳入到我们的架构中。结果表明，亵渎和道德特征对于抑郁症的检测是重要的。我们将该模型应用于Reddit Pirina数据集上的抑郁症检测，并进一步考虑在Reddit RSDD数据集中检测发帖用户的抑郁症状态，该数据集考虑了每个用户的多个帖子。采用我们提出的特征的结果在两个设置下均取得了最先进的成果，即2.65%和6.73%的绝对误差率。

    We propose a deep architecture for depression detection from social media posts. The proposed architecture builds upon BERT to extract language representations from social media posts and combines these representations using an attentive bidirectional GRU network. We incorporate affective information, by augmenting the text representations with features extracted from a pretrained emotion classifier. Motivated by psychological literature we propose to incorporate profanity and morality features of posts and words in our architecture using a late fusion scheme. Our analysis indicates that morality and profanity can be important features for depression detection. We apply our model for depression detection on Reddit posts on the Pirina dataset, and further consider the setting of detecting depressed users, given multiple posts per user, proposed in the Reddit RSDD dataset. The inclusion of the proposed features yields state-of-the-art results in both settings, namely 2.65% and 6.73% abso
    
[^13]: SIGMORPHON 2023交互示范的基线模型

    SIGMORPHON 2023 Shared Task of Interlinear Glossing: Baseline Model. (arXiv:2303.14234v1 [cs.CL])

    [http://arxiv.org/abs/2303.14234](http://arxiv.org/abs/2303.14234)

    本文介绍了SIGMORPHON 2023交互示范的基线系统，利用变压器架构，将内插发生视为序列标记任务，实现了IGT的自动处理。

    

    语言文献是语言保护的关键方面，通常包括创建交互式标注文本（IGT）。创建IGT费时且繁琐，自动化处理可以节省宝贵的人力成本。本文介绍了SIGMORPHON 2023交互示范的基线系统。在我们的系统中，我们利用变压器架构，并将内插发生视为序列标记任务。

    Language documentation is a critical aspect of language preservation, often including the creation of Interlinear Glossed Text (IGT). Creating IGT is time-consuming and tedious, and automating the process can save valuable annotator effort.  This paper describes the baseline system for the SIGMORPHON 2023 Shared Task of Interlinear Glossing. In our system, we utilize a transformer architecture and treat gloss generation as a sequence labelling task.
    
[^14]: 自然语言处理在非专业人士文本摘要中的应用：一篇叙述性文献综述

    Lay Text Summarisation Using Natural Language Processing: A Narrative Literature Review. (arXiv:2303.14222v1 [cs.CL])

    [http://arxiv.org/abs/2303.14222](http://arxiv.org/abs/2303.14222)

    本综述总结了使用自然语言处理生成平民易懂摘要的不同方法，并发现基于转换器的方法最有效，为缓解研究人员负担和促进社会与科学交流提供了帮助。

    

    以平民易懂的语言总结研究结果对于促进公众了解研究成果至关重要。利用自然语言处理生成简化版摘要有望缓解研究人员的工作负担，弥合科学与社会之间的差距。这篇叙述性文献综述的目的是描述和比较不同的文本摘要方法，以生成平民易懂的摘要。我们在Web of Science，Google Scholar，IEEE Xplore，Association for Computing Machinery Digital Library 和 arXiv 等数据库中搜索了2022年5月6日以前发表的文章。我们包括了关于用于生成平民易懂的摘要的自动文本摘要方法的原始研究。我们筛选了82篇文章，并包括了在2020年至2021年期间发布的8篇相关论文，全部使用相同的数据集。结果表明，基于转换器的方法（如最近流行的Bidirectional Encoder Representations from Transformers（BERT）和使用抽取间隔句进行预训练的模型）是最有效的生成平民易懂的摘要的方法。

    Summarisation of research results in plain language is crucial for promoting public understanding of research findings. The use of Natural Language Processing to generate lay summaries has the potential to relieve researchers' workload and bridge the gap between science and society. The aim of this narrative literature review is to describe and compare the different text summarisation approaches used to generate lay summaries. We searched the databases Web of Science, Google Scholar, IEEE Xplore, Association for Computing Machinery Digital Library and arXiv for articles published until 6 May 2022. We included original studies on automatic text summarisation methods to generate lay summaries. We screened 82 articles and included eight relevant papers published between 2020 and 2021, all using the same dataset. The results show that transformer-based methods such as Bidirectional Encoder Representations from Transformers (BERT) and Pre-training with Extracted Gap-sentences for Abstractiv
    
[^15]: MUG: 一项通用的会议理解与生成基准

    MUG: A General Meeting Understanding and Generation Benchmark. (arXiv:2303.13939v1 [cs.CL])

    [http://arxiv.org/abs/2303.13939](http://arxiv.org/abs/2303.13939)

    本文建立了一个通用的会议理解和生成基准(MUG)，以评估一系列自然语言处理任务的性能，为口语语言处理技术的发展提供支持

    

    听取视频会议和在线课程的长时间音频记录以获取信息极为低效。即使自动语音识别系统将记录转录为长形式的口语语言文档，仅阅读语音识别转录也只能在一定程度上加快寻找信息的速度。研究表明，关键词提取、主题分割和摘要等一系列自然语言处理应用显著提高用户获得重要信息的效率。会议场景是应用这些口语语言处理能力最有价值的场景之一。然而，缺乏大规模公开的会议数据集对这些自然语言处理任务的进展产生了严重阻碍。为了促进自然语言处理技术的发展，我们建立了一个大规模的通用会议理解和生成基准(MUG)，以评估一系列自然语言处理任务的性能，包括主题分割、话题级别和会话级别的摘要提取等。

    Listening to long video/audio recordings from video conferencing and online courses for acquiring information is extremely inefficient. Even after ASR systems transcribe recordings into long-form spoken language documents, reading ASR transcripts only partly speeds up seeking information. It has been observed that a range of NLP applications, such as keyphrase extraction, topic segmentation, and summarization, significantly improve users' efficiency in grasping important information. The meeting scenario is among the most valuable scenarios for deploying these spoken language processing (SLP) capabilities. However, the lack of large-scale public meeting datasets annotated for these SLP tasks severely hinders their advancement. To prompt SLP advancement, we establish a large-scale general Meeting Understanding and Generation Benchmark (MUG) to benchmark the performance of a wide range of SLP tasks, including topic segmentation, topic-level and session-level extractive summarization and 
    
[^16]: 大型语言模型的公正引导少样本提示

    Fairness-guided Few-shot Prompting for Large Language Models. (arXiv:2303.13217v1 [cs.CL])

    [http://arxiv.org/abs/2303.13217](http://arxiv.org/abs/2303.13217)

    本文提出了一种新的搜索策略-FairPrompt，在保证公正性的前提下，通过评估提示预测偏差，确定近似最优的提示，从而改进大型语言模型的上下文学习性能，实验表明该方法在准确性和公正性方面均优于现有方法。

    

    大型语言模型已经表现出惊人的能力，能够通过几个输入输出示例构建的提示进行直接应用来解决众多下游任务。但是，先前的研究表明，由于训练示例，示例顺序和提示格式的变化导致上下文学习容易出现高度不稳定性。因此，构建适当的提示对于改进上下文学习的性能至关重要。在这篇文章中，我们从预测偏差的角度重新探讨了这个问题。具体而言，我们引入了一个指标来评估固定提示相对于标签或给定属性的预测偏差。然后我们通过实验证明了预测偏差较大的提示总是导致不令人满意的预测质量。基于这个观察，我们提出了一种新的搜索策略，基于贪婪搜索来确定近似最优的提示，从而改进上下文学习的性能。我们提出的方法叫做"公正提示"，其中融入了公平性约束，以指导搜索不展现出对某些人群的偏见。我们在多种少样本分类任务上证明了FairPrompt的有效性，并展示了它在准确性和公正性方面均优于现有的最先进方法。

    Large language models have demonstrated surprising ability to perform in-context learning, i.e., these models can be directly applied to solve numerous downstream tasks by conditioning on a prompt constructed by a few input-output examples. However, prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats. Therefore, the construction of an appropriate prompt is essential for improving the performance of in-context learning. In this paper, we revisit this problem from the view of predictive bias. Specifically, we introduce a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes. Then we empirically show that prompts with higher bias always lead to unsatisfactory predictive quality. Based on this observation, we propose a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context l
    
[^17]: SPeC：软提示校准在临床笔记摘要中降低性能变异的研究

    SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization. (arXiv:2303.13035v1 [cs.CL])

    [http://arxiv.org/abs/2303.13035](http://arxiv.org/abs/2303.13035)

    研究通过引入软提示嵌入，提出Soft Prompt-Based Calibration (SPeC)管道，来减轻输入变量对输出多样性的影响，降低性能变异. 此方法不仅比大语言模型(LLM)性能稳定，而且在临床笔记摘要任务上表现优于最先进的模型.

    

    电子健康记录（EHR）存储着包括病历、诊断、治疗和检测结果在内的大量患者信息。这些记录对于医疗保健专业人员做出明智的患者护理决策非常关键。摘要临床笔记可以帮助医疗保健专业人员更好地发现潜在健康风险，以及做出更好的决策。这一过程通过确保医疗保健专业人员可以访问最相关和最新的患者数据，有助于减少错误并提高患者的护理效果。最近的研究表明，将提示与大语言模型（LLM）相结合可以显著提高摘要任务的效率。然而，我们发现这种方法也会导致输出方差增加，即使提示意义相似，输出也会有明显的差异。为了解决这一挑战，我们引入了一个模型无关的软提示校准（SPeC）流程，该流程采用软提示嵌入来减轻输入变量对输出多样性的影响。我们的实验表明，SPeC不仅可以降低LLM的性能变异，而且在临床笔记摘要任务上优于现有的最先进模型。

    Electronic health records (EHRs) store an extensive array of patient information, encompassing medical histories, diagnoses, treatments, and test outcomes. These records are crucial for enabling healthcare providers to make well-informed decisions regarding patient care. Summarizing clinical notes further assists healthcare professionals in pinpointing potential health risks and making better-informed decisions. This process contributes to reducing errors and enhancing patient outcomes by ensuring providers have access to the most pertinent and current patient data. Recent research has shown that incorporating prompts with large language models (LLMs) substantially boosts the efficacy of summarization tasks. However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings. To tackle this challenge, we introduce a model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft prom
    
[^18]: LLM是万能的大师吗？探索LLM的领域不可知推理技能。

    Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning Skills of LLMs. (arXiv:2303.12810v1 [cs.CL])

    [http://arxiv.org/abs/2303.12810](http://arxiv.org/abs/2303.12810)

    本文探究了大型语言模型(LLM)在不同领域推理任务上的表现，并发现LLM在类比和道德推理方面表现出色，在空间推理任务上表现较差。这对于LLM未来的发展具有重要意义。

    

    大型语言模型(LLM)类似于人类推理的潜力一直是机器学习界争议最激烈的话题之一。然而，人类的推理能力是多方面的，可以通过各种形式进行体现，包括类比、空间和道德推理等。这一事实引发了一个问题，LLM能否在所有这些不同领域中同样表现出色。本研究旨在通过直接使用或从现有类比和空间推理数据集中汲取启示，对LLM在不同推理任务上的表现进行研究。此外，为了评估LLM像人类一样推理的能力，研究还对更开放、自然的语言问题进行了评估。我的研究结果表明，LLM在类比和道德推理方面表现出色，但在空间推理任务上表现得不够熟练。我认为这些实验对于推动LLM未来的发展，特别是在改进空间推理能力方面具有重要意义。

    The potential of large language models (LLMs) to reason like humans has been a highly contested topic in Machine Learning communities. However, the reasoning abilities of humans are multifaceted and can be seen in various forms, including analogical, spatial and moral reasoning, among others. This fact raises the question whether LLMs can perform equally well across all these different domains. This research work aims to investigate the performance of LLMs on different reasoning tasks by conducting experiments that directly use or draw inspirations from existing datasets on analogical and spatial reasoning. Additionally, to evaluate the ability of LLMs to reason like human, their performance is evaluted on more open-ended, natural language questions. My findings indicate that LLMs excel at analogical and moral reasoning, yet struggle to perform as proficiently on spatial reasoning tasks. I believe these experiments are crucial for informing the future development of LLMs, particularly 
    
[^19]: SIFT: 稀疏等FLOP转换以最大限度提高训练效率

    SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency. (arXiv:2303.11525v1 [cs.LG])

    [http://arxiv.org/abs/2303.11525](http://arxiv.org/abs/2303.11525)

    本研究提出了一种名为SIFT的方法，用于提高深度神经网络的训练效率、准确性和表示能力，通过稀疏等FLOP转换，缩短训练时间。

    

    最近的研究探索了使用权重稀疏性来改善深度神经网络（DNN）的训练效率（与训练FLOPS相关的测试准确性）。 这些工作旨在减少训练FLOP，但使用稀疏权重进行训练通常会导致准确性损失或需要更长的训练周期，使得结果的训练效率不够清晰。 相比之下，我们专注于使用稀疏性提高准确性，同时使用与密集模型相同的FLOPS，并通过更高的准确性展示训练效率提高。 在本文中，我们介绍了SIFT，一组用作密集层的即插即用替代品来提高其表示能力和FLOP效率的稀疏等FLOP转换。 每个转换都由一个单一参数（稀疏级别）参数化，并提供更大的搜索空间以找到最佳的稀疏掩膜。

    Recent works have explored the use of weight sparsity to improve the training efficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs). These works aim to reduce training FLOPs but training with sparse weights often leads to accuracy loss or requires longer train schedules, making the resulting training efficiency less clear. In contrast, we focus on using sparsity to increase accuracy while using the same FLOPS as the dense model and show training efficiency gains through higher accuracy. In this work, we introduce SIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in replacements for dense layers to improve their representational capacity and FLOP efficiency. Each transformation is parameterized by a single parameter (sparsity level) and provides a larger search space to find optimal sparse masks. Without changing any training hyperparameters, replacing dense layers with SIFT leads to significant improvements across computer vision (CV) and
    
[^20]: 自一致学习：生成器和鉴别器的合作

    Self-Consistent Learning: Cooperation between Generators and Discriminators. (arXiv:2303.09075v1 [cs.CL])

    [http://arxiv.org/abs/2303.09075](http://arxiv.org/abs/2303.09075)

    本文提出了一个自一致学习的框架，通过鉴别器和生成器的合作训练，解决了标准GAN训练不稳定、样本容易偏离实际数据分布、鉴别模型改进饱和等问题。实验结果表明，该模型不仅优于最先进的GAN，在文本和图像生成任务中也实现了高质量的合成。

    

    最近，使用生成的数据来提高下游鉴别模型的性能已经因预训练语言模型的巨大发展而广受欢迎。在大多数先前的研究中，生成模型和鉴别模型是分别训练的，因此它们不能适应彼此的任何变化。因此，生成的样本很容易偏离实际数据分布，而鉴别模型的改进很快就会达到饱和。生成对抗网络（GAN）通过一种对抗性过程与鉴别模型训练生成模型以实现联合训练。然而，标准GAN的训练极不稳定，往往难以收敛。在本文中，为了解决这些问题，我们提出了一个自一致学习框架，其中一个鉴别器和一个生成器以闭环形式合作训练。鉴别器和生成器在多轮更新中相互增强，生成的样本逐渐接近实际数据分布，而鉴别模型不断提高其性能。实验结果表明，我们的模型不仅在各种数据集上优于最先进的GAN，而且在文本和图像生成任务中实现了高质量的合成。

    Using generated data to improve the performance of downstream discriminative models has recently gained popularity due to the great development of pre-trained language models. In most previous studies, generative models and discriminative models are trained separately and thus could not adapt to any changes in each other. As a result, the generated samples can easily deviate from the real data distribution, while the improvement of the discriminative model quickly reaches saturation. Generative adversarial networks (GANs) train generative models via an adversarial process with discriminative models to achieve joint training. However, the training of standard GANs is notoriously unstable and often falls short of convergence. In this paper, to address these issues, we propose a $\textit{self-consistent learning}$ framework, in which a discriminator and a generator are cooperatively trained in a closed-loop form. The discriminator and the generator enhance each other during multiple round
    
[^21]: GPT-4技术报告

    GPT-4 Technical Report. (arXiv:2303.08774v1 [cs.CL])

    [http://arxiv.org/abs/2303.08774](http://arxiv.org/abs/2303.08774)

    GPT-4是一个大规模多模态模型，可以接收图像和文本输入并产生文本输出，能够在各种专业和学术基准测试中表现出人类水平的表现，包括通过模拟的律师考试。该项目的核心组件是开发基础设施和优化方法，可在广泛的规模范围内表现预测性。

    

    我们报告了GPT-4的开发，它是一个可以接受图像和文本输入并产生文本输出的大规模多模态模型。虽然在许多现实场景中不如人类，但GPT-4在各种专业和学术基准测试中表现出人类水平的表现，包括通过模拟的律师考试，成绩排名在前10％左右。GPT-4是一个基于Transformer的模型，预训练用于预测文档中的下一个标记。后训练对齐过程提高了事实性和符合期望行为的性能指标。项目的核心组件是开发基础设施和优化方法，可在广泛的规模范围内表现预测性。这使我们能够准确预测GPT-4的某些性能方面，而这些性能是基于使用不超过GPT-4计算能力的1/1,000的模型训练的。

    We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.
    
[^22]: NL4Opt 比赛：基于自然语言描述构建优化问题

    NL4Opt Competition: Formulating Optimization Problems Based on Their Natural Language Descriptions. (arXiv:2303.08233v1 [cs.CL])

    [http://arxiv.org/abs/2303.08233](http://arxiv.org/abs/2303.08233)

    NL4Opt比赛旨在研究如何从自然语言描述中提取出优化问题的含义和表述，并通过自然语言与非专业人士进行交互。竞赛分为两个子任务：(1) 识别和标记对应于优化问题组件的语义实体;(2)从检测到的问题实体生成意义表示(即逻辑形式)。

    

    自然语言优化（NL4Opt）竞赛旨在研究如何根据优化问题的文本描述提取其含义和表述的方法。具体而言，该竞赛的目标是通过使用自然语言中介来使非专业人士能够接口使用优化求解器，以增加其可访问性和可用性。我们将这一挑战性目标分为两个子任务：(1)识别和标记对应于优化问题组件的语义实体;(2)从检测到的问题实体生成意义表示(即逻辑形式)。第一个任务旨在通过检测和标记优化问题的实体来减少歧义。第二个任务创建了一个线性规划(LP)问题的中间表示，该中间表示被转换为商用求解器可用的格式。在本报告中，我们介绍了LP单词问题数据集和NL4Opt比赛的共享任务，并总结了竞赛条目的结果。

    The Natural Language for Optimization (NL4Opt) Competition was created to investigate methods of extracting the meaning and formulation of an optimization problem based on its text description. Specifically, the goal of the competition is to increase the accessibility and usability of optimization solvers by allowing non-experts to interface with them using natural language. We separate this challenging goal into two sub-tasks: (1) recognize and label the semantic entities that correspond to the components of the optimization problem; (2) generate a meaning representation (i.e., a logical form) of the problem from its detected problem entities. The first task aims to reduce ambiguity by detecting and tagging the entities of the optimization problems. The second task creates an intermediate representation of the linear programming (LP) problem that is converted into a format that can be used by commercial solvers. In this report, we present the LP word problem dataset and shared tasks f
    
[^23]: MEDBERT.de：一个基于德语的、针对医学领域专门设计的全面BERT模型

    MEDBERT.de: A Comprehensive German BERT Model for the Medical Domain. (arXiv:2303.08179v1 [cs.CL])

    [http://arxiv.org/abs/2303.08179](http://arxiv.org/abs/2303.08179)

    本文介绍了medBERT.de，这是一个用于德语医学领域的BERT模型，通过在大规模语料库上的训练，在八个不同的医学基准测试中取得最新的最先进的表现。该模型对长文本特别有用，而数据去重和有效的分词则只对模型性能产生了较小的影响。

    

    本文介绍了medBERT.de，这是一个针对德语医学领域专门设计的预训练BERT模型。该模型已经在470万份德语医学文档的大型语料库上进行了训练，并在八个不同的医学基准测试中取得了新的最先进的效果，涉及各种学科和医学文献类型。除了评估该模型的整体性能外，本文还对其能力进行了更深入的分析。我们研究了数据去重对模型性能的影响，以及使用更有效的分词方法的潜在好处。我们的结果表明，像medBERT.de这样的领域专用模型特别适用于较长的文本，并且数据去重不一定会导致性能改善。此外，我们发现有效的分词只在提高模型性能方面发挥了较小的作用，并且大多数改进源于模型的预训练。

    This paper presents medBERT.de, a pre-trained German BERT model specifically designed for the German medical domain. The model has been trained on a large corpus of 4.7 Million German medical documents and has been shown to achieve new state-of-the-art performance on eight different medical benchmarks covering a wide range of disciplines and medical document types. In addition to evaluating the overall performance of the model, this paper also conducts a more in-depth analysis of its capabilities. We investigate the impact of data deduplication on the model's performance, as well as the potential benefits of using more efficient tokenization methods. Our results indicate that domain-specific models such as medBERT.de are particularly useful for longer texts, and that deduplication of training data does not necessarily lead to improved performance. Furthermore, we found that efficient tokenization plays only a minor role in improving model performance, and attribute most of the improved
    
[^24]: 机器人导航的音视语言地图

    Audio Visual Language Maps for Robot Navigation. (arXiv:2303.07522v1 [cs.RO])

    [http://arxiv.org/abs/2303.07522](http://arxiv.org/abs/2303.07522)

    该论文提出了一种音视语言地图(AVLMaps)，用于存储跨模态信息，实现机器人根据多模态查询在地图中索引目标的导航方式。在模拟实验中，AVLMaps实现了从多模态提示的零次学习式多模态目标导航，并提供了更好的召回率。

    

    与世界的互动是一种多感官的体验，但是许多机器人仍然主要依赖视觉感知来绘制和导航他们的环境。本文提出了音视语言地图(AVLMaps)，这是一个统一的3D空间地图表示，用于存储来自音频、视觉和语言线索的跨模态信息。在导航的情境下，我们展示了AVLMaps能够使机器人系统根据多模态查询(例如，文本描述、图像或地标的音频片段)在地图中索引目标。特别是，添加音频信息使机器人能够更可靠地消除目标位置的歧义性。在模拟实验中，我们展示了AVLMaps能够实现从多模态提示进行零次学习的多模态目标导航，并在模糊场景中提供50%更好的召回率。

    While interacting in the world is a multi-sensory experience, many robots continue to predominantly rely on visual perception to map and navigate in their environments. In this work, we propose Audio-Visual-Language Maps (AVLMaps), a unified 3D spatial map representation for storing cross-modal information from audio, visual, and language cues. AVLMaps integrate the open-vocabulary capabilities of multimodal foundation models pre-trained on Internet-scale data by fusing their features into a centralized 3D voxel grid. In the context of navigation, we show that AVLMaps enable robot systems to index goals in the map based on multimodal queries, e.g., textual descriptions, images, or audio snippets of landmarks. In particular, the addition of audio information enables robots to more reliably disambiguate goal locations. Extensive experiments in simulation show that AVLMaps enable zero-shot multimodal goal navigation from multimodal prompts and provide 50% better recall in ambiguous scenar
    
[^25]: 降落伞：评估交互式人机共同撰写系统

    Parachute: Evaluating Interactive Human-LM Co-writing Systems. (arXiv:2303.06333v1 [cs.HC])

    [http://arxiv.org/abs/2303.06333](http://arxiv.org/abs/2303.06333)

    本文提出了一个以人为中心的评估框架Parachute，用于交互式共同撰写系统的评估，该框架包含了分类的实用指标，可以用于评估和比较共同撰写系统。

    This paper proposes a human-centered evaluation framework, Parachute, for interactive co-writing systems, which includes categorized practical metrics and can be used to evaluate and compare co-writing systems.

    语言模型的飞速发展引起了人们对于利用语言模型构建共同撰写系统的极大兴趣，其中人类和语言模型交互地为共同的写作成果做出贡献。然而，缺乏对于交互式环境下共同撰写系统的评估研究。我们提出了一个以人为中心的评估框架Parachute，用于交互式共同撰写系统的评估。Parachute展示了交互评估的综合视角，其中每个评估方面都包含了分类的实用指标。此外，我们提供了一个使用案例来演示如何使用Parachute评估和比较共同撰写系统。

    A surge of advances in language models (LMs) has led to significant interest in using LMs to build co-writing systems, in which humans and LMs interactively contribute to a shared writing artifact. However, there is a lack of studies assessing co-writing systems in interactive settings. We propose a human-centered evaluation framework, Parachute, for interactive co-writing systems. Parachute showcases an integrative view of interaction evaluation, where each evaluation aspect consists of categorized practical metrics. Furthermore, we present Parachute with a use case to demonstrate how to evaluate and compare co-writing systems using Parachute.
    
[^26]: ICL-D3IE：上下文学习+多样展示更新，用于文档信息抽取（arXiv:2303.05063v2 [cs.CL] UPDATED）

    ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction. (arXiv:2303.05063v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.05063](http://arxiv.org/abs/2303.05063)

    这篇论文提出了一个简单而有效的上下文学习框架ICL-D3IE，这个框架使LLM在不同类型演示下的DIE任务中表现出色，具有改进性能的反馈机制，同时涵盖了位置和格式方面的演示示例。

    

    大型语言模型（LLM）如GPT-3和ChatGPT在各种自然语言处理（NLP）任务中展示了卓越的成果，尤其是应用于上下文学习，即基于少量演示示例进行推理。尽管在NLP任务中取得了成功，但尚未进行研究以评估LLM在使用上下文学习执行文档信息抽取（DIE）的能力。应用LLM执行DIE存在两个挑战：模态和任务差距。为此，我们提出了一种简单而有效的上下文学习框架ICL-D3IE，它使LLM能够使用不同类型的演示示例执行DIE。具体而言，我们从难以训练的文档中提取最困难和最不同的片段作为演示示例，以便受益于所有测试实例。我们设计了描述关系的演示示例，使LLM能够理解位置关系。我们引入了格式化演示示例，以方便提取答案。此外，我们采用了反馈机制，更新了演示示例，以进一步提高ICL-D3IE的性能。

    Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated remarkable results in various natural language processing (NLP) tasks with in-context learning, which involves inference based on a few demonstration examples. Despite their successes in NLP tasks, no investigation has been conducted to assess the ability of LLMs to perform document information extraction (DIE) using in-context learning. Applying LLMs to DIE poses two challenges: the modality and task gap. To this end, we propose a simple but effective in-context learning framework called ICL-D3IE, which enables LLMs to perform DIE with different types of demonstration examples. Specifically, we extract the most difficult and distinct segments from hard training documents as hard demonstrations for benefiting all test instances. We design demonstrations describing relationships that enable LLMs to understand positional relationships. We introduce formatting demonstrations for easy answer extraction. Additionally
    
[^27]: 金融自然语言理解任务中的模型无关元学习

    Model-Agnostic Meta-Learning for Natural Language Understanding Tasks in Finance. (arXiv:2303.02841v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.02841](http://arxiv.org/abs/2303.02841)

    本文研究了金融领域自然语言理解任务中的模型无关元学习算法，取得了最先进的性能表现。

    

    金融领域的自然语言理解因缺乏标注数据和特殊语言而具有挑战性。近年来，研究人员提出使用预训练语言模型和多任务学习来学习稳健的表示。然而，过度微调经常导致过拟合，多任务学习可能会偏袒大量数据的任务。为了解决这些问题，本文研究了低资源金融自然语言理解任务中的模型无关元学习算法。我们的贡献包括：1.我们探索了使用多种类型任务的MAML方法的性能：GLUE数据集，SNLI，Sci-Tail和Financial PhraseBank；2.我们研究了在一个真实场景的基于推特文本的股票价格预测问题中使用MAML方法的性能。根据实验结果，我们的模型实现了最先进的性能，证明了我们的方法可以快速适应。

    Natural language understanding(NLU) is challenging for finance due to the lack of annotated data and the specialized language in that domain. As a result, researchers have proposed to use pre-trained language model and multi-task learning to learn robust representations. However, aggressive fine-tuning often causes over-fitting and multi-task learning may favor tasks with significantly larger amounts data, etc. To address these problems, in this paper, we investigate model-agnostic meta-learning algorithm(MAML) in low-resource financial NLU tasks. Our contribution includes: 1. we explore the performance of MAML method with multiple types of tasks: GLUE datasets, SNLI, Sci-Tail and Financial PhraseBank; 2. we study the performance of MAML method with multiple single-type tasks: a real scenario stock price prediction problem with twitter text data. Our models achieve the state-of-the-art performance according to the experimental results, which demonstrate that our method can adapt fast a
    
[^28]: 意义的线性空间：视觉语言模型中的组合结构

    Linear Spaces of Meanings: Compositional Structures in Vision-Language Models. (arXiv:2302.14383v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14383](http://arxiv.org/abs/2302.14383)

    本文研究了视觉语言模型中的组合结构，并提出了一种使用嵌入空间中较小集合的向量组合来近似表示来自编码器的表示形式的方法，将这些向量视为“理想单词”，并在CLIP的嵌入中以实验方式探索了这些结构的可用性。

    

    本文研究了预训练视觉语言模型（VLM）中的数据嵌入的组合结构。传统上，组合性与预先存在的词汇表中的单词嵌入的代数运算有关。相反，我们试图使用嵌入空间中较小集合的向量组合来近似表示来自编码器的表示形式。这些向量可以被看作是在模型的嵌入空间中直接生成概念的“理想单词”。我们首先从几何学的角度提出了理解组合结构的框架。然后，我们解释了VLM嵌入在概率上的这些组合结构的含义，并提供了它们在实践中产生的直觉。最后，我们在CLIP的嵌入中以实验方式探索了这些结构，并评估了它们在解决分类、去偏和检索等不同视觉语言任务中的有用性。我们的结果表明，嵌入空间中简单的线性代数运算可以实现与更复杂的方法相媲美甚至更好的性能，证明了所提出的意义的线性空间的有效性和可解释性。

    We investigate compositional structures in data embeddings from pre-trained vision-language models (VLMs). Traditionally, compositionality has been associated with algebraic operations on embeddings of words from a pre-existing vocabulary. In contrast, we seek to approximate representations from an encoder as combinations of a smaller set of vectors in the embedding space. These vectors can be seen as "ideal words" for generating concepts directly within the embedding space of the model. We first present a framework for understanding compositional structures from a geometric perspective. We then explain what these compositional structures entail probabilistically in the case of VLM embeddings, providing intuitions for why they arise in practice. Finally, we empirically explore these structures in CLIP's embeddings and we evaluate their usefulness for solving different vision-language tasks such as classification, debiasing, and retrieval. Our results show that simple linear algebraic o
    
[^29]: 回顾链将语言模型与反馈对齐

    Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02676](http://arxiv.org/abs/2302.02676)

    该研究提出了一种新颖的技术，即回顾链，可以轻松优化，并可以从任何形式的反馈中学习，而不受其极性的影响。

    

    从人类偏好中学习对于语言模型具有重要意义，这样才能对人类有所帮助并符合人类和社会价值观。先前的研究通过从人类反馈中学习来理解和遵循指令取得了显著成功。然而，这些方法要么是基于被人类注释者喜欢的手动挑选的模型生成，使得它们在数据利用方面效果不佳且普遍应用具有挑战性，要么依赖于奖励函数和强化学习，这容易出现奖励函数不完美和极难优化的问题。在本文中，我们提出了一种新颖的技术，“回顾链”，它易于优化，并可以从任何形式的反馈中学习，而不受其极性的影响。我们的想法受到了人类如何从以语言形式呈现的广泛反馈中学习的启发。我们将所有类型的反馈转换成句子，然后用它们来微调模型，从而利用这种方法。

    Learning from human preferences is important for language models to be helpful and useful for humans, and to align with human and social values. Prior work have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them ineffective in terms of data utilization and challenging to apply in general, or they depend on reward functions and reinforcement learning, which are prone to imperfect reward function and extremely challenging to optimize. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sentences, which are then used to fine-tune the model, allowing us to take advantage
    
[^30]: 基于语言模型的知识图谱嵌入编辑

    Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.10405](http://arxiv.org/abs/2301.10405)

    本文提出了一种新的任务——编辑基于语言模型的知识图谱嵌入，旨在实现对KG嵌入的数据高效和快速更新。针对这一任务，提出了一个简单而强大的方案——KGEditor，可以更好地更新特定事实而不影响其余部分的性能。

    

    近几十年来，使用语言模型进行知识图谱（KG）嵌入已经取得了实证成功。但是，基于语言模型的KG嵌入通常作为静态工件部署，修改起来具有挑战性，需要重新训练。为了解决这个问题，本文提出了一种新的任务，即编辑基于语言模型的KG嵌入。该任务旨在实现对KG嵌入的数据高效和快速更新，而不影响其余部分的性能。我们构建了四个新数据集：E-FB15k237、A-FB15k237、E-WN18RR 和 A-WN18RR，并评估了几种知识编辑基线，证明了之前的模型处理该任务的能力有限。我们进一步提出了一个简单但强大的基线——KGEditor，它利用超网络的附加参数层来编辑/添加事实。全面的实验结果表明，当更新特定事实而不影响其余部分的性能时，KGEditor 的表现更好。

    Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
    
[^31]: 基于Prompt学习与传播结构的零样本谣言检测

    Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning. (arXiv:2212.01117v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.01117](http://arxiv.org/abs/2212.01117)

    本文提出了一种基于Prompt学习和传播结构的零样本谣言检测框架，其能够有效地检测不同领域和语言的谣言，并可适应非预料中断事件的影响。

    

    在社交媒体时代，谣言随着事件的发生而传播，严重影响了真相的传播。之前的研究表明，由于缺乏标注资源，很难检测出使用少数语言的谣言。而且，昨天没有涉及到的非预料中断事件加剧了数据资源的稀缺性。本文提出了一种基于Prompt学习的新型零样本框架，用于检测不同领域或用不同语言展现的谣言。具体地说，我们首先将社交媒体上传播的谣言表示为多样的传播线程，然后设计了一个分层Prompt编码机制，学习了无语言环境下的上下文表示，以用于促进对不同领域和语言下的谣言数据转换。此外，我们从传播线程中建模领域不变的结构特征，以整合有影响力的社区反应的结构位置表示，以进一步增强领域适应性。同时，我们引入新的虚拟响应机制，以加强模型的推断能力。

    The spread of rumors along with breaking events seriously hinders the truth in the era of social media. Previous studies reveal that due to the lack of annotated resources, rumors presented in minority languages are hard to be detected. Furthermore, the unforeseen breaking events not involved in yesterday's news exacerbate the scarcity of data resources. In this work, we propose a novel zero-shot framework based on prompt learning to detect rumors falling in different domains or presented in different languages. More specifically, we firstly represent rumor circulated on social media as diverse propagation threads, then design a hierarchical prompt encoding mechanism to learn language-agnostic contextual representations for both prompts and rumor data. To further enhance domain adaptation, we model the domain-invariant structural features from the propagation threads, to incorporate structural position representations of influential community response. In addition, a new virtual respon
    
[^32]: 多模态少样本时间动作检测

    Multi-Modal Few-Shot Temporal Action Detection. (arXiv:2211.14905v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.14905](http://arxiv.org/abs/2211.14905)

    提出了一个新的多模态少样本时间动作检测问题，针对这个问题提出了一个新的 MUPPET 方法，通过在视觉-语言模型中构建多模态提示，并使用多模态聚类算法来组合时序连续的片段，解决了问题。在少样本和零样本场景下表现出了优越性，并验证了不同组件的有效性。

    

    少样本 (FS) 和零样本 (ZS) 学习是缩放时间动作检测 (TAD) 到新类的两种不同方法。前者将经过预训练的视觉模型适应于新任务，该任务由每类仅有一个视频表示，而后者通过利用新类的语义描述而不需要训练示例。在本文中，我们介绍了一种新的多模态少样本 (MMFS) TAD 问题，它可以被视为通过共同利用少数支持视频和新类名字来结合 FS-TAD 和 ZS-TAD 的方法。为了解决这个问题，我们进一步提出了一种新颖的 MUlti-modality PromPt mETa-learning (MUPPET) 方法。这是通过有效地连接预训练的视觉和语言模型，同时最大程度地重用已经学习的能力来实现的。具体来说，我们通过使用元学习适配器装备的视觉语义分词器，将支持视频映射到视觉-语言模型的文本标记空间中来构建多模态提示。为了解决大的类内变化，我们提出了一种新的多模态聚类算法来对时间上一致的提议片段进行分组。全面的实验展示了所提出的方法在少样本和零样本场景下的优势，以及不同组件的有效性。

    Few-shot (FS) and zero-shot (ZS) learning are two different approaches for scaling temporal action detection (TAD) to new classes. The former adapts a pretrained vision model to a new task represented by as few as a single video per class, whilst the latter requires no training examples by exploiting a semantic description of the new class. In this work, we introduce a new multi-modality few-shot (MMFS) TAD problem, which can be considered as a marriage of FS-TAD and ZS-TAD by leveraging few-shot support videos and new class names jointly. To tackle this problem, we further introduce a novel MUlti-modality PromPt mETa-learning (MUPPET) method. This is enabled by efficiently bridging pretrained vision and language models whilst maximally reusing already learned capacity. Concretely, we construct multi-modal prompts by mapping support videos into the textual token space of a vision-language model using a meta-learned adapter-equipped visual semantics tokenizer. To tackle large intra-clas
    
[^33]: 见识你所错过的：语义完成学习的视觉-语言预训练

    Seeing What You Miss: Vision-Language Pre-training with Semantic Completion Learning. (arXiv:2211.13437v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.13437](http://arxiv.org/abs/2211.13437)

    本文提出了一种新的“语义完成学习”任务，以改善现有蒙面建模任务忽略全局语义特征的问题，通过捕捉相应蒙面数据的缺失语义来补充它们的信息。

    

    跨模态对齐对于视觉-语言预训练（VLP）模型学习不同模态下的正确相关信息至关重要。本文针对以前的蒙面建模任务主要关注重构蒙面标记而忽略了蒙面数据生成的全局语义特征的问题，提出了一种新的“语义完成学习”任务，以便全局对局部的对齐。具体来说，SCL任务通过捕捉相应蒙面数据的缺失语义来补充它们的信息。

    Cross-modal alignment is essential for vision-language pre-training (VLP) models to learn the correct corresponding information across different modalities. For this purpose, inspired by the success of masked language modeling (MLM) tasks in the NLP pre-training area, numerous masked modeling tasks have been proposed for VLP to further promote cross-modal interactions. The core idea of previous masked modeling tasks is to focus on reconstructing the masked tokens based on visible context for learning local-to-local alignment. However, most of them pay little attention to the global semantic features generated for the masked data, resulting in a limited cross-modal alignment ability of global representations. Therefore, in this paper, we propose a novel Semantic Completion Learning (SCL) task, complementary to existing masked modeling tasks, to facilitate global-to-local alignment. Specifically, the SCL task complements the missing semantics of masked data by capturing the corresponding
    
[^34]: 视觉基础下的常识知识获取

    Visually Grounded Commonsense Knowledge Acquisition. (arXiv:2211.12054v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.12054](http://arxiv.org/abs/2211.12054)

    本文介绍了CLEVER，一种以视觉-语言预训练模型为基础的常识知识提取方法，通过包含有关实体对的图像包汇总出常识关系，避免了对图像实例进行人工注释的问题。

    

    大规模的常识知识库为广泛的AI应用提供动力，其中自动提取常识知识（CKE）是一个关键且具有挑战性的问题。由文本提取CKE知识是已知的受限于本质的稀疏性和推理偏差。另一方面，视觉知觉包含有关真实世界实体的丰富常识知识，例如（人，可以持有，瓶子），这些知识可作为获取基于常识的知识的有希望来源。我们提出CLEVER，将CKE作为一种远距离监督多实例学习问题进行了规定，模型通过关于实体对的图像包汇总出常识关系而无需对图像实例进行人工注释。CLEVER使用了基于视觉-语言预训练模型深入理解图像包中的每个图像，并从中选择信息性实例，以通过新颖的关系汇总方式概括常识知识实体关系信息。

    Large-scale commonsense knowledge bases empower a broad range of AI applications, where the automatic extraction of commonsense knowledge (CKE) is a fundamental and challenging problem. CKE from text is known for suffering from the inherent sparsity and reporting bias of commonsense in text. Visual perception, on the other hand, contains rich commonsense knowledge about real-world entities, e.g., (person, can_hold, bottle), which can serve as promising sources for acquiring grounded commonsense knowledge. In this work, we present CLEVER, which formulates CKE as a distantly supervised multi-instance learning problem, where models learn to summarize commonsense relations from a bag of images about an entity pair without any human annotation on image instances. To address the problem, CLEVER leverages vision-language pre-training models for deep understanding of each image in the bag, and selects informative instances from the bag to summarize commonsense entity relations via a novel cont
    
[^35]: 社交媒体文本深度时间建模在临床抑郁症中的应用

    Deep Temporal Modelling of Clinical Depression through Social Media Text. (arXiv:2211.07717v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.07717](http://arxiv.org/abs/2211.07717)

    本文通过使用抑郁症状检测分类器，从社交媒体文本提取临床相关特征，建立了一个模型用于检测用户的临床抑郁症，通过提供不同时间粒度的准确度度量来评估该模型。

    

    本文描述了一种基于用户时间轴社交媒体帖子的模型，用于检测用户的临床抑郁症。我们的模型使用了抑郁症状检测（DSD）分类器，该分类器基于最大数量的已经过临床医师注释的推文。我们随后使用我们的DSD模型来提取临床相关特征，例如抑郁症评分及其随后的时间模式，以及用户发布活动模式，例如量化他们的“无活动”或“沉默”。此外，为了评估这些提取特征的有效性，我们创建了三种数据集，包括来自两个现有的众所周知的用户级别抑郁症检测基准数据集的测试数据集。然后，我们提供了基于单个特征、基线特征和特征削减测试的准确度度量，在不同时间粒度的几个级别上。

    We describe the development of a model to detect user-level clinical depression based on a user's temporal social media posts. Our model uses a Depression Symptoms Detection (DSD) classifier, which is trained on the largest existing samples of clinician annotated tweets for clinical depression symptoms. We subsequently use our DSD model to extract clinically relevant features, e.g., depression scores and their consequent temporal patterns, as well as user posting activity patterns, e.g., quantifying their ``no activity'' or ``silence.'' Furthermore, to evaluate the efficacy of these extracted features, we create three kinds of datasets including a test dataset, from two existing well-known benchmark datasets for user-level depression detection. We then provide accuracy measures based on single features, baseline features and feature ablation tests, at several different levels of temporal granularity. The relevant data distributions and clinical depression detection related settings can
    
[^36]: 上下文知识：面向知识丰富的半参数语言模型

    Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models. (arXiv:2210.16433v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.16433](http://arxiv.org/abs/2210.16433)

    本文提出了一种名为「Knowledge-in-Context」的半参数语言模型架构，通过外部存储器带入各种类型的知识以帮助解决自然语言处理任务，并且可以自适应地选择最有用的知识片段。

    

    完全参数语言模型通常需要大量的模型参数来存储在零/少样本设置中解决多个自然语言任务所需的知识。此外，在没有昂贵的模型重新训练的情况下难以适应不断发展的世界知识。本文提出了一种新颖的半参数语言模型架构，名为 "Knowledge-in-Context"（KiC），它通过一个知识丰富的外部存储器赋予一个参数化的文本到文本语言模型的知识。具体而言，外部存储器包含六种不同类型的知识：实体、词典、常识、事件、脚本和因果知识。对于每个输入实例，KiC 模型会自适应地选择一种知识类型并检索最有用的知识片段。输入实例连同其知识增强被馈送到文本到文本模型（例如 T5）中以生成输出答案，在提示后输入和输出都以自然语言形式呈现。

    Fully-parametric language models generally require a huge number of model parameters to store the necessary knowledge for solving multiple natural language tasks in zero/few-shot settings. In addition, it is hard to adapt to the evolving world knowledge without the costly model re-training. In this paper, we develop a novel semi-parametric language model architecture, Knowledge-in-Context (KiC), which empowers a parametric text-to-text language model with a knowledge-rich external memory. Specifically, the external memory contains six different types of knowledge: entity, dictionary, commonsense, event, script, and causality knowledge. For each input instance, the KiC model adaptively selects a knowledge type and retrieves the most helpful pieces of knowledge. The input instance along with its knowledge augmentation is fed into a text-to-text model (e.g., T5) to generate the output answer, where both the input and the output are in natural language forms after prompting. Interestingly,
    
[^37]: 基于提议器和回归器的端到端实体检测方法

    End-to-End Entity Detection with Proposer and Regressor. (arXiv:2210.10260v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.10260](http://arxiv.org/abs/2210.10260)

    该论文提出了一种基于提议器和回归器的端到端实体检测方法，通过利用特征金字塔网络生成高质量的实体提议，并对提议进行精细调整以生成最终的预测结果。该模型具有查询语义丰富、实体定位精度高、模型训练容易等优点，还引入了空间调制变压器来增强内部关系的建模能力。实验结果表明，该方法显著优于现有的最先进方法。

    

    命名实体识别是自然语言处理中的传统任务。特别是，由于嵌套场景的普遍存在，嵌套实体识别受到广泛关注。最近的研究将目标检测中的集合预测被转移应用于应对实体嵌套，但是这些方法的问题在于需要手动创建查询向量，无法适应上下文中丰富的语义信息。本文提出了一种基于提议器和回归器的端到端实体检测方法来解决这些问题。首先，提议器利用特征金字塔网络生成高质量的实体提议。然后，回归器对提议进行精细调整以生成最终的预测结果。该模型采用了仅编码器架构，因此具有查询语义丰富、实体定位精度高、模型训练容易等优点。此外，我们引入了空间调制变压器来增强模型对不同实体之间内部关系的建模能力。在两个基准数据集上的实验结果表明，我们的模型显著优于现有的最先进方法。

    Named entity recognition is a traditional task in natural language processing. In particular, nested entity recognition receives extensive attention for the widespread existence of the nesting scenario. The latest research migrates the well-established paradigm of set prediction in object detection to cope with entity nesting. However, the manual creation of query vectors, which fail to adapt to the rich semantic information in the context, limits these approaches. An end-to-end entity detection approach with proposer and regressor is presented in this paper to tackle the issues. First, the proposer utilizes the feature pyramid network to generate high-quality entity proposals. Then, the regressor refines the proposals for generating the final prediction. The model adopts encoder-only architecture and thus obtains the advantages of the richness of query semantics, high precision of entity localization, and easiness of model training. Moreover, we introduce the novel spatially modulated
    
[^38]: 零样本即时事件模式感知技术

    Zero-Shot On-the-Fly Event Schema Induction. (arXiv:2210.06254v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.06254](http://arxiv.org/abs/2210.06254)

    通过利用大型语言模型生成源文档，可以在零样本的情况下即时生成任何主题的完整事件模式，比人工策划的模式更完整。

    

    如何应对一次大流行爆发中涉及到的事件？计划一场婚礼需要采取哪些措施？这些问题的答案可以通过收集许多与该复杂事件有关的文档、提取相关信息并进行分析来找到。我们提出了一种新的方法，利用大型语言模型生成源文档，以便根据高层事件定义预测特定事件、参数和它们之间的关系，从而构建描述整个复杂事件的模式。使用我们的模型，可以零样本地即时生成任何主题的完整模式，无需进行任何手动数据收集。此外，我们开发了高效的方法从文本中提取相关信息，并在一系列实验中表明，在大多数情况下，这些模式比人工策划的模式更完整。最后，我们展示了这个框架与需要数百万人类标注的注释示例的最新事件模式感知方法在性能上的可比性。

    What are the events involved in a pandemic outbreak? What steps should be taken when planning a wedding? The answers to these questions can be found by collecting many documents on the complex event of interest, extracting relevant information, and analyzing it. We present a new approach in which large language models are utilized to generate source documents that allow predicting, given a high-level event definition, the specific events, arguments, and relations between them to construct a schema that describes the complex event in its entirety. Using our model, complete schemas on any topic can be generated on-the-fly without any manual data collection, i.e., in a zero-shot manner. Moreover, we develop efficient methods to extract pertinent information from texts and demonstrate in a series of experiments that these schemas are considered to be more complete than human-curated ones in the majority of examined scenarios. Finally, we show that this framework is comparable in performanc
    
[^39]: MAP：多模态不确定性感知的视觉语言预训练模型

    MAP: Multimodal Uncertainty-Aware Vision-Language Pre-training Model. (arXiv:2210.05335v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.05335](http://arxiv.org/abs/2210.05335)

    本文提出了一种利用概率分布编码器进行多模态不确定性建模的预训练模型MAP，该模型在多项下游任务中均表现优异，超越了现有模型。

    

    多模态语义理解经常需要处理不确定性，导致所获得的信息倾向于涉及多个目标。这种不确定性对我们的解释来说是有问题的，包括跨模态和内部模态的不确定性。鲜有研究探讨该不确定性的建模，特别是在未标记的数据集上预训练和在特定任务下游数据集中进行微调。在本文中，我们利用序列级交互通过概率分布编码器（PDE）将所有模态的表示投影为概率分布。与现有的确定性方法相比，这种不确定性建模可以传递更丰富的多模态语义信息和更复杂的关系。此外，我们将不确定性建模与流行的预训练框架结合起来，并提出适合的预训练任务:基于分布的视觉语言对比（D-VLC）、基于分布的掩蔽语言建模（D-MLM）和分布式图像检索（D-IR）。我们评估了我们提出的模型——多模态不确定性感知的视觉语言预训练模型（MAP）在各种下游任务中的表现，包括视觉问答、图像字幕和指称表达理解。实验结果表明，MAP比其确定性对应物表现更好，并在几个基准测试中达到了最先进的性能。

    Multimodal semantic understanding often has to deal with uncertainty, which means the obtained messages tend to refer to multiple targets. Such uncertainty is problematic for our interpretation, including inter- and intra-modal uncertainty. Little effort has studied the modeling of this uncertainty, particularly in pre-training on unlabeled datasets and fine-tuning in task-specific downstream datasets. In this paper, we project the representations of all modalities as probabilistic distributions via a Probability Distribution Encoder (PDE) by utilizing sequence-level interactions. Compared to the existing deterministic methods, such uncertainty modeling can convey richer multimodal semantic information and more complex relationships. Furthermore, we integrate uncertainty modeling with popular pre-training frameworks and propose suitable pre-training tasks: Distribution-based Vision-Language Contrastive learning (D-VLC), Distribution-based Masked Language Modeling (D-MLM), and Distribut
    
[^40]: 《一种针对多样对话生成的等大小硬EM算法》

    An Equal-Size Hard EM Algorithm for Diverse Dialogue Generation. (arXiv:2209.14627v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.14627](http://arxiv.org/abs/2209.14627)

    本文提出了一种平衡约束的等大小硬EM算法，用于训练多解码器模型以实现多样的对话生成，可在小型模型中生成高质量的多样化响应。

    

    开放领域对话系统旨在以自然语言文本与人类互动。尽管像ChatGPT这样的超大型对话系统最近取得了成功，但使用中小型对话系统仍然是常见的做法，因为它们更加轻便易用。然而，在较小的模型中生成多样的对话响应是具有挑战性的。在本文中，我们提出了一种等大小硬EM（EqHard-EM）算法，用于训练多解码器模型以实现多样的对话生成。我们的算法以硬方式将样本分配给解码器，并额外施加平衡约束条件，以确保所有解码器都经过充分的训练。我们提供了详细的理论分析来证明我们的方法。此外，我们在两个大规模的开放领域对话数据集上进行实验，证明我们的EqHard-EM算法可以生成高质量的多样化响应。

    Open-domain dialogue systems aim to interact with humans through natural language texts in an open-ended fashion. Despite the recent success of super large dialogue systems such as ChatGPT, using medium-to-small-sized dialogue systems remains the common practice as they are more lightweight and accessible; however, generating diverse dialogue responses is challenging, especially with smaller models. In this work, we propose an Equal-size Hard Expectation--Maximization (EqHard-EM) algorithm to train a multi-decoder model for diverse dialogue generation. Our algorithm assigns a sample to a decoder in a hard manner and additionally imposes an equal-assignment constraint to ensure that all decoders are well-trained. We provide detailed theoretical analysis to justify our approach. Further, experiments on two large-scale open-domain dialogue datasets verify that our EqHard-EM algorithm generates high-quality diverse responses.
    
[^41]: 通过多语言微调和回译实现的多语言双向无监督翻译

    Multilingual Bidirectional Unsupervised Translation Through Multilingual Finetuning and Back-Translation. (arXiv:2209.02821v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.02821](http://arxiv.org/abs/2209.02821)

    该论文提出了一种两阶段的方法，实现单个NMT模型对未见过的语言同英语之间的双向无监督翻译。该方法包括多语言微调和双向回译，成功提高了翻译质量。

    

    我们提出了一种两阶段方法，用于训练单个NMT模型以将未见过的语言从和到英语进行翻译。对于第一阶段，我们将编码器-解码器模型初始化为预训练的XLM-R和RoBERTa权重，然后对40种语言到英语的并行数据进行多语言微调。我们发现，该模型可以推广到未见过的语言的零-shot翻译。对于第二阶段，我们利用这种推广能力从单语言数据集生成合成并行数据，然后使用连续的双向回译轮次进行训练。我们称这种方法为EcXTra（{E}nglish-{c}entric Crosslingual ({X}) {Tra}nsfer）。我们的方法在概念上很简单，只使用标准的交叉熵目标，并且是数据驱动的，依次利用辅助并行数据和单语言数据。我们评估我们在7种低资源语言上的无监督NMT结果，发现每一轮回译训练都进一步优化了双向无监督翻译质量，相比基线提高了多达10个BLEU分数。

    We propose a two-stage approach for training a single NMT model to translate unseen languages both to and from English. For the first stage, we initialize an encoder-decoder model to pretrained XLM-R and RoBERTa weights, then perform multilingual fine-tuning on parallel data in 40 languages to English. We find this model can generalize to zero-shot translations on unseen languages. For the second stage, we leverage this generalization ability to generate synthetic parallel data from monolingual datasets, then train with successive rounds of bidirectional back-translation.  We term our approach EcXTra ({E}nglish-{c}entric Crosslingual ({X}) {Tra}nsfer). Our approach is conceptually simple, only using a standard cross-entropy objective throughout, and also is data-driven, sequentially leveraging auxiliary parallel data and monolingual data. We evaluate our unsupervised NMT results on 7 low-resource languages, and find that each round of back-translation training further refines bidirecti
    
[^42]: 自然语言处理的高效方法：一项调查

    Efficient Methods for Natural Language Processing: A Survey. (arXiv:2209.00099v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.00099](http://arxiv.org/abs/2209.00099)

    这篇论文调查了当前高效NLP方法和研究结果，旨在在有限的资源下为进行NLP提供指南，并指向开发更有效方法的有希望的研究方向。

    

    自然语言处理（NLP）的最近研究通过扩展模型参数和训练数据取得了吸引人的结果；但是，仅使用规模来提高性能意味着资源消耗也增加。这些资源包括数据、时间、存储或能源，所有这些资源自然受限且分布不均。这促使研究出更有效的方法以达到类似结果而需要较少的资源。本调查综合和关联当前高效NLP中的方法和研究结果。我们旨在为在有限的资源下进行NLP提供指导，并指向开发更有效方法的有希望的研究方向。

    Recent work in natural language processing (NLP) has yielded appealing results from scaling model parameters and training data; however, using only scale to improve performance means that resource consumption also grows. Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed. This motivates research into efficient methods that require fewer resources to achieve similar results. This survey synthesizes and relates current methods and findings in efficient NLP. We aim to provide both guidance for conducting NLP under limited resources, and point towards promising research directions for developing more efficient methods.
    
[^43]: 基于双流Transformer的通用事件边界字幕生成

    Dual-Stream Transformer for Generic Event Boundary Captioning. (arXiv:2207.03038v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.03038](http://arxiv.org/abs/2207.03038)

    本文提出了一种双流Transformer的通用事件边界字幕生成方法，结合多个预训练模型和边界类型提示，以及单词级别的集成策略，实现了生成更人性化的字幕，并在GEBC测试集上取得了令人满意的结果。

    

    本文介绍了我们参加CVPR2022通用事件边界字幕生成比赛的优胜解决方案。该任务要求字幕生成模型在给定视频边界周围能够理解瞬时状态变化，使其比传统视频字幕生成任务更具挑战性。文章提出了一种双流Transformer，改进了视频内容编码和字幕生成两个方面：(1)我们利用三个预训练模型从不同粒度提取视频特征。此外，我们利用边界类型作为提示，帮助模型生成字幕。(2)我们特别设计一个称为双流Transformer的模型，以学习区分性边界字幕表示。(3)为了生成与内容相关且更加人性化的字幕，我们通过设计一个单词级别的集成策略来改善描述质量。在GEBC测试集上前景不俗的结果证明了我们提出的方法的有效性。

    This paper describes our champion solution for the CVPR2022 Generic Event Boundary Captioning (GEBC) competition. GEBC requires the captioning model to have a comprehension of instantaneous status changes around the given video boundary, which makes it much more challenging than conventional video captioning task. In this paper, a Dual-Stream Transformer with improvements on both video content encoding and captions generation is proposed: (1) We utilize three pre-trained models to extract the video features from different granularities. Moreover, we exploit the types of boundary as hints to help the model generate captions. (2) We particularly design an model, termed as Dual-Stream Transformer, to learn discriminative representations for boundary captioning. (3) Towards generating content-relevant and human-like captions, we improve the description quality by designing a word-level ensemble strategy. The promising results on the GEBC test split demonstrate the efficacy of our proposed 
    
[^44]: 翻译：汉语拼音是否有助于多语言语言建模？

    Does Transliteration Help Multilingual Language Modeling?. (arXiv:2201.12501v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2201.12501](http://arxiv.org/abs/2201.12501)

    本文探究了把使用不同书写系统的相近语言音译成同一种书写系统，对于多语言语言模型的提升的影响，发现音译可以提高低资源语言的表现，而不会对资源相对较高的语言产生负面影响。

    

    由于大部分语言缺乏大规模的代表性语料库，对于多语言语言模型（MLLM）来说，从现有的语料库中提取最重要的信息非常重要。在这方面，不同语言的文本表现形式的多样性使得MLLM面临困难，因为相近的语言之间词汇重叠较少。因此，把使用不同书写系统的相近的语言音译成同一种书写系统可以提高MLLM的下游任务表现。本文中，我们预训练两个ALBERT模型，以实证的方式测量音译对MLLM的影响。我们特别关注印度语-雅利安语系，该系在世界上拥有最高的书写系统多样性。然后，我们在IndicGLUE基准测试中对模型进行评估。我们进行曼－惠特尼U检验，以严格验证音译的效果是否显著。我们发现，音译有利于低资源语言，而不会对资源相对较高的语言产生负面影响。

    As there is a scarcity of large representative corpora for most languages, it is important for Multilingual Language Models (MLLM) to extract the most out of existing corpora. In this regard, script diversity presents a challenge to MLLMs by reducing lexical overlap among closely related languages. Therefore, transliterating closely related languages that use different writing scripts to a common script may improve the downstream task performance of MLLMs. In this paper, we pretrain two ALBERT models to empirically measure the effect of transliteration on MLLMs. We specifically focus on the Indo-Aryan language family, which has the highest script diversity in the world. Afterward, we evaluate our models on the IndicGLUE benchmark. We perform Mann-Whitney U test to rigorously verify whether the effect of transliteration is significant or not. We find that transliteration benefits the low-resource languages without negatively affecting the comparatively high-resource languages. We also m
    

