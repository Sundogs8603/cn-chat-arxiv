# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Mitigating Reversal Curse via Semantic-aware Permutation Training](https://arxiv.org/abs/2403.00758) | 逆转诅咒问题是导致因果语言模型无法进行双向推理的根本原因之一，在这篇论文中，我们提出了通过语义感知的置换训练来缓解这一问题。 |
| [^2] | [AtP*: An efficient and scalable method for localizing LLM behaviour to components](https://arxiv.org/abs/2403.00745) | AtP*是一种将LLM行为准确定位到组件的高效可扩展方法，通过解决Attribution Patching存在的显著假阴性问题，提供了显著改进以及进一步的性能提升。 |
| [^3] | [Dialect prejudice predicts AI decisions about people's character, employability, and criminality](https://arxiv.org/abs/2403.00742) | 语言模型对非裔美国英语说话者持有负面的潜在刻板印象，展现了方言偏见。 |
| [^4] | [Few-Shot Relation Extraction with Hybrid Visual Evidence](https://arxiv.org/abs/2403.00724) | 提出了一种利用文本和视觉语义信息联合学习多模态表示的多模态少样本关系抽取模型(MFS-HVE)。 |
| [^5] | [Self-Consistent Decoding for More Factual Open Responses](https://arxiv.org/abs/2403.00696) | 将自洽解码方法扩展到开放响应生成，通过整合投票，并在NLI评估中展示了“样本与选择”方法相对于其他解码器可以提高30%的真实性。 |
| [^6] | [A Bit of a Problem: Measurement Disparities in Dataset Sizes Across Languages](https://arxiv.org/abs/2403.00686) | 通过定义字节溢价及开发工具，帮助比较不同语言的数据集大小，促进多语言模型发展和数据实践的公平性 |
| [^7] | [Modeling the Quality of Dialogical Explanations](https://arxiv.org/abs/2403.00662) | 本研究探讨了解释对话的质量建模，通过研究解释者和被解释者之间的互动以及它们如何与解释质量相关联，以确保被解释者成功理解。 |
| [^8] | [Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores](https://arxiv.org/abs/2403.00553) | 本论文提出了一种用于衡量文本多样性的标准分数，通过实证研究发现压缩算法可以捕捉类似于$n$-gram重叠同质性得分的信息，并结合多种度量方法来报告分数，适用于不同类型的文本分析。 |
| [^9] | [Large Language Models for Simultaneous Named Entity Extraction and Spelling Correction](https://arxiv.org/abs/2403.00528) | 本文研究使用解码器模型来生成地提取命名实体，并自动校正输入文本中的拼写错误。 |
| [^10] | [ROME: Memorization Insights from Text, Probability and Hidden State in Large Language Models](https://arxiv.org/abs/2403.00510) | ROME提出了一种新方法，通过比较记忆和非记忆样本之间的差异，探索大型语言模型中的记忆化，这有助于在不访问训练数据的情况下了解模型记忆的洞察和影响因素。 |
| [^11] | [Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese](https://arxiv.org/abs/2403.00509) | 基于上下文构建表示（CCR）的历史心理文本分析流程结合了心理测量学和自然语言处理技术，用于从古典中文语料库中提取心理构建，采用间接监督对比学习方法。 |
| [^12] | [PoTeC: A German Naturalistic Eye-tracking-while-reading Corpus](https://arxiv.org/abs/2403.00506) | PoTeC是第一个包含领域专家和新手眼动数据的自然阅读语料库，并且采用了完全交互设计，可用于各种研究分析。 |
| [^13] | [Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition](https://arxiv.org/abs/2403.00499) | 不同学派对于机器是否理解文本存在分歧，作者提出了两种明确定义的理解工作定义，明确承认了意识的问题，并与哲学、心理学和神经科学等丰富文献建立联系。 |
| [^14] | [LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues](https://arxiv.org/abs/2403.00462) | LUCID旨在通过高质量和语言复杂的数据，以及高度自动化的LLM模型，解决现有数据集领域覆盖有限、对话现象有限、未标记的特点，以及需要大量人力投入的问题。 |
| [^15] | [Your Model Is Not Predicting Depression Well And That Is Why: A Case Study of PRIMATE Dataset](https://arxiv.org/abs/2403.00438) | PRIMATE数据集的研究揭示了社交媒体文本中抑郁水平评估的注释质量问题，通过精神健康专业人士重新注释，提高了无欲望症状检测的测试集质量。 |
| [^16] | [Hierarchical Indexing for Retrieval-Augmented Opinion Summarization](https://arxiv.org/abs/2403.00435) | HIRO 是一种用于无监督抽象意见摘要的方法，通过学习索引结构来提取输入评论中流行意见的句子簇，并利用预训练的大型语言模型生成相关的摘要，得到更具语义结构的编码空间和更具代表性的摘要。 |
| [^17] | [LLMs for Targeted Sentiment in News Headlines: Exploring Different Levels of Prompt Prescriptiveness](https://arxiv.org/abs/2403.00418) | 提出了探索LLMs在新闻标题有针对性情感分析中不同级别提示规范性的方法，以提高其性能。 |
| [^18] | [Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models](https://arxiv.org/abs/2403.00417) | 标记化显著影响语言模型的性能，本研究从单词级别到子词级别追溯了标记化器的演变并提出了从认知科学中的“最小努力原则”获得启示，助力标记化器发展。 |
| [^19] | [Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with Fact-Checking in Turkish](https://arxiv.org/abs/2403.00411) | 提出了FCTR数据集，旨在解决英语以外语言，尤其是土耳其语，的数据稀缺问题，并探讨了跨语言转移学习在低资源语言中的有效性，实验证明数据集潜力推动土耳其语研究。 |
| [^20] | [Provably Robust DPO: Aligning Language Models with Noisy Feedback](https://arxiv.org/abs/2403.00409) | 通过引入面向随机偏好翻转的策略优化通用框架，本研究旨在理解存在嘈杂反馈时的DPO算法，从而解决语言模型对齐人类兴趣中的挑战。 |
| [^21] | [Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs](https://arxiv.org/abs/2403.00393) | 本论文提出了私人基准设定方法，通过保持测试数据的私密性，使模型在不揭露测试数据的情况下进行评估，解决了当前基准设定中存在数据污染的问题。 |
| [^22] | [Post-decoder Biasing for End-to-End Speech Recognition of Multi-turn Medical Interview](https://arxiv.org/abs/2403.00370) | 提出了一种后解码器偏置的方法，用于端到端语音识别，特别适用于包含大量领域特定稀有单词的情况，同时推出了 MED-IT 数据集，以解决学术界缺乏知识密集型数据集的问题。 |
| [^23] | [Self-Consistent Reasoning-based Aspect-Sentiment Quad Prediction with Extract-Then-Assign Strategy](https://arxiv.org/abs/2403.00354) | SCRAP采用自洽推理和提取-分配策略，显著改善了模型在处理复杂推理任务和正确预测四元组方面的能力，提高了ASQP中的可解释性和准确性。 |
| [^24] | [Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code Large Language Models](https://arxiv.org/abs/2403.00338) | 提出了一种名为Semi-Instruct的方法，桥接了自然指导和自我指导两种范式，能够将自然指导中的多样但不当的代码转换为适当的指令-代码配对，并设计了一种新颖的测试用例构建方法以验证生成代码的正确性 |
| [^25] | [DPP-Based Adversarial Prompt Searching for Lanugage Models](https://arxiv.org/abs/2403.00292) | 通过Auto-regressive Selective Replacement Ascent (ASRA)算法，我们成功引导预训练语言模型生成有毒内容，具有很好的效果。 |
| [^26] | [Gender Bias in Large Language Models across Multiple Languages](https://arxiv.org/abs/2403.00277) | 本研究调查了不同语种中大型语言模型生成的输出中的性别偏见，使用了三种测量方法，结果显示存在显著的性别偏见。 |
| [^27] | [Extracting Polymer Nanocomposite Samples from Full-Length Documents](https://arxiv.org/abs/2403.00260) | 使用大型语言模型从全文材料科学研究论文中提取聚合物纳米复合材料（PNCs）样本列表，引入新的基准和评估技术，研究不同提示策略以及自一致性的应用，发现即使是先进的LLMs也难以完全提取所有样本。 |
| [^28] | [EUROPA: A Legal Multilingual Keyphrase Generation Dataset](https://arxiv.org/abs/2403.00252) | 提出了一个用于法律领域多语关键词生成的数据集EUROPA，包含所有24种欧盟官方语言，表明在特定领域多语言语料库上仍有改进空间。 |
| [^29] | [CASIMIR: A Corpus of Scientific Articles enhanced with Multiple Author-Integrated Revisions](https://arxiv.org/abs/2403.00241) | CASIMIR提供了一个包含多个作者综合修订的科学文章语料库，以促进对科学文章写作修订步骤的研究。 |
| [^30] | [Benchmarking zero-shot stance detection with FlanT5-XXL: Insights from training data, prompting, and decoding strategies into its near-SoTA performance](https://arxiv.org/abs/2403.00236) | 通过使用FlanT5-XXL和SemEval 2016数据集，研究了零-shot立场检测在推特上的性能表现及其对提示和解码策略的敏感性，揭示了其能够匹敌或超越最先进基准测试的能力，并识别了其中的潜在偏见。 |
| [^31] | [Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models](https://arxiv.org/abs/2403.00231) | 提出了Multimodal ArXiv数据集，包括ArXivCap和ArXivQA，用于增强大型视觉-语言模型对科学理解的能力，ArXivQA通过科学图生成问题，显著提高了数学推理准确率。 |
| [^32] | [A Semantic Distance Metric Learning approach for Lexical Semantic Change Detection](https://arxiv.org/abs/2403.00226) | 提出了一种用于词汇语义变化检测的语义距离度量学习方法，通过使用两个阶段的学习方法和感知编码器，实现了在多语言中优于以往方法的表现。 |
| [^33] | [Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART](https://arxiv.org/abs/2403.00212) | 使用XLSR Wav2Vec2和mBART在自定义数据集上进行微调，实现了视频内容的多语言转录和翻译，为个性化语音提供了可访问的解决方案 |
| [^34] | [Improving Socratic Question Generation using Data Augmentation and Preference Optimization](https://arxiv.org/abs/2403.00199) | 通过数据增强和偏好优化，改进了苏格拉底提问生成方法，减轻教师繁重的工作量，防止生成无效问题。 |
| [^35] | [AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs](https://arxiv.org/abs/2403.00198) | AXOLOTL是一个新颖的后处理框架，通过零样本学习的三步过程，识别和解决偏见，指导模型自我去偏见其输出，从而实现公平性并保持模型性能。 |
| [^36] | ["Flex Tape Can't Fix That": Bias and Misinformation in Edited Language Models](https://arxiv.org/abs/2403.00180) | 该研究调查了编辑语言模型中偏见放大的问题，引入了一个新的基准数据集Seesaw-CF，首次深入研究了权重编辑方法对模型偏见的影响。 |
| [^37] | [TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision](https://arxiv.org/abs/2403.00165) | 本文提出了一种最小监督的分层文本分类方法，利用每个节点的唯一类名作为唯一监督，同时结合大型语言模型（LLM）提高分类性能。 |
| [^38] | [EBBS: An Ensemble with Bi-Level Beam Search for Zero-Shot Machine Translation](https://arxiv.org/abs/2403.00144) | 提出了一种集成方法EBBS，配合新颖的双层束搜索算法，能够优于直接和通过第三语言进行的翻译，并实现知识蒸馏来提高推理效率。 |
| [^39] | [Ensemble-Based Unsupervised Discontinuous Constituency Parsing by Tree Averaging](https://arxiv.org/abs/2403.00143) | 通过树平均法构建集成解析器，稳定并提升无监督不连续成分句法分析性能，实验结果表明该方法在所有指标上均优于基准线 |
| [^40] | [EROS: Entity-Driven Controlled Policy Document Summarization](https://arxiv.org/abs/2403.00141) | 通过使用受控抽象摘要，提出了一种名为EROS的模型，用于显著改善隐私政策文件的可解释性和可读性，强调了包含关键隐私相关实体和组织理由的重要性。 |
| [^41] | [Prompting ChatGPT for Translation: A Comparative Analysis of Translation Brief and Persona Prompts](https://arxiv.org/abs/2403.00127) | 讨论在ChatGPT中将翻译简要和翻译者/作者人物角色融入提示设计的有效性，发现虽然有助于促进人类之间的翻译通信，但对于改善ChatGPT翻译质量的效果有限。 |
| [^42] | [FAC$^2$E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition](https://arxiv.org/abs/2403.00126) | FAC$^2$E框架通过分离语言和认知能力，提供了多维和可解释的评估方式，并将LLMs应用能力分解为回忆知识、利用知识和解决问题三个子步骤，从而为LLMs提供了双重诊断。 |
| [^43] | [LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario](https://arxiv.org/abs/2403.00108) | LoRA作为攻击者渗透LLM安全，研究探讨了在共享与玩耍场景下可能实现的攻击机会。 |
| [^44] | [PROC2PDDL: Open-Domain Planning Representations from Texts](https://arxiv.org/abs/2403.00092) | Proc2PDDL是包含开放领域程序性文本和专家注释的第一个数据集，展示了对于动作的先决条件和效果的定义在语言模型方面的不足，为未来整合语言模型和形式规划提供了启示。 |
| [^45] | [Resonance RoPE: Improving Context Length Generalization of Large Language Models](https://arxiv.org/abs/2403.00071) | Resonance RoPE是一种新颖方法，通过调整RoPE特征的插值来缩小训练短-测试长场景下的泛化差距，在不增加额外在线计算成本的情况下显著提高模型性能。 |
| [^46] | [Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization](https://arxiv.org/abs/2403.00067) | 本研究旨在通过将相同输入上下文的查询组合为单个提示，以最小化重复调用来优化使用大型语言模型在会议摘要中的推理。 |
| [^47] | [SEED: Customize Large Language Models with Sample-Efficient Adaptation for Code Generation](https://arxiv.org/abs/2403.00046) | SEED提出了一种名为Sample-Efficient adaptation with Error-Driven learning的新颖适应方法，利用LLMs产生的错误作为学习机会，从而实现对代码生成任务的高效学习。 |
| [^48] | [Evolving to the Future: Unseen Event Adaptive Fake News Detection on Social Media](https://arxiv.org/abs/2403.00037) | 提出了面向未知事件的适应性假新闻检测框架FADE，通过自适应增强和图对比学习训练目标预测器，同时独立训练事件预测器，最终减轻事件偏见。 |
| [^49] | [TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning](https://arxiv.org/abs/2402.19467) | TV-TREES是第一个多模态蕴涵树生成器，通过生成视频直接蕴涵的简单前提与高级结论之间的蕴涵关系树，实现了可解释联合模态推理，并在挑战性的TVQA数据集上展示了最先进的零-shot性能。 |
| [^50] | [$\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation](https://arxiv.org/abs/2402.19457) | $\texttt{COSMIC}$是一种以相互信息为基础的新的摘要评估方法，有效预测下游任务表现，并与人类判断相关性强。竞争性能优于$\texttt{BERTScore}$和$\texttt{ROUGE}$。 |
| [^51] | [Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process](https://arxiv.org/abs/2402.19350) | 该研究引入了一个促进显式和隐式知识的框架，用于多跳问题回答，从人类阅读过程的角度连接输入文段和预训练知识。 |
| [^52] | [WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset](https://arxiv.org/abs/2402.19282) | WanJuan-CC是一个安全高质量的开源英文网络文本数据集，通过处理大规模的Common Crawl数据并经过多项筛选和过滤步骤得到，为语言模型的预训练提供了重要资源。 |
| [^53] | [When does word order matter and when doesn't it?](https://arxiv.org/abs/2402.18838) | 本文研究了语言模型对词序的敏感度问题，通过量化词序信息量，发现在语言提示提供冗余信息时，模型对词序变化不敏感，且不同任务间的不敏感程度有所差异。 |
| [^54] | [RORA: Robust Free-Text Rationale Evaluation](https://arxiv.org/abs/2402.18678) | RORA 提出了一种新的评估方法，用于衡量自由文本理由对标签的新信息贡献，并在评估中优于现有方法。 |
| [^55] | [Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore](https://arxiv.org/abs/2402.18045) | 本文系统评估了多语言LLMs跨语言和地理区域的事实准确性，并发现英语在事实准确性和生成事实数量方面优于其他语言，同时多语言模型存在对来自西方大陆事实信息的偏见。 |
| [^56] | [Large Language Models on Tabular Data -- A Survey](https://arxiv.org/abs/2402.17944) | 该研究综述了大型语言模型在处理表格数据上的应用，包括关键技术、指标、数据集、模型和优化方法，为未来研究方向提供了启示。 |
| [^57] | [DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers](https://arxiv.org/abs/2402.16914) | 将恶意提示分解为独立的子提示使得LLM越狱攻击更难被检测 |
| [^58] | [Likelihood-based Mitigation of Evaluation Bias in Large Language Models](https://arxiv.org/abs/2402.15987) | 该论文研究了基于大型语言模型（LLM）的评估器中的似然偏差，并提出了一种缓解这种偏差的方法。 |
| [^59] | [MATHWELL: Generating Educational Math Word Problems at Scale](https://arxiv.org/abs/2402.15861) | 使用MATHWELL模型生成了迄今为止最大的英文数学应用题数据集，其中包含20,490个问题，经领域专家评分结果显示，MATHWELL的问题中具有可执行解决方案并符合所有标准的份额比其他选择高出40%，其中74%的可解决问题同时做到了准确和适当。 |
| [^60] | [Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models](https://arxiv.org/abs/2402.15481) | 提出了Prejudice-Caprice Framework（PCF）来全面衡量LLMs中的歧视，考虑了它们在不同上下文中的一贯偏见偏好和偏好变化。 |
| [^61] | [How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries](https://arxiv.org/abs/2402.15302) | 本研究探讨了大型语言模型（LLMs）对指令中心响应的容忍度，并提出了一个包含复杂查询的数据集，旨在揭示触发不道德响应的方法。 |
| [^62] | [CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models](https://arxiv.org/abs/2402.15021) | 本文提出了一个框架，显著提高现有模型编码组合性语言的能力，在组合性基准上取得超过10% 的绝对改进，同时保持或提高了在标准对象识别和检索基准上的性能。 |
| [^63] | [What's in a Name? Auditing Large Language Models for Race and Gender Bias](https://arxiv.org/abs/2402.14875) | 调查发现，大型语言模型存在种族和性别偏见，尤其对与黑人女性相关的名字表现最不利。审计在模型部署和实施时的重要性得到强调。 |
| [^64] | [Can You Learn Semantics Through Next-Word Prediction? The Case of Entailment](https://arxiv.org/abs/2402.13956) | 作者调查了神经LM是否可以通过下一个词预测来解码蕴涵判断，发现它们可以远高于随机几率地解码自然句子之间的蕴涵关系，暗示LM隐含地模拟了语义的某些方面。 |
| [^65] | [Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent](https://arxiv.org/abs/2402.13717) | Neeko利用动态低秩适配器（LoRA）策略，有效处理多角色扮演过程中的挑战，提升了对不同属性、个性和说话模式的适应能力。 |
| [^66] | [CMNER: A Chinese Multimodal NER Dataset based on Social Media](https://arxiv.org/abs/2402.13693) | 本研究在中国最大的社交媒体平台微博上构建了一个中文多模态实体识别数据集（CMNER），包含5,000条微博帖子和18,326张对应图片，并展示了将图片纳入NER任务中的有效性。 |
| [^67] | [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353) | DoRA是一种将预训练权重分解为幅度和方向两个组成部分，并使用LoRA进行方向更新的低秩适应方法，通过增强学习能力和训练稳定性来提高对LLaMA，LLaVA和VL-B的微调性能。 |
| [^68] | [UFO: A UI-Focused Agent for Windows OS Interaction](https://arxiv.org/abs/2402.07939) | UFO是一个专注于Windows操作系统上应用程序的用户界面智能体，利用GPT-Vision的能力来满足用户需求。它通过观察和分析Windows应用程序的图形用户界面和控制信息，实现无缝导航和操作以满足用户的请求。UFO的控制交互模块使得无需人工干预即可实现动作连接和完全自动化执行，使繁琐和耗时的过程变为简单任务。经过测试，UFO在各种场景中取得了良好效果。 |
| [^69] | [Direct Language Model Alignment from Online AI Feedback](https://arxiv.org/abs/2402.04792) | 本论文提出了一种名为OAIF的方法，通过在线反馈来改善DAP方法，该方法使用LLM作为标注器，从而在多个任务中展示出优于离线DAP和RLHF的性能 |
| [^70] | [Large Language Models As MOOCs Graders](https://arxiv.org/abs/2402.03776) | 该研究探索了利用大型语言模型（LLMs）代替MOOCs中同伴评分的可行性，旨在解决大规模在线开放课程中评估学生写作任务的问题。 |
| [^71] | [Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation](https://arxiv.org/abs/2402.03268) | 本文研究了预训练语言模型的推理能力，并提出了从聚合间接推理路径的角度理解语言模型如何产生推理能力。通过对知识图谱和数学问题数据集进行实验和分析，发现增加无标签的随机游走推理路径可以提高实际应用中的多步推理能力。 |
| [^72] | [Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses](https://arxiv.org/abs/2402.02648) | 本文提出了链式反馈（CoF）方法，以缓解大型语言模型在回答问题中出现不一致性的问题。同时，作者还提出了递归链式反馈（R-CoF）方法，并提到了进一步的研究。这些方法可以提高回答的可靠性和有效性。 |
| [^73] | [Measuring Moral Inconsistencies in Large Language Models](https://arxiv.org/abs/2402.01719) | 本研究提出了一种新的信息论度量方法，称为语义图熵（SGE），用于测量道德情景中大型语言模型（LLM）的一致性。与现有的一致性度量方法相比，SGE在五个LLMs上与人类判断更好地相关，为研究LLM不一致性的根本原因提供了新的思路。 |
| [^74] | [LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction](https://arxiv.org/abs/2312.12343) | LatestEval提出了一种自动方法，通过动态和时间敏感的测试构建不受数据污染的阅读理解评估，避免了使用预先训练语言模型的训练语料库，从而鼓励模型更好地推断答案。 |
| [^75] | [Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use](https://arxiv.org/abs/2312.04455) | 本文证明了大型语言模型中关注分配的波形模式对其在需要高度上下文意识的任务中的性能有显著影响。我们提出了一种名为“Attention Buckets”的推理方法，通过多个并行过程和不同的旋转位置嵌入角度，增强了模型对不同上下文位置的意识，从而减轻了忽视关键信息的风险。 |
| [^76] | [One Size Does Not Fit All: Customizing Open-Domain Procedures](https://arxiv.org/abs/2311.09510) | 测试了多种简单的多LLM代理体系结构，发现两个LLM代理按顺序使用的简单结构表现最佳，一个编辑通用操作流程，另一个验证可执行性，在定制操作流程方面取得了显著效果，有望进一步探索多代理编辑体系结构的价值。 |
| [^77] | [SAIE Framework: Support Alone Isn't Enough -- Advancing LLM Training with Adversarial Remarks](https://arxiv.org/abs/2311.08107) | 通过在训练过程中引入交互式讨论，SAIE框架促进了学习模型与伙伴模型之间的支持性和对抗性对话，从而提升模型的理解能力和推理能力。 |
| [^78] | [Speaker attribution in German parliamentary debates with QLoRA-adapted large language models](https://arxiv.org/abs/2309.09902) | 使用QLoRA调整的大型语言模型在德国议会辩论中自动化发言人归属，为计算分析政治话语提供了有前途的途径。 |
| [^79] | [MuLTI: Efficient Video-and-Language Understanding with Text-Guided MultiWay-Sampler and Multiple Choice Modeling](https://arxiv.org/abs/2303.05707) | MuLTI是一种高精度高效的视频与语言理解模型，通过设计基于文本引导的多途采样器，实现了高效有效的特征融合和快速适应下游任务 |
| [^80] | [InceptionXML: A Lightweight Framework with Synchronized Negative Sampling for Short Text Extreme Classification](https://arxiv.org/abs/2109.07319) | 提出了一种轻量级框架InceptionXML，通过在embedding维度上重新分配卷积操作，应对短文本查询中的单词顺序缺失，同时提出了InceptionXML+框架，通过同步标签筛选器和极端分类器，改进了动态硬负采样技术。 |
| [^81] | [CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering.](http://arxiv.org/abs/2401.13170) | CFMatch提出了一个在开放域问答中将自动答案等价评估与人工专家判断对齐的方法，通过提供明确一致的评估指南并引入高效、稳健且轻量级的判别式AE分类器匹配方法来解决当前评估指标与人类判断不一致的问题。 |
| [^82] | [GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse.](http://arxiv.org/abs/2401.01523) | 通过基于迷因的社交虐待研究对大型多模态模型的安全洞察，我们引入了综合的迷因基准测试集GOAT-Bench，评估各种LMMs在识别和回应迷因中体现的微妙社交虐待方面的能力。 |
| [^83] | [Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning.](http://arxiv.org/abs/2310.03309) | 利用大型语言模型进行演绎推理是一个具有挑战性的问题。这篇论文提出了一个简明有序的方法，将任务分解为子任务并且人类化地组织思维，以提高演绎推理的效果。 |
| [^84] | [Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation.](http://arxiv.org/abs/2310.02304) | 本文提出了一种自学优化器（STOP），通过递归自我改进的代码生成，使用融合了语言模型的脚手架程序来改进自身，从而生成性能更好的程序。 |
| [^85] | [EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE.](http://arxiv.org/abs/2308.11971) | 本文引入了一种名为EVE的高效视觉-语言预训练模型，通过遮蔽信号建模和模态感知的方式，实现了统一的多模态Transformer网络，加速了训练进程，并取得了良好的效果。 |
| [^86] | [How Good Are Large Language Models at Out-of-Distribution Detection?.](http://arxiv.org/abs/2308.10261) | 本文通过对大型语言模型进行实证调查，探索了分布外检测的能力。作者发现了LLM在分布外检测方面的差异，并采用了新的生成式微调方法，提高了模型的性能。 |
| [^87] | [Do Language Models Refer?.](http://arxiv.org/abs/2308.05576) | 论文探讨了语言模型是否具有指称能力，并通过借鉴语言哲学外部主义传统的观点，提出了LMs可以指称的理由。 |
| [^88] | [Survey on Sociodemographic Bias in Natural Language Processing.](http://arxiv.org/abs/2306.08158) | 本文调查了209篇关于NLP模型偏见的论文，其中大部分涉及社会人口统计偏见。研究者提出了社会人口统计偏见的定义，并确定了NLP偏见研究的三个主要类别。当前去偏见技术只是隐藏了偏见而不是真正去除它，需要进一步改进。 |
| [^89] | [Text Conditional Alt-Text Generation for Twitter Images.](http://arxiv.org/abs/2305.14779) | 本文针对Twitter上分享的图像提出了一种文本条件下的替代文本生成方法。通过CLIP前缀模型，该模型结合图像和推文中的文本信息，生成关于图像的上下文相关的替代文本。 |
| [^90] | [GPT-4 Technical Report.](http://arxiv.org/abs/2303.08774) | GPT-4是一个大规模多模态模型，可以接收图像和文本输入并产生文本输出，能够在各种专业和学术基准测试中表现出人类水平的表现，包括通过模拟的律师考试。该项目的核心组件是开发基础设施和优化方法，可在广泛的规模范围内表现预测性。 |
| [^91] | [Prompt-based Zero-shot Relation Extraction with Semantic Knowledge Augmentation.](http://arxiv.org/abs/2112.04539) | 本文提出了一种基于Prompt和语义知识增强的模型，用于在零样本情况下识别未见关系。采用了一个新的单词级别的类比推理方法，生成了具有未见关系的增强实例。设计了基于外部知识图谱的提示，增加了已见关系的语义知识信息。 |

# 详细

[^1]: 通过语义感知置换训练来缓解逆转诅咒

    Mitigating Reversal Curse via Semantic-aware Permutation Training

    [https://arxiv.org/abs/2403.00758](https://arxiv.org/abs/2403.00758)

    逆转诅咒问题是导致因果语言模型无法进行双向推理的根本原因之一，在这篇论文中，我们提出了通过语义感知的置换训练来缓解这一问题。

    

    大型语言模型（LLM）在各种任务中取得了令人印象深刻的表现，然而最近的研究表明，因果关系的LLM遭遇了“逆转诅咒”。一个典型的例子是，模型知道“A的父亲是B”，但无法推理出“B的孩子是A”。这一局限性对人工通用智能（AGI）的进展构成了挑战，因为它暗示了模型在理解和应用双向推理方面存在差距。本文首先进行了大量评估，并确定了逆转诅咒的根本原因在于训练和推断阶段之间的词序不同，即因果语言模型在训练数据中预测先行词的能力不足。因此，考虑到在训练数据上进行排列可以被视为潜在解决方案，因为这可以使模型预测先行词或标记。然而，先前的排列方法可能受到截断影响。

    arXiv:2403.00758v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the "reversal curse". It is a typical example that the model knows "A's father is B", but is unable to reason "B's child is A". This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may dis
    
[^2]: AtP*：一种将LLM行为定位到组件的高效可扩展方法

    AtP*: An efficient and scalable method for localizing LLM behaviour to components

    [https://arxiv.org/abs/2403.00745](https://arxiv.org/abs/2403.00745)

    AtP*是一种将LLM行为准确定位到组件的高效可扩展方法，通过解决Attribution Patching存在的显著假阴性问题，提供了显著改进以及进一步的性能提升。

    

    Activation Patching是一种直接计算行为因果归因于模型组件的方法。然而，要全面应用该方法，需要进行一次成本随模型组件数量线性增加的扫描，这可能对SoTA大型语言模型（LLMs）来说成本过高。我们研究了Attribution Patching（AtP），这是对Activation Patching的一种快速基于梯度的近似方法，并发现了两类导致AtP出现显著假阴性的故障模式。我们提出了AtP*的变体，通过两种改变来解决这些故障模式，同时保持可扩展性。我们首次系统研究了AtP及其他快速激活修补方法的对比，并表明AtP明显优于所有其他研究方法，而AtP*进一步提供了显著改进。最后，我们提供了一种方法来限制AtP*估计的假阴性剩余概率。

    arXiv:2403.00745v1 Announce Type: cross  Abstract: Activation Patching is a method of directly computing causal attributions of behavior to model components. However, applying it exhaustively requires a sweep with cost scaling linearly in the number of model components, which can be prohibitively expensive for SoTA Large Language Models (LLMs). We investigate Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives. We propose a variant of AtP called AtP*, with two changes to address these failure modes while retaining scalability. We present the first systematic study of AtP and alternative methods for faster activation patching and show that AtP significantly outperforms all other investigated methods, with AtP* providing further significant improvement. Finally, we provide a method to bound the probability of remaining false negatives of AtP* estimates.
    
[^3]: 方言偏见预测人工智能对人物性格、就业能力和犯罪倾向的决策

    Dialect prejudice predicts AI decisions about people's character, employability, and criminality

    [https://arxiv.org/abs/2403.00742](https://arxiv.org/abs/2403.00742)

    语言模型对非裔美国英语说话者持有负面的潜在刻板印象，展现了方言偏见。

    

    数亿人现在与语言模型互动，其用途从作为写作辅助到影响招聘决策。然而，已知这些语言模型会传播系统性种族偏见，使它们对像非洲裔美国人这样的群体做出有问题的偏见判断。本文展示了语言模型体现了一种潜在的种族文化偏见，即方言偏见：我们扩展了关于美国人对非裔美国英语说话者持有种族语言刻板印象的研究，并发现语言模型也有同样的偏见，表现出潜在的刻板印象，这种刻板印象比实验中记录的任何人类关于非裔美国人的刻板印象都更为负面。

    arXiv:2403.00742v1 Announce Type: cross  Abstract: Hundreds of millions of people now interact with language models, with uses ranging from serving as a writing aid to informing hiring decisions. Yet these language models are known to perpetuate systematic racial prejudices, making their judgments biased in problematic ways about groups like African Americans. While prior research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice: we extend research showing that Americans hold raciolinguistic stereotypes about speakers of African American English and find that language models have the same prejudice, exhibiting covert stereotypes that are more negative than any human stereotypes about African Americans ever experimentally recorde
    
[^4]: 基于混合视觉证据的少样本关系抽取

    Few-Shot Relation Extraction with Hybrid Visual Evidence

    [https://arxiv.org/abs/2403.00724](https://arxiv.org/abs/2403.00724)

    提出了一种利用文本和视觉语义信息联合学习多模态表示的多模态少样本关系抽取模型(MFS-HVE)。

    

    少样本关系抽取的目标是在只有少量标记实例可供训练时，预测句子中命名实体之间的关系。现有的少样本关系抽取方法主要关注单一模态信息，例如仅文本。这会降低性能，当文本中描述的命名实体之间没有明确的上下文时。我们提出了一种多模态少样本关系抽取模型(MFS-HVE)，该模型利用文本和视觉语义信息共同学习多模态表示。MFS-HVE包括语义特征提取器和多模态融合组件。MFS-HVE语义特征提取器被设计用于提取文本和视觉特征。视觉特征包括全局图像特征和图像内的局部物体特征。MFS-HVE多模态融合单元利用图像引导注意力和目标引导注意力集成来自各种模态的信息。

    arXiv:2403.00724v1 Announce Type: new  Abstract: The goal of few-shot relation extraction is to predict relations between name entities in a sentence when only a few labeled instances are available for training. Existing few-shot relation extraction methods focus on uni-modal information such as text only. This reduces performance when there are no clear contexts between the name entities described in text. We propose a multi-modal few-shot relation extraction model (MFS-HVE) that leverages both textual and visual semantic information to learn a multi-modal representation jointly. The MFS-HVE includes semantic feature extractors and multi-modal fusion components. The MFS-HVE semantic feature extractors are developed to extract both textual and visual features. The visual features include global image features and local object features within the image. The MFS-HVE multi-modal fusion unit integrates information from various modalities using image-guided attention, object-guided attentio
    
[^5]: 自洽解码以获得更真实的开放响应

    Self-Consistent Decoding for More Factual Open Responses

    [https://arxiv.org/abs/2403.00696](https://arxiv.org/abs/2403.00696)

    将自洽解码方法扩展到开放响应生成，通过整合投票，并在NLI评估中展示了“样本与选择”方法相对于其他解码器可以提高30%的真实性。

    

    自洽性已经成为提高大型语言模型生成准确短答案能力的强大方法。在以前的定义中，它只关注从生成文本中解析出的最终答案的准确性。本文将这一思想扩展到开放响应生成，通过在解码方法中整合投票来实现。每个输出句子是从多个样本中选择的，基于简单的令牌重叠得分，并且条件是先前的选择。我们将这种“样本与选择”方法与贪婪解码、束搜索、核采样以及DoLA、P-CRR和S-CRR等最近引入的避免产生幻觉的解码器进行比较。我们展示了“样本与选择”方法在基于NLI的评估中比这些解码器在CNN/DM和XSum子集上提高了30%的真实性，在维持与参考摘要相当的ROUGE-1 F1分数的同时。我们进行了人类验证。

    arXiv:2403.00696v1 Announce Type: new  Abstract: Self-consistency has emerged as a powerful method for improving the accuracy of short answers generated by large language models. As previously defined, it only concerns the accuracy of a final answer parsed from generated text. In this work, we extend the idea to open response generation, by integrating voting into the decoding method. Each output sentence is selected from among multiple samples, conditioning on the previous selections, based on a simple token overlap score. We compare this "Sample & Select" method to greedy decoding, beam search, nucleus sampling, and the recently introduced hallucination avoiding decoders of DoLA, P-CRR, and S-CRR. We show that Sample & Select improves factuality by a 30% relative margin against these decoders in NLI-based evaluation on the subsets of CNN/DM and XSum used in the FRANK benchmark, while maintaining comparable ROUGE-1 F1 scores against reference summaries. We collect human verifications 
    
[^6]: 跨语言数据集大小差异中的度量问题

    A Bit of a Problem: Measurement Disparities in Dataset Sizes Across Languages

    [https://arxiv.org/abs/2403.00686](https://arxiv.org/abs/2403.00686)

    通过定义字节溢价及开发工具，帮助比较不同语言的数据集大小，促进多语言模型发展和数据实践的公平性

    

    在我们的研究中，我们将两种语言之间的字节溢价定义为用于编码这些语言中匹配内容的文本所需字节的比率。我们计算了1155种语言的字节溢价，并使用线性回归来估计其他语言的字节溢价。我们发布了一个工具，可以获取任意两种语言的字节溢价，从而实现对跨语言数据集大小的比较，以便更公平地开发多语言模型和数据实践。

    arXiv:2403.00686v1 Announce Type: new  Abstract: How should text dataset sizes be compared across languages? Even for content-matched (parallel) corpora, UTF-8 encoded text can require a dramatically different number of bytes for different languages. In our work, we define the byte premium between two languages as the ratio of bytes used to encode content-matched text in those languages. We compute byte premiums for 1155 languages, and we use linear regressions to estimate byte premiums for other languages. We release a tool to obtain byte premiums for any two languages, enabling comparisons of dataset sizes across languages for more equitable multilingual model development and data practices.
    
[^7]: 建模对话式解释的质量

    Modeling the Quality of Dialogical Explanations

    [https://arxiv.org/abs/2403.00662](https://arxiv.org/abs/2403.00662)

    本研究探讨了解释对话的质量建模，通过研究解释者和被解释者之间的互动以及它们如何与解释质量相关联，以确保被解释者成功理解。

    

    解释在我们的生活中无处不在。大多数情况下，解释以对话形式出现，解释者与被解释者讨论感兴趣的概念或现象。由于两位参与者之间的知识差距，让被解释者理解并不是一件简单的事情。先前的研究关注了专家解释者成功对话中的解释步骤、对话行为和主题的交互。然而，日常生活中的解释经常失败，这带来了一个问题，即什么会使对话成功。在这项工作中，我们研究解释对话，关注解释者和被解释者之间的互动以及它们如何与解释质量相关联，以确保被解释者成功理解。特别是，我们首先构建了一个来自Reddit论坛“像我五岁一样解释”中的399个对话语料库，并对其进行了交互流程和解释质量的注释。

    arXiv:2403.00662v1 Announce Type: new  Abstract: Explanations are pervasive in our lives. Mostly, they occur in dialogical form where an {\em explainer} discusses a concept or phenomenon of interest with an {\em explainee}. Leaving the explainee with a clear understanding is not straightforward due to the knowledge gap between the two participants. Previous research looked at the interaction of explanation moves, dialogue acts, and topics in successful dialogues with expert explainers. However, daily-life explanations often fail, raising the question of what makes a dialogue successful. In this work, we study explanation dialogues in terms of the interactions between the explainer and explainee and how they correlate with the quality of explanations in terms of a successful understanding on the explainee's side. In particular, we first construct a corpus of 399 dialogues from the Reddit forum {\em Explain Like I am Five} and annotate it for interaction flows and explanation quality. We
    
[^8]: 规范文本多样性的测量：一个工具和对分数的比较分析

    Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores

    [https://arxiv.org/abs/2403.00553](https://arxiv.org/abs/2403.00553)

    本论文提出了一种用于衡量文本多样性的标准分数，通过实证研究发现压缩算法可以捕捉类似于$n$-gram重叠同质性得分的信息，并结合多种度量方法来报告分数，适用于不同类型的文本分析。

    

    大型语言模型生成的输出之间的多样性塑造了人们对其质量和实用性的看法。我们的工作通过实证研究英语文本的多样性得分。我们发现，计算效率高的压缩算法捕捉到与$n$-gram的重叠同质性得分所衡量的信息相似。此外，结合多种度量方法——压缩比、长$n$-gram的自重复、Self-BLEU和BERTScore——足以报告，因为它们彼此之间的相互关联较低。这些分数的适用性超出了生成模型的分析；例如，我们突出了在指导调整数据集和人类生成的文本上的应用。我们发布了一个多样性程度

    arXiv:2403.00553v1 Announce Type: new  Abstract: The diversity across outputs generated by large language models shapes the perception of their quality and utility. Prompt leaks, templated answer structure, and canned responses across different interactions are readily noticed by people, but there is no standard score to measure this aspect of model behavior. In this work we empirically investigate diversity scores on English texts. We find that computationally efficient compression algorithms capture information similar to what is measured by slow to compute $n$-gram overlap homogeneity scores. Further, a combination of measures -- compression ratios, self-repetition of long $n$-grams and Self-BLEU and BERTScore -- are sufficient to report, as they have low mutual correlation with each other. The applicability of scores extends beyond analysis of generative models; for example, we highlight applications on instruction-tuning datasets and human-produced texts. We release a diversity sc
    
[^9]: 用于同时进行命名实体提取和拼写校正的大型语言模型

    Large Language Models for Simultaneous Named Entity Extraction and Spelling Correction

    [https://arxiv.org/abs/2403.00528](https://arxiv.org/abs/2403.00528)

    本文研究使用解码器模型来生成地提取命名实体，并自动校正输入文本中的拼写错误。

    

    语言模型（LMs）如BERT已被证明在识别文本中的命名实体（NE）任务上表现良好。BERT LM通常被用作分类器，用于将输入文本中的单个标记分类，或将标记范围分类为可能的NE类别之一。本文假设仅解码器的大型语言模型（LLMs）也可以被生成性地用于提取NE以及可能恢复NE的正确表面形式，其中输入文本中存在的任何拼写错误都将被自动纠正。我们对两个BERT LMs进行微调作为基线，以及八个开源LLMs，在通过将光学字符识别（OCR）应用于日本商店收据图像而获得的文本上进行NE生成任务的微调，本工作中，我们不尝试找到或评估文本中NE的位置。我们表明最优微调的LLM的表现如何

    arXiv:2403.00528v1 Announce Type: new  Abstract: Language Models (LMs) such as BERT, have been shown to perform well on the task of identifying Named Entities (NE) in text. A BERT LM is typically used as a classifier to classify individual tokens in the input text, or to classify spans of tokens, as belonging to one of a set of possible NE categories.   In this paper, we hypothesise that decoder-only Large Language Models (LLMs) can also be used generatively to extract both the NE, as well as potentially recover the correct surface form of the NE, where any spelling errors that were present in the input text get automatically corrected.   We fine-tune two BERT LMs as baselines, as well as eight open-source LLMs, on the task of producing NEs from text that was obtained by applying Optical Character Recognition (OCR) to images of Japanese shop receipts; in this work, we do not attempt to find or evaluate the location of NEs in the text.   We show that the best fine-tuned LLM performs as 
    
[^10]: ROME: 大型语言模型中文本、概率和隐藏状态的记忆洞察

    ROME: Memorization Insights from Text, Probability and Hidden State in Large Language Models

    [https://arxiv.org/abs/2403.00510](https://arxiv.org/abs/2403.00510)

    ROME提出了一种新方法，通过比较记忆和非记忆样本之间的差异，探索大型语言模型中的记忆化，这有助于在不访问训练数据的情况下了解模型记忆的洞察和影响因素。

    

    探究大型语言模型的记忆化具有重要意义。先前的研究建立了用于量化记忆的指标，探讨了各种影响因素，如数据复制、模型大小和提示长度，并通过将模型输出与训练语料库进行比较来评估记忆化。然而，训练语料库规模巨大且其预处理耗时。为了在不访问训练数据的情况下探索记忆化，我们提出了一种名为ROME的新方法，在此方法中，通过比较记忆化和非记忆化样本之间的差异来探索记忆化。具体来说，模型首先将选定的样本分为记忆化和非记忆化组，并通过文本、概率和隐藏状态的见解比较这两组中的演示。实验结果显示包括词长、词性、词频、均值和方差在内的因素的差异。

    arXiv:2403.00510v1 Announce Type: cross  Abstract: Probing the memorization of large language models holds significant importance. Previous works have established metrics for quantifying memorization, explored various influencing factors, such as data duplication, model size, and prompt length, and evaluated memorization by comparing model outputs with training corpora. However, the training corpora are of enormous scale and its pre-processing is time-consuming. To explore memorization without accessing training data, we propose a novel approach, named ROME, wherein memorization is explored by comparing disparities across memorized and non-memorized. Specifically, models firstly categorize the selected samples into memorized and non-memorized groups, and then comparing the demonstrations in the two groups from the insights of text, probability, and hidden state. Experimental findings show the disparities in factors including word length, part-of-speech, word frequency, mean and varianc
    
[^11]: 调查死亡思维：基于上下文构建表示（CCR）的历史心理文本分析对于古典中文

    Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese

    [https://arxiv.org/abs/2403.00509](https://arxiv.org/abs/2403.00509)

    基于上下文构建表示（CCR）的历史心理文本分析流程结合了心理测量学和自然语言处理技术，用于从古典中文语料库中提取心理构建，采用间接监督对比学习方法。

    

    在这项工作中，我们为古典中文开发了一个历史心理文本分析流程。数千年来，人类用各种语言创作文本；然而，大多数计算文献都集中在当代语言和语料库上。历史心理学这一新兴领域依赖计算技术，利用自然语言处理（NLP）中开发的新方法，从历史语料库中提取心理学方面的内容。名为上下文化结构表征（CCR）的当前流程，结合了心理测量学（即心理调查）领域的专业知识和通过基于transformer的语言模型生成的文本表示，以测量古典中文语料库中的传统主义、规范力量和集体主义等心理构建。鉴于可用数据的稀缺性，我们提出了间接监督对比学习方法，并建立了第一个中文...

    arXiv:2403.00509v1 Announce Type: cross  Abstract: In this work, we develop a pipeline for historical-psychological text analysis in classical Chinese. Humans have produced texts in various languages for thousands of years; however, most of the computational literature is focused on contemporary languages and corpora. The emerging field of historical psychology relies on computational techniques to extract aspects of psychology from historical corpora using new methods developed in natural language processing (NLP). The present pipeline, called Contextualized Construct Representations (CCR), combines expert knowledge in psychometrics (i.e., psychological surveys) with text representations generated via transformer-based language models to measure psychological constructs such as traditionalism, norm strength, and collectivism in classical Chinese corpora. Considering the scarcity of available data, we propose an indirect supervised contrastive learning approach and build the first Chin
    
[^12]: PoTeC：一个德语自然眼动阅读语料库

    PoTeC: A German Naturalistic Eye-tracking-while-reading Corpus

    [https://arxiv.org/abs/2403.00506](https://arxiv.org/abs/2403.00506)

    PoTeC是第一个包含领域专家和新手眼动数据的自然阅读语料库，并且采用了完全交互设计，可用于各种研究分析。

    

    arXiv:2403.00506v1 公告类型：新的 摘要：波茨坦教科书语料库（PoTeC）是一个自然眼动阅读语料库，包含了75位参与者阅读12篇科学文章的数据。 PoTeC是第一个包含领域专家和新手眼动数据的自然阅读语料库，采用了2x2x2完全交互设计，包括参与者的学习水平和学习学科作为受试者间因素，文本领域作为受试者内因素。 参与者的阅读理解通过一系列文本理解问题进行评估，他们的领域知识通过每篇文本的文本独立背景问题进行测试。 这些材料针对不同级别的多种语言特征进行了注释。 我们设想PoTeC可用于各种研究，包括但不限于对专家和非专家重新分析。

    arXiv:2403.00506v1 Announce Type: new  Abstract: The Potsdam Textbook Corpus (PoTeC) is a naturalistic eye-tracking-while-reading corpus containing data from 75 participants reading 12 scientific texts. PoTeC is the first naturalistic eye-tracking-while-reading corpus that contains eye-movements from domain-experts as well as novices in a within-participant manipulation: It is based on a 2x2x2 fully-crossed factorial design which includes the participants' level of study and the participants' discipline of study as between-subject factors and the text domain as a within-subject factor. The participants' reading comprehension was assessed by a series of text comprehension questions and their domain knowledge was tested by text-independent background questions for each of the texts. The materials are annotated for a variety of linguistic features at different levels. We envision PoTeC to be used for a wide range of studies including but not limited to analyses of expert and non-expert re
    
[^13]: 僵尸懂吗？一种选择自己冒险探索机器认知的方式

    Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition

    [https://arxiv.org/abs/2403.00499](https://arxiv.org/abs/2403.00499)

    不同学派对于机器是否理解文本存在分歧，作者提出了两种明确定义的理解工作定义，明确承认了意识的问题，并与哲学、心理学和神经科学等丰富文献建立联系。

    

    最近LLMs的进展引发了一个关于它们是否理解文本的辩论。在这篇立场论文中，我们认为在这场辩论中，反对派对于理解有不同的定义，尤其在意识的作用上存在分歧。为了证实这一观点，我们提出一个思想实验，涉及一个在每个可能的基准上都表现出色的开源聊天机器人 $Z$，看似没有主观经验。我们问$Z$是否有理解能力，并展示在AI研究中的不同学派似乎以不同方式回答这个问题，揭示了它们在术语上的分歧。在未来，我们提出了两种明确定义的理解工作定义，明确承认了意识的问题，并与哲学、心理学和神经科学等丰富文献建立联系。

    arXiv:2403.00499v1 Announce Type: new  Abstract: Recent advances in LLMs have sparked a debate on whether they understand text. In this position paper, we argue that opponents in this debate hold different definitions for understanding, and particularly differ in their view on the role of consciousness. To substantiate this claim, we propose a thought experiment involving an open-source chatbot $Z$ which excels on every possible benchmark, seemingly without subjective experience. We ask whether $Z$ is capable of understanding, and show that different schools of thought within seminal AI research seem to answer this question differently, uncovering their terminological disagreement. Moving forward, we propose two distinct working definitions for understanding which explicitly acknowledge the question of consciousness, and draw connections with a rich literature in philosophy, psychology and neuroscience.
    
[^14]: LUCID: 由LLM生成的复杂且有趣对话

    LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues

    [https://arxiv.org/abs/2403.00462](https://arxiv.org/abs/2403.00462)

    LUCID旨在通过高质量和语言复杂的数据，以及高度自动化的LLM模型，解决现有数据集领域覆盖有限、对话现象有限、未标记的特点，以及需要大量人力投入的问题。

    

    arXiv:2403.00462v1 公告类型:新 摘要:虚拟助手在对话能力方面即将迈向一个重要进步，这得益于基于变压器的大型语言模型（LLMs）的最新进展。然而，实现真正革命性的任务导向对话能力的主要瓶颈仍然是高质量和语言复杂的数据的稀缺性。现有数据集在规模上令人印象深刻，但领域覆盖范围有限，包含寥寥无几的真正具挑战性的对话现象；这些现象通常未标记，这使得在没有费时费力的人类评估的情况下难以评估模型的优势和劣势。此外，直到现在，创建高质量对话数据仍需要相当大量的人力投入，这限制了这些数据集的规模以及快速为新目标领域的数据增量。我们的目标是通过LUCID来克服这些问题，这是一个模块化且高度自动化的LLM

    arXiv:2403.00462v1 Announce Type: new  Abstract: Virtual assistants are poised to take a dramatic leap forward in terms of their dialogue capabilities, spurred by recent advances in transformer-based Large Language Models (LLMs). Yet a major bottleneck to achieving genuinely transformative task-oriented dialogue capabilities remains the scarcity of high quality and linguistically sophisticated data. Existing datasets, while impressive in scale, have limited domain coverage and contain few genuinely challenging conversational phenomena; those which are present are typically unlabelled, making it difficult to assess the strengths and weaknesses of models without time-consuming and costly human evaluation. Moreover, creating high quality dialogue data has until now required considerable human input, limiting both the scale of these datasets and the ability to rapidly bootstrap data for a new target domain. We aim to overcome these issues with LUCID, a modularised and highly automated LLM-
    
[^15]: 您的模型未能有效预测抑郁，原因何在：PRIMATE数据集的案例研究

    Your Model Is Not Predicting Depression Well And That Is Why: A Case Study of PRIMATE Dataset

    [https://arxiv.org/abs/2403.00438](https://arxiv.org/abs/2403.00438)

    PRIMATE数据集的研究揭示了社交媒体文本中抑郁水平评估的注释质量问题，通过精神健康专业人士重新注释，提高了无欲望症状检测的测试集质量。

    

    本文讨论了用于基于自然语言处理的社交媒体文本进行抑郁水平估计的精神健康数据集中注释的质量。尽管先前的研究依赖于用二进制类别注释的社交媒体数据集，即抑郁或非抑郁，但最近的数据集（如D2S和PRIMATE）旨在使用PHQ-9症状进行更细致的注释。然而，大多数这些数据集依赖于没有领域知识的众包工作者进行注释。针对PRIMATE数据集，我们的研究揭示了针对注释有效性的担忧，特别是缺乏兴趣或愉悦症状。通过精神健康专业人士的重新注释，我们引入了更精细的标签和文本范围作为证据，识别出相当数量的误报。我们的精细注释将在数据使用协议下发布，为无欲望症状检测提供了更高质量的测试集。本研究凸显了解决这一问题的必要性。

    arXiv:2403.00438v1 Announce Type: new  Abstract: This paper addresses the quality of annotations in mental health datasets used for NLP-based depression level estimation from social media texts. While previous research relies on social media-based datasets annotated with binary categories, i.e. depressed or non-depressed, recent datasets such as D2S and PRIMATE aim for nuanced annotations using PHQ-9 symptoms. However, most of these datasets rely on crowd workers without the domain knowledge for annotation. Focusing on the PRIMATE dataset, our study reveals concerns regarding annotation validity, particularly for the lack of interest or pleasure symptom. Through reannotation by a mental health professional, we introduce finer labels and textual spans as evidence, identifying a notable number of false positives. Our refined annotations, to be released under a Data Use Agreement, offer a higher-quality test set for anhedonia detection. This study underscores the necessity of addressing a
    
[^16]: 用于检索增强意见摘要的分层索引

    Hierarchical Indexing for Retrieval-Augmented Opinion Summarization

    [https://arxiv.org/abs/2403.00435](https://arxiv.org/abs/2403.00435)

    HIRO 是一种用于无监督抽象意见摘要的方法，通过学习索引结构来提取输入评论中流行意见的句子簇，并利用预训练的大型语言模型生成相关的摘要，得到更具语义结构的编码空间和更具代表性的摘要。

    

    我们提出了一种用于无监督抽象意见摘要的方法，结合了抽取方法的可归因性和可扩展性以及大型语言模型(LLMs)的连贯性和流畅性。我们的方法，HIRO，学习了一个将句子映射到通过语义组织的离散层次结构路径的索引结构。在推断时，我们填充索引并使用它来识别和检索包含输入评论中流行意见的句子簇。然后，我们使用一个预训练的LLM生成一个基于这些提取的证据簇的可读摘要。我们的方法的模块化性允许我们在每个阶段评估其有效性。我们展示了HIRO学习了比先前工作更具语义结构的编码空间，并生成了更符合输入评论中意见的摘要。人类评估证实，HIRO生成的摘要更连贯、详细和准确。

    arXiv:2403.00435v1 Announce Type: new  Abstract: We propose a method for unsupervised abstractive opinion summarization, that combines the attributability and scalability of extractive approaches with the coherence and fluency of Large Language Models (LLMs). Our method, HIRO, learns an index structure that maps sentences to a path through a semantically organized discrete hierarchy. At inference time, we populate the index and use it to identify and retrieve clusters of sentences containing popular opinions from input reviews. Then, we use a pretrained LLM to generate a readable summary that is grounded in these extracted evidential clusters. The modularity of our approach allows us to evaluate its efficacy at each stage. We show that HIRO learns an encoding space that is more semantically structured than prior work, and generates summaries that are more representative of the opinions in the input reviews. Human evaluation confirms that HIRO generates more coherent, detailed and accur
    
[^17]: LLMs用于新闻标题的有针对性情感分析：探索不同级别的提示规范化

    LLMs for Targeted Sentiment in News Headlines: Exploring Different Levels of Prompt Prescriptiveness

    [https://arxiv.org/abs/2403.00418](https://arxiv.org/abs/2403.00418)

    提出了探索LLMs在新闻标题有针对性情感分析中不同级别提示规范性的方法，以提高其性能。

    

    新闻标题常常通过有意识地以特定方式描绘实体来引发情感，这使得新闻标题的有针对性情感分析(TSA)成为一项值得做但具有挑战的任务。微调的编码器模型展现出令人满意的TSA性能，但它们的背景知识有限，需要有标记的数据集。LLMs由于其广泛的语言和世界知识以及上下文学习能力，为TSA提供了一个潜在的通用解决方案，然而它们的性能受提示设计的影响很大。通过与主观任务的注释范式进行类比，我们探讨了提示设计对LLMs在新闻标题TSA中性能的影响。我们评估了使用不同级别的规范提示（从纯粹的零样本到符合注释指南的精心准备的少样本提示）的最先进LLMs的预测准确性。认识到TSA的主观性质，我们评估

    arXiv:2403.00418v1 Announce Type: new  Abstract: News headlines often evoke sentiment by intentionally portraying entities in particular ways, making targeted sentiment analysis (TSA) of headlines a worthwhile but difficult task. Fine-tuned encoder models show satisfactory TSA performance, but their background knowledge is limited, and they require a labeled dataset. LLMs offer a potentially universal solution for TSA due to their broad linguistic and world knowledge along with in-context learning abilities, yet their performance is heavily influenced by prompt design. Drawing parallels with annotation paradigms for subjective tasks, we explore the influence of prompt design on the performance of LLMs for TSA of news headlines. We evaluate the predictive accuracy of state-of-the-art LLMs using prompts with different levels of prescriptiveness, ranging from plain zero-shot to elaborate few-shot prompts matching annotation guidelines. Recognizing the subjective nature of TSA, we evaluate
    
[^18]: 重新思考标记化：为大型语言模型打造更好的标记化器

    Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models

    [https://arxiv.org/abs/2403.00417](https://arxiv.org/abs/2403.00417)

    标记化显著影响语言模型的性能，本研究从单词级别到子词级别追溯了标记化器的演变并提出了从认知科学中的“最小努力原则”获得启示，助力标记化器发展。

    

    标记化显著影响语言模型(LMs)的性能。本文追溯了标记化器从单词级别到子词级别的演变，分析它们如何平衡标记和类型以增强模型的适应性同时控制复杂性。尽管像字节对编码(BPE)这样的子词标记器克服了许多单词标记器的局限，但它们在处理非拉丁语言方面遇到困难，并且严重依赖大量的训练数据和计算资源来掌握多词表达的微妙之处。本文认为，标记化器不仅仅是技术工具，还应该从关于人类语言处理的认知科学中汲取灵感。该研究随后介绍了认知科学中的“最小努力原则”，即人类自然寻求减少认知努力，并讨论了该原则对标记器发展的益处。基于这一原则，本文提出...

    arXiv:2403.00417v1 Announce Type: new  Abstract: Tokenization significantly influences language models(LMs)' performance. This paper traces the evolution of tokenizers from word-level to subword-level, analyzing how they balance tokens and types to enhance model adaptability while controlling complexity. Despite subword tokenizers like Byte Pair Encoding (BPE) overcoming many word tokenizer limitations, they encounter difficulties in handling non-Latin languages and depend heavily on extensive training data and computational resources to grasp the nuances of multiword expressions (MWEs). This article argues that tokenizers, more than mere technical tools, should drawing inspiration from the cognitive science about human language processing. This study then introduces the "Principle of Least Effort" from cognitive science, that humans naturally seek to reduce cognitive effort, and discusses the benefits of this principle for tokenizer development. Based on this principle, the paper prop
    
[^19]: 跨语言学习与低资源微调：以土耳其事实检查为例的案例研究

    Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with Fact-Checking in Turkish

    [https://arxiv.org/abs/2403.00411](https://arxiv.org/abs/2403.00411)

    提出了FCTR数据集，旨在解决英语以外语言，尤其是土耳其语，的数据稀缺问题，并探讨了跨语言转移学习在低资源语言中的有效性，实验证明数据集潜力推动土耳其语研究。

    

    通过社交媒体平台迅速传播错误信息引起了人们对其对公众舆论的影响的担忧。尽管错误信息在其他语言中普遍存在，但该领域的大部分研究集中在英语上。因此，其他语言，包括土耳其语，的数据集稀缺。为了解决这一问题，我们介绍了由3238个真实声明组成的FCTR数据集。该数据集涵盖多个领域，并整合了三家土耳其事实检查组织收集的证据。此外，我们旨在评估跨语言转移学习对于低资源语言的有效性，特别关注土耳其语。我们展示了大型语言模型在这一背景下的上下文学习（零次和少次）表现。实验结果表明，该数据集有推动土耳其语研究的潜力。

    arXiv:2403.00411v1 Announce Type: new  Abstract: The rapid spread of misinformation through social media platforms has raised concerns regarding its impact on public opinion. While misinformation is prevalent in other languages, the majority of research in this field has concentrated on the English language. Hence, there is a scarcity of datasets for other languages, including Turkish. To address this concern, we have introduced the FCTR dataset, consisting of 3238 real-world claims. This dataset spans multiple domains and incorporates evidence collected from three Turkish fact-checking organizations. Additionally, we aim to assess the effectiveness of cross-lingual transfer learning for low-resource languages, with a particular focus on Turkish. We demonstrate in-context learning (zero-shot and few-shot) performance of large language models in this context. The experimental results indicate that the dataset has the potential to advance research in the Turkish language.
    
[^20]: 可证明鲁棒的DPO: 用有噪反馈对齐语言模型

    Provably Robust DPO: Aligning Language Models with Noisy Feedback

    [https://arxiv.org/abs/2403.00409](https://arxiv.org/abs/2403.00409)

    通过引入面向随机偏好翻转的策略优化通用框架，本研究旨在理解存在嘈杂反馈时的DPO算法，从而解决语言模型对齐人类兴趣中的挑战。

    

    最近，从基于喜好反馈学习作为一种与人类兴趣对齐的有前景方法已经引起了广泛关注。虽然这些对齐的生成模型在各种任务中展示出令人印象深刻的能力，但它们对高质量人类喜好数据的依赖在实际应用中构成了瓶颈。具体来说，数据集中有噪（不正确和模糊）的偏好对可能会限制语言模型准确捕捉人类意图。虽然从业者最近提出了启发式方法来减轻噪声偏好的影响，但对它们的工作完整理论理解仍然难以捉摸。在这项工作中，我们旨在通过引入一个面向在随机偏好翻转存在的策略优化的通用框架来弥合这一差距。我们特别关注直接偏好优化（DPO）算法，因为它假设偏好遵循 Bradley-Te

    arXiv:2403.00409v1 Announce Type: cross  Abstract: Learning from preference-based feedback has recently gained traction as a promising approach to align language models with human interests. While these aligned generative models have demonstrated impressive capabilities across various tasks, their dependence on high-quality human preference data poses a bottleneck in practical applications. Specifically, noisy (incorrect and ambiguous) preference pairs in the dataset might restrict the language models from capturing human intent accurately. While practitioners have recently proposed heuristics to mitigate the effect of noisy preferences, a complete theoretical understanding of their workings remain elusive.   In this work, we aim to bridge this gap by by introducing a general framework for policy optimization in the presence of random preference flips. We focus on the direct preference optimization (DPO) algorithm in particular since it assumes that preferences adhere to the Bradley-Te
    
[^21]: 防止污染和提高LLM比较评估的私人基准设定

    Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs

    [https://arxiv.org/abs/2403.00393](https://arxiv.org/abs/2403.00393)

    本论文提出了私人基准设定方法，通过保持测试数据的私密性，使模型在不揭露测试数据的情况下进行评估，解决了当前基准设定中存在数据污染的问题。

    

    基准设定是评估LLM的事实标准，因为它速度快、可复制且成本低廉。然而，最近的研究指出，今天大多数开源基准设定已经被污染或泄露到LLM中，这意味着LLM在预训练和/或微调期间可以访问测试数据。这对迄今为止进行的基准研究的有效性以及未来使用基准进行评估提出了严重关切。为了解决这个问题，我们提出了Private Benchmarking，这是一个方案，其中测试数据集保持私密，模型在不向模型透露测试数据的情况下进行评估。我们描述了各种场景（取决于对模型所有者或数据集所有者的信任），并提出了使用私人基准设定避免数据污染的解决方案。对于需要保护模型权重的情况，我们描述了来自机密计算和密码学的解决方案。

    arXiv:2403.00393v1 Announce Type: cross  Abstract: Benchmarking is the de-facto standard for evaluating LLMs, due to its speed, replicability and low cost. However, recent work has pointed out that the majority of the open source benchmarks available today have been contaminated or leaked into LLMs, meaning that LLMs have access to test data during pretraining and/or fine-tuning. This raises serious concerns about the validity of benchmarking studies conducted so far and the future of evaluation using benchmarks. To solve this problem, we propose Private Benchmarking, a solution where test datasets are kept private and models are evaluated without revealing the test data to the model. We describe various scenarios (depending on the trust placed on model owners or dataset owners), and present solutions to avoid data contamination using private benchmarking. For scenarios where the model weights need to be kept private, we describe solutions from confidential computing and cryptography t
    
[^22]: 后解码器偏置用于端到端多轮医学访谈语音识别

    Post-decoder Biasing for End-to-End Speech Recognition of Multi-turn Medical Interview

    [https://arxiv.org/abs/2403.00370](https://arxiv.org/abs/2403.00370)

    提出了一种后解码器偏置的方法，用于端到端语音识别，特别适用于包含大量领域特定稀有单词的情况，同时推出了 MED-IT 数据集，以解决学术界缺乏知识密集型数据集的问题。

    

    端到端（E2E）方法正逐渐取代混合模型，用于自动语音识别（ASR）任务。然而，E2E模型的优化缺乏处理解码偏移的直观方法，尤其是在具有大量领域特定稀有单词且具有特定重要含义的情况下。此外，学术界缺乏知识密集型语音数据集一直是一个重要的限制因素，通常使用的语音语料库与现实对话存在显著差异。为了解决这些挑战，我们提出了Medical Interview（MED-IT），一个包含大量知识密集型命名实体的多轮会诊语音数据集。我们还探讨了增强E2E模型对稀有单词识别性能的方法。我们提出了一种新颖的方法，即后解码器偏置，它基于训练转录的分布构建了一个转换概率矩阵。

    arXiv:2403.00370v1 Announce Type: new  Abstract: End-to-end (E2E) approach is gradually replacing hybrid models for automatic speech recognition (ASR) tasks. However, the optimization of E2E models lacks an intuitive method for handling decoding shifts, especially in scenarios with a large number of domain-specific rare words that hold specific important meanings. Furthermore, the absence of knowledge-intensive speech datasets in academia has been a significant limiting factor, and the commonly used speech corpora exhibit significant disparities with realistic conversation. To address these challenges, we present Medical Interview (MED-IT), a multi-turn consultation speech dataset that contains a substantial number of knowledge-intensive named entities. We also explore methods to enhance the recognition performance of rare words for E2E models. We propose a novel approach, post-decoder biasing, which constructs a transform probability matrix based on the distribution of training transc
    
[^23]: 自洽推理基于提取-分配策略的方面情感四元预测

    Self-Consistent Reasoning-based Aspect-Sentiment Quad Prediction with Extract-Then-Assign Strategy

    [https://arxiv.org/abs/2403.00354](https://arxiv.org/abs/2403.00354)

    SCRAP采用自洽推理和提取-分配策略，显著改善了模型在处理复杂推理任务和正确预测四元组方面的能力，提高了ASQP中的可解释性和准确性。

    

    在方面情感四元预测（ASQP）任务中，用于预测情感四元的生成方法显示出了有希望的结果。然而，它们仍然受到不精确预测和有限可解释性的困扰，这是由数据稀缺性和四元组合成过程建模不足引起的。本文提出了自洽推理-基于方面情感四元预测（SCRAP），优化其模型以按顺序生成推理和相应的情感四元。 SCRAP采用提取-然后-分配推理策略，紧密模仿人类认知。最后，SCRAP通过一致性投票显著改善了模型处理复杂推理任务以及通过一致性投票正确预测四元组的能力，从而提高了ASQP中的可解释性和准确性。

    arXiv:2403.00354v1 Announce Type: new  Abstract: In the task of aspect sentiment quad prediction (ASQP), generative methods for predicting sentiment quads have shown promising results. However, they still suffer from imprecise predictions and limited interpretability, caused by data scarcity and inadequate modeling of the quadruplet composition process. In this paper, we propose Self-Consistent Reasoning-based Aspect-sentiment quadruple Prediction (SCRAP), optimizing its model to generate reasonings and the corresponding sentiment quadruplets in sequence. SCRAP adopts the Extract-Then-Assign reasoning strategy, which closely mimics human cognition. In the end, SCRAP significantly improves the model's ability to handle complex reasoning tasks and correctly predict quadruplets through consistency voting, resulting in enhanced interpretability and accuracy in ASQP.
    
[^24]: Semi-Instruct: 桥接自然指导与自我指导以应用于大型编程语言模型

    Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code Large Language Models

    [https://arxiv.org/abs/2403.00338](https://arxiv.org/abs/2403.00338)

    提出了一种名为Semi-Instruct的方法，桥接了自然指导和自我指导两种范式，能够将自然指导中的多样但不当的代码转换为适当的指令-代码配对，并设计了一种新颖的测试用例构建方法以验证生成代码的正确性

    

    arXiv:2403.00338v1 公告类型：新的 摘要：指导调整在代码大型语言模型（Code LLMs）中的程序合成任务中起着关键作用。目前，收集调整数据的两种主要范式是自然指令（人工编写）和自我指令（自动生成）。自然指令包含多样且正确的代码，但缺乏指令-代码配对，并存在不当的嵌套单行代码格式。相反，自我指令自动生成适当配对数据。然而，由于生成重复内容，存在多样性低的问题，并且无法确保代码的正确性。为了桥接这两个范式，我们提出了Semi-Instruct。它首先通过类似于自我指令的方法，将自然指令中的多样但不当的代码转换为适当的指令-代码配对。为了验证生成代码的正确性，我们设计了一种新颖的测试用例构建方法，通过生成用例输入并执行正确代码来实现。

    arXiv:2403.00338v1 Announce Type: new  Abstract: Instruction tuning plays a pivotal role in Code Large Language Models (Code LLMs) for the task of program synthesis. Presently, two dominant paradigms for collecting tuning data are natural-instruct (human-written) and self-instruct (automatically generated). Natural-instruct includes diverse and correct codes but lacks instruction-code pairs, and exists improper code formats like nested single-line codes. In contrast, self-instruct automatically generates proper paired data. However, it suffers from low diversity due to generating duplicates and cannot ensure the correctness of codes. To bridge the both paradigms, we propose \textbf{Semi-Instruct}. It first converts diverse but improper codes from natural-instruct into proper instruction-code pairs through a method similar to self-instruct. To verify the correctness of generated codes, we design a novel way to construct test cases by generating cases' inputs and executing correct codes 
    
[^25]: 基于DPP的对抗性提示搜索用于语言模型

    DPP-Based Adversarial Prompt Searching for Lanugage Models

    [https://arxiv.org/abs/2403.00292](https://arxiv.org/abs/2403.00292)

    通过Auto-regressive Selective Replacement Ascent (ASRA)算法，我们成功引导预训练语言模型生成有毒内容，具有很好的效果。

    

    语言模型存在生成毫无意义和冒犯性内容的风险，这妨碍了它们的安全部署。因此，在部署之前发现并修改预训练语言模型潜在的有毒输出是至关重要的。本研究中，我们通过自动搜索提示来引导预训练语言模型生成特定目标输出的有毒内容。该问题具有挑战性，因为文本数据的离散性以及针对语言模型的单次前向传递所需的大量计算资源。为了应对这些挑战，我们提出了自回归选择替代上升（ASRA）算法，这是一种基于确定性点过程（DPP）的选择提示的离散优化算法。对六种不同预训练语言模型的实验结果表明，ASRA对引发有毒内容具有很好的效果。

    arXiv:2403.00292v1 Announce Type: new  Abstract: Language models risk generating mindless and offensive content, which hinders their safe deployment. Therefore, it is crucial to discover and modify potential toxic outputs of pre-trained language models before deployment. In this work, we elicit toxic content by automatically searching for a prompt that directs pre-trained language models towards the generation of a specific target output. The problem is challenging due to the discrete nature of textual data and the considerable computational resources required for a single forward pass of the language model. To combat these challenges, we introduce Auto-regressive Selective Replacement Ascent (ASRA), a discrete optimization algorithm that selects prompts based on both quality and similarity with determinantal point process (DPP). Experimental results on six different pre-trained language models demonstrate the efficacy of ASRA for eliciting toxic content. Furthermore, our analysis reve
    
[^26]: 多语种大型语言模型中的性别偏见

    Gender Bias in Large Language Models across Multiple Languages

    [https://arxiv.org/abs/2403.00277](https://arxiv.org/abs/2403.00277)

    本研究调查了不同语种中大型语言模型生成的输出中的性别偏见，使用了三种测量方法，结果显示存在显著的性别偏见。

    

    随着大型语言模型(LLMs)在各种应用中的不断部署，评估LLMs中嵌入的性别偏见的影响变得至关重要。自然语言处理（NLP）领域中的性别偏见主题已经受到相当关注，特别是在英语环境下。然而，对于非英语语言中的性别偏见的调查仍然相对较少且分析不足。在本研究中，我们研究不同语言的LLMs生成的输出中的性别偏见。我们使用三种测量方法：1）在给定性别相关上下文的情况下选择描述性词汇时的性别偏见。2）在给定描述性词汇的情况下选择性别相关代词（她/他）时的性别偏见。3）在LLM生成的对话主题中的性别偏见。我们使用我们的三种测量方法研究了各语种中GPT系列LLMs的输出。我们的研究结果显示了显著的性别偏见。

    arXiv:2403.00277v1 Announce Type: new  Abstract: With the growing deployment of large language models (LLMs) across various applications, assessing the influence of gender biases embedded in LLMs becomes crucial. The topic of gender bias within the realm of natural language processing (NLP) has gained considerable focus, particularly in the context of English. Nonetheless, the investigation of gender bias in languages other than English is still relatively under-explored and insufficiently analyzed. In this work, We examine gender bias in LLMs-generated outputs for different languages. We use three measurements: 1) gender bias in selecting descriptive words given the gender-related context. 2) gender bias in selecting gender-related pronouns (she/he) given the descriptive words. 3) gender bias in the topics of LLM-generated dialogues. We investigate the outputs of the GPT series of LLMs in various languages using our three measurement methods. Our findings revealed significant gender b
    
[^27]: 从全文档案中提取聚合物纳米复合材料样本

    Extracting Polymer Nanocomposite Samples from Full-Length Documents

    [https://arxiv.org/abs/2403.00260](https://arxiv.org/abs/2403.00260)

    使用大型语言模型从全文材料科学研究论文中提取聚合物纳米复合材料（PNCs）样本列表，引入新的基准和评估技术，研究不同提示策略以及自一致性的应用，发现即使是先进的LLMs也难以完全提取所有样本。

    

    本文研究了使用大型语言模型（LLMs）从全文材料科学研究论文中提取聚合物纳米复合材料（PNCs）样本列表的方法。挑战在于PNC样本的复杂性，其属性散布在文本中。标记PNCs详细信息的复杂性限制了数据的可用性，使得由于难以创建全面的命名实体跨度注释，传统的文档级关系抽取技术变得不切实际。为解决这一问题，我们引入了新的基准和评估技术，并探讨了零-shot方式下的不同提示策略。我们还将自一致性融入以提高性能。我们的研究结果表明，即使是先进的LLMs也很难从文章中提取所有样本。最后，我们分析了这一过程中遇到的错误，并将其分为三类。

    arXiv:2403.00260v1 Announce Type: new  Abstract: This paper investigates the use of large language models (LLMs) for extracting sample lists of polymer nanocomposites (PNCs) from full-length materials science research papers. The challenge lies in the complex nature of PNC samples, which have numerous attributes scattered throughout the text. The complexity of annotating detailed information on PNCs limits the availability of data, making conventional document-level relation extraction techniques impractical due to the challenge in creating comprehensive named entity span annotations. To address this, we introduce a new benchmark and an evaluation technique for this task and explore different prompting strategies in a zero-shot manner. We also incorporate self-consistency to improve the performance. Our findings show that even advanced LLMs struggle to extract all of the samples from an article. Finally, we analyze the errors encountered in this process, categorizing them into three ma
    
[^28]: EUROPA：一个法律多语关键词生成数据集

    EUROPA: A Legal Multilingual Keyphrase Generation Dataset

    [https://arxiv.org/abs/2403.00252](https://arxiv.org/abs/2403.00252)

    提出了一个用于法律领域多语关键词生成的数据集EUROPA，包含所有24种欧盟官方语言，表明在特定领域多语言语料库上仍有改进空间。

    

    关键词生成主要在学术研究文章的背景下进行探索，特别侧重于科学领域和英语。 在这项工作中，我们提出了EUROPA，一个用于法律领域多语关键词生成的数据集。 它源自欧洲法院的法律判决，并包含了所有24种欧盟官方语言中的实例。 我们在我们的语料库上运行多语言模型并分析结果，展示了在像我们提出的特定领域多语言语料库上有改进空间。

    arXiv:2403.00252v1 Announce Type: cross  Abstract: Keyphrase generation has primarily been explored within the context of academic research articles, with a particular focus on scientific domains and the English language. In this work, we present EUROPA, a dataset for multilingual keyphrase generation in the legal domain. It is derived from legal judgments from the Court of Justice of the European Union (EU), and contains instances in all 24 EU official languages. We run multilingual models on our corpus and analyze the results, showing room for improvement on a domain-specific multilingual corpus such as the one we present.
    
[^29]: CASIMIR：一个包含多个作者综合修订的科学文章语料库

    CASIMIR: A Corpus of Scientific Articles enhanced with Multiple Author-Integrated Revisions

    [https://arxiv.org/abs/2403.00241](https://arxiv.org/abs/2403.00241)

    CASIMIR提供了一个包含多个作者综合修订的科学文章语料库，以促进对科学文章写作修订步骤的研究。

    

    写作科学文章是一项具有挑战性的任务，因为它是一种高度规范化和具体的体裁，因此精通书面交流对有效传达研究发现和观点至关重要。在本文中，我们提出了一个关于科学文章写作过程中修订步骤的原始文本资源。这个名为CASIMIR的新数据集包含来自OpenReview的15,646篇科学文章的多个修订版本，以及它们的同行评审。文章的连续版本对在句子级别对齐，同时保留段落位置信息作为支持未来修订研究的元数据在语篇级别。每一对修订后的句子都通过自动提取的编辑和相关修订意图进行了丰富。为了评估数据集的初始质量，我们对几种最先进的文本修订方法进行了定性研究，并进行了比较。

    arXiv:2403.00241v1 Announce Type: new  Abstract: Writing a scientific article is a challenging task as it is a highly codified and specific genre, consequently proficiency in written communication is essential for effectively conveying research findings and ideas. In this article, we propose an original textual resource on the revision step of the writing process of scientific articles. This new dataset, called CASIMIR, contains the multiple revised versions of 15,646 scientific articles from OpenReview, along with their peer reviews. Pairs of consecutive versions of an article are aligned at sentence-level while keeping paragraph location information as metadata for supporting future revision studies at the discourse level. Each pair of revised sentences is enriched with automatically extracted edits and associated revision intention. To assess the initial quality on the dataset, we conducted a qualitative study of several state-of-the-art text revision approaches and compared various
    
[^30]: 使用FlanT5-XXL进行零-shot立场检测的基准测试：从训练数据、提示和解码策略中探讨其接近SOTA的表现

    Benchmarking zero-shot stance detection with FlanT5-XXL: Insights from training data, prompting, and decoding strategies into its near-SoTA performance

    [https://arxiv.org/abs/2403.00236](https://arxiv.org/abs/2403.00236)

    通过使用FlanT5-XXL和SemEval 2016数据集，研究了零-shot立场检测在推特上的性能表现及其对提示和解码策略的敏感性，揭示了其能够匹敌或超越最先进基准测试的能力，并识别了其中的潜在偏见。

    

    我们研究了基于LLM的零-shot立场检测在推特上的表现。使用FlanT5-XXL，一个经过调整指令的开源LLM，在SemEval 2016任务6A、6B和P-Stance数据集上，我们研究了在不同提示和解码策略下的表现及其变化，以及模型的潜在偏见。我们展示零-shot方法可以匹敌甚至胜过最先进的基准测试，包括微调模型。我们提供了对其表现的各种见解，包括对指令和提示的敏感性，解码策略，提示的困惑度，以及提示中存在的否定和反对。最后，我们确保LLM没有在测试数据集上进行训练，并确定了一种可能部分解释解码策略之间表现差异的积极偏见。

    arXiv:2403.00236v1 Announce Type: cross  Abstract: We investigate the performance of LLM-based zero-shot stance detection on tweets. Using FlanT5-XXL, an instruction-tuned open-source LLM, with the SemEval 2016 Tasks 6A, 6B, and P-Stance datasets, we study the performance and its variations under different prompts and decoding strategies, as well as the potential biases of the model. We show that the zero-shot approach can match or outperform state-of-the-art benchmarks, including fine-tuned models. We provide various insights into its performance including the sensitivity to instructions and prompts, the decoding strategies, the perplexity of the prompts, and to negations and oppositions present in prompts. Finally, we ensure that the LLM has not been trained on test datasets, and identify a positivity bias which may partially explain the performance differences across decoding strategie
    
[^31]: Multimodal ArXiv: 用于提升大型视觉-语言模型对科学理解的数据集

    Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models

    [https://arxiv.org/abs/2403.00231](https://arxiv.org/abs/2403.00231)

    提出了Multimodal ArXiv数据集，包括ArXivCap和ArXivQA，用于增强大型视觉-语言模型对科学理解的能力，ArXivQA通过科学图生成问题，显著提高了数学推理准确率。

    

    大型视觉-语言模型（LVLMs），以GPT-4V为例，在涉及自然场景中的具体图像的各种任务中表现出色。然而，由于科学领域训练数据集的稀缺，它们在解释抽象图形（例如几何形状和科学图）方面的能力仍然有限。为了填补这一空白，我们介绍了Multimodal ArXiv，包括ArXivCap和ArXivQA，以增强LVLMs的科学理解。ArXivCap是一个包含来自涵盖各种科学领域的572K份ArXiv论文的6.4M张图像和3.9M个标题的图像标题数据集。借鉴ArXivCap，我们介绍了ArXivQA，这是一个通过提示GPT-4V生成的基于科学图的问答数据集。ArXivQA极大地增强了LVLMs的数学推理能力，在多模态数学推理基准上实现了10.4%的绝对准确率提升。此外，利用ArXivCap，我们设计了四个从视觉到文本的任务。

    arXiv:2403.00231v1 Announce Type: cross  Abstract: Large vision-language models (LVLMs), exemplified by GPT-4V, excel across diverse tasks involving concrete images from natural scenes. However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains. To fill this gap, we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension. ArXivCap is a figure-caption dataset comprising 6.4M images and 3.9M captions sourced from 572K ArXiv papers spanning various scientific domains. Drawing from ArXivCap, we introduce ArXivQA, a question-answering dataset generated by prompting GPT-4V based on scientific figures. ArXivQA greatly enhances LVLMs' mathematical reasoning capabilities, achieving a 10.4% absolute accuracy gain on a multimodal mathematical reasoning benchmark. Furthermore, employing ArXivCap, we devise four vision-to-text tas
    
[^32]: 用于词汇语义变化检测的语义距离度量学习方法

    A Semantic Distance Metric Learning approach for Lexical Semantic Change Detection

    [https://arxiv.org/abs/2403.00226](https://arxiv.org/abs/2403.00226)

    提出了一种用于词汇语义变化检测的语义距离度量学习方法，通过使用两个阶段的学习方法和感知编码器，实现了在多语言中优于以往方法的表现。

    

    检测词汇的时间语义变化是各种自然语言处理应用的重要任务，必须对时间敏感地进行预测。词汇语义变化检测（SCD）任务考虑在两个不同的文本语料库$C_1$和$C_2$之间预测给定目标词$w$是否改变了含义的问题。为此，我们提出了一种使用现有的Word-in-Context（WiC）数据集的监督两阶段SCD方法。在第一阶段，对于目标词$w$，我们学习了两个感知感知编码器，表示给定语料库中所选句子中$w$的含义。接下来，在第二阶段，我们学习了一种感知感知距离度量，比较目标词在$C_1$和$C_2$中的所有出现的语义表示。对多个SCD基准数据集的实验结果表明，我们提出的方法始终优于所有先前提出的多种语言的SCD方法。

    arXiv:2403.00226v1 Announce Type: new  Abstract: Detecting temporal semantic changes of words is an important task for various NLP applications that must make time-sensitive predictions. Lexical Semantic Change Detection (SCD) task considers the problem of predicting whether a given target word, $w$, changes its meaning between two different text corpora, $C_1$ and $C_2$. For this purpose, we propose a supervised two-staged SCD method that uses existing Word-in-Context (WiC) datasets. In the first stage, for a target word $w$, we learn two sense-aware encoder that represents the meaning of $w$ in a given sentence selected from a corpus. Next, in the second stage, we learn a sense-aware distance metric that compares the semantic representations of a target word across all of its occurrences in $C_1$ and $C_2$. Experimental results on multiple benchmark datasets for SCD show that our proposed method consistently outperforms all previously proposed SCD methods for multiple languages, esta
    
[^33]: 使用在自定义数据集上微调的XLSR Wav2Vec2和mBART进行视频的转录和翻译

    Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART

    [https://arxiv.org/abs/2403.00212](https://arxiv.org/abs/2403.00212)

    使用XLSR Wav2Vec2和mBART在自定义数据集上进行微调，实现了视频内容的多语言转录和翻译，为个性化语音提供了可访问的解决方案

    

    这项研究解决了使用极少数据训练个性化语音识别模型的挑战。仅利用来自YouTube视频的14分钟自定义音频，我们采用基于检索的语音转换（RVC）创建了一个自定义的Common Voice 16.0语料库。随后，在这个数据集上对一个跨语言自监督表示（XLSR）Wav2Vec2模型进行了微调。开发的基于Web的GUI可以高效地转录和翻译输入的印地语视频。通过集成XLSR Wav2Vec2和mBART，该系统将翻译后的文本与视频时间轴对齐，为个性化语音的多语言视频内容转录和翻译提供了可访问的解决方案。

    arXiv:2403.00212v1 Announce Type: new  Abstract: This research addresses the challenge of training an ASR model for personalized voices with minimal data. Utilizing just 14 minutes of custom audio from a YouTube video, we employ Retrieval-Based Voice Conversion (RVC) to create a custom Common Voice 16.0 corpus. Subsequently, a Cross-lingual Self-supervised Representations (XLSR) Wav2Vec2 model is fine-tuned on this dataset. The developed web-based GUI efficiently transcribes and translates input Hindi videos. By integrating XLSR Wav2Vec2 and mBART, the system aligns the translated text with the video timeline, delivering an accessible solution for multilingual video content transcription and translation for personalized voice.
    
[^34]: 利用数据增强和偏好优化改进苏格拉底提问生成

    Improving Socratic Question Generation using Data Augmentation and Preference Optimization

    [https://arxiv.org/abs/2403.00199](https://arxiv.org/abs/2403.00199)

    通过数据增强和偏好优化，改进了苏格拉底提问生成方法，减轻教师繁重的工作量，防止生成无效问题。

    

    苏格拉底方法是一种引导学生独立解决问题而不直接揭示问题解决方案的方法。本文提出一种通过数据增强和偏好优化改进苏格拉底提问生成的方法，用于增强巨大语言模型自动生成苏格拉底问题，以减轻教师的繁重工作量。研究表明，现有涉及提示这些巨大语言模型的方法有时会产生无效的输出，例如直接揭示问题解决方案或提供无关或过早的问题。为了解决这一问题，本研究首先提出一种数据增强方法，以丰富现有的苏格拉底提问数据集；其次，提出一种方法来优化开源巨大语言模型，例如LLama 2，以更倾向于地面真值问题。

    arXiv:2403.00199v1 Announce Type: new  Abstract: The Socratic method is a way of guiding students toward solving a problem independently without directly revealing the solution to the problem. Although this method has been shown to significantly improve student learning outcomes, it remains a complex labor-intensive task for instructors. Large language models (LLMs) can be used to augment human effort by automatically generating Socratic questions for students. However, existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions. To alleviate this problem, inspired by reinforcement learning with AI feedback (RLAIF), we first propose a data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways. Next, we propose a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questio
    
[^35]: AXOLOTL：通过辅助自我去偏见大型语言模型输出实现公平性

    AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs

    [https://arxiv.org/abs/2403.00198](https://arxiv.org/abs/2403.00198)

    AXOLOTL是一个新颖的后处理框架，通过零样本学习的三步过程，识别和解决偏见，指导模型自我去偏见其输出，从而实现公平性并保持模型性能。

    

    预训练的大型语言模型（LLMs）极大地推动了自然语言处理能力，但容易受其训练数据中存在的偏见影响，导致在各种应用中出现不公平结果。尽管已经提出了许多策略来减轻偏见，但它们通常需要大量计算资源，可能会损害模型性能。在这项工作中，我们介绍了AXOLOTL，这是一个新颖的后处理框架，能够独立于任务和模型运行，在不直接访问内部参数的情况下利用公共API与LLMs交互。通过类似零样本学习的三步过程，AXOLOTL识别偏见，提出解决方案，并指导模型自我去偏见其输出。这种方法最小化了计算成本，并保持了模型性能，使AXOLOTL成为一个具有广泛适用性和易用性的去偏见LLM输出的有前景工具。

    arXiv:2403.00198v1 Announce Type: cross  Abstract: Pre-trained Large Language Models (LLMs) have significantly advanced natural language processing capabilities but are susceptible to biases present in their training data, leading to unfair outcomes in various applications. While numerous strategies have been proposed to mitigate bias, they often require extensive computational resources and may compromise model performance. In this work, we introduce AXOLOTL, a novel post-processing framework, which operates agnostically across tasks and models, leveraging public APIs to interact with LLMs without direct access to internal parameters. Through a three-step process resembling zero-shot learning, AXOLOTL identifies biases, proposes resolutions, and guides the model to self-debias its outputs. This approach minimizes computational costs and preserves model performance, making AXOLOTL a promising tool for debiasing LLM outputs with broad applicability and ease of use.
    
[^36]: "Flex Tape不能修复这个": 编辑语言模型中的偏见和错误信息

    "Flex Tape Can't Fix That": Bias and Misinformation in Edited Language Models

    [https://arxiv.org/abs/2403.00180](https://arxiv.org/abs/2403.00180)

    该研究调查了编辑语言模型中偏见放大的问题，引入了一个新的基准数据集Seesaw-CF，首次深入研究了权重编辑方法对模型偏见的影响。

    

    模型编辑已经成为更新存储在语言模型中的知识的一种具有成本效益的策略。然而，在编辑应用后，模型编辑可能会产生意想不到的后果：与编辑无关的信息也可能被更改，并且模型的其他一般行为可能被错误地改变。在这项工作中，我们调查了模型编辑方法如何意外地加剧了模型后编辑的偏见。我们引入了一个新的基准数据集Seesaw-CF，用于衡量模型编辑的偏见相关伤害，并进行了首次深入研究不同权重编辑方法如何影响模型偏见。具体而言，我们专注于与种族、地理来源和性别等人口属性相关的偏见，以及由编辑语言模型生成的长文本中的定性缺陷。我们发现，编辑模型在变得对亚洲、非洲等属性的属性不确定度愈高时表现出不同程度的更为偏见行为。

    arXiv:2403.00180v1 Announce Type: new  Abstract: Model editing has emerged as a cost-effective strategy to update knowledge stored in language models. However, model editing can have unintended consequences after edits are applied: information unrelated to the edits can also be changed, and other general behaviors of the model can be wrongly altered. In this work, we investigate how model editing methods unexpectedly amplify model biases post-edit. We introduce a novel benchmark dataset, Seesaw-CF, for measuring bias-related harms of model editing and conduct the first in-depth investigation of how different weight-editing methods impact model bias. Specifically, we focus on biases with respect to demographic attributes such as race, geographic origin, and gender, as well as qualitative flaws in long-form texts generated by edited language models. We find that edited models exhibit, to various degrees, more biased behavior as they become less confident in attributes for Asian, African,
    
[^37]: TELEClass: 税务学丰富和LLM增强的最小监督分层文本分类

    TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision

    [https://arxiv.org/abs/2403.00165](https://arxiv.org/abs/2403.00165)

    本文提出了一种最小监督的分层文本分类方法，利用每个节点的唯一类名作为唯一监督，同时结合大型语言模型（LLM）提高分类性能。

    

    分层文本分类旨在将每个文档分类为标签Taxonomy中的一组类别。本文旨在研究使用最少监督：仅使用每个节点的唯一类名作为监督来进行分层文本分类。最近，大型语言模型（LLM）通过零提示在各种任务上表现出竞争性能，但这种方法在分层设置中表现较差，因为在提示中包含大而结构化的标签空间是无效的。另一方面，以前的弱监督分层文本分类方法仅利用原始的Taxonomy骨架，忽略了文本语料库中隐藏的丰富信息，这些信息可以用作额外的类别指示信息。

    arXiv:2403.00165v1 Announce Type: new  Abstract: Hierarchical text classification aims to categorize each document into a set of classes in a label taxonomy. Most earlier works focus on fully or semi-supervised methods that require a large amount of human annotated data which is costly and time-consuming to acquire. To alleviate human efforts, in this paper, we work on hierarchical text classification with the minimal amount of supervision: using the sole class name of each node as the only supervision. Recently, large language models (LLM) show competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting, because it is ineffective to include the large and structured label space in a prompt. On the other hand, previous weakly-supervised hierarchical text classification methods only utilize the raw taxonomy skeleton and ignore the rich information hidden in the text corpus that can serve as additional class-indicative 
    
[^38]: EBBS: 一个具有双层束搜索的集成方法用于零翻译机器翻译

    EBBS: An Ensemble with Bi-Level Beam Search for Zero-Shot Machine Translation

    [https://arxiv.org/abs/2403.00144](https://arxiv.org/abs/2403.00144)

    提出了一种集成方法EBBS，配合新颖的双层束搜索算法，能够优于直接和通过第三语言进行的翻译，并实现知识蒸馏来提高推理效率。

    

    当我们用特定的翻译方向训练多语言模型时，零翻译的能力就会出现；模型可以直接在未见过的方向进行翻译。另外，零翻译也可以通过第三种语言（例如英语）来实现。在我们的工作中，我们发现直接和通过第三种语言进行的翻译都存在噪音，并且表现不尽如人意。我们提出了EBBS，一个具有新颖的双层束搜索算法的集成方法，其中每个集成组件在下层逐步探索自己的预测，但它们通过上层的“软投票”机制进行同步。在两个流行的多语言翻译数据集上的结果表明，EBBS始终优于直接和通过第三种语言进行的翻译，以及现有的集成技术。此外，我们可以将集成的知识传回到多语言模型中，以提高推理效率；值得注意的是，我们的E

    arXiv:2403.00144v1 Announce Type: cross  Abstract: The ability of zero-shot translation emerges when we train a multilingual model with certain translation directions; the model can then directly translate in unseen directions. Alternatively, zero-shot translation can be accomplished by pivoting through a third language (e.g., English). In our work, we observe that both direct and pivot translations are noisy and achieve less satisfactory performance. We propose EBBS, an ensemble method with a novel bi-level beam search algorithm, where each ensemble component explores its own prediction step by step at the lower level but they are synchronized by a "soft voting" mechanism at the upper level. Results on two popular multilingual translation datasets show that EBBS consistently outperforms direct and pivot translations as well as existing ensemble techniques. Further, we can distill the ensemble's knowledge back to the multilingual model to improve inference efficiency; profoundly, our E
    
[^39]: 基于集成的无监督不连续成分句法分析：树平均法

    Ensemble-Based Unsupervised Discontinuous Constituency Parsing by Tree Averaging

    [https://arxiv.org/abs/2403.00143](https://arxiv.org/abs/2403.00143)

    通过树平均法构建集成解析器，稳定并提升无监督不连续成分句法分析性能，实验结果表明该方法在所有指标上均优于基准线

    

    我们解决了无监督不连续成分句法分析的问题，在这个问题中我们观察到先前唯一模型的性能存在高方差。我们提出通过对现有不连续解析器的不同运行构建一个集成，并通过平均预测树来稳定和提升性能。首先，我们针对不同的二元性和连续性设置提供了全面的树平均计算复杂度分析（以P和NP完全为单位）。然后，我们开发了一种高效的精确算法来处理这一任务，在我们的实验中对所有样本运行时间均合理。在三个数据集上的结果显示我们的方法在所有指标上均优于所有基准线，我们还对我们的方法进行了深入分析。

    arXiv:2403.00143v1 Announce Type: cross  Abstract: We address unsupervised discontinuous constituency parsing, where we observe a high variance in the performance of the only previous model. We propose to build an ensemble of different runs of the existing discontinuous parser by averaging the predicted trees, to stabilize and boost performance. To begin with, we provide comprehensive computational complexity analysis (in terms of P and NP-complete) for tree averaging under different setups of binarity and continuity. We then develop an efficient exact algorithm to tackle the task, which runs in a reasonable time for all samples in our experiments. Results on three datasets show our method outperforms all baselines in all metrics; we also provide in-depth analyses of our approach.
    
[^40]: EROS：基于实体的受控政策文件摘要

    EROS: Entity-Driven Controlled Policy Document Summarization

    [https://arxiv.org/abs/2403.00141](https://arxiv.org/abs/2403.00141)

    通过使用受控抽象摘要，提出了一种名为EROS的模型，用于显著改善隐私政策文件的可解释性和可读性，强调了包含关键隐私相关实体和组织理由的重要性。

    

    隐私政策文件在向个人介绍组织对用户个人数据的收集、使用和保护方面发挥着关键作用。然而，它们以长篇、复杂和晦涩的语言而闻名，尤其涉及与隐私相关的实体。因此，对试图理解组织数据使用政策的用户构成了重大挑战。本文提出通过使用受控抽象摘要来增强政策文件的可解释性和可读性 -- 我们强制生成的摘要包括关键的隐私相关实体（如数据和媒体）以及组织的理由（如目标和原因）在收集这些实体时。为实现这一目标，我们开发了PD-Sum，一个带有标记的隐私相关实体标签的政策文件摘要数据集。我们提出的模型EROS通过基于跨度的实体提取模型识别关键实体。

    arXiv:2403.00141v1 Announce Type: cross  Abstract: Privacy policy documents have a crucial role in educating individuals about the collection, usage, and protection of users' personal data by organizations. However, they are notorious for their lengthy, complex, and convoluted language especially involving privacy-related entities. Hence, they pose a significant challenge to users who attempt to comprehend organization's data usage policy. In this paper, we propose to enhance the interpretability and readability of policy documents by using controlled abstractive summarization -- we enforce the generated summaries to include critical privacy-related entities (e.g., data and medium) and organization's rationale (e.g.,target and reason) in collecting those entities. To achieve this, we develop PD-Sum, a policy-document summarization dataset with marked privacy-related entity labels. Our proposed model, EROS, identifies critical entities through a span-based entity extraction model and em
    
[^41]: 使用提示ChatGPT进行翻译：翻译简要和人物角色提示的比较分析

    Prompting ChatGPT for Translation: A Comparative Analysis of Translation Brief and Persona Prompts

    [https://arxiv.org/abs/2403.00127](https://arxiv.org/abs/2403.00127)

    讨论在ChatGPT中将翻译简要和翻译者/作者人物角色融入提示设计的有效性，发现虽然有助于促进人类之间的翻译通信，但对于改善ChatGPT翻译质量的效果有限。

    

    LLMs中的提示工程已显示出改善翻译质量的潜力。然而，在提示设计中融入翻译概念的潜力仍未得到充分探讨。本文讨论了在ChatGPT中为翻译任务设计提示时将翻译简要概念和翻译者及作者的人物角色结合起来的有效性。研究结果表明，尽管某些元素有助于促进人与人之间的翻译通信，但它们在提高ChatGPT翻译质量方面的效果有限。这凸显了需要更多探索性研究，研究翻译理论家和实践者如何开发目前根植于人与人交流范式的概念工具集，以应用于涉及人机交互的新兴工作流中的翻译目的。

    arXiv:2403.00127v1 Announce Type: new  Abstract: Prompt engineering in LLMs has shown potential for improving translation quality. However, the potential of incorporating translation concepts in prompt design remains largely underexplored. Against this backdrop, this paper discusses the effectiveness of incorporating the conceptual tool of translation brief and the personas of translator and author into prompt design for translation tasks in ChatGPT. Findings suggest that, although certain elements are constructive in facilitating human to human communication for translation tasks, their effectiveness is limited for improving translation quality in ChatGPT. This accentuates the need for more explorative research on how translation theorists and practitioners can develop the current set of conceptual tools rooted in the human to human communication paradigm for translation purposes in this emerging workflow involving human machine interaction.
    
[^42]: FAC$^2$E: 通过分离语言和认知来更好地理解大型语言模型的能力

    FAC$^2$E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition

    [https://arxiv.org/abs/2403.00126](https://arxiv.org/abs/2403.00126)

    FAC$^2$E框架通过分离语言和认知能力，提供了多维和可解释的评估方式，并将LLMs应用能力分解为回忆知识、利用知识和解决问题三个子步骤，从而为LLMs提供了双重诊断。

    

    大型语言模型（LLMs）主要通过在各种文本理解和生成任务上的整体性能进行评估。然而，这种范式未能全面区分细粒度的语言和认知技能，导致对LLMs能力的解释不足。本文提出了FAC$^2$E，一种用于细粒度和基于认知的LLMs能力评估的框架。具体而言，我们通过分离与语言相关的能力和认知相关的能力，以多维和可解释的方式来制定LLMs的评估。此外，通过从LLMs中提取中间推理，我们进一步将应用特定能力的过程分解为三个子步骤：回忆相关知识、利用知识和解决问题。最后，FAC$^2$E评估每个细粒度能力的每个子步骤，为LLMs提供了双重诊断。

    arXiv:2403.00126v1 Announce Type: new  Abstract: Large language models (LLMs) are primarily evaluated by overall performance on various text understanding and generation tasks. However, such a paradigm fails to comprehensively differentiate the fine-grained language and cognitive skills, rendering the lack of sufficient interpretation to LLMs' capabilities. In this paper, we present FAC$^2$E, a framework for Fine-grAined and Cognition-grounded LLMs' Capability Evaluation. Specifically, we formulate LLMs' evaluation in a multi-dimensional and explainable manner by dissociating the language-related capabilities and the cognition-related ones. Besides, through extracting the intermediate reasoning from LLMs, we further break down the process of applying a specific capability into three sub-steps: recalling relevant knowledge, utilizing knowledge, and solving problems. Finally, FAC$^2$E evaluates each sub-step of each fine-grained capability, providing a two-faceted diagnosis for LLMs. Uti
    
[^43]: 将LoRA作为攻击！在Share-and-Play场景下穿透LLM安全

    LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario

    [https://arxiv.org/abs/2403.00108](https://arxiv.org/abs/2403.00108)

    LoRA作为攻击者渗透LLM安全，研究探讨了在共享与玩耍场景下可能实现的攻击机会。

    

    对LLMs进行微调对于增强其特定任务的性能并确保模型行为与人类偏好保持一致至关重要。在各种微调方法中，LoRA因其效率和易用性而备受推崇，允许最终用户轻松在开源平台上发布和采用轻量的LoRA模块，以定制其模型以适应不同需求。然而，这种方便的共享与玩耍设置打开了新的攻击面，攻击者可以将LoRA作为攻击者，例如背门注入，并广泛分发对抗性LoRA给社区。这可能导致不利的后果。尽管共享LoRA模块存在巨大的潜在风险，但这一方面尚未得到充分探讨。为了填补这一空白，在本研究中，我们深入探讨了在不断增长的共享与玩耍场景中可能做出的攻击机会。具体而言，我们研究了如何将后门注入LoRA模块并深入探讨。

    arXiv:2403.00108v1 Announce Type: cross  Abstract: Fine-tuning LLMs is crucial to enhancing their task-specific performance and ensuring model behaviors are aligned with human preferences. Among various fine-tuning methods, LoRA is popular for its efficiency and ease to use, allowing end-users to easily post and adopt lightweight LoRA modules on open-source platforms to tailor their model for different customization. However, such a handy share-and-play setting opens up new attack surfaces, that the attacker can render LoRA as an attacker, such as backdoor injection, and widely distribute the adversarial LoRA to the community easily. This can result in detrimental outcomes. Despite the huge potential risks of sharing LoRA modules, this aspect however has not been fully explored. To fill the gap, in this study we thoroughly investigate the attack opportunities enabled in the growing share-and-play scenario. Specifically, we study how to inject backdoor into the LoRA module and dive deep
    
[^44]: PROC2PDDL：来自文本的开放领域规划表示

    PROC2PDDL: Open-Domain Planning Representations from Texts

    [https://arxiv.org/abs/2403.00092](https://arxiv.org/abs/2403.00092)

    Proc2PDDL是包含开放领域程序性文本和专家注释的第一个数据集，展示了对于动作的先决条件和效果的定义在语言模型方面的不足，为未来整合语言模型和形式规划提供了启示。

    

    在基于文本的环境中进行规划仍然是人工智能系统的一项重大挑战。最近的方法使用语言模型来预测规划领域定义（例如，PDDL），但仅在封闭领域模拟环境中进行了评估。为了解决这个问题，我们提出了Proc2PDDL，这是第一个包含开放领域程序性文本和专家注释的PDDL表示的数据集。利用这个数据集，我们评估了最先进的模型在定义动作的先决条件和效果方面的表现。我们发现Proc2PDDL是非常具有挑战性的，GPT-3.5 的成功率接近于0%，而 GPT-4 的成功率约为35%。我们的分析显示出语法和语义错误，表明语言模型在生成领域特定程序和推理事件方面存在不足。我们希望这个分析和数据集有助于未来在整合语言模型和形式规划的最佳方面方面的进展。

    arXiv:2403.00092v1 Announce Type: new  Abstract: Planning in a text-based environment continues to be a major challenge for AI systems. Recent approaches have used language models to predict a planning domain definition (e.g., PDDL) but have only been evaluated in closed-domain simulated environments. To address this, we present Proc2PDDL , the first dataset containing open-domain procedural texts paired with expert-annotated PDDL representations. Using this dataset, we evaluate state-of-the-art models on defining the preconditions and effects of actions. We show that Proc2PDDL is highly challenging, with GPT-3.5's success rate close to 0% and GPT-4's around 35%. Our analysis shows both syntactic and semantic errors, indicating LMs' deficiency in both generating domain-specific prgorams and reasoning about events. We hope this analysis and dataset helps future progress towards integrating the best of LMs and formal planning.
    
[^45]: 提升大型语言模型的上下文长度泛化能力：共振 RoPE

    Resonance RoPE: Improving Context Length Generalization of Large Language Models

    [https://arxiv.org/abs/2403.00071](https://arxiv.org/abs/2403.00071)

    Resonance RoPE是一种新颖方法，通过调整RoPE特征的插值来缩小训练短-测试长场景下的泛化差距，在不增加额外在线计算成本的情况下显著提高模型性能。

    

    本文针对大型语言模型（LLMs）中的训练短-测试长（TSTL）场景挑战，引入了Rotary Position Embedding（RoPE）技术，解决了在较短序列上预训练的模型在较长序列中遇到位置超出分布（OOD）的困难。我们提出了Resonance RoPE，一种新颖的方法，通过精细调整RoPE特征的插值来缩小TSTL场景中的泛化差距，显著提高了模型性能，而无需额外的在线计算成本。此外，我们提出了PosGen，这是一个新的合成基准，专门针对TSTL场景中的精细行为分析，旨在从长上下文中不断增加的令牌生成困难和识别新令牌位置的挑战中分离出来。我们在合成任务上的实验表明，在应用Resonance RoPE后，Transformer模型可以识别OOD位置。

    arXiv:2403.00071v1 Announce Type: cross  Abstract: This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences. We introduce Resonance RoPE, a novel approach designed to narrow the generalization gap in TSTL scenarios by refining the interpolation of RoPE features for OOD positions, significantly improving the model performance without additional online computational costs. Furthermore, we present PosGen, a new synthetic benchmark specifically designed for fine-grained behavior analysis in TSTL scenarios, aiming to isolate the constantly increasing difficulty of token generation on long contexts from the challenges of recognizing new token positions. Our experiments on synthetic tasks show that after applying Resonance RoPE, Transformers recognize OOD position bet
    
[^46]: Query-OPT：通过多查询指令优化大型语言模型在会议摘要中的推理

    Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization

    [https://arxiv.org/abs/2403.00067](https://arxiv.org/abs/2403.00067)

    本研究旨在通过将相同输入上下文的查询组合为单个提示，以最小化重复调用来优化使用大型语言模型在会议摘要中的推理。

    

    这项工作关注基于查询的会议摘要任务，在此任务中，针对特定查询对上下文（会议记录）生成摘要。使用大型语言模型（LLMs）进行此任务时，即使上下文保持不变，每个新查询也需要对LLM推理端点/API进行一次新调用。然而，反复调用LLM推理端点会显著增加在生产中使用它们的成本，这使得许多实际用例中LLMs都不切实际。为解决这一问题，在本文中，我们研究了是否可以成功地将相同输入上下文的查询组合为单个提示以最小化重复调用，在会议摘要中使用。在这方面，我们通过比较各种流行的LLM（GPT-4、PaLM-2、LLaMA-2、Mistral和FLAN-T5）在单查询和多查询设置中的表现进行了广泛实验。

    arXiv:2403.00067v1 Announce Type: new  Abstract: This work focuses on the task of query-based meeting summarization in which the summary of a context (meeting transcript) is generated in response to a specific query. When using Large Language Models (LLMs) for this task, a new call to the LLM inference endpoint/API is required for each new query even if the context stays the same. However, repeated calls to the LLM inference endpoints would significantly increase the costs of using them in production, making LLMs impractical for many real-world use cases. To address this problem, in this paper, we investigate whether combining the queries for the same input context in a single prompt to minimize repeated calls can be successfully used in meeting summarization. In this regard, we conduct extensive experiments by comparing the performance of various popular LLMs: GPT-4, PaLM-2, LLaMA-2, Mistral, and FLAN-T5 in single-query and multi-query settings. We observe that while most LLMs tend to
    
[^47]: 使用样本高效适应对大型语言模型进行自定义以进行代码生成

    SEED: Customize Large Language Models with Sample-Efficient Adaptation for Code Generation

    [https://arxiv.org/abs/2403.00046](https://arxiv.org/abs/2403.00046)

    SEED提出了一种名为Sample-Efficient adaptation with Error-Driven learning的新颖适应方法，利用LLMs产生的错误作为学习机会，从而实现对代码生成任务的高效学习。

    

    虽然大型语言模型（LLMs）在代码生成方面取得了重大进展，但在特定场景下仍然存在困难。这些场景通常需要调整LLMs以满足特定需求，但实际可用的训练数据有限，导致代码生成性能较差。如何有效地调整LLMs以适应新场景并使用更少的训练样本是当前代码生成面临的主要挑战。在本文中，我们提出了一种名为SEED的新颖适应方法，即Sample-Efficient adaptation with Error-Driven learning for code generation。SEED利用LLMs产生的错误作为学习机会，利用错误修订来克服自身缺点，从而实现有效学习。具体而言，SEED涉及识别LLMs生成的错误代码，使用Self-revise进行代码修订，优化模型并迭代地进行适应。

    arXiv:2403.00046v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have made significant progress in code generation, they still struggle with code generation tasks in specific scenarios. These scenarios usually necessitate the adaptation of LLMs to fulfill specific needs, but the limited training data available in practice leads to poor code generation performance. How to effectively adapt LLMs to new scenarios with fewer training samples is a major challenge for current code generation. In this paper, we propose a novel adaptation approach named SEED, which stands for Sample-Efficient adaptation with Error-Driven learning for code generation. SEED leverages the errors made by LLMs as learning opportunities, using error revision to overcome its own shortcomings, thus achieving efficient learning. Specifically, SEED involves identifying error code generated by LLMs, employing Self-revise for code revision, optimizing the model with revised code, and iteratively ad
    
[^48]: 未来发展：社交媒体上看不见事件的适应性假新闻检测

    Evolving to the Future: Unseen Event Adaptive Fake News Detection on Social Media

    [https://arxiv.org/abs/2403.00037](https://arxiv.org/abs/2403.00037)

    提出了面向未知事件的适应性假新闻检测框架FADE，通过自适应增强和图对比学习训练目标预测器，同时独立训练事件预测器，最终减轻事件偏见。

    

    随着社交媒体的快速发展，假新闻在社交媒体上的广泛传播日益威胁个人和社会。在社交媒体动态环境中，假新闻检测旨在开发一个模型，该模型在新闻报道过去事件的基础上进行训练。目标是预测和识别有关未来事件的假新闻，这些事件通常与过去完全不同。然而，现有的假新闻检测方法存在鲁棒性不足，无法泛化到看不见的事件。为了解决这个问题，我们引入了基于未来自适应事件的假新闻检测（FADE）框架。具体来说，我们通过自适应增强策略和图对比学习训练目标预测器，以进行更稳健的整体预测。同时，我们独立训练一个仅事件的预测器以获得有偏见的预测。然后，我们通过获得最终预测来进一步减轻事件偏见。

    arXiv:2403.00037v1 Announce Type: cross  Abstract: With the rapid development of social media, the wide dissemination of fake news on social media is increasingly threatening both individuals and society. In the dynamic landscape of social media, fake news detection aims to develop a model trained on news reporting past events. The objective is to predict and identify fake news about future events, which often relate to subjects entirely different from those in the past. However, existing fake detection methods exhibit a lack of robustness and cannot generalize to unseen events. To address this, we introduce Future ADaptive Event-based Fake news Detection (FADE) framework. Specifically, we train a target predictor through an adaptive augmentation strategy and graph contrastive learning to make more robust overall predictions. Simultaneously, we independently train an event-only predictor to obtain biased predictions. Then we further mitigate event bias by obtaining the final prediction
    
[^49]: TV-TREES：用于神经符号视频推理的多模态蕴涵树

    TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning

    [https://arxiv.org/abs/2402.19467](https://arxiv.org/abs/2402.19467)

    TV-TREES是第一个多模态蕴涵树生成器，通过生成视频直接蕴涵的简单前提与高级结论之间的蕴涵关系树，实现了可解释联合模态推理，并在挑战性的TVQA数据集上展示了最先进的零-shot性能。

    

    在处理电视剪辑等复杂的多模态内容进行问答是一项具有挑战性的任务。这部分是因为当前的视频-语言模型依赖于单模态推理，在处理长输入时性能下降，并且缺乏可解释性。我们提出了TV-TREES，这是第一个多模态蕴涵树生成器。TV-TREES作为一种促进可解释联合模态推理的视频理解方法，通过生成视频直接蕴涵的简单前提与高级结论之间的蕴涵关系树。随后，我们引入了多模态蕴涵树生成任务来评估此类方法的推理质量。我们的方法在具有挑战性的TVQA数据集上的实验结果展示了可解释的、具有最先进零-shot性能的完整视频剪辑，展示了与黑盒方法相比的最佳实践。

    arXiv:2402.19467v1 Announce Type: cross  Abstract: It is challenging to perform question-answering over complex, multimodal content such as television clips. This is in part because current video-language models rely on single-modality reasoning, have lowered performance on long inputs, and lack interpetability. We propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions. We then introduce the task of multimodal entailment tree generation to evaluate the reasoning quality of such methods. Our method's experimental results on the challenging TVQA dataset demonstrate intepretable, state-of-the-art zero-shot performance on full video clips, illustrating a best of both worlds contrast to black-box methods.
    
[^50]: $\texttt{COSMIC}$: 相互信息用于任务无关摘要评估

    $\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation

    [https://arxiv.org/abs/2402.19457](https://arxiv.org/abs/2402.19457)

    $\texttt{COSMIC}$是一种以相互信息为基础的新的摘要评估方法，有效预测下游任务表现，并与人类判断相关性强。竞争性能优于$\texttt{BERTScore}$和$\texttt{ROUGE}$。

    

    评估总结质量存在显著挑战。为此，我们提出了一种新颖的面向任务的评估方法，根据总结器生成对下游任务有用且保留任务结果的摘要能力。我们在理论上建立了这些任务的结果错误概率与源文本和生成摘要之间的相互信息之间的直接关系。我们引入了$\texttt{COSMIC}$作为这一度量的实际实现，展示了它与基于人类判断的度量之间的强相关性，以及它在预测下游任务性能方面的有效性。对已建立的度量如$\texttt{BERTScore}$和$\texttt{ROUGE}$的比较分析凸显了$\texttt{COSMIC}$的竞争性能。

    arXiv:2402.19457v1 Announce Type: cross  Abstract: Assessing the quality of summarizers poses significant challenges. In response, we propose a novel task-oriented evaluation approach that assesses summarizers based on their capacity to produce summaries that are useful for downstream tasks, while preserving task outcomes. We theoretically establish a direct relationship between the resulting error probability of these tasks and the mutual information between source texts and generated summaries. We introduce $\texttt{COSMIC}$ as a practical implementation of this metric, demonstrating its strong correlation with human judgment-based metrics and its effectiveness in predicting downstream task performance. Comparative analyses against established metrics like $\texttt{BERTScore}$ and $\texttt{ROUGE}$ highlight the competitive performance of $\texttt{COSMIC}$.
    
[^51]: 基于人类阅读过程的多跳问题回答中促进显式和隐式知识

    Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process

    [https://arxiv.org/abs/2402.19350](https://arxiv.org/abs/2402.19350)

    该研究引入了一个促进显式和隐式知识的框架，用于多跳问题回答，从人类阅读过程的角度连接输入文段和预训练知识。

    

    预训练语言模型（PLMs）利用思维链（CoT）模拟人类推理和推断过程，实现了在多跳QA方面高效的性能。然而，当处理复杂问题时，PLMs的推理能力和人类之间仍存在差距。心理学研究表明，在阅读过程中，输入文段中的显式信息与人类先验知识之间存在重要联系。然而，当前的研究未能充分关注从人类认知研究的角度链接输入文段和基于PLMs预训练知识。在本研究中，我们引入了一个促进显式和隐式知识（PEI）框架，使用提示连接显式和隐式知识，与人类阅读过程对齐，用于多跳QA。我们将输入文段视为显式知识，利用它们通过统一提示推导隐式知识。

    arXiv:2402.19350v1 Announce Type: new  Abstract: Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to simulate human reasoning and inference processes, achieving proficient performance in multi-hop QA. However, a gap persists between PLMs' reasoning abilities and those of humans when tackling complex problems. Psychological studies suggest a vital connection between explicit information in passages and human prior knowledge during reading. Nevertheless, current research has given insufficient attention to linking input passages and PLMs' pre-training-based knowledge from the perspective of human cognition studies. In this study, we introduce a \textbf{P}rompting \textbf{E}xplicit and \textbf{I}mplicit knowledge (PEI) framework, which uses prompts to connect explicit and implicit knowledge, aligning with human reading process for multi-hop QA. We consider the input passages as explicit knowledge, employing them to elicit implicit knowledge through unified prompt reason
    
[^52]: WanJuan-CC：一个安全且高质量的开源英文网络文本数据集

    WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset

    [https://arxiv.org/abs/2402.19282](https://arxiv.org/abs/2402.19282)

    WanJuan-CC是一个安全高质量的开源英文网络文本数据集，通过处理大规模的Common Crawl数据并经过多项筛选和过滤步骤得到，为语言模型的预训练提供了重要资源。

    

    本文介绍了 WanJuan-CC，这是一个安全且高质量的开源英文网络文本数据集，来源于Common Crawl数据。研究解决了为语言模型构建大规模预训练数据集所面临的挑战，这需要大量高质量数据。设计了一个全面的流程来处理Common Crawl数据，包括提取、启发式规则过滤、模糊去重、内容安全过滤和数据质量过滤。从大约680亿个原始英文文档中，我们获得了22万亿标记的安全数据，并从中选出了10万亿标记的高质量数据作为WanJuan-CC的一部分。我们已经开源了这个数据集中的3000亿标记。该论文还提供了与数据质量相关的统计信息，使用户可以根据自己的需求选择适当的数据。为评估数据集的质量和实用性，我们使用WanJuan-CC训练了10亿参数和30亿参数的模型。

    arXiv:2402.19282v1 Announce Type: new  Abstract: This paper presents WanJuan-CC, a safe and high-quality open-sourced English webtext dataset derived from Common Crawl data. The study addresses the challenges of constructing large-scale pre-training datasets for language models, which require vast amounts of high-quality data. A comprehensive process was designed to handle Common Crawl data, including extraction, heuristic rule filtering, fuzzy deduplication, content safety filtering, and data quality filtering. From approximately 68 billion original English documents, we obtained 2.22T Tokens of safe data and selected 1.0T Tokens of high-quality data as part of WanJuan-CC. We have open-sourced 300B Tokens from this dataset. The paper also provides statistical information related to data quality, enabling users to select appropriate data according to their needs. To evaluate the quality and utility of the dataset, we trained 1B-parameter and 3B-parameter models using WanJuan-CC and ano
    
[^53]: 什么时候词序重要，什么时候不重要？

    When does word order matter and when doesn't it?

    [https://arxiv.org/abs/2402.18838](https://arxiv.org/abs/2402.18838)

    本文研究了语言模型对词序的敏感度问题，通过量化词序信息量，发现在语言提示提供冗余信息时，模型对词序变化不敏感，且不同任务间的不敏感程度有所差异。

    

    在自然语言理解（NLU）任务中，语言模型（LMs）可能对词序变化不敏感。本文提出语言冗余性可以解释这一现象，即词序和其他语言提示（如格标）提供重叠且冗余信息。我们的假设是，当顺序提供冗余信息时，模型对词序的不敏感表现，而不同任务之间的不敏感程度各不相同。我们使用无序和打乱顺序的句子之间的互信息（MI）来量化词序的信息量。我们的结果显示，词序信息越不具信息量，模型在无序和打乱顺序的句子之间的预测越一致。我们还发现这种影响在不同任务之间存在差异：对于一些任务，如SST-2，LM的预测几乎总是与原始结果一致，即使点间互信息（PMI）发生变化，也是如此。

    arXiv:2402.18838v1 Announce Type: new  Abstract: Language models (LMs) may appear insensitive to word order changes in natural language understanding (NLU) tasks. In this paper, we propose that linguistic redundancy can explain this phenomenon, whereby word order and other linguistic cues such as case markers provide overlapping and thus redundant information. Our hypothesis is that models exhibit insensitivity to word order when the order provides redundant information, and the degree of insensitivity varies across tasks. We quantify how informative word order is using mutual information (MI) between unscrambled and scrambled sentences. Our results show the effect that the less informative word order is, the more consistent the model's predictions are between unscrambled and scrambled sentences. We also find that the effect varies across tasks: for some tasks, like SST-2, LMs' prediction is almost always consistent with the original one even if the Pointwise-MI (PMI) changes, while fo
    
[^54]: RORA：强大的自由文本理由评估

    RORA: Robust Free-Text Rationale Evaluation

    [https://arxiv.org/abs/2402.18678](https://arxiv.org/abs/2402.18678)

    RORA 提出了一种新的评估方法，用于衡量自由文本理由对标签的新信息贡献，并在评估中优于现有方法。

    

    自由文本理由在可解释的自然语言处理中发挥着重要作用，弥合了模型决策背后的知识和推理差距。然而，由于潜在推理路径的多样性及相应缺乏明确的真相基础，其评估仍然是一个挑战。现有的评估指标依赖于理由支持目标标签的程度，但我们发现这些指标在评估意外泄漏标签的理由时存在不足。为解决这个问题，我们提出了RORA，一种针对标签泄漏的强大的自由文本理由评估方法。RORA量化了理由为证明标签提供的新信息。这是通过评估与泄漏特征抗干扰的预测性家族条件V信息实现的。RORA在评估人工撰写、合成或模型生成的理由方面始终优于现有方法。

    arXiv:2402.18678v1 Announce Type: new  Abstract: Free-text rationales play a pivotal role in explainable NLP, bridging the knowledge and reasoning gaps behind a model's decision-making. However, due to the diversity of potential reasoning paths and a corresponding lack of definitive ground truth, their evaluation remains a challenge. Existing evaluation metrics rely on the degree to which a rationale supports a target label, but we find these fall short in evaluating rationales that inadvertently leak the labels. To address this problem, we propose RORA, a Robust free-text Rationale evaluation against label leakage. RORA quantifies the new information supplied by a rationale to justify the label. This is achieved by assessing the conditional V-information \citep{hewitt-etal-2021-conditional} with a predictive family robust against leaky features that can be exploited by a small model. RORA consistently outperforms existing approaches in evaluating human-written, synthetic, or model-gen
    
[^55]: Multi-FAct: 使用FActScore评估多语言LLM的多区域知识

    Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore

    [https://arxiv.org/abs/2402.18045](https://arxiv.org/abs/2402.18045)

    本文系统评估了多语言LLMs跨语言和地理区域的事实准确性，并发现英语在事实准确性和生成事实数量方面优于其他语言，同时多语言模型存在对来自西方大陆事实信息的偏见。

    

    大型语言模型（LLMs）容易出现事实上的幻觉，生成与已知知识相矛盾的文本。尽管广泛研究了英语中的这一问题，但对于多语言LLMs知之甚少。本文系统评估了多语言LLMs跨语言和地理区域的事实准确性。我们引入了一个新颖的多语言事实评估流程，将FActScore（Min等，2023）改编为多样化语言。我们在九种语言上的分析显示，英语在事实准确性和生成事实数量方面始终表现优异。此外，多语言模型展现出对来自西方大陆的事实信息的偏见。这些发现突显了对改进多语言事实性评估的需求，并强调了LLMs的事实生成中的地理偏见。

    arXiv:2402.18045v1 Announce Type: new  Abstract: Large Language Models (LLMs) are prone to factuality hallucination, generating text that contradicts established knowledge. While extensive research has addressed this in English, little is known about multilingual LLMs. This paper systematically evaluates multilingual LLMs' factual accuracy across languages and geographic regions. We introduce a novel pipeline for multilingual factuality evaluation, adapting FActScore(Min et al., 2023) for diverse languages. Our analysis across nine languages reveals that English consistently outperforms others in factual accuracy and quantity of generated facts. Furthermore, multilingual models demonstrate a bias towards factual information from Western continents. These findings highlight the need for improved multilingual factuality assessment and underscore geographical biases in LLMs' fact generation.
    
[^56]: 大型语言模型在表格数据上的应用--一项调查

    Large Language Models on Tabular Data -- A Survey

    [https://arxiv.org/abs/2402.17944](https://arxiv.org/abs/2402.17944)

    该研究综述了大型语言模型在处理表格数据上的应用，包括关键技术、指标、数据集、模型和优化方法，为未来研究方向提供了启示。

    

    大型语言模型在表格数据建模方面的应用取得了突破性进展，包括预测、表格数据合成、问答和表格理解等多种任务。每个任务都带来独特的挑战和机遇。然而，目前缺乏对该研究领域中关键技术、指标、数据集、模型和优化方法的全面审查。本调查旨在填补这一空白，总结并比较这些领域中的最新进展，提供对数据集、指标和方法论的全面调查和分类。它识别了现有文献中的优势、局限性、未开发领域和空白，同时为这一重要且快速发展的领域的未来研究方向提供了一些见解。它还提供了相关的代码和数据集引用。

    arXiv:2402.17944v1 Announce Type: new  Abstract: Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehen
    
[^57]: DrAttack: 提示分解和重构使强大的LLM越狱者

    DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers

    [https://arxiv.org/abs/2402.16914](https://arxiv.org/abs/2402.16914)

    将恶意提示分解为独立的子提示使得LLM越狱攻击更难被检测

    

    本文发现将恶意提示分解为独立的子提示能够有效模糊其潜在的恶意意图，使之以片段化、不易检测的形式呈现，从而解决了这些局限性。我们引入了一个用于越狱攻击的自动提示分解和重构框架（DrAttack）。DrAttack包括三个关键组件：(a) 将原始提示进行“分解”为子提示，(b) 通过上下文学习中的语义上相似但隐含的“重构”这些子提示

    arXiv:2402.16914v1 Announce Type: cross  Abstract: The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks, which adversarially trigger LLMs to output harmful content. However, current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned LLMs. This paper discovers that decomposing a malicious prompt into separated sub-prompts can effectively obscure its underlying malicious intent by presenting it in a fragmented, less detectable form, thereby addressing these limitations. We introduce an automatic prompt \textbf{D}ecomposition and \textbf{R}econstruction framework for jailbreak \textbf{Attack} (DrAttack). DrAttack includes three key components: (a) `Decomposition' of the original prompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly by in-context learning with semantically similar but h
    
[^58]: 基于似然的大型语言模型评估偏差的缓解

    Likelihood-based Mitigation of Evaluation Bias in Large Language Models

    [https://arxiv.org/abs/2402.15987](https://arxiv.org/abs/2402.15987)

    该论文研究了基于大型语言模型（LLM）的评估器中的似然偏差，并提出了一种缓解这种偏差的方法。

    

    大型语言模型(LLMs)被广泛用于评估自然语言生成任务的自动化指标。然而，似然作为衡量LLM对句子可信度的指标，可能会因句子表面差异（如词序和句子结构）而变化。因此，如果将LLMs用于评估，可能存在似然偏差：它们可能会高估具有较高似然性的句子，而低估具有较低似然性的句子。本文对LLM评估器中似然偏差的存在和影响进行了研究。我们还提出了一种缓解似然偏差的方法。我们的方法利用高度偏置的实例作为少样本示例进行上下文学习。我们在评估数据到文本和语法错误纠正任务时的实验结果显示，我们测试的几种LLMs显示出似然偏差。此外，我们提出的方法成功地减轻了这种偏差

    arXiv:2402.15987v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics. However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods. In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators. We also propose a method to mitigate the likelihood bias. Our method utilizes highly biased instances as few-shot examples for in-context learning. Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias. Furthermore, our proposed method successfully mitigates this bias, also impr
    
[^59]: MATHWELL: 在规模上生成教育数学应用题

    MATHWELL: Generating Educational Math Word Problems at Scale

    [https://arxiv.org/abs/2402.15861](https://arxiv.org/abs/2402.15861)

    使用MATHWELL模型生成了迄今为止最大的英文数学应用题数据集，其中包含20,490个问题，经领域专家评分结果显示，MATHWELL的问题中具有可执行解决方案并符合所有标准的份额比其他选择高出40%，其中74%的可解决问题同时做到了准确和适当。

    

    数学应用题在K-8教育中至关重要，但编写它们耗时且需要领域专业知识。我们认为语言模型可以通过自动生成规模化问题来支持K-8数学教育。为了教育性，生成的问题必须是1）可解决的，2）准确的，3）适当的。现有数据集未标记这些标准，因此不适合训练问题生成器。我们引入了MATHWELL，这是一个经过专家注释数据进行迭代微调的70B Llama-2模型，用于生成K-8数学应用题。借助MATHWELL，我们生成了迄今为止最大的英文应用题数据集，其中包含20,490个问题。经领域专家评分的3,484个问题发现，MATHWELL拥有比其他选择更高的可执行解决方案和满足所有标准的问题份额高出40％，其中74％的问题具有可解的、准确的和适当的解决方案。

    arXiv:2402.15861v1 Announce Type: new  Abstract: Math word problems are critical K-8 educational tools, but writing them is time-consuming and requires domain expertise. We suggest that language models can support K-8 math education by automatically generating problems at scale. To be educational, generated problems must be 1) solvable, 2) accurate, and 3) appropriate. Existing datasets are unlabeled for these criteria, making them ill-suited for training problem generators. We introduce MATHWELL, a Llama-2 (70B) model iteratively finetuned to generate K-8 math word problems using data from expert annotation. Using MATHWELL, we generate the largest English word problem dataset to date, containing 20,490 problems. 3,484 are scored by domain experts who find MATHWELL has a 40% higher share of problems that have executable solutions and meet all criteria than alternatives, with 74% of its problems with executable solutions being solvable, accurate, and appropriate.
    
[^60]: 偏见和反复无常：衡量大型语言模型社会歧视的统计框架

    Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models

    [https://arxiv.org/abs/2402.15481](https://arxiv.org/abs/2402.15481)

    提出了Prejudice-Caprice Framework（PCF）来全面衡量LLMs中的歧视，考虑了它们在不同上下文中的一贯偏见偏好和偏好变化。

    

    arXiv:2402.15481v1 公告类型: 新的 摘要: 大型语言模型（LLMs）在社会运营中的日益融合加剧了它们对经济、法律、教育和医疗等重要领域决策的影响，引发了公众对这些模型涉及歧视安全和可靠性的担忧。然而，先前的歧视测量框架仅评估LLMs的平均歧视行为，往往由于忽视了一个额外的导致歧视的因素，即LLMs在不同上下文中的预测变化而变得不足。在这项工作中，我们提出了Prejudice-Caprice Framework（PCF），通过考虑LLMs的一贯偏见偏好和在多样上

    arXiv:2402.15481v1 Announce Type: new  Abstract: The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability. However, prior discrimination measuring frameworks solely assess the average discriminatory behavior of LLMs, often proving inadequate due to the overlook of an additional discrimination-leading factor, i.e., the LLMs' prediction variation across diverse contexts. In this work, we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently biased preference and preference variation across diverse contexts. Specifically, we mathematically dissect the aggregated contextualized discrimination risk of LLMs into prejudice risk, originating from LLMs' persistent prejudice, and caprice risk, stemmin
    
[^61]: 有关LLMs指令中心响应的（不道德）程度有多高？揭示安全防护栏对有害查询的漏洞

    How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries

    [https://arxiv.org/abs/2402.15302](https://arxiv.org/abs/2402.15302)

    本研究探讨了大型语言模型（LLMs）对指令中心响应的容忍度，并提出了一个包含复杂查询的数据集，旨在揭示触发不道德响应的方法。

    

    在这项研究中，我们解决了一个围绕大型语言模型（LLMs）安全和道德使用日益关注的问题。尽管这些模型具有潜力，但它们可能会被各种复杂的方法欺骗，产生有害或不道德内容，包括“越狱”技术和有针对性的操纵。我们的工作集中在一个特定问题上：LLMs在要求它们生成以伪代码、程序或软件片段为中心的响应时，有多大程度上可能会被误导，而不是生成普通文本。为了调查这个问题，我们引入了TechHazardQA，一个数据集，其中包含应以文本和以指令为中心格式（例如伪代码）回答的复杂查询，旨在识别不道德响应的触发器。我们查询了一系列LLMs-- Llama-2-13b，Llama-2-7b，Mistral-V2和Mistral 8X7B--并要求它们生成文本和指令为中心的响应。为了评估我们的方法，

    arXiv:2402.15302v1 Announce Type: new  Abstract: In this study, we tackle a growing concern around the safety and ethical use of large language models (LLMs). Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including 'jailbreaking' techniques and targeted manipulation. Our work zeroes in on a specific issue: to what extent LLMs can be led astray by asking them to generate responses that are instruction-centric such as a pseudocode, a program or a software snippet as opposed to vanilla text. To investigate this question, we introduce TechHazardQA, a dataset containing complex queries which should be answered in both text and instruction-centric formats (e.g., pseudocodes), aimed at identifying triggers for unethical responses. We query a series of LLMs -- Llama-2-13b, Llama-2-7b, Mistral-V2 and Mistral 8X7B -- and ask them to generate both text and instruction-centric responses. For evaluation we rep
    
[^62]: CLoVe: 在对比视觉-语言模型中编码组合性语言

    CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models

    [https://arxiv.org/abs/2402.15021](https://arxiv.org/abs/2402.15021)

    本文提出了一个框架，显著提高现有模型编码组合性语言的能力，在组合性基准上取得超过10% 的绝对改进，同时保持或提高了在标准对象识别和检索基准上的性能。

    

    近年来，视觉和语言任务的表现显著提升。基础视觉-语言模型（VLMs）如CLIP已在多个设置中得到应用，并展示了在几个任务上的显著性能。这些模型擅长于对象中心识别，但学习的文本表示似乎对词序不变，未能以新颖方式组成已知概念。然而，没有证据表明任何VLM，包括大规模单流模型如GPT-4V，成功识别组合。本文介绍了一个框架，显著提高了现有模型编码组合性语言的能力，在组合性基准上取得了超过10% 的绝对改进，同时在标准对象识别和检索基准上保持或提高了性能。我们的代码和预训练模型可在https://github.com/netflix/处公开获取。

    arXiv:2402.15021v1 Announce Type: cross  Abstract: Recent years have witnessed a significant increase in the performance of Vision and Language tasks. Foundational Vision-Language Models (VLMs), such as CLIP, have been leveraged in multiple settings and demonstrated remarkable performance across several tasks. Such models excel at object-centric recognition yet learn text representations that seem invariant to word order, failing to compose known concepts in novel ways. However, no evidence exists that any VLM, including large-scale single-stream models such as GPT-4V, identifies compositions successfully. In this paper, we introduce a framework to significantly improve the ability of existing models to encode compositional language, with over 10% absolute improvement on compositionality benchmarks, while maintaining or improving the performance on standard object-recognition and retrieval benchmarks. Our code and pre-trained models are publicly available at https://github.com/netflix/
    
[^63]: 名字的含义是什么？审计大型语言模型中的种族和性别偏见

    What's in a Name? Auditing Large Language Models for Race and Gender Bias

    [https://arxiv.org/abs/2402.14875](https://arxiv.org/abs/2402.14875)

    调查发现，大型语言模型存在种族和性别偏见，尤其对与黑人女性相关的名字表现最不利。审计在模型部署和实施时的重要性得到强调。

    

    我们采用审计设计来调查最先进的大型语言模型中的偏见，包括GPT-4。在我们的研究中，我们引发模型在各种情景下为个人提供建议，比如在购车谈判或选举结果预测过程中。我们发现该建议系统性地对与种族少数群体和女性常见相关的名字产生不利影响。与黑人女性相关的名字得到的结果最不利。这些偏见在42个提示模板和多个模型中都是一致的，表明这是一个系统性问题，而不是孤立事件。在提示中提供数值、与决策相关的锚点可以成功抵消偏见，而定性细节的影响并不一致，甚至可能会加剧差异。我们的研究结果强调了在语言模型部署和实施时进行审计的重要性，以减轻其潜在影响。

    arXiv:2402.14875v1 Announce Type: cross  Abstract: We employ an audit design to investigate biases in state-of-the-art large language models, including GPT-4. In our study, we elicit prompt the models for advice regarding an individual across a variety of scenarios, such as during car purchase negotiations or election outcome predictions. We find that the advice systematically disadvantages names that are commonly associated with racial minorities and women. Names associated with Black women receive the least advantageous outcomes. The biases are consistent across 42 prompt templates and several models, indicating a systemic issue rather than isolated incidents. While providing numerical, decision-relevant anchors in the prompt can successfully counteract the biases, qualitative details have inconsistent effects and may even increase disparities. Our findings underscore the importance of conducting audits at the point of LLM deployment and implementation to mitigate their potential for
    
[^64]: 你能通过下一个词预测学习语义吗？以蕴涵为例

    Can You Learn Semantics Through Next-Word Prediction? The Case of Entailment

    [https://arxiv.org/abs/2402.13956](https://arxiv.org/abs/2402.13956)

    作者调查了神经LM是否可以通过下一个词预测来解码蕴涵判断，发现它们可以远高于随机几率地解码自然句子之间的蕴涵关系，暗示LM隐含地模拟了语义的某些方面。

    

    Merrill等人（2022）认为，在理论上，最优LM预测的概率编码了关于蕴涵关系的语义信息，但是由于Merrill等人提出的强烈理想化假设，不清楚神经LM在训练语料库上是否通过这种方式学习蕴涵。在这项工作中，我们调查了他们的理论是否可以用于从神经LM中解码蕴涵判断。我们发现类似于他们的测试可以在许多数据集和LM中解码自然句子之间的蕴涵关系，远远超过随机机会，尽管不是完美的。这表明LM隐含地模拟了语义的某些方面，以预测句子共现模式上的语义效应。但是，我们发现实际上预测蕴涵的测试与理论测试的方向相反。因此，我们重新审视了潜在的理论假设。

    arXiv:2402.13956v1 Announce Type: new  Abstract: Do LMs infer the semantics of text from co-occurrence patterns in their training data? Merrill et al. (2022) argue that, in theory, probabilities predicted by an optimal LM encode semantic information about entailment relations, but it is unclear whether neural LMs trained on corpora learn entailment in this way because of strong idealizing assumptions made by Merrill et al. In this work, we investigate whether their theory can be used to decode entailment judgments from neural LMs. We find that a test similar to theirs can decode entailment relations between natural sentences, well above random chance, though not perfectly, across many datasets and LMs. This suggests LMs implicitly model aspects of semantics to predict semantic effects on sentence co-occurrence patterns. However, we find the test that predicts entailment in practice works in the opposite direction to the theoretical test. We thus revisit the assumptions underlying the o
    
[^65]: Neeko：利用动态LoRA实现高效多角色扮演代理

    Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent

    [https://arxiv.org/abs/2402.13717](https://arxiv.org/abs/2402.13717)

    Neeko利用动态低秩适配器（LoRA）策略，有效处理多角色扮演过程中的挑战，提升了对不同属性、个性和说话模式的适应能力。

    

    大型语言模型（LLMs）在开放领域对话代理程序中起着革命性作用，但在多角色扮演（MCRP）场景中遇到挑战。为了解决这个问题，我们提出了Neeko，这是一个专为高效模仿多个角色而设计的创新框架。与现有方法不同，Neeko采用动态低秩适配器（LoRA）策略，使其能够无缝适应不同的角色。我们的框架将角色扮演过程分解为代理预训练、多个角色扮演和角色增量学习，有效处理已知和未知角色。这种动态方法，结合为每个角色设计的独特LoRA块，增强了Neeko对独特属性、个性和说话模式的适应能力。因此，Neeko在MCRP方面表现出比大多数现有方法更出色的性能，为用户提供更具吸引力和多样化的互动体验。代码和数据可在（链接中提供）。

    arXiv:2402.13717v1 Announce Type: new  Abstract: Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios. To address the issue, we present Neeko, an innovative framework designed for efficient multiple characters imitation. Unlike existing methods, Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters. Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles. This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko's adaptability to unique attributes, personalities, and speaking patterns. As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences. Code and data are available at
    
[^66]: 基于社交媒体的中文多模态实体识别数据集CMNER

    CMNER: A Chinese Multimodal NER Dataset based on Social Media

    [https://arxiv.org/abs/2402.13693](https://arxiv.org/abs/2402.13693)

    本研究在中国最大的社交媒体平台微博上构建了一个中文多模态实体识别数据集（CMNER），包含5,000条微博帖子和18,326张对应图片，并展示了将图片纳入NER任务中的有效性。

    

    多模态命名实体识别（MNER）是一项旨在从文本中提取命名实体的关键任务，其通过相关图片提供支持。然而，在中文MNER领域，数据明显不足，严重阻碍了该自然语言处理任务的进展。因此，在本研究中，我们利用来自微博的数据编制了一个中文多模态实体识别数据集（CMNER）。我们的数据集包括5,000条微博帖子，配对18,326张对应的图片。实体被分类为四个不同类别：人物、地点、组织和杂项。我们在CMNER上进行了基线实验，结果强调了将图片纳入NER中的有效性。此外，我们在公开的英文MNER数据集（Twitter2015）上进行了跨语言实验，结果证实了我们的假设，即中文和英文之间存在一致性。

    arXiv:2402.13693v1 Announce Type: new  Abstract: Multimodal Named Entity Recognition (MNER) is a pivotal task designed to extract named entities from text with the support of pertinent images. Nonetheless, a notable paucity of data for Chinese MNER has considerably impeded the progress of this natural language processing task within the Chinese domain. Consequently, in this study, we compile a Chinese Multimodal NER dataset (CMNER) utilizing data sourced from Weibo, China's largest social media platform. Our dataset encompasses 5,000 Weibo posts paired with 18,326 corresponding images. The entities are classified into four distinct categories: person, location, organization, and miscellaneous. We perform baseline experiments on CMNER, and the outcomes underscore the effectiveness of incorporating images for NER. Furthermore, we conduct cross-lingual experiments on the publicly available English MNER dataset (Twitter2015), and the results substantiate our hypothesis that Chinese and Eng
    
[^67]: DoRA: 分解权重的低秩适应

    DoRA: Weight-Decomposed Low-Rank Adaptation

    [https://arxiv.org/abs/2402.09353](https://arxiv.org/abs/2402.09353)

    DoRA是一种将预训练权重分解为幅度和方向两个组成部分，并使用LoRA进行方向更新的低秩适应方法，通过增强学习能力和训练稳定性来提高对LLaMA，LLaVA和VL-B的微调性能。

    

    在广泛使用的参数高效调整（PEFT）方法中，由于避免了额外的推理成本，LoRA及其变种方法因此变得非常流行。然而，这些方法与完全微调（FT）之间仍然存在精度差距。在这项工作中，我们首先引入了一种新颖的权重分解分析方法来研究FT和LoRA之间的内在差异。为了模拟FT的学习能力，我们提出了一种称为DoRA的权重分解低秩适应方法。DoRA将预训练的权重分解为两个组成部分，幅度和方向，并且具体使用LoRA进行方向更新，以有效地减少可训练参数的数量。通过使用DoRA，我们增强了LoRA的学习能力和训练稳定性，同时避免了任何额外的推理开销。在微调LLaMA，LLaVA和VL-B上，DoRA始终优于LoRA。

    arXiv:2402.09353v1 Announce Type: new Abstract: Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-B
    
[^68]: UFO: 一个专注于Windows操作系统交互的用户界面智能体

    UFO: A UI-Focused Agent for Windows OS Interaction

    [https://arxiv.org/abs/2402.07939](https://arxiv.org/abs/2402.07939)

    UFO是一个专注于Windows操作系统上应用程序的用户界面智能体，利用GPT-Vision的能力来满足用户需求。它通过观察和分析Windows应用程序的图形用户界面和控制信息，实现无缝导航和操作以满足用户的请求。UFO的控制交互模块使得无需人工干预即可实现动作连接和完全自动化执行，使繁琐和耗时的过程变为简单任务。经过测试，UFO在各种场景中取得了良好效果。

    

    我们介绍了UFO，一个创新的专注于Windows操作系统上应用程序的用户界面智能体，利用了GPT-Vision的能力来满足用户需求。UFO采用双智能体框架，精确观察和分析Windows应用程序的图形用户界面（GUI）和控制信息。这使得智能体可以无缝地在单个应用程序内以及跨应用程序进行导航和操作，以满足用户的需求，即使涉及多个应用程序。该框架包括一个控制交互模块，实现无需人工干预的动作连接，并实现完全自动化执行。因此，UFO将艰巨而耗时的过程转变为仅通过自然语言命令就可以完成的简单任务。我们在9个流行的Windows应用程序上对UFO进行了测试，涵盖了反映用户日常使用情景的各种情况。通过定量指标和真实案例研究得出的结果强调了UFO的效果。

    We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users' daily usage. The results, derived from both quantitative metrics and real-case studies, underscore t
    
[^69]: 来自在线人工智能反馈的直接语言模型对齐

    Direct Language Model Alignment from Online AI Feedback

    [https://arxiv.org/abs/2402.04792](https://arxiv.org/abs/2402.04792)

    本论文提出了一种名为OAIF的方法，通过在线反馈来改善DAP方法，该方法使用LLM作为标注器，从而在多个任务中展示出优于离线DAP和RLHF的性能

    

    最近，直接对齐偏好（DAP）方法如DPO已成为对于从人类反馈中进行增强学习的高效替代方法，不要求单独的奖励模型。然而，DAP方法中使用的偏好数据集通常在训练之前收集，并且从不更新，因此反馈纯粹是离线的。此外，这些数据集中的回应通常是从一个与被对齐的语言模型不同的语言模型中采样的，由于模型在训练过程中会变化，对齐阶段必然是非策略的。在本研究中，我们认为在线反馈是关键，可以改善DAP方法。我们的方法，在线人工智能反馈（OAIF），使用LLM作为标注器：在每个训练迭代中，我们从当前模型中采样两个回应，并提示LLM标注器选择哪个更受欢迎，从而提供在线反馈。尽管简单，但通过多个任务中的人工评估，我们证明OAIF优于离线DAP和RLHF

    Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF 
    
[^70]: 大型语言模型作为MOOCs评分者

    Large Language Models As MOOCs Graders

    [https://arxiv.org/abs/2402.03776](https://arxiv.org/abs/2402.03776)

    该研究探索了利用大型语言模型（LLMs）代替MOOCs中同伴评分的可行性，旨在解决大规模在线开放课程中评估学生写作任务的问题。

    

    大规模在线开放课程（MOOCs）为拥有电脑和互联网访问权限的全球任何人提供免费教育的机会。尽管如此，这些课程的大规模注册意味着一位教师几乎不可能评估每个学生的写作任务。因此，同伴评分通常是首选方法，通常由简单明了的评分标准指导。然而，同伴评分在可靠度和有效性方面常常存在问题。在这项研究中，我们利用18个不同的场景，探索利用大型语言模型（LLMs）替代MOOCs中的同伴评分的可行性。具体而言，我们关注两种最先进的LLMs：GPT-4和GPT-3.5，并涵盖三门不同的课程：入门天文学，天体生物学以及天文学的历史与哲学。为了训练LLMs，我们使用了基于零-shot连续思考（Zero-shot-CoT）提示技术的变种的三个不同提示：结合Zero-shot-CoT的提示。

    Massive open online courses (MOOCs) unlock the doors to free education for anyone around the globe with access to a computer and the internet. Despite this democratization of learning, the massive enrollment in these courses means it is almost impossible for one instructor to assess every student's writing assignment. As a result, peer grading, often guided by a straightforward rubric, is the method of choice. While convenient, peer grading often falls short in terms of reliability and validity. In this study, using 18 distinct settings, we explore the feasibility of leveraging large language models (LLMs) to replace peer grading in MOOCs. Specifically, we focus on two state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy. To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with
    
[^71]: 从推理路径聚合的角度理解语言模型的推理能力

    Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation

    [https://arxiv.org/abs/2402.03268](https://arxiv.org/abs/2402.03268)

    本文研究了预训练语言模型的推理能力，并提出了从聚合间接推理路径的角度理解语言模型如何产生推理能力。通过对知识图谱和数学问题数据集进行实验和分析，发现增加无标签的随机游走推理路径可以提高实际应用中的多步推理能力。

    

    预训练的语言模型能够在没有明确微调的情况下执行复杂的推理。为了理解预训练与下一个标记预测目标的关系如何促使推理能力的出现，我们提出可以将语言模型视为在预训练时通过聚合间接的推理路径来得出新结论。我们发现，这个视角在逻辑推理和数学推理等关键情况下非常有效。具体而言，我们将推理路径形式化为在知识/推理图上的随机游走路径。对学习的语言模型分布的分析表明，相关随机游走路径概率的加权和是解释语言模型推理的合理方式。对多个知识图谱和数学问题数据集进行的实验和分析揭示了训练对随机游走路径的影响，并表明增加无标签的随机游走推理路径可以提高现实世界的多步推理能力。

    Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs). More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step r
    
[^72]: 链式反馈：缓解回答不一致性的影响

    Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses

    [https://arxiv.org/abs/2402.02648](https://arxiv.org/abs/2402.02648)

    本文提出了链式反馈（CoF）方法，以缓解大型语言模型在回答问题中出现不一致性的问题。同时，作者还提出了递归链式反馈（R-CoF）方法，并提到了进一步的研究。这些方法可以提高回答的可靠性和有效性。

    

    大型语言模型（LLMs）在回答知识密集型问题时经常出现不一致的情况，即使输入相同，也会提供不同的输出。当用户表达坚决相反的立场时，LLMs调整其回答的质量会变差，尽管初始回答是正确的。这些行为降低了这些模型提供的回答的可靠性和有效性。在本文中，我们试图：1）通过展示链式反馈（CoF）如何导致LLMs更加偏离实际答案，引起过度依赖ChatGPT等AI代理带来的固有风险；2）提出一种新的提示方法，递归链式反馈（R-CoF），我们正在进行进一步研究。CoF系统接收一个开放式多步问题，然后我们重复提供无意义的反馈，要求再次尝试。我们的初步实验表明，这种反馈只会降低回答的质量。另一方面，

    Large Language Models (LLMs) frequently suffer from knowledge-intensive questions, often being inconsistent by providing different outputs despite given the same input. The response quality worsens when the user expresses a firm opposing stance which causes the LLMs to adjust its response despite the correct initial one. These behaviors decrease the reliability and validity of the responses provided by these models. In this paper, we attempt to 1) raise awareness of the inherent risks that follow from overly relying on AI agents like ChatGPT by showing how Chain-of-Feedback (CoF) triggers LLMs to deviate more from the actual answer and 2) suggest a novel prompting method, Recursive Chain of Feedback (R-CoF), that we are conducting further study. The CoF system takes in an open-ended multi-step question. Then, we repetitively provide meaningless feedback requesting another attempt. Our preliminary experiments show that such feedback only decreases the quality of the response. On the oth
    
[^73]: 在大型语言模型中测量道德不一致性

    Measuring Moral Inconsistencies in Large Language Models

    [https://arxiv.org/abs/2402.01719](https://arxiv.org/abs/2402.01719)

    本研究提出了一种新的信息论度量方法，称为语义图熵（SGE），用于测量道德情景中大型语言模型（LLM）的一致性。与现有的一致性度量方法相比，SGE在五个LLMs上与人类判断更好地相关，为研究LLM不一致性的根本原因提供了新的思路。

    

    如果语义等价的提示产生语义等价的响应，那么大型语言模型(LLM)被认为是一致的。尽管最近的进展展示了LLMs在对话系统中令人印象深刻的能力，但我们表明即使是最先进的LLMs在生成方面也存在高度不一致性，这对它们的可靠性提出了质疑。先前的研究尝试用任务特定的准确度来衡量这一点。然而，这种方法对于没有“正确”答案的道德情景（例如，道路交运问题）是不合适的。为了解决这个问题，我们提出了一种新的信息论度量方法，称为语义图熵（SGE），来衡量LLM在道德情景中的一致性。我们利用“经验法则”（RoTs）来解释模型的决策策略，并进一步增强我们的度量方法。与现有的一致性度量方法相比，SGE与人类判断在五个LLMs上更好地相关。在未来，我们的目标是调查LLM不一致性的根本原因。

    A Large Language Model~(LLM) is considered consistent if semantically equivalent prompts produce semantically equivalent responses. Despite recent advancements showcasing the impressive capabilities of LLMs in conversational systems, we show that even state-of-the-art LLMs are highly inconsistent in their generations, questioning their reliability. Prior research has tried to measure this with task-specific accuracies. However, this approach is unsuitable for moral scenarios, such as the trolley problem, with no ``correct'' answer. To address this issue, we propose a novel information-theoretic measure called Semantic Graph Entropy~(SGE) to measure the consistency of an LLM in moral scenarios. We leverage ``Rules of Thumb''~(RoTs) to explain a model's decision-making strategies and further enhance our metric. Compared to existing consistency metrics, SGE correlates better with human judgments across five LLMs. In the future, we aim to investigate the root causes of LLM inconsistencies 
    
[^74]: LatestEval: 通过动态和时间敏感的测试构建解决语言模型评估中的数据污染问题

    LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction

    [https://arxiv.org/abs/2312.12343](https://arxiv.org/abs/2312.12343)

    LatestEval提出了一种自动方法，通过动态和时间敏感的测试构建不受数据污染的阅读理解评估，避免了使用预先训练语言模型的训练语料库，从而鼓励模型更好地推断答案。

    

    随着预先在超大规模自动抓取语料库上进行训练的语言模型的出现，评估中的数据污染越来越普遍。这个问题导致在准确评估模型能力和泛化能力方面存在重大挑战。在本文中，我们提出了LatestEval，这是一种自动方法，利用最近的文本创建不受污染的阅读理解评估。LatestEval通过仅使用在最近时间窗口内发布的文本来避免数据污染，确保不会与预先训练语言模型的训练语料库重叠。我们开发了LatestEval自动化流水线，用于1）收集最新文本；2）识别关键信息，以及3）构建针对该信息的问题，同时从上下文中删除现有答案。这鼓励模型根据剩余上下文推断答案，而不仅是复制粘贴。

    arXiv:2312.12343v3 Announce Type: replace-cross  Abstract: Data contamination in evaluation is getting increasingly prevalent with the emergence of language models pre-trained on super large, automatically crawled corpora. This problem leads to significant challenges in the accurate assessment of model capabilities and generalisations. In this paper, we propose LatestEval, an automatic method that leverages the most recent texts to create uncontaminated reading comprehension evaluations. LatestEval avoids data contamination by only using texts published within a recent time window, ensuring no overlap with the training corpora of pre-trained language models. We develop the LatestEval automated pipeline to 1) gather the latest texts; 2) identify key information, and 3) construct questions targeting the information while removing the existing answers from the context. This encourages models to infer the answers themselves based on the remaining context, rather than just copy-paste. Our e
    
[^75]: 强化关注力中最短的支柱：增强大型语言模型的上下文意识，以实现有效的工具使用

    Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use

    [https://arxiv.org/abs/2312.04455](https://arxiv.org/abs/2312.04455)

    本文证明了大型语言模型中关注分配的波形模式对其在需要高度上下文意识的任务中的性能有显著影响。我们提出了一种名为“Attention Buckets”的推理方法，通过多个并行过程和不同的旋转位置嵌入角度，增强了模型对不同上下文位置的意识，从而减轻了忽视关键信息的风险。

    

    在本文中，我们证明了大型语言模型(LLMs)中关注分配中的内在波形模式显著影响它们在需要高度上下文意识的任务中的性能，例如利用LLMs进行工具使用。具体而言，当关键信息在上下文中位于关注波形的低谷区域时，模型可能会忽视该信息，导致性能下降。为了解决这个问题，我们提出了一种名为“Attention Buckets”的新型推理方法。它允许LLMs通过多个并行过程处理输入。每个过程使用不同的基准角度进行旋转位置嵌入，从而创建出一个独特的关注波形。通过用一个过程的关注低谷补偿另一个过程的关注高峰，我们的方法增强了LLM对不同上下文位置的意识，从而减轻了忽视关键信息的风险。

    In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchm
    
[^76]: 一刀切并非银弹：定制开放领域的操作流程

    One Size Does Not Fit All: Customizing Open-Domain Procedures

    [https://arxiv.org/abs/2311.09510](https://arxiv.org/abs/2311.09510)

    测试了多种简单的多LLM代理体系结构，发现两个LLM代理按顺序使用的简单结构表现最佳，一个编辑通用操作流程，另一个验证可执行性，在定制操作流程方面取得了显著效果，有望进一步探索多代理编辑体系结构的价值。

    

    如何操作的流程，比如如何种植花园，现在被数百万用户使用，但有时需要自定义以满足用户的特定需求，例如不使用杀虫剂种植花园。我们的目标是衡量和改进一个LLM执行此类定制的能力。我们的方法是测试几种简单的多LLM代理体系结构，用新的评估集CustomPlans对其进行定制，该数据集由200多个WikiHow操作流程组成，每个流程都有特定的定制需求。我们发现简单的具有两个LLM代理的体系结构表现最佳，它们按顺序使用，一个编辑通用操作流程，另一个验证其可执行性，明显优于端到端提示的LLM（10.5％绝对）。这表明LLM可以被合理有效地配置以进行操作流程的定制。这也表明，多代理编辑体系结构可能值得进一步探讨，以用于其他类型的定制。

    arXiv:2311.09510v2 Announce Type: replace  Abstract: How-to procedures, such as how to plant a garden, are now used by millions of users, but sometimes need customizing to meet a user's specific needs, e.g., planting a garden without pesticides. Our goal is to measure and improve an LLM's ability to perform such customization. Our approach is to test several simple multi-LLM-agent architectures for customization, as well as an end-to-end LLM, using a new evaluation set, called CustomPlans, of over 200 WikiHow procedures each with a customization need. We find that a simple architecture with two LLM agents used sequentially performs best, one that edits a generic how-to procedure and one that verifies its executability, significantly outperforming (10.5% absolute) an end-to-end prompted LLM. This suggests that LLMs can be configured reasonably effectively for procedure customization. This also suggests that multi-agent editing architectures may be worth exploring further for other custo
    
[^77]: SAIE框架：单纯支持并不足够-通过对抗性评注推进LLM训练

    SAIE Framework: Support Alone Isn't Enough -- Advancing LLM Training with Adversarial Remarks

    [https://arxiv.org/abs/2311.08107](https://arxiv.org/abs/2311.08107)

    通过在训练过程中引入交互式讨论，SAIE框架促进了学习模型与伙伴模型之间的支持性和对抗性对话，从而提升模型的理解能力和推理能力。

    

    大型语言模型（LLMs）可以通过与其他模型或人类讨论来证明或批评其预测结果，从而丰富它们对实例的内在理解。尽管已经证明推理阶段的主动讨论可以提高性能，但这类交互在训练阶段并未得到充分探讨。我们假设将交互式讨论纳入训练过程可以增强模型的理解能力，并改进它们的推理和语言表达能力。本文介绍了SAIE框架，该框架促进了学习模型与伙伴模型之间的支持性和对抗性讨论。学习模型接收伙伴的回复，然后根据此讨论更新其参数。这种动态调整过程在整个训练阶段持续进行，以响应学习模型不断发展的输出。

    arXiv:2311.08107v2 Announce Type: replace  Abstract: Large Language Models (LLMs) can justify or critique their predictions through discussions with other models or humans, thereby enriching their intrinsic understanding of instances. While proactive discussions in the inference phase have been shown to boost performance, such interactions have not been extensively explored during the training phase. We hypothesize that incorporating interactive discussions into the training process can enhance the models' understanding and improve their reasoning and verbal expression abilities during inference. This work introduces the SAIE framework, which facilitates supportive and adversarial discussions between learner and partner models. The learner model receives responses from the partner, and its parameters are then updated based on this discussion. This dynamic adjustment process continues throughout the training phase, responding to the evolving outputs of the learner model. Our empirical e
    
[^78]: 使用QLoRA调整的大型语言模型在德国议会辩论中的发言人归属

    Speaker attribution in German parliamentary debates with QLoRA-adapted large language models

    [https://arxiv.org/abs/2309.09902](https://arxiv.org/abs/2309.09902)

    使用QLoRA调整的大型语言模型在德国议会辩论中自动化发言人归属，为计算分析政治话语提供了有前途的途径。

    

    不断增长的政治文本为深入了解政治动态和意识形态提供了新的机会，但也增加了手动分析的工作量。自动发言人归属可以检测言辞事件中谁对谁说了什么，并与语义角色标注密切相关，是计算文本分析的重要步骤。我们研究了大型语言模型家族LLama 2在2017-2021年德国议会辩论中自动化发言人归属的潜力。我们使用QLoRA对Llama 2进行微调，观察到我们的方法在GermEval 2023共享任务中取得了竞争性表现。我们的结果揭示了大型语言模型在自动化发言人归属方面的能力，为政治话语的计算分析和发展开辟了有前途的途径。

    arXiv:2309.09902v2 Announce Type: replace  Abstract: The growing body of political texts opens up new opportunities for rich insights into political dynamics and ideologies but also increases the workload for manual analysis. Automated speaker attribution, which detects who said what to whom in a speech event and is closely related to semantic role labeling, is an important processing step for computational text analysis. We study the potential of the large language model family Llama 2 to automate speaker attribution in German parliamentary debates from 2017-2021. We fine-tune Llama 2 with QLoRA, an efficient training strategy, and observe our approach to achieve competitive performance in the GermEval 2023 Shared Task On Speaker Attribution in German News Articles and Parliamentary Debates. Our results shed light on the capabilities of large language models in automating speaker attribution, revealing a promising avenue for computational analysis of political discourse and the develo
    
[^79]: MuLTI：基于文本引导的多途采样器和多选模型的高效视频与语言理解

    MuLTI: Efficient Video-and-Language Understanding with Text-Guided MultiWay-Sampler and Multiple Choice Modeling

    [https://arxiv.org/abs/2303.05707](https://arxiv.org/abs/2303.05707)

    MuLTI是一种高精度高效的视频与语言理解模型，通过设计基于文本引导的多途采样器，实现了高效有效的特征融合和快速适应下游任务

    

    视频与语言理解在行业中有各种应用，如视频问答、文本视频检索和多标签分类。现有的视频与语言理解方法一般采用繁重的多模态编码器和特征融合模块，消耗高计算成本。本文提出了MuLTI，一种高精度高效的视频与语言理解模型，实现了高效有效的特征融合和快速适应下游任务。具体而言，我们设计了一种基于自适应池化残差映射和自注意力模块的文本引导多途采样器，用于采样长序列和融合多模态特征，从而降低了计算成本，并解决了之前采样器导致的性能降级问题。

    arXiv:2303.05707v2 Announce Type: replace-cross  Abstract: Video-and-language understanding has a variety of applications in the industry, such as video question answering, text-video retrieval, and multi-label classification. Existing video-and-language understanding methods generally adopt heavy multi-modal encoders and feature fusion modules, which consume high computational costs. Specially, they have difficulty dealing with dense video frames or long text prevalent in industrial applications. This paper proposes MuLTI, a highly accurate and efficient video-and-language understanding model that achieves efficient and effective feature fusion and rapid adaptation to downstream tasks. Specifically, we design a Text-Guided MultiWay-Sampler based on adapt-pooling residual mapping and self-attention modules to sample long sequences and fuse multi-modal features, which reduces the computational costs and addresses performance degradation caused by previous samplers. Therefore, MuLTI can 
    
[^80]: InceptionXML：一种带有同步负采样的轻量级框架，用于短文本极端分类

    InceptionXML: A Lightweight Framework with Synchronized Negative Sampling for Short Text Extreme Classification

    [https://arxiv.org/abs/2109.07319](https://arxiv.org/abs/2109.07319)

    提出了一种轻量级框架InceptionXML，通过在embedding维度上重新分配卷积操作，应对短文本查询中的单词顺序缺失，同时提出了InceptionXML+框架，通过同步标签筛选器和极端分类器，改进了动态硬负采样技术。

    

    短文本数据对大量目标标签进行自动注释，被称为短文本极端分类，已经在许多应用中得到应用，包括相关搜索预测和产品推荐任务。本文提出了一种卷积架构InceptionXML，其轻量但功能强大，并且能够应对搜索和推荐任务中短文本查询中固有的缺乏单词顺序的特点。我们通过将卷积的操作沿着嵌入维度重新构建，而不是像传统CNNs一样沿着单词维度进行文本分类，证明了应用卷积的有效性。为了将我们的模型扩展到具有数百万标签的数据集，我们还提出了InceptionXML+框架，通过同步标签筛选器和极端分类器，改进了最近提出的动态硬负采样技术在标签筛选中的缺陷。

    arXiv:2109.07319v3 Announce Type: replace-cross  Abstract: Automatic annotation of short-text data to a large number of target labels, referred to as Short Text Extreme Classification, has found numerous applications including prediction of related searches and product recommendation tasks. In this paper, we propose a convolutional architecture InceptionXML which is light-weight, yet powerful, and robust to the inherent lack of word-order in short-text queries encountered in search and recommendation tasks. We demonstrate the efficacy of applying convolutions by recasting the operation along the embedding dimension instead of the word dimension as applied in conventional CNNs for text classification. Towards scaling our model to datasets with millions of labels, we also propose InceptionXML+ framework which improves upon the shortcomings of the recently proposed dynamic hard-negative mining technique for label shortlisting by synchronizing the label-shortlister and extreme classifier. 
    
[^81]: CFMatch: 将自动答案等价评估与人工专家判断在开放域问答中对齐

    CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering. (arXiv:2401.13170v1 [cs.CL])

    [http://arxiv.org/abs/2401.13170](http://arxiv.org/abs/2401.13170)

    CFMatch提出了一个在开放域问答中将自动答案等价评估与人工专家判断对齐的方法，通过提供明确一致的评估指南并引入高效、稳健且轻量级的判别式AE分类器匹配方法来解决当前评估指标与人类判断不一致的问题。

    

    问答系统只有在我们知道答案是否正确的情况下才能取得进展，但对于许多最具挑战和有趣的问答示例，当前用于确定答案等价性的评估指标通常与人类判断不一致，尤其是来自大型语言模型（LLM）的更冗长、自由形式的答案。存在两个挑战：缺乏数据和模型过大：基于LLM的评分器可以更好地与人工评判员相关联，但这个任务只在有限的问答数据集上进行了测试，即使可用，对模型的更新也有限，因为LLM过大且往往昂贵。我们通过提供明确一致的指南来解决这两个问题，这些指南用于从专业人工问答比赛中采纳机器问答在答案等价性评估方面的标准。我们还引入了一种标准评估和一种更高效、稳健且轻量级的判别式AE分类器匹配方法（CFMatch，大小小于1MB），经过训练和验证以更准确地评估答案等价性。

    Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current evaluation metrics to determine answer equivalence (AE) often do not align with human judgments, particularly more verbose, free-form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big: LLM-based scorers can correlate better with human judges, but this task has only been tested on limited QA datasets, and even when available, update of the model is limited because LLMs are large and often expensive. We rectify both of these issues by providing clear and consistent guidelines for evaluating AE in machine QA adopted from professional human QA contests. We also introduce a combination of standard evaluation and a more efficient, robust, and lightweight discriminate AE classifier-based matching method (CFMatch, smaller than 1 MB), trained and validated to more accurately evalu
    
[^82]: GOAT-Bench: 通过基于迷因的社交虐待研究对大型多模态模型的安全洞察

    GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse. (arXiv:2401.01523v1 [cs.CL])

    [http://arxiv.org/abs/2401.01523](http://arxiv.org/abs/2401.01523)

    通过基于迷因的社交虐待研究对大型多模态模型的安全洞察，我们引入了综合的迷因基准测试集GOAT-Bench，评估各种LMMs在识别和回应迷因中体现的微妙社交虐待方面的能力。

    

    社交媒体的指数级增长深刻改变了信息的创造、传播和吸收方式，在数字时代产生了前所未有的影响。遗憾的是，这个爆炸也导致了网络迷因的滥用数量显著增加。评估迷因的负面影响是相当具有挑战性的，因为它们通常具有微妙和隐晦的含义，这些含义不能直接通过显性的文本和图像传达出来。鉴于此，大型多模态模型(LMMs)作为处理多样化多模态任务的卓越能力的焦点引起了人们的兴趣。针对这一发展，我们的论文旨在深入研究各种LMMs(如GPT-4V)识别和回应迷因中体现的微妙社交虐待方面的能力。我们引入了综合的迷因基准测试集GOAT-Bench，其中包含超过6K个多样的迷因，涵盖的主题包括隐性仇恨言论、性别歧视和网络欺凌等。利用GOAT-Be

    The exponential growth of social media has profoundly transformed how information is created, disseminated, and absorbed, exceeding any precedent in the digital age. Regrettably, this explosion has also spawned a significant increase in the online abuse of memes. Evaluating the negative impact of memes is notably challenging, owing to their often subtle and implicit meanings, which are not directly conveyed through the overt text and imagery. In light of this, large multimodal models (LMMs) have emerged as a focal point of interest due to their remarkable capabilities in handling diverse multimodal tasks. In response to this development, our paper aims to thoroughly examine the capacity of various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of social abuse manifested in memes. We introduce the comprehensive meme benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing GOAT-Be
    
[^83]: 简明有序的感知有助于大型语言模型进行演绎推理

    Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning. (arXiv:2310.03309v1 [cs.CL])

    [http://arxiv.org/abs/2310.03309](http://arxiv.org/abs/2310.03309)

    利用大型语言模型进行演绎推理是一个具有挑战性的问题。这篇论文提出了一个简明有序的方法，将任务分解为子任务并且人类化地组织思维，以提高演绎推理的效果。

    

    利用大型语言模型（LLMs）解决演绎推理问题已经引起了越来越多的关注。在复杂的演绎问题中仍然很难取得令人满意的结果，这类问题具有大量前提（即事实或规则），其中涉及实体之间错综复杂的关系，需要进行多跳推理。一种直观的解决方案是将原始任务分解为较小的子任务，然后以前向（例如选择-推理）或反向（例如LAMBADA）方式将多个因果推理步骤连接在一起。然而，这些技术不可避免地需要大量的总体阶段，导致计算开销大，并且有更高的可能性产生误导性的步骤。除了逐阶段分解之外，我们还从人类问题解决的另一个方面获得了启发。人类倾向于提炼出最相关的信息并有序地组织思维（例如创建思维导图），这有助于他们对问题进行有效的推理。

    Exploiting large language models (LLMs) to tackle deductive reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex deductive problems, characterized by plenty of premises (i.e., facts or rules) entailing intricate relationships among entities and requiring multi-hop reasoning. One intuitive solution is to decompose the original task into smaller sub-tasks, and then chain the multiple casual reasoning steps together in a forward (e.g., Selection-Inference) or backward (e.g., LAMBADA) direction. However, these techniques inevitably necessitate a large number of overall stages, leading to computationally expensive operations and a higher possibility of making misleading steps. In addition to stage-by-stage decomposition, we draw inspiration from another aspect of human problem-solving. Humans tend to distill the most relevant information and organize their thoughts systematically (e.g., creating mind maps), which assists th
    
[^84]: 自学优化器（STOP）：递归自我改进的代码生成

    Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation. (arXiv:2310.02304v1 [cs.CL])

    [http://arxiv.org/abs/2310.02304](http://arxiv.org/abs/2310.02304)

    本文提出了一种自学优化器（STOP），通过递归自我改进的代码生成，使用融合了语言模型的脚手架程序来改进自身，从而生成性能更好的程序。

    

    最近几年的人工智能系统（例如思维树和程序辅助语言模型）取得了一些重要进展，通过提供一个“脚手架”程序来解决问题，该程序构建了多次调用语言模型以生成更好的输出。脚手架程序通常使用Python等编程语言编写。在这项工作中，我们使用了一个融合了语言模型的脚手架程序来改进自身。我们从一个种子“改进器”开始，通过多次查询语言模型并返回最佳解决方案，根据给定的效用函数来改进输入程序。然后，我们运行这个种子改进器来改进自身。在一系列细分任务中，得到的改进改进器生成的程序在性能上明显优于种子改进器。随后，我们对语言模型提出的各种自我改进策略进行了分析，包括波束搜索、遗传算法和模拟退火。由于语言模型本身没有改变，这并不是一种增长领域。

    Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a "scaffolding" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed "improver" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not fu
    
[^85]: EVE: 使用遮蔽预测和模态感知的高效视觉-语言预训练

    EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE. (arXiv:2308.11971v1 [cs.CV])

    [http://arxiv.org/abs/2308.11971](http://arxiv.org/abs/2308.11971)

    本文引入了一种名为EVE的高效视觉-语言预训练模型，通过遮蔽信号建模和模态感知的方式，实现了统一的多模态Transformer网络，加速了训练进程，并取得了良好的效果。

    

    在本文中，我们介绍了一种名为EVE的高效视觉-语言基础模型，它是由一种统一的Transformer进行预训练的统一多模态模型。EVE通过在图像-文本对上进行遮蔽信号建模来统一视觉和语言的预训练任务，以重建可见信号，即图像像素和文本标记。通过集成模态感知的稀疏专家混合模块，EVE在一个共享的Transformer网络中编码了视觉和语言，并通过选择性地切换到不同的专家来捕捉模态特定信息。

    Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 3.5x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy t
    
[^86]: 大型语言模型在分布外检测方面有多好？

    How Good Are Large Language Models at Out-of-Distribution Detection?. (arXiv:2308.10261v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10261](http://arxiv.org/abs/2308.10261)

    本文通过对大型语言模型进行实证调查，探索了分布外检测的能力。作者发现了LLM在分布外检测方面的差异，并采用了新的生成式微调方法，提高了模型的性能。

    

    分布外（OOD）检测在提高机器学习（ML）模型的可靠性方面起着至关重要的作用。大型语言模型（LLM）的出现在ML社区引起了范式转变，展示了它们在各种自然语言处理任务中的出色能力。尽管现有的研究已经以BERT、RoBERTa和GPT-2等相对小规模的Transformer模型探索了OOD检测，但在规模、预训练目标和推理范式方面的明显差异引发了对这些发现在LLM中的适用性的质疑。本文在LLMs领域进行了开创性的实证调查，重点关注7B到65B大小的LLaMA系列。我们对常用的OOD检测器进行了全面评估，审查了它们在零梯度和微调场景下的性能。值得注意的是，我们将之前的判别式的内部分布微调改为生成式微调，使LLM的预训练目标与之一致。

    Out-of-distribution (OOD) detection plays a vital role in enhancing the reliability of machine learning (ML) models. The emergence of large language models (LLMs) has catalyzed a paradigm shift within the ML community, showcasing their exceptional capabilities across diverse natural language processing tasks. While existing research has probed OOD detection with relative small-scale Transformers like BERT, RoBERTa and GPT-2, the stark differences in scales, pre-training objectives, and inference paradigms call into question the applicability of these findings to LLMs. This paper embarks on a pioneering empirical investigation of OOD detection in the domain of LLMs, focusing on LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate commonly-used OOD detectors, scrutinizing their performance in both zero-grad and fine-tuning scenarios. Notably, we alter previous discriminative in-distribution fine-tuning into generative fine-tuning, aligning the pre-training objective of LLM
    
[^87]: 语言模型是否具有指称能力？

    Do Language Models Refer?. (arXiv:2308.05576v1 [cs.CL])

    [http://arxiv.org/abs/2308.05576](http://arxiv.org/abs/2308.05576)

    论文探讨了语言模型是否具有指称能力，并通过借鉴语言哲学外部主义传统的观点，提出了LMs可以指称的理由。

    

    语言模型（LMs）用语言做什么？大家都同意它们能够生成（大部分）连贯的句子。但是它们用这些字符串表达了什么，还是只是以一种令人信服的语言运用的模拟中胡言乱语？这是一个模糊的问题，有许多方法可以使其明确化。这里我们将解决该问题的一个方面，即，LMs的词语是否指称：即，LMs的输出是否实现了“词语-世界”之间的联系。有初步的理由认为它们不具备指称能力，因为LMs没有像普通语言用户那样与世界互动。借鉴语言哲学的外部主义传统的观点，我们认为表象是误导的，有充分的理由认为LMs可以指称。

    What do language models (LMs) do with language? Everyone agrees that they produce sequences of (mostly) coherent sentences. But are they saying anything with those strings or simply babbling in a convincing simulacrum of language use? This is a vague question, and there are many ways of making it precise. Here we will address one aspect of the question, namely, whether LMs' words refer: that is, whether the outputs of LMs achieve "word-to-world" connections. There is prima facie reason to think they do not since LMs do not interact with the world in the way that ordinary language users do. Drawing on insights from the externalist tradition in philosophy of language, we argue that appearances are misleading and that there is good reason to think that LMs can refer.
    
[^88]: 自然语言处理中社会人口统计偏见的调查

    Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])

    [http://arxiv.org/abs/2306.08158](http://arxiv.org/abs/2306.08158)

    本文调查了209篇关于NLP模型偏见的论文，其中大部分涉及社会人口统计偏见。研究者提出了社会人口统计偏见的定义，并确定了NLP偏见研究的三个主要类别。当前去偏见技术只是隐藏了偏见而不是真正去除它，需要进一步改进。

    

    深度神经网络在训练过程中往往会学习到非预期的偏见，这在实际应用中可能会产生有害的影响。本文对209篇关于NLP模型中偏见的论文进行了调查，其中大部分论文涉及社会人口统计偏见。为了更好地理解偏见与真实世界的危害之间的区别，我们借鉴心理学和行为经济学的思想，提出了社会人口统计偏见的定义。我们确定了NLP偏见研究的三个主要类别：偏见类型、量化偏见和去偏见。我们认为当前对于量化偏见的方法存在可靠性问题，许多偏见度量并不涉及真实世界中的偏见，当前的去偏见技术是表面的，只是隐藏了偏见，而不是真正去除它。最后，我们提供了未来工作的建议。

    Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
    
[^89]: Twitter图像的文本条件下的替代文本生成

    Text Conditional Alt-Text Generation for Twitter Images. (arXiv:2305.14779v1 [cs.CV])

    [http://arxiv.org/abs/2305.14779](http://arxiv.org/abs/2305.14779)

    本文针对Twitter上分享的图像提出了一种文本条件下的替代文本生成方法。通过CLIP前缀模型，该模型结合图像和推文中的文本信息，生成关于图像的上下文相关的替代文本。

    

    本文提出了一种针对社交媒体特别是Twitter上分享的图像生成替代文本（或alt-text）描述的方法。与图像的字幕不同，文本替换文本更加直白描述和上下文特定。此外，关键是，发布到Twitter上的图像通常是由用户编写的文本附加的，尽管这些文本不一定描述图像，但可能提供有用的上下文信息，如果正确利用可以提供信息，例如推文可能会命名图片中模型之前没有见过的不常见的对象。我们通过一个CLIP前缀模型来解决这个问题，该模型提取图像的嵌入并将其传递给映射网络，该网络输出单词嵌入空间中的短序列，或称为“前缀”，我们将推文本身的文本也连接到其中。这样，模型就可以在文章中条件化视觉和文本信息。然后将合并的多模式前缀作为预训练的语言模型的提示输入。

    In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter. This task is more than just a special case of image captioning, as alt-text is both more literally descriptive and context-specific. Also critically, images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly leveraged can be informative -- e.g. the tweet may name an uncommon object in the image that the model has not previously seen. We address this with a CLIP prefix model that extracts an embedding of the image and passes it to a mapping network that outputs a short sequence in word embedding space, or a ``prefix'', to which we also concatenate the text from the tweet itself. This lets the model condition on both visual and textual information from the post. The combined multimodal prefix is then fed as a prompt to a pretrained lang
    
[^90]: GPT-4技术报告

    GPT-4 Technical Report. (arXiv:2303.08774v1 [cs.CL])

    [http://arxiv.org/abs/2303.08774](http://arxiv.org/abs/2303.08774)

    GPT-4是一个大规模多模态模型，可以接收图像和文本输入并产生文本输出，能够在各种专业和学术基准测试中表现出人类水平的表现，包括通过模拟的律师考试。该项目的核心组件是开发基础设施和优化方法，可在广泛的规模范围内表现预测性。

    

    我们报告了GPT-4的开发，它是一个可以接受图像和文本输入并产生文本输出的大规模多模态模型。虽然在许多现实场景中不如人类，但GPT-4在各种专业和学术基准测试中表现出人类水平的表现，包括通过模拟的律师考试，成绩排名在前10％左右。GPT-4是一个基于Transformer的模型，预训练用于预测文档中的下一个标记。后训练对齐过程提高了事实性和符合期望行为的性能指标。项目的核心组件是开发基础设施和优化方法，可在广泛的规模范围内表现预测性。这使我们能够准确预测GPT-4的某些性能方面，而这些性能是基于使用不超过GPT-4计算能力的1/1,000的模型训练的。

    We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.
    
[^91]: 基于Prompt和语义知识增强的零样本关系抽取

    Prompt-based Zero-shot Relation Extraction with Semantic Knowledge Augmentation. (arXiv:2112.04539v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2112.04539](http://arxiv.org/abs/2112.04539)

    本文提出了一种基于Prompt和语义知识增强的模型，用于在零样本情况下识别未见关系。采用了一个新的单词级别的类比推理方法，生成了具有未见关系的增强实例。设计了基于外部知识图谱的提示，增加了已见关系的语义知识信息。

    

    在关系三元组抽取(RTE)任务中，识别没有训练实例的未见关系是一个具有挑战性的任务。已经尝试使用问答模型或关系描述来识别未见关系，但这些方法缺乏有关已见和未见关系之间联系的语义信息。本文提出了一种基于Prompt和语义知识增强的模型（ZS-SKA），用于在零样本情况下识别未见关系。我们提出了一种新的基于类比推理的单词级句子翻译规则，并使用该规则从具有已见关系的实例中生成具有未见关系的增强实例。我们设计了基于外部知识图谱的加权虚拟标签构建的提示信息，以整合从已见关系中学到的语义知识信息。我们构造了加权虚拟标签词，而不是在提示模板中使用实际的标签集。我们学习了RTE任务的表示形式，

    In relation triplet extraction (RTE), recognizing unseen (new) relations for which there are no training instances is a challenging task. Efforts have been made to recognize unseen relations based on question-answering models or relation descriptions. However, these approaches miss the semantic information about connections between seen and unseen relations. In this paper, We propose a prompt-based model with semantic knowledge augmentation (ZS-SKA) to recognize unseen relations under the zero-shot setting. We present a new word-level analogy-based sentence translation rule and generate augmented instances with unseen relations from instances with seen relations using that new rule. We design prompts with weighted virtual label construction based on an external knowledge graph to integrate semantic knowledge information learned from seen relations. Instead of using the actual label sets in the prompt template, we construct weighted virtual label words. We learn the representations of b
    

