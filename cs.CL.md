# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning.](http://arxiv.org/abs/2306.16413) | MultiZoo和MultiBench是用于多模态深度学习的标准化工具包，提供了多模态算法的实现和大规模基准测试，以促进对多模态模型能力和限制的理解，并确保易用性和可重复性。 |
| [^2] | [Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language.](http://arxiv.org/abs/2306.16410) | LENS是一种利用大型语言模型的模块化方法，通过推理图像的视觉模块输出进行计算机视觉问题的处理。它不需要多模态训练即可与更大更复杂的系统具有竞争力。 |
| [^3] | [Towards Measuring the Representation of Subjective Global Opinions in Language Models.](http://arxiv.org/abs/2306.16388) | 本文提出了一个方法来评估大型语言模型对全球观点的代表性。通过构建一个包含跨国调查问题和答案的数据集，并定义一个相似度度量标准，研究发现默认情况下语言模型的回应更倾向于某些人群的观点，但当模型考虑特定国家的观点时，回应会更加贴近该国家的观点。 |
| [^4] | [Multi-Site Clinical Federated Learning using Recursive and Attentive Models and NVFlare.](http://arxiv.org/abs/2306.16367) | 本论文提出了一种多站点临床联邦学习的方法，使用递归和注意力模型以及NVFlare框架。研究引入了两种示例性的自然语言处理模型，LSTM模型和BERT模型，这些模型在理解上下文和语义方面表现出了优异的性能。 |
| [^5] | [Representation Learning via Variational Bayesian Networks.](http://arxiv.org/abs/2306.16326) | VBN是一种利用层次和关系信息的新颖的贝叶斯实体表示学习模型，特别适用于数据稀缺的“长尾”实体建模。通过使用层次先验和明确关系约束，VBN提供了更好的建模效果，并通过密度表示实体，对数据稀缺情况下的不确定性进行建模。我们还提出了一种可扩展的变分贝叶斯优化算法来实现快速的近似贝叶斯推断。 |
| [^6] | [Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models.](http://arxiv.org/abs/2306.16322) | 该论文评估了GPT-3.5和GPT-4模型在七个阿拉伯语NLP任务上的性能，并发现GPT-4在五个任务上的表现优于GPT-3.5。研究还对挑战性的方言数据集上的情感分析任务进行了深入分析，提供了LLMs在该任务上取得卓越结果的见解。同时介绍了一个方便评估这些任务的新的Python接口。 |
| [^7] | [An Adversarial Multi-Task Learning Method for Chinese Text Correction with Semantic Detection.](http://arxiv.org/abs/2306.16313) | 本论文提出了一种用于中文文本纠错的对抗多任务学习方法和语义检测，在中文句子上下文中增强了字符多义的建模和检测能力，并通过实验证明了方法的有效性。 |
| [^8] | [ChatGPT may excel in States Medical Licensing Examination but falters in basic Linear Algebra.](http://arxiv.org/abs/2306.16282) | ChatGPT在国家医疗执照考试中表现出色，但在基础线性代数教学上存在重大数学错误和逻辑推断失败的问题，不适合作为学生的教师。 |
| [^9] | [Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting.](http://arxiv.org/abs/2306.16275) | 本研究提出了一种通过迭代提示与ChatGPT或GPT-4进行多轮交互的方法来改进食物影响摘要的准确性。这种方法在产品特定指导(PSG)开发中具有潜力应用的价值。 |
| [^10] | [Emotion Analysis of Tweets Banning Education in Afghanistan.](http://arxiv.org/abs/2306.16268) | 这项研究介绍了第一个用于阿富汗普什图语变体的情绪标注数据集，该数据集包含7600条推文，以研究塔利班禁止妇女接受教育的情绪反应，并通过多种神经架构对达里语情感分类进行基准测试。 |
| [^11] | [CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models.](http://arxiv.org/abs/2306.16244) | 通过人工智能与人类协同合作构建的CBBQ中文偏差基准数据集，全面衡量了与中国文化和价值观相关的14个社会维度中的刻板印象和社会偏见，对于检测模型偏见具有广泛的覆盖范围和高度的多样性。 |
| [^12] | [Inferring the Goals of Communicating Agents from Actions and Instructions.](http://arxiv.org/abs/2306.16207) | 本文介绍了一个模型，可以通过观察行动和指令推断合作团队的目标，并评估了其与人类判断的相关性。 |
| [^13] | [Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation.](http://arxiv.org/abs/2306.16195) | 本研究提出了一个通过动态图知识聚合来提升对话生成的新框架，它能够更好地利用来自帖子和外部图知识的异质特征，从而解决图知识和文本之间的语义差异问题。 |
| [^14] | [SkillNet-X: A Multilingual Multitask Model with Sparsely Activated Skills.](http://arxiv.org/abs/2306.16176) | 学术论文提出了一种名为SkillNet-X的多语言多任务模型，通过稀疏激活相关的技能模块，能够在不同语言的多个任务上表现出更好的性能。 |
| [^15] | [Generative User-Experience Research for Developing Domain-specific Natural Language Processing Applications.](http://arxiv.org/abs/2306.16143) | 本论文提出了一种在开发领域特定自然语言处理应用中整合生成式用户体验研究的方法。该方法将领域用户纳入原型开发的不同阶段，以更好地了解用户需求和评估用户价值的变化。 |
| [^16] | [A Framework for Identifying Depression on Social Media: MentalRiskES@IberLEF 2023.](http://arxiv.org/abs/2306.16125) | 该论文介绍了在社交媒体上识别抑郁症的框架，使用机器学习和深度学习技术来解决四个预测子任务，并发现使用句子嵌入作为线性回归器的输入产生了更好的结果。 |
| [^17] | [Is ChatGPT a Biomedical Expert? -- Exploring the Zero-Shot Performance of Current GPT Models in Biomedical Tasks.](http://arxiv.org/abs/2306.16108) | 通过对 GPT-3.5-Turbo 和 GPT-4 在生物医学任务中的表现评估，发现它们能够通过零样本学习和相关片段的支撑与领先系统相竞争，但在检索任务中表现不如其他系统。 |
| [^18] | [ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases.](http://arxiv.org/abs/2306.16092) | 本论文介绍了一个基于开源的法律大型语言模型ChatLaw，它通过综合外部知识库为中国法律领域的数字化转型提供支持。该模型采用了精心设计的法律领域微调数据集，并通过将向量数据库检索与关键词检索相结合的方法解决了法律数据筛选中的模型幻觉问题。同时，引入了一种自注意力方法以增强对文本中关键信息的注意力。 |
| [^19] | [Long-term Conversation Analysis: Exploring Utility and Privacy.](http://arxiv.org/abs/2306.16071) | 本文研究了长期对话分析中的隐私保护问题。通过使用McAdams系数和频谱平滑的组合方法，我们提出了一种隐私保护特征提取方法，该方法在提高隐私保护的同时保持了实用性。 |
| [^20] | [What Sentiment and Fun Facts We Learnt Before FIFA World Cup Qatar 2022 Using Twitter and AI.](http://arxiv.org/abs/2306.16049) | 本文使用机器学习和收集的Twitter推文，对2022年卡塔尔世界杯之前的情感和有趣事实进行了分析。结果表明人们对世界杯的开幕持积极态度。 |
| [^21] | [Exploring Spatial-Temporal Variations of Public Discourse on Social Media: A Case Study on the First Wave of the Coronavirus Pandemic in Italy.](http://arxiv.org/abs/2306.16031) | 本论文提出了一种方法，通过社交媒体上的语言行为研究了意大利新冠疫情第一波流行期间公众对事件的时空变化反应。研究使用时间序列分析和聚类技术探索了推文使用趋势，通过定性比较分析确定了对应的术语类别。研究结果证实了已有的心理观察结果，即事件的震中和外围区域的公众言论呈现出明显的时空聚类特征。 |
| [^22] | [Accelerating Transducers through Adjacent Token Merging.](http://arxiv.org/abs/2306.16009) | 提出了一种相邻令牌合并（A-ToMe）的新方法，通过逐渐合并具有相似关键值的相邻令牌，减少了总的时间步长，并加快了语音识别系统的推理速度。实验证明，该方法可以显著减少令牌数量并提高推理速度，同时准确性损失微乎其微。 |
| [^23] | [Prompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition.](http://arxiv.org/abs/2306.16007) | 这篇论文提出了两种新方法在语音识别中进行零样本领域适应，通过使用大型语言模型进行二次修正和深度融合，只使用一个领域提示即可有效降低词错误率，并且有更好的实体和超出词汇的召回效果。 |
| [^24] | [Streamlining Social Media Information Retrieval for Public Health Research with Deep Learning.](http://arxiv.org/abs/2306.16001) | 本研究介绍了一个使用深度学习简化社交媒体信息检索的框架，通过识别医学实体、标准化实体和分配UMLS概念，构建了一个用于COVID-19相关推文的症状词典。 |
| [^25] | [Sentence-to-Label Generation Framework for Multi-task Learning of Japanese Sentence Classification and Named Entity Recognition.](http://arxiv.org/abs/2306.15978) | 本研究提出了一种句子到标签生成框架，用于多任务学习日语句子分类和命名实体识别。通过整合句子分类和命名实体识别任务，并提出约束机制来提高格式准确性，结果显示整合能够增强两个任务的性能。 |
| [^26] | [You Can Generate It Again: Data-to-text Generation with Verification and Correction Prompting.](http://arxiv.org/abs/2306.15933) | 本文提出了一种多步骤生成、验证和纠正的数据生成文本方法，通过专门的错误指示提示来改善输出质量。 |
| [^27] | [Most Language Models can be Poets too: An AI Writing Assistant and Constrained Text Generation Studio.](http://arxiv.org/abs/2306.15926) | 这项研究展示了如何通过在语言模型中应用过滤函数来生成有限约束文本，并提出了一个AI写作助手工具，可以根据用户的需求生成带有各种约束条件的文本。 |
| [^28] | [Confidence-Calibrated Ensemble Dense Phrase Retrieval.](http://arxiv.org/abs/2306.15917) | 本文研究了如何优化基于Transformer的密集语段检索（DPR）算法，使用置信度校准的集合预测方法取得了最先进的结果，并发现不同领域的最优粒度也有所差异。 |
| [^29] | [Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias.](http://arxiv.org/abs/2306.15895) | 本论文研究了大型语言模型作为属性化训练数据生成器的应用。通过使用具有多样性属性的提示，我们能够生成多样化且归因的数据。研究表明，在高基数和多样领域的数据集中，使用属性化提示对生成模型性能有积极影响。此外，论文还展示了关于偏差、多样性和效率的全面实证研究结果，并得出了三个关键观察：系统性偏差存在于生成数据中，多样性和效率之间存在权衡，属性化训练数据生成可以改善模型性能。 |
| [^30] | [Symbol emergence as interpersonal cross-situational learning: the emergence of lexical knowledge with combinatoriality.](http://arxiv.org/abs/2306.15837) | 本研究提出了一个计算模型，通过交互学习和命名游戏，实现了代理之间通过词序列交流的方式，从而促进了具有组合性的语义知识的出现。 |
| [^31] | [MAT: Mixed-Strategy Game of Adversarial Training in Fine-tuning.](http://arxiv.org/abs/2306.15826) | 本论文提出了一种新颖的混合策略对抗训练算法（MAT），通过在细调阶段加入对抗训练，显著提高了模型的泛化能力和鲁棒性，通过采样方法建立了MAT。实验证明，MAT在大规模预训练模型上的性能明显优于其他方法。 |
| [^32] | [Confidence-based Ensembles of End-to-End Speech Recognition Models.](http://arxiv.org/abs/2306.15824) | 本文提出了一种基于置信度的端到端语音识别模型的集成方法，通过只使用最有信心的模型的输出来提高性能，并通过两个应用程序的实验证明了方法的有效性。 |
| [^33] | [FLuRKA: Fast fused Low-Rank & Kernel Attention.](http://arxiv.org/abs/2306.15799) | FLuRKA是一种融合低秩和核注意力的新型transformer类别，相较于传统的近似技术，在运行时间性能和质量方面都表现出显著的提升。 |
| [^34] | [Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese.](http://arxiv.org/abs/2306.15788) | 该研究评估了GPT-3.5和GPT-4在巴西葡萄牙语语法错误修正方面的有效性，结果显示虽然GPT-4的召回率较高，但语言模型倾向于过度修正。 |
| [^35] | [Next Steps for Human-Centered Generative AI: A Technical Perspective.](http://arxiv.org/abs/2306.15774) | 这项研究从技术角度定义和提出了人类中心生成式人工智能(HGAI)的下一步工作，包括与人类价值观对齐、适应人类的意图表达和增强人类在协作工作流中的能力。这个工作的目标是吸引跨学科研究团队对HGAI的新兴想法进行讨论，并保持未来工作景观的整体连贯性。 |
| [^36] | [Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost.](http://arxiv.org/abs/2306.15766) | 本研究提出了一种利用大型语言模型进行标注的方法，以最小成本提升NLP模型的泛化能力。通过差异性采样策略，我们证明了这种方法在分类和排序任务上能够显著改善模型效果。 |
| [^37] | [Identity Construction in a Misogynist Incels Forum.](http://arxiv.org/abs/2306.15745) | 本研究使用定量文本和网络分析方法，研究了最大的黑洞incels论坛如何讨论身份群体。研究发现该社区产生了许多新的身份术语，存在物质主义的意识形态。对此我们讨论了对自动化 misogynist hate speech 检测研究的影响。 |
| [^38] | [Biomedical Entity Recognition by Detection and Matching.](http://arxiv.org/abs/2306.15736) | 本研究提出了一种名为DMNER的新型生物医学实体识别框架，通过检测实体边界和匹配生物医学实体来提高NER的性能。在有监督NER、远程监督NER和多数据集合并训练NER等场景中，DMNER都展示了良好的适用性。 |
| [^39] | [A Weakly Supervised Classifier and Dataset of White Supremacist Language.](http://arxiv.org/abs/2306.15732) | 这篇论文提出了一个用于检测白人至上主义极端主义语言的数据集和分类器，通过使用大量文本数据集以及包括中性和反种族主义数据，在不同领域中进行弱监督训练，提高了对新领域的泛化性能，并通过使用反种族主义文本作为反例来减轻偏见。 |
| [^40] | [On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection.](http://arxiv.org/abs/2306.15705) | 本论文提出了一个用于高效的无数据对抗检测的通用对抗扰动方法，通过发现对抗样本与高维输入中的特定向量的关系，计算出通用对抗扰动（UAPs）。基于此，提出了一个无数据对抗检测框架，通过对UAPs对正常样本和对抗样本的反应产生不同的结果，从而实现了在各种文本分类任务上具有竞争力的检测性能。具体实验结果显示，该方法维持了与正常推断相等的时间消耗。 |
| [^41] | [Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale.](http://arxiv.org/abs/2306.15687) | Voicebox是一种大规模的多语言通用语音生成模型，通过使用非自回归的流匹配模型，在文本和音频上下文条件下进行训练，可以实现零样本跨语言文本到语音合成，噪声去除，内容编辑，风格转换和多样化的样本生成等多种任务。 |
| [^42] | [Master-ASR: Achieving Multilingual Scalability and Low-Resource Adaptation in ASR with Modular Learning.](http://arxiv.org/abs/2306.15686) | Master-ASR是一个基于模块化学习的ASR框架，通过共享模块来实现多语言可扩展性和低资源适应能力，解决了ASR面临的可扩展性和低资源适应的挑战。 |
| [^43] | [Implementing contextual biasing in GPU decoder for online ASR.](http://arxiv.org/abs/2306.15685) | 本论文提出了一种在实时GPU解码中集成上下文偏置的方法，以提高ASR预测的准确性，并允许动态上下文切换。 |
| [^44] | [Automatic Annotation of Direct Speech in Written French Narratives.](http://arxiv.org/abs/2306.15634) | 本论文旨在为法语中的自动注释直接言语（AADS）创建一个统一的框架。研究采用最大的法语叙述数据集进行了广泛评估，结果表明该任务仍需大量努力，并强调了不同基线模型的特点。 |
| [^45] | [Extending Context Window of Large Language Models via Positional Interpolation.](http://arxiv.org/abs/2306.15595) | 通过位置插值方法，我们可以在最小微调的情况下将RoPE-based预训练语言模型的上下文窗口扩展到最多32768，并在多个任务上获得强有力的实证结果。通过线性降低输入位置索引的大小，我们保持了扩展模型在原始上下文窗口内任务的质量。 |
| [^46] | [3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement.](http://arxiv.org/abs/2306.15354) | 3D-Speaker是一个大规模的多设备、多距离和多方言语音语料库，用于研究语音表示解缠。它包含了10,000多个说话人的数据，可以用来评估大型通用语音模型和探索域外学习和自监督学习方法。 |
| [^47] | [MindDial: Belief Dynamics Tracking with Theory-of-Mind Modeling for Situated Neural Dialogue Generation.](http://arxiv.org/abs/2306.15253) | MindDial是一个使用心智模拟进行信念动态跟踪的对话生成框架，可以在场景化环境中生成自由对话来协商共识。 |
| [^48] | [Investigating Cross-Domain Behaviors of BERT in Review Understanding.](http://arxiv.org/abs/2306.15123) | 该研究调查了在产品评论理解的各种任务中，BERT模型在不同域上的跨域行为。尽管单域模型在对应域上略有提高，多域模型在评估多域数据时表现更好，并且在平均测试中也更优。尽管单域模型微调可以提高准确性，但会增加计算资源消耗。 |
| [^49] | [Uncovering Political Hate Speech During Indian Election Campaign: A New Low-Resource Dataset and Baselines.](http://arxiv.org/abs/2306.14764) | 本论文介绍了一个新的低资源数据集IEHate，其中包含11,457条与印度州立选举活动相关的印地语推文。通过对数据集的分析，研究者们揭示了政治交流中仇恨言论的普遍性和不同形式的憎恨语言，并使用多种算法对数据集进行了基准测试。实验结果表明，需要更高级的技术来提高低资源语言中仇恨言论的检测性能。 |
| [^50] | [Stance Prediction and Analysis of Twitter data : A case study of Ghana 2020 Presidential Elections.](http://arxiv.org/abs/2306.14203) | 本研究通过对2020年加纳总统选举期间的推特数据进行立场分析，揭示了推特用户对两位主要候选人的观点，并通过词典和机器学习方法对数据集进行评估，得到了最佳性能。 |
| [^51] | [DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer.](http://arxiv.org/abs/2305.19567) | 本文提出了一种基于离散码和混合器相协作的端到端表现力TTS，它采用新的输入表示和简单的架构来实现改进的韵律建模，证明了其有效性。 |
| [^52] | [Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective.](http://arxiv.org/abs/2305.15408) | 本文从理论层面探究了带有“思维链”提示的大型语言模型在解决基本数学和决策问题中的能力，发现自回归Transformer大小恒定即可解决任务，揭示了“思维链”提示的背后机制。 |
| [^53] | [Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models.](http://arxiv.org/abs/2304.01046) | 本文研究了一种训练阅读理解和逻辑推理模型的反向方法，利用相对准确性的策略来训练模型，通过Polytuplet Loss函数来确保优先学习答案选择的相对正确性，获得了不错的成果，提出了具有一般性的训练方法和模型架构。 |
| [^54] | [EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records.](http://arxiv.org/abs/2301.07695) | 该论文提出了一个面向电子病历数据的文本转SQL数据集，该数据集具有一系列独特挑战，包括生成SQL查询、理解时间表达式以及区分有无答案的问题。 |
| [^55] | [Localising In-Domain Adaptation of Transformer-Based Biomedical Language Models.](http://arxiv.org/abs/2212.10422) | 本研究针对生物医学领域内自适应问题，探讨了两种途径来在非英语语言中产生生物医学语言模型。一种是通过神经机器翻译将英文资源翻译为目标语言，注重数量；另一种是直接基于高质量、狭谱的语料库进行本地化。这些方法有助于解决资源较少语言如意大利语的领域内适应问题。 |
| [^56] | [NarraSum: A Large-Scale Dataset for Abstractive Narrative Summarization.](http://arxiv.org/abs/2212.01476) | NarraSum是一个大规模的故事概括数据集，旨在鼓励故事概括研究。实验结果显示，目前的概括模型在该数据集上与人类表现存在明显差距。 |
| [^57] | [QueryForm: A Simple Zero-shot Form Entity Query Framework.](http://arxiv.org/abs/2211.07730) | QueryForm是一种简单的零样本表单实体查询框架，通过使用双重提示机制和利用大规模查询-实体对进行预训练，能够从结构化文档中提取实体值，无需目标特定的训练数据，达到了新的最先进技术水平。 |
| [^58] | [Hierarchical MixUp Multi-label Classification with Imbalanced Interdisciplinary Research Proposals.](http://arxiv.org/abs/2209.13912) | 层次混合多标签分类方法用于解决不平衡的跨学科研究提案中的独特问题，包括层次结构的标签、异构的语义和不平衡的数量。 |
| [^59] | [Local Byte Fusion for Neural Machine Translation.](http://arxiv.org/abs/2205.11490) | 本文提出了一种基于字节的本地字节融合方法，用于神经机器翻译。该方法可以解决当前NLP模型中子词标记化方案的刚性和对其他语料库适应性差的问题，同时避免了在多语种语料库中过度切分低资源语言的影响。 |
| [^60] | [EHRKit: A Python Natural Language Processing Toolkit for Electronic Health Record Texts.](http://arxiv.org/abs/2204.06604) | EHRKit是一个用于处理电子健康记录文本的Python工具包，它集成了多个NLP任务和基于MIMIC-III数据的接口，可以进行命名实体识别、摘要生成、机器翻译等任务。 |
| [^61] | [High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning.](http://arxiv.org/abs/2203.01311) | 本文研究了高模态场景下的高效表示学习，提出了两种新的信息论度量方法来量化模态和交互的异质性，以加速对多样化和少被研究的模态的推广。 (arXiv:2203.01311v4 [cs.LG] UPDATED) |
| [^62] | [A Longitudinal Multi-modal Dataset for Dementia Monitoring and Diagnosis.](http://arxiv.org/abs/2109.01537) | 该论文提出了一个纵向多模态数据集，用于痴呆监测和诊断。通过分析语言、言语和语用指标，可以区分神经退行性疾病患者和对照组，从而为痴呆研究提供了宝贵的资源。 |

# 详细

[^1]: MultiZoo & MultiBench: 用于多模态深度学习的标准化工具包

    MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning. (arXiv:2306.16413v1 [cs.LG])

    [http://arxiv.org/abs/2306.16413](http://arxiv.org/abs/2306.16413)

    MultiZoo和MultiBench是用于多模态深度学习的标准化工具包，提供了多模态算法的实现和大规模基准测试，以促进对多模态模型能力和限制的理解，并确保易用性和可重复性。

    

    学习多模态表示涉及整合来自多种异构数据源的信息。为了加快对少研究的模态和任务的进展，同时确保现实世界的稳健性，我们发布了MultiZoo，一个公共工具包，其中包含> 20个核心多模态算法的标准化实现，以及MultiBench，一个涵盖15个数据集，10个模态，20个预测任务和6个研究领域的大规模基准测试。这些提供了一个自动化的端到端机器学习流水线，简化和标准化数据加载、实验设置和模型评估。为了实现全面评估，我们提供了一套全面的方法来评估（1）泛化能力，（2）时间和空间复杂度，和（3）模态鲁棒性。MultiBench为更好地了解多模态模型的功能和限制铺平了道路，同时确保易于使用、可访问性和可重复性。我们的工具包是公开可用的。

    Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of > 20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, wi
    
[^2]: 以自然语言为镜，实现能看见的语言模型：计算机视觉的LENS方法

    Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language. (arXiv:2306.16410v1 [cs.CL])

    [http://arxiv.org/abs/2306.16410](http://arxiv.org/abs/2306.16410)

    LENS是一种利用大型语言模型的模块化方法，通过推理图像的视觉模块输出进行计算机视觉问题的处理。它不需要多模态训练即可与更大更复杂的系统具有竞争力。

    

    我们提出了LENS，一种利用大型语言模型（LLMs）解决计算机视觉问题的模块化方法。我们的系统使用语言模型推理图像的一组独立且高度描述性的视觉模块的输出，提供关于图像的详尽信息。我们在纯计算机视觉环境中评估了该方法，如零和少样本目标识别，同时也在视觉和语言问题上进行了测试。LENS可以应用于任何现成的LLM，并且我们发现有LENS的LLMs的性能高度竞争，而无需进行任何多模态训练。我们在https://github.com/ContextualAI/lens开源我们的代码，并提供一个交互式演示。

    We propose LENS, a modular approach for tackling computer vision problems by leveraging the power of large language models (LLMs). Our system uses a language model to reason over outputs from a set of independent and highly descriptive vision modules that provide exhaustive information about an image. We evaluate the approach on pure computer vision settings such as zero- and few-shot object recognition, as well as on vision and language problems. LENS can be applied to any off-the-shelf LLM and we find that the LLMs with LENS perform highly competitively with much bigger and much more sophisticated systems, without any multimodal training whatsoever. We open-source our code at https://github.com/ContextualAI/lens and provide an interactive demo.
    
[^3]: 测量语言模型中主观全球观点的方法研究

    Towards Measuring the Representation of Subjective Global Opinions in Language Models. (arXiv:2306.16388v1 [cs.CL])

    [http://arxiv.org/abs/2306.16388](http://arxiv.org/abs/2306.16388)

    本文提出了一个方法来评估大型语言模型对全球观点的代表性。通过构建一个包含跨国调查问题和答案的数据集，并定义一个相似度度量标准，研究发现默认情况下语言模型的回应更倾向于某些人群的观点，但当模型考虑特定国家的观点时，回应会更加贴近该国家的观点。

    

    大型语言模型（LLMs）可能无法公平地代表社会问题中多样化的全球观点。本文开发了一个定量框架，用于评估模型生成的回答与哪些人的观点更为相似。我们首先构建了一个数据集GlobalOpinionQA，包含了来自跨国调查的问题和答案，旨在捕捉不同国家关于全球问题的多样观点。然后，我们定义了一个度量标准，以国家为条件，量化了LLM生成的调查回答与人类回答之间的相似性。在我们的框架下，我们对一个经过宪法AI培训的LLM进行了三个实验，分别考虑其帮助性、诚实性和无害性。默认情况下，LLM的回应更倾向于与某些人群的观点更类似，例如来自美国、欧洲和南美洲的人群，凸显了潜在的偏见。当我们提示模型考虑某个特定国家的观点时，回应会更加类似于该国家的观点。

    Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. We first build a dataset, GlobalOpinionQA, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an LLM trained to be helpful, honest, and harmless with Constitutional AI. By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country's perspective, responses shift to be more similar to the
    
[^4]: 多站点临床联邦学习使用递归和注意力模型以及NVFlare

    Multi-Site Clinical Federated Learning using Recursive and Attentive Models and NVFlare. (arXiv:2306.16367v1 [cs.LG])

    [http://arxiv.org/abs/2306.16367](http://arxiv.org/abs/2306.16367)

    本论文提出了一种多站点临床联邦学习的方法，使用递归和注意力模型以及NVFlare框架。研究引入了两种示例性的自然语言处理模型，LSTM模型和BERT模型，这些模型在理解上下文和语义方面表现出了优异的性能。

    

    数字健康数据的快速增长引发了对利用机器学习方法（如自然语言处理）来审查医疗记录、临床笔记和其他基于文本的健康信息的兴趣。虽然自然语言处理技术在增强患者护理和决策制定方面展示出了巨大的潜力，但数据隐私和遵守法规仍然是关键问题。联邦学习成为一种可行的解决方案，使多个组织能够在不传播原始数据的情况下共同训练机器学习模型。本文通过将联邦学习、自然语言处理模型和由NVIDIA开发的NVFlare框架相结合，提出了一种实用的医疗自然语言处理方法。我们介绍了两个示例性的自然语言处理模型：基于长短期记忆（LSTM）的模型和双向编码器表示转换（BERT），它们在理解上下文和语义方面表现出 exceptional 性能。

    The prodigious growth of digital health data has precipitated a mounting interest in harnessing machine learning methodologies, such as natural language processing (NLP), to scrutinize medical records, clinical notes, and other text-based health information. Although NLP techniques have exhibited substantial potential in augmenting patient care and informing clinical decision-making, data privacy and adherence to regulations persist as critical concerns. Federated learning (FL) emerges as a viable solution, empowering multiple organizations to train machine learning models collaboratively without disseminating raw data. This paper proffers a pragmatic approach to medical NLP by amalgamating FL, NLP models, and the NVFlare framework, developed by NVIDIA. We introduce two exemplary NLP models, the Long-Short Term Memory (LSTM)-based model and Bidirectional Encoder Representations from Transformers (BERT), which have demonstrated exceptional performance in comprehending context and semant
    
[^5]: 通过变分贝叶斯网络实现表示学习

    Representation Learning via Variational Bayesian Networks. (arXiv:2306.16326v1 [cs.LG])

    [http://arxiv.org/abs/2306.16326](http://arxiv.org/abs/2306.16326)

    VBN是一种利用层次和关系信息的新颖的贝叶斯实体表示学习模型，特别适用于数据稀缺的“长尾”实体建模。通过使用层次先验和明确关系约束，VBN提供了更好的建模效果，并通过密度表示实体，对数据稀缺情况下的不确定性进行建模。我们还提出了一种可扩展的变分贝叶斯优化算法来实现快速的近似贝叶斯推断。

    

    我们提出了一种新颖的贝叶斯实体表示学习模型-变分贝叶斯网络 (VBN)，该模型利用层次和关系信息，并对“长尾”中的实体建模特别有用，因为这种情况下数据稀缺。VBN通过两种互补机制提供了更好的长尾实体建模：首先，VBN采用了信息丰富的层次先验，使共享共同祖先的实体之间的信息传播成为可能。此外，VBN建模了实体之间的明确关系，强制实体之间的互补结构和一致性，引导学习的表示向更有意义的空间布局。其次，VBN通过密度表示实体（而不是向量），从而对数据稀缺情况下起到补充作用的不确定性进行建模。最后，我们提出了一种可扩展的变分贝叶斯优化算法，实现了快速的近似贝叶斯推断。我们评估了VBN在语言学任务上的有效性。

    We present Variational Bayesian Network (VBN) - a novel Bayesian entity representation learning model that utilizes hierarchical and relational side information and is particularly useful for modeling entities in the ``long-tail'', where the data is scarce. VBN provides better modeling for long-tail entities via two complementary mechanisms: First, VBN employs informative hierarchical priors that enable information propagation between entities sharing common ancestors. Additionally, VBN models explicit relations between entities that enforce complementary structure and consistency, guiding the learned representations towards a more meaningful arrangement in space. Second, VBN represents entities by densities (rather than vectors), hence modeling uncertainty that plays a complementary role in coping with data scarcity. Finally, we propose a scalable Variational Bayes optimization algorithm that enables fast approximate Bayesian inference. We evaluate the effectiveness of VBN on linguist
    
[^6]: Taqyim: 使用ChatGPT模型评估阿拉伯语NLP任务

    Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models. (arXiv:2306.16322v1 [cs.CL])

    [http://arxiv.org/abs/2306.16322](http://arxiv.org/abs/2306.16322)

    该论文评估了GPT-3.5和GPT-4模型在七个阿拉伯语NLP任务上的性能，并发现GPT-4在五个任务上的表现优于GPT-3.5。研究还对挑战性的方言数据集上的情感分析任务进行了深入分析，提供了LLMs在该任务上取得卓越结果的见解。同时介绍了一个方便评估这些任务的新的Python接口。

    

    大型语言模型（LLM）在各种下游任务上表现出令人印象深刻的性能，无需细调，并且包括ChatGPT，一种构建在诸如GPT-3.5和GPT-4等LLMs之上的聊天型模型。尽管与英语相比，这些模型的训练比例较低，但它们在其他语言中也展现出了出色的能力。在本研究中，我们评估了GPT-3.5和GPT-4模型在七个不同的阿拉伯语NLP任务上的性能：情感分析、翻译、音译、改写、词性标注、摘要提取和音素标注。我们的研究结果显示，GPT-4在这七个任务中有五个的表现优于GPT-3.5。此外，我们对情感分析任务进行了广泛分析，揭示了LLMs在具有挑战性的方言数据集上取得优异结果的原因。此外，我们还引入了一个新的Python接口https://github.com/ARBML/Taqyim，以便轻松评估这些任务。

    Large language models (LLMs) have demonstrated impressive performance on various downstream tasks without requiring fine-tuning, including ChatGPT, a chat-based model built on top of LLMs such as GPT-3.5 and GPT-4. Despite having a lower training proportion compared to English, these models also exhibit remarkable capabilities in other languages. In this study, we assess the performance of GPT-3.5 and GPT-4 models on seven distinct Arabic NLP tasks: sentiment analysis, translation, transliteration, paraphrasing, part of speech tagging, summarization, and diacritization. Our findings reveal that GPT-4 outperforms GPT-3.5 on five out of the seven tasks. Furthermore, we conduct an extensive analysis of the sentiment analysis task, providing insights into how LLMs achieve exceptional results on a challenging dialectal dataset. Additionally, we introduce a new Python interface https://github.com/ARBML/Taqyim that facilitates the evaluation of these tasks effortlessly.
    
[^7]: 一种用于中文文本纠错的对抗多任务学习方法和语义检测

    An Adversarial Multi-Task Learning Method for Chinese Text Correction with Semantic Detection. (arXiv:2306.16313v1 [cs.CL])

    [http://arxiv.org/abs/2306.16313](http://arxiv.org/abs/2306.16313)

    本论文提出了一种用于中文文本纠错的对抗多任务学习方法和语义检测，在中文句子上下文中增强了字符多义的建模和检测能力，并通过实验证明了方法的有效性。

    

    文本纠错，尤其是更广泛应用场景下的语义纠错，对于提高文本的流畅性和写作效率有着极高的需求。本文提出了一种对抗多任务学习方法，旨在增强中文句子上下文中字符多义的建模和检测能力。其中，引入了掩码语言模型和评分语言模型作为一对不仅耦合而且对抗的学习任务。此外，还引入了蒙特卡洛树搜索策略和一个策略网络，以实现带有语义检测的高效中文文本纠错任务。实验在三个数据集和五种可比较的方法上进行，实验结果表明，我们的方法在中文文本纠错任务中表现良好，能够更好地实现语义合理性。

    Text correction, especially the semantic correction of more widely used scenes, is strongly required to improve, for the fluency and writing efficiency of the text. An adversarial multi-task learning method is proposed to enhance the modeling and detection ability of character polysemy in Chinese sentence context. Wherein, two models, the masked language model and scoring language model, are introduced as a pair of not only coupled but also adversarial learning tasks. Moreover, the Monte Carlo tree search strategy and a policy network are introduced to accomplish the efficient Chinese text correction task with semantic detection. The experiments are executed on three datasets and five comparable methods, and the experimental results show that our method can obtain good performance in Chinese text correction task for better semantic rationality.
    
[^8]: ChatGPT在国家医疗执照考试中表现出色，但在基础线性代数上不佳

    ChatGPT may excel in States Medical Licensing Examination but falters in basic Linear Algebra. (arXiv:2306.16282v1 [cs.CL])

    [http://arxiv.org/abs/2306.16282](http://arxiv.org/abs/2306.16282)

    ChatGPT在国家医疗执照考试中表现出色，但在基础线性代数教学上存在重大数学错误和逻辑推断失败的问题，不适合作为学生的教师。

    

    ChatGPT的出现是迅速的，虽然它在某些领域表现出积极影响，但它的影响并不普遍有利。我们的分析集中在ChatGPT在数学教育中的能力，特别是在教授基础线性代数方面。虽然ChatGPT在某些情况下可以提供准确且有动力的答案，但我们必须认识到它在许多情况下会出现重大的数学错误，以及在逻辑推断方面的失败。这些情况引发了对系统对数学的真正理解的担忧，因为它似乎更依赖于视觉模式而不是真正的理解。此外，ChatGPT作为学生的教师的适用性也值得考虑。

    The emergence of ChatGPT has been rapid, and although it has demonstrated positive impacts in certain domains, its influence is not universally advantageous. Our analysis focuses on ChatGPT's capabilities in Mathematics Education, particularly in teaching basic Linear Algebra. While there are instances where ChatGPT delivers accurate and well-motivated answers, it is crucial to recognize numerous cases where it makes significant mathematical errors and fails in logical inference. These occurrences raise concerns regarding the system's genuine understanding of mathematics, as it appears to rely more on visual patterns rather than true comprehension. Additionally, the suitability of ChatGPT as a teacher for students also warrants consideration.
    
[^9]: 利用GPT-4进行食物影响摘要以通过迭代提示增强产品特定指导开发

    Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting. (arXiv:2306.16275v1 [cs.CL])

    [http://arxiv.org/abs/2306.16275](http://arxiv.org/abs/2306.16275)

    本研究提出了一种通过迭代提示与ChatGPT或GPT-4进行多轮交互的方法来改进食物影响摘要的准确性。这种方法在产品特定指导(PSG)开发中具有潜力应用的价值。

    

    食物影响从新药申请(NDA)中的摘要是产品特定指导(PSG)开发和评估的重要组成部分。然而，从大量药物申请审查文件中手动摘要食物影响是耗时的，这引发了开发自动化方法的需求。最近大型语言模型(LLMs)如ChatGPT和GPT-4的进展已经展示了在改善自动文本摘要效果方面的巨大潜力，但其在PSG评估中准确概括食物影响的能力尚不清楚。在这项研究中，我们引入了一种简单而有效的方法，即迭代提示，通过多轮交互更有效和高效地与ChatGPT或GPT-4进行互动。具体而言，我们提出了一个三轮迭代提示的方法来进行食物影响摘要，其中在连续的轮次中分别提供了关键字聚焦和长度控制的提示以细化。

    Food effect summarization from New Drug Application (NDA) is an essential component of product-specific guidance (PSG) development and assessment. However, manual summarization of food effect from extensive drug application review documents is time-consuming, which arouses a need to develop automated methods. Recent advances in large language models (LLMs) such as ChatGPT and GPT-4, have demonstrated great potential in improving the effectiveness of automated text summarization, but its ability regarding the accuracy in summarizing food effect for PSG assessment remains unclear. In this study, we introduce a simple yet effective approach, iterative prompting, which allows one to interact with ChatGPT or GPT-4 more effectively and efficiently through multi-turn interaction. Specifically, we propose a three-turn iterative prompting approach to food effect summarization in which the keyword-focused and length-controlled prompts are respectively provided in consecutive turns to refine the 
    
[^10]: 在阿富汗禁止教育的推文情绪分析

    Emotion Analysis of Tweets Banning Education in Afghanistan. (arXiv:2306.16268v1 [cs.CL])

    [http://arxiv.org/abs/2306.16268](http://arxiv.org/abs/2306.16268)

    这项研究介绍了第一个用于阿富汗普什图语变体的情绪标注数据集，该数据集包含7600条推文，以研究塔利班禁止妇女接受教育的情绪反应，并通过多种神经架构对达里语情感分类进行基准测试。

    

    本文介绍了第一个用于阿富汗普什图语变体的情绪标注数据集。LetHerLearn数据集包含了7600条推文，这些推文是对塔利班于2022年禁止妇女接受教育的反应，并且已根据埃克曼情绪类别进行了手动标注。我们在此详细介绍了数据收集和标注过程，呈现了相关数据集统计信息以及对所得数据集进行的初步实验，对达里语情感分类任务进行了多种不同神经架构的基准测试。

    This paper introduces the first emotion annotated dataset for the Dari variant of Persian spoken in Afghanistan. The LetHerLearn dataset contains 7,600 tweets posted in reaction to the Taliban ban of women rights to education in 2022 and has been manually annotated according to Ekman emotion categories. We here detail the data collection and annotation process, present relevant dataset statistics as well as initial experiments on the resulting dataset, benchmarking a number of different neural architectures for the task of Dari emotion classification.
    
[^11]: CBBQ: 通过人工智能与人类协同合作为大型语言模型构建的中文偏差基准数据集

    CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models. (arXiv:2306.16244v1 [cs.CL])

    [http://arxiv.org/abs/2306.16244](http://arxiv.org/abs/2306.16244)

    通过人工智能与人类协同合作构建的CBBQ中文偏差基准数据集，全面衡量了与中国文化和价值观相关的14个社会维度中的刻板印象和社会偏见，对于检测模型偏见具有广泛的覆盖范围和高度的多样性。

    

    全面衡量大型语言模型的社会偏见对于检测和降低高能力人工智能模型的道德风险至关重要。在这项工作中，我们介绍了一个由人类专家和生成式语言模型共同构建的中文偏差基准数据集，包括与中国文化和价值观相关的14个社会维度中的刻板印象和社会偏见。在数据集的整理过程中，包括4个关键步骤：通过广泛的文献评论识别偏见，生成模糊的上下文，通过人工智能辅助消除模糊的上下文，以及手动审查和重组。数据集中的测试实例是从3000多个经过严格质量控制的高质量模板手动提取的。数据集具有广泛的覆盖范围和高度的多样性。广泛的实验证明了该数据集在检测模型偏见方面的有效性，10个公开可用的中文大型语言模型均表现出明显的偏见。

    Holistically measuring societal biases of large language models is crucial for detecting and reducing ethical risks in highly capable AI models. In this work, we present a Chinese Bias Benchmark dataset that consists of over 100K questions jointly constructed by human experts and generative language models, covering stereotypes and societal biases in 14 social dimensions related to Chinese culture and values. The curation process contains 4 essential steps: bias identification via extensive literature review, ambiguous context generation, AI-assisted disambiguous context generation, snd manual review \& recomposition. The testing instances in the dataset are automatically derived from 3K+ high-quality templates manually authored with stringent quality control. The dataset exhibits wide coverage and high diversity. Extensive experiments demonstrate the effectiveness of the dataset in detecting model bias, with all 10 publicly available Chinese large language models exhibiting strong bia
    
[^12]: 从行动和指令中推断沟通代理的目标

    Inferring the Goals of Communicating Agents from Actions and Instructions. (arXiv:2306.16207v1 [cs.AI])

    [http://arxiv.org/abs/2306.16207](http://arxiv.org/abs/2306.16207)

    本文介绍了一个模型，可以通过观察行动和指令推断合作团队的目标，并评估了其与人类判断的相关性。

    

    当人类合作时，他们经常通过口头沟通和非口头行动来协调活动，利用这些信息来推断共同的目标和计划。我们如何建模这种推理能力？本文介绍了一个合作团队的模型，其中一个代理（主要的）可以通过自然语言指令向另一个代理（助理）传达关于共同计划的信息，使用GPT-3作为指令语句的似然函数。然后，我们展示了一个第三方观察者如何通过多模态贝叶斯逆向规划从行动和指令中推断团队的目标，计算在代理人会采取和交流以实现目标的假设下的目标的后验分布。我们通过将其与多代理系统的网格世界中的人类目标推断进行比较来评估这种方法，发现我们模型的推断与人类判断密切相关（R = 0.96）。与仅从行动推断相比，我们还发现...

    When humans cooperate, they frequently coordinate their activity through both verbal communication and non-verbal actions, using this information to infer a shared goal and plan. How can we model this inferential ability? In this paper, we introduce a model of a cooperative team where one agent, the principal, may communicate natural language instructions about their shared plan to another agent, the assistant, using GPT-3 as a likelihood function for instruction utterances. We then show how a third person observer can infer the team's goal via multi-modal Bayesian inverse planning from actions and instructions, computing the posterior distribution over goals under the assumption that agents will act and communicate rationally to achieve them. We evaluate this approach by comparing it with human goal inferences in a multi-agent gridworld, finding that our model's inferences closely correlate with human judgments (R = 0.96). When compared to inference from actions alone, we also find th
    
[^13]: 通过动态图知识聚合提升对话生成

    Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation. (arXiv:2306.16195v1 [cs.CL])

    [http://arxiv.org/abs/2306.16195](http://arxiv.org/abs/2306.16195)

    本研究提出了一个通过动态图知识聚合来提升对话生成的新框架，它能够更好地利用来自帖子和外部图知识的异质特征，从而解决图知识和文本之间的语义差异问题。

    

    将外部图知识融入神经对话机器人模型已被证明有效提升对话生成。然而，在传统的图神经网络（GNN）中，图上的信息传递与文本无关，导致图表征和文本之间存在语义差异。现有模型的训练方式导致了图知识和文本之间的语义鸿沟。本研究提出了一种新的知识图增强对话生成的框架。我们动态构建一个带有伪节点的多跳知识图，在图中的每一步中都将语言模型纳入特征聚合。为了避免学习在普通子图上引起的语义偏差，所提出的框架应用了分层图注意力，以聚合伪节点上的图特征，最终获得全局特征。因此，该框架能够更好地利用来自帖子和外部图知识的异质特征。

    Incorporating external graph knowledge into neural chatbot models has been proven effective for enhancing dialogue generation. However, in conventional graph neural networks (GNNs), message passing on a graph is independent from text, resulting in the graph representation hidden space differing from that of the text. This training regime of existing models therefore leads to a semantic gap between graph knowledge and text. In this study, we propose a novel framework for knowledge graph enhanced dialogue generation. We dynamically construct a multi-hop knowledge graph with pseudo nodes to involve the language model in feature aggregation within the graph at all steps. To avoid the semantic biases caused by learning on vanilla subgraphs, the proposed framework applies hierarchical graph attention to aggregate graph features on pseudo nodes and then attains a global feature. Therefore, the framework can better utilise the heterogeneous features from both the post and external graph knowle
    
[^14]: SkillNet-X：一种具有稀疏激活技能的多语言多任务模型

    SkillNet-X: A Multilingual Multitask Model with Sparsely Activated Skills. (arXiv:2306.16176v1 [cs.CL])

    [http://arxiv.org/abs/2306.16176](http://arxiv.org/abs/2306.16176)

    学术论文提出了一种名为SkillNet-X的多语言多任务模型，通过稀疏激活相关的技能模块，能够在不同语言的多个任务上表现出更好的性能。

    

    传统的多任务学习方法只能在任务或语言上利用通用知识，即失去了跨语言或跨任务的知识。本文提出了一种名为SkillNet-X的通用多语言多任务模型，能够通过单一模型处理来自不同语言的多个不同任务。为此，我们定义了几个语言特定的技能和任务特定的技能，每个技能对应一个技能模块。SkillNet-X稀疏激活与目标任务或目标语言相关的技能模块的部分。作为知识传递中心，技能模块能够连续吸收与任务相关的知识和语言相关的知识。基于Transformer，我们修改了多头注意力层和前馈网络层以适应技能模块。我们在四种语言的十一个自然语言理解数据集上评估了SkillNet-X。结果表明，SkillNet-X的性能优于单独针对每个任务的多任务学习方法。

    Traditional multitask learning methods basically can only exploit common knowledge in task- or language-wise, which lose either cross-language or cross-task knowledge. This paper proposes a general multilingual multitask model, named SkillNet-X, which enables a single model to tackle many different tasks from different languages. To this end, we define several language-specific skills and task-specific skills, each of which corresponds to a skill module. SkillNet-X sparsely activates parts of the skill modules which are relevant either to the target task or the target language. Acting as knowledge transit hubs, skill modules are capable of absorbing task-related knowledge and language-related knowledge consecutively. Based on Transformer, we modify the multi-head attention layer and the feed forward network layer to accommodate skill modules. We evaluate SkillNet-X on eleven natural language understanding datasets in four languages. Results show that SkillNet-X performs better than tas
    
[^15]: 为开发领域特定自然语言处理应用而进行的生成式用户体验研究

    Generative User-Experience Research for Developing Domain-specific Natural Language Processing Applications. (arXiv:2306.16143v1 [cs.CL])

    [http://arxiv.org/abs/2306.16143](http://arxiv.org/abs/2306.16143)

    本论文提出了一种在开发领域特定自然语言处理应用中整合生成式用户体验研究的方法。该方法将领域用户纳入原型开发的不同阶段，以更好地了解用户需求和评估用户价值的变化。

    

    用户体验（UX）是人机交互（HCI）研究的一部分，专注于提高系统用户的直观性、透明度、简洁性和信任度。大多数针对机器学习（ML）或自然语言处理（NLP）的UX研究都采用数据驱动的方法，即没有关注用户需求，并仅仅将领域用户用于可用性评估。此外，更典型的UX方法是先针对用户的可用性进行定制，而不是首先了解用户需求。本文提出了一种将生成式UX研究整合到开发领域NLP应用中的方法。生成式UX研究将领域用户纳入原型开发的初始阶段，即构思和概念评估阶段，以及最后一阶段评估用户价值的变化。案例研究中，我们报道了一个针对过程工业中日常操作的领域特定语义搜索的完整原型开发过程。

    User experience (UX) is a part of human-computer interaction (HCI) research and focuses on increasing intuitiveness, transparency, simplicity, and trust for system users. Most of the UX research for machine learning (ML) or natural language processing (NLP) focuses on a data-driven methodology, i.e., it fails to focus on users' requirements, and engages domain users mainly for usability evaluation. Moreover, more typical UX methods tailor the systems towards user usability, unlike learning about the user needs first. The paper proposes a methodology for integrating generative UX research into developing domain NLP applications. Generative UX research employs domain users at the initial stages of prototype development, i.e., ideation and concept evaluation, and the last stage for evaluating the change in user value. In the case study, we report the full-cycle prototype development of a domain-specific semantic search for daily operations in the process industry. Our case study shows tha
    
[^16]: 在社交媒体上识别抑郁症的框架：MentalRiskES@IberLEF 2023

    A Framework for Identifying Depression on Social Media: MentalRiskES@IberLEF 2023. (arXiv:2306.16125v1 [cs.CL])

    [http://arxiv.org/abs/2306.16125](http://arxiv.org/abs/2306.16125)

    该论文介绍了在社交媒体上识别抑郁症的框架，使用机器学习和深度学习技术来解决四个预测子任务，并发现使用句子嵌入作为线性回归器的输入产生了更好的结果。

    

    本文描述了我们参与IberLEF 2023的MentalRiskES任务。该任务涉及根据个人在社交媒体上的活动来预测他们可能患抑郁症的可能性。数据集由175个Telegram用户的对话组成，每个用户根据他们患病证据进行标记。我们使用传统机器学习和深度学习技术的组合来解决四个预测子任务：二分类、简单回归、多类别分类和多类别回归。我们通过训练一个模型来解决多类别回归问题，然后将预测结果转换为适用于其他三个子任务的结果。我们比较了两种不同建模方法的性能：对基于BERT的模型进行微调和使用句子嵌入作为线性回归器的输入，后者产生了更好的结果。可以在以下链接找到复现我们结果的代码：https://github.com/simonsanvil/EarlyDep

    This paper describes our participation in the MentalRiskES task at IberLEF 2023. The task involved predicting the likelihood of an individual experiencing depression based on their social media activity. The dataset consisted of conversations from 175 Telegram users, each labeled according to their evidence of suffering from the disorder. We used a combination of traditional machine learning and deep learning techniques to solve four predictive subtasks: binary classification, simple regression, multiclass classification, and multiclass regression. We approached this by training a model to solve the multiclass regression case and then transforming the predictions to work for the other three subtasks. We compare the performance of two different modeling approaches: fine-tuning a BERT-based model and using sentence embeddings as inputs to a linear regressor, with the latter yielding better results. The code to reproduce our results can be found at: https://github.com/simonsanvil/EarlyDep
    
[^17]: ChatGPT 是一个生物医学专家吗？——探索当前 GPT 模型在生物医学任务中的零样本性能。

    Is ChatGPT a Biomedical Expert? -- Exploring the Zero-Shot Performance of Current GPT Models in Biomedical Tasks. (arXiv:2306.16108v1 [cs.CL])

    [http://arxiv.org/abs/2306.16108](http://arxiv.org/abs/2306.16108)

    通过对 GPT-3.5-Turbo 和 GPT-4 在生物医学任务中的表现评估，发现它们能够通过零样本学习和相关片段的支撑与领先系统相竞争，但在检索任务中表现不如其他系统。

    

    我们评估了商业大型语言模型 GPT-3.5-Turbo 和 GPT-4 在 2023 年 BioASQ 挑战中的任务表现。在任务 11b 第二阶段中，也就是答案生成任务中，这两个模型通过简单的零样本学习和相关片段的支撑表现出了与领先系统相竞争的能力。值得注意的是，即使没有相关片段，它们的性能也令人满意，尽管没有达到最佳系统的水平。有趣的是，较旧且更便宜的 GPT-3.5-Turbo 系统在基于事实和列表答案的问答环境中能够与 GPT-4 相竞争。在任务 11b 第一阶段中，侧重于检索，通过零样本学习的查询扩展提高了模型的性能，但与其他系统相比仍然有所不足。重新运行这些实验所需的代码可在 GitHub 上获得。

    We assessed the performance of commercial Large Language Models (LLMs) GPT-3.5-Turbo and GPT-4 on tasks from the 2023 BioASQ challenge. In Task 11b Phase B, which is focused on answer generation, both models demonstrated competitive abilities with leading systems. Remarkably, they achieved this with simple zero-shot learning, grounded with relevant snippets. Even without relevant snippets, their performance was decent, though not on par with the best systems. Interestingly, the older and cheaper GPT-3.5-Turbo system was able to compete with GPT-4 in the grounded Q&A setting on factoid and list answers. In Task 11b Phase A, focusing on retrieval, query expansion through zero-shot learning improved performance, but the models fell short compared to other systems. The code needed to rerun these experiments is available through GitHub.
    
[^18]: ChatLaw: 基于开源法律大型语言模型的综合外部知识库

    ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases. (arXiv:2306.16092v1 [cs.CL])

    [http://arxiv.org/abs/2306.16092](http://arxiv.org/abs/2306.16092)

    本论文介绍了一个基于开源的法律大型语言模型ChatLaw，它通过综合外部知识库为中国法律领域的数字化转型提供支持。该模型采用了精心设计的法律领域微调数据集，并通过将向量数据库检索与关键词检索相结合的方法解决了法律数据筛选中的模型幻觉问题。同时，引入了一种自注意力方法以增强对文本中关键信息的注意力。

    

    大型语言模型（LLM）已经展示出在各个领域中改变自然语言处理任务的潜力，引发了对垂直特定大模型的极大兴趣。然而，与像BloombergGPT和FinGPT这样利用其独特数据积累在金融领域取得进展的专有模型不同，中国法律领域中没有类似的大型语言模型来促进数字化转型。在本文中，我们提出了一个名为ChatLaw的开源法律大型语言模型。由于数据质量的重要性，我们精心设计了一个法律领域微调数据集。此外，为了解决在参考数据检索过程中法律数据筛选中的模型幻觉问题，我们引入了一种将向量数据库检索与关键词检索相结合的方法，以有效减少只依靠向量数据库检索的不准确性。此外，我们提出了一种自注意力方法来增强模型的对文本中关键信息的注意。

    Large Language Models (LLMs) have shown the potential to revolutionize natural language processing tasks in various domains, sparking great interest in vertical-specific large models. However, unlike proprietary models such as BloombergGPT and FinGPT, which have leveraged their unique data accumulations to make strides in the finance domain, there hasn't not many similar large language models in the Chinese legal domain to facilitate its digital transformation.  In this paper, we propose an open-source legal large language model named ChatLaw. Due to the importance of data quality, we carefully designed a legal domain fine-tuning dataset. Additionally, to overcome the problem of model hallucinations in legal data screening during reference data retrieval, we introduce a method that combines vector database retrieval with keyword retrieval to effectively reduce the inaccuracy of relying solely on vector database retrieval. Furthermore, we propose a self-attention method to enhance the a
    
[^19]: 长期对话分析：探索实用性和隐私保护

    Long-term Conversation Analysis: Exploring Utility and Privacy. (arXiv:2306.16071v1 [eess.AS])

    [http://arxiv.org/abs/2306.16071](http://arxiv.org/abs/2306.16071)

    本文研究了长期对话分析中的隐私保护问题。通过使用McAdams系数和频谱平滑的组合方法，我们提出了一种隐私保护特征提取方法，该方法在提高隐私保护的同时保持了实用性。

    

    分析日常生活中记录的对话需要隐私保护。在本文中，我们探索了一种基于输入特征维度降低、频谱平滑和基于McAdams系数的低成本说话者匿名化技术的隐私保护特征提取方法。我们通过语音活动检测和说话者分割系统评估了特征提取方法的实用性，通过语音识别和说话者验证模型确定了隐私保护。我们展示了McAdams系数和频谱平滑的组合在提高隐私保护的同时保持了实用性。

    The analysis of conversations recorded in everyday life requires privacy protection. In this contribution, we explore a privacy-preserving feature extraction method based on input feature dimension reduction, spectral smoothing and the low-cost speaker anonymization technique based on McAdams coefficient. We assess the utility of the feature extraction methods with a voice activity detection and a speaker diarization system, while privacy protection is determined with a speech recognition and a speaker verification model. We show that the combination of McAdams coefficient and spectral smoothing maintains the utility while improving privacy.
    
[^20]: 通过Twitter和AI在2022年卡塔尔世界杯之前学到的情感和有趣事实

    What Sentiment and Fun Facts We Learnt Before FIFA World Cup Qatar 2022 Using Twitter and AI. (arXiv:2306.16049v1 [cs.CL])

    [http://arxiv.org/abs/2306.16049](http://arxiv.org/abs/2306.16049)

    本文使用机器学习和收集的Twitter推文，对2022年卡塔尔世界杯之前的情感和有趣事实进行了分析。结果表明人们对世界杯的开幕持积极态度。

    

    Twitter是一个连接大多数国家的社交媒体平台，可以实时发现新闻。由于Twitter上的推文通常很短并表达公众情感，因此可作为全球事件的观点挖掘和情感分析的数据源。本文提出了一个有效的解决方案，用于提供与FIFA世界杯相关的推文的情感分析。在社区中首次收集了至少13万条推文，并将其作为数据集来评估所提出的机器学习解决方案的性能。这些推文是通过卡塔尔世界杯2022的相关标签和关键词收集的。本文使用Vader算法进行情感分析。通过机器学习方法和收集的Twitter推文，我们发现了在世界杯之前一段时间内与几个重要方面相关的情感和有趣事实。结果显示人们对世界杯的开幕持积极态度。

    Twitter is a social media platform bridging most countries and allows real-time news discovery. Since the tweets on Twitter are usually short and express public feelings, thus provide a source for opinion mining and sentiment analysis for global events. This paper proposed an effective solution, in providing a sentiment on tweets related to the FIFA World Cup. At least 130k tweets, as the first in the community, are collected and implemented as a dataset to evaluate the performance of the proposed machine learning solution. These tweets are collected with the related hashtags and keywords of the Qatar World Cup 2022. The Vader algorithm is used in this paper for sentiment analysis. Through the machine learning method and collected Twitter tweets, we discovered the sentiments and fun facts of several aspects important to the period before the World Cup. The result shows people are positive to the opening of the World Cup.
    
[^21]: 在社交媒体上探索公共话语的时空变化：意大利新冠疫情第一波流行的案例研究

    Exploring Spatial-Temporal Variations of Public Discourse on Social Media: A Case Study on the First Wave of the Coronavirus Pandemic in Italy. (arXiv:2306.16031v1 [cs.CL])

    [http://arxiv.org/abs/2306.16031](http://arxiv.org/abs/2306.16031)

    本论文提出了一种方法，通过社交媒体上的语言行为研究了意大利新冠疫情第一波流行期间公众对事件的时空变化反应。研究使用时间序列分析和聚类技术探索了推文使用趋势，通过定性比较分析确定了对应的术语类别。研究结果证实了已有的心理观察结果，即事件的震中和外围区域的公众言论呈现出明显的时空聚类特征。

    

    本文提出了一种方法，通过社交媒体上的语言行为来研究社会对重要事件（例如SARS CoV2大流行期间发生的事件）的反应，特别是空间和时间方面的特征。我们的方法包括使用时间序列分析和聚类技术基于推文使用趋势来确定时空范畴。然后，我们通过定性比较分析，将每个类别中的显著术语鉴定为手工编码类别中基于缩放F分数聚合的术语。为了说明这种方法，我们在意大利新冠疫情的第一波流行中进行了案例研究。我们使用我们提出的方法来探索已有的心理观察结果，即身体距离事件的远近会影响事件相关信息的传播。我们通过展示疾病的震中和外围区域对应于清晰的时间序列聚类来确认这些发现。

    This paper proposes a methodology for exploring how linguistic behaviour on social media can be used to explore societal reactions to important events such as those that transpired during the SARS CoV2 pandemic. In particular, where spatial and temporal aspects of events are important features. Our methodology consists of grounding spatial-temporal categories in tweet usage trends using time-series analysis and clustering. Salient terms in each category were then identified through qualitative comparative analysis based on scaled f-scores aggregated into hand-coded categories. To exemplify this approach, we conducted a case study on the first wave of the coronavirus in Italy. We used our proposed methodology to explore existing psychological observations which claimed that physical distance from events affects what is communicated about them. We confirmed these findings by showing that the epicentre of the disease and peripheral regions correspond to clear time-series clusters and that
    
[^22]: 加速传感器通过相邻令牌合并

    Accelerating Transducers through Adjacent Token Merging. (arXiv:2306.16009v1 [cs.CL])

    [http://arxiv.org/abs/2306.16009](http://arxiv.org/abs/2306.16009)

    提出了一种相邻令牌合并（A-ToMe）的新方法，通过逐渐合并具有相似关键值的相邻令牌，减少了总的时间步长，并加快了语音识别系统的推理速度。实验证明，该方法可以显著减少令牌数量并提高推理速度，同时准确性损失微乎其微。

    

    最近的端到端自动语音识别（ASR）系统通常利用基于Transformer的声学编码器，在高帧率下生成嵌入。然而，由于自注意力的二次计算，这种设计在处理长音频信号时效率低下。为了解决这个问题，我们提出了一种新的方法，即相邻令牌合并（A-ToMe），它逐渐合并具有相似关键值的相邻令牌。通过这种方式，可以减少总的时间步长，并加快编码器和联合网络的推理速度。在LibriSpeech上的实验证明，我们的方法可以减少57%的令牌，并在GPU上提高70%的推理速度，而几乎没有损失准确性。此外，我们还证明A-ToMe也是一种减少长形式ASR令牌的有效解决方案，其中输入语音由多个话语组成。

    Recent end-to-end automatic speech recognition (ASR) systems often utilize a Transformer-based acoustic encoder that generates embedding at a high frame rate. However, this design is inefficient, particularly for long speech signals due to the quadratic computation of self-attention. To address this, we propose a new method, Adjacent Token Merging (A-ToMe), which gradually combines adjacent tokens with high similarity scores between their key values. In this way, the total time step could be reduced, and the inference of both the encoder and joint network is accelerated. Experiments on LibriSpeech show that our method can reduce 57% of tokens and improve the inference speed on GPU by 70% without any notable loss of accuracy. Additionally, we demonstrate that A-ToMe is also an effective solution to reduce tokens in long-form ASR, where the input speech consists of multiple utterances.
    
[^23]: 提示大型语言模型在语音识别中进行零样本领域适应

    Prompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition. (arXiv:2306.16007v1 [cs.CL])

    [http://arxiv.org/abs/2306.16007](http://arxiv.org/abs/2306.16007)

    这篇论文提出了两种新方法在语音识别中进行零样本领域适应，通过使用大型语言模型进行二次修正和深度融合，只使用一个领域提示即可有效降低词错误率，并且有更好的实体和超出词汇的召回效果。

    

    语言模型（LMs）的整合已被证明是解决语音识别中的领域转移的有效方法。然而，这些方法通常需要大量的目标领域文本数据来训练LMs。与这些方法不同的是，在本研究中，我们只使用一个特定领域的文本提示，提出了两种使用LLaMA（一个具有70亿参数的大型语言模型）进行零样本ASR领域适应的方法。LLaMA有两种用法：1）二次修正：使用LLaMA重新排列给定ASR系统的N个最佳假设；2）深度LLM融合：将LLaMA整合到基于编码器-解码器的ASR系统的解码器中。实验证明，只使用一个领域提示，两种方法都可以有效降低超出领域的TedLium-2和SPGISpeech数据集的词错误率（WER）。尤其是，深度LLM融合具有更好的实体和超出词汇的单词召回优势。

    The integration of Language Models (LMs) has proven to be an effective way to address domain shifts in speech recognition. However, these approaches usually require a significant amount of target domain text data for the training of LMs. Different from these methods, in this work, with only a domain-specific text prompt, we propose two zero-shot ASR domain adaptation methods using LLaMA, a 7-billion-parameter large language model (LLM). LLM is used in two ways: 1) second-pass rescoring: reranking N-best hypotheses of a given ASR system with LLaMA; 2) deep LLM-fusion: incorporating LLM into the decoder of an encoder-decoder based ASR system. Experiments show that, with only one domain prompt, both methods can effectively reduce word error rates (WER) on out-of-domain TedLium-2 and SPGISpeech datasets. Especially, the deep LLM-fusion has the advantage of better recall of entity and out-of-vocabulary words.
    
[^24]: 用深度学习简化社交媒体信息检索以支持公共卫生研究

    Streamlining Social Media Information Retrieval for Public Health Research with Deep Learning. (arXiv:2306.16001v1 [cs.CL])

    [http://arxiv.org/abs/2306.16001](http://arxiv.org/abs/2306.16001)

    本研究介绍了一个使用深度学习简化社交媒体信息检索的框架，通过识别医学实体、标准化实体和分配UMLS概念，构建了一个用于COVID-19相关推文的症状词典。

    

    社交媒体在流行病监测中的利用已经得到了很好的证实。然而，当使用预定义的词汇表来检索相关语料库时，常常会引入偏见。本研究介绍了一个框架，旨在构建医学俗语和统一医学语言系统（UMLS）概念的广泛字典。该框架由三个模块组成：基于BERT的命名实体识别（NER）模型，用于从社交媒体内容中识别出医学实体；深度学习驱动的标准化模块，用于对提取出的实体进行规范化处理；半监督聚类模块，将最可能的UMLS概念分配给每个规范化实体。我们将该框架应用于从2020年2月1日到2022年4月30日期间与COVID-19相关的推文，生成了一个症状词典（可在https://github.com/ningkko/UMLS_colloquialism/上获取），其中包含9,249个标准化实体，映射到876个UMLS概念和38,175个俚语表达。该框架的演示

    The utilization of social media in epidemic surveillance has been well established. Nonetheless, bias is often introduced when pre-defined lexicons are used to retrieve relevant corpus. This study introduces a framework aimed at curating extensive dictionaries of medical colloquialisms and Unified Medical Language System (UMLS) concepts. The framework comprises three modules: a BERT-based Named Entity Recognition (NER) model that identifies medical entities from social media content, a deep-learning powered normalization module that standardizes the extracted entities, and a semi-supervised clustering module that assigns the most probable UMLS concept to each standardized entity. We applied this framework to COVID-19-related tweets from February 1, 2020, to April 30, 2022, generating a symptom dictionary (available at https://github.com/ningkko/UMLS_colloquialism/) composed of 9,249 standardized entities mapped to 876 UMLS concepts and 38,175 colloquial expressions. This framework demo
    
[^25]: 多任务学习日语句子分类和命名实体识别的句子到标签生成框架

    Sentence-to-Label Generation Framework for Multi-task Learning of Japanese Sentence Classification and Named Entity Recognition. (arXiv:2306.15978v1 [cs.CL])

    [http://arxiv.org/abs/2306.15978](http://arxiv.org/abs/2306.15978)

    本研究提出了一种句子到标签生成框架，用于多任务学习日语句子分类和命名实体识别。通过整合句子分类和命名实体识别任务，并提出约束机制来提高格式准确性，结果显示整合能够增强两个任务的性能。

    

    信息提取是自然语言处理中一个关键的子领域。本研究介绍了一种句子分类和命名实体识别多任务（SCNM）方法，将句子分类（SC）和命名实体识别（NER）结合在一起。我们开发了一个句子到标签生成（SLG）框架用于SCNM，并构建了一个包含SC和NER数据的维基百科数据集。通过格式转换器，我们统一了输入格式，并使用生成模型生成SC标签、NER标签和相关文本片段。我们提出了一个约束机制（CM）来提高生成的格式准确性。我们的结果显示，与独立任务相比，SCNM中的SC准确度提高了1.13个百分点，NER提高了1.06个百分点，CM将格式准确性从63.61提高到100。研究结果表明SC和NER之间存在相互强化的效应，整合可以提高两个任务的性能。

    Information extraction(IE) is a crucial subfield within natural language processing. In this study, we introduce a Sentence Classification and Named Entity Recognition Multi-task (SCNM) approach that combines Sentence Classification (SC) and Named Entity Recognition (NER). We develop a Sentence-to-Label Generation (SLG) framework for SCNM and construct a Wikipedia dataset containing both SC and NER. Using a format converter, we unify input formats and employ a generative model to generate SC-labels, NER-labels, and associated text segments. We propose a Constraint Mechanism (CM) to improve generated format accuracy. Our results show SC accuracy increased by 1.13 points and NER by 1.06 points in SCNM compared to standalone tasks, with CM raising format accuracy from 63.61 to 100. The findings indicate mutual reinforcement effects between SC and NER, and integration enhances both tasks' performance.
    
[^26]: 通过验证和纠正提示进行数据生成文本生成

    You Can Generate It Again: Data-to-text Generation with Verification and Correction Prompting. (arXiv:2306.15933v1 [cs.CL])

    [http://arxiv.org/abs/2306.15933](http://arxiv.org/abs/2306.15933)

    本文提出了一种多步骤生成、验证和纠正的数据生成文本方法，通过专门的错误指示提示来改善输出质量。

    

    尽管现有模型取得了显著进展，从结构化数据输入生成文本描述（称为数据生成文本）仍然是一个具有挑战性的任务。在本文中，我们提出了一种新的方法，通过引入包括生成、验证和纠正阶段的多步骤过程，超越了传统的一次性生成方法。我们的方法，VCP（验证和纠正提示），从模型生成初始输出开始。然后，我们继续验证所生成文本的不同方面的正确性。验证步骤的观察结果被转化为专门的错误指示提示，该提示指示模型在重新生成输出时考虑已识别的错误。为了增强模型的纠正能力，我们开发了一个经过精心设计的培训过程。该过程使模型能够融入错误指示提示的反馈，从而改善输出生成。

    Despite significant advancements in existing models, generating text descriptions from structured data input, known as data-to-text generation, remains a challenging task. In this paper, we propose a novel approach that goes beyond traditional one-shot generation methods by introducing a multi-step process consisting of generation, verification, and correction stages. Our approach, VCP(Verification and Correction Prompting), begins with the model generating an initial output. We then proceed to verify the correctness of different aspects of the generated text. The observations from the verification step are converted into a specialized error-indication prompt, which instructs the model to regenerate the output while considering the identified errors. To enhance the model's correction ability, we have developed a carefully designed training procedure. This procedure enables the model to incorporate feedback from the error-indication prompt, resulting in improved output generation. Throu
    
[^27]: 大多数语言模型也可以成为诗人：一个AI写作助手和有限文本生成工具

    Most Language Models can be Poets too: An AI Writing Assistant and Constrained Text Generation Studio. (arXiv:2306.15926v1 [cs.CL])

    [http://arxiv.org/abs/2306.15926](http://arxiv.org/abs/2306.15926)

    这项研究展示了如何通过在语言模型中应用过滤函数来生成有限约束文本，并提出了一个AI写作助手工具，可以根据用户的需求生成带有各种约束条件的文本。

    

    尽管有关有限自然语言生成领域的快速进展，但对已被词汇、语义或音韵约束的语言模型的潜力研究时间很少。我们发现，大多数语言模型即使在显著约束下也能生成引人入胜的文本。我们提出了一种简单而普适的技术，通过在生成文本单元之前组合应用过滤函数到语言模型的词汇，来修改语言模型的输出。这种方法是即插即用的，不需要对模型进行修改。为展示这种技术的价值，我们介绍了一个易于使用的AI写作助手，名为有限文本生成工具（CTGS）。CTGS允许用户生成或选择具有各种约束条件的文本，例如禁止某个字母，强制生成的单词具有一定的音节数，或强制生成与给定上下文相关的单词等。

    Despite rapid advancement in the field of Constrained Natural Language Generation, little time has been spent on exploring the potential of language models which have had their vocabularies lexically, semantically, and/or phonetically constrained. We find that most language models generate compelling text even under significant constraints. We present a simple and universally applicable technique for modifying the output of a language model by compositionally applying filter functions to the language models vocabulary before a unit of text is generated. This approach is plug-and-play and requires no modification to the model. To showcase the value of this technique, we present an easy to use AI writing assistant called Constrained Text Generation Studio (CTGS). CTGS allows users to generate or choose from text with any combination of a wide variety of constraints, such as banning a particular letter, forcing the generated words to have a certain number of syllables, and/or forcing the 
    
[^28]: 置信度校准的集合式密集短语检索

    Confidence-Calibrated Ensemble Dense Phrase Retrieval. (arXiv:2306.15917v1 [cs.CL])

    [http://arxiv.org/abs/2306.15917](http://arxiv.org/abs/2306.15917)

    本文研究了如何优化基于Transformer的密集语段检索（DPR）算法，使用置信度校准的集合预测方法取得了最先进的结果，并发现不同领域的最优粒度也有所差异。

    

    本文中，我们考虑了不需要进一步预训练的基于Transformer的密集语段检索（DPR）算法（由Karpukhin等人于2020年开发）的优化程度。我们的方法包括两个关键洞察：我们在不同短语长度（例如一句和五句）上应用DPR上下文编码器，并对所有这些不同分割的结果进行置信度校准的集合预测。这种相对详尽的方法在Google NQ和SQuAD等基准数据集上取得了最先进的结果。我们还将我们的方法应用于特定领域的数据集，结果表明不同的颗粒度对于不同的领域是最优的。

    In this paper, we consider the extent to which the transformer-based Dense Passage Retrieval (DPR) algorithm, developed by (Karpukhin et. al. 2020), can be optimized without further pre-training. Our method involves two particular insights: we apply the DPR context encoder at various phrase lengths (e.g. one-sentence versus five-sentence segments), and we take a confidence-calibrated ensemble prediction over all of these different segmentations. This somewhat exhaustive approach achieves start-of-the-art results on benchmark datasets such as Google NQ and SQuAD. We also apply our method to domain-specific datasets, and the results suggest how different granularities are optimal for different domains
    
[^29]: 大型语言模型作为属性化训练数据生成器：多样性和偏差的故事

    Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias. (arXiv:2306.15895v1 [cs.CL])

    [http://arxiv.org/abs/2306.15895](http://arxiv.org/abs/2306.15895)

    本论文研究了大型语言模型作为属性化训练数据生成器的应用。通过使用具有多样性属性的提示，我们能够生成多样化且归因的数据。研究表明，在高基数和多样领域的数据集中，使用属性化提示对生成模型性能有积极影响。此外，论文还展示了关于偏差、多样性和效率的全面实证研究结果，并得出了三个关键观察：系统性偏差存在于生成数据中，多样性和效率之间存在权衡，属性化训练数据生成可以改善模型性能。

    

    近期大型语言模型(LLMs)被广泛应用于各种自然语言处理(NLP)任务的训练数据生成。尽管之前的研究探索了使用生成数据进行模型训练的不同方法，但它们通常依赖于简单的类别条件提示，这可能限制了生成数据的多样性，并且继承了LLM的系统性偏差。因此，我们研究了使用具有多样属性的提示(例如指定长度和风格等属性)进行训练数据生成，这有潜力产生多样和归因的生成数据。我们的研究关注具有高基数和多样领域的数据集，在这方面，我们证明了属性化提示在生成模型性能方面优于简单的类别条件提示。此外，我们还展示了一项包括偏差、多样性和效率等关键方面的全面实证研究，并强调了三个关键观察：首先，系统性偏差在生成数据中存在；其次，多样性和效率之间存在权衡；最后，进行属性化训练数据生成可以改善模型性能。

    Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, sy
    
[^30]: 从人际交互面临场景学习看符号的出现：语义知识的组合性出现机制的研究

    Symbol emergence as interpersonal cross-situational learning: the emergence of lexical knowledge with combinatoriality. (arXiv:2306.15837v1 [cs.CL])

    [http://arxiv.org/abs/2306.15837](http://arxiv.org/abs/2306.15837)

    本研究提出了一个计算模型，通过交互学习和命名游戏，实现了代理之间通过词序列交流的方式，从而促进了具有组合性的语义知识的出现。

    

    本文介绍了一个计算模型，通过Metropolis-Hastings命名游戏和交互学习，使代理在符号出现系统中能够通过组合性获得词汇知识。许多计算模型已被提出用于研究紧急交流中的组合性和认知与发展机器人中的符号出现。然而，现有模型没有充分解决基于感知运动信息的类别形成和通过单一综合模型中的词序列交流的符号交流。我们提出的模型通过使用多模态感知运动信息进行类别形成，并通过代理间的词序列交流实现符号的出现，从而促进了具有组合性的语义知识的出现。此外，该模型使得代理能够通过结合与词相关的信息预测未观察到的情况的感知运动信息。

    We present a computational model for a symbol emergence system that enables the emergence of lexical knowledge with combinatoriality among agents through a Metropolis-Hastings naming game and cross-situational learning. Many computational models have been proposed to investigate combinatoriality in emergent communication and symbol emergence in cognitive and developmental robotics. However, existing models do not sufficiently address category formation based on sensory-motor information and semiotic communication through the exchange of word sequences within a single integrated model. Our proposed model facilitates the emergence of lexical knowledge with combinatoriality by performing category formation using multimodal sensory-motor information and enabling semiotic communication through the exchange of word sequences among agents in a unified model. Furthermore, the model enables an agent to predict sensory-motor information for unobserved situations by combining words associated wit
    
[^31]: MAT: 对微调中对抗训练的混合策略游戏

    MAT: Mixed-Strategy Game of Adversarial Training in Fine-tuning. (arXiv:2306.15826v1 [cs.CL])

    [http://arxiv.org/abs/2306.15826](http://arxiv.org/abs/2306.15826)

    本论文提出了一种新颖的混合策略对抗训练算法（MAT），通过在细调阶段加入对抗训练，显著提高了模型的泛化能力和鲁棒性，通过采样方法建立了MAT。实验证明，MAT在大规模预训练模型上的性能明显优于其他方法。

    

    细调大规模预训练语言模型已被证明在各种自然语言处理任务中有效。之前的研究表明，在细调阶段加入对抗训练可以显著提高模型的泛化能力和鲁棒性。然而，从博弈论的角度来看，这种对抗训练的应用对应于纯策略游戏，其在策略范围方面存在固有的限制，因此仍有改进空间。为了推动性能边界，我们提出了一种新颖的混合策略对抗训练算法（MAT）。在方法上，我们使用熵镜像下降推导了对抗训练的混合策略游戏的纳什均衡，通过采样方法建立了MAT。为了验证MAT的有效性，我们在BERT和RoBERTa等大规模预训练模型上进行了大量基准实验。MAT在性能上显著优于其他方法。

    Fine-tuning large-scale pre-trained language models has been demonstrated effective for various natural language processing (NLP) tasks. Previous studies have established that incorporating adversarial training during the fine-tuning stage can significantly enhance model generalization and robustness. However, from the perspective of game theory, such utilizations of adversarial training correspond to pure-strategy games, which are inherently limited in terms of the scope of their strategies, thereby still having room for improvement. In order to push the performance boundaries, we propose a novel Mixed-strategy Adversarial Training algorithm (MAT). Methodologically, we derive the Nash equilibrium of a mixed-strategy game for adversarial training using Entropy Mirror Descent to establish MAT by sampling method. To verify the effectiveness of MAT, we conducted extensive benchmark experiments on large-scale pre-trained models, such as BERT and RoBERTa. MAT significantly outperforms the s
    
[^32]: 基于置信度的端到端语音识别模型的集成

    Confidence-based Ensembles of End-to-End Speech Recognition Models. (arXiv:2306.15824v1 [eess.AS])

    [http://arxiv.org/abs/2306.15824](http://arxiv.org/abs/2306.15824)

    本文提出了一种基于置信度的端到端语音识别模型的集成方法，通过只使用最有信心的模型的输出来提高性能，并通过两个应用程序的实验证明了方法的有效性。

    

    每年端到端语音识别模型的数量不断增长。这些模型经常被调整到新领域或语言，导致专家系统大量涌现，在目标数据上取得了很好的结果，但在自己专业领域之外的性能却相对较差。我们通过基于置信度的集成来探索这些专家模型的组合：在模型中只使用最有信心的模型的输出。我们假设模型的目标数据除了一个小的验证集外是不可用的。我们通过两个应用程序展示了我们方法的有效性。首先，我们展示了一个由5个单语模型组成的基于置信度的集成优于通过专用语言识别模块进行模型选择的系统。其次，我们证明了可以将基础模型和调整模型相结合，在原始和目标数据上取得强大的结果。我们在多个数据集和模型架构上验证了所有结果。

    The number of end-to-end speech recognition models grows every year. These models are often adapted to new domains or languages resulting in a proliferation of expert systems that achieve great results on target data, while generally showing inferior performance outside of their domain of expertise. We explore combination of such experts via confidence-based ensembles: ensembles of models where only the output of the most-confident model is used. We assume that models' target data is not available except for a small validation set. We demonstrate effectiveness of our approach with two applications. First, we show that a confidence-based ensemble of 5 monolingual models outperforms a system where model selection is performed via a dedicated language identification block. Second, we demonstrate that it is possible to combine base and adapted models to achieve strong results on both original and target data. We validate all our results on multiple datasets and model architectures.
    
[^33]: FLuRKA: 快速融合低秩和核注意力

    FLuRKA: Fast fused Low-Rank & Kernel Attention. (arXiv:2306.15799v1 [cs.LG])

    [http://arxiv.org/abs/2306.15799](http://arxiv.org/abs/2306.15799)

    FLuRKA是一种融合低秩和核注意力的新型transformer类别，相较于传统的近似技术，在运行时间性能和质量方面都表现出显著的提升。

    

    自从transformer结构的提出以来，许多高效的近似自注意力技术已经变得流行起来。其中两种流行的技术类别是低秩和核方法。我们观察到这两种方法的优势相互补充，利用这些协同效应来融合低秩和核方法，产生了一种新的transformer类别：FLuRKA（快速低秩和核注意力）。FLuRKA相对于这些近似技术提供了可观的性能提升，并且具有高质量。我们在理论和实证方面评估了FLuRKA的运行时间性能和质量。我们的运行时间分析提供了多种参数配置，在这些配置下，FLuRKA具有加速效果；我们的准确性分析限定了FLuRKA相对于全注意力的误差。我们实例化了三种FLuRKA变体，相对于低秩和核方法分别实现了高达3.3倍和1.7倍的经验加速。这意味着更快的运行时间，而且质量仍然保持不错。

    Many efficient approximate self-attention techniques have become prevalent since the inception of the transformer architecture. Two popular classes of these techniques are low-rank and kernel methods. Each of these methods has its own strengths. We observe these strengths synergistically complement each other and exploit these synergies to fuse low-rank and kernel methods, producing a new class of transformers: FLuRKA (Fast Low-Rank and Kernel Attention). FLuRKA provide sizable performance gains over these approximate techniques and are of high quality. We theoretically and empirically evaluate both the runtime performance and quality of FLuRKA. Our runtime analysis posits a variety of parameter configurations where FLuRKA exhibit speedups and our accuracy analysis bounds the error of FLuRKA with respect to full-attention. We instantiate three FLuRKA variants which experience empirical speedups of up to 3.3x and 1.7x over low-rank and kernel methods respectively. This translates to spe
    
[^34]: 在巴西葡萄牙语的语法错误修正方面评估GPT-3.5和GPT-4

    Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese. (arXiv:2306.15788v1 [cs.CL])

    [http://arxiv.org/abs/2306.15788](http://arxiv.org/abs/2306.15788)

    该研究评估了GPT-3.5和GPT-4在巴西葡萄牙语语法错误修正方面的有效性，结果显示虽然GPT-4的召回率较高，但语言模型倾向于过度修正。

    

    我们调查了GPT-3.5和GPT-4这两个大型语言模型在巴西葡萄牙语的语法错误修正（GEC）工具中的有效性，并将其性能与Microsoft Word和Google Docs进行了比较。我们引入了一个针对巴西葡萄牙语的GEC数据集，包括四个类别：语法、拼写、互联网和快速输入。我们的结果显示，尽管GPT-4的召回率比其他方法高，但语言模型倾向于具有较低的精确度，导致过度修正。这项研究展示了语言模型作为巴西葡萄牙语实际GEC工具的潜力，并鼓励进一步探索语言模型在非英语语言和其他教育环境中的应用。

    We investigate the effectiveness of GPT-3.5 and GPT-4, two large language models, as Grammatical Error Correction (GEC) tools for Brazilian Portuguese and compare their performance against Microsoft Word and Google Docs. We introduce a GEC dataset for Brazilian Portuguese with four categories: Grammar, Spelling, Internet, and Fast typing. Our results show that while GPT-4 has higher recall than other methods, LLMs tend to have lower precision, leading to overcorrection. This study demonstrates the potential of LLMs as practical GEC tools for Brazilian Portuguese and encourages further exploration of LLMs for non-English languages and other educational settings.
    
[^35]: 人类中心生成式人工智能的下一步：技术视角

    Next Steps for Human-Centered Generative AI: A Technical Perspective. (arXiv:2306.15774v1 [cs.HC])

    [http://arxiv.org/abs/2306.15774](http://arxiv.org/abs/2306.15774)

    这项研究从技术角度定义和提出了人类中心生成式人工智能(HGAI)的下一步工作，包括与人类价值观对齐、适应人类的意图表达和增强人类在协作工作流中的能力。这个工作的目标是吸引跨学科研究团队对HGAI的新兴想法进行讨论，并保持未来工作景观的整体连贯性。

    

    通过反复跨学科讨论，我们从技术角度为人类中心生成式人工智能(HGAI)定义和提出了下一步的工作。我们贡献了一个路线图，概述了生成式人工智能在三个层面上的未来方向：与人类价值观对齐；适应人类的意图表达；增强人类在协作工作流中的能力。该路线图旨在吸引跨学科研究团队对HGAI的新兴想法进行全面的讨论，同时保持未来工作景观的整体连贯性。

    Through iterative, cross-disciplinary discussions, we define and propose next-steps for Human-centered Generative AI (HGAI) from a technical perspective. We contribute a roadmap that lays out future directions of Generative AI spanning three levels: Aligning with human values; Accommodating humans' expression of intents; and Augmenting humans' abilities in a collaborative workflow. This roadmap intends to draw interdisciplinary research teams to a comprehensive list of emergent ideas in HGAI, identifying their interested topics while maintaining a coherent big picture of the future work landscape.
    
[^36]: 大型语言模型作为标注者：以最小成本增强NLP模型的泛化能力

    Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost. (arXiv:2306.15766v1 [cs.CL])

    [http://arxiv.org/abs/2306.15766](http://arxiv.org/abs/2306.15766)

    本研究提出了一种利用大型语言模型进行标注的方法，以最小成本提升NLP模型的泛化能力。通过差异性采样策略，我们证明了这种方法在分类和排序任务上能够显著改善模型效果。

    

    最先进的监督式NLP模型能够达到很高的准确度，但对于低数据领域的输入也容易出现失败，例如训练数据中未包含的领域。作为收集特定领域的真实标签的近似方法，我们研究了使用大型语言模型(LLM)对输入进行标注并提升NLP模型的泛化能力。具体而言，给定LLM标注的预算，我们提出了一种算法来选择最具信息量的输入进行标注和重新训练NLP模型。我们发现流行的基于不确定性采样的主动学习策略效果不佳。相反，我们提出了一种基于基础模型和微调NLP模型之间预测分数差异的采样策略，利用了大多数NLP模型都是从基础模型微调而来的事实。分类(语义相似度)和排序(语义搜索)任务的实验证明我们的采样策略带来了显著的收益。

    State-of-the-art supervised NLP models achieve high accuracy but are also susceptible to failures on inputs from low-data regimes, such as domains that are not represented in training data. As an approximation to collecting ground-truth labels for the specific domain, we study the use of large language models (LLMs) for annotating inputs and improving the generalization of NLP models. Specifically, given a budget for LLM annotations, we present an algorithm for sampling the most informative inputs to annotate and retrain the NLP model. We find that popular active learning strategies such as uncertainty-based sampling do not work well. Instead, we propose a sampling strategy based on the difference in prediction scores between the base model and the finetuned NLP model, utilizing the fact that most NLP models are finetuned from a base model. Experiments with classification (semantic similarity) and ranking (semantic search) tasks show that our sampling strategy leads to significant gain
    
[^37]: 《在一个对女性厌恶的incels论坛中的身份建构》

    Identity Construction in a Misogynist Incels Forum. (arXiv:2306.15745v1 [cs.CL])

    [http://arxiv.org/abs/2306.15745](http://arxiv.org/abs/2306.15745)

    本研究使用定量文本和网络分析方法，研究了最大的黑洞incels论坛如何讨论身份群体。研究发现该社区产生了许多新的身份术语，存在物质主义的意识形态。对此我们讨论了对自动化 misogynist hate speech 检测研究的影响。

    

    本文使用定量文本和网络分析方法，研究了incels.is，即最大的黑洞incels论坛如何讨论身份群体。我们发现该社区产生了许多新的身份术语，尽管女性的术语最常见，但其他少数群体的提及也在增加。对身份群体的关联分析表明，这个社区存在着物质主义的意识形态，其中身体外貌、性别和种族等决定了人的价值。我们讨论了对自动化 misogynist hate speech 检测研究的影响。

    Online communities of involuntary celibates (incels) are a prominent source of misogynist hate speech. In this paper, we use quantitative text and network analysis approaches to examine how identity groups are discussed on incels.is, the largest black-pilled incels forum. We find that this community produces a wide range of novel identity terms and, while terms for women are most common, mentions of other minoritized identities are increasing. An analysis of the associations made with identity groups suggests an essentialist ideology where physical appearance, as well as gender and racial hierarchies, determine human value. We discuss implications for research into automated misogynist hate speech detection.
    
[^38]: 通过检测和匹配进行生物医学实体识别

    Biomedical Entity Recognition by Detection and Matching. (arXiv:2306.15736v1 [cs.CL])

    [http://arxiv.org/abs/2306.15736](http://arxiv.org/abs/2306.15736)

    本研究提出了一种名为DMNER的新型生物医学实体识别框架，通过检测实体边界和匹配生物医学实体来提高NER的性能。在有监督NER、远程监督NER和多数据集合并训练NER等场景中，DMNER都展示了良好的适用性。

    

    生物医学命名实体识别（BNER）是许多生物医学文本挖掘任务的基础。与一般的NER不同，BNER需要全面掌握领域知识，而且在训练数据之外融入外部知识是一个重大挑战。本研究提出了一个新的BNER框架，称为DMNER。通过利用已有的实体表示模型SAPBERT，我们将BNER作为一个两步骤的过程来处理：实体边界检测和生物医学实体匹配。DMNER在多种NER场景中具有适用性：1）在有监督NER中，我们观察到DMNER有效纠正了基线NER模型的输出，从而进一步提高了性能。2）在远程监督NER中，将MRC和AutoNER作为跨度边界检测器相结合，使DMNER能够实现令人满意的结果。3）对于通过合并多个数据集进行NER训练，我们采用了与DS-NER类似的框架，但还额外利用ChatGPT来获得高质量的训练短语。

    Biomedical named entity recognition (BNER) serves as the foundation for numerous biomedical text mining tasks. Unlike general NER, BNER require a comprehensive grasp of the domain, and incorporating external knowledge beyond training data poses a significant challenge. In this study, we propose a novel BNER framework called DMNER. By leveraging existing entity representation models SAPBERT, we tackle BNER as a two-step process: entity boundary detection and biomedical entity matching. DMNER exhibits applicability across multiple NER scenarios: 1) In supervised NER, we observe that DMNER effectively rectifies the output of baseline NER models, thereby further enhancing performance. 2) In distantly supervised NER, combining MRC and AutoNER as span boundary detectors enables DMNER to achieve satisfactory results. 3) For training NER by merging multiple datasets, we adopt a framework similar to DS-NER but additionally leverage ChatGPT to obtain high-quality phrases in the training. Through
    
[^39]: 一个弱监督分类器和白人至上主义语言数据集

    A Weakly Supervised Classifier and Dataset of White Supremacist Language. (arXiv:2306.15732v1 [cs.CL])

    [http://arxiv.org/abs/2306.15732](http://arxiv.org/abs/2306.15732)

    这篇论文提出了一个用于检测白人至上主义极端主义语言的数据集和分类器，通过使用大量文本数据集以及包括中性和反种族主义数据，在不同领域中进行弱监督训练，提高了对新领域的泛化性能，并通过使用反种族主义文本作为反例来减轻偏见。

    

    我们提出了一个用于检测白人至上主义极端主义语言的数据集和分类器，这是在线仇恨言论中日益增长的问题。我们的弱监督分类器是在从明确的白人至上主义领域配对中性和反种族主义数据的大型文本数据集上训练的。我们证明了这种方法提高了对新领域的泛化性能。将反种族主义文本作为白人至上主义语言的反例可以减轻偏见。

    We present a dataset and classifier for detecting the language of white supremacist extremism, a growing issue in online hate speech. Our weakly supervised classifier is trained on large datasets of text from explicitly white supremacist domains paired with neutral and anti-racist data from similar domains. We demonstrate that this approach improves generalization performance to new domains. Incorporating anti-racist texts as counterexamples to white supremacist language mitigates bias.
    
[^40]: 关于用于高效的无数据对抗检测的通用对抗扰动

    On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection. (arXiv:2306.15705v1 [cs.CL])

    [http://arxiv.org/abs/2306.15705](http://arxiv.org/abs/2306.15705)

    本论文提出了一个用于高效的无数据对抗检测的通用对抗扰动方法，通过发现对抗样本与高维输入中的特定向量的关系，计算出通用对抗扰动（UAPs）。基于此，提出了一个无数据对抗检测框架，通过对UAPs对正常样本和对抗样本的反应产生不同的结果，从而实现了在各种文本分类任务上具有竞争力的检测性能。具体实验结果显示，该方法维持了与正常推断相等的时间消耗。

    

    检测经过精心设计的对抗样本以欺骗模型是确保社交安全应用的关键步骤。然而，现有的对抗检测方法需要访问足够的训练数据，这引发了与隐私泄露和通用性相关的明显关注。在这项工作中，我们验证了攻击算法产生的对抗样本与高维输入中的特定向量密切相关。这些向量称为通用对抗扰动（UAPs），可以在没有原始训练数据的情况下计算得出。基于这一发现，我们提出了一个无数据对抗检测框架，该框架通过对通用对抗扰动对正常样本和对抗样本产生不同的反应。实验结果显示，我们的方法在各种文本分类任务上具有竞争力的检测性能，并且维持了与正常推断相等的时间消耗。

    Detecting adversarial samples that are carefully crafted to fool the model is a critical step to socially-secure applications. However, existing adversarial detection methods require access to sufficient training data, which brings noteworthy concerns regarding privacy leakage and generalizability. In this work, we validate that the adversarial sample generated by attack algorithms is strongly related to a specific vector in the high-dimensional inputs. Such vectors, namely UAPs (Universal Adversarial Perturbations), can be calculated without original training data. Based on this discovery, we propose a data-agnostic adversarial detection framework, which induces different responses between normal and adversarial samples to UAPs. Experimental results show that our method achieves competitive detection performance on various text classification tasks, and maintains an equivalent time consumption to normal inference.
    
[^41]: Voicebox：大规模的多语言通用语音生成模型

    Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale. (arXiv:2306.15687v1 [eess.AS])

    [http://arxiv.org/abs/2306.15687](http://arxiv.org/abs/2306.15687)

    Voicebox是一种大规模的多语言通用语音生成模型，通过使用非自回归的流匹配模型，在文本和音频上下文条件下进行训练，可以实现零样本跨语言文本到语音合成，噪声去除，内容编辑，风格转换和多样化的样本生成等多种任务。

    

    大规模生成模型，如GPT和DALL-E已经改变了自然语言处理和计算机视觉研究的方式。这些模型不仅可以生成高质量的文本或图像输出，而且还是通用的，可以解决未被明确教授的任务。相比之下，语音生成模型在规模和任务通用化方面仍然比较原始。在本文中，我们介绍了Voicebox，这是最多功能的面向规模的文本引导生成模型。Voicebox是一个非自回归的流匹配模型，通过在音频上下文和文本条件下进行训练，用50,000小时的未经过滤或增强的语音进行填充。与GPT类似，Voicebox可以通过上下文学习执行多种不同的任务，但更加灵活，因为它还可以对未来的上下文进行条件约束。Voicebox可以用于单语或跨语言零样本的文本到语音合成，噪声去除，内容编辑，风格转换和多样化的样本生成。特别是，Voicebox

    Large-scale generative models such as GPT and DALL-E have revolutionized natural language processing and computer vision research. These models not only generate high fidelity text or image outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are neither filtered nor enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox 
    
[^42]: Master-ASR: 实现ASR的多语言可扩展性和低资源适应性的模块化学习方法

    Master-ASR: Achieving Multilingual Scalability and Low-Resource Adaptation in ASR with Modular Learning. (arXiv:2306.15686v1 [eess.AS])

    [http://arxiv.org/abs/2306.15686](http://arxiv.org/abs/2306.15686)

    Master-ASR是一个基于模块化学习的ASR框架，通过共享模块来实现多语言可扩展性和低资源适应能力，解决了ASR面临的可扩展性和低资源适应的挑战。

    

    尽管自动语音识别(ASR)最近取得了令人印象深刻的性能，但我们观察到两个主要挑战阻碍其更广泛的应用：(1)难以引入可扩展性模型，支持有限的训练、推理和存储开销的更多语言；(2)低资源适应能力，能够在避免过拟合和灾难性遗忘问题的同时实现有效的低资源适应。受到最近的研究结果的启发，我们假设我们可以通过跨语言广泛共享的模块来解决上述挑战。为此，我们提出了一种ASR框架，称为\METHODNS，\textit{首次}同时实现了强大的多语言可扩展性和低资源适应能力，这得益于它的模块化即装配策略。具体地，\METHOD学习了一小组通用的子模块，并自适应地将它们组装到不同的语言中，以降低多语言开销并实现有效的适应性。

    Despite the impressive performance recently achieved by automatic speech recognition (ASR), we observe two primary challenges that hinder its broader applications: (1) The difficulty of introducing scalability into the model to support more languages with limited training, inference, and storage overhead; (2) The low-resource adaptation ability that enables effective low-resource adaptation while avoiding over-fitting and catastrophic forgetting issues. Inspired by recent findings, we hypothesize that we can address the above challenges with modules widely shared across languages. To this end, we propose an ASR framework, dubbed \METHODNS, that, \textit{for the first time}, simultaneously achieves strong multilingual scalability and low-resource adaptation ability thanks to its modularize-then-assemble strategy. Specifically, \METHOD learns a small set of generalizable sub-modules and adaptively assembles them for different languages to reduce the multilingual overhead and enable effec
    
[^43]: 在GPU解码器中实现上下文偏置用于在线ASR

    Implementing contextual biasing in GPU decoder for online ASR. (arXiv:2306.15685v1 [eess.AS])

    [http://arxiv.org/abs/2306.15685](http://arxiv.org/abs/2306.15685)

    本论文提出了一种在实时GPU解码中集成上下文偏置的方法，以提高ASR预测的准确性，并允许动态上下文切换。

    

    GPU解码显著加速了ASR预测的输出。虽然已经在在线ASR解码中使用了GPU，但是尚未对GPU上的后处理和重新评分进行适当的研究。利用可用的上下文信息进行重新评分可以大大提高ASR预测的准确性。之前的研究已经证明了在离线和在线CPU场景中，在解码和对语言模型（LM）权重进行偏置方面使用格子重新评分的可行性。在实时GPU解码中，会生成部分识别假设而不生成格子，这使得偏置的实现更加复杂。本文提出并描述了一种在实时GPU解码中集成上下文偏置的方法，同时利用了标准的Kaldi GPU解码器。除了对部分ASR预测进行偏置外，我们的方法还允许动态上下文切换，直接在GPU上对每个语音段进行灵活的重新评分。代码已经公开发布并使用了开源测试数据集进行了测试。

    GPU decoding significantly accelerates the output of ASR predictions. While GPUs are already being used for online ASR decoding, post-processing and rescoring on GPUs have not been properly investigated yet. Rescoring with available contextual information can considerably improve ASR predictions. Previous studies have proven the viability of lattice rescoring in decoding and biasing language model (LM) weights in offline and online CPU scenarios. In real-time GPU decoding, partial recognition hypotheses are produced without lattice generation, which makes the implementation of biasing more complex. The paper proposes and describes an approach to integrate contextual biasing in real-time GPU decoding while exploiting the standard Kaldi GPU decoder. Besides the biasing of partial ASR predictions, our approach also permits dynamic context switching allowing a flexible rescoring per each speech segment directly on GPU. The code is publicly released and tested with open-sourced test sets.
    
[^44]: 自动注释法语书面叙述中的直接言语

    Automatic Annotation of Direct Speech in Written French Narratives. (arXiv:2306.15634v1 [cs.CL])

    [http://arxiv.org/abs/2306.15634](http://arxiv.org/abs/2306.15634)

    本论文旨在为法语中的自动注释直接言语（AADS）创建一个统一的框架。研究采用最大的法语叙述数据集进行了广泛评估，结果表明该任务仍需大量努力，并强调了不同基线模型的特点。

    

    自动注释法语书面文本中的直接言语（AADS）经常在计算机叙述理解中使用。已经研究了基于规则或深度神经网络的方法，特别是针对英语或德语。然而，对于我们的目标语言法语，很少有相关研究。我们的目标是创建一个统一的框架，用于设计和评估法语中的AADS模型。为此，我们整合了迄今为止标注有每个单词的最大的法语叙述数据集；我们针对序列标注或其他语言的AADS适应了各种基线模型；并且我们设计并进行了广泛评估，重点是泛化能力。结果表明，这个任务仍然需要大量的工作，并强调了每个基线模型的特点。尽管这个框架可以改进，但它是鼓励更多关于这个主题的研究的一步。

    The automatic annotation of direct speech (AADS) in written text has been often used in computational narrative understanding. Methods based on either rules or deep neural networks have been explored, in particular for English or German languages. Yet, for French, our target language, not many works exist. Our goal is to create a unified framework to design and evaluate AADS models in French. For this, we consolidated the largest-to-date French narrative dataset annotated with DS per word; we adapted various baselines for sequence labelling or from AADS in other languages; and we designed and conducted an extensive evaluation focused on generalisation. Results show that the task still requires substantial efforts and emphasise characteristics of each baseline. Although this framework could be improved, it is a step further to encourage more research on the topic.
    
[^45]: 通过位置插值扩展大型语言模型的上下文窗口

    Extending Context Window of Large Language Models via Positional Interpolation. (arXiv:2306.15595v1 [cs.CL])

    [http://arxiv.org/abs/2306.15595](http://arxiv.org/abs/2306.15595)

    通过位置插值方法，我们可以在最小微调的情况下将RoPE-based预训练语言模型的上下文窗口扩展到最多32768，并在多个任务上获得强有力的实证结果。通过线性降低输入位置索引的大小，我们保持了扩展模型在原始上下文窗口内任务的质量。

    

    我们提出了一种位置插值（PI）方法，可以在最小微调的情况下将RoPE-based预训练语言模型（如LLaMA模型）的上下文窗口大小扩展到最多32768，并且在需要长上下文的各种任务（包括密钥检索、语言建模和长篇文档摘要等）上展现出良好的实证结果。同时，通过位置插值扩展的模型在原始上下文窗口内的任务中相对保持良好的质量。为了实现这一目标，位置插值线性地降低输入位置索引的大小，以匹配原始的上下文窗口大小，而不是超过训练时上下文长度，这可能会导致严重的高注意力分数，完全破坏自注意机制。我们的理论研究表明，插值的上界至少是推断的上界的$\sim 600 \times$要小，进一步证明了其稳定性。

    We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least $\sim 600 \times$ smaller than that of extrapolation, further demonstrating its stability. Models extend
    
[^46]: 3D-Speaker：用于语音表示解缠的大规模多设备、多距离和多方言语料库

    3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement. (arXiv:2306.15354v1 [cs.CL])

    [http://arxiv.org/abs/2306.15354](http://arxiv.org/abs/2306.15354)

    3D-Speaker是一个大规模的多设备、多距离和多方言语音语料库，用于研究语音表示解缠。它包含了10,000多个说话人的数据，可以用来评估大型通用语音模型和探索域外学习和自监督学习方法。

    

    在语音社区中，分离语音话语中的不相关信息是一个关键的研究课题。不同的语音相关任务专注于提取不同的语音表示，同时最小化其他不相关信息的影响。我们提出了一个大规模的语音语料库，以促进语音表示解缠的研究。3D-Speaker包含超过10,000个说话人，每个说话人同时由多个设备录制，在不同的距离上，并且一些说话人会讲多种方言。多维音频数据的受控组合产生了一个多样的混合语音表示纠缠矩阵，从而激发出解开它们的有趣方法。3D-Speaker的多领域性质还使其成为评估大型通用语音模型和实验域外学习和自监督学习方法的合适资源。

    Disentangling uncorrelated information in speech utterances is a crucial research topic within speech community. Different speech-related tasks focus on extracting distinct speech representations while minimizing the affects of other uncorrelated information. We present a large-scale speech corpus to facilitate the research of speech representation disentanglement. 3D-Speaker contains over 10,000 speakers, each of whom are simultaneously recorded by multiple Devices, locating at different Distances, and some speakers are speaking multiple Dialects. The controlled combinations of multi-dimensional audio data yield a matrix of a diverse blend of speech representation entanglement, thereby motivating intriguing methods to untangle them. The multi-domain nature of 3D-Speaker also makes it a suitable resource to evaluate large universal speech models and experiment methods of out-of-domain learning and self-supervised learning. https://3dspeaker.github.io/
    
[^47]: MindDial: 带有心智模拟的信念动态跟踪用于场景化神经对话生成

    MindDial: Belief Dynamics Tracking with Theory-of-Mind Modeling for Situated Neural Dialogue Generation. (arXiv:2306.15253v1 [cs.CL])

    [http://arxiv.org/abs/2306.15253](http://arxiv.org/abs/2306.15253)

    MindDial是一个使用心智模拟进行信念动态跟踪的对话生成框架，可以在场景化环境中生成自由对话来协商共识。

    

    人类在交流中自由表达意义或共识的同时进行对话。尽管大型生成语言模型具有令人印象深刻的对话能力，但它们并未考虑到共享的场景环境中个体的上下文理解差异。本文提出了MindDial，一种新颖的对话框架，可以生成场景化的自由对话来协商共识。我们设计了一个明确的心智模块，可以追踪三个层次的信念，即说话者的信念、说话者对听众信念的预测以及基于前两者之间的共同信念。然后，说话行为分类头将决定是否继续对话、结束此轮对话或采取与任务相关的行动。我们使用了一个共识对齐的数据集MutualFriend，增加了信念动态注释，目标是根据两个代理之间的自由对话找到一个共同的朋友。实验证明，我们的模型在心智状态建模方面取得了良好的效果。

    Humans talk in free-form while negotiating the expressed meanings or common ground. Despite the impressive conversational abilities of the large generative language models, they do not consider the individual differences in contextual understanding in a shared situated environment. In this work, we propose MindDial, a novel conversational framework that can generate situated free-form responses to negotiate common ground. We design an explicit mind module that can track three-level beliefs -- the speaker's belief, the speaker's prediction of the listener's belief, and the common belief based on the gap between the first two. Then the speaking act classification head will decide to continue to talk, end this turn, or take task-related action. We augment a common ground alignment dataset MutualFriend with belief dynamics annotation, of which the goal is to find a single mutual friend based on the free chat between two agents. Experiments show that our model with mental state modeling can
    
[^48]: 研究BERT在评论理解中的跨域行为

    Investigating Cross-Domain Behaviors of BERT in Review Understanding. (arXiv:2306.15123v1 [cs.CL])

    [http://arxiv.org/abs/2306.15123](http://arxiv.org/abs/2306.15123)

    该研究调查了在产品评论理解的各种任务中，BERT模型在不同域上的跨域行为。尽管单域模型在对应域上略有提高，多域模型在评估多域数据时表现更好，并且在平均测试中也更优。尽管单域模型微调可以提高准确性，但会增加计算资源消耗。

    

    评论分数预测需要理解评论文本，这是自然语言处理的一个关键实际应用。由于产品评论中的文本领域不同，常见做法是在不同领域的评论上对BERT模型进行微调。然而，目前还没有对BERT模型在产品评论理解的各种任务中的跨域行为进行实证研究。在本项目中，我们研究了在单域和多域亚马逊评论数据上微调的文本分类BERT模型。通过我们的发现，尽管单域模型在对应域上的性能略有提高，但在多域数据上评估时，多域模型优于单域模型，特别是在单域模型未进行微调的单域数据上，以及在考虑所有测试时的平均性能。虽然通过单域模型微调可以略微提高准确性，但计算资源也会增加。

    Review score prediction requires review text understanding, a critical real-world application of natural language processing. Due to dissimilar text domains in product reviews, a common practice is fine-tuning BERT models upon reviews of differing domains. However, there has not yet been an empirical study of cross-domain behaviors of BERT models in the various tasks of product review understanding. In this project, we investigate text classification BERT models fine-tuned on single-domain and multi-domain Amazon review data. In our findings, though single-domain models achieved marginally improved performance on their corresponding domain compared to multi-domain models, multi-domain models outperformed single-domain models when evaluated on multi-domain data, single-domain data the single-domain model was not fine-tuned on, and on average when considering all tests. Though slight increases in accuracy can be achieved through single-domain model fine-tuning, computational resources an
    
[^49]: 揭示印度选举期间的政治仇恨言论：一个新的低资源数据集和基准 （arXiv:2306.14764v2 [cs.CL] UPDATED）

    Uncovering Political Hate Speech During Indian Election Campaign: A New Low-Resource Dataset and Baselines. (arXiv:2306.14764v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.14764](http://arxiv.org/abs/2306.14764)

    本论文介绍了一个新的低资源数据集IEHate，其中包含11,457条与印度州立选举活动相关的印地语推文。通过对数据集的分析，研究者们揭示了政治交流中仇恨言论的普遍性和不同形式的憎恨语言，并使用多种算法对数据集进行了基准测试。实验结果表明，需要更高级的技术来提高低资源语言中仇恨言论的检测性能。

    

    政治演讲中仇恨言论的检测是一个关键问题，而在低资源语言中更具挑战性。为了解决这个问题，我们介绍了一个名为IEHate的新数据集，其中包含11,457条手动标注的与2021年11月1日至2022年3月9日期间印度州立选举活动相关的印地语推文。我们对数据集进行了详细分析，重点关注政治交流中仇恨言论的普遍性和使用的不同形式的憎恨语言。此外，我们使用一系列机器学习、深度学习和基于transformer的算法对数据集进行了基准测试。我们的实验表明，这些模型的性能可以进一步提高，强调了在低资源语言中检测仇恨言论需要更高级的技术。特别是，人工评估相对较高的得分强调了利用人工和自动化方法的重要性，以实现有效的仇恨言论检测。

    The detection of hate speech in political discourse is a critical issue, and this becomes even more challenging in low-resource languages. To address this issue, we introduce a new dataset named IEHate, which contains 11,457 manually annotated Hindi tweets related to the Indian Assembly Election Campaign from November 1, 2021, to March 9, 2022. We performed a detailed analysis of the dataset, focusing on the prevalence of hate speech in political communication and the different forms of hateful language used. Additionally, we benchmark the dataset using a range of machine learning, deep learning, and transformer-based algorithms. Our experiments reveal that the performance of these models can be further improved, highlighting the need for more advanced techniques for hate speech detection in low-resource languages. In particular, the relatively higher score of human evaluation over algorithms emphasizes the importance of utilizing both human and automated approaches for effective hate 
    
[^50]: 推特数据中的立场预测与分析：加纳2020年总统选举的案例研究

    Stance Prediction and Analysis of Twitter data : A case study of Ghana 2020 Presidential Elections. (arXiv:2306.14203v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.14203](http://arxiv.org/abs/2306.14203)

    本研究通过对2020年加纳总统选举期间的推特数据进行立场分析，揭示了推特用户对两位主要候选人的观点，并通过词典和机器学习方法对数据集进行评估，得到了最佳性能。

    

    2020年12月7日，加纳人参加投票，确定下一任总统。为了从这次总统选举中获取洞察力，我们进行了立场分析（不总是等同于情感分析），以了解推特这个受欢迎的社交媒体平台如何反映其用户对两位主要总统候选人的意见。我们使用推特API（Tweepy）收集了总共99,356条推文，并对其中的3,090条进行了人工标注，分为三类：反对，中立和支持。然后我们对这些推文进行了预处理。利用两种基于词典的方法（VADER和TextBlob）以及五种基于监督式机器学习的方法（支持向量机（SVM），逻辑回归（LR），多项式朴素贝叶斯（MNB），随机梯度下降（SGD）和随机森林（RF））对结果数据集进行了评估，参考了准确率，精确率，召回率和F1分数等指标。最佳性能由L算法实现。

    On December 7, 2020, Ghanaians participated in the polls to determine their president for the next four years. To gain insights from this presidential election, we conducted stance analysis (which is not always equivalent to sentiment analysis) to understand how Twitter, a popular social media platform, reflected the opinions of its users regarding the two main presidential candidates. We collected a total of 99,356 tweets using the Twitter API (Tweepy) and manually annotated 3,090 tweets into three classes: Against, Neutral, and Support. We then performed preprocessing on the tweets. The resulting dataset was evaluated using two lexicon-based approaches, VADER and TextBlob, as well as five supervised machine learning-based approaches: Support Vector Machine (SVM), Logistic Regression (LR), Multinomial Na\"ive Bayes (MNB), Stochastic Gradient Descent (SGD), and Random Forest (RF), based on metrics such as accuracy, precision, recall, and F1-score. The best performance was achieved by L
    
[^51]: DC CoMix TTS：一种与混合器协作的端到端表现力TTS，利用离散码实现改进的韵律建模

    DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer. (arXiv:2305.19567v1 [cs.SD])

    [http://arxiv.org/abs/2305.19567](http://arxiv.org/abs/2305.19567)

    本文提出了一种基于离散码和混合器相协作的端到端表现力TTS，它采用新的输入表示和简单的架构来实现改进的韵律建模，证明了其有效性。

    

    尽管中性TTS取得了巨大的成功，但内容泄漏仍然是一个挑战。本文提出了一种新的输入表示和简单的架构来实现改进的韵律建模。受最近在TTS中使用离散码取得的成功启发，我们将离散码引入到参考编码器的输入中。具体来说，我们利用音频压缩模型中的向量量化器来利用它已经训练过的多样化的声学信息。此外，我们将修改后的MLP-Mixer应用到参考编码器中，使得架构更加轻盈。因此，我们以端到端的方式训练韵律转移TTS。我们通过主观和客观评估证明了我们方法的有效性。我们在实验中证明了，当离散码作为输入时，参考编码器可以学习到更好的与说话人无关的韵律。另外，即使输入参数更少，我们也可以获得可比较的结果。

    Despite the huge successes made in neutral TTS, content-leakage remains a challenge. In this paper, we propose a new input representation and simple architecture to achieve improved prosody modeling. Inspired by the recent success in the use of discrete code in TTS, we introduce discrete code to the input of the reference encoder. Specifically, we leverage the vector quantizer from the audio compression model to exploit the diverse acoustic information it has already been trained on. In addition, we apply the modified MLP-Mixer to the reference encoder, making the architecture lighter. As a result, we train the prosody transfer TTS in an end-to-end manner. We prove the effectiveness of our method through both subjective and objective evaluations. We demonstrate that the reference encoder learns better speaker-independent prosody when discrete code is utilized as input in the experiments. In addition, we obtain comparable results even when fewer parameters are inputted.
    
[^52]: 从理论角度揭示“思维链”背后的奥秘

    Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective. (arXiv:2305.15408v1 [cs.LG])

    [http://arxiv.org/abs/2305.15408](http://arxiv.org/abs/2305.15408)

    本文从理论层面探究了带有“思维链”提示的大型语言模型在解决基本数学和决策问题中的能力，发现自回归Transformer大小恒定即可解决任务，揭示了“思维链”提示的背后机制。

    

    最近的研究发现，"思维链"提示能够显著提高大型语言模型（LLMs）的性能，特别是在涉及数学或推理的复杂任务中。尽管获得了巨大的实证成功，但“思维链”背后的机制以及它如何释放LLMs的潜力仍然是神秘的。本文首次从理论上回答了这些问题。具体而言，我们研究了LLMs带有“思维链”在解决基本数学和决策问题中的能力。我们首先给出一个不可能的结果，表明任何有限深度的Transformer都不能直接输出正确的基本算术/方程任务的答案，除非模型大小随着输入长度的增加呈超多项式增长。相反，我们通过构造证明，大小恒定的自回归Transformer足以通过使用常用的数学语言形式生成“思维链”推导来解决这两个任务。

    Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the capacity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that any bounded-depth Transformer cannot directly output correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of a constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language forma
    
[^53]: “Polytuplet Loss: 训练阅读理解和逻辑推理模型的反向方法”

    Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2304.01046](http://arxiv.org/abs/2304.01046)

    本文研究了一种训练阅读理解和逻辑推理模型的反向方法，利用相对准确性的策略来训练模型，通过Polytuplet Loss函数来确保优先学习答案选择的相对正确性，获得了不错的成果，提出了具有一般性的训练方法和模型架构。

    

    在整个学校教育过程中，学生们将受到阅读理解和逻辑推理的考验。学生们已经开发了各种策略来完成此类考试，其中有些被认为是通常表现优于其他策略的。这样一种策略涉及强调相对准确性而非绝对准确性，理论上可以在不完全掌握解题所需信息的情况下得出正确答案。本文研究了应用这种策略来训练迁移学习模型以解决阅读理解和逻辑推理问题的有效性。这些模型在具有挑战性的阅读理解和逻辑推理基准数据集ReClor上进行了评估。尽管以前的研究集中于逻辑推理技能，但我们专注于一种通用的训练方法和模型架构。我们提出了Polytuplet Loss函数，是三元组损失函数的扩展，以确保优先学习答案选择的相对正确性而非学习绝对正确性。

    Throughout schooling, students are tested on reading comprehension and logical reasoning. Students have developed various strategies for completing such exams, some of which are generally thought to outperform others. One such strategy involves emphasizing relative accuracy over absolute accuracy and can theoretically produce the correct answer without full knowledge of the information required to solve the question. This paper examines the effectiveness of applying such a strategy to train transfer learning models to solve reading comprehension and logical reasoning questions. The models were evaluated on the ReClor dataset, a challenging reading comprehension and logical reasoning benchmark. While previous studies targeted logical reasoning skills, we focus on a general training method and model architecture. We propose the polytuplet loss function, an extension of the triplet loss function, to ensure prioritization of learning the relative correctness of answer choices over learning
    
[^54]: EHRSQL：面向电子病历的实用文本转SQL基准测试

    EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records. (arXiv:2301.07695v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.07695](http://arxiv.org/abs/2301.07695)

    该论文提出了一个面向电子病历数据的文本转SQL数据集，该数据集具有一系列独特挑战，包括生成SQL查询、理解时间表达式以及区分有无答案的问题。

    

    我们为电子病历（EHR）提供了一个新的文本到SQL数据集。对话是由222个医院工作人员包括医生、护士、保险审查和健康档案团队等手机而来。为了构建关于结构化EHR数据的QA数据集，我们在一所大学医院进行了一次民调并制作了模板话术以创建种子问题。然后，我们手动将它们链接到两个开源的EHR数据库（MIMIC-III和eICU）中，并在数据集中包含了来自民意调查的各种时间表达式和未能回答的问题。我们的数据集提出了一系列独特的挑战：模型需要 1）生成反映医院中各种需求的SQL查询，包括简单的检索和复杂的操作，如计算生存率，2）理解各种时间表达式以回答与时间敏感的医疗问题相关的问题，3）根据预测区分给定问题是可回答还是不可回答。

    We present a new text-to-SQL dataset for electronic health records (EHRs). The utterances were collected from 222 hospital staff, including physicians, nurses, insurance review and health records teams, and more. To construct the QA dataset on structured EHR data, we conducted a poll at a university hospital and templatized the responses to create seed questions. Then, we manually linked them to two open-source EHR databases, MIMIC-III and eICU, and included them with various time expressions and held-out unanswerable questions in the dataset, which were all collected from the poll. Our dataset poses a unique set of challenges: the model needs to 1) generate SQL queries that reflect a wide range of needs in the hospital, including simple retrieval and complex operations such as calculating survival rate, 2) understand various time expressions to answer time-sensitive questions in healthcare, and 3) distinguish whether a given question is answerable or unanswerable based on the predicti
    
[^55]: 基于转换器的生物医学语言模型的领域内自适应的本地化

    Localising In-Domain Adaptation of Transformer-Based Biomedical Language Models. (arXiv:2212.10422v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10422](http://arxiv.org/abs/2212.10422)

    本研究针对生物医学领域内自适应问题，探讨了两种途径来在非英语语言中产生生物医学语言模型。一种是通过神经机器翻译将英文资源翻译为目标语言，注重数量；另一种是直接基于高质量、狭谱的语料库进行本地化。这些方法有助于解决资源较少语言如意大利语的领域内适应问题。

    

    在数字医疗时代，医院每天产生的大量文本信息构成了一个重要但未充分利用的资产，可以通过特定任务、精细调整的生物医学语言表示模型来利用，从而改善患者护理和管理。对于这些专门领域，先前的研究表明，来自广覆盖点检的微调模型在大规模领域内资源的额外训练轮次上可以获益很大。然而，这些资源通常对于像意大利这样资源较少的语言是不可及的，使得当地医疗机构无法进行领域内适应。为了缩小这个差距，我们的工作探讨了两种可行的方法来在非英语语言中生成生物医学语言模型，以意大利语为具体案例：一种基于英文资源的神经机器翻译，追求数量而不是质量；另一种基于高质量、狭谱的语料库的方法，直接进行本地化。

    In the era of digital healthcare, the huge volumes of textual information generated every day in hospitals constitute an essential but underused asset that could be exploited with task-specific, fine-tuned biomedical language representation models, improving patient care and management. For such specialized domains, previous research has shown that fine-tuning models stemming from broad-coverage checkpoints can largely benefit additional training rounds over large-scale in-domain resources. However, these resources are often unreachable for less-resourced languages like Italian, preventing local medical institutions to employ in-domain adaptation. In order to reduce this gap, our work investigates two accessible approaches to derive biomedical language models in languages other than English, taking Italian as a concrete use-case: one based on neural machine translation of English resources, favoring quantity over quality; the other based on a high-grade, narrow-scoped corpus natively w
    
[^56]: NarraSum:一个用于故事概括的大规模数据集

    NarraSum: A Large-Scale Dataset for Abstractive Narrative Summarization. (arXiv:2212.01476v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.01476](http://arxiv.org/abs/2212.01476)

    NarraSum是一个大规模的故事概括数据集，旨在鼓励故事概括研究。实验结果显示，目前的概括模型在该数据集上与人类表现存在明显差距。

    

    故事概括旨在产生一个简洁版本的故事，描述其最显著的事件和角色。概括故事是有挑战性的，因为它需要理解事件因果关系和角色行为。为了鼓励在这个方向上的研究，我们提出了NarraSum，一个大规模的故事概括数据集。它包含了12.2万个故事文档，这些文档来自于电影和电视剧的情节描述，涵盖了各种不同的流派，以及它们相应的抽象概括。实验表明，在NarraSum上，人类与最先进的概括模型之间存在着巨大的性能差距。我们希望这个数据集能促进未来在概括方向的研究，以及更广泛的自然语言理解和生成的研究。该数据集可在https://github.com/zhaochaocs/narrasum获得。

    Narrative summarization aims to produce a distilled version of a narrative to describe its most salient events and characters. Summarizing a narrative is challenging as it requires an understanding of event causality and character behaviors. To encourage research in this direction, we propose NarraSum, a large-scale narrative summarization dataset. It contains 122K narrative documents, which are collected from plot descriptions of movies and TV episodes with diverse genres, and their corresponding abstractive summaries. Experiments show that there is a large performance gap between humans and the state-of-the-art summarization models on NarraSum. We hope that this dataset will promote future research in summarization, as well as broader studies of natural language understanding and generation. The dataset is available at https://github.com/zhaochaocs/narrasum.
    
[^57]: QueryForm: 一种简单的零样本表单实体查询框架

    QueryForm: A Simple Zero-shot Form Entity Query Framework. (arXiv:2211.07730v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07730](http://arxiv.org/abs/2211.07730)

    QueryForm是一种简单的零样本表单实体查询框架，通过使用双重提示机制和利用大规模查询-实体对进行预训练，能够从结构化文档中提取实体值，无需目标特定的训练数据，达到了新的最先进技术水平。

    

    零样本迁移学习对于文档理解是一个至关重要但未被充分研究的场景，有助于减少标注文档实体所需的高成本。我们提出了一种新颖的基于查询的框架QueryForm，该框架以零样本的方式从类似表单的文档中提取实体值。QueryForm包含一个双重提示机制，将文档模式和特定实体类型组合成一个查询，用于提示Transformer模型执行单个实体提取任务。此外，我们提议利用从类似表单的网页生成的大规模查询-实体对进行QueryForm的预训练，这些网页带有弱HTML注释。通过将预训练和微调统一到相同的基于查询的框架中，QueryForm使模型能够从包含各种实体和布局的结构化文档中学习，从而更好地推广到目标文档类型，无需目标特定的训练数据。QueryForm在平均水平上建立了新的最先进技术水平。

    Zero-shot transfer learning for document understanding is a crucial yet under-investigated scenario to help reduce the high cost involved in annotating document entities. We present a novel query-based framework, QueryForm, that extracts entity values from form-like documents in a zero-shot fashion. QueryForm contains a dual prompting mechanism that composes both the document schema and a specific entity type into a query, which is used to prompt a Transformer model to perform a single entity extraction task. Furthermore, we propose to leverage large-scale query-entity pairs generated from form-like webpages with weak HTML annotations to pre-train QueryForm. By unifying pre-training and fine-tuning into the same query-based framework, QueryForm enables models to learn from structured documents containing various entities and layouts, leading to better generalization to target document types without the need for target-specific training data. QueryForm sets new state-of-the-art average 
    
[^58]: 层次混合多标签分类在不平衡的跨学科研究提案中的应用

    Hierarchical MixUp Multi-label Classification with Imbalanced Interdisciplinary Research Proposals. (arXiv:2209.13912v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.13912](http://arxiv.org/abs/2209.13912)

    层次混合多标签分类方法用于解决不平衡的跨学科研究提案中的独特问题，包括层次结构的标签、异构的语义和不平衡的数量。

    

    资助机构依赖于领域专家和研究提案之间的主题匹配来指定提案审阅人。随着提案越来越跨学科，如何准确地对提案的跨学科性质进行建模和分类，并找到具有合适专业知识的专家审阅人变得具有挑战性。解决这一挑战的关键步骤是准确地对提案的跨学科标签进行建模和分类。现有的方法和应用相关文献，如文本分类和提案分类，在同时解决由跨学科提案数据引入的三个关键问题方面还不足：1）提案的学科标签具有从粗粒度到细粒度的层次结构，例如从信息科学到AI到AI的基本原理。2）各个主要文本部分具有异构的语义，起不同的作用。3）提案的数量在各个标签之间是不平衡的。

    Funding agencies are largely relied on a topic matching between domain experts and research proposals to assign proposal reviewers. As proposals are increasingly interdisciplinary, it is challenging to profile the interdisciplinary nature of a proposal, and, thereafter, find expert reviewers with an appropriate set of expertise. An essential step in solving this challenge is to accurately model and classify the interdisciplinary labels of a proposal. Existing methodological and application-related literature, such as textual classification and proposal classification, are insufficient in jointly addressing the three key unique issues introduced by interdisciplinary proposal data: 1) the hierarchical structure of discipline labels of a proposal from coarse-grain to fine-grain, e.g., from information science to AI to fundamentals of AI. 2) the heterogeneous semantics of various main textual parts that play different roles in a proposal; 3) the number of proposals is imbalanced between no
    
[^59]: 本地字节融合用于神经机器翻译

    Local Byte Fusion for Neural Machine Translation. (arXiv:2205.11490v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.11490](http://arxiv.org/abs/2205.11490)

    本文提出了一种基于字节的本地字节融合方法，用于神经机器翻译。该方法可以解决当前NLP模型中子词标记化方案的刚性和对其他语料库适应性差的问题，同时避免了在多语种语料库中过度切分低资源语言的影响。

    

    当前NLP模型中，子词标记化方案是主要的技术。然而，这种方案可能过于死板，并且在一个语料库上构建的标记器对其他平行语料库的适应性不佳。观察发现，在多语种语料库中，子词标记化方案会对低资源语言进行过度切分，从而导致翻译性能下降。子词标记化的一个简单替代方法是基于字节的方法，即使用编码方案（如UTF-8）将输入进行字节序列标记化。字节标记通常在子字符粒度上表示输入，即一个字符可以由多个字节标记序列表示。这导致字节序列比字符序列长得多。在较低层中强制执行局部信息的聚合可以指导模型构建更高层次的语义信息。我们提出了一种利用字节n-gram和单词边界的本地字节融合（LOBEF）方法用于基于字节的机器翻译。

    Subword tokenization schemes are the dominant technique used in current NLP models. However, such schemes can be rigid and tokenizers built on one corpus do not adapt well to other parallel corpora. It has also been observed that in multilingual corpora, subword tokenization schemes over-segment low-resource languages leading to a drop in translation performance. A simple alternative to subword tokenizers is byte-based methods i.e. tokenization into byte sequences using encoding schemes such as UTF-8. Byte tokens often represent inputs at a sub-character granularity i.e. one character can be represented by a sequence of multiple byte tokens. This results in byte sequences that are significantly longer than character sequences. Enforcing aggregation of local information in the lower layers can guide the model to build higher-level semantic information. We propose a Local Byte Fusion (LOBEF) method for byte-based machine translation -- utilizing byte $n$-gram and word boundaries -- to ag
    
[^60]: EHRKit：用于电子健康记录文本的Python自然语言处理工具包

    EHRKit: A Python Natural Language Processing Toolkit for Electronic Health Record Texts. (arXiv:2204.06604v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.06604](http://arxiv.org/abs/2204.06604)

    EHRKit是一个用于处理电子健康记录文本的Python工具包，它集成了多个NLP任务和基于MIMIC-III数据的接口，可以进行命名实体识别、摘要生成、机器翻译等任务。

    

    电子健康记录（EHR）是现代医疗系统的重要组成部分，影响着医疗服务、运营和研究。尽管EHR中有结构化信息，但非结构化文本引起了广泛关注，成为一个引人注目的研究领域。近期神经自然语言处理（NLP）方法的成功导致了处理非结构化临床笔记的新方向。在本研究中，我们创建了一个用于临床文本的Python库，EHRKit。该库包含两个主要部分：MIMIC-III特定函数和任务特定函数。第一部分介绍了一系列用于访问MIMIC-III NOTEEVENTS数据的接口，包括基本搜索、信息检索和信息提取。第二部分集成了许多第三方库，用于12个离线NLP任务，例如命名实体识别、摘要生成、机器翻译等。

    The Electronic Health Record (EHR) is an essential part of the modern medical system and impacts healthcare delivery, operations, and research. Unstructured text is attracting much attention despite structured information in the EHRs and has become an exciting research field. The success of the recent neural Natural Language Processing (NLP) method has led to a new direction for processing unstructured clinical notes. In this work, we create a python library for clinical texts, EHRKit. This library contains two main parts: MIMIC-III-specific functions and tasks specific functions. The first part introduces a list of interfaces for accessing MIMIC-III NOTEEVENTS data, including basic search, information retrieval, and information extraction. The second part integrates many third-party libraries for up to 12 off-shelf NLP tasks such as named entity recognition, summarization, machine translation, etc.
    
[^61]: 高模态多模态Transformer：量化模态与交互异质性以进行高模态表示学习

    High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning. (arXiv:2203.01311v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01311](http://arxiv.org/abs/2203.01311)

    本文研究了高模态场景下的高效表示学习，提出了两种新的信息论度量方法来量化模态和交互的异质性，以加速对多样化和少被研究的模态的推广。 (arXiv:2203.01311v4 [cs.LG] UPDATED)

    

    许多现实世界的问题本质上是多模态的，例如人类用于交流的口语、手势和语用学，以及机器人上的力、本体感和视觉传感器。虽然多模态学习引起了广泛的兴趣，但这些方法主要关注一小组模态，主要是语言、视觉和音频。为了加速向多样化和少被研究的模态推广，本文研究了高模态场景下的高效表示学习，涉及一个大量的不同模态。由于为每个新模态添加新模型变得代价过高，关键的技术挑战是异质性量化：我们如何衡量哪些模态编码了类似的信息和交互，以便允许与先前的模态共享参数？本文提出了两种新的信息论度量方法来量化异质性：(1)模态异质性研究了两个模态之间的相似性。

    Many real-world problems are inherently multimodal, from spoken language, gestures, and paralinguistics humans use to communicate, to force, proprioception, and visual sensors on robots. While there has been an explosion of interest in multimodal learning, these methods are focused on a small set of modalities primarily in language, vision, and audio. In order to accelerate generalization towards diverse and understudied modalities, this paper studies efficient representation learning for high-modality scenarios involving a large set of diverse modalities. Since adding new models for every new modality becomes prohibitively expensive, a critical technical challenge is heterogeneity quantification: how can we measure which modalities encode similar information and interactions in order to permit parameter sharing with previous modalities? This paper proposes two new information theoretic metrics for heterogeneity quantification: (1) modality heterogeneity studies how similar 2 modalitie
    
[^62]: 用于痴呆监测和诊断的纵向多模态数据集

    A Longitudinal Multi-modal Dataset for Dementia Monitoring and Diagnosis. (arXiv:2109.01537v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2109.01537](http://arxiv.org/abs/2109.01537)

    该论文提出了一个纵向多模态数据集，用于痴呆监测和诊断。通过分析语言、言语和语用指标，可以区分神经退行性疾病患者和对照组，从而为痴呆研究提供了宝贵的资源。

    

    痴呆是一系列神经退行性疾病，影响越来越多的全球老龄人口的记忆和认知能力。自动化分析语言、言语和语用指标作为认知衰退的潜在指标日益受到关注。在这里，我们提出了一个新颖的纵向多模态数据集，该数据集在自然环境下收集了轻度痴呆患者和配对的年龄匹配对照组的数据，时间跨度为几个月。多模态数据包括口头会话，其中的一部分被转录，以及输入和书写的思考内容，以及相关的非语言信息，如笔画和按键。我们详细描述了该数据集，并着重讨论了使用语音模态的任务。后者涉及利用数据的纵向特性来区分对照组和痴呆患者。我们的实验显示，会话间语音的变化在不同的会话之间存在显著差异。

    Dementia is a family of neurogenerative conditions affecting memory and cognition in an increasing number of individuals in our globally aging population. Automated analysis of language, speech and paralinguistic indicators have been gaining popularity as potential indicators of cognitive decline. Here we propose a novel longitudinal multi-modal dataset collected from people with mild dementia and age matched controls over a period of several months in a natural setting. The multi-modal data consists of spoken conversations, a subset of which are transcribed, as well as typed and written thoughts and associated extra-linguistic information such as pen strokes and keystrokes. We describe the dataset in detail and proceed to focus on a task using the speech modality. The latter involves distinguishing controls from people with dementia by exploiting the longitudinal nature of the data. Our experiments showed significant differences in how the speech varied from session to session in the 
    

