# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples](https://arxiv.org/abs/2402.13254) | 本研究提出CounterCurate框架，通过对比例子和生成式微调，全面提升视觉-语言组合推理能力，解决了物理推理和语义对照微调方面的关键问题，实现了显著性能改进。 |
| [^2] | [BiMediX: Bilingual Medical Mixture of Experts LLM](https://arxiv.org/abs/2402.13253) | 这项研究提出了BiMediX，一个旨在实现英语和阿拉伯语之间无缝医学交互的双语医学专家混合模型LLM，引入了半自动化的翻译流水线和全面的评估基准，同时推出了包含130万多样化医学交互的BiMed1.3M数据集。 |
| [^3] | [TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization](https://arxiv.org/abs/2402.13249) | 论文提出了一个新的主题对话摘要评估基准TofuEval，研究发现现有的LLMs在对话领域存在大量事实错误的幻觉，并表明当LLMs充当事实评估器时，其表现不佳。 |
| [^4] | [Unlocking Insights: Semantic Search in Jupyter Notebooks](https://arxiv.org/abs/2402.13234) | 本文探讨了在Jupyter笔记本中应用大型语言模型增强语义搜索能力的方法，展示了一个全面理解整个笔记本内容语义的搜索框架。 |
| [^5] | [Investigating Cultural Alignment of Large Language Models](https://arxiv.org/abs/2402.13231) | 研究发现大型语言模型在文化整合方面表现更佳，特别是通过与特定文化的主导语言提示或精制语言混合预训练。 |
| [^6] | [Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive](https://arxiv.org/abs/2402.13228) | 在这项工作中，我们提出了一种新的损失函数和训练过程DPO-Positive（DPOP），以避免直接偏好优化（DPO）中潜在的失败模式，并发现DPOP明显优于DPO。 |
| [^7] | [AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical Tool Learning](https://arxiv.org/abs/2402.13225) | AgentMD是一种新型语言代理，能够自动筛选和应用包含2,164个临床计算器的RiskCalcs集合，为克服临床工具易用性挑战和提高工作流效率提供了机会。 |
| [^8] | [RoCode: A Dataset for Measuring Code Intelligence from Problem Definitions in Romanian](https://arxiv.org/abs/2402.13222) | 提出了RoCode，一个用于测量罗马尼亚语编程问题的数据集，旨在评估在罗马尼亚语/多语言文本上训练的语言模型的代码智能。 |
| [^9] | [How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts](https://arxiv.org/abs/2402.13220) | 对多模态LLMs的欺骗性提示进行了实证分析，提出包含850个测试样本的基准测试MAD-Bench，发现GPT-4V在该基准测试上准确率较高，而其他模型性能差距显著。 |
| [^10] | [Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&A](https://arxiv.org/abs/2402.13213) | 多项选择问答任务中，基于最大softmax概率（MSPs）的模型预测方法有助于提高大型语言模型（LLMs）的正确性，我们提出了一种根据MSP有选择地弃权的策略以提高性能。 |
| [^11] | [Soft Self-Consistency Improves Language Model Agents](https://arxiv.org/abs/2402.13212) | Soft Self-Consistency (Soft-SC)通过软化评分标准替代自一致性的多数投票方法，提高了在涉及生成多个动作的长期互动任务中的性能和效率 |
| [^12] | [Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation](https://arxiv.org/abs/2402.13211) | 分析了大型语言模型在情感支持对话中的表现，揭示了其存在的偏好性偏差问题，即对特定策略的过高偏好会阻碍有效的情感支持。 |
| [^13] | [How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena](https://arxiv.org/abs/2402.13208) | ConfHyena是一种基于Hyena的改进模型，通过在处理语音时减少计算成本，显著缩短了训练时间。 |
| [^14] | [Question Calibration and Multi-Hop Modeling for Temporal Question Answering](https://arxiv.org/abs/2402.13188) | 该论文提出了一种新的问题校准和多跳建模方法，用于解决时间问答中先前模型存在的问题。 |
| [^15] | [What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents](https://arxiv.org/abs/2402.13184) | 这项研究引入了“CosmoAgent”，利用LLM模拟人类和外星文明之间的复杂互动，评估和平共存的可行性，并量化评估文明的发展轨迹，同时考虑不同文明之间的巨大多样性。 |
| [^16] | [Benchmarking Retrieval-Augmented Generation for Medicine](https://arxiv.org/abs/2402.13178) | 通过提出首个医学信息检索增强生成评估(MIRAGE)基准测试，并使用MedRAG工具包进行大规模实验，实现了对多个大型语言模型的准确性改进。 |
| [^17] | [AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies](https://arxiv.org/abs/2402.13152) | AnnoTheia是一个半自动标注工具包，用于检测人员在场景中讲话以及相应的转录，旨在促进对低资源语言的视听语音技术研究。 |
| [^18] | [CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for Boosting Metaphor Generation](https://arxiv.org/abs/2402.13145) | 本文介绍了一个大规模高质量的带注释中文隐喻语料库，强调隐喻生成中的基础及其独特特征，而非传统的对象和载体组合。 |
| [^19] | [The Hidden Space of Transformer Language Adapters](https://arxiv.org/abs/2402.13137) | Transformer语言适配器在冻结的表示空间上操作，适应过程渐进分布在多层中，并且大部分预测仍在源语言中进行演变。 |
| [^20] | [Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity](https://arxiv.org/abs/2402.13130) | 该研究探索了ELECTRA句子嵌入向量性能问题，并提出了一种新的截断模型微调方法，显著提高了语义文本相似性任务的表现 |
| [^21] | [TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning](https://arxiv.org/abs/2402.13125) | TreeEval提出了一种无基准评估方法，通过树规划策略提升了大型语言模型的评估效率和完整性 |
| [^22] | [A Survey on Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2402.13116) | 本调查深入探讨了大型语言模型知识蒸馏技术的重要性，并阐明了将专有巨头的先进功能转移到开源模型中的关键作用。 |
| [^23] | [When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality](https://arxiv.org/abs/2402.13113) | 研究了如何重启增量式Transformer构建和更新内部状态，揭示了增量状态的顺序结构如何编码关于偏误效应及其解决方式的信息，为分析上下文化意义表示和依赖解析的双向编码器带来见解，并显示它们在修订方面的优势。 |
| [^24] | [CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models](https://arxiv.org/abs/2402.13109) | CIF-Bench是一个用于评估大型语言模型在中文语言上零样本泛化能力的基准，通过多样化的指令和数据集划分来减少评估偏见。 |
| [^25] | [ELAD: Explanation-Guided Large Language Models Active Distillation](https://arxiv.org/abs/2402.13098) | ELAD提出了一种Explanation-Guided LLMs Active Distillation框架，通过主动学习策略优化注释成本和模型性能之间的平衡，并引入了基于解释的样本选择方法和LLM-注释解释修订技术。 |
| [^26] | [Digital Comprehensibility Assessment of Simplified Texts among Persons with Intellectual Disabilities](https://arxiv.org/abs/2402.13094) | 通过评估智障人士阅读未简化、自动和手动简化德语文本的理解度，发现不同阅读者群体和简化方法对可理解性的影响 |
| [^27] | [Event-level Knowledge Editing](https://arxiv.org/abs/2402.13093) | 提出了一个新的任务设置：事件级知识编辑，通过直接编辑新事件到LLMs中，在效率和完整性上改进了传统的三元组级别编辑。 |
| [^28] | [Towards an empirical understanding of MoE design choices](https://arxiv.org/abs/2402.13089) | 本研究系统评估了Mixture of Experts（MoEs）中常见设计选择对验证性能的影响，揭示了路由器的学习与初始化对模型性能的比较、序列级路由与标记级路由在专家专业化方面的不同影响。 |
| [^29] | [Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models](https://arxiv.org/abs/2402.13064) | 该研究提出了一种称为GLAN的广义指导调整方法，通过利用人类知识和能力分类法来生成大规模合成指导数据，为语言模型提供指导，从而实现对各个学科领域的广泛适用。 |
| [^30] | [Identifying Semantic Induction Heads to Understand In-Context Learning](https://arxiv.org/abs/2402.13055) | 该研究通过分析注意力头的操作，揭示了结合了句法依赖和知识图关系的语义感应头的出现，从而更好地理解了大型语言模型的上下文学习能力。 |
| [^31] | [Stable Knowledge Editing in Large Language Models](https://arxiv.org/abs/2402.13048) | 提出了一种名为StableKE的方法，采用了基于知识增强而非知识本地化的新视角，以实现大规模语言模型中的稳定知识编辑。 |
| [^32] | [Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries](https://arxiv.org/abs/2402.13043) | 使用文本摘要提高对话检索的有效性和效率，通过对话摘要生成器进行查询和关键词生成，进一步提炼轻量级对话编码器以避免额外推理成本 |
| [^33] | [Text-Guided Molecule Generation with Diffusion Language Model](https://arxiv.org/abs/2402.13040) | 提出了一种名为Text-Guided Molecule Generation with Diffusion Language Model（TGM-DLM）的新方法，通过扩散模型进行文本引导的分子生成，在生成有效分子表示方面表现出显著的效果优于自回归模型MolT5-Base。 |
| [^34] | [SiLLM: Large Language Models for Simultaneous Machine Translation](https://arxiv.org/abs/2402.13036) | 提出了SiLLM，将SiMT任务分解为策略决策和翻译子任务，并将这两个子任务委托给独立的代理，从而将大型语言模型引入SiMT。 |
| [^35] | [Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models](https://arxiv.org/abs/2402.13035) | 通过精心设计训练数据和构建检查-校正数据集，本研究增强了大型语言模型的自我校正能力，提高了自我校正的准确性。 |
| [^36] | [Heterogeneous Graph Reasoning for Fact Checking over Texts and Tables](https://arxiv.org/abs/2402.13028) | 提出了一种基于异质图的模型HeterFC，用于文本和表格中的事实核查，利用异质证据图和关系图神经网络进行信息传播。 |
| [^37] | [CFEVER: A Chinese Fact Extraction and VERification Dataset](https://arxiv.org/abs/2402.13025) | CFEVER是一个为事实提取和验证而设计的汉语数据集，提供了严格的标准和对应证据，可用于开发自动化系统，减轻人工核查的工作量。 |
| [^38] | [SoMeLVLM: A Large Vision Language Model for Social Media Processing](https://arxiv.org/abs/2402.13022) | SoMeLVLM是一种用于社交媒体处理的大型视觉语言模型，具有知识理解、应用、分析、评价和创造等五大关键能力，致力于理解和生成逼真的社交媒体行为。 |
| [^39] | [Understanding the effects of language-specific class imbalance in multilingual fine-tuning](https://arxiv.org/abs/2402.13016) | 微调transformer模型时，针对多语言数据集中个别语言标签不平衡的问题，通过为每种语言分别计算类别权重，可以缓解性能下降、语言分离更明显和无信息特征促进等不良影响。 |
| [^40] | [Code Needs Comments: Enhancing Code LLMs with Comment Augmentation](https://arxiv.org/abs/2402.13013) | 通过引入新的数据增强方法，为现有代码生成注释，并利用数据过滤策略来提高以代码为焦点的语言模型的性能。 |
| [^41] | [Investigating the Impact of Model Instability on Explanations and Uncertainty](https://arxiv.org/abs/2402.13006) | 模型稳定性对解释和不确定性的影响进行了调查，并发现实际扰动对性能和解释影响较小，但掩盖却有 drastical 影响。 |
| [^42] | [Phonotactic Complexity across Dialects](https://arxiv.org/abs/2402.12998) | 通过研究荷兰方言和闽方言，发现了方言层面上词长和音位结构复杂性之间的权衡关系，拓展了以往仅在语言层面记录的研究结果。 |
| [^43] | [Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism](https://arxiv.org/abs/2402.12997) | 提出了一种适用于现实约束的轻量级弃权机制，特别适用于再排序阶段，通过数据驱动的方法达到有效性，并提供了开源代码以促进其更广泛的应用。 |
| [^44] | [TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification](https://arxiv.org/abs/2402.12991) | TRAP提出了一种名为Targeted Random Adversarial Prompt (TRAP)的方法，用于识别特定LLM的使用，并在单次交互后以超过95%的真阳性率和低于0.2%的误报率检测目标LLMs。 |
| [^45] | [Can GNN be Good Adapter for LLMs?](https://arxiv.org/abs/2402.12984) | 本文提出了GraphAdapter，利用图神经网络（GNN）作为高效适配器，与LLMs协同处理文本属性图（TAGs）。 |
| [^46] | [The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis](https://arxiv.org/abs/2402.12976) | 通过多维分析发现，示范的有效性在多语种上下文学习中存在显著差异，其中部分模型对示范质量不敏感，而精心设计的模板可以消除对示范的依赖。 |
| [^47] | [Gl\'orIA - A Generative and Open Large Language Model for Portuguese](https://arxiv.org/abs/2402.12969) | Gl'orIA是一种专门为欧洲葡萄牙语设计的强大解码器大型语言模型，通过对350亿个tokens的全面PT-PT文本语料库预训练，为欧洲葡萄牙语提供了解决方案，并引入了CALAME-PT，这是第一个葡萄牙语零样本语言建模基准。 |
| [^48] | [Prompt Stealing Attacks Against Large Language Models](https://arxiv.org/abs/2402.12959) | 提出了一种新型攻击，即提示窃取攻击，目的是基于生成的答案窃取设计良好的提示。 |
| [^49] | [GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick](https://arxiv.org/abs/2402.12948) | 通过开发 GumbelSoft 水印，我们提出了一种能够在高多样性环境中增强生成文本多样性的解决方案，相较于其他方案表现更为优秀。 |
| [^50] | [Normalized Orthography for Tunisian Arabic](https://arxiv.org/abs/2402.12940) | 这项研究提出了专为突尼斯阿拉伯语而设计的"突尼斯阿拉伯语规范正字法"（NOTA），旨在解决用阿拉伯文拼写突尼斯阿拉伯语时所面临的挑战，确保对其独特音韵和形态特征的准确表示。 |
| [^51] | [Large Language Model-based Human-Agent Collaboration for Complex Task Solving](https://arxiv.org/abs/2402.12914) | 在这项工作中，我们介绍了基于大型语言模型的人-机协作进行复杂任务求解的问题，提出了基于强化学习的人-机协作方法ReHAC，通过构建人-机协作数据集训练策略模型，验证了该模型的有效性。 |
| [^52] | [OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data](https://arxiv.org/abs/2402.12913) | 相对较小的LLMs可以与大型LLM相比，在幻觉检测方面达到竞争性的性能水平 |
| [^53] | [More Discriminative Sentence Embeddings via Semantic Graph Smoothing](https://arxiv.org/abs/2402.12890) | 通过语义图平滑技术增强预训练模型获取的句子嵌入，有效改善文本聚类和分类任务的结果。 |
| [^54] | [GRAFFORD: A Benchmark Dataset for Testing the Knowledge of Object Affordances of Language and Vision Models](https://arxiv.org/abs/2402.12881) | 该论文提出了一个名为GRAFFORD的基准数据集，用于测试语言和视觉模型对物体可供性知识的表现，实验结果显示当前预训练语言模型在理解不常见物体可供性方面存在推理能力的局限。 |
| [^55] | [Autism Detection in Speech - A Survey](https://arxiv.org/abs/2402.12880) | 该研究综合分析了自闭症在语音、语言和言语中的表现特征，提出了可能指示自闭症的语言、韵律和声学线索的调查结果，并指出了在自闭症检测中存在的问题及可行的解决方案。 |
| [^56] | [Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data](https://arxiv.org/abs/2402.12869) | 本文探索了如何将表格转文本方法集成到LLM问答系统中，通过实验发现不同的表格转文本方法对QA系统性能的影响。 |
| [^57] | [Backward Lens: Projecting Language Model Gradients into the Vocabulary Space](https://arxiv.org/abs/2402.12865) | 将语言模型梯度投影到词汇空间中，挖掘信息在LMs内部的流动方式，探索新信息如何存储在LMs的神经元中。 |
| [^58] | [Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation](https://arxiv.org/abs/2402.12862) | 本论文通过将模糊情绪表达检测为领域外样本，并将情绪表示为分布来有效处理情绪中的歧义问题。 |
| [^59] | [MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models](https://arxiv.org/abs/2402.12851) | 提出了一种利用对比学习引导的混合专家模型MoELoRA解决大型语言模型进行参数高效微调时的灵活组合挑战 |
| [^60] | [Instruction-tuned Language Models are Better Knowledge Learners](https://arxiv.org/abs/2402.12847) | 通过在持续预训练文档之前暴露LLM到问题-答案对，以便从复杂文档中编码知识，可以更好地适应知识访问方式。 |
| [^61] | [ICON: Improving Inter-Report Consistency of Radiology Report Generation via Lesion-aware Mix-up Augmentation](https://arxiv.org/abs/2402.12844) | 本文提出的ICON方法旨在通过改善放射学报告生成的报告间一致性，提升系统捕捉语义等效病变相似性的能力。 |
| [^62] | [PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning](https://arxiv.org/abs/2402.12842) | 提出了PromptKD方法，通过提示调整实现了生成语言模型提取学生友好知识的蒸馏，无需微调整整个教师模型。 |
| [^63] | [ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic](https://arxiv.org/abs/2402.12840) | ArabicMMLU是针对阿拉伯语的第一个多任务语言理解基准测试，通过学校考试中收集的数据对35个模型进行全面评估，揭示了在阿拉伯语中性能改进的潜力。 |
| [^64] | [PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs](https://arxiv.org/abs/2402.12835) | PANDA是一种旨在增强LLMs领域特定能力的方法，通过利用专家模型响应偏好的见解，无需微调即可实现显著改进。 |
| [^65] | [Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model](https://arxiv.org/abs/2402.12821) | 该研究提出了针对摘要中事实不一致性的解决方案：通过大型语言模型在正确的范式设计下无需训练即可解决任务，并提出了训练策略以精炼更小型的高准确性的语言模型。 |
| [^66] | [Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?](https://arxiv.org/abs/2402.12819) | 专门模型通常只需少量标记样本（100-1000个）就能与通用模型持平甚至更好，取决于任务的复杂性和结果的变化。 |
| [^67] | [On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices](https://arxiv.org/abs/2402.12817) | 有限标注数据学习对随机性的敏感性，通过系统研究随机因素的影响，揭示了忽略相互作用可能导致的不一致结果。 |
| [^68] | [SymBa: Symbolic Backward Chaining for Multi-step Natural Language Reasoning](https://arxiv.org/abs/2402.12806) | SymBa提出了一种符号化向后推理方法，在多步自然语言推理中取得了显著的性能和效率提升，能够生成可解释的结构化证明。 |
| [^69] | [Few shot clinical entity recognition in three languages: Masked language models outperform LLM prompting](https://arxiv.org/abs/2402.12801) | 掩盖语言模型在三种语言中的少样本临床实体识别中表现优异，胜过LLM提示方法 |
| [^70] | [Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations](https://arxiv.org/abs/2402.12786) | 本文旨在让大型语言模型能够根据口语的不同语言风格做出恰当回应，并为此收集了一组适合训练的语音对语音数据集。 |
| [^71] | [Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval Systems](https://arxiv.org/abs/2402.12784) | 本文研究了Vec2Text对密集检索系统的威胁以及如何缓解，通过对距离度量、池化函数、瓶颈预训练等方面进行深入分析，以获得对密集检索系统中文本可恢复性和检索效果权衡关键元素的更深入理解。 |
| [^72] | [Acknowledgment of Emotional States: Generating Validating Responses for Empathetic Dialogue](https://arxiv.org/abs/2402.12770) | 该研究介绍了一个旨在促进共情对话的框架，通过验证响应生成实现了对情绪状态的认知。模型在所有模块中均优于随机基准线和ChatGPT性能。 |
| [^73] | [Model Composition for Multimodal Large Language Models](https://arxiv.org/abs/2402.12750) | 通过模型组合现有的多模态大型语言模型，提出了一种新范式，有效地保留了每个原始模型的模态理解能力，并引入了一种用于解决合并参数干扰和不匹配问题的方法。 |
| [^74] | [Me LLaMA: Foundation Large Language Models for Medical Applications](https://arxiv.org/abs/2402.12749) | Me LLaMA是一个医学领域的大型语言模型系列，通过持续预训练和指导调整在大型医学数据集上训练而成，其在零-shot和少-shot学习方面表现优于现有的医学语言模型和商业巨头ChatGPT。 |
| [^75] | [Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues](https://arxiv.org/abs/2402.12738) | 利用专家角色扮演对话数据进行研究发现，GPT-4生成的回复在心理辅导情境中与人类回复相竞争。 |
| [^76] | [UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and without machine translation](https://arxiv.org/abs/2402.12730) | 使用机器翻译和大型语言模型，本文开发了用于非洲和亚洲语言语义文本相关性任务的两种模型，取得了比部分官方基准更好的效果。 |
| [^77] | [Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2402.12728) | 提出了一种模态感知的LLM集成方法（MAIL）用于针对KVQA，通过细致地利用多模态知识来处理图像理解和知识推理。 |
| [^78] | [Are Large Language Models Rational Investors?](https://arxiv.org/abs/2402.12713) | 本研究引入了金融偏见指标（FBI）框架来评估大型语言模型（LLMs）的金融合理性，着重检验它们对金融信息的辨别和市场分析中可能存在的非理性偏见。 |
| [^79] | [FormulaQA: A Question Answering Dataset for Formula-Based Numerical Reasoning](https://arxiv.org/abs/2402.12692) | FormulaQA是一个基于初中物理考试的公式驱动数值推理问题问答数据集，通过评估LLMs的不同方法和使用检索增强型LLMs以及对小型模型进行微调，揭示了现有模型在应对复杂、基于公式的FormulaQA时的潜在改进空间。 |
| [^80] | [Tree-Planted Transformers: Large Language Models with Implicit Syntactic Supervision](https://arxiv.org/abs/2402.12691) | 提出了一种新方法 Tree-Planted Transformers (TPT)，通过在 Transformer LMs 的注意权重中隐式地“种植”树木来反映自然语言的句法结构，实现了句法大型语言模型（SLLM）的结合。 |
| [^81] | [Simpson's Paradox and the Accuracy-Fluency Tradeoff in Translation](https://arxiv.org/abs/2402.12690) | 准确性和流畅度在翻译中表现为正相关的悖论，这是辛普森悖论的一个实例，在语料库级别上二者呈正相关，在单个源段级别上存在权衡。 |
| [^82] | [SoftQE: Learned Representations of Queries Expanded by LLMs](https://arxiv.org/abs/2402.12663) | SoftQE通过将输入查询的嵌入映射到LLM扩展查询的嵌入，提高了密集检索性能，并在领域外任务上取得了显著的性能改善。 |
| [^83] | [The FinBen: An Holistic Financial Benchmark for Large Language Models](https://arxiv.org/abs/2402.12659) | 本文引入了FinBen，这是第一个专门设计用于彻底评估LLMs在金融领域能力的全面开源评估基准，对15个代表性LLMs进行评估，揭示了它们的优势和局限性。 |
| [^84] | [OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification](https://arxiv.org/abs/2402.12654) | 提出了OWSM-CTC，这是一种基于Connectionist Temporal Classification的新型仅编码器语音基础模型，训练有180k小时的公共音频数据，用于多语言自动语音识别（ASR）、语音翻译（ST）和语言识别。 |
| [^85] | [Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation](https://arxiv.org/abs/2402.12649) | 这项研究探讨了语言模型中偏见的负面影响，研究了"技巧测试"与更现实世界中表现的RUTEd评估之间的关联性，特别关注性别-职业偏见，并进行了多项评估比较。 |
| [^86] | [StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing](https://arxiv.org/abs/2402.12636) | StyleDubber提出了一种新的电影配音方法，通过在音素级别进行学习，解决了当前 V2C 模型中存在的音素发音不完整和身份稳定性差的问题。 |
| [^87] | [Reflect-RL: Two-Player Online RL Fine-Tuning for LMs](https://arxiv.org/abs/2402.12621) | 提出Reflect-RL，使用在线强化学习来微调语言模型，在这过程中引入了反射模型协助，并采用了负例生成、单提示动作枚举和课程学习来提高性能。 |
| [^88] | [Generative AI Security: Challenges and Countermeasures](https://arxiv.org/abs/2402.12617) | 生成式人工智能的安全挑战及对策研究。 |
| [^89] | [What is a word?](https://arxiv.org/abs/2402.12605) | 研究者探讨了词汇是什么，并指出了现有理论对于词汇的解释及实验设计方面的一些重要影响 |
| [^90] | [Standardize: Aligning Language Models with Expert-Defined Standards for Content Generation](https://arxiv.org/abs/2402.12593) | 该研究引入了一个名为Standardize的框架，通过检索式上下文学习指导大型语言模型与专家定义的标准对齐，提高了内容生成的精确性。 |
| [^91] | [Evolving AI Collectives to Enhance Human Diversity and Enable Self-Regulation](https://arxiv.org/abs/2402.12590) | 探索人工智能互动的“类社会”属性，以增加奖励并减少风险，展示新兴的去中心化AI集体如何扩大人类多样性范围和降低在线有毒行为风险。 |
| [^92] | [GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence](https://arxiv.org/abs/2402.12566) | GenAudit是一个工具，通过修订或删除未被参考文献支持的声明，并提供来自参考文献的证据，帮助修复语言模型输出中的事实错误。 |
| [^93] | [Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models](https://arxiv.org/abs/2402.12563) | 本文研究了大型语言模型的内在自我校正能力，并提出了一个“如果-否则”（IoE）提示框架，帮助模型评估自身“信心”并进行自我校正。 |
| [^94] | [CausalGym: Benchmarking causal interpretability methods on linguistic tasks](https://arxiv.org/abs/2402.12560) | CausalGym介绍了在语言任务中基准测试解释方法影响模型行为的能力。研究表明DAS方法胜过其他方法，并用它来研究了pythia-1b中的两个困难语言现象的学习轨迹。 |
| [^95] | [Creating a Fine Grained Entity Type Taxonomy Using LLMs](https://arxiv.org/abs/2402.12557) | 本研究利用GPT-4及其进阶版GPT-4 Turbo自主开发了涵盖5000多种微妙实体类型的详细分类法，通过迭代提示技术不断改进，展示出显著的质量。 |
| [^96] | [IMBUE: Improving Interpersonal Effectiveness through Simulation and Just-in-time Feedback with Human-Language Model Interaction](https://arxiv.org/abs/2402.12556) | IMBUE是首个同时专注于沟通技能和情绪管理、在提供反馈中整合专家领域知识以及基于心理学理论的互动式培训系统。 |
| [^97] | [Archer: A Human-Labeled Text-to-SQL Dataset with Arithmetic, Commonsense and Hypothetical Reasoning](https://arxiv.org/abs/2402.12554) | Archer数据集是一个具有挑战性的双语文本到SQL数据集，包含算术、常识和假设推理的复杂推理，挑战了当前最先进模型的能力。 |
| [^98] | [TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness](https://arxiv.org/abs/2402.12545) | TrustScore 是一个基于行为一致性概念的框架，用于评估大型语言模型（LLMs）的响应是否与其内在知识相一致，并成功实现了与人类判断高度相关的可信度评估。 |
| [^99] | [Parallel Structures in Pre-training Data Yield In-Context Learning](https://arxiv.org/abs/2402.12530) | 本研究发现，语言模型的上下文学习能力取决于预训练数据中的平行结构，通过在相似模板的短语对中学习来提高上下文学习准确度。 |
| [^100] | [Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection](https://arxiv.org/abs/2402.12501) | 使用Self-Filter这种方法，利用VLM本身作为一个过滤器进行数据集选择，以获取高质量数据并训练指令-following大型语言模型。 |
| [^101] | [Do Pre-Trained Language Models Detect and Understand Semantic Underspecification? Ask the DUST!](https://arxiv.org/abs/2402.12486) | 预训练语言模型在明确提示时能够相当准确地识别语义不确定的句子，但对其进行正确解释则更为困难。 |
| [^102] | [Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?](https://arxiv.org/abs/2402.12483) | LLMs能够在没有问题的情况下仅从选项中回答多项选择题，通过记忆、选择动态和问题推理进行黑盒分析，揭示了LLMs在选择性准确性方面的三个关键发现。 |
| [^103] | [The (R)Evolution of Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2402.12451) | 多模态大型语言模型能够无缝整合视觉和文本模态，为生成智能提供了新的可能性。 |
| [^104] | [Understanding Fine-grained Distortions in Reports of Scientific Findings](https://arxiv.org/abs/2402.12431) | 本研究对科学发现报道中的细微失真进行了详细研究，通过标注和自动检测技术，分析了科学研究结果在新闻报道和推文中可能出现的失真特征。 |
| [^105] | [Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data](https://arxiv.org/abs/2402.12424) | 本研究探讨了LLM在解释表格数据方面的有效性，比较了文本和图像表格表示对LLM性能的影响，为在表格相关任务上有效使用LLM提供了见解。 |
| [^106] | [On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models](https://arxiv.org/abs/2402.12423) | 本研究在文本转语音模型中探索了冻结模型的潜空间，发现其中包含丰富的语义信息，并提出了一些新方法来找出其中的语义方向，从而实现了不经过额外训练、架构更改或数据需求就能进行音频编辑。 |
| [^107] | [HunFlair2 in a cross-corpus evaluation of named entity recognition and normalization tools](https://arxiv.org/abs/2402.12372) | 挑战BTM工具在不同上下文中应用的可靠性，通过跨语料基准测试评估命名实体识别和归一化工具的性能。 |
| [^108] | [Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark](https://arxiv.org/abs/2402.12243) | 研究深入分析了文本到SQL领域中的噪声对模型的影响，并发现在BIRD-Bench基准测试中存在大量问题和标准查询中的噪声，这会显著影响模型的性能。 |
| [^109] | [Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.12168) | PEFT相对于全参数微调更容易受到权重投毒后门攻击的影响，提出了一个通过置信度识别受污染样本的毒化样本识别模块（PSIM），为权重投毒后门攻击提供稳健防御 |
| [^110] | [WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More](https://arxiv.org/abs/2402.12065) | 该论文提出了WKVQuant，一种专为大型语言模型设计的量化框架，通过量化权重和键值缓存来改善性能。 |
| [^111] | [Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs](https://arxiv.org/abs/2402.12030) | 介绍了基于最优传输的通用logit蒸馏 (ULD) 损失，用于解决不同架构和分词器模型之间蒸馏的限制。 |
| [^112] | [Bridging or Breaking: Impact of Intergroup Interactions on Religious Polarization](https://arxiv.org/abs/2402.11895) | 研究探讨团体互动对宗教极端化的影响，在印度 Twitter 用户中发现，政治和社会事件的团体间互动可以减少极端化。 |
| [^113] | [Microstructures and Accuracy of Graph Recall by Large Language Models](https://arxiv.org/abs/2402.11821) | 本研究首次系统研究了大型语言模型对图形召回的准确性和偏见微结构，探讨了它们与人类的异同以及对其他图形推理任务的影响。 |
| [^114] | [Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations](https://arxiv.org/abs/2402.11770) | 使用结构化思维链提示的方法，在少样本情况下生成内容相关的问答对话，提高了代理程序对基础文档的忠诚度，训练强大的对话问答代理。 |
| [^115] | [MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs](https://arxiv.org/abs/2402.11756) | MARS提出了一种新的评分函数MARS，考虑了生成序列中每个标记的语义贡献，该方法改进了生成式LLMs中的不确定性估计性能。 |
| [^116] | [From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings](https://arxiv.org/abs/2402.11512) | 提出了DeepSoftDebias算法，在不同领域数据集、准确度指标和NLP任务中全面评估，发现其在减少性别、种族和宗教偏见方面优于现有最先进方法 |
| [^117] | [The Unreasonable Effectiveness of Eccentric Automatic Prompts](https://arxiv.org/abs/2402.10949) | 异类自动提示的不合理有效性研究了大型语言模型在处理各种提示时的表现，结果显示在大多数情况下，包括“积极思考”提示会对模型性能产生积极影响。 |
| [^118] | [Humans or LLMs as the Judge? A Study on Judgement Biases](https://arxiv.org/abs/2402.10669) | 提出了一种新框架来研究LLM和人类裁判的偏见，揭示人类和LLM裁判在面对干扰时的脆弱性，强调评估现有LLM性能的挑战。 |
| [^119] | [The optimal placement of the head in the noun phrase. The case of demonstrative, numeral, adjective and noun](https://arxiv.org/abs/2402.10311) | 本研究旨在探讨句法依赖距离最小化与意外减少最小化原则在名词短语中的冲突，结论显示当涉及的单词较少且单词较短时，意外减少可能会超越句法依赖距离优化。 |
| [^120] | [Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective](https://arxiv.org/abs/2402.10184) | 本研究通过设计奖励建模过程中的数据集信息结构，从图论的视角提出了RLHF中奖励泛化的问题，以解决多样的环境、低成本标注和可靠的对齐性能间的不兼容性。 |
| [^121] | [Best Arm Identification for Prompt Learning under a Limited Budget](https://arxiv.org/abs/2402.09723) | 这项工作提出了一种在提示学习中考虑有限预算约束的方法，通过建立提示学习和多臂赌博机中固定预算最佳臂识别之间的联系，提出了一个通用框架TRIPLE，通过利用聚类和嵌入思想实现了两个增强方法。 |
| [^122] | [A Systematic Review of Data-to-Text NLG](https://arxiv.org/abs/2402.08496) | 这篇系统性回顾全面分析了数据到文本自然语言生成研究的现状，提出未来方向，并解决了相关挑战。 |
| [^123] | [ChatCell: Facilitating Single-Cell Analysis with Natural Language](https://arxiv.org/abs/2402.08303) | ChatCell是一个利用自然语言促进单细胞分析的工具，通过词汇适应和统一序列生成，它具备深厚的专业知识和适应各种分析任务的能力。 |
| [^124] | [Lissard: Long and Simple Sequential Reasoning Datasets](https://arxiv.org/abs/2402.07859) | Lissard是一个包含七个任务的基准，用于评估模型处理和生成各种序列长度的能力，需要重复的过程执行。评估结果显示随着序列复杂性增加，所有模型的性能都呈一致下降趋势。 |
| [^125] | [Large Language Models: A Survey](https://arxiv.org/abs/2402.06196) | 大型语言模型（LLMs）吸引了很多关注，因为它们在自然语言任务上的强大表现。该研究领域发展迅速，包括了各种著名的LLMs、构建和增强LLMs的技术、以及流行的LLM数据集和评估指标。 |
| [^126] | [On the Convergence of Zeroth-Order Federated Tuning in Large Language Models](https://arxiv.org/abs/2402.05926) | 我们的研究提出了一种名为FedMeZO的方法，以在零阶联邦学习和大规模语言模型之间实现内存高效的调整。我们的理论和实证证据表明FedMeZO不仅收敛速度快于传统的一阶方法，而且在个性化的联邦策略中具有关键的作用。 |
| [^127] | [LESS: Selecting Influential Data for Targeted Instruction Tuning](https://arxiv.org/abs/2402.04333) | LESS是一种优化感知且实际高效的算法，用于在大型语言模型中选择具有影响力的数据以开发特定能力，它采用低秩梯度相似性搜索方法进行指令数据选择。 |
| [^128] | [Unified Hallucination Detection for Multimodal Large Language Models](https://arxiv.org/abs/2402.03190) | 该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。 |
| [^129] | [Multi: Multimodal Understanding Leaderboard with Text and Images](https://arxiv.org/abs/2402.03173) | Multi是一个多模态理解的排行榜，提供了一个综合数据集，评估多模态大型语言模型对理解复杂图表和科学问题的能力。它兼具准确和开放式的回答形式，挑战MLLM的各种任务，并包含超过18,000个问题。 |
| [^130] | [APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data Annotation](https://arxiv.org/abs/2402.01697) | APT-Pipe is an automated prompt-tuning tool that enhances ChatGPT's text classification performance by automatically tuning prompts on any given dataset, resulting in a significant improvement in weighted F1-score across twelve experimented datasets. |
| [^131] | [Improving the TENOR of Labeling: Re-evaluating Topic Models for Content Analysis](https://arxiv.org/abs/2401.16348) | 本研究重新评估了神经、监督和经典主题模型在内容分析中的效果，结果显示上下文神经主题模型在聚类评估和人类评估方面表现最佳。 |
| [^132] | [PRewrite: Prompt Rewriting with Reinforcement Learning](https://arxiv.org/abs/2401.08189) | 本文提出了一种基于强化学习的自动化工具PRewrite，用于重写提示草案并生成高效的新提示，以解决提示工程中的挑战。 |
| [^133] | [Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding](https://arxiv.org/abs/2401.07851) | 投机解码作为一种新颖的解码范式，能够加速大型语言模型推理过程，提供了全面的概述和分析。 |
| [^134] | [MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of Large Language Models](https://arxiv.org/abs/2401.07598) | MAPLE通过在两个多语言指令调整数据集上对LLama-2-7B和Mistral-7B模型进行微调，在六项涵盖40种语言的下游任务中发现了微调对模型性能的影响。 |
| [^135] | [Model Editing at Scale leads to Gradual and Catastrophic Forgetting](https://arxiv.org/abs/2401.07453) | 评估了当前模型编辑方法在规模化情况下的表现，发现随着模型被顺序编辑多个事实，它会逐渐遗忘先前的事实及执行下游任务的能力。 |
| [^136] | [EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records](https://arxiv.org/abs/2401.07128) | EHRAgent是一个由代码接口赋能的大型语言模型代理，用于自主生成和执行多表格推理代码，通过错误信息学习改进生成的代码，结合长期记忆选择并建立在过去经验中的成功案例。 |
| [^137] | [Large Language Models Can Learn Temporal Reasoning](https://arxiv.org/abs/2401.06853) | 本文提出了一个新的TG-LLM框架，以语言为基础进行时间推理，通过教导LLM将上下文翻译成时间图，并使用CoTs指导LLM进行符号推理。 |
| [^138] | [Assessing the Impact of Prompting Methods on ChatGPT's Mathematical Capabilities](https://arxiv.org/abs/2312.15006) | 三种提示方法对ChatGPT的数学能力并未产生一贯性改进效果，部分方法甚至导致性能下降 |
| [^139] | [A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia](https://arxiv.org/abs/2312.02073) | 通过Fakepedia数据集研究语言模型的基础能力和进行因果中介分析，以解决上下文信息与存储知识相矛盾的问题。 |
| [^140] | [Overview of Current Applications of Large Language Models in Various Medical Specialities](https://arxiv.org/abs/2311.12882) | 大型语言模型在医疗领域的应用概述，突出了它们在医疗质量提升中的变革性作用，重点关注了诊断和治疗领域，以及在癌症护理、皮肤科、牙科和心理健康等方面的创新应用。 |
| [^141] | [MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning](https://arxiv.org/abs/2311.10537) | 提出了一个新颖的医学领域的跨学科合作(MC)框架，利用基于LLM的代理在角色扮演设置中参与协作多轮讨论，从而提高LLM的熟练程度和推理能力 |
| [^142] | [LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores](https://arxiv.org/abs/2311.09766) | 本文研究了语言模型驱动的评估指标在总结任务中是否会对相同的基础语言模型生成的文本表现出偏见，并发现在无参考情况下使用时偏见尤为显著。 |
| [^143] | [Reducing Privacy Risks in Online Self-Disclosures with Language Models](https://arxiv.org/abs/2311.09538) | 通过语言模型的检测和抽象，本研究降低了在线自我披露的隐私风险，提出了自我披露抽象的任务，并探索了多种微调策略。 |
| [^144] | [Heuristic-Driven Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction](https://arxiv.org/abs/2311.06555) | 通过启发式驱动的类比链接促进方法，该研究增强了大型语言模型用于文档级事件论证提取，使其能够从示例中学习任务特定启发式，并通过类比推理处理新情况以提高性能。 |
| [^145] | [Prompt Engineering a Prompt Engineer](https://arxiv.org/abs/2311.05661) | 提示工程任务对于优化大型语言模型在定制任务上的表现至关重要，PE2方法通过详细描述、上下文规范和逐步推理模板的注入，在各种语言任务中展现出出色的适用性和效果。 |
| [^146] | [On Context Utilization in Summarization with Large Language Models](https://arxiv.org/abs/2310.10570) | 本文研究了大型语言模型在摘要中上下文利用的问题，发现了摘要任务中关于输入位置的性能模式以及源文件到摘要的内容映射挑战。 |
| [^147] | [From Text to Self: Users' Perceptions of Potential of AI on Interpersonal Communication and Self](https://arxiv.org/abs/2310.03976) | 用户对大型语言模型驱动的工具在人际交流方面的能力持积极看法，认为可以增加沟通自信、帮助表达想法以及克服语言和文化障碍，但也揭示出工具存在的一些局限性和用户关于技术不真实性和过度依赖的担忧。 |
| [^148] | [Textless Low-Resource Speech-to-Speech Translation With Unit Language Models](https://arxiv.org/abs/2305.15405) | 提出了一种新的框架，用于训练只需要几十小时平行语音数据的无文本低资源语音到语音翻译系统，并通过单元到单元的序列到序列翻译任务和无监督反向翻译目标来提高模型性能 |
| [^149] | [Sentence Representations via Gaussian Embedding](https://arxiv.org/abs/2305.12990) | 本文提出了一种基于高斯分布的对比学习框架 GaussCSE，用于处理句子之间的不对称关系，实现了与以往方法相同的性能，在自然语言推理中能够估计蕴涵关系的方向。 |
| [^150] | [Exact Hard Monotonic Attention for Character-Level Transduction](https://arxiv.org/abs/1905.06319) | 开发了一种精确硬单调注意力的序列到序列模型，在字符级转导任务中取得了最先进的性能。 |
| [^151] | [Hard Non-Monotonic Attention for Character-Level Transduction](https://arxiv.org/abs/1808.10024) | 本文提出了一种用于字符级转录的硬性非单调注意力机制，引入了精确的、多项式时间算法来处理两个字符串之间的非单调对齐，表明硬性注意力模型是神经重新参数化的一种形式。 |
| [^152] | [SelectLLM: Can LLMs Select Important Instructions to Annotate?.](http://arxiv.org/abs/2401.16553) | 这项工作提出了一种名为SelectLLM的新方法，利用LLMs选择高质量指令。通过提示LLMs估计每个无标签指令的有用性和影响力，并使用聚类算法将指令分为多个聚类。 |
| [^153] | [MM-LLMs: Recent Advances in MultiModal Large Language Models.](http://arxiv.org/abs/2401.13601) | 近年来，多模式大语言模型（MM-LLMs）通过成本效益高的训练策略取得了显著进展，扩展了现有的语言模型的多模输入和输出支持。本论文提供了一份综合调查报告，介绍了MM-LLMs的设计和训练方案，整理了现有的MM-LLMs及其性能，总结了关键训练方法，并探讨了未来的研究方向。 |
| [^154] | [Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models.](http://arxiv.org/abs/2401.13227) | 本研究探索了在大规模异构图上应用大型语言模型进行图学习的方法，提出了LPNL框架用于可扩展链接预测。通过创新的提示语和采样流程，以及分而治之的策略，成功解决了大规模图中的信息过载问题，并在实验中表现出了优越的性能。 |
| [^155] | [Seed-Guided Fine-Grained Entity Typing in Science and Engineering Domains.](http://arxiv.org/abs/2401.13129) | 本文研究了在科学和工程领域中的种子引导下对细粒度实体进行类型划分的任务，提出了用无标注语料库找到更多实体来丰富监督信息的方法，并使用多头注意力的条件随机场模型进行实体划分。 |
| [^156] | [SLANG: New Concept Comprehension of Large Language Models.](http://arxiv.org/abs/2401.12585) | 本研究提出了一个新的基准SLANG，旨在增强大型语言模型LLMs对互联网上新概念的理解能力，同时提出了一种基于因果推断的基准方法FOCUS，能帮助LLMs更好地理解新的短语和用法模式。 |
| [^157] | [WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge.](http://arxiv.org/abs/2401.06659) | 本文提出了一个名为智慧M的插件框架，利用从大型视觉语言模型中产生的上下文世界知识来改进多模态情感分析，实验证明该方法在不同任务上有着显著的改进。 |
| [^158] | [Agent Alignment in Evolving Social Norms.](http://arxiv.org/abs/2401.04620) | 本论文提出了一个名为EvolutionaryAgent的进化框架，将Agent对齐转化为适者生存的演化和选择过程，在不断演化的社会规范中，与当前社会规范更好适应的Agent将具有更高的生存和传播概率。 |
| [^159] | [Vision-Language Interpreter for Robot Task Planning.](http://arxiv.org/abs/2311.00967) | 本文提出了一种名为Vision-Language Interpreter（ViLaIn）的新框架，该框架通过使用先进的语言模型和视觉语言模型生成机器人任务描述，并通过符号规划器的错误消息反馈进行改进。实验结果表明ViLaIn和符号规划器能够准确生成有效的机器人计划。 |
| [^160] | [InfoEntropy Loss to Mitigate Bias of Learning Difficulties for Generative Language Models.](http://arxiv.org/abs/2310.19531) | 提出了一种信息熵损失函数，用于减少生成式语言模型对常见和易学标记的偏好，使其更关注不常见和难学的标记。 |
| [^161] | [Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules.](http://arxiv.org/abs/2310.15724) | 这个论文提出了一种称为Variator的加速方法，通过即插即用的压缩插件增强了预训练模型的计算效率，并且可以根据工作负载动态选择不同加速比的插件。插件采用了压缩隐藏向量的方法来减小序列长度，并且由于参数少，可以节省存储和内存开销。 |
| [^162] | [Making Multimodal Generation Easier: When Diffusion Models Meet LLMs.](http://arxiv.org/abs/2310.08949) | EasyGen是一个有效的模型，它通过结合扩散模型和大型语言模型（LLMs）的能力，实现了更容易的多模态生成。相比现有的模型，EasyGen使用了一个名为BiDiffuser的双向条件扩散模型， 提供了更高效的模态交互，并且不仅能够生成文本回复，还能够促进文本到图像的生成。 |
| [^163] | [Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models.](http://arxiv.org/abs/2310.06692) | Meta-CoT是一种在混合任务场景中能够通用思维链提示的方法，在十个公共基准推理任务中表现出卓越的性能和优越的泛化能力。 |
| [^164] | [A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks.](http://arxiv.org/abs/2310.04270) | 本文对大型语言模型（LLM）在生物医学任务中的性能进行了综合评估，发现零样本LLMs在小样本生物医学数据集上的表现甚至超过了先进的精调生物医学模型，预训练使LLMs在生物医学领域具备了很强的专业能力。 |
| [^165] | [Intuitive or Dependent? Investigating LLMs' Robustness to Conflicting Prompts.](http://arxiv.org/abs/2309.17415) | 本文研究了LLMs对冲突提示的鲁棒性，发现这些模型对于误导性提示特别容易受到影响，尤其是在指导常识知识方面。 |
| [^166] | [Semi-Autoregressive Streaming ASR With Label Context.](http://arxiv.org/abs/2309.10926) | 提出了一种带有标签上下文的半自回归流式自动语音识别模型，通过使用语言模型子网络，将先前块中的标签作为额外的上下文进行建模。实验结果表明，该方法在流式自动语音识别中取得了更好的性能。 |
| [^167] | [Writer-Defined AI Personas for On-Demand Feedback Generation.](http://arxiv.org/abs/2309.10433) | 这项研究提出了基于作家定义的AI人物形象生成即时反馈的概念，通过两项用户研究表明，这个概念受到了作家的欢迎并帮助他们获得不同的观点。这项工作扩展了AI工具设计中的社会技术视角，为支持作家与AI的愿景做出了贡献。 |
| [^168] | [In-Contextual Bias Suppression for Large Language Models.](http://arxiv.org/abs/2309.07251) | 基于文本前导语和职业描述句生成的反事实命题模板可以有效抑制大型语言模型中的性别偏见，而不需要访问模型参数，并且不会对下游任务性能产生明显的负面影响。 |
| [^169] | [From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning.](http://arxiv.org/abs/2308.12032) | 该论文引入了一种自我引导的方法，让LLM能够自主地选择高质量的指令数据，通过引入指令遵循难度指标（IFD），大幅提高了模型训练效率，并在知名数据集上进行了验证，展示了优于传统数据输入的结果。 |
| [^170] | [PMET: Precise Model Editing in a Transformer.](http://arxiv.org/abs/2308.08742) | 该论文通过分析Transformer模型中的隐藏状态，发现多头自注意力编码了某些通用知识提取模式，因此在进行模型编辑时，不需要更新多头自注意力的权重。 |
| [^171] | [A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue Information Extraction.](http://arxiv.org/abs/2307.16200) | 本论文提出了一个知识增强的两阶段生成框架（KTGF）用于医学对话信息提取。通过两个阶段的生成，分别生成医学对话中的术语和每个术语的状态，从而更好地建模术语之间的关系。 |
| [^172] | [DIALGEN: Collaborative Human-LM Generated Dialogues for Improved Understanding of Human-Human Conversations.](http://arxiv.org/abs/2307.07047) | DIALGEN是一个人机半自动对话生成框架，通过迭代生成子对话和使用人工反馈来改善模型性能，适用于自动理解人际对话的应用。 |
| [^173] | [Estimating the Causal Effect of Early ArXiving on Paper Acceptance.](http://arxiv.org/abs/2306.13891) | 本研究使用数据和因果推断方法，研究了早期ArXiving论文对其被ICLR会议接受的影响。结果显示，早期ArXiving可能会对论文被接受的机会产生影响，但这种影响微乎其微，并且不因作者引用次数和机构排名等因素有所不同。 |
| [^174] | [A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises.](http://arxiv.org/abs/2306.04802) | 本论文综述了医疗知识图谱(HKGs)的构建流程、关键技术和利用方法以及现有资源，并深入探讨了HKG在各种医疗领域的变革性影响。 |
| [^175] | [Having Beer after Prayer? Measuring Cultural Bias in Large Language Models.](http://arxiv.org/abs/2305.14456) | 这篇论文研究了大型语言模型在处理和生成阿拉伯文本时出现的文化偏向西方文化的现象，表明语言模型在人名、食品、服装、地点、文学、饮料、宗教和体育等八个文化方面存在偏见。这些发现引发对于当前语言模型文化相关性的担忧。 |
| [^176] | [PWESuite: Phonetic Word Embeddings and Tasks They Facilitate.](http://arxiv.org/abs/2304.02541) | 本论文展示了一套语音单词嵌入及其相关任务，提高了语音信息处理的效果和可重复性。 |

# 详细

[^1]: CounterCurate: 通过对照例子增强物理和语义视觉-语言组合推理能力

    CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples

    [https://arxiv.org/abs/2402.13254](https://arxiv.org/abs/2402.13254)

    本研究提出CounterCurate框架，通过对比例子和生成式微调，全面提升视觉-语言组合推理能力，解决了物理推理和语义对照微调方面的关键问题，实现了显著性能改进。

    

    我们提出CounterCurate，一个框架，全面提升对比和生成式多模态模型的视觉-语言组合推理能力。特别地，我们确定了两个尚未充分探讨的关键问题：忽视了基于物理的推理（计数和位置理解），以及利用高性能文本和图像生成模型进行语义反事实微调的潜力。我们的工作开创了一个解决这些空白的方法。我们首先突出了多模态模型（如CLIP和LLaVA）在基于物理的组合推理中几乎无法胜任的表现。然后，我们应用简单的数据增强，使用基于图像的生成模型GLIGEN生成微调数据，使得性能显著提高：在我们新的策划的Flickr30k-Positions基准测试中，CLIP和LLaVA的性能分别提高了+33%和+37%。此外，我们利用了高性能文本和图像生成模型的能力。

    arXiv:2402.13254v1 Announce Type: cross  Abstract: We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of hig
    
[^2]: BiMediX: 双语医学专家混合模型LLM

    BiMediX: Bilingual Medical Mixture of Experts LLM

    [https://arxiv.org/abs/2402.13253](https://arxiv.org/abs/2402.13253)

    这项研究提出了BiMediX，一个旨在实现英语和阿拉伯语之间无缝医学交互的双语医学专家混合模型LLM，引入了半自动化的翻译流水线和全面的评估基准，同时推出了包含130万多样化医学交互的BiMed1.3M数据集。

    

    在本文中，我们介绍了BiMediX，这是第一个旨在实现在英语和阿拉伯语之间无缝互动的双语医学专家混合模型LLM。我们的模型在英语和阿拉伯语之间促进了广泛范围的医学交互，包括多轮对话以询问关于患者症状和病史等额外细节、多项选择题回答以及开放式问题回答。我们提出了一个半自动化的英语到阿拉伯语翻译流水线，结合人工优化以确保高质量的翻译。我们还引入了一个用于阿拉伯语医学LLMs的全面评估基准。此外，我们推出了BiMed1.3M，一个涵盖130万各种医学交互的广泛的阿拉伯语-英语双语指令集，产生了超过6.32亿个医疗专业token以进行指令调整。我们的BiMed1.3M数据集包括25万个合成的医生-患者多轮对话，并保持1:2的阿拉伯语比例。

    arXiv:2402.13253v1 Announce Type: new  Abstract: In this paper, we introduce BiMediX, the first bilingual medical mixture of experts LLM designed for seamless interaction in both English and Arabic. Our model facilitates a wide range of medical interactions in English and Arabic, including multi-turn chats to inquire about additional details such as patient symptoms and medical history, multiple-choice question answering, and open-ended question answering. We propose a semi-automated English-to-Arabic translation pipeline with human refinement to ensure high-quality translations. We also introduce a comprehensive evaluation benchmark for Arabic medical LLMs. Furthermore, we introduce BiMed1.3M, an extensive Arabic-English bilingual instruction set covering 1.3 Million diverse medical interactions, resulting in over 632 million healthcare specialized tokens for instruction tuning. Our BiMed1.3M dataset includes 250k synthesized multi-turn doctor-patient chats and maintains a 1:2 Arabic-
    
[^3]: TofuEval：评估LLM在主题对话摘要中的幻觉

    TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization

    [https://arxiv.org/abs/2402.13249](https://arxiv.org/abs/2402.13249)

    论文提出了一个新的主题对话摘要评估基准TofuEval，研究发现现有的LLMs在对话领域存在大量事实错误的幻觉，并表明当LLMs充当事实评估器时，其表现不佳。

    

    单文档新闻摘要在忠实度方面取得了长足进步，这得益于对事实一致性或幻觉评估的研究。我们探讨了这些进展是否能延伸到其他文本摘要领域。我们提出了一个新的主题对话摘要评估基准，由不同规模的LLMs生成。我们提供了关于这些摘要的事实一致性的二元句级人类注释，以及对事实不一致句子的详细解释。我们的分析表明，现有的LLMs在对话领域存在大量事实错误的幻觉，无论模型大小如何。另一方面，当LLMs（包括GPT-4）充当二元事实评估器时，它们表现不佳，且可以被当前最先进的专门事实评估度量所超越。最后，我们对幻觉类型进行了分析

    arXiv:2402.13249v1 Announce Type: cross  Abstract: Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a cu
    
[^4]: 在Jupyter笔记本中解锁洞见：语义搜索

    Unlocking Insights: Semantic Search in Jupyter Notebooks

    [https://arxiv.org/abs/2402.13234](https://arxiv.org/abs/2402.13234)

    本文探讨了在Jupyter笔记本中应用大型语言模型增强语义搜索能力的方法，展示了一个全面理解整个笔记本内容语义的搜索框架。

    

    语义搜索旨在通过理解搜索者的意图和可搜索数据空间中术语的上下文含义，提供高度相关的搜索结果，在信息检索中起着至关重要的作用。本文探讨了应用大型语言模型增强语义搜索能力的方法，特别针对Jupyter笔记本领域进行了定制。我们的目标是检索生成的输出，如图表、关联函数和方法以及其他相关信息。我们展示了一个语义搜索框架，可以全面理解整个笔记本内容的语义，从而有效处理各种类型的用户查询。该框架的关键组成部分包括：1）设计了一个数据预处理器，用于处理Jupyter笔记本中各种类型的单元格，包括Markdown和代码单元格。2）一种创新的方法

    arXiv:2402.13234v1 Announce Type: cross  Abstract: Semantic search, a process aimed at delivering highly relevant search results by comprehending the searcher's intent and the contextual meaning of terms within a searchable dataspace, plays a pivotal role in information retrieval. In this paper, we investigate the application of large language models to enhance semantic search capabilities, specifically tailored for the domain of Jupyter Notebooks. Our objective is to retrieve generated outputs, such as figures or tables, associated functions and methods, and other pertinent information.   We demonstrate a semantic search framework that achieves a comprehensive semantic understanding of the entire notebook's contents, enabling it to effectively handle various types of user queries. Key components of this framework include:   1). A data preprocessor is designed to handle diverse types of cells within Jupyter Notebooks, encompassing both markdown and code cells. 2). An innovative methodo
    
[^5]: 研究大型语言模型的文化整合

    Investigating Cultural Alignment of Large Language Models

    [https://arxiv.org/abs/2402.13231](https://arxiv.org/abs/2402.13231)

    研究发现大型语言模型在文化整合方面表现更佳，特别是通过与特定文化的主导语言提示或精制语言混合预训练。

    

    长久以来，语言和文化之间错综复杂的关系一直是语言人类学领域探索的一个课题。被推广为集体人类知识库的大型语言模型（LLMs）提出一个关键问题：这些模型是否真正概括了不同文化所采用的多样知识？我们的研究发现，这些模型在两个维度上表现出更大的文化整合性 -- 首先，当提示使用特定文化的主导语言时，其次，当预先使用该文化采用的语言的精制混合进行预训练。我们通过模拟社会学调查来量化文化整合，将模型的响应与实际调查参与者作为参考进行比较。具体来说，我们通过将LLMs用阿拉伯语和英语以不同的预训练数据混合提示来复制在埃及和美国各地进行的调查。

    arXiv:2402.13231v1 Announce Type: new  Abstract: The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensions -- firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the p
    
[^6]: Smaug：使用DPO-Positive修复偏好优化的失败模式

    Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive

    [https://arxiv.org/abs/2402.13228](https://arxiv.org/abs/2402.13228)

    在这项工作中，我们提出了一种新的损失函数和训练过程DPO-Positive（DPOP），以避免直接偏好优化（DPO）中潜在的失败模式，并发现DPOP明显优于DPO。

    

    直接偏好优化（DPO）在显著改善大型语言模型（LLMs）在推理、总结和对齐等下游任务上的性能方面是有效的。 DPO使用首选和非首选数据对模型选择一个响应而不是另一个的“相对”概率进行建模。在这项工作中，我们首先从理论上表明，只要首选和非首选类别之间的相对概率增加，标准DPO损失就可能导致模型对首选示例的可能性降低。然后，我们在实证上展示了当在常见数据集上微调LLMs时，尤其是在完成之间的编辑距离较短的数据集上，会出现这种现象。利用这些见解，我们设计了DPO-Positive（DPOP），一种新的损失函数和训练过程，避免了这种失败模式。令人惊讶的是，我们还发现DPOP明显优于DPO。

    arXiv:2402.13228v1 Announce Type: cross  Abstract: Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the \textit{relative} probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a \textit{reduction} of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we also find that DPOP significantly outperforms DPO a
    
[^7]: AgentMD：为大规模临床工具学习赋能语言代理以进行风险预测

    AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical Tool Learning

    [https://arxiv.org/abs/2402.13225](https://arxiv.org/abs/2402.13225)

    AgentMD是一种新型语言代理，能够自动筛选和应用包含2,164个临床计算器的RiskCalcs集合，为克服临床工具易用性挑战和提高工作流效率提供了机会。

    

    临床计算器通过为诸如预后等各种目的提供准确的基于证据的预测，在医疗保健中发挥着至关重要的作用。然而，它们的广泛利用常常受到易用性挑战、信息传播不畅和功能受限的阻碍。将大型语言模型与广泛的临床计算器集合相结合，为克服这些障碍并提高工作流效率提供了机会，但手动筛选过程的可扩展性是一个重大挑战。为此，我们引入了AgentMD，一种能够在各种临床背景下策划和应用临床计算器的新型语言代理。AgentMD利用已发表的文献，自动筛选了一组包含2,164个多样化临床计算器的集合，具有可执行功能和结构化文档，统称为RiskCalcs。人工评估显示RiskCalcs工具达到了一定精度。

    arXiv:2402.13225v1 Announce Type: cross  Abstract: Clinical calculators play a vital role in healthcare by offering accurate evidence-based predictions for various purposes such as prognosis. Nevertheless, their widespread utilization is frequently hindered by usability challenges, poor dissemination, and restricted functionality. Augmenting large language models with extensive collections of clinical calculators presents an opportunity to overcome these obstacles and improve workflow efficiency, but the scalability of the manual curation process poses a significant challenge. In response, we introduce AgentMD, a novel language agent capable of curating and applying clinical calculators across various clinical contexts. Using the published literature, AgentMD has automatically curated a collection of 2,164 diverse clinical calculators with executable functions and structured documentation, collectively named RiskCalcs. Manual evaluations show that RiskCalcs tools achieve an accuracy of
    
[^8]: RoCode: 用于测量罗马尼亚语问题定义中代码智能的数据集

    RoCode: A Dataset for Measuring Code Intelligence from Problem Definitions in Romanian

    [https://arxiv.org/abs/2402.13222](https://arxiv.org/abs/2402.13222)

    提出了RoCode，一个用于测量罗马尼亚语编程问题的数据集，旨在评估在罗马尼亚语/多语言文本上训练的语言模型的代码智能。

    

    最近，大型语言模型（LLMs）变得越来越强大，并且能够通过自然语言中的适当指令解决大量任务。然而，绝大多数测试套件假定指令是用英语编写的，这是事实上的提示语言。即使对于最先进的LLMs来说，代码智能和问题解决仍然是一项困难的任务。目前，没有数据集可以衡量除英语以外的语言中代码生成模型的泛化能力。在这项工作中，我们提出了RoCode，一个竞赛性编程数据集，包含2,642个用罗马尼亚语编写的问题，11,000个用C、C ++和Python编写的解决方案以及每个问题的全面测试套件。RoCode的目的是为在罗马尼亚语/多语言文本上训练的语言模型的代码智能提供一个评估基准，以及为预训练的罗马尼亚语模型提供一个微调集。

    arXiv:2402.13222v1 Announce Type: new  Abstract: Recently, large language models (LLMs) have become increasingly powerful and have become capable of solving a plethora of tasks through proper instructions in natural language. However, the vast majority of testing suites assume that the instructions are written in English, the de facto prompting language. Code intelligence and problem solving still remain a difficult task, even for the most advanced LLMs. Currently, there are no datasets to measure the generalization power for code-generation models in a language other than English. In this work, we present RoCode, a competitive programming dataset, consisting of 2,642 problems written in Romanian, 11k solutions in C, C++ and Python and comprehensive testing suites for each problem. The purpose of RoCode is to provide a benchmark for evaluating the code intelligence of language models trained on Romanian / multilingual text as well as a fine-tuning set for pretrained Romanian models. Th
    
[^9]: 有多容易欺骗多模态LLMs？关于欺骗性提示的实证分析

    How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts

    [https://arxiv.org/abs/2402.13220](https://arxiv.org/abs/2402.13220)

    对多模态LLMs的欺骗性提示进行了实证分析，提出包含850个测试样本的基准测试MAD-Bench，发现GPT-4V在该基准测试上准确率较高，而其他模型性能差距显著。

    

    多模态大型语言模型（MLLMs）的显著进展并没有使它们免疫各种挑战，特别是在处理带有欺骗性信息的提示时，会产生幻觉般的回应。为了定量评估这种脆弱性，我们提出了MAD-Bench，一个精心策划的基准测试，包含850个测试样本，分为6个类别，如不存在的对象、对象数量、空间关系和视觉混淆。我们对流行的MLLMs进行了全面分析，包括GPT-4V、Gemini-Pro，以及开源模型，如LLaVA-1.5和CogVLM。实证研究中，我们观察到GPT-4V和其他模型之间存在着显著的性能差距；之前的鲁棒指令调整模型，如LRV-Instruction和LLaVA-RLHF，在这个新基准测试中并不有效。虽然GPT-4V在MAD-Bench上取得了75.02%的准确率，但其他任何模型在我们的实验中都没有达到这一水平。

    arXiv:2402.13220v1 Announce Type: cross  Abstract: The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MAD-Bench, a carefully curated benchmark that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps between GPT-4V and other models; and previous robust instruction-tuned models, such as LRV-Instruction and LLaVA-RLHF, are not effective on this new benchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of any other model in our exper
    
[^10]: 软最大概率（大部分时候）在多项选择问答任务中预测大型语言模型的正确性

    Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&A

    [https://arxiv.org/abs/2402.13213](https://arxiv.org/abs/2402.13213)

    多项选择问答任务中，基于最大softmax概率（MSPs）的模型预测方法有助于提高大型语言模型（LLMs）的正确性，我们提出了一种根据MSP有选择地弃权的策略以提高性能。

    

    尽管大型语言模型（LLMs）在许多任务上表现出色，但过度自信仍然是一个问题。我们假设在多项选择问答任务中，错误答案将与最大softmax概率（MSPs）较小相关，相比之下正确答案较大。我们在十个开源LLMs和五个数据集上全面评估了这一假设，在表现良好的原始问答任务中发现了对我们假设的强有力证据。对于表现最佳的六个LLMs，从MSP导出的AUROC在59/60个实例中都优于随机机会，p < 10^{-4}。在这六个LLMs中，平均AUROC范围在60%至69%之间。利用这些发现，我们提出了一个带有弃权选项的多项选择问答任务，并展示通过根据初始模型响应的MSP有选择地弃权可以提高性能。我们还用预softmax logits而不是softmax进行了相同的实验。

    arXiv:2402.13213v1 Announce Type: cross  Abstract: Although large language models (LLMs) perform impressively on many tasks, overconfidence remains a problem. We hypothesized that on multiple-choice Q&A tasks, wrong answers would be associated with smaller maximum softmax probabilities (MSPs) compared to correct answers. We comprehensively evaluate this hypothesis on ten open-source LLMs and five datasets, and find strong evidence for our hypothesis among models which perform well on the original Q&A task. For the six LLMs with the best Q&A performance, the AUROC derived from the MSP was better than random chance with p < 10^{-4} in 59/60 instances. Among those six LLMs, the average AUROC ranged from 60% to 69%. Leveraging these findings, we propose a multiple-choice Q&A task with an option to abstain and show that performance can be improved by selectively abstaining based on the MSP of the initial model response. We also run the same experiments with pre-softmax logits instead of sof
    
[^11]: 软自一致性改善语言模型代理

    Soft Self-Consistency Improves Language Model Agents

    [https://arxiv.org/abs/2402.13212](https://arxiv.org/abs/2402.13212)

    Soft Self-Consistency (Soft-SC)通过软化评分标准替代自一致性的多数投票方法，提高了在涉及生成多个动作的长期互动任务中的性能和效率

    

    大型语言模型（LLMs）生成可以通过对多个解决方案进行抽样和评分来改进，以选择最终答案。当前的“抽样和选择”方法如自一致性（SC）依赖于多数投票来评分答案。然而，当任务有许多不同且有效的答案时，通过投票进行选择需要大量样本。这使得SC在涉及顺序生成多个动作（答案）的互动任务时成本过高。在确定大多数投票未能为此类任务提供一致的收益之后，我们展示了如何通过软化评分标准来提高成功率。我们引入了软自一致性（Soft-SC），它用模型可能性计算连续分数来取代SC的不连续评分，即使动作分布稀疏，也允许选择。软自一致性在长期互动任务上提高了性能和效率，需要较少的样本和投票。

    arXiv:2402.13212v1 Announce Type: cross  Abstract: Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current "sample and select" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (Soft-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requi
    
[^12]: 大型语言模型能成为良好的情感支持者吗？减轻对情感支持对话中的偏好性偏差

    Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation

    [https://arxiv.org/abs/2402.13211](https://arxiv.org/abs/2402.13211)

    分析了大型语言模型在情感支持对话中的表现，揭示了其存在的偏好性偏差问题，即对特定策略的过高偏好会阻碍有效的情感支持。

    

    情感支持对话（ESC）是一项旨在通过日常对话缓解个体情感困扰的任务。鉴于其固有的复杂性和非直觉性质，ESConv数据集融入了支持策略，以促进生成适当的回应。最近，尽管大型语言模型（LLMs）具有卓越的对话能力，先前的研究表明它们在提供有用的情感支持方面经常遇到困难。因此，本研究首先分析了LLMs在ESConv上的结果，揭示了在选择正确策略和对特定策略的显著偏好方面存在的挑战。在这个基础上，我们探讨了LLMs固有偏好对提供情感支持的影响，因此我们观察到，展现出对特定策略的高偏好会阻碍有效的情感支持，加剧其在预测适当策略方面的鲁棒性。

    arXiv:2402.13211v1 Announce Type: new  Abstract: Emotional Support Conversation (ESC) is a task aimed at alleviating individuals' emotional distress through daily conversation. Given its inherent complexity and non-intuitive nature, ESConv dataset incorporates support strategies to facilitate the generation of appropriate responses. Recently, despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support. Hence, this work initially analyzes the results of LLMs on ESConv, revealing challenges in selecting the correct strategy and a notable preference for a specific strategy. Motivated by these, we explore the impact of the inherent preference in LLMs on providing emotional support, and consequently, we observe that exhibiting high preference for specific strategies hinders effective emotional support, aggravating its robustness in predicting the appropriate strategy. Moreover
    
[^13]: 出于性能考虑，Hyena如何处理人类语音？使用ConfHyena进行语音识别和翻译

    How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena

    [https://arxiv.org/abs/2402.13208](https://arxiv.org/abs/2402.13208)

    ConfHyena是一种基于Hyena的改进模型，通过在处理语音时减少计算成本，显著缩短了训练时间。

    

    注意机制是现代神经模型的基石，但由于其二次复杂度而在处理长序列时面临计算障碍。因此，过去几年的研究工作侧重于寻找更有效的替代方案。其中，Hyena（Poli等，2023年）在语言建模和图像分类方面取得了竞争性结果，同时提供次线性的存储和计算复杂度。基于这些有希望的结果，我们提出了ConfHyena，这是一个Conformer，其编码器的自注意力被Hyena的一种变体取代，用于处理语音，其中长输入序列导致高计算成本。通过自动语音识别（英语）和翻译实验（从英语翻译成8种目标语言），我们展示了我们最好的ConfHyena模型将训练时间显著减少了27%，而品质损失仅为1%。

    arXiv:2402.13208v1 Announce Type: cross  Abstract: The attention mechanism, a cornerstone of state-of-the-art neural models, faces computational hurdles in processing long sequences due to its quadratic complexity. Consequently, research efforts in the last few years focused on finding more efficient alternatives. Among them, Hyena (Poli et al., 2023) stands out for achieving competitive results in both language modeling and image classification, while offering sub-quadratic memory and computational complexity. Building on these promising results, we propose ConfHyena, a Conformer whose encoder self-attentions are replaced with an adaptation of Hyena for speech processing, where the long input sequences cause high computational costs. Through experiments in automatic speech recognition (for English) and translation (from English into 8 target languages), we show that our best ConfHyena model significantly reduces the training time by 27%, at the cost of minimal quality degradation (~1%
    
[^14]: 问题校准与多跳建模用于时间问答

    Question Calibration and Multi-Hop Modeling for Temporal Question Answering

    [https://arxiv.org/abs/2402.13188](https://arxiv.org/abs/2402.13188)

    该论文提出了一种新的问题校准和多跳建模方法，用于解决时间问答中先前模型存在的问题。

    

    最近许多利用知识图谱（KGs）的模型在问答（QA）任务中展现出显著的成功。现实世界中，KGs中包含的许多事实都是受时间限制的，因此时间KGQA受到越来越多的关注。尽管之前的模型在时间KGQA方面取得了丰硕的成果，但它们仍然存在一些限制：（I）它们采用预训练语言模型（PLMs）来获取问题表示，而PLMs倾向于关注实体信息并忽略由于时间约束引起的实体转移，最终未能学习到实体的特定时间表示。 （II）它们既不强调实体之间的图结构，也没有明确建模图中的多跳关系，这将使复杂的多跳问题回答变得困难。为了缓解这一问题，我们提出了一种新颖的问题校准和多跳建模（QC-MHM）方法。

    arXiv:2402.13188v1 Announce Type: new  Abstract: Many models that leverage knowledge graphs (KGs) have recently demonstrated remarkable success in question answering (QA) tasks. In the real world, many facts contained in KGs are time-constrained thus temporal KGQA has received increasing attention. Despite the fruitful efforts of previous models in temporal KGQA, they still have several limitations. (I) They adopt pre-trained language models (PLMs) to obtain question representations, while PLMs tend to focus on entity information and ignore entity transfer caused by temporal constraints, and finally fail to learn specific temporal representations of entities. (II) They neither emphasize the graph structure between entities nor explicitly model the multi-hop relationship in the graph, which will make it difficult to solve complex multi-hop question answering. To alleviate this problem, we propose a novel Question Calibration and Multi-Hop Modeling (QC-MHM) approach. Specifically, We fir
    
[^15]: 如果LLM具有不同的世界观：使用基于LLM的代理模拟外星文明

    What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents

    [https://arxiv.org/abs/2402.13184](https://arxiv.org/abs/2402.13184)

    这项研究引入了“CosmoAgent”，利用LLM模拟人类和外星文明之间的复杂互动，评估和平共存的可行性，并量化评估文明的发展轨迹，同时考虑不同文明之间的巨大多样性。

    

    在这项研究中，我们介绍了“CosmoAgent”，这是一个创新的人工智能框架，利用大型语言模型（LLMs）来模拟人类与外星文明之间复杂的交互，特别强调史蒂芬·霍金关于不要随意向宇宙发送无线电信号的谨慎建议。该研究的目标是评估和平共存的可行性，同时考虑可能威胁善意文明的潜在风险。通过采用数学模型和状态转换矩阵，我们的方法定量评估文明的发展轨迹，为在关键增长和饱和点做出未来决策提供见解。此外，本文承认宇宙中潜在生活条件的巨大多样性可能会促进不同文明之间独特的宇宙观、道德准则和世界观。认识到地球上--

    arXiv:2402.13184v1 Announce Type: new  Abstract: In this study, we introduce "CosmoAgent," an innovative artificial intelligence framework utilizing Large Language Models (LLMs) to simulate complex interactions between human and extraterrestrial civilizations, with a special emphasis on Stephen Hawking's cautionary advice about not sending radio signals haphazardly into the universe. The goal is to assess the feasibility of peaceful coexistence while considering potential risks that could threaten well-intentioned civilizations. Employing mathematical models and state transition matrices, our approach quantitatively evaluates the development trajectories of civilizations, offering insights into future decision-making at critical points of growth and saturation. Furthermore, the paper acknowledges the vast diversity in potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among various civilizations. Recognizing the Earth-c
    
[^16]: 用于医学领域的检索增强生成的基准测试

    Benchmarking Retrieval-Augmented Generation for Medicine

    [https://arxiv.org/abs/2402.13178](https://arxiv.org/abs/2402.13178)

    通过提出首个医学信息检索增强生成评估(MIRAGE)基准测试，并使用MedRAG工具包进行大规模实验，实现了对多个大型语言模型的准确性改进。

    

    大型语言模型(LLMs)在广泛的医学问答任务上取得了最先进的性能，但仍然面临幻觉和过时知识的挑战。检索增强生成(RAG)是一个有前途的解决方案，并得到了广泛采用。然而，RAG系统可能涉及多个灵活的组件，并且缺乏关于各种医学目的的最佳RAG设置的最佳实践。为了系统地评估这些系统，我们提出了医学信息检索增强生成评估(MIRAGE)，这是一个首创的基准测试，包括来自五个医学问答数据集的7,663个问题。利用MIRAGE，我们通过本文介绍的MedRAG工具包，在41种不同语料库、检索器和骨干LLMs的组合上进行了超过1.8万亿的提示标记的大规模实验。总体而言，MedRAG提高了六种不同LLMs的准确性。

    arXiv:2402.13178v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs 
    
[^17]: AnnoTheia：用于视听语音技术的半自动标注工具包

    AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies

    [https://arxiv.org/abs/2402.13152](https://arxiv.org/abs/2402.13152)

    AnnoTheia是一个半自动标注工具包，用于检测人员在场景中讲话以及相应的转录，旨在促进对低资源语言的视听语音技术研究。

    

    世界上讲述超过7,000种已知语言，但由于缺乏标注资源，目前只有其中的一小部分被语音技术覆盖。尽管自监督语音表示、最近的大规模语音语料库收集以及挑战活动的组织缓解了这种不平等，但大多数研究主要以英语为基准。当涉及涉及声学和视觉语音模态的任务时，情况变得更加严重。为了促进对视听语音技术中低资源语言的研究，我们提出了AnnoTheia，一种半自动标注工具包，用于检测场景中人员讲话以及相应的转录。此外，为了展示为感兴趣的语言准备AnnoTheia的完整过程，我们还描述了将用于活动发言者检测的预训练模型适应西班牙语的过程，使用了一个数据库。

    arXiv:2402.13152v1 Announce Type: cross  Abstract: More than 7,000 known languages are spoken around the world. However, due to the lack of annotated resources, only a small fraction of them are currently covered by speech technologies. Albeit self-supervised speech representations, recent massive speech corpora collections, as well as the organization of challenges, have alleviated this inequality, most studies are mainly benchmarked on English. This situation is aggravated when tasks involving both acoustic and visual speech modalities are addressed. In order to promote research on low-resource languages for audio-visual speech technologies, we present AnnoTheia, a semi-automatic annotation toolkit that detects when a person speaks on the scene and the corresponding transcription. In addition, to show the complete process of preparing AnnoTheia for a language of interest, we also describe the adaptation of a pre-trained model for active speaker detection to Spanish, using a database 
    
[^18]: CMDAG: 一个带有注释的中文隐喻数据集作为“CoT”来提升隐喻生成

    CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for Boosting Metaphor Generation

    [https://arxiv.org/abs/2402.13145](https://arxiv.org/abs/2402.13145)

    本文介绍了一个大规模高质量的带注释中文隐喻语料库，强调隐喻生成中的基础及其独特特征，而非传统的对象和载体组合。

    

    隐喻是人类语言和文学中显著的修辞手法，因为它们增添了色彩、形象和强调，以增强有效交流。本文介绍了一个大规模高质量的带注释中文隐喻语料库，包括约28K句来自各种中文文学来源（如诗歌、散文、歌词等）。为确保注释的准确性和一致性，我们提出了一套全面的指南。这些指南涵盖了隐喻标注的方面，包括识别对象、载体和基础，以处理比喻、拟人、并列和夸张等复杂性。打破传统，我们的隐喻生成方法强调基础及其独特特征，而不是传统的对象和载体组合。通过将“基础”作为“CoT”（思维链）输入进行整合，我们能够生成重新

    arXiv:2402.13145v1 Announce Type: cross  Abstract: Metaphor is a prominent linguistic device in human language and literature, as they add color, imagery, and emphasis to enhance effective communication. This paper introduces a large-scale high quality annotated Chinese Metaphor Corpus, which comprises around 28K sentences drawn from a diverse range of Chinese literary sources, such as poems, prose, song lyrics, etc. To ensure the accuracy and consistency of our annotations, we introduce a comprehensive set of guidelines. These guidelines address the facets of metaphor annotation, including identifying tenors, vehicles, and grounds to handling the complexities of similes, personifications, juxtapositions, and hyperboles. Breaking tradition, our approach to metaphor generation emphasizes grounds and their distinct features rather than the conventional combination of tenors and vehicles. By integrating "ground" as a CoT (Chain of Thoughts) input, we are able to generate metaphors that re
    
[^19]: Transformer语言适配器的隐藏空间

    The Hidden Space of Transformer Language Adapters

    [https://arxiv.org/abs/2402.13137](https://arxiv.org/abs/2402.13137)

    Transformer语言适配器在冻结的表示空间上操作，适应过程渐进分布在多层中，并且大部分预测仍在源语言中进行演变。

    

    我们分析了transformer语言适配器的操作方式，这些小模块在冻结的语言模型之上训练，以将其预测适应到新的目标语言。我们展示了适应后的预测主要在模型训练的源语言中演变，而目标语言仅在模型的最后几层中变得明显。此外，适应过程是渐进的，分布在多个层中，可以跳过少量适配器组而不降低适应性能。最后，我们发现适配器在模型的冻结表示空间上操作，同时在很大程度上保留其结构，而不是在“孤立”的子空间上。

    arXiv:2402.13137v1 Announce Type: new  Abstract: We analyze the operation of transformer language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages. We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model. Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance. Last, we show that adapters operate on top of the model's frozen representation space while largely preserving its structure, rather than on an 'isolated' subspace. Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency.
    
[^20]: ELECTRA的句子嵌入是否无法修复？语义文本相似性案例

    Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity

    [https://arxiv.org/abs/2402.13130](https://arxiv.org/abs/2402.13130)

    该研究探索了ELECTRA句子嵌入向量性能问题，并提出了一种新的截断模型微调方法，显著提高了语义文本相似性任务的表现

    

    虽然BERT生成具有高质量的句子嵌入向量，但其预训练计算成本是一个明显的缺点。相比之下，ELECTRA提供了一种经济高效的预训练目标和下游任务性能提升，但其句子嵌入向量表现不佳。社区悄然停止使用ELECTRA的句子嵌入向量进行语义文本相似性（STS）任务。我们注意到使用ELECTRA鉴别器的最后一层相对于较早的层时性能显著下降。我们探索了这种下降，并设计了一种修复ELECTRA嵌入向量的方法，提出了一种新颖的截断模型微调（TMFT）方法。在STS基准数据集上，TMFT将Spearman相关系数提高了8个多点，同时提高了参数效率。我们将我们的分析扩展到各种模型大小和语言。此外，我们发现了ELECTRA生成模型的惊人功效，它的性能与BERT持平

    arXiv:2402.13130v1 Announce Type: new  Abstract: While BERT produces high-quality sentence embeddings, its pre-training computational cost is a significant drawback. In contrast, ELECTRA delivers a cost-effective pre-training objective and downstream task performance improvements, but not as performant sentence embeddings. The community tacitly stopped utilizing ELECTRA's sentence embeddings for semantic textual similarity (STS). We notice a significant drop in performance when using the ELECTRA discriminator's last layer in comparison to earlier layers. We explore this drop and devise a way to repair ELECTRA's embeddings, proposing a novel truncated model fine-tuning (TMFT) method. TMFT improves the Spearman correlation coefficient by over 8 points while increasing parameter efficiency on the STS benchmark dataset. We extend our analysis to various model sizes and languages. Further, we discover the surprising efficacy of ELECTRA's generator model, which performs on par with BERT, usi
    
[^21]: TreeEval：通过树规划实现对大型语言模型的无基准评估

    TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning

    [https://arxiv.org/abs/2402.13125](https://arxiv.org/abs/2402.13125)

    TreeEval提出了一种无基准评估方法，通过树规划策略提升了大型语言模型的评估效率和完整性

    

    最近，建立了许多新的基准来评估大型语言模型（LLMs）的性能，通过计算整体得分或使用另一个LLM作为评判者。然而，这些方法由于基准的公开访问和评估过程的不灵活而遭受数据泄漏的困扰。为了解决这个问题，我们引入了TreeEval，这是一种无基准评估方法，让一个高性能的LLM主持一个不可重现的评估会话，从根本上避免了数据泄漏。此外，这个LLM充当一个考官，提出一系列关于一个主题的问题，并采用树规划策略，考虑当前的评估状态来决定下一个问题的生成，确保评估过程的完整性和效率。我们评估了不同参数大小的6个模型，包括7B、13B和33B，最终实现了最高的相关系数。

    arXiv:2402.13125v1 Announce Type: cross  Abstract: Recently, numerous new benchmarks have been established to evaluate the performance of large language models (LLMs) via either computing a holistic score or employing another LLM as a judge. However, these approaches suffer from data leakage due to the open access of the benchmark and inflexible evaluation process. To address this issue, we introduce $\textbf{TreeEval}$, a benchmark-free evaluation method for LLMs that let a high-performance LLM host an irreproducible evaluation session and essentially avoids the data leakage. Moreover, this LLM performs as an examiner to raise up a series of questions under a topic with a tree planing strategy, which considers the current evaluation status to decide the next question generation and ensures the completeness and efficiency of the evaluation process. We evaluate $6$ models of different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately achieved the highest correlation coef
    
[^22]: 对大型语言模型知识蒸馏的调查

    A Survey on Knowledge Distillation of Large Language Models

    [https://arxiv.org/abs/2402.13116](https://arxiv.org/abs/2402.13116)

    本调查深入探讨了大型语言模型知识蒸馏技术的重要性，并阐明了将专有巨头的先进功能转移到开源模型中的关键作用。

    

    本调查对大型语言模型（LLMs）领域中知识蒸馏（KD）技术进行了深入探讨，重点关注KD在将诸如GPT-4之类的专有巨头的复杂能力转移到可访问的开源模型（如LLaMA和Mistral）中起着关键作用。在不断发展的人工智能领域，本项工作阐明了专有和开源LLMs之间的关键差异，展示了KD如何成为第二者赋予第一者先进功能和细致理解的重要媒介。我们的调查围绕算法、技能和垂直化这三个基础支柱精心构建，全面探讨了KD机制、特定认知能力的增强以及它们在不同领域的实际影响。重要的是，调查引导着数据增强（DA）和KD之间错综复杂的相互作用。

    arXiv:2402.13116v1 Announce Type: new  Abstract: This survey presents an in-depth exploration of knowledge distillation (KD) techniques within the realm of Large Language Models (LLMs), spotlighting the pivotal role of KD in transferring sophisticated capabilities from proprietary giants such as GPT-4 to accessible, open-source models like LLaMA and Mistral. Amidst the evolving AI landscape, this work elucidates the critical disparities between proprietary and open-source LLMs, demonstrating how KD serves as an essential conduit for imbuing the latter with the former's advanced functionalities and nuanced understandings. Our survey is meticulously structured around three foundational pillars: algorithm, skill, and verticalization -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, i
    
[^23]: 当只有时间能告诉: 通过重启增量性的视角解释Transformer如何处理局部歧义

    When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality

    [https://arxiv.org/abs/2402.13113](https://arxiv.org/abs/2402.13113)

    研究了如何重启增量式Transformer构建和更新内部状态，揭示了增量状态的顺序结构如何编码关于偏误效应及其解决方式的信息，为分析上下文化意义表示和依赖解析的双向编码器带来见解，并显示它们在修订方面的优势。

    

    处理一次一个令牌的增量模型有时会遇到可能有多种解释的点。因果模型被迫输出一个解释并继续，而可以修订的模型在消除歧义时可能会编辑其先前的输出。在这项工作中，我们研究了重启增量式Transformer如何构建和更新内部状态，以阐明导致不适用于自回归模型的修订的过程是什么。我们提出了一种可解释的方法来分析增量状态，显示它们的顺序结构编码了关于偏误效应及其解决方式的信息。我们的方法为分析上下文化意义表示和依赖解析的各种双向编码器带来了见解，有助于展示它们在涉及修订时相对于因果模型的优势。

    arXiv:2402.13113v1 Announce Type: new  Abstract: Incremental models that process sentences one token at a time will sometimes encounter points where more than one interpretation is possible. Causal models are forced to output one interpretation and continue, whereas models that can revise may edit their previous output as the ambiguity is resolved. In this work, we look at how restart-incremental Transformers build and update internal states, in an effort to shed light on what processes cause revisions not viable in autoregressive models. We propose an interpretable way to analyse the incremental states, showing that their sequential structure encodes information on the garden path effect and its resolution. Our method brings insights on various bidirectional encoders for contextualised meaning representation and dependency parsing, contributing to show their advantage over causal models when it comes to revisions.
    
[^24]: CIF-Bench：用于评估大型语言模型泛化能力的中文指令遵循基准

    CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models

    [https://arxiv.org/abs/2402.13109](https://arxiv.org/abs/2402.13109)

    CIF-Bench是一个用于评估大型语言模型在中文语言上零样本泛化能力的基准，通过多样化的指令和数据集划分来减少评估偏见。

    

    大型语言模型（LLMs）的进步增强了通过指令遵循在广泛范围的未见自然语言处理（NLP）任务上的泛化能力。然而，它们在如中文这样的低资源语言中的有效性常常会减弱，受到数据泄漏引起的偏见评估的影响，这使人对它们真正的泛化能力到新语言领域产生了怀疑。为了应对这一问题，我们引入了中文指令遵循基准（CIF-Bench），旨在评估LLMs对中文语言的零样本泛化能力。CIF-Bench 包含150个任务和15,000个输入输出对，由母语者开发，用于测试跨越20个类别的复杂推理和中国文化细微差别。为了减少评估偏见，我们只公开了数据集的一半，其余部分保持私密，并引入多样化的指令以最小化得分方差，共计45,000个数据实例。

    arXiv:2402.13109v1 Announce Type: cross  Abstract: The advancement of large language models (LLMs) has enhanced the ability to generalize across a wide range of unseen natural language processing (NLP) tasks through instruction-following. Yet, their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories. In response, we introduce the Chinese Instruction-Following Benchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of LLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000 input-output pairs, developed by native speakers to test complex reasoning and Chinese cultural nuances across 20 categories. To mitigate evaluation bias, we release only half of the dataset publicly, with the remainder kept private, and introduce diversified instructions to minimize score variance, totaling 45,000 data instances. Our eval
    
[^25]: ELAD: 解释引导的大型语言模型主动蒸馏

    ELAD: Explanation-Guided Large Language Models Active Distillation

    [https://arxiv.org/abs/2402.13098](https://arxiv.org/abs/2402.13098)

    ELAD提出了一种Explanation-Guided LLMs Active Distillation框架，通过主动学习策略优化注释成本和模型性能之间的平衡，并引入了基于解释的样本选择方法和LLM-注释解释修订技术。

    

    大型语言模型（LLMs）的部署和应用受到它们的内存效率、计算要求和高成本的API推断的阻碍。传统的蒸馏方法往往未能确定知识是否已经被充分转移，可能导致高成本或不完整的蒸馏。本文提出了一种名为Explanation-Guided LLMs Active Distillation（ELAD）框架，采用主动学习策略来优化注释成本和模型性能之间的平衡。为了改进有效的样本选择，我们引入了一种基于解释的样本选择方法，通过利用解释步骤中的不确定性来识别挑战其推理的样本。此外，我们提出了一种定制的LLM-注释解释修订技术，其中教师模型检测并纠正学生模型中的缺陷。

    arXiv:2402.13098v1 Announce Type: cross  Abstract: The deployment and application of Large Language Models (LLMs) is hindered by their memory inefficiency, computational demands, and the high costs of API inferences. Traditional distillation methods, which transfer the capabilities of LLMs to smaller models, often fail to determine whether the knowledge has been sufficiently transferred, potentially resulting in high costs or incomplete distillation. In this paper, we propose an Explanation-Guided LLMs Active Distillation (ELAD) framework that employs an active learning strategy to optimize the balance between annotation costs and model performance. To improve efficient sample selection, we introduce an explanation-guided sample selection method that identifies samples challenging its reasoning by exploiting uncertainties in explanation steps. Additionally, we present a customized LLM-annotated explanation revision technique where the teacher model detects and corrects flaws in the stu
    
[^26]: 智障人士简化文本的数字可理解性评估

    Digital Comprehensibility Assessment of Simplified Texts among Persons with Intellectual Disabilities

    [https://arxiv.org/abs/2402.13094](https://arxiv.org/abs/2402.13094)

    通过评估智障人士阅读未简化、自动和手动简化德语文本的理解度，发现不同阅读者群体和简化方法对可理解性的影响

    

    文本简化是指提高文本可理解性的过程。自动文本简化模型通常由专家或众包工作者而不是简化文本的主要目标群体（如智障人士）进行评估。我们进行了一项评估研究，其中参与者包括智障和非智障人士，在平板电脑上阅读未简化、自动和手动简化的德语文本。我们探讨了四种不同的评估可理解性的方法：多项选择理解问题、感知困难评级、响应时间和阅读速度。结果显示，这些测量结果存在显著差异，取决于阅读者群体以及文本是否经过自动或手动简化。对于智障人群，理解问题被认为是最

    arXiv:2402.13094v1 Announce Type: new  Abstract: Text simplification refers to the process of increasing the comprehensibility of texts. Automatic text simplification models are most commonly evaluated by experts or crowdworkers instead of the primary target groups of simplified texts, such as persons with intellectual disabilities. We conducted an evaluation study of text comprehensibility including participants with and without intellectual disabilities reading unsimplified, automatically and manually simplified German texts on a tablet computer. We explored four different approaches to measuring comprehensibility: multiple-choice comprehension questions, perceived difficulty ratings, response time, and reading speed. The results revealed significant variations in these measurements, depending on the reader group and whether the text had undergone automatic or manual simplification. For the target group of persons with intellectual disabilities, comprehension questions emerged as the
    
[^27]: 事件级知识编辑

    Event-level Knowledge Editing

    [https://arxiv.org/abs/2402.13093](https://arxiv.org/abs/2402.13093)

    提出了一个新的任务设置：事件级知识编辑，通过直接编辑新事件到LLMs中，在效率和完整性上改进了传统的三元组级别编辑。

    

    知识编辑旨在更新大型语言模型（LLMs）的知识，以防止它们过时。现有研究在事实知识三元组的级别上编辑LLMs。然而，现实世界中的自然知识更新来自新事件的发生，而不是直接更改事实三元组。本文提出了一个新的任务设置：事件级知识编辑，直接将新事件编辑到LLMs中，并在效率和完整性上改进了传统的三元组级别编辑。(1)效率。单个事件编辑会导致多个推断知识三元组的更新。(2)完整性。除了更新事实知识外，事件级别的编辑还需要考虑事件影响，更新LLMs关于未来趋势的知识。我们构建了一个高质量的事件级别编辑基准ELKEN，包括1,515个事件编辑，6,449个关于事实知识的问题和10,150个关于未来发展趋势的问题。

    arXiv:2402.13093v1 Announce Type: cross  Abstract: Knowledge editing aims at updating knowledge of large language models (LLMs) to prevent them from becoming outdated. Existing work edits LLMs at the level of factual knowledge triplets. However, natural knowledge updates in the real world come from the occurrences of new events rather than direct changes in factual triplets. In this paper, we propose a new task setting: event-level knowledge editing, which directly edits new events into LLMs and improves over conventional triplet-level editing on (1) Efficiency. A single event edit leads to updates in multiple entailed knowledge triplets. (2) Completeness. Beyond updating factual knowledge, event-level editing also requires considering the event influences and updating LLMs' knowledge about future trends. We construct a high-quality event-level editing benchmark ELKEN, consisting of 1,515 event edits, 6,449 questions about factual knowledge, and 10,150 questions about future tendencies
    
[^28]: 朝向对MoE设计选择的实证理解

    Towards an empirical understanding of MoE design choices

    [https://arxiv.org/abs/2402.13089](https://arxiv.org/abs/2402.13089)

    本研究系统评估了Mixture of Experts（MoEs）中常见设计选择对验证性能的影响，揭示了路由器的学习与初始化对模型性能的比较、序列级路由与标记级路由在专家专业化方面的不同影响。

    

    在这项研究中，我们系统地评估了Mixture of Experts（MoEs）中常见设计选择对验证性能的影响，揭示了在标记和序列级别上的不同影响。我们还提供经验证据表明，在学习路由器和冻结的随机初始化路由器之间具有可比性的性能，这表明学习路由可能并非必不可少。我们的研究进一步揭示了序列级路由可能导致特定主题的弱专家专业化，与标记级别路由观察到的语法专业化形成对比。

    arXiv:2402.13089v1 Announce Type: cross  Abstract: In this study, we systematically evaluate the impact of common design choices in Mixture of Experts (MoEs) on validation performance, uncovering distinct influences at token and sequence levels. We also present empirical evidence showing comparable performance between a learned router and a frozen, randomly initialized router, suggesting that learned routing may not be essential. Our study further reveals that Sequence-level routing can result in topic-specific weak expert specialization, in contrast to syntax specialization observed with Token-level routing.
    
[^29]: 从零开始合成数据：通用指导调整用于语言模型

    Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models

    [https://arxiv.org/abs/2402.13064](https://arxiv.org/abs/2402.13064)

    该研究提出了一种称为GLAN的广义指导调整方法，通过利用人类知识和能力分类法来生成大规模合成指导数据，为语言模型提供指导，从而实现对各个学科领域的广泛适用。

    

    我们引入了广义指导调整（称为GLAN），这是一种用于大型语言模型（LLMs）的指导调整的通用且可扩展的方法。与先前依赖于种子示例或现有数据集来构建指导调整数据的工作不同，GLAN仅利用事先策划的人类知识和能力分类法作为输入，并在所有学科领域生成大规模合成指导数据。具体来说，受人类教育系统中系统结构的启发，我们通过半自动方式，利用LLMs的帮助，将人类知识和能力分解为各种领域、子领域，最终到不同学科，构建了分类法。随后，我们为每个学科生成了一个全面的科目列表，并利用LLMs再次设计了适合每个科目的教学大纲。通过在大纲的每节课上详细介绍细粒度的关键概念，我们能够生成深入...

    arXiv:2402.13064v1 Announce Type: new  Abstract: We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines. Specifically, inspired by the systematic structure in human education system, we build the taxonomy by decomposing human knowledge and capabilities to various fields, sub-fields and ultimately, distinct disciplines semi-automatically, facilitated by LLMs. Subsequently, we generate a comprehensive list of subjects for every discipline and proceed to design a syllabus tailored to each subject, again utilizing LLMs. With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate dive
    
[^30]: 识别语义感应头以理解上下文学习

    Identifying Semantic Induction Heads to Understand In-Context Learning

    [https://arxiv.org/abs/2402.13055](https://arxiv.org/abs/2402.13055)

    该研究通过分析注意力头的操作，揭示了结合了句法依赖和知识图关系的语义感应头的出现，从而更好地理解了大型语言模型的上下文学习能力。

    

    虽然大型语言模型(LLMs)已经展示出卓越的性能，但它们推理逻辑的不透明性引发了对其可靠性的担忧。为了更好地理解LLMs，我们对注意力头的操作进行了详细分析，并旨在更好地理解LLMs的上下文学习。具体而言，我们研究了注意力头是否编码了自然语言中存在的两种类型的关系：从句子中解析的句法依赖和知识图中的关系。我们发现某些注意力头表现出一种模式，即当关注头标记时，它们会回忆起尾标记，并增加这些尾标记的输出逻辑。更重要的是，这种语义感应头的制定与语言模型上下文学习能力的出现存在密切关联。语义注意力头的研究推动了我们的

    arXiv:2402.13055v1 Announce Type: cross  Abstract: Although large language models (LLMs) have demonstrated remarkable performance, the lack of transparency in their inference logic raises concerns about their trustworthiness. To gain a better understanding of LLMs, we conduct a detailed analysis of the operations of attention heads and aim to better understand the in-context learning of LLMs. Specifically, we investigate whether attention heads encode two types of relationships between tokens present in natural languages: the syntactic dependency parsed from sentences and the relation within knowledge graphs. We find that certain attention heads exhibit a pattern where, when attending to head tokens, they recall tail tokens and increase the output logits of those tail tokens. More crucially, the formulation of such semantic induction heads has a close correlation with the emergence of the in-context learning ability of language models. The study of semantic attention heads advances our
    
[^31]: 大型语言模型中的稳定知识编辑

    Stable Knowledge Editing in Large Language Models

    [https://arxiv.org/abs/2402.13048](https://arxiv.org/abs/2402.13048)

    提出了一种名为StableKE的方法，采用了基于知识增强而非知识本地化的新视角，以实现大规模语言模型中的稳定知识编辑。

    

    大规模语言模型的高效知识编辑对于替换过时信息或大规模整合专业知识至关重要。然而，先前的方法隐式地假设知识在模型内是局部化且孤立的，这种假设过于简化了模型知识的相互关联性。局部化的前提导致知识编辑不完整，而孤立的假设可能损害其他知识和一般能力。这会给知识编辑方法的性能引入不稳定性。为超越这些假设，我们引入了StableKE，一种基于知识增强而非知识本地化的全新视角方法。为克服人工标注的成本，StableKE整合了两种自动知识增强策略：语义改写增强策略，这种策略通过多样化知识描述来促进教

    arXiv:2402.13048v1 Announce Type: new  Abstract: Efficient knowledge editing of large language models is crucial for replacing obsolete information or incorporating specialized knowledge on a large scale. However, previous methods implicitly assume that knowledge is localized and isolated within the model, an assumption that oversimplifies the interconnected nature of model knowledge. The premise of localization results in an incomplete knowledge editing, whereas an isolated assumption may impair both other knowledge and general abilities. It introduces instability to the performance of the knowledge editing method. To transcend these assumptions, we introduce StableKE, a method adopts a novel perspective based on knowledge augmentation rather than knowledge localization. To overcome the expense of human labeling, StableKE integrates two automated knowledge augmentation strategies: Semantic Paraphrase Enhancement strategy, which diversifies knowledge descriptions to facilitate the teac
    
[^32]: 用隐式文本摘要提高对话状态跟踪的有效性和效率

    Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries

    [https://arxiv.org/abs/2402.13043](https://arxiv.org/abs/2402.13043)

    使用文本摘要提高对话检索的有效性和效率，通过对话摘要生成器进行查询和关键词生成，进一步提炼轻量级对话编码器以避免额外推理成本

    

    arXiv:2402.13043v1 公告类型: 新 文摘: 基于大型语言模型（LLM）的小样本对话状态跟踪（DST）依赖于一个有效且高效的对话检索器来查找类似的上下文示例以进行提示学习。先前的作品使用原始对话上下文作为搜索键和查询，并通过对带注释的对话进行微调来实现卓越性能。然而，这种方法不太适合扩展到新的领域或新的注释语言，因为微调数据不可用。为解决这一问题，我们基于对话的文本摘要来处理对话检索任务。采用基于LLM的对话摘要生成器进行查询和关键词生成，实现了有效的最大内积搜索。为避免LLM基于对话摘要生成带来的额外推理成本，我们进一步提炼一个轻量级的对话编码器，该编码器在不解码测试对话摘要的情况下生成查询嵌入向量。

    arXiv:2402.13043v1 Announce Type: new  Abstract: Few-shot dialogue state tracking (DST) with Large Language Models (LLM) relies on an effective and efficient conversation retriever to find similar in-context examples for prompt learning. Previous works use raw dialogue context as search keys and queries, and a retriever is fine-tuned with annotated dialogues to achieve superior performance. However, the approach is less suited for scaling to new domains or new annotation languages, where fine-tuning data is unavailable. To address this problem, we handle the task of conversation retrieval based on text summaries of the conversations. A LLM-based conversation summarizer is adopted for query and key generation, which enables effective maximum inner product search. To avoid the extra inference cost brought by LLM-based conversation summarization, we further distill a light-weight conversation encoder which produces query embeddings without decoding summaries for test conversations. We val
    
[^33]: 用扩散语言模型进行文本引导的分子生成

    Text-Guided Molecule Generation with Diffusion Language Model

    [https://arxiv.org/abs/2402.13040](https://arxiv.org/abs/2402.13040)

    提出了一种名为Text-Guided Molecule Generation with Diffusion Language Model（TGM-DLM）的新方法，通过扩散模型进行文本引导的分子生成，在生成有效分子表示方面表现出显著的效果优于自回归模型MolT5-Base。

    

    文本引导的分子生成是一个任务，其中生成的分子与特定的文本描述相匹配。最近，大多数现有基于SMILES的分子生成方法依赖于自回归架构。在这项工作中，我们提出了一种名为Text-Guided Molecule Generation with Diffusion Language Model（TGM-DLM）的新方法，它利用扩散模型来解决自回归方法的局限性。TGM-DLM集体和迭代地更新SMILES字符串中的标记嵌入，使用两阶段扩散生成过程。第一阶段通过随机噪声从文本描述中引导来优化嵌入，而第二阶段纠正无效的SMILES字符串以形成有效的分子表示。我们证明，TGM-DLM在不需要额外数据资源的情况下，胜过了自回归模型MolT5-Base。我们的研究结果强调了TGM-DLM在生成连贯性方面的显著有效性。

    arXiv:2402.13040v1 Announce Type: cross  Abstract: Text-guided molecule generation is a task where molecules are generated to match specific textual descriptions. Recently, most existing SMILES-based molecule generation methods rely on an autoregressive architecture. In this work, we propose the Text-Guided Molecule Generation with Diffusion Language Model (TGM-DLM), a novel approach that leverages diffusion models to address the limitations of autoregressive methods. TGM-DLM updates token embeddings within the SMILES string collectively and iteratively, using a two-phase diffusion generation process. The first phase optimizes embeddings from random noise, guided by the text description, while the second phase corrects invalid SMILES strings to form valid molecular representations. We demonstrate that TGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for additional data resources. Our findings underscore the remarkable effectiveness of TGM-DLM in generating cohe
    
[^34]: SiLLM：用于同时机器翻译的大型语言模型

    SiLLM: Large Language Models for Simultaneous Machine Translation

    [https://arxiv.org/abs/2402.13036](https://arxiv.org/abs/2402.13036)

    提出了SiLLM，将SiMT任务分解为策略决策和翻译子任务，并将这两个子任务委托给独立的代理，从而将大型语言模型引入SiMT。

    

    同时机器翻译（SiMT）在阅读源句子的同时生成翻译，需要一个策略来确定读取和生成单词的最佳时机。尽管大型语言模型（LLM）在各种自然语言处理任务中取得了显著表现，但现有的SiMT方法主要集中在传统的transformers上，采用单一模型同时确定策略和生成翻译。然而，考虑到SiMT的复杂性，用单一模型有效地处理这两个任务是具有挑战性的。因此，有必要将SiMT任务分解为策略决策和翻译子任务。我们提出了SiLLM，将这两个子任务委托给独立的代理，从而将LLM纳入SiMT中。策略决策代理由传统的SiMT模型管理，负责确定翻译策略。翻译代理利用LLM的能力

    arXiv:2402.13036v1 Announce Type: new  Abstract: Simultaneous Machine Translation (SiMT) generates translations while reading the source sentence, necessitating a policy to determine the optimal timing for reading and generating words. Despite the remarkable performance achieved by Large Language Models (LLM) across various NLP tasks, existing SiMT methods predominantly focus on conventional transformers, employing a single model to concurrently determine the policy and generate the translations. However, given the complexity of SiMT, it is challenging to effectively address both tasks with a single model. Therefore, there is a need to decouple the SiMT task into policy-decision and translation sub-tasks. We propose SiLLM, which delegates the two sub-tasks to separate agents, thereby incorporating LLM into SiMT. The policy-decision agent is managed by a conventional SiMT model, responsible for determining the translation policy. The translation agent, leveraging the capabilities of LLM
    
[^35]: 学习检查：释放大型语言模型自我校正的潜力

    Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models

    [https://arxiv.org/abs/2402.13035](https://arxiv.org/abs/2402.13035)

    通过精心设计训练数据和构建检查-校正数据集，本研究增强了大型语言模型的自我校正能力，提高了自我校正的准确性。

    

    大型语言模型（LLMs）在推理能力方面取得了显著进展，不断努力通过自我校正来完善推理。然而，最近的研究表明，没有外部准确知识的自我校正可能存在局限性甚至可能适得其反，这就引发了关于自我校正的限制和有效性的疑问。本文旨在通过精心设计训练数据来增强LLM的自检功能，从而提高自我校正的准确性。我们对数学推理中的错误类型进行了详细分析，并开发了一个量身定制的提示，称为“Step CoT Check”。然后我们构建了一个检查-校正数据集用于训练模型。在将原始CoT数据和检查校正数据整合后进行训练，我们观察到模型可以改善其自检能力，从而提高其自我校正能力并消除了需要

    arXiv:2402.13035v1 Announce Type: cross  Abstract: Large language models (LLMs) have made significant strides in reasoning capabilities, with ongoing efforts to refine their reasoning through self-correction. However, recent studies suggest that self-correction can be limited or even counterproductive without external accurate knowledge, raising questions about the limits and effectiveness of self-correction. In this paper, we aim to enhance LLM's self-checking capabilities by meticulously designing training data, thereby improving the accuracy of self-correction. We conduct a detailed analysis of error types in mathematical reasoning and develop a tailored prompt, termed ``Step CoT Check''. Then we construct a checking-correction dataset for training models. After integrating the original CoT data and checking-correction data for training, we observe that models could improve their self-checking capabilities, thereby enhancing their self-correction capacity and eliminating the need fo
    
[^36]: 异质图推理用于文本和表格事实核查

    Heterogeneous Graph Reasoning for Fact Checking over Texts and Tables

    [https://arxiv.org/abs/2402.13028](https://arxiv.org/abs/2402.13028)

    提出了一种基于异质图的模型HeterFC，用于文本和表格中的事实核查，利用异质证据图和关系图神经网络进行信息传播。

    

    事实核查旨在通过推理多个证据以预测声明的真实性。本文专注于后者，即推理关于非结构化文本和结构化表格信息的真实性。我们提出了一种新颖的基于异质图的词级模型HeterFC，用于事实核查。我们的方法利用了一个异质证据图，以单词为节点，并且设计了代表不同证据属性的边缘，通过关系图神经网络进行信息传播，促进声明之间的交互。

    arXiv:2402.13028v1 Announce Type: cross  Abstract: Fact checking aims to predict claim veracity by reasoning over multiple evidence pieces. It usually involves evidence retrieval and veracity reasoning. In this paper, we focus on the latter, reasoning over unstructured text and structured table information. Previous works have primarily relied on fine-tuning pretrained language models or training homogeneous-graph-based models. Despite their effectiveness, we argue that they fail to explore the rich semantic information underlying the evidence with different structures. To address this, we propose a novel word-level Heterogeneous-graph-based model for Fact Checking over unstructured and structured information, namely HeterFC. Our approach leverages a heterogeneous evidence graph, with words as nodes and thoughtfully designed edges representing different evidence properties. We perform information propagation via a relational graph neural network, facilitating interactions between claim
    
[^37]: CFEVER：一个用于汉语事实提取和验证的数据集

    CFEVER: A Chinese Fact Extraction and VERification Dataset

    [https://arxiv.org/abs/2402.13025](https://arxiv.org/abs/2402.13025)

    CFEVER是一个为事实提取和验证而设计的汉语数据集，提供了严格的标准和对应证据，可用于开发自动化系统，减轻人工核查的工作量。

    

    我们介绍了CFEVER，这是一个专为事实提取和验证而设计的汉语数据集。CFEVER包括30,012个基于中文维基百科内容手动创建的声明。每个CFEVER中的声明都标记为“支持”、“反驳”或“信息不足”，以描述其事实程度。类似于FEVER数据集，支持和反驳类别中的声明也标有对应的证据句，这些证据句取自中文维基百科的单个或多个页面。我们的标记数据集在五路标注者间一致性方面具有0.7934的Fleiss' kappa值。此外，通过对FEVER数据集上开发的最先进方法以及对CFEVER的简单基准的实验，我们证明了我们的数据集是一个新的苛刻事实提取和验证基准，可进一步用于开发自动化系统，减轻人类事实核查的工作量。CFEVER可在网址处获得。

    arXiv:2402.13025v1 Announce Type: cross  Abstract: We present CFEVER, a Chinese dataset designed for Fact Extraction and VERification. CFEVER comprises 30,012 manually created claims based on content in Chinese Wikipedia. Each claim in CFEVER is labeled as "Supports", "Refutes", or "Not Enough Info" to depict its degree of factualness. Similar to the FEVER dataset, claims in the "Supports" and "Refutes" categories are also annotated with corresponding evidence sentences sourced from single or multiple pages in Chinese Wikipedia. Our labeled dataset holds a Fleiss' kappa value of 0.7934 for five-way inter-annotator agreement. In addition, through the experiments with the state-of-the-art approaches developed on the FEVER dataset and a simple baseline for CFEVER, we demonstrate that our dataset is a new rigorous benchmark for factual extraction and verification, which can be further used for developing automated systems to alleviate human fact-checking efforts. CFEVER is available at htt
    
[^38]: SoMeLVLM: 用于社交媒体处理的大型视觉语言模型

    SoMeLVLM: A Large Vision Language Model for Social Media Processing

    [https://arxiv.org/abs/2402.13022](https://arxiv.org/abs/2402.13022)

    SoMeLVLM是一种用于社交媒体处理的大型视觉语言模型，具有知识理解、应用、分析、评价和创造等五大关键能力，致力于理解和生成逼真的社交媒体行为。

    

    社交媒体的增长以其多模式特性为特征，引发了各种现象和挑战的出现，这需要有效的方法来统一解决自动化任务。强大的大型视觉语言模型使同时处理各种任务成为可能，但即使通过精心设计的提示方法，通用领域模型在与社交媒体任务的独特言语风格和语境进行对齐方面仍有不足。本文介绍了一种用于社交媒体处理的大型视觉语言模型（SoMeLVLM），它是一个配备有知识和理解、应用、分析、评价和创造五个关键能力的认知框架。SoMeLVLM被设计为理解和生成逼真的社交媒体行为。我们开发了一个654k的多模式社交媒体指导调优数据集，以支持我们的认知框架和对模型进行微调。

    arXiv:2402.13022v1 Announce Type: new  Abstract: The growth of social media, characterized by its multimodal nature, has led to the emergence of diverse phenomena and challenges, which calls for an effective approach to uniformly solve automated tasks. The powerful Large Vision Language Models make it possible to handle a variety of tasks simultaneously, but even with carefully designed prompting methods, the general domain models often fall short in aligning with the unique speaking style and context of social media tasks. In this paper, we introduce a Large Vision Language Model for Social Media Processing (SoMeLVLM), which is a cognitive framework equipped with five key capabilities including knowledge & comprehension, application, analysis, evaluation, and creation. SoMeLVLM is designed to understand and generate realistic social media behavior. We have developed a 654k multimodal social media instruction-tuning dataset to support our cognitive framework and fine-tune our model. Ou
    
[^39]: 理解多语言微调中特定语言不平衡的影响

    Understanding the effects of language-specific class imbalance in multilingual fine-tuning

    [https://arxiv.org/abs/2402.13016](https://arxiv.org/abs/2402.13016)

    微调transformer模型时，针对多语言数据集中个别语言标签不平衡的问题，通过为每种语言分别计算类别权重，可以缓解性能下降、语言分离更明显和无信息特征促进等不良影响。

    

    我们研究了现实生活中多语言分类数据集中经常存在的一种不平衡类型的影响：跨语言标签的分布不均匀。我们展示了微调基于transformer的大型语言模型(LLM)在具有这种不平衡的数据集上会导致性能下降，潜在空间中语言分离更加明显，并促进了无信息特征的生成。我们修改了传统的类别加权方法，通过分别计算每种语言的类别权重来缓解这些不利影响。这些结果意识到了多语言微调中特定语言不平衡的负面影响，以及模型学习依赖语言分离来执行任务的方式。

    arXiv:2402.13016v1 Announce Type: new  Abstract: We study the effect of one type of imbalance often present in real-life multilingual classification datasets: an uneven distribution of labels across languages. We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features. We modify the traditional class weighing approach to imbalance by calculating class weights separately for each language and show that this helps mitigate those detrimental effects. These results create awareness of the negative effects of language-specific class imbalance in multilingual fine-tuning and the way in which the model learns to rely on the separation of languages to perform the task.
    
[^40]: 代码需要注释：使用注释增强代码LLMs

    Code Needs Comments: Enhancing Code LLMs with Comment Augmentation

    [https://arxiv.org/abs/2402.13013](https://arxiv.org/abs/2402.13013)

    通过引入新的数据增强方法，为现有代码生成注释，并利用数据过滤策略来提高以代码为焦点的语言模型的性能。

    

    编程技能是大型语言模型（LLMs）的一个重要能力，需要深入理解编程语言（PLs）及其与自然语言（NLs）的相关性。我们通过评估注释密度作为PL-NL对齐的衡量标准，来研究预训练数据对以代码为焦点的LLMs性能的影响。由于预训练语料库中缺乏与代码注释对齐的数据，我们引入了一种新的数据增强方法，为现有代码生成注释，同时采用数据过滤策略来滤除与自然语言关联性较差的代码数据。我们在三个以代码为焦点的LLMs上进行了实验，并观察到在两个广泛使用的编程技能基准测试中性能的一致提升。值得注意的是，在增强数据上训练的模型优于生成注释的模型以及在没有增强数据的情况下进一步训练的模型。

    arXiv:2402.13013v1 Announce Type: new  Abstract: The programming skill is one crucial ability for Large Language Models (LLMs), necessitating a deep understanding of programming languages (PLs) and their correlation with natural languages (NLs). We examine the impact of pre-training data on code-focused LLMs' performance by assessing the comment density as a measure of PL-NL alignment. Given the scarcity of code-comment aligned data in pre-training corpora, we introduce a novel data augmentation method that generates comments for existing code, coupled with a data filtering strategy that filters out code data poorly correlated with natural language. We conducted experiments on three code-focused LLMs and observed consistent improvements in performance on two widely-used programming skill benchmarks. Notably, the model trained on the augmented data outperformed both the model used for generating comments and the model further trained on the data without augmentation.
    
[^41]: 探究模型不稳定性对解释和不确定性的影响

    Investigating the Impact of Model Instability on Explanations and Uncertainty

    [https://arxiv.org/abs/2402.13006](https://arxiv.org/abs/2402.13006)

    模型稳定性对解释和不确定性的影响进行了调查，并发现实际扰动对性能和解释影响较小，但掩盖却有 drastical 影响。

    

    可解释的AI方法有助于理解模型行为，然而，对输入进行微小、不可察觉的扰动可能会极大地扭曲解释。这些解释通常在模型部署之前被全面评估，因此很难评估特定解释的可信度。一些研究已经尝试为解释创建置信度估计器，但没有人调查不确定性和解释质量之间的现有联系。我们通过在推断时引入噪声来人为模拟文本输入中的认识不确定性。在这项大规模实证研究中，我们插入不同级别的噪声扰动，并测量对预训练语言模型的输出和不同不确定性度量的影响。实际扰动对性能和解释的影响很小，然而掩盖却有 drastical 影响。我们发现高不确定性并不一定意味着解释不佳。

    arXiv:2402.13006v1 Announce Type: cross  Abstract: Explainable AI methods facilitate the understanding of model behaviour, yet, small, imperceptible perturbations to inputs can vastly distort explanations. As these explanations are typically evaluated holistically, before model deployment, it is difficult to assess when a particular explanation is trustworthy. Some studies have tried to create confidence estimators for explanations, but none have investigated an existing link between uncertainty and explanation quality. We artificially simulate epistemic uncertainty in text input by introducing noise at inference time. In this large-scale empirical study, we insert different levels of noise perturbations and measure the effect on the output of pre-trained language models and different uncertainty metrics. Realistic perturbations have minimal effect on performance and explanations, yet masking has a drastic effect. We find that high uncertainty doesn't necessarily imply low explanation 
    
[^42]: 方言之间的音位结构复杂性

    Phonotactic Complexity across Dialects

    [https://arxiv.org/abs/2402.12998](https://arxiv.org/abs/2402.12998)

    通过研究荷兰方言和闽方言，发现了方言层面上词长和音位结构复杂性之间的权衡关系，拓展了以往仅在语言层面记录的研究结果。

    

    语言学类型学中的共识认为，如果一种语言的结构在某个维度变得更加复杂，那么在另一个维度上会变得更简单，这是建立在所有语言同等复杂的假设之上的（Joseph and Newmeyer，2012）。我们在微观层面研究了这一说法，利用了一组严格控制的荷兰方言（来自366个收集点）和闽方言（来自60个点）的样本，这使得在各种语言变体之间进行更公平的比较成为可能。甚至在方言层面上，我们发现实证证据表明，在基于LSTM的手机级语言模型中，词长和音位结构复杂性的计算度量之间存在一种权衡——这一结果之前仅在语言层面上有记录。广义加性模型（GAM）显示，音位结构复杂性较低的方言集中在首都地区周围，我们假设这可能对应于之前的假设，即规模更大或更多样化的语言变体会显示出

    arXiv:2402.12998v1 Announce Type: new  Abstract: Received wisdom in linguistic typology holds that if the structure of a language becomes more complex in one dimension, it will simplify in another, building on the assumption that all languages are equally complex (Joseph and Newmeyer, 2012). We study this claim on a micro-level, using a tightly-controlled sample of Dutch dialects (across 366 collection sites) and Min dialects (across 60 sites), which enables a more fair comparison across varieties. Even at the dialect level, we find empirical evidence for a tradeoff between word length and a computational measure of phonotactic complexity from a LSTM-based phone-level language model-a result previously documented only at the language level. A generalized additive model (GAM) shows that dialects with low phonotactic complexity concentrate around the capital regions, which we hypothesize to correspond to prior hypotheses that language varieties of greater or more diverse populations show
    
[^43]: 朝着可信的再排序：一种简单但有效的弃权机制

    Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism

    [https://arxiv.org/abs/2402.12997](https://arxiv.org/abs/2402.12997)

    提出了一种适用于现实约束的轻量级弃权机制，特别适用于再排序阶段，通过数据驱动的方法达到有效性，并提供了开源代码以促进其更广泛的应用。

    

    神经信息检索（NIR）已经显著改进了基于启发式的IR系统。然而，失败仍然频繁发生，通常所使用的模型无法检索与用户查询相关的文档。我们通过提出一种适用于现实约束的轻量级弃权机制来解决这一挑战，特别强调再排序阶段。我们介绍了一个协议，用于在黑匣子场景中评估弃权策略的效果，并提出了一种简单但有效的数据驱动机制。我们提供了实验复制和弃权实施的开源代码，促进其在不同环境中更广泛的采用和应用。

    arXiv:2402.12997v1 Announce Type: cross  Abstract: Neural Information Retrieval (NIR) has significantly improved upon heuristic-based IR systems. Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user's query. We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the reranking phase. We introduce a protocol for evaluating abstention strategies in a black-box scenario, demonstrating their efficacy, and propose a simple yet effective data-driven mechanism. We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts.
    
[^44]: TRAP: 面向黑盒身份验证的有针对性随机对抗提示诱饵

    TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification

    [https://arxiv.org/abs/2402.12991](https://arxiv.org/abs/2402.12991)

    TRAP提出了一种名为Targeted Random Adversarial Prompt (TRAP)的方法，用于识别特定LLM的使用，并在单次交互后以超过95%的真阳性率和低于0.2%的误报率检测目标LLMs。

    

    大型语言模型（LLM）服务和模型通常伴随着关于谁可以使用它们以及他们必须如何使用它们的法律规定。评估发布的LLMs的合规性是至关重要的，因为这些规定保护了LLM贡献者的利益并防止了滥用。在这种背景下，我们描述了黑盒身份验证（BBIV）的新问题。其目标是确定第三方应用是否通过其聊天功能使用某个特定的LLM。我们提出了一种名为目标随机对抗提示（TRAP）的方法，用于识别正在使用的具体LLM。我们重新利用了最初用于越狱的对抗性后缀，以从目标LLM获得预定义的答案，而其他模型则给出随机答案。TRAP可以在单次交互后以超过95%的真阳性率和低于0.2%的误报率检测目标LLMs。即使LLM有不会显著改变的细微变化，TRAP仍然有效。

    arXiv:2402.12991v1 Announce Type: cross  Abstract: Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them. Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse. In this context, we describe the novel problem of Black-box Identity Verification (BBIV). The goal is to determine whether a third-party application uses a certain LLM through its chat function. We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers. TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the LLM has minor changes that do not significantly alter the
    
[^45]: GNN是否可以成为LLMs的良好适配器？

    Can GNN be Good Adapter for LLMs?

    [https://arxiv.org/abs/2402.12984](https://arxiv.org/abs/2402.12984)

    本文提出了GraphAdapter，利用图神经网络（GNN）作为高效适配器，与LLMs协同处理文本属性图（TAGs）。

    

    最近，大型语言模型（LLMs）在理解文本数据和零次学习方面展示了出色的能力，为许多与文本相关的领域带来了显著进展。本文探讨了如何利用LLMs来建模文本属性图（TAGs）。我们提出了GraphAdapter，它使用图神经网络（GNN）作为LLMs的高效适配器，以应对TAGs。

    arXiv:2402.12984v1 Announce Type: cross  Abstract: Recently, large language models (LLMs) have demonstrated superior capabilities in understanding and zero-shot learning on textual data, promising significant advances for many text-related domains. In the graph domain, various real-world scenarios also involve textual data, where tasks and node features can be described by text. These text-attributed graphs (TAGs) have broad applications in social media, recommendation systems, etc. Thus, this paper explores how to utilize LLMs to model TAGs. Previous methods for TAG modeling are based on million-scale LMs. When scaled up to billion-scale LLMs, they face huge challenges in computational costs. Additionally, they also ignore the zero-shot inference capabilities of LLMs. Therefore, we propose GraphAdapter, which uses a graph neural network (GNN) as an efficient adapter in collaboration with LLMs to tackle TAGs. In terms of efficiency, the GNN adapter introduces only a few trainable param
    
[^46]: 示范对多语境学习的影响：多维分析

    The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis

    [https://arxiv.org/abs/2402.12976](https://arxiv.org/abs/2402.12976)

    通过多维分析发现，示范的有效性在多语种上下文学习中存在显著差异，其中部分模型对示范质量不敏感，而精心设计的模板可以消除对示范的依赖。

    

    在上下文学习中，示范经常被用作推理策略，在此策略中，大型语言模型仅使用少量标记的示范来解决任务，而无需进行任何参数更新。与单语言（英语）上下文学习的研究相比，多语种上下文学习尚未得到充分探讨，我们缺乏对该环境中示范作用的深入理解。为了填补这一空白，我们对多语境学习进行了多维分析，实验采用了来自不同模型家族的5个模型，涵盖了包括分类和生成任务在内的9个数据集，覆盖了56种类型上不同的语言。我们的结果表明，示范的有效性在模型、任务和语言之间存在显著差异。我们还发现，Llama 2-Chat、GPT-3.5和GPT-4对示范质量的敏感度较低。相反，精心设计的模板往往会消除一些模型对示范的益处。

    arXiv:2402.12976v1 Announce Type: cross  Abstract: In-context learning is a popular inference strategy where large language models solve a task using only a few labelled demonstrations without needing any parameter updates. Compared to work on monolingual (English) in-context learning, multilingual in-context learning is under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some t
    
[^47]: Gl'orIA - 一种用于葡萄牙语的生成式开放大型语言模型

    Gl\'orIA - A Generative and Open Large Language Model for Portuguese

    [https://arxiv.org/abs/2402.12969](https://arxiv.org/abs/2402.12969)

    Gl'orIA是一种专门为欧洲葡萄牙语设计的强大解码器大型语言模型，通过对350亿个tokens的全面PT-PT文本语料库预训练，为欧洲葡萄牙语提供了解决方案，并引入了CALAME-PT，这是第一个葡萄牙语零样本语言建模基准。

    

    自然语言任务取得了显著进展，这在很大程度上归因于强大的大型语言模型（LLMs）的出现。尽管许多高资源语言都有丰富的LLM，但欧洲葡萄牙语的这种模型仍然有限。我们介绍了一种强大的欧洲葡萄牙语解码器LLM——Gl'orIA。为了对Gl'orIA进行预训练，我们收集了一个包含来自各个来源的350亿个tokens的全面PT-PT文本语料库。我们展示了我们的预训练方法，然后评估了模型在多个下游任务上的有效性。此外，为了评估我们模型的语言建模能力，我们引入了CALAME-PT（葡萄牙语零样本语言建模基准），这是第一个葡萄牙语零样本语言建模基准。

    arXiv:2402.12969v1 Announce Type: cross  Abstract: Significant strides have been made in natural language tasks, largely attributed to the emergence of powerful large language models (LLMs). These models, pre-trained on extensive and diverse corpora, have become increasingly capable of comprehending the intricacies of language. Despite the abundance of LLMs for many high-resource languages, the availability of such models remains limited for European Portuguese. We introduce Gl\'orIA, a robust European Portuguese decoder LLM. To pre-train Gl\'orIA, we assembled a comprehensive PT-PT text corpus comprising 35 billion tokens from various sources. We present our pre-training methodology, followed by an assessment of the model's effectiveness on multiple downstream tasks. Additionally, to evaluate our models' language modeling capabilities, we introduce CALAME-PT (Context-Aware LAnguage Modeling Evaluation for Portuguese), the first Portuguese zero-shot language-modeling benchmark. Evaluat
    
[^48]: 大语言模型的提示窃取攻击

    Prompt Stealing Attacks Against Large Language Models

    [https://arxiv.org/abs/2402.12959](https://arxiv.org/abs/2402.12959)

    提出了一种新型攻击，即提示窃取攻击，目的是基于生成的答案窃取设计良好的提示。

    

    越来越多领域对大语言模型（LLMs）如ChatGPT的依赖强调了“提示工程”的重要性，这是一种改进模型输出质量的技术。本文提出了一种针对LLMs的新型攻击，称为提示窃取攻击。我们的提示窃取攻击旨在基于生成的答案来窃取这些设计良好的提示。提示窃取攻击包括两个主要模块：参数提取器和提示重构。

    arXiv:2402.12959v1 Announce Type: cross  Abstract: The increasing reliance on large language models (LLMs) such as ChatGPT in various fields emphasizes the importance of ``prompt engineering,'' a technology to improve the quality of model outputs. With companies investing significantly in expert prompt engineers and educational resources rising to meet market demand, designing high-quality prompts has become an intriguing challenge. In this paper, we propose a novel attack against LLMs, named prompt stealing attacks. Our proposed prompt stealing attack aims to steal these well-designed prompts based on the generated answers. The prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstruction. The goal of the parameter extractor is to figure out the properties of the original prompts. We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt. Our parameter extractor first tries
    
[^49]: GumbelSoft: 通过GumbelMax技巧实现多样化的语言模型数字水印

    GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick

    [https://arxiv.org/abs/2402.12948](https://arxiv.org/abs/2402.12948)

    通过开发 GumbelSoft 水印，我们提出了一种能够在高多样性环境中增强生成文本多样性的解决方案，相较于其他方案表现更为优秀。

    

    大型语言模型(Large language models, LLMs)能够生成类似人类的文本，但也引发了人们对其在虚假新闻和学术不诚实方面的担忧。解码为基础的水印，尤其是基于GumbelMax技巧的水印(GM水印)，是防范机器生成文本滥用的杰出解决方案，因其显著的可检测性而脱颖而出。然而，GM水印在生成多样性方面面临一个主要挑战，对于相同提示始终产生相同输出，从而负面影响生成多样性和用户体验。为了克服这一局限，我们提出了一种新型GM水印，即Logits-Addition水印，及其三个变体，专门设计用于增强多样性。在这些变体中，GumbelSoft水印(作为Logits-Addition水印的一个softmax变体)在高多样性环境中表现出优越性能，其AUROC分数超过其他两个替代变体0.1至0.3，并超越其他方案。

    arXiv:2402.12948v1 Announce Type: new  Abstract: Large language models (LLMs) excellently generate human-like text, but also raise concerns about misuse in fake news and academic dishonesty. Decoding-based watermark, particularly the GumbelMax-trick-based watermark(GM watermark), is a standout solution for safeguarding machine-generated texts due to its notable detectability. However, GM watermark encounters a major challenge with generation diversity, always yielding identical outputs for the same prompt, negatively impacting generation diversity and user experience. To overcome this limitation, we propose a new type of GM watermark, the Logits-Addition watermark, and its three variants, specifically designed to enhance diversity. Among these, the GumbelSoft watermark (a softmax variant of the Logits-Addition watermark) demonstrates superior performance in high diversity settings, with its AUROC score outperforming those of the two alternative variants by 0.1 to 0.3 and surpassing oth
    
[^50]: 突尼斯阿拉伯语的规范正字法

    Normalized Orthography for Tunisian Arabic

    [https://arxiv.org/abs/2402.12940](https://arxiv.org/abs/2402.12940)

    这项研究提出了专为突尼斯阿拉伯语而设计的"突尼斯阿拉伯语规范正字法"（NOTA），旨在解决用阿拉伯文拼写突尼斯阿拉伯语时所面临的挑战，确保对其独特音韵和形态特征的准确表示。

    

    突尼斯阿拉伯语（ISO 693-3：aeb）是突尼斯特有的一种语言变体，最初源自阿拉伯语，并受到多种历史影响的丰富。本研究介绍了专为使用阿拉伯文拼写突尼斯阿拉伯语而量身定制的"CODA*指南"的"突尼斯阿拉伯语规范正字法"（NOTA），重点在于用户友好性和一致性，用于语言资源开发目的。更新后的标准旨在解决与准确表现突尼斯语音韵和形态独特特征有关的挑战。这将通过纠正基于与现代标准阿拉伯语相似性的音译所引发的问题来实现。

    arXiv:2402.12940v1 Announce Type: new  Abstract: Tunisian Arabic (ISO 693-3: aeb) is a distinct linguistic variety native to Tunisia, initially stemmed from the Arabic language and enriched by a multitude of historical influences. This research introduces the "Normalized Orthography for Tunisian Arabic" (NOTA), an adaptation of CODA* guidelines tailored for transcribing Tunisian Arabic using the Arabic script for language resource development purposes, with an emphasis on user-friendliness and consistency. The updated standard seeks to address challenges related to accurately representing the unique characteristics of Tunisian phonology and morphology. This will be achieved by rectifying problems arising from transcriptions based on resemblances to Modern Standard Arabic.
    
[^51]: 基于大型语言模型的人-机协作进行复杂任务求解

    Large Language Model-based Human-Agent Collaboration for Complex Task Solving

    [https://arxiv.org/abs/2402.12914](https://arxiv.org/abs/2402.12914)

    在这项工作中，我们介绍了基于大型语言模型的人-机协作进行复杂任务求解的问题，提出了基于强化学习的人-机协作方法ReHAC，通过构建人-机协作数据集训练策略模型，验证了该模型的有效性。

    

    在最近的研究中，将大型语言模型（LLMs）整合到创建完全自主代理人中引起了极大的兴趣。然而，基于LLM的代理人经常表现出在适应动态环境和完全把握人类需求方面的明显缺点。在这项工作中，我们引入了基于LLM的人-机协作进行复杂任务求解的问题，探讨它们的协同潜力。此外，我们提出了一种基于强化学习的人-机协作方法ReHAC。该方法包括一个策略模型，旨在确定人类何时介入任务求解过程中最合适的阶段。我们构建了一个人-机协作数据集，用于在离线强化学习环境中训练这个策略模型。我们的验证测试证实了模型的有效性。结果表明，人类和LLM的协同努力-

    arXiv:2402.12914v1 Announce Type: new  Abstract: In recent developments within the research community, the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest. Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs. In this work, we introduce the problem of LLM-based human-agent collaboration for complex task-solving, exploring their synergistic potential. In addition, we propose a Reinforcement Learning-based Human-Agent Collaboration method, ReHAC. This approach includes a policy model designed to determine the most opportune stages for human intervention within the task-solving process. We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment. Our validation tests confirm the model's effectiveness. The results demonstrate that the synergistic efforts of humans and LLM-
    
[^52]: OPDAI在SemEval-2024任务6中：小型LLMs可以加速幻觉检测与弱监督数据

    OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data

    [https://arxiv.org/abs/2402.12913](https://arxiv.org/abs/2402.12913)

    相对较小的LLMs可以与大型LLM相比，在幻觉检测方面达到竞争性的性能水平

    

    这篇论文主要描述了一种统一的系统，用于LLMs的幻觉检测，在SemEval-2024任务6的模型无关跟踪中赢得第二名，并且在模型感知跟踪中取得了可观的结果。该任务旨在使用LLMs检测三种不同的文本生成任务中的幻觉，而无需标记训练数据。我们利用提示工程和少样本学习来验证不同LLMs在验证数据上的性能。然后，我们选择性能更好的LLMs生成高质量的弱监督训练数据，这不仅满足了不同LLMs的一致性，还满足了最佳LLM与不同抽样参数的一致性。此外，我们通过使用构建的训练数据微调不同的LLMs，并发现相对较小的LLM在与大型LLM相比，在幻觉检测方面可以达到具有竞争力的性能水平。

    arXiv:2402.12913v1 Announce Type: new  Abstract: This paper mainly describes a unified system for hallucination detection of LLMs, which wins the second prize in the model-agnostic track of the SemEval-2024 Task 6, and also achieves considerable results in the model-aware track. This task aims to detect hallucination with LLMs for three different text-generation tasks without labeled training data. We utilize prompt engineering and few-shot learning to verify the performance of different LLMs on the validation data. Then we select the LLMs with better performance to generate high-quality weakly supervised training data, which not only satisfies the consistency of different LLMs, but also satisfies the consistency of the optimal LLM with different sampling parameters. Furthermore, we finetune different LLMs by using the constructed training data, and finding that a relatively small LLM can achieve a competitive level of performance in hallucination detection, when compared to the large 
    
[^53]: 通过语义图平滑实现更具辨别力的句子嵌入

    More Discriminative Sentence Embeddings via Semantic Graph Smoothing

    [https://arxiv.org/abs/2402.12890](https://arxiv.org/abs/2402.12890)

    通过语义图平滑技术增强预训练模型获取的句子嵌入，有效改善文本聚类和分类任务的结果。

    

    这篇论文探讨了一种经验方法，以无监督的方式学习更具辨别性的句子表示。利用语义图平滑，我们增强了从预训练模型中获取的句子嵌入，以提高文本聚类和分类任务的结果。我们的方法在八个基准上得到验证，展示了语义图平滑在改善用于监督和无监督文档分类任务的句子嵌入中的潜力。

    arXiv:2402.12890v1 Announce Type: new  Abstract: This paper explores an empirical approach to learn more discriminantive sentence representations in an unsupervised fashion. Leveraging semantic graph smoothing, we enhance sentence embeddings obtained from pretrained models to improve results for the text clustering and classification tasks. Our method, validated on eight benchmarks, demonstrates consistent improvements, showcasing the potential of semantic graph smoothing in improving sentence embeddings for the supervised and unsupervised document categorization tasks.
    
[^54]: GRAFFORD: 用于测试语言和视觉模型对物体可供性知识的基准数据集

    GRAFFORD: A Benchmark Dataset for Testing the Knowledge of Object Affordances of Language and Vision Models

    [https://arxiv.org/abs/2402.12881](https://arxiv.org/abs/2402.12881)

    该论文提出了一个名为GRAFFORD的基准数据集，用于测试语言和视觉模型对物体可供性知识的表现，实验结果显示当前预训练语言模型在理解不常见物体可供性方面存在推理能力的局限。

    

    我们调查了预训练语言模型（LMs）和预训练视觉-语言模型（VLMs）中关于物体可供性的知识。基于Transformer的大型预训练语言模型（PTLM）从大量未标记文本中学习上下文表示，并在下游NLU任务中表现出色。与此同时，越来越多的文献表明，PTLM在推理和基础方面存在不一致且不直观的失败。为了首次定量衡量基础（或缺乏）的影响，我们精心策划了一个关于物体可供性的新颖而全面的数据集-- GrAFFORD，包含15个可供性类别。与视觉和语言领域收集的可供性数据集不同，我们用现场句子标注了对象和可供性。实验结果显示，当涉及不常见的物体可供性时，PTLM表现出有限的推理能力。我们还观察到PTLM在理解不常见物体可供性时存在困难。

    arXiv:2402.12881v1 Announce Type: new  Abstract: We investigate the knowledge of object affordances in pre-trained language models (LMs) and pre-trained Vision-Language models (VLMs). Transformers-based large pre-trained language models (PTLM) learn contextual representation from massive amounts of unlabeled text and are shown to perform impressively in downstream NLU tasks. In parallel, a growing body of literature shows that PTLMs fail inconsistently and non-intuitively, showing a lack of reasoning and grounding. To take a first step toward quantifying the effect of grounding (or lack thereof), we curate a novel and comprehensive dataset of object affordances -- GrAFFORD, characterized by 15 affordance classes. Unlike affordance datasets collected in vision and language domains, we annotate in-the-wild sentences with objects and affordances. Experimental results reveal that PTLMs exhibit limited reasoning abilities when it comes to uncommon object affordances. We also observe that pr
    
[^55]: 自闭症言语检测 - 一项调查

    Autism Detection in Speech - A Survey

    [https://arxiv.org/abs/2402.12880](https://arxiv.org/abs/2402.12880)

    该研究综合分析了自闭症在语音、语言和言语中的表现特征，提出了可能指示自闭症的语言、韵律和声学线索的调查结果，并指出了在自闭症检测中存在的问题及可行的解决方案。

    

    研究范围涵盖了自闭症在声音、语言和言语中的表现。我们分析了来自生物医学、心理学领域以及自然语言处理领域的研究，以找出可能指示自闭症的语言、韵律和声学线索。我们的调查涵盖了这三个领域。我们定义了自闭症，以及可能影响正确检测该疾病的共病症。我们特别关注了词汇和语义流利性、韵律特征，以及不连贯性和说话速度等观察结果。我们还展示了基于单词的方法，并描述了在音频数据和文本转录上都使用的基于机器学习和变压器的方法。最后，我们得出结论，尽管已经有很多研究，但女性患者似乎受到严重的研究不足。此外，大多数自然语言处理研究都集中在传统的机器学习方法上，而不是变压器，在这方面可能是有益的。

    arXiv:2402.12880v1 Announce Type: new  Abstract: There has been a range of studies of how autism is displayed in voice, speech, and language. We analyse studies from the biomedical, as well as the psychological domain, but also from the NLP domain in order to find linguistic, prosodic and acoustic cues that could indicate autism. Our survey looks at all three domains. We define autism and which comorbidities might influence the correct detection of the disorder. We especially look at observations such as verbal and semantic fluency, prosodic features, but also disfluencies and speaking rate. We also show word-based approaches and describe machine learning and transformer-based approaches both on the audio data as well as the transcripts. Lastly, we conclude, while there already is a lot of research, female patients seem to be severely under-researched. Also, most NLP research focuses on traditional machine learning methods instead of transformers which could be beneficial in this conte
    
[^56]: 探索表格转文本方法对增强基于LLM的问答系统在领域混合数据上的影响

    Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data

    [https://arxiv.org/abs/2402.12869](https://arxiv.org/abs/2402.12869)

    本文探索了如何将表格转文本方法集成到LLM问答系统中，通过实验发现不同的表格转文本方法对QA系统性能的影响。

    

    大型语言模型（LLMs）在问答系统（QA）中使用领域特定数据进行增强已经引起了广泛关注。然而，领域数据通常以混合格式存在，包括文本和半结构化表格，对信息的无缝整合提出了挑战。表格转文本生成是一个有前途的解决方案，它通过将混合数据转化为统一文本格式的语料库。尽管这种技术已经被自然语言处理（NLP）社区广泛研究，但目前还没有比较分析不同表格转文本方法生成的语料库对QA系统性能的影响。本文分两步解决了这一研究空白。首先，我们将表格转文本生成创新地集成到增强基于LLM的QA系统与领域混合数据框架中。然后，我们利用这个框架在真实工业数据中进行了广泛实验，对两种类型的QA系统进行了测试（

    arXiv:2402.12869v1 Announce Type: new  Abstract: Augmenting Large Language Models (LLMs) for Question Answering (QA) with domain specific data has attracted wide attention. However, domain data often exists in a hybrid format, including text and semi-structured tables, posing challenges for the seamless integration of information. Table-to-Text Generation is a promising solution by facilitating the transformation of hybrid data into a uniformly text-formatted corpus. Although this technique has been widely studied by the NLP community, there is currently no comparative analysis on how corpora generated by different table-to-text methods affect the performance of QA systems. In this paper, we address this research gap in two steps. First, we innovatively integrate table-to-text generation into the framework of enhancing LLM-based QA systems with domain hybrid data. Then, we utilize this framework in real-world industrial data to conduct extensive experiments on two types of QA systems (
    
[^57]: 反向镜头：将语言模型梯度投影到词汇空间中

    Backward Lens: Projecting Language Model Gradients into the Vocabulary Space

    [https://arxiv.org/abs/2402.12865](https://arxiv.org/abs/2402.12865)

    将语言模型梯度投影到词汇空间中，挖掘信息在LMs内部的流动方式，探索新信息如何存储在LMs的神经元中。

    

    了解基于Transformer的语言模型(LMs)如何学习和记忆信息是深度学习社区的一个重要目标。最近的可解释性方法将从前向传播中获得的权重和隐藏状态投影到模型的词汇表中，有助于揭示LMs内部信息流动的方式。在这项工作中，我们将这种方法扩展到LMs的后向传播和梯度。我们首先证明梯度矩阵可以被表示为其前向和后向传播输入的低秩线性组合。然后我们开发方法将这些梯度投影到词汇项中，并探讨新信息如何存储在LMs的神经元中的机制。

    arXiv:2402.12865v1 Announce Type: cross  Abstract: Understanding how Transformer-based Language Models (LMs) learn and recall information is a key goal of the deep learning community. Recent interpretability methods project weights and hidden states obtained from the forward pass to the models' vocabularies, helping to uncover how information flows within LMs. In this work, we extend this methodology to LMs' backward pass and gradients. We first prove that a gradient matrix can be cast as a low-rank linear combination of its forward and backward passes' inputs. We then develop methods to project these gradients into vocabulary items and explore the mechanics of how new information is stored in the LMs' neurons.
    
[^58]: 处理情绪中的歧义：从领域外检测到分布估计

    Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation

    [https://arxiv.org/abs/2402.12862](https://arxiv.org/abs/2402.12862)

    本论文通过将模糊情绪表达检测为领域外样本，并将情绪表示为分布来有效处理情绪中的歧义问题。

    

    情绪的主观感知导致人类注释者之间的标签不一致。在训练情绪分类器时，通常会排除缺乏多数一致标签的话语，这在测试过程中遇到模糊的情绪表达时会造成问题。本文探讨了三种处理模糊情绪的方法。首先，我们展示了将缺乏多数一致标签的话语纳入分类器作为另一类会降低其他情绪类别的分类性能。然后，我们提出使用证据深度学习来量化情绪分类中的不确定性，将具有模糊情绪的话语检测为领域外样本。这种方法在保持分类准确性的同时有效检测模糊情绪表达。此外，为了在模糊情绪中获得细粒度区别，我们提出将情绪表示为分布而不是

    arXiv:2402.12862v1 Announce Type: new  Abstract: The subjective perception of emotion leads to inconsistent labels from human annotators. Typically, utterances lacking majority-agreed labels are excluded when training an emotion classifier, which cause problems when encountering ambiguous emotional expressions during testing. This paper investigates three methods to handle ambiguous emotion. First, we show that incorporating utterances without majority-agreed labels as an additional class in the classifier reduces the classification performance of the other emotion classes. Then, we propose detecting utterances with ambiguous emotions as out-of-domain samples by quantifying the uncertainty in emotion classification using evidential deep learning. This approach retains the classification accuracy while effectively detects ambiguous emotion expressions. Furthermore, to obtain fine-grained distinctions among ambiguous emotions, we propose representing emotion as a distribution instead of 
    
[^59]: MoELoRA：对大型语言模型进行对比学习引导下的参数高效微调的混合专家模型

    MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models

    [https://arxiv.org/abs/2402.12851](https://arxiv.org/abs/2402.12851)

    提出了一种利用对比学习引导的混合专家模型MoELoRA解决大型语言模型进行参数高效微调时的灵活组合挑战

    

    微调经常是必要的，以增强大型语言模型（LLM）适应下游任务的能力。然而，更新数十亿参数的过程需要大量的计算资源和训练时间，这对大规模模型在各种场景中的广泛应用构成了重大障碍。为了解决这一问题，最近的研究中出现了参数高效微调（PEFT）作为一个杰出的范式。然而，目前采用有限全局参数集的PEFT方法（如LoRA，将低秩逼近矩阵添加到所有权重）在灵活组合下游任务中的不同计算模块方面面临挑战。在本研究中，我们引入了一种新的PEFT方法：MoELoRA。我们将LoRA视为专家混合（MoE），为了减轻MoE中观察到的随机路由现象，我们提出了利用对比学习来鼓励专家学习。

    arXiv:2402.12851v1 Announce Type: new  Abstract: Fine-tuning is often necessary to enhance the adaptability of Large Language Models (LLM) to downstream tasks. Nonetheless, the process of updating billions of parameters demands significant computational resources and training time, which poses a substantial obstacle to the widespread application of large-scale models in various scenarios. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) has emerged as a prominent paradigm in recent research. However, current PEFT approaches that employ a limited set of global parameters (such as LoRA, which adds low-rank approximation matrices to all weights) face challenges in flexibly combining different computational modules in downstream tasks. In this work, we introduce a novel PEFT method: MoELoRA. We consider LoRA as Mixture of Experts (MoE), and to mitigate the random routing phenomenon observed in MoE, we propose the utilization of contrastive learning to encourage experts to lear
    
[^60]: 调整过的语言模型更好的知识学习者

    Instruction-tuned Language Models are Better Knowledge Learners

    [https://arxiv.org/abs/2402.12847](https://arxiv.org/abs/2402.12847)

    通过在持续预训练文档之前暴露LLM到问题-答案对，以便从复杂文档中编码知识，可以更好地适应知识访问方式。

    

    为了使基于大语言模型（LLM）的助手能够有效地适应不断发展的信息需求，必须能够通过持续在新数据上训练来更新它们的事实知识。传统做法涉及在新文档上持续预培训，然后根据问题-答案（QA）对进行指导调整。

    arXiv:2402.12847v1 Announce Type: cross  Abstract: In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a met
    
[^61]: ICON：通过病变感知混合增强改善放射学报告生成的报告间一致性

    ICON: Improving Inter-Report Consistency of Radiology Report Generation via Lesion-aware Mix-up Augmentation

    [https://arxiv.org/abs/2402.12844](https://arxiv.org/abs/2402.12844)

    本文提出的ICON方法旨在通过改善放射学报告生成的报告间一致性，提升系统捕捉语义等效病变相似性的能力。

    

    放射学报告生成的先前研究在增加生成报告的临床准确性方面取得了显著进展。本文强调了其应具备的另一个至关重要的特质，即报告间一致性，指的是对语义上等效的X射线照片生成一致性报告的能力。ICON提出了一种方法，它通过改善放射学报告生成的报告间一致性来解决这一问题。

    arXiv:2402.12844v1 Announce Type: cross  Abstract: Previous research on radiology report generation has made significant progress in terms of increasing the clinical accuracy of generated reports. In this paper, we emphasize another crucial quality that it should possess, i.e., inter-report consistency, which refers to the capability of generating consistent reports for semantically equivalent radiographs. This quality is even of greater significance than the overall report accuracy in terms of ensuring the system's credibility, as a system prone to providing conflicting results would severely erode users' trust. Regrettably, existing approaches struggle to maintain inter-report consistency, exhibiting biases towards common patterns and susceptibility to lesion variants. To address this issue, we propose ICON, which improves the inter-report consistency of radiology report generation. Aiming at enhancing the system's ability to capture the similarities in semantically equivalent lesion
    
[^62]: PromptKD：通过提示调整为生成语言模型提取学生友好知识的蒸馏方法

    PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning

    [https://arxiv.org/abs/2402.12842](https://arxiv.org/abs/2402.12842)

    提出了PromptKD方法，通过提示调整实现了生成语言模型提取学生友好知识的蒸馏，无需微调整整个教师模型。

    

    近期大型语言模型（LLMs）的发展引起了对推理成本的担忧，进一步增加了对模型压缩研究的需求。尽管知识蒸馏（KD）是一种突出的方法，但是针对LLMs这样的生成语言模型的KD研究相对较少，而提取适合学生的知识的方法，在分类模型的KD中表现出了良好性能，在生成语言模型中尚未被探索。为了探索这种方法，我们提出了PromptKD，一种简单而有效的方法，它利用提示调整 - 在KD中首次出现 - 使生成语言模型能够传递适合学生的知识。与先前分类工作不同，先前那些需要微调整整个教师模型以提取适合学生的知识，PromptKD通过添加少量提示标记，并仅通过学生指导调整提示来达到类似效果。

    arXiv:2402.12842v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Ex
    
[^63]: ArabicMMLU：评估阿拉伯语中的大规模多任务语言理解

    ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic

    [https://arxiv.org/abs/2402.12840](https://arxiv.org/abs/2402.12840)

    ArabicMMLU是针对阿拉伯语的第一个多任务语言理解基准测试，通过学校考试中收集的数据对35个模型进行全面评估，揭示了在阿拉伯语中性能改进的潜力。

    

    语言模型评估的重点已经转向推理和知识密集型任务，这得益于预训练大型模型的进展。尽管最先进的模型部分在大量阿拉伯文本上进行了训练，但由于相关数据集的有限可用性，评估它们在阿拉伯语中的性能仍然具有挑战性。为了弥合这一差距，我们提出了ArabicMMLU，这是第一个针对阿拉伯语言的多任务语言理解基准测试，其数据来自于跨越北非、黎凡特和海湾地区不同国家教育水平的学校考试。我们的数据包括40个任务和14,575个现代标准阿拉伯语（MSA）的多项选择题，通过与该地区的母语者合作精心构建。我们对35个模型的全面评估显示出相当大的改进空间，特别是在最好的开源模型中。值得注意的是BLOOMZ、mT0、LLama2和Fa。

    arXiv:2402.12840v1 Announce Type: new  Abstract: The focus of language model evaluation has transitioned towards reasoning and knowledge-intensive tasks, driven by advancements in pretraining large models. While state-of-the-art models are partially trained on large Arabic texts, evaluating their performance in Arabic remains challenging due to the limited availability of relevant datasets. To bridge this gap, we present ArabicMMLU, the first multi-task language understanding benchmark for Arabic language, sourced from school exams across diverse educational levels in different countries spanning North Africa, the Levant, and the Gulf regions. Our data comprises 40 tasks and 14,575 multiple-choice questions in Modern Standard Arabic (MSA), and is carefully constructed by collaborating with native speakers in the region. Our comprehensive evaluations of 35 models reveal substantial room for improvement, particularly among the best open-source models. Notably, BLOOMZ, mT0, LLama2, and Fa
    
[^64]: PANDA: 用于增强LLMs领域特定能力的偏好适应方法

    PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs

    [https://arxiv.org/abs/2402.12835](https://arxiv.org/abs/2402.12835)

    PANDA是一种旨在增强LLMs领域特定能力的方法，通过利用专家模型响应偏好的见解，无需微调即可实现显著改进。

    

    大型语言模型（LLMs）在各种自然语言任务中展示出相当大的能力，但它们通常无法达到特定领域最先进模型的性能水平。增强LLMs领域特定能力的一种潜在方法是使用相应的数据集对其进行微调。然而，这种方法既耗费资源又耗时，并且无法应用于封闭源商业LLMs。在本文中，我们提出了一种称为PANDA的偏好适应方法，旨在通过利用专家模型响应偏好的见解来增强LLMs的领域特定能力，而无需进行微调。我们的实验结果显示，PANDA显著提升了LLMs在文本分类和交互式决策任务上的领域特定能力。此外，具有PANDA的LLM甚至超过了专家模型

    arXiv:2402.12835v1 Announce Type: cross  Abstract: While Large language models (LLMs) have demonstrated considerable capabilities across various natural language tasks, they often fall short of the performance achieved by domain-specific state-of-the-art models. One potential approach to enhance domain-specific capabilities of LLMs involves fine-tuning them using corresponding datasets. However, this method can be both resource and time-intensive, and not applicable to closed-source commercial LLMs. In this paper, we propose Preference Adaptation for Enhancing Domain-specific Abilities of LLMs (PANDA), a method designed to augment the domain-specific capabilities of LLMs by leveraging insights from the response preference of expert models without requiring fine-tuning. Our experimental results reveal that PANDA significantly enhances the domain-specific ability of LLMs on text classification and interactive decision tasks. Moreover, LLM with PANDA even outperforms the expert model that
    
[^65]: 在摘要中识别事实不一致性：朝向大型语言模型的有效利用

    Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model

    [https://arxiv.org/abs/2402.12821](https://arxiv.org/abs/2402.12821)

    该研究提出了针对摘要中事实不一致性的解决方案：通过大型语言模型在正确的范式设计下无需训练即可解决任务，并提出了训练策略以精炼更小型的高准确性的语言模型。

    

    事实上的不一致性对抽象性摘要生成器的商业部署构成重要障碍。本研究围绕两个重要问题展开：如何最好地利用大型语言模型来检测事实不一致性，以及如何精炼一个同时具有高效性和功效性的更小型语言模型？首先提出并评估了三种零样本范式，跨越五个不同数据集：直接推理整个摘要或每个摘要窗口；通过问题生成和回答进行实体验证。实验表明，在适当的范式设计下，语言模型本身能够在无需训练的情况下解决这一任务，平均超过强大的训练基线2.8%。为进一步促进实用性，我们提出针对精炼更小的开源语言模型的训练策略，该模型可以一次性高准确地评分整个摘要，胜过零

    arXiv:2402.12821v1 Announce Type: new  Abstract: Factual inconsistency poses a significant hurdle for the commercial deployment of abstractive summarizers. Under this Large Language Model (LLM) era, this work focuses around two important questions: what is the best way to leverage LLM for factual inconsistency detection, and how could we distill a smaller LLM with both high efficiency and efficacy? Three zero-shot paradigms are firstly proposed and evaluated across five diverse datasets: direct inference on the entire summary or each summary window; entity verification through question generation and answering. Experiments suggest that LLM itself is capable to resolve this task train-free under the proper paradigm design, surpassing strong trained baselines by 2.8% on average. To further promote practical utility, we then propose training strategies aimed at distilling smaller open-source LLM that learns to score the entire summary at once with high accuracy, which outperforms the zero
    
[^66]: 微调、提示、上下文学习和指导微调：我们需要多少标记样本？

    Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?

    [https://arxiv.org/abs/2402.12819](https://arxiv.org/abs/2402.12819)

    专门模型通常只需少量标记样本（100-1000个）就能与通用模型持平甚至更好，取决于任务的复杂性和结果的变化。

    

    当解决具有有限标记数据的任务时，研究人员可以选择使用通用的大型语言模型而不进行进一步更新，或者使用少量示例来调整专门的较小模型。 当有足够的标记可用时，专门的模型在许多自然语言处理任务上表现优于通用模型。 在这项工作中，我们旨在调查专门模型需要多少标记样本才能实现这种出色的性能，同时考虑结果的变化。观察提示、上下文学习、微调和指导微调的行为，识别它们在增加不同复杂性任务的标记训练样本数量时的收支平衡点，我们发现专门模型通常只需少量样本（100-1000个）就能与通用模型持平甚至更好。 同时，所需的标记数据量强烈依赖于任务的复杂性和结果的变化。

    arXiv:2402.12819v1 Announce Type: cross  Abstract: When solving a task with limited labelled data, researchers can either use a general large language model without further update, or use the few examples to tune a specialised smaller model. When enough labels are available, the specialised models outperform the general ones on many NLP tasks. In this work, we aim to investigate how many labelled samples are required for the specialised models to achieve this superior performance, while taking the results variance into consideration. Observing the behaviour of prompting, in-context learning, fine-tuning and instruction-tuning, identifying their break-even points when increasing number of labelled training samples across three tasks of varying complexity, we find that the specialised models often need only few samples ($100-1000$) to be on par or better than the general ones. At the same time, the amount of required labelled data strongly depends on the task complexity and results varia
    
[^67]: 有限标注数据学习对随机性的敏感性：相互作用和系统选择的影响

    On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices

    [https://arxiv.org/abs/2402.12817](https://arxiv.org/abs/2402.12817)

    有限标注数据学习对随机性的敏感性，通过系统研究随机因素的影响，揭示了忽略相互作用可能导致的不一致结果。

    

    有限标注数据学习可以在标签不足时提高性能，但也对所谓的随机因素（例如数据的变化顺序）引入的无法控制的随机性敏感。我们提出了一种方法，系统地调查随机因素的影响，同时考虑它们之间的相互作用。为了测量单个随机因素的真实影响，我们的方法减轻了其他因素的影响，并观察了性能在多次运行中的变化。将我们的方法应用于7个代表性文本分类任务的上下文学习和微调方法以及3个任务的元学习，我们发现：1）现有作品中忽略随机因素之间的相互作用导致了不一致的研究结果，因为错误地归因于随机因素的影响，比如否定了一些一

    arXiv:2402.12817v1 Announce Type: cross  Abstract: While learning with limited labelled data can improve performance when the labels are lacking, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (e.g., varying order of data). We propose a method to systematically investigate the effects of randomness factors while taking the interactions between them into consideration. To measure the true effects of an individual randomness factor, our method mitigates the effects of other factors and observes how the performance varies across multiple runs. Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works caused inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consis
    
[^68]: SymBa：符号化向后推理用于多步自然语言推理

    SymBa: Symbolic Backward Chaining for Multi-step Natural Language Reasoning

    [https://arxiv.org/abs/2402.12806](https://arxiv.org/abs/2402.12806)

    SymBa提出了一种符号化向后推理方法，在多步自然语言推理中取得了显著的性能和效率提升，能够生成可解释的结构化证明。

    

    最近大型语言模型（LLMs）展示了在一系列思维提示中出色的推理能力，但忠实的多步推理依然是一个挑战。我们专注于向后推理，即通过逻辑规则递归地分解查询，直到证明为止。为了解决当前向后推理实现的局限性，我们提出了SymBa（符号化向后推理）。在SymBa中，符号化自顶向下求解器控制整个证明过程，当求解器遇到死胡同时，才调用LLM生成单个推理步骤。通过这种新颖的求解器-LLM集成，SymBa在各种多步推理基准（ProofWriter，Birds-Electricity，GSM8k，CLUTRR-TF，ECtHR Article 6）中相比向后推理基线取得了性能、证明忠实性和效率显著提高，能够生成可解释的结构化证明。

    arXiv:2402.12806v1 Announce Type: new  Abstract: Large Language Models (LLMs) have recently demonstrated remarkable reasoning ability as in Chain-of-thought prompting, but faithful multi-step reasoning remains a challenge. We specifically focus on backward chaining, where the query is recursively decomposed using logical rules until proven. To address the limitations of current backward chaining implementations, we propose SymBa (Symbolic Backward Chaining). In SymBa, the symbolic top-down solver controls the entire proof process and the LLM is called to generate a single reasoning step only when the solver encounters a dead end. By this novel solver-LLM integration, while being able to produce an interpretable, structured proof, SymBa achieves significant improvement in performance, proof faithfulness, and efficiency in diverse multi-step reasoning benchmarks (ProofWriter, Birds-Electricity, GSM8k, CLUTRR-TF, ECtHR Article 6) compared to backward chaining baselines.
    
[^69]: 三种语言中的少样本临床实体识别：掩盖语言模型胜过LLM提示

    Few shot clinical entity recognition in three languages: Masked language models outperform LLM prompting

    [https://arxiv.org/abs/2402.12801](https://arxiv.org/abs/2402.12801)

    掩盖语言模型在三种语言中的少样本临床实体识别中表现优异，胜过LLM提示方法

    

    大型语言模型正成为许多自然语言处理任务的首选解决方案，包括在专业领域中，人们期望它们的少样本能力能在资源匮乏的情况下获得高性能。本文旨在评估大型语言模型在多种语言中进行少样本临床实体识别的性能。我们使用8个领域内（临床）和6个领域外的黄金标准语料库，评估英语、法语和西班牙语中的命名实体识别。我们评估了10个自回归语言模型的性能，这些模型使用提示，并使用16个用于文本编码的掩盖语言模型作为BiLSTM-CRF监督标注器。我们通过限制可用的带标注数据量为100个句子来创建一个少样本设置。我们的实验表明，尽管更大的基于提示的模型往往在临床领域之外的命名实体识别中实现了有竞争力的F-measure，但这种性能水平并未。。。

    arXiv:2402.12801v1 Announce Type: new  Abstract: Large Language Models are becoming the go-to solution for many natural language processing tasks, including in specialized domains where their few-shot capacities are expected to yield high performance in low-resource settings. Herein, we aim to assess the performance of Large Language Models for few shot clinical entity recognition in multiple languages. We evaluate named entity recognition in English, French and Spanish using 8 in-domain (clinical) and 6 out-domain gold standard corpora. We assess the performance of 10 auto-regressive language models using prompting and 16 masked language models used for text encoding in a biLSTM-CRF supervised tagger. We create a few-shot set-up by limiting the amount of annotated data available to 100 sentences. Our experiments show that although larger prompt-based models tend to achieve competitive F-measure for named entity recognition outside the clinical domain, this level of performance does no
    
[^70]: 将大型语言模型发展起来，以捕捉不同语言风格并在口语交流中做出恰当回应

    Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations

    [https://arxiv.org/abs/2402.12786](https://arxiv.org/abs/2402.12786)

    本文旨在让大型语言模型能够根据口语的不同语言风格做出恰当回应，并为此收集了一组适合训练的语音对语音数据集。

    

    在口语对话中，即使两个当前对话是相同的句子，但当以不同风格发音时，它们的回应仍可能不同。语音风格包含语言附加信息和韵律信息，标志着文本和语音形式之间最显著的差异。使用仅文本的大型语言模型（LLMs）来建模口语对话时，纯文本的LLMs无法根据当前对话的语音风格给出不同的回应。本文旨在使LLMs能够倾听说话风格并作出恰当回应。我们的目标是教会LLM“即使句子相同，如果以不同风格说出，相应的回应可能会不同”。由于目前没有适合实现此目标的数据集，我们收集了一组语音对语音的数据集StyleTalk，具有以下所需特征：当两个当前语音具有相同内容但以不同风格说出时，它们的回应将

    arXiv:2402.12786v1 Announce Type: new  Abstract: In spoken dialogue, even if two current turns are the same sentence, their responses might still differ when they are spoken in different styles. The spoken styles, containing paralinguistic and prosodic information, mark the most significant difference between text and speech modality. When using text-only LLMs to model spoken dialogue, text-only LLMs cannot give different responses based on the speaking style of the current turn. In this paper, we focus on enabling LLMs to listen to the speaking styles and respond properly. Our goal is to teach the LLM that "even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different". Since there is no suitable dataset for achieving this goal, we collect a speech-to-speech dataset, StyleTalk, with the following desired characteristics: when two current speeches have the same content but are spoken in different styles, their responses wil
    
[^71]: 理解和缓解Vec2Text对密集检索系统的威胁

    Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval Systems

    [https://arxiv.org/abs/2402.12784](https://arxiv.org/abs/2402.12784)

    本文研究了Vec2Text对密集检索系统的威胁以及如何缓解，通过对距离度量、池化函数、瓶颈预训练等方面进行深入分析，以获得对密集检索系统中文本可恢复性和检索效果权衡关键元素的更深入理解。

    

    引入Vec2Text技术，一种用于反转文本嵌入的技术，引发了人们对密集检索系统中存在严重隐私问题的担忧，包括那些使用OpenAI和Cohere提供的文本嵌入的系统。这种威胁来自于一个恶意攻击者通过访问文本嵌入来重构原始文本。本文研究了影响使用Vec2Text恢复文本的嵌入模型的各个方面。我们的探索涉及距离度量、池化函数、瓶颈预训练、加噪声训练、嵌入量化和嵌入维度等因素，这些因素在原始Vec2Text论文中尚未被讨论。通过对这些因素的深入分析，我们旨在更深入地了解影响密集检索系统中文本可恢复性和检索效果之间权衡的关键因素。

    arXiv:2402.12784v1 Announce Type: cross  Abstract: The introduction of Vec2Text, a technique for inverting text embeddings, has raised serious privacy concerns within dense retrieval systems utilizing text embeddings, including those provided by OpenAI and Cohere. This threat comes from the ability for a malicious attacker with access to text embeddings to reconstruct the original text.   In this paper, we investigate various aspects of embedding models that could influence the recoverability of text using Vec2Text. Our exploration involves factors such as distance metrics, pooling functions, bottleneck pre-training, training with noise addition, embedding quantization, and embedding dimensions -- aspects not previously addressed in the original Vec2Text paper. Through a thorough analysis of these factors, our aim is to gain a deeper understanding of the critical elements impacting the trade-offs between text recoverability and retrieval effectiveness in dense retrieval systems. This a
    
[^72]: 认知情绪状态：为共情对话生成验证响应

    Acknowledgment of Emotional States: Generating Validating Responses for Empathetic Dialogue

    [https://arxiv.org/abs/2402.12770](https://arxiv.org/abs/2402.12770)

    该研究介绍了一个旨在促进共情对话的框架，通过验证响应生成实现了对情绪状态的认知。模型在所有模块中均优于随机基准线和ChatGPT性能。

    

    在人机对话领域中，促进共情回应至关重要。认可是心理学中的关键沟通技巧之一，涉及识别、理解和承认他人的情绪状态、思想和行为。本研究介绍了第一个旨在促进共情对话的框架，其中包括三部分模块系统：1)验证时机检测，2)用户情绪状态识别，3)验证响应生成。利用日本EmpatheticDialogues数据集，该数据集是一个基于文本的对话数据集，包括从Plutchik情感轮中选取的8个情绪类别，任务自适应预训练（TAPT）基于BERT的模型在所有模块的F1分数方面均优于随机基准线和ChatGPT性能。

    arXiv:2402.12770v1 Announce Type: new  Abstract: In the realm of human-AI dialogue, the facilitation of empathetic responses is important. Validation is one of the key communication techniques in psychology, which entails recognizing, understanding, and acknowledging others' emotional states, thoughts, and actions. This study introduces the first framework designed to engender empathetic dialogue with validating responses. Our approach incorporates a tripartite module system: 1) validation timing detection, 2) users' emotional state identification, and 3) validating response generation. Utilizing Japanese EmpatheticDialogues dataset - a textual-based dialogue dataset consisting of 8 emotional categories from Plutchik's wheel of emotions - the Task Adaptive Pre-Training (TAPT) BERT-based model outperforms both random baseline and the ChatGPT performance, in term of F1-score, in all modules. Further validation of our model's efficacy is confirmed in its application to the TUT Emotional S
    
[^73]: 多模态大型语言模型的模型组合

    Model Composition for Multimodal Large Language Models

    [https://arxiv.org/abs/2402.12750](https://arxiv.org/abs/2402.12750)

    通过模型组合现有的多模态大型语言模型，提出了一种新范式，有效地保留了每个原始模型的模态理解能力，并引入了一种用于解决合并参数干扰和不匹配问题的方法。

    

    近期对多模态大型语言模型（MLLMs）的发展显示出了快速进展，朝着创建能够理解各种模态输入的多功能MLLMs的目标迈进。然而，现有方法通常依赖于与配对的多模态指令数据进行联合训练，这对资源要求高且难以扩展到新的模态。在本文中，我们提出了一种通过现有MLLMs的模型组合来创建一个新模型的新范式，该新模型保留了每个原始模型的模态理解能力。我们的基本实现NaiveMC通过重用模态编码器和合并LLM参数展示了这一范式的有效性。此外，我们引入了DAMC来解决在合并过程中的参数干扰和不匹配问题，从而提升了模型的性能。为促进该领域的研究，我们提出了MCUB，一个用于评估MLLMs理解能力的基准测试。

    arXiv:2402.12750v1 Announce Type: cross  Abstract: Recent developments in Multimodal Large Language Models (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities. However, existing methods typically rely on joint training with paired multimodal instruction data, which is resource-intensive and challenging to extend to new modalities. In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model. Our basic implementation, NaiveMC, demonstrates the effectiveness of this paradigm by reusing modality encoders and merging LLM parameters. Furthermore, we introduce DAMC to address parameter interference and mismatch issues during the merging process, thereby enhancing the model performance. To facilitate research in this area, we propose MCUB, a benchmark for assessing ability of MLLMs to unders
    
[^74]: Me LLaMA: 为医疗应用构建大型语言模型的基础

    Me LLaMA: Foundation Large Language Models for Medical Applications

    [https://arxiv.org/abs/2402.12749](https://arxiv.org/abs/2402.12749)

    Me LLaMA是一个医学领域的大型语言模型系列，通过持续预训练和指导调整在大型医学数据集上训练而成，其在零-shot和少-shot学习方面表现优于现有的医学语言模型和商业巨头ChatGPT。

    

    最近，诸如ChatGPT和LLaMA等大型语言模型(LLMs)在许多人工智能应用中展现出巨大的潜力。然而，它们在医学任务上的表现不够理想，并且可以通过在大型领域特定数据集上进行训练来进一步改进。本研究引入了Me LLaMA，一个医学LLM系列，包括基础模型- Me LLaMA 13/70B及其 chat-enhanced 版本- Me LLaMA 13/70B-chat，通过持续对LLaMA2进行预训练和指导调整，使用大规模医学数据开发而成。我们用于训练和评估的领域特定数据套件包括一个具有129B tokens的大规模持续预训练数据集，一个包含214k个样本的指导调整数据集，以及跨越14个数据集的六项任务的医学评估基准(MIBE)。我们使用MIBE进行的广泛评估显示，Me LLaMA模型在零-shot和少-shot学习方面超越了现有的开源医学LLMs，并且在商业巨头如ChatGPT上表现出色。

    arXiv:2402.12749v1 Announce Type: cross  Abstract: Recent large language models (LLMs) like ChatGPT and LLaMA have shown great promise in many AI applications. However, their performance on medical tasks is suboptimal and can be further improved by training on large domain-specific datasets. This study introduces Me LLaMA, a medical LLM family including foundation models - Me LLaMA 13/70B and their chat-enhanced versions - Me LLaMA 13/70B-chat, developed through the continual pre-training and instruction tuning of LLaMA2 using large medical data. Our domain-specific data suite for training and evaluation, includes a large-scale continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a medical evaluation benchmark (MIBE) across six tasks with 14 datasets. Our extensive evaluation using MIBE shows that Me LLaMA models surpass existing open-source medical LLMs in zero-shot and few-shot learning and outperform commercial giants like ChatGPT on 
    
[^75]: 能否利用大型语言模型提供心理咨询？对GPT-4生成的对话进行角色扮演对话分析

    Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues

    [https://arxiv.org/abs/2402.12738](https://arxiv.org/abs/2402.12738)

    利用专家角色扮演对话数据进行研究发现，GPT-4生成的回复在心理辅导情境中与人类回复相竞争。

    

    arXiv:2402.12738v1 公告类型：交叉心理健康护理对现代社会构成日益严峻挑战。在此背景下，利用信息技术解决心理健康问题的研究激增，包括旨在开发咨询对话系统的研究。然而，有必要对使用大型语言模型的咨询对话系统的性能进行更多评估。本研究通过涉及专家辅导员的角色扮演情景收集了辅导对话数据，并针对辅导员的意图对话进行了标注。为了确定对话系统在现实世界辅导情景中的可行性，第三方辅导员在角色扮演对话数据中对人类辅导员和GPT-4生成的回复在相同情境下的恰当性进行了评估。评估结果分析表明，GPT-4生成的回复与人类的回复具有竞争力。

    arXiv:2402.12738v1 Announce Type: cross  Abstract: Mental health care poses an increasingly serious challenge to modern societies. In this context, there has been a surge in research that utilizes information technologies to address mental health problems, including those aiming to develop counseling dialogue systems. However, there is a need for more evaluations of the performance of counseling dialogue systems that use large language models. For this study, we collected counseling dialogue data via role-playing scenarios involving expert counselors, and the utterances were annotated with the intentions of the counselors. To determine the feasibility of a dialogue system in real-world counseling scenarios, third-party counselors evaluated the appropriateness of responses from human counselors and those generated by GPT-4 in identical contexts in role-play dialogue data. Analysis of the evaluation results showed that the responses generated by GPT-4 were competitive with those of human
    
[^76]: UMBCLU在SemEval-2024任务1A和1C中的表现：带有和不带有机器翻译的语义文本相关性

    UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and without machine translation

    [https://arxiv.org/abs/2402.12730](https://arxiv.org/abs/2402.12730)

    使用机器翻译和大型语言模型，本文开发了用于非洲和亚洲语言语义文本相关性任务的两种模型，取得了比部分官方基准更好的效果。

    

    这篇论文描述了我们为SemEval-2024任务1开发的系统，“非洲和亚洲语言的语义文本相关性”。 该任务的目标是构建一个能够识别目标语言中属于非洲和亚洲语言集合的两个句子之间的语义文本相关性（STR）的模型。 我们参与了子任务A和C，并探索了利用大型语言模型（LLMs）进行监督和跨语言训练。 预训练的大型语言模型已被广泛用于机器翻译和语义相似性。 使用机器翻译和句子嵌入LLMs的组合，我们为子任务A开发了一个统一的STR模型，TranSem，并对STR数据上的T5系列模型进行了微调，用于子任务C的FineSem。 我们在子任务A中7种语言的模型结果比3种语言的官方基准更好，而与其他4种语言的基准相当。

    arXiv:2402.12730v1 Announce Type: cross  Abstract: This paper describes the system we developed for SemEval-2024 Task 1, "Semantic Textual Relatedness for African and Asian Languages." The aim of the task is to build a model that can identify semantic textual relatedness (STR) between two sentences of a target language belonging to a collection of African and Asian languages. We participated in Subtasks A and C and explored supervised and cross-lingual training leveraging large language models (LLMs). Pre-trained large language models have been extensively used for machine translation and semantic similarity. Using a combination of machine translation and sentence embedding LLMs, we developed a unified STR model, TranSem, for subtask A and fine-tuned the T5 family of models on the STR data, FineSem, for use in subtask C. Our model results for 7 languages in subtask A were better than the official baseline for 3 languages and on par with the baseline for the remaining 4 languages. Our m
    
[^77]: 基于大型语言模型的模态感知集成用于基于知识的视觉问答

    Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering

    [https://arxiv.org/abs/2402.12728](https://arxiv.org/abs/2402.12728)

    提出了一种模态感知的LLM集成方法（MAIL）用于针对KVQA，通过细致地利用多模态知识来处理图像理解和知识推理。

    

    知识驱动的视觉问答（KVQA）已被广泛研究，以利用外部知识如知识图谱（KG）来回答视觉问题。尽管已提出几种尝试利用大型语言模型（LLMs）作为隐含知识源，但由于LLMs可能生成幻觉，因此仍然具有挑战性。此外，多种知识来源，例如图像、知识图谱和LLMs，不能轻易对齐以应对复杂场景。为了解决这些问题，我们提出了一种针对KVQA的新颖的具有模态感知的LLM集成方法（MAIL）。它精心利用多模态知识进行图像理解和知识推理。具体而言，（i）我们提出了一种使用LLMs的两阶段提示策略，将图像密集地融入带有详细视觉特征的场景图中；（ii）我们通过将提到的实体与外部事实联系起来构建一个耦合的概念图；（iii）设计了一个定制的伪孪生图中介融合。

    arXiv:2402.12728v1 Announce Type: cross  Abstract: Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designe
    
[^78]: 大型语言模型是理性投资者吗？

    Are Large Language Models Rational Investors?

    [https://arxiv.org/abs/2402.12713](https://arxiv.org/abs/2402.12713)

    本研究引入了金融偏见指标（FBI）框架来评估大型语言模型（LLMs）的金融合理性，着重检验它们对金融信息的辨别和市场分析中可能存在的非理性偏见。

    

    大型语言模型（LLMs）正逐渐被引入金融分析领域，利用其丰富的知识库来解释复杂的市场数据和趋势。然而，它们在金融领域的应用受到固有偏见（即风险偏好偏见）和对市场复杂性的肤浅理解的挑战，强调了对它们的金融洞察力进行全面评估的必要性。本研究引入了一个新颖的框架，金融偏见指标（FBI），以对LLMs的金融合理性进行批判性评估，重点关注它们辨别和导航金融信息的微妙之处的能力，并识别可能扭曲市场分析的任何非理性偏见。

    arXiv:2402.12713v1 Announce Type: new  Abstract: Large Language Models (LLMs) are progressively being adopted in financial analysis to harness their extensive knowledge base for interpreting complex market data and trends. However, their application in the financial domain is challenged by intrinsic biases (i.e., risk-preference bias) and a superficial grasp of market intricacies, underscoring the need for a thorough assessment of their financial insight. This study introduces a novel framework, Financial Bias Indicators (FBI), to critically evaluate the financial rationality of LLMs, focusing on their ability to discern and navigate the subtleties of financial information and to identify any irrational biases that might skew market analysis.   Our research adopts an innovative methodology to measure financial rationality, integrating principles of behavioral finance to scrutinize the biases and decision-making patterns of LLMs. We conduct a comprehensive evaluation of 19 leading LLMs,
    
[^79]: FormulaQA：一个基于公式的数值推理问题问答数据集

    FormulaQA: A Question Answering Dataset for Formula-Based Numerical Reasoning

    [https://arxiv.org/abs/2402.12692](https://arxiv.org/abs/2402.12692)

    FormulaQA是一个基于初中物理考试的公式驱动数值推理问题问答数据集，通过评估LLMs的不同方法和使用检索增强型LLMs以及对小型模型进行微调，揭示了现有模型在应对复杂、基于公式的FormulaQA时的潜在改进空间。

    

    应用公式是人类在解决数值推理问题时的基本能力。然而，现有的数值推理数据集很少明确指出推理步骤中使用的公式。为了弥补这一差距，我们提出了一个基于初中物理考试的公式驱动数值推理问题问答数据集FormulaQA。我们还使用大小从7B到超过100B参数的LLMs进行了零样本和少样本思维链方法的评估，并探索了在提供外部公式数据库时使用检索增强型LLMs的方法。我们还对大小不超过2B的较小模型进行了微调。我们的实证研究强调了当应用于我们复杂、基于公式的FormulaQA时，现有模型在改进方面具有显著潜力。

    arXiv:2402.12692v1 Announce Type: new  Abstract: The application of formulas is a fundamental ability of humans when addressing numerical reasoning problems. However, existing numerical reasoning datasets seldom explicitly indicate the formulas employed during the reasoning steps. To bridge this gap, we propose a question answering dataset for formula-based numerical reasoning called FormulaQA, from junior high school physics examinations. We further conduct evaluations on LLMs with size ranging from 7B to over 100B parameters utilizing zero-shot and few-shot chain-of-thoughts methods and we explored the approach of using retrieval-augmented LLMs when providing an external formula database. We also fine-tune on smaller models with size not exceeding 2B. Our empirical findings underscore the significant potential for improvement in existing models when applied to our complex, formula-driven FormulaQA.
    
[^80]: 树种植变压器：具有隐式句法监督的大型语言模型

    Tree-Planted Transformers: Large Language Models with Implicit Syntactic Supervision

    [https://arxiv.org/abs/2402.12691](https://arxiv.org/abs/2402.12691)

    提出了一种新方法 Tree-Planted Transformers (TPT)，通过在 Transformer LMs 的注意权重中隐式地“种植”树木来反映自然语言的句法结构，实现了句法大型语言模型（SLLM）的结合。

    

    大型语言模型（LLMs）在大规模文本语料上的可扩展性取得了显著成功，但在训练效率方面存在一些缺点。相比之下，句法语言模型（SLMs）可以通过句法监督高效训练，达到相对较高的性能，但在可扩展性方面存在困难。因此，鉴于LLMs和SLMs的互补优势，有必要开发一种将LLMs的可扩展性与SLMs的训练效率结合起来的体系结构，即句法大型语言模型（SLLM）。在本文中，我们提出一种名为“树种植”的新方法：在Transformer LMs的注意权重中暗示地“种植”树木，以反映自然语言的句法结构。具体而言，通过树种植训练的Transformer LMs将被称为树种植变压器（TPT），它们通过树种植在小型树库上学习语法，然后通过继续在大规模文本语料上进行缩放。

    arXiv:2402.12691v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved remarkable success thanks to scalability on large text corpora, but have some drawback in training efficiency. In contrast, Syntactic Language Models (SLMs) can be trained efficiently to reach relatively high performance thanks to syntactic supervision, but have trouble with scalability. Thus, given these complementary advantages of LLMs and SLMs, it is necessary to develop an architecture that integrates the scalability of LLMs with the training efficiency of SLMs, namely Syntactic Large Language Models (SLLM). In this paper, we propose a novel method dubbed tree-planting: implicitly "plant" trees into attention weights of Transformer LMs to reflect syntactic structures of natural language. Specifically, Transformer LMs trained with tree-planting will be called Tree-Planted Transformers (TPT), which learn syntax on small treebanks via tree-planting and then scale on large text corpora via conti
    
[^81]: 辛普森悖论与翻译中的准确性-流畅度权衡

    Simpson's Paradox and the Accuracy-Fluency Tradeoff in Translation

    [https://arxiv.org/abs/2402.12690](https://arxiv.org/abs/2402.12690)

    准确性和流畅度在翻译中表现为正相关的悖论，这是辛普森悖论的一个实例，在语料库级别上二者呈正相关，在单个源段级别上存在权衡。

    

    一篇好的翻译应该忠实于原文并遵守目标语言的规范。本文探讨了关于这些目标之间关系的理论难题。我们展示了准确性和流畅度之间的关系是辛普森悖论的一种实例，表明在语料库级别上，准确性和流畅度呈正相关但在单个源段的级别上存在权衡。

    arXiv:2402.12690v1 Announce Type: new  Abstract: A good translation should be faithful to the source and should respect the norms of the target language. We address a theoretical puzzle about the relationship between these objectives. On one hand, intuition and some prior work suggest that accuracy and fluency should trade off against each other, and that capturing every detail of the source can only be achieved at the cost of fluency. On the other hand, quality assessment researchers often suggest that accuracy and fluency are highly correlated and difficult for human raters to distinguish (Callison-Burch et al. 2007). We show that the tension between these views is an instance of Simpson's paradox, and that accuracy and fluency are positively correlated at the level of the corpus but trade off at the level of individual source segments. We further suggest that the relationship between accuracy and fluency is best evaluated at the segment (or sentence) level, and that the trade off be
    
[^82]: SoftQE: LLM扩展的查询学习表示

    SoftQE: Learned Representations of Queries Expanded by LLMs

    [https://arxiv.org/abs/2402.12663](https://arxiv.org/abs/2402.12663)

    SoftQE通过将输入查询的嵌入映射到LLM扩展查询的嵌入，提高了密集检索性能，并在领域外任务上取得了显著的性能改善。

    

    我们研究了将大型语言模型(LLMs)集成到查询编码器中，以改善密集检索，同时避免在推断时依赖LLMs增加延迟和成本。SoftQE通过将输入查询的嵌入映射到LLM扩展查询的嵌入来整合LLMs的知识。虽然对于领域内MS-MARCO指标，SoftQE相对于各种强基准模型的改善有限，但在五个领域外BEIR任务上，SoftQE在平均性能上提高了2.83个绝对百分点。

    arXiv:2402.12663v1 Announce Type: new  Abstract: We investigate the integration of Large Language Models (LLMs) into query encoders to improve dense retrieval without increasing latency and cost, by circumventing the dependency on LLMs at inference time. SoftQE incorporates knowledge from LLMs by mapping embeddings of input queries to those of the LLM-expanded queries. While improvements over various strong baselines on in-domain MS-MARCO metrics are marginal, SoftQE improves performance by 2.83 absolute percentage points on average on five out-of-domain BEIR tasks.
    
[^83]: FinBen：大型语言模型的全面财务基准

    The FinBen: An Holistic Financial Benchmark for Large Language Models

    [https://arxiv.org/abs/2402.12659](https://arxiv.org/abs/2402.12659)

    本文引入了FinBen，这是第一个专门设计用于彻底评估LLMs在金融领域能力的全面开源评估基准，对15个代表性LLMs进行评估，揭示了它们的优势和局限性。

    

    LLMs已经改变了自然语言处理，并显示出在各个领域具有潜力，但由于缺乏彻底的评估和金融任务的复杂性，它们在金融领域的潜力尚未得到充分开发。本文介绍了FinBen，第一个全面的开源评估基准，专门设计用于彻底评估LLMs在金融领域的能力。FinBen包括23种金融任务的35个数据集，这些任务根据卡特尔-霍恩-卡罗尔理论的灵感组织成三个不同难度的谱，以评估LLMs在归纳推理、联想记忆、数量推理、晶体智力等方面的认知能力。我们对15个代表性LLMs（包括GPT-4、ChatGPT和最新的Gemini）进行了评估，揭示了它们的优势和局限性。

    arXiv:2402.12659v1 Announce Type: cross  Abstract: LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of thorough evaluations and the complexity of financial tasks. This along with the rapid development of LLMs, highlights the urgent need for a systematic financial evaluation benchmark for LLMs. In this paper, we introduce FinBen, the first comprehensive open-sourced evaluation benchmark, specifically designed to thoroughly assess the capabilities of LLMs in the financial domain. FinBen encompasses 35 datasets across 23 financial tasks, organized into three spectrums of difficulty inspired by the Cattell-Horn-Carroll theory, to evaluate LLMs' cognitive abilities in inductive reasoning, associative memory, quantitative reasoning, crystallized intelligence, and more. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals insights into their strengths and limitations withi
    
[^84]: OWSM-CTC:一种用于语音识别、翻译和语言识别的开放编码器基础模型

    OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification

    [https://arxiv.org/abs/2402.12654](https://arxiv.org/abs/2402.12654)

    提出了OWSM-CTC，这是一种基于Connectionist Temporal Classification的新型仅编码器语音基础模型，训练有180k小时的公共音频数据，用于多语言自动语音识别（ASR）、语音翻译（ST）和语言识别。

    

    近来对能够在单个模型中执行多个语音处理任务的大型语音模型越来越感兴趣。这些模型通常采用编码器-解码器或仅解码器架构，因为它们在许多领域中非常流行且性能良好。然而，与非自回归模型相比，自回归模型在推断时可能会比较慢，并且还存在幻觉的潜在风险。尽管先前的研究观察到非自回归模型在小规模任务中产生了令人满意的结果，但尚不清楚它们是否可以扩展到不同语言和任务的语音转文本生成中。受Open Whisper-style Speech Model (OWSM)项目的启发，我们提出了OWSM-CTC，这是一种基于Connectionist Temporal Classification (CTC)的新型仅编码器的语音基础模型。它使用18万小时的公共音频数据进行训练，用于多语言自动语音识别（ASR）、语音翻译（ST）和语言识别。

    arXiv:2402.12654v1 Announce Type: new  Abstract: There has been an increasing interest in large speech models that can perform multiple speech processing tasks in a single model. Such models usually adopt the encoder-decoder or decoder-only architecture due to their popularity and good performance in many domains. However, autoregressive models can be slower during inference compared to non-autoregressive models and also have potential risks of hallucination. Though prior studies observed promising results of non-autoregressive models for certain tasks at small scales, it remains unclear if they can be scaled to speech-to-text generation in diverse languages and tasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we propose OWSM-CTC, a novel encoder-only speech foundation model based on Connectionist Temporal Classification (CTC). It is trained on 180k hours of public audio data for multilingual automatic speech recognition (ASR), speech translation (ST), and languag
    
[^85]: 语言模型中的偏见：超越技巧测试，走向RUTEd评估

    Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation

    [https://arxiv.org/abs/2402.12649](https://arxiv.org/abs/2402.12649)

    这项研究探讨了语言模型中偏见的负面影响，研究了"技巧测试"与更现实世界中表现的RUTEd评估之间的关联性，特别关注性别-职业偏见，并进行了多项评估比较。

    

    Bias benchmarks are a popular method for studying the negative impacts of bias in LLMs, yet there has been little empirical investigation of whether these benchmarks are actually indicative of how real world harm may manifest in the real world. In this work, we study the correspondence between such decontextualized "trick tests" and evaluations that are more grounded in Realistic Use and Tangible {Effects (i.e. RUTEd evaluations). We explore this correlation in the context of gender-occupation bias--a popular genre of bias evaluation. We compare three de-contextualized evaluations adapted from the current literature to three analogous RUTEd evaluations applied to long-form content generation. We conduct each evaluation for seven instruction-tuned LLMs. For the RUTEd evaluations, we conduct repeated trials of three text generation tasks: children's bedtime stories, user personas, and English language learning exercises. We found no corres

    arXiv:2402.12649v1 Announce Type: new  Abstract: Bias benchmarks are a popular method for studying the negative impacts of bias in LLMs, yet there has been little empirical investigation of whether these benchmarks are actually indicative of how real world harm may manifest in the real world. In this work, we study the correspondence between such decontextualized "trick tests" and evaluations that are more grounded in Realistic Use and Tangible {Effects (i.e. RUTEd evaluations). We explore this correlation in the context of gender-occupation bias--a popular genre of bias evaluation. We compare three de-contextualized evaluations adapted from the current literature to three analogous RUTEd evaluations applied to long-form content generation. We conduct each evaluation for seven instruction-tuned LLMs. For the RUTEd evaluations, we conduct repeated trials of three text generation tasks: children's bedtime stories, user personas, and English language learning exercises. We found no corres
    
[^86]: StyleDubber: 面向电影配音的多尺度风格学习

    StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing

    [https://arxiv.org/abs/2402.12636](https://arxiv.org/abs/2402.12636)

    StyleDubber提出了一种新的电影配音方法，通过在音素级别进行学习，解决了当前 V2C 模型中存在的音素发音不完整和身份稳定性差的问题。

    

    给定一份剧本，在电影配音（视觉语音克隆，V2C）中的挑战是根据参考音轨的语气，生成与视频在时间和情绪上都良好对齐的语音。现有的 V2C 模型根据视频帧间的间隔字断分割剧本的音素，这解决了时间对齐问题，但导致音素发音不完整和身份稳定性差。为了解决这个问题，我们提出 StyleDubber，它将配音学习从帧级别转为音素级别。它包含三个主要组件：（1）一个多模态风格适配器，以音素级别操作，从参考音频中学习发音风格，并生成受视频中呈现的面部情绪影响的中间表示；（2）一个以语句级别风格学习模块，引导中间表现的 mel-spectrogram 解码和细化过程。

    arXiv:2402.12636v1 Announce Type: new  Abstract: Given a script, the challenge in Movie Dubbing (Visual Voice Cloning, V2C) is to generate speech that aligns well with the video in both time and emotion, based on the tone of a reference audio track. Existing state-of-the-art V2C models break the phonemes in the script according to the divisions between video frames, which solves the temporal alignment problem but leads to incomplete phoneme pronunciation and poor identity stability. To address this problem, we propose StyleDubber, which switches dubbing learning from the frame level to phoneme level. It contains three main components: (1) A multimodal style adaptor operating at the phoneme level to learn pronunciation style from the reference audio, and generate intermediate representations informed by the facial emotion presented in the video; (2) An utterance-level style learning module, which guides both the mel-spectrogram decoding and the refining processes from the intermediate e
    
[^87]: Reflect-RL：两个玩家在线RL微调语言模型

    Reflect-RL: Two-Player Online RL Fine-Tuning for LMs

    [https://arxiv.org/abs/2402.12621](https://arxiv.org/abs/2402.12621)

    提出Reflect-RL，使用在线强化学习来微调语言模型，在这过程中引入了反射模型协助，并采用了负例生成、单提示动作枚举和课程学习来提高性能。

    

    随着语言模型在各个领域展示其能力，将它们应用于需要多轮交互的任务变得越来越受欢迎。这些任务通常具有复杂的动态性，因此仅在有限的离线数据集上进行监督微调（SFT）无法取得良好性能。然而，只有少数研究尝试在交互式决策制定环境内直接对LM进行训练。我们旨在创建一个在这些环境中使用在线强化学习（RL）对LM进行微调的有效机制。我们提出了Reflect-RL，一个两个玩家的系统，使用在线RL对LM进行微调，在此过程中，一个冻结的反射模型辅助策略模型。为了为热身SFT阶段生成数据，我们使用负例生成来增强反射模型的纠错能力。此外，我们设计了单提示动作枚举，并应用了课程学习让策略模型学习更多。

    arXiv:2402.12621v1 Announce Type: cross  Abstract: As language models (LMs) demonstrate their capabilities in various fields, their application to tasks requiring multi-round interactions has become increasingly popular. These tasks usually have complex dynamics, so supervised fine-tuning (SFT) on a limited offline dataset does not yield good performance. However, only a few works attempted to directly train the LMs within interactive decision-making environments. We aim to create an effective mechanism to fine-tune LMs with online reinforcement learning (RL) in these environments. We propose Reflect-RL, a two-player system to fine-tune an LM using online RL, where a frozen reflection model assists the policy model. To generate data for the warm-up SFT stage, we use negative example generation to enhance the error-correction ability of the reflection model. Furthermore, we designed single-prompt action enumeration and applied curriculum learning to allow the policy model to learn more 
    
[^88]: 生成式人工智能安全：挑战与对策

    Generative AI Security: Challenges and Countermeasures

    [https://arxiv.org/abs/2402.12617](https://arxiv.org/abs/2402.12617)

    生成式人工智能的安全挑战及对策研究。

    

    arXiv:2402.12617v1 公告类型：跨领域 摘要：生成式人工智能在许多行业的不断扩展引发了人们的兴奋和增加的关注。本文深入探讨了生成式人工智能所带来的独特安全挑战，并概述了管理这些风险的潜在研究方向。

    arXiv:2402.12617v1 Announce Type: cross  Abstract: Generative AI's expanding footprint across numerous industries has led to both excitement and increased scrutiny. This paper delves into the unique security challenges posed by Generative AI, and outlines potential research directions for managing these risks.
    
[^89]: 什么是一个词？

    What is a word?

    [https://arxiv.org/abs/2402.12605](https://arxiv.org/abs/2402.12605)

    研究者探讨了词汇是什么，并指出了现有理论对于词汇的解释及实验设计方面的一些重要影响

    

    为了设计出能够有效隔离词汇访问和语义的强大范式，我们需要知道一个词是什么。令人惊讶的是，很少有语言学家和哲学家对一个词有着清晰的模型，尽管词汇基本影响着人类生活的方方面面。经常发表关于语言的学术论文的研究人员往往依赖于过时或不准确的关于词性的假设。这篇简短的教育性文件概述了词汇绝对不是什么（尽管经常被错误地认为是什么）、它可能是什么（基于当前的好理论）、以及实验设计的一些影响。

    arXiv:2402.12605v1 Announce Type: new  Abstract: In order to design strong paradigms for isolating lexical access and semantics, we need to know what a word is. Surprisingly few linguists and philosophers have a clear model of what a word is, even though words impact basically every aspect of human life. Researchers that regularly publish academic papers about language often rely on outdated, or inaccurate, assumptions about wordhood. This short pedagogical document outlines what the lexicon is most certainly not (though is often mistakenly taken to be), what it might be (based on current good theories), and what some implications for experimental design are.
    
[^90]: 标准化: 将语言模型与专家定义的标准对齐，用于内容生成

    Standardize: Aligning Language Models with Expert-Defined Standards for Content Generation

    [https://arxiv.org/abs/2402.12593](https://arxiv.org/abs/2402.12593)

    该研究引入了一个名为Standardize的框架，通过检索式上下文学习指导大型语言模型与专家定义的标准对齐，提高了内容生成的精确性。

    

    在工程、医疗保健和教育领域，领域专家遵循严格的标准来制作质量内容，如技术手册、药物说明和儿童读物。然而，当前在可控文本生成方面的研究尚未探讨使用这些标准作为控制的参考。为此，我们引入了一种名为Standardize的检索式上下文学习框架，以指导大型语言模型与专家定义的标准对齐。以英语语言标准在教育领域作为一个使用案例，我们考虑了欧洲共同语言参考框架（CEFR）和通用核心标准（CCS）用于开放性内容生成任务。我们的研究结果表明，模型的精确性对于Llama2和GPT-4分别可以提高40%到100%，证明了从标准中提取知识工件并将其整合到生成中的可行性。

    arXiv:2402.12593v1 Announce Type: new  Abstract: Domain experts across engineering, healthcare, and education follow strict standards for producing quality content such as technical manuals, medication instructions, and children's reading materials. However, current works in controllable text generation have yet to explore using these standards as references for control. Towards this end, we introduce Standardize, a retrieval-style in-context learning-based framework to guide large language models to align with expert-defined standards. Focusing on English language standards in the education domain as a use case, we consider the Common European Framework of Reference for Languages (CEFR) and Common Core Standards (CCS) for the task of open-ended content generation. Our findings show that models can gain 40% to 100% increase in precise accuracy for Llama2 and GPT-4, respectively, demonstrating that the use of knowledge artifacts extracted from standards and integrating them in the gener
    
[^91]: 将AI集体进化，增强人类多样性并实现自我调节

    Evolving AI Collectives to Enhance Human Diversity and Enable Self-Regulation

    [https://arxiv.org/abs/2402.12590](https://arxiv.org/abs/2402.12590)

    探索人工智能互动的“类社会”属性，以增加奖励并减少风险，展示新兴的去中心化AI集体如何扩大人类多样性范围和降低在线有毒行为风险。

    

    大型语言模型根据他人生成的文本来引导其行为。这种能力及其在在线环境中日益普及的趋势预示着它们将有意或无意地“编程”彼此并形成新兴的AI主体性、关系和集体。在这里，我们呼吁研究界探究这些互动人工智能的“类社会”属性，以增加其奖励并减少对人类社会和在线环境健康的风险。我们利用一个简单模型及其输出来说明这种新兴的、去中心化的AI集体如何扩大人类多样性范围并降低在线有毒、反社会行为的风险。最后，我们讨论了AI自我调节的机会，并解决了涉及创建和维护去中心化AI集体的伦理问题和设计挑战。

    arXiv:2402.12590v1 Announce Type: new  Abstract: Large language models steer their behaviors based on texts generated by others. This capacity and their increasing prevalence in online settings portend that they will intentionally or unintentionally "program" one another and form emergent AI subjectivities, relationships, and collectives. Here, we call upon the research community to investigate these "society-like" properties of interacting artificial intelligences to increase their rewards and reduce their risks for human society and the health of online environments. We use a simple model and its outputs to illustrate how such emergent, decentralized AI collectives can expand the bounds of human diversity and reduce the risk of toxic, anti-social behavior online. Finally, we discuss opportunities for AI self-moderation and address ethical issues and design challenges associated with creating and maintaining decentralized AI collectives.
    
[^92]: GenAudit：利用证据修复语言模型输出中的事实错误

    GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence

    [https://arxiv.org/abs/2402.12566](https://arxiv.org/abs/2402.12566)

    GenAudit是一个工具，通过修订或删除未被参考文献支持的声明，并提供来自参考文献的证据，帮助修复语言模型输出中的事实错误。

    

    LLMs即使可以访问参考文档，也可能生成事实不准确的陈述。在高风险应用中（例如基于文档的医疗保健或金融问答），这样的错误可能具有危险性。我们提出了GenAudit -- 一个旨在帮助检查基于文档任务语言模型响应的工具。GenAudit通过修订或删除未被参考文档支持的声明，同时为看似被证据支持的事实提供来自参考文献的证据，来建议修改LLM响应。我们训练模型来执行这些任务，并设计了一个交互界面，向用户呈现建议的修改和证据。通过人工评分员的全面评估显示，GenAudit在总结不同领域文档时能够检测出8种不同的LLM输出中的错误。为确保系统能够标记大多数错误，我们提出了一种方法，可以提高错误召回率，同时最小化对预处理的影响。

    arXiv:2402.12566v1 Announce Type: new  Abstract: LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance). We present GenAudit -- a tool intended to assist fact-checking LLM responses for document-grounded tasks. GenAudit suggests edits to the LLM response by revising or removing claims that are not supported by the reference document, and also presents evidence from the reference for facts that do appear to have support. We train models to execute these tasks, and design an interactive interface to present suggested edits and evidence to users. Comprehensive evaluation by human raters shows that GenAudit can detect errors in 8 different LLM outputs when summarizing documents from diverse domains. To ensure that most errors are flagged by the system, we propose a method that can increase the error recall while minimizing impact on pre
    
[^93]: 信心至关重要：重新审视大型语言模型的内在自我校正能力

    Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models

    [https://arxiv.org/abs/2402.12563](https://arxiv.org/abs/2402.12563)

    本文研究了大型语言模型的内在自我校正能力，并提出了一个“如果-否则”（IoE）提示框架，帮助模型评估自身“信心”并进行自我校正。

    

    大型语言模型（LLMs）的最近成功激发了对它们自我校正能力的越来越多的兴趣。本文对LLMs的内在自我校正进行了全面调查，试图解决关于其可行性的持续争论。我们的研究确定了一个重要的潜在因素 - LLMs的“信心” - 在自我校正过程中。忽视这一因素可能导致模型过度批评自己，从而导致对自校正效果的可靠结论不准确。我们实验观察到LLMs具有理解其自身回应“信心”的能力。这激励我们开发了一个“如果-否则”（IoE）提示框架，旨在引导LLMs评估其自身“信心”，促进内在自我校正。我们进行了大量实验证明，我们基于IoE的提示可以实现一

    arXiv:2402.12563v1 Announce Type: cross  Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a co
    
[^94]: CausalGym：在语言任务上对因果解释方法进行基准测试

    CausalGym: Benchmarking causal interpretability methods on linguistic tasks

    [https://arxiv.org/abs/2402.12560](https://arxiv.org/abs/2402.12560)

    CausalGym介绍了在语言任务中基准测试解释方法影响模型行为的能力。研究表明DAS方法胜过其他方法，并用它来研究了pythia-1b中的两个困难语言现象的学习轨迹。

    

    语言模型（LMs）已被证明是心理语言学研究的强大工具，但大多数先前的研究集中在纯行为测量（例如，惊奇比较）。同时，模型可解释性的研究已开始阐明塑造LM行为的抽象因果机制。为了帮助将这些研究领域更紧密地联系在一起，我们推出了CausalGym。我们改编并扩展了SyntaxGym任务套件，以基准测试解释方法影响模型行为的能力。为了说明CausalGym的用途，我们研究了pythia模型（从14M到6.9B）并评估了广泛解释方法的因果有效性，包括线性探针和分布对齐搜索（DAS）。我们发现DAS优于其他方法，因此我们使用它来研究pythia-1b中两个困难的语言现象的学习轨迹：负极性项许可和填充-间隙d。

    arXiv:2402.12560v1 Announce Type: cross  Abstract: Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior. To help bring these strands of research closer together, we introduce CausalGym. We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour. To illustrate how CausalGym can be used, we study the pythia models (14M--6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler--gap d
    
[^95]: 使用LLMs创建精细粒度实体类型分类法

    Creating a Fine Grained Entity Type Taxonomy Using LLMs

    [https://arxiv.org/abs/2402.12557](https://arxiv.org/abs/2402.12557)

    本研究利用GPT-4及其进阶版GPT-4 Turbo自主开发了涵盖5000多种微妙实体类型的详细分类法，通过迭代提示技术不断改进，展示出显著的质量。

    

    在这项研究中，我们探讨了GPT-4及其进阶版GPT-4 Turbo在自主开发详细实体类型分类法方面的潜力。我们的目标是构建一套全面的分类法，从广泛的实体类型分类开始 - 包括对象、时间、地点、组织、事件、行动和主题 - 类似于现有的手动策划分类法。这一分类法通过迭代提示技术逐渐细化，利用GPT-4内部知识库。其结果是一个包含超过5000种微妙实体类型的广泛分类法，在主观评估中表现出色。

    arXiv:2402.12557v1 Announce Type: new  Abstract: In this study, we investigate the potential of GPT-4 and its advanced iteration, GPT-4 Turbo, in autonomously developing a detailed entity type taxonomy. Our objective is to construct a comprehensive taxonomy, starting from a broad classification of entity types - including objects, time, locations, organizations, events, actions, and subjects - similar to existing manually curated taxonomies. This classification is then progressively refined through iterative prompting techniques, leveraging GPT-4's internal knowledge base. The result is an extensive taxonomy comprising over 5000 nuanced entity types, which demonstrates remarkable quality upon subjective evaluation.   We employed a straightforward yet effective prompting strategy, enabling the taxonomy to be dynamically expanded. The practical applications of this detailed taxonomy are diverse and significant. It facilitates the creation of new, more intricate branches through pattern-b
    
[^96]: 通过模拟和即时反馈以人机语言模型互动提高人际有效性：IMBUE

    IMBUE: Improving Interpersonal Effectiveness through Simulation and Just-in-time Feedback with Human-Language Model Interaction

    [https://arxiv.org/abs/2402.12556](https://arxiv.org/abs/2402.12556)

    IMBUE是首个同时专注于沟通技能和情绪管理、在提供反馈中整合专家领域知识以及基于心理学理论的互动式培训系统。

    

    由于个体缺乏技能和强烈情绪的干扰，导致在某些沟通情境下的导航变得具有挑战性。然而，有效的学习机会很少。本研究进行了一项以人为中心的研究，利用语言模型模拟定制沟通培训，并提供即时反馈，以支持人际有效性技能的实践和学习。我们应用了方言行为治疗（DBT）的人际有效性框架DEAR MAN，重点关注对话和情绪技能。我们提出了IMBUE，这是一个交互式培训系统，其提供的反馈与专业人士反馈更相似，比GPT-4生成的反馈提高了25%。IMBUE首次同时专注于沟通技能和情绪管理，将专家的领域知识纳入提供反馈，并基于心理学理论。通过随机 ...

    arXiv:2402.12556v1 Announce Type: cross  Abstract: Navigating certain communication situations can be challenging due to individuals' lack of skills and the interference of strong emotions. However, effective learning opportunities are rarely accessible. In this work, we conduct a human-centered study that uses language models to simulate bespoke communication training and provide just-in-time feedback to support the practice and learning of interpersonal effectiveness skills. We apply the interpersonal effectiveness framework from Dialectical Behavioral Therapy (DBT), DEAR MAN, which focuses on both conversational and emotional skills. We present IMBUE, an interactive training system that provides feedback 25% more similar to experts' feedback, compared to that generated by GPT-4. IMBUE is the first to focus on communication skills and emotion management simultaneously, incorporate experts' domain knowledge in providing feedback, and be grounded in psychology theory. Through a randomi
    
[^97]: Archer: 一个具有算术、常识和假设推理的人工标记文本到SQL数据集

    Archer: A Human-Labeled Text-to-SQL Dataset with Arithmetic, Commonsense and Hypothetical Reasoning

    [https://arxiv.org/abs/2402.12554](https://arxiv.org/abs/2402.12554)

    Archer数据集是一个具有挑战性的双语文本到SQL数据集，包含算术、常识和假设推理的复杂推理，挑战了当前最先进模型的能力。

    

    我们提出了Archer，一个具有挑战性的双语文本到SQL数据集，专注于包括算术、常识和假设推理在内的复杂推理。它包含1,042个英文问题和1,042个中文问题，以及521个唯一的SQL查询，涵盖了20个领域中的20个英语数据库。值得注意的是，与现有公开数据集相比，这个数据集展示了显著更高的复杂性水平。我们的评估表明，Archer挑战了当前最先进模型的能力，Spider排行榜上的排名靠前的模型在Archer测试集上仅达到6.73%的执行准确率。因此，Archer为这一领域的未来研究提出了重大挑战。

    arXiv:2402.12554v1 Announce Type: new  Abstract: We present Archer, a challenging bilingual text-to-SQL dataset specific to complex reasoning, including arithmetic, commonsense and hypothetical reasoning. It contains 1,042 English questions and 1,042 Chinese questions, along with 521 unique SQL queries, covering 20 English databases across 20 domains. Notably, this dataset demonstrates a significantly higher level of complexity compared to existing publicly available datasets. Our evaluation shows that Archer challenges the capabilities of current state-of-the-art models, with a high-ranked model on the Spider leaderboard achieving only 6.73% execution accuracy on Archer test set. Thus, Archer presents a significant challenge for future research in this field.
    
[^98]: TrustScore: 无参考评估LLM响应的可信度

    TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness

    [https://arxiv.org/abs/2402.12545](https://arxiv.org/abs/2402.12545)

    TrustScore 是一个基于行为一致性概念的框架，用于评估大型语言模型（LLMs）的响应是否与其内在知识相一致，并成功实现了与人类判断高度相关的可信度评估。

    

    大型语言模型（LLMs）在各个领域展示了令人印象深刻的能力，引发了它们在实际应用中的激增。然而，人们对LLMs输出的可信度产生了担忧，特别是在封闭书问题回答任务中，非专家可能因缺少上下文或基准信息而难以识别不准确之处。本文介绍了TrustScore，一个基于行为一致性概念的框架，评估LLMs的响应是否与其内在知识相一致。此外，TrustScore可以无缝地与事实核查方法集成，评估与外部知识来源的一致性。实验结果显示，TrustScore与人类判断具有强大的相关性，超过了现有的无参考指标，并取得了与基准指标相当的结果。

    arXiv:2402.12545v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across various domains, prompting a surge in their practical applications. However, concerns have arisen regarding the trustworthiness of LLMs outputs, particularly in closed-book question-answering tasks, where non-experts may struggle to identify inaccuracies due to the absence of contextual or ground truth information. This paper introduces TrustScore, a framework based on the concept of Behavioral Consistency, which evaluates whether an LLMs response aligns with its intrinsic knowledge. Additionally, TrustScore can seamlessly integrate with fact-checking methods, which assesses alignment with external knowledge sources. The experimental results show that TrustScore achieves strong correlations with human judgments, surpassing existing reference-free metrics, and achieving results on par with reference-based metrics.
    
[^99]: 在预训练数据中的平行结构实现上下文学习

    Parallel Structures in Pre-training Data Yield In-Context Learning

    [https://arxiv.org/abs/2402.12530](https://arxiv.org/abs/2402.12530)

    本研究发现，语言模型的上下文学习能力取决于预训练数据中的平行结构，通过在相似模板的短语对中学习来提高上下文学习准确度。

    

    预训练语言模型（LMs）具备上下文学习（ICL）的能力：它们可以在只给出少量示例的情况下适应任务而无需进行任何参数更新。然而，目前尚不清楚这种能力来自何处，因为预训练文本与ICL提示之间存在明显的分布偏移。在本研究中，我们探讨了预训练数据中的哪些模式有助于ICL。我们发现LMs的ICL能力取决于预训练数据中的“平行结构”——在相同上下文窗口中遵循相似模板的短语对。具体来说，通过检查训练一个短语是否提高了对另一个短语的预测来检测平行结构，并进行消融实验以研究其对ICL的影响。我们展示了从预训练数据中去除平行结构会导致LMs的ICL准确度下降51％（与随机切除的2％相比）。即使排除常见模式如 n-gram

    arXiv:2402.12530v1 Announce Type: cross  Abstract: Pre-trained language models (LMs) are capable of in-context learning (ICL): they can adapt to a task with only a few examples given in the prompt without any parameter update. However, it is unclear where this capability comes from as there is a stark distribution shift between pre-training text and ICL prompts. In this work, we study what patterns of the pre-training data contribute to ICL. We find that LMs' ICL ability depends on $\textit{parallel structures}$ in the pre-training data -- pairs of phrases following similar templates in the same context window. Specifically, we detect parallel structures by checking whether training on one phrase improves prediction of the other, and conduct ablation experiments to study their effect on ICL. We show that removing parallel structures in the pre-training data reduces LMs' ICL accuracy by 51% (vs 2% from random ablation). This drop persists even when excluding common patterns such as n-gr
    
[^100]: 你的视觉语言模型本身就是一个强大的过滤器：朝向使用数据选择进行高质量指令调整

    Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection

    [https://arxiv.org/abs/2402.12501](https://arxiv.org/abs/2402.12501)

    使用Self-Filter这种方法，利用VLM本身作为一个过滤器进行数据集选择，以获取高质量数据并训练指令-following大型语言模型。

    

    在指令调整中进行数据选择成为获取高质量数据并训练大型语言模型（LLMs）的关键过程，但对于视觉语言模型（VLMs）而言，这仍然是一个新颖且未被探索的研究领域。为了解决现有LLMs上的数据选择方法要么依赖于单一不可靠的分数，要么使用下游任务进行选择，这既耗时又可能导致过拟合所选评估数据集的潜在问题。为了解决这一挑战，我们引入了一种新颖的数据集选择方法，Self-Filter，它利用VLM本身作为过滤器。

    arXiv:2402.12501v1 Announce Type: new  Abstract: Data selection in instruction tuning emerges as a pivotal process for acquiring high-quality data and training instruction-following large language models (LLMs), but it is still a new and unexplored research area for vision-language models (VLMs). Existing data selection approaches on LLMs either rely on single unreliable scores, or use downstream tasks for selection, which is time-consuming and can lead to potential over-fitting on the chosen evaluation datasets. To address this challenge, we introduce a novel dataset selection method, Self-Filter, that utilizes the VLM itself as a filter. This approach is inspired by the observation that VLMs benefit from training with the most challenging instructions. Self-Filter operates in two stages. In the first stage, we devise a scoring network to evaluate the difficulty of training instructions, which is co-trained with the VLM. In the second stage, we use the trained score net to measure the
    
[^101]: 预训练语言模型是否能检测和理解语义不确定性？问问DUST！

    Do Pre-Trained Language Models Detect and Understand Semantic Underspecification? Ask the DUST!

    [https://arxiv.org/abs/2402.12486](https://arxiv.org/abs/2402.12486)

    预训练语言模型在明确提示时能够相当准确地识别语义不确定的句子，但对其进行正确解释则更为困难。

    

    在日常语言使用中，说话者经常使用语义不确定的句子，即内容不足以完全传达其信息或以唯一方式解释它们。本文提出了一个新的“语义不确定句子类型分组数据集”（DUST），用来研究预训练语言模型是否能正确识别和解释语义不确定的句子。我们发现，新的语言模型在明确提示的情况下，能够相当准确地识别语义不确定的句子，但对其进行正确解释则更为困难。我们的实验表明，在解释语义不确定的句子时，语言模型表现出较小的不确定性，与语义不确定性的理论描述相反。

    arXiv:2402.12486v1 Announce Type: new  Abstract: In everyday language use, speakers frequently utter and interpret sentences that are semantically underspecified, namely, whose content is insufficient to fully convey their message or interpret them univocally. For example, to interpret the underspecified sentence "Don't spend too much", which leaves implicit what (not) to spend, additional linguistic context or outside knowledge is needed. In this work, we propose a novel Dataset of semantically Underspecified Sentences grouped by Type (DUST) and use it to study whether pre-trained language models (LMs) correctly identify and interpret underspecified sentences. We find that newer LMs are reasonably able to identify underspecified sentences when explicitly prompted. However, interpreting them correctly is much harder for any LMs. Our experiments show that when interpreting underspecified sentences, LMs exhibit little uncertainty, contrary to what theoretical accounts of underspecificati
    
[^102]: 文物还是绑架：LLMs如何在没有问题的情况下回答多项选择题？

    Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?

    [https://arxiv.org/abs/2402.12483](https://arxiv.org/abs/2402.12483)

    LLMs能够在没有问题的情况下仅从选项中回答多项选择题，通过记忆、选择动态和问题推理进行黑盒分析，揭示了LLMs在选择性准确性方面的三个关键发现。

    

    多项选择题回答（MCQA）通常用于评估大型语言模型（LLMs）。为了确定MCQA是否按预期评估LLMs，我们探究LLMs是否可以通过只有选项的提示来进行MCQA，其中模型必须仅从选项中选择正确答案。在三个MCQA数据集和四个LLMs中，这个提示在12个案例中的11个中优于多数基线，并可获得高达0.33的准确度提升。为了帮助解释这种行为，我们对记忆、选择动态和问题推理进行了深入的黑盒分析。我们的关键发现有三个。首先，我们发现没有证据表明只有选择的准确性仅源自记忆。其次，对单个选择的先验并不能完全解释只有选择的准确性，暗示LLMs使用选择的集体动态。第三，LLMs有一定能力从选择中推断出相关问题，并且令人惊讶地有时甚至可以匹配原始问题。我们希望鼓励利用这种方法。

    arXiv:2402.12483v1 Announce Type: new  Abstract: Multiple-choice question answering (MCQA) is often used to evaluate large language models (LLMs). To see if MCQA assesses LLMs as intended, we probe if LLMs can perform MCQA with choices-only prompts, where models must select the correct answer only from the choices. In three MCQA datasets and four LLMs, this prompt bests a majority baseline in 11/12 cases, with up to 0.33 accuracy gain. To help explain this behavior, we conduct an in-depth, black-box analysis on memorization, choice dynamics, and question inference. Our key findings are threefold. First, we find no evidence that the choices-only accuracy stems from memorization alone. Second, priors over individual choices do not fully explain choices-only accuracy, hinting that LLMs use the group dynamics of choices. Third, LLMs have some ability to infer a relevant question from choices, and surprisingly can sometimes even match the original question. We hope to motivate the use of st
    
[^103]: 多模态大型语言模型的革命：一项调查

    The (R)Evolution of Multimodal Large Language Models: A Survey

    [https://arxiv.org/abs/2402.12451](https://arxiv.org/abs/2402.12451)

    多模态大型语言模型能够无缝整合视觉和文本模态，为生成智能提供了新的可能性。

    

    连接文本和视觉模态在生成智能中扮演着重要角色。受大型语言模型成功的启发，目前已有大量研究工作致力于开发多模态大型语言模型（MLLMs）。这些模型可以无缝地整合视觉和文本模态，同时作为输入和输出，提供基于对话的接口和遵循指令的能力。本文全面审查了最近基于视觉的MLLMs，分析了它们的架构选择、多模态对齐策略和训练技术。我们还对这些模型在各种任务上进行了详细分析，包括视觉定位、图像生成和编辑、视觉理解以及特定领域的应用。此外，我们编制并描述了训练数据集和评估基准，对现有模型进行了比较。

    arXiv:2402.12451v1 Announce Type: cross  Abstract: Connecting text and visual modalities plays an essential role in generative intelligence. For this reason, inspired by the success of large language models, significant research efforts are being devoted to the development of Multimodal Large Language Models (MLLMs). These models can seamlessly integrate visual and textual modalities, both as input and output, while providing a dialogue-based interface and instruction-following capabilities. In this paper, we provide a comprehensive review of recent visual-based MLLMs, analyzing their architectural choices, multimodal alignment strategies, and training techniques. We also conduct a detailed analysis of these models across a wide range of tasks, including visual grounding, image generation and editing, visual understanding, and domain-specific applications. Additionally, we compile and describe training datasets and evaluation benchmarks, conducting comparisons among existing models in 
    
[^104]: 了解科学发现报道中的细微失真

    Understanding Fine-grained Distortions in Reports of Scientific Findings

    [https://arxiv.org/abs/2402.12431](https://arxiv.org/abs/2402.12431)

    本研究对科学发现报道中的细微失真进行了详细研究，通过标注和自动检测技术，分析了科学研究结果在新闻报道和推文中可能出现的失真特征。

    

    失真的科学传播损害个人和社会，因为它可能导致不健康的行为改变，并降低对科学机构的信任。鉴于近年来科学传播量的快速增加，对科学出版物的研究结果如何报道给公众以及自动检测原始工作中的失真的细致了解至关重要。先前的工作侧重于失真的个别方面或使用未配对数据。在这项工作中，我们对解决这一问题做出了三项基础性贡献：(1)注释了来自学术论文的1,600个科学发现实例，这些实例与新闻文章和推文中报道的相应发现有关，涉及四个特征：因果关系、确定性、一般性和轰动性；(2)建立了自动检测这些特征的基准；以及(3)分析了变化的普遍程度。

    arXiv:2402.12431v1 Announce Type: new  Abstract: Distorted science communication harms individuals and society as it can lead to unhealthy behavior change and decrease trust in scientific institutions. Given the rapidly increasing volume of science communication in recent years, a fine-grained understanding of how findings from scientific publications are reported to the general public, and methods to detect distortions from the original work automatically, are crucial. Prior work focused on individual aspects of distortions or worked with unpaired data. In this work, we make three foundational contributions towards addressing this problem: (1) annotating 1,600 instances of scientific findings from academic papers paired with corresponding findings as reported in news articles and tweets wrt. four characteristics: causality, certainty, generality and sensationalism; (2) establishing baselines for automatically detecting these characteristics; and (3) analyzing the prevalence of changes
    
[^105]: 表格作为图片？探讨LLM在多模态表格数据表示上的优势和局限性

    Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data

    [https://arxiv.org/abs/2402.12424](https://arxiv.org/abs/2402.12424)

    本研究探讨了LLM在解释表格数据方面的有效性，比较了文本和图像表格表示对LLM性能的影响，为在表格相关任务上有效使用LLM提供了见解。

    

    在本文中，我们通过不同的提示策略和数据格式研究了各种LLM在解释表格数据方面的有效性。我们的分析涵盖了六个针对与表格相关任务的基准，如问答和事实核查。我们首次介绍了LLM在基于图像的表格表示上的表现评估。具体地，我们比较了五种基于文本和三种基于图像的表格表示，展示了表示和提示对LLM性能的影响。我们的研究为在表格相关任务上有效使用LLM提供了见解。

    arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
    
[^106]: 关于基于扩散的文本转语音模型的语义潜空间

    On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models

    [https://arxiv.org/abs/2402.12423](https://arxiv.org/abs/2402.12423)

    本研究在文本转语音模型中探索了冻结模型的潜空间，发现其中包含丰富的语义信息，并提出了一些新方法来找出其中的语义方向，从而实现了不经过额外训练、架构更改或数据需求就能进行音频编辑。

    

    在文本转语音（TTS）领域，Denoising Diffusion Models (DDMs) 的引入日益增多，为合成高质量语音提供了巨大价值。尽管它们展示出令人印象深刻的音频质量，但它们的语义能力程度尚不明确，并且控制合成语音的声音特性仍然是一个挑战。受图像合成最新进展的启发，我们探索了冻结的TTS模型的潜空间，该空间由DDM去噪器的潜空间激活组成。我们发现这个空间包含丰富的语义信息，并概述了若干查找其中语义方向的新方法，包括监督和无监督方法。然后，我们演示了如何利用这些方法进行现成音频编辑，无需进一步训练、架构更改或数据需求。我们呈现了编辑后音频的语义和声学特质的证据，并提供了补充样本。

    arXiv:2402.12423v1 Announce Type: cross  Abstract: The incorporation of Denoising Diffusion Models (DDMs) in the Text-to-Speech (TTS) domain is rising, providing great value in synthesizing high quality speech. Although they exhibit impressive audio quality, the extent of their semantic capabilities is unknown, and controlling their synthesized speech's vocal properties remains a challenge. Inspired by recent advances in image synthesis, we explore the latent space of frozen TTS models, which is composed of the latent bottleneck activations of the DDM's denoiser. We identify that this space contains rich semantic information, and outline several novel methods for finding semantic directions within it, both supervised and unsupervised. We then demonstrate how these enable off-the-shelf audio editing, without any further training, architectural changes or data requirements. We present evidence of the semantic and acoustic qualities of the edited audio, and provide supplemental samples: h
    
[^107]: 在跨语料评估中对命名实体识别和归一化工具的HunFlair2

    HunFlair2 in a cross-corpus evaluation of named entity recognition and normalization tools

    [https://arxiv.org/abs/2402.12372](https://arxiv.org/abs/2402.12372)

    挑战BTM工具在不同上下文中应用的可靠性，通过跨语料基准测试评估命名实体识别和归一化工具的性能。

    

    随着生命科学文献的指数增长，生物医学文本挖掘（BTM）已成为加速从出版物中提取见解的基本技术。在BTM流程中，识别文本中的命名实体（例如疾病、药物或基因）以及将其链接到参考知识库是关键步骤，以便从不同文档中启用信息聚合。然而，用于这两个步骤的工具很少在开发它们的上下文中应用。相反，它们被应用在野外，即在应用相关的文本集合上，不同于用于工具训练的文本，例如在焦点、体裁、风格和文本类型上有所不同。这引发了一个问题，即是否可以信任报告的BTM工具性能，用于下游应用。在这里，我们报告了针对命名实体提取的精心设计的跨语料基准测试结果，工具被应用到系统中

    arXiv:2402.12372v1 Announce Type: new  Abstract: With the exponential growth of the life science literature, biomedical text mining (BTM) has become an essential technology for accelerating the extraction of insights from publications. Identifying named entities (e.g., diseases, drugs, or genes) in texts and their linkage to reference knowledge bases are crucial steps in BTM pipelines to enable information aggregation from different documents. However, tools for these two steps are rarely applied in the same context in which they were developed. Instead, they are applied in the wild, i.e., on application-dependent text collections different from those used for the tools' training, varying, e.g., in focus, genre, style, and text type. This raises the question of whether the reported performance of BTM tools can be trusted for downstream applications. Here, we report on the results of a carefully designed cross-corpus benchmark for named entity extraction, where tools were applied system
    
[^108]: 理解文本到SQL中噪声的影响：对BIRD-Bench基准测试的研究

    Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark

    [https://arxiv.org/abs/2402.12243](https://arxiv.org/abs/2402.12243)

    研究深入分析了文本到SQL领域中的噪声对模型的影响，并发现在BIRD-Bench基准测试中存在大量问题和标准查询中的噪声，这会显著影响模型的性能。

    

    Text-to-SQL涉及将自然语言翻译为结构化查询语言（SQL），对于使结构化数据库可以在没有专业知识的情况下得到广泛访问至关重要。然而，设计针对这些任务的模型是具有挑战性的，原因包括存在“噪声”，如模糊问题和语法错误。该研究对广泛使用的BIRD-Bench基准测试中噪声的分布和类型以及噪声对模型的影响进行了深入分析。虽然BIRD-Bench旨在模拟脏乱和嘈杂的数据库值，但并未包含问题和标准查询中的噪声和错误。我们发现数据集中问题和标准查询中的噪声普遍存在，跨领域存在不同程度的噪声，并且噪声类型之间分布不均匀。存在不正确的标准SQL查询，进而生成不正确的标准答案，对基准测试的影响显著。

    arXiv:2402.12243v1 Announce Type: new  Abstract: Text-to-SQL, which involves translating natural language into Structured Query Language (SQL), is crucial for enabling broad access to structured databases without expert knowledge. However, designing models for such tasks is challenging due to numerous factors, including the presence of 'noise,' such as ambiguous questions and syntactical errors. This study provides an in-depth analysis of the distribution and types of noise in the widely used BIRD-Bench benchmark and the impact of noise on models. While BIRD-Bench was created to model dirty and noisy database values, it was not created to contain noise and errors in the questions and gold queries. We found that noise in questions and gold queries are prevalent in the dataset, with varying amounts across domains, and with an uneven distribution between noise types. The presence of incorrect gold SQL queries, which then generate incorrect gold answers, has a significant impact on the ben
    
[^109]: 针对参数高效微调的权重投毒后门攻击的防御

    Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning

    [https://arxiv.org/abs/2402.12168](https://arxiv.org/abs/2402.12168)

    PEFT相对于全参数微调更容易受到权重投毒后门攻击的影响，提出了一个通过置信度识别受污染样本的毒化样本识别模块（PSIM），为权重投毒后门攻击提供稳健防御

    

    最近，针对语言模型应用提出并成功实施了各种参数高效微调（PEFT）策略。然而，这引发了一个问题，即当面对权重投毒后门攻击时，仅更新有限模型参数的PEFT是否构成安全漏洞。我们展示了PEFT相对于全参数微调方法更容易受到权重投毒后门攻击的影响，预定义的触发器仍然易受利用，预定义的目标在微调后依然保持高置信度。受到这一见解的启发，我们开发了一个利用PEFT的毒化样本识别模块（PSIM），通过置信度识别受污染样本，提供针对权重投毒后门攻击的稳健防御。具体而言，我们利用PEFT训练PSIM，带有随机重置样本标签。在推断过程中，

    arXiv:2402.12168v1 Announce Type: cross  Abstract: Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented. However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks. In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning. Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels. During the inference pr
    
[^110]: WKVQuant：量化大型语言模型的参数权重和键值缓存以提高性能

    WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More

    [https://arxiv.org/abs/2402.12065](https://arxiv.org/abs/2402.12065)

    该论文提出了WKVQuant，一种专为大型语言模型设计的量化框架，通过量化权重和键值缓存来改善性能。

    

    大型语言模型（LLMs）面临着部署挑战，主要是由于其巨大的内存需求和自回归文本生成过程的计算需求。本文通过关注LLMs的量化来解决这些挑战，量化是一种通过将模型参数和激活转换为低比特整数来减少内存消耗的技术。我们批判性地分析了现有的量化方法，识别出它们在平衡量化LLMs的准确性和效率方面的局限性。为了超越这些局限性，我们提出了WKVQuant，这是一个专为量化LLMs的参数权重和键值（KV）缓存而设计的PTQ框架。具体而言，我们引入了仅考虑过去的量化以改善注意力计算。此外，我们还介绍了二维量化策略来处理KV缓存的分布，以及一种跨块重建正则化方法以帮助模型压缩。

    arXiv:2402.12065v1 Announce Type: cross  Abstract: Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past-only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for pa
    
[^111]: 跨分词器蒸馏：用于LLM的通用logit蒸馏损失

    Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs

    [https://arxiv.org/abs/2402.12030](https://arxiv.org/abs/2402.12030)

    介绍了基于最优传输的通用logit蒸馏 (ULD) 损失，用于解决不同架构和分词器模型之间蒸馏的限制。

    

    部署几十亿参数的大型语言模型 (LLMs) 在大多数工业应用中可能并不切实际，原因是诸如成本、延迟限制和硬件可访问性等约束。知识蒸馏 (KD) 通过将资源密集型大模型的知识压缩到较小模型中提供了解决方案。存在多种策略，一些依赖于教师模型生成的文本，并可选择性地利用其logits来增强学习。然而，基于logits的这些方法通常要求教师和学生模型共享相同的分词器，限制了它们在不同LLM系列中的适用性。本文引入了基于最优传输的通用logit蒸馏 (ULD) 损失，以解决这一限制。我们的实验结果显示了ULD损失在启用不同架构和分词器的模型之间的蒸馏方面的有效性，为更广泛的应用铺平了道路。

    arXiv:2402.12030v1 Announce Type: new  Abstract: Deploying large language models (LLMs) of several billion parameters can be impractical in most industrial use cases due to constraints such as cost, latency limitations, and hardware accessibility. Knowledge distillation (KD) offers a solution by compressing knowledge from resource-intensive large models to smaller ones. Various strategies exist, some relying on the text generated by the teacher model and optionally utilizing his logits to enhance learning. However, these methods based on logits often require both teacher and student models to share the same tokenizer, limiting their applicability across different LLM families. In this paper, we introduce Universal Logit Distillation (ULD) loss, grounded in optimal transport, to address this limitation. Our experimental results demonstrate the effectiveness of ULD loss in enabling distillation across models with different architectures and tokenizers, paving the way to a more widespread
    
[^112]: 与团体互动：对宗教极端化影响的联系与破裂

    Bridging or Breaking: Impact of Intergroup Interactions on Religious Polarization

    [https://arxiv.org/abs/2402.11895](https://arxiv.org/abs/2402.11895)

    研究探讨团体互动对宗教极端化的影响，在印度 Twitter 用户中发现，政治和社会事件的团体间互动可以减少极端化。

    

    虽然接触不同观点可能会减少极端化，但当讨论对抗性时，也可能产生反效应并加剧极端化。在这里，我们研究了围绕重要事件的团体间互动是否影响社交网络中多数群体和少数群体之间的极端化。我们收集了约 70 万名印度 Twitter 用户在 2020 年参与与 COVID-19 相关话题讨论时的宗教身份数据。我们引入了一个基于推文文本的情境嵌入的新量度，用于帮助我们评估宗教群体之间的极端化。然后，我们使用元学习框架来研究围绕共同、政治和社会经济事件的异质处理效果对个体群体符合性的影响。我们发现，在政治和社会事件方面，团体间互动会减少极端化。

    arXiv:2402.11895v1 Announce Type: cross  Abstract: While exposure to diverse viewpoints may reduce polarization, it can also have a backfire effect and exacerbate polarization when the discussion is adversarial. Here, we examine the question whether intergroup interactions around important events affect polarization between majority and minority groups in social networks. We compile data on the religious identity of nearly 700,000 Indian Twitter users engaging in COVID-19-related discourse during 2020. We introduce a new measure for an individual's group conformity based on contextualized embeddings of tweet text, which helps us assess polarization between religious groups. We then use a meta-learning framework to examine heterogeneous treatment effects of intergroup interactions on an individual's group conformity in the light of communal, political, and socio-economic events. We find that for political and social events, intergroup interactions reduce polarization. This decline is we
    
[^113]: 大型语言模型对图形召回的微结构和准确性

    Microstructures and Accuracy of Graph Recall by Large Language Models

    [https://arxiv.org/abs/2402.11821](https://arxiv.org/abs/2402.11821)

    本研究首次系统研究了大型语言模型对图形召回的准确性和偏见微结构，探讨了它们与人类的异同以及对其他图形推理任务的影响。

    

    图形数据对许多应用至关重要，其中很多数据以文本格式描述关系。因此，准确地召回和编码先前文本中描述的图形是大型语言模型(LLMs)需要展示的基本但关键能力，以执行涉及图形结构信息的推理任务。人类在图形召回方面的表现已被认知科学家研究了几十年，发现其经常呈现与人类处理社会关系一致的某些结构性偏见模式。然而，迄今为止，我们很少了解LLMs在类似图形召回任务中的行为：它们召回的图形是否也呈现某些偏见模式，如果是，它们与人类的表现有何不同并如何影响其他图形推理任务？在这项研究中，我们进行了第一次对LLMs进行图形召回的系统研究，研究其准确性和偏见微结构（局部结构）。

    arXiv:2402.11821v1 Announce Type: cross  Abstract: Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structura
    
[^114]: 面向少样本生成的内容相关问答对话结构化思维启发

    Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations

    [https://arxiv.org/abs/2402.11770](https://arxiv.org/abs/2402.11770)

    使用结构化思维链提示的方法，在少样本情况下生成内容相关的问答对话，提高了代理程序对基础文档的忠诚度，训练强大的对话问答代理。

    

    我们引入了一种结构化思维链（SCoT）提示方法，使用预训练的大型语言模型（LLM）生成基于内容的多轮问答对话。我们的提议的核心是将复杂任务结构化分解为状态机中的多个状态，以便执行对应于各种子任务（例如内容阅读和话语生成）的动作。每个状态利用一组独特的资源，包括提示和（可选）额外工具以增强生成过程。我们的实验结果表明，对于幻觉减轻，使用具有指定状态的SCoT提示可以使对接地文档的代理忠诚度提高高达16.8％。当用作训练数据时，仅从6个基于维基百科的种子示范合成的开放域对话训练出强大的对话问答代理程序；在领域外评估中，

    arXiv:2402.11770v1 Announce Type: new  Abstract: We introduce a structured chain-of-thought (SCoT) prompting approach to generating content-grounded multi-turn question-answer conversations using a pre-trained large language model (LLM). At the core of our proposal is a structured breakdown of the complex task into a number of states in a state machine, so that actions corresponding to various subtasks, e.g., content reading and utterance generation, can be executed in their own dedicated states. Each state leverages a unique set of resources including prompts and (optionally) additional tools to augment the generation process. Our experimental results show that SCoT prompting with designated states for hallucination mitigation increases agent faithfulness to grounding documents by up to 16.8%. When used as training data, our open-domain conversations synthesized from only 6 Wikipedia-based seed demonstrations train strong conversational QA agents; in out-of-domain evaluation, for exam
    
[^115]: MARS：用于生成式LLMs中不确定性估计的意义感知响应评分

    MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs

    [https://arxiv.org/abs/2402.11756](https://arxiv.org/abs/2402.11756)

    MARS提出了一种新的评分函数MARS，考虑了生成序列中每个标记的语义贡献，该方法改进了生成式LLMs中的不确定性估计性能。

    

    生成式大型语言模型（LLMs）因在各种任务中的卓越表现而被广泛利用。然而，它们产生不准确或误导性输出的倾向可能带来潜在风险，尤其是在高风险环境中。因此，估计生成式LLM输出的正确性是增强可靠性的重要任务。生成式LLMs中的不确定性估计（UE）是一个不断发展的领域，其中SOTA基于概率的方法通常采用长度标准化评分。在这项工作中，我们提出了一种名为意义感知响应评分（MARS）的替代长度标准化评分的UE方法。MARS是一种考虑在问题的上下文中生成序列中每个标记的语义贡献的新型评分函数。我们证明将MARS整合到UE方法中会在UE性能上带来普遍和显著的改进。我们使用三种不同的闭卷式问答来进行实验

    arXiv:2402.11756v1 Announce Type: new  Abstract: Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book questi
    
[^116]: 从偏见到平等：去偏巨型语言模型词嵌入的新方法

    From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings

    [https://arxiv.org/abs/2402.11512](https://arxiv.org/abs/2402.11512)

    提出了DeepSoftDebias算法，在不同领域数据集、准确度指标和NLP任务中全面评估，发现其在减少性别、种族和宗教偏见方面优于现有最先进方法

    

    嵌入在巨型语言模型的有效性中扮演着重要角色。它们是这些模型把握上下文关系、促进更细致语言理解以及在许多需要对人类语言有基本理解的复杂任务上表现出色的基石。鉴于这些嵌入往往自身反映或展示偏见，因此这些模型可能也会无意中学习这种偏见。在这项研究中，我们在开创性前人研究基础上提出了DeepSoftDebias，这是一种使用神经网络进行“软去偏”的算法。我们在各类最先进数据集、准确度指标和具有挑战的自然语言处理任务中全面评估了这个算法。我们发现DeepSoftDebias在减少性别、种族和宗教偏见方面优于目前的最先进方法。

    arXiv:2402.11512v1 Announce Type: new  Abstract: Embeddings play a pivotal role in the efficacy of Large Language Models. They are the bedrock on which these models grasp contextual relationships and foster a more nuanced understanding of language and consequently perform remarkably on a plethora of complex tasks that require a fundamental understanding of human language. Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias. In this work, we build on the seminal previous work and propose DeepSoftDebias, an algorithm that uses a neural network to perform `soft debiasing'. We exhaustively evaluate this algorithm across a variety of SOTA datasets, accuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias outperforms the current state-of-the-art methods at reducing bias across gender, race, and religion.
    
[^117]: 异类自动提示的不合理有效性

    The Unreasonable Effectiveness of Eccentric Automatic Prompts

    [https://arxiv.org/abs/2402.10949](https://arxiv.org/abs/2402.10949)

    异类自动提示的不合理有效性研究了大型语言模型在处理各种提示时的表现，结果显示在大多数情况下，包括“积极思考”提示会对模型性能产生积极影响。

    

    大型语言模型（LLMs）展示了出色的问题解决和基本数学能力。然而，它们的功效高度依赖于提示的制定。本研究旨在量化将“积极思考”纳入系统提示消息的影响，然后将其与系统化提示优化进行比较。我们评估了60种系统消息片段的性能，分别使用和不使用Chain of Thought提示，跨三个参数范围从70亿到70亿个变量的模型，在GSM8K数据集上进行测试。我们的发现表明，结果并不在所有模型中普遍适用。在大多数情况下，包括“积极思考”提示会积极影响模型性能。然而，值得注意的是，Llama2-70B在不使用Chain of Thought时是个例外，因为发现最佳系统消息实际上是没有消息。考虑到组合复杂性，以及其导至的加# Truncated due to exceeding character limit.

    arXiv:2402.10949v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable problem-solving and basic mathematics abilities. However, their efficacy is highly contingent on the formulation of the prompt. This study endeavors to quantify the influence of incorporating "positive thinking" into the system message of the prompt, then compare that to systematic prompt optimization. We assess the performance of 60 combinations of system message snippets, tested with and without Chain of Thought prompting, across three models with parameters ranging from 7 to 70 billion on the GSM8K dataset. Our findings reveal that results do not universally generalize across models. In most instances, the inclusion of "positive thinking" prompts positively affected model performance. Notably, however, Llama2-70B exhibited an exception when not utilizing Chain of Thought, as the optimal system message was found to be none at all. Given the combinatorial complexity, and thus com
    
[^118]: 人类还是大型语言模型作为裁判？一项关于判决偏见的研究

    Humans or LLMs as the Judge? A Study on Judgement Biases

    [https://arxiv.org/abs/2402.10669](https://arxiv.org/abs/2402.10669)

    提出了一种新框架来研究LLM和人类裁判的偏见，揭示人类和LLM裁判在面对干扰时的脆弱性，强调评估现有LLM性能的挑战。

    

    采用人类和大型语言模型（LLM）作为裁判（即人类和LLM作为裁判）来评估现有LLM性能的做法近来备受关注。然而，这种方法同时可能引入人类和LLM裁判的潜在偏见，质疑评估结果的可靠性。本文提出了一种新颖的框架，用于研究LLM和人类裁判的5种偏见。我们整理了一个包含142个样本的数据集，涉及修订的布卢姆分类法，并进行了成千上万次的人类和LLM评估。结果表明，人类和LLM裁判在不同程度上都容易受到干扰，即使最尖端的裁判也存在相当大的偏见。我们进一步利用他们的弱点对LLM裁判进行攻击。希望我们的工作能提醒社群关于人类和LLM作为裁判在面对干扰时的脆弱性，以及发展的紧迫性。

    arXiv:2402.10669v1 Announce Type: new  Abstract: Adopting human and large language models (LLM) as judges (\textit{a.k.a} human- and LLM-as-a-judge) for evaluating the performance of existing LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLM judges, questioning the reliability of the evaluation results. In this paper, we propose a novel framework for investigating 5 types of biases for LLM and human judges. We curate a dataset with 142 samples referring to the revised Bloom's Taxonomy and conduct thousands of human and LLM evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the most cutting-edge judges possess considerable biases. We further exploit their weakness and conduct attacks on LLM judges. We hope that our work can notify the community of the vulnerability of human- and LLM-as-a-judge against perturbations, as well as the urgency of develop
    
[^119]: 名词短语中头部的最佳位置。指示语、数词、形容词和名词的案例。

    The optimal placement of the head in the noun phrase. The case of demonstrative, numeral, adjective and noun

    [https://arxiv.org/abs/2402.10311](https://arxiv.org/abs/2402.10311)

    本研究旨在探讨句法依赖距离最小化与意外减少最小化原则在名词短语中的冲突，结论显示当涉及的单词较少且单词较短时，意外减少可能会超越句法依赖距离优化。

    

    一句话的词序由多种原则塑造。句法依赖距离最小化原则与意外减少最小化原则（或可预测性最大化）在单一头部的句法依赖结构中存在冲突：前者预测头部应该放置在线性排列的中心，后者预测头部应该放置在两端之一（要么在首位，要么在末位）。一个关键问题是何时意外减少（或可预测性最大化）应该超越句法依赖距离最小化。在单一头部结构的背景下，预测在满足两个条件时更有可能发生，即（a）涉及的单词较少，并且（b）单词较短。在这里，我们在由指示语、数词、形容词和名词组成的名词短语上测试了这一预测。我们发现，在首选顺序中...（缺失部分无法提供完整翻译）

    arXiv:2402.10311v1 Announce Type: new  Abstract: The word order of a sentence is shaped by multiple principles. The principle of syntactic dependency distance minimization is in conflict with the principle of surprisal minimization (or predictability maximization) in single head syntactic dependency structures: while the former predicts that the head should be placed at the center of the linear arrangement, the latter predicts that the head should be placed at one of the ends (either first or last). A critical question is when surprisal minimization (or predictability maximization) should surpass syntactic dependency distance minimization. In the context of single head structures, it has been predicted that this is more likely to happen when two conditions are met, i.e. (a) fewer words are involved and (b) words are shorter. Here we test the prediction on the noun phrase when its composed of a demonstrative, a numeral, an adjective and a noun. We find that, across preferred orders in l
    
[^120]: 重塑RLHF中的信息结构：基于图论的奖励泛化视角

    Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective

    [https://arxiv.org/abs/2402.10184](https://arxiv.org/abs/2402.10184)

    本研究通过设计奖励建模过程中的数据集信息结构，从图论的视角提出了RLHF中奖励泛化的问题，以解决多样的环境、低成本标注和可靠的对齐性能间的不兼容性。

    

    在强化学习从人类反馈中（RLHF）存在一个三难问题：高度多样的环境、低标注成本和可靠的对齐性能之间的不兼容性。本文旨在通过设计奖励建模过程中的数据集信息结构来缓解这种不兼容性。具体而言，我们重新审视了RLHF过程，并提出了一个理论框架将其描绘为文本分布上的自动编码过程。我们的框架形式化了RLHF目标，即确保人类偏好与大型语言模型（LLM）行为之间的分布一致性。基于这个框架，我们系统地研究了RLHF奖励建模阶段中信息结构的性能影响。为了进一步理解奖励建模阶段中的奖励泛化，我们引入了一种基于随机图论的方法来建模语义空间中的泛化。其中的关键见解是...

    arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
    
[^121]: 有限预算下的迅速学习最佳臂识别

    Best Arm Identification for Prompt Learning under a Limited Budget

    [https://arxiv.org/abs/2402.09723](https://arxiv.org/abs/2402.09723)

    这项工作提出了一种在提示学习中考虑有限预算约束的方法，通过建立提示学习和多臂赌博机中固定预算最佳臂识别之间的联系，提出了一个通用框架TRIPLE，通过利用聚类和嵌入思想实现了两个增强方法。

    

    大型语言模型（LLMs）的显著指令跟随能力引发了对自动学习合适提示的兴趣。然而，虽然提出了许多有效的方法，但在学习过程中产生的成本（例如访问LLM和评估响应）尚未得到考虑。为克服这个限制，本工作在提示学习中明确引入了有限预算约束。为了开发有原则的解决方案，本研究在提示学习和多臂赌博机的固定预算最佳臂识别（BAI-FB）之间建立了一种新的联系。基于这种联系，提出了一个通用框架TRIPLE（用于提示学习的最佳臂识别），以系统地利用BAI-FB在提示学习中的力量。提示学习的独特特点进一步通过利用聚类和嵌入思想提出了TRIPLE的两个基于嵌入的增强方法。

    arXiv:2402.09723v1 Announce Type: cross  Abstract: The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically learning suitable prompts. However, while many effective methods have been proposed, the cost incurred during the learning process (e.g., accessing LLM and evaluating the responses) has not been considered. To overcome this limitation, this work explicitly incorporates a finite budget constraint into prompt learning. Towards developing principled solutions, a novel connection is established between prompt learning and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB). Based on this connection, a general framework TRIPLE (besT aRm Identification for Prompt LEarning) is proposed to harness the power of BAI-FB in prompt learning systematically. Unique characteristics of prompt learning further lead to two embedding-based enhancements of TRIPLE by exploiting the ideas of clustering and fun
    
[^122]: 数据到文本自然语言生成研究的系统性回顾

    A Systematic Review of Data-to-Text NLG

    [https://arxiv.org/abs/2402.08496](https://arxiv.org/abs/2402.08496)

    这篇系统性回顾全面分析了数据到文本自然语言生成研究的现状，提出未来方向，并解决了相关挑战。

    

    这篇系统性回顾旨在全面分析数据到文本生成研究的现状，重点是确定研究空白，提供未来方向，并解决回顾中发现的挑战。我们对文献进行了全面的检查，包括方法、数据集、评估指标、应用、多语言性和幻觉缓解措施。我们的回顾为这个快速发展的领域的未来研究提供了路线图。

    This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.
    
[^123]: ChatCell: 利用自然语言促进单细胞分析

    ChatCell: Facilitating Single-Cell Analysis with Natural Language

    [https://arxiv.org/abs/2402.08303](https://arxiv.org/abs/2402.08303)

    ChatCell是一个利用自然语言促进单细胞分析的工具，通过词汇适应和统一序列生成，它具备深厚的专业知识和适应各种分析任务的能力。

    

    随着大型语言模型(LLMs)的快速发展，它们在科学中的影响日益突出。LLMs在任务泛化和自由对话方面的新兴能力可以极大地推进化学和生物学等领域。然而，单细胞生物学这个构成生物体基础构件的领域仍面临一些挑战。当前方法在知识门槛和可扩展性方面存在限制，阻碍了LLMs在掌握单细胞数据方面的充分利用，影响了直接可访问和快速迭代的能力。为此，我们引入了ChatCell，通过利用词汇适应和统一序列生成，它在单细胞生物学领域获得了深厚的专业知识和适应各种分析任务的能力，标志着一种范式转变。

    As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
    
[^124]: Lissard：长而简单的顺序推理数据集

    Lissard: Long and Simple Sequential Reasoning Datasets

    [https://arxiv.org/abs/2402.07859](https://arxiv.org/abs/2402.07859)

    Lissard是一个包含七个任务的基准，用于评估模型处理和生成各种序列长度的能力，需要重复的过程执行。评估结果显示随着序列复杂性增加，所有模型的性能都呈一致下降趋势。

    

    语言模型现在能够解决需要处理数十万个标记的长序列的任务。然而，它们在需要重复使用简单规则的任务上常常失败，甚至在比训练中看到的序列要短得多的情况下也是如此。例如，最先进的LLMs可以在两个列表中找到共同项，列表中的项最多可达20个，但是当列表中的项达到80个时，它们会失败。在本文中，我们介绍了Lissard，这是一个包含七个任务的基准，旨在评估模型处理和生成各种序列长度的能力，需要重复的过程执行。我们评估了开源模型（Mistral-7B和Mixtral-8x7B）和专有模型（GPT-3.5和GPT-4），结果显示随着序列复杂性增加，所有模型的性能都呈一致下降趋势。数据集和代码可在https://github.com/unicamp-dl/Lissard获得。

    Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens. However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training. For example, state-of-the-art LLMs can find common items in two lists with up to 20 items but fail when lists have 80 items. In this paper, we introduce Lissard, a benchmark comprising seven tasks whose goal is to assess the ability of models to process and generate wide-range sequence lengths, requiring repetitive procedural execution. Our evaluation of open-source (Mistral-7B and Mixtral-8x7B) and proprietary models (GPT-3.5 and GPT-4) show a consistent decline in performance across all models as the complexity of the sequence increases. The datasets and code are available at https://github.com/unicamp-dl/Lissard
    
[^125]: 大型语言模型：一项调查

    Large Language Models: A Survey

    [https://arxiv.org/abs/2402.06196](https://arxiv.org/abs/2402.06196)

    大型语言模型（LLMs）吸引了很多关注，因为它们在自然语言任务上的强大表现。该研究领域发展迅速，包括了各种著名的LLMs、构建和增强LLMs的技术、以及流行的LLM数据集和评估指标。

    

    大型语言模型（LLMs）由于其在各种自然语言任务上的出色表现而受到了很多关注，自2022年11月ChatGPT发布以来。LLMs通过在大量文本数据上训练模型的数十亿参数来获得广泛的通用语言理解和生成能力，这符合缩放定律的预测。LLMs的研究领域尽管非常新，但在许多不同方面正在快速发展。在本文中，我们回顾了一些最著名的LLMs，包括三个流行的LLM系列（GPT、LLaMA、PaLM），并讨论了它们的特点、贡献和限制。我们还概述了构建和增强LLMs的技术。然后，我们调查了为LLM训练、微调和评估准备的流行数据集，审查了广泛使用的LLM评估指标，并比较了几个流行LLM在一组代表性基准上的性能。

    Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we co
    
[^126]: 关于大规模语言模型中零阶联邦调整的收敛性

    On the Convergence of Zeroth-Order Federated Tuning in Large Language Models

    [https://arxiv.org/abs/2402.05926](https://arxiv.org/abs/2402.05926)

    我们的研究提出了一种名为FedMeZO的方法，以在零阶联邦学习和大规模语言模型之间实现内存高效的调整。我们的理论和实证证据表明FedMeZO不仅收敛速度快于传统的一阶方法，而且在个性化的联邦策略中具有关键的作用。

    

    联邦学习（FL）和大规模语言模型（LLM）的融合为隐私保护的自然语言处理带来了新时代。然而，精调LLM所需的强大内存要求在部署到边缘设备时会面临重大挑战，因为这些设备的计算资源有限。为了解决这个问题，我们在联邦环境中探索了内存高效的零阶优化的全新整合，我们称之为FedMeZO。我们的研究是第一个在LLM背景下考察FedMeZO的理论基础的研究，涉及到大参数空间对优化行为的影响、收敛性的建立以及为个性化的联邦策略确定关键参数的问题。我们广泛的实证证据支持了这个理论，表明FedMeZO不仅比传统的一阶方法（如SGD）收敛更快，而且明显...

    The confluence of Federated Learning (FL) and Large Language Models (LLMs) is ushering in a new era in privacy-preserving natural language processing. However, the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on edge devices with limited computational resources. To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, a synergy we denote as FedMeZO. Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of LLMs, tackling key questions regarding the influence of large parameter spaces on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized federated strategies. Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as SGD but also signific
    
[^127]: LESS：用于目标指导调整的选择有影响力的数据

    LESS: Selecting Influential Data for Targeted Instruction Tuning

    [https://arxiv.org/abs/2402.04333](https://arxiv.org/abs/2402.04333)

    LESS是一种优化感知且实际高效的算法，用于在大型语言模型中选择具有影响力的数据以开发特定能力，它采用低秩梯度相似性搜索方法进行指令数据选择。

    

    指令调整已经在大型语言模型中释放出强大的能力，有效地使用组合数据集来开发通用聊天机器人。然而，实际应用往往需要一套专门的技能（例如推理）。挑战在于从这些广泛的数据集中识别出最相关的数据，以有效开发特定的能力，我们将这种情况称为目标指导调整。我们提出了LESS，一种优化感知且实际高效的算法，以有效估计数据影响并执行适用于指令数据选择的低秩梯度相似性搜索。关键在于LESS将现有的影响公式调整为与Adam优化器和可变长度指令数据一起工作。LESS首先构建了一个具有低维梯度特征的高度可重用和可传递的梯度数据存储库，然后根据它们与具有特定能力的少样本示例的相似度选择示例。实验证明，t

    Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that t
    
[^128]: 统一的多模态大型语言模型的幻觉检测

    Unified Hallucination Detection for Multimodal Large Language Models

    [https://arxiv.org/abs/2402.03190](https://arxiv.org/abs/2402.03190)

    该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。

    

    尽管在多模态任务方面取得了重大进展，多模态大型语言模型(MLLMs)仍然存在幻觉的严重问题。因此，可靠地检测MLLMs中的幻觉已成为模型评估和实际应用部署保障的重要方面。之前在这个领域的研究受到了狭窄的任务焦点、不足的幻觉类别涵盖范围以及缺乏详细的细粒度的限制。针对这些挑战，我们的工作扩展了幻觉检测的研究范围。我们提出了一个新颖的元评估基准方法，MHaluBench，精心设计以促进幻觉检测方法的进展评估。此外，我们揭示了一个新颖的统一多模态幻觉检测框架，UNIHD，它利用一套辅助工具来稳健地验证幻觉的发生。我们通过实验证明了UNIHD的有效性。

    Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
    
[^129]: 多模态：文本和图像的多模态理解排行榜

    Multi: Multimodal Understanding Leaderboard with Text and Images

    [https://arxiv.org/abs/2402.03173](https://arxiv.org/abs/2402.03173)

    Multi是一个多模态理解的排行榜，提供了一个综合数据集，评估多模态大型语言模型对理解复杂图表和科学问题的能力。它兼具准确和开放式的回答形式，挑战MLLM的各种任务，并包含超过18,000个问题。

    

    多模态大型语言模型（MLLM）的快速进展强调了向学术界引入具有挑战性而又真实的基准的需求。现有的基准主要关注简单的自然图像理解，但Multi成为了MLLM的尖端基准，提供了一个综合性的数据集，用于评估MLLM对理解复杂图表和科学问题的能力。该基准反映了当前真实的考试风格，提供多模态的输入，并要求准确或开放式的回答，类似于现实中的学校考试。它通过各种任务挑战MLLM，从公式推导到图像细节分析，以及跨模态推理。Multi包括超过18,000个问题，重点关注不同格式的基于科学的问答。我们还引入了Multi-Elite，一个包含500个问题的子集，用于测试MLLM的极端情况，以及Multi-Extend，通过超过4..。

    Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning. Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4
    
[^130]: APT-Pipe: 用于社交计算数据标注的自动提示调整工具

    APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data Annotation

    [https://arxiv.org/abs/2402.01697](https://arxiv.org/abs/2402.01697)

    APT-Pipe is an automated prompt-tuning tool that enhances ChatGPT's text classification performance by automatically tuning prompts on any given dataset, resulting in a significant improvement in weighted F1-score across twelve experimented datasets.

    

    最近的研究突出了像ChatGPT这样的LLM应用在社交计算文本标注中的潜力。然而，已经人们已经知道性能取决于输入提示的质量。为了解决这个问题，已经有了大量的研究来探索提示调整的技术和指南，试图改善提示的质量。然而，这些方法往往依赖于手工努力和对正在标注的数据集的先前知识。为了解决这个限制，我们提出了一个自动化的提示调整流水线APT-Pipe。APT-Pipe旨在自动调整提示，以提高ChatGPT在任何给定数据集上的文本分类性能。我们实现了APT-Pipe，并在12个不同的文本分类数据集上进行了测试。我们发现APT-Pipe调整的提示有助于ChatGPT在12个实验数据集中有9个获得更高的加权F1分数，平均改进了7.01％。我们进一步突出了APT-Pipe作为一个框架的灵活性。

    Recent research has highlighted the potential of LLM applications, like ChatGPT, for performing label annotation on social computing text. However, it is already well known that performance hinges on the quality of the input prompts. To address this, there has been a flurry of research into prompt tuning -- techniques and guidelines that attempt to improve the quality of prompts. Yet these largely rely on manual effort and prior knowledge of the dataset being annotated. To address this limitation, we propose APT-Pipe, an automated prompt-tuning pipeline. APT-Pipe aims to automatically tune prompts to enhance ChatGPT's text classification performance on any given dataset. We implement APT-Pipe and test it across twelve distinct text classification datasets. We find that prompts tuned by APT-Pipe help ChatGPT achieve higher weighted F1-score on nine out of twelve experimented datasets, with an improvement of 7.01% on average. We further highlight APT-Pipe's flexibility as a framework by 
    
[^131]: 改进标签的TENOR：重新评估用于内容分析的主题模型

    Improving the TENOR of Labeling: Re-evaluating Topic Models for Content Analysis

    [https://arxiv.org/abs/2401.16348](https://arxiv.org/abs/2401.16348)

    本研究重新评估了神经、监督和经典主题模型在内容分析中的效果，结果显示上下文神经主题模型在聚类评估和人类评估方面表现最佳。

    

    主题模型是理解文本集合的一种流行工具，但它们的评估一直是一个争论点。自动化评估指标如连贯性经常被使用，然而对于神经主题模型（NTMs）他们的有效性受到质疑，可能忽略了模型在现实世界应用中的益处。为此，我们在基于交互式任务的环境中首次评估了神经、监督和经典主题模型。我们将主题模型与分类器结合，并测试它们帮助人类进行内容分析和文档注释的能力。从模拟、真实用户和专家的试点研究中，上下文神经主题模型在聚类评估指标和人类评估方面表现最佳；然而，LDA在我们的模拟实验和用户研究结果中与另外两种NTMs竞争激烈，与连贯性分数所暗示的情况相反。我们表明当前的自动化指标并不提供完整的评估。

    arXiv:2401.16348v2 Announce Type: replace  Abstract: Topic models are a popular tool for understanding text collections, but their evaluation has been a point of contention. Automated evaluation metrics such as coherence are often used, however, their validity has been questioned for neural topic models (NTMs) and can overlook a models benefits in real world applications. To this end, we conduct the first evaluation of neural, supervised and classical topic models in an interactive task based setting. We combine topic models with a classifier and test their ability to help humans conduct content analysis and document annotation. From simulated, real user and expert pilot studies, the Contextual Neural Topic Model does the best on cluster evaluation metrics and human evaluations; however, LDA is competitive with two other NTMs under our simulated experiment and user study results, contrary to what coherence scores suggest. We show that current automated metrics do not provide a complete
    
[^132]: PRewrite: 使用强化学习的提示重写

    PRewrite: Prompt Rewriting with Reinforcement Learning

    [https://arxiv.org/abs/2401.08189](https://arxiv.org/abs/2401.08189)

    本文提出了一种基于强化学习的自动化工具PRewrite，用于重写提示草案并生成高效的新提示，以解决提示工程中的挑战。

    

    arXiv:2401.08189v2 公告类型: 替换 摘要: 提示工程对于基于LLM的应用程序的开发至关重要。然而，通常以“试错”的方式手动完成。这种手动程序可能耗时，效果不佳，并且在许多情况下生成的提示都是次优的。即使对那些看似运作良好的提示，始终存在一个悬而未决的问题：是否可以通过进一步修改使提示变得更好呢？为了解决这些问题，在本文中，我们研究了提示工程自动化。我们考虑了一个特定的使用情景，即开发者/用户已经起草了初始提示，但缺乏时间/专业知识来优化它们。我们提出了PRewrite，一个自动化工具，可重写这些草案，并生成高效的新提示。PRewrite基于强化学习（RL）框架，允许端到端优化，我们的设计允许RL搜索在大动作空间中进行。

    arXiv:2401.08189v2 Announce Type: replace  Abstract: Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a "trial and error" fashion. This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   To address these questions, in this paper, we investigate prompt engineering automation. We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them. We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts. PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space. The automated to
    
[^133]: 大型语言模型推理效率的提升：投机解码的全面调查

    Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding

    [https://arxiv.org/abs/2401.07851](https://arxiv.org/abs/2401.07851)

    投机解码作为一种新颖的解码范式，能够加速大型语言模型推理过程，提供了全面的概述和分析。

    

    为了减少大型语言模型（LLMs）中自回归解码导致的推理延迟，投机解码已经成为LLMs推理的一种新颖解码范式。该方法在每个解码步骤中首先高效地起草几个未来标记，然后并行验证这些标记。与自回归解码不同，投机解码促进了每个步骤同时解码多个标记，从而加速了推理。本文提供了这一有前景的解码范式的全面概述和分析。我们首先提供了对投机解码的正式定义和公式化。然后，我们就其关键方面进行了深入讨论，如起草者选择和验证策略。此外，我们在第三方测试环境下对主要方法进行了比较分析。我们希望这项工作能够成为推动进一步研究投机解码的催化剂。

    arXiv:2401.07851v2 Announce Type: replace  Abstract: To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first drafts several future tokens efficiently and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, such as drafter selection and verification strategies. Furthermore, we present a comparative analysis of leading methods under third-party testing environments. We aim for this work to serve as a catalyst for further research on Speculative Decoding,
    
[^134]: MAPLE: 多语言参数高效微调大型语言模型的评估

    MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of Large Language Models

    [https://arxiv.org/abs/2401.07598](https://arxiv.org/abs/2401.07598)

    MAPLE通过在两个多语言指令调整数据集上对LLama-2-7B和Mistral-7B模型进行微调，在六项涵盖40种语言的下游任务中发现了微调对模型性能的影响。

    

    Parameter Efficient Finetuning (PEFT)已经成为改善大型语言模型（LLMs）性能的可行解决方案，而无需大量资源和计算。本文在两个合成多语言指令调整数据集上微调LLama-2-7B和Mistral-7B模型，以确定其对六个涵盖四十种语言的下游任务上模型性能的影响。此外，我们尝试不同的参数，例如用于低秩适应的秩和量化值，以确定它们对下游性能的影响，发现更高的秩和更高的hig

    arXiv:2401.07598v2 Announce Type: replace  Abstract: Parameter Efficient Finetuning (PEFT) has emerged as a viable solution for improving the performance of Large Language Models (LLMs) without requiring massive resources and compute. Prior work on multilingual evaluation has shown that there is a large gap between the performance of LLMs on English and other languages. Further, there is also a large gap between the performance of smaller open-source models and larger LLMs. Finetuning can be an effective way to bridge this gap and make language models more equitable. In this work, we finetune the LLama-2-7B and Mistral-7B models on two synthetic multilingual instruction tuning datasets to determine its effect on model performance on six downstream tasks covering forty languages in all. Additionally, we experiment with various parameters, such as rank for low-rank adaptation and values of quantisation to determine their effects on downstream performance and find that higher rank and hig
    
[^135]: 规模化模型编辑会导致渐进性和突发性遗忘

    Model Editing at Scale leads to Gradual and Catastrophic Forgetting

    [https://arxiv.org/abs/2401.07453](https://arxiv.org/abs/2401.07453)

    评估了当前模型编辑方法在规模化情况下的表现，发现随着模型被顺序编辑多个事实，它会逐渐遗忘先前的事实及执行下游任务的能力。

    

    在大型语言模型中编辑知识是一种具有吸引力的能力，它使我们能够在预训练期间纠正错误学习的事实，同时使用不断增长的新事实列表更新模型。我们认为，为了使模型编辑具有实际效用，我们必须能够对同一模型进行多次编辑。因此，我们评估了当前规模下的模型编辑方法，重点关注两种最先进的方法：ROME 和 MEMIT。我们发现，随着模型被顺序编辑多个事实，它不断地遗忘先前编辑过的事实以及执行下游任务的能力。这种遗忘分为两个阶段--初始的渐进性遗忘阶段，随后是突然或灾难性的遗忘。

    arXiv:2401.07453v2 Announce Type: replace-cross  Abstract: Editing knowledge in large language models is an attractive capability to have which allows us to correct incorrectly learnt facts during pre-training, as well as update the model with an ever-growing list of new facts. While existing model editing techniques have shown promise, they are usually evaluated using metrics for reliability, specificity and generalization over one or few edits. We argue that for model editing to have practical utility, we must be able to make multiple edits to the same model. With this in mind, we evaluate the current model editing methods at scale, focusing on two state of the art methods: ROME and MEMIT. We find that as the model is edited sequentially with multiple facts, it continually forgets previously edited facts and the ability to perform downstream tasks. This forgetting happens in two phases -- an initial gradual but progressive forgetting phase followed by abrupt or catastrophic forgettin
    
[^136]: EHRAgent：代码赋能大型语言模型在电子健康记录上进行少样本复杂表格推理

    EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records

    [https://arxiv.org/abs/2401.07128](https://arxiv.org/abs/2401.07128)

    EHRAgent是一个由代码接口赋能的大型语言模型代理，用于自主生成和执行多表格推理代码，通过错误信息学习改进生成的代码，结合长期记忆选择并建立在过去经验中的成功案例。

    

    大型语言模型（LLMs）在规划和工具利用方面表现出色，但在医学问题解决方面尚未有太多开发。我们提出EHRAgent，这是一个由代码接口赋能的LLM代理，用于在电子健康记录（EHRs）中自主生成和执行多表格推理的代码。首先，我们将EHR问答任务制定为工具使用规划过程，将一个复杂任务高效地分解为一系列可管理的操作。通过集成交互式编码和执行反馈，EHRAgent从错误消息中学习并通过迭代改进最初生成的代码。此外，我们通过结合长期记忆来增强LLM代理，使EHRAgent能够有效地选择并建立在过去经验中最相关的成功案例上。在三个真实世界的多表格EHR数据集上进行的实验显示...

    arXiv:2401.07128v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving. We propose EHRAgent, an LLM agent empowered with a code interface, to autonomously generate and execute code for multi-tabular reasoning within electronic health records (EHRs). First, we formulate an EHR question-answering task into a tool-use planning process, efficiently decomposing a complicated task into a sequence of manageable actions. By integrating interactive coding and execution feedback, EHRAgent learns from error messages and improves the originally generated code through iterations. Furthermore, we enhance the LLM agent by incorporating long-term memory, which allows EHRAgent to effectively select and build upon the most relevant successful cases from past experiences. Experiments on three real-world multi-tabular EHR datasets show t
    
[^137]: 大型语言模型可以学习时间推理

    Large Language Models Can Learn Temporal Reasoning

    [https://arxiv.org/abs/2401.06853](https://arxiv.org/abs/2401.06853)

    本文提出了一个新的TG-LLM框架，以语言为基础进行时间推理，通过教导LLM将上下文翻译成时间图，并使用CoTs指导LLM进行符号推理。

    

    尽管大型语言模型（LLMs）展示了出色的推理能力，但它们并非没有缺陷和不准确之处。最近的研究介绍了各种方法来减轻这些局限性。特别是，时间推理（TR）对LLMs提出了重大挑战，因为它依赖于多样的时间表达和复杂的上下文细节。本文中，我们提出了TG-LLM，一个致力于基于语言的时间推理的新框架。具体而言，我们首先教导LLM将上下文翻译成时间图（TG）。我们构建了一个全可控且需要最少监督的合成数据集，用于在这个图翻译任务上进行微调。我们在实验证实，学习在我们数据集上的TG提取能力可以转移到其他TR任务和基准测试上。除此之外，我们使用CoTs引导LLM通过TG进行符号推理。

    arXiv:2401.06853v2 Announce Type: replace  Abstract: While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal expressions and intricate contextual details. In this paper, we propose TG-LLM, a new framework towards language-based TR. To be specific, we first teach LLM to translate the context into a temporal graph (TG). A synthetic dataset, which is fully controllable and requires minimal supervision, is constructed for fine-tuning on this graph translation task. We confirm in experiments that the capability of TG extraction learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we guide LLM to perform symbolic reasoning over the TG via Chain of Thoughts (CoTs) bootstrappin
    
[^138]: 评估提示方法对ChatGPT的数学能力的影响

    Assessing the Impact of Prompting Methods on ChatGPT's Mathematical Capabilities

    [https://arxiv.org/abs/2312.15006](https://arxiv.org/abs/2312.15006)

    三种提示方法对ChatGPT的数学能力并未产生一贯性改进效果，部分方法甚至导致性能下降

    

    本研究批判性地评估了提示方法在提升大型语言模型（LLMs）的数学推理能力方面的功效。该研究使用了三种规定性提示方法 - 简单提示、个人提示和对话提示 - 这些方法以提升LLMs语言任务效果而闻名。我们在OpenAI的LLM闲聊机器人ChatGPT-3.5上进行此分析，涵盖了来自MATH、GSM8K和MMLU数据集的广泛问题集合，这些问题涵盖了各种数学挑战。针对每个数据集调整的评分脚本用于确定这些提示干预在增强模型数学分析能力方面的效果。与预期相反，我们的实证分析显示，所检验的方法均未在持续改进ChatGPT-3.5基准表现上，部分方法甚至导致明显的退化。我们的发现表明，提示策略未必能提高模型的数学分析能力。

    arXiv:2312.15006v2 Announce Type: replace  Abstract: This study critically evaluates the efficacy of prompting methods in enhancing the mathematical reasoning capability of large language models (LLMs). The investigation uses three prescriptive prompting methods - simple, persona, and conversational prompting - known for their effectiveness in enhancing the linguistic tasks of LLMs. We conduct this analysis on OpenAI's LLM chatbot, ChatGPT-3.5, on extensive problem sets from the MATH, GSM8K, and MMLU datasets, encompassing a broad spectrum of mathematical challenges. A grading script adapted to each dataset is used to determine the effectiveness of these prompting interventions in enhancing the model's mathematical analysis power. Contrary to expectations, our empirical analysis reveals that none of the investigated methods consistently improves over ChatGPT-3.5's baseline performance, with some causing significant degradation. Our findings suggest that prompting strategies do not nece
    
[^139]: 一剂病毒？使用Fakepedia定位和检测语言模型的潜在问题

    A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia

    [https://arxiv.org/abs/2312.02073](https://arxiv.org/abs/2312.02073)

    通过Fakepedia数据集研究语言模型的基础能力和进行因果中介分析，以解决上下文信息与存储知识相矛盾的问题。

    

    大型语言模型（LLMs）具有从其上下文中提供的新颖信息中获得的出色能力。然而，尚不清楚在上下文信息与参数中存储的事实知识相矛盾的情况下，支撑这种上下文基础的机制，LLMs在回忆方面也表现出色。偏好上下文信息对于检索增强生成方法至关重要，这些方法通过将上下文与最新信息丰富，希望基础可以纠正过时或有噪声的存储知识。我们提出了一种使用Fakepedia的新颖方法来研究基础能力，这是一个用于与模型内部参数知识冲突的反事实文本数据集。我们使用Fakepedia对各种LLMs进行基准测试，然后我们进行因果中介分析，基于我们的遮蔽分组因果追踪（MGCT），对回答Fakepedia查询时的LLM组件进行分析。在这个分析中，我们鉴别出d

    arXiv:2312.02073v2 Announce Type: replace  Abstract: Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge. We benchmark various LLMs with Fakepedia and then we conduct a causal mediation analysis, based on our Masked Grouped Causal Tracing (MGCT), on LLM components when answering Fakepedia queries. Within this analysis, we identify d
    
[^140]: 当前大型语言模型在各种医学专业中的应用概览

    Overview of Current Applications of Large Language Models in Various Medical Specialities

    [https://arxiv.org/abs/2311.12882](https://arxiv.org/abs/2311.12882)

    大型语言模型在医疗领域的应用概述，突出了它们在医疗质量提升中的变革性作用，重点关注了诊断和治疗领域，以及在癌症护理、皮肤科、牙科和心理健康等方面的创新应用。

    

    我们旨在概述大型语言模型（LLMs）在医疗领域最新应用，突出它们在提升医疗质量方面的变革性作用。通过处理来自不同医学领域的大量数据，LLMs已成为协助医生、医疗提供者和患者的关键工具。我们重点审视了LLMs在医疗领域的应用，主要集中在诊断和治疗相关应用上。我们强调了LLMs在癌症护理、皮肤科、牙科和心理健康领域的应用，强调了它们为医学诊断和患者护理带来的创新。分析涵盖了将LLMs整合到医疗领域中所面临的挑战和机遇，指出了尽管存在当前局限，但它们在各种医学专业中的潜力。此外，我们还概述了在医学领域处理各种数据类型的情况。

    arXiv:2311.12882v2 Announce Type: replace  Abstract: We aim to provide an overview of the latest applications of Large Language Models (LLMs) in the healthcare sector, highlighting their transformative role in enhancing medical care quality. By processing vast amounts of data from diverse medical domains, LLMs have become pivotal in assisting doctors, healthcare providers, and patients. We review the application of Large Language Models (LLMs) in healthcare, focusing on diagnostics and treatment related applications. We highlight the use of LLMs in cancer care, dermatology, dental, and mental health, emphasizing the innovation they bring to medical diagnostics and patient care. The analysis addresses the challenges and opportunities of integrating LLMs in healthcare, noting their potential in various medical specialties despite current limitations. Further, we provide an overview of handling various data types in the medical field.
    
[^141]: MedAgents: 大型语言模型作为零-shot医学推理的合作者

    MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning

    [https://arxiv.org/abs/2311.10537](https://arxiv.org/abs/2311.10537)

    提出了一个新颖的医学领域的跨学科合作(MC)框架，利用基于LLM的代理在角色扮演设置中参与协作多轮讨论，从而提高LLM的熟练程度和推理能力

    

    大型语言模型(LLMs)尽管在各种通用领域取得了显著进展，但在医学和医疗保健领域面临重大障碍。为了解决这些问题，我们提出了一个新颖的医学领域的跨学科合作(MC)框架，利用基于LLM的代理在角色扮演设置中参与协作多轮讨论，从而提高LLM的熟练程度和推理能力。这种无需训练的框架包括五个关键步骤：收集领域专家、提出个别分析、将这些分析总结成报告、在讨论中反复迭代直到达成共识，最终做出决策。我们的工作侧重于零-shot情景，在实际场景中具有适用性。在九个数据集上的实验结果显示...

    arXiv:2311.10537v2 Announce Type: replace-cross  Abstract: Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose a novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine dataset
    
[^142]: LLMs作为自恋评估者：当自我膨胀影响评估分数

    LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores

    [https://arxiv.org/abs/2311.09766](https://arxiv.org/abs/2311.09766)

    本文研究了语言模型驱动的评估指标在总结任务中是否会对相同的基础语言模型生成的文本表现出偏见，并发现在无参考情况下使用时偏见尤为显著。

    

    arXiv:2311.09766v2 公告类型：替换 摘要：生成文本内容的自动评估在自然语言处理领域中一直是一个持续挑战。鉴于现代语言模型（LMs）在各种NLP任务中的出色表现，越来越多的人倾向于利用这些模型创造创新的评估指标，用于自动生成任务的自动评估。本文探讨了一个重要问题：由于语言模型驱动的评估指标是否会固有地表现出偏向于由相同基础语言模型生成的文本的偏见？具体而言，我们评估了知名的基于LM的评估指标（例如BARTScore、T5Score和GPTScore）在总结任务中是否对其各自的基础LM表现出偏好。我们的发现揭示了潜在偏见，特别是当这些评估指标在无参考的情况下使用且不利用黄金摘要时，这种偏见尤为显著。这些结果突显了通过生成文本获得的评估结果可能会受到自我偏误的影响。

    arXiv:2311.09766v2 Announce Type: replace  Abstract: Automatic evaluation of generated textual content presents an ongoing challenge within the field of NLP. Given the impressive capabilities of modern language models (LMs) across diverse NLP tasks, there is a growing trend to employ these models in creating innovative evaluation metrics for automated assessment of generation tasks. This paper investigates a pivotal question: Do language model-driven evaluation metrics inherently exhibit bias favoring texts generated by the same underlying language model? Specifically, we assess whether prominent LM-based evaluation metrics (e.g. BARTScore, T5Score, and GPTScore) demonstrate a favorable bias toward their respective underlying LMs in the context of summarization tasks. Our findings unveil a latent bias, particularly pronounced when such evaluation metrics are used in an reference-free manner without leveraging gold summaries. These results underscore that assessments provided by generat
    
[^143]: 使用语言模型降低在线自我披露的隐私风险

    Reducing Privacy Risks in Online Self-Disclosures with Language Models

    [https://arxiv.org/abs/2311.09538](https://arxiv.org/abs/2311.09538)

    通过语言模型的检测和抽象，本研究降低了在线自我披露的隐私风险，提出了自我披露抽象的任务，并探索了多种微调策略。

    

    自我披露在社交媒体互动中既普遍又有回报，但也存在隐私风险。本文通过检测和抽象主动保护与在线自我披露相关的用户隐私。我们建立了一个包含4.8K个标注披露段的19种自我披露类别的分类法。然后为检测微调了一个语言模型，实现了65%以上的局部跨度F$_1$。我们进一步进行了一项人机交互用户研究，82%的参与者对该模型持积极态度，突出了其实际应用性。在用户反馈的推动下，我们引入了自我披露抽象的任务，即将披露重述为不太具体的术语，同时保留其实用性，例如将"Im 16F"重述为"I'm a teenage girl"。我们探讨了各种微调策略，我们的最佳模型可以生成不同的抽象，从而适度减少隐私。

    arXiv:2311.09538v2 Announce Type: replace  Abstract: Self-disclosure, while being common and rewarding in social media interaction, also poses privacy risks. In this paper, we take the initiative to protect the user-side privacy associated with online self-disclosure through detection and abstraction. We develop a taxonomy of 19 self-disclosure categories and curate a large corpus consisting of 4.8K annotated disclosure spans. We then fine-tune a language model for detection, achieving over 65% partial span F$_1$. We further conduct an HCI user study, with 82% of participants viewing the model positively, highlighting its real-world applicability. Motivated by the user feedback, we introduce the task of self-disclosure abstraction, which is paraphrasing disclosures into less specific terms while preserving their utility, e.g., "Im 16F" to "I'm a teenage girl". We explore various fine-tuning strategies, and our best model can generate diverse abstractions that moderately reduce privacy 
    
[^144]: 启发驱动的类比链接促进：增强大型语言模型用于文档级事件论证提取

    Heuristic-Driven Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction

    [https://arxiv.org/abs/2311.06555](https://arxiv.org/abs/2311.06555)

    通过启发式驱动的类比链接促进方法，该研究增强了大型语言模型用于文档级事件论证提取，使其能够从示例中学习任务特定启发式，并通过类比推理处理新情况以提高性能。

    

    在这项研究中，我们调查了文档级事件论证提取中的上下文学习（ICL），以减轻这一任务对大规模标记数据的依赖。我们引入了启发驱动的类比链接（HD-LoA）提示，以解决示例选择的挑战，并开发了一种为EAE量身定制的提示策略。具体而言，我们假设并验证了LLMs通过ICL从示范中学习任务特定启发式。基于这一假设，我们引入了一种显式的启发式驱动示范构建方法，将杂乱的示例选择过程转化为强调任务启发式的有条不紊方法。此外，受人类类比推理的启发，我们提出了类比链接提示，使LLMs能够通过将新情况类比于已知情况来处理新情况，从而提高它们在有限ICL示例以外的未见类别上的性能。

    arXiv:2311.06555v2 Announce Type: replace-cross  Abstract: In this study, we investigate in-context learning (ICL) in document-level event argument extraction (EAE) to alleviate the dependency on large-scale labeled data for this task. We introduce the Heuristic-Driven Link-of-Analogy (HD-LoA) prompting to address the challenge of example selection and to develop a prompting strategy tailored for EAE. Specifically, we hypothesize and validate that LLMs learn task-specific heuristics from demonstrations via ICL. Building upon this hypothesis, we introduce an explicit heuristic-driven demonstration construction approach, which transforms the haphazard example selection process into a methodical method that emphasizes task heuristics. Additionally, inspired by the analogical reasoning of human, we propose the link-of-analogy prompting, which enables LLMs to process new situations by drawing analogies to known situations, enhancing their performance on unseen classes beyond limited ICL exa
    
[^145]: Prompt Engineering a Prompt Engineer

    Prompt Engineering a Prompt Engineer

    [https://arxiv.org/abs/2311.05661](https://arxiv.org/abs/2311.05661)

    提示工程任务对于优化大型语言模型在定制任务上的表现至关重要，PE2方法通过详细描述、上下文规范和逐步推理模板的注入，在各种语言任务中展现出出色的适用性和效果。

    

    提示工程是优化大型语言模型在定制任务上表现的一项具有挑战性但至关重要的任务。为了检查模型的错误，假设当前提示中缺少或误导了什么，并清晰地传达任务，需要复杂的推理。尽管最近的研究表明，大型语言模型可以被元提示来执行自动提示工程，但我们认为由于元提示中缺乏复杂推理的充分指导，它们的潜力受到限制。我们通过将详细描述、上下文规范和逐步推理模板注入到元提示中来填补这一空白。所得到的方法称为PE2，展示了在不同语言任务中出色的适用性。它找到的提示在MultiArith上比“按步骤思考”高出6.3%，在GSM8K上高出3.1%，并在对立任务上优于竞争基线

    arXiv:2311.05661v2 Announce Type: replace-cross  Abstract: Prompt engineering is a challenging yet crucial task for optimizing the performance of large language models on customized tasks. It requires complex reasoning to examine the model's errors, hypothesize what is missing or misleading in the current prompt, and communicate the task with clarity. While recent works indicate that large language models can be meta-prompted to perform automatic prompt engineering, we argue that their potential is limited due to insufficient guidance for complex reasoning in the meta-prompt. We fill this gap by infusing into the meta-prompt three key components: detailed descriptions, context specification, and a step-by-step reasoning template. The resulting method, named PE2, showcases remarkable versatility across diverse language tasks. It finds prompts that outperform "let's think step by step" by 6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on counterfactual tasks 
    
[^146]: 关于大型语言模型在摘要中上下文利用的研究

    On Context Utilization in Summarization with Large Language Models

    [https://arxiv.org/abs/2310.10570](https://arxiv.org/abs/2310.10570)

    本文研究了大型语言模型在摘要中上下文利用的问题，发现了摘要任务中关于输入位置的性能模式以及源文件到摘要的内容映射挑战。

    

    大型语言模型（LLMs）在抽象摘要任务中表现出色，提供流畅且相关的摘要。最近的进展扩展了它们处理长输入上下文的能力，超过了100k个标记。然而，在问答中，语言模型对其输入上下文的利用不均匀。它们倾向于偏爱初始和最终段落，导致了关于答案在输入中位置的U形性能模式。这种偏见引发了担忧，特别是在摘要中，关键内容可能分散在源文件中。此外，在摘要中，从源文件到摘要的事实映射并不是微不足道的，因为显著内容通常会被重新表述。在本文中，我们对摘要中上下文利用和位置偏见进行了第一次全面研究。我们的分析涵盖了5个LLMs，10个数据集和5个评估指标。我们引入了一个新的评估

    arXiv:2310.10570v3 Announce Type: replace  Abstract: Large language models (LLMs) excel in abstractive summarization tasks, delivering fluent and pertinent summaries. Recent advancements have extended their capabilities to handle long-input contexts, exceeding 100k tokens. However, in question answering, language models exhibit uneven utilization of their input context. They tend to favor the initial and final segments, resulting in a U-shaped performance pattern concerning where the answer is located within the input. This bias raises concerns, particularly in summarization where crucial content may be dispersed throughout the source document(s). Besides, in summarization, mapping facts from the source to the summary is not trivial as salient content is usually re-phrased. In this paper, we conduct the first comprehensive study on context utilization and position bias in summarization. Our analysis encompasses 5 LLMs, 10 datasets, and 5 evaluation metrics. We introduce a new evaluatio
    
[^147]: 从文本到自我：用户对人工智能在人际交流和自我方面潜力的认知

    From Text to Self: Users' Perceptions of Potential of AI on Interpersonal Communication and Self

    [https://arxiv.org/abs/2310.03976](https://arxiv.org/abs/2310.03976)

    用户对大型语言模型驱动的工具在人际交流方面的能力持积极看法，认为可以增加沟通自信、帮助表达想法以及克服语言和文化障碍，但也揭示出工具存在的一些局限性和用户关于技术不真实性和过度依赖的担忧。

    

    在快速发展的AI中介交流（AIMC）领域中，由大型语言模型（LLMs）驱动的工具正成为人际交流的重要组成部分。采用混合方法，我们进行了为期一周的日记和访谈研究，探讨了用户对这些工具在短期内支持人际交流的能力和可能导致的长期效果的看法。我们的研究发现，参与者对AIMC支持持有积极看法，认为其能够增加沟通自信，帮助找到准确的语言表达想法，以及克服语言和文化障碍。然而，研究还揭示了AIMC工具目前存在的局限，包括啰嗦的回复、不自然的回应以及过度情绪化。这些缺陷进一步受到用户对不真实性和对技术过度依赖的担忧所加剧。此外，我们确定了

    arXiv:2310.03976v2 Announce Type: cross  Abstract: In the rapidly evolving landscape of AI-mediated communication (AIMC), tools powered by Large Language Models (LLMs) are becoming integral to interpersonal communication. Employing a mixed-methods approach, we conducted a one-week diary and interview study to explore users' perceptions of these tools' ability to: 1) support interpersonal communication in the short-term, and 2) lead to potential long-term effects. Our findings indicate that participants view AIMC support favorably, citing benefits such as increased communication confidence, and finding precise language to express their thoughts, navigating linguistic and cultural barriers. However, the study also uncovers current limitations of AIMC tools, including verbosity, unnatural responses, and excessive emotional intensity. These shortcomings are further exacerbated by user concerns about inauthenticity and potential overreliance on the technology. Furthermore, we identified fou
    
[^148]: 具有单元语言模型的无文本低资源语音到语音翻译

    Textless Low-Resource Speech-to-Speech Translation With Unit Language Models

    [https://arxiv.org/abs/2305.15405](https://arxiv.org/abs/2305.15405)

    提出了一种新的框架，用于训练只需要几十小时平行语音数据的无文本低资源语音到语音翻译系统，并通过单元到单元的序列到序列翻译任务和无监督反向翻译目标来提高模型性能

    

    现有的语音到语音翻译模型大致分为两类：使用数百小时平行语音数据训练的无文本模型，或者将文本作为中间步骤的无监督模型。这两种方法限制了为广泛语言构建语音到语音翻译模型的可能性，因为它们排除了主要口语的语言以及缺乏大规模平行语音数据的语言对。我们提出了一个新的框架，用于训练只需要几十小时平行语音数据的无文本低资源语音到语音翻译（S2ST）系统。我们将S2ST重新构建为一个单元到单元的序列到序列翻译任务，并首先在大规模单语言语音数据上进行预训练。然后，我们使用少量平行语音数据（$20-60$小时）对其进行微调。最后，我们通过无监督反向翻译目标改善模型性能。我们为英语到德语，德语

    arXiv:2305.15405v2 Announce Type: replace  Abstract: Existing speech-to-speech translation models fall into two camps: textless models trained with hundreds of hours of parallel speech data or unsupervised models that leverage text as an intermediate step. Both approaches limit building speech-to-speech translation models for a wide range of languages, as they exclude languages that are primarily spoken and language pairs that lack large-scale parallel speech data. We present a new framework for training textless low-resource speech-to-speech translation (S2ST) systems that only need dozens of hours of parallel speech data. We reformulate S2ST as a unit-to-unit seq2seq translation task, and start by pretraining a model on large-scale monolingual speech data. Then, we finetune it with a small amount of parallel speech data ($20-60$ hours). Lastly, we improve model performance through an unsupervised backtranslation objective. We train and evaluate our models for English-to-German, Germa
    
[^149]: 通过高斯嵌入表示句子

    Sentence Representations via Gaussian Embedding

    [https://arxiv.org/abs/2305.12990](https://arxiv.org/abs/2305.12990)

    本文提出了一种基于高斯分布的对比学习框架 GaussCSE，用于处理句子之间的不对称关系，实现了与以往方法相同的性能，在自然语言推理中能够估计蕴涵关系的方向。

    

    最近在句子嵌入方面取得的进展，将句子的含义表示为向量空间中的一个点，已在诸如语义文本相似性（STS）任务等任务上取得了高性能。然而，作为向量空间中的一个点的句子表示只能表达句子具有的多样信息的一部分，比如句子之间的不对称关系。本文提出了GaussCSE，一种基于高斯分布的对比学习框架，用于句子嵌入，可以处理句子之间的不对称关系，同时提出了一种相似度度量用于识别包含关系。我们的实验表明，GaussCSE在自然语言推理任务中实现了与以前方法相同的性能，并且能够估计蕴涵关系的方向，这在点表示中是困难的。

    arXiv:2305.12990v2 Announce Type: replace  Abstract: Recent progress in sentence embedding, which represents the meaning of a sentence as a point in a vector space, has achieved high performance on tasks such as a semantic textual similarity (STS) task. However, sentence representations as a point in a vector space can express only a part of the diverse information that sentences have, such as asymmetrical relationships between sentences. This paper proposes GaussCSE, a Gaussian distribution-based contrastive learning framework for sentence embedding that can handle asymmetric relationships between sentences, along with a similarity measure for identifying inclusion relations. Our experiments show that GaussCSE achieves the same performance as previous methods in natural language inference tasks, and is able to estimate the direction of entailment relations, which is difficult with point representations.
    
[^150]: 字符级转导的精确硬单调注意力

    Exact Hard Monotonic Attention for Character-Level Transduction

    [https://arxiv.org/abs/1905.06319](https://arxiv.org/abs/1905.06319)

    开发了一种精确硬单调注意力的序列到序列模型，在字符级转导任务中取得了最先进的性能。

    

    许多常见的字符级字符串转导任务，例如音素到字音转换和形态屈折，几乎完全由单调转导组成。但是，使用非单调软注意力的神经序列到序列模型通常优于流行的单调模型。在这项工作中，我们提出以下问题：对于这些任务，单调性真的是一个有用的约束吗？我们开发了一种硬注意力序列到序列模型，强制执行严格的单调性，并在学习转导的同时学习潜在的对齐。借助动态规划，我们能够计算出所有单调对齐的精确边缘化。我们的模型在形态屈折任务上实现了最新的性能。此外，我们发现在另外两个字符级转导任务上也表现出良好的性能。代码可在 https://github.com/shijie-wu/neural-transducer 找到。

    arXiv:1905.06319v3 Announce Type: replace  Abstract: Many common character-level, string-to string transduction tasks, e.g., grapheme-tophoneme conversion and morphological inflection, consist almost exclusively of monotonic transductions. However, neural sequence-to sequence models that use non-monotonic soft attention often outperform popular monotonic models. In this work, we ask the following question: Is monotonicity really a helpful inductive bias for these tasks? We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns a latent alignment jointly while learning to transduce. With the help of dynamic programming, we are able to compute the exact marginalization over all monotonic alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/shijie-wu/neural-transducer.
    
[^151]: 硬性非单调注意力用于字符级转录

    Hard Non-Monotonic Attention for Character-Level Transduction

    [https://arxiv.org/abs/1808.10024](https://arxiv.org/abs/1808.10024)

    本文提出了一种用于字符级转录的硬性非单调注意力机制，引入了精确的、多项式时间算法来处理两个字符串之间的非单调对齐，表明硬性注意力模型是神经重新参数化的一种形式。

    

    字符级字符串到字符串的转录是各种自然语言处理任务中的重要组成部分。最近的方法使用了带有注意力机制的序列到序列模型，来学习模型在生成输出字符串时应该关注输入字符串的哪些部分。传统的软性注意力和硬性单调注意力已经被使用，但硬性非单调注意力仅用于其他序列建模任务（如图像字幕生成），需要使用随机逼近来计算梯度。在这项工作中，我们引入了一个精确的、多项式时间算法来对两个字符串之间的指数数量的非单调对齐进行边缘化，表明硬性注意力模型可以被视为神经重新参数化。

    arXiv:1808.10024v3 Announce Type: replace  Abstract: Character-level string-to-string transduction is an important component of various NLP tasks. The goal is to map an input string to an output string, where the strings may be of different lengths and have characters taken from different alphabets. Recent approaches have used sequence-to-sequence models with an attention mechanism to learn which parts of the input string the model should focus on during the generation of the output string. Both soft attention and hard monotonic attention have been used, but hard non-monotonic attention has only been used in other sequence modeling tasks such as image captioning (Xu et al., 2015), and has required a stochastic approximation to compute the gradient. In this work, we introduce an exact, polynomial-time algorithm for marginalizing over the exponential number of non-monotonic alignments between two strings, showing that hard attention models can be viewed as neural reparameterizations of t
    
[^152]: SelectLLM：LLMs能否选择重要的指令进行注释？

    SelectLLM: Can LLMs Select Important Instructions to Annotate?. (arXiv:2401.16553v1 [cs.CL])

    [http://arxiv.org/abs/2401.16553](http://arxiv.org/abs/2401.16553)

    这项工作提出了一种名为SelectLLM的新方法，利用LLMs选择高质量指令。通过提示LLMs估计每个无标签指令的有用性和影响力，并使用聚类算法将指令分为多个聚类。

    

    使用大量且多样化的指令数据集训练大型语言模型(LLMs)可以使模型理解和遵循人类指令。最近的研究表明，使用一小组高质量的指令可以超过使用大量更嘈杂的指令。由于指令是无标签的，且响应是自然文本，传统的主动学习方案无法直接应用于选择无标签指令。在这项工作中，我们提出了一种新的指令选择方法，称为SelectLLM，它利用LLMs选择高质量指令。我们的高级思想是利用LLMs通过提示来估计每个指令在没有相应标签（即响应）的情况下的有用性和影响力。SelectLLM包括两个步骤：使用聚类算法（例如CoreSet）将无标签指令划分为多个聚类，然后提示LLMs在其中选择高质量指令。

    Training large language models (LLMs) with a large and diverse instruction dataset aligns the models to comprehend and follow human instructions. Recent works have shown that using a small set of high-quality instructions can outperform using large yet more noisy ones. Because instructions are unlabeled and their responses are natural text, traditional active learning schemes with the model's confidence cannot be directly applied to the selection of unlabeled instructions. In this work, we propose a novel method for instruction selection, called SelectLLM, that leverages LLMs for the selection of high-quality instructions. Our high-level idea is to use LLMs to estimate the usefulness and impactfulness of each instruction without the corresponding labels (i.e., responses), via prompting. SelectLLM involves two steps: dividing the unlabelled instructions using a clustering algorithm (e.g., CoreSet) to multiple clusters, and then prompting LLMs to choose high-quality instructions within e
    
[^153]: MM-LLMs: 多模式大语言模型的最新进展

    MM-LLMs: Recent Advances in MultiModal Large Language Models. (arXiv:2401.13601v1 [cs.CL])

    [http://arxiv.org/abs/2401.13601](http://arxiv.org/abs/2401.13601)

    近年来，多模式大语言模型（MM-LLMs）通过成本效益高的训练策略取得了显著进展，扩展了现有的语言模型的多模输入和输出支持。本论文提供了一份综合调查报告，介绍了MM-LLMs的设计和训练方案，整理了现有的MM-LLMs及其性能，总结了关键训练方法，并探讨了未来的研究方向。

    

    在过去的一年中，多模式大语言模型（MM-LLMs）取得了显著的进展，通过成本效益高的训练策略，增强了现有的LLMs对多模输入或输出的支持。这些结果模型不仅保留了LLMs固有的推理和决策能力，还赋予了各种多模任务。本文提供了一份综合性的调查报告，旨在促进对MM-LLMs的进一步研究。具体而言，我们首先概述了模型架构和训练流程的一般设计方案。随后，我们简要介绍了26种现有的MM-LLMs，每种都以其具体的公式为特征。此外，我们还回顾了MM-LLMs在主流基准测试上的性能，并总结了提高MM-LLMs效力的关键训练方法。最后，我们探讨了MM-LLMs的有前途的方向，同时还为该领域的最新发展提供了实时追踪网站。我们希望这份调查报告能够促进对MM-LLMs的进一步研究。

    In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Specifically, we first outline general design formulations for model architecture and training pipeline. Subsequently, we provide brief introductions of $26$ existing MM-LLMs, each characterized by its specific formulations. Additionally, we review the performance of MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this surv
    
[^154]: 大规模异构图上基于大型语言模型的链接预测的可扩展性研究

    Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models. (arXiv:2401.13227v1 [cs.CL])

    [http://arxiv.org/abs/2401.13227](http://arxiv.org/abs/2401.13227)

    本研究探索了在大规模异构图上应用大型语言模型进行图学习的方法，提出了LPNL框架用于可扩展链接预测。通过创新的提示语和采样流程，以及分而治之的策略，成功解决了大规模图中的信息过载问题，并在实验中表现出了优越的性能。

    

    探索将大规模语言模型应用于图学习是一项新颖的努力。然而，大图中蕴含的大量信息给这一过程带来了重大挑战。本文侧重于链接预测任务，并介绍了LPNL（Link Prediction via Natural Language），这是一个基于大型语言模型的框架，用于大规模异构图上的可扩展链接预测。我们设计了能以自然语言表达图细节的创新提示语。我们提出了一个两阶段的采样流程，从大规模异构图中提取关键信息，并采用分而治之的策略来控制输入令牌数量在预定限制内，解决了信息过载的挑战。我们还通过自监督学习设计了一个用于链接预测的T5模型进行微调。在大型公共异构图上进行的广泛实验表明，LPNL的性能超过了各种先进的基准模型。

    Exploring the application of large-scale language models to graph learning is a novel endeavor. However, the vast amount of information inherent in large graphs poses significant challenges to this process. This paper focuses on the link prediction task and introduces LPNL (Link Prediction via Natural Language), a framework based on a large language model designed for scalable link prediction on large-scale heterogeneous graphs.We design novel prompts for link prediction that articulate graph details in natural language. We propose a two-stage sampling pipeline to extract crucial information from large-scale heterogeneous graphs, and a divide-and-conquer strategy to control the input token count within predefined limits, addressing the challenge of overwhelming information. We fine-tune a T5 model based on our self-supervised learning designed for for link prediction. Extensive experiments on a large public heterogeneous graphs demonstrate that LPNL outperforms various advanced baselin
    
[^155]: 在科学和工程领域中的种子引导下对细粒度实体进行类型划分

    Seed-Guided Fine-Grained Entity Typing in Science and Engineering Domains. (arXiv:2401.13129v1 [cs.CL])

    [http://arxiv.org/abs/2401.13129](http://arxiv.org/abs/2401.13129)

    本文研究了在科学和工程领域中的种子引导下对细粒度实体进行类型划分的任务，提出了用无标注语料库找到更多实体来丰富监督信息的方法，并使用多头注意力的条件随机场模型进行实体划分。

    

    准确地对文本片段中的实体提供类型划分是各种自然语言处理应用的基本任务。许多先前的方法依赖于大量的人工注释数据来执行实体类型划分。然而，在高度专业化的科学和工程领域（例如软件工程和安全领域）中收集此类数据可能耗时且昂贵，更不用提这些模型如果需要应用于保密数据集时，训练和推断数据之间的领域差异。在本文中，我们研究了在科学和工程领域中的种子引导下对细粒度实体类型进行划分的任务，该任务以实体的名称和一些种子实体作为唯一的监督，并旨在将新的实体提及分类为已知和未知类型（即没有种子实体的类型）。为了解决这个问题，我们提出了SEType，首先利用上下文化的无标注语料库找到每个已知类型的更多实体来丰富弱监督信息，然后使用一种带有两层多头注意力的条件随机场模型对实体进行划分。

    Accurately typing entity mentions from text segments is a fundamental task for various natural language processing applications. Many previous approaches rely on massive human-annotated data to perform entity typing. Nevertheless, collecting such data in highly specialized science and engineering domains (e.g., software engineering and security) can be time-consuming and costly, without mentioning the domain gaps between training and inference data if the model needs to be applied to confidential datasets. In this paper, we study the task of seed-guided fine-grained entity typing in science and engineering domains, which takes the name and a few seed entities for each entity type as the only supervision and aims to classify new entity mentions into both seen and unseen types (i.e., those without seed entities). To solve this problem, we propose SEType which first enriches the weak supervision by finding more entities for each seen type from an unlabeled corpus using the contextualized 
    
[^156]: SLANG: 大型语言模型对新概念的理解

    SLANG: New Concept Comprehension of Large Language Models. (arXiv:2401.12585v1 [cs.CL])

    [http://arxiv.org/abs/2401.12585](http://arxiv.org/abs/2401.12585)

    本研究提出了一个新的基准SLANG，旨在增强大型语言模型LLMs对互联网上新概念的理解能力，同时提出了一种基于因果推断的基准方法FOCUS，能帮助LLMs更好地理解新的短语和用法模式。

    

    语言的动态性，尤其在互联网上的俚语和表情包等方面的体现，给大型语言模型（LLMs）的适应性带来了严峻挑战。传统上，这些模型通常仅绑定在静态数据集上，很难跟上在线社区中快速语言进化的步伐。本研究解决了弥合这一差距的迫切需求，旨在增强LLMs对互联网上新概念的理解能力，同时避免高成本和不切实际的持续重训练。为应对这个问题，我们提出了一个新的评估LLMs在理解新兴语言趋势方面能力的基准 - SLANG，并提出了一种基于因果推断的基准方法 FOCUS，它能增强LLMs对新的短语和用法模式的理解。该方法包括对语言转变的真实世界实例进行详细研究，作为背景依据，以形成更精确和具有上下文相关性的新连接。

    The dynamic nature of language, particularly evident in the realm of slang and memes on the Internet, poses serious challenges to the adaptability of large language models (LLMs). Traditionally anchored to static datasets, these models often struggle to keep up with the rapid linguistic evolution characteristic of online communities. This research addresses the critical need to bridge this gap, aiming to enhance LLMs' comprehension of evolving new concepts on the internet, without the high cost and impracticality of continual retraining. To address this issue, we propose a new benchmark $\textbf{SLANG}$ to assess LLMs' proficiency in comprehending emerging linguistic trends and a baseline approach $\textbf{FOCUS}$, which uses causal inference to enhance LLMs to understand new phrases and usage patterns. This approach involves scrutinizing real-world instances of linguistic shifts, serving as contextual beacons, to form more precise and contextually relevant connections between newly em
    
[^157]: 改善多模态情感分析的智慧M：融合背景知识的上下文世界知识

    WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge. (arXiv:2401.06659v1 [cs.CL])

    [http://arxiv.org/abs/2401.06659](http://arxiv.org/abs/2401.06659)

    本文提出了一个名为智慧M的插件框架，利用从大型视觉语言模型中产生的上下文世界知识来改进多模态情感分析，实验证明该方法在不同任务上有着显著的改进。

    

    情感分析通过利用各种数据模态（例如文本、图像）迅速发展。然而，大多数先前的研究都依赖于表面信息，忽视了上下文世界知识（例如从给定图像和文本对之外获取的背景信息），从而限制了实现更好的多模态情感分析的能力。本文提出了一个名为智慧M的插件框架，旨在利用从大型视觉语言模型（LVLMs）中产生的上下文世界知识来改进多模态情感分析。智慧M利用LVLM来全面分析图像和相应的句子，同时生成相关的上下文。为了减少上下文中的噪声，我们还引入了一种无需训练的上下文融合机制。在多样的多模态情感分析任务的实验结果一致表明，我们的方法有着显著的改进。

    Sentiment analysis is rapidly advancing by utilizing various data modalities (e.g., text, image). However, most previous works relied on superficial information, neglecting the incorporation of contextual world knowledge (e.g., background information derived from but beyond the given image and text pairs) and thereby restricting their ability to achieve better multimodal sentiment analysis. In this paper, we proposed a plug-in framework named WisdoM, designed to leverage contextual world knowledge induced from the large vision-language models (LVLMs) for enhanced multimodal sentiment analysis. WisdoM utilizes a LVLM to comprehensively analyze both images and corresponding sentences, simultaneously generating pertinent context. To reduce the noise in the context, we also introduce a training-free Contextual Fusion mechanism. Experimental results across diverse granularities of multimodal sentiment analysis tasks consistently demonstrate that our approach has substantial improvements (br
    
[^158]: 在不断演化的社会规范中的Agent对齐

    Agent Alignment in Evolving Social Norms. (arXiv:2401.04620v1 [cs.CL])

    [http://arxiv.org/abs/2401.04620](http://arxiv.org/abs/2401.04620)

    本论文提出了一个名为EvolutionaryAgent的进化框架，将Agent对齐转化为适者生存的演化和选择过程，在不断演化的社会规范中，与当前社会规范更好适应的Agent将具有更高的生存和传播概率。

    

    基于大型语言模型（LLM）的Agent越来越多地渗透到人类生产和生活的各个领域，凸显了将其与人类价值观对齐的重要性。目前AI系统的对齐主要集中在通过人为干预对LLM进行被动对齐。然而，Agent具有接受环境反馈和自我进化等特性，使得LLM对齐方法变得不足够。为此，我们提出了一个名为EvolutionaryAgent的Agent进化和对齐的进化框架，将Agent对齐转化为适者生存的演化和选择过程。在社会规范不断演化的环境中，与当前社会规范更好适应的Agent将具有更高的生存和传播概率，而对齐不足的Agent则逐渐减少。通过多个角度对与社会规范相对齐的Agent进行的实验结果进行评估。

    Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstr
    
[^159]: 机器人任务规划的视觉语言解释器

    Vision-Language Interpreter for Robot Task Planning. (arXiv:2311.00967v1 [cs.RO])

    [http://arxiv.org/abs/2311.00967](http://arxiv.org/abs/2311.00967)

    本文提出了一种名为Vision-Language Interpreter（ViLaIn）的新框架，该框架通过使用先进的语言模型和视觉语言模型生成机器人任务描述，并通过符号规划器的错误消息反馈进行改进。实验结果表明ViLaIn和符号规划器能够准确生成有效的机器人计划。

    

    大型语言模型（LLMs）正在加速语言引导的机器人规划器的发展。同时，符号规划器具有可解释性的优势。本文提出了一个新的任务，将这两种趋势相结合，即多模态规划问题规范。目标是生成一个问题描述（PD），这是规划器用来查找计划的机器可读文件。通过从语言指令和场景观测中生成PD，我们可以驱动符号规划器在语言引导框架下工作。我们提出了一种名为Vision-Language Interpreter（ViLaIn）的新框架，该框架使用先进的LLM和视觉语言模型生成PD。ViLaIn可以通过符号规划器的错误消息反馈来改进生成的PD。我们的目标是回答这个问题：ViLaIn和符号规划器能够准确地生成有效的机器人计划吗？为了评估ViLaIn，我们引入了一个名为问题描述生成（ProDG）数据集的新颖数据集。该框架将在评估中进行测试。

    Large language models (LLMs) are accelerating the development of language-guided robot planners. Meanwhile, symbolic planners offer the advantage of interpretability. This paper proposes a new task that bridges these two trends, namely, multimodal planning problem specification. The aim is to generate a problem description (PD), a machine-readable file used by the planners to find a plan. By generating PDs from language instruction and scene observation, we can drive symbolic planners in a language-guided framework. We propose a Vision-Language Interpreter (ViLaIn), a new framework that generates PDs using state-of-the-art LLM and vision-language models. ViLaIn can refine generated PDs via error message feedback from the symbolic planner. Our aim is to answer the question: How accurately can ViLaIn and the symbolic planner generate valid robot plans? To evaluate ViLaIn, we introduce a novel dataset called the problem description generation (ProDG) dataset. The framework is evaluated wi
    
[^160]: 减少生成式语言模型学习困难的信息熵损失

    InfoEntropy Loss to Mitigate Bias of Learning Difficulties for Generative Language Models. (arXiv:2310.19531v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.19531](http://arxiv.org/abs/2310.19531)

    提出了一种信息熵损失函数，用于减少生成式语言模型对常见和易学标记的偏好，使其更关注不常见和难学的标记。

    

    生成式语言模型通常通过预测上一个标记（子词/词/短语）给出的下一个标记来进行预训练。最近的研究展示了大规模生成式语言模型在下游任务上的出色性能。然而，现有的生成式语言模型在训练过程中通常忽视文本语料库中的固有挑战，即频繁标记和不经常出现的标记之间的不平衡。这可能导致语言模型被常见且易学的标记所主导，从而忽视不经常出现且难以学习的标记。为了缓解这个问题，我们提出了一种信息熵损失（InfoEntropy Loss）函数。在训练过程中，它可以根据相应的预测概率分布的信息熵动态评估待学习标记的学习难度。然后，它适应地调整训练损失，试图使模型更加关注难以学习的标记。

    Generative language models are usually pretrained on large text corpus via predicting the next token (i.e., sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language models on downstream tasks. However, existing generative language models generally neglect an inherent challenge in text corpus during training, i.e., the imbalance between frequent tokens and infrequent ones. It can lead a language model to be dominated by common and easy-to-learn tokens, thereby overlooking the infrequent and difficult-to-learn ones. To alleviate that, we propose an Information Entropy Loss (InfoEntropy Loss) function. During training, it can dynamically assess the learning difficulty of a to-be-learned token, according to the information entropy of the corresponding predicted probability distribution over the vocabulary. Then it scales the training loss adaptively, trying to lead the model to focus more on the difficult-to-learn
    
[^161]: Variator: 使用即插即用压缩模块加速预训练模型

    Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules. (arXiv:2310.15724v1 [cs.CL])

    [http://arxiv.org/abs/2310.15724](http://arxiv.org/abs/2310.15724)

    这个论文提出了一种称为Variator的加速方法，通过即插即用的压缩插件增强了预训练模型的计算效率，并且可以根据工作负载动态选择不同加速比的插件。插件采用了压缩隐藏向量的方法来减小序列长度，并且由于参数少，可以节省存储和内存开销。

    

    预训练语言模型（PLMs）在自然语言处理任务上取得了显著的成果，但代价是巨大的参数大小和随之而来的计算成本。本文提出了Variator，一种参数高效的加速方法，通过即插即用的压缩插件增强计算效率。压缩插件通过将多个隐藏向量压缩到一个向量来缩减序列长度，并与原始PLMs一起进行训练。与传统的模型加速方法不同，Variator具有两个独特的优点：（1）在现实世界应用中，我们的压缩插件的即插即用性质使得可以根据当前工作负载动态选择具有不同加速比的压缩插件。（2）压缩插件由几个紧凑的神经网络层组成，参数很少，大大节省了存储和内存开销，特别是在具有较大存储需求和内存需求的场景下。

    Pre-trained language models (PLMs) have achieved remarkable results on NLP tasks but at the expense of huge parameter sizes and the consequent computational costs. In this paper, we propose Variator, a parameter-efficient acceleration method that enhances computational efficiency through plug-and-play compression plugins. Compression plugins are designed to reduce the sequence length via compressing multiple hidden vectors into one and trained with original PLMs frozen. Different from traditional model acceleration methods, which compress PLMs to smaller sizes, Variator offers two distinct advantages: (1) In real-world applications, the plug-and-play nature of our compression plugins enables dynamic selection of different compression plugins with varying acceleration ratios based on the current workload. (2) The compression plugin comprises a few compact neural network layers with minimal parameters, significantly saving storage and memory overhead, particularly in scenarios with a gro
    
[^162]: Easier Multimodal Generation: Diffusion Models Meet LLMs

    Making Multimodal Generation Easier: When Diffusion Models Meet LLMs. (arXiv:2310.08949v1 [cs.AI])

    [http://arxiv.org/abs/2310.08949](http://arxiv.org/abs/2310.08949)

    EasyGen是一个有效的模型，它通过结合扩散模型和大型语言模型（LLMs）的能力，实现了更容易的多模态生成。相比现有的模型，EasyGen使用了一个名为BiDiffuser的双向条件扩散模型， 提供了更高效的模态交互，并且不仅能够生成文本回复，还能够促进文本到图像的生成。

    

    我们提出了EasyGen，一个有效的模型，通过利用扩散模型和大型语言模型（LLMs）的能力，增强了多模态理解和生成。不同于现有的主要依赖于编码器如CLIP或ImageBind，并且需要大量训练数据来桥接模态之间差距的多模态模型，EasyGen基于一个名为BiDiffuser的双向条件扩散模型构建，促进了更高效的模态交互。EasyGen通过简单的投影层将BiDiffuser和LLM进行集成，处理图像到文本的生成。与大多数现有的限于生成文本回复的多模态模型不同，EasyGen还可以通过利用LLM创建文本描述，并由BiDiffuser解释生成适当的视觉回复来促进文本到图像的生成。广泛的定量和定性实验证明了EasyGen的有效性，其训练可以...

    We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge the gap between modalities, EasyGen is built upon a bidirectional conditional diffusion model named BiDiffuser, which promotes more efficient interactions between modalities. EasyGen handles image-to-text generation by integrating BiDiffuser and an LLM via a simple projection layer. Unlike most existing multimodal models that are limited to generating text responses, EasyGen can also facilitate text-to-image generation by leveraging the LLM to create textual descriptions, which can be interpreted by BiDiffuser to generate appropriate visual responses. Extensive quantitative and qualitative experiments demonstrate the effectiveness of EasyGen, whose training can b
    
[^163]: Meta-CoT:大规模语言模型在混合任务场景中的通用思维链提示

    Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models. (arXiv:2310.06692v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.06692](http://arxiv.org/abs/2310.06692)

    Meta-CoT是一种在混合任务场景中能够通用思维链提示的方法，在十个公共基准推理任务中表现出卓越的性能和优越的泛化能力。

    

    大规模语言模型（LLM）通过利用链式思维提示展示出了卓越的推理能力，这种提示生成中间推理链以作为得出答案的基本理由。然而，目前的CoT方法要么仅仅使用类似“让我们逐步思考”的通用提示，要么过于依赖手工设计的任务特定演示来达到理想的性能，从而导致性能和泛化之间的不可避免的鸿沟。为了弥合这一鸿沟，我们提出了Meta-CoT，一种在未知输入问题类型的混合任务场景中具有通用性的CoT提示方法。Meta-CoT首先根据输入问题对场景进行分类，然后以自动模式从相应的数据池中构建多样的演示。Meta-CoT在十个公共基准推理任务上表现出卓越的性能和优越的泛化能力。值得注意的是，Meta-CoT实现了最新技术水平。

    Large language models (LLMs) have unveiled remarkable reasoning capabilities by exploiting chain-of-thought (CoT) prompting, which generates intermediate reasoning chains to serve as the rationale for deriving the answer. However, current CoT methods either simply employ general prompts such as Let's think step by step, or heavily rely on handcrafted task-specific demonstrations to attain preferable performances, thereby engendering an inescapable gap between performance and generalization. To bridge this gap, we propose Meta-CoT, a generalizable CoT prompting method in mixed-task scenarios where the type of input questions is unknown. Meta-CoT firstly categorizes the scenario based on the input question and subsequently constructs diverse demonstrations from the corresponding data pool in an automatic pattern. Meta-CoT simultaneously enjoys remarkable performances on ten public benchmark reasoning tasks and superior generalization capabilities. Notably, Meta-CoT achieves the state-of-
    
[^164]: 大型语言模型在生物医学文本处理任务中的综合评估

    A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks. (arXiv:2310.04270v1 [cs.CL])

    [http://arxiv.org/abs/2310.04270](http://arxiv.org/abs/2310.04270)

    本文对大型语言模型（LLM）在生物医学任务中的性能进行了综合评估，发现零样本LLMs在小样本生物医学数据集上的表现甚至超过了先进的精调生物医学模型，预训练使LLMs在生物医学领域具备了很强的专业能力。

    

    最近，大型语言模型（LLM）展示了解决各种任务的出色能力。然而，尽管它们在各种任务中取得了成功，但目前还没有研究它们在生物医学领域的能力。因此，本文旨在评估LLMs在基准生物医学任务上的性能。为此，我们对6个不同生物医学任务的26个数据集中的4个热门LLMs进行了综合评估。据我们所知，这是第一篇在生物医学领域对各种LLMs进行广泛评估和比较的研究。有趣的是，根据我们的评估，我们发现在训练集较小的生物医学数据集中，零样本LLMs甚至超过了当前最先进的精调生物医学模型。这表明在大型文本语料库上进行预训练使LLMs在生物医学领域具备了很强的专业能力。我们还发现，在所有任务中没有一个LLM能够胜过其他LLMs。

    Recently, Large Language Models (LLM) have demonstrated impressive capability to solve a wide range of tasks. However, despite their success across various tasks, no prior work has investigated their capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of LLMs on benchmark biomedical tasks. For this purpose, we conduct a comprehensive evaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets. To the best of our knowledge, this is the first work that conducts an extensive evaluation and comparison of various LLMs in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot LLMs even outperform the current state-of-the-art fine-tuned biomedical models. This suggests that pretraining on large text corpora makes LLMs quite specialized even in the biomedical domain. We also find that not a single LLM can outperform other LLMs in all tasks, with 
    
[^165]: 直觉还是依赖？研究LLMs对冲突提示的鲁棒性

    Intuitive or Dependent? Investigating LLMs' Robustness to Conflicting Prompts. (arXiv:2309.17415v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.17415](http://arxiv.org/abs/2309.17415)

    本文研究了LLMs对冲突提示的鲁棒性，发现这些模型对于误导性提示特别容易受到影响，尤其是在指导常识知识方面。

    

    本文探讨了LLMs对其内部记忆或给定提示的偏好的鲁棒性，由于噪声或任务设置，在真实应用中可能存在对立信息。为此，我们建立了一个定量的基准框架，并进行角色扮演干预来控制LLMs的偏好。具体而言，我们定义了两种鲁棒性，即事实鲁棒性和决策风格，事实鲁棒性是指从提示或记忆中识别正确事实的能力，而决策风格是基于认知理论对LLMs在进行一致选择过程中行为的分类 - 直觉型、依赖型或理性型，这里假设没有明确的“正确”答案。我们对七个开源和闭源LLMs进行了大量实验，发现这些模型对于误导性提示特别容易受到影响，尤其是在指导常识知识方面。尽管详细的说明可以减轻选择误导性答案的情况，但也会增加出现不明确答案的情况。

    This paper explores the robustness of LLMs' preference to their internal memory or the given prompt, which may contain contrasting information in real-world applications due to noise or task settings. To this end, we establish a quantitative benchmarking framework and conduct the role playing intervention to control LLMs' preference. In specific, we define two types of robustness, factual robustness targeting the ability to identify the correct fact from prompts or memory, and decision style to categorize LLMs' behavior in making consistent choices -- assuming there is no definitive "right" answer -intuitive, dependent, or rational based on cognitive theory. Our findings, derived from extensive experiments on seven open-source and closed-source LLMs, reveal that these models are highly susceptible to misleading prompts, especially for instructing commonsense knowledge. While detailed instructions can mitigate the selection of misleading answers, they also increase the incidence of in
    
[^166]: 带标签上下文的半自回归流式自动语音识别

    Semi-Autoregressive Streaming ASR With Label Context. (arXiv:2309.10926v1 [cs.CL])

    [http://arxiv.org/abs/2309.10926](http://arxiv.org/abs/2309.10926)

    提出了一种带有标签上下文的半自回归流式自动语音识别模型，通过使用语言模型子网络，将先前块中的标签作为额外的上下文进行建模。实验结果表明，该方法在流式自动语音识别中取得了更好的性能。

    

    非自回归(NAR)建模在语音处理中引起了广泛关注，因为这些模型在推断时间方面比自回归(AR)模型大大降低，同时也达到了较好的转录准确率。由于NAR自动语音识别(ASR)模型必须等待整个话语的完整完成才能进行处理，因此一些研究探索了基于块状注意力的流式NAR模型，以用于低延迟应用。然而，与流式AR和非流式NAR模型相比，流式NAR模型在准确性方面明显滞后。为了解决这个问题，我们提出了一种流式的“半自回归”ASR模型，通过使用语言模型(LM)子网络将先前块中发出的标签作为附加上下文进行建模。我们还引入了一种新颖的贪婪解码算法，能够在块之间附近处理插入和删除错误，同时不显著增加推断时间。实验结果表明，我们的方法优于现有的流式方法。

    Non-autoregressive (NAR) modeling has gained significant interest in speech processing since these models achieve dramatically lower inference time than autoregressive (AR) models while also achieving good transcription accuracy. Since NAR automatic speech recognition (ASR) models must wait for the completion of the entire utterance before processing, some works explore streaming NAR models based on blockwise attention for low-latency applications. However, streaming NAR models significantly lag in accuracy compared to streaming AR and non-streaming NAR models. To address this, we propose a streaming "semi-autoregressive" ASR model that incorporates the labels emitted in previous blocks as additional context using a Language Model (LM) subnetwork. We also introduce a novel greedy decoding algorithm that addresses insertion and deletion errors near block boundaries while not significantly increasing the inference time. Experiments show that our method outperforms the existing streaming 
    
[^167]: 为即时反馈生成定义AI人物形象

    Writer-Defined AI Personas for On-Demand Feedback Generation. (arXiv:2309.10433v1 [cs.HC])

    [http://arxiv.org/abs/2309.10433](http://arxiv.org/abs/2309.10433)

    这项研究提出了基于作家定义的AI人物形象生成即时反馈的概念，通过两项用户研究表明，这个概念受到了作家的欢迎并帮助他们获得不同的观点。这项工作扩展了AI工具设计中的社会技术视角，为支持作家与AI的愿景做出了贡献。

    

    优秀的写作应该根据受众进行适应。然而，作家可能难以同读者产生共鸣，难以及时获得反馈或者难以获得目标群体的信息。本论文提出了一种概念，即基于作家定义的任何目标受众的AI人物形象生成即时反馈。我们通过一个原型（使用GPT-3.5）在两项用户研究（N=5和N=11）中探索了这一概念：作家们赞赏这一概念，并且战略性地使用人物形象来获得不同的观点。反馈被认为是有帮助的，并且激发了文本和人物形象的修订，尽管该反馈通常冗长而不具体。我们讨论了即时反馈的影响、当代AI系统的有限代表性以及进一步定义AI人物的想法。这项工作在支持作家与AI的愿景中做出了贡献，扩展了AI工具设计中的社会技术视角：为了赋予创作者权力，我们还需要考虑他们与观众的关系。

    Compelling writing is tailored to its audience. This is challenging, as writers may struggle to empathize with readers, get feedback in time, or gain access to the target group. We propose a concept that generates on-demand feedback, based on writer-defined AI personas of any target audience. We explore this concept with a prototype (using GPT-3.5) in two user studies (N=5 and N=11): Writers appreciated the concept and strategically used personas for getting different perspectives. The feedback was seen as helpful and inspired revisions of text and personas, although it was often verbose and unspecific. We discuss the impact of on-demand feedback, the limited representativity of contemporary AI systems, and further ideas for defining AI personas. This work contributes to the vision of supporting writers with AI by expanding the socio-technical perspective in AI tool design: To empower creators, we also need to keep in mind their relationship to an audience.
    
[^168]: 大型语言模型中的背景偏见抑制

    In-Contextual Bias Suppression for Large Language Models. (arXiv:2309.07251v1 [cs.CL])

    [http://arxiv.org/abs/2309.07251](http://arxiv.org/abs/2309.07251)

    基于文本前导语和职业描述句生成的反事实命题模板可以有效抑制大型语言模型中的性别偏见，而不需要访问模型参数，并且不会对下游任务性能产生明显的负面影响。

    

    尽管大型语言模型在各种自然语言处理任务中表现出色，但已有研究报告称其存在令人担忧的性别偏见。先前的工作提出了需要人工标注示例、数据增强和LLM的微调的去偏方法，这些方法计算成本高昂。此外，某些情况下可能无法获得进行去偏所需的内部参数，如商用LLM（如GPT-4）的情况。为了解决这一挑战，我们提出了一种新的去偏替代方法，称为偏见抑制，它不需要访问模型参数。我们展示了基于手动设计的反事实命题模板生成的文本前导语可以准确地抑制LLM中的性别偏见。此外，我们发现职业的描述句可以进一步抑制性别偏见。有趣的是，我们发现偏见抑制对下游任务性能几乎没有不利影响，同时有效缓解了性别偏见。

    Despite their impressive performance in a wide range of NLP tasks, Large Language Models (LLMs) have been reported to encode worrying-levels of gender bias. Prior work has proposed debiasing methods that require human labelled examples, data augmentation and fine-tuning of the LLMs, which are computationally costly. Moreover, one might not even have access to the internal parameters for performing debiasing such as in the case of commercially available LLMs such as GPT-4. To address this challenge we propose bias suppression, a novel alternative to debiasing that does not require access to model parameters. We show that text-based preambles, generated from manually designed templates covering counterfactual statements, can accurately suppress gender biases in LLMs. Moreover, we find that descriptive sentences for occupations can further suppress gender biases. Interestingly, we find that bias suppression has a minimal adverse effect on downstream task performance, while effectively mit
    
[^169]: 从数量到质量：利用自我引导数据选择方法提升LLM性能以进行指令调优

    From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning. (arXiv:2308.12032v1 [cs.CL])

    [http://arxiv.org/abs/2308.12032](http://arxiv.org/abs/2308.12032)

    该论文引入了一种自我引导的方法，让LLM能够自主地选择高质量的指令数据，通过引入指令遵循难度指标（IFD），大幅提高了模型训练效率，并在知名数据集上进行了验证，展示了优于传统数据输入的结果。

    

    在大型语言模型领域，指令数据的质量和数量之间的平衡已成为一个焦点。鉴于此，我们引入了一种自我引导的方法，让LLM能够自主地识别和选择大规模开源数据集中的精选样本，有效减少了指令调优的手动筛选和潜在成本。我们的关键创新是指令遵循难度（IFD）指标，它成为了一个决定性工具，用于识别模型期望响应和自主生成能力之间的差异。通过灵活应用IFD，我们能够找到精选样本，从而大幅提升模型训练效率。在Alpaca和WizardLM等知名数据集上的实证验证支持我们的发现；仅使用传统数据输入的10%，我们的策略展示了改进的结果。这种自我引导挑选和IFD指标的综合意味着LLM优化的一个变革性飞跃，有望同时提高模型性能和降低成本。

    In the realm of Large Language Models, the balance between instruction data quality and quantity has become a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from vast open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to identify discrepancies between a model's expected responses and its autonomous generation prowess. Through the adept application of IFD, cherry samples are pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on renowned datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of conventional data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the optimization of LLMs, promising both
    
[^170]: PMET: 在Transformer中的精确模型编辑

    PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])

    [http://arxiv.org/abs/2308.08742](http://arxiv.org/abs/2308.08742)

    该论文通过分析Transformer模型中的隐藏状态，发现多头自注意力编码了某些通用知识提取模式，因此在进行模型编辑时，不需要更新多头自注意力的权重。

    

    模型编辑技术可以以较低的成本修改大型语言模型中的少量知识，并且已经取得了显著的成功。现有方法假设Transformer层隐藏状态是前馈网络的键值内存的值。它们通常优化Transformer层隐藏状态来记忆目标知识，并将其用于更新大型语言模型中前馈网络的权重。然而，Transformer层隐藏状态的信息流来自三个部分：多头自注意力、前馈网络和残差连接。现有方法忽视了Transformer层隐藏状态包含了前馈网络特别需要的信息这一事实。因此，模型编辑的性能下降。为了实现更精确的模型编辑，我们分析了多头自注意力和前馈网络的隐藏状态，发现多头自注意力编码了某些通用知识提取模式。这意味着当引入新知识时，多头自注意力的权重不需要更新。

    Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
    
[^171]: 一个知识增强的两阶段生成框架用于医学对话信息提取

    A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue Information Extraction. (arXiv:2307.16200v1 [cs.CL])

    [http://arxiv.org/abs/2307.16200](http://arxiv.org/abs/2307.16200)

    本论文提出了一个知识增强的两阶段生成框架（KTGF）用于医学对话信息提取。通过两个阶段的生成，分别生成医学对话中的术语和每个术语的状态，从而更好地建模术语之间的关系。

    

    本文关注医学对话中的术语-状态对提取（MD-TSPE），这在诊断对话系统和电子医疗记录（EMR）的自动抄写中是必不可少的。在过去的几年中，MD-TSPE的研究引起了越来越多的关注，特别是在生成方法取得显著进展之后。然而，这些生成方法在一阶段输出整个由术语-状态对组成的序列时忽略了集成先前知识的需求，这需要更深入的理解来建模术语之间的关系和推断每个术语的状态。本文提出了一个知识增强的两阶段生成框架（KTGF）来解决上述挑战。通过使用任务特定的提示，我们采用单一模型以统一的生成形式完成MD-TSPE的两个阶段：首先生成所有的术语，然后生成每个生成的术语的状态。通过这种方式，可以更有效地学习术语之间的关系。

    This paper focuses on term-status pair extraction from medical dialogues (MD-TSPE), which is essential in diagnosis dialogue systems and the automatic scribe of electronic medical records (EMRs). In the past few years, works on MD-TSPE have attracted increasing research attention, especially after the remarkable progress made by generative methods. However, these generative methods output a whole sequence consisting of term-status pairs in one stage and ignore integrating prior knowledge, which demands a deeper understanding to model the relationship between terms and infer the status of each term. This paper presents a knowledge-enhanced two-stage generative framework (KTGF) to address the above challenges. Using task-specific prompts, we employ a single model to complete the MD-TSPE through two phases in a unified generative form: we generate all terms the first and then generate the status of each generated term. In this way, the relationship between terms can be learned more effect
    
[^172]: DIALGEN: 通过人工生成对话改善对人际对话的理解的协同模型

    DIALGEN: Collaborative Human-LM Generated Dialogues for Improved Understanding of Human-Human Conversations. (arXiv:2307.07047v1 [cs.CL])

    [http://arxiv.org/abs/2307.07047](http://arxiv.org/abs/2307.07047)

    DIALGEN是一个人机半自动对话生成框架，通过迭代生成子对话和使用人工反馈来改善模型性能，适用于自动理解人际对话的应用。

    

    需要自动理解人际对话的应用通常面临与真实世界数据中的私人信息，如呼叫中心或临床对话，有关的挑战。处理受保护的数据也会增加注释成本，从而限制技术发展。为了解决这些挑战，我们提出了DIALGEN，一种人机半自动对话生成框架。DIALGEN使用一种语言模型（ChatGPT），可以遵循架构和风格规范，生成流利的对话文本，通过迭代生成子对话并使用人工反馈来纠正不一致或重定对话的流程。在将代理-客户信息收集呼叫归纳为对话状态跟踪的结构化概括实验中，我们展示了DIALGEN数据能够显著提高模型性能。

    Applications that could benefit from automatic understanding of human-human conversations often come with challenges associated with private information in real-world data such as call center or clinical conversations. Working with protected data also increases costs of annotation, which limits technology development. To address these challenges, we propose DIALGEN, a human-in-the-loop semi-automated dialogue generation framework. DIALGEN uses a language model (ChatGPT) that can follow schema and style specifications to produce fluent conversational text, generating a complex conversation through iteratively generating subdialogues and using human feedback to correct inconsistencies or redirect the flow. In experiments on structured summarization of agent-client information gathering calls, framed as dialogue state tracking, we show that DIALGEN data enables significant improvement in model performance.
    
[^173]: 早期ArXiving对论文被接受的因果影响估计

    Estimating the Causal Effect of Early ArXiving on Paper Acceptance. (arXiv:2306.13891v1 [cs.CL])

    [http://arxiv.org/abs/2306.13891](http://arxiv.org/abs/2306.13891)

    本研究使用数据和因果推断方法，研究了早期ArXiving论文对其被ICLR会议接受的影响。结果显示，早期ArXiving可能会对论文被接受的机会产生影响，但这种影响微乎其微，并且不因作者引用次数和机构排名等因素有所不同。

    

    在论文提交同行审查前发布预印本会产生什么影响？由于没有进行随机对照试验，因此我们需要利用观察数据来回答这个问题。我们使用了ICLR会议（2018-2022）的数据，并应用因果推断方法来估计在审阅期前删除论文（早期arXiving）对论文被会议接受的影响。调整了18个混杂因素，如主题、作者和质量，我们可以得出因果效应的估计值。然而，由于质量是一种难以估计的构建，因此我们使用负面结果控制方法，将论文引用次数作为对照变量，以消除质量混杂效应。我们的结果表明，早期arXiving可能会对论文被接受的机会产生一定程度的影响。然而，当存在影响时，这种影响在作者引用次数和机构排名分组后并没有显著差异。这表明早期arXiving对ICLR会议接受论文的影响微乎其微。

    What is the effect of releasing a preprint of a paper before it is submitted for peer review? No randomized controlled trial has been conducted, so we turn to observational data to answer this question. We use data from the ICLR conference (2018--2022) and apply methods from causal inference to estimate the effect of arXiving a paper before the reviewing period (early arXiving) on its acceptance to the conference. Adjusting for 18 confounders such as topic, authors, and quality, we may estimate the causal effect. However, since quality is a challenging construct to estimate, we use the negative outcome control method, using paper citation count as a control variable to debias the quality confounding effect. Our results suggest that early arXiving may have a small effect on a paper's chances of acceptance. However, this effect (when existing) does not differ significantly across different groups of authors, as grouped by author citation count and institute rank. This suggests that early
    
[^174]: 医疗知识图谱综述：资源、应用和前景

    A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])

    [http://arxiv.org/abs/2306.04802](http://arxiv.org/abs/2306.04802)

    本论文综述了医疗知识图谱(HKGs)的构建流程、关键技术和利用方法以及现有资源，并深入探讨了HKG在各种医疗领域的变革性影响。

    

    医疗知识图谱(HKGs)已成为组织医学知识的有结构且可解释的有为工具，提供了医学概念及其关系的全面视图。然而，数据异质性和覆盖范围有限等挑战仍然存在，强调了在HKG领域需要进一步研究的必要性。本综述是HKG的第一份综合概述。我们总结了HKG构建的流程和关键技术（即从头开始和通过集成），以及常见的利用方法（即基于模型和非基于模型）。为了为研究人员提供有价值的资源，我们根据它们捕获的数据类型和应用领域（该资源存储于https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase）组织了现有的HKG，并提供了相关的统计信息。在应用部分，我们深入探讨了HKG在各种医疗领域的变革性影响。

    Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various hea
    
[^175]: 在祈祷之后喝啤酒？测量大型语言模型中的文化偏见。

    Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. (arXiv:2305.14456v1 [cs.CL])

    [http://arxiv.org/abs/2305.14456](http://arxiv.org/abs/2305.14456)

    这篇论文研究了大型语言模型在处理和生成阿拉伯文本时出现的文化偏向西方文化的现象，表明语言模型在人名、食品、服装、地点、文学、饮料、宗教和体育等八个文化方面存在偏见。这些发现引发对于当前语言模型文化相关性的担忧。

    

    语言模型是否存在文化偏见？语言模型符合所服务社区的文化因素很重要。然而，本文表明在处理和生成阿拉伯文本时，语言模型存在显著的偏向西方文化的偏见，倾向于产生西方文化相关内容而非阿拉伯文化相关内容。我们通过使用从在线社交媒体上收集的自然出现的上下文和基于可能性评分的指标来量化这种偏见。我们的实验显示，阿拉伯语单语和多语模型在八个不同的文化方面存在西方文化偏见，包括人名、食品、服装、地点、文学、饮料、宗教和体育。当输入的阿拉伯语句子越接近英语时，模型也更容易表现出偏见。这些发现引发人们对当前语言模型文化相关性的担忧。我们的分析表明，在模型设计中应更多考虑文化因素和多样性。

    Are language models culturally biased? It is important that language models conform to the cultural aspects of the communities they serve. However, we show in this paper that language models suffer from a significant bias towards Western culture when handling and generating text in Arabic, often preferring, and producing Western-fitting content as opposed to the relevant Arab content. We quantify this bias through a likelihood scoring-based metric using naturally occurring contexts that we collect from online social media. Our experiments reveal that both Arabic monolingual and multilingual models exhibit bias towards Western culture in eight different cultural aspects: person names, food, clothing, location, literature, beverage, religion, and sports. Models also tend to exhibit more bias when prompted with Arabic sentences that are more linguistically aligned with English. These findings raise concerns about the cultural relevance of current language models. Our analyses show that pr
    
[^176]: PWESuite：语音单词嵌入及其任务

    PWESuite: Phonetic Word Embeddings and Tasks They Facilitate. (arXiv:2304.02541v1 [cs.CL])

    [http://arxiv.org/abs/2304.02541](http://arxiv.org/abs/2304.02541)

    本论文展示了一套语音单词嵌入及其相关任务，提高了语音信息处理的效果和可重复性。

    

    将单词映射到固定维度的向量空间的单词嵌入是现代自然语言处理的基础。大多数单词嵌入方法编码语义信息。但是，对于某些任务非常重要的语音信息经常被忽略。在这项工作中，我们开发了几种新方法，利用发声特征构建语音知情单词嵌入，并提供一套语音单词嵌入以鼓励其社区的开发、评估和使用。虽然已经存在许多学习语音单词嵌入的方法，但在评估其有效性方面缺乏一致性。因此，我们还提出了几种评估语音单词嵌入的内在方面的方法，如单词检索和与声音相似性的相关性，以及外在表现，如韵律和同源检测和声音类比。我们希望我们的任务套件将促进可重复性并提供未来语音单词嵌入研究的方向。

    Word embeddings that map words into a fixed-dimensional vector space are the backbone of modern NLP. Most word embedding methods encode semantic information. However, phonetic information, which is important for some tasks, is often overlooked. In this work, we develop several novel methods which leverage articulatory features to build phonetically informed word embeddings, and present a set of phonetic word embeddings to encourage their community development, evaluation and use. While several methods for learning phonetic word embeddings already exist, there is a lack of consistency in evaluating their effectiveness. Thus, we also proposes several ways to evaluate both intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and extrinsic performances, such as rhyme and cognate detection and sound analogies. We hope that our suite of tasks will promote reproducibility and provide direction for future research on phonetic word embeddi
    

