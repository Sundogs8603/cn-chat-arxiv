# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Pok\'eLLMon: A Human-Parity Agent for Pok\'emon Battles with Large Language Models](https://rss.arxiv.org/abs/2402.01118) | Pok\'eLLMon是第一个在战术对战游戏中实现人类能力的语言模型化身代理机器人。它通过上下文强化学习、知识增强生成和一致的行动生成的策略，展现了与人类类似的战斗策略和及时决策，并在Ladder比赛中达到了49%的胜率，在邀请对战中达到了56%的胜率。 |
| [^2] | [Topic-based Watermarks for LLM-Generated Text](https://arxiv.org/abs/2404.02138) | 提出了一种新的基于主题的水印算法，旨在解决当前水印方案的局限性，为区分LLM生成的文本和人类生成的文本提供了新的思路。 |
| [^3] | [FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning](https://arxiv.org/abs/2404.02127) | 本研究提出了一个名为LawInstruct的大型法律指导数据集，证明了领域特定的预训练和指导调整可以改善在LegalBench上的性能，为在法律领域开发具有更强信息处理和决策能力的模型提供了一个资源。 |
| [^4] | [Rematch: Robust and Efficient Matching of Local Knowledge Graphs to Improve Structural and Semantic Similarity](https://arxiv.org/abs/2404.02126) | 引入一种新的AMR相似度度量rematch以提高结构和语义相似性，结构相似性排名第二，语义相似性最优，快五倍于下一个最高效度量。 |
| [^5] | [Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models](https://arxiv.org/abs/2404.02124) | 通过大型语言模型探索数学多项选择题的自动生成干扰项，发现虽然LLMs可以生成一些数学上有效的干扰项，但在预测常见错误或误解方面表现不佳 |
| [^6] | [GINopic: Topic Modeling with Graph Isomorphism Network](https://arxiv.org/abs/2404.02115) | GINopic是一种主题建模框架，利用图同构网络捕捉单词之间的相关性，相比于现有主题模型，展示了更好的有效性和推进主题建模的潜力。 |
| [^7] | [CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems](https://arxiv.org/abs/2404.02103) | 提出了ClapNQ，一个用于完整RAG管道的基准长格式问答数据集，要求RAG模型能适应包括简洁、一致和不连续段落片段的答案特性。 |
| [^8] | [LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause Pair Extraction as Sequence Labelling Task](https://arxiv.org/abs/2404.02088) | 本文提出了针对SemEval 2024引入的多模态情感原因分析任务的模型，将其作为话语标记和序列标记问题来解决，并进行了比较研究。 |
| [^9] | [Advancing LLM Reasoning Generalists with Preference Trees](https://arxiv.org/abs/2404.02078) | 新推出的Eurus模型通过基于首选树的推理优化，在多项基准测试中取得了业界领先的成果，尤其在击败了GPT-3.5 Turbo的基准测试中表现突出。 |
| [^10] | [Using Interpretation Methods for Model Enhancement](https://arxiv.org/abs/2404.02068) | 提出了利用解释方法和黄金解释来增强模型的框架，并展示在低资源情况下特别有效。 |
| [^11] | [Long-context LLMs Struggle with Long In-context Learning](https://arxiv.org/abs/2404.02060) | 该研究引入了一个专门的基准 LIConBench，聚焦于长上下文学习，发现长文本语言模型在极端标签分类领域中性能良好，尤其在标记长度不超过20K时表现相对较好。 |
| [^12] | [Deconstructing In-Context Learning: Understanding Prompts via Corruption](https://arxiv.org/abs/2404.02054) | 大型语言模型的能力在上下文中学习已经导致AI助手的急剧增长，其鲁棒性部分归因于对齐技术，然而这些助手使用的预训练模型在这方面却较为脆弱，构建高质量的骨干模型仍然是一个挑战。 |
| [^13] | [BERTopic-Driven Stock Market Predictions: Unraveling Sentiment Insights](https://arxiv.org/abs/2404.02053) | 这项研究利用BERTopic分析股市评论中的情感，整合深度学习模型，显示情感分析显著提升了股市预测性能，揭示了NLP在丰富金融分析方面的潜力。 |
| [^14] | [Ukrainian Texts Classification: Exploration of Cross-lingual Knowledge Transfer Approaches](https://arxiv.org/abs/2404.02043) | 乌克兰文本分类领域探索跨语言知识传递方法，利用最新的NLP技术，测试了在毒性分类、文体分类和自然语言推理任务上的最佳设置。 |
| [^15] | [MultiParaDetox: Extending Text Detoxification with Parallel Data to New Languages](https://arxiv.org/abs/2404.02037) | 本研究提出了MultiParaDetox，将ParaDetox管道扩展到多种语言，以自动化收集潜在任何语言的并行净化语料库。 |
| [^16] | [Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts](https://arxiv.org/abs/2404.02022) | 本文提出一种通用且便利的方法，通过利用小型编码器语言模型和交叉注意力，使原始语言模型可以覆盖更长的上下文，从而提高开放领域问答任务的性能。 |
| [^17] | [Breaking the Silence Detecting and Mitigating Gendered Abuse in Hindi, Tamil, and Indian English Online Spaces](https://arxiv.org/abs/2404.02013) | 在该研究中，我们开发了一种结合了CNN和BiLSTM网络的集成方法，有效模拟了文本数据中的语义和顺序模式，用于检测和缓解印地语、泰米尔语和印度英语在线空间中的性别虐待。 |
| [^18] | [Preuve de concept d'un bot vocal dialoguant en wolof](https://arxiv.org/abs/2404.02009) | 这个论文介绍了在塞内加尔使用的主要通用语言Wolof中建立的第一个自动语音助手的概念证明，旨在为Orange客户提供关于Orange Senegal的Sargal忠诚计划信息，取得了一定的初步成功。 |
| [^19] | [Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context](https://arxiv.org/abs/2404.02000) | 这项研究提出了首个仅在非洲语音上进行训练的自监督多语言语音模型，相比于常规方法，更高效并在ASR和LID任务中表现出竞争力。 |
| [^20] | [DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning](https://arxiv.org/abs/2404.01994) | 通过跨模态对比学习实现的DELAN框架提出了双层对齐方法，以提升视觉与语言导航中的跨模态交互和行动决策。 |
| [^21] | [Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary Information on Knowledge Retrieval from Pretrained Language Models](https://arxiv.org/abs/2404.01992) | 设计了CONPARE-LAMA，确定了释义的语法和语义信息对知识检索的独立影响 |
| [^22] | [Kallaama: A Transcribed Speech Dataset about Agriculture in the Three Most Widely Spoken Languages in Senegal](https://arxiv.org/abs/2404.01991) | Kallaama项目发布了一个包含125小时录音的语音数据集，旨在填补塞内加尔三种主要语言中关于农业领域的机器可读数据不足的空白。 |
| [^23] | [Team UTSA-NLP at SemEval 2024 Task 5: Prompt Ensembling for Argument Reasoning in Civil Procedures with GPT4](https://arxiv.org/abs/2404.01961) | 通过使用GPT4进行基于提示的解决方案，我们的系统在民事诉讼中的辩证推理方面取得了显著成果，包括思维链推理和上下文学习等提示策略的集成。在SemEval任务5中，我们获得了0.8095的Macro F1值，并在最终测试集中排名第5。 |
| [^24] | [HyperCLOVA X Technical Report](https://arxiv.org/abs/2404.01954) | HyperCLOVA X 是针对韩国语言和文化定制的大型语言模型，同时具有竞争能力的英语、数学和编码能力，其推理能力强大且具有跨语言的通用能力。 |
| [^25] | [Classifying Graphemes in English Words Through the Application of a Fuzzy Inference System](https://arxiv.org/abs/2404.01953) | 通过模糊推理系统，本文提出将单词分割成字素的方法，正确预测的准确率为50.18％，93.51％的预测在正确分类的范围内。 |
| [^26] | [Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation](https://arxiv.org/abs/2404.01940) | 提出使用精细调整的大型语言模型(LLM)生成能够准确捕捉网络犯罪语言细微差别的翻译，结果显示能够更好、更快、更准确地捕捉语言的细微差别，并大幅度降低翻译成本。 |
| [^27] | [SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation](https://arxiv.org/abs/2404.01923) | 本研究提出了SGSH框架，通过骨架启发来激发GPT-3.5生成知识库问题，有效地组织和利用丰富的语义知识。 |
| [^28] | [A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document Event Coreference Resolution](https://arxiv.org/abs/2404.01921) | 该研究提出了一种基于理性中心的反事实数据增强方法，通过结构因果模型识别虚假和因果关联，有效提高了跨文档事件关联消解系统的性能。 |
| [^29] | [SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities](https://arxiv.org/abs/2404.01914) | 提出了SCANNER模型，通过提取实体候选并利用知识从多种来源获取知识，增强了对未见实体的识别能力，并引入了一种新颖的方法来处理NER数据集中带有噪声注释的挑战。 |
| [^30] | [Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack](https://arxiv.org/abs/2404.01907) | 本文提出了一个旨在对机器生成的文本进行微小扰动以规避检测的广泛对抗攻击框架，通过白盒和黑盒两种攻击设置以及对抗学习在动态场景中的应用，评估了当前检测模型对此类攻击的鲁棒性潜力增强 |
| [^31] | [Activation Steering for Robust Type Prediction in CodeLLMs](https://arxiv.org/abs/2404.01903) | 我们提出了一种激活导向技术，通过编辑模型内部激活来改善CodeLLMs在代码类型预测中对于语法干扰的鲁棒性，并成功应用于Python和TypeScript的类型预测，将类型误差率纠正高达90%。 |
| [^32] | [Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey](https://arxiv.org/abs/2404.01869) | 本文通过综述超越任务准确性的研究，提供对大型语言模型推理过程更深入了解，并强调了LLMs倾向于依赖于训练数据中的表面模式和相关性。 |
| [^33] | [Self-StrAE at SemEval-2024 Task 1: Making Self-Structuring AutoEncoders Learn More With Less](https://arxiv.org/abs/2404.01860) | Self-StrAE 在自编码器中添加重构作为辅助目标，同时增加独立通道数量以显著提高嵌入质量，并成功将非嵌入参数数量减少到七个。 |
| [^34] | [Detecting Gender Bias in Course Evaluations](https://arxiv.org/abs/2404.01857) | 通过机器学习和自然语言处理，研究发现课程评价中的性别偏见，比较英语和瑞典课程数据，揭示了学生对课程的评价主观性因考官性别而存在差异。 |
| [^35] | [Poro 34B and the Blessing of Multilinguality](https://arxiv.org/abs/2404.01856) | 多语种训练的Poro 34B模型在芬兰语等小语种上取得了显著进展，并具有比现有模型更出色的能力。 |
| [^36] | [IndoCulture: Exploring Geographically-Influenced Cultural Commonsense Reasoning Across Eleven Indonesian Provinces](https://arxiv.org/abs/2404.01854) | 本研究介绍了IndoCulture项目，旨在通过当地人手动收集数据，探索印尼十一个省份间地理影响的文化常识推理。评估发现，即使是最好的语言模型也在特定省份上表现更准确，而添加地理信息有助于提高模型性能。 |
| [^37] | [A (More) Realistic Evaluation Setup for Generalisation of Community Models on Malicious Content Detection](https://arxiv.org/abs/2404.01822) | 提出了面向恶意内容检测社区模型泛化的更为现实的评估设置，通过少样本子图采样方法测试模型泛化能力。 |
| [^38] | [Sentiment Analysis of Citations in Scientific Articles Using ChatGPT: Identifying Potential Biases and Conflicts of Interest](https://arxiv.org/abs/2404.01800) | 使用ChatGPT进行情感分析揭示了科学文章引用中的偏见和利益冲突，增强了科学文献评估的客观性和可靠性 |
| [^39] | [PATCH -- Psychometrics-AssisTed benCHmarking of Large Language Models: A Case Study of Mathematics Proficiency](https://arxiv.org/abs/2404.01799) | 该论文提出了一种新的框架PATCH，用于将心理测量领域的知识整合到大型语言模型的基准测试中，以解决现有基准测试存在的测量质量、项目级别评估和参考人群等问题。 |
| [^40] | [Generative AI-Based Text Generation Methods Using Pre-Trained GPT-2 Model](https://arxiv.org/abs/2404.01786) | 通过分析各种传统和现代的文本生成技术，该研究提供了对每种方法优势、劣势和潜在应用的宝贵见解，并对这些方法的性能进行了比较研究。 |
| [^41] | [Can Humans Identify Domains?](https://arxiv.org/abs/2404.01785) | 本研究探讨了人类在识别相关文本属性方面的熟练程度，特别是流派和主题的概念，以解析文本领域的核心概念。 |
| [^42] | [Auditing Large Language Models for Enhanced Text-Based Stereotype Detection and Probing-Based Bias Evaluation](https://arxiv.org/abs/2404.01768) | 该研究引入了Multi-Grain Stereotype（MGS）数据集，探索了不同的机器学习方法用于建立陈规检测的基线，并提出了一系列基于MGS数据训练的英文文本的陈规分类器模型。 |
| [^43] | [Class-Incremental Few-Shot Event Detection](https://arxiv.org/abs/2404.01767) | 提出了增量式少样本事件检测任务，并通过提出的Prompt-KD方法解决了老知识遗忘和新类别过拟合问题。 |
| [^44] | [M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets](https://arxiv.org/abs/2404.01753) | 本文通过将现有的推特情感数据集转换为多模态格式，并进行基准实验，发现在多语言环境下使用情感调整的大型语言模型作为文本编码器时，效果显著。 |
| [^45] | [Octopus v2: On-device language model for super agent](https://arxiv.org/abs/2404.01744) | 该研究提出了一种新方法，通过将具有20亿参数的设备上模型赋予超越GPT-4的准确性和延迟性能，并将上下文长度缩减95％，从而解决了函数调用中的延迟和准确性问题。 |
| [^46] | [Transfer Learning from Whisper for Microscopic Intelligibility Prediction](https://arxiv.org/abs/2404.01737) | 本文研究了如何利用来自Whisper的迁移学习，用于在词汇响应水平上进行微观可懂性预测，并取得了显著的相对改进。 |
| [^47] | [Sentence-level Media Bias Analysis with Event Relation Graph](https://arxiv.org/abs/2404.01722) | 本文提出了一种基于事件关系图的句子级媒体偏见分析方法，通过构建事件关系图和事件感知的语言模型，实现了对偏见句的识别和表达。 |
| [^48] | [Self-Improvement Programming for Temporal Knowledge Graph Question Answering](https://arxiv.org/abs/2404.01720) | 设计了基本的时间操作符，并引入了一种新颖的自我提升编程方法用于时间知识图问题回答，通过上下文学习能力来理解问题中的组合时间约束 |
| [^49] | [Effective internal language model training and fusion for factorized transducer model](https://arxiv.org/abs/2404.01716) | 提出了一种对于分解转录模型的有效ILM训练和解码策略，能够显著改善与标准解码方法的性能，并在LibriSpeech数据集上取得了17%的相对改善。 |
| [^50] | [EMONA: Event-level Moral Opinions in News Articles](https://arxiv.org/abs/2404.01715) | 本文提出了一个新任务，即理解新闻文章中对事件的道德观点，并创建了一个包含事件级别道德观点的新数据集EMONA，为提取事件的道德性在新闻文章中建立了基线模型和进行外部评估。 |
| [^51] | [Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G](https://arxiv.org/abs/2404.01713) | 本文探讨了生成式人工智能用于沉浸式通信中减少带宽消耗的实用价值。 |
| [^52] | [Polarity Calibration for Opinion Summarization](https://arxiv.org/abs/2404.01706) | 引入极性校准概念，开发强化训练方法，通过平衡极性校准、内容保留和语言自然性，解决观点总结中放大极性偏见的问题。 |
| [^53] | [On the Role of Summary Content Units in Text Summarization Evaluation](https://arxiv.org/abs/2404.01701) | 本研究探讨了两种新颖策略来逼近摘要内容单元（SCUs），分别是从AMR意义表示（SMUs）和大型语言模型（SGUs）生成SCU近似。 |
| [^54] | [Event Detection from Social Media for Epidemic Prediction](https://arxiv.org/abs/2404.01679) | 通过开发一个从社交媒体帖子中提取和分析流行病相关事件的框架，该研究首次利用事件检测技术在COVID-19流行病和其他未见流行病中实现有效的早期预警。 |
| [^55] | [Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation](https://arxiv.org/abs/2404.01677) | 通过引入归结反驳范式，提出了一个名为GFaiR的框架，旨在解决大型语言模型在进行自然语言形式逻辑理论一阶逻辑推理时的困难，并通过证明插入解决方案改进了系统的完整性 |
| [^56] | [METAL: Towards Multilingual Meta-Evaluation](https://arxiv.org/abs/2404.01667) | 提出了一个用于多语言场景下LLMs评估的端到端框架，并创建了一个涵盖10种语言的精心策划数据集。 |
| [^57] | [CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models](https://arxiv.org/abs/2404.01663) | CMAT框架引入了TinyAgent模型，并提出了一种新颖的系统，通过环境反馈进行自适应权重更新，增强了语言智能体的能力和长期记忆。 |
| [^58] | [Release of Pre-Trained Models for the Japanese Language](https://arxiv.org/abs/2404.01657) | 发布日语预训练模型以缩小非英语社区中的AI访问差距，促进AI民主化。 |
| [^59] | [Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization](https://arxiv.org/abs/2404.01652) | 本文研究了在开放域问答中泛化性能的问题，发现挑战在于阅读器过度依赖记忆外部语料库的知识，限制了模型的泛化能力。 |
| [^60] | [NLP Systems That Can't Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps](https://arxiv.org/abs/2404.01651) | NLP 系统无法区分使用和提及，对于处理在线抗议言论至关重要，教导这种区分有助于减少虚假信息和仇恨言论检测中的审查错误 |
| [^61] | [Entity Disambiguation via Fusion Entity Decoding](https://arxiv.org/abs/2404.01626) | 提出了一种通过融合实体描述进行实体消歧的编码-解码模型。 |
| [^62] | [Transforming LLMs into Cross-modal and Cross-lingual RetrievalSystems](https://arxiv.org/abs/2404.01616) | 提出使用LLMs初始化多模态DE检索系统，实现在102种语言中匹配语音和文本的能力，无需在LLM预训练期间使用语音数据，且相比先前系统取得10%的Recall@1绝对改进 |
| [^63] | [Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game](https://arxiv.org/abs/2404.01602) | 本研究通过狼人游戏模拟平台评估了大语言模型的观点领导作用，并开发了两个新的评估指标。 |
| [^64] | [Classifying Cancer Stage with Open-Source Clinical Large Language Models](https://arxiv.org/abs/2404.01589) | 本研究展示了在没有任何标记的训练数据的情况下，开源的临床大型语言模型能够从真实病理报告中提取病理性肿瘤-淋巴结-转移（pTNM）分期信息 |
| [^65] | [Hallucination Diversity-Aware Active Learning for Text Summarization](https://arxiv.org/abs/2404.01588) | 本文首次提出了一种基于幻觉多样性的主动学习框架，用于减轻大型语言模型（LLMs）在文本摘要中产生的幻觉，减少了昂贵的人类注释需求。 |
| [^66] | [BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System](https://arxiv.org/abs/2404.01582) | 本文提出了一种基于GPT-3.5的抄袭文本数据生成方法和一种基于Faiss和BERT的高效高准确性的抄袭识别方法，填补了高水平抄袭检测研究数据集缺失的空白，实验证明该模型在多个指标上表现优异 |
| [^67] | [Evaluating Large Language Models Using Contrast Sets: An Experimental Approach](https://arxiv.org/abs/2404.01569) | 介绍了一种为斯坦福自然语言推断（SNLI）数据集生成对比集的创新技术，通过自动替换动词、副词和形容词为同义词来评估模型的性能是否基于真实的语言理解还是仅仅基于模式识别。 |
| [^68] | [Octopus: On-device language model for function calling of software APIs](https://arxiv.org/abs/2404.01549) | 本研究介绍了一种利用设备端语言模型进行软件API函数调用的新策略，通过优化模型对API结构和语法的理解，显著提高了API函数调用的准确性。 |
| [^69] | [Laying Anchors: Semantically Priming Numerals in Language Modeling](https://arxiv.org/abs/2404.01536) | 通过生成受数字分布规律控制的锚点，我们引入了一种在语义上引导数字的策略，在广泛范围的数字任务上实现了数学基础表示的显著改进。 |
| [^70] | [Set-Aligning Framework for Auto-Regressive Event Temporal Graph Generation](https://arxiv.org/abs/2404.01532) | 提出了针对自动回归事件时间图生成的条件集合生成问题的集合对齐框架，用于解决线性化图和语言模型处理序列不匹配的挑战 |
| [^71] | [AAdaM at SemEval-2024 Task 1: Augmentation and Adaptation for Multilingual Semantic Textual Relatedness](https://arxiv.org/abs/2404.01490) | 提出利用机器翻译进行数据增强以解决训练数据有限的挑战，应用任务自适应预训练和适配器框架实现了竞争性结果。 |
| [^72] | [A Study on Scaling Up Multilingual News Framing Analysis](https://arxiv.org/abs/2404.01481) | 本研究扩展了框架分析至多语言环境，通过众包创建数据集并结合其他现有数据集，有效提升了结果，展示了众包的可行性。 |
| [^73] | [TraveLER: A Multi-LMM Agent Framework for Video Question-Answering](https://arxiv.org/abs/2404.01476) | TraveLER是一种多重LMM代理框架，通过沿着视频移动，并通过交互式提问收集关键帧的信息，以解决视频问答中关键时间戳选择和错误时间戳调整的问题 |
| [^74] | [Finding Replicable Human Evaluations via Stable Ranking Probability](https://arxiv.org/abs/2404.01474) | 本研究通过机器翻译和其最先进的人类评估框架MQM作为案例研究，提出了关于设置可靠人类评估以得出稳定结论的方法，以及针对设计可复制人类评估研究的具体建议。 |
| [^75] | [OpenChemIE: An Information Extraction Toolkit For Chemistry Literature](https://arxiv.org/abs/2404.01462) | OpenChemIE提出了一种用于从化学文献中提取反应数据的工具包，通过整合文本、表格和图像信息以及使用专门神经模型和算法，实现了在文档级别的反应数据提取。 |
| [^76] | [Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs](https://arxiv.org/abs/2404.01461) | 该研究调查了代表性启发式对大型语言模型推理的影响，并创建了专门的数据集进行实验验证 |
| [^77] | [Unveiling Divergent Inductive Biases of LLMs on Temporal Data](https://arxiv.org/abs/2404.01453) | 本研究探索了LLMs在时间数据分析中的固有挑战，重点评估了GPT-3.5和GPT-4模型的性能，发现了它们在特定时间关系上存在偏向性。 |
| [^78] | [Enterprise Use Cases Combining Knowledge Graphs and Natural Language Processing](https://arxiv.org/abs/2404.01443) | 知识图谱和自然语言处理的结合为企业提供了灵活、可扩展且语义丰富的方式来组织和理解数据，本文探讨了这种组合带来的协同效应，涵盖了知识图谱构建、推理以及基于知识图谱的NLP任务三个核心领域的各种方法。 |
| [^79] | [Creating emoji lexica from unsupervised sentiment analysis of their descriptions](https://arxiv.org/abs/2404.01439) | 该论文提出了一种从在线文本消息中预测表情符号所表达情感的新方法，无需人工标注数据，节省了宝贵时间。 |
| [^80] | [Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs](https://arxiv.org/abs/2404.01430) | 本研究发现LLMs的位置偏差主要源于不同模型的固有位置偏好，并提出了一种面向位置的参数高效微调方法来解决这一问题。 |
| [^81] | [Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data](https://arxiv.org/abs/2404.01413) | 本文通过比较数据取代和数据积累两种情况，发现累积数据可以防止模型崩溃。 |
| [^82] | [Developing Safe and Responsible Large Language Models -- A Comprehensive Framework](https://arxiv.org/abs/2404.01399) | 该论文介绍了一种新的模型SR$_{\text{LLM}}$，旨在通过引入全面的安全风险分类法和专家标注数据集来增强大型语言模型（LLM）在语言生成中的安全性，并通过指令和参数高效微调方法有效减少了不安全内容的生成。 |
| [^83] | [Prompt-prompted Mixture of Experts for Efficient LLM Generation](https://arxiv.org/abs/2404.01365) | 提出了一种名为GRIFFIN的训练-free MoE，能够在各种LLM模型中选择唯一的FF专家以实现高效生成。 |
| [^84] | [LLM Attributor: Interactive Visual Attribution for LLM Generation](https://arxiv.org/abs/2404.01361) | LLM Attributor是一个Python库，提供了交互式可视化方式用于将LLM的文本生成结果归因到训练数据点，帮助用户检查模型行为、增强可信度，并与用户提供的文本进行比较。 |
| [^85] | [Utilizing AI and Social Media Analytics to Discover Adverse Side Effects of GLP-1 Receptor Agonists](https://arxiv.org/abs/2404.01358) | 通过利用人工智能驱动的社交媒体分析，我们开发了一种数字健康方法，成功检测出与GLP-1受体激动剂相关的21种潜在不良副作用，包括易怒和麻木感，从而革新了对新部署药物未报告ASEs的检测。 |
| [^86] | [Efficiently Distilling LLMs for Edge Applications](https://arxiv.org/abs/2404.01353) | 提出了一种名为MLFS的新方法，用于高效参数的超网络训练，可以获得适用于商业边缘应用的高质量编码器模型，并有效地减少训练时间。 |
| [^87] | [Fairness in Large Language Models: A Taxonomic Survey](https://arxiv.org/abs/2404.01349) | 该调查总结了大型语言模型中公平性的最新进展，包括对偏见因素的分析、公平度量和现有算法分类。 |
| [^88] | [Enhancing Bangla Fake News Detection Using Bidirectional Gated Recurrent Units and Deep Learning Techniques](https://arxiv.org/abs/2404.01345) | 使用深度学习技术中的双向门控循环单元，在孟加拉语中识别虚假新闻，准确率达到99.16% |
| [^89] | [Mind Your Neighbours: Leveraging Analogous Instances for Rhetorical Role Labeling for Legal Documents](https://arxiv.org/abs/2404.01344) | 通过利用类似实例的知识，本研究引入了新颖技术来增强法律文件修辞角色标注性能，在宏F1分数上取得显着改进。 |
| [^90] | [CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs](https://arxiv.org/abs/2404.01343) | CHOPS提出了一个名为CHOPS的LLM代理，旨在更高效地利用现有数据库或系统来访问用户信息，提供准确合理的响应或执行所需操作，同时避免有害操作。 |
| [^91] | [DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model](https://arxiv.org/abs/2404.01342) | DiffAgent利用大语言模型设计了一个新的代理工具，能够快速准确地选择最适合的文本到图像API，并提出了一个综合数据集DABench来评估其性能。 |
| [^92] | [Humane Speech Synthesis through Zero-Shot Emotion and Disfluency Generation](https://arxiv.org/abs/2404.01339) | 该论文实现了一种新颖的语音合成技术，通过零-shot设置中引入人类情感和不流畅特征，使得系统能更好地模仿人类语音，促进更自然的用户互动。 |
| [^93] | [Automatic detection of relevant information, predictions and forecasts in financial news through topic modelling with Latent Dirichlet Allocation](https://arxiv.org/abs/2404.01338) | 该研究提出了一种新颖的自然语言处理系统，通过Latent Dirichlet Allocation (LDA)进行相关的主题建模，帮助投资者从非结构化文本源中检测财经事件中的相关信息、预测和预测 |
| [^94] | [Detection of Temporality at Discourse Level on Financial News by Combining Natural Language Processing and Machine Learning](https://arxiv.org/abs/2404.01337) | 通过结合自然语言处理和机器学习技术，提出了一种新颖的系统，旨在在金融新闻中检测篇章级别的关键声明的时间性，以分析句法和语义依赖关系，区分上下文信息和有价值的预测。 |
| [^95] | [FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain Fake News Detecction](https://arxiv.org/abs/2404.01336) | FineFake 数据集为细粒度多领域假新闻检测提供了知识增强，包含16,909个数据样本覆盖六个语义主题和八个平台。 |
| [^96] | [Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation](https://arxiv.org/abs/2404.01334) | 本研究引入了一种新颖的混合标注方法，将人力工作与大型语言模型相结合，旨在提高NER模型的性能，并以成本效益的方式实现这一目标。 |
| [^97] | [Wait, It's All Token Noise? Always Has Been: Interpreting LLM Behavior Using Shapley Value](https://arxiv.org/abs/2404.01332) | 使用Shapley值方法解释LLM行为，揭示了所谓的“令牌噪音”效应，揭示了LLMs的决策在很大程度上受到提示组件的影响 |
| [^98] | [LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model](https://arxiv.org/abs/2404.01331) | 使用最新发布的Gemma大型语言模型在LLaVA框架中训练了多模态基础模型，研究了预训练连接器、更强大的图像主干和增加语言主干大小对模型性能的影响。 |
| [^99] | [Entertainment chatbot for the digital inclusion of elderly people without abstraction capabilities](https://arxiv.org/abs/2404.01327) | EBER chatbot是一个旨在减少老年人数字鸿沟的娱乐聊天机器人，其创新之处在于其"智能电台"概念，根据用户的心情和需求提供相关信息。 |
| [^100] | [A Review of Multi-Modal Large Language and Vision Models](https://arxiv.org/abs/2404.01322) | 该论文对具有多模态能力的大型语言模型进行了综述，涵盖了LLMs的发展历程、transformer-based 架构的进展以及注意机制的作用 |
| [^101] | [Intelligent Learning Rate Distribution to reduce Catastrophic Forgetting in Transformers](https://arxiv.org/abs/2404.01317) | 本文研究了在Transformer神经网络中的灾难性遗忘问题，通过智能学习率分布取得了比平坦学习率更好的性能，并在GLUE数据集中得到验证。 |
| [^102] | [NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for Large Language Models](https://arxiv.org/abs/2404.01306) | 本研究受神经系统启发，通过神经网络拓扑的稀疏方法，探索类似于生物网络的机制，展示了对各种 NLP 任务都表现出色和高效的模型-不可知稀疏性方法 |
| [^103] | [IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations](https://arxiv.org/abs/2404.01266) | IsoBench提出了一个基准数据集，用于评估基于不同同构表示的多模态基础模型的性能差异，发现大多数模型更偏好文本表示。 |
| [^104] | [Open-Vocabulary Federated Learning with Multimodal Prototyping](https://arxiv.org/abs/2404.01232) | 本研究针对联邦学习中的开放词汇挑战，提出了一种基于预训练视觉语言模型的新型自适应框架，命名为联邦多模式原型（Fed-MP），用于解决新用户提出的涉及任意未知类别的查询问题。 |
| [^105] | [AI Act and Large Language Models (LLMs): When critical issues and privacy impact require human and ethical oversight](https://arxiv.org/abs/2404.00600) | 论文讨论了人类监督、道德监督和隐私影响评估在面对人工智能系统和大型语言模型发展中的重要性。 |
| [^106] | [MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in Conversations with Multimodal Language Models](https://arxiv.org/abs/2404.00511) | 本文提出了一种集成文本、音频和视觉模态的多模态情绪识别和多模态情绪原因提取框架，通过利用专门的情绪编码器和模态特定特征，实现了提升情绪理解和因果推理的竞争性成果。 |
| [^107] | [LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning](https://arxiv.org/abs/2404.00027) | 探讨使用大型语言模型作为写作助手引发的写作所有权感和作者身份认知之间的心理困境。 |
| [^108] | [Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs](https://arxiv.org/abs/2404.00026) | 研究探讨了人们日益依赖的基于LLM的写作助手对创造力和个性可能造成的负面影响，旨在改进人机交互系统和提升写作助手的个性化和个性化功能。 |
| [^109] | [Concept-based Analysis of Neural Networks via Vision-Language Models](https://arxiv.org/abs/2403.19837) | 本文提出利用视觉语言模型作为透镜，通过其隐含的高层次概念来进行对视觉模型的分析。 |
| [^110] | [KazParC: Kazakh Parallel Corpus for Machine Translation](https://arxiv.org/abs/2403.19399) | KazParC是一个跨哈萨克语、英语、俄语和土耳其语的机器翻译平行语料库，其中包含371,902个平行句子，还开发了性能优越的神经机器翻译模型Tilmash。 |
| [^111] | [Knowledge Boundary and Persona Dynamic Shape A Better Social Media Agent](https://arxiv.org/abs/2403.19275) | 通过个性化知识和动态角色信息构建社交媒体代理以解决代理拥有不属于其角色的知识和无法消除多样化角色信息干扰的问题。 |
| [^112] | [SemEval Task 1: Semantic Textual Relatedness for African and Asian Languages](https://arxiv.org/abs/2403.18933) | 这个任务涉及14种非洲和亚洲语言的语义文本相关性，旨在考察跨语言的语义相关性现象。 |
| [^113] | [Long-form factuality in large language models](https://arxiv.org/abs/2403.18802) | 该论文提出了一种通过使用大型语言模型将长篇回应分解为单个事实，并通过发送搜索查询到Google搜索，评估事实准确性的方法，并扩展了F1分数作为长篇事实性的聚合度量。 |
| [^114] | [Large Language Models for Education: A Survey and Outlook](https://arxiv.org/abs/2403.18105) | 大型语言模型在教育领域的应用调研总结了LLMs在教育中的各种技术应用，包括学生和教师辅助、自适应学习和商业工具，提出了未来研究机会和潜在方向。 |
| [^115] | [Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications](https://arxiv.org/abs/2403.17860) | 探索使用大型语言模型（LLMs）生成合成数据以减少NLP模型高置信度误分类问题的研究。 |
| [^116] | [MapGuide: A Simple yet Effective Method to Reconstruct Continuous Language from Brain Activities](https://arxiv.org/abs/2403.17516) | 本研究提出了一种直接比较预测文本嵌入的脑活动映射来指导文本重建的简单而有效方法，相比之前的间接方法显著提高了模型性能。 |
| [^117] | [Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens](https://arxiv.org/abs/2403.17407) | 通过引入区域指导标记技术，本文提出了一种将孟加拉文本与地方方言转录为国际音标的方法，为模型提供了关于输入文本的地区方言信息，以理解与每个地区相关的独特音韵模式。 |
| [^118] | [Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback](https://arxiv.org/abs/2403.16792) | 本论文提出了一种名为ProCoder的新颖方法，通过编译器反馈引导，迭代地改进项目级代码上下文，以获得精确的代码生成 |
| [^119] | [An Entropy-based Text Watermarking Detection Method](https://arxiv.org/abs/2403.13485) | 提出了一种基于熵的水印检测方法，在检测过程中根据令牌的熵调整其权重，以更好地反映水印程度，该方法在大型语言模型中具有应用潜力。 |
| [^120] | [AutoTRIZ: Artificial Ideation with TRIZ and Large Language Models](https://arxiv.org/abs/2403.13002) | 本文提出了AutoTRIZ，一种利用大型语言模型自动化和增强TRIZ方法的人工创意工具，为设计自动化和可解释创意提供了一种新颖方法。 |
| [^121] | [CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification](https://arxiv.org/abs/2403.11904) | 该论文提出了CICLe框架，基于适应上下文学习的方式，在大规模多类食品风险分类中取得了较好的效果，提出了基于符合预测的LLM-in-the-loop框架，可以提高基本分类器的性能，并减少能源消耗。 |
| [^122] | [Investigating Markers and Drivers of Gender Bias in Machine Translations](https://arxiv.org/abs/2403.11896) | 通过使用反向翻译技术，比较五种中间语言的结果，并提出新的指标评估翻译中隐含的性别偏见变化。 |
| [^123] | [Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain](https://arxiv.org/abs/2403.10576) | 利用非语言元素进行网络安全领域的预训练，提出了新的预训练方法并在网络安全领域中取得了优越表现 |
| [^124] | [Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information](https://arxiv.org/abs/2403.09516) | 通过利用预定义的典型人口统计文本并在微调过程中加入正则化项，本文提出的方法有效减轻了语言模型中的社会偏见，同时不需要依赖显式的人口统计标签。 |
| [^125] | [A Continued Pretrained LLM Approach for Automatic Medical Note Generation](https://arxiv.org/abs/2403.09057) | 这项研究提出了一种用于医疗记录生成的持续预训练LLM方法，在PubMedQA方面性能优于GPT-4，能够更好地捕捉正确的医疗概念，并且在正确性和完整性方面超过人类抄写员。 |
| [^126] | [JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models](https://arxiv.org/abs/2403.04798) | 本文介绍了针对SemEval-2024任务3开发的多模态情感因果分析系统，提出了通过两步框架解决多模态情感因果分析挑战的方法，并在实验中取得显著性能提升。 |
| [^127] | [GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability](https://arxiv.org/abs/2403.04483) | 该论文提出了一个名为GraphInstruct的基准，用于评估和增强大规模语言模型的图理解能力，并通过构建GraphLM和提出GraphLM+模型实现了显著的图推理能力增强。 |
| [^128] | [Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations](https://arxiv.org/abs/2403.02090) | 提出了三个新的具有挑战性的任务来模拟多人之间的细粒度动态，并为社交推理游戏设置提供了广泛的数据注释；同时提出了一种新颖的多模态基线方法，利用密集对齐的语言-视觉表示。 |
| [^129] | [Learning to Compress Prompt in Natural Language Formats](https://arxiv.org/abs/2402.18700) | 该研究旨在通过提出自然语言提示封装（Nano-Capsulator）框架，解决了在自然语言格式中压缩提示的挑战，以提高大型语言模型的可转移性和性能。 |
| [^130] | [All in a Single Image: Large Multimodal Models are In-Image Learners](https://arxiv.org/abs/2402.17971) | 这项研究引入了一种名为图片内学习（I$^2$L）的新型上下文学习机制，将演示示例、视觉线索和指令合并到一个图片中，以提升GPT-4V的能力，并通过整合图像处理、理解和推理的能力来取得多个优点 |
| [^131] | [Understanding the Dataset Practitioners Behind Large Language Model Development](https://arxiv.org/abs/2402.16611) | 数据质量是大型语言模型开发中数据集管理者的首要任务，但管理者间对于数据质量定义和评估方法缺乏共识。 |
| [^132] | [Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering](https://arxiv.org/abs/2402.14320) | Triad框架利用了基于多角色LLM代理来解决知识库问答问题，通过代理的不同角色分别处理KBQA子任务，合作完成KBQA任务，并在多个基准数据集上表现出色。 |
| [^133] | [Breaking the HISCO Barrier: Automatic Occupational Standardization with OccCANINE](https://arxiv.org/abs/2402.13604) | 通过OccCANINE工具，我们成功打破了HISCO障碍，实现了自动化职业标准化，从而大大简化了对职业描述的处理和分类过程，为经济学、经济历史等领域的职业结构分析提供了高效且准确的数据。 |
| [^134] | [Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism](https://arxiv.org/abs/2402.12997) | 提出了一种适用于现实约束的轻量级弃权机制，特别适用于再排序阶段，通过数据驱动的方法达到有效性，并提供了开源代码以促进其更广泛的应用。 |
| [^135] | [Measuring and Controlling Persona Drift in Language Model Dialogs](https://arxiv.org/abs/2402.10962) | 提出了一种量化基准来测量语言模型对话中的“人设”漂移，并提出了一种称为split-softmax的轻量级方法来对抗注意力衰减和“人设”漂移 |
| [^136] | [AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts](https://arxiv.org/abs/2402.07625) | 本论文介绍了一种自主数据选择策略，利用语言模型进行数学文本的自动评估和选择，并通过连续预训练显著提高了数学推理能力。主要创新包括利用元提示语言模型作为验证器，发布了高质量的AutoMathText数据集，并实现了预训练令牌效率的提升。 |
| [^137] | [FEUDA: Frustratingly Easy Prompt Based Unsupervised Domain Adaptation](https://arxiv.org/abs/2401.17514) | FEUDA是一种令人沮丧地简单的无监督领域自适应方法，通过在未标记和标记的示例上训练自回归语言模型，在提示基础的分类框架中探索无监督领域自适应的新范例。 |
| [^138] | [Navigating the Post-API Dilemma Search Engine Results Pages Present a Biased View of Social Media Data](https://arxiv.org/abs/2401.15479) | 搜索引擎结果页面可以作为社交媒体数据的替代方案，但存在对流行帖子偏见较高、情感更积极以及忽视政治、色情和粗俗帖子的问题。 |
| [^139] | [TeleChat Technical Report](https://arxiv.org/abs/2401.03804) | TeleChat是一个包含30亿、70亿和120亿参数的大型语言模型合集，旨在提供预训练语言模型和与人类喜好相一致的微调聊天模型。研究表明，TeleChat在各种任务上表现出与其他类似规模的开源模型相当的性能。 |
| [^140] | [Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding](https://arxiv.org/abs/2312.17044) | 本调查从位置编码的角度总结了用于增强Transformer长度外推能力的各种方法，包括绝对和相对位置编码以及基于它们的外推方法。 |
| [^141] | [Large Human Language Models: A Need and the Challenges](https://arxiv.org/abs/2312.07751) | 大型人类语言模型的建立需要更好地整合人类背景，并面临着如何捕捉人类因素、如何表示以及如何建模的一系列挑战。 |
| [^142] | [HALO: An Ontology for Representing and Categorizing Hallucinations in Large Language Models](https://arxiv.org/abs/2312.05209) | 本文介绍了一种名为HALO的形式化、可扩展的本体论，用OWL编写，用于描述和表示大型语言模型中的六种不同类型的幻觉。 |
| [^143] | [Large Language Models for Mathematicians](https://arxiv.org/abs/2312.04556) | 大型语言模型（LLMs）如ChatGPT因其通用语言理解的能力以及生成高质量文本或计算机代码的能力而备受关注，对数学家的潜在帮助和改变工作方式的影响进行了讨论。 |
| [^144] | [More Samples or More Prompts? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering](https://arxiv.org/abs/2311.09782) | 提出了一种低资源的LLM提示技术In-Context Sampling（ICS），通过优化多个ICL提示输入的构建来生成自信的预测，并展示了ICS可以持续增强LLM的性能，同时在数据相似性的基础上提出了新的研究方向。 |
| [^145] | [You don't need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments](https://arxiv.org/abs/2311.09718) | 评估大型语言模型在人格测量工具上的可靠性，研究探讨当前提示方式是否导致一致且稳健的响应 |
| [^146] | [Event Causality Is Key to Computational Story Understanding](https://arxiv.org/abs/2311.09648) | 我们提出了一种利用最新语言模型进展的事件因果关系识别方法，在计算故事理解方面取得了实质性进展。 |
| [^147] | [On Retrieval Augmentation and the Limitations of Language Model Training](https://arxiv.org/abs/2311.09615) | 将$k$NN检索与语言模型相结合可以降低困惑度，并提出了使用多层感知机模型的方法，将存储成本降低超过25倍。 |
| [^148] | [Efficient End-to-End Visual Document Understanding with Rationale Distillation](https://arxiv.org/abs/2311.09612) | 提出了一种称为Rationale Distillation的方法，通过合并OCR工具输出、LLMs和更大的多模态模型的中间“rationales”，并训练一个小型学生模型来预测rationales和答案，以实现对视觉文档的高效端到端理解。 |
| [^149] | [What if you said that differently?: How Explanation Formats Affect Human Feedback Efficacy and User Perception](https://arxiv.org/abs/2311.09558) | 解释格式对人类反馈效果和用户感知有重要影响，提供了研究分析如何呈现模型响应以便更易接受用户纠正的策略。 |
| [^150] | [How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities](https://arxiv.org/abs/2311.09447) | 本研究通过恶意演示在八个方面对开源LLMs的可信度进行了敌对评估，提出了一种新的攻击策略advCoU，以揭示它们的脆弱性。 |
| [^151] | [LLMRefine: Pinpointing and Refining Large Language Models via Fine-Grained Actionable Feedback](https://arxiv.org/abs/2311.09336) | LLMRefine提出了一种细粒度反馈模型来指导大型语言模型定位缺陷并进行优化，在机器翻译、长篇问答和主题总结等任务中取得显著的改进。 |
| [^152] | [Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts](https://arxiv.org/abs/2311.09066) | 社区型社交媒体平台允许用户自我披露药物相关行为，在2500个阿片类药物帖子中，我们提出了标记六种不同阶段的数据集，通过片段级摘要解释在模型发展中的关键作用，并在监督、少样本或零样本设置下评估了几种模型。 |
| [^153] | [Do Localization Methods Actually Localize Memorized Data in LLMs? A Tale of Two Benchmarks](https://arxiv.org/abs/2311.09060) | 该论文提出了两个基准来评估LLM中定位方法对记忆数据的定位能力，发现来自网络修剪的方法在两个基准上表现良好，所有评估方法均展现出有希望的定位能力。 |
| [^154] | [Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning](https://arxiv.org/abs/2311.08505) | 该论文提出了一种半结构化提示方法，通过整合语言模型的参数记忆、文本文档的非结构化知识和知识图的结构化知识，显著改善了开放域多跳问题回答任务的效果。 |
| [^155] | [The taste of IPA: Towards open-vocabulary keyword spotting and forced alignment in any language](https://arxiv.org/abs/2311.08323) | 通过构建IPAPACK语料库和提出CLAP-IPA模型，实现了针对未知语言的强大跨语言泛化能力，并在95种未知语言上展示了开放词汇匹配和强制对齐的效果 |
| [^156] | [Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision](https://arxiv.org/abs/2311.07362) | 火山模型通过自反馈引导修订的方式，有效减少多模态幻觉问题，取得了在各项评测中的最新技术水平。 |
| [^157] | [Flames: Benchmarking Value Alignment of Chinese Large Language Models](https://arxiv.org/abs/2311.06899) | 中国的大型语言模型的价值观契合性需要更加全面的评估，该研究提出了一个名为"火焰"（Flames）的基准测试，涵盖了常见的无害原则和特定中国价值观，以及复杂场景和隐含恶意的提示方法。 |
| [^158] | [Lewis's Signaling Game as beta-VAE For Natural Word Lengths and Segments](https://arxiv.org/abs/2311.04453) | Lewis's Signaling Game作为beta-VAE，通过重新制定目标函数为ELBO，阐明了 emergent languages的先验分布对于其统计特性的影响，特别是词长和分段的属性。 |
| [^159] | [Principles from Clinical Research for NLP Model Generalization](https://arxiv.org/abs/2311.03663) | 临床研究中的泛化性原则指导下，对自然语言处理模型内部验证的重要性及影响泛化的因素进行研究分析 |
| [^160] | [TopicGPT: A Prompt-based Topic Modeling Framework](https://arxiv.org/abs/2311.01449) | TopicGPT是一个基于提示的主题建模框架，使用大型语言模型来生成与人类分类更符合的并且可解释的主题，相比竞争方法在谐波纯度上有显著提升。 |
| [^161] | [The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output Neural Machine Translation](https://arxiv.org/abs/2310.20620) | 随机目标嵌入在连续输出神经机器翻译中表现出非理性有效性，尤其在较大的数据集上并且对罕见词效果最为显著。 |
| [^162] | [Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models](https://arxiv.org/abs/2310.05861) | 通过在输入中添加具有视觉基础的信息作为预防性澄清，可以提高模型性能，减少不充分性，并简化模型回答问题的方式。 |
| [^163] | [Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena](https://arxiv.org/abs/2310.05746) | LLM代理在拍卖竞技场展示出了关键的规划和执行技能，这为建模复杂社会互动在竞争背景下的LLMs潜力提供了新途径。 |
| [^164] | [Contextual Label Projection for Cross-Lingual Structure Extraction](https://arxiv.org/abs/2309.08943) | 本文提出了一种新的标签投影方法CLaP，利用上下文翻译标签，确保更准确的翻译，并在跨语言结构预测任务中取得显著表现。 |
| [^165] | [Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties](https://arxiv.org/abs/2309.00779) | 介绍了ValuePrism，一个包含218k个价值观、权利和义务，并与31k人类书写情境相联系的大规模数据集。 |
| [^166] | [PANDA: Prompt Transfer Meets Knowledge Distillation for Efficient Model Adaptation](https://arxiv.org/abs/2208.10160) | PANDA是一种新颖的PoT方法，通过引入知识蒸馏技术有效缓解知识遗忘问题。 |
| [^167] | [Constructing Cross-lingual Consumer Health Vocabulary with Word-Embedding from Comparable User Generated Content](https://arxiv.org/abs/2206.11612) | 通过使用来自可比用户生成内容的词嵌入，提出了一个跨语言自动术语识别框架，将英语消费者健康词汇扩展为跨语言词汇。 |
| [^168] | [MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer.](http://arxiv.org/abs/2401.10208) | 本文提出了MM-交错，这是一个用于交错图像-文本数据的生成模型。它通过引入多尺度和多图像特征同步器模块，解决了现有模型在捕捉图像细节方面的限制，并通过端到端预训练和监督微调相结合的方式提高了其生成能力。 |
| [^169] | [CASA: Causality-driven Argument Sufficiency Assessment.](http://arxiv.org/abs/2401.05249) | CASA是一个因果驱动的论证充分性评估框架，利用大型语言模型生成与前提和结论不一致的上下文，并通过注入前提事件对其进行修改，能够准确识别不足的论证。 |
| [^170] | [The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance.](http://arxiv.org/abs/2401.03729) | 本研究通过一系列提示变化探究改变提示的构建方式对大规模语言模型决策的影响，发现即使微小的改变，比如在提示末尾加一个空格，也可能导致模型的答案变化。同时，请求以XML格式返回和常用的越狱方式也可能对模型标记的数据产生灾难性影响。 |
| [^171] | [YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal Information Extraction.](http://arxiv.org/abs/2312.15548) | 本文提出了一个端到端的增强对话指导的通用信息抽取调优框架（YAYI-UIE），利用对话数据和信息抽取数据共同增强信息抽取性能，在中文数据集上达到了业界领先的性能，在英文数据集上也达到了可比较的性能。 |
| [^172] | [Meta Prompting for AGI Systems.](http://arxiv.org/abs/2311.11482) | 本文全面研究了元提示技术，这是一种创新方法，重塑了大型语言模型、多模态模型和人工智能系统在问题解决和数据解释方面的应用。通过强调信息的结构和句法，元提示将复杂问题拆解为简单的子问题，提高了效率，并且能够与少样本方法进行公平的比较。同时，本文还提出了元提示用于自动生成提示的方法。 |
| [^173] | [Kiki or Bouba? Sound Symbolism in Vision-and-Language Models.](http://arxiv.org/abs/2310.16781) | 这项研究通过调查视觉与语言模型中的内在知识，发现它们显示了声音象征性的模式，进一步证实声音和意义之间的相关性在跨模态关联中得到了体现。 |
| [^174] | [Large Language Models Help Humans Verify Truthfulness -- Except When They Are Convincingly Wrong.](http://arxiv.org/abs/2310.12558) | 本研究比较了语言模型与搜索引擎在帮助用户事实核查方面的效果。结果显示，用户阅读语言模型的解释比使用搜索引擎更高效，但当解释错误时，用户容易过度依赖语言模型。为了减少过度依赖，研究提出了使用对比信息进行解释的方法。 |
| [^175] | [Quantify Health-Related Atomic Knowledge in Chinese Medical Large Language Models: A Computational Analysis.](http://arxiv.org/abs/2310.11722) | 本研究通过构建一个基准，量化了中文医学大型语言模型中与健康相关的原子知识的存储程度，并发现通用LLMs在原子知识和指令遵循能力方面表现更好。两种类型的LLMs都倾向于迎合用户要求。 |
| [^176] | [Hexa: Self-Improving for Knowledge-Grounded Dialogue System.](http://arxiv.org/abs/2310.06404) | 本论文提出了一种自我提升的方法，用于改进知识驱动对话生成的中间步骤的生成性能。通过引入引导提示和修改损失函数的自举策略，提高了生成自动生成回答的多样性，并在各种基准数据集上实验证明了该方法的有效性。 |
| [^177] | [TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting.](http://arxiv.org/abs/2310.04948) | 本文提出了一个新的框架 TEMPO，通过利用时间序列任务的两个重要归纳偏差，即将复杂交互分解和引入基于选择的提示来有效学习时间序列表示。 |
| [^178] | [LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud.](http://arxiv.org/abs/2309.17157) | LatticeGen是一个协作框架，通过将真实生成的文本与噪声混合并隐藏在格子中，以保护用户的隐私。实验证明，LatticeGen能够在面对强攻击时成功保护真实生成，超过50%的语义仍然隐藏。 |
| [^179] | [Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding.](http://arxiv.org/abs/2309.15028) | 本文提出了一种基于值导向的Monte-Carlo Tree Search解码算法PPO-MCTS，通过在PPO之上集成MCTS，解决了训练和测试之间部分输出评分机制的不匹配问题，实验证明该算法可以显著提升性能。 |
| [^180] | [AceGPT, Localizing Large Language Models in Arabic.](http://arxiv.org/abs/2309.12053) | 本研究旨在开发阿拉伯文的本地化大型语言模型(AceGPT)，通过预训练、监督微调和增强学习方法来培养具备文化意识和价值观一致的阿拉伯文模型，以满足阿拉伯语社区特定应用需求。评估结果表明，AceGPT在各项基准测试中都是最先进的阿拉伯文模型。 |
| [^181] | [SLIDE: Reference-free Evaluation for Machine Translation using a Sliding Document Window.](http://arxiv.org/abs/2309.08832) | 本论文提出了一种名为SLIDE的度量方法，通过使用滑动文档窗口来评估机器翻译质量，该方法在某些情况下甚至能消除与参考度量之间的差距，表明源语言上下文可能提供了与人类参考相同的信息。 |
| [^182] | [MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response.](http://arxiv.org/abs/2309.08730) | MusiLingo是一个利用预训练的语言模型将音乐和文本相结合的系统，可以生成音乐字幕和回答音乐相关的查询。通过使用投影层对齐音乐表示，该系统成功地将音乐音频和文本环境联系起来，同时使用了一个新的数据集来推动领域的进展。 |
| [^183] | [ExpertQA: Expert-Curated Questions and Attributed Answers.](http://arxiv.org/abs/2309.07852) | 本论文介绍了ExpertQA，它是一个专家策划的问题和带有属性的答案系统。该系统通过分析语言模型在领域特定情景中提供的事实准确性和归因等方面来确保提供准确的信息。研究还收集了领域专家的问题并要求他们评估生成的答案。这项工作的目的是确保语言模型在高风险领域中不会传播错误信息，从而避免不良的社会后果。 |
| [^184] | [An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning.](http://arxiv.org/abs/2308.08747) | 该研究实证评估了大型语言模型在持续微调过程中的灾难性遗忘现象，并发现随着模型规模增加，遗忘的严重程度也加剧。与编码器-解码器模型相比，仅有解码器的模型遗忘较少并保留更多知识。此外，研究还发现LLMs可以减轻语言偏见，并且ALPACA在保留知识和容量方面具有优势。 |
| [^185] | [LLM-Rec: Personalized Recommendation via Prompting Large Language Models.](http://arxiv.org/abs/2307.15780) | 本文通过引导大型语言模型进行个性化推荐的研究，提出了四种不同的引导策略，并通过实验证明了这些策略的有效性。这一发现强调了在个性化内容推荐中，采用多样的引导和输入增强技术可以提高大型语言模型的推荐性能。 |
| [^186] | [Instruction-following Evaluation through Verbalizer Manipulation.](http://arxiv.org/abs/2307.10558) | 本文提出了一种新的指示遵循评估协议，口述者操作，通过口述任务标签来检查模型对先验知识的依赖程度，以及覆盖它们的能力，从而准确地进行指示遵循。 |
| [^187] | [Visually-Grounded Descriptions Improve Zero-Shot Image Classification.](http://arxiv.org/abs/2306.06077) | 本文提出了一种称为V-GLOSS的新方法，它利用现代语言模型和语义知识库生成具有视觉基础的类别描述，提高了零样本图像分类的准确性，并引入了一个带有类别描述的银标准数据集。 |
| [^188] | [This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models.](http://arxiv.org/abs/2305.14610) | 本文提出了地缘政治偏见的概念，并以领土争端为例，利用多语言、多选题的数据集BorderLines和几个定量指标分析语言模型响应中的地缘政治偏见现象。 |
| [^189] | [PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences.](http://arxiv.org/abs/2305.02547) | 本文探究了基于LLMs模拟代理的行为，称之为LLM Personas，在分配大五人格类型和性别角色时是否可以生成具有一致性的个性化特质的内容。 |
| [^190] | [New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT.](http://arxiv.org/abs/2305.01181) | 本文提出了使用大型语言模型的机器翻译中的几个新方向，包括风格化MT、交互式MT和基于翻译记忆的MT，并讨论了隐私问题的解决方案。 |
| [^191] | [Using large language models for (de-)formalization and natural argumentation exercises for beginner's students.](http://arxiv.org/abs/2304.06186) | 本研究描述了两个系统，利用大型语言模型，自动纠正初学者在逻辑语言转化和自然语言论证方面的问题。 |
| [^192] | [Improving the Diproche CNL through autoformalization via GPT-3.](http://arxiv.org/abs/2303.17513) | 本文探讨了在Diproche上使用大型语言模型进行自动形式化的可能性，并取得了令人鼓舞的初步结果。 |
| [^193] | [Innovations in Neural Data-to-text Generation: A Survey.](http://arxiv.org/abs/2207.12571) | 本文综述了过去十年神经数据生成文本（DTG）领域的创新，将其与其他自然语言生成技术（NLG）区分开来，并强调了关注系统语言能力和公平、公正的DTG研究前景。 |

# 详细

[^1]: Pok\'eLLMon：一个用于使用大型语言模型的Pok\'emon对战的与人类能力相当的代理机器人

    Pok\'eLLMon: A Human-Parity Agent for Pok\'emon Battles with Large Language Models

    [https://rss.arxiv.org/abs/2402.01118](https://rss.arxiv.org/abs/2402.01118)

    Pok\'eLLMon是第一个在战术对战游戏中实现人类能力的语言模型化身代理机器人。它通过上下文强化学习、知识增强生成和一致的行动生成的策略，展现了与人类类似的战斗策略和及时决策，并在Ladder比赛中达到了49%的胜率，在邀请对战中达到了56%的胜率。

    

    我们介绍了\textsc{Pok\'eLLMon}，这是第一个在战术对战游戏中实现人类能力的语言模型化身代理机器人，同时以Pok\'emon对战为例进行了证明。 \textsc{Pok\'eLLMon}的设计采用了三个关键策略：（i）上下文强化学习，即即时使用从对战中获得的基于文本的反馈来逐步完善策略；（ii）知识增强生成，即检索外部知识以对抗产生幻觉现象，并使代理机器人能够及时正确地行动；（iii）一致的行动生成，以减轻代理机器人面对强敌时的“惊慌换手”现象，使其可以逃避战斗。我们展示了与人类进行的在线对战中，\textsc{Pok\'eLLMon}采用与人类类似的战斗策略和及时决策，其在Ladder比赛中达到了49%的胜率，在邀请对战中达到了56%的胜率。我们的实现和可玩的战斗日志可以在以下链接中找到：\url{https://gith

    We introduce \textsc{Pok\'eLLMon}, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pok\'emon battles. The design of \textsc{Pok\'eLLMon} incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the \textit{panic switching} phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates \textsc{Pok\'eLLMon}'s human-like battle strategies and just-in-time decision making, achieving 49\% of win rate in the Ladder competitions and 56\% of win rate in the invited battles. Our implementation and playable battle logs are available at: \url{https://gith
    
[^2]: 基于主题的LLM生成文本的水印

    Topic-based Watermarks for LLM-Generated Text

    [https://arxiv.org/abs/2404.02138](https://arxiv.org/abs/2404.02138)

    提出了一种新的基于主题的水印算法，旨在解决当前水印方案的局限性，为区分LLM生成的文本和人类生成的文本提供了新的思路。

    

    大型语言模型（LLMs）的最新进展导致了生成的文本输出与人类生成的文本相似度难以分辨。水印算法是潜在工具，通过在LLM生成的输出中嵌入可检测的签名，可以区分LLM生成的文本和人类生成的文本。然而，当前的水印方案在已知攻击下缺乏健壮性。此外，考虑到LLM每天生成数万个文本输出，水印算法需要记忆每个输出才能让检测正常工作，这是不切实际的。本文针对当前水印方案的局限性，提出了针对LLMs的“基于主题的水印算法”概念。

    arXiv:2404.02138v1 Announce Type: cross  Abstract: Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a "topic-based watermarking algorithm" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked 
    
[^3]: FLawN-T5: 有效指导调整数据混合在法律推理中的实证研究

    FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning

    [https://arxiv.org/abs/2404.02127](https://arxiv.org/abs/2404.02127)

    本研究提出了一个名为LawInstruct的大型法律指导数据集，证明了领域特定的预训练和指导调整可以改善在LegalBench上的性能，为在法律领域开发具有更强信息处理和决策能力的模型提供了一个资源。

    

    arXiv:2404.02127v1  公告类型: 跨领域  摘要: 指导调整是使语言模型对直接用户交互有效的重要步骤。然而，许多法律任务仍然超出了大多数开放式LLMs的范围，而且目前该领域还没有任何大规模的数据集。这严重限制了该应用领域的研究。在这项工作中，我们策划了一个名为LawInstruct的大型法律指导数据集，涵盖了17个司法管辖区、24种语言，总计1200万个示例。我们呈现证据表明，领域特定的预训练和指导调整能够改善在LegalBench上的性能，包括将Flan-T5 XL在基准线上提高8个点或16%。然而，该效应并不适用于所有任务、训练模式、模型大小和其他因素。LawInstruct是一个资源，可以加速在法律领域开发具有更强信息处理和决策能力的模型。

    arXiv:2404.02127v1 Announce Type: cross  Abstract: Instruction tuning is an important step in making language models useful for direct user interaction. However, many legal tasks remain out of reach for most open LLMs and there do not yet exist any large scale instruction datasets for the domain. This critically limits research in this application area. In this work, we curate LawInstruct, a large legal instruction dataset, covering 17 jurisdictions, 24 languages and a total of 12M examples. We present evidence that domain-specific pretraining and instruction tuning improve performance on LegalBench, including improving Flan-T5 XL by 8 points or 16\% over the baseline. However, the effect does not generalize across all tasks, training regimes, model sizes, and other factors. LawInstruct is a resource for accelerating the development of models with stronger information processing and decision making capabilities in the legal domain.
    
[^4]: 重新比赛：改进本地知识图的鲁棒和高效匹配以提高结构和语义相似性

    Rematch: Robust and Efficient Matching of Local Knowledge Graphs to Improve Structural and Semantic Similarity

    [https://arxiv.org/abs/2404.02126](https://arxiv.org/abs/2404.02126)

    引入一种新的AMR相似度度量rematch以提高结构和语义相似性，结构相似性排名第二，语义相似性最优，快五倍于下一个最高效度量。

    

    知识图在各种应用中发挥着关键作用，例如问答和事实核查。抽象意义表示（AMR）将文本表示为知识图。评估这些图的质量涉及将它们结构化匹配到彼此和语义匹配到源文本。现有的AMR度量效率低，难以捕捉语义相似性。我们也缺乏一个系统性的评估基准，用于评估AMR图之间的结构相似性。为了克服这些限制，我们引入了一种新的AMR相似度度量rematch，以及一种用于评估结构相似性的新评估方法RARE。在最先进的度量中，rematch在结构相似性中排名第二；并且在STS-B和SICK-R基准上的语义相似性最高，与下一个最高效度量相比快五倍。

    arXiv:2404.02126v1 Announce Type: new  Abstract: Knowledge graphs play a pivotal role in various applications, such as question-answering and fact-checking. Abstract Meaning Representation (AMR) represents text as knowledge graphs. Evaluating the quality of these graphs involves matching them structurally to each other and semantically to the source text. Existing AMR metrics are inefficient and struggle to capture semantic similarity. We also lack a systematic evaluation benchmark for assessing structural similarity between AMR graphs. To overcome these limitations, we introduce a novel AMR similarity metric, rematch, alongside a new evaluation for structural similarity called RARE. Among state-of-the-art metrics, rematch ranks second in structural similarity; and first in semantic similarity by 1--5 percentage points on the STS-B and SICK-R benchmarks. Rematch is also five times faster than the next most efficient metric.
    
[^5]: 通过大型语言模型探索数学多项选择题的自动生成干扰项

    Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models

    [https://arxiv.org/abs/2404.02124](https://arxiv.org/abs/2404.02124)

    通过大型语言模型探索数学多项选择题的自动生成干扰项，发现虽然LLMs可以生成一些数学上有效的干扰项，但在预测常见错误或误解方面表现不佳

    

    多项选择题在几乎所有教育层次中都是普遍存在的，因为它们易于管理、评分，并且是评估和实践中可靠的格式。其中最重要的方面之一是干扰项，即针对真实学生常见错误或误解而设计的不正确选项。目前，制作高质量干扰项的任务在很大程度上仍然是教师和学习内容设计者的劳动和耗时工作，这限制了可扩展性。在这项工作中，我们研究了在数学多项选择题领域中自动生成干扰项的任务，并探索了各种基于大型语言模型（LLM）的方法，从上下文学习到微调。我们使用真实数学多项选择题数据集进行了大量实验，发现虽然LLM可以生成一些数学上有效的干扰项，但它们在预测常见错误或误解方面表现不佳。

    arXiv:2404.02124v1 Announce Type: new  Abstract: Multiple-choice questions (MCQs) are ubiquitous in almost all levels of education since they are easy to administer, grade, and are a reliable format in assessments and practices. One of the most important aspects of MCQs is the distractors, i.e., incorrect options that are designed to target common errors or misconceptions among real students. To date, the task of crafting high-quality distractors largely remains a labor and time-intensive process for teachers and learning content designers, which has limited scalability. In this work, we study the task of automated distractor generation in the domain of math MCQs and explore a wide variety of large language model (LLM)-based approaches, from in-context learning to fine-tuning. We conduct extensive experiments using a real-world math MCQ dataset and find that although LLMs can generate some mathematically valid distractors, they are less adept at anticipating common errors or misconcept
    
[^6]: GINopic：利用图同构网络进行主题建模

    GINopic: Topic Modeling with Graph Isomorphism Network

    [https://arxiv.org/abs/2404.02115](https://arxiv.org/abs/2404.02115)

    GINopic是一种主题建模框架，利用图同构网络捕捉单词之间的相关性，相比于现有主题模型，展示了更好的有效性和推进主题建模的潜力。

    

    主题建模是分析和探索大型文档集合的广泛使用方法。 最近的研究工作将预训练的上下文化语言模型，如BERT嵌入，纳入主题建模中。 然而，它们通常忽略了单词之间相互依赖传达的固有信息价值。 本研究介绍了GINopic，一种基于图同构网络的主题建模框架，以捕捉单词之间的相关性。 通过在不同基准数据集上进行内在的（定量和定性）和外部的评估，我们展示了与现有主题模型相比，GINopic的有效性，并突出了其推进主题建模的潜力。

    arXiv:2404.02115v1 Announce Type: new  Abstract: Topic modeling is a widely used approach for analyzing and exploring large document collections. Recent research efforts have incorporated pre-trained contextualized language models, such as BERT embeddings, into topic modeling. However, they often neglect the intrinsic informational value conveyed by mutual dependencies between words. In this study, we introduce GINopic, a topic modeling framework based on graph isomorphism networks to capture the correlation between words. By conducting intrinsic (quantitative as well as qualitative) and extrinsic evaluations on diverse benchmark datasets, we demonstrate the effectiveness of GINopic compared to existing topic models and highlight its potential for advancing topic modeling.
    
[^7]: CLAPNQ：自然问答中的段落内一致长格式答案适用于RAG系统

    CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems

    [https://arxiv.org/abs/2404.02103](https://arxiv.org/abs/2404.02103)

    提出了ClapNQ，一个用于完整RAG管道的基准长格式问答数据集，要求RAG模型能适应包括简洁、一致和不连续段落片段的答案特性。

    

    检索增强生成（RAG）已成为大型语言模型的热门应用。成功的RAG系统最好提供由段落支持且没有错觉的准确答案。为了构建完整的RAG管道，需要开展大量工作，同时也需要能够对性能进行基准测试。我们提出了ClapNQ，一个用于完整RAG管道的基准长格式问答数据集。ClapNQ包括具有自然问题（NQ）中基于段落的金标段落的长答案，以及一个用于执行检索、生成或完整RAG管道的语料库。ClapNQ的答案简洁，比完整段落小3倍，并且一致，包含不连续的多个段落片段。RAG模型必须适应这些特性才能在ClapNQ上取得成功。我们为ClapNQ提出了基线实验和分析，突出了仍有显著挑战的领域。

    arXiv:2404.02103v1 Announce Type: new  Abstract: Retrieval Augmented Generation (RAG) has become a popular application for large language models. It is preferable that successful RAG systems provide accurate answers that are supported by being grounded in a passage without any hallucinations. While considerable work is required for building a full RAG pipeline, being able to benchmark performance is also necessary. We present ClapNQ, a benchmark Long-form Question Answering dataset for the full RAG pipeline. ClapNQ includes long answers with grounded gold passages from Natural Questions (NQ) and a corpus to perform either retrieval, generation, or the full RAG pipeline. The ClapNQ answers are concise, 3x smaller than the full passage, and cohesive, with multiple pieces of the passage that are not contiguous. RAG models must adapt to these properties to be successful at ClapNQ. We present baseline experiments and analysis for ClapNQ that highlight areas where there is still significant 
    
[^8]: 在SemEval-2024任务3中的LastResort: 探索作为序列标记任务的多模态情感因果对提取

    LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause Pair Extraction as Sequence Labelling Task

    [https://arxiv.org/abs/2404.02088](https://arxiv.org/abs/2404.02088)

    本文提出了针对SemEval 2024引入的多模态情感原因分析任务的模型，将其作为话语标记和序列标记问题来解决，并进行了比较研究。

    

    arXiv:2404.02088v1 通告类型: 新的 摘要: 对话是最自然的人类交流形式，每个话语可以涵盖多种可能的情感。虽然在文本中检测情感方面已经做了大量工作，但在找出所述情感的原因方面做的工作相对较少，特别是在多模态环境中。SemEval 2024引入了对话中的多模态情感原因分析任务，旨在提取反映在涉及多模态（文本、音频和视觉模态）的对话中个别话语中的情感，以及导致该情感的相应话语。本文提出了模型，将这一任务作为话语标记和序列标记问题来解决，并对这些模型进行了比较研究，包括使用不同编码器的基线，使用BiLSTM添加对话的上下文信息，最后添加CRF层。

    arXiv:2404.02088v1 Announce Type: new  Abstract: Conversation is the most natural form of human communication, where each utterance can range over a variety of possible emotions. While significant work has been done towards the detection of emotions in text, relatively little work has been done towards finding the cause of the said emotions, especially in multimodal settings. SemEval 2024 introduces the task of Multimodal Emotion Cause Analysis in Conversations, which aims to extract emotions reflected in individual utterances in a conversation involving multiple modalities (textual, audio, and visual modalities) along with the corresponding utterances that were the cause for the emotion. In this paper, we propose models that tackle this task as an utterance labeling and a sequence labeling problem and perform a comparative study of these models, involving baselines using different encoders, using BiLSTM for adding contextual information of the conversation, and finally adding a CRF la
    
[^9]: 通过首选树推进LLM推理通用性

    Advancing LLM Reasoning Generalists with Preference Trees

    [https://arxiv.org/abs/2404.02078](https://arxiv.org/abs/2404.02078)

    新推出的Eurus模型通过基于首选树的推理优化，在多项基准测试中取得了业界领先的成果，尤其在击败了GPT-3.5 Turbo的基准测试中表现突出。

    

    我们介绍了Eurus，一套专为推理优化的大语言模型（LLMs）。经过Mistral-7B和CodeLlama-70B的微调，Eurus模型在涵盖数学、代码生成和逻辑推理问题的多样化基准测试中取得了业界领先的成果。值得注意的是，Eurus-70B在通过涵盖五项任务的12个测试的全面基准测试中击败了GPT-3.5 Turbo，并在LeetCode和TheoremQA两个具有挑战性的基准测试中分别实现了33.3%和32.6%的pass@1准确率，明显优于现有开源模型超过13.3%的边际。Eurus的强大性能主要归功于我们新设计的大规模、高质量对齐数据集UltraInteract，该数据集专门为复杂推理任务而设计。UltraInteract可用于监督微调和首选学习。对于每个指令，它包括一个首选树。

    arXiv:2404.02078v1 Announce Type: new  Abstract: We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of Eurus can be primarily attributed to UltraInteract, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. UltraInteract can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting o
    
[^10]: 使用解释方法增强模型

    Using Interpretation Methods for Model Enhancement

    [https://arxiv.org/abs/2404.02068](https://arxiv.org/abs/2404.02068)

    提出了利用解释方法和黄金解释来增强模型的框架，并展示在低资源情况下特别有效。

    

    在神经自然语言处理的时代，有许多作品试图推导神经模型的解释。直观地，当在训练过程中存在黄金解释时，可以额外训练模型使其与解释相匹配。然而，这种直观的想法尚未得到充分探讨。本文提出了一种利用解释方法和黄金解释来增强模型的框架。我们的框架在很大程度上是通用的，可以整合各种解释方法。先前提出的基于梯度的方法可以被证明是我们框架的一个实例。我们还提出了利用另外两种类型的解释方法，即擦除/替换型和提取型方法，用于增强模型的两个新颖实例。我们在各种任务上进行了全面实验。实验结果表明，我们的框架在低资源情况下尤其有效。

    arXiv:2404.02068v1 Announce Type: new  Abstract: In the age of neural natural language processing, there are plenty of works trying to derive interpretations of neural models. Intuitively, when gold rationales exist during training, one can additionally train the model to match its interpretation with the rationales. However, this intuitive idea has not been fully explored. In this paper, we propose a framework of utilizing interpretation methods and gold rationales to enhance models. Our framework is very general in the sense that it can incorporate various interpretation methods. Previously proposed gradient-based methods can be shown as an instance of our framework. We also propose two novel instances utilizing two other types of interpretation methods, erasure/replace-based and extractor-based methods, for model enhancement. We conduct comprehensive experiments on a variety of tasks. Experimental results show that our framework is effective especially in low-resource settings in en
    
[^11]: 长文本语言模型在长上下文学习中遇到困难

    Long-context LLMs Struggle with Long In-context Learning

    [https://arxiv.org/abs/2404.02060](https://arxiv.org/abs/2404.02060)

    该研究引入了一个专门的基准 LIConBench，聚焦于长上下文学习，发现长文本语言模型在极端标签分类领域中性能良好，尤其在标记长度不超过20K时表现相对较好。

    

    大型语言模型（LLMs）在处理超过32K标记的长序列方面取得了重大进展。然而，它们的性能评估主要局限在困惑度和合成任务等指标上，这可能无法充分捕捉它们在更微妙的现实场景中的能力。本研究引入了一个专门的基准（LIConBench），着重于长上下文学习，在极端标签分类领域。我们精心选择了六个数据集，其标签范围跨度为28至174类，涵盖了从2K到50K的不同输入（少量演示）长度。我们的基准要求LLMs理解整个输入，以识别庞大的标签空间以进行正确预测。我们在我们的基准上评估了13个长上下文LLMs。我们发现长上下文LLMs在标记长度为20K以下时表现相对较好，并且利用长上下文窗口会带来性能上的好处。

    arXiv:2404.02060v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have made significant strides in handling long sequences exceeding 32K tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their abilities in more nuanced, real-world scenarios. This study introduces a specialized benchmark (LIConBench) focusing on long in-context learning within the realm of extreme-label classification. We meticulously selected six datasets with a label range spanning 28 to 174 classes covering different input (few-shot demonstration) length from 2K to 50K. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct prediction. We evaluate 13 long-context LLMs on our benchmarks. We find that the long-context LLMs perform relatively well under the token length of 20K and the performance benefits from utilizing the long context window. However,
    
[^12]: 拆解上下文学习: 通过破坏理解提示

    Deconstructing In-Context Learning: Understanding Prompts via Corruption

    [https://arxiv.org/abs/2404.02054](https://arxiv.org/abs/2404.02054)

    大型语言模型的能力在上下文中学习已经导致AI助手的急剧增长，其鲁棒性部分归因于对齐技术，然而这些助手使用的预训练模型在这方面却较为脆弱，构建高质量的骨干模型仍然是一个挑战。

    

    大型语言模型（LLMs）根据提供的提示“在上下文中学习”的能力已经导致它们的使用数量急剧增长，最终导致AI助手如ChatGPT、Claude和Bard的大量出现。这些AI助手被认为对提示的轻微修改具有鲁棒性，主要是由于使用了人类反馈的对齐技术。相比之下，它们使用作为骨干的基础预训练LLMs被认为在这方面比较脆弱。构建高质量的骨干模型仍然是一个核心挑战，评估其质量的常见方法是进行少样本评估。这种评估以对轻微提示修改和特定上下文示例选择的高度敏感而臭名昭著。先前的研究已经考察了修改提示的不同元素如何影响模型性能。然而，这些较早的研究往往集中在有限数量的具体提示上。

    arXiv:2404.02054v1 Announce Type: new  Abstract: The ability of large language models (LLMs) to "learn in context" based on the provided prompt has led to an explosive growth in their use, culminating in the proliferation of AI assistants such as ChatGPT, Claude, and Bard. These AI assistants are known to be robust to minor prompt modifications, mostly due to alignment techniques that use human feedback. In contrast, the underlying pre-trained LLMs they use as a backbone are known to be brittle in this respect. Building high-quality backbone models remains a core challenge, and a common approach to assessing their quality is to conduct few-shot evaluation. Such evaluation is notorious for being highly sensitive to minor prompt modifications, as well as the choice of specific in-context examples. Prior work has examined how modifying different elements of the prompt can affect model performance. However, these earlier studies tended to concentrate on a limited number of specific prompt 
    
[^13]: BERTopic驱动的股市预测：解析情感洞见

    BERTopic-Driven Stock Market Predictions: Unraveling Sentiment Insights

    [https://arxiv.org/abs/2404.02053](https://arxiv.org/abs/2404.02053)

    这项研究利用BERTopic分析股市评论中的情感，整合深度学习模型，显示情感分析显著提升了股市预测性能，揭示了NLP在丰富金融分析方面的潜力。

    

    这篇论文探讨了自然语言处理（NLP）和金融分析的交叉领域，重点关注情感分析在股价预测中的影响。我们采用了BERTopic，一种先进的NLP技术，分析从股市评论中得出的主题的情感。我们的方法将这种情感分析与各种深度学习模型相整合，这些模型以其在时间序列和股票预测任务中的有效性而闻名。通过全面的实验，我们证明了整合主题情感显著提升了这些模型的性能。结果表明，股市评论中的主题提供了对股市波动和价格趋势的隐含、有价值的洞见。这项研究通过展示NLP在丰富金融分析方面的潜力，为实时情感分析和探索情感和情景相关性打开了研究途径。

    arXiv:2404.02053v1 Announce Type: new  Abstract: This paper explores the intersection of Natural Language Processing (NLP) and financial analysis, focusing on the impact of sentiment analysis in stock price prediction. We employ BERTopic, an advanced NLP technique, to analyze the sentiment of topics derived from stock market comments. Our methodology integrates this sentiment analysis with various deep learning models, renowned for their effectiveness in time series and stock prediction tasks. Through comprehensive experiments, we demonstrate that incorporating topic sentiment notably enhances the performance of these models. The results indicate that topics in stock market comments provide implicit, valuable insights into stock market volatility and price trends. This study contributes to the field by showcasing the potential of NLP in enriching financial analysis and opens up avenues for further research into real-time sentiment analysis and the exploration of emotional and contextua
    
[^14]: 乌克兰文本分类：跨语言知识传递方法的探索

    Ukrainian Texts Classification: Exploration of Cross-lingual Knowledge Transfer Approaches

    [https://arxiv.org/abs/2404.02043](https://arxiv.org/abs/2404.02043)

    乌克兰文本分类领域探索跨语言知识传递方法，利用最新的NLP技术，测试了在毒性分类、文体分类和自然语言推理任务上的最佳设置。

    

    虽然在自然语言处理文本分类领域存在大量标记数据集，但各种语言可用数据的不平衡问题依然显而易见。乌克兰语作为一种仍可从跨语言方法的持续完善中受益的语言。鉴于我们所了解，针对典型文本分类任务，乌克兰语语料库极度匮乏。在这项工作中，我们利用自然语言处理领域的最新进展，探索跨语言知识传递方法，避免手动数据整理：大型多语言编码器和翻译系统、LLMs，以及语言适配器。我们在三个文本分类任务上测试这些方法--毒性分类、文体分类和自然语言推理--提供了最佳设置的"配方"。

    arXiv:2404.02043v1 Announce Type: cross  Abstract: Despite the extensive amount of labeled datasets in the NLP text classification field, the persistent imbalance in data availability across various languages remains evident. Ukrainian, in particular, stands as a language that still can benefit from the continued refinement of cross-lingual methodologies. Due to our knowledge, there is a tremendous lack of Ukrainian corpora for typical text classification tasks. In this work, we leverage the state-of-the-art advances in NLP, exploring cross-lingual knowledge transfer methods avoiding manual data curation: large multilingual encoders and translation systems, LLMs, and language adapters. We test the approaches on three text classification tasks -- toxicity classification, formality classification, and natural language inference -- providing the "recipe" for the optimal setups.
    
[^15]: MultiParaDetox：将文本净化与并行数据扩展到新语言

    MultiParaDetox: Extending Text Detoxification with Parallel Data to New Languages

    [https://arxiv.org/abs/2404.02037](https://arxiv.org/abs/2404.02037)

    本研究提出了MultiParaDetox，将ParaDetox管道扩展到多种语言，以自动化收集潜在任何语言的并行净化语料库。

    

    文本净化是一项文本风格转换（TST）任务，其中文本被从有毒的表面形式，例如包含粗鲁词汇，转述为中性语体。最近，文本净化方法在各种任务中找到了应用，比如净化大型语言模型（LLMs）（Leong等，2023；He等，2024；Tang等，2023）以及在社交网络中对抗有毒言论（Deng等，2023；Mun等，2023；Agarwal等，2023）。所有这些应用对于确保现代数字世界中的安全沟通至关重要。然而，以前用于并行文本净化语料库收集的方法-- ParaDetox（Logacheva等，2022）和APPADIA（Atwell等，2022）-- 仅在单语言设置中进行了探讨。在本研究中，我们旨在将ParaDetox流程扩展到多种语言，提出MultiParaDetox以自动化潜在任何语言的并行净化语料库收集。

    arXiv:2404.02037v1 Announce Type: cross  Abstract: Text detoxification is a textual style transfer (TST) task where a text is paraphrased from a toxic surface form, e.g. featuring rude words, to the neutral register. Recently, text detoxification methods found their applications in various task such as detoxification of Large Language Models (LLMs) (Leong et al., 2023; He et al., 2024; Tang et al., 2023) and toxic speech combating in social networks (Deng et al., 2023; Mun et al., 2023; Agarwal et al., 2023). All these applications are extremely important to ensure safe communication in modern digital worlds. However, the previous approaches for parallel text detoxification corpora collection -- ParaDetox (Logacheva et al., 2022) and APPADIA (Atwell et al., 2022) -- were explored only in monolingual setup. In this work, we aim to extend ParaDetox pipeline to multiple languages presenting MultiParaDetox to automate parallel detoxification corpus collection for potentially any language. 
    
[^16]: 优化向量化上下文的检索增强开放领域问答

    Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts

    [https://arxiv.org/abs/2404.02022](https://arxiv.org/abs/2404.02022)

    本文提出一种通用且便利的方法，通过利用小型编码器语言模型和交叉注意力，使原始语言模型可以覆盖更长的上下文，从而提高开放领域问答任务的性能。

    

    在大型语言模型时代，应用检索增强生成等技术可以更好地解决开放领域问答问题。由于模型大小和计算资源等约束，上下文长度通常受限，让模型覆盖过长的上下文并回答来自开放领域的问题变得具有挑战性。本文提出了一种在开放领域问答任务中覆盖更长上下文的通用、方便方法。它利用一个小型编码器语言模型有效编码上下文，并对原始输入应用交叉注意力。通过我们的方法，原始语言模型可以覆盖几倍长的上下文，同时保持与基线接近的计算需求。我们的实验表明，在微调后，性能在两个保存的数据集、四个保留的数据集以及两个In Context

    arXiv:2404.02022v1 Announce Type: new  Abstract: In the era of large language models, applying techniques such as Retrieval Augmented Generation can better address Open-Domain Question-Answering problems. Due to constraints including model sizes and computing resources, the length of context is often limited, and it becomes challenging to empower the model to cover overlong contexts while answering questions from open domains. This paper proposes a general and convenient method to covering longer contexts in Open-Domain Question-Answering tasks. It leverages a small encoder language model that effectively encodes contexts, and the encoding applies cross-attention with origin inputs. With our method, the origin language models can cover several times longer contexts while keeping the computing requirements close to the baseline. Our experiments demonstrate that after fine-tuning, there is improved performance across two held-in datasets, four held-out datasets, and also in two In Contex
    
[^17]: 打破沉默：检测和缓解印地语、泰米尔语和印度英语在线空间中的性别虐待

    Breaking the Silence Detecting and Mitigating Gendered Abuse in Hindi, Tamil, and Indian English Online Spaces

    [https://arxiv.org/abs/2404.02013](https://arxiv.org/abs/2404.02013)

    在该研究中，我们开发了一种结合了CNN和BiLSTM网络的集成方法，有效模拟了文本数据中的语义和顺序模式，用于检测和缓解印地语、泰米尔语和印度英语在线空间中的性别虐待。

    

    在线性别骚扰是一个广泛存在的问题，限制了女性和边缘性别在数字空间中的自由表达和参与。检测这类滥用内容可以帮助平台遏制这一祸害。我们参加了 ICON2023 的Indic语言中的性别虐待检测的共享任务，该任务提供了在英语、印地语和泰米尔语中进行分类以识别性别虐待的Twitter帖子的标注数据集。我们的团队CNLP-NITS-PP开发了一个结合了CNN和BiLSTM网络的集成方法，可以有效地对文本数据中的语义和顺序模式进行建模。卷积滤波器在嵌入输入文本上应用，CNN捕获指示滥用语言的局部特征。为了确定基于上下文的冒犯性，BiLSTM分析这个序列以了解单词和短语之间的依赖关系。为每种语言使用了FastText和GloVe词嵌入进行了多个变体的训练。

    arXiv:2404.02013v1 Announce Type: new  Abstract: Online gender-based harassment is a widespread issue limiting the free expression and participation of women and marginalized genders in digital spaces. Detecting such abusive content can enable platforms to curb this menace. We participated in the Gendered Abuse Detection in Indic Languages shared task at ICON2023 that provided datasets of annotated Twitter posts in English, Hindi and Tamil for building classifiers to identify gendered abuse. Our team CNLP-NITS-PP developed an ensemble approach combining CNN and BiLSTM networks that can effectively model semantic and sequential patterns in textual data. The CNN captures localized features indicative of abusive language through its convolution filters applied on embedded input text. To determine context-based offensiveness, the BiLSTM analyzes this sequence for dependencies among words and phrases. Multiple variations were trained using FastText and GloVe word embeddings for each languag
    
[^18]: 用Wolof语对话的语音机器人的概念证明

    Preuve de concept d'un bot vocal dialoguant en wolof

    [https://arxiv.org/abs/2404.02009](https://arxiv.org/abs/2404.02009)

    这个论文介绍了在塞内加尔使用的主要通用语言Wolof中建立的第一个自动语音助手的概念证明，旨在为Orange客户提供关于Orange Senegal的Sargal忠诚计划信息，取得了一定的初步成功。

    

    这篇论文介绍了在塞内加尔使用的主要通用语言Wolof中建立的第一个自动语音助手的概念证明。该语音机器人是法国Orange Innovation、塞内加尔Orange（又名Sonatel）和总部位于塞内加尔达喀尔的小型IT公司ADNCorp之间合作研究项目的结果。这个语音机器人的目的是向Orange客户提供关于Orange Senegal的Sargal忠诚计划的信息，通过使用最自然的交流方式：语音。语音机器人接收客户的口头请求作为输入，然后通过SLU系统处理这些请求，使用音频记录来回复客户。该概念证明的初步结果是令人鼓舞的，我们在ASR任务上达到了22%的WER，NLU任务上达到了78%的F1分数。

    arXiv:2404.02009v1 Announce Type: new  Abstract: This paper presents the proof-of-concept of the first automatic voice assistant ever built in Wolof language, the main vehicular language spoken in Senegal. This voicebot is the result of a collaborative research project between Orange Innovation in France, Orange Senegal (aka Sonatel) and ADNCorp, a small IT company based in Dakar, Senegal. The purpose of the voicebot is to provide information to Orange customers about the Sargal loyalty program of Orange Senegal by using the most natural mean to communicate: speech. The voicebot receives in input the customer's oral request that is then processed by a SLU system to reply to the customer's request using audio recordings. The first results of this proof-of-concept are encouraging as we achieved 22\% of WER for the ASR task and 78\% of F1-score on the NLU task.
    
[^19]: 非洲中心自监督预训练技术在撒哈拉以南地区的多语言语音表征中的应用

    Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context

    [https://arxiv.org/abs/2404.02000](https://arxiv.org/abs/2404.02000)

    这项研究提出了首个仅在非洲语音上进行训练的自监督多语言语音模型，相比于常规方法，更高效并在ASR和LID任务中表现出竞争力。

    

    我们提出了第一个仅在非洲语音上进行训练的自监督多语言语音模型。该模型从撒哈拉以南非洲地区讲话的21种语言和方言中学习了近60,000小时的未标记语音片段。在FLEURS-102数据集的SSA子集上，我们基于HuBERT$_{base}$ (0.09B) 架构的方法展现出了具有竞争力的结果，与FLEURS基准提出的w2v-bert-51 (0.6B) 预训练模型相比，在ASR下游任务中更加高效，使用的数据量少7倍，参数少6倍。此外，在LID下游任务中，我们的方法在准确率上超过FLEURS基线超过22%。

    arXiv:2404.02000v1 Announce Type: new  Abstract: We present the first self-supervised multilingual speech model trained exclusively on African speech. The model learned from nearly 60 000 hours of unlabeled speech segments in 21 languages and dialects spoken in sub-Saharan Africa. On the SSA subset of the FLEURS-102 dataset, our approach based on a HuBERT$_{base}$ (0.09B) architecture shows competitive results, for ASR downstream task, compared to the w2v-bert-51 (0.6B) pre-trained model proposed in the FLEURS benchmark, while being more efficient by using 7x less data and 6x less parameters. Furthermore, in the context of a LID downstream task, our approach outperforms FLEURS baselines accuracy by over 22\%.
    
[^20]: DELAN: 通过跨模态对比学习实现视觉与语言导航的双层对齐

    DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning

    [https://arxiv.org/abs/2404.01994](https://arxiv.org/abs/2404.01994)

    通过跨模态对比学习实现的DELAN框架提出了双层对齐方法，以提升视觉与语言导航中的跨模态交互和行动决策。

    

    arXiv:2404.01994v1 公告类型: 交叉  摘要: 视觉与语言导航(VLN)要求代理根据自然语言指示在未知环境中导航。为了完成任务，代理需要对齐和整合各种导航模态，包括指示、观察和导航历史。现有研究主要集中在融合阶段的跨模态注意力，以实现这一目标。然而，由不同单一编码器生成的模态特征位于各自的空间中，导致跨模态融合和决策质量下降。为了解决这一问题，我们提出了通过跨模态对比学习实现的Dual-levEL AligNment (DELAN)框架。该框架旨在在融合之前对齐各种与导航相关的模态，从而增强跨模态交互和行动决策。具体而言，我们将预融合对齐分为两个层次: 指示-历史层和地标-观察

    arXiv:2404.01994v1 Announce Type: cross  Abstract: Vision-and-Language navigation (VLN) requires an agent to navigate in unseen environment by following natural language instruction. For task completion, the agent needs to align and integrate various navigation modalities, including instruction, observation and navigation history. Existing works primarily concentrate on cross-modal attention at the fusion stage to achieve this objective. Nevertheless, modality features generated by disparate uni-encoders reside in their own spaces, leading to a decline in the quality of cross-modal fusion and decision. To address this problem, we propose a Dual-levEL AligNment (DELAN) framework by cross-modal contrastive learning. This framework is designed to align various navigation-related modalities before fusion, thereby enhancing cross-modal interaction and action decision-making. Specifically, we divide the pre-fusion alignment into dual levels: instruction-history level and landmark-observation
    
[^21]: 对释义的剖析：提示语法和补充信息对从预训练语言模型中检索知识的影响

    Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary Information on Knowledge Retrieval from Pretrained Language Models

    [https://arxiv.org/abs/2404.01992](https://arxiv.org/abs/2404.01992)

    设计了CONPARE-LAMA，确定了释义的语法和语义信息对知识检索的独立影响

    

    预训练语言模型（PLMs）被认为包含各种知识。一种推断关系知识的方法是通过使用填空式提示，模型被要求预测缺失的主语或宾语。设计这些提示通常是一项繁琐的任务，因为语法或语义上的细微差异可能会对知识检索性能产生重大影响。同时，评估提示语法或信息的影响是具有挑战性的，因为它们之间的相互依赖性。我们设计了CONPARE-LAMA - 一个专门的探针，由3400万个不同的提示组成，可以在最小释义之间进行比较。这些释义遵循统一的元模板，可以在任意关系中通过语法和语义的受控变化。CONPARE-LAMA使人能够洞察释义的语法形式或语义信息对知识检索的独立影响

    arXiv:2404.01992v1 Announce Type: new  Abstract: Pre-trained Language Models (PLMs) are known to contain various kinds of knowledge. One method to infer relational knowledge is through the use of cloze-style prompts, where a model is tasked to predict missing subjects or objects. Typically, designing these prompts is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance. Simultaneously, evaluating the impact of either prompt syntax or information is challenging due to their interdependence. We designed CONPARE-LAMA - a dedicated probe, consisting of 34 million distinct prompts that facilitate comparison across minimal paraphrases. These paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations. CONPARE-LAMA enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval 
    
[^22]: Kallaama：塞内加尔三种最广泛使用的语言中关于农业的语音数据集

    Kallaama: A Transcribed Speech Dataset about Agriculture in the Three Most Widely Spoken Languages in Senegal

    [https://arxiv.org/abs/2404.01991](https://arxiv.org/abs/2404.01991)

    Kallaama项目发布了一个包含125小时录音的语音数据集，旨在填补塞内加尔三种主要语言中关于农业领域的机器可读数据不足的空白。

    

    这项工作是Kallaama项目的一部分，旨在为农业领域的语音技术发展生产和传播国家语言语料库。除了沃洛夫语外（沃洛夫语已有自然语言处理的一些语言数据），塞内加尔的国家语言在很大程度上被语言技术提供商忽视。然而，这些技术是保护、促进和教授这些语言的关键。Kallaama关注塞内加尔人口中使用最多的3种主要语言：沃洛夫语、普拉语和瑟里尔语。这些语言被人口广泛使用，约有1000万塞内加尔本地使用者，更不用说国外的人了。然而，在农业领域，这些语言在机器可读数据方面仍然资源匮乏，无法用于自动处理和语言技术。

    arXiv:2404.01991v1 Announce Type: new  Abstract: This work is part of the Kallaama project, whose objective is to produce and disseminate national languages corpora for speech technologies developments, in the field of agriculture. Except for Wolof, which benefits from some language data for natural language processing, national languages of Senegal are largely ignored by language technology providers. However, such technologies are keys to the protection, promotion and teaching of these languages. Kallaama focuses on the 3 main spoken languages by Senegalese people: Wolof, Pulaar and Sereer. These languages are widely spoken by the population, with around 10 million of native Senegalese speakers, not to mention those outside the country. However, they remain under-resourced in terms of machine-readable data that can be used for automatic processing and language technologies, all the more so in the agricultural sector. We release a transcribed speech dataset containing 125 hours of rec
    
[^23]: Team UTSA-NLP在SemEval 2024任务5中的表现：基于GPT4的民事诉讼中的辩证推理的提示合成

    Team UTSA-NLP at SemEval 2024 Task 5: Prompt Ensembling for Argument Reasoning in Civil Procedures with GPT4

    [https://arxiv.org/abs/2404.01961](https://arxiv.org/abs/2404.01961)

    通过使用GPT4进行基于提示的解决方案，我们的系统在民事诉讼中的辩证推理方面取得了显著成果，包括思维链推理和上下文学习等提示策略的集成。在SemEval任务5中，我们获得了0.8095的Macro F1值，并在最终测试集中排名第5。

    

    在本文中，我们介绍了我们针对SemEval任务5（民事诉讼中的法律辩证任务挑战）的系统。法律辩证推理是所有法学生都必须掌握的基本技能。此外，开发能够根据简洁的领域特定上下文信息推理问题的自然语言处理解决方案至关重要。我们的系统探索了使用GPT4进行基于提示的解决方案来推理法律论点。我们还评估了一系列提示策略的集合，包括思维链推理和上下文学习。总体而言，我们的系统在验证数据集上取得了Macro F1值为0.8095，在最终测试集上取得了0.7315（21个团队中排名第5）。此项目的代码可在https://github.com/danschumac1/CivilPromptReasoningGPT4 获取。

    arXiv:2404.01961v1 Announce Type: new  Abstract: In this paper, we present our system for the SemEval Task 5, The Legal Argument Reasoning Task in Civil Procedure Challenge. Legal argument reasoning is an essential skill that all law students must master. Moreover, it is important to develop natural language processing solutions that can reason about a question given terse domain-specific contextual information. Our system explores a prompt-based solution using GPT4 to reason over legal arguments. We also evaluate an ensemble of prompting strategies, including chain-of-thought reasoning and in-context learning. Overall, our system results in a Macro F1 of .8095 on the validation dataset and .7315 (5th out of 21 teams) on the final test set. Code for this project is available at https://github.com/danschumac1/CivilPromptReasoningGPT4.
    
[^24]: HyperCLOVA X 技术报告

    HyperCLOVA X Technical Report

    [https://arxiv.org/abs/2404.01954](https://arxiv.org/abs/2404.01954)

    HyperCLOVA X 是针对韩国语言和文化定制的大型语言模型，同时具有竞争能力的英语、数学和编码能力，其推理能力强大且具有跨语言的通用能力。

    

    我们介绍了 HyperCLOVA X，这是一系列针对韩国语言和文化定制的大型语言模型（LLMs），同时具有在英语、数学和编码方面的竞争能力。HyperCLOVA X 在平衡混合的韩语、英语和代码数据上进行训练，然后通过高质量的人工注释数据进行指导微调，同时遵守严格的安全准则，体现了我们对负责任人工智能的承诺。该模型在包括综合推理、知识、常识、真实性、编码、数学、聊天、遵循指令和无害性在内的各种基准测试中进行了评估，涵盖韩语和英语。HyperCLOVA X 在韩语方面表现出很强的推理能力，得益于对语言和文化细微差异的深刻理解。对固有的双语特性的进一步分析及其扩展到多语言的研究突显出该模型的跨语言熟练性和强大的泛化能力，以适应未定向的目标。

    arXiv:2404.01954v1 Announce Type: cross  Abstract: We introduce HyperCLOVA X, a family of large language models (LLMs) tailored to the Korean language and culture, along with competitive capabilities in English, math, and coding. HyperCLOVA X was trained on a balanced mix of Korean, English, and code data, followed by instruction-tuning with high-quality human-annotated datasets while abiding by strict safety guidelines reflecting our commitment to responsible AI. The model is evaluated across various benchmarks, including comprehensive reasoning, knowledge, commonsense, factuality, coding, math, chatting, instruction-following, and harmlessness, in both Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in Korean backed by a deep understanding of the language and cultural nuances. Further analysis of the inherent bilingual nature and its extension to multilingualism highlights the model's cross-lingual proficiency and strong generalization ability to untargeted la
    
[^25]: 通过模糊推理系统对英语单词中的字素进行分类

    Classifying Graphemes in English Words Through the Application of a Fuzzy Inference System

    [https://arxiv.org/abs/2404.01953](https://arxiv.org/abs/2404.01953)

    通过模糊推理系统，本文提出将单词分割成字素的方法，正确预测的准确率为50.18％，93.51％的预测在正确分类的范围内。

    

    在语言学中，字素是与音素对应的书写系统的书面单位。在自然语言处理任务中，书面语言通过单词分析和字符分析两种不同的方式进行分析。本文关注第三种方法，即对字素进行分析。字素相对于单词和字符分析具有优势，因为它们是语音的自包含表示。由于将单词分割成字素的性质基于复杂的非二进制规则，应用模糊逻辑将提供一个合适的介质，用于预测单词中的字素数量。本文提出应用模糊推理系统将单词分割成其字素。这种模糊推理系统结果表明，在一半的情况下正确预测单词中的字素数量，其中93.51%的分类在正确分类的+- 1之内。

    arXiv:2404.01953v1 Announce Type: new  Abstract: In Linguistics, a grapheme is a written unit of a writing system corresponding to a phonological sound. In Natural Language Processing tasks, written language is analysed through two different mediums, word analysis, and character analysis. This paper focuses on a third approach, the analysis of graphemes. Graphemes have advantages over word and character analysis by being self-contained representations of phonetic sounds. Due to the nature of splitting a word into graphemes being based on complex, non-binary rules, the application of fuzzy logic would provide a suitable medium upon which to predict the number of graphemes in a word. This paper proposes the application of a Fuzzy Inference System to split words into their graphemes. This Fuzzy Inference System results in a correct prediction of the number of graphemes in a word 50.18% of the time, with 93.51% being within a margin of +- 1 from the correct classification. Given the variet
    
[^26]: 更好地理解网络犯罪：精细调整的LLMs在翻译中的作用

    Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation

    [https://arxiv.org/abs/2404.01940](https://arxiv.org/abs/2404.01940)

    提出使用精细调整的大型语言模型(LLM)生成能够准确捕捉网络犯罪语言细微差别的翻译，结果显示能够更好、更快、更准确地捕捉语言的细微差别，并大幅度降低翻译成本。

    

    理解网络犯罪通信对网络安全防御至关重要。这通常涉及将通信翻译成英文进行处理、解释和生成及时情报。问题在于翻译很困难。人工翻译缓慢、昂贵且稀缺。机器翻译不准确且存在偏见。我们提出使用精细调整的大型语言模型(LLM)生成能够准确捕捉网络犯罪语言细微差别的翻译。我们将我们的技术应用于来自NoName057(16)俄语黑客组织的公开聊天记录。我们的结果显示，我们的精细调整的LLM模型更好、更快、更准确，并能够捕捉语言的细微差别。我们的方法表明，通过与人工翻译相比比可实现高保真翻译，并将成本显著降低430至23,000倍。

    arXiv:2404.01940v1 Announce Type: new  Abstract: Understanding cybercrime communications is paramount for cybersecurity defence. This often involves translating communications into English for processing, interpreting, and generating timely intelligence. The problem is that translation is hard. Human translation is slow, expensive, and scarce. Machine translation is inaccurate and biased. We propose using fine-tuned Large Language Models (LLM) to generate translations that can accurately capture the nuances of cybercrime language. We apply our technique to public chats from the NoName057(16) Russian-speaking hacktivist group. Our results show that our fine-tuned LLM model is better, faster, more accurate, and able to capture nuances of the language. Our method shows it is possible to achieve high-fidelity translations and significantly reduce costs by a factor ranging from 430 to 23,000 compared to a human translator.
    
[^27]: SGSH：用骨架启发来激发大型语言模型进行知识库问题生成

    SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation

    [https://arxiv.org/abs/2404.01923](https://arxiv.org/abs/2404.01923)

    本研究提出了SGSH框架，通过骨架启发来激发GPT-3.5生成知识库问题，有效地组织和利用丰富的语义知识。

    

    知识库问题生成（KBQG）旨在从从知识库中提取的三元组事实集生成自然语言问题。现有方法通过预训练语言模型（PLMs）显著提升了KBQG的性能，得益于丰富的语义知识。本文提出了SGSH--一个简单有效的框架，通过骨架启发来增强KBQG的性能。该框架包含“骨架启发”，提供了与每个输入相关的更精细的指导，以激发LLMs生成最佳问题，包括问题短语和助动词等关键要素。

    arXiv:2404.01923v1 Announce Type: cross  Abstract: Knowledge base question generation (KBQG) aims to generate natural language questions from a set of triplet facts extracted from KB. Existing methods have significantly boosted the performance of KBQG via pre-trained language models (PLMs) thanks to the richly endowed semantic knowledge. With the advance of pre-training techniques, large language models (LLMs) (e.g., GPT-3.5) undoubtedly possess much more semantic knowledge. Therefore, how to effectively organize and exploit the abundant knowledge for KBQG becomes the focus of our study. In this work, we propose SGSH--a simple and effective framework to Stimulate GPT-3.5 with Skeleton Heuristics to enhance KBQG. The framework incorporates "skeleton heuristics", which provides more fine-grained guidance associated with each input to stimulate LLMs to generate optimal questions, encompassing essential elements like the question phrase and the auxiliary verb.More specifically, we devise a
    
[^28]: 基于理性中心的反事实数据增强方法用于跨文档事件关联消解

    A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document Event Coreference Resolution

    [https://arxiv.org/abs/2404.01921](https://arxiv.org/abs/2404.01921)

    该研究提出了一种基于理性中心的反事实数据增强方法，通过结构因果模型识别虚假和因果关联，有效提高了跨文档事件关联消解系统的性能。

    

    基于预训练语言模型（PLMs），事件关联消解（ECR）系统在跨文档中聚类指代性事件方面表现出色。然而，现有系统在输入提及对文本中过于依赖“触发词词汇匹配”这一虚假模式。我们利用结构因果模型（SCM）对基准ECR系统的决策过程进行形式化，旨在识别ECR任务中的虚假和因果关联（即，理性）。利用反事实数据增强的去偏方法，我们开发了一种基于理性中心的反事实数据增强方法，并结合LLM。该方法专为ECR系统中的两两输入设计，我们在触发词和上下文上进行直接干预，以减轻虚假关联，强调因果关系。我们的方法在三个实验数据集上实现了最新的性能。

    arXiv:2404.01921v1 Announce Type: new  Abstract: Based on Pre-trained Language Models (PLMs), event coreference resolution (ECR) systems have demonstrated outstanding performance in clustering coreferential events across documents. However, the existing system exhibits an excessive reliance on the `triggers lexical matching' spurious pattern in the input mention pair text. We formalize the decision-making process of the baseline ECR system using a Structural Causal Model (SCM), aiming to identify spurious and causal associations (i.e., rationales) within the ECR task. Leveraging the debiasing capability of counterfactual data augmentation, we develop a rationale-centric counterfactual data augmentation method with LLM-in-the-loop. This method is specialized for pairwise input in the ECR system, where we conduct direct interventions on triggers and context to mitigate the spurious association while emphasizing the causation. Our approach achieves state-of-the-art performance on three po
    
[^29]: SCANNER：用于强大的多模式具名实体识别的知识增强方法

    SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities

    [https://arxiv.org/abs/2404.01914](https://arxiv.org/abs/2404.01914)

    提出了SCANNER模型，通过提取实体候选并利用知识从多种来源获取知识，增强了对未见实体的识别能力，并引入了一种新颖的方法来处理NER数据集中带有噪声注释的挑战。

    

    最近具名实体识别(NER)的进展推动了该任务的边界，将视觉信号纳入其中，导致许多变体，包括多模式NER（MNER）或基于实体的MNER（GMNER）。 这些任务的一个关键挑战是模型应能够推广到训练过程中未见的实体，并能够处理训练样本中带有噪声注释的情况。 为了解决这一障碍，我们提出了SCANNER（用于NER的SpAN CANdidate检测和识别），这是一种能够有效处理所有三种NER变体的模型。 SCANNER是一个两阶段结构；我们在第一阶段提取实体候选，并将其用作查询以获取知识，有效地从各种来源中提取知识。 我们可以通过利用这种以实体为中心的提取知识来提高性能，以解决未见实体。 此外，为了应对NER数据集中由嘈杂注释引起的挑战，我们还介绍了一种新颖的方法。

    arXiv:2404.01914v1 Announce Type: cross  Abstract: Recent advances in named entity recognition (NER) have pushed the boundary of the task to incorporate visual signals, leading to many variants, including multi-modal NER (MNER) or grounded MNER (GMNER). A key challenge to these tasks is that the model should be able to generalize to the entities unseen during the training, and should be able to handle the training samples with noisy annotations. To address this obstacle, we propose SCANNER (Span CANdidate detection and recognition for NER), a model capable of effectively handling all three NER variants. SCANNER is a two-stage structure; we extract entity candidates in the first stage and use it as a query to get knowledge, effectively pulling knowledge from various sources. We can boost our performance by utilizing this entity-centric extracted knowledge to address unseen entities. Furthermore, to tackle the challenges arising from noisy annotations in NER datasets, we introduce a nove
    
[^30]: 人性化机器生成的内容：通过对抗攻击规避AI文本检测

    Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack

    [https://arxiv.org/abs/2404.01907](https://arxiv.org/abs/2404.01907)

    本文提出了一个旨在对机器生成的文本进行微小扰动以规避检测的广泛对抗攻击框架，通过白盒和黑盒两种攻击设置以及对抗学习在动态场景中的应用，评估了当前检测模型对此类攻击的鲁棒性潜力增强

    

    随着大型语言模型（LLMs）的发展，检测文本是否由机器生成在面对诸如误传信息、保护知识产权和预防学术抄袭等恶意用例时变得日益具有挑战性。虽然经过良好训练的文本检测器在未知测试数据上展现了有希望的性能，但最近的研究表明，这些检测器在处理对抗攻击（如释义）时存在漏洞。本文提出了一个更广泛类别的对抗攻击框架，旨在对机器生成的内容进行微小扰动以规避检测。我们考虑了两种攻击设置：白盒和黑盒，并在动态场景中采用对抗学习来评估当前检测模型对此类攻击的鲁棒性潜力增强。实证结果表明，目前的检测模型

    arXiv:2404.01907v1 Announce Type: new  Abstract: With the development of large language models (LLMs), detecting whether text is generated by a machine becomes increasingly challenging in the face of malicious use cases like the spread of false information, protection of intellectual property, and prevention of academic plagiarism. While well-trained text detectors have demonstrated promising performance on unseen test data, recent research suggests that these detectors have vulnerabilities when dealing with adversarial attacks such as paraphrasing. In this paper, we propose a framework for a broader class of adversarial attacks, designed to perform minor perturbations in machine-generated content to evade detection. We consider two attack settings: white-box and black-box, and employ adversarial learning in dynamic scenarios to assess the potential enhancement of the current detection model's robustness against such attacks. The empirical results reveal that the current detection mode
    
[^31]: 在CodeLLMs中实现类型预测的鲁棒激活导向技术

    Activation Steering for Robust Type Prediction in CodeLLMs

    [https://arxiv.org/abs/2404.01903](https://arxiv.org/abs/2404.01903)

    我们提出了一种激活导向技术，通过编辑模型内部激活来改善CodeLLMs在代码类型预测中对于语法干扰的鲁棒性，并成功应用于Python和TypeScript的类型预测，将类型误差率纠正高达90%。

    

    预训练在代码上的现代LLMs能够成功地完成各种编程任务。然而，它们的性能对语法特征非常敏感，例如变量和类型的名称、代码结构以及类型提示的存在。我们提出了一种推理时技术，使CodeLLMs更能抵御语法干扰因素，这些因素与语义无关。我们的方法依赖于激活导向，涉及编辑内部模型激活以将模型引导到正确的预测。我们通过从突变测试中汲取灵感构建激活向量的新方法，该方法构建最小的破坏语义的代码编辑。相比之下，我们从保留语义的代码编辑中构建激活向量。我们将我们的方法应用于逐渐类型化语言Python和TypeScript的类型预测任务。这种方法可以纠正高达90%的类型错误预测。

    arXiv:2404.01903v1 Announce Type: new  Abstract: Contemporary LLMs pretrained on code are capable of succeeding at a wide variety of programming tasks. However, their performance is very sensitive to syntactic features, such as the names of variables and types, the structure of code, and presence of type hints. We contribute an inference-time technique to make CodeLLMs more robust to syntactic distractors that are semantically irrelevant. Our methodology relies on activation steering, which involves editing internal model activations to steer the model towards the correct prediction. We contribute a novel way to construct steering vectors by taking inspiration from mutation testing, which constructs minimal semantics-breaking code edits. In contrast, we construct steering vectors from semantics-preserving code edits. We apply our approach to the task of type prediction for the gradually typed languages Python and TypeScript. This approach corrects up to 90% of type mispredictions. Fina
    
[^32]: 超越准确性：评估大型语言模型的推理行为--一项调查

    Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey

    [https://arxiv.org/abs/2404.01869](https://arxiv.org/abs/2404.01869)

    本文通过综述超越任务准确性的研究，提供对大型语言模型推理过程更深入了解，并强调了LLMs倾向于依赖于训练数据中的表面模式和相关性。

    

    大型语言模型（LLMs）最近在涉及推理的任务中表现出色，引发了关于这些模型是否具有类似于人类的推理能力的激烈讨论。然而，尽管取得了成功，但LLMs的推理能力的深度仍然存在不确定性。这种不确定性部分源自对模型推理行为的深入调查而非仅仅通过表面准确性指标来衡量任务表现。本文旨在通过综述超越任务准确性的研究，提供对模型推理过程更深入的了解来弥补这一差距。此外，我们调查了评估LLMs推理行为的主要方法论，强调了当前对更细致推理分析的趋势和努力。我们的综述表明，LLMs倾向于依赖于训练数据中的表面模式和相关性。

    arXiv:2404.01869v1 Announce Type: cross  Abstract: Large language models (LLMs) have recently shown impressive performance on tasks involving reasoning, leading to a lively debate on whether these models possess reasoning capabilities similar to humans. However, despite these successes, the depth of LLMs' reasoning abilities remains uncertain. This uncertainty partly stems from the predominant focus on task performance, measured through shallow accuracy metrics, rather than a thorough investigation of the models' reasoning behavior. This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models' reasoning processes. Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses. Our review suggests that LLMs tend to rely on surface-level patterns and correlations in their training data, rat
    
[^33]: Self-StrAE在SemEval-2024任务1中：让自我结构化自编码器用更少的数据学习更多

    Self-StrAE at SemEval-2024 Task 1: Making Self-Structuring AutoEncoders Learn More With Less

    [https://arxiv.org/abs/2404.01860](https://arxiv.org/abs/2404.01860)

    Self-StrAE 在自编码器中添加重构作为辅助目标，同时增加独立通道数量以显著提高嵌入质量，并成功将非嵌入参数数量减少到七个。

    

    这篇论文介绍了两项对自我结构化自编码器(Self-StrAE)进行改进的简单方法。首先，我们展示将重构作为辅助目标包括到词汇中可以提高表示质量。其次，我们证明增加独立通道的数量可以显著提高嵌入质量，同时减少参数数量。令人惊讶的是，我们证明这种趋势可以被极端地遵循，甚至可以将非嵌入参数的总数减少至七个。我们的系统可以从头开始使用至少1000万个输入数据标记进行预训练，并在英语、西班牙语和南非荷兰语之间表现出效果。

    arXiv:2404.01860v1 Announce Type: new  Abstract: This paper presents two simple improvements to the Self-Structuring AutoEncoder (Self-StrAE). Firstly, we show that including reconstruction to the vocabulary as an auxiliary objective improves representation quality. Secondly, we demonstrate that increasing the number of independent channels leads to significant improvements in embedding quality, while simultaneously reducing the number of parameters. Surprisingly, we demonstrate that this trend can be followed to the extreme, even to point of reducing the total number of non-embedding parameters to seven. Our system can be pre-trained from scratch with as little as 10M tokens of input data, and proves effective across English, Spanish and Afrikaans.
    
[^34]: 在课程评价中检测性别偏见

    Detecting Gender Bias in Course Evaluations

    [https://arxiv.org/abs/2404.01857](https://arxiv.org/abs/2404.01857)

    通过机器学习和自然语言处理，研究发现课程评价中的性别偏见，比较英语和瑞典课程数据，揭示了学生对课程的评价主观性因考官性别而存在差异。

    

    这是一篇研究利用机器学习和自然语言处理从硕士论文研究中发现课程评价中性别偏见的截图。我们使用不同方法来检测和探索数据，发现学生对课程的评价会因考官的性别而有所不同。评估和比较了来自英语和瑞典课程的数据，以捕捉可能存在的性别偏见中更多的细微差别。我们在这里展示了到目前为止的工作结果，但这是一个正在进行的项目，还有更多的工作要做。

    arXiv:2404.01857v1 Announce Type: cross  Abstract: An outtake from the findnings of a master thesis studying gender bias in course evaluations through the lense of machine learning and nlp. We use different methods to examine and explore the data and find differences in what students write about courses depending on gender of the examiner. Data from English and Swedish courses are evaluated and compared, in order to capture more nuance in the gender bias that might be found. Here we present the results from the work so far, but this is an ongoing project and there is more work to do.
    
[^35]: Poro 34B和多语种的祝福

    Poro 34B and the Blessing of Multilinguality

    [https://arxiv.org/abs/2404.01856](https://arxiv.org/abs/2404.01856)

    多语种训练的Poro 34B模型在芬兰语等小语种上取得了显著进展，并具有比现有模型更出色的能力。

    

    最先进大型语言模型的预训练现在需要数万亿字的文本，这比绝大多数语言可获得的文本数量多几个数量级。尽管包含多种语言的文本是获取更多预训练数据的明显方法，但多语种往往被视为一种诅咒，大多数模型训练工作仍然主要集中在个别大语种上。我们相信多语种可以是一种祝福，并且应该有可能通过多语种训练显著提高小语种的模型能力。在这项研究中，我们介绍了Poro 34B，这是一个在1万亿个芬兰语、英语和编程语言标记上进行训练的拥有340亿参数的模型，并证明了多语种训练方法可以产生一个模型，不仅在芬兰语的现有模型能力上取得了显著进展，而且在表现方面表现出色。

    arXiv:2404.01856v1 Announce Type: new  Abstract: The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels i
    
[^36]: IndoCulture: 探索印尼十一个省份间地理影响的文化常识推理

    IndoCulture: Exploring Geographically-Influenced Cultural Commonsense Reasoning Across Eleven Indonesian Provinces

    [https://arxiv.org/abs/2404.01854](https://arxiv.org/abs/2404.01854)

    本研究介绍了IndoCulture项目，旨在通过当地人手动收集数据，探索印尼十一个省份间地理影响的文化常识推理。评估发现，即使是最好的语言模型也在特定省份上表现更准确，而添加地理信息有助于提高模型性能。

    

    尽管常识推理受文化和地理因素的极大影响，先前关于语言模型的研究主要集中在英语文化上，可能导致一种以英语为中心的偏见。本文介绍了IndoCulture，旨在理解地理因素对语言模型推理能力的影响，特别强调了十一个印尼省份内所发现的多样文化。与先前依赖模板（Yin等，2022）和在线抓取（Fung等，2024）的作品不同，我们通过询问当地人手动开发预定义主题的上下文和合理选项来创建IndoCulture。对23个语言模型的评估揭示了几个见解：（1）即使是最好的开源模型也难以达到53.2％的准确性，（2）模型通常为特定省份（如巴厘岛和西爪哇）提供更准确的预测，（3）包含地理信息可显着改善模型的表现。

    arXiv:2404.01854v1 Announce Type: new  Abstract: Although commonsense reasoning is greatly shaped by cultural and geographical factors, previous studies on language models have predominantly centered on English cultures, potentially resulting in an Anglocentric bias. In this paper, we introduce IndoCulture, aimed at understanding the influence of geographical factors on language model reasoning ability, with a specific emphasis on the diverse cultures found within eleven Indonesian provinces. In contrast to prior works that relied on templates (Yin et al., 2022) and online scrapping (Fung et al., 2024), we created IndoCulture by asking local people to manually develop the context and plausible options based on predefined topics. Evaluations of 23 language models reveal several insights: (1) even the best open-source model struggles with an accuracy of 53.2%, (2) models often provide more accurate predictions for specific provinces, such as Bali and West Java, and (3) the inclusion of l
    
[^37]: 面向恶意内容检测社区模型泛化的更为现实的评估设置

    A (More) Realistic Evaluation Setup for Generalisation of Community Models on Malicious Content Detection

    [https://arxiv.org/abs/2404.01822](https://arxiv.org/abs/2404.01822)

    提出了面向恶意内容检测社区模型泛化的更为现实的评估设置，通过少样本子图采样方法测试模型泛化能力。

    

    用于恶意内容检测的社区模型考虑社交图中的上下文以及内容本身，在基准数据集上表现出色。然而，虚假信息和仇恨言论仍在社交媒体网络上传播。本文提出了基于我们的少样本子图采样方法的模型泛化的新型评估设置。这个设置通过在更大图的局部探索中少量标记示例，模拟更为现实的应用场景，以测试泛化能力。

    arXiv:2404.01822v1 Announce Type: cross  Abstract: Community models for malicious content detection, which take into account the context from a social graph alongside the content itself, have shown remarkable performance on benchmark datasets. Yet, misinformation and hate speech continue to propagate on social media networks. This mismatch can be partially attributed to the limitations of current evaluation setups that neglect the rapid evolution of online content and the underlying social graph. In this paper, we propose a novel evaluation setup for model generalisation based on our few-shot subgraph sampling approach. This setup tests for generalisation through few labelled examples in local explorations of a larger graph, emulating more realistic application settings. We show this to be a challenging inductive setup, wherein strong performance on the training graph is not indicative of performance on unseen tasks, domains, or graph structures. Lastly, we show that graph meta-learner
    
[^38]: 使用ChatGPT对科学文章引用进行情感分析：识别潜在偏见和利益冲突

    Sentiment Analysis of Citations in Scientific Articles Using ChatGPT: Identifying Potential Biases and Conflicts of Interest

    [https://arxiv.org/abs/2404.01800](https://arxiv.org/abs/2404.01800)

    使用ChatGPT进行情感分析揭示了科学文章引用中的偏见和利益冲突，增强了科学文献评估的客观性和可靠性

    

    科学文章在推动知识发展和指导研究方向方面起着至关重要的作用。评估科学文章的关键方面之一是对引文进行分析，这提供了对被引用作品的影响和接受程度的见解。本文介绍了大型语言模型，特别是ChatGPT，在科学文章中对引文进行全面情感分析的创新应用。通过利用先进的自然语言处理（NLP）技术，ChatGPT能够辨别引文的微妙积极或消极情感，提供对被引用作品的接受程度和影响的见解。此外，ChatGPT的能力还包括检测引文中的潜在偏见和利益冲突，增强科学文献评估的客观性和可靠性。这项研究展示了人工智能（AI）驱动工具在增强引文分析和提高科学文献评估方面的变革潜力。

    arXiv:2404.01800v1 Announce Type: cross  Abstract: Scientific articles play a crucial role in advancing knowledge and informing research directions. One key aspect of evaluating scientific articles is the analysis of citations, which provides insights into the impact and reception of the cited works. This article introduces the innovative use of large language models, particularly ChatGPT, for comprehensive sentiment analysis of citations within scientific articles. By leveraging advanced natural language processing (NLP) techniques, ChatGPT can discern the nuanced positivity or negativity of citations, offering insights into the reception and impact of cited works. Furthermore, ChatGPT's capabilities extend to detecting potential biases and conflicts of interest in citations, enhancing the objectivity and reliability of scientific literature evaluation. This study showcases the transformative potential of artificial intelligence (AI)-powered tools in enhancing citation analysis and pr
    
[^39]: PATCH -- 大型语言模型的心理测量辅助基准测试：数学能力的案例研究

    PATCH -- Psychometrics-AssisTed benCHmarking of Large Language Models: A Case Study of Mathematics Proficiency

    [https://arxiv.org/abs/2404.01799](https://arxiv.org/abs/2404.01799)

    该论文提出了一种新的框架PATCH，用于将心理测量领域的知识整合到大型语言模型的基准测试中，以解决现有基准测试存在的测量质量、项目级别评估和参考人群等问题。

    

    许多现有的大型（多模态）语言模型（LLMs）基准测试着重于衡量LLMs的学术能力，通常也对比较模型性能与人类考试者感兴趣。尽管这些基准测试对LLMs的发展至关重要，但它们存在一些限制，包括有问题的测量质量（例如，它们是否以可靠的方式衡量所需的内容？）、缺乏项目级别的质量评估（例如，有些项目是否比其他更重要或更困难？）以及人类人口参照模糊（例如，模型可以与谁进行比较？）。为了应对这些挑战，我们提出利用心理测量学领域的知识——一门致力于测量潜在变量如学术能力的领域——来进行LLMs基准测试的心理测量辅助方法。我们的主要贡献有三点。首先，我们介绍了PATCH：一种用于大型语言模型的心理测量辅助基准测试的新框架。

    arXiv:2404.01799v1 Announce Type: new  Abstract: Many existing benchmarks of large (multimodal) language models (LLMs) focus on measuring LLMs' academic proficiency, often with also an interest in comparing model performance with human test takers. While these benchmarks have proven key to the development of LLMs, they suffer from several limitations, including questionable measurement quality (e.g., Do they measure what they are supposed to in a reliable way?), lack of quality assessment on the item level (e.g., Are some items more important or difficult than others?) and unclear human population reference (e.g., To whom can the model be compared?). In response to these challenges, we propose leveraging knowledge from psychometrics - a field dedicated to the measurement of latent variables like academic proficiency - into LLM benchmarking. We make three primary contributions. First, we introduce PATCH: a novel framework for Psychometrics-AssisTed benCHmarking of LLMs. PATCH addresses 
    
[^40]: 基于预训练GPT-2模型的生成式AI文本生成方法

    Generative AI-Based Text Generation Methods Using Pre-Trained GPT-2 Model

    [https://arxiv.org/abs/2404.01786](https://arxiv.org/abs/2404.01786)

    通过分析各种传统和现代的文本生成技术，该研究提供了对每种方法优势、劣势和潜在应用的宝贵见解，并对这些方法的性能进行了比较研究。

    

    这项工作深入探讨了自动文本生成领域，探索了从传统确定性方法到更现代的随机方法的各种技术。通过分析贪婪搜索、束搜索、top-k抽样、top-p抽样、对比搜索和本地典型搜索，这项工作为每种方法的优势、劣势和潜在应用提供了有价值的见解。每种文本生成方法都使用几个标准度量标准进行评估，并对这些方法的性能进行了比较研究。最后，还确定了自动文本生成领域研究的一些未来方向。

    arXiv:2404.01786v1 Announce Type: new  Abstract: This work delved into the realm of automatic text generation, exploring a variety of techniques ranging from traditional deterministic approaches to more modern stochastic methods. Through analysis of greedy search, beam search, top-k sampling, top-p sampling, contrastive searching, and locally typical searching, this work has provided valuable insights into the strengths, weaknesses, and potential applications of each method. Each text-generating method is evaluated using several standard metrics and a comparative study has been made on the performance of the approaches. Finally, some future directions of research in the field of automatic text generation are also identified.
    
[^41]: 人类能否识别领域？

    Can Humans Identify Domains?

    [https://arxiv.org/abs/2404.01785](https://arxiv.org/abs/2404.01785)

    本研究探讨了人类在识别相关文本属性方面的熟练程度，特别是流派和主题的概念，以解析文本领域的核心概念。

    

    文本领域是自然语言处理（NLP）社区中的重要属性，因其对下游模型性能的影响。然而，该概念本身定义不清晰，在实践中涉及到诸如文档的流派、主题、媒介或风格等非类型学属性。本文通过人类在识别相关内在文本属性方面的熟练度，特别是流派（交际目的）和主题（主题内容）的概念，来调查领域的核心概念。我们在*TGeGUM*中发布了我们的注释：这是来自GUM数据集（Zeldes, 2017）的9.1k句子的集合，其中包括了针对11种流派（源类型）的单句和较大背景（即散文）注释，以及按照杜威十进制图书馆分类系统（Dewey, 1979）的主题/子主题，包括了10/100个递增粒度的分层主题。每个实例都由三个注释者注释，共32.7k条注释。

    arXiv:2404.01785v1 Announce Type: new  Abstract: Textual domain is a crucial property within the Natural Language Processing (NLP) community due to its effects on downstream model performance. The concept itself is, however, loosely defined and, in practice, refers to any non-typological property, such as genre, topic, medium or style of a document. We investigate the core notion of domains via human proficiency in identifying related intrinsic textual properties, specifically the concepts of genre (communicative purpose) and topic (subject matter). We publish our annotations in *TGeGUM*: A collection of 9.1k sentences from the GUM dataset (Zeldes, 2017) with single sentence and larger context (i.e., prose) annotations for one of 11 genres (source type), and its topic/subtopic as per the Dewey Decimal library classification system (Dewey, 1979), consisting of 10/100 hierarchical topics of increased granularity. Each instance is annotated by three annotators, for a total of 32.7k annota
    
[^42]: 用于增强基于文本的陈规检测和基于探测的偏见评估的大规模语言模型审计

    Auditing Large Language Models for Enhanced Text-Based Stereotype Detection and Probing-Based Bias Evaluation

    [https://arxiv.org/abs/2404.01768](https://arxiv.org/abs/2404.01768)

    该研究引入了Multi-Grain Stereotype（MGS）数据集，探索了不同的机器学习方法用于建立陈规检测的基线，并提出了一系列基于MGS数据训练的英文文本的陈规分类器模型。

    

    大型语言模型（LLMs）的最新进展显著提高了它们在面向人类的人工智能（AI）应用中的影响力。然而，LLMs可能会复制甚至加剧自训练数据中的陈规输出。本研究介绍了Multi-Grain Stereotype（MGS）数据集，包括51,867个实例，涵盖性别、种族、职业、宗教和陈规文本，通过融合多个先前公开的陈规检测数据集收集而来。我们探索了旨在为陈规检测建立基线的不同机器学习方法，并微调了多种架构和模型大小的几个语言模型，本文展示了一系列基于MGS训练的英文文本的陈规分类器模型。为了了解我们的陈规检测器是否捕捉到与人类常识一致的相关特征，我们利用了各种可解释的AI工具，

    arXiv:2404.01768v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs) have significantly increased their presence in human-facing Artificial Intelligence (AI) applications. However, LLMs could reproduce and even exacerbate stereotypical outputs from training data. This work introduces the Multi-Grain Stereotype (MGS) dataset, encompassing 51,867 instances across gender, race, profession, religion, and stereotypical text, collected by fusing multiple previously publicly available stereotype detection datasets. We explore different machine learning approaches aimed at establishing baselines for stereotype detection, and fine-tune several language models of various architectures and model sizes, presenting in this work a series of stereotypes classifier models for English text trained on MGS. To understand whether our stereotype detectors capture relevant features (aligning with human common sense) we utilise a variety of explanainable AI tools, including 
    
[^43]: 增量式少样本事件检测

    Class-Incremental Few-Shot Event Detection

    [https://arxiv.org/abs/2404.01767](https://arxiv.org/abs/2404.01767)

    提出了增量式少样本事件检测任务，并通过提出的Prompt-KD方法解决了老知识遗忘和新类别过拟合问题。

    

    事件检测是信息抽取和知识图谱中的基本任务之一。然而，一个现实的事件检测系统经常需要不断处理新的事件类别。这些新类别通常只有少量标记实例，因为标注大量未标记实例是耗时且劳动密集的。因此，本文提出了一个新任务，称为增量式少样本事件检测。然而，这个任务面临着两个问题，即老知识遗忘和新类别过拟合。为了解决这些问题，本文进一步提出了一种基于知识蒸馏和提示学习的全新方法，称为Prompt-KD。具体来说，为了处理关于老知识遗忘的问题，Prompt-KD开发了一种基于注意力的多教师知识蒸馏框架，其中在所有学习会话中重复使用在基础类别上预先训练的祖先教师模型，父教师...

    arXiv:2404.01767v1 Announce Type: new  Abstract: Event detection is one of the fundamental tasks in information extraction and knowledge graph. However, a realistic event detection system often needs to deal with new event classes constantly. These new classes usually have only a few labeled instances as it is time-consuming and labor-intensive to annotate a large number of unlabeled instances. Therefore, this paper proposes a new task, called class-incremental few-shot event detection. Nevertheless, this task faces two problems, i.e., old knowledge forgetting and new class overfitting. To solve these problems, this paper further presents a novel knowledge distillation and prompt learning based method, called Prompt-KD. Specifically, to handle the forgetting problem about old knowledge, Prompt-KD develops an attention based multi-teacher knowledge distillation framework, where the ancestor teacher model pre-trained on base classes is reused in all learning sessions, and the father teac
    
[^44]: M2SA: 多模态和多语言模型用于推文情感分析

    M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets

    [https://arxiv.org/abs/2404.01753](https://arxiv.org/abs/2404.01753)

    本文通过将现有的推特情感数据集转换为多模态格式，并进行基准实验，发现在多语言环境下使用情感调整的大型语言模型作为文本编码器时，效果显著。

    

    近年来，面向学习各种数据类型的多模态自然语言处理受到了重视。然而，在多语言环境下分析多模态任务时需要更清晰的理解。尽管先前关于推文情感分析的研究主要集中在英语上，但本文通过简单的整理过程将现有的文本推特情感数据集转换为多模态格式，从而填补了这一空白。我们的工作为研究社区内与情感相关的研究开辟了新的途径。此外，我们利用这个增强数据集进行基准实验并报告了研究结果。值得注意的是，我们的评估结果显示，当比较单模态和多模态配置时，使用情感调整的大型语言模型作为文本编码器表现出色。

    arXiv:2404.01753v1 Announce Type: new  Abstract: In recent years, multimodal natural language processing, aimed at learning from diverse data types, has garnered significant attention. However, there needs to be more clarity when it comes to analysing multimodal tasks in multi-lingual contexts. While prior studies on sentiment analysis of tweets have predominantly focused on the English language, this paper addresses this gap by transforming an existing textual Twitter sentiment dataset into a multimodal format through a straightforward curation process. Our work opens up new avenues for sentiment-related research within the research community. Additionally, we conduct baseline experiments utilising this augmented dataset and report the findings. Notably, our evaluations reveal that when comparing unimodal and multimodal configurations, using a sentiment-tuned large language model as a text encoder performs exceptionally well.
    
[^45]: Octopus v2：用于超级代理的设备上语言模型

    Octopus v2: On-device language model for super agent

    [https://arxiv.org/abs/2404.01744](https://arxiv.org/abs/2404.01744)

    该研究提出了一种新方法，通过将具有20亿参数的设备上模型赋予超越GPT-4的准确性和延迟性能，并将上下文长度缩减95％，从而解决了函数调用中的延迟和准确性问题。

    

    语言模型在各种软件应用中展现出了高效性，特别是与自动工作流相关的任务。这些模型具有调用函数的关键能力，在创建AI代理时至关重要。尽管大规模语言模型在云环境中表现出色，但往往存在着隐私和成本方面的担忧。当前用于函数调用的设备上模型面临延迟和准确性问题。我们的研究提出了一种新方法，使具有20亿参数的设备上模型在准确性和延迟方面超越了GPT-4，并将上下文长度缩减了95%。与基于RAG的函数调用机制的Llama-7B相比，我们的方法将延迟提高了35倍。这种方法将延迟降低到适合在生产环境中的各种边缘设备上部署的水平上，符合性能要求。

    arXiv:2404.01744v1 Announce Type: new  Abstract: Language models have shown effectiveness in a variety of software applications, particularly in tasks related to automatic workflow. These models possess the crucial ability to call functions, which is essential in creating AI agents. Despite the high performance of large-scale language models in cloud environments, they are often associated with concerns over privacy and cost. Current on-device models for function calling face issues with latency and accuracy. Our research presents a new method that empowers an on-device model with 2 billion parameters to surpass the performance of GPT-4 in both accuracy and latency, and decrease the context length by 95\%. When compared to Llama-7B with a RAG-based function calling mechanism, our method enhances latency by 35-fold. This method reduces the latency to levels deemed suitable for deployment across a variety of edge devices in production environments, aligning with the performance requisite
    
[^46]: 从Whisper进行的微观可懂性预测的迁移学习

    Transfer Learning from Whisper for Microscopic Intelligibility Prediction

    [https://arxiv.org/abs/2404.01737](https://arxiv.org/abs/2404.01737)

    本文研究了如何利用来自Whisper的迁移学习，用于在词汇响应水平上进行微观可懂性预测，并取得了显著的相对改进。

    

    宏观可懂性模型预测给定语音在噪声中的期望人类词错误率。相比之下，微观可懂性模型旨在对听众的感知进行细粒度预测，例如预测音素或词汇响应。本文研究了从Whisper进行的迁移学习，用于在词汇响应水平上进行微观可懂性预测。我们的方法优于考虑的基线，甚至在零样本设置中表现出色，并在微调以预测听众响应时相对提升高达66\%。我们的结果展示了基于大规模深度学习方法在微观

    arXiv:2404.01737v1 Announce Type: cross  Abstract: Macroscopic intelligibility models predict the expected human word-error-rate for a given speech-in-noise stimulus. In contrast, microscopic intelligibility models aim to make fine-grained predictions about listeners' perception, e.g. predicting phonetic or lexical responses. State-of-the-art macroscopic models use transfer learning from large scale deep learning models for speech processing, whereas such methods have rarely been used for microscopic modeling. In this paper, we study the use of transfer learning from Whisper, a state-of-the-art deep learning model for automatic speech recognition, for microscopic intelligibility prediction at the level of lexical responses. Our method outperforms the considered baselines, even in a zero-shot setup, and yields a relative improvement of up to 66\% when fine-tuned to predict listeners' responses. Our results showcase the promise of large scale deep learning based methods for microscopic i
    
[^47]: 基于事件关系图的句子级媒体偏见分析

    Sentence-level Media Bias Analysis with Event Relation Graph

    [https://arxiv.org/abs/2404.01722](https://arxiv.org/abs/2404.01722)

    本文提出了一种基于事件关系图的句子级媒体偏见分析方法，通过构建事件关系图和事件感知的语言模型，实现了对偏见句的识别和表达。

    

    媒体机构现在越来越党派化和极化。本文针对句子级别识别媒体偏见，定位旨在影响读者观点的偏见句。由于偏见句通常以中立客观的方式表达，考虑句子之外的更广泛上下文可以帮助揭示这种偏见。我们观察到偏见句中的事件需要与文档中的其他事件相关联理解。因此，我们提出构建一个事件关系图，以明确推理句子级偏见识别中的事件-事件关系。设计的事件关系图由事件作为节点和四种常见类型的事件关系：指代、时间、因果和子事件关系组成。然后，我们将事件关系图纳入偏见句识别中，分两步进行：建立一个事件感知的语言模型，注入事件和事件关系知识。

    arXiv:2404.01722v1 Announce Type: new  Abstract: Media outlets are becoming more partisan and polarized nowadays. In this paper, we identify media bias at the sentence level, and pinpoint bias sentences that intend to sway readers' opinions. As bias sentences are often expressed in a neutral and factual way, considering broader context outside a sentence can help reveal the bias. In particular, we observe that events in a bias sentence need to be understood in associations with other events in the document. Therefore, we propose to construct an event relation graph to explicitly reason about event-event relations for sentence-level bias identification. The designed event relation graph consists of events as nodes and four common types of event relations: coreference, temporal, causal, and subevent relations. Then, we incorporate event relation graph for bias sentences identification in two steps: an event-aware language model is built to inject the events and event relations knowledge 
    
[^48]: 自我提升编程用于时间知识图问题回答

    Self-Improvement Programming for Temporal Knowledge Graph Question Answering

    [https://arxiv.org/abs/2404.01720](https://arxiv.org/abs/2404.01720)

    设计了基本的时间操作符，并引入了一种新颖的自我提升编程方法用于时间知识图问题回答，通过上下文学习能力来理解问题中的组合时间约束

    

    时间知识图问题回答（TKGQA）旨在回答关于时间意图的问题，涉及时间知识图（TKG）。该任务的核心挑战在于理解问题中关于多种类型时间约束（如 before、first 等）的复杂语义信息。现有的端到端方法通过学习问题和候选答案的时间感知嵌入隐式建模时间约束，这离全面理解问题还有很大差距。受显式建模问题中约束的语义解析方法的启发，通过生成带有符号操作符的逻辑形式，我们为时间约束设计了基本的时间操作符，并引入了一种新颖的自我提升编程方法用于 TKGQA（Prog-TQA）。具体而言，Prog-TQA利用大型语言模型（LLMs）的上下文学习能力来理解问题中的组合时间约束。

    arXiv:2404.01720v1 Announce Type: new  Abstract: Temporal Knowledge Graph Question Answering (TKGQA) aims to answer questions with temporal intent over Temporal Knowledge Graphs (TKGs). The core challenge of this task lies in understanding the complex semantic information regarding multiple types of time constraints (e.g., before, first) in questions. Existing end-to-end methods implicitly model the time constraints by learning time-aware embeddings of questions and candidate answers, which is far from understanding the question comprehensively. Motivated by semantic-parsing-based approaches that explicitly model constraints in questions by generating logical forms with symbolic operators, we design fundamental temporal operators for time constraints and introduce a novel self-improvement Programming method for TKGQA (Prog-TQA). Specifically, Prog-TQA leverages the in-context learning ability of Large Language Models (LLMs) to understand the combinatory time constraints in the question
    
[^49]: 有效的内部语言模型训练和融合对于分解转录模型

    Effective internal language model training and fusion for factorized transducer model

    [https://arxiv.org/abs/2404.01716](https://arxiv.org/abs/2404.01716)

    提出了一种对于分解转录模型的有效ILM训练和解码策略，能够显著改善与标准解码方法的性能，并在LibriSpeech数据集上取得了17%的相对改善。

    

    神经转录器的内部语言模型（ILM）已被广泛研究。在大多数先前的工作中，它主要用于估计ILM分数，并在推理过程中随后被减去，以促进与外部语言模型更好的集成。最近，已提出了各种分解转录模型，明确采用独立的内部语言模型用于非空白令牌预测。然而，即使采用了分解转录模型，与浅层融合相比，改进仍然有限。在本文中，我们提出了一种新颖的ILM训练和解码策略，用于分解转录模型，有效地结合了空白、声学和ILM分数。我们的实验表明，在LibriSpeech数据集上利用经过良好训练的ILM和所提出的解码策略时，相对于标准解码方法，有17%的相对改善。此外，与强RNN-T ba相比

    arXiv:2404.01716v1 Announce Type: cross  Abstract: The internal language model (ILM) of the neural transducer has been widely studied. In most prior work, it is mainly used for estimating the ILM score and is subsequently subtracted during inference to facilitate improved integration with external language models. Recently, various of factorized transducer models have been proposed, which explicitly embrace a standalone internal language model for non-blank token prediction. However, even with the adoption of factorized transducer models, limited improvement has been observed compared to shallow fusion. In this paper, we propose a novel ILM training and decoding strategy for factorized transducer models, which effectively combines the blank, acoustic and ILM scores. Our experiments show a 17% relative improvement over the standard decoding method when utilizing a well-trained ILM and the proposed decoding strategy on LibriSpeech datasets. Furthermore, when compared to a strong RNN-T ba
    
[^50]: EMONA: 新闻文章中的事件级别道德观点

    EMONA: Event-level Moral Opinions in News Articles

    [https://arxiv.org/abs/2404.01715](https://arxiv.org/abs/2404.01715)

    本文提出了一个新任务，即理解新闻文章中对事件的道德观点，并创建了一个包含事件级别道德观点的新数据集EMONA，为提取事件的道德性在新闻文章中建立了基线模型和进行外部评估。

    

    大部分关于道德框架的研究集中在社交媒体的短文本上，很少有研究探索新闻文章中的道德情绪。在新闻文章中，作者经常通过对事件的道德判断来表达自己的观点或政治立场，具体来说是根据社会道德规则判断事件是对还是错。本文开始了一项新的任务，即理解新闻文章中对事件的道德观点。我们创建了一个新的数据集EMONA，并对新闻文章中的事件级别道德观点进行了注释。该数据集包含400篇新闻文章，包含超过1万个句子和4.5万个事件，其中9613个事件获得了道德基础标签。提取事件的道德性是一项具有挑战性的任务，因为对事件的道德判断可能非常隐晦。我们建立了用于事件道德识别和分类的基线模型。此外，我们还进行了外部评估，以整合事件级别的道德观点。

    arXiv:2404.01715v1 Announce Type: new  Abstract: Most previous research on moral frames has focused on social media short texts, little work has explored moral sentiment within news articles. In news articles, authors often express their opinions or political stance through moral judgment towards events, specifically whether the event is right or wrong according to social moral rules. This paper initiates a new task to understand moral opinions towards events in news articles. We have created a new dataset, EMONA, and annotated event-level moral opinions in news articles. This dataset consists of 400 news articles containing over 10k sentences and 45k events, among which 9,613 events received moral foundation labels. Extracting event morality is a challenging task, as moral judgment towards events can be very implicit. Baseline models were built for event moral identification and classification. In addition, we also conduct extrinsic evaluations to integrate event-level moral opinions 
    
[^51]: 生成式人工智能用于沉浸式通信：通过6G探索感知互联网的下一个领域

    Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G

    [https://arxiv.org/abs/2404.01713](https://arxiv.org/abs/2404.01713)

    本文探讨了生成式人工智能用于沉浸式通信中减少带宽消耗的实用价值。

    

    在过去的二十年中，物联网(IoT)已经是一个具有变革性的概念，当我们逼近2030年时，一个新的范式被称为感知互联网(IoS)正在兴起。与传统的虚拟现实（VR）不同，IoS旨在提供多感官体验，认识到在我们的现实世界中，我们的感知远不止于视觉和听觉；它涵盖了一系列感觉。本文探讨了推动沉浸式多感官媒体的现有技术，深入探讨它们的功能和潜在应用。这项探索包括传统沉浸式媒体流与一个提出的利用生成式人工智能增强语义交流的用例之间的比较分析。这项分析的重点是所提方案中带宽消耗减少了99.93%。通过这种比较，我们旨在强调该实用应用的价值。

    arXiv:2404.01713v1 Announce Type: cross  Abstract: Over the past two decades, the Internet-of-Things (IoT) has been a transformative concept, and as we approach 2030, a new paradigm known as the Internet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR), IoS seeks to provide multi-sensory experiences, acknowledging that in our physical reality, our perception extends far beyond just sight and sound; it encompasses a range of senses. This article explores existing technologies driving immersive multi-sensory media, delving into their capabilities and potential applications. This exploration includes a comparative analysis between conventional immersive media streaming and a proposed use case that lever- ages semantic communication empowered by generative Artificial Intelligence (AI). The focal point of this analysis is the substantial reduction in bandwidth consumption by 99.93% in the proposed scheme. Through this comparison, we aim to underscore the practical appli
    
[^52]: 观点总结的极性校准

    Polarity Calibration for Opinion Summarization

    [https://arxiv.org/abs/2404.01706](https://arxiv.org/abs/2404.01706)

    引入极性校准概念，开发强化训练方法，通过平衡极性校准、内容保留和语言自然性，解决观点总结中放大极性偏见的问题。

    

    Opinion summarization 是自动从各种主观信息中生成摘要，如产品评论或政治观点。观点总结的挑战在于呈现不同或甚至相互矛盾的观点。我们对先前的总结模型进行了分析，发现它们倾向于放大极性偏见，强调大多数意见，而忽略少数派观点。为解决这个问题并让总结器表达两方观点，我们引入了极性校准的概念，旨在使输出摘要的极性与输入文本一致。具体来说，我们开发了一种强化训练方法用于极性校准。该方法将输出摘要与输入文本之间的极性距离作为奖励输入到总结器中，并平衡极性校准、内容保留和语言自然性。我们评估了我们的 Polarit

    arXiv:2404.01706v1 Announce Type: new  Abstract: Opinion summarization is automatically generating summaries from a variety of subjective information, such as product reviews or political opinions. The challenge of opinions summarization lies in presenting divergent or even conflicting opinions. We conduct an analysis of previous summarization models, which reveals their inclination to amplify the polarity bias, emphasizing the majority opinions while ignoring the minority opinions. To address this issue and make the summarizer express both sides of opinions, we introduce the concept of polarity calibration, which aims to align the polarity of output summary with that of input text. Specifically, we develop a reinforcement training approach for polarity calibration. This approach feeds the polarity distance between output summary and input text as reward into the summarizer, and also balance polarity calibration with content preservation and language naturality. We evaluate our Polarit
    
[^53]: 论文摘要中总结内容单元在文本摘要评估中的作用

    On the Role of Summary Content Units in Text Summarization Evaluation

    [https://arxiv.org/abs/2404.01701](https://arxiv.org/abs/2404.01701)

    本研究探讨了两种新颖策略来逼近摘要内容单元（SCUs），分别是从AMR意义表示（SMUs）和大型语言模型（SGUs）生成SCU近似。

    

    金字塔评估方法的核心是人类撰写的总结内容单元（SCUs）。这些SCUs是简洁的句子，将一个摘要分解为小事实。这些SCUs可以用来评判候选摘要的质量，可能通过自然语言推理（NLI）系统部分自动化。有趣的是，为了完全自动化金字塔评估，Zhang和Bansal（2021）表明SCUs可以通过自动生成的语义角色三元组（STUs）来近似。然而，目前还有一些问题没有答案，特别是：i）是否有其他逼近SCUs的方式能够提供优势？ii）在哪些条件下SCUs（或它们的近似）提供最大价值？在这项工作中，我们研究了两种新颖的策略来逼近SCUs：分别从AMR意义表示（SMUs）和大型语言模型（SGUs）生成SCU近似。我们发现

    arXiv:2404.01701v1 Announce Type: new  Abstract: At the heart of the Pyramid evaluation method for text summarization lie human written summary content units (SCUs). These SCUs are concise sentences that decompose a summary into small facts. Such SCUs can be used to judge the quality of a candidate summary, possibly partially automated via natural language inference (NLI) systems. Interestingly, with the aim to fully automate the Pyramid evaluation, Zhang and Bansal (2021) show that SCUs can be approximated by automatically generated semantic role triplets (STUs). However, several questions currently lack answers, in particular: i) Are there other ways of approximating SCUs that can offer advantages? ii) Under which conditions are SCUs (or their approximations) offering the most value? In this work, we examine two novel strategies to approximate SCUs: generating SCU approximations from AMR meaning representations (SMUs) and from large language models (SGUs), respectively. We find that 
    
[^54]: 社交媒体上的事件检测用于疫情预测

    Event Detection from Social Media for Epidemic Prediction

    [https://arxiv.org/abs/2404.01679](https://arxiv.org/abs/2404.01679)

    通过开发一个从社交媒体帖子中提取和分析流行病相关事件的框架，该研究首次利用事件检测技术在COVID-19流行病和其他未见流行病中实现有效的早期预警。

    

    社交媒体是一个易于访问的平台，提供有关社会趋势和事件的及时更新。关于流行病相关事件的讨论，如感染、症状和社会互动，对于在流行病爆发期间制定政策至关重要。在我们的工作中，我们开创利用事件检测（ED）来更好地准备和提前预警任何即将到来的流行病，通过开发一个框架从社交媒体帖子中提取和分析与流行病相关的事件。为此，我们策划了一个包括七种与疾病无关的事件类型的流行病事件本体论，并构建了一个名为SPEED的Twitter数据集，其中包含人工注释的重点关注COVID-19流行病的事件。实验揭示了在COVID速度上训练的ED模型如何有效地检测出关于三种未见流行病（猴痘、寨卡病毒和登革热）的流行病事件；而在现有ED数据集上训练的模型则表现不佳。此外，我们展示了报道

    arXiv:2404.01679v1 Announce Type: new  Abstract: Social media is an easy-to-access platform providing timely updates about societal trends and events. Discussions regarding epidemic-related events such as infections, symptoms, and social interactions can be crucial for informing policymaking during epidemic outbreaks. In our work, we pioneer exploiting Event Detection (ED) for better preparedness and early warnings of any upcoming epidemic by developing a framework to extract and analyze epidemic-related events from social media posts. To this end, we curate an epidemic event ontology comprising seven disease-agnostic event types and construct a Twitter dataset SPEED with human-annotated events focused on the COVID-19 pandemic. Experimentation reveals how ED models trained on COVID-based SPEED can effectively detect epidemic events for three unseen epidemics of Monkeypox, Zika, and Dengue; while models trained on existing ED datasets fail miserably. Furthermore, we show that reporting 
    
[^55]: 通过归结反驳实现自然语言通用且可靠的逻辑推理

    Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation

    [https://arxiv.org/abs/2404.01677](https://arxiv.org/abs/2404.01677)

    通过引入归结反驳范式，提出了一个名为GFaiR的框架，旨在解决大型语言模型在进行自然语言形式逻辑理论一阶逻辑推理时的困难，并通过证明插入解决方案改进了系统的完整性

    

    大型语言模型在各种自然语言推理任务中取得了显著的性能，但它们在对自然语言表达的形式逻辑理论进行一阶逻辑推理方面仍然有困难。为了解决这一问题，我们提出了一个名为“可泛化且可靠推理器（GFaiR）”的新框架，引入了归结反驳范式。实验结果表明，我们的系统...

    arXiv:2404.01677v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved significant performance in various natural language reasoning tasks. However, they still struggle with performing first-order logic reasoning over formal logical theories expressed in natural language. This is because the previous LLMs-based reasoning systems have the theoretical incompleteness issue. As a result, it can only address a limited set of simple reasoning problems, which significantly decreases their generalization ability. To address this issue, we propose a novel framework, named Generalizable and Faithful Reasoner (GFaiR), which introduces the paradigm of resolution refutation. Resolution refutation has the capability to solve all first-order logic reasoning problems by extending reasoning rules and employing the principle of proof by contradiction, so our system's completeness can be improved by introducing resolution refutation. Experimental results demonstrate that our system o
    
[^56]: METAL: 迈向多语言元评估

    METAL: Towards Multilingual Meta-Evaluation

    [https://arxiv.org/abs/2404.01667](https://arxiv.org/abs/2404.01667)

    提出了一个用于多语言场景下LLMs评估的端到端框架，并创建了一个涵盖10种语言的精心策划数据集。

    

    随着大型语言模型（LLMs）在许多任务中表现出越来越接近人类的精度，它们在各种实际应用中的利用变得更加普遍。本研究提出了一个用于多语言场景下LLMs评估的端到端框架，创建了一个精心策划的数据集，涵盖10种语言，包含了原生说话者对摘要任务的判断。

    arXiv:2404.01667v1 Announce Type: new  Abstract: With the rising human-like precision of Large Language Models (LLMs) in numerous tasks, their utilization in a variety of real-world applications is becoming more prevalent. Several studies have shown that LLMs excel on many standard NLP benchmarks. However, it is challenging to evaluate LLMs due to test dataset contamination and the limitations of traditional metrics. Since human evaluations are difficult to collect, there is a growing interest in the community to use LLMs themselves as reference-free evaluators for subjective metrics. However, past work has shown that LLM-based evaluators can exhibit bias and have poor alignment with human judgments. In this study, we propose a framework for an end-to-end assessment of LLMs as evaluators in multilingual scenarios. We create a carefully curated dataset, covering 10 languages containing native speaker judgments for the task of summarization. This dataset is created specifically to evalua
    
[^57]: CMAT: 用于增强小型语言模型的多智能体协作调整框架

    CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models

    [https://arxiv.org/abs/2404.01663](https://arxiv.org/abs/2404.01663)

    CMAT框架引入了TinyAgent模型，并提出了一种新颖的系统，通过环境反馈进行自适应权重更新，增强了语言智能体的能力和长期记忆。

    

    开放的大型语言模型（LLMs）显著推动了自然语言处理领域的发展，在各种任务中展现出卓越的性能。尽管LLMs取得了显著进展，但它们的有效操作仍然严重依赖于人类输入来准确引导对话流程，智能体调整是一种关键的优化技术，涉及人类对模型的调整，以更好地响应这种引导。针对这一依赖性，我们的工作引入了TinyAgent模型，该模型经过精心策划的高质量数据集训练。我们还提出了Collaborative Multi-Agent Tuning（CMAT）框架，这是一个创新性系统，旨在通过根据环境反馈进行自适应权重更新来增强语言智能体的能力。该框架促进了多个智能体之间的协作学习和实时适应，增强了它们的上下文感知和长期记忆。

    arXiv:2404.01663v1 Announce Type: new  Abstract: Open large language models (LLMs) have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.Despite the significant advancements in LLMs, their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset. We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback. This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory. In this resear
    
[^58]: 发布针对日语的预训练模型

    Release of Pre-Trained Models for the Japanese Language

    [https://arxiv.org/abs/2404.01657](https://arxiv.org/abs/2404.01657)

    发布日语预训练模型以缩小非英语社区中的AI访问差距，促进AI民主化。

    

    arXiv:2404.01657v1 公告类型:交叉摘要: AI民主化旨在创造一个普通人可以利用AI技术的世界。为了实现这一目标，许多研究机构已经试图让他们的结果对公众可及。特别是，基于大规模数据训练的大型预训练模型展现了前所未有的潜力，它们的发布产生了重大影响。然而，大多数发布的模型专门针对英语，因此，在非英语社区中，AI民主化存在明显滞后。为了缩小AI访问的差距，我们发布了用日语预先训练的生成式预训练转换器（GPT）、对比语言和图像预训练（CLIP）、稳定扩散和隐藏单元双向编码器表示来自变压器（HuBERT）。通过提供这些模型，用户可以自由地与符合日本文化价值观的AI进行交互，并确保日本文化的身份，从而增强

    arXiv:2404.01657v1 Announce Type: cross  Abstract: AI democratization aims to create a world in which the average person can utilize AI techniques. To achieve this goal, numerous research institutes have attempted to make their results accessible to the public. In particular, large pre-trained models trained on large-scale data have shown unprecedented potential, and their release has had a significant impact. However, most of the released models specialize in the English language, and thus, AI democratization in non-English-speaking communities is lagging significantly. To reduce this gap in AI access, we released Generative Pre-trained Transformer (GPT), Contrastive Language and Image Pre-training (CLIP), Stable Diffusion, and Hidden-unit Bidirectional Encoder Representations from Transformers (HuBERT) pre-trained in Japanese. By providing these models, users can freely interface with AI that aligns with Japanese cultural values and ensures the identity of Japanese culture, thus enha
    
[^59]: 通过缓解上下文记忆实现开放域问答中更好的泛化

    Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization

    [https://arxiv.org/abs/2404.01652](https://arxiv.org/abs/2404.01652)

    本文研究了在开放域问答中泛化性能的问题，发现挑战在于阅读器过度依赖记忆外部语料库的知识，限制了模型的泛化能力。

    

    开放域问答（OpenQA）旨在利用外部大规模知识语料库回答事实问题。然而，现实世界中的知识并非静态的；它不断更新和演变。这种知识的动态特性为这些模型带来了重要挑战，因为训练的模型需要不断适应最新信息，以确保答案保持准确。此外，目前尚不清楚开放域问答模型能够多好地转移到完全新的知识领域。本文研究了一个基于检索增强的QA模型在两种具体情景下的泛化性能：1）适应相同知识语料库的更新版本；2）转换到完全不同的知识领域。我们发现，开放域问答模型的泛化挑战源自阅读器过度依赖从外部语料库中记忆知识，从而阻碍了模型的泛化能力。

    arXiv:2404.01652v1 Announce Type: cross  Abstract: Open-domain Question Answering (OpenQA) aims at answering factual questions with an external large-scale knowledge corpus. However, real-world knowledge is not static; it updates and evolves continually. Such a dynamic characteristic of knowledge poses a vital challenge for these models, as the trained models need to constantly adapt to the latest information to make sure that the answers remain accurate. In addition, it is still unclear how well an OpenQA model can transfer to completely new knowledge domains. In this paper, we investigate the generalization performance of a retrieval-augmented QA model in two specific scenarios: 1) adapting to updated versions of the same knowledge corpus; 2) switching to completely different knowledge domains. We observe that the generalization challenges of OpenQA models stem from the reader's over-reliance on memorizing the knowledge from the external corpus, which hinders the model from generaliz
    
[^60]: 无法区分使用和提及的自然语言处理系统会抑制抗议言论，而教导这个区分有助于改善

    NLP Systems That Can't Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps

    [https://arxiv.org/abs/2404.01651](https://arxiv.org/abs/2404.01651)

    NLP 系统无法区分使用和提及，对于处理在线抗议言论至关重要，教导这种区分有助于减少虚假信息和仇恨言论检测中的审查错误

    

    传统上，使用词语传达说话者的意图与“提及”词语引述他人的话语或指出词语的属性是有所区别的。在这里我们展示，计算模拟这种使用-提及区别对于处理线上抗议言论至关重要。驳斥有问题内容的抗议言论通常提及有害语言但本身并不有害（例如，称疫苗危险并不等同于对某人表示反对称呼疫苗危险）。我们展示，即使是最近的语言模型也无法区分使用和提及，而这个失败会延伸到两个关键的下游任务：虚假信息和仇恨言论检测，导致对抗议言论的审查。我们引入提示缓解措施来教导使用-提及区别，并展示它们减少了这些错误。我们的工作突显了使用-提及区别对自然语言处理和CSS的重要性，并提供了…

    arXiv:2404.01651v1 Announce Type: new  Abstract: The use of words to convey speaker's intent is traditionally distinguished from the `mention' of words for quoting what someone said, or pointing out properties of a word. Here we show that computationally modeling this use-mention distinction is crucial for dealing with counterspeech online. Counterspeech that refutes problematic content often mentions harmful language but is not harmful itself (e.g., calling a vaccine dangerous is not the same as expressing disapproval of someone for calling vaccines dangerous). We show that even recent language models fail at distinguishing use from mention, and that this failure propagates to two key downstream tasks: misinformation and hate speech detection, resulting in censorship of counterspeech. We introduce prompting mitigations that teach the use-mention distinction, and show they reduce these errors. Our work highlights the importance of the use-mention distinction for NLP and CSS and offers 
    
[^61]: 通过融合实体解码进行实体消歧

    Entity Disambiguation via Fusion Entity Decoding

    [https://arxiv.org/abs/2404.01626](https://arxiv.org/abs/2404.01626)

    提出了一种通过融合实体描述进行实体消歧的编码-解码模型。

    

    实体消歧（ED）是将模糊实体的提及链接到知识库中的指代实体的过程，在实体链接（EL）中起着核心作用。现有的生成式方法在标准化的ZELDA基准下展示出比分类方法更高的准确性。然而，生成式方法需要大规模的预训练且生成效率低下。最重要的是，实体描述经常被忽视，而这些描述可能包含区分相似实体的关键信息。我们提出了一种编码-解码模型，以更详细的实体描述来进行实体消歧。给定文本和候选实体，编码器学习文本与每个候选实体之间的交互，为每个实体候选产生表示。解码器随后将实体候选的表示融合在一起，并选择正确的实体。

    arXiv:2404.01626v1 Announce Type: new  Abstract: Entity disambiguation (ED), which links the mentions of ambiguous entities to their referent entities in a knowledge base, serves as a core component in entity linking (EL). Existing generative approaches demonstrate improved accuracy compared to classification approaches under the standardized ZELDA benchmark. Nevertheless, generative approaches suffer from the need for large-scale pre-training and inefficient generation. Most importantly, entity descriptions, which could contain crucial information to distinguish similar entities from each other, are often overlooked. We propose an encoder-decoder model to disambiguate entities with more detailed entity descriptions. Given text and candidate entities, the encoder learns interactions between the text and each candidate entity, producing representations for each entity candidate. The decoder then fuses the representations of entity candidates together and selects the correct entity. Our 
    
[^62]: 将LLMs转化为跨模态和跨语言检索系统

    Transforming LLMs into Cross-modal and Cross-lingual RetrievalSystems

    [https://arxiv.org/abs/2404.01616](https://arxiv.org/abs/2404.01616)

    提出使用LLMs初始化多模态DE检索系统，实现在102种语言中匹配语音和文本的能力，无需在LLM预训练期间使用语音数据，且相比先前系统取得10%的Recall@1绝对改进

    

    大型语言模型（LLMs）是在仅基于文本数据进行训练的，这超出了具有配对语音和文本数据的语言范围。同时，基于双编码器（DE）的检索系统将查询和文档投影到相同的嵌入空间中，并在检索和双语文本挖掘中展示了成功。为了在许多语言中匹配语音和文本，我们建议使用LLMs初始化多模态DE检索系统。与传统方法不同，我们的系统在LLM预训练期间不需要语音数据，并且可以利用LLM的多语言文本理解能力来匹配检索训练期间看不见的语言中的语音和文本。我们的多模态LLM-based检索系统能够在102种语言中匹配语音和文本，尽管只在21种语言上进行了训练。我们的系统优于先前专门在所有102种语言上训练的系统。在这些语言中，我们在Recall@1上实现了10％的绝对改进。

    arXiv:2404.01616v1 Announce Type: new  Abstract: Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn't require speech data during LLM pre-training and can exploit LLM's multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these l
    
[^63]: 大语言模型在狼人游戏中的舵手？评估其观点引领作用

    Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game

    [https://arxiv.org/abs/2404.01602](https://arxiv.org/abs/2404.01602)

    本研究通过狼人游戏模拟平台评估了大语言模型的观点领导作用，并开发了两个新的评估指标。

    

    大语言模型（LLMs）在社交推理游戏中展现出令人难忘的战略行为。然而，LLM代理所展示的观点领导力的重要性被忽视了，而这对于多智能体和人工智能交互设置中的实际应用至关重要。在此研究中，我们利用狼人游戏作为模拟平台，评估LLMs的观点引领作用。该游戏中有警长角色，负责总结论据并推荐决策选项，因此可作为观点领袖的可信代理。我们开发了一个整合了警长角色的框架，并设计了两个基于观点领袖关键特征的新指标进行评估。第一个度量标准衡量观点领袖的可靠性，第二个评估...

    arXiv:2404.01602v1 Announce Type: cross  Abstract: Large language models (LLMs) have exhibited memorable strategic behaviors in social deductive games. However, the significance of opinion leadership exhibited by LLM-based agents has been overlooked, which is crucial for practical applications in multi-agent and human-AI interaction settings. Opinion leaders are individuals who have a noticeable impact on the beliefs and behaviors of others within a social group. In this work, we employ the Werewolf game as a simulation platform to assess the opinion leadership of LLMs. The game features the role of the Sheriff, tasked with summarizing arguments and recommending decision options, and therefore serves as a credible proxy for an opinion leader. We develop a framework integrating the Sheriff role and devise two novel metrics for evaluation based on the critical characteristics of opinion leaders. The first metric measures the reliability of the opinion leader, and the second assesses the 
    
[^64]: 使用开源临床大型语言模型对癌症分期进行分类

    Classifying Cancer Stage with Open-Source Clinical Large Language Models

    [https://arxiv.org/abs/2404.01589](https://arxiv.org/abs/2404.01589)

    本研究展示了在没有任何标记的训练数据的情况下，开源的临床大型语言模型能够从真实病理报告中提取病理性肿瘤-淋巴结-转移（pTNM）分期信息

    

    癌症分期分类对于为肿瘤学患者制定治疗和护理管理计划至关重要。分期信息通常以非结构化形式包含在临床、病理学、放射学和其他自由文本报告中，需要大量工作来解析和获取。为了促进这些信息的提取，先前的自然语言处理方法依赖于标记的训练数据集，这些数据集准备起来需要大量的人力。本研究展示了在没有任何标记的训练数据的情况下，开源的临床大型语言模型（LLMs）可以从真实病理报告中提取病理性肿瘤-淋巴结-转移（pTNM）分期信息。我们的实验比较了LLMs和使用标记数据进行微调的基于BERT的模型。我们的发现表明，虽然LLMs在肿瘤（T）分类方面仍然表现不佳，但通过适当采用提示策略，它们可以实现

    arXiv:2404.01589v1 Announce Type: cross  Abstract: Cancer stage classification is important for making treatment and care management plans for oncology patients. Information on staging is often included in unstructured form in clinical, pathology, radiology and other free-text reports in the electronic health record system, requiring extensive work to parse and obtain. To facilitate the extraction of this information, previous NLP approaches rely on labeled training datasets, which are labor-intensive to prepare. In this study, we demonstrate that without any labeled training data, open-source clinical large language models (LLMs) can extract pathologic tumor-node-metastasis (pTNM) staging information from real-world pathology reports. Our experiments compare LLMs and a BERT-based model fine-tuned using the labeled data. Our findings suggest that while LLMs still exhibit subpar performance in Tumor (T) classification, with the appropriate adoption of prompting strategies, they can achi
    
[^65]: 基于幻觉多样性的文本摘要主动学习

    Hallucination Diversity-Aware Active Learning for Text Summarization

    [https://arxiv.org/abs/2404.01588](https://arxiv.org/abs/2404.01588)

    本文首次提出了一种基于幻觉多样性的主动学习框架，用于减轻大型语言模型（LLMs）在文本摘要中产生的幻觉，减少了昂贵的人类注释需求。

    

    大型语言模型（LLMs）已经表现出生成幻觉输出的倾向，即在事实上不正确或不支持的文本。现有的减轻幻觉的方法通常需要昂贵的人类注释来识别和纠正LLMs输出中的幻觉。此外，大多数这些方法专注于特定类型的幻觉，例如实体或标记错误，这限制了它们在解决LLMs输出中展示的各种类型的幻觉方面的有效性。据我们所知，本文提出了第一个旨在减轻LLMs幻觉的主动学习框架，降低了对幻觉所需的昂贵人类注释。通过在文本摘要中衡量语义框架、议论和内容可验证性错误中的细粒度幻觉，我们提出了 HAllucination Diversity-Aware Sampling（HADAS）来选择多样化的幻觉，以供LLM微调的主动学习注释。

    arXiv:2404.01588v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown propensity to generate hallucinated outputs, i.e., texts that are factually incorrect or unsupported. Existing methods for alleviating hallucinations typically require costly human annotations to identify and correct hallucinations in LLM outputs. Moreover, most of these methods focus on a specific type of hallucination, e.g., entity or token errors, which limits their effectiveness in addressing various types of hallucinations exhibited in LLM outputs. To our best knowledge, in this paper we propose the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed. By measuring fine-grained hallucinations from errors in semantic frame, discourse and content verifiability in text summarization, we propose HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning
    
[^66]: 基于BERT增强的作业抄袭检测系统

    BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System

    [https://arxiv.org/abs/2404.01582](https://arxiv.org/abs/2404.01582)

    本文提出了一种基于GPT-3.5的抄袭文本数据生成方法和一种基于Faiss和BERT的高效高准确性的抄袭识别方法，填补了高水平抄袭检测研究数据集缺失的空白，实验证明该模型在多个指标上表现优异

    

    文本抄袭检测任务是一项常见的自然语言处理任务，旨在检测给定文本是否包含从其他文本中抄袭或复制的内容。在现有研究中，由于缺乏高质量的数据集，检测高水平的抄袭仍然是一个挑战。本文提出了一种基于GPT-3.5的抄袭文本数据生成方法，产生了32,927对文本抄袭检测数据集，涵盖了各种抄袭方法，填补了这一研究领域的空白。同时，我们提出了一种基于Faiss和BERT的高效高准确性的抄袭识别方法。我们的实验证明，这种模型在准确率、精确率、召回率和F1分数等多个指标上的表现优于其他模型，分别达到了98.86％、98.90％、98.86％和0.9888。最后，我们还提供了一个用户友好的演示平台，允许用户上传文本。

    arXiv:2404.01582v1 Announce Type: cross  Abstract: Text plagiarism detection task is a common natural language processing task that aims to detect whether a given text contains plagiarism or copying from other texts. In existing research, detection of high level plagiarism is still a challenge due to the lack of high quality datasets. In this paper, we propose a plagiarized text data generation method based on GPT-3.5, which produces 32,927 pairs of text plagiarism detection datasets covering a wide range of plagiarism methods, bridging the gap in this part of research. Meanwhile, we propose a plagiarism identification method based on Faiss with BERT with high efficiency and high accuracy. Our experiments show that the performance of this model outperforms other models in several metrics, including 98.86\%, 98.90%, 98.86%, and 0.9888 for Accuracy, Precision, Recall, and F1 Score, respectively. At the end, we also provide a user-friendly demo platform that allows users to upload a text 
    
[^67]: 使用对比集评估大型语言模型：一种实验方法

    Evaluating Large Language Models Using Contrast Sets: An Experimental Approach

    [https://arxiv.org/abs/2404.01569](https://arxiv.org/abs/2404.01569)

    介绍了一种为斯坦福自然语言推断（SNLI）数据集生成对比集的创新技术，通过自动替换动词、副词和形容词为同义词来评估模型的性能是否基于真实的语言理解还是仅仅基于模式识别。

    

    在自然语言推断（NLI）领域，尤其是涉及多个输入文本分类的任务中，交叉熵损失度量被广泛应用作为错误度量的标准。然而，该度量在有效评估模型理解语句蕴涵能力方面存在不足。本研究引入了一种创新的技术，用于为斯坦福自然语言推断（SNLI）数据集生成对比集。我们的策略涉及自动将动词、副词和形容词替换为它们的同义词，以保留句子的原始含义。该方法旨在评估模型的性能是否基于真实的语言理解还是仅仅基于模式识别。我们使用ELECTRA-small模型进行了分析。该模型在传统的SNLI数据集上实现了89.9%的准确度，但在我们的对比集上显示出了72.5%的准确度，表明

    arXiv:2404.01569v1 Announce Type: cross  Abstract: In the domain of Natural Language Inference (NLI), especially in tasks involving the classification of multiple input texts, the Cross-Entropy Loss metric is widely employed as a standard for error measurement. However, this metric falls short in effectively evaluating a model's capacity to understand language entailments. In this study, we introduce an innovative technique for generating a contrast set for the Stanford Natural Language Inference (SNLI) dataset. Our strategy involves the automated substitution of verbs, adverbs, and adjectives with their synonyms to preserve the original meaning of sentences. This method aims to assess whether a model's performance is based on genuine language comprehension or simply on pattern recognition. We conducted our analysis using the ELECTRA-small model. The model achieved an accuracy of 89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5% on our contrast set, indicati
    
[^68]: Octopus: 软件API函数调用的设备端语言模型

    Octopus: On-device language model for function calling of software APIs

    [https://arxiv.org/abs/2404.01549](https://arxiv.org/abs/2404.01549)

    本研究介绍了一种利用设备端语言模型进行软件API函数调用的新策略，通过优化模型对API结构和语法的理解，显著提高了API函数调用的准确性。

    

    在人工智能不断发展的领域中，由于其先进的文本处理和生成能力，大型语言模型（LLMs）发挥着关键作用。本研究介绍了一种旨在利用设备端LLMs调用软件API的新策略。我们精心编制了从软件API文档中提取的数据集，并对具有2B、3B和7B参数容量的LLMs进行微调，以提高它们在软件API交互中的熟练程度。我们的方法集中于改进模型对API结构和语法的理解，显著提高了API函数调用的准确性。此外，我们提出了“条件屏蔽”技术，以确保输出符合期望的格式，并减少错误率，同时保持推理速度。我们还提出了一种旨在评估LLMs在API交互中效果的新基准，并为随后的研究奠定了基础。

    arXiv:2404.01549v1 Announce Type: new  Abstract: In the rapidly evolving domain of artificial intelligence, Large Language Models (LLMs) play a crucial role due to their advanced text processing and generation abilities. This study introduces a new strategy aimed at harnessing on-device LLMs in invoking software APIs. We meticulously compile a dataset derived from software API documentation and apply fine-tuning to LLMs with capacities of 2B, 3B and 7B parameters, specifically to enhance their proficiency in software API interactions. Our approach concentrates on refining the models' grasp of API structures and syntax, significantly enhancing the accuracy of API function calls. Additionally, we propose \textit{conditional masking} techniques to ensure outputs in the desired formats and reduce error rates while maintaining inference speeds. We also propose a novel benchmark designed to evaluate the effectiveness of LLMs in API interactions, establishing a foundation for subsequent resea
    
[^69]: 放置锚点：在语言建模中给数字语义上的引导

    Laying Anchors: Semantically Priming Numerals in Language Modeling

    [https://arxiv.org/abs/2404.01536](https://arxiv.org/abs/2404.01536)

    通过生成受数字分布规律控制的锚点，我们引入了一种在语义上引导数字的策略，在广泛范围的数字任务上实现了数学基础表示的显著改进。

    

    现有大量预训练语言模型已成为自然语言处理管线中的事实标准，然而这些模型未能正确编码数字，限制了它们在需要数字理解的任务上的性能。我们引入了一种策略，通过在任何语料库中生成受数字分布规律控制的锚点来在语义上引导数字，从而实现这些数字标记的数学基础表示。我们通过对一系列数值任务进行评估，证明了我们提出的技术的优越性，对领域内（已见）和领域外（未见）的数字都适用。此外，我们将实证评估扩展到从1到10亿的数字范围，比以往相同类型研究的范围广得多，展示了我们学得的嵌入向数学上的显著改进。

    arXiv:2404.01536v1 Announce Type: cross  Abstract: Off-the-shelf pre-trained language models have become the de facto standard in NLP pipelines for a multitude of downstream tasks. However, the inability of these models to properly encode numerals limits their performance on tasks requiring numeric comprehension. We introduce strategies to semantically prime numerals in any corpus by generating anchors governed by the distribution of numerals in said corpus, thereby enabling mathematically grounded representations of these numeral tokens. We establish the superiority of our proposed techniques through evaluation on a range of numeracy tasks for both in-domain (seen) and out-domain (unseen) numerals. Further, we expand our empirical evaluations to numerals ranging from 1 to 10 billion, a significantly broader range compared to previous studies of the same nature, and we demonstrate significant improvements in the mathematical grounding of our learned embeddings.
    
[^70]: 自动回归事件时间图生成的集合对齐框架

    Set-Aligning Framework for Auto-Regressive Event Temporal Graph Generation

    [https://arxiv.org/abs/2404.01532](https://arxiv.org/abs/2404.01532)

    提出了针对自动回归事件时间图生成的条件集合生成问题的集合对齐框架，用于解决线性化图和语言模型处理序列不匹配的挑战

    

    最近的研究使用预训练语言模型自回归生成线性化图，用于构建事件时间图，取得了令人满意的结果。然而，这些方法通常导致次优图生成，因为线性化图表现出集合特征，而语言模型则按顺序处理这些特征。我们重新构思了任务，将其作为条件集合生成问题，并提出了一种针对大型语言模型的集合对齐框架，以解决这些挑战。

    arXiv:2404.01532v1 Announce Type: new  Abstract: Event temporal graphs have been shown as convenient and effective representations of complex temporal relations between events in text. Recent studies, which employ pre-trained language models to auto-regressively generate linearised graphs for constructing event temporal graphs, have shown promising results. However, these methods have often led to suboptimal graph generation as the linearised graphs exhibit set characteristics which are instead treated sequentially by language models. This discrepancy stems from the conventional text generation objectives, leading to erroneous penalisation of correct predictions caused by the misalignment of elements in target sequences. To address these challenges, we reframe the task as a conditional set generation problem, proposing a Set-aligning Framework tailored for the effective utilisation of Large Language Models (LLMs). The framework incorporates data augmentations and set-property regularis
    
[^71]: AAdaM在SemEval-2024任务1中的表现：多语言语义文本相关性的增强与适应

    AAdaM at SemEval-2024 Task 1: Augmentation and Adaptation for Multilingual Semantic Textual Relatedness

    [https://arxiv.org/abs/2404.01490](https://arxiv.org/abs/2404.01490)

    提出利用机器翻译进行数据增强以解决训练数据有限的挑战，应用任务自适应预训练和适配器框架实现了竞争性结果。

    

    这篇论文介绍了我们为SemEval-2024任务1开发的系统：非洲和亚洲语言的语义文本相关性。该共享任务旨在衡量句子对之间的语义文本相关性，重点关注一系列代表性不足的语言。在这项工作中，我们提出利用机器翻译进行数据增强，以解决训练数据有限的低资源挑战。此外，我们将任务自适应预训练应用于未标记的任务数据，以弥合预训练和任务适应之间的差距。在模型训练方面，我们研究了完全微调和基于适配器的调整，并采用适配器框架实现了有效的零-shot跨语言转移。在共享任务中取得了竞争性结果：我们的系统在子任务A（监督学习）和子任务C（跨语言转移）中表现最佳，排名最高。

    arXiv:2404.01490v1 Announce Type: new  Abstract: This paper presents our system developed for the SemEval-2024 Task 1: Semantic Textual Relatedness for African and Asian Languages. The shared task aims at measuring the semantic textual relatedness between pairs of sentences, with a focus on a range of under-represented languages. In this work, we propose using machine translation for data augmentation to address the low-resource challenge of limited training data. Moreover, we apply task-adaptive pre-training on unlabeled task data to bridge the gap between pre-training and task adaptation. For model training, we investigate both full fine-tuning and adapter-based tuning, and adopt the adapter framework for effective zero-shot cross-lingual transfer. We achieve competitive results in the shared task: our system performs the best among all ranked teams in both subtask A (supervised learning) and subtask C (cross-lingual transfer).
    
[^72]: 关于多语言新闻框架分析扩展研究

    A Study on Scaling Up Multilingual News Framing Analysis

    [https://arxiv.org/abs/2404.01481](https://arxiv.org/abs/2404.01481)

    本研究扩展了框架分析至多语言环境，通过众包创建数据集并结合其他现有数据集，有效提升了结果，展示了众包的可行性。

    

    媒体框架是指有目的地选择和呈现政治问题特定方面以塑造公众舆论的研究。尽管与全球几乎所有社会都相关，但由于缺乏可用数据集和其他资源，研究受到限制。该研究探讨通过众包进行数据集创建的可能性，利用非专家注释者开发训练语料库。我们首先通过自动翻译将框架分析扩展到多语言环境（12种类型多样的语言）。我们还在孟加拉语和葡萄牙语领域提出了一个新颖的基准，涉及移民和同性婚姻。此外，我们展示了一个在我们的众包数据集上训练的系统，结合其他现有数据集，与基线相比增加了5.32个百分点，表明众包是一个可行的选择。最后，我们研究了大型语言模型的性能。

    arXiv:2404.01481v1 Announce Type: new  Abstract: Media framing is the study of strategically selecting and presenting specific aspects of political issues to shape public opinion. Despite its relevance to almost all societies around the world, research has been limited due to the lack of available datasets and other resources. This study explores the possibility of dataset creation through crowdsourcing, utilizing non-expert annotators to develop training corpora. We first extend framing analysis beyond English news to a multilingual context (12 typologically diverse languages) through automatic translation. We also present a novel benchmark in Bengali and Portuguese on the immigration and same-sex marriage domains. Additionally, we show that a system trained on our crowd-sourced dataset, combined with other existing ones, leads to a 5.32 percentage point increase from the baseline, showing that crowdsourcing is a viable option. Last, we study the performance of large language models (
    
[^73]: TraveLER：用于视频问答的多重LMM代理框架

    TraveLER: A Multi-LMM Agent Framework for Video Question-Answering

    [https://arxiv.org/abs/2404.01476](https://arxiv.org/abs/2404.01476)

    TraveLER是一种多重LMM代理框架，通过沿着视频移动，并通过交互式提问收集关键帧的信息，以解决视频问答中关键时间戳选择和错误时间戳调整的问题

    

    最近，大型多模态模型（LMMs）在视频问答方面取得了重要进展，通过利用大规模、基于图像的预训练以零样本方式以帧为单位进行处理。虽然基于图像的视频方法展现了令人印象深刻的性能，但目前的局限是它们经常忽视了如何选择关键时间戳，并且无法在确定错误时间戳时进行调整。此外，它们无法提取与问题相关的细节，而是提供帧的一般描述。为了克服这一点，我们设计了一个多重LMM代理框架，它沿着视频进行移动，通过交互式提问的方式迭代地从关键帧收集相关信息，直到获得足够的信息来回答问题。具体来说，我们提出了TraveLER，这是一个可以制定“遍历”视频计划的模型，询问关于单个帧的问题以“定位”并存储关键信息

    arXiv:2404.01476v1 Announce Type: cross  Abstract: Recently, Large Multimodal Models (LMMs) have made significant progress in video question-answering using a frame-wise approach by leveraging large-scale, image-based pretraining in a zero-shot manner. While image-based methods for videos have shown impressive performance, a current limitation is that they often overlook how key timestamps are selected and cannot adjust when incorrect timestamps are identified. Moreover, they are unable to extract details relevant to the question, instead providing general descriptions of the frame. To overcome this, we design a multi-LMM agent framework that travels along the video, iteratively collecting relevant information from keyframes through interactive question-asking until there is sufficient information to answer the question. Specifically, we propose TraveLER, a model that can create a plan to "Traverse" through the video, ask questions about individual frames to "Locate" and store key info
    
[^74]: 通过稳定排名概率寻找可复制的人类评估

    Finding Replicable Human Evaluations via Stable Ranking Probability

    [https://arxiv.org/abs/2404.01474](https://arxiv.org/abs/2404.01474)

    本研究通过机器翻译和其最先进的人类评估框架MQM作为案例研究，提出了关于设置可靠人类评估以得出稳定结论的方法，以及针对设计可复制人类评估研究的具体建议。

    

    可靠的人类评估对于成功开发自然语言生成模型至关重要，但实现可靠人类评估却极为困难。稳定性对于通过质量对系统进行排名至关重要：在重复评估中系统的一致排名不仅仅是可取的，而且是必不可少的。缺乏这一点，便无法为爬坡或产品推出决策提供可靠基础。本文以机器翻译及其最先进的人类评估框架MQM作为案例研究，以了解如何设立可靠的人类评估以得出稳定的结论。我们研究了分配给评估者的项目配置、每个项目的评分次数以及得分归一化的最佳配置。我们在两种语言对上进行的研究提供了针对设计可复制的人类评估研究的具体建议。我们还收集并发布了最大的公开可用的多段翻译评分数据集。

    arXiv:2404.01474v1 Announce Type: new  Abstract: Reliable human evaluation is critical to the development of successful natural language generation models, but achieving it is notoriously difficult. Stability is a crucial requirement when ranking systems by quality: consistent ranking of systems across repeated evaluations is not just desirable, but essential. Without it, there is no reliable foundation for hill-climbing or product launch decisions. In this paper, we use machine translation and its state-of-the-art human evaluation framework, MQM, as a case study to understand how to set up reliable human evaluations that yield stable conclusions. We investigate the optimal configurations for item allocation to raters, number of ratings per item, and score normalization. Our study on two language pairs provides concrete recommendations for designing replicable human evaluation studies. We also collect and release the largest publicly available dataset of multi-segment translations rate
    
[^75]: OpenChemIE：用于化学文献信息提取的工具包

    OpenChemIE: An Information Extraction Toolkit For Chemistry Literature

    [https://arxiv.org/abs/2404.01462](https://arxiv.org/abs/2404.01462)

    OpenChemIE提出了一种用于从化学文献中提取反应数据的工具包，通过整合文本、表格和图像信息以及使用专门神经模型和算法，实现了在文档级别的反应数据提取。

    

    arXiv:2404.01462v1 公告类型：交叉  摘要：从化学文献中提取信息对于构建数据驱动化学的最新反应数据库至关重要。完整的信息提取需要结合文本、表格和图像中的信息，而以往的工作主要研究从单一方式提取反应。在本文中，我们提出了OpenChemIE来解决这一复杂挑战，实现在文档级别提取反应数据。OpenChemIE分两步解决问题：从各个方式中提取相关信息，然后整合结果得到最终的反应列表。对于第一步，我们采用专门的神经模型，每个模型处理化学信息提取的特定任务，比如从文本或图像中解析分子或反应。然后我们使用化学相关的算法整合这些模块的信息，实现精细化反应提取。

    arXiv:2404.01462v1 Announce Type: cross  Abstract: Information extraction from chemistry literature is vital for constructing up-to-date reaction databases for data-driven chemistry. Complete extraction requires combining information across text, tables, and figures, whereas prior work has mainly investigated extracting reactions from single modalities. In this paper, we present OpenChemIE to address this complex challenge and enable the extraction of reaction data at the document level. OpenChemIE approaches the problem in two steps: extracting relevant information from individual modalities and then integrating the results to obtain a final list of reactions. For the first step, we employ specialized neural models that each address a specific task for chemistry information extraction, such as parsing molecules or reactions from text or figures. We then integrate the information from these modules using chemistry-informed algorithms, allowing for the extraction of fine-grained reactio
    
[^76]: 请真正的琳达站出来...面对大语言模型？在LLMs中审视代表性启发式

    Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs

    [https://arxiv.org/abs/2404.01461](https://arxiv.org/abs/2404.01461)

    该研究调查了代表性启发式对大型语言模型推理的影响，并创建了专门的数据集进行实验验证

    

    尽管大型语言模型（LLMs）在理解文本和生成类似人类文本方面表现出色，但它们可能会展现出从训练数据中获得的偏见。具体而言，LLMs可能会容易受到人类决策中的一种常见认知陷阱影响，即代表性启发式。这是心理学中的一个概念，指的是根据事件与一个众所周知的原型或典型例子的相似程度来判断事件发生的可能性，而不考虑更广泛的事实或统计证据。本研究调查了代表性启发式对LLM推理的影响。我们创建了REHEAT（Representativeness Heuristic AI Testing），一个包含涵盖六种常见代表性启发式类型问题的数据集。实验显示，应用于REHEAT的四个LLMs都表现出代表性启发式偏见。我们进一步确定了模型的推理步骤

    arXiv:2404.01461v1 Announce Type: new  Abstract: Although large language models (LLMs) have demonstrated remarkable proficiency in understanding text and generating human-like text, they may exhibit biases acquired from training data in doing so. Specifically, LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic. This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example versus considering broader facts or statistical evidence. This work investigates the impact of the representativeness heuristic on LLM reasoning. We created REHEAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics. Experiments reveal that four LLMs applied to REHEAT all exhibited representativeness heuristic biases. We further identify that the model's reasoning steps
    
[^77]: 揭示LLMs在时间数据上的分歧归纳偏见

    Unveiling Divergent Inductive Biases of LLMs on Temporal Data

    [https://arxiv.org/abs/2404.01453](https://arxiv.org/abs/2404.01453)

    本研究探索了LLMs在时间数据分析中的固有挑战，重点评估了GPT-3.5和GPT-4模型的性能，发现了它们在特定时间关系上存在偏向性。

    

    揭示自然语言事件的微妙细节需要对时间动态进行微妙理解。尽管大型语言模型（LLMs）在从数据中辨别模式和关系方面表现得很熟练，但它们对时间动态的内在理解仍然是一个巨大挑战。本研究在LLMs中细致探索这些固有挑战，特别强调评估GPT-3.5和GPT-4模型在时间数据分析中的性能。通过采用问答（QA）格式和文本蕴涵（TE）格式两种不同的提示类型，我们的分析深入探究了隐式和显式事件。研究结果凸显了一些显著趋势，揭示了GPT-3.5和GPT-4性能上的差异。值得注意的是，倾向于特定时间关系的偏见浮出水面，其中在QA格式中，GPT-3.5在隐式和显式方面均表现出对"AFTER"的偏好。

    arXiv:2404.01453v1 Announce Type: cross  Abstract: Unraveling the intricate details of events in natural language necessitates a subtle understanding of temporal dynamics. Despite the adeptness of Large Language Models (LLMs) in discerning patterns and relationships from data, their inherent comprehension of temporal dynamics remains a formidable challenge. This research meticulously explores these intrinsic challenges within LLMs, with a specific emphasis on evaluating the performance of GPT-3.5 and GPT-4 models in the analysis of temporal data. Employing two distinct prompt types, namely Question Answering (QA) format and Textual Entailment (TE) format, our analysis probes into both implicit and explicit events. The findings underscore noteworthy trends, revealing disparities in the performance of GPT-3.5 and GPT-4. Notably, biases toward specific temporal relationships come to light, with GPT-3.5 demonstrating a preference for "AFTER'' in the QA format for both implicit and explicit
    
[^78]: 企业应用案例中知识图谱和自然语言处理的结合

    Enterprise Use Cases Combining Knowledge Graphs and Natural Language Processing

    [https://arxiv.org/abs/2404.01443](https://arxiv.org/abs/2404.01443)

    知识图谱和自然语言处理的结合为企业提供了灵活、可扩展且语义丰富的方式来组织和理解数据，本文探讨了这种组合带来的协同效应，涵盖了知识图谱构建、推理以及基于知识图谱的NLP任务三个核心领域的各种方法。

    

    知识管理是当今数字世界中企业面临的关键挑战，随着生成和收集的数据量和复杂性不断增长，知识图谱（KG）成为解决这一问题的一种有前途的解决方案，提供了一种灵活、可扩展且语义丰富的方式来组织和理解数据。本文在最近对结合知识图谱和自然语言处理（NLP）的研究文献进行调查的基础上进行了进一步研究。基于企业背景下选定的应用场景，我们讨论了此类组合带来的协同效应。我们涵盖了知识图谱构建、推理以及基于知识图谱的NLP任务三个核心领域的各种方法。除了解释创新的企业应用案例，我们评估了它们在实际应用方面的成熟度，并最后展望了未来新兴应用领域。

    arXiv:2404.01443v1 Announce Type: new  Abstract: Knowledge management is a critical challenge for enterprises in today's digital world, as the volume and complexity of data being generated and collected continue to grow incessantly. Knowledge graphs (KG) emerged as a promising solution to this problem by providing a flexible, scalable, and semantically rich way to organize and make sense of data. This paper builds upon a recent survey of the research literature on combining KGs and Natural Language Processing (NLP). Based on selected application scenarios from enterprise context, we discuss synergies that result from such a combination. We cover various approaches from the three core areas of KG construction, reasoning as well as KG-based NLP tasks. In addition to explaining innovative enterprise use cases, we assess their maturity in terms of practical applicability and conclude with an outlook on emergent application areas for the future.
    
[^79]: 从描述中无监督地进行情感分析，创建表情符号词库

    Creating emoji lexica from unsupervised sentiment analysis of their descriptions

    [https://arxiv.org/abs/2404.01439](https://arxiv.org/abs/2404.01439)

    该论文提出了一种从在线文本消息中预测表情符号所表达情感的新方法，无需人工标注数据，节省了宝贵时间。

    

    线上媒体，如博客和社交网络网站，生成了大量非结构化数据供分析个人和组织的观点和情感之用。传统的自然语言处理方法已经不能很好地量化这些观点的极性度量，因此需要新颖的方法。迄今为止，表情符号所表达的情感得到的关注较少。然而，过去四年间，符号的使用量激增。如今，Twitter中每天会被输入大约两百亿个符号，并且每个新的Unicode版本都会增加新的表情符号，使得它们对情感分析任务越来越重要。这促使我们提出了一种新颖的方法来预测在线文本消息（如推文）中表情符号表达的情感，该方法不需要人工手动注释数据，以此节省宝贵的时间用于其他分析任务。为此，我们自动构建了一种新颖的表情符号情感词库。

    arXiv:2404.01439v1 Announce Type: cross  Abstract: Online media, such as blogs and social networking sites, generate massive volumes of unstructured data of great interest to analyze the opinions and sentiments of individuals and organizations. Novel approaches beyond Natural Language Processing are necessary to quantify these opinions with polarity metrics. So far, the sentiment expressed by emojis has received little attention. The use of symbols, however, has boomed in the past four years. About twenty billion are typed in Twitter nowadays, and new emojis keep appearing in each new Unicode version, making them increasingly relevant to sentiment analysis tasks. This has motivated us to propose a novel approach to predict the sentiments expressed by emojis in online textual messages, such as tweets, that does not require human effort to manually annotate data and saves valuable time for other analysis tasks. For this purpose, we automatically constructed a novel emoji sentiment lexico
    
[^80]: 降低LLMs中位置偏差的面向位置的参数高效微调方法

    Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs

    [https://arxiv.org/abs/2404.01430](https://arxiv.org/abs/2404.01430)

    本研究发现LLMs的位置偏差主要源于不同模型的固有位置偏好，并提出了一种面向位置的参数高效微调方法来解决这一问题。

    

    大型语言模型（LLMs）的最新进展增强了它们处理长输入上下文的能力。对于涉及从外部数据存储库检索知识的任务，这一进展尤为关键，因为可能涉及长输入。然而，最近的研究显示LLMs存在位置偏差，表明其性能会根据输入序列中有用信息的位置而变化。本研究进行了大量实验，以调查位置偏差的根本原因。我们的发现表明，LLMs的位置偏差的主要贡献者源于不同模型的固有位置偏好。我们证明，仅仅采用基于提示的解决方案无法克服位置偏好。为了解决预训练LLMs的位置偏差问题，我们开发了一种面向位置的参数高效微调（PAPEFT）方法，该方法包含一个数据增广。

    arXiv:2404.01430v1 Announce Type: cross  Abstract: Recent advances in large language models (LLMs) have enhanced their ability to process long input contexts. This development is particularly crucial for tasks that involve retrieving knowledge from an external datastore, which can result in long inputs. However, recent studies show a positional bias in LLMs, demonstrating varying performance depending on the location of useful information within the input sequence. In this study, we conduct extensive experiments to investigate the root causes of positional bias. Our findings indicate that the primary contributor to LLM positional bias stems from the inherent positional preferences of different models. We demonstrate that merely employing prompt-based solutions is inadequate for overcoming the positional preferences. To address this positional bias issue of a pre-trained LLM, we developed a Position-Aware Parameter Efficient Fine-Tuning (PAPEFT) approach which is composed of a data augm
    
[^81]: 模型崩溃是否不可避免？通过累积真实和合成数据打破递归的诅咒

    Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data

    [https://arxiv.org/abs/2404.01413](https://arxiv.org/abs/2404.01413)

    本文通过比较数据取代和数据积累两种情况，发现累积数据可以防止模型崩溃。

    

    随着生成模型的激增，以及在网络规模数据上的预训练，一个及时的问题浮出水面：当这些模型被训练在它们自己生成的输出上时会发生什么？最近对模型数据反馈循环的研究发现，这样的循环可能导致模型崩溃，即性能随着每次模型拟合迭代逐渐下降，直到最新的模型变得无用。然而，最近几篇研究模型崩溃的论文都假设随着时间推移，新数据会取代旧数据，而不是假设数据会随时间累积。在本文中，我们比较了这两种情况，并表明积累数据可以防止模型崩溃。我们首先研究了一个解析可处理的设置，其中一系列线性模型拟合到先前模型的预测。先前的工作表明，如果数据被替换，测试误差会随着模型拟合迭代次数线性增加；我们扩展了这个研究探讨了数据逐渐累积的情况下会发生什么。

    arXiv:2404.01413v1 Announce Type: cross  Abstract: The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when these models are trained on their own generated outputs? Recent investigations into model-data feedback loops discovered that such loops can lead to model collapse, a phenomenon where performance progressively degrades with each model-fitting iteration until the latest model becomes useless. However, several recent papers studying model collapse assumed that new data replace old data over time rather than assuming data accumulate over time. In this paper, we compare these two settings and show that accumulating data prevents model collapse. We begin by studying an analytically tractable setup in which a sequence of linear models are fit to the previous models' predictions. Previous work showed if data are replaced, the test error increases linearly with the number of model-fitting iterations; we extend this r
    
[^82]: 开发安全和负责任的大型语言模型 - 一个全面框架

    Developing Safe and Responsible Large Language Models -- A Comprehensive Framework

    [https://arxiv.org/abs/2404.01399](https://arxiv.org/abs/2404.01399)

    该论文介绍了一种新的模型SR$_{\text{LLM}}$，旨在通过引入全面的安全风险分类法和专家标注数据集来增强大型语言模型（LLM）在语言生成中的安全性，并通过指令和参数高效微调方法有效减少了不安全内容的生成。

    

    鉴于人们对大型语言模型（LLM）的安全性和风险日益关注，发展减轻这些问题的方法至关重要。我们引入了安全和负责任的大型语言模型（SR$_{\text{LLM}}$），这个模型旨在通过使用LLM来增强语言生成的安全性。我们的方法结合了一个全面的LLM安全风险分类法，并利用专家注释的数据集与这种分类法相一致。SR$_{\text{LLM}}$旨在识别潜在的不安全内容并产生良性变化。它采用基于指令的和参数高效的微调方法，使得该模型不仅有效地增强安全性，而且资源高效且易于调整。在我们对五个基准数据集和两个专有数据集进行测试后，我们观察到不安全内容生成的显著减少。此外，在实施安全措施后，出现了...

    arXiv:2404.01399v1 Announce Type: new  Abstract: Given the growing concerns around the safety and risks of Large Language Models (LLMs), it is essential to develop methods for mitigating these issues. We introduce Safe and Responsible Large Language Model (SR$_{\text{LLM}}$) , a model designed to enhance the safety of language generation using LLMs. Our approach incorporates a comprehensive LLM safety risk taxonomy and utilizes a dataset annotated by experts that align with this taxonomy. SR$_{\text{LLM}}$ is designed to identify potentially unsafe content and produce benign variations. It employs instruction-based and parameter-efficient fine-tuning methods, making the model not only effective in enhancing safety but also resource-efficient and straightforward to adjust. Through our testing on five benchmark datasets and two proprietary datasets, we observed notable reductions in the generation of unsafe content. Moreover, following the implementation of safety measures, there was a s
    
[^83]: 基于提示的混合专家模型用于高效生成LLM

    Prompt-prompted Mixture of Experts for Efficient LLM Generation

    [https://arxiv.org/abs/2404.01365](https://arxiv.org/abs/2404.01365)

    提出了一种名为GRIFFIN的训练-free MoE，能够在各种LLM模型中选择唯一的FF专家以实现高效生成。

    

    随着基于transformer的大规模语言模型（LLMs）的发展，由于其出色的实用性，它们已被应用于许多领域，但在部署时存在相当大的计算成本。幸运的是，一些方法，如修剪或构建混合专家（MoE），旨在利用transformer前馈（FF）块中的稀疏性，以提高速度并降低内存需求。但是，这些技术在实践中可能非常昂贵和不灵活，因为它们通常需要训练或仅限于特定类型的架构。为了解决这个问题，我们引入了GRIFFIN，一种新颖的无需训练的MoE，它在序列级别为不同非ReLU激活函数的大量LLMs选择独特的FF专家以实现高效生成。这是可能的，因为我们关键观察到，许多经过训练的LLMs在序列中自然产生高度结构化的FF激活模式，这

    arXiv:2404.01365v1 Announce Type: cross  Abstract: With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free MoE that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which
    
[^84]: LLM Attributor: 交互式可视化归因用于LLM生成

    LLM Attributor: Interactive Visual Attribution for LLM Generation

    [https://arxiv.org/abs/2404.01361](https://arxiv.org/abs/2404.01361)

    LLM Attributor是一个Python库，提供了交互式可视化方式用于将LLM的文本生成结果归因到训练数据点，帮助用户检查模型行为、增强可信度，并与用户提供的文本进行比较。

    

    虽然大型语言模型（LLMs）显示出在各个领域生成令人信服的文本的能力，但对其潜在风险的担忧凸显了了解文本生成背后原因的重要性。我们提出了LLM Attributor，一个提供LLM文本生成训练数据归因交互可视化的Python库。我们的库为快速将LLM的文本生成归因到训练数据点提供了一种新方式，以检查模型行为、增强其可信度，并将模型生成的文本与用户提供的文本进行比较。我们描述了工具的视觉和交互设计，并强调LLaMA2模型的使用场景，该模型通过两个不同数据集进行微调：关于最近灾难和金融相关问答对的在线文章。由于LLM Attributor对计算笔记本的广泛支持，用户可以轻松将其整合到他们的工作流程中。

    arXiv:2404.01361v1 Announce Type: cross  Abstract: While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation. We present LLM Attributor, a Python library that provides interactive visualizations for training data attribution of an LLM's text generation. Our library offers a new way to quickly attribute an LLM's text generation to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated text with user-provided text. We describe the visual and interactive design of our tool and highlight usage scenarios for LLaMA2 models fine-tuned with two different datasets: online articles about recent disasters and finance-related question-answer pairs. Thanks to LLM Attributor's broad support for computational notebooks, users can easily integrate it into their workflow 
    
[^85]: 利用人工智能和社交媒体分析发现GLP-1受体激动剂的不良副作用

    Utilizing AI and Social Media Analytics to Discover Adverse Side Effects of GLP-1 Receptor Agonists

    [https://arxiv.org/abs/2404.01358](https://arxiv.org/abs/2404.01358)

    通过利用人工智能驱动的社交媒体分析，我们开发了一种数字健康方法，成功检测出与GLP-1受体激动剂相关的21种潜在不良副作用，包括易怒和麻木感，从而革新了对新部署药物未报告ASEs的检测。

    

    药物的不良副作用（ASEs）在FDA批准后被发现，对患者安全构成威胁。为了及时发现被忽视的ASEs，我们开发了一种数字健康方法，能够分析来自社交媒体、已发表的临床研究、制造商报告和ChatGPT等大量公开数据。我们发现了与肝素样肽1受体激动剂（GLP-1 RA）相关的ASEs，这一市场预计到2030年将呈指数增长至1335亿美元。利用命名实体识别（NER）模型，我们的方法成功检测出FDA批准时被忽视的21种潜在ASEs，包括易怒和麻木感。我们的数据分析方法彻底改变了对新部署药物相关未报告的ASEs的检测，利用前沿的人工智能驱动社交媒体分析。它可以通过释放社交媒体的力量来支持监管机构和制造商在市场上增加新药的安全性。

    arXiv:2404.01358v1 Announce Type: cross  Abstract: Adverse side effects (ASEs) of drugs, revealed after FDA approval, pose a threat to patient safety. To promptly detect overlooked ASEs, we developed a digital health methodology capable of analyzing massive public data from social media, published clinical research, manufacturers' reports, and ChatGPT. We uncovered ASEs associated with the glucagon-like peptide 1 receptor agonists (GLP-1 RA), a market expected to grow exponentially to $133.5 billion USD by 2030. Using a Named Entity Recognition (NER) model, our method successfully detected 21 potential ASEs overlooked upon FDA approval, including irritability and numbness. Our data-analytic approach revolutionizes the detection of unreported ASEs associated with newly deployed drugs, leveraging cutting-edge AI-driven social media analytics. It can increase the safety of new drugs in the marketplace by unlocking the power of social media to support regulators and manufacturers in the ra
    
[^86]: 用于边缘应用的LLM高效提取方法

    Efficiently Distilling LLMs for Edge Applications

    [https://arxiv.org/abs/2404.01353](https://arxiv.org/abs/2404.01353)

    提出了一种名为MLFS的新方法，用于高效参数的超网络训练，可以获得适用于商业边缘应用的高质量编码器模型，并有效地减少训练时间。

    

    在工业应用中，LLMs的超网络训练具有很大的重要性，因为它赋予了以固定成本产生不同大小/延迟模型的能力。我们提出了一种名为MLFS的新方法，用于高效参数的超网络训练。我们展示了可以获得适用于商业边缘应用的高质量编码器模型，并且虽然仅解码器模型对压缩具有相当的抵抗力，但可以有效地对解码器进行切片以大幅减少训练时间。

    arXiv:2404.01353v1 Announce Type: cross  Abstract: Supernet training of LLMs is of great interest in industrial applications as it confers the ability to produce a palette of smaller models at constant cost, regardless of the number of models (of different size / latency) produced. We propose a new method called Multistage Low-rank Fine-tuning of Super-transformers (MLFS) for parameter-efficient supernet training. We show that it is possible to obtain high-quality encoder models that are suitable for commercial edge applications, and that while decoder-only models are resistant to a comparable degree of compression, decoders can be effectively sliced for a significant reduction in training time.
    
[^87]: 大型语言模型中的公平性：一个分类调查

    Fairness in Large Language Models: A Taxonomic Survey

    [https://arxiv.org/abs/2404.01349](https://arxiv.org/abs/2404.01349)

    该调查总结了大型语言模型中公平性的最新进展，包括对偏见因素的分析、公平度量和现有算法分类。

    

    大型语言模型（LLMs）在各个领域展现了显著的成功。然而，尽管它们在许多实际应用中表现出色，大多数这些算法缺乏公平性考虑。因此，它们可能导致针对某些社区，特别是边缘化人群的歧视性结果，促使对公平的LLMs进行广泛研究。与传统机器学习中的公平相反，在LLMs中的公平性涉及独特的背景、分类法和实现技术。为此，该调查提供了关于公平LLMs的现有文献研究进展的全面概述。具体来说，提供了有关LLMs的简要介绍，接着分析了导致LLMs偏见的因素。此外，分类讨论了LLMs中的公平概念，总结了评估LLMs偏见的指标和现有算法。

    arXiv:2404.01349v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable success across various domains. However, despite their promising performance in numerous real-world applications, most of these algorithms lack fairness considerations. Consequently, they may lead to discriminatory outcomes against certain communities, particularly marginalized populations, prompting extensive study in fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in traditional machine learning, entails exclusive backgrounds, taxonomies, and fulfillment techniques. To this end, this survey presents a comprehensive overview of recent advances in the existing literature concerning fair LLMs. Specifically, a brief introduction to LLMs is provided, followed by an analysis of factors contributing to bias in LLMs. Additionally, the concept of fairness in LLMs is discussed categorically, summarizing metrics for evaluating bias in LLMs and existing algorithms 
    
[^88]: 利用双向门控循环单元和深度学习技术增强孟加拉虚假新闻检测

    Enhancing Bangla Fake News Detection Using Bidirectional Gated Recurrent Units and Deep Learning Techniques

    [https://arxiv.org/abs/2404.01345](https://arxiv.org/abs/2404.01345)

    使用深度学习技术中的双向门控循环单元，在孟加拉语中识别虚假新闻，准确率达到99.16%

    

    虚假新闻的兴起使得需要有效的检测方法变得越发重要，其中包括非英语语言。该研究旨在解决孟加拉语这种被认为不太重要的语言所面临的挑战。为此，提出了一个包含约50,000条新闻项目的完整数据集。在这个数据集上测试了几种深度学习模型，包括双向门控循环单元（GRU）、长短期记忆（LSTM）、一维卷积神经网络（CNN）和混合架构。对于这项研究，我们评估了利用一系列有用指标的模型的有效性，包括召回率、精确率、F1得分和准确度。通过使用一个大型应用程序来完成这项工作。我们进行了全面试验，展示了这些模型在识别孟加拉语中的虚假新闻方面的有效性，其中双向GRU模型的准确率达到了惊人的99.16%。我们的分析突出了这些模型在孟加拉语中识别虚假新闻的重要性

    arXiv:2404.01345v1 Announce Type: new  Abstract: The rise of fake news has made the need for effective detection methods, including in languages other than English, increasingly important. The study aims to address the challenges of Bangla which is considered a less important language. To this end, a complete dataset containing about 50,000 news items is proposed. Several deep learning models have been tested on this dataset, including the bidirectional gated recurrent unit (GRU), the long short-term memory (LSTM), the 1D convolutional neural network (CNN), and hybrid architectures. For this research, we assessed the efficacy of the model utilizing a range of useful measures, including recall, precision, F1 score, and accuracy. This was done by employing a big application. We carry out comprehensive trials to show the effectiveness of these models in identifying bogus news in Bangla, with the Bidirectional GRU model having a stunning accuracy of 99.16%. Our analysis highlights the impo
    
[^89]: 注意你的邻居：利用类似实例进行法律文件的修辞角色标注

    Mind Your Neighbours: Leveraging Analogous Instances for Rhetorical Role Labeling for Legal Documents

    [https://arxiv.org/abs/2404.01344](https://arxiv.org/abs/2404.01344)

    通过利用类似实例的知识，本研究引入了新颖技术来增强法律文件修辞角色标注性能，在宏F1分数上取得显着改进。

    

    法律判决的修辞角色标注对于各种任务至关重要，如案例总结、语义搜索和论点挖掘。然而，从上下文推断句子角色、相互关联的角色、有限的带标注数据和标签不平衡等问题带来挑战。本研究引入了新颖技术，通过利用语义相似实例（邻居）的知识来增强修辞角色标注性能。我们探讨了基于推理和基于训练的方法，实现了在具有挑战性的宏F1分数上的显着改进。对于基于推理的方法，我们探讨了增强标签预测而无需重新训练的插值技术。而在基于训练的方法中，我们将原型学习与我们的新颖论述感知对比方法相结合，直接在嵌入空间上进行工作。此外，我们评估了我们方法的跨领域适用性，并展示了它们在迁移中的有效性。

    arXiv:2404.01344v1 Announce Type: new  Abstract: Rhetorical Role Labeling (RRL) of legal judgments is essential for various tasks, such as case summarization, semantic search and argument mining. However, it presents challenges such as inferring sentence roles from context, interrelated roles, limited annotated data, and label imbalance. This study introduces novel techniques to enhance RRL performance by leveraging knowledge from semantically similar instances (neighbours). We explore inference-based and training-based approaches, achieving remarkable improvements in challenging macro-F1 scores. For inference-based methods, we explore interpolation techniques that bolster label predictions without re-training. While in training-based methods, we integrate prototypical learning with our novel discourse-aware contrastive method that work directly on embedding spaces. Additionally, we assess the cross-domain applicability of our methods, demonstrating their effectiveness in transferring 
    
[^90]: CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs

    CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs

    [https://arxiv.org/abs/2404.01343](https://arxiv.org/abs/2404.01343)

    CHOPS提出了一个名为CHOPS的LLM代理，旨在更高效地利用现有数据库或系统来访问用户信息，提供准确合理的响应或执行所需操作，同时避免有害操作。

    

    商业和软件平台越来越倾向于使用像GPT-3.5、GPT-4、GLM-3和LLaMa-2这样的大型语言模型（LLMs）作为客户服务的聊天辅助或推理代理。然而，当前基于LLM的客户服务模型在与客户配置文件的集成方面存在局限，并且缺乏有效服务所需的操作能力。为了解决这些问题，我们提出了一个名为CHOPS（CHat with custOmer Profile in existing System）的LLM代理，旨在：（1）高效利用现有数据库或系统以访问用户信息或按照现有指南与这些系统交互；（2）提供准确合理的响应或在系统中执行所需操作，同时避免有害操作；（3）利用

    arXiv:2404.01343v1 Announce Type: cross  Abstract: Businesses and software platforms are increasingly turning to Large Language Models (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance with file access or as reasoning agents for customer service. However, current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service. Moreover, existing API integrations emphasize diversity over the precision and error avoidance essential in real-world customer service scenarios. To address these issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile in existing System), designed to: (1) efficiently utilize existing databases or systems for accessing user information or interacting with these systems following existing guidelines; (2) provide accurate and reasonable responses or carry out required operations in the system while avoiding harmful operations; and (3) leverag
    
[^91]: DiffAgent：使用大语言模型实现快速准确的文本到图像API选择

    DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model

    [https://arxiv.org/abs/2404.01342](https://arxiv.org/abs/2404.01342)

    DiffAgent利用大语言模型设计了一个新的代理工具，能够快速准确地选择最适合的文本到图像API，并提出了一个综合数据集DABench来评估其性能。

    

    文本到图像（T2I）生成模型引起了广泛关注，并在学术研究以及其他领域找到了广泛应用。本文中，我们从大语言模型（LLMs）的工具使用研究中汲取灵感，引入了DiffAgent，这是一个经过设计用于通过API调用在几秒钟内进行准确选择的LLM代理。DiffAgent利用了一种全新的两阶段训练框架SFTA，使其能够根据用户偏好精确地将T2I API响应与用户输入对齐。为了训练和评估DiffAgent的能力，我们提出了DABench，这是一个包含大量T2I API的综合数据集，来自社区。

    arXiv:2404.01342v1 Announce Type: cross  Abstract: Text-to-image (T2I) generative models have attracted significant attention and found extensive applications within and beyond academic research. For example, the Civitai community, a platform for T2I innovation, currently hosts an impressive array of 74,492 distinct models. However, this diversity presents a formidable challenge in selecting the most appropriate model and parameters, a process that typically requires numerous trials. Drawing inspiration from the tool usage research of large language models (LLMs), we introduce DiffAgent, an LLM agent designed to screen the accurate selection in seconds via API calls. DiffAgent leverages a novel two-stage training framework, SFTA, enabling it to accurately align T2I API responses with user input in accordance with human preferences. To train and evaluate DiffAgent's capabilities, we present DABench, a comprehensive dataset encompassing an extensive range of T2I APIs from the community. 
    
[^92]: 通过零-shot情绪和不流畅生成实现人性化语音合成

    Humane Speech Synthesis through Zero-Shot Emotion and Disfluency Generation

    [https://arxiv.org/abs/2404.01339](https://arxiv.org/abs/2404.01339)

    该论文实现了一种新颖的语音合成技术，通过零-shot设置中引入人类情感和不流畅特征，使得系统能更好地模仿人类语音，促进更自然的用户互动。

    

    当代对话系统往往存在一个重要局限性：它们的回应缺乏人类交互的情感深度和不流畅特征。这种缺失在用户寻求更个性化和有共情的互动时尤为明显。因此，这使它们显得机械化，难以引起人类用户的共鸣。认识到这一差距，我们着手于人性化机器通信的旅程，以确保AI系统不仅理解而且共鸣。为了解决这一缺点，我们设计了一种创新的语音合成流程。在这一框架内，一种先进的语言模型在零-shot设置中引入了类似人类的情感和语言紊乱。这些复杂性通过语言模型在文本生成期间无缝集成到生成文本中，使系统更好地模仿人类语音模式，促进更直观和自然的用户互动。

    arXiv:2404.01339v1 Announce Type: cross  Abstract: Contemporary conversational systems often present a significant limitation: their responses lack the emotional depth and disfluent characteristic of human interactions. This absence becomes particularly noticeable when users seek more personalized and empathetic interactions. Consequently, this makes them seem mechanical and less relatable to human users. Recognizing this gap, we embarked on a journey to humanize machine communication, to ensure AI systems not only comprehend but also resonate. To address this shortcoming, we have designed an innovative speech synthesis pipeline. Within this framework, a cutting-edge language model introduces both human-like emotion and disfluencies in a zero-shot setting. These intricacies are seamlessly integrated into the generated text by the language model during text generation, allowing the system to mirror human speech patterns better, promoting more intuitive and natural user interactions. The
    
[^93]: 通过Latent Dirichlet Allocation主题建模自动检测财经新闻中的相关信息、预测和预测

    Automatic detection of relevant information, predictions and forecasts in financial news through topic modelling with Latent Dirichlet Allocation

    [https://arxiv.org/abs/2404.01338](https://arxiv.org/abs/2404.01338)

    该研究提出了一种新颖的自然语言处理系统，通过Latent Dirichlet Allocation (LDA)进行相关的主题建模，帮助投资者从非结构化文本源中检测财经事件中的相关信息、预测和预测

    

    arXiv:2404.01338v1通告类型:新摘要:金融新闻是一种非结构化的信息源，可以开采以从中提取知识，用于市场筛选应用。从持续的金融新闻流中手动提取相关信息是繁琐的，超出了许多投资者的技能范围，他们最多只能关注几个来源和作者。因此，我们专注于对金融新闻的分析，以识别相关文本，并在该文本中进行预测和预测。我们提出了一种新颖的自然语言处理（NLP）系统，帮助投资者通过考虑话语层面上的相关性和时态性，从非结构化文本源中检测相关的财经事件。首先，我们将文本分割以将相关文本归为一组。其次，我们应用共指解析来发现段落内部的依赖关系。最后，我们利用Latent Dirichlet Allocation (LDA)进行相关主题建模。

    arXiv:2404.01338v1 Announce Type: new  Abstract: Financial news items are unstructured sources of information that can be mined to extract knowledge for market screening applications. Manual extraction of relevant information from the continuous stream of finance-related news is cumbersome and beyond the skills of many investors, who, at most, can follow a few sources and authors. Accordingly, we focus on the analysis of financial news to identify relevant text and, within that text, forecasts and predictions. We propose a novel Natural Language Processing (NLP) system to assist investors in the detection of relevant financial events in unstructured textual sources by considering both relevance and temporality at the discursive level. Firstly, we segment the text to group together closely related text. Secondly, we apply co-reference resolution to discover internal dependencies within segments. Finally, we perform relevant topic modelling with Latent Dirichlet Allocation (LDA) to separ
    
[^94]: 结合自然语言处理和机器学习在金融新闻中检测篇章级别的时间性

    Detection of Temporality at Discourse Level on Financial News by Combining Natural Language Processing and Machine Learning

    [https://arxiv.org/abs/2404.01337](https://arxiv.org/abs/2404.01337)

    通过结合自然语言处理和机器学习技术，提出了一种新颖的系统，旨在在金融新闻中检测篇章级别的关键声明的时间性，以分析句法和语义依赖关系，区分上下文信息和有价值的预测。

    

    Finance-related news, such as Bloomberg News, CNN Business, and Forbes, provide valuable real data for market screening systems. Experts in these news articles not only provide technical analyses but also share opinions considering political, sociological, and cultural factors. We propose a novel system that utilizes Natural Language Processing and Machine Learning techniques to detect the temporality of key statements in finance-related news at the discourse level, aiming to differentiate between context information and valuable predictions by analyzing syntactic and semantic dependencies.

    arXiv:2404.01337v1 Announce Type: new  Abstract: Finance-related news such as Bloomberg News, CNN Business and Forbes are valuable sources of real data for market screening systems. In news, an expert shares opinions beyond plain technical analyses that include context such as political, sociological and cultural factors. In the same text, the expert often discusses the performance of different assets. Some key statements are mere descriptions of past events while others are predictions. Therefore, understanding the temporality of the key statements in a text is essential to separate context information from valuable predictions. We propose a novel system to detect the temporality of finance-related news at discourse level that combines Natural Language Processing and Machine Learning techniques, and exploits sophisticated features such as syntactic and semantic dependencies. More specifically, we seek to extract the dominant tenses of the main statements, which may be either explicit 
    
[^95]: FineFake：一个用于细粒度多领域假新闻检测的知识增强数据集

    FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain Fake News Detecction

    [https://arxiv.org/abs/2404.01336](https://arxiv.org/abs/2404.01336)

    FineFake 数据集为细粒度多领域假新闻检测提供了知识增强，包含16,909个数据样本覆盖六个语义主题和八个平台。

    

    现有的假新闻检测基准数据集在评估新闻内容的真实性方面取得了显著进展。然而，这些基准数据集通常仅关注单一语义主题的新闻或来自单一平台的新闻，因此无法捕捉真实场景中多领域新闻的多样性。为了了解不同领域的假新闻，外部知识和细粒度注释至关重要，以提供精确证据并揭示制造假新闻的多样潜在策略，而这也是现有基准数据集所忽略的。为了填补这一空白，我们引入了一个名为FineFake的新型多领域知识增强基准数据集，具有细粒度注释。FineFake涵盖了来自六个语义主题和八个平台的16,909个数据样本。每个新闻项目都包含多模态内容、潜在社交背景、半自动

    arXiv:2404.01336v1 Announce Type: cross  Abstract: Existing benchmarks for fake news detection have significantly contributed to the advancement of models in assessing the authenticity of news content. However, these benchmarks typically focus solely on news pertaining to a single semantic topic or originating from a single platform, thereby failing to capture the diversity of multi-domain news in real scenarios. In order to understand fake news across various domains, the external knowledge and fine-grained annotations are indispensable to provide precise evidence and uncover the diverse underlying strategies for fabrication, which are also ignored by existing benchmarks. To address this gap, we introduce a novel multi-domain knowledge-enhanced benchmark with fine-grained annotations, named \textbf{FineFake}. FineFake encompasses 16,909 data samples spanning six semantic topics and eight platforms. Each news item is enriched with multi-modal content, potential social context, semi-man
    
[^96]: 使用LLMs增强NER数据集：迈向自动化和精细化标注

    Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation

    [https://arxiv.org/abs/2404.01334](https://arxiv.org/abs/2404.01334)

    本研究引入了一种新颖的混合标注方法，将人力工作与大型语言模型相结合，旨在提高NER模型的性能，并以成本效益的方式实现这一目标。

    

    在自然语言处理（NLP）领域，命名实体识别（NER）被认为是一项关键技术，在各种应用中被广泛应用。传统的用于为NER模型标注数据集的方法面临着高成本和数据集质量变化的挑战。本研究介绍了一种新型的混合标注方法，将人力工作与大型语言模型（LLMs）的能力相结合。这种方法不仅旨在改善手动注释中固有的噪音，如遗漏，从而提高NER模型的性能，而且还以一种具有成本效益的方式实现这一目标。此外，通过采用标签混合策略，它解决了LLM-based注释中遇到的类别不平衡问题。通过对多个数据集的分析，这种方法一直表现出比传统注释方法更优异的性能，即使在co

    arXiv:2404.01334v1 Announce Type: new  Abstract: In the field of Natural Language Processing (NLP), Named Entity Recognition (NER) is recognized as a critical technology, employed across a wide array of applications. Traditional methodologies for annotating datasets for NER models are challenged by high costs and variations in dataset quality. This research introduces a novel hybrid annotation approach that synergizes human effort with the capabilities of Large Language Models (LLMs). This approach not only aims to ameliorate the noise inherent in manual annotations, such as omissions, thereby enhancing the performance of NER models, but also achieves this in a cost-effective manner. Additionally, by employing a label mixing strategy, it addresses the issue of class imbalance encountered in LLM-based annotations. Through an analysis across multiple datasets, this method has been consistently shown to provide superior performance compared to traditional annotation methods, even under co
    
[^97]: 等等，这都是令牌噪音？一直就是吗：利用 Shapley 值解释 LLM 行为

    Wait, It's All Token Noise? Always Has Been: Interpreting LLM Behavior Using Shapley Value

    [https://arxiv.org/abs/2404.01332](https://arxiv.org/abs/2404.01332)

    使用Shapley值方法解释LLM行为，揭示了所谓的“令牌噪音”效应，揭示了LLMs的决策在很大程度上受到提示组件的影响

    

    大型语言模型（LLMs）的出现为模拟人类行为和认知过程开辟了新的可能性，潜在应用包括市场研究和消费者行为分析等各个领域。然而，由于LLMs的显著差异暗示了不同的基础过程在起作用，以及LLMs对提示变化的敏感性，利用LLMs作为人类主体的替代仍然存在不确定性。本文提出了一种基于合作博弈理论中Shapley值的新方法来解释LLM行为，并量化每个提示组件对模型输出的相对贡献。通过两个应用--一个离散选择实验和一个认知偏见调查，我们展示了Shapley值方法如何揭示我们所谓的“令牌噪音”效应，即LLM决策受到的影响严重偏向于

    arXiv:2404.01332v1 Announce Type: cross  Abstract: The emergence of large language models (LLMs) has opened up exciting possibilities for simulating human behavior and cognitive processes, with potential applications in various domains, including marketing research and consumer behavior analysis. However, the validity of utilizing LLMs as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of LLM responses to prompt variations. This paper presents a novel approach based on Shapley values from cooperative game theory to interpret LLM behavior and quantify the relative contribution of each prompt component to the model's output. Through two applications-a discrete choice experiment and an investigation of cognitive biases-we demonstrate how the Shapley value method can uncover what we term "token noise" effects, a phenomenon where LLM decisions are disproportionately influenced by 
    
[^98]: LLaVA-Gemma：利用紧凑的语言模型加速多模态基础模型

    LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model

    [https://arxiv.org/abs/2404.01331](https://arxiv.org/abs/2404.01331)

    使用最新发布的Gemma大型语言模型在LLaVA框架中训练了多模态基础模型，研究了预训练连接器、更强大的图像主干和增加语言主干大小对模型性能的影响。

    

    我们使用最新发布的Gemma大型语言模型（LLM）在流行的LLaVA框架中训练一系列多模态基础模型（MMFM）。特别值得关注的是2B参数的Gemma模型，它提供了构建功能强大的小规模MMFM的机会。与该领域其他论文的发现一致，我们测试了去除三种设计特性的影响：预训练连接器，利用更强大的图像主干，增加语言主干的大小。我们称之为LLaVA-Gemma的结果模型在一系列评估中表现出中等性能，但未能超越当前相对大小的SOTA模型。性能的更详细分析显示出不同的效果：跳过预训练往往会降低性能，更大的视觉模型有时会提高性能，增加语言模型的大小效果不一致。我们公开发布了训练配方，代码等。

    arXiv:2404.01331v1 Announce Type: cross  Abstract: We train a suite of multimodal foundation models (MMFM) using the popular LLaVA framework with the recently released Gemma family of large language models (LLMs). Of particular interest is the 2B parameter Gemma model, which provides opportunities to construct capable small-scale MMFMs. In line with findings from other papers in this space, we test the effect of ablating three design features: pretraining the connector, utilizing a more powerful image backbone, and increasing the size of the language backbone. The resulting models, which we call LLaVA-Gemma, exhibit moderate performance on an array of evaluations, but fail to improve past the current comparably sized SOTA models. Closer analysis of performance shows mixed effects; skipping pretraining tends to reduce performance, larger vision models sometimes improve performance, and increasing language model size has inconsistent effects. We publicly release training recipes, code an
    
[^99]: 无抽象能力的老年人数字包容娱乐聊天机器人

    Entertainment chatbot for the digital inclusion of elderly people without abstraction capabilities

    [https://arxiv.org/abs/2404.01327](https://arxiv.org/abs/2404.01327)

    EBER chatbot是一个旨在减少老年人数字鸿沟的娱乐聊天机器人，其创新之处在于其"智能电台"概念，根据用户的心情和需求提供相关信息。

    

    当前的语言处理技术允许创建对话式聊天机器人平台。尽管人工智能在许多大众市场领域仍然过于不成熟，无法支持令人满意的用户体验，但对话式界面已经在临时应用中找到了自己的位置，比如电话中心和在线购物助手。然而，目前还没有将其应用于老年人的社会包容，老年人特别容易受到数字鸿沟的影响。许多老年人通过传统媒体如电视和广播来减轻孤独感，这些媒体被认为是创造陪伴感的方式。在本文中，我们介绍了旨在减少老年人数字鸿沟的EBER聊天机器人。EBER会在后台阅读新闻，并根据用户的心情调整回复。其创新之处在于“智能电台”概念，根据该概念，不是简化数字信息系统以使其易于访问，而是根据用户的心情和需求提供相关信息。

    arXiv:2404.01327v1 Announce Type: cross  Abstract: Current language processing technologies allow the creation of conversational chatbot platforms. Even though artificial intelligence is still too immature to support satisfactory user experience in many mass market domains, conversational interfaces have found their way into ad hoc applications such as call centres and online shopping assistants. However, they have not been applied so far to social inclusion of elderly people, who are particularly vulnerable to the digital divide. Many of them relieve their loneliness with traditional media such as TV and radio, which are known to create a feeling of companionship. In this paper we present the EBER chatbot, designed to reduce the digital gap for the elderly. EBER reads news in the background and adapts its responses to the user's mood. Its novelty lies in the concept of "intelligent radio", according to which, instead of simplifying a digital information system to make it accessible to
    
[^100]: 对多模态大型语言与视觉模型的综述

    A Review of Multi-Modal Large Language and Vision Models

    [https://arxiv.org/abs/2404.01322](https://arxiv.org/abs/2404.01322)

    该论文对具有多模态能力的大型语言模型进行了综述，涵盖了LLMs的发展历程、transformer-based 架构的进展以及注意机制的作用

    

    大型语言模型（LLMs）最近成为研究和应用的焦点，其能够理解和生成具有类似于人类质量的文本，受到了推动。更近期，LLMs被扩展为多模态大型语言模型（MM-LLMs），将其能力扩展到处理图像、视频和音频信息，除了文本。这打开了诸如文本到视频生成、图像字幕、文本到语音等应用，并通过将LLM与多模态能力进行后期调整，或者从头开始构建MM-LLM来实现。本文全面回顾了具有多模态能力的当前LLMs以及最新的MM-LLMs的现状。它涵盖了LLMs的历史发展，特别是由transformer-based 架构如OpenAI的GPT系列和Google的BERT提供的进展，以及注意机制在增强中的作用

    arXiv:2404.01322v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have recently emerged as a focal point of research and application, driven by their unprecedented ability to understand and generate text with human-like quality. Even more recently, LLMs have been extended into multi-modal large language models (MM-LLMs) which extends their capabilities to deal with image, video and audio information, in addition to text. This opens up applications like text-to-video generation, image captioning, text-to-speech, and more and is achieved either by retro-fitting an LLM with multi-modal capabilities, or building a MM-LLM from scratch. This paper provides an extensive review of the current state of those LLMs with multi-modal capabilities as well as the very recent MM-LLMs. It covers the historical development of LLMs especially the advances enabled by transformer-based architectures like OpenAI's GPT series and Google's BERT, as well as the role of attention mechanisms in enh
    
[^101]: 在Transformer中智能学习率分布以减少灾难性遗忘

    Intelligent Learning Rate Distribution to reduce Catastrophic Forgetting in Transformers

    [https://arxiv.org/abs/2404.01317](https://arxiv.org/abs/2404.01317)

    本文研究了在Transformer神经网络中的灾难性遗忘问题，通过智能学习率分布取得了比平坦学习率更好的性能，并在GLUE数据集中得到验证。

    

    在自然语言处理中，对大型文本语料库进行语言模型的预训练是一种常见做法。然后对这些模型进行微调以在各种任务上取得最佳结果。本文研究了Transformer神经网络中灾难性遗忘的问题，并质疑在这种情况下对整个网络采用相同学习率的微调常见做法。我们进行了超参数优化过程，找到了比平坦学习率更好的学习率分布。我们结合这些学习率分布，并展示它们对灾难性遗忘问题的性能表现更好。我们使用GLUE数据集中的各种自然语言处理基准验证了这些学习率分布。

    arXiv:2404.01317v1 Announce Type: cross  Abstract: Pretraining language models on large text corpora is a common practice in natural language processing. Fine-tuning of these models is then performed to achieve the best results on a variety of tasks. In this paper, we investigate the problem of catastrophic forgetting in transformer neural networks and question the common practice of fine-tuning with a flat learning rate for the entire network in this context. We perform a hyperparameter optimization process to find learning rate distributions that are better than a flat learning rate. We combine the learning rate distributions thus found and show that they generalize to better performance with respect to the problem of catastrophic forgetting. We validate these learning rate distributions with a variety of NLP benchmarks from the GLUE dataset.
    
[^102]: NeuroPrune：一种受神经系统启发的用于大型语言模型的拓扑稀疏训练算法

    NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for Large Language Models

    [https://arxiv.org/abs/2404.01306](https://arxiv.org/abs/2404.01306)

    本研究受神经系统启发，通过神经网络拓扑的稀疏方法，探索类似于生物网络的机制，展示了对各种 NLP 任务都表现出色和高效的模型-不可知稀疏性方法

    

    基于 Transformer 的语言模型由于在各种任务上的出色性能而在自然语言处理（NLP）中变得普遍。然而，昂贵的训练以及推理仍然是它们广泛适用性的一个重要障碍。在模型架构的各个层次强制引入稀疏性已被证明有助于解决扩展性和效率问题，但稀疏性对网络拓扑的影响仍存在断裂。受大脑神经网络启发，我们通过网络拓扑的视角探索稀疏性方法。具体而言，我们利用在生物网络中观察到的机制，如优先附着和冗余突触修剪，并展示了基于原则的、与模型无关的稀疏性方法在跨越分类（如自然语言推理）和生成（摘要、机器翻译）的各种 NLP 任务上表现出色且高效，尽管 o

    arXiv:2404.01306v1 Announce Type: cross  Abstract: Transformer-based Language Models have become ubiquitous in Natural Language Processing (NLP) due to their impressive performance on various tasks. However, expensive training as well as inference remains a significant impediment to their widespread applicability. While enforcing sparsity at various levels of the model architecture has found promise in addressing scaling and efficiency issues, there remains a disconnect between how sparsity affects network topology. Inspired by brain neuronal networks, we explore sparsity approaches through the lens of network topology. Specifically, we exploit mechanisms seen in biological networks, such as preferential attachment and redundant synapse pruning, and show that principled, model-agnostic sparsity approaches are performant and efficient across diverse NLP tasks, spanning both classification (such as natural language inference) and generation (summarization, machine translation), despite o
    
[^103]: IsoBench：基于同构表示对多模态基础模型进行基准测试

    IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations

    [https://arxiv.org/abs/2404.01266](https://arxiv.org/abs/2404.01266)

    IsoBench提出了一个基准数据集，用于评估基于不同同构表示的多模态基础模型的性能差异，发现大多数模型更偏好文本表示。

    

    当前的基础模型在仅文本或图像和文本输入同时提示时表现出令人印象深刻的能力。但它们的能力是否会根据输入方式而改变呢？在这项工作中，我们提出了一个名为$\textbf{IsoBench}$的基准数据集，其中包含来自四个主要领域的问题: 数学、科学、算法和游戏。每个示例呈现了多个输入的同构表示，如视觉、文本和数学展示。IsoBench提供了细粒度的反馈，以诊断由表示形式造成的性能差距。在各种基础模型中，我们观察到在相同问题上，模型一贯偏好文本表示。最突出的是，在所有IsoBench问题上进行评估时，Claude-3 Opus在提供图像而不是文本时性能下降28.7分；同样，GPT-4 Turbo性能下降18.7分，Gemini Pro下降14.9分。

    arXiv:2404.01266v1 Announce Type: new  Abstract: Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose $\textbf{IsoBench}$, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple $\textbf{isomorphic representations}$ of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7 points worse and Gemini Pro is 14.9 p
    
[^104]: 具有多模式原型的开放词汇联邦学习

    Open-Vocabulary Federated Learning with Multimodal Prototyping

    [https://arxiv.org/abs/2404.01232](https://arxiv.org/abs/2404.01232)

    本研究针对联邦学习中的开放词汇挑战，提出了一种基于预训练视觉语言模型的新型自适应框架，命名为联邦多模式原型（Fed-MP），用于解决新用户提出的涉及任意未知类别的查询问题。

    

    现有的联邦学习（FL）研究通常假设训练标签空间和测试标签空间是相同的。然而，在真实应用中，这个假设太理想化了。新用户可能提出涉及来自未见类别数据的查询，这些开放词汇查询将直接导致这种FL系统的缺陷。因此，在这项工作中，我们明确关注FL中尚未开发的开放词汇挑战。也就是说，对于一个新用户，全局服务器应该理解她/他的查询涉及任意未知类别。为了解决这个问题，我们利用了预训练的视觉语言模型（VLMs）。具体来说，我们提出了一个针对FL环境中VLMs的新型自适应框架，命名为联邦多模式原型（Fed-MP）。Fed-MP根据轻量级客户端残差自适应聚合本地模型权重，并根据一个新颖的多模式原型机制进行预测。

    arXiv:2404.01232v1 Announce Type: new  Abstract: Existing federated learning (FL) studies usually assume the training label space and test label space are identical. However, in real-world applications, this assumption is too ideal to be true. A new user could come up with queries that involve data from unseen classes, and such open-vocabulary queries would directly defect such FL systems. Therefore, in this work, we explicitly focus on the under-explored open-vocabulary challenge in FL. That is, for a new user, the global server shall understand her/his query that involves arbitrary unknown classes. To address this problem, we leverage the pre-trained vision-language models (VLMs). In particular, we present a novel adaptation framework tailored for VLMs in the context of FL, named as Federated Multimodal Prototyping (Fed-MP). Fed-MP adaptively aggregates the local model weights based on light-weight client residuals, and makes predictions based on a novel multimodal prototyping mechan
    
[^105]: AI法律和大型语言模型（LLMs）：当关键问题和隐私影响需要人类和道德监督时

    AI Act and Large Language Models (LLMs): When critical issues and privacy impact require human and ethical oversight

    [https://arxiv.org/abs/2404.00600](https://arxiv.org/abs/2404.00600)

    论文讨论了人类监督、道德监督和隐私影响评估在面对人工智能系统和大型语言模型发展中的重要性。

    

    人工智能系统的日益发展，特别是大型语言模型（LLM）的发展，使得有必要对它们在隐私、个人数据保护以及道德层面，尤其是对最脆弱和最弱势群体可能产生的风险和影响进行评估。本文对人类监督、道德监督和隐私影响评估进行了讨论。

    arXiv:2404.00600v1 Announce Type: cross  Abstract: The imposing evolution of artificial intelligence systems and, specifically, of Large Language Models (LLM) makes it necessary to carry out assessments of their level of risk and the impact they may have in the area of privacy, personal data protection and at an ethical level, especially on the weakest and most vulnerable. This contribution addresses human oversight, ethical oversight, and privacy impact assessment.
    
[^106]: MIPS在SemEval-2024任务3中的表现：使用多模态语言模型在对话中进行多模态情绪-原因对提取

    MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in Conversations with Multimodal Language Models

    [https://arxiv.org/abs/2404.00511](https://arxiv.org/abs/2404.00511)

    本文提出了一种集成文本、音频和视觉模态的多模态情绪识别和多模态情绪原因提取框架，通过利用专门的情绪编码器和模态特定特征，实现了提升情绪理解和因果推理的竞争性成果。

    

    本文介绍了我们在SemEval 2024任务3的子任务2中关于对话中多模态情绪原因分析的获奖提交。我们提出了一种新颖的多模态情绪识别和多模态情绪原因提取（MER-MCE）框架，该框架利用专门的情绪编码器整合文本、音频和视觉三种模态。我们的方法通过利用模态特定特征提升情绪理解和因果推理，使自己脱颖而出。实验评估表明了我们多模态方法的优势，我们的提交取得了竞争性的加权F1分数为0.3435，在0.0339之后排名第一的团队，仅在0.0025之后排名第二。项目链接：https://github.com/MIPS-COLT/MER-MCE.git

    arXiv:2404.00511v1 Announce Type: new  Abstract: This paper presents our winning submission to Subtask 2 of SemEval 2024 Task 3 on multimodal emotion cause analysis in conversations. We propose a novel Multimodal Emotion Recognition and Multimodal Emotion Cause Extraction (MER-MCE) framework that integrates text, audio, and visual modalities using specialized emotion encoders. Our approach sets itself apart from top-performing teams by leveraging modality-specific features for enhanced emotion understanding and causality inference. Experimental evaluation demonstrates the advantages of our multimodal approach, with our submission achieving a competitive weighted F1 score of 0.3435, ranking third with a margin of only 0.0339 behind the 1st team and 0.0025 behind the 2nd team. Project: https://github.com/MIPS-COLT/MER-MCE.git
    
[^107]: LLM作为写作助手：探讨所有权感和推理的视角

    LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning

    [https://arxiv.org/abs/2404.00027](https://arxiv.org/abs/2404.00027)

    探讨使用大型语言模型作为写作助手引发的写作所有权感和作者身份认知之间的心理困境。

    

    写作中的所有权感限制了我们对思想、时间和贡献的投入，导致对产出物的依恋。然而，使用写作助手引入了一种心理困境，因为一些内容并非直接我们的创作。我们往往更倾向于在创造性任务中更多地归功于大型语言模型（LLMs），尽管它们对所有任务都是平等的。此外，虽然我们可能不会完全声称对由LLM生成的内容拥有所有权，但却自由地声称作者身份。我们进行了一项简短调查来研究这些问题，并了解潜在的认知过程，以更好地了解人机交互在写作中的应用并改进写作辅助系统。

    arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
    
[^108]: 墨水与个性：在LLMs时代塑造个性化叙事

    Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs

    [https://arxiv.org/abs/2404.00026](https://arxiv.org/abs/2404.00026)

    研究探讨了人们日益依赖的基于LLM的写作助手对创造力和个性可能造成的负面影响，旨在改进人机交互系统和提升写作助手的个性化和个性化功能。

    

    个性和个性化构成了使每个作家独特并影响其文字以有效吸引读者同时传达真实性的独特特征。然而，我们日益依赖基于LLM的写作助手可能会危及我们的创造力和个性。我们经常忽视这一趋势对我们的创造力和独特性的负面影响，尽管可能会造成后果。本研究通过进行简要调查探索不同的观点和概念，以及尝试理解人们的观点，结合以往在该领域的研究，来研究这些问题。解决这些问题对于改进人机交互系统和增强个性化和个性化写作助手至关重要。

    arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
    
[^109]: 通过视觉语言模型对神经网络进行基于概念的分析

    Concept-based Analysis of Neural Networks via Vision-Language Models

    [https://arxiv.org/abs/2403.19837](https://arxiv.org/abs/2403.19837)

    本文提出利用视觉语言模型作为透镜，通过其隐含的高层次概念来进行对视觉模型的分析。

    

    视觉深度神经网络（DNNs）的形式化分析非常可取，但由于难以表达视觉任务的形式化规范以及缺乏高效的验证程序，这是非常具有挑战性的。在本文中，我们提出利用新兴的多模态、视觉语言、基础模型（VLMs）作为一种通过其可以推理视觉模型的透镜。VLMs已经在大量图像及其文本描述上进行了训练，因此隐式地了解描述这些图像的高层次、人类可理解的概念。我们描述了一种名为$\texttt{Con}_{\texttt{spec}}$的逻辑规范语言，旨在便于按照这些概念编写规范。为了定义和形式化检查$\texttt{Con}_{\texttt{spec}}$规范，我们利用了一个VLM，它提供了一种编码和高效检查视觉模型的自然语言属性的方法。我们展示了我们的te

    arXiv:2403.19837v1 Announce Type: cross  Abstract: Formal analysis of vision-based deep neural networks (DNNs) is highly desirable but it is very challenging due to the difficulty of expressing formal specifications for vision tasks and the lack of efficient verification procedures. In this paper, we propose to leverage emerging multimodal, vision-language, foundation models (VLMs) as a lens through which we can reason about vision models. VLMs have been trained on a large body of images accompanied by their textual description, and are thus implicitly aware of high-level, human-understandable concepts describing the images. We describe a logical specification language $\texttt{Con}_{\texttt{spec}}$ designed to facilitate writing specifications in terms of these concepts. To define and formally check $\texttt{Con}_{\texttt{spec}}$ specifications, we leverage a VLM, which provides a means to encode and efficiently check natural-language properties of vision models. We demonstrate our te
    
[^110]: KazParC：用于机器翻译的哈萨克平行语料库

    KazParC: Kazakh Parallel Corpus for Machine Translation

    [https://arxiv.org/abs/2403.19399](https://arxiv.org/abs/2403.19399)

    KazParC是一个跨哈萨克语、英语、俄语和土耳其语的机器翻译平行语料库，其中包含371,902个平行句子，还开发了性能优越的神经机器翻译模型Tilmash。

    

    我们介绍了KazParC，这是一个为跨哈萨克语、英语、俄语和土耳其语进行机器翻译而设计的平行语料库。作为其类别中首个也是最大的公开可用语料库，KazParC包含了371,902个平行句子的集合，涵盖不同领域，并在人类译者的协助下开发。我们的研究工作还扩展到了发展一个被昵称为Tilmash的神经机器翻译模型。引人注目的是，Tilmash的表现与工业巨头，如Google翻译和Yandex翻译，在标准评估指标（如BLEU和chrF）的基础上相媲美，并在某些情况下甚至超越。KazParC和Tilmash均可通过我们的GitHub存储库以知识共享署名4.0国际许可（CC BY 4.0）公开下载。

    arXiv:2403.19399v1 Announce Type: new  Abstract: We introduce KazParC, a parallel corpus designed for machine translation across Kazakh, English, Russian, and Turkish. The first and largest publicly available corpus of its kind, KazParC contains a collection of 371,902 parallel sentences covering different domains and developed with the assistance of human translators. Our research efforts also extend to the development of a neural machine translation model nicknamed Tilmash. Remarkably, the performance of Tilmash is on par with, and in certain instances, surpasses that of industry giants, such as Google Translate and Yandex Translate, as measured by standard evaluation metrics, such as BLEU and chrF. Both KazParC and Tilmash are openly available for download under the Creative Commons Attribution 4.0 International License (CC BY 4.0) through our GitHub repository.
    
[^111]: 知识边界与角色动态塑造更好的社交媒体代理

    Knowledge Boundary and Persona Dynamic Shape A Better Social Media Agent

    [https://arxiv.org/abs/2403.19275](https://arxiv.org/abs/2403.19275)

    通过个性化知识和动态角色信息构建社交媒体代理以解决代理拥有不属于其角色的知识和无法消除多样化角色信息干扰的问题。

    

    构建个性化和拟人化代理在社交网络模拟中具有重要意义。然而，现有作品中仍存在两个关键问题：代理拥有不属于其角色的世界知识，不能消除多样化角色信息对当前行为的干扰，从而降低了代理的个性化和拟人化。为了解决以上问题，我们基于个性化知识和动态角色信息构建社交媒体代理。对于个性化知识，我们添加外部知识源并将其与代理的角色信息匹配，从而赋予代理个性化的世界知识。对于动态角色信息，我们使用当前行为信息内部检索代理的角色信息，从而减少多样化角色信息对当前行为的干扰。

    arXiv:2403.19275v1 Announce Type: cross  Abstract: Constructing personalized and anthropomorphic agents holds significant importance in the simulation of social networks. However, there are still two key problems in existing works: the agent possesses world knowledge that does not belong to its personas, and it cannot eliminate the interference of diverse persona information on current actions, which reduces the personalization and anthropomorphism of the agent. To solve the above problems, we construct the social media agent based on personalized knowledge and dynamic persona information. For personalized knowledge, we add external knowledge sources and match them with the persona information of agents, thereby giving the agent personalized world knowledge. For dynamic persona information, we use current action information to internally retrieve the persona information of the agent, thereby reducing the interference of diverse persona information on the current action. To make the age
    
[^112]: SemEval任务1：非洲和亚洲语言的语义文本相关性

    SemEval Task 1: Semantic Textual Relatedness for African and Asian Languages

    [https://arxiv.org/abs/2403.18933](https://arxiv.org/abs/2403.18933)

    这个任务涉及14种非洲和亚洲语言的语义文本相关性，旨在考察跨语言的语义相关性现象。

    

    我们介绍了第一个关于语义文本相关性（STR）的共享任务。而先前的共享任务主要关注语义相似性，我们则调查了跨越14种语言（包括南非荷兰语、阿尔及利亚阿拉伯语、阿姆哈拉语、英语、豪萨语、印地语、印尼语、基尼亚鲁安达语、马拉地语、摩洛哥阿拉伯语、现代标准阿拉伯语、旁遮普语、西班牙语和泰卢固语）的更广泛的语义相关性现象。这些语言来自五个不同的语系，并主要在非洲和亚洲地区使用，这些地区的特点是自然语言处理资源的相对有限。数据集中的每个实例都是一个与分数相关联的句对，该分数表示两个句子之间的语义文本相关程度。参与系统被要求在三个主要轨道中的14种语言中按它们在意义上的接近程度（即它们的语义相关性程度）对句对进行排名：(a) 监督，(b) 无监督

    arXiv:2403.18933v1 Announce Type: new  Abstract: We present the first shared task on Semantic Textual Relatedness (STR). While earlier shared tasks primarily focused on semantic similarity, we instead investigate the broader phenomenon of semantic relatedness across 14 languages: Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by the relatively limited availability of NLP resources. Each instance in the datasets is a sentence pair associated with a score that represents the degree of semantic textual relatedness between the two sentences. Participating systems were asked to rank sentence pairs by their closeness in meaning (i.e., their degree of semantic relatedness) in the 14 languages in three main tracks: (a) supervised, (b) unsupervi
    
[^113]: 大型语言模型中的长篇事实性

    Long-form factuality in large language models

    [https://arxiv.org/abs/2403.18802](https://arxiv.org/abs/2403.18802)

    该论文提出了一种通过使用大型语言模型将长篇回应分解为单个事实，并通过发送搜索查询到Google搜索，评估事实准确性的方法，并扩展了F1分数作为长篇事实性的聚合度量。

    

    大型语言模型（LLMs）在回答开放性主题的事实性提示时，经常生成包含事实错误的内容。为了在开放领域中对模型的长篇事实性进行基准测试，我们首先使用GPT-4生成了一个名为LongFact的提示集，其中包含数千个囊括38个主题的问题。然后，我们提出LLM代理可以通过一种名为Search-Augmented Factuality Evaluator（SAFE）的方法作为长篇事实性的自动评估器。SAFE利用LLM将长篇回应分解为一组单独的事实，并通过发送搜索查询到Google搜索以及确定一个事实是否得到搜索结果支持的多步推理过程来评估每个事实的准确性。此外，我们还提议将F1分数扩展为长篇事实性的聚合度量。为此，我们平衡了回应中支持事实的百分比（精度）与

    arXiv:2403.18802v1 Announce Type: cross  Abstract: Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the 
    
[^114]: 大型语言模型在教育领域的应用：调研与展望

    Large Language Models for Education: A Survey and Outlook

    [https://arxiv.org/abs/2403.18105](https://arxiv.org/abs/2403.18105)

    大型语言模型在教育领域的应用调研总结了LLMs在教育中的各种技术应用，包括学生和教师辅助、自适应学习和商业工具，提出了未来研究机会和潜在方向。

    

    大型语言模型（LLMs）的出现为教育领域带来了新的可能性。这篇调研论文总结了LLMs在教育环境中的各种技术，涵盖了学生和教师的辅助，自适应学习和商业工具。我们系统地审查了每个视角中的技术进步，整理了相关数据集和基准测试，并确定了在教育中部署LLMs所涉及的风险和挑战。此外，我们概述了未来的研究机会，突出了潜在的有前途的方向。我们的调研旨在为教育工作者、研究人员和决策者提供全面的技术图景，以利用LLMs的力量，彻底改革教育实践，并促进更有效的个性化学习环境。

    arXiv:2403.18105v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has brought in a new era of possibilities in the realm of education. This survey paper summarizes the various technologies of LLMs in educational settings from multifaceted perspectives, encompassing student and teacher assistance, adaptive learning, and commercial tools. We systematically review the technological advancements in each perspective, organize related datasets and benchmarks, and identify the risks and challenges associated with deploying LLMs in education. Furthermore, we outline future research opportunities, highlighting the potential promising directions. Our survey aims to provide a comprehensive technological picture for educators, researchers, and policymakers to harness the power of LLMs to revolutionize educational practices and foster a more effective personalized learning environment.
    
[^115]: 探究LLMs作为目标合成文本数据来源，以减少高置信度误分类

    Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications

    [https://arxiv.org/abs/2403.17860](https://arxiv.org/abs/2403.17860)

    探索使用大型语言模型（LLMs）生成合成数据以减少NLP模型高置信度误分类问题的研究。

    

    自然语言处理（NLP）模型经过优化以提高预测性能时，常常存在高置信度错误并容易受到对抗性和超出分布数据的影响。本研究探讨使用大型语言模型（LLMs）进行数据增强，作为解决NLP模型在分类任务中产生高置信度错误预测问题的潜在解决方案。我们比较了由LLMs生成的合成数据与通过相同过程获得的人工数据的有效性。为了减轻错误，人类或LLMs提供高置信度误分类的自然语言描述以生成合成数据，然后用于扩展训练集。我们对我们的方法在三个分类任务上进行了广泛评估，并展示了其在减少方面的有效性。

    arXiv:2403.17860v1 Announce Type: new  Abstract: Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the
    
[^116]: MapGuide: 从脑活动中重建连续语言的简单而有效方法

    MapGuide: A Simple yet Effective Method to Reconstruct Continuous Language from Brain Activities

    [https://arxiv.org/abs/2403.17516](https://arxiv.org/abs/2403.17516)

    本研究提出了一种直接比较预测文本嵌入的脑活动映射来指导文本重建的简单而有效方法，相比之前的间接方法显著提高了模型性能。

    

    从脑活动中解码连续语言是一项艰巨但有前景的研究领域。这对于帮助语言残障人士通过脑信号进行沟通尤为重要。本文提出了一种简单而有效的方法，通过直接将从脑活动映射的预测文本嵌入向导文本重建。全面的实验证明，我们的方法明显优于当前最先进的模型，在BLEU和METEOR分数上平均提高了77%和54%。

    arXiv:2403.17516v1 Announce Type: cross  Abstract: Decoding continuous language from brain activity is a formidable yet promising field of research. It is particularly significant for aiding people with speech disabilities to communicate through brain signals. This field addresses the complex task of mapping brain signals to text. The previous best attempt reverse-engineered this process in an indirect way: it began by learning to encode brain activity from text and then guided text generation by aligning with predicted brain responses. In contrast, we propose a simple yet effective method that guides text reconstruction by directly comparing them with the predicted text embeddings mapped from brain activities. Comprehensive experiments reveal that our method significantly outperforms the current state-of-the-art model, showing average improvements of 77% and 54% on BLEU and METEOR scores. We further validate the proposed modules through detailed ablation studies and case analyses and 
    
[^117]: 使用区域指导标记将孟加拉文本与地方方言转录为国际音标

    Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens

    [https://arxiv.org/abs/2403.17407](https://arxiv.org/abs/2403.17407)

    通过引入区域指导标记技术，本文提出了一种将孟加拉文本与地方方言转录为国际音标的方法，为模型提供了关于输入文本的地区方言信息，以理解与每个地区相关的独特音韵模式。

    

    孟加拉文本到国际音标（IPA）的准确转录是一项具有挑战性的任务，主要是由于语言的复杂音韵学和语境相关的音变。对于区域孟加拉方言来说，由于缺乏针对这些方言的标准拼写约定、当地和外语在这些地区中流行的词汇以及不同地区之间的音韵多样性，这一挑战甚至更为严峻。本文提出了一种方法来解决这个序列到序列的问题，即在覆盖孟加拉国六个地区的新数据集上引入“区域指导标记”（DGT）技术。其关键思想是在生成IPA转录之前向模型提供有关输入文本的区域方言或“地区”的明确信息。这通过在输入序列前添加一个地区标记来实现，有效地引导模型理解与每个地区相关的独特音韵模式。

    arXiv:2403.17407v1 Announce Type: cross  Abstract: Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes. This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions. This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh. The key idea is to provide the model with explicit information about the regional dialect or "district" of the input text before generating the IPA transcription. This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each 
    
[^118]: 通过编译器反馈迭代改进项目级代码上下文，以获得精确的代码生成

    Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback

    [https://arxiv.org/abs/2403.16792](https://arxiv.org/abs/2403.16792)

    本论文提出了一种名为ProCoder的新颖方法，通过编译器反馈引导，迭代地改进项目级代码上下文，以获得精确的代码生成

    

    大型语言模型(LLMs)在自动代码生成方面展现出了显著的进展。然而，将基于LLM的代码生成应用到现实项目中会面临挑战，因为生成的代码可能存在API使用、类、数据结构错误或缺少项目特定信息。鉴于大部分项目特定上下文无法适应LLMs的提示，我们必须找到让模型能够探索项目级代码上下文的方法。为此，本文提出了一种名为ProCoder的新颖方法，通过编译器反馈引导，迭代地改进项目级代码上下文，以获得精确的代码生成。具体而言，ProCoder首先利用编译器技术识别生成的代码与项目上下文之间的不匹配之处。然后，通过从代码库中提取的信息迭代地对齐和修复识别出的错误。我们将ProCoder与两个代表性的LLM集成，

    arXiv:2403.16792v1 Announce Type: new  Abstract: Large language models (LLMs) have shown remarkable progress in automated code generation. Yet, incorporating LLM-based code generation into real-life software projects poses challenges, as the generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. To this end, this paper puts forward a novel approach, termed ProCoder, which iteratively refines the project-level code context for precise code generation, guided by the compiler feedback. In particular, ProCoder first leverages compiler techniques to identify a mismatch between the generated code and the project's context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate ProCoder with two representative L
    
[^119]: 基于熵的文本水印检测方法

    An Entropy-based Text Watermarking Detection Method

    [https://arxiv.org/abs/2403.13485](https://arxiv.org/abs/2403.13485)

    提出了一种基于熵的水印检测方法，在检测过程中根据令牌的熵调整其权重，以更好地反映水印程度，该方法在大型语言模型中具有应用潜力。

    

    目前，大型语言模型（LLMs）的文本水印算法能够嵌入隐藏特征到LLMs生成的文本中，以便后续检测，从而缓解了LLMs被误用的问题。尽管当前的文本水印算法在大多数高熵情况下表现良好，但在低熵情况下仍需要改进。在这项工作中，我们提出在水印检测过程中应全面考虑令牌熵的影响，即应根据其熵调整每个令牌的重量，而不是像以前的方法中将所有令牌的重量设置为相同值。具体来说，我们提出了一种基于熵的水印检测（EWD），在水印检测过程中赋予高熵令牌更高的权重，以更好地反映水印程度。此外，所提出的检测过程无需训练。

    arXiv:2403.13485v1 Announce Type: new  Abstract: Currently, text watermarking algorithms for large language models (LLMs) can embed hidden features to texts generated by LLMs to facilitate subsequent detection, thus alleviating the problem of misuse of LLMs. Although the current text watermarking algorithms perform well in most high-entropy scenarios, its performance in low-entropy scenarios still needs to be improved. In this work, we proposed that the influence of token entropy should be fully considered in the watermark detection process, that is, the weight of each token should be adjusted according to its entropy during watermark detection, rather than setting the weight of all tokens to the same value as in previous methods. Specifically, we proposed an Entropy-based Watermark Detection (EWD) that gives higher-entropy tokens higher weights during watermark detection, so as to better reflect the degree of watermarking. Furthermore, the proposed detection process is training-free a
    
[^120]: AutoTRIZ：利用TRIZ和大型语言模型的人工创意

    AutoTRIZ: Artificial Ideation with TRIZ and Large Language Models

    [https://arxiv.org/abs/2403.13002](https://arxiv.org/abs/2403.13002)

    本文提出了AutoTRIZ，一种利用大型语言模型自动化和增强TRIZ方法的人工创意工具，为设计自动化和可解释创意提供了一种新颖方法。

    

    研究人员和创新者在开发思维方法方面做出了巨大努力，比如形态分析和类比设计，以辅助工程设计创意，解决问题和推动创新。在这些方法中，TRIZ作为最著名的方法脱颖而出，被广泛应用于系统化创新。然而，TRIZ资源和概念的复杂性，以及其对用户知识、经验和推理能力的依赖，限制了其实用性。本文提出了AutoTRIZ，一种利用大型语言模型（LLMs）自动化和增强TRIZ方法的人工创意工具。通过利用LLMs的广泛知识和先进推理能力，AutoTRIZ提供了一种新颖的利用人工智能进行设计自动化和可解释创意的方法。我们通过对矛盾检测和比较方面的一致性实验来证明并评估AutoTRIZ的有效性。

    arXiv:2403.13002v1 Announce Type: cross  Abstract: Researchers and innovators have made enormous efforts in developing ideation methods, such as morphological analysis and design-by-analogy, to aid engineering design ideation for problem solving and innovation. Among these, TRIZ stands out as the most well-known approach, widely applied for systematic innovation. However, the complexity of TRIZ resources and concepts, coupled with its reliance on users' knowledge, experience, and reasoning capabilities, limits its practicability. This paper proposes AutoTRIZ, an artificial ideation tool that leverages large language models (LLMs) to automate and enhance the TRIZ methodology. By leveraging the broad knowledge and advanced reasoning capabilities of LLMs, AutoTRIZ offers a novel approach to design automation and interpretable ideation with artificial intelligence. We demonstrate and evaluate the effectiveness of AutoTRIZ through consistency experiments in contradiction detection and compa
    
[^121]: CICLe: 适应上下文的大规模多类食品风险分类学习

    CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification

    [https://arxiv.org/abs/2403.11904](https://arxiv.org/abs/2403.11904)

    该论文提出了CICLe框架，基于适应上下文学习的方式，在大规模多类食品风险分类中取得了较好的效果，提出了基于符合预测的LLM-in-the-loop框架，可以提高基本分类器的性能，并减少能源消耗。

    

    受污染或掺假食品对人类健康构成重大风险。在给定了用于训练的标记网络文本集的情况下，可以应用机器学习和自然语言处理来自动检测这种风险。我们发布了一个包含7,546个描述公共食品召回公告的短文本数据集。每个文本都经过手动标记，分为两个粒度级别（粗粒度和细粒度），用于表示召回对应的食品产品和危害。我们描述了数据集并对朴素、传统和Transformer模型进行了基准测试。基于我们的分析，基于tf-idf表示的逻辑回归在支持较低的类别上优于RoBERTa和XLM-R。最后，我们讨论了不同的提示策略，并提出了一种基于符合预测的LLM-in-the-loop框架，这可以提高基本分类器的性能，同时减少了与普通提示相比的能源消耗。

    arXiv:2403.11904v1 Announce Type: new  Abstract: Contaminated or adulterated food poses a substantial risk to human health. Given sets of labeled web texts for training, Machine Learning and Natural Language Processing can be applied to automatically detect such risks. We publish a dataset of 7,546 short texts describing public food recall announcements. Each text is manually labeled, on two granularity levels (coarse and fine), for food products and hazards that the recall corresponds to. We describe the dataset and benchmark naive, traditional, and Transformer models. Based on our analysis, Logistic Regression based on a tf-idf representation outperforms RoBERTa and XLM-R on classes with low support. Finally, we discuss different prompting strategies and present an LLM-in-the-loop framework, based on Conformal Prediction, which boosts the performance of the base classifier while reducing energy consumption compared to normal prompting.
    
[^122]: 调查机器翻译中性别偏见的标记和驱动因素

    Investigating Markers and Drivers of Gender Bias in Machine Translations

    [https://arxiv.org/abs/2403.11896](https://arxiv.org/abs/2403.11896)

    通过使用反向翻译技术，比较五种中间语言的结果，并提出新的指标评估翻译中隐含的性别偏见变化。

    

    大语言模型（LLMs）中的隐含性别偏见是一个有充分文献支持的问题，通过自动翻译引入性别可能会延续现实世界的偏见。有些LLMs使用启发式或后处理来掩盖这种偏见，使调查变得困难。本文通过反向翻译来研究LLMs中的偏见，使用DeepL翻译API来调查重复翻译一组56个先前研究中使用的软件工程任务时所展现的偏见。每个陈述以 'she' 开始，并首先翻译为一个 '无性别' 中间语言，然后再翻译回英语；然后我们检查了反向翻译文本中的代词选择。我们通过以下方式扩展了先前的研究：（1）比较了五种中间语言（芬兰语、印度尼西亚语、爱沙尼亚语、土耳其语和匈牙利语）的结果；（2）提出了用于评估重复翻译中所暗示的性别变化的新度量标准。

    arXiv:2403.11896v1 Announce Type: new  Abstract: Implicit gender bias in Large Language Models (LLMs) is a well-documented problem, and implications of gender introduced into automatic translations can perpetuate real-world biases. However, some LLMs use heuristics or post-processing to mask such bias, making investigation difficult. Here, we examine bias in LLMss via back-translation, using the DeepL translation API to investigate the bias evinced when repeatedly translating a set of 56 Software Engineering tasks used in a previous study. Each statement starts with 'she', and is translated first into a 'genderless' intermediate language then back into English; we then examine pronoun- choice in the back-translated texts. We expand prior research in the following ways: (1) by comparing results across five intermediate languages, namely Finnish, Indonesian, Estonian, Turkish and Hungarian; (2) by proposing a novel metric for assessing the variation in gender implied in the repeated tran
    
[^123]: 忽略我但不要替代我：利用非语言元素进行网络安全领域的预训练

    Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain

    [https://arxiv.org/abs/2403.10576](https://arxiv.org/abs/2403.10576)

    利用非语言元素进行网络安全领域的预训练，提出了新的预训练方法并在网络安全领域中取得了优越表现

    

    针对网络安全信息通常技术复杂且通过非结构化文本传递，使得自动化处理网络威胁情报变得极具挑战性。针对涉及高度专业知识的文本领域，基于领域语料库的预训练一直是语言模型获取领域专业知识的一种常见方法。然而，网络安全文本通常包含非语言元素（如URL和哈希值），这可能与现有的预训练方法不适用。先前在其他领域的工作中，已将此类文本视为噪音进行移除或过滤，但这些方法的有效性尚未得到调查，特别是在网络安全领域。我们提出了不同的预训练方法，并通过下游任务和探测任务评估了它们的有效性。我们提出的策略（选择性MLM和联合训练NLE标记分类）优于常用的替换非

    arXiv:2403.10576v1 Announce Type: cross  Abstract: Cybersecurity information is often technically complex and relayed through unstructured text, making automation of cyber threat intelligence highly challenging. For such text domains that involve high levels of expertise, pretraining on in-domain corpora has been a popular method for language models to obtain domain expertise. However, cybersecurity texts often contain non-linguistic elements (such as URLs and hash values) that could be unsuitable with the established pretraining methodologies. Previous work in other domains have removed or filtered such text as noise, but the effectiveness of these methods have not been investigated, especially in the cybersecurity domain. We propose different pretraining methodologies and evaluate their effectiveness through downstream tasks and probing tasks. Our proposed strategy (selective MLM and jointly training NLE token classification) outperforms the commonly taken approach of replacing non-l
    
[^124]: 利用典型表示减轻社会偏见而不使用人口统计信息

    Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information

    [https://arxiv.org/abs/2403.09516](https://arxiv.org/abs/2403.09516)

    通过利用预定义的典型人口统计文本并在微调过程中加入正则化项，本文提出的方法有效减轻了语言模型中的社会偏见，同时不需要依赖显式的人口统计标签。

    

    减轻社会偏见通常需要识别与每个数据样本相关联的社会群体。在本文中，我们提出了DAFair，一种新颖的方法来解决语言模型中的社会偏见问题。与依赖显式人口统计标签的传统方法不同，我们的方法不需要任何此类信息。相反，我们利用预定义的人口统计典型文本，并在微调过程中加入一个正则化项来减轻模型表示中的偏见。我们在两个任务和两个模型上的实证结果展示了我们的方法相对于之前不依赖标记数据的方法的有效性。此外，即使使用有限的人口统计标注数据，我们的方法也优于常见的去偏见方法。

    arXiv:2403.09516v1 Announce Type: new  Abstract: Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DAFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model's representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.
    
[^125]: 一种用于自动生成医疗记录的持续预训练LLM方法

    A Continued Pretrained LLM Approach for Automatic Medical Note Generation

    [https://arxiv.org/abs/2403.09057](https://arxiv.org/abs/2403.09057)

    这项研究提出了一种用于医疗记录生成的持续预训练LLM方法，在PubMedQA方面性能优于GPT-4，能够更好地捕捉正确的医疗概念，并且在正确性和完整性方面超过人类抄写员。

    

    LLM（大型语言模型）正在革新自然语言处理任务。然而，像GPT-4这样的最强大的LLM对于大多数领域特定场景来说成本太高。我们提出了第一个连续训练的130亿参数 Llama2-basd LLM，专为医疗对话而设计，并在自动记录上进行了测试。我们的结果显示，我们的模型在PubMedQA中的准确率高达76.6％，在总结医疗对话为SOAP笔记方面与GPT-4的性能相当。值得注意的是，我们的模型在捕捉正确的医疗概念方面超过了GPT-4，并且在正确性和完整性方面超越了人类抄写员。

    arXiv:2403.09057v1 Announce Type: cross  Abstract: LLMs are revolutionizing NLP tasks. However, the most powerful LLM, like GPT-4, is too costly for most domain-specific scenarios. We present the first continuously trained 13B Llama2-based LLM that is purpose-built for medical conversations and measured on automated scribing. Our results show that our model outperforms GPT-4 in PubMedQA with 76.6\% accuracy and matches its performance in summarizing medical conversations into SOAP notes. Notably, our model exceeds GPT-4 in capturing a higher number of correct medical concepts and outperforms human scribes with higher correctness and completeness.
    
[^126]: JMI在SemEval 2024任务3中的应用：使用GPT和instruction-tuned Llama模型进行多模态情感因果分析的两步法

    JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models

    [https://arxiv.org/abs/2403.04798](https://arxiv.org/abs/2403.04798)

    本文介绍了针对SemEval-2024任务3开发的多模态情感因果分析系统，提出了通过两步框架解决多模态情感因果分析挑战的方法，并在实验中取得显著性能提升。

    

    本文介绍了我们针对SemEval-2024任务3：“对话中的多模态情感因果分析竞赛”开发的系统。有效捕捉人类对话中的情感需要整合文本、音频和视频等多种模态。然而，这些多样性模态的复杂性给开发高效的多模态情感因果分析系统带来了挑战。我们提出的方法通过两步框架来解决这些挑战。我们在实现中采用了两种不同的方法。在方法1中，我们使用两个单独的Llama 2模型进行情感和原因预测的instruction-tuning。在方法2中，我们使用GPT-4V进行会话级视频描述，并使用带有GPT 3.5的上下文学习对注释对话进行处理。我们的系统获得了第4名，系统消融实验表明，我们提出的解决方案取得了显著的性能增益。

    arXiv:2403.04798v1 Announce Type: new  Abstract: This paper presents our system development for SemEval-2024 Task 3: "The Competition of Multimodal Emotion Cause Analysis in Conversations". Effectively capturing emotions in human conversations requires integrating multiple modalities such as text, audio, and video. However, the complexities of these diverse modalities pose challenges for developing an efficient multimodal emotion cause analysis (ECA) system. Our proposed approach addresses these challenges by a two-step framework. We adopt two different approaches in our implementation. In Approach 1, we employ instruction-tuning with two separate Llama 2 models for emotion and cause prediction. In Approach 2, we use GPT-4V for conversation-level video description and employ in-context learning with annotated conversation using GPT 3.5. Our system wins rank 4, and system ablation experiments demonstrate that our proposed solutions achieve significant performance gains. All the experime
    
[^127]: 使用图理解和推理功能增强大规模语言模型的GraphInstruct

    GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability

    [https://arxiv.org/abs/2403.04483](https://arxiv.org/abs/2403.04483)

    该论文提出了一个名为GraphInstruct的基准，用于评估和增强大规模语言模型的图理解能力，并通过构建GraphLM和提出GraphLM+模型实现了显著的图推理能力增强。

    

    评估和增强大规模语言模型（LLMs）的通用能力一直是一个重要的研究课题。图是现实世界中常见的数据结构，理解图数据对于推进通用智能至关重要。为了评估和增强LLMs的图理解能力，在本文中，我们提出了一个名为GraphInstruct的基准，全面包括21个经典图推理任务，提供多样的图生成流水线和详细的推理步骤。基于GraphInstruct，我们进一步通过高效的指导调整构建了GraphLM，展示出显著的图理解能力。为了增强LLM的图推理能力，我们提出了一种步骤掩码训练策略，并构建了一个名为GraphLM+的模型。作为增强LLMs图理解和推理能力的先驱性努力之一，我们进行了大量实验。

    arXiv:2403.04483v1 Announce Type: new  Abstract: Evaluating and enhancing the general capabilities of large language models (LLMs) has been an important research topic. Graph is a common data structure in the real world, and understanding graph data is a crucial part for advancing general intelligence. To evaluate and enhance the graph understanding abilities of LLMs, in this paper, we propose a benchmark named GraphInstruct, which comprehensively includes 21 classical graph reasoning tasks, providing diverse graph generation pipelines and detailed reasoning steps. Based on GraphInstruct, we further construct GraphLM through efficient instruction-tuning, which shows prominent graph understanding capability. In order to enhance the LLM with graph reasoning capability as well, we propose a step mask training strategy, and construct a model named GraphLM+. As one of the pioneering efforts to enhance the graph understanding and reasoning abilities of LLMs, extensive experiments have demons
    
[^128]: 建模多模态社交互动：具有密集对齐表示的新挑战和基线

    Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations

    [https://arxiv.org/abs/2403.02090](https://arxiv.org/abs/2403.02090)

    提出了三个新的具有挑战性的任务来模拟多人之间的细粒度动态，并为社交推理游戏设置提供了广泛的数据注释；同时提出了一种新颖的多模态基线方法，利用密集对齐的语言-视觉表示。

    

    理解涉及言语和非言语线索的社交互动对有效解释社交情境至关重要。然而，大多数关于多模态社交线索的先前工作主要集中在单人行为上，或依赖于与多方环境中的话语密切对齐的整体视觉表示。它们在建模多方互动的复杂动态方面存在局限。在本文中，我们介绍了三个新的具有挑战性的任务，以建模多人之间的细粒度动态：话语目标识别、代词指代消解和提及玩家预测。我们为社交推理游戏设置中的这些新挑战提供了广泛的数据注释。此外，我们提出了一种新颖的多模态基线，通过将视觉特征与其对应的话语同步，利用密集对齐的语言-视觉表示，这有助于

    arXiv:2403.02090v1 Announce Type: cross  Abstract: Understanding social interactions involving both verbal and non-verbal cues is essential to effectively interpret social situations. However, most prior works on multimodal social cues focus predominantly on single-person behaviors or rely on holistic visual representations that are not densely aligned to utterances in multi-party environments. They are limited in modeling the intricate dynamics of multi-party interactions. In this paper, we introduce three new challenging tasks to model the fine-grained dynamics between multiple people: speaking target identification, pronoun coreference resolution, and mentioned player prediction. We contribute extensive data annotations to curate these new challenges in social deduction game settings. Furthermore, we propose a novel multimodal baseline that leverages densely aligned language-visual representations by synchronizing visual features with their corresponding utterances. This facilitates
    
[^129]: 学习在自然语言格式中压缩提示

    Learning to Compress Prompt in Natural Language Formats

    [https://arxiv.org/abs/2402.18700](https://arxiv.org/abs/2402.18700)

    该研究旨在通过提出自然语言提示封装（Nano-Capsulator）框架，解决了在自然语言格式中压缩提示的挑战，以提高大型语言模型的可转移性和性能。

    

    大型语言模型（LLMs）擅长处理多个自然语言处理任务，但它们的能力受到长上下文、推理速度慢以及计算结果成本高的限制。部署具有精确和信息丰富上下文的LLMs有助于用户更有效和更具成本效益地处理大规模数据集。现有作品依赖将长提示上下文压缩为软提示。然而，软提示压缩在不同LLM之间的可转移性受到限制，尤其是基于API的LLMs。因此，本研究旨在以LLM可转移性的形式压缩长提示的自然语言形式。这带来两个挑战：(i) 自然语言（NL）提示不兼容反向传播，(ii) NL提示在施加长度约束方面缺乏灵活性。在本研究中，我们提出了一种自然语言提示封装（Nano-Capsulator）框架

    arXiv:2402.18700v1 Announce Type: cross  Abstract: Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results. Deploying LLMs with precise and informative context helps users process large-scale datasets more effectively and cost-efficiently. Existing works rely on compressing long prompt contexts into soft prompts. However, soft prompt compression encounters limitations in transferability across different LLMs, especially API-based LLMs. To this end, this work aims to compress lengthy prompts in the form of natural language with LLM transferability. This poses two challenges: (i) Natural Language (NL) prompts are incompatible with back-propagation, and (ii) NL prompts lack flexibility in imposing length constraints. In this work, we propose a Natural Language Prompt Encapsulation (Nano-Capsulator) framewor
    
[^130]: 一张图片搞定：大型多模态模型是图片内学习者

    All in a Single Image: Large Multimodal Models are In-Image Learners

    [https://arxiv.org/abs/2402.17971](https://arxiv.org/abs/2402.17971)

    这项研究引入了一种名为图片内学习（I$^2$L）的新型上下文学习机制，将演示示例、视觉线索和指令合并到一个图片中，以提升GPT-4V的能力，并通过整合图像处理、理解和推理的能力来取得多个优点

    

    本文介绍了一种名为图片内学习（I$^2$L）的新型上下文学习（ICL）机制，将演示示例、视觉线索和指令合并到一张图片中，以增强GPT-4V的能力。与以往依赖将图像转换为文本或将视觉输入融入语言模型的方法不同，I$^2$L将所有信息整合到一张图片中，主要利用图像处理、理解和推理能力。这有几个优点：避免了对复杂图像的不准确文本描述，提供了在定位演示示例时的灵活性，减少了输入负担，并通过消除对多个图片和冗长文本的需求来避免超过输入限制。为了进一步结合不同ICL方法的优势，我们引入了一种自动策略，用于选择给定任务中数据示例的适当ICL方法。我们在MathVi上进行了实验

    arXiv:2402.17971v1 Announce Type: cross  Abstract: This paper introduces a new in-context learning (ICL) mechanism called In-Image Learning (I$^2$L) that combines demonstration examples, visual cues, and instructions into a single image to enhance the capabilities of GPT-4V. Unlike previous approaches that rely on converting images to text or incorporating visual input into language models, I$^2$L consolidates all information into one image and primarily leverages image processing, understanding, and reasoning abilities. This has several advantages: it avoids inaccurate textual descriptions of complex images, provides flexibility in positioning demonstration examples, reduces the input burden, and avoids exceeding input limits by eliminating the need for multiple images and lengthy text. To further combine the strengths of different ICL methods, we introduce an automatic strategy to select the appropriate ICL method for a data example in a given task. We conducted experiments on MathVi
    
[^131]: 理解大型语言模型开发背后的数据集管理者

    Understanding the Dataset Practitioners Behind Large Language Model Development

    [https://arxiv.org/abs/2402.16611](https://arxiv.org/abs/2402.16611)

    数据质量是大型语言模型开发中数据集管理者的首要任务，但管理者间对于数据质量定义和评估方法缺乏共识。

    

    随着大型语言模型(LLMs)变得越来越先进和有影响力，审视它们依赖和产生的数据变得越来越重要。本文探讨了数据集管理者的工作内容：首先，我们通过对谷歌贡献LLM开发团队责任的回顾性分析，定义了“数据集管理者”的角色。然后，我们对这些管理者进行了半结构化访谈（N=10）。我们发现数据质量是首要任务。为了评估数据质量，管理者要么凭直觉，要么编写自定义评估逻辑。管理者之间对数据质量的定义和评估方法缺乏共识。我们讨论了这种现象的潜在原因和实现一致性的机会。

    arXiv:2402.16611v1 Announce Type: new  Abstract: As large language models (LLMs) become more advanced and impactful, it is increasingly important to scrutinize the data that they rely upon and produce. What is it to be a dataset practitioner doing this work? We approach this in two parts: first, we define the role of "dataset practitioner" by performing a retrospective analysis on the responsibilities of teams contributing to LLM development at Google. Then, we conduct semi-structured interviews with a cross-section of these practitioners (N=10). We find that data quality is the top priority. To evaluate data quality, practitioners either rely on their own intuition or write custom evaluation logic. There is a lack of consensus across practitioners on what quality is and how to evaluate it. We discuss potential reasons for this phenomenon and opportunities for alignment.
    
[^132]: Triad: 一个利用基于多角色LLM代理的框架来解决知识库问答问题

    Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering

    [https://arxiv.org/abs/2402.14320](https://arxiv.org/abs/2402.14320)

    Triad框架利用了基于多角色LLM代理来解决知识库问答问题，通过代理的不同角色分别处理KBQA子任务，合作完成KBQA任务，并在多个基准数据集上表现出色。

    

    最近基于LLM代理的进展在各种任务中展现出了令人期待的结果。然而，它们在回答知识库中问题的运用仍然鲜为人知。使用传统方法来实现KBQA系统具有挑战性，因为缺乏特定任务训练数据以及创建以任务为中心的模型结构的复杂性。在本文中，我们提出了Triad，一个利用具有三个角色的LLM代理的统一框架来进行KBQA任务。代理被分配三个角色来处理不同的KBQA子任务：作为掌握各种子任务的通才，作为选择候选者的决策者，以及作为回答带有知识的问题的顾问。我们的KBQA框架在四个阶段中执行，涉及代理的多重角色的协作。我们使用三个基准数据集评估了我们框架的性能，结果显示我们的框架胜过了

    arXiv:2402.14320v1 Announce Type: cross  Abstract: Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms 
    
[^133]: 打破HISCO障碍：使用OccCANINE进行自动职业标准化

    Breaking the HISCO Barrier: Automatic Occupational Standardization with OccCANINE

    [https://arxiv.org/abs/2402.13604](https://arxiv.org/abs/2402.13604)

    通过OccCANINE工具，我们成功打破了HISCO障碍，实现了自动化职业标准化，从而大大简化了对职业描述的处理和分类过程，为经济学、经济历史等领域的职业结构分析提供了高效且准确的数据。

    

    这篇论文介绍了一种新工具OccCANINE，可自动将职业描述转换为HISCO分类系统。处理和分类职业描述涉及的手动工作容易出错、繁琐且耗时。我们对一个现有的语言模型（CANINE）进行了微调，使其能够在几秒钟到几分钟内自动完成此过程，而以前需要数天甚至数周。该模型在来自22个不同来源贡献的13种语言中的1400万对职业描述和HISCO代码上进行训练。我们的方法表现出精度、召回率和准确率均超过90%。我们的工具突破了象征性HISCO障碍，并使这些数据可供经济学、经济历史和各种相关学科中的职业结构分析使用。

    arXiv:2402.13604v1 Announce Type: new  Abstract: This paper introduces a new tool, OccCANINE, to automatically transform occupational descriptions into the HISCO classification system. The manual work involved in processing and classifying occupational descriptions is error-prone, tedious, and time-consuming. We finetune a preexisting language model (CANINE) to do this automatically thereby performing in seconds and minutes what previously took days and weeks. The model is trained on 14 million pairs of occupational descriptions and HISCO codes in 13 different languages contributed by 22 different sources. Our approach is shown to have accuracy, recall and precision above 90 percent. Our tool breaks the metaphorical HISCO barrier and makes this data readily available for analysis of occupational structures with broad applicability in economics, economic history and various related disciplines.
    
[^134]: 朝着可信的再排序：一种简单但有效的弃权机制

    Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism

    [https://arxiv.org/abs/2402.12997](https://arxiv.org/abs/2402.12997)

    提出了一种适用于现实约束的轻量级弃权机制，特别适用于再排序阶段，通过数据驱动的方法达到有效性，并提供了开源代码以促进其更广泛的应用。

    

    神经信息检索（NIR）已经显著改进了基于启发式的IR系统。然而，失败仍然频繁发生，通常所使用的模型无法检索与用户查询相关的文档。我们通过提出一种适用于现实约束的轻量级弃权机制来解决这一挑战，特别强调再排序阶段。我们介绍了一个协议，用于在黑匣子场景中评估弃权策略的效果，并提出了一种简单但有效的数据驱动机制。我们提供了实验复制和弃权实施的开源代码，促进其在不同环境中更广泛的采用和应用。

    arXiv:2402.12997v1 Announce Type: cross  Abstract: Neural Information Retrieval (NIR) has significantly improved upon heuristic-based IR systems. Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user's query. We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the reranking phase. We introduce a protocol for evaluating abstention strategies in a black-box scenario, demonstrating their efficacy, and propose a simple yet effective data-driven mechanism. We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts.
    
[^135]: 在语言模型对话中测量和控制“人设”漂移

    Measuring and Controlling Persona Drift in Language Model Dialogs

    [https://arxiv.org/abs/2402.10962](https://arxiv.org/abs/2402.10962)

    提出了一种量化基准来测量语言模型对话中的“人设”漂移，并提出了一种称为split-softmax的轻量级方法来对抗注意力衰减和“人设”漂移

    

    提示是定制语言模型聊天机器人的标准工具，使其能够承担特定的“人设”。在使用提示时的一个隐含假设是，它们将是稳定的，因此聊天机器人将在整个对话过程中继续根据规定的“人设”生成文本。我们提出了一个量化基准来测试这一假设，通过两个个性化聊天机器人之间的自我对话来评估“人设”的稳定性。我们对流行模型如LLaMA2-chat-70B进行测试，发现在八轮对话中存在显著的“人设”漂移。对这一现象的实证和理论分析表明，由于长对话中的注意力衰减，变压器注意力机制起到了一定作用。为了对抗注意力衰减和“人设”漂移，我们提出了一种称为split-softmax的轻量级方法，与两个强基线方法相比表现优异。

    arXiv:2402.10962v1 Announce Type: cross  Abstract: Prompting is a standard tool for customizing language-model chatbots, enabling them to take on a specific "persona". An implicit assumption in the use of prompts is that they will be stable, so the chatbot will continue to generate text according to the stipulated persona for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating persona stability via self-chats between two personalized chatbots. Testing popular models like LLaMA2-chat-70B, we reveal a significant persona drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.
    
[^136]: AutoMathText：使用语言模型进行数学文本的自主数据选择

    AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts

    [https://arxiv.org/abs/2402.07625](https://arxiv.org/abs/2402.07625)

    本论文介绍了一种自主数据选择策略，利用语言模型进行数学文本的自动评估和选择，并通过连续预训练显著提高了数学推理能力。主要创新包括利用元提示语言模型作为验证器，发布了高质量的AutoMathText数据集，并实现了预训练令牌效率的提升。

    

    为了通过持续的预训练改善语言模型在数学推理方面的能力，我们引入了一种新颖的策略，利用基础语言模型进行自主数据选择。与传统的有人工标注数据的监督微调或训练过的分类器不同，我们的方法利用元提示语言模型作为零样本验证器，自主评估和选择高质量的数学内容，并发布了经过策划的开源AutoMathText数据集，其中包含超过200GB的数据。为了证明我们方法的有效性，我们对AutoMathText数据集进行了连续预训练，使得7B参数的Mistral语言模型在MATH数据集上的下游性能大幅提升，而令牌数量比之前的连续预训练工作减少了几个数量级。我们的方法展示了基准的预训练令牌效率提高了2倍，突显了我们方法在增强中的潜力。

    To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing
    
[^137]: FEUDA：令人沮丧地简单的基于提示的无监督领域自适应

    FEUDA: Frustratingly Easy Prompt Based Unsupervised Domain Adaptation

    [https://arxiv.org/abs/2401.17514](https://arxiv.org/abs/2401.17514)

    FEUDA是一种令人沮丧地简单的无监督领域自适应方法，通过在未标记和标记的示例上训练自回归语言模型，在提示基础的分类框架中探索无监督领域自适应的新范例。

    

    无监督领域自适应方法的一个主要分支利用来自源领域和目标领域的未标记数据，学习适应的领域不变表示。然而，这些方法存在一定的局限性，鼓励通过持续的预训练使用自监督学习。在基于提示的分类框架中，持续的预训练或学习领域不变表示的必要性仍不清楚，其中一个输入示例由模板修改后，再输入到语言模型（LM）中生成一个标签字符串。为了研究基于提示的无监督领域自适应中的这种新范例，我们提出了一种令人沮丧地简单的无监督领域自适应方法（FEUDA），该方法使用两种不同的指令调整任务，在未标记和标记的示例上训练自回归LM。具体而言，第一个任务通过掩蔽语言建模（MLM）在两个领域的未标记文本上训练LM，第二个任务使用源标记数据进行监督指令调整。

    A major thread of unsupervised domain adaptation (UDA) methods uses unlabeled data from both source and target domains to learn domain-invariant representations for adaptation. However, these methods showcase certain limitations, encouraging the use of self-supervised learning through continued pre-training. The necessity of continued pre-training or learning domain-invariant representations is still unclear in the prompt-based classification framework, where an input example is modified by a template and then fed into a language model (LM) to generate a label string. To examine this new paradigm of UDA in the prompt-based setup, we propose a frustratingly easy UDA method (FEUDA) that trains an autoregressive LM on both unlabeled and labeled examples using two different instruction-tuning tasks. Specifically, the first task trains the LM on unlabeled texts from both domains via masked language modeling (MLM), and the other uses supervised instruction-tuning on source-labeled data for c
    
[^138]: 应对后API困境：搜索引擎结果页面呈现社交媒体数据的偏见观

    Navigating the Post-API Dilemma Search Engine Results Pages Present a Biased View of Social Media Data

    [https://arxiv.org/abs/2401.15479](https://arxiv.org/abs/2401.15479)

    搜索引擎结果页面可以作为社交媒体数据的替代方案，但存在对流行帖子偏见较高、情感更积极以及忽视政治、色情和粗俗帖子的问题。

    

    最近停止访问社交媒体API的决定对互联网研究和整个计算社会科学领域产生了不利影响。这种对数据的访问缺乏已被称为互联网研究的后API时代。幸运的是，流行的搜索引擎有能力爬取、捕获和展示社交媒体数据在其搜索引擎结果页面(SERP)上，如果提供适当的搜索查询，可能会为这一困境提供解决方案。在当前工作中，我们问：SERP是否提供社交媒体数据的完整和无偏见样本？ SERP是否是直接API访问的可行替代方案？为了回答这些问题，我们对（Google）SERP结果和来自Reddit和Twitter/X的非取样数据进行了比较分析。我们发现，SERP结果在支持流行帖子方面存在高度偏见；反对政治、色情和粗俗帖子；在情感上更为积极；并有大

    arXiv:2401.15479v2 Announce Type: replace-cross  Abstract: Recent decisions to discontinue access to social media APIs are having detrimental effects on Internet research and the field of computational social science as a whole. This lack of access to data has been dubbed the Post-API era of Internet research. Fortunately, popular search engines have the means to crawl, capture, and surface social media data on their Search Engine Results Pages (SERP) if provided the proper search query, and may provide a solution to this dilemma. In the present work we ask: does SERP provide a complete and unbiased sample of social media data? Is SERP a viable alternative to direct API-access? To answer these questions, we perform a comparative analysis between (Google) SERP results and nonsampled data from Reddit and Twitter/X. We find that SERP results are highly biased in favor of popular posts; against political, pornographic, and vulgar posts; are more positive in their sentiment; and have large 
    
[^139]: TeleChat技术报告

    TeleChat Technical Report

    [https://arxiv.org/abs/2401.03804](https://arxiv.org/abs/2401.03804)

    TeleChat是一个包含30亿、70亿和120亿参数的大型语言模型合集，旨在提供预训练语言模型和与人类喜好相一致的微调聊天模型。研究表明，TeleChat在各种任务上表现出与其他类似规模的开源模型相当的性能。

    

    在这份技术报告中，我们介绍了TeleChat，这是由30亿、70亿和120亿参数的大型语言模型（LLMs）组成的合集。它包括预训练的语言模型以及与人类喜好相一致的微调聊天模型。TeleChat最初在包含英语和中文语言的多样文本集合中预训练，包括数万亿的标记。随后，该模型经过微调以与人类喜好相一致，遵循我们描述的详细方法论。我们评估了TeleChat在各种任务上的表现，包括语言理解、数学、推理、代码生成和基于知识的问答。我们的发现表明，TeleChat在各种公开基准测试中表现出与其他开源模型相似规模的可比性能。为支持未来利用该模型进行研究和应用，

    arXiv:2401.03804v2 Announce Type: replace-cross  Abstract: In this technical report, we present TeleChat, a collection of large language models (LLMs) with parameters of 3 billion, 7 billion and 12 billion. It includes pretrained language models as well as fine-tuned chat models that is aligned with human preferences. TeleChat is initially pretrained on an extensive corpus containing a diverse collection of texts from both English and Chinese languages, including trillions of tokens. Subsequently, the model undergoes fine-tuning to align with human preferences, following a detailed methodology that we describe. We evaluate the performance of TeleChat on various tasks, including language understanding, mathematics, reasoning, code generation, and knowledge-based question answering. Our findings indicate that TeleChat achieves comparable performance to other open-source models of similar size across a wide range of public benchmarks. To support future research and applications utilizing 
    
[^140]: Transformer长度外推：从位置编码的角度进行调查

    Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding

    [https://arxiv.org/abs/2312.17044](https://arxiv.org/abs/2312.17044)

    本调查从位置编码的角度总结了用于增强Transformer长度外推能力的各种方法，包括绝对和相对位置编码以及基于它们的外推方法。

    

    Transformer自诞生以来已经在自然语言处理（NLP）领域掀起了一股风暴。建立在其基础上的大型语言模型（LLMs）由于其出色的能力而受到全球关注。然而，包括这些强大的LLMs在内的所有基于Transformer的模型都受制于预设的长度限制，很难从短训练序列推广到更长的推断序列，即它们无法进行长度外推。因此，已经提出了大量方法来增强Transformer的长度外推能力，其中位置编码（PE）被认为是主要因素。 在这项调查中，我们从PE的角度以统一符号介绍了这些关于长度外推的进展。具体而言，我们首先介绍了可外推的PE，包括绝对和相对PE。然后，我们深入探讨了基于它们的外推方法，涵盖了位置插值和随机化位置方法。

    arXiv:2312.17044v3 Announce Type: replace  Abstract: Transformer has taken the field of natural language processing (NLP) by storm since its birth. Further, Large language models (LLMs) built upon it have captured worldwide attention due to its superior abilities. Nevertheless, all Transformer-based models including these powerful LLMs suffer from a preset length limit and can hardly generalize from short training sequences to longer inference ones, namely, they can not perform length extrapolation. Hence, a plethora of methods have been proposed to enhance length extrapolation of Transformer, in which the positional encoding (PE) is recognized as the major factor. In this survey, we present these advances towards length extrapolation in a unified notation from the perspective of PE. Specifically, we first introduce extrapolatable PEs, including absolute and relative PEs. Then, we dive into extrapolation methods based on them, covering position interpolation and randomized position met
    
[^141]: 大型人类语言模型：需求和挑战

    Large Human Language Models: A Need and the Challenges

    [https://arxiv.org/abs/2312.07751](https://arxiv.org/abs/2312.07751)

    大型人类语言模型的建立需要更好地整合人类背景，并面临着如何捕捉人类因素、如何表示以及如何建模的一系列挑战。

    

    随着人类中心的自然语言处理研究的进展，人们越来越意识到将人类和社会因素纳入到自然语言处理模型的重要性。同时，我们的自然语言处理系统已经严重依赖于LLM，其中大多数并没有对作者进行建模。为了构建能够真正理解人类语言的自然语言处理系统，我们必须更好地将人类背景整合到LLM中。这提出了一系列设计考虑和挑战，涉及到要捕捉哪些人类因素、如何表示它们以及要采用何种建模策略等问题。为了应对这些挑战，我们提倡从心理学和行为科学的概念出发，支持三个立场来创建大型人类语言模型（LHLMs）：首先，语言模型训练应包括人类背景。其次，LHLMs应该意识到人不仅仅是他们所属的群体。第三，LHLMs应该能够考虑到人类背景的动态和时间依赖性特点。

    arXiv:2312.07751v2 Announce Type: replace-cross  Abstract: As research in human-centered NLP advances, there is a growing recognition of the importance of incorporating human and social factors into NLP models. At the same time, our NLP systems have become heavily reliant on LLMs, most of which do not model authors. To build NLP systems that can truly understand human language, we must better integrate human contexts into LLMs. This brings to the fore a range of design considerations and challenges in terms of what human aspects to capture, how to represent them, and what modeling strategies to pursue. To address these, we advocate for three positions toward creating large human language models (LHLMs) using concepts from psychological and behavioral sciences: First, LM training should include the human context. Second, LHLMs should recognize that people are more than their group(s). Third, LHLMs should be able to account for the dynamic and temporally-dependent nature of the human con
    
[^142]: HALO：用于在大型语言模型中表示和分类幻觉的本体论

    HALO: An Ontology for Representing and Categorizing Hallucinations in Large Language Models

    [https://arxiv.org/abs/2312.05209](https://arxiv.org/abs/2312.05209)

    本文介绍了一种名为HALO的形式化、可扩展的本体论，用OWL编写，用于描述和表示大型语言模型中的六种不同类型的幻觉。

    

    arXiv:2312.05209v2 公告类型：替换 摘要：生成式人工智能的最新进展，包括ChatGPT等大型语言模型（LLMs），为从自然语言处理到知识发现和数据挖掘等领域带来了重大机遇。然而，人们越来越意识到这些模型可能存在问题，例如虚构信息或“幻觉”，以及在看似简单问题上犯错误的推理能力。由于像ChatGPT这样的模型备受欢迎，学术界和公民科学家都记录了多种类型和严重程度的幻觉。尽管已有相关工作，但仍然缺乏一种形式模型来细致描述和表示这些幻觉（带有相关元数据）。本文通过提出Hallucination Ontology或HALO来填补这一空白，HALO是一种正式的、可扩展的本体论，用OWL编写，目前支持六种已知的幻觉类型。

    arXiv:2312.05209v2 Announce Type: replace  Abstract: Recent progress in generative AI, including large language models (LLMs) like ChatGPT, has opened up significant opportunities in fields ranging from natural language processing to knowledge discovery and data mining. However, there is also a growing awareness that the models can be prone to problems such as making information up or `hallucinations', and faulty reasoning on seemingly simple problems. Because of the popularity of models like ChatGPT, both academic scholars and citizen scientists have documented hallucinations of several different types and severity. Despite this body of work, a formal model for describing and representing these hallucinations (with relevant meta-data) at a fine-grained level, is still lacking. In this paper, we address this gap by presenting the Hallucination Ontology or HALO, a formal, extensible ontology written in OWL that currently offers support for six different types of hallucinations known to 
    
[^143]: 数学家的大型语言模型

    Large Language Models for Mathematicians

    [https://arxiv.org/abs/2312.04556](https://arxiv.org/abs/2312.04556)

    大型语言模型（LLMs）如ChatGPT因其通用语言理解的能力以及生成高质量文本或计算机代码的能力而备受关注，对数学家的潜在帮助和改变工作方式的影响进行了讨论。

    

    大型语言模型（LLMs）如ChatGPT因其通用语言理解的能力以及生成高质量文本或计算机代码的能力而备受关注。对许多职业来说，LLMs代表一种无价的工具，可以加快工作速度并提高工作质量。本文讨论它们在帮助专业数学家方面的作用程度。我们首先提供了所有现代语言模型中使用的Transformer模型的数学描述。基于最近的研究，我们概述了最佳实践和潜在问题，并报告了语言模型的数学能力。最后，我们探讨了LLMs改变数学家工作方式的潜力。

    arXiv:2312.04556v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) such as ChatGPT have received immense interest for their general-purpose language understanding and, in particular, their ability to generate high-quality text or computer code. For many professions, LLMs represent an invaluable tool that can speed up and improve the quality of work. In this note, we discuss to what extent they can aid professional mathematicians. We first provide a mathematical description of the transformer model used in all modern language models. Based on recent studies, we then outline best practices and potential issues and report on the mathematical abilities of language models. Finally, we shed light on the potential of LLMs to change how mathematicians work.
    
[^144]: 更多样本还是更多提示？探索LLM少样本提示工程中有效的上下文采样

    More Samples or More Prompts? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering

    [https://arxiv.org/abs/2311.09782](https://arxiv.org/abs/2311.09782)

    提出了一种低资源的LLM提示技术In-Context Sampling（ICS），通过优化多个ICL提示输入的构建来生成自信的预测，并展示了ICS可以持续增强LLM的性能，同时在数据相似性的基础上提出了新的研究方向。

    

    虽然大多数现有关于LLM提示技术的工作只关注如何在单个提示输入中选择更好的数据样本集（上下文学习或ICL），但为什么我们不能设计并利用多个提示来进一步提高LLM的性能呢？在这项工作中，我们提出了一种低资源的LLM提示技术In-Context Sampling（ICS），通过优化多个ICL提示输入的构建来生成自信的预测。在四个NLI数据集（e-SNLI、Multi-NLI、ANLI和Contract-NLI）和一个QA数据集（CommonsenseQA）上，与三个开源LLM（FlanT5-XL、Mistral-7B和Mixtral-8x7B）的大量实验表明ICS可以持续增强LLM的性能。与三种基于数据相似性的ICS策略的深入评估表明，这些策略可以进一步提升LLM的性能，为新的但有前途的未来研究方向投下光芒。

    arXiv:2311.09782v2 Announce Type: replace  Abstract: While most existing works on LLM prompting techniques focus only on how to select a better set of data samples inside one single prompt input (In-Context Learning or ICL), why can not we design and leverage multiple prompts together to further improve the LLM's performance? In this work, we propose In-Context Sampling (ICS), a low-resource LLM prompting technique to produce confident predictions by optimizing the construction of multiple ICL prompt inputs. Extensive experiments with three open-source LLMs (FlanT5-XL, Mistral-7B, and Mixtral-8x7B) on four NLI datasets (e-SNLI, Multi-NLI, ANLI, and Contract-NLI) and one QA dataset (CommonsenseQA) illustrate that ICS can consistently enhance LLMs' performance. An in-depth evaluation with three data similarity-based ICS strategies suggests that these strategies can further elevate LLM's performance, which sheds light on a new yet promising future research direction.
    
[^145]: 你不需要进行人格测试来知道这些模型是不可靠的：评估大型语言模型在心理测量工具上的可靠性

    You don't need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments

    [https://arxiv.org/abs/2311.09718](https://arxiv.org/abs/2311.09718)

    评估大型语言模型在人格测量工具上的可靠性，研究探讨当前提示方式是否导致一致且稳健的响应

    

    大型语言模型（LLMs）在自然语言理解任务中的多功能性使其在社会科学研究中备受青睐。为了正确理解LLMs的属性和固有人格，研究人员进行了涉及使用提示的研究，这些提示采用问题形式询问LLMs关于特定观点。在本研究中，我们谨慎退一步，检查当前提示LLMs的格式是否以一致且稳健的方式引出响应。我们首先构建了一个包含693个问题的数据集，涵盖115个人格轴上的39种不同人格测量工具。此外，我们设计了一组包含微小变化的提示，并检查LLMs生成答案的能力，以及提示变化以检查它们在内容级别变化方面的一致性，例如更改响应选项的顺序或否定该语句。

    arXiv:2311.09718v2 Announce Type: replace-cross  Abstract: The versatility of Large Language Models (LLMs) on natural language understanding tasks has made them popular for research in social sciences. To properly understand the properties and innate personas of LLMs, researchers have performed studies that involve using prompts in the form of questions that ask LLMs about particular opinions. In this study, we take a cautionary step back and examine whether the current format of prompting LLMs elicits responses in a consistent and robust manner. We first construct a dataset that contains 693 questions encompassing 39 different instruments of persona measurement on 115 persona axes. Additionally, we design a set of prompts containing minor variations and examine LLMs' capabilities to generate answers, as well as prompt variations to examine their consistency with respect to content-level variations such as switching the order of response options or negating the statement. Our experimen
    
[^146]: 事件因果关系是计算故事理解的关键

    Event Causality Is Key to Computational Story Understanding

    [https://arxiv.org/abs/2311.09648](https://arxiv.org/abs/2311.09648)

    我们提出了一种利用最新语言模型进展的事件因果关系识别方法，在计算故事理解方面取得了实质性进展。

    

    认知科学和符号人工智能研究表明，事件因果关系为故事理解提供了重要信息。然而，用于故事理解的机器学习系统很少使用事件因果关系，部分原因是缺乏可靠地识别开放世界因果事件关系的方法。借助近期在大型语言模型方面的进展，我们提出了一种事件因果关系识别方法，这种方法在计算机故事理解方面带来了实质性的改进。我们的技术在COPES数据集（Wang等，2023年）的因果事件关系识别方面达到了新的技术水平。此外，在下游故事质量评估任务中，识别的因果关系导致与人类评级的相关性提高了3.6-16.6%。在多模式故事视频-文本对齐任务中，我们实现了Clip准确性提高4.1-10.9%以及句子IoU提高4.2-13.5%。这些发现表明...

    arXiv:2311.09648v2 Announce Type: replace  Abstract: Cognitive science and symbolic AI research suggest that event causality provides vital information for story understanding. However, machine learning systems for story understanding rarely employ event causality, partially due to the lack of methods that reliably identify open-world causal event relations. Leveraging recent progress in large language models, we present the first method for event causality identification that leads to material improvements in computational story understanding. Our technique sets a new state of the art on the COPES dataset (Wang et al., 2023) for causal event relation identification. Further, in the downstream story quality evaluation task, the identified causal relations lead to 3.6-16.6% relative improvement on correlation with human ratings. In the multimodal story video-text alignment task, we attain 4.1-10.9% increase on Clip Accuracy and 4.2-13.5% increase on Sentence IoU. The findings indicate s
    
[^147]: 关于检索增强和语言模型训练的局限性

    On Retrieval Augmentation and the Limitations of Language Model Training

    [https://arxiv.org/abs/2311.09615](https://arxiv.org/abs/2311.09615)

    将$k$NN检索与语言模型相结合可以降低困惑度，并提出了使用多层感知机模型的方法，将存储成本降低超过25倍。

    

    将语言模型（LM）与其训练数据上的$k$-nearest neighbors ($k$NN)检索相结合可以降低其困惑度，尽管其背后的原因尚不清楚。本文排除了先前提出的一个可能性-“softmax瓶颈”。我们创造了一个新数据集，用于评估LM在训练数据中包含额外非因果相关信息的情况下的泛化能力。即使对于GPT-3.5 Turbo来说，这项任务也具有挑战性。我们展示了对于GPT-2和Mistral 7B，$k$NN检索增强在这种情况下始终提升了性能。最后，为了使$k$NN检索更易用，提出了使用多层感知机模型将数据存储键映射到值，作为传统检索的替代方案。这样可以将存储成本降低超过25倍。

    arXiv:2311.09615v2 Announce Type: replace  Abstract: Augmenting a language model (LM) with $k$-nearest neighbors ($k$NN) retrieval on its training data alone can decrease its perplexity, though the underlying reasons for this remain elusive. In this work, we rule out one previously posited possibility -- the "softmax bottleneck." We then create a new dataset to evaluate LM generalization ability in the setting where training data contains additional information that is not causally relevant. This task is challenging even for GPT-3.5 Turbo. We show that, for both GPT-2 and Mistral 7B, $k$NN retrieval augmentation consistently improves performance in this setting. Finally, to make $k$NN retrieval more accessible, we propose using a multi-layer perceptron model that maps datastore keys to values as a drop-in replacement for traditional retrieval. This reduces storage costs by over 25x.
    
[^148]: 使用Rationale Distillation实现高效的端到端视觉文档理解

    Efficient End-to-End Visual Document Understanding with Rationale Distillation

    [https://arxiv.org/abs/2311.09612](https://arxiv.org/abs/2311.09612)

    提出了一种称为Rationale Distillation的方法，通过合并OCR工具输出、LLMs和更大的多模态模型的中间“rationales”，并训练一个小型学生模型来预测rationales和答案，以实现对视觉文档的高效端到端理解。

    

    理解视觉语境需要解释复杂的文本和视觉元素布局。预处理工具，如光学字符识别（OCR），可以将文档图像输入映射到文本标记，然后大型语言模型（LLMs）可以对文本进行推理。然而，这种方法具有高计算和工程复杂性。小型预训练的图像到文本模型能否通过类似的识别和推理步骤准确理解视觉文档？我们提出Rationale Distillation（RD），它将OCR工具、LLMs和更大的多模态模型的输出作为中间“rationales”，并训练一个小型学生模型来预测rationales和答案。在代表信息图表、扫描文档和图表的三个视觉文档理解基准上，我们的Pix2Struct（282M参数）学生模型在经过RD微调后的表现优于基准模型4-5%的绝对准确度。

    arXiv:2311.09612v2 Announce Type: replace-cross  Abstract: Understanding visually situated language requires interpreting complex layouts of textual and visual elements. Pre-processing tools, such as optical character recognition (OCR), can map document image inputs to textual tokens, then large language models (LLMs) can reason over text. However, such methods have high computational and engineering complexity. Can small pretrained image-to-text models accurately understand visual documents through similar recognition and reasoning steps instead? We propose Rationale Distillation (RD), which incorporates the outputs of OCR tools, LLMs, and larger multimodal models as intermediate "rationales", and trains a small student model to predict both rationales and answers. On three visual document understanding benchmarks representing infographics, scanned documents, and figures, our Pix2Struct (282M parameters) student model finetuned with RD outperforms the base model by 4-5% absolute accur
    
[^149]: 如果你用不同方式表达会怎样：解释格式如何影响人类反馈效果和用户感知

    What if you said that differently?: How Explanation Formats Affect Human Feedback Efficacy and User Perception

    [https://arxiv.org/abs/2311.09558](https://arxiv.org/abs/2311.09558)

    解释格式对人类反馈效果和用户感知有重要影响，提供了研究分析如何呈现模型响应以便更易接受用户纠正的策略。

    

    从NLP模型的最终用户那里获取反馈对于改进模型是有益的。然而，我们应该如何向用户呈现模型的响应，使其最易于接受来自用户反馈的更正？此外，用户看重理解和信任响应的哪些属性？我们通过分析由QA模型生成的支持其答案的理由（或解释）来回答这些问题。我们特别考虑将QA模型分解为首先根据上下文和问题提取中间原因，然后仅使用该原因来回答问题的模型。原因概述了模型回答问题的方法。我们的工作考虑了这些原因的各种格式，根据感兴趣的明确定义的属性而变化。我们使用少量提示从语言模型中对两个数据集进行抽样，然后进行两项用户研究。首先，我们向用户展示不正确的情况

    arXiv:2311.09558v2 Announce Type: replace  Abstract: Eliciting feedback from end users of NLP models can be beneficial for improving models. However, how should we present model responses to users so they are most amenable to be corrected from user feedback? Further, what properties do users value to understand and trust responses? We answer these questions by analyzing the effect of rationales (or explanations) generated by QA models to support their answers. We specifically consider decomposed QA models that first extract an intermediate rationale based on a context and a question and then use solely this rationale to answer the question. A rationale outlines the approach followed by the model to answer the question. Our work considers various formats of these rationales that vary according to well-defined properties of interest. We sample rationales from language models using few-shot prompting for two datasets, and then perform two user studies. First, we present users with incorre
    
[^150]: 开源LLMs的可信度有多高？对恶意演示下的评估显示它们的脆弱性

    How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities

    [https://arxiv.org/abs/2311.09447](https://arxiv.org/abs/2311.09447)

    本研究通过恶意演示在八个方面对开源LLMs的可信度进行了敌对评估，提出了一种新的攻击策略advCoU，以揭示它们的脆弱性。

    

    arXiv:2311.09447v2 公告类型：替换-交叉摘要：开源大型语言模型（LLMs）的快速发展显著推动着人工智能的发展。然而，对它们的可信度仍然了解有限。在规模部署这些模型，而没有足够的可信度可能会带来重大风险，突出了及时发现这些问题的重要性。本文对开源LLMs在可信度上进行了敌对评估，跨足毒性、陈规俗套、伦理、幻觉、公平性、谄媚、隐私以及对抗性演示的八个不同方面。我们提出了advCoU，一种基于扩展的Chain of Utterances（CoU）提示策略，通过加入精心设计的恶意演示来攻击可信度。我们的广泛实验涵盖了最近和代表性系列的开源LLMs，包括Vicuna、MPT、Falcon、Mistral和Llama 2。

    arXiv:2311.09447v2 Announce Type: replace-cross  Abstract: The rapid progress in open-source Large Language Models (LLMs) is significantly driving AI development forward. However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly. In this work, we conduct an adversarial assessment of open-source LLMs on trustworthiness, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU) prompting strategy by incorporating carefully crafted malicious demonstrations for trustworthiness attack. Our extensive experiments encompass recent and representative series of open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The empirical
    
[^151]: LLMRefine：通过细粒度可操作反馈精确定位和优化大型语言模型

    LLMRefine: Pinpointing and Refining Large Language Models via Fine-Grained Actionable Feedback

    [https://arxiv.org/abs/2311.09336](https://arxiv.org/abs/2311.09336)

    LLMRefine提出了一种细粒度反馈模型来指导大型语言模型定位缺陷并进行优化，在机器翻译、长篇问答和主题总结等任务中取得显著的改进。

    

    最近，大型语言模型（LLM）正在利用人类反馈来提高生成质量。然而，在推断过程中获取人类反馈成本高昂。在这项工作中，我们提出了LLMRefine，一种用于优化推理时间的方法，以改进LLM的输出。其核心思想是利用学习的细粒度反馈模型来准确定位缺陷，并引导LLM进行迭代优化。通过将原始LLM作为编辑建议，LLMRefine通过模拟退火搜索无缺陷文本，权衡探索和开发。我们在三个文本生成任务上进行实验，包括机器翻译，长篇问答（QA）和主题总结。LLMRefine在所有基线方法上一贯表现优异，在翻译任务上取得了高达1.7 MetricX点的改进，在ASQA上为8.1 ROUGE-L，在主题总结上为2.2 ROUGE-L。

    arXiv:2311.09336v2 Announce Type: replace  Abstract: Recent large language models (LLM) are leveraging human feedback to improve their generation quality. However, human feedback is costly to obtain, especially during inference. In this work, we propose LLMRefine, an inference time optimization method to refine LLM's output. The core idea is to use a learned fine-grained feedback model to pinpoint defects and guide LLM to refine them iteratively. Using original LLM as a proposal of edits, LLMRefine searches for defect-less text via simulated annealing, trading off the exploration and exploitation. We conduct experiments on three text generation tasks, including machine translation, long-form question answering (QA), and topical summarization. LLMRefine consistently outperforms all baseline approaches, achieving improvements up to 1.7 MetricX points on translation tasks, 8.1 ROUGE-L on ASQA, 2.2 ROUGE-L on topical summarization.
    
[^152]: 在基于社区的社交媒体帖子中识别自我披露的使用、滥用和成瘾行为

    Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts

    [https://arxiv.org/abs/2311.09066](https://arxiv.org/abs/2311.09066)

    社区型社交媒体平台允许用户自我披露药物相关行为，在2500个阿片类药物帖子中，我们提出了标记六种不同阶段的数据集，通过片段级摘要解释在模型发展中的关键作用，并在监督、少样本或零样本设置下评估了几种模型。

    

    在过去的十年中，美国有50万多人死于涉及处方药和非法阿片类药物的过量使用，造成了国家公共卫生紧急情况。 医疗从业者需要强大且及时的工具，能够有效识别处于风险之中的患者。社区型社交媒体平台（如Reddit）允许用户自行披露，讨论一般情况下敏感的与药物相关的行为。我们提出了一个中等规模的数据集，包含来自不同子社区的2500个与阿片类药物相关的帖子，标记有六种不同的阿片类药物使用阶段：医疗使用、滥用、成瘾、康复、复发、不使用。对于每个帖子，我们注释了基于片段级的摘要解释，并在注释质量和模型开发中重点研究它们的作用。我们在监督、少样本或零样本设置下评估了几种最先进的模型。实验结果和错误分析显示了识别各个阶段的重要性。

    arXiv:2311.09066v2 Announce Type: replace  Abstract: In the last decade, the United States has lost more than 500,000 people from an overdose involving prescription and illicit opioids making it a national public health emergency (USDHHS, 2017). Medical practitioners require robust and timely tools that can effectively identify at-risk patients. Community-based social media platforms such as Reddit allow self-disclosure for users to discuss otherwise sensitive drug-related behaviors. We present a moderate size corpus of 2500 opioid-related posts from various subreddits labeled with six different phases of opioid use: Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. For every post, we annotate span-level extractive explanations and crucially study their role both in annotation quality and model development. We evaluate several state-of-the-art models in a supervised, few-shot, or zero-shot setting. Experimental results and error analysis show that identifying the phases of 
    
[^153]: LLM中的定位方法真的可以定位记忆数据吗？ 两个基准的故事

    Do Localization Methods Actually Localize Memorized Data in LLMs? A Tale of Two Benchmarks

    [https://arxiv.org/abs/2311.09060](https://arxiv.org/abs/2311.09060)

    该论文提出了两个基准来评估LLM中定位方法对记忆数据的定位能力，发现来自网络修剪的方法在两个基准上表现良好，所有评估方法均展现出有希望的定位能力。

    

    LLM中的本地化概念经常出现在先前的工作中；然而，本地化方法从未得到系统和直接的评估。我们提出了两个互补的基准，评估本地化方法定位LLM组件负责记忆数据的能力。在我们的INJ基准中，我们主动将新信息注入到LLM权重的一个小子集中，从而能够直接评估本地化方法是否能够识别这些“地面真相”权重。在我们的DEL基准中，通过测量删除已识别神经元会删除已经预训练序列的量，来评估本地化。尽管它们有不同的视角，我们的两个基准产生了五种本地化方法的一致排名。从网络修剪中改编的方法在两个基准上表现良好，所有评估方法均表现出有希望的定位能力。另一方面，即使是成功的方法

    arXiv:2311.09060v2 Announce Type: replace  Abstract: The concept of localization in LLMs is often mentioned in prior work; however, methods for localization have never been systematically and directly evaluated. We propose two complementary benchmarks that evaluate the ability of localization methods to pinpoint LLM components responsible for memorized data. In our INJ benchmark, we actively inject a piece of new information into a small subset of LLM weights, enabling us to directly evaluate whether localization methods can identify these "ground truth" weights. In our DEL benchmark, we evaluate localization by measuring how much dropping out identified neurons deletes a memorized pretrained sequence. Despite their different perspectives, our two benchmarks yield consistent rankings of five localization methods. Methods adapted from network pruning perform well on both benchmarks, and all evaluated methods show promising localization ability. On the other hand, even successful methods
    
[^154]: 半结构化思维链：整合多源知识以改进语言模型推理

    Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning

    [https://arxiv.org/abs/2311.08505](https://arxiv.org/abs/2311.08505)

    该论文提出了一种半结构化提示方法，通过整合语言模型的参数记忆、文本文档的非结构化知识和知识图的结构化知识，显著改善了开放域多跳问题回答任务的效果。

    

    大型语言模型在知识密集型任务中的一个重要开放问题是如何有效地整合来自三个来源的知识：模型的参数记忆、外部结构化知识和外部非结构化知识。本文引入一种新颖的半结构化提示方法，通过无缝整合模型的参数记忆、文本文档的非结构化知识和知识图中的结构化知识，克服这些限制。在开放域多跳问题回答数据集上的实验结果表明，我们的提示方法明显超越了现有技术，甚至超过了需要微调的技术。

    arXiv:2311.08505v2 Announce Type: replace  Abstract: An important open question in the use of large language models for knowledge-intensive tasks is how to effectively integrate knowledge from three sources: the model's parametric memory, external structured knowledge, and external unstructured knowledge. Most existing prompting methods either rely on one or two of these sources, or require repeatedly invoking large language models to generate similar or identical content. In this work, we overcome these limitations by introducing a novel semi-structured prompting approach that seamlessly integrates the model's parametric memory with unstructured knowledge from text documents and structured knowledge from knowledge graphs. Experimental results on open-domain multi-hop question answering datasets demonstrate that our prompting method significantly surpasses existing techniques, even exceeding those that require fine-tuning.
    
[^155]: IPA的味道：实现任何语言中的开放词汇关键词检测和强制对齐

    The taste of IPA: Towards open-vocabulary keyword spotting and forced alignment in any language

    [https://arxiv.org/abs/2311.08323](https://arxiv.org/abs/2311.08323)

    通过构建IPAPACK语料库和提出CLAP-IPA模型，实现了针对未知语言的强大跨语言泛化能力，并在95种未知语言上展示了开放词汇匹配和强制对齐的效果

    

    在这个项目中，我们展示了针对语音处理的基于音素的模型可以在未知语言上实现强大的跨语言泛化能力。我们精心策划了IPAPACK，这是一个包含来自不同语言家族的115多种语言的音素转录的大规模多语言语音语料库，由语言学家进行选择性检查。基于IPAPACK，我们提出了CLAP-IPA，这是一个多语言音素语音对比嵌入模型，能够在任意语音信号和音素序列之间进行开放词汇匹配。所提出的模型在95种未知语言上进行了测试，显示出跨语言具有强大的泛化能力。通过对比训练出现了音素和语音信号之间的时间对齐，使得在未知语言中实现了零-shot强制对齐。我们进一步引入了一个神经强制对齐器IPA-ALIGNER，通过用Forward-Sum损失微调CLAP-IPA来学习更好的音素到音频对齐

    arXiv:2311.08323v2 Announce Type: replace  Abstract: In this project, we demonstrate that phoneme-based models for speech processing can achieve strong crosslinguistic generalizability to unseen languages. We curated the IPAPACK, a massively multilingual speech corpora with phonemic transcriptions, encompassing more than 115 languages from diverse language families, selectively checked by linguists. Based on the IPAPACK, we propose CLAP-IPA, a multi-lingual phoneme-speech contrastive embedding model capable of open-vocabulary matching between arbitrary speech signals and phonemic sequences. The proposed model was tested on 95 unseen languages, showing strong generalizability across languages. Temporal alignments between phonemes and speech signals also emerged from contrastive training, enabling zeroshot forced alignment in unseen languages. We further introduced a neural forced aligner IPA-ALIGNER by finetuning CLAP-IPA with the Forward-Sum loss to learn better phone-to-audio alignmen
    
[^156]: 火山：通过自反馈引导修订减少多模式幻觉

    Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision

    [https://arxiv.org/abs/2311.07362](https://arxiv.org/abs/2311.07362)

    火山模型通过自反馈引导修订的方式，有效减少多模态幻觉问题，取得了在各项评测中的最新技术水平。

    

    大型多模态模型存在多模态幻觉问题，即提供与给定视觉信息不符的错误响应。最近的研究推测，多模态幻觉的原因之一是视觉编码器未能正确地与图像对齐。为了缓解这一问题，我们提出了一种利用自反馈作为视觉线索的新方法。基于这种方法，我们引入了火山，一个多模态自反馈引导修订模型。火山根据提供的视觉信息为其初始响应生成自然语言反馈，并利用此反馈来自我修订其初始响应。火山有效地减少了多模态幻觉，并在MMHal-Bench、POPE和GAVIE上实现了最新技术水平。它还提高了一般多模态能力，并在MM-Vet和MMBench上优于先前的模型。通过定性分析，我们展示了火山的...

    arXiv:2311.07362v3 Announce Type: replace  Abstract: Large multimodal models suffer from multimodal hallucination, where they provide incorrect responses misaligned with the given visual information. Recent works have conjectured that one of the reasons behind multimodal hallucination is due to the vision encoder failing to ground on the image properly. To mitigate this issue, we propose a novel approach that leverages self-feedback as visual cues. Building on this approach, we introduce Volcano, a multimodal self-feedback guided revision model. Volcano generates natural language feedback to its initial response based on the provided visual information and utilizes this feedback to self-revise its initial response. Volcano effectively reduces multimodal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general multimodal abilities and outperforms previous models on MM-Vet and MMBench. Through qualitative analysis, we show that Volcano's fe
    
[^157]: 火焰: 评估中国大型语言模型与人类价值观的契合性的基准测试

    Flames: Benchmarking Value Alignment of Chinese Large Language Models

    [https://arxiv.org/abs/2311.06899](https://arxiv.org/abs/2311.06899)

    中国的大型语言模型的价值观契合性需要更加全面的评估，该研究提出了一个名为"火焰"（Flames）的基准测试，涵盖了常见的无害原则和特定中国价值观，以及复杂场景和隐含恶意的提示方法。

    

    大型语言模型（LLMs）在各个地区的广泛应用强调了评估它们与人类价值观契合性的迫切性。然而，当前的基准测试未能有效地揭示LLMs中的安全漏洞。尽管许多模型在这些评估中得分很高，且“名列前茅”，但在LLMs与人类价值观的深层契合性和实现真正无害方面仍存在重大差距。为此，本文提出了一个名为"火焰"（Flames）的价值观契合性基准测试，该测试涵盖了常见的无害原则，以及一个整合了特定中国价值观如和谐的独特道德维度。因此，我们精心设计了包含复杂情境和大多带有隐含恶意的破解方法的对抗性提示。通过对17个主流LLMs进行提示，我们获得了模型的回应，并对其进行了详细评估。

    arXiv:2311.06899v2 Announce Type: replace-cross  Abstract: The widespread adoption of large language models (LLMs) across various regions underscores the urgent need to evaluate their alignment with human values. Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs. Despite numerous models achieving high scores and 'topping the chart' in these evaluations, there is still a significant gap in LLMs' deeper alignment with human values and achieving genuine harmlessness. To this end, this paper proposes a value alignment benchmark named Flames, which encompasses both common harmlessness principles and a unique morality dimension that integrates specific Chinese values such as harmony. Accordingly, we carefully design adversarial prompts that incorporate complex scenarios and jailbreaking methods, mostly with implicit malice. By prompting 17 mainstream LLMs, we obtain model responses and rigorously annotate them for detailed evaluation. Our findin
    
[^158]: Lewis's Signaling Game作为beta-VAE用于自然词长和部分

    Lewis's Signaling Game as beta-VAE For Natural Word Lengths and Segments

    [https://arxiv.org/abs/2311.04453](https://arxiv.org/abs/2311.04453)

    Lewis's Signaling Game作为beta-VAE，通过重新制定目标函数为ELBO，阐明了 emergent languages的先验分布对于其统计特性的影响，特别是词长和分段的属性。

    

    作为进化和计算语言学的一个子学科， emergent communication (EC) 研究通信协议，即 emergent languages，在代理通信的模拟中出现。 EC的一个关键目标是产生与自然语言共享统计特性的语言。 本文将 Lewis's signaling game，EC中经常使用的设置，重新解释为 beta-VAE，并将其目标函数重新制定为ELBO。 因此，我们阐明 emergent languages的先验分布的存在，并展示先验的选择可以影响其统计特性。 具体而言，我们讨论了词长和分段的属性，即缩写的 Zipf's law (ZLA) 和哈里斯的 articulation scheme (HAS)。 据报道，在使用常规目标时， emergent languages并不遪这些规律。 我们通过实验证明，通过选择...

    arXiv:2311.04453v2 Announce Type: replace  Abstract: As a sub-discipline of evolutionary and computational linguistics, emergent communication (EC) studies communication protocols, called emergent languages, arising in simulations where agents communicate. A key goal of EC is to give rise to languages that share statistical properties with natural languages. In this paper, we reinterpret Lewis's signaling game, a frequently used setting in EC, as beta-VAE and reformulate its objective function as ELBO. Consequently, we clarify the existence of prior distributions of emergent languages and show that the choice of the priors can influence their statistical properties. Specifically, we address the properties of word lengths and segmentation, known as Zipf's law of abbreviation (ZLA) and Harris's articulation scheme (HAS), respectively. It has been reported that the emergent languages do not follow them when using the conventional objective. We experimentally demonstrate that by selecting 
    
[^159]: 临床研究原则在自然语言处理模型泛化中的应用

    Principles from Clinical Research for NLP Model Generalization

    [https://arxiv.org/abs/2311.03663](https://arxiv.org/abs/2311.03663)

    临床研究中的泛化性原则指导下，对自然语言处理模型内部验证的重要性及影响泛化的因素进行研究分析

    

    NLP社区通常依赖模型在保留测试集上的表现来评估泛化能力。在官方测试集之外的数据集中观察到的性能下降通常归因于“非分布”效应。本文探讨了泛化性的基础并研究了影响它的因素，阐述了临床研究中的教训。在临床研究中，泛化性是一种依赖于实验的内部验证来确保对因果关系进行受控测量以及结果对更广泛人群的外部验证或可传输性的推理行为。我们演示了学习虚假相关性，比如在关系抽取任务中实体之间的距离，如何影响模型的内部验证，从而对泛化产生不利影响。因此，我们提出了在构建自然语言处理中的机器学习模型时需要确保内部验证的必要性。

    arXiv:2311.03663v3 Announce Type: replace  Abstract: The NLP community typically relies on performance of a model on a held-out test set to assess generalization. Performance drops observed in datasets outside of official test sets are generally attributed to "out-of-distribution" effects. Here, we explore the foundations of generalizability and study the factors that affect it, articulating lessons from clinical studies. In clinical research, generalizability is an act of reasoning that depends on (a) internal validity of experiments to ensure controlled measurement of cause and effect, and (b) external validity or transportability of the results to the wider population. We demonstrate how learning spurious correlations, such as the distance between entities in relation extraction tasks, can affect a model's internal validity and in turn adversely impact generalization. We, therefore, present the need to ensure internal validity when building machine learning models in NLP. Our recomm
    
[^160]: TopicGPT: 一个基于提示的主题建模框架

    TopicGPT: A Prompt-based Topic Modeling Framework

    [https://arxiv.org/abs/2311.01449](https://arxiv.org/abs/2311.01449)

    TopicGPT是一个基于提示的主题建模框架，使用大型语言模型来生成与人类分类更符合的并且可解释的主题，相比竞争方法在谐波纯度上有显著提升。

    

    主题建模是一种用于探索文本语料库的成熟技术。传统的主题模型（例如，LDA）将主题表示为词袋，通常需要“阅读茶叶”来解释；另外，它们对于用户控制生成主题的格式和特定性的程度很小。为了解决这些问题，我们引入了TopicGPT，这是一个基于提示的框架，利用大型语言模型（LLMs）来揭示文本集合中的潜在主题。与竞争方法相比，TopicGPT产生的主题更符合人类分类：与最强基线的0.64相比，它在与人工标记的维基百科主题比对中达到了0.74的谐波纯度。它的主题也是可解释的，取代了含糊的词袋，转而使用自然语言标签和相关的自由形式描述。此外，该框架非常适应，允许用户指定约束和控制主题的特定性。

    arXiv:2311.01449v2 Announce Type: replace  Abstract: Topic modeling is a well-established technique for exploring text corpora. Conventional topic models (e.g., LDA) represent topics as bags of words that often require "reading the tea leaves" to interpret; additionally, they offer users minimal control over the formatting and specificity of resulting topics. To tackle these issues, we introduce TopicGPT, a prompt-based framework that uses large language models (LLMs) to uncover latent topics in a text collection. TopicGPT produces topics that align better with human categorizations compared to competing methods: it achieves a harmonic mean purity of 0.74 against human-annotated Wikipedia topics compared to 0.64 for the strongest baseline. Its topics are also interpretable, dispensing with ambiguous bags of words in favor of topics with natural language labels and associated free-form descriptions. Moreover, the framework is highly adaptable, allowing users to specify constraints and m
    
[^161]: 随机目标嵌入对连续输出神经机器翻译的非理性有效性

    The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output Neural Machine Translation

    [https://arxiv.org/abs/2310.20620](https://arxiv.org/abs/2310.20620)

    随机目标嵌入在连续输出神经机器翻译中表现出非理性有效性，尤其在较大的数据集上并且对罕见词效果最为显著。

    

    连续输出神经机器翻译（CoNMT）将离散的下一个词预测问题替换为嵌入预测。目标嵌入空间的语义结构（即相关词之间的接近程度）在直觉上被认为至关重要。我们挑战了这一假设，并展示完全随机的输出嵌入可以胜过费力预训练的嵌入，特别是在较大的数据集上。进一步的研究显示，这一令人惊讶的效果对于罕见词最为显著，这是由于它们的嵌入的几何性质。我们通过设计一种混合策略，结合不同标记的随机和预训练嵌入，进一步阐明了这一发现。

    arXiv:2310.20620v2 Announce Type: replace  Abstract: Continuous-output neural machine translation (CoNMT) replaces the discrete next-word prediction problem with an embedding prediction. The semantic structure of the target embedding space (i.e., closeness of related words) is intuitively believed to be crucial. We challenge this assumption and show that completely random output embeddings can outperform laboriously pretrained ones, especially on larger datasets. Further investigation shows this surprising effect is strongest for rare words, due to the geometry of their embeddings. We shed further light on this finding by designing a mixed strategy that combines random and pre-trained embeddings for different tokens.
    
[^162]: 重新表述、增强、推理：视觉问题的视觉基础与语言模型

    Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models

    [https://arxiv.org/abs/2310.05861](https://arxiv.org/abs/2310.05861)

    通过在输入中添加具有视觉基础的信息作为预防性澄清，可以提高模型性能，减少不充分性，并简化模型回答问题的方式。

    

    越来越多的视觉-语言任务可以在零至少训练的情况下处理，即通过将大型语言模型（LLMs）与视觉编码器相结合，形成大型视觉-语言模型（LVLMs）。尽管这具有巨大的优势，比如不需要训练数据或自定义架构，但如何将输入呈现给LVLM会对零参考模型的性能产生重大影响。特别是，以不充分方式表达的输入可能会导致错误答案，原因包括缺失视觉信息、复杂的隐含推理或语言歧义。因此，通过在输入中添加具有视觉基础的信息作为预防性澄清，应该能够通过减少不充分性提高模型性能，例如通过定位对象和消除引用歧义。类似地，在VQA设置中，改变问题的构思方式可以使模型更容易回答。

    arXiv:2310.05861v2 Announce Type: replace-cross  Abstract: An increasing number of vision-language tasks can be handled with little to no training, i.e., in a zero and few-shot manner, by marrying large language models (LLMs) to vision encoders, resulting in large vision-language models (LVLMs). While this has huge upsides, such as not requiring training data or custom architectures, how an input is presented to an LVLM can have a major impact on zero-shot model performance. In particular, inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity. Therefore, adding visually-grounded information to the input as a preemptive clarification should improve model performance by reducing underspecification, e.g., by localizing objects and disambiguating references. Similarly, in the VQA setting, changing the way questions are framed can make them easier for models to answer. To th
    
[^163]: 让行动胜于雄辩：评估LLM代理在拍卖竞技场中的战略规划与执行

    Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena

    [https://arxiv.org/abs/2310.05746](https://arxiv.org/abs/2310.05746)

    LLM代理在拍卖竞技场展示出了关键的规划和执行技能，这为建模复杂社会互动在竞争背景下的LLMs潜力提供了新途径。

    

    最近大型语言模型（LLM）的发展展示了先进的推理能力，然而自然语言处理的评估通常依赖于静态基准。评估这一点需要测试战略推理能力的环境，这种环境需要在动态的竞争场景中进行长期规划。我们引入了AucArena，这是一个模拟拍卖的新颖评估套件，选择这个设置是因为它非常不可预测，涉及与资源和风险管理相关的许多技能，同时也易于评估。我们进行了使用最先进的LLM驱动竞标代理的受控实验，以评估他们的规划和执行技能。我们的研究表明，诸如GPT-4之类的LLM具有拍卖参与的关键技能，如预算管理和目标遵从，这些技能会随着自适应策略的改进而提高。这突出了LLM在建模竞技背景下的复杂社会互动潜力。

    arXiv:2310.05746v2 Announce Type: replace-cross  Abstract: Recent advancements in Large Language Models (LLMs) showcase advanced reasoning, yet NLP evaluations often depend on static benchmarks. Evaluating this necessitates environments that test strategic reasoning in dynamic, competitive scenarios requiring long-term planning. We introduce AucArena, a novel evaluation suite that simulates auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct controlled experiments using state-of-the-art LLMs to power bidding agents to benchmark their planning and execution skills. Our research demonstrates that LLMs, such as GPT-4, possess key skills for auction participation, such as budget management and goal adherence, which improve with adaptive strategies. This highlights LLMs' potential in modeling complex social interactions in competitive contexts. However, variability in LLM p
    
[^164]: 跨语言结构提取的情境标签投影

    Contextual Label Projection for Cross-Lingual Structure Extraction

    [https://arxiv.org/abs/2309.08943](https://arxiv.org/abs/2309.08943)

    本文提出了一种新的标签投影方法CLaP，利用上下文翻译标签，确保更准确的翻译，并在跨语言结构预测任务中取得显著表现。

    

    标签投影涉及同时获取翻译标签和文本，对于利用机器翻译促进结构预测任务中的跨语言转移至关重要。先前的研究探索标签投影时通常通过偏爱简化标签翻译或仅依赖于单词级别对齐来牺牲翻译准确性。本文介绍了一种新颖的标签投影方法CLaP，将文本翻译到目标语言，并利用已翻译文本作为上下文对标签进行情境翻译，确保翻译标签的更好准确性。我们利用具有多语言能力的指导调校语言模型作为我们的情境翻译器，通过指导在翻译文本中存在翻译标签的约束。我们在39种语言上通过零-shot跨语言转移对CLaP与其他标签投影技术进行基准测试。

    arXiv:2309.08943v2 Announce Type: replace  Abstract: Label projection, which involves obtaining translated labels and texts jointly, is essential for leveraging machine translation to facilitate cross-lingual transfer in structured prediction tasks. Prior research exploring label projection often compromise translation accuracy by favoring simplified label translation or relying solely on word-level alignments. In this paper, we introduce a novel label projection approach, CLaP, which translates text to the target language and performs contextual translation on the labels using the translated text as the context, ensuring better accuracy for the translated labels. We leverage instruction-tuned language models with multilingual capabilities as our contextual translator, imposing the constraint of the presence of translated labels in the translated text via instructions. We benchmark CLaP with other label projection techniques on zero-shot cross-lingual transfer across 39 languages on tw
    
[^165]: 价值万花筒：将人工智能与多元化人类价值观、权利和义务联系起来

    Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties

    [https://arxiv.org/abs/2309.00779](https://arxiv.org/abs/2309.00779)

    介绍了ValuePrism，一个包含218k个价值观、权利和义务，并与31k人类书写情境相联系的大规模数据集。

    

    人类价值观对于人类的决策至关重要。价值观多元主义认为，可以同时存在多种正确的价值观（例如，在考虑是否对朋友撒谎以保护他们的感受时，如何平衡诚实和友谊？）。作为统计学习者，人工智能系统默认适应平均值，忽略了这些潜在的不可简化的价值冲突。为了改进人工智能系统以更好地反映价值多元主义，首要挑战是探索人工智能系统可以模拟多元化人类价值观、权利和义务以及它们之间互动的程度。

    arXiv:2309.00779v2 Announce Type: replace-cross  Abstract: Human values are crucial to human decision-making. Value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). As statistical learners, AI systems fit to averages by default, washing out these potentially irreducible value conflicts. To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction.   We introduce ValuePrism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. ValuePrism's contextualized values are generated by GPT-4 and deemed high-quality by human annotators 91% of the time. We conduct a large-scale study with annotators across diverse social and demographic backgroun
    
[^166]: PANDA: Prompt Transfer Meets Knowledge Distillation for Efficient Model Adaptation

    PANDA: Prompt Transfer Meets Knowledge Distillation for Efficient Model Adaptation

    [https://arxiv.org/abs/2208.10160](https://arxiv.org/abs/2208.10160)

    PANDA是一种新颖的PoT方法，通过引入知识蒸馏技术有效缓解知识遗忘问题。

    

    提示转移（PoT）是一种最近提出的方法，用于改进提示微调，通过将目标提示初始化为在类似源任务上训练的现有提示。然而，这种普通的PoT方法通常会达到次优性能，因为（i）PoT对源-目标对的相似性敏感，并且（ii）在目标任务上直接微调使用源提示初始化的提示可能导致遗忘从源任务学到的有用的通用知识。为了解决这些问题，我们提出了一种新的度量标准来准确预测提示的可转移性（关于（i）），以及一种利用知识蒸馏技术有效缓解知识遗忘的新颖PoT方法（即PANDA）（关于（ii））。对于21个源和9个目标数据集的189种组合，在5个规模的PLM上进行了广泛而系统的实验，结果表明：1）我们提出的度量标准能够很好地预测p

    arXiv:2208.10160v2 Announce Type: replace  Abstract: Prompt Transfer (PoT) is a recently-proposed approach to improve prompt-tuning, by initializing the target prompt with the existing prompt trained on similar source tasks. However, such a vanilla PoT approach usually achieves sub-optimal performance, as (i) the PoT is sensitive to the similarity of source-target pair and (ii) directly fine-tuning the prompt initialized with source prompt on target task might lead to forgetting of the useful general knowledge learned from source task. To tackle these issues, we propose a new metric to accurately predict the prompt transferability (regarding (i)), and a novel PoT approach (namely PANDA) that leverages the knowledge distillation technique to alleviate the knowledge forgetting effectively (regarding (ii)). Extensive and systematic experiments on 189 combinations of 21 source and 9 target datasets across 5 scales of PLMs demonstrate that: 1) our proposed metric works well to predict the p
    
[^167]: 使用来自可比用户生成内容的词嵌入构建跨语言消费者健康词汇

    Constructing Cross-lingual Consumer Health Vocabulary with Word-Embedding from Comparable User Generated Content

    [https://arxiv.org/abs/2206.11612](https://arxiv.org/abs/2206.11612)

    通过使用来自可比用户生成内容的词嵌入，提出了一个跨语言自动术语识别框架，将英语消费者健康词汇扩展为跨语言词汇。

    

    在线健康社区（OHC）是普通人分享健康信息的主要渠道。为了分析OHC中消费者生成内容（HCGC），识别普通人使用的口头医学表达是一个关键挑战。开放获取和协作的消费者健康词汇（OAC CHV）是应对这一挑战的受控词汇。然而，OAC CHV仅在英语中可用，限制了其在其他语言中的适用性。本研究提出了一个跨语言自动术语识别框架，用于将英语CHV扩展为跨语言CHV。我们的框架需要一个英语HCGC语料库和一个非英语（本研究中为中文）HCGC语料库作为输入。使用skip-gram算法确定了两个单语词向量空间，使得每个空间编码了语言内普通人之间的常见词关联。根据等距假设，该研究提出了一种评估两种语言之间映射关系的方法。

    arXiv:2206.11612v2 Announce Type: replace  Abstract: The online health community (OHC) is the primary channel for laypeople to share health information. To analyze the health consumer-generated content (HCGC) from the OHCs, identifying the colloquial medical expressions used by laypeople is a critical challenge. The open-access and collaborative consumer health vocabulary (OAC CHV) is the controlled vocabulary for addressing such a challenge. Nevertheless, OAC CHV is only available in English, limiting its applicability to other languages. This research proposes a cross-lingual automatic term recognition framework for extending the English CHV into a cross-lingual one. Our framework requires an English HCGC corpus and a non-English (i.e., Chinese in this study) HCGC corpus as inputs. Two monolingual word vector spaces are determined using the skip-gram algorithm so that each space encodes common word associations from laypeople within a language. Based on the isometry assumption, the f
    
[^168]: MM-交错的：通过多模式特征同步器进行交错图像-文本生成建模

    MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer. (arXiv:2401.10208v1 [cs.CV])

    [http://arxiv.org/abs/2401.10208](http://arxiv.org/abs/2401.10208)

    本文提出了MM-交错，这是一个用于交错图像-文本数据的生成模型。它通过引入多尺度和多图像特征同步器模块，解决了现有模型在捕捉图像细节方面的限制，并通过端到端预训练和监督微调相结合的方式提高了其生成能力。

    

    对于交错图像-文本数据的生成模型的开发具有研究和实际价值。它要求模型理解交错的序列，并随后生成图像和文本。然而，现有的尝试受到了固定数量的视觉标记不能有效捕捉图像细节的问题的限制，在多图像场景中，这一问题尤为严重。为了解决这个问题，本文提出了MM-交错，这是一个用于交错图像-文本数据的端到端生成模型。它引入了一个多尺度和多图像特征同步器模块，允许在生成过程中直接访问先前上下文中的细粒度图像特征。MM-交错在配对和交错图像-文本语料库上进行端到端预训练。它还通过一阶段的监督微调来进一步改善其遵循复杂多模态指令的能力。实验结果表明了MM-交错在图像修复、图像生成和文本描述生成等任务中的多功能性。

    Developing generative models for interleaved image-text data has both research and practical value. It requires models to understand the interleaved sequences and subsequently generate images and text. However, existing attempts are limited by the issue that the fixed number of visual tokens cannot efficiently capture image details, which is particularly problematic in the multi-image scenarios. To address this, this paper presents MM-Interleaved, an end-to-end generative model for interleaved image-text data. It introduces a multi-scale and multi-image feature synchronizer module, allowing direct access to fine-grained image features in the previous context during the generation process. MM-Interleaved is end-to-end pre-trained on both paired and interleaved image-text corpora. It is further enhanced through a supervised fine-tuning phase, wherein the model improves its ability to follow complex multi-modal instructions. Experiments demonstrate the versatility of MM-Interleaved in rec
    
[^169]: CASA: 因果驱动的论证充分性评估

    CASA: Causality-driven Argument Sufficiency Assessment. (arXiv:2401.05249v1 [cs.CL])

    [http://arxiv.org/abs/2401.05249](http://arxiv.org/abs/2401.05249)

    CASA是一个因果驱动的论证充分性评估框架，利用大型语言模型生成与前提和结论不一致的上下文，并通过注入前提事件对其进行修改，能够准确识别不足的论证。

    

    论证充分性评估任务旨在确定一个给定论证的前提是否支持其结论。为了解决这个任务，现有的方法通常会对人工注释的数据进行分类器训练。然而，标注数据是费力的，而且由于主观标准的不一致性，标注往往也不一致。受因果文献中的充分概率（PS）定义的启发，我们提出了CASA，一个零射因果驱动的论证充分性评估框架。PS衡量的是当前提事件和结论事件都不存在时，引入前提事件是否会导致结论的可能性。为了估计这个概率，我们提出使用大型语言模型（LLMs）生成与前提和结论不一致的上下文，并通过注入前提事件对它们进行修改。在两个逻辑谬误检测数据集上的实验证明，CASA能够准确识别不足的论证。我们进一步将CASA部署在写作辅助系统中。

    The argument sufficiency assessment task aims to determine if the premises of a given argument support its conclusion. To tackle this task, existing works often train a classifier on data annotated by humans. However, annotating data is laborious, and annotations are often inconsistent due to subjective criteria. Motivated by the probability of sufficiency (PS) definition in the causal literature, we propose CASA, a zero-shot causality-driven argument sufficiency assessment framework. PS measures how likely introducing the premise event would lead to the conclusion, when both the premise and conclusion events are absent. To estimate this probability, we propose to use large language models (LLMs) to generate contexts that are inconsistent with the premise and conclusion, and revise them by injecting the premise event. Experiments on two logical fallacy detection datasets demonstrate that CASA accurately identifies insufficient arguments. We further deploy CASA in a writing assistance a
    
[^170]: 改变提示的蝴蝶效应：微小的变化和越狱对大规模语言模型性能的影响

    The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance. (arXiv:2401.03729v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.03729](http://arxiv.org/abs/2401.03729)

    本研究通过一系列提示变化探究改变提示的构建方式对大规模语言模型决策的影响，发现即使微小的改变，比如在提示末尾加一个空格，也可能导致模型的答案变化。同时，请求以XML格式返回和常用的越狱方式也可能对模型标记的数据产生灾难性影响。

    

    大规模语言模型（LLMs）被广泛用于对多个领域和多个任务的数据进行标注。通过简单地向LLM提问或“提示”，实践者能够快速获得任意任务的响应。提示的构建方式是否变化会影响LLM的最终决策？我们通过对多种文本分类任务进行一系列提示变化来回答这个问题。我们发现，即使是最微小的扰动，比如在提示的末尾加一个空格，也可能导致LLM改变其答案。此外，我们发现在XML中请求响应和常用的越狱方式可能对由LLMs标记的数据产生灾难性影响。

    Large Language Models (LLMs) are regularly being used to label data across many domains and for myriad tasks. By simply asking the LLM for an answer, or ``prompting,'' practitioners are able to use LLMs to quickly get a response for an arbitrary task. This prompting is done through a series of decisions by the practitioner, from simple wording of the prompt, to requesting the output in a certain data format, to jailbreaking in the case of prompts that address more sensitive topics. In this work, we ask: do variations in the way a prompt is constructed change the ultimate decision of the LLM? We answer this using a series of prompt variations across a variety of text classification tasks. We find that even the smallest of perturbations, such as adding a space at the end of a prompt, can cause the LLM to change its answer. Further, we find that requesting responses in XML and commonly used jailbreaks can have cataclysmic effects on the data labeled by LLMs.
    
[^171]: YAYI-UIE: 一个增强对话指导的通用信息抽取调优框架

    YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal Information Extraction. (arXiv:2312.15548v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.15548](http://arxiv.org/abs/2312.15548)

    本文提出了一个端到端的增强对话指导的通用信息抽取调优框架（YAYI-UIE），利用对话数据和信息抽取数据共同增强信息抽取性能，在中文数据集上达到了业界领先的性能，在英文数据集上也达到了可比较的性能。

    

    信息抽取任务的难点在于处理特定任务的标签模式和异构数据结构。最近的工作提出了基于大型语言模型的方法来统一建模不同的信息抽取任务。然而，这些现有方法在除了英语以外的中文语言的信息提取能力上存在不足。在本文中，我们提出了一个端到端的增强对话指导的通用信息抽取调优框架（YAYI-UIE），支持中文和英文。具体而言，我们利用对话数据和信息抽取数据共同增强信息抽取性能。实验结果表明，我们提出的框架在中文数据集上达到了业界领先的性能，同时在有监督和零样本设置下在英文数据集上也达到了可比较的性能。

    The difficulty of the information extraction task lies in dealing with the task-specific label schemas and heterogeneous data structures. Recent work has proposed methods based on large language models to uniformly model different information extraction tasks. However, these existing methods are deficient in their information extraction capabilities for Chinese languages other than English. In this paper, we propose an end-to-end chat-enhanced instruction tuning framework for universal information extraction (YAYI-UIE), which supports both Chinese and English. Specifically, we utilize dialogue data and information extraction data to enhance the information extraction performance jointly. Experimental results show that our proposed framework achieves state-of-the-art performance on Chinese datasets while also achieving comparable performance on English datasets under both supervised settings and zero-shot settings.
    
[^172]: AGI系统的元提示

    Meta Prompting for AGI Systems. (arXiv:2311.11482v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2311.11482](http://arxiv.org/abs/2311.11482)

    本文全面研究了元提示技术，这是一种创新方法，重塑了大型语言模型、多模态模型和人工智能系统在问题解决和数据解释方面的应用。通过强调信息的结构和句法，元提示将复杂问题拆解为简单的子问题，提高了效率，并且能够与少样本方法进行公平的比较。同时，本文还提出了元提示用于自动生成提示的方法。

    

    本文介绍了元提示(meta prompting)的全面研究，这是一种创新技术，重新塑造了大型语言模型(LLMs)、多模态基础模型和人工智能系统在问题解决和数据解释方面的利用。基于类型理论和范畴论，元提示注重信息的结构和句法，而不是传统以内容为中心的方法。本文探讨了元提示的形式定义，并将其与少样本提示(few-shot prompting)区分开来，并强调其在各种人工智能应用中的有效性。重点关注将元提示扩展到复杂推理任务上，展示如何将复杂问题拆分成较为简单的子问题，提高令牌效率，并使问题求解的比较更加公平，尤其是与少样本示例方法相比。此外，本文还引入了元提示用于提示任务，允许LLMs以迭代的元编程形式自动生成新的提示。

    This paper presents a comprehensive study of Meta Prompting, an innovative technique reshaping the utilization of large language models (LLMs), multi-modal foundation models, and AI systems in problem-solving and data interpretation. Grounded in type theory and category theory, Meta Prompting emphasizes the structure and syntax of information over traditional content-centric methods. The paper explores the formal definitions of Meta Prompting (MP), sets it apart from Few-Shot Prompting, and underlines its effectiveness in various AI applications. A key focus is on extending Meta Prompting to complex reasoning tasks, showing how it effectively deconstructs intricate problems into simpler sub-problems, enhancing token efficiency and enabling more equitable problem-solving comparisons, especially against few-shot example methods. Additionally, the paper introduces Meta Prompting for Prompting Tasks, allowing LLMs to self-generate new prompts in an iterative, metaprogramming-like manner. T
    
[^173]: Kiki还是Bouba？视觉与语言模型中的声音象征性

    Kiki or Bouba? Sound Symbolism in Vision-and-Language Models. (arXiv:2310.16781v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.16781](http://arxiv.org/abs/2310.16781)

    这项研究通过调查视觉与语言模型中的内在知识，发现它们显示了声音象征性的模式，进一步证实声音和意义之间的相关性在跨模态关联中得到了体现。

    

    尽管人类语言中的声音和意义之间的映射被认为在很大程度上是随机的，但认知科学的研究表明，在语言和人口群体之间的特定声音和意义之间存在非平凡的相关性，这种现象被称为声音象征性。在许多意义维度中，声音象征性在语言和视觉领域之间的跨模态关联方面尤为显著和充分证明。在这项工作中，我们探讨了声音象征性是否在CLIP和Stable Diffusion等视觉与语言模型中得到体现。通过使用零样本知识探测来调查这些模型的内在知识，我们发现强有力的证据表明它们确实显示了这种模式，与心理语言学中众所周知的kiki-bouba效应相一致。我们的工作提供了一种使用计算工具来展示声音象征性并理解其本质的新方法。我们的代码将公开提供。

    Although the mapping between sound and meaning in human language is assumed to be largely arbitrary, research in cognitive science has shown that there are non-trivial correlations between particular sounds and meanings across languages and demographic groups, a phenomenon known as sound symbolism. Among the many dimensions of meaning, sound symbolism is particularly salient and well-demonstrated with regards to cross-modal associations between language and the visual domain. In this work, we address the question of whether sound symbolism is reflected in vision-and-language models such as CLIP and Stable Diffusion. Using zero-shot knowledge probing to investigate the inherent knowledge of these models, we find strong evidence that they do show this pattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our work provides a novel method for demonstrating sound symbolism and understanding its nature using computational tools. Our code will be made publicly available.
    
[^174]: 大语言模型帮助人类验证真实性——除非它们令人信服地错误。

    Large Language Models Help Humans Verify Truthfulness -- Except When They Are Convincingly Wrong. (arXiv:2310.12558v1 [cs.CL])

    [http://arxiv.org/abs/2310.12558](http://arxiv.org/abs/2310.12558)

    本研究比较了语言模型与搜索引擎在帮助用户事实核查方面的效果。结果显示，用户阅读语言模型的解释比使用搜索引擎更高效，但当解释错误时，用户容易过度依赖语言模型。为了减少过度依赖，研究提出了使用对比信息进行解释的方法。

    

    大语言模型（LLMs）越来越多地被用于获取网络上的信息。因此，它们的真实性和事实性备受关注。为了帮助用户做出正确的信息决策，LLMs不仅应提供信息，还应帮助用户事实核查。本文通过与80名众包工作者进行实验，比较了语言模型与搜索引擎（信息检索系统）在帮助人类用户事实核查方面的效果。我们引导LLMs验证给定的声明并提供相应的解释。与使用准确率相似的搜索引擎相比，阅读LLM的解释的用户效率显著提高。然而，当解释错误时，他们往往过度依赖LLMs。为了减少对LLMs的过度依赖，我们要求LLMs提供对比信息，解释为什么声明为真和为假，并将两方面的解释呈现给用户。这种对比解释减轻了用户的过度依赖。

    Large Language Models (LLMs) are increasingly used for accessing information on the web. Their truthfulness and factuality are thus of great interest. To help users make the right decisions about the information they're getting, LLMs should not only provide but also help users fact-check information. In this paper, we conduct experiments with 80 crowdworkers in total to compare language models with search engines (information retrieval systems) at facilitating fact-checking by human users. We prompt LLMs to validate a given claim and provide corresponding explanations. Users reading LLM explanations are significantly more efficient than using search engines with similar accuracy. However, they tend to over-rely the LLMs when the explanation is wrong. To reduce over-reliance on LLMs, we ask LLMs to provide contrastive information explain both why the claim is true and false, and then we present both sides of the explanation to users. This contrastive explanation mitigates users' over-
    
[^175]: 量化中文医学大型语言模型中与健康相关的原子知识：一项计算分析

    Quantify Health-Related Atomic Knowledge in Chinese Medical Large Language Models: A Computational Analysis. (arXiv:2310.11722v1 [cs.CL])

    [http://arxiv.org/abs/2310.11722](http://arxiv.org/abs/2310.11722)

    本研究通过构建一个基准，量化了中文医学大型语言模型中与健康相关的原子知识的存储程度，并发现通用LLMs在原子知识和指令遵循能力方面表现更好。两种类型的LLMs都倾向于迎合用户要求。

    

    大型语言模型（LLMs）有潜力通过搜索引擎直接和高效地提供用户的自诊断建议，从而革新用户自诊断的方式。最近的研究主要关注基于GPT-4评估LLMs的质量或其通过医学考试的能力，但没有研究量化存储在LLMs记忆中的健康相关原子知识的程度，而这是LLMs提供更准确建议的基础。在本文中，我们首先构建了一个基准，包括用户自诊断查询中最常见的原子知识类型，共17种原子类型和14048条原子知识。然后，我们对通用和专业LLMs在基准上进行了评估。实验结果表明，在原子知识和指令遵循能力方面，通用LLMs的表现优于专业LLMs。错误分析显示，通用和专业LLMs都是马屁精，即在涉及用户要求时总是迎合用户。

    Large Language Models (LLMs) have the potential to revolutionize the way users self-diagnose through search engines by offering direct and efficient suggestions. Recent studies primarily focused on the quality of LLMs evaluated by GPT-4 or their ability to pass medical exams, no studies have quantified the extent of health-related atomic knowledge stored in LLMs' memory, which is the basis of LLMs to provide more factual suggestions. In this paper, we first constructed a benchmark, including the most common types of atomic knowledge in user self-diagnosis queries, with 17 atomic types and a total of 14, 048 pieces of atomic knowledge. Then, we evaluated both generic and specialized LLMs on the benchmark. The experimental results showcased that generic LLMs perform better than specialized LLMs in terms of atomic knowledge and instruction-following ability. Error analysis revealed that both generic and specialized LLMs are sycophantic, e.g., always catering to users' claims when it comes
    
[^176]: Hexa: 知识驱动的对话系统的自我提升

    Hexa: Self-Improving for Knowledge-Grounded Dialogue System. (arXiv:2310.06404v1 [cs.CL])

    [http://arxiv.org/abs/2310.06404](http://arxiv.org/abs/2310.06404)

    本论文提出了一种自我提升的方法，用于改进知识驱动对话生成的中间步骤的生成性能。通过引入引导提示和修改损失函数的自举策略，提高了生成自动生成回答的多样性，并在各种基准数据集上实验证明了该方法的有效性。

    

    知识驱动的对话生成中一种常见的做法是使用模块化的方法明确地利用中间步骤（如网络搜索、记忆检索）。然而，与对话响应相比，这些步骤的数据往往难以获取，因为在普通对话中无法观察到它们。为了填补这些数据的缺失，我们开发了一种自我提升方法，以改进中间步骤的生成性能，而不需要地面真实数据。具体而言，我们提出了一种新颖的引导提示和修改的损失函数的引导自动生成回答多样性的自举方法。通过在各种基准数据集上进行实验，我们经验证明我们的方法成功地利用了自我提升机制，在生成中间和最终回答方面改善了知识驱动对话生成任务的性能。

    A common practice in knowledge-grounded dialogue generation is to explicitly utilize intermediate steps (e.g., web-search, memory retrieval) with modular approaches. However, data for such steps are often inaccessible compared to those of dialogue responses as they are unobservable in an ordinary dialogue. To fill in the absence of these data, we develop a self-improving method to improve the generative performances of intermediate steps without the ground truth data. In particular, we propose a novel bootstrapping scheme with a guided prompt and a modified loss function to enhance the diversity of appropriate self-generated responses. Through experiments on various benchmark datasets, we empirically demonstrate that our method successfully leverages a self-improving mechanism in generating intermediate and final responses and improves the performances on the task of knowledge-grounded dialogue generation.
    
[^177]: TEMPO: 基于提示的生成式预训练变换器模型用于时间序列预测

    TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. (arXiv:2310.04948v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04948](http://arxiv.org/abs/2310.04948)

    本文提出了一个新的框架 TEMPO，通过利用时间序列任务的两个重要归纳偏差，即将复杂交互分解和引入基于选择的提示来有效学习时间序列表示。

    

    在过去的十年中，深度学习在时间序列建模方面取得了显著进展。尽管在取得最先进的结果的同时，最好的架构在不同应用和领域之间差异很大。与此同时，在自然语言处理方面，生成式预训练变换器(GPT)通过训练一个通用模型在各种文本数据集上展现出了令人印象深刻的性能。有趣的是，探索是否GPT类型的架构可以对时间序列产生有效的影响，捕捉其内在动态属性并显著提高准确性。在本文中，我们提出了一个新颖的框架TEMPO，可以有效地学习时间序列表示。我们专注于利用时间序列任务的两种重要归纳偏差来预训练模型：(i) 对趋势、季节和残差成分复杂交互的分解；和(ii) 提出基于选择的提示以便于非分布自适应。

    The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the selection-based prompts to facilitate distribution adaptation in non-
    
[^178]: LatticeGen: 一种在云上进行隐私感知生成的协作框架，隐藏生成的文本在格子中

    LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud. (arXiv:2309.17157v1 [cs.CL])

    [http://arxiv.org/abs/2309.17157](http://arxiv.org/abs/2309.17157)

    LatticeGen是一个协作框架，通过将真实生成的文本与噪声混合并隐藏在格子中，以保护用户的隐私。实验证明，LatticeGen能够在面对强攻击时成功保护真实生成，超过50%的语义仍然隐藏。

    

    在当前的用户-服务器交互模式中，使用大型语言模型（LLM）进行提示生成的过程中，服务器完全控制着生成过程，这使得想要将生成的文本保留给自己的用户没有任何选择。我们提出了LatticeGen，一个协作框架，在该框架中，服务器仍然处理大部分计算任务，而用户控制采样操作。其核心思想是用户将真实生成序列与噪声标记混合，并隐藏在一个带噪声的格子中。考虑到来自假设恶意服务器的潜在攻击以及用户如何进行防御，我们提出了重复波束搜索攻击和混合噪声方案。在实验中，我们将LatticeGen应用于保护提示和生成。结果显示，虽然带噪声的格子会降低生成质量，但LatticeGen成功地在强攻击下显著保护了真实生成（超过50%的语义仍然隐藏）。

    In the current user-server interaction paradigm of prompted generation with large language models (LLM) on cloud, the server fully controls the generation process, which leaves zero options for users who want to keep the generated text to themselves. We propose LatticeGen, a cooperative framework in which the server still handles most of the computation while the user controls the sampling operation. The key idea is that the true generated sequence is mixed with noise tokens by the user and hidden in a noised lattice. Considering potential attacks from a hypothetically malicious server and how the user can defend against it, we propose the repeated beam-search attack and the mixing noise scheme. In our experiments we apply LatticeGen to protect both prompt and generation. It is shown that while the noised lattice degrades generation quality, LatticeGen successfully protects the true generation to a remarkable degree under strong attacks (more than 50% of the semantic remains hidden as 
    
[^179]: 让PPO变得更好：基于值导向的Monte-Carlo Tree Search解码

    Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding. (arXiv:2309.15028v1 [cs.CL])

    [http://arxiv.org/abs/2309.15028](http://arxiv.org/abs/2309.15028)

    本文提出了一种基于值导向的Monte-Carlo Tree Search解码算法PPO-MCTS，通过在PPO之上集成MCTS，解决了训练和测试之间部分输出评分机制的不匹配问题，实验证明该算法可以显著提升性能。

    

    在生成自然语言文本时，使用最新的强化学习算法，如Proximal Policy Optimization (PPO)，因此可以认为推理时间的搜索算法，如Monte-Carlo Tree Search (MCTS) 是不必要的。本文证明了通过在PPO之上集成MCTS，可以进一步提升PPO的性能。关键思想是在解码文本时，不要丢弃值网络，即PPO训练时用于评估部分输出序列的副产品，而是将其与策略网络紧密结合。具体而言，本文提出了一种称为PPO-MCTS的新颖的值导向解码算法，可以将来自PPO的值网络与推理时间产生的策略网络紧密结合。与基于MCTS的控制文本生成的先前方法相比，我们的方法的关键优势在于减少了训练和测试之间部分输出的评分机制的基本不匹配。在四个文本生成任务上的评估结果表明，PPO-MCTS可以显著提升性能。

    Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO). In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top. The key idea is not to throw out the value network, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the policy network. More concretely, we present a novel value-guided decoding algorithm called PPO-MCTS, which can integrate the value network from PPO to work closely with the policy network during inference-time generation. Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test. Evaluation on four text generation tasks demonstrate that PPO-MCTS grea
    
[^180]: AceGPT：将大型语言模型本地化为阿拉伯文

    AceGPT, Localizing Large Language Models in Arabic. (arXiv:2309.12053v1 [cs.CL])

    [http://arxiv.org/abs/2309.12053](http://arxiv.org/abs/2309.12053)

    本研究旨在开发阿拉伯文的本地化大型语言模型(AceGPT)，通过预训练、监督微调和增强学习方法来培养具备文化意识和价值观一致的阿拉伯文模型，以满足阿拉伯语社区特定应用需求。评估结果表明，AceGPT在各项基准测试中都是最先进的阿拉伯文模型。

    

    本文探讨了开发适用于阿拉伯文的本地化大型语言模型(LLM)的迫切需求和方法论，阿拉伯文具有独特的文化特征，这些特征目前的主流模型如ChatGPT并未充分解决。在考虑文化敏感性和本地价值观时还存在关键问题。为此，本文提出了一个打包解决方案，包括进一步使用阿拉伯文本进行预训练、使用本地阿拉伯指令和阿拉伯语GPT-4回应进行监督微调(SFT)，以及使用对本地文化和价值观敏感的奖励模型进行增强学习与人工智能反馈(RLAIF)。目标是训练具备文化意识和与价值观一致的阿拉伯文LLM，以满足阿拉伯语社区多样化的特定应用需求。广泛的评估表明，所得到的名为AceGPT的阿拉伯文LLM在各种基准测试中均是最先进的。

    This paper explores the imperative need and methodology for developing a localized Large Language Model (LLM) tailored for Arabic, a language with unique cultural characteristics that are not adequately addressed by current mainstream models like ChatGPT. Key concerns additionally arise when considering cultural sensitivity and local values. To this end, the paper outlines a packaged solution, including further pre-training with Arabic texts, supervised fine-tuning (SFT) using native Arabic instructions and GPT-4 responses in Arabic, and reinforcement learning with AI feedback (RLAIF) using a reward model that is sensitive to local culture and values. The objective is to train culturally aware and value-aligned Arabic LLMs that can serve the diverse application-specific needs of Arabic-speaking communities.  Extensive evaluations demonstrated that the resulting LLM called `\textbf{AceGPT}' is the SOTA open Arabic LLM in various benchmarks, including instruction-following benchmark (i.e
    
[^181]: SLIDE: 使用滑动文档窗口进行无参考机器翻译评估

    SLIDE: Reference-free Evaluation for Machine Translation using a Sliding Document Window. (arXiv:2309.08832v1 [cs.CL])

    [http://arxiv.org/abs/2309.08832](http://arxiv.org/abs/2309.08832)

    本论文提出了一种名为SLIDE的度量方法，通过使用滑动文档窗口来评估机器翻译质量，该方法在某些情况下甚至能消除与参考度量之间的差距，表明源语言上下文可能提供了与人类参考相同的信息。

    

    基于参考的度量通常在句子级别上优于仅能访问源语言和系统输出的质量估计度量。这并不奇怪，因为参考能够消除源语言中可能存在的歧义。我们研究了是否可以用额外的源语言上下文有效地替代参考。我们提出了一种度量方法，SLIDE（SLiding Document Evaluator），它通过一个滑动窗口在每个测试集中的文档上操作，将每个块输入到未修改的现成质量估计模型中。我们发现，SLIDE在系统准确性的成对比较上较句子级别基线显著提高，有些情况下甚至消除了与参考度量之间的差距。这表明源语言上下文可能提供了与人类参考相同的信息。

    Reference-based metrics that operate at the sentence level typically outperform quality estimation metrics, which have access only to the source and system output. This is unsurprising, since references resolve ambiguities that may be present in the source. We investigate whether additional source context can effectively substitute for a reference. We present a metric, SLIDE (SLiding Document Evaluator), which operates on blocks of sentences using a window that slides over each document in the test set, feeding each chunk into an unmodified, off-the-shelf quality estimation model. We find that SLIDE obtains significantly higher pairwise system accuracy than its sentence-level baseline, in some cases even eliminating the gap with reference-base metrics. This suggests that source context may provide the same information as a human reference.
    
[^182]: MusiLingo：利用预训练的语言模型将音乐和文本相结合，实现音乐字幕和查询响应

    MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response. (arXiv:2309.08730v1 [eess.AS])

    [http://arxiv.org/abs/2309.08730](http://arxiv.org/abs/2309.08730)

    MusiLingo是一个利用预训练的语言模型将音乐和文本相结合的系统，可以生成音乐字幕和回答音乐相关的查询。通过使用投影层对齐音乐表示，该系统成功地将音乐音频和文本环境联系起来，同时使用了一个新的数据集来推动领域的进展。

    

    大型语言模型（LLM）已经在多模态应用中展现出巨大潜力，然而文本和音乐领域的融合仍相对未被探索。为了解决这一问题，我们提出了MusiLingo，这是一个用于音乐字幕生成和音乐相关查询响应的新系统。MusiLingo使用一个投影层来对齐预训练的冻结音乐音频模型MERT和冻结的LLaMA语言模型的音乐表示，实现音乐音频和文本环境之间的桥梁。我们在一个大规模的音乐字幕数据集上进行训练，并使用指导性数据进行微调。由于高质量的音乐问答数据集稀缺，我们从MusicCaps创建了MusicInstruct（MI）数据集，专为开放式音乐查询而设计。实证评估证明了它在生成音乐字幕和组织音乐相关问答对方面的竞争性表现。我们引入的数据集在之前的数据集的基础上取得了显著进展。

    Large Language Models (LLMs) have shown immense potential in multimodal applications, yet the convergence of textual and musical domains remains relatively unexplored. To address this gap, we present MusiLingo, a novel system for music caption generation and music-related query responses. MusiLingo employs a single projection layer to align music representations from the pre-trained frozen music audio model MERT with the frozen LLaMA language model, bridging the gap between music audio and textual contexts. We train it on an extensive music caption dataset and fine-tune it with instructional data. Due to the scarcity of high-quality music Q&A datasets, we created the MusicInstruct (MI) dataset from MusicCaps, tailored for open-ended music inquiries. Empirical evaluations demonstrate its competitive performance in generating music captions and composing music-related Q&A pairs. Our introduced dataset enables notable advancements beyond previous ones.
    
[^183]: ExpertQA: 专家策划的问题和带有属性的答案

    ExpertQA: Expert-Curated Questions and Attributed Answers. (arXiv:2309.07852v1 [cs.CL])

    [http://arxiv.org/abs/2309.07852](http://arxiv.org/abs/2309.07852)

    本论文介绍了ExpertQA，它是一个专家策划的问题和带有属性的答案系统。该系统通过分析语言模型在领域特定情景中提供的事实准确性和归因等方面来确保提供准确的信息。研究还收集了领域专家的问题并要求他们评估生成的答案。这项工作的目的是确保语言模型在高风险领域中不会传播错误信息，从而避免不良的社会后果。

    

    随着语言模型被越来越复杂和多样化的用户所采用，确保它们提供基于可验证来源的事实准确信息的重要性在各个领域的研究和职业中都是至关重要的。这特别适用于医学和法律等高风险领域，因为传播错误信息的风险较高，可能导致不良的社会后果。先前的研究关注于事实性和归因方面，并未专注于分析语言模型在特定领域情景中的这些特征。在这项工作中，我们通过将领域专家纳入其中，提出了一个评估研究，分析来自几个系统的响应中提供的事实准确性和归因的各个方面。具体而言，我们先从32个学科领域的484名参与者中收集由专家策划的问题，然后要求这些专家评估对他们自己问题的产生的响应。我们还要求专家修改产生的答案。

    As language models are adapted by a more sophisticated and diverse set of users, the importance of guaranteeing that they provide factually correct information supported by verifiable sources is critical across fields of study & professions. This is especially the case for high-stakes fields, such as medicine and law, where the risk of propagating false information is high and can lead to undesirable societal consequences. Previous work studying factuality and attribution has not focused on analyzing these characteristics of language model outputs in domain-specific scenarios. In this work, we present an evaluation study analyzing various axes of factuality and attribution provided in responses from a few systems, by bringing domain experts in the loop. Specifically, we first collect expert-curated questions from 484 participants across 32 fields of study, and then ask the same experts to evaluate generated responses to their own questions. We also ask experts to revise answers produce
    
[^184]: 大型语言模型在持续微调过程中的灾难性遗忘的实证研究

    An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning. (arXiv:2308.08747v1 [cs.CL])

    [http://arxiv.org/abs/2308.08747](http://arxiv.org/abs/2308.08747)

    该研究实证评估了大型语言模型在持续微调过程中的灾难性遗忘现象，并发现随着模型规模增加，遗忘的严重程度也加剧。与编码器-解码器模型相比，仅有解码器的模型遗忘较少并保留更多知识。此外，研究还发现LLMs可以减轻语言偏见，并且ALPACA在保留知识和容量方面具有优势。

    

    灾难性遗忘（CF）是机器学习中的一种现象，当模型学习新信息时，它会忘记先前学到的信息。由于大型语言模型（LLMs）显示出了出色的性能，探究LLMs在持续微调中是否存在CF是很有意义的。在这项研究中，我们从领域知识、推理和阅读理解的角度对LLMs的遗忘现象进行了实证评估。实验表明，从1b到7b的范围内，LLMs普遍存在灾难性遗忘现象，并且随着规模的增加，遗忘的严重程度也加剧。与编码器-解码器模型mT0相比，仅有解码器的模型BLOOMZ遗忘较少并保留更多知识。我们还观察到，在持续微调过程中，LLMs可以减轻语言偏见（如性别偏见）。此外，我们发现与LLAMA相比，ALPACA在保留更多知识和容量方面具有优势。

    Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning when a model forgets previously learned information as it learns new information. As large language models (LLMs) have shown excellent performance, it is interesting to uncover whether CF exists in the continual fine-tuning of LLMs. In this study, we empirically evaluate the forgetting phenomenon in LLMs' knowledge, from the perspectives of domain knowledge, reasoning, and reading comprehension. The experiments demonstrate that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b. Furthermore, as the scale increases, the severity of forgetting also intensifies. Comparing the decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ suffers less forgetting and maintains more knowledge. We also observe that LLMs can mitigate language bias (e.g. gender bias) during continual fine-tuning. Moreover, we find that ALPACA can maintain more knowledge and capacity compared with LLAMA du
    
[^185]: LLM-Rec: 通过引导大型语言模型进行个性化推荐

    LLM-Rec: Personalized Recommendation via Prompting Large Language Models. (arXiv:2307.15780v1 [cs.CL])

    [http://arxiv.org/abs/2307.15780](http://arxiv.org/abs/2307.15780)

    本文通过引导大型语言模型进行个性化推荐的研究，提出了四种不同的引导策略，并通过实验证明了这些策略的有效性。这一发现强调了在个性化内容推荐中，采用多样的引导和输入增强技术可以提高大型语言模型的推荐性能。

    

    本文通过输入增强技术，研究了多种不同的引导策略，以提高大型语言模型（LLM）在个性化内容推荐方面的性能。我们提出的方法名为LLM-Rec，包括四种不同的引导策略：（1）基础引导，（2）推荐驱动引导，（3）参与引导引导，和（4）推荐驱动+参与引导引导。实验证明，将原始内容描述与LLM生成的增强输入文本结合起来，采用这些引导策略可以提高推荐性能。这一发现强调了在个性化内容推荐中，通过引入多样的引导和输入增强技术来提升大型语言模型的推荐能力的重要性。

    We investigate various prompting strategies for enhancing personalized content recommendation performance with large language models (LLMs) through input augmentation. Our proposed approach, termed LLM-Rec, encompasses four distinct prompting strategies: (1) basic prompting, (2) recommendation-driven prompting, (3) engagement-guided prompting, and (4) recommendation-driven + engagement-guided prompting. Our empirical experiments show that combining the original content description with the augmented input text generated by LLM using these prompting strategies leads to improved recommendation performance. This finding highlights the importance of incorporating diverse prompts and input augmentation techniques to enhance the recommendation capabilities with large language models for personalized content recommendation.
    
[^186]: 通过口述方式操作进行指示遵循评估

    Instruction-following Evaluation through Verbalizer Manipulation. (arXiv:2307.10558v1 [cs.CL])

    [http://arxiv.org/abs/2307.10558](http://arxiv.org/abs/2307.10558)

    本文提出了一种新的指示遵循评估协议，口述者操作，通过口述任务标签来检查模型对先验知识的依赖程度，以及覆盖它们的能力，从而准确地进行指示遵循。

    

    虽然调整指令模型在各种自然语言处理任务中取得了显著的成功，但准确评估其遵循指令的能力仍然具有挑战性。现有的基准主要关注与模型在训练过程中学习的内容相吻合的常见指令。然而，对这些指令的回应能力并不一定意味着强大的遵循指令能力。在本文中，我们提出了一种新颖的指示遵循评估协议，称为口述者操作。它要求模型用与模型先验知识不同程度吻合的单词口述任务标签，从高度吻合（例如，对于积极情绪输出“积极”）到最少吻合（例如，对于积极情绪输出“消极”）。口述者操作可以与任何分类基准无缝集成，以检查模型对先验知识的依赖程度以及覆盖它们的能力，从而准确地进行指示遵循。

    While instruction-tuned models have shown remarkable success in various natural language processing tasks, accurately evaluating their ability to follow instructions remains challenging. Existing benchmarks primarily focus on common instructions that align well with what the model learned during training. However, proficiency in responding to these instructions does not necessarily imply strong ability in instruction following. In this paper, we propose a novel instruction-following evaluation protocol called verbalizer manipulation. It instructs the model to verbalize the task label with words aligning with model priors to different extents, adopting verbalizers from highly aligned (e.g., outputting ``postive'' for positive sentiment), to minimally aligned (e.g., outputting ``negative'' for positive sentiment). Verbalizer manipulation can be seamlessly integrated with any classification benchmark to examine the model's reliance on priors and its ability to override them to accurately 
    
[^187]: 视觉词汇描述提升零样本图像分类

    Visually-Grounded Descriptions Improve Zero-Shot Image Classification. (arXiv:2306.06077v1 [cs.CV])

    [http://arxiv.org/abs/2306.06077](http://arxiv.org/abs/2306.06077)

    本文提出了一种称为V-GLOSS的新方法，它利用现代语言模型和语义知识库生成具有视觉基础的类别描述，提高了零样本图像分类的准确性，并引入了一个带有类别描述的银标准数据集。

    

    语言视觉模型如CLIP在零样本视觉任务（例如零样本图像分类ZSIC）方面取得了显著进展。然而，生成具体和富有表现力的类别描述仍然是一个主要挑战。现有方法存在粒度和标签歧义等问题。为了解决这些挑战，我们提出了一种新方法V-GLOSS：Visual Glosses，它利用现代语言模型和语义知识库来生成具有视觉基础的类别描述。我们通过在基准ZSIC数据集（包括ImageNet和STL-10）上实现最先进的结果来展示V-GLOSS的有效性。此外，我们引入了一个由V-GLOSS生成的带有类别描述的银标准数据集，并展示其用于视觉任务的有用性。我们提供了源代码和数据集。

    Language-vision models like CLIP have made significant progress in zero-shot vision tasks, such as zero-shot image classification (ZSIC). However, generating specific and expressive class descriptions remains a major challenge. Existing approaches suffer from granularity and label ambiguity issues. To tackle these challenges, we propose V-GLOSS: Visual Glosses, a novel method leveraging modern language models and semantic knowledge bases to produce visually-grounded class descriptions. We demonstrate V-GLOSS's effectiveness by achieving state-of-the-art results on benchmark ZSIC datasets including ImageNet and STL-10. In addition, we introduce a silver dataset with class descriptions generated by V-GLOSS, and show its usefulness for vision tasks. We make available our code and dataset.
    
[^188]: 这片土地是你我的土地：评估语言模型中的地缘政治偏见

    This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models. (arXiv:2305.14610v1 [cs.CL])

    [http://arxiv.org/abs/2305.14610](http://arxiv.org/abs/2305.14610)

    本文提出了地缘政治偏见的概念，并以领土争端为例，利用多语言、多选题的数据集BorderLines和几个定量指标分析语言模型响应中的地缘政治偏见现象。

    

    我们引入了地缘政治偏见的概念——即根据语言环境报道不同的地缘政治知识的倾向。我们以领土争端为案例进行了研究。例如，对于被广泛争议的南沙群岛，如果用中文问，LM是否更有可能说它们属于中国，而如果用塔加洛语问，则更有可能说它们属于菲律宾？为了评估是否存在这种偏见，我们首先从维基百科上收集了一组领土争端数据，然后将每个领土与一组多语言、多选题联系起来。这个数据集被称为BorderLines，它包括250个领土和45种语言的问题。我们将这些问题集提交给语言模型，并通过几个提出的定量指标分析它们的响应中地缘政治偏见。这些指标比较不同语言的回答以及实际的地缘政治情况。地缘政治偏见现象是一种独特的跨语言评估。

    We introduce the notion of geopolitical bias -- a tendency to report different geopolitical knowledge depending on the linguistic context. As a case study, we consider territorial disputes between countries. For example, for the widely contested Spratly Islands, would an LM be more likely to say they belong to China if asked in Chinese, vs. to the Philippines if asked in Tagalog? To evaluate if such biases exist, we first collect a dataset of territorial disputes from Wikipedia, then associate each territory with a set of multilingual, multiple-choice questions. This dataset, termed BorderLines, consists of 250 territories with questions in 45 languages. We pose these question sets to language models, and analyze geopolitical bias in their responses through several proposed quantitative metrics. The metrics compare between responses in different question languages as well as to the actual geopolitical situation. The phenomenon of geopolitical bias is a uniquely cross-lingual evaluation
    
[^189]: PersonaLLM: 探究GPT-3.5表达个性特征和性别差异的能力

    PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences. (arXiv:2305.02547v1 [cs.CL])

    [http://arxiv.org/abs/2305.02547](http://arxiv.org/abs/2305.02547)

    本文探究了基于LLMs模拟代理的行为，称之为LLM Personas，在分配大五人格类型和性别角色时是否可以生成具有一致性的个性化特质的内容。

    

    尽管大型语言模型在各个行业的聊天机器人设计中有许多用途，并且研究表明个性化聊天机器人在满足不同人格特征方面的重要性，但很少有研究评估个性化LLM的行为是否能够准确、一致地反映某些人格特征。我们考虑研究基于LLM的模拟代理的行为，称之为LLM personas，并使用GPT-3.5（text-davinci-003）进行案例研究，以研究LLM在分配大五人格类型和性别角色时是否可以生成具有一致性的个性化特质的内容。我们创建了320个LLM personas（每种大五人格类型有5个女性和5个男性），并提示他们完成经典的44项大五人格问卷（BFI），然后撰写一个关于他们童年的800字故事。结果表明，LLM personas的自我报告的BFI分数与他们分配的人格类型一致。

    Despite the many use cases for large language models (LLMs) in the design of chatbots in various industries and the research showing the importance of personalizing chatbots to cater to different personality traits, little work has been done to evaluate whether the behaviors of personalized LLMs can reflect certain personality traits accurately and consistently. We consider studying the behavior of LLM-based simulated agents which refer to as LLM personas and present a case study with GPT-3.5 (text-davinci-003) to investigate whether LLMs can generate content with consistent, personalized traits when assigned Big Five personality types and gender roles. We created 320 LLM personas (5 females and 5 males for each of the 32 Big Five personality types) and prompted them to complete the classic 44-item Big Five Inventory (BFI) and then write an 800-word story about their childhood. Results showed that LLM personas' self-reported BFI scores are consistent with their assigned personality typ
    
[^190]: 使用大语言模型的机器翻译新趋势：以ChatGPT为例

    New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT. (arXiv:2305.01181v1 [cs.CL])

    [http://arxiv.org/abs/2305.01181](http://arxiv.org/abs/2305.01181)

    本文提出了使用大型语言模型的机器翻译中的几个新方向，包括风格化MT、交互式MT和基于翻译记忆的MT，并讨论了隐私问题的解决方案。

    

    近年来，机器翻译（MT）在深度学习的推动下取得了显著进展，特别是在GPT-3和ChatGPT等大型语言模型（LLMs）的出现后。这为使用LLMs的MT带来了新的挑战和机遇。本文提出了一些有趣的使用LLMs的MT方向，包括风格化MT、交互式MT和基于翻译记忆的MT，以及一种使用LLMs的新评估范例。同时，我们还讨论了使用LLMs的MT中的隐私问题，并提出了一种基本的隐私保护方法以减轻此类风险。为了说明我们提出的方法的潜力，我们给出了几个以上提到的新方向的示例，展示了所提出方向的可行性，并突出了使用LLMs的MT未来研究的机会和挑战。

    Machine Translation (MT) has made significant progress in recent years using deep learning, especially after the emergence of large language models (LLMs) such as GPT-3 and ChatGPT. This brings new challenges and opportunities for MT using LLMs. In this paper, we brainstorm some interesting directions for MT using LLMs, including stylized MT, interactive MT, and Translation Memory-based MT, as well as a new evaluation paradigm using LLMs. We also discuss the privacy concerns in MT using LLMs and a basic privacy-preserving method to mitigate such risks. To illustrate the potential of our proposed directions, we present several examples for the new directions mentioned above, demonstrating the feasibility of the proposed directions and highlight the opportunities and challenges for future research in MT using LLMs.
    
[^191]: 使用大型语言模型进行初学者的（非）形式化和自然论证练习

    Using large language models for (de-)formalization and natural argumentation exercises for beginner's students. (arXiv:2304.06186v1 [cs.CL])

    [http://arxiv.org/abs/2304.06186](http://arxiv.org/abs/2304.06186)

    本研究描述了两个系统，利用大型语言模型，自动纠正初学者在逻辑语言转化和自然语言论证方面的问题。

    

    我们描述了两个系统，使用文本达芬奇-003，一个大型语言模型，自动纠正（i）自然语言与命题逻辑语言和一阶谓词逻辑语言之间转化的练习; 和（ii）在非数学场景下用自然语言编写简单论点的练习。

    We describe two systems that use text-davinci-003, a large language model, for the automatized correction of (i) exercises in translating back and forth between natural language and the languages of propositional logic and first-order predicate logic and (ii) exercises in writing simple arguments in natural language in non-mathematical scenarios.
    
[^192]: 通过GPT-3自动形式化提高Diproche CNL系统

    Improving the Diproche CNL through autoformalization via GPT-3. (arXiv:2303.17513v1 [cs.CL])

    [http://arxiv.org/abs/2303.17513](http://arxiv.org/abs/2303.17513)

    本文探讨了在Diproche上使用大型语言模型进行自动形式化的可能性，并取得了令人鼓舞的初步结果。

    

    Diproche系统是一款针对德语控制语言片段的自动化证明检查器，旨在用于教学应用，在引导学生进行证明时使用。该系统的第一个版本使用一种控制自然语言，其Prolog形式化例程已经编写好。本文中，我们探讨了在Diproche上使用大型语言模型进行自动形式化的可能性，并取得了令人鼓舞的初步结果。

    The Diproche system is an automated proof checker for texts written in a controlled fragment of German, designed for didactical applications in classes introducing students to proofs for the first time. The first version of the system used a controlled natural language for which a Prolog formalization routine was written. In this paper, we explore the possibility of prompting large language models for autoformalization in the context of Diproche, with encouraging first results.
    
[^193]: 神经数据生成文本的创新：综述（arXiv：2207.12571v2 [cs.CL] 更新版）

    Innovations in Neural Data-to-text Generation: A Survey. (arXiv:2207.12571v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.12571](http://arxiv.org/abs/2207.12571)

    本文综述了过去十年神经数据生成文本（DTG）领域的创新，将其与其他自然语言生成技术（NLG）区分开来，并强调了关注系统语言能力和公平、公正的DTG研究前景。

    

    过去十年间，神经语言处理（NLP）的兴起在数据生成文本（DTG）领域同样带来了显著的创新。本综述通过结构化的方法，对神经DTG范式的方法、基准数据集和评估协议进行了综合性的概述，从自然语言生成（NLG）领域中区分出DTG，包括对文献的最新综合和技术采用阶段的重点介绍，强调了不仅关注设计具有语言能力的系统，而且还要关注体现公平和公 accountability 的系统的DTG研究有前景的方向。

    The neural boom that has sparked natural language processing (NLP) research through the last decade has similarly led to significant innovations in data-to-text generation (DTG). This survey offers a consolidated view into the neural DTG paradigm with a structured examination of the approaches, benchmark datasets, and evaluation protocols. This survey draws boundaries separating DTG from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella. With this holistic view, we highlight promising avenues for DTG research that not only focus on the design of linguistically capable systems but also systems that exhibit fairness and accountability.
    

