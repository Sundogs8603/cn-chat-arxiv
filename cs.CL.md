# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Towards LLM-RecSys Alignment with Textual ID Learning](https://arxiv.org/abs/2403.19021) | 通过提出IDGen，将每个推荐项目表示为独特、简洁、语义丰富的文本ID，从而使得基于大型语言模型的推荐更好地与自然语言生成对齐。 |
| [^2] | [Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models](https://arxiv.org/abs/2403.11802) | 提出了一种名为Counting-Stars的简单、高效、合理策略，用于评估长上下文大型语言模型的能力，并在实验中发现GPT-4 Turbo和Kimi Chat在此任务上取得显著性能。 |
| [^3] | [ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large Language Model](https://arxiv.org/abs/2403.06765) | 本研究提出了ConspEmoLLM，这是第一个集成情感信息的大型语言模型，通过对阴谋理论文本的情感特征进行综合分析，能够执行多项任务，包括阴谋理论检测、理论类型分类和相关文本检测。 |
| [^4] | [Biomedical Entity Linking as Multiple Choice Question Answering](https://arxiv.org/abs/2402.15189) | 提出了一种新颖的模型BioELQA，将生物医学实体链接看作是多项选择问答，通过使用快速检索器获得候选实体，实现了更好的实体链接效果。 |
| [^5] | [Multi-modal Stance Detection: New Datasets and Model](https://arxiv.org/abs/2402.14298) | 本文研究了多模式立场检测，提出了新的数据集和模型，并展示了所提出的TMPT框架在多模式立场检测中取得了最先进的性能 |
| [^6] | [Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?](https://arxiv.org/abs/2402.12025) | 这项研究关注语音翻译领域的发展，通过将语音基础模型与大语言模型结合，为解决多模态任务提供了新的统一模型，但目前各种评估方法和设置多样性阻碍了确定每个架构构建块的最佳解决方案的识别。 |
| [^7] | [GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving](https://arxiv.org/abs/2402.10104) | GeoEval基准测试用于评估LLMs和MMs在几何问题解决上的性能，发现WizardMath模型在主要子集上表现出色，但在具有挑战性的子集上准确率较低。 |
| [^8] | [Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features](https://arxiv.org/abs/2402.02969) | 通过研究随机特征，我们发现注意力层具有较高的词敏感性，这对于理解transformers的成功以及自然语言处理任务中的上下文含义非常重要。 |
| [^9] | [PipeNet: Question Answering with Semantic Pruning over Knowledge Graphs](https://arxiv.org/abs/2401.17536) | 本论文提出了PipeNet方法，通过语义修剪技术在知识图谱上进行问题回答。该方法通过关联-修剪-推理的流程来修剪噪声节点，以提高图推理的效率，同时获得良好的子图表示。 |
| [^10] | [TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese.](http://arxiv.org/abs/2401.16640) | 这篇论文开发了用于低资源环境中的开放式基础模型，以巴西葡萄牙语为例，发布在GitHub和Hugging Face上供社区使用和进一步开发。 |
| [^11] | [How Can Large Language Models Understand Spatial-Temporal Data?.](http://arxiv.org/abs/2401.14192) | 本文提出了一种名为STG-LLM的创新方法，用于使大型语言模型能够理解时空数据并进行预测。该方法利用STG-Tokenizer将复杂的图形数据转化为简洁的标记，再通过STG-Adapter将标记化数据与LLM的理解能力进行连接。通过微调参数，STG-LLM能够有效地把握标记的语义，同时保留LLM的自然语言理解能力。通过广泛的实验验证了STG-LLM的优越性能。 |
| [^12] | [Are self-explanations from Large Language Models faithful?.](http://arxiv.org/abs/2401.07927) | 大型语言模型的自我解释是否可靠是一个重要的AI安全考虑因素，我们提出使用自洽性检测作为评估其可靠性和解释能力的方法。 |
| [^13] | [Semantic-Aware Contrastive Sentence Representation Learning with Large Language Models.](http://arxiv.org/abs/2310.10962) | 本论文提出了SemCSR框架，利用大型语言模型（LLM）的生成和评估能力，无需人工标注就能自动构建高质量的自然语言推理样式语料库，用于解决对比学习中的挑战。 |
| [^14] | [Making Multimodal Generation Easier: When Diffusion Models Meet LLMs.](http://arxiv.org/abs/2310.08949) | EasyGen是一个有效的模型，它通过结合扩散模型和大型语言模型（LLMs）的能力，实现了更容易的多模态生成。相比现有的模型，EasyGen使用了一个名为BiDiffuser的双向条件扩散模型， 提供了更高效的模态交互，并且不仅能够生成文本回复，还能够促进文本到图像的生成。 |
| [^15] | [Identifying the Risks of LM Agents with an LM-Emulated Sandbox.](http://arxiv.org/abs/2309.15817) | 通过使用LM模拟工具执行和开发基于LM的自动安全评估器，该论文提出了一种解决测试LM代理的高成本和寻找高风险问题的方法。 |
| [^16] | [Large language models can accurately predict searcher preferences.](http://arxiv.org/abs/2309.10621) | 大型语言模型可以通过从真实用户那里获取高质量的第一方数据来准确预测搜索者的偏好。 |
| [^17] | [Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs.](http://arxiv.org/abs/2309.05516) | 本文提出一种名为SignRound的优化权重舍入的方法，通过使用有符号梯度进行轻量级分块调整，解决了大型语言模型(LLMs)的量化挑战。 |
| [^18] | [ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base.](http://arxiv.org/abs/2305.05994) | 本文提出了ANALOGYKB，一种使用百万规模知识库的类比推理方法，能够使语言模型在类比推理任务上取得比之前的最先进方法更好的结果。 |

# 详细

[^1]: 朝向LLM-RecSys对齐与文本ID学习的方向

    Towards LLM-RecSys Alignment with Textual ID Learning

    [https://arxiv.org/abs/2403.19021](https://arxiv.org/abs/2403.19021)

    通过提出IDGen，将每个推荐项目表示为独特、简洁、语义丰富的文本ID，从而使得基于大型语言模型的推荐更好地与自然语言生成对齐。

    

    基于大型语言模型(LLMs)的生成式推荐已经将传统的基于排名的推荐方式转变为文本生成范例。然而，与固有操作人类词汇的标准NLP任务相反，目前生成式推荐领域的研究在如何在文本生成范式中以简洁而有意义的ID表示有效编码推荐项目方面存在困难。为了更好地对齐LLMs与推荐需求，我们提出了IDGen，使用人类语言标记将每个项目表示为独特、简洁、语义丰富、与平台无关的文本ID。这通过在基于LLM的推荐系统旁训练文本ID生成器来实现，使个性化推荐能够无缝集成到自然语言生成中。值得注意的是，由于用户历史记录以自然语言表达并与原始数据集解耦，我们的方法提出了潜在的

    arXiv:2403.19021v1 Announce Type: cross  Abstract: Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potenti
    
[^2]: Counting-Stars：一种评估长上下文大型语言模型的简单、高效、合理策略

    Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models

    [https://arxiv.org/abs/2403.11802](https://arxiv.org/abs/2403.11802)

    提出了一种名为Counting-Stars的简单、高效、合理策略，用于评估长上下文大型语言模型的能力，并在实验中发现GPT-4 Turbo和Kimi Chat在此任务上取得显著性能。

    

    近期的研究主要集中在开发具有强大长上下文能力的大型语言模型（LLMs），由于缺乏适当的评估策略，对领先的LLMs（例如ChatGPT和KimiChat）的长上下文处理能力和性能了解甚少。为了填补这一空白，我们提出了一个简单、高效、合理的长上下文LLMs评估策略作为一个新的基准，名为Counting-Stars。Counting-Stars旨在要求LLMs充分理解和捕捉长上下文中的长依赖关系，并能够收集跨越整个上下文的多个证据之间的相互依赖来完成任务。基于Counting-Stars，我们进行实验评估了两个领先的长上下文LLMs，即GPT-4 Turbo和Kimi Chat。实验结果表明，GPT-4 Turbo和Kimi Chat在Counting-Stars任务上取得了显著的表现。

    arXiv:2403.11802v1 Announce Type: new  Abstract: While recent research endeavors have concentrated on developing Large Language Models (LLMs) with robust long-context capabilities, due to the lack of appropriate evaluation strategies, relatively little is known about how well the long-context processing abilities and performance of leading LLMs (e.g., ChatGPT and KimiChat). To address this gap, we propose a simple, efficient, and reasonable strategy for evaluating long-context LLMs as a new benchmark, named Counting-Stars. The Counting-Stars is designed to require LLMs to fully understand and capture long dependencies in long contexts and be able to collect inter-dependency across multiple pieces of evidence spanning the entire context to finish the task. Based on the Counting-Stars, we conduct experiments to evaluate the two leading long-context LLMs, i.e., GPT-4 Turbo and Kimi Chat. The experimental results indicate that GPT-4 Turbo and Kimi Chat achieve significant performance in th
    
[^3]: 使用基于情感的大型语言模型检测阴谋理论

    ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large Language Model

    [https://arxiv.org/abs/2403.06765](https://arxiv.org/abs/2403.06765)

    本研究提出了ConspEmoLLM，这是第一个集成情感信息的大型语言模型，通过对阴谋理论文本的情感特征进行综合分析，能够执行多项任务，包括阴谋理论检测、理论类型分类和相关文本检测。

    

    互联网给社会带来了好处和伤害。后者的一个主要例子是误导信息，包括充斥网络的阴谋理论。 自然语言处理的最新进展，特别是大型语言模型（LLMs）的出现，已经提高了准确检测误导信息的前景。然而，大多数基于LLM的阴谋理论检测方法仅专注于二元分类，并未考虑误导信息与情感特征（即情感和情绪）之间的重要关系。通过对揭示其独特情感特征的阴谋文本的全面分析，我们提出了ConspEmoLLM，这是第一个集成情感信息且能够执行涉及阴谋理论的多样任务的开源LLM。 这些任务不仅包括阴谋理论检测，还包括理论类型分类和相关文本检测。

    arXiv:2403.06765v1 Announce Type: new  Abstract: The internet has brought both benefits and harms to society. A prime example of the latter is misinformation, including conspiracy theories, which flood the web. Recent advances in natural language processing, particularly the emergence of large language models (LLMs), have improved the prospects of accurate misinformation detection. However, most LLM-based approaches to conspiracy theory detection focus only on binary classification and fail to account for the important relationship between misinformation and affective features (i.e., sentiment and emotions). Driven by a comprehensive analysis of conspiracy text that reveals its distinctive affective features, we propose ConspEmoLLM, the first open-source LLM that integrates affective information and is able to perform diverse tasks relating to conspiracy theories. These tasks include not only conspiracy theory detection, but also classification of theory type and detection of related d
    
[^4]: 将生物医学实体链接视为多项选择问答

    Biomedical Entity Linking as Multiple Choice Question Answering

    [https://arxiv.org/abs/2402.15189](https://arxiv.org/abs/2402.15189)

    提出了一种新颖的模型BioELQA，将生物医学实体链接看作是多项选择问答，通过使用快速检索器获得候选实体，实现了更好的实体链接效果。

    

    尽管预训练语言模型在生物医学实体链接（BioEL）方面取得了显著进展，但对于细粒度和长尾实体仍然存在挑战。为了解决这些挑战，我们提出了BioELQA，这是一种将生物医学实体链接视为多项选择问答的新颖模型。BioELQA首先利用快速检索器获得候选实体，将提及和候选实体共同呈现给生成器，然后输出与其选定实体相关的预测符号。这种公式使得不同候选实体之间的明确比较成为可能，从而捕捉了提及和实体之间以及实体之间的精细交互。为了改善长尾实体的泛化能力，我们检索相似的已标记训练实例作为线索，并将输入与检索实例连接到生成器。广泛的实验结果表明，BioELQA的表现优于统计结果。

    arXiv:2402.15189v1 Announce Type: cross  Abstract: Although biomedical entity linking (BioEL) has made significant progress with pre-trained language models, challenges still exist for fine-grained and long-tailed entities. To address these challenges, we present BioELQA, a novel model that treats Biomedical Entity Linking as Multiple Choice Question Answering. BioELQA first obtains candidate entities with a fast retriever, jointly presents the mention and candidate entities to a generator, and then outputs the predicted symbol associated with its chosen entity. This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves. To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator. Extensive experimental results show that BioELQA outperforms stat
    
[^5]: 多模式立场检测：新数据集和模型

    Multi-modal Stance Detection: New Datasets and Model

    [https://arxiv.org/abs/2402.14298](https://arxiv.org/abs/2402.14298)

    本文研究了多模式立场检测，提出了新的数据集和模型，并展示了所提出的TMPT框架在多模式立场检测中取得了最先进的性能

    

    立场检测是一项具有挑战性的任务，旨在从社交媒体平台中识别针对特定目标的公众意见。以往的立场检测工作主要集中在纯文本上。本文研究了包含文本和图像的推文的多模式立场检测，这在当今快速增长的社交媒体平台上是普遍存在的，人们经常发布多模式消息。为此，我们基于Twitter创建了五个新的不同领域的多模式立场检测数据集，其中每个示例包含文本和图像。此外，我们提出了一个简单而有效的目标多模式提示调整（TMPT）框架，其中利用目标信息从文本和视觉模态中学习多模式立场特征。对我们的三个基准数据集的实验证结果表明，所提出的TMPT在多模式立场检测中实现了最先进的性能。

    arXiv:2402.14298v1 Announce Type: new  Abstract: Stance detection is a challenging task that aims to identify public opinion from social media platforms with respect to specific targets. Previous work on stance detection largely focused on pure texts. In this paper, we study multi-modal stance detection for tweets consisting of texts and images, which are prevalent in today's fast-growing social media platforms where people often post multi-modal messages. To this end, we create five new multi-modal stance detection datasets of different domains based on Twitter, in which each example consists of a text and an image. In addition, we propose a simple yet effective Targeted Multi-modal Prompt Tuning framework (TMPT), where target information is leveraged to learn multi-modal stance features from textual and visual modalities. Experimental results on our three benchmark datasets show that the proposed TMPT achieves state-of-the-art performance in multi-modal stance detection.
    
[^6]: 使用语音基础模型和大语言模型的语音翻译：存在和缺失的内容是什么？

    Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?

    [https://arxiv.org/abs/2402.12025](https://arxiv.org/abs/2402.12025)

    这项研究关注语音翻译领域的发展，通过将语音基础模型与大语言模型结合，为解决多模态任务提供了新的统一模型，但目前各种评估方法和设置多样性阻碍了确定每个架构构建块的最佳解决方案的识别。

    

    自然语言处理（NLP）领域最近发生了一场变革性的转变，随着基础模型的出现，特别是彻底改变了基于文本的NLP的大型语言模型（LLMs）。这种范式已经扩展到其他形式，包括语音，在那里研究人员正在积极探索将语音基础模型（SFMs）和LLMs结合成单一的统一模型，以解决多模态任务。在这些任务中，本文着重于语音到文本翻译（ST）。通过审查该主题上发表的论文，我们提出了迄今为止提出的架构解决方案和训练策略的统一观点，强调它们之间的相似之处和差异之处。基于这一研究，我们不仅整理了所学到的经验教训，还展示了多样化的设置和评估方法如何阻碍对每个架构构建块的最佳性能解决方案的识别。

    arXiv:2402.12025v1 Announce Type: new  Abstract: The field of natural language processing (NLP) has recently witnessed a transformative shift with the emergence of foundation models, particularly Large Language Models (LLMs) that have revolutionized text-based NLP. This paradigm has extended to other modalities, including speech, where researchers are actively exploring the combination of Speech Foundation Models (SFMs) and LLMs into single, unified models capable of addressing multimodal tasks. Among such tasks, this paper focuses on speech-to-text translation (ST). By examining the published papers on the topic, we propose a unified view of the architectural solutions and training strategies presented so far, highlighting similarities and differences among them. Based on this examination, we not only organize the lessons learned but also show how diverse settings and evaluation approaches hinder the identification of the best-performing solution for each architectural building block 
    
[^7]: GeoEval：用于评估LLMs和多模型在几何问题解决上的基准

    GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving

    [https://arxiv.org/abs/2402.10104](https://arxiv.org/abs/2402.10104)

    GeoEval基准测试用于评估LLMs和MMs在几何问题解决上的性能，发现WizardMath模型在主要子集上表现出色，但在具有挑战性的子集上准确率较低。

    

    近期在大型语言模型（LLMs）和多模型（MMs）方面的进展展示了它们在问题解决方面的卓越能力。然而，它们在处理几何数学问题方面的熟练程度，即需要综合理解文本和视觉信息，尚未得到彻底评估。为了填补这一空白，我们推出了GeoEval基准测试，这是一个全面的集合，包括一个主要子集合的2000个问题，一个重点关注反推理的750个问题子集合，一个增强子集合的2000个问题以及一个难题子集合的300个问题。这个基准测试有助于更深入地研究LLMs和MMs在解决几何数学问题时的性能。我们对十个LLMs和MMs在这些不同子集上的评估结果显示，WizardMath模型表现出色，在主要子集上达到55.67%的准确率，但在具有挑战性的子集上只有6.00%的准确率。这突出了关键的需求。

    arXiv:2402.10104v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) and Multi-Modal Models (MMs) have demonstrated their remarkable capabilities in problem-solving. Yet, their proficiency in tackling geometry math problems, which necessitates an integrated understanding of both textual and visual information, has not been thoroughly evaluated. To address this gap, we introduce the GeoEval benchmark, a comprehensive collection that includes a main subset of 2000 problems, a 750 problem subset focusing on backward reasoning, an augmented subset of 2000 problems, and a hard subset of 300 problems. This benchmark facilitates a deeper investigation into the performance of LLMs and MMs on solving geometry math problems. Our evaluation of ten LLMs and MMs across these varied subsets reveals that the WizardMath model excels, achieving a 55.67\% accuracy rate on the main subset but only a 6.00\% accuracy on the challenging subset. This highlights the critical ne
    
[^8]: 关于注意力层的词敏感性的理解：通过随机特征的研究

    Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features

    [https://arxiv.org/abs/2402.02969](https://arxiv.org/abs/2402.02969)

    通过研究随机特征，我们发现注意力层具有较高的词敏感性，这对于理解transformers的成功以及自然语言处理任务中的上下文含义非常重要。

    

    揭示transformers异常成功背后原因需要更好地理解为什么注意力层适用于自然语言处理任务。特别是，这些任务要求预测模型捕捉上下文含义，即使句子很长，这往往取决于一个或几个词。我们的工作在随机特征的典型设置中研究了这一关键属性，称为词敏感性（WS）。我们展示了注意力层具有较高的WS，即在嵌入空间中存在一个向量，能够大幅扰动随机注意力特征映射。这个论点关键地利用了注意力层中softmax的作用，突显了它相对于其他激活函数（如ReLU）的优势。相反，标准随机特征的WS是$1/\sqrt{n}$阶的，$n$是文本样本中的单词数，因此它随上下文的长度而衰减。然后，我们将这些关于词敏感性的结果转化为泛化界：由于...

    Unveiling the reasons behind the exceptional success of transformers requires a better understanding of why attention layers are suitable for NLP tasks. In particular, such tasks require predictive models to capture contextual meaning which often depends on one or few words, even if the sentence is long. Our work studies this key property, dubbed word sensitivity (WS), in the prototypical setting of random features. We show that attention layers enjoy high WS, namely, there exists a vector in the space of embeddings that largely perturbs the random attention features map. The argument critically exploits the role of the softmax in the attention layer, highlighting its benefit compared to other activations (e.g., ReLU). In contrast, the WS of standard random features is of order $1/\sqrt{n}$, $n$ being the number of words in the textual sample, and thus it decays with the length of the context. We then translate these results on the word sensitivity into generalization bounds: due to th
    
[^9]: PipeNet:在知识图谱上使用语义修剪的问题回答

    PipeNet: Question Answering with Semantic Pruning over Knowledge Graphs

    [https://arxiv.org/abs/2401.17536](https://arxiv.org/abs/2401.17536)

    本论文提出了PipeNet方法，通过语义修剪技术在知识图谱上进行问题回答。该方法通过关联-修剪-推理的流程来修剪噪声节点，以提高图推理的效率，同时获得良好的子图表示。

    

    众所周知，在问题回答中引入显式的知识图谱(KG)可以带来好处。现有的方法通常遵循一个基础推理流程，在该流程中，首先将实体节点与查询(问题和候选答案)进行关联，然后使用推理模块对匹配的多跳子图进行推理，用于预测答案。虽然这个流程在从庞大的KG中提取必要信息方面取得了很大的改善，但在放大关联的子图时，效率仍然面临挑战。本文旨在找到子图中的语义相关的实体节点，以提高使用KG进行图推理时的效率。我们提出了一个基于关联-修剪-推理的流程来修剪噪声节点，在显著降低计算和内存使用的同时，还能获得良好的子图表示。具体来说，修剪模块首先根据匹配范围之间的依赖距离对概念节点进行评分，然后对其进行修剪。

    It is well acknowledged that incorporating explicit knowledge graphs (KGs) can benefit question answering. Existing approaches typically follow a grounding-reasoning pipeline in which entity nodes are first grounded for the query (question and candidate answers), and then a reasoning module reasons over the matched multi-hop subgraph for answer prediction. Although the pipeline largely alleviates the issue of extracting essential information from giant KGs, efficiency is still an open challenge when scaling up hops in grounding the subgraphs. In this paper, we target at finding semantically related entity nodes in the subgraph to improve the efficiency of graph reasoning with KG. We propose a grounding-pruning-reasoning pipeline to prune noisy nodes, remarkably reducing the computation cost and memory usage while also obtaining decent subgraph representation. In detail, the pruning module first scores concept nodes based on the dependency distance between matched spans and then prunes 
    
[^10]: TeenyTinyLlama：基于巴西葡萄牙语训练的开源微型语言模型

    TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese. (arXiv:2401.16640v1 [cs.CL])

    [http://arxiv.org/abs/2401.16640](http://arxiv.org/abs/2401.16640)

    这篇论文开发了用于低资源环境中的开放式基础模型，以巴西葡萄牙语为例，发布在GitHub和Hugging Face上供社区使用和进一步开发。

    

    大型语言模型（LLMs）在自然语言处理方面取得了显著的进展，但在各种语言中的进展还不平衡。虽然大多数LLMs是在像英语这样的高资源语言中训练的，但多语言模型通常比单语言模型表现稍差。此外，它们的多语言基础有时会限制它们产生的副产品，如计算需求和许可制度。在本研究中，我们记录了为在低资源环境中使用而量身定制的开放式基础模型的开发过程、其局限性和优势。这就是TeenyTinyLlama：两个用于巴西葡萄牙语文本生成的紧凑型模型。我们在GitHub和Hugging Face上以宽松的Apache 2.0许可证发布它们，供社区使用和进一步开发。详见https://github.com/Nkluge-correa/TeenyTinyLlama

    Large language models (LLMs) have significantly advanced natural language processing, but their progress has yet to be equal across languages. While most LLMs are trained in high-resource languages like English, multilingual models generally underperform monolingual ones. Additionally, aspects of their multilingual foundation sometimes restrict the byproducts they produce, like computational demands and licensing regimes. In this study, we document the development of open-foundation models tailored for use in low-resource settings, their limitations, and their benefits. This is the TeenyTinyLlama pair: two compact models for Brazilian Portuguese text generation. We release them under the permissive Apache 2.0 license on GitHub and Hugging Face for community use and further development. See https://github.com/Nkluge-correa/TeenyTinyLlama
    
[^11]: 如何让大型语言模型理解时空数据？

    How Can Large Language Models Understand Spatial-Temporal Data?. (arXiv:2401.14192v1 [cs.LG])

    [http://arxiv.org/abs/2401.14192](http://arxiv.org/abs/2401.14192)

    本文提出了一种名为STG-LLM的创新方法，用于使大型语言模型能够理解时空数据并进行预测。该方法利用STG-Tokenizer将复杂的图形数据转化为简洁的标记，再通过STG-Adapter将标记化数据与LLM的理解能力进行连接。通过微调参数，STG-LLM能够有效地把握标记的语义，同时保留LLM的自然语言理解能力。通过广泛的实验验证了STG-LLM的优越性能。

    

    尽管大型语言模型（LLM）在自然语言处理和计算机视觉等任务中占据主导地位，但利用它们的能力进行时空预测仍然具有挑战性。时序文本与复杂的时空数据之间的差异阻碍了该应用的实现。为了解决这个问题，本文提出了STG-LLM，一种创新的方法，为LLM赋予了时空预测的能力。我们通过以下方式解决数据不匹配的问题：1）STG-Tokenizer：这个时空图形标记器将复杂的图形数据转化为简洁的标记，捕捉了空间和时间关系；2）STG-Adapter：这个精简的适配器由线性编码和解码层组成，填补了标记化数据和LLM理解之间的差距。通过仅微调一小部分参数，它可以有效地把握STG-Tokenizer生成的标记的语义，同时保留LLM的原始自然语言理解能力。通过在多种数据集上进行广泛实验，我们验证了STG-LLM的优越性能。

    While Large Language Models (LLMs) dominate tasks like natural language processing and computer vision, harnessing their power for spatial-temporal forecasting remains challenging. The disparity between sequential text and complex spatial-temporal data hinders this application. To address this issue, this paper introduces STG-LLM, an innovative approach empowering LLMs for spatial-temporal forecasting. We tackle the data mismatch by proposing: 1) STG-Tokenizer: This spatial-temporal graph tokenizer transforms intricate graph data into concise tokens capturing both spatial and temporal relationships; 2) STG-Adapter: This minimalistic adapter, consisting of linear encoding and decoding layers, bridges the gap between tokenized data and LLM comprehension. By fine-tuning only a small set of parameters, it can effectively grasp the semantics of tokens generated by STG-Tokenizer, while preserving the original natural language understanding capabilities of LLMs. Extensive experiments on diver
    
[^12]: 大型语言模型的自我解释是否可靠?

    Are self-explanations from Large Language Models faithful?. (arXiv:2401.07927v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.07927](http://arxiv.org/abs/2401.07927)

    大型语言模型的自我解释是否可靠是一个重要的AI安全考虑因素，我们提出使用自洽性检测作为评估其可靠性和解释能力的方法。

    

    经过训练的大型语言模型在许多任务上表现出色，甚至能够提供其行为的解释。由于这些模型对公众是直接可访问的，因此存在这样的风险，即令人信服但错误的解释可能导致对大型语言模型的无支撑的自信。因此，解释能力和可靠性是AI安全的重要考虑因素。评估自我解释的可靠性和可解释性是一项具有挑战性的任务，因为这些模型对于人类来说过于复杂，无法注释什么是正确的解释。为了解决这个问题，我们提出使用自洽性检测作为可靠性的衡量指标。例如，如果一个大型语言模型说某组词对于做出预测很重要，那么在没有这些词的情况下，它应该无法做出相同的预测。虽然自洽性检测是一种常见的可靠性方法，但之前尚未应用于大型语言模型的自我解释中。我们将自洽性检测应用于...

    Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to t
    
[^13]: 带有大型语言模型的语义感知对比句子表示学习

    Semantic-Aware Contrastive Sentence Representation Learning with Large Language Models. (arXiv:2310.10962v1 [cs.CL])

    [http://arxiv.org/abs/2310.10962](http://arxiv.org/abs/2310.10962)

    本论文提出了SemCSR框架，利用大型语言模型（LLM）的生成和评估能力，无需人工标注就能自动构建高质量的自然语言推理样式语料库，用于解决对比学习中的挑战。

    

    对比学习已被证明在学习更好的句子表示方面非常有效。然而，为了训练对比学习模型，需要大量带标签的句子来明确构建正负对（例如在自然语言推理数据集中）。不幸的是，获取足够高质量的标注数据既费时又耗资源，因此研究人员开始关注开发无监督句子表示学习方法。由于这些非结构化的随机抽样句子之间没有明确的关联，构建正负对可能会很困难和有问题。为了解决这些挑战，在本文中，我们提出了一种语义感知的对比句子表示框架（SemCSR）。通过利用大型语言模型（LLM）的生成和评估能力，我们可以自动构建一个高质量的自然语言推理样式语料库，而无需任何人工标注。

    Contrastive learning has been proven to be effective in learning better sentence representations. However, to train a contrastive learning model, large numbers of labeled sentences are required to construct positive and negative pairs explicitly, such as those in natural language inference (NLI) datasets. Unfortunately, acquiring sufficient high-quality labeled data can be both time-consuming and resource-intensive, leading researchers to focus on developing methods for learning unsupervised sentence representations. As there is no clear relationship between these unstructured randomly-sampled sentences, building positive and negative pairs over them is tricky and problematic. To tackle these challenges, in this paper, we propose SemCSR, a semantic-aware contrastive sentence representation framework. By leveraging the generation and evaluation capabilities of large language models (LLMs), we can automatically construct a high-quality NLI-style corpus without any human annotation, and f
    
[^14]: Easier Multimodal Generation: Diffusion Models Meet LLMs

    Making Multimodal Generation Easier: When Diffusion Models Meet LLMs. (arXiv:2310.08949v1 [cs.AI])

    [http://arxiv.org/abs/2310.08949](http://arxiv.org/abs/2310.08949)

    EasyGen是一个有效的模型，它通过结合扩散模型和大型语言模型（LLMs）的能力，实现了更容易的多模态生成。相比现有的模型，EasyGen使用了一个名为BiDiffuser的双向条件扩散模型， 提供了更高效的模态交互，并且不仅能够生成文本回复，还能够促进文本到图像的生成。

    

    我们提出了EasyGen，一个有效的模型，通过利用扩散模型和大型语言模型（LLMs）的能力，增强了多模态理解和生成。不同于现有的主要依赖于编码器如CLIP或ImageBind，并且需要大量训练数据来桥接模态之间差距的多模态模型，EasyGen基于一个名为BiDiffuser的双向条件扩散模型构建，促进了更高效的模态交互。EasyGen通过简单的投影层将BiDiffuser和LLM进行集成，处理图像到文本的生成。与大多数现有的限于生成文本回复的多模态模型不同，EasyGen还可以通过利用LLM创建文本描述，并由BiDiffuser解释生成适当的视觉回复来促进文本到图像的生成。广泛的定量和定性实验证明了EasyGen的有效性，其训练可以...

    We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge the gap between modalities, EasyGen is built upon a bidirectional conditional diffusion model named BiDiffuser, which promotes more efficient interactions between modalities. EasyGen handles image-to-text generation by integrating BiDiffuser and an LLM via a simple projection layer. Unlike most existing multimodal models that are limited to generating text responses, EasyGen can also facilitate text-to-image generation by leveraging the LLM to create textual descriptions, which can be interpreted by BiDiffuser to generate appropriate visual responses. Extensive quantitative and qualitative experiments demonstrate the effectiveness of EasyGen, whose training can b
    
[^15]: 使用LM模拟沙盒识别LM代理的风险

    Identifying the Risks of LM Agents with an LM-Emulated Sandbox. (arXiv:2309.15817v1 [cs.AI])

    [http://arxiv.org/abs/2309.15817](http://arxiv.org/abs/2309.15817)

    通过使用LM模拟工具执行和开发基于LM的自动安全评估器，该论文提出了一种解决测试LM代理的高成本和寻找高风险问题的方法。

    

    最近的语言模型（LM）代理和工具使用的技术进步，例如ChatGPT插件，使得代理具备了丰富的功能，但也放大了潜在的风险，如泄露私人数据或引发财务损失。识别这些风险是一项耗时的工作，需要实施工具，手动设置每个测试场景的环境，并找到风险案例。随着工具和代理变得越来越复杂，测试这些代理的高成本将使寻找高风险、长尾风险变得越来越困难。为了解决这些挑战，我们引入了ToolEmu：一个使用LM来模拟工具执行的框架，可以在不需要手动实例化的情况下对LM代理进行各种工具和场景的测试。除了模拟器，我们还开发了一个基于LM的自动安全评估器，用于检查代理的失败并量化相关风险。我们通过人工评估测试了工具模拟器和评估器，并发现了6个...

    Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, manually setting up the environment for each test scenario, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tailed risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables the testing of LM agents against a diverse range of tools and scenarios, without manual instantiation. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 6
    
[^16]: 大型语言模型能够准确预测搜索者的偏好

    Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])

    [http://arxiv.org/abs/2309.10621](http://arxiv.org/abs/2309.10621)

    大型语言模型可以通过从真实用户那里获取高质量的第一方数据来准确预测搜索者的偏好。

    

    相关性标签是评估和优化搜索系统的关键。获取大量相关性标签通常需要第三方标注人员，但存在低质量数据的风险。本论文介绍了一种改进标签质量的替代方法，通过从真实用户那里获得仔细反馈来获取高质量的第一方数据。

    Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that 
    
[^17]: 通过有符号梯度下降优化LLMs量化中的权重舍入

    Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05516](http://arxiv.org/abs/2309.05516)

    本文提出一种名为SignRound的优化权重舍入的方法，通过使用有符号梯度进行轻量级分块调整，解决了大型语言模型(LLMs)的量化挑战。

    

    大型语言模型(LLMs)在执行语言相关任务方面表现出了非凡的能力。然而，由于其巨大的内存和存储需求，它们的部署面临着重大挑战。为了解决这个问题，仅针对权重的量化，特别是3位和4位仅针对权重的量化，已经成为最可行的解决方案之一。随着位数的减少，量化网格变得更加宽泛，从而强调了上下舍入的重要性。尽管先前的研究表明，在某些情况下，通过添加扰动细调上下舍入可以提高准确性，但我们的研究受制于这些扰动的精确且有限的边界，只有改变舍入值的阈值才具有重要性。因此，我们提出了一种简洁高效的优化权重舍入任务的方法。我们的方法名为SignRound，它涉及使用有符号梯度的轻量级分块调整。

    Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gra
    
[^18]: ANALOGYKB：使用百万规模知识库开启语言模型的类比推理能力。

    ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base. (arXiv:2305.05994v1 [cs.CL])

    [http://arxiv.org/abs/2305.05994](http://arxiv.org/abs/2305.05994)

    本文提出了ANALOGYKB，一种使用百万规模知识库的类比推理方法，能够使语言模型在类比推理任务上取得比之前的最先进方法更好的结果。

    

    类比推理是人类的一项基本认知能力，然而，由于缺乏模型训练资源，目前的语言模型仍然难以在类比推理任务中达到人类的表现水平。本文提出了ANALOGYKB，这是一个百万规模的类比知识库，它由现有的知识图谱导出。ANALOGYKB从知识图谱中识别了两种类型的类比：1）相同关系的类比，可以直接从知识图谱中提取；2）类似关系的类比，则由大型语言模型（InstructGPT）启用的选择和过滤管道进行识别，再经过少量人工质量控制。在两个类比推理任务（类比识别和生成）的一系列数据集上的评估表明，ANALOGYKB成功地使语言模型取得了比之前的最先进方法更好的结果。

    Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large LMs (InstructGPT), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables LMs to achieve much better results than previous state-of-the-art methods.
    

