# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery.](http://arxiv.org/abs/2304.13714) | 本研究评估了在临床环境中使用GPT-3.5和GPT-4解决医学问题的安全性以及与信息技术咨询服务报告的一致性。研究结果表明，两个LLMs都可以以安全和一致的方式满足医生的信息需求。 |
| [^2] | [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.](http://arxiv.org/abs/2304.13712) | 本文提供了一个LLMs的使用综述，探讨了在各种自然语言处理任务中的使用和限制。 |
| [^3] | [HeySQuAD: A Spoken Question Answering Dataset.](http://arxiv.org/abs/2304.13689) | HeySQuAD 是一个大规模口语化问答数据集，旨在衡量机器理解并回答嘈杂的口语提问的能力，同时使用转录的人类口语提问进行训练能显著提高模型表现。 |
| [^4] | [Using Implicit Feedback to Improve Question Generation.](http://arxiv.org/abs/2304.13664) | 本研究提出了一个名为GEN的系统，利用一小组句子/问题对作为输入，通过基于模式的方法生成问题。该系统可以从用户的反馈中学习并提高生成问题的质量。 |
| [^5] | [A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering.](http://arxiv.org/abs/2304.13649) | 本文提出了一种新的对称双编码稠密检索框架DEDR，以弥合文本和图像之间的空间差距，并进一步引入了MM-FiD，一种多模式融合解码器模型，用于知识密集型视觉问答任务，其效果优于现有最先进方法。 |
| [^6] | [ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries.](http://arxiv.org/abs/2304.13620) | 本文提出了ChartSumm数据集，用于长短摘要自动生成任务，包括84000多个图表及其元数据和描述。研究发现，现有的自动摘要模型虽然得分不错，但经常面临错觉、漏掉重要数据点以及不正确解释复杂趋势等问题。 |
| [^7] | [Shades of meaning: Uncovering the geometry of ambiguous word representations through contextualised language models.](http://arxiv.org/abs/2304.13597) | 本研究通过利用上下文化语言模型的方法，发现这些模型的单词表征捕捉到了无歧义、异义和多义词之间的细微有意义差别，为对词汇歧义的心理学概念化提供了定量支持以及对上下文信息如何影响单词表示提出了新的挑战。 |
| [^8] | [Toxic comments reduce the activity of volunteer editors on Wikipedia.](http://arxiv.org/abs/2304.13568) | 本文分析了维基百科六种最活跃的语言版本中8.5万名编辑的用户对话页面上发表的5700万条评论，在短期内发现毒性评论不断降低编辑的活跃程度，并导致每个用户的活跃天数估计减少至少0.5-2天，长期效应更为显著，因为它们显着增加了编辑离开的风险。 |
| [^9] | [Impact of Position Bias on Language Models in Token Classification.](http://arxiv.org/abs/2304.13567) | 研究了语言模型在token分类任务中的位置偏差问题，通过实验表明在具体任务中，BERT、ERNIE、ELECTRA等编码器以及GPT2和BLOOM等解码器的平均性能下降了3%和9%。 |
| [^10] | [Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables.](http://arxiv.org/abs/2304.13559) | 提出了一种新型数据库系统MMDB，它可以无缝查询文本和表格数据，通过使用所谓的多模态运算符（MMOps）实现文本集合的转换，该方法不仅优于现有的最先进方法，在对未见过的数据进行模型微调时，也具有显著的优势。 |
| [^11] | ["I'm" Lost in Translation: Pronoun Missteps in Crowdsourced Data Sets.](http://arxiv.org/abs/2304.13557) | 这项研究发现在英语和日语之间的翻译中存在男性代词偏见，同时也检测到了对女性、中性和/或非二元代词存在的微妙反应的偏见。他们提出了针对代词翻译偏见的问题，并提供了将复数嵌入NLP数据集的解决方案。 |
| [^12] | [Multidimensional Evaluation for Text Style Transfer Using ChatGPT.](http://arxiv.org/abs/2304.13462) | 本研究探究使用ChatGPT作为多维评估器，测试其在文本风格转换中的性能，并与现有的自动度量标准和人类判断进行比较。与自动度量标准相比，ChatGPT在与人类判断的相关性上具有竞争力。 |
| [^13] | [Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System.](http://arxiv.org/abs/2304.13343) | 该论文提出了一种自控内存系统，可以使大规模语言模型能够处理任意长度的输入，从而显著提高模型的性能表现。 |
| [^14] | [Nominal Topology for Data Languages.](http://arxiv.org/abs/2304.13337) | 该论文提出了一种新的拓扑视角，用于描述可以被轨道有限的名义单子群识别的数据语言，并探讨了 pro-轨道有限方程的表现能力。 |
| [^15] | [A Case-Based Reasoning Framework for Adaptive Prompting in Cross-Domain Text-to-SQL.](http://arxiv.org/abs/2304.13301) | 本文提出了一个基于案例推理框架的跨域文本到SQL自适应提示的解决方案，可以精确控制与案例相关和不相关的知识，解决了大型语言模型提示设计不良限制性能的问题。 |
| [^16] | [From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping.](http://arxiv.org/abs/2304.13273) | 本研究提出了一种从关联到生成的零-shot方法：通过将图像/视频投影到语言模态并在生成任务中生成描述性字幕。该方法在多个基准数据集上显著优于现有的最先进方法，为无监督跨模态映射提供了一个新的视角，并具有在视频字幕，图像合成和文本到图像生成等领域的潜在应用。 |
| [^17] | [Exploring the Curious Case of Code Prompts.](http://arxiv.org/abs/2304.13250) | 研究比较了代码提示和文本提示在多种自然语言任务上的性能表现，发现除了少数任务外，代码提示并不总是比文本提示更好，代码提示风格对性能有一定影响，而微调可以提高代码提示的性能。 |
| [^18] | [Towards Explainable and Safe Conversational Agents for Mental Health: A Survey.](http://arxiv.org/abs/2304.13191) | 这篇论文调查了现有的心理健康会话型智能助手，提出了改进的新见解，并介绍了如何构建责任VMHA，以提出后续问题或提供知情回应，丰富用户体验。 |
| [^19] | [TABLET: Learning From Instructions For Tabular Data.](http://arxiv.org/abs/2304.13188) | 该论文提出了TABLET，这是一个由20个不同的包含指令注释的表格数据集组成的基准测试，可以提高大型语言模型在表格预测问题上的效果，并评估了指令在保真度和LLM在表格预测方面的局限性。 |
| [^20] | [Sebis at SemEval-2023 Task 7: A Joint System for Natural Language Inference and Evidence Retrieval from Clinical Trial Reports.](http://arxiv.org/abs/2304.13180) | 本文描述了两个NLP系统：一个为自然语言推理，一个为临床试验数据证据检索。它们分别采用了流水线模型和联合模型，并在最终的集成系统中融合输出。 |
| [^21] | [Modeling Spoken Information Queries for Virtual Assistants: Open Problems, Challenges and Opportunities.](http://arxiv.org/abs/2304.13149) | 讨论了虚拟助手中口语信息查询建模的问题和挑战，以及信息检索方法中的机遇；探讨了如何通过查询领域分类、知识图谱等提高语音识别的准确性；简要概述了语音识别中的挑战。 |
| [^22] | [ESimCSE Unsupervised Contrastive Learning Jointly with UDA Semi-Supervised Learning for Large Label System Text Classification Mode.](http://arxiv.org/abs/2304.13140) | 本文提出ESimCSE无监督比较学习和UDA半监督比较学习模型相结合，通过联合训练技术解决了大标签系统文本分类的多个问题，并在公共数据集上实现了准确率提高。 |
| [^23] | [LAST: Scalable Lattice-Based Speech Modelling in JAX.](http://arxiv.org/abs/2304.13134) | LAST是一个基于JAX的库，用于实现灵活易用的格模型语音转录器。它采用了一系列适用于解决现代架构性能特征和自动微分中的细微差别新挑战的通用技术，并在TPUv3和V100 GPU上得到了验证。 |
| [^24] | [Pretrain on just structure: Understanding linguistic inductive biases using transfer learning.](http://arxiv.org/abs/2304.13060) | 通过在人造结构数据上进行预先训练和在英语上微调，我们研究了自然语言处理中三种归纳偏置类型：递归的层级处理、无限制的标记-标记依赖以及基于Zipfian幂律词汇分布的归纳偏置，我们得出复杂标记-标记交互形成了最好的归纳偏置的结论。 |
| [^25] | [ThreatCrawl: A BERT-based Focused Crawler for the Cybersecurity Domain.](http://arxiv.org/abs/2304.11960) | 本文提出了一种基于BERT的焦点爬虫ThreatCrawl，使用主题建模和关键词提取技术来筛选出最可能包含有价值CTI信息的网页。 |
| [^26] | [Boosting Theory-of-Mind Performance in Large Language Models via Prompting.](http://arxiv.org/abs/2304.11490) | 本研究通过提示提高大型语言模型（LLMs）在心智理论（ToM）任务上的表现，证明了上下文学习可以提升LLMs在复杂推理特别是ToM任务中的表现。 |
| [^27] | [GPT-NER: Named Entity Recognition via Large Language Models.](http://arxiv.org/abs/2304.10428) | 本文提出了GPT-NER来解决大型语言模型在命名实体识别任务（NER）上表现不佳的问题，它通过将序列标记任务转化为生成任务，将LLM能够容易地适应NER任务。同时，为了有效解决LLMs“幻觉”问题，作者们提出了自我验证策略，通过提示LLMs询问自身来确定提取的实体是否属于实际存在的实体。 |
| [^28] | [LLM as A Robotic Brain: Unifying Egocentric Memory and Control.](http://arxiv.org/abs/2304.09349) | 本文提出了一个统一自我中心记忆和控制的框架LLM-Brain，使用大规模语言模型作为机器人大脑进行零-shot学习。该框架包括封闭式多轮对话，覆盖了感知、规划、控制和记忆，具有很好的泛化性能，适用于多个机器人任务。 |
| [^29] | [On the Risks of Stealing the Decoding Algorithms of Language Models.](http://arxiv.org/abs/2303.04729) | 这项工作首次展示，一个拥有典型API访问权限的对手可以以极低的金钱成本窃取GPT-2和GPT-3等LM的解码算法的类型和超参数。 |
| [^30] | [Predicting Sentence-Level Factuality of News and Bias of Media Outlets.](http://arxiv.org/abs/2301.11850) | 本论文提出了一种针对整个媒体的细粒度可靠性分析方法，在手动制作的“FactNews”数据库上，通过 fine-tuning BERT 模型预测新闻报道的句子级别事实性和媒体倾向。此方法可应用于任何其他语言。 |
| [^31] | [Incorporating Knowledge into Document Summarisation: an Application of Prefix-Tuning on GPT-2.](http://arxiv.org/abs/2301.11719) | 本论文研究了将事实知识纳入生成的摘要的可能性，具体采用前缀调整的方法，实验结果表明，此方法可以生成保留知识的摘要，而且可以提升整体性能。 |
| [^32] | [Prompting Is Programming: A Query Language for Large Language Models.](http://arxiv.org/abs/2212.06094) | LMP将语言模型提示从纯文本提示扩展为文本提示和脚本的直观组合，实现了一种新的语言模型编程方式。 |
| [^33] | [Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification.](http://arxiv.org/abs/2211.11158) | 本文提出了一种名为LaBo的语言模型引导的概念瓶颈模型，能够不需要手动指定关键概念并实现与黑盒子模型相似的性能，在图像分类中具有重要的可解释性。 |
| [^34] | [Language Modelling with Pixels.](http://arxiv.org/abs/2207.06991) | 本文介绍了一个名为PIXEL的基于像素的预训练语言模型，它可以将文本渲染为图像，解决了扩展支持的语言数量时出现的词汇瓶颈问题，且在形态学和语义任务上显著优于BERT。 |
| [^35] | [Detection of sepsis during emergency department triage using machine learning.](http://arxiv.org/abs/2204.07657) | 本研究利用机器学习开发出一种检测急诊科分诊前败血症的模型，其性能优于标准败血症筛查算法。 |
| [^36] | [Cross-linguistic differences in gender congruency effects: Evidence from meta-analyses.](http://arxiv.org/abs/2109.03490) | 通过元分析证实，日尔曼语言和斯拉夫语言中的性别一致效应比罗曼语言更加稳健，但效应大小适中，并且存在研究间的变异性。 |

# 详细

[^1]: 评估GPT-3.5和GPT-4在支持医疗保健信息需求方面的实际作用

    Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. (arXiv:2304.13714v1 [cs.AI])

    [http://arxiv.org/abs/2304.13714](http://arxiv.org/abs/2304.13714)

    本研究评估了在临床环境中使用GPT-3.5和GPT-4解决医学问题的安全性以及与信息技术咨询服务报告的一致性。研究结果表明，两个LLMs都可以以安全和一致的方式满足医生的信息需求。

    

    尽管在医疗保健领域使用大型语言模型(LLMs)越来越受关注，但当前的探索并未评估LLMs在临床环境中的实用性和安全性。我们的目标是确定两个LLM是否可以以安全和一致的方式满足由医生提交的信息需求问题。我们将66个来自信息技术咨询服务的问题通过简单的提示提交给GPT-3.5和GPT-4。12名医生评估了LLM响应对患者造成伤害的可能性以及与信息技术咨询服务的现有报告的一致性。医生的评估基于多数票汇总。对于没有任何问题，大多数医生认为任何一个LLM响应都不会造成伤害。对于GPT-3.5，8个问题的响应与信息技术咨询报告一致，20个不一致，9个无法评估。有29个响应没有多数票表示“同意”、“不同意”和“无法评估”。

    Despite growing interest in using large language models (LLMs) in healthcare, current explorations do not assess the real-world utility and safety of LLMs in clinical settings. Our objective was to determine whether two LLMs can serve information needs submitted by physicians as questions to an informatics consultation service in a safe and concordant manner. Sixty six questions from an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple prompts. 12 physicians assessed the LLM responses' possibility of patient harm and concordance with existing reports from an informatics consultation service. Physician assessments were summarized based on majority vote. For no questions did a majority of physicians deem either LLM response as harmful. For GPT-3.5, responses to 8 questions were concordant with the informatics consult report, 20 discordant, and 9 were unable to be assessed. There were 29 responses with no majority on "Agree", "Disagree", and "Unable to assess". Fo
    
[^2]: 发挥LLMs在实践中的力量：ChatGPT及其应用的综述调查

    Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. (arXiv:2304.13712v1 [cs.CL])

    [http://arxiv.org/abs/2304.13712](http://arxiv.org/abs/2304.13712)

    本文提供了一个LLMs的使用综述，探讨了在各种自然语言处理任务中的使用和限制。

    

    本文为从事下游自然语言处理（NLP）任务的从业人员和最终用户提供了一个全面实用的指南，介绍了如何利用Large Language Models（LLMs）。我们从模型、数据和下游任务的角度提供了LLMs的使用讨论和见解。首先，我们介绍了当前的GPT和BERT样式的LLMs。然后，讨论了预训练数据、训练数据和测试数据的影响。最重要的是，我们详细讨论了大型语言模型在各种自然语言处理任务中的使用和非使用情况，例如知识密集型任务、传统自然语言理解任务、自然语言生成任务、紧急能力以及特定任务的考虑。我们呈现了各种使用和非使用情况，以说明LLMs在实际情况下的实际应用和限制。我们还试图了解数据对于LLMs应用的重要性。

    This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of dat
    
[^3]: HeySQuAD: 一个口语化问答数据集

    HeySQuAD: A Spoken Question Answering Dataset. (arXiv:2304.13689v1 [cs.CL])

    [http://arxiv.org/abs/2304.13689](http://arxiv.org/abs/2304.13689)

    HeySQuAD 是一个大规模口语化问答数据集，旨在衡量机器理解并回答嘈杂的口语提问的能力，同时使用转录的人类口语提问进行训练能显著提高模型表现。

    

    人类口语提问对于评估口语问答系统的性能至关重要，尤其是数字助手等多个实际应用场景。本文提出了一个新的大规模社区共享的口语问答数据集 HeySQuAD，它由76k个人类口语提问、97k个机器生成的问题以及相应的文本答案组成，这些答案源自 SQuAD QA 数据集。HeySQuAD 的目标是衡量机器理解嘈杂的口语提问并准确回答问题的能力。为此，我们在人类口语和机器生成的问题上进行了广泛的基准测试，以量化来自两方面噪声的差异及对模型和回答准确度的影响。重要的是，在口语问答任务中，我们希望回答的是人类口语提问，我们观察到使用转录的人类口语提问和原始 SQuAD 问题进行训练，能够显著提高（12.51%）模型的表现，而不是仅使用原始 SQuAD 数据集。

    Human-spoken questions are critical to evaluating the performance of spoken question answering (SQA) systems that serve several real-world use cases including digital assistants. We present a new large-scale community-shared SQA dataset, HeySQuAD that consists of 76k human-spoken questions and 97k machine-generated questions and corresponding textual answers derived from the SQuAD QA dataset. The goal of HeySQuAD is to measure the ability of machines to understand noisy spoken questions and answer the questions accurately. To this end, we run extensive benchmarks on the human-spoken and machine-generated questions to quantify the differences in noise from both sources and its subsequent impact on the model and answering accuracy. Importantly, for the task of SQA, where we want to answer human-spoken questions, we observe that training using the transcribed human-spoken and original SQuAD questions leads to significant improvements (12.51%) over training using only the original SQuAD te
    
[^4]: 使用隐式反馈提高问答生成质量

    Using Implicit Feedback to Improve Question Generation. (arXiv:2304.13664v1 [cs.CL])

    [http://arxiv.org/abs/2304.13664](http://arxiv.org/abs/2304.13664)

    本研究提出了一个名为GEN的系统，利用一小组句子/问题对作为输入，通过基于模式的方法生成问题。该系统可以从用户的反馈中学习并提高生成问题的质量。

    

    问答生成(QG)是自然语言处理(NLP)的一个任务，旨在从文本中自动生成问题。许多应用程序可以从自动生成的问题中受益，但通常需要通过选择或编辑这些问题来使其更准确。本研究提出了一个名为GEN的系统，该系统从这些（隐式）反馈中学习。该系统采用基于模式的方法，将一小组句子/问题对作为输入，并创建模式，然后将其应用于新的未见过的句子。每个由用户纠正后生成的问题都作为下一次迭代的新种子使用，因此每次都会创建更多的模式。我们还利用用户所做的更正来评分模式，从而对生成的问题进行排序。实验表明，该系统优于现有的QG模型，可以生成高质量的问题。

    Question Generation (QG) is a task of Natural Language Processing (NLP) that aims at automatically generating questions from text. Many applications can benefit from automatically generated questions, but often it is necessary to curate those questions, either by selecting or editing them. This task is informative on its own, but it is typically done post-generation, and, thus, the effort is wasted. In addition, most existing systems cannot incorporate this feedback back into them easily. In this work, we present a system, GEN, that learns from such (implicit) feedback. Following a pattern-based approach, it takes as input a small set of sentence/question pairs and creates patterns which are then applied to new unseen sentences. Each generated question, after being corrected by the user, is used as a new seed in the next iteration, so more patterns are created each time. We also take advantage of the corrections made by the user to score the patterns and therefore rank the generated qu
    
[^5]: 一种用于知识密集型视觉问答的对称双编码稠密检索框架

    A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering. (arXiv:2304.13649v1 [cs.CV])

    [http://arxiv.org/abs/2304.13649](http://arxiv.org/abs/2304.13649)

    本文提出了一种新的对称双编码稠密检索框架DEDR，以弥合文本和图像之间的空间差距，并进一步引入了MM-FiD，一种多模式融合解码器模型，用于知识密集型视觉问答任务，其效果优于现有最先进方法。

    

    知识密集型视觉问答（KI-VQA）是指回答关于图像的问题，其答案不在图像中。本文提出了一种新的KI-VQA任务流程，包括一个检索器和一个阅读器。首先，我们介绍了DEDR，它是一种对称双编码密集检索框架，其中使用单模（文本）和多模编码器将文档和查询编码为共享嵌入空间。我们引入了一种迭代知识蒸馏方法，以弥合这两个编码器中的表示空间之间的差距。对两个成熟的KI-VQA数据集OK-VQA和FVQA进行广泛的评估表明，DEDR在OK-VQA和FVQA上的性能比最先进的基线方法分别提高了11.6％和30.9％。利用DEDR检索到的段落，我们还进一步介绍了MM-FiD，一种编码器-解码器多模式融合解码器模型，用于为KI-VQA任务生成文本答案。MM-FiD将问题、图像和每个检索到的段落编码为单独的向量，并通过从它们的连接解码生成答案。广泛的实验表明，MM-FiD在OK-VQA和FVQA数据集上均优于最先进的方法。

    Knowledge-Intensive Visual Question Answering (KI-VQA) refers to answering a question about an image whose answer does not lie in the image. This paper presents a new pipeline for KI-VQA tasks, consisting of a retriever and a reader. First, we introduce DEDR, a symmetric dual encoding dense retrieval framework in which documents and queries are encoded into a shared embedding space using uni-modal (textual) and multi-modal encoders. We introduce an iterative knowledge distillation approach that bridges the gap between the representation spaces in these two encoders. Extensive evaluation on two well-established KI-VQA datasets, i.e., OK-VQA and FVQA, suggests that DEDR outperforms state-of-the-art baselines by 11.6% and 30.9% on OK-VQA and FVQA, respectively. Utilizing the passages retrieved by DEDR, we further introduce MM-FiD, an encoder-decoder multi-modal fusion-in-decoder model, for generating a textual answer for KI-VQA tasks. MM-FiD encodes the question, the image, and each retri
    
[^6]: ChartSumm：长短摘要自动生成任务的全面基准数据集

    ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries. (arXiv:2304.13620v1 [cs.CL])

    [http://arxiv.org/abs/2304.13620](http://arxiv.org/abs/2304.13620)

    本文提出了ChartSumm数据集，用于长短摘要自动生成任务，包括84000多个图表及其元数据和描述。研究发现，现有的自动摘要模型虽然得分不错，但经常面临错觉、漏掉重要数据点以及不正确解释复杂趋势等问题。

    

    自动将图表转换为文本摘要是视障人士的有效工具，同时为用户提供表格数据的自然语言精确洞察力。大型、结构良好的数据集始终是数据驱动模型的关键部分。本文提出了ChartSumm：一个大规模基准数据集，包括共84363个图表及其元数据和描述，涵盖广泛的主题和图表类型，可生成长短摘要。强基线模型的广泛实验表明，尽管这些模型通过实现各种自动评估指标的得分来生成流畅且信息丰富的摘要，但它们经常遇到一些问题，例如产生错觉，漏掉重要的数据点，以及不正确地解释图表中的复杂趋势。我们还通过自动翻译工具探讨了将ChartSumm扩展到其他语言的潜力。这使得我们的数据集成为一个有挑战的任务。

    Automatic chart to text summarization is an effective tool for the visually impaired people along with providing precise insights of tabular data in natural language to the user. A large and well-structured dataset is always a key part for data driven models. In this paper, we propose ChartSumm: a large-scale benchmark dataset consisting of a total of 84,363 charts along with their metadata and descriptions covering a wide range of topics and chart types to generate short and long summaries. Extensive experiments with strong baseline models show that even though these models generate fluent and informative summaries by achieving decent scores in various automatic evaluation metrics, they often face issues like suffering from hallucination, missing out important data points, in addition to incorrect explanation of complex trends in the charts. We also investigated the potential of expanding ChartSumm to other languages using automated translation tools. These make our dataset a challeng
    
[^7]: 意思的多义性：通过上下文化语言模型揭示模糊词汇表征的几何学

    Shades of meaning: Uncovering the geometry of ambiguous word representations through contextualised language models. (arXiv:2304.13597v1 [cs.CL])

    [http://arxiv.org/abs/2304.13597](http://arxiv.org/abs/2304.13597)

    本研究通过利用上下文化语言模型的方法，发现这些模型的单词表征捕捉到了无歧义、异义和多义词之间的细微有意义差别，为对词汇歧义的心理学概念化提供了定量支持以及对上下文信息如何影响单词表示提出了新的挑战。

    

    词汇歧义是语言科学中一个深刻而持久的挑战。我们的工作通过一系列模拟研究，利用最近在上下文化语言模型方面的进展，提供了对词汇歧义心理理解的新见解。尽管这些模型本身并没有任何与词语意思相关的理解，它们只是学习基于其他词汇提供的周围环境来预测词语。我们的分析表明，它们的表征捕捉到可以与词典分类和心理理论相一致的，对于无歧义、异义和多义词之间的细微有意义差别。这些发现为现代心理学对词汇歧义的概念化提供了定量支持，并提出了理解上下文信息如何影响单词表示的新挑战。

    Lexical ambiguity presents a profound and enduring challenge to the language sciences. Researchers for decades have grappled with the problem of how language users learn, represent and process words with more than one meaning. Our work offers new insight into psychological understanding of lexical ambiguity through a series of simulations that capitalise on recent advances in contextual language models. These models have no grounded understanding of the meanings of words at all; they simply learn to predict words based on the surrounding context provided by other words. Yet, our analyses show that their representations capture fine-grained meaningful distinctions between unambiguous, homonymous, and polysemous words that align with lexicographic classifications and psychological theorising. These findings provide quantitative support for modern psychological conceptualisations of lexical ambiguity and raise new challenges for understanding of the way that contextual information shapes 
    
[^8]: 毒性评论减少维基百科志愿编辑的活跃程度

    Toxic comments reduce the activity of volunteer editors on Wikipedia. (arXiv:2304.13568v1 [cs.CY])

    [http://arxiv.org/abs/2304.13568](http://arxiv.org/abs/2304.13568)

    本文分析了维基百科六种最活跃的语言版本中8.5万名编辑的用户对话页面上发表的5700万条评论，在短期内发现毒性评论不断降低编辑的活跃程度，并导致每个用户的活跃天数估计减少至少0.5-2天，长期效应更为显著，因为它们显着增加了编辑离开的风险。

    

    维基百科是历史上最成功的协作项目之一，是有史以来最大的百科全书，全球数百万用户依赖它作为信息和事实核查以及深入研究的第一来源。由于维基百科完全依赖其志愿编辑的努力，因此影响到编辑的毒性言论可能特别显着。在本文中，我们分析了六种最活跃的维基百科语言版本中8.5万名编辑的用户对话页面上发表的5700万条评论，以研究毒性对编辑行为的潜在影响。我们发现，毒性评论不断降低编辑的活跃程度，导致每个用户在短期内的活跃天数估计损失0.5-2天。考虑到维基百科的活跃贡献者数量，这相当于多年的人力成果损失。毒性评论的影响在长期更为显著，因为它们显着增加了编辑离开的风险。

    Wikipedia is one of the most successful collaborative projects in history. It is the largest encyclopedia ever created, with millions of users worldwide relying on it as the first source of information as well as for fact-checking and in-depth research. As Wikipedia relies solely on the efforts of its volunteer-editors, its success might be particularly affected by toxic speech. In this paper, we analyze all 57 million comments made on user talk pages of 8.5 million editors across the six most active language editions of Wikipedia to study the potential impact of toxicity on editors' behaviour. We find that toxic comments consistently reduce the activity of editors, leading to an estimated loss of 0.5-2 active days per user in the short term. This amounts to multiple human-years of lost productivity when considering the number of active contributors to Wikipedia. The effects of toxic comments are even greater in the long term, as they significantly increase the risk of editors leaving 
    
[^9]: 位置偏差对token分类中的语言模型的影响

    Impact of Position Bias on Language Models in Token Classification. (arXiv:2304.13567v1 [cs.CL])

    [http://arxiv.org/abs/2304.13567](http://arxiv.org/abs/2304.13567)

    研究了语言模型在token分类任务中的位置偏差问题，通过实验表明在具体任务中，BERT、ERNIE、ELECTRA等编码器以及GPT2和BLOOM等解码器的平均性能下降了3%和9%。

    

    语言模型在自然语言处理任务中表现出了最先进的性能。命名实体识别(NER)或词性标注等下游任务已知存在数据不平衡问题，特别是在正负示例的比例和类不平衡方面。本文研究了语言模型的另一个特定问题，即token分类任务中正示例的位置偏差。因此，我们对基于Token分类基准测试的语言模型的性能进行了深入的位置偏差评估。我们的研究包括CoNLL03和OntoNote5.0用于NER，English Tree Bank UD_en和TweeBank用于POS标记。我们提出了一种评估方法，以研究Transformer模型中的位置偏差。我们发现像BERT、ERNIE、ELECTRA这样的编码器和像GPT2 和BLOOM这样的解码器平均性能下降了3%和9%。

    Language Models (LMs) have shown state-of-the-art performance in Natural Language Processing (NLP) tasks. Downstream tasks such as Named Entity Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data imbalance issues, specifically in terms of the ratio of positive to negative examples, and class imbalance. In this paper, we investigate an additional specific issue for language models, namely the position bias of positive examples in token classification tasks. Therefore, we conduct an in-depth evaluation of the impact of position bias on the performance of LMs when fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We propose an evaluation approach to investigate position bias in Transformer models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as GPT2 and BLOOM can suffer from this bias with an average drop of 3\% and 9\% in their performan
    
[^10]: 实现无缝查询文本和表格的多模态数据库管理系统

    Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables. (arXiv:2304.13559v1 [cs.DB])

    [http://arxiv.org/abs/2304.13559](http://arxiv.org/abs/2304.13559)

    提出了一种新型数据库系统MMDB，它可以无缝查询文本和表格数据，通过使用所谓的多模态运算符（MMOps）实现文本集合的转换，该方法不仅优于现有的最先进方法，在对未见过的数据进行模型微调时，也具有显著的优势。

    

    本文提出了一种新型数据库系统，即多模态数据库（MMDBs），它可以使用SQL语言无缝地查询文本和表格数据。为了实现在MMDB中使用SQL语言进行文本数据的无缝查询，我们提出了所谓的多模态运算符（MMOps），它们基于最近大型语言模型（例如GPT-3）的进展。MMOps的主要思想是，它们允许将文本集合作为表格进行处理，而无需手动转换数据。正如我们在评估中展示的那样，我们的MMDB原型不仅在准确性和性能方面优于最先进的方法（如文本转表），而且在对一个未见过的文本集合进行模型微调时，需要的训练数据数量也显著减少。

    In this paper, we propose Multi-Modal Databases (MMDBs), which is a new class of database systems that can seamlessly query text and tables using SQL. To enable seamless querying of textual data using SQL in an MMDB, we propose to extend relational databases with so-called multi-modal operators (MMOps) which are based on the advances of recent large language models such as GPT-3. The main idea of MMOps is that they allow text collections to be treated as tables without the need to manually transform the data. As we show in our evaluation, our MMDB prototype can not only outperform state-of-the-art approaches such as text-to-table in terms of accuracy and performance but it also requires significantly less training data to fine-tune the model for an unseen text collection.
    
[^11]: 在众包数据集中“我”迷失在翻译中：代词错误步骤的问题

    "I'm" Lost in Translation: Pronoun Missteps in Crowdsourced Data Sets. (arXiv:2304.13557v1 [cs.CL])

    [http://arxiv.org/abs/2304.13557](http://arxiv.org/abs/2304.13557)

    这项研究发现在英语和日语之间的翻译中存在男性代词偏见，同时也检测到了对女性、中性和/或非二元代词存在的微妙反应的偏见。他们提出了针对代词翻译偏见的问题，并提供了将复数嵌入NLP数据集的解决方案。

    

    随着虚拟助手在全球范围内的普及，越来越需要这些语音系统以各种语言自然地进行交流。众包倡议已经专注于对大型开放数据集进行多语言翻译，以用于自然语言处理（NLP）。然而，语言翻译通常不是一对一的，并且偏见可能会逐渐渗入。在这项最新工作中，我们关注了在众包Tatoeba数据库中英语和日语之间翻译的代词问题。我们发现整体上存在男性代词偏见，即使在其他方式中考虑到语言的复数。重要的是，我们检测到翻译过程中反映了对女性、中性和/或非二元代词存在的微妙反应的偏见。我们提出了代词翻译中的偏见问题，并提供了将复数嵌入NLP数据集的实际解决方案。

    As virtual assistants continue to be taken up globally, there is an ever-greater need for these speech-based systems to communicate naturally in a variety of languages. Crowdsourcing initiatives have focused on multilingual translation of big, open data sets for use in natural language processing (NLP). Yet, language translation is often not one-to-one, and biases can trickle in. In this late-breaking work, we focus on the case of pronouns translated between English and Japanese in the crowdsourced Tatoeba database. We found that masculine pronoun biases were present overall, even though plurality in language was accounted for in other ways. Importantly, we detected biases in the translation process that reflect nuanced reactions to the presence of feminine, neutral, and/or non-binary pronouns. We raise the issue of translation bias for pronouns and offer a practical solution to embed plurality in NLP data sets.
    
[^12]: 使用ChatGPT多维评估文本风格转换

    Multidimensional Evaluation for Text Style Transfer Using ChatGPT. (arXiv:2304.13462v1 [cs.CL])

    [http://arxiv.org/abs/2304.13462](http://arxiv.org/abs/2304.13462)

    本研究探究使用ChatGPT作为多维评估器，测试其在文本风格转换中的性能，并与现有的自动度量标准和人类判断进行比较。与自动度量标准相比，ChatGPT在与人类判断的相关性上具有竞争力。

    

    我们研究了使用ChatGPT作为一种多维评估器来评估文本风格转换的潜力，与现有的自动度量标准和人类判断相比。我们专注于零-shot设置，即使用特定的任务指令提示ChatGPT，在风格强度、内容保留和流畅度三个常用的文本风格转换评估维度上测试其性能。我们在不同级别上进行了全面的相关分析，比较了两个转换方向（以及总体）的结果。与现有的自动度量标准相比，ChatGPT在与人类判断的相关性上具有竞争力。这些初步结果有望首次揭示大语言模型在彩色文本生成的多维评估中的作用。

    We investigate the potential of ChatGPT as a multidimensional evaluator for the task of \emph{Text Style Transfer}, alongside, and in comparison to, existing automatic metrics as well as human judgements. We focus on a zero-shot setting, i.e. prompting ChatGPT with specific task instructions, and test its performance on three commonly-used dimensions of text style transfer evaluation: style strength, content preservation, and fluency. We perform a comprehensive correlation analysis for two transfer directions (and overall) at different levels. Compared to existing automatic metrics, ChatGPT achieves competitive correlations with human judgments. These preliminary results are expected to provide a first glimpse into the role of large language models in the multidimensional evaluation of stylized text generation.
    
[^13]: 自控内存系统释放大规模语言模型的无限输入容量

    Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System. (arXiv:2304.13343v1 [cs.CL])

    [http://arxiv.org/abs/2304.13343](http://arxiv.org/abs/2304.13343)

    该论文提出了一种自控内存系统，可以使大规模语言模型能够处理任意长度的输入，从而显著提高模型的性能表现。

    

    大规模语言模型（LLMs）受制于无法处理过长的输入。为了解决这个问题，我们提出了自控内存（SCM）系统，以释放大规模语言模型的无限输入容量。我们的SCM系统由三个关键模块组成：语言模型代理、内存流和内存控制器。语言模型代理迭代地处理超长输入，并将所有历史信息存储在内存流中。内存控制器为代理提供长期存储器（归档存储器）和短期存储器（闪存），以生成精确连贯的响应。控制器确定应激活哪些来自归档存储器的记忆，并如何将它们合并到模型输入中。我们的SCM系统可以与任何LLMs集成，以使它们能够处理超长文本而无需修改或微调。实验结果表明，我们的SCM系统使得LLMs能够处理长度高达8192个令牌的输入，实现了在多个基准数据集上的最佳表现，证明了它在提高大规模语言模型性能方面的有效性。

    Large-scale Language Models (LLMs) are constrained by their inability to process lengthy inputs. To address this limitation, we propose the Self-Controlled Memory (SCM) system to unleash infinite-length input capacity for large-scale language models. Our SCM system is composed of three key modules: the language model agent, the memory stream, and the memory controller. The language model agent iteratively processes ultra-long inputs and stores all historical information in the memory stream. The memory controller provides the agent with both long-term memory (archived memory) and short-term memory (flash memory) to generate precise and coherent responses. The controller determines which memories from archived memory should be activated and how to incorporate them into the model input. Our SCM system can be integrated with any LLMs to enable them to process ultra-long texts without any modification or fine-tuning. Experimental results show that our SCM system enables LLMs, which are not
    
[^14]: 数据语言的名义拓扑

    Nominal Topology for Data Languages. (arXiv:2304.13337v1 [cs.CL])

    [http://arxiv.org/abs/2304.13337](http://arxiv.org/abs/2304.13337)

    该论文提出了一种新的拓扑视角，用于描述可以被轨道有限的名义单子群识别的数据语言，并探讨了 pro-轨道有限方程的表现能力。

    

    我们提出了一种新的拓扑视角，用于描述可以被轨道有限的名义单子群识别的数据语言。为此，我们引入了 pro- 轨道有限的名义拓扑空间。在全局有界支持大小的前提下，它们与名义 Stone 空间重合，并且被证明与名义布尔代数的一个子范畴双重等价。可识别的数据语言被表征为 pro-轨道有限单词的拓扑闭开集。此外，我们通过建立 Reiterman 的伪变种定理的名义版本，探讨了 pro-轨道有限方程的表现能力。

    We propose a novel topological perspective on data languages recognizable by orbit-finite nominal monoids. For this purpose, we introduce pro-orbit-finite nominal topological spaces. Assuming globally bounded support sizes, they coincide with nominal Stone spaces and are shown to be dually equivalent to a subcategory of nominal boolean algebras. Recognizable data languages are characterized as topologically clopen sets of pro-orbit-finite words. In addition, we explore the expressive power of pro-orbit-finite equations by establishing a nominal version of Reiterman's pseudovariety theorem.
    
[^15]: 跨域文本到SQL自适应提示的基于案例推理框架

    A Case-Based Reasoning Framework for Adaptive Prompting in Cross-Domain Text-to-SQL. (arXiv:2304.13301v1 [cs.CL])

    [http://arxiv.org/abs/2304.13301](http://arxiv.org/abs/2304.13301)

    本文提出了一个基于案例推理框架的跨域文本到SQL自适应提示的解决方案，可以精确控制与案例相关和不相关的知识，解决了大型语言模型提示设计不良限制性能的问题。

    

    最近流行的大型语言模型（例如Codex、ChatGPT和GPT-4）在AI社区方面有了显著的进展，包括文本到SQL的任务。一些关于大型语言模型的评估和分析表明，它们有潜力生成SQL查询，但是它们所使用的提示设计不良（例如简单的结构或随机抽样）限制了大型语言模型的性能，并可能导致不必要或无关的输出。为了解决这些问题，我们提出了CBR-ApSQL，这是一个基于案例推理（CBR）的框架，与GPT-3.5相结合，用于在文本到SQL任务中对与案例相关和不相关的知识进行精确控制。我们设计了自适应提示，以灵活调整GPT-3.5的输入，其中涉及（1）通过去语义化输入问题来自适应检索案例，根据问题意图，以及（2）自适应回退机制，以确保提示的信息量和案例与提示之间的相关性。在去语义化阶段中，我们设计了Semantic D

    Recent advancements in Large Language Models (LLMs), such as Codex, ChatGPT and GPT-4 have significantly impacted the AI community, including Text-to-SQL tasks. Some evaluations and analyses on LLMs show their potential to generate SQL queries but they point out poorly designed prompts (e.g. simplistic construction or random sampling) limit LLMs' performance and may cause unnecessary or irrelevant outputs. To address these issues, we propose CBR-ApSQL, a Case-Based Reasoning (CBR)-based framework combined with GPT-3.5 for precise control over case-relevant and case-irrelevant knowledge in Text-to-SQL tasks. We design adaptive prompts for flexibly adjusting inputs for GPT-3.5, which involves (1) adaptively retrieving cases according to the question intention by de-semantizing the input question, and (2) an adaptive fallback mechanism to ensure the informativeness of the prompt, as well as the relevance between cases and the prompt. In the de-semanticization phase, we designed Semantic D
    
[^16]: 从关联到生成：无监督跨模态映射的纯文本字幕生成

    From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping. (arXiv:2304.13273v1 [cs.CV])

    [http://arxiv.org/abs/2304.13273](http://arxiv.org/abs/2304.13273)

    本研究提出了一种从关联到生成的零-shot方法：通过将图像/视频投影到语言模态并在生成任务中生成描述性字幕。该方法在多个基准数据集上显著优于现有的最先进方法，为无监督跨模态映射提供了一个新的视角，并具有在视频字幕，图像合成和文本到图像生成等领域的潜在应用。

    

    随着以CLIP和ALIGN为代表的视觉-语言预训练模型的发展，CLIP的零-shot能力在图像分类和图像-文本检索等基于关联的视觉任务中取得了重大突破。但是，CLIP难以应用于基于生成的任务。这是由于缺乏解码器架构和生成的预训练任务。我们提出了K最近邻跨模态映射（Knight），一种从关联到生成的零-shot方法。通过窄字幕任务的纯文本无监督预训练来有效地将图像/视频投影到语言模态并在生成任务中生成描述性字幕。实验结果表明，Knight在多个基准数据集上显著优于现有的最先进方法。我们的方法为无监督跨模态映射提供了一个新的视角，并且将在视频字幕，图像合成和文本到图像生成等领域具有潜在应用。

    With the development of Vision-Language Pre-training Models (VLPMs) represented by CLIP and ALIGN, significant breakthroughs have been achieved for association-based visual tasks such as image classification and image-text retrieval by the zero-shot capability of CLIP without fine-tuning. However, CLIP is hard to apply to generation-based tasks. This is due to the lack of decoder architecture and pre-training tasks for generation. Although previous works have created generation capacity for CLIP through additional language models, a modality gap between the CLIP representations of different modalities and the inability of CLIP to model the offset of this gap, which fails the concept to transfer across modalities. To solve the problem, we try to map images/videos to the language modality and generate captions from the language modality. In this paper, we propose the K-nearest-neighbor Cross-modality Mapping (Knight), a zero-shot method from association to generation. With text-only unsu
    
[^17]: 探究代码提示的神奇案例

    Exploring the Curious Case of Code Prompts. (arXiv:2304.13250v1 [cs.CL])

    [http://arxiv.org/abs/2304.13250](http://arxiv.org/abs/2304.13250)

    研究比较了代码提示和文本提示在多种自然语言任务上的性能表现，发现除了少数任务外，代码提示并不总是比文本提示更好，代码提示风格对性能有一定影响，而微调可以提高代码提示的性能。

    

    最近的研究表明，在自然语言的代码表示上提示语言模型可以提高结构化推理任务的性能。然而，这些任务仅构成所有自然语言任务的一小部分。在我们的工作中，我们将回答是否代码提示是一般与语言模型互动的首选方式。我们在三种流行的 GPT 模型 (davinci, code-davinci-002 和 text-davinci-002) 上比较代码提示和文本提示在更广泛的任务选择上 (如 QA, 情感分析, 摘要等) 的性能，并发现除了少数例外，代码提示并不总是比文本提示更好。此外，我们还表明，代码提示风格在某些任务上对性能有很大影响，但并非所有任务都会受到影响，对文本说明进行微调会导致代码提示相对性能更好。

    Recent work has shown that prompting language models with code-like representations of natural language leads to performance improvements on structured reasoning tasks. However, such tasks comprise only a small subset of all natural language tasks. In our work, we seek to answer whether or not code-prompting is the preferred way of interacting with language models in general. We compare code and text prompts across three popular GPT models (davinci, code-davinci-002, and text-davinci-002) on a broader selection of tasks (e.g., QA, sentiment, summarization) and find that with few exceptions, code prompts do not consistently outperform text prompts. Furthermore, we show that the style of code prompt has a large effect on performance for some but not all tasks and that fine-tuning on text instructions leads to better relative performance of code prompts.
    
[^18]: 面向心理健康的可解释和安全的会话型智能助手：一项调查

    Towards Explainable and Safe Conversational Agents for Mental Health: A Survey. (arXiv:2304.13191v1 [cs.AI])

    [http://arxiv.org/abs/2304.13191](http://arxiv.org/abs/2304.13191)

    这篇论文调查了现有的心理健康会话型智能助手，提出了改进的新见解，并介绍了如何构建责任VMHA，以提出后续问题或提供知情回应，丰富用户体验。

    

    虚拟心理健康助手（VMHA）在持续推进，以支持每年6000万次初级医疗保健就诊和600万次急诊室就诊的负担过重的全球医疗保健系统。这些系统是由临床心理学家、精神科医师和人工智能（AI）研究人员为认知行为疗法（CBT）构建的。目前，VMHA的作用是通过信息提供情感支持，重点不在与患者进行深入的反思对话。需要更全面、安全和可解释的方法来构建负责任的VMHA，以提出后续问题或提供知情回应。这项调查对现有的心理健康会话型智能助手进行了系统的批判性审查，随后提出了关于VMHA改进的新见解，包括环境知识、数据集和它们在临床决策支持中的新兴角色。我们还提供了丰富用户体验的新方向。

    Virtual Mental Health Assistants (VMHAs) are seeing continual advancements to support the overburdened global healthcare system that gets 60 million primary care visits, and 6 million Emergency Room (ER) visits annually. These systems are built by clinical psychologists, psychiatrists, and Artificial Intelligence (AI) researchers for Cognitive Behavioral Therapy (CBT). At present, the role of VMHAs is to provide emotional support through information, focusing less on developing a reflective conversation with the patient. A more comprehensive, safe and explainable approach is required to build responsible VMHAs to ask follow-up questions or provide a well-informed response. This survey offers a systematic critical review of the existing conversational agents in mental health, followed by new insights into the improvements of VMHAs with contextual knowledge, datasets, and their emerging role in clinical decision support. We also provide new directions toward enriching the user experience
    
[^19]: TABLET：基于指令学习表格数据

    TABLET: Learning From Instructions For Tabular Data. (arXiv:2304.13188v1 [cs.LG])

    [http://arxiv.org/abs/2304.13188](http://arxiv.org/abs/2304.13188)

    该论文提出了TABLET，这是一个由20个不同的包含指令注释的表格数据集组成的基准测试，可以提高大型语言模型在表格预测问题上的效果，并评估了指令在保真度和LLM在表格预测方面的局限性。

    

    在训练表格预测的机器学习模型时，获取高质量的数据通常是一项重大挑战，特别是在隐私敏感和成本高的领域，比如医学和金融。向大型语言模型（LLM）提供自然语言指令提供了另一种解决方案。然而，指令如何有效地利用LLM中的知识来解决表格预测问题仍不清楚。为了弥补这一差距，我们介绍了TABLET，这是一个由20个不同的包含指令注释的表格数据集组成的基准测试，这些指令在措辞、细节和技术性方面各不相同。此外，TABLET还包括指令的逻辑和结构修改。我们发现，在上下文指令的帮助下，Flan-T5 11b的零示例F1性能平均提高了44％，在TABLET上，ChatGPT的提升为13％。此外，我们评估了指令保真度，探讨了使用LLM进行表格预测的局限性。我们发现LLM通常会忽略指令并依赖于数据中存在的偏差。

    Acquiring high-quality data is often a significant challenge in training machine learning (ML) models for tabular prediction, particularly in privacy-sensitive and costly domains like medicine and finance. Providing natural language instructions to large language models (LLMs) offers an alternative solution. However, it is unclear how effectively instructions leverage the knowledge in LLMs for solving tabular prediction problems. To address this gap, we introduce TABLET, a benchmark of 20 diverse tabular datasets annotated with instructions that vary in their phrasing, granularity, and technicality. Additionally, TABLET includes the instructions' logic and structured modifications to the instructions. We find in-context instructions increase zero-shot F1 performance for Flan-T5 11b by 44% on average and 13% for ChatGPT on TABLET. Also, we explore the limitations of using LLMs for tabular prediction in our benchmark by evaluating instruction faithfulness. We find LLMs often ignore instr
    
[^20]: Sebis在SemEval-2023任务7中：临床试验报告中自然语言推理和证据检索的联合系统

    Sebis at SemEval-2023 Task 7: A Joint System for Natural Language Inference and Evidence Retrieval from Clinical Trial Reports. (arXiv:2304.13180v1 [cs.CL])

    [http://arxiv.org/abs/2304.13180](http://arxiv.org/abs/2304.13180)

    本文描述了两个NLP系统：一个为自然语言推理，一个为临床试验数据证据检索。它们分别采用了流水线模型和联合模型，并在最终的集成系统中融合输出。

    

    随着每天产生的临床试验报告数量增加，跟进告知基于证据的医疗建议的新发现变得越来越难。为了帮助自动化这一过程并协助医疗专家，正在开发NLP解决方案。这激发了SemEval-2023任务7，该任务的目标是开发一个NLP系统，以处理从临床试验数据中提取证据和进行自然语言推理的两个任务。本文介绍我们开发的两个系统。第一个是流水线系统，单独建模了这两个任务，而第二个是联合系统，采用共享表示和多任务学习方法同时学习这两个任务。最终系统将它们的输出合并为一个集成系统。我们规范化模型，介绍其特点和挑战，并对实现的结果进行分析。

    With the increasing number of clinical trial reports generated every day, it is becoming hard to keep up with novel discoveries that inform evidence-based healthcare recommendations. To help automate this process and assist medical experts, NLP solutions are being developed. This motivated the SemEval-2023 Task 7, where the goal was to develop an NLP system for two tasks: evidence retrieval and natural language inference from clinical trial data. In this paper, we describe our two developed systems. The first one is a pipeline system that models the two tasks separately, while the second one is a joint system that learns the two tasks simultaneously with a shared representation and a multi-task learning approach. The final system combines their outputs in an ensemble system. We formalize the models, present their characteristics and challenges, and provide an analysis of achieved results.
    
[^21]: 虚拟助手中口语信息查询的建模：未解决的问题，挑战和机遇

    Modeling Spoken Information Queries for Virtual Assistants: Open Problems, Challenges and Opportunities. (arXiv:2304.13149v1 [cs.IR])

    [http://arxiv.org/abs/2304.13149](http://arxiv.org/abs/2304.13149)

    讨论了虚拟助手中口语信息查询建模的问题和挑战，以及信息检索方法中的机遇；探讨了如何通过查询领域分类、知识图谱等提高语音识别的准确性；简要概述了语音识别中的挑战。

    

    虚拟助手正在成为越来越重要的基于语音的信息检索平台，它们可以帮助用户完成各种任务。本论文讨论了关于虚拟助手口语信息查询建模的未解决的问题和挑战，并列出了信息检索方法和研究可以应用于提高虚拟助手语音识别质量的机会。我们讨论了如何通过查询领域分类、知识图谱和用户交互数据以及查询个性化来帮助改善口语信息领域查询的准确识别。最后，我们还简要概述了语音识别中当前存在的问题和挑战。

    Virtual assistants are becoming increasingly important speech-driven Information Retrieval platforms that assist users with various tasks.  We discuss open problems and challenges with respect to modeling spoken information queries for virtual assistants, and list opportunities where Information Retrieval methods and research can be applied to improve the quality of virtual assistant speech recognition.  We discuss how query domain classification, knowledge graphs and user interaction data, and query personalization can be helpful to improve the accurate recognition of spoken information domain queries. Finally, we also provide a brief overview of current problems and challenges in speech recognition.
    
[^22]: ESimCSE无监督对比学习联合UDA半监督学习用于大标签系统文本分类模型

    ESimCSE Unsupervised Contrastive Learning Jointly with UDA Semi-Supervised Learning for Large Label System Text Classification Mode. (arXiv:2304.13140v1 [cs.LG])

    [http://arxiv.org/abs/2304.13140](http://arxiv.org/abs/2304.13140)

    本文提出ESimCSE无监督比较学习和UDA半监督比较学习模型相结合，通过联合训练技术解决了大标签系统文本分类的多个问题，并在公共数据集上实现了准确率提高。

    

    在自然语言处理任务中，文本分类面临的挑战包括多个标签系统、数据分布不均匀和高噪声。为了解决这些问题，本文通过使用联合训练技术，将ESimCSE无监督比较学习和UDA半监督比较学习模型相结合。ESimCSE模型利用无标签数据高效地学习文本向量表示，从而实现更好的分类结果；而UDA则通过半监督学习方法使用无标签数据进行训练，提高模型的预测性能和稳定性，并进一步提高模型的泛化能力。此外，模型训练过程中采用了对抗训练技术FGM和PGD，以提高模型的鲁棒性和可靠性。实验结果表明，与基线模型相比，在公共数据集Ruesters上有8%和10%的准确率提高。

    The challenges faced by text classification with large tag systems in natural language processing tasks include multiple tag systems, uneven data distribution, and high noise. To address these problems, the ESimCSE unsupervised comparative learning and UDA semi-supervised comparative learning models are combined through the use of joint training techniques in the models.The ESimCSE model efficiently learns text vector representations using unlabeled data to achieve better classification results, while UDA is trained using unlabeled data through semi-supervised learning methods to improve the prediction performance of the models and stability, and further improve the generalization ability of the model. In addition, adversarial training techniques FGM and PGD are used in the model training process to improve the robustness and reliability of the model. The experimental results show that there is an 8% and 10% accuracy improvement relative to Baseline on the public dataset Ruesters as we
    
[^23]: LAST: 基于JAX的可扩展格模型语音建模

    LAST: Scalable Lattice-Based Speech Modelling in JAX. (arXiv:2304.13134v1 [cs.CL])

    [http://arxiv.org/abs/2304.13134](http://arxiv.org/abs/2304.13134)

    LAST是一个基于JAX的库，用于实现灵活易用的格模型语音转录器。它采用了一系列适用于解决现代架构性能特征和自动微分中的细微差别新挑战的通用技术，并在TPUv3和V100 GPU上得到了验证。

    

    我们引入了一个名为LAST的库，它是基于格模型的语音转录器。LAST强调了灵活性、易用性和可扩展性，并实现了训练和推断所需的可微加权有限状态自动机（WFSA）算法，这些算法可以扩展到整个话语的识别格上。尽管这些WFSA算法在文献中已经被广泛应用，但是现代架构的性能特征和自动微分中的细微差别新的挑战已经出现。我们描述了用于解决这些挑战的一系列通用技术，并通过TPUv3和V100 GPU上的基准测试展示了它们的有效性。

    We introduce LAST, a LAttice-based Speech Transducer library in JAX. With an emphasis on flexibility, ease-of-use, and scalability, LAST implements differentiable weighted finite state automaton (WFSA) algorithms needed for training \& inference that scale to a large WFSA such as a recognition lattice over the entire utterance. Despite these WFSA algorithms being well-known in the literature, new challenges arise from performance characteristics of modern architectures, and from nuances in automatic differentiation. We describe a suite of generally applicable techniques employed in LAST to address these challenges, and demonstrate their effectiveness with benchmarks on TPUv3 and V100 GPU.
    
[^24]: 只用结构先预训练：利用迁移学习理解语言归纳偏置

    Pretrain on just structure: Understanding linguistic inductive biases using transfer learning. (arXiv:2304.13060v1 [cs.CL])

    [http://arxiv.org/abs/2304.13060](http://arxiv.org/abs/2304.13060)

    通过在人造结构数据上进行预先训练和在英语上微调，我们研究了自然语言处理中三种归纳偏置类型：递归的层级处理、无限制的标记-标记依赖以及基于Zipfian幂律词汇分布的归纳偏置，我们得出复杂标记-标记交互形成了最好的归纳偏置的结论。

    

    无论是人类还是变压器语言模型都能在没有明确的结构监督下学习语言。什么样的归纳式学习偏置使得这种学习成为可能？在这项研究中，我们通过在人造结构数据上预先训练并在英语上微调来采用不同的归纳式学习偏置对语言模型进行偏置。我们的实验设置使我们能够积极控制语言模型的归纳偏置。通过我们的实验，我们研究了三种归纳偏置的比较成功:1)递归的层级处理的归纳偏置2)不受限的标记-标记依赖，这些依赖关系不能由上下文无关文法建模3)Zipfian幂律词汇分布的归纳偏置。我们表明，复杂的标记-标记交互形成了最好的归纳偏置，并且这在非上下文无关情况下最为强烈。我们还表明，Z(targetEntity)分布在英语上也是合适的预先训练分布。

    Both humans and transformer language models are able to learn language without explicit structural supervision. What inductive learning biases make this learning possible? In this study, we examine the effect of different inductive learning biases by predisposing language models with structural biases through pretraining on artificial structured data, and then evaluating by fine-tuning on English. Our experimental setup gives us the ability to actively control the inductive bias of language models. With our experiments, we investigate the comparative success of three types of inductive bias: 1) an inductive bias for recursive, hierarchical processing 2) an inductive bias for unrestricted token-token dependencies that can't be modeled by context-free grammars, and 3) an inductive bias for a Zipfian power-law vocabulary distribution. We show that complex token-token interactions form the best inductive biases, and that this is strongest in the non-context-free case. We also show that a Z
    
[^25]: ThreatCrawl：基于BERT的网络安全焦点爬虫

    ThreatCrawl: A BERT-based Focused Crawler for the Cybersecurity Domain. (arXiv:2304.11960v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2304.11960](http://arxiv.org/abs/2304.11960)

    本文提出了一种基于BERT的焦点爬虫ThreatCrawl，使用主题建模和关键词提取技术来筛选出最可能包含有价值CTI信息的网页。

    

    可公开获取的信息对于网络威胁情报（CTI）来说包含有价值的信息。这可以用于预防已经在其他系统上发生的攻击。但是，虽然有不同的标准来交流这些信息，但很多信息是以非标准化的方式在文章或博客帖子中共享的。手动浏览多个在线门户和新闻页面以发现新威胁并提取它们是一项耗时的任务。为了自动化这个扫描过程的一部分，多篇论文提出了使用自然语言处理（NLP）从文档中提取威胁指示器（IOCs）的提取器。然而，虽然这已经解决了从文档中提取信息的问题，但很少考虑搜索这些文档。本文提出了一种新的焦点爬虫ThreatCrawl，它使用双向编码器表示（BERT）搜索网络安全领域中的相关文档。ThreatCrawl使用主题建模和关键词提取技术来识别相关网站和网页，然后应用基于BERT的分类器来优先考虑最可能包含有价值CTI信息的网页。

    Publicly available information contains valuable information for Cyber Threat Intelligence (CTI). This can be used to prevent attacks that have already taken place on other systems. Ideally, only the initial attack succeeds and all subsequent ones are detected and stopped. But while there are different standards to exchange this information, a lot of it is shared in articles or blog posts in non-standardized ways. Manually scanning through multiple online portals and news pages to discover new threats and extracting them is a time-consuming task. To automize parts of this scanning process, multiple papers propose extractors that use Natural Language Processing (NLP) to extract Indicators of Compromise (IOCs) from documents. However, while this already solves the problem of extracting the information out of documents, the search for these documents is rarely considered. In this paper, a new focused crawler is proposed called ThreatCrawl, which uses Bidirectional Encoder Representations 
    
[^26]: 通过提示提高大型语言模型的心智理论表现

    Boosting Theory-of-Mind Performance in Large Language Models via Prompting. (arXiv:2304.11490v1 [cs.AI])

    [http://arxiv.org/abs/2304.11490](http://arxiv.org/abs/2304.11490)

    本研究通过提示提高大型语言模型（LLMs）在心智理论（ToM）任务上的表现，证明了上下文学习可以提升LLMs在复杂推理特别是ToM任务中的表现。

    

    2023年，大型语言模型（LLMs）在许多任务中表现出色，但在复杂推理方面仍面临挑战。心智理论（ToM）任务需要理解代理人的信念、目标和心理状态，对于涉及人类的常识推理至关重要，因此提高LLM在这方面的表现至关重要。本研究测量了GPT-4和三个GPT-3.5变体（Davinci-2、Davinci-3、GPT-3.5-Turbo）的ToM表现，并研究了上下文学习提高它们的ToM理解力的有效性。我们评估了包含两步思维推理和逐步思考说明的提示。我们发现，通过人类反馈的强化学习（RLHF）训练的LLMs（除Davinci-2外的所有模型）通过上下文学习提高了它们的ToM准确性。GPT-4在零轮情况下表现最佳，达到了近80%的ToM准确性，但仍不足测试集上87%的人类准确性。然而，当提供上下文学习的提示时，GPT-4和三个GPT-3.5变体的ToM准确性显著高于无提示时，其中表现最好的模型（GPT-3.5-Turbo）达到了92%的准确性。我们的研究展示了上下文学习提升LLM在复杂推理尤其是ToM任务中表现的潜力。

    Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-c
    
[^27]: GPT-NER：基于大型语言模型的命名实体识别

    GPT-NER: Named Entity Recognition via Large Language Models. (arXiv:2304.10428v1 [cs.CL])

    [http://arxiv.org/abs/2304.10428](http://arxiv.org/abs/2304.10428)

    本文提出了GPT-NER来解决大型语言模型在命名实体识别任务（NER）上表现不佳的问题，它通过将序列标记任务转化为生成任务，将LLM能够容易地适应NER任务。同时，为了有效解决LLMs“幻觉”问题，作者们提出了自我验证策略，通过提示LLMs询问自身来确定提取的实体是否属于实际存在的实体。

    

    尽管大规模语言模型（LLM）在各种NLP任务上已经实现了最先进的性能，但其NER性能仍然明显低于监督基线。这是由于命名实体识别（NER）和LLMs之间的差距：前者在本质上是序列标记任务，后者是一种文本生成模型。在本文中，我们提出了GPT-NER来解决这个问题。 GPT-NER通过将序列标记任务转换为生成任务来弥合差距，LLMs可以轻松适应。例如，将在输入文本“哥伦布是一座城市”中查找位置实体的任务转换为生成文本序列“@@哥伦布##是一座城市”，其中特殊标记@@##标记要提取的实体。为了有效解决LLMs“幻觉”问题，即LLMs有很强的倾向将空输入过度自信地标记为实体，我们提出了自我验证策略，通过提示LLMs询问自身来确定提取的实体是否属于实际存在的实体。

    Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model.  In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text "Columbus is a city" is transformed to generate the text sequence "@@Columbus## is a city", where special tokens @@## marks the entity to extract. To efficiently address the "hallucination" issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a lab
    
[^28]: LLM作为机器人的大脑：统一自我中心记忆与控制

    LLM as A Robotic Brain: Unifying Egocentric Memory and Control. (arXiv:2304.09349v1 [cs.AI])

    [http://arxiv.org/abs/2304.09349](http://arxiv.org/abs/2304.09349)

    本文提出了一个统一自我中心记忆和控制的框架LLM-Brain，使用大规模语言模型作为机器人大脑进行零-shot学习。该框架包括封闭式多轮对话，覆盖了感知、规划、控制和记忆，具有很好的泛化性能，适用于多个机器人任务。

    

    体感人工智能研究和开发具备物理或虚拟实体（即机器人）并能够与环境动态交互的智能系统。记忆和控制是体感系统的两个基本部分，通常需要分别使用框架进行建模。本文提出了一个新的、可推广的框架，称为LLM-Brain：使用大规模语言模型作为机器人大脑，统一自我中心记忆和控制。LLM-Brain框架集成了多个多模态语言模型用于机器人任务，利用零-shot学习方法。LLM-Brain中的所有组件使用自然语言进行封闭式多轮对话，包括感知、规划、控制和记忆。系统的核心是一个具备自我中心记忆和控制机器人的实体LLM。我们通过研究两个下游任务：主动探索和实体问答来演示LLM-Brain。

    Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The
    
[^29]: 论盗用语言模型解码算法的风险

    On the Risks of Stealing the Decoding Algorithms of Language Models. (arXiv:2303.04729v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04729](http://arxiv.org/abs/2303.04729)

    这项工作首次展示，一个拥有典型API访问权限的对手可以以极低的金钱成本窃取GPT-2和GPT-3等LM的解码算法的类型和超参数。

    

    现代语言模型（LM）生成文本的关键组成部分是选择和调整解码算法。这些算法确定如何从LM生成的内部概率分布中生成文本。选择解码算法并调整其超参数的过程需要显著的时间、手动工作和计算，还需要进行广泛的人类评估。因此，解码算法的身份和超参数被认为是极其有价值的。在这项工作中，我们首次展示了一个拥有典型API访问权限的对手可以以极低的金钱成本窃取其解码算法的类型和超参数。我们的攻击对用于文本生成API的流行LM有效，包括GPT-2和GPT-3。我们证明了只需花费几美元，例如0.8美元、1美元、4美元和40美元，就可以盗取此类信息。

    A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. In this work, we show, for the first time, that an adversary with typical API access to an LM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. Our attack is effective against popular LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the feasibility of stealing such information with only a few dollars, e.g., $\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of GPT-3.
    
[^30]: 预测新闻事实性和媒体倾向的句子级别可靠性分析

    Predicting Sentence-Level Factuality of News and Bias of Media Outlets. (arXiv:2301.11850v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11850](http://arxiv.org/abs/2301.11850)

    本论文提出了一种针对整个媒体的细粒度可靠性分析方法，在手动制作的“FactNews”数据库上，通过 fine-tuning BERT 模型预测新闻报道的句子级别事实性和媒体倾向。此方法可应用于任何其他语言。

    

    预测新闻报道的事实性和媒体倾向对于自动化的新闻信誉和事实核查是很重要的。本文提出了对整个媒体进行细粒度可靠性分析的方法。我们研究了预测新闻报道的句子级别事实性和媒体倾向，这可以更精确地解释整个 source 的可靠程度。我们首先手动制作了一个大型的句子级别数据库，“FactNews”，由 6191 个专家注释的句子组成，注释依据来自 AllSides 的事实性和媒体倾向定义。最后，由于巴西存在严重的虚假新闻和政治极化问题，我们提供了用于葡萄牙语的数据集和基线模型。但是，我们的方法可以应用于任何其他语言。

    Predicting the factuality of news reporting and bias of media outlets is surely relevant for automated news credibility and fact-checking. While prior work has focused on the veracity of news, we propose a fine-grained reliability analysis of the entire media. Specifically, we study the prediction of sentence-level factuality of news reporting and bias of media outlets, which may explain more accurately the overall reliability of the entire source. We first manually produced a large sentence-level dataset, titled "FactNews", composed of 6,191 sentences expertly annotated according to factuality and media bias definitions from AllSides. As a result, baseline models for sentence-level factuality prediction were presented by fine-tuning BERT. Finally, due to the severity of fake news and political polarization in Brazil, both dataset and baseline were proposed for Portuguese. However, our approach may be applied to any other language.
    
[^31]: 将知识纳入文档摘要生成中：基于GPT-2的前缀调整应用

    Incorporating Knowledge into Document Summarisation: an Application of Prefix-Tuning on GPT-2. (arXiv:2301.11719v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11719](http://arxiv.org/abs/2301.11719)

    本论文研究了将事实知识纳入生成的摘要的可能性，具体采用前缀调整的方法，实验结果表明，此方法可以生成保留知识的摘要，而且可以提升整体性能。

    

    尽管现在文档摘要技术得到了很大的发展，但是生成的摘要和原始文本之间的事实不一致仍然时有发生。本研究探索了采用提示来将事实知识纳入生成的摘要的可能性。我们具体研究了前缀调整，它使用一组可训练的连续前缀提示和离散自然语言提示来帮助摘要生成。实验结果表明，可训练的前缀可以帮助摘要模型准确地从离散提示中提取信息，从而生成保留知识的摘要，这些摘要在事实上与离散提示一致。生成的摘要的ROUGE改进表明，将事实知识明确地添加到摘要生成过程中可以提升整体性能，显示出在其他自然语言处理任务中应用的巨大潜力。

    Despite the great development of document summarisation techniques nowadays, factual inconsistencies between the generated summaries and the original texts still occur from time to time. This study explores the possibility of adopting prompts to incorporate factual knowledge into generated summaries. We specifically study prefix-tuning that uses a set of trainable continuous prefix prompts together with discrete natural language prompts to aid summary generation. Experimental results demonstrate that the trainable prefixes can help the summarisation model extract information from discrete prompts precisely, thus generating knowledge-preserving summaries that are factually consistent with the discrete prompts. The ROUGE improvements of the generated summaries indicate that explicitly adding factual knowledge into the summarisation process could boost the overall performance, showing great potential for applying it to other natural language processing tasks.
    
[^32]: Prompting就是编程: 一种大语言模型的查询语言

    Prompting Is Programming: A Query Language for Large Language Models. (arXiv:2212.06094v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.06094](http://arxiv.org/abs/2212.06094)

    LMP将语言模型提示从纯文本提示扩展为文本提示和脚本的直观组合，实现了一种新的语言模型编程方式。

    

    大型语言模型在问答和代码生成等各种任务上展现出了优异的表现。从高层次上讲，给定输入，语言模型可以用统计上的可能性自动完成序列。基于此，用户通过语言指令或示例来提示这些模型，以执行各种下游任务。高级提示方法甚至可以暗示模型、用户和计算器等外部工具之间的交互。然而，为了获得最先进的性能或将语言模型适应特定任务，必须实现复杂的任务-和模型特定的程序，这仍然可能需要特定的交互。基于此，我们提出了语言模型编程（LMP）的新概念。LMP将语言模型提示从纯文本提示扩展为文本提示和脚本的直观组合。此外，LMP允许指定语言模型的约束条件。

    Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction.  Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the languag
    
[^33]: 瓶中语言：语言模型引导的概念瓶颈用于可解释的图像分类

    Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification. (arXiv:2211.11158v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.11158](http://arxiv.org/abs/2211.11158)

    本文提出了一种名为LaBo的语言模型引导的概念瓶颈模型，能够不需要手动指定关键概念并实现与黑盒子模型相似的性能，在图像分类中具有重要的可解释性。

    

    概念瓶颈模型是一种本质上可解释的模型，可以将模型决策分解为可读的概念，使人们能够轻松理解模型失败的原因，这对于高风险应用非常重要。概念瓶颈模型需要手动指定概念，通常表现不如黑盒子模型，限制了它们的广泛采用。我们解决了这些缺点，并首次展示了如何构建高性能的概念瓶颈模型，而不需要手动指定，其准确性类似于黑盒子模型。我们的方法“Language Guided Bottlenecks”（LaBo）利用一种语言模型GPT-3来定义可能的瓶颈组成的大型空间。在给定问题域的情况下，LaBo使用GPT-3生成有关类别的事实句子，形成候选概念。LaBo通过一种新颖的子模块效用高效搜索可能的瓶颈，促进选择有区分力和多样性的信息。最终，GPT-3的句子概念可以使用C与图像对齐。

    Concept Bottleneck Models (CBM) are inherently interpretable models that factor model decisions into human-readable concepts. They allow people to easily understand why a model is failing, a critical feature for high-stakes applications. CBMs require manually specified concepts and often under-perform their black box counterparts, preventing their broad adoption. We address these shortcomings and are first to show how to construct high-performance CBMs without manual specification of similar accuracy to black box models. Our approach, Language Guided Bottlenecks (LaBo), leverages a language model, GPT-3, to define a large space of possible bottlenecks. Given a problem domain, LaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts. LaBo efficiently searches possible bottlenecks through a novel submodular utility that promotes the selection of discriminative and diverse information. Ultimately, GPT-3's sentential concepts can be aligned to images using C
    
[^34]: 使用像素的语言建模

    Language Modelling with Pixels. (arXiv:2207.06991v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.06991](http://arxiv.org/abs/2207.06991)

    本文介绍了一个名为PIXEL的基于像素的预训练语言模型，它可以将文本渲染为图像，解决了扩展支持的语言数量时出现的词汇瓶颈问题，且在形态学和语义任务上显著优于BERT。

    

    语言模型在有限的输入集合上进行定义，这导致在尝试扩展支持的语言数量时出现词汇瓶颈。解决这个瓶颈会导致在嵌入矩阵中所能表示的内容与输出层的计算问题之间的权衡。本文引入了名为PIXEL的基于像素的语言编码器。PIXEL是预训练的语言模型，可以将文本渲染为图像，从而可以基于拼写相似性或像素的共同激活来跨语言传递表示。PIXEL训练时不是预测标记分布，而是重构被屏蔽的块的像素。我们对与BART相同的英语数据进行了86M参数PIXEL模型的预训练，并在形态学和语义任务上进行了评估，涵盖了不同类型的语言，包括各种非拉丁文字。我们发现，在语法和语义处理方面，PIXEL的性能显著优于BERT。

    Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches instead of predicting a distribution over tokens. We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing
    
[^35]: 利用机器学习在急诊科分诊中检测败血症

    Detection of sepsis during emergency department triage using machine learning. (arXiv:2204.07657v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.07657](http://arxiv.org/abs/2204.07657)

    本研究利用机器学习开发出一种检测急诊科分诊前败血症的模型，其性能优于标准败血症筛查算法。

    

    身体器官功能障碍的败血症是全球死亡和危重疾病的主要原因之一。本研究的目的是比较标准败血症筛查算法和在电子健康记录分诊数据上训练的机器学习算法在急诊科分诊前（未使用实验室诊断）的败血症检测性能。研究得出机器学习模型的 AUC 为 0.9423，敏感性为 71.09%。

    Sepsis is a life-threatening condition with organ dysfunction and is a leading cause of death and critical illness worldwide. Even a few hours of delay in the treatment of sepsis results in increased mortality. Early detection of sepsis during emergency department triage would allow early initiation of lab analysis, antibiotic administration, and other sepsis treatment protocols. The purpose of this study was to compare sepsis detection performance at ED triage (prior to the use of laboratory diagnostics) of the standard sepsis screening algorithm (SIRS with source of infection) and a machine learning algorithm trained on EHR triage data. A machine learning model (KATE Sepsis) was developed using patient encounters with triage data from 16participating hospitals. KATE Sepsis and standard screening were retrospectively evaluated on the adult population of 512,949 medical records. KATE Sepsis demonstrates an AUC of 0.9423 (0.9401 - 0.9441) with sensitivity of 71.09% (70.12% - 71.98%) and
    
[^36]: 跨语言性别一致性影响的跨语言差异：基于元分析的证据。

    Cross-linguistic differences in gender congruency effects: Evidence from meta-analyses. (arXiv:2109.03490v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2109.03490](http://arxiv.org/abs/2109.03490)

    通过元分析证实，日尔曼语言和斯拉夫语言中的性别一致效应比罗曼语言更加稳健，但效应大小适中，并且存在研究间的变异性。

    

    有人认为，单词制作的准备顺序取决于说话者所用的语言。当德语或荷兰语的说话者制作小猫的翻译时，会在制作过程的较早阶段选择标记性别的限定词。而法语或意大利语的说话者会将限定词或形容词的编码推迟到可用名词的音韵形式之后。因此，即使单词的顺序相同（例如在德语中是“die kleine Katze”，在法语中是“le petit chat”），它们的计划顺序不同，可能需要在制作开始前进行不同程度的提前计划。这种早期和晚期选择语言之间的区别是为了解释观察到的指出德日等语言但不包括罗曼语系语言中的图片命名速度较慢的性别干扰效应。进行元分析以直接测试这一跨语言性假说。分析表明，在图片命名反应时间中，性别一致效应在日尔曼语言和斯拉夫语言中的稳健性比罗曼语系语言更高。但是，效应大小适中，并且分析还揭示了效应大小在研究之间存在相当的变异性。这些结果指出了性别标记影响语言制作的跨语言差异，同时也强调了考虑影响性别一致性效应大小的方法和语境因素的重要性。

    It has been proposed that the order in which words are prepared for production depends on the speaker's language. When producing the translation equivalent of the small cat, speakers of German or Dutch select the gender-marked determiner at a relatively early stage of production. Speakers of French or Italian postpone the encoding of a determiner or adjective until the phonological form of the noun is available. Hence, even though the words are produced in the same order (e.g., die kleine Katze in German, le petit chat in French), they are not planned in the same order and might require different amounts of advanced planning prior to production onset. This distinction between early and late selection languages was proposed to account for the observation that speakers of Germanic and Slavic languages, but not of Romance languages, are slower to name pictures in the context of a distractor word of a different gender. Meta-analyses are conducted to provide the first direct test of this cr
    

