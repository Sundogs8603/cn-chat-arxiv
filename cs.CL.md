# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Grounded Intuition of GPT-Vision's Abilities with Scientific Images.](http://arxiv.org/abs/2311.02069) | 本研究使用基于示例的定性评估方法，发现GPT-Vision在科学图像的替代文本生成方面表现出对提示、反事实文本和相对空间关系的敏感性。这有助于加快研究人员对新模型的直观理解，并展示GPT-Vision如何提高信息可访问性。 |
| [^2] | [Vicinal Risk Minimization for Few-Shot Cross-lingual Transfer in Abusive Language Detection.](http://arxiv.org/abs/2311.02025) | 本研究提出了一种少样本跨语言滥用语言检测的邻近风险最小化方法，通过数据增强和持续预训练进行领域自适应，取得了令人鼓舞的结果。具体而言，在七种不同语言和三个不同领域的情况下，通过提出的MIXAG数据增强方法，显著提升了少样本跨语言滥用语言检测的性能。 |
| [^3] | [ProSG: Using Prompt Synthetic Gradients to Alleviate Prompt Forgetting of RNN-like Language Models.](http://arxiv.org/abs/2311.01981) | 本文解决了RNN-like语言模型在生成过程中遗忘提示的问题，提出了使用合成梯度教导模型记住提示的架构。 |
| [^4] | [The language of prompting: What linguistic properties make a prompt successful?.](http://arxiv.org/abs/2311.01967) | 本文研究了不同大小的预训练和指导调优的语言模型在语言结构上有所不同的提示上的表现，结果显示语言模型在性能上对提示的语言属性有较高的敏感性。 |
| [^5] | [Don't Make Your LLM an Evaluation Benchmark Cheater.](http://arxiv.org/abs/2311.01964) | 本文讨论了不恰当使用评估基准和误导性解释评估结果的潜在风险和影响，特别关注了基准泄漏现象。 |
| [^6] | [Too Much Information: Keeping Training Simple for BabyLMs.](http://arxiv.org/abs/2311.01955) | 本文介绍了格罗宁根大学在BabyLM挑战赛中的工作，探讨了将训练过程简单化的策略。研究发现，对于训练语言模型而言，仅改变上下文大小可以带来显著提升，在各项任务中表现优于基线模型。 |
| [^7] | [Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks.](http://arxiv.org/abs/2311.01949) | 提示增强上下文学习（HICL）是一种新的范例，使得大型语言模型（LLM）在知识密集型任务中表现出色。HICL利用LLM的推理能力从示范中提取与查询相关的知识，并通过更明确的提示方式来增强LLM的学习。通过引入一个与提示相关的示例检索器（HER），我们还能选择有信息量的示例来增强示范。对于开放域问答，HICL在三个基准测试上实现了显著的改进。 |
| [^8] | [Constructing Temporal Dynamic Knowledge Graphs from Interactive Text-based Games.](http://arxiv.org/abs/2311.01928) | 本研究提出了一种新颖的神经网络模型TDGU，将动态知识图谱表示为一系列带有时间戳的图事件，并使用时态基图神经网络对其进行建模。通过实验，我们证明了TDGU在文本游戏TextWorld数据集上的性能优于基线DGU。 |
| [^9] | [GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling.](http://arxiv.org/abs/2311.01927) | GateLoop是一种完全数据控制的线性递归序列模型，优于现有模型，可以提供数据控制的相对位置信息给Attention。 |
| [^10] | [Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review.](http://arxiv.org/abs/2311.01918) | 本综述文章探讨了大型语言模型（LLMs）在医学中的应用和意义。LLMs在知识检索、研究支持、临床工作流自动化和诊断辅助方面具有巨大潜力，尤其是多模态LLMs可以处理医学影像和电子健康记录等多样化数据类型以增强诊断能力。 |
| [^11] | [BoschAI @ PLABA 2023: Leveraging Edit Operations in End-to-End Neural Sentence Simplification.](http://arxiv.org/abs/2311.01907) | 本文描述了基于Llama 2的系统，在处理生物医学文本简化的PLABA共享任务中排名第一。通过引入句子级和标记级的损失权重，该系统能够产生与人工注释者相似的简化结果，语言更简单，并且进行更多的编辑操作。 |
| [^12] | [Indicative Summarization of Long Discussions.](http://arxiv.org/abs/2311.01882) | 本文提出了一种利用大型语言模型生成指示性摘要的方法，能够帮助概览长篇讨论，并根据论述句进行聚类和分类生成摘要，这对于理解论坛讨论非常有用。 |
| [^13] | [Sentiment Analysis through LLM Negotiations.](http://arxiv.org/abs/2311.01876) | 这篇论文介绍了一种情感分析的多LLM谈判框架，通过迭代生成器和判别器的过程达到共识，以解决单个LLM无法做出完美决策的问题。 |
| [^14] | [Efficient Black-Box Adversarial Attacks on Neural Text Detectors.](http://arxiv.org/abs/2311.01873) | 本文研究了高效的黑盒对抗攻击神经文本检测器的方法，通过参数调整和字符级变异等策略，可以修改GPT-3.5生成的文本，使其对人类不可疑但能导致神经文本检测器误判。 |
| [^15] | [Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval.](http://arxiv.org/abs/2311.01870) | Multi-EuP是一个新的多语言基准数据集，涵盖了来自欧洲议会的22K个多语言文档，旨在研究信息检索中的公平性，包括语言和人口偏见。它提供了真实的多语言语料库和跨语言相关性评判，并提供了与文档相关的丰富人口统计信息，可用于评估单语和多语信息检索。 |
| [^16] | [Towards Concept-Aware Large Language Models.](http://arxiv.org/abs/2311.01866) | 本文研究了概念在语言模型中的作用，并探讨了开发概念感知语言模型的方法。通过预训练LLMs或使用现有LLMs的输出，我们证明了这种方法更好地符合人类直觉并改善了预测的鲁棒性。 |
| [^17] | [SortNet: Learning To Rank By a Neural-Based Sorting Algorithm.](http://arxiv.org/abs/2311.01864) | SortNet是一种使用神经网络作为比较器来进行自适应排序的算法，通过迭代过程构建训练集，根据成对项目之间的排序示例来训练神经网络。 |
| [^18] | [$R^3$-NL2GQL: A Hybrid Models Approach for for Accuracy Enhancing and Hallucinations Mitigation.](http://arxiv.org/abs/2311.01862) | $R^3$-NL2GQL是一种通过利用较小和较大的Foundation Models进行重新排名、重写和细化的方法，以提高准确性和减轻幻觉，解决了NL2GQL任务中GQL生成能力和跨模式通用能力的挑战。 |
| [^19] | [FAME: Flexible, Scalable Analogy Mappings Engine.](http://arxiv.org/abs/2311.01860) | 这项工作提出了一个灵活可扩展的类比映射引擎，通过自动提取常识表示，并使用这些表示来确定实体之间的映射关系。与以往方法不同的是，该引擎可以处理部分类比，并提供新实体的建议。实验证明其在经典2x2类比问题上的准确率为81.2％，在更大的问题上为77.8％，此外，该引擎还优于人类表现。 |
| [^20] | [Large Language Models to the Rescue: Reducing the Complexity in Scientific Workflow Development Using ChatGPT.](http://arxiv.org/abs/2311.01825) | 本研究调查了使用大型语言模型ChatGPT支持科学工作流时的效率。通过在两个科学领域进行三个用户研究，研究结果显示LLMs能够高效解释工作流，但在组件交换和有目的工作方面的性能较低。 |
| [^21] | [Minimalist Grammar: Construction without Overgeneration.](http://arxiv.org/abs/2311.01820) | 该论文介绍了如何编写极简语法，并利用许可者/-被许可者来处理异常情况，避免过度生成。 |
| [^22] | [Mitigating Framing Bias with Polarity Minimization Loss.](http://arxiv.org/abs/2311.01817) | 该论文提出了一种新的损失函数，在输入的极化文章之间减小极性差异，以减轻框架偏见。实验证明，这种方法可以显著减少框架偏见，尤其是在训练模型以最小化信息编框偏见的极性损失时。 |
| [^23] | [AFPQ: Asymmetric Floating Point Quantization for LLMs.](http://arxiv.org/abs/2311.01792) | AFPQ提出了一种面向LLMs的非对称浮点量化方法，通过为正值和负值设置不同的比例尺，显著提高了准确性，并且可以与其他量化方法相结合，无需额外存储空间。 |
| [^24] | [TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine.](http://arxiv.org/abs/2311.01786) | TCM-GPT是一种用于传统中医领域适应的大型语言模型的高效预训练方法，通过构建一个传统中医专用语料库进行预训练，取得了良好的效果。 |
| [^25] | [UP4LS: User Profile Constructed by Multiple Attributes for Enhancing Linguistic Steganalysis.](http://arxiv.org/abs/2311.01775) | 本文提出了一个名为UP4LS的新框架，通过多个属性构建用户信息以增强语言隐写分析的性能。实验结果显示，该框架可以有效提升隐写分析的性能。 |
| [^26] | [PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion.](http://arxiv.org/abs/2311.01767) | 这个论文介绍了PPTC基准，用来评估大型语言模型在根据用户指令创建和编辑PPT文件方面的表现。通过测试，发现GPT-4在准确率方面表现最好，为75.1%。 |
| [^27] | [Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation.](http://arxiv.org/abs/2311.01766) | 本研究提出了一种基于多模态证据的立场抽取网络（SEN）来检测上下文错误的误导信息。通过考虑不同证据的立场，我们提供了一种更准确的检测方法，并引入了基于共现关系的支持-反驳分数。这种方法在公共大规模数据上进行的实验证明了其有效性。 |
| [^28] | [Indo LEGO-ABSA: A Multitask Generative Aspect Based Sentiment Analysis for Indonesian Language.](http://arxiv.org/abs/2311.01757) | 本研究旨在针对印度尼西亚语，利用生成式预训练语言模型实现多任务生成式基于方面的情感分析方法，并开发了Indo LEGO-ABSA模型。 |
| [^29] | [EmojiLM: Modeling the New Emoji Language.](http://arxiv.org/abs/2311.01751) | EmojiLM是一个专门处理文本-表情符号双向翻译的模型，通过合成大规模的文本-表情符号并行语料库Text2Emoji进行训练，在公共基准测试和人工评估中表现出优异的性能，对于与表情符号相关的下游任务有益。 |
| [^30] | [SAC$^3$: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency.](http://arxiv.org/abs/2311.01740) | SAC^3是一种基于语义感知交叉检查一致性的方法，可以可靠地检测黑盒语言模型中的幻觉。该方法通过加入语义等效问题扰动和跨模型响应一致性检查等机制，能够有效识别问题级别和模型级别的幻觉。实证分析表明，SAC^3在检测非事实和事实陈述方面优于现有技术最新水平。 |
| [^31] | [Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models.](http://arxiv.org/abs/2311.01732) | Proto-lm是一种基于原型网络的大型语言模型（LLM）内置可解释性框架，通过在微调阶段学习可解释的嵌入来提供解释性，同时保持竞争性能。该方法为创建可解释性模型提供了新的可能性。 |
| [^32] | [An Empirical Study of Benchmarking Chinese Aspect Sentiment Quad Prediction.](http://arxiv.org/abs/2311.01713) | 本研究构建了两个大规模的中国ASQP数据集，对生成式预训练变压器（GPT）系列模型在ASQP上的性能进行了评估，并展示了改进ASQP技术和提高GPT性能的重要性。 |
| [^33] | [A New Korean Text Classification Benchmark for Recognizing the Political Intents in Online Newspapers.](http://arxiv.org/abs/2311.01712) | 这项工作提出了一个用于自动识别韩国在线报纸中政治意图的新的韩文文本分类数据集，该数据集包含12000篇新闻文章，并提供了基于深度学习的分类模型。这是目前最大规模的韩国新闻数据集，可以处理长文本和多任务分类问题。 |
| [^34] | [Data-Free Distillation of Language Model by Text-to-Text Transfer.](http://arxiv.org/abs/2311.01689) | 本文提出了一种无数据知识蒸馏（DFKD）框架，即DFKD-T$^{3}$，利用预训练的生成式语言模型作为数据生成器，将通用领域语料库转化为压缩友好的任务数据。实验证明该方法能够提升各种下游任务的蒸馏性能。 |
| [^35] | [CASE: Commonsense-Augmented Score with an Expanded Answer Space.](http://arxiv.org/abs/2311.01684) | 我们提出了一个带有扩展答案空间和常识增强评分机制（CASE），它通过根据输入中单词的语义关系分配重要性权重来改善多项选择问答任务中的评分机制。我们还通过生成概念上类似于选项的词汇不同的答案来进一步扩展答案空间。这一方法在降低噪声和提供隐含常识知识方面表现出色，并取得了更好的性能。 |
| [^36] | [DialogBench: Evaluating LLMs as Human-like Dialogue Systems.](http://arxiv.org/abs/2311.01677) | 本文提出了DialogBench，一个对话评估基准，用于评估LLMs作为人类对话系统的能力。通过对28个LLMs的广泛测试，发现指导微调对提升性能效果显著。 |
| [^37] | [Plot Retrieval as an Assessment of Abstract Semantic Association.](http://arxiv.org/abs/2311.01666) | 本研究提出了一个名为情节检索的新任务，通过生成一个标记数据集来训练和评估信息检索模型在此任务上的性能。这个任务要求模型能够准确估计查询和候选情节之间的抽象语义关联度，而不仅仅是依赖于词汇或语义匹配。 |
| [^38] | [MARRS: Multimodal Reference Resolution System.](http://arxiv.org/abs/2311.01650) | MARRS是一个在设备上运行的多模态参考解析系统，能够处理对话式、视觉和背景上下文，并通过不同的机器学习模型实现上下文查询的处理。这个系统能够在保护用户隐私的同时理解上下文。 |
| [^39] | [VQPy: An Object-Oriented Approach to Modern Video Analytics.](http://arxiv.org/abs/2311.01623) | VQPy是一种面向对象的视频分析方法，它使用Python变体作为前端，并具有可扩展的后端，可以自动构建和优化基于视频对象的处理流程。 |
| [^40] | [ACQUIRED: A Dataset for Answering Counterfactual Questions In Real-Life Videos.](http://arxiv.org/abs/2311.01620) | ACQUIRED是一个用于回答真实生活视频中反事实问题的数据集，在多模态模型反事实推理能力领域弥补了目前数据不足的问题，并提供了多样的真实世界场景和推理维度来评估模型的泛化能力。 |
| [^41] | [FLAP: Fast Language-Audio Pre-training.](http://arxiv.org/abs/2311.01615) | FLAP是一种快速的语言音频预训练方法，通过掩盖、对比学习和重构，有效地学习对齐的音频和语言表示，以及利用大型语言模型（LLMs）增强文本输入。该方法在音频-文本检索任务上取得了最新技术性能（SoTA）。 |
| [^42] | [KG-FRUS: a Novel Graph-based Dataset of 127 Years of US Diplomatic Relations.](http://arxiv.org/abs/2311.01606) | 本文介绍了KG-FRUS数据集，该数据集以知识图谱的形式编码了超过30万份美国政府外交文件，并使用提取的实体、元数据及来自Wikidata的额外实体和关系创建了一个基于图的数据集。该数据集的关系捕捉了外交、国际关系和政治等复杂领域的协同和动力。文章演示了不同的探索数据集方法。 |
| [^43] | [Faithful and Robust Local Interpretability for Textual Predictions.](http://arxiv.org/abs/2311.01605) | 提出了一种名为FRED的新颖方法，用于解释文本预测。FRED可以识别文档中的关键词，并且通过与最先进的方法进行的实证评估证明了其在提供对文本模型的深入见解方面的有效性。 |
| [^44] | [MetaReVision: Meta-Learning with Retrieval for Visually Grounded Compositional Concept Acquisition.](http://arxiv.org/abs/2311.01580) | MetaReVision是一种检索增强的元学习模型，通过使用检索到的基本概念作为支持集合来快速学习和识别新的图像基础组合概念。 |
| [^45] | [Preserving the knowledge of long clinical texts using aggregated ensembles of large language models.](http://arxiv.org/abs/2311.01571) | 本文提出了一种使用聚合集成模型的方法来保留长篇临床文本的知识。与以往方法不同，我们将集成学习与文本聚合相结合，并在两个临床预测任务上训练多个大型语言模型。实验证明，我们的方法可以在处理长输入和多样性数据集时提升大型语言模型的性能。 |
| [^46] | [Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers.](http://arxiv.org/abs/2311.01555) | 中文总结出的一句话要点：本研究提出了一种指令蒸馏方法，通过将大型语言模型的一对一排序能力蒸馏为更高效的单点排序，显著提高了大型语言模型作为零-shot排序器的效率和性能。 |
| [^47] | [Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization.](http://arxiv.org/abs/2311.01544) | 本研究引入了一种新的方法，即不同的令牌指标（DTM），用于评估压缩后的大型语言模型（LLM）。通过关注令牌的差异性，DTM提供了对模型压缩微妙之处的深入洞察，并且在不损害文本生成质量的情况下可以实现显著的精确度和稀疏度水平。该研究还提出了一种利用DTM进行模型稀疏化和量化的方法，并发现可以修剪掉超过90%的LLM组件和量化超过80%的参数。 |
| [^48] | [What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning.](http://arxiv.org/abs/2311.01487) | 通过综合复杂的视觉推理任务，可以有效改善多模式大型语言模型在评估基准上的性能。我们提出了一种自动创建高质量复杂视觉推理指令的系统方法。 |
| [^49] | [Relation Extraction from News Articles (RENA): A Tool for Epidemic Surveillance.](http://arxiv.org/abs/2311.01472) | RENA是一种基于浏览器的关系提取工具，用于从英语新闻文章中提取与传染病相关的关键实体和语义关系，为流行病监测提供实时解析和关键信息提取的能力。 |
| [^50] | [Leveraging Language Models to Detect Greenwashing.](http://arxiv.org/abs/2311.01469) | 本研究引入了一种新的方法，利用语言模型来检测绿色虚假宣传风险。开发了一种量化绿色虚假宣传风险的数学形式，建立了优化的ClimateBERT模型，并进行了结果比较分析。实验表明，我们的方法对于这一任务具有良好的探索方向。 |
| [^51] | [Remember what you did so you know what to do next.](http://arxiv.org/abs/2311.01468) | 本文通过使用一个大型语言模型（LLM）为模拟机器人制定计划，在ScienceWorld中实现30类目标。实验结果显示，LLM在马尔可夫假设的情况下比强化学习方法的性能提高了1.4倍，当填充尽可能多的先前步骤时提高到3.5倍，即使只训练了6.5%的数据，也比基于强化学习方法的性能提高了2.2倍。不同类别的动作表现差异很大，说明平均任务可能会隐藏性能问题。 |
| [^52] | [Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI.](http://arxiv.org/abs/2311.01463) | 这篇论文描述了在医疗人工智能中创建可靠、可信和无偏置的LLM模型的关键要素，着重于量化、验证和缓解幻觉问题，并讨论了LLM在医疗领域的未来发展。 |
| [^53] | [FlashDecoding++: Faster Large Language Model Inference on GPUs.](http://arxiv.org/abs/2311.01282) | FlashDecoding++是一种快速的LLM推理引擎，通过解决同步部分softmax更新、未充分利用扁平GEMM计算和静态数据流导致的性能损失等挑战，实现了大规模语言模型推理的加速。 |
| [^54] | [Improving Interpersonal Communication by Simulating Audiences with Language Models.](http://arxiv.org/abs/2311.00687) | 本论文提出了一个基于大型语言模型（LLM）模拟的框架，通过探索解决方案空间、生成沟通候选以及模拟受众反应，来改善人际沟通。通过评估八个涵盖人际沟通基本过程的场景，展示了该框架的有效性。 |
| [^55] | [On the effect of curriculum learning with developmental data for grammar acquisition.](http://arxiv.org/abs/2311.00128) | 这项研究发现语法习得主要受到对语音数据的暴露驱动，并通过课程学习方法进一步提高其性能。 |
| [^56] | [CapsFusion: Rethinking Image-Text Data at Scale.](http://arxiv.org/abs/2310.20550) | CapsFusion是一个先进的框架，通过利用大型语言模型整合和细化来自网络图像-文本对和合成字幕的信息，提供了更高质量、更可扩展的多模态预训练数据。 |
| [^57] | [Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization.](http://arxiv.org/abs/2310.20033) | 本文提出了一种使用ChatGPT来生成高质量反馈数据以改善临床笔记总结的事实一致性的新方法。 |
| [^58] | [Managing AI Risks in an Era of Rapid Progress.](http://arxiv.org/abs/2310.17688) | 在人工智能快速进展的时代，我们提出了管理即将到来的先进人工智能系统所带来的风险的优先事项。 |
| [^59] | [Detecting Pretraining Data from Large Language Models.](http://arxiv.org/abs/2310.16789) | 这项研究探讨了如何检测大型语言模型的预训练数据，提出了一个动态基准和一种新的检测方法，以解决数据隐私和不透明性的问题。 |
| [^60] | [Can ChatGPT Perform Reasoning Using the IRAC Method in Analyzing Legal Scenarios Like a Lawyer?.](http://arxiv.org/abs/2310.14880) | 本论文研究了ChatGPT是否能够像律师一样使用IRAC方法分析法律情景。通过构建一个包含马来西亚合同法和澳大利亚社会法情景的语料库，并使用IRAC方法对其进行分析，作者发现ChatGPT在法律分析方面具有潜力。 |
| [^61] | ["Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters.](http://arxiv.org/abs/2310.09219) | 本文对LLM生成的推荐信中的性别偏见进行了细致的研究，并设计了评估方法来展现通过语言风格和词汇内容来体现的性别偏见。 |
| [^62] | [Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment.](http://arxiv.org/abs/2310.08372) | 该论文研究了通过知识增强和对齐两种方法来提高基于知识的对话系统的事实一致性，以解决生成与提供的知识源事实不一致的回复的问题。 |
| [^63] | [Glitter or Gold? Deriving Structured Insights from Sustainability Reports via Large Language Models.](http://arxiv.org/abs/2310.05628) | 通过大型语言模型和信息抽取技术，本研究提取了公司可持续性报告中的结构化ESG相关信息，为利益相关者提供简洁、信息丰富和可行动的数据。 |
| [^64] | [Dynamic Top-k Estimation Consolidates Disagreement between Feature Attribution Methods.](http://arxiv.org/abs/2310.05619) | 本文提出了一种通过分析归因分数的连续属性来确定应显示的最佳 k 个标记的动态 Top-k 估计方法，用于整合特征归因方法之间的分歧。实验证明，动态 k 主要改进了集成梯度和 GradientXInput 的表现，为人类解释提供了具有信息价值的归因信号。 |
| [^65] | [Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems.](http://arxiv.org/abs/2310.05280) | 这项研究评估了对话系统中的人格偏见对社交偏见的影响，并建立了一个综合评估框架来衡量不同人格采用下的偏见程度。 |
| [^66] | [Automating Behavioral Testing in Machine Translation.](http://arxiv.org/abs/2309.02553) | 本文提出了一种利用大型语言模型自动生成源句子的方法，以测试机器翻译模型在多种情况下的行为。通过对多个机器翻译系统应用该方法，发现在测试结果与传统准确率度量存在差异的情况下，仍可观察到一致的趋势。 |
| [^67] | [ChatGPT for GTFS: From Words to Information.](http://arxiv.org/abs/2308.02618) | 本研究探索了使用ChatGPT语言模型从GTFS数据中检索信息的可行性，验证了ChatGPT（GPT-3.5）在GTFS规范理解和信息提取方面的能力。程序合成方法在信息检索任务中表现出更高的准确率，为解决GTFS数据信息获取问题提供了一种有效的方法。 |
| [^68] | [Guiding Language Models of Code with Global Context using Monitors.](http://arxiv.org/abs/2306.10763) | 本文提出了一种使用监视器引导全局上下文的方法来指导代码语言模型，在处理类型、功能或API等全局上下文时，能够提高代码语言模型的性能和准确性。 |
| [^69] | [Fine-Tuning Language Models with Advantage-Induced Policy Alignment.](http://arxiv.org/abs/2306.02231) | 本论文提出了一种新算法APA，其采用优势诱导策略对齐用于强化学习语言模型。相对于传统方法（PPO），APA在语言任务中表现更好，避免了模型的崩溃与不稳定性。 |
| [^70] | [Grammar Prompting for Domain-Specific Language Generation with Large Language Models.](http://arxiv.org/abs/2305.19234) | 本文提出了一种基于语法提示的方法，使用专用的语法来增强示例，为大型语言模型（LLM）在特定领域的语言生成任务中使用外部知识和特定约束条件进行上下文学习。 |
| [^71] | [Are Diffusion Models Vision-And-Language Reasoners?.](http://arxiv.org/abs/2305.16397) | 本文针对扩散-语言图像生成模型进行转换和评估，介绍了生成-鉴别评估基准(GDBench)基于7个视觉语言复杂任务，并发现转换后的模型在组合性任务方面的表现优于CLIP，通过微调可提高其组合性能。 |
| [^72] | [LLMDet: A Third Party Large Language Models Generated Text Detection Tool.](http://arxiv.org/abs/2305.15004) | LLMDet是一个第三方大型语言模型生成文本检测工具，能够从特定的语言模型中确定生成文本的来源，并满足精细追踪、中间判断和快速检测的要求。 |
| [^73] | [Gender Biases in Automatic Evaluation Metrics: A Case Study on Image Captioning.](http://arxiv.org/abs/2305.14711) | 本文研究了模型评估度量中的性别偏见对图像字幕任务的影响，并提出了替代方案以解决这一问题。 |
| [^74] | [Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment.](http://arxiv.org/abs/2305.13669) | 本文提出了MixAlign框架，通过与用户和知识库交互，实现自动的问题-知识对齐，从而解决了语言模型因无法正确理解问题和知识而导致的幻觉问题。 |
| [^75] | [Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference.](http://arxiv.org/abs/2305.13484) | Flover是一种用于自回归模型并行推断的时间融合框架，解决了并行性不足和灵活性差的问题，可以实现更加高效的推断性能。 |
| [^76] | [Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models.](http://arxiv.org/abs/2305.13112) | 本文重新思考了大语言模型时代下对话型推荐系统的评估问题，提出了一种基于大语言模型的交互式评估方法iEvaLM，通过实验证明了该方法相较于现有评估协议具有显著的改进，并强调了对外部知识的评估。 |
| [^77] | [ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain Dialogue Systems.](http://arxiv.org/abs/2305.07797) | ACCENT是一种基于常识知识库的事件常识评价方法，通过对话中提取的事件-关系元组与CSKB的兼容性评估响应，是一种有效的评价方法。 |
| [^78] | [DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction.](http://arxiv.org/abs/2304.11015) | DIN-SQL通过将复杂的文本到SQL任务分解为子问题，并将这些子问题的解决方案馈入到大型语言模型中，显著提高了它们的表现，使准确性超过了当前最先进的技术。 |
| [^79] | [OpenAGI: When LLM Meets Domain Experts.](http://arxiv.org/abs/2304.04370) | 基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。 |
| [^80] | [Why think step-by-step? Reasoning emerges from the locality of experience.](http://arxiv.org/abs/2304.03843) | 本文通过语言模型研究何时以及为什么推理是有帮助的，测试推理在训练数据由相互影响强烈的局部变量集群组成时是否有效。通过一步步的推理，能够将准确的局部推理链接在一起，以估算在训练中没有同时观察到的变量之间的关系。 |
| [^81] | [Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model.](http://arxiv.org/abs/2212.09146) | 本论文研究了检索增强语言模型的推理能力，发现检索器和语言模型之间存在推卸责任的问题，且检索器选择的句子和语言模型不考虑句子之间的复杂关系都会影响推理性能。针对这些问题，本文提出了一种新的框架ReForMask，采用掩码检索方法来更好地捕捉语句之间的复杂关系并在多个任务上实现了显著优化。 |
| [^82] | [Towards Abstractive Timeline Summarisation using Preference-based Reinforcement Learning.](http://arxiv.org/abs/2211.07596) | 本文提出了一种基于偏好的强化学习方法，在抽象摘要的基础上优化时间线摘要的质量和可读性，通过自动化和人工评估验证了方法的有效性。 |
| [^83] | [Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems.](http://arxiv.org/abs/2204.00763) | 本文提出了一种用于评估任务导向的对话系统的隐喻用户模拟器。该模拟器可以生成类似人类对话的模拟评估，并且提供了一个测试者框架来生成不同能力的对话系统变体。 |
| [^84] | [CoPaSul Manual -- Contour-based parametric and superpositional intonation stylization.](http://arxiv.org/abs/1612.04765) | CoPaSul工具包提供了自动的韵律标注和特征提取功能，使用了基于轮廓的参数化和叠加韵律风格化的方法。通过该工具包可以得到与韵律边界和突出性相关的特征，并可以通过系数聚类得到韵律轮廓类别。 |

# 详细

[^1]: 使用科学图像来揭示GPT-Vision的能力的基于直觉的研究

    Grounded Intuition of GPT-Vision's Abilities with Scientific Images. (arXiv:2311.02069v1 [cs.CL])

    [http://arxiv.org/abs/2311.02069](http://arxiv.org/abs/2311.02069)

    本研究使用基于示例的定性评估方法，发现GPT-Vision在科学图像的替代文本生成方面表现出对提示、反事实文本和相对空间关系的敏感性。这有助于加快研究人员对新模型的直观理解，并展示GPT-Vision如何提高信息可访问性。

    

    GPT-Vision在许多视觉语言任务上给我们留下了深刻印象，但也带来了一个常见的新挑战：我们对其能力和限制几乎一无所知。在本研究中，我们规范化了一种许多人本能地试图培养对这个新模型的“基于直觉”理解的过程。受到远离基准测试而倾向于基于示例的定性评估的最新趋势的启发，我们借鉴了社会科学和人机交互中的扎根理论和主题分析，建立了一个严格的自然语言处理定性评估框架。我们使用这种技术来研究科学图像的替代文本生成，发现GPT-Vision对提示、图像中的反事实文本和相对空间关系特别敏感。我们的方法和分析旨在帮助研究人员加快对新模型的基于直觉的理解，同时展示GPT-Vision如何应用于使信息更易于获取。

    GPT-Vision has impressed us on a range of vision-language tasks, but it comes with the familiar new challenge: we have little idea of its capabilities and limitations. In our study, we formalize a process that many have instinctively been trying already to develop "grounded intuition" of this new model. Inspired by the recent movement away from benchmarking in favor of example-driven qualitative evaluation, we draw upon grounded theory and thematic analysis in social science and human-computer interaction to establish a rigorous framework for qualitative evaluation in natural language processing. We use our technique to examine alt text generation for scientific figures, finding that GPT-Vision is particularly sensitive to prompting, counterfactual text in images, and relative spatial relationships. Our method and analysis aim to help researchers ramp up their own grounded intuitions of new models while exposing how GPT-Vision can be applied to make information more accessible.
    
[^2]: 少样本跨语言滥用语言检测中的邻近风险最小化

    Vicinal Risk Minimization for Few-Shot Cross-lingual Transfer in Abusive Language Detection. (arXiv:2311.02025v1 [cs.CL])

    [http://arxiv.org/abs/2311.02025](http://arxiv.org/abs/2311.02025)

    本研究提出了一种少样本跨语言滥用语言检测的邻近风险最小化方法，通过数据增强和持续预训练进行领域自适应，取得了令人鼓舞的结果。具体而言，在七种不同语言和三个不同领域的情况下，通过提出的MIXAG数据增强方法，显著提升了少样本跨语言滥用语言检测的性能。

    

    从高资源语言到中低资源语言的跨语言迁移学习已经取得了令人鼓舞的结果。然而，目标语言中资源的稀缺性仍然是一个挑战。在这项工作中，我们采用数据增强和持续预训练进行领域自适应，以改善跨语言滥用语言检测。对于数据增强，我们分析了两种现有的基于邻近风险最小化的技术，并提出了MIXAG，一种根据实例表示的角度插值一对实例的新型数据增强方法。我们的实验涉及七种与英语在语言类型上不同的语言和三个不同的领域。结果显示，数据增强策略可以提升少样本跨语言滥用语言检测的性能。具体而言，我们观察到在所有目标语言中，MIXAG在多领域和多语言环境中显著改善。最后，我们通过错误分析展示了领域自适应如何改进迁移学习模型的性能。

    Cross-lingual transfer learning from high-resource to medium and low-resource languages has shown encouraging results. However, the scarcity of resources in target languages remains a challenge. In this work, we resort to data augmentation and continual pre-training for domain adaptation to improve cross-lingual abusive language detection. For data augmentation, we analyze two existing techniques based on vicinal risk minimization and propose MIXAG, a novel data augmentation method which interpolates pairs of instances based on the angle of their representations. Our experiments involve seven languages typologically distinct from English and three different domains. The results reveal that the data augmentation strategies can enhance few-shot cross-lingual abusive language detection. Specifically, we observe that consistently in all target languages, MIXAG improves significantly in multidomain and multilingual environments. Finally, we show through an error analysis how the domain adap
    
[^3]: 使用提示合成梯度缓解RNN-like语言模型的遗忘现象

    ProSG: Using Prompt Synthetic Gradients to Alleviate Prompt Forgetting of RNN-like Language Models. (arXiv:2311.01981v1 [cs.CL])

    [http://arxiv.org/abs/2311.01981](http://arxiv.org/abs/2311.01981)

    本文解决了RNN-like语言模型在生成过程中遗忘提示的问题，提出了使用合成梯度教导模型记住提示的架构。

    

    近年来，NLP研究人员越来越关注RNN-like语言模型，并且有几个模型取得了显著进展，展示出与传统Transformer相当的性能。然而，由于RNN的循环性质，这种语言模型只能在一组固定长度的状态向量中存储信息。因此，尽管经过了许多改进和优化，当给出复杂的指令或提示时，它们仍然会遗忘。作为语言模型的主要和最关注的功能，解决在生成过程中的遗忘问题是非常重要的。本文针对在生成过程中缓解提示遗忘的问题，提出了一种通过合成梯度教导模型在生成过程中记住提示的架构。为了强制模型记住提示，我们导出了编码提示的状态，然后将其转化为模型参数的修改。

    RNN-like language models are getting renewed attention from NLP researchers in recent years and several models have made significant progress, which demonstrates performance comparable to traditional transformers. However, due to the recurrent nature of RNNs, this kind of language model can only store information in a set of fixed-length state vectors. As a consequence, they still suffer from forgetfulness though after a lot of improvements and optimizations, when given complex instructions or prompts. As the prompted generation is the main and most concerned function of LMs, solving the problem of forgetting in the process of generation is no wonder of vital importance. In this paper, focusing on easing the prompt forgetting during generation, we proposed an architecture to teach the model memorizing prompt during generation by synthetic gradient. To force the model to memorize the prompt, we derive the states that encode the prompt, then transform it into model parameter modification
    
[^4]: 提示的语言：什么语言属性使得提示成功？

    The language of prompting: What linguistic properties make a prompt successful?. (arXiv:2311.01967v1 [cs.CL])

    [http://arxiv.org/abs/2311.01967](http://arxiv.org/abs/2311.01967)

    本文研究了不同大小的预训练和指导调优的语言模型在语言结构上有所不同的提示上的表现，结果显示语言模型在性能上对提示的语言属性有较高的敏感性。

    

    最新一代的语言模型可以通过提示来在许多自然语言处理任务中实现令人印象深刻的零样本或少样本性能。然而，由于性能对提示的选择非常敏感，人们付出了相当大的努力来进行众包提示或设计用于优化提示的方法。然而，我们仍然缺乏对提示的语言属性与任务性能之间的系统理解。在这项工作中，我们研究了不同大小的预训练和指导调优的语言模型在语义上等效但在语言结构上有所不同的提示上的表现。我们通过调查语法属性（如情态、时态、语态和语气）以及通过使用同义词引入词汇-语义变化。我们的发现与常见假设相矛盾，即语言模型在低困惑度的提示上达到最佳性能，这些提示反映了预训练或指导调优数据中的语言使用。提示在数据集或模型之间转移效果不佳，性能有所下降。

    The latest generation of LLMs can be prompted to achieve impressive zero-shot or few-shot performance in many NLP tasks. However, since performance is highly sensitive to the choice of prompts, considerable effort has been devoted to crowd-sourcing prompts or designing methods for prompt optimisation. Yet, we still lack a systematic understanding of how linguistic properties of prompts correlate with task performance. In this work, we investigate how LLMs of different sizes, pre-trained and instruction-tuned, perform on prompts that are semantically equivalent, but vary in linguistic structure. We investigate both grammatical properties such as mood, tense, aspect and modality, as well as lexico-semantic variation through the use of synonyms. Our findings contradict the common assumption that LLMs achieve optimal performance on lower perplexity prompts that reflect language use in pretraining or instruction-tuning data. Prompts transfer poorly between datasets or models, and performanc
    
[^5]: 不要让你的LLM成为一个评估基准欺骗者

    Don't Make Your LLM an Evaluation Benchmark Cheater. (arXiv:2311.01964v1 [cs.CL])

    [http://arxiv.org/abs/2311.01964](http://arxiv.org/abs/2311.01964)

    本文讨论了不恰当使用评估基准和误导性解释评估结果的潜在风险和影响，特别关注了基准泄漏现象。

    

    大型语言模型（LLMs）已经极大地推动了人工智能的前沿，实现了模型能力的显著提升。为了评估模型性能，通常的做法是构建评估基准，以测量LLMs在不同方面的能力水平。尽管已经发布了许多高质量的基准，但对于这些基准的合理使用和不同模型的公平比较的关注越来越多。鉴于这些关注，本文讨论了不恰当使用评估基准和误导性解释评估结果的潜在风险和影响。特别地，我们关注了一个特殊问题，即导致不恰当评估的\emph{基准泄漏}，即评估集相关的数据偶尔被用于模型训练。由于预训练数据通常是在模型测试之前准备的，因此这种现象变得更加普遍。

    Large language models~(LLMs) have greatly advanced the frontiers of artificial intelligence, attaining remarkable improvement in model capacity. To assess the model performance, a typical approach is to construct evaluation benchmarks for measuring the ability level of LLMs in different aspects. Despite that a number of high-quality benchmarks have been released, the concerns about the appropriate use of these benchmarks and the fair comparison of different models are increasingly growing. Considering these concerns, in this paper, we discuss the potential risk and impact of inappropriately using evaluation benchmarks and misleadingly interpreting the evaluation results. Specially, we focus on a special issue that would lead to inappropriate evaluation, \ie \emph{benchmark leakage}, referring that the data related to evaluation sets is occasionally used for model training. This phenomenon now becomes more common since pre-training data is often prepared ahead of model test. We conduct 
    
[^6]: 太多信息：保持BabyLM训练简单

    Too Much Information: Keeping Training Simple for BabyLMs. (arXiv:2311.01955v1 [cs.CL])

    [http://arxiv.org/abs/2311.01955](http://arxiv.org/abs/2311.01955)

    本文介绍了格罗宁根大学在BabyLM挑战赛中的工作，探讨了将训练过程简单化的策略。研究发现，对于训练语言模型而言，仅改变上下文大小可以带来显著提升，在各项任务中表现优于基线模型。

    

    本文详细介绍了格罗宁根大学在BabyLM挑战赛中的工作。我们遵循婴儿一样，语言模型应该先介于较简单的概念，然后建立在此基础上理解更复杂的概念的思想。我们通过多个角度，即上下文大小、词汇量和数据的整体语言复杂性，来研究简单到复杂的策略。我们发现，只有上下文大小这个简单的改变对训练语言模型真正有益。然而，这个简单的上下文大小的变化使我们在(Super)GLUE任务上平均提高了2个点，在MSGS任务上提高了1个点，在BLiMP任务上平均提高了12%。我们的上下文受限模型胜过了训练了10倍数据量的基线模型。

    This paper details the work of the University of Groningen for the BabyLM Challenge. We follow the idea that, like babies, language models should be introduced to simpler concepts first and build off of that knowledge to understand more complex concepts. We examine this strategy of simple-then-complex through a variety of lenses, namely context size, vocabulary, and overall linguistic complexity of the data. We find that only one, context size, is truly beneficial to training a language model. However this simple change to context size gives us improvements of 2 points on average on (Super)GLUE tasks, 1 point on MSGS tasks, and 12\% on average on BLiMP tasks. Our context-limited model outperforms the baseline that was trained on 10$\times$ the amount of data.
    
[^7]: 通过提示增强上下文学习使得大型语言模型在知识密集型任务中表现出色

    Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks. (arXiv:2311.01949v1 [cs.CL])

    [http://arxiv.org/abs/2311.01949](http://arxiv.org/abs/2311.01949)

    提示增强上下文学习（HICL）是一种新的范例，使得大型语言模型（LLM）在知识密集型任务中表现出色。HICL利用LLM的推理能力从示范中提取与查询相关的知识，并通过更明确的提示方式来增强LLM的学习。通过引入一个与提示相关的示例检索器（HER），我们还能选择有信息量的示例来增强示范。对于开放域问答，HICL在三个基准测试上实现了显著的改进。

    

    随着大型语言模型（LLM）规模的增加，上下文学习（ICL）能力已经出现，使得它们能够从示范中学习输入-标签映射，并在下游任务中表现良好。然而，在标准ICL设置下，LLM有时会忽略示范中与查询相关的信息，导致错误的预测。为了解决这个限制，我们提出了一种新的范例称为提示增强上下文学习（HICL），来探索ICL在知识密集型任务中的潜力，特别是在开放域问答中。HICL利用LLM的推理能力从示范中提取与查询相关的知识，然后将这些知识与LLM进行更明确的提示。此外，我们跟踪知识的来源，以识别特定的示例，并引入一个与提示相关的示例检索器（HER）来选择有信息量的示例进行增强示范。我们使用HER评估了HICL在3个开放域问答基准测试中，并观察到了显著的改进。

    In-context learning (ICL) ability has emerged with the increasing scale of large language models (LLMs), enabling them to learn input-label mappings from demonstrations and perform well on downstream tasks. However, under the standard ICL setting, LLMs may sometimes neglect query-related information in demonstrations, leading to incorrect predictions. To address this limitation, we propose a new paradigm called Hint-enhanced In-Context Learning (HICL) to explore the power of ICL in open-domain question answering, an important form in knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extract query-related knowledge from demonstrations, then concatenates the knowledge to prompt LLMs in a more explicit way. Furthermore, we track the source of this knowledge to identify specific examples, and introduce a Hint-related Example Retriever (HER) to select informative examples for enhanced demonstrations. We evaluate HICL with HER on 3 open-domain QA benchmarks, and observe av
    
[^8]: 从交互式文本游戏构建时间动态知识图谱

    Constructing Temporal Dynamic Knowledge Graphs from Interactive Text-based Games. (arXiv:2311.01928v1 [cs.CL])

    [http://arxiv.org/abs/2311.01928](http://arxiv.org/abs/2311.01928)

    本研究提出了一种新颖的神经网络模型TDGU，将动态知识图谱表示为一系列带有时间戳的图事件，并使用时态基图神经网络对其进行建模。通过实验，我们证明了TDGU在文本游戏TextWorld数据集上的性能优于基线DGU。

    

    在自然语言处理中，交互式文本游戏被用作测试交互式人工智能系统的试验场。之前的研究提出了通过离散知识图谱表示游戏状态的方式来玩文本游戏，其中知识图谱由Discrete Graph Updater (DGU) 构建。虽然DGU在解释性方面表现出色，但由于缺乏时间性和对具有相同标签的复杂环境的有限泛化能力，其知识图谱的准确性较低。为了解决DGU的弱点并保持其高解释性，我们提出了时间离散图更新器 (TDGU)，这是一种新颖的神经网络模型，将动态知识图谱表示为一系列带有时间戳的图事件，并使用时态基图神经网络对其进行建模。通过对从基于文本的游戏TextWorld收集的数据集进行实验，我们证明了TDGU优于基线DGU。

    In natural language processing, interactive text-based games serve as a test bed for interactive AI systems. Prior work has proposed to play text-based games by acting based on discrete knowledge graphs constructed by the Discrete Graph Updater (DGU) to represent the game state from the natural language description. While DGU has shown promising results with high interpretability, it suffers from lower knowledge graph accuracy due to its lack of temporality and limited generalizability to complex environments with objects with the same label. In order to address DGU's weaknesses while preserving its high interpretability, we propose the Temporal Discrete Graph Updater (TDGU), a novel neural network model that represents dynamic knowledge graphs as a sequence of timestamped graph events and models them using a temporal point based graph neural network. Through experiments on the dataset collected from a text-based game TextWorld, we show that TDGU outperforms the baseline DGU. We furthe
    
[^9]: GateLoop: 完全数据控制的线性递归用于序列建模

    GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling. (arXiv:2311.01927v1 [cs.LG])

    [http://arxiv.org/abs/2311.01927](http://arxiv.org/abs/2311.01927)

    GateLoop是一种完全数据控制的线性递归序列模型，优于现有模型，可以提供数据控制的相对位置信息给Attention。

    

    线性递归已被证明是一种有效建模长序列的强大工具。在这项工作中，我们表明现有模型未能充分利用其潜力。在这一发现的基础上，我们开发了GateLoop，这是一种基础性的序列模型，通过使用数据控制的状态转换来推广线性递归模型，如S4、S5、LRU和RetNet。利用这一理论进步，GateLoop在自回归语言建模方面在实证上优于现有模型。我们的方法具有低成本的$O(l)$递归模式和高度优化的关联扫描实现的高效$O(l \log_{2} l)$并行模式。此外，我们还推导出了一个$O(l^2)$的代理注意力模式，揭示了对Transformer和最近提出的架构的显著影响。具体而言，我们证明了我们的方法可以被解释为向Attention提供数据控制的相对位置信息。而许多现有模型仅依赖于数据无关的位置信息。

    Linear Recurrence has proven to be a powerful tool for modeling long sequences efficiently. In this work, we show that existing models fail to take full advantage of its potential. Motivated by this finding, we develop GateLoop, a foundational sequence model that generalizes linear recurrent models such as S4, S5, LRU and RetNet, by employing data-controlled state transitions. Utilizing this theoretical advance, GateLoop empirically outperforms existing models for auto-regressive language modeling. Our method comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \log_{2} l)$ parallel mode making use of highly optimized associative scan implementations. Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing remarkable implications for Transformer and recently proposed architectures. Specifically, we prove that our approach can be interpreted as providing data-controlled relative-positional information to Attention. While many existing models solely rely on da
    
[^10]: 大型语言模型阐明了人工医疗助手的进展路径：一项综述

    Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review. (arXiv:2311.01918v1 [cs.CL])

    [http://arxiv.org/abs/2311.01918](http://arxiv.org/abs/2311.01918)

    本综述文章探讨了大型语言模型（LLMs）在医学中的应用和意义。LLMs在知识检索、研究支持、临床工作流自动化和诊断辅助方面具有巨大潜力，尤其是多模态LLMs可以处理医学影像和电子健康记录等多样化数据类型以增强诊断能力。

    

    随着人工智能的快速发展，大型语言模型（LLMs）展现了模拟人类级别语言理解和推理的潜力。这引发了将LLMs应用于增强医疗各个方面的重要兴趣，范围从医学教育到临床决策支持。然而，医学涉及多方面的数据模态和微妙的推理技能，这给LLMs的整合带来了挑战。本文综述了LLMs在医学中的应用和影响。首先，考察了通用型和专门化LLMs的基本应用，展示了它们在知识检索、研究支持、临床工作流自动化和诊断辅助方面的作用。鉴于医学的固有多模态特性，该综述进一步关注多模态LLMs，研究其处理医学影像和电子健康记录等多样化数据类型以增强诊断能力的能力。

    With the rapid development of artificial intelligence, large language models (LLMs) have shown promising capabilities in mimicking human-level language comprehension and reasoning. This has sparked significant interest in applying LLMs to enhance various aspects of healthcare, ranging from medical education to clinical decision support. However, medicine involves multifaceted data modalities and nuanced reasoning skills, presenting challenges for integrating LLMs. This paper provides a comprehensive review on the applications and implications of LLMs in medicine. It begins by examining the fundamental applications of general-purpose and specialized LLMs, demonstrating their utilities in knowledge retrieval, research support, clinical workflow automation, and diagnostic assistance. Recognizing the inherent multimodality of medicine, the review then focuses on multimodal LLMs, investigating their ability to process diverse data types like medical imaging and EHRs to augment diagnostic ac
    
[^11]: BoschAI @ PLABA 2023: 利用编辑操作在端到端神经句子简化中的应用

    BoschAI @ PLABA 2023: Leveraging Edit Operations in End-to-End Neural Sentence Simplification. (arXiv:2311.01907v1 [cs.CL])

    [http://arxiv.org/abs/2311.01907](http://arxiv.org/abs/2311.01907)

    本文描述了基于Llama 2的系统，在处理生物医学文本简化的PLABA共享任务中排名第一。通过引入句子级和标记级的损失权重，该系统能够产生与人工注释者相似的简化结果，语言更简单，并且进行更多的编辑操作。

    

    自动简化可以帮助普通人理解复杂的科学文本。语言模型经常用于将复杂语言转换为简单语言。本文中，我们描述了基于Llama 2的系统，该系统在处理生物医学文本简化的PLABA共享任务中排名第一。我们发现输入和输出之间共享的标记的数量很多，导致训练信号较弱和保守的编辑模型。为了缓解这些问题，我们提出了句子级和标记级的损失权重。它们给予修改的标记更高的权重，修改通过编辑距离和编辑操作进行指示。我们在PLABA数据集上进行了实证评估，并发现这两种方法都使简化结果更接近人工注释者创建的结果（+1.8% / +3.5% SARI），语言更简单（-1 / -1.1 FKGL），并且编辑更多（1.6x / 1.8x编辑距离），相比于使用标准交叉熵进行微调的相同模型。

    Automatic simplification can help laypeople to comprehend complex scientific text. Language models are frequently applied to this task by translating from complex to simple language. In this paper, we describe our system based on Llama 2, which ranked first in the PLABA shared task addressing the simplification of biomedical text. We find that the large portion of shared tokens between input and output leads to weak training signals and conservatively editing models. To mitigate these issues, we propose sentence-level and token-level loss weights. They give higher weight to modified tokens, indicated by edit distance and edit operations, respectively. We conduct an empirical evaluation on the PLABA dataset and find that both approaches lead to simplifications closer to those created by human annotators (+1.8% / +3.5% SARI), simpler language (-1 / -1.1 FKGL) and more edits (1.6x / 1.8x edit distance) compared to the same model fine-tuned with standard cross entropy. We furthermore show 
    
[^12]: 长篇讨论的指示性总结

    Indicative Summarization of Long Discussions. (arXiv:2311.01882v1 [cs.CL])

    [http://arxiv.org/abs/2311.01882](http://arxiv.org/abs/2311.01882)

    本文提出了一种利用大型语言模型生成指示性摘要的方法，能够帮助概览长篇讨论，并根据论述句进行聚类和分类生成摘要，这对于理解论坛讨论非常有用。

    

    在线论坛鼓励人们在许多主题上交流和讨论不同的立场。它们不仅提供了一个展示自己观点的机会，还可以汇集广泛的其他观点。然而，由此产生的长篇讨论很难概览。本文提出了一种利用大型语言模型（LLMs）生成长篇讨论指示性摘要的新型无监督方法，基本上作为目录。我们的方法首先对论述句进行聚类，生成聚类标签作为抽象摘要，然后将生成的聚类标签分类为论证框架，从而得到一个两级摘要。基于经过广泛优化的提示工程方法，我们评估了19个LLM用于生成聚类标签和框架分类。为了评估我们指示性摘要的实用性，我们通过一个名为Discussion Explorer的新的可视化界面进行了目标驱动的用户研究：结果表明我们的方法可以提供有用的指示性摘要。

    Online forums encourage the exchange and discussion of different stances on many topics. Not only do they provide an opportunity to present one's own arguments, but may also gather a broad cross-section of others' arguments. However, the resulting long discussions are difficult to overview. This paper presents a novel unsupervised approach using large language models (LLMs) to generating indicative summaries for long discussions that basically serve as tables of contents. Our approach first clusters argument sentences, generates cluster labels as abstractive summaries, and classifies the generated cluster labels into argumentation frames resulting in a two-level summary. Based on an extensively optimized prompt engineering approach, we evaluate 19~LLMs for generative cluster labeling and frame classification. To evaluate the usefulness of our indicative summaries, we conduct a purpose-driven user study via a new visual interface called Discussion Explorer: It shows that our proposed in
    
[^13]: 情感分析通过LLM谈判

    Sentiment Analysis through LLM Negotiations. (arXiv:2311.01876v1 [cs.CL])

    [http://arxiv.org/abs/2311.01876](http://arxiv.org/abs/2311.01876)

    这篇论文介绍了一种情感分析的多LLM谈判框架，通过迭代生成器和判别器的过程达到共识，以解决单个LLM无法做出完美决策的问题。

    

    情感分析的标准范式是依赖于一个单一的LLM，并在上下文学习的框架下在一轮中做出决策。然而，这个框架的关键劣势在于单个LLM生成的单次输出可能无法提供完美的决策，就像人类有时需要多次尝试才能做对一样。这在情感分析的任务中尤为明显，因为需要深入推理来解决输入中的复杂语言现象（如从句组合、讽刺等）。为了解决这个问题，本文引入了一种多LLM谈判框架用于情感分析。该框架包括一个带有推理的生成器来提供决策和解释，并一个用于评估生成器可信度的推导解释的判别器。生成器和判别器迭代直至达成共识。所提出的框架自然地解决了前述的挑战，因为我们能够

    A standard paradigm for sentiment analysis is to rely on a singular LLM and makes the decision in a single round under the framework of in-context learning. This framework suffers the key disadvantage that the single-turn output generated by a single LLM might not deliver the perfect decision, just as humans sometimes need multiple attempts to get things right. This is especially true for the task of sentiment analysis where deep reasoning is required to address the complex linguistic phenomenon (e.g., clause composition, irony, etc) in the input.  To address this issue, this paper introduces a multi-LLM negotiation framework for sentiment analysis. The framework consists of a reasoning-infused generator to provide decision along with rationale, a explanation-deriving discriminator to evaluate the credibility of the generator. The generator and the discriminator iterate until a consensus is reached. The proposed framework naturally addressed the aforementioned challenge, as we are able
    
[^14]: 高效黑盒对抗攻击神经文本检测器

    Efficient Black-Box Adversarial Attacks on Neural Text Detectors. (arXiv:2311.01873v1 [cs.CL])

    [http://arxiv.org/abs/2311.01873](http://arxiv.org/abs/2311.01873)

    本文研究了高效的黑盒对抗攻击神经文本检测器的方法，通过参数调整和字符级变异等策略，可以修改GPT-3.5生成的文本，使其对人类不可疑但能导致神经文本检测器误判。

    

    神经文本检测器是训练用于检测给定文本是由语言模型生成还是由人类编写的模型。本文研究了三种简单而资源高效的策略（参数调整，提示工程和字符级变异），用于修改由GPT-3.5生成的文本，这些修改对人类来说不可疑或不易察觉，但会导致神经文本检测器误分类。结果表明，特别是参数调整和字符级变异是有效的策略。

    Neural text detectors are models trained to detect whether a given text was generated by a language model or written by a human. In this paper, we investigate three simple and resource-efficient strategies (parameter tweaking, prompt engineering, and character-level mutations) to alter texts generated by GPT-3.5 that are unsuspicious or unnoticeable for humans but cause misclassification by neural text detectors. The results show that especially parameter tweaking and character-level mutations are effective strategies.
    
[^15]: 多语言欧洲议会数据集用于分析信息检索中的偏见

    Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval. (arXiv:2311.01870v1 [cs.CL])

    [http://arxiv.org/abs/2311.01870](http://arxiv.org/abs/2311.01870)

    Multi-EuP是一个新的多语言基准数据集，涵盖了来自欧洲议会的22K个多语言文档，旨在研究信息检索中的公平性，包括语言和人口偏见。它提供了真实的多语言语料库和跨语言相关性评判，并提供了与文档相关的丰富人口统计信息，可用于评估单语和多语信息检索。

    

    我们介绍了Multi-EuP，这是一个新的多语言基准数据集，包括来自欧洲议会的22K个多语言文档，涵盖了24种语言。该数据集旨在研究多语言信息检索（IR）环境下的公平性，以分析在排名上的语言和人口偏见。它拥有一个真实的多语言语料库，其中的主题被翻译成了所有24种语言，并提供跨语言相关性评判。此外，它还提供了与文档相关的丰富人口统计信息，便于研究人口偏见。我们报告了Multi-EuP在单语和多语信息检索基准测试中的有效性。我们还进行了一项关于标记化策略选择引起的语言偏见的初步实验。

    We present Multi-EuP, a new multilingual benchmark dataset, comprising 22K multi-lingual documents collected from the European Parliament, spanning 24 languages. This dataset is designed to investigate fairness in a multilingual information retrieval (IR) context to analyze both language and demographic bias in a ranking context. It boasts an authentic multilingual corpus, featuring topics translated into all 24 languages, as well as cross-lingual relevance judgments. Furthermore, it offers rich demographic information associated with its documents, facilitating the study of demographic bias. We report the effectiveness of Multi-EuP for benchmarking both monolingual and multilingual IR. We also conduct a preliminary experiment on language bias caused by the choice of tokenization strategy.
    
[^16]: 朝着概念感知的大型语言模型

    Towards Concept-Aware Large Language Models. (arXiv:2311.01866v1 [cs.CL])

    [http://arxiv.org/abs/2311.01866](http://arxiv.org/abs/2311.01866)

    本文研究了概念在语言模型中的作用，并探讨了开发概念感知语言模型的方法。通过预训练LLMs或使用现有LLMs的输出，我们证明了这种方法更好地符合人类直觉并改善了预测的鲁棒性。

    

    概念在各种人类认知功能中起着关键作用，包括学习、推理和交流。然而，目前对于赋予机器形成和推理概念的能力的研究非常有限。尤其是，目前的大型语言模型（LLMs）主要在词元级别上操作，而不是概念级别。本文分析了当代LLMs对人类概念及其结构的捕捉能力，并讨论了在不同阶段中开发概念感知LLMs的方法。我们提出了一种使用概念进行预训练的LLMs方法，并探讨了使用现有LLMs输出的更简单方法。尽管简单，我们的概念验证证明了更好地匹配人类直觉，并提升了预测的鲁棒性。这些初步结果显示了概念感知LLMs的潜力。

    Concepts play a pivotal role in various human cognitive functions, including learning, reasoning and communication. However, there is very little work on endowing machines with the ability to form and reason with concepts. In particular, state-of-the-art large language models (LLMs) work at the level of tokens, not concepts.  In this work, we analyze how well contemporary LLMs capture human concepts and their structure. We then discuss ways to develop concept-aware LLMs, taking place at different stages of the pipeline. We sketch a method for pretraining LLMs using concepts, and also explore the simpler approach that uses the output of existing LLMs. Despite its simplicity, our proof-of-concept is shown to better match human intuition, as well as improve the robustness of predictions. These preliminary results underscore the promise of concept-aware LLMs.
    
[^17]: SortNet: 通过神经网络排序算法进行学习排序

    SortNet: Learning To Rank By a Neural-Based Sorting Algorithm. (arXiv:2311.01864v1 [cs.LG])

    [http://arxiv.org/abs/2311.01864](http://arxiv.org/abs/2311.01864)

    SortNet是一种使用神经网络作为比较器来进行自适应排序的算法，通过迭代过程构建训练集，根据成对项目之间的排序示例来训练神经网络。

    

    关于相关性排名的问题，即根据给定的标准对一组对象进行排序。由于用户可能偏好不同的相关性标准，因此排序算法应该能够根据用户需求进行调整。学习排序的任务在文献中存在两种主要方法：1）通过示例学习的得分函数，评估每个对象的属性，生成可用于对对象进行排序的绝对相关性值；2）一种成对方法，通过使用对象对来学习“偏好函数”，定义哪一个对象应该首先排名。在本文中，我们提出了SortNet，一种使用神经网络作为比较器来对对象进行自适应排序的算法。神经网络的训练集提供了对于成对项目之间所需排序的示例，并且通过迭代过程构建，每次迭代都会添加最具信息性的训练示例。此外，比较器采用了连接主义体系结构。

    The problem of relevance ranking consists of sorting a set of objects with respect to a given criterion. Since users may prefer different relevance criteria, the ranking algorithms should be adaptable to the user needs. Two main approaches exist in literature for the task of learning to rank: 1) a score function, learned by examples, which evaluates the properties of each object yielding an absolute relevance value that can be used to order the objects or 2) a pairwise approach, where a "preference function" is learned using pairs of objects to define which one has to be ranked first. In this paper, we present SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. The neural network training set provides examples of the desired ordering between pairs of items and it is constructed by an iterative procedure which, at each iteration, adds the most informative training examples. Moreover, the comparator adopts a connectionist architecture that 
    
[^18]: $R^3$-NL2GQL:一种用于提高准确性和减轻幻觉的混合模型方法

    $R^3$-NL2GQL: A Hybrid Models Approach for for Accuracy Enhancing and Hallucinations Mitigation. (arXiv:2311.01862v1 [cs.CL])

    [http://arxiv.org/abs/2311.01862](http://arxiv.org/abs/2311.01862)

    $R^3$-NL2GQL是一种通过利用较小和较大的Foundation Models进行重新排名、重写和细化的方法，以提高准确性和减轻幻觉，解决了NL2GQL任务中GQL生成能力和跨模式通用能力的挑战。

    

    当前使用Foundation Models构建的NL2SQL任务取得了令人称赞的结果，然而直接将其应用于自然语言到图查询语言（NL2GQL）任务面临挑战，原因是GQL和SQL表达式之间存在显著差异，且GQL存在多种类型。我们的实验表明，在NL2GQL任务中，更大的Foundation Models展示了优越的跨模式通用能力，而较小的Foundation Models则通过微调难以提高其GQL生成能力。然而，在微调后，较小的模型表现出更好的意图理解和更高的语法准确性。与基于规则和槽填充技术不同，我们引入了R3-NL2GQL，该方法将较小和较大的Foundation Models用作重新排名、重写和细化器。该方法利用较小模型的理解能力进行信息的重新排名和重写，并利用卓越的通用化和生成能力进行细化。

    While current NL2SQL tasks constructed using Foundation Models have achieved commendable results, their direct application to Natural Language to Graph Query Language (NL2GQL) tasks poses challenges due to the significant differences between GQL and SQL expressions, as well as the numerous types of GQL. Our extensive experiments reveal that in NL2GQL tasks, larger Foundation Models demonstrate superior cross-schema generalization abilities, while smaller Foundation Models struggle to improve their GQL generation capabilities through fine-tuning. However, after fine-tuning, smaller models exhibit better intent comprehension and higher grammatical accuracy. Diverging from rule-based and slot-filling techniques, we introduce R3-NL2GQL, which employs both smaller and larger Foundation Models as reranker, rewriter and refiner. The approach harnesses the comprehension ability of smaller models for information reranker and rewriter, and the exceptional generalization and generation capabiliti
    
[^19]: FAME：灵活可扩展的类比映射引擎

    FAME: Flexible, Scalable Analogy Mappings Engine. (arXiv:2311.01860v1 [cs.CL])

    [http://arxiv.org/abs/2311.01860](http://arxiv.org/abs/2311.01860)

    这项工作提出了一个灵活可扩展的类比映射引擎，通过自动提取常识表示，并使用这些表示来确定实体之间的映射关系。与以往方法不同的是，该引擎可以处理部分类比，并提供新实体的建议。实验证明其在经典2x2类比问题上的准确率为81.2％，在更大的问题上为77.8％，此外，该引擎还优于人类表现。

    

    类比是人类认知的核心能力之一；在面对新情境时，我们经常从其他领域中转移先前的经验。大多数关于计算类比的工作都严重依赖复杂的手工制作输入。在本研究中，我们放松了输入要求，只需对实体进行映射。我们自动提取常识表示，并使用它们来确定实体之间的映射关系。与以前的方法不同，我们的框架可以处理部分类比，并建议添加新的实体。此外，我们的方法的输出易于解释，用户可以理解为什么选择了特定的映射。实验证明，我们的模型正确地映射了81.2％的经典2x2类比问题（猜测水平=50％）。在更大的问题上，它的准确率达到77.8％（平均猜测水平=13.1％）。在另一个实验中，我们展示了我们的算法胜过了人类的表现，并且自动建议的新实体类似于人类建议的实体。

    Analogy is one of the core capacities of human cognition; when faced with new situations, we often transfer prior experience from other domains. Most work on computational analogy relies heavily on complex, manually crafted input. In this work, we relax the input requirements, requiring only names of entities to be mapped. We automatically extract commonsense representations and use them to identify a mapping between the entities. Unlike previous works, our framework can handle partial analogies and suggest new entities to be added. Moreover, our method's output is easily interpretable, allowing for users to understand why a specific mapping was chosen.  Experiments show that our model correctly maps 81.2% of classical 2x2 analogy problems (guess level=50%). On larger problems, it achieves 77.8% accuracy (mean guess level=13.1%). In another experiment, we show our algorithm outperforms human performance, and the automatic suggestions of new entities resemble those suggested by humans. 
    
[^20]: 大型语言模型拯救行动：使用ChatGPT减少科学工作流开发的复杂性

    Large Language Models to the Rescue: Reducing the Complexity in Scientific Workflow Development Using ChatGPT. (arXiv:2311.01825v1 [cs.DC])

    [http://arxiv.org/abs/2311.01825](http://arxiv.org/abs/2311.01825)

    本研究调查了使用大型语言模型ChatGPT支持科学工作流时的效率。通过在两个科学领域进行三个用户研究，研究结果显示LLMs能够高效解释工作流，但在组件交换和有目的工作方面的性能较低。

    

    科学工作流系统越来越受欢迎，用于在大型数据集上表达和执行复杂的数据分析流程，因为它们通过在大型计算集群上的自动并行化提供了可复制性、可靠性和可扩展性的分析。然而，由于涉及许多黑盒工具和必要的深层基础设施栈，实现工作流非常困难。与此同时，用户支持工具很少，并且可用示例的数量远远低于传统编程语言。为了解决这些挑战，我们研究了大型语言模型（LLMs），特别是ChatGPT，在处理科学工作流时支持用户的效率。我们在两个科学领域进行了三个用户研究，评估了ChatGPT的理解、适应和扩展工作流的能力。我们的结果表明，LLMs可以有效解释工作流，但在交换组件或有目的的工作方面性能较低。

    Scientific workflow systems are increasingly popular for expressing and executing complex data analysis pipelines over large datasets, as they offer reproducibility, dependability, and scalability of analyses by automatic parallelization on large compute clusters. However, implementing workflows is difficult due to the involvement of many black-box tools and the deep infrastructure stack necessary for their execution. Simultaneously, user-supporting tools are rare, and the number of available examples is much lower than in classical programming languages. To address these challenges, we investigate the efficiency of Large Language Models (LLMs), specifically ChatGPT, to support users when dealing with scientific workflows. We performed three user studies in two scientific domains to evaluate ChatGPT for comprehending, adapting, and extending workflows. Our results indicate that LLMs efficiently interpret workflows but achieve lower performance for exchanging components or purposeful wo
    
[^21]: 极简语法：无过度生成的构造

    Minimalist Grammar: Construction without Overgeneration. (arXiv:2311.01820v1 [cs.CL])

    [http://arxiv.org/abs/2311.01820](http://arxiv.org/abs/2311.01820)

    该论文介绍了如何编写极简语法，并利用许可者/-被许可者来处理异常情况，避免过度生成。

    

    本文介绍了如何编写极简语法（MG）的指南。为了将指南呈现为一种算法，我们使用上下文无关文法（CFG）的一种变体作为输入格式。如果CFG没有递归，即没有非终结符可以（间接）导出包含自身的右手边，则可以排除过度生成。构建的MG利用许可者/-被许可者作为特殊的异常处理方式。在一个推导$A\_eats\_B\mapsto^* peter\_eats\_apples$的CFG格式中，其中$A$和$B$生成名词短语，通常会导致过度生成，例如$i\_eats\_apples$。为了避免过度生成，CFG需要许多非终结符和规则，主要产生相同的单词，以处理异常情况。然而，在我们的MG中，我们可以将产生相同单词的CFG规则总结为一个项目，并通过适当分配许可者/-被许可者来处理异常情况。使用这种技术的困难在于，在大多数生成过程中，许多许可者/-被许可者无效

    In this paper we give instructions on how to write a minimalist grammar (MG). In order to present the instructions as an algorithm, we use a variant of context free grammars (CFG) as an input format. We can exclude overgeneration, if the CFG has no recursion, i.e. no non-terminal can (indirectly) derive to a right-hand side containing itself. The constructed MGs utilize licensors/-ees as a special way of exception handling. A CFG format for a derivation $A\_eats\_B\mapsto^* peter\_eats\_apples$, where $A$ and $B$ generate noun phrases, normally leads to overgeneration, e.\,g., $i\_eats\_apples$. In order to avoid overgeneration, a CFG would need many non-terminal symbols and rules, that mainly produce the same word, just to handle exceptions. In our MGs however, we can summarize CFG rules that produce the same word in one item and handle exceptions by a proper distribution of licensees/-ors. The difficulty with this technique is that in most generations the majority of licensees/-ors i
    
[^22]: 使用极性最小化损失减轻框架偏见

    Mitigating Framing Bias with Polarity Minimization Loss. (arXiv:2311.01817v1 [cs.CL])

    [http://arxiv.org/abs/2311.01817](http://arxiv.org/abs/2311.01817)

    该论文提出了一种新的损失函数，在输入的极化文章之间减小极性差异，以减轻框架偏见。实验证明，这种方法可以显著减少框架偏见，尤其是在训练模型以最小化信息编框偏见的极性损失时。

    

    框架偏见通过扭曲实际事件的感知，加剧了政治极化。持有不同政治立场的媒体机构往往在报道相同事件时使用极化语言。我们提出了一种新的损失函数，鼓励模型在输入的极化文章之间减小极性差异，以减轻框架偏见。具体而言，我们的损失函数旨在同时优化模型以双向映射极性的两个极点。我们的实验结果表明，引入提出的极性最小化损失可以显著减少框架偏见，与基于BART的多文档摘要模型相比。值得注意的是，我们发现当模型被训练以最小化与信息编框偏见（即选择性报道信息）相关的极性损失时，这种方法的效果最为显著。

    Framing bias plays a significant role in exacerbating political polarization by distorting the perception of actual events. Media outlets with divergent political stances often use polarized language in their reporting of the same event. We propose a new loss function that encourages the model to minimize the polarity difference between the polarized input articles to reduce framing bias. Specifically, our loss is designed to jointly optimize the model to map polarity ends bidirectionally. Our experimental results demonstrate that incorporating the proposed polarity minimization loss leads to a substantial reduction in framing bias when compared to a BART-based multi-document summarization model. Notably, we find that the effectiveness of this approach is most pronounced when the model is trained to minimize the polarity loss associated with informational framing bias (i.e., skewed selection of information to report).
    
[^23]: AFPQ：面向LLMs的非对称浮点量化

    AFPQ: Asymmetric Floating Point Quantization for LLMs. (arXiv:2311.01792v1 [cs.CL])

    [http://arxiv.org/abs/2311.01792](http://arxiv.org/abs/2311.01792)

    AFPQ提出了一种面向LLMs的非对称浮点量化方法，通过为正值和负值设置不同的比例尺，显著提高了准确性，并且可以与其他量化方法相结合，无需额外存储空间。

    

    大型语言模型（LLMs）在各种任务中表现出色，但面临有限的内存容量和带宽的部署挑战。低位权重量化可以节省内存并加速推断。尽管浮点（FP）格式在LLM量化中表现出良好性能，但它们在小组大小或子4位时往往表现不佳。我们发现，之前的FP量化缺乏不对称性，不适合处理LLM权重张量的不对称值分布。在这项工作中，我们提出了非对称FP量化（AFPQ），为正值和负值设置了分别的比例尺。我们的方法显著提高了准确性，并可以轻松地插入其他量化方法，包括GPTQ和AWQ，以获得更好的性能。此外，与非对称整数（INT）量化相比，不需要额外的存储空间。代码可在https://github.com/zhangsichengsjtu/AFPQ获得。

    Large language models (LLMs) show great performance in various tasks, but face deployment challenges from limited memory capacity and bandwidth. Low-bit weight quantization can save memory and accelerate inference. Although floating-point (FP) formats show good performance in LLM quantization, they tend to perform poorly with small group sizes or sub-4 bits. We find the reason is that the absence of asymmetry in previous FP quantization makes it unsuitable for handling asymmetric value distribution of LLM weight tensors. In this work, we propose asymmetric FP quantization (AFPQ), which sets separate scales for positive and negative values. Our method leads to large accuracy improvements and can be easily plugged into other quantization methods, including GPTQ and AWQ, for better performance. Besides, no additional storage is needed compared with asymmetric integer (INT) quantization. The code is available at https://github.com/zhangsichengsjtu/AFPQ.
    
[^24]: TCM-GPT:用于传统中医领域适应的大型语言模型的高效预训练

    TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine. (arXiv:2311.01786v1 [cs.CL])

    [http://arxiv.org/abs/2311.01786](http://arxiv.org/abs/2311.01786)

    TCM-GPT是一种用于传统中医领域适应的大型语言模型的高效预训练方法，通过构建一个传统中医专用语料库进行预训练，取得了良好的效果。

    

    预训练和微调已成为各种自然语言处理任务中的一种有前途的范式。预训练的大型语言模型（LLM）的有效性得到了进一步提升，并具有在医学领域，特别是传统中医领域应用的潜力。然而，将这些通用模型应用于特定领域往往产生次优结果，主要是由于缺乏领域知识、独特目标和计算效率等挑战。此外，它们在专业领域（如传统中医）的有效性需要进行全面评估。为了解决上述问题，我们提出了一种新颖的领域特定的TCMDA（传统中医领域适应）方法，即利用领域特定语料库进行高效预训练。具体而言，我们首先通过识别领域关键词并从通用语料库中检索，构建了一个大型的传统中医特定语料库TCM-Corpus-1B。然后，我们的TCMDA方法使用这个语料库进行预训练。通过实验验证了我们方法的有效性。

    Pre-training and fine-tuning have emerged as a promising paradigm across various natural language processing (NLP) tasks. The effectiveness of pretrained large language models (LLM) has witnessed further enhancement, holding potential for applications in the field of medicine, particularly in the context of Traditional Chinese Medicine (TCM). However, the application of these general models to specific domains often yields suboptimal results, primarily due to challenges like lack of domain knowledge, unique objectives, and computational efficiency. Furthermore, their effectiveness in specialized domains, such as Traditional Chinese Medicine, requires comprehensive evaluation. To address the above issues, we propose a novel domain specific TCMDA (TCM Domain Adaptation) approach, efficient pre-training with domain-specific corpus. Specifically, we first construct a large TCM-specific corpus, TCM-Corpus-1B, by identifying domain keywords and retreving from general corpus. Then, our TCMDA 
    
[^25]: UP4LS: 由多个属性构建的用户信息用于增强语言隐写分析

    UP4LS: User Profile Constructed by Multiple Attributes for Enhancing Linguistic Steganalysis. (arXiv:2311.01775v1 [cs.CL])

    [http://arxiv.org/abs/2311.01775](http://arxiv.org/abs/2311.01775)

    本文提出了一个名为UP4LS的新框架，通过多个属性构建用户信息以增强语言隐写分析的性能。实验结果显示，该框架可以有效提升隐写分析的性能。

    

    语言隐写分析任务旨在有效检测通过语言隐写术生成的隐写物。现有的隐写分析方法忽视了用户个性化特征，导致在社交网络中表现较差。隐写物的有限出现进一步增加了检测的复杂性。本文提出了一个新颖的框架 UP4LS，用于增强隐写分析性能，该框架采用用户信息为核心。具体来说，通过深入分析帖子内容，我们探索了用户属性，如写作习惯、心理状态和关注领域，从而为隐写分析构建了用户信息。对于每个属性，我们设计了特征提取模块。通过现有方法中的深度学习网络，将提取到的特征映射到高维用户特征上。然后，使用语言模型来提取内容特征。将用户和内容特征进行集成以优化特征表示。在训练阶段，我们优先考虑隐写物的分布。实验表明，UP4LS 可以有效提升隐写分析的性能。

    Linguistic steganalysis (LS) tasks aim to effectively detect stegos generated by linguistic steganography. Existing LS methods overlook the distinctive user characteristics, leading to weak performance in social networks. The limited occurrence of stegos further complicates detection. In this paper, we propose the UP4LS, a novel framework with the User Profile for enhancing LS performance. Specifically, by delving into post content, we explore user attributes like writing habits, psychological states, and focal areas, thereby building the user profile for LS. For each attribute, we design the identified feature extraction module. The extracted features are mapped to high-dimensional user features via deep-learning networks from existing methods. Then the language model is employed to extract content features. The user and content features are integrated to optimize feature representation. During the training phase, we prioritize the distribution of stegos. Experiments demonstrate that 
    
[^26]: PPTC基准：评估大型语言模型在PowerPoint任务完成中的表现

    PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion. (arXiv:2311.01767v1 [cs.CL])

    [http://arxiv.org/abs/2311.01767](http://arxiv.org/abs/2311.01767)

    这个论文介绍了PPTC基准，用来评估大型语言模型在根据用户指令创建和编辑PPT文件方面的表现。通过测试，发现GPT-4在准确率方面表现最好，为75.1%。

    

    近期对大型语言模型（LLM）的评估主要集中在测试它们对基本自然语言任务的零次/少次尝试能力以及将指令翻译成工具API的能力上。然而，对于利用复杂工具完成复杂多轮、多模态指令的LLM的评估尚未进行研究。为了填补这个空白，我们引入了PowerPoint任务完成（PPTC）基准，评估LLM根据用户指令创建和编辑PPT文件的能力。它包含279个涵盖不同主题的多轮对话，涉及多模态操作的数百个指令。我们还提出了PPTX-Match评估系统，该系统根据预测文件而不是标签API序列来评估LLM是否完成了指令，因此支持各种LLM生成的API序列。我们测试了3个闭合型LLM和6个开源LLM。结果表明，GPT-4在准确率方面优于其他LLM，达到了75.1%。

    Recent evaluations of Large Language Models (LLMs) have centered around testing their zero-shot/few-shot capabilities for basic natural language tasks and their ability to translate instructions into tool APIs. However, the evaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal instructions in a complex multi-modal environment has not been investigated. To address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark to assess LLMs' ability to create and edit PPT files based on user instructions. It contains 279 multi-turn sessions covering diverse topics and hundreds of instructions involving multi-modal operations. We also propose the PPTX-Match Evaluation System that evaluates if LLMs finish the instruction based on the prediction file rather than the label API sequence, thus it supports various LLM-generated API sequences. We measure 3 closed LLMs and 6 open-source LLMs. The results show that GPT-4 outperforms other LLMs with 75.1\% accuracy i
    
[^27]: 支持还是反驳：分析证据立场以检测上下文错误的误导信息

    Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation. (arXiv:2311.01766v1 [cs.CL])

    [http://arxiv.org/abs/2311.01766](http://arxiv.org/abs/2311.01766)

    本研究提出了一种基于多模态证据的立场抽取网络（SEN）来检测上下文错误的误导信息。通过考虑不同证据的立场，我们提供了一种更准确的检测方法，并引入了基于共现关系的支持-反驳分数。这种方法在公共大规模数据上进行的实验证明了其有效性。

    

    在线误导信息已经成为一个国家级的社会问题，是各种在线伤害的主要来源之一。其中一种常见的误导信息形式是上下文错误（OOC）信息，其中不同的信息被错误地关联起来，例如真实图像与虚假的文本标题或误导性的文本描述。尽管一些研究试图通过外部证据来抵御上下文错误的误导信息，但它们往往忽视了不同立场的不同证据的作用。受到证据立场代表不同检测结果的偏见的启发，我们提出了一种能够在统一框架中提取多模态证据的立场的立场抽取网络（SEN）。此外，我们还引入了基于命名实体的共现关系计算的支持-反驳分数到文本SEN中。对公共大规模数据的大量实验证明了我们的方法的有效性。

    Mis- and disinformation online have become a major societal problem as major sources of online harms of different kinds. One common form of mis- and disinformation is out-of-context (OOC) information, where different pieces of information are falsely associated, e.g., a real image combined with a false textual caption or a misleading textual description. Although some past studies have attempted to defend against OOC mis- and disinformation through external evidence, they tend to disregard the role of different pieces of evidence with different stances. Motivated by the intuition that the stance of evidence represents a bias towards different detection results, we propose a stance extraction network (SEN) that can extract the stances of different pieces of multi-modal evidence in a unified framework. Moreover, we introduce a support-refutation score calculated based on the co-occurrence relations of named entities into the textual SEN. Extensive experiments on a public large-scale data
    
[^28]: Indo LEGO-ABSA：一种针对印度尼西亚语的多任务生成式基于方面的情感分析方法

    Indo LEGO-ABSA: A Multitask Generative Aspect Based Sentiment Analysis for Indonesian Language. (arXiv:2311.01757v1 [cs.CL])

    [http://arxiv.org/abs/2311.01757](http://arxiv.org/abs/2311.01757)

    本研究旨在针对印度尼西亚语，利用生成式预训练语言模型实现多任务生成式基于方面的情感分析方法，并开发了Indo LEGO-ABSA模型。

    

    方面级情感分析是一种自然语言处理方法，旨在识别和理解与实体的特定方面相关的情感。前期研究已经利用生成式预训练语言模型进行方面级情感分析。LEGO-ABSA是一个成功利用生成式预训练语言模型进行方面级情感分析的框架，特别在英文中。LEGO-ABSA利用多任务学习和提示方法来提高模型性能。然而，在印度尼西亚语环境中尚未应用该方法。因此，本研究旨在利用生成式预训练语言模型在印度尼西亚语的方面级情感分析中实现多任务学习和提示方法。在本研究中，开发了Indo LEGO-ABSA模型，该模型是一个基于方面的情感分析模型。

    Aspect-based sentiment analysis is a method in natural language processing aimed at identifying and understanding sentiments related to specific aspects of an entity. Aspects are words or phrases that represent an aspect or attribute of a particular entity. Previous research has utilized generative pre-trained language models to perform aspect-based sentiment analysis. LEGO-ABSA is one framework that has successfully employed generative pre-trained language models in aspect-based sentiment analysis, particularly in English. LEGO-ABSA uses a multitask learning and prompting approach to enhance model performance. However, the application of this approach has not been done in the context of Bahasa Indonesia. Therefore, this research aims to implement the multitask learning and prompting approach in aspect-based sentiment analysis for Bahasa Indonesia using generative pre-trained language models. In this study, the Indo LEGO-ABSA model is developed, which is an aspect-based sentiment analy
    
[^29]: EmojiLM: 对新的表情符号语言进行建模

    EmojiLM: Modeling the New Emoji Language. (arXiv:2311.01751v1 [cs.CL])

    [http://arxiv.org/abs/2311.01751](http://arxiv.org/abs/2311.01751)

    EmojiLM是一个专门处理文本-表情符号双向翻译的模型，通过合成大规模的文本-表情符号并行语料库Text2Emoji进行训练，在公共基准测试和人工评估中表现出优异的性能，对于与表情符号相关的下游任务有益。

    

    随着互联网的快速发展，在线社交媒体通过其多样的内容迎来了来自不同背景的人们。表情符号的使用越来越多，得益于表情符号在文化或语言边界之外的丰富信息。然而，目前关于表情符号的研究仅限于单个表情符号的预测，并且有限的数据资源可用于进一步研究这一有趣的语言现象。为此，我们从大型语言模型中合成了一个大规模的文本-表情符号并行语料库Text2Emoji。基于这个并行语料库，我们提取了一个序列到序列模型EmojiLM，专门用于文本-表情符号的双向翻译。公共基准测试和人工评估的大量实验表明，我们提出的模型优于强基准线，并且并行语料库对于与表情符号相关的下游任务有益。

    With the rapid development of the internet, online social media welcomes people with different backgrounds through its diverse content. The increasing usage of emoji becomes a noticeable trend thanks to emoji's rich information beyond cultural or linguistic borders. However, the current study on emojis is limited to single emoji prediction and there are limited data resources available for further study of the interesting linguistic phenomenon. To this end, we synthesize a large text-emoji parallel corpus, Text2Emoji, from a large language model. Based on the parallel corpus, we distill a sequence-to-sequence model, EmojiLM, which is specialized in the text-emoji bidirectional translation. Extensive experiments on public benchmarks and human evaluation demonstrate that our proposed model outperforms strong baselines and the parallel corpus benefits emoji-related downstream tasks.
    
[^30]: SAC^3: 基于语义感知交叉检查一致性的黑盒语言模型可靠幻觉检测

    SAC$^3$: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency. (arXiv:2311.01740v1 [cs.CL])

    [http://arxiv.org/abs/2311.01740](http://arxiv.org/abs/2311.01740)

    SAC^3是一种基于语义感知交叉检查一致性的方法，可以可靠地检测黑盒语言模型中的幻觉。该方法通过加入语义等效问题扰动和跨模型响应一致性检查等机制，能够有效识别问题级别和模型级别的幻觉。实证分析表明，SAC^3在检测非事实和事实陈述方面优于现有技术最新水平。

    

    幻觉检测是了解现代语言模型可信度的关键步骤。为了实现这一目标，我们重新审视了基于语言模型自一致性的现有检测方法，并发现了两种幻觉类型，即基于问题和基于模型的幻觉，它们仅通过自一致性检查无法有效识别。基于这一发现，我们提出了一种新的基于采样的方法，即语义感知交叉检查一致性（SAC^3），它在自一致性检查原则的基础上加入了额外的机制，通过利用语义等效问题扰动和跨模型响应一致性检查来检测问题级别和模型级别的幻觉。通过广泛而系统的实证分析，我们证明SAC^3在检测非事实和事实陈述方面胜过了现有技术的最新水平。

    Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC$^3$) that expands on the principle of self-consistency checking. Our SAC$^3$ approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC$^3$ outperforms the state of the art in detecting both non-factual and factual statements across multip
    
[^31]: Proto-lm：一种基于原型网络的大型语言模型内置可解释性框架

    Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models. (arXiv:2311.01732v1 [cs.CL])

    [http://arxiv.org/abs/2311.01732](http://arxiv.org/abs/2311.01732)

    Proto-lm是一种基于原型网络的大型语言模型（LLM）内置可解释性框架，通过在微调阶段学习可解释的嵌入来提供解释性，同时保持竞争性能。该方法为创建可解释性模型提供了新的可能性。

    

    大型语言模型（LLM）在自然语言处理（NLP）领域有显著进展，但其缺乏可解释性是一个主要关注点。目前用于解释LLMs的方法是事后的，在推理时间之后应用，并且存在一些限制，比如它们关注低级特征并且在更高级文本单位上缺乏可解释性。在这项工作中，我们引入了proto-lm，这是一个基于原型网络的白盒子框架，允许LLMs在微调阶段学习即时可解释的嵌入，同时保持具有竞争力的性能。通过对各种NLP任务的实验，我们证明了我们方法的适用性和可解释性，并且我们的结果表明了在不牺牲性能的情况下创建可解释性模型的新可能性。这种在LLMs中的新颖解释性方法可以为无需牺牲性能的更可解释性模型铺平道路。

    Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP), but their lack of interpretability has been a major concern. Current methods for interpreting LLMs are post hoc, applied after inference time, and have limitations such as their focus on low-level features and lack of explainability at higher level text units. In this work, we introduce proto-lm, a prototypical network-based white-box framework that allows LLMs to learn immediately interpretable embeddings during the fine-tuning stage while maintaining competitive performance. Our method's applicability and interpretability are demonstrated through experiments on a wide range of NLP tasks, and our results indicate a new possibility of creating interpretable models without sacrificing performance. This novel approach to interpretability in LLMs can pave the way for more interpretable models without the need to sacrifice performance.
    
[^32]: 对中国方面情感四元预测进行基准测试的实证研究

    An Empirical Study of Benchmarking Chinese Aspect Sentiment Quad Prediction. (arXiv:2311.01713v1 [cs.CL])

    [http://arxiv.org/abs/2311.01713](http://arxiv.org/abs/2311.01713)

    本研究构建了两个大规模的中国ASQP数据集，对生成式预训练变压器（GPT）系列模型在ASQP上的性能进行了评估，并展示了改进ASQP技术和提高GPT性能的重要性。

    

    方面情感四元预测（ASQP）是方面级情感分析的一个关键子任务。目前的ASQP数据集特点是规模小且四元组密度低，这阻碍了技术的发展。为了扩大容量，我们构建了两个大规模的中国ASQP数据集，从多个在线平台收集。这些数据集具有几个显著的特点：更大的规模（每个数据集都有10,000+个样本），丰富的方面类别，每个句子更多的词数以及比现有ASQP数据集更高的密度。此外，我们首次评估了生成式预训练变压器（GPT）系列模型在ASQP上的性能，并展示了潜在的问题。与最先进的ASQP基准线实验强调了需要探索额外技术来解决ASQP的需求，以及进一步研究改进GPT性能的方法的重要性。

    Aspect sentiment quad prediction (ASQP) is a critical subtask of aspect-level sentiment analysis. Current ASQP datasets are characterized by their small size and low quadruple density, which hinders technical development. To expand capacity, we construct two large Chinese ASQP datasets crawled from multiple online platforms. The datasets hold several significant characteristics: larger size (each with 10,000+ samples) and rich aspect categories, more words per sentence, and higher density than existing ASQP datasets. Moreover, we are the first to evaluate the performance of Generative Pre-trained Transformer (GPT) series models on ASQP and exhibit potential issues. The experiments with state-of-the-art ASQP baselines underscore the need to explore additional techniques to address ASQP, as well as the importance of further investigation into methods to improve the performance of GPTs.
    
[^33]: 一种用于识别在线新闻中政治意图的新韩文文本分类基准（arXiv:2311.01712v1 [cs.CL]）

    A New Korean Text Classification Benchmark for Recognizing the Political Intents in Online Newspapers. (arXiv:2311.01712v1 [cs.CL])

    [http://arxiv.org/abs/2311.01712](http://arxiv.org/abs/2311.01712)

    这项工作提出了一个用于自动识别韩国在线报纸中政治意图的新的韩文文本分类数据集，该数据集包含12000篇新闻文章，并提供了基于深度学习的分类模型。这是目前最大规模的韩国新闻数据集，可以处理长文本和多任务分类问题。

    

    许多用户在各种杂志上阅读在线文章时，可能会在区分文本中的隐含意图方面遇到困难。在这项工作中，我们通过理解文本的语境，专注于自动识别给定在线报纸的政治意图。为了解决这个任务，我们提出了一个包含各种文章的新颖的韩文文本分类数据集。我们还提供了基于深度学习的文本分类基准模型，这些模型是在所提出的数据集上训练得到的。我们的数据集包含来自韩国六家最具代表性的报纸组织政治栏目的12000篇新闻文章，这些文章可能包含有政治意图。所有的文本样本同时以两个方面进行标记（1）政治倾向的水平和（2）亲政府的水平。据我们所知，我们的论文是包含长文本并解决多任务分类问题的最大规模的韩国新闻数据集。我们还训练了最近的状态。

    Many users reading online articles in various magazines may suffer considerable difficulty in distinguishing the implicit intents in texts. In this work, we focus on automatically recognizing the political intents of a given online newspaper by understanding the context of the text. To solve this task, we present a novel Korean text classification dataset that contains various articles. We also provide deep-learning-based text classification baseline models trained on the proposed dataset. Our dataset contains 12,000 news articles that may contain political intentions, from the politics section of six of the most representative newspaper organizations in South Korea. All the text samples are labeled simultaneously in two aspects (1) the level of political orientation and (2) the level of pro-government. To the best of our knowledge, our paper is the most large-scale Korean news dataset that contains long text and addresses multi-task classification problems. We also train recent state-
    
[^34]: 无数据的语言模型蒸馏：通过文本到文本转换实现

    Data-Free Distillation of Language Model by Text-to-Text Transfer. (arXiv:2311.01689v1 [cs.CL])

    [http://arxiv.org/abs/2311.01689](http://arxiv.org/abs/2311.01689)

    本文提出了一种无数据知识蒸馏（DFKD）框架，即DFKD-T$^{3}$，利用预训练的生成式语言模型作为数据生成器，将通用领域语料库转化为压缩友好的任务数据。实验证明该方法能够提升各种下游任务的蒸馏性能。

    

    当原始训练数据不可用时，无数据知识蒸馏（DFKD）在压缩模型方面起着至关重要的作用。先前在自然语言处理领域对DFKD的研究主要集中在对类别任务进行蒸馏的仅编码器结构，忽视了生成式语言建模的重要进展。在本研究中，我们提出了一种新颖的DFKD框架，名为DFKD-T$^{3}$，预训练的生成式语言模型也可以作为一个可控的数据生成器，用于模型压缩。这个新颖的DFKD-T$^{3}$框架导致了一个端到端可学习的文本到文本框架，将通用领域语料库转化为压缩友好的任务数据，旨在提高模型的特异性和多样性。大量实验证明我们的方法可以提升在情感分析、语言可接受性和信息提取等各种下游任务中的蒸馏性能。此外，我们还展示了生成的文本可以直接应用于...

    Data-Free Knowledge Distillation (DFKD) plays a vital role in compressing the model when original training data is unavailable. Previous works for DFKD in NLP mainly focus on distilling encoder-only structures like BERT on classification tasks, which overlook the notable progress of generative language modeling. In this work, we propose a novel DFKD framework, namely DFKD-T$^{3}$, where the pretrained generative language model can also serve as a controllable data generator for model compression. This novel framework DFKD-T$^{3}$ leads to an end-to-end learnable text-to-text framework to transform the general domain corpus to compression-friendly task data, targeting to improve both the \textit{specificity} and \textit{diversity}. Extensive experiments show that our method can boost the distillation performance in various downstream tasks such as sentiment analysis, linguistic acceptability, and information extraction. Furthermore, we show that the generated texts can be directly used 
    
[^35]: CASE: 带有扩展答案空间的常识增强评分机制

    CASE: Commonsense-Augmented Score with an Expanded Answer Space. (arXiv:2311.01684v1 [cs.CL])

    [http://arxiv.org/abs/2311.01684](http://arxiv.org/abs/2311.01684)

    我们提出了一个带有扩展答案空间和常识增强评分机制（CASE），它通过根据输入中单词的语义关系分配重要性权重来改善多项选择问答任务中的评分机制。我们还通过生成概念上类似于选项的词汇不同的答案来进一步扩展答案空间。这一方法在降低噪声和提供隐含常识知识方面表现出色，并取得了更好的性能。

    

    由于在训练中获得的知识，LLMs在自然语言处理任务上展现出令人印象深刻的零样本性能。在多项选择问答任务中，LM概率被用作每个答案选择的合理性的不完美度量。基本评分的一个主要限制是将所有单词都视为同等重要。我们提出了一个带有扩展答案空间的常识增强评分机制（CASE），通过根据输入中单词与其他单词之间的语义关系分配重要性权重来解决这个限制。动态加权方法不仅降低了不重要单词的噪声，还向模型提供了可能对回答问题有用的隐含常识知识。我们还在扩展答案空间方面遵循先前的工作，通过生成概念上类似于选项的词汇不同的答案。当与答案空间扩展相结合时，能表现出更好的性能。

    LLMs have demonstrated impressive zero-shot performance on NLP tasks thanks to the knowledge they acquired in their training. In multiple-choice QA tasks, the LM probabilities are used as an imperfect measure of the plausibility of each answer choice. One of the major limitations of the basic score is that it treats all words as equally important. We propose CASE, a Commonsense-Augmented Score with an Expanded Answer Space. CASE addresses this limitation by assigning importance weights for individual words based on their semantic relations to other words in the input. The dynamic weighting approach outperforms basic LM scores, not only because it reduces noise from unimportant words, but also because it informs the model of implicit commonsense knowledge that may be useful for answering the question. We then also follow prior work in expanding the answer space by generating lexically-divergent answers that are conceptually-similar to the choices. When combined with answer space expansi
    
[^36]: DialogBench: 将LLMs作为人类对话系统进行评估

    DialogBench: Evaluating LLMs as Human-like Dialogue Systems. (arXiv:2311.01677v1 [cs.CL])

    [http://arxiv.org/abs/2311.01677](http://arxiv.org/abs/2311.01677)

    本文提出了DialogBench，一个对话评估基准，用于评估LLMs作为人类对话系统的能力。通过对28个LLMs的广泛测试，发现指导微调对提升性能效果显著。

    

    大型语言模型(LLMs)在新的对话能力方面取得了显著突破，刷新了人们对对话系统的印象。对话系统长期以来的目标是足够像人类，以便通过满足交流、情感和社交归属的需要与用户建立长期联系。因此，迫切需要评估LLMs作为人类对话系统的能力。本文提出了DialogBench，一个对话评估基准，目前包含12个对话任务，评估LLMs作为人类对话系统应具备的能力。具体来说，我们使用GPT-4生成每个任务的评估实例。我们首先根据广泛使用的设计原则设计基本提示，并进一步减轻现有的偏见，生成更高质量的评估实例。我们对28个LLMs进行了广泛的测试（包括预训练和监督指导调优），结果显示指导微调效益显著。

    Large language models (LLMs) have achieved remarkable breakthroughs in new dialogue capabilities, refreshing human's impressions on dialogue systems. The long-standing goal of dialogue systems is to be human-like enough to establish long-term connections with users by satisfying the need for communication, affection and social belonging. Therefore, there has been an urgent need to evaluate LLMs as human-like dialogue systems. In this paper, we propose DialogBench, a dialogue evaluation benchmark that currently contains $12$ dialogue tasks to assess the capabilities of LLMs as human-like dialogue systems should have. Specifically, we prompt GPT-4 to generate evaluation instances for each task. We first design the basic prompt based on widely-used design principles and further mitigate the existing biases to generate higher-quality evaluation instances. Our extensive test over $28$ LLMs (including pre-trained and supervised instruction-tuning) shows that instruction fine-tuning benefits 
    
[^37]: 将情节检索作为抽象语义关联度评估的方法

    Plot Retrieval as an Assessment of Abstract Semantic Association. (arXiv:2311.01666v1 [cs.IR])

    [http://arxiv.org/abs/2311.01666](http://arxiv.org/abs/2311.01666)

    本研究提出了一个名为情节检索的新任务，通过生成一个标记数据集来训练和评估信息检索模型在此任务上的性能。这个任务要求模型能够准确估计查询和候选情节之间的抽象语义关联度，而不仅仅是依赖于词汇或语义匹配。

    

    从书籍中检索相关情节是一项关键任务，可以提高读者的阅读体验和效率。读者通常只提供一个基于自己理解、摘要或猜测的抽象和模糊的描述作为查询，这要求检索模型具有强大的能力来估计查询和候选情节之间的抽象语义关联度。然而，现有的信息检索数据集不能很好地反映这种能力。在本文中，我们提出了一种名为情节检索的标记数据集，用于训练和评估IR模型在新任务情节检索上的性能。情节检索中的文本对具有较少的词重叠和更多的抽象语义关联度，可以反映IR模型估计抽象语义关联度的能力，而不仅仅是传统的词汇或语义匹配。通过各种词汇检索、稀疏检索和密集检索的大量实验

    Retrieving relevant plots from the book for a query is a critical task, which can improve the reading experience and efficiency of readers. Readers usually only give an abstract and vague description as the query based on their own understanding, summaries, or speculations of the plot, which requires the retrieval model to have a strong ability to estimate the abstract semantic associations between the query and candidate plots. However, existing information retrieval (IR) datasets cannot reflect this ability well. In this paper, we propose Plot Retrieval, a labeled dataset to train and evaluate the performance of IR models on the novel task Plot Retrieval. Text pairs in Plot Retrieval have less word overlap and more abstract semantic association, which can reflect the ability of the IR models to estimate the abstract semantic association, rather than just traditional lexical or semantic matching. Extensive experiments across various lexical retrieval, sparse retrieval, dense retrieval
    
[^38]: MARRS: 多模态参考解析系统

    MARRS: Multimodal Reference Resolution System. (arXiv:2311.01650v1 [cs.CL])

    [http://arxiv.org/abs/2311.01650](http://arxiv.org/abs/2311.01650)

    MARRS是一个在设备上运行的多模态参考解析系统，能够处理对话式、视觉和背景上下文，并通过不同的机器学习模型实现上下文查询的处理。这个系统能够在保护用户隐私的同时理解上下文。

    

    成功处理上下文对于任何对话理解任务都是至关重要的。这个上下文可能是对话式的（依赖于之前的用户查询或系统回答），也可能是视觉的（依赖于用户看到的东西，例如他们的屏幕上），或者是背景的（基于一些信号，比如响起的闹钟或者正在播放的音乐）。在这项工作中，我们介绍了MARRS（多模态参考解析系统），它是一个在设备上运行的自然语言理解系统的框架，在处理对话式、视觉和背景上下文方面负责。特别是，我们提出了不同的机器学习模型来实现上下文查询的处理；具体而言，一个用于实现参考解析，一个用于通过查询重写处理上下文。我们还描述了这些模型如何相互补充，形成一个统一、连贯、轻量级的系统，可以在保护用户隐私的同时理解上下文。

    Successfully handling context is essential for any dialog understanding task. This context maybe be conversational (relying on previous user queries or system responses), visual (relying on what the user sees, for example, on their screen), or background (based on signals such as a ringing alarm or playing music). In this work, we present an overview of MARRS, or Multimodal Reference Resolution System, an on-device framework within a Natural Language Understanding system, responsible for handling conversational, visual and background context. In particular, we present different machine learning models to enable handing contextual queries; specifically, one to enable reference resolution, and one to handle context via query rewriting. We also describe how these models complement each other to form a unified, coherent, lightweight system that can understand context while preserving user privacy.
    
[^39]: VQPy：一种面向现代视频分析的面向对象方法。

    VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])

    [http://arxiv.org/abs/2311.01623](http://arxiv.org/abs/2311.01623)

    VQPy是一种面向对象的视频分析方法，它使用Python变体作为前端，并具有可扩展的后端，可以自动构建和优化基于视频对象的处理流程。

    

    视频分析广泛应用于当今系统和服务中。在视频分析的前沿是用户开发的视频查询，以找到特定感兴趣的对象。基于视频对象（例如人，动物，汽车等）与传统面向对象语言建模的对象相似的洞察力，我们提出了一种面向视频分析的面向对象方法。这种方法名为VQPy，包括一个前端（一种Python变体，其中包含用户可以表达视频对象及其交互的结构）和一个可扩展的后端，可以基于视频对象自动生成和优化管道。我们已经实施和开源了VQPy，它已经作为Cisco DeepVision框架的一部分产品化。

    Video analytics is widely used in contemporary systems and services. At the forefront of video analytics are video queries that users develop to find objects of particular interest. Building upon the insight that video objects (e.g., human, animals, cars, etc.), the center of video analytics, are similar in spirit to objects modeled by traditional object-oriented languages, we propose to develop an object-oriented approach to video analytics. This approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant with constructs that make it easy for users to express video objects and their interactions$\unicode{x2015}$as well as an extensible backend that can automatically construct and optimize pipelines based on video objects. We have implemented and open-sourced VQPy, which has been productized in Cisco as part of its DeepVision framework.
    
[^40]: ACQUIRED: 用于回答真实生活视频中反事实问题的数据集

    ACQUIRED: A Dataset for Answering Counterfactual Questions In Real-Life Videos. (arXiv:2311.01620v1 [cs.CV])

    [http://arxiv.org/abs/2311.01620](http://arxiv.org/abs/2311.01620)

    ACQUIRED是一个用于回答真实生活视频中反事实问题的数据集，在多模态模型反事实推理能力领域弥补了目前数据不足的问题，并提供了多样的真实世界场景和推理维度来评估模型的泛化能力。

    

    多模态反事实推理对于人工智能系统来说是一项重要而具有挑战性的能力。它涉及基于视觉和语言输入预测假设情境下的结果，这使得AI模型能够从失败中学习，并探索假设场景。尽管其重要性，目前只有少量针对多模态模型反事实推理能力的数据集。其中，它们只覆盖合成环境或特定类型的事件（例如交通碰撞），使得在多样的真实世界场景和推理维度中可靠地评估模型泛化能力变得困难。为了克服这些限制，我们开发了一个视频问答数据集ACQUIRED：它由3.9K个带注释的视频组成，涵盖了各种事件类型，同时包含了第一人称和第三人称视角，以确保关注真实世界的多样性。此外，每个视频都标注有三个不同领域的问题。

    Multimodal counterfactual reasoning is a vital yet challenging ability for AI systems. It involves predicting the outcomes of hypothetical circumstances based on vision and language inputs, which enables AI models to learn from failures and explore hypothetical scenarios. Despite its importance, there are only a few datasets targeting the counterfactual reasoning abilities of multimodal models. Among them, they only cover reasoning over synthetic environments or specific types of events (e.g. traffic collisions), making them hard to reliably benchmark the model generalization ability in diverse real-world scenarios and reasoning dimensions. To overcome these limitations, we develop a video question answering dataset, ACQUIRED: it consists of 3.9K annotated videos, encompassing a wide range of event types and incorporating both first and third-person viewpoints, which ensures a focus on real-world diversity. In addition, each video is annotated with questions that span three distinct di
    
[^41]: FLAP: 快速语言音频预训练

    FLAP: Fast Language-Audio Pre-training. (arXiv:2311.01615v1 [cs.SD])

    [http://arxiv.org/abs/2311.01615](http://arxiv.org/abs/2311.01615)

    FLAP是一种快速的语言音频预训练方法，通过掩盖、对比学习和重构，有效地学习对齐的音频和语言表示，以及利用大型语言模型（LLMs）增强文本输入。该方法在音频-文本检索任务上取得了最新技术性能（SoTA）。

    

    我们提出了一种名为FLAP的快速语言音频预训练方法，通过掩盖、对比学习和重构，有效地学习对齐的音频和语言表示。为了提高效率，FLAP随机丢掉音频谱图的标记，仅专注于剩余标记进行自监督学习。通过跨模态对比学习，FLAP学习将配对的音频和文本表示在共享潜在空间中对齐。值得注意的是，FLAP利用多个掩模视图进行跨模态对比，并学习重构音频标记的掩模部分。此外，FLAP利用大型语言模型（LLMs）增强文本输入，从而提高了性能。这些方法导致更强大且信息丰富的音频-文本表示，使得FLAP在AudioCaps（达到53.0％ R@1）和Clotho（达到25.5％ R@1）的音频-文本检索任务中实现了最新技术性能（SoTA）。

    We propose Fast Language-Audio Pre-training (FLAP), a self-supervised approach that efficiently and effectively learns aligned audio and language representations through masking, contrastive learning and reconstruction. For efficiency, FLAP randomly drops audio spectrogram tokens, focusing solely on the remaining ones for self-supervision. Through inter-modal contrastive learning, FLAP learns to align paired audio and text representations in a shared latent space. Notably, FLAP leverages multiple augmented views via masking for inter-modal contrast and learns to reconstruct the masked portion of audio tokens. Moreover, FLAP leverages large language models (LLMs) to augment the text inputs, contributing to improved performance. These approaches lead to more robust and informative audio-text representations, enabling FLAP to achieve state-of-the-art (SoTA) performance on audio-text retrieval tasks on AudioCaps (achieving 53.0% R@1) and Clotho (achieving 25.5% R@1).
    
[^42]: KG-FRUS：一个由127年美国外交关系知识图谱编码的新型图形数据集

    KG-FRUS: a Novel Graph-based Dataset of 127 Years of US Diplomatic Relations. (arXiv:2311.01606v1 [cs.CL])

    [http://arxiv.org/abs/2311.01606](http://arxiv.org/abs/2311.01606)

    本文介绍了KG-FRUS数据集，该数据集以知识图谱的形式编码了超过30万份美国政府外交文件，并使用提取的实体、元数据及来自Wikidata的额外实体和关系创建了一个基于图的数据集。该数据集的关系捕捉了外交、国际关系和政治等复杂领域的协同和动力。文章演示了不同的探索数据集方法。

    

    本文介绍了KG-FRUS数据集，该数据集包含超过30万份美国政府外交文件，并以知识图谱（KG）的形式进行编码。我们利用《美国对外关系》（FRUS）的数据（以XML文件形式提供），提取其中关于文件、个人和国家的信息。我们使用提取的实体及其相关元数据创建了一个基于图的数据集。此外，我们还从Wikidata中补充了额外的实体和关系。KG中的关系捕捉了研究和理解外交、国际关系和政治等复杂领域所需的协同和动力。这远远超出了仅仅收集文件而忽略文本中实体之间关系的简单集合。我们通过演示不同的探索KG方法展示了该数据集的各种可能性。在本文中，我们展示了如何使用查询语言来回答简单的研究问题。

    In the current paper, we present the KG-FRUS dataset, comprised of more than 300,000 US government diplomatic documents encoded in a Knowledge Graph (KG). We leverage the data of the Foreign Relations of the United States (FRUS) (available as XML files) to extract information about the documents and the individuals and countries mentioned within them. We use the extracted entities, and associated metadata, to create a graph-based dataset. Further, we supplement the created KG with additional entities and relations from Wikidata. The relations in the KG capture the synergies and dynamics required to study and understand the complex fields of diplomacy, foreign relations, and politics. This goes well beyond a simple collection of documents which neglects the relations between entities in the text. We showcase a range of possibilities of the current dataset by illustrating different approaches to probe the KG. In the paper, we exemplify how to use a query language to answer simple researc
    
[^43]: 对于文本预测的忠实和稳健的本地可解释性

    Faithful and Robust Local Interpretability for Textual Predictions. (arXiv:2311.01605v1 [cs.CL])

    [http://arxiv.org/abs/2311.01605](http://arxiv.org/abs/2311.01605)

    提出了一种名为FRED的新颖方法，用于解释文本预测。FRED可以识别文档中的关键词，并且通过与最先进的方法进行的实证评估证明了其在提供对文本模型的深入见解方面的有效性。

    

    可解释性对于机器学习模型在关键领域中得到信任和部署是至关重要的。然而，现有的用于解释文本模型的方法通常复杂，并且缺乏坚实的数学基础，它们的性能也不能保证。在本文中，我们提出了一种新颖的方法FRED（Faithful and Robust Explainer for textual Documents），用于解释文本预测。FRED可以识别文档中的关键词，当这些词被移除时对预测结果产生重大影响。我们通过正式的定义和对可解释分类器的理论分析，确立了FRED的可靠性。此外，我们还通过与最先进的方法进行的实证评估，证明了FRED在提供对文本模型的深入见解方面的有效性。

    Interpretability is essential for machine learning models to be trusted and deployed in critical domains. However, existing methods for interpreting text models are often complex, lack solid mathematical foundations, and their performance is not guaranteed. In this paper, we propose FRED (Faithful and Robust Explainer for textual Documents), a novel method for interpreting predictions over text. FRED identifies key words in a document that significantly impact the prediction when removed. We establish the reliability of FRED through formal definitions and theoretical analyses on interpretable classifiers. Additionally, our empirical evaluation against state-of-the-art methods demonstrates the effectiveness of FRED in providing insights into text models.
    
[^44]: MetaReVision: 使用检索进行元学习的视觉基础组合概念获取

    MetaReVision: Meta-Learning with Retrieval for Visually Grounded Compositional Concept Acquisition. (arXiv:2311.01580v1 [cs.CL])

    [http://arxiv.org/abs/2311.01580](http://arxiv.org/abs/2311.01580)

    MetaReVision是一种检索增强的元学习模型，通过使用检索到的基本概念作为支持集合来快速学习和识别新的图像基础组合概念。

    

    人类能够通过回忆和推广从过去的经验中获取的基本概念来学习新的组合概念。受到这一观察的启发，本文提出了一种检索增强的元学习模型 - MetaReVision，以解决视觉基础组合概念学习问题。MetaReVision由一个检索模块和一个元学习模块组成，旨在将检索到的基本概念作为支持集合并用于元训练视觉-语言模型来识别基于图像的组合概念。通过从检索器构建的episode进行元学习，MetaReVision学习到了一个通用的组合表示，可以快速更新以识别新的组合概念。我们创建了CompCOCO和CompFlickr来评估基于图像的组合概念学习。实验结果表明，MetaReVision优于其他竞争基线，并且检索模块起着重要作用。

    Humans have the ability to learn novel compositional concepts by recalling and generalizing primitive concepts acquired from past experiences. Inspired by this observation, in this paper, we propose MetaReVision, a retrieval-enhanced meta-learning model to address the visually grounded compositional concept learning problem. The proposed MetaReVision consists of a retrieval module and a meta-learning module which are designed to incorporate retrieved primitive concepts as a supporting set to meta-train vision-anguage models for grounded compositional concept recognition. Through meta-learning from episodes constructed by the retriever, MetaReVision learns a generic compositional representation that can be fast updated to recognize novel compositional concepts. We create CompCOCO and CompFlickr to benchmark the grounded compositional concept learning. Our experimental results show that MetaReVision outperforms other competitive baselines and the retrieval module plays an important role 
    
[^45]: 使用聚合集成模型保留长篇临床文本的知识

    Preserving the knowledge of long clinical texts using aggregated ensembles of large language models. (arXiv:2311.01571v1 [cs.CL])

    [http://arxiv.org/abs/2311.01571](http://arxiv.org/abs/2311.01571)

    本文提出了一种使用聚合集成模型的方法来保留长篇临床文本的知识。与以往方法不同，我们将集成学习与文本聚合相结合，并在两个临床预测任务上训练多个大型语言模型。实验证明，我们的方法可以在处理长输入和多样性数据集时提升大型语言模型的性能。

    

    临床文本，如入院记录、出院小结和进展记录，包含丰富而宝贵的信息，可用于各种临床结果预测任务。然而，将基于BERT的大型语言模型应用于临床文本面临两个主要挑战：输入长度的限制和数据来源的多样性。本文提出了一种新颖的方法，使用聚合集成的大型语言模型来保留长篇临床文本的知识。与以往研究单独使用模型集成或文本聚合方法不同，我们将集成学习与文本聚合相结合，在两个临床结果预测任务（死亡预测和住院天数预测）上训练多个大型语言模型。我们展示了我们的方法可以比基线、独立的集成和聚合效果更好，并且可以在处理长输入和多样性数据集时提高大型语言模型的性能。

    Clinical texts, such as admission notes, discharge summaries, and progress notes, contain rich and valuable information that can be used for various clinical outcome prediction tasks. However, applying large language models, such as BERT-based models, to clinical texts poses two major challenges: the limitation of input length and the diversity of data sources. This paper proposes a novel method to preserve the knowledge of long clinical texts using aggregated ensembles of large language models. Unlike previous studies which use model ensembling or text aggregation methods separately, we combine ensemble learning with text aggregation and train multiple large language models on two clinical outcome tasks: mortality prediction and length of stay prediction. We show that our method can achieve better results than baselines, ensembling, and aggregation individually, and can improve the performance of large language models while handling long inputs and diverse datasets. We conduct extensi
    
[^46]: 翻译后的论文标题：指令蒸馏使得大型语言模型成为高效的零-shot排序器

    Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers. (arXiv:2311.01555v1 [cs.IR])

    [http://arxiv.org/abs/2311.01555](http://arxiv.org/abs/2311.01555)

    中文总结出的一句话要点：本研究提出了一种指令蒸馏方法，通过将大型语言模型的一对一排序能力蒸馏为更高效的单点排序，显著提高了大型语言模型作为零-shot排序器的效率和性能。

    

    近期的研究表明，大型语言模型（LLMs）作为零-shot相关性排序器具有巨大潜力。典型的方法涉及对文档进行一对一或一对多的比较。尽管这些一对多和一对一的方法有效，但效率不高，且严重依赖复杂的提示工程。为了解决这个问题，我们引入了一种新颖的指令蒸馏方法。其核心思想是将开源LLMs的一对一排序能力蒸馏为更简单但更高效的单点排序。具体来说，给定相同的LLMs，我们首先使用复杂的指令采用有效的一对一方法对文档进行排序，然后将教师的预测结果转化为采用更简单的指令的单点排序方法。在BEIR、TREC和ReDial数据集上的评估结果表明，指令蒸馏可以将效率提高10到100倍，同时提高LLMs的排序性能。此外，我们的方法超过了性能

    Recent studies have demonstrated the great potential of Large Language Models (LLMs) serving as zero-shot relevance rankers. The typical approach involves making comparisons between pairs or lists of documents. Although effective, these listwise and pairwise methods are not efficient and also heavily rely on intricate prompt engineering. To tackle this problem, we introduce a novel instruction distillation method. The key idea is to distill the pairwise ranking ability of open-sourced LLMs to a simpler but more efficient pointwise ranking. Specifically, given the same LLM, we first rank documents using the effective pairwise approach with complex instructions, and then distill the teacher predictions to the pointwise approach with simpler instructions. Evaluation results on the BEIR, TREC, and ReDial datasets demonstrate that instruction distillation can improve efficiency by 10 to 100x and also enhance the ranking performance of LLMs. Furthermore, our approach surpasses the performanc
    
[^47]: 不同的令牌指标：通过测量衰减来修剪LLM组件并优化量化

    Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization. (arXiv:2311.01544v1 [cs.CL])

    [http://arxiv.org/abs/2311.01544](http://arxiv.org/abs/2311.01544)

    本研究引入了一种新的方法，即不同的令牌指标（DTM），用于评估压缩后的大型语言模型（LLM）。通过关注令牌的差异性，DTM提供了对模型压缩微妙之处的深入洞察，并且在不损害文本生成质量的情况下可以实现显著的精确度和稀疏度水平。该研究还提出了一种利用DTM进行模型稀疏化和量化的方法，并发现可以修剪掉超过90%的LLM组件和量化超过80%的参数。

    

    大型语言模型（LLM）以其强大的能力改变了自然语言处理。然而，它们不断增长的大小引发了关于它们的有效部署和LLM压缩的担忧。本研究介绍了一种新的评估压缩LLM的方法，即不同的令牌指标（DTM），解决了传统指标如困惑度无法准确反映文本生成质量的局限性。DTM关注令牌的差异性，提供了对模型压缩微妙之处的更深入洞察。我们的结果表明，在不损害文本生成质量的情况下，可以达到显著的精确度和稀疏度水平。此外，DTM还可以更精确地评估每个组件的影响。利用第一个不同的令牌指标（FDTM）在模型稀疏化中显示，超过90%的所有组件可以修剪掉。对于量化，FDTM表明超过80%的参数可以进行量化。

    Large Language Models (LLMs) have reshaped natural language processing with their impressive capabilities. Their ever-increasing size, however, raised concerns about their effective deployment and the need for LLM compressions. This study introduces the Divergent Token metrics (DTMs), a novel approach for assessing compressed LLMs, addressing the limitations of traditional measures like perplexity that fail to accurately reflect text generation quality. DTMs focus on token divergence, providing deeper insights into the subtleties of model compression. Our results indicate that significant levels of precision and sparsity can be achieved without compromising text generation quality. Moreover, DTMs offers a more precise evaluation of each component's impact individually. Utilizing the First Divergent Token metric (FDTM) in model sparsification reveals that nearly 20% of all components can be pruned over 90%. In terms of quantization, the FDTM suggests that over 80% of parameters can be s
    
[^48]: 优秀的视觉指导有什么特点？综合复杂的视觉推理指令用于视觉指导调整

    What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning. (arXiv:2311.01487v1 [cs.CV])

    [http://arxiv.org/abs/2311.01487](http://arxiv.org/abs/2311.01487)

    通过综合复杂的视觉推理任务，可以有效改善多模式大型语言模型在评估基准上的性能。我们提出了一种自动创建高质量复杂视觉推理指令的系统方法。

    

    视觉指导调整是提高多模式大型语言模型（MLLMs）的零样本泛化能力的重要方法。最近提出了许多着眼于不同焦点和特征的视觉指导数据集，使得MLLMs在评估基准上取得了令人惊讶的结果。为了开发更强大的MLLMs，本文旨在研究一个更基本的问题：“什么样的视觉指导才是好的？”通过进行全面的实证研究，我们发现侧重于复杂视觉推理任务的指导对于改善MLLMs在评估基准上的性能特别有效。基于这一发现，我们设计了一个系统的方法来自动创建高质量的复杂视觉推理指令。我们的方法采用合成-复杂化-重构的范式，利用多个阶段逐渐增加指令的复杂性，同时保证质量。

    Visual instruction tuning is an essential approach to improving the zero-shot generalization capability of Multi-modal Large Language Models (MLLMs). A surge of visual instruction datasets with various focuses and characteristics have been proposed recently, enabling MLLMs to achieve surprising results on evaluation benchmarks. To develop more capable MLLMs, in this paper, we aim to investigate a more fundamental question: ``what makes for good visual instructions?''. By conducting a comprehensive empirical study, we find that instructions focused on complex visual reasoning tasks are particularly effective in improving the performance of MLLMs on evaluation benchmarks. Building upon this finding, we design a systematic approach to automatically creating high-quality complex visual reasoning instructions. Our approach employs a synthesis-complication-reformulation paradigm, leveraging multiple stages to gradually increase the complexity of the instructions while guaranteeing quality. B
    
[^49]: 从新闻文章中提取关系（RENA）：用于流行病监测的工具

    Relation Extraction from News Articles (RENA): A Tool for Epidemic Surveillance. (arXiv:2311.01472v1 [cs.CL])

    [http://arxiv.org/abs/2311.01472](http://arxiv.org/abs/2311.01472)

    RENA是一种基于浏览器的关系提取工具，用于从英语新闻文章中提取与传染病相关的关键实体和语义关系，为流行病监测提供实时解析和关键信息提取的能力。

    

    关系提取从新闻文章（RENA）是一个基于浏览器的工具，旨在提取与传染病相关的英语新闻文章中的关键实体及其语义关系。该系统采用React框架构建，为用户提供了一个优雅且用户友好的界面。它允许用户输入新闻文章并从两个模型中选择，以生成所提供文本中的关系的全面列表。因此，RENA允许实时解析新闻文章提取流行病监测的关键信息，为开源情报驱动的流行病预警系统EPIWATCH做出贡献。

    Relation Extraction from News Articles (RENA) is a browser-based tool designed to extract key entities and their semantic relationships in English language news articles related to infectious diseases. Constructed using the React framework, this system presents users with an elegant and user-friendly interface. It enables users to input a news article and select from a choice of two models to generate a comprehensive list of relations within the provided text. As a result, RENA allows real-time parsing of news articles to extract key information for epidemic surveillance, contributing to EPIWATCH, an open-source intelligence-based epidemic warning system.
    
[^50]: 利用语言模型检测环保虚假宣传

    Leveraging Language Models to Detect Greenwashing. (arXiv:2311.01469v1 [cs.CL])

    [http://arxiv.org/abs/2311.01469](http://arxiv.org/abs/2311.01469)

    本研究引入了一种新的方法，利用语言模型来检测绿色虚假宣传风险。开发了一种量化绿色虚假宣传风险的数学形式，建立了优化的ClimateBERT模型，并进行了结果比较分析。实验表明，我们的方法对于这一任务具有良好的探索方向。

    

    近年来，气候变化的后果越来越引起公众的关注。因此，企业在可持续发展报告中强调其环保努力以增强公众形象。然而，对此类报告的审核缺乏严格的监管，可能导致绿色虚假宣传。在本研究中，我们引入了一种新的方法来对绿色虚假宣传风险进行训练语言模型。我们的主要贡献包括：开发了一种数学形式来量化绿色虚假宣传风险，提出了一个针对该问题的优化ClimateBERT模型，并进行了结果的比较分析。在一个包含可持续发展报告的测试集上，我们的最佳模型实现了平均准确率86.34%和F1值0.67，表明我们的方法对于这一任务具有探索的良好方向。

    In recent years, climate change repercussions have increasingly captured public interest. Consequently, corporations are emphasizing their environmental efforts in sustainability reports to bolster their public image. Yet, the absence of stringent regulations in review of such reports allows potential greenwashing. In this study, we introduce a novel methodology to train a language model on generated labels for greenwashing risk. Our primary contributions encompass: developing a mathematical formulation to quantify greenwashing risk, a fine-tuned ClimateBERT model for this problem, and a comparative analysis of results. On a test set comprising of sustainability reports, our best model achieved an average accuracy score of 86.34% and F1 score of 0.67, demonstrating that our methods show a promising direction of exploration for this task.
    
[^51]: 记住你所做的，这样你就知道接下来该做什么。

    Remember what you did so you know what to do next. (arXiv:2311.01468v1 [cs.CL])

    [http://arxiv.org/abs/2311.01468](http://arxiv.org/abs/2311.01468)

    本文通过使用一个大型语言模型（LLM）为模拟机器人制定计划，在ScienceWorld中实现30类目标。实验结果显示，LLM在马尔可夫假设的情况下比强化学习方法的性能提高了1.4倍，当填充尽可能多的先前步骤时提高到3.5倍，即使只训练了6.5%的数据，也比基于强化学习方法的性能提高了2.2倍。不同类别的动作表现差异很大，说明平均任务可能会隐藏性能问题。

    

    我们探讨使用一个中等规模的大型语言模型（GPT-J 6B参数），为模拟机器人在ScienceWorld中实现30类目标（一个用于小学科学实验的文本游戏模拟器）制定计划。先前发表的实证研究声称，与强化学习相比，大型语言模型（LLMs）的适用性较差（Wang等，2022）。使用马尔可夫假设（单个前一个步骤），LLM的性能超过基于强化学习的方法1.4倍。当我们尽可能多地填充LLM的输入缓冲区时，改进效果提高到3.5倍。即使只对6.5%的训练数据进行训练，我们观察到与基于强化学习的方法相比，性能提高了2.2倍。我们的实验显示，对于30类动作，性能差异很大，表明对任务进行平均可能会隐藏显著的性能问题。与我们同时进行的Lin等人（2023）的工作证明了一种两部分方法（SwiftSa）

    We explore using a moderately sized large language model (GPT-J 6B parameters) to create a plan for a simulated robot to achieve 30 classes of goals in ScienceWorld, a text game simulator for elementary science experiments. Previously published empirical work claimed that large language models (LLMs) are a poor fit (Wang et al., 2022) compared to reinforcement learning. Using the Markov assumption (a single previous step), the LLM outperforms the reinforcement learning-based approach by a factor of 1.4. When we fill the LLM's input buffer with as many prior steps as possible, improvement rises to 3.5x. Even when training on only 6.5% of the training data, we observe a 2.2x improvement over the reinforcement-learning-based approach. Our experiments show that performance varies widely across the 30 classes of actions, indicating that averaging over tasks can hide significant performance issues. In work contemporaneous with ours, Lin et al. (2023) demonstrated a two-part approach (SwiftSa
    
[^52]: 支持可信度的LLM创建过程：处理医疗AI中的幻觉

    Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI. (arXiv:2311.01463v1 [cs.CL])

    [http://arxiv.org/abs/2311.01463](http://arxiv.org/abs/2311.01463)

    这篇论文描述了在医疗人工智能中创建可靠、可信和无偏置的LLM模型的关键要素，着重于量化、验证和缓解幻觉问题，并讨论了LLM在医疗领域的未来发展。

    

    在短时间内，大型语言模型在多个领域中迅速增多。然而，由于准确性、连贯性和幻觉等问题，医疗领域对其采用存在犹豫。鉴于医疗事关重大，许多研究人员甚至提出在解决这些问题之前不应使用这些模型。在本文中，我们描述了创建可靠、可信和无偏置模型的关键要素，这是其在医疗领域应用的必要条件。具体而言，我们着重于在医疗背景下对幻觉进行量化、验证和缓解。最后，我们讨论了LLM在医疗领域未来的可能发展方向。

    Large language models have proliferated across multiple domains in as short period of time. There is however hesitation in the medical and healthcare domain towards their adoption because of issues like factuality, coherence, and hallucinations. Give the high stakes nature of healthcare, many researchers have even cautioned against its usage until these issues are resolved. The key to the implementation and deployment of LLMs in healthcare is to make these models trustworthy, transparent (as much possible) and explainable. In this paper we describe the key elements in creating reliable, trustworthy, and unbiased models as a necessary condition for their adoption in healthcare. Specifically we focus on the quantification, validation, and mitigation of hallucinations in the context in healthcare. Lastly, we discuss how the future of LLMs in healthcare may look like.
    
[^53]: FlashDecoding++: 在GPU上加速大规模语言模型推理的更快算法

    FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v1 [cs.LG])

    [http://arxiv.org/abs/2311.01282](http://arxiv.org/abs/2311.01282)

    FlashDecoding++是一种快速的LLM推理引擎，通过解决同步部分softmax更新、未充分利用扁平GEMM计算和静态数据流导致的性能损失等挑战，实现了大规模语言模型推理的加速。

    

    随着大规模语言模型在各个领域的重要性日益增加，加速语言模型推理仍然存在一些挑战未解决：(1) 同步部分softmax更新。softmax操作需要同步更新每个部分softmax结果，导致LLM中注意力计算的开销增加约20%。(2) 未充分利用扁平GEMM计算。在LLM推理中执行GEMM的矩阵形状是扁平的，导致在先前的设计中填充零后计算未充分利用，性能损失超过50%。(3) 静态数据流导致的性能损失。LLM中的内核性能取决于不同的输入数据特征、硬件配置等。单一和静态的数据流可能导致LLM推理中不同形状的GEMM的性能损失达到50.25%。我们提出了FlashDecoding++，一种快速支持主流LLM和硬件后端的LLM推理引擎。为了解决上述挑战，FlashDecoding++实现了以下目标：

    As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and >50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.  We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
    
[^54]: 通过使用语言模型模拟受众群体，改善人际沟通

    Improving Interpersonal Communication by Simulating Audiences with Language Models. (arXiv:2311.00687v1 [cs.AI])

    [http://arxiv.org/abs/2311.00687](http://arxiv.org/abs/2311.00687)

    本论文提出了一个基于大型语言模型（LLM）模拟的框架，通过探索解决方案空间、生成沟通候选以及模拟受众反应，来改善人际沟通。通过评估八个涵盖人际沟通基本过程的场景，展示了该框架的有效性。

    

    我们如何与他人进行沟通以实现自己的目标？我们利用先前的经验或他人的建议，或者通过预测对方的反应来构造候选表达。然而，我们的经验是有限和有偏见的，而且对潜在结果进行推理可能是困难且认知上具有挑战性的。本文中，我们探讨了如何利用大型语言模型（LLM）模拟来帮助我们更好地沟通。我们提出了探索-生成-模拟（EGS）框架，该框架接受任何一个个体与一个目标受众进行沟通的场景作为输入。EGS（1）通过生成与场景相关的多样化建议来探索解决方案空间，（2）生成以部分建议为条件的沟通候选，（3）模拟不同受众的反应，以确定最佳候选和建议的使用。我们在涵盖人际沟通十个基本过程的八个场景上评估了该框架。

    How do we communicate with others to achieve our goals? We use our prior experience or advice from others, or construct a candidate utterance by predicting how it will be received. However, our experiences are limited and biased, and reasoning about potential outcomes can be difficult and cognitively challenging. In this paper, we explore how we can leverage Large Language Model (LLM) simulations to help us communicate better. We propose the Explore-Generate-Simulate (EGS) framework, which takes as input any scenario where an individual is communicating to an audience with a goal they want to achieve. EGS (1) explores the solution space by producing a diverse set of advice relevant to the scenario, (2) generates communication candidates conditioned on subsets of the advice, and (3) simulates the reactions from various audiences to determine both the best candidate and advice to use. We evaluate the framework on eight scenarios spanning the ten fundamental processes of interpersonal com
    
[^55]: 关于使用发展性数据进行语法习得的课程学习效果研究

    On the effect of curriculum learning with developmental data for grammar acquisition. (arXiv:2311.00128v1 [cs.CL])

    [http://arxiv.org/abs/2311.00128](http://arxiv.org/abs/2311.00128)

    这项研究发现语法习得主要受到对语音数据的暴露驱动，并通过课程学习方法进一步提高其性能。

    

    本研究探讨了语法习得在语言“简单性”和数据的来源模态（语音 vs 文本）方面的影响程度。通过使用BabyBERTa作为探针，我们发现语法习得主要受到对语音数据的暴露的驱动，尤其是通过对两个BabyLM训练数据集（AO-Childes和Open Subtitles）的暴露。我们通过检查将输入数据以不同方式呈现给模型的方法得出了这一发现。首先，我们评估了基于序列级复杂性的学习计划的影响。然后，我们研究了学习“块”的影响——这些块覆盖了源数据集中每个语料库中每个标记数量平衡的文本范围。最后，我们探索了不同程度地让模型接触不同语料库的学习计划。在所有情况下，我们发现过度接触AO-Childes和Open Subtitles显著提高了性能。我们通过一个可比较的控制数据集来验证这些发现，该数据集中曝光程度较低。

    This work explores the degree to which grammar acquisition is driven by language `simplicity' and the source modality (speech vs. text) of data. Using BabyBERTa as a probe, we find that grammar acquisition is largely driven by exposure to speech data, and in particular through exposure to two of the BabyLM training corpora: AO-Childes and Open Subtitles. We arrive at this finding by examining various ways of presenting input data to our model. First, we assess the impact of various sequence-level complexity based curricula. We then examine the impact of learning over `blocks' -- covering spans of text that are balanced for the number of tokens in each of the source corpora (rather than number of lines). Finally, we explore curricula that vary the degree to which the model is exposed to different corpora. In all cases, we find that over-exposure to AO-Childes and Open Subtitles significantly drives performance. We verify these findings through a comparable control dataset in which expos
    
[^56]: CapsFusion: 重新思考大规模图像-文本数据

    CapsFusion: Rethinking Image-Text Data at Scale. (arXiv:2310.20550v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.20550](http://arxiv.org/abs/2310.20550)

    CapsFusion是一个先进的框架，通过利用大型语言模型整合和细化来自网络图像-文本对和合成字幕的信息，提供了更高质量、更可扩展的多模态预训练数据。

    

    大规模多模态模型展示了在零样本情况下执行多样化多模态任务的显著泛化能力。大规模基于网络的图像-文本对在这一成功中起着根本性的贡献，但存在着过多的噪声。最近的研究使用由生成式字幕模型合成的替代字幕，并取得了显著的基准性能。然而，我们的实验证明，在使用合成字幕训练的模型中存在着显著的可扩展性不足和世界知识丧失问题，这些问题在其初始基准成功中大部分被掩盖了。经过进一步的研究，我们确定根本原因是现有合成字幕中过于简化的语言结构和缺乏知识细节。为了提供更高质量、更可扩展的多模态预训练数据，我们提出了CapsFusion，这是一个先进的框架，利用大型语言模型来整合和细化来自网络图像-文本对和合成字幕的信息。

    Large multimodal models demonstrate remarkable generalist ability to perform diverse multimodal tasks in a zero-shot manner. Large-scale web-based image-text pairs contribute fundamentally to this success, but suffer from excessive noise. Recent studies use alternative captions synthesized by captioning models and have achieved notable benchmark performance. However, our experiments reveal significant Scalability Deficiency and World Knowledge Loss issues in models trained with synthetic captions, which have been largely obscured by their initial benchmark success. Upon closer examination, we identify the root cause as the overly-simplified language structure and lack of knowledge details in existing synthetic captions. To provide higher-quality and more scalable multimodal pretraining data, we propose CapsFusion, an advanced framework that leverages large language models to consolidate and refine information from both web-based image-text pairs and synthetic captions. Extensive experi
    
[^57]: 用于临床总结中事实对齐的合成模仿编辑反馈

    Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization. (arXiv:2310.20033v1 [cs.CL])

    [http://arxiv.org/abs/2310.20033](http://arxiv.org/abs/2310.20033)

    本文提出了一种使用ChatGPT来生成高质量反馈数据以改善临床笔记总结的事实一致性的新方法。

    

    大型语言模型（LLMs）如GPT和LLaMA系列在捕捉和浓缩关键上下文信息及在总结任务中实现最先进的性能方面表现出了异常能力。然而，社区对这些模型的虚构问题的担忧仍在不断上升。LLMs有时会生成虚构的摘要，这在临床领域的NLP任务（例如临床笔记总结）中可能会导致严重错误的诊断。使用人类反馈对LLMs进行微调已经显示出在生成过程中实现事实一致性的承诺，但这种训练过程需要高质量的人工注释数据，而在临床领域获取这样的数据可能非常昂贵。在这项工作中，我们提出了一种新的管道，使用ChatGPT代替人类专家生成高质量的反馈数据，以改善临床笔记总结的事实一致性。

    Large Language Models (LLMs) like the GPT and LLaMA families have demonstrated exceptional capabilities in capturing and condensing critical contextual information and achieving state-of-the-art performance in the summarization task. However, community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using human feedback has shown the promise of aligning LLMs to be factually consistent during generation, but such training procedure requires high-quality human-annotated data, which can be extremely expensive to get in the clinical domain. In this work, we propose a new pipeline using ChatGPT instead of human experts to generate high-quality feedback data for improving factual consistency in the clinical note summari
    
[^58]: 在快速发展时代管理人工智能风险

    Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v1 [cs.CY] CROSS LISTED)

    [http://arxiv.org/abs/2310.17688](http://arxiv.org/abs/2310.17688)

    在人工智能快速进展的时代，我们提出了管理即将到来的先进人工智能系统所带来的风险的优先事项。

    

    在这篇简短的共识文中，我们概述了即将到来的先进人工智能系统所带来的风险。我们审查了大规模的社会危害和恶意使用，以及人类对自主人工智能系统失去控制的不可逆转的损失。鉴于人工智能的快速和持续进展，我们提出了人工智能研发和治理的优先事项。

    In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose priorities for AI R&D and governance.
    
[^59]: 检测大型语言模型的预训练数据

    Detecting Pretraining Data from Large Language Models. (arXiv:2310.16789v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.16789](http://arxiv.org/abs/2310.16789)

    这项研究探讨了如何检测大型语言模型的预训练数据，提出了一个动态基准和一种新的检测方法，以解决数据隐私和不透明性的问题。

    

    虽然大型语言模型（LLM）被广泛应用，但用于训练它们的数据很少被公开。考虑到这些数据的规模之大，可能包含受版权保护的材料、个人可识别信息以及用于广泛报道的参考基准测试数据，我们几乎可以肯定它们包含了潜在的问题文本。然而，我们目前无法知道这些文本中包含了哪些类型的数据以及比例。在本文中，我们研究了预训练数据检测问题：在不知道预训练数据的情况下，给定一段文本和对LLM的黑盒访问，我们能否确定模型是否是在提供的文本上进行了训练？为了方便这项研究，我们引入了一个动态基准WIKIMIA，使用在模型训练之前和之后创建的数据来支持金标准检测。我们还引入了一种新的检测方法Min-K% Prob，基于一个简单的假设：一个未见过的例子可能包含几个具有较低概率的离群词。

    Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low pro
    
[^60]: ChatGPT能像律师一样使用IRAC方法分析法律情景吗？

    Can ChatGPT Perform Reasoning Using the IRAC Method in Analyzing Legal Scenarios Like a Lawyer?. (arXiv:2310.14880v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.14880](http://arxiv.org/abs/2310.14880)

    本论文研究了ChatGPT是否能够像律师一样使用IRAC方法分析法律情景。通过构建一个包含马来西亚合同法和澳大利亚社会法情景的语料库，并使用IRAC方法对其进行分析，作者发现ChatGPT在法律分析方面具有潜力。

    

    近来，大语言模型(如ChatGPT)在法律领域引起了很大的关注，因为其突出的能力能够处理各种法律任务。然而，目前还不清楚LLMs是否能够像律师那样分析法律案例并进行推理。因此，我们构建了一个新颖的语料库，包含马来西亚合同法和澳大利亚社会法关于被赡养儿童的情景。我们使用IRAC方法将ChatGPT应用于对语料库进行分析，IRAC方法是法律专业人士广泛使用的组织法律分析的框架。语料库中的每个情景都以半结构化格式注释了完整的IRAC分析，以便机器和法律专业人士能够解释和理解这些注释。此外，我们进行了第一次对ChatGPT进行IRAC分析的实证评估，以了解其与法律专业人士分析的吻合程度。

    Large Language Models (LLMs), such as ChatGPT, have drawn a lot of attentions recently in the legal domain due to its emergent ability to tackle a variety of legal tasks. However, it is still unknown if LLMs are able to analyze a legal case and perform reasoning in the same manner as lawyers. Therefore, we constructed a novel corpus consisting of scenarios pertain to Contract Acts Malaysia and Australian Social Act for Dependent Child. ChatGPT is applied to perform analysis on the corpus using the IRAC method, which is a framework widely used by legal professionals for organizing legal analysis. Each scenario in the corpus is annotated with a complete IRAC analysis in a semi-structured format so that both machines and legal professionals are able to interpret and understand the annotations. In addition, we conducted the first empirical assessment of ChatGPT for IRAC analysis in order to understand how well it aligns with the analysis of legal professionals. Our experimental results she
    
[^61]: "凯利是一个温暖的人，约瑟夫是一个榜样": LLM生成的推荐信中的性别偏见

    "Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v1 [cs.CL])

    [http://arxiv.org/abs/2310.09219](http://arxiv.org/abs/2310.09219)

    本文对LLM生成的推荐信中的性别偏见进行了细致的研究，并设计了评估方法来展现通过语言风格和词汇内容来体现的性别偏见。

    

    随着生成语言模型的进步，用户已经开始使用大型语言模型（LLM）来协助撰写各种类型的内容，包括推荐信等职业文件。尽管它们的方便性，但这些应用引入了前所未有的公平问题。由于生成的推荐信可能被用户直接在职业或学术场景中使用，它们有可能造成直接的社会伤害，如降低女性申请者的成功率。因此，对于未来的缓解和监控，全面研究此类实际应用情况中的公平问题和相关伤害势在必行。在本文中，我们对LLM生成的推荐信中的性别偏见进行了批判性的研究。受社会科学研究结果的启发，我们设计了评估方法，通过两个维度来展现LLM生成的信件中的性别偏见：语言风格的偏见和词汇内容的偏见。此外，我们还研究了推荐信中性别偏见的程度。

    As generative language models advance, users have started to utilize Large Language Models (LLMs) to assist in writing various types of content, including professional documents such as recommendation letters. Despite their convenience, these applications introduce unprecedented fairness concerns. As generated reference letters might be directly utilized by users in professional or academic scenarios, they have the potential to cause direct social harms, such as lowering success rates for female applicants. Therefore, it is imminent and necessary to comprehensively study fairness issues and associated harms in such real-world use cases for future mitigation and monitoring. In this paper, we critically examine gender bias in LLM-generated reference letters. Inspired by findings in social science, we design evaluation methods to manifest gender biases in LLM-generated letters through 2 dimensions: biases in language style and biases in lexical content. Furthermore, we investigate the ext
    
[^62]: 通过知识增强和对齐来提升基于知识的对话系统的事实一致性

    Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment. (arXiv:2310.08372v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.08372](http://arxiv.org/abs/2310.08372)

    该论文研究了通过知识增强和对齐两种方法来提高基于知识的对话系统的事实一致性，以解决生成与提供的知识源事实不一致的回复的问题。

    

    预训练语言模型（PLMs）基于知识的对话系统容易生成与提供的知识源事实不一致的回复。在这种不一致的回复中，对话模型无法准确表达其依赖的外部知识。受先前工作的启发，该工作发现变压器中的前馈网络（FFNs）负责事实知识的表达，因此我们通过知识增强和对齐两种方法，对FFNs的事实表达能力进行了高效改进。我们首先提出了K-Dial方法，通过引入变压器中的扩展FFNs以增强特定模式的知识对话输入的事实知识表达。此外，我们还通过强化学习方法对FFNs在回复中的表达进行了调整，以使其与事实一致的最优知识对齐。

    Pretrained language models (PLMs) based knowledge-grounded dialogue systems are prone to generate responses that are factually inconsistent with the provided knowledge source. In such inconsistent responses, the dialogue models fail to accurately express the external knowledge they rely upon. Inspired by previous work which identified that feed-forward networks (FFNs) within Transformers are responsible for factual knowledge expressions, we investigate two methods to efficiently improve the factual expression capability {of FFNs} by knowledge enhancement and alignment respectively. We first propose \textsc{K-Dial}, which {explicitly} introduces {extended FFNs in Transformers to enhance factual knowledge expressions} given the specific patterns of knowledge-grounded dialogue inputs. Additionally, we apply the reinforcement learning for factual consistency (RLFC) method to implicitly adjust FFNs' expressions in responses by aligning with gold knowledge for the factual consistency prefere
    
[^63]: 从可持续性报告中通过大型语言模型推导出结构化见解：闪光还是黄金？

    Glitter or Gold? Deriving Structured Insights from Sustainability Reports via Large Language Models. (arXiv:2310.05628v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05628](http://arxiv.org/abs/2310.05628)

    通过大型语言模型和信息抽取技术，本研究提取了公司可持续性报告中的结构化ESG相关信息，为利益相关者提供简洁、信息丰富和可行动的数据。

    

    在过去的十年中，鉴于投资者对环境、社会和治理（ESG）问题越来越关注，一些监管机构开始要求上市公司披露非财务信息。这些信息以各种非结构化的多模态文档形式公开发布。因此，将这些数据聚合和整合到一个一致的框架中，以进一步推导出跨公司和市场的可持续性实践见解并不直观。鉴于这些前提，自然而然地，我们可以采用信息抽取（IE）技术为利益相关者提供简洁、信息丰富和可行动的数据。在本研究中，我们突破了传统的文本处理技术，利用大型语言模型（LLMs），结合突出的上下文学习技术和检索增强生成（RAG）范式，从公司的可持续性报告中提取具有语义结构的与ESG相关的信息。

    Over the last decade, several regulatory bodies have started requiring the disclosure of non-financial information from publicly listed companies, in light of the investors' increasing attention to Environmental, Social, and Governance (ESG) issues. Such information is publicly released in a variety of non-structured and multi-modal documentation. Hence, it is not straightforward to aggregate and consolidate such data in a cohesive framework to further derive insights about sustainability practices across companies and markets. Given these premises, it is natural to resort to Information Extraction (IE) techniques to provide concise, informative, and actionable data to the stakeholders. Moving beyond traditional text processing techniques, in this work we leverage Large Language Models (LLMs), along with the prominent in-context learning technique and the Retrieved Augmented Generation (RAG) paradigm, to extract semantically structured ESG-related information from companies' sustainabi
    
[^64]: 动态 Top-k 估计方法用于整合特征归因方法之间的分歧

    Dynamic Top-k Estimation Consolidates Disagreement between Feature Attribution Methods. (arXiv:2310.05619v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05619](http://arxiv.org/abs/2310.05619)

    本文提出了一种通过分析归因分数的连续属性来确定应显示的最佳 k 个标记的动态 Top-k 估计方法，用于整合特征归因方法之间的分歧。实验证明，动态 k 主要改进了集成梯度和 GradientXInput 的表现，为人类解释提供了具有信息价值的归因信号。

    

    特征归因分数用于通过突出显示 k 个标记来向用户解释文本分类器的预测。本文提出了一种通过分析归因分数的连续属性来确定应显示的最佳 k 个标记的方法。我们的方法在句子之间是动态的，不依赖于具体的方法，并且可以处理句子长度的偏差。我们通过在 NLI 任务中比较多种方法和人类之间的一致性，使用固定的 k 和动态的 k。我们发现，在使用静态的 k 时，基于扰动的方法和 Vanilla Gradient 在大多数方法之间和方法与人类之间的一致性指标上表现得最好。然而，它们在使用动态 k 时的优势消失了，而动态 k 主要改进了集成梯度和 GradientXInput 的表现。据我们所知，这是首次证明通过分析归因分数的连续属性对于整合人类解释的归因信号是具有信息价值的。

    Feature attribution scores are used for explaining the prediction of a text classifier to users by highlighting a k number of tokens. In this work, we propose a way to determine the number of optimal k tokens that should be displayed from sequential properties of the attribution scores. Our approach is dynamic across sentences, method-agnostic, and deals with sentence length bias. We compare agreement between multiple methods and humans on an NLI task, using fixed k and dynamic k. We find that perturbation-based methods and Vanilla Gradient exhibit highest agreement on most method--method and method--human agreement metrics with a static k. Their advantage over other methods disappears with dynamic ks which mainly improve Integrated Gradient and GradientXInput. To our knowledge, this is the first evidence that sequential properties of attribution scores are informative for consolidating attribution signals for human interpretation.
    
[^65]: 个性化随机鹦鹉更危险吗？评估对话系统中的人格偏见

    Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05280](http://arxiv.org/abs/2310.05280)

    这项研究评估了对话系统中的人格偏见对社交偏见的影响，并建立了一个综合评估框架来衡量不同人格采用下的偏见程度。

    

    最近大型语言模型的发展使其能够按照自由形式的指令进行操作，包括在对话中模仿通用或特定人口群体的人格。通用人格指的是来自某一人口群体的个体（例如亚洲人），而特定人格可以是历史人物的实际姓名。虽然采用人格使对话系统更具吸引力和亲和力，但也存在潜在风险，可能通过与用户的交互而加剧社会偏见，进一步造成社会伤害。在本文中，我们系统地研究“人格偏见”，我们将其定义为有害对话模型行为对不同人格采用的敏感性。我们将人格偏见分为有害表达和有害认同两类，同时建立了一个全面的评估框架，以衡量五个方面的人格偏见：冒犯性、有毒延续、关怀、刻板印象的认同以及

    Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. Generic personas refer to an individual from a demographic group (e.g. an Asian person), whereas specific personas can be actual names of historical figures. While the adoption of personas allows dialogue systems to be more engaging and approachable to users, it also carries the potential risk of exacerbating social biases in model responses, further causing societal harms through interactions with users. In this paper, we systematically study "persona biases", which we define to be the sensitivity of harmful dialogue model behaviors to different persona adoptions. We categorize persona biases into biases in harmful expression and harmful agreement, as well as establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and To
    
[^66]: 自动化机器翻译的行为测试

    Automating Behavioral Testing in Machine Translation. (arXiv:2309.02553v1 [cs.CL])

    [http://arxiv.org/abs/2309.02553](http://arxiv.org/abs/2309.02553)

    本文提出了一种利用大型语言模型自动生成源句子的方法，以测试机器翻译模型在多种情况下的行为。通过对多个机器翻译系统应用该方法，发现在测试结果与传统准确率度量存在差异的情况下，仍可观察到一致的趋势。

    

    NLP中的行为测试通过分析输入-输出行为来细粒度评估系统的语言能力。然而，目前关于机器翻译中行为测试的研究仅限于手工设计的测试范围有限、涵盖的语言种类也有限。为了解决这一限制，我们提出利用大型语言模型生成多样化的源句子，以测试机器翻译模型在不同情况下的行为。然后，我们可以使用相同的语言模型生成备选集，以验证机器翻译模型是否表现出预期的行为。我们的方法旨在使机器翻译系统的行为测试实际可行，同时只需要最少的人力投入。在实验中，我们将提出的评估框架应用于多个可用的机器翻译系统，结果显示，尽管总体上通过率与传统准确率度量可观察到的趋势相符，但仍存在差异。

    Behavioral testing in NLP allows fine-grained evaluation of systems by examining their linguistic capabilities through the analysis of input-output behavior. Unfortunately, existing work on behavioral testing in Machine Translation (MT) is currently restricted to largely handcrafted tests covering a limited range of capabilities and languages. To address this limitation, we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations. We can then verify whether the MT model exhibits the expected behavior through matching candidate sets that are also generated using LLMs. Our approach aims to make behavioral testing of MT systems practical while requiring only minimal human effort. In our experiments, we apply our proposed evaluation framework to assess multiple available MT systems, revealing that while in general pass-rates follow the trends observable from traditional accuracy-based metri
    
[^67]: ChatGPT用于GTFS: 从文字到信息

    ChatGPT for GTFS: From Words to Information. (arXiv:2308.02618v1 [cs.IR])

    [http://arxiv.org/abs/2308.02618](http://arxiv.org/abs/2308.02618)

    本研究探索了使用ChatGPT语言模型从GTFS数据中检索信息的可行性，验证了ChatGPT（GPT-3.5）在GTFS规范理解和信息提取方面的能力。程序合成方法在信息检索任务中表现出更高的准确率，为解决GTFS数据信息获取问题提供了一种有效的方法。

    

    广泛使用的公交通行数据发布标准General Transit Feed Specification（GTFS）是表格数据，信息分散在不同的文件中，需要专门的工具或包来检索信息。与此同时，使用大型语言模型进行文本和信息检索的趋势也在增长。本研究的想法是看看当前广泛采用的LLMs（ChatGPT）是否能够使用自然语言指令从GTFS中检索信息。我们首先测试ChatGPT（GPT-3.5）是否理解GTFS规范。GPT-3.5在我们的多项选择问题（MCQ）中正确回答了77%。接下来，我们利用过滤的GTFS数据集对LLM进行信息提取任务。对于信息检索，我们比较了零-shot和程序合成。程序合成的效果更好，在简单问题上达到了约90%的准确率，在复杂问题上达到了约40%的准确率。

    The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. GTFS being tabular data, with information spread across different files, necessitates specialized tools or packages to retrieve information. Concurrently, the use of Large Language Models for text and information retrieval is growing. The idea of this research is to see if the current widely adopted LLMs (ChatGPT) are able to retrieve information from GTFS using natural language instructions. We first test whether ChatGPT (GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our multiple-choice questions (MCQ) correctly. Next, we task the LLM with information extractions from a filtered GTFS feed with 4 routes. For information retrieval, we compare zero-shot and program synthesis. Program synthesis works better, achieving ~90% accuracy on simple questions and ~40% accuracy on complex questions.
    
[^68]: 使用监视器引导全局上下文指导代码语言模型

    Guiding Language Models of Code with Global Context using Monitors. (arXiv:2306.10763v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.10763](http://arxiv.org/abs/2306.10763)

    本文提出了一种使用监视器引导全局上下文的方法来指导代码语言模型，在处理类型、功能或API等全局上下文时，能够提高代码语言模型的性能和准确性。

    

    代码语言模型（LMs）在周围代码提供足够上下文时效果很好。但当需要在存储库或链接库中使用在训练过程中未见过的类型、功能或API时，这种情况就不再成立。LMs在对这种全局上下文的意识有限时会出现错误预测的情况。集成开发环境（IDEs）通过静态分析帮助开发人员了解存储库上下文。我们将开发人员享受到的这种帮助扩展到了LMs。我们提出了监视器引导解码（MGD）的方法，其中监视器使用静态分析来引导解码过程。我们构建了一个用于Java方法补全的存储库级数据集PragmaticCode，并在其上评估了MGD。在不同参数规模的模型上，通过监视类型一致的对象解引用，MGD能够持续提高编译率并与真实结果达成一致。此外，具有更少参数的LMs，在与MGD相结合时能够超越更大的LMs的性能。

    Language models of code (LMs) work well when the surrounding code provides sufficient context. This is not true when it becomes necessary to use types, functionality or APIs defined elsewhere in the repository or a linked library, especially those not seen during training. LMs suffer from limited awareness of such global context and end up hallucinating.  Integrated development environments (IDEs) assist developers in understanding repository context using static analysis. We extend this assistance, enjoyed by developers, to LMs. We propose monitor-guided decoding (MGD) where a monitor uses static analysis to guide the decoding. We construct a repository-level dataset PragmaticCode for method-completion in Java and evaluate MGD on it. On models of varying parameter scale, by monitoring for type-consistent object dereferences, MGD consistently improves compilation rates and agreement with ground truth. Further, LMs with fewer parameters, when augmented with MGD, can outperform larger LM
    
[^69]: 采用优势诱导策略对齐的Fine-Tuning语言模型

    Fine-Tuning Language Models with Advantage-Induced Policy Alignment. (arXiv:2306.02231v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02231](http://arxiv.org/abs/2306.02231)

    本论文提出了一种新算法APA，其采用优势诱导策略对齐用于强化学习语言模型。相对于传统方法（PPO），APA在语言任务中表现更好，避免了模型的崩溃与不稳定性。

    

    人类反馈强化学习（RLHF）已经成为将大型语言模型（LLMs）与人类偏好对齐的可靠方法。在众多RLHF技术中，接近策略优化（PPO）是最常用的方法之一。然而，尽管PPO很流行，但它可能会遭受模式崩溃、不稳定和效率低下的问题。我们展示了一种新颖的算法--基于估计优势的平方误差损失函数的优势诱导策略对齐（APA），可以减轻这些问题。我们通过实验证明，当使用单独的奖励模型作为评估器时，APA在语言任务中始终比PPO表现出更好的性能。此外，与PPO相比，APA可以更稳定地控制模型与初始策略的偏差，确保模型提高性能而不会崩溃为确定性输出。除了经验结果之外，我们还提供了APA的理论分析。

    Reinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning large language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal policy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO may suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can be alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment (APA), which leverages a squared error loss function based on the estimated advantages. We demonstrate empirically that APA consistently outperforms PPO in language tasks by a large margin, when a separate reward model is employed as the evaluator. In addition, compared with PPO, APA offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output. In addition to empirical results, we also prov
    
[^70]: 基于大语言模型的特定领域语言生成中的语法提示

    Grammar Prompting for Domain-Specific Language Generation with Large Language Models. (arXiv:2305.19234v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.19234](http://arxiv.org/abs/2305.19234)

    本文提出了一种基于语法提示的方法，使用专用的语法来增强示例，为大型语言模型（LLM）在特定领域的语言生成任务中使用外部知识和特定约束条件进行上下文学习。

    

    大型语言模型（LLM）可以从仅有几个上下文示例中学习执行各种自然语言任务。然而，对于从高度结构化的语言（例如，从语义解析到复杂的特定领域语言）生成字符串，LLM只从少量示例中进行泛化是具有挑战性的。我们探讨了$\textbf{语法提示}$作为一种简单的方法，通过在背科斯-诺尔范式（BNF）中表达的语法来启用LLM使用外部知识和特定领域的约束条件来进行上下文学习。语法提示使用一个专门的语法来增强每个演示示例，该语法足以生成特定的输出示例，其中该专门的语法是全DSL语法的子集。对于推理，LLM首先预测一个给定测试输入的BNF语法，然后根据语法规则生成输出。实验表明，语法提示可以使LLM在特定领域的语言生成任务中表现出色。

    Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples. However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We explore $\textbf{grammar prompting}$ as a simple approach for enabling LLMs to use external knowledge and domain-specific constraints, expressed through a grammar expressed in Backus--Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perfor
    
[^71]: 扩散模型是否是视觉语言推理器？

    Are Diffusion Models Vision-And-Language Reasoners?. (arXiv:2305.16397v1 [cs.CV])

    [http://arxiv.org/abs/2305.16397](http://arxiv.org/abs/2305.16397)

    本文针对扩散-语言图像生成模型进行转换和评估，介绍了生成-鉴别评估基准(GDBench)基于7个视觉语言复杂任务，并发现转换后的模型在组合性任务方面的表现优于CLIP，通过微调可提高其组合性能。

    

    近期，使用去噪扩散过程的文本-图像生成模型已取得了巨大的定性成功。然而，与鉴别式视觉-语言模型不同，将基于扩散的生成模型置于自动细粒度定量评估高级现象（如组合性）的任务中是一项非常棘手的任务。为此，我们开展了两项创新。首先，我们使用一种称为DiffusionITM的新方法将基于扩散的模型（在我们的情况下，是稳定扩散）转换为任何图像文本匹配(ITM)任务。其次，我们引入了7个复杂的视觉语言任务、偏差评估和详细分析的生成-鉴别评估基准(GDBench)。我们发现，Stable Diffusion + DiffusionITM在许多任务上具有竞争力，并在组合性任务（如CLEVR和Winoground等）上优于CLIP。我们通过在MS-COCO上微调保持图像特征的转移设置进一步提高其组合性能。

    Text-conditioned image generation models have recently shown immense qualitative success using denoising diffusion processes. However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality. Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, Stable Diffusion) for any image-text matching (ITM) task using a novel method called DiffusionITM. Second, we introduce the Generative-Discriminative Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis. We find that Stable Diffusion + DiffusionITM is competitive on many tasks and outperforms CLIP on compositional tasks like like CLEVR and Winoground. We further boost its compositional performance with a transfer setup by fine-tuning on MS-COCO while retaining ge
    
[^72]: LLMDet:一种第三方大型语言模型生成文本检测工具

    LLMDet: A Third Party Large Language Models Generated Text Detection Tool. (arXiv:2305.15004v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15004](http://arxiv.org/abs/2305.15004)

    LLMDet是一个第三方大型语言模型生成文本检测工具，能够从特定的语言模型中确定生成文本的来源，并满足精细追踪、中间判断和快速检测的要求。

    

    大型语言模型（LLM）生成的文本与高质量的人工撰写文本非常相似，引发了对其在传播虚假信息和学术不端行为中的潜在滥用的担忧。因此，迫切需要一种高度实用的检测工具，能够准确识别给定文本的来源。然而，现有的检测工具通常依赖于对LLM的访问，并且只能区分机器生成和人工撰写的文本，未能满足精细追踪、中间判断和快速检测的要求。因此，我们提出了LLMDet，一种特定于模型的安全、高效、可扩展的检测工具，可以从特定的LLM（如GPT-2、OPT、LLaMA等）中获取文本。在LLMDet中，我们记录了显著n-gram的下一个标记概率作为特征，用于计算每个LLM的代理困惑度。通过联合分析LLM的代理困惑度，我们可以确定生成文本的来源。

    Generated texts from large language models (LLMs) are remarkably close to high-quality human-authored text, raising concerns about their potential misuse in spreading false information and academic misconduct. Consequently, there is an urgent need for a highly practical detection tool capable of accurately identifying the source of a given text. However, existing detection tools typically rely on access to LLMs and can only differentiate between machine-generated and human-authored text, failing to meet the requirements of fine-grained tracing, intermediary judgment, and rapid detection. Therefore, we propose LLMDet, a model-specific, secure, efficient, and extendable detection tool, that can source text from specific LLMs, such as GPT-2, OPT, LLaMA, and others. In LLMDet, we record the next-token probabilities of salient n-grams as features to calculate proxy perplexity for each LLM. By jointly analyzing the proxy perplexities of LLMs, we can determine the source of the generated text
    
[^73]: 自动评估度量中的性别偏见：基于图像字幕的案例研究

    Gender Biases in Automatic Evaluation Metrics: A Case Study on Image Captioning. (arXiv:2305.14711v1 [cs.CL])

    [http://arxiv.org/abs/2305.14711](http://arxiv.org/abs/2305.14711)

    本文研究了模型评估度量中的性别偏见对图像字幕任务的影响，并提出了替代方案以解决这一问题。

    

    预训练模型评估度量在图像字幕等各种自然语言生成任务中已经证明具有强大的性能，并与人类判断高度相关。然而，它们对公平性的影响尚未被深入探讨——人们普遍认为预训练模型可能会编码社会偏见，并将其用于评估目的可能会无意中表现并潜在地放大偏见。本文针对图像字幕任务，对模型评估度量中的性别偏见进行了系统研究。具体而言，我们首先确定和量化了不同评估度量中关于职业、活动和物体概念的性别偏见。然后，我们展示了使用这些有偏见的度量带来的负面后果，比如在部署过程中偏向有偏见的生成模型，并通过强化学习向生成模型传播偏见。我们还提出了一个简单但有效的替代方案。

    Pretrained model-based evaluation metrics have demonstrated strong performance with high correlations with human judgments in various natural language generation tasks such as image captioning. Despite the impressive results, their impact on fairness is under-explored -- it is widely acknowledged that pretrained models can encode societal biases, and utilizing them for evaluation purposes may inadvertently manifest and potentially amplify biases. In this paper, we conduct a systematic study in gender biases of model-based evaluation metrics with a focus on image captioning tasks. Specifically, we first identify and quantify gender biases in different evaluation metrics regarding profession, activity, and object concepts. Then, we demonstrate the negative consequences of using these biased metrics, such as favoring biased generation models in deployment and propagating the biases to generation models through reinforcement learning. We also present a simple but effective alternative to r
    
[^74]: 通过交互式问题-知识对齐解决语言模型幻觉问题

    Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment. (arXiv:2305.13669v1 [cs.CL])

    [http://arxiv.org/abs/2305.13669](http://arxiv.org/abs/2305.13669)

    本文提出了MixAlign框架，通过与用户和知识库交互，实现自动的问题-知识对齐，从而解决了语言模型因无法正确理解问题和知识而导致的幻觉问题。

    

    尽管语言模型近期进展显著，但仍面临幻觉问题，可能会生成误导性和不支持的回答。一种缓解幻觉问题的常见方法是从知识库中检索和整合支持证据。然而，用户的问题通常与存储的知识不太对齐，因为他们在提问前不知道可用的信息。这种不对齐可能限制语言模型定位和利用知识的能力，可能迫使其通过忽略或覆盖检索到的证据而产生幻觉。为了解决这个问题，我们介绍了 MixAlign，一个框架，它与用户和知识库交互以获得并整合关于用户问题与存储信息相关性的澄清信息。 MixAlign 采用语言模型实现自动问题-知识对齐，并在需要时通过人工用户澄清进一步增强这种对齐。

    Despite the remarkable recent advances in language models, they still struggle with the hallucination problem and can generate misleading and unsupported responses. A common approach to mitigate the hallucination issue is retrieving and incorporating supporting evidence from a knowledge base. However, user questions usually do not align well with the stored knowledge, as they are unaware of the information available before asking questions. This misalignment can limit the language model's ability to locate and utilize the knowledge, potentially forcing it to hallucinate by ignoring or overriding the retrieved evidence. To address this issue, we introduce MixAlign, a framework that interacts with both the user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information. MixAlign employs a language model to achieve automatic question-knowledge alignment and, if necessary, further enhances this alignment through human user clari
    
[^75]: Flover：一种用于高效自回归模型并行推断的时间融合框架

    Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v1 [cs.DC])

    [http://arxiv.org/abs/2305.13484](http://arxiv.org/abs/2305.13484)

    Flover是一种用于自回归模型并行推断的时间融合框架，解决了并行性不足和灵活性差的问题，可以实现更加高效的推断性能。

    

    在深度学习领域快速发展的背景下，模型推断性能成为一个关键因素，尤其是在模型变得更加复杂并被部署在多个应用场景中的情况下。自回归模型由于在众多生成任务中表现优异，因此备受关注。这些模型设计上采用了一种时间依赖结构，其中当前token的概率分布受到前面token的影响。然而，这种本质上的序列特性遵循马尔可夫链假设，缺乏时间并行性，因此存在独特的挑战。特别是在工业背景下，推断请求遵循泊松时间分布，需要不同的响应长度，这种并行性的缺失更加明显。现有的解决方案如动态批处理和并发模型实例，然而，这些粗粒度的方法存在严重的开销和缺乏灵活性，无法实现最优化。

    In the rapidly evolving field of deep learning, the performance of model inference has become a pivotal aspect as models become more complex and are deployed in diverse applications. Among these, autoregressive models stand out due to their state-of-the-art performance in numerous generative tasks. These models, by design, harness a temporal dependency structure, where the current token's probability distribution is conditioned on preceding tokens. This inherently sequential characteristic, however, adheres to the Markov Chain assumption and lacks temporal parallelism, which poses unique challenges. Particularly in industrial contexts where inference requests, following a Poisson time distribution, necessitate diverse response lengths, this absence of parallelism is more profound. Existing solutions, such as dynamic batching and concurrent model instances, nevertheless, come with severe overheads and a lack of flexibility, these coarse-grained methods fall short of achieving optimal la
    
[^76]: 在大语言模型时代重新思考对话型推荐系统的评估

    Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models. (arXiv:2305.13112v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13112](http://arxiv.org/abs/2305.13112)

    本文重新思考了大语言模型时代下对话型推荐系统的评估问题，提出了一种基于大语言模型的交互式评估方法iEvaLM，通过实验证明了该方法相较于现有评估协议具有显著的改进，并强调了对外部知识的评估。

    

    最近大语言模型（LLMs）的成功表明其在发展更强大的对话型推荐系统（CRSs）方面具有巨大潜力，这些系统依赖于自然语言对话来满足用户需求。本文探讨了利用ChatGPT进行对话型推荐的可行性，并揭示了现有评估协议的不足之处。现有评估协议可能过分强调与由人类标注者生成的地面真实物品或话语的匹配，而忽视了作为一种有能力的CRS的交互性质。为了克服这种局限性，我们进一步提出了一种基于LLMs的交互式评估方法，名为iEvaLM，该方法利用了基于LLMs的用户模拟器。我们的评估方法可以模拟用户和系统之间的各种交互场景。通过对两个公开可用的CRS数据集进行实验，我们证明了与流行的评估协议相比的显著改进。此外，我们强调了外部知识的评估。

    The recent success of large language models (LLMs) has shown great potential to develop more powerful conversational recommender systems (CRSs), which rely on natural language conversations to satisfy user needs. In this paper, we embark on an investigation into the utilization of ChatGPT for conversational recommendation, revealing the inadequacy of the existing evaluation protocol. It might over-emphasize the matching with the ground-truth items or utterances generated by human annotators, while neglecting the interactive nature of being a capable CRS. To overcome the limitation, we further propose an interactive Evaluation approach based on LLMs named iEvaLM that harnesses LLM-based user simulators. Our evaluation approach can simulate various interaction scenarios between users and systems. Through the experiments on two publicly available CRS datasets, we demonstrate notable improvements compared to the prevailing evaluation protocol. Furthermore, we emphasize the evaluation of ex
    
[^77]: ACCENT:一种开放领域对话系统的自动事件常识评价方法。

    ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain Dialogue Systems. (arXiv:2305.07797v1 [cs.CL])

    [http://arxiv.org/abs/2305.07797](http://arxiv.org/abs/2305.07797)

    ACCENT是一种基于常识知识库的事件常识评价方法，通过对话中提取的事件-关系元组与CSKB的兼容性评估响应，是一种有效的评价方法。

    

    常识推理在人类交流中普遍存在，因此对于开放领域的对话系统来说是一个重要特征。但是，评估对话系统中的常识推理仍然是一个挑战。我们首先关注事件常识，它考虑事件及其关系，在对话和一般常识推理中至关重要。我们提出ACCENT，一种受常识知识库 (CSKBs) 授权的事件常识评价指标。ACCENT首先从对话中提取事件-关系元组，然后通过计算它们与CSKB的兼容性来评估响应。为了评估ACCENT，我们构建了第一个面向开放域对话的事件常识评价数据集。我们的实验结果表明，ACCENT是一种有效的事件常识评估指标，比现有基线模型更能与人类判断相关联。

    Commonsense reasoning is omnipresent in human communications and thus is an important feature for open-domain dialogue systems. However, evaluating commonsense in dialogue systems is still an open challenge. We take the first step by focusing on event commonsense that considers events and their relations, and is crucial in both dialogues and general commonsense reasoning. We propose ACCENT, an event commonsense evaluation metric empowered by commonsense knowledge bases (CSKBs). ACCENT first extracts event-relation tuples from a dialogue, and then evaluates the response by scoring the tuples in terms of their compatibility with the CSKB. To evaluate ACCENT, we construct the first public event commonsense evaluation dataset for open-domain dialogues. Our experiments show that ACCENT is an efficient metric for event commonsense evaluation, which achieves higher correlations with human judgments than existing baselines.
    
[^78]: DIN-SQL: 自纠正的文本到SQL分解式上下文学习

    DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction. (arXiv:2304.11015v1 [cs.CL])

    [http://arxiv.org/abs/2304.11015](http://arxiv.org/abs/2304.11015)

    DIN-SQL通过将复杂的文本到SQL任务分解为子问题，并将这些子问题的解决方案馈入到大型语言模型中，显著提高了它们的表现，使准确性超过了当前最先进的技术。

    

    本文研究了将复杂的文本到SQL任务分解为较小的子任务，并且这种分解如何显著提高大型语言模型在推理过程中的表现。我们展示了尽管SQL查询具有声明式结构，但可以将其分解为子问题，并将这些子问题的解决方案馈入到大型语言模型中，从而显著提高它们的表现。我们的实验表明，这种方法能够稳定提高三种大型语言模型的表现，大约提高了10％，将大型语言模型的准确性推向最新水平，并在Holdout Spider数据集上甚至超过了经过精调的大型模型。

    We study the problem of decomposing a complex text-to-sql task into smaller sub-tasks and how such a decomposition can significantly improve the performance of Large Language Models (LLMs) in the reasoning process. There is currently a significant gap between the performance of fine-tuned models and prompting approaches using LLMs on challenging text-to-sql datasets such as Spider. We show that SQL queries, despite their declarative structure, can be broken down into sub-problems and the solutions of those sub-problems can be fed into LLMs to significantly improve their performance. Our experiments with three LLMs show that this approach consistently improves their performance by roughly 10%, pushing the accuracy of LLMs towards state-of-the-art, and even beating large fine-tuned models on the holdout Spider dataset.
    
[^79]: OpenAGI：当LLM遇到领域专家

    OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])

    [http://arxiv.org/abs/2304.04370](http://arxiv.org/abs/2304.04370)

    基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。

    

    人类具有将基本技能组合成复杂技能以解决复杂任务的显著能力。这种能力对于人工智能同样重要，因此，我们断言，除了开发大型综合智能模型外，将不同领域专家模型应用于复杂任务解决能力同样关键，以在人工智能通用智能的追求中使其具备这种能力。最近的大型语言模型（LLM）的发展证明其具有出色的学习和推理能力，使它们成为选择、综合和执行外部模型以解决复杂任务的控制器的有前途的选择。在这个项目中，我们开发了一个名为OpenAGI的开源AGI研究平台，专门设计为提供复杂的多步骤任务，并配有任务特定的数据集、评估指标和各种可扩展模型。OpenAGI将复杂任务阐释为自然语言问答，旨在促进领域专家和语言模型之间的协同作用。

    Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
    
[^80]: 为什么要逐步思考？推理源于经验的局部性。

    Why think step-by-step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v1 [cs.AI])

    [http://arxiv.org/abs/2304.03843](http://arxiv.org/abs/2304.03843)

    本文通过语言模型研究何时以及为什么推理是有帮助的，测试推理在训练数据由相互影响强烈的局部变量集群组成时是否有效。通过一步步的推理，能够将准确的局部推理链接在一起，以估算在训练中没有同时观察到的变量之间的关系。

    

    人类有着强大而神秘的推理能力。通过一系列纯粹的思维步骤，我们可以推理出我们无法直接得出的推论 - 尽管我们从世界上没有得到任何额外数据。同样地，大型语言模型可以通过一步步的推理，在回答问题之前生成中间步骤，从而更好地完成复杂的任务。我们使用语言模型研究何时以及为什么推理是有帮助的，测试推理在训练数据由相互影响强烈的局部变量集群组成时是否有效。这些训练条件能够将准确的局部推理链接在一起，以估算在训练中没有同时观察到的变量之间的关系。我们使用贝叶斯网络定义的联合分布的样品对自回归变压器进行训练，但每个样品只包括其中的一部分变量。我们比较使用推理生成的变量子集与使用完整集合进行训练的方案的性能。

    Humans have a powerful and mysterious capacity to reason. By working through a series of purely mental steps, we can make inferences we would not be capable of making directly -- despite that fact that we get no additional data from the world. Similarly, large language models can perform better at complex tasks through chain-of-thought reasoning, where they generate intermediate steps before answering a question. We use language models to investigate the questions of when and why reasoning is helpful, testing the hypothesis that reasoning is effective when training data consisting of local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences in order to estimate relationships between variables that were not seen together in training. We train an autoregressive transformer on samples from joint distributions defined by Bayes nets, but only include a subset of all the variables in each sample. We compare lang
    
[^81]: 检索增强语言模型是否具备推理能力？检索模块和语言模型之争

    Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model. (arXiv:2212.09146v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09146](http://arxiv.org/abs/2212.09146)

    本论文研究了检索增强语言模型的推理能力，发现检索器和语言模型之间存在推卸责任的问题，且检索器选择的句子和语言模型不考虑句子之间的复杂关系都会影响推理性能。针对这些问题，本文提出了一种新的框架ReForMask，采用掩码检索方法来更好地捕捉语句之间的复杂关系并在多个任务上实现了显著优化。

    

    预先训练的语言模型采用检索器来选择支持文档，在解决常见的NLP问题（包括语言建模和问答）方面表现出良好的效果，并且具有可解释性。本文首先研究了检索增强语言模型（REALM，kNN-LM，FiD和DPR，ATLAS和Flan-T5和Contriever耦合）在不同任务中推理检索语句的优点和局限性。我们展示了检索-阅读模型在推理方面的局限性既来自检索模块，也来自语言模型。实验结果表明，检索器使用的相似度度量通常不足以用于推理任务。此外，我们发现，检索增强模型中的语言模型不考虑语句之间的复杂关系，导致即使使用较大的模型，推理性能也不佳。此外，我们发现检索器和语言模型在推理中面临着“责怪游戏”的问题：当检索器选择正确的语句时，语言模型可以进行良好的推理；当检索器选择错误的语句时，语言模型无法进行良好的推理。为解决这些问题，我们提出了一种新的框架ReForMask，采用掩码检索方法来更好地捕捉语句之间的复杂关系。我们的实验结果表明，ReForMask在多种常见的NLP任务上显著优于现有的检索增强模型。

    Augmenting pretrained language models with retrievers to select the supporting documents has shown promise in effectively solving common NLP problems, including language modeling and question answering, in an interpretable way. In this paper, we first study the strengths and weaknesses of different retriever-augmented language models (REALM, $k$NN-LM, FiD coupled with DPR, and ATLAS and Flan-T5 coupled with Contriever) in reasoning over the retrieved statements in different tasks. We show how the retrieve-then-read models' limitations in reasoning are rooted both in the retriever module as well as the language model. Our experimental results demonstrate that the similarity metric used by the retrievers is generally insufficient for reasoning tasks. Additionally, we show that the language models in retriever-augmented models do not take the complicated relations between the statements into account, which leads to poor reasoning performance even when using the larger models. Moreover, we
    
[^82]: 使用基于偏好的强化学习实现抽象时间线摘要

    Towards Abstractive Timeline Summarisation using Preference-based Reinforcement Learning. (arXiv:2211.07596v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07596](http://arxiv.org/abs/2211.07596)

    本文提出了一种基于偏好的强化学习方法，在抽象摘要的基础上优化时间线摘要的质量和可读性，通过自动化和人工评估验证了方法的有效性。

    

    本文介绍了一种新颖的流程，用于总结多个新闻来源报道的时间线。基于Transformer的抽象摘要模型可以生成连贯且简洁的长文档摘要，但在时间线摘要（TLS）等特定任务上可能无法超越已建立的抽取方法。虽然抽取摘要更忠实于来源，但可能不太易读且包含冗余或不必要的信息。本文提出了一种基于偏好的强化学习（PBRL）方法，用于将预训练的抽象摘要器适应于TLS，可以克服抽取时间线摘要的缺点。我们定义了一个复合奖励函数，通过关键词和偏好标签学习，并将其用于通过线下强化学习对预训练的抽象摘要器进行微调。我们对三个数据集进行了自动化和人工评估，发现我们的方法优于可比较的方法。

    This paper introduces a novel pipeline for summarising timelines of events reported by multiple news sources. Transformer-based models for abstractive summarisation generate coherent and concise summaries of long documents but can fail to outperform established extractive methods on specialised tasks such as timeline summarisation (TLS). While extractive summaries are more faithful to their sources, they may be less readable and contain redundant or unnecessary information. This paper proposes a preference-based reinforcement learning (PBRL) method for adapting pretrained abstractive summarisers to TLS, which can overcome the drawbacks of extractive timeline summaries. We define a compound reward function that learns from keywords of interest and pairwise preference labels, which we use to fine-tune a pretrained abstractive summariser via offline reinforcement learning. We carry out both automated and human evaluation on three datasets, finding that our method outperforms a comparable 
    
[^83]: 用于评估任务导向的对话系统的隐喻用户模拟器

    Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems. (arXiv:2204.00763v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.00763](http://arxiv.org/abs/2204.00763)

    本文提出了一种用于评估任务导向的对话系统的隐喻用户模拟器。该模拟器可以生成类似人类对话的模拟评估，并且提供了一个测试者框架来生成不同能力的对话系统变体。

    

    任务导向的对话系统（TDS）主要在离线环境中或通过人工评估进行评估。评估通常仅限于单轮或非常耗时。作为一个替代方案，模拟用户行为的用户模拟器可以让我们考虑一系列用户目标，以生成类似人类对话的模拟评估。然而，利用现有用户模拟器评估TDS具有挑战性，因为用户模拟器主要设计用于优化TDS的对话策略，并具有有限的评估能力。此外，用户模拟器的评估是一个开放性挑战。在这项工作中，我们提出了一种用于端到端TDS评估的隐喻用户模拟器，其中我们将模拟器定义为隐喻性的，如果它在与系统的交互中模拟用户的类比思维。我们还提出了一个基于测试者的评估框架来生成具有不同能力的对话系统变体。我们的用户模拟器构建了一个隐喻性用户模型。

    Task-oriented dialogue systems (TDSs) are assessed mainly in an offline setting or through human evaluation. The evaluation is often limited to single-turn or is very time-intensive. As an alternative, user simulators that mimic user behavior allow us to consider a broad set of user goals to generate human-like conversations for simulated evaluation. Employing existing user simulators to evaluate TDSs is challenging as user simulators are primarily designed to optimize dialogue policies for TDSs and have limited evaluation capabilities. Moreover, the evaluation of user simulators is an open challenge.  In this work, we propose a metaphorical user simulator for end-to-end TDS evaluation, where we define a simulator to be metaphorical if it simulates user's analogical thinking in interactions with systems. We also propose a tester-based evaluation framework to generate variants, i.e., dialogue systems with different capabilities. Our user simulator constructs a metaphorical user model th
    
[^84]: CoPaSul手册--基于轮廓的参数化和叠加韵律风格化

    CoPaSul Manual -- Contour-based parametric and superpositional intonation stylization. (arXiv:1612.04765v11 [cs.CL] UPDATED)

    [http://arxiv.org/abs/1612.04765](http://arxiv.org/abs/1612.04765)

    CoPaSul工具包提供了自动的韵律标注和特征提取功能，使用了基于轮廓的参数化和叠加韵律风格化的方法。通过该工具包可以得到与韵律边界和突出性相关的特征，并可以通过系数聚类得到韵律轮廓类别。

    

    CoPaSul工具包的目的是自动的韵律标注和从音节到语句级别的韵律特征提取。在这个框架下，韵律被表示为全局和局部轮廓的叠加，这些轮廓在多项式系数的参数化描述下。在全局层面上（通常与但不一定限于语调短语相关），风格化用于以时间变化的F0水平和范围来表示音调。在局部层面上（例如，重音组），描述局部轮廓形状。通过这种参数化方法，可以得到几个与韵律边界和突出性相关的特征。此外，通过系数聚类，可以以自下而上的方式获得韵律轮廓类别。除了基于风格化的特征提取外，还可以使用标准的F0和能量测量（例如，平均值和方差）以及韵律方面的特征。

    The purposes of the CoPaSul toolkit are (1) automatic prosodic annotation and (2) prosodic feature extraction from syllable to utterance level. CoPaSul stands for contour-based, parametric, superpositional intonation stylization. In this framework intonation is represented as a superposition of global and local contours that are described parametrically in terms of polynomial coefficients. On the global level (usually associated but not necessarily restricted to intonation phrases) the stylization serves to represent register in terms of time-varying F0 level and range. On the local level (e.g. accent groups), local contour shapes are described. From this parameterization several features related to prosodic boundaries and prominence can be derived. Furthermore, by coefficient clustering prosodic contour classes can be obtained in a bottom-up way. Next to the stylization-based feature extraction also standard F0 and energy measures (e.g. mean and variance) as well as rhythmic aspects c
    

