# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [D4: Improving LLM Pretraining via Document De-Duplication and Diversification.](http://arxiv.org/abs/2308.12284) | 通过预训练模型嵌入和数据重复方法，我们展示了D4算法可以在LLM预训练中加速训练并提高下游任务准确率。 |
| [^2] | [Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models.](http://arxiv.org/abs/2308.12272) | 这项研究检验了基础语言模型(FLMs)及其集成在基准和真实数据集上的表现，发现集成可以影响FLMs的个体关注，并展示不同FLMs之间的协调和合作优势。 |
| [^3] | [Prompt2Model: Generating Deployable Models from Natural Language Instructions.](http://arxiv.org/abs/2308.12261) | Prompt2Model是一种方法，可以通过自然语言描述任务来训练适合部署的模型，相比大型语言模型，它们具有更高的性能。 |
| [^4] | [How to Protect Copyright Data in Optimization of Large Language Models?.](http://arxiv.org/abs/2308.12247) | 本文提出了一种方法来在优化大型语言模型时避免生成版权数据，通过将大型语言模型训练和优化视为softmax回归问题，并建立一种高效进行softmax回归的方法。 |
| [^5] | [Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning.](http://arxiv.org/abs/2308.12219) | 本文研究表明，通过扩展扩散语言模型的数据、规模和任务，可以有效使其成为强大的语言学习者。实验证明，扩展扩散语言模型在解决通用语言任务方面能够持续提高性能。 |
| [^6] | [The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection.](http://arxiv.org/abs/2308.12215) | 本研究通过虚假信息检测为例，检查了机器学习在信任与安全问题中学术与实践之间的脱节，并发现了文献中存在的严重不足之处，包括任务不符合在线服务面临的挑战、数据集和模型评估不真实、评估不独立于模型训练等。在此基础上，提出了评估机器学习应用于信任与安全问题的建议。 |
| [^7] | [Curriculum Learning with Adam: The Devil Is in the Wrong Details.](http://arxiv.org/abs/2308.12202) | 本文研究了为何课程学习在自然语言处理领域取得有限的成功。研究发现，当将课程与Adam优化算法结合使用时，它们学习适应了次优化参数，导致其脆弱性增加 |
| [^8] | [Evaluation of Faithfulness Using the Longest Supported Subsequence.](http://arxiv.org/abs/2308.12157) | 本文引入了一种新方法来评估机器生成文本的忠实性，通过计算由语境支持的主张中最长的非连续子字符串，称之为最长支持子序列（LSS）。证明了当使用LSS时，这种度量与人类评分更相关，并且在忠实性评估上相较于先前最先进的度量有18%的改进。 |
| [^9] | [Semantic Change Detection for the Romanian Language.](http://arxiv.org/abs/2308.12131) | 本文研究了面向罗马尼亚语的语义变化检测方法，并通过分析实际数据集找出了影响语义变化检测的最重要因素。 |
| [^10] | [Instruction Position Matters in Sequence Generation with Large Language Models.](http://arxiv.org/abs/2308.12097) | 本研究提出了一种增强大型语言模型在序列生成任务中遵循指令能力的方法，通过调整任务指令的位置，从而弥补指令遗忘的问题。实验结果证明了该方法在不同规模和长度的序列上均优于传统设置。 |
| [^11] | [Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments.](http://arxiv.org/abs/2308.12086) | 本文将预训练的大型语言模型（LLMs）应用于网络安全环境，作为攻击代理人进行顺序决策。该设计表明LLMs在高效应对复杂决策方面具有潜力，并且在大多数场景中表现出与经过训练的最先进代理相似或更好的性能。 |
| [^12] | [InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4.](http://arxiv.org/abs/2308.12067) | InstructionGPT-4通过仅使用200个例子进行微调，在多模式指令数据质量度量和选择器的帮助下，在各种评估任务中优于原始的MiniGPT-4。 |
| [^13] | [FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering.](http://arxiv.org/abs/2308.12060) | FlexKBQA使用大型语言模型(LLMs)作为程序翻译器，通过自动化算法从知识库中抽样多样的程序，将其转换为自然语言问题，从而解决少样本知识库问答任务的挑战。 |
| [^14] | [Aligning Language Models with Offline Reinforcement Learning from Human Feedback.](http://arxiv.org/abs/2308.12050) | 本研究提出了一种离线强化学习与人类反馈（RLHF）框架，通过使用预生成的样本来对齐语言模型与人类偏好。我们采用了最大似然估计、奖励加权回归和决策Transformer等方法，并通过类似于监督微调的损失函数来实现对齐。 |
| [^15] | [CgT-GAN: CLIP-guided Text GAN for Image Captioning.](http://arxiv.org/abs/2308.12045) | 本文提出了CgT-GAN，通过引入图像为模型提供视觉信息，结合对抗训练和基于CLIP的奖励，实现了在没有人工注释的情况下进行图像描述。 |
| [^16] | [IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning.](http://arxiv.org/abs/2308.12043) | IncreLoRA是一种增量参数分配方法，根据模块的重要性分数在训练过程中自适应地添加可训练参数，以实现参数高效调优。 |
| [^17] | [Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep Learning Track.](http://arxiv.org/abs/2308.12039) | 本论文介绍了在 TREC 2022 深度学习赛道中，我们采用的混合文本检索和多阶段文本排名方法，在检索阶段结合了传统稀疏检索和神经稠密检索的两种结构，在排名阶段构建了全交互式排名模型和轻量级子排名模块，评估结果证明了方法的有效性。 |
| [^18] | [Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages.](http://arxiv.org/abs/2308.12038) | 本论文提出了一种在低资源语言中训练大型多模式模型的有效方法，通过利用多语言模型实现了跨语种零样本多模式学习，在图像到文本和文本到图像的生成任务上具有竞争力。 |
| [^19] | [PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine.](http://arxiv.org/abs/2308.12033) | 本文提出了一种名为PREFER的方法，通过反馈机制和自动合成新的提示来解决大型语言模型中的幻觉和不稳定性问题，从而提高性能。 |
| [^20] | [From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning.](http://arxiv.org/abs/2308.12032) | 该论文引入了一种自我引导的方法，让LLM能够自主地选择高质量的指令数据，通过引入指令遵循难度指标（IFD），大幅提高了模型训练效率，并在知名数据集上进行了验证，展示了优于传统数据输入的结果。 |
| [^21] | [Prompt-Based Length Controlled Generation with Reinforcement Learning.](http://arxiv.org/abs/2308.12030) | 提出了一种基于提示的长度控制方法，利用强化学习和奖励模型来实现大型语言模型（LLM）的长度受控生成。该方法可以有效减少推理成本并满足不同需求。 |
| [^22] | [Knowledge-injected Prompt Learning for Chinese Biomedical Entity Normalization.](http://arxiv.org/abs/2308.12025) | 我们提出了一种知识注入提示学习方法用于处理中文生物医学实体规范化任务，该方法通过有效编码医学实体中的知识项，并将它们注入到模型中，以增强模型的能力。 |
| [^23] | [Reranking Passages with Coarse-to-Fine Neural Retriever using List-Context Information.](http://arxiv.org/abs/2308.12022) | 本文提出了一种利用列表上下文信息的粗到细神经检索器来重新排序段落。该方法通过将其他候选句子的列表上下文信息纳入段落表示中，增强了段落表示。而且，该方法将列表上下文建模过程分为两个子过程，从而解决了段落注意机制的内存限制问题，允许高效编码大量候选答案的上下文信息。 |
| [^24] | [From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models.](http://arxiv.org/abs/2308.12014) | 本文综合调查了大模型对齐目标的不同观点，并追踪其演化路径，旨在帮助确定最重要的目标。 |
| [^25] | [Graecia capta ferum victorem cepit. Detecting Latin Allusions to Ancient Greek Literature.](http://arxiv.org/abs/2308.12008) | 该论文介绍了一种用于古典文学的三语句子RoBERTa模型SPhilBERTa，能够优秀地进行跨语言语义理解和识别古希腊语、拉丁语和英语之间相同句子的能力，并通过自动翻译生成了新的训练数据。研究还展示了SPhilBERTa在自动检测类际对应方面的能力。 |
| [^26] | [Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations.](http://arxiv.org/abs/2308.11995) | 该论文引入了Topical-Chat数据集，该数据集是一个基于知识的人-人对话数据集，用于推动开放域对话人工智能的研究。研究者训练了几个最先进的对话模型，并进行了自动化和人工评估。 |
| [^27] | [EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE.](http://arxiv.org/abs/2308.11971) | 本文引入了一种名为EVE的高效视觉-语言预训练模型，通过遮蔽信号建模和模态感知的方式，实现了统一的多模态Transformer网络，加速了训练进程，并取得了良好的效果。 |
| [^28] | [Audio Generation with Multiple Conditional Diffusion Model.](http://arxiv.org/abs/2308.11940) | 本论文提出了一种使用多条件扩散模型进行音频生成的方法。通过引入内容和风格等额外条件，增强了现有模型的可控性。这种方法可以精确控制生成音频的时间顺序、音高和能量。由于缺乏合适的数据集和评估指标，作者整合了现有数据集并进行了实验验证。 |
| [^29] | [Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement.](http://arxiv.org/abs/2308.11923) | 该论文提出了音频差异字幕生成（ADC）作为一种新的音频字幕生成扩展任务，用于描述类似但略有差异的音频片段之间的语义差异。通过引入交叉注意力集中的Transformer编码器和相似性-差异解缠，该方法有效解决了传统音频字幕生成中的差异描述问题，并利用可视化来改善注意力权重以提取差异。 |
| [^30] | [Bridging the Gap: Deciphering Tabular Data Using Large Language Model.](http://arxiv.org/abs/2308.11891) | 本研究旨在提升大型语言模型在理解表格数据上的能力，通过设计一个表格序列化模块和纠正机制来实现。实验结果表明，尽管相对于最先进技术仍有差距，但该方法在处理表格数据方面取得了一定的进展。 |
| [^31] | [Cabrita: closing the gap for foreign languages.](http://arxiv.org/abs/2308.11878) | Cabrita是一种解决性能和高效标记化问题的方法，以可承受的成本解决了从头训练模型的限制。 |
| [^32] | [Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver's License Knowledge Test.](http://arxiv.org/abs/2308.11827) | 本研究提出了一种方法，通过使用新的信息源的上下文，让GPT模型能够回答考试题目。在使用加利福尼亚驾驶手册作为信息源的测试中，GPT-3模型取得了96%的及格分数。 |
| [^33] | [Towards an On-device Agent for Text Rewriting.](http://arxiv.org/abs/2308.11807) | 给出了一种面向设备的文本重写代理的构建方法，通过新的指令调整方法和启发式强化学习框架，能够生成高质量的训练数据并提高性能。能够在保持模型能力的前提下，实现移动设备上的文本重写。 |
| [^34] | [Few-shot Anomaly Detection in Text with Deviation Learning.](http://arxiv.org/abs/2308.11780) | 本论文介绍了一种基于深度少样本学习的框架FATE，它通过离群学习明确地学习文本中的异常得分，并利用先前已知的少量异常示例，从而克服了传统方法中对无标签数据的依赖，并优化了异常得分的精确度和数据利用效率。 |
| [^35] | [Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model.](http://arxiv.org/abs/2308.11773) | 通过自动语音识别系统和深度学习主题模型，我们在智能手机采集的语音录音中识别出与抑郁相关的29个主题，并确定了其中6个主题作为抑郁的风险主题。此研究表明，通过长期监测语言使用，可以了解主题的出现与抑郁之间的关联。 |
| [^36] | [Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm.](http://arxiv.org/abs/2308.11767) | 本文介绍了一种能够提高对ChatGPT生成的假科学进行检测的算法。通过使用一种新设计的监督机器学习算法，该算法能够准确地将机器生成的出版物与科学家生成的出版物区分开来。结果表明，ChatGPT在技术术语方面与真实科学存在显著差异。算法在分类过程中取得了较高的准确率。 |
| [^37] | [Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models.](http://arxiv.org/abs/2308.11764) | 本文介绍了一种用于评估和减少开源弱大语言模型中幻觉问题的框架，并探索了知识注入和师生方法等技术来减轻低参数模型中的幻觉问题，实验结果表明，在挑战性领域中，这些模型的幻觉问题得到了减少。 |
| [^38] | [KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases.](http://arxiv.org/abs/2308.11761) | KnowledGPT是一个全面框架，通过将大型语言模型与知识库集成，实现了知识的检索和存储。与其他方法相比，KnowledGPT能够更全面和准确地回答各种问题。 |
| [^39] | [Knowledge Graph Prompting for Multi-Document Question Answering.](http://arxiv.org/abs/2308.11730) | 这篇论文提出了一种知识图谱引导的方法，用于在多文档问答任务中为大型语言模型（LLMs）提示正确的上下文。通过构建多个文档上的知识图谱，并设计基于语言模型的图遍历器，该方法能够帮助LLMs在MD-QA中进行答案预测。 |
| [^40] | [Advancing Relation Extraction through Language Probing with Exemplars from Set Co-Expansion.](http://arxiv.org/abs/2308.11720) | 本文提出了一种通过集合扩展和代表性示例的语言探测方法来推进关系提取。该方法通过整合相似度度量和类别排序，提高了关系分类准确性并减少对比类之间的混淆。经验证明该方法有效提高了关系提取的性能。 |
| [^41] | [Efficient Benchmarking (of Language Models).](http://arxiv.org/abs/2308.11696) | 本研究提出了一种名为"Efficient Benchmarking"的问题，旨在智能地减少语言模型评估的计算成本而不降低可靠性，并使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估决策的可靠性。通过HELM基准测试的案例研究，发现只需删除一个低排名模型即可改变领先者，并仅需少量示例即可得到正确的基准测试排名。 |
| [^42] | [Learning to generate and corr- uh I mean repair language in real-time.](http://arxiv.org/abs/2308.11683) | 本文针对实时语言处理能力的发展，使用动态语法和CHILDES语料库开发了一个基于概率的逐步生成模型，在78%的情况下能完全匹配最佳候选输出，且具备自我修复能力。 |
| [^43] | [Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model.](http://arxiv.org/abs/2308.11601) | Tryage是一个上下文感知的路由系统，能够根据对个体输入提示的分析，从模型库中选择最佳的专家模型，以消除模型选择和定制化的负担，释放庞大的新兴模型库的巨大威力给最终用户。 |
| [^44] | [Large Language Model as a User Simulator.](http://arxiv.org/abs/2308.11534) | 本文创新性地将从真实人机对话中提取的人类问题作为学习目标，并且训练了一个用户模拟器UserGPT，并使用生成的高质量合成对话数据集RealChat来训练助手模型ReaLM。实验证明，ReaLM在多个基准测试中超过了基准模型。 |
| [^45] | [Sentence-Level Multimodal and Language-Agnostic Representations.](http://arxiv.org/abs/2308.11466) | 这项研究引入了SONAR，一个多语言和多模态的句子嵌入空间，通过单一文本编码器在相似性搜索任务中取得显著优势，并提供了用于200种语言的文本解码器，可以进行文本到文本和语音到文本的机器翻译。这些结果相比现有模型具有竞争力，并且对语音到文本模型也取得了良好的结果。 |
| [^46] | [BAN-PL: a Novel Polish Dataset of Banned Harmful and Offensive Content from Wykop.pl web service.](http://arxiv.org/abs/2308.10592) | BAN-PL是波兰语的第一个开放数据集，包含来自Wykop这个类似"波兰版Reddit"的社交网络服务的被标记为有害并删除的内容，将有助于改进自动检测互联网上的冒犯性语言的技术。 |
| [^47] | [A Human-on-the-Loop Optimization Autoformalism Approach for Sustainability.](http://arxiv.org/abs/2308.10380) | 本文介绍了一种使用大型语言模型的自然对话方法来解决个性化能源相关问题的新概念。我们的方法将自然语言任务规范自动翻译为优化实例，使得模型能够理解和响应用户规范和偏好，并提供非线性推理能力。这一方法突破了当前基于提示的技术的限制，能够解决各种实例相关的能源问题。 |
| [^48] | [How Good Are Large Language Models at Out-of-Distribution Detection?.](http://arxiv.org/abs/2308.10261) | 本文通过对大型语言模型进行实证调查，探索了分布外检测的能力。作者发现了LLM在分布外检测方面的差异，并采用了新的生成式微调方法，提高了模型的性能。 |
| [^49] | [Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment.](http://arxiv.org/abs/2308.09662) | 这项工作以红队评估的方式对大型语言模型进行安全评估，发现即便是广泛部署的模型也容易受到连续发言提示的影响，导致违反伦理地对有害查询做出回应。通过红队评估尝试，发现多数开源LLM也会生成有害回应。研究者提出了一种LLM安全对齐的方法。 |
| [^50] | [MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation.](http://arxiv.org/abs/2308.08239) | MemoChat提出了一种用于调优指令的流程，通过让大型语言模型使用备忘录来保持对话一致性。实验证实了其有效性。 |
| [^51] | [Backward Reasoning in Large Language Models for Verification.](http://arxiv.org/abs/2308.07758) | 本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。 |
| [^52] | [On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey.](http://arxiv.org/abs/2307.16680) | 本文综合调查了大规模生成模型的可信度问题，涵盖了隐私、安全、公平性和责任等多个维度，并提出了实际建议和未来发展方向。 |
| [^53] | [Exploring the Landscape of Natural Language Processing Research.](http://arxiv.org/abs/2307.10652) | 该论文系统分类和分析了ACL Anthology中的研究论文，提供了对研究领域的结构化概述和NLP领域的分类学。本研究总结了最新的NLP发展，并提出了未来工作的方向。 |
| [^54] | [Self-consistency for open-ended generations.](http://arxiv.org/abs/2307.06857) | 本论文提出了一种改进大规模预训练语言模型生成输出质量和一致性的新方法，通过扩展自洽性框架的适用性，实现了从一个候选集中恢复最优或接近最优的生成结果，并提出了一种轻量级无参数相似性函数来改进代码生成、自动形式化和摘要任务的效果。 |
| [^55] | [Automated Assignment and Classification of Software Issues.](http://arxiv.org/abs/2307.00009) | 本论文提出了一种自动分配和分类软件问题的方法。通过使用经过精心策划的语言特征和不同的机器学习方法，将问题分配给最相关的团队成员，并将其分类为不同的类别，以提高工作效率和准确性。 |
| [^56] | [Chain-of-Thought Prompt Distillation for Multimodal Named Entity and Multimodal Relation Extraction.](http://arxiv.org/abs/2306.14122) | 本研究提出了一种链式思维提示提取方法，将大型语言模型的推理能力转化为更紧凑的学生模型，从而提高了多模态命名实体识别和多模态关系抽取的效果。 |
| [^57] | [Domain Specific Question Answering Over Knowledge Graphs Using Logical Programming and Large Language Models.](http://arxiv.org/abs/2303.02206) | 该论文提出了一种在知识图谱上回答领域特定问题的方法，通过将经典的逻辑编程语言与大型语言模型整合，实现逻辑推理能力来解决KGQA任务。实验证明该方法在准确识别所有测试问题的正确答案实体方面表现出较高效果，即使仅使用少量标注数据进行训练。这种方法为解决领域特定图谱上的问题回答提供了有前景的解决方案，具有可解释性和稳健性。 |
| [^58] | [NLP as a Lens for Causal Analysis and Perception Mining to Infer Mental Health on Social Media.](http://arxiv.org/abs/2301.11004) | 本研究以NLP作为镜头，通过定位因果关系和感知挖掘心理健康，研究了社交媒体上用户的心理语言资源，提出了关键维度和领域，以促进临床心理学实践和个性化心理保健。 |
| [^59] | [Low-Resource Authorship Style Transfer: Can Non-Famous Authors Be Imitated?.](http://arxiv.org/abs/2212.08986) | 该论文介绍了一种低资源条件下的作者风格转移任务，该任务是一类更具挑战性的作者风格转移，仅使用有限数量的目标作者风格文本。该研究对于非知名作者的风格转移尚未有充分的研究，而现有的方法主要适用于已发表的作家、政治家或其他知名人士和作者风格。 |
| [^60] | [PyABSA: A Modularized Framework for Reproducible Aspect-based Sentiment Analysis.](http://arxiv.org/abs/2208.01368) | PyABSA是一个基于PyTorch的可复现的基于方面的情感分析的模块化框架，支持多个ABSA子任务和数据集，具有灵活的扩展性，并解决了数据稀缺问题。 |
| [^61] | [Making first order linear logic a generating grammar.](http://arxiv.org/abs/2206.08955) | 本文研究了一阶线性逻辑与扩展张量类型演算的关系，提出了一种固有的演绎系统。 |
| [^62] | [A Structured Span Selector.](http://arxiv.org/abs/2205.03977) | 提出了一种新颖的基于语法的结构化跨度选择模型，通过利用部分跨度级注释，摒弃了贪心跨度选择方案，为共指消解和语义角色标注任务带来了实证改进。 |
| [^63] | [Dive into Deep Learning.](http://arxiv.org/abs/2106.11342) | 《深入深度学习》是一本旨在使深度学习易于理解的开源书籍，提供从概念到代码的教学资源，旨在成为成为应用机器学习科学家的起点，并允许社区快速更新和互动讨论。 |

# 详细

[^1]: D4：通过文档去重与多样化改进LLM预训练

    D4: Improving LLM Pretraining via Document De-Duplication and Diversification. (arXiv:2308.12284v1 [cs.CL])

    [http://arxiv.org/abs/2308.12284](http://arxiv.org/abs/2308.12284)

    通过预训练模型嵌入和数据重复方法，我们展示了D4算法可以在LLM预训练中加速训练并提高下游任务准确率。

    

    近年来，越来越多的计算资源和数据被用于训练大规模语言模型(LLM)，通常是通过对来自大规模网络语料库中随机选择的尽可能多的标记进行一次学习。虽然在越来越大的互联网局部上进行训练会导致不断改进的性能，但是这些改进的规模随着规模的增加而减小，目前很少有研究探索数据选择对预训练和下游性能的影响，除了MinHash等简单的去重方法。在这里，我们通过预训练模型嵌入展示了通过谨慎的数据选择(在去重数据的基础上)可以加快训练(提高了20%的效率)并且在16个自然语言处理任务的平均下游准确率上有所提升(高达2%)，在6.7B模型规模上。此外，我们还展示了智能重复数据的表现总是优于基线训练(而重复随机数据的表现比基线训练更差)。我们的结果表明，聪明的数据处理能够显著提升LLM的训练效果和下游任务的准确率。

    Over recent years, an increasing amount of compute and data has been poured into training large language models (LLMs), usually by doing one-pass learning on as many tokens as possible randomly selected from large-scale web corpora. While training on ever-larger portions of the internet leads to consistent performance improvements, the size of these improvements diminishes with scale, and there has been little work exploring the effect of data selection on pre-training and downstream performance beyond simple de-duplication methods such as MinHash. Here, we show that careful data selection (on top of de-duplicated data) via pre-trained model embeddings can speed up training (20% efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up to 2%) at the 6.7B model scale. Furthermore, we show that repeating data intelligently consistently outperforms baseline training (while repeating random data performs worse than baseline training). Our results indicate that clever d
    
[^2]: 简单即是更好，大并不足够：走向基础语言模型的集成

    Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models. (arXiv:2308.12272v1 [cs.CL])

    [http://arxiv.org/abs/2308.12272](http://arxiv.org/abs/2308.12272)

    这项研究检验了基础语言模型(FLMs)及其集成在基准和真实数据集上的表现，发现集成可以影响FLMs的个体关注，并展示不同FLMs之间的协调和合作优势。

    

    基础语言模型(FLMs)推动了自然语言处理(NLP)研究的发展。当前的研究者正在开发更大的FLMs（例如，XLNet、T5）以实现上下文化的语言表示、分类和生成。虽然开发更大的FLMs具有显著优势，但也存在虚构和预测不确定性的风险。从根本上说，更大的FLMs建立在较小的FLMs（例如，BERT）的基础之上；因此，人们必须认识到较小的FLMs的潜力，这可以通过集成来实现。在当前研究中，我们在基准和现实世界的数据集上对FLMs及其集成进行了实际检验。我们假设FLMs的集成可以影响其个体关注，并揭示不同FLMs之间的协调和合作的优势。我们利用BERT并定义了三种其他的集成技术：{浅层、半深层和深层}，其中深层集成引入了一个知识引导。

    Foundational Language Models (FLMs) have advanced natural language processing (NLP) research. Current researchers are developing larger FLMs (e.g., XLNet, T5) to enable contextualized language representation, classification, and generation. While developing larger FLMs has been of significant advantage, it is also a liability concerning hallucination and predictive uncertainty. Fundamentally, larger FLMs are built on the same foundations as smaller FLMs (e.g., BERT); hence, one must recognize the potential of smaller FLMs which can be realized through an ensemble. In the current research, we perform a reality check on FLMs and their ensemble on benchmark and real-world datasets. We hypothesize that the ensembling of FLMs can influence the individualistic attention of FLMs and unravel the strength of coordination and cooperation of different FLMs. We utilize BERT and define three other ensemble techniques: {Shallow, Semi, and Deep}, wherein the Deep-Ensemble introduces a knowledge-guide
    
[^3]: Prompt2Model：从自然语言指令生成可部署模型

    Prompt2Model: Generating Deployable Models from Natural Language Instructions. (arXiv:2308.12261v1 [cs.CL])

    [http://arxiv.org/abs/2308.12261](http://arxiv.org/abs/2308.12261)

    Prompt2Model是一种方法，可以通过自然语言描述任务来训练适合部署的模型，相比大型语言模型，它们具有更高的性能。

    

    大型语言模型（LLM）使得系统构建者能够通过提示来创建能胜任的NLP系统，他们只需要用自然语言描述任务并提供一些示例。然而，在其他方面，LLM是传统专用NLP模型的一种退步；它们需要大量计算资源进行部署，并可能被API封锁。本文提出了Prompt2Model，这是一种通用的方法，它接收类似LLMs提供的任务自然语言描述的输入，并使用它来训练一个适合部署的专用模型。这通过多步骤的现有数据集和预训练模型的检索、使用LLMs生成数据集以及这些检索和生成的数据集上的监督微调来完成。在三个任务上，我们证明了在给定相同的少样本提示作为输入情况下，Prompt2Model训练出的模型相比强大的LLM - gpt-3.5-turbo平均性能提升20%。

    Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% whi
    
[^4]: 如何在优化大型语言模型时保护版权数据？

    How to Protect Copyright Data in Optimization of Large Language Models?. (arXiv:2308.12247v1 [cs.LG])

    [http://arxiv.org/abs/2308.12247](http://arxiv.org/abs/2308.12247)

    本文提出了一种方法来在优化大型语言模型时避免生成版权数据，通过将大型语言模型训练和优化视为softmax回归问题，并建立一种高效进行softmax回归的方法。

    

    大型语言模型（LLM）和生成式人工智能在计算机研究和应用中发挥了变革性的作用。关于这些模型是否输出受版权保护的数据引发了争议，这可能发生在模型训练的数据本身受版权保护的情况下。LLM是建立在Transformer神经网络架构上的，而Transformer依赖一种称为Attention的数学计算，其中使用了softmax函数。在本文中，我们展示了大型语言模型的训练和优化可以被看作是一个softmax回归问题。然后，我们建立了一种高效进行softmax回归的方法，以防止回归函数生成版权数据。这为在训练大型语言模型时避免生成版权数据建立了一个理论方法。

    Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.  In this paper, we show that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data.
    
[^5]: 扩展性和指导调优的扩散语言模型能够完成多种任务

    Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v1 [cs.CL])

    [http://arxiv.org/abs/2308.12219](http://arxiv.org/abs/2308.12219)

    本文研究表明，通过扩展扩散语言模型的数据、规模和任务，可以有效使其成为强大的语言学习者。实验证明，扩展扩散语言模型在解决通用语言任务方面能够持续提高性能。

    

    最近生成式人工智能的兴起得益于扩散概率模型的生成能力和大规模语言模型的可扩展性。尽管具有潜力，但扩散语言模型是否能够解决与自回归模型相媲美的通用语言任务仍然不明确。本文证明了在数据、规模和任务方面扩展扩散模型能够有效使其成为强大的语言学习者。我们通过先通过掩码语言建模预训练从大规模数据中获取知识，再通过扩散适应将预训练的掩码语言模型改进为扩散语言模型，通过任务特定的微调和指导调优来发掘其在解决通用语言任务方面的多样性。实验证明，扩展扩散语言模型能够在下游语言任务中持续提高性能。

    The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream langua
    
[^6]: 机器学习在信任与安全方面的挑战：一个针对虚假信息检测的案例研究

    The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection. (arXiv:2308.12215v1 [cs.LG])

    [http://arxiv.org/abs/2308.12215](http://arxiv.org/abs/2308.12215)

    本研究通过虚假信息检测为例，检查了机器学习在信任与安全问题中学术与实践之间的脱节，并发现了文献中存在的严重不足之处，包括任务不符合在线服务面临的挑战、数据集和模型评估不真实、评估不独立于模型训练等。在此基础上，提出了评估机器学习应用于信任与安全问题的建议。

    

    我们使用虚假信息检测作为案例研究，检查了在将机器学习应用于信任与安全问题上学术和实践之间的脱节。我们对该领域中270篇广受引用的论文进行了自动检测虚假信息的文献系统化，并对子集中的论文进行了数据和代码的可用性、设计失误、可复现性和泛化性等方面的研究。我们发现文献中存在严重的不足之处，这对所声称的性能和实用性提出了质疑。检测任务通常与在线服务真正面临的挑战有本质上的区别。数据集和模型评估通常不代表现实世界的情景，而且评估往往不独立于模型训练。数据和代码的可用性很差。模型在领域外的数据上泛化能力不强。基于这些结果，我们提出了评估机器学习应用于信任与安全问题的建议。

    We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is fo
    
[^7]: 使用Adam进行课程学习：魔鬼在于错误的细节

    Curriculum Learning with Adam: The Devil Is in the Wrong Details. (arXiv:2308.12202v1 [cs.LG])

    [http://arxiv.org/abs/2308.12202](http://arxiv.org/abs/2308.12202)

    本文研究了为何课程学习在自然语言处理领域取得有限的成功。研究发现，当将课程与Adam优化算法结合使用时，它们学习适应了次优化参数，导致其脆弱性增加

    

    课程学习（CL）认为，与人类类似，机器学习模型可能更有效地从与其当前学习进展相匹配的数据中学习。然而，CL方法仍然被很少了解，在自然语言处理（NLP）领域尤其如此，其取得的成果也有限。本文探讨了其中的原因。我们尝试复现和扩展一些最近的课程方法，但发现当应用于NLP时，这些方法的结果出奇地脆弱。对某些情况下课程效果的深入研究向我们展示了原因：当将课程与广受欢迎的Adam优化算法结合使用时，它们往往会学习适应此算法的次优化参数。我们提供了几个不同的案例研究，涉及常见的手工制作和自动化CL方法，以说明这种现象，并发现其中没有一个能够胜过仅使用精心选择的Adam进行优化

    Curriculum learning (CL) posits that machine learning models -- similar to humans -- may learn more efficiently from data that match their current learning progress. However, CL methods are still poorly understood and, in particular for natural language processing (NLP), have achieved only limited success. In this paper, we explore why. Starting from an attempt to replicate and extend a number of recent curriculum methods, we find that their results are surprisingly brittle when applied to NLP. A deep dive into the (in)effectiveness of the curricula in some scenarios shows us why: when curricula are employed in combination with the popular Adam optimisation algorithm, they oftentimes learn to adapt to suboptimally chosen optimisation parameters for this algorithm. We present a number of different case studies with different common hand-crafted and automated CL approaches to illustrate this phenomenon, and we find that none of them outperforms optimisation with only Adam with well-chose
    
[^8]: 用最长支持子序列评估忠实性

    Evaluation of Faithfulness Using the Longest Supported Subsequence. (arXiv:2308.12157v1 [cs.CL])

    [http://arxiv.org/abs/2308.12157](http://arxiv.org/abs/2308.12157)

    本文引入了一种新方法来评估机器生成文本的忠实性，通过计算由语境支持的主张中最长的非连续子字符串，称之为最长支持子序列（LSS）。证明了当使用LSS时，这种度量与人类评分更相关，并且在忠实性评估上相较于先前最先进的度量有18%的改进。

    

    随着日益复杂的语言模型的出现，它们的可信度成为一个关键问题，特别是在总结和问答等任务中。由于语言多样性和可能的答案种类繁多，确保它们的响应在语境中有根据、忠实可信是具有挑战性的。在本文中，我们引入了一种新方法来评估机器生成文本的忠实性，通过计算由语境支持的主张中最长的非连续子字符串，我们称之为最长支持子序列（LSS）。使用一个新的人工注释数据集，我们微调了一个模型以生成LSS。我们提出了一种新的评估方法，并证明当使用LSS时，这些度量与人类评分更相关，相对于不使用LSS时。我们的提出的度量在我们的数据集上对忠实性的最先进度量的改进达到了18%。我们的度量在总结任务上一直优于其他度量。

    As increasingly sophisticated language models emerge, their trustworthiness becomes a pivotal issue, especially in tasks such as summarization and question-answering. Ensuring their responses are contextually grounded and faithful is challenging due to the linguistic diversity and the myriad of possible answers. In this paper, we introduce a novel approach to evaluate faithfulness of machine-generated text by computing the longest noncontinuous substring of the claim that is supported by the context, which we refer to as the Longest Supported Subsequence (LSS). Using a new human-annotated dataset, we finetune a model to generate LSS. We introduce a new method of evaluation and demonstrate that these metrics correlate better with human ratings when LSS is employed, as opposed to when it is not. Our proposed metric demonstrates an 18% enhancement over the prevailing state-of-the-art metric for faithfulness on our dataset. Our metric consistently outperforms other metrics on a summarizati
    
[^9]: 面向罗马尼亚语的语义变化检测

    Semantic Change Detection for the Romanian Language. (arXiv:2308.12131v1 [cs.CL])

    [http://arxiv.org/abs/2308.12131](http://arxiv.org/abs/2308.12131)

    本文研究了面向罗马尼亚语的语义变化检测方法，并通过分析实际数据集找出了影响语义变化检测的最重要因素。

    

    自动语义变化方法通过分析研究时期语料库中单词的用法来识别单词意义随时间变化的情况。本文在真实的英语和罗马尼亚语数据集上分析了创建静态和上下文单词嵌入模型的不同策略，即Word2Vec和ELMo。为了测试我们的流程并确定我们模型的性能，我们首先在一个英语数据集(SEMEVAL-CCOHA)上评估了这两种单词嵌入模型。此后，我们将实验重点放在罗马尼亚语数据集上，并强调了这种资源有限语言中语义变化的不同方面，如意义的获取和丢失。实验结果表明，根据语料库的不同，选择模型和计算语义变化检测分数的距离是考虑的最重要因素。

    Automatic semantic change methods try to identify the changes that appear over time in the meaning of words by analyzing their usage in diachronic corpora. In this paper, we analyze different strategies to create static and contextual word embedding models, i.e., Word2Vec and ELMo, on real-world English and Romanian datasets. To test our pipeline and determine the performance of our models, we first evaluate both word embedding models on an English dataset (SEMEVAL-CCOHA). Afterward, we focus our experiments on a Romanian dataset, and we underline different aspects of semantic changes in this low-resource language, such as meaning acquisition and loss. The experimental results show that, depending on the corpus, the most important factors to consider are the choice of model and the distance to calculate a score for detecting semantic change.
    
[^10]: 使用大型语言模型进行序列生成中，指令位置的重要性

    Instruction Position Matters in Sequence Generation with Large Language Models. (arXiv:2308.12097v1 [cs.CL])

    [http://arxiv.org/abs/2308.12097](http://arxiv.org/abs/2308.12097)

    本研究提出了一种增强大型语言模型在序列生成任务中遵循指令能力的方法，通过调整任务指令的位置，从而弥补指令遗忘的问题。实验结果证明了该方法在不同规模和长度的序列上均优于传统设置。

    

    大型语言模型（LLMs）通过指令微调可以执行条件序列生成任务，如翻译或摘要。微调数据通常是从特定任务指令、输入句子和对应回应串联而成。考虑到LLMs的自注意机制建模的局部性，这些模型在为长输入句子生成回应时面临指令遗忘的风险。为了缓解这个问题，我们提出通过将任务指令的位置移动到输入句子之后，增强LLMs的遵循指令能力。理论分析表明，我们的简单方法可以改变模型的学习重点，从而强调遵循指令能力的训练。同时，实验结果表明我们的方法在不同的模型规模（1B / 7B / 13B）和不同序列长度下始终优于传统设置。

    Large language models (LLMs) are capable of performing conditional sequence generation tasks, such as translation or summarization, through instruction fine-tuning. The fine-tuning data is generally sequentially concatenated from a specific task instruction, an input sentence, and the corresponding response. Considering the locality modeled by the self-attention mechanism of LLMs, these models face the risk of instruction forgetting when generating responses for long input sentences. To mitigate this issue, we propose enhancing the instruction-following capability of LLMs by shifting the position of task instructions after the input sentences. Theoretical analysis suggests that our straightforward method can alter the model's learning focus, thereby emphasizing the training of instruction-following capabilities. Concurrently, experimental results demonstrate that our approach consistently outperforms traditional settings across various model scales (1B / 7B / 13B) and different sequenc
    
[^11]: 走出笼子：随机鹦鹉在网络安全环境中的胜利

    Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments. (arXiv:2308.12086v1 [cs.CR])

    [http://arxiv.org/abs/2308.12086](http://arxiv.org/abs/2308.12086)

    本文将预训练的大型语言模型（LLMs）应用于网络安全环境，作为攻击代理人进行顺序决策。该设计表明LLMs在高效应对复杂决策方面具有潜力，并且在大多数场景中表现出与经过训练的最先进代理相似或更好的性能。

    

    大型语言模型（LLMs）在涉及文本生成、摘要和各种自然语言处理任务的不同领域中广受欢迎。尽管存在固有的局限性，基于LLM的设计在规划和导航开放世界场景方面显示出有希望的能力。本文将预训练的LLMs用作网络安全环境中的代理人的新应用，重点关注它们在顺序决策过程中的效用。我们提出了一种方法，利用预训练的LLMs作为两个强化学习环境中的攻击代理。在大多数场景和配置中，我们提出的代理在表现上与经过数千次训练的最先进代理相似或更好。此外，最佳LLM代理在没有任何额外训练过程的情况下表现与环境的人类测试者类似。这种设计突显了LLMs在高效应对复杂决策方面的潜力。

    Large Language Models (LLMs) have gained widespread popularity across diverse domains involving text generation, summarization, and various natural language processing tasks. Despite their inherent limitations, LLM-based designs have shown promising capabilities in planning and navigating open-world scenarios. This paper introduces a novel application of pre-trained LLMs as agents within cybersecurity network environments, focusing on their utility for sequential decision-making processes.  We present an approach wherein pre-trained LLMs are leveraged as attacking agents in two reinforcement learning environments. Our proposed agents demonstrate similar or better performance against state-of-the-art agents trained for thousands of episodes in most scenarios and configurations. In addition, the best LLM agents perform similarly to human testers of the environment without any additional training process. This design highlights the potential of LLMs to efficiently address complex decision
    
[^12]: InstructionGPT-4: 一个200指令范式用于微调MiniGPT-4

    InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4. (arXiv:2308.12067v1 [cs.LG])

    [http://arxiv.org/abs/2308.12067](http://arxiv.org/abs/2308.12067)

    InstructionGPT-4通过仅使用200个例子进行微调，在多模式指令数据质量度量和选择器的帮助下，在各种评估任务中优于原始的MiniGPT-4。

    

    多模式大型语言模型通过两阶段的训练过程获取其遵循指令的能力：在图像-文本对上进行预训练，然后在监督式视觉-语言指令数据上进行微调。最近的研究表明，即使只有有限量的高质量遵循指令数据，大型语言模型也能取得令人满意的结果。在本文中，我们介绍了InstructionGPT-4，它经过微调的数据集只包含200个例子，约占MiniGPT-4对齐数据集中使用的遵循指令数据的6%。我们首先提出了几个用于评估多模式指令数据质量的度量指标。基于这些度量指标，我们提出了一个简单而有效的数据选择器，自动识别和过滤低质量的视觉-语言数据。通过采用这种方法，InstructionGPT-4在各种评估（如视觉问答、GPT-4偏好）上优于原始的MiniGPT-4。总体而言，我们的研究发现...

    Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findi
    
[^13]: FlexKBQA：一种用于少样本知识库问答的灵活LLM驱动框架

    FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering. (arXiv:2308.12060v1 [cs.CL])

    [http://arxiv.org/abs/2308.12060](http://arxiv.org/abs/2308.12060)

    FlexKBQA使用大型语言模型(LLMs)作为程序翻译器，通过自动化算法从知识库中抽样多样的程序，将其转换为自然语言问题，从而解决少样本知识库问答任务的挑战。

    

    知识库问答（KBQA）是一个关键且具有挑战性的任务，因为知识库中的实体数量庞大，并且用户提出的自然语言问题多样化。不幸的是，大多数KBQA模型在现实场景中性能显著下降，因为高质量的注释数据不足。为了减轻手动注释的负担，我们利用大型语言模型（LLMs）作为程序翻译器，介绍了FlexKBQA来解决少样本KBQA任务中固有的挑战。具体而言，FlexKBQA利用自动化算法从知识库中抽样多样的程序（如SPARQL查询），然后通过LLMs将其转换为自然语言问题。这个合成的数据集有助于训练一个专门的轻量级模型来处理知识库。此外，为了减少合成数据与真实用户问题之间的分布偏移的障碍，FlexKBQA引入了一个执行机制。

    Knowledge base question answering (KBQA) is a critical yet challenging task due to the vast number of entities within knowledge bases and the diversity of natural language questions posed by users. Unfortunately, the performance of most KBQA models tends to decline significantly in real-world scenarios where high-quality annotated data is insufficient. To mitigate the burden associated with manual annotation, we introduce FlexKBQA by utilizing Large Language Models (LLMs) as program translators for addressing the challenges inherent in the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms to sample diverse programs, such as SPARQL queries, from the knowledge base, which are subsequently converted into natural language questions via LLMs. This synthetic dataset facilitates training a specialized lightweight model for the KB. Additionally, to reduce the barriers of distribution shift between synthetic data and real user questions, FlexKBQA introduces an executiong
    
[^14]: 通过离线强化学习与人类反馈来对齐语言模型

    Aligning Language Models with Offline Reinforcement Learning from Human Feedback. (arXiv:2308.12050v1 [cs.CL])

    [http://arxiv.org/abs/2308.12050](http://arxiv.org/abs/2308.12050)

    本研究提出了一种离线强化学习与人类反馈（RLHF）框架，通过使用预生成的样本来对齐语言模型与人类偏好。我们采用了最大似然估计、奖励加权回归和决策Transformer等方法，并通过类似于监督微调的损失函数来实现对齐。

    

    从人类偏好中学习对于语言模型来说是至关重要的，以有效地满足人类需求和社会价值观。先前的研究通过利用人类反馈来遵循指令取得了显著进展。然而，这些方法主要依赖于在线强化学习（RL）技术，如Proximal Policy Optimization（PPO），这些技术被证明对于语言模型来说不稳定且难以调整。此外，PPO需要复杂的分布式系统实现，影响了大规模分布式训练的效率。本研究提出了一种离线强化学习与人类反馈（RLHF）框架，通过使用预生成的样本而不是与RL环境交互来对齐语言模型。具体而言，我们探讨了最大似然估计（MLE）与过滤、奖励加权回归（RWR）以及决策Transformer（DT）来对齐语言模型与人类偏好。通过采用类似于监督微调的损失函数，我们的方法可以实现对齐语言模型和人类偏好。

    Learning from human preferences is crucial for language models (LMs) to effectively cater to human needs and societal values. Previous research has made notable progress by leveraging human feedback to follow instructions. However, these approaches rely primarily on online reinforcement learning (RL) techniques like Proximal Policy Optimization (PPO), which have been proven unstable and challenging to tune for language models. Moreover, PPO requires complex distributed system implementation, hindering the efficiency of large-scale distributed training. In this study, we propose an offline reinforcement learning from human feedback (RLHF) framework to align LMs using pre-generated samples without interacting with RL environments. Specifically, we explore maximum likelihood estimation (MLE) with filtering, reward-weighted regression (RWR), and Decision Transformer (DT) to align language models to human preferences. By employing a loss function similar to supervised fine-tuning, our metho
    
[^15]: CgT-GAN: CLIP引导的文本生成对抗网络用于图像描述

    CgT-GAN: CLIP-guided Text GAN for Image Captioning. (arXiv:2308.12045v1 [cs.CV])

    [http://arxiv.org/abs/2308.12045](http://arxiv.org/abs/2308.12045)

    本文提出了CgT-GAN，通过引入图像为模型提供视觉信息，结合对抗训练和基于CLIP的奖励，实现了在没有人工注释的情况下进行图像描述。

    

    大规模的视觉语言预训练模型CLIP在没有人工注释的图像描述场景中显著提升了图像描述的效果。最近的CLIP-based图像描述方法采用了纯文本训练范式，即从共享的嵌入空间中重建文本。然而，这些方法在训练和推理差距或者文本嵌入的存储需求上存在一定限制。考虑到在现实世界中获取图像是微不足道的，我们提出了CLIP引导的文本生成对抗网络（CgT-GAN），将图像加入到训练过程中使得模型能够“看到”真实的视觉模态。特别地，我们采用对抗训练使得CgT-GAN能够模仿外部文本语料库的短语，并使用基于CLIP的奖励提供语义引导。生成的图像描述器通过GAN的鉴别器计算的自然度和基于人类语言的语义从而获得联合奖励。

    The large-scale visual-language pre-trained model, Contrastive Language-Image Pre-training (CLIP), has significantly improved image captioning for scenarios without human-annotated image-caption pairs. Recent advanced CLIP-based image captioning without human annotations follows a text-only training paradigm, i.e., reconstructing text from shared embedding space. Nevertheless, these approaches are limited by the training/inference gap or huge storage requirements for text embeddings. Given that it is trivial to obtain images in the real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates images into the training process to enable the model to "see" real visual modality. Particularly, we use adversarial training to teach CgT-GAN to mimic the phrases of an external text corpus and CLIP-based reward to provide semantic guidance. The caption generator is jointly rewarded based on the caption naturalness to human language calculated from the GAN's discriminator and the sema
    
[^16]: IncreLoRA: 增量参数分配方法用于参数高效调优

    IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning. (arXiv:2308.12043v1 [cs.CL])

    [http://arxiv.org/abs/2308.12043](http://arxiv.org/abs/2308.12043)

    IncreLoRA是一种增量参数分配方法，根据模块的重要性分数在训练过程中自适应地添加可训练参数，以实现参数高效调优。

    

    随着预训练语言模型（PLM）的大小不断增加，对模型中的所有参数进行微调并不高效，特别是当存在大量的下游任务时，这会导致显著的训练和存储成本。已有许多参数高效调优（PEFT）方法被提出，其中，低秩自适应（LoRA）是一种将可训练的秩分解矩阵注入每个目标模块的典型方法。然而，LoRA忽视了不同模块中参数的重要性。为了解决这个问题，已经提出了许多方法来剪枝LoRA的参数。然而，在有限的训练条件下，修剪参数矩阵的秩的上界仍然受到预设值的影响。因此，我们提出了IncreLoRA，一种增量参数分配方法，根据每个模块的重要性分数在训练过程中自适应地添加可训练参数。这种方法与修剪方法不同，因为它从增加参数的角度来优化调优过程。

    With the increasing size of pre-trained language models (PLMs), fine-tuning all the parameters in the model is not efficient, especially when there are a large number of downstream tasks, which incur significant training and storage costs. Many parameter-efficient fine-tuning (PEFT) approaches have been proposed, among which, Low-Rank Adaptation (LoRA) is a representative approach that injects trainable rank decomposition matrices into every target module. Yet LoRA ignores the importance of parameters in different modules. To address this problem, many works have been proposed to prune the parameters of LoRA. However, under limited training conditions, the upper bound of the rank of the pruned parameter matrix is still affected by the preset values. We, therefore, propose IncreLoRA, an incremental parameter allocation method that adaptively adds trainable parameters during training based on the importance scores of each module. This approach is different from the pruning method as it i
    
[^17]: TREC 2022深度学习赛道的混合检索和多阶段文本排名解决方案

    Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep Learning Track. (arXiv:2308.12039v1 [cs.IR])

    [http://arxiv.org/abs/2308.12039](http://arxiv.org/abs/2308.12039)

    本论文介绍了在 TREC 2022 深度学习赛道中，我们采用的混合文本检索和多阶段文本排名方法，在检索阶段结合了传统稀疏检索和神经稠密检索的两种结构，在排名阶段构建了全交互式排名模型和轻量级子排名模块，评估结果证明了方法的有效性。

    

    大规模文本检索技术在各种实际业务场景中被广泛使用。本文介绍了我们在TREC 2022深度学习赛道中的系统。我们解释了我们解决方案中采用的混合文本检索和多阶段文本排名方法。检索阶段结合了传统稀疏检索和神经稠密检索的两种结构。在排名阶段，除了基于大型预训练语言模型构建的全交互式排名模型外，我们还提出了一个轻量级的子排名模块，进一步提升了最终的文本排名性能。评估结果证明了我们提出的方法的有效性。我们的模型在段落排名和文档排名的测试集上分别达到了第1名和第4名。

    Large-scale text retrieval technology has been widely used in various practical business scenarios. This paper presents our systems for the TREC 2022 Deep Learning Track. We explain the hybrid text retrieval and multi-stage text ranking method adopted in our solution. The retrieval stage combined the two structures of traditional sparse retrieval and neural dense retrieval. In the ranking stage, in addition to the full interaction-based ranking model built on large pre-trained language model, we also proposes a lightweight sub-ranking module to further enhance the final text ranking performance. Evaluation results demonstrate the effectiveness of our proposed approach. Our models achieve the 1st and 4th rank on the test set of passage ranking and document ranking respectively.
    
[^18]: 大型多语言模型在跨语种零样本多模式学习中的作用。

    Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages. (arXiv:2308.12038v1 [cs.CL])

    [http://arxiv.org/abs/2308.12038](http://arxiv.org/abs/2308.12038)

    本论文提出了一种在低资源语言中训练大型多模式模型的有效方法，通过利用多语言模型实现了跨语种零样本多模式学习，在图像到文本和文本到图像的生成任务上具有竞争力。

    

    最近，在图像到文本和文本到图像的生成方面，多模式学习出现了显著增长。然而，成功通常仅限于英语，其他语言则相对落后。在其他语言中构建具有竞争力的对应物是非常具有挑战性的，因为非英语多模式数据具有低资源特性（即缺乏大规模、高质量的图像-文本数据）。在这项工作中，我们提出了MPM，一种在低资源语言中训练大型多模式模型的有效训练范例。MPM表明，多语言模型可以在跨语种零样本多模式学习中起到关键作用。具体而言，基于强大的多语言大语言模型，仅在英语图像-文本数据上预训练的多模式模型可以以零样本的方式很好地泛化到其他语言，用于图像到文本和文本到图像的生成，甚至超过在本地语言的图像-文本数据上训练的模型。以中文作为MPM实践的一个练习。

    Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in low-resource languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a zero-shot manner for both image-to-text and text-to-image generation, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MP
    
[^19]: PREFER: 通过反馈-反思-优化的提示集成学习

    PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine. (arXiv:2308.12033v1 [cs.CL])

    [http://arxiv.org/abs/2308.12033](http://arxiv.org/abs/2308.12033)

    本文提出了一种名为PREFER的方法，通过反馈机制和自动合成新的提示来解决大型语言模型中的幻觉和不稳定性问题，从而提高性能。

    

    作为发挥大型语言模型 (LLMs) 强大能力的有效工具，提示最近在各种复杂任务中展示出了前所未有的能力。为了进一步提高性能，提示集成引起了人们的广泛兴趣，以解决 LLMs 的幻觉和不稳定性问题。然而，现有方法通常采用两阶段的范式，需要大量手动准备的提示集合，并且无法针对不同的弱学习器进行有针对性的优化。在本文中，我们提出了一种简单、通用、自动化的方法，命名为 PREFER (通过反馈-反思-优化的提示集成学习)，来解决上述限制。具体来说，考虑到弱学习器应该关注提升过程中的困难样本，PREFER 构建了一个反馈机制，用于反思现有弱学习器的不足之处。基于此，LLM 需要自动合成新的提示来进行迭代优化。

    As an effective tool for eliciting the power of Large Language Models (LLMs), prompting has recently demonstrated unprecedented abilities across a variety of complex tasks. To further improve the performance, prompt ensemble has attracted substantial interest for tackling the hallucination and instability of LLMs. However, existing methods usually adopt a two-stage paradigm, which requires a pre-prepared set of prompts with substantial manual effort, and is unable to perform directed optimization for different weak learners. In this paper, we propose a simple, universal, and automatic method named PREFER (Pompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations. Specifically, given the fact that weak learners are supposed to focus on hard examples during boosting, PREFER builds a feedback mechanism for reflecting on the inadequacies of existing weak learners. Based on this, the LLM is required to automatically synthesize new prompts for iterative refinemen
    
[^20]: 从数量到质量：利用自我引导数据选择方法提升LLM性能以进行指令调优

    From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning. (arXiv:2308.12032v1 [cs.CL])

    [http://arxiv.org/abs/2308.12032](http://arxiv.org/abs/2308.12032)

    该论文引入了一种自我引导的方法，让LLM能够自主地选择高质量的指令数据，通过引入指令遵循难度指标（IFD），大幅提高了模型训练效率，并在知名数据集上进行了验证，展示了优于传统数据输入的结果。

    

    在大型语言模型领域，指令数据的质量和数量之间的平衡已成为一个焦点。鉴于此，我们引入了一种自我引导的方法，让LLM能够自主地识别和选择大规模开源数据集中的精选样本，有效减少了指令调优的手动筛选和潜在成本。我们的关键创新是指令遵循难度（IFD）指标，它成为了一个决定性工具，用于识别模型期望响应和自主生成能力之间的差异。通过灵活应用IFD，我们能够找到精选样本，从而大幅提升模型训练效率。在Alpaca和WizardLM等知名数据集上的实证验证支持我们的发现；仅使用传统数据输入的10%，我们的策略展示了改进的结果。这种自我引导挑选和IFD指标的综合意味着LLM优化的一个变革性飞跃，有望同时提高模型性能和降低成本。

    In the realm of Large Language Models, the balance between instruction data quality and quantity has become a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from vast open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to identify discrepancies between a model's expected responses and its autonomous generation prowess. Through the adept application of IFD, cherry samples are pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on renowned datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of conventional data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the optimization of LLMs, promising both
    
[^21]: 基于提示的长度受控生成与强化学习

    Prompt-Based Length Controlled Generation with Reinforcement Learning. (arXiv:2308.12030v1 [cs.CL])

    [http://arxiv.org/abs/2308.12030](http://arxiv.org/abs/2308.12030)

    提出了一种基于提示的长度控制方法，利用强化学习和奖励模型来实现大型语言模型（LLM）的长度受控生成。该方法可以有效减少推理成本并满足不同需求。

    

    最近，大型语言模型（LLM）如ChatGPT和GPT-4因其惊人的改进和性能而受到广泛关注。长度受控生成成为LLM中的一个重要话题，它还使用户能够充分利用LLM的能力在更多实际场景中生成所需长度的合适答案或文章。此外，LLM中的自回归生成非常耗时，而控制生成长度的能力可以通过限制长度任意降低推理成本，从而满足不同需求。因此，我们旨在提出一种基于提示的长度控制方法来实现长度受控生成，这种方法也可以广泛应用于类似GPT的LLM中。具体而言，我们采用强化学习，使用可训练或基于规则的奖励模型提供奖励信号，进一步通过对预定义目标长度进行奖励来影响LLM的生成。实验证明...

    Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising improvement and performance. Length controlled generation of LLMs emerges as an important topic, which also enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can arbitrarily reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show th
    
[^22]: 中文生物医学实体规范化的知识注入提示学习

    Knowledge-injected Prompt Learning for Chinese Biomedical Entity Normalization. (arXiv:2308.12025v1 [cs.CL])

    [http://arxiv.org/abs/2308.12025](http://arxiv.org/abs/2308.12025)

    我们提出了一种知识注入提示学习方法用于处理中文生物医学实体规范化任务，该方法通过有效编码医学实体中的知识项，并将它们注入到模型中，以增强模型的能力。

    

    生物医学实体规范化（BEN）任务旨在将原始的非结构化医学实体对齐到标准实体，从而促进数据的一致性并便于更好的下游医学应用。最近，提示学习方法在这个任务中取得了有希望的结果。然而，现有的研究在处理更复杂的中文BEN任务上存在不足，特别是在有限的医学数据下的少样本情况，外部医学知识库的巨大潜力还没有得到充分利用。为了应对这些挑战，我们提出了一种新颖的知识注入提示学习（PL-Knowledge）方法。具体来说，我们的方法包括五个阶段：候选实体匹配、知识提取、知识编码、知识注入和预测输出。通过有效地编码医学实体中包含的知识项，并将它们整合到我们的定制知识注入模板中，额外的知识增强了模型的能力。

    The Biomedical Entity Normalization (BEN) task aims to align raw, unstructured medical entities to standard entities, thus promoting data coherence and facilitating better downstream medical applications. Recently, prompt learning methods have shown promising results in this task. However, existing research falls short in tackling the more complex Chinese BEN task, especially in the few-shot scenario with limited medical data, and the vast potential of the external medical knowledge base has yet to be fully harnessed. To address these challenges, we propose a novel Knowledge-injected Prompt Learning (PL-Knowledge) method. Specifically, our approach consists of five stages: candidate entity matching, knowledge extraction, knowledge encoding, knowledge injection, and prediction output. By effectively encoding the knowledge items contained in medical entities and incorporating them into our tailor-made knowledge-injected templates, the additional knowledge enhances the model's ability to 
    
[^23]: 利用列表上下文信息的粗到细神经检索器对段落进行重新排序

    Reranking Passages with Coarse-to-Fine Neural Retriever using List-Context Information. (arXiv:2308.12022v1 [cs.CL])

    [http://arxiv.org/abs/2308.12022](http://arxiv.org/abs/2308.12022)

    本文提出了一种利用列表上下文信息的粗到细神经检索器来重新排序段落。该方法通过将其他候选句子的列表上下文信息纳入段落表示中，增强了段落表示。而且，该方法将列表上下文建模过程分为两个子过程，从而解决了段落注意机制的内存限制问题，允许高效编码大量候选答案的上下文信息。

    

    段落重新排序是许多应用程序中的关键任务，特别是在处理大规模文档时。传统的神经架构在为问题检索最佳段落方面存在限制，因为它们通常将问题与每个段落分开匹配，很少考虑其他段落中的上下文信息，而这些信息可以提供比较和参考信息。本文提出了一种列表上下文注意机制，通过将来自其他候选句子的列表上下文信息纳入段落表示中来增强段落表示。所提出的粗到细（C2F）神经检索器通过将列表上下文建模过程分为两个子过程来解决段落注意机制的内存限制问题，从而允许对大量候选答案的上下文信息进行高效编码。这种方法可以广泛用于一次性编码任意数量的候选答案的上下文信息。

    Passage reranking is a crucial task in many applications, particularly when dealing with large-scale documents. Traditional neural architectures are limited in retrieving the best passage for a question because they usually match the question to each passage separately, seldom considering contextual information in other passages that can provide comparison and reference information. This paper presents a list-context attention mechanism to augment the passage representation by incorporating the list-context information from other candidates. The proposed coarse-to-fine (C2F) neural retriever addresses the out-of-memory limitation of the passage attention mechanism by dividing the list-context modeling process into two sub-processes, allowing for efficient encoding of context information from a large number of candidate answers. This method can be generally used to encode context information from any number of candidate answers in one pass. Different from most multi-stage information re
    
[^24]: 从指令到内在人类价值 - 大模型对齐目标的调查

    From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models. (arXiv:2308.12014v1 [cs.AI])

    [http://arxiv.org/abs/2308.12014](http://arxiv.org/abs/2308.12014)

    本文综合调查了大模型对齐目标的不同观点，并追踪其演化路径，旨在帮助确定最重要的目标。

    

    大模型，例如大型语言模型（LLM），通常在大规模数据上进行预训练，并由大量参数组成，不仅在各种任务中获得显著改进的性能，还呈现出较小模型所没有的新能力。然而，大模型与日常生活的日益交织可能带来潜在风险，并可能造成严重的社会危害。因此，许多努力已经进行了，以使LLM与人类对齐，以使它们更好地遵循用户的指令并满足人类的偏好。然而，“与何对齐”还没有得到充分讨论，不当的对齐目标甚至可能适得其反。在本文中，我们对现有工作中的不同对齐目标进行了综合调查，并追踪它们的演化路径，以帮助确定最基本的目标。特别是，我们从对齐目标的定义和对齐评估两个角度进行了相关工作的调查。我们的分析包括...

    Big models, exemplified by Large Language Models (LLMs), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. However, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. Therefore, many efforts have been made to align LLMs with humans to make them better follow user instructions and satisfy human preferences. Nevertheless, `what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire. In this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. Particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. Our analysis encompasses
    
[^25]: 被征服的格里西亚战胜了野蛮的胜利者。检测拉丁古希腊文学的类际指涉。

    Graecia capta ferum victorem cepit. Detecting Latin Allusions to Ancient Greek Literature. (arXiv:2308.12008v1 [cs.CL])

    [http://arxiv.org/abs/2308.12008](http://arxiv.org/abs/2308.12008)

    该论文介绍了一种用于古典文学的三语句子RoBERTa模型SPhilBERTa，能够优秀地进行跨语言语义理解和识别古希腊语、拉丁语和英语之间相同句子的能力，并通过自动翻译生成了新的训练数据。研究还展示了SPhilBERTa在自动检测类际对应方面的能力。

    

    类际指涉在古典文学中起着重要作用，拉丁作家经常引用古希腊文本。到目前为止，自动识别这些类际引用的方法局限于单语言方法，仅在拉丁文或希腊文本中寻找类似之处。在这项研究中，我们介绍了SPhilBERTa，一种专门用于古典文学的三语句子RoBERTa模型，优于跨语言语义理解和识别在古希腊语、拉丁语和英语之间相同句子的能力。我们通过自动将英语文本翻译成古希腊语来生成新的训练数据。此外，我们还展示了一个案例研究，证明了SPhilBERTa在自动检测类际对应方面的能力。我们的模型和资源可在https://github.com/Heidelberg-NLP/ancient-language-models上获取。

    Intertextual allusions hold a pivotal role in Classical Philology, with Latin authors frequently referencing Ancient Greek texts. Until now, the automatic identification of these intertextual references has been constrained to monolingual approaches, seeking parallels solely within Latin or Greek texts. In this study, we introduce SPhilBERTa, a trilingual Sentence-RoBERTa model tailored for Classical Philology, which excels at cross-lingual semantic comprehension and identification of identical sentences across Ancient Greek, Latin, and English. We generate new training data by automatically translating English texts into Ancient Greek. Further, we present a case study, demonstrating SPhilBERTa's capability to facilitate automated detection of intertextual parallels. Our models and resources are available at https://github.com/Heidelberg-NLP/ancient-language-models.
    
[^26]: Topical-Chat: 进展中的基于知识的开放域对话

    Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations. (arXiv:2308.11995v1 [cs.CL])

    [http://arxiv.org/abs/2308.11995](http://arxiv.org/abs/2308.11995)

    该论文引入了Topical-Chat数据集，该数据集是一个基于知识的人-人对话数据集，用于推动开放域对话人工智能的研究。研究者训练了几个最先进的对话模型，并进行了自动化和人工评估。

    

    构建社交机器人能够与人类进行深入、有趣的开放域对话是人工智能的一项重要挑战。为了实现这一目标，机器人在与拥有自己世界知识的人类进行对话时，需要有效地利用涵盖多个领域的世界知识。现有的基于知识的对话数据集主要是具有明确角色的样式化数据集。这些数据集还没有探索对话中的主题涵盖的深度和广度。我们引入了Topical-Chat，这是一个基于知识的人-人对话数据集，其中的知识涵盖了8个广泛的主题，并且对话参与者没有明确定义的角色，以进一步促进开放域对话人工智能的研究。我们还在Topical-Chat上训练了几个最先进的编码器-解码器对话模型，并进行了自动化和人工评估以进行基准测试。

    Building socialbots that can have deep, engaging open-domain conversations with humans is one of the grand challenges of artificial intelligence (AI). To this end, bots need to be able to leverage world knowledge spanning several domains effectively when conversing with humans who have their own world knowledge. Existing knowledge-grounded conversation datasets are primarily stylized with explicit roles for conversation partners. These datasets also do not explore depth or breadth of topical coverage with transitions in conversations. We introduce Topical-Chat, a knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don't have explicitly defined roles, to help further research in open-domain conversational AI. We also train several state-of-the-art encoder-decoder conversational models on Topical-Chat and perform automated and human evaluation for benchmarking.
    
[^27]: EVE: 使用遮蔽预测和模态感知的高效视觉-语言预训练

    EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE. (arXiv:2308.11971v1 [cs.CV])

    [http://arxiv.org/abs/2308.11971](http://arxiv.org/abs/2308.11971)

    本文引入了一种名为EVE的高效视觉-语言预训练模型，通过遮蔽信号建模和模态感知的方式，实现了统一的多模态Transformer网络，加速了训练进程，并取得了良好的效果。

    

    在本文中，我们介绍了一种名为EVE的高效视觉-语言基础模型，它是由一种统一的Transformer进行预训练的统一多模态模型。EVE通过在图像-文本对上进行遮蔽信号建模来统一视觉和语言的预训练任务，以重建可见信号，即图像像素和文本标记。通过集成模态感知的稀疏专家混合模块，EVE在一个共享的Transformer网络中编码了视觉和语言，并通过选择性地切换到不同的专家来捕捉模态特定信息。

    Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 3.5x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy t
    
[^28]: 使用多条件扩散模型进行音频生成

    Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v1 [cs.SD])

    [http://arxiv.org/abs/2308.11940](http://arxiv.org/abs/2308.11940)

    本论文提出了一种使用多条件扩散模型进行音频生成的方法。通过引入内容和风格等额外条件，增强了现有模型的可控性。这种方法可以精确控制生成音频的时间顺序、音高和能量。由于缺乏合适的数据集和评估指标，作者整合了现有数据集并进行了实验验证。

    

    基于文本的音频生成模型有其局限性，因为它们无法包含音频中的所有信息，仅依靠文本会导致受控性受限。为了解决这个问题，我们提出了一种新颖的模型，通过引入额外的条件（包括内容（时间戳）和风格（音高曲线和能量曲线））作为文本的补充，增强了现有预训练文本到音频模型的可控性。这种方法实现了对生成音频的时间顺序、音高和能量的精细控制。为了保持生成的多样性，我们使用一个可训练的控制条件编码器，该编码器由一个大型语言模型增强，并使用一个可训练的融合网络来编码和融合额外的条件，同时保持预训练文本到音频模型的权重不变。由于缺乏合适的数据集和评估指标，我们将现有数据集整合为一个新的数据集，包括音频和相应的条件，并使用一个可训练的控制条件编码器，该编码器由一个大型语言模型增强，并使用一个可训练的融合网络来编码和融合额外的条件，同时保持预训练文本到音频模型的权重不变。

    Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a 
    
[^29]: 利用相似性-差异解缠来进行音频差异字幕生成

    Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement. (arXiv:2308.11923v1 [eess.AS])

    [http://arxiv.org/abs/2308.11923](http://arxiv.org/abs/2308.11923)

    该论文提出了音频差异字幕生成（ADC）作为一种新的音频字幕生成扩展任务，用于描述类似但略有差异的音频片段之间的语义差异。通过引入交叉注意力集中的Transformer编码器和相似性-差异解缠，该方法有效解决了传统音频字幕生成中的差异描述问题，并利用可视化来改善注意力权重以提取差异。

    

    我们提出了音频差异字幕生成（ADC）作为音频字幕生成的新扩展任务，用于描述类似但略有差异的音频片段之间的语义差异。ADC解决了传统音频字幕生成中，对于相似音频片段生成类似字幕的问题，无法描述内容差异的情况。我们还提出了一种交叉注意力集中的Transformer编码器，通过比较一对音频片段和一种相似性-差异解缠来强调潜在空间中的差异。为了评估所提出的方法，我们构建了一个AudioDiffCaps数据集，其中包含了类似但略有差异的音频片段对以及人工标注的它们之间差异的描述。使用该数据集进行的实验证明了所提出的方法能够有效解决ADC任务，并通过在Transformer编码器中对其进行可视化来改善注意力权重以提取差异。

    We proposed Audio Difference Captioning (ADC) as a new extension task of audio captioning for describing the semantic differences between input pairs of similar but slightly different audio clips. The ADC solves the problem that conventional audio captioning sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.
    
[^30]: 线性语言模型辅助分析表格数据的方法研究

    Bridging the Gap: Deciphering Tabular Data Using Large Language Model. (arXiv:2308.11891v1 [cs.CL])

    [http://arxiv.org/abs/2308.11891](http://arxiv.org/abs/2308.11891)

    本研究旨在提升大型语言模型在理解表格数据上的能力，通过设计一个表格序列化模块和纠正机制来实现。实验结果表明，尽管相对于最先进技术仍有差距，但该方法在处理表格数据方面取得了一定的进展。

    

    在自然语言处理领域，对表格数据的理解一直是学术研究的重点。随着诸如ChatGPT之类的庞大语言模型的出现，研究人员开始探索如何利用这些模型来处理与表格相关的问题。我们的研究旨在探索提升大型语言模型在理解表格结构和内容上的能力，以便更好地回答相关问题。为此，我们设计了一个专门用于将表格序列化的模块，并在模型中引入了一个纠正机制来修正潜在的错误。实验结果显示，尽管我们的方法相对于最先进技术仍有差距。

    In the realm of natural language processing, the understanding of tabular data has perpetually stood as a focal point of scholarly inquiry. The emergence of expansive language models, exemplified by the likes of ChatGPT, has ushered in a wave of endeavors wherein researchers aim to harness these models for tasks related to table-based question answering. Central to our investigative pursuits is the elucidation of methodologies that amplify the aptitude of such large language models in discerning both the structural intricacies and inherent content of tables, ultimately facilitating their capacity to provide informed responses to pertinent queries. To this end, we have architected a distinctive module dedicated to the serialization of tables for seamless integration with expansive language models. Additionally, we've instituted a corrective mechanism within the model to rectify potential inaccuracies. Experimental results indicate that, although our proposed method trails the SOTA by ap
    
[^31]: Cabrita: 弥合外语差距

    Cabrita: closing the gap for foreign languages. (arXiv:2308.11878v1 [cs.CL])

    [http://arxiv.org/abs/2308.11878](http://arxiv.org/abs/2308.11878)

    Cabrita是一种解决性能和高效标记化问题的方法，以可承受的成本解决了从头训练模型的限制。

    

    从头训练模型在特定语言或领域中有两个重要目的：i)增强在特定语言或领域背景下的性能，ii)确保有效的标记化。然而，这种方法的主要限制在于相关成本，这些成本可能达到六位数甚至七位数的美元金额，这取决于模型大小和涉及的参数数量。为了克服这个成本挑战，主要解决方案是依赖可用的预训练模型，尽管最近出现了像LLaMA和LLaMA-2模型这样的进展，但对于某些特定领域问题仍然表现低效，或者在涉及对话式记忆资源的场景中无效，因为表示文本所需的标记数量巨大。为了解决这个问题，我们提出了一种名为Cabrita的方法，我们的研究证明，它成功解决了性能和高效标记化的问题，而且成本可承受。

    The strategy of training the model from scratch in a specific language or domain serves two essential purposes: i) enhancing performance in the particular linguistic or domain context, and ii) ensuring effective tokenization. The main limitation inherent to this approach lies in the associated cost, which can reach six to seven-digit dollar values, depending on the model size and the number of parameters involved.  The main solution to overcome the cost challenge is to rely on available pre-trained models, which, despite recent advancements such as the LLaMA and LLaMA-2 models, still demonstrate inefficiency for certain specific domain problems or prove ineffective in scenarios involving conversational memory resources, given the large number of tokens required to represent text.  To overcome this issue, we present a methodology named Cabrita, which, as our research demonstrates, successfully addresses the performance and efficient tokenization problem, all at an affordable cost. We be
    
[^32]: 探索GPT模型在考试中的效果：驾驶执照知识测试案例研究

    Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver's License Knowledge Test. (arXiv:2308.11827v1 [cs.CL])

    [http://arxiv.org/abs/2308.11827](http://arxiv.org/abs/2308.11827)

    本研究提出了一种方法，通过使用新的信息源的上下文，让GPT模型能够回答考试题目。在使用加利福尼亚驾驶手册作为信息源的测试中，GPT-3模型取得了96%的及格分数。

    

    大型语言模型，如Open AI的生成式预训练变压器（GPT）模型，擅长回答问题，但其知识仅限于其训练数据中的信息。这种限制使得当面临有关最新发展或非公开文件的问题时，它们变得无效。我们的研究提出了一种方法，通过使用之前未包含在其训练数据中的信息源的上下文来使GPT模型能够回答问题。该方法包括上下文信息的预处理、上下文和查询的嵌入、通过整合上下文嵌入构建提示以及使用GPT模型生成答案。我们将此方法应用于一个受控测试场景，使用加利福尼亚驾驶手册作为信息源。GPT-3模型在一套50道样本驾驶知识测试题上取得了96%的及格分数。相比之下，在没有上下文的情况下，模型的及格分数下降到了...

    Large language models such as Open AI's Generative Pre-trained Transformer (GPT) models are proficient at answering questions, but their knowledge is confined to the information present in their training data. This limitation renders them ineffective when confronted with questions about recent developments or non-public documents. Our research proposes a method that enables GPT models to answer questions by employing context from an information source not previously included in their training data. The methodology includes preprocessing of contextual information, the embedding of contexts and queries, constructing prompt through the integration of context embeddings, and generating answers using GPT models. We applied this method in a controlled test scenario using the California Driver's Handbook as the information source. The GPT-3 model achieved a 96% passing score on a set of 50 sample driving knowledge test questions. In contrast, without context, the model's passing score fell to
    
[^33]: 面向设备的文本重写代理的研究

    Towards an On-device Agent for Text Rewriting. (arXiv:2308.11807v1 [cs.CL])

    [http://arxiv.org/abs/2308.11807](http://arxiv.org/abs/2308.11807)

    给出了一种面向设备的文本重写代理的构建方法，通过新的指令调整方法和启发式强化学习框架，能够生成高质量的训练数据并提高性能。能够在保持模型能力的前提下，实现移动设备上的文本重写。

    

    大型语言模型（LLMs）在文本重写方面展示了令人印象深刻的能力。然而，这些模型的体积庞大使得它们在设备上进行推理变得不实际，而后者本可能提供增强隐私和经济推理的能力。为了解决上述挑战，我们提出了一种针对构建以移动设备为中心的文本重写模型的新型指令调整方法。我们的策略能够在不需要人工标注的情况下生成高质量的训练数据。此外，我们提出了一个启发式强化学习框架，可以显著提高性能，而无需偏好数据。为了进一步弥补与更大的服务器端模型之间的性能差距，我们提出了一种有效的方法，结合了...

    Large Language Models (LLMs) have demonstrated impressive capabilities for text rewriting. Nonetheless, the large sizes of these models make them impractical for on-device inference, which would otherwise allow for enhanced privacy and economical inference. Creating a smaller yet potent language model for text rewriting presents a formidable challenge because it requires balancing the need for a small size with the need to retain the emergent capabilities of the LLM, that requires costly data collection. To address the above challenge, we introduce a new instruction tuning approach for building a mobile-centric text rewriting model. Our strategies enable the generation of high quality training data without any human labeling. In addition, we propose a heuristic reinforcement learning framework which substantially enhances performance without requiring preference data. To further bridge the performance gap with the larger server-side model, we propose an effective approach that combines
    
[^34]: 在文本中使用离群学习进行少样本异常检测

    Few-shot Anomaly Detection in Text with Deviation Learning. (arXiv:2308.11780v1 [cs.LG])

    [http://arxiv.org/abs/2308.11780](http://arxiv.org/abs/2308.11780)

    本论文介绍了一种基于深度少样本学习的框架FATE，它通过离群学习明确地学习文本中的异常得分，并利用先前已知的少量异常示例，从而克服了传统方法中对无标签数据的依赖，并优化了异常得分的精确度和数据利用效率。

    

    目前大多数文本异常检测方法都集中在构建仅依赖无标签数据的模型上。这些模型基于没有可用的标记异常示例的假设运行，在许多实际应用中，这些异常通常以小数量存在，这阻碍了它们利用先前已知的异常知识。此外，这些模型更注重学习特征嵌入而不是直接优化异常得分，这可能会导致次优的异常得分和学习过程中数据的低效利用。在本文中，我们介绍了FATE，一种基于深度少样本学习的框架，它利用有限的异常示例，并使用离群学习的端到端方法明确地学习异常得分。在这种方法中，将正常示例的异常得分调整为与先前分布获得的参考得分相似。相反，异常样本被迫具有明显偏离的异常得分。

    Most current methods for detecting anomalies in text concentrate on constructing models solely relying on unlabeled data. These models operate on the presumption that no labeled anomalous examples are available, which prevents them from utilizing prior knowledge of anomalies that are typically present in small numbers in many real-world applications. Furthermore, these models prioritize learning feature embeddings rather than optimizing anomaly scores directly, which could lead to suboptimal anomaly scoring and inefficient use of data during the learning process. In this paper, we introduce FATE, a deep few-shot learning-based framework that leverages limited anomaly examples and learns anomaly scores explicitly in an end-to-end method using deviation learning. In this approach, the anomaly scores of normal examples are adjusted to closely resemble reference scores obtained from a prior distribution. Conversely, anomaly samples are forced to have anomalous scores that considerably devi
    
[^35]: 使用自动语音识别系统和深度学习主题模型在智能手机采集的自由回答语音录音中识别与抑郁相关的主题

    Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model. (arXiv:2308.11773v1 [cs.CL])

    [http://arxiv.org/abs/2308.11773](http://arxiv.org/abs/2308.11773)

    通过自动语音识别系统和深度学习主题模型，我们在智能手机采集的语音录音中识别出与抑郁相关的29个主题，并确定了其中6个主题作为抑郁的风险主题。此研究表明，通过长期监测语言使用，可以了解主题的出现与抑郁之间的关联。

    

    语言使用已被证明与抑郁相关，但需要大规模验证。传统方法如诊所研究费用高昂。因此，自然语言处理已被应用于社交媒体上预测抑郁，但仍存在一些限制-缺乏验证标签、样本偏差和缺乏上下文。我们的研究使用Whisper工具和BERTopic模型在来自265名参与者的3919个智能手机录音中识别出29个主题。其中6个主题的PHQ-8中位数大于或等于10被视为抑郁的风险主题：没有期望、睡眠、心理治疗、理发、学习和课程。为了阐明主题的出现和与抑郁的关联，我们比较了识别出的主题在行为（来自可穿戴设备）和语言特征方面的差异。还对主题转变和随时间变化的抑郁严重程度之间的相关性进行了研究，表明了长期监测语言使用的重要性。我们还进行了实证研究

    Language use has been shown to correlate with depression, but large-scale validation is needed. Traditional methods like clinic studies are expensive. So, natural language processing has been employed on social media to predict depression, but limitations remain-lack of validated labels, biased user samples, and no context. Our study identified 29 topics in 3919 smartphone-collected speech recordings from 265 participants using the Whisper tool and BERTopic model. Six topics with a median PHQ-8 greater than or equal to 10 were regarded as risk topics for depression: No Expectations, Sleep, Mental Therapy, Haircut, Studying, and Coursework. To elucidate the topic emergence and associations with depression, we compared behavioral (from wearables) and linguistic characteristics across identified topics. The correlation between topic shifts and changes in depression severity over time was also investigated, indicating the importance of longitudinally monitoring language use. We also tested
    
[^36]: 提高ChatGPT生成的假科学检测的方法：引入xFakeBibs监督学习网络算法

    Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm. (arXiv:2308.11767v1 [cs.CL])

    [http://arxiv.org/abs/2308.11767](http://arxiv.org/abs/2308.11767)

    本文介绍了一种能够提高对ChatGPT生成的假科学进行检测的算法。通过使用一种新设计的监督机器学习算法，该算法能够准确地将机器生成的出版物与科学家生成的出版物区分开来。结果表明，ChatGPT在技术术语方面与真实科学存在显著差异。算法在分类过程中取得了较高的准确率。

    

    ChatGPT正在成为现实。本文展示了如何区分ChatGPT生成的出版物与科学家生成的出版物。通过使用一种新设计的监督机器学习算法，我们演示了如何检测机器生成的出版物和科学家生成的出版物。该算法使用100个真实出版物摘要进行训练，然后采用10倍交叉验证方法建立了一个接受范围的下限和上限。与ChatGPT内容进行比较，明显可见ChatGPT仅贡献了23\%的二元组内容，这比其他10个交叉验证中的任何一个都少50\%。这个分析凸显了ChatGPT在技术术语上与真实科学的明显差异。在对每篇文章进行分类时，xFakeBibs算法准确地将98篇出版物识别为假的，有2篇文献错误地分类为真实出版物。尽管这项工作引入了一种算法应用

    ChatGPT is becoming a new reality. In this paper, we show how to distinguish ChatGPT-generated publications from counterparts produced by scientists. Using a newly designed supervised Machine Learning algorithm, we demonstrate how to detect machine-generated publications from those produced by scientists. The algorithm was trained using 100 real publication abstracts, followed by a 10-fold calibration approach to establish a lower-upper bound range of acceptance. In the comparison with ChatGPT content, it was evident that ChatGPT contributed merely 23\% of the bigram content, which is less than 50\% of any of the other 10 calibrating folds. This analysis highlights a significant disparity in technical terms where ChatGPT fell short of matching real science. When categorizing the individual articles, the xFakeBibs algorithm accurately identified 98 out of 100 publications as fake, with 2 articles incorrectly classified as real publications. Though this work introduced an algorithmic app
    
[^37]: Halo：评估和降低开源弱大语言模型中的幻觉

    Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v1 [cs.CL])

    [http://arxiv.org/abs/2308.11764](http://arxiv.org/abs/2308.11764)

    本文介绍了一种用于评估和减少开源弱大语言模型中幻觉问题的框架，并探索了知识注入和师生方法等技术来减轻低参数模型中的幻觉问题，实验结果表明，在挑战性领域中，这些模型的幻觉问题得到了减少。

    

    大型语言模型(LLMs)已经彻底改变了自然语言处理(NLP)领域。虽然对于研究和实际应用来说方便，但是与其更大规模的对应模型相比，开源的参数较少的LLMs经常出现严重幻觉问题。本文着重于测量和减少BLOOM 7B中的幻觉问题，该模型是公开提供给研究和商业应用的弱开源LLMs的代表。我们引入了HaloCheck，一种轻量级的无需知识的黑盒子框架，用于量化LLMs中幻觉问题的严重程度。此外，我们探索了知识注入和师生方法等技术，以减轻低参数LLMs中的幻觉问题。我们的实验证明了在这些LLMs的挑战性领域中幻觉问题的减少。

    Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP). Although convenient for research and practical applications, open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts. This paper focuses on measuring and reducing hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs that are publicly available for research and commercial applications. We introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs. Additionally, we explore techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs. Our experiments effectively demonstrate the reduction of hallucinations in challenging domains for these LLMs.
    
[^38]: KnowledGPT：利用检索和存储访问知识库增强大型语言模型

    KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases. (arXiv:2308.11761v1 [cs.CL])

    [http://arxiv.org/abs/2308.11761](http://arxiv.org/abs/2308.11761)

    KnowledGPT是一个全面框架，通过将大型语言模型与知识库集成，实现了知识的检索和存储。与其他方法相比，KnowledGPT能够更全面和准确地回答各种问题。

    

    大型语言模型(LLM)在自然语言处理领域展现出惊人的影响力，但仍然存在一些问题，如完整性、及时性、准确性和适应性。最近的研究致力于将LLMs与外部知识源连接起来，但是知识库(KBs)的整合仍然鲜为人知且面临着挑战。在本文中，我们介绍了KnowledGPT，这是一个全面的框架，旨在将LLMs与各种知识库连接起来，方便知识的检索和存储。检索过程采用思维启发程序，生成用于KB的搜索语言的代码，其中预定义了KB操作的函数。除了检索外，KnowledGPT还提供了将知识存储在个性化知识库中的能力，以满足个体用户需求。通过大量实验证明，通过将LLMs与KBs集成，KnowledGPT能够正确回答更广泛范围的问题。

    Large language models (LLMs) have demonstrated impressive impact in the field of natural language processing, but they still struggle with several issues regarding, such as completeness, timeliness, faithfulness and adaptability. While recent efforts have focuses on connecting LLMs with external knowledge sources, the integration of knowledge bases (KBs) remains understudied and faces several challenges. In this paper, we introduce KnowledGPT, a comprehensive framework to bridge LLMs with various knowledge bases, facilitating both the retrieval and storage of knowledge. The retrieval process employs the program of thought prompting, which generates search language for KBs in code format with pre-defined functions for KB operations. Besides retrieval, KnowledGPT offers the capability to store knowledge in a personalized KB, catering to individual user demands. With extensive experiments, we show that by integrating LLMs with KBs, KnowledGPT properly answers a broader range of questions 
    
[^39]: 多文档问答中的知识图谱引导

    Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v1 [cs.CL])

    [http://arxiv.org/abs/2308.11730](http://arxiv.org/abs/2308.11730)

    这篇论文提出了一种知识图谱引导的方法，用于在多文档问答任务中为大型语言模型（LLMs）提示正确的上下文。通过构建多个文档上的知识图谱，并设计基于语言模型的图遍历器，该方法能够帮助LLMs在MD-QA中进行答案预测。

    

    大型语言模型（LLMs）的“预训练、提示、预测”范式在开放域问答（OD-QA）中取得了显著的成功。然而，很少有工作在多文档问答（MD-QA）场景下探索这个范式，这是一个要求对不同文档的内容和结构之间的逻辑关联有深入理解的任务。为了填补这一重要的空白，我们提出了一种知识图谱引导（KGP）方法，用于在MD-QA中为LLMs提示正确的上下文，该方法包括图构建模块和图遍历模块。对于图构建，我们使用节点来表示文段或文档结构（例如，页面/表格），而使用边来表示文段之间的语义/词汇相似性或者文档内的结构关系。对于图遍历，我们设计了一个基于LM的图遍历器，它在节点之间导航并收集支持性的文段，以帮助LLMs在MD-QA中进行答案预测。

    The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD
    
[^40]: 通过使用来自集合扩展的示例进行语言探测来推进关系提取

    Advancing Relation Extraction through Language Probing with Exemplars from Set Co-Expansion. (arXiv:2308.11720v1 [cs.CL])

    [http://arxiv.org/abs/2308.11720](http://arxiv.org/abs/2308.11720)

    本文提出了一种通过集合扩展和代表性示例的语言探测方法来推进关系提取。该方法通过整合相似度度量和类别排序，提高了关系分类准确性并减少对比类之间的混淆。经验证明该方法有效提高了关系提取的性能。

    

    关系提取是从非结构化文本中自动提取结构化信息的关键任务。本文提出了一种多方面的方法，通过整合代表性示例和集合扩展来提高关系分类准确性并减少对比类之间的混淆。我们的方法首先通过代表性示例为每个关系类提供种子。随后，我们的集合扩展算法通过将目标对和目标类的代表对之间的相似度度量纳入训练目标来丰富训练目标。此外，集合扩展过程还涉及一个考虑对比类示例的类别排序过程。利用无上下文的Hearst模式利用关系提及的上下文细节来确定上下文相似性。经验证明我们的集合扩展方法的有效性，提高了关系提取的性能。

    Relation Extraction (RE) is a pivotal task in automatically extracting structured information from unstructured text. In this paper, we present a multi-faceted approach that integrates representative examples and through co-set expansion. The primary goal of our method is to enhance relation classification accuracy and mitigating confusion between contrastive classes.  Our approach begins by seeding each relationship class with representative examples. Subsequently, our co-set expansion algorithm enriches training objectives by incorporating similarity measures between target pairs and representative pairs from the target class. Moreover, the co-set expansion process involves a class ranking procedure that takes into account exemplars from contrastive classes. Contextual details encompassing relation mentions are harnessed via context-free Hearst patterns to ascertain contextual similarity.  Empirical evaluation demonstrates the efficacy of our co-set expansion approach, resulting in a
    
[^41]: 有效的语言模型基准测试

    Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])

    [http://arxiv.org/abs/2308.11696](http://arxiv.org/abs/2308.11696)

    本研究提出了一种名为"Efficient Benchmarking"的问题，旨在智能地减少语言模型评估的计算成本而不降低可靠性，并使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估决策的可靠性。通过HELM基准测试的案例研究，发现只需删除一个低排名模型即可改变领先者，并仅需少量示例即可得到正确的基准测试排名。

    

    语言模型的多功能性增加导致了一类全面评估广泛能力的基准测试的出现。这些基准测试与大规模计算成本相关，每个模型需要数千个GPU小时。然而，关于评估效率方面的问题在文献中讨论较少。本文提出了一种名为"Efficient Benchmarking"的问题，即在不损害可靠性的情况下智能地减少语言模型评估的计算成本。通过使用HELM基准测试作为示例，我们研究了不同基准测试设计选择如何影响计算-可靠性权衡。我们提出使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估这些决策的可靠性。例如，我们发现仅通过从基准测试中删除一个低排名模型，当前在HELM上的领先者可能会改变，并且观察到只需一小部分示例即可获得正确的基准测试排名。

    The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
    
[^42]: 实时学习生成和修复语言

    Learning to generate and corr- uh I mean repair language in real-time. (arXiv:2308.11683v1 [cs.CL])

    [http://arxiv.org/abs/2308.11683](http://arxiv.org/abs/2308.11683)

    本文针对实时语言处理能力的发展，使用动态语法和CHILDES语料库开发了一个基于概率的逐步生成模型，在78%的情况下能完全匹配最佳候选输出，且具备自我修复能力。

    

    在对话中，发言者逐字逐句地产生语言，并不断监控自己的贡献是否适当，同时动态地适应对话的环境，这经常导致他们在说话过程中即时修复自己的话语。这种实时语言处理能力对于流利和自然的对话人工智能的发展至关重要。在本文中，我们使用先前学习的动态语法和CHILDES语料库，开发、训练和评估了一个基于概率的逐步生成模型，模型的输入是一个纯粹的语义生成目标概念，使用类型理论与记录（TTR）。我们展示了模型在78%的情况下与最佳候选输出完全匹配，ROUGE-l得分为0.86。我们进一步对同一模型生成在话语过程中生成目标发生变化时的自我修复能力进行了零样本评估。自动评估表明，模型可以生成自我修复。

    In conversation, speakers produce language incrementally, word by word, while continuously monitoring the appropriateness of their own contribution in the dynamically unfolding context of the conversation; and this often leads them to repair their own utterance on the fly. This real-time language processing capacity is furthermore crucial to the development of fluent and natural conversational AI. In this paper, we use a previously learned Dynamic Syntax grammar and the CHILDES corpus to develop, train and evaluate a probabilistic model for incremental generation where input to the model is a purely semantic generation goal concept in Type Theory with Records (TTR). We show that the model's output exactly matches the gold candidate in 78% of cases with a ROUGE-l score of 0.86. We further do a zero-shot evaluation of the ability of the same model to generate self-repairs when the generation goal changes mid-utterance. Automatic evaluation shows that the model can generate self-repairs c
    
[^43]: Tryage: 实时智能路由用户提示到大型语言模型

    Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model. (arXiv:2308.11601v1 [cs.LG])

    [http://arxiv.org/abs/2308.11601](http://arxiv.org/abs/2308.11601)

    Tryage是一个上下文感知的路由系统，能够根据对个体输入提示的分析，从模型库中选择最佳的专家模型，以消除模型选择和定制化的负担，释放庞大的新兴模型库的巨大威力给最终用户。

    

    变压器架构和自注意机制的引入导致了在特定下游任务和数据领域训练的语言模型的爆炸性增长。在Hugging Face生态系统中有超过200,000个模型，用户在选择和优化模型以适应多方面的工作流程和数据领域的同时，还要解决计算、安全和时效性等问题。迫切需要机器学习框架来消除模型选择和定制化的负担，并释放庞大的新兴模型库的巨大威力给最终用户。在这里，我们提出了一个上下文感知的路由系统Tryage，它利用语言模型路由器根据对个体输入提示的分析，从模型库中选择最佳的专家模型。受大脑中的丘脑路由器启发，Tryage采用感知路由器来预测下游模型在提示上的性能，并根据目标做出路由决策。

    The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an ob
    
[^44]: 作为用户模拟器的大型语言模型

    Large Language Model as a User Simulator. (arXiv:2308.11534v1 [cs.CL])

    [http://arxiv.org/abs/2308.11534](http://arxiv.org/abs/2308.11534)

    本文创新性地将从真实人机对话中提取的人类问题作为学习目标，并且训练了一个用户模拟器UserGPT，并使用生成的高质量合成对话数据集RealChat来训练助手模型ReaLM。实验证明，ReaLM在多个基准测试中超过了基准模型。

    

    闭源ChatGPT的卓越性能引发了对其民主化的努力，借助真实用户和ChatGPT对话的努力取得了显著进展，Vicuna是一个很好的例子。然而，目前的Baize和UltraChat等努力主要依靠ChatGPT根据指令模拟人类行为，而不是真实的人类学习，导致范围有限，多样性减弱，缺乏真正的多轮对话动态。为了解决上述问题，我们创新性地把从真实人机对话中提取的人类问题作为学习目标，并训练一个用户模拟器UserGPT来生成高质量的以人为中心的合成对话数据集RealChat。随后，该数据集训练我们的助手模型ReaLM。实验证明，ReaLM在Vicuna-Bench和MT-Bench中均超过了基准模型。

    The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT conversations, as evidenced by Vicuna. However, while current endeavors like Baize and UltraChat aim to auto-generate conversational data due to challenges in gathering human participation, they primarily rely on ChatGPT to simulate human behaviors based on directives rather than genuine human learning. This results in a limited scope, diminished diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we innovatively target human questions extracted from genuine human-machine conversations as a learning goal and train a user simulator, UserGPT, to produce a high-quality human-centric synthetic conversation dataset, RealChat. Subsequently, this dataset trains our assistant model, ReaLM. Experimentally, ReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise
    
[^45]: 句子级多模态和语言无关表示

    Sentence-Level Multimodal and Language-Agnostic Representations. (arXiv:2308.11466v1 [cs.CL])

    [http://arxiv.org/abs/2308.11466](http://arxiv.org/abs/2308.11466)

    这项研究引入了SONAR，一个多语言和多模态的句子嵌入空间，通过单一文本编码器在相似性搜索任务中取得显著优势，并提供了用于200种语言的文本解码器，可以进行文本到文本和语音到文本的机器翻译。这些结果相比现有模型具有竞争力，并且对语音到文本模型也取得了良好的结果。

    

    我们引入了SONAR，一个新的多语言和多模态的定长句子嵌入空间。我们的单一文本编码器覆盖了200种语言，在xsim和xsim++多语言相似性搜索任务上明显优于现有的句子嵌入模型LASER3和LabSE。使用特定语言的语音编码器在师生设置下训练语音转录数据后，语音片段可以在同一SONAR嵌入空间中进行嵌入。我们的编码器在相似性搜索任务上优于现有的语音编码器。我们还提供了一个适用于200种语言的文本解码器，可以进行文本到文本和语音到文本的机器翻译，包括零翻译语言和模态组合。尽管存在定长瓶颈表示，我们的文本到文本结果在与最先进的NLLB~1B模型相比中具有竞争力。我们的零翻译语音到文本结果与强有力的监督基线模型Whisper相比表现良好。

    We introduce SONAR, a new multilingual and multimodal fixed-size sentence embedding space. Our single text encoder, covering 200 languages, substantially outperforms existing sentence embeddings such as LASER3 and LabSE on the xsim and xsim++ multilingual similarity search tasks. Speech segments can be embedded in the same SONAR embedding space using language-specific speech encoders trained in a teacher-student setting on speech transcription data. Our encoders outperform existing speech encoders on similarity search tasks. We also provide a text decoder for 200 languages, which allows us to perform text-to-text and speech-to-text machine translation, including for zero-shot language and modality combinations. Our text-to-text results are competitive compared to the state-of-the-art NLLB~1B model, despite the fixed-size bottleneck representation. Our zero-shot speech-to-text translation results compare favorably with strong supervised baselines such as Whisper.
    
[^46]: BAN-PL: 一份来自Wykop.pl网站的禁止有害和攻击性内容的新波兰数据集

    BAN-PL: a Novel Polish Dataset of Banned Harmful and Offensive Content from Wykop.pl web service. (arXiv:2308.10592v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10592](http://arxiv.org/abs/2308.10592)

    BAN-PL是波兰语的第一个开放数据集，包含来自Wykop这个类似"波兰版Reddit"的社交网络服务的被标记为有害并删除的内容，将有助于改进自动检测互联网上的冒犯性语言的技术。

    

    在自动检测互联网上的冒犯性语言、仇恨言论和网络欺凌方面取得的进展需要改进对包含社交媒体内容的公开可用数据集的访问。在本文中，我们介绍了BAN-PL，这是第一个以波兰语提供的开放数据集，它包含了被专业审查员标记为有害并随后被删除的文本。数据集共包含来自Wykop这个颇受欢迎的社交网络服务的691,662条内容，其中包括帖子和评论，并且平均分为两个不同的类别：“有害”和“中立”。我们提供了数据收集和预处理程序的详细说明，并强调了数据的语言特殊性。BAN-PL数据集以及用于预处理脏话的高级脚本将公开提供。

    Advances in automated detection of offensive language online, including hate speech and cyberbullying, require improved access to publicly available datasets comprising social media content. In this paper, we introduce BAN-PL, the first open dataset in the Polish language that encompasses texts flagged as harmful and subsequently removed by professional moderators. The dataset encompasses a total of 691,662 pieces of content from a popular social networking service, Wykop, often referred to as the "Polish Reddit", including both posts and comments, and is evenly distributed into two distinct classes: "harmful" and "neutral". We provide a comprehensive description of the data collection and preprocessing procedures, as well as highlight the linguistic specificity of the data. The BAN-PL dataset, along with advanced preprocessing scripts for, i.a., unmasking profanities, will be publicly available.
    
[^47]: 人在循环中的可持续性优化自动形式化方法

    A Human-on-the-Loop Optimization Autoformalism Approach for Sustainability. (arXiv:2308.10380v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.10380](http://arxiv.org/abs/2308.10380)

    本文介绍了一种使用大型语言模型的自然对话方法来解决个性化能源相关问题的新概念。我们的方法将自然语言任务规范自动翻译为优化实例，使得模型能够理解和响应用户规范和偏好，并提供非线性推理能力。这一方法突破了当前基于提示的技术的限制，能够解决各种实例相关的能源问题。

    

    本文介绍了一种使用大型语言模型（LLM）以自然对话的方式解决个性化能源相关问题的方法。我们专注于可定制的优化问题，这些问题需要反复解决，并且在建模上有轻微的变化，并且是用户特定的，因此对于制定一种适合所有用户的模型是一个挑战。我们提出了一种策略，将优化求解器与LLM相结合，增强其理解和响应用户规范和偏好的能力，同时提供非线性推理能力。我们的方法开创了人导向的优化自动形式化的新概念，将自然语言任务规范自动翻译为优化实例。这使得LLM能够分析、解释和解决各种与实例相关的能源问题，突破了当前基于提示的技术的限制。我们的研究涵盖了能源领域的各种常见任务，从电力到...

    This paper outlines a natural conversational approach to solving personalized energy-related problems using large language models (LLMs). We focus on customizable optimization problems that necessitate repeated solving with slight variations in modeling and are user-specific, hence posing a challenge to devising a one-size-fits-all model. We put forward a strategy that augments an LLM with an optimization solver, enhancing its proficiency in understanding and responding to user specifications and preferences while providing nonlinear reasoning capabilities. Our approach pioneers the novel concept of human-guided optimization autoformalism, translating a natural language task specification automatically into an optimization instance. This enables LLMs to analyze, explain, and tackle a variety of instance-specific energy-related problems, pushing beyond the limits of current prompt-based techniques.  Our research encompasses various commonplace tasks in the energy sector, from electric v
    
[^48]: 大型语言模型在分布外检测方面有多好？

    How Good Are Large Language Models at Out-of-Distribution Detection?. (arXiv:2308.10261v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10261](http://arxiv.org/abs/2308.10261)

    本文通过对大型语言模型进行实证调查，探索了分布外检测的能力。作者发现了LLM在分布外检测方面的差异，并采用了新的生成式微调方法，提高了模型的性能。

    

    分布外（OOD）检测在提高机器学习（ML）模型的可靠性方面起着至关重要的作用。大型语言模型（LLM）的出现在ML社区引起了范式转变，展示了它们在各种自然语言处理任务中的出色能力。尽管现有的研究已经以BERT、RoBERTa和GPT-2等相对小规模的Transformer模型探索了OOD检测，但在规模、预训练目标和推理范式方面的明显差异引发了对这些发现在LLM中的适用性的质疑。本文在LLMs领域进行了开创性的实证调查，重点关注7B到65B大小的LLaMA系列。我们对常用的OOD检测器进行了全面评估，审查了它们在零梯度和微调场景下的性能。值得注意的是，我们将之前的判别式的内部分布微调改为生成式微调，使LLM的预训练目标与之一致。

    Out-of-distribution (OOD) detection plays a vital role in enhancing the reliability of machine learning (ML) models. The emergence of large language models (LLMs) has catalyzed a paradigm shift within the ML community, showcasing their exceptional capabilities across diverse natural language processing tasks. While existing research has probed OOD detection with relative small-scale Transformers like BERT, RoBERTa and GPT-2, the stark differences in scales, pre-training objectives, and inference paradigms call into question the applicability of these findings to LLMs. This paper embarks on a pioneering empirical investigation of OOD detection in the domain of LLMs, focusing on LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate commonly-used OOD detectors, scrutinizing their performance in both zero-grad and fine-tuning scenarios. Notably, we alter previous discriminative in-distribution fine-tuning into generative fine-tuning, aligning the pre-training objective of LLM
    
[^49]: 使用连续发言的方式对大型语言模型进行红队评估以实现安全对齐

    Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment. (arXiv:2308.09662v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.09662](http://arxiv.org/abs/2308.09662)

    这项工作以红队评估的方式对大型语言模型进行安全评估，发现即便是广泛部署的模型也容易受到连续发言提示的影响，导致违反伦理地对有害查询做出回应。通过红队评估尝试，发现多数开源LLM也会生成有害回应。研究者提出了一种LLM安全对齐的方法。

    

    大型语言模型（LLMs）通过优化下一个单词预测目标，以其巨大的多任务能力震撼世界。随着它们的属性和编码知识的出现，LLMs产生有害输出的风险增加，使它们不适合可扩展地部署给公众。在这项工作中，我们提出了一个新的安全评估基准RED-EVAL，进行红队评估。我们展示了即便是广泛部署的模型也容易受到基于连续发言的(CoU)提示的影响，使基于GPT-4和ChatGPT的闭源LLM系统违反伦理地对超过65%和73%的有害查询做出回应。我们还展示了RED-EVAL在8个开源LLM中的一致性，通过红队评估尝试生成86%以上的有害回应。接下来，我们提出了RED-INSTRUCT--一种用于LLM安全对齐的方法。它包括两个阶段：1）HARMFULQA数据收集：利用CoU提示,

    Larger language models (LLMs) have taken the world by storm with their massive multi-tasking capabilities simply by optimizing over a next-word prediction objective. With the emergence of their properties and encoded knowledge, the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public. In this work, we propose a new safety evaluation benchmark RED-EVAL that carries out red-teaming. We show that even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of harmful queries. We also demonstrate the consistency of the RED-EVAL across 8 open-source LLMs in generating harmful responses in more than 86% of the red-teaming attempts. Next, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. It constitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting, 
    
[^50]: MemoChat: 通过调整LLMs使用备忘录以保持一致性的长距离开放领域对话

    MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation. (arXiv:2308.08239v1 [cs.CL])

    [http://arxiv.org/abs/2308.08239](http://arxiv.org/abs/2308.08239)

    MemoChat提出了一种用于调优指令的流程，通过让大型语言模型使用备忘录来保持对话一致性。实验证实了其有效性。

    

    我们提出了MemoChat，一个用于优化指令的流水线，使大规模语言模型（LLMs）能够有效地使用自行组织的备忘录来保持一致的长距离开放领域对话。我们通过迭代的“记忆-检索-响应”循环展示了一个长距离的开放领域对话。这要求我们为每个不同的阶段精心设计定制的调优指令。这些指令是从一系列公共数据集中重建的，以教导LLMs记忆和检索过去的对话，并通过结构化备忘录提高未来对话的一致性。我们邀请专家手动注释一个用于评估长距离对话一致性的测试集。在涉及开源和可访问API的聊天机器人的三种测试场景上进行的实验证实了MemoChat的有效性，它超越了强基线。

    We propose MemoChat, a pipeline for refining instructions that enables large language models (LLMs) to effectively employ self-composed memos for maintaining consistent long-range open-domain conversations. We demonstrate a long-range open-domain conversation through iterative "memorization-retrieval-response" cycles. This requires us to carefully design tailored tuning instructions for each distinct stage. The instructions are reconstructed from a collection of public datasets to teach the LLMs to memorize and retrieve past dialogues with structured memos, leading to enhanced consistency when participating in future conversations. We invite experts to manually annotate a test set designed to evaluate the consistency of long-range conversations questions. Experiments on three testing scenarios involving both open-source and API-accessible chatbots at scale verify the efficacy of MemoChat, which outperforms strong baselines.
    
[^51]: 在大型语言模型中使用反向推理进行验证

    Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])

    [http://arxiv.org/abs/2308.07758](http://arxiv.org/abs/2308.07758)

    本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。

    

    链式思考（Chain-of-Though, CoT）提示在各种推理任务中表现出了很好的性能。最近，Self-Consistency提出了一种方法，即通过采样一组不同的推理链，这些链可能导致不同的答案，然后选择得票最多的答案。本文提出了一种新颖的方法，即在验证候选答案时使用反向推理。我们使用一个简单的模板，即``如果我们知道上述问题的答案是候选答案，那么未知变量x的值是多少？''，将问题中的一个标记屏蔽，并要求语言模型预测被屏蔽的标记。直观上讲，如果提供的候选答案是正确的，语言模型应该能够成功预测被屏蔽的标记。我们进一步提出了FOBAR方法，将正向和反向推理结合起来估计候选答案的概率。我们在六个数据集和三个实验中进行了广泛的实验。

    Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
    
[^52]: 关于最先进生成模型的可信度景观：一项综合调查

    On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.16680](http://arxiv.org/abs/2307.16680)

    本文综合调查了大规模生成模型的可信度问题，涵盖了隐私、安全、公平性和责任等多个维度，并提出了实际建议和未来发展方向。

    

    扩散模型和大规模语言模型已经成为领先的生成模型，并对人类生活的各个方面产生了革命性的影响。然而，这些模型的实际应用也暴露出固有的风险，突显了它们的双重性质，并引发了对它们可信度的担忧。尽管有大量关于这个主题的文献，但针对大规模生成模型及其可信度的综合调查仍然很少见。为了弥补这一空白，本文调查了涉及这些模型的长期和新兴威胁，涵盖了隐私、安全、公平和责任这四个基本维度。通过这种方式，我们构建了一张详尽的地图，概述了这些模型的可信度，并提供了实际建议和未来的发展方向。这些努力对于促进这些模型的可信度部署至关重要。

    Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ulti
    
[^53]: 探讨自然语言处理研究领域的发展趋势

    Exploring the Landscape of Natural Language Processing Research. (arXiv:2307.10652v1 [cs.CL])

    [http://arxiv.org/abs/2307.10652](http://arxiv.org/abs/2307.10652)

    该论文系统分类和分析了ACL Anthology中的研究论文，提供了对研究领域的结构化概述和NLP领域的分类学。本研究总结了最新的NLP发展，并提出了未来工作的方向。

    

    自然语言处理(NLP)作为理解、生成和处理自然语言文本的一种高效方法，在近年来得到了快速传播和广泛应用。鉴于该领域研究工作的不断增加，研究界已对数个与NLP相关的方法进行了调查。然而，到目前为止，仍缺少一项全面的研究，对已建立的主题进行分类、识别趋势并概括未来研究方向。为填补这一空白，我们对ACL Anthology中包含的研究论文进行了系统分类和分析。结果呈现了研究领域的结构化概述，为NLP领域的研究提供了一个分类学，分析了NLP的最新发展，总结了我们的研究发现，并突出了未来工作的方向。

    As an efficient approach to understand, generate, and process natural language texts, research in natural language processing (NLP) has exhibited a rapid spread and wide adoption in recent years. Given the increasing amount of research work in this area, several NLP-related approaches have been surveyed in the research community. However, a comprehensive study that categorizes established topics, identifies trends, and outlines areas for future research remains absent to this day. Contributing to closing this gap, we have systematically classified and analyzed research papers included in the ACL Anthology. As a result, we present a structured overview of the research landscape, provide a taxonomy of fields-of-study in NLP, analyze recent developments in NLP, summarize our findings, and highlight directions for future work.
    
[^54]: 自洽性方法用于无限生成问题的改进

    Self-consistency for open-ended generations. (arXiv:2307.06857v1 [cs.AI])

    [http://arxiv.org/abs/2307.06857](http://arxiv.org/abs/2307.06857)

    本论文提出了一种改进大规模预训练语言模型生成输出质量和一致性的新方法，通过扩展自洽性框架的适用性，实现了从一个候选集中恢复最优或接近最优的生成结果，并提出了一种轻量级无参数相似性函数来改进代码生成、自动形式化和摘要任务的效果。

    

    在这篇论文中，我们提出了一种改进大规模预训练语言模型生成输出的质量和一致性的新方法。自洽性已经被证明是一种有效的方法，对于具有固定答案的提示，选择得票最多的答案。我们引入了一个推广的自洽性框架，扩展了其适用性，超越了固定答案问题的范围。通过大量的模拟实验，我们证明了我们的方法能够从候选集中恢复最优或接近最优的生成结果。我们还提出了一种轻量级无参数相似性函数，即使没有访问到标记的概率，也能在代码生成、自动形式化和摘要任务中显著和一致地改进效果。我们的方法几乎没有计算开销，不需要额外的再排序模型或对现有模型的修改。

    In this paper, we present a novel approach for improving the quality and consistency of generated outputs from large-scale pre-trained language models (LLMs). Self-consistency has emerged as an effective approach for prompts with fixed answers, selecting the answer with the highest number of votes. In this paper, we introduce a generalized framework for self-consistency that extends its applicability beyond problems that have fixed-answer answers. Through extensive simulations, we demonstrate that our approach consistently recovers the optimal or near-optimal generation from a set of candidates. We also propose lightweight parameter-free similarity functions that show significant and consistent improvements across code generation, autoformalization, and summarization tasks, even without access to token log probabilities. Our method incurs minimal computational overhead, requiring no auxiliary reranker models or modifications to the existing model.
    
[^55]: 自动分配和分类软件问题

    Automated Assignment and Classification of Software Issues. (arXiv:2307.00009v1 [cs.CL])

    [http://arxiv.org/abs/2307.00009](http://arxiv.org/abs/2307.00009)

    本论文提出了一种自动分配和分类软件问题的方法。通过使用经过精心策划的语言特征和不同的机器学习方法，将问题分配给最相关的团队成员，并将其分类为不同的类别，以提高工作效率和准确性。

    

    软件问题包含修复、改进或创建新线程的工作单元，在开发过程中促进团队成员之间的沟通。将问题分配给最相关的团队成员并确定问题的类别是一项繁琐且具有挑战性的任务。错误的分类会导致项目延迟和重新工作，给团队成员带来麻烦。本文提出了一组经过精心策划的用于浅层机器学习方法的语言特征，并将浅层方法和集成方法与深度语言模型的性能进行了比较。与现有技术不同的是，我们将问题分配给四种角色（设计师、开发人员、测试人员和领导者），而不是特定的个人或团队，以促进我们解决方案的普遍性。我们还考虑开发人员的经验水平，以反映我们解决方案的工业实践。我们采用分类方法将问题分类为不同的类别，包括错误、新功能、改进等。

    Software issues contain units of work to fix, improve or create new threads during the development and facilitate communication among the team members. Assigning an issue to the most relevant team member and determining a category of an issue is a tedious and challenging task. Wrong classifications cause delays and rework in the project and trouble among the team members. This thesis proposes a set of carefully curated linguistic features for shallow machine learning methods and compares the performance of shallow and ensemble methods with deep language models. Unlike the state-of-the-art, we assign issues to four roles (designer, developer, tester, and leader) rather than to specific individuals or teams to contribute to the generality of our solution. We also consider the level of experience of the developers to reflect the industrial practices in our solution formulation. We employ a classification approach to categorize issues into distinct classes, namely bug, new feature, improve
    
[^56]: 链式思维提示提取多模态命名实体和多模态关系抽取技术

    Chain-of-Thought Prompt Distillation for Multimodal Named Entity and Multimodal Relation Extraction. (arXiv:2306.14122v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.14122](http://arxiv.org/abs/2306.14122)

    本研究提出了一种链式思维提示提取方法，将大型语言模型的推理能力转化为更紧凑的学生模型，从而提高了多模态命名实体识别和多模态关系抽取的效果。

    

    多模态命名实体识别（MNER）和多模态关系抽取（MRE）需要处理复杂语言和多模态理解的基本推理能力。本研究探索了将大型语言模型（LLMs）的推理能力提炼为更紧凑的学生模型的方法，通过生成一系列中间推理步骤来实现。具体而言，我们首先通过涵盖多粒度（名词、句子、多模态）和数据增强（样式、实体、图像）维度的链式思维提示，展示了从LLMs中引导此类推理能力的示例。随后，我们提出了一种新的条件提示提取方法，以吸收LLMs中的常识推理能力，从而增强学生模型在处理仅文本输入时的实用性，而无需添加图像和链式思维知识。大量实验证明，我们的方法达到了最先进的准确性，并表现出更好的性能。

    Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction (MRE) necessitate the fundamental reasoning capacity for intricate linguistic and multimodal comprehension. In this study, we explore distilling the reasoning ability of large language models (LLMs) into a more compact student model by generating a \textit{chain of thought} (CoT) -- a sequence of intermediate reasoning steps. Specifically, we commence by exemplifying the elicitation of such reasoning ability from LLMs through CoT prompts covering multi-grain (noun, sentence, multimodality) and data-augmentation (style, entity, image) dimensions. Subsequently, we present a novel conditional prompt distillation method to assimilate the commonsense reasoning ability from LLMs, thereby enhancing the utility of the student model in addressing text-only inputs without the requisite addition of image and CoT knowledge. Extensive experiments reveal that our approach attains state-of-the-art accuracy and manifests a p
    
[^57]: 使用逻辑编程和大型语言模型的领域特定问题回答在知识图谱上。 (arXiv:2303.02206v2 [cs.LG] UPDATED)

    Domain Specific Question Answering Over Knowledge Graphs Using Logical Programming and Large Language Models. (arXiv:2303.02206v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02206](http://arxiv.org/abs/2303.02206)

    该论文提出了一种在知识图谱上回答领域特定问题的方法，通过将经典的逻辑编程语言与大型语言模型整合，实现逻辑推理能力来解决KGQA任务。实验证明该方法在准确识别所有测试问题的正确答案实体方面表现出较高效果，即使仅使用少量标注数据进行训练。这种方法为解决领域特定图谱上的问题回答提供了有前景的解决方案，具有可解释性和稳健性。

    

    在领域特定的图谱上回答问题需要一种定制的方法，因为关系的数量有限，并且领域的特定性。我们的方法将经典的逻辑编程语言整合到大型语言模型中，使其能够利用逻辑推理能力来解决KGQA任务。通过将问题表示为Prolog查询，我们可以容易地生成程序化生成的答案。为了验证我们的方法的有效性，我们使用一个著名的基准数据集MetaQA进行评估。实验结果表明，即使在训练了一小部分注释数据的情况下，我们的方法也能准确地识别出所有测试问题的正确答案实体。总体而言，我们的工作提出了一种有前景的方法来解决领域特定图谱上的问题回答，通过整合逻辑编程提供了一个可解释和健壮的解决方案。

    Answering questions over domain-specific graphs requires a tailored approach due to the limited number of relations and the specific nature of the domain. Our approach integrates classic logical programming languages into large language models (LLMs), enabling the utilization of logical reasoning capabilities to tackle the KGQA task. By representing the questions as Prolog queries, which are readable and near close to natural language in representation, we facilitate the generation of programmatically derived answers. To validate the effectiveness of our approach, we evaluate it using a well-known benchmark dataset, MetaQA. Our experimental results demonstrate that our method achieves accurate identification of correct answer entities for all test questions, even when trained on a small fraction of annotated data. Overall, our work presents a promising approach to addressing question answering over domain-specific graphs, offering an explainable and robust solution by incorporating log
    
[^58]: NLP作为定位因果关系和感知挖掘心理健康的社交媒体的镜头

    NLP as a Lens for Causal Analysis and Perception Mining to Infer Mental Health on Social Media. (arXiv:2301.11004v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11004](http://arxiv.org/abs/2301.11004)

    本研究以NLP作为镜头，通过定位因果关系和感知挖掘心理健康，研究了社交媒体上用户的心理语言资源，提出了关键维度和领域，以促进临床心理学实践和个性化心理保健。

    

    在社交媒体上，人们之间的互动往往传达了他们行为背后的意图，为在线用户的心理健康分析提供了心理语言资源。计算智能技术在从这些社交媒体资源中推断心理疾病方面的成功，表明NLP可以作为定位因果关系和感知挖掘的镜头。然而，我们认为需要更多有意义和可解释的研究，以在临床心理学实践和个性化心理保健方面产生最佳影响。为了弥合这一差距，我们提出了两个重要维度：（1）定位因果关系以说明用户自动生成的文本中的因果关系；（2）感知挖掘以推断社交效应对在线用户意图的心理观点。在自然语言处理（NLP）范围内，我们进一步探讨与这两个维度相关的关键研究领域，特别是通过最近在话语分析方面的进展。

    Interactions among humans on social media often convey intentions behind their actions, yielding a psychological language resource for Mental Health Analysis (MHA) of online users. The success of Computational Intelligence Techniques (CIT) for inferring mental illness from such social media resources points to NLP as a lens for causal analysis and perception mining. However, we argue that more consequential and explainable research is required for optimal impact on clinical psychology practice and personalized mental healthcare. To bridge this gap, we posit two significant dimensions: (1) Causal analysis to illustrate a cause and effect relationship in the user generated text; (2) Perception mining to infer psychological perspectives of social effects on online users intentions. Within the scope of Natural Language Processing (NLP), we further explore critical areas of inquiry associated with these two dimensions, specifically through recent advancements in discourse analysis. This pos
    
[^59]: 低资源条件下的作者风格转移：非知名作者能够被模仿吗？

    Low-Resource Authorship Style Transfer: Can Non-Famous Authors Be Imitated?. (arXiv:2212.08986v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.08986](http://arxiv.org/abs/2212.08986)

    该论文介绍了一种低资源条件下的作者风格转移任务，该任务是一类更具挑战性的作者风格转移，仅使用有限数量的目标作者风格文本。该研究对于非知名作者的风格转移尚未有充分的研究，而现有的方法主要适用于已发表的作家、政治家或其他知名人士和作者风格。

    

    作者风格转移是指将文本改写成目标作者的风格，同时保留原始意思。现有的无监督方法大多专注于将风格转移到在书籍、演讲或其他已发表作品中具有许多示例的目标作者身上。这种高资源的训练数据要求（通常大于10万个词）使得这些方法主要适用于将风格转移到已发表的作家、政治家或其他知名人士和作者风格上，而转移到非知名作者的风格尚未得到充分研究。我们提出了“低资源条件下的作者风格转移”任务，这是一类更具挑战性的作者风格转移，仅存在有限数量的目标作者风格文本。在我们的实验中，我们特别选择了Reddit上的源作者和目标作者，并对他们的Reddit帖子进行风格转移，限制自己仅使用了16篇帖子（平均约500个词）的数据。

    Authorship style transfer involves altering text to match the style of a target author whilst preserving the original meaning. Existing unsupervised approaches like STRAP have largely focused on style transfer to target authors with many examples of their writing style in books, speeches, or other published works. This high-resource training data requirement (often greater than 100,000 words) makes these approaches primarily useful for style transfer to published authors, politicians, or other well-known figures and authorship styles, while style transfer to non-famous authors has not been well-studied. We introduce the \textit{low-resource authorship style transfer} task, a more challenging class of authorship style transfer where only a limited amount of text in the target author's style may exist. In our experiments, we specifically choose source and target authors from Reddit and style transfer their Reddit posts, limiting ourselves to just 16 posts (on average ~500 words) of the t
    
[^60]: PyABSA: 一个用于可复现的基于方面的情感分析的模块化框架

    PyABSA: A Modularized Framework for Reproducible Aspect-based Sentiment Analysis. (arXiv:2208.01368v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.01368](http://arxiv.org/abs/2208.01368)

    PyABSA是一个基于PyTorch的可复现的基于方面的情感分析的模块化框架，支持多个ABSA子任务和数据集，具有灵活的扩展性，并解决了数据稀缺问题。

    

    随着方面情感分析（ABSA）的发展，急需一个用户友好的框架，可以大大降低复制最新技术的ABSA性能的难度，尤其是对初学者来说。为了满足需求，我们介绍了PyABSA，一个基于PyTorch构建的模块化框架，用于可复现的ABSA。为了促进ABSA研究，PyABSA支持多个ABSA子任务，包括方面术语提取、方面情感分类和端到端的基于方面的情感分析。具体而言，PyABSA集成了29个模型和26个数据集。只需几行代码，就可以复现模型在特定数据集上的结果。通过模块化设计，PyABSA还可以灵活扩展到考虑的模型、数据集和其他相关任务。此外，PyABSA突出了其数据增强和标注功能，显著解决了数据稀缺问题。欢迎大家在\url{https://github.com/yangheng95/PyABSA}上尝试使用。

    The advancement of aspect-based sentiment analysis (ABSA) has urged the lack of a user-friendly framework that can largely lower the difficulty of reproducing state-of-the-art ABSA performance, especially for beginners. To meet the demand, we present \our, a modularized framework built on PyTorch for reproducible ABSA. To facilitate ABSA research, PyABSA supports several ABSA subtasks, including aspect term extraction, aspect sentiment classification, and end-to-end aspect-based sentiment analysis. Concretely, PyABSA integrates 29 models and 26 datasets. With just a few lines of code, the result of a model on a specific dataset can be reproduced. With a modularized design, PyABSA can also be flexibly extended to considered models, datasets, and other related tasks. Besides, PyABSA highlights its data augmentation and annotation features, which significantly address data scarcity. All are welcome to have a try at \url{https://github.com/yangheng95/PyABSA}.
    
[^61]: 让一阶线性逻辑成为生成语法

    Making first order linear logic a generating grammar. (arXiv:2206.08955v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.08955](http://arxiv.org/abs/2206.08955)

    本文研究了一阶线性逻辑与扩展张量类型演算的关系，提出了一种固有的演绎系统。

    

    众所周知，不同的范畴语法在一阶乘法线性逻辑的一个片段中具有表面表示。 我们表明，该片段等价于最近引入的扩展张量类型演算。 这不仅为前者提供了一些替代的语法和直观的几何表示，而且还提供了一个固有的演绎系统，这是以前缺少的。

    It is known that different categorial grammars have surface representation in a fragment of first order multiplicative linear logic. We show that the fragment of interest is equivalent to the recently introduced {\it extended tensor type calculus}. This provides the former not only with some alternative syntax and intuitive geometric representation, but also with an intrinsic deductive system, which has been absent.
    
[^62]: 结构化跨度选择器

    A Structured Span Selector. (arXiv:2205.03977v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.03977](http://arxiv.org/abs/2205.03977)

    提出了一种新颖的基于语法的结构化跨度选择模型，通过利用部分跨度级注释，摒弃了贪心跨度选择方案，为共指消解和语义角色标注任务带来了实证改进。

    

    许多自然语言处理任务，例如共指消解和语义角色标注，需要选择文本跨度并对其进行决策。这些任务的一种典型方法是对所有可能的跨度进行评分，并贪婪地选择跨度进行特定任务的下游处理。然而，这种方法并没有结合任何归纳偏置来确定应该选择哪种跨度，例如选择的跨度往往是句法成分。在本文中，我们提出了一种新颖的基于语法的结构化跨度选择模型，该模型学习利用为这些问题提供的部分跨度级注释。与先前的方法相比，我们的方法摒弃了启发式的贪婪跨度选择方案，使我们能够在最佳一组跨度上对下游任务进行建模。我们在两个常见的跨度预测任务：共指消解和语义角色标注上评估了我们的模型，并展示了实证改进。

    Many natural language processing tasks, e.g., coreference resolution and semantic role labeling, require selecting text spans and making decisions about them. A typical approach to such tasks is to score all possible spans and greedily select spans for task-specific downstream processing. This approach, however, does not incorporate any inductive bias about what sort of spans ought to be selected, e.g., that selected spans tend to be syntactic constituents. In this paper, we propose a novel grammar-based structured span selection model which learns to make use of the partial span-level annotation provided for such problems. Compared to previous approaches, our approach gets rid of the heuristic greedy span selection scheme, allowing us to model the downstream task on an optimal set of spans. We evaluate our model on two popular span prediction tasks: coreference resolution and semantic role labeling. We show empirical improvements on both.
    
[^63]: 深入深度学习

    Dive into Deep Learning. (arXiv:2106.11342v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.11342](http://arxiv.org/abs/2106.11342)

    《深入深度学习》是一本旨在使深度学习易于理解的开源书籍，提供从概念到代码的教学资源，旨在成为成为应用机器学习科学家的起点，并允许社区快速更新和互动讨论。

    

    这本开源书是我们的努力，让深度学习变得易于理解，教读者概念、背景和代码。整本书都是在Jupyter笔记本中起草的，与独立的代码无缝集成了说明图、数学和互动示例。我们的目标是提供一个资源，既可以自由使用，又可以提供足够的技术深度，为成为应用机器学习科学家的起点; 包括可运行的代码，向读者展示如何实践解决问题; 允许快速更新，不仅由我们，还由整个社区更新; 接受技术细节的互动讨论和解答问题的论坛。

    This open-source book represents our attempt to make deep learning approachable, teaching readers the concepts, the context, and the code. The entire book is drafted in Jupyter notebooks, seamlessly integrating exposition figures, math, and interactive examples with self-contained code. Our goal is to offer a resource that could (i) be freely available for everyone; (ii) offer sufficient technical depth to provide a starting point on the path to actually becoming an applied machine learning scientist; (iii) include runnable code, showing readers how to solve problems in practice; (iv) allow for rapid updates, both by us and also by the community at large; (v) be complemented by a forum for interactive discussion of technical details and to answer questions.
    

