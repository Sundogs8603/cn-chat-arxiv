# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models](https://rss.arxiv.org/abs/2402.01620) | MAGDi是一种结构化蒸馏方法，通过将多个大语言模型之间的推理交互表示为图形，来改善较小语言模型的推理能力。 |
| [^2] | [Towards a \textbf{RAG}-based Summarization Agent for the Electron-Ion Collider](https://arxiv.org/abs/2403.15729) | 开发了一种面向电子离子对撞机的基于RAG的摘要生成代理，能够压缩信息并引用相关回复，为合作者提供重大优势 |
| [^3] | [An Entropy-based Text Watermarking Detection Method](https://arxiv.org/abs/2403.13485) | 提出了一种基于熵的水印检测方法，在检测过程中根据令牌的熵调整其权重，以更好地反映水印程度，该方法在大型语言模型中具有应用潜力。 |
| [^4] | [From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards](https://arxiv.org/abs/2403.13213) | 本文探讨了针对表现性伤害和服务质量伤害的羊驼2安全保障措施的有效性，并指出了大型语言模型在实用性和安全性之间的权衡关系。 |
| [^5] | [CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data](https://arxiv.org/abs/2403.11346) | 提出了CantonMT项目，利用合成反向翻译数据对粤语至英语NMT模型进行微调，并为研究人员提供用户友好的界面和开源工具包，以促进研究 |
| [^6] | [ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models](https://arxiv.org/abs/2403.11103) | 通过让大语言模型自省特定领域，生成领域相关属性并创建属性丰富的训练数据，同时绕过复杂结构的挑战，实现了生成命名实体识别数据集的创新策略。 |
| [^7] | [BAGEL: Bootstrapping Agents by Guiding Exploration with Language](https://arxiv.org/abs/2403.08140) | BAGEL 提出了一种新方法，通过循环操作将随机探索到的轨迹或合成指令转换为演示，以使语言模型代理能够在新环境中自主学习适应。 |
| [^8] | [Exploring Safety Generalization Challenges of Large Language Models via Code](https://arxiv.org/abs/2403.07865) | 本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。 |
| [^9] | [Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds](https://arxiv.org/abs/2403.06609) | 大型语言模型在临床推理中展现出潜力，但存在幻觉问题和与医生决策路径不一致的挑战。 |
| [^10] | [Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models](https://arxiv.org/abs/2403.06448) | 提出了一种利用大型语言模型内部状态进行实时幻觉检测的无监督训练框架，并引入了一个新的基准用于评估多个大型语言模型的幻觉检测。 |
| [^11] | [Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset](https://arxiv.org/abs/2403.04460) | Pearl数据集利用了角色和知识增强的大型语言模型，提供了具体用户偏好，领域专业性和更相关的推荐。 |
| [^12] | [Learning to Maximize Mutual Information for Chain-of-Thought Distillation](https://arxiv.org/abs/2403.03348) | 通过最大化两个任务的表示特征的互信息，提出了一种解决思维链蒸馏中标签预测任务与知识集成不足问题的变分方法。 |
| [^13] | [Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics](https://arxiv.org/abs/2403.01509) | 通过在每个层的末端探测其隐藏状态，使用一个上下文化的词识别任务，本文具体研究了Llama2的自下而上词汇语义演变，发现较低层的表示编码了词汇语义，而较高层负责预测。 |
| [^14] | [WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection](https://arxiv.org/abs/2403.01472) | 本研究提出了一种名为WARDEN的新方法，通过在嵌入文本中加入多个可能的水印方向，增加了水印消除的难度，以应对EaaS中背门水印被移除的新威胁。 |
| [^15] | [Self-Consistent Reasoning-based Aspect-Sentiment Quad Prediction with Extract-Then-Assign Strategy](https://arxiv.org/abs/2403.00354) | SCRAP采用自洽推理和提取-分配策略，显著改善了模型在处理复杂推理任务和正确预测四元组方面的能力，提高了ASQP中的可解释性和准确性。 |
| [^16] | [Resonance RoPE: Improving Context Length Generalization of Large Language Models](https://arxiv.org/abs/2403.00071) | Resonance RoPE是一种新颖方法，通过调整RoPE特征的插值来缩小训练短-测试长场景下的泛化差距，在不增加额外在线计算成本的情况下显著提高模型性能。 |
| [^17] | [PRSA: Prompt Reverse Stealing Attacks against Large Language Models](https://arxiv.org/abs/2402.19200) | 本文提出了针对商业LLMs的提示反窃取攻击框架PRSA，通过分析输入-输出对的关键特征实现攻击。 |
| [^18] | [VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models](https://arxiv.org/abs/2402.18374) | VerifiNER是一个后续验证框架，通过知识识别并修正现有命名实体识别方法的错误，以实现更忠实的预测。 |
| [^19] | [UniVS: Unified and Universal Video Segmentation with Prompts as Queries](https://arxiv.org/abs/2402.18115) | UniVS提出了使用提示作为查询的统一视频分割架构，通过平均化前一帧中目标的提示特征来解码掩码，并引入了基于目标的提示交叉注意层，从而解决了统一视频分割模型的挑战。 |
| [^20] | [Large Language Models on Tabular Data -- A Survey](https://arxiv.org/abs/2402.17944) | 该研究综述了大型语言模型在处理表格数据上的应用，包括关键技术、指标、数据集、模型和优化方法，为未来研究方向提供了启示。 |
| [^21] | [Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data](https://arxiv.org/abs/2402.17644) | 本研究引入了QRData基准测试，评估了大型语言模型在统计和因果推理方面的能力，结果显示最强模型GPT-4在该测试中准确率为58％，存在改进空间。 |
| [^22] | [Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder](https://arxiv.org/abs/2402.17433) | 通过Contrastive EEG-Text Masked Autoencoder（CET-MAE）和E2T-PTR框架，提出了一种新的模型和方法来增强基于EEG的语言解码。 |
| [^23] | [Set the Clock: Temporal Alignment of Pretrained Language Models](https://arxiv.org/abs/2402.16797) | 该研究探讨了预训练语言模型的时间混乱问题，并提出了时间对齐的方法，实验证明将LMs对齐到最近时间可以显著提高性能 |
| [^24] | [How Important Is Tokenization in French Medical Masked Language Models?](https://arxiv.org/abs/2402.15010) | 子词标记化成为自然语言处理领域的主流标准，但其成功因素，如不同任务和语言的最佳分割粒度、数据源对标记工具的影响以及形态信息在印欧语言中的作用，仍不明确。 |
| [^25] | [GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data](https://arxiv.org/abs/2402.14973) | 提出了一种名为GenCeption的新型MLLM评估框架，可以仅利用单模态数据评估跨模态语义一致性，并有效反映模型产生幻觉的倾向，具有较强的相关性和潜力于流行的MLLM基准结果。 |
| [^26] | [Ranking Large Language Models without Ground Truth](https://arxiv.org/abs/2402.14860) | 不需要基准实况或参考响应的条件下，通过考虑模型的三元组来排名大型语言模型，并提出了两种排名方法。 |
| [^27] | [Generalizing Reward Modeling for Out-of-Distribution Preference Learning](https://arxiv.org/abs/2402.14760) | 通过元学习方法优化通用奖励模型，以解决超出分布偏好学习问题，并提高LLMs在有限偏好反馈下的泛化能力 |
| [^28] | [CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples](https://arxiv.org/abs/2402.13254) | 本研究提出CounterCurate框架，通过对比例子和生成式微调，全面提升视觉-语言组合推理能力，解决了物理推理和语义对照微调方面的关键问题，实现了显著性能改进。 |
| [^29] | [The Hidden Space of Transformer Language Adapters](https://arxiv.org/abs/2402.13137) | Transformer语言适配器在冻结的表示空间上操作，适应过程渐进分布在多层中，并且大部分预测仍在源语言中进行演变。 |
| [^30] | [Simpson's Paradox and the Accuracy-Fluency Tradeoff in Translation](https://arxiv.org/abs/2402.12690) | 准确性和流畅度在翻译中表现为正相关的悖论，这是辛普森悖论的一个实例，在语料库级别上二者呈正相关，在单个源段级别上存在权衡。 |
| [^31] | [Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?](https://arxiv.org/abs/2402.12483) | LLMs能够在没有问题的情况下仅从选项中回答多项选择题，通过记忆、选择动态和问题推理进行黑盒分析，揭示了LLMs在选择性准确性方面的三个关键发现。 |
| [^32] | [Emergent Word Order Universals from Cognitively-Motivated Language Models](https://arxiv.org/abs/2402.12363) | 认知驱动的语言模型显示出可以解释许多词序普遍规律的优势 |
| [^33] | [GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations](https://arxiv.org/abs/2402.12348) | 该论文通过博弈论任务评估了LLMs在竞争环境中的推理能力，观察到LLMs在不同游戏场景下表现出不同行为，具有重要的战略推理局限性。 |
| [^34] | [Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion](https://arxiv.org/abs/2402.12195) | 提出了一个两阶段范式"浏览和集中"，通过在将特征输入LLMs之前进行深入的多模态上下文融合，解决了多模态内容理解中的 prior-LLM 模态隔离问题 |
| [^35] | [LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs](https://arxiv.org/abs/2402.11804) | 本文利用大型语言模型（LLMs）生成图形结构提示，以增强预训练的图神经网络（GNNs），提出一种新的方法论见解，实现了在任意知识图上进行低资源归纳推理的高通用性。 |
| [^36] | [MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs](https://arxiv.org/abs/2402.11756) | MARS提出了一种新的评分函数MARS，考虑了生成序列中每个标记的语义贡献，该方法改进了生成式LLMs中的不确定性估计性能。 |
| [^37] | [One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation](https://arxiv.org/abs/2402.11683) | 通过释放涵盖观点摘要评估相关七个维度的新数据集SUMMEVAL-OP，研究人员提出了 Op-I-Prompt 作为一种独立于维度的提示方法，以及 Op-Prompts 作为一组依赖于维度的提示，可以表明 Op-I-Prompt 是评估观点摘要的一个很好的替代方案，实现了平均 Spearman 相关性为 0.70。 |
| [^38] | [k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text](https://arxiv.org/abs/2402.11399) | k-SemStamp是一种简单而有效的语义水印方案，通过利用k均值聚类代替LSH来提高鲁棒性和抽样效率，同时保持生成质量，为机器生成文本检测提供了更有效的工具。 |
| [^39] | [Training Language Model Agents without Modifying Language Models](https://arxiv.org/abs/2402.11359) | 提出一种新的方法，在不修改语言模型的情况下训练语言模型代理，通过进化代理的功能来解决下游任务 |
| [^40] | [When LLMs Meet Cunning Questions: A Fallacy Understanding Benchmark for Large Language Models](https://arxiv.org/abs/2402.11100) | 该论文提出了一个谬误理解基准FLUB，挑战大型语言模型在推理和理解能力上，重点是通过设计狡猾问题评估LLMs的谬误理解能力。 |
| [^41] | [Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements](https://arxiv.org/abs/2402.10614) | 本文通过辩论调节LLMs，使其生成可控的支持用户定义论点的声明，改进了LLMs的可控性，并提出了DEBATunE流程。通过两个LLMs之间的多轮辩论生成高质量的训练数据，以支持生成有更高质量和更突出的声明。 |
| [^42] | [BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains](https://arxiv.org/abs/2402.10373) | BioMistral是一种面向生物医学领域的开源预训练大型语言模型集合，在医学问答任务中表现出优越性能并具有竞争优势。 |
| [^43] | [Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning](https://arxiv.org/abs/2402.10110) | 本文介绍了一种名为选择性反射调节的新方法，该方法通过教师LLM的反射和自省与学生LLM的数据选择能力相结合，自动优化现有的指令调节数据，从而实现了高效的指令调节和卓越性能的LLM。 |
| [^44] | [Text Detoxification as Style Transfer in English and Hindi](https://arxiv.org/abs/2402.07767) | 本文研究了文本解毒化的任务，旨在将有毒文本自动转化为无毒文本。通过知识转移、多任务学习和删除重建方法，我们提出了三种解决方案。我们利用Dementieva等人提供的数据集进行实验，并引入了一个小型的印地语平行数据集用于评估。 |
| [^45] | [Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning](https://arxiv.org/abs/2402.06255) | 本文提出了一种名为Prompt Adversarial Tuning (PAT)的方法，通过训练一个防御控制机制并将其作为前缀嵌入到用户提示中，实现对大型语言模型（LLMs）的越狱行为的防御。实验证明该方法在保护LLMs免受产生有害信息的影响方面效果显著。 |
| [^46] | [Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation](https://arxiv.org/abs/2402.05699) | 本文提出了一个通过社交场景模拟来自对齐大型语言模型的方法，以减轻其被滥用造成的潜在不良影响。通过一个名为MATRIX的虚拟排练空间，LLM可以在回答查询前考虑社交后果，并通过MATRIX-simulated数据的微调，保持对人类价值的遵从和推理速度的平衡。实验证明，在温和假设下，带有MATRIX的LLM胜过了宪法AI。 |
| [^47] | [AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers](https://arxiv.org/abs/2402.05602) | AttnLRP是首个能够忠实且全面地归因Transformer模型的输入和潜在表示，并具有与单一反向传播相似的计算效率的方法。它通过扩展逐层相关传递归因方法以处理注意力层来解决了黑盒Transformer模型的归因问题，具有超越现有方法的准确性和理解潜在表示的能力。 |
| [^48] | [Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset](https://arxiv.org/abs/2402.05547) | 本研究介绍了“ChatCoach”，一个集成人工智能与人类医生合作的框架，在交流医疗辅导中利用大型语言模型，提供模拟环境和实时反馈，以帮助医学学员提高沟通技巧。 |
| [^49] | [On Provable Length and Compositional Generalization](https://arxiv.org/abs/2402.04875) | 本研究针对包括深度集合、变压器、状态空间模型和简单递归神经网络等多种架构，探索了可证明的长度和组合泛化，认为对于长度和组合泛化，不同架构需要不同程度的表示识别。 |
| [^50] | [Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models](https://arxiv.org/abs/2402.03142) | 本文提出了一种简单、通用、非参数的剪枝算法KEN，它能在保持模型性能的同时大幅节省内存，通过选择性地保留最重要的参数实现了对transformer模型的优化。与其他方法相比，KEN在最少参数减少25%的情况下实现了与原始模型相等或更好的性能。 |
| [^51] | [BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback](https://arxiv.org/abs/2402.02479) | BRAIn是一种基于贝叶斯奖励条件化缩减推断的自然语言生成方法，通过反馈来改进RLHF，在LLM对齐中表现出较好的可扩展性和性能。 |
| [^52] | [DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models](https://arxiv.org/abs/2402.02392) | DeLLMa是一个旨在提高不确定环境下决策精度的框架，通过多步骤的脚手架程序，借鉴决策理论和效用理论的原则，可以显著提高大型语言模型的决策性能。 |
| [^53] | [OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838) | OLMo是一种真正开放的语言模型，旨在提供给研究社区强大的工具，以促进对语言模型科学的研究和创新。 |
| [^54] | [Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning](https://arxiv.org/abs/2402.00530) | 提出了一种名为“超级过滤”的方法，能够使用较小和较弱的模型对用于训练较大和较强的语言模型的指令数据进行过滤，从而降低了过滤成本，并在标准基准测试中获得更好的性能。 |
| [^55] | [BPDec: Unveiling the Potential of Masked Language Modeling Decoder in BERT pretraining](https://arxiv.org/abs/2401.15861) | 本文揭示了BPDec（BERT预训练解码器）的潜力，强调增强的掩码语言建模解码器设计及研究在BERT预训练中的重要性。 |
| [^56] | [PRewrite: Prompt Rewriting with Reinforcement Learning](https://arxiv.org/abs/2401.08189) | 本文提出了一种基于强化学习的自动化工具PRewrite，用于重写提示草案并生成高效的新提示，以解决提示工程中的挑战。 |
| [^57] | [Model Editing at Scale leads to Gradual and Catastrophic Forgetting](https://arxiv.org/abs/2401.07453) | 评估了当前模型编辑方法在规模化情况下的表现，发现随着模型被顺序编辑多个事实，它会逐渐遗忘先前的事实及执行下游任务的能力。 |
| [^58] | [Language Models Understand Numbers, at Least Partially](https://arxiv.org/abs/2401.03735) | 本研究表明，大型语言模型在某种程度上理解数字，可以通过压缩和编码的方式执行算术计算。 |
| [^59] | [BloomVQA: Assessing Hierarchical Multi-modal Comprehension](https://arxiv.org/abs/2312.12716) | 提出了新VQA数据集BloomVQA，基于Bloom的分类法，通过层次图表示实现数据增强和模型一致性评估，揭示大型视觉语言模型在高级理解任务上的性能下降。 |
| [^60] | [It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning](https://arxiv.org/abs/2311.07532) | 大型语言模型在排除推理过程中遇到困难，提出了一种新的排除推理方法PoE与COT，发现此方法在多项选择问题上的表现不如选择正确答案，并指出了研究中发现的一致性和错误分析的问题。 |
| [^61] | [Adaptive Text Watermark for Large Language Models.](http://arxiv.org/abs/2401.13927) | 这个论文提出了一种自适应的大型语言模型文字水印策略，通过辅助模型测量高熵令牌分布，将水印自适应地添加到具有高熵的令牌分布中，同时保持低熵令牌分布不变，以提高文本质量和水印的稳健性。 |
| [^62] | [MLLMReID: Multimodal Large Language Model-based Person Re-identification.](http://arxiv.org/abs/2401.13201) | MLLMReID是一种基于多模态大语言模型的人物再识别方法，通过微调模型并将其视觉编码器作为主干进行优化，解决了MLLM在ReID任务中的设计指令和特征学习效果的问题。 |
| [^63] | [Progressive Distillation Based on Masked Generation Feature Method for Knowledge Graph Completion.](http://arxiv.org/abs/2401.12997) | 本文提出了一种基于遮蔽生成特征的渐进蒸馏方法，用于知识图谱补全任务，通过预蒸馏和压缩预训练模型，以及引入遮蔽生成的教师-学生特征，显著降低了模型的复杂性，并解决了特征表示能力差距的问题。 |
| [^64] | [DeepEdit: Knowledge Editing as Decoding with Constraints.](http://arxiv.org/abs/2401.10471) | DeepEdit是一种神经符号方法，通过更好的推理一致性和对更新知识的意识，提高了大型语言模型的知识编辑能力，对多跳问题数据集MQuaKE取得了显著的进展。 |
| [^65] | [Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies.](http://arxiv.org/abs/2401.06760) | 本文研究了一系列现代度量标准的“动态范围”，以提供对度量标准之间和内部得分差异的共同理解，并通过人类感知水平的系统差异进行衡量。 |
| [^66] | [Adapting Large Language Models for Document-Level Machine Translation.](http://arxiv.org/abs/2401.06468) | 本文研究了适应大型语言模型进行文档级机器翻译的过程。实验结果显示，这些专门的模型在某些情况下超过了GPT-4的翻译性能，但在其他情况下仍然存在离标翻译问题，需要进一步改进和探索。 |
| [^67] | [Extreme Compression of Large Language Models via Additive Quantization.](http://arxiv.org/abs/2401.06118) | 本文提出的算法在大规模语言模型的极端压缩方面取得了较好的性能，相比最新技术，在给定的压缩预算下准确性更高。 |
| [^68] | [LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization.](http://arxiv.org/abs/2401.06034) | LinguAlchemy是一种将语言类型学和地理元素融合的正则化技术，能够显著提高预训练语言模型（PLMs）在未见语言上的泛化性能。 |
| [^69] | [Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space.](http://arxiv.org/abs/2312.12141) | 通过探索剩余流和分析词汇空间中的子值，我们定位了大型语言模型中的事实知识，并找到了存储了有关“法国，首都，巴黎”的知识的位置。 |
| [^70] | [Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning.](http://arxiv.org/abs/2310.10707) | 该论文在攻击性内容改写方面引入了上下文学习方法，并通过有限数量的输入-标签演示对来指导模型生成特定查询的所需输出，从而提高可用性和减少攻击性。 |
| [^71] | [Online Speculative Decoding.](http://arxiv.org/abs/2310.07177) | 在线推测解码是通过利用多余计算能力，在LLM服务集群中持续更新草稿模型，从而加速大型语言模型推理的一种方法。 |
| [^72] | [Fooling the Textual Fooler via Randomizing Latent Representations.](http://arxiv.org/abs/2310.01452) | 该论文提出了一种轻量级的攻击无关防御策略AdvFooler，通过随机化输入的潜在表示来困惑基于查询的黑盒攻击，从而迷惑文本愚弄者。 |
| [^73] | [LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games.](http://arxiv.org/abs/2309.17234) | 本文提出使用可评分的谈判游戏作为LLMs的新评估框架，创建了一个多样的测试平台，并通过系统化的零-shot思维链提示（CoT）展示了代理人可以成功谈判。该研究揭示了GPT-4在该任务上的性能差距。 |
| [^74] | [I Wish to Have an Argument: Argumentative Reasoning in Large Language Models.](http://arxiv.org/abs/2309.16938) | 该论文评估了当代大型语言模型在争论性推理方面的能力，并发现其争论性推理性能与输入和输出表示密切相关。研究还发现了“典型效应”，并探讨了在思维链提示下解决病态问题时的性能优势。 |
| [^75] | [Usability Evaluation of Spoken Humanoid Embodied Conversational Agents in Mobile Serious Games.](http://arxiv.org/abs/2309.07773) | 本文通过实证调查评估了移动严肃游戏应用中口语化人形机器人对可用性的影响，结果表明用户更喜欢与高人类相似度的机器人进行交互。 |
| [^76] | [Incorporating Class-based Language Model for Named Entity Recognition in Factorized Neural Transducer.](http://arxiv.org/abs/2309.07648) | 这项研究在分解神经传输器中加入了基于类别的语言模型，提升了命名实体识别的能力。 |
| [^77] | [Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts.](http://arxiv.org/abs/2309.06135) | 提出了Prompting4Debugging（P4D）作为一个调试和红队测试工具，可以自动找到扩散模型的问题提示，以测试部署的安全机制的可靠性。 |
| [^78] | [Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media.](http://arxiv.org/abs/2309.03564) | 本研究评估了监督学习和大型语言模型在识别中国社交媒体中的认知偏差和自杀风险方面的功效。结果表明大型语言模型在这两个任务上具有很高的效果。 |
| [^79] | [A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks.](http://arxiv.org/abs/2307.12114) | 这项研究评估了四种指导细调大型语言模型在临床和生物医学任务上的表现，并发现它们在零样本和少样本情况下接近最先进模型的性能，尤其在问答任务上表现良好。然而，在分类和关系抽取任务上的表现稍逊于特定训练于医学领域的模型。没有一个模型在所有研究任务上胜过其他模型，有些模型更适合特定任务。 |
| [^80] | [Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain.](http://arxiv.org/abs/2307.03042) | 本研究提出了一种参数高效微调（PEFT）方法，在临床领域使用临床记录训练了一个专门适配临床领域的LLaMA-LoRA模型，同时提出了一个两步PEFT框架，用于将其与Downstream LLaMA-LoRA适配器进行融合，以实现领域适应。 |
| [^81] | [Open-Domain Text Evaluation via Meta Distribution Modeling.](http://arxiv.org/abs/2306.11879) | 本文提出了一种新颖的开放领域文本生成模型评估方法——元分布方法（MDM），该方法将两个概率分布的对比映射到质量度量上，可以视为分布的分布，其可用于开放领域文本生成的评估。 |
| [^82] | [DAPR: A Benchmark on Document-Aware Passage Retrieval.](http://arxiv.org/abs/2305.13915) | DAPR是一个文档感知段落检索的基准测试，挑战在于如何从长文档中找到正确的段落并返回准确结果。 |
| [^83] | [Discern and Answer: Mitigating the Impact of Misinformation in Retrieval-Augmented Models with Discriminators.](http://arxiv.org/abs/2305.01579) | 本文研究了现有检索增强语言模型假设所有检索信息都是正确的假设的问题，在实际应用中可能存在虚假信息导致冲突的情况下，提出了通过精细调整鉴别器和提示鉴别能力引出鲁棒性的方法，这显著改善了模型在知识冲突下的效果；同时提供了关于交替精细调整模型和上下文学习的新的结论。 |

# 详细

[^1]: MAGDi：结构化蒸馏多智能体交互图在较小的语言模型中改善推理能力

    MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models

    [https://rss.arxiv.org/abs/2402.01620](https://rss.arxiv.org/abs/2402.01620)

    MAGDi是一种结构化蒸馏方法，通过将多个大语言模型之间的推理交互表示为图形，来改善较小语言模型的推理能力。

    

    大语言模型（LLM）智能体之间的多智能体交互在各种推理任务中表现出重大改进。然而，这些方法涉及多个模型之间的长时间生成，成本高昂。此外，这些多智能体方法无法提供一个最终的、单一的模型进行高效推理。为了解决这个问题，我们引入了MAGDi，一种新的方法，用于将多个LLM之间的推理交互结构化蒸馏到较小的模型中。MAGDi通过将多智能体交互表示为图形，使用图形编码器增强基础学生模型，并使用三个目标函数进行知识蒸馏：下一个令牌预测、正确和错误推理之间的对比损失以及基于图形的目标来建模交互结构。在七个广泛使用的常识和数学推理基准测试上的实验结果表明，MAGDi改善了较小模型的推理能力，优于几种方法。

    Multi-agent interactions between Large Language Model (LLM) agents have shown major improvements on diverse reasoning tasks. However, these involve long generations from multiple models across several rounds, making them expensive. Moreover, these multi-agent approaches fail to provide a final, single model for efficient inference. To address this, we introduce MAGDi, a new method for structured distillation of the reasoning interactions between multiple LLMs into smaller LMs. MAGDi teaches smaller models by representing multi-agent interactions as graphs, augmenting a base student model with a graph encoder, and distilling knowledge using three objective functions: next-token prediction, a contrastive loss between correct and incorrect reasoning, and a graph-based objective to model the interaction structure. Experiments on seven widely-used commonsense and math reasoning benchmarks show that MAGDi improves the reasoning capabilities of smaller models, outperforming several methods th
    
[^2]: 面向电子离子对撞机的基于RAG的摘要生成代理

    Towards a \textbf{RAG}-based Summarization Agent for the Electron-Ion Collider

    [https://arxiv.org/abs/2403.15729](https://arxiv.org/abs/2403.15729)

    开发了一种面向电子离子对撞机的基于RAG的摘要生成代理，能够压缩信息并引用相关回复，为合作者提供重大优势

    

    复杂性和庞大的信息量涵盖了大规模实验的文件、论文、数据和其他资源，导致导航这些多样形式信息的任务需要大量时间和精力，对于新合作者和早期科学家来说尤为艰巨。为了解决这个问题，正在开发一种基于检索增强生成（RAG）的EIC摘要生成人工智能代理（RAGS4EIC）。该人工智能代理不仅压缩信息，还有效引用相关回复，为合作者提供了重大优势。我们的项目采取了两步方法：首先，查询包含所有相关实验信息的综合向量数据库；其次，利用大型语言模型（LLM）根据用户查询和检索数据生成包含引用的简洁摘要。我们描述了使用RAG评估的评估方法

    arXiv:2403.15729v1 Announce Type: cross  Abstract: The complexity and sheer volume of information encompassing documents, papers, data, and other resources from large-scale experiments demand significant time and effort to navigate, making the task of accessing and utilizing these varied forms of information daunting, particularly for new collaborators and early-career scientists. To tackle this issue, a Retrieval Augmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under development. This AI-Agent not only condenses information but also effectively references relevant responses, offering substantial advantages for collaborators. Our project involves a two-step approach: first, querying a comprehensive vector database containing all pertinent experiment information; second, utilizing a Large Language Model (LLM) to generate concise summaries enriched with citations based on user queries and retrieved data. We describe the evaluation methods that use RAG assessments 
    
[^3]: 基于熵的文本水印检测方法

    An Entropy-based Text Watermarking Detection Method

    [https://arxiv.org/abs/2403.13485](https://arxiv.org/abs/2403.13485)

    提出了一种基于熵的水印检测方法，在检测过程中根据令牌的熵调整其权重，以更好地反映水印程度，该方法在大型语言模型中具有应用潜力。

    

    目前，大型语言模型（LLMs）的文本水印算法能够嵌入隐藏特征到LLMs生成的文本中，以便后续检测，从而缓解了LLMs被误用的问题。尽管当前的文本水印算法在大多数高熵情况下表现良好，但在低熵情况下仍需要改进。在这项工作中，我们提出在水印检测过程中应全面考虑令牌熵的影响，即应根据其熵调整每个令牌的重量，而不是像以前的方法中将所有令牌的重量设置为相同值。具体来说，我们提出了一种基于熵的水印检测（EWD），在水印检测过程中赋予高熵令牌更高的权重，以更好地反映水印程度。此外，所提出的检测过程无需训练。

    arXiv:2403.13485v1 Announce Type: new  Abstract: Currently, text watermarking algorithms for large language models (LLMs) can embed hidden features to texts generated by LLMs to facilitate subsequent detection, thus alleviating the problem of misuse of LLMs. Although the current text watermarking algorithms perform well in most high-entropy scenarios, its performance in low-entropy scenarios still needs to be improved. In this work, we proposed that the influence of token entropy should be fully considered in the watermark detection process, that is, the weight of each token should be adjusted according to its entropy during watermark detection, rather than setting the weight of all tokens to the same value as in previous methods. Specifically, we proposed an Entropy-based Watermark Detection (EWD) that gives higher-entropy tokens higher weights during watermark detection, so as to better reflect the degree of watermarking. Furthermore, the proposed detection process is training-free a
    
[^4]: 从表现性伤害到服务质量伤害:羊驼2安全保障的案例研究

    From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards

    [https://arxiv.org/abs/2403.13213](https://arxiv.org/abs/2403.13213)

    本文探讨了针对表现性伤害和服务质量伤害的羊驼2安全保障措施的有效性，并指出了大型语言模型在实用性和安全性之间的权衡关系。

    

    近期大型语言模型（LLM）的进展导致它们在各个领域被广泛采用。然而，这些进步也引入了额外的安全风险，并引发了对其对已经边缘化人群的不利影响的担忧。尽管存在越来越多的减轻措施来开发安全保障措施，比如监督式的安全定向微调和利用来自人类反馈的安全强化学习，但关于这些模型的安全性和内在偏见仍存在多重关注。此外，先前的研究已经证明，为了安全而优化的模型通常会展示夸大的安全行为，比如出于预防措施而倾向于不回应某些请求。因此，文献中已经记录了这些模型在实用性和安全性之间的明显权衡。在本文中，我们进一步研究了安全措施的有效性，通过评估...

    arXiv:2403.13213v1 Announce Type: cross  Abstract: Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations. Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluatin
    
[^5]: CantonMT: 汉英NMT平台，使用合成反向翻译数据对模型进行微调

    CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data

    [https://arxiv.org/abs/2403.11346](https://arxiv.org/abs/2403.11346)

    提出了CantonMT项目，利用合成反向翻译数据对粤语至英语NMT模型进行微调，并为研究人员提供用户友好的界面和开源工具包，以促进研究

    

    arXiv:2403.11346v1 消息类型：跨领域 摘要：对于低资源语言的神经机器翻译(NMT)仍然是自然语言处理研究人员面临的挑战。在这项工作中，我们将一个标准的数据增强方法——反向翻译，应用到了新的语言翻译方向粤语至英语。我们介绍了我们使用有限数量真实数据和生成的合成数据(包括OpusMT, NLLB,和mBART)进行微调的模型。我们使用了一系列不同指标包括基于词汇和嵌入的自动评估。此外，我们为这项\textsc{CantonMT}研究项目中包含的模型创建了一个用户友好的界面，并提供便利实现粤语至英语MT研究。研究人员可以通过我们的开源\textsc{CantonMT}工具包\url{https://github.com/kenrickkung/CantoneseTranslation}向平台添加更多模型。

    arXiv:2403.11346v1 Announce Type: cross  Abstract: Neural Machine Translation (NMT) for low-resource languages is still a challenging task in front of NLP researchers. In this work, we deploy a standard data augmentation methodology by back-translation to a new language translation direction Cantonese-to-English. We present the models we fine-tuned using the limited amount of real data and the synthetic data we generated using back-translation including OpusMT, NLLB, and mBART. We carried out automatic evaluation using a range of different metrics including lexical-based and embedding-based. Furthermore. we create a user-friendly interface for the models we included in this\textsc{ CantonMT} research project and make it available to facilitate Cantonese-to-English MT research. Researchers can add more models into this platform via our open-source\textsc{ CantonMT} toolkit \url{https://github.com/kenrickkung/CantoneseTranslation}.
    
[^6]: ProgGen:通过自反大语言模型逐步生成命名实体识别数据集

    ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models

    [https://arxiv.org/abs/2403.11103](https://arxiv.org/abs/2403.11103)

    通过让大语言模型自省特定领域，生成领域相关属性并创建属性丰富的训练数据，同时绕过复杂结构的挑战，实现了生成命名实体识别数据集的创新策略。

    

    虽然大语言模型（LLMs）在跨领域上表现出卓越的适应性，但这些模型在结构化知识提取任务（如命名实体识别NER）方面经常表现不佳。本文探讨了一种创新的、具有成本效益的策略，利用具有适度NER能力的LLMs来生成优秀的NER数据集。我们的方法不同于基本的类条件提示，而是指导LLMs对特定领域进行自我反思，从而生成具有领域相关属性（例如影评的类别和情感）的属性丰富的训练数据。此外，我们预先生成实体术语，然后围绕这些实体开发NER上下文数据，有效规避了LLMs对复杂结构的挑战。我们在通用和专业领域展开的实验显示，相对于传统数据生成方法，性能得到了显著提升，同时成本更低。

    arXiv:2403.11103v1 Announce Type: new  Abstract: Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER). This paper explores an innovative, cost-efficient strategy to harness LLMs with modest NER capabilities for producing superior NER datasets. Our approach diverges from the basic class-conditional prompts by instructing LLMs to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data. Furthermore, we preemptively generate entity terms and then develop NER context data around these entities, effectively bypassing the LLMs' challenges with complex structures. Our experiments across both general and niche domains reveal significant performance enhancements over conventional data generation methods while being mor
    
[^7]: BAGEL: 通过语言引导探索引导代理自主学习

    BAGEL: Bootstrapping Agents by Guiding Exploration with Language

    [https://arxiv.org/abs/2403.08140](https://arxiv.org/abs/2403.08140)

    BAGEL 提出了一种新方法，通过循环操作将随机探索到的轨迹或合成指令转换为演示，以使语言模型代理能够在新环境中自主学习适应。

    

    在数字环境（例如Web浏览器和REST API）中通过执行动作来遵循自然语言指令对于语言模型代理是一项具有挑战性的任务。不幸的是，语言模型代理经常在没有人类演示的情况下无法推广到新环境。本文提出了BAGEL，一种无需人类监督即可引导语言模型代理的方法。BAGEL将一组随机探索的轨迹或合成指令转换成演示，通过两个噪声语言模型组件之间的往返来完成：一个将轨迹转换为合成指令的语言模型标记器，以及一个将合成指令映射为经过改进的轨迹的零样本语言模型代理。通过迭代执行这些往返操作，BAGEL可以快速将最初的轨迹分布转变为那些被自然语言描述完善的轨迹。我们使用BAGEL演示来在测试时间通过在上下文学习上调零样本语言模型代理。

    arXiv:2403.08140v1 Announce Type: new  Abstract: Following natural language instructions by executing actions in digital environments (e.g. web-browsers and REST APIs) is a challenging task for language model (LM) agents. Unfortunately, LM agents often fail to generalize to new environments without human demonstrations. This work presents BAGEL, a method for bootstrapping LM agents without human supervision. BAGEL converts a seed set of randomly explored trajectories or synthetic instructions, into demonstrations, via round-trips between two noisy LM components: an LM labeler which converts a trajectory into a synthetic instruction, and a zero-shot LM agent which maps the synthetic instruction into a refined trajectory. By performing these round-trips iteratively, BAGEL quickly converts the initial distribution of trajectories towards those that are well-described by natural language. We use BAGEL demonstrations to adapt a zero shot LM agent at test time via in-context learning over re
    
[^8]: 通过代码探索大型语言模型的安全泛化挑战

    Exploring Safety Generalization Challenges of Large Language Models via Code

    [https://arxiv.org/abs/2403.07865](https://arxiv.org/abs/2403.07865)

    本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。

    

    大型语言模型（LLMs）的快速发展带来了自然语言处理方面的显著能力，但也引发了人们对它们潜在误用的担忧。本文引入了CodeAttack，一个将自然语言输入转换为代码输入的框架，为测试LLMs的安全泛化提供了一个新颖的环境。我们对包括GPT-4、Claude-2和Llama-2系列在内的最新LLMs进行了全面研究，发现这些模型对于代码输入存在共同的安全漏洞：CodeAttack在超过80%的时间内始终绕过所有模型的安全保护。

    arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
    
[^9]: 通过知识种子指导大型语言模型的临床推理

    Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds

    [https://arxiv.org/abs/2403.06609](https://arxiv.org/abs/2403.06609)

    大型语言模型在临床推理中展现出潜力，但存在幻觉问题和与医生决策路径不一致的挑战。

    

    临床推理是医生在评估和管理患者时采用的认知过程。这个过程通常涉及建议必要的检查，诊断患者疾病，并决定适当的治疗等。准确的临床推理需要广泛的医学知识和丰富的临床经验，为医生设置了很高的门槛。最近，像ChatGPT和GPT-4这样的大型语言模型(LLMs)显示出在临床推理中的潜力。然而，这些LLMs容易出现幻觉问题，而LLMs的推理过程可能与医生的临床决策路径不一致。在这项研究中，我们引入了一种

    arXiv:2403.06609v1 Announce Type: cross  Abstract: Clinical reasoning refers to the cognitive process that physicians employ in evaluating and managing patients. This process typically involves suggesting necessary examinations, diagnosing patients' diseases, and deciding on appropriate therapies, etc. Accurate clinical reasoning requires extensive medical knowledge and rich clinical experience, setting a high bar for physicians. This is particularly challenging in developing countries due to the overwhelming number of patients and limited physician resources, contributing significantly to global health inequity and necessitating automated clinical reasoning approaches. Recently, the emergence of large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated their potential in clinical reasoning. However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision path of physicians. In this study, we introduce a 
    
[^10]: 基于大型语言模型内部状态的无监督实时幻觉检测

    Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models

    [https://arxiv.org/abs/2403.06448](https://arxiv.org/abs/2403.06448)

    提出了一种利用大型语言模型内部状态进行实时幻觉检测的无监督训练框架，并引入了一个新的基准用于评估多个大型语言模型的幻觉检测。

    

    大型语言模型中的幻觉是指产生连贯但事实不准确的响应。为了解决LLMs中幻觉的问题，本文提出了MIND，一种利用LLMs内部状态进行实时幻觉检测的无监督训练框架。同时，我们还提出了HELM，一个用于评估多个LLMs幻觉检测的新基准，在LLMs推理过程中具有多样化的LLM输出和内部状态。

    arXiv:2403.06448v1 Announce Type: cross  Abstract: Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs. Previous studies have mainly concentrated on post-processing techniques for hallucination detection, which tend to be computationally intensive and limited in effectiveness due to their separation from the LLM's inference process. To overcome these limitations, we introduce MIND, an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations. Additionally, we present HELM, a new benchmark for evaluating hallucination detection across multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs during their inference process. Our experiments d
    
[^11]: Pearl: 一项基于评论驱动的角色知识对话式推荐数据集

    Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset

    [https://arxiv.org/abs/2403.04460](https://arxiv.org/abs/2403.04460)

    Pearl数据集利用了角色和知识增强的大型语言模型，提供了具体用户偏好，领域专业性和更相关的推荐。

    

    arXiv:2403.04460v1 公告类型：新摘要：对话式推荐系统是一个新兴领域，尤其是随着大型语言模型（LLMs）的进步，使得对话输入的多样化推理引起了社区的越来越大的兴趣。尽管取得了进展，但该领域还有许多方面有待探索。目前可用的用于对话式推荐的公共数据集缺乏特定用户偏好和对推荐的解释，从而妨碍了高质量的推荐。为了解决这些挑战，我们提出了一种新颖的对话式推荐数据集，命名为PEARL，与角色和知识增强的LLM模拟器相结合。我们从真实评论中获得详细的角色和知识，并构建了一个超过57k对话的大规模数据集。我们的实验结果表明，PEARL中的话语包括更具体的用户偏好，显示了在目标领域的专业知识，并提供了更相关的推荐。

    arXiv:2403.04460v1 Announce Type: new  Abstract: Conversational recommender system is an emerging area that has garnered an increasing interest in the community, especially with the advancements in large language models (LLMs) that enable diverse reasoning over conversational input. Despite the progress, the field has many aspects left to explore. The currently available public datasets for conversational recommendation lack specific user preferences and explanations for recommendations, hindering high-quality recommendations. To address such challenges, we present a novel conversational recommendation dataset named PEARL, synthesized with persona- and knowledge-augmented LLM simulators. We obtain detailed persona and knowledge from real-world reviews and construct a large-scale dataset with over 57k dialogues. Our experimental results demonstrate that utterances in PEARL include more specific user preferences, show expertise in the target domain, and provide recommendations more relev
    
[^12]: 学习最大化互信息进行思维链提炼

    Learning to Maximize Mutual Information for Chain-of-Thought Distillation

    [https://arxiv.org/abs/2403.03348](https://arxiv.org/abs/2403.03348)

    通过最大化两个任务的表示特征的互信息，提出了一种解决思维链蒸馏中标签预测任务与知识集成不足问题的变分方法。

    

    知识蒸馏是将大型复杂模型的知识传递给较小模型的技术，是实现高效人工智能部署的关键一步。通过利用思维链 (CoT) 蒸馏的新方法——逐步蒸馏 (DSS)，已经展示出为较小模型赋予其较大同行的优越推理能力的潜力。在DSS中，蒸馏模型通过一个多任务学习框架同时获得生成理由和预测标签的能力。然而，DSS忽略了这两个训练任务之间的内在关系，导致CoT知识与标签预测任务的有效整合不足。为此，我们从信息瓶颈的角度研究了两个任务之间的相互关系，并将其表述为最大化两个任务的表示特征的互信息。我们提出了一种变分方法来解决这个问题。

    arXiv:2403.03348v1 Announce Type: cross  Abstract: Knowledge distillation, the technique of transferring knowledge from large, complex models to smaller ones, marks a pivotal step towards efficient AI deployment. Distilling Step-by-Step (DSS), a novel method utilizing chain-of-thought (CoT) distillation, has demonstrated promise by imbuing smaller models with the superior reasoning capabilities of their larger counterparts. In DSS, the distilled model acquires the ability to generate rationales and predict labels concurrently through a multi-task learning framework. However, DSS overlooks the intrinsic relationship between the two training tasks, leading to ineffective integration of CoT knowledge with the task of label prediction. To this end, we investigate the mutual relationship of the two tasks from Information Bottleneck perspective and formulate it as maximizing the mutual information of the representation features of the two tasks. We propose a variational approach to solve thi
    
[^13]: 奇幻语义与寻找之地：探讨生成型LLM的哪些层次反映词汇语义

    Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics

    [https://arxiv.org/abs/2403.01509](https://arxiv.org/abs/2403.01509)

    通过在每个层的末端探测其隐藏状态，使用一个上下文化的词识别任务，本文具体研究了Llama2的自下而上词汇语义演变，发现较低层的表示编码了词汇语义，而较高层负责预测。

    

    大型语言模型在一般语言理解任务中取得了显著的成功。然而，作为一个追求下一个标记预测目标的生成方法家族，这些模型的语义演变随着深度并没有得到充分探索，不像它们的前辈，比如BERT类架构。本文具体研究了一款流行LLM，即Llama2的自下而上词汇语义演变，通过在每个层的末端探测其隐藏状态，使用一个上下文化的词识别任务。我们的实验表明，较低层的表示编码了词汇语义，而较高层，其语义归纳较弱，负责预测。这与具有判别目标的模型形成对比，比如掩码语言建模，在那里较高层获得更好的词汇语义。结论进一步得到性能单调增加的支持。

    arXiv:2403.01509v1 Announce Type: new  Abstract: Large language models have achieved remarkable success in general language understanding tasks. However, as a family of generative methods with the objective of next token prediction, the semantic evolution with the depth of these models are not fully explored, unlike their predecessors, such as BERT-like architectures. In this paper, we specifically investigate the bottom-up evolution of lexical semantics for a popular LLM, namely Llama2, by probing its hidden states at the end of each layer using a contextualized word identification task. Our experiments show that the representations in lower layers encode lexical semantics, while the higher layers, with weaker semantic induction, are responsible for prediction. This is in contrast to models with discriminative objectives, such as mask language modeling, where the higher layers obtain better lexical semantics. The conclusion is further supported by the monotonic increase in performance
    
[^14]: WARDEN：多方向背门水印用于Embedding-as-a-Service版权保护

    WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection

    [https://arxiv.org/abs/2403.01472](https://arxiv.org/abs/2403.01472)

    本研究提出了一种名为WARDEN的新方法，通过在嵌入文本中加入多个可能的水印方向，增加了水印消除的难度，以应对EaaS中背门水印被移除的新威胁。

    

    Embedding as a Service（EaaS）已成为一种广泛采用的解决方案，为自然语言处理（NLP）中的各种下游任务提供特征提取能力。先前的研究表明，EaaS容易受到模型抽取攻击的威胁；然而，通过向文本嵌入添加背门水印，并随后验证攻击模型的发布后，可以缓解这一问题。通过对最近用于EaaS的水印策略EmbMarker的分析，我们设计了一种新颖的CSE（Cluster、Selection、Elimination）攻击，它能够移除背门水印同时保持嵌入的高效性，表明先前的水印方法是可以被突破的。针对这一新威胁，我们提出了一种新的协议，通过整合多种可能的水印方向使水印的移除变得更具挑战性。我们的防御方法WARDEN显著增加了水印消除的难度。

    arXiv:2403.01472v1 Announce Type: cross  Abstract: Embedding as a Service (EaaS) has become a widely adopted solution, which offers feature extraction capabilities for addressing various downstream tasks in Natural Language Processing (NLP). Prior studies have shown that EaaS can be prone to model extraction attacks; nevertheless, this concern could be mitigated by adding backdoor watermarks to the text embeddings and subsequently verifying the attack models post-publication. Through the analysis of the recent watermarking strategy for EaaS, EmbMarker, we design a novel CSE (Clustering, Selection, Elimination) attack that removes the backdoor watermark while maintaining the high utility of embeddings, indicating that the previous watermarking approach can be breached. In response to this new threat, we propose a new protocol to make the removal of watermarks more challenging by incorporating multiple possible watermark directions. Our defense approach, WARDEN, notably increases the ste
    
[^15]: 自洽推理基于提取-分配策略的方面情感四元预测

    Self-Consistent Reasoning-based Aspect-Sentiment Quad Prediction with Extract-Then-Assign Strategy

    [https://arxiv.org/abs/2403.00354](https://arxiv.org/abs/2403.00354)

    SCRAP采用自洽推理和提取-分配策略，显著改善了模型在处理复杂推理任务和正确预测四元组方面的能力，提高了ASQP中的可解释性和准确性。

    

    在方面情感四元预测（ASQP）任务中，用于预测情感四元的生成方法显示出了有希望的结果。然而，它们仍然受到不精确预测和有限可解释性的困扰，这是由数据稀缺性和四元组合成过程建模不足引起的。本文提出了自洽推理-基于方面情感四元预测（SCRAP），优化其模型以按顺序生成推理和相应的情感四元。 SCRAP采用提取-然后-分配推理策略，紧密模仿人类认知。最后，SCRAP通过一致性投票显著改善了模型处理复杂推理任务以及通过一致性投票正确预测四元组的能力，从而提高了ASQP中的可解释性和准确性。

    arXiv:2403.00354v1 Announce Type: new  Abstract: In the task of aspect sentiment quad prediction (ASQP), generative methods for predicting sentiment quads have shown promising results. However, they still suffer from imprecise predictions and limited interpretability, caused by data scarcity and inadequate modeling of the quadruplet composition process. In this paper, we propose Self-Consistent Reasoning-based Aspect-sentiment quadruple Prediction (SCRAP), optimizing its model to generate reasonings and the corresponding sentiment quadruplets in sequence. SCRAP adopts the Extract-Then-Assign reasoning strategy, which closely mimics human cognition. In the end, SCRAP significantly improves the model's ability to handle complex reasoning tasks and correctly predict quadruplets through consistency voting, resulting in enhanced interpretability and accuracy in ASQP.
    
[^16]: 提升大型语言模型的上下文长度泛化能力：共振 RoPE

    Resonance RoPE: Improving Context Length Generalization of Large Language Models

    [https://arxiv.org/abs/2403.00071](https://arxiv.org/abs/2403.00071)

    Resonance RoPE是一种新颖方法，通过调整RoPE特征的插值来缩小训练短-测试长场景下的泛化差距，在不增加额外在线计算成本的情况下显著提高模型性能。

    

    本文针对大型语言模型（LLMs）中的训练短-测试长（TSTL）场景挑战，引入了Rotary Position Embedding（RoPE）技术，解决了在较短序列上预训练的模型在较长序列中遇到位置超出分布（OOD）的困难。我们提出了Resonance RoPE，一种新颖的方法，通过精细调整RoPE特征的插值来缩小TSTL场景中的泛化差距，显著提高了模型性能，而无需额外的在线计算成本。此外，我们提出了PosGen，这是一个新的合成基准，专门针对TSTL场景中的精细行为分析，旨在从长上下文中不断增加的令牌生成困难和识别新令牌位置的挑战中分离出来。我们在合成任务上的实验表明，在应用Resonance RoPE后，Transformer模型可以识别OOD位置。

    arXiv:2403.00071v1 Announce Type: cross  Abstract: This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences. We introduce Resonance RoPE, a novel approach designed to narrow the generalization gap in TSTL scenarios by refining the interpolation of RoPE features for OOD positions, significantly improving the model performance without additional online computational costs. Furthermore, we present PosGen, a new synthetic benchmark specifically designed for fine-grained behavior analysis in TSTL scenarios, aiming to isolate the constantly increasing difficulty of token generation on long contexts from the challenges of recognizing new token positions. Our experiments on synthetic tasks show that after applying Resonance RoPE, Transformers recognize OOD position bet
    
[^17]: PRSA：大型语言模型的提示反盗窃攻击

    PRSA: Prompt Reverse Stealing Attacks against Large Language Models

    [https://arxiv.org/abs/2402.19200](https://arxiv.org/abs/2402.19200)

    本文提出了针对商业LLMs的提示反窃取攻击框架PRSA，通过分析输入-输出对的关键特征实现攻击。

    

    提示作为重要的知识产权，使得大型语言模型（LLMs）能够执行特定任务而无需微调，突显了它们不断增长的重要性。随着基于提示的服务的崛起，如提示市场和LLM应用程序，提供者经常通过输入-输出示例展示提示的能力，以吸引用户。然而，这种范式提出了一个关键的安全问题：暴露输入-输出对是否会对潜在提示泄漏构成风险，侵犯开发者的知识产权？就我们所知，这个问题还没有得到全面探讨。为了弥补这一空白，在本文中，我们进行了首次深入探讨，并提出了一个针对商业LLMs的提示反窃取攻击框架，即PRSA。PRSA的主要思想是通过分析输入-输出对的关键特征，我们模仿并g

    arXiv:2402.19200v1 Announce Type: cross  Abstract: Prompt, recognized as crucial intellectual property, enables large language models (LLMs) to perform specific tasks without the need of fine-tuning, underscoring their escalating importance. With the rise of prompt-based services, such as prompt marketplaces and LLM applications, providers often display prompts' capabilities through input-output examples to attract users. However, this paradigm raises a pivotal security concern: does the exposure of input-output pairs pose the risk of potential prompt leakage, infringing on the intellectual property rights of the developers? To our knowledge, this problem still has not been comprehensively explored yet. To remedy this gap, in this paper, we perform the first in depth exploration and propose a novel attack framework for reverse-stealing prompts against commercial LLMs, namely PRSA. The main idea of PRSA is that by analyzing the critical features of the input-output pairs, we mimic and g
    
[^18]: VerifiNER: 通过大型语言模型基于知识驱动的推理增强的命名实体识别

    VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models

    [https://arxiv.org/abs/2402.18374](https://arxiv.org/abs/2402.18374)

    VerifiNER是一个后续验证框架，通过知识识别并修正现有命名实体识别方法的错误，以实现更忠实的预测。

    

    最近在特定领域命名实体识别（NER）中的方法，如生物医学NER，已经取得了显著进展。然而，它们仍然缺乏忠实性，会产生错误的预测。我们认为实体的知识可以在验证预测的正确性方面发挥作用。尽管知识的有用性，使用知识来纠正这些错误并不容易，因为知识本身并不直接指示出真实标签。因此，我们提出了VerifiNER，一个利用知识从现有NER方法中识别错误并将其修正为更忠实预测的后续验证框架。我们的框架利用大型语言模型的推理能力，在验证过程中充分基于知识和上下文信息。我们通过对生物医学数据集的广泛实验验证了VerifiNER的有效性。结果表明VerifiNER可以

    arXiv:2402.18374v1 Announce Type: new  Abstract: Recent approaches in domain-specific named entity recognition (NER), such as biomedical NER, have shown remarkable advances. However, they still lack of faithfulness, producing erroneous predictions. We assume that knowledge of entities can be useful in verifying the correctness of the predictions. Despite the usefulness of knowledge, resolving such errors with knowledge is nontrivial, since the knowledge itself does not directly indicate the ground-truth label. To this end, we propose VerifiNER, a post-hoc verification framework that identifies errors from existing NER methods using knowledge and revises them into more faithful predictions. Our framework leverages the reasoning abilities of large language models to adequately ground on knowledge and the contextual information in the verification process. We validate effectiveness of VerifiNER through extensive experiments on biomedical datasets. The results suggest that VerifiNER can su
    
[^19]: UniVS：使用提示作为查询的统一和通用视频分割

    UniVS: Unified and Universal Video Segmentation with Prompts as Queries

    [https://arxiv.org/abs/2402.18115](https://arxiv.org/abs/2402.18115)

    UniVS提出了使用提示作为查询的统一视频分割架构，通过平均化前一帧中目标的提示特征来解码掩码，并引入了基于目标的提示交叉注意层，从而解决了统一视频分割模型的挑战。

    

    尽管统一图像分割（IS）取得了最新进展，但开发统一的视频分割（VS）模型仍然是一项挑战。这主要是因为通用类别指定的VS任务需要检测所有对象并跟踪它们跨连续帧，而由提示引导的VS任务需要在整个视频中重新识别目标并使用视觉/文本提示，使得用相同架构处理不同任务变得困难。我们试图解决这些问题，并提出了一种新颖的统一VS架构，即UniVS，通过使用提示作为查询。UniVS将前一帧中目标的提示特征作为其初始查询平均化，以明确解码掩码，并在掩码解码器中引入基于目标的提示交叉注意层，以在内存池中整合提示特征。通过将先前帧中的实体的预测掩码作为它们的视觉提示，UniVS将不同的VS任务转换成通用的VS问题。

    arXiv:2402.18115v1 Announce Type: cross  Abstract: Despite the recent advances in unified image segmentation (IS), developing a unified video segmentation (VS) model remains a challenge. This is mainly because generic category-specified VS tasks need to detect all objects and track them across consecutive frames, while prompt-guided VS tasks require re-identifying the target with visual/text prompts throughout the entire video, making it hard to handle the different tasks with the same architecture. We make an attempt to address these issues and present a novel unified VS architecture, namely UniVS, by using prompts as queries. UniVS averages the prompt features of the target from previous frames as its initial query to explicitly decode masks, and introduces a target-wise prompt cross-attention layer in the mask decoder to integrate prompt features in the memory pool. By taking the predicted masks of entities from previous frames as their visual prompts, UniVS converts different VS ta
    
[^20]: 大型语言模型在表格数据上的应用--一项调查

    Large Language Models on Tabular Data -- A Survey

    [https://arxiv.org/abs/2402.17944](https://arxiv.org/abs/2402.17944)

    该研究综述了大型语言模型在处理表格数据上的应用，包括关键技术、指标、数据集、模型和优化方法，为未来研究方向提供了启示。

    

    大型语言模型在表格数据建模方面的应用取得了突破性进展，包括预测、表格数据合成、问答和表格理解等多种任务。每个任务都带来独特的挑战和机遇。然而，目前缺乏对该研究领域中关键技术、指标、数据集、模型和优化方法的全面审查。本调查旨在填补这一空白，总结并比较这些领域中的最新进展，提供对数据集、指标和方法论的全面调查和分类。它识别了现有文献中的优势、局限性、未开发领域和空白，同时为这一重要且快速发展的领域的未来研究方向提供了一些见解。它还提供了相关的代码和数据集引用。

    arXiv:2402.17944v1 Announce Type: new  Abstract: Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehen
    
[^21]: LLMs是否具备基于数据的统计和因果推理能力？用数据对先进的定量推理进行基准测试

    Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data

    [https://arxiv.org/abs/2402.17644](https://arxiv.org/abs/2402.17644)

    本研究引入了QRData基准测试，评估了大型语言模型在统计和因果推理方面的能力，结果显示最强模型GPT-4在该测试中准确率为58％，存在改进空间。

    

    量化推理是分析数据的关键技能，然而对这种能力的评估仍然有限。为了填补这一空白，我们引入了Quantitative Reasoning with Data（QRData）基准测试，旨在评估大型语言模型在统计和因果推理方面与现实世界数据的能力。该基准测试包括一个精心构建的包含来自教科书、在线学习材料和学术论文的数据表的411个问题的数据集。为了比较模型在数据和文本上的定量推理能力，我们还在基准测试中添加了一个包含290个仅文本问题的辅助数据集，即QRText。我们评估了自然语言推理、基于程序推理和代理推理方法，包括Chain-of-Thought、Program-of-Thoughts、ReAct和代码解释器辅助等在各种模型上的表现。最强的模型GPT-4的准确率达到了58％，但仍有很大的改进空间。

    arXiv:2402.17644v1 Announce Type: cross  Abstract: Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58%, which has a large room for improvement. Among open-source
    
[^22]: 通过从预训练对比性EEG-文本蒙版自动编码器中转移的表示增强EEG到文本解码

    Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder

    [https://arxiv.org/abs/2402.17433](https://arxiv.org/abs/2402.17433)

    通过Contrastive EEG-Text Masked Autoencoder（CET-MAE）和E2T-PTR框架，提出了一种新的模型和方法来增强基于EEG的语言解码。

    

    从无创脑电图（EEG）重建自然语言具有很大的潜力，作为脑机接口（BCI）的语言解码技术。然而，基于EEG的语言解码仍处于初级阶段，面临诸多技术问题，如：1）缺乏一个能够有效整合跨模态（EEG和文本之间）自学习与EEG特征或文本序列的模内自重构的混合策略；2）未充分利用大型语言模型（LLMs）来增强基于EEG的语言解码。为解决上述问题，我们提出了对比性EEG-文本蒙版自动编码器（CET-MAE），这是一个通过专用的多流编码器在EEG和文本之间以及内部进行复合自监督学习的新型模型。此外，我们开发了一个名为E2T-PTR（使用预训练可转移表示进行EEG到文本解码）的框架，该框架利用预训练模组

    arXiv:2402.17433v1 Announce Type: new  Abstract: Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs). However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of large language models (LLMs) to enhance EEG-based language decoding. To address above issues, we propose the Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that orchestrates compound self-supervised learning across and within EEG and text through a dedicated multi-stream encoder. Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modul
    
[^23]: 设定时间：预训练语言模型的时间对齐

    Set the Clock: Temporal Alignment of Pretrained Language Models

    [https://arxiv.org/abs/2402.16797](https://arxiv.org/abs/2402.16797)

    该研究探讨了预训练语言模型的时间混乱问题，并提出了时间对齐的方法，实验证明将LMs对齐到最近时间可以显著提高性能

    

    语言模型（LMs）在来自不同时间点的网络文本上进行训练，通常没有任何明确的时间基础。本研究调查了预训练LMs的时间混乱，并探讨了将它们的内部知识对齐到目标时间的各种方法，我们称之为“时间对齐”。为此，我们首先自动构建了一个包含20K个时态问题及其答案的数据集，涵盖从2000年到2023年的每一年。根据这个数据集，我们在实践中表明，预训练的LMs（例如LLaMa2），尽管有最近的预训练截止日期（例如2022年），大多数使用更早的知识来回答问题（例如在2019年）。然后，我们开发了几种方法，从提示到微调，来对齐LMs在回答问题时使用最新的知识，并探讨了这种对齐中的各种因素。我们的实验证明，将LLaMa2对齐到2022年可以将其性能提高高达62%

    arXiv:2402.16797v1 Announce Type: new  Abstract: Language models (LMs) are trained on web text originating from many points in time and, in general, without any explicit temporal grounding. This work investigates the temporal chaos of pretrained LMs and explores various methods to align their internal knowledge to a target time, which we call "temporal alignment." To do this, we first automatically construct a dataset containing 20K time-sensitive questions and their answers for each year from 2000 to 2023. Based on this dataset, we empirically show that pretrained LMs (e.g., LLaMa2), despite having a recent pretraining cutoff (e.g., 2022), mostly answer questions using earlier knowledge (e.g., in 2019). We then develop several methods, from prompting to finetuning, to align LMs to use their most recent knowledge when answering questions, and investigate various factors in this alignment. Our experiments show that aligning LLaMa2 to the year 2022 can boost its performance by up to 62% 
    
[^24]: 法语医用口罩语言模型中的标记化有多重要？

    How Important Is Tokenization in French Medical Masked Language Models?

    [https://arxiv.org/abs/2402.15010](https://arxiv.org/abs/2402.15010)

    子词标记化成为自然语言处理领域的主流标准，但其成功因素，如不同任务和语言的最佳分割粒度、数据源对标记工具的影响以及形态信息在印欧语言中的作用，仍不明确。

    

    近年来，基于子词的标记化已成为自然语言处理（NLP）领域中的主流标准，主要是由于预训练语言模型的广泛应用。然而，导致其成功的确切因素，如不同任务和语言的最佳分割粒度，数据源对标记工具的影响以及形态信息在印欧语言中的作用，仍然不够清楚。这在生物医学术语方面尤为重要，其特点是具有管理形态素组合的特定规则。

    arXiv:2402.15010v1 Announce Type: cross  Abstract: Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models. This shift began with Byte-Pair Encoding (BPE) and was later followed by the adoption of SentencePiece and WordPiece. While subword tokenization consistently outperforms character and word-level tokenization, the precise factors contributing to its success remain unclear. Key aspects such as the optimal segmentation granularity for diverse tasks and languages, the influence of data sources on tokenizers, and the role of morphological information in Indo-European languages remain insufficiently explored. This is particularly pertinent for biomedical terminology, characterized by specific rules governing morpheme combinations. Despite the agglutinative nature of biomedical terminology, existing language models do not explicitly incorporate 
    
[^25]: GenCeption：使用未标记的单模态数据评估多模态LLM

    GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data

    [https://arxiv.org/abs/2402.14973](https://arxiv.org/abs/2402.14973)

    提出了一种名为GenCeption的新型MLLM评估框架，可以仅利用单模态数据评估跨模态语义一致性，并有效反映模型产生幻觉的倾向，具有较强的相关性和潜力于流行的MLLM基准结果。

    

    多模态大型语言模型（MLLMs）通常使用昂贵的带标注的多模态基准进行评估。然而，这些基准通常难以跟上MLLM评估的快速发展要求。我们提出了GenCeption，这是一个新颖的无需注释的MLLM评估框架，仅需要单模态数据来评估跨模态语义一致性，并反映出模型产生幻觉的倾向。类似于流行的DrawCeption游戏，GenCeption从一个非文本样本开始，并经历一系列迭代的描述和生成步骤。迭代之间的语义漂移使用GC@T指标进行量化。我们的实证发现验证了GenCeption的有效性，并显示出与流行的MLLM基准结果的强相关性。GenCeption可以通过利用普遍存在且以前未见的单模态数据来扩展，以减轻训练数据的污染。

    arXiv:2402.14973v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models' inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption's efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.
    
[^26]: 在没有基准实况的情况下对大型语言模型进行排名

    Ranking Large Language Models without Ground Truth

    [https://arxiv.org/abs/2402.14860](https://arxiv.org/abs/2402.14860)

    不需要基准实况或参考响应的条件下，通过考虑模型的三元组来排名大型语言模型，并提出了两种排名方法。

    

    随着大型语言模型（LLMs）的普及和影响力的增强，评估和排名LLMs已成为一个重要问题。现有的评估方法要么需要获取昂贵的人类响应，要么使用LLMs成对地互相评估，这可能不够可靠。本文提供了一个新的视角，在给定一组提示数据集（比如问题、说明等）和一组LLMs的情况下，我们在没有任何基准实况或参考响应的情况下对它们进行排名。受到现实生活的启发，其中专家和有知识的人都能识别一个新手，我们的主要思路是考虑模型的三元组，其中每个模型评估其他两个模型，能够以很高的概率正确识别最差的模型。我们还分析了我们的想法并提供了成功的充分条件。通过反复应用这一想法，我们提出了两种对LLMs进行排名的方法。

    arXiv:2402.14860v1 Announce Type: cross  Abstract: Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generati
    
[^27]: 泛化奖励建模用于超出分布偏好学习

    Generalizing Reward Modeling for Out-of-Distribution Preference Learning

    [https://arxiv.org/abs/2402.14760](https://arxiv.org/abs/2402.14760)

    通过元学习方法优化通用奖励模型，以解决超出分布偏好学习问题，并提高LLMs在有限偏好反馈下的泛化能力

    

    偏好学习(PL)结合大型语言模型(LLMs)旨在使LLMs生成与人类偏好一致。以往有关从人类反馈中学习的强化学习(RLHF)的研究已在分布内的PL中取得了良好结果。然而，由于获取人类反馈的难度，为每个遇到的分布离散训练奖励模型是具有挑战性的。因此，在超出分布(OOD) PL中通过优化通用奖励模型来增强LLMs有限偏好反馈的泛化能力是实用的。本研究通过元学习方法来解决OOD PL问题。在元训练期间，利用双层优化算法来学习一个能够引导策略学习以使之与人类偏好一致的奖励模型。在遇到测试分布时，元测试过程使用学习到的奖励模型进行正则化策略优化。

    arXiv:2402.14760v1 Announce Type: cross  Abstract: Preference learning (PL) with large language models (LLMs) aims to align the LLMs' generations with human preferences. Previous work on reinforcement learning from human feedback (RLHF) has demonstrated promising results in in-distribution PL. However, due to the difficulty of obtaining human feedback, discretely training reward models for every encountered distribution is challenging. Thus, out-of-distribution (OOD) PL is practically useful for enhancing the generalization ability of LLMs with limited preference feedback. This work addresses OOD PL by optimizing a general reward model through a meta-learning approach. During meta-training, a bilevel optimization algorithm is utilized to learn a reward model capable of guiding policy learning to align with human preferences across various distributions. When encountering a test distribution, the meta-test procedure conducts regularized policy optimization using the learned reward model
    
[^28]: CounterCurate: 通过对照例子增强物理和语义视觉-语言组合推理能力

    CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples

    [https://arxiv.org/abs/2402.13254](https://arxiv.org/abs/2402.13254)

    本研究提出CounterCurate框架，通过对比例子和生成式微调，全面提升视觉-语言组合推理能力，解决了物理推理和语义对照微调方面的关键问题，实现了显著性能改进。

    

    我们提出CounterCurate，一个框架，全面提升对比和生成式多模态模型的视觉-语言组合推理能力。特别地，我们确定了两个尚未充分探讨的关键问题：忽视了基于物理的推理（计数和位置理解），以及利用高性能文本和图像生成模型进行语义反事实微调的潜力。我们的工作开创了一个解决这些空白的方法。我们首先突出了多模态模型（如CLIP和LLaVA）在基于物理的组合推理中几乎无法胜任的表现。然后，我们应用简单的数据增强，使用基于图像的生成模型GLIGEN生成微调数据，使得性能显著提高：在我们新的策划的Flickr30k-Positions基准测试中，CLIP和LLaVA的性能分别提高了+33%和+37%。此外，我们利用了高性能文本和图像生成模型的能力。

    arXiv:2402.13254v1 Announce Type: cross  Abstract: We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of hig
    
[^29]: Transformer语言适配器的隐藏空间

    The Hidden Space of Transformer Language Adapters

    [https://arxiv.org/abs/2402.13137](https://arxiv.org/abs/2402.13137)

    Transformer语言适配器在冻结的表示空间上操作，适应过程渐进分布在多层中，并且大部分预测仍在源语言中进行演变。

    

    我们分析了transformer语言适配器的操作方式，这些小模块在冻结的语言模型之上训练，以将其预测适应到新的目标语言。我们展示了适应后的预测主要在模型训练的源语言中演变，而目标语言仅在模型的最后几层中变得明显。此外，适应过程是渐进的，分布在多个层中，可以跳过少量适配器组而不降低适应性能。最后，我们发现适配器在模型的冻结表示空间上操作，同时在很大程度上保留其结构，而不是在“孤立”的子空间上。

    arXiv:2402.13137v1 Announce Type: new  Abstract: We analyze the operation of transformer language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages. We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model. Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance. Last, we show that adapters operate on top of the model's frozen representation space while largely preserving its structure, rather than on an 'isolated' subspace. Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency.
    
[^30]: 辛普森悖论与翻译中的准确性-流畅度权衡

    Simpson's Paradox and the Accuracy-Fluency Tradeoff in Translation

    [https://arxiv.org/abs/2402.12690](https://arxiv.org/abs/2402.12690)

    准确性和流畅度在翻译中表现为正相关的悖论，这是辛普森悖论的一个实例，在语料库级别上二者呈正相关，在单个源段级别上存在权衡。

    

    一篇好的翻译应该忠实于原文并遵守目标语言的规范。本文探讨了关于这些目标之间关系的理论难题。我们展示了准确性和流畅度之间的关系是辛普森悖论的一种实例，表明在语料库级别上，准确性和流畅度呈正相关但在单个源段的级别上存在权衡。

    arXiv:2402.12690v1 Announce Type: new  Abstract: A good translation should be faithful to the source and should respect the norms of the target language. We address a theoretical puzzle about the relationship between these objectives. On one hand, intuition and some prior work suggest that accuracy and fluency should trade off against each other, and that capturing every detail of the source can only be achieved at the cost of fluency. On the other hand, quality assessment researchers often suggest that accuracy and fluency are highly correlated and difficult for human raters to distinguish (Callison-Burch et al. 2007). We show that the tension between these views is an instance of Simpson's paradox, and that accuracy and fluency are positively correlated at the level of the corpus but trade off at the level of individual source segments. We further suggest that the relationship between accuracy and fluency is best evaluated at the segment (or sentence) level, and that the trade off be
    
[^31]: 文物还是绑架：LLMs如何在没有问题的情况下回答多项选择题？

    Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?

    [https://arxiv.org/abs/2402.12483](https://arxiv.org/abs/2402.12483)

    LLMs能够在没有问题的情况下仅从选项中回答多项选择题，通过记忆、选择动态和问题推理进行黑盒分析，揭示了LLMs在选择性准确性方面的三个关键发现。

    

    多项选择题回答（MCQA）通常用于评估大型语言模型（LLMs）。为了确定MCQA是否按预期评估LLMs，我们探究LLMs是否可以通过只有选项的提示来进行MCQA，其中模型必须仅从选项中选择正确答案。在三个MCQA数据集和四个LLMs中，这个提示在12个案例中的11个中优于多数基线，并可获得高达0.33的准确度提升。为了帮助解释这种行为，我们对记忆、选择动态和问题推理进行了深入的黑盒分析。我们的关键发现有三个。首先，我们发现没有证据表明只有选择的准确性仅源自记忆。其次，对单个选择的先验并不能完全解释只有选择的准确性，暗示LLMs使用选择的集体动态。第三，LLMs有一定能力从选择中推断出相关问题，并且令人惊讶地有时甚至可以匹配原始问题。我们希望鼓励利用这种方法。

    arXiv:2402.12483v1 Announce Type: new  Abstract: Multiple-choice question answering (MCQA) is often used to evaluate large language models (LLMs). To see if MCQA assesses LLMs as intended, we probe if LLMs can perform MCQA with choices-only prompts, where models must select the correct answer only from the choices. In three MCQA datasets and four LLMs, this prompt bests a majority baseline in 11/12 cases, with up to 0.33 accuracy gain. To help explain this behavior, we conduct an in-depth, black-box analysis on memorization, choice dynamics, and question inference. Our key findings are threefold. First, we find no evidence that the choices-only accuracy stems from memorization alone. Second, priors over individual choices do not fully explain choices-only accuracy, hinting that LLMs use the group dynamics of choices. Third, LLMs have some ability to infer a relevant question from choices, and surprisingly can sometimes even match the original question. We hope to motivate the use of st
    
[^32]: 从认知驱动的语言模型中得出的词序普遍规律

    Emergent Word Order Universals from Cognitively-Motivated Language Models

    [https://arxiv.org/abs/2402.12363](https://arxiv.org/abs/2402.12363)

    认知驱动的语言模型显示出可以解释许多词序普遍规律的优势

    

    世界上的语言表现出某些所谓的类型学或蕴含规律；例如，主-宾-谓（SOV）的词序通常使用后置词。解释这些偏好的来源是语言学的一个关键目标。我们通过使用具有认知偏差的语言模型（LMs）进行计算模拟研究词序普遍规律。我们的实验证明，具有类型学典型词序的语言倾向于具有由具有认知合理偏差的LMs估计的较低困惑度：句法偏差、特定的解析策略和记忆限制。这表明，这些认知偏差和可预测性（困惑度）之间的相互作用可以解释词序普遍规律的许多方面。这也展示了认知驱动LMs的优势，在计算模拟语言普遍规律时通常用于认知建模。

    arXiv:2402.12363v1 Announce Type: new  Abstract: The world's languages exhibit certain so-called typological or implicational universals; for example, Subject-Object-Verb (SOV) word order typically employs postpositions. Explaining the source of such biases is a key goal in linguistics. We study the word-order universals through a computational simulation with language models (LMs). Our experiments show that typologically typical word orders tend to have lower perplexity estimated by LMs with cognitively plausible biases: syntactic biases, specific parsing strategies, and memory limitations. This suggests that the interplay of these cognitive biases and predictability (perplexity) can explain many aspects of word-order universals. This also showcases the advantage of cognitively-motivated LMs, which are typically employed in cognitive modeling, in the computational simulation of language universals.
    
[^33]: 通过博弈论评估揭示LLM的战略推理局限性的GTBench

    GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations

    [https://arxiv.org/abs/2402.12348](https://arxiv.org/abs/2402.12348)

    该论文通过博弈论任务评估了LLMs在竞争环境中的推理能力，观察到LLMs在不同游戏场景下表现出不同行为，具有重要的战略推理局限性。

    

    随着大型语言模型（LLMs）被整合到关键的现实世界应用中，它们的战略和逻辑推理能力变得越来越关键。本文通过博弈论任务评估LLMs在竞争环境中的推理能力，例如，需要纯逻辑和战略推理来与对手竞争的棋盘游戏和纸牌游戏。我们首先提出了GTBench，这是一个以语言驱动的环境，包括10个广泛认可的任务，涵盖了全面的游戏分类法：完整信息与不完整信息，动态与静态，以及概率与确定性场景。然后，我们研究了两个关键问题：（1）表征LLMs的博弈论推理；（2）LLM对抗LLM的比赛作为推理评估。我们观察到（1）LLMs在各种游戏场景下有不同的行为；例如，LLMs在完整和确定性游戏中失败，但它们在概率游戏中具有竞争力。

    arXiv:2402.12348v1 Announce Type: cross  Abstract: As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we investigate two key problems: (1) Characterizing game-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming
    
[^34]: 通过 prior-LLM 上下文融合来理解多模态内容

    Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion

    [https://arxiv.org/abs/2402.12195](https://arxiv.org/abs/2402.12195)

    提出了一个两阶段范式"浏览和集中"，通过在将特征输入LLMs之前进行深入的多模态上下文融合，解决了多模态内容理解中的 prior-LLM 模态隔离问题

    

    随着大型语言模型（LLMs）的兴起，近期将LLMs与预训练的视觉模型相结合的多模态大型语言模型（MLLMs）已经展现出在各种视觉语言任务上令人印象深刻的性能。然而，它们在理解涉及多张图片的上下文方面仍有不足。这一缺陷的主要原因是，在将视觉特征输入LLM主干之前，每张图片的视觉特征都是由冻结的编码器单独编码的，缺乏对其他图片和多模态指令的意识。我们将这一问题称为 prior-LLM 模态隔离，并提出了一个两阶段范式，即“浏览和集中”，以实现在将特征输入LLMs之前进行深入的多模态上下文融合。这种范式最初“浏览”输入以获取关键见解，然后再次回顾输入“集中”于关键细节，通过这些见解的指导，从而实现对多模态内容更全面的理解。

    arXiv:2402.12195v1 Announce Type: new  Abstract: With the bloom of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) that incorporate LLMs with pre-trained vision models have recently demonstrated impressive performance across diverse vision-language tasks. However, they fall short to comprehend context involving multiple images. A primary reason for this shortcoming is that the visual features for each images are encoded individually by frozen encoders before feeding into the LLM backbone, lacking awareness of other images and the multimodal instructions. We term this issue as prior-LLM modality isolation and propose a two phase paradigm, browse-and-concentrate, to enable in-depth multimodal context fusion prior to feeding the features into LLMs. This paradigm initially "browses" through the inputs for essential insights, and then revisits the inputs to "concentrate" on crucial details, guided by these insights, to achieve a more comprehensive understanding of the
    
[^35]: LLM作为提示器：在任意知识图上进行低资源归纳推理

    LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs

    [https://arxiv.org/abs/2402.11804](https://arxiv.org/abs/2402.11804)

    本文利用大型语言模型（LLMs）生成图形结构提示，以增强预训练的图神经网络（GNNs），提出一种新的方法论见解，实现了在任意知识图上进行低资源归纳推理的高通用性。

    

    知识图（KG）归纳推理旨在推断训练期间未见过的新KG中缺失的事实，在各种应用中被广泛采用。KG归纳推理的一个关键挑战是处理在文本和结构方面都稀缺的低资源场景。本文尝试利用大型语言模型（LLMs）来解决这一挑战。具体来说，我们利用最先进的LLMs生成图形结构提示，以增强预训练的图神经网络（GNNs），从而为KG归纳推理方法带来新的方法论见解，以及在实践中具有很高的普适性。在方法论方面，我们引入了一种新颖的预训练和提示框架ProLINK，旨在在任意KG上进行低资源归纳推理，而无需额外训练。在实践方面，我们在36个低资源数据集上对我们的方法进行了实验评估。

    arXiv:2402.11804v1 Announce Type: new  Abstract: Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-res
    
[^36]: MARS：用于生成式LLMs中不确定性估计的意义感知响应评分

    MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs

    [https://arxiv.org/abs/2402.11756](https://arxiv.org/abs/2402.11756)

    MARS提出了一种新的评分函数MARS，考虑了生成序列中每个标记的语义贡献，该方法改进了生成式LLMs中的不确定性估计性能。

    

    生成式大型语言模型（LLMs）因在各种任务中的卓越表现而被广泛利用。然而，它们产生不准确或误导性输出的倾向可能带来潜在风险，尤其是在高风险环境中。因此，估计生成式LLM输出的正确性是增强可靠性的重要任务。生成式LLMs中的不确定性估计（UE）是一个不断发展的领域，其中SOTA基于概率的方法通常采用长度标准化评分。在这项工作中，我们提出了一种名为意义感知响应评分（MARS）的替代长度标准化评分的UE方法。MARS是一种考虑在问题的上下文中生成序列中每个标记的语义贡献的新型评分函数。我们证明将MARS整合到UE方法中会在UE性能上带来普遍和显著的改进。我们使用三种不同的闭卷式问答来进行实验

    arXiv:2402.11756v1 Announce Type: new  Abstract: Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book questi
    
[^37]: 一种支配所有的提示：LLMs 用于观点摘要评估

    One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation

    [https://arxiv.org/abs/2402.11683](https://arxiv.org/abs/2402.11683)

    通过释放涵盖观点摘要评估相关七个维度的新数据集SUMMEVAL-OP，研究人员提出了 Op-I-Prompt 作为一种独立于维度的提示方法，以及 Op-Prompts 作为一组依赖于维度的提示，可以表明 Op-I-Prompt 是评估观点摘要的一个很好的替代方案，实现了平均 Spearman 相关性为 0.70。

    

    使用传统基于参考的度量对观点摘要进行评估很少提供全面的评估，并且已经显示出与人类判断的相关性相对较低。最近的研究表明，使用大型语言模型（LLMs）作为无参考度量的NLG评估，然而，它们在观点摘要评估方面尚未被探索。此外，有限的观点摘要评估数据集阻碍了进展。为了解决这个问题，我们发布了涵盖与观点摘要评估相关的7个维度的SUMMEVAL-OP数据集：流畅性、连贯性、相关性、忠实度、方面覆盖、情感一致性和特异性。我们研究了 Op-I-Prompt，一个独立于维度的提示，以及 Op-Prompts，一个依赖于维度的用于观点摘要评估的提示集。实验证明，Op-I-Prompt 是评估观点摘要的一个很好的替代方案，实现了平均 Spearman 相关性为 0.70。

    arXiv:2402.11683v1 Announce Type: new  Abstract: Evaluation of opinion summaries using conventional reference-based metrics rarely provides a holistic evaluation and has been shown to have a relatively low correlation with human judgments. Recent studies suggest using Large Language Models (LLMs) as reference-free metrics for NLG evaluation, however, they remain unexplored for opinion summary evaluation. Moreover, limited opinion summary evaluation datasets inhibit progress. To address this, we release the SUMMEVAL-OP dataset covering 7 dimensions related to the evaluation of opinion summaries: fluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency, and specificity. We investigate Op-I-Prompt a dimension-independent prompt, and Op-Prompts, a dimension-dependent set of prompts for opinion summary evaluation. Experiments indicate that Op-I-Prompt emerges as a good alternative for evaluating opinion summaries achieving an average Spearman correlation of 0.70 w
    
[^38]: k-SemStamp：一种基于聚类的语义水印用于检测机器生成的文本

    k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text

    [https://arxiv.org/abs/2402.11399](https://arxiv.org/abs/2402.11399)

    k-SemStamp是一种简单而有效的语义水印方案，通过利用k均值聚类代替LSH来提高鲁棒性和抽样效率，同时保持生成质量，为机器生成文本检测提供了更有效的工具。

    

    最近的水印生成算法在语言生成过程中注入可检测的签名，以便进行事后检测。虽然基于标记级别的水印容易受到改写攻击，但SemStamp (Hou等人，2023)在句子的语义表示上应用水印，并展示出很好的鲁棒性。SemStamp利用局部敏感哈希（LSH）来利用任意超平面对语义空间进行分区，导致在鲁棒性和速度之间存在次优的权衡。我们提出k-SemStamp，这是SemStamp的一个简单而有效的增强版，利用k均值聚类作为局部敏感哈希的替代方案，以了解内在的语义结构来分区嵌入空间。实验结果表明，k-SemStamp显著提高了其鲁棒性和抽样效率，同时保持生成质量，推进了更有效的机器生成文本检测工具。

    arXiv:2402.11399v1 Announce Type: new  Abstract: Recent watermarked generation algorithms inject detectable signatures during language generation to facilitate post-hoc detection. While token-level watermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023) applies watermark on the semantic representation of sentences and demonstrates promising robustness. SemStamp employs locality-sensitive hashing (LSH) to partition the semantic space with arbitrary hyperplanes, which results in a suboptimal tradeoff between robustness and speed. We propose k-SemStamp, a simple yet effective enhancement of SemStamp, utilizing k-means clustering as an alternative of LSH to partition the embedding space with awareness of inherent semantic structure. Experimental results indicate that k-SemStamp saliently improves its robustness and sampling efficiency while preserving the generation quality, advancing a more effective tool for machine-generated text detection.
    
[^39]: 在不修改语言模型的情况下训练语言模型代理

    Training Language Model Agents without Modifying Language Models

    [https://arxiv.org/abs/2402.11359](https://arxiv.org/abs/2402.11359)

    提出一种新的方法，在不修改语言模型的情况下训练语言模型代理，通过进化代理的功能来解决下游任务

    

    研究人员和实践者最近已经将强大的大型语言模型（LLMs）重新定义为代理，使它们能够通过使用专门的功能自动化地完成复杂任务。为了促进LLM代理的发展，我们提出了一种在不修改LLM权重的情况下训练LLM代理的新范式，当LLM难以或无法进行修改时尤其有用。受到人类不断锻造工具以适应现实任务的启发，而不是改变我们的生物结构以适应一组静态工具，我们提出逐步锻造代理的功能，以更好地解决下游任务，而不是修改LLM权重。通过将这些功能视为可学习的“代理参数”并利用人工智能模型训练的基本思想，我们开发了AgentOptimizer，利用LLM更新代理的功能，并设计了一种代理训练算法

    arXiv:2402.11359v1 Announce Type: new  Abstract: Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent's functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters' and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents' functions and devise an agent training algorithm with tw
    
[^40]: 当大型语言模型遇到狡猾问题：用于大型语言模型的谬误理解基准

    When LLMs Meet Cunning Questions: A Fallacy Understanding Benchmark for Large Language Models

    [https://arxiv.org/abs/2402.11100](https://arxiv.org/abs/2402.11100)

    该论文提出了一个谬误理解基准FLUB，挑战大型语言模型在推理和理解能力上，重点是通过设计狡猾问题评估LLMs的谬误理解能力。

    

    最近，大型语言模型(LLMs)在语言理解和生成方面取得了显著进展。本文通过提出一个名为FaLlacy Understanding Benchmark (FLUB)的基准来挑战LLMs的推理和理解能力，其中包含易于人类理解但难于模型把握的狡猾问题。具体来说，FLUB专注于从真实互联网环境中收集到的棘手、幽默和误导性问题。我们设计了三个难度递增的任务，用于评估LLMs的谬误理解能力。基于FLUB，我们研究了多个代表性的和先进的LLMs的表现，表明我们的FLUB是具有挑战性的并值得未来进一步研究的。

    arXiv:2402.11100v1 Announce Type: new  Abstract: Recently, Large Language Models (LLMs) have made remarkable evolutions in language understanding and generation. Following this, various benchmarks for measuring all kinds of capabilities of LLMs have sprung up. In this paper, we challenge the reasoning and understanding abilities of LLMs by proposing a FaLlacy Understanding Benchmark (FLUB) containing cunning questions that are easy for humans to understand but difficult for models to grasp. Specifically, the cunning questions that FLUB focuses on mainly consist of the tricky, humorous, and misleading questions collected from the real internet environment. And we design three tasks with increasing difficulty in the FLUB benchmark to evaluate the fallacy understanding ability of LLMs. Based on FLUB, we investigate the performance of multiple representative and advanced LLMs, reflecting our FLUB is challenging and worthy of more future study. Interesting discoveries and valuable insights 
    
[^41]: 通过辩论调节LLMs以生成可控的具有争议性的声明

    Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements

    [https://arxiv.org/abs/2402.10614](https://arxiv.org/abs/2402.10614)

    本文通过辩论调节LLMs，使其生成可控的支持用户定义论点的声明，改进了LLMs的可控性，并提出了DEBATunE流程。通过两个LLMs之间的多轮辩论生成高质量的训练数据，以支持生成有更高质量和更突出的声明。

    

    LLMs代表不同的人群，尤其是少数群体，并产生支持其多样化甚至有争议观点的声明对于创造一个包容的环境至关重要。然而，现有的LLMs缺乏足够的控制性来支持生成内容的立场，其中往往包含不一致、中立或有偏见的声明。在本文中，我们改进了LLMs在生成支持用户在提示中定义的论点的声明时的可控性。我们发现两个持有相反立场的LLMs之间的多轮辩论产生了更高质量和更突出的声明，这些声明对于改善LLMs的可控性是重要的训练数据。受此启发，我们开发了一种新颖的Debate & Tuning（“DEBATunE”）流程，通过微调LLMs生成通过辩论获得的声明。为了检验DEBATunE，我们整理了迄今为止涵盖710个争议性主题的最大数据集。

    arXiv:2402.10614v1 Announce Type: cross  Abstract: Making LLMs speak for different, especially minority groups of people, and generate statements supporting their diverse or even controversial perspectives is critical to creating an inclusive environment. However, existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements. In this paper, we improve the controllability of LLMs in generating statements supporting an argument the user defined in the prompt. We find that multi-round debates between two LLMs with opposite stances generate higher-quality and more salient statements for each, which are important training data to improve the controllability of LLMs. Motivated by this, we develop a novel debate & tuning ("DEBATunE") pipeline finetuning LLMs to generate the statements obtained via debate. To examine DEBATunE, we curate the largest dataset of debate topics so far, which covers 710 contro
    
[^42]: BioMistral：面向医学领域的开源预训练大型语言模型集合

    BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains

    [https://arxiv.org/abs/2402.10373](https://arxiv.org/abs/2402.10373)

    BioMistral是一种面向生物医学领域的开源预训练大型语言模型集合，在医学问答任务中表现出优越性能并具有竞争优势。

    

    大型语言模型（LLMs）近年来展示出卓越的多功能性，为医疗保健和医学等专业领域提供潜在应用。尽管有各种针对健康领域定制的开源LLMs可用，但将通用LLMs调整到医学领域仍面临重大挑战。本文介绍了BioMistral，一种专为生物医学领域量身定制的开源LLM，采用Mistral作为基础模型，并在PubMed Central上进一步进行预训练。我们在包含10个已建立的英文医学问答（QA）任务的基准上对BioMistral进行了全面评估。我们还探讨通过量化和模型合并方法获得的轻量级模型。我们的结果表明，BioMistral相较于现有开源医学模型具有优越性能，并与专有对手具有竞争优势。最后，为了解决

    arXiv:2402.10373v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address
    
[^43]: 选择性反射调节：LLM指令调节的学生选择数据回收

    Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning

    [https://arxiv.org/abs/2402.10110](https://arxiv.org/abs/2402.10110)

    本文介绍了一种名为选择性反射调节的新方法，该方法通过教师LLM的反射和自省与学生LLM的数据选择能力相结合，自动优化现有的指令调节数据，从而实现了高效的指令调节和卓越性能的LLM。

    

    指令调节对于大型语言模型（LLM）来说非常关键，以实现更好的指令跟踪和任务适应能力，但其成功在很大程度上取决于训练数据的质量。许多最近的方法都致力于改进数据质量，但往往忽视了数据与正在微调的学生模型的兼容性。本文介绍了一种新的范式——选择性反射调节，通过结合教师LLM的反射和自省，以自动优化现有的指令调节数据。这种师生合作产生了高质量且与学生LLM兼容的指令响应对，从而实现了高效的指令调节和卓越性能的LLM。选择性反射调节是一种数据增强和合成方法，通常能改善LLM微调和自我优化，而无需额外的计算资源。

    arXiv:2402.10110v1 Announce Type: cross  Abstract: Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality. Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned. This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data. This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance. Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without co
    
[^44]: 文本解毒作为英语和印地语中的风格转移

    Text Detoxification as Style Transfer in English and Hindi

    [https://arxiv.org/abs/2402.07767](https://arxiv.org/abs/2402.07767)

    本文研究了文本解毒化的任务，旨在将有毒文本自动转化为无毒文本。通过知识转移、多任务学习和删除重建方法，我们提出了三种解决方案。我们利用Dementieva等人提供的数据集进行实验，并引入了一个小型的印地语平行数据集用于评估。

    

    本文关注文本解毒，即自动将有毒文本转化为非有毒文本。这项任务有助于更安全、更尊重的在线交流，并可被视为文本风格转移（TST）任务，在此任务中，文本风格发生变化，但内容保持不变。我们提出了三种方法：从类似任务中进行知识转移，多任务学习方法，将序列到序列建模与各种毒性分类任务相结合，并采用删除和重建方法。为支持我们的研究，我们利用了Dementieva等人提供的数据集（2021年），该数据集包含与有毒文本对应的多个版本的解毒文本。在我们的实验中，我们通过专家人工注释员选择了最佳的变体，创建了一个数据集，其中每个有毒句子与一个适当的解毒版本配对。此外，我们还引入了一个小型的印地语平行数据集，与英语数据集的一部分对齐，适用于评估目的。

    This paper focuses on text detoxification, i.e., automatically converting toxic text into non-toxic text. This task contributes to safer and more respectful online communication and can be considered a Text Style Transfer (TST) task, where the text style changes while its content is preserved. We present three approaches: knowledge transfer from a similar task, multi-task learning approach, combining sequence-to-sequence modeling with various toxicity classification tasks, and, delete and reconstruct approach. To support our research, we utilize a dataset provided by Dementieva et al.(2021), which contains multiple versions of detoxified texts corresponding to toxic texts. In our experiments, we selected the best variants through expert human annotators, creating a dataset where each toxic sentence is paired with a single, appropriate detoxified version. Additionally, we introduced a small Hindi parallel dataset, aligning with a part of the English dataset, suitable for evaluation purp
    
[^45]: 进取的鲍勃通过提示对抗调整抵制越狱行为

    Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning

    [https://arxiv.org/abs/2402.06255](https://arxiv.org/abs/2402.06255)

    本文提出了一种名为Prompt Adversarial Tuning (PAT)的方法，通过训练一个防御控制机制并将其作为前缀嵌入到用户提示中，实现对大型语言模型（LLMs）的越狱行为的防御。实验证明该方法在保护LLMs免受产生有害信息的影响方面效果显著。

    

    尽管大型语言模型（LLM）在各种应用中取得了巨大的成功，但它们也容易受到特定提示的影响，从而绕过内置的安全措施并提供危险或非法内容，这种现象被称为越狱行为。为了保护LLMs免受产生有害信息的影响，提出了各种防御策略，其中大多数集中在内容过滤或模型的对抗训练方面。在本文中，我们提出了一种名为Prompt Adversarial Tuning（PAT）的方法，通过训练一个防御控制机制并将其作为前缀嵌入到用户提示中来实现我们的防御策略。我们设计了一个类似对抗训练的训练过程，以实现我们的优化目标，交替更新攻击和防御控制机制。据我们所知，我们是第一个从提示调整的角度实施防御的人。一旦应用，我们的方法几乎不会影响LLMs的操作效率。实验表明我们的方法在抵御越狱行为方面具有良好的效果。

    Although Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak. To protect LLMs from producing harmful information, various defense strategies are proposed, with most focusing on content filtering or adversarial training of models. In this paper, we propose an approach named Prompt Adversarial Tuning (PAT) to train a defense control mechanism, which is then embedded as a prefix to user prompts to implement our defense strategy. We design a training process similar to adversarial training to achieve our optimized goal, alternating between updating attack and defense controls. To our knowledge, we are the first to implement defense from the perspective of prompt tuning. Once employed, our method will hardly impact the operational efficiency of LLMs. Experiments show that our method i
    
[^46]: 通过基于垄断对话的社交场景模拟实现大型语言模型的自对齐

    Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation

    [https://arxiv.org/abs/2402.05699](https://arxiv.org/abs/2402.05699)

    本文提出了一个通过社交场景模拟来自对齐大型语言模型的方法，以减轻其被滥用造成的潜在不良影响。通过一个名为MATRIX的虚拟排练空间，LLM可以在回答查询前考虑社交后果，并通过MATRIX-simulated数据的微调，保持对人类价值的遵从和推理速度的平衡。实验证明，在温和假设下，带有MATRIX的LLM胜过了宪法AI。

    

    将大型语言模型(LLMs)与人类价值对齐，以减轻其被滥用造成的潜在不良影响，具有重要意义。本文借鉴社会学的见解，即认识到所有各方的关切是塑造人类价值观的关键因素，提出了一种自对齐LLMs的新方向：社交场景模拟。为此，我们提出了一个名为MATRIX的创新社交场景模拟器，它可以模拟用户输入查询周围的现实场景，使LLM在回答前能够考虑社交后果。MATRIX类似于一个“垄断对话”下的虚拟排练空间，LLM在其中扮演与查询相关的多个角色并进行自我实践。为了引入这种对齐能力，我们使用MATRIX模拟数据对LLM进行微调，确保其在不影响推理速度的情况下符合人类价值观。理论上，我们证明了在温和假设下，带有MATRIX的LLM胜过了宪法AI。最后，大量实验证实了我们的方法在多个任务上都取得了最佳性能。

    Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms Constitutional AI under mild assumptions. Finally, extensive experiments validate that ou
    
[^47]: AttnLRP: 注意力感知的逐层相关传递用于Transformer

    AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers

    [https://arxiv.org/abs/2402.05602](https://arxiv.org/abs/2402.05602)

    AttnLRP是首个能够忠实且全面地归因Transformer模型的输入和潜在表示，并具有与单一反向传播相似的计算效率的方法。它通过扩展逐层相关传递归因方法以处理注意力层来解决了黑盒Transformer模型的归因问题，具有超越现有方法的准确性和理解潜在表示的能力。

    

    大型语言模型容易产生偏见的预测和幻象，这突显了理解其模型内部推理过程的重要性。然而，实现对整个黑盒Transformer模型的准确归因并保持计算效率是一个尚未解决的挑战。通过扩展逐层相关传递归因方法以处理注意力层，我们有效地解决了这些挑战。虽然存在部分解决方案，但我们的方法是首个能够忠实且全面地归因Transformer模型的输入和潜在表示，同时计算效率与单一反向传播相似。通过对Llama 2、Flan-T5和Vision Transformer架构上与现有方法的广泛评估，我们证明了我们提出的方法在准确性方面超过了其他方法，并能够理解潜在表示，为概念打开了大门。

    Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a singular backward pass. Through extensive evaluations against existing methods on Llama 2, Flan-T5 and the Vision Transformer architecture, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concep
    
[^48]: 在交流医疗辅导上对大型语言模型进行基准测试：一种新的系统和数据集

    Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset

    [https://arxiv.org/abs/2402.05547](https://arxiv.org/abs/2402.05547)

    本研究介绍了“ChatCoach”，一个集成人工智能与人类医生合作的框架，在交流医疗辅导中利用大型语言模型，提供模拟环境和实时反馈，以帮助医学学员提高沟通技巧。

    

    在医疗保健领域，自然语言处理（NLP）的传统应用主要集中在以患者为中心的服务上，增强患者互动和护理交付，例如医学对话系统。然而，NLP在帮助经验不丰富的医生，特别是在交流医疗辅导等领域的潜力仍然很少被探索。我们引入了“ChatCoach”，一个集成的人工智能合作框架。在这个框架内，一个患者代理和一个辅导代理共同支持医学学员在会诊过程中练习医学沟通技巧。与传统的对话系统不同，ChatCoach提供了一个模拟环境，医生可以在其中与患者代理进行医学对话。同时，辅导代理会提供实时反馈给医生。为了构建ChatCoach系统，我们开发了一个数据集，并集成了ChatGPT和Llama2等大型语言模型，旨在评估它们在交流医疗辅导方面的效果。

    Traditional applications of natural language processing (NLP) in healthcare have predominantly focused on patient-centered services, enhancing patient interactions and care delivery, such as through medical dialogue systems. However, the potential of NLP to benefit inexperienced doctors, particularly in areas such as communicative medical coaching, remains largely unexplored. We introduce ``ChatCoach,'' an integrated human-AI cooperative framework. Within this framework, both a patient agent and a coaching agent collaboratively support medical learners in practicing their medical communication skills during consultations. Unlike traditional dialogue systems, ChatCoach provides a simulated environment where a human doctor can engage in medical dialogue with a patient agent. Simultaneously, a coaching agent provides real-time feedback to the doctor. To construct the ChatCoach system, we developed a dataset and integrated Large Language Models such as ChatGPT and Llama2, aiming to assess 
    
[^49]: 关于可证明的长度和组合泛化

    On Provable Length and Compositional Generalization

    [https://arxiv.org/abs/2402.04875](https://arxiv.org/abs/2402.04875)

    本研究针对包括深度集合、变压器、状态空间模型和简单递归神经网络等多种架构，探索了可证明的长度和组合泛化，认为对于长度和组合泛化，不同架构需要不同程度的表示识别。

    

    长度泛化——对训练时未见到的更长序列的泛化能力，以及组合泛化——对训练时未见到的令牌组合的泛化能力，在序列到序列模型中是重要的非分布化泛化形式。在这项工作中，我们在包括深度集合、变压器、状态空间模型和简单递归神经网络在内的一系列架构中，朝着可证明的长度和组合泛化迈出了第一步。根据架构的不同，我们证明了不同程度的表示识别的必要性，例如与真实表示具有线性或排列关系。

    Length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models. In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets. Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.
    
[^50]: "少即是多：一种针对大型语言模型的通用简单非参数剪枝算法"

    Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models

    [https://arxiv.org/abs/2402.03142](https://arxiv.org/abs/2402.03142)

    本文提出了一种简单、通用、非参数的剪枝算法KEN，它能在保持模型性能的同时大幅节省内存，通过选择性地保留最重要的参数实现了对transformer模型的优化。与其他方法相比，KEN在最少参数减少25%的情况下实现了与原始模型相等或更好的性能。

    

    神经网络剪枝由于神经网络模型的复杂性以及在各个领域的广泛应用而变得越来越重要。现有的剪枝算法通常存在架构特异性、过度复杂和依赖复杂计算等限制，使它们在实际应用中变得不可行。本文提出了基于核密度估计（KDE）的简单、通用、非结构化剪枝算法KEN。KEN的目标是通过有选择性地保留最重要的参数，同时将其他参数恢复到预训练状态，从而构建优化后的transformer模型。这种方法在保持模型性能的同时，只存储优化后的子网络，实现了显著的内存节省。对七个transformer模型进行了广泛的评估，结果表明KEN在最少参数减少25%的情况下实现了与原始模型相等或更好的性能。与其他方法进行了深入对比。

    Neural network pruning has become increasingly crucial due to the complexity of neural network models and their widespread use in various fields. Existing pruning algorithms often suffer from limitations such as architecture specificity, excessive complexity and reliance on complex calculations, rendering them impractical for real-world applications. In this paper, we propose KEN: a straightforward, universal and unstructured pruning algorithm based on Kernel Density Estimation (KDE). KEN aims to construct optimized transformer models by selectively preserving the most significant parameters while restoring others to their pre-training state. This approach maintains model performance while allowing storage of only the optimized subnetwork, leading to significant memory savings. Extensive evaluations on seven transformer models demonstrate that KEN achieves equal or better performance than the original models with a minimum parameter reduction of 25%. In-depth comparisons against other 
    
[^51]: BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback

    BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback

    [https://arxiv.org/abs/2402.02479](https://arxiv.org/abs/2402.02479)

    BRAIn是一种基于贝叶斯奖励条件化缩减推断的自然语言生成方法，通过反馈来改进RLHF，在LLM对齐中表现出较好的可扩展性和性能。

    

    在人类反馈的强化学习领域，继Proximal Policy Optimization (PPO)取得成功之后，提出了一种新的方法，如Sequence Likelihood Calibration (SLiC)和Direct Policy Optimization (DPO)，这些方法是离线的，并且以间接的方式使用奖励。这些技术，特别是DPO，由于其可扩展性和性能，最近已经成为LLM对齐的首选工具。然而，它们遗漏了PPO方法的重要特征。诸如SLiC或RRHF的方法仅利用奖励模型(RM)进行排序/偏好，丢失了细粒度信息，忽略了RM的参数形式(例如Bradley-Terry、Plackett-Luce)；而诸如DPO的方法甚至不使用单独的奖励模型。在这项工作中，我们提出了一种新颖的方法，命名为BRAIn，它将RM作为分布匹配方法的一部分重新引入。BRAIn考虑到了LLM分布在假设输出质量良好的条件下，并应用B...

    Following the success of Proximal Policy Optimization (PPO) for Reinforcement Learning from Human Feedback (RLHF), new techniques such as Sequence Likelihood Calibration (SLiC) and Direct Policy Optimization (DPO) have been proposed that are offline in nature and use rewards in an indirect manner. These techniques, in particular DPO, have recently become the tools of choice for LLM alignment due to their scalability and performance. However, they leave behind important features of the PPO approach. Methods such as SLiC or RRHF make use of the Reward Model (RM) only for ranking/preference, losing fine-grained information and ignoring the parametric form of the RM (eg., Bradley-Terry, Plackett-Luce), while methods such as DPO do not use even a separate reward model. In this work, we propose a novel approach, named BRAIn, that re-introduces the RM as part of a distribution matching approach.BRAIn considers the LLM distribution conditioned on the assumption of output goodness and applies B
    
[^52]: DeLLMa:一个用于大型语言模型下决策的框架

    DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models

    [https://arxiv.org/abs/2402.02392](https://arxiv.org/abs/2402.02392)

    DeLLMa是一个旨在提高不确定环境下决策精度的框架，通过多步骤的脚手架程序，借鉴决策理论和效用理论的原则，可以显著提高大型语言模型的决策性能。

    

    大型语言模型（LLMs）在商业、工程和医学等领域被广泛应用，这些领域往往面临决策不确定性的问题，这是一个关键但具有挑战性的任务。本文表明，在决策问题上直接使用LLMs往往效果较差，尤其是在问题复杂性增加时。为了克服这个限制，我们提出了DeLLMa（Decision-making Large Language Model assistant）框架，旨在提高不确定环境下的决策精度。DeLLMa包括一个多步骤的脚手架程序，借鉴了决策理论和效用理论的原则，提供了一个最优的、可审计的决策过程。我们在涉及真实农业和金融数据的决策环境中验证了我们的框架。结果表明，DeLLMa可以显著提高LLMs的决策性能，准确性可提高高达40%以上。

    Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over co
    
[^53]: OLMo: 加速语言模型科学

    OLMo: Accelerating the Science of Language Models

    [https://arxiv.org/abs/2402.00838](https://arxiv.org/abs/2402.00838)

    OLMo是一种真正开放的语言模型，旨在提供给研究社区强大的工具，以促进对语言模型科学的研究和创新。

    

    语言模型（LM）已广泛应用于自然语言处理研究和商业产品。随着商业重要性的增加，最强大的模型已经封闭起来，只能通过专有接口访问，其训练数据、架构和开发细节没有透露。考虑到这些细节对于科学研究这些模型的重要性，包括其偏见和潜在风险，我们认为研究社区有权访问强大而真正开放的LM。为此，本技术报告详细介绍了OLMo的首个版本，这是一种最先进、真正开放的语言模型，以及构建和研究语言建模科学的框架。与之前只发布模型权重和推理代码的努力不同，我们发布OLMo和整个框架，包括训练数据、训练和评估代码。我们希望这个发布能增强开放研究社区的能力，并激发更多的创新。

    Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspi
    
[^54]: 超级过滤：用于快速指令调整的弱到强数据过滤

    Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning

    [https://arxiv.org/abs/2402.00530](https://arxiv.org/abs/2402.00530)

    提出了一种名为“超级过滤”的方法，能够使用较小和较弱的模型对用于训练较大和较强的语言模型的指令数据进行过滤，从而降低了过滤成本，并在标准基准测试中获得更好的性能。

    

    指令调整对于改进LLM至关重要，但通常会遇到低质量和冗余数据的问题。指令调整的数据过滤在提高调整过程的效率和性能方面已被证明很重要。但是，由于LLMs在该过程中的参与，这也导致了额外的成本和计算。为了减少过滤成本，我们研究了超级过滤：可以使用较小且较弱的模型来选择要调整更大和更强模型的数据吗？尽管弱和强语言模型之间存在性能差距，但我们发现它们高度一致的能力可以感知指令的难度和数据选择结果。这使得我们可以使用一个更小更高效的模型来过滤用于训练更大语言模型的指令数据。它不仅大大加快了数据过滤的速度，而且经过过滤数据微调的LLM在标准基准测试中取得了更好的性能。大量实验证实了其有效性和效率。

    Instruction tuning is critical to improve LLMs but usually suffers from low-quality and redundant data. Data filtering for instruction tuning has proved important in improving both the efficiency and performance of the tuning process. But it also leads to extra cost and computation due to the involvement of LLMs in this process. To reduce the filtering cost, we study Superfiltering: Can we use a smaller and weaker model to select data for finetuning a larger and stronger model? Despite the performance gap between weak and strong language models, we find their highly consistent capability to perceive instruction difficulty and data selection results. This enables us to use a much smaller and more efficient model to filter the instruction data used to train a larger language model. Not only does it largely speed up the data filtering, but the filtered-data-finetuned LLM achieves even better performance on standard benchmarks. Extensive experiments validate the efficacy and efficiency of 
    
[^55]: BPDec: 揭示BERT预训练中掩码语言建模解码器的潜力

    BPDec: Unveiling the Potential of Masked Language Modeling Decoder in BERT pretraining

    [https://arxiv.org/abs/2401.15861](https://arxiv.org/abs/2401.15861)

    本文揭示了BPDec（BERT预训练解码器）的潜力，强调增强的掩码语言建模解码器设计及研究在BERT预训练中的重要性。

    

    BERT（来自Transformer的双向编码表示）通过其在许多任务上出色的性能彻底改变了自然语言处理领域。然而，大多数研究人员主要集中在与模型结构相关的增强，例如相对位置嵌入和更有效的注意机制。还有一些人深入研究了与掩码语言建模相关的预训练技巧，包括整词掩码。DeBERTa引入了一种针对BERT编码器模型进行预训练的增强解码器，证明效果非常显著。我们认为围绕增强掩码语言建模解码器的设计和研究并未得到应有的重视。在本文中，我们提出了几种增强解码器的设计，并介绍了BPDec（BERT预训练解码器），这是一种用于建模训练的新方法。通常，预训练的BERT模型会针对特定的自然语

    arXiv:2401.15861v2 Announce Type: replace-cross  Abstract: BERT (Bidirectional Encoder Representations from Transformers) has revolutionized the field of natural language processing through its exceptional performance on numerous tasks. Yet, the majority of researchers have mainly concentrated on enhancements related to the model structure, such as relative position embedding and more efficient attention mechanisms. Others have delved into pretraining tricks associated with Masked Language Modeling, including whole word masking. DeBERTa introduced an enhanced decoder adapted for BERT's encoder model for pretraining, proving to be highly effective. We argue that the design and research around enhanced masked language modeling decoders have been underappreciated. In this paper, we propose several designs of enhanced decoders and introduce BPDec (BERT Pretraining Decoder), a novel method for modeling training. Typically, a pretrained BERT model is fine-tuned for specific Natural Language 
    
[^56]: PRewrite: 使用强化学习的提示重写

    PRewrite: Prompt Rewriting with Reinforcement Learning

    [https://arxiv.org/abs/2401.08189](https://arxiv.org/abs/2401.08189)

    本文提出了一种基于强化学习的自动化工具PRewrite，用于重写提示草案并生成高效的新提示，以解决提示工程中的挑战。

    

    arXiv:2401.08189v2 公告类型: 替换 摘要: 提示工程对于基于LLM的应用程序的开发至关重要。然而，通常以“试错”的方式手动完成。这种手动程序可能耗时，效果不佳，并且在许多情况下生成的提示都是次优的。即使对那些看似运作良好的提示，始终存在一个悬而未决的问题：是否可以通过进一步修改使提示变得更好呢？为了解决这些问题，在本文中，我们研究了提示工程自动化。我们考虑了一个特定的使用情景，即开发者/用户已经起草了初始提示，但缺乏时间/专业知识来优化它们。我们提出了PRewrite，一个自动化工具，可重写这些草案，并生成高效的新提示。PRewrite基于强化学习（RL）框架，允许端到端优化，我们的设计允许RL搜索在大动作空间中进行。

    arXiv:2401.08189v2 Announce Type: replace  Abstract: Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a "trial and error" fashion. This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   To address these questions, in this paper, we investigate prompt engineering automation. We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them. We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts. PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space. The automated to
    
[^57]: 规模化模型编辑会导致渐进性和突发性遗忘

    Model Editing at Scale leads to Gradual and Catastrophic Forgetting

    [https://arxiv.org/abs/2401.07453](https://arxiv.org/abs/2401.07453)

    评估了当前模型编辑方法在规模化情况下的表现，发现随着模型被顺序编辑多个事实，它会逐渐遗忘先前的事实及执行下游任务的能力。

    

    在大型语言模型中编辑知识是一种具有吸引力的能力，它使我们能够在预训练期间纠正错误学习的事实，同时使用不断增长的新事实列表更新模型。我们认为，为了使模型编辑具有实际效用，我们必须能够对同一模型进行多次编辑。因此，我们评估了当前规模下的模型编辑方法，重点关注两种最先进的方法：ROME 和 MEMIT。我们发现，随着模型被顺序编辑多个事实，它不断地遗忘先前编辑过的事实以及执行下游任务的能力。这种遗忘分为两个阶段--初始的渐进性遗忘阶段，随后是突然或灾难性的遗忘。

    arXiv:2401.07453v2 Announce Type: replace-cross  Abstract: Editing knowledge in large language models is an attractive capability to have which allows us to correct incorrectly learnt facts during pre-training, as well as update the model with an ever-growing list of new facts. While existing model editing techniques have shown promise, they are usually evaluated using metrics for reliability, specificity and generalization over one or few edits. We argue that for model editing to have practical utility, we must be able to make multiple edits to the same model. With this in mind, we evaluate the current model editing methods at scale, focusing on two state of the art methods: ROME and MEMIT. We find that as the model is edited sequentially with multiple facts, it continually forgets previously edited facts and the ability to perform downstream tasks. This forgetting happens in two phases -- an initial gradual but progressive forgetting phase followed by abrupt or catastrophic forgettin
    
[^58]: 语言模型在某种程度上理解数字

    Language Models Understand Numbers, at Least Partially

    [https://arxiv.org/abs/2401.03735](https://arxiv.org/abs/2401.03735)

    本研究表明，大型语言模型在某种程度上理解数字，可以通过压缩和编码的方式执行算术计算。

    

    大型语言模型(LLMs)在各种任务中展现出令人印象深刻的能力，但其不透明的内部机制限制了它们在数学问题中的应用。在本文中，我们研究了一个基本问题：语言模型是否理解数字，数学中的基本元素。基于一个假设，即LLMs应该能够在其隐藏状态中压缩数字以解决数学问题，我们构建了一个合成数据集，包括加法问题，并利用线性探测器从隐藏状态中读取输入数字。实验结果支持LLMs中存在压缩的数字。然而，精确重建原始数字是困难的，表明压缩过程可能不是无损的。进一步的实验证明，LLMs可以利用编码的数字来执行算术计算，并且计算能力随模型大小的增加而扩展。我们的初步研究表明，LLMs在数字上展现出部分理解。

    Large language models (LLMs) have exhibited impressive competence in various tasks, but their opaque internal mechanisms hinder their use in mathematical problems. In this paper, we study a fundamental question: whether language models understand numbers, a basic element in math. Based on an assumption that LLMs should be capable of compressing numbers in their hidden states to solve mathematical problems, we construct a synthetic dataset comprising addition problems and utilize linear probes to read out input numbers from the hidden states. Experimental results support the existence of compressed numbers in LLMs. However, it is difficult to precisely reconstruct the original numbers, indicating that the compression process may not be lossless. Further experiments show that LLMs can utilize encoded numbers to perform arithmetic computations, and the computational ability scales up with the model size. Our preliminary research suggests that LLMs exhibit a partial understanding of number
    
[^59]: BloomVQA：评估分层多模态理解

    BloomVQA: Assessing Hierarchical Multi-modal Comprehension

    [https://arxiv.org/abs/2312.12716](https://arxiv.org/abs/2312.12716)

    提出了新VQA数据集BloomVQA，基于Bloom的分类法，通过层次图表示实现数据增强和模型一致性评估，揭示大型视觉语言模型在高级理解任务上的性能下降。

    

    我们提出了一个新颖的VQA数据集BloomVQA，旨在促进对大型视觉语言模型在理解任务上的全面评估。与当前的基准不同，它们通常侧重于基于事实的记忆和没有理论基础的简单推理任务，我们收集了基于图片故事的多项选择样本，反映了不同层次的理解，正如布鲁姆的分类法所展示的，在教育研究中被广泛采用的经典框架。我们的数据映射到一种新颖的分层图表示，实现了自动数据增强和表征模型一致性的新措施。我们对最近的多模态模型进行了分级评估和可靠性分析。与低级任务相比，我们发现在需要高级理解和认知能力的任务上表现下降，VQA准确性下降了高达38.0%。与早期模型相比，GPT-4V表现出...

    arXiv:2312.12716v2 Announce Type: replace-cross  Abstract: We propose a novel VQA dataset, BloomVQA, to facilitate comprehensive evaluation of large vision-language models on comprehension tasks. Unlike current benchmarks that often focus on fact-based memorization and simple reasoning tasks without theoretical grounding, we collect multiple-choice samples based on picture stories that reflect different levels of comprehension, as laid out in Bloom's Taxonomy, a classic framework for learning assessment widely adopted in education research. Our data maps to a novel hierarchical graph representation which enables automatic data augmentation and novel measures characterizing model consistency. We perform graded evaluation and reliability analysis on recent multi-modal models. In comparison to low-level tasks, we observe decreased performance on tasks requiring advanced comprehension and cognitive skills with up to 38.0% drop in VQA accuracy. In comparison to earlier models, GPT-4V demons
    
[^60]: 错误并不容易：大型语言模型在排除推理过程中遇到困难

    It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning

    [https://arxiv.org/abs/2311.07532](https://arxiv.org/abs/2311.07532)

    大型语言模型在排除推理过程中遇到困难，提出了一种新的排除推理方法PoE与COT，发现此方法在多项选择问题上的表现不如选择正确答案，并指出了研究中发现的一致性和错误分析的问题。

    

    链式思维（COT）提示可以帮助大型语言模型（LLMs）朝着正确答案进行推理，但其在朝着错误答案进行推理方面的有效性尚未被探究。当与COT一起使用时，这种排除推理（PoE）可以增强自我一致性、可解释性以及诸如排除性医学诊断等任务。因此，我们提出了在多项选择问题中进行PoE与COT的方法，LLMs必须朝着不正确的选项进行推理。我们评估了GPT-3.5、LLaMA-2和Falcon在总共四个常识和科学推理数据集上执行带有COT的PoE的能力。我们发现PoE策略总是表现不如选择正确答案的策略。这两种策略的一致性也低于每种策略的自我一致性。为了进一步研究这些问题，我们进行了错误分析并提出了未来工作的建议。

    arXiv:2311.07532v2 Announce Type: replace  Abstract: Chain-of-thought (COT) prompting can help large language models (LLMs) reason toward correct answers, but its efficacy in reasoning toward incorrect answers is unexplored. This process of elimination (PoE), when used with COT, can enhance self-consistency, interpretability, and tasks such as medical diagnoses of exclusion. Thus, we propose PoE with COT, where LLMs must reason toward incorrect options on multiple-choice questions. We evaluate the ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of four commonsense and scientific reasoning datasets. We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy. To study these issues further, we conduct error analyses and give suggestions for future work.
    
[^61]: 大型语言模型自适应文字水印

    Adaptive Text Watermark for Large Language Models. (arXiv:2401.13927v1 [cs.CL])

    [http://arxiv.org/abs/2401.13927](http://arxiv.org/abs/2401.13927)

    这个论文提出了一种自适应的大型语言模型文字水印策略，通过辅助模型测量高熵令牌分布，将水印自适应地添加到具有高熵的令牌分布中，同时保持低熵令牌分布不变，以提高文本质量和水印的稳健性。

    

    大型语言模型的发展引发了人们对人工智能生成文本滥用的担忧，而基于大型语言模型生成的文字水印成为潜在解决方案。然而，在保持水印强度、稳健性和无需预先知道提示或模型的情况下检测水印的同时，生成高质量的带水印文本是具有挑战性的。本文提出了一种自适应的水印策略来解决这个问题。为了提高文本质量和保持稳健性，我们根据辅助模型测量的高熵令牌分布自适应地添加水印，而保持低熵令牌分布不变。为了保证安全性并进一步减小水印对文本质量的影响，我们不再使用从随机秘钥生成的固定红/绿名单，而是根据前一个语义嵌入将输出对数比例适应性缩放。

    The advancement of Large Language Models (LLMs) has led to increasing concerns about the misuse of AI-generated text, and watermarking for LLM-generated text has emerged as a potential solution. However, it is challenging to generate high-quality watermarked text while maintaining strong security, robustness, and the ability to detect watermarks without prior knowledge of the prompt or model. This paper proposes an adaptive watermarking strategy to address this problem. To improve the text quality and maintain robustness, we adaptively add watermarking to token distributions with high entropy measured using an auxiliary model and keep the low entropy token distributions untouched. For the sake of security and to further minimize the watermark's impact on text quality, instead of using a fixed green/red list generated from a random secret key, which can be vulnerable to decryption and forgery, we adaptively scale up the output logits in proportion based on the semantic embedding of prev
    
[^62]: MLLMReID: 基于多模态大语言模型的人物再识别

    MLLMReID: Multimodal Large Language Model-based Person Re-identification. (arXiv:2401.13201v1 [cs.CV])

    [http://arxiv.org/abs/2401.13201](http://arxiv.org/abs/2401.13201)

    MLLMReID是一种基于多模态大语言模型的人物再识别方法，通过微调模型并将其视觉编码器作为主干进行优化，解决了MLLM在ReID任务中的设计指令和特征学习效果的问题。

    

    多模态大语言模型（MLLM）在许多任务中取得了令人满意的结果。然而，它们在人物再识别（ReID）任务中的表现尚未被研究。本文将研究如何将它们适应于ReID任务。一种直观的想法是使用ReID图像-文本数据集对MLLM进行微调，然后将它们的视觉编码器作为ReID的主干。然而，仍存在两个明显的问题：（1）为ReID设计指令时，MLLM可能过度拟合特定指令，而设计各种指令将导致更高的成本。（2）LLM的潜在图像特征向量没有参与损失计算。指令学习，对齐图像-文本特征，导致间接优化和学习目标不充分利用特征，限制了人物特征学习的效果。为了解决这些问题，本文提出了MLLMReID：基于多模态大语言模型的ReID。首先，我们提出了公共指令。

    Multimodal large language models (MLLM) have achieved satisfactory results in many tasks. However, their performance in the task of person re-identification (ReID) has not been explored to date. This paper will investigate how to adapt them for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID image-text datasets, and then use their visual encoder as a backbone for ReID. However, there still exist two apparent issues: (1) Designing instructions for ReID, MLLMs may overfit specific instructions, and designing a variety of instructions will lead to higher costs. (2) Latent image feature vectors from LLMs are not involved in loss computation. Instructional learning, aligning image-text features, results in indirect optimization and a learning objective that inadequately utilizes features, limiting effectiveness in person feature learning. To address these problems, this paper proposes MLLMReID: Multimodal Large Language Model-based ReID. Firstly, we proposed Common Instru
    
[^63]: 基于遮蔽生成特征方法的渐进蒸馏用于知识图谱补全

    Progressive Distillation Based on Masked Generation Feature Method for Knowledge Graph Completion. (arXiv:2401.12997v1 [cs.CL])

    [http://arxiv.org/abs/2401.12997](http://arxiv.org/abs/2401.12997)

    本文提出了一种基于遮蔽生成特征的渐进蒸馏方法，用于知识图谱补全任务，通过预蒸馏和压缩预训练模型，以及引入遮蔽生成的教师-学生特征，显著降低了模型的复杂性，并解决了特征表示能力差距的问题。

    

    最近几年，基于预训练语言模型 (PLM) 的知识图谱补全 (KGC) 模型展示了有希望的结果。然而，PLM 模型的大量参数和高计算成本对其在下游任务中的应用提出了挑战。本文提出了一种基于遮蔽生成特征的渐进蒸馏方法，用于 KGC 任务，旨在显著降低预训练模型的复杂性。具体而言，我们对 PLM 进行预蒸馏，得到高质量的教师模型，并压缩 PLM 网络得到多等级的学生模型。然而，传统的特征蒸馏在教师模型中只有单一信息表示的限制。为了解决这个问题，我们提出了教师-学生特征的遮蔽生成，其中包含更丰富的表示信息。此外，教师和学生之间存在显著的表示能力差距。因此，我们设计了一种渐进式的蒸馏方法。

    In recent years, knowledge graph completion (KGC) models based on pre-trained language model (PLM) have shown promising results. However, the large number of parameters and high computational cost of PLM models pose challenges for their application in downstream tasks. This paper proposes a progressive distillation method based on masked generation features for KGC task, aiming to significantly reduce the complexity of pre-trained models. Specifically, we perform pre-distillation on PLM to obtain high-quality teacher models, and compress the PLM network to obtain multi-grade student models. However, traditional feature distillation suffers from the limitation of having a single representation of information in teacher models. To solve this problem, we propose masked generation of teacher-student features, which contain richer representation information. Furthermore, there is a significant gap in representation ability between teacher and student. Therefore, we design a progressive dist
    
[^64]: DeepEdit: 带有约束的解码式知识编辑

    DeepEdit: Knowledge Editing as Decoding with Constraints. (arXiv:2401.10471v1 [cs.CL])

    [http://arxiv.org/abs/2401.10471](http://arxiv.org/abs/2401.10471)

    DeepEdit是一种神经符号方法，通过更好的推理一致性和对更新知识的意识，提高了大型语言模型的知识编辑能力，对多跳问题数据集MQuaKE取得了显著的进展。

    

    我们将大型语言模型（LLMs）的知识编辑视为带有约束的解码过程。我们提出了DeepEdit（基于深度优先搜索的渐进式解码知识编辑），这是一种神经符号方法，通过更好的推理一致性、问题相关性和对更新知识的意识来改进知识编辑。DeepEdit可灵活应用于所有黑盒LLMs：不需要访问模型参数、表示或输出词汇分布。DeepEdit逐步产生高质量的推理步骤，以实现有效的知识编辑。它利用深度优先搜索来修改LLMs的输出，从而提高输出对问题的相关性和对更新知识的意识。在知识编辑方面，DeepEdit在控制LLMs产生更简洁的推理方面表现出色。在MQuaKE上，DeepEdit在定量上取得了显著的进展，这是一个具有挑战性的多跳问题数据集。

    We develop a new perspective of knowledge editing for large language models (LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that improves knowledge editing with better coherence of reasoning, relevance to the question, and awareness of updated knowledge. DeepEdit can be flexibly applied to all black-box LLMs: it does not require any access to the model parameters, representations, or output vocabulary distributions. DeepEdit progressively produces the high-quality reasoning steps towards effective knowledge editing. It utilizes a depth-first search to revise the LLMs' output, which improves the output's informativeness to the input question and awareness of the updated knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit yields significant gains on MQuaKE, a challenging multi-hop que
    
[^65]: 解决度量数值和准确性之间的迷宫：导航指标

    Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies. (arXiv:2401.06760v1 [cs.CL])

    [http://arxiv.org/abs/2401.06760](http://arxiv.org/abs/2401.06760)

    本文研究了一系列现代度量标准的“动态范围”，以提供对度量标准之间和内部得分差异的共同理解，并通过人类感知水平的系统差异进行衡量。

    

    十年前，机器翻译研究中有一个单一的度量标准BLEU。如今，没有这样的共识，因此研究人员很难发展和保持之前推动研究和部署决策的那种启发性直觉。本文研究了一系列现代度量标准的“动态范围”，以提供对度量标准之间和内部得分差异的共同理解；换句话说，我们要问在度量标准Y中，两个系统需要有多大的得分差异X，人类才能注意到？我们使用一个新的大型数据集ToShip23进行评估，使用它发现度量标准达到人类感知水平的系统差异，我们通过成对系统准确性来衡量。此外，我们还表明与标准的统计p值在测试集上的稳定性相比，使用该方法建立差异准确性更为稳定。

    Ten years ago a single metric, BLEU, governed progress in machine translation research. For better or worse, there is no such consensus today, and consequently it is difficult for researchers to develop and retain the kinds of heuristic intuitions about metric deltas that drove earlier research and deployment decisions. This paper investigates the "dynamic range" of a number of modern metrics in an effort to provide a collective understanding of the meaning of differences in scores both within and among metrics; in other words, we ask what point difference X in metric Y is required between two systems for humans to notice? We conduct our evaluation on a new large dataset, ToShip23, using it to discover deltas at which metrics achieve system-level differences that are meaningful to humans, which we measure by pairwise system accuracy. We additionally show that this method of establishing delta-accuracy is more stable than the standard use of statistical p-values in regards to testset si
    
[^66]: 适应大型语言模型进行文档级机器翻译的研究

    Adapting Large Language Models for Document-Level Machine Translation. (arXiv:2401.06468v1 [cs.CL])

    [http://arxiv.org/abs/2401.06468](http://arxiv.org/abs/2401.06468)

    本文研究了适应大型语言模型进行文档级机器翻译的过程。实验结果显示，这些专门的模型在某些情况下超过了GPT-4的翻译性能，但在其他情况下仍然存在离标翻译问题，需要进一步改进和探索。

    

    大型语言模型（LLMs）在各种自然语言处理（NLP）任务中取得了重要进展。最近的研究表明，在任务特定的微调之后，中等规模的LLMs往往胜过其更大的对应模型。在这项工作中，我们深入研究了将LLMs调整为特定语言对的文档级机器翻译（DocMT）的过程。首先，我们探讨了提示策略对下游翻译性能的影响。然后，我们进行了大量实验，使用了两种微调方法、三种LLM主干和18个涉及九种语言对的翻译任务。我们的研究结果表明，在某些情况下，这些专门的模型甚至在翻译性能上超过了GPT-4，而在其他情况下，即使它们专门在双语平行文档上进行了微调，仍然明显存在离标翻译问题。此外，我们对这些针对DocMT量身定制的LLMs进行了深入分析，探讨了如翻译准确度改善、多源信息整合等各个方面。

    Large language models (LLMs) have made significant strides in various natural language processing (NLP) tasks. Recent research shows that the moderately-sized LLMs often outperform their larger counterparts after task-specific fine-tuning. In this work, we delve into the process of adapting LLMs to specialize in document-level machine translation (DocMT) for a specific language pair. Firstly, we explore how prompt strategies affect downstream translation performance. Then, we conduct extensive experiments with two fine-tuning methods, three LLM backbones, and 18 translation tasks across nine language pairs. Our findings indicate that in some cases, these specialized models even surpass GPT-4 in translation performance, while they still significantly suffer from the off-target translation issue in others, even if they are exclusively fine-tuned on bilingual parallel documents. Furthermore, we provide an in-depth analysis of these LLMs tailored for DocMT, exploring aspects such as transl
    
[^67]: 大规模语言模型的极端压缩通过加性量化

    Extreme Compression of Large Language Models via Additive Quantization. (arXiv:2401.06118v1 [cs.LG])

    [http://arxiv.org/abs/2401.06118](http://arxiv.org/abs/2401.06118)

    本文提出的算法在大规模语言模型的极端压缩方面取得了较好的性能，相比最新技术，在给定的压缩预算下准确性更高。

    

    准确的开源大规模语言模型(LLMs)的出现引发了对这些模型进行量化技术的竞赛，从而使其能够在最终用户设备上执行。在本文中，我们从多码本量化(MCQ)的经典方法角度重新思考了“极端”LLM压缩的问题，即针对非常低的位数，例如每个参数2到3位。我们的工作建立在加性量化这一经典算法之上，并将其适应于语言模型的量化。由此得到的算法在LLM压缩方面推进了最新技术，以给定压缩预算的准确性而言，优于所有最近提出的技术。例如，当将Llama 2模型压缩到每个参数2位时，我们的算法将7B模型量化为6.93困惑度(相对于之前最佳工作改进1.29，相对于FP16改进1.81)，13B模型量化为5.70困惑度(改进0.36)，70B模型量化为3.94困惑度。

    The emergence of accurate open large language models (LLMs) has led to a race towards quantization techniques for such models enabling execution on end-user devices. In this paper, we revisit the problem of "extreme" LLM compression--defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter, from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our work builds on top of Additive Quantization, a classic algorithm from the MCQ family, and adapts it to the quantization of language models. The resulting algorithm advances the state-of-the-art in LLM compression, outperforming all recently-proposed techniques in terms of accuracy at a given compression budget. For instance, when compressing Llama 2 models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93 perplexity (a 1.29 improvement relative to the best prior work, and 1.81 points from FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B model to 3.94 
    
[^68]: LinguAlchemy: 将语言类型学和地理元素融合以实现对未见语言的泛化

    LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization. (arXiv:2401.06034v1 [cs.CL])

    [http://arxiv.org/abs/2401.06034](http://arxiv.org/abs/2401.06034)

    LinguAlchemy是一种将语言类型学和地理元素融合的正则化技术，能够显著提高预训练语言模型（PLMs）在未见语言上的泛化性能。

    

    预训练语言模型（PLMs）在多个任务和语言上展示出了非凡的泛化能力。然而，对于未见过的语言，PLMs的泛化能力较差，导致语言性能明显下降，甚至生成的回应与随机基准相当荒唐。这一限制一直以来都是PLMs的一个长期问题，涉及到语言建模技术的多样性和平等获取问题。在这项工作中，我们通过引入LinguAlchemy来解决这个限制，这是一种正则化技术，将语言的各个方面（包括类型学、地理和语系）纳入PLMs的表示中，以更好地表征相应的语言约束。与完全微调的模型相比，LinguAlchemy显著提高了mBERT和XLM-R对未见语言的准确性绩效，分别提高了约18%和约2%，展现出较高的未见语言泛化能力。

    Pretrained language models (PLMs) have shown remarkable generalization toward multiple tasks and languages. Nonetheless, the generalization of PLMs towards unseen languages is poor, resulting in significantly worse language performance, or even generating nonsensical responses that are comparable to a random baseline. This limitation has been a longstanding problem of PLMs raising the problem of diversity and equal access to language modeling technology. In this work, we solve this limitation by introducing LinguAlchemy, a regularization technique that incorporates various aspects of languages covering typological, geographical, and phylogenetic constraining the resulting representation of PLMs to better characterize the corresponding linguistics constraints. LinguAlchemy significantly improves the accuracy performance of mBERT and XLM-R on unseen languages by ~18% and ~2%, respectively compared to fully finetuned models and displaying a high degree of unseen language generalization. W
    
[^69]: 在大型语言模型中定位事实知识：探索剩余流和分析词汇空间中的子值。

    Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space. (arXiv:2312.12141v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.12141](http://arxiv.org/abs/2312.12141)

    通过探索剩余流和分析词汇空间中的子值，我们定位了大型语言模型中的事实知识，并找到了存储了有关“法国，首都，巴黎”的知识的位置。

    

    通过探索剩余流和分析词汇空间中的子值，我们找到了大型语言模型中事实知识的位置。我们发现当投影到词汇空间时，子值具有可人类解释的概念的原因。子值的softmax之前的值通过一个加法函数相加，因此词汇空间中前几个标记的概率会增加。基于此，我们发现使用对数概率增加来计算层和子值的重要性比概率增加更好，因为对数概率增加的曲线呈线性单调增形状。此外，我们计算内积来评估前馈网络（FFN）的子值被前面的层激活的程度。根据我们的方法，我们找到了事实知识“法国，首都，巴黎”存储的位置。具体来说，注意力层存储“巴黎与法国相关”。FFN层存储“巴黎是一个首都/城市”，由注意力子值激活。

    We find the location of factual knowledge in large language models by exploring the residual stream and analyzing subvalues in vocabulary space. We find the reason why subvalues have human-interpretable concepts when projecting into vocabulary space. The before-softmax values of subvalues are added by an addition function, thus the probability of top tokens in vocabulary space will increase. Based on this, we find using log probability increase to compute the significance of layers and subvalues is better than probability increase, since the curve of log probability increase has a linear monotonically increasing shape. Moreover, we calculate the inner products to evaluate how much a feed-forward network (FFN) subvalue is activated by previous layers. Base on our methods, we find where factual knowledge <France, capital, Paris> is stored. Specifically, attention layers store "Paris is related to France". FFN layers store "Paris is a capital/city", activated by attention subvalues relate
    
[^70]: 演示就是你需要的一切：利用上下文学习推进攻击性内容改写

    Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning. (arXiv:2310.10707v1 [cs.CL])

    [http://arxiv.org/abs/2310.10707](http://arxiv.org/abs/2310.10707)

    该论文在攻击性内容改写方面引入了上下文学习方法，并通过有限数量的输入-标签演示对来指导模型生成特定查询的所需输出，从而提高可用性和减少攻击性。

    

    改写攻击性内容是一种更好的替代内容删除的方法，有助于改善沟通环境的文明程度。然而，监督式的改写器在保留意义和意图的同时，对大量标记数据依赖性较高。它们也保留了原始内容的大部分攻击性，这引发了对它们整体可用性的疑问。在本文中，我们旨在通过探索上下文学习（ICL）与大型语言模型（LLM）相结合，帮助从业者开发可用的改写器，即使用有限数量的输入-标签演示对来引导模型生成特定查询的所需输出。我们的研究主要关注关键因素，如演示的数量和顺序，排除提示指令，以及降低测量毒性。我们在包括我们提出的上下文感知礼貌改写数据集在内的三个数据集上进行了原则性评估，其中包括对话式的粗鲁发言、礼貌改写等。

    Paraphrasing of offensive content is a better alternative to content removal and helps improve civility in a communication environment. Supervised paraphrasers; however, rely heavily on large quantities of labelled data to help preserve meaning and intent. They also retain a large portion of the offensiveness of the original content, which raises questions on their overall usability. In this paper we aim to assist practitioners in developing usable paraphrasers by exploring In-Context Learning (ICL) with large language models (LLMs), i.e., using a limited number of input-label demonstration pairs to guide the model in generating desired outputs for specific queries. Our study focuses on key factors such as -- number and order of demonstrations, exclusion of prompt instruction, and reduction in measured toxicity. We perform principled evaluation on three datasets, including our proposed Context-Aware Polite Paraphrase dataset, comprising of dialogue-style rude utterances, polite paraphr
    
[^71]: 在线推测解码

    Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])

    [http://arxiv.org/abs/2310.07177](http://arxiv.org/abs/2310.07177)

    在线推测解码是通过利用多余计算能力，在LLM服务集群中持续更新草稿模型，从而加速大型语言模型推理的一种方法。

    

    推测解码是通过利用较小的草稿模型来预测目标模型的输出，从而加速大型语言模型（LLM）推理的关键技术。然而，在面对多样的文本输入和草稿模型与目标模型之间的显著能力差距时，其有效性可能受到限制。我们引入了在线推测解码（OSD）来解决这一挑战。其主要思想是利用LLM服务集群中丰富的多余计算能力，根据观察到的用户查询数据持续更新（多个）草稿模型。由于LLM推理受内存限制，典型的LLM服务集群中的剩余计算能力可以用于在线重新训练草稿模型，从而使训练成本保持中性。由于LLM服务的查询分布相对简单，根据查询分布进行重新训练可以使草稿模型更准确地预测目标模型的输出。

    Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
    
[^72]: 通过随机化潜在表示来迷惑文本愚弄者

    Fooling the Textual Fooler via Randomizing Latent Representations. (arXiv:2310.01452v1 [cs.CL])

    [http://arxiv.org/abs/2310.01452](http://arxiv.org/abs/2310.01452)

    该论文提出了一种轻量级的攻击无关防御策略AdvFooler，通过随机化输入的潜在表示来困惑基于查询的黑盒攻击，从而迷惑文本愚弄者。

    

    尽管在各种自然语言处理任务中表现出色，但近期的研究表明，自然语言处理模型容易受到敌对攻击的影响，即微小地改变输入以导致模型的错误行为。其中，敌对词级扰动是一个被广泛研究和有效的攻击策略。这些攻击在黑盒设置中起作用，不需要访问模型结构或参数，因此可能对现有的自然语言处理应用产生不利影响。为了进行攻击，对手多次查询受害模型，以确定输入文本中最重要的单词，并用它们对应的同义词替换这些单词。在这项工作中，我们提出了一种轻量级和攻击无关的防御，其主要目标是困惑基于查询的黑盒攻击中产生敌对示例的过程；即愚弄文本愚弄者。这种防御名为AdvFooler，通过随机化输入的潜在表示来实现。

    Despite outstanding performance in a variety of NLP tasks, recent studies have revealed that NLP models are vulnerable to adversarial attacks that slightly perturb the input to cause the models to misbehave. Among these attacks, adversarial word-level perturbations are well-studied and effective attack strategies. Since these attacks work in black-box settings, they do not require access to the model architecture or model parameters and thus can be detrimental to existing NLP applications. To perform an attack, the adversary queries the victim model many times to determine the most important words in an input text and to replace these words with their corresponding synonyms. In this work, we propose a lightweight and attack-agnostic defense whose main goal is to perplex the process of generating an adversarial example in these query-based black-box attacks; that is to fool the textual fooler. This defense, named AdvFooler, works by randomizing the latent representation of the input at 
    
[^73]: LLM-辩论: 使用交互式多智能体协商游戏评估LLMs

    LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games. (arXiv:2309.17234v1 [cs.CL])

    [http://arxiv.org/abs/2309.17234](http://arxiv.org/abs/2309.17234)

    本文提出使用可评分的谈判游戏作为LLMs的新评估框架，创建了一个多样的测试平台，并通过系统化的零-shot思维链提示（CoT）展示了代理人可以成功谈判。该研究揭示了GPT-4在该任务上的性能差距。

    

    越来越多的人对使用大型语言模型（LLMs）作为代理人来解决可能需要评估复杂情况的现实任务感兴趣。然而，我们对LLMs的推理和决策能力有限的理解，在某种程度上是由于缺乏专门的评估基准。由于谈判和妥协是我们日常沟通和合作的关键方面，我们提出使用可评分的谈判游戏作为LLMs的新评估框架。我们创建了一个多样的基于文本的、多智能体的、多问题的、语义丰富的谈判游戏测试平台，难度可调。为了解决这一挑战，代理人需要具备强大的算术、推理、探索和规划能力，同时无缝地整合它们。通过系统化的零-shot思维链提示（CoT），我们展示了代理人可以进行谈判并持续达成成功交易。我们用多个指标量化性能，并观察到GPT-4与原文之间存在很大差距。

    There is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs' reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and 
    
[^74]: 我希望争论：大型语言模型中的争论性推理

    I Wish to Have an Argument: Argumentative Reasoning in Large Language Models. (arXiv:2309.16938v1 [cs.CL])

    [http://arxiv.org/abs/2309.16938](http://arxiv.org/abs/2309.16938)

    该论文评估了当代大型语言模型在争论性推理方面的能力，并发现其争论性推理性能与输入和输出表示密切相关。研究还发现了“典型效应”，并探讨了在思维链提示下解决病态问题时的性能优势。

    

    我们评估当代大型语言模型（LLMs）进行争论性推理的能力。我们将实验框架定为争论挖掘（AM）和争论对提取（APE）任务，并评估它们在输入和输出表示的抽象级别上执行推理的能力（例如，任意标签集，语义图）。我们发现，尽管 LLMs 在 AM 和 APE 上能够与或超过最先进的水平，但它们的争论性推理性能非常依赖于输入和输出表示。我们还发现了一个“典型效应”，过多的典型实例会对任务性能产生负面影响，最佳数量约为4-5个。然而，这些结果并不适用于思维链（CoT）提示：我们发现典型效应被抵消，我们的结果表明在病态问题下，CoT 可以实现更好的性能。我们希望所报告的工作能够促进争论性推理的改进。

    We evaluate the ability of contemporary large language models (LLMs) to perform argumentative reasoning. We frame our experiments in terms of the argument mining (AM) and argument pair extraction (APE) tasks, and evaluate their ability to perform reasoning at increasing levels of abstraction in the input and output representations (e.g., arbitrary label sets, semantic graphs). We find that, although LLMs are able to match or surpass the state-of-the-art in AM and APE, their argumentative reasoning performance is very dependent on the input and output representation. We also find an "exemplar effect", where too many exemplars increasingly become detrimental for task performance, and about 4-5 being the optimal amount. Neither result extends to chain-of-thought (CoT) prompting: we find the exemplar effect to be nullified, and our results suggest that CoT allows for better performance under ill-conditioned problems. We hope that the work reported contributes to the improvement of argument
    
[^75]: 移动严肃游戏中口语化人形机器人对可用性的评估

    Usability Evaluation of Spoken Humanoid Embodied Conversational Agents in Mobile Serious Games. (arXiv:2309.07773v1 [cs.HC])

    [http://arxiv.org/abs/2309.07773](http://arxiv.org/abs/2309.07773)

    本文通过实证调查评估了移动严肃游戏应用中口语化人形机器人对可用性的影响，结果表明用户更喜欢与高人类相似度的机器人进行交互。

    

    本文对移动严肃游戏应用中口语化人形机器人（HECA）对可用性的影响进行了实证调查。研究旨在评估多个机器人和人类交互幻觉对交互质量的影响。实验研究了两种机器人呈现方式：高人类相似度的机器人（HECA）和低人类相似度的机器人（文本）。实验的目的是评估高人类相似度机器人是否能够引发人类幻觉并影响可用性。高人类相似度机器人是根据ECA设计模型进行设计的，该模型是一种ECA开发的指导方针。实验结果显示，90位参与者更喜欢与HECA进行交互。两个版本之间的差异在统计学上具有显著性，效应大小较大（d=1.01），许多参与者通过解释选择来证明他们的选择。

    This paper presents an empirical investigation of the extent to which spoken Humanoid Embodied Conversational Agents (HECAs) can foster usability in mobile serious game (MSG) applications. The aim of the research is to assess the impact of multiple agents and illusion of humanness on the quality of the interaction. The experiment investigates two styles of agent presentation: an agent of high human-likeness (HECA) and an agent of low human-likeness (text). The purpose of the experiment is to assess whether and how agents of high humanlikeness can evoke the illusion of humanness and affect usability. Agents of high human-likeness were designed by following the ECA design model that is a proposed guide for ECA development. The results of the experiment with 90 participants show that users prefer to interact with the HECAs. The difference between the two versions is statistically significant with a large effect size (d=1.01), with many of the participants justifying their choice by saying
    
[^76]: 在分解神经传输器中引入基于类别的语言模型来进行命名实体识别

    Incorporating Class-based Language Model for Named Entity Recognition in Factorized Neural Transducer. (arXiv:2309.07648v1 [eess.AS])

    [http://arxiv.org/abs/2309.07648](http://arxiv.org/abs/2309.07648)

    这项研究在分解神经传输器中加入了基于类别的语言模型，提升了命名实体识别的能力。

    

    尽管最近几年端到端（E2E）模型在语音识别方面取得了巨大的进展，但命名实体识别仍然具有挑战性但又对语义理解至关重要。为了增强E2E模型中识别命名实体的能力，先前的研究主要集中在各种基于规则或基于注意力的上下文偏置算法上。然而，它们的性能可能对偏置权重敏感，或者由于对命名实体列表的过度关注而降低，并且存在误触发的风险。受传统混合系统中基于类别的语言模型（LM）在命名实体识别中的成功启发，以及在分解神经传输器（FNT）中声学和语言信息的有效解耦，我们提出了一种新颖的E2E模型来将基于类别的LMs纳入FNT中，称为C-FNT。在C-FNT中，命名实体的语言模型得分可以与其类别关联，而不是与其表面形式关联。

    In spite of the excellent strides made by end-to-end (E2E) models in speech recognition in recent years, named entity recognition is still challenging but critical for semantic understanding. In order to enhance the ability to recognize named entities in E2E models, previous studies mainly focus on various rule-based or attention-based contextual biasing algorithms. However, their performance might be sensitive to the biasing weight or degraded by excessive attention to the named entity list, along with a risk of false triggering. Inspired by the success of the class-based language model (LM) in named entity recognition in conventional hybrid systems and the effective decoupling of acoustic and linguistic information in the factorized neural Transducer (FNT), we propose a novel E2E model to incorporate class-based LMs into FNT, which is referred as C-FNT. In C-FNT, the language model score of named entities can be associated with the name class instead of its surface form. The experime
    
[^77]: Prompting4Debugging: 通过发现问题提示来对文本到图像扩散模型进行红队测试

    Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts. (arXiv:2309.06135v1 [cs.CL])

    [http://arxiv.org/abs/2309.06135](http://arxiv.org/abs/2309.06135)

    提出了Prompting4Debugging（P4D）作为一个调试和红队测试工具，可以自动找到扩散模型的问题提示，以测试部署的安全机制的可靠性。

    

    文本到图像扩散模型，例如稳定扩散（SD），最近展现出高质量内容生成的显著能力，并成为近期变革性人工智能浪潮的代表之一。然而，这种进步也带来了对该生成技术滥用的日益关注，特别是用于生成受版权保护或不适合在工作环境中查看的图像。虽然已经做出了一些努力来通过模型微调来过滤不适当的图像/提示或删除不希望的概念/风格，但这些安全机制对于多样化的问题提示的可靠性仍然不清楚。在这项工作中，我们提出了Prompting4Debugging（P4D）作为一个调试和红队测试工具，它可以自动找到扩散模型的问题提示，以测试部署的安全机制的可靠性。我们展示了我们的P4D工具在发现具有安全机制的SD模型的新漏洞方面的有效性。具体而言，我们的结果显示...

    Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shown remarkable ability in high-quality content generation, and become one of the representatives for the recent wave of transformative AI. Nevertheless, such advance comes with an intensifying concern about the misuse of this generative technology, especially for producing copyrighted or NSFW (i.e. not safe for work) images. Although efforts have been made to filter inappropriate images/prompts or remove undesirable concepts/styles via model fine-tuning, the reliability of these safety mechanisms against diversified problematic prompts remains largely unexplored. In this work, we propose Prompting4Debugging (P4D) as a debugging and red-teaming tool that automatically finds problematic prompts for diffusion models to test the reliability of a deployed safety mechanism. We demonstrate the efficacy of our P4D tool in uncovering new vulnerabilities of SD models with safety mechanisms. Particularly, our result shows t
    
[^78]: 评估监督学习和大型语言模型在识别中国社交媒体中的认知偏差和自杀风险方面的功效

    Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media. (arXiv:2309.03564v1 [cs.CL])

    [http://arxiv.org/abs/2309.03564](http://arxiv.org/abs/2309.03564)

    本研究评估了监督学习和大型语言模型在识别中国社交媒体中的认知偏差和自杀风险方面的功效。结果表明大型语言模型在这两个任务上具有很高的效果。

    

    大型语言模型，特别是类似快速发展的GPT系列，因其广泛的影响力而受到关注。尽管在心理学等医学领域对它们的适用性存在浓厚兴趣，但对真实世界数据的具体探索仍然很少。与此同时，社交媒体平台上的用户越来越多地表达个人情感；在特定的主题下，这些情感通常表现为消极情绪，有时会升级为自杀倾向。及时辨识这样的认知偏差和自杀风险对有效干预和潜在避免严重情况至关重要。我们的研究通过在中国社交媒体平台上进行两个关键任务：自杀风险和认知偏差识别的实验，进入了这个领域。使用监督学习作为基准，我们通过三种不同的策略：零样本、少样本和微调，考察了大型语言模型的功效。

    Large language models, particularly those akin to the rapidly progressing GPT series, are gaining traction for their expansive influence. While there is keen interest in their applicability within medical domains such as psychology, tangible explorations on real-world data remain scant. Concurrently, users on social media platforms are increasingly vocalizing personal sentiments; under specific thematic umbrellas, these sentiments often manifest as negative emotions, sometimes escalating to suicidal inclinations. Timely discernment of such cognitive distortions and suicidal risks is crucial to effectively intervene and potentially avert dire circumstances. Our study ventured into this realm by experimenting on two pivotal tasks: suicidal risk and cognitive distortion identification on Chinese social media platforms. Using supervised learning as a baseline, we examined and contrasted the efficacy of large language models via three distinct strategies: zero-shot, few-shot, and fine-tunin
    
[^79]: 零样本和少样本情况下应用于临床和生物医学任务的指导细调大型语言模型的研究

    A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks. (arXiv:2307.12114v1 [cs.CL])

    [http://arxiv.org/abs/2307.12114](http://arxiv.org/abs/2307.12114)

    这项研究评估了四种指导细调大型语言模型在临床和生物医学任务上的表现，并发现它们在零样本和少样本情况下接近最先进模型的性能，尤其在问答任务上表现良好。然而，在分类和关系抽取任务上的表现稍逊于特定训练于医学领域的模型。没有一个模型在所有研究任务上胜过其他模型，有些模型更适合特定任务。

    

    我们评估了四种最先进的指导细调大型语言模型（LLM）——ChatGPT、Flan-T5 UL2、Tk-Instruct和Alpaca——在13个实际世界的临床和生物医学自然语言处理（NLP）任务中的表现，例如命名实体识别（NER）、问答（QA）、关系抽取（RE）等。我们的综合结果表明，在大多数任务的零样本和少样本情况下，评估的LLM开始接近最先进模型的性能，尤其对于QA任务表现得特别好，即使它们之前没有见过这些任务的示例。然而，我们观察到分类和关系抽取任务的表现低于特定训练于医学领域的模型（如PubMedBERT）可以达到的水平。最后，我们注意到没有一个LLM在所有研究任务上都胜过其他模型，有些模型更适合于特定的任务。

    We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.
    
[^80]: LLaMA在临床领域的参数高效微调

    Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain. (arXiv:2307.03042v1 [cs.CL])

    [http://arxiv.org/abs/2307.03042](http://arxiv.org/abs/2307.03042)

    本研究提出了一种参数高效微调（PEFT）方法，在临床领域使用临床记录训练了一个专门适配临床领域的LLaMA-LoRA模型，同时提出了一个两步PEFT框架，用于将其与Downstream LLaMA-LoRA适配器进行融合，以实现领域适应。

    

    传统上，将预训练的语言模型适应到新领域，如临床应用，需要重新训练所有参数。然而，由于训练这些大型语言模型所需的计算资源巨大，这种方法的实践性越来越被证明是不切实际的。为了解决这个问题，参数高效微调（PEFT）技术提供了一种可行的解决方案，通过选择性地微调一个小的附加参数集，显著减少了领域适应所需的计算资源。在本研究中，我们提出了临床LLaMA-LoRA，这是一个构建在开源LLaMA模型上的PEFT适配器层。临床LLaMA-LoRA使用从MIMIC-IV数据库中获取的临床记录进行训练，从而创建了一个专为临床领域设计的专用适配器。此外，我们提出了一个两步PEFT框架，将临床LLaMA-LoRA与Downstream LLaMA-LoRA进行融合，后者是另一个专为下游任务设计的PEFT适配器。

    Adapting pretrained language models to novel domains, such as clinical applications, traditionally involves retraining their entire set of parameters. However, this approach is increasingly proven to be impractical owing to the substantial computational requirements associated with training such large language models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) techniques offer a viable solution by selectively fine-tuning a small subset of additional parameters, significantly reducing the computational requirements for domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT adapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is trained using clinical notes obtained from the MIMIC-IV database, thereby creating a specialised adapter designed for the clinical domain. Additionally, we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with Downstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks. W
    
[^81]: 基于元分布建模的开放领域文本评估

    Open-Domain Text Evaluation via Meta Distribution Modeling. (arXiv:2306.11879v1 [cs.CL])

    [http://arxiv.org/abs/2306.11879](http://arxiv.org/abs/2306.11879)

    本文提出了一种新颖的开放领域文本生成模型评估方法——元分布方法（MDM），该方法将两个概率分布的对比映射到质量度量上，可以视为分布的分布，其可用于开放领域文本生成的评估。

    

    最近，基于大型预训练语言模型（LLMs）的开放领域文本生成模型取得了显著的性能提升。然而，为了控制和评估这些模型是否达到所需属性仍然是一个挑战，因为传统的基于参考文本的度量标准如BLEU、ROUGE和METEOR对于开放式生成任务来说是不足够的。同样地，虽然具备训练鉴别器的度量标准表现出了希望的前景，但是获取高质量的训练数据则是一项非常困难的任务。本文提出了一种新颖的方法来评估开放领域文本生成——元分布方法（MDM）。通过考虑LLMs参数数量上升和性能提升之间的相关性，MDM 创造了一个映射，将两个概率分布的对比（一个已知优于另一个）映射到质量度量上，该度量可以视为分布的分布，即元分布。我们研究了MDM在评估开放领域文本生成中的应用。

    Recent advances in open-domain text generation models powered by large pre-trained language models (LLMs) have achieved remarkable performance. However, evaluating and controlling these models for desired attributes remains a challenge, as traditional reference-based metrics such as BLEU, ROUGE, and METEOR are insufficient for open-ended generation tasks. Similarly, while trainable discriminator-based evaluation metrics show promise, obtaining high-quality training data is a non-trivial task. In this paper, we introduce a novel approach to evaluate open-domain generation - the Meta-Distribution Methods (MDM). Drawing on the correlation between the rising parameter counts and the improving performance of LLMs, MDM creates a mapping from the contrast of two probabilistic distributions -- one known to be superior to the other -to quality measures, which can be viewed as a distribution of distributions i.e. Meta-Distribution. We investigate MDM for open-domain text generation evaluation 
    
[^82]: DAPR：文档感知段落检索的基准测试

    DAPR: A Benchmark on Document-Aware Passage Retrieval. (arXiv:2305.13915v1 [cs.IR])

    [http://arxiv.org/abs/2305.13915](http://arxiv.org/abs/2305.13915)

    DAPR是一个文档感知段落检索的基准测试，挑战在于如何从长文档中找到正确的段落并返回准确结果。

    

    最近的神经检索主要关注短文本的排名，并且在处理长文档方面存在挑战。现有的工作主要评估排名段落或整个文档。然而，许多情况下，用户希望从庞大的语料库中找到长文档中的相关段落，例如法律案例，研究论文等，此时段落往往提供很少的文档上下文，这就挑战了当前的方法找到正确的文档并返回准确的结果。为了填补这个空白，我们提出并命名了Document-Aware Passage Retrieval（DAPR）任务，并构建了一个包括来自不同领域的多个数据集的基准测试，涵盖了DAPR和整个文档检索。在实验中，我们通过不同的方法，包括在文档摘要中添加文档级别的内容，汇总段落表示和使用BM25进行混合检索，扩展了最先进的神经段落检索器。这个混合检索系统，总体基准测试显示，我们提出的DAPR任务是一个具有挑战性和重要性的问题，需要进一步研究。

    Recent neural retrieval mainly focuses on ranking short texts and is challenged with long documents. Existing work mainly evaluates either ranking passages or whole documents. However, there are many cases where the users want to find a relevant passage within a long document from a huge corpus, e.g. legal cases, research papers, etc. In this scenario, the passage often provides little document context and thus challenges the current approaches to finding the correct document and returning accurate results. To fill this gap, we propose and name this task Document-Aware Passage Retrieval (DAPR) and build a benchmark including multiple datasets from various domains, covering both DAPR and whole-document retrieval. In experiments, we extend the state-of-the-art neural passage retrievers with document-level context via different approaches including prepending document summary, pooling over passage representations, and hybrid retrieval with BM25. The hybrid-retrieval systems, the overall b
    
[^83]: 区分和回答：通过辨别器缓解检索增强模型中虚假信息的影响

    Discern and Answer: Mitigating the Impact of Misinformation in Retrieval-Augmented Models with Discriminators. (arXiv:2305.01579v1 [cs.CL])

    [http://arxiv.org/abs/2305.01579](http://arxiv.org/abs/2305.01579)

    本文研究了现有检索增强语言模型假设所有检索信息都是正确的假设的问题，在实际应用中可能存在虚假信息导致冲突的情况下，提出了通过精细调整鉴别器和提示鉴别能力引出鲁棒性的方法，这显著改善了模型在知识冲突下的效果；同时提供了关于交替精细调整模型和上下文学习的新的结论。

    

    大多数现有的检索增强语言模型（LM）假定所有检索到的信息都是事实上正确的。本文研究一个更加现实的场景，即检索到的文档可能包含虚假信息，从而导致它们之间存在冲突。我们观察到，现有模型在精调和上下文少样本学习设置中对这种信息高度脆弱。我们提出了一些方法，通过明确地对鉴别器进行精细调整或提示来引出GPT-3的鉴别能力，使检索增强LM对虚假信息具有鲁棒性。我们在开放域问答方面的实证结果表明，这些方法显著改善了LM对知识冲突的鲁棒性。我们还提供了关于交替精细调整模型的决策与上下文学习过程的发现，为利用两者的最佳方式铺平了新的道路。

    Most existing retrieval-augmented language models (LMs) for question answering assume all retrieved information is factually correct. In this work, we study a more realistic scenario in which retrieved documents may contain misinformation, causing conflicts among them. We observe that the existing models are highly brittle to such information in both fine-tuning and in-context few-shot learning settings. We propose approaches to make retrieval-augmented LMs robust to misinformation by explicitly fine-tuning a discriminator or prompting to elicit discrimination capability in GPT-3. Our empirical results on open-domain question answering show that these approaches significantly improve LMs' robustness to knowledge conflicts. We also provide our findings on interleaving the fine-tuned model's decision with the in-context learning process, paving a new path to leverage the best of both worlds.
    

