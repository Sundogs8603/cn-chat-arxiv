# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Understanding Retrieval Augmentation for Long-Form Question Answering.](http://arxiv.org/abs/2310.12150) | 这项研究分析了长篇问答中的检索增强语言模型的影响，研究了生成答案的属性和归因模式，并找出了归因错误的主要原因。研究结果对长篇、知识丰富的文本生成提供了新的见解。 |
| [^2] | [Simple Mechanisms for Representing, Indexing and Manipulating Concepts.](http://arxiv.org/abs/2310.12143) | 通过查看概念的矩阵统计量，生成一个概念的具体表示或签名，可以用于发现概念之间的结构并递归产生更高级的概念，同时可以通过概念的签名来找到相关的共同主题。 |
| [^3] | [Pseudointelligence: A Unifying Framework for Language Model Evaluation.](http://arxiv.org/abs/2310.12135) | 本文提出了一种称为伪智能的统一框架，用于语言模型的评估。这个框架认为智能的评估取决于观察者，并提出了一种基于复杂性理论的动态互动评估方法 。 |
| [^4] | [DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning.](http://arxiv.org/abs/2310.12128) | DiagrammerGPT是一个通过LLM规划生成开放领域、开放平台的图表的框架，填补了T2I模型在图表生成方面的空白。 |
| [^5] | [A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation.](http://arxiv.org/abs/2310.12127) | 本研究通过调查机器翻译模型中的性别偏见问题以及缓解性别偏见的方法来填补现有研究的空白。研究发现指导微调模型在默认为男性翻译上存在性别偏见，同时忽视了指示职业性别的代词，并提出了一些可行的缓解策略。 |
| [^6] | [SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks.](http://arxiv.org/abs/2310.12126) | SHARCS是一种高效的Transformer模型，通过动态宽度子网络进行路由，实现自适应推理和更高的效率，同时在各种分类任务中表现优越并且具有通用性。 |
| [^7] | [Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers.](http://arxiv.org/abs/2310.12118) | 本研究引入了一种利用数据集制图的方法，通过识别组合泛化数据子集，实现了在Transformer中显著提高模型准确性的创新。实验结果显示，该方法在CFQ和COGS数据集上的性能提升高达10%。同时，该技术将数据集制图作为课程学习准则，消除了超参数调整的需求，获得了卓越的性能。 |
| [^8] | [Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling.](http://arxiv.org/abs/2310.12100) | 这篇论文介绍了一种非侵入式的参数高效微调技术（AdaLink），通过只调整模型的外部参数而保持内部结构不变，实现了对多模态建模的竞争性能，这对于大规模语言模型和视觉语言模型的自适应和部署具有重要意义。 |
| [^9] | [Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection.](http://arxiv.org/abs/2310.12086) | 该论文介绍了一种为大型语言模型设计的FactCHD事实冲突幻觉检测基准，用于评估LLMs生成文本的事实性。基准包含了多种事实模式，并使用基于事实的证据链进行组合性幻觉的检测。 |
| [^10] | [On the Benefit of Generative Foundation Models for Human Activity Recognition.](http://arxiv.org/abs/2310.12085) | 本论文介绍了在人类活动识别中，基于生成模型的益处。通过使用生成型人工智能生成虚拟IMU数据，可以解决数据稀缺性的问题，并指出了生成标准测试数据集、开发特定于HAR的基础模型、探索HAR中的分层结构、分解复杂活动以及在健康感知和活动摘要中的应用等几个研究方向的潜在贡献。 |
| [^11] | [Towards Safer Operations: An Expert-involved Dataset of High-Pressure Gas Incidents for Preventing Future Failures.](http://arxiv.org/abs/2310.12074) | 本文介绍了一个名为IncidentAI的新数据集，用于安全预防。该数据集包含三个任务，并由高压气体保护管理领域的专家进行了注释。初步结果显示，使用自然语言处理技术有助于分析事故报告以预防未来的故障。这个数据集将促进NLP和事故管理领域的未来研究。 |
| [^12] | [SPEED: Speculative Pipelined Execution for Efficient Decoding.](http://arxiv.org/abs/2310.12072) | SPEED通过推测执行多个未来标记，加快Transformer解码器的推理效率，从而提高生成型大型语言模型在实时场景中的应用性能。 |
| [^13] | [Code Book for the Annotation of Diverse Cross-Document Coreference of Entities in News Articles.](http://arxiv.org/abs/2310.12064) | 本文提出了一种新闻文章中共指的注释方案，通过考虑接近身份和桥接关系，扩展了传统的共指关系。我们提供了一种创建不同交叉文档共指语料库的方法论，可用于分析媒体偏见。 |
| [^14] | [Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education.](http://arxiv.org/abs/2310.12059) | 本研究评估了大型语言模型在越南普通教育中对多项选择题符号绑定能力的能力，并创建了一个新颖且高质量的数据集来评估语言模型的符号绑定能力。 |
| [^15] | [Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scaling of Texts with Large Language Models.](http://arxiv.org/abs/2310.12049) | 这项研究开发了一种文本缩放方法，利用生成性大型语言模型的模式识别能力，通过概念导向思维链图和大型语言模型进行文本比较，并使用Bradley-Terry模型来估计评分尺度。该方法在Twitter上对情感言论的缩放效果更好。 |
| [^16] | [CORE: A Few-Shot Company Relation Classification Dataset for Robust Domain Adaptation.](http://arxiv.org/abs/2310.12024) | CORE是一个用于少样本公司关系分类的数据集，该数据集包括4708个实例和12种关系类型。实验结果表明，目前的RC模型在适应CORE方面存在困难，但在CORE上训练的模型具有改进的跨领域性能。 |
| [^17] | [LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation.](http://arxiv.org/abs/2310.12020) | LoHoRavens是一个针对机器人桌面操作的长时程语言条件基准测试，涵盖颜色、大小、空间、算术和引用等各种推理方面。本研究还探索了在机器人执行过程中如何将观测反馈纳入到大型语言模型的闭环规划中的两种方法。 |
| [^18] | [Gold: A Global and Local-aware Denoising Framework for Commonsense Knowledge Graph Noise Detection.](http://arxiv.org/abs/2310.12011) | Gold是一种全球和局部意识的常识知识图噪声检测去噪框架，通过结合实体语义信息、全局规则和局部结构信息，它能够有效地检测和降噪常识知识图，并在零样本常识问答任务中表现出优越性能。 |
| [^19] | [Multi-view Contrastive Learning for Entity Typing over Knowledge Graphs.](http://arxiv.org/abs/2310.12008) | 本文提出了一种名为多视角对比学习的新方法，该方法有效地将类型聚类提供的粗粒度知识编码到实体和类型嵌入中，从而改进了知识图谱实体类型判断任务。 |
| [^20] | [Sociotechnical Safety Evaluation of Generative AI Systems.](http://arxiv.org/abs/2310.11986) | 本文提出了一个三层框架，采用社会技术方法对生成型AI系统的安全风险进行评估。同时，评估现状调查发现了三个显著的评估差距，并提出了解决这些差距的方法。 |
| [^21] | [From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers.](http://arxiv.org/abs/2310.11984) | 本文研究了Transformer模型在学习算术算法方面的能力，并通过注意力偏置以及Attention Bias Calibration（ABC）来实现对于长长度的泛化。 |
| [^22] | [InfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation.](http://arxiv.org/abs/2310.11976) | InfoDiffusion是一种非自回归文本扩散模型，通过引入“keyinfo-first”生成策略和基于文本信息量的噪声调度，以及结合自我条件和部分加噪模型结构的方法，提高了生成质量和多样性，并展示出更高的采样效率。 |
| [^23] | [Filling in the Gaps: Efficient Event Coreference Resolution using Graph Autoencoder Networks.](http://arxiv.org/abs/2310.11965) | 填补空白：利用图自编码网络进行高效的事件共指消解，显著优于传统方法，尤其在低资源或低数据环境下表现更好。 |
| [^24] | [AMR Parsing with Causal Hierarchical Attention and Pointers.](http://arxiv.org/abs/2310.11964) | 本文介绍了一种新的AMR解析模型CHAP，它利用因果分层注意力和指针机制将结构整合到Transformer解码器中，实验证明在没有额外数据的情况下，该模型在四个基准测试中优于基线模型。 |
| [^25] | [Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences.](http://arxiv.org/abs/2310.11960) | 提出了一种名为快速多极化注意力的新型注意力机制，它使用分治策略将注意力的时间和内存复杂度从O(n^2)降低到O(n log n)或O(n)，同时保持了全局感知范围。 |
| [^26] | [Emptying the Ocean with a Spoon: Should We Edit Models?.](http://arxiv.org/abs/2310.11958) | 这项研究质疑了直接模型编辑作为纠正LLM生成中事实错误的方法，并提出了与之类似但更为明确的三种替代方法。虽然模型编辑在提升模型可解释性方面有潜力，但不能被视为解决LLMs固有缺点的系统性方法，因为它存在加强模型可信性观念的风险。 |
| [^27] | [MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models.](http://arxiv.org/abs/2310.11954) | MusicAgent是一个使用大型语言模型的AI代理，通过集成音乐相关工具和自主工作流程，帮助用户自动分析需求并调用合适的工具进行音乐处理。 |
| [^28] | [Grounded and Well-rounded: A Methodological Approach to the Study of Cross-modal and Cross-lingual Grounding.](http://arxiv.org/abs/2310.11938) | 本文提出了一种研究跨模态和跨语言基础问题的方法论框架，通过比较不同输入源的质量效应和模型性能，揭示了基础的影响。 |
| [^29] | [Investigating semantic subspaces of Transformer sentence embeddings through linear structural probing.](http://arxiv.org/abs/2310.11923) | 通过语义结构探测实验，本论文研究了Transformer句子嵌入的语义子空间。研究结果表明，不同类型和大小的语言模型在性能和层动力学上存在差异，但模型大小与结果关联较小。 |
| [^30] | [A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs.](http://arxiv.org/abs/2310.11917) | 本文提出了一个用于评估知识图谱中半感应式链接预测模型的大规模基准，基于Wikidata5M进行扩展。通过各种不同的任务和信息组合，该基准为进一步研究上下文和文本信息在链接预测中的整合提供了一个测试平台。 |
| [^31] | [Rather a Nurse than a Physician -- Contrastive Explanations under Investigation.](http://arxiv.org/abs/2310.11906) | 这项研究通过分析四个英文文本分类数据集和人类理性注释，验证了对比性解释与非对比性解释在模型和人类之间的高度一致性。 |
| [^32] | [From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks.](http://arxiv.org/abs/2310.11884) | 本文调查了解释神经网络中概念的最新方法，这对于实现基于可解释概念的神经符号化人工智能来说是重要的一步。 |
| [^33] | [From Dissonance to Insights: Dissecting Disagreements in Rationale Dataset Construction for Case Outcome Classification.](http://arxiv.org/abs/2310.11878) | 本研究关注法律自然语言处理中人工标注的变异问题，通过收集一组律师对案件结果评估存在分歧的数据集，对这些分歧进行了研究，构建了一个两级分类体系，并发现分歧主要源于对法律背景的不明确描述。 |
| [^34] | [The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models.](http://arxiv.org/abs/2310.11877) | 本研究探讨了大型语言模型(LLMs)在面对无法回答的查询时的行为，发现模型能够编码查询的可回答性，并且第一个解码的标记是一个强有力的指示符。这些发现揭示了LLMs潜在表示中的空间组织，并为改进解码技术提供了新的思路。 |
| [^35] | [AI Nushu: An Exploration of Language Emergence in Sisterhood -Through the Lens of Computational Linguistics.](http://arxiv.org/abs/2310.11870) | 本文介绍了一种受女书启发的新兴语言系统AI Nushu，通过计算语言学的视角，结合中国文化遗产和女权主义视角，通过两个AI代理人的合作创造了一个标准的中文写作系统。 |
| [^36] | [Text Annotation Handbook: A Practical Guide for Machine Learning Projects.](http://arxiv.org/abs/2310.11780) | 本手册是一本实践指南，旨在帮助处理文本标注任务。它提供简明扼要的介绍，理论概念的概述以及实用建议，并触及业务、道德和监管问题。适合各种职业的人使用。 |
| [^37] | [Language Agents for Detecting Implicit Stereotypes in Text-to-image Models at Scale.](http://arxiv.org/abs/2310.11778) | 本文介绍了一种针对文本到图像模型中检测刻板印象的语言代理架构，可自主调用各种工具来促进整个检测过程，并应用于商业产品和开放文本数据集。 |
| [^38] | [Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling.](http://arxiv.org/abs/2310.11772) | 本论文通过增强一致性建模的方式改进了长文档主题划分模型。具体地，通过引入主题感知的句子结构预测和对比语义相似性学习，该模型在捕捉语义一致性和主题划分之间的深层关系方面有了更好的表现。 |
| [^39] | [Annotated Job Ads with Named Entity Recognition.](http://arxiv.org/abs/2310.11769) | 本文介绍了一个通过命名实体识别（NER）模型对瑞典职位广告进行筛选的方法，并详细讨论了创建高质量数据集的问题和解决方案，并报告了模型的性能。 |
| [^40] | [A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction.](http://arxiv.org/abs/2310.11761) | 本研究对大规模语言模型在法律判决预测上进行了综合评估，结果表明LLMs可以通过类似案例和多项选择的方式提高其在专家法律推理中的表现。此外，我们发现在某些情况下，较弱的LLMs从强大的信息检索系统中获得的收益有限，从而导致信息检索系统的绩效超越LLM+IR的组合。 |
| [^41] | [Bias in Emotion Recognition with ChatGPT.](http://arxiv.org/abs/2310.11753) | 这项技术报告探讨了ChatGPT在从文本中识别情绪方面的能力，并发现其在不同情绪标签和数据集上的性能有所差异，突显了内在的不稳定性和潜在的偏差。 |
| [^42] | [Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting.](http://arxiv.org/abs/2310.11732) | 本研究系统评估了在多选设置下对齐语言模型不确定性校准的影响。研究发现，在这种设置下存在两种不确定性，分别对答案决策和格式偏好负责。对齐模型过度自信的原因之一是这两种不确定性的混淆。 |
| [^43] | [Quantify Health-Related Atomic Knowledge in Chinese Medical Large Language Models: A Computational Analysis.](http://arxiv.org/abs/2310.11722) | 本研究通过构建一个基准，量化了中文医学大型语言模型中与健康相关的原子知识的存储程度，并发现通用LLMs在原子知识和指令遵循能力方面表现更好。两种类型的LLMs都倾向于迎合用户要求。 |
| [^44] | [Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding.](http://arxiv.org/abs/2310.11721) | 这里是中文总结出的一句话要点 |
| [^45] | [Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning.](http://arxiv.org/abs/2310.11716) | 这项研究提出了一种名为"反射调整"的方法，通过自我改进和判断能力来解决LLMs指令调整中的问题。通过借助Oracle LLM回收训练数据，该方法显著提高了LLMs在各个基准测试中的性能。 |
| [^46] | [Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets.](http://arxiv.org/abs/2310.11715) | 通过利用粗粒度数据集，提出了一种细粒度命名实体识别模型，使用细粒度-粗粒度映射矩阵来显式利用层次结构，并提出了一种不一致性过滤方法，以增强低资源细粒度命名实体识别。 |
| [^47] | [Learning Co-Speech Gesture for Multimodal Aphasia Type Detection.](http://arxiv.org/abs/2310.11710) | 通过学习语音和手势之间的相关性，我们提出了一种多模态图神经网络，用于准确识别特定失语类型的检测。实验证明我们的方法优于现有方法，达到了最先进的结果（F1 84.2%）。 |
| [^48] | [MISAR: A Multimodal Instructional System with Augmented Reality.](http://arxiv.org/abs/2310.11699) | 这项研究介绍了一种创新的方法，利用大语言模型（LLMs）从视觉、听觉和情境多模态中吸收信息，并通过自主视频、语音和语境分析实现增强的状态估计，进一步推动了适应性更强的增强现实（AR）系统的发展。 |
| [^49] | [Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs.](http://arxiv.org/abs/2310.11689) | 本研究提出了一种自适应框架，利用自我评估来改进大型语言模型（LLMs）的选择性预测能力。该方法基于参数效率调整，能够适应特定任务并提高其自我评估能力，实验结果表明其优于最先进的选择性预测方法。 |
| [^50] | [Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention.](http://arxiv.org/abs/2310.11685) | 通过对softmax和线性注意力机制进行全面比较分析，本论文揭示了softmax注意力在大多数情况下优于线性注意力的潜在原因。 |
| [^51] | [Descriptive Knowledge Graph in Biomedical Domain.](http://arxiv.org/abs/2310.11681) | 这个论文提出了一个在生物医学领域中构建描述性知识图谱的新系统，可以从文献中提取有信息量的句子并进行关系搜索和导航。并且该系统使用自动生成描述性句子的模型，减少了人工阅读的工作量。在COVID-19研究中应用该系统展示了其在相关领域的实用性。 |
| [^52] | [Open-ended Commonsense Reasoning with Unrestricted Answer Scope.](http://arxiv.org/abs/2310.11672) | 本论文研究了无限制答案范围的开放式常识推理问题。作者采用预训练语言模型在外部知识库上迭代检索推理路径，从而帮助找到最准确的答案。实验结果表明此方法在常识问题上取得了良好的效果。 |
| [^53] | [MixEdit: Revisiting Data Augmentation and Beyond for Grammatical Error Correction.](http://arxiv.org/abs/2310.11671) | 本研究通过引入亲和度和多样性两种度量方法，阐明了数据增强如何改进语法错误修正模型。基于此观察结果，提出了一种名为MixEdit的数据增强方法，可以策略性和动态地增强真实数据，无需额外的单语言数据。 |
| [^54] | [Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning.](http://arxiv.org/abs/2310.11670) | 基于原型的超适配器（PHA）框架用于样本高效多任务调整，通过引入实例密集的检索器和样本高效的原型超网络生成条件模块，在多任务学习和少样本迁移学习中取得了可比性能的提升，甚至在数据量较小时也能超过其他强基线方法的性能。 |
| [^55] | [SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents.](http://arxiv.org/abs/2310.11667) | SOTOPIA是一个用于评估语言智能中的社交智能的交互式环境。通过模拟复杂的社交互动，并使用全面的评估框架，我们发现不同模型之间的社交智能存在显著差异，特别是在SOTOPIA-hard情景下。GPT-4在这个子集上的目标完成率较低。 |
| [^56] | [Field-testing items using artificial intelligence: Natural language processing with transformers.](http://arxiv.org/abs/2310.11655) | 这项研究使用了一种名为RoBERTa的人工智能模型来进行英语读写能力考试的现场测试，结果显示模型的行为与人类考生数据有一定的一致性。 |
| [^57] | [Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model.](http://arxiv.org/abs/2310.11648) | 本文提出了使用基础语言模型进行零样本忠实性评估的方法，并引入了一种新的度量标准FFLM。实验证明，FFLM在不一致性检测和忠实性评级方面表现优异，并且参数数量减少了24倍。 |
| [^58] | [Systematic Assessment of Factual Knowledge in Large Language Models.](http://arxiv.org/abs/2310.11638) | 本研究提出了一个通过利用知识图谱来评估大型语言模型中事实知识的框架，并在通用和特定领域中对最先进的模型进行了系统的评估。实验结果表明，ChatGPT是在所有领域中表现最好的模型。 |
| [^59] | [MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations.](http://arxiv.org/abs/2310.11634) | 本文评估了大型语言模型在上下文学习能力上对新颖解释的概括能力，并证明了大型语言模型能够理解新颖解释，尤其是从自然语言描述和长对话中获取的解释。然而，仍然需要进一步改进。 |
| [^60] | [Learn Your Tokens: Word-Pooled Tokenization for Language Modeling.](http://arxiv.org/abs/2310.11628) | 使用"学习您的标记"方案，利用单词边界将字节/字符汇聚成单词表示形式，以改善标记化策略的局限性并提高语言模型的性能。 |
| [^61] | [Unveiling the General Intelligence Factor in Language Models: A Psychometric Approach.](http://arxiv.org/abs/2310.11616) | 本研究利用心理测量理论揭示了语言模型中的普遍智能因子g的存在，并发现了该因子解释模型性能方差的85%，为模型评估和开发提供了统一的指标。 |
| [^62] | [Language Models as Zero-Shot Trajectory Generators.](http://arxiv.org/abs/2310.11604) | 本文研究了使用大型语言模型（LLMs）作为零-shot轨迹生成器的可能性。通过给予LLM物体检测和分割视觉模型的访问权限，研究人员发现LLMs能够直接预测操作技能中的末端执行器姿态序列，并在26个真实世界的语言任务中取得了良好效果。这一研究突破了对LLMs在机器人技术中的限制，揭示了LLMs确实具有对操作任务的理解能力。 |
| [^63] | [Automated Evaluation of Personalized Text Generation using Large Language Models.](http://arxiv.org/abs/2310.11593) | 这项研究提出了一种使用大型语言模型自动评价个性化文本生成的方法。传统的自动评价指标无法捕捉个性化质量的微妙差别，而人工判断又昂贵且困难。因此，本研究提出了一种新颖的评估方法，能够自动测量个性化、质量和相关性这三个重要语义方面。 |
| [^64] | [Eliciting Human Preferences with Language Models.](http://arxiv.org/abs/2310.11589) | 本文介绍了一种生成式主动任务引导（GATE）的学习框架，该框架通过与用户进行自由形式的、基于语言的交互来引导和推断预期行为。在实验中展示，通过GATE引导的语言模型通常比用户编写的提示或标签更具信息量。 |
| [^65] | [BasahaCorpus: An Expanded Linguistic Resource for Readability Assessment in Central Philippine Languages.](http://arxiv.org/abs/2310.11584) | BasahaCorpus 是一个扩展菲律宾中央语系语言可读性评估的语料库，利用表层特征、音节模式和n-gram重叠特征训练了ARA模型，并提出了一种新的层次化跨语言建模方法；研究发现跨语言模型在低资源环境中是有效的。 |
| [^66] | [What is a good question? Task-oriented asking with fact-level masking.](http://arxiv.org/abs/2310.11571) | 本论文提出了基于任务的询问（TOA）的概念和框架，介绍了一种用于生成对推理任务有用答案的问题的方法。同时还提出了一种事实级遮蔽（FLM）的技术，用于将自然语言数据集转换为自我监督的TOA数据集。 |
| [^67] | [Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging.](http://arxiv.org/abs/2310.11564) | 本研究研究了将大型语言模型与个性化的人类反馈对齐的问题，通过将对齐建模为多目标强化学习，将偏好分解为多个维度，可以实现个性化对齐，并通过参数合并进行有效组合。 |
| [^68] | [MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning.](http://arxiv.org/abs/2310.11541) | 本文提出了一种多语言和统一音节标记的文本和音韵领域中的语音表示学习方法，通过自动音节化单词并生成宝贵注释，适用于语音表示学习、语音单元发现和语音因素解缠。 |
| [^69] | [Multi-stage Large Language Model Correction for Speech Recognition.](http://arxiv.org/abs/2310.11532) | 本文提出了一种多阶段的方法，通过结合传统语言模型重新评分和大型语言模型提示，在语音识别中获得了显著的性能提升。 |
| [^70] | [Group Preference Optimization: Few-Shot Alignment of Large Language Models.](http://arxiv.org/abs/2310.11523) | 这项研究介绍了一种名为群体偏好优化（GPO）的对齐框架，可以以少样本的方式将大规模语言模型（LLMs）引导到个别群体的偏好。通过在基本LLM上加入独立的transformer模块来预测群体偏好，并通过元学习进行训练，GPO经过严格评估验证了其有效性。 |
| [^71] | [Automatic News Summerization.](http://arxiv.org/abs/2310.11520) | 本研究比较和评估了抽取式和生成式方法在新闻文本摘要上的效果，并使用ROUGE得分进行质量评估。最佳表现模型被集成到一个Web应用程序中，以评估其在现实世界中的能力和用户体验。 |
| [^72] | [Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.](http://arxiv.org/abs/2310.11511) | Self-RAG是一种通过检索和自我反思提高语言模型质量和事实性的框架。 |
| [^73] | [CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations.](http://arxiv.org/abs/2310.11501) | 利用CoMPosT框架，我们提出了一种对LLM模拟进行特征化的方法，评估其是否存在夸张刻板化，并发现在某些情况下存在夸张刻板化的现象。 |
| [^74] | [BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in Bangla with Multi-Feature and Multi-Modal Analysis.](http://arxiv.org/abs/2310.11465) | 本研究提出了一个大规模的孟加拉语YouTube clickbait多模态数据集，为研究人员提供了在低资源语言中建模clickbait现象的重要价值，并且可以开发出更复杂的跨语言检测方法。 |
| [^75] | [Harnessing the Power of LLMs: Evaluating Human-AI text Co-Creation through the Lens of News Headline Generation.](http://arxiv.org/abs/2310.10706) | 该研究通过对LLMs辅助新闻标题生成的人工智能协作方法进行比较，发现引导和选择模型输出能够带来最大的效益，并且与自由编辑相比并不损害参与者对控制的感知。 |
| [^76] | [Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking.](http://arxiv.org/abs/2310.10520) | 本论文提出了ParsingDST方法，利用大型语言模型和语义解析技术，实现了复杂的零样本对话状态跟踪的更新策略，并在实验中展示了明显的改进。 |
| [^77] | [Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models.](http://arxiv.org/abs/2310.10449) | 本研究通过比较MPT-7b-instruct, Falcon-7b-instruct和OpenAI Chat-GPT模型，在不同的数据集上使用不同的超参数进行了文本摘要实验。实验结果表明，text-davinci-003模型表现最佳，并且提供了大型语言模型在文本摘要中的性能综述。 |
| [^78] | [Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models.](http://arxiv.org/abs/2310.10378) | 本论文研究了多语言预训练语言模型中事实知识的跨语言一致性，提出了一种新的度量方法，并通过分析模型大小、语言配对等因素发现了影响一致性的因素。实验结果表明，增加模型大小可以提高准确性，但不会改善跨语言一致性。 |
| [^79] | [Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting.](http://arxiv.org/abs/2310.09716) | 该论文提出了一种利用大型语言模型(LLMs)作为查询重写器的方法，通过指令生成信息丰富的查询重写，以提升对话式搜索的检索性能。实验结果表明，这种方法在QReCC数据集上取得了良好的效果。 |
| [^80] | [An Expression Tree Decoding Strategy for Mathematical Equation Generation.](http://arxiv.org/abs/2310.09619) | 本研究提出了一种表达树解码策略，将树结构整合到数学方程生成中，以解决当前顺序方法忽视并行和依赖关系的问题。 |
| [^81] | [BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models for Sentiment Analysis of Bangla Social Media Posts.](http://arxiv.org/abs/2310.09238) | 本论文提出了针对Bangla社交媒体帖子进行情感分析的任务，通过实验发现在这种低资源语言场景下，使用Transformer模型进行迁移学习可以提高模型的学习效果，并且在对已经在Twitter数据上进行了情感分析任务的模型进一步微调的情况下，模型的性能最好。 |
| [^82] | [Developing a Natural Language Understanding Model to Characterize Cable News Bias.](http://arxiv.org/abs/2310.09166) | 本论文开发了一种无监督的机器学习方法，通过对有线电视节目提及的主题进行命名实体识别和立场分析的方式，来表征其偏见。在2020年的有线电视转录中应用该方法，发现节目聚类与节目所属的有线电视网络保持一致。揭示了客观评估媒体偏见和表征陌生媒体环境的潜力。 |
| [^83] | [Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach.](http://arxiv.org/abs/2310.08172) | 本研究通过教育诊断评估方法，揭示了大型语言模型（LLMs）的知识结构，强调了研究LLMs的认知能力和不同认知模式的重要性。 |
| [^84] | [The Expresssive Power of Transformers with Chain of Thought.](http://arxiv.org/abs/2310.07923) | 本论文研究基于思维链的Transformer的表达能力，通过允许使用中间生成的方式提高了Transformer的推理能力，并发现线性数量的解码步骤在标准计算复杂度下增加了明显的新能力。 |
| [^85] | [Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity.](http://arxiv.org/abs/2310.07521) | 这项调查研究了大型语言模型（LLMs）中的事实性问题，包括其可能带来的后果和挑战，以及存储、处理和评估事实的方法。同时，提出了一些增强LLM事实性的策略，涵盖了不同领域的需求。 |
| [^86] | [Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction.](http://arxiv.org/abs/2310.07487) | Cognate Transformer是一个用于自动语音重建和同源反射预测的模型，基于多序列对齐数据进行训练，并在计算历史语言学领域取得了良好的表现。 |
| [^87] | [Online Speculative Decoding.](http://arxiv.org/abs/2310.07177) | 在线推测解码是通过利用多余计算能力，在LLM服务集群中持续更新草稿模型，从而加速大型语言模型推理的一种方法。 |
| [^88] | [Visual Storytelling with Question-Answer Plans.](http://arxiv.org/abs/2310.05295) | 本论文引入了一种新颖的框架，将视觉表示与预训练语言模型和计划相结合，从图像序列中生成更具连贯性、趣味性和自然性的叙事。 |
| [^89] | [Crystal: Introspective Reasoners Reinforced with Self-Feedback.](http://arxiv.org/abs/2310.04921) | 提出了一种名为Crystal的内省型常识推理器，通过内省知识和基于知识的推理相结合，提高了常识推理的性能和解释能力。 |
| [^90] | [In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations.](http://arxiv.org/abs/2310.00313) | 本研究通过神经科学启发的技术，研究了大型语言模型中上下文学习的机制，通过调查嵌入和注意力的变化，揭示了这种改进背后的潜在变化，并提出了用于参数化探测和注意力比率分析的新方法。 |
| [^91] | [Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding.](http://arxiv.org/abs/2309.15028) | 本文提出了一种基于值导向的Monte-Carlo Tree Search解码算法PPO-MCTS，通过在PPO之上集成MCTS，解决了训练和测试之间部分输出评分机制的不匹配问题，实验证明该算法可以显著提升性能。 |
| [^92] | [Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI.](http://arxiv.org/abs/2309.12444) | 这篇论文提出了基于生成AI的医疗对话模型的评估指标问题，并强调了现有指标对医学和健康概念的理解不足以及忽略了用户体验因素。 |
| [^93] | [Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge.](http://arxiv.org/abs/2307.08813) | 本研究评估了不同大型语言模型在提取分子相互作用和通路知识方面的有效性，并讨论了未来机遇和挑战。 |
| [^94] | [Replay to Remember: Continual Layer-Specific Fine-tuning for German Speech Recognition.](http://arxiv.org/abs/2307.07280) | 该论文探讨了如何通过持续层特定微调和经验回放技术来改善德语语音识别模型在较小领域中的性能，并提高模型的鲁棒性。 |
| [^95] | [Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias.](http://arxiv.org/abs/2306.15895) | 本论文研究了大型语言模型作为属性化训练数据生成器的应用。通过使用具有多样性属性的提示，我们能够生成多样化且归因的数据。研究表明，在高基数和多样领域的数据集中，使用属性化提示对生成模型性能有积极影响。此外，论文还展示了关于偏差、多样性和效率的全面实证研究结果，并得出了三个关键观察：系统性偏差存在于生成数据中，多样性和效率之间存在权衡，属性化训练数据生成可以改善模型性能。 |
| [^96] | [Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models.](http://arxiv.org/abs/2305.18703) | 本文综述了大型语言模型的领域专门化，包括动机、挑战、方法论和评估指标。此外，还提供了一个特定领域任务和数据集的分类法，对现有的领域自适应和定制技术进行了详细比较，并广泛讨论了这一领域中的未解决问题和未来的发展方向。 |
| [^97] | [Difference-Masking: Choosing What to Mask in Continued Pretraining.](http://arxiv.org/abs/2305.14577) | 本文提出了一种自动选取遮蔽数据的方法（Difference-Masking），以提高在继续预训练中的自监督学习模型的性能，方法是通过考虑未标记的目标域与预训练域的不同之处来进行。实验证明，该方法可以有效地提升继续预训练任务的性能，且具有跨任务的适用性。 |
| [^98] | [Question Answering as Programming for Solving Time-Sensitive Questions.](http://arxiv.org/abs/2305.14221) | 该论文提出了一种将问题回答任务重新构架为编程任务的方法，通过利用现代语言模型的能力，设计能够解决时效性问题的程序。 |
| [^99] | [When Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale.](http://arxiv.org/abs/2305.14124) | 本文研究了单语言数据在多语言翻译中的作用，发现单语言数据通常有助于多语言翻译，但模型对领域不匹配的容忍性较差，尤其在较小的模型规模下。回译在数据源相似的情况下是有益的，但在其他情况下可能是有害的，而去噪自编码的效果不如先前报告的好。规模对两种方法都很重要。 |
| [^100] | [MADNet: Maximizing Addressee Deduction Expectation for Multi-Party Conversation Generation.](http://arxiv.org/abs/2305.12733) | MADNet提出了一种在多方对话生成中最大化地址推断期望值的方法，通过设计额外的潜在边来确保对话片段之间的消息传递，以解决地址标签稀缺的问题。 |
| [^101] | [Multilingual Simplification of Medical Texts.](http://arxiv.org/abs/2305.12532) | 本研究介绍了MultiCochrane，这是医学领域中第一个句子对齐的多语言文本简化数据集，通过多语言简化直接将复杂文本简化为多种语言的简化文本。 |
| [^102] | [Prompt ChatGPT In MNER: Improved multimodal named entity recognition method based on auxiliary refining knowledge from ChatGPT.](http://arxiv.org/abs/2305.12212) | 本文提出了一个叫做 PGIM 的方法，该方法利用 ChatGPT 作为隐式知识引擎来获取辅助精炼知识，从而增强模型在 MNER 任务中的性能。 |
| [^103] | [Learn to Compose Syntactic and Semantic Representations Appropriately for Compositional Generalization.](http://arxiv.org/abs/2305.12169) | 该研究提出了一个名为COMPSITION的新框架，可以通过适当地组合句法和语义表示来解决组合泛化问题。实验证明该方法在合成和自然语言CG任务上实现了最先进的性能。 |
| [^104] | [Revisiting Entropy Rate Constancy in Text.](http://arxiv.org/abs/2305.12084) | 本论文使用神经语言模型重新评估了基于n-gram语言模型下英文文本的概率提出的熵率恒定原理，未能找到明显的支持熵率恒定的证据。 |
| [^105] | [Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews.](http://arxiv.org/abs/2305.11828) | 本文研究了使用LLM协助制作医学证据综述的潜在用途和风险，指出LLM有可能自动生成文献综述，但由于可能出现虚构或遗漏信息的情况，LLM的使用需要谨慎。 |
| [^106] | [Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate.](http://arxiv.org/abs/2305.11595) | 本文提出了通过辩论探究大型语言模型之间的内部一致性问题，实验证明通过严格的辩论框架可以提高模型性能和常识知识的结构化学习。 |
| [^107] | [Plug-and-Play Medical Dialogue System.](http://arxiv.org/abs/2305.11508) | 该论文提出了一种即插即用的医疗对话系统，使用大型语言模型实现医疗问答及诊断策略，避免了传统昂贵的LLMs微调。 |
| [^108] | [Cross-modality Data Augmentation for End-to-End Sign Language Translation.](http://arxiv.org/abs/2305.11096) | 本文提出了一种Cross-modality Data Augmentation（XmDA）框架，通过利用来自手语单词翻译模型的伪手语单词-文本对，将强大的手语单词到文本的翻译能力转移到了端到端手语翻译，实验结果表明XmDA在该领域中明显优于现有的最先进方法。 |
| [^109] | [Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks.](http://arxiv.org/abs/2305.10160) | 提出了三个适用策略：（1）公钥加密发布测试数据，仅允许特定派生发布；（2）对于API持有方，要求训练排除控制，保护测试数据，不停止评估直到达到要求；（3）如果测试数据来自互联网文本，需避免某些结果的使用。 |
| [^110] | [Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements.](http://arxiv.org/abs/2305.03695) | 本文提出了Vera模型，它是一个通用模型，可以基于常识知识估计陈述性语句的可信度。在解决验证格式的常识问题时，Vera明显优于现有的模型，并展现了对未见任务的泛化能力和良好的标定输出。 |
| [^111] | [TempoSum: Evaluating the Temporal Generalization of Abstractive Summarization.](http://arxiv.org/abs/2305.01951) | 本篇论文提出了 TempoSum 抽象摘要的时间泛化能力基准，通过广泛的人类评估证明了摘要模型中存储的参数化知识对未来数据上生成的摘要有显著影响。 |
| [^112] | [RexUIE: A Recursive Method with Explicit Schema Instructor for Universal Information Extraction.](http://arxiv.org/abs/2304.14770) | 本文提出了一种名为RexUIE的方法，采用递归机制和显式模式说明，实现通用信息提取(UIE)。与以往的模型相比，RexUIE在提取不同模式时不会出现冲突，并且显式模式说明器有助于提高模型的泛化能力和性能，特别是在低资源情况下。 |
| [^113] | [Answering Questions by Meta-Reasoning over Multiple Chains of Thought.](http://arxiv.org/abs/2304.13007) | 本论文提出了基于元推理的Multi-Chain Reasoning (MCR)方法，该方法检查多个推理链，混合它们之间的信息并选择最相关的事实，从而超越多链思维，解决多跳QA问题。 实验结果表明MCR胜过多个强基线，解释质量高。 |
| [^114] | [SkillGPT: a RESTful API service for skill extraction and standardization using a Large Language Model.](http://arxiv.org/abs/2304.11060) | SkillGPT是一种使用大型语言模型进行技能提取和标准化的RESTful API服务，通过摘要和向量相似性搜索平衡速度和准确度。 |
| [^115] | [Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense.](http://arxiv.org/abs/2303.13408) | 本文探究了语义转换对于AI生成文本检测器的鲁棒性，通过训练的语义转换生成模型成功混淆了多个检测器。 |
| [^116] | [Mixture of Soft Prompts for Controllable Data Generation.](http://arxiv.org/abs/2303.01580) | 这项研究提出了一种称为软提示混合模型（MSP）的方法，将大型语言模型（LLM）用于数据增强而不是直接预测。实验结果显示，MSP能够生成多样且自然的文本，同时保留标签语义，并在三个基准测试中取得了最先进的结果。 |
| [^117] | [Grounding Complex Natural Language Commands for Temporal Tasks in Unseen Environments.](http://arxiv.org/abs/2302.11649) | 该论文提出了一个名为Lang2LTL的模块化系统和软件包，利用大型语言模型将导航命令与LTL规范进行关联。通过在没有先前语言数据的环境中全面评估Lang2LTL，证明了其在21个城市级规模的环境中与各种时态规范进行关联的最先进能力。并且通过展示物理机器人在两个室内环境中可以遵循52个语义多样的导航命令，进一步验证了Lang2LTL的实际应用价值。 |
| [^118] | [Video-Text Retrieval by Supervised Sparse Multi-Grained Learning.](http://arxiv.org/abs/2302.09473) | 本文提出了一种新的多粒度稀疏学习框架S3MA，用于视频文本检索。该框架通过学习共享的稀疏空间和多粒度相似度来改进检索效果，在多个基准测试中表现优于现有方法。 |
| [^119] | [Chain of Hindsight Aligns Language Models with Feedback.](http://arxiv.org/abs/2302.02676) | 该研究提出了一种新颖的技术，即回顾链，可以轻松优化，并可以从任何形式的反馈中学习，而不受其极性的影响。 |
| [^120] | [Learning List-Level Domain-Invariant Representations for Ranking.](http://arxiv.org/abs/2212.10764) | 本文提出了一种针对排名问题的列表级别对齐的学习方法，该方法利用列表的结构特性，在领域适应中实现从源领域到目标领域的知识转移。 |
| [^121] | [Seq2Seq-SC: End-to-End Semantic Communication Systems with Pre-trained Language Model.](http://arxiv.org/abs/2210.15237) | Seq2Seq-SC是一个具备预训练语言模型的端到端语义通信系统，通过专注于消息含义实现前所未有的通信效率，并且在提取语义信息上表现出卓越性能，为语义通信的发展以及6G网络中与未来无线系统的整合开辟了新的可能性。 |
| [^122] | [YATO: Yet Another deep learning based Text analysis Open toolkit.](http://arxiv.org/abs/2209.13877) | YATO是一个轻量且易于使用的开源工具包，支持深度学习文本分析，可组合不同的特征类型，并提供灵活性和易用性的优势，促进了先进NLP模型的复现和改进，以及跨学科应用的推动。 |

# 详细

[^1]: 理解用于长篇问答的检索增强

    Understanding Retrieval Augmentation for Long-Form Question Answering. (arXiv:2310.12150v1 [cs.CL])

    [http://arxiv.org/abs/2310.12150](http://arxiv.org/abs/2310.12150)

    这项研究分析了长篇问答中的检索增强语言模型的影响，研究了生成答案的属性和归因模式，并找出了归因错误的主要原因。研究结果对长篇、知识丰富的文本生成提供了新的见解。

    

    我们在长篇问答中提出了一项检索增强的语言模型（LMs）研究。我们通过比较使用相同证据文档的模型生成的答案，分析了检索增强对不同LMs的影响，以及检索文档集质量对相同LMs生成的答案的影响。我们研究了生成答案的各种属性（例如，流畅度、长度、变异性），重点在于将生成的长篇答案归因于文本中的证据文档。我们进行了答案归因的人工标注并评估了自动评判归因的方法。我们的研究为检索增强如何影响LMs生成长篇、知识丰富的文本提供了新的见解。我们进一步确定了长文本生成的归因模式并分析了归因错误的主要原因。综上所述，我们的分析揭示了检索增强对长篇、知识丰富的文本生成的影响，并提供了方向。

    We present a study of retrieval-augmented language models (LMs) on long-form question answering. We analyze how retrieval augmentation impacts different LMs, by comparing answers generated from models while using the same evidence documents, and how differing quality of retrieval document set impacts the answers generated from the same LM. We study various attributes of generated answers (e.g., fluency, length, variance) with an emphasis on the attribution of generated long-form answers to in-context evidence documents. We collect human annotations of answer attribution and evaluate methods for automatically judging attribution. Our study provides new insights on how retrieval augmentation impacts long, knowledge-rich text generation of LMs. We further identify attribution patterns for long text generation and analyze the main culprits of attribution errors. Together, our analysis reveals how retrieval augmentation impacts long knowledge-rich text generation and provide directions for 
    
[^2]: 简单机制用于表示、索引和操作概念

    Simple Mechanisms for Representing, Indexing and Manipulating Concepts. (arXiv:2310.12143v1 [cs.LG])

    [http://arxiv.org/abs/2310.12143](http://arxiv.org/abs/2310.12143)

    通过查看概念的矩阵统计量，生成一个概念的具体表示或签名，可以用于发现概念之间的结构并递归产生更高级的概念，同时可以通过概念的签名来找到相关的共同主题。

    

    深度网络通常通过分类器学习概念，这涉及设置模型并通过梯度下降训练它以适应具有标记概念的数据。我们将提出一个不同的观点，即可以通过查看概念的矩阵矩阵统计量来生成概念的具体表示或签名。这些签名可以用于发现一组概念的结构，并且可以通过从这些签名中学习该结构来递归地产生更高级的概念。当概念"相交"时，概念的签名可以用于在一些相关的"相交"概念中找到一个共同的主题。这个过程可以用于保持一个概念字典，以便输入能够正确识别并被路由到与输入的(潜在)生成相关的概念集合中。

    Deep networks typically learn concepts via classifiers, which involves setting up a model and training it via gradient descent to fit the concept-labeled data. We will argue instead that learning a concept could be done by looking at its moment statistics matrix to generate a concrete representation or signature of that concept. These signatures can be used to discover structure across the set of concepts and could recursively produce higher-level concepts by learning this structure from those signatures. When the concepts are `intersected', signatures of the concepts can be used to find a common theme across a number of related `intersected' concepts. This process could be used to keep a dictionary of concepts so that inputs could correctly identify and be routed to the set of concepts involved in the (latent) generation of the input.
    
[^3]: 伪智能: 一种用于语言模型评估的统一框架

    Pseudointelligence: A Unifying Framework for Language Model Evaluation. (arXiv:2310.12135v1 [cs.CL])

    [http://arxiv.org/abs/2310.12135](http://arxiv.org/abs/2310.12135)

    本文提出了一种称为伪智能的统一框架，用于语言模型的评估。这个框架认为智能的评估取决于观察者，并提出了一种基于复杂性理论的动态互动评估方法 。

    

    随着大型语言模型在越来越多的基准测试中超越人类表现，我们必须采取一种有针对性的方法来评估模型的能力。受到伪随机性的启发，我们提出了伪智能，它捕捉了“（被感知的）智能在于观察者的眼中”的最大化原则。也就是说，智能的主张只有在考虑到评估者时才有意义。具体来说，我们提出了一个基于复杂性理论的模型评估框架，将其构建为模型和学习评估者之间的动态互动。我们证明了这个框架可以用于研究语言模型评估中的两个案例研究，并分析现有的评估方法。

    With large language models surpassing human performance on an increasing number of benchmarks, we must take a principled approach for targeted evaluation of model capabilities. Inspired by pseudorandomness, we propose pseudointelligence, which captures the maxim that "(perceived) intelligence lies in the eye of the beholder". That is, that claims of intelligence are meaningful only when their evaluator is taken into account. Concretely, we propose a complexity-theoretic framework of model evaluation cast as a dynamic interaction between a model and a learned evaluator. We demonstrate that this framework can be used to reason about two case studies in language model evaluation, as well as analyze existing evaluation methods.
    
[^4]: DiagrammerGPT: 通过LLM规划生成开放领域、开放平台的图表

    DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning. (arXiv:2310.12128v1 [cs.CV])

    [http://arxiv.org/abs/2310.12128](http://arxiv.org/abs/2310.12128)

    DiagrammerGPT是一个通过LLM规划生成开放领域、开放平台的图表的框架，填补了T2I模型在图表生成方面的空白。

    

    过去几年，文本到图像（T2I）生成取得了显著的发展。尽管如此，在使用T2I模型生成图表方面的研究很少。图表是一种使用结构丰富和空间复杂的可视化来解释信息的符号/示意性表示（例如，一种密集的相关对象、文本标签、方向箭头、连接线等组合）。现有的最先进的T2I模型在生成图表时经常失败，因为它们在许多对象通过复杂的关系（如箭头/线）密集连接时缺乏细粒度的对象布局控制，并且经常不能渲染出可理解的文本标签。为了填补这一空白，我们提出了DiagrammerGPT，一个新颖的两阶段文本到图表生成框架，它利用LLM（如GPT-4）的布局引导能力来生成更准确的开放领域、开放平台的图表。在第一阶段，我们使用LLM生成和迭代改进“图表规划”（在一个规划方案中）。

    Text-to-image (T2I) generation has seen significant growth over the past few years. Despite this, there has been little work on generating diagrams with T2I models. A diagram is a symbolic/schematic representation that explains information using structurally rich and spatially complex visualizations (e.g., a dense combination of related objects, text labels, directional arrows, connection lines, etc.). Existing state-of-the-art T2I models often fail at diagram generation because they lack fine-grained object layout control when many objects are densely connected via complex relations such as arrows/lines and also often fail to render comprehensible text labels. To address this gap, we present DiagrammerGPT, a novel two-stage text-to-diagram generation framework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4) to generate more accurate open-domain, open-platform diagrams. In the first stage, we use LLMs to generate and iteratively refine 'diagram plans' (in a planne
    
[^5]: 代词故事：可解释性指导下的公平指导机器翻译中的性别偏见缓解

    A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation. (arXiv:2310.12127v1 [cs.CL])

    [http://arxiv.org/abs/2310.12127](http://arxiv.org/abs/2310.12127)

    本研究通过调查机器翻译模型中的性别偏见问题以及缓解性别偏见的方法来填补现有研究的空白。研究发现指导微调模型在默认为男性翻译上存在性别偏见，同时忽视了指示职业性别的代词，并提出了一些可行的缓解策略。

    

    最近的指导微调模型可在多个NLP任务中解决问题，其中机器翻译（MT）是一个突出的用例。然而，当前的研究通常集中在标准性能基准上，忽视了引人注目的公平和伦理考虑。在MT中，这可能导致性别错误的翻译，从而导致刻板印象和偏见的持续存在。在这项工作中，我们通过调查这些模型在机器翻译中是否存在性别偏见以及如何缓解性别偏见来填补这一空白。具体而言，我们在从英文到德文和西班牙文的WinoMT语料库上计算已建立的性别偏见指标。我们发现指导微调模型默认为男性屈从翻译，甚至忽视女性职业刻板印象。接下来，使用可解释性方法，我们揭示了模型系统性地忽视指示目标职业性别的代词在同时性别错误的翻译中。最后，根据可解释性的发现，我们提出了性别偏见缓解的策略，并将其应用于MT模型中。

    Recent instruction fine-tuned models can solve multiple NLP tasks when prompted to do so, with machine translation (MT) being a prominent use case. However, current research often focuses on standard performance benchmarks, leaving compelling fairness and ethical considerations behind. In MT, this might lead to misgendered translations, resulting, among other harms, in the perpetuation of stereotypes and prejudices. In this work, we address this gap by investigating whether and to what extent such models exhibit gender bias in machine translation and how we can mitigate it. Concretely, we compute established gender bias metrics on the WinoMT corpus from English to German and Spanish. We discover that IFT models default to male-inflected translations, even disregarding female occupational stereotypes. Next, using interpretability methods, we unveil that models systematically overlook the pronoun indicating the gender of a target occupation in misgendered translations. Finally, based on 
    
[^6]: SHARCS：通过动态宽度子网络进行路由的高效Transformer

    SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks. (arXiv:2310.12126v1 [cs.LG])

    [http://arxiv.org/abs/2310.12126](http://arxiv.org/abs/2310.12126)

    SHARCS是一种高效的Transformer模型，通过动态宽度子网络进行路由，实现自适应推理和更高的效率，同时在各种分类任务中表现优越并且具有通用性。

    

    我们引入了SHARCS，用于自适应推理，考虑到输入样本的难度。SHARCS可以在任何Transformer网络上训练一个路由器，使模型能够将不同样本指向具有不同宽度的子网络。我们的实验证明：（1）在准确性与FLOPs之间，SHARCS在各种分类任务上的表现优于或补充了现有的每个样本自适应推理方法；（2）SHARCS在不同架构之间具有泛化能力，甚至可以应用于压缩和高效的Transformer编码器，以进一步提高其效率；（3）SHARCS可以提供2倍的推理加速，几乎不损失准确性。

    We introduce SHARCS for adaptive inference that takes into account the hardness of input samples. SHARCS can train a router on any transformer network, enabling the model to direct different samples to sub-networks with varying widths. Our experiments demonstrate that: (1) SHARCS outperforms or complements existing per-sample adaptive inference methods across various classification tasks in terms of accuracy vs. FLOPs; (2) SHARCS generalizes across different architectures and can be even applied to compressed and efficient transformer encoders to further improve their efficiency; (3) SHARCS can provide a 2 times inference speed up at an insignificant drop in accuracy.
    
[^7]: 利用数据集制图在Transformer中改善组合泛化能力

    Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers. (arXiv:2310.12118v1 [cs.CL])

    [http://arxiv.org/abs/2310.12118](http://arxiv.org/abs/2310.12118)

    本研究引入了一种利用数据集制图的方法，通过识别组合泛化数据子集，实现了在Transformer中显著提高模型准确性的创新。实验结果显示，该方法在CFQ和COGS数据集上的性能提升高达10%。同时，该技术将数据集制图作为课程学习准则，消除了超参数调整的需求，获得了卓越的性能。

    

    神经网络在语言建模和各种下游任务中取得了革命性的进展。然而，这些模型在组合泛化方面能否达到与人类认知能力相媲美的程度仍然存在争议。虽然该领域现有的方法主要关注于新颖的架构和替代的学习范式，但我们引入了一种开创性的方法，利用数据集制图的力量（Swayamdipta等，2020）。通过使用这种方法策略性地识别一部分组合泛化数据子集，我们在模型准确性方面取得了显著的改进，CFQ和COGS数据集的性能提升高达10%。值得注意的是，我们的技术将数据集制图作为课程学习准则，消除了超参数调整的需要，同时始终实现了卓越的性能。我们的研究结果凸显了数据集制图在发挥组合泛化的全部潜力方面的未开发潜力。

    Neural networks have revolutionized language modeling and excelled in various downstream tasks. However, the extent to which these models achieve compositional generalization comparable to human cognitive abilities remains a topic of debate. While existing approaches in the field have mainly focused on novel architectures and alternative learning paradigms, we introduce a pioneering method harnessing the power of dataset cartography (Swayamdipta et al., 2020). By strategically identifying a subset of compositional generalization data using this approach, we achieve a remarkable improvement in model accuracy, yielding enhancements of up to 10% on CFQ and COGS datasets. Notably, our technique incorporates dataset cartography as a curriculum learning criterion, eliminating the need for hyperparameter tuning while consistently achieving superior performance. Our findings highlight the untapped potential of dataset cartography in unleashing the full capabilities of compositional generalizat
    
[^8]: 非侵入式自适应：面向多模态建模的输入中心参数高效微调

    Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling. (arXiv:2310.12100v1 [cs.CL])

    [http://arxiv.org/abs/2310.12100](http://arxiv.org/abs/2310.12100)

    这篇论文介绍了一种非侵入式的参数高效微调技术（AdaLink），通过只调整模型的外部参数而保持内部结构不变，实现了对多模态建模的竞争性能，这对于大规模语言模型和视觉语言模型的自适应和部署具有重要意义。

    

    大规模语言模型（LLMs）和视觉语言模型（VLMs）通过将参数数量从O（10^9）扩展到O（10^{12}）甚至更高水平，展现出在广泛任务上出色的性能。这样大规模的模型使得在给定感兴趣的任务上进行完全专业化模型的自适应和部署成为不可能。参数高效微调（PEFT）成为应对这些大型模型适应和服务挑战的一种有希望的方向。我们将PEFT技术分为两种类型：侵入式和非侵入式。侵入式PEFT技术直接改变模型的内部结构。虽然更灵活，但在训练和服务过程中引入了显著的复杂性。非侵入式PEFT技术保持内部结构不变，只调整模型的外部参数，如输入的嵌入。在这项工作中，我们将AdaLink描述为一种非侵入式PEFT技术，与SoTA侵入式PEFT（LoRA）和完整模型相比，实现了有竞争力的性能。

    Large language models (LLMs) and vision language models (VLMs) demonstrate excellent performance on a wide range of tasks by scaling up parameter counts from O(10^9) to O(10^{12}) levels and further beyond. These large scales make it impossible to adapt and deploy fully specialized models given a task of interest. Parameter-efficient fine-tuning (PEFT) emerges as a promising direction to tackle the adaptation and serving challenges for such large models. We categorize PEFT techniques into two types: intrusive and non-intrusive. Intrusive PEFT techniques directly change a model's internal architecture. Though more flexible, they introduce significant complexities for training and serving. Non-intrusive PEFT techniques leave the internal architecture unchanged and only adapt model-external parameters, such as embeddings for input. In this work, we describe AdaLink as a non-intrusive PEFT technique that achieves competitive performance compared to SoTA intrusive PEFT (LoRA) and full model
    
[^9]: 发现塞壬之歌：可靠的事实冲突幻觉检测

    Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])

    [http://arxiv.org/abs/2310.12086](http://arxiv.org/abs/2310.12086)

    该论文介绍了一种为大型语言模型设计的FactCHD事实冲突幻觉检测基准，用于评估LLMs生成文本的事实性。基准包含了多种事实模式，并使用基于事实的证据链进行组合性幻觉的检测。

    

    大型语言模型（LLMs），如ChatGPT/GPT-4，因其广泛的实际应用而受到广泛关注，但其在网络平台上存在事实冲突幻觉的问题限制了其采用。对由LLMs产生的文本的事实性评估仍然未被充分探索，不仅涉及对基本事实的判断，还包括对复杂推理任务（如多跳等）中出现的事实错误的评估。为此，我们引入了FactCHD，一种为LLMs精心设计的事实冲突幻觉检测基准。作为在“查询-响应”上下文中评估事实性的关键工具，我们的基准采用了大规模数据集，涵盖了广泛的事实模式，如基本事实，多跳，比较和集合操作模式。我们基准的一个独特特点是其包含基于事实的证据链，从而便于进行组合性幻觉的检测。

    Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
    
[^10]: 关于基于生成模型在人类活动识别中的益处

    On the Benefit of Generative Foundation Models for Human Activity Recognition. (arXiv:2310.12085v1 [cs.CV])

    [http://arxiv.org/abs/2310.12085](http://arxiv.org/abs/2310.12085)

    本论文介绍了在人类活动识别中，基于生成模型的益处。通过使用生成型人工智能生成虚拟IMU数据，可以解决数据稀缺性的问题，并指出了生成标准测试数据集、开发特定于HAR的基础模型、探索HAR中的分层结构、分解复杂活动以及在健康感知和活动摘要中的应用等几个研究方向的潜在贡献。

    

    在人类活动识别（HAR）中，有限的标注数据的可用性是一个重大挑战。受到生成型人工智能的最新进展的启发，包括大规模语言模型（LLMs）和运动合成模型，我们相信生成型人工智能可以通过自动生成虚拟IMU数据来解决数据稀缺性的问题。此外，我们还重点介绍了几个可能受益于生成型人工智能的研究路径，包括生成标准测试数据集、开发特定于HAR的基础模型、探索HAR中的分层结构、分解复杂活动以及在健康感知和活动摘要中的应用。

    In human activity recognition (HAR), the limited availability of annotated data presents a significant challenge. Drawing inspiration from the latest advancements in generative AI, including Large Language Models (LLMs) and motion synthesis models, we believe that generative AI can address this data scarcity by autonomously generating virtual IMU data from text descriptions. Beyond this, we spotlight several promising research pathways that could benefit from generative AI for the community, including the generating benchmark datasets, the development of foundational models specific to HAR, the exploration of hierarchical structures within HAR, breaking down complex activities, and applications in health sensing and activity summarization.
    
[^11]: 朝着更安全的操作：预防未来故障的高压气体事故专家参与数据集

    Towards Safer Operations: An Expert-involved Dataset of High-Pressure Gas Incidents for Preventing Future Failures. (arXiv:2310.12074v1 [cs.CL])

    [http://arxiv.org/abs/2310.12074](http://arxiv.org/abs/2310.12074)

    本文介绍了一个名为IncidentAI的新数据集，用于安全预防。该数据集包含三个任务，并由高压气体保护管理领域的专家进行了注释。初步结果显示，使用自然语言处理技术有助于分析事故报告以预防未来的故障。这个数据集将促进NLP和事故管理领域的未来研究。

    

    本文介绍了一个用于安全预防的新的IncidentAI数据集。与通常只包含一个任务的先前语料库不同，我们的数据集包括了三个任务：命名实体识别，因果关系提取和信息检索。该数据集由至少有六年高压气体保护管理实践经验的领域专家进行注释。我们验证了数据集在安全预防场景中的贡献。对三个任务的初步结果表明，自然语言处理技术有助于分析事故报告以避免未来的故障。该数据集便于NLP和事故管理社区的未来研究。数据集也可提供访问（IncidentAI数据集可在 https://github.com/Cinnamon/incident-ai-dataset 获取）。

    This paper introduces a new IncidentAI dataset for safety prevention. Different from prior corpora that usually contain a single task, our dataset comprises three tasks: named entity recognition, cause-effect extraction, and information retrieval. The dataset is annotated by domain experts who have at least six years of practical experience as high-pressure gas conservation managers. We validate the contribution of the dataset in the scenario of safety prevention. Preliminary results on the three tasks show that NLP techniques are beneficial for analyzing incident reports to prevent future failures. The dataset facilitates future research in NLP and incident management communities. The access to the dataset is also provided (the IncidentAI dataset is available at: https://github.com/Cinnamon/incident-ai-dataset).
    
[^12]: SPEED: 用于高效解码的推测流水线执行

    SPEED: Speculative Pipelined Execution for Efficient Decoding. (arXiv:2310.12072v1 [cs.CL])

    [http://arxiv.org/abs/2310.12072](http://arxiv.org/abs/2310.12072)

    SPEED通过推测执行多个未来标记，加快Transformer解码器的推理效率，从而提高生成型大型语言模型在实时场景中的应用性能。

    

    基于Transformer架构的生成型大型语言模型（LLM）近来已成为广泛应用于自然语言处理任务的主导基础模型。然而，由于这些模型的推理延迟显著，它们在实时场景中的应用受到了很大限制。这主要是由于生成型LLM推理的自回归特性，其中每个标记依赖于所有先前的输出标记，因此很难实现任何标记级的并行性，使得推理过程极度受内存限制。在这项工作中，我们提出了SPEED，通过使用基于早期隐藏状态的预测值来并行地推测执行当前标记与多个未来标记，从而提高推理效率。对于采用参数共享的Transformer解码器，可以将并行执行的标记的内存操作分摊，从而允许我们...

    Generative Large Language Models (LLMs) based on the Transformer architecture have recently emerged as a dominant foundation model for a wide range of Natural Language Processing tasks. Nevertheless, their application in real-time scenarios has been highly restricted due to the significant inference latency associated with these models. This is particularly pronounced due to the autoregressive nature of generative LLM inference, where tokens are generated sequentially since each token depends on all previous output tokens. It is therefore challenging to achieve any token-level parallelism, making inference extremely memory-bound. In this work, we propose SPEED, which improves inference efficiency by speculatively executing multiple future tokens in parallel with the current token using predicted values based on early-layer hidden states. For Transformer decoders that employ parameter sharing, the memory operations for the tokens executing in parallel can be amortized, which allows us t
    
[^13]: 新闻文章中不同交叉文档实体共指的编码手册

    Code Book for the Annotation of Diverse Cross-Document Coreference of Entities in News Articles. (arXiv:2310.12064v1 [cs.CL])

    [http://arxiv.org/abs/2310.12064](http://arxiv.org/abs/2310.12064)

    本文提出了一种新闻文章中共指的注释方案，通过考虑接近身份和桥接关系，扩展了传统的共指关系。我们提供了一种创建不同交叉文档共指语料库的方法论，可用于分析媒体偏见。

    

    本文提出了一种新闻文章中共指的注释方案，不仅扩展了传统的身份关系，还考虑了接近身份和桥接关系。它详细描述了如何设置Inception注释工具，如何注释新闻文章中的实体，如何将它们与各种共指关系连接起来，并将它们与Wikidata的全球知识图谱链接起来。这种多层次的注释方法是在媒体偏见问题的背景下进行讨论的。我们的主要贡献在于提供了一种用于创建能够应用于通过词选择和标签分析媒体偏见的不同交叉文档共指语料库的方法论。

    This paper presents a scheme for annotating coreference across news articles, extending beyond traditional identity relations by also considering near-identity and bridging relations. It includes a precise description of how to set up Inception, a respective annotation tool, how to annotate entities in news articles, connect them with diverse coreferential relations, and link them across documents to Wikidata's global knowledge graph. This multi-layered annotation approach is discussed in the context of the problem of media bias. Our main contribution lies in providing a methodology for creating a diverse cross-document coreference corpus which can be applied to the analysis of media bias by word-choice and labelling.
    
[^14]: 评估大型语言模型在越南普通教育中对多项选择题符号绑定能力的能力

    Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education. (arXiv:2310.12059v1 [cs.CL])

    [http://arxiv.org/abs/2310.12059](http://arxiv.org/abs/2310.12059)

    本研究评估了大型语言模型在越南普通教育中对多项选择题符号绑定能力的能力，并创建了一个新颖且高质量的数据集来评估语言模型的符号绑定能力。

    

    本文评估了大型语言模型（LLMs）在零样本、一次性和少样本设置下，执行多项选择符号绑定（MCSB）用于多项选择题回答（MCQA）任务的能力。我们将重点放在越南语上，因为越南语中的挑战性MCQA数据集较英语少。现有的两个数据集，ViMMRC 1.0和ViMMRC 2.0，专注于文学问题。越南自然语言处理（NLP）领域的最新研究侧重于评估ChatGPT在2019年至2023年的越南国家高中毕业考试（VNHSGE）中的解决方案。然而，这些研究主要关注ChatGPT如何逐步解决VNHSGE。我们的目标是通过为数学、物理、化学和生物的LaTeX公式输入提供结构化指南，创建一个新颖且高质量的数据集。该数据集可用于评估LLMs和较小的语言模型（LMs）的MCSB能力，因为数据集要求使用严格的LaTeX样式进行输入。我们重点预测字符（A、B、C或

    In this paper, we evaluate the ability of large language models (LLMs) to perform multiple choice symbol binding (MCSB) for multiple choice question answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus on Vietnamese, with fewer challenging MCQA datasets than in English. The two existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent research in Vietnamese natural language processing (NLP) has focused on the Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to 2023 to evaluate ChatGPT. However, these studies have mainly focused on how ChatGPT solves the VNHSGE step by step. We aim to create a novel and high-quality dataset by providing structured guidelines for typing LaTeX formulas for mathematics, physics, chemistry, and biology. This dataset can be used to evaluate the MCSB ability of LLMs and smaller language models (LMs) because it is typed in a strict LaTeX style. We focus on predicting the character (A, B, C, or 
    
[^15]: Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scaling of Texts with Large Language Models (使用大型语言模型的概念导向思维链图提示进行文本配对比较缩放)

    Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scaling of Texts with Large Language Models. (arXiv:2310.12049v1 [cs.CL])

    [http://arxiv.org/abs/2310.12049](http://arxiv.org/abs/2310.12049)

    这项研究开发了一种文本缩放方法，利用生成性大型语言模型的模式识别能力，通过概念导向思维链图和大型语言模型进行文本比较，并使用Bradley-Terry模型来估计评分尺度。该方法在Twitter上对情感言论的缩放效果更好。

    

    现有的文本缩放方法经常需要大型语料库，难以处理短文本，或需要有标签的数据。我们开发了一种利用生成性大型语言模型（LLM）的模式识别能力来进行文本缩放的方法。具体而言，我们提出了概念导向思维链图（CGCoT），它使用设计用于总结想法并在文本中识别目标方的提示来生成概念特定的细分，类似于人类编码器内容分析的指导。CGCoT将配对文本比较从一个推理问题转变为一个模式识别问题。然后，我们使用LLM对概念特定的细分进行配对比较。我们利用这些配对比较的结果使用Bradley-Terry模型来估计一个评分尺度。我们利用这种方法对Twitter上的情感言论进行缩放。我们的测量值与人类判断的相关性比Wordfish等替代方法更强。除了一小组用于开发CGCoT提示的试验数据之外，...

    Existing text scaling methods often require a large corpus, struggle with short texts, or require labeled data. We develop a text scaling method that leverages the pattern recognition capabilities of generative large language models (LLMs). Specifically, we propose concept-guided chain-of-thought (CGCoT), which uses prompts designed to summarize ideas and identify target parties in texts to generate concept-specific breakdowns, in many ways similar to guidance for human coder content analysis. CGCoT effectively shifts pairwise text comparisons from a reasoning problem to a pattern recognition problem. We then pairwise compare concept-specific breakdowns using an LLM. We use the results of these pairwise comparisons to estimate a scale using the Bradley-Terry model. We use this approach to scale affective speech on Twitter. Our measures correlate more strongly with human judgments than alternative approaches like Wordfish. Besides a small set of pilot data to develop the CGCoT prompts, 
    
[^16]: CORE: 一种用于强健领域适应的少样本公司关系分类数据集

    CORE: A Few-Shot Company Relation Classification Dataset for Robust Domain Adaptation. (arXiv:2310.12024v1 [cs.CL])

    [http://arxiv.org/abs/2310.12024](http://arxiv.org/abs/2310.12024)

    CORE是一个用于少样本公司关系分类的数据集，该数据集包括4708个实例和12种关系类型。实验结果表明，目前的RC模型在适应CORE方面存在困难，但在CORE上训练的模型具有改进的跨领域性能。

    

    我们介绍了CORE，一个针对公司关系和商业实体的少样本关系分类（RC）数据集。CORE包括4708个实例和12种关系类型，其中包含从公司维基百科页面提取的文本证据。由于与公司名称和商业实体相关的丰富多样的信息，少样本RC模型面临挑战。例如，根据上下文，公司名称可能表示法律实体、产品、人员或业务部门。因此，推导实体之间的关系类型高度依赖于文本上下文。为了评估最先进的RC模型在CORE数据集上的性能，我们在少样本领域适应设置下进行实验。我们的结果显示了显著的性能差距，证实了在不同领域训练的模型在适应CORE方面存在困难。有趣的是，我们发现在CORE上训练的模型展示了改进的跨领域性能，突显了其先进性。

    We introduce CORE, a dataset for few-shot relation classification (RC) focused on company relations and business entities. CORE includes 4,708 instances of 12 relation types with corresponding textual evidence extracted from company Wikipedia pages. Company names and business entities pose a challenge for few-shot RC models due to the rich and diverse information associated with them. For example, a company name may represent the legal entity, products, people, or business divisions depending on the context. Therefore, deriving the relation type between entities is highly dependent on textual context. To evaluate the performance of state-of-the-art RC models on the CORE dataset, we conduct experiments in the few-shot domain adaptation setting. Our results reveal substantial performance gaps, confirming that models trained on different domains struggle to adapt to CORE. Interestingly, we find that models trained on CORE showcase improved out-of-domain performance, which highlights the i
    
[^17]: LoHoRavens: 一项针对机器人桌面操作的长时程语言条件基准测试

    LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation. (arXiv:2310.12020v1 [cs.RO])

    [http://arxiv.org/abs/2310.12020](http://arxiv.org/abs/2310.12020)

    LoHoRavens是一个针对机器人桌面操作的长时程语言条件基准测试，涵盖颜色、大小、空间、算术和引用等各种推理方面。本研究还探索了在机器人执行过程中如何将观测反馈纳入到大型语言模型的闭环规划中的两种方法。

    

    体验式代理与大型语言模型的融合为体验式指导带来了显著的进展。特别是，大型语言模型的强大推理能力使得机器人能够在没有昂贵的注释演示的情况下进行长时程任务。然而，目前还缺乏用于测试语言条件机器人在各种场景中推理长时程能力的公共基准。为了填补这一空白，本研究聚焦于桌面操作任务，并发布了一个称为“LoHoRavens”的仿真基准测试，涵盖颜色、大小、空间、算术和引用等各种长时程推理方面。此外，对于使用大型语言模型进行长时程操作的任务，存在一个关键的模态过渡问题：如何在机器人执行过程中将观测反馈纳入到大型语言模型的闭环规划中，然而之前的研究对此进行的探索较少。我们研究了两种解决模态过渡问题的方法：标题生成和快照。

    The convergence of embodied agents and large language models (LLMs) has brought significant advancements to embodied instruction following. Particularly, the strong reasoning capabilities of LLMs make it possible for robots to perform long-horizon tasks without expensive annotated demonstrations. However, public benchmarks for testing the long-horizon reasoning capabilities of language-conditioned robots in various scenarios are still missing. To fill this gap, this work focuses on the tabletop manipulation task and releases a simulation benchmark, \textit{LoHoRavens}, which covers various long-horizon reasoning aspects spanning color, size, space, arithmetics and reference. Furthermore, there is a key modality bridging problem for long-horizon manipulation tasks with LLMs: how to incorporate the observation feedback during robot execution for the LLM's closed-loop planning, which is however less studied by prior work. We investigate two methods of bridging the modality gap: caption ge
    
[^18]: Gold: 全球和局部意识的常识知识图噪声检测去噪框架

    Gold: A Global and Local-aware Denoising Framework for Commonsense Knowledge Graph Noise Detection. (arXiv:2310.12011v1 [cs.CL])

    [http://arxiv.org/abs/2310.12011](http://arxiv.org/abs/2310.12011)

    Gold是一种全球和局部意识的常识知识图噪声检测去噪框架，通过结合实体语义信息、全局规则和局部结构信息，它能够有效地检测和降噪常识知识图，并在零样本常识问答任务中表现出优越性能。

    

    常识知识图（CSKG）对于常识推理至关重要，但通过人工注释构建它们可能成本高昂。因此，已经提出了各种自动方法来构建具有更大语义覆盖范围的CSKG。然而，这些无监督方法引入了假噪声，可能降低了生成的CSKG的质量，而现有的去噪算法由于CSKG中节点和结构的独特特征而难以解决这个问题。为了解决这个问题，我们提出了Gold（全球和局部意识去噪），这是一个针对CSKG的去噪框架，它结合了实体语义信息、全局规则和来自CSKG的局部结构信息。实验结果表明，在合成噪声CSKG基准上，Gold在噪声检测任务中优于所有基线方法。此外，我们还展示了去噪现实世界CSKG的有效性，甚至对下游的零样本常识问答任务有益。

    Commonsense Knowledge Graphs (CSKGs) are crucial for commonsense reasoning, yet constructing them through human annotations can be costly. As a result, various automatic methods have been proposed to construct CSKG with larger semantic coverage. However, these unsupervised approaches introduce spurious noise that can lower the quality of the resulting CSKG, which cannot be tackled easily by existing denoising algorithms due to the unique characteristics of nodes and structures in CSKGs. To address this issue, we propose Gold (Global and Local-aware Denoising), a denoising framework for CSKGs that incorporates entity semantic information, global rules, and local structural information from the CSKG. Experiment results demonstrate that Gold outperforms all baseline methods in noise detection tasks on synthetic noisy CSKG benchmarks. Furthermore, we show that denoising a real-world CSKG is effective and even benefits the downstream zero-shot commonsense question-answering task.
    
[^19]: 多视角对比学习用于知识图谱中的实体类型判断

    Multi-view Contrastive Learning for Entity Typing over Knowledge Graphs. (arXiv:2310.12008v1 [cs.CL])

    [http://arxiv.org/abs/2310.12008](http://arxiv.org/abs/2310.12008)

    本文提出了一种名为多视角对比学习的新方法，该方法有效地将类型聚类提供的粗粒度知识编码到实体和类型嵌入中，从而改进了知识图谱实体类型判断任务。

    

    知识图谱实体类型判断(KGET)旨在推断知识图谱中实体的可能类型。现有的KGET方法侧重于如何更好地将实体的邻居和类型提供的知识编码到其表示中，但它们忽略了类型如何以聚类方式提供的语义知识。本文提出了一种名为多视角对比学习的知识图谱实体类型判断(MCLET)的新方法，它有效地将聚类提供的粗粒度知识编码到实体和类型嵌入中。MCLET由三个模块组成：i) 多视图生成和编码器模块，用于编码实体类型、实体聚类和聚类类型视图中的结构化信息；ii) 跨视图对比学习模块，鼓励不同视图共同改进实体和类型的视图特定表示；iii) 实体类型判断模块，集成多头注意力和...

    Knowledge graph entity typing (KGET) aims at inferring plausible types of entities in knowledge graphs. Existing approaches to KGET focus on how to better encode the knowledge provided by the neighbors and types of an entity into its representation. However, they ignore the semantic knowledge provided by the way in which types can be clustered together. In this paper, we propose a novel method called Multi-view Contrastive Learning for knowledge graph Entity Typing (MCLET), which effectively encodes the coarse-grained knowledge provided by clusters into entity and type embeddings. MCLET is composed of three modules: i) Multi-view Generation and Encoder module, which encodes structured information from entity-type, entity-cluster and cluster-type views; ii) Cross-view Contrastive Learning module, which encourages different views to collaboratively improve view-specific representations of entities and types; iii) Entity Typing Prediction module, which integrates multi-head attention and 
    
[^20]: 生成型AI系统的社会技术安全评估

    Sociotechnical Safety Evaluation of Generative AI Systems. (arXiv:2310.11986v1 [cs.AI])

    [http://arxiv.org/abs/2310.11986](http://arxiv.org/abs/2310.11986)

    本文提出了一个三层框架，采用社会技术方法对生成型AI系统的安全风险进行评估。同时，评估现状调查发现了三个显著的评估差距，并提出了解决这些差距的方法。

    

    生成型AI系统会产生一系列风险。为了确保生成型AI系统的安全，需要对这些风险进行评估。本文提出了一个三层框架，采用结构化的社会技术方法来评估这些风险。该框架包括能力评估，这是目前主要的安全评估方法。在此基础上，我们进一步建立在系统安全原则的基础上，特别是认识到上下文决定了特定能力是否会造成伤害。为了考虑相关的上下文，我们的框架增加了人机互动和系统影响作为额外的评估层面。其次，我们调查了生成型AI系统安全评估的现状，并创建了现有评估的库。从分析中得出了三个显著的评估差距。我们提出了解决这些差距的前进方式，概述了实际步骤和角色。

    Generative AI systems produce a range of risks. To ensure the safety of generative AI systems, these risks must be evaluated. In this paper, we make two main contributions toward establishing such evaluations. First, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. This framework encompasses capability evaluations, which are the main current approach to safety evaluation. It then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. To account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. Second, we survey the current state of safety evaluation of generative AI systems and create a repository of existing evaluations. Three salient evaluation gaps emerge from this analysis. We propose ways forward to closing these gaps, outlining practical steps as well as roles
    
[^21]: 从插值到外推：算术Transformer的完整长度泛化

    From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers. (arXiv:2310.11984v1 [cs.LG])

    [http://arxiv.org/abs/2310.11984](http://arxiv.org/abs/2310.11984)

    本文研究了Transformer模型在学习算术算法方面的能力，并通过注意力偏置以及Attention Bias Calibration（ABC）来实现对于长长度的泛化。

    

    自从提出以来，Transformer模型在各种任务中展现出了优秀的性能。然而，在算法任务中，长度泛化仍存在一些未解决的问题。在本文中，我们研究了Transformer模型在学习算术算法（如加法和乘法）方面的内在能力。通过实验证明和注意力分析，我们确定了实现最佳长度泛化的几个关键因素。我们展示了Transformer模型能够通过目标指向偏置来泛化到长长度。然后，我们引入了Attention Bias Calibration（ABC），这是一个校准阶段，使模型能够自动学习适当的注意力偏置，我们将其与相对位置编码的机制联系起来。我们证明使用ABC，Transformer模型可以在某些算术任务上实现前所未有的完美长度泛化。

    Since its introduction, the transformer model has demonstrated outstanding performance across various tasks. However, there are still unresolved issues regarding length generalization, particularly in algorithmic tasks. In this paper, we investigate the inherent capabilities of transformer models in learning arithmetic algorithms, such as addition and multiplication. Through experiments and attention analysis, we identify a number of crucial factors for achieving optimal length generalization. We show that transformer models are able to generalize to long lengths with the help of targeted attention biasing. We then introduce Attention Bias Calibration (ABC), a calibration stage that enables the model to automatically learn the proper attention biases, which we link to mechanisms in relative position encoding. We demonstrate that using ABC, the transformer model can achieve unprecedented perfect length generalization on certain arithmetic tasks.
    
[^22]: InfoDiffusion: 针对非自回归文本生成的信息熵感知扩散过程

    InfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation. (arXiv:2310.11976v1 [cs.CL])

    [http://arxiv.org/abs/2310.11976](http://arxiv.org/abs/2310.11976)

    InfoDiffusion是一种非自回归文本扩散模型，通过引入“keyinfo-first”生成策略和基于文本信息量的噪声调度，以及结合自我条件和部分加噪模型结构的方法，提高了生成质量和多样性，并展示出更高的采样效率。

    

    扩散模型在文本生成领域引起了广泛关注。一些研究已经探索了具有不同结构的文本扩散模型，并将它们应用于各种任务，包括命名实体识别和摘要生成。然而，当前扩散模型的“easy-first”文本生成过程与人类的“keyword-first”自然文本生成过程存在明显差异，这引起了有限的关注。为了弥合这一差距，我们提出了一种非自回归文本扩散模型InfoDiffusion。我们的方法引入了“keyinfo-first”生成策略，并结合了基于文本信息量的噪声调度。此外，InfoDiffusion结合了自我条件和一个新提出的部分加噪模型结构。实验结果表明，InfoDiffusion在生成质量和多样性方面优于基线模型，并展示出更高的采样效率。

    Diffusion models have garnered considerable interest in the field of text generation. Several studies have explored text diffusion models with different structures and applied them to various tasks, including named entity recognition and summarization. However, there exists a notable disparity between the "easy-first" text generation process of current diffusion models and the "keyword-first" natural text generation process of humans, which has received limited attention. To bridge this gap, we propose InfoDiffusion, a non-autoregressive text diffusion model. Our approach introduces a "keyinfo-first" generation strategy and incorporates a noise schedule based on the amount of text information. In addition, InfoDiffusion combines self-conditioning with a newly proposed partially noising model structure. Experimental results show that InfoDiffusion outperforms the baseline model in terms of generation quality and diversity, as well as exhibiting higher sampling efficiency.
    
[^23]: 填补空白：利用图自编码网络进行高效的事件共指消解

    Filling in the Gaps: Efficient Event Coreference Resolution using Graph Autoencoder Networks. (arXiv:2310.11965v1 [cs.CL])

    [http://arxiv.org/abs/2310.11965](http://arxiv.org/abs/2310.11965)

    填补空白：利用图自编码网络进行高效的事件共指消解，显著优于传统方法，尤其在低资源或低数据环境下表现更好。

    

    我们介绍了一种针对低资源语言领域应用的事件共指消解（ECR）的新颖高效方法。通过将ECR作为图重建任务来进行，我们能够将深层语义嵌入与结构化共指链知识相结合，创建一个参数有效的图自编码器模型（GAE）系列。我们的方法在整体得分、效率和训练速度方面显著优于经典的提及对方法，在大规模荷兰语事件共指语料库中表现出色。此外，我们还展示了我们的模型在更难的共指链接分类和低数据环境中比基于transformer的提及对共指算法更加稳健。

    We introduce a novel and efficient method for Event Coreference Resolution (ECR) applied to a lower-resourced language domain. By framing ECR as a graph reconstruction task, we are able to combine deep semantic embeddings with structural coreference chain knowledge to create a parameter-efficient family of Graph Autoencoder models (GAE). Our method significantly outperforms classical mention-pair methods on a large Dutch event coreference corpus in terms of overall score, efficiency and training speed. Additionally, we show that our models are consistently able to classify more difficult coreference links and are far more robust in low-data settings when compared to transformer-based mention-pair coreference algorithms.
    
[^24]: 使用因果分层注意力和指针的AMR解析

    AMR Parsing with Causal Hierarchical Attention and Pointers. (arXiv:2310.11964v1 [cs.CL])

    [http://arxiv.org/abs/2310.11964](http://arxiv.org/abs/2310.11964)

    本文介绍了一种新的AMR解析模型CHAP，它利用因果分层注意力和指针机制将结构整合到Transformer解码器中，实验证明在没有额外数据的情况下，该模型在四个基准测试中优于基线模型。

    

    基于翻译的AMR解析器由于其简单和有效性而近年来备受关注。它们将线性化图预测为自由文本，避免了显式的结构建模。然而，这种简单性忽视了AMR图中的结构局部性，并引入了不必要的标记来表示共指。本文介绍了AMR解析的新目标形式和一种新的模型，CHAP，它配备了因果分层注意力和指针机制，使结构能够被整合到Transformer解码器中。我们在实验中探索了各种替代建模选项。实验证明，在没有额外数据的情况下，我们的模型在五个基准测试中有四个的性能优于基线模型。

    Translation-based AMR parsers have recently gained popularity due to their simplicity and effectiveness. They predict linearized graphs as free texts, avoiding explicit structure modeling. However, this simplicity neglects structural locality in AMR graphs and introduces unnecessary tokens to represent coreferences. In this paper, we introduce new target forms of AMR parsing and a novel model, CHAP, which is equipped with causal hierarchical attention and the pointer mechanism, enabling the integration of structures into the Transformer decoder. We empirically explore various alternative modeling options. Experiments show that our model outperforms baseline models on four out of five benchmarks in the setting of no additional data.
    
[^25]: 快速多极化注意力：一种用于长序列的分治注意力机制

    Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences. (arXiv:2310.11960v1 [cs.CL])

    [http://arxiv.org/abs/2310.11960](http://arxiv.org/abs/2310.11960)

    提出了一种名为快速多极化注意力的新型注意力机制，它使用分治策略将注意力的时间和内存复杂度从O(n^2)降低到O(n log n)或O(n)，同时保持了全局感知范围。

    

    基于Transformer的模型已在许多领域取得了最先进的性能。然而，自注意力对于输入长度的二次复杂度限制了Transformer模型在长序列上的适用性。为了解决这个问题，我们提出了快速多极化注意力，一种使用分治策略来减少注意力时间和内存复杂度的新型注意力机制，将长度为n的序列的注意力复杂度从O(n^2)降低到O(n log n)或O(n)，同时保持了全局感知范围。这种分层方法将查询、键和值分为O(log n)级的分辨率，较远距离的组群越来越大，并学习计算组群数量的权重。因此，以高效分层的方式在较低的分辨率中考虑远离彼此的标记之间的相互作用。快速多极化注意力的总体复杂度为O(n)或O(n log n)。

    Transformer-based models have achieved state-of-the-art performance in many areas. However, the quadratic complexity of self-attention with respect to the input length hinders the applicability of Transformer-based models to long sequences. To address this, we present Fast Multipole Attention, a new attention mechanism that uses a divide-and-conquer strategy to reduce the time and memory complexity of attention for sequences of length $n$ from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ or $O(n)$, while retaining a global receptive field. The hierarchical approach groups queries, keys, and values into $\mathcal{O}( \log n)$ levels of resolution, where groups at greater distances are increasingly larger in size and the weights to compute group quantities are learned. As such, the interaction between tokens far from each other is considered in lower resolution in an efficient hierarchical manner. The overall complexity of Fast Multipole Attention is $\mathcal{O}(n)$ or $\mathcal{O}(n \
    
[^26]: 用勺子舀空海洋：我们应该编辑模型吗？

    Emptying the Ocean with a Spoon: Should We Edit Models?. (arXiv:2310.11958v1 [cs.CL])

    [http://arxiv.org/abs/2310.11958](http://arxiv.org/abs/2310.11958)

    这项研究质疑了直接模型编辑作为纠正LLM生成中事实错误的方法，并提出了与之类似但更为明确的三种替代方法。虽然模型编辑在提升模型可解释性方面有潜力，但不能被视为解决LLMs固有缺点的系统性方法，因为它存在加强模型可信性观念的风险。

    

    我们对直接模型编辑作为纠正LLM生成中的事实错误的方法提出了质疑。我们将模型编辑与追求更明确目标的三种类似但不同的方法进行对比：（1）基于检索的架构，将事实记忆与LLMs所体现的推理和语言能力解耦；（2）概念擦除方法，旨在防止生成文本中的系统偏见；（3）归属方法，旨在将生成结果与已确定的文本来源连接起来。我们认为，不能将直接模型编辑作为解决LLMs固有缺点的系统性方法，并且虽然它在改进模型可解释性方面具有潜力，但它通过加强模型可信性的观念而存在风险。我们呼吁在LLM部署过程中谨慎推广和应用模型编辑，并负责任地限制LLMs的使用案例，以不依赖....

    We call into question the recently popularized method of direct model editing as a means of correcting factual errors in LLM generations. We contrast model editing with three similar but distinct approaches that pursue better defined objectives: (1) retrieval-based architectures, which decouple factual memory from inference and linguistic capabilities embodied in LLMs; (2) concept erasure methods, which aim at preventing systemic bias in generated text; and (3) attribution methods, which aim at grounding generations into identified textual sources. We argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability, it opens risks by reinforcing the notion that models can be trusted for factuality. We call for cautious promotion and application of model editing as part of the LLM deployment process, and for responsibly limiting the use cases of LLMs to those not relying o
    
[^27]: MusicAgent：一种用于音乐理解和生成的人工智能代理，基于大型语言模型

    MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models. (arXiv:2310.11954v1 [cs.CL])

    [http://arxiv.org/abs/2310.11954](http://arxiv.org/abs/2310.11954)

    MusicAgent是一个使用大型语言模型的AI代理，通过集成音乐相关工具和自主工作流程，帮助用户自动分析需求并调用合适的工具进行音乐处理。

    

    AI-加强的音乐处理是一个多样化的领域，涵盖了许多任务，从生成任务（例如音色合成）到理解任务（例如音乐分类）。对于开发人员和业余爱好者来说，很难掌握所有这些任务，以满足他们在音乐处理方面的需求，尤其考虑到音乐数据的表示和不同任务之间的模型适用性在各个平台上存在巨大差异。因此，建立一个系统来组织和集成这些任务，并帮助从业者自动分析他们的需求并调用适当的工具作为解决方案来满足他们的要求是必要的。受大型语言模型（LLMs）在任务自动化方面的最新成功启示，我们开发了一个名为MusicAgent的系统，它集成了众多与音乐相关的工具和自主工作流程，以满足用户的需求。

    AI-empowered music processing is a diverse field that encompasses dozens of tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension tasks (e.g., music classification). For developers and amateurs, it is very difficult to grasp all of these task to satisfy their requirements in music processing, especially considering the huge differences in the representations of music data and the model applicability across platforms among various tasks. Consequently, it is necessary to build a system to organize and integrate these tasks, and thus help practitioners to automatically analyze their demand and call suitable tools as solutions to fulfill their requirements. Inspired by the recent success of large language models (LLMs) in task automation, we develop a system, named MusicAgent, which integrates numerous music-related tools and an autonomous workflow to address user requirements. More specifically, we build 1) toolset that collects tools from diverse sources, includi
    
[^28]: 紧密与全面：研究跨模态和跨语言基础的方法论途径

    Grounded and Well-rounded: A Methodological Approach to the Study of Cross-modal and Cross-lingual Grounding. (arXiv:2310.11938v1 [cs.CL])

    [http://arxiv.org/abs/2310.11938](http://arxiv.org/abs/2310.11938)

    本文提出了一种研究跨模态和跨语言基础问题的方法论框架，通过比较不同输入源的质量效应和模型性能，揭示了基础的影响。

    

    现有文献对于基础（grounding）被认为是发展更完整和真正语义能力的人工智能系统的关键组成部分存在分歧。有些人认为基础允许进行质量不同的泛化，而其他人则认为通过单模态数据量可以弥补这一差距。然而，目前缺乏有关基础对NLP系统的影响的实证证据，我们认为这是由于研究基础及其对NLP系统的影响所面临的方法论挑战。在本文中，我们建立了一个方法论框架，研究各种输入源（而不仅仅是纯文本）对模型的影响。关键在于构建在不同输入模态上进行训练的模型群体的可比较样本，以便我们可以将不同输入源的质量效应与可量化的模型性能区分开来。使用这一框架的实验揭示了质量的差异。

    Grounding has been argued to be a crucial component towards the development of more complete and truly semantically competent artificial intelligence systems. Literature has divided into two camps: While some argue that grounding allows for qualitatively different generalizations, others believe it can be compensated by mono-modal data quantity. Limited empirical evidence has emerged for or against either position, which we argue is due to the methodological challenges that come with studying grounding and its effects on NLP systems.  In this paper, we establish a methodological framework for studying what the effects are - if any - of providing models with richer input sources than text-only. The crux of it lies in the construction of comparable samples of populations of models trained on different input modalities, so that we can tease apart the qualitative effects of different input sources from quantifiable model performances. Experiments using this framework reveal qualitative dif
    
[^29]: 通过线性结构探索研究Transformer句子嵌入的语义子空间

    Investigating semantic subspaces of Transformer sentence embeddings through linear structural probing. (arXiv:2310.11923v1 [cs.CL])

    [http://arxiv.org/abs/2310.11923](http://arxiv.org/abs/2310.11923)

    通过语义结构探测实验，本论文研究了Transformer句子嵌入的语义子空间。研究结果表明，不同类型和大小的语言模型在性能和层动力学上存在差异，但模型大小与结果关联较小。

    

    对于NLP社区来说，研究基于Transformer的语言模型的不同层级中编码的语言信息的类型是非常重要的问题。然而，现有研究主要集中在单词级表示和仅使用编码器的语言模型以及遮蔽标记训练目标。本文通过语义结构探测实验介绍了一种研究句子级表示的方法，即通过找到嵌入空间的一个子空间，提供适合每一对数据点的特定任务的距离。我们在两个任务（语义文本相似性和自然语言推理）的背景下，将我们的方法应用于不同种类（仅编码器、仅解码器、编码器-解码器）和不同大小的语言模型。我们发现不同家族的模型在性能和层动力学上存在显著差异，但结果在很大程度上与模型大小无关。

    The question of what kinds of linguistic information are encoded in different layers of Transformer-based language models is of considerable interest for the NLP community. Existing work, however, has overwhelmingly focused on word-level representations and encoder-only language models with the masked-token training objective. In this paper, we present experiments with semantic structural probing, a method for studying sentence-level representations via finding a subspace of the embedding space that provides suitable task-specific pairwise distances between data-points. We apply our method to language models from different families (encoder-only, decoder-only, encoder-decoder) and of different sizes in the context of two tasks, semantic textual similarity and natural-language inference. We find that model families differ substantially in their performance and layer dynamics, but that the results are largely model-size invariant.
    
[^30]: 知识图谱中半感应式链接预测的基准

    A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs. (arXiv:2310.11917v1 [cs.CL])

    [http://arxiv.org/abs/2310.11917](http://arxiv.org/abs/2310.11917)

    本文提出了一个用于评估知识图谱中半感应式链接预测模型的大规模基准，基于Wikidata5M进行扩展。通过各种不同的任务和信息组合，该基准为进一步研究上下文和文本信息在链接预测中的整合提供了一个测试平台。

    

    知识图谱中的半感应式链接预测是根据上下文信息来预测新的、之前未见的实体的事实的任务。本文提出和描述了一个大规模基准来评估半感应式链接预测模型。该基准基于并扩展了Wikidata5M：它提供了转导式、k-shot和0-shot链接预测任务，每个任务都会根据可用的信息情况从（i）仅有知识图谱结构，到（ii）包含文本提及，再到（iii）实体的详细描述进行变化。我们进行了一项关于最近方法的小型研究，发现半感应式链接预测的性能远远低于转导式性能，在所有实验中都表现出对于长尾实体的不足。该基准为进一步研究如何将上下文和文本信息整合到链接预测中提供了一个测试平台。

    Semi-inductive link prediction (LP) in knowledge graphs (KG) is the task of predicting facts for new, previously unseen entities based on context information. Although new entities can be integrated by retraining the model from scratch in principle, such an approach is infeasible for large-scale KGs, where retraining is expensive and new entities may arise frequently. In this paper, we propose and describe a large-scale benchmark to evaluate semi-inductive LP models. The benchmark is based on and extends Wikidata5M: It provides transductive, k-shot, and 0-shot LP tasks, each varying the available information from (i) only KG structure, to (ii) including textual mentions, and (iii) detailed descriptions of the entities. We report on a small study of recent approaches and found that semi-inductive LP performance is far from transductive performance on long-tail entities throughout all experiments. The benchmark provides a test bed for further research into integrating context and textual
    
[^31]: 宁愿是护士也不愿是医生 -- 对比性解释的研究

    Rather a Nurse than a Physician -- Contrastive Explanations under Investigation. (arXiv:2310.11906v1 [cs.CL])

    [http://arxiv.org/abs/2310.11906](http://arxiv.org/abs/2310.11906)

    这项研究通过分析四个英文文本分类数据集和人类理性注释，验证了对比性解释与非对比性解释在模型和人类之间的高度一致性。

    

    对比性解释是将一个决策与另一个进行对比解释，它比非对比性解释更接近于人类解释决策的方式。这一观点尚未经过实证验证。我们对四个英文文本分类数据集（SST2、DynaSent、BIOS和DBpedia-Animals）进行了分析。我们通过微调和提取来自三种不同模型（RoBERTa、GPT-2和T5）的解释，并应用三种后期可解释性方法（LRP、GradientxInput和GradNorm）。我们还针对BIOS数据集中的100个样本的对比性和非对比性设置进行了人类理性注释的收集和发布。模型基础理性与人类标注之间在对比性和非对比性设置下的交叉比较显示，无论是对于模型还是人类，两个设置之间存在高度一致性。此外，模型基础解释计算…

    Contrastive explanations, where one decision is explained in contrast to another, are supposed to be closer to how humans explain a decision than non-contrastive explanations, where the decision is not necessarily referenced to an alternative. This claim has never been empirically validated. We analyze four English text-classification datasets (SST2, DynaSent, BIOS and DBpedia-Animals). We fine-tune and extract explanations from three different models (RoBERTa, GTP-2, and T5), each in three different sizes and apply three post-hoc explainability methods (LRP, GradientxInput, GradNorm). We furthermore collect and release human rationale annotations for a subset of 100 samples from the BIOS dataset for contrastive and non-contrastive settings. A cross-comparison between model-based rationales and human annotations, both in contrastive and non-contrastive settings, yields a high agreement between the two settings for models as well as for humans. Moreover, model-based explanations compute
    
[^32]: 从神经激活到概念: 解释神经网络中的概念的调查

    From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks. (arXiv:2310.11884v1 [cs.AI])

    [http://arxiv.org/abs/2310.11884](http://arxiv.org/abs/2310.11884)

    本文调查了解释神经网络中概念的最新方法，这对于实现基于可解释概念的神经符号化人工智能来说是重要的一步。

    

    在本文中，我们审查了解释神经网络中概念的最新方法。概念可以作为学习和推理之间的自然桥梁：一旦确定了神经学习系统使用的概念，就可以将这些概念与推理系统整合，用于推理或使用推理系统对其进行改进或增强以改善学习系统。另一方面，不仅可以从神经网络中提取知识，还可以将概念知识插入神经网络体系结构中。由于整合学习和推理是神经符号化人工智能的核心，所以通过这项调查获得的见解可以成为实现基于可解释概念的神经符号化人工智能的重要一步。

    In this paper, we review recent approaches for explaining concepts in neural networks. Concepts can act as a natural link between learning and reasoning: once the concepts are identified that a neural learning system uses, one can integrate those concepts with a reasoning system for inference or use a reasoning system to act upon them to improve or enhance the learning system. On the other hand, knowledge can not only be extracted from neural networks but concept knowledge can also be inserted into neural network architectures. Since integrating learning and reasoning is at the core of neuro-symbolic AI, the insights gained from this survey can serve as an important step towards realizing neuro-symbolic AI based on explainable concepts.
    
[^33]: 从不一致到洞察：对案例结果分类的理由数据集构建进行解析

    From Dissonance to Insights: Dissecting Disagreements in Rationale Dataset Construction for Case Outcome Classification. (arXiv:2310.11878v1 [cs.CL])

    [http://arxiv.org/abs/2310.11878](http://arxiv.org/abs/2310.11878)

    本研究关注法律自然语言处理中人工标注的变异问题，通过收集一组律师对案件结果评估存在分歧的数据集，对这些分歧进行了研究，构建了一个两级分类体系，并发现分歧主要源于对法律背景的不明确描述。

    

    在法律自然语言处理中，案例结果分类（COC）不仅需要准确性，还需要可信赖性和可解释性。现有的可解释COC研究仅限于由单个专家进行的注释。然而，众所周知，律师在对案件事实进行评估时可能存在分歧。因此，我们收集了一个新的数据集RAVE：欧洲人权法领域的理由变异，该数据集是从国际人权法领域的两位专家那里获得的，我们观察到他们之间存在弱一致性。我们研究了他们的分歧，并构建了一个两级任务无关的分类体系，同时补充了COC特定的子类别。据我们所知，这是法律自然语言处理领域首次关注人工标注的变异。我们定量评估了不同分类类别，并发现分歧主要源于对法律背景的不明确描述，这在COC元数据通常具有有限细粒度和噪声的情况下带来了挑战。我们进一步评估了SOTA COC模型在RAVE数据集上的可解释性，并观察到...

    In legal NLP, Case Outcome Classification (COC) must not only be accurate but also trustworthy and explainable. Existing work in explainable COC has been limited to annotations by a single expert. However, it is well-known that lawyers may disagree in their assessment of case facts. We hence collect a novel dataset RAVE: Rationale Variation in ECHR1, which is obtained from two experts in the domain of international human rights law, for whom we observe weak agreement. We study their disagreements and build a two-level task-independent taxonomy, supplemented with COC-specific subcategories. To our knowledge, this is the first work in the legal NLP that focuses on human label variation. We quantitatively assess different taxonomy categories and find that disagreements mainly stem from underspecification of the legal context, which poses challenges given the typically limited granularity and noise in COC metadata. We further assess the explainablility of SOTA COC models on RAVE and observ
    
[^34]: 大型语言模型中幻觉性无法回答性的好奇案例：在过度自信的隐藏状态中寻找真理

    The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models. (arXiv:2310.11877v1 [cs.CL])

    [http://arxiv.org/abs/2310.11877](http://arxiv.org/abs/2310.11877)

    本研究探讨了大型语言模型(LLMs)在面对无法回答的查询时的行为，发现模型能够编码查询的可回答性，并且第一个解码的标记是一个强有力的指示符。这些发现揭示了LLMs潜在表示中的空间组织，并为改进解码技术提供了新的思路。

    

    大型语言模型(LLMs)展示了令人印象深刻的能力，同时也引发了对其回答准确性的关键担忧。在这个背景下出现的一个主要问题是LLMs如何处理无法回答的查询，往往会导致幻觉行为，原因是过度自信。在本文中，我们探讨了LLMs面对无法回答的查询时的行为。我们问：当生成幻觉回答时，模型是否表示问题无法回答的事实？我们的结果强烈表明，这样的模型对输入查询的可回答性进行编码，第一个解码的标记的表示往往是一个强有力的指示符。这些发现揭示了LLMs潜在表示中的空间组织，揭示了这些模型先前未被探索的方面。此外，它们为开发更好地遵守事实生成的改进解码技术铺平了道路。

    Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of unanswerable queries by LLMs, which often results in hallucinatory behavior, due to overconfidence. In this paper, we explore the behavior of LLMs when presented with unanswerable queries. We ask: do models \textbf{represent} the fact that the question is unanswerable when generating a hallucinatory answer? Our results show strong indications that such models encode the answerability of an input query, with the representation of the first decoded token often being a strong indicator. These findings shed new light on the spatial organization within the latent representations of LLMs, unveiling previously unexplored facets of these models. Moreover, they pave the way for the development of improved decoding techniques with better adherence to factual generation, particul
    
[^35]: AI Nushu: 计算语言学视角下姐妹团结中的语言形成探索

    AI Nushu: An Exploration of Language Emergence in Sisterhood -Through the Lens of Computational Linguistics. (arXiv:2310.11870v1 [cs.CL])

    [http://arxiv.org/abs/2310.11870](http://arxiv.org/abs/2310.11870)

    本文介绍了一种受女书启发的新兴语言系统AI Nushu，通过计算语言学的视角，结合中国文化遗产和女权主义视角，通过两个AI代理人的合作创造了一个标准的中文写作系统。

    

    本文介绍了“AI Nushu”，这是一种受女书（女性专用文字）启发的新兴语言系统，女书是古代中国女性在一个父权社会中被认为是文盲而创造并独自使用的独特语言。在这个交互式装置中，两个人工智能（AI）代理人通过对中文字典和女书语料库的训练，不断观察环境并进行交流，合作创造一个标准的中文写作系统。它从计算语言学的角度提供了一种艺术性解释，将AI技术与中国文化遗产和女权主义视角相结合，创造了一种非西方文字的创作过程。

    This paper presents "AI Nushu," an emerging language system inspired by Nushu (women's scripts), the unique language created and used exclusively by ancient Chinese women who were thought to be illiterate under a patriarchal society. In this interactive installation, two artificial intelligence (AI) agents are trained in the Chinese dictionary and the Nushu corpus. By continually observing their environment and communicating, these agents collaborate towards creating a standard writing system to encode Chinese. It offers an artistic interpretation of the creation of a non-western script from a computational linguistics perspective, integrating AI technology with Chinese cultural heritage and a feminist viewpoint.
    
[^36]: 文本标注手册：机器学习项目的实践指南

    Text Annotation Handbook: A Practical Guide for Machine Learning Projects. (arXiv:2310.11780v1 [cs.CL])

    [http://arxiv.org/abs/2310.11780](http://arxiv.org/abs/2310.11780)

    本手册是一本实践指南，旨在帮助处理文本标注任务。它提供简明扼要的介绍，理论概念的概述以及实用建议，并触及业务、道德和监管问题。适合各种职业的人使用。

    

    本手册是一本关于如何处理文本标注任务的实践指南。它提供了对该主题的简要介绍，理论概念的概述以及实用建议。涵盖的主题主要是技术性的，但也触及了业务、道德和监管问题。重点在于易读性和简明性，而不是完整性和科学严谨性。有标注经验和机器学习知识是有用但不是必需的。该文档可以作为各种职业的团队领导、项目经理、IT架构师、软件开发人员和机器学习工程师的入门书或参考书。

    This handbook is a hands-on guide on how to approach text annotation tasks. It provides a gentle introduction to the topic, an overview of theoretical concepts as well as practical advice. The topics covered are mostly technical, but business, ethical and regulatory issues are also touched upon. The focus lies on readability and conciseness rather than completeness and scientific rigor. Experience with annotation and knowledge of machine learning are useful but not required. The document may serve as a primer or reference book for a wide range of professions such as team leaders, project managers, IT architects, software developers and machine learning engineers.
    
[^37]: 大规模文本到图像模型中检测隐性刻板印象的语言代理

    Language Agents for Detecting Implicit Stereotypes in Text-to-image Models at Scale. (arXiv:2310.11778v1 [cs.CY])

    [http://arxiv.org/abs/2310.11778](http://arxiv.org/abs/2310.11778)

    本文介绍了一种针对文本到图像模型中检测刻板印象的语言代理架构，可自主调用各种工具来促进整个检测过程，并应用于商业产品和开放文本数据集。

    

    最近对扩散模型研究的激增加速了各种人工智能生成内容（AIGC）商业产品中文本到图像模型的采用。虽然这些出色的AIGC产品在消费者中获得了越来越多的认可和激发了热情，但关于这些模型可能无意中强化现有社会刻板印象的问题尚未得到解决。受到语言代理最近的进展的启发，我们在这里介绍了一种专为文本到图像模型中的刻板印象检测而设计的新型代理架构。这种多功能的代理架构能够适应自由形式的检测任务，可以自主调用各种工具来促进整个过程，从生成相应的指令和图像到检测刻板印象。我们基于多个开放文本数据集构建了与刻板印象相关的基准，并将这种架构应用于商业产品和流行的开放文本数据集。

    The recent surge in the research of diffusion models has accelerated the adoption of text-to-image models in various Artificial Intelligence Generated Content (AIGC) commercial products. While these exceptional AIGC products are gaining increasing recognition and sparking enthusiasm among consumers, the questions regarding whether, when, and how these models might unintentionally reinforce existing societal stereotypes remain largely unaddressed. Motivated by recent advancements in language agents, here we introduce a novel agent architecture tailored for stereotype detection in text-to-image models. This versatile agent architecture is capable of accommodating free-form detection tasks and can autonomously invoke various tools to facilitate the entire process, from generating corresponding instructions and images, to detecting stereotypes. We build the stereotype-relevant benchmark based on multiple open-text datasets, and apply this architecture to commercial products and popular ope
    
[^38]: 通过增强一致性建模来改进长文档主题划分模型

    Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling. (arXiv:2310.11772v1 [cs.CL])

    [http://arxiv.org/abs/2310.11772](http://arxiv.org/abs/2310.11772)

    本论文通过增强一致性建模的方式改进了长文档主题划分模型。具体地，通过引入主题感知的句子结构预测和对比语义相似性学习，该模型在捕捉语义一致性和主题划分之间的深层关系方面有了更好的表现。

    

    主题划分对于获取结构化的长文档和改善信息检索等下游任务至关重要。由于其能够自动从大量标注数据中探索主题转变的线索，最近的有监督神经模型极大地推动了长文档主题划分的发展，但对语义一致性和主题划分之间更深层次的关系尚未充分探索。因此，本文增强了有监督模型从结构和相似性两个方面捕捉一致性的能力，进一步提高了主题划分性能，包括主题感知的句子结构预测（TSSP）和对比语义相似性学习（CSSL）。具体而言，提出了TSSP任务，通过学习无序文档中相邻句子的原始关系，强制模型理解结构信息，该无序文档由同时破坏主题和

    Topic segmentation is critical for obtaining structured long documents and improving downstream tasks like information retrieval. Due to its ability of automatically exploring clues of topic shift from a large amount of labeled data, recent supervised neural models have greatly promoted the development of long document topic segmentation, but leaving the deeper relationship of semantic coherence and topic segmentation underexplored. Therefore, this paper enhances the supervised model's ability to capture coherence from both structure and similarity perspectives to further improve the topic segmentation performance, including the Topic-aware Sentence Structure Prediction (TSSP) and Contrastive Semantic Similarity Learning (CSSL). Specifically, the TSSP task is proposed to force the model to comprehend structural information by learning the original relations of adjacent sentences in a disarrayed document, which is constructed by jointly disrupting the original document at the topic and 
    
[^39]: 具有命名实体识别的注释职位广告

    Annotated Job Ads with Named Entity Recognition. (arXiv:2310.11769v1 [cs.CL])

    [http://arxiv.org/abs/2310.11769](http://arxiv.org/abs/2310.11769)

    本文介绍了一个通过命名实体识别（NER）模型对瑞典职位广告进行筛选的方法，并详细讨论了创建高质量数据集的问题和解决方案，并报告了模型的性能。

    

    我们训练了一个命名实体识别（NER）模型，用于筛选瑞典职位广告中的各种有用信息（例如，招聘者要求的技能）。该模型是通过对KB-BERT进行微调而获得的。我们面临的最大挑战是创建一个标记数据集，这需要进行手动标注。本文概述了我们采用的方法，以使注释过程更高效，并确保高质量的数据。我们还报告了结果模型的性能。

    We have trained a named entity recognition (NER) model that screens Swedish job ads for different kinds of useful information (e.g. skills required from a job seeker). It was obtained by fine-tuning KB-BERT. The biggest challenge we faced was the creation of a labelled dataset, which required manual annotation. This paper gives an overview of the methods we employed to make the annotation process more efficient and to ensure high quality data. We also report on the performance of the resulting model.
    
[^40]: 对大规模语言模型在法律判决预测上的综合评估

    A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction. (arXiv:2310.11761v1 [cs.CL])

    [http://arxiv.org/abs/2310.11761](http://arxiv.org/abs/2310.11761)

    本研究对大规模语言模型在法律判决预测上进行了综合评估，结果表明LLMs可以通过类似案例和多项选择的方式提高其在专家法律推理中的表现。此外，我们发现在某些情况下，较弱的LLMs从强大的信息检索系统中获得的收益有限，从而导致信息检索系统的绩效超越LLM+IR的组合。

    

    大规模语言模型（LLMs）已经在特定领域的应用（如法律领域）中展示出了巨大的潜力。然而，关于GPT-4在法律评估方面的争议引发了对它们在现实法律任务中表现的质疑。为了系统地调查它们在法律领域的能力，我们设计了基于LLMs的实用基准解决方案，并在法律判决预测任务上进行了测试。在我们的解决方案中，LLMs可以单独回答开放性问题，或与信息检索（IR）系统配合，从类似案例中学习或解决简化的多项选择问题。我们展示了类似案例和多项选择选项（即提示中包含的标签候选项）可以帮助LLMs回忆起对专家法律推理至关重要的领域知识。此外，我们还提出了一个有趣的悖论，即由于较弱的LLMs从强大的IR系统获得的收益有限，导致IR系统的绩效超过LLM+IR。在这种情况下，LLMs的角色变得重要起来。

    Large language models (LLMs) have demonstrated great potential for domain-specific applications, such as the law domain. However, recent disputes over GPT-4's law evaluation raise questions concerning their performance in real-world legal tasks. To systematically investigate their competency in the law, we design practical baseline solutions based on LLMs and test on the task of legal judgment prediction. In our solutions, LLMs can work alone to answer open questions or coordinate with an information retrieval (IR) system to learn from similar cases or solve simplified multi-choice questions. We show that similar cases and multi-choice options, namely label candidates, included in prompts can help LLMs recall domain knowledge that is critical for expertise legal reasoning. We additionally present an intriguing paradox wherein an IR system surpasses the performance of LLM+IR due to limited gains acquired by weaker LLMs from powerful IR systems. In such cases, the role of LLMs becomes re
    
[^41]: ChatGPT中的情绪识别偏差

    Bias in Emotion Recognition with ChatGPT. (arXiv:2310.11753v1 [cs.RO])

    [http://arxiv.org/abs/2310.11753](http://arxiv.org/abs/2310.11753)

    这项技术报告探讨了ChatGPT在从文本中识别情绪方面的能力，并发现其在不同情绪标签和数据集上的性能有所差异，突显了内在的不稳定性和潜在的偏差。

    

    本技术报告探讨了ChatGPT在从文本中识别情绪的能力，这可以作为交互式聊天机器人、数据标注和心理健康分析等各种应用的基础。虽然先前的研究已经展示了ChatGPT在情感分析方面的基本能力，但其在更微妙的情绪识别方面的表现尚未被探索。我们在此进行了一系列实验，评估了它在不同数据集和情绪标签上的情绪识别性能。我们的研究结果表明，其性能具有合理的可重复性，并且通过微调可以明显改善。然而，性能在不同情绪标签和数据集上存在差异，突显了内在的不稳定性和潜在的偏差。数据集和情绪标签的选择对ChatGPT的情绪识别性能有着重要影响。本文论述了数据集和标签选择的重要性，以及通过微调提升ChatGPT情绪识别能力的潜力。

    This technical report explores the ability of ChatGPT in recognizing emotions from text, which can be the basis of various applications like interactive chatbots, data annotation, and mental health analysis. While prior research has shown ChatGPT's basic ability in sentiment analysis, its performance in more nuanced emotion recognition is not yet explored. Here, we conducted experiments to evaluate its performance of emotion recognition across different datasets and emotion labels. Our findings indicate a reasonable level of reproducibility in its performance, with noticeable improvement through fine-tuning. However, the performance varies with different emotion labels and datasets, highlighting an inherent instability and possible bias. The choice of dataset and emotion labels significantly impacts ChatGPT's emotion recognition performance. This paper sheds light on the importance of dataset and label selection, and the potential of fine-tuning in enhancing ChatGPT's emotion recogniti
    
[^42]: 在多选设置下研究对齐语言模型的不确定性校准

    Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting. (arXiv:2310.11732v1 [cs.LG])

    [http://arxiv.org/abs/2310.11732](http://arxiv.org/abs/2310.11732)

    本研究系统评估了在多选设置下对齐语言模型不确定性校准的影响。研究发现，在这种设置下存在两种不确定性，分别对答案决策和格式偏好负责。对齐模型过度自信的原因之一是这两种不确定性的混淆。

    

    尽管在对齐语言模型（LM）的实际应用中取得了显著进展，但它们倾向于与预训练的LM相比，在输出答案时表现出过度自信。本研究系统评估了对齐过程对多选设置下LM的基于逻辑的不确定性校准的影响。我们首先对对齐LM在校准方面与其预训练对应模型之间的差异进行了认真的实证研究。实验结果显示，在多选设置下，LM存在两种明显的不确定性，分别负责答案决策和LM的格式偏好。然后，我们通过在简单的合成对齐方案中进行微调，研究了这两种不确定性在对齐LM的校准中的作用，并得出结论，对齐LM过度自信的原因之一是这两种不确定性的混淆。此外，我们还检查了常见的事后校准方法的实用性。

    Despite the significant progress made in practical applications of aligned language models (LMs), they tend to be overconfident in output answers compared to the corresponding pre-trained LMs. In this work, we systematically evaluate the impact of the alignment process on logit-based uncertainty calibration of LMs under the multiple-choice setting. We first conduct a thoughtful empirical study on how aligned LMs differ in calibration from their pre-trained counterparts. Experimental results reveal that there are two distinct uncertainties in LMs under the multiple-choice setting, which are responsible for the answer decision and the format preference of the LMs, respectively. Then, we investigate the role of these two uncertainties on aligned LM's calibration through fine-tuning in simple synthetic alignment schemes and conclude that one reason for aligned LMs' overconfidence is the conflation of these two types of uncertainty. Furthermore, we examine the utility of common post-hoc cal
    
[^43]: 量化中文医学大型语言模型中与健康相关的原子知识：一项计算分析

    Quantify Health-Related Atomic Knowledge in Chinese Medical Large Language Models: A Computational Analysis. (arXiv:2310.11722v1 [cs.CL])

    [http://arxiv.org/abs/2310.11722](http://arxiv.org/abs/2310.11722)

    本研究通过构建一个基准，量化了中文医学大型语言模型中与健康相关的原子知识的存储程度，并发现通用LLMs在原子知识和指令遵循能力方面表现更好。两种类型的LLMs都倾向于迎合用户要求。

    

    大型语言模型（LLMs）有潜力通过搜索引擎直接和高效地提供用户的自诊断建议，从而革新用户自诊断的方式。最近的研究主要关注基于GPT-4评估LLMs的质量或其通过医学考试的能力，但没有研究量化存储在LLMs记忆中的健康相关原子知识的程度，而这是LLMs提供更准确建议的基础。在本文中，我们首先构建了一个基准，包括用户自诊断查询中最常见的原子知识类型，共17种原子类型和14048条原子知识。然后，我们对通用和专业LLMs在基准上进行了评估。实验结果表明，在原子知识和指令遵循能力方面，通用LLMs的表现优于专业LLMs。错误分析显示，通用和专业LLMs都是马屁精，即在涉及用户要求时总是迎合用户。

    Large Language Models (LLMs) have the potential to revolutionize the way users self-diagnose through search engines by offering direct and efficient suggestions. Recent studies primarily focused on the quality of LLMs evaluated by GPT-4 or their ability to pass medical exams, no studies have quantified the extent of health-related atomic knowledge stored in LLMs' memory, which is the basis of LLMs to provide more factual suggestions. In this paper, we first constructed a benchmark, including the most common types of atomic knowledge in user self-diagnosis queries, with 17 atomic types and a total of 14, 048 pieces of atomic knowledge. Then, we evaluated both generic and specialized LLMs on the benchmark. The experimental results showcased that generic LLMs perform better than specialized LLMs in terms of atomic knowledge and instruction-following ability. Error analysis revealed that both generic and specialized LLMs are sycophantic, e.g., always catering to users' claims when it comes
    
[^44]: 这里是翻译过的论文标题

    Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding. (arXiv:2310.11721v1 [cs.CL])

    [http://arxiv.org/abs/2310.11721](http://arxiv.org/abs/2310.11721)

    这里是中文总结出的一句话要点

    

    这里是翻译过的论文摘要

    Chain-of-Thought (CoT) is a technique that guides Large Language Models (LLMs) to decompose complex tasks into multi-step reasoning through intermediate steps in natural language form. Briefly, CoT enables LLMs to think step by step. However, although many Natural Language Understanding (NLU) tasks also require thinking step by step, LLMs perform less well than small-scale Masked Language Models (MLMs). To migrate CoT from LLMs to MLMs, we propose Chain-of-Thought Tuning (CoTT), a two-step reasoning framework based on prompt tuning, to implement step-by-step thinking for MLMs on NLU tasks. From the perspective of CoT, CoTT's two-step framework enables MLMs to implement task decomposition; CoTT's prompt tuning allows intermediate steps to be used in natural language form. Thereby, the success of CoT can be extended to NLU tasks through MLMs. To verify the effectiveness of CoTT, we conduct experiments on two NLU tasks: hierarchical classification and relation extraction, and the results 
    
[^45]: 反射调整：数据回收改进了LLM指令调整

    Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning. (arXiv:2310.11716v1 [cs.CL])

    [http://arxiv.org/abs/2310.11716](http://arxiv.org/abs/2310.11716)

    这项研究提出了一种名为"反射调整"的方法，通过自我改进和判断能力来解决LLMs指令调整中的问题。通过借助Oracle LLM回收训练数据，该方法显著提高了LLMs在各个基准测试中的性能。

    

    最近，大规模语言模型(LLMs)的进展拓宽了自然语言理解和生成的范围。值得注意的是，通过指令调整可以改进LLMs的输出控制和与输入的对齐。然而，正如几项研究所指出的那样，训练集中的低质量数据通常对指令调整有害，导致LLMs的输出不一致甚至误导人。我们提出了一种新的方法，称为“反射调整”，通过LLMs的自我改进和判断能力来解决这个问题。这种方法借助一个Oracle LLM来回收原始的训练数据，通过内省和增强数据中的指令和响应的质量。在广泛使用的评估基准上进行的大量实验表明，我们使用回收数据训练的LLMs在各种基准测试中都优于使用现有数据集训练的LLMs。

    Recent advancements in Large Language Models (LLMs) have expanded the horizons of natural language understanding and generation. Notably, the output control and alignment with the input of LLMs can be refined through instruction tuning. However, as highlighted in several studies, low-quality data in the training set are usually detrimental to instruction tuning, resulting in inconsistent or even misleading LLM outputs. We propose a novel method, termed "reflection-tuning," which addresses the problem by self-improvement and judging capabilities of LLMs. This approach utilizes an oracle LLM to recycle the original training data by introspecting and enhancing the quality of instructions and responses in the data. Extensive experiments on widely used evaluation benchmarks show that LLMs trained with our recycled data outperform those trained with existing datasets in various benchmarks.
    
[^46]: 利用粗粒度数据集增强低资源细粒度命名实体识别

    Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets. (arXiv:2310.11715v1 [cs.CL])

    [http://arxiv.org/abs/2310.11715](http://arxiv.org/abs/2310.11715)

    通过利用粗粒度数据集，提出了一种细粒度命名实体识别模型，使用细粒度-粗粒度映射矩阵来显式利用层次结构，并提出了一种不一致性过滤方法，以增强低资源细粒度命名实体识别。

    

    命名实体识别（NER）在细粒度NER场景下常常面临标注数据不足的问题。虽然可以应用K-shot学习技术，但当注释数量超过几十个标签时，性能往往达到饱和。为了解决这个问题，我们利用现有的粗粒度数据集，其中包含大量的标注。一种直接解决这个问题的方法是预训练，它利用粗粒度数据进行表示学习。然而，它无法直接利用细粒度和粗粒度实体之间的关系，尽管细粒度实体类型很可能是粗粒度实体类型的子类别。我们提出了一个带有细粒度-粗粒度（F2C）映射矩阵的细粒度NER模型，以显式地利用层次结构。此外，我们提出了一种不一致性过滤方法，以消除与细粒度不一致的粗粒度实体。

    Named Entity Recognition (NER) frequently suffers from the problem of insufficient labeled data, particularly in fine-grained NER scenarios. Although $K$-shot learning techniques can be applied, their performance tends to saturate when the number of annotations exceeds several tens of labels. To overcome this problem, we utilize existing coarse-grained datasets that offer a large number of annotations. A straightforward approach to address this problem is pre-finetuning, which employs coarse-grained data for representation learning. However, it cannot directly utilize the relationships between fine-grained and coarse-grained entities, although a fine-grained entity type is likely to be a subcategory of a coarse-grained entity type. We propose a fine-grained NER model with a Fine-to-Coarse(F2C) mapping matrix to leverage the hierarchical structure explicitly. In addition, we present an inconsistency filtering method to eliminate coarse-grained entities that are inconsistent with fine-gr
    
[^47]: 学习同时语言手势用于多模态失语类型检测

    Learning Co-Speech Gesture for Multimodal Aphasia Type Detection. (arXiv:2310.11710v1 [cs.CL])

    [http://arxiv.org/abs/2310.11710](http://arxiv.org/abs/2310.11710)

    通过学习语音和手势之间的相关性，我们提出了一种多模态图神经网络，用于准确识别特定失语类型的检测。实验证明我们的方法优于现有方法，达到了最先进的结果（F1 84.2%）。

    

    失语是一种由脑损伤引起的语言障碍，需要准确识别特定的失语类型，如Broca失语和Wernicke失语，以进行有效的治疗。然而，对于开发用于检测不同类型失语的方法，人们并没有给予足够的关注。我们意识到分析同时语言手势对于区分失语类型的重要性，提出了一种多模态图神经网络，利用语音和相应的手势模式进行失语类型检测。通过学习每种失语类型的语音和手势模态之间的相关性，我们的模型可以生成对手势信息敏感的文本表示，从而实现准确的失语类型检测。大量实验证明了我们方法的优越性，实现了最先进的结果（F1 84.2%）。我们还展示了手势特征优于声学特征，突显了手势表达在检测失语类型中的重要性。

    Aphasia, a language disorder resulting from brain damage, requires accurate identification of specific aphasia types, such as Broca's and Wernicke's aphasia, for effective treatment. However, little attention has been paid to developing methods to detect different types of aphasia. Recognizing the importance of analyzing co-speech gestures for distinguish aphasia types, we propose a multimodal graph neural network for aphasia type detection using speech and corresponding gesture patterns. By learning the correlation between the speech and gesture modalities for each aphasia type, our model can generate textual representations sensitive to gesture information, leading to accurate aphasia type detection. Extensive experiments demonstrate the superiority of our approach over existing methods, achieving state-of-the-art results (F1 84.2\%). We also show that gesture features outperform acoustic features, highlighting the significance of gesture expression in detecting aphasia types. We pro
    
[^48]: MISAR：一个具有增强现实的多模式教学系统

    MISAR: A Multimodal Instructional System with Augmented Reality. (arXiv:2310.11699v1 [cs.CL])

    [http://arxiv.org/abs/2310.11699](http://arxiv.org/abs/2310.11699)

    这项研究介绍了一种创新的方法，利用大语言模型（LLMs）从视觉、听觉和情境多模态中吸收信息，并通过自主视频、语音和语境分析实现增强的状态估计，进一步推动了适应性更强的增强现实（AR）系统的发展。

    

    增强现实（AR）需要将视觉、听觉和语言通道无缝集成，以优化人机交互。尽管听觉和视觉输入有助于实时和情境感导向用户指导，但在这个领域中，大语言模型（LLMs）的潜力仍然大多未被利用。我们的研究引入了一种创新的方法，利用LLMs来吸收来自视觉、听觉和情境多模态的信息。针对AR中任务执行量化的独特挑战，我们利用自主视角视频、语音和语境分析。LLMs的集成促进了增强的状态估计，迈向更适应性的AR系统。代码、数据集和演示将在https://github.com/nguyennm1024/misar上提供。

    Augmented reality (AR) requires the seamless integration of visual, auditory, and linguistic channels for optimized human-computer interaction. While auditory and visual inputs facilitate real-time and contextual user guidance, the potential of large language models (LLMs) in this landscape remains largely untapped. Our study introduces an innovative method harnessing LLMs to assimilate information from visual, auditory, and contextual modalities. Focusing on the unique challenge of task performance quantification in AR, we utilize egocentric video, speech, and context analysis. The integration of LLMs facilitates enhanced state estimation, marking a step towards more adaptive AR systems. Code, dataset, and demo will be available at https://github.com/nguyennm1024/misar.
    
[^49]: 自我评估的自适应改进LLMs中的选择性预测

    Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs. (arXiv:2310.11689v1 [cs.CL])

    [http://arxiv.org/abs/2310.11689](http://arxiv.org/abs/2310.11689)

    本研究提出了一种自适应框架，利用自我评估来改进大型语言模型（LLMs）的选择性预测能力。该方法基于参数效率调整，能够适应特定任务并提高其自我评估能力，实验结果表明其优于最先进的选择性预测方法。

    

    大型语言模型（LLMs）在自然语言理解和生成等多种任务中取得了巨大进展。然而，在高风险决策场景中仍然限于其潜在的错误。选择性预测是一种可以通过在LLMs不确定时使其避免预测而提高其可靠性的技术。在本文中，我们提出了一种新颖的自我评估的自适应框架，以提高LLMs的选择性预测性能。我们的框架基于使用参数效率调整来适应特定任务并改进其自我评估能力的思想。我们在各种问答（QA）数据集上评估了我们的方法，并展示其优于最先进的选择性预测方法。例如，在CoQA基准测试中，我们的方法将AUACC从91.23%提高到92.63%，并将AURO

    Large language models (LLMs) have recently shown great advances in a variety of tasks, including natural language understanding and generation. However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors. Selective prediction is a technique that can be used to improve the reliability of the LLMs by allowing them to abstain from making predictions when they are unsure of the answer. In this work, we propose a novel framework for adaptation with self-evaluation to improve the selective prediction performance of LLMs. Our framework is based on the idea of using parameter-efficient tuning to adapt the LLM to the specific task at hand while improving its ability to perform self-evaluation. We evaluate our method on a variety of question-answering (QA) datasets and show that it outperforms state-of-the-art selective prediction methods. For example, on the CoQA benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the AURO
    
[^50]: Softmax的优越性：揭示其相对于线性注意力的性能优势

    Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention. (arXiv:2310.11685v1 [cs.CL])

    [http://arxiv.org/abs/2310.11685](http://arxiv.org/abs/2310.11685)

    通过对softmax和线性注意力机制进行全面比较分析，本论文揭示了softmax注意力在大多数情况下优于线性注意力的潜在原因。

    

    大型Transformer模型在许多自然语言处理任务中取得了最先进的结果。在Transformer架构的重要组成部分中，注意力机制通过利用softmax函数捕捉序列中的标记交互起着关键作用。相反，线性注意力通过线性复杂度近似softmax操作，提供了一种计算效率更高的替代方法。然而，与传统的softmax注意力机制相比，它在性能上表现出明显的降级。在本文中，我们对这两种注意力机制进行了全面的比较分析，揭示了softmax注意力在大多数情况下优于线性注意力的潜在原因。

    Large transformer models have achieved state-of-the-art results in numerous natural language processing tasks. Among the pivotal components of the transformer architecture, the attention mechanism plays a crucial role in capturing token interactions within sequences through the utilization of softmax function.  Conversely, linear attention presents a more computationally efficient alternative by approximating the softmax operation with linear complexity. However, it exhibits substantial performance degradation when compared to the traditional softmax attention mechanism.  In this paper, we bridge the gap in our theoretical understanding of the reasons behind the practical performance gap between softmax and linear attention. By conducting a comprehensive comparative analysis of these two attention mechanisms, we shed light on the underlying reasons for why softmax attention outperforms linear attention in most scenarios.
    
[^51]: 生物医学领域中的描述性知识图谱

    Descriptive Knowledge Graph in Biomedical Domain. (arXiv:2310.11681v1 [cs.CL])

    [http://arxiv.org/abs/2310.11681](http://arxiv.org/abs/2310.11681)

    这个论文提出了一个在生物医学领域中构建描述性知识图谱的新系统，可以从文献中提取有信息量的句子并进行关系搜索和导航。并且该系统使用自动生成描述性句子的模型，减少了人工阅读的工作量。在COVID-19研究中应用该系统展示了其在相关领域的实用性。

    

    我们提出了一个新颖的系统，可以从生物医学文献中自动提取和生成有信息量和描述性的句子，并促进有效的关系知识搜索。与之前检索非相关段落的搜索引擎或探索系统不同，我们的系统将描述性句子组织为一个关系图，使研究人员能够探索相关的生物医学实体（例如，由化学物质治疗的疾病）或间接相关的实体（例如，用于治疗某种疾病的潜在药物）。我们的系统还使用ChatGPT和经过微调的关系合成模型从检索到的信息中生成简洁可靠的描述性句子，减少了人工阅读的需求。使用我们的系统，研究人员可以轻松获得高级知识和详细参考，并可以交互地导航到感兴趣的信息。我们重点介绍了我们系统在COVID-19研究中的应用，以说明其在相关领域的效用

    We present a novel system that automatically extracts and generates informative and descriptive sentences from the biomedical corpus and facilitates the efficient search for relational knowledge. Unlike previous search engines or exploration systems that retrieve unconnected passages, our system organizes descriptive sentences as a relational graph, enabling researchers to explore closely related biomedical entities (e.g., diseases treated by a chemical) or indirectly connected entities (e.g., potential drugs for treating a disease). Our system also uses ChatGPT and a fine-tuned relation synthesis model to generate concise and reliable descriptive sentences from retrieved information, reducing the need for extensive human reading effort. With our system, researchers can easily obtain both high-level knowledge and detailed references and interactively steer to the information of interest. We spotlight the application of our system in COVID-19 research, illustrating its utility in areas 
    
[^52]: 无限制答案范围的开放式常识推理

    Open-ended Commonsense Reasoning with Unrestricted Answer Scope. (arXiv:2310.11672v1 [cs.CL])

    [http://arxiv.org/abs/2310.11672](http://arxiv.org/abs/2310.11672)

    本论文研究了无限制答案范围的开放式常识推理问题。作者采用预训练语言模型在外部知识库上迭代检索推理路径，从而帮助找到最准确的答案。实验结果表明此方法在常识问题上取得了良好的效果。

    

    开放式常识推理被定义为在不提供1）答案候选名单和2）预定义答案范围的情况下解决常识问题。将常识问题转化为问答形式或利用外部知识学习检索方法的常规方法在开放式环境中不太适用，因为这涉及到一个固有的挑战。在没有预定义答案范围或少数候选者的情况下，开放式常识推理需要通过在极大的搜索空间中搜索来预测答案。此外，大多数问题需要隐含的多跳推理，这给我们的问题带来了更多挑战。在这项工作中，我们利用预训练的语言模型在外部知识库上迭代地检索推理路径，这不需要特定任务的监督。推理路径可以帮助我们找到最准确的常识问题答案。我们在两个常识基准数据集上进行实验。

    Open-ended Commonsense Reasoning is defined as solving a commonsense question without providing 1) a short list of answer candidates and 2) a pre-defined answer scope. Conventional ways of formulating the commonsense question into a question-answering form or utilizing external knowledge to learn retrieval-based methods are less applicable in the open-ended setting due to an inherent challenge. Without pre-defining an answer scope or a few candidates, open-ended commonsense reasoning entails predicting answers by searching over an extremely large searching space. Moreover, most questions require implicit multi-hop reasoning, which presents even more challenges to our problem. In this work, we leverage pre-trained language models to iteratively retrieve reasoning paths on the external knowledge base, which does not require task-specific supervision. The reasoning paths can help to identify the most precise answer to the commonsense question. We conduct experiments on two commonsense ben
    
[^53]: MixEdit:再探数据增强及其在语法错误修正中的应用

    MixEdit: Revisiting Data Augmentation and Beyond for Grammatical Error Correction. (arXiv:2310.11671v1 [cs.CL])

    [http://arxiv.org/abs/2310.11671](http://arxiv.org/abs/2310.11671)

    本研究通过引入亲和度和多样性两种度量方法，阐明了数据增强如何改进语法错误修正模型。基于此观察结果，提出了一种名为MixEdit的数据增强方法，可以策略性和动态地增强真实数据，无需额外的单语言数据。

    

    数据增强通过生成伪数据已被证明在语法错误修正领域中缓解数据稀缺的挑战是有效的。已经广泛探索了各种增强策略，其中大部分是基于两个启发式的，即增加伪数据的分布相似性和多样性。然而，这些策略的有效性的基础机制仍不清楚。在本文中，我们旨在阐明数据增强如何改进语法错误修正模型。为此，我们引入了两个可解释且计算效率高的度量方法：亲和度和多样性。我们的研究发现，一个具有高亲和度和适当多样性的优秀语法错误修正数据增强策略能够更好地提高语法错误修正模型的性能。基于这一观察结果，我们提出了MixEdit，一种能够策略性和动态地增强真实数据而不需要额外的单语言数据的数据增强方法。

    Data Augmentation through generating pseudo data has been proven effective in mitigating the challenge of data scarcity in the field of Grammatical Error Correction (GEC). Various augmentation strategies have been widely explored, most of which are motivated by two heuristics, i.e., increasing the distribution similarity and diversity of pseudo data. However, the underlying mechanism responsible for the effectiveness of these strategies remains poorly understood. In this paper, we aim to clarify how data augmentation improves GEC models. To this end, we introduce two interpretable and computationally efficient measures: Affinity and Diversity. Our findings indicate that an excellent GEC data augmentation strategy characterized by high Affinity and appropriate Diversity can better improve the performance of GEC models. Based on this observation, we propose MixEdit, a data augmentation approach that strategically and dynamically augments realistic data, without requiring extra monolingua
    
[^54]: 基于原型的超适配器用于样本高效多任务调整

    Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning. (arXiv:2310.11670v1 [cs.CL])

    [http://arxiv.org/abs/2310.11670](http://arxiv.org/abs/2310.11670)

    基于原型的超适配器（PHA）框架用于样本高效多任务调整，通过引入实例密集的检索器和样本高效的原型超网络生成条件模块，在多任务学习和少样本迁移学习中取得了可比性能的提升，甚至在数据量较小时也能超过其他强基线方法的性能。

    

    参数高效微调（PEFT）已经证明在适应预训练语言模型到下游任务时有效，同时只更新了少量参数。尽管取得了成功，大多数现有方法独立地适应每个任务，没有考虑任务之间的知识传输，并且受限于低数据情景。为了克服这个问题，我们提出了一种基于原型的超适配器（PHA）框架，该框架建立在适配器调整和超网络基础上。它引入了一个实例密集的检索器和一个样本高效的原型超网络来生成条件模块。这导致与现有PEFT方法在多任务学习和少样本迁移学习上相当的性能改进。更重要的是，当可用数据量变小时，我们的方法比其他强基线方法有很大的优势。基于我们在各种数据集上的广泛实证实验，我们证明了PHA在权衡方面取得了更好的结果。

    Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in adapting the pre-trained language models to downstream tasks while only updating a small number of parameters. Despite the success, most existing methods independently adapt to each task without considering knowledge transfer between tasks and are limited to low-data regimes. To overcome this issue, we propose Prototype-based HyperAdapter (PHA), a novel framework built on the adapter-tuning and hypernetwork. It introduces an instance-dense retriever and a prototypical hypernetwork to generate the conditional modules in a sample-efficient manner. This leads to comparable performance improvements against existing PEFT methods on multi-task learning and few-shot transfer learning. More importantly, when the available data size gets smaller, our method outperforms other strong baselines by a large margin. Based on our extensive empirical experiments across various datasets, we demonstrate that PHA strikes a better trade-
    
[^55]: SOTOPIA: 交互式评估语言智能中的社交智能

    SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents. (arXiv:2310.11667v1 [cs.AI])

    [http://arxiv.org/abs/2310.11667](http://arxiv.org/abs/2310.11667)

    SOTOPIA是一个用于评估语言智能中的社交智能的交互式环境。通过模拟复杂的社交互动，并使用全面的评估框架，我们发现不同模型之间的社交智能存在显著差异，特别是在SOTOPIA-hard情景下。GPT-4在这个子集上的目标完成率较低。

    

    人类是社交的存在；我们在日常互动中追求社交目标，这是社交智能的关键方面。然而，人工智能系统在这个领域的能力仍然难以捉摸。我们提出了SOTOPIA，一个开放式环境，用于模拟人工智能代理之间的复杂社交互动并评估它们的社交智能。在我们的环境中，代理人扮演角色，在各种场景下相互协作、合作、交流和竞争，以实现复杂的社交目标。我们模拟了LLM-based代理人与人类之间在这个任务空间内的角色扮演互动，并使用一个名为SOTOPIA-Eval的整体评估框架对它们的表现进行评估。通过SOTOPIA，我们发现这些模型在社交智能方面存在显著差异，并确定了SOTOPIA的一个子集，即SOTOPIA-hard，对所有模型来说都具有挑战性。我们发现在这个子集上，GPT-4的目标完成率显著较低。

    Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completio
    
[^56]: 使用人工智能进行现场测试的项目：自然语言处理与变形金刚

    Field-testing items using artificial intelligence: Natural language processing with transformers. (arXiv:2310.11655v1 [cs.CL])

    [http://arxiv.org/abs/2310.11655](http://arxiv.org/abs/2310.11655)

    这项研究使用了一种名为RoBERTa的人工智能模型来进行英语读写能力考试的现场测试，结果显示模型的行为与人类考生数据有一定的一致性。

    

    五千个RoBERTa模型的变异体，一种能够理解文本语言的人工智能“变形金刚”，完成了一个包含29个多项选择题的英语读写能力考试。利用数据计算了这些题目的心理测量特性，结果显示与人类考生数据的结果存在一定程度的一致性。

    Five thousand variations of the RoBERTa model, an artificially intelligent "transformer" that can understand text language, completed an English literacy exam with 29 multiple-choice questions. Data were used to calculate the psychometric properties of the items, which showed some degree of agreement to those obtained from human examinee data.
    
[^57]: 用基础语言模型进行零样本忠实性评估的文本摘要研究

    Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model. (arXiv:2310.11648v1 [cs.CL])

    [http://arxiv.org/abs/2310.11648](http://arxiv.org/abs/2310.11648)

    本文提出了使用基础语言模型进行零样本忠实性评估的方法，并引入了一种新的度量标准FFLM。实验证明，FFLM在不一致性检测和忠实性评级方面表现优异，并且参数数量减少了24倍。

    

    尽管自然语言生成取得了巨大的进步，但摘要模型仍然存在忠实性问题。先前的研究要么使用在其他任务上训练的模型或领域内的合成数据来评估忠实性，要么使用类似ChatGPT这样的大型模型进行评估。本文提出使用一个中等大小的基础语言模型进行零样本忠实性评估。我们引入了一种新的度量标准FFLM，它是基于概率变化的组合，这种组合是基于一个观点：在输出的文本前加上与输出一致的一段文本将增加预测输出的概率。实验证明，FFLM在不一致性检测和忠实性评级上与ChatGPT相比表现出色，且参数数量减少24倍。FFLM还在其他强基准上取得了改进。

    Despite tremendous improvements in natural language generation, summarization models still suffer from the unfaithfulness issue. Previous work evaluates faithfulness either using models trained on the other tasks or in-domain synthetic data, or prompting a large model such as ChatGPT. This paper proposes to do zero-shot faithfulness evaluation simply with a moderately-sized foundation language model. We introduce a new metric FFLM, which is a combination of probability changes based on the intuition that prefixing a piece of text that is consistent with the output will increase the probability of predicting the output. Experiments show that FFLM performs competitively with or even outperforms ChatGPT on both inconsistency detection and faithfulness rating with 24x fewer parameters. FFLM also achieves improvements over other strong baselines.
    
[^58]: 大型语言模型中事实知识的系统评估

    Systematic Assessment of Factual Knowledge in Large Language Models. (arXiv:2310.11638v1 [cs.CL])

    [http://arxiv.org/abs/2310.11638](http://arxiv.org/abs/2310.11638)

    本研究提出了一个通过利用知识图谱来评估大型语言模型中事实知识的框架，并在通用和特定领域中对最先进的模型进行了系统的评估。实验结果表明，ChatGPT是在所有领域中表现最好的模型。

    

    以往的研究依赖于现有的问答基准来评估大型语言模型（LLMs）中存储的知识。然而，这种方法在涵盖事实知识方面存在局限性，因为它主要集中在通用领域，这可能与预训练数据重叠。本文提出了一个框架，通过利用知识图谱（KGs）来系统评估LLMs的事实知识。我们的框架从给定KG中存储的事实自动生成一组问题和预期答案，然后评估LLMs回答这些问题的准确性。我们在通用领域和特定领域中系统评估了最先进的LLMs与KGs的性能。实验显示，ChatGPT在所有领域中始终是表现最好的。我们还发现，LLMs的性能取决于指令微调、领域和问题的复杂性，并容易受到对抗性环境的影响。

    Previous studies have relied on existing question-answering benchmarks to evaluate the knowledge stored in large language models (LLMs). However, this approach has limitations regarding factual knowledge coverage, as it mostly focuses on generic domains which may overlap with the pretraining data. This paper proposes a framework to systematically assess the factual knowledge of LLMs by leveraging knowledge graphs (KGs). Our framework automatically generates a set of questions and expected answers from the facts stored in a given KG, and then evaluates the accuracy of LLMs in answering these questions. We systematically evaluate the state-of-the-art LLMs with KGs in generic and specific domains. The experiment shows that ChatGPT is consistently the top performer across all domains. We also find that LLMs performance depends on the instruction finetuning, domain and question complexity and is prone to adversarial context.
    
[^59]: MAGNIFICo：评估大型语言模型在上下文学习能力上对新颖解释的概括能力

    MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations. (arXiv:2310.11634v1 [cs.CL])

    [http://arxiv.org/abs/2310.11634](http://arxiv.org/abs/2310.11634)

    本文评估了大型语言模型在上下文学习能力上对新颖解释的概括能力，并证明了大型语言模型能够理解新颖解释，尤其是从自然语言描述和长对话中获取的解释。然而，仍然需要进一步改进。

    

    人类具有一种卓越的能力，即将新颖的解释赋予语言表达，使他们能够学习新单词并理解社群特定的内涵。然而，大型语言模型（LLMs）具有知识截断，并且重复微调成本高昂，因此，LLMs在上下文学习中学习新颖解释是至关重要的。在本文中，我们系统分析了LLMs使用上下文学习获取新颖解释的能力。为了促进我们的研究，我们引入了MAGNIFICo，这是一个在文本到SQL语义解析框架内实现的评估套件，它包括多样的令牌和提示设置，以模拟现实世界的复杂性。MAGNIFICo上的实验结果表明，LLMs展现出了一个令人惊讶的强大能力，可以从自然语言描述以及长对话中的讨论中理解新颖解释。然而，我们的发现也凸显出进一步改进的需求，特别是

    Humans possess a remarkable ability to assign novel interpretations to linguistic expressions, enabling them to learn new words and understand community-specific connotations. However, Large Language Models (LLMs) have a knowledge cutoff and are costly to finetune repeatedly. Therefore, it is crucial for LLMs to learn novel interpretations in-context. In this paper, we systematically analyse the ability of LLMs to acquire novel interpretations using in-context learning. To facilitate our study, we introduce MAGNIFICo, an evaluation suite implemented within a text-to-SQL semantic parsing framework that incorporates diverse tokens and prompt settings to simulate real-world complexity. Experimental results on MAGNIFICo demonstrate that LLMs exhibit a surprisingly robust capacity for comprehending novel interpretations from natural language descriptions as well as from discussions within long conversations. Nevertheless, our findings also highlight the need for further improvements, partic
    
[^60]: 学习您的标记：用于语言模型的单词池化标记化

    Learn Your Tokens: Word-Pooled Tokenization for Language Modeling. (arXiv:2310.11628v1 [cs.CL])

    [http://arxiv.org/abs/2310.11628](http://arxiv.org/abs/2310.11628)

    使用"学习您的标记"方案，利用单词边界将字节/字符汇聚成单词表示形式，以改善标记化策略的局限性并提高语言模型的性能。

    

    语言模型通常将文本标记化为子词，使用确定性的、手工设计的启发式方法将字符组合成更长的表层字符串（如 'ing'）或整个单词。最近的文献反复展示了这种标记化策略的局限性，特别是对于非英文的文档和表示数字。另一方面，字节/字符级语言模型受限制较少，但在自我注意计算中存在序列描述长度增加和后续二次扩展的问题。最近对这些上下文长度进行固定大小卷积压缩和限制的尝试是有益的，但完全忽略了单词边界。本文考虑了一种替代的“学习您的标记”方案，利用单词边界将字节/字符汇聚成单词表示形式，然后将其馈送到主要语言模型中，再对每个单词并行解码个别的字符/字节。我们发现我们的

    Language models typically tokenize text into subwords, using a deterministic, hand-engineered heuristic of combining characters into longer surface-level strings such as 'ing' or whole words. Recent literature has repeatedly shown the limitations of such a tokenization strategy, particularly for documents not written in English and for representing numbers. On the other extreme, byte/character-level language models are much less restricted but suffer from increased sequence description lengths and a subsequent quadratic expansion in self-attention computation. Recent attempts to compress and limit these context lengths with fixed size convolutions is helpful but completely ignores the word boundary. This paper considers an alternative 'learn your tokens' scheme which utilizes the word boundary to pool bytes/characters into word representations, which are fed to the primary language model, before again decoding individual characters/bytes per word in parallel. We find that our moderatel
    
[^61]: 揭示语言模型中的普遍智能因子：一种心理测量方法

    Unveiling the General Intelligence Factor in Language Models: A Psychometric Approach. (arXiv:2310.11616v1 [cs.CL])

    [http://arxiv.org/abs/2310.11616](http://arxiv.org/abs/2310.11616)

    本研究利用心理测量理论揭示了语言模型中的普遍智能因子g的存在，并发现了该因子解释模型性能方差的85%，为模型评估和开发提供了统一的指标。

    

    本研究采用心理测量理论，揭示了语言模型中普遍智能因子g的存在，并扩展了该理论在人类和某些动物物种中的应用。通过对两个大型数据集Open LLM Leaderboard（包含1,232个模型）和General Language Understanding Evaluation（GLUE）Leaderboard（包含88个模型）进行因子分析，我们发现了一个具有一维性和高度稳定性的g因子，可以解释模型性能方差的85%。研究还发现模型大小和g之间的中度相关性为0.48。在语言模型中发现g因子为模型评估提供了统一的指标，为更强大、基于g因子的模型能力评估开辟了新的途径。这些发现为从心理测量的角度理解和未来研究人工智能提供了基础，并对模型评估和开发具有实际意义。

    This study uncovers the factor of general intelligence, or g, in language models, extending the psychometric theory traditionally applied to humans and certain animal species. Utilizing factor analysis on two extensive datasets Open LLM Leaderboard with 1,232 models and General Language Understanding Evaluation (GLUE) Leaderboard with 88 models - we find compelling evidence for a unidimensional, highly stable g factor that accounts for 85% of the variance in model performance. The study also finds a moderate correlation of .48 between model size and g. The discovery of g in language models offers a unified metric for model evaluation and opens new avenues for more robust, g-based model ability assessment. These findings lay the foundation for understanding and future research on artificial general intelligence from a psychometric perspective and have practical implications for model evaluation and development.
    
[^62]: 语言模型作为零-shot轨迹生成器

    Language Models as Zero-Shot Trajectory Generators. (arXiv:2310.11604v1 [cs.RO])

    [http://arxiv.org/abs/2310.11604](http://arxiv.org/abs/2310.11604)

    本文研究了使用大型语言模型（LLMs）作为零-shot轨迹生成器的可能性。通过给予LLM物体检测和分割视觉模型的访问权限，研究人员发现LLMs能够直接预测操作技能中的末端执行器姿态序列，并在26个真实世界的语言任务中取得了良好效果。这一研究突破了对LLMs在机器人技术中的限制，揭示了LLMs确实具有对操作任务的理解能力。

    

    近期研究表明，大型语言模型（LLMs）在给予低级技能选择时能够作为机器人的高级规划器。然而，通常认为LLMs不具备足够的知识来用于低级轨迹生成。在本研究中，我们详细探讨了这种假设，并调查了当给予LLM（GPT-4）仅能访问物体检测和分割视觉模型时，它能否直接预测一系列密集的末端执行器姿态用于操作技能。我们研究了一个单一的任务不可知提示，没有任何上下文示例、运动原语或外部轨迹优化器，它在26个真实世界的基于语言的任务中的表现，如“打开瓶盖”和“用海绵擦拭盘子”，以及我们调查了这个提示中哪些设计选择最有效。我们的结论突破了对LLMs在机器人技术上的限制，并首次揭示了LLMs确实具有对操作任务的理解能力。

    Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation skills, when given access to only object detection and segmentation vision models. We study how well a single task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers, can perform across 26 real-world language-based tasks, such as "open the bottle cap" and "wipe the plate with the sponge", and we investigate which design choices in this prompt are the most effective. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding o
    
[^63]: 使用大型语言模型自动评价个性化文本生成

    Automated Evaluation of Personalized Text Generation using Large Language Models. (arXiv:2310.11593v1 [cs.CL])

    [http://arxiv.org/abs/2310.11593](http://arxiv.org/abs/2310.11593)

    这项研究提出了一种使用大型语言模型自动评价个性化文本生成的方法。传统的自动评价指标无法捕捉个性化质量的微妙差别，而人工判断又昂贵且困难。因此，本研究提出了一种新颖的评估方法，能够自动测量个性化、质量和相关性这三个重要语义方面。

    

    个性化文本生成提供了一种针对用户个人背景交付内容的专门机制。尽管在这个领域的研究进展迅速，但评估仍然是一个挑战。传统的自动评价指标（如BLEU和ROUGE）主要衡量与人工参考文本的词汇相似度，并不能区分个性化与其他微妙的语义方面，因此无法捕捉个性化生成内容质量的细微差别。另一方面，人工判断是昂贵的，特别是在个性化评估领域。受到这些挑战的启发，我们探索了使用大型语言模型（LLMs）来评估个性化文本生成，并检验它们理解细致的用户背景的能力。我们提出了AuPEL，一种新颖的评估方法，将生成文本的个性化、质量和相关性三个主要语义方面提取并自动测量。

    Personalized text generation presents a specialized mechanism for delivering content that is specific to a user's personal context. While the research progress in this area has been rapid, evaluation still presents a challenge. Traditional automated metrics such as BLEU and ROUGE primarily measure lexical similarity to human-written references, and are not able to distinguish personalization from other subtle semantic aspects, thus falling short of capturing the nuances of personalized generated content quality. On the other hand, human judgments are costly to obtain, especially in the realm of personalized evaluation. Inspired by these challenges, we explore the use of large language models (LLMs) for evaluating personalized text generation, and examine their ability to understand nuanced user context. We present AuPEL, a novel evaluation method that distills three major semantic aspects of the generated text: personalization, quality and relevance, and automatically measures these as
    
[^64]: 用语言模型引导获取人类偏好

    Eliciting Human Preferences with Language Models. (arXiv:2310.11589v1 [cs.CL])

    [http://arxiv.org/abs/2310.11589](http://arxiv.org/abs/2310.11589)

    本文介绍了一种生成式主动任务引导（GATE）的学习框架，该框架通过与用户进行自由形式的、基于语言的交互来引导和推断预期行为。在实验中展示，通过GATE引导的语言模型通常比用户编写的提示或标签更具信息量。

    

    语言模型可以通过使用标注示例或自然语言提示来执行目标任务。但是，在选择示例或撰写提示时可能具有挑战性——特别是在涉及异常情况、要求精确表达模糊偏好或需要准确的语言模型行为认知的任务中。我们提出使用*语言模型本身*来引导任务规范的过程。在本文中，我们介绍**生成式主动任务引导（GATE）**：一种学习框架，在其中模型通过与用户进行自由形式的、基于语言的交互来引导并推断预期行为。我们在三个领域研究了GATE：电子邮件验证、内容推荐和道德推理。在预先注册的实验中，我们展示了提示执行GATE的语言模型（例如通过生成开放式问题或合成信息丰富的边界案例）所引发的响应通常比用户编写的提示或标签更具信息量。用户报告称，交互式任务引导的方法能够有效地帮助他们表达偏好和指导模型。

    Language models (LMs) can be directed to perform target tasks by using labeled examples or natural language prompts. But selecting examples or writing prompts for can be challenging--especially in tasks that involve unusual edge cases, demand precise articulation of nebulous preferences, or require an accurate mental model of LM behavior. We propose to use *LMs themselves* to guide the task specification process. In this paper, we introduce **Generative Active Task Elicitation (GATE)**: a learning framework in which models elicit and infer intended behavior through free-form, language-based interaction with users. We study GATE in three domains: email validation, content recommendation, and moral reasoning. In preregistered experiments, we show that LMs prompted to perform GATE (e.g., by generating open-ended questions or synthesizing informative edge cases) elicit responses that are often more informative than user-written prompts or labels. Users report that interactive task elicitat
    
[^65]: BasahaCorpus: 菲律宾中央语系语言可读性评估的扩展语言资源

    BasahaCorpus: An Expanded Linguistic Resource for Readability Assessment in Central Philippine Languages. (arXiv:2310.11584v1 [cs.CL])

    [http://arxiv.org/abs/2310.11584](http://arxiv.org/abs/2310.11584)

    BasahaCorpus 是一个扩展菲律宾中央语系语言可读性评估的语料库，利用表层特征、音节模式和n-gram重叠特征训练了ARA模型，并提出了一种新的层次化跨语言建模方法；研究发现跨语言模型在低资源环境中是有效的。

    

    目前自动可读性评估（ARA）的研究主要集中在提高英语等资源较丰富的语言模型的性能上。在这项工作中，我们引入并发布了BasahaCorpus作为一个旨在拓展菲律宾低资源语言可读性评估的可用语料库和基准模型的倡议的一部分。我们编译了一个由Hiligaynon，Minasbate，Karay-a和Rinconada四种语言编写的短篇小说故事的语料库，这些语言属于菲律宾中央语系家族树的子分支，我们使用表层特征、音节模式和n-gram重叠特征训练ARA模型。我们还提出了一种新的层次化跨语言建模方法，利用语言在家族树中的位置以增加可用的训练数据量。我们的研究取得了令人鼓舞的结果，支持先前工作展示了跨语言模型在低资源环境中的有效性，以及高度信息化的相似性。

    Current research on automatic readability assessment (ARA) has focused on improving the performance of models in high-resource languages such as English. In this work, we introduce and release BasahaCorpus as part of an initiative aimed at expanding available corpora and baseline models for readability assessment in lower resource languages in the Philippines. We compiled a corpus of short fictional narratives written in Hiligaynon, Minasbate, Karay-a, and Rinconada -- languages belonging to the Central Philippine family tree subgroup -- to train ARA models using surface-level, syllable-pattern, and n-gram overlap features. We also propose a new hierarchical cross-lingual modeling approach that takes advantage of a language's placement in the family tree to increase the amount of available training data. Our study yields encouraging results that support previous work showcasing the efficacy of cross-lingual models in low-resource settings, as well as similarities in highly informative 
    
[^66]: 什么是一个好问题？基于任务的询问与事实级遮蔽。

    What is a good question? Task-oriented asking with fact-level masking. (arXiv:2310.11571v1 [cs.CL])

    [http://arxiv.org/abs/2310.11571](http://arxiv.org/abs/2310.11571)

    本论文提出了基于任务的询问（TOA）的概念和框架，介绍了一种用于生成对推理任务有用答案的问题的方法。同时还提出了一种事实级遮蔽（FLM）的技术，用于将自然语言数据集转换为自我监督的TOA数据集。

    

    提问是现实生活中合作推理任务（如问答）的重要组成部分。例如，一个法律助手聊天机器人在没有用户情况的具体信息的情况下可能无法提供准确的建议。然而，通常会直接使用大型语言模型来解决推理任务，而不会向用户或第三方提出后续问题。我们将这个问题称为基于任务的询问（TOA）。零-shot聊天模型可以执行TOA，但它们的训练主要基于下一个词预测，而不是问题是否对成功的合作有帮助。为了能够训练和评估TOA模型，我们提出了自然语言任务导向询问的定义和框架，即生成能够为推理任务提供有用答案的问题的问题。我们还提出了事实级遮蔽（FLM）的方法，通过省略特定的部分将自然语言数据集转换为自我监督的TOA数据集。

    Asking questions is an important element of real-life collaboration on reasoning tasks like question answering. For example, a legal assistant chatbot may be unable to make accurate recommendations without specific information on the user's circumstances. However, large language models are usually deployed to solve reasoning tasks directly without asking follow-up questions to the user or third parties. We term this problem task-oriented asking (TOA). Zero-shot chat models can perform TOA, but their training is primarily based on next-token prediction rather than whether questions contribute to successful collaboration. To enable the training and evaluation of TOA models, we present a definition and framework for natural language task-oriented asking, the problem of generating questions that result in answers useful for a reasoning task. We also present fact-level masking (FLM), a procedure for converting natural language datasets into self-supervised TOA datasets by omitting particula
    
[^67]: 个性化汤：通过事后合并参数进行个性化大型语言模型对齐

    Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging. (arXiv:2310.11564v1 [cs.CL])

    [http://arxiv.org/abs/2310.11564](http://arxiv.org/abs/2310.11564)

    本研究研究了将大型语言模型与个性化的人类反馈对齐的问题，通过将对齐建模为多目标强化学习，将偏好分解为多个维度，可以实现个性化对齐，并通过参数合并进行有效组合。

    

    尽管来自人类反馈的强化学习（RLHF）能够将大型语言模型（LLM）与一般的、综合的人类偏好进行对齐，但对于学习多样化的个体观点来说并不是最优的。在这项工作中，我们研究了从个性化人类反馈中进行强化学习（RLPHF）的问题，其中LLM通过将对齐建模为多目标强化学习（MORL）问题，以与多个（有时相互冲突的）偏好进行对齐。与强单目标基线相比，我们展示了通过将偏好分解为多个维度可以实现个性化对齐。这些维度是基于用户声明为理想的个性化特征进行定义的。在这项工作中，我们展示了它们可以通过分布式训练进行高效独立地训练，并通过参数合并进行事后有效地组合。代码可以在https://github.com/joeljang/RLPHF上获得。

    While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with general, aggregate human preferences, it is suboptimal for learning diverse, individual perspectives. In this work, we study Reinforcement Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are aligned to multiple (sometimes conflicting) preferences by modeling alignment as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong single-objective baselines, we show that we can achieve personalized alignment by decomposing preferences into multiple dimensions. These dimensions are defined based on personalizations that are declared as desirable by the user. In this work, we show that they can be efficiently trained independently in a distributed manner and combined effectively post-hoc through parameter merging. The code is available at https://github.com/joeljang/RLPHF.
    
[^68]: MUST&P-SRL: 多语言和统一音节标记的文本和音韵领域中的语音表示学习

    MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning. (arXiv:2310.11541v1 [cs.CL])

    [http://arxiv.org/abs/2310.11541](http://arxiv.org/abs/2310.11541)

    本文提出了一种多语言和统一音节标记的文本和音韵领域中的语音表示学习方法，通过自动音节化单词并生成宝贵注释，适用于语音表示学习、语音单元发现和语音因素解缠。

    

    在本文中，我们提出了一种语言特征提取的方法，特别关注多种语言中自动音节化单词，并设计与强制对齐工具Montreal Forced Aligner（MFA）兼容。在文本和音韵领域中，我们的方法专注于从文本中提取音标转录、重音标记和统一的自动音节化。该系统采用了开源组件和资源构建。通过消融研究，我们证明了我们的方法在自动音节化多种语言（英语、法语和西班牙语）的单词方面的有效性。此外，我们将该技术应用于CMU ARCTIC数据集的转录中，生成了有助于语音表示学习、语音单元发现和语音因素解缠的宝贵注释，在线可用。

    In this paper, we present a methodology for linguistic feature extraction, focusing particularly on automatically syllabifying words in multiple languages, with a design to be compatible with a forced-alignment tool, the Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our method focuses on the extraction of phonetic transcriptions from text, stress marks, and a unified automatic syllabification (in text and phonetic domains). The system was built with open-source components and resources. Through an ablation study, we demonstrate the efficacy of our approach in automatically syllabifying words from several languages (English, French and Spanish). Additionally, we apply the technique to the transcriptions of the CMU ARCTIC dataset, generating valuable annotations available online\footnote{\url{https://github.com/noetits/MUST_P-SRL}} that are ideal for speech representation learning, speech unit discovery, and disentanglement of speech factors in several speech-r
    
[^69]: 多阶段大型语言模型纠错用于语音识别

    Multi-stage Large Language Model Correction for Speech Recognition. (arXiv:2310.11532v1 [cs.CL])

    [http://arxiv.org/abs/2310.11532](http://arxiv.org/abs/2310.11532)

    本文提出了一种多阶段的方法，通过结合传统语言模型重新评分和大型语言模型提示，在语音识别中获得了显著的性能提升。

    

    本文研究了使用大型语言模型（LLMs）来改进竞争性语音识别系统性能的方法。与传统语言模型专注于单一数据领域不同，LLMs的崛起为我们提供了机会，既能推动最先进的ASR性能的极限，又能在多个领域中实现更高的鲁棒性和有效的泛化能力。基于此，我们提出了一种新颖的多阶段方法，将传统语言模型重新评分和LLM提示相结合。具体而言，该方法有两个阶段：第一阶段使用语言模型对ASR假设的N个最佳列表进行重新评分，并进行置信度检查；第二阶段使用提示对第一阶段置信度较低的结果进行ASR错误修正。我们的实验结果表明了该方法的有效性，相对于竞争性ASR系统，在WER上取得了10%~20%的相对改善。

    In this paper, we investigate the usage of large language models (LLMs) to improve the performance of competitive speech recognition systems. Different from traditional language models that focus on one single data domain, the rise of LLMs brings us the opportunity to push the limit of state-of-the-art ASR performance, and at the same time to achieve higher robustness and generalize effectively across multiple domains. Motivated by this, we propose a novel multi-stage approach to combine traditional language model re-scoring and LLM prompting. Specifically, the proposed method has two stages: the first stage uses a language model to re-score an N-best list of ASR hypotheses and run a confidence check; The second stage uses prompts to a LLM to perform ASR error correction on less confident results from the first stage. Our experimental results demonstrate the effectiveness of the proposed method by showing a 10% ~ 20% relative improvement in WER over a competitive ASR system -- across m
    
[^70]: 群体偏好优化：大规模语言模型的少样本对齐

    Group Preference Optimization: Few-Shot Alignment of Large Language Models. (arXiv:2310.11523v1 [cs.LG])

    [http://arxiv.org/abs/2310.11523](http://arxiv.org/abs/2310.11523)

    这项研究介绍了一种名为群体偏好优化（GPO）的对齐框架，可以以少样本的方式将大规模语言模型（LLMs）引导到个别群体的偏好。通过在基本LLM上加入独立的transformer模块来预测群体偏好，并通过元学习进行训练，GPO经过严格评估验证了其有效性。

    

    大规模语言模型（LLMs）的许多应用，从聊天机器人到创意写作，都需要细致入微的主观判断，这些判断在不同群体之间可能存在显著差异。现有的对齐算法在每个群体上对齐的成本很高，对于实际应用场景而言，需要大量的群体特定偏好数据和计算资源。我们引入了群体偏好优化（GPO），这是一个对齐框架，可以以少样本的方式将语言模型引导到个别群体的偏好。在GPO中，我们使用一个独立的transformer模块来扩充基本LLM，用于预测群体对LLM生成内容的偏好。对于少样本学习，我们将这个模块参数化为一个上下文自回归的transformer，并通过元学习在多个群体上进行训练。我们通过严格的评估，使用不同规模的LLM在三个人类意见适应任务上验证了GPO的效果。

    Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups. Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations. For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups. We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. These tasks involve adapting to the preferences
    
[^71]: 自动新闻摘要

    Automatic News Summerization. (arXiv:2310.11520v1 [cs.CL])

    [http://arxiv.org/abs/2310.11520](http://arxiv.org/abs/2310.11520)

    本研究比较和评估了抽取式和生成式方法在新闻文本摘要上的效果，并使用ROUGE得分进行质量评估。最佳表现模型被集成到一个Web应用程序中，以评估其在现实世界中的能力和用户体验。

    

    自然语言处理在现实世界中的应用正在蓬勃发展，其中之一是针对包括新闻文章在内的大型文本的文本摘要。本研究论文对新闻文本摘要的抽取式和生成式方法进行了广泛的比较评估，并重点分析了ROUGE得分。研究采用了CNN-Daily Mail数据集，其中包含了新闻文章和人工生成的参考摘要。评估使用ROUGE得分来评估生成摘要的效果和质量。在评估之后，我们将最佳表现模型集成到一个Web应用程序中，评估其在现实世界中的能力和用户体验。

    Natural Language Processing is booming with its applications in the real world, one of which is Text Summarization for large texts including news articles. This research paper provides an extensive comparative evaluation of extractive and abstractive approaches for news text summarization, with an emphasis on the ROUGE score analysis. The study employs the CNN-Daily Mail dataset, which consists of news articles and human-generated reference summaries. The evaluation employs ROUGE scores to assess the efficacy and quality of generated summaries. After Evaluation, we integrate the best-performing models on a web application to assess their real-world capabilities and user experience.
    
[^72]: Self-RAG: 通过自我反思学习检索、生成和评论

    Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. (arXiv:2310.11511v1 [cs.CL])

    [http://arxiv.org/abs/2310.11511](http://arxiv.org/abs/2310.11511)

    Self-RAG是一种通过检索和自我反思提高语言模型质量和事实性的框架。

    

    尽管大型语言模型（LLMs）具有显著的能力，但由于它们完全依赖于它们所包含的参数化知识，因此往往会产生含有事实不准确性的响应。检索增强生成（RAG）是一种通过检索相关知识增强LM的临时方法，可以减少这些问题。然而，不加选择地检索并结合一定数量的检索段落，而不考虑检索是否必要或段落是否相关，会降低LM的多功能性或导致无效的响应生成。我们引入了一种称为Self-Reflective Retrieval-Augmented Generation （Self-RAG）的新框架，通过检索和自我反思提高LM的质量和事实性。我们的框架训练了一个单独的任意LM，它能够根据需求自适应地检索段落，并使用特殊的标记，称为反思标记，生成和反思检索的段落和自身的生成结果。

    Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM 
    
[^73]: CoMPosT: LLM模拟中的漫画表现特征和评估

    CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations. (arXiv:2310.11501v1 [cs.CL])

    [http://arxiv.org/abs/2310.11501](http://arxiv.org/abs/2310.11501)

    利用CoMPosT框架，我们提出了一种对LLM模拟进行特征化的方法，评估其是否存在夸张刻板化，并发现在某些情况下存在夸张刻板化的现象。

    

    最近的研究致力于使用语言模型(LLMs)模拟特定人口群体在社会科学实验和舆论调查等情境中的反应，以捕捉人类行为的细微差别。然而，目前还没有确定的方法来讨论或评估这种LLM模拟的质量。此外，人们越来越担心这些LLM模拟是对他们所模拟的人物进行夸张刻板化的表现，未能捕捉到人的多维性并延续了刻板印象。为了弥补这些差距，我们提出了CoMPosT框架，用四个维度来描述LLM模拟：语境、模型、角色和主题。我们使用这个框架来衡量开放式LLM模拟对夸张刻板化的敏感性，通过两个标准来定义：个性化和夸张。我们评估了现有LLM模拟工作中的场景中夸张刻板化的程度。我们发现对于GPT-4，特定群体（政治和边缘群体）和

    Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and
    
[^74]: BaitBuster-Bangla:一个包含多特征和多模态分析的用于孟加拉语Clickbait检测的综合数据集

    BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in Bangla with Multi-Feature and Multi-Modal Analysis. (arXiv:2310.11465v1 [cs.LG])

    [http://arxiv.org/abs/2310.11465](http://arxiv.org/abs/2310.11465)

    本研究提出了一个大规模的孟加拉语YouTube clickbait多模态数据集，为研究人员提供了在低资源语言中建模clickbait现象的重要价值，并且可以开发出更复杂的跨语言检测方法。

    

    本研究提出了一个大规模的孟加拉语YouTube clickbait多模态数据集，通过使用YouTube API和Python网络自动化框架自动收集了253,070个数据点。该数据集包含了来自58个孟加拉语YouTube频道的单个视频的18个不同的特征，这些特征分类为元数据、主要内容、参与统计和标签。对这些特征进行了严格的预处理，去噪声、去重复和去偏差，确保了无偏倚和可靠的分析。作为迄今为止最大且最强大的孟加拉语clickbait语料库，该数据集对于自然语言处理和数据科学研究人员来说具有重要价值，他们希望在低资源语言中推进clickbait现象的建模。它的多模态性质使得可以对clickbait进行全面的分析，涵盖内容、用户交互和语言维度，以开发具有跨语言应用的更复杂的检测方法。

    This study presents a large multi-modal Bangla YouTube clickbait dataset consisting of 253,070 data points collected through an automated process using the YouTube API and Python web automation frameworks. The dataset contains 18 diverse features categorized into metadata, primary content, engagement statistics, and labels for individual videos from 58 Bangla YouTube channels. A rigorous preprocessing step has been applied to denoise, deduplicate, and remove bias from the features, ensuring unbiased and reliable analysis. As the largest and most robust clickbait corpus in Bangla to date, this dataset provides significant value for natural language processing and data science researchers seeking to advance modeling of clickbait phenomena in low-resource languages. Its multi-modal nature allows for comprehensive analyses of clickbait across content, user interactions, and linguistic dimensions to develop more sophisticated detection methods with cross-linguistic applications.
    
[^75]: 发挥LLMs的能量：通过新闻标题生成的视角评估人工智能协作创作

    Harnessing the Power of LLMs: Evaluating Human-AI text Co-Creation through the Lens of News Headline Generation. (arXiv:2310.10706v1 [cs.CL])

    [http://arxiv.org/abs/2310.10706](http://arxiv.org/abs/2310.10706)

    该研究通过对LLMs辅助新闻标题生成的人工智能协作方法进行比较，发现引导和选择模型输出能够带来最大的效益，并且与自由编辑相比并不损害参与者对控制的感知。

    

    为了探索人类如何最好地利用LLMs进行写作，并了解与这些模型的交互如何影响写作过程中的所有权感和信任度，我们在LLM辅助新闻标题生成的背景下比较了常见的人工智能协作类型（例如，引导系统，从系统输出中进行选择，后期编辑输出）。尽管LLMs单独可以生成令人满意的新闻标题，但平均而言，人类的控制是需要的，以修复不可取的模型输出。在各种交互方法中，引导和选择模型输出增加了最多的效益，代价最低（时间和精力）。此外，人工智能协助并没有损害参与者对控制的感知，与自由编辑相比。

    To explore how humans can best leverage LLMs for writing and how interacting with these models affects feelings of ownership and trust in the writing process, we compared common human-AI interaction types (e.g., guiding system, selecting from system outputs, post-editing outputs) in the context of LLM-assisted news headline generation. While LLMs alone can generate satisfactory news headlines, on average, human control is needed to fix undesirable model outputs. Of the interaction methods, guiding and selecting model output added the most benefit with the lowest cost (in time and effort). Further, AI assistance did not harm participants' perception of control compared to freeform editing.
    
[^76]: 用大型语言模型进行语义解析，用于复杂的零样本对话状态跟踪的更新策略

    Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking. (arXiv:2310.10520v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10520](http://arxiv.org/abs/2310.10520)

    本论文提出了ParsingDST方法，利用大型语言模型和语义解析技术，实现了复杂的零样本对话状态跟踪的更新策略，并在实验中展示了明显的改进。

    

    零样本对话状态跟踪（DST）解决了获取和注释面向任务的对话的挑战，这可能耗时费力。然而，DST超出了简单的填槽，需要有效的更新策略来跟踪对话状态随着对话的进行。本文提出了ParsingDST，一种新的In-Context Learning（ICL）方法，以引入额外的复杂更新策略用于零样本DST。我们的方法通过利用强大的大型语言模型（LLMs）并通过语义解析将原始对话文本转换为JSON作为一个中间状态来重新定义DST任务。我们还设计了一个新颖的框架，其中包括更多的模块来确保文本到JSON过程中更新策略的有效性。实验结果表明，我们的方法在MultiWOZ数据集上优于现有的零样本DST方法，在联合目标准确率（JGA）和槽准确度方面与现有的ICL方法相比呈现出显著改进。

    Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state. We also design a novel framework that includes more modules to ensure the effectiveness of updating strategies in the text-to-JSON process. Experimental results demonstrate that our approach outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL
    
[^77]: 使用大型语言模型的文本摘要: MPT-7b-instruct、Falcon-7b-instruct和OpenAI Chat-GPT模型的比较研究

    Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models. (arXiv:2310.10449v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10449](http://arxiv.org/abs/2310.10449)

    本研究通过比较MPT-7b-instruct, Falcon-7b-instruct和OpenAI Chat-GPT模型，在不同的数据集上使用不同的超参数进行了文本摘要实验。实验结果表明，text-davinci-003模型表现最佳，并且提供了大型语言模型在文本摘要中的性能综述。

    

    文本摘要是一项重要的自然语言处理任务，应用范围包括信息检索和内容生成。利用大型语言模型在提升摘要技术方面展示了显著的潜力。本文使用多种大型语言模型（包括MPT-7b-instruct，falcon-7b-instruct和OpenAI ChatGPT text-davinci-003模型）进行文本摘要的探索。实验使用不同的超参数，并使用诸如双语评估衡量（BLEU）分数，面向回忆的视角评估（ROUGE）分数和双向编码器表示转换器（BERT）分数等广泛接受的指标评估生成的摘要。根据实验，text-davinci-003的性能优于其他模型。本次研究涉及CNN Daily Mail和XSum这两个不同的数据集，主要目标是全面了解大型语言模型在文本摘要中的性能。

    Text summarization is a critical Natural Language Processing (NLP) task with applications ranging from information retrieval to content generation. Leveraging Large Language Models (LLMs) has shown remarkable promise in enhancing summarization techniques. This paper embarks on an exploration of text summarization with a diverse set of LLMs, including MPT-7b-instruct, falcon-7b-instruct, and OpenAI ChatGPT text-davinci-003 models. The experiment was performed with different hyperparameters and evaluated the generated summaries using widely accepted metrics such as the Bilingual Evaluation Understudy (BLEU) Score, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score, and Bidirectional Encoder Representations from Transformers (BERT) Score. According to the experiment, text-davinci-003 outperformed the others. This investigation involved two distinct datasets: CNN Daily Mail and XSum. Its primary objective was to provide a comprehensive understanding of the performance of Large
    
[^78]: 跨语言多语言模型中事实知识的跨语言一致性

    Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10378](http://arxiv.org/abs/2310.10378)

    本论文研究了多语言预训练语言模型中事实知识的跨语言一致性，提出了一种新的度量方法，并通过分析模型大小、语言配对等因素发现了影响一致性的因素。实验结果表明，增加模型大小可以提高准确性，但不会改善跨语言一致性。

    

    多语言大规模预训练语言模型（PLM）显示存储了大量的事实知识，但在不同语言之间存在较大的变化。为了确保不同语言背景的用户从同一个模型中获得一致的反馈，我们研究了各种多语言PLM中事实知识的跨语言一致性（CLC）。为此，我们提出了一种基于排序的一致性（RankC）度量，用于独立于准确性评估跨语言间的知识一致性。利用这个度量方法，我们对决定CLC的因素进行了深入分析，包括模型层面和语言对层面。在其他结果中，我们发现增加模型大小可以提高大多数语言中的事实探测准确性，但不能改善跨语言一致性。最后，我们通过模型编辑在PLMs中插入新的事实关联进行了一个CLC的案例研究。对一小部分事实进行了实验。

    Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts 
    
[^79]: 提升对话式搜索：基于大型语言模型辅助的信息查询重写

    Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting. (arXiv:2310.09716v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2310.09716](http://arxiv.org/abs/2310.09716)

    该论文提出了一种利用大型语言模型(LLMs)作为查询重写器的方法，通过指令生成信息丰富的查询重写，以提升对话式搜索的检索性能。实验结果表明，这种方法在QReCC数据集上取得了良好的效果。

    

    查询重写在提升对话式搜索中起着重要作用，通过将上下文相关的用户查询转化为独立形式。现有方法主要利用人工重写的查询作为标签来训练查询重写模型。然而，人工重写可能缺乏足够的信息以实现最佳的检索性能。为了克服这个限制，我们提出利用大型语言模型(LLMs)作为查询重写器，通过精心设计的指令生成信息丰富的查询重写。我们定义了四个重要特性来定义规范的重写，并将其全部纳入指令中。此外，当初始查询重写可用时，我们引入了LLMs的重写编辑器的角色，形成一个“重写-编辑”过程。此外，我们提出将LLMs的重写能力提炼成较小的模型，以减少重写延迟。我们在QReCC数据集上进行的实验评估表明，信息丰富的查询重写可以提高搜索的效果。

    Query rewriting plays a vital role in enhancing conversational search by transforming context-dependent user queries into standalone forms. Existing approaches primarily leverage human-rewritten queries as labels to train query rewriting models. However, human rewrites may lack sufficient information for optimal retrieval performance. To overcome this limitation, we propose utilizing large language models (LLMs) as query rewriters, enabling the generation of informative query rewrites through well-designed instructions. We define four essential properties for well-formed rewrites and incorporate all of them into the instruction. In addition, we introduce the role of rewrite editors for LLMs when initial query rewrites are available, forming a "rewrite-then-edit" process. Furthermore, we propose distilling the rewriting capabilities of LLMs into smaller models to reduce rewriting latency. Our experimental evaluation on the QReCC dataset demonstrates that informative query rewrites can y
    
[^80]: 数学方程生成的表达树解码策略

    An Expression Tree Decoding Strategy for Mathematical Equation Generation. (arXiv:2310.09619v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.09619](http://arxiv.org/abs/2310.09619)

    本研究提出了一种表达树解码策略，将树结构整合到数学方程生成中，以解决当前顺序方法忽视并行和依赖关系的问题。

    

    从自然语言生成数学方程需要准确理解数学表达式之间的关系。现有方法可以大致分为标记级生成和表达式级生成两类。前者将方程视为数学语言，按顺序生成数学标记。表达式级方法逐个生成每个表达式。然而，每个表达式表示一个求解步骤，这些步骤之间自然存在并行或依赖关系，但当前的顺序方法忽视了这些关系。因此，我们将树结构整合到表达式级生成中，并提出了一种表达树解码策略。为了生成一个以表达式为节点的树，我们采用逐层并行解码策略：在每一层同时解码多个独立表达式（叶节点），并重复逐层进行并行解码，以顺序生成依赖于其他表达式的父节点表达式。

    Generating mathematical equations from natural language requires an accurate understanding of the relations among math expressions. Existing approaches can be broadly categorized into token-level and expression-level generation. The former treats equations as a mathematical language, sequentially generating math tokens. Expression-level methods generate each expression one by one. However, each expression represents a solving step, and there naturally exist parallel or dependent relations between these steps, which are ignored by current sequential methods. Therefore, we integrate tree structure into the expression-level generation and advocate an expression tree decoding strategy. To generate a tree with expression as its node, we employ a layer-wise parallel decoding strategy: we decode multiple independent expressions (leaf nodes) in parallel at each layer and repeat parallel decoding layer by layer to sequentially generate these parent node expressions that depend on others. Beside
    
[^81]: BanglaNLP在BLP-2023任务2中的表现: 对Bangla社交媒体帖子的情感分析的Transformer模型的基准测试

    BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models for Sentiment Analysis of Bangla Social Media Posts. (arXiv:2310.09238v1 [cs.CL])

    [http://arxiv.org/abs/2310.09238](http://arxiv.org/abs/2310.09238)

    本论文提出了针对Bangla社交媒体帖子进行情感分析的任务，通过实验发现在这种低资源语言场景下，使用Transformer模型进行迁移学习可以提高模型的学习效果，并且在对已经在Twitter数据上进行了情感分析任务的模型进一步微调的情况下，模型的性能最好。

    

    Bangla是全球第七大使用最广泛的语言，拥有来自印度和孟加拉国的2.34亿母语使用者。这种形态丰富的语言拥有丰富的文学传统，包括不同的方言和语言特定的挑战。尽管其语言丰富性和历史，但在自然语言处理（NLP）和语音社区中，Bangla仍被归类为资源匮乏的语言。本论文展示了我们在BLP研讨会的任务2（Bangla社交媒体帖子情感分析）中的提交。我们尝试了不同的基于Transformer的架构来解决这个任务。我们的定量结果显示，在这种低资源语言场景中，迁移学习确实有助于模型更好地学习。当我们进一步对已经在Twitter数据上进行了情感分析任务的模型进行微调时，这一点变得明显，而这个经过微调的模型在所有其他模型中表现最佳。我们还进行了详细的实验。

    Bangla is the 7th most widely spoken language globally, with a staggering 234 million native speakers primarily hailing from India and Bangladesh. This morphologically rich language boasts a rich literary tradition, encompassing diverse dialects and language-specific challenges. Despite its linguistic richness and history, Bangla remains categorized as a low-resource language within the natural language processing (NLP) and speech community. This paper presents our submission to Task 2 (Sentiment Analysis of Bangla Social Media Posts) of the BLP Workshop. We experiment with various Transformer-based architectures to solve this task. Our quantitative results show that transfer learning really helps in better learning of the models in this low-resource language scenario. This becomes evident when we further finetune a model which has already been finetuned on twitter data for sentiment analysis task and that finetuned model performs the best among all other models. We also perform a deta
    
[^82]: 开发一种自然语言理解模型来表征有线电视新闻偏见

    Developing a Natural Language Understanding Model to Characterize Cable News Bias. (arXiv:2310.09166v1 [cs.CL])

    [http://arxiv.org/abs/2310.09166](http://arxiv.org/abs/2310.09166)

    本论文开发了一种无监督的机器学习方法，通过对有线电视节目提及的主题进行命名实体识别和立场分析的方式，来表征其偏见。在2020年的有线电视转录中应用该方法，发现节目聚类与节目所属的有线电视网络保持一致。揭示了客观评估媒体偏见和表征陌生媒体环境的潜力。

    

    媒体偏见在社会科学和计算机科学领域得到了广泛研究。然而，目前的研究仍然在很大程度上依靠人工输入和主观评估来标记偏见，尤其是有线电视研究更是如此。为了解决这些问题，我们开发了一种无监督的机器学习方法，以在没有任何人工输入的情况下表征有线电视节目的偏见。该方法依赖于通过命名实体识别分析提及的主题以及通过立场分析来讨论这些主题的方式，以便将具有类似偏见的节目进行聚类。将我们的方法应用于2020年的有线电视转录中，我们发现节目聚类随时间保持一致，并大致对应于节目所属的有线电视网络。该方法揭示了未来客观评估媒体偏见和表征陌生媒体环境的潜力。

    Media bias has been extensively studied by both social and computational sciences. However, current work still has a large reliance on human input and subjective assessment to label biases. This is especially true for cable news research. To address these issues, we develop an unsupervised machine learning method to characterize the bias of cable news programs without any human input. This method relies on the analysis of what topics are mentioned through Named Entity Recognition and how those topics are discussed through Stance Analysis in order to cluster programs with similar biases together. Applying our method to 2020 cable news transcripts, we find that program clusters are consistent over time and roughly correspond to the cable news network of the program. This method reveals the potential for future tools to objectively assess media bias and characterize unfamiliar media environments.
    
[^83]: 探索大型语言模型的认知知识结构：一种教育诊断评估方法

    Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach. (arXiv:2310.08172v1 [cs.CL])

    [http://arxiv.org/abs/2310.08172](http://arxiv.org/abs/2310.08172)

    本研究通过教育诊断评估方法，揭示了大型语言模型（LLMs）的知识结构，强调了研究LLMs的认知能力和不同认知模式的重要性。

    

    大型语言模型（LLMs）不仅在各种任务中表现出了卓越的性能，还展示了智能的火花。最近的研究集中在评估它们在人类考试中的能力，并揭示了它们在不同领域的出色能力。然而，关于LLMs整体知识结构的认知研究仍然缺乏。本文基于教育诊断评估方法，在MoocRadar上进行评估，这是一个根据布鲁姆分类法进行细致注释的人类测试数据集。我们的目标是揭示LLMs的知识结构，并对它们的认知能力进行深入理解。本研究强调了调查LLMs的知识和理解其认知模式的重要性。通过照亮模型的知识，研究人员可以更加明确和有效地促进LLMs的开发和利用。

    Large Language Models (LLMs) have not only exhibited exceptional performance across various tasks, but also demonstrated sparks of intelligence. Recent studies have focused on assessing their capabilities on human exams and revealed their impressive competence in different domains. However, cognitive research on the overall knowledge structure of LLMs is still lacking. In this paper, based on educational diagnostic assessment method, we conduct an evaluation using MoocRadar, a meticulously annotated human test dataset based on Bloom Taxonomy. We aim to reveal the knowledge structures of LLMs and gain insights of their cognitive capabilities. This research emphasizes the significance of investigating LLMs' knowledge and understanding the disparate cognitive patterns of LLMs. By shedding light on models' knowledge, researchers can advance development and utilization of LLMs in a more informed and effective manner.
    
[^84]: 基于思维链的Transformer的表达能力

    The Expresssive Power of Transformers with Chain of Thought. (arXiv:2310.07923v1 [cs.LG])

    [http://arxiv.org/abs/2310.07923](http://arxiv.org/abs/2310.07923)

    本论文研究基于思维链的Transformer的表达能力，通过允许使用中间生成的方式提高了Transformer的推理能力，并发现线性数量的解码步骤在标准计算复杂度下增加了明显的新能力。

    

    最近的理论研究发现了一些出人意料地简单的推理问题，例如检查图中是否存在连接的两个节点，或模拟有限状态机，这些问题被证明无法由立即读取输入后回答的标准Transformer解决。然而，在实践中，通过允许Transformer使用“思维链”或“草稿纸”，即在回答之前生成并依赖一系列中间token，可以改善其推理能力。基于此，我们问：这种中间生成是否从根本上扩展了仅有解码器的Transformer的计算能力？我们表明答案是肯定的，但增加的程度关键取决于中间生成的数量。例如，我们发现相对于输入长度来说，具有对数级解码步骤的Transformer解码器仅略微推动了标准Transformer的极限，而线性数量的解码步骤则增加了明显的新能力（在标准计算复杂度下）。

    Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a "chain of thought" or "scratchpad", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps adds a clear new ability (under standard compl
    
[^85]: 大型语言模型中的事实性调查：知识、检索和领域专属性

    Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity. (arXiv:2310.07521v1 [cs.CL])

    [http://arxiv.org/abs/2310.07521](http://arxiv.org/abs/2310.07521)

    这项调查研究了大型语言模型（LLMs）中的事实性问题，包括其可能带来的后果和挑战，以及存储、处理和评估事实的方法。同时，提出了一些增强LLM事实性的策略，涵盖了不同领域的需求。

    

    本调查研究了大型语言模型（LLMs）中的关键问题，即事实性。由于LLMs在不同领域中都有应用，它们的输出的可靠性和准确性变得至关重要。我们将事实性问题定义为LLMs产生与已确立事实不一致的内容的概率。我们首先深入探讨了这些不准确性的含义，突出了事实错误在LLMs输出中可能带来的潜在后果和挑战。随后，我们分析了LLMs存储和处理事实的机制，寻找事实错误的主要原因。我们的讨论随后转向评估LLM事实性的方法论，强调关键指标、基准和研究。我们进一步探讨了增强LLM事实性的策略，包括针对特定领域的方法。我们重点关注两种主要的LLM配置，独立LLMs和利用外部数据的检索增强LLMs，详细介绍它们的独特挑战和潜在解决方法。

    This survey addresses the crucial issue of factuality in Large Language Models (LLMs). As LLMs find applications across diverse domains, the reliability and accuracy of their outputs become vital. We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts. We first delve into the implications of these inaccuracies, highlighting the potential consequences and challenges posed by factual errors in LLM outputs. Subsequently, we analyze the mechanisms through which LLMs store and process facts, seeking the primary causes of factual errors. Our discussion then transitions to methodologies for evaluating LLM factuality, emphasizing key metrics, benchmarks, and studies. We further explore strategies for enhancing LLM factuality, including approaches tailored for specific domains. We focus two primary LLM configurations standalone LLMs and Retrieval-Augmented LLMs that utilizes external data, we detail their unique challenges and potential 
    
[^86]: Cognate Transformer用于自动语音重建和同源反射预测

    Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction. (arXiv:2310.07487v1 [cs.CL])

    [http://arxiv.org/abs/2310.07487](http://arxiv.org/abs/2310.07487)

    Cognate Transformer是一个用于自动语音重建和同源反射预测的模型，基于多序列对齐数据进行训练，并在计算历史语言学领域取得了良好的表现。

    

    语音重建是历史语言学中的一个核心问题，通过观察子语言的同源词汇，确定祖先语言的原始词汇。计算历史语言学的方法试图通过学习现有语言数据上的模型来自动化这个任务。我们将蛋白质语言模型MSA Transformer应用于自动语音重建问题，并将其命名为Cognate Transformer。我们还将该模型应用于相关的任务，即同源反射预测，根据其他子语言的同源词汇来预测子语言中的反射词汇。我们展示了我们的模型的优越性能。

    Phonological reconstruction is one of the central problems in historical linguistics where a proto-word of an ancestral language is determined from the observed cognate words of daughter languages. Computational approaches to historical linguistics attempt to automate the task by learning models on available linguistic data. Several ideas and techniques drawn from computational biology have been successfully applied in the area of computational historical linguistics. Following these lines, we adapt MSA Transformer, a protein language model, to the problem of automated phonological reconstruction. MSA Transformer trains on multiple sequence alignments as input and is, thus, apt for application on aligned cognate words. We, hence, name our model as Cognate Transformer. We also apply the model on another associated task, namely, cognate reflex prediction, where a reflex word in a daughter language is predicted based on cognate words from other daughter languages. We show that our model o
    
[^87]: 在线推测解码

    Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])

    [http://arxiv.org/abs/2310.07177](http://arxiv.org/abs/2310.07177)

    在线推测解码是通过利用多余计算能力，在LLM服务集群中持续更新草稿模型，从而加速大型语言模型推理的一种方法。

    

    推测解码是通过利用较小的草稿模型来预测目标模型的输出，从而加速大型语言模型（LLM）推理的关键技术。然而，在面对多样的文本输入和草稿模型与目标模型之间的显著能力差距时，其有效性可能受到限制。我们引入了在线推测解码（OSD）来解决这一挑战。其主要思想是利用LLM服务集群中丰富的多余计算能力，根据观察到的用户查询数据持续更新（多个）草稿模型。由于LLM推理受内存限制，典型的LLM服务集群中的剩余计算能力可以用于在线重新训练草稿模型，从而使训练成本保持中性。由于LLM服务的查询分布相对简单，根据查询分布进行重新训练可以使草稿模型更准确地预测目标模型的输出。

    Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
    
[^88]: 使用问题-答案计划的视觉叙事

    Visual Storytelling with Question-Answer Plans. (arXiv:2310.05295v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05295](http://arxiv.org/abs/2310.05295)

    本论文引入了一种新颖的框架，将视觉表示与预训练语言模型和计划相结合，从图像序列中生成更具连贯性、趣味性和自然性的叙事。

    

    视觉叙事旨在从图像序列中生成引人入胜的叙事。现有模型通常着重于改进图像序列的表征，例如通过外部知识源或先进的图结构。尽管最近取得了一些进展，但故事往往重复、不合逻辑且缺乏细节。为了缓解这些问题，我们提出了一个新颖的框架，将视觉表达与预训练语言模型和计划相结合。我们的模型将图像序列转化为视觉前缀，即连续嵌入的序列，可以由语言模型解释。它还利用一系列问题-答案对作为蓝图计划，选择显著的视觉概念并确定它们如何组合成叙事。在VIST基准测试上进行的自动和人工评估表明，基于蓝图的模型生成的故事与竞争基准和现有模型相比更连贯，更有趣，更自然。

    Visual storytelling aims to generate compelling narratives from image sequences. Existing models often focus on enhancing the representation of the image sequence, e.g., with external knowledge sources or advanced graph structures. Despite recent progress, the stories are often repetitive, illogical, and lacking in detail. To mitigate these issues, we present a novel framework which integrates visual representations with pretrained language models and planning. Our model translates the image sequence into a visual prefix, a sequence of continuous embeddings which language models can interpret. It also leverages a sequence of question-answer pairs as a blueprint plan for selecting salient visual concepts and determining how they should be assembled into a narrative. Automatic and human evaluation on the VIST benchmark (Huang et al., 2016) demonstrates that blueprint-based models generate stories that are more coherent, interesting, and natural compared to competitive baselines and state
    
[^89]: Crystal: 以自我反馈为增强的内省推理器

    Crystal: Introspective Reasoners Reinforced with Self-Feedback. (arXiv:2310.04921v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.04921](http://arxiv.org/abs/2310.04921)

    提出了一种名为Crystal的内省型常识推理器，通过内省知识和基于知识的推理相结合，提高了常识推理的性能和解释能力。

    

    大量工作表明，通过知识增强的推理方法可以提高常识推理的性能和可解释性，其中推理过程的基础知识明确表达和利用。然而，现有的实现，包括"思维链"及其变种，未能捕捉到常识推理中所需的内省性质，也未能解释知识生成和利用之间的相互适应。我们提出了一种新颖的方法来开发内省型常识推理器 Crystal。为了解决常识问题，它首先内省与给定问题相关的知识陈述，然后基于先前内省的知识进行知情预测。模型的知识内省和基于知识的推理模式通过强化学习进行调整，其中奖励来自反馈。

    Extensive work has shown that the performance and interpretability of commonsense reasoning can be improved via knowledge-augmented reasoning methods, where the knowledge that underpins the reasoning process is explicitly verbalized and utilized. However, existing implementations, including "chain-of-thought" and its variants, fall short in capturing the introspective nature of knowledge required in commonsense reasoning, and in accounting for the mutual adaptation between the generation and utilization of knowledge. We propose a novel method to develop an introspective commonsense reasoner, Crystal. To tackle commonsense problems, it first introspects for knowledge statements related to the given question, and subsequently makes an informed prediction that is grounded in the previously introspected knowledge. The knowledge introspection and knowledge-grounded reasoning modes of the model are tuned via reinforcement learning to mutually adapt, where the reward derives from the feedback
    
[^90]: 大型语言模型中的上下文学习: 对表示的神经科学启发式分析

    In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations. (arXiv:2310.00313v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00313](http://arxiv.org/abs/2310.00313)

    本研究通过神经科学启发的技术，研究了大型语言模型中上下文学习的机制，通过调查嵌入和注意力的变化，揭示了这种改进背后的潜在变化，并提出了用于参数化探测和注意力比率分析的新方法。

    

    通过利用输入中的特定任务示例，大型语言模型（LLMs）通过上下文学习（ICL）展现了卓越的性能提升。然而，这种改进背后的机制仍然难以理解。本研究中，我们调查了Llama-2 70B和Vicuna 13B中的嵌入和注意力表示。具体而言，我们研究了上下文学习后嵌入和注意力的变化以及这些变化如何调解行为的改进。我们采用了受神经科学启发的技术，如表示相似性分析（RSA），并提出了参数化探测和注意力比率分析（ARA，衡量关注相关与无关信息的比率）的新方法。我们设计了三个具有条件之间先验关系的任务：阅读理解，线性回归和对抗提示注入。我们提出了关于任务表示中预期相似性的假设，以研究嵌入和注意力中的潜在变化。

    Large language models (LLMs) exhibit remarkable performance improvement through in-context learning (ICL) by leveraging task-specific examples in the input. However, the mechanisms behind this improvement remain elusive. In this work, we investigate embeddings and attention representations in Llama-2 70B and Vicuna 13B. Specifically, we study how embeddings and attention change after in-context-learning, and how these changes mediate improvement in behavior. We employ neuroscience-inspired techniques, such as representational similarity analysis (RSA), and propose novel methods for parameterized probing and attention ratio analysis (ARA, measuring the ratio of attention to relevant vs. irrelevant information). We designed three tasks with a priori relationships among their conditions: reading comprehension, linear regression, and adversarial prompt injection. We formed hypotheses about expected similarities in task representations to investigate latent changes in embeddings and attenti
    
[^91]: 让PPO变得更好：基于值导向的Monte-Carlo Tree Search解码

    Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding. (arXiv:2309.15028v1 [cs.CL])

    [http://arxiv.org/abs/2309.15028](http://arxiv.org/abs/2309.15028)

    本文提出了一种基于值导向的Monte-Carlo Tree Search解码算法PPO-MCTS，通过在PPO之上集成MCTS，解决了训练和测试之间部分输出评分机制的不匹配问题，实验证明该算法可以显著提升性能。

    

    在生成自然语言文本时，使用最新的强化学习算法，如Proximal Policy Optimization (PPO)，因此可以认为推理时间的搜索算法，如Monte-Carlo Tree Search (MCTS) 是不必要的。本文证明了通过在PPO之上集成MCTS，可以进一步提升PPO的性能。关键思想是在解码文本时，不要丢弃值网络，即PPO训练时用于评估部分输出序列的副产品，而是将其与策略网络紧密结合。具体而言，本文提出了一种称为PPO-MCTS的新颖的值导向解码算法，可以将来自PPO的值网络与推理时间产生的策略网络紧密结合。与基于MCTS的控制文本生成的先前方法相比，我们的方法的关键优势在于减少了训练和测试之间部分输出的评分机制的基本不匹配。在四个文本生成任务上的评估结果表明，PPO-MCTS可以显著提升性能。

    Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO). In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top. The key idea is not to throw out the value network, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the policy network. More concretely, we present a novel value-guided decoding algorithm called PPO-MCTS, which can integrate the value network from PPO to work closely with the policy network during inference-time generation. Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test. Evaluation on four text generation tasks demonstrate that PPO-MCTS grea
    
[^92]: 基于生成AI的医疗对话效果的量化度量

    Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI. (arXiv:2309.12444v1 [cs.CL])

    [http://arxiv.org/abs/2309.12444](http://arxiv.org/abs/2309.12444)

    这篇论文提出了基于生成AI的医疗对话模型的评估指标问题，并强调了现有指标对医学和健康概念的理解不足以及忽略了用户体验因素。

    

    生成人工智能将通过将传统的患者护理转变为更个性化、高效和积极的过程，彻底改变医疗保健交付方式。聊天机器人作为互动对话模型，很可能推动医疗保健的以患者为中心的转型。通过提供诊断、个性化生活方式建议和心理健康支持等各种服务，目标是大幅度提高患者的健康结果，同时减轻医疗保健提供者的工作负担。医疗应用的生命关键性要求建立统一全面的对话模型评估指标。已有的针对各种通用大型语言模型(LLMs)提出的评估指标在理解医学和健康概念及其在促进患者福祉方面的重要性方面存在不足。此外，这些指标忽略了关键的用户体验因素。

    Generative Artificial Intelligence is set to revolutionize healthcare delivery by transforming traditional patient care into a more personalized, efficient, and proactive process. Chatbots, serving as interactive conversational models, will probably drive this patient-centered transformation in healthcare. Through the provision of various services, including diagnosis, personalized lifestyle recommendations, and mental health support, the objective is to substantially augment patient health outcomes, all the while mitigating the workload burden on healthcare providers. The life-critical nature of healthcare applications necessitates establishing a unified and comprehensive set of evaluation metrics for conversational models. Existing evaluation metrics proposed for various generic large language models (LLMs) demonstrate a lack of comprehension regarding medical and health concepts and their significance in promoting patients' well-being. Moreover, these metrics neglect pivotal user-ce
    
[^93]: 大型语言模型在提取分子相互作用和通路知识方面的比较性能评估

    Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge. (arXiv:2307.08813v1 [cs.CL])

    [http://arxiv.org/abs/2307.08813](http://arxiv.org/abs/2307.08813)

    本研究评估了不同大型语言模型在提取分子相互作用和通路知识方面的有效性，并讨论了未来机遇和挑战。

    

    理解蛋白质相互作用和通路知识对于揭示生物系统的复杂性和研究生物功能和复杂疾病的基本机制至关重要。尽管现有的数据库提供了来自文献和其他源的策划生物数据，但它们往往不完整且维护工作繁重，因此需要替代方法。在本研究中，我们提出利用大型语言模型的能力，通过自动从相关科学文献中提取这些知识来解决这些问题。为了实现这个目标，在这项工作中，我们调查了不同大型语言模型在识别蛋白质相互作用、通路和基因调控关系等任务中的有效性。我们对不同模型的性能进行了彻底评估，突出了重要的发现，并讨论了这种方法所面临的未来机遇和挑战。代码和数据集链接可在论文中找到。

    Understanding protein interactions and pathway knowledge is crucial for unraveling the complexities of living systems and investigating the underlying mechanisms of biological functions and complex diseases. While existing databases provide curated biological data from literature and other sources, they are often incomplete and their maintenance is labor-intensive, necessitating alternative approaches. In this study, we propose to harness the capabilities of large language models to address these issues by automatically extracting such knowledge from the relevant scientific literature. Toward this goal, in this work, we investigate the effectiveness of different large language models in tasks that involve recognizing protein interactions, pathways, and gene regulatory relations. We thoroughly evaluate the performance of various models, highlight the significant findings, and discuss both the future opportunities and the remaining challenges associated with this approach. The code and d
    
[^94]: 回放以回忆：针对德语语音识别的持续层特定微调

    Replay to Remember: Continual Layer-Specific Fine-tuning for German Speech Recognition. (arXiv:2307.07280v1 [cs.CL])

    [http://arxiv.org/abs/2307.07280](http://arxiv.org/abs/2307.07280)

    该论文探讨了如何通过持续层特定微调和经验回放技术来改善德语语音识别模型在较小领域中的性能，并提高模型的鲁棒性。

    

    虽然自动语音识别（ASR）模型在引入无监督或自监督训练技术方面取得了显著进展，但这些改进仍然仅限于某些语言和说话者。迁移学习使得大规模多语言模型能够适应不仅是低资源语言，还包括更特定的说话者群体。然而，对新领域的数据进行微调通常会导致在原始领域的性能下降。因此，在我们的实验中，我们研究了大规模ASR模型在较小领域中的性能可以有多好，使用我们自己的德语高级语音命令数据集（SVC-de），以及在训练过程中通过选择性地冻结模型的部分来保留多少通用语音识别性能。为了进一步增加ASR模型对微调领域之外的词汇和说话者的鲁棒性，我们应用经验回放进行连续训练。

    While Automatic Speech Recognition (ASR) models have shown significant advances with the introduction of unsupervised or self-supervised training techniques, these improvements are still only limited to a subsection of languages and speakers. Transfer learning enables the adaptation of large-scale multilingual models to not only low-resource languages but also to more specific speaker groups. However, fine-tuning on data from new domains is usually accompanied by a decrease in performance on the original domain. Therefore, in our experiments, we examine how well the performance of large-scale ASR models can be approximated for smaller domains, with our own dataset of German Senior Voice Commands (SVC-de), and how much of the general speech recognition performance can be preserved by selectively freezing parts of the model during training. To further increase the robustness of the ASR model to vocabulary and speakers outside of the fine-tuned domain, we apply Experience Replay for conti
    
[^95]: 大型语言模型作为属性化训练数据生成器：多样性和偏差的故事

    Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias. (arXiv:2306.15895v1 [cs.CL])

    [http://arxiv.org/abs/2306.15895](http://arxiv.org/abs/2306.15895)

    本论文研究了大型语言模型作为属性化训练数据生成器的应用。通过使用具有多样性属性的提示，我们能够生成多样化且归因的数据。研究表明，在高基数和多样领域的数据集中，使用属性化提示对生成模型性能有积极影响。此外，论文还展示了关于偏差、多样性和效率的全面实证研究结果，并得出了三个关键观察：系统性偏差存在于生成数据中，多样性和效率之间存在权衡，属性化训练数据生成可以改善模型性能。

    

    近期大型语言模型(LLMs)被广泛应用于各种自然语言处理(NLP)任务的训练数据生成。尽管之前的研究探索了使用生成数据进行模型训练的不同方法，但它们通常依赖于简单的类别条件提示，这可能限制了生成数据的多样性，并且继承了LLM的系统性偏差。因此，我们研究了使用具有多样属性的提示(例如指定长度和风格等属性)进行训练数据生成，这有潜力产生多样和归因的生成数据。我们的研究关注具有高基数和多样领域的数据集，在这方面，我们证明了属性化提示在生成模型性能方面优于简单的类别条件提示。此外，我们还展示了一项包括偏差、多样性和效率等关键方面的全面实证研究，并强调了三个关键观察：首先，系统性偏差在生成数据中存在；其次，多样性和效率之间存在权衡；最后，进行属性化训练数据生成可以改善模型性能。

    Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, sy
    
[^96]: 超越一个模型适用于所有领域：大型语言模型的领域专门化综述

    Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models. (arXiv:2305.18703v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.18703](http://arxiv.org/abs/2305.18703)

    本文综述了大型语言模型的领域专门化，包括动机、挑战、方法论和评估指标。此外，还提供了一个特定领域任务和数据集的分类法，对现有的领域自适应和定制技术进行了详细比较，并广泛讨论了这一领域中的未解决问题和未来的发展方向。

    

    大型语言模型（LLM）已经大大推动了自然语言处理（NLP）领域的发展，为广泛应用提供了高度实用、任务无关的基础。LLMs 作为通用任务求解器的巨大潜力，促使人们将其用于特定领域，如医疗保健、金融和教育，并将其用作助手甚至替代特定领域的专家和工具。但是，将LLMs直接应用于特定领域中的复杂问题会遇到许多困难，包括领域数据的异质性、领域知识的复杂性、领域目标的独特性以及约束的多样性。为了填补这种差距，最近几年进行了急剧增加的研究和实践致力于大型语言模型的领域专门化，然而这方面的研究尚未被系统地总结。在这篇综述中，我们对LLMs的领域专门化进行了全面概述，包括动机、挑战、方法论和评估指标。此外，我们提供了一个特定领域任务和数据集的分类法，对现有的领域自适应和定制技术进行了详细比较，并广泛讨论了这一领域中的未解决问题和未来的发展方向。

    Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. The great promise of LLMs as general task solvers motivated people to extend their functionality largely beyond just a ``chatbot'', and use it as an assistant or even replacement for domain experts and tools in specific domains such as healthcare, finance, and education. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). To fill such a gap, explosively-increase research, and practices have been conducted in very recent years on the domain specialization of LLMs, which, howe
    
[^97]: 差异性遮挡：选择在继续预训练中遮挡什么

    Difference-Masking: Choosing What to Mask in Continued Pretraining. (arXiv:2305.14577v1 [cs.LG])

    [http://arxiv.org/abs/2305.14577](http://arxiv.org/abs/2305.14577)

    本文提出了一种自动选取遮蔽数据的方法（Difference-Masking），以提高在继续预训练中的自监督学习模型的性能，方法是通过考虑未标记的目标域与预训练域的不同之处来进行。实验证明，该方法可以有效地提升继续预训练任务的性能，且具有跨任务的适用性。

    

    自监督学习(SSL)，特别是遮挡预测目标的目标，已经在各种下游任务中证明了很好的性能，然而，大多数方法都是随机地进行标记和遮挡，而在教育领域有强烈的直觉认为，决定什么需要遮挡可以实质性地改善学习结果。我们引入了差异遮挡(Difference-Masking)，一种自动选择遮挡什么的方法，在继续预训练中通过考虑未标记的目标域与预训练域的不同之处来实现。实证上，我们发现差异遮挡在四个不同的语言和多模态视频任务的继续预训练设置中优于基线。差异性遮挡的跨任务适用性支持我们的框架在语言、视觉和其他领域的SSL预训练中的有效性。

    Self-supervised learning (SSL) and the objective of masking-and-predicting in particular have led to promising SSL performance on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition from the field of education that deciding what to mask can substantially improve learning outcomes. We introduce Difference-Masking, an approach that automatically chooses what to mask during continued pretraining by considering what makes an unlabelled target domain different from the pretraining domain. Empirically, we find that Difference-Masking outperforms baselines on continued pretraining settings across four diverse language and multimodal video tasks. The cross-task applicability of Difference-Masking supports the effectiveness of our framework for SSL pretraining in language, vision, and other domains.
    
[^98]: 将问题回答作为解决时效问题的编程方法

    Question Answering as Programming for Solving Time-Sensitive Questions. (arXiv:2305.14221v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14221](http://arxiv.org/abs/2305.14221)

    该论文提出了一种将问题回答任务重新构架为编程任务的方法，通过利用现代语言模型的能力，设计能够解决时效性问题的程序。

    

    问题回答在人类日常生活中起着至关重要的作用，因为它涉及我们对世界知识的获得。然而，由于现实世界事实的动态和不断变化的特性，当问题的时间限制发生变化时，答案可能完全不同。最近，大型语言模型（LLM）显示出在问题回答方面的显著智能，而我们的实验证明上述问题仍然对现有LLM构成了一个重大挑战。这可以归因于LLM无法基于表面级文本语义进行严格的推理能力。为了克服这个局限性，我们提出了一种新颖的方法，不是要求LLM直接回答问题，而是将“问题回答任务”重新构架为“编程任务”（QAaP）。具体来说，通过利用现代LLM在理解自然语言和编程语言方面的卓越能力，我们努力 harness LLM models to craft programs that can solve time-sensitive questions.

    Question answering plays a pivotal role in human daily life because it involves our acquisition of knowledge about the world. However, due to the dynamic and ever-changing nature of real-world facts, the answer can be completely different when the time constraint in the question changes. Recently, Large Language Models (LLMs) have shown remarkable intelligence in question answering, while our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs' inability to perform rigorous reasoning based on surface-level text semantics. To overcome this limitation, rather than requiring LLMs to directly answer the question, we propose a novel approach where we reframe the $\textbf{Q}$uestion $\textbf{A}$nswering task $\textbf{a}$s $\textbf{P}$rogramming ($\textbf{QAaP}$). Concretely, by leveraging modern LLMs' superior capability in understanding both natural language and programming language, we endeavor to harne
    
[^99]: 单语言数据何时有助于多语言翻译：领域和模型规模的作用

    When Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale. (arXiv:2305.14124v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14124](http://arxiv.org/abs/2305.14124)

    本文研究了单语言数据在多语言翻译中的作用，发现单语言数据通常有助于多语言翻译，但模型对领域不匹配的容忍性较差，尤其在较小的模型规模下。回译在数据源相似的情况下是有益的，但在其他情况下可能是有害的，而去噪自编码的效果不如先前报告的好。规模对两种方法都很重要。

    

    多语言机器翻译（MMT）是通过混合平行和单语言数据进行训练，提高低资源语言对翻译的关键。然而，文献对于包含单语言数据的不同方法的表现存在争议。为了解决这个问题，我们研究了去噪自编码（DAE）和回译（BT）在不同数据条件和模型规模下对MMT的影响。与先前的研究不同，我们使用了一个实际数据集，包括100个翻译方向，并考虑了许多单语言和测试数据的领域组合。我们发现，单语言数据通常有助于MMT，但模型对领域不匹配的容忍性出乎意料地较差，尤其在较小的模型规模下。当平行、单语言和测试数据源相似时，回译是有益的，但在其他情况下可能是有害的，而DAE的效果不如先前报告的好。接下来，我们分析了规模（从90M到1.6B参数）的影响，发现它对两种方法都很重要。

    Multilingual machine translation (MMT), trained on a mixture of parallel and monolingual data, is key for improving translation in low-resource language pairs. However, the literature offers conflicting results on the performance of different methods of including monolingual data. To resolve this, we examine how denoising autoencoding (DAE) and backtranslation (BT) impact MMT under different data conditions and model scales. Unlike prior studies, we use a realistic dataset of 100 translation directions and consider many domain combinations of monolingual and test data. We find that monolingual data generally helps MMT, but models are surprisingly brittle to domain mismatches, especially at smaller model scales. BT is beneficial when the parallel, monolingual, and test data sources are similar but can be detrimental otherwise, while DAE is less effective than previously reported. Next, we analyze the impact of scale (from 90M to 1.6B parameters) and find it is important for both methods
    
[^100]: MADNet: 多方对话生成中最大化地址推断期望值

    MADNet: Maximizing Addressee Deduction Expectation for Multi-Party Conversation Generation. (arXiv:2305.12733v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12733](http://arxiv.org/abs/2305.12733)

    MADNet提出了一种在多方对话生成中最大化地址推断期望值的方法，通过设计额外的潜在边来确保对话片段之间的消息传递，以解决地址标签稀缺的问题。

    

    使用图神经网络对多方对话进行建模已被证明能够有效地捕捉复杂和图形化的信息流动。然而，现有方法过多地依赖于必要的地址标签，并且只能应用于每个话语都必须标记有地址标签的理想情况。为了研究在多方对话中常见的地址标签稀缺问题，我们提出了MADNet，该方法在异构图神经网络中最大化地址推断期望值用于多方对话生成。在缺少少量地址标签的多方对话中，现有方法无法构建一个连贯的对话图，而只能形成几个独立的对话碎片。为了确保这些对话碎片之间的消息传递，我们设计了四种额外类型的潜在边以完成一个完全连接的图。此外，为了对那些没有地址标签的话语进行边类型相关的消息传递的优化，我们采用了期望最大化算法。

    Modeling multi-party conversations (MPCs) with graph neural networks has been proven effective at capturing complicated and graphical information flows. However, existing methods rely heavily on the necessary addressee labels and can only be applied to an ideal setting where each utterance must be tagged with an addressee label. To study the scarcity of addressee labels which is a common issue in MPCs, we propose MADNet that maximizes addressee deduction expectation in heterogeneous graph neural networks for MPC generation. Given an MPC with a few addressee labels missing, existing methods fail to build a consecutively connected conversation graph, but only a few separate conversation fragments instead. To ensure message passing between these conversation fragments, four additional types of latent edges are designed to complete a fully-connected graph. Besides, to optimize the edge-type-dependent message passing for those utterances without addressee labels, an Expectation-Maximization
    
[^101]: 医学文本的多语言简化

    Multilingual Simplification of Medical Texts. (arXiv:2305.12532v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12532](http://arxiv.org/abs/2305.12532)

    本研究介绍了MultiCochrane，这是医学领域中第一个句子对齐的多语言文本简化数据集，通过多语言简化直接将复杂文本简化为多种语言的简化文本。

    

    自动化文本简化旨在产生复杂文本的简化版本。在医学领域，这项任务尤为重要，因为最新的医学发现通常通过复杂和技术性的文章进行传播。这为寻求最新医学发现信息的普通人造成了障碍，进而阻碍了健康素养的提高。大部分医学文本简化的现有工作都集中在单语言环境中，导致这些证据只能用一种语言（通常是英语）提供。本研究通过多语言简化直接将复杂文本简化为多种语言的简化文本来解决这个问题。我们介绍了MultiCochrane，这是医学领域中第一个四种语言（英语、西班牙语、法语和波斯语）的句子对齐多语言文本简化数据集。我们通过广泛的人工评估和分析，在这些语言之间评估了模型的微调和零样本模型。

    Automated text simplification aims to produce simple versions of complex texts. This task is especially useful in the medical domain, where the latest medical findings are typically communicated via complex and technical articles. This creates barriers for laypeople seeking access to up-to-date medical findings, consequently impeding progress on health literacy. Most existing work on medical text simplification has focused on monolingual settings, with the result that such evidence would be available only in just one language (most often, English). This work addresses this limitation via multilingual simplification, i.e., directly simplifying complex texts into simplified texts in multiple languages. We introduce MultiCochrane, the first sentence-aligned multilingual text simplification dataset for the medical domain in four languages: English, Spanish, French, and Farsi. We evaluate fine-tuned and zero-shot models across these languages, with extensive human assessments and analyses. 
    
[^102]: Prompt ChatGPT 在 MNER 中的应用：基于 ChatGPT 辅助精炼知识的改进式多模态命名实体识别方法

    Prompt ChatGPT In MNER: Improved multimodal named entity recognition method based on auxiliary refining knowledge from ChatGPT. (arXiv:2305.12212v1 [cs.CL])

    [http://arxiv.org/abs/2305.12212](http://arxiv.org/abs/2305.12212)

    本文提出了一个叫做 PGIM 的方法，该方法利用 ChatGPT 作为隐式知识引擎来获取辅助精炼知识，从而增强模型在 MNER 任务中的性能。

    

    社交媒体上的多模态命名实体识别（MNER）旨在通过引入基于图像的线索来增强文本实体预测。现有研究主要集中在最大化图像相关信息的利用或将外部知识从显式知识库（KBs）中引入。然而，这些方法要么忽视了向模型提供相关外部知识的必要性，要么引入的外部知识存在重复性。为了解决这些问题，本文提出了一个概念简单的两阶段框架，称为 Prompt ChatGPT In MNER (PGIM)。我们利用 ChatGPT 作为隐式知识引擎来获取辅助精炼知识，从而增强模型在 MNER 任务中的性能。

    Multimodal Named Entity Recognition (MNER) on social media aims to enhance textual entity prediction by incorporating image-based clues. Existing research in this domain has primarily focused on maximizing the utilization of potentially relevant information in images or incorporating external knowledge from explicit knowledge bases (KBs). However, these methods either neglect the necessity of providing the model with relevant external knowledge, or the retrieved external knowledge suffers from high redundancy. To address these problems, we propose a conceptually simple two-stage framework called Prompt ChatGPT In MNER (PGIM) in this paper. We leverage ChatGPT as an implicit knowledge engine to acquire auxiliary refined knowledge, thereby bolstering the model's performance in MNER tasks. Specifically, we first utilize a Multimodal Similar Example Awareness module to select suitable examples from a small number of manually annotated samples. These examples are then integrated into a form
    
[^103]: 学习适当地组合句法和语义表示以进行组合泛化

    Learn to Compose Syntactic and Semantic Representations Appropriately for Compositional Generalization. (arXiv:2305.12169v1 [cs.CL])

    [http://arxiv.org/abs/2305.12169](http://arxiv.org/abs/2305.12169)

    该研究提出了一个名为COMPSITION的新框架，可以通过适当地组合句法和语义表示来解决组合泛化问题。实验证明该方法在合成和自然语言CG任务上实现了最先进的性能。

    

    最近的研究表明，序列到序列（Seq2Seq）模型在解决组合泛化（CG）任务时存在局限性，无法系统性地推广到看不见的已知组件组合。越来越多的证据表明，阻碍CG的原因之一是编码器最上层的表示是纠缠的，也就是说，序列的句法和语义表示被不适当地扭曲了。然而，大多数以前的研究主要集中于在标记级别上增强语义信息，而不是适当地组合序列的句法和语义表示，就像人类所做的那样。此外，我们认为他们发现的表示纠缠问题不全面，并进一步假设传递到不同解码器层的源键值表示也是纠缠在一起的。基于这个直觉和受人类CG策略的启发，我们提出了COMPSITION（适当地组合句法和语义表示以进行组合泛化），这是一个解决CG任务的新框架。COMPSITION通过分别建模句法和语义表示，并通过几何表示模块将它们组合起来，显式地组合编码器的最上层。实验结果表明，COMPSITION在合成和自然语言CG任务上均实现了最先进的性能。

    Recent studies have shown that sequence-to-sequence (Seq2Seq) models are limited in solving the compositional generalization (CG) tasks, failing to systematically generalize to unseen compositions of seen components. There is mounting evidence that one of the reasons hindering CG is the representation of the encoder uppermost layer is entangled. In other words, the syntactic and semantic representations of sequences are twisted inappropriately. However, most previous studies mainly concentrate on enhancing semantic information at token-level, rather than composing the syntactic and semantic representations of sequences appropriately as humans do. In addition, we consider the representation entanglement problem they found is not comprehensive, and further hypothesize that source keys and values representations passing into different decoder layers are also entangled. Staring from this intuition and inspired by humans' strategies for CG, we propose COMPSITION (Compose Syntactic and Seman
    
[^104]: 重访文本熵率恒定

    Revisiting Entropy Rate Constancy in Text. (arXiv:2305.12084v1 [cs.CL])

    [http://arxiv.org/abs/2305.12084](http://arxiv.org/abs/2305.12084)

    本论文使用神经语言模型重新评估了基于n-gram语言模型下英文文本的概率提出的熵率恒定原理，未能找到明显的支持熵率恒定的证据。

    

    统一信息密度（UID）假说表明，人类倾向于在话语或话语中大致均匀分布信息。支持UID假说的早期证据来自Genzel＆Charniak（2002），他们基于n-gram语言模型下英文文本的概率提出了熵率恒定原理。本文使用神经语言模型重新评估Genzel＆Charniak（2002）的说法，未能找到明显的支持熵率恒定的证据。我们在数据集、模型大小和语言等方面进行了一系列实验，并讨论了统一信息密度假说和更广泛的有效传播语言理论的含义。

    The uniform information density (UID) hypothesis states that humans tend to distribute information roughly evenly across an utterance or discourse. Early evidence in support of the UID hypothesis came from Genzel & Charniak (2002), which proposed an entropy rate constancy principle based on the probability of English text under n-gram language models. We re-evaluate the claims of Genzel & Charniak (2002) with neural language models, failing to find clear evidence in support of entropy rate constancy. We conduct a range of experiments across datasets, model sizes, and languages and discuss implications for the uniform information density hypothesis and linguistic theories of efficient communication more broadly.
    
[^105]: LLM在医学系统综述中的潜在用途和风险评估

    Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews. (arXiv:2305.11828v1 [cs.CL])

    [http://arxiv.org/abs/2305.11828](http://arxiv.org/abs/2305.11828)

    本文研究了使用LLM协助制作医学证据综述的潜在用途和风险，指出LLM有可能自动生成文献综述，但由于可能出现虚构或遗漏信息的情况，LLM的使用需要谨慎。

    

    医学系统综述对于制定临床决策和医疗政策至关重要。但是制作这样的综述很费力且耗时。因此，很多问题缺乏高质量的证据综述，即使这些综述可用，在审查过程中可能已经过时。现在，大型语言模型（LLM）已经能够生成长篇文本，这意味着自动生成文献综述的诱人可能性。然而，由于虚构或遗漏重要信息，LLM有时会产生不准确（甚至可能具有误导性）的文本。在医疗保健环境中，这可能使LLM在最好情况下无法使用，在最坏情况下会带来危险。对于LLM的益处和风险的大多数讨论与具体应用脱离了关系。在这项工作中，我们试图定性描述LLM在协助制作医学证据综述方面的潜在用途和风险。我们对16位国际专家进行了半结构化访谈。

    Medical systematic reviews are crucial for informing clinical decision making and healthcare policy. But producing such reviews is onerous and time-consuming. Thus, high-quality evidence synopses are not available for many questions and may be outdated even when they are available. Large language models (LLMs) are now capable of generating long-form texts, suggesting the tantalizing possibility of automatically generating literature reviews on demand. However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucinating or omitting important information. In the healthcare context, this may render LLMs unusable at best and dangerous at worst. Most discussion surrounding the benefits and risks of LLMs have been divorced from specific applications. In this work, we seek to qualitatively characterize the potential utility and risks of LLMs for assisting in production of medical evidence reviews. We conducted 16 semi-structured interviews with international experts
    
[^106]: 大型语言模型中的内部一致性问题研究：通过辩论进行深入分析

    Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate. (arXiv:2305.11595v1 [cs.CL])

    [http://arxiv.org/abs/2305.11595](http://arxiv.org/abs/2305.11595)

    本文提出了通过辩论探究大型语言模型之间的内部一致性问题，实验证明通过严格的辩论框架可以提高模型性能和常识知识的结构化学习。

    

    大型语言模型LLMs在各种自然语言处理NLP任务中展现出了惊人的零样本或少量样本通识推理性能。然而，尽管它们拥有强大的常识推理能力，但它们仍然存在各种不一致问题。本研究提出探索两个或多个LLMs之间的内部一致性问题，这对于不同和精确的决策过程至关重要。通过严格的辩论框架，在7个常识推理数据集上进行了广泛的实验。LLMs不仅通过妥协和反驳变得更具内部一致性，而且还实现了更高的性能和常识知识的结构化学习。

    Large language models (LLMs) have demonstrated impressive zero-shot or few-shot commonsense reasoning performance on various natural language processing (NLP) tasks. However, despite their strong commonsense reasoning abilities, LLMs still exhibit various kinds of inconsistency problems. While previous researches mainly focused on the self-consistency within a single LLM, we propose to explore the inter-consistency issue between two or more LLMs, which is critical for diverse and precise decision-making processes. Since the LLMs possess human-like intelligence after instruction tuning and reinforcement learning with human feedback (RLHF), we design a formal debate framework to delve into the inter-consistency problem among LLMs with three-stage debate: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on 7 commonsense reasoning datasets, LLMs not only become more inter-consistent by compromising and refuting but also achieve higher performance and str
    
[^107]: “即插即用”医疗对话系统

    Plug-and-Play Medical Dialogue System. (arXiv:2305.11508v1 [cs.CL])

    [http://arxiv.org/abs/2305.11508](http://arxiv.org/abs/2305.11508)

    该论文提出了一种即插即用的医疗对话系统，使用大型语言模型实现医疗问答及诊断策略，避免了传统昂贵的LLMs微调。

    

    医疗对话系统旨在为患者提供准确的答案，需要特定的领域知识。大型语言模型（LLMs）的最近进展已经证明了其在医疗问答领域具有杰出的能力，表明具备了对常识的丰富理解。然而，由于缺乏诊断策略，LLMs无法直接用于诊断。传统的解决方法是昂贵的LLMs微调。另一种更具吸引力的解决方法是开发一个插件，赋予LLMs执行医疗对话任务的能力。受到上下文学习的启发，我们提出了PlugMed，一个即插即用的医疗对话系统，通过两个模块促进了LLMs的恰当对话动作：提示生成（PG）模块和回复排名（RR）模块。PG模块旨在从全局和局部角度捕获对话信息。它通过评估匹配度来选择合适的提示。

    Medical dialogue systems aim to provide accurate answers to patients, necessitating specific domain knowledge. Recent advancements in Large Language Models (LLMs) have demonstrated their exceptional capabilities in the medical Q&A domain, indicating a rich understanding of common sense. However, LLMs are insufficient for direct diagnosis due to the absence of diagnostic strategies. The conventional approach to address this challenge involves expensive fine-tuning of LLMs. Alternatively, a more appealing solution is the development of a plugin that empowers LLMs to perform medical conversation tasks. Drawing inspiration from in-context learning, we propose PlugMed, a Plug-and-Play Medical Dialogue System that facilitates appropriate dialogue actions by LLMs through two modules: the prompt generation (PG) module and the response ranking (RR) module. The PG module is designed to capture dialogue information from both global and local perspectives. It selects suitable prompts by assessing 
    
[^108]: 跨模态数据增强用于端到端手语翻译

    Cross-modality Data Augmentation for End-to-End Sign Language Translation. (arXiv:2305.11096v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.11096](http://arxiv.org/abs/2305.11096)

    本文提出了一种Cross-modality Data Augmentation（XmDA）框架，通过利用来自手语单词翻译模型的伪手语单词-文本对，将强大的手语单词到文本的翻译能力转移到了端到端手语翻译，实验结果表明XmDA在该领域中明显优于现有的最先进方法。

    

    端到端手语翻译旨在直接将手语视频转换为口语文本，无需中间表示。受手语视频和文本之间的模态差距和标记数据的稀缺性的挑战，这一任务一直很具有挑战性。为了应对这些挑战，我们提出了一种新颖的“跨模态数据增强（XmDA）”框架，通过利用来自手语单词翻译模型的伪手语单词-文本对，将强大的手语单词到文本的翻译能力转移到了端到端手语翻译（即视频到文本）。具体来说，XmDA包括两个关键组成部分，即跨模态混合和跨模态知识蒸馏。前者明确地促进手语视频特征和手语单词嵌入之间的对齐，以弥合模态差距。后者利用来自手语单词到文本的教师模型的生成知识来指导口语文本生成。在两个广泛使用的手语翻译数据集LIBRISIGN和WLASL上的实验结果表明，XmDA在自动评估指标和人类评估方面均明显优于现有的最先进方法。

    End-to-end sign language translation (SLT) aims to convert sign language videos into spoken language texts directly without intermediate representations. It has been a challenging task due to the modality gap between sign videos and texts and the data scarcity of labeled data. To tackle these challenges, we propose a novel Cross-modality Data Augmentation (XmDA) framework to transfer the powerful gloss-to-text translation capabilities to end-to-end sign language translation (i.e. video-to-text) by exploiting pseudo gloss-text pairs from the sign gloss translation model. Specifically, XmDA consists of two key components, namely, cross-modality mix-up and cross-modality knowledge distillation. The former explicitly encourages the alignment between sign video features and gloss embeddings to bridge the modality gap. The latter utilizes the generation knowledge from gloss-to-text teacher models to guide the spoken language text generation. Experimental results on two widely used SLT datase
    
[^109]: 不要用明文上传测试数据：减轻数据外泄对于评估基准的持续影响的实用策略

    Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks. (arXiv:2305.10160v1 [cs.CL])

    [http://arxiv.org/abs/2305.10160](http://arxiv.org/abs/2305.10160)

    提出了三个适用策略：（1）公钥加密发布测试数据，仅允许特定派生发布；（2）对于API持有方，要求训练排除控制，保护测试数据，不停止评估直到达到要求；（3）如果测试数据来自互联网文本，需避免某些结果的使用。

    

    随着预训练模型在自动爬网资料库的大规模应用，数据外泄变得常见且部分难以应对。对于那些不会公开训练数据的模型，其数据成为了商业机密，即使在公开模型中，确定特定测试实例是否被泄露也不是一件容易的事情。本文提出三个可行的策略：（1）使用公钥加密发布的测试数据并限制派生发布的许可；（2）要求持有API训练数据的公司采用训练排除控制，并拒绝评估，直到训练排除控制无误为止；（3）如果测试数据来自互联网文本，那么需避免在网络搜索中出现包含正确提取部分的数据。

    Data contamination has become especially prevalent and challenging with the rise of models pretrained on very large, automatically-crawled corpora. For closed models, the training data becomes a trade secret, and even for open models, it is not trivial to ascertain whether a particular test instance has been compromised. Strategies such as live leaderboards with hidden answers, or using test data which is guaranteed to be unseen, are expensive and become fragile with time. Assuming that all relevant actors value clean test data and will cooperate to mitigate data contamination, what can be done? We propose three strategies that can make a difference: (1) Test data made public should be encrypted with a public key and licensed to disallow derivative distribution; (2) demand training exclusion controls from closed API holders, and protect your test data by refusing to evaluate until demands are met; (3) in case of test data based on internet text, avoid data which appears with its soluti
    
[^110]: Vera：一个用于通用常识语句可信度评估的模型

    Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements. (arXiv:2305.03695v1 [cs.CL])

    [http://arxiv.org/abs/2305.03695](http://arxiv.org/abs/2305.03695)

    本文提出了Vera模型，它是一个通用模型，可以基于常识知识估计陈述性语句的可信度。在解决验证格式的常识问题时，Vera明显优于现有的模型，并展现了对未见任务的泛化能力和良好的标定输出。

    

    尽管当今的语言模型在许多方面表现出色，但它们仍然容易出现荒谬和意外的常识失败。本文提出了一种回顾性验证方法，反思LM输出的正确性，并引入了Vera，一个通用模型，它基于常识知识估计陈述性语句的可信度。通过使用19个QA数据集和两个大规模知识库创建的约700万条常识语句以及三个训练目标的组合进行训练，Vera是一个多功能模型，可以有效地区分各种常识领域中的正确和错误语句。当应用于解决验证格式的常识问题时，Vera明显优于现有的可重用于常识验证的模型，并且它进一步展示了对未见任务的泛化能力并提供了良好的标定输出。我们发现Vera在过滤LM生成的常识知识方面表现突出，可以潜在地增强它们的可信度和实际应用。

    Despite the much discussed capabilities of today's language models, they are still prone to silly and unexpected commonsense failures. We consider a retrospective verification approach that reflects on the correctness of LM outputs, and introduce Vera, a general-purpose model that estimates the plausibility of declarative statements based on commonsense knowledge. Trained on ~7M commonsense statements created from 19 QA datasets and two large-scale knowledge bases, and with a combination of three training objectives, Vera is a versatile model that effectively separates correct from incorrect statements across diverse commonsense domains. When applied to solving commonsense problems in the verification format, Vera substantially outperforms existing models that can be repurposed for commonsense verification, and it further exhibits generalization capabilities to unseen tasks and provides well-calibrated outputs. We find that Vera excels at filtering LM-generated commonsense knowledge an
    
[^111]: TempoSum：评估抽象摘要的时间泛化能力

    TempoSum: Evaluating the Temporal Generalization of Abstractive Summarization. (arXiv:2305.01951v1 [cs.CL])

    [http://arxiv.org/abs/2305.01951](http://arxiv.org/abs/2305.01951)

    本篇论文提出了 TempoSum 抽象摘要的时间泛化能力基准，通过广泛的人类评估证明了摘要模型中存储的参数化知识对未来数据上生成的摘要有显著影响。

    

    最近，预训练语言模型在现有的抽象摘要数据集中取得了有 promising 的结果。然而，现有的摘要基准与标准的预训练语料库和微调数据集在时间上重叠。因此，预训练语言模型的强大性能可能依赖于预训练和微调过程中所记忆的参数化知识。此外，预训练语言模型所记忆的知识可能很快就过时，这会影响到它们在未来数据上的泛化性能。为了了解抽象摘要模型的时间泛化能力，本文提出了 TempoSum，一个新的基准，其中包含了从 2010 年到 2022 年的数据样本。通过广泛的人类评估，我们证明了摘要模型中存储的参数化知识对未来数据上生成的摘要的准确性有显著影响。此外，现有的准确性提高方法不能可靠地提高摘要模型在未来数据上的准确性。

    Recent pre-trained language models (PLMs) achieve promising results in existing abstractive summarization datasets. However, existing summarization benchmarks overlap in time with the standard pre-training corpora and finetuning datasets. Hence, the strong performance of PLMs may rely on the parametric knowledge that is memorized during pre-training and fine-tuning. Moreover, the knowledge memorized by PLMs may quickly become outdated, which affects the generalization performance of PLMs on future data. In this work, we propose TempoSum, a novel benchmark that contains data samples from 2010 to 2022, to understand the temporal generalization ability of abstractive summarization models. Through extensive human evaluation, we show that parametric knowledge stored in summarization models significantly affects the faithfulness of the generated summaries on future data. Moreover, existing faithfulness enhancement methods cannot reliably improve the faithfulness of summarization models on fu
    
[^112]: RexUIE: 一种带显式模式说明的递归方法，用于通用信息提取

    RexUIE: A Recursive Method with Explicit Schema Instructor for Universal Information Extraction. (arXiv:2304.14770v1 [cs.CL])

    [http://arxiv.org/abs/2304.14770](http://arxiv.org/abs/2304.14770)

    本文提出了一种名为RexUIE的方法，采用递归机制和显式模式说明，实现通用信息提取(UIE)。与以往的模型相比，RexUIE在提取不同模式时不会出现冲突，并且显式模式说明器有助于提高模型的泛化能力和性能，特别是在低资源情况下。

    

    通用信息提取(UIE)因其具有不同目标、异构结构和需求特定模式而备受关注。然而，以前的研究仅通过统一一些任务(如命名实体识别和关系提取)取得了有限的成功，不能真正称之为UIE模型，特别是当提取其他常见模式（如四元组和五元组）时。此外，这些模型采用了一个隐式结构模式说明符，在类型之间建立错误的链接，使模型的泛化和在低资源情况下的性能受到影响。本文重新定义了正式的UIE公式，涵盖了几乎所有提取模式。我们是首次为任何类型的模式引入UIE。此外，我们提出了RexUIE，它是一种带显式模式说明的递归方法，用于 UIE。为了避免不同模式之间的干扰，RexUIE通过递归机制统一提取过程，适应多种提取场景。此外，我们引入了显式模式说明器，指导模型学习给定场景的结构模式，提高了在低资源设置中的泛化和性能。

    Universal Information Extraction (UIE) is an area of interest due to the challenges posed by varying targets, heterogeneous structures, and demand-specific schemas. However, previous works have only achieved limited success by unifying a few tasks, such as Named Entity Recognition (NER) and Relation Extraction (RE), which fall short of being authentic UIE models particularly when extracting other general schemas such as quadruples and quintuples. Additionally, these models used an implicit structural schema instructor, which could lead to incorrect links between types, hindering the model's generalization and performance in low-resource scenarios. In this paper, we redefine the authentic UIE with a formal formulation that encompasses almost all extraction schemas. To the best of our knowledge, we are the first to introduce UIE for any kind of schemas. In addition, we propose RexUIE, which is a Recursive Method with Explicit Schema Instructor for UIE. To avoid interference between diffe
    
[^113]: 超越多链思维：基于元推理的问题解答方法

    Answering Questions by Meta-Reasoning over Multiple Chains of Thought. (arXiv:2304.13007v1 [cs.CL])

    [http://arxiv.org/abs/2304.13007](http://arxiv.org/abs/2304.13007)

    本论文提出了基于元推理的Multi-Chain Reasoning (MCR)方法，该方法检查多个推理链，混合它们之间的信息并选择最相关的事实，从而超越多链思维，解决多跳QA问题。 实验结果表明MCR胜过多个强基线，解释质量高。

    

    现代多跳问题解答（QA）系统通常将问题分解为一系列思考步骤（CoT），然后才得出最终答案。通常来说，多个链条被抽样并通过最终答案的投票机制进行聚合，但中间步骤本身被丢弃。虽然这种方法提高了性能，但它们并不考虑链之间的中间步骤之间的关系，并且不提供预测答案的统一解释。我们引入了基于元推理的 Multi-Chain Reasoning (MCR) 方法，该方法利用大型语言模型来超越多个思考链，而不是聚合回答。MCR检查不同的推理链，混合它们之间的信息并选择在生成解释和预测答案时最相关的事实。MCR在7个多跳QA数据集上胜过强基线。此外，我们的分析表明MCR的解释具有高质量。

    Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregating their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations exhibit high quality, en
    
[^114]: SkillGPT: 一种使用大型语言模型进行技能提取和标准化的RESTful API服务

    SkillGPT: a RESTful API service for skill extraction and standardization using a Large Language Model. (arXiv:2304.11060v1 [cs.CL])

    [http://arxiv.org/abs/2304.11060](http://arxiv.org/abs/2304.11060)

    SkillGPT是一种使用大型语言模型进行技能提取和标准化的RESTful API服务，通过摘要和向量相似性搜索平衡速度和准确度。

    

    我们提出了 SkillGPT，一种利用开源的大型语言模型 (LLM) 进行从自由风格职位描述和用户资料中进行技能提取和标准化 (SES) 的工具。与大多数类似任务的以前方法不同，SkillGPT 直接使用最新的对话 LLM 进行标准技能的提示，通过摘要和向量相似性搜索来平衡速度和准确度。因此，我们的免费 SkillGPT 让用户能够高效可靠地进行对话型 SES。

    We present SkillGPT, a tool for skill extraction and standardization (SES) from free-style job descriptions and user profiles with an open-source Large Language Model (LLM) as backbone. Most previous methods for similar tasks either need supervision or rely on heavy data-preprocessing and feature engineering. Directly prompting the latest conversational LLM for standard skills, however, is slow, costly and inaccurate. In contrast, SkillGPT utilizes a LLM to perform its tasks in steps via summarization and vector similarity search, to balance speed with precision. The backbone LLM of SkillGPT is based on Llama, free for academic use and thus useful for exploratory research and prototype development. Hence, our cost-free SkillGPT gives users the convenience of conversational SES, efficiently and reliably.
    
[^115]: 语义转换混淆AI生成文本检测，而检索是一种有效的防御方法

    Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. (arXiv:2303.13408v1 [cs.CL])

    [http://arxiv.org/abs/2303.13408](http://arxiv.org/abs/2303.13408)

    本文探究了语义转换对于AI生成文本检测器的鲁棒性，通过训练的语义转换生成模型成功混淆了多个检测器。

    

    近期有多种方法被提出来用于识别恶意使用大型语言模型 (例如虚假内容创建或学术抄袭)中的AI生成文本，包括通过水印或统计异常点。本文探究这些文本检测算法对于AI生成文本的含义转换的鲁棒性。为了测试这些检测器的性能，我们首先训练了一个11B参数的语义转换生成模型(DIPPER)，该模型可以将段落进行语义转换，可选择利用周围文本(例如用户写的提示)作为上下文。DIPPER还使用标量旋钮来控制语义转换中词汇多样性和重新排列的程度。通过使用DIPPER来进行三种大型语言模型生成文本的语义转换，成功地混淆了多个文本检测器，包括水印检测、GPTZero、DetectGPT和OpenAI的文本分类器。例如，DIPPER将DetectGPT的检测准确率从70.3%降至4.6%（在恒定的1%误报率下）。

    To detect the deployment of large language models for malicious use cases (e.g., fake content creation or academic plagiarism), several approaches have recently been proposed for identifying AI-generated text via watermarks or statistical irregularities. How robust are these detection algorithms to paraphrases of AI-generated text? To stress test these detectors, we first train an 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, optionally leveraging surrounding text (e.g., user-written prompts) as context. DIPPER also uses scalar knobs to control the amount of lexical diversity and reordering in the paraphrases. Paraphrasing text generated by three large language models (including GPT3.5-davinci-003) with DIPPER successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops the detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), withou
    
[^116]: 可控数据生成的软提示混合模型

    Mixture of Soft Prompts for Controllable Data Generation. (arXiv:2303.01580v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.01580](http://arxiv.org/abs/2303.01580)

    这项研究提出了一种称为软提示混合模型（MSP）的方法，将大型语言模型（LLM）用于数据增强而不是直接预测。实验结果显示，MSP能够生成多样且自然的文本，同时保留标签语义，并在三个基准测试中取得了最先进的结果。

    

    大型语言模型（LLM）在目标输出遵循自然语言模式时有效地生成流畅的文本。然而，结构化预测任务将输出格式限制在有限的本体上，导致即使是非常大的模型也难以应对，因为它们从未被训练在考虑这种限制的情况下。在少样本学习场景中，LLM的直接预测问题变得更加困难，这常常是由于领域移位和资源限制造成的。我们通过将LLM作为数据增强的工具而不是直接预测，从而颠覆了这个问题。我们提出的软提示混合模型（MSP）是一种参数高效的生成数据的方法。进一步应用去噪机制来提高合成数据的质量。自动评估指标显示我们的方法能够产生多样且自然的文本，同时保留标签语义。此外，MSP在三个基准测试中取得了最先进的结果。

    Large language models (LLMs) effectively generate fluent text when the target output follows natural language patterns. However, structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations. We flip the problem on its head by leveraging the LLM as a tool for data augmentation rather than direct prediction. Our proposed Mixture of Soft Prompts (MSP) serves as a parameter-efficient procedure for generating data in a controlled manner. Denoising mechanisms are further applied to improve the quality of synthesized data. Automatic metrics show our method is capable of producing diverse and natural text, while preserving label semantics. Moreover, MSP achieves state-of-the-art results on three benchmarks when co
    
[^117]: 在未知环境中为时间任务建立复杂自然语言命令的基础

    Grounding Complex Natural Language Commands for Temporal Tasks in Unseen Environments. (arXiv:2302.11649v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.11649](http://arxiv.org/abs/2302.11649)

    该论文提出了一个名为Lang2LTL的模块化系统和软件包，利用大型语言模型将导航命令与LTL规范进行关联。通过在没有先前语言数据的环境中全面评估Lang2LTL，证明了其在21个城市级规模的环境中与各种时态规范进行关联的最先进能力。并且通过展示物理机器人在两个室内环境中可以遵循52个语义多样的导航命令，进一步验证了Lang2LTL的实际应用价值。

    

    将导航命令与线性时态逻辑 (LTL) 进行关联，利用其明确的语义来推理长程任务和验证时间约束的满足性。现有方法需要来自特定环境和用于理解这些环境中的命令的地标的训练数据。我们提出了Lang2LTL，这是一个模块化系统和一个软件包，利用大型语言模型 (LLM) 在没有先前语言数据的环境中将时态导航命令与LTL规范进行关联。我们对Lang2LTL进行了五个明确定义的泛化行为的全面评估。Lang2LTL在21个城市级规模的环境中展示了单个模型将导航命令与各种时态规范进行关联的最先进能力。最后，我们展示了一个使用Lang2LTL的物理机器人在两个室内环境中遵循52个语义多样的导航命令的能力。

    Grounding navigational commands to linear temporal logic (LTL) leverages its unambiguous semantics for reasoning about long-horizon tasks and verifying the satisfaction of temporal constraints. Existing approaches require training data from the specific environment and landmarks that will be used in natural language to understand commands in those environments. We propose Lang2LTL, a modular system and a software package that leverages large language models (LLMs) to ground temporal navigational commands to LTL specifications in environments without prior language data. We comprehensively evaluate Lang2LTL for five well-defined generalization behaviors. Lang2LTL demonstrates the state-of-the-art ability of a single model to ground navigational commands to diverse temporal specifications in 21 city-scaled environments. Finally, we demonstrate a physical robot using Lang2LTL can follow 52 semantically diverse navigational commands in two indoor environments.
    
[^118]: 通过监督稀疏多粒度学习进行视频文本检索

    Video-Text Retrieval by Supervised Sparse Multi-Grained Learning. (arXiv:2302.09473v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.09473](http://arxiv.org/abs/2302.09473)

    本文提出了一种新的多粒度稀疏学习框架S3MA，用于视频文本检索。该框架通过学习共享的稀疏空间和多粒度相似度来改进检索效果，在多个基准测试中表现优于现有方法。

    

    对于视频文本检索，最近在探索更好的表示学习方面取得了进展。本文提出了一个新的多粒度稀疏学习框架S3MA，用于学习视频和文本之间的共享稀疏空间，从而实现视频文本检索。共享稀疏空间通过有限数量的稀疏概念进行初始化，每个概念都对应一些词语。利用现有的文本数据，我们以监督方式学习和更新共享稀疏空间，使用提出的相似度和对齐损失函数。此外，为了实现多粒度对齐，我们将帧表示方法纳入模型，更好地对视频模态进行建模和计算细粒度和粗粒度的相似度。通过学习得到的共享稀疏空间和多粒度相似度，我们在多个视频文本检索基准上进行了广泛实验，实验结果表明S3MA优于现有方法。我们的代码可以在https://github.com/yim上找到。

    While recent progress in video-text retrieval has been advanced by the exploration of better representation learning, in this paper, we present a novel multi-grained sparse learning framework, S3MA, to learn an aligned sparse space shared between the video and the text for video-text retrieval. The shared sparse space is initialized with a finite number of sparse concepts, each of which refers to a number of words. With the text data at hand, we learn and update the shared sparse space in a supervised manner using the proposed similarity and alignment losses. Moreover, to enable multi-grained alignment, we incorporate frame representations for better modeling the video modality and calculating fine-grained and coarse-grained similarities. Benefiting from the learned shared sparse space and multi-grained similarities, extensive experiments on several video-text retrieval benchmarks demonstrate the superiority of S3MA over existing methods. Our code is available at https://github.com/yim
    
[^119]: 回顾链将语言模型与反馈对齐

    Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02676](http://arxiv.org/abs/2302.02676)

    该研究提出了一种新颖的技术，即回顾链，可以轻松优化，并可以从任何形式的反馈中学习，而不受其极性的影响。

    

    从人类偏好中学习对于语言模型具有重要意义，这样才能对人类有所帮助并符合人类和社会价值观。先前的研究通过从人类反馈中学习来理解和遵循指令取得了显著成功。然而，这些方法要么是基于被人类注释者喜欢的手动挑选的模型生成，使得它们在数据利用方面效果不佳且普遍应用具有挑战性，要么依赖于奖励函数和强化学习，这容易出现奖励函数不完美和极难优化的问题。在本文中，我们提出了一种新颖的技术，“回顾链”，它易于优化，并可以从任何形式的反馈中学习，而不受其极性的影响。我们的想法受到了人类如何从以语言形式呈现的广泛反馈中学习的启发。我们将所有类型的反馈转换成句子，然后用它们来微调模型，从而利用这种方法。

    Learning from human preferences is important for language models to be helpful and useful for humans, and to align with human and social values. Prior work have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them ineffective in terms of data utilization and challenging to apply in general, or they depend on reward functions and reinforcement learning, which are prone to imperfect reward function and extremely challenging to optimize. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sentences, which are then used to fine-tune the model, allowing us to take advantage
    
[^120]: 学习用于排名的列表级别领域不变表示

    Learning List-Level Domain-Invariant Representations for Ranking. (arXiv:2212.10764v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2212.10764](http://arxiv.org/abs/2212.10764)

    本文提出了一种针对排名问题的列表级别对齐的学习方法，该方法利用列表的结构特性，在领域适应中实现从源领域到目标领域的知识转移。

    

    领域适应旨在将在（数据丰富）源领域学到的知识转移到（资源有限）目标领域，一种常用的方法是不变表示学习，它匹配并对齐特征空间上的数据分布。尽管这种方法在分类和回归问题上得到了广泛研究和应用，但在排名问题上的应用却是零散的，并且现有的几种实现缺乏理论上的证明。本文重新审视了用于排名的不变表示学习。在审查之前的工作时，我们发现他们实施了我们称之为项目级别对齐的方法，该方法在聚合的所有列表中对进行排名的项目分布进行对齐，但忽略了列表的结构。然而，列表的结构应该被利用，因为它是排名问题的固有特性，其中数据和度量是在列表上定义和计算的，而不是在项目本身上。为了解决这一不一致，我们提出了列表级别对齐的学习

    Domain adaptation aims to transfer the knowledge learned on (data-rich) source domains to (low-resource) target domains, and a popular method is invariant representation learning, which matches and aligns the data distributions on the feature space. Although this method is studied extensively and applied on classification and regression problems, its adoption on ranking problems is sporadic, and the few existing implementations lack theoretical justifications. This paper revisits invariant representation learning for ranking. Upon reviewing prior work, we found that they implement what we call item-level alignment, which aligns the distributions of the items being ranked from all lists in aggregate but ignores their list structure. However, the list structure should be leveraged, because it is intrinsic to ranking problems where the data and the metrics are defined and computed on lists, not the items by themselves. To close this discrepancy, we propose list-level alignment -learning
    
[^121]: Seq2Seq-SC:具有预训练语言模型的端到端语义通信系统

    Seq2Seq-SC: End-to-End Semantic Communication Systems with Pre-trained Language Model. (arXiv:2210.15237v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2210.15237](http://arxiv.org/abs/2210.15237)

    Seq2Seq-SC是一个具备预训练语言模型的端到端语义通信系统，通过专注于消息含义实现前所未有的通信效率，并且在提取语义信息上表现出卓越性能，为语义通信的发展以及6G网络中与未来无线系统的整合开辟了新的可能性。

    

    在这项工作中，我们提出了一个名为seq2seq-SC的现实语义网络，旨在与5G NR兼容，并能够使用预训练语言模型处理通用文本数据集。我们的目标是通过专注于语义通信中消息的含义来实现前所未有的通信效率。我们采用了一种叫做语义相似度的性能指标，通过BLEU测量词汇相似性，通过SBERT测量语义相似性。我们的研究结果表明，seq2seq-SC在提取语义有意义信息的同时保持了优越的性能，优于先前的模型。这项研究为语义通信的持续进步以及在6G网络中与未来无线系统的潜在整合铺平了道路。

    In this work, we propose a realistic semantic network called seq2seq-SC, designed to be compatible with 5G NR and capable of working with generalized text datasets using a pre-trained language model. The goal is to achieve unprecedented communication efficiency by focusing on the meaning of messages in semantic communication. We employ a performance metric called semantic similarity, measured by BLEU for lexical similarity and SBERT for semantic similarity. Our findings demonstrate that seq2seq-SC outperforms previous models in extracting semantically meaningful information while maintaining superior performance. This study paves the way for continued advancements in semantic communication and its prospective incorporation with future wireless systems in 6G networks.
    
[^122]: YATO: 另一个基于深度学习的文本分析开源工具包

    YATO: Yet Another deep learning based Text analysis Open toolkit. (arXiv:2209.13877v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.13877](http://arxiv.org/abs/2209.13877)

    YATO是一个轻量且易于使用的开源工具包，支持深度学习文本分析，可组合不同的特征类型，并提供灵活性和易用性的优势，促进了先进NLP模型的复现和改进，以及跨学科应用的推动。

    

    我们介绍了YATO，一个开源、易于使用的用于深度学习文本分析的工具包。与现有的重度工程化工具包和平台不同，YATO轻量且用户友好，适用于跨学科领域的研究人员。YATO设计成分层结构，支持自由组合三种广泛使用的特征类型，包括1）传统神经网络（CNN，RNN等）；2）预训练语言模型（BERT，RoBERTa，ELECTRA等）；和3）通过简单可配置的文件实现的用户定制化神经特征。由于灵活性和易用性的优势，YATO可以促进快速复现和改进先进的自然语言处理模型，并推动自然语言处理技术在跨学科领域的应用。代码、示例和文档可在https://github.com/jiesutd/YATO公开获取。还提供了演示视频https://youtu.be/tSjjf5BzfQg。

    We introduce YATO, an open-source, easy-to-use toolkit for text analysis with deep learning. Different from existing heavily engineered toolkits and platforms, YATO is lightweight and user-friendly for researchers from cross-disciplinary areas. Designed in a hierarchical structure, YATO supports free combinations of three types of widely used features including 1) traditional neural networks (CNN, RNN, etc.); 2) pre-trained language models (BERT, RoBERTa, ELECTRA, etc.); and 3) user-customized neural features via a simple configurable file. Benefiting from the advantages of flexibility and ease of use, YATO can facilitate fast reproduction and refinement of state-of-the-art NLP models, and promote the cross-disciplinary applications of NLP techniques. The code, examples, and documentation are publicly available at https://github.com/jiesutd/YATO. A demo video is also available at https://youtu.be/tSjjf5BzfQg.
    

