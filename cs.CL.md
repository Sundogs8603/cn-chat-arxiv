# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [CBQ: Cross-Block Quantization for Large Language Models](https://rss.arxiv.org/abs/2312.07950) | CBQ是一种用于大型语言模型的跨块重构型后训练量化方法。CBQ通过使用同源重构方案来建立块间的长程依赖关系，最小化误差积累。CBQ还采用了粗到精的预处理策略和自适应的取整技术，使其能够有效处理极端异常值并提高整体量化精度。 |
| [^2] | [Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models](https://arxiv.org/abs/2403.18814) | Mini-Gemini 挖掘了VLMs的潜力，通过高分辨率视觉标记、高质量数据和VLM引导生成等方式，缩小了与先进模型之间的性能差距 |
| [^3] | [Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation](https://arxiv.org/abs/2403.18804) | 本研究通过知识蒸馏的视角，探讨了模块化方法在不同模型之间的可迁移性，提出了一种简单直接的方法，将预训练的任务特定模块在同系列PLM之间进行转移。 |
| [^4] | [Projective Methods for Mitigating Gender Bias in Pre-trained Language Models](https://arxiv.org/abs/2403.18803) | 研究探讨了将用于静态词嵌入的最简单投影去偏置方法应用于BERT内部表示的效果，发现这种方法既可以减少内在偏见，又可以缓解下游任务中观察到的偏见。 |
| [^5] | [Long-form factuality in large language models](https://arxiv.org/abs/2403.18802) | 该论文提出了一种通过使用大型语言模型将长篇回应分解为单个事实，并通过发送搜索查询到Google搜索，评估事实准确性的方法，并扩展了F1分数作为长篇事实性的聚合度量。 |
| [^6] | [Towards a World-English Language Model for On-Device Virtual Assistants](https://arxiv.org/abs/2403.18783) | 该论文提出了为设备上的虚拟助手打造世界英语语言模型的方法，通过结合英语的区域变体，采用适配器瓶颈来模拟特定方言特征，实现了准确性、延迟和内存方面的性能平衡。 |
| [^7] | [CheckEval: Robust Evaluation Framework using Large Language Model via Checklist](https://arxiv.org/abs/2403.18771) | CheckEval是一种使用大型语言模型构建的评估框架，通过详细的子方面和布尔问题清单简化了评估过程，增强了评估结果的稳健性和可靠性，通过SummEval基准验证其有效性。 |
| [^8] | [Improved Neural Protoform Reconstruction via Reflex Prediction](https://arxiv.org/abs/2403.18769) | 该论文提出通过反射来预测祖语的方法，从而改进了神经原型重建模型。 |
| [^9] | [CYCLE: Learning to Self-Refine the Code Generation](https://arxiv.org/abs/2403.18746) | CYCLE框架提出了学习如何根据可用反馈，如测试套件报告的执行结果，自我调整错误生成的方法，成功地解决了代码语言模型无法自我调整的问题 |
| [^10] | [Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding](https://arxiv.org/abs/2403.18715) | 这项研究提出了一种名为指示对比解码(ICD)方法，旨在减少大规模视觉-语言模型(LVLMs)推断过程中的幻觉，通过对标准和指示扰动的分布进行对比，从原始分布中减去幻觉概念。 |
| [^11] | [The Invalsi Benchmark: measuring Language Models Mathematical and Language understanding in Italian](https://arxiv.org/abs/2403.18697) | 该研究提出了两个新的基准，用于评估语言模型在意大利语的数学和语言理解能力，为当前语言模型的性能提供了具有挑战性的评估标准。 |
| [^12] | [Scaling Laws For Dense Retrieval](https://arxiv.org/abs/2403.18684) | 该研究探究了密集检索模型的性能是否遵循其他神经模型的缩放规律，并提出使用对比对数似然作为评估指标进行了广泛实验。 |
| [^13] | [NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method](https://arxiv.org/abs/2403.18680) | NL-ITI通过引入非线性探测和多令牌干预，成功改进了ITI方法，在多个多选基准上取得了显著的性能改进。 |
| [^14] | [Fact Checking Beyond Training Set](https://arxiv.org/abs/2403.18671) | 论文探讨了超越训练集的事实核查问题，通过提出新颖的对抗算法和训练方法，使事实核查流程在不同领域中更加稳健。 |
| [^15] | [Improving Content Recommendation: Knowledge Graph-Based Semantic Contrastive Learning for Diversity and Cold-Start Users](https://arxiv.org/abs/2403.18667) | 提出了一种基于知识图的语义对比学习方法，通过混合多任务学习在用户-项目和项目-项目交互上进行训练，从而提供个性化和多样化的推荐。 |
| [^16] | [SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens](https://arxiv.org/abs/2403.18647) | SDSAT提出了一种加速大型语言模型推断的方案，通过使用具有灵活解码能力的语义自适应令牌，可以增强模型生成高质量草稿令牌的能力，并实现超过3.5倍和3.0倍的速度提升。 |
| [^17] | [Vulnerability Detection with Code Language Models: How Far Are We?](https://arxiv.org/abs/2403.18624) | 我们提出了一个新的数据集PrimeVul，用于训练和评估代码语言模型进行漏洞检测，通过引入一套新颖的数据标记技术，实现了与人工验证基准相当的标签准确性，显著扩大了数据集。 |
| [^18] | [A survey on learning models of spiking neural membrane systems and spiking neural networks](https://arxiv.org/abs/2403.18609) | 该论文调查了尖峰神经膜系统和尖峰神经网络学习模型的结构、功能、优缺点，并对机器学习和深度学习模型的最新应用进行了调查。 |
| [^19] | [Debiasing Sentence Embedders through Contrastive Word Pairs](https://arxiv.org/abs/2403.18555) | 本文探讨了一种通过对比词对消除句子嵌入器中线性和非线性偏差信息的方法 |
| [^20] | [Attention-aware semantic relevance predicting Chinese sentence reading](https://arxiv.org/abs/2403.18542) | 提出了一种基于“关注语境”的方法，可以更准确地预测中文阅读任务中的凝视持续时间。 |
| [^21] | [A Path Towards Legal Autonomy: An interoperable and explainable approach to extracting, transforming, loading and computing legal information using large language models, expert systems and Bayesian networks](https://arxiv.org/abs/2403.18537) | 本文提出了一种可互操作和可解释的方法，利用大语言模型、专家系统和贝叶斯网络来提取、转换、加载和计算法律信息，从而实现通向法律自治的路径。 |
| [^22] | [Language Plays a Pivotal Role in the Object-Attribute Compositional Generalization of CLIP](https://arxiv.org/abs/2403.18525) | 本研究发现，通过大型数据集训练的CLIP模型在对象-属性组合泛化中表现出明显优势，为泛化和数据集规模之间的关系提供了重要见解。 |
| [^23] | [AcTED: Automatic Acquisition of Typical Event Duration for Semi-supervised Temporal Commonsense QA](https://arxiv.org/abs/2403.18504) | 提出一种投票驱动的半监督方法，自动获取事件的典型持续时间，以此作为伪标签数据，实现了在时间常识问答任务中的最先进性能。 |
| [^24] | [Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction](https://arxiv.org/abs/2403.18447) | 本文提出了LMTraj（基于语言的多模态轨迹预测器），将轨迹预测任务转化为一种类似于问答问题的形式，通过将数值和图像数据转换为自然语言空间来引导语言模型继续生成与轨迹相关的多模态预测。 |
| [^25] | [DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via Structural Word Alignment](https://arxiv.org/abs/2403.18435) | 引入DELTA，利用结构化词对齐预训练判别式编码器进行法律案例检索，通过强调关键事实来提高表示的判别能力。 |
| [^26] | [Exploring language relations through syntactic distances and geographic proximity](https://arxiv.org/abs/2403.18430) | 通过句法距离和地理邻近性探索语言关系，使用POS trigrams最大化捕捉句法变化，建立语言连接并揭示语言家族和群体的簇。 |
| [^27] | [TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions](https://arxiv.org/abs/2403.18426) | 提出了一个用于自动提示生成的框架，构建了一个包含160,230个提示的大规模数据集TriviaHG，并提出了一种评估方法来衡量提示的质量属性。 |
| [^28] | [SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks](https://arxiv.org/abs/2403.18423) | 提出了一种称为Semantic Robust Defence (SemRoDe)的新方法，通过宏观对抗训练策略增强了语言模型（LMs）的鲁棒性，学习了能够连接对抗领域和基本领域的鲁棒表示。 |
| [^29] | [BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text](https://arxiv.org/abs/2403.18421) | BioMedLM是一个27亿参数的语言模型，在PubMed文献上训练，可以在生物医学领域表现出色，尤其适用于多项选择问题回答和患者提问。 |
| [^30] | [An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM](https://arxiv.org/abs/2403.18406) | 本研究提出了一种新颖的策略，使用单一的Vision Language Model (VLM) 来进行零样本视频问答，将视频转换为单个合成图像以实现视频理解。 |
| [^31] | [Improving Attributed Text Generation of Large Language Models via Preference Learning](https://arxiv.org/abs/2403.18381) | 通过偏好学习建模和引入自动偏好优化框架，该研究解决了大型语言模型生成不可靠内容的挑战，并提出了一种自动生成归因偏好数据的方法。 |
| [^32] | [BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models](https://arxiv.org/abs/2403.18365) | BLADE框架将黑盒大型语言模型与小型领域特定模型结合，既保留了领域特定知识和专业见解，又提供了强大的语言理解和推理能力。 |
| [^33] | [Evaluation of Semantic Search and its Role in Retrieved-Augmented-Generation (RAG) for Arabic Language](https://arxiv.org/abs/2403.18350) | 本文旨在建立阿拉伯语义搜索的基准，并在检索增强生成（RAG）框架内评估其有效性。 |
| [^34] | [Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback](https://arxiv.org/abs/2403.18349) | 这里是中文总结出的一句话要点: 该论文研究了拒绝机制在提高大型语言模型可靠性中的作用，提出了一种基于知识反馈的强化学习框架RLKF。 |
| [^35] | [Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective](https://arxiv.org/abs/2403.18346) | 提出了一个因果框架用于解释多模态大型语言模型在视觉问答问题中的偏差，并引入了一个新的挑战性数据集MORE，同时提出两种减轻单模态偏差的策略。 |
| [^36] | [IterAlign: Iterative Constitutional Alignment of Large Language Models](https://arxiv.org/abs/2403.18341) | 提出了IterAlign，是一种数据驱动的宪法发现和自对齐框架，通过红队测试发现LLM的弱点，并使用更强大的LLM自动发现新的宪法，从而指导LLM的自校正。 |
| [^37] | [A Dataset for Pharmacovigilance in German, French, and Japanese: Annotating Adverse Drug Reactions across Languages](https://arxiv.org/abs/2403.18336) | 本研究构建了一个多语言的药物不良反应语料库，包括德语、法语和日语文本，包含了12种实体类型、四种属性类型和13种关系类型的注释，为医疗保健领域开发现实世界多语言语言模型做出贡献。 |
| [^38] | [Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications](https://arxiv.org/abs/2403.18327) | 本文提出了一种方法，利用两个LLM的副本与验证器结合使用，能够自动评估其在自然语言描述和正式规范之间转换的能力，无需额外的人工输入。 |
| [^39] | [Chinese Offensive Language Detection:Current Status and Future Directions](https://arxiv.org/abs/2403.18314) | 总体而言，这篇论文讨论了在中文中检测 offensive 语言的挑战，并强调了开发解决这一问题的特定模型和工具。 |
| [^40] | [Dual Instruction Tuning with Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2403.18295) | 提出了双重指令调整策略，通过引入中间推理状态预测任务和指令重构任务，从前向和后向两个方向精密建模数学推理，以增强LLMs对指令的理解和执行。 |
| [^41] | [Few-Shot Recalibration of Language Models](https://arxiv.org/abs/2403.18286) | 提出了一种新框架，用于少样本特定切片重校准语言模型，实现在任意分布片段上获得校准的置信度估计。 |
| [^42] | [BlendX: Complex Multi-Intent Detection with Blended Patterns](https://arxiv.org/abs/2403.18277) | BlendX提出了一套精制数据集，采用多样化模式、基于规则的启发式方法和OpenAI的ChatGPT生成工具，同时引入了新的评估指标来提高多意图检测的质量。 |
| [^43] | [RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era of Transformers](https://arxiv.org/abs/2403.18276) | Mamba模型基于状态空间模型，在多个序列建模任务中取得了与Transformer相当的性能，并在经典信息检索任务--文档排名中展现了其有效性。 |
| [^44] | [Toward Interactive Regional Understanding in Vision-Large Language Models](https://arxiv.org/abs/2403.18260) | 该研究提出了被称为RegionVLM的模型，具备显式的区域建模能力，通过引入新的信息源Local Narratives，设计简洁而创新的架构，实现了交互式区域理解，取得了优越性能。 |
| [^45] | [MD-PK: Metaphor Detection via Prompt Learning and Knowledge Distillation](https://arxiv.org/abs/2403.18253) | 引入知识蒸馏和提示学习的方法解决隐喻检测中的语言规则应用和数据稀疏性问题，通过提示信息和软标签优化学生模型，提高隐喻检测的准确性。 |
| [^46] | [Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models](https://arxiv.org/abs/2403.18252) | 提出了Visual Table，一种为MLLMs量身定制的新型视觉表示，通过提供层次化文本描述的全面视觉场景来弥补现有视觉表示的不足。 |
| [^47] | [Since the Scientific Literature Is Multilingual, Our Models Should Be Too](https://arxiv.org/abs/2403.18251) | 论文指出科学文献多语言特性，主张当前模型和基准应反映语言多样性，提出改善非英语文档性能的建议。 |
| [^48] | [Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges](https://arxiv.org/abs/2403.18249) | 本研究提出了一种强假新闻攻击方法VLPrompt，通过消除对额外数据收集的需求，同时保持语境一致性和原始文本的复杂性，可以更好地缩小LLM生成虚假新闻的欺骗力差距。 |
| [^49] | [ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech Corpus](https://arxiv.org/abs/2403.18182) | ZAEBUC-Spoken是一个多语言多方言的阿拉伯语-英语语音语料库，其中包含阿拉伯语和英语的多种变体以及两种语言之间的代码切换，为自动语音识别提供了挑战性的数据集。 |
| [^50] | [Mechanisms of non-factual hallucinations in language models](https://arxiv.org/abs/2403.18167) | 研究揭示了语言模型中非事实性幻觉的两个通用机制：主题属性知识不足和未能正确选择对象属性，这有助于深入理解和减轻幻觉。 |
| [^51] | [Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models](https://arxiv.org/abs/2403.18159) | 通过信号传播分析，提出了一种改进大型语言模型的量化知识蒸馏方法，并提供了ov-freeze稳定KD-QAT过程的洞察。 |
| [^52] | [Large Language Models as Financial Data Annotators: A Study on Effectiveness and Efficiency](https://arxiv.org/abs/2403.18152) | 大型语言模型在金融数据标注方面表现出显著的效率，可以作为非专家标注员的替代品，关键在于定制化提示和特定示例。 |
| [^53] | [Large Language Models Produce Responses Perceived to be Empathic](https://arxiv.org/abs/2403.18148) | 通过两项研究，发现大型语言模型生成的回复在共情性方面被认为比人类撰写的回复更具有共情性，这表明了在人际支持方面使用LLMs的潜力。 |
| [^54] | [Juru: Legal Brazilian Large Language Model from Reputable Sources](https://arxiv.org/abs/2403.18140) | Juru 模型通过从巴西法律来源提取的19亿个唯一标记，展示了领域专门化可以在减少预训练数据量方面发挥作用，但这种专门化会导致同一语言中其他知识领域性能下降。 |
| [^55] | [For those who don't know (how) to ask: Building a dataset of technology questions for digital newcomers](https://arxiv.org/abs/2403.18125) | 构建了一个针对数字新手的技术问题数据集，以应对大型语言模型无法处理的词汇或概念障碍，为解决不清晰或非标准语言查询对模型输出影响问题提供基础。 |
| [^56] | [ChatGPT Role-play Dataset: Analysis of User Motives and Model Naturalness](https://arxiv.org/abs/2403.18121) | 研究调查了ChatGPT在不同对话环境下的行为，介绍并分析了角色扮演数据集，突出了用户与ChatGPT交互时的动机多样性和AI自然度的变化，为改进人机交流有效性提供了新思路。 |
| [^57] | [Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization](https://arxiv.org/abs/2403.18120) | 通过将非正式的数学陈述翻译为形式的Isabelle代码并进行自动验证，我们提供了一种机制，可以自动拒绝在内部一致性方面与形式化问题陈述不一致的解决方案。 |
| [^58] | [Large Language Models for Education: A Survey and Outlook](https://arxiv.org/abs/2403.18105) | 大型语言模型在教育领域的应用调研总结了LLMs在教育中的各种技术应用，包括学生和教师辅助、自适应学习和商业工具，提出了未来研究机会和潜在方向。 |
| [^59] | [GPTs and Language Barrier: A Cross-Lingual Legal QA Examination](https://arxiv.org/abs/2403.18098) | 本研究探讨了GPT在跨语言法律问答领域的应用，通过分析英语和日语提示对性能的影响，为发展更高效准确的跨语言问答解决方案做出贡献。 |
| [^60] | [Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models](https://arxiv.org/abs/2403.18093) | 将提示技术作为检索系统的最后阶段，通过BM25预排序和基于BERT的重新排序的支持，可以显著提高法律文件检索的准确性。 |
| [^61] | [Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer](https://arxiv.org/abs/2403.18063) | 该论文提出了光谱卷积变压器 (SCT)，通过结合局部信息的卷积操作和全局信息的复杂傅里叶基础，实现了对视觉变压器中实部和复部多视图光谱算子的协调，从而实现了更好的性能。 |
| [^62] | [COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning](https://arxiv.org/abs/2403.18058) | COIG-CQIA 是一个高质量的中文指令微调数据集，旨在构建一个多样化、广泛的指令微调数据集，以更好地使模型行为与人类交互保持一致。 |
| [^63] | [Supervisory Prompt Training](https://arxiv.org/abs/2403.18051) | 提出了一种督导提示训练（SPT）方法，利用双LLM系统生成高效提示并引入影响分数概念，通过优化提示成功提高了LLMs的性能。 |
| [^64] | [The Impact of Syntactic and Semantic Proximity on Machine Translation with Back-Translation](https://arxiv.org/abs/2403.18031) | 机器翻译中，语义接近对反向翻译的影响很重要，推测跨语种平行的语义依赖性是无监督神经机器翻译成功的关键。 |
| [^65] | [Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER](https://arxiv.org/abs/2403.18025) | 提出了Mask Specific Language Modeling（MSLM）方法来改善LM在微调过程中对目标领域知识的敏感性，通过加权领域特定术语的重要性进行学习。 |
| [^66] | [Enriching Word Usage Graphs with Cluster Definitions](https://arxiv.org/abs/2403.18024) | 通过聚类定义，将现有多种语言的词语使用图谱WUGs进行丰富，人类评估表明这些定义更好地匹配WUGs中的簇，对于可解释的语义变化建模非常有帮助。 |
| [^67] | [DORE: A Dataset For Portuguese Definition Generation](https://arxiv.org/abs/2403.18018) | DORE是第一个用于葡萄牙语的定义生成数据集，填补了这一领域的空白，包含超过10万个定义，并评估了多种基于深度学习的模型。 |
| [^68] | [Intrinsic Subgraph Generation for Interpretable Graph based Visual Question Answering](https://arxiv.org/abs/2403.17647) | 该论文介绍了一种用于图像问答的可解释方法，通过内在生成子图来提供决策洞察，并在GQA数据集上取得了竞争性能。 |
| [^69] | [Mix-Initiative Response Generation with Dynamic Prefix Tuning](https://arxiv.org/abs/2403.17636) | 提出了一种使用动态前缀调整的混合倡议响应生成框架，解决了对话系统中的交叉污染问题，并能够在监督和非监督设置下学习倡议感知前缀。 |
| [^70] | [Language Models are Free Boosters for Biomedical Imaging Tasks](https://arxiv.org/abs/2403.17343) | 本研究揭示了基于残差的大型语言模型在生物医学成像任务中作为编码器的意想不到的有效性，利用冻结的变压器块进行直接处理视觉令牌，从而提高各种生物医学成像应用的性能。 |
| [^71] | [Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language](https://arxiv.org/abs/2403.17143) | 本文应用引导远程监督方法，为德语创建了最大的传记关系抽取数据集，同时发布了手动标注的评估数据集。 |
| [^72] | [Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models](https://arxiv.org/abs/2403.16915) | 本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调，在专题文档检索中显著改善了效果。 |
| [^73] | [Visually Guided Generative Text-Layout Pre-training for Document Intelligence](https://arxiv.org/abs/2403.16516) | 提出了一种名为ViTLP的可视引导的生成文本布局预训练技术，能够处理任意长度的词汇密集型文档，并且可以作为OCR模型用于文本定位和识别。 |
| [^74] | [LLMs Are Few-Shot In-Context Low-Resource Language Learners](https://arxiv.org/abs/2403.16512) | 该研究对25种低资源语言和7种相对较高资源语言上的情境学习（ICL）及其跨语言变体进行了研究，发现了在低资源语言中使用LLMs进行ICL的有效性，提出了替代方法查询对齐，并为低资源语言的ICL提供了宝贵见解。 |
| [^75] | [$\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models](https://arxiv.org/abs/2403.16432) | 基于提示的语言模型的优化过程揭示了生成对抗提示以误导模型的见解，引发了对该范式对抗性脆弱性的担忧。 |
| [^76] | [Centered Masking for Language-Image Pre-Training](https://arxiv.org/abs/2403.15837) | 使用中心掩蔽的GLIP技术在语言-图像预训练中取代了随机掩蔽，利用高斯分布提高了性能，并且易于获得且适用于不具有明显中心焦点的数据集。 |
| [^77] | [The opportunities and risks of large language models in mental health](https://arxiv.org/abs/2403.14814) | 大型语言模型在心理健康领域有望提供新颖的解决方案，但应注意其应用可能带来的风险，并积极采取策略减轻这些风险。 |
| [^78] | [X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment](https://arxiv.org/abs/2403.11399) | 提出了两种成本有效的方法解决大规模多模态模型训练数据的挑战，并在英语-韩语-中文多语言、多模态训练数据集上开发了表现优越的双语多模态模型。 |
| [^79] | [Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants' API Invocation Capabilities](https://arxiv.org/abs/2403.11128) | 提出了自动动态评估（AutoDE）方法，用于评估人工智能助手的API调用能力，避免静态评估中导致的误导性评估。 |
| [^80] | [Sabi\'a-2: A New Generation of Portuguese Large Language Models](https://arxiv.org/abs/2403.09887) | Sabi'a-2是一代新的葡萄牙大型语言模型，其中的Sabi'a-2 Medium模型在多个考试中的表现超越了GPT-4，且在大多数考试中超过了GPT-3.5，同时专业化对模型的性能有显著影响，可在无需增大模型尺寸的情况下以比GPT-4便宜10倍的价格提供。 |
| [^81] | [ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text](https://arxiv.org/abs/2403.09131) | ProSwitch通过知识引导的指令微调，在专业和非专业风格之间生成文本，并在专业性评估和质量评估方面表现出优越性。 |
| [^82] | [NLPre: a revised approach towards language-centric benchmarking of Natural Language Preprocessing systems](https://arxiv.org/abs/2403.04507) | NLPre提出了一种可靠且公平的语言中心基准测试方法，使得可以全面持续评估多个NLPre工具的性能，并可靠地跟踪其表现。 |
| [^83] | [NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models](https://arxiv.org/abs/2403.03100) | NaturalSpeech 3利用分解设计的扩散模型实现零-shot方式生成自然语音 |
| [^84] | [SoftTiger: A Clinical Foundation Model for Healthcare Workflows](https://arxiv.org/abs/2403.00868) | SoftTiger是一个专为医疗工作流设计的临床大型语言模型，通过处理临床笔记的结构化，实现了基本临床任务以及更复杂的下游临床任务的执行。 |
| [^85] | [Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization](https://arxiv.org/abs/2402.17574) | Agent-Pro提出了一种基于LLM的代理，通过策略级别的反思和优化，可以从互动经验中学习并逐步提升其行为策略。 |
| [^86] | [Probing Multimodal Large Language Models for Global and Local Semantic Representation](https://arxiv.org/abs/2402.17304) | 通过研究发现，多模态大型语言模型的中间层能够更好地编码全局语义信息，在视觉-语言任务中表现出更好的性能。顶层可能过多关注局部信息，导致理解全局信息的能力下降。 |
| [^87] | [Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2402.15764) | PEP提出了一个新方法来改善LLMs的数学能力，通过在推理之前细化和阐明问题背景，提升全局上下文建模能力，减少解析困难。 |
| [^88] | [Structure Guided Large Language Model for SQL Generation](https://arxiv.org/abs/2402.13284) | 通过引入结构信息，提出了一个结构引导的SQL生成模型，以改善大型语言模型生成SQL的准确性和可执行性。 |
| [^89] | [Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism](https://arxiv.org/abs/2402.12997) | 提出了一种适用于现实约束的轻量级弃权机制，特别适用于再排序阶段，通过数据驱动的方法达到有效性，并提供了开源代码以促进其更广泛的应用。 |
| [^90] | [GPT-4's assessment of its performance in a USMLE-based case study](https://arxiv.org/abs/2402.09654) | 本研究探讨了GPT-4在医疗应用中的表现评估。实验结果表明，反馈对相对置信度有影响，但并不一致地增加或减少。 |
| [^91] | [Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283) | 这篇调查提供了LLM对话安全性的全面概述，涵盖了攻击、防御和评估三个关键方面，旨在提高对该主题的理解并促进进一步的研究。 |
| [^92] | [OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2402.01739) | OpenMoE是一种开源的混合专家语言模型，通过训练和发布一系列具有可复现性的解码器模型，我们确认了MoE模型相比密集模型具有更有利的成本效益平衡，并且进行了对路由机制的深入分析，得出了三个重要发现。 |
| [^93] | [Batched Low-Rank Adaptation of Foundation Models](https://arxiv.org/abs/2312.05677) | 提出了Fast LoRA（FLoRA）框架，使得基础模型的低秩调整可以高效批处理异构请求，并在绩效上保持竞争性。 |
| [^94] | [PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models](https://arxiv.org/abs/2311.08590) | PEMA是一种参数有效的微调方法，通过插件外部内存自适应实现了对预训练语言模型的微调，绕过了对所有权重的访问需求，同时利用外部内存和适配器权重矩阵来提高效率。 |
| [^95] | [A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://arxiv.org/abs/2311.08268) | 提出一种利用大型语言模型自动生成有效的越狱提示的自动框架ReNeLLM，显著提高攻击成功率，同时大大减少时间成本。 |
| [^96] | [LLatrieval: LLM-Verified Retrieval for Verifiable Generation](https://arxiv.org/abs/2311.07838) | 可验证生成中检索的文件不仅帮助LLM生成正确答案，还作为用户验证LLM输出的证据，但目前广泛使用的检索器已成为性能瓶颈，需要解决。 |
| [^97] | [A Few-Shot Learning Focused Survey on Recent Named Entity Recognition and Relation Classification Methods](https://arxiv.org/abs/2310.19055) | 调研了最近应用于命名实体识别和关系分类的深度学习模型中少样本学习的性能表现。 |
| [^98] | [From Text to Source: Results in Detecting Large Language Model-Generated Content](https://arxiv.org/abs/2309.13322) | 本研究探讨了跨模型检测方法，在评估分类器是否能检测来自目标LLM的文本方面取得了关键发现 |
| [^99] | [GlotScript: A Resource and Tool for Low Resource Writing System Identification](https://arxiv.org/abs/2309.13320) | GlotScript是一种用于低资源书写系统识别的资源和工具，提供了7000多种语言的经过验证的书写系统，同时涵盖了所有161种Unicode 15.0脚本，并能帮助清理多语种语料库以及分析不同语言模型对低资源脚本和语言的覆盖。 |
| [^100] | [Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks](https://arxiv.org/abs/2305.14965) | 该论文提出了正式化和已知越狱分类法以填补对商用大规模语言模型（LLMs）被越狱攻击的缺乏研究，调查现有的越狱方法及其在开源和商用LLMs上的有效性。 |
| [^101] | [Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?.](http://arxiv.org/abs/2401.12492) | 本研究比较了以群体属性、个体用户和组合方法来模拟人的上下文。合并群体和个体特征显著提高了用户级回归任务的性能，而模拟个体用户则显著提高了单个文档级分类任务的性能。 |
| [^102] | [Few-Shot Detection of Machine-Generated Text using Style Representations.](http://arxiv.org/abs/2401.06712) | 本研究提出了一种小样本检测方法，通过使用样式表示来检测机器生成的文本与人类撰写的文本的区别，以解决滥用语言模型带来的问题。 |
| [^103] | [EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction.](http://arxiv.org/abs/2401.06201) | EASYTOOL是一个将多样化而冗长的工具文档转化为统一而简洁的工具指示的框架，用于增强基于LLM的代理的能力。通过从多个来源提取关键信息，并提供标准化的工具描述和功能，EasyTool显著降低了标记消耗，并提高了在真实场景中的工具利用性能。 |
| [^104] | [Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives.](http://arxiv.org/abs/2401.02009) | 自我对比是一种通过对比不同求解视角和总结差异，提高大型语言模型（LLM）的反思能力的方法。 |
| [^105] | [Retrieval-Augmented Generation for Large Language Models: A Survey.](http://arxiv.org/abs/2312.10997) | 本综述论文调查了基于检索增强的大型语言模型的发展，包括三个主要范式：Naive RAG、Advanced RAG和Modular RAG。RAG通过整合外部数据库的知识，增强模型的准确性和可信度，并实现了持续更新知识和整合领域特定信息的功能。 |
| [^106] | [CARE: Co-Attention Network for Joint Entity and Relation Extraction.](http://arxiv.org/abs/2308.12531) | 本论文提出了一种用于联合实体和关系抽取的共同注意力网络（CARE），通过学习分离表示和双向交互来解决特征混淆和子任务交互不足的问题，实验表明该模型在多个基准数据集上具有优越性能。 |
| [^107] | [NLP-based detection of systematic anomalies among the narratives of consumer complaints.](http://arxiv.org/abs/2308.11138) | 本文开发了一种基于自然语言处理的方法，用于检测消费者投诉叙述中的系统异常。这种方法可以解决分类算法对于较小且频繁出现的系统异常检测的问题，并将投诉叙述转化为定量数据进行分析。 |
| [^108] | [\`{I}r\`{o}y\`{i}nSpeech: A multi-purpose Yor\`{u}b\'{a} Speech Corpus.](http://arxiv.org/abs/2307.16071) | 本论文介绍了IróyìnSpeech语料库，这是一个多功能的约鲁巴语语音语料库，可用于TTS和ASR任务，包含38.5小时的数据，由80名志愿者录制。 |
| [^109] | [Good, but not always Fair: An Evaluation of Gender Bias for three commercial Machine Translation Systems.](http://arxiv.org/abs/2306.05882) | 本文对三个商业机器翻译系统进行了细致评估，在关注性别翻译和偏见的前提下，揭示了它们性别翻译方面的显着差异和偏见，这一点不会因为它们整体翻译质量的好坏而有所改变。 |
| [^110] | [ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems.](http://arxiv.org/abs/2306.04357) | 本研究提出了一种针对对话响应选择的后训练技术Dial-MAE，利用生成方法更好地压缩对话语义至密集向量，并提高对话响应选择准确性。 |
| [^111] | [Sentiment Analysis in Finance: From Transformers Back to eXplainable Lexicons (XLex).](http://arxiv.org/abs/2306.03997) | 本论文提出了一种名为XLex的新方法，它结合了基于词典和Transformer模型的优点，可以在金融情感分析中提供高效且可解释的结果。 |
| [^112] | [Improving Language Models with Advantage-based Offline Policy Gradients.](http://arxiv.org/abs/2305.14718) | 本文介绍了一种简单的训练算法Left-over Lunch RL （LoL-RL），使用离线策略梯度学习任何序列到序列数据，从而实现优化LM效用的方法。 |
| [^113] | [ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review.](http://arxiv.org/abs/2305.03123) | 本文研究关注ChatGPT面临的可持续性、隐私、数字鸿沟和伦理问题，提出了SPADE评估的必要性，并给出了缓解和建议。 |
| [^114] | [Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space.](http://arxiv.org/abs/2305.02151) | 探索了语言特征对多语言表示空间中的跨语言传递性能的影响，初步提供了方法以增强对语言上相距较远的语言的传递能力。 |
| [^115] | [InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual Topic Modeling.](http://arxiv.org/abs/2304.03544) | 本文提出了一种基于互信息最大化的跨语言主题建模方法InfoCTM，通过主题对齐方法规范化主题生成并寻找更多链接的跨语言词汇，有效解决了重复主题和低覆盖字典的问题。 |
| [^116] | [HIVE: Harnessing Human Feedback for Instructional Visual Editing.](http://arxiv.org/abs/2303.09618) | 本文提出了一种新的框架，利用人类反馈进行指导性视觉编辑。通过收集被编辑图像的人类反馈，并学习奖励函数捕捉用户的偏好，可以缓解数据限制所带来的偏差，并提高模型性能。 |
| [^117] | [Adapting Prompt for Few-shot Table-to-Text Generation.](http://arxiv.org/abs/2302.12468) | 这项研究提出了一个新的框架AdaPTGen，通过将领域特定知识的提示模板调整为模型所需，来解决缺乏标注数据的限制。该框架注入了常规表格相关描述的表示，充分利用未标记的领域特定知识，并允许设计各种任务来探索领域特定知识。 |
| [^118] | [Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram.](http://arxiv.org/abs/2301.10856) | 本文研究了16个俄罗斯媒体机构和732个电报频道之间的互动，发现新闻媒体不仅通过电报传播现有的叙事，而且会从电报平台源材料，研究结果表明2.3％至26.7％的文章将主题归因于电报活动。 |

# 详细

[^1]: 跨块量化：用于大型语言模型的跨块量化方法

    CBQ: Cross-Block Quantization for Large Language Models

    [https://rss.arxiv.org/abs/2312.07950](https://rss.arxiv.org/abs/2312.07950)

    CBQ是一种用于大型语言模型的跨块重构型后训练量化方法。CBQ通过使用同源重构方案来建立块间的长程依赖关系，最小化误差积累。CBQ还采用了粗到精的预处理策略和自适应的取整技术，使其能够有效处理极端异常值并提高整体量化精度。

    

    后训练量化（PTQ）在以极低成本压缩大型语言模型（LLM）方面起着重要作用。然而，现有的PTQ方法只关注处理单个层或单个块内的异常值，忽略了块之间的依赖关系，在低位设置中导致严重的性能下降。本文提出了一种基于块间重构的跨块PTQ方法CBQ。CBQ采用了一种同源重构方案来实现块间的长程依赖关系，以最小化误差积累。此外，CBQ还结合了一种粗到精的预处理策略（CFP）来抑制权重和激活值的异常值，并配合一种自适应的LoRA取整技术实现精确的权重量化。这些创新使CBQ不仅能够有效处理极端异常值，还能提高整体量化精度。广泛的实验证明，CBQ在低位量化（W4A4，W4A8等）方面具有优越性能。

    Post-training quantization (PTQ) has played a key role in compressing large language models (LLMs) with ultra-low costs. However, existing PTQ methods only focus on handling the outliers within one layer or one block, which ignores the dependency of blocks and leads to severe performance degradation in low-bit settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. CBQ employs a cross-block dependency using a homologous reconstruction scheme, establishing long-range dependencies across multiple blocks to minimize error accumulation. Furthermore, CBQ incorporates a coarse-to-fine preprocessing (CFP) strategy for suppressing weight and activation outliers, coupled with an adaptive LoRA-Rounding technique for precise weight quantization. These innovations enable CBQ to not only handle extreme outliers effectively but also improve overall quantization accuracy. Extensive experiments show that CBQ achieves superior low-bit quantization (W4A4, W4A8, W
    
[^2]: Mini-Gemini: 挖掘多模态视觉语言模型的潜力

    Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models

    [https://arxiv.org/abs/2403.18814](https://arxiv.org/abs/2403.18814)

    Mini-Gemini 挖掘了VLMs的潜力，通过高分辨率视觉标记、高质量数据和VLM引导生成等方式，缩小了与先进模型之间的性能差距

    

    在这项工作中，我们介绍了Mini-Gemini，这是一个简单而有效的框架，可以增强多模态视觉语言模型（VLMs）。尽管VLMs在促进基本视觉对话和推理方面取得了进展，但与GPT-4和Gemini等先进模型相比仍存在性能差距。我们试图通过从高分辨率视觉标记、高质量数据和VLM引导生成三个方面挖掘VLMs的潜力，以缩小这一差距。为了增强视觉标记，我们提出利用额外的视觉编码器进行高分辨率细化，而不增加视觉标记数量。我们进一步构建了一个高质量数据集，促进精确的图像理解和基于推理的生成，拓展了当前VLMs的操作范围。总的来说，Mini-Gemini进一步挖掘了VLMs的潜力，并赋予当前框架图像理解、推理和生成的能力。

    arXiv:2403.18814v1 Announce Type: cross  Abstract: In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared to advanced models like GPT-4 and Gemini. We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i.e., high-resolution visual tokens, high-quality data, and VLM-guided generation. To enhance visual tokens, we propose to utilize an additional visual encoder for high-resolution refinement without increasing the visual token count. We further construct a high-quality dataset that promotes precise image comprehension and reasoning-based generation, expanding the operational scope of current VLMs. In general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, reasoning, and gen
    
[^3]: 模块化可迁移吗？以知识蒸馏视角的案例研究

    Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation

    [https://arxiv.org/abs/2403.18804](https://arxiv.org/abs/2403.18804)

    本研究通过知识蒸馏的视角，探讨了模块化方法在不同模型之间的可迁移性，提出了一种简单直接的方法，将预训练的任务特定模块在同系列PLM之间进行转移。

    

    深度学习模块化的兴起展示了它在各种自然语言处理应用中的潜力。参数高效微调（PEFT）模块化已被证明适用于各种用例，从领域自适应到多语言设置。然而，所有这项工作涵盖了模块组件在单个预训练语言模型（PLM）内训练和部署的情况。这种特定于模型的设置对模块化架构试图实现的模块化构成了实质性限制。我们探讨了当前模块化方法是否可以在模型之间可迁移，以及我们是否可以将更强大和更大的PLM中的模块转移到更小的PLM中。在这项工作中，我们旨在通过常用于模型压缩的知识蒸馏镜头填补这一空白，并提出了一种非常直观的方法，将预训练的任务特定PEFT模块在同系列PLM之间进行转移。

    arXiv:2403.18804v1 Announce Type: new  Abstract: The rise of Modular Deep Learning showcases its potential in various Natural Language Processing applications. Parameter-efficient fine-tuning (PEFT) modularity has been shown to work for various use cases, from domain adaptation to multilingual setups. However, all this work covers the case where the modular components are trained and deployed within one single Pre-trained Language Model (PLM). This model-specific setup is a substantial limitation on the very modularity that modular architectures are trying to achieve. We ask whether current modular approaches are transferable between models and whether we can transfer the modules from more robust and larger PLMs to smaller ones. In this work, we aim to fill this gap via a lens of Knowledge Distillation, commonly used for model compression, and present an extremely straightforward approach to transferring pre-trained, task-specific PEFT modules between same-family PLMs. Moreover, we pro
    
[^4]: 用于缓解预训练语言模型性别偏见的投影方法

    Projective Methods for Mitigating Gender Bias in Pre-trained Language Models

    [https://arxiv.org/abs/2403.18803](https://arxiv.org/abs/2403.18803)

    研究探讨了将用于静态词嵌入的最简单投影去偏置方法应用于BERT内部表示的效果，发现这种方法既可以减少内在偏见，又可以缓解下游任务中观察到的偏见。

    

    在自然语言处理中缓解性别偏见的方法与去偏置静态词嵌入有着悠久的历史。最近，注意力转向了去偏置预训练语言模型。本文研究了最简单的投影去偏置方法在应用于BERT内部表示时能够帮助到什么程度。投影方法实现快速，使用少量保存的参数，并且不对现有模型参数进行更新。我们评估了这些方法在减少BERT对内在偏见的效果，通过BERT下一个句子预测任务进行测量，以及在微调下游任务时缓解观察到的偏见。为此，我们还对用于量化内在偏见的流行性别偏见评估测试进行了批判性分析，从而得到了一个增强的测试集和新的偏见测量。我们发现投影方法在减少内在偏见和缓解下游偏见方面都可以取得良好效果。

    arXiv:2403.18803v1 Announce Type: new  Abstract: Mitigation of gender bias in NLP has a long history tied to debiasing static word embeddings. More recently, attention has shifted to debiasing pre-trained language models. We study to what extent the simplest projective debiasing methods, developed for word embeddings, can help when applied to BERT's internal representations. Projective methods are fast to implement, use a small number of saved parameters, and make no updates to the existing model parameters. We evaluate the efficacy of the methods in reducing both intrinsic bias, as measured by BERT's next sentence prediction task, and in mitigating observed bias in a downstream setting when fine-tuned. To this end, we also provide a critical analysis of a popular gender-bias assessment test for quantifying intrinsic bias, resulting in an enhanced test set and new bias measures. We find that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but 
    
[^5]: 大型语言模型中的长篇事实性

    Long-form factuality in large language models

    [https://arxiv.org/abs/2403.18802](https://arxiv.org/abs/2403.18802)

    该论文提出了一种通过使用大型语言模型将长篇回应分解为单个事实，并通过发送搜索查询到Google搜索，评估事实准确性的方法，并扩展了F1分数作为长篇事实性的聚合度量。

    

    大型语言模型（LLMs）在回答开放性主题的事实性提示时，经常生成包含事实错误的内容。为了在开放领域中对模型的长篇事实性进行基准测试，我们首先使用GPT-4生成了一个名为LongFact的提示集，其中包含数千个囊括38个主题的问题。然后，我们提出LLM代理可以通过一种名为Search-Augmented Factuality Evaluator（SAFE）的方法作为长篇事实性的自动评估器。SAFE利用LLM将长篇回应分解为一组单独的事实，并通过发送搜索查询到Google搜索以及确定一个事实是否得到搜索结果支持的多步推理过程来评估每个事实的准确性。此外，我们还提议将F1分数扩展为长篇事实性的聚合度量。为此，我们平衡了回应中支持事实的百分比（精度）与

    arXiv:2403.18802v1 Announce Type: cross  Abstract: Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the 
    
[^6]: 为设备上的虚拟助手打造世界英语语言模型

    Towards a World-English Language Model for On-Device Virtual Assistants

    [https://arxiv.org/abs/2403.18783](https://arxiv.org/abs/2403.18783)

    该论文提出了为设备上的虚拟助手打造世界英语语言模型的方法，通过结合英语的区域变体，采用适配器瓶颈来模拟特定方言特征，实现了准确性、延迟和内存方面的性能平衡。

    

    虚拟助手（VAs）的神经网络语言模型（NNLMs）通常是基于语言、地区，有时甚至还有设备的依赖性，这增加了扩展和维护的工作量。将NNLMs组合用于一个或多个类别是改善扩展性的一种方式。在这项工作中，我们结合区域性英语变体构建了一个“世界英语”NNLM，用于设备上的VAs。特别是，我们研究了适配器瓶颈的应用，以模拟我们现有生产NNLMS中的特定方言特征{并增强多方言基线}。我们发现适配器模块在模拟方言方面比专门化整个子网络更有效。基于这一见解，并利用我们生产模型的设计，我们介绍了一个新的世界英语NNLM架构，符合我们单方言模型的准确性、延迟和内存约束。

    arXiv:2403.18783v1 Announce Type: new  Abstract: Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are generally language-, region-, and in some cases, device-dependent, which increases the effort to scale and maintain them. Combining NNLMs for one or more of the categories is one way to improve scalability. In this work, we combine regional variants of English to build a ``World English'' NNLM for on-device VAs. In particular, we investigate the application of adapter bottlenecks to model dialect-specific characteristics in our existing production NNLMs {and enhance the multi-dialect baselines}. We find that adapter modules are more effective in modeling dialects than specializing entire sub-networks. Based on this insight and leveraging the design of our production models, we introduce a new architecture for World English NNLM that meets the accuracy, latency, and memory constraints of our single-dialect models.
    
[^7]: CheckEval: 使用大型语言模型通过清单构建健壮评估框架

    CheckEval: Robust Evaluation Framework using Large Language Model via Checklist

    [https://arxiv.org/abs/2403.18771](https://arxiv.org/abs/2403.18771)

    CheckEval是一种使用大型语言模型构建的评估框架，通过详细的子方面和布尔问题清单简化了评估过程，增强了评估结果的稳健性和可靠性，通过SummEval基准验证其有效性。

    

    我们介绍了CheckEval，一种使用大型语言模型的新型评估框架，解决了当前评估方法中的歧义和不一致性挑战。CheckEval通过将评估标准分解为详细的子方面，并为每个构建布尔问题清单，简化了评估过程。这种方法不仅使过程更具可解释性，还通过专注于特定的评估维度显着增强了结果的稳健性和可靠性。通过使用SummEval基准进行的专注案例研究验证，CheckEval显示出与人类判断的强相关性。此外，它展示了高度一致的互注者一致性。这些发现突显了CheckEval在客观、灵活和精确评估方面的有效性。通过提供可定制和互动的框架，CheckEval为LL的使用设立了新的标准。

    arXiv:2403.18771v1 Announce Type: new  Abstract: We introduce CheckEval, a novel evaluation framework using Large Language Models, addressing the challenges of ambiguity and inconsistency in current evaluation methods. CheckEval addresses these challenges by dividing evaluation criteria into detailed sub-aspects and constructing a checklist of Boolean questions for each, simplifying the evaluation. This approach not only renders the process more interpretable but also significantly enhances the robustness and reliability of results by focusing on specific evaluation dimensions. Validated through a focused case study using the SummEval benchmark, CheckEval indicates a strong correlation with human judgments. Furthermore, it demonstrates a highly consistent Inter-Annotator Agreement. These findings highlight the effectiveness of CheckEval for objective, flexible, and precise evaluations. By offering a customizable and interactive framework, CheckEval sets a new standard for the use of LL
    
[^8]: 通过反射预测改进神经原型重建

    Improved Neural Protoform Reconstruction via Reflex Prediction

    [https://arxiv.org/abs/2403.18769](https://arxiv.org/abs/2403.18769)

    该论文提出通过反射来预测祖语的方法，从而改进了神经原型重建模型。

    

    Protolanguage重建对历史语言学至关重要。比较方法是语言科学史上最有影响力的理论和方法论框架之一，允许语言学家根据定期的声音变化的假设从反射关系的现代词中推断出原型形式（重建的祖先词）。众多计算语言学家试图通过各种计算模型实现比较重建，在这些模型中，最成功的是监督编码器-解码器模型，这些模型将在给定一组反射词的情况下，预测原型形式作为一个序列到序列问题。我们认为，这一框架忽略了比较方法中最重要的一个方面：不仅应该可以从同源词组（相关反射的集合）推断出原型形式，同时还应该能够从原型形式推断出反射。

    arXiv:2403.18769v1 Announce Type: new  Abstract: Protolanguage reconstruction is central to historical linguistics. The comparative method, one of the most influential theoretical and methodological frameworks in the history of the language sciences, allows linguists to infer protoforms (reconstructed ancestral words) from their reflexes (related modern words) based on the assumption of regular sound change. Not surprisingly, numerous computational linguists have attempted to operationalize comparative reconstruction through various computational models, the most successful of which have been supervised encoder-decoder models, which treat the problem of predicting protoforms given sets of reflexes as a sequence-to-sequence problem. We argue that this framework ignores one of the most important aspects of the comparative method: not only should protoforms be inferable from cognate sets (sets of related reflexes) but the reflexes should also be inferable from the protoforms. Leveraging a
    
[^9]: CYCLE: 学习自我调整代码生成

    CYCLE: Learning to Self-Refine the Code Generation

    [https://arxiv.org/abs/2403.18746](https://arxiv.org/abs/2403.18746)

    CYCLE框架提出了学习如何根据可用反馈，如测试套件报告的执行结果，自我调整错误生成的方法，成功地解决了代码语言模型无法自我调整的问题

    

    预训练的代码语言模型在代码生成中取得了有希望的表现，并提高了人类开发者的编程效率。然而，现有对代码语言模型的评估通常忽视了它们的自我调整能力，而是只关注一次性预测的准确性。在代码语言模型未能实现正确程序的情况下，开发者实际上很难调试和修复错误的预测，因为这些预测不是由开发者自己编写的。不幸的是，我们的研究表明，代码语言模型也无法高效地自我调整其错误的生成。

    arXiv:2403.18746v1 Announce Type: cross  Abstract: Pre-trained code language models have achieved promising performance in code generation and improved the programming efficiency of human developers. However, their self-refinement capability is typically overlooked by the existing evaluations of code LMs, which focus only on the accuracy of the one-time prediction. For the cases when code LMs fail to implement the correct program, developers actually find it hard to debug and fix the faulty prediction since it is not written by the developers themselves. Unfortunately, our study reveals that code LMs cannot efficiently self-refine their faulty generations as well.   In this paper, we propose CYCLE framework, learning to self-refine the faulty generation according to the available feedback, such as the execution results reported by the test suites. We evaluate CYCLE on three popular code generation benchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE successfully maintai
    
[^10]: 使用指示对比解码减轻大规模视觉-语言模型中的幻觉

    Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding

    [https://arxiv.org/abs/2403.18715](https://arxiv.org/abs/2403.18715)

    这项研究提出了一种名为指示对比解码(ICD)方法，旨在减少大规模视觉-语言模型(LVLMs)推断过程中的幻觉，通过对标准和指示扰动的分布进行对比，从原始分布中减去幻觉概念。

    

    大规模视觉-语言模型(LVLMs)越来越擅长从视觉输入生成具有上下文细节和连贯性的响应。然而，在多模式决策和开放式生成中应用它们时，其应用受到幻觉的阻碍，即生成的文本不准确地代表了视觉内容。为了解决这一问题，本文介绍了指示对比解码(ICD)方法，这是一种旨在在LVLM推断过程中减少幻觉的新方法。我们的方法受到我们观察到的扰动指示显著加剧多模态融合模块中的幻觉的启发。ICD对标准和指示扰动的分布进行对比，从而增加对齐不确定性并有效地从原始分布中减去幻觉概念。通过在判别基准(POPE和MME)和生成基准上进行全面实验

    arXiv:2403.18715v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) are increasingly adept at generating contextually detailed and coherent responses from visual inputs. However, their application in multimodal decision-making and open-ended generation is hindered by a notable rate of hallucinations, where generated text inaccurately represents the visual contents. To address this issue, this paper introduces the Instruction Contrastive Decoding (ICD) method, a novel approach designed to reduce hallucinations during LVLM inference. Our method is inspired by our observation that what we call disturbance instructions significantly exacerbate hallucinations in multimodal fusion modules. ICD contrasts distributions from standard and instruction disturbance, thereby increasing alignment uncertainty and effectively subtracting hallucinated concepts from the original distribution. Through comprehensive experiments on discriminative benchmarks (POPE and MME) and a generativ
    
[^11]: Invalsi基准：衡量语言模型在意大利语的数学和语言理解能力

    The Invalsi Benchmark: measuring Language Models Mathematical and Language understanding in Italian

    [https://arxiv.org/abs/2403.18697](https://arxiv.org/abs/2403.18697)

    该研究提出了两个新的基准，用于评估语言模型在意大利语的数学和语言理解能力，为当前语言模型的性能提供了具有挑战性的评估标准。

    

    尽管意大利语在所有指标上都是一种高资源语言，但目前并没有一种专门针对该语言进行预训练的语言模型。这导致了可用于评估意大利语语言模型性能的基准数目较少。本文提出了两个新的基准，用于评估模型在意大利语的数学理解和语言理解方面的性能。这些基准基于意大利学校系统内11至18岁学生进行的实际测试，并已由多位教学和教育学专家验证。

    arXiv:2403.18697v1 Announce Type: new  Abstract: While Italian is by all metrics a high resource language, currently, there are isn't a Language Model pre-trained exclusively in this language. This results in a lower number of available benchmarks to evaluate the performance of language models in Italian.   This work presents two new benchmarks to evaluate the models performance on mathematical understanding and language understanding in Italian. These benchmarks are based on real tests that are undertaken by students of age between 11 and 18 within the Italian school system and have therefore been validated by several experts in didactics and pedagogy.   To validate this dataset we evaluate the performance of 9 language models that are the best performing when writing in Italian, including our own fine-tuned models. We show that this is a challenging benchmark where current language models are bound by 60\% accuracy.   We believe that the release of this dataset paves the way for impr
    
[^12]: 密集检索的扩展规律

    Scaling Laws For Dense Retrieval

    [https://arxiv.org/abs/2403.18684](https://arxiv.org/abs/2403.18684)

    该研究探究了密集检索模型的性能是否遵循其他神经模型的缩放规律，并提出使用对比对数似然作为评估指标进行了广泛实验。

    

    将神经模型扩展到更大规模已经在多项任务中取得了显著进展，特别是在语言生成方面。先前的研究发现，神经模型的性能常遵循可预测的扩展规律，与训练集大小和模型大小等因素相关。这一洞察力非常宝贵，尤其是随着大规模实验变得越来越耗费资源。然而，由于检索指标的离散性以及检索任务中训练数据和模型大小之间的复杂关系，密集检索中的这种扩展规律尚未得到充分探讨。在本研究中，我们调查了密集检索模型的性能是否遵循其他神经模型的缩放规律。我们建议使用对比对数似然作为评估指标，并对实现了不同参数数量并使用不同数量的数据训练的密集检索模型进行了广泛实验。

    arXiv:2403.18684v1 Announce Type: cross  Abstract: Scaling up neural models has yielded significant advancements in a wide array of tasks, particularly in language generation. Previous studies have found that the performance of neural models frequently adheres to predictable scaling laws, correlated with factors such as training set size and model size. This insight is invaluable, especially as large-scale experiments grow increasingly resource-intensive. Yet, such scaling law has not been fully explored in dense retrieval due to the discrete nature of retrieval metrics and complex relationships between training data and model sizes in retrieval tasks. In this study, we investigate whether the performance of dense retrieval models follows the scaling law as other neural models. We propose to use contrastive log-likelihood as the evaluation metric and conduct extensive experiments with dense retrieval models implemented with different numbers of parameters and trained with different amo
    
[^13]: NL-ITI：优化探测和干预以改进ITI方法

    NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method

    [https://arxiv.org/abs/2403.18680](https://arxiv.org/abs/2403.18680)

    NL-ITI通过引入非线性探测和多令牌干预，成功改进了ITI方法，在多个多选基准上取得了显著的性能改进。

    

    大语言模型(LLM)容易返回虚假信息，这是人工智能领域的一个主要挑战。本文探讨了推理时干预(Inference-Time-Intervention, ITI)方法引入的范式。首先，ITI方法识别包含最多所需知识类型(例如真实信息)的注意力头。随后，在推理过程中，LLM激活被移动到所选注意力头的子集。我们通过引入非线性探测和多令牌干预-非线性ITI(NL-ITI)进一步改进了ITI框架。NL-ITI在多个多选基准上进行了测试，包括TruthfulQA，我们在这项基准上相对于基线ITI结果报告了约14%的MC1指标改进。NL-ITI还在其他测试集上取得了令人鼓舞的成绩-在MMLU的商业伦理子领域上，比基线LLaMA2-7B有约18%的MC1改进。此外，NL-ITI在效果更好的同时也更少侵入。

    arXiv:2403.18680v1 Announce Type: new  Abstract: Large Language Models (LLM) are prone to returning false information. It constitutes one of major challenges in the AI field. In our work, we explore paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it identifies attention heads, which contain the highest amount of desired type of knowledge (e.g., truthful). Afterwards, during inference, LLM activations are shifted for chosen subset of attention heads. We further improved the ITI framework by introducing a nonlinear probing and multi-token intervention - Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice benchmarks, including TruthfulQA, on which we report around 14% MC1 metric improvement with respect to the baseline ITI results. NL-ITI achieves also encouraging results on other testsets - on Business Ethics subdomain of MMLU, around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI performs better while being less invasive in t
    
[^14]: 超越训练集的事实核查

    Fact Checking Beyond Training Set

    [https://arxiv.org/abs/2403.18671](https://arxiv.org/abs/2403.18671)

    论文探讨了超越训练集的事实核查问题，通过提出新颖的对抗算法和训练方法，使事实核查流程在不同领域中更加稳健。

    

    评估日常言论的真实性耗时且在某些情况下需要领域专业知识。我们通过实证证明，常用的事实核查流程，即检索器-阅读器，在使用一个领域的标记数据进行训练后，当在另一个领域中使用时性能会下降。之后，我们深入研究了流程的每个组成部分，并提出了新颖的算法来解决这个问题。我们提出了一种对抗算法，使检索器组件能够抵御分布转移。我们的核心思想是首先在标记的源数据上训练一个双编码器，然后通过使用未标记的目标数据对两个独立的文档编码器和言论编码器进行对抗训练。然后我们专注于阅读器组件，并建议训练它以对言论和证据文档的顺序不敏感。我们的实证评估支持这样一个阅读器显示出更高的性能。

    arXiv:2403.18671v1 Announce Type: new  Abstract: Evaluating the veracity of everyday claims is time consuming and in some cases requires domain expertise. We empirically demonstrate that the commonly used fact checking pipeline, known as the retriever-reader, suffers from performance deterioration when it is trained on the labeled data from one domain and used in another domain. Afterwards, we delve into each component of the pipeline and propose novel algorithms to address this problem. We propose an adversarial algorithm to make the retriever component robust against distribution shift. Our core idea is to initially train a bi-encoder on the labeled source data, and then, to adversarially train two separate document and claim encoders using unlabeled target data. We then focus on the reader component and propose to train it such that it is insensitive towards the order of claims and evidence documents. Our empirical evaluations support the hypothesis that such a reader shows a higher
    
[^15]: 改进内容推荐：基于知识图的语义对比学习用于多样性和冷启动用户

    Improving Content Recommendation: Knowledge Graph-Based Semantic Contrastive Learning for Diversity and Cold-Start Users

    [https://arxiv.org/abs/2403.18667](https://arxiv.org/abs/2403.18667)

    提出了一种基于知识图的语义对比学习方法，通过混合多任务学习在用户-项目和项目-项目交互上进行训练，从而提供个性化和多样化的推荐。

    

    处理与数据稀疏性、冷启动问题和推荐系统多样性相关的挑战既至关重要又具有挑战性。许多当前的解决方案利用知识图来解决这些问题，结合基于项目和用户-项目协作信号。这些方法的一个普遍趋势是专注于提高排名性能，但会增加模型复杂性、降低多样性并使任务变得更复杂。提供既个性化又多样化的推荐是至关重要的，而不仅仅依赖于实现高排名性能，比如点击率、召回率等。本文提出了一种混合多任务学习方法，通过在用户-项目和项目-项目交互上进行训练。我们在描述性文本上应用基于项目的对比学习，根据项目元数据对正负样本进行抽样。我们的方法使模型能更好地理解关系

    arXiv:2403.18667v1 Announce Type: cross  Abstract: Addressing the challenges related to data sparsity, cold-start problems, and diversity in recommendation systems is both crucial and demanding. Many current solutions leverage knowledge graphs to tackle these issues by combining both item-based and user-item collaborative signals. A common trend in these approaches focuses on improving ranking performance at the cost of escalating model complexity, reducing diversity, and complicating the task. It is essential to provide recommendations that are both personalized and diverse, rather than solely relying on achieving high rank-based performance, such as Click-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task learning approach, training on user-item and item-item interactions. We apply item-based contrastive learning on descriptive text, sampling positive and negative pairs based on item metadata. Our approach allows the model to better understand the relationships 
    
[^16]: SDSAT：通过具有语义自适应令牌的推测解码加速LLM推断

    SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens

    [https://arxiv.org/abs/2403.18647](https://arxiv.org/abs/2403.18647)

    SDSAT提出了一种加速大型语言模型推断的方案，通过使用具有灵活解码能力的语义自适应令牌，可以增强模型生成高质量草稿令牌的能力，并实现超过3.5倍和3.0倍的速度提升。

    

    我们提出了一种用于大型语言模型（LLMs）的加速方案，通过具有语义自适应令牌（SDSAT）进行推测解码。该设计的主要目标是增强LLM模型生成草稿标记的能力，而不影响模型的准确性。核心策略包括：1）通过合并具有灵活解码能力的语义自适应令牌来微调模型，而不改变其结构，使其能够生成高质量的草稿标记。2）通过使用不影响标准令牌的训练方法，模型可以在其原始框架之上获得并行解码能力，而训练开销最小。3）我们设计了使用贪婪搜索和核采样的“两步起草然后验证”生成策略。在CodeLlama-13B和7B模型上进行的实验结果显示，速度分别提高了3.5倍和3.0倍以上。

    arXiv:2403.18647v1 Announce Type: new  Abstract: We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary objective of this design is to enhance the LLM model's ability to generate draft tokens more accurately without compromising the model's accuracy. The core strategies involve: 1) Fine-tune the model by incorporating semantic adaptive tokens that possess flexible decoding capabilities without changing its structure, allowing them to generate high-quality draft tokens. 2) By employing a training method that does not affect the standard tokens, the model can acquire parallel decoding abilities atop its original framework with minimal training overhead. 3) We have designed the "two-step-draft-then-verify" generation strategies using both greedy search and nucleus sampling. Experiments conducted on the CodeLlama-13B and 7B models have yielded speed increases of over 3.5X and 3.0X, respectively. Ple
    
[^17]: 使用代码语言模型进行漏洞检测：我们离目标有多远？

    Vulnerability Detection with Code Language Models: How Far Are We?

    [https://arxiv.org/abs/2403.18624](https://arxiv.org/abs/2403.18624)

    我们提出了一个新的数据集PrimeVul，用于训练和评估代码语言模型进行漏洞检测，通过引入一套新颖的数据标记技术，实现了与人工验证基准相当的标签准确性，显著扩大了数据集。

    

    在代码语言模型（code LMs）和漏洞检测备受关注的背景下，我们研究了代码语言模型用于检测漏洞的有效性。我们的分析揭示了现有漏洞数据集存在的重大缺陷，包括数据质量低、标签准确性差以及高重复率，导致在现实漏洞检测场景中模型性能不可靠。此外，这些数据集使用的评估方法也不能代表真实世界的漏洞检测情景。

    arXiv:2403.18624v1 Announce Type: cross  Abstract: In the context of the rising interest in code language models (code LMs) and vulnerability detection, we study the effectiveness of code LMs for detecting vulnerabilities. Our analysis reveals significant shortcomings in existing vulnerability datasets, including poor data quality, low label accuracy, and high duplication rates, leading to unreliable model performance in realistic vulnerability detection scenarios. Additionally, the evaluation methods used with these datasets are not representative of real-world vulnerability detection.   To address these challenges, we introduce PrimeVul, a new dataset for training and evaluating code LMs for vulnerability detection. PrimeVul incorporates a novel set of data labeling techniques that achieve comparable label accuracy to human-verified benchmarks while significantly expanding the dataset. It also implements a rigorous data de-duplication and chronological data splitting strategy to miti
    
[^18]: 有关尖峰神经膜系统和尖峰神经网络学习模型的调查

    A survey on learning models of spiking neural membrane systems and spiking neural networks

    [https://arxiv.org/abs/2403.18609](https://arxiv.org/abs/2403.18609)

    该论文调查了尖峰神经膜系统和尖峰神经网络学习模型的结构、功能、优缺点，并对机器学习和深度学习模型的最新应用进行了调查。

    

    尖峰神经网络（SNN）是一种受生物启发的神经网络模型，具有某些类似大脑的特性。在过去几十年里，由于深度学习的成功现象，这种模型在计算机科学界越来越受到关注。在SNN中，神经元之间的通信通过尖峰和尖峰列进行。这使得这些模型与“标准”人工神经网络（ANN）不同，后者将尖峰的频率替换为实值信号。尖峰神经P系统（SNPS）可以被认为是基于形式自动机原理更多的SNN分支，许多变体都在膜计算理论框架内发展。本文首先简要比较了SNN和SNPS的结构和功能、优缺点。文章的重点部分是调查了机器学习和深度学习模型的最新结果和应用。

    arXiv:2403.18609v1 Announce Type: cross  Abstract: Spiking neural networks (SNN) are a biologically inspired model of neural networks with certain brain-like properties. In the past few decades, this model has received increasing attention in computer science community, owing also to the successful phenomenon of deep learning. In SNN, communication between neurons takes place through the spikes and spike trains. This differentiates these models from the ``standard'' artificial neural networks (ANN) where the frequency of spikes is replaced by real-valued signals. Spiking neural P systems (SNPS) can be considered a branch of SNN based more on the principles of formal automata, with many variants developed within the framework of the membrane computing theory. In this paper, we first briefly compare structure and function, advantages and drawbacks of SNN and SNPS. A key part of the article is a survey of recent results and applications of machine learning and deep learning models of both
    
[^19]: 通过对比词对消除句子嵌入器中的偏差

    Debiasing Sentence Embedders through Contrastive Word Pairs

    [https://arxiv.org/abs/2403.18555](https://arxiv.org/abs/2403.18555)

    本文探讨了一种通过对比词对消除句子嵌入器中线性和非线性偏差信息的方法

    

    在过去的几年中，各种句子嵌入器已成为目前自然语言处理（NLP）机器学习方法成功的重要组成部分。不幸的是，多个来源已经表明，这些嵌入方法所训练的数据集中固有的偏差是由它们学习到的。文献中存在许多不同的方法来消除嵌入中的偏差，大多数这些方法适用于词嵌入，更少的情况适用于句子嵌入。问题在于，大多数去偏方法直接从词嵌入转移而来，因此这些方法未能考虑到句子嵌入器的非线性特性及其产生的嵌入。文献中已经表明，如果使用这种方法去除语句嵌入中的偏差，则偏差信息仍然存在。在本文中，我们探索了一种方法来消除线性和非线性偏差信息。

    arXiv:2403.18555v1 Announce Type: new  Abstract: Over the last years, various sentence embedders have been an integral part in the success of current machine learning approaches to Natural Language Processing (NLP). Unfortunately, multiple sources have shown that the bias, inherent in the datasets upon which these embedding methods are trained, is learned by them. A variety of different approaches to remove biases in embeddings exists in the literature. Most of these approaches are applicable to word embeddings and in fewer cases to sentence embeddings. It is problematic that most debiasing approaches are directly transferred from word embeddings, therefore these approaches fail to take into account the nonlinear nature of sentence embedders and the embeddings they produce. It has been shown in literature that bias information is still present if sentence embeddings are debiased using such methods. In this contribution, we explore an approach to remove linear and nonlinear bias informa
    
[^20]: 关注语境语义相关性预测中文句子阅读

    Attention-aware semantic relevance predicting Chinese sentence reading

    [https://arxiv.org/abs/2403.18542](https://arxiv.org/abs/2403.18542)

    提出了一种基于“关注语境”的方法，可以更准确地预测中文阅读任务中的凝视持续时间。

    

    近年来，已经提出了几种有影响力的计算模型和度量标准，用于预测人类如何理解和处理句子。本研究受Transformer中的注意力算法和人类记忆机制启发，提出了一种“关注语境”的方法来计算语境语义相关性。这种新方法考虑了语境部分的不同贡献和期望效应，使其能够充分整合语境信息。关注语境方法还有助于模拟现有的阅读模型并对其进行评估。最终得到的“关注语境”语义相关性度量标准比现有方法更准确地预测了记录在眼动追踪语料库中的中文阅读任务中凝视持续时间。该研究的发现进一步为现有的研究提供了有力支持。

    arXiv:2403.18542v1 Announce Type: new  Abstract: In recent years, several influential computational models and metrics have been proposed to predict how humans comprehend and process sentence. One particularly promising approach is contextual semantic similarity. Inspired by the attention algorithm in Transformer and human memory mechanisms, this study proposes an ``attention-aware'' approach for computing contextual semantic relevance. This new approach takes into account the different contributions of contextual parts and the expectation effect, allowing it to incorporate contextual information fully. The attention-aware approach also facilitates the simulation of existing reading models and evaluate them. The resulting ``attention-aware'' metrics of semantic relevance can more accurately predict fixation durations in Chinese reading tasks recorded in an eye-tracking corpus than those calculated by existing approaches. The study's findings further provide strong support for the prese
    
[^21]: 通往法律自治的路径：利用大语言模型、专家系统和贝叶斯网络提取、转换、加载和计算法律信息的可互操作和可解释方法

    A Path Towards Legal Autonomy: An interoperable and explainable approach to extracting, transforming, loading and computing legal information using large language models, expert systems and Bayesian networks

    [https://arxiv.org/abs/2403.18537](https://arxiv.org/abs/2403.18537)

    本文提出了一种可互操作和可解释的方法，利用大语言模型、专家系统和贝叶斯网络来提取、转换、加载和计算法律信息，从而实现通向法律自治的路径。

    

    法律自治 - 人工智能代理的合法活动 - 可以通过两种方式实现。一种方式是对AI行为者（如开发者、部署者和用户）和AI资源（如数据）施加约束，或者是对AI代理在环境中可能产生影响的范围和影响程度施加约束。后一种方法涉及将关于由AI驱动的设备的现有规则编码到控制这些设备的AI代理软件中（例如，将关于无人机设备操作范围限制的规则编码到无人机设备的代理软件中）。然而，这是一个挑战，因为这种方法的有效性需要一种提取、加载、转换和计算法律信息的方法，这种方法既可解释又可在法律上相互操作，并能让AI代理推理法律。本文中，我们将概述使用大型语言模型、专家系统和贝叶斯网络证明这种方法的原理。

    arXiv:2403.18537v1 Announce Type: new  Abstract: Legal autonomy - the lawful activity of artificial intelligence agents - can be achieved in one of two ways. It can be achieved either by imposing constraints on AI actors such as developers, deployers and users, and on AI resources such as data, or by imposing constraints on the range and scope of the impact that AI agents can have on the environment. The latter approach involves encoding extant rules concerning AI driven devices into the software of AI agents controlling those devices (e.g., encoding rules about limitations on zones of operations into the agent software of an autonomous drone device). This is a challenge since the effectivity of such an approach requires a method of extracting, loading, transforming and computing legal information that would be both explainable and legally interoperable, and that would enable AI agents to reason about the law. In this paper, we sketch a proof of principle for such a method using large 
    
[^22]: 语言在CLIP对象-属性组合泛化中发挥关键作用

    Language Plays a Pivotal Role in the Object-Attribute Compositional Generalization of CLIP

    [https://arxiv.org/abs/2403.18525](https://arxiv.org/abs/2403.18525)

    本研究发现，通过大型数据集训练的CLIP模型在对象-属性组合泛化中表现出明显优势，为泛化和数据集规模之间的关系提供了重要见解。

    

    arXiv:2403.18525v1 公告类型:跨领域 摘要:视觉-语言模型，如CLIP，在各种类型的分布变化下展现了令人鼓舞的超出分布的泛化能力。最近的研究尝试调查这种能力的主要原因。在这项工作中，我们沿用了相同的思路，但专注于特定类型的超出分布数据 - 具有新颖属性-对象对组合的图像 - 并研究这样的模型是否能够成功地将这些图像分类到组合类别中。我们精心设计了一个名为ImageNet-AO的真实图像测试数据集，其中包含了CLIP训练集中不太可能遇到的对象的属性。我们发现，通过大型数据集训练的CLIP模型（如OpenAI CLIP，LAION-400M和LAION-2B）在有效的组合超出分布泛化方面比受监督模型和通过较小数据集训练的CLIP模型（如CC-12M和YFCC-15M）表现出数量级的改进。我们的结果证明了该方法提供了关于规模、数据集和泛化之间关系的见解。

    arXiv:2403.18525v1 Announce Type: cross  Abstract: Vision-language models, such as CLIP, have shown promising Out-of-Distribution (OoD) generalization under various types of distribution shifts. Recent studies attempted to investigate the leading cause of this capability. In this work, we follow the same path, but focus on a specific type of OoD data - images with novel compositions of attribute-object pairs - and study whether such models can successfully classify those images into composition classes. We carefully designed an authentic image test dataset called ImageNet-AO, consisting of attributes for objects that are unlikely encountered in the CLIP training sets. We found that CLIPs trained with large datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude improvement in effective compositional OoD generalization compared to both supervised models and CLIPs trained with smaller datasets, such as CC-12M and YFCC-15M. Our results provide evidence that the sca
    
[^23]: AcTED：半监督时间常识问答中典型事件持续时间的自动获取

    AcTED: Automatic Acquisition of Typical Event Duration for Semi-supervised Temporal Commonsense QA

    [https://arxiv.org/abs/2403.18504](https://arxiv.org/abs/2403.18504)

    提出一种投票驱动的半监督方法，自动获取事件的典型持续时间，以此作为伪标签数据，实现了在时间常识问答任务中的最先进性能。

    

    我们提出一种投票驱动的半监督方法，自动获取事件的典型持续时间，并将其用作伪标签数据。人类评估表明，我们的伪标签表现出惊人的高准确性和平衡覆盖率。在时间常识问答任务中，实验结果显示，仅使用400个事件的伪例子，我们实现了与现有基于BERT的弱监督方法相当的性能，而这些方法需要大量的训练样本。与RoBERTa基线相比，我们的最佳方法在准确匹配上实现了7％的提升，建立了最先进的性能。

    arXiv:2403.18504v1 Announce Type: new  Abstract: We propose a voting-driven semi-supervised approach to automatically acquire the typical duration of an event and use it as pseudo-labeled data. The human evaluation demonstrates that our pseudo labels exhibit surprisingly high accuracy and balanced coverage. In the temporal commonsense QA task, experimental results show that using only pseudo examples of 400 events, we achieve performance comparable to the existing BERT-based weakly supervised approaches that require a significant amount of training examples. When compared to the RoBERTa baselines, our best approach establishes state-of-the-art performance with a 7% improvement in Exact Match.
    
[^24]: 语言能否胜过数值回归？基于语言的多模态轨迹预测

    Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction

    [https://arxiv.org/abs/2403.18447](https://arxiv.org/abs/2403.18447)

    本文提出了LMTraj（基于语言的多模态轨迹预测器），将轨迹预测任务转化为一种类似于问答问题的形式，通过将数值和图像数据转换为自然语言空间来引导语言模型继续生成与轨迹相关的多模态预测。

    

    arXiv:2403.18447v1 公告类型：新摘要：语言模型展示了在上下文理解和生成性能方面令人印象深刻的能力。受到近期语言基础模型成功的启发，在本文中，我们提出了LMTraj（基于语言的多模态轨迹预测器），将轨迹预测任务重新设计为一种类似于问答问题的形式。与传统的将轨迹坐标序列视为连续信号的数值回归模型不同，我们将其视为类似于文本提示的离散信号。特别地，我们首先将轨迹坐标的输入空间转换成自然语言空间。在这里，行人的整个时间序列轨迹被转换为一个文本提示，场景图像通过图像字幕被描述为文本信息。然后，转换后的数值和图像数据被包装进问答模板中，供语言模型使用。接下来，为了引导语言模型继续生成与轨迹相关的多模态预测，我们提出了一种联合训练策略，同时优化文本和图像的编码器和解码器。

    arXiv:2403.18447v1 Announce Type: new  Abstract: Language models have demonstrated impressive ability in context understanding and generative performance. Inspired by the recent success of language foundation models, in this paper, we propose LMTraj (Language-based Multimodal Trajectory predictor), which recasts the trajectory prediction task into a sort of question-answering problem. Departing from traditional numerical regression models, which treat the trajectory coordinate sequence as continuous signals, we consider them as discrete signals like text prompts. Specially, we first transform an input space for the trajectory coordinate into the natural language space. Here, the entire time-series trajectories of pedestrians are converted into a text prompt, and scene images are described as text information through image captioning. The transformed numerical and image data are then wrapped into the question-answering template for use in a language model. Next, to guide the language mo
    
[^25]: 利用结构化词对齐预训练判别式编码器进行法律案例检索

    DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via Structural Word Alignment

    [https://arxiv.org/abs/2403.18435](https://arxiv.org/abs/2403.18435)

    引入DELTA，利用结构化词对齐预训练判别式编码器进行法律案例检索，通过强调关键事实来提高表示的判别能力。

    

    最近的研究表明，使用预训练语言模型进行法律案例检索是有效的。现有的大部分研究侧重于提高[CLS]标记的上下文嵌入的表征能力，并使用文本语义相似性计算相关性。然而，在法律领域，文本语义相似性并不总是意味着案例之间足够相关。相反，在法律案例中，相关性主要取决于影响最终判决的关键事实的相似性。为此，我们引入了DELTA，这是一个为法律案例检索设计的判别式模型。基本思想是在法律案例中找到关键事实，并将[CLS]标记的上下文嵌入逼近这些关键事实，同时远离非关键事实。

    arXiv:2403.18435v1 Announce Type: cross  Abstract: Recent research demonstrates the effectiveness of using pre-trained language models for legal case retrieval. Most of the existing works focus on improving the representation ability for the contextualized embedding of the [CLS] token and calculate relevance using textual semantic similarity. However, in the legal domain, textual semantic similarity does not always imply that the cases are relevant enough. Instead, relevance in legal cases primarily depends on the similarity of key facts that impact the final judgment. Without proper treatments, the discriminative ability of learned representations could be limited since legal cases are lengthy and contain numerous non-key facts. To this end, we introduce DELTA, a discriminative model designed for legal case retrieval. The basic idea involves pinpointing key facts in legal cases and pulling the contextualized embedding of the [CLS] token closer to the key facts while pushing away from 
    
[^26]: 通过句法距离和地理邻近性探索语言关系

    Exploring language relations through syntactic distances and geographic proximity

    [https://arxiv.org/abs/2403.18430](https://arxiv.org/abs/2403.18430)

    通过句法距离和地理邻近性探索语言关系，使用POS trigrams最大化捕捉句法变化，建立语言连接并揭示语言家族和群体的簇。

    

    语言被分为共享相同语言特征的语系。虽然这种方法在理解不同语言之间的基因关系方面取得了成功，但需要更多的分析来准确量化它们的关联性，特别是在较少研究的语言层面，比如句法。本文使用从Universal Dependencies数据集中提取的一系列词性（POS）来探索语言距离。在信息论框架内，我们展示了采用POS三元组最大化捕捉句法变化的可能性，同时与可用数据量兼容。通过基于POS分布的成对距离评估建立语言连接。有趣的是，我们的分析揭示了明确对应于众所周知的语言家族和群体的簇，异常情况被解释为独特的形态类型学。此外，我们获得

    arXiv:2403.18430v1 Announce Type: new  Abstract: Languages are grouped into families that share common linguistic traits. While this approach has been successful in understanding genetic relations between diverse languages, more analyses are needed to accurately quantify their relatedness, especially in less studied linguistic levels such as syntax. Here, we explore linguistic distances using series of parts of speech (POS) extracted from the Universal Dependencies dataset. Within an information-theoretic framework, we show that employing POS trigrams maximizes the possibility of capturing syntactic variations while being at the same time compatible with the amount of available data. Linguistic connections are then established by assessing pairwise distances based on the POS distributions. Intriguingly, our analysis reveals definite clusters that correspond to well known language families and groups, with exceptions explained by distinct morphological typologies. Furthermore, we obtain
    
[^27]: TriviaHG：用于从事实性问题生成自动提示的数据集

    TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions

    [https://arxiv.org/abs/2403.18426](https://arxiv.org/abs/2403.18426)

    提出了一个用于自动提示生成的框架，构建了一个包含160,230个提示的大规模数据集TriviaHG，并提出了一种评估方法来衡量提示的质量属性。

    

    现在，个人倾向于与大型语言模型进行对话，寻找他们问题的答案。在这样的答案对任何人都很容易获得的时代，刺激和保持人类的认知能力，以及确保人类保持良好推理能力变得至关重要。本研究通过提出提示（而不是最终答案或在给出答案之前）作为一种可行的解决方案来满足这些需求。我们介绍了一个用于事实性问题的自动提示生成框架，利用它构建了TriviaHG，这是一个新颖的大规模数据集，包含来自TriviaQA数据集的16,645个问题对应的160,230个提示。此外，我们提出了一种自动评估方法，用于衡量提示的收敛性和熟悉度质量属性。为了评估TriviaHG数据集和所提出的评估方法，我们邀请了10名个体注释2,791个提示，并分配了6名研究人员

    arXiv:2403.18426v1 Announce Type: new  Abstract: Nowadays, individuals tend to engage in dialogues with Large Language Models, seeking answers to their questions. In times when such answers are readily accessible to anyone, the stimulation and preservation of human's cognitive abilities, as well as the assurance of maintaining good reasoning skills by humans becomes crucial. This study addresses such needs by proposing hints (instead of final answers or before giving answers) as a viable solution. We introduce a framework for the automatic hint generation for factoid questions, employing it to construct TriviaHG, a novel large-scale dataset featuring 160,230 hints corresponding to 16,645 questions from the TriviaQA dataset. Additionally, we present an automatic evaluation method that measures the Convergence and Familiarity quality attributes of hints. To evaluate the TriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals to annotate 2,791 hints and tasked 6 hu
    
[^28]: SemRoDe: 宏观对抗训练以学习对单词级攻击具有鲁棒性的表示

    SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks

    [https://arxiv.org/abs/2403.18423](https://arxiv.org/abs/2403.18423)

    提出了一种称为Semantic Robust Defence (SemRoDe)的新方法，通过宏观对抗训练策略增强了语言模型（LMs）的鲁棒性，学习了能够连接对抗领域和基本领域的鲁棒表示。

    

    语言模型（LMs）是自然语言处理任务中不可或缺的工具，但它们对对抗攻击的脆弱性仍然令人担忧。尽管当前的研究已经探索了对抗训练技术，但其在防御单词级攻击方面的改进仍然有限。在这项工作中，我们提出了一种名为语义鲁棒防御（SemRoDe）的新方法，这是一种宏观对抗训练策略，旨在增强LMs的鲁棒性。受到图像领域最近研究的启发，我们探讨并确认在像语言这样的离散数据设置中，通过单词替换生成的对抗样本确实属于一个展现与基本域具有高Wasserstein距离的对抗领域。我们的方法学习了一个能够连接这两个领域的鲁棒表示。我们假设如果样本被投影到一个非对抗领域，而是投影到一个具有最小偏移的领域，那么可能会...

    arXiv:2403.18423v1 Announce Type: new  Abstract: Language models (LMs) are indispensable tools for natural language processing tasks, but their vulnerability to adversarial attacks remains a concern. While current research has explored adversarial training techniques, their improvements to defend against word-level attacks have been limited. In this work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing inspiration from recent studies in the image domain, we investigate and later confirm that in a discrete data setting such as language, adversarial samples generated via word substitutions do indeed belong to an adversarial domain exhibiting a high Wasserstein distance from the base domain. Our method learns a robust representation that bridges these two domains. We hypothesize that if samples were not projected into an adversarial domain, but instead to a domain with minimal shift, it w
    
[^29]: BioMedLM：基于生物医学文本训练的27亿参数语言模型

    BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text

    [https://arxiv.org/abs/2403.18421](https://arxiv.org/abs/2403.18421)

    BioMedLM是一个27亿参数的语言模型，在PubMed文献上训练，可以在生物医学领域表现出色，尤其适用于多项选择问题回答和患者提问。

    

    arXiv:2403.18421v1 公告类型：跨领域 摘要：GPT-4和Med-PaLM 2等模型在各种生物医学NLP任务上表现出色。然而，这些模型有数千亿个参数，计算代价高昂，需要用户通过互联网发送输入数据，并且是在未知数据来源上训练的。更小且更有针对性的模型能否竞争？为了解决这个问题，我们构建并发布了BioMedLM，一个仅在PubMed摘要和全文上训练的27亿参数GPT风格的自回归模型。在进行微调时，BioMedLM可以产生强大的多项选择生物医学问题回答结果，与更大的模型竞争，例如在MedMCQA（dev）上取得57.3%的得分，在MMLU医学遗传学考试上取得69.0%的得分。BioMedLM还可以进行微调，以对医学话题上患者提出的问题提供有用的答案。这表明较小的模型潜在地可以作为透明且隐私性的服务提供者

    arXiv:2403.18421v1 Announce Type: cross  Abstract: Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance on a wide variety of biomedical NLP tasks. However, these models have hundreds of billions of parameters, are computationally expensive to run, require users to send their input data over the internet, and are trained on unknown data sources. Can smaller, more targeted models compete? To address this question, we build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive model trained exclusively on PubMed abstracts and full articles. When fine-tuned, BioMedLM can produce strong multiple-choice biomedical question-answering results competitive with much larger models, such as achieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical Genetics exam. BioMedLM can also be fine-tuned to produce useful answers to patient questions on medical topics. This demonstrates that smaller models can potentially serve as transparent, privacy-
    
[^30]: 图像网格可能比视频更有价值：使用VLM进行零样本视频问答

    An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM

    [https://arxiv.org/abs/2403.18406](https://arxiv.org/abs/2403.18406)

    本研究提出了一种新颖的策略，使用单一的Vision Language Model (VLM) 来进行零样本视频问答，将视频转换为单个合成图像以实现视频理解。

    

    受最近大型语言模型（LLM）复杂推理能力的启发，人们提出了各种用于连接视频模态的策略。其中一种突出的策略涉及视频语言模型（VideoLMs），通过训练一个可学习的接口将先进的视觉编码器与LLMs连接起来。最近，出现了另一种旨在通过多个阶段跨模态进行模态桥接的策略，利用现成的基础模型，如VideoLMs和LLMs。本研究中，我们介绍了一种简单却新颖的策略，只使用单一的视觉语言模型（VLM）。我们的出发点是视频包含一系列图像或帧，这些图像与时间信息交织在一起的简单洞察。视频理解的精髓在于巧妙地管理每个帧的时间方面以及空间细节。初始时，我们通过排列多个帧将视频转换为单个合成图像。

    arXiv:2403.18406v1 Announce Type: cross  Abstract: Stimulated by the sophisticated reasoning capabilities of recent Large Language Models (LLMs), a variety of strategies for bridging video modality have been devised. A prominent strategy involves Video Language Models (VideoLMs), which train a learnable interface with video data to connect advanced vision encoders with LLMs. Recently, an alternative strategy has surfaced, employing readily available foundation models, such as VideoLMs and LLMs, across multiple stages for modality bridging. In this study, we introduce a simple yet novel strategy where only a single Vision Language Model (VLM) is utilized. Our starting point is the plain insight that a video comprises a series of images, or frames, interwoven with temporal information. The essence of video comprehension lies in adeptly managing the temporal aspects along with the spatial details of each frame. Initially, we transform a video into a single composite image by arranging mul
    
[^31]: 通过偏好学习改进大型语言模型的属性文本生成

    Improving Attributed Text Generation of Large Language Models via Preference Learning

    [https://arxiv.org/abs/2403.18381](https://arxiv.org/abs/2403.18381)

    通过偏好学习建模和引入自动偏好优化框架，该研究解决了大型语言模型生成不可靠内容的挑战，并提出了一种自动生成归因偏好数据的方法。

    

    大型语言模型已广泛应用于自然语言处理，但它们面临生成不可靠内容的挑战。最近的研究旨在通过归因作为提供证据（即引用）的手段来减少错误信息和幻觉。然而，目前的归因方法通常侧重于检索阶段和自动评估，忽视了在人类学术写作中反映引文机制以增强可信度。本文通过将归因任务建模为偏好学习，并引入自动偏好优化（APO）框架来解决这些挑战。首先，我们通过从现有数据集中收集和过滤创建了一个包含6,330个示例供后期训练使用的精心收集集合。其次，考虑到标记偏好数据的高成本，我们进一步提出了一种自动生成归因偏好数据的自动方法，生成了95,263对数据。

    arXiv:2403.18381v1 Announce Type: cross  Abstract: Large language models have been widely adopted in natural language processing, yet they face the challenge of generating unreliable content. Recent works aim to reduce misinformation and hallucinations by resorting to attribution as a means to provide evidence (i.e., citations). However, current attribution methods usually focus on the retrieval stage and automatic evaluation that neglect mirroring the citation mechanisms in human scholarly writing to bolster credibility. In this paper, we address these challenges by modelling the attribution task as preference learning and introducing an Automatic Preference Optimization (APO) framework. First, we create a curated collection for post-training with 6,330 examples by collecting and filtering from existing datasets. Second, considering the high cost of labelling preference data, we further propose an automatic method to synthesize attribution preference data resulting in 95,263 pairs. Mo
    
[^32]: BLADE：利用小型领域特定模型增强黑盒大型语言模型

    BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models

    [https://arxiv.org/abs/2403.18365](https://arxiv.org/abs/2403.18365)

    BLADE框架将黑盒大型语言模型与小型领域特定模型结合，既保留了领域特定知识和专业见解，又提供了强大的语言理解和推理能力。

    

    大型语言模型（LLMs）如ChatGPT和GPT-4具有通用性和能够解决各种任务。然而，在开放域数据上开发的通用LLMs可能缺乏垂直领域（如法律、医学等）任务所需的领域特定知识。为了解决这一问题，先前的方法要么使用领域特定数据进行持续预训练，要么使用检索增强来支持通用LLMs。不幸的是，这些策略在实际应用中要么成本高昂，要么不可靠。因此，我们提出了一个名为BLADE的新颖框架，它增强了黑盒大型语言模型与小型领域特定模型。

    arXiv:2403.18365v1 Announce Type: new  Abstract: Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable of addressing a diverse range of tasks. However, general LLMs, which are developed on open-domain data, may lack the domain-specific knowledge essential for tasks in vertical domains, such as legal, medical, etc. To address this issue, previous approaches either conduct continuous pre-training with domain-specific data or employ retrieval augmentation to support general LLMs. Unfortunately, these strategies are either cost-intensive or unreliable in practical applications. To this end, we present a novel framework named BLADE, which enhances Black-box LArge language models with small Domain-spEcific models. BLADE consists of a black-box LLM and a small domain-specific LM. The small LM preserves domain-specific knowledge and offers specialized insights, while the general LLM contributes robust language comprehension and reasoning capabilities. Specifically, our 
    
[^33]: 评估语义搜索及其在阿拉伯语言中检索增强生成（RAG）中的作用

    Evaluation of Semantic Search and its Role in Retrieved-Augmented-Generation (RAG) for Arabic Language

    [https://arxiv.org/abs/2403.18350](https://arxiv.org/abs/2403.18350)

    本文旨在建立阿拉伯语义搜索的基准，并在检索增强生成（RAG）框架内评估其有效性。

    

    机器学习和深度学习的最新进展带来了语义相似性的概念，已在多个应用中证明具有巨大益处，大大取代了关键词搜索。然而，评估语义相似性并在各种文档中为特定查询进行搜索仍然是一项复杂的任务。这种复杂性源于任务的多方面性，缺乏标准基准，而这些挑战对于阿拉伯语言而言更加复杂。本文致力于为阿拉伯语义搜索建立一个简单而强大的基准。此外，为了精确评估这些指标和数据集的有效性，我们在检索增强生成（RAG）框架内进行语义搜索的评估。

    arXiv:2403.18350v1 Announce Type: new  Abstract: The latest advancements in machine learning and deep learning have brought forth the concept of semantic similarity, which has proven immensely beneficial in multiple applications and has largely replaced keyword search. However, evaluating semantic similarity and conducting searches for a specific query across various documents continue to be a complicated task. This complexity is due to the multifaceted nature of the task, the lack of standard benchmarks, whereas these challenges are further amplified for Arabic language. This paper endeavors to establish a straightforward yet potent benchmark for semantic search in Arabic. Moreover, to precisely evaluate the effectiveness of these metrics and the dataset, we conduct our assessment of semantic search within the framework of retrieval augmented generation (RAG).
    
[^34]: 拒绝提高可靠性：使用强化学习从知识反馈训练LLMs拒绝未知问题

    Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback

    [https://arxiv.org/abs/2403.18349](https://arxiv.org/abs/2403.18349)

    这里是中文总结出的一句话要点: 该论文研究了拒绝机制在提高大型语言模型可靠性中的作用，提出了一种基于知识反馈的强化学习框架RLKF。

    

    大型语言模型（LLMs）经常生成错误输出，被称为幻想，这是由于它们在辨别超出其知识范围的问题时的局限性。虽然解决幻想一直是研究的焦点，以往的努力主要集中在提高正确性而未充分考虑拒绝机制的重要性。本文全面研究了拒绝的作用，引入了模型可靠性的概念以及相应的度量标准。这些度量标准衡量了模型在提供准确响应的同时，灵活拒绝超出其知识边界的问题，从而最小化幻想。为了提高LLMs固有的可靠性，我们提出了一种名为知识反馈强化学习（RLKF）的新对齐框架。

    arXiv:2403.18349v1 Announce Type: new  Abstract: Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope. While addressing hallucination has been a focal point in research, previous efforts primarily concentrate on enhancing correctness without giving due consideration to the significance of rejection mechanisms. In this paper, we conduct a comprehensive examination of the role of rejection, introducing the notion of model reliability along with corresponding metrics. These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby minimizing hallucinations. To improve the inherent reliability of LLMs, we present a novel alignment framework called Reinforcement Learning from Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically determine the model's knowledge boundary and 
    
[^35]: 在多模态大型语言模型中量化和减轻单模态偏差：因果关系视角

    Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective

    [https://arxiv.org/abs/2403.18346](https://arxiv.org/abs/2403.18346)

    提出了一个因果框架用于解释多模态大型语言模型在视觉问答问题中的偏差，并引入了一个新的挑战性数据集MORE，同时提出两种减轻单模态偏差的策略。

    

    大型语言模型（LLMs）的最新进展促进了多模态LLMs（MLLMs）的发展。尽管它们具有令人印象深刻的能力，但MLLMs通常过度依赖单模态偏差（例如语言偏差和视觉偏差），导致在复杂多模态任务中给出不正确答案。为了调查这个问题，我们提出了一个因果框架来解释视觉问答（VQA）问题中的偏差。在我们的框架内，我们设计了一个因果图来阐明MLLMs对VQA问题的预测，并通过深入的因果分析评估偏差的因果效果。受因果图的启发，我们引入了一个新颖的MORE数据集，包含12,000个VQA实例。该数据集旨在挑战MLLMs的能力，需要多跳推理和克服单模态偏差。此外，我们提出了两种策略来减轻单模态偏差并增强MLLMs的推理能力。

    arXiv:2403.18346v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have facilitated the development of Multimodal LLMs (MLLMs). Despite their impressive capabilities, MLLMs often suffer from an over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers in complex multimodal tasks. To investigate this issue, we propose a causal framework to interpret the biases in Visual Question Answering (VQA) problems. Within our framework, we devise a causal graph to elucidate the predictions of MLLMs on VQA problems, and assess the causal effect of biases through an in-depth causal analysis. Motivated by the causal graph, we introduce a novel MORE dataset, consisting of 12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities, necessitating multi-hop reasoning and the surmounting of unimodal biases. Furthermore, we propose two strategies to mitigate unimodal biases and enhance MLLMs' reasoning capabiliti
    
[^36]: IterAlign: 大型语言模型的迭代式宪法对齐

    IterAlign: Iterative Constitutional Alignment of Large Language Models

    [https://arxiv.org/abs/2403.18341](https://arxiv.org/abs/2403.18341)

    提出了IterAlign，是一种数据驱动的宪法发现和自对齐框架，通过红队测试发现LLM的弱点，并使用更强大的LLM自动发现新的宪法，从而指导LLM的自校正。

    

    随着大型语言模型（LLMs）的快速发展，将LLMs与人类价值观和社会规范对齐以确保其可靠性和安全性变得至关重要。强化学习与人类反馈（RLHF）和宪法AI（CAI）已被提出用于LLM对齐。然而，这些方法要求大量的人类注释或明确预定义的宪法，这些都是劳动密集型和资源消耗型的。为了克服这些缺点，我们研究了基于宪法的LLM对齐，并提出了一种名为IterAlign的数据驱动宪法发现和自对齐框架。IterAlign利用红队测试揭示LLM的弱点，并使用更强大的LLM自动发现新的宪法。然后，这些宪法被用来指导基于LLM的自校正。这种宪法发现流程可以迭代自动运行，以发现针对特定LLMs的新宪法。

    arXiv:2403.18341v1 Announce Type: new  Abstract: With the rapid development of large language models (LLMs), aligning LLMs with human values and societal norms to ensure their reliability and safety has become crucial. Reinforcement learning with human feedback (RLHF) and Constitutional AI (CAI) have been proposed for LLM alignment. However, these methods require either heavy human annotations or explicitly pre-defined constitutions, which are labor-intensive and resource-consuming. To overcome these drawbacks, we study constitution-based LLM alignment and propose a data-driven constitution discovery and self-alignment framework called IterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM and automatically discovers new constitutions using a stronger LLM. These constitutions are then used to guide self-correction of the base LLM. Such a constitution discovery pipeline can be run iteratively and automatically to discover new constitutions that specifically target
    
[^37]: 面向德语、法语和日语的药物警戒数据集：跨语言标注药物不良反应

    A Dataset for Pharmacovigilance in German, French, and Japanese: Annotating Adverse Drug Reactions across Languages

    [https://arxiv.org/abs/2403.18336](https://arxiv.org/abs/2403.18336)

    本研究构建了一个多语言的药物不良反应语料库，包括德语、法语和日语文本，包含了12种实体类型、四种属性类型和13种关系类型的注释，为医疗保健领域开发现实世界多语言语言模型做出贡献。

    

    用户生成的数据来源在揭示药物不良反应（ADRs）方面变得越来越重要，数字世界中的讨论数量也在增加。然而，现有的临床语料库主要围绕英文科学文章展开。本研究提出了一个包括德语、法语和日语中关于ADR的文本的多语言语料库，来源包括患者论坛、社交媒体和临床报告。我们的语料库包含了12种实体类型、四种属性类型和13种关系类型的注释。这有助于为医疗保健开发现实世界的多语言语言模型。我们提供了一些统计数据以凸显与语料库相关的某些挑战，并进行了初步实验，为在单一语言内部和跨语言之间提取实体和实体之间关系提供了强大的基线。

    arXiv:2403.18336v1 Announce Type: new  Abstract: User-generated data sources have gained significance in uncovering Adverse Drug Reactions (ADRs), with an increasing number of discussions occurring in the digital world. However, the existing clinical corpora predominantly revolve around scientific articles in English. This work presents a multilingual corpus of texts concerning ADRs gathered from diverse sources, including patient fora, social media, and clinical reports in German, French, and Japanese. Our corpus contains annotations covering 12 entity types, four attribute types, and 13 relation types. It contributes to the development of real-world multilingual language models for healthcare. We provide statistics to highlight certain challenges associated with the corpus and conduct preliminary experiments resulting in strong baselines for extracting entities and relations between these entities, both within and across languages.
    
[^38]: LLM可以进行正式对话吗？自动评估LLMs在转换和解释正式规范中的表现

    Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications

    [https://arxiv.org/abs/2403.18327](https://arxiv.org/abs/2403.18327)

    本文提出了一种方法，利用两个LLM的副本与验证器结合使用，能够自动评估其在自然语言描述和正式规范之间转换的能力，无需额外的人工输入。

    

    利益相关者经常用自然语言描述系统需求，然后由领域专家将其转换为形式化语法，从而增加设计成本。本文评估了大型语言模型（LLMs）在自然语言描述和正式规范之间转换的能力。我们提出了一种方法，可以利用两个LLM的副本与现成的验证器结合使用，无需任何额外的人工输入就可以自动评估其翻译能力。我们的方法使用语言语法生成形式化语法，自动生成数据集。我们进行了经验评估以衡量这种翻译任务的准确性。

    arXiv:2403.18327v1 Announce Type: cross  Abstract: Stakeholders often describe system requirements using natural language which are then converted to formal syntax by a domain-expert leading to increased design costs. This paper assesses the capabilities of Large Language Models (LLMs) in converting between natural language descriptions and formal specifications. Existing work has evaluated the capabilities of LLMs in generating formal syntax such as source code but such experiments are typically hand-crafted and use problems that are likely to be in the training set of LLMs, and often require human-annotated datasets. We propose an approach that can use two copies of an LLM in conjunction with an off-the-shelf verifier to automatically evaluate its translation abilities without any additional human input. Our approach generates formal syntax using language grammars to automatically generate a dataset. We conduct an empirical evaluation to measure the accuracy of this translation task 
    
[^39]: 中国 offensive 语言检测：现状与未来方向

    Chinese Offensive Language Detection:Current Status and Future Directions

    [https://arxiv.org/abs/2403.18314](https://arxiv.org/abs/2403.18314)

    总体而言，这篇论文讨论了在中文中检测 offensive 语言的挑战，并强调了开发解决这一问题的特定模型和工具。

    

    虽然社交媒体平台正在做出相当大的努力监测和规范用户生成内容，但在数字空间中，恶意语言（如仇恨言论或网络欺凌）的普遍存在仍然是一个重要挑战。鉴于维护文明和尊重的在线环境的重要性，迫切需要能够实时检测恶意言论的自动系统。然而，为了开发处理汉语等语言的有效系统，面临着重大挑战，因为这些语言的复杂和微妙性使得自动处理变得困难。本文全面总结了中国 offensive 语言检测情况，审查了当前的基准和方法，并重点介绍了用于解决在这种复杂语言中检测恶意语言的独特挑战的特定模型和工具。

    arXiv:2403.18314v1 Announce Type: cross  Abstract: Despite the considerable efforts being made to monitor and regulate user-generated content on social media platforms, the pervasiveness of offensive language, such as hate speech or cyberbullying, in the digital space remains a significant challenge. Given the importance of maintaining a civilized and respectful online environment, there is an urgent and growing need for automatic systems capable of detecting offensive speech in real time. However, developing effective systems for processing languages such as Chinese presents a significant challenge, owing to the language's complex and nuanced nature, which makes it difficult to process automatically. This paper provides a comprehensive overview of offensive language detection in Chinese, examining current benchmarks and approaches and highlighting specific models and tools for addressing the unique challenges of detecting offensive language in this complex language. The primary object
    
[^40]: 大型语言模型对数学推理的双重指令调整

    Dual Instruction Tuning with Large Language Models for Mathematical Reasoning

    [https://arxiv.org/abs/2403.18295](https://arxiv.org/abs/2403.18295)

    提出了双重指令调整策略，通过引入中间推理状态预测任务和指令重构任务，从前向和后向两个方向精密建模数学推理，以增强LLMs对指令的理解和执行。

    

    最近的进展突显了利用大型语言模型（LLMs）在数学推理任务中利用CoT数据进行指令调整的成功。尽管进行了精细调整的LLMs，在CoT生成中仍存在错误、缺失和多余的步骤，导致答案预测不准确。为了缓解这一问题，我们提出了一种双重指令调整策略，以从前向和后向两个方向精密建模数学推理。这涉及引入中间推理状态预测任务（前向推理）和指令重构任务（后向推理），以增强LLMs对指令的理解和执行。这些任务的训练实例是基于现有的数学指令调整数据集构建的。随后，LLMs经过多任务微调，同时使用现有的数学指令和新创建的数据。

    arXiv:2403.18295v1 Announce Type: new  Abstract: Recent advancements highlight the success of instruction tuning with large language models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical reasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions. To alleviate this problem, we propose a dual instruction tuning strategy to meticulously model mathematical reasoning from both forward and reverse directions. This involves introducing the Intermediate Reasoning State Prediction task (forward reasoning) and the Instruction Reconstruction task (reverse reasoning) to enhance the LLMs' understanding and execution of instructions. Training instances for these tasks are constructed based on existing mathematical instruction tuning datasets. Subsequently, LLMs undergo multi-task fine-tuning using both existing mathematical instructions and the newly created data. Com
    
[^41]: 少样本重校准语言模型

    Few-Shot Recalibration of Language Models

    [https://arxiv.org/abs/2403.18286](https://arxiv.org/abs/2403.18286)

    提出了一种新框架，用于少样本特定切片重校准语言模型，实现在任意分布片段上获得校准的置信度估计。

    

    最近的研究发现了一些有希望的方法，可以从语言模型（LMs）中提取出校准良好的置信度估计，其中模型的置信度分数反映了其正确性可能性。然而，虽然LMs在广泛分布上可能具有良好的校准性，但这往往隐藏在更窄分片内存在显著的校准不准确性（例如，在数学中存在系统性过度自信可能会平衡历史中的系统性不足自信，从而在总体上实现完美校准）。为了获得任何分布片段的校准良好的置信度估计，我们提出了一种用于少样本特定切片重校准的新框架。具体来说，我们训练一个重新校准模型，该模型接受来自任何给定切片的少量无标签示例，并预测一条重新映射置信度分数以使其对该切片更准确的曲线。我们训练的模型可以重新校准任意新的切片，而无需使用该切片的任何标记数据。这使我们能够识别d

    arXiv:2403.18286v1 Announce Type: cross  Abstract: Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify d
    
[^42]: BlendX：融合模式的复杂多意图检测

    BlendX: Complex Multi-Intent Detection with Blended Patterns

    [https://arxiv.org/abs/2403.18277](https://arxiv.org/abs/2403.18277)

    BlendX提出了一套精制数据集，采用多样化模式、基于规则的启发式方法和OpenAI的ChatGPT生成工具，同时引入了新的评估指标来提高多意图检测的质量。

    

    arXiv:2403.18277v1 公告类型：新 提要：以任务为导向的对话（TOD）系统通常设计时假定每个话语代表一个意图。然而，这种假设可能不准确反映现实世界中的情况，在那里用户经常在一个话语中表达多个意图。虽然多意图检测（MID）引起了人们的兴趣，但现有的领域内数据集（如MixATIS和MixSNIPS）在其制定上存在局限性。为了解决这些问题，我们提出了BlendX，一套精制数据集，其特征模式比其前身更多样化，提升了其复杂性和多样性。对于数据集构建，我们利用基于规则的启发式方法和一个生成工具--OpenAI的ChatGPT--，并采用了相似性驱动策略进行话语选择。为了确保所提出数据集的质量，我们还引入了三个评估话语统计属性的新指标。

    arXiv:2403.18277v1 Announce Type: new  Abstract: Task-oriented dialogue (TOD) systems are commonly designed with the presumption that each utterance represents a single intent. However, this assumption may not accurately reflect real-world situations, where users frequently express multiple intents within a single utterance. While there is an emerging interest in multi-intent detection (MID), existing in-domain datasets such as MixATIS and MixSNIPS have limitations in their formulation. To address these issues, we present BlendX, a suite of refined datasets featuring more diverse patterns than their predecessors, elevating both its complexity and diversity. For dataset construction, we utilize both rule-based heuristics as well as a generative tool -- OpenAI's ChatGPT -- which is augmented with a similarity-driven strategy for utterance selection. To ensure the quality of the proposed datasets, we also introduce three novel metrics that assess the statistical properties of an utterance
    
[^43]: RankMamba，在Transformer时代对Mamba文档排名性能的基准测试

    RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era of Transformers

    [https://arxiv.org/abs/2403.18276](https://arxiv.org/abs/2403.18276)

    Mamba模型基于状态空间模型，在多个序列建模任务中取得了与Transformer相当的性能，并在经典信息检索任务--文档排名中展现了其有效性。

    

    Transformer结构在自然语言处理（NLP）、计算机视觉（CV）和信息检索(IR)等多个应用的机器学习领域取得了巨大成功。Transformer架构的核心机制--注意力，在训练中需要$O(n^2)$的时间复杂度，在推断中需要$O(n)$的时间复杂度。许多工作已经提出改进注意力机制的可扩展性，比如Flash Attention和Multi-query Attention。另一方面的工作旨在设计新的机制来取代注意力。最近，基于状态空间模型的一个显著模型结构--Mamba，在多个序列建模任务中取得了与Transformer相当的性能。

    arXiv:2403.18276v1 Announce Type: cross  Abstract: Transformer structure has achieved great success in multiple applied machine learning communities, such as natural language processing (NLP), computer vision (CV) and information retrieval (IR). Transformer architecture's core mechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$ time complexity in inference. Many works have been proposed to improve the attention mechanism's scalability, such as Flash Attention and Multi-query Attention. A different line of work aims to design new mechanisms to replace attention. Recently, a notable model structure -- Mamba, which is based on state space models, has achieved transformer-equivalent performance in multiple sequence modeling tasks.   In this work, we examine \mamba's efficacy through the lens of a classical IR task -- document ranking. A reranker model takes a query and a document as input, and predicts a scalar relevance score. This task demands the language mod
    
[^44]: 在视觉-大规模语言模型中实现交互式区域理解

    Toward Interactive Regional Understanding in Vision-Large Language Models

    [https://arxiv.org/abs/2403.18260](https://arxiv.org/abs/2403.18260)

    该研究提出了被称为RegionVLM的模型，具备显式的区域建模能力，通过引入新的信息源Local Narratives，设计简洁而创新的架构，实现了交互式区域理解，取得了优越性能。

    

    最近的视觉-语言预训练（VLP）模型取得了显著进展。然而，这些模型过分依赖于捕捉图像粗糙和全局信息的图像-文本对，导致它们在区域理解能力上存在局限性。在本研究中，我们引入了\textbf{RegionVLM}，具备显式的区域建模能力，使其能够理解用户指定的图像区域。为实现这一目标，我们设计了一个简单却创新的架构，无需修改模型架构或目标函数。此外，我们利用了一个包含新颖信息源的数据集，即局部叙事，这在先前的VLP研究中被忽视。我们的实验证明，我们的单一通用模型不仅实现了交互式对话系统，还在各种零样本区域理解任务上表现出优越性能。

    arXiv:2403.18260v1 Announce Type: cross  Abstract: Recent Vision-Language Pre-training (VLP) models have demonstrated significant advancements. Nevertheless, these models heavily rely on image-text pairs that capture only coarse and global information of an image, leading to a limitation in their regional understanding ability. In this work, we introduce \textbf{RegionVLM}, equipped with explicit regional modeling capabilities, allowing them to understand user-indicated image regions. To achieve this, we design a simple yet innovative architecture, requiring no modifications to the model architecture or objective function. Additionally, we leverage a dataset that contains a novel source of information, namely Localized Narratives, which has been overlooked in previous VLP research. Our experiments demonstrate that our single generalist model not only achieves an interactive dialogue system but also exhibits superior performance on various zero-shot region understanding tasks, without c
    
[^45]: 通过提示学习和知识蒸馏的隐喻检测

    MD-PK: Metaphor Detection via Prompt Learning and Knowledge Distillation

    [https://arxiv.org/abs/2403.18253](https://arxiv.org/abs/2403.18253)

    引入知识蒸馏和提示学习的方法解决隐喻检测中的语言规则应用和数据稀疏性问题，通过提示信息和软标签优化学生模型，提高隐喻检测的准确性。

    

    隐喻在日常生活中随处可见，但是检测它们却是一个重大挑战。先前的方法经常因语言规则应用不当而难以应对，并忽视了数据稀疏性的问题。为了解决这些挑战，我们将知识蒸馏和提示学习引入隐喻检测中。具体来说，我们设计了一个专为隐喻检测任务量身定制的提示学习模板。通过屏蔽目标词并提供相关提示信息，我们引导模型准确推断这些词的上下文含义。这种方法不仅减轻了目标词字面含义的干扰，还确保了对于隐喻检测的MIP语言规则的正确利用。此外，我们利用一种配备先前知识的教师模型生成有意义的软标签，引导学生模型的优化过程。软标签的引入类似于标签平滑，有助于改善模型的泛化能力。

    arXiv:2403.18253v1 Announce Type: new  Abstract: Metaphors are ubiquitous in daily life, yet detecting them poses a significant challenge. Previous approaches often struggled with improper application of language rules and overlooked the issue of data sparsity. To address these challenges, we introduce knowledge distillation and prompt learning into metaphor detection. Specifically, we devise a prompt learning template tailored for the metaphor detection task. By masking target words and providing relevant prompt information, we guide the model to accurately infer the contextual meaning of these words. This approach not only mitigates the interference from the literal meaning of target words but also ensures the proper utilization of MIP language rules for metaphor detection. Moreover, we employ a teacher model equipped with prior knowledge to generate meaningful soft labels, guiding the optimization process of the student model. The inclusion of soft labels, akin to label smoothing, h
    
[^46]: 超越嵌入：多模态模型中视觉表格的价值

    Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models

    [https://arxiv.org/abs/2403.18252](https://arxiv.org/abs/2403.18252)

    提出了Visual Table，一种为MLLMs量身定制的新型视觉表示，通过提供层次化文本描述的全面视觉场景来弥补现有视觉表示的不足。

    

    视觉表示学习一直是计算机视觉中的基石，从具有人类注释标签的监督学习发展到对齐来自互联网的图像-文本对。尽管多模态大语言模型（MLLMs）方面取得了近期的进展，但它们依赖的视觉表示（如CLIP嵌入）通常缺乏关键的外部世界知识，这对于现实世界的视觉推理至关重要。在这项工作中，我们提出了Visual Table，这是为MLLMs量身定制的新型视觉表示。它提供全面视觉场景的层次化文本描述，包括场景描述和涵盖类别、属性和实例级别知识的多个以对象为中心的描述。我们进一步开发了一个可扩展的生成器，用于从GPT4V的小规模注释中生成视觉表格，并训练它。广泛的评估表明，通过将生成的视觉表格作为额外视觉表示，我们

    arXiv:2403.18252v1 Announce Type: cross  Abstract: Visual representation learning has been a cornerstone in computer vision, evolving from supervised learning with human-annotated labels to aligning image-text pairs from the Internet. Despite recent advancements in multi-modal large language models (MLLMs), the visual representations they rely on, such as CLIP embeddings, often lack access to external world knowledge critical for real-world visual reasoning. In this work, we propose Visual Table, a novel visual representation tailored for MLLMs. It provides hierarchical text descriptions of holistic visual scenes, consisting of a scene description and multiple object-centric descriptions that encompass categories, attributes, and knowledge at instance level. We further develop a scalable generator for visual table generation and train it on small-scale annotations from GPT4V. Extensive evaluations demonstrate that, with generated visual tables as additional visual representations, our 
    
[^47]: 由于科学文献是多语言的，我们的模型也应该如此

    Since the Scientific Literature Is Multilingual, Our Models Should Be Too

    [https://arxiv.org/abs/2403.18251](https://arxiv.org/abs/2403.18251)

    论文指出科学文献多语言特性，主张当前模型和基准应反映语言多样性，提出改善非英语文档性能的建议。

    

    英语长期以来被认为是科学研究的$\textit{lingua franca}$，这一观念在涉及科学文献表示的自然语言处理（NLP）研究中得到了体现。在这篇立场论文中，我们定量地展示文献大多是多语言的，并提出当前的模型和基准应该反映这种语言多样性。我们提供证据表明基于文本的模型无法为非英语论文创建有意义的表示，并强调跨多语言领域无差别地使用仅英语模型对用户造成的负面影响。最后，我们提出建议，希望NLP社区能够改善对非英语文档的性能。

    arXiv:2403.18251v1 Announce Type: new  Abstract: English has long been assumed the $\textit{lingua franca}$ of scientific research, and this notion is reflected in the natural language processing (NLP) research involving scientific document representation. In this position piece, we quantitatively show that the literature is largely multilingual and argue that current models and benchmarks should reflect this linguistic diversity. We provide evidence that text-based models fail to create meaningful representations for non-English papers and highlight the negative user-facing impacts of using English-only models non-discriminately across a multilingual domain. We end with suggestions for the NLP community on how to improve performance on non-English documents.
    
[^48]: 探索LLM生成的虚假新闻的欺骗力：对现实世界检测挑战的研究

    Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges

    [https://arxiv.org/abs/2403.18249](https://arxiv.org/abs/2403.18249)

    本研究提出了一种强假新闻攻击方法VLPrompt，通过消除对额外数据收集的需求，同时保持语境一致性和原始文本的复杂性，可以更好地缩小LLM生成虚假新闻的欺骗力差距。

    

    最近，大型语言模型（LLMs）的进展使得在复杂领域如医疗保健领域尤为可能创造虚假新闻成为可能。研究突显了LLM生成的虚假新闻在有无人类辅助的情况下的欺骗力差距，但对于激励技术的潜力尚未完全探索。因此，本研究旨在确定激励策略是否能有效地缩小这一差距。当前基于LLM的虚假新闻攻击需要人类干预进行信息收集，通常会忽略细节并且无法保持上下文一致性。因此，为了更好地理解威胁策略，我们提出了一种称为条件变分自动编码器式提示（VLPrompt）的强假新闻攻击方法。与当前方法不同，VLPrompt消除了对额外数据收集的需求，同时保持了语境的连贯性并保留了原始文本的复杂性。

    arXiv:2403.18249v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPro
    
[^49]: ZAEBUC-Spoken: 一个多语言多方言的阿拉伯语-英语语音语料库

    ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech Corpus

    [https://arxiv.org/abs/2403.18182](https://arxiv.org/abs/2403.18182)

    ZAEBUC-Spoken是一个多语言多方言的阿拉伯语-英语语音语料库，其中包含阿拉伯语和英语的多种变体以及两种语言之间的代码切换，为自动语音识别提供了挑战性的数据集。

    

    我们介绍了ZAEBUC-Spoken，一个多语言多方言的阿拉伯语-英语语音语料库。该语料库包括十二小时的Zoom会议内容，涉及多位扮演工作情境中学生头脑风暴某一主题然后与对话者讨论的发言者。会议涵盖不同的主题，并分为具有不同语言设置的阶段。这个语料库为自动语音识别（ASR）提供了具有挑战性的数据集，包括两种语言（阿拉伯语和英语），其中阿拉伯语以多种变体（现代标准阿拉伯、海湾阿拉伯语和埃及阿拉伯语）存在，英语则使用不同口音。除此之外，语料库的复杂性还表现在这些语言和方言之间的代码切换。作为我们工作的一部分，我们从已建立的转录指南中汲取灵感，提出一套处理会话性语音、代码切换和正字法问题的指南。

    arXiv:2403.18182v1 Announce Type: new  Abstract: We present ZAEBUC-Spoken, a multilingual multidialectal Arabic-English speech corpus. The corpus comprises twelve hours of Zoom meetings involving multiple speakers role-playing a work situation where Students brainstorm ideas for a certain topic and then discuss it with an Interlocutor. The meetings cover different topics and are divided into phases with different language setups. The corpus presents a challenging set for automatic speech recognition (ASR), including two languages (Arabic and English) with Arabic spoken in multiple variants (Modern Standard Arabic, Gulf Arabic, and Egyptian Arabic) and English used with various accents. Adding to the complexity of the corpus, there is also code-switching between these languages and dialects. As part of our work, we take inspiration from established sets of transcription guidelines to present a set of guidelines handling issues of conversational speech, code-switching and orthography of 
    
[^50]: 语言模型中非事实性幻觉的机制

    Mechanisms of non-factual hallucinations in language models

    [https://arxiv.org/abs/2403.18167](https://arxiv.org/abs/2403.18167)

    研究揭示了语言模型中非事实性幻觉的两个通用机制：主题属性知识不足和未能正确选择对象属性，这有助于深入理解和减轻幻觉。

    

    现今最先进的语言模型（LMs）有时会产生与世界知识不符的非事实幻觉。尽管人们已经付出了大量努力来检测和减轻幻觉，但理解它们的内在机制仍然是困难的。 我们的研究调查了幻觉的机制原因，特别是 LM 在对主题关系查询做出回答时错误地预测对象属性的非事实形式。通过因果中介分析和嵌入空间投影，我们确认了跨不同规模和设计的 LM 中共享的两个造成幻觉的一般机制原因：1）在较低层 MLPs 中主题属性知识不足，以及2）在较高层注意力头和 MLPs 中未能选择正确的对象属性。这两个机制展示了不同程度的主宾关系、预测不确定性和扰动鲁棒性。此外，我们还审查了 LM 的预训练检查点。

    arXiv:2403.18167v1 Announce Type: cross  Abstract: State-of-the-art language models (LMs) sometimes generate non-factual hallucinations that misalign with world knowledge. Despite extensive efforts to detect and mitigate hallucinations, understanding their internal mechanisms remains elusive. Our study investigates the mechanistic causes of hallucination, specifically non-factual ones where the LM incorrectly predicts object attributes in response to subject-relation queries. With causal mediation analysis and embedding space projection, we identify two general mechanistic causes of hallucinations shared across LMs of various scales and designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and 2) failing to select the correct object attribute in upper layer attention heads and MLPs. These two mechanisms exhibit varying degrees of subject-object association, predictive uncertainty and perturbation robustness. Additionally, we scrutinize LM pre-training checkpoints, r
    
[^51]: 噢！我们冷冻：通过信号传播分析改进大型语言模型的量化知识蒸馏

    Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models

    [https://arxiv.org/abs/2403.18159](https://arxiv.org/abs/2403.18159)

    通过信号传播分析，提出了一种改进大型语言模型的量化知识蒸馏方法，并提供了ov-freeze稳定KD-QAT过程的洞察。

    

    大型生成模型，如大型语言模型（LLMs）和扩散模型分别在NLP和计算机视觉领域引起了革命。然而，它们的推理速度慢，计算和内存需求高，这使得在边缘设备上部署它们变得具有挑战性。在这项研究中，我们提出了一种轻量级的量化感知微调技术，使用知识蒸馏（KD-QAT）来改善使用常用数据集改进4位重量量化的LLMs的性能，以实现流行的语言使用案例，在设备聊天应用中。为了改进这种微调范式，作为主要贡献，我们通过经验研究训练过程中的梯度传播，提供对KD-QAT稳定性的洞察，以更好地理解基于KD-QAT的方法对低位量化误差的脆弱性。根据我们的见解，我们提出了ov-freeze，一种稳定KD-QAT过程的简单技术。最后，我们进行了实验

    arXiv:2403.18159v1 Announce Type: cross  Abstract: Large generative models, such as large language models (LLMs) and diffusion models have as revolutionized the fields of NLP and computer vision respectively. However, their slow inference, high computation and memory requirement makes it challenging to deploy them on edge devices. In this study, we propose a light-weight quantization aware fine tuning technique using knowledge distillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs using commonly available datasets to realize a popular language use case, on device chat applications. To improve this paradigm of finetuning, as main contributions, we provide insights into stability of KD-QAT by empirically studying the gradient propagation during training to better understand the vulnerabilities of KD-QAT based approaches to low-bit quantization errors. Based on our insights, we propose ov-freeze, a simple technique to stabilize the KD-QAT process. Finally, we expe
    
[^52]: 大型语言模型作为金融数据标注员：有效性和效率研究

    Large Language Models as Financial Data Annotators: A Study on Effectiveness and Efficiency

    [https://arxiv.org/abs/2403.18152](https://arxiv.org/abs/2403.18152)

    大型语言模型在金融数据标注方面表现出显著的效率，可以作为非专家标注员的替代品，关键在于定制化提示和特定示例。

    

    在金融领域收集带标签的数据集具有挑战性，因为领域专家稀缺且雇佣他们的成本较高。尽管大型语言模型（LLMs）在一般领域数据集的数据标注任务中表现出色，但它们在特定领域数据集上的有效性仍未得到充分探讨。为了填补这一空白，我们研究了LLMs作为金融文档中提取关系的高效数据标注员的潜力。我们将三个LLMs（GPT-4、PaLM 2和MPT Instruct）生成的标注与专家标注员和众包工作者进行比较。我们证明当前最先进的LLMs可以作为非专家众包工作者的合适替代品。我们使用各种提示和参数设置对模型进行分析，并发现为每个关系组定制提示，通过提供属于那些组的具体示例是至关重要的。此外，我们引入了一个可靠性指数（LLM-RelI）。

    arXiv:2403.18152v1 Announce Type: new  Abstract: Collecting labeled datasets in finance is challenging due to scarcity of domain experts and higher cost of employing them. While Large Language Models (LLMs) have demonstrated remarkable performance in data annotation tasks on general domain datasets, their effectiveness on domain specific datasets remains underexplored. To address this gap, we investigate the potential of LLMs as efficient data annotators for extracting relations in financial documents. We compare the annotations produced by three LLMs (GPT-4, PaLM 2, and MPT Instruct) against expert annotators and crowdworkers. We demonstrate that the current state-of-the-art LLMs can be sufficient alternatives to non-expert crowdworkers. We analyze models using various prompts and parameter settings and find that customizing the prompts for each relation group by providing specific examples belonging to those groups is paramount. Furthermore, we introduce a reliability index (LLM-RelI
    
[^53]: 大型语言模型生成被认为具有共情的回应

    Large Language Models Produce Responses Perceived to be Empathic

    [https://arxiv.org/abs/2403.18148](https://arxiv.org/abs/2403.18148)

    通过两项研究，发现大型语言模型生成的回复在共情性方面被认为比人类撰写的回复更具有共情性，这表明了在人际支持方面使用LLMs的潜力。

    

    大型语言模型（LLMs）在许多任务中表现出令人惊讶的性能，包括撰写显示共情的支持性消息。在这项研究中，我们让这些模型根据描述常见生活经历的帖子生成共情消息，如工作场景、育儿、人际关系以及其他引发焦虑和愤怒的情况。在两项研究中（N=192，202），我们向人类评分员展示了由几种模型（GPT4 Turbo，Llama2 和 Mistral）撰写的各种回复，并让人们根据这些回复在共情程度上评分。我们发现，LLM 生成的回复被一致评为比人类撰写的回复更具共情性。语言分析还表明，这些模型在使用标点符号、表情符号和某些词汇方面具有明显、可预测的“风格”。这些结果凸显了在需要共情的情境中利用LLMs提升人类同行支持的潜力。

    arXiv:2403.18148v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated surprising performance on many tasks, including writing supportive messages that display empathy. Here, we had these models generate empathic messages in response to posts describing common life experiences, such as workplace situations, parenting, relationships, and other anxiety- and anger-eliciting situations. Across two studies (N=192, 202), we showed human raters a variety of responses written by several models (GPT4 Turbo, Llama2, and Mistral), and had people rate these responses on how empathic they seemed to be. We found that LLM-generated responses were consistently rated as more empathic than human-written responses. Linguistic analyses also show that these models write in distinct, predictable ``styles", in terms of their use of punctuation, emojis, and certain words. These results highlight the potential of using LLMs to enhance human peer support in contexts where empathy is i
    
[^54]: Juru: 来自可靠来源的巴西法律大语言模型

    Juru: Legal Brazilian Large Language Model from Reputable Sources

    [https://arxiv.org/abs/2403.18140](https://arxiv.org/abs/2403.18140)

    Juru 模型通过从巴西法律来源提取的19亿个唯一标记，展示了领域专门化可以在减少预训练数据量方面发挥作用，但这种专门化会导致同一语言中其他知识领域性能下降。

    

    与预训练大型语言模型相关的高计算成本限制了相关研究。为解决这一问题，出现了两种策略：领域专门化和使用高质量数据进行预训练。为探索这些策略，我们使用来自可靠巴西法律来源的19亿个唯一标记专门化了Sabi\'a-2 Small模型，并在法律和一般知识考试中进行了少样本评估。我们的模型Juru展示了领域专门化在减少预训练数据量方面的优势。然而，这种专门化是以在同一语言中其他知识领域性能下降为代价的。这项研究有助于增加的科学证据，表明预训练数据的选择可能提高大型语言模型的性能，从而能够以较低成本探索这些模型。

    arXiv:2403.18140v1 Announce Type: cross  Abstract: The high computational cost associated with pretraining large language models limits their research. Two strategies have emerged to address this issue: domain specialization and pretraining with high-quality data. To explore these strategies, we specialized the Sabi\'a-2 Small model with 1.9 billion unique tokens from reputable Brazilian legal sources and conducted few-shot evaluations on legal and general knowledge exams. Our model, Juru, demonstrates the benefits of domain specialization with a reduced amount of pretraining data. However, this specialization comes at the expense of degrading performance in other knowledge areas within the same language. This study contributes to the growing body of scientific evidence showing that pretraining data selection may enhance the performance of large language models, enabling the exploration of these models at a lower cost.
    
[^55]: 对于不懂如何提问的人：构建一个针对数字新手的技术问题数据集

    For those who don't know (how) to ask: Building a dataset of technology questions for digital newcomers

    [https://arxiv.org/abs/2403.18125](https://arxiv.org/abs/2403.18125)

    构建了一个针对数字新手的技术问题数据集，以应对大型语言模型无法处理的词汇或概念障碍，为解决不清晰或非标准语言查询对模型输出影响问题提供基础。

    

    尽管大型语言模型的崛起开辟了学习数字技术的丰富新机会，但许多在技术边缘的人由于词汇或概念障碍而难以获得和保持能力，无法提出恰当的问题。虽然有许多努力去理解LLM创建内容的真实性以及LLM回答问题的能力，但人们并不太了解不清晰或非标准语言查询如何影响模型输出。我们提议创建一个捕捉数字新手和局外人问题的数据集，利用我们从十年一对一辅导中收集的数据。本文详细介绍了我们计划的努力和该数据集的一些潜在用途。

    arXiv:2403.18125v1 Announce Type: new  Abstract: While the rise of large language models (LLMs) has created rich new opportunities to learn about digital technology, many on the margins of this technology struggle to gain and maintain competency due to lexical or conceptual barriers that prevent them from asking appropriate questions. Although there have been many efforts to understand factuality of LLM-created content and ability of LLMs to answer questions, it is not well understood how unclear or nonstandard language queries affect the model outputs. We propose the creation of a dataset that captures questions of digital newcomers and outsiders, utilizing data we have compiled from a decade's worth of one-on-one tutoring. In this paper we lay out our planned efforts and some potential uses of this dataset.
    
[^56]: ChatGPT角色扮演数据集：用户动机与模型自然度分析

    ChatGPT Role-play Dataset: Analysis of User Motives and Model Naturalness

    [https://arxiv.org/abs/2403.18121](https://arxiv.org/abs/2403.18121)

    研究调查了ChatGPT在不同对话环境下的行为，介绍并分析了角色扮演数据集，突出了用户与ChatGPT交互时的动机多样性和AI自然度的变化，为改进人机交流有效性提供了新思路。

    

    近年来，像ChatGPT这样的互动型大型语言模型的发展已经彻底改变了各个领域；然而，它们在自然和角色扮演对话环境中的行为仍未得到充分探讨。在我们的研究中，我们通过深入分析ChatGPT在不同环境中的对话中的表现，特别是在正常对话和角色扮演设置中的互动，来填补这一空白。我们引入了一个新颖的数据集，其中包含广泛范围的人机对话，带有用户动机和模型自然度的注释，以研究（i）人类如何与对话AI模型交互，以及（ii）AI模型响应的自然度。我们的研究突显了用户与ChatGPT互动时的动机多样性和AI自然度的变化，不仅展示了人与AI之间自然对话的微妙动力学，而且为改进人机通信的有效性提供了新途径。

    arXiv:2403.18121v1 Announce Type: new  Abstract: Recent advances in interactive large language models like ChatGPT have revolutionized various domains; however, their behavior in natural and role-play conversation settings remains underexplored. In our study, we address this gap by deeply investigating how ChatGPT behaves during conversations in different settings by analyzing its interactions in both a normal way and a role-play setting. We introduce a novel dataset of broad range of human-AI conversations annotated with user motives and model naturalness to examine (i) how humans engage with the conversational AI model, and (ii) how natural are AI model responses. Our study highlights the diversity of user motives when interacting with ChatGPT and variable AI naturalness, showing not only the nuanced dynamics of natural conversations between humans and AI, but also providing new avenues for improving the effectiveness of human-AI communication.
    
[^57]: 不要相信：验证--用自动形式化为基础的LLM定量推理

    Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization

    [https://arxiv.org/abs/2403.18120](https://arxiv.org/abs/2403.18120)

    通过将非正式的数学陈述翻译为形式的Isabelle代码并进行自动验证，我们提供了一种机制，可以自动拒绝在内部一致性方面与形式化问题陈述不一致的解决方案。

    

    大型语言模型（LLM），如Google的Minerva和OpenAI的GPT系列，正在越来越能够解决数学定量推理问题。然而，它们在推理步骤和答案中仍然存在没有理由的逻辑和计算错误。本文利用LLMs的训练语料库包含足够多的形式化数学示例（例如在Isabelle中，一个形式定理证明环境），它们可以被提示将非正式的数学陈述翻译即自动形式化为形式的Isabelle代码--该代码可以被自动验证内部一致性。这提供了一个机制，可以自动拒绝那些其形式化版本在其内部或与形式化问题陈述不一致的解决方案。我们在GSM8K、MATH和MultiArith数据集上评估了我们的方法，并证明我们的方法提供了一个一直比

    arXiv:2403.18120v1 Announce Type: new  Abstract: Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than 
    
[^58]: 大型语言模型在教育领域的应用：调研与展望

    Large Language Models for Education: A Survey and Outlook

    [https://arxiv.org/abs/2403.18105](https://arxiv.org/abs/2403.18105)

    大型语言模型在教育领域的应用调研总结了LLMs在教育中的各种技术应用，包括学生和教师辅助、自适应学习和商业工具，提出了未来研究机会和潜在方向。

    

    大型语言模型（LLMs）的出现为教育领域带来了新的可能性。这篇调研论文总结了LLMs在教育环境中的各种技术，涵盖了学生和教师的辅助，自适应学习和商业工具。我们系统地审查了每个视角中的技术进步，整理了相关数据集和基准测试，并确定了在教育中部署LLMs所涉及的风险和挑战。此外，我们概述了未来的研究机会，突出了潜在的有前途的方向。我们的调研旨在为教育工作者、研究人员和决策者提供全面的技术图景，以利用LLMs的力量，彻底改革教育实践，并促进更有效的个性化学习环境。

    arXiv:2403.18105v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has brought in a new era of possibilities in the realm of education. This survey paper summarizes the various technologies of LLMs in educational settings from multifaceted perspectives, encompassing student and teacher assistance, adaptive learning, and commercial tools. We systematically review the technological advancements in each perspective, organize related datasets and benchmarks, and identify the risks and challenges associated with deploying LLMs in education. Furthermore, we outline future research opportunities, highlighting the potential promising directions. Our survey aims to provide a comprehensive technological picture for educators, researchers, and policymakers to harness the power of LLMs to revolutionize educational practices and foster a more effective personalized learning environment.
    
[^59]: GPT与语言障碍：跨语言法律问答的研究

    GPTs and Language Barrier: A Cross-Lingual Legal QA Examination

    [https://arxiv.org/abs/2403.18098](https://arxiv.org/abs/2403.18098)

    本研究探讨了GPT在跨语言法律问答领域的应用，通过分析英语和日语提示对性能的影响，为发展更高效准确的跨语言问答解决方案做出贡献。

    

    在这篇论文中，我们使用COLIEE Task 4数据集探讨了生成式预训练变换器（GPTs）在跨语言法律问答系统中的应用。在COLIEE Task 4中，给定一个陈述和一组作为上下文的相关法律文章，目标是确定该陈述是否在法律上有效，即是否可以从提供的上下文文章中推断出来，也称为蕴涵任务。通过评估四种不同的英语和日语提示和数据组合，我们为多语言法律问答场景中GPTs的性能提供了宝贵的见解，有助于开发在法律领域更高效准确的跨语言问答解决方案。

    arXiv:2403.18098v1 Announce Type: cross  Abstract: In this paper, we explore the application of Generative Pre-trained Transformers (GPTs) in cross-lingual legal Question-Answering (QA) systems using the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a set of related legal articles that serve as context, the objective is to determine whether the statement is legally valid, i.e., if it can be inferred from the provided contextual articles or not, which is also known as an entailment task. By benchmarking four different combinations of English and Japanese prompts and data, we provide valuable insights into GPTs' performance in multilingual legal QA scenarios, contributing to the development of more efficient and accurate cross-lingual QA solutions in the legal domain.
    
[^60]: 提升法律文件检索：采用大型语言模型的多阶段方法

    Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models

    [https://arxiv.org/abs/2403.18093](https://arxiv.org/abs/2403.18093)

    将提示技术作为检索系统的最后阶段，通过BM25预排序和基于BERT的重新排序的支持，可以显著提高法律文件检索的准确性。

    

    大规模语言模型，如GPT-3.5、GPT-4和LLaMA等拥有数十亿参数的模型正变得越来越普遍。许多研究探索了有效的提示技术，以利用这些LLM在各种研究问题上的能力。在法律数据领域，具体在检索上，由于法律文章数量庞大且长度可观，直接应用提示技术面临着挑战。这项研究专注于最大程度地发挥提示的潜力，将其置于检索系统的最后阶段，前面有两个阶段的支持：BM25预排序和基于BERT的重新排序。在COLIEE 2023数据集上的实验表明，将提示技术整合到检索系统中显著提高了检索精度。然而，错误分析揭示了检索系统中仍需要解决的一些现有问题。

    arXiv:2403.18093v1 Announce Type: cross  Abstract: Large language models with billions of parameters, such as GPT-3.5, GPT-4, and LLaMA, are increasingly prevalent. Numerous studies have explored effective prompting techniques to harness the power of these LLMs for various research problems. Retrieval, specifically in the legal data domain, poses a challenging task for the direct application of Prompting techniques due to the large number and substantial length of legal articles. This research focuses on maximizing the potential of prompting by placing it as the final phase of the retrieval system, preceded by the support of two phases: BM25 Pre-ranking and BERT-based Re-ranking. Experiments on the COLIEE 2023 dataset demonstrate that integrating prompting techniques on LLMs into the retrieval system significantly improves retrieval accuracy. However, error analysis reveals several existing issues in the retrieval system that still need resolution.
    
[^61]: 光谱卷积变压器：协调视觉变压器中的实部和复部多视图光谱算子

    Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer

    [https://arxiv.org/abs/2403.18063](https://arxiv.org/abs/2403.18063)

    该论文提出了光谱卷积变压器 (SCT)，通过结合局部信息的卷积操作和全局信息的复杂傅里叶基础，实现了对视觉变压器中实部和复部多视图光谱算子的协调，从而实现了更好的性能。

    

    视觉中使用的Transformer已经通过各种结构进行了研究 - 如ViT、PVT和Swin。这些工作旨在改进注意力机制并使其更加高效。与此不同的是，人们感受到了包含局部信息的需要，这导致在Transformer中引入卷积，如CPVT和CvT。我们使用复杂傅立叶基础捕捉全局信息，通过各种方法，如AFNO、GFNet和Spectformer实现全局令牌混合。我们提倡结合数据的三种不同视图 - 局部、全局和长程依赖性。我们还研究了仅使用实域光谱表示的最简单全局表示 - 通过Hartley变换获得。我们在初始层中使用卷积算子捕捉局部信息。通过这两个贡献，我们能够优化并获得一个提供改进性能的光谱卷积变压器（SCT）。

    arXiv:2403.18063v1 Announce Type: cross  Abstract: Transformers used in vision have been investigated through diverse architectures - ViT, PVT, and Swin. These have worked to improve the attention mechanism and make it more efficient. Differently, the need for including local information was felt, leading to incorporating convolutions in transformers such as CPVT and CvT. Global information is captured using a complex Fourier basis to achieve global token mixing through various methods, such as AFNO, GFNet, and Spectformer. We advocate combining three diverse views of data - local, global, and long-range dependence. We also investigate the simplest global representation using only the real domain spectral representation - obtained through the Hartley transform. We use a convolutional operator in the initial layers to capture local information. Through these two contributions, we are able to optimize and obtain a spectral convolution transformer (SCT) that provides improved performance 
    
[^62]: COIG-CQIA：只需质量——面向中文指令微调的论文

    COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning

    [https://arxiv.org/abs/2403.18058](https://arxiv.org/abs/2403.18058)

    COIG-CQIA 是一个高质量的中文指令微调数据集，旨在构建一个多样化、广泛的指令微调数据集，以更好地使模型行为与人类交互保持一致。

    

    最近，大型语言模型（LLMs）取得了显著进展，特别是在英语领域。这些进展使得这些LLMs能够以前所未有的准确性和流畅度理解并执行复杂指令。然而，尽管取得了这些进展，中文指令微调的发展仍存在明显差距。中文语言的独特语言特征和文化深度为指令微调任务带来挑战。现有的数据集要么源自以英语为中心的LLMs，要么不适合与现实中文用户的交互模式相符。为填补这一差距，我们引入了COIG-CQIA，一个高质量的中文指令微调数据集。我们的目标是构建一个多样化、广泛的指令微调数据集，以更好地使模型行为与人类交互保持一致。为此，我们从不同来源收集了高质量的人类编写语料库。

    arXiv:2403.18058v1 Announce Type: cross  Abstract: Recently, there have been significant advancements in large language models (LLMs), particularly focused on the English language. These advancements have enabled these LLMs to understand and execute complex instructions with unprecedented accuracy and fluency. However, despite these advancements, there remains a noticeable gap in the development of Chinese instruction tuning. The unique linguistic features and cultural depth of the Chinese language pose challenges for instruction tuning tasks. Existing datasets are either derived from English-centric LLMs or are ill-suited for aligning with the interaction patterns of real-world Chinese users. To bridge this gap, we introduce COIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to build a diverse, wide-ranging instruction-tuning dataset to better align model behavior with human interactions. To this end, we collect a high-quality human-written corpus from various so
    
[^63]: 督导提示训练

    Supervisory Prompt Training

    [https://arxiv.org/abs/2403.18051](https://arxiv.org/abs/2403.18051)

    提出了一种督导提示训练（SPT）方法，利用双LLM系统生成高效提示并引入影响分数概念，通过优化提示成功提高了LLMs的性能。

    

    大型语言模型（LLMs）的性能在很大程度上取决于提示的质量，这些提示通常是手工设计的并且特定于任务，使得它们昂贵且不可扩展。我们提出了一种新颖的方法，即督导提示训练（SPT）。SPT利用双LLM系统自动生成高效的提示。在该系统中，一个LLM，即生成器，执行任务，而另一个LLM，即校正器，提供反馈并生成改进的提示。与先前的技术相比，生成器和校正器在时间上共同并持续改进它们的提示。我们还介绍了“影响分数”的概念，用于衡量提示的句子级有效性。我们的方法在四个基准上进行了测试，测试LLMs中幻觉的水平。值得注意的是，我们成功将GPT-4在GSM8K上的准确率从65.8%提高到94.1%（增加28.3%）。SPT通过优化提示提高了LLMs的性能。

    arXiv:2403.18051v1 Announce Type: cross  Abstract: The performance of Large Language Models (LLMs) relies heavily on the quality of prompts, which are often manually engineered and task-specific, making them costly and non-scalable. We propose a novel approach, Supervisory Prompt Training (SPT). SPT automates the generation of highly effective prompts using a dual LLM system. In this system, one LLM, the generator, performs a task while the other, the corrector, provides feedback and generates improved prompts. In contrast to earlier techniques, both the generator and corrector collaboratively and continuously improve their prompts over time. We also introduce the concept of \textit{impact scores} to measure the sentence-level effectiveness of the prompts. Our method was tested on four benchmarks, testing the level of hallucinations in LLMs. Notably, we were able to increase the accuracy of GPT-4 on GSM8K from 65.8\% to 94.1\% (28.3\% increase). SPT advances LLMs by refining prompts to
    
[^64]: 机器翻译中句法和语义接近对反向翻译的影响

    The Impact of Syntactic and Semantic Proximity on Machine Translation with Back-Translation

    [https://arxiv.org/abs/2403.18031](https://arxiv.org/abs/2403.18031)

    机器翻译中，语义接近对反向翻译的影响很重要，推测跨语种平行的语义依赖性是无监督神经机器翻译成功的关键。

    

    arXiv:2403.18031v1公告类型：新摘要：无监督即时反向翻译，结合多语言预训练，是无监督神经机器翻译的主要方法。然而，在理论上，该方法一般不应起作用。因此，我们进行了针对人工语言的受控实验，以确定哪些语言属性使反向翻译成为有效的训练方法，覆盖了词汇、句法和语义属性。我们发现，与普遍认为的相反，（i）平行词频分布，（ii）部分共享词汇和（iii）不同语种之间的类似句法结构并不足以解释反向翻译的成功。然而，我们显示，即使粗糙的语义信号（跨语言相似的词汇领域）也确实通过反向翻译改善了两种语言的对齐。我们猜测，富含语义依赖性并且跨语种平行的语义对成功的无监督基础。

    arXiv:2403.18031v1 Announce Type: new  Abstract: Unsupervised on-the-fly back-translation, in conjunction with multilingual pretraining, is the dominant method for unsupervised neural machine translation. Theoretically, however, the method should not work in general. We therefore conduct controlled experiments with artificial languages to determine what properties of languages make back-translation an effective training method, covering lexical, syntactic, and semantic properties. We find, contrary to popular belief, that (i) parallel word frequency distributions, (ii) partially shared vocabulary, and (iii) similar syntactic structure across languages are not sufficient to explain the success of back-translation. We show however that even crude semantic signal (similar lexical fields across languages) does improve alignment of two languages through back-translation. We conjecture that rich semantic dependencies, parallel across languages, are at the root of the success of unsupervised 
    
[^65]: 通过特定掩码损失改善预训练语言模型的敏感性：以生物医学实体识别为例

    Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER

    [https://arxiv.org/abs/2403.18025](https://arxiv.org/abs/2403.18025)

    提出了Mask Specific Language Modeling（MSLM）方法来改善LM在微调过程中对目标领域知识的敏感性，通过加权领域特定术语的重要性进行学习。

    

    将语言模型（LMs）调整到新领域通常通过在特定领域数据上微调预训练LM（PLM）来实现。微调将新知识引入LM，使它能够理解和有效执行目标域任务。然而，微调可能会无意中变得不够敏感，如果它忽视了源域和目标域之间的广泛差异（例如在词义上）。为了解决微调不敏感的问题，我们提出了Mask Specific Language Modeling（MSLM），一种通过在微调过程中适当加权领域特定术语（DS-terms）的重要性来有效获取目标领域知识的方法。MSLM同时屏蔽DS术语和通用词，然后通过确保LM受到更大惩罚来学习特定于掩码的损失。

    arXiv:2403.18025v1 Announce Type: cross  Abstract: Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for in
    
[^66]: 用聚类定义丰富词语使用图谱

    Enriching Word Usage Graphs with Cluster Definitions

    [https://arxiv.org/abs/2403.18024](https://arxiv.org/abs/2403.18024)

    通过聚类定义，将现有多种语言的词语使用图谱WUGs进行丰富，人类评估表明这些定义更好地匹配WUGs中的簇，对于可解释的语义变化建模非常有帮助。

    

    我们提出了一个词语使用图谱（WUGs）数据集，其中现有的多种语言的WUGs加入了作为意义定义的聚类标签。这些标签是通过经过微调的编码器-解码器语言模型从头开始生成的。进行的人类评估表明，这些定义与现有的WUGs中的簇更匹配，比起两个基线系统从WordNet中选择的定义。与此同时，这种方法易于使用且易于扩展到新的语言。产生的丰富数据集对于进行可解释的语义变化建模非常有帮助。

    arXiv:2403.18024v1 Announce Type: new  Abstract: We present a dataset of word usage graphs (WUGs), where the existing WUGs for multiple languages are enriched with cluster labels functioning as sense definitions. They are generated from scratch by fine-tuned encoder-decoder language models. The conducted human evaluation has shown that these definitions match the existing clusters in WUGs better than the definitions chosen from WordNet by two baseline systems. At the same time, the method is straightforward to use and easy to extend to new languages. The resulting enriched datasets can be extremely helpful for moving on to explainable semantic change modeling.
    
[^67]: DORE：一份用于葡萄牙语定义生成的数据集

    DORE: A Dataset For Portuguese Definition Generation

    [https://arxiv.org/abs/2403.18018](https://arxiv.org/abs/2403.18018)

    DORE是第一个用于葡萄牙语的定义生成数据集，填补了这一领域的空白，包含超过10万个定义，并评估了多种基于深度学习的模型。

    

    arXiv:2403.18018v1 公告类型：新的 摘要：定义建模（DM）是自动为特定单词生成词典定义的任务。具有DM能力的计算系统可以在多个受众中受益，因为DM被视为监督自然语言生成问题，这些系统需要大量带注释的数据集来训练机器学习（ML）模型。已经发布了一些用于英语和其他高资源语言的DM数据集。尽管葡萄牙语在大多数自然语言处理任务中被认为是一种中/高资源语言，且被2亿多母语人口使用，但目前尚无葡萄牙语的DM数据集。在这项研究中，我们通过引入DORE填补了这一空白；这是第一个用于葡萄牙语的定义建模数据集，包含超过10万个定义。我们还在DORE上评估了几种基于深度学习的DM模型，并报告了结果。

    arXiv:2403.18018v1 Announce Type: new  Abstract: Definition modelling (DM) is the task of automatically generating a dictionary definition for a specific word. Computational systems that are capable of DM can have numerous applications benefiting a wide range of audiences. As DM is considered a supervised natural language generation problem, these systems require large annotated datasets to train the machine learning (ML) models. Several DM datasets have been released for English and other high-resource languages. While Portuguese is considered a mid/high-resource language in most natural language processing tasks and is spoken by more than 200 million native speakers, there is no DM dataset available for Portuguese. In this research, we fill this gap by introducing DORE; the first dataset for Definition MOdelling for PoRtuguEse containing more than 100,000 definitions. We also evaluate several deep learning based DM models on DORE and report the results. The dataset and the findings o
    
[^68]: 用于可解释图像问答的内在子图生成

    Intrinsic Subgraph Generation for Interpretable Graph based Visual Question Answering

    [https://arxiv.org/abs/2403.17647](https://arxiv.org/abs/2403.17647)

    该论文介绍了一种用于图像问答的可解释方法，通过内在生成子图来提供决策洞察，并在GQA数据集上取得了竞争性能。

    

    深度学习在视觉问答（VQA）中取得了巨大成功，同时也增加了对可解释方法的需求。大多数可解释人工智能（XAI）方法侧重于生成事后解释，而非采取内在方法，后者特征化了可解释模型。在这项工作中，我们介绍了一种用于基于图的VQA的可解释方法，并在GQA数据集上展示了竞争性能。这种方法弥合了解释性和性能之间的差距。我们的模型被设计成在问答过程中本质上生成一个子图作为解释，提供决策制定的洞察。为了评估这些生成的子图的质量，我们将它们与建立的用于图神经网络的事后解释能力方法进行比较，并进行人类评估。此外，我们提出了与...

    arXiv:2403.17647v1 Announce Type: new  Abstract: The large success of deep learning based methods in Visual Question Answering (VQA) has concurrently increased the demand for explainable methods. Most methods in Explainable Artificial Intelligence (XAI) focus on generating post-hoc explanations rather than taking an intrinsic approach, the latter characterizing an interpretable model. In this work, we introduce an interpretable approach for graph-based VQA and demonstrate competitive performance on the GQA dataset. This approach bridges the gap between interpretability and performance. Our model is designed to intrinsically produce a subgraph during the question-answering process as its explanation, providing insight into the decision making. To evaluate the quality of these generated subgraphs, we compare them against established post-hoc explainability methods for graph neural networks, and perform a human evaluation. Moreover, we present quantitative metrics that correlate with the 
    
[^69]: 使用动态前缀调整的混合倡议响应生成

    Mix-Initiative Response Generation with Dynamic Prefix Tuning

    [https://arxiv.org/abs/2403.17636](https://arxiv.org/abs/2403.17636)

    提出了一种使用动态前缀调整的混合倡议响应生成框架，解决了对话系统中的交叉污染问题，并能够在监督和非监督设置下学习倡议感知前缀。

    

    混合倡议在控制对话方向中起着关键作用。对于发言者， passively 响应或 proactively 主导会导致完全不同的响应。然而，大多数对话系统专注于训练一个统一的响应生成模型，而不区分不同的倡议。这导致了交叉污染问题，模型混淆了不同的倡议并生成不合适的响应。此外，为倡议标签获取大量人类注释可能很昂贵。为了解决这个问题，我们提出了一个通用的混合倡议动态前缀调整框架 (IDPT)，以解耦不同倡议与生成模型，该模型在监督和非监督设置下学习倡议感知前缀。具体来说，IDPT将倡议因素解耦为不同的前缀参数，并使用注意机制调整倡议的选择。

    arXiv:2403.17636v1 Announce Type: new  Abstract: Mixed initiative serves as one of the key factors in controlling conversation directions. For a speaker, responding passively or leading proactively would result in rather different responses. However, most dialogue systems focus on training a holistic response generation model without any distinction among different initiatives. It leads to the cross-contamination problem, where the model confuses different initiatives and generates inappropriate responses. Moreover, obtaining plenty of human annotations for initiative labels can be expensive. To address this issue, we propose a general mix-Initiative Dynamic Prefix Tuning framework (IDPT) to decouple different initiatives from the generation model, which learns initiative-aware prefixes in both supervised and unsupervised settings. Specifically, IDPT decouples initiative factors into different prefix parameters and uses the attention mechanism to adjust the selection of initiatives in 
    
[^70]: 语言模型是生物医学成像任务的免费助推器

    Language Models are Free Boosters for Biomedical Imaging Tasks

    [https://arxiv.org/abs/2403.17343](https://arxiv.org/abs/2403.17343)

    本研究揭示了基于残差的大型语言模型在生物医学成像任务中作为编码器的意想不到的有效性，利用冻结的变压器块进行直接处理视觉令牌，从而提高各种生物医学成像应用的性能。

    

    在这项研究中，我们揭示了基于残差的大型语言模型（LLMs）在生物医学成像任务中作为编码器的意想不到的有效性，这是传统上缺乏语言或文本数据的领域。该方法不同于已建立的方法，通过利用从预训练的LLMs中提取的冻结变压器块作为创新的编码器层，直接处理视觉令牌。这种策略与通常依赖于语言驱动提示和输入的标准多模态视觉语言框架有着显著的不同。我们发现这些LLMs能够提升各种生物医学成像应用的性能，包括2D和3D视觉分类任务，充当即插即用的助推器。更有趣的是，作为副产品，我们发现所提出的框架实现了卓越的性能，在M的广泛、标准化数据集中取得了新的最先进结果。

    arXiv:2403.17343v1 Announce Type: cross  Abstract: In this study, we uncover the unexpected efficacy of residual-based large language models (LLMs) as part of encoders for biomedical imaging tasks, a domain traditionally devoid of language or textual data. The approach diverges from established methodologies by utilizing a frozen transformer block, extracted from pre-trained LLMs, as an innovative encoder layer for the direct processing of visual tokens. This strategy represents a significant departure from the standard multi-modal vision-language frameworks, which typically hinge on language-driven prompts and inputs. We found that these LLMs could boost performance across a spectrum of biomedical imaging applications, including both 2D and 3D visual classification tasks, serving as plug-and-play boosters. More interestingly, as a byproduct, we found that the proposed framework achieved superior performance, setting new state-of-the-art results on extensive, standardized datasets in M
    
[^71]: 引导远程监督用于多语言关系抽取数据：适应新语言

    Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language

    [https://arxiv.org/abs/2403.17143](https://arxiv.org/abs/2403.17143)

    本文应用引导远程监督方法，为德语创建了最大的传记关系抽取数据集，同时发布了手动标注的评估数据集。

    

    摘要：关系抽取对于在数字人文学和相关学科背景下提取和理解传记信息至关重要。社区对构建能够训练机器学习模型提取关系的数据集越来越感兴趣。然而，标注这样的数据集可能既昂贵又耗时，而且仅限于英语。本文应用了引导式远程监督方法，为德语创建了一个大型传记关系抽取数据集。我们的数据集包含了超过80,000个实例，涵盖了九种关系类型，是最大的德语传记关系抽取数据集。我们还创建了一个手动标注的数据集，包含2000个实例用于评估模型，并与利用引导式远程监督方法编制的数据集一起发布。我们在自动生成的数据集上训练了几种最先进的机器学习模型，并将其发布。

    arXiv:2403.17143v1 Announce Type: new  Abstract: Relation extraction is essential for extracting and understanding biographical information in the context of digital humanities and related subjects. There is a growing interest in the community to build datasets capable of training machine learning models to extract relationships. However, annotating such datasets can be expensive and time-consuming, in addition to being limited to English. This paper applies guided distant supervision to create a large biographical relationship extraction dataset for German. Our dataset, composed of more than 80,000 instances for nine relationship types, is the largest biographical German relationship extraction dataset. We also create a manually annotated dataset with 2000 instances to evaluate the models and release it together with the dataset compiled using guided distant supervision. We train several state-of-the-art machine learning models on the automatically created dataset and release them as 
    
[^72]: 利用预训练语言模型进行粗调优的专题文档检索

    Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models

    [https://arxiv.org/abs/2403.16915](https://arxiv.org/abs/2403.16915)

    本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调，在专题文档检索中显著改善了效果。

    

    在信息检索系统中，利用预训练语言模型（PLM-based IR）进行微调需要学习查询表示和查询-文档关系，除了下游任务特定的学习。本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调。通过在粗调优学习查询表示和查询-文档关系，我们旨在减少微调的负担，提高下游IR任务的学习效果。我们提出了用于粗调优的查询-文档对预测（QDPP），其预测查询-文档对的适当性。评估实验显示，所提出的方法显著改善了四个专题文档检索数据集中的MRR和/或nDCG@5。此外，查询预测任务的结果表明，粗调优促进了查询表示和查询-文档关系的学习。

    arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
    
[^73]: 可视引导的生成式文本布局预训练用于文档智能

    Visually Guided Generative Text-Layout Pre-training for Document Intelligence

    [https://arxiv.org/abs/2403.16516](https://arxiv.org/abs/2403.16516)

    提出了一种名为ViTLP的可视引导的生成文本布局预训练技术，能够处理任意长度的词汇密集型文档，并且可以作为OCR模型用于文本定位和识别。

    

    先前的研究表明，预训练技术可以提升视觉文档理解（VDU）的性能，通常需要模型获得感知和推理文档文本和布局（例如文本和表格单元的位置）的能力。为此，我们提出了一种名为ViTLP的可视引导的生成文本布局预训练技术。给定一个文档图像，该模型优化分层语言和布局建模目标，以生成交错的文本和布局序列。此外，为了解决Transformers处理长文档的局限性，我们引入了一种简单而有效的多段生成式预训练方案，使ViTLP能够处理任意长度的词汇密集型文档。ViTLP可以作为本地OCR模型，用于定位和识别文档图像中的文本。此外，ViTLP可以有效应用于各种下游VDU任务。大量实验证明ViTLP可以...

    arXiv:2403.16516v1 Announce Type: new  Abstract: Prior study shows that pre-training techniques can boost the performance of visual document understanding (VDU), which typically requires models to gain abilities to perceive and reason both document texts and layouts (e.g., locations of texts and table-cells). To this end, we propose visually guided generative text-layout pre-training, named ViTLP. Given a document image, the model optimizes hierarchical language and layout modeling objectives to generate the interleaved text and layout sequence. In addition, to address the limitation of processing long documents by Transformers, we introduce a straightforward yet effective multi-segment generative pre-training scheme, facilitating ViTLP to process word-intensive documents of any length. ViTLP can function as a native OCR model to localize and recognize texts of document images. Besides, ViTLP can be effectively applied to various downstream VDU tasks. Extensive experiments show that Vi
    
[^74]: LLMs是少样本情境低资源语言学习器

    LLMs Are Few-Shot In-Context Low-Resource Language Learners

    [https://arxiv.org/abs/2403.16512](https://arxiv.org/abs/2403.16512)

    该研究对25种低资源语言和7种相对较高资源语言上的情境学习（ICL）及其跨语言变体进行了研究，发现了在低资源语言中使用LLMs进行ICL的有效性，提出了替代方法查询对齐，并为低资源语言的ICL提供了宝贵见解。

    

    在情境学习（ICL）的支持下，大型语言模型（LLMs）可以利用短时的情境信息执行各种任务，这为缩小高资源语言和低资源语言之间的差距提供了重要途径。然而，目前只有少数研究探讨了针对低资源语言的ICL，其中大部分集中在相对高资源的语言，比如法语和西班牙语。在这项工作中，我们对25种低资源语言和7种相对较高资源语言上的ICL及其跨语言变体（X-ICL）进行了广泛研究。我们的研究不仅评估了LLMs在低资源语言中使用ICL的有效性，还发现了情境标签对齐的缺陷，并引入了更有效的替代方法：查询对齐。此外，我们为低资源语言的ICL的各个方面提供了宝贵的见解。我们的研究总结了少样本情境学习的重要性。

    arXiv:2403.16512v1 Announce Type: cross  Abstract: In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages. Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages. Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of ICL for low-resource languages. Our study concludes the significance of few-shot in-cont
    
[^75]: $\textit{LinkPrompt}$: 基于提示的语言模型的自然和通用对抗攻击

    $\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models

    [https://arxiv.org/abs/2403.16432](https://arxiv.org/abs/2403.16432)

    基于提示的语言模型的优化过程揭示了生成对抗提示以误导模型的见解，引发了对该范式对抗性脆弱性的担忧。

    

    Prompt-based learning 是一种新的语言模型训练范式，它将预训练语言模型（PLMs）调整到下游任务，从而在各种自然语言处理（NLP）任务中提升了性能基准。一些研究表明，通过优化搜索提示的有效性，而不是使用固定的提示模板来微调模型。这种基于提示优化过程对PLMs的学习也揭示了生成对抗提示以误导模型的见解，引发了对这一范式对抗性脆弱性的担忧。最近的研究表明，可以生成通用对抗触发器（UATs）来改变不仅目标PLMs的预测，还有对应Prompt-based Fine-tuning Models（PFMs）的预测。然而，以前作品中发现的UATs通常是无法阅读的令牌或字符。

    arXiv:2403.16432v1 Announce Type: cross  Abstract: Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters a
    
[^76]: 语言-图像预训练的中心掩蔽技术

    Centered Masking for Language-Image Pre-Training

    [https://arxiv.org/abs/2403.15837](https://arxiv.org/abs/2403.15837)

    使用中心掩蔽的GLIP技术在语言-图像预训练中取代了随机掩蔽，利用高斯分布提高了性能，并且易于获得且适用于不具有明显中心焦点的数据集。

    

    我们引入了用于语言-图像预训练（GLIP）的高斯掩蔽，这是一种新颖、直接和有效的技术，用于在视觉-语言模型的预训练过程中对图像补丁进行掩蔽。GLIP基于快速语言-图像预训练（FLIP），该方法在训练CLIP模型时随机屏蔽图像补丁。GLIP将随机屏蔽替换为中心掩蔽，使用高斯分布，并受到图像中心重要性的启发。在一系列下游数据集和任务中，GLIP保留了与FLIP相同的计算节省能力，同时改善了性能，这是由我们的实验结果所证实的。我们展示了GLIP的好处很容易获得，无需精细调整高斯，也适用于包含无明显中心焦点图片的数据集。

    arXiv:2403.15837v1 Announce Type: cross  Abstract: We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel, straightforward, and effective technique for masking image patches during pre-training of a vision-language model. GLIP builds on Fast Language-Image Pre-Training (FLIP), which randomly masks image patches while training a CLIP model. GLIP replaces random masking with centered masking, that uses a Gaussian distribution and is inspired by the importance of image patches at the center of the image. GLIP retains the same computational savings as FLIP, while improving performance across a range of downstream datasets and tasks, as demonstrated by our experimental results. We show the benefits of GLIP to be easy to obtain, requiring no delicate tuning of the Gaussian, and also applicable to data sets containing images without an obvious center focus.
    
[^77]: 大型语言模型在心理健康领域的机会和风险

    The opportunities and risks of large language models in mental health

    [https://arxiv.org/abs/2403.14814](https://arxiv.org/abs/2403.14814)

    大型语言模型在心理健康领域有望提供新颖的解决方案，但应注意其应用可能带来的风险，并积极采取策略减轻这些风险。

    

    全球心理健康问题的发生率正在上升，人们越来越意识到现有的心理保健模式无法充分扩展以满足需求。随着大型语言模型（LLMs）的出现，人们对它们具有创造新颖、大规模解决方案以支持心理健康的承诺感到乐观。尽管它们还处于初期阶段，LLMs已被应用于与心理健康相关的任务。本综述总结了已有文献中关于利用LLMs提供心理健康教育、评估和干预的努力，并突出了每个领域中产生积极影响的关键机会。然后，我们强调了将LLMs应用于心理健康领域所伴随的风险，并鼓励采用策略来减轻这些风险。对于心理健康支持的迫切需求必须与负责任的心理健康LLMs的开发、测试和部署相平衡。特别关键的是确保心理健康...

    arXiv:2403.14814v1 Announce Type: cross  Abstract: Global rates of mental health concerns are rising and there is increasing realization that existing models of mental healthcare will not adequately expand to meet the demand. With the emergence of large language models (LLMs) has come great optimism regarding their promise to create novel, large-scale solutions to support mental health. Despite their nascence, LLMs have already been applied to mental health-related tasks. In this review, we summarize the extant literature on efforts to use LLMs to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health LLMs. Especially critical is ensuring that mental he
    
[^78]: X-LLaVA: 优化双语大规模视觉语言对齐

    X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment

    [https://arxiv.org/abs/2403.11399](https://arxiv.org/abs/2403.11399)

    提出了两种成本有效的方法解决大规模多模态模型训练数据的挑战，并在英语-韩语-中文多语言、多模态训练数据集上开发了表现优越的双语多模态模型。

    

    大规模语言模型（LLMs）的显著发展正在扩展到大规模多模态模型（LMMs）的领域，这些模型集成了除文本以外的多种数据类型。然而，多模态模型的特性导致在创建训练数据方面存在显着的开销。此外，为LMMs构建多语言数据也面临着语言多样性和复杂性的挑战。因此，在这项研究中，我们提出了两种成本有效的方法来解决这个问题：（1）多语言LLM的词汇扩展和预训练，以及（2）使用GPT4-V自动和精心构建多模态数据集。基于这些方法，我们构建了一个包含91K英文-韩文-中文的多语言、多模态训练数据集。此外，我们开发了一个双语多模态模型，在韩语和英语中表现出卓越的性能，超过了现有方法。

    arXiv:2403.11399v1 Announce Type: new  Abstract: The impressive development of large language models (LLMs) is expanding into the realm of large multimodal models (LMMs), which incorporate multiple types of data beyond text. However, the nature of multimodal models leads to significant expenses in the creation of training data. Furthermore, constructing multilingual data for LMMs presents its own set of challenges due to language diversity and complexity. Therefore, in this study, we propose two cost-effective methods to solve this problem: (1) vocabulary expansion and pretraining of multilingual LLM for specific languages, and (2) automatic and elaborate construction of multimodal datasets using GPT4-V. Based on015 these methods, we constructed a 91K English-Korean-Chinese multilingual, multimodal training dataset. Additionally, we developed a bilingual multimodal model that exhibits excellent performance in both Korean and English, surpassing existing approaches.
    
[^79]: 超越静态评估：一种动态方法来评估人工智能助手的API调用能力

    Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants' API Invocation Capabilities

    [https://arxiv.org/abs/2403.11128](https://arxiv.org/abs/2403.11128)

    提出了自动动态评估（AutoDE）方法，用于评估人工智能助手的API调用能力，避免静态评估中导致的误导性评估。

    

    随着大型语言模型（LLMs）的兴起，人工智能助手利用工具的能力，特别是通过API调用，已经显著提升。这种进步需要更准确的评估方法。许多现有研究采用静态评估，即基于预定义的对话历史评估人工智能助手的API调用。然而，这种评估方法可能会具有误导性，因为在实际情况下，人工智能助手可能无法根据之前的人类互动生成API调用。我们提出了自动动态评估（AutoDE），以评估助手的API调用能力，而无需人类参与以及耗费大量资源的直接人机交互方法。在我们的框架中，我们努力模拟真实的人机交互中的人类对话模式，使用基于LLM的用户代理，并配备用户脚本以确保人机对齐。实验结果突显出AutoDE揭示了错误。

    arXiv:2403.11128v1 Announce Type: new  Abstract: With the rise of Large Language Models (LLMs), AI assistants' ability to utilize tools, especially through API calls, has advanced notably. This progress has necessitated more accurate evaluation methods. Many existing studies adopt static evaluation, where they assess AI assistants' API call based on pre-defined dialogue histories. However, such evaluation method can be misleading, as an AI assistant might fail in generating API calls from preceding human interaction in real cases. Instead of the resource-intensive method of direct human-machine interactions, we propose Automated Dynamic Evaluation (AutoDE) to assess an assistant's API call capability without human involvement. In our framework, we endeavor to closely mirror genuine human conversation patterns in human-machine interactions, using a LLM-based user agent, equipped with a user script to ensure human alignment. Experimental results highlight that AutoDE uncovers errors over
    
[^80]: Sabi\'a-2:一代新的葡萄牙大型语言模型

    Sabi\'a-2: A New Generation of Portuguese Large Language Models

    [https://arxiv.org/abs/2403.09887](https://arxiv.org/abs/2403.09887)

    Sabi'a-2是一代新的葡萄牙大型语言模型，其中的Sabi'a-2 Medium模型在多个考试中的表现超越了GPT-4，且在大多数考试中超过了GPT-3.5，同时专业化对模型的性能有显著影响，可在无需增大模型尺寸的情况下以比GPT-4便宜10倍的价格提供。

    

    我们介绍了Sabi'a-2，这是一族在葡萄牙文本上训练的大型语言模型。这些模型在多个考试中进行了评估，包括巴西大学的入学考试、专业认证考试以及各种学科（如会计、经济学、工程学、法律和医学）的研究生入学考试。我们的结果显示，到目前为止我们最优秀的模型Sabi'a-2 Medium，在64场考试中有23场与GPT-4的表现相匹敌或超越，并且在64场考试中有58场超过了GPT-3.5。值得注意的是，专业化对模型的性能有显著影响，无需增大模型尺寸，我们可以提供Sabi'a-2 Medium，每个记号的价格比GPT-4便宜10倍。最后，我们发现数学和编码是需要改进的关键能力。

    arXiv:2403.09887v1 Announce Type: cross  Abstract: We introduce Sabi\'a-2, a family of large language models trained on Portuguese texts. The models are evaluated on a diverse range of exams, including entry-level tests for Brazilian universities, professional certification exams, and graduate-level exams for various disciplines such as accounting, economics, engineering, law and medicine. Our results reveal that our best model so far, Sabi\'a-2 Medium, matches or surpasses GPT-4's performance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64 exams. Notably, specialization has a significant impact on a model's performance without the need to increase its size, allowing us to offer Sabi\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4. Finally, we identified that math and coding are key abilities that need improvement.
    
[^81]: ProSwitch：知识引导的语言模型微调，生成专业和非专业风格的文本

    ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text

    [https://arxiv.org/abs/2403.09131](https://arxiv.org/abs/2403.09131)

    ProSwitch通过知识引导的指令微调，在专业和非专业风格之间生成文本，并在专业性评估和质量评估方面表现出优越性。

    

    大语言模型（LLMs）在各种语言应用中表现出有效性，包括文本摘要和可控文本生成。然而，关于它们通过微调在不同风格间切换的能力的研究仍未被充分探讨。本研究聚焦于文本专业性，并引入了一种新颖的方法，名为ProSwitch，通过知识引导的指令微调，使语言模型具备生成专业和非专业回复的能力。ProSwitch分为三个阶段：数据准备，用于收集领域知识和训练语料库；指令微调，用于优化带有多种指令格式的语言模型；全面评估，用于评估生成文本的专业性区分能力和基于参考的质量。 ProSwitch相对于通用和专门语言模型的比较分析显示了我们的方法的优越性。

    arXiv:2403.09131v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including text summarization and controlled text generation. However, studies into their capacity of switching between styles via fine-tuning remain underexplored. This study concentrates on textual professionalism and introduces a novel methodology, named ProSwitch, which equips a language model with the ability to produce both professional and non-professional responses through knowledge-guided instruction tuning. ProSwitch unfolds across three phases: data preparation for gathering domain knowledge and training corpus; instruction tuning for optimizing language models with multiple levels of instruction formats; and comprehensive evaluation for assessing the professionalism discrimination and reference-based quality of generated text. Comparative analysis of ProSwitch against both general and specialized language models reveals that our appro
    
[^82]: NLPre: 一种面向自然语言处理系统的语言中心基准测试方法的改进

    NLPre: a revised approach towards language-centric benchmarking of Natural Language Preprocessing systems

    [https://arxiv.org/abs/2403.04507](https://arxiv.org/abs/2403.04507)

    NLPre提出了一种可靠且公平的语言中心基准测试方法，使得可以全面持续评估多个NLPre工具的性能，并可靠地跟踪其表现。

    

    随着基于变压器的架构的不断发展，我们观察到自然语言处理（NLPre）工具的崛起，这些工具能够解决初步的NLP任务（例如标记化、词性标注、依存句法分析或形态分析）而无需任何外部语言指导。在费时费力地比较新型解决方案与依赖基于规则的形态分析器或词典的成熟预处理工具包的基础上是很困难的。鉴于现有NLPre评估方法的缺陷，我们探讨了一种可靠和公平的评估方法和性能报告。受GLUE基准测试的启发，提出的语言中心基准测试系统实现了对多个NLPre工具的全面持续评估，同时可靠地跟踪它们的性能。原型应用程序被配置为波兰语，并与精心组织的NLPre-PL基准套件集成。基于这一基准，我们进行了一项实验...

    arXiv:2403.04507v1 Announce Type: new  Abstract: With the advancements of transformer-based architectures, we observe the rise of natural language preprocessing (NLPre) tools capable of solving preliminary NLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or morphological analysis) without any external linguistic guidance. It is arduous to compare novel solutions to well-entrenched preprocessing toolkits, relying on rule-based morphological analysers or dictionaries. Aware of the shortcomings of existing NLPre evaluation approaches, we investigate a novel method of reliable and fair evaluation and performance reporting. Inspired by the GLUE benchmark, the proposed language-centric benchmarking system enables comprehensive ongoing evaluation of multiple NLPre tools, while credibly tracking their performance. The prototype application is configured for Polish and integrated with the thoroughly assembled NLPre-PL benchmark. Based on this benchmark, we conduct an ex
    
[^83]: NaturalSpeech 3: 利用分解编解码器和扩散模型实现零-shot语音合成

    NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models

    [https://arxiv.org/abs/2403.03100](https://arxiv.org/abs/2403.03100)

    NaturalSpeech 3利用分解设计的扩散模型实现零-shot方式生成自然语音

    

    近期大规模文本到语音（TTS）模型取得了显著进展，然而在语音质量、相似度和韵律方面仍存在不足。鉴于语音复杂地包含各种属性（例如内容、韵律、音色和声学细节），给生成带来了重大挑战，一个自然的想法是将语音因子分解为代表不同属性的各个子空间，并单独生成它们。在此基础上，我们提出了NaturalSpeech 3，这是一个具有新颖的分解扩散模型的TTS系统，可以以零-shot方式生成自然语音。具体来说，1) 我们设计了一个具有分解向量量化（FVQ）的神经编解码器，将语音波形分解为内容、韵律、音色和声学细节的子空间；2) 我们提出了一个分解扩散模型，根据其相应的提示生成每个子空间中的属性。借助这种分解设计，NaturalSpeech 3能够ef

    arXiv:2403.03100v1 Announce Type: cross  Abstract: While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can ef
    
[^84]: SoftTiger: 用于医疗工作流的临床基础模型

    SoftTiger: A Clinical Foundation Model for Healthcare Workflows

    [https://arxiv.org/abs/2403.00868](https://arxiv.org/abs/2403.00868)

    SoftTiger是一个专为医疗工作流设计的临床大型语言模型，通过处理临床笔记的结构化，实现了基本临床任务以及更复杂的下游临床任务的执行。

    

    我们发布并介绍了SoftTiger，一个专为医疗保健工作流设计的临床大型语言模型（CLaM）作为基础模型。临床笔记的叙述性和非结构化特性是医疗智能化的主要障碍。我们致力于按照国际互操作性标准将临床笔记结构化为临床数据，涉及国际患者摘要、临床印象和医疗接触三个关键子任务的数据收集和标注。然后，我们使用公开和验证的临床数据对最先进的LLM进行监督微调。训练过程中，目标模型首先能够支持基本的临床任务，如缩写扩展和时间信息提取，然后学习执行更复杂的下游临床任务，如印象和接触摘要。此外，我们解决了医疗模型中的一些建模挑战。

    arXiv:2403.00868v1 Announce Type: cross  Abstract: We release and introduce SoftTiger, a clinical large language model (CLaM) designed as a foundation model for healthcare workflows. The narrative and unstructured nature of clinical notes is a major obstacle for healthcare intelligentization. We address a critical problem of structuring clinical notes into clinical data, according to international interoperability standards. We collect and annotate data for three critical subtasks, namely, international patient summary, clinical impression and medical encounter. We then supervised fine-tuned a state-of-the-art LLM using public and credentialed clinical data. The training is orchestrated in a way that the target model can first support basic clinical tasks such as abbreviation expansion and temporal information extraction, and then learn to perform more complex downstream clinical tasks such as impression and encounter summary. Moreover, we address, several modeling challenges in the he
    
[^85]: Agent-Pro: 通过策略级别反思和优化学习进化

    Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization

    [https://arxiv.org/abs/2402.17574](https://arxiv.org/abs/2402.17574)

    Agent-Pro提出了一种基于LLM的代理，通过策略级别的反思和优化，可以从互动经验中学习并逐步提升其行为策略。

    

    大型语言模型表现出在各种任务中具有强大问题解决能力。然而，大多数基于LLM的代理都是特定任务求解器，并具有复杂的提示工程，而不是能够通过互动学习和进化的代理。本文提出了Agent-Pro：一种基于LLM的代理，具有策略级别的反思和优化，可以从互动经验中学习丰富的专业知识，并逐渐提升其行为策略。具体来说，它涉及一个动态信念生成和反思过程，用于策略演化。

    arXiv:2402.17574v1 Announce Type: new  Abstract: Large Language Models exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover
    
[^86]: 探究多模态大型语言模型对全局和局部语义表示的影响

    Probing Multimodal Large Language Models for Global and Local Semantic Representation

    [https://arxiv.org/abs/2402.17304](https://arxiv.org/abs/2402.17304)

    通过研究发现，多模态大型语言模型的中间层能够更好地编码全局语义信息，在视觉-语言任务中表现出更好的性能。顶层可能过多关注局部信息，导致理解全局信息的能力下降。

    

    大型语言模型的成功启发了研究人员将其优秀的表示能力转移到其他模态。最近的一些研究利用图像描述对齐数据集训练多模态大型语言模型（MLLMs），在图像到文本任务中取得了最新的性能表现。然而，很少有研究探讨MLLMs是否真正理解完整的图像信息，即全局信息，或者它们只能捕捉一些局部对象信息。本研究发现模型的中间层可以编码更多全局语义信息，其表示向量在视觉-语言蕴涵任务上表现更好，而不是顶层。我们通过目标检测任务进一步探究模型的局部语义表示。我们得出的结论是顶层可能过多专注于局部信息，导致减弱了对全局信息的理解能力。

    arXiv:2402.17304v1 Announce Type: cross  Abstract: The success of large language models has inspired researchers to transfer their exceptional representing ability to other modalities. Several recent works leverage image-caption alignment datasets to train multimodal large language models (MLLMs), which achieve state-of-the-art performance on image-to-text tasks. However, there are very few studies exploring whether MLLMs truly understand the complete image information, i.e., global information, or if they can only capture some local object information. In this study, we find that the intermediate layers of models can encode more global semantic information, whose representation vectors perform better on visual-language entailment tasks, rather than the topmost layers. We further probe models for local semantic representation through object detection tasks. And we draw a conclusion that the topmost layers may excessively focus on local information, leading to a diminished ability to en
    
[^87]: 在跳槽之前三思：问题细化提示改善大型语言模型的数学推理能力

    Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models

    [https://arxiv.org/abs/2402.15764](https://arxiv.org/abs/2402.15764)

    PEP提出了一个新方法来改善LLMs的数学能力，通过在推理之前细化和阐明问题背景，提升全局上下文建模能力，减少解析困难。

    

    大型语言模型（LLMs）在自然语言处理任务中表现出色，但在复杂推理任务中仍面临挑战，并且对输入上下文敏感。本研究提出了一种新的方法，名为问题细化提示（PEP），旨在在推理之前分解和阐明问题背景，从而增强全局上下文建模和减少解析困难。实验结果表明，PEP在复杂推理任务上表现出色，对于问题提出的效果显著。

    arXiv:2402.15764v1 Announce Type: cross  Abstract: Large language models~(LLMs) have exhibited impressive performance across NLP tasks. So far they still face challenges in complex reasoning tasks and can be sensitive to input context. Despite significant efforts have been invested in enhancing reasoning process and improving prefix-prompts robustness, the crucial role of problem context has been overlooked. In this study, we propose a new approach to improve the mathematical capacities of LLMs, named Problem Elaboration Prompting~(PEP). Specifically, PEP decomposes and elucidates the problem context before reasoning, thus enhancing the global context modeling and reducing the parsing difficulties. Experiments on datasets demonstrate promising performances on complex reasoning and indicate the beneficial impact for ill-formed problems. For instance, with the GPT-3.5 model~(\texttt{text-davinci-003}), we observed a 9.93\% improvement with greedy decoding and 8.80\% improvement with self
    
[^88]: 结构引导的大型语言模型用于SQL生成

    Structure Guided Large Language Model for SQL Generation

    [https://arxiv.org/abs/2402.13284](https://arxiv.org/abs/2402.13284)

    通过引入结构信息，提出了一个结构引导的SQL生成模型，以改善大型语言模型生成SQL的准确性和可执行性。

    

    生成准确的结构化查询语言（SQL）是一个长期存在的问题，特别是在将用户的语义查询与结构化数据库匹配，然后生成结构化SQL方面。现有模型通常将查询和数据库模式输入到LLM中，并依赖LLM执行语义-结构匹配并生成结构化SQL。然而，这种解决方案忽略了用户查询和数据库中的结构信息，而这些信息可以用来增强结构化SQL的生成。这一疏忽可能导致不准确或无法执行的SQL生成。为了充分利用结构，我们提出了一个结构到SQL的框架，利用固有的结构信息来改善LLM的SQL生成。具体地，我们介绍了我们的结构引导SQL（SGU-SQL）生成模型。

    arXiv:2402.13284v1 Announce Type: cross  Abstract: Generating accurate Structured Querying Language (SQL) is a long-standing problem, especially in matching users' semantic queries with structured databases and then generating structured SQL. Existing models typically input queries and database schemas into the LLM and rely on the LLM to perform semantic-structure matching and generate structured SQL. However, such solutions overlook the structural information within user queries and databases, which can be utilized to enhance the generation of structured SQL. This oversight can lead to inaccurate or unexecutable SQL generation. To fully exploit the structure, we propose a structure-to-SQL framework, which leverages the inherent structure information to improve the SQL generation of LLMs. Specifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model. SGU-SQL first links user queries and databases in a structure-enhanced manner. It then decomposes complicated linked str
    
[^89]: 朝着可信的再排序：一种简单但有效的弃权机制

    Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism

    [https://arxiv.org/abs/2402.12997](https://arxiv.org/abs/2402.12997)

    提出了一种适用于现实约束的轻量级弃权机制，特别适用于再排序阶段，通过数据驱动的方法达到有效性，并提供了开源代码以促进其更广泛的应用。

    

    神经信息检索（NIR）已经显著改进了基于启发式的IR系统。然而，失败仍然频繁发生，通常所使用的模型无法检索与用户查询相关的文档。我们通过提出一种适用于现实约束的轻量级弃权机制来解决这一挑战，特别强调再排序阶段。我们介绍了一个协议，用于在黑匣子场景中评估弃权策略的效果，并提出了一种简单但有效的数据驱动机制。我们提供了实验复制和弃权实施的开源代码，促进其在不同环境中更广泛的采用和应用。

    arXiv:2402.12997v1 Announce Type: cross  Abstract: Neural Information Retrieval (NIR) has significantly improved upon heuristic-based IR systems. Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user's query. We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the reranking phase. We introduce a protocol for evaluating abstention strategies in a black-box scenario, demonstrating their efficacy, and propose a simple yet effective data-driven mechanism. We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts.
    
[^90]: GPT-4在基于USMLE的案例研究中的表现评估

    GPT-4's assessment of its performance in a USMLE-based case study

    [https://arxiv.org/abs/2402.09654](https://arxiv.org/abs/2402.09654)

    本研究探讨了GPT-4在医疗应用中的表现评估。实验结果表明，反馈对相对置信度有影响，但并不一致地增加或减少。

    

    本研究调查GPT-4在医疗应用中的表现评估。通过使用简单的提示技术，从美国医学执照考试（USMLE）问卷中提取问题的方式，任务是评估模型在提问之前和提问之后的置信度得分。问卷根据是否有反馈分为两组：反馈组（WF）和无反馈组（NF）。要求模型在每个问题之前和之后提供绝对和相对置信度得分。通过使用统计工具分析实验结果，研究了WF和NF组的置信度变异性。此外，进行了顺序分析以观察WF和NF组的性能变化。结果表明，反馈会影响相对置信度，但并不总是增加或减少。

    arXiv:2402.09654v1 Announce Type: new  Abstract: This study investigates GPT-4's assessment of its performance in healthcare applications. A simple prompting technique was used to prompt the LLM with questions taken from the United States Medical Licensing Examination (USMLE) questionnaire and it was tasked to evaluate its confidence score before posing the question and after asking the question. The questionnaire was categorized into two groups-questions with feedback (WF) and questions with no feedback(NF) post-question. The model was asked to provide absolute and relative confidence scores before and after each question. The experimental findings were analyzed using statistical tools to study the variability of confidence in WF and NF groups. Additionally, a sequential analysis was conducted to observe the performance variation for the WF and NF groups. Results indicate that feedback influences relative confidence but doesn't consistently increase or decrease it. Understanding the p
    
[^91]: 攻击、防御和评估LLM对话安全性的调查

    Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey

    [https://arxiv.org/abs/2402.09283](https://arxiv.org/abs/2402.09283)

    这篇调查提供了LLM对话安全性的全面概述，涵盖了攻击、防御和评估三个关键方面，旨在提高对该主题的理解并促进进一步的研究。

    

    arXiv:2402.09283v1 公告类型: 新的摘要: 大型语言模型（LLMs）在对话应用中已经很常见。然而，它们可能被误用生成有害回复的风险引起了严重的社会关切，并激发了LLM对话安全性的最新研究。因此，在此调查中，我们提供了最近研究的全面概述，涵盖了LLM对话安全性的三个关键方面：攻击、防御和评估。我们的目标是提供一个结构化的摘要，增进对LLM对话安全性的理解，并鼓励进一步研究这一重要课题。为了方便参考，我们根据我们的分类法对所有在此调查中提到的研究进行了分类，可在以下网址找到：https://github.com/niconi19/LLM-conversation-safety。

    arXiv:2402.09283v1 Announce Type: new Abstract: Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.
    
[^92]: OpenMoE：开源混合专家语言模型的早期努力

    OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models

    [https://arxiv.org/abs/2402.01739](https://arxiv.org/abs/2402.01739)

    OpenMoE是一种开源的混合专家语言模型，通过训练和发布一系列具有可复现性的解码器模型，我们确认了MoE模型相比密集模型具有更有利的成本效益平衡，并且进行了对路由机制的深入分析，得出了三个重要发现。

    

    为了帮助开源社区更好地理解基于混合专家(MoE)的大型语言模型(LLM)，我们训练并发布了OpenMoE，一系列完全开放源码和可复现的仅解码器MoE LLM，参数范围从650M到34B，训练数据超过1T个标记。我们的研究证实，MoE-based LLM可以提供比密集LLM更有利的成本效益平衡，突出了未来LLM开发的潜在有效性。本研究的另一个重要贡献是对我们的OpenMoE模型中的路由机制进行深入分析，得到了三个重要发现：上下文无关专业化、早期路由学习和末尾降低。我们发现，MoE模型中的路由决策主要基于标记ID，与上下文相关性很小。标记到专家的分配在预训练阶段早期确定，并且基本保持不变。这种不完全的路由可能导致...

    To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development.   One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can resu
    
[^93]: 基于批处理的基础模型低秩调整

    Batched Low-Rank Adaptation of Foundation Models

    [https://arxiv.org/abs/2312.05677](https://arxiv.org/abs/2312.05677)

    提出了Fast LoRA（FLoRA）框架，使得基础模型的低秩调整可以高效批处理异构请求，并在绩效上保持竞争性。

    

    最近，低秩适应（LoRA）因通过引入可训练的低秩矩阵微调基础模型并减少可训练参数的数量而引起关注。虽然LoRA提供了许多优点，但其在实时为各种全球用户提供服务时无法高效处理多个特定任务适配器的能力受到限制。这为需要为每个传入请求个性化、特定任务适应的场景中造成了性能瓶颈。为了减轻这一约束，我们提出了快速LoRA（FLoRA）框架，其中批处理中的每个输入示例都可以与其独特的低秩适应权重相关联，从而实现对异构请求的高效批处理。我们通过实证表明，FLoRA保留了LoRA的绩效优点，在跨越8种语言的MultiPL-E代码生成基准测试上展示出竞争结果。

    arXiv:2312.05677v2 Announce Type: replace-cross  Abstract: Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While LoRA offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request. To mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that FLoRA retains the performance merits of LoRA, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 8 languages and 
    
[^94]: PEMA：一种可在离线调整的外部插件内存自适应语言模型

    PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models

    [https://arxiv.org/abs/2311.08590](https://arxiv.org/abs/2311.08590)

    PEMA是一种参数有效的微调方法，通过插件外部内存自适应实现了对预训练语言模型的微调，绕过了对所有权重的访问需求，同时利用外部内存和适配器权重矩阵来提高效率。

    

    预训练语言模型（PLM）在各种下游自然语言处理任务中表现出色。然而，训练大型语言模型需要大量内存和计算资源。由于资源需求巨大，许多PLM权重是机密的，用户被迫将其数据与模型所有者共享，以便为特定任务进行微调。为了克服这些限制，我们引入了插件外部内存自适应（PEMA），一种参数有效的微调（PEFT）方法，实现了在不需要访问所有权重的情况下对PLM进行微调。PEMA在推理期间集成了来自测试数据的上下文表示以执行下游任务。它使用外部内存存储由PLM生成的上下文表示与目标标记相映射。我们的方法利用了PLM最终层中类似LoRA的瓶颈适配器的权重矩阵来提高效率。

    arXiv:2311.08590v2 Announce Type: replace  Abstract: Pre-trained language models (PLMs) show impressive performance in various downstream NLP tasks. However, pre-training large language models demands substantial memory and training compute. Furthermore, due to the substantial resources required, many PLM weights are confidential. Consequently, users are compelled to share their data with model owners for fine-tuning specific tasks. To overcome the limitations, we introduce Plug-in External Memory Adaptation (PEMA), a Parameter-Efficient Fine-Tuning (PEFT) method, enabling PLM fine-tuning without requiring access to all the weights. PEMA integrates with context representations from test data during inference to perform downstream tasks. It uses external memory to store PLM-generated context representations mapped with target tokens. Our method utilizes weight matrices of LoRA-like bottlenecked adapter in the PLM's final layer to enhance efficiency. Our approach also includes Gradual Un
    
[^95]: 伪装成羊的狼：普遍的嵌套越狱提示可以轻松愚弄大型语言模型

    A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily

    [https://arxiv.org/abs/2311.08268](https://arxiv.org/abs/2311.08268)

    提出一种利用大型语言模型自动生成有效的越狱提示的自动框架ReNeLLM，显著提高攻击成功率，同时大大减少时间成本。

    

    大型语言模型（LLMs），如ChatGPT和GPT-4，旨在提供有用和安全的响应。然而，被称为“越狱”的对抗性提示可以规避保障措施，导致LLMs生成潜在有害内容。探索越狱提示可以帮助更好地揭示LLMs的弱点，并进一步引导我们安全地保护它们。不幸的是，现有的越狱方法要么遭受复杂的手工设计，要么需要在其他白盒模型上进行优化，从而损害了泛化性或效率。在本文中，我们将越狱提示攻击概括为两个方面：（1）提示重写和（2）场景嵌套。基于此，我们提出了ReNeLLM，一个利用LLMs自身生成有效越狱提示的自动框架。大量实验证明，与现有基线相比，ReNeLLM显著提高了攻击成功率，同时大大减少了时间成本。

    arXiv:2311.08268v2 Announce Type: replace  Abstract: Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, compromising generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Ou
    
[^96]: LLatrieval：LLM-验证检索用于可验证生成

    LLatrieval: LLM-Verified Retrieval for Verifiable Generation

    [https://arxiv.org/abs/2311.07838](https://arxiv.org/abs/2311.07838)

    可验证生成中检索的文件不仅帮助LLM生成正确答案，还作为用户验证LLM输出的证据，但目前广泛使用的检索器已成为性能瓶颈，需要解决。

    

    可验证生成旨在使大型语言模型（LLM）生成具有支撑文件的文本，这使用户能够灵活验证答案，并使LLM的输出更可靠。检索在可验证生成中起着至关重要的作用。具体而言，检索到的文件不仅补充知识以帮助LLM生成正确答案，还作为支持用户验证LLM输出的证据。然而，广泛使用的检索器成为整个流程的瓶颈，并限制了整体性能。由于通常具有的参数比大型语言模型少得多，并且尚未证明能够良好地扩展到LLM的规模，因此它们的能力通常比LLMs差。如果检索器未能正确找到支持文件，则LLM将无法生成正确和可验证的答案，这会掩盖LLM的显著能力。为解决这些问

    arXiv:2311.07838v2 Announce Type: replace  Abstract: Verifiable generation aims to let the large language model (LLM) generate text with supporting documents, which enables the user to flexibly verify the answer and makes the LLM's output more reliable. Retrieval plays a crucial role in verifiable generation. Specifically, the retrieved documents not only supplement knowledge to help the LLM generate correct answers, but also serve as supporting evidence for the user to verify the LLM's output. However, the widely used retrievers become the bottleneck of the entire pipeline and limit the overall performance. Their capabilities are usually inferior to LLMs since they often have much fewer parameters than the large language model and have not been demonstrated to scale well to the size of LLMs. If the retriever does not correctly find the supporting documents, the LLM can not generate the correct and verifiable answer, which overshadows the LLM's remarkable abilities. To address these li
    
[^97]: 最近少样本学习集中的命名实体识别和关系分类方法调研

    A Few-Shot Learning Focused Survey on Recent Named Entity Recognition and Relation Classification Methods

    [https://arxiv.org/abs/2310.19055](https://arxiv.org/abs/2310.19055)

    调研了最近应用于命名实体识别和关系分类的深度学习模型中少样本学习的性能表现。

    

    命名实体识别（NER）和关系分类（RC）是从非结构化文本中提取信息并将其格式化为机器可读格式的重要步骤。我们介绍了最近解决命名实体识别和关系分类问题的深度学习模型的调研，重点关注少样本学习性能。我们的调研有助于研究人员了解文本挖掘和从原始文本中提取结构化信息的最新技术。

    arXiv:2310.19055v2 Announce Type: replace  Abstract: Named Entity Recognition (NER) and Relation Classification (RC) are important steps in extracting information from unstructured text and formatting it into a machine-readable format. We present a survey of recent deep learning models that address named entity recognition and relation classification, with focus on few-shot learning performance. Our survey is helpful for researchers in knowing the recent techniques in text mining and extracting structured information from raw text.
    
[^98]: 从文本到来源: 在检测大型语言模型生成内容方面的结果

    From Text to Source: Results in Detecting Large Language Model-Generated Content

    [https://arxiv.org/abs/2309.13322](https://arxiv.org/abs/2309.13322)

    本研究探讨了跨模型检测方法，在评估分类器是否能检测来自目标LLM的文本方面取得了关键发现

    

    大型语言模型（LLMs）的广泛应用引起了对信息错误和道德影响的担忧。为了解决这些问题，需要开发稳健的方法来检测和归因LLMs生成的文本。本文研究了“跨模型检测”，评估了一个经过训练以区分源LLM生成的文本和人工编写文本的分类器是否也能检测目标LLM生成的文本，而无需进一步训练。该研究全面探讨了不同大小和家族的LLMs，评估了对分类器泛化的对话微调技术、量化和水印的影响。研究还探讨了模型归因，包括源模型识别、模型家族和模型大小分类，以及量化和水印检测。我们的结果揭示了几个关键发现：

    arXiv:2309.13322v2 Announce Type: replace  Abstract: The widespread use of Large Language Models (LLMs), celebrated for their ability to generate human-like text, has raised concerns about misinformation and ethical implications. Addressing these concerns necessitates the development of robust methods to detect and attribute text generated by LLMs. This paper investigates "Cross-Model Detection," by evaluating whether a classifier trained to distinguish between source LLM-generated and human-written text can also detect text from a target LLM without further training. The study comprehensively explores various LLM sizes and families, and assesses the impact of conversational fine-tuning techniques, quantization, and watermarking on classifier generalization. The research also explores Model Attribution, encompassing source model identification, model family, and model size classification, in addition to quantization and watermarking detection. Our results reveal several key findings: a
    
[^99]: GlotScript：一种用于低资源书写系统识别的资源和工具

    GlotScript: A Resource and Tool for Low Resource Writing System Identification

    [https://arxiv.org/abs/2309.13320](https://arxiv.org/abs/2309.13320)

    GlotScript是一种用于低资源书写系统识别的资源和工具，提供了7000多种语言的经过验证的书写系统，同时涵盖了所有161种Unicode 15.0脚本，并能帮助清理多语种语料库以及分析不同语言模型对低资源脚本和语言的覆盖。

    

    我们介绍了GlotScript，这是一个用于低资源书写系统识别的开放资源和工具。GlotScript-R是一个资源，提供了7000多种语言的经过验证的书写系统。它通过整合现有的书写系统资源中的信息进行编制。GlotScript-T是一个书写系统识别工具，涵盖了所有161种Unicode 15.0脚本。对于输入文本，它返回其脚本分布，其中脚本由ISO 15924代码标识。我们还展示了GlotScript的两个用例。首先，我们演示了GlotScript如何帮助清理多语种语料库，例如mC4和OSCAR。其次，我们使用GlotScript分析了多个语言模型的标记化，例如GPT-4，并提供了关于每个语言模型覆盖低资源脚本和语言的见解。希望GlotScript能成为自然语言处理社区中针对低资源语言工作的有用资源。

    arXiv:2309.13320v2 Announce Type: replace  Abstract: We present GlotScript, an open resource and tool for low resource writing system identification. GlotScript-R is a resource that provides the attested writing systems for more than 7,000 languages. It is compiled by aggregating information from existing writing system resources. GlotScript-T is a writing system identification tool that covers all 161 Unicode 15.0 scripts. For an input text, it returns its script distribution where scripts are identified by ISO 15924 codes. We also present two use cases for GlotScript. First, we demonstrate that GlotScript can help cleaning multilingual corpora such as mC4 and OSCAR. Second, we analyze the tokenization of a number of language models such as GPT-4 using GlotScript and provide insights on the coverage of low resource scripts and languages by each language model. We hope that GlotScript will become a useful resource for work on low resource languages in the NLP community. GlotScript-R an
    
[^100]: 欺骗LLMs让其不遵从：正式化、分析和检测越狱行为

    Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks

    [https://arxiv.org/abs/2305.14965](https://arxiv.org/abs/2305.14965)

    该论文提出了正式化和已知越狱分类法以填补对商用大规模语言模型（LLMs）被越狱攻击的缺乏研究，调查现有的越狱方法及其在开源和商用LLMs上的有效性。

    

    最近对商用大规模语言模型（LLMs）的探索表明，非专家用户可以通过简单操纵他们的提示来越狱LLMs；导致退化的输出行为、隐私与安全漏洞、冒犯性输出以及违反内容监管政策。有限的研究已进行了对这些攻击及其缓解措施的正式化和分析。我们通过提出一个形式化描述和已知（及可能的）越狱分类法来填补这一差距。我们调查现有的越狱方法及其在开源和商用LLMs（如基于GPT的模型、OPT、BLOOM和FLAN-T5-XXL）上的有效性。我们进一步讨论了越狱检测在针对已知攻击方面的挑战。为了我们的分析，我们收集了4项任务的3700个越狱提示的数据集。我们将随着模型输出一起公开这一数据集。

    arXiv:2305.14965v2 Announce Type: replace  Abstract: Recent explorations with commercial Large Language Models (LLMs) have shown that non-expert users can jailbreak LLMs by simply manipulating their prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies. Limited studies have been conducted to formalize and analyze these attacks and their mitigations. We bridge this gap by proposing a formalism and a taxonomy of known (and possible) jailbreaks. We survey existing jailbreak methods and their effectiveness on open-source and commercial LLMs (such as GPT-based models, OPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak detection in terms of their effectiveness against known attacks. For our analysis, we collect a dataset of 3700 jailbreak prompts across 4 tasks. We will make the dataset public along with the model outputs.
    
[^101]: 比较以人为中心的语言建模：模拟群体、个体特点还是两者兼顾？

    Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?. (arXiv:2401.12492v1 [cs.CL])

    [http://arxiv.org/abs/2401.12492](http://arxiv.org/abs/2401.12492)

    本研究比较了以群体属性、个体用户和组合方法来模拟人的上下文。合并群体和个体特征显著提高了用户级回归任务的性能，而模拟个体用户则显著提高了单个文档级分类任务的性能。

    

    自然语言处理在将人的上下文纳入其模型中取得了进展，但使用群体属性（如45岁以上的人群）还是模拟个体人物更有效的问题尚未确定。群体属性在技术上更容易实现，但是过于粗糙：并非所有45岁以上的人都以相同的方式书写。相反，模拟个体人物能够捕捉每个人身份的复杂性，允许更个性化的表示，但我们可能需要模拟无限数量的用户并且需要可能无法获取的数据。我们比较了通过群体属性、个体用户和组合方法来模拟人的上下文。将群体和个体特征结合起来，显著提高了基于用户文档的用户级回归任务（如年龄估计或个性评估）的性能。模拟个体用户显著提高了单个文档级分类任务（如立场和主题检测）的性能。

    Natural language processing has made progress in incorporating human context into its models, but whether it is more effective to use group-wise attributes (e.g., over-45-year-olds) or model individuals remains open. Group attributes are technically easier but coarse: not all 45-year-olds write the same way. In contrast, modeling individuals captures the complexity of each person's identity. It allows for a more personalized representation, but we may have to model an infinite number of users and require data that may be impossible to get. We compare modeling human context via group attributes, individual users, and combined approaches. Combining group and individual features significantly benefits user-level regression tasks like age estimation or personality assessment from a user's documents. Modeling individual users significantly improves the performance of single document-level classification tasks like stance and topic detection. We also find that individual-user modeling does w
    
[^102]: 使用样式表示进行机器生成文本的小样本检测

    Few-Shot Detection of Machine-Generated Text using Style Representations. (arXiv:2401.06712v1 [cs.CL])

    [http://arxiv.org/abs/2401.06712](http://arxiv.org/abs/2401.06712)

    本研究提出了一种小样本检测方法，通过使用样式表示来检测机器生成的文本与人类撰写的文本的区别，以解决滥用语言模型带来的问题。

    

    受到指导调整的语言模型的出现使得人类写作的逼真模仿面临着重大滥用风险。然而，我们可以通过检测一段文本是由语言模型还是人类撰写而成来对抗此类滥用行为。本文提出了一种基于样式表示的小样本检测方法，避免了神经网络检测器在面对数据转换时的规约不足的挑战，同时也避免了在推理或检测时需要访问可能生成文档的模型的问题。

    The advent of instruction-tuned language models that convincingly mimic human writing poses a significant risk of abuse. For example, such models could be used for plagiarism, disinformation, spam, or phishing. However, such abuse may be counteracted with the ability to detect whether a piece of text was composed by a language model rather than a human. Some previous approaches to this problem have relied on supervised methods trained on corpora of confirmed human and machine-written documents. Unfortunately, model under-specification poses an unavoidable challenge for neural network-based detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors. Other previous approaches require access to the models that may have generated a document in question at inference or detection time, which is often impractical. In light of these challenges, we pursue a fundamentally d
    
[^103]: EASYTOOL: 使用简洁的工具指示增强基于LLM的代理

    EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction. (arXiv:2401.06201v1 [cs.CL])

    [http://arxiv.org/abs/2401.06201](http://arxiv.org/abs/2401.06201)

    EASYTOOL是一个将多样化而冗长的工具文档转化为统一而简洁的工具指示的框架，用于增强基于LLM的代理的能力。通过从多个来源提取关键信息，并提供标准化的工具描述和功能，EasyTool显著降低了标记消耗，并提高了在真实场景中的工具利用性能。

    

    为了解决复杂的现实世界任务，越来越多的关注点放在了在大型语言模型(LLM)应用中的工具利用上。为了开发基于LLM的代理，通常需要LLM从不同的工具文档中理解许多工具功能。但这些文档可能是多样化的、冗余的或不完整的，这极大地影响了LLM在使用工具方面的能力。为了解决这个问题，我们介绍了EASYTOOL，这是一个将多样化而冗长的工具文档转化为统一而简洁的工具指示，以便更容易地使用工具。EasyTool从不同来源的广泛工具文档中提取出关键信息，并详细说明一个统一的接口（即工具指示），为基于LLM的代理提供标准化的工具描述和功能。对多个不同任务的广泛实验表明，EasyTool可以显著减少标记的消耗，并改善在现实场景中的工具利用性能。

    To address intricate real-world tasks, there has been a rising interest in tool utilization in applications of large language models (LLMs). To develop LLM-based agents, it usually requires LLMs to understand many tool functions from different tool documentation. But these documentations could be diverse, redundant or incomplete, which immensely affects the capability of LLMs in using tools. To solve this, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise tool instruction for easier tool usage. EasyTool purifies essential information from extensive tool documentation of different sources, and elaborates a unified interface (i.e., tool instruction) to offer standardized tool descriptions and functionalities for LLM-based agents. Extensive experiments on multiple different tasks demonstrate that EasyTool can significantly reduce token consumption and improve the performance of tool utilization in real-world scenarios. Our co
    
[^104]: 自我对比：通过不一致的求解视角获得更好的反思能力

    Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives. (arXiv:2401.02009v1 [cs.CL])

    [http://arxiv.org/abs/2401.02009](http://arxiv.org/abs/2401.02009)

    自我对比是一种通过对比不同求解视角和总结差异，提高大型语言模型（LLM）的反思能力的方法。

    

    大型语言模型（LLM）的反思能力引起了广泛关注。一种事后提示策略，例如反思和自我改进，根据自我评估或外部反馈来改善LLM的响应。然而，最近的研究表明，在没有外部反馈的情况下，LLM的内在反思是不稳定的。我们的调查揭示了自我评估反馈质量是关键瓶颈。我们发现LLM在自我评估时常常表现出过度自信或高度随机性，提供固执或不一致的反馈，导致反思能力不佳。为了解决这个问题，我们提出了自我对比的方法：它根据请求自适应地探索多样的求解视角，对比差异，并将这些差异总结为一个检查表，用于重新审视和消除差异。我们的方法赋予LLM多样的视角以减轻固执偏见。此外，差异指示了潜在的错误或固有的不确定性。

    The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties tha
    
[^105]: 基于检索增强的大型语言模型：一项调研

    Retrieval-Augmented Generation for Large Language Models: A Survey. (arXiv:2312.10997v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.10997](http://arxiv.org/abs/2312.10997)

    本综述论文调查了基于检索增强的大型语言模型的发展，包括三个主要范式：Naive RAG、Advanced RAG和Modular RAG。RAG通过整合外部数据库的知识，增强模型的准确性和可信度，并实现了持续更新知识和整合领域特定信息的功能。

    

    大型语言模型（LLMs）展示了显著的能力，但面临幻觉、过时知识和非透明、不可追溯的推理过程等挑战。检索增强生成（RAG）已经成为一种有前途的解决方案，通过整合来自外部数据库的知识，增强模型的准确性和可信度，特别适用于知识密集型任务，并允许持续更新知识和整合领域特定信息。RAG将LLMs自身的知识与庞大、动态的外部数据库相结合，实现协同效应。本综述论文详细考察了RAG范式的发展，包括Naive RAG、Advanced RAG和Modular RAG。它详细审视了RAG框架的三个基本要素，包括检索、生成和增强技术。本文强调了嵌入在RAG框架中的最新技术。

    Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in e
    
[^106]: CARE: 用于联合实体和关系抽取的共同注意力网络

    CARE: Co-Attention Network for Joint Entity and Relation Extraction. (arXiv:2308.12531v1 [cs.CL])

    [http://arxiv.org/abs/2308.12531](http://arxiv.org/abs/2308.12531)

    本论文提出了一种用于联合实体和关系抽取的共同注意力网络（CARE），通过学习分离表示和双向交互来解决特征混淆和子任务交互不足的问题，实验表明该模型在多个基准数据集上具有优越性能。

    

    联合实体和关系抽取是信息抽取的基本任务，包括两个子任务：命名实体识别和关系抽取。大多数现有的联合抽取方法都存在特征混淆或两个子任务之间交互不足的问题。在这项工作中，我们提出了一种用于联合实体和关系抽取的共同注意力网络（CARE）。我们的方法涉及学习每个子任务的分离表示，旨在避免特征重叠。我们的方法的核心是共同注意力模块，它捕捉两个子任务之间的双向交互，使模型能够利用实体信息进行关系预测，反之亦然，从而促进相互增强。在三个联合实体-关系抽取基准数据集（NYT、WebNLG和SciERC）上的广泛实验表明，我们提出的模型达到了卓越的性能，超过了现有的基线模型。

    Joint entity and relation extraction is the fundamental task of information extraction, consisting of two subtasks: named entity recognition and relation extraction. Most existing joint extraction methods suffer from issues of feature confusion or inadequate interaction between two subtasks. In this work, we propose a Co-Attention network for joint entity and Relation Extraction (CARE). Our approach involves learning separate representations for each subtask, aiming to avoid feature overlap. At the core of our approach is the co-attention module that captures two-way interaction between two subtasks, allowing the model to leverage entity information for relation prediction and vice versa, thus promoting mutual enhancement. Extensive experiments on three joint entity-relation extraction benchmark datasets (NYT, WebNLG and SciERC) show that our proposed model achieves superior performance, surpassing existing baseline models.
    
[^107]: 基于自然语言处理的消费者投诉叙述中系统异常的检测方法

    NLP-based detection of systematic anomalies among the narratives of consumer complaints. (arXiv:2308.11138v1 [stat.ME])

    [http://arxiv.org/abs/2308.11138](http://arxiv.org/abs/2308.11138)

    本文开发了一种基于自然语言处理的方法，用于检测消费者投诉叙述中的系统异常。这种方法可以解决分类算法对于较小且频繁出现的系统异常检测的问题，并将投诉叙述转化为定量数据进行分析。

    

    我们开发了一种基于自然语言处理的方法，用于检测投诉叙述中的系统异常，简称为系统异常。尽管分类算法被用于检测明显的异常，但在较小且频繁出现的系统异常情况下，算法可能会因为各种原因而失效，包括技术原因和人工分析师的自然限制。因此，在分类之后的下一步中，我们将投诉叙述转化为定量数据，然后使用一种算法来检测系统异常。我们使用消费者金融保护局的消费者投诉数据库中的投诉叙述来说明整个过程。

    We develop an NLP-based procedure for detecting systematic nonmeritorious consumer complaints, simply called systematic anomalies, among complaint narratives. While classification algorithms are used to detect pronounced anomalies, in the case of smaller and frequent systematic anomalies, the algorithms may falter due to a variety of reasons, including technical ones as well as natural limitations of human analysts. Therefore, as the next step after classification, we convert the complaint narratives into quantitative data, which are then analyzed using an algorithm for detecting systematic anomalies. We illustrate the entire procedure using complaint narratives from the Consumer Complaint Database of the Consumer Financial Protection Bureau.
    
[^108]: IróyìnSpeech：一个多功能的约鲁巴语语音语料库

    \`{I}r\`{o}y\`{i}nSpeech: A multi-purpose Yor\`{u}b\'{a} Speech Corpus. (arXiv:2307.16071v1 [cs.CL])

    [http://arxiv.org/abs/2307.16071](http://arxiv.org/abs/2307.16071)

    本论文介绍了IróyìnSpeech语料库，这是一个多功能的约鲁巴语语音语料库，可用于TTS和ASR任务，包含38.5小时的数据，由80名志愿者录制。

    

    我们介绍了IróyìnSpeech语料库，这是一个受到希望增加高质量、免费可用的当代约鲁巴语语音的影响而创建的新数据集。我们发布了一个多用途的数据集，可用于TTS和ASR任务。我们从新闻和创意写作领域收集了文字句子，在开放授权（CC-BY-4.0）下进行筛选，并由多位发音人录制每个句子。我们将5000个发音句提供给Common Voice平台，在线众包转录。该数据集总共有38.5小时的数据，由80名志愿者录制。

    We introduce the \`{I}r\`{o}y\`{i}nSpeech corpus -- a new dataset influenced by a desire to increase the amount of high quality, freely available, contemporary Yor\`{u}b\'{a} speech. We release a multi-purpose dataset that can be used for both TTS and ASR tasks. We curated text sentences from the news and creative writing domains under an open license i.e., CC-BY-4.0 and had multiple speakers record each sentence. We provide 5000 of our utterances to the Common Voice platform to crowdsource transcriptions online. The dataset has 38.5 hours of data in total, recorded by 80 volunteers.
    
[^109]: 好的，但并非总是公正：对三个商业机器翻译系统性别偏见的评估

    Good, but not always Fair: An Evaluation of Gender Bias for three commercial Machine Translation Systems. (arXiv:2306.05882v1 [cs.CL])

    [http://arxiv.org/abs/2306.05882](http://arxiv.org/abs/2306.05882)

    本文对三个商业机器翻译系统进行了细致评估，在关注性别翻译和偏见的前提下，揭示了它们性别翻译方面的显着差异和偏见，这一点不会因为它们整体翻译质量的好坏而有所改变。

    

    机器翻译（MT）在质量方面继续取得重大进展，并且正在越来越大规模地被采用。因此，分析已转向更微妙的方面、复杂的现象以及可能从广泛使用MT工具中产生的风险。本文针对三个商业MT系统 - Google Translate、DeepL和Modern MT进行了细致的评估，重点关注性别翻译和偏见。对于三种语言对（英语/西班牙语、英语/意大利语和英语/法语），我们在各种自然语境下的性别现象的多个粒度级别上审查了这些系统的行为。我们的研究考虑了在线MT工具的当前状态，揭示了三个系统性别翻译方面的显着差异，尽管它们的整体翻译质量不错，但是每个系统都表现出不同程度的偏见。

    Machine Translation (MT) continues to make significant strides in quality and is increasingly adopted on a larger scale. Consequently, analyses have been redirected to more nuanced aspects, intricate phenomena, as well as potential risks that may arise from the widespread use of MT tools. Along this line, this paper offers a meticulous assessment of three commercial MT systems - Google Translate, DeepL, and Modern MT - with a specific focus on gender translation and bias. For three language pairs (English/Spanish, English/Italian, and English/French), we scrutinize the behavior of such systems at several levels of granularity and on a variety of naturally occurring gender phenomena in translation. Our study takes stock of the current state of online MT tools, by revealing significant discrepancies in the gender translation of the three systems, with each system displaying varying degrees of bias despite their overall translation quality.
    
[^110]: 用于基于检索的对话系统的上下文掩码自编码器

    ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems. (arXiv:2306.04357v1 [cs.CL])

    [http://arxiv.org/abs/2306.04357](http://arxiv.org/abs/2306.04357)

    本研究提出了一种针对对话响应选择的后训练技术Dial-MAE，利用生成方法更好地压缩对话语义至密集向量，并提高对话响应选择准确性。

    

    对话响应选择旨在根据给定的用户和系统话语历史记录从几个候选响应中选择适当的响应。最近的研究通过后训练大多依赖于单纯的掩码语言建模方法来提高对话响应选择的准确性。但是，最近开发的生成方法在IR社区展示了有希望的文本表示能力，这可能会导致更好的对话语义建模。因此，在本文中，我们提出 Dial-MAE（对话上下文掩码自编码器），这是一种简单而有效的针对对话响应选择的后训练技术。 Dial-MAE使用一个不对称的编码器-解码器架构，学习将对话的语义更好地压缩到密集向量中。 Dial-MAE的过程包括由深度编码器创建带有掩码对话上下文的对话嵌入，然后是浅解码器，该解码器使用此嵌入以及上下文向量来生成响应。

    Dialogue response selection aims to select an appropriate response from several candidates based on a given user and system utterance history. Recent studies have been improving the accuracy of dialogue response selection through post-training, mostly relying on naive masked language modeling methods. However, the recently developed generative methods have shown promising text representation capabilities in IR community, which could potentially lead to better dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE (Dialogue Contextual Masking Auto-encoder), a straightforward yet effective post-training technique tailored for dialogue response selection. Dial-MAE uses an asymmetric encoder-decoder architecture that learns to better compress the semantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE involves a deep encoder creating a dialogue embedding with the masked dialogue context, followed by a shallow decoder that uses this embedding along with
    
[^111]: 金融情感分析：从Transformer回归到可解释性词典(XLex)

    Sentiment Analysis in Finance: From Transformers Back to eXplainable Lexicons (XLex). (arXiv:2306.03997v1 [cs.CL])

    [http://arxiv.org/abs/2306.03997](http://arxiv.org/abs/2306.03997)

    本论文提出了一种名为XLex的新方法，它结合了基于词典和Transformer模型的优点，可以在金融情感分析中提供高效且可解释的结果。

    

    基于词典的金融情感分析（SA）利用人工专家创建的专门注释的词典从金融文本中提取情感。虽然基于词典的方法在文本数据上实现简单且速度快，但需要相当大的手动注释工作来创建、维护和更新词典。这些方法也被认为不如基于深度学习的方法（如Transformer模型）优越，由于它们在各种NLP任务中的出色表现，已经成为主流。然而，Transformer需要大量的数据和计算资源进行训练和测试。此外，它们涉及显著的预测时间，使其不适用于实时生产环境或处理能力受限的系统。在本文中，我们介绍了一种名为eXplainable Lexicons（XLex）的新方法，将基于词典的方法和Transformer模型的优势结合起来。

    Lexicon-based sentiment analysis (SA) in finance leverages specialized, manually annotated lexicons created by human experts to extract sentiment from financial texts. Although lexicon-based methods are simple to implement and fast to operate on textual data, they require considerable manual annotation efforts to create, maintain, and update the lexicons. These methods are also considered inferior to the deep learning-based approaches, such as transformer models, which have become dominant in various NLP tasks due to their remarkable performance. However, transformers require extensive data and computational resources for both training and testing. Additionally, they involve significant prediction times, making them unsuitable for real-time production environments or systems with limited processing capabilities. In this paper, we introduce a novel methodology named eXplainable Lexicons (XLex) that combines the advantages of both lexicon-based methods and transformer models. We propose 
    
[^112]: 利用基于优势的离线策略梯度改进语言模型

    Improving Language Models with Advantage-based Offline Policy Gradients. (arXiv:2305.14718v1 [cs.CL])

    [http://arxiv.org/abs/2305.14718](http://arxiv.org/abs/2305.14718)

    本文介绍了一种简单的训练算法Left-over Lunch RL （LoL-RL），使用离线策略梯度学习任何序列到序列数据，从而实现优化LM效用的方法。

    

    根据用户定义的质量或风格限制提高语言模型生成是具有挑战性的。典型的方法包括学习额外的人工编写数据，使用启发式方法过滤“低质量”数据和/或使用强化学习与人体反馈（RLHF）。然而，过滤会删除有价值的训练信号，而数据收集和RLHF不断需要额外的人工编写或LM探索数据，这可能成本高。一个自然的问题是“我们可以利用RL来优化现有的众包和互联网数据上的LM效用吗？”为此，我们提出了剩余午餐强化学习（LoL-RL），这是一种简单的训练算法，使用离线策略梯度来学习语言生成任务作为1步RL游戏。 LoL-RL可以微调LM，以优化任意基于分类器或人定义的效用函数的任何序列到序列数据。使用不同大小模型的五个不同语言生成任务的实验

    Improving language model generations according to some user-defined quality or style constraints is challenging. Typical approaches include learning on additional human-written data, filtering ``low-quality'' data using heuristics and/or using reinforcement learning with human feedback (RLHF). However, filtering can remove valuable training signals, whereas data collection and RLHF constantly require additional human-written or LM exploration data which can be costly to obtain. A natural question to ask is ``Can we leverage RL to optimize LM utility on existing crowd-sourced and internet data?''  To this end, we present Left-over Lunch RL (LoL-RL), a simple training algorithm that uses offline policy gradients for learning language generation tasks as a 1-step RL game. LoL-RL can finetune LMs to optimize arbitrary classifier-based or human-defined utility functions on any sequence-to-sequence data. Experiments with five different language generation tasks using models of varying sizes 
    
[^113]: ChatGPT 需要进行SPADE（可持续性、隐私、数字鸿沟和伦理）评估：一项综述。

    ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])

    [http://arxiv.org/abs/2305.03123](http://arxiv.org/abs/2305.03123)

    本文研究关注ChatGPT面临的可持续性、隐私、数字鸿沟和伦理问题，提出了SPADE评估的必要性，并给出了缓解和建议。

    

    ChatGPT是另一个大型语言模型（LLM），由于其性能和有效的对话能力，在研究和工业界中得到了巨大的关注。最近，许多研究已经发表，以展示ChatGPT和其他LLMs的有效性、效率、集成和情感。相反，本研究关注的是大多数被忽视的重要方面，即可持续性、隐私、数字鸿沟和伦理，并建议不仅仅是ChatGPT，而是在对话机器人类别中的每一个后续入口都应该进行SPADE评估。本文详细讨论了关于ChatGPT的问题和关注点与上述特征一致。我们通过一些初步的数据收集和可视化以及假设的事实来支持我们的假设。我们还为每个问题提出了缓解和建议。此外，我们还提供了一些未来方向和开放问题的探讨。

    ChatGPT is another large language model (LLM) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail about the issues and concerns raised over chatGPT in line with aforementioned characteristics. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also s
    
[^114]: 语言距离与多语言表示空间中的跨语言传递的相关性研究

    Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space. (arXiv:2305.02151v1 [cs.CL])

    [http://arxiv.org/abs/2305.02151](http://arxiv.org/abs/2305.02151)

    探索了语言特征对多语言表示空间中的跨语言传递性能的影响，初步提供了方法以增强对语言上相距较远的语言的传递能力。

    

    先前的研究已经探讨了不同语言特征对跨语言传递性能的影响。本研究探讨了这种效应如何映射到表示空间中。过去的研究集中在微调期间多语言语言模型的跨语言对齐上的影响，而本研究研究的是由MLLMs生成的相应语言表示空间的绝对演变。我们特别强调语言特征的作用，并调查其与表示空间和跨语言传递性能的影响之间的相互关系。此外，本文提供了初步证据，说明如何利用这些发现增强对语言上相距较远的语言的传递能力。

    Prior research has investigated the impact of various linguistic features on cross-lingual transfer performance. In this study, we investigate the manner in which this effect can be mapped onto the representation space. While past studies have focused on the impact on cross-lingual alignment in multilingual language models during fine-tuning, this study examines the absolute evolution of the respective language representation spaces produced by MLLMs. We place a specific emphasis on the role of linguistic characteristics and investigate their inter-correlation with the impact on representation spaces and cross-lingual transfer performance. Additionally, this paper provides preliminary evidence of how these findings can be leveraged to enhance transfer to linguistically distant languages.
    
[^115]: InfoCTM: 一种基于互信息最大化的跨语言主题建模方法

    InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual Topic Modeling. (arXiv:2304.03544v1 [cs.CL])

    [http://arxiv.org/abs/2304.03544](http://arxiv.org/abs/2304.03544)

    本文提出了一种基于互信息最大化的跨语言主题建模方法InfoCTM，通过主题对齐方法规范化主题生成并寻找更多链接的跨语言词汇，有效解决了重复主题和低覆盖字典的问题。

    

    跨语言主题模型在跨语言文本分析中使用广泛，可以揭示对齐的潜在主题。但是，大多数现有方法存在产生重复主题的问题以及由于低覆盖字典导致的性能下降。本文提出了一种基于互信息的跨语言主题建模方法InfoCTM。与先前工作中的直接对齐不同，我们提出了一种基于互信息的主题对齐方法。这可作为规范化，以正确对齐主题并防止词的退化主题表示，从而减轻了重复主题问题。为了解决低覆盖字典问题，我们进一步提出了一种跨语言词汇链接方法，该方法在给定字典的翻译以外，寻找更多链接的跨语言词汇进行主题对齐。在英语，中文和日语数据集上的广泛实验表明，我们的方法优于现有的基准线，产生更多多样化且信息丰富的主题，同时不损害跨语言对齐性能。

    Cross-lingual topic models have been prevalent for cross-lingual text analysis by revealing aligned latent topics. However, most existing methods suffer from producing repetitive topics that hinder further analysis and performance decline caused by low-coverage dictionaries. In this paper, we propose the Cross-lingual Topic Modeling with Mutual Information (InfoCTM). Instead of the direct alignment in previous work, we propose a topic alignment with mutual information method. This works as a regularization to properly align topics and prevent degenerate topic representations of words, which mitigates the repetitive topic issue. To address the low-coverage dictionary issue, we further propose a cross-lingual vocabulary linking method that finds more linked cross-lingual words for topic alignment beyond the translations of a given dictionary. Extensive experiments on English, Chinese, and Japanese datasets demonstrate that our method outperforms state-of-the-art baselines, producing more
    
[^116]: HIVE：利用人类反馈进行指导性视觉编辑

    HIVE: Harnessing Human Feedback for Instructional Visual Editing. (arXiv:2303.09618v1 [cs.CV])

    [http://arxiv.org/abs/2303.09618](http://arxiv.org/abs/2303.09618)

    本文提出了一种新的框架，利用人类反馈进行指导性视觉编辑。通过收集被编辑图像的人类反馈，并学习奖励函数捕捉用户的偏好，可以缓解数据限制所带来的偏差，并提高模型性能。

    

    研究表明，将人类反馈纳入大型语言模型生成的文本对齐到人类偏好至关重要。本文假设，最先进的指导性图像编辑模型，其输出基于输入图像和编辑指令，同样可以从人类反馈中受益，因为其输出可能不符合用户的正确指令和偏好。本文提出了一种利用人类反馈进行指导性视觉编辑（HIVE）的新框架。具体而言，我们在编辑的图像上收集人类反馈并学习奖励函数以捕捉基础用户偏好。随后，我们引入可扩展的扩散模型微调方法，可根据估计的奖励值融入人类偏好。此外，为减轻数据限制带来的偏差，我们贡献了1M训练数据集，3.6K奖励数据集以用于奖励学习，以及1K评估数据集，以提高指导性图像编辑模型的性能。

    Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of inst
    
[^117]: 为少数据样本的表格生成自适应提示

    Adapting Prompt for Few-shot Table-to-Text Generation. (arXiv:2302.12468v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.12468](http://arxiv.org/abs/2302.12468)

    这项研究提出了一个新的框架AdaPTGen，通过将领域特定知识的提示模板调整为模型所需，来解决缺乏标注数据的限制。该框架注入了常规表格相关描述的表示，充分利用未标记的领域特定知识，并允许设计各种任务来探索领域特定知识。

    

    预训练语言模型（PLM）在表格生成任务中取得了显著进展。然而，缺乏领域特定知识很难弥合表格数据和文本之间的拓扑差距，尤其是在具有有限资源的实际应用中。为了解决标注数据不足的限制，我们提出了一种新的框架：自适应生成提示（AdaPTGen）。AdaPTGen的核心是将领域特定知识的提示模板调整为模型所需，带来了至少三个好处：（1）它注入了常规表格相关描述的表示，以弥合表格数据和文本之间的拓扑差距；（2）它使我们能够充分利用大量未标记的领域特定知识，从而减轻了PLMs缺乏领域知识的固有缺点；（3）它允许我们设计各种任务来探索领域特定知识。在三个开放领域的实验和分析中进行了广泛实验。

    Pretrained language models (PLMs) have made remarkable progress in table-to-text generation tasks. However, the lack of domain-specific knowledge makes it challenging to bridge the topological gap between tabular data and text, especially in real-world applications with limited resources. To mitigate the limitation of insufficient labeled data, we propose a novel framework: Adapt-Prompt-to-Generate (AdaPTGen). The core insight of AdaPTGen is to adapt prompt templates of domain-specific knowledge into the model, which brings at least three benefits: (1) it injects representation of normal table-related descriptions to bridge the topological gap between tabular data and texts; (2) it enables us to use large amounts of unlabeled domain-specific knowledge fully, which can alleviate the PLMs' inherent shortcomings of lacking domain knowledge; (3) it allows us to design various tasks to explore the domain-specific knowledge. Extensive experiments and analyses are conducted on three open-doma
    
[^118]: 部分动员：跟踪俄罗斯媒体和电报之间的多语言信息流

    Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram. (arXiv:2301.10856v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2301.10856](http://arxiv.org/abs/2301.10856)

    本文研究了16个俄罗斯媒体机构和732个电报频道之间的互动，发现新闻媒体不仅通过电报传播现有的叙事，而且会从电报平台源材料，研究结果表明2.3％至26.7％的文章将主题归因于电报活动。

    

    在俄罗斯入侵乌克兰后，针对俄罗斯在线媒体的虚假信息和宣传，包括俄罗斯之声和卫星新闻在内的俄罗斯媒体在欧洲遭到禁止。为了保持观众数量，许多俄罗斯媒体开始在电报等消息服务上大力宣传其内容。在这项工作中，我们研究了2022年期间16家俄罗斯媒体机构如何与732个电报频道互动和利用。利用基础模型MPNet、DP-means聚类和Hawkes过程，我们跟踪新闻网站和电报频道之间的叙事传播情况。我们表明，新闻媒体不仅通过电报传播现有的叙事，而且他们会从电报平台源材料。在我们研究的网站中，2.3％（ura.news）至26.7％（ukraina.ru）的文章讨论了源于/导致电报活动的内容。最后，通过跟踪个别主题的扩散，我们测量新闻网站发表文章的速率。

    In response to disinformation and propaganda from Russian online media following the Russian invasion of Ukraine, Russian outlets including Russia Today and Sputnik News were banned throughout Europe. To maintain viewership, many of these Russian outlets began to heavily promote their content on messaging services like Telegram. In this work, we study how 16 Russian media outlets interacted with and utilized 732 Telegram channels throughout 2022. Leveraging the foundational model MPNet, DP-means clustering, and Hawkes Processes, we trace how narratives spread between news sites and Telegram channels. We show that news outlets not only propagate existing narratives through Telegram, but that they source material from the messaging platform. Across the sites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of articles discuss content that originated/resulted from activity on Telegram. Finally, tracking the spread of individual topics, we measure the rate at which news website
    

