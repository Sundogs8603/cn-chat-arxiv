# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Executable Code Actions Elicit Better LLM Agents](https://rss.arxiv.org/abs/2402.01030) | 使用可执行的Python代码整合LLM智能体行动，提升了成功率高达20%。 |
| [^2] | [Event Detection from Social Media for Epidemic Prediction](https://arxiv.org/abs/2404.01679) | 通过开发一个从社交媒体帖子中提取和分析流行病相关事件的框架，该研究首次利用事件检测技术在COVID-19流行病和其他未见流行病中实现有效的早期预警。 |
| [^3] | [Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models](https://arxiv.org/abs/2403.19521) | 通过深入研究Transformer-based语言模型在事实回忆任务中的机制，我们发现了零/少次样本情况下的特定任务头、MLP层和残差流的功能，以及抗过度自信机制。 |
| [^4] | [Enhancing Efficiency in Sparse Models with Sparser Selection](https://arxiv.org/abs/2403.18926) | 提出了一种新颖的MoE模型\tool，通过利用小型专家和基于阈值的路由器，使标记能够选择性地仅涉及到必要的参数，从而在减少MoE层计算负载50%以上的同时提高模型性能。 |
| [^5] | [KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation](https://arxiv.org/abs/2403.06642) | 提出了一种知识增强的大型语言模型用于推荐的方法，通过使用外部知识来帮助生成真实可用的文本，并包括知识为基础的对比学习方案进行训练。 |
| [^6] | [Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models](https://arxiv.org/abs/2402.18945) | 论文提出了一种名为Syntactic Ghost的新方法，实现了对预训练语言模型进行无感知和通用的后门植入。 |
| [^7] | [How do Large Language Models Handle Multilingualism?](https://arxiv.org/abs/2402.18815) | 大型语言模型展示了处理多语言任务的出色性能，研究发现在不同层次中处理多语言输入的策略，以及处理特定语言时的语言特定神经元存在。 |
| [^8] | [DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning](https://arxiv.org/abs/2402.18137) | 本文提出了 DecisionNCE 框架，通过隐式偏好学习实体多模态表示，实现了提取任务进展信息和与语言指令对齐的有效方法 |
| [^9] | [Gradient-Free Adaptive Global Pruning for Pre-trained Language Models](https://arxiv.org/abs/2402.17946) | 提出了自适应全局剪枝（AdaGP）框架，通过重新定义全局剪枝过程为可管理的协调子问题，实现对大型语言模型的资源高效优化，显著提高性能。 |
| [^10] | [Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs](https://arxiv.org/abs/2402.11218) | 该研究提出了一个名为DATG的框架，利用动态属性图调节关键属性词和关键反属性词的发生，实现了有效的属性控制，在毒性缓解和情感转化任务中表现出19.29%的控制准确性增强。 |
| [^11] | [Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment](https://arxiv.org/abs/2402.10207) | 本文介绍了Rewards-in-Context（RiC）方法，该方法通过多个奖励条件控制基础模型的响应，并应用有监督的微调进行对齐。它具有简单性和适应性，并支持在推理时动态调整用户偏好。 |
| [^12] | [Chain-of-Thought Reasoning Without Prompting](https://arxiv.org/abs/2402.10200) | 本研究发现，通过改变解码过程而不是使用提示工程，可以从预训练的大型语言模型中引出思维链推理路径，这种方法绕过了提示的限制，并且可以评估模型的内在推理能力。 |
| [^13] | [InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning](https://arxiv.org/abs/2402.06332) | 在本文中，我们介绍了开源数学推理LLMs InternLM-Math，该模型以其数学能力代表了抽象推理能力。我们的模型以统一的方式整合了逻辑推理、奖励建模、形式推理、数据增强和代码解释器，并使用监督学习使其成为一个多功能的数学推理器、验证器、证明器和增强器。在各种基准测试中，包括GSM8K、MATH、匈牙利数学考试、MathBench-ZH和MiniF2F，在上下文学习、监督微调和代码辅助推理的设置下，InternLM-Math取得了开源的最先进性能。我们的预训练模型在MiniF2F测试集上达到了30.3的得分。我们还研究了如何使用LEAN解决数学问题，并探讨了其在多任务学习设置下的性能。 |
| [^14] | [Isotropy, Clusters, and Classifiers](https://arxiv.org/abs/2402.03191) | 同性质的嵌入空间对聚类和线性分类目标具有负面影响，这一事实得到了本文的实证支持，并对文献中的先前结果有所启示。 |
| [^15] | [Decoding-time Realignment of Language Models](https://arxiv.org/abs/2402.02992) | 本研究提出了解码时间对齐（DeRa）方法，可以在不重新训练模型的情况下探索和评估不同的规则化强度，从而对齐语言模型和人类偏好。 |
| [^16] | [Rendering Graphs for Graph Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2402.02130) | 本文在多模态大型语言模型中为图推理任务引入了视觉信息，并提出了一个新的基准测试数据集GITQA。实验证明，结合文本和视觉信息的结果比单一模态效果更好。 |
| [^17] | [SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models](https://arxiv.org/abs/2312.09818) | 本论文介绍了一个新任务，即视频笑声推理，旨在解释特定视频中人们笑的原因，并提出了数据集SMILE。通过利用大型语言模型生成文本视频表示，我们的基线能够生成合理的笑声解释。 |
| [^18] | [UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation](https://arxiv.org/abs/2311.15296) | 评估了中文大型语言模型在文本生成中的虚构能力，并指出现有基准评估通常采用受限制的生成技术，无法满足真实需求中的无约束文本生成。 |
| [^19] | [StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving](https://arxiv.org/abs/2311.08803) | StrategyLLM提出了一个框架，利用大型语言模型的能力自动构建可推广和一致的少次提示，优于竞争基线，不需要人工参与。 |
| [^20] | [Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios](https://arxiv.org/abs/2311.08154) | 自一致性是一种通用的集成优化方法，可以应用于几乎所有情景中，能够解决语言模型推理中的重复性和局部最优性问题。 |
| [^21] | [A Corpus for Sentence-level Subjectivity Detection on English News Articles](https://arxiv.org/abs/2305.18034) | 该研究开发了新的标注指南，建立了用于英语新闻文章句子级主观性检测的语料库，并表明多语境训练的模型在该任务上取得了最佳性能。 |
| [^22] | [AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation.](http://arxiv.org/abs/2312.13010) | AgentCoder是一种基于多Agent的代码生成和测试优化解决方案，通过程序员Agent、测试设计师Agent和测试执行Agent的协作，实现了在平衡代码生成和有效测试用例生成与执行方面的挑战中的突破。 |
| [^23] | [JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning.](http://arxiv.org/abs/2310.02953) | JsonTuning是一种面向通用、强大和可控的指令调优方法，通过利用JSON的结构化特性，帮助模型理解任务要素及其关系，从而扩展了通用性、提高了稳健性，并增强了对输出的控制。 |
| [^24] | [MindDial: Belief Dynamics Tracking with Theory-of-Mind Modeling for Situated Neural Dialogue Generation.](http://arxiv.org/abs/2306.15253) | MindDial是一个使用心智模拟进行信念动态跟踪的对话生成框架，可以在场景化环境中生成自由对话来协商共识。 |
| [^25] | [PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization.](http://arxiv.org/abs/2306.05087) | PandaLM是一个评估LLM指令调优的自动基准，它能够区分最优模型，并关注于主观因素。 |
| [^26] | [Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling.](http://arxiv.org/abs/2305.09993) | Reprompting是一种无需人类干预的算法，通过迭代采样新配方解决多步推理任务，比人类编写的思维链提示表现更好，还可以提高较弱模型的性能。 |

# 详细

[^1]: 可执行代码行动能够激发更出色的LLM智能体

    Executable Code Actions Elicit Better LLM Agents

    [https://rss.arxiv.org/abs/2402.01030](https://rss.arxiv.org/abs/2402.01030)

    使用可执行的Python代码整合LLM智能体行动，提升了成功率高达20%。

    

    大型语言模型（LLM）智能体具备执行广泛行动的能力，如调用工具和控制机器人等，在解决现实世界的挑战中显示出巨大潜力。LLM智能体通常通过生成JSON或文本的预定义格式来产生行动，这通常受限于受限制的行动空间（例如，预定义工具的范围）和受限的灵活性（例如，无法组合多个工具）。本研究提出使用可执行的Python代码将LLM智能体的行动整合到一个统一的行动空间中（CodeAct）。CodeAct与Python解释器集成，可以执行代码行动，并通过多轮交互在新的观察中动态修订先前的行动或发出新的行动。我们对17个LLM在API-Bank和新编制的基准测试中进行了广泛分析，结果显示CodeAct的性能超过了广泛使用的替代方案（成功率高出20%）。CodeAct的令人鼓舞的表现激励我们建立了一个开源的...

    Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-sourc
    
[^2]: 社交媒体上的事件检测用于疫情预测

    Event Detection from Social Media for Epidemic Prediction

    [https://arxiv.org/abs/2404.01679](https://arxiv.org/abs/2404.01679)

    通过开发一个从社交媒体帖子中提取和分析流行病相关事件的框架，该研究首次利用事件检测技术在COVID-19流行病和其他未见流行病中实现有效的早期预警。

    

    社交媒体是一个易于访问的平台，提供有关社会趋势和事件的及时更新。关于流行病相关事件的讨论，如感染、症状和社会互动，对于在流行病爆发期间制定政策至关重要。在我们的工作中，我们开创利用事件检测（ED）来更好地准备和提前预警任何即将到来的流行病，通过开发一个框架从社交媒体帖子中提取和分析与流行病相关的事件。为此，我们策划了一个包括七种与疾病无关的事件类型的流行病事件本体论，并构建了一个名为SPEED的Twitter数据集，其中包含人工注释的重点关注COVID-19流行病的事件。实验揭示了在COVID速度上训练的ED模型如何有效地检测出关于三种未见流行病（猴痘、寨卡病毒和登革热）的流行病事件；而在现有ED数据集上训练的模型则表现不佳。此外，我们展示了报道

    arXiv:2404.01679v1 Announce Type: new  Abstract: Social media is an easy-to-access platform providing timely updates about societal trends and events. Discussions regarding epidemic-related events such as infections, symptoms, and social interactions can be crucial for informing policymaking during epidemic outbreaks. In our work, we pioneer exploiting Event Detection (ED) for better preparedness and early warnings of any upcoming epidemic by developing a framework to extract and analyze epidemic-related events from social media posts. To this end, we curate an epidemic event ontology comprising seven disease-agnostic event types and construct a Twitter dataset SPEED with human-annotated events focused on the COVID-19 pandemic. Experimentation reveals how ED models trained on COVID-based SPEED can effectively detect epidemic events for three unseen epidemics of Monkeypox, Zika, and Dengue; while models trained on existing ED datasets fail miserably. Furthermore, we show that reporting 
    
[^3]: 解释基于Transformer模型的语言模型在事实回忆中的关键机制

    Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models

    [https://arxiv.org/abs/2403.19521](https://arxiv.org/abs/2403.19521)

    通过深入研究Transformer-based语言模型在事实回忆任务中的机制，我们发现了零/少次样本情况下的特定任务头、MLP层和残差流的功能，以及抗过度自信机制。

    

    本文深入探讨了Transformer-based语言模型在事实回忆任务中所采用的机制。在零次样本情况下，给定类似“法国的首都是”的提示，特定任务的注意力头会从上下文中提取主题实体，如“法国”，并将其传递给后续的MLP以回忆所需的答案，如“巴黎”。我们引入了一种新颖的分析方法，旨在将MLP的输出分解为人类可理解的组件。通过这种方法，我们量化了跟随这些特定任务头的MLP层的功能。在残差流中，它会擦除或放大来自各个头的信息。此外，它会生成一个组件，将残差流重新定向到预期答案的方向。这些零次机制也适用于少次样本情况。此外，我们观察到一种广泛存在的抗过度自信机制。

    arXiv:2403.19521v1 Announce Type: cross  Abstract: In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks. In zero-shot scenarios, given a prompt like "The capital of France is," task-specific attention heads extract the topic entity, such as "France," from the context and pass it to subsequent MLPs to recall the required answer such as "Paris." We introduce a novel analysis method aimed at decomposing the outputs of the MLP into components understandable by humans. Through this method, we quantify the function of the MLP layer following these task-specific heads. In the residual stream, it either erases or amplifies the information originating from individual heads. Moreover, it generates a component that redirects the residual stream towards the direction of its expected answer. These zero-shot mechanisms are also employed in few-shot scenarios. Additionally, we observed a widely existent anti-overconfidence mechanism in 
    
[^4]: 用更稀疏的选择提高稀疏模型的效率

    Enhancing Efficiency in Sparse Models with Sparser Selection

    [https://arxiv.org/abs/2403.18926](https://arxiv.org/abs/2403.18926)

    提出了一种新颖的MoE模型\tool，通过利用小型专家和基于阈值的路由器，使标记能够选择性地仅涉及到必要的参数，从而在减少MoE层计算负载50%以上的同时提高模型性能。

    

    稀疏模型，包括稀疏的专家混合（MoE）模型，已经成为缩放Transformer模型的有效方法。然而，它们通常存在计算效率低的问题，因为大量参数通过将值乘以零或低激活值无谓参与计算。为了解决这一问题，我们提出了一种名为\tool 的新颖MoE模型，旨在提升稀疏MoE模型的功效和效率。 \tool 利用小型专家和基于阈值的路由器，使标记能够选择性地仅涉及到必要的参数。我们在语言建模和机器翻译任务上进行了大量实验，结果表明\tool 可以在不牺牲性能的情况下，将MoE层的计算负载减少50\%以上，同时提高模型性能。此外，我们展示了\tool 的通用性，通过将其应用于密集模型，在推断期间实现稀疏计算。

    arXiv:2403.18926v1 Announce Type: cross  Abstract: Sparse models, including sparse Mixture-of-Experts (MoE) models, have emerged as an effective approach for scaling Transformer models. However, they often suffer from computational inefficiency since a significant number of parameters are unnecessarily involved in computations via multiplying values by zero or low activation values. To address this issue, we present \tool, a novel MoE designed to enhance both the efficacy and efficiency of sparse MoE models. \tool leverages small experts and a threshold-based router to enable tokens to selectively engage only essential parameters. Our extensive experiments on language modeling and machine translation tasks demonstrate that \tool can enhance model performance while decreasing the computation load at MoE layers by over 50\% without sacrificing performance. Furthermore, we present the versatility of \tool by applying it to dense models, enabling sparse computation during inference. We pro
    
[^5]: KELLMRec: 知识增强大型语言模型用于推荐

    KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation

    [https://arxiv.org/abs/2403.06642](https://arxiv.org/abs/2403.06642)

    提出了一种知识增强的大型语言模型用于推荐的方法，通过使用外部知识来帮助生成真实可用的文本，并包括知识为基础的对比学习方案进行训练。

    

    在推荐系统领域，利用语义信息是一个重要的研究问题，旨在补充主流基于ID的方法的缺失部分。随着LLM的兴起，它作为知识库的能力和推理能力为这一研究领域开辟了新的可能性，使基于LLM的推荐成为新兴研究方向。然而，直接使用LLM来处理推荐场景中的语义信息是不可靠和次优的，由于存在幻觉等问题。应对这一问题的一种有前途的方法是利用外部知识来帮助LLM生成真实可用的文本。受以上动机的启发，我们提出了一种知识增强的LLMRec方法。除了在提示中使用外部知识外，所提出的方法还包括一个基于知识的对比学习方案用于训练。在公共数据集和企业中进行的实验

    arXiv:2403.06642v1 Announce Type: cross  Abstract: The utilization of semantic information is an important research problem in the field of recommender systems, which aims to complement the missing parts of mainstream ID-based approaches. With the rise of LLM, its ability to act as a knowledge base and its reasoning capability have opened up new possibilities for this research area, making LLM-based recommendation an emerging research direction. However, directly using LLM to process semantic information for recommendation scenarios is unreliable and sub-optimal due to several problems such as hallucination. A promising way to cope with this is to use external knowledge to aid LLM in generating truthful and usable text. Inspired by the above motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to using external knowledge in prompts, the proposed method also includes a knowledge-based contrastive learning scheme for training. Experiments on public datasets and in-enter
    
[^6]: Syntactic Ghost：一种对预训练语言模型进行的无感知通用后门攻击

    Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models

    [https://arxiv.org/abs/2402.18945](https://arxiv.org/abs/2402.18945)

    论文提出了一种名为Syntactic Ghost的新方法，实现了对预训练语言模型进行无感知和通用的后门植入。

    

    预训练语言模型（PLMs）被发现容易受到后门攻击，可以将漏洞转移到各种下游任务中。然而，现有的PLM后门攻击采用明显的触发器，在手动对准的情况下进行，因此在效果、隐匿性和通用性方面无法同时满足期望目标。本文提出了一种新方法，实现了不可见和通用的后门植入，称为Syntactic Ghost（简称为synGhost）。具体来说，该方法敌意地使用具有不同预定义句法结构的毒害样本作为隐蔽触发器，然后将后门植入到预训练表示空间，而不会破坏原始知识。毒害样本的输出表示在特征空间中尽可能均匀地分布，通过对比学习形成广泛的后门。此外，在亮

    arXiv:2402.18945v1 Announce Type: cross  Abstract: Pre-trained language models (PLMs) have been found susceptible to backdoor attacks, which can transfer vulnerabilities to various downstream tasks. However, existing PLM backdoors are conducted with explicit triggers under the manually aligned, thus failing to satisfy expectation goals simultaneously in terms of effectiveness, stealthiness, and universality. In this paper, we propose a novel approach to achieve invisible and general backdoor implantation, called \textbf{Syntactic Ghost} (synGhost for short). Specifically, the method hostilely manipulates poisoned samples with different predefined syntactic structures as stealth triggers and then implants the backdoor to pre-trained representation space without disturbing the primitive knowledge. The output representations of poisoned samples are distributed as uniformly as possible in the feature space via contrastive learning, forming a wide range of backdoors. Additionally, in light 
    
[^7]: 大型语言模型如何处理多语言？

    How do Large Language Models Handle Multilingualism?

    [https://arxiv.org/abs/2402.18815](https://arxiv.org/abs/2402.18815)

    大型语言模型展示了处理多语言任务的出色性能，研究发现在不同层次中处理多语言输入的策略，以及处理特定语言时的语言特定神经元存在。

    

    大型语言模型（LLMs）展现出在各种语言上出色的性能。本文探讨了一个问题：大型语言模型如何处理多语言？我们引入了一个框架，描述了LLMs处理多语言输入的过程：在前几层中，LLMs理解问题，将多语言输入转换为英语以便促进任务解决阶段。在中间层中，LLMs通过以英语思考并整合多语言知识来进行解决问题，利用自注意力和前馈结构，分别获取事实内容。在最后几层中，LLMs生成与查询的原始语言一致的响应。此外，我们研究了处理特定语言时特定语言神经元的存在。为了检测由输入语言激活的神经元，即使没有标签，我们创新性地设计了一个并行语言特定的

    arXiv:2402.18815v1 Announce Type: cross  Abstract: Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages. In this work, we delve into the question: How do LLMs handle multilingualism? We introduce a framework that depicts LLMs' processing of multilingual inputs: In the first several layers, LLMs understand the question, converting multilingual inputs into English to facilitate the task-solving phase. In the intermediate layers, LLMs engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the self-attention and feed-forward structures, respectively. In the last several layers, LLMs generate responses that align with the original language of the query. In addition, we investigate the existence of language-specific neurons when processing a certain language. To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specif
    
[^8]: DecisionNCE: 通过隐式偏好学习实体多模态表示

    DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning

    [https://arxiv.org/abs/2402.18137](https://arxiv.org/abs/2402.18137)

    本文提出了 DecisionNCE 框架，通过隐式偏好学习实体多模态表示，实现了提取任务进展信息和与语言指令对齐的有效方法

    

    多模态预训练已被证明是自主机器人中表示学习的三大目标：1）提取局部和全局任务进展信息；2）强化视觉表示的时间一致性；3）捕获轨迹级语言基础的有效策略。大部分已有方法通过不同的目标来处理这些问题，往往导致次优解。本文提出了一个通用统一目标，可以同时从图像序列中提取有意义的任务进展信息，并将它们与语言指令无缝对齐。我们发现，通过隐式偏好，在视觉轨迹与其对应的语言指令相比不匹配对更好地对齐时，流行的 Bradley-Terry 模型可以通过适当的奖励重新参数化而变为表示学习。结果产生的 DecisionNCE 框架，类似于 InfoNC

    arXiv:2402.18137v1 Announce Type: cross  Abstract: Multimodal pretraining has emerged as an effective strategy for the trinity of goals of representation learning in autonomous robots: 1) extracting both local and global task progression information; 2) enforcing temporal consistency of visual representation; 3) capturing trajectory-level language grounding. Most existing methods approach these via separate objectives, which often reach sub-optimal solutions. In this paper, we propose a universal unified objective that can simultaneously extract meaningful task progression information from image sequences and seamlessly align them with language instructions. We discover that via implicit preferences, where a visual trajectory inherently aligns better with its corresponding language instruction than mismatched pairs, the popular Bradley-Terry model can transform into representation learning through proper reward reparameterizations. The resulted framework, DecisionNCE, mirrors an InfoNC
    
[^9]: 无梯度自适应全局剪枝用于预训练语言模型

    Gradient-Free Adaptive Global Pruning for Pre-trained Language Models

    [https://arxiv.org/abs/2402.17946](https://arxiv.org/abs/2402.17946)

    提出了自适应全局剪枝（AdaGP）框架，通过重新定义全局剪枝过程为可管理的协调子问题，实现对大型语言模型的资源高效优化，显著提高性能。

    

    大型语言模型（LLMs）如LLaMA和GPT在自然语言处理中的转变性影响受到它们计算需求过高的限制。剪枝作为一种关键的压缩策略出现，引入稀疏性以增强内存和计算效率。然而，传统的全局剪枝对LLMs来说由于可扩展性问题而不实用，而本地剪枝，尽管效率高，却导致次优解决方案。为解决这些挑战，我们提出了自适应全局剪枝（AdaGP），这是一个重新定义全局剪枝处理为可管理的协调子问题的新框架，可以实现资源有效的全局最优化优化。AdaGP的方法将LLMs概念化为一系列模块化函数，并利用辅助变量进行问题分解，不仅便于在LLMs上实现实际应用，而且显示出显著的性能改进。

    arXiv:2402.17946v1 Announce Type: new  Abstract: The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose Adaptive Global Pruning (AdaGP), a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. AdaGP's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, part
    
[^10]: 使用动态属性图控制大型语言模型的文本生成

    Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs

    [https://arxiv.org/abs/2402.11218](https://arxiv.org/abs/2402.11218)

    该研究提出了一个名为DATG的框架，利用动态属性图调节关键属性词和关键反属性词的发生，实现了有效的属性控制，在毒性缓解和情感转化任务中表现出19.29%的控制准确性增强。

    

    控制文本生成（CTG）旨在生成具有特定所需属性的文本。在本研究中，我们引入了一个可插拔的CTG框架，用于大型语言模型（LLMs），名为基于动态属性图的控制文本生成（DATG）。该框架利用属性评分器评估由LLMs生成的句子的属性，并构建动态属性图。DATG调节关键属性词和关键反属性词的发生，实现了有效的属性控制，而不影响模型的原始能力。我们在两个任务中跨四个数据集进行实验：毒性缓解和情感转化，利用五个LLMs作为基础模型。我们的发现突出了在控制准确性方面的显着提高，实现了在四个数据集中最有利的任务中对基线方法的峰值改进为19.29％。此外，我们观察到显着

    arXiv:2402.11218v1 Announce Type: new  Abstract: Controlled Text Generation (CTG) aims to produce texts that exhibit specific desired attributes. In this study, we introduce a pluggable CTG framework for Large Language Models (LLMs) named Dynamic Attribute Graphs-based controlled text generation (DATG). This framework utilizes an attribute scorer to evaluate the attributes of sentences generated by LLMs and constructs dynamic attribute graphs. DATG modulates the occurrence of key attribute words and key anti-attribute words, achieving effective attribute control without compromising the original capabilities of the model. We conduct experiments across four datasets in two tasks: toxicity mitigation and sentiment transformation, employing five LLMs as foundational models. Our findings highlight a remarkable enhancement in control accuracy, achieving a peak improvement of 19.29% over baseline methods in the most favorable task across four datasets. Additionally, we observe a significant 
    
[^11]: 基于上下文的奖励：基于动态偏好调整的多目标基础模型对齐

    Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment

    [https://arxiv.org/abs/2402.10207](https://arxiv.org/abs/2402.10207)

    本文介绍了Rewards-in-Context（RiC）方法，该方法通过多个奖励条件控制基础模型的响应，并应用有监督的微调进行对齐。它具有简单性和适应性，并支持在推理时动态调整用户偏好。

    

    我们考虑了基于人类偏好的基础模型多目标对齐问题，这是实现有益和无害的人工智能系统的关键步骤。然而，使用强化学习（RL）对大型基础模型进行微调通常是昂贵且不稳定的，并且人类偏好的多维度、异质性和冲突性进一步复杂化了对齐过程。在本文中，我们引入了Rewards-in-Context（RiC）方法，它使得基础模型的响应取决于其提示上下文中的多个奖励，并应用有监督的微调来进行对齐。RiC的显著特点是简单性和适应性，因为它只需要对单个基础模型进行有监督的微调，并支持在推理时动态调整用户偏好。受到抽象的凸优化问题的解析解的启发，我们提出了一种动态推理时调整方法。

    arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
    
[^12]: 不需要提示的思维链推理

    Chain-of-Thought Reasoning Without Prompting

    [https://arxiv.org/abs/2402.10200](https://arxiv.org/abs/2402.10200)

    本研究发现，通过改变解码过程而不是使用提示工程，可以从预训练的大型语言模型中引出思维链推理路径，这种方法绕过了提示的限制，并且可以评估模型的内在推理能力。

    

    在提升大型语言模型（LLMs）的推理能力方面，以往的研究主要集中在特定的提示技术，如少样本或零样本的思维链提示。这些方法虽然有效，但往往需要手动进行大量的提示工程。我们的研究采用了一种新的方法：LLMs是否可以在没有提示的情况下有效推理？我们的研究结果表明，有趣的是，通过简单地改变解码过程，就可以从预训练的LLMs中引出思维链推理路径。我们研究了前$k$个替代标记，发现这些序列中经常存在思维链路径。这种方法不仅绕过了提示的混淆因素，还可以评估LLMs的内在推理能力。此外，我们观察到解码路径中的思维链存在与模型解码的置信度较高的相关性。

    arXiv:2402.10200v1 Announce Type: new  Abstract: In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the \textit{decoding} process. Rather than conventional greedy decoding, we investigate the top-$k$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' \textit{intrinsic} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decod
    
[^13]: InternLM-Math：面向可验证推理的开放数学大语言模型

    InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning

    [https://arxiv.org/abs/2402.06332](https://arxiv.org/abs/2402.06332)

    在本文中，我们介绍了开源数学推理LLMs InternLM-Math，该模型以其数学能力代表了抽象推理能力。我们的模型以统一的方式整合了逻辑推理、奖励建模、形式推理、数据增强和代码解释器，并使用监督学习使其成为一个多功能的数学推理器、验证器、证明器和增强器。在各种基准测试中，包括GSM8K、MATH、匈牙利数学考试、MathBench-ZH和MiniF2F，在上下文学习、监督微调和代码辅助推理的设置下，InternLM-Math取得了开源的最先进性能。我们的预训练模型在MiniF2F测试集上达到了30.3的得分。我们还研究了如何使用LEAN解决数学问题，并探讨了其在多任务学习设置下的性能。

    

    大型语言模型的数学能力可以表示其抽象推理能力。本文介绍并开源我们的数学推理LLMs InternLM-Math，该模型是从InternLM2继续预训练而来。我们以统一的seq2seq格式统一了逻辑推理、奖励建模、形式推理、数据增强和代码解释器，并且监督我们的模型成为一个多功能的数学推理器、验证器、证明器和增强器。这些能力可用于开发下一代数学LLMs或自身迭代。在包括GSM8K、MATH、匈牙利数学考试、MathBench-ZH和MiniF2F在内的各种非正式和正式基准测试中，InternLM-Math在上下文学习、监督微调和代码辅助推理的设置下取得了开源的最先进性能。我们的预训练模型在无微调的情况下，在MiniF2F测试集上达到了30.3。我们进一步研究了如何使用LEAN来解决数学问题，并研究了其在多任务学习设置下的性能。

    The math abilities of large language models can represent their abstract reasoning ability. In this paper, we introduce and open-source our math reasoning LLMs InternLM-Math which is continue pre-trained from InternLM2. We unify chain-of-thought reasoning, reward modeling, formal reasoning, data augmentation, and code interpreter in a unified seq2seq format and supervise our model to be a versatile math reasoner, verifier, prover, and augmenter. These abilities can be used to develop the next math LLMs or self-iteration. InternLM-Math obtains open-sourced state-of-the-art performance under the setting of in-context learning, supervised fine-tuning, and code-assisted reasoning in various informal and formal benchmarks including GSM8K, MATH, Hungary math exam, MathBench-ZH, and MiniF2F. Our pre-trained model achieves 30.3 on the MiniF2F test set without fine-tuning. We further explore how to use LEAN to solve math problems and study its performance under the setting of multi-task learnin
    
[^14]: 同性质，聚类和分类器

    Isotropy, Clusters, and Classifiers

    [https://arxiv.org/abs/2402.03191](https://arxiv.org/abs/2402.03191)

    同性质的嵌入空间对聚类和线性分类目标具有负面影响，这一事实得到了本文的实证支持，并对文献中的先前结果有所启示。

    

    最近，关于嵌入空间是否均匀利用所有维度（即是否具有同性质）的问题引起了讨论。有证据支持和反对在嵌入空间中实施同性质。在本文中，我们强调同性质对嵌入空间的要求与聚类的存在不兼容，这也对线性分类目标产生了负面影响。我们通过实验证明了这个事实，并用它来阐明文献中的先前结果。

    Whether embedding spaces use all their dimensions equally, i.e., whether they are isotropic, has been a recent subject of discussion. Evidence has been accrued both for and against enforcing isotropy in embedding spaces. In the present paper, we stress that isotropy imposes requirements on the embedding space that are not compatible with the presence of clusters -- which also negatively impacts linear classification objectives. We demonstrate this fact empirically and use it to shed light on previous results from the literature.
    
[^15]: 论文标题：解码时间对齐的语言模型

    Decoding-time Realignment of Language Models

    [https://arxiv.org/abs/2402.02992](https://arxiv.org/abs/2402.02992)

    本研究提出了解码时间对齐（DeRa）方法，可以在不重新训练模型的情况下探索和评估不同的规则化强度，从而对齐语言模型和人类偏好。

    

    将语言模型与人类偏好对齐对于减少模型中的错误和偏差非常重要。对齐技术，如从人类反馈中进行的强化学习（RLHF），通常被视为在人类偏好奖励和鼓励保持与未对齐模型接近的接近性规则项之间进行优化的权衡。选择适当的规则化水平至关重要：规则化不足可能导致由于奖励欺骗而降低模型能力，而过度规则化则阻碍对齐。传统方法找到最佳规则化水平需要使用不同规则化强度重新训练多个模型。然而，这个过程耗费资源，特别是对于大型模型来说。为了解决这个挑战，我们提出了解码时间对齐（DeRa），一种简单的方法，在无需重新训练的情况下探索和评估不同的规则化强度。DeRa可以对对齐模型的程度进行控制。

    Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of al
    
[^16]: 在多模态大型语言模型中为图推理渲染图形

    Rendering Graphs for Graph Reasoning in Multimodal Large Language Models

    [https://arxiv.org/abs/2402.02130](https://arxiv.org/abs/2402.02130)

    本文在多模态大型语言模型中为图推理任务引入了视觉信息，并提出了一个新的基准测试数据集GITQA。实验证明，结合文本和视觉信息的结果比单一模态效果更好。

    

    大型语言模型(LLMs)在机器人规划、知识图谱补全和常识推理等任务中越来越多地使用图结构，LLMs能够理解文本格式的图信息，但忽视了丰富的视觉模态，而视觉是人类理解结构信息和进行图推理的直观方式。将图结构表示为视觉图像(即视觉图)的潜在益处和能力仍未被探索。本文在图推理任务中首次引入视觉信息，并提出一个新的基准测试数据集GITQA，其中每个样本是一个元组(图、图像、文本描述)。我们利用最先进的多模态LLMs在GITQA基准测试数据集上进行了大量实验证明，结合文本和视觉信息的结果比单一模态效果更好。此外，在LLaVA-7B/13B模型的微调上表现出色。

    Large Language Models (LLMs) are increasingly used for various tasks with graph structures, such as robotic planning, knowledge graph completion, and common-sense reasoning. Though LLMs can comprehend graph information in a textual format, they overlook the rich visual modality, which is an intuitive way for humans to comprehend structural information and conduct graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., visual graph) is still unexplored. In this paper, we take the first step in incorporating visual information into graph reasoning tasks and propose a new benchmark GITQA, where each sample is a tuple (graph, image, textual description). We conduct extensive experiments on the GITQA benchmark using state-of-the-art multimodal LLMs. Results on graph reasoning tasks show that combining textual and visual information together performs better than using one modality alone. Moreover, the LLaVA-7B/13B models finetuned on 
    
[^17]: SMILE：多模态数据集用于语言模型理解视频中的笑声

    SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models

    [https://arxiv.org/abs/2312.09818](https://arxiv.org/abs/2312.09818)

    本论文介绍了一个新任务，即视频笑声推理，旨在解释特定视频中人们笑的原因，并提出了数据集SMILE。通过利用大型语言模型生成文本视频表示，我们的基线能够生成合理的笑声解释。

    

    尽管人工智能最近取得了进展，但构建社交智能仍然是一个挑战。其中，笑声是人类社交互动中发生的独特表达之一。在这项工作中，我们面对了机器理解视频中笑声背后理由的新挑战，即视频笑声推理。我们介绍了这一新任务，解释人们在特定视频中为什么会笑的原因，并提出了用于这一任务的数据集SMILE。我们提出了一个基线，通过利用大型语言模型（LLMs）的推理能力和文本视频表示生成合理的笑声解释。实验表明，我们的基线可以生成可信的笑声解释。我们进一步探讨了我们的基线在探测其他视频理解任务和野外视频方面的可扩展性。我们发布了我们的数据集、代码和模型。

    arXiv:2312.09818v2 Announce Type: replace-cross  Abstract: Despite the recent advances of the artificial intelligence, building social intelligence remains a challenge. Among social signals, laughter is one of the distinctive expressions that occurs during social interactions between humans. In this work, we tackle a new challenge for machines to understand the rationale behind laughter in video, Video Laugh Reasoning. We introduce this new task to explain why people laugh in a particular video and a dataset for this task. Our proposed dataset, SMILE, comprises video clips and language descriptions of why people laugh. We propose a baseline by leveraging the reasoning capacity of large language models (LLMs) with textual video representation. Experiments show that our baseline can generate plausible explanations for laughter. We further investigate the scalability of our baseline by probing other video understanding tasks and in-the-wild videos. We release our dataset, code, and model 
    
[^18]: UHGEval：通过无约束生成评估中文大型语言模型的虚构能力

    UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation

    [https://arxiv.org/abs/2311.15296](https://arxiv.org/abs/2311.15296)

    评估了中文大型语言模型在文本生成中的虚构能力，并指出现有基准评估通常采用受限制的生成技术，无法满足真实需求中的无约束文本生成。

    

    大型语言模型（LLMs）已成为当代自然语言处理领域的重要贡献者，并越来越广泛地应用于各种行业。然而，这些大规模概率统计模型目前无法保证在专业内容生成中的必要质量。这些模型经常产生虚构的文本，影响它们在专业环境中的实用性。为了评估LLMs在文本生成中的真实可靠性，许多倡议已开发了用于虚构现象的基准评估。然而，这些基准评估通常利用受限制的生成技术，因为成本和时间限制。这些技术包括使用定向幻觉诱导和故意改变真实文本以产生幻觉的策略。这些方法与实际需求的无约束文本生成不一致。

    arXiv:2311.15296v2 Announce Type: replace  Abstract: Large language models (LLMs) have emerged as pivotal contributors in contemporary natural language processing and are increasingly being applied across a diverse range of industries. However, these large-scale probabilistic statistical models cannot currently ensure the requisite quality in professional content generation. These models often produce hallucinated text, compromising their practical utility in professional contexts. To assess the authentic reliability of LLMs in text generation, numerous initiatives have developed benchmark evaluations for hallucination phenomena. Nevertheless, these benchmarks frequently utilize constrained generation techniques due to cost and temporal constraints. These techniques encompass the use of directed hallucination induction and strategies that deliberately alter authentic text to produce hallucinations. These approaches are not congruent with the unrestricted text generation demanded by rea
    
[^19]: StrategyLLM：大型语言模型作为问题解决的策略生成器、执行器、优化器和评估器

    StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving

    [https://arxiv.org/abs/2311.08803](https://arxiv.org/abs/2311.08803)

    StrategyLLM提出了一个框架，利用大型语言模型的能力自动构建可推广和一致的少次提示，优于竞争基线，不需要人工参与。

    

    大多数现有的思维链 (CoT) 提示方法存在泛化和一致性问题，因为它们常常依赖于特定实例的解决方案，这些解决方案可能不适用于其他情况，并缺乏在推理步骤中的任务级一致性。为解决这些限制，我们提出了一个全面的框架，StrategyLLM，利用LLM的能力自动构建可推广和一致的少次提示以用于各种任务。为此，StrategyLLM 使用四个基于LLM的代理：策略生成器、执行器、优化器和评估器，共同工作以为给定任务生成、评估和选择有前途的策略。实验结果表明，在13个数据集上跨4个挑战性任务上，不需要人工参与，StrategyLLM 在数学推理（34.21%->38.79%）、常见推理等任务上优于竞争基线CoT-SC，该基线需要人工注释的解决方案。

    arXiv:2311.08803v2 Announce Type: replace  Abstract: Most existing chain-of-thought (CoT) prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other cases and lack task-level consistency in their reasoning steps. To address these limitations, we propose a comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to construct generalizable and consistent few-shot prompts for various tasks automatically. To this end, StrategyLLM employs four LLM-based agents: strategy generator, executor, optimizer, and evaluator, working together to generate, evaluate, and select promising strategies for a given task. The experimental results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning (34.21% $\rightarrow$ 38.79%), commonse
    
[^20]: 再问一次：自一致性改善语言模型在（几乎）所有场景中的推理

    Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios

    [https://arxiv.org/abs/2311.08154](https://arxiv.org/abs/2311.08154)

    自一致性是一种通用的集成优化方法，可以应用于几乎所有情景中，能够解决语言模型推理中的重复性和局部最优性问题。

    

    尽管思维链（CoT）提示结合语言模型在复杂推理任务上取得了令人鼓舞的结果，但CoT提示中通常使用的贪婪解码会导致重复性和局部最优性。为解决这一缺点，集成优化尝试获得多个推理路径以得到最终答案集成。然而，当前的集成优化方法要么简单地采用基于规则的后处理，比如“自一致性”，要么训练一个基于几个与任务相关的人类注释的附加模型来在多个推理路径中选择最佳路径，但未能推广到现实设置，其中输入问题类型未知或推理路径的答案格式未知。为了避免它们的局限性，我们提出了“自一致性”，这是一种通用的集成优化方法，在几乎所有情景中适用，其中输入问题的类型...

    arXiv:2311.08154v2 Announce Type: replace-cross  Abstract: Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local optimality. To address this shortcoming, ensemble-optimization tries to obtain multiple reasoning paths to get the final answer assembly. However, current ensemble-optimization methods either simply employ rule-based post-processing such as \textit{self-consistency}, or train an additional model based on several task-related human annotations to select the best one among multiple reasoning paths, yet fail to generalize to realistic settings where the type of input questions is unknown or the answer format of reasoning paths is unknown. To avoid their limitations, we propose \textbf{Self-Agreement}, a generalizable ensemble-optimization method applying in almost all scenarios where the type of input question
    
[^21]: 用于英语新闻文章句子级主观性检测的语料库

    A Corpus for Sentence-level Subjectivity Detection on English News Articles

    [https://arxiv.org/abs/2305.18034](https://arxiv.org/abs/2305.18034)

    该研究开发了新的标注指南，建立了用于英语新闻文章句子级主观性检测的语料库，并表明多语境训练的模型在该任务上取得了最佳性能。

    

    我们制定了用于句子级主观性检测的新颖标注指南，不局限于特定语言的线索。我们利用这些指南收集了NewsSD-ENG，这是从有争议话题的英语新闻文章中提取的638个客观句子和411个主观句子的语料库。我们的语料库为英语及其他语言的主观性检测铺平了道路，而无需依赖于词典或机器翻译等特定语言工具。我们在单语、多语和跨语言设置下评估了最先进的多语言变压器模型在该任务上的表现。为此，我们重新注释了现有的意大利语语料库。我们发现，在多语境中训练的模型在该任务上取得了最佳性能。

    arXiv:2305.18034v2 Announce Type: replace  Abstract: We develop novel annotation guidelines for sentence-level subjectivity detection, which are not limited to language-specific cues. We use our guidelines to collect NewsSD-ENG, a corpus of 638 objective and 411 subjective sentences extracted from English news articles on controversial topics. Our corpus paves the way for subjectivity detection in English and across other languages without relying on language-specific tools, such as lexicons or machine translation. We evaluate state-of-the-art multilingual transformer-based models on the task in mono-, multi-, and cross-language settings. For this purpose, we re-annotate an existing Italian corpus. We observe that models trained in the multilingual setting achieve the best performance on the task.
    
[^22]: AgentCoder: 基于多Agent的代码生成和迭代测试与优化

    AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation. (arXiv:2312.13010v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.13010](http://arxiv.org/abs/2312.13010)

    AgentCoder是一种基于多Agent的代码生成和测试优化解决方案，通过程序员Agent、测试设计师Agent和测试执行Agent的协作，实现了在平衡代码生成和有效测试用例生成与执行方面的挑战中的突破。

    

    自然语言处理(NLP)的发展在大型语言模型(LLM)的发展推动下取得了显著的进展。这些模型在代码生成方面实现了革命性的突破，帮助开发人员提高软件的效率。尽管取得了这些进展，但在平衡代码段的生成与有效的测试用例生成和执行方面仍存在挑战。为了解决这些问题，本文介绍了一种新颖的解决方案——多Agent助手代码生成(AgentCoder)，该解决方案包括一个多Agent框架和专门的Agent：程序员Agent、测试设计师Agent和测试执行Agent。在编码过程中，程序员Agent将根据测试执行Agent的反馈重点关注代码的生成和改进。测试设计师Agent将为生成的代码生成测试用例，测试执行Agent将使用测试用例运行代码并将反馈写入到编程者

    The advancement of natural language processing (NLP) has been significantly boosted by the development of transformer-based large language models (LLMs). These models have revolutionized NLP tasks, particularly in code generation, aiding developers in creating software with enhanced efficiency. Despite their advancements, challenges in balancing code snippet generation with effective test case generation and execution persist. To address these issues, this paper introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution comprising a multi-agent framework with specialized agents: the programmer agent, the test designer agent, and the test executor agent. During the coding procedure, the programmer agent will focus on the code generation and refinement based on the test executor agent's feedback. The test designer agent will generate test cases for the generated code, and the test executor agent will run the code with the test cases and write the feedback to the prog
    
[^23]: JsonTuning：面向通用、强大和可控的指令调优

    JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning. (arXiv:2310.02953v1 [cs.CL])

    [http://arxiv.org/abs/2310.02953](http://arxiv.org/abs/2310.02953)

    JsonTuning是一种面向通用、强大和可控的指令调优方法，通过利用JSON的结构化特性，帮助模型理解任务要素及其关系，从而扩展了通用性、提高了稳健性，并增强了对输出的控制。

    

    指令调优已成为利用大型语言模型（LLM）能力的关键过程，通过提供明确的任务指令，从而在各种任务中提高性能。然而，目前的文本-文本指令调优（TextTuning）方法由于任务的模糊性和缺乏明确的结构而存在通用性、稳健性和可控性的限制。在本文中，我们提出了JsonTuning，这是一种新的结构到结构的指令调优方法。通过利用JSON的多功能和结构化特性来表示任务，JsonTuning通过帮助模型理解关键任务要素及其关系，扩展了通用性，通过最小化歧义性提高了稳健性，并通过提供对输出的显式控制增强了可控性。我们对不同的语言模型和评估基准进行了全面的比较研究。实验结果表明，JsonTuning在性能上优于TextTuning。

    Instruction tuning has emerged as a crucial process for harnessing the capabilities of large language models (LLMs) by providing explicit task instructions, leading to improved performance in various tasks. However, prevalent text-to-text instruction tuning (TextTuning) methods suffer from limitations in generalization, robustness, and controllability due to the ambiguity and lack of explicit structure in tasks. In this paper, we propose JsonTuning, a novel structure-to-structure approach for instruction tuning. By leveraging the versatility and structured nature of JSON to represent tasks, JsonTuning enhances generalization by helping the model understand essential task elements and their relations, improves robustness by minimizing ambiguity, and increases controllability by providing explicit control over the output. We conduct a comprehensive comparative study with diverse language models and evaluation benchmarks. Experimental results show that JsonTuning outperforms TextTuning in
    
[^24]: MindDial: 带有心智模拟的信念动态跟踪用于场景化神经对话生成

    MindDial: Belief Dynamics Tracking with Theory-of-Mind Modeling for Situated Neural Dialogue Generation. (arXiv:2306.15253v1 [cs.CL])

    [http://arxiv.org/abs/2306.15253](http://arxiv.org/abs/2306.15253)

    MindDial是一个使用心智模拟进行信念动态跟踪的对话生成框架，可以在场景化环境中生成自由对话来协商共识。

    

    人类在交流中自由表达意义或共识的同时进行对话。尽管大型生成语言模型具有令人印象深刻的对话能力，但它们并未考虑到共享的场景环境中个体的上下文理解差异。本文提出了MindDial，一种新颖的对话框架，可以生成场景化的自由对话来协商共识。我们设计了一个明确的心智模块，可以追踪三个层次的信念，即说话者的信念、说话者对听众信念的预测以及基于前两者之间的共同信念。然后，说话行为分类头将决定是否继续对话、结束此轮对话或采取与任务相关的行动。我们使用了一个共识对齐的数据集MutualFriend，增加了信念动态注释，目标是根据两个代理之间的自由对话找到一个共同的朋友。实验证明，我们的模型在心智状态建模方面取得了良好的效果。

    Humans talk in free-form while negotiating the expressed meanings or common ground. Despite the impressive conversational abilities of the large generative language models, they do not consider the individual differences in contextual understanding in a shared situated environment. In this work, we propose MindDial, a novel conversational framework that can generate situated free-form responses to negotiate common ground. We design an explicit mind module that can track three-level beliefs -- the speaker's belief, the speaker's prediction of the listener's belief, and the common belief based on the gap between the first two. Then the speaking act classification head will decide to continue to talk, end this turn, or take task-related action. We augment a common ground alignment dataset MutualFriend with belief dynamics annotation, of which the goal is to find a single mutual friend based on the free chat between two agents. Experiments show that our model with mental state modeling can
    
[^25]: PandaLM：LLM指令调优优化的自动评估基准

    PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. (arXiv:2306.05087v1 [cs.CL])

    [http://arxiv.org/abs/2306.05087](http://arxiv.org/abs/2306.05087)

    PandaLM是一个评估LLM指令调优的自动基准，它能够区分最优模型，并关注于主观因素。

    

    由于超参数选择的复杂性和评估调整模型的困难性，LLM（大型语言模型）的指令调优仍然是一项具有挑战性的任务。为确定最佳超参数，需要一个自动的、强大且可靠的评估基准。然而，由于评估准确性和隐私保护的挑战，建立这样一个基准并不是一项简单的任务。为应对这些挑战，我们引入了一款名为PandaLM的评测大型语言模型，该模型经过训练，能够区分出多个LLM中最佳的模型。PandaLM的关注点不仅限于传统评估数据集的客观正确性，还涵盖了诸如相对简洁性、清晰度、遵循说明、全面性和形式性等重要主观因素。为确保PandaLM的可靠性，我们收集了一个多样化的人工注释测试数据集，其中所有上下文都是生成的。

    Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated
    
[^26]: Reprompting: 通过吉布斯采样自动推断思维链的提示

    Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling. (arXiv:2305.09993v1 [cs.LG])

    [http://arxiv.org/abs/2305.09993](http://arxiv.org/abs/2305.09993)

    Reprompting是一种无需人类干预的算法，通过迭代采样新配方解决多步推理任务，比人类编写的思维链提示表现更好，还可以提高较弱模型的性能。

    

    我们引入了Reprompting，这是一种迭代采样算法，可以在没有人类干预的情况下搜索给定任务的思维链配方。通过吉布斯采样，我们推断适用于一组训练样例的思维链配方。我们的方法使用先前采样的解作为父提示，迭代地采样新的配方来解决其他训练问题。在需要多步推理的五个Big-Bench Hard任务中，Reprompting的表现始终优于零样本、少样本和人类编写的思维链基线。Reprompting还可以促进知识从一个更强的模型到一个较弱的模型的转移，从而大大提高了较弱模型的性能。总体而言，Reprompting相对于使用人类编写的思维链提示的先前最先进方法，带来了高达+17个性能改进。

    We introduce Reprompting, an iterative sampling algorithm that searches for the Chain-of-Thought (CoT) recipes for a given task without human intervention. Through Gibbs sampling, we infer CoT recipes that work consistently well for a set of training samples. Our method iteratively samples new recipes using previously sampled solutions as parent prompts to solve other training problems. On five Big-Bench Hard tasks that require multi-step reasoning, Reprompting achieves consistently better performance than the zero-shot, few-shot, and human-written CoT baselines. Reprompting can also facilitate transfer of knowledge from a stronger model to a weaker model leading to substantially improved performance of the weaker model. Overall, Reprompting brings up to +17 point improvements over the previous state-of-the-art method that uses human-written CoT prompts.
    

