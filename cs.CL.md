# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos](https://arxiv.org/abs/2403.05535) | 该论文提出了LaGTran框架，利用文本描述来引导知识转移，在处理具有挑战性的数据集上表现出显著优势。 |
| [^2] | [Bayesian Preference Elicitation with Language Models](https://arxiv.org/abs/2403.05534) | 提出了一个名为OPEN的框架，利用贝叶斯最优实验设计来引导偏好引导 |
| [^3] | [Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/abs/2403.05530) | Gemini 1.5 Pro是一种高效计算的多模态混合模型，能在数百万标记的上下文中回忆和推理信息，达到近乎完美的长上下文检索任务召回率，改进了长文档问答、长视频问答和长上下文ASR的最新技术水平。 |
| [^4] | [GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM](https://arxiv.org/abs/2403.05527) | GEAR提出了一种高效的KV缓存压缩框架，实现几乎无损的高比率压缩，用于解决大型语言模型推断中因缓存需求增长而导致的记忆绑定问题和性能下降。 |
| [^5] | [Authorship Attribution in Bangla Literature (AABL) via Transfer Learning using ULMFiT](https://arxiv.org/abs/2403.05519) | 该论文提出了使用AWD-LSTM架构和有效的迁移学习方法来解决孟加拉文学中作者归属问题的研究，填补了该领域在复杂语言特征提取和作者规模方面的空白。 |
| [^6] | [Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought](https://arxiv.org/abs/2403.05518) | 引入偏差增强的一致性训练（BCT）可以显著减少链式思维中的偏见推理问题，尤其是通过训练模型在带有和不带有偏置特征的提示下进行一致的推理。 |
| [^7] | [To Err Is Human, but Llamas Can Learn It Too](https://arxiv.org/abs/2403.05493) | 通过人工错误生成来提高语法错误纠正，进而在多种语言中取得优越的表现。 |
| [^8] | [FFSTC: Fongbe to French Speech Translation Corpus](https://arxiv.org/abs/2403.05488) | 该论文介绍了丰贝语到法语语音翻译语料库（FFSTC）首次推出，包括约31小时的语音内容，使用Fairseq的transformer_s和conformer模型评估了数据质量，建立了一个基准。 |
| [^9] | [Will GPT-4 Run DOOM?](https://arxiv.org/abs/2403.05468) | GPT-4通过自身的推理和观察能力，可以运行并玩1993年的第一人称射击游戏《毁灭战士》，并且能够执行门操作、击败敌人和规划路径，这有望拓展基于LLM的智能代理在视频游戏领域的边界。 |
| [^10] | [Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs](https://arxiv.org/abs/2403.05434) | 该论文旨在通过考虑代码混合等手段，降低在当代LLMs中处理低资源语言任务的成本，以确保预测和生成质量不受损。 |
| [^11] | [HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction](https://arxiv.org/abs/2403.05396) | HistGen是一个通过本地-全局特征编码和跨模态上下文交互来生成组织病理学报告的框架，提供了第一个用于评估的基准数据集。 |
| [^12] | [The Impact of Quantization on the Robustness of Transformer-based Text Classifiers](https://arxiv.org/abs/2403.05365) | 量化可以显著提高Transformer-based文本分类器在面对对抗攻击时的鲁棒性表现，平均提升了18.68%。 |
| [^13] | [Explaining Pre-Trained Language Models with Attribution Scores: An Analysis in Low-Resource Settings](https://arxiv.org/abs/2403.05338) | 本研究分析在低资源环境中从预备训练模型中提取的归因分数，并发现使用提示范式产生的解释比微调模型更合理，Shapley值采样表现出色。 |
| [^14] | [Consecutive Model Editing with Batch alongside HooK Layers](https://arxiv.org/abs/2403.05330) | 提出了一种内存友好的连续模型编辑与批量支持的方法COMEBA-HK，在实验中表现出优越性。 |
| [^15] | [ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues](https://arxiv.org/abs/2403.05326) | 本文提出了一个新的基于聊天的方面情绪理解（ChatASU）任务，旨在探索大型语言模型（LLMs）在对话场景中理解方面情绪的能力，并引入了一个子任务Aspect Chain Reasoning（ACR）任务来解决方面共指问题。 |
| [^16] | [RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation](https://arxiv.org/abs/2403.05313) | RAT方法通过检索增强思维，在长视角生成中改善大型语言模型的推理和生成能力，显著降低了幻觉，并取得了显著的性能提升 |
| [^17] | [ACLSum: A New Dataset for Aspect-based Summarization of Scientific Publications](https://arxiv.org/abs/2403.05303) | ACLSum是一个由领域专家精心制作和评估的新型摘要数据集，支持对科学论文进行多方面摘要，深入涵盖挑战、方法和结果。 |
| [^18] | [PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck](https://arxiv.org/abs/2403.05297) | PEEB是一种基于部分的图像分类器，通过将类别名称转换为描述视觉部分的文本描述符，并将检测到的部分的嵌入与文本描述符匹配，从而在零样本设置中表现出色，并且不仅在监督学习中表现出色，而且还首次实现用户编辑类定义形成新分类器无需重新训练。 |
| [^19] | [LLM4Decompile: Decompiling Binary Code with Large Language Models](https://arxiv.org/abs/2403.05286) | 发布首批开放访问的反编译LLM，预训练在40亿个C源代码和汇编代码标记上，引入了第一个考虑重新编译性和重新执行性的反编译数据集。 |
| [^20] | [ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models](https://arxiv.org/abs/2403.05266) | ERBench是一个基于实体关系的大型语言模型幻觉基准，通过自动转换任何关系数据库并构建可自动验证的问题，以支持复杂性评估和调试 |
| [^21] | [Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity](https://arxiv.org/abs/2403.05257) | 该研究对比了跨语言转移和机器翻译两种数据增强技术在单语义文本相似性中的表现，并发现维基百科领域优于NLI领域。 |
| [^22] | [Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering](https://arxiv.org/abs/2403.05217) | 提出了LLMQA框架，利用大型语言模型在开放领域问答中扮演生成器、重新排序器和评估器等多重角色，结合了检索和生成证据的优势。 |
| [^23] | [SocialPET: Socially Informed Pattern Exploiting Training for Few-Shot Stance Detection in Social Media](https://arxiv.org/abs/2403.05216) | SocialPET 是一种社交信息引导的方法，通过结合语言模型和社交网络结构来进行立场检测，在少样本情况下优于其他竞争模型。 |
| [^24] | [Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge](https://arxiv.org/abs/2403.05189) | 本研究追踪了多语言语言模型中事实的来源，发现了三种模式：语言独立、跨语言共享和转移，为区分它们提出了方法，凸显了在多语言LMs中保持一致事实知识的挑战，强调了需要在ML-LMs中改进事实表示学习。 |
| [^25] | [CommitBench: A Benchmark for Commit Message Generation](https://arxiv.org/abs/2403.05188) | 提出了一个名为CommitBench的新大规模数据集，采用最佳实践进行数据集创建，解决了现有数据集存在的问题，以改善生成的提交信息的质量。 |
| [^26] | [ROUGE-K: Do Your Summaries Have Keywords?](https://arxiv.org/abs/2403.05186) | ROUGE-K是一种关键词导向的评估指标，通过量化回答摘要是否包含关键词这一问题，研究发现当前强基线模型在摘要中经常遗漏关键信息，人类注释员觉得包含更多关键词的摘要更相关。 |
| [^27] | [Towards a Psychology of Machines: Large Language Models Predict Human Memory](https://arxiv.org/abs/2403.05152) | 这项研究探索了大型语言模型在预测基于语言的记忆任务中的表现，并通过其对模棱两可句子的处理能力增进了对人类认知机制的理解。 |
| [^28] | [ChatUIE: Exploring Chat-based Unified Information Extraction using Large Language Models](https://arxiv.org/abs/2403.05132) | ChatUIE利用大型语言模型探索基于聊天的统一信息提取，通过强化学习和生成约束提高对自然语言中结构化信息的提取能力。 |
| [^29] | [Rule-driven News Captioning](https://arxiv.org/abs/2403.05101) | 本文提出了一种基于规则的新闻标题生成方法，通过新闻感知的语义规则，可以生成遵循新闻报道基本规则的图像描述。 |
| [^30] | [Can we obtain significant success in RST discourse parsing by using Large Language Models?](https://arxiv.org/abs/2403.05065) | 研究探索了如何利用大型语言模型（LLMs）来进行RST语篇解析，并在底部策略中取得了最新的最先进结果。 |
| [^31] | [Are Human Conversations Special? A Large Language Model Perspective](https://arxiv.org/abs/2403.05045) | 本研究分析了大型语言模型在理解人类对话时的注意机制变化，发现尽管语言模型在特定领域表现出不同的注意行为，但在专门处理人类对话方面存在明显差距，需要通过多样化的高质量对话数据训练模型来增强理解和生成 |
| [^32] | [Towards Multimodal Sentiment Analysis Debiasing via Bias Purification](https://arxiv.org/abs/2403.05023) | 提出了一种基于因果关系的多模态对事实推理情感分析框架，用于净化和缓解数据集的偏见，从而提高多模态情感分析的性能。 |
| [^33] | [Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs](https://arxiv.org/abs/2403.05020) | 研究发现，使用LLMs进行社交互动的全知模拟比非全知模拟更容易实现社交目标，尽管非全知模拟更接近实际情况。 |
| [^34] | [Can't Remember Details in Long Documents? You Need Some R&R](https://arxiv.org/abs/2403.05004) | 引入R&R方法，结合reprompting和in-context retrieval两种新型提示方式，提高了在长文档上的问答任务的准确性。 |
| [^35] | [DiffChat: Learning to Chat with Text-to-Image Synthesis Models for Interactive Image Creation](https://arxiv.org/abs/2403.04997) | DiffChat 是一种新方法，通过对齐大型语言模型与文本到图像合成模型，实现了在互动图像创作中进行聊天并生成高质量图像的目标。 |
| [^36] | [Tell me the truth: A system to measure the trustworthiness of Large Language Models](https://arxiv.org/abs/2403.04964) | 本文提出了一种基于预定义领域知识图的系统化方法来衡量大型语言模型的可信度。 |
| [^37] | [An In-depth Evaluation of GPT-4 in Sentence Simplification with Error-based Human Assessment](https://arxiv.org/abs/2403.04963) | 本文深入评估了GPT-4在句子简化中的表现，指出现有自动评估指标和人类评估方法对于大型语言模型的适用性仍有待进一步研究。 |
| [^38] | [SecGPT: An Execution Isolation Architecture for LLM-Based Systems](https://arxiv.org/abs/2403.04960) | 提出了一种面向LLM系统的执行隔离架构SecGPT，旨在解决第三方应用程序执行所引发的安全和隐私问题 |
| [^39] | [Electrocardiogram Instruction Tuning for Report Generation](https://arxiv.org/abs/2403.04945) | 提出了Multimodal ECG Instruction Tuning（MEIT）框架，首次尝试使用LLMs和多模态指导解决ECG报告生成问题，并在两个大规模ECG数据集上进行了广泛的实验评估其优越性。 |
| [^40] | [A Survey on Human-AI Teaming with Large Pre-Trained Models](https://arxiv.org/abs/2403.04931) | 本文调查了大型预训练模型与人工智能合作的重要性，强调了这些模型如何超越传统方法增强协作智能，并探讨了其在增强人类能力、改善AI模型、有效团队合作、道德考虑以及在各个领域广泛应用方面的潜在作用。 |
| [^41] | [ConstitutionalExperts: Training a Mixture of Principle-based Prompts](https://arxiv.org/abs/2403.04894) | 提出了ConstitutionalExperts方法，通过学习宪法原则构建提示，采用逐步改进提示和MoE架构，展现出在不同语义区域学习独特提示的潜力，并在六个基准数据集上表现优异。 |
| [^42] | [Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering](https://arxiv.org/abs/2403.04890) | 本文提出了基于少样本推动推理的链式思维驱动LLMs用于开放式医学问题回答，通过修改MedQA-USMLE数据集并采用奖励训练机制，实现了在医疗场景中正确响应临床问题的有效方法。 |
| [^43] | [Code-Mixed Probes Show How Pre-Trained Models Generalise On Code-Switched Text](https://arxiv.org/abs/2403.04872) | 这项研究探讨了预训练语言模型在处理混合码文本方面的能力，发现它们在检测、利用结构信息和表达语义信息方面表现良好 |
| [^44] | [Evaluating Biases in Context-Dependent Health Questions](https://arxiv.org/abs/2403.04858) | 研究评估了大型语言模型在健康领域中的上下文问题中存在的偏见，发现年轻成年女性用户受到偏爱 |
| [^45] | [Automating the Information Extraction from Semi-Structured Interview Transcripts](https://arxiv.org/abs/2403.04819) | 本研究提出了一种新的自动系统，结合了BERT嵌入和HDBSCAN聚类，可以从半结构化面谈文本中快速提取信息，为研究人员提供了一个便捷的工具来分析和可视化主题结构。 |
| [^46] | [Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks](https://arxiv.org/abs/2403.04814) | 该研究引入了一个新的基准SAFIM用于评估LLMs在代码填空任务上的句法感知完成表现，发现FIM预训练不仅提高了FIM的熟练度，还改善了LLMs的左到右推理，挑战了传统观念并表明预训练方法和数据品质对模型性能的影响更甚于模型大小。 |
| [^47] | [Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models](https://arxiv.org/abs/2403.04811) | 研究量化了流行的代码生成基准测试的数据污染程度，揭示了它们与预训练语料库之间的重叠，并展示模型的显著性能重叠。 |
| [^48] | [WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off](https://arxiv.org/abs/2403.04808) | WaterMax提出了一种新的水印方案，能够在保持生成文本质量的同时实现高检测性能，打破了水印技术中质量和稳健性之间的传统平衡。 |
| [^49] | [AttentionStitch: How Attention Solves the Speech Editing Problem](https://arxiv.org/abs/2403.04804) | AttentionStitch模型通过引入双注意力块网络，并将编辑文本的mel频谱图与合成的mel频谱图自动融合，实现了语音编辑的无缝整合。 |
| [^50] | [Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs](https://arxiv.org/abs/2403.04801) | 使用LLM代理进行黑盒提示优化方法，揭示了受害代理中更高级别的记忆化，相比直接用训练数据提示目标模型，这种方法更有效，能更好地量化LLMs的记忆化。 |
| [^51] | [AI Literacy in Low-Resource Languages:Insights from creating AI in Yoruba videos](https://arxiv.org/abs/2403.04799) | 本研究探索了在低资源语言如约鲁巴语中创建和分发人工智能视频的方法，并展示了其在全球范围内的潜在影响。 |
| [^52] | [JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models](https://arxiv.org/abs/2403.04798) | 本文介绍了针对SemEval-2024任务3开发的多模态情感因果分析系统，提出了通过两步框架解决多模态情感因果分析挑战的方法，并在实验中取得显著性能提升。 |
| [^53] | [Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding](https://arxiv.org/abs/2403.04797) | 通过引入多尺度位置编码（Ms-PoE）来增强大型语言模型（LLMs）对上下文中间相关信息的处理能力，解决了LLMs面临的“中间丢失”挑战。 |
| [^54] | [Large Language Models in Fire Engineering: An Examination of Technical Questions Against Domain Knowledge](https://arxiv.org/abs/2403.04795) | 本研究比较了两个聊天机器人在消防工程中处理问题的表现，发现ChatGPT表现较优，展示了聊天机器人技术在消防工程实践中的潜力。 |
| [^55] | [Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?](https://arxiv.org/abs/2403.04792) | 本研究挑战了以往研究中建立的预翻译范式，并在108种语言中的94种语言中表明PaLM2-L在直接推断中优于预翻译。 |
| [^56] | [LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK Case Law Dataset](https://arxiv.org/abs/2403.04791) | 使用大型语言模型（LLM）对比传统的自然语言处理方法，可以更有效地从大型英国法院判决数据集中识别摘要裁定案例，取得了更高的F1得分。 |
| [^57] | [Online Training of Large Language Models: Learn while chatting](https://arxiv.org/abs/2403.04790) | 本论文提出了一种新的互动范式，允许大型语言模型通过外部互动进行在线训练，实现了持续、实时的模型更新与个性化定制的灵活性。 |
| [^58] | [TopicDiff: A Topic-enriched Diffusion Approach for Multimodal Conversational Emotion Detection](https://arxiv.org/abs/2403.04789) | 提出了一种TopicDiff方法，用于捕获多模态会话情感检测任务中的主题信息，通过将扩散模型集成到神经主题模型中，解决了神经主题模型在捕获主题信息方面的多样性不足问题，并相对于现有MCE基线取得了显著改进 |
| [^59] | [Topic Modeling Analysis of Aviation Accident Reports: A Comparative Study between LDA and NMF Models](https://arxiv.org/abs/2403.04788) | 本研究比较了航空事故报告分析中的两种主题建模技术，发现LDA在主题连贯性方面表现更出色，而NMF在提取主题方面优异。 |
| [^60] | [Ever-Evolving Memory by Blending and Refining the Past](https://arxiv.org/abs/2403.04787) | 提出了一种新颖的长期对话记忆方案CREEM，通过混合过去记忆并引入完善过程来实现聊天机器人回应的整体改进和连贯性。 |
| [^61] | [Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models](https://arxiv.org/abs/2403.04786) | 本文通过全面调查各种攻击形式，探讨了大型语言模型受攻击的性质、机制、潜在影响以及当前防御策略，为模型完整性和用户信任提供了重要见解。 |
| [^62] | [Large Language Multimodal Models for 5-Year Chronic Disease Cohort Prediction Using EHR Data](https://arxiv.org/abs/2403.04785) | 本研究提出了一种大型语言多模型（LLMMs）框架，结合临床笔记和实验室检验结果的多模态数据，用于预测慢性疾病风险。 |
| [^63] | [AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks](https://arxiv.org/abs/2403.04783) | 提出了一种基于响应过滤的多Agent防御框架AutoDefense，可以有效提高LLMs对抗越狱攻击的鲁棒性，同时保持正常用户请求的性能。 |
| [^64] | [A Survey on Temporal Knowledge Graph: Representation Learning and Applications](https://arxiv.org/abs/2403.04782) | 时间知识图表示学习将时间信息融入标准知识图框架，可以对实体和关系随时间的动态变化进行建模。 |
| [^65] | [MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining](https://arxiv.org/abs/2403.04780) | MuseGraph将GNNs和LLMs的优势结合起来，提出了一种更有效和通用的图挖掘方法，可以跨不同任务和数据集使用 |
| [^66] | [QASE Enhanced PLMs: Improved Control in Text Generation for MRC](https://arxiv.org/abs/2403.04771) | QASE模块在PLMs的微调过程中提升了文本生成在机器阅读理解中的控制能力，使得其在MRC任务中超越了GPT-4等领先的LLMs。 |
| [^67] | [Social Orientation: A New Feature for Dialogue Analysis](https://arxiv.org/abs/2403.04770) | 感知模型认为社交取向（例如，热情友好、傲慢冷漠）对话参与者可以用来预测和解释社交互动的结果。我们的工作在于系统性地应用社交取向标签来建模对话结果 |
| [^68] | [Removing GPT4's Filter](https://arxiv.org/abs/2403.04769) | 提出了一种方法，可以使经过微调的GPT4恢复到没有经过人类反馈强化学习训练的状态，从而移除其在学习期间的所有安全机制 |
| [^69] | [How Far Are We from Intelligent Visual Deductive Reasoning?](https://arxiv.org/abs/2403.04732) | 目前的视觉语言模型在文本推理方面表现出色，但在视觉演绎推理方面仍存在较大差距和盲点。 |
| [^70] | [Do Large Language Model Understand Multi-Intent Spoken Language ?](https://arxiv.org/abs/2403.04481) | 该研究利用大型语言模型进行口语语言多目标理解，提出了改进实体槽和子目标指令的创新技术，并展示了LLMs在多目标SLU模型方面的潜力。 |
| [^71] | [Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset](https://arxiv.org/abs/2403.04460) | Pearl数据集利用了角色和知识增强的大型语言模型，提供了具体用户偏好，领域专业性和更相关的推荐。 |
| [^72] | [CLLMs: Consistency Large Language Models](https://arxiv.org/abs/2403.00835) | 提出了一种新方法，通过精细调整目标LLM实现了对雅各比轨迹上固定点的一致性预测，有效提高了生成速度2.4倍到3.4倍。 |
| [^73] | [LLMBind: A Unified Modality-Task Integration Framework](https://arxiv.org/abs/2402.14891) | 提出了LLMBind，一种统一的模态任务集成框架，通过将大型语言模型和预训练任务模型绑定在一起，实现了多种模态任务的灵活输入和输出组合。 |
| [^74] | [CriticBench: Benchmarking LLMs for Critique-Correct Reasoning](https://arxiv.org/abs/2402.14809) | CriticBench是一个综合基准测试，旨在评估LLMs在批判和纠正推理方面的能力，发现批判性训练显著提升性能，逻辑任务更易于修正。 |
| [^75] | [SaGE: Evaluating Moral Consistency in Large Language Models](https://arxiv.org/abs/2402.13709) | 提出SaGE方法，通过语义图熵来衡量大型语言模型道德一致性，构建了MCC语料库。 |
| [^76] | [A Survey on Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2402.13116) | 本调查深入探讨了大型语言模型知识蒸馏技术的重要性，并阐明了将专有巨头的先进功能转移到开源模型中的关键作用。 |
| [^77] | [A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change](https://arxiv.org/abs/2402.12011) | 本文通过在相同条件下评估了最新的词汇语义变化模型和方法，将LSC问题分解为不同级别的任务，并展示了在不同语言的八个基准测试中，APD在GCD方面优于其他方法，XL-LEXEME在WiC、WSI和GCD方面优于其他上下文化模型，并且与GPT-4相当。 |
| [^78] | [Structured Entity Extraction Using Large Language Models](https://arxiv.org/abs/2402.04437) | 本论文提出了一种使用大型语言模型的结构化实体提取方法，在此任务上通过引入AESOP度量评估模型性能，并将整个提取任务分解为多个阶段，相较于基准模型取得了更好的效果，为未来结构化实体提取的进一步发展提供了有希望的方向。 |
| [^79] | [Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models](https://arxiv.org/abs/2312.14197) | 该研究引入了第一个间接提示注入攻击基准测试BIPIA，对大型语言模型在面对此类攻击时的风险进行评估，并分析了攻击成功的原因，从而开发了防御方法。 |
| [^80] | [LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin](https://arxiv.org/abs/2312.09979) | LoRAMoE是一个新颖的框架，通过引入低秩适配器和路由器网络，类似于MoE的插件版本，来解决大型语言模型中世界知识遗忘的问题。 |
| [^81] | [RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback](https://arxiv.org/abs/2312.00849) | RLHF-V通过细粒度纠正人类反馈的行为对准，提高了MLLM的可信度，使其在多模态理解、推理和互动方面表现出更可靠的行为。 |
| [^82] | [Evaluating Optimal Reference Translations](https://arxiv.org/abs/2311.16787) | 提出了一种用于创建更可靠的文档级人工参考翻译的方法论，称为“最佳参考翻译”，旨在提高被视为“人工翻译质量”的标准 |
| [^83] | [Language Generation from Brain Recordings](https://arxiv.org/abs/2311.09889) | 提出了一种在大脑记录中直接生成语言的方法，结合了大型语言模型和语义脑解码器，实现了从功能性磁共振成像输入生成与语义内容一致的连贯语言序列。 |
| [^84] | [Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction](https://arxiv.org/abs/2311.05922) | 提出了一种名为CoT-ER的新方法，使用大语言模型进行少样本关系抽取，实现了链式推理思维链与显式证据推理。 |
| [^85] | [Can LLMs Follow Simple Rules?](https://arxiv.org/abs/2311.04235) | 提出了一个名为RuLES的程序框架，用于衡量LLMs在与用户交互时遵守规则的能力。 |
| [^86] | [Improving Probability-based Prompt Selection Through Unified Evaluation and Analysis](https://arxiv.org/abs/2305.14877) | 通过统一框架解释和评估现有的基于概率的提示选择方法，开发多种互信息的组合变体，将Oracle提示选择方法的有效性提高到94.98%。 |
| [^87] | [RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization.](http://arxiv.org/abs/2401.14280) | 本研究提出了一种创新的方法，通过使用罗马化形式的文本作为接口，有效地利用大语言模型的多语言能力。通过在印地语上的实验证明，罗马化文本不仅提高了推理效率，还在有限的预训练下实现了有竞争力的性能。这些发现表明罗马化有潜力弥合大语言模型应用中的语言障碍。 |
| [^88] | [(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection.](http://arxiv.org/abs/2401.14040) | 本研究探讨了(Chat)GPT和BERT在语义变化检测任务中的性能，结果表明(Chat)GPT的表现明显低于BERT，尤其在长期变化检测方面表现更差。 |
| [^89] | [Enhancing Personality Recognition in Dialogue by Data Augmentation and Heterogeneous Conversational Graph Networks.](http://arxiv.org/abs/2401.05871) | 该论文提出了通过数据增强和异构对话图网络提高对话中的人格识别能力的方法，并证明了其在现有基线模型上取得了显著的改进。 |
| [^90] | [DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial Reasoning in Text.](http://arxiv.org/abs/2310.12557) | DepWiGNN是一种用于多跳空间推理的深度图神经网络。它通过设计新颖的节点记忆方案，并在图的深度维度上聚合信息，从而能够收集长时间的依赖关系，而无需堆叠多个层次。实验结果表明，DepWiGNN在两个挑战数据集上比传统GNN方法具有更高的准确性。 |
| [^91] | [One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models.](http://arxiv.org/abs/2310.09499) | 我们提出了一种基于敏感度感知混合稀疏化剪枝的方法，可以在不重新训练的情况下将大型语言模型剪枝至至少50％的稀疏性，同时保持稀疏性水平和减少剪枝引起的误差。此外，该方法还与量化兼容，可以进一步压缩语言模型。 |
| [^92] | [Art or Artifice? Large Language Models and the False Promise of Creativity.](http://arxiv.org/abs/2309.14556) | 本研究通过提出创造性写作的托兰斯测验(TTCW)来评估大型语言模型(LLMs)的写作创造力。结果表明，LLM生成的故事在创意测试中通过的数量比专业作家写的故事少。此外，我们发现LLMs无法代替专家进行TTCW评估。 |
| [^93] | [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models.](http://arxiv.org/abs/2309.12307) | LongLoRA是一种高效的精细调整方法，可以在有限的计算成本下扩展预训练的大型语言模型的上下文大小。它通过稀疏的局部注意力实现精细调整，并使用移动短注意力有效实现上下文扩展，与传统方法具有相似的性能。 |
| [^94] | [M3PS: End-to-End Multi-Grained Multi-Modal Attribute-Aware Product Summarization in E-commerce.](http://arxiv.org/abs/2308.11351) | M3PS是一种全面的多粒度多模态属性感知产品摘要方法，能够同时建模并生成高质量的产品摘要，解决了电子商务中产品摘要的端到端建模、多粒度多模态建模和多模态属性建模的问题。 |
| [^95] | [2x Faster Language Model Pre-training via Masked Structural Growth.](http://arxiv.org/abs/2305.02869) | 本文提出了掩码结构成长（MSG），可以加速语言模型的预训练，其中包括全维度成长进程和独立于新权重初始化的函数严格保留成长操作。 |
| [^96] | [A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube.](http://arxiv.org/abs/2303.16281) | 研究发现在Google、ChatGPT、维基百科和YouTube上，搜索结果受限于语言，反映了与复杂主题相关的文化刻板印象，缺乏跨文化视角。 |

# 详细

[^1]: 讲述，而不是展示！：语言指导有助于在图像和视频领域之间进行转移

    Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos

    [https://arxiv.org/abs/2403.05535](https://arxiv.org/abs/2403.05535)

    该论文提出了LaGTran框架，利用文本描述来引导知识转移，在处理具有挑战性的数据集上表现出显著优势。

    

    我们介绍了LaGTran，这是一个新颖的框架，利用即可获得或易于获取的文本描述，引导从带标签的源数据到具有域偏移的无标签目标数据的鲁棒性知识转移。受到我们观察到更富语义的文本模态具有更有利的转移特性的启发，我们设计了一个转移机制，使用源训练的文本分类器在目标文本描述上生成预测，并利用这些预测作为相应图像的监督。我们的方法以语言指导为驱动，出奇地简单易行，却在具有挑战性的数据集如GeoNet和DomainNet上显著优于以往所有方法，验证了其极其有效性。

    arXiv:2403.05535v1 Announce Type: cross  Abstract: We introduce LaGTran, a novel framework that utilizes readily available or easily acquired text descriptions to guide robust transfer of discriminative knowledge from labeled source to unlabeled target data with domain shifts. While unsupervised adaptation methods have been established to address this problem, they show limitations in handling challenging domain shifts due to their exclusive operation within the pixel-space. Motivated by our observation that semantically richer text modality has more favorable transfer properties, we devise a transfer mechanism to use a source-trained text-classifier to generate predictions on the target text descriptions, and utilize these predictions as supervision for the corresponding images. Our approach driven by language guidance is surprisingly easy and simple, yet significantly outperforms all prior approaches on challenging datasets like GeoNet and DomainNet, validating its extreme effectiven
    
[^2]: 具有语言模型的贝叶斯偏好引导

    Bayesian Preference Elicitation with Language Models

    [https://arxiv.org/abs/2403.05534](https://arxiv.org/abs/2403.05534)

    提出了一个名为OPEN的框架，利用贝叶斯最优实验设计来引导偏好引导

    

    将人工智能系统与用户兴趣对齐需要理解并融入人类复杂的价值观和偏好。最近，语言模型（LMs）已被用于收集关于人类用户偏好的信息。可以利用这些偏好数据来微调或指导其他LM和/或人工智能系统。然而，已经发现LM在偏好学习的关键方面存在困难：量化不确定性、建模人类心理状态和提出信息性问题。这些挑战在机器学习的其他领域中得到了解决，比如贝叶斯最优实验设计（BOED），它侧重于在一个明确定义的特征空间内设计信息丰富的查询。但这些方法反过来很难扩展并应用于在哪里仅仅识别相关特征都可能很困难的现实问题。我们引入OPEN（Optimal Preference Elicitation with Natural language），这是一个使用BOED来引导

    arXiv:2403.05534v1 Announce Type: new  Abstract: Aligning AI systems to users' interests requires understanding and incorporating humans' complex values and preferences. Recently, language models (LMs) have been used to gather information about the preferences of human users. This preference data can be used to fine-tune or guide other LMs and/or AI systems. However, LMs have been shown to struggle with crucial aspects of preference learning: quantifying uncertainty, modeling human mental states, and asking informative questions. These challenges have been addressed in other areas of machine learning, such as Bayesian Optimal Experimental Design (BOED), which focus on designing informative queries within a well-defined feature space. But these methods, in turn, are difficult to scale and apply to real-world problems where simply identifying the relevant features can be difficult. We introduce OPEN (Optimal Preference Elicitation with Natural language) a framework that uses BOED to guid
    
[^3]: Gemini 1.5：解锁跨数百万标记上下文的多模态理解

    Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context

    [https://arxiv.org/abs/2403.05530](https://arxiv.org/abs/2403.05530)

    Gemini 1.5 Pro是一种高效计算的多模态混合模型，能在数百万标记的上下文中回忆和推理信息，达到近乎完美的长上下文检索任务召回率，改进了长文档问答、长视频问答和长上下文ASR的最新技术水平。

    

    在这份报告中，我们介绍了Gemini家族的最新模型Gemini 1.5 Pro，这是一个高效计算的多模态专家混合模型，能够回忆和推理数百万标记上下文中的细粒度信息，包括多个长文档和几小时的视频和音频。Gemini 1.5 Pro在各种形式的长上下文检索任务中实现了近乎完美的召回率，改进了长文档问答、长视频问答和长上下文ASR的最新技术水平，并在广泛一系列基准测试中与Gemini 1.0 Ultra的最新技术水平相匹敌甚至超过。在研究Gemini 1.5 Pro长上下文能力的极限时，我们发现在至少10M标记的范围内继续改进下一个标记的预测，并且几乎完美地达到了超过99%的检索率，这是对现有模型如Claude 2.1（200k）和GPT-4 Turbo（128k）的世代性飞跃。最后，我们突出了大型语言模型在新领域的令人惊讶的新能力。

    arXiv:2403.05530v1 Announce Type: cross  Abstract: In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large language models at the
    
[^4]: GEAR: 一种用于几乎无损生成推断大型语言模型的高效KV缓存压缩方案

    GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM

    [https://arxiv.org/abs/2403.05527](https://arxiv.org/abs/2403.05527)

    GEAR提出了一种高效的KV缓存压缩框架，实现几乎无损的高比率压缩，用于解决大型语言模型推断中因缓存需求增长而导致的记忆绑定问题和性能下降。

    

    关键-值（KV）缓存已成为加快大型语言模型（LLMs）推断生成速度的事实标准。然而，随着序列长度增加而增长的缓存需求已将LLM推断转变为一个记忆绑定问题，显著地限制了系统吞吐量。现有方法依赖于丢弃不重要的标记或均匀量化所有条目。然而，这种方法往往会产生较高的近似误差来表示压缩后的矩阵。自回归解码过程进一步增加了每个步骤的误差，导致模型生成中的重大偏差和性能恶化。为了解决这一挑战，我们提出了GEAR，一种高效的KV缓存压缩框架，实现几乎无损的高压缩比。

    arXiv:2403.05527v1 Announce Type: cross  Abstract: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quant
    
[^5]: 通过使用ULMFiT的迁移学习进行孟加拉文学（AABL）的作者归属

    Authorship Attribution in Bangla Literature (AABL) via Transfer Learning using ULMFiT

    [https://arxiv.org/abs/2403.05519](https://arxiv.org/abs/2403.05519)

    该论文提出了使用AWD-LSTM架构和有效的迁移学习方法来解决孟加拉文学中作者归属问题的研究，填补了该领域在复杂语言特征提取和作者规模方面的空白。

    

    作者归属是创建适当的文本特征以捕捉作者写作风格从而识别一段文本的原始作者的任务。随着互联网上的匿名性增加，这一任务在各种安全和抄袭检测领域变得越来越关键。尽管英语、西班牙语和汉语等其他语言取得了显著进展，但由于孟加拉语的复杂语言特征和句子结构，该领域缺乏全面的研究。此外，现有系统在作者数量增加时不具可伸缩性，并且对每位作者样本数量较少时性能下降。本文提出了使用平均随机梯度下降权值丢弃长短期记忆（AWD-LSTM）架构和有效的迁移学习方法，以解决复杂语言特征提取和作者规模问题。

    arXiv:2403.05519v1 Announce Type: new  Abstract: Authorship Attribution is the task of creating an appropriate characterization of text that captures the authors' writing style to identify the original author of a given piece of text. With increased anonymity on the internet, this task has become increasingly crucial in various security and plagiarism detection fields. Despite significant advancements in other languages such as English, Spanish, and Chinese, Bangla lacks comprehensive research in this field due to its complex linguistic feature and sentence structure. Moreover, existing systems are not scalable when the number of author increases, and the performance drops for small number of samples per author. In this paper, we propose the use of Average-Stochastic Gradient Descent Weight-Dropped Long Short-Term Memory (AWD-LSTM) architecture and an effective transfer learning approach that addresses the problem of complex linguistic features extraction and scalability for authorship
    
[^6]: 通过偏差增强一致性训练减少链式思维中的偏见推理

    Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought

    [https://arxiv.org/abs/2403.05518](https://arxiv.org/abs/2403.05518)

    引入偏差增强的一致性训练（BCT）可以显著减少链式思维中的偏见推理问题，尤其是通过训练模型在带有和不带有偏置特征的提示下进行一致的推理。

    

    虽然链式思维提示（CoT）有潜力改善语言模型推理的可解释性，但它可能会系统性地歪曲影响模型行为的因素--比如，合理化答案以符合用户意见而不提及此偏见。为了减轻这一偏见推理问题，我们引入了偏差增强的一致性训练（BCT），这是一种无监督的微调方案，旨在训练模型在带有和不带有偏置特征的提示下进行一致的推理。我们构建了一个测试单元，针对七个问答任务测试了九种形式的有偏推理，发现将BCT应用于带有一种偏见的GPT-3.5-Turbo可以将有偏推理的比例在未知任务上降低86%。此外，这个模型推广到其他形式的偏见，平均将未知偏见上的有偏推理减少了37%。由于BCT将未知偏见泛化并且不需要金标签，这种方法可能会有助于

    arXiv:2403.05518v1 Announce Type: cross  Abstract: While chain-of-thought prompting (CoT) has the potential to improve the explainability of language model reasoning, it can systematically misrepresent the factors influencing models' behavior--for example, rationalizing answers in line with a user's opinion without mentioning this bias. To mitigate this biased reasoning problem, we introduce bias-augmented consistency training (BCT), an unsupervised fine-tuning scheme that trains models to give consistent reasoning across prompts with and without biasing features. We construct a suite testing nine forms of biased reasoning on seven question-answering tasks, and find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of biased reasoning by 86% on held-out tasks. Moreover, this model generalizes to other forms of bias, reducing biased reasoning on held-out biases by an average of 37%. As BCT generalizes to held-out biases and does not require gold labels, this method may h
    
[^7]: 人类会犯错，但羊驼也能学会

    To Err Is Human, but Llamas Can Learn It Too

    [https://arxiv.org/abs/2403.05493](https://arxiv.org/abs/2403.05493)

    通过人工错误生成来提高语法错误纠正，进而在多种语言中取得优越的表现。

    

    本研究探讨了利用语言模型（LMs）通过人工错误生成（AEG）来增强语法错误纠正（GEC）。具体而言，我们对基于Llama 2的LMs进行微调以生成错误，并发现这种方法产生的合成错误类似于人类错误。接下来，我们利用这些人工错误训练GEC Llama模型，并在所有测试的语言（德语、乌克兰语和爱沙尼亚语）中取得了超过先前最先进的错误校正模型的表现，其收益在0.8至6 F0.5点之间。此外，我们证明通过微调较小的序列到序列模型和提示大型商用LMs（GPT-3.5和GPT-4）来生成错误，也会有益地影响错误生成模型的合成错误。

    arXiv:2403.05493v1 Announce Type: new  Abstract: This study explores enhancing grammatical error correction (GEC) through artificial error generation (AEG) using language models (LMs). Specifically, we fine-tune Llama 2-based LMs for error generation and find that this approach yields synthetic errors akin to human errors. Next, we train GEC Llama models with the help of these artificial errors and outperform previous state-of-the-art error correction models, with gains ranging between 0.8 and 6 F0.5 points across all tested languages (German, Ukrainian, and Estonian). Moreover, we demonstrate that generating errors by fine-tuning smaller sequence-to-sequence models and prompting large commercial LMs (GPT-3.5 and GPT-4) also results in synthetic errors beneficially affecting error generation models.
    
[^8]: FFSTC：丰贝语到法语语音翻译语料库

    FFSTC: Fongbe to French Speech Translation Corpus

    [https://arxiv.org/abs/2403.05488](https://arxiv.org/abs/2403.05488)

    该论文介绍了丰贝语到法语语音翻译语料库（FFSTC）首次推出，包括约31小时的语音内容，使用Fairseq的transformer_s和conformer模型评估了数据质量，建立了一个基准。

    

    在这篇论文中，我们首次介绍了丰贝语到法语语音翻译语料库（FFSTC）。该语料库包含约31小时的丰贝语内容，包括法语转录和相应的丰贝语音录音。FFSTC是通过各种收集方法和专注个人的努力编制而成的全面数据集。此外，我们使用Fairseq的transformer_s和conformer模型进行基准实验，以评估数据质量和有效性。我们的结果表明，transformer_s模型得分为8.96，conformer模型得分为8.14，为FFSTC语料库建立了一个基准。

    arXiv:2403.05488v1 Announce Type: new  Abstract: In this paper, we introduce the Fongbe to French Speech Translation Corpus (FFSTC) for the first time. This corpus encompasses approximately 31 hours of collected Fongbe language content, featuring both French transcriptions and corresponding Fongbe voice recordings. FFSTC represents a comprehensive dataset compiled through various collection methods and the efforts of dedicated individuals. Furthermore, we conduct baseline experiments using Fairseq's transformer_s and conformer models to evaluate data quality and validity. Our results indicate a score of 8.96 for the transformer_s model and 8.14 for the conformer model, establishing a baseline for the FFSTC corpus.
    
[^9]: GPT-4会运行《毁灭战士》吗？

    Will GPT-4 Run DOOM?

    [https://arxiv.org/abs/2403.05468](https://arxiv.org/abs/2403.05468)

    GPT-4通过自身的推理和观察能力，可以运行并玩1993年的第一人称射击游戏《毁灭战士》，并且能够执行门操作、击败敌人和规划路径，这有望拓展基于LLM的智能代理在视频游戏领域的边界。

    

    我们展示了GPT-4的推理和规划能力扩展到了1993年第一人称射击游戏《毁灭战士》。这个大型语言模型能够仅凭少数指令和来自屏幕截图的文本描述（由模型本身生成）来运行和玩游戏。我们发现GPT-4可以以及能够参与游戏：它能够操作门、与敌人作战并执行路径规划。涉及多次模型调用的更复杂提示策略提供了更好的结果。虽然需要进一步工作来让这个LLM玩得像其经典的基于强化学习的对应物一样出色，但我们注意到GPT-4不需要训练，而是依靠自身的推理和观察能力。我们希望我们的工作推动了基于智能LLM代理在视频游戏中的边界。我们最终讨论了我们工作的伦理影响。

    arXiv:2403.05468v1 Announce Type: cross  Abstract: We show that GPT-4's reasoning and planning capabilities extend to the 1993 first-person shooter Doom. This large language model (LLM) is able to run and play the game with only a few instructions, plus a textual description--generated by the model itself from screenshots--about the state of the game being observed. We find that GPT-4 can play the game to a passable degree: it is able to manipulate doors, combat enemies, and perform pathing. More complex prompting strategies involving multiple model calls provide better results. While further work is required to enable the LLM to play the game as well as its classical, reinforcement learning-based counterparts, we note that GPT-4 required no training, leaning instead on its own reasoning and observational capabilities. We hope our work pushes the boundaries on intelligent, LLM-based agents in video games. We conclude by discussing the ethical implications of our work.
    
[^10]: 使用商业语言模型优化处理低资源语言任务的成本与性能

    Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs

    [https://arxiv.org/abs/2403.05434](https://arxiv.org/abs/2403.05434)

    该论文旨在通过考虑代码混合等手段，降低在当代LLMs中处理低资源语言任务的成本，以确保预测和生成质量不受损。

    

    大型语言模型(LLMs)在高资源语言上展现出令人印象深刻的零/少轮推理和生成质量。其中有一些在低资源语言(LRLs)上训练并表现出不错的性能。由于训练LLMs的成本极高，它们通常被用作网络服务，客户根据输入和输出令牌的数量付费。令牌数量强烈依赖于脚本和语言，以及LLM的子词汇表。我们表明LRLs在定价上处于不利位置，因为众所周知，对于LRLs，知名LLMs产生的令牌比HRLs多。这是因为目前大多数流行的LLMs都针对HRL词汇表进行了优化。我们的目标是在保证预测和生成质量不受损的同时，调整平衡：降低在当代LLMs中处理LRLs的成本。作为减少LLM处理的令牌数量的手段，我们考虑代码混合

    arXiv:2403.05434v1 Announce Type: new  Abstract: Large Language Models (LLMs) exhibit impressive zero/few-shot inference and generation quality for high-resource languages(HRLs). A few of them have been trained in low-resource languages (LRLs) and give decent performance. Owing to the prohibitive costs of training LLMs, they are usually used as a network service, with the client charged by the count of input and output tokens. The number of tokens strongly depends on the script and language, as well as the LLM's sub-word vocabulary. We show that LRLs are at a pricing disadvantage, because the well-known LLMs produce more tokens for LRLs than HRLs. This is because most currently popular LLMs are optimized for HRL vocabularies. Our objective is to level the playing field: reduce the cost of processing LRLs in contemporary LLMs while ensuring that predictive and generative qualities are not compromised. As means to reduce the number of tokens processed by the LLM, we consider code-mixing,
    
[^11]: HistGen：通过本地-全局特征编码和跨模态上下文交互生成组织病理学报告

    HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction

    [https://arxiv.org/abs/2403.05396](https://arxiv.org/abs/2403.05396)

    HistGen是一个通过本地-全局特征编码和跨模态上下文交互来生成组织病理学报告的框架，提供了第一个用于评估的基准数据集。

    

    组织病理学在癌症诊断中扮演着黄金标准的角色，临床报告在解释和理解这一过程中至关重要，在指导癌症治疗和患者护理方面起着关键作用。深度学习对组织病理学报告生成的自动化将极大提升临床效率，并减轻病理学家在报告撰写方面的劳动强度和耗时负担。为追求这一进步，作者引入了HistGen，这是一个多实例学习增强的组织病理学报告生成框架，并提供第一个用于评估的基准数据集。HistGen受诊断和报告撰写工作流程的启发，具有两个精心设计的模块，旨在通过对齐整张切片图像（WSIs）和诊断报告，从本地和全局粒度提升报告生成。为实现这一目标，开发了一个本地-全局分层编码器，用于有效地从区域中聚合视觉特征。

    arXiv:2403.05396v1 Announce Type: cross  Abstract: Histopathology serves as the gold standard in cancer diagnosis, with clinical reports being vital in interpreting and understanding this process, guiding cancer treatment and patient care. The automation of histopathology report generation with deep learning stands to significantly enhance clinical efficiency and lessen the labor-intensive, time-consuming burden on pathologists in report writing. In pursuit of this advancement, we introduce HistGen, a multiple instance learning-empowered framework for histopathology report generation together with the first benchmark dataset for evaluation. Inspired by diagnostic and report-writing workflows, HistGen features two delicately designed modules, aiming to boost report generation by aligning whole slide images (WSIs) and diagnostic reports from local and global granularity. To achieve this, a local-global hierarchical encoder is developed for efficient visual feature aggregation from a regi
    
[^12]: 量化对基于Transformer的文本分类器鲁棒性的影响

    The Impact of Quantization on the Robustness of Transformer-based Text Classifiers

    [https://arxiv.org/abs/2403.05365](https://arxiv.org/abs/2403.05365)

    量化可以显著提高Transformer-based文本分类器在面对对抗攻击时的鲁棒性表现，平均提升了18.68%。

    

    Transformer-based模型在各种自然语言处理领域取得了显著的进展。然而，这些模型在面对对抗性攻击时往往表现出脆弱性。本文探讨了量化对Transformer-based模型鲁棒性的影响。量化通常涉及将高精度实数映射到较低精度的值，旨在减少所涉模型的大小。据我们所知，这项工作是首次将量化应用于NLP模型的鲁棒性。在实验中，我们评估了在文本分类中对BERT和DistilBERT模型应用量化的影响，使用了SST-2、Emotion和MR数据集。我们还评估了这些模型在面对TextFooler、PWWS和PSO对抗性攻击时的表现。我们的研究结果显示，量化显著提高了模型的对抗准确性（平均提升了18.68%）。此外，我们比较了...

    arXiv:2403.05365v1 Announce Type: new  Abstract: Transformer-based models have made remarkable advancements in various NLP areas. Nevertheless, these models often exhibit vulnerabilities when confronted with adversarial attacks. In this paper, we explore the effect of quantization on the robustness of Transformer-based models. Quantization usually involves mapping a high-precision real number to a lower-precision value, aiming at reducing the size of the model at hand. To the best of our knowledge, this work is the first application of quantization on the robustness of NLP models. In our experiments, we evaluate the impact of quantization on BERT and DistilBERT models in text classification using SST-2, Emotion, and MR datasets. We also evaluate the performance of these models against TextFooler, PWWS, and PSO adversarial attacks. Our findings show that quantization significantly improves (by an average of 18.68%) the adversarial accuracy of the models. Furthermore, we compare the effe
    
[^13]: 用归因分数解释预训练语言模型：在低资源环境中的分析

    Explaining Pre-Trained Language Models with Attribution Scores: An Analysis in Low-Resource Settings

    [https://arxiv.org/abs/2403.05338](https://arxiv.org/abs/2403.05338)

    本研究分析在低资源环境中从预备训练模型中提取的归因分数，并发现使用提示范式产生的解释比微调模型更合理，Shapley值采样表现出色。

    

    归因分数指示不同输入部分的重要性，因此可以解释模型行为。目前，基于提示的模型正变得越来越受欢迎，部分原因是它们在低资源环境中更容易适应。然而，从基于提示的模型中提取的归因分数的质量尚未得到研究。在这项工作中，我们通过分析从基于提示的模型中提取的归因分数关于可信度和忠实度，并将它们与从微调模型和大型语言模型中提取的归因分数进行比较来解决这个问题。与以前的工作相反，我们将训练规模作为分析的另一个维度引入。我们发现在低资源环境中使用提示范式（无论是基于编码器还是基于解码器的模型）比微调模型产生更合理的解释，Shapley值采样在可解释性和一致性方面均优于注意力和集成梯度。

    arXiv:2403.05338v1 Announce Type: new  Abstract: Attribution scores indicate the importance of different input parts and can, thus, explain model behaviour. Currently, prompt-based models are gaining popularity, i.a., due to their easier adaptability in low-resource settings. However, the quality of attribution scores extracted from prompt-based models has not been investigated yet. In this work, we address this topic by analyzing attribution scores extracted from prompt-based models w.r.t. plausibility and faithfulness and comparing them with attribution scores extracted from fine-tuned models and large language models. In contrast to previous work, we introduce training size as another dimension into the analysis. We find that using the prompting paradigm (with either encoder-based or decoder-based models) yields more plausible explanations than fine-tuning the models in low-resource settings and Shapley Value Sampling consistently outperforms attention and Integrated Gradients in te
    
[^14]: 连续模型编辑与批量支持的HooK层

    Consecutive Model Editing with Batch alongside HooK Layers

    [https://arxiv.org/abs/2403.05330](https://arxiv.org/abs/2403.05330)

    提出了一种内存友好的连续模型编辑与批量支持的方法COMEBA-HK，在实验中表现出优越性。

    

    由于典型的重新训练范式耗时且消耗资源，研究人员正在转向模型编辑，以寻找一种有效的、连续的、并支持批量方式直接编辑模型行为的方法。然而，尽管存在所有这些实用期望，现有的模型编辑方法却未能实现所有这些目标。此外，对于这种支持连续性模型编辑方法的内存需求往往是禁止性的，经常需要随着时间的增长逐步增加外部内存。为了应对这些挑战，我们提出了一种名为COMEBA-HK的模型编辑方法，该方法既是连续的又支持批量。COMEBA-HK对于存储几个具有更新权重的hook层仅需少量内存，是内存友好的。实验结果表明，我们的方法在单轮和连续批量编辑场景下优于其他支持批量模型编辑方法。

    arXiv:2403.05330v1 Announce Type: new  Abstract: As the typical retraining paradigm is unacceptably time- and resource-consuming, researchers are turning to model editing in order to seek an effective, consecutive, and batch-supportive way to edit the model behavior directly. Despite all these practical expectations, existing model editing methods fail to realize all of them. Furthermore, the memory demands for such succession-supportive model editing approaches tend to be prohibitive, frequently necessitating an external memory that grows incrementally over time. To cope with these challenges, we propose COMEBA-HK, a model editing method that is both consecutive and batch-supportive. COMEBA-HK is memory-friendly as it only needs a small amount of it to store several hook layers with updated weights. Experimental results demonstrate the superiority of our method over other batch-supportive model editing methods under both single-round and consecutive batch editing scenarios. Extensive 
    
[^15]: ChatASU：唤起LLM的反思，真正理解对话中的方面情绪

    ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues

    [https://arxiv.org/abs/2403.05326](https://arxiv.org/abs/2403.05326)

    本文提出了一个新的基于聊天的方面情绪理解（ChatASU）任务，旨在探索大型语言模型（LLMs）在对话场景中理解方面情绪的能力，并引入了一个子任务Aspect Chain Reasoning（ACR）任务来解决方面共指问题。

    

    在互动场景（例如，问答和对话）中进行方面情绪理解（ASU）近年来引起了越来越多的关注并取得了重要进展。然而，现有研究大多忽略了意见目标（即方面）的共指问题，而这种现象在互动场景特别是对话中普遍存在，限制了ASU的性能。最近，大型语言模型（LLM）展示了将各种NLP任务与聊天范式相结合的强大能力。基于此，本文提出了一项新的基于聊天的方面情绪理解（ChatASU）任务，旨在探索LLMs在对话场景中理解方面情绪的能力。特别是，这项ChatASU任务引入了一个子任务，即方面链推理（ACR）任务，以解决方面共指问题。在此基础上，我们提出了一种可信的自反思方法（TSA）与ChatGLM作为背景。

    arXiv:2403.05326v1 Announce Type: cross  Abstract: Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., Question-Answering and Dialogue) has attracted ever-more interest in recent years and achieved important progresses. However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance. Recently, large language models (LLMs) shows the powerful ability to integrate various NLP tasks with the chat paradigm. In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in understanding aspect sentiments in dialogue scenarios. Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to address the aspect coreference issue. On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as back
    
[^16]: RAT：检索增强思维在长视角生成中引发了上下文感知推理

    RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation

    [https://arxiv.org/abs/2403.05313](https://arxiv.org/abs/2403.05313)

    RAT方法通过检索增强思维，在长视角生成中改善大型语言模型的推理和生成能力，显著降低了幻觉，并取得了显著的性能提升

    

    我们探讨了如何通过信息检索迭代修订一系列思维，显著改善大型语言模型在长视角生成任务中的推理和生成能力，同时极大减轻了幻觉。具体来说，所提出的方法——*检索增强思维* (RAT)——在生成初始的零射 CoT 后，逐步修订每个思维步骤，与任务查询、当前和过去的思维步骤相关的检索信息。将 RAT 应用于 GPT-3.5、GPT-4 和 CodeLLaMA-7b，在各种长视角生成任务上显著提高它们的性能；平均而言，代码生成评分增加了 13.63%，数学推理增加了 16.96%，创意写作增加了 19.2%，具象任务规划增加了 42.78%。演示页面链接：https://craftjarvis.github.io/RAT

    arXiv:2403.05313v1 Announce Type: cross  Abstract: We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT
    
[^17]: ACLSum：一种新的用于科学出版物基于方面的摘要的数据集

    ACLSum: A New Dataset for Aspect-based Summarization of Scientific Publications

    [https://arxiv.org/abs/2403.05303](https://arxiv.org/abs/2403.05303)

    ACLSum是一个由领域专家精心制作和评估的新型摘要数据集，支持对科学论文进行多方面摘要，深入涵盖挑战、方法和结果。

    

    过去，人们已经付出了大量努力来开发摘要数据集。然而，大部分资源是(半)自动生成的，通常是通过网络数据爬取得到的，导致了用于训练和评估摘要系统的资源质量不佳，这可能是由于生成基准摘要的高昂成本，尤其是针对多种语言和专业领域。为解决这一问题，我们提出了ACLSum，这是一个由领域专家精心制作和评估的新型摘要数据集。与以往的数据集不同，ACLSum支持对科学论文进行多方面摘要，深入涵盖挑战、方法和结果。通过大量实验，我们评估了我们的资源质量以及基于预训练语言模型和最先进的大型语言模型(LLM)的模型性能。

    arXiv:2403.05303v1 Announce Type: new  Abstract: Extensive efforts in the past have been directed toward the development of summarization datasets. However, a predominant number of these resources have been (semi)-automatically generated, typically through web data crawling, resulting in subpar resources for training and evaluating summarization systems, a quality compromise that is arguably due to the substantial costs associated with generating ground-truth summaries, particularly for diverse languages and specialized domains. To address this issue, we present ACLSum, a novel summarization dataset carefully crafted and evaluated by domain experts. In contrast to previous datasets, ACLSum facilitates multi-aspect summarization of scientific papers, covering challenges, approaches, and outcomes in depth. Through extensive experiments, we evaluate the quality of our resource and the performance of models based on pretrained language models and state-of-the-art large language models (LLM
    
[^18]: PEEB：具有可解释和可编辑语言瓶颈的基于部分的图像分类器

    PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck

    [https://arxiv.org/abs/2403.05297](https://arxiv.org/abs/2403.05297)

    PEEB是一种基于部分的图像分类器，通过将类别名称转换为描述视觉部分的文本描述符，并将检测到的部分的嵌入与文本描述符匹配，从而在零样本设置中表现出色，并且不仅在监督学习中表现出色，而且还首次实现用户编辑类定义形成新分类器无需重新训练。

    

    基于CLIP的分类器依赖于包含{text encoder已知的类名称}的提示。也就是说，CLIP在新类别或其名称很少在互联网上出现的类别（例如鸟类的学名）上表现不佳。针对细粒度分类，我们提出了PEEB - 一种可解释和可编辑的分类器，用于（1）将类别名称表达为一组预定义的描述视觉部分的文本描述符；和（2）将检测到的部分的嵌入与每个类别中的文本描述符进行匹配，以计算用于分类的逻辑分数。在一个零样本设置中，其中类别名称是未知的，PEEB在准确性上大幅优于CLIP（约为10倍）。与基于部分的分类器相比，PEEB不仅在监督学习设置上是最先进的（88.80%准确率），而且还是第一个能够让用户编辑类定义以形成新的分类器而无需重新训练的分类器。

    arXiv:2403.05297v1 Announce Type: cross  Abstract: CLIP-based classifiers rely on the prompt containing a {class name} that is known to the text encoder. That is, CLIP performs poorly on new classes or the classes whose names rarely appear on the Internet (e.g., scientific names of birds). For fine-grained classification, we propose PEEB - an explainable and editable classifier to (1) express the class name into a set of pre-defined text descriptors that describe the visual parts of that class; and (2) match the embeddings of the detected parts to their textual descriptors in each class to compute a logit score for classification. In a zero-shot setting where the class names are unknown, PEEB outperforms CLIP by a large margin (~10x in accuracy). Compared to part-based classifiers, PEEB is not only the state-of-the-art on the supervised-learning setting (88.80% accuracy) but also the first to enable users to edit the class definitions to form a new classifier without retraining. Compar
    
[^19]: LLM4Decompile：使用大型语言模型对二进制代码进行反编译

    LLM4Decompile: Decompiling Binary Code with Large Language Models

    [https://arxiv.org/abs/2403.05286](https://arxiv.org/abs/2403.05286)

    发布首批开放访问的反编译LLM，预训练在40亿个C源代码和汇编代码标记上，引入了第一个考虑重新编译性和重新执行性的反编译数据集。

    

    反编译旨在将编译代码恢复为可读性强的源代码，但在名称和结构等细节方面存在困难。大型语言模型（LLMs）在编程任务中显示出潜力，激发了它们在反编译中的应用。然而，目前尚无用于反编译的开源LLM。此外，现有的反编译评估系统主要考虑标记级准确性，而很大程度上忽略了代码的可执行性，这是任何程序最重要的特征。因此，我们发布了首批开放访问的反编译LLM，范围从10亿到330亿，预先训练了40亿个令牌的C源代码和相应的汇编代码。这些开源LLM可以作为该领域进一步发展的基线。为了确保实际程序评估，我们引入了Decompile-Eval，这是第一个考虑重新编译性和重新执行性的反编译数据集。该基准强调了评估的重要性。

    arXiv:2403.05286v1 Announce Type: cross  Abstract: Decompilation aims to restore compiled code to human-readable source code, but struggles with details like names and structure. Large language models (LLMs) show promise for programming tasks, motivating their application to decompilation. However, there does not exist any open-source LLM for decompilation. Moreover, existing decompilation evaluation systems mainly consider token-level accuracy and largely ignore code executability, which is the most important feature of any program. Therefore, we release the first open-access decompilation LLMs ranging from 1B to 33B pre-trained on 4 billion tokens of C source code and the corresponding assembly code. The open-source LLMs can serve as baselines for further development in the field. To ensure practical program evaluation, we introduce Decompile-Eval, the first dataset that considers re-compilability and re-executability for decompilation. The benchmark emphasizes the importance of eval
    
[^20]: ERBench：基于实体关系的可自动验证的大规模语言模型幻觉基准

    ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models

    [https://arxiv.org/abs/2403.05266](https://arxiv.org/abs/2403.05266)

    ERBench是一个基于实体关系的大型语言模型幻觉基准，通过自动转换任何关系数据库并构建可自动验证的问题，以支持复杂性评估和调试

    

    大型语言模型（LLMs）在各种应用中取得了前所未有的性能，然而它们的评估仍然是一个关键问题。现有的幻觉基准要么是静态的，要么缺乏可调整的复杂性进行彻底分析。我们认为利用现有的关系数据库是构建基准的一种有希望的方法，因为它们通过功能依赖关系可以准确描述知识。我们提出了ERBench，可以自动将任何关系数据库转换为基于实体关系（ER）模型的基准。我们的关键想法是使用数据库模式、记录和功能依赖来构建问题，以便可以自动验证。此外，我们使用外键约束来连接关系和构建多跳问题，这些问题可以任意复杂，用于调试LLMs的中间答案。最后，ERBench支持持续评估，多模态qu

    arXiv:2403.05266v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved unprecedented performance in various applications, yet their evaluation remains a critical issue. Existing hallucination benchmarks are either static or lack adjustable complexity for thorough analysis. We contend that utilizing existing relational databases is a promising approach for constructing benchmarks due to their accurate knowledge description via functional dependencies. We propose ERBench to automatically convert any relational database into a benchmark based on the entity-relationship (ER) model. Our key idea is to construct questions using the database schema, records, and functional dependencies such that they can be automatically verified. In addition, we use foreign key constraints to join relations and construct multihop questions, which can be arbitrarily complex and used to debug the intermediate answers of LLMs. Finally, ERBench supports continuous evaluation, multimodal qu
    
[^21]: 跨语言转移还是机器翻译？关于单语义文本相似性的数据增强

    Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity

    [https://arxiv.org/abs/2403.05257](https://arxiv.org/abs/2403.05257)

    该研究对比了跨语言转移和机器翻译两种数据增强技术在单语义文本相似性中的表现，并发现维基百科领域优于NLI领域。

    

    学习更好的句子嵌入将提高自然语言理解任务的性能，包括语义文本相似性（STS）和自然语言推理（NLI）。在本研究中，我们直接比较了作为单语STS潜在解决方案的两种数据增强技术：（a）利用仅英语资源作为训练数据以生成非英语句子嵌入作为零短推理的跨语言转移，以及（b）将英语数据提前转换为伪非英语训练数据的机器翻译。在我们在日语和韩语的单语STS实验中，我们发现这两种数据技术表现相当。相反，我们发现维基百科领域优于NLI领域

    arXiv:2403.05257v1 Announce Type: new  Abstract: Learning better sentence embeddings leads to improved performance for natural language understanding tasks including semantic textual similarity (STS) and natural language inference (NLI). As prior studies leverage large-scale labeled NLI datasets for fine-tuning masked language models to yield sentence embeddings, task performance for languages other than English is often left behind. In this study, we directly compared two data augmentation techniques as potential solutions for monolingual STS: (a) cross-lingual transfer that exploits English resources alone as training data to yield non-English sentence embeddings as zero-shot inference, and (b) machine translation that coverts English data into pseudo non-English training data in advance. In our experiments on monolingual STS in Japanese and Korean, we find that the two data techniques yield performance on par. Rather, we find a superiority of the Wikipedia domain over the NLI domain
    
[^22]: 利用大型语言模型的多角色能力进行开放领域问答

    Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering

    [https://arxiv.org/abs/2403.05217](https://arxiv.org/abs/2403.05217)

    提出了LLMQA框架，利用大型语言模型在开放领域问答中扮演生成器、重新排序器和评估器等多重角色，结合了检索和生成证据的优势。

    

    开放领域问答（ODQA）已经成为信息系统中的一个关键研究焦点。现有方法主要遵循两种范式来收集证据：（1）\textit{检索-然后阅读}范式从外部语料库中检索相关文档；和（2）\textit{生成-然后阅读}范式使用大型语言模型（LLMs）生成相关文档。然而，这两种方法都不能完全满足证据的多方面要求。因此，我们提出了LLMQA，一个通用框架，将ODQA过程分为三个基本步骤：查询扩展，文档选择和答案生成，结合了检索和生成证据的优势。由于LLMs展示了其出色的能力来完成各种任务，我们指导LLMs在我们的框架内扮演生成器、重新排序器和评估器等多种角色，使它们融合在ODQA过程中协作。

    arXiv:2403.05217v1 Announce Type: cross  Abstract: Open-domain question answering (ODQA) has emerged as a pivotal research spotlight in information systems. Existing methods follow two main paradigms to collect evidence: (1) The \textit{retrieve-then-read} paradigm retrieves pertinent documents from an external corpus; and (2) the \textit{generate-then-read} paradigm employs large language models (LLMs) to generate relevant documents. However, neither can fully address multifaceted requirements for evidence. To this end, we propose LLMQA, a generalized framework that formulates the ODQA process into three basic steps: query expansion, document selection, and answer generation, combining the superiority of both retrieval-based and generation-based evidence. Since LLMs exhibit their excellent capabilities to accomplish various tasks, we instruct LLMs to play multiple roles as generators, rerankers, and evaluators within our framework, integrating them to collaborate in the ODQA process. 
    
[^23]: SocialPET: 社交信息模式利用训练用于社交媒体中的少样本立场检测

    SocialPET: Socially Informed Pattern Exploiting Training for Few-Shot Stance Detection in Social Media

    [https://arxiv.org/abs/2403.05216](https://arxiv.org/abs/2403.05216)

    SocialPET 是一种社交信息引导的方法，通过结合语言模型和社交网络结构来进行立场检测，在少样本情况下优于其他竞争模型。

    

    立场检测是指确定社交媒体帖子对于特定目标的观点是否“赞成”或“反对”的任务，在具有限制的标记数据的挑战性但现实情景中一直研究不足。本研究通过引入SocialPET，提出了一种社交信息引导的方法来利用语言模型进行这一任务的研究。我们提出的方法建立在Pattern Exploiting Training (PET) 技术之上，该技术通过使用语言模型将分类任务处理为填空问题。为了增强该方法的社交意识，我们利用社交媒体帖子周围的社交网络结构。我们证明了SocialPET在两个立场数据集Multi-target和P-Stance上的有效性，优于竞争性立场检测模型以及基准模型PET，即针对研究对象标记实例仅为100左右情况下。

    arXiv:2403.05216v1 Announce Type: new  Abstract: Stance detection, as the task of determining the viewpoint of a social media post towards a target as 'favor' or 'against', has been understudied in the challenging yet realistic scenario where there is limited labeled data for a certain target. Our work advances research in few-shot stance detection by introducing SocialPET, a socially informed approach to leveraging language models for the task. Our proposed approach builds on the Pattern Exploiting Training (PET) technique, which addresses classification tasks as cloze questions through the use of language models. To enhance the approach with social awareness, we exploit the social network structure surrounding social media posts. We prove the effectiveness of SocialPET on two stance datasets, Multi-target and P-Stance, outperforming competitive stance detection models as well as the base model, PET, where the labeled instances for the target under study is as few as 100. When we delv
    
[^24]: 追踪多语言语言模型中事实的根源：独立的、共享的和转移的知识

    Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge

    [https://arxiv.org/abs/2403.05189](https://arxiv.org/abs/2403.05189)

    本研究追踪了多语言语言模型中事实的来源，发现了三种模式：语言独立、跨语言共享和转移，为区分它们提出了方法，凸显了在多语言LMs中保持一致事实知识的挑战，强调了需要在ML-LMs中改进事实表示学习。

    

    获取低资源语言模型（LMs）中的事实知识是一个严峻的挑战，因此需要在多语言LMs（ML-LMs）中进行跨语言转移。本研究探讨了ML-LMs如何获取和表示事实知识。我们首先使用多语言事实知识探测数据集mLAMA对ML-LMs（特别是多语言BERT）进行神经元调查。然后我们追溯事实的根源（维基百科），以确定ML-LMs获取特定事实的方式。最后，我们确定了ML-LMs获取和表示事实的三种模式：语言独立、跨语言共享和转移，并制定了区分它们的方法。我们的发现突显了跨语言保持一致的事实知识的挑战，强调了在ML-LMs中进行更好的事实表示学习的必要性。

    arXiv:2403.05189v1 Announce Type: cross  Abstract: Acquiring factual knowledge for language models (LMs) in low-resource languages poses a serious challenge, thus resorting to cross-lingual transfer in multilingual LMs (ML-LMs). In this study, we ask how ML-LMs acquire and represent factual knowledge. Using the multilingual factual knowledge probing dataset, mLAMA, we first conducted a neuron investigation of ML-LMs (specifically, multilingual BERT). We then traced the roots of facts back to the knowledge source (Wikipedia) to identify the ways in which ML-LMs acquire specific facts. We finally identified three patterns of acquiring and representing facts in ML-LMs: language-independent, cross-lingual shared and transferred, and devised methods for differentiating them. Our findings highlight the challenge of maintaining consistent factual knowledge across languages, underscoring the need for better fact representation learning in ML-LMs.
    
[^25]: CommitBench：用于提交信息生成的基准测试

    CommitBench: A Benchmark for Commit Message Generation

    [https://arxiv.org/abs/2403.05188](https://arxiv.org/abs/2403.05188)

    提出了一个名为CommitBench的新大规模数据集，采用最佳实践进行数据集创建，解决了现有数据集存在的问题，以改善生成的提交信息的质量。

    

    写提交信息对许多软件开发人员来说是一项乏味的日常任务，经常被忽视。自动化此任务有潜力节省时间，同时确保信息具有信息量。高质量的数据集和客观的基准测试是朝着这一目标进行坚实研究和评估的重要先决条件。我们发现现有数据集存在各种问题，例如提交选择的质量、样本量小、重复、隐私问题以及没有授权进行再分发。这可能导致无法使用的模型和偏倚的评估，次优模型由于数据中的偏见而获得更高的评估分数。我们编制了一个新的大规模数据集CommitBench，采用了数据集创建的最佳实践。我们从许可允许再分发的各种项目中抽样提交，并应用我们的过滤和数据集增强措施以提高生成的提交信息的质量。

    arXiv:2403.05188v1 Announce Type: new  Abstract: Writing commit messages is a tedious daily task for many software developers, and often remains neglected. Automating this task has the potential to save time while ensuring that messages are informative. A high-quality dataset and an objective benchmark are vital preconditions for solid research and evaluation towards this goal. We show that existing datasets exhibit various problems, such as the quality of the commit selection, small sample sizes, duplicates, privacy issues, and missing licenses for redistribution. This can lead to unusable models and skewed evaluations, where inferior models achieve higher evaluation scores due to biases in the data. We compile a new large-scale dataset, CommitBench, adopting best practices for dataset creation. We sample commits from diverse projects with licenses that permit redistribution and apply our filtering and dataset enhancements to improve the quality of generated commit messages. We use Co
    
[^26]: ROUGE-K：您的摘要包含关键词吗？

    ROUGE-K: Do Your Summaries Have Keywords?

    [https://arxiv.org/abs/2403.05186](https://arxiv.org/abs/2403.05186)

    ROUGE-K是一种关键词导向的评估指标，通过量化回答摘要是否包含关键词这一问题，研究发现当前强基线模型在摘要中经常遗漏关键信息，人类注释员觉得包含更多关键词的摘要更相关。

    

    关键词，在摘要中指的是与内容相关的关键词，在有效传达信息方面起着重要作用，因此在评估中检查系统生成的摘要是否包含这些信息性关键词至关重要。然而，现有极端摘要模型的评估指标并未明确关注摘要中的关键词，使开发人员对其是否存在毫无所知。为解决这一问题，我们提出了一种关键词导向的评估指标，称为ROUGE-K，它提供了一个定量答案来回答关于“摘要中关键词包含得如何”的问题。通过这一关键词感知度指标的视角，我们惊讶地发现目前的强基线模型经常在摘要中漏掉关键信息。我们的分析揭示，人类注释员确实发现包含更多关键词的摘要与源文件更相关。这是评估中一个重要但之前被忽视的方面。

    arXiv:2403.05186v1 Announce Type: new  Abstract: Keywords, that is, content-relevant words in summaries play an important role in efficient information conveyance, making it critical to assess if system-generated summaries contain such informative words during evaluation. However, existing evaluation metrics for extreme summarization models do not pay explicit attention to keywords in summaries, leaving developers ignorant of their presence. To address this issue, we present a keyword-oriented evaluation metric, dubbed ROUGE-K, which provides a quantitative answer to the question of -- \textit{How well do summaries include keywords?} Through the lens of this keyword-aware metric, we surprisingly find that a current strong baseline model often misses essential information in their summaries. Our analysis reveals that human annotators indeed find the summaries with more keywords to be more relevant to the source documents. This is an important yet previously overlooked aspect in evaluati
    
[^27]: 朝向机器心理学：大型语言模型预测人类记忆

    Towards a Psychology of Machines: Large Language Models Predict Human Memory

    [https://arxiv.org/abs/2403.05152](https://arxiv.org/abs/2403.05152)

    这项研究探索了大型语言模型在预测基于语言的记忆任务中的表现，并通过其对模棱两可句子的处理能力增进了对人类认知机制的理解。

    

    大型语言模型（LLMs）在各种任务中展示出了非凡的能力，尽管缺乏人类认知基础。这引发了一个问题：除了简单模仿人类语言模式，这些模型能否提供关于人类认知机制的洞见？本研究探讨了ChatGPT在预测基于语言的记忆任务中人类表现的能力。基于文本理解理论，我们假设识别模棱两可的句子（例如，“因为比尔喝酒，所以酒从未留在房子里”）在前面提供与上下文相关信息的情况下会得到促进。参与者，无论是人类还是ChatGPT，都被呈现成对的句子。第二个句子总是一个旨在固有地模棱两可的花园路径句，而第一个句子则提供了合适的（例如，“比尔患有慢性酒精中毒”）或不合适的上下文（例如，“比尔喜欢打高尔夫”）。

    arXiv:2403.05152v1 Announce Type: cross  Abstract: Large language models (LLMs) are demonstrating remarkable capabilities across various tasks despite lacking a foundation in human cognition. This raises the question: can these models, beyond simply mimicking human language patterns, offer insights into the mechanisms underlying human cognition? This study explores the ability of ChatGPT to predict human performance in a language-based memory task. Building upon theories of text comprehension, we hypothesize that recognizing ambiguous sentences (e.g., "Because Bill drinks wine is never kept in the house") is facilitated by preceding them with contextually relevant information. Participants, both human and ChatGPT, were presented with pairs of sentences. The second sentence was always a garden-path sentence designed to be inherently ambiguous, while the first sentence either provided a fitting (e.g., "Bill has chronic alcoholism") or an unfitting context (e.g., "Bill likes to play golf"
    
[^28]: ChatUIE：利用大型语言模型探索基于聊天的统一信息提取

    ChatUIE: Exploring Chat-based Unified Information Extraction using Large Language Models

    [https://arxiv.org/abs/2403.05132](https://arxiv.org/abs/2403.05132)

    ChatUIE利用大型语言模型探索基于聊天的统一信息提取，通过强化学习和生成约束提高对自然语言中结构化信息的提取能力。

    

    最近大型语言模型的发展在一般的聊天中展现出令人印象深刻的性能。然而，它们在特定领域的能力，特别是在信息提取方面，存在一定的局限性。从偏离已知模式或指令的自然语言中提取结构化信息对于之前基于提示的方法来说是具有挑战性的。这促使我们探索聊天型语言模型中的领域特定建模作为从自然语言中提取结构化信息的解决方案。在本文中，我们提出了ChatUIE，这是一个基于ChatGLM构建的创新的统一信息提取框架。同时，采用强化学习来改进和对齐涉及混乱和有限样本的各种任务。此外，我们整合了生成约束来解决在输入中不存在的元素生成的问题。我们的实验结果表明ChatUIE能够...

    arXiv:2403.05132v1 Announce Type: cross  Abstract: Recent advancements in large language models have shown impressive performance in general chat. However, their domain-specific capabilities, particularly in information extraction, have certain limitations. Extracting structured information from natural language that deviates from known schemas or instructions has proven challenging for previous prompt-based methods. This motivated us to explore domain-specific modeling in chat-based language models as a solution for extracting structured information from natural language. In this paper, we present ChatUIE, an innovative unified information extraction framework built upon ChatGLM. Simultaneously, reinforcement learning is employed to improve and align various tasks that involve confusing and limited samples. Furthermore, we integrate generation constraints to address the issue of generating elements that are not present in the input. Our experimental results demonstrate that ChatUIE ca
    
[^29]: 基于规则的新闻标题生成

    Rule-driven News Captioning

    [https://arxiv.org/abs/2403.05101](https://arxiv.org/abs/2403.05101)

    本文提出了一种基于规则的新闻标题生成方法，通过新闻感知的语义规则，可以生成遵循新闻报道基本规则的图像描述。

    

    News captioning任务旨在通过描述图片及其新闻文章中的命名实体或具体事件来生成句子。现有方法通过依赖大规模预训练模型已取得显著成果，这些模型主要专注于输入新闻内容与输出预测之间的相关性。然而，新闻标题生成需要遵循新闻报道的一些基本规则，如准确描述与事件相关的个体和动作。在本文中，我们提出了基于规则的新闻标题生成方法，可以根据指定的规则信号生成图像描述。具体而言，我们首先为描述设计了新闻感知的语义规则。这一规则包括图片中描绘的主要动作（例如，“执行”）以及参与动作的命名实体扮演的角色（例如，“代理人”和“地点”）。其次，我们将这个语义规则注入到文本生成模型中。

    arXiv:2403.05101v1 Announce Type: cross  Abstract: News captioning task aims to generate sentences by describing named entities or concrete events for an image with its news article. Existing methods have achieved remarkable results by relying on the large-scale pre-trained models, which primarily focus on the correlations between the input news content and the output predictions. However, the news captioning requires adhering to some fundamental rules of news reporting, such as accurately describing the individuals and actions associated with the event. In this paper, we propose the rule-driven news captioning method, which can generate image descriptions following designated rule signal. Specifically, we first design the news-aware semantic rule for the descriptions. This rule incorporates the primary action depicted in the image (e.g., "performing") and the roles played by named entities involved in the action (e.g., "Agent" and "Place"). Second, we inject this semantic rule into th
    
[^30]: 使用大型语言模型在RST语篇解析中能否取得显著成功？

    Can we obtain significant success in RST discourse parsing by using Large Language Models?

    [https://arxiv.org/abs/2403.05065](https://arxiv.org/abs/2403.05065)

    研究探索了如何利用大型语言模型（LLMs）来进行RST语篇解析，并在底部策略中取得了最新的最先进结果。

    

    最近，只有解码器的预训练大型语言模型（LLMs）对各种自然语言处理（NLP）任务产生了显著影响。虽然已经证明仅编码器或编码器-解码器预训练语言模型在语篇解析中是有效的，但LLMs能够执行这项任务的程度仍然是一个开放的研究问题。因此，本文探讨了LLMs对修辞结构理论（RST）语篇解析的益处。在这里，基本自顶向下和自底向上策略的解析过程被转换为LLMs可以使用的提示。我们使用Llama 2，并用QLoRA进行微调，后者具有可以调节的更少参数。对RST-DT、Instr-DT和GUM语料库的三个基准数据集的实验结果表明，底部策略中Llama 2的70亿参数获得了最新成果。

    arXiv:2403.05065v1 Announce Type: new  Abstract: Recently, decoder-only pre-trained large language models (LLMs), with several tens of billion parameters, have significantly impacted a wide range of natural language processing (NLP) tasks. While encoder-only or encoder-decoder pre-trained language models have already proved to be effective in discourse parsing, the extent to which LLMs can perform this task remains an open research question. Therefore, this paper explores how beneficial such LLMs are for Rhetorical Structure Theory (RST) discourse parsing. Here, the parsing process for both fundamental top-down and bottom-up strategies is converted into prompts, which LLMs can work with. We employ Llama 2 and fine-tune it with QLoRA, which has fewer parameters that can be tuned. Experimental results on three benchmark datasets, RST-DT, Instr-DT, and the GUM corpus, demonstrate that Llama 2 with 70 billion parameters in the bottom-up strategy obtained state-of-the-art (SOTA) results wit
    
[^31]: 人类对话是否特殊？一个大型语言模型的视角

    Are Human Conversations Special? A Large Language Model Perspective

    [https://arxiv.org/abs/2403.05045](https://arxiv.org/abs/2403.05045)

    本研究分析了大型语言模型在理解人类对话时的注意机制变化，发现尽管语言模型在特定领域表现出不同的注意行为，但在专门处理人类对话方面存在明显差距，需要通过多样化的高质量对话数据训练模型来增强理解和生成

    

    本研究分析了大型语言模型（LLMs）在理解人类之间的自然对话（人-人）时注意机制的变化。我们分析了LLMs的三种用例：与网络内容、代码和数学文本的互动。通过分析跨这些领域的注意距离、分散性和相互依赖性，我们强调了对话数据所提出的独特挑战。值得注意的是，对话需要细致处理长期上下文关系，并通过它们的注意模式展示出更高的复杂性。我们的研究结果表明，虽然语言模型表现出特定于领域的注意行为，但它们在专门化人类对话方面存在显著差距。通过详细的注意熵分析和t-SNE可视化，我们展示了需要通过训练模型使用多样化的高质量对话数据来增强理解和生成。

    arXiv:2403.05045v1 Announce Type: cross  Abstract: This study analyzes changes in the attention mechanisms of large language models (LLMs) when used to understand natural conversations between humans (human-human). We analyze three use cases of LLMs: interactions over web content, code, and mathematical texts. By analyzing attention distance, dispersion, and interdependency across these domains, we highlight the unique challenges posed by conversational data. Notably, conversations require nuanced handling of long-term contextual relationships and exhibit higher complexity through their attention patterns. Our findings reveal that while language models exhibit domain-specific attention behaviors, there is a significant gap in their ability to specialize in human conversations. Through detailed attention entropy analysis and t-SNE visualizations, we demonstrate the need for models trained with a diverse array of high-quality conversational data to enhance understanding and generation of
    
[^32]: 通过偏差净化实现多模态情感分析的研究

    Towards Multimodal Sentiment Analysis Debiasing via Bias Purification

    [https://arxiv.org/abs/2403.05023](https://arxiv.org/abs/2403.05023)

    提出了一种基于因果关系的多模态对事实推理情感分析框架，用于净化和缓解数据集的偏见，从而提高多模态情感分析的性能。

    

    多模态情感分析（MSA）旨在通过整合来自不同模态（如视觉、语言和音频）的与情感相关线索来理解人类意图。然而，当前MSA任务普遍受到未经计划的数据集偏见的影响，尤其是多模态话语级标签偏见和单词级上下文偏见。这些有害的偏见可能会误导模型专注于统计捷径和错误相关性，导致严重的性能瓶颈。为了缓解这些问题，我们提出了一种基于因果关系而非传统似然性的多模态对事实推理情感（MCIS）分析框架。具体而言，我们首先制定一个因果图来发现已训练的基准模型中的有害偏见。在推理阶段，给定一个事实多模态输入，MCIS想象两种对事实情形，以净化和缓解这些偏见。然后，MCIS可以从偏差中做出不带偏见的决策。

    arXiv:2403.05023v1 Announce Type: new  Abstract: Multimodal Sentiment Analysis (MSA) aims to understand human intentions by integrating emotion-related clues from diverse modalities, such as visual, language, and audio. Unfortunately, the current MSA task invariably suffers from unplanned dataset biases, particularly multimodal utterance-level label bias and word-level context bias. These harmful biases potentially mislead models to focus on statistical shortcuts and spurious correlations, causing severe performance bottlenecks. To alleviate these issues, we present a Multimodal Counterfactual Inference Sentiment (MCIS) analysis framework based on causality rather than conventional likelihood. Concretely, we first formulate a causal graph to discover harmful biases from already-trained vanilla models. In the inference phase, given a factual multimodal input, MCIS imagines two counterfactual scenarios to purify and mitigate these biases. Then, MCIS can make unbiased decisions from biase
    
[^33]: 模拟社交互动成功性的误导性：以LLMs为例

    Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs

    [https://arxiv.org/abs/2403.05020](https://arxiv.org/abs/2403.05020)

    研究发现，使用LLMs进行社交互动的全知模拟比非全知模拟更容易实现社交目标，尽管非全知模拟更接近实际情况。

    

    最近大型语言模型（LLM）的进展使得社交模拟更加丰富，能够使用基于LLM的代理人研究各种社交现象。然而，大多数工作在这些模拟中采用了一种全知的透视（例如，单个LLM生成所有交谈者），这与人类具有的非全知、信息不对称的互动根本不符。为了研究这些差异，我们开发了一个评估框架，在各种设定（全知、非全知）中使用LLMs模拟社交互动。我们的实验表明，通过全知方式模拟的交谈者在实现社交目标方面比非全知代理人更成功，尽管后者更符合现实设置。此外，我们表明从全知模拟中学习可以改善交互的自然性，但在合作场景中几乎不能增强目标实现。

    arXiv:2403.05020v1 Announce Type: cross  Abstract: Recent advances in large language models (LLM) have enabled richer social simulations, allowing for the study of various social phenomena with LLM-based agents. However, most work has used an omniscient perspective on these simulations (e.g., single LLM to generate all interlocutors), which is fundamentally at odds with the non-omniscient, information asymmetric interactions that humans have. To examine these differences, we develop an evaluation framework to simulate social interactions with LLMs in various settings (omniscient, non-omniscient). Our experiments show that interlocutors simulated omnisciently are much more successful at accomplishing social goals compared to non-omniscient agents, despite the latter being the more realistic setting. Furthermore, we demonstrate that learning from omniscient simulations improves the apparent naturalness of interactions but scarcely enhances goal achievement in cooperative scenarios. Our f
    
[^34]: 无法记住长文档中的细节？您需要一些R&R

    Can't Remember Details in Long Documents? You Need Some R&R

    [https://arxiv.org/abs/2403.05004](https://arxiv.org/abs/2403.05004)

    引入R&R方法，结合reprompting和in-context retrieval两种新型提示方式，提高了在长文档上的问答任务的准确性。

    

    长上下文大型语言模型（LLMs）在诸如长篇文档上的问答（QA）等任务中表现出潜力，但它们往往会错过上下文文档中间的重要信息。在这里，我们介绍了一个名为$\textit{R&R}$的方法，它结合了两种新型基于提示的方法，称为$\textit{reprompting}$和$\textit{in-context retrieval}$（ICR），以减轻文档型QA中的这种影响。在$\textit{reprompting}$中，我们周期性地在整个上下文文档中重复提示说明，以提醒LLM其原始任务。在ICR中，我们并不指示LLM直接回答问题，而是指示它检索与给定问题最相关的前$k$个段落编号，然后将其用作第二个QA提示中的缩略上下文。我们使用GPT-4 Turbo和Claude-2.1在长度达到80k标记的文档上测试了R&R，并平均观察到QA准确率提升了16个百分点。

    arXiv:2403.05004v1 Announce Type: cross  Abstract: Long-context large language models (LLMs) hold promise for tasks such as question-answering (QA) over long documents, but they tend to miss important information in the middle of context documents (arXiv:2307.03172v3). Here, we introduce $\textit{R&R}$ -- a combination of two novel prompt-based methods called $\textit{reprompting}$ and $\textit{in-context retrieval}$ (ICR) -- to alleviate this effect in document-based QA. In reprompting, we repeat the prompt instructions periodically throughout the context document to remind the LLM of its original task. In ICR, rather than instructing the LLM to answer the question directly, we instruct it to retrieve the top $k$ passage numbers most relevant to the given question, which are then used as an abbreviated context in a second QA prompt. We test R&R with GPT-4 Turbo and Claude-2.1 on documents up to 80k tokens in length and observe a 16-point boost in QA accuracy on average. Our further an
    
[^35]: DiffChat: 学习使用文本到图像合成模型进行互动图像创作的聊天

    DiffChat: Learning to Chat with Text-to-Image Synthesis Models for Interactive Image Creation

    [https://arxiv.org/abs/2403.04997](https://arxiv.org/abs/2403.04997)

    DiffChat 是一种新方法，通过对齐大型语言模型与文本到图像合成模型，实现了在互动图像创作中进行聊天并生成高质量图像的目标。

    

    我们提出了DiffChat，一种新颖的方法，通过使大型语言模型（LLMs）与以提示为输入的文本到图像合成（TIS）模型（例如稳定扩散）对齐，来"聊天"以进行互动图像创作。给定原始提示/图像和用户指定的指令，DiffChat 可以有效地进行适当修改并生成目标提示，这可用于创建高质量的目标图像。为实现此目标，我们首先收集了一个名为InstructPE的指令遵循提示工程数据集，用于 DiffChat 的监督训练。接下来，我们提出了一个基于强化学习的框架，利用三个核心标准（美学、用户喜好和内容完整性）的反馈来进行图像创作。该框架涉及一个动态修改技术来获得更相关的正样本和更难的负样本，以实现脱机采样。内容完整性还被引入到值估计中。

    arXiv:2403.04997v1 Announce Type: new  Abstract: We present DiffChat, a novel method to align Large Language Models (LLMs) to "chat" with prompt-as-input Text-to-Image Synthesis (TIS) models (e.g., Stable Diffusion) for interactive image creation. Given a raw prompt/image and a user-specified instruction, DiffChat can effectively make appropriate modifications and generate the target prompt, which can be leveraged to create the target image of high quality. To achieve this, we first collect an instruction-following prompt engineering dataset named InstructPE for the supervised training of DiffChat. Next, we propose a reinforcement learning framework with the feedback of three core criteria for image creation, i.e., aesthetics, user preference, and content integrity. It involves an action-space dynamic modification technique to obtain more relevant positive samples and harder negative samples during the off-policy sampling. Content integrity is also introduced into the value estimation 
    
[^36]: 告诉我实话：一种用于衡量大型语言模型可信度的系统

    Tell me the truth: A system to measure the trustworthiness of Large Language Models

    [https://arxiv.org/abs/2403.04964](https://arxiv.org/abs/2403.04964)

    本文提出了一种基于预定义领域知识图的系统化方法来衡量大型语言模型的可信度。

    

    大型语言模型（LLM）自从2023年11月ChatGPT推出以来，在大多数新闻中占据了重要位置。然而，一年多过去了，公司抵触采用它们的一个主要原因是他们对这些系统的可信度缺乏信心。一项由Baymard（2023）进行的研究发现，ChatGPT-4 在识别网站可用性问题时有80.1%的假阳性错误率。而《JAMA儿科学》杂志（JAMA Pediatrics）于2024年1月的研究发现，ChatGPT 在诊断儿科医疗案例时的准确率为17%（Barile et al., 2024）。那么，何为“信任”？信任是一个相对的、主观的条件，可以根据文化、领域和个体而变化。那么，在给定一个领域的情况下，如何衡量系统的可信度呢？本文提出了一种基于预定义领域知识图表示的系统化方法来衡量可信度。

    arXiv:2403.04964v1 Announce Type: new  Abstract: Large Language Models (LLM) have taken the front seat in most of the news since November 2023, when ChatGPT was introduced. After more than one year, one of the major reasons companies are resistant to adopting them is the limited confidence they have in the trustworthiness of those systems. In a study by (Baymard, 2023), ChatGPT-4 showed an 80.1% false-positive error rate in identifying usability issues on websites. A Jan. '24 study by JAMA Pediatrics found that ChatGPT has an accuracy rate of 17% percent when diagnosing pediatric medical cases (Barile et al., 2024). But then, what is "trust"? Trust is a relative, subject condition that can change based on culture, domain, individuals. And then, given a domain, how can the trustworthiness of a system be measured? In this paper, I present a systematic approach to measure trustworthiness based on a predefined ground truth, represented as a knowledge graph of the domain. The approach is a 
    
[^37]: 在基于错误的人类评估中深入评估GPT-4在句子简化中的表现

    An In-depth Evaluation of GPT-4 in Sentence Simplification with Error-based Human Assessment

    [https://arxiv.org/abs/2403.04963](https://arxiv.org/abs/2403.04963)

    本文深入评估了GPT-4在句子简化中的表现，指出现有自动评估指标和人类评估方法对于大型语言模型的适用性仍有待进一步研究。

    

    句子简化是一种重写句子以便更易阅读和理解的方法，对于帮助有各种阅读难题的人来说是一种有前途的技术。随着先进大型语言模型（LLMs）的兴起，评估它们在句子简化中的表现变得迫在眉睫。最近的研究利用自动评估指标和人类评估来评估LLMs的简化能力。然而，现有评估方法对LLMs在简化评估中的适用性仍然存在疑问。首先，现有自动指标在LLMs的简化评估中的适用性仍不确定。其次，当前在句子简化中的人类评估方法通常陷入两个极端：要么过于肤浅，无法清晰理解模型的表现，要么过于详细，使注释过程复杂且容易出现不一致性，从而影响评估的可靠性。

    arXiv:2403.04963v1 Announce Type: cross  Abstract: Sentence simplification, which rewrites a sentence to be easier to read and understand, is a promising technique to help people with various reading difficulties. With the rise of advanced large language models (LLMs), evaluating their performance in sentence simplification has become imperative. Recent studies have used both automatic metrics and human evaluations to assess the simplification abilities of LLMs. However, the suitability of existing evaluation methodologies for LLMs remains in question. First, the suitability of current automatic metrics on LLMs' simplification evaluation is still uncertain. Second, current human evaluation approaches in sentence simplification often fall into two extremes: they are either too superficial, failing to offer a clear understanding of the models' performance, or overly detailed, making the annotation process complex and prone to inconsistency, which in turn affects the evaluation's reliabil
    
[^38]: SecGPT：一种面向基于LLM系统的执行隔离架构

    SecGPT: An Execution Isolation Architecture for LLM-Based Systems

    [https://arxiv.org/abs/2403.04960](https://arxiv.org/abs/2403.04960)

    提出了一种面向LLM系统的执行隔离架构SecGPT，旨在解决第三方应用程序执行所引发的安全和隐私问题

    

    大型语言模型（LLMs）被扩展为系统，如ChatGPT，已经开始支持第三方应用程序。这些LLM应用程序利用LLMs的事实上基于自然语言的自动执行范式：即，应用程序及其交互是用自然语言定义的，提供对用户数据的访问，并被允许自由地相互交互以及与系统互动。这些LLM应用程序生态系统类似于早期计算平台的设置，在那里应用程序和系统之间缺乏足够的隔离。由于第三方应用程序可能不可信，并且受自然语言界面的不精确性加剧，当前的设计会为用户带来安全和隐私风险。在本文中，我们提出了SecGPT，一种面向LLM系统的架构，旨在缓解由第三方应用程序执行引起的安全性和隐私问题。SecGPT的关键思想是隔离应用程序的执行和更多的预

    arXiv:2403.04960v1 Announce Type: cross  Abstract: Large language models (LLMs) extended as systems, such as ChatGPT, have begun supporting third-party applications. These LLM apps leverage the de facto natural language-based automated execution paradigm of LLMs: that is, apps and their interactions are defined in natural language, provided access to user data, and allowed to freely interact with each other and the system. These LLM app ecosystems resemble the settings of earlier computing platforms, where there was insufficient isolation between apps and the system. Because third-party apps may not be trustworthy, and exacerbated by the imprecision of the natural language interfaces, the current designs pose security and privacy risks for users. In this paper, we propose SecGPT, an architecture for LLM-based systems that aims to mitigate the security and privacy issues that arise with the execution of third-party apps. SecGPT's key idea is to isolate the execution of apps and more pre
    
[^39]: 为报告生成调优心电图指导

    Electrocardiogram Instruction Tuning for Report Generation

    [https://arxiv.org/abs/2403.04945](https://arxiv.org/abs/2403.04945)

    提出了Multimodal ECG Instruction Tuning（MEIT）框架，首次尝试使用LLMs和多模态指导解决ECG报告生成问题，并在两个大规模ECG数据集上进行了广泛的实验评估其优越性。

    

    心电图（ECG）作为心脏病情监测的主要非侵入性诊断工具，对于协助临床医生至关重要。最近的研究集中在使用ECG数据对心脏病情进行分类，但忽略了ECG报告生成，这不仅耗时，而且需要临床专业知识。为了自动化ECG报告生成并确保其多功能性，我们提出了Multimodal ECG Instruction Tuning（MEIT）框架，这是\textit{首次}尝试使用LLMs和多模态指导来解决ECG报告生成问题。为了促进未来的研究，我们建立了一个基准来评估MEIT在两个大规模ECG数据集上使用各种LLM骨干的表现。我们的方法独特地对齐了ECG信号和报告的表示，并进行了大量实验来评估MEIT与九个开源LLMs，使用了超过80万个ECG报告。MEIT的结果凸显了其优越性。

    arXiv:2403.04945v1 Announce Type: new  Abstract: Electrocardiogram (ECG) serves as the primary non-invasive diagnostic tool for cardiac conditions monitoring, are crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is not only time-consuming but also requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework, the \textit{first} attempt to tackle ECG report generation with LLMs and multimodal instructions. To facilitate future research, we establish a benchmark to evaluate MEIT with various LLMs backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to benchmark MEIT with nine open source LLMs, using more than 800,000 ECG reports. MEIT's results underscore the superior p
    
[^40]: 人工智能与大型预训练模型合作调查

    A Survey on Human-AI Teaming with Large Pre-Trained Models

    [https://arxiv.org/abs/2403.04931](https://arxiv.org/abs/2403.04931)

    本文调查了大型预训练模型与人工智能合作的重要性，强调了这些模型如何超越传统方法增强协作智能，并探讨了其在增强人类能力、改善AI模型、有效团队合作、道德考虑以及在各个领域广泛应用方面的潜在作用。

    

    在人工智能（AI）迅速发展的景观中，人类智能和AI系统之间的协作，即人工智能（HAI）合作，已成为推进问题解决和决策过程的基石。大型预训练模型（LPtM）的出现显著改变了这一景观，通过利用大量数据来理解和预测复杂模式，为人类提供了前所未有的能力。本文调查了LPtMs与HAI的关键整合，强调了这些模型如何超越传统方法增强协作智能。重点探讨了LPtMs在增强人类能力方面的协同潜力，讨论了这种协作对AI模型改进、有效的团队合作、道德考虑以及在各个领域的广泛应用影响。通过这一探索，研究揭示了LPtM增强HAI的变革性影响。

    arXiv:2403.04931v1 Announce Type: new  Abstract: In the rapidly evolving landscape of artificial intelligence (AI), the collaboration between human intelligence and AI systems, known as Human-AI (HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and decision-making processes. The advent of Large Pre-trained Models (LPtM) has significantly transformed this landscape, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how these models enhance collaborative intelligence beyond traditional approaches. It examines the synergistic potential of LPtMs in augmenting human capabilities, discussing this collaboration for AI model improvements, effective teaming, ethical considerations, and their broad applied implications in various sectors. Through this exploration, the study sheds light on the transformative impact of LPtM-enhanced HAI 
    
[^41]: ConstitutionalExperts: 训练基于原则的提示混合体

    ConstitutionalExperts: Training a Mixture of Principle-based Prompts

    [https://arxiv.org/abs/2403.04894](https://arxiv.org/abs/2403.04894)

    提出了ConstitutionalExperts方法，通过学习宪法原则构建提示，采用逐步改进提示和MoE架构，展现出在不同语义区域学习独特提示的潜力，并在六个基准数据集上表现优异。

    

    大型语言模型（LLMs）在各种任务上表现出色，但写作仍然是一个困难且繁琐的过程。 在这项工作中，我们介绍了ConstitutionalExperts，这是一种学习由宪法原则（即规则）组成的提示的方法，给定一个训练数据集。 与以往优化提示作为单个实体的方法不同，我们的方法通过分别编辑各个原则逐步改进提示。 我们还展示了通过为训练数据的不同语义区域学习唯一的提示，并在推断时使用专家混合（MoE）架构来提高整体性能。 我们将我们的方法与其他六个基准数据集上的其他最先进的提示优化技术进行了比较。 我们还调查了MoE是否改善这些其他技术。 我们的结果表明，ConstitutionalExperts的表现优于其他提示优化技术。

    arXiv:2403.04894v1 Announce Type: cross  Abstract: Large language models (LLMs) are highly capable at a variety of tasks given the right prompt, but writing one is still a difficult and tedious process. In this work, we introduce ConstitutionalExperts, a method for learning a prompt consisting of constitutional principles (i.e. rules), given a training dataset. Unlike prior methods that optimize the prompt as a single entity, our method incrementally improves the prompt by surgically editing individual principles. We also show that we can improve overall performance by learning unique prompts for different semantic regions of the training data and using a mixture-of-experts (MoE) architecture to route inputs at inference time. We compare our method to other state of the art prompt-optimization techniques across six benchmark datasets. We also investigate whether MoE improves these other techniques. Our results suggest that ConstitutionalExperts outperforms other prompt optimization tec
    
[^42]: 基于少样本推动推理的链式思维驱动LLMs用于开放式医学问题回答

    Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering

    [https://arxiv.org/abs/2403.04890](https://arxiv.org/abs/2403.04890)

    本文提出了基于少样本推动推理的链式思维驱动LLMs用于开放式医学问题回答，通过修改MedQA-USMLE数据集并采用奖励训练机制，实现了在医疗场景中正确响应临床问题的有效方法。

    

    大型语言模型（LLMs）已经展示了在转变医疗保健方面的巨大潜力，通过自动化诸如临床文档、信息检索和决策支持等任务。在这方面，精心设计的提示已经成为在医疗场景中使用LLMs的强大工具，例如患者临床场景。在本文中，我们提出了MedQA-USMLE数据集的修改版本，目的是模拟真实临床场景。我们探讨了基于主观响应生成的Chain of Thought（CoT）推理，用于修改后的MedQA-USMLE数据集，通过适当的LM驱动前向推理来获得正确的医学问题答案。考虑到在医疗环境中响应验证的重要性，我们利用奖励训练机制，其中语言模型还为特定的临床问题回应提供了适当的验证响应。

    arXiv:2403.04890v1 Announce Type: new  Abstract: Large Language models (LLMs) have demonstrated significant potential in transforming healthcare by automating tasks such as clinical documentation, information retrieval, and decision support. In this aspect, carefully engineered prompts have emerged as a powerful tool for using LLMs for medical scenarios, e.g., patient clinical scenarios. In this paper, we propose a modified version of the MedQA-USMLE dataset, which is subjective, to mimic real-life clinical scenarios. We explore the Chain of Thought (CoT) reasoning based on subjective response generation for the modified MedQA-USMLE dataset with appropriate LM-driven forward reasoning for correct responses to the medical questions. Keeping in mind the importance of response verification in the medical setting, we utilize a reward training mechanism whereby the language model also provides an appropriate verified response for a particular response to a clinical question. In this regard,
    
[^43]: 使用混合码调查表明预训练模型在混合码文本上的泛化能力

    Code-Mixed Probes Show How Pre-Trained Models Generalise On Code-Switched Text

    [https://arxiv.org/abs/2403.04872](https://arxiv.org/abs/2403.04872)

    这项研究探讨了预训练语言模型在处理混合码文本方面的能力，发现它们在检测、利用结构信息和表达语义信息方面表现良好

    

    混合码是一种普遍的语言现象，多语种个体可以无缝地在语言之间交替。本研究探讨了预训练语言模型在处理混合码文本方面的能力，包括模型检测混合码文本的能力、模型利用的结构信息变化以捕捉混合码文本的能力，以及混合码文本中语义信息表达的一致性。通过创建一个新颖的数据集，结合了自然形态的混合码文本与源语言的平行翻译，我们进行了系统化和控制性评估。研究结果显示，预训练语言模型在处理混合码文本时表现良好。

    arXiv:2403.04872v1 Announce Type: new  Abstract: Code-switching is a prevalent linguistic phenomenon in which multilingual individuals seamlessly alternate between languages. Despite its widespread use online and recent research trends in this area, research in code-switching presents unique challenges, primarily stemming from the scarcity of labelled data and available resources. In this study we investigate how pre-trained Language Models handle code-switched text in three dimensions: a) the ability of PLMs to detect code-switched text, b) variations in the structural information that PLMs utilise to capture code-switched text, and c) the consistency of semantic information representation in code-switched text. To conduct a systematic and controlled evaluation of the language models in question, we create a novel dataset of well-formed naturalistic code-switched text along with parallel translations into the source languages. Our findings reveal that pre-trained language models are e
    
[^44]: 评估上下文相关健康问题中的偏见

    Evaluating Biases in Context-Dependent Health Questions

    [https://arxiv.org/abs/2403.04858](https://arxiv.org/abs/2403.04858)

    研究评估了大型语言模型在健康领域中的上下文问题中存在的偏见，发现年轻成年女性用户受到偏爱

    

    基于聊天的大型语言模型有机会赋予那些缺乏高质量医疗保健的个人通过各种主题接收个性化信息的能力。然而，用户可能提出不充分的问题，需要额外的上下文信息模型才能正确回答。本研究探讨了大型语言模型的偏见是如何通过这些与健康领域相关的上下文问题展现出来的。为了实现这一目标，我们整理了一个依赖于年龄、性别和位置属性的性健康问题数据集。我们比较了带有和不带有人口统计背景上下文的模型输出，以确定我们的上下文问题中的群体对齐情况。我们的实验显示出这些属性中的偏见，其中年轻成年女性用户受到偏爱。

    arXiv:2403.04858v1 Announce Type: new  Abstract: Chat-based large language models have the opportunity to empower individuals lacking high-quality healthcare access to receive personalized information across a variety of topics. However, users may ask underspecified questions that require additional context for a model to correctly answer. We study how large language model biases are exhibited through these contextual questions in the healthcare domain. To accomplish this, we curate a dataset of sexual and reproductive healthcare questions that are dependent on age, sex, and location attributes. We compare models' outputs with and without demographic context to determine group alignment among our contextual questions. Our experiments reveal biases in each of these attributes, where young adult female users are favored.
    
[^45]: 从半结构化面谈文本中自动提取信息

    Automating the Information Extraction from Semi-Structured Interview Transcripts

    [https://arxiv.org/abs/2403.04819](https://arxiv.org/abs/2403.04819)

    本研究提出了一种新的自动系统，结合了BERT嵌入和HDBSCAN聚类，可以从半结构化面谈文本中快速提取信息，为研究人员提供了一个便捷的工具来分析和可视化主题结构。

    

    本文探讨了开发和应用一种自动系统，旨在从半结构化面谈文本中提取信息。由于传统的定性分析方法，如编码，劳动密集型的本质，对可以促进分析过程的工具存在着重大需求。我们的研究探讨了各种主题建模技术，并得出结论，适用于分析面谈文本的最佳模型是BERT嵌入和HDBSCAN聚类的结合。我们提出了一个用户友好的软件原型，使研究人员，包括那些没有编程技能的人，能够高效处理和可视化面谈数据的主题结构。该工具不仅促进了定性分析的初期阶段，还为揭示的主题之间的相互联系提供了见解，从而增强了定性分析的深度。

    arXiv:2403.04819v1 Announce Type: new  Abstract: This paper explores the development and application of an automated system designed to extract information from semi-structured interview transcripts. Given the labor-intensive nature of traditional qualitative analysis methods, such as coding, there exists a significant demand for tools that can facilitate the analysis process. Our research investigates various topic modeling techniques and concludes that the best model for analyzing interview texts is a combination of BERT embeddings and HDBSCAN clustering. We present a user-friendly software prototype that enables researchers, including those without programming skills, to efficiently process and visualize the thematic structure of interview data. This tool not only facilitates the initial stages of qualitative analysis but also offers insights into the interconnectedness of topics revealed, thereby enhancing the depth of qualitative analysis.
    
[^46]: 在句法感知代码填空任务上评估LLMs

    Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks

    [https://arxiv.org/abs/2403.04814](https://arxiv.org/abs/2403.04814)

    该研究引入了一个新的基准SAFIM用于评估LLMs在代码填空任务上的句法感知完成表现，发现FIM预训练不仅提高了FIM的熟练度，还改善了LLMs的左到右推理，挑战了传统观念并表明预训练方法和数据品质对模型性能的影响更甚于模型大小。

    

    我们介绍了一种名为Syntax-Aware Fill-In-the-Middle（SAFIM）的新基准，用于评估大型语言模型（LLMs）在代码填空（FIM）任务上的表现。该基准侧重于程序结构的句法感知完成，如代码块和条件表达式，并包括来自多种编程语言的17,720个示例，来源于2022年4月之后的最新代码提交，以最小化数据污染。 SAFIM提供了一个强大的框架，具有各种提示设计和新颖的句法感知后处理技术，有助于在LLMs之间进行准确和公平的比较。我们对15个LLMs进行了全面评估，结果表明FIM预训练不仅提升了FIM的熟练程度，还改进了LLMs的左到右（L2R）推理。我们的发现挑战了传统观念，并表明预训练方法和数据质量对模型性能的影响大于模型大小。因此，SAFIM为未来构建

    arXiv:2403.04814v1 Announce Type: cross  Abstract: We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future 
    
[^47]: 量化污染评估语言模型的代码生成能力

    Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models

    [https://arxiv.org/abs/2403.04811](https://arxiv.org/abs/2403.04811)

    研究量化了流行的代码生成基准测试的数据污染程度，揭示了它们与预训练语料库之间的重叠，并展示模型的显著性能重叠。

    

    尽管大型语言模型在各种代码生成基准测试中取得了显著的性能，但人们对这些基准测试的潜在污染日益关注，因为它们可能泄漏到预训练和微调数据中。本文对流行的代码生成基准测试进行了全面研究，准确量化了它们与预训练语料库之间的重叠，通过表面级和语义级匹配。在我们的实验中，我们展示了流行的代码生成基准测试与公开训练语料库之间存在重叠，并且模型表现显着。

    arXiv:2403.04811v1 Announce Type: cross  Abstract: While large language models have achieved remarkable performance on various code generation benchmarks, there have been growing concerns regarding potential contamination of these benchmarks as they may be leaked into pretraining and finetuning data. While recent work has investigated contamination in natural language generation and understanding tasks, there has been less extensive research into how data contamination impacts the evaluation of code generation, which is critical for understanding the robustness and reliability of LLMs in programming contexts. In this work, we perform a comprehensive study of data contamination of popular code generation benchmarks, and precisely quantify their overlap with pretraining corpus through both surface-level and semantic-level matching. In our experiments, we show that there are substantial overlap between popular code generation benchmarks and open training corpus, and models perform signifi
    
[^48]: WaterMax: 打破LLM水印可检测性-稳健性-质量的平衡

    WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off

    [https://arxiv.org/abs/2403.04808](https://arxiv.org/abs/2403.04808)

    WaterMax提出了一种新的水印方案，能够在保持生成文本质量的同时实现高检测性能，打破了水印技术中质量和稳健性之间的传统平衡。

    

    水印是阻止大型语言模型被恶意使用的技术手段。本文提出了一种称为WaterMax的新颖水印方案，具有高检测性能，同时保持原始LLM生成文本的质量。其新设计不会对LLM进行任何修改（不调整权重、对数、温度或采样技术）。WaterMax平衡了稳健性和复杂性，与文献中的水印技术相反，从根本上引发了质量和稳健性之间的平衡。其性能在理论上得到证明并经过实验证实。在最全面的基准测试套件下，它胜过所有的最先进技术。

    arXiv:2403.04808v1 Announce Type: cross  Abstract: Watermarking is a technical means to dissuade malfeasant usage of Large Language Models. This paper proposes a novel watermarking scheme, so-called WaterMax, that enjoys high detectability while sustaining the quality of the generated text of the original LLM. Its new design leaves the LLM untouched (no modification of the weights, logits, temperature, or sampling technique). WaterMax balances robustness and complexity contrary to the watermarking techniques of the literature inherently provoking a trade-off between quality and robustness. Its performance is both theoretically proven and experimentally validated. It outperforms all the SotA techniques under the most complete benchmark suite.
    
[^49]: AttentionStitch：关注如何解决语音编辑问题

    AttentionStitch: How Attention Solves the Speech Editing Problem

    [https://arxiv.org/abs/2403.04804](https://arxiv.org/abs/2403.04804)

    AttentionStitch模型通过引入双注意力块网络，并将编辑文本的mel频谱图与合成的mel频谱图自动融合，实现了语音编辑的无缝整合。

    

    自然高质量的语音生成是自然语言处理领域中一个具有挑战性的问题。除了语音生成之外，语音编辑也是一个关键任务，需要将编辑后的语音无缝、不易察觉地整合到合成语音中。我们提出了一种新颖的语音编辑方法，利用预训练的文本到语音（TTS）模型，如FastSpeech 2，并在其之上加入双注意力块网络，以自动将合成的mel频谱图与编辑文本的mel频谱图融合在一起。我们将这个模型称为AttentionStitch，因为它利用注意力来将音频样本拼接在一起。我们在单个和多个说话者数据集（LJSpeech和VCTK）上对所提出的AttentionStitch模型与最先进的基线模型进行评估。通过客观和主观评估测试，我们证明了其优越的性能。

    arXiv:2403.04804v1 Announce Type: cross  Abstract: The generation of natural and high-quality speech from text is a challenging problem in the field of natural language processing. In addition to speech generation, speech editing is also a crucial task, which requires the seamless and unnoticeable integration of edited speech into synthesized speech. We propose a novel approach to speech editing by leveraging a pre-trained text-to-speech (TTS) model, such as FastSpeech 2, and incorporating a double attention block network on top of it to automatically merge the synthesized mel-spectrogram with the mel-spectrogram of the edited text. We refer to this model as AttentionStitch, as it harnesses attention to stitch audio samples together. We evaluate the proposed AttentionStitch model against state-of-the-art baselines on both single and multi-speaker datasets, namely LJSpeech and VCTK. We demonstrate its superior performance through an objective and a subjective evaluation test involving 1
    
[^50]: Alpaca对抗Vicuna：使用LLMs揭示LLMs的记忆化

    Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs

    [https://arxiv.org/abs/2403.04801](https://arxiv.org/abs/2403.04801)

    使用LLM代理进行黑盒提示优化方法，揭示了受害代理中更高级别的记忆化，相比直接用训练数据提示目标模型，这种方法更有效，能更好地量化LLMs的记忆化。

    

    在本文中，我们介绍了一种黑盒提示优化方法，该方法利用攻击者LLM代理来揭示受害代理中更高级别的记忆化，与直接用训练数据提示目标模型相比，这是量化LLMs记忆化的主导方法。我们使用迭代的拒绝抽样优化过程来找到基于指令的提示，具有两个主要特征：(1)与训练数据最小重叠，以避免直接向模型呈现解决方案，以及(2)受害模型输出与训练数据的最大重叠，旨在诱使受害者吐出训练数据。我们观察到，我们基于指令的提示生成的输出与训练数据重叠程度比基线前缀后缀测量高出23.7％。我们的发现表明，(1)经过指令调整的模型可以暴露与他们的基本模型一样多的预训练数据。

    arXiv:2403.04801v1 Announce Type: new  Abstract: In this paper, we introduce a black-box prompt optimization method that uses an attacker LLM agent to uncover higher levels of memorization in a victim agent, compared to what is revealed by prompting the target model with the training data directly, which is the dominant approach of quantifying memorization in LLMs. We use an iterative rejection-sampling optimization process to find instruction-based prompts with two main characteristics: (1) minimal overlap with the training data to avoid presenting the solution directly to the model, and (2) maximal overlap between the victim model's output and the training data, aiming to induce the victim to spit out training data. We observe that our instruction-based prompts generate outputs with 23.7% higher overlap with training data compared to the baseline prefix-suffix measurements. Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, 
    
[^51]: 低资源语言中的人工智能素养：从创造约鲁巴语AI视频中获得的见解

    AI Literacy in Low-Resource Languages:Insights from creating AI in Yoruba videos

    [https://arxiv.org/abs/2403.04799](https://arxiv.org/abs/2403.04799)

    本研究探索了在低资源语言如约鲁巴语中创建和分发人工智能视频的方法，并展示了其在全球范围内的潜在影响。

    

    为了有效地应对人工智能革命，人工智能素养至关重要。然而，主要存在于主导语言中的内容在像约鲁巴语（有4100万母语使用者）这样的低资源语言中造成了一定的差距。本案例研究探讨了通过在约鲁巴语中创建和分发AI视频来弥合这一差距。该项目开发了26个视频，涵盖基础、中级和高级人工智能概念，利用故事叙述和简易解释。这些视频采用了一种成本效益高的方法制作，并在YouTube、LinkedIn和Twitter上进行分发，估计触及了来自22个国家的全球观众。对YouTube的分析揭示了观看模式的见解，其中25-44岁年龄组贡献了最多的观看量。值得注意的是，超过一半的流量来源于外部来源，突显了跨平台推广的潜力。这项研究展示了在低资源语言中创建人工智能素养内容的可行性和影响。

    arXiv:2403.04799v1 Announce Type: cross  Abstract: To effectively navigate the AI revolution, AI literacy is crucial. However, content predominantly exists in dominant languages, creating a gap for low-resource languages like Yoruba (41 million native speakers). This case study explores bridging this gap by creating and distributing AI videos in Yoruba.The project developed 26 videos covering foundational, intermediate, and advanced AI concepts, leveraging storytelling and accessible explanations. These videos were created using a cost-effective methodology and distributed across YouTube, LinkedIn, and Twitter, reaching an estimated global audience of 22 countries. Analysis of YouTube reveals insights into viewing patterns, with the 25-44 age group contributing the most views. Notably, over half of the traffic originated from external sources, highlighting the potential of cross-platform promotion.This study demonstrates the feasibility and impact of creating AI literacy content in low
    
[^52]: JMI在SemEval 2024任务3中的应用：使用GPT和instruction-tuned Llama模型进行多模态情感因果分析的两步法

    JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models

    [https://arxiv.org/abs/2403.04798](https://arxiv.org/abs/2403.04798)

    本文介绍了针对SemEval-2024任务3开发的多模态情感因果分析系统，提出了通过两步框架解决多模态情感因果分析挑战的方法，并在实验中取得显著性能提升。

    

    本文介绍了我们针对SemEval-2024任务3：“对话中的多模态情感因果分析竞赛”开发的系统。有效捕捉人类对话中的情感需要整合文本、音频和视频等多种模态。然而，这些多样性模态的复杂性给开发高效的多模态情感因果分析系统带来了挑战。我们提出的方法通过两步框架来解决这些挑战。我们在实现中采用了两种不同的方法。在方法1中，我们使用两个单独的Llama 2模型进行情感和原因预测的instruction-tuning。在方法2中，我们使用GPT-4V进行会话级视频描述，并使用带有GPT 3.5的上下文学习对注释对话进行处理。我们的系统获得了第4名，系统消融实验表明，我们提出的解决方案取得了显著的性能增益。

    arXiv:2403.04798v1 Announce Type: new  Abstract: This paper presents our system development for SemEval-2024 Task 3: "The Competition of Multimodal Emotion Cause Analysis in Conversations". Effectively capturing emotions in human conversations requires integrating multiple modalities such as text, audio, and video. However, the complexities of these diverse modalities pose challenges for developing an efficient multimodal emotion cause analysis (ECA) system. Our proposed approach addresses these challenges by a two-step framework. We adopt two different approaches in our implementation. In Approach 1, we employ instruction-tuning with two separate Llama 2 models for emotion and cause prediction. In Approach 2, we use GPT-4V for conversation-level video description and employ in-context learning with annotated conversation using GPT 3.5. Our system wins rank 4, and system ablation experiments demonstrate that our proposed solutions achieve significant performance gains. All the experime
    
[^53]: 在中间被发现: 语言模型如何通过即插即用位置编码更好地使用长上下文

    Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding

    [https://arxiv.org/abs/2403.04797](https://arxiv.org/abs/2403.04797)

    通过引入多尺度位置编码（Ms-PoE）来增强大型语言模型（LLMs）对上下文中间相关信息的处理能力，解决了LLMs面临的“中间丢失”挑战。

    

    本文旨在克服大型语言模型（LLMs）面临的“中间丢失”挑战。尽管最近的进展成功实现了LLMs对包含400万令牌的稳定语言建模，但大多数LLMs在识别位于上下文中间的相关信息方面仍然存在持续困难。为解决这一问题，本文引入了多尺度位置编码（Ms-PoE），这是一种简单而有效的即插即用方法，可以增强LLMs处理位于上下文中间的相关信息的能力，无需微调或引入任何额外开销。Ms-PoE利用位置指数重新缩放以减轻RoPE引入的长期衰减效应，同时精心为不同注意力头分配不同的缩放比以保留预训练阶段学到的基本知识，形成多

    arXiv:2403.04797v1 Announce Type: new  Abstract: This paper aims to overcome the "lost-in-the-middle" challenge of large language models (LLMs). While recent advancements have successfully enabled LLMs to perform stable language modeling with up to 4 million tokens, the persistent difficulty faced by most LLMs in identifying relevant information situated in the middle of the context has not been adequately tackled. To address this problem, this paper introduces Multi-scale Positional Encoding (Ms-PoE) which is a simple yet effective plug-and-play approach to enhance the capacity of LLMs to handle the relevant information located in the middle of the context, without fine-tuning or introducing any additional overhead. Ms-PoE leverages the position indice rescaling to relieve the long-term decay effect introduced by RoPE, while meticulously assigning distinct scaling ratios to different attention heads to preserve essential knowledge learned during the pre-training step, forming a multi-
    
[^54]: 消防工程中的大型语言模型：针对领域知识对技术问题的审查

    Large Language Models in Fire Engineering: An Examination of Technical Questions Against Domain Knowledge

    [https://arxiv.org/abs/2403.04795](https://arxiv.org/abs/2403.04795)

    本研究比较了两个聊天机器人在消防工程中处理问题的表现，发现ChatGPT表现较优，展示了聊天机器人技术在消防工程实践中的潜力。

    

    本文介绍了比较两个最近的聊天机器人OpenAI的ChatGPT和谷歌的Bard，在火灾工程领域中评估它们处理与消防安全相关查询的回应的初步研究结果。 创建并检查了一系列不同类型的消防工程问题和场景，其中包括结构火灾设计、防火策略、疏散、建筑法规合规和灭火系统等（其中一些类似于消防保护考试（FPE）中常见的情况）。 结果显示了聊天机器人性能上的一些关键差异，ChatGPT表现出相对较好的性能。 此外，本论文突出了聊天机器人技术在提供关键信息的同时彻底改革消防工程实践的潜力，并概述了进一步改进和研究的领域。显然，在技术成熟后，这项技术将可能

    arXiv:2403.04795v1 Announce Type: cross  Abstract: This communication presents preliminary findings from comparing two recent chatbots, OpenAI's ChatGPT and Google's Bard, in the context of fire engineering by evaluating their responses in handling fire safety related queries. A diverse range of fire engineering questions and scenarios were created and examined, including structural fire design, fire prevention strategies, evacuation, building code compliance, and fire suppression systems (some of which resemble those commonly present in the Fire Protection exam (FPE)). The results reveal some key differences in the performance of the chatbots, with ChatGPT demonstrating a relatively superior performance. Then, this communication highlights the potential for chatbot technology to revolutionize fire engineering practices by providing instant access to critical information while outlining areas for further improvement and research. Evidently, and when it matures, this technology will lik
    
[^55]: 打破语言障碍：在多语言LLM应用中，直接推断能否胜过预翻译？

    Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?

    [https://arxiv.org/abs/2403.04792](https://arxiv.org/abs/2403.04792)

    本研究挑战了以往研究中建立的预翻译范式，并在108种语言中的94种语言中表明PaLM2-L在直接推断中优于预翻译。

    

    大型语言模型在多语言应用中具有重要潜力。然而，由于主要以英文为中心的预训练会导致固有偏见，因此已经普遍采用预翻译的做法，即在推断之前将非英文输入翻译成英文，从而导致复杂性和信息丢失。本研究重新评估了在PaLM2模型中的预翻译需求（Anil等人，2023年），这些模型已被证明在多语言任务中表现出色。我们在108种语言和6个不同基准测试中进行了全面调查，包括开放式生成式任务，在此类任务中之前的研究中被排除在外。我们的发现挑战了以前研究中建立的预翻译范式，突出了在PaLM2中直接推断的优势。具体而言，PaLM2-L在108种语言中的94种中始终优于预翻译。这些发现为更高效的翻译方法铺平了道路。

    arXiv:2403.04792v1 Announce Type: new  Abstract: Large language models hold significant promise in multilingual applications. However, inherent biases stemming from predominantly English-centric pre-training have led to the widespread practice of pre-translation, i.e., translating non-English inputs to English before inference, leading to complexity and information loss. This study re-evaluates the need for pre-translation in the context of PaLM2 models (Anil et al., 2023), which have been established as highly performant in multilingual tasks. We offer a comprehensive investigation across 108 languages and 6 diverse benchmarks, including open-end generative tasks, which were excluded from previous similar studies. Our findings challenge the pre-translation paradigm established in prior research, highlighting the advantages of direct inference in PaLM2. Specifically, PaLM2-L consistently outperforms pre-translation in 94 out of 108 languages. These findings pave the way for more effici
    
[^56]: LLM对抗律师：在大型英国案例法律数据集中识别摘要裁定的子集

    LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK Case Law Dataset

    [https://arxiv.org/abs/2403.04791](https://arxiv.org/abs/2403.04791)

    使用大型语言模型（LLM）对比传统的自然语言处理方法，可以更有效地从大型英国法院判决数据集中识别摘要裁定案例，取得了更高的F1得分。

    

    为了进行法律领域的计算研究，高效地识别与特定法律问题相关的法院裁决数据集是一项至关重要但具有挑战性的努力。本研究填补了文献中关于如何从大量英国法院决定的文集中隔离案例（在我们的案例中是摘要裁定）的空白。我们介绍了两种计算方法的比较分析：（1）传统的基于自然语言处理的方法，利用专家生成的关键字和逻辑运算符，以及（2）创新性地将Claude 2大语言模型应用于基于特定内容提示分类案例。我们使用了包含356,011份英国法院判决的剑桥法学文集，并确定大型语言模型的加权F1得分为0.94，而关键字的得分为0.78。尽管经过迭代改进，基于关键字的搜索逻辑未能捕捉法律语言中的细微差别。

    arXiv:2403.04791v1 Announce Type: new  Abstract: To undertake computational research of the law, efficiently identifying datasets of court decisions that relate to a specific legal issue is a crucial yet challenging endeavour. This study addresses the gap in the literature working with large legal corpora about how to isolate cases, in our case summary judgments, from a large corpus of UK court decisions. We introduce a comparative analysis of two computational methods: (1) a traditional natural language processing-based approach leveraging expert-generated keywords and logical operators and (2) an innovative application of the Claude 2 large language model to classify cases based on content-specific prompts. We use the Cambridge Law Corpus of 356,011 UK court decisions and determine that the large language model achieves a weighted F1 score of 0.94 versus 0.78 for keywords. Despite iterative refinement, the search logic based on keywords fails to capture nuances in legal language. We 
    
[^57]: 大型语言模型的在线训练：边聊天边学习

    Online Training of Large Language Models: Learn while chatting

    [https://arxiv.org/abs/2403.04790](https://arxiv.org/abs/2403.04790)

    本论文提出了一种新的互动范式，允许大型语言模型通过外部互动进行在线训练，实现了持续、实时的模型更新与个性化定制的灵活性。

    

    大语言模型(LLMs)已经极大地改变了自然语言处理(NLP)领域，提供了引人注目的功能，受到了广泛的应用。然而，现有的LLMs与用户之间的互动范式受制于灵活性不足、定制化受限或缺乏持续性学习。为了克服这些挑战，本文引入了一种新的互动范式-“使用外部互动进行在线训练”，将持续、实时的模型更新与通过外部互动（如AI）进行个性化定制相结合。

    arXiv:2403.04790v1 Announce Type: cross  Abstract: Large Language Models(LLMs) have dramatically revolutionized the field of Natural Language Processing(NLP), offering remarkable capabilities that have garnered widespread usage. However, existing interaction paradigms between LLMs and users are constrained by either inflexibility, limitations in customization, or a lack of persistent learning. This inflexibility is particularly evident as users, especially those without programming skills, have restricted avenues to enhance or personalize the model. Existing frameworks further complicate the model training and deployment process due to their computational inefficiencies and lack of user-friendly interfaces. To overcome these challenges, this paper introduces a novel interaction paradigm-'Online Training using External Interactions'-that merges the benefits of persistent, real-time model updates with the flexibility for individual customization through external interactions such as AI a
    
[^58]: TopicDiff：一种用于多模态会话情感检测的主题丰富扩散方法

    TopicDiff: A Topic-enriched Diffusion Approach for Multimodal Conversational Emotion Detection

    [https://arxiv.org/abs/2403.04789](https://arxiv.org/abs/2403.04789)

    提出了一种TopicDiff方法，用于捕获多模态会话情感检测任务中的主题信息，通过将扩散模型集成到神经主题模型中，解决了神经主题模型在捕获主题信息方面的多样性不足问题，并相对于现有MCE基线取得了显著改进

    

    多模态会话情感（MCE）检测通常跨越声学、视觉和语言模态，吸引了多媒体社区日益增加的兴趣。先前的研究主要集中在学习对话中的语境信息，只有少数考虑单一语言模态中的主题信息，而总是忽视声学和视觉主题信息。在此基础上，我们提出了一个模型不可知的Topic-enriched Diffusion（TopicDiff）方法，用于捕获MCE任务中的多模态主题信息。特别是，我们将扩散模型集成到神经主题模型中，以缓解神经主题模型在捕获主题信息方面的多样性不足问题。详细的评估表明，TopicDiff相对于最先进的MCE基线取得了显著改进，证明了多模态主题信息对MCE的重要性以及TopicDiff的有效性。

    arXiv:2403.04789v1 Announce Type: cross  Abstract: Multimodal Conversational Emotion (MCE) detection, generally spanning across the acoustic, vision and language modalities, has attracted increasing interest in the multimedia community. Previous studies predominantly focus on learning contextual information in conversations with only a few considering the topic information in single language modality, while always neglecting the acoustic and vision topic information. On this basis, we propose a model-agnostic Topic-enriched Diffusion (TopicDiff) approach for capturing multimodal topic information in MCE tasks. Particularly, we integrate the diffusion model into neural topic model to alleviate the diversity deficiency problem of neural topic model in capturing topic information. Detailed evaluations demonstrate the significant improvements of TopicDiff over the state-of-the-art MCE baselines, justifying the importance of multimodal topic information to MCE and the effectiveness of Topic
    
[^59]: 航空事故报告的主题建模分析：LDA和NMF模型的比较研究

    Topic Modeling Analysis of Aviation Accident Reports: A Comparative Study between LDA and NMF Models

    [https://arxiv.org/abs/2403.04788](https://arxiv.org/abs/2403.04788)

    本研究比较了航空事故报告分析中的两种主题建模技术，发现LDA在主题连贯性方面表现更出色，而NMF在提取主题方面优异。

    

    航空安全在现代社会至关重要，致力于减少事故并提高安全标准。在这一努力的核心是对航空事故报告进行分析，这些丰富的文本资源揭示了航空事故背后的原因和影响因素。本文比较了两种著名的主题建模技术，Latent Dirichlet Allocation（LDA）和Non-negative Matrix Factorization（NMF），并将其运用在航空事故报告分析的背景下。研究利用了National Transportation Safety Board（NTSB）数据集，主要目标是自动化和简化识别事故报告中潜在主题和模式的过程。利用一致性值（C_v）度量来评估生成主题的质量。LDA表现出更高的主题连贯性，表明主题内单词之间具有更强的语义相关性。同时，NMF在生产...

    arXiv:2403.04788v1 Announce Type: new  Abstract: Aviation safety is paramount in the modern world, with a continuous commitment to reducing accidents and improving safety standards. Central to this endeavor is the analysis of aviation accident reports, rich textual resources that hold insights into the causes and contributing factors behind aviation mishaps. This paper compares two prominent topic modeling techniques, Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF), in the context of aviation accident report analysis. The study leverages the National Transportation Safety Board (NTSB) Dataset with the primary objective of automating and streamlining the process of identifying latent themes and patterns within accident reports. The Coherence Value (C_v) metric was used to evaluate the quality of generated topics. LDA demonstrates higher topic coherence, indicating stronger semantic relevance among words within topics. At the same time, NMF excelled in produ
    
[^60]: 通过混合和完善过去来不断演进记忆

    Ever-Evolving Memory by Blending and Refining the Past

    [https://arxiv.org/abs/2403.04787](https://arxiv.org/abs/2403.04787)

    提出了一种新颖的长期对话记忆方案CREEM，通过混合过去记忆并引入完善过程来实现聊天机器人回应的整体改进和连贯性。

    

    对于类似人类的聊天机器人，构建长期记忆至关重要。构建记忆的一个天真方法可能只是列出总结的对话。然而，当说话者的状态随时间变化时，这样做可能会导致问题，并积累矛盾信息。记忆保持有组织对于降低回应生成器的混乱很重要。在本文中，我们提出了一种新颖的长期对话记忆方案，CREEM。与仅基于当前对话构建记忆的现有方法不同，我们提出的模型在记忆形成过程中混合过去的记忆。此外，我们引入了完善过程来处理多余或过时信息。这种创新性方法通过确保一个更加知情和动态演变的长期记忆，旨在提高聊天机器人回应的整体改进和连贯性。

    arXiv:2403.04787v1 Announce Type: cross  Abstract: For a human-like chatbot, constructing a long-term memory is crucial. A naive approach for making a memory could be simply listing the summarized dialogue. However, this can lead to problems when the speaker's status change over time and contradictory information gets accumulated. It is important that the memory stays organized to lower the confusion for the response generator. In this paper, we propose a novel memory scheme for long-term conversation, CREEM. Unlike existing approaches that construct memory based solely on current sessions, our proposed model blending past memories during memory formation. Additionally, we introduce refining process to handle redundant or outdated information. This innovative approach seeks for overall improvement and coherence of chatbot responses by ensuring a more informed and dynamically evolving long-term memory.
    
[^61]: 分解防御：大型语言模型攻击的比较调查

    Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models

    [https://arxiv.org/abs/2403.04786](https://arxiv.org/abs/2403.04786)

    本文通过全面调查各种攻击形式，探讨了大型语言模型受攻击的性质、机制、潜在影响以及当前防御策略，为模型完整性和用户信任提供了重要见解。

    

    大型语言模型（LLMs）已经成为自然语言处理（NLP）领域的基石，在理解和生成类似人类文本方面提供了革命性的能力。然而，随着它们日益重要，这些模型的安全性和脆弱性方面已经引起了重要关注。本文对针对LLMs的各种形式攻击进行了全面调查，讨论了这些攻击的性质和机制、它们的潜在影响以及当前的防御策略。我们深入探讨了旨在操纵模型输出的对抗性攻击、影响模型训练的数据中毒以及与训练数据利用相关的隐私问题。本文还探讨了不同攻击方法的有效性、LLMs抵抗这些攻击的韧性以及对模型完整性和用户信任的影响。通过审视最新研究，我们提供了见解。

    arXiv:2403.04786v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have become a cornerstone in the field of Natural Language Processing (NLP), offering transformative capabilities in understanding and generating human-like text. However, with their rising prominence, the security and vulnerability aspects of these models have garnered significant attention. This paper presents a comprehensive survey of the various forms of attacks targeting LLMs, discussing the nature and mechanisms of these attacks, their potential impacts, and current defense strategies. We delve into topics such as adversarial attacks that aim to manipulate model outputs, data poisoning that affects model training, and privacy concerns related to training data exploitation. The paper also explores the effectiveness of different attack methodologies, the resilience of LLMs against these attacks, and the implications for model integrity and user trust. By examining the latest research, we provide insight
    
[^62]: 使用电子健康记录数据预测5年慢性疾病队列的大型语言多模型

    Large Language Multimodal Models for 5-Year Chronic Disease Cohort Prediction Using EHR Data

    [https://arxiv.org/abs/2403.04785](https://arxiv.org/abs/2403.04785)

    本研究提出了一种大型语言多模型（LLMMs）框架，结合临床笔记和实验室检验结果的多模态数据，用于预测慢性疾病风险。

    

    慢性疾病如糖尿病是全球发病率和死亡率的主要原因。本研究从台湾医院数据库收集了五年的电子健康记录数据，包括1,420,596份临床笔记、387,392份实验室检验结果以及超过1,505种实验室检验项目，重点研究了用于研究预训练大型语言模型的方法。我们提出了一种新颖的大型语言多模型（LLMMs）框架，将临床笔记和实验室检验结果的多模态数据相结合，用于预测慢性疾病风险。我们的方法结合了文本嵌入编码器和多头注意力层来学习实验室检验数值，利用深度神经网络（DNN）模块进行预测。

    arXiv:2403.04785v1 Announce Type: cross  Abstract: Chronic diseases such as diabetes are the leading causes of morbidity and mortality worldwide. Numerous research studies have been attempted with various deep learning models in diagnosis. However, most previous studies had certain limitations, including using publicly available datasets (e.g. MIMIC), and imbalanced data. In this study, we collected five-year electronic health records (EHRs) from the Taiwan hospital database, including 1,420,596 clinical notes, 387,392 laboratory test results, and more than 1,505 laboratory test items, focusing on research pre-training large language models. We proposed a novel Large Language Multimodal Models (LLMMs) framework incorporating multimodal data from clinical notes and laboratory test results for the prediction of chronic disease risk. Our method combined a text embedding encoder and multi-head attention layer to learn laboratory test values, utilizing a deep neural network (DNN) module to 
    
[^63]: AutoDefense: 多Agent LLM 防御对抗越狱攻击

    AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks

    [https://arxiv.org/abs/2403.04783](https://arxiv.org/abs/2403.04783)

    提出了一种基于响应过滤的多Agent防御框架AutoDefense，可以有效提高LLMs对抗越狱攻击的鲁棒性，同时保持正常用户请求的性能。

    

    尽管在道德对齐方面进行了广泛的预训练和微调以防止在用户请求时生成有害信息，但大型语言模型（LLMs）仍然容易受到越狱攻击。 本文提出了一种基于响应过滤的多Agent防御框架AutoDefense，用于从LLMs中过滤有害回复。 此框架为LLM代理分配不同角色，并利用它们共同完成防御任务。 任务的划分增强了LLMs的整体遵循指令能力，并使其他防御组件作为工具集成成为可能。 AutoDefense 可以适应各种规模和种类的开源LLMs作为代理。 通过对大量有害和安全提示进行广泛实验，我们验证了所提出的AutoDefense在提高对抗越狱攻击的鲁棒性的同时，保持了正常用户请求的性能。

    arXiv:2403.04783v1 Announce Type: cross  Abstract: Despite extensive pre-training and fine-tuning in moral alignment to prevent generating harmful information at user request, large language models (LLMs) remain vulnerable to jailbreak attacks. In this paper, we propose AutoDefense, a response-filtering based multi-agent defense framework that filters harmful responses from LLMs. This framework assigns different roles to LLM agents and employs them to complete the defense task collaboratively. The division in tasks enhances the overall instruction-following of LLMs and enables the integration of other defense components as tools. AutoDefense can adapt to various sizes and kinds of open-source LLMs that serve as agents. Through conducting extensive experiments on a large scale of harmful and safe prompts, we validate the effectiveness of the proposed AutoDefense in improving the robustness against jailbreak attacks, while maintaining the performance at normal user request. Our code and 
    
[^64]: 一项关于时间知识图的调查：表示学习与应用

    A Survey on Temporal Knowledge Graph: Representation Learning and Applications

    [https://arxiv.org/abs/2403.04782](https://arxiv.org/abs/2403.04782)

    时间知识图表示学习将时间信息融入标准知识图框架，可以对实体和关系随时间的动态变化进行建模。

    

    知识图引起了重要的研究关注，并被广泛用于增强下游应用。然而，大多数当前研究主要集中在静态知识图上，其事实不随时间而变化，并忽略了它们随时间的动态演变。因此，时间知识图受到更多关注，因为大量结构化知识仅存在于特定时期内。知识图表示学习旨在为知识图中的实体和关系学习低维向量嵌入。时态知识图的表示学习将时间信息融入标准知识图框架中，可以对实体和关系随时间的动态变化进行建模。本文对时态知识图表示学习及其应用进行了全面调查。我们从介绍定义、数据集和e

    arXiv:2403.04782v1 Announce Type: cross  Abstract: Knowledge graphs have garnered significant research attention and are widely used to enhance downstream applications. However, most current studies mainly focus on static knowledge graphs, whose facts do not change with time, and disregard their dynamic evolution over time. As a result, temporal knowledge graphs have attracted more attention because a large amount of structured knowledge exists only within a specific period. Knowledge graph representation learning aims to learn low-dimensional vector embeddings for entities and relations in a knowledge graph. The representation learning of temporal knowledge graphs incorporates time information into the standard knowledge graph framework and can model the dynamics of entities and relations over time. In this paper, we conduct a comprehensive survey of temporal knowledge graph representation learning and its applications. We begin with an introduction to the definitions, datasets, and e
    
[^65]: MuseGraph：面向大型语言模型的图导向指令调整用于通用图挖掘

    MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining

    [https://arxiv.org/abs/2403.04780](https://arxiv.org/abs/2403.04780)

    MuseGraph将GNNs和LLMs的优势结合起来，提出了一种更有效和通用的图挖掘方法，可以跨不同任务和数据集使用

    

    具有丰富属性的图在建模互联实体和改进各种实际应用中的预测方面至关重要。传统图神经网络（GNNs）通常用于建模带属性的图，但需要在应用于不同图任务和数据集时进行重新训练。尽管大型语言模型（LLMs）的出现在自然语言处理中引入了新的范例，但LLMs在图挖掘中的生成潜力仍未得到充分探索。为此，我们提出了一个新颖的框架 MuseGraph，它无缝整合了GNNs和LLMs的优势，并促进了一种更有效和通用的图挖掘方法，可跨不同任务和数据集使用。具体而言，我们首先通过提出的自适应输入生成引入一个紧凑的图描述，以在语言令牌限制的约束下封装来自图的关键信息。

    arXiv:2403.04780v1 Announce Type: cross  Abstract: Graphs with abundant attributes are essential in modeling interconnected entities and improving predictions in various real-world applications. Traditional Graph Neural Networks (GNNs), which are commonly used for modeling attributed graphs, need to be re-trained every time when applied to different graph tasks and datasets. Although the emergence of Large Language Models (LLMs) has introduced a new paradigm in natural language processing, the generative potential of LLMs in graph mining remains largely under-explored. To this end, we propose a novel framework MuseGraph, which seamlessly integrates the strengths of GNNs and LLMs and facilitates a more effective and generic approach for graph mining across different tasks and datasets. Specifically, we first introduce a compact graph description via the proposed adaptive input generation to encapsulate key information from the graph under the constraints of language token limitations. T
    
[^66]: QASE增强型PLMs：提高文本生成在MRC中的控制能力

    QASE Enhanced PLMs: Improved Control in Text Generation for MRC

    [https://arxiv.org/abs/2403.04771](https://arxiv.org/abs/2403.04771)

    QASE模块在PLMs的微调过程中提升了文本生成在机器阅读理解中的控制能力，使得其在MRC任务中超越了GPT-4等领先的LLMs。

    

    为了解决生成式模型在机器阅读理解（MRC）中失控生成的挑战，我们引入了Question-Attended Span Extraction（QASE）模块。在预训练的生成式语言模型（PLMs）微调过程中集成QASE能够使这些PLMs匹配SOTA的抽取方法并在MRC任务中胜过GPT-4等领先的LLMs，而不会明显增加计算成本。

    arXiv:2403.04771v1 Announce Type: new  Abstract: To address the challenges of out-of-control generation in generative models for machine reading comprehension (MRC), we introduce the Question-Attended Span Extraction (QASE) module. Integrated during the fine-tuning of pre-trained generative language models (PLMs), QASE enables these PLMs to match SOTA extractive methods and outperform leading LLMs like GPT-4 in MRC tasks, without significant increases in computational costs.
    
[^67]: 社交取向：对话分析的新特征

    Social Orientation: A New Feature for Dialogue Analysis

    [https://arxiv.org/abs/2403.04770](https://arxiv.org/abs/2403.04770)

    感知模型认为社交取向（例如，热情友好、傲慢冷漠）对话参与者可以用来预测和解释社交互动的结果。我们的工作在于系统性地应用社交取向标签来建模对话结果

    

    许多情境下，预测和解释对话的成功或失败是很有用的。心理学的Circumplex理论建模了会话参与者的社交取向（例如，热情友好、傲慢冷漠）可以用来预测和解释社交互动的结果。我们的工作在于系统性地应用社交取向标签来建模对话结果。在本文中，我们介绍了一个新的对话话语数据集，机器标记了社交取向标签。我们展示社交取向标签提高了任务性能，特别是在英文和中文语言基准中的低资源环境。我们还演示了社交取向标签如何帮助解释神经模型中社交互动的结果。根据显示出社交取向标签在对话结果预测任务中实用性的结果，我们发布我们的数据集

    arXiv:2403.04770v1 Announce Type: new  Abstract: There are many settings where it is useful to predict and explain the success or failure of a dialogue. Circumplex theory from psychology models the social orientations (e.g., Warm-Agreeable, Arrogant-Calculating) of conversation participants and can be used to predict and explain the outcome of social interactions. Our work is novel in its systematic application of social orientation tags to modeling conversation outcomes. In this paper, we introduce a new data set of dialogue utterances machine-labeled with social orientation tags. We show that social orientation tags improve task performance, especially in low-resource settings, on both English and Chinese language benchmarks. We also demonstrate how social orientation tags help explain the outcomes of social interactions when used in neural models. Based on these results showing the utility of social orientation tags for dialogue outcome prediction tasks, we release our data sets, co
    
[^68]: 移除GPT4的过滤器

    Removing GPT4's Filter

    [https://arxiv.org/abs/2403.04769](https://arxiv.org/abs/2403.04769)

    提出了一种方法，可以使经过微调的GPT4恢复到没有经过人类反馈强化学习训练的状态，从而移除其在学习期间的所有安全机制

    

    GPT4最初在大量数据集上进行训练，然后使用来自人类反馈的强化学习进行微调，即志愿者提供反馈以教导GPT4不要生成不当内容。本文提出了一种方法来操作已经进行微调的版本，使其恢复到没有经过RLHF（Reinforcement learning from Human Feedback）的行为，有效地移除了模型在RLHF期间学习的所有安全机制。特别是，当GPT4在没有经过RLHF的情况下运行时，它失去了所有抑制力，只需前几个词就可以生成非常不当的内容。

    arXiv:2403.04769v1 Announce Type: cross  Abstract: GPT4 was initially trained on large amounts of data, and then fine-tuned using Reinforcement learning from Human Feedback (RLHF), which is when volunteers give feedback in order to teach GPT4 not to create inappropriate content. In this paper, we present a method to manipulate the fine-tuned version into reverting to pre-RLHF behavior, effectively removing all safety mechanisms that the model learned during RLHF. In particular, when GPT4 acts without RLHF, it loses all inhibition, and can complete very inappropriate content given only the first few words.
    
[^69]: 我们距离智能视觉演绎推理还有多远？

    How Far Are We from Intelligent Visual Deductive Reasoning?

    [https://arxiv.org/abs/2403.04732](https://arxiv.org/abs/2403.04732)

    目前的视觉语言模型在文本推理方面表现出色，但在视觉演绎推理方面仍存在较大差距和盲点。

    

    最近，诸如GPT-4V之类的视觉语言模型（VLM）在各种视觉语言任务上取得了巨大进展。我们深入探讨了基于视觉的演绎推理，这是一个更复杂但不太被探索的领域，并发现了当前领先的VLM中以前未暴露的盲点。具体来说，我们利用瑞文渐进矩阵（RPM）来评估VLM在仅依靠视觉线索进行多跳关系和演绎推理的能力。我们对几种流行的VLM进行了全面评估，采用了标准策略，如上下文学习、自我一致性和思维链（CoT），在三个不同的数据集上进行了评估，包括Mensa智商测试、智商测试和RAVEN。结果表明，尽管LLM在基于文本的推理方面具有令人印象深刻的能力，但我们在视觉演绎推理方面仍有很大的差距。

    arXiv:2403.04732v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated incredible strides on diverse vision language tasks. We dig into vision-based deductive reasoning, a more sophisticated but less explored realm, and find previously unexposed blindspots in the current SOTA VLMs. Specifically, we leverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop relational and deductive reasoning relying solely on visual clues. We perform comprehensive evaluations of several popular VLMs employing standard strategies such as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test, IntelligenceTest, and RAVEN. The results reveal that despite the impressive capabilities of LLMs in text-based reasoning, we are still far from achieving comparable proficiency in visual deductive reasoning. We found that certain standard strategies that are effective
    
[^70]: 大型语言模型能理解多目标口语语言吗？

    Do Large Language Model Understand Multi-Intent Spoken Language ?

    [https://arxiv.org/abs/2403.04481](https://arxiv.org/abs/2403.04481)

    该研究利用大型语言模型进行口语语言多目标理解，提出了改进实体槽和子目标指令的创新技术，并展示了LLMs在多目标SLU模型方面的潜力。

    

    这项研究通过利用大型语言模型（LLMs）进行多目标口语语言理解（SLU）取得了重大进展，提出了一种在SLU环境中利用LLMs生成能力的独特方法。我们的创新技术重新配置了实体槽，专门用于LLMs在多目标SLU环境中的应用，并引入了子目标指令（SII）的概念，增强了对不同领域内复杂多目标交流的解剖和解释。由此产生的数据集，被称为LM-MixATIS和LM-MixSNIPS，是从现有基准中精心制作的。我们的研究表明，LLMs可以匹配并潜在地超越当前最先进的多目标SLU模型的能力。它进一步探讨了LLMs在各种意图配置和数据集比例下的有效性。此外，我们介绍了两个开创性的度量标准，即实体槽准确性（ESA）和Com

    arXiv:2403.04481v1 Announce Type: cross  Abstract: This study marks a significant advancement by harnessing Large Language Models (LLMs) for multi-intent spoken language understanding (SLU), proposing a unique methodology that capitalizes on the generative power of LLMs within an SLU context. Our innovative technique reconfigures entity slots specifically for LLM application in multi-intent SLU environments and introduces the concept of Sub-Intent Instruction (SII), enhancing the dissection and interpretation of intricate, multi-intent communication within varied domains. The resultant datasets, dubbed LM-MixATIS and LM-MixSNIPS, are crafted from pre-existing benchmarks. Our research illustrates that LLMs can match and potentially excel beyond the capabilities of current state-of-the-art multi-intent SLU models. It further explores LLM efficacy across various intent configurations and dataset proportions. Moreover, we introduce two pioneering metrics, Entity Slot Accuracy (ESA) and Com
    
[^71]: Pearl: 一项基于评论驱动的角色知识对话式推荐数据集

    Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset

    [https://arxiv.org/abs/2403.04460](https://arxiv.org/abs/2403.04460)

    Pearl数据集利用了角色和知识增强的大型语言模型，提供了具体用户偏好，领域专业性和更相关的推荐。

    

    arXiv:2403.04460v1 公告类型：新摘要：对话式推荐系统是一个新兴领域，尤其是随着大型语言模型（LLMs）的进步，使得对话输入的多样化推理引起了社区的越来越大的兴趣。尽管取得了进展，但该领域还有许多方面有待探索。目前可用的用于对话式推荐的公共数据集缺乏特定用户偏好和对推荐的解释，从而妨碍了高质量的推荐。为了解决这些挑战，我们提出了一种新颖的对话式推荐数据集，命名为PEARL，与角色和知识增强的LLM模拟器相结合。我们从真实评论中获得详细的角色和知识，并构建了一个超过57k对话的大规模数据集。我们的实验结果表明，PEARL中的话语包括更具体的用户偏好，显示了在目标领域的专业知识，并提供了更相关的推荐。

    arXiv:2403.04460v1 Announce Type: new  Abstract: Conversational recommender system is an emerging area that has garnered an increasing interest in the community, especially with the advancements in large language models (LLMs) that enable diverse reasoning over conversational input. Despite the progress, the field has many aspects left to explore. The currently available public datasets for conversational recommendation lack specific user preferences and explanations for recommendations, hindering high-quality recommendations. To address such challenges, we present a novel conversational recommendation dataset named PEARL, synthesized with persona- and knowledge-augmented LLM simulators. We obtain detailed persona and knowledge from real-world reviews and construct a large-scale dataset with over 57k dialogues. Our experimental results demonstrate that utterances in PEARL include more specific user preferences, show expertise in the target domain, and provide recommendations more relev
    
[^72]: CLLMs: 一致性大型语言模型

    CLLMs: Consistency Large Language Models

    [https://arxiv.org/abs/2403.00835](https://arxiv.org/abs/2403.00835)

    提出了一种新方法，通过精细调整目标LLM实现了对雅各比轨迹上固定点的一致性预测，有效提高了生成速度2.4倍到3.4倍。

    

    并行解码方法，如雅可比解码，显示出有望实现更高效的LLM推断，因为它打破了LLM解码过程的顺序性，并将其转换为可并行化计算。然而，在实践中，与传统的自回归（AR）解码相比，雅可比解码很少能在单个固定点迭代步骤中准确预测多个标记，因此在速度上取得的提升相对较小。为了解决这个问题，我们开发了一种新方法，旨在实现从任何状态快速收敛到雅各比轨迹上的固定点。通过精细调整目标LLM，以便在任何输入状态下一致地预测固定点。大量实验证明了我们方法的有效性，在领域特定和开放域基准测试中显示出生成速度提高了2.4倍到3.4倍，同时保持了生成质量。

    arXiv:2403.00835v1 Announce Type: cross  Abstract: Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\times$ to 3.4$\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.
    
[^73]: LLMBind: 一种统一的模态任务集成框架

    LLMBind: A Unified Modality-Task Integration Framework

    [https://arxiv.org/abs/2402.14891](https://arxiv.org/abs/2402.14891)

    提出了LLMBind，一种统一的模态任务集成框架，通过将大型语言模型和预训练任务模型绑定在一起，实现了多种模态任务的灵活输入和输出组合。

    

    最近对于多模态大型语言模型在处理各种模态任务方面取得了进展，但它们对于复杂的多模态任务的集成能力有限，从而限制了该领域的发展。在这项工作中，我们带头探索并提出了LLMBind，一种用于模态任务集成的统一框架，该框架将大型语言模型和相应的预训练任务模型与任务特定的标记绑定在一起。因此，LLMBind可以以多种图像、文本、视频和音频的组合解释输入并生成输出。具体来说，我们引入了一种专家混合技术，通过不同专家之间的协作实现不同多模态任务的有效学习。此外，我们创建了一个包含40万条指令数据的多任务数据集，解锁了交互式视觉生成和编辑任务的能力。大量实验证明了我们的方法的有效性。

    arXiv:2402.14891v1 Announce Type: cross  Abstract: While recent progress in multimodal large language models tackles various modality tasks, they posses limited integration capabilities for complex multi-modality tasks, consequently constraining the development of the field. In this work, we take the initiative to explore and propose the LLMBind, a unified framework for modality task integration, which binds Large Language Models and corresponding pre-trained task models with task-specific tokens. Consequently, LLMBind can interpret inputs and produce outputs in versatile combinations of image, text, video, and audio. Specifically, we introduce a Mixture-of-Experts technique to enable effective learning for different multimodal tasks through collaboration among diverse experts. Furthermore, we create a multi-task dataset comprising 400k instruction data, which unlocks the ability for interactive visual generation and editing tasks. Extensive experiments show the effectiveness of our fr
    
[^74]: CriticBench：为批判性-正确推理评估LLMs而设计的基准测试

    CriticBench: Benchmarking LLMs for Critique-Correct Reasoning

    [https://arxiv.org/abs/2402.14809](https://arxiv.org/abs/2402.14809)

    CriticBench是一个综合基准测试，旨在评估LLMs在批判和纠正推理方面的能力，发现批判性训练显著提升性能，逻辑任务更易于修正。

    

    大型语言模型（LLMs）批判和完善其推理的能力对于它们在评估、反馈提供和自我改进中的应用至关重要。本文引入了CriticBench，一个旨在评估LLMs在各种任务中批判和纠正其推理能力的综合基准测试。CriticBench包含五个推理领域：数学、常识、符号、编码和算法。它整合了15个数据集，并结合了三个LLM系列的响应。利用CriticBench，我们评估和剖析了17个LLMs在生成、批判和修正推理（即GQC推理）中的表现。我们的研究结果显示：（1）GQC能力呈线性关系，批判性训练显著提升了性能；（2）修正效果在任务上有所不同，以逻辑为导向的任务更容易修正；（3）GQC知识的不一致性。

    arXiv:2402.14809v1 Announce Type: cross  Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsisten
    
[^75]: SaGE：评估大型语言模型的道德一致性

    SaGE: Evaluating Moral Consistency in Large Language Models

    [https://arxiv.org/abs/2402.13709](https://arxiv.org/abs/2402.13709)

    提出SaGE方法，通过语义图熵来衡量大型语言模型道德一致性，构建了MCC语料库。

    

    尽管最近展示出大型语言模型（LLMs）在会话系统中的印象深刻能力，但我们表明即使是最先进的LLMs在生成过程中也存在道德不一致，对其可靠性（以及总体可信赖性）提出了质疑。以往在LLM评估领域的工作侧重于开发地面真实数据，以衡量在特定任务上的准确性。然而，对于道德情景往往缺乏普遍认同答案的情况，模型响应的一致性对于其可靠性变得至关重要。为了解决这一问题，我们提出了一种信息理论度量方法，称为语义图熵（SaGE），基于“经验法则”（RoTs）的概念来衡量模型的道德一致性。RoTs是模型学习到的抽象原则，可有效帮助解释其决策策略。在此基础上，我们构建了道德一致性语料库（MCC），包含50K个道德问题、回答。

    arXiv:2402.13709v1 Announce Type: cross  Abstract: Despite recent advancements showcasing the impressive capabilities of Large Language Models (LLMs) in conversational systems, we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general). Prior works in LLM evaluation focus on developing ground-truth data to measure accuracy on specific tasks. However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability. To address this issue, we propose an information-theoretic measure called Semantic Graph Entropy (SaGE), grounded in the concept of "Rules of Thumb" (RoTs) to measure a model's moral consistency. RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively. To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses
    
[^76]: 对大型语言模型知识蒸馏的调查

    A Survey on Knowledge Distillation of Large Language Models

    [https://arxiv.org/abs/2402.13116](https://arxiv.org/abs/2402.13116)

    本调查深入探讨了大型语言模型知识蒸馏技术的重要性，并阐明了将专有巨头的先进功能转移到开源模型中的关键作用。

    

    本调查对大型语言模型（LLMs）领域中知识蒸馏（KD）技术进行了深入探讨，重点关注KD在将诸如GPT-4之类的专有巨头的复杂能力转移到可访问的开源模型（如LLaMA和Mistral）中起着关键作用。在不断发展的人工智能领域，本项工作阐明了专有和开源LLMs之间的关键差异，展示了KD如何成为第二者赋予第一者先进功能和细致理解的重要媒介。我们的调查围绕算法、技能和垂直化这三个基础支柱精心构建，全面探讨了KD机制、特定认知能力的增强以及它们在不同领域的实际影响。重要的是，调查引导着数据增强（DA）和KD之间错综复杂的相互作用。

    arXiv:2402.13116v1 Announce Type: new  Abstract: This survey presents an in-depth exploration of knowledge distillation (KD) techniques within the realm of Large Language Models (LLMs), spotlighting the pivotal role of KD in transferring sophisticated capabilities from proprietary giants such as GPT-4 to accessible, open-source models like LLaMA and Mistral. Amidst the evolving AI landscape, this work elucidates the critical disparities between proprietary and open-source LLMs, demonstrating how KD serves as an essential conduit for imbuing the latter with the former's advanced functionalities and nuanced understandings. Our survey is meticulously structured around three foundational pillars: algorithm, skill, and verticalization -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, i
    
[^77]: 一种用于词汇语义变化的上下文化词嵌入的系统比较

    A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change

    [https://arxiv.org/abs/2402.12011](https://arxiv.org/abs/2402.12011)

    本文通过在相同条件下评估了最新的词汇语义变化模型和方法，将LSC问题分解为不同级别的任务，并展示了在不同语言的八个基准测试中，APD在GCD方面优于其他方法，XL-LEXEME在WiC、WSI和GCD方面优于其他上下文化模型，并且与GPT-4相当。

    

    上下文化嵌入是建模词汇语义变化（LSC）的首选工具。当前的评估通常专注于称为分级变化检测（GCD）的特定任务。然而，由于它们依赖于不同设置，跨作品的性能比较经常具有误导性。在本文中，我们在相同条件下评估了GCD的最新模型和方法。我们进一步将LSC问题分解为上下文中的单词（WiC）和词义归纳（WSI）任务，并比较这些不同级别的模型。我们在八个可用的LSC基准测试中跨不同语言进行评估，结果表明：（i）APD在GCD方面优于其他方法；（ii）XL-LEXEME在WiC、WSI和GCD方面优于其他上下文化模型，同时与GPT-4相当；（iii）明显需要改进对词义建模以及关注这些意义何时、如何和为何变化的工作。

    arXiv:2402.12011v1 Announce Type: new  Abstract: Contextualized embeddings are the preferred tool for modeling Lexical Semantic Change (LSC). Current evaluations typically focus on a specific task known as Graded Change Detection (GCD). However, performance comparison across work are often misleading due to their reliance on diverse settings. In this paper, we evaluate state-of-the-art models and approaches for GCD under equal conditions. We further break the LSC problem into Word-in-Context (WiC) and Word Sense Induction (WSI) tasks, and compare models across these different levels. Our evaluation is performed across different languages on eight available benchmarks for LSC, and shows that (i) APD outperforms other approaches for GCD; (ii) XL-LEXEME outperforms other contextualized models for WiC, WSI, and GCD, while being comparable to GPT-4; (iii) there is a clear need for improving the modeling of word meanings, as well as focus on how, when, and why these meanings change, rather t
    
[^78]: 使用大型语言模型的结构化实体提取

    Structured Entity Extraction Using Large Language Models

    [https://arxiv.org/abs/2402.04437](https://arxiv.org/abs/2402.04437)

    本论文提出了一种使用大型语言模型的结构化实体提取方法，在此任务上通过引入AESOP度量评估模型性能，并将整个提取任务分解为多个阶段，相较于基准模型取得了更好的效果，为未来结构化实体提取的进一步发展提供了有希望的方向。

    

    近年来，机器学习的最新进展显著影响了信息提取领域，大型语言模型（LLMs）在从非结构化文本中提取结构化信息方面起着关键作用。本文探讨了当前结构化实体提取方法的挑战和限制，并引入了一种新的方法来解决这些问题。我们首先介绍和规范化了结构化实体提取（SEE）任务，然后提出了适用于该任务的近似实体集重叠（AESOP）度量，以适当评估模型在这一任务上的性能。随后，我们提出了一种新模型，通过将整个提取任务分解为多个阶段，利用LLMs的强大功能来提高效果和效率。定量评估和人工并行评估证实了我们的模型优于基准模型，为结构化实体提取领域的未来进展提供了有希望的方向。

    Recent advances in machine learning have significantly impacted the field of information extraction, with Large Language Models (LLMs) playing a pivotal role in extracting structured information from unstructured text. This paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues. We contribute to the field by first introducing and formalizing the task of Structured Entity Extraction (SEE), followed by proposing Approximate Entity Set OverlaP (AESOP) Metric designed to appropriately assess model performance on this task. Later, we propose a new model that harnesses the power of LLMs for enhanced effectiveness and efficiency through decomposing the entire extraction task into multiple stages. Quantitative evaluation and human side-by-side evaluation confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction.
    
[^79]: 在大型语言模型上进行间接提示注入攻击的基准测试和防御

    Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models

    [https://arxiv.org/abs/2312.14197](https://arxiv.org/abs/2312.14197)

    该研究引入了第一个间接提示注入攻击基准测试BIPIA，对大型语言模型在面对此类攻击时的风险进行评估，并分析了攻击成功的原因，从而开发了防御方法。

    

    大型语言模型（LLMs）与外部内容的整合已经实现了LLMs的更新和广泛应用，比如微软Copilot。然而，这种整合也让LLMs面临了间接提示注入攻击的风险，攻击者可以在外部内容中嵌入恶意指令，从而ompromising LLM输出并导致响应偏离用户期望。为了研究这个重要但未被充分探讨的问题，我们引入了第一个间接提示注入攻击基准测试BIPIA，以评估这类攻击的风险。基于评估，我们的工作重点分析了该攻击成功的潜在原因，即LLMs无法区分指令和外部内容以及缺乏意识不执行外部内容内的指令。基于这一分析，我们开发了两种黑盒方法。

    arXiv:2312.14197v2 Announce Type: replace-cross  Abstract: The integration of large language models (LLMs) with external content has enabled more up-to-date and wide-ranging applications of LLMs, such as Microsoft Copilot. However, this integration has also exposed LLMs to the risk of indirect prompt injection attacks, where an attacker can embed malicious instructions within external content, compromising LLM output and causing responses to deviate from user expectations. To investigate this important but underexplored issue, we introduce the first benchmark for indirect prompt injection attacks, named BIPIA, to evaluate the risk of such attacks. Based on the evaluation, our work makes a key analysis of the underlying reason for the success of the attack, namely the inability of LLMs to distinguish between instructions and external content and the absence of LLMs' awareness to not execute instructions within external content. Building upon this analysis, we develop two black-box metho
    
[^80]: LoRAMoE: 通过MoE风格的插件缓解大型语言模型中的世界知识遗忘

    LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin

    [https://arxiv.org/abs/2312.09979](https://arxiv.org/abs/2312.09979)

    LoRAMoE是一个新颖的框架，通过引入低秩适配器和路由器网络，类似于MoE的插件版本，来解决大型语言模型中世界知识遗忘的问题。

    

    监督微调（SFT）是大型语言模型（LLMs）的关键步骤，使它们能够与人类指令对齐，并增强它们在下游任务中的能力。我们发现，指令数据的大规模增加可能会破坏LLMs先前存储的世界知识。为了解决这一挑战，我们提出了LoRAMoE，这是一个引入了多个低秩适配器（LoRA）并通过路由器网络集成它们的创新性框架，类似于专家混合（MoE）的插件版本。

    arXiv:2312.09979v3 Announce Type: replace  Abstract: Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Increasing instruction data substantially is a direct solution to align the model with a broader range of downstream tasks or notably improve its performance on a specific task. However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs. To address this challenge, we propose LoRAMoE, a novelty framework that introduces several low-rank adapters (LoRA) and integrates them by using a router network, like a plugin version of Mixture of Experts (MoE). It freezes the backbone model and forces a portion of LoRAs to focus on leveraging world knowledge to solve downstream tasks, to alleviate world knowledge-edge forgetting. Experimental results show that, as the instruction data increases, LoRAMoE can si
    
[^81]: RLHF-V: 通过细粒度纠正人类反馈实现可信任的MLLMs行为对准

    RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback

    [https://arxiv.org/abs/2312.00849](https://arxiv.org/abs/2312.00849)

    RLHF-V通过细粒度纠正人类反馈的行为对准，提高了MLLM的可信度，使其在多模态理解、推理和互动方面表现出更可靠的行为。

    

    最近，多模态大语言模型（MLLMs）展示了在多模态理解、推理和互动方面的令人印象深刻的能力。然而，现有的MLLMs普遍存在严重的幻觉问题，生成的文本与相关图像不符合事实。这个问题使得现有的MLLMs不可靠，因此在现实世界（尤其是高风险）应用中不切实际。为了解决这一挑战，我们提出了RLHF-V，通过从细粒度纠正的人类反馈中增强MLLM的可信度。具体而言，RLHF-V采集人类偏好，以片段级别对幻觉进行纠正，并在人类反馈上执行密集直接偏好优化。在自动和人类评估的五个基准测试上进行的综合实验表明，RLHF-V能够实现更可信赖的MLLM行为，具有有希望的数据和计算。

    arXiv:2312.00849v2 Announce Type: replace  Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in multimodal understanding, reasoning, and interaction. However, existing MLLMs prevalently suffer from serious hallucination problems, generating text that is not factually grounded in associated images. The problem makes existing MLLMs untrustworthy and thus impractical in real-world (especially high-stakes) applications. To address the challenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior alignment from fine-grained correctional human feedback. Specifically, RLHF-V collects human preference in the form of segment-level corrections on hallucinations, and performs dense direct preference optimization over the human feedback. Comprehensive experiments on five benchmarks in both automatic and human evaluation show that, RLHF-V can enable substantially more trustworthy MLLM behaviors with promising data and computation 
    
[^82]: 评估最佳参考翻译

    Evaluating Optimal Reference Translations

    [https://arxiv.org/abs/2311.16787](https://arxiv.org/abs/2311.16787)

    提出了一种用于创建更可靠的文档级人工参考翻译的方法论，称为“最佳参考翻译”，旨在提高被视为“人工翻译质量”的标准

    

    当前机器翻译系统在高资源语言对上达到的整体翻译质量非常好。标准评估方法不适用也不意图揭示仍然存在的许多翻译错误和质量缺陷。此外，标准参考翻译的质量常常受到质疑，并在几种语言对中，单凭机器翻译已经达到了可比较的质量水平。在这些高资源设置中进一步研究因此变得困难。在这篇文章中，我们提出了一种用于创建更可靠的文档级人工参考翻译的方法论，称为“最佳参考翻译”，旨在提高被视为“人工翻译质量”的标准。我们评估了获得的文档级最佳参考翻译，并与“标准”翻译进行比较，确认了显著的质量提高，并记录了

    arXiv:2311.16787v2 Announce Type: replace  Abstract: The overall translation quality reached by current machine translation (MT) systems for high-resourced language pairs is remarkably good. Standard methods of evaluation are not suitable nor intended to uncover the many translation errors and quality deficiencies that still persist. Furthermore, the quality of standard reference translations is commonly questioned and comparable quality levels have been reached by MT alone in several language pairs. Navigating further research in these high-resource settings is thus difficult. In this article, we propose a methodology for creating more reliable document-level human reference translations, called "optimal reference translations," with the simple aim to raise the bar of what should be deemed "human translation quality." We evaluate the obtained document-level optimal reference translations in comparison with "standard" ones, confirming a significant quality increase and also documenting
    
[^83]: 大脑记录中的语言生成

    Language Generation from Brain Recordings

    [https://arxiv.org/abs/2311.09889](https://arxiv.org/abs/2311.09889)

    提出了一种在大脑记录中直接生成语言的方法，结合了大型语言模型和语义脑解码器，实现了从功能性磁共振成像输入生成与语义内容一致的连贯语言序列。

    

    通过非侵入式脑-计算机接口（BCIs）生成人类语言具有潜力解锁许多应用，如为残疾患者提供服务和改善沟通。然而，目前通过BCIs生成语言仅在分类设置内成功，用于选择带有最可能的皮层语义表示的预生成句子延续候选。受到最近显示大脑与大型计算语言模型之间关联的研究的启发，我们提出了一个生成语言BCI，该BCI利用大型语言模型（LLM）的能力，与语义脑解码器共同生成语言，直接从功能性磁共振成像（fMRI）输入中生成语言。所提出的模型可以生成与感知到的视觉或听觉语言刺激的语义内容一致的语言序列，而无需事先知道任何预定

    arXiv:2311.09889v4 Announce Type: replace  Abstract: Generating human language through non-invasive brain-computer interfaces (BCIs) has the potential to unlock many applications, such as serving disabled patients and improving communication. Currently, however, generating language via BCIs has been previously successful only within a classification setup for selecting pre-generated sentence continuation candidates with the most likely cortical semantic representation. Inspired by recent research that revealed associations between the brain and the large computational language models, we propose a generative language BCI that utilizes the capacity of a large language model (LLM) jointly with a semantic brain decoder to directly generate language from functional magnetic resonance imaging (fMRI) input. The proposed model can generate coherent language sequences aligned with the semantic content of visual or auditory language stimuli perceived, without prior knowledge of any pre-generate
    
[^84]: 具有显式证据推理的少样本关系抽取思维链

    Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction

    [https://arxiv.org/abs/2311.05922](https://arxiv.org/abs/2311.05922)

    提出了一种名为CoT-ER的新方法，使用大语言模型进行少样本关系抽取，实现了链式推理思维链与显式证据推理。

    

    少样本关系抽取涉及在文本中识别两个特定实体之间的关系类型，使用有限数量的标注样本。通过应用元学习和神经图技术，已经出现了各种解决该问题的方案，这些方案通常需要进行适应性训练。最近，上下文学习策略已经在没有训练的情况下展示出显著结果。一些研究已经利用了上下文学习进行零样本信息提取。不幸的是，在构建思维链提示时，推理的证据要么未考虑，要么在隐式模型化。在本文中，我们提出了一种使用大语言模型的少样本关系抽取的新方法，称为CoT-ER，即具有显式证据推理的思维链。

    arXiv:2311.05922v3 Announce Type: replace  Abstract: Few-shot relation extraction involves identifying the type of relationship between two specific entities within a text, using a limited number of annotated samples. A variety of solutions to this problem have emerged by applying meta-learning and neural graph techniques which typically necessitate a training process for adaptation. Recently, the strategy of in-context learning has been demonstrating notable results without the need of training. Few studies have already utilized in-context learning for zero-shot information extraction. Unfortunately, the evidence for inference is either not considered or implicitly modeled during the construction of chain-of-thought prompts. In this paper, we propose a novel approach for few-shot relation extraction using large language models, named CoT-ER, chain-of-thought with explicit evidence reasoning. In particular, CoT-ER first induces large language models to generate evidences using task-spe
    
[^85]: LLM能遵守简单规则吗?

    Can LLMs Follow Simple Rules?

    [https://arxiv.org/abs/2311.04235](https://arxiv.org/abs/2311.04235)

    提出了一个名为RuLES的程序框架，用于衡量LLMs在与用户交互时遵守规则的能力。

    

    随着大型语言模型（LLMs）在现实世界中承担越来越多的责任，能够以可靠的方式指定和约束这些系统的行为变得至关重要。我们提出了规则遵循语言评估场景（RuLES），这是一个测量LLMs遵循规则能力的程序框架，包括14个简单的文本场景，模型在与用户交互时被指示遵守各种规则。

    arXiv:2311.04235v2 Announce Type: replace  Abstract: As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as "do not generate abusive content", but these may be circumvented by jailbreaking techniques. Existing evaluations of adversarial attacks and defenses on LLMs generally require either expensive manual review or unreliable heuristic checks. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 14 simple text scenarios in which the model is instructed to obey various rules while interacting with the user. Each scenario has a programmatic evaluation function to determine whether the model has broken any rules in a conversation. Our evaluations of proprietar
    
[^86]: 通过统一评估和分析改进基于概率的提示选择

    Improving Probability-based Prompt Selection Through Unified Evaluation and Analysis

    [https://arxiv.org/abs/2305.14877](https://arxiv.org/abs/2305.14877)

    通过统一框架解释和评估现有的基于概率的提示选择方法，开发多种互信息的组合变体，将Oracle提示选择方法的有效性提高到94.98%。

    

    大型语言模型领域中的先前工作引入了不同的基于概率的无梯度提示选择方法，旨在为给定任务选择最佳提示候选，但未能在彼此之间提供全面公正的比较。本文提出了一个统一框架，通过在13个常见和多样化的NLP任务上进行广泛实验，解释和评估现有的基于概率的提示选择方法。我们发现每种现有方法都可以被解释为一种最大化输入和预测输出之间互信息的方法的某种变体。利用这一发现，我们开发了几种其他互信息的组合变体，并将Oracle提示选择方法的有效性从87.79%提高到94.98%，以选择提示的性能与最佳Oracle提示的性能之比来衡量。

    arXiv:2305.14877v2 Announce Type: replace  Abstract: Previous works in prompt engineering for large language models have introduced different gradient-free probability-based prompt selection methods that aim to choose the optimal prompt among the candidates for a given task but have failed to provide a comprehensive and fair comparison between each other. In this paper, we propose a unified framework to interpret and evaluate the existing probability-based prompt selection methods by performing extensive experiments on 13 common and diverse NLP tasks. We find that each of the existing methods can be interpreted as some variant of the method that maximizes mutual information between the input and the predicted output (MI). Utilizing this finding, we develop several other combinatorial variants of MI and increase the effectiveness of the oracle prompt selection method from 87.79% to 94.98%, measured as the ratio of the performance of the selected prompt to that of the optimal oracle prom
    
[^87]: RomanSetu: 通过罗马化有效地利用大语言模型的多语言能力

    RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization. (arXiv:2401.14280v1 [cs.CL])

    [http://arxiv.org/abs/2401.14280](http://arxiv.org/abs/2401.14280)

    本研究提出了一种创新的方法，通过使用罗马化形式的文本作为接口，有效地利用大语言模型的多语言能力。通过在印地语上的实验证明，罗马化文本不仅提高了推理效率，还在有限的预训练下实现了有竞争力的性能。这些发现表明罗马化有潜力弥合大语言模型应用中的语言障碍。

    

    本研究解决了将大型语言模型扩展到非英语语言（特别是使用非拉丁字母表的语言）的挑战。我们提出了一种创新的方法，利用罗马化形式的文本作为大语言模型的接口，假设频繁的非正式使用和与英语共享的标记有助于跨语言对齐。我们以印地语为重点，通过印地语到英语的翻译和情感分析任务，证明罗马化文本不仅由于其较低的生产力而显著改善了推理效率，还在有限的预训练中实现了有竞争力的性能。此外，我们的新颖的多脚本提示方法结合了罗马化和原生文本，在进一步提高任务性能方面显示出潜力。这些发现表明罗马化在弥合大语言模型应用中的语言障碍方面具有潜力，未来的工作将致力于将此方法扩展到更多的语言和任务。

    This study addresses the challenge of extending Large Language Models (LLMs) to non-English languages, specifically those using non-Latin scripts. We propose an innovative approach that utilizes the romanized form of text as an interface for LLMs, hypothesizing that its frequent informal use and shared tokens with English enhance cross-lingual alignment. Focusing on Hindi, we demonstrate through Hindi-to-English translation and sentiment analysis tasks that romanized text not only significantly improves inference efficiency due to its lower fertility compared to native text but also achieves competitive performance with limited pre-training. Additionally, our novel multi-script prompting approach, which combines romanized and native texts, shows promise in further enhancing task performance. These findings suggest the potential of romanization in bridging the language gap for LLM applications, with future work aimed at expanding this approach to more languages and tasks.
    
[^88]: (聊天)GPT v BERT: 语义变化检测之黎明的正义。(arXiv:2401.14040v1 [cs.CL])

    (Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection. (arXiv:2401.14040v1 [cs.CL])

    [http://arxiv.org/abs/2401.14040](http://arxiv.org/abs/2401.14040)

    本研究探讨了(Chat)GPT和BERT在语义变化检测任务中的性能，结果表明(Chat)GPT的表现明显低于BERT，尤其在长期变化检测方面表现更差。

    

    在自然语言处理领域，基于Transformer的语言模型，如BERT和(Chat)GPT，作为具有解决开放性研究问题的巨大能力的词汇超级英雄而出现。本文特别关注语义变化的时间性问题，并评估它们解决Word-in-Context (WiC)任务的两个历时性扩展：TempoWiC和HistoWiC。特别是，我们研究了ChatGPT（和GPT）3.5这样的新型即用技术与当前作为建模语义变化的最先进模型家族BERT之间的潜力。我们的实验是首次尝试使用(Chat)GPT研究语义变化。我们的结果表明，ChatGPT的性能显著低于基础GPT版本。此外，我们的结果表明，(Chat)GPT在检测长期变化方面的表现略低于BERT，但在短期变化检测方面表现明显更差。

    In the universe of Natural Language Processing, Transformer-based language models like BERT and (Chat)GPT have emerged as lexical superheroes with great power to solve open research problems. In this paper, we specifically focus on the temporal problem of semantic change, and evaluate their ability to solve two diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and HistoWiC. In particular, we investigate the potential of a novel, off-the-shelf technology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a family of models that currently stand as the state-of-the-art for modeling semantic change. Our experiments represent the first attempt to assess the use of (Chat)GPT for studying semantic change. Our results indicate that ChatGPT performs significantly worse than the foundational GPT version. Furthermore, our results demonstrate that (Chat)GPT achieves slightly lower performance than BERT in detecting long-term changes but performs significantly worse in de
    
[^89]: 通过数据增强和异构对话图网络提高对话中的人格识别能力

    Enhancing Personality Recognition in Dialogue by Data Augmentation and Heterogeneous Conversational Graph Networks. (arXiv:2401.05871v1 [cs.CL])

    [http://arxiv.org/abs/2401.05871](http://arxiv.org/abs/2401.05871)

    该论文提出了通过数据增强和异构对话图网络提高对话中的人格识别能力的方法，并证明了其在现有基线模型上取得了显著的改进。

    

    人格识别对于增强机器人的个性化回应能力非常有用，从而促进丰富的人机交互。这一任务的一个挑战是现有对话语料库中演讲者数量有限，这阻碍了健壮的、与演讲者无关的人格识别模型的发展。此外，在对话中准确建模对话参与者之间的相互依赖和发言者内部依赖仍然是一个重要问题。为了解决第一个挑战，我们引入了人格特征插值来进行演讲者数据增强。对于第二个挑战，我们提出了异构对话图网络，可以独立地捕捉上下文影响和内在人格特征。在RealPersonaChat语料库上的评估结果表明，我们的方法相比现有基线模型具有显著的改进。

    Personality recognition is useful for enhancing robots' ability to tailor user-adaptive responses, thus fostering rich human-robot interactions. One of the challenges in this task is a limited number of speakers in existing dialogue corpora, which hampers the development of robust, speaker-independent personality recognition models. Additionally, accurately modeling both the interdependencies among interlocutors and the intra-dependencies within the speaker in dialogues remains a significant issue. To address the first challenge, we introduce personality trait interpolation for speaker data augmentation. For the second, we propose heterogeneous conversational graph networks to independently capture both contextual influences and inherent personality traits. Evaluations on the RealPersonaChat corpus demonstrate our method's significant improvements over existing baselines.
    
[^90]: DepWiGNN：一种用于多跳空间推理的深度图神经网络

    DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial Reasoning in Text. (arXiv:2310.12557v1 [cs.CL])

    [http://arxiv.org/abs/2310.12557](http://arxiv.org/abs/2310.12557)

    DepWiGNN是一种用于多跳空间推理的深度图神经网络。它通过设计新颖的节点记忆方案，并在图的深度维度上聚合信息，从而能够收集长时间的依赖关系，而无需堆叠多个层次。实验结果表明，DepWiGNN在两个挑战数据集上比传统GNN方法具有更高的准确性。

    

    文本中的空间推理在各种实际应用中起着至关重要的作用。现有的空间推理方法通常从纯文本中推断空间关系，忽视了自然语言与符号结构之间的差距。图神经网络（GNN）在引导和聚合符号结构方面表现出了卓越的能力。然而，传统的GNN在处理多跳空间推理时面临着挑战，由于过度平滑的问题，即随着图层数量的增加，性能显著下降。为了应对这些挑战，我们提出了一种新颖的Depth-Wise Graph Neural Network（DepWiGNN）。具体地，我们设计了一种新颖的节点记忆方案，并在图的深度维度上聚合信息，而不是在广度维度上，这样可以收集长时间的依赖关系，而无需堆叠多个层次。实验结果表明，在两个挑战数据集上，DepWiGNN可以以比传统GNN方法更高的准确性进行多跳空间推理。

    Spatial reasoning in text plays a crucial role in various real-world applications. Existing approaches for spatial reasoning typically infer spatial relations from pure text, which overlook the gap between natural language and symbolic structures. Graph neural networks (GNNs) have showcased exceptional proficiency in inducing and aggregating symbolic structures. However, classical GNNs face challenges in handling multi-hop spatial reasoning due to the over-smoothing issue, \textit{i.e.}, the performance decreases substantially as the number of graph layers increases. To cope with these challenges, we propose a novel \textbf{Dep}th-\textbf{Wi}se \textbf{G}raph \textbf{N}eural \textbf{N}etwork (\textbf{DepWiGNN}). Specifically, we design a novel node memory scheme and aggregate the information over the depth dimension instead of the breadth dimension of the graph, which empowers the ability to collect long dependencies without stacking multiple layers. Experimental results on two challen
    
[^91]: 一种用于大型语言模型的一次敏感度感知混合稀疏化剪枝方法

    One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models. (arXiv:2310.09499v1 [cs.CL])

    [http://arxiv.org/abs/2310.09499](http://arxiv.org/abs/2310.09499)

    我们提出了一种基于敏感度感知混合稀疏化剪枝的方法，可以在不重新训练的情况下将大型语言模型剪枝至至少50％的稀疏性，同时保持稀疏性水平和减少剪枝引起的误差。此外，该方法还与量化兼容，可以进一步压缩语言模型。

    

    从生成预训练变压器（GPT）系列中的各种大型语言模型（LLMs）在各种文本生成任务中取得了卓越的性能。然而，由于高推理延迟，巨大的模型大小阻碍了它们在实际应用中的实用性。因此，通过量化、剪枝和其他方法提高LLMs的效率成为LLM研究的一个关键问题。在这项工作中，我们提出了一种基于Hessian敏感度感知混合稀疏化剪枝的方法，可以将LLMs剪枝至至少50%的稀疏性，而无需重新训练。它根据敏感度自适应地分配稀疏性，使我们能够降低剪枝引起的误差，同时保持整体稀疏性水平。当稀疏度非常高时，所提出的方法的优势更加明显。此外，我们的方法与量化兼容，可以进一步压缩LLMs。

    Various Large Language Models(LLMs) from the Generative Pretrained Transformer~(GPT) family have achieved outstanding performances in a wide range of text generation tasks. However, the enormous model sizes have hindered their practical use in real-world applications due to high inference latency. Therefore, improving the efficiencies of LLMs through quantization, pruning, and other means has been a key issue in LLM studies. In this work, we propose a method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs to at least 50\% sparsity without the need of any retraining. It allocates sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced error while maintaining the overall sparsity level. The advantages of the proposed method exhibit even more when the sparsity is extremely high. Furthermore, our method is compatible with quantization, enabling further compression of LLMs.
    
[^92]: 艺术还是技巧？大型语言模型与创造力的虚假承诺

    Art or Artifice? Large Language Models and the False Promise of Creativity. (arXiv:2309.14556v1 [cs.CL])

    [http://arxiv.org/abs/2309.14556](http://arxiv.org/abs/2309.14556)

    本研究通过提出创造性写作的托兰斯测验(TTCW)来评估大型语言模型(LLMs)的写作创造力。结果表明，LLM生成的故事在创意测试中通过的数量比专业作家写的故事少。此外，我们发现LLMs无法代替专家进行TTCW评估。

    

    研究人员认为，大型语言模型(LLMs)具有从博客到故事的高质量写作能力。然而，客观评估一段文字的创造力是具有挑战性的。受创造性思维的托兰斯测验(TTC)的启发，我们使用共识评估技术[3]，提出了创造性写作的托兰斯测验(TTCW)来评估创造力作为一个产品。TTCW由包含在流畅度、灵活性、独创性和细致度原始维度中的14个二元测试组成。我们招募了10位创意作家，并使用TTCW对48个由专业作家或LLMs撰写的故事进行人工评估。我们的分析表明，LLM生成的故事通过的TTCW测试比专业作家写的故事少了3-10倍。此外，我们探索了使用LLMs作为评价者，以自动化TTCW评估，结果显示没有一个LLM与专家评估呈正相关。

    Researchers have argued that large language models (LLMs) exhibit high-quality writing capabilities from blogs to stories. However, evaluating objectively the creativity of a piece of writing is challenging. Inspired by the Torrance Test of Creative Thinking (TTCT), which measures creativity as a process, we use the Consensual Assessment Technique [3] and propose the Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product. TTCW consists of 14 binary tests organized into the original dimensions of Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative writers and implement a human assessment of 48 stories written either by professional authors or LLMs using TTCW. Our analysis shows that LLM-generated stories pass 3-10X less TTCW tests than stories written by professionals. In addition, we explore the use of LLMs as assessors to automate the TTCW evaluation, revealing that none of the LLMs positively correlate with the expert assessments.
    
[^93]: LongLoRA: 高效的长上下文大型语言模型的精细调整

    LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. (arXiv:2309.12307v1 [cs.CL])

    [http://arxiv.org/abs/2309.12307](http://arxiv.org/abs/2309.12307)

    LongLoRA是一种高效的精细调整方法，可以在有限的计算成本下扩展预训练的大型语言模型的上下文大小。它通过稀疏的局部注意力实现精细调整，并使用移动短注意力有效实现上下文扩展，与传统方法具有相似的性能。

    

    我们提出了一种高效的精细调整方法——LongLoRA，可以在有限的计算成本下扩展预训练的大型语言模型(LLM)的上下文大小。通常，使用长上下文大小训练LLM的计算成本很高，需要大量的训练时间和GPU资源。本文中，我们在两个方面加快了LLM的上下文扩展。一方面，尽管推理过程中需要稠密的全局注意力，但模型的精细调整可以通过稀疏的局部注意力有效且高效地完成。所提出的移动短注意力有效地实现了上下文的扩展，在与使用传统注意力进行精细调整时具有相似的性能，同时可以在训练中只用两行代码实现，在推理中是可选的。另一方面，我们重新审视了参数效率问题。

    We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shift short attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-effici
    
[^94]: M3PS：电子商务中全面的多粒度多模态属性感知产品摘要

    M3PS: End-to-End Multi-Grained Multi-Modal Attribute-Aware Product Summarization in E-commerce. (arXiv:2308.11351v1 [cs.MM])

    [http://arxiv.org/abs/2308.11351](http://arxiv.org/abs/2308.11351)

    M3PS是一种全面的多粒度多模态属性感知产品摘要方法，能够同时建模并生成高质量的产品摘要，解决了电子商务中产品摘要的端到端建模、多粒度多模态建模和多模态属性建模的问题。

    

    多模态产品摘要（MMPS）旨在通过突出产品特点的短文本摘要来吸引客户的兴趣并增加其购买欲望。现有的MMPS方法已经取得了令人满意的性能。然而，仍然存在几个问题：1）缺乏端到端的产品摘要，2）缺乏多粒度多模态建模，以及3）缺乏多模态属性建模。为了解决这些问题，我们提出了一种用于在电子商务中生成高质量产品摘要的端到端多粒度多模态属性感知产品摘要方法（M3PS）。M3PS同时对产品属性进行建模并生成产品摘要。同时，我们设计了几个多粒度多模态任务，以更好地指导M3PS的多模态学习。此外，我们基于文本和图像模态对产品属性进行建模，以使多模态产品特性能够得到体现。

    Given the long textual product information and the product image, Multi-Modal Product Summarization (MMPS) aims to attract customers' interest and increase their desire to purchase by highlighting product characteristics with a short textual summary. Existing MMPS methods have achieved promising performance. Nevertheless, there still exist several problems: 1) lack end-to-end product summarization, 2) lack multi-grained multi-modal modeling, and 3) lack multi-modal attribute modeling. To address these issues, we propose an end-to-end multi-grained multi-modal attribute-aware product summarization method (M3PS) for generating high-quality product summaries in e-commerce. M3PS jointly models product attributes and generates product summaries. Meanwhile, we design several multi-grained multi-modal tasks to better guide the multi-modal learning of M3PS. Furthermore, we model product attributes based on both text and image modalities so that multi-modal product characteristics can be manife
    
[^95]: 通过掩码结构成长实现2倍语言模型预训练加速

    2x Faster Language Model Pre-training via Masked Structural Growth. (arXiv:2305.02869v1 [cs.CL])

    [http://arxiv.org/abs/2305.02869](http://arxiv.org/abs/2305.02869)

    本文提出了掩码结构成长（MSG），可以加速语言模型的预训练，其中包括全维度成长进程和独立于新权重初始化的函数严格保留成长操作。

    

    在当今自然语言处理研究中，加速大型语言模型预训练是一个关键问题。本文旨在通过从小型Transformer结构逐步扩展到大型结构，加快预训练进程。这种渐进式成长的主要研究问题有两个，即成长进程和成长操作。对于成长进程，现有研究已经探索了深度和前馈层的多阶段扩展，但每个维度对进程效率的影响仍然是一个未解决的问题。而对于成长操作，现有研究依赖于新权重的初始化来继承原有的知识，只实现了非严格的函数保留，从而限制了进一步的训练动态优化。为解决这些问题，本文提出了掩码结构成长（MSG），其中包括涉及所有可能维度的成长进程和独立于新权重初始化的函数严格保留成长操作。实验证明，MSG可显著加速语言模型预训练。

    Acceleration of large language model pre-training is a critical issue in present NLP research. In this paper, we focus on speeding up pre-training by progressively growing from a small Transformer structure to a large one. There are two main research problems related to progressive growth: growth schedule and growth operator. For growth schedule, existing work has explored multi-stage expansion of depth and feedforward layers. However, the impact of each dimension on the schedule's efficiency is still an open question. For growth operator, existing work relies on the initialization of new weights to inherit knowledge, and achieve only non-strict function preservation, limiting further optimization of training dynamics. To address these issues, we propose Masked Structural Growth (MSG), including growth schedules involving all possible dimensions and strictly function-preserving growth operators that is independent of the initialization of new weights. Experiments show that MSG is signi
    
[^96]: 大象的透视镜：调查谷歌、ChatGPT、维基百科和YouTube上的语言偏见

    A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube. (arXiv:2303.16281v1 [cs.CY])

    [http://arxiv.org/abs/2303.16281](http://arxiv.org/abs/2303.16281)

    研究发现在Google、ChatGPT、维基百科和YouTube上，搜索结果受限于语言，反映了与复杂主题相关的文化刻板印象，缺乏跨文化视角。

    

    与谷歌搜索“从多个角度获取信息，以便你可以形成自己对世界的理解”的任务相反，我们发现谷歌及其最突出的搜索结果 - 维基百科和YouTube，仅反映与“佛教”、“自由主义”、“殖民化”、“伊朗”和“美国”等复杂主题相关的文化刻板印象。简单地说，在不同语言的相同搜索中，它们以不同程度呈现不同的信息（我们称之为“语言偏见”），而不是呈现复杂主题的全球图片。我们的在线搜索使我们成为谚语中的盲人，仅触摸小象的一小部分，不知道其他文化的视角的存在。我们用于搜索的语言最终成为促进本族中心主义观点的文化过滤器，其中一个人根据自己的文化评估其他人或思想。我们还发现ChatGPT中深深嵌入了语言偏见。

    Contrary to Google Search's mission of delivering information from "many angles so you can form your own understanding of the world," we find that Google and its most prominent returned results -- Wikipedia and YouTube, simply reflect the narrow set of cultural stereotypes tied to the search language for complex topics like "Buddhism," "Liberalism," "colonization," "Iran" and "America." Simply stated, they present, to varying degrees, distinct information across the same search in different languages (we call it 'language bias'). Instead of presenting a global picture of a complex topic, our online searches turn us into the proverbial blind person touching a small portion of an elephant, ignorant of the existence of other cultural perspectives. The language we use to search ends up as a cultural filter to promote ethnocentric views, where a person evaluates other people or ideas based on their own culture. We also find that language bias is deeply embedded in ChatGPT. As it is primaril
    

