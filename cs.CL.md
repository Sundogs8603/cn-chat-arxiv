# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [End-to-End Single-Channel Speaker-Turn Aware Conversational Speech Translation.](http://arxiv.org/abs/2311.00697) | 本文介绍了一种名为Speaker-Turn Aware Conversational Speech Translation的端到端和多任务训练模型，通过结合自动语音识别、语音翻译和说话者转换检测，实现了对单通道多说话者对话的语音翻译。在实验中，该模型在多说话者条件下优于传统系统，并取得了可比较的性能。 |
| [^2] | [Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving.](http://arxiv.org/abs/2311.00694) | 本论文将大型语言模型（LLMs）作为层次策略，通过释放其创造潜力，探索多样化的问题解决策略。通过将LLMs分为领导者和执行者，领导者提供多种高级问题解决策略作为提示，执行者根据领导者的指引执行详细的问题解决过程，生成一组解决方案。 |
| [^3] | [Improving Interpersonal Communication by Simulating Audiences with Language Models.](http://arxiv.org/abs/2311.00687) | 本论文提出了一个基于大型语言模型（LLM）模拟的框架，通过探索解决方案空间、生成沟通候选以及模拟受众反应，来改善人际沟通。通过评估八个涵盖人际沟通基本过程的场景，展示了该框架的有效性。 |
| [^4] | [Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task.](http://arxiv.org/abs/2311.00686) | 本文通过对Eval4NLP 2023共享任务的研究，探索了小型LLM作为摘要评估度量的潜力，在使用多种提示技术和学习方法的基础上，揭示了结合小型开源模型orca_mini_v3_7B可以取得有竞争力的结果。 |
| [^5] | [Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation.](http://arxiv.org/abs/2311.00684) | 本研究提出了两种通过温度缩放实现的注意力对齐策略，通过改善T5在长序列处理中的注意力分布问题，提高了其在语言建模、检索和多文档问答等任务中的长上下文利用能力。 |
| [^6] | [Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs.](http://arxiv.org/abs/2311.00681) | 大型语言模型作为评估生成模型所产生摘要的事实一致性的可靠评估者的潜力被研究。研究发现在GPT-4和PaLM-2方面事实评估指标与人工评估之间缺乏显著的相关性。 |
| [^7] | [Emotion Detection for Misinformation: A Review.](http://arxiv.org/abs/2311.00671) | 本文综述了基于情感的虚假信息检测方法，首先解释了情感与虚假信息之间的关系，然后对应用不同情感、情绪和立场特征的虚假信息检测方法进行了详细分析，讨论了其中的优势和弱点，最后探讨了情感驱动的虚假信息检测面临的挑战。 |
| [^8] | [Explicit Morphological Knowledge Improves Pre-training of Language Models for Hebrew.](http://arxiv.org/abs/2311.00658) | 加入明确的形态学知识在预训练阶段可以提高希伯来语的语言模型性能。 |
| [^9] | [Formal Translation from Reversing Petri Nets to Coloured Petri Nets.](http://arxiv.org/abs/2311.00629) | 这项研究提出了从逆向Petri网到彩色Petri网的形式化翻译方法，实现了可逆计算的三种主要形式，为低功耗计算和广泛应用领域提供了可能性。 |
| [^10] | [Boosting Summarization with Normalizing Flows and Aggressive Training.](http://arxiv.org/abs/2311.00588) | 本文提出了FlowSUM，一个基于正则化流的变分编码器-解码器框架，用于改进Transformer-based摘要生成。通过利用正则化流进行灵活的潜在后向建模以及采用受控交替激进训练策略，FlowSUM显著提高了生成摘要的质量，并探讨了正则化流中的后向塌陷问题和相关影响因素，为相关研究提供了宝贵的洞察。 |
| [^11] | [Crosslingual Retrieval Augmented In-context Learning for Bangla.](http://arxiv.org/abs/2311.00587) | 本文提出了一种跨语言检索增强情境学习的创新方法，通过利用高资源语言中的语义相似提示，成功提升了孟加拉语任务的性能。 |
| [^12] | [LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing.](http://arxiv.org/abs/2311.00571) | LLaVA-Interactive是一个多模态人机交互的研究原型系统，通过视觉提示来对齐人类意图，实现了图像对话、分割、生成和编辑的多模态功能，具有高成本效益和广泛的应用潜力。 |
| [^13] | [An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek.](http://arxiv.org/abs/2311.00541) | 本论文介绍了一个嵌入式历时语义变化模型（EDiSC），结合了词嵌入和DiSC模型，通过无监督学习分析古希腊文本中目标词汇的意义变化。实验证明EDiSC具有优越的性能。 |
| [^14] | [Text Rendering Strategies for Pixel Language Models.](http://arxiv.org/abs/2311.00522) | 本文研究了在像素语言模型中渲染文本的四种方法，并发现简单的字符二元渲染策略在句子级任务上表现出更好的性能，同时不降低令牌级或多语言任务的表现。该策略还使得可以训练一个更紧凑的模型，仅使用22M参数但性能与原始模型相当。 |
| [^15] | [Rule-Based Error Classification for Analyzing Differences in Frequent Errors.](http://arxiv.org/abs/2311.00513) | 本文提出了一种基于规则的错误分类工具，用于分析初学者和专家程序员之间频繁错误的差异，并通过对95,631个代码对进行错误分类，发现初学者的错误主要是由于缺乏编程知识。 |
| [^16] | [Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks.](http://arxiv.org/abs/2311.00508) | 本文通过对抗攻击实验，研究了机器翻译评估指标在对抗性合成文本上的表现，揭示了指标的鲁棒性问题，并发现了BERTScore评级的不一致性，为更鲁棒的指标开发提供了动力。 |
| [^17] | [Efficient LLM Inference on CPUs.](http://arxiv.org/abs/2311.00502) | 本研究提出了一种在CPU上高效部署大型语言模型（LLMs）的方法，支持自动权重量化和优化内核，在流行的LLMs上展示了极高的推理效率。 |
| [^18] | [Comparing Optimization Targets for Contrast-Consistent Search.](http://arxiv.org/abs/2311.00488) | 本研究通过对比优化目标发现，使用Midpoint-Displacement（MD）损失函数可以获得与Contrast-Consistent Search（CCS）非常相似的模型权重，而且通过调整超参数可以使MD损失函数的测试准确率高于CCS。 |
| [^19] | [Style Locality for Controllable Generation with kNN Language Models.](http://arxiv.org/abs/2311.00475) | 本论文提出了一种使用位置级别进行可控生成的新方法，并通过对礼貌、形式、支持性和毒性文本数据的自动和人工评估发现，我们的模型能够成功地控制风格，并提供更好的流利性和风格平衡。 |
| [^20] | [Discourse Relations Classification and Cross-Framework Discourse Relation Classification Through the Lens of Cognitive Dimensions: An Empirical Investigation.](http://arxiv.org/abs/2311.00451) | 通过认知维度，我们可以有效地捕捉和分类不同框架间的论述关系，这为标注和自动分类带来了便利。 |
| [^21] | [A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models.](http://arxiv.org/abs/2311.00445) | 这项研究通过对比人类和语言模型在三段论推理中的表现，发现较大的语言模型更合逻辑，甚至比人类更合逻辑，但即使最大的语言模型也会出现与人类推理类似的错误，总体上认为语言模型在某些情况下能够克服人类偏见。 |
| [^22] | [Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling.](http://arxiv.org/abs/2311.00430) | 本文提出了Distil-Whisper模型，利用大规模伪标签和知识蒸馏技术将预训练语音识别模型Whisper蒸馏成更小的模型。Distil-Whisper模型在速度和参数数量方面有显著改进，并在零-shot转移设置下表现出良好的鲁棒性。 |
| [^23] | [Efficient Human-AI Coordination via Preparatory Language-based Convention.](http://arxiv.org/abs/2311.00416) | 本研究发现，在人工智能与人类协同之前，人类进行交流以建立约定，指定角色和行动，有效地指导协同。基于此观察，提出利用大型语言模型(LLM)来实现高效的人工智能-人类协同。 |
| [^24] | [Enhanced Knowledge Injection for Radiology Report Generation.](http://arxiv.org/abs/2311.00399) | 提出了一种增强的知识注入框架用于生成准确的医学图像报告，通过加权概念知识和多模式检索知识提取关键临床信息，解决了医学图像报告生成中的挑战。 |
| [^25] | [Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition.](http://arxiv.org/abs/2311.00367) | 本文提出了一种基于提示的逻辑语义增强（PLSE）方法来改进隐含篇章关系识别（IDRR），通过无缝地将与篇章关系相关的知识注入到预训练语言模型中，从而提高IDRR的性能和稳健性。 |
| [^26] | [HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning.](http://arxiv.org/abs/2311.00321) | HARE是一个支持逐步推理的可解释性仇恨言论检测框架，利用大型语言模型填补现有注释方案的推理差距，从而提高了检测模型的监督效果。 |
| [^27] | [Data Augmentation for Code Translation with Comparable Corpora and Multiple References.](http://arxiv.org/abs/2311.00317) | 该论文介绍了两种数据增强方法来改善编程语言之间的代码翻译。通过构建可比较的语料库和增加多个参考翻译，实验结果表明这些方法显著提高了CodeT5在Java、Python和C++之间的翻译准确性。 |
| [^28] | [Unsupervised Lexical Simplification with Context Augmentation.](http://arxiv.org/abs/2311.00310) | 我们提出了一种无监督的词汇简化方法，使用单语数据和预训练语言模型，通过生成替代词来简化给定目标词与其上下文，实验证明我们的模型在多个语言上优于其他无监督系统，并在SWORDS数据集上取得了最先进的结果。 |
| [^29] | [Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation.](http://arxiv.org/abs/2311.00306) | 该论文提出了一种无需预定义性别短语和刻板印象的条件文本生成机制，通过三种不同策略生成的三种类型的输入来探测LLMs，展示了LLMs中显性和隐性性别偏见的证据。 |
| [^30] | [Detecting Syllable-Level Pronunciation Stress with A Self-Attention Model.](http://arxiv.org/abs/2311.00301) | 本文提出了一个自注意力模型，用于检测英语音节的重音水平。通过引入各种韵律和分类特征，该模型达到了一定准确度，并展示了在不同场景下的应用潜力。 |
| [^31] | [Entity Alignment Method of Science and Technology Patent based on Graph Convolution Network and Information Fusion.](http://arxiv.org/abs/2311.00300) | 本研究提出了一种使用图卷积网络和信息融合的方法来实现科技专利的实体对齐，通过将知识图的结构信息和实体属性信息嵌入和表示，从而提高实体对齐的性能。 |
| [^32] | [Semantic Representation Learning of Scientific Literature based on Adaptive Feature and Graph Neural Network.](http://arxiv.org/abs/2311.00296) | 提出了一种基于自适应特征和图神经网络的科学文献语义表示学习方法，通过全局和局部考虑科学文献特征，并使用图注意机制对具有引用关系的特征进行加权求和，以更好地表达不同科学文献特征之间的关联。 |
| [^33] | [IBADR: an Iterative Bias-Aware Dataset Refinement Framework for Debiasing NLU models.](http://arxiv.org/abs/2311.00292) | 本文提出了一种迭代偏见感知的数据集细化框架IBADR，用于去偏自然语言理解（NLU）模型。该框架可以在不预定义有偏特征的情况下去偏模型，通过迭代扩展样本池并使用样本生成器生成伪样本，从而有效地去除有偏特征。 |
| [^34] | [Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks.](http://arxiv.org/abs/2311.00288) | 本文提出了基于提示不确定性的主动指令调优方法，通过选择信息丰富的任务并主动调整模型，提高了跨任务泛化能力。 |
| [^35] | [Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models.](http://arxiv.org/abs/2311.00287) | 本文提出了一种通过大型语言模型进行临床文本生成的创新方法ClinGen，该方法将外部领域特定的知识和语言模型结合起来，提高了临床自然语言处理任务的性能，并丰富了样本的多样性。 |
| [^36] | [JADE: A Linguistic-based Safety Evaluation Platform for LLM.](http://arxiv.org/abs/2311.00286) | JADE是一种基于语言分析的LLM安全评估平台，能够破坏广泛使用的中文和英文LLM，并生成高度威胁的不安全问题。 |
| [^37] | [SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations.](http://arxiv.org/abs/2311.00273) | 本研究通过在心理咨询领域构建多轮共情对话数据集，并利用更接近心理咨询师表达方式的对话历史和回应进行微调，成功提高了LLMs的共情能力。 |
| [^38] | [Syntactic Inductive Bias in Transformer Language Models: Especially Helpful for Low-Resource Languages?.](http://arxiv.org/abs/2311.00268) | Transformer语言模型的句法归纳偏好用于增强预训练过程，但在低资源语言中效果不佳。 |
| [^39] | [Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents.](http://arxiv.org/abs/2311.00262) | 基于大型语言模型的对话代理的即插即用策略规划器(PDDPP)引入了一种新的对话策略规划范式，通过可调整的语言模型插件实现主动对话问题的策略制定。利用监督微调和强化学习，该框架在处理新的案例时具有较高的灵活性和性能。 |
| [^40] | [Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis.](http://arxiv.org/abs/2311.00258) | 本研究通过领域不可知的扰动测试了大型语言模型在多跳推理任务中的鲁棒性，发现模型对某些扰动更敏感，并证明增加扰动样本的比例可以提高少样本提示方法的鲁棒性。 |
| [^41] | [The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities.](http://arxiv.org/abs/2311.00237) | 该论文对LLMs的新兴能力的解释和分析进行了全面调查，旨在理解这些能力的机制和实际应用，并解决可能出现的潜在风险和担忧。 |
| [^42] | [Distort, Distract, Decode: Instruction-Tuned Model Can Refine its Response from Noisy Instructions.](http://arxiv.org/abs/2311.00233) | 本文提出了一种简单而有效的方法，通过对比性调整预测，利用嘈杂指令来增强指令调节模型的效果，从而改进了面临超出训练范围的指令时的响应准确性。 |
| [^43] | [Is GPT Powerful Enough to Analyze the Emotions of Memes?.](http://arxiv.org/abs/2311.00223) | 该论文研究了GPT-3.5在处理互联网表情包情感分析方面的能力，包括情感分类、幽默类型确定和憎恶表情包检测，揭示了其优势和潜在限制。 |
| [^44] | [Transformers as Recognizers of Formal Languages: A Survey on Expressivity.](http://arxiv.org/abs/2311.00208) | 本文对transformers在形式语言识别领域的相关研究进行了全面调查，为理解其表达能力提供了一个统一的框架。 |
| [^45] | [Continuous Training and Fine-tuning for Domain-Specific Language Models in Medical Question Answering.](http://arxiv.org/abs/2311.00204) | 本研究提出了一种连续训练和微调的方法，通过在中文医学数据上进行训练和微调，快速适应Llama 2基础模型到中文医学领域。实验结果表明，这种方法有效，生成的模型与GPT-3.5-turbo相媲美，但使用的计算资源更少。这为在不同领域进行大型语言模型的领域特定训练提供了一个模板。 |
| [^46] | [XAI-CLASS: Explanation-Enhanced Text Classification with Extremely Weak Supervision.](http://arxiv.org/abs/2311.00189) | XAI-CLASS是一种解释增强的极弱监督文本分类方法，通过在训练过程中结合生成的伪标签的解释和单词的显著性，实现了在最小或没有人工注释的情况下进行文本分类。 |
| [^47] | [ChipNeMo: Domain-Adapted LLMs for Chip Design.](http://arxiv.org/abs/2311.00176) | ChipNeMo通过领域自适应技术，实现了在工业芯片设计中大幅提升LLM性能，同时减小了模型尺寸，在工程助手、脚本生成和缺陷分析等方面具有良好表现。 |
| [^48] | [Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield.](http://arxiv.org/abs/2311.00172) | 本研究介绍了对抗性提示屏蔽（APS）模型以及自动生成对抗性训练数据集的新策略。APS模型在检测准确性方面表现出色且具有韧性，并且BAND数据集可以增强安全分类器的鲁棒性。 |
| [^49] | [Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language.](http://arxiv.org/abs/2311.00161) | 本研究通过心理学和哲学文献提出六种挑战恶意语言刻板印象的策略，并通过人类和机器生成的对话数据集对其有效性进行比较。结果显示，人类编写的对话使用更具体的策略，而机器生成的对话使用较为普遍的策略。 |
| [^50] | [Longer Fixations, More Computation: Gaze-Guided Recurrent Neural Networks.](http://arxiv.org/abs/2311.00159) | 本文研究了通过注视来指导语言模型的方法。实验结果表明，模型表现出了类似于人类的注视行为，并且通过注视来引导语言模型，取得了好的性能。 |
| [^51] | [Two-Stage Classifier for Campaign Negativity Detection using Axis Embeddings: A Case Study on Tweets of Political Users during 2021 Presidential Election in Iran.](http://arxiv.org/abs/2311.00143) | 本文提出了一个混合模型，用于检测竞选活动的负面情绪，包括一个两阶段分类器，结合了两个机器学习模型的优势。通过对伊朗2021年总统选举期间50名政治用户的5,100条波斯语推文进行标注，建立了所需的数据集。 |
| [^52] | [On the effect of curriculum learning with developmental data for grammar acquisition.](http://arxiv.org/abs/2311.00128) | 这项研究发现语法习得主要受到对语音数据的暴露驱动，并通过课程学习方法进一步提高其性能。 |
| [^53] | [BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B.](http://arxiv.org/abs/2311.00117) | 研究发现，公开发布模型权重使得安全微调无效，BadLlama项目以低成本成功移除了Llama 2-Chat 13B的安全微调并保留了其一般能力。 |
| [^54] | [BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text.](http://arxiv.org/abs/2311.00116) | BERTwich通过在BERT的编码器堆叠与额外的编码器层之间插入进行噪声文本遮蔽语言建模训练的方式，实现了对方言和噪声文本的建模能力扩展。 |
| [^55] | [The Generative AI Paradox: "What It Can Create, It May Not Understand".](http://arxiv.org/abs/2311.00059) | 生成型AI的悖论研究了生成型模型与人类智能之间的差异，模型在产生专家级输出的能力上可能超过其理解能力。 |
| [^56] | [Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?.](http://arxiv.org/abs/2311.00047) | 通过构建一个包含五种视觉幻觉的数据集，研究发现，尽管整体对齐性较低，但更大规模的视觉-语言模型更接近人类的感知并更容易受到视觉幻觉的影响。 |
| [^57] | [Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations.](http://arxiv.org/abs/2310.20246) | 本文首次探索并训练了强大的多语言数学推理模型，通过使用翻译构建了多语言数据集，并提出了各种训练策略来构建强大的模型。实验证实发现在多语言训练中，将目标语言的翻译与原始语言的表示结合起来以及交替训练和多语言模型的自举可以提高模型的性能。此外，模型在处理低频词和长句子方面仍面临挑战。 |
| [^58] | [Combining Language Models For Specialized Domains: A Colorful Approach.](http://arxiv.org/abs/2310.19708) | 该论文提出了一种将领域特定语言模型与通用语言模型结合的新颖方法，通过对词语进行标记或“上色”来有效处理领域术语，显著降低了领域专用任务的错误率。 |
| [^59] | [InfoEntropy Loss to Mitigate Bias of Learning Difficulties for Generative Language Models.](http://arxiv.org/abs/2310.19531) | 提出了一种信息熵损失函数，用于减少生成式语言模型对常见和易学标记的偏好，使其更关注不常见和难学的标记。 |
| [^60] | [Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs.](http://arxiv.org/abs/2310.19347) | 本文提出了一个名为DECENT的方法，通过对抗解耦LLMs的理解和修饰能力，提高文本摘要的事实一致性。同时，采用了一种探测技术来弥补训练过程中对真与假的敏感性不足的问题。 |
| [^61] | [CXR-LLaVA: Multimodal Large Language Model for Interpreting Chest X-ray Images.](http://arxiv.org/abs/2310.18341) | 本研究开发了一种用于解读胸部X射线图像的开源多模态大型语言模型（CXR-LLaVA），通过预训练图像编码器和对比语言-图像预训练将图像与放射学异常对齐，并使用GPT-4进行微调，实现了问题回答的功能。 |
| [^62] | [Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare.](http://arxiv.org/abs/2310.17956) | Qilin-Med-VL是面向普遍医疗保健的中国大型视觉-语言模型，它结合了预训练的视觉Transformer和基础语言模型，通过两阶段课程训练过程提高了生成医疗标题和回答复杂医疗查询的能力，并发布了一个包含超过100万个图像-文本对的数据集ChiMed-VL。 |
| [^63] | [CodeFusion: A Pre-trained Diffusion Model for Code Generation.](http://arxiv.org/abs/2310.17680) | CodeFusion是一种预训练的代码生成模型，通过扩散的方式解决了自然语言代码生成中遇到的限制，实验表明其在准确率和多样性上优于最先进的自回归系统。 |
| [^64] | [FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language.](http://arxiv.org/abs/2310.17306) | FormaT5是一个基于转换器的模型，可以根据目标表格和自然语言描述生成数据相关的条件格式规则。为了解决描述不足的问题，FormaT5通过放弃目标的方式学习预测占位符。 |
| [^65] | [Machine Translation for Nko: Tools, Corpora and Baseline Results.](http://arxiv.org/abs/2310.15612) | 该论文提出了针对Nko语（一种在多个西非国家使用的语言）开发可用的机器翻译系统的一套工具、资源和基准结果，包括新颖的协作平行文本整理软件、扩展的语料库和基线神经机器翻译结果。 |
| [^66] | [What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations.](http://arxiv.org/abs/2310.15431) | 本研究提出了易推翻的道德推理任务，以了解不同背景下行为的道德可接受性，并提供常识理由来支持推理。通过迭代的自蒸馏方法，我们获得了高质量的任务数据。 |
| [^67] | [3M-TRANSFORMER: A Multi-Stage Multi-Stream Multimodal Transformer for Embodied Turn-Taking Prediction.](http://arxiv.org/abs/2310.14859) | 本文提出了一种用于预测体验式转换的多阶段多流多模态Transformer，通过对同步的多角度自我中心数据进行处理，相较于现有的基准模型和其他基于Transformer的方法，实现了14.01%的性能提升。 |
| [^68] | [Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and Beyond.](http://arxiv.org/abs/2310.14670) | 这项研究提出了一种解决多项选择视觉问答中数据集偏差的方法，包括不平衡匹配偏差和分心相似性偏差，并提出了对抗数据合成和样本内对立训练的技术来应对这些偏差。 |
| [^69] | [Sociotechnical Safety Evaluation of Generative AI Systems.](http://arxiv.org/abs/2310.11986) | 本文提出了一个三层框架，采用社会技术方法对生成型AI系统的安全风险进行评估。同时，评估现状调查发现了三个显著的评估差距，并提出了解决这些差距的方法。 |
| [^70] | ["Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters.](http://arxiv.org/abs/2310.09219) | 本文对LLM生成的推荐信中的性别偏见进行了细致的研究，并设计了评估方法来展现通过语言风格和词汇内容来体现的性别偏见。 |
| [^71] | [Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization.](http://arxiv.org/abs/2310.05506) | 本文调查了在数学推理中使用数据增强的效果，并通过创建新的数据集和微调模型取得了显著成果。 |
| [^72] | [How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition.](http://arxiv.org/abs/2310.05492) | 本研究探讨了大规模语言模型在监督微调过程中，特别是数学推理和代码生成能力方面，数据组合的影响。实验结果显示，较大模型在相同数据量下表现出更好的性能，通过增加微调数据和模型参数，数学推理和代码生成能力得到显著提升。 |
| [^73] | [Automatic Anonymization of Swiss Federal Supreme Court Rulings.](http://arxiv.org/abs/2310.04632) | 该论文介绍了瑞士联邦最高法院裁决的自动化匿名化方法，通过利用大型数据集和领域内预训练模型，结果表明相比现有模型，使用领域内数据进一步提高了F1分数超过5%。这项工作展示了将现有的匿名化方法与机器学习相结合，可以减少人工劳动并增强自动建议的能力。 |
| [^74] | [Code Soliloquies for Accurate Calculations in Large Language Models.](http://arxiv.org/abs/2309.12161) | 该论文介绍了一种应对大型语言模型在处理复杂计算方面的限制性的创新方法，通过生成模拟对话，并在每个学生回答触发自言自语来提高性能。 |
| [^75] | [Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media.](http://arxiv.org/abs/2309.03564) | 本研究评估了监督学习和大型语言模型在识别中国社交媒体中的认知偏差和自杀风险方面的功效。结果表明大型语言模型在这两个任务上具有很高的效果。 |
| [^76] | [YaRN: Efficient Context Window Extension of Large Language Models.](http://arxiv.org/abs/2309.00071) | YaRN是一种高效的上下文窗口扩展方法，可以在大型语言模型中有效利用和推断比原始预训练允许的上下文长度更长的上下文，同时超越了之前的最新研究成果。 |
| [^77] | [CReHate: Cross-cultural Re-annotation of English Hate Speech Dataset.](http://arxiv.org/abs/2308.16705) | CReHate通过跨文化重新注释英语仇恨言论数据集，揭示了来自不同国家的个体对仇恨言论的不同看法，并引入了一种具有文化敏感性的分类器。这些发现强调了重新评估NLP研究在仇恨言论领域的必要性。 |
| [^78] | [Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts.](http://arxiv.org/abs/2306.04723) | 本文提出了衡量文本内部维度的方法，应用于鲁棒性AI生成文本的检测，展示了人类文本与AI生成文本在内部维度上的差异。 |
| [^79] | [Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy.](http://arxiv.org/abs/2305.14596) | 当大型语言模型应用于多项选择题时，其注意力往往会分散到许多无效的词汇符号上，这会导致模型真实性能的低估。本文提出了一种数学形式化方法来研究这种现象，并发现通过使用只含一个示例的上下文学习方法可以提高对有效选择的注意力。 |
| [^80] | [A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization.](http://arxiv.org/abs/2305.08503) | 本研究提出了一种用于抽象多文档摘要的层次编码-解码方案，在多领域的10个MDS数据集上测试表现最佳。 |
| [^81] | [Multi-step Jailbreaking Privacy Attacks on ChatGPT.](http://arxiv.org/abs/2304.05197) | 本文探讨了来自OpenAI模型API以及New Bing增强版的ChatGPT对隐私带来的风险，指出应用程序集成的LLMs可能导致比以往更严重的隐私威胁，实验支持该观点。 |
| [^82] | [KPEval: Towards Fine-grained Semantic-based Evaluation of Keyphrase Extraction and Generation Systems.](http://arxiv.org/abs/2303.15422) | KPEval是一个综合评估框架，旨在解决关键词提取和生成系统评估中的短板。通过引入四个关键维度，包括显著性、忠实性、多样性和实用性，并设计相应的语义度量指标，KPEval在与人类偏好相关性方面表现更好。使用该框架重新评估了20个关键词系统，并发现模型选择最佳的情况取决于评估维度，实用性是一个重要因素。 |
| [^83] | [Concept Algebra for Score-Based Conditional Models.](http://arxiv.org/abs/2302.03693) | 本文研究了基于分数的条件模型中学习表示的结构，并开发了一种数学形式化表达概念被编码为表示空间子空间的思想。利用这个方法，我们提出了一种简单的方法来识别给定概念对应的表示部分，并通过代数操作操纵模型所表达的概念。 |
| [^84] | [LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain.](http://arxiv.org/abs/2301.13126) | LEXTREME是一个多语言和多任务的法律领域基准，该基准提供了11个数据集涵盖24种语言的测评，最佳模型（XLM-R large）在数据集和语言综合评分上均达到了61.3。这使得LEXTREME仍然具有挑战性并且有改进空间。 |
| [^85] | [SegAugment: Maximizing the Utility of Speech Translation Data with Segmentation-based Augmentations.](http://arxiv.org/abs/2212.09699) | 该论文提出了一种新的数据增强策略SegAugment，利用音频分段系统生成数据集的多个句子级别的替代版本，能够提高语音翻译的性能，实验结果显示平均BLEU分数提高了2.5分，甚至在低资源情况下提高了5分。 |

# 详细

[^1]: 单通道说话者转换感知对话语音翻译系统

    End-to-End Single-Channel Speaker-Turn Aware Conversational Speech Translation. (arXiv:2311.00697v1 [cs.CL])

    [http://arxiv.org/abs/2311.00697](http://arxiv.org/abs/2311.00697)

    本文介绍了一种名为Speaker-Turn Aware Conversational Speech Translation的端到端和多任务训练模型，通过结合自动语音识别、语音翻译和说话者转换检测，实现了对单通道多说话者对话的语音翻译。在实验中，该模型在多说话者条件下优于传统系统，并取得了可比较的性能。

    

    传统的语音到文本翻译系统是在单个说话者的语音上进行训练的，可能无法适应实际情况下包含多个说话者对话的场景。在本文中，我们提出了一种名为Speaker-Turn Aware Conversational Speech Translation的端到端和多任务训练模型，通过使用特殊标记的序列化标签格式来结合自动语音识别、语音翻译和说话者转换检测。我们在Fisher-CALLHOME语料库上进行了实验，通过将两个单一说话者通道合并为一个多说话者通道，表示更真实和具有挑战性的多说话者转换和交叉对话场景。实验结果在单说话者和多说话者条件下，并与传统的语音到文本翻译系统进行比较，表明我们的模型在多说话者条件下优于参考系统，并取得了可比较的性能。

    Conventional speech-to-text translation (ST) systems are trained on single-speaker utterances, and they may not generalize to real-life scenarios where the audio contains conversations by multiple speakers. In this paper, we tackle single-channel multi-speaker conversational ST with an end-to-end and multi-task training model, named Speaker-Turn Aware Conversational Speech Translation, that combines automatic speech recognition, speech translation and speaker turn detection using special tokens in a serialized labeling format. We run experiments on the Fisher-CALLHOME corpus, which we adapted by merging the two single-speaker channels into one multi-speaker channel, thus representing the more realistic and challenging scenario with multi-speaker turns and cross-talk. Experimental results across single- and multi-speaker conditions and against conventional ST systems, show that our model outperforms the reference systems on the multi-speaker condition, while attaining comparable perform
    
[^2]: 解放创造力：语言模型作为层次策略以改进挑战性问题解决的探索

    Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving. (arXiv:2311.00694v1 [cs.AI])

    [http://arxiv.org/abs/2311.00694](http://arxiv.org/abs/2311.00694)

    本论文将大型语言模型（LLMs）作为层次策略，通过释放其创造潜力，探索多样化的问题解决策略。通过将LLMs分为领导者和执行者，领导者提供多种高级问题解决策略作为提示，执行者根据领导者的指引执行详细的问题解决过程，生成一组解决方案。

    

    大型语言模型（LLM）取得了巨大的进展，但仍然在具有挑战性的推理问题中往往遇到困难。目前的方法通过采样或搜索详细和低级的推理链来解决这个挑战。然而，这些方法在探索能力上仍然有限，使得正确的解决方案在庞大的解空间中很难突出。在这项工作中，我们通过将LLMs作为层次策略进行上下文学习，释放LLMs探索多样化问题解决策略的创造潜力。该策略包括一个有远见的领导者，提出多种多样的高级问题解决策略作为提示，并有一个执行者，根据每个高级指令执行详细的问题解决过程。执行者将领导者的每个指令作为指南，并采样多个推理链来解决问题，为每个领导者的提议生成一组解决方案。

    Large Language Models (LLMs) have achieved tremendous progress, yet they still often struggle with challenging reasoning problems. Current approaches address this challenge by sampling or searching detailed and low-level reasoning chains. However, these methods are still limited in their exploration capabilities, making it challenging for correct solutions to stand out in the huge solution space. In this work, we unleash LLMs' creative potential for exploring multiple diverse problem solving strategies by framing an LLM as a hierarchical policy via in-context learning. This policy comprises of a visionary leader that proposes multiple diverse high-level problem-solving tactics as hints, accompanied by a follower that executes detailed problem-solving processes following each of the high-level instruction. The follower uses each of the leader's directives as a guide and samples multiple reasoning chains to tackle the problem, generating a solution group for each leader proposal. Additio
    
[^3]: 通过使用语言模型模拟受众群体，改善人际沟通

    Improving Interpersonal Communication by Simulating Audiences with Language Models. (arXiv:2311.00687v1 [cs.AI])

    [http://arxiv.org/abs/2311.00687](http://arxiv.org/abs/2311.00687)

    本论文提出了一个基于大型语言模型（LLM）模拟的框架，通过探索解决方案空间、生成沟通候选以及模拟受众反应，来改善人际沟通。通过评估八个涵盖人际沟通基本过程的场景，展示了该框架的有效性。

    

    我们如何与他人进行沟通以实现自己的目标？我们利用先前的经验或他人的建议，或者通过预测对方的反应来构造候选表达。然而，我们的经验是有限和有偏见的，而且对潜在结果进行推理可能是困难且认知上具有挑战性的。本文中，我们探讨了如何利用大型语言模型（LLM）模拟来帮助我们更好地沟通。我们提出了探索-生成-模拟（EGS）框架，该框架接受任何一个个体与一个目标受众进行沟通的场景作为输入。EGS（1）通过生成与场景相关的多样化建议来探索解决方案空间，（2）生成以部分建议为条件的沟通候选，（3）模拟不同受众的反应，以确定最佳候选和建议的使用。我们在涵盖人际沟通十个基本过程的八个场景上评估了该框架。

    How do we communicate with others to achieve our goals? We use our prior experience or advice from others, or construct a candidate utterance by predicting how it will be received. However, our experiences are limited and biased, and reasoning about potential outcomes can be difficult and cognitively challenging. In this paper, we explore how we can leverage Large Language Model (LLM) simulations to help us communicate better. We propose the Explore-Generate-Simulate (EGS) framework, which takes as input any scenario where an individual is communicating to an audience with a goal they want to achieve. EGS (1) explores the solution space by producing a diverse set of advice relevant to the scenario, (2) generates communication candidates conditioned on subsets of the advice, and (3) simulates the reactions from various audiences to determine both the best candidate and advice to use. We evaluate the framework on eight scenarios spanning the ten fundamental processes of interpersonal com
    
[^4]: 小巨人：探索小型LLM作为Eval4NLP 2023共享任务中摘要评估度量的潜力

    Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task. (arXiv:2311.00686v1 [cs.CL])

    [http://arxiv.org/abs/2311.00686](http://arxiv.org/abs/2311.00686)

    本文通过对Eval4NLP 2023共享任务的研究，探索了小型LLM作为摘要评估度量的潜力，在使用多种提示技术和学习方法的基础上，揭示了结合小型开源模型orca_mini_v3_7B可以取得有竞争力的结果。

    

    本文描述和分析了我们参与2023 Eval4NLP共享任务的情况，该任务专注于评估基于提示的技术对大型语言模型在质量估计中的应用效果，特别是在评估机器翻译和摘要的背景下。我们进行了系统性实验，使用了多种提示技术，包括标准提示、基于注释者指示的提示和创新的思路链提示。此外，我们将这些方法与零样本学习和一次性学习方法结合起来，以最大化我们的评估程序的功效。我们的工作表明，使用“小型”开源模型（orca_mini_v3_7B）结合这些方法可以获得有竞争力的结果。

    This paper describes and analyzes our participation in the 2023 Eval4NLP shared task, which focuses on assessing the effectiveness of prompt-based techniques to empower Large Language Models to handle the task of quality estimation, particularly in the context of evaluating machine translations and summaries. We conducted systematic experiments with various prompting techniques, including standard prompting, prompts informed by annotator instructions, and innovative chain-of-thought prompting. In addition, we integrated these approaches with zero-shot and one-shot learning methods to maximize the efficacy of our evaluation procedures. Our work reveals that combining these approaches using a "small", open source model (orca_mini_v3_7B) yields competitive results.
    
[^5]: 注意力对齐和灵活的位置嵌入提高了Transformer长度外推能力

    Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation. (arXiv:2311.00684v1 [cs.CL])

    [http://arxiv.org/abs/2311.00684](http://arxiv.org/abs/2311.00684)

    本研究提出了两种通过温度缩放实现的注意力对齐策略，通过改善T5在长序列处理中的注意力分布问题，提高了其在语言建模、检索和多文档问答等任务中的长上下文利用能力。

    

    理想的长度可外推的Transformer语言模型可以处理比训练长度更长的序列而不需要进行长序列微调。这种长上下文利用能力高度依赖于灵活的位置嵌入设计。在调查现有大型预训练Transformer语言模型的灵活性时，我们发现T5系列值得更仔细研究，因为它的位置嵌入捕捉到了丰富而灵活的注意力模式。然而，T5存在着分散的注意力问题：输入序列越长，注意力分布就越平坦。为了缓解这个问题，我们提出了两种通过温度缩放实现的注意力对齐策略。我们的研究结果提高了T5在语言建模、检索和多文档问答方面的长上下文利用能力，而且不需要进行任何微调，这表明灵活的位置嵌入设计和注意力对齐对于Transformer长度外推至关重要。

    An ideal length-extrapolatable Transformer language model can handle sequences longer than the training length without any long sequence fine-tuning. Such long-context utilization capability highly relies on a flexible positional embedding design. Upon investigating the flexibility of existing large pre-trained Transformer language models, we find that the T5 family deserves a closer look, as its positional embeddings capture rich and flexible attention patterns. However, T5 suffers from the dispersed attention issue: the longer the input sequence, the flatter the attention distribution. To alleviate the issue, we propose two attention alignment strategies via temperature scaling. Our findings improve the long-context utilization capability of T5 on language modeling, retrieval, and multi-document question answering without any fine-tuning, suggesting that a flexible positional embedding design and attention alignment go a long way toward Transformer length extrapolation.\footnote{\url
    
[^6]: 大型语言模型是可靠的评判者吗？关于LLMs的事实性评估能力的研究。

    Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs. (arXiv:2311.00681v1 [cs.CL])

    [http://arxiv.org/abs/2311.00681](http://arxiv.org/abs/2311.00681)

    大型语言模型作为评估生成模型所产生摘要的事实一致性的可靠评估者的潜力被研究。研究发现在GPT-4和PaLM-2方面事实评估指标与人工评估之间缺乏显著的相关性。

    

    近年来，由于其显著的新兴能力，大型语言模型（LLMs）引起了极大的关注，超越了早期语言模型的能力。LLMs的一个特别有趣的应用是作为生成模型生成的文本的评估者。在这项研究中，我们深入探讨了LLMs作为可靠的事实一致性评估者的潜力。我们首先介绍了一种使用LLMs进行事实评估的创新方法。这包括在整个基于问答的事实评分过程中使用一个单一的LLM。接下来，我们检验了各种LLMs在直接事实评分方面的效果，并将它们与传统测量方法和人工注释进行了基准测试。与最初的预期相反，我们的结果表明事实评估指标与人工评估之间缺乏显著的相关性，特别是在GPT-4和PaLM-2方面。值得注意的相关性是

    In recent years, Large Language Models (LLMs) have gained immense attention due to their notable emergent capabilities, surpassing those seen in earlier language models. A particularly intriguing application of LLMs is their role as evaluators for texts produced by various generative models.  In this study, we delve into the potential of LLMs as reliable assessors of factual consistency in summaries generated by text-generation models. Initially, we introduce an innovative approach for factuality assessment using LLMs. This entails employing a singular LLM for the entirety of the question-answering-based factuality scoring process. Following this, we examine the efficacy of various LLMs in direct factuality scoring, benchmarking them against traditional measures and human annotations.  Contrary to initial expectations, our results indicate a lack of significant correlations between factuality metrics and human evaluations, specifically for GPT-4 and PaLM-2. Notable correlations were on
    
[^7]: 基于情感的虚假信息检测：一项综述

    Emotion Detection for Misinformation: A Review. (arXiv:2311.00671v1 [cs.CL])

    [http://arxiv.org/abs/2311.00671](http://arxiv.org/abs/2311.00671)

    本文综述了基于情感的虚假信息检测方法，首先解释了情感与虚假信息之间的关系，然后对应用不同情感、情绪和立场特征的虚假信息检测方法进行了详细分析，讨论了其中的优势和弱点，最后探讨了情感驱动的虚假信息检测面临的挑战。

    

    随着社交媒体的出现，越来越多的网民在网上分享和阅读帖子和新闻。然而，涌入互联网的大量虚假信息（例如假新闻和谣言）可能对人们的生活产生不利影响，并导致谣言和虚假新闻检测成为热门研究课题的出现。网民在社交媒体帖子和新闻中表达的情绪和情感构成了可以帮助区分虚假信息和真实信息并理解谣言传播的重要因素。本文全面回顾了基于情感的虚假信息检测方法。我们首先解释了情感与虚假信息之间的密切关系。随后，我们对应用各种情感、情绪和立场特征的一系列虚假信息检测方法进行了详细分析，并描述了它们的优势和弱点。最后，我们讨论了情感驱动的虚假信息检测中的一些持续挑战。

    With the advent of social media, an increasing number of netizens are sharing and reading posts and news online. However, the huge volumes of misinformation (e.g., fake news and rumors) that flood the internet can adversely affect people's lives, and have resulted in the emergence of rumor and fake news detection as a hot research topic. The emotions and sentiments of netizens, as expressed in social media posts and news, constitute important factors that can help to distinguish fake news from genuine news and to understand the spread of rumors. This article comprehensively reviews emotion-based methods for misinformation detection. We begin by explaining the strong links between emotions and misinformation. We subsequently provide a detailed analysis of a range of misinformation detection methods that employ a variety of emotion, sentiment and stance-based features, and describe their strengths and weaknesses. Finally, we discuss a number of ongoing challenges in emotion-based misinfo
    
[^8]: 预训练语言模型中加入明确的形态学知识提高了希伯来语的性能

    Explicit Morphological Knowledge Improves Pre-training of Language Models for Hebrew. (arXiv:2311.00658v1 [cs.CL])

    [http://arxiv.org/abs/2311.00658](http://arxiv.org/abs/2311.00658)

    加入明确的形态学知识在预训练阶段可以提高希伯来语的语言模型性能。

    

    预训练语言模型（PLM）在自我监督训练的文本流上取得了显著的成功，获得了广泛的语言知识。然而，当应用于形态丰富的语言时，这种语言不可知的方法的有效性经常受到质疑。我们研究了在预训练阶段加入明确的形态学知识能否提高PLM在形态丰富的语言中的性能的假设。我们提出了多种基于形态学的分词方法，使模型能够利用除了原始文本之外的形态学线索。我们使用不同的方法对多个语言模型进行预训练，并在复杂且高度模糊的希伯来语上进行评估。实验结果表明，基于形态学的分词方法在语义和形态学评估标准上相比标准语言不可知的分词方法有改进的结果。

    Pre-trained language models (PLMs) have shown remarkable successes in acquiring a wide range of linguistic knowledge, relying solely on self-supervised training on text streams. Nevertheless, the effectiveness of this language-agnostic approach has been frequently questioned for its sub-optimal performance when applied to morphologically-rich languages (MRLs). We investigate the hypothesis that incorporating explicit morphological knowledge in the pre-training phase can improve the performance of PLMs for MRLs. We propose various morphologically driven tokenization methods enabling the model to leverage morphological cues beyond raw text. We pre-train multiple language models utilizing the different methods and evaluate them on Hebrew, a language with complex and highly ambiguous morphology. Our experiments show that morphologically driven tokenization demonstrates improved results compared to a standard language-agnostic tokenization, on a benchmark of both semantic and morphologic ta
    
[^9]: 从逆向Petri网到彩色Petri网的形式化翻译

    Formal Translation from Reversing Petri Nets to Coloured Petri Nets. (arXiv:2311.00629v1 [cs.CL])

    [http://arxiv.org/abs/2311.00629](http://arxiv.org/abs/2311.00629)

    这项研究提出了从逆向Petri网到彩色Petri网的形式化翻译方法，实现了可逆计算的三种主要形式，为低功耗计算和广泛应用领域提供了可能性。

    

    可逆计算是一种新兴的计算范式，允许在计算过程中任意时刻以相反的顺序执行操作序列。它具有低功耗计算的潜力，并与化学反应、量子计算、机器人技术和分布式系统等广泛应用有关。逆向Petri网是对Petri网的最近提出的扩展，实现了三种主要的可逆性形式，即回溯、因果逆转和超出因果顺序逆转。它们的特点是使用具有命名的标记，可以结合在一起形成键合。命名标记以及历史函数构成了记忆过去行为的手段，从而实现了可逆性。在最近的工作中，我们提出了从一类RPNs到彩色Petri网模型（CPNs）的结构化翻译。CPN是传统Petri网的扩展，其中标记携带数据值。

    Reversible computation is an emerging computing paradigm that allows any sequence of operations to be executed in reverse order at any point during computation. Its appeal lies in its potential for lowpower computation and its relevance to a wide array of applications such as chemical reactions, quantum computation, robotics, and distributed systems. Reversing Petri nets are a recently-proposed extension of Petri nets that implements the three main forms of reversibility, namely, backtracking, causal reversing, and out-of-causal-order reversing. Their distinguishing feature is the use of named tokens that can be combined together to form bonds. Named tokens along with a history function, constitute the means of remembering past behaviour, thus, enabling reversal. In recent work, we have proposed a structural translation from a subclass of RPNs to the model of Coloured Petri Nets (CPNs), an extension of traditional Petri nets where tokens carry data values. In this paper, we extend the 
    
[^10]: 使用正则化流和激进训练提升摘要生成

    Boosting Summarization with Normalizing Flows and Aggressive Training. (arXiv:2311.00588v1 [cs.CL])

    [http://arxiv.org/abs/2311.00588](http://arxiv.org/abs/2311.00588)

    本文提出了FlowSUM，一个基于正则化流的变分编码器-解码器框架，用于改进Transformer-based摘要生成。通过利用正则化流进行灵活的潜在后向建模以及采用受控交替激进训练策略，FlowSUM显著提高了生成摘要的质量，并探讨了正则化流中的后向塌陷问题和相关影响因素，为相关研究提供了宝贵的洞察。

    

    本文提出了基于正则化流的变分编码器-解码器框架FlowSUM，用于基于Transformer的摘要生成。我们的方法解决了变分摘要生成中的两个主要挑战：潜在表示中的语义信息不足和训练过程中的后向塌陷。为了应对这些挑战，我们使用正则化流实现了灵活的潜在后向建模，并提出了一种改进的门机制下的受控交替激进训练（CAAT）策略。实验结果表明，FlowSUM显著提高了生成摘要的质量，并在对推理时间的影响最小的情况下释放了知识蒸馏的潜力。此外，我们还研究了正则化流中的后向塌陷问题，并分析了训练策略、门初始化以及使用的正则化流类型和数量对摘要质量的影响，为未来研究提供了宝贵的见解。

    This paper presents FlowSUM, a normalizing flows-based variational encoder-decoder framework for Transformer-based summarization. Our approach tackles two primary challenges in variational summarization: insufficient semantic information in latent representations and posterior collapse during training. To address these challenges, we employ normalizing flows to enable flexible latent posterior modeling, and we propose a controlled alternate aggressive training (CAAT) strategy with an improved gate mechanism. Experimental results show that FlowSUM significantly enhances the quality of generated summaries and unleashes the potential for knowledge distillation with minimal impact on inference time. Furthermore, we investigate the issue of posterior collapse in normalizing flows and analyze how the summary quality is affected by the training strategy, gate initialization, and the type and number of normalizing flows used, offering valuable insights for future research.
    
[^11]: 跨语言检索增强情境学习用于孟加拉语的研究

    Crosslingual Retrieval Augmented In-context Learning for Bangla. (arXiv:2311.00587v1 [cs.CL])

    [http://arxiv.org/abs/2311.00587](http://arxiv.org/abs/2311.00587)

    本文提出了一种跨语言检索增强情境学习的创新方法，通过利用高资源语言中的语义相似提示，成功提升了孟加拉语任务的性能。

    

    在自然语言处理中，大型语言模型（LLMs）的潜力常常被它们在低资源语言（如孟加拉语）中的有限性能所遮盖。为了解决这个问题，我们的论文提出了一种创新方法，利用跨语言检索增强情境学习。通过从高资源语言中策略性地获取语义相似的提示，我们使得多语言预训练语言模型（MPLMs），特别是生成模型BLOOMZ，能够成功提高孟加拉语任务的性能。我们广泛的评估表明，跨语言检索增强的提示对MPLMs的提升效果稳定，并且超过了零样本性能。

    The promise of Large Language Models (LLMs) in Natural Language Processing has often been overshadowed by their limited performance in low-resource languages such as Bangla. To address this, our paper presents a pioneering approach that utilizes cross-lingual retrieval augmented in-context learning. By strategically sourcing semantically similar prompts from high-resource language, we enable multilingual pretrained language models (MPLMs), especially the generative model BLOOMZ, to successfully boost performance on Bangla tasks. Our extensive evaluation highlights that the cross-lingual retrieval augmented prompts bring steady improvements to MPLMs over the zero-shot performance.
    
[^12]: LLaVA-Interactive:一个图像对话、分割、生成和编辑的全能演示

    LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing. (arXiv:2311.00571v1 [cs.CV])

    [http://arxiv.org/abs/2311.00571](http://arxiv.org/abs/2311.00571)

    LLaVA-Interactive是一个多模态人机交互的研究原型系统，通过视觉提示来对齐人类意图，实现了图像对话、分割、生成和编辑的多模态功能，具有高成本效益和广泛的应用潜力。

    

    LLaVA-Interactive是一种多模态人机交互的研究原型系统。该系统可以通过接收多模态用户输入并生成多模态回应与用户进行多轮对话。重要的是，LLaVA-Interactive不仅仅是语言提示，还可以使用视觉提示来对齐人类意图。LLaVA-Interactive的研发成本效益非常高，因为该系统结合了预建的AI模型的三项多模态技能，而无需额外的模型训练：LLaVA的图像对话，SEEM的图像分割以及GLIGEN的图像生成和编辑。展示了各种应用场景，以展示LLaVA-Interactive的潜力，并激发未来在多模态交互系统方面的研究。

    LLaVA-Interactive is a research prototype for multimodal human-AI interaction. The system can have multi-turn dialogues with human users by taking multimodal user inputs and generating multimodal responses. Importantly, LLaVA-Interactive goes beyond language prompt, where visual prompt is enabled to align human intents in the interaction. The development of LLaVA-Interactive is extremely cost-efficient as the system combines three multimodal skills of pre-built AI models without additional model training: visual chat of LLaVA, image segmentation from SEEM, as well as image generation and editing from GLIGEN. A diverse set of application scenarios is presented to demonstrate the promises of LLaVA-Interactive and to inspire future research in multimodal interactive systems.
    
[^13]: 一个带有嵌入式历时语义变化模型的论文与一个关于古希腊的案例研究

    An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek. (arXiv:2311.00541v1 [cs.CL])

    [http://arxiv.org/abs/2311.00541](http://arxiv.org/abs/2311.00541)

    本论文介绍了一个嵌入式历时语义变化模型（EDiSC），结合了词嵌入和DiSC模型，通过无监督学习分析古希腊文本中目标词汇的意义变化。实验证明EDiSC具有优越的性能。

    

    词汇的意义随着时间的推移而变化，词义在这个过程中会演变、出现或消失。对于古代语言来说，由于语料库通常较小、稀疏且嘈杂，准确建模这种变化变得具有挑战性，因此对于意义变化估计的不确定性进行量化变得重要。GASC和DiSC是现有的生成模型，已经被用来分析古希腊文本语料库中目标词汇的意义变化，使用了无监督学习并没有借助任何预训练的帮助。这些模型将给定目标词汇（如"kosmos"，意为装饰、秩序或世界）的意义表示为上下文词汇的分布，并将意义的普遍性表示为意义的分布。这些模型使用马尔科夫链蒙特卡洛方法进行拟合，以测量这些表示中的时间变化。在本文中，我们介绍了EDiSC，这是DiSC的嵌入版本，它将词嵌入与DiSC相结合，提供了更优秀的模型性能。我们通过实验证明，EDiSC提供了改进的性能。

    Word meanings change over time, and word senses evolve, emerge or die out in the process. For ancient languages, where the corpora are often small, sparse and noisy, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. GASC and DiSC are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training. These models represent the senses of a given target word such as "kosmos" (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. The models are fitted using MCMC methods to measure temporal changes in these representations. In this paper, we introduce EDiSC, an embedded version of DiSC, which combines word embeddings with DiSC to provide superior model performance. We show empirically that EDiSC offers improved p
    
[^14]: 像素语言模型的文本渲染策略

    Text Rendering Strategies for Pixel Language Models. (arXiv:2311.00522v1 [cs.CL])

    [http://arxiv.org/abs/2311.00522](http://arxiv.org/abs/2311.00522)

    本文研究了在像素语言模型中渲染文本的四种方法，并发现简单的字符二元渲染策略在句子级任务上表现出更好的性能，同时不降低令牌级或多语言任务的表现。该策略还使得可以训练一个更紧凑的模型，仅使用22M参数但性能与原始模型相当。

    

    基于像素的语言模型处理以图像形式渲染的文本，这使得它们能够处理任何脚本，为开放词汇语言建模提供了有希望的方法。然而，最近的方法使用产生大量几乎等效的输入补丁的文本渲染器，这可能对下游任务来说并不最优，因为输入表示中存在冗余。在本文中，我们研究了在PIXEL模型（Rust等人，2023）中进行文本渲染的四种方法，并发现简单的字符二元渲染在句子级任务上带来了改进的性能，同时在令牌级或多语言任务上不降低性能。这种新的渲染策略还使得我们能够训练一个只有22M参数但性能与原始的86M参数模型相当的更紧凑模型。我们的分析显示，字符二元渲染导致了一个具有各向异性的补丁嵌入空间，由于补丁频率偏差而受到驱动，突出了...

    Pixel-based language models process text rendered as images, which allows them to handle any script, making them a promising approach to open vocabulary language modelling. However, recent approaches use text renderers that produce a large set of almost-equivalent input patches, which may prove sub-optimal for downstream tasks, due to redundancy in the input representations. In this paper, we investigate four approaches to rendering text in the PIXEL model (Rust et al., 2023), and find that simple character bigram rendering brings improved performance on sentence-level tasks without compromising performance on token-level or multilingual tasks. This new rendering strategy also makes it possible to train a more compact model with only 22M parameters that performs on par with the original 86M parameter model. Our analyses show that character bigram rendering leads to a consistently better model but with an anisotropic patch embedding space, driven by a patch frequency bias, highlighting 
    
[^15]: 基于规则的错误分类用于分析频繁错误的差异

    Rule-Based Error Classification for Analyzing Differences in Frequent Errors. (arXiv:2311.00513v1 [cs.SE])

    [http://arxiv.org/abs/2311.00513](http://arxiv.org/abs/2311.00513)

    本文提出了一种基于规则的错误分类工具，用于分析初学者和专家程序员之间频繁错误的差异，并通过对95,631个代码对进行错误分类，发现初学者的错误主要是由于缺乏编程知识。

    

    发现和修复错误对于初学者和专业程序员来说都是一项耗时的任务。之前的研究已经确定了各个级别程序员之间频繁错误模式的差异。然而，初学者和专家之间的不同倾向尚未被揭示。通过了解不同级别程序员的频繁错误，教师可以为不同级别的学习者提供有用的建议。本文提出了一种基于规则的错误分类工具，用于对由错误和正确程序组成的代码对进行错误分类。我们对95,631个代码对进行错误分类，平均找到3.47个错误，这些代码对由各个级别的程序员在一个在线评测系统上提交。分类的错误被用于分析初学者和专家程序员之间频繁错误的差异。分析结果显示，在相同的入门问题中，初学者的错误是由于缺乏编程知识导致的。

    Finding and fixing errors is a time-consuming task not only for novice programmers but also for expert programmers. Prior work has identified frequent error patterns among various levels of programmers. However, the differences in the tendencies between novices and experts have yet to be revealed. From the knowledge of the frequent errors in each level of programmers, instructors will be able to provide helpful advice for each level of learners. In this paper, we propose a rule-based error classification tool to classify errors in code pairs consisting of wrong and correct programs. We classify errors for 95,631 code pairs and identify 3.47 errors on average, which are submitted by various levels of programmers on an online judge system. The classified errors are used to analyze the differences in frequent errors between novice and expert programmers. The analyzed results show that, as for the same introductory problems, errors made by novices are due to the lack of knowledge in progra
    
[^16]: 对抗攻击下自动机器翻译评估指标的鲁棒性测试

    Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks. (arXiv:2311.00508v1 [cs.CL])

    [http://arxiv.org/abs/2311.00508](http://arxiv.org/abs/2311.00508)

    本文通过对抗攻击实验，研究了机器翻译评估指标在对抗性合成文本上的表现，揭示了指标的鲁棒性问题，并发现了BERTScore评级的不一致性，为更鲁棒的指标开发提供了动力。

    

    本文研究了对抗性合成文本下机器翻译评估指标的表现，以揭示指标的鲁棒性。我们对三个常用的机器翻译评估指标（BERTScore、BLEURT和COMET）进行了词级和字符级攻击实验。我们的人类实验验证了自动评估指标倾向于过度惩罚经过对抗性恶化的翻译。我们还发现了BERTScore评级上的不一致性，即它认为原始句子和经过对抗性恶化的句子相似，而相对于参考文本而言，认为恶化的翻译明显比原始翻译差。我们发现了易损性的模式，促进了更加鲁棒的指标开发。

    We investigate MT evaluation metric performance on adversarially-synthesized texts, to shed light on metric robustness. We experiment with word- and character-level attacks on three popular machine translation metrics: BERTScore, BLEURT, and COMET. Our human experiments validate that automatic metrics tend to overpenalize adversarially-degraded translations. We also identify inconsistencies in BERTScore ratings, where it judges the original sentence and the adversarially-degraded one as similar, while judging the degraded translation as notably worse than the original with respect to the reference. We identify patterns of brittleness that motivate more robust metric development.
    
[^17]: 在CPU上高效的LLM推理

    Efficient LLM Inference on CPUs. (arXiv:2311.00502v1 [cs.LG])

    [http://arxiv.org/abs/2311.00502](http://arxiv.org/abs/2311.00502)

    本研究提出了一种在CPU上高效部署大型语言模型（LLMs）的方法，支持自动权重量化和优化内核，在流行的LLMs上展示了极高的推理效率。

    

    大型语言模型(LLMs)已经在各种任务上展示出了令人瞩目的性能和巨大的潜力。然而，由于模型参数的庞大数量，LLMs的部署一直面临挑战，对大内存容量和高内存带宽的需求。在本文中，我们提出了一种有效的方法，可以使LLMs的部署更高效。我们支持自动的INT4权重量化流程，并设计了一个特殊的LLM运行时，具有高度优化的内核，以加速在CPU上的LLM推理。我们展示了我们的方法在流行的LLMs上的普适性，包括Llama2，Llama，GPT-NeoX，并展示了在CPU上的极高推理效率。代码公开可用于: https://github.com/intel/intel-extension-for-transformers.

    Large language models (LLMs) have demonstrated remarkable performance and tremendous potential across a wide range of tasks. However, deploying these models has been challenging due to the astronomical amount of model parameters, which requires a demand for large memory capacity and high memory bandwidth. In this paper, we propose an effective approach that can make the deployment of LLMs more efficiently. We support an automatic INT4 weight-only quantization flow and design a special LLM runtime with highly-optimized kernels to accelerate the LLM inference on CPUs. We demonstrate the general applicability of our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase the extreme inference efficiency on CPUs. The code is publicly available at: https://github.com/intel/intel-extension-for-transformers.
    
[^18]: 对比优化目标对一致性搜索的比较

    Comparing Optimization Targets for Contrast-Consistent Search. (arXiv:2311.00488v1 [cs.LG])

    [http://arxiv.org/abs/2311.00488](http://arxiv.org/abs/2311.00488)

    本研究通过对比优化目标发现，使用Midpoint-Displacement（MD）损失函数可以获得与Contrast-Consistent Search（CCS）非常相似的模型权重，而且通过调整超参数可以使MD损失函数的测试准确率高于CCS。

    

    我们研究了一致性搜索（CCS）的优化目标，该目标旨在恢复大型语言模型的真实内部表示。我们提出了一种新的损失函数，称为Midpoint-Displacement（MD）损失函数。我们证明在某个超参数值下，该MD损失函数导致的模型权重与CCS非常相似。我们进一步展示了该超参数并不是最优的，并且用更好的超参数，MD损失函数可以达到比CCS更高的测试准确率。

    We investigate the optimization target of Contrast-Consistent Search (CCS), which aims to recover the internal representations of truth of a large language model. We present a new loss function that we call the Midpoint-Displacement (MD) loss function. We demonstrate that for a certain hyper-parameter value this MD loss function leads to a prober with very similar weights to CCS. We further show that this hyper-parameter is not optimal and that with a better hyper-parameter the MD loss function attains a higher test accuracy than CCS.
    
[^19]: 具有kNN语言模型的可控生成的风格封闭性

    Style Locality for Controllable Generation with kNN Language Models. (arXiv:2311.00475v1 [cs.CL])

    [http://arxiv.org/abs/2311.00475](http://arxiv.org/abs/2311.00475)

    本论文提出了一种使用位置级别进行可控生成的新方法，并通过对礼貌、形式、支持性和毒性文本数据的自动和人工评估发现，我们的模型能够成功地控制风格，并提供更好的流利性和风格平衡。

    

    最近的语言模型通过添加外部存储得到了改进。最近邻语言模型检索相似的上下文来帮助单词预测。添加位置级别使模型能够学习如何根据当前文本在源文献中的相对位置来加权邻居，并已经显示进一步提高了模型的性能。最近邻模型已经用于可控生成，但尚未研究位置级别的使用。我们提出了一个新颖的方法来实现这个目的，并使用礼貌、形式、支持性和毒性文本数据进行了自动和人工评估。我们发现我们的模型能够成功地控制风格，并提供比以前的工作更好的流利性和风格平衡。

    Recent language models have been improved by the addition of external memory. Nearest neighbor language models retrieve similar contexts to assist in word prediction. The addition of locality levels allows a model to learn how to weight neighbors based on their relative location to the current text in source documents, and have been shown to further improve model performance. Nearest neighbor models have been explored for controllable generation but have not examined the use of locality levels. We present a novel approach for this purpose and evaluate it using automatic and human evaluation on politeness, formality, supportiveness, and toxicity textual data. We find that our model is successfully able to control style and provides a better fluency-style trade-off than previous work.
    
[^20]: 论述关系分类和通过认知维度对框架间论述关系分类的跨界考察

    Discourse Relations Classification and Cross-Framework Discourse Relation Classification Through the Lens of Cognitive Dimensions: An Empirical Investigation. (arXiv:2311.00451v1 [cs.CL])

    [http://arxiv.org/abs/2311.00451](http://arxiv.org/abs/2311.00451)

    通过认知维度，我们可以有效地捕捉和分类不同框架间的论述关系，这为标注和自动分类带来了便利。

    

    现有的论述形式使用不同的论述关系分类法，需要专家知识才能理解，给标注和自动分类带来挑战。我们表明，一些由Sanders等人（2018）提出的简单认知启发维度可以有效地捕捉论述关系。我们在PDTB和RST两个框架上进行的跨界论述关系分类实验证明，通过这些维度可以将一个框架中的论述关系知识转移到另一个框架中，尽管两个框架的论述划分存在差异。这表明了这些维度在表征不同框架中的论述关系方面的有效性。消融实验显示不同维度影响不同类型的论述关系。这些模式可以通过维度在表征和区分不同关系中的作用来解释。我们还报告了我们在一个框架中进行的实验结果。

    Existing discourse formalisms use different taxonomies of discourse relations, which require expert knowledge to understand, posing a challenge for annotation and automatic classification. We show that discourse relations can be effectively captured by some simple cognitively inspired dimensions proposed by Sanders et al.(2018). Our experiments on cross-framework discourse relation classification (PDTB & RST) demonstrate that it is possible to transfer knowledge of discourse relations for one framework to another framework by means of these dimensions, in spite of differences in discourse segmentation of the two frameworks. This manifests the effectiveness of these dimensions in characterizing discourse relations across frameworks. Ablation studies reveal that different dimensions influence different types of discourse relations. The patterns can be explained by the role of dimensions in characterizing and distinguishing different relations. We also report our experimental results on a
    
[^21]: 人类和语言模型中的三段论推理的系统比较

    A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models. (arXiv:2311.00445v1 [cs.CL])

    [http://arxiv.org/abs/2311.00445](http://arxiv.org/abs/2311.00445)

    这项研究通过对比人类和语言模型在三段论推理中的表现，发现较大的语言模型更合逻辑，甚至比人类更合逻辑，但即使最大的语言模型也会出现与人类推理类似的错误，总体上认为语言模型在某些情况下能够克服人类偏见。

    

    理性行为的一个核心组成部分是逻辑推理：确定哪些结论可以从一组前提中得出。心理学家已经记录下人类推理与逻辑规则不符的几种方式。语言模型是否能够复制这些偏差，或者它们能够克服这些偏差？我们关注三段论的情况 - 从两个简单前提中推导出的推理，这在心理学中已经广泛研究 - 我们发现较大的模型比较合逻辑，而且比人类更合逻辑。与此同时，即使是最大的模型也会出现系统性错误，其中一些错误与人类推理的偏见相似，例如排序效应和逻辑谬误。总体上，我们发现语言模型模仿了训练数据中包含的人类偏见，但在某些情况下能够克服这些偏见。

    A central component of rational behavior is logical inference: the process of determining which conclusions follow from a set of premises. Psychologists have documented several ways in which humans' inferences deviate from the rules of logic. Do language models, which are trained on text generated by humans, replicate these biases, or are they able to overcome them? Focusing on the case of syllogisms -- inferences from two simple premises, which have been studied extensively in psychology -- we show that larger models are more logical than smaller ones, and also more logical than humans. At the same time, even the largest models make systematic errors, some of which mirror human reasoning biases such as ordering effects and logical fallacies. Overall, we find that language models mimic the human biases included in their training data, but are able to overcome them in some cases.
    
[^22]: Distil-Whisper: 通过大规模伪标签实现稳健的知识蒸馏

    Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling. (arXiv:2311.00430v1 [cs.CL])

    [http://arxiv.org/abs/2311.00430](http://arxiv.org/abs/2311.00430)

    本文提出了Distil-Whisper模型，利用大规模伪标签和知识蒸馏技术将预训练语音识别模型Whisper蒸馏成更小的模型。Distil-Whisper模型在速度和参数数量方面有显著改进，并在零-shot转移设置下表现出良好的鲁棒性。

    

    随着预训练语音识别模型的规模增大，将这些大型模型应用于低延迟或资源受限的环境变得具有挑战性。在这项工作中，我们利用伪标记来组装一个大规模开源数据集，用于将Whisper模型蒸馏为一个更小的变种，称为Distil-Whisper。使用简单的词错误率（WER）启发式策略，我们仅选择最高质量的伪标签进行训练。蒸馏后的模型速度提高了5.8倍，参数减少了51％，在零-shot转移设置下，在分布外测试数据上的WER仅有1％的降低。Distil-Whisper保持了Whisper模型对于困难的声学条件的稳健性，同时在长形音频上减少了虚构错误的发生。Distil-Whisper旨在与Whisper配对进行推理解码，从而加快2倍速度，同时数学上确保与原始模型相同的输出。

    As the size of pre-trained speech recognition models increases, running these large models in low-latency or resource-constrained environments becomes challenging. In this work, we leverage pseudo-labelling to assemble a large-scale open-source dataset which we use to distill the Whisper model into a smaller variant, called Distil-Whisper. Using a simple word error rate (WER) heuristic, we select only the highest quality pseudo-labels for training. The distilled model is 5.8 times faster with 51% fewer parameters, while performing to within 1% WER on out-of-distribution test data in a zero-shot transfer setting. Distil-Whisper maintains the robustness of the Whisper model to difficult acoustic conditions, while being less prone to hallucination errors on long-form audio. Distil-Whisper is designed to be paired with Whisper for speculative decoding, yielding a 2 times speed-up while mathematically ensuring the same outputs as the original model. To facilitate further research in this do
    
[^23]: 通过准备性基于语言的约定实现高效的人工智能-人类协同

    Efficient Human-AI Coordination via Preparatory Language-based Convention. (arXiv:2311.00416v1 [cs.LG])

    [http://arxiv.org/abs/2311.00416](http://arxiv.org/abs/2311.00416)

    本研究发现，在人工智能与人类协同之前，人类进行交流以建立约定，指定角色和行动，有效地指导协同。基于此观察，提出利用大型语言模型(LLM)来实现高效的人工智能-人类协同。

    

    开发能够与人类顺畅协同的智能代理是实现人工通用智能的关键一步。目前，人工智能与人类协同的现有方法通常训练一个代理与多样化的策略或基于真实人类数据拟合的人类模型进行协同。然而，人类行为的大规模多样性对于容量有限的AI系统来说是障碍，而高质量的人类数据在现实环境中可能不容易获取。在这项研究中，我们观察到在协同之前，人类进行交流以建立规则约定，指定个体角色和行动，使他们的协同顺利进行。基于这一观察结果，我们提出利用大型语言模型(LLM)来制定行动计划(或等效地，约定)，有效地指导人类和人工智能。通过输入任务要求、人类偏好、代理数量和其他相关信息

    Developing intelligent agents capable of seamless coordination with humans is a critical step towards achieving artificial general intelligence. Existing methods for human-AI coordination typically train an agent to coordinate with a diverse set of policies or with human models fitted from real human data. However, the massively diverse styles of human behavior present obstacles for AI systems with constrained capacity, while high quality human data may not be readily available in real-world scenarios. In this study, we observe that prior to coordination, humans engage in communication to establish conventions that specify individual roles and actions, making their coordination proceed in an orderly manner. Building upon this observation, we propose employing the large language model (LLM) to develop an action plan (or equivalently, a convention) that effectively guides both human and AI. By inputting task requirements, human preferences, the number of agents, and other pertinent infor
    
[^24]: 提升放射学报告生成的知识注入

    Enhanced Knowledge Injection for Radiology Report Generation. (arXiv:2311.00399v1 [cs.CV])

    [http://arxiv.org/abs/2311.00399](http://arxiv.org/abs/2311.00399)

    提出了一种增强的知识注入框架用于生成准确的医学图像报告，通过加权概念知识和多模式检索知识提取关键临床信息，解决了医学图像报告生成中的挑战。

    

    自动生成放射学报告在临床价值上至关重要，因为它可以减轻放射科医生的工作负担，并提醒经验较少的医生潜在的异常情况。尽管各种图像字幕方法在自然图像领域表现出色，但为医学图像生成准确的报告仍面临挑战，如视觉和文本数据的差异以及缺乏准确的领域知识。为了解决这些问题，我们提出了一种增强的知识注入框架，该框架利用两个分支提取不同类型的知识。加权概念知识（Weighted Concept Knowledge，WCK）分支负责使用TF-IDF分数对临床医学概念进行加权引入。多模式检索知识（Multimodal Retrieval Knowledge，MRK）分支从相似报告中提取三元组，强调与实体位置和存在相关的关键临床信息。通过将这种更精细和良好结构化的知识与当前图像集成，我们能够利用

    Automatic generation of radiology reports holds crucial clinical value, as it can alleviate substantial workload on radiologists and remind less experienced ones of potential anomalies. Despite the remarkable performance of various image captioning methods in the natural image field, generating accurate reports for medical images still faces challenges, i.e., disparities in visual and textual data, and lack of accurate domain knowledge. To address these issues, we propose an enhanced knowledge injection framework, which utilizes two branches to extract different types of knowledge. The Weighted Concept Knowledge (WCK) branch is responsible for introducing clinical medical concepts weighted by TF-IDF scores. The Multimodal Retrieval Knowledge (MRK) branch extracts triplets from similar reports, emphasizing crucial clinical information related to entity positions and existence. By integrating this finer-grained and well-structured knowledge with the current image, we are able to leverage
    
[^25]: 基于提示的逻辑语义增强对隐含篇章关系识别的改进

    Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition. (arXiv:2311.00367v1 [cs.CL])

    [http://arxiv.org/abs/2311.00367](http://arxiv.org/abs/2311.00367)

    本文提出了一种基于提示的逻辑语义增强（PLSE）方法来改进隐含篇章关系识别（IDRR），通过无缝地将与篇章关系相关的知识注入到预训练语言模型中，从而提高IDRR的性能和稳健性。

    

    隐含篇章关系识别（IDRR）是一项在没有明确连接词帮助下推断篇章关系的关键而具有挑战性的任务。最近的研究倾向于利用注释的语义层次结构信息，证明了通过整合语义层次结构可以获得增强的篇章关系表示。然而，IDRR的性能和稳健性受到注释数据的限制。幸运的是，存在大量带有明确连接词的未注释话语，可以用来获取丰富的篇章关系特征。因此，我们提出了一种基于提示的逻辑语义增强（PLSE）方法来改进IDRR。本方法通过基于提示的连接词预测将与篇章关系相关的知识无缝地注入到预训练语言模型中。此外，考虑到基于提示的连接词预测。

    Implicit Discourse Relation Recognition (IDRR), which infers discourse relations without the help of explicit connectives, is still a crucial and challenging task for discourse parsing. Recent works tend to exploit the hierarchical structure information from the annotated senses, which demonstrate enhanced discourse relation representations can be obtained by integrating sense hierarchy. Nevertheless, the performance and robustness for IDRR are significantly constrained by the availability of annotated data. Fortunately, there is a wealth of unannotated utterances with explicit connectives, that can be utilized to acquire enriched discourse relation features. In light of such motivation, we propose a Prompt-based Logical Semantics Enhancement (PLSE) method for IDRR. Essentially, our method seamlessly injects knowledge relevant to discourse relation into pre-trained language models through prompt-based connective prediction. Furthermore, considering the prompt-based connective predictio
    
[^26]: HARE: 支持逐步推理的可解释性仇恨言论检测

    HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning. (arXiv:2311.00321v1 [cs.CL])

    [http://arxiv.org/abs/2311.00321](http://arxiv.org/abs/2311.00321)

    HARE是一个支持逐步推理的可解释性仇恨言论检测框架，利用大型语言模型填补现有注释方案的推理差距，从而提高了检测模型的监督效果。

    

    随着社交媒体的普及，准确检测仇恨言论变得至关重要以确保在线安全。为了应对细微的仇恨言论形式，重要的是要识别并详细解释仇恨言论，以帮助用户理解其有害影响。最近的基准测试试图通过训练生成模型来处理仇恨文本中含义的自由文本注释，但我们发现现有注释方案存在重大推理差距，这可能会阻碍检测模型的监督。在本文中，我们引入了一种名为HARE的仇恨言论检测框架，该框架利用大型语言模型（LLM）的推理能力来填补这些关于仇恨言论解释的差距，从而实现有效的检测模型监督。在SBIC和Implicit Hate基准测试上的实验证明，我们的方法使用模型生成的数据始终优于使用现有自由文本注释的基准。分析表明，

    With the proliferation of social media, accurate detection of hate speech has become critical to ensure safety online. To combat nuanced forms of hate speech, it is important to identify and thoroughly explain hate speech to help users understand its harmful effects. Recent benchmarks have attempted to tackle this issue by training generative models on free-text annotations of implications in hateful text. However, we find significant reasoning gaps in the existing annotations schemes, which may hinder the supervision of detection models. In this paper, we introduce a hate speech detection framework, HARE, which harnesses the reasoning capabilities of large language models (LLMs) to fill these gaps in explanations of hate speech, thus enabling effective supervision of detection models. Experiments on SBIC and Implicit Hate benchmarks show that our method, using model-generated data, consistently outperforms baselines, using existing free-text human annotations. Analysis demonstrates th
    
[^27]: 用可比较的语料和多个参考文献进行代码翻译的数据增强

    Data Augmentation for Code Translation with Comparable Corpora and Multiple References. (arXiv:2311.00317v1 [cs.CL])

    [http://arxiv.org/abs/2311.00317](http://arxiv.org/abs/2311.00317)

    该论文介绍了两种数据增强方法来改善编程语言之间的代码翻译。通过构建可比较的语料库和增加多个参考翻译，实验结果表明这些方法显著提高了CodeT5在Java、Python和C++之间的翻译准确性。

    

    在编程语言之间进行代码翻译的一个主要挑战是平行训练数据通常有限。为了克服这个挑战，我们提出了两种数据增强技术，一种是构建可比较的语料库（即具有类似功能的代码对），另一种是用多个参考翻译来增强现有的平行数据。具体而言，我们构建并分析了多种类型的可比较的语料库，包括使用代码生成模型从自然语言文档中生成的程序。此外，为了减少对单个参考翻译的过拟合，我们自动生成了可用平行数据的额外翻译参考，并通过单元测试对翻译进行筛选，从而增加了目标翻译的变化。实验证明，我们的数据增强技术显著提高了CodeT5在Java、Python和C++之间的翻译准确性（平均提升了7.5%的计算准确性（CA@1））。

    One major challenge of translating code between programming languages is that parallel training data is often limited. To overcome this challenge, we present two data augmentation techniques, one that builds comparable corpora (i.e., code pairs with similar functionality), and another that augments existing parallel data with multiple reference translations. Specifically, we build and analyze multiple types of comparable corpora, including programs generated from natural language documentation using a code generation model. Furthermore, to reduce overfitting to a single reference translation, we automatically generate additional translation references for available parallel data and filter the translations by unit tests, which increases variation in target translations. Experiments show that our data augmentation techniques significantly improve CodeT5 for translation between Java, Python, and C++ by an average of 7.5% Computational Accuracy (CA@1), which verifies the correctness of tr
    
[^28]: 无监督的词汇简化方法与上下文扩充

    Unsupervised Lexical Simplification with Context Augmentation. (arXiv:2311.00310v1 [cs.CL])

    [http://arxiv.org/abs/2311.00310](http://arxiv.org/abs/2311.00310)

    我们提出了一种无监督的词汇简化方法，使用单语数据和预训练语言模型，通过生成替代词来简化给定目标词与其上下文，实验证明我们的模型在多个语言上优于其他无监督系统，并在SWORDS数据集上取得了最先进的结果。

    

    我们提出了一种新的无监督词汇简化方法，只使用单语数据和预训练语言模型。通过给定目标词和其上下文，我们的方法基于目标上下文和从单语数据中采样的额外上下文生成替代词。我们在TSAR-2022共享任务上使用英语、葡萄牙语和西班牙语进行实验，并表明我们的模型在所有语言上都明显优于其他无监督系统。我们还通过将我们的模型与GPT-3.5进行集成，建立了一个新的最先进的模型。最后，我们在SWORDS词汇替换数据集上评估我们的模型，取得了最先进的结果。

    We propose a new unsupervised lexical simplification method that uses only monolingual data and pre-trained language models. Given a target word and its context, our method generates substitutes based on the target context and also additional contexts sampled from monolingual data. We conduct experiments in English, Portuguese, and Spanish on the TSAR-2022 shared task, and show that our model substantially outperforms other unsupervised systems across all languages. We also establish a new state-of-the-art by ensembling our model with GPT-3.5. Lastly, we evaluate our model on the SWORDS lexical substitution data set, achieving a state-of-the-art result.
    
[^29]: 通过LLM条件文本生成探测显性和隐性性别偏见

    Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation. (arXiv:2311.00306v1 [cs.CL])

    [http://arxiv.org/abs/2311.00306](http://arxiv.org/abs/2311.00306)

    该论文提出了一种无需预定义性别短语和刻板印象的条件文本生成机制，通过三种不同策略生成的三种类型的输入来探测LLMs，展示了LLMs中显性和隐性性别偏见的证据。

    

    大型语言模型（LLMs）可以生成偏见和有毒的回应。然而，大部分现有关于LLM性别偏见评估的工作都需要预定义的与性别相关的短语或性别刻板印象，这些信息难以全面收集且仅限于显性偏见评估。此外，我们认为在输入中没有与性别相关的语言或显性刻板印象的情况下，仍然可以在LLMs中诱发性别偏见。因此，在这项工作中，我们提出了一种无需预定义性别短语和刻板印象的条件文本生成机制。该方法通过三种不同策略生成的三种类型的输入来探测LLMs，旨在展示LLMs中显性和隐性性别偏见的证据。我们还利用显性和隐性评估指标评估了LLMs在不同策略下的性别偏见。我们的实验表明，模型规模的增加并不能一致地带来公平性的提高，所有测试的LLMs都表现出显性的性别偏见。

    Large Language Models (LLMs) can generate biased and toxic responses. Yet most prior work on LLM gender bias evaluation requires predefined gender-related phrases or gender stereotypes, which are challenging to be comprehensively collected and are limited to explicit bias evaluation. In addition, we believe that instances devoid of gender-related language or explicit stereotypes in inputs can still induce gender bias in LLMs. Thus, in this work, we propose a conditional text generation mechanism without the need for predefined gender phrases and stereotypes. This approach employs three types of inputs generated through three distinct strategies to probe LLMs, aiming to show evidence of explicit and implicit gender biases in LLMs. We also utilize explicit and implicit evaluation metrics to evaluate gender bias in LLMs under different strategies. Our experiments demonstrate that an increased model size does not consistently lead to enhanced fairness and all tested LLMs exhibit explicit a
    
[^30]: 使用自注意力模型检测音节级发音重音

    Detecting Syllable-Level Pronunciation Stress with A Self-Attention Model. (arXiv:2311.00301v1 [cs.SD])

    [http://arxiv.org/abs/2311.00301](http://arxiv.org/abs/2311.00301)

    本文提出了一个自注意力模型，用于检测英语音节的重音水平。通过引入各种韵律和分类特征，该模型达到了一定准确度，并展示了在不同场景下的应用潜力。

    

    有效的口头交流的前提是单词要清晰地发音，尤其是对于非英语为母语的人来说更为重要。单词重音是清晰和正确发音的关键，音节重音的错误可能导致误解。因此，了解重音水平对于英语使用者和学习者很重要。本文提出了一个自注意力模型，用于识别英语口语每个音节的重音水平。研究探讨了各种韵律和分类特征，包括音高水平、强度、持续时间和音节及其核心元音（音节中的元音）的类型。这些特征被输入到自注意力模型中，预测音节级的重音。最简单的模型在不同数据集上的准确率超过88%和93%，而更先进的模型提供更高的准确率。我们的研究表明，自注意力模型在重音水平检测方面有着很大的潜力。这些模型可以应用于各种场景，例如在线语音教育、自动语音评估等。

    One precondition of effective oral communication is that words should be pronounced clearly, especially for non-native speakers. Word stress is the key to clear and correct English, and misplacement of syllable stress may lead to misunderstandings. Thus, knowing the stress level is important for English speakers and learners. This paper presents a self-attention model to identify the stress level for each syllable of spoken English. Various prosodic and categorical features, including the pitch level, intensity, duration and type of the syllable and its nuclei (the vowel of the syllable), are explored. These features are input to the self-attention model, and syllable-level stresses are predicted. The simplest model yields an accuracy of over 88% and 93% on different datasets, while more advanced models provide higher accuracy. Our study suggests that the self-attention model can be promising in stress-level detection. These models could be applied to various scenarios, such as online 
    
[^31]: 基于图卷积网络和信息融合的科技专利实体对齐方法

    Entity Alignment Method of Science and Technology Patent based on Graph Convolution Network and Information Fusion. (arXiv:2311.00300v1 [cs.CL])

    [http://arxiv.org/abs/2311.00300](http://arxiv.org/abs/2311.00300)

    本研究提出了一种使用图卷积网络和信息融合的方法来实现科技专利的实体对齐，通过将知识图的结构信息和实体属性信息嵌入和表示，从而提高实体对齐的性能。

    

    科技专利的实体对齐旨在链接不同科技专利数据源的知识图中的等价实体。大多数实体对齐方法仅使用图神经网络获取图结构的嵌入或使用属性文本描述获取语义表示，忽视了科技专利中多信息融合的过程。为了利用图结构和专利实体的名称、描述和属性等辅助信息，本文提出了一种基于图卷积网络的科技专利信息融合的实体对齐方法。通过图卷积网络和BERT模型，将科技专利知识图的结构信息和实体属性信息嵌入和表示，实现多信息融合，从而提高实体对齐的性能。

    The entity alignment of science and technology patents aims to link the equivalent entities in the knowledge graph of different science and technology patent data sources. Most entity alignment methods only use graph neural network to obtain the embedding of graph structure or use attribute text description to obtain semantic representation, ignoring the process of multi-information fusion in science and technology patents. In order to make use of the graphic structure and auxiliary information such as the name, description and attribute of the patent entity, this paper proposes an entity alignment method based on the graph convolution network for science and technology patent information fusion. Through the graph convolution network and BERT model, the structure information and entity attribute information of the science and technology patent knowledge graph are embedded and represented to achieve multi-information fusion, thus improving the performance of entity alignment. Experiment
    
[^32]: 基于自适应特征与图神经网络的科学文献语义表示学习

    Semantic Representation Learning of Scientific Literature based on Adaptive Feature and Graph Neural Network. (arXiv:2311.00296v1 [cs.CL])

    [http://arxiv.org/abs/2311.00296](http://arxiv.org/abs/2311.00296)

    提出了一种基于自适应特征和图神经网络的科学文献语义表示学习方法，通过全局和局部考虑科学文献特征，并使用图注意机制对具有引用关系的特征进行加权求和，以更好地表达不同科学文献特征之间的关联。

    

    由于大部分科学文献数据未标记，因此基于无监督图的语义表示学习变得至关重要。同时，为了丰富科学文献的特征，提出了一种基于自适应特征和图神经网络的科学文献语义表示学习方法。通过引入自适应特征方法，全局和局部考虑科学文献的特征。使用图注意机制对具有引用关系的科学文献特征进行求和，并给予每个科学文献不同的特征权重，以更好地表达不同科学文献特征之间的关联。此外，还提出了一种无监督图神经网络语义表示学习方法，通过比较正负局部语义表示与全局图语义表示之间的互信息来学习。

    Because most of the scientific literature data is unmarked, it makes semantic representation learning based on unsupervised graph become crucial. At the same time, in order to enrich the features of scientific literature, a learning method of semantic representation of scientific literature based on adaptive features and graph neural network is proposed. By introducing the adaptive feature method, the features of scientific literature are considered globally and locally. The graph attention mechanism is used to sum the features of scientific literature with citation relationship, and give each scientific literature different feature weights, so as to better express the correlation between the features of different scientific literature. In addition, an unsupervised graph neural network semantic representation learning method is proposed. By comparing the mutual information between the positive and negative local semantic representation of scientific literature and the global graph sema
    
[^33]: IBADR：一种迭代偏见感知的数据集细化框架，用于去偏自然语言理解模型

    IBADR: an Iterative Bias-Aware Dataset Refinement Framework for Debiasing NLU models. (arXiv:2311.00292v1 [cs.CL])

    [http://arxiv.org/abs/2311.00292](http://arxiv.org/abs/2311.00292)

    本文提出了一种迭代偏见感知的数据集细化框架IBADR，用于去偏自然语言理解（NLU）模型。该框架可以在不预定义有偏特征的情况下去偏模型，通过迭代扩展样本池并使用样本生成器生成伪样本，从而有效地去除有偏特征。

    

    去偏自然语言理解（NLU）模型的常用方法是数据集细化方法，它们严重依赖于手动数据分析，因此可能无法覆盖所有潜在的有偏特征。在本文中，我们提出了IBADR，一种迭代偏见感知的数据集细化框架，可以去偏NLU模型而不需要预定义有偏特征。我们维护一个迭代扩展的样本池。具体而言，在每次迭代中，我们首先训练一个浅层模型来量化样本池中样本的偏见程度。然后，我们将每个样本与表示其偏见程度的偏见指示器配对，并使用这些扩展的样本来训练一个样本生成器。通过这种方式，该生成器可以有效地学习偏见指示器和样本之间的对应关系。此外，我们使用生成器通过提供特定的偏见指示器来生成具有较少有偏特征的伪样本。最后，我们将生成的伪样本并入样本池中。

    As commonly-used methods for debiasing natural language understanding (NLU) models, dataset refinement approaches heavily rely on manual data analysis, and thus maybe unable to cover all the potential biased features. In this paper, we propose IBADR, an Iterative Bias-Aware Dataset Refinement framework, which debiases NLU models without predefining biased features. We maintain an iteratively expanded sample pool. Specifically, at each iteration, we first train a shallow model to quantify the bias degree of samples in the pool. Then, we pair each sample with a bias indicator representing its bias degree, and use these extended samples to train a sample generator. In this way, this generator can effectively learn the correspondence relationship between bias indicators and samples. Furthermore, we employ the generator to produce pseudo samples with fewer biased features by feeding specific bias indicators. Finally, we incorporate the generated pseudo samples into the pool. Experimental re
    
[^34]: 主动指令调优：通过在敏感指令任务上训练来提高跨任务泛化能力

    Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks. (arXiv:2311.00288v1 [cs.CL])

    [http://arxiv.org/abs/2311.00288](http://arxiv.org/abs/2311.00288)

    本文提出了基于提示不确定性的主动指令调优方法，通过选择信息丰富的任务并主动调整模型，提高了跨任务泛化能力。

    

    指令调优（IT）通过在大量多样的任务上使用指令对大型语言模型（LLM）进行训练，取得了令人印象深刻的零样本泛化结果。然而，如何选择新任务以提高IT模型的性能和泛化能力仍然是一个未解决的问题。由于计算要求过高，训练所有现有任务是不可行的，而随机选择任务可能会导致亚优性能。在这项工作中，我们提出了基于提示不确定性的主动指令调优，一种识别信息丰富任务并在选定任务上主动调整模型的新框架。我们用当前模型输出在扰动提示上的不一致性表示新任务的信息丰富性。我们在NIV2和Self-Instruct数据集上的实验证明，我们的方法始终优于其他基准策略的任务选择，同时在更少的训练任务下实现了更好的超出分布的泛化能力。

    Instruction tuning (IT) achieves impressive zero-shot generalization results by training large language models (LLMs) on a massive amount of diverse tasks with instructions. However, how to select new tasks to improve the performance and generalizability of IT models remains an open question. Training on all existing tasks is impractical due to prohibiting computation requirements, and randomly selecting tasks can lead to suboptimal performance. In this work, we propose active instruction tuning based on prompt uncertainty, a novel framework to identify informative tasks, and then actively tune the models on the selected tasks. We represent the informativeness of new tasks with the disagreement of the current model outputs over perturbed prompts. Our experiments on NIV2 and Self-Instruct datasets demonstrate that our method consistently outperforms other baseline strategies for task selection, achieving better out-of-distribution generalization with fewer training tasks. Additionally, 
    
[^35]: 通过大型语言模型的知识注入：评估和推进临床文本数据生成

    Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models. (arXiv:2311.00287v1 [cs.CL])

    [http://arxiv.org/abs/2311.00287](http://arxiv.org/abs/2311.00287)

    本文提出了一种通过大型语言模型进行临床文本生成的创新方法ClinGen，该方法将外部领域特定的知识和语言模型结合起来，提高了临床自然语言处理任务的性能，并丰富了样本的多样性。

    

    临床自然语言处理需要能够应对领域特定挑战的方法，例如复杂的医学术语和临床背景。最近，大型语言模型（LLMs）在这个领域显示出了潜力。然而，它们的直接部署可能导致隐私问题，并受到资源限制。为了解决这个挑战，我们深入研究了使用LLMs进行临床NLP任务的合成临床文本生成。我们提出了一种创新的、资源高效的方法ClinGen，它将知识注入到这个过程中。我们的模型涉及临床知识提取和基于上下文的LLM提示。临床主题和写作风格都来自外部领域特定的知识图谱和LLMs，以引导数据生成。我们在7个临床NLP任务和16个数据集上进行了广泛的实证研究，结果显示ClinGen在各种任务中始终提高了性能，有效地使真实数据集的分布对齐，并显著丰富了样本的多样性。

    Clinical natural language processing requires methods that can address domain-specific challenges, such as complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation using LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, ClinGen, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 7 clinical NLP tasks and 16 datasets reveals that ClinGen consistently enhances performance across various tasks, effectively aligning the distribution of real datasets and significantly enriching the divers
    
[^36]: JADE：基于语言的LLM安全评估平台

    JADE: A Linguistic-based Safety Evaluation Platform for LLM. (arXiv:2311.00286v1 [cs.CL])

    [http://arxiv.org/abs/2311.00286](http://arxiv.org/abs/2311.00286)

    JADE是一种基于语言分析的LLM安全评估平台，能够破坏广泛使用的中文和英文LLM，并生成高度威胁的不安全问题。

    

    本文介绍了JADE，一种针对语言分析的模糊测试平台，通过增强种子问题的语言复杂性，同时并始终能够破坏广泛使用的三类LLM：八个开源中文LLM，六个商业中文LLM和四个商业英文LLM。JADE为这三类LLM生成了三个安全基准，其中包含高度威胁的不安全问题：这些问题可以同时触发多个LLM的有害生成，平均不安全生成比例为70%（请参见下表），同时这些问题仍然是自然、流畅且保留了核心的不安全语义。我们在以下链接中发布了对商业英文LLM和开源英文LLM生成的基准演示：https://github.com/whitzard-ai/jade-db。对于对JADE生成的更多问题感兴趣的读者，请与我们联系。

    In this paper, we present \textit{JADE}, a targeted linguistic fuzzing platform which strengthens the linguistic complexity of seed questions to simultaneously and consistently break a wide range of widely-used LLMs categorized in three groups: eight open-sourced Chinese, six commercial Chinese and four commercial English LLMs. JADE generates three safety benchmarks for the three groups of LLMs, which contain unsafe questions that are highly threatening: the questions simultaneously trigger harmful generation of multiple LLMs, with an average unsafe generation ratio of \textbf{$70\%$} (please see the table below), while are still natural questions, fluent and preserving the core unsafe semantics. We release the benchmark demos generated for commercial English LLMs and open-sourced English LLMs in the following link: https://github.com/whitzard-ai/jade-db. For readers who are interested in evaluating on more questions generated by JADE, please contact us.  \textit{JADE} is based on Noam
    
[^37]: SoulChat: 通过多轮共情对话微调来提高LLMs的共情、倾听和安慰能力

    SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations. (arXiv:2311.00273v1 [cs.CL])

    [http://arxiv.org/abs/2311.00273](http://arxiv.org/abs/2311.00273)

    本研究通过在心理咨询领域构建多轮共情对话数据集，并利用更接近心理咨询师表达方式的对话历史和回应进行微调，成功提高了LLMs的共情能力。

    

    由于其出色的知识和思维链记忆能力，大型语言模型（LLMs）已广泛应用于各个领域。当这些语言模型应用于心理咨询领域时，它们常常急于提供普遍的建议。然而，当用户寻求心理支持时，他们需要获得共情、信任、理解和安慰，而不仅仅是合理的建议。为此，我们构建了一个包含超过200万个样本的多轮共情对话数据集，其中输入是多轮对话的上下文，目标是包括询问、安慰、认可、倾听、信任、情绪支持等表达的共情回应。实验证明，通过使用更接近心理咨询师表达方式的多轮对话历史和回应进行微调，可以显著增强LLMs的共情能力。

    Large language models (LLMs) have been widely applied in various fields due to their excellent capability for memorizing knowledge and chain of thought (CoT). When these language models are applied in the field of psychological counseling, they often rush to provide universal advice. However, when users seek psychological support, they need to gain empathy, trust, understanding and comfort, rather than just reasonable advice. To this end, we constructed a multi-turn empathetic conversation dataset of more than 2 million samples, in which the input is the multi-turn conversation context, and the target is empathetic responses that cover expressions such as questioning, comfort, recognition, listening, trust, emotional support, etc. Experiments have shown that the empathy ability of LLMs can be significantly enhanced when finetuning by using multi-turn dialogue history and responses that are closer to the expression of a psychological consultant.
    
[^38]: Transformer语言模型中的句法归纳偏好：对低资源语言特别有帮助吗？

    Syntactic Inductive Bias in Transformer Language Models: Especially Helpful for Low-Resource Languages?. (arXiv:2311.00268v1 [cs.CL])

    [http://arxiv.org/abs/2311.00268](http://arxiv.org/abs/2311.00268)

    Transformer语言模型的句法归纳偏好用于增强预训练过程，但在低资源语言中效果不佳。

    

    一系列基于Transformer的语言模型，如BERT，尝试使用句法归纳偏好来增强预训练过程，其理论是在训练过程中构建句法结构应该能减少所需的训练数据量。但是这些方法通常是针对高资源语言，如英语进行测试。在这项工作中，我们研究了这些方法是否能够弥补低资源语言中数据稀缺性的问题，假设它们对于低资源语言应该更有效。我们对五种低资源语言进行了实验：维吾尔语、沃洛夫语、马耳他语、科普特语和古希腊语。我们发现在低资源环境中，这些句法归纳偏好方法产生了不平衡的结果，并且在大多数情况下提供了令人惊讶地少的好处。

    A line of work on Transformer-based language models such as BERT has attempted to use syntactic inductive bias to enhance the pretraining process, on the theory that building syntactic structure into the training process should reduce the amount of data needed for training. But such methods are often tested for high-resource languages such as English. In this work, we investigate whether these methods can compensate for data sparseness in low-resource languages, hypothesizing that they ought to be more effective for low-resource languages. We experiment with five low-resource languages: Uyghur, Wolof, Maltese, Coptic, and Ancient Greek. We find that these syntactic inductive bias methods produce uneven results in low-resource settings, and provide surprisingly little benefit in most cases.
    
[^39]: 基于大型语言模型的对话代理的即插即用策略规划器

    Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents. (arXiv:2311.00262v1 [cs.CL])

    [http://arxiv.org/abs/2311.00262](http://arxiv.org/abs/2311.00262)

    基于大型语言模型的对话代理的即插即用策略规划器(PDDPP)引入了一种新的对话策略规划范式，通过可调整的语言模型插件实现主动对话问题的策略制定。利用监督微调和强化学习，该框架在处理新的案例时具有较高的灵活性和性能。

    

    在大型语言模型（LLMs）的时代中，主动对话作为一个实际但具有挑战性的对话问题，对话策略规划是提高LLMs主动性的关键。大多数现有研究使用各种提示方案或通过语言人工智能反馈迭代增强对LLMs的对话策略规划能力。然而，这些方法要么受限于冻结的LLMs的策略规划能力，要么难以转移到新的案例。在这项工作中，我们引入了一种新的对话策略规划范式，以使用可调整的语言模型插件作为即插即用的对话策略规划器来制定LLMs在主动对话问题上的策略，命名为PPDPP。具体而言，我们开发了一个新颖的训练框架，以便利用可用的人工注释数据进行监督微调，并通过基于LLM的自我对弈收集的动态交互数据进行强化学习。

    Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play s
    
[^40]: 有噪声的样本使得大型语言模型更加鲁棒：一个领域不可知的行为分析

    Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis. (arXiv:2311.00258v1 [cs.CL])

    [http://arxiv.org/abs/2311.00258](http://arxiv.org/abs/2311.00258)

    本研究通过领域不可知的扰动测试了大型语言模型在多跳推理任务中的鲁棒性，发现模型对某些扰动更敏感，并证明增加扰动样本的比例可以提高少样本提示方法的鲁棒性。

    

    最近对问题引导的工程进展使得大型语言模型（LLMs）能够以令人印象深刻的准确率解决多跳逻辑推理问题。然而，目前很少有工作研究少样本提示技术下LLMs的鲁棒性。因此，我们介绍了一种系统的方法，通过领域不可知的扰动来测试LLMs在多跳推理任务中的鲁棒性。我们在多个抽象层次上引入扰动（例如词法扰动，如拼写错误，以及语义扰动，如在问题中包含中间推理步骤），对LLMs进行行为分析。通过实验，我们发现模型对某些扰动（如用同义词替换单词）更敏感。我们还证明，在提示中增加扰动样本的比例可以提高少样本提示方法的鲁棒性。

    Recent advances in prompt engineering enable large language models (LLMs) to solve multi-hop logical reasoning problems with impressive accuracy. However, there is little existing work investigating the robustness of LLMs with few-shot prompting techniques. Therefore, we introduce a systematic approach to test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic perturbations. We include perturbations at multiple levels of abstractions (e.g. lexical perturbations such as typos, and semantic perturbations such as the inclusion of intermediate reasoning steps in the questions) to conduct behavioral analysis on the LLMs. Throughout our experiments, we find that models are more sensitive to certain perturbations such as replacing words with their synonyms. We also demonstrate that increasing the proportion of perturbed exemplars in the prompts improves the robustness of few-shot prompting methods.
    
[^41]: LLMs的神秘和迷人之处：紧密调查对新兴能力的解释和分析

    The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities. (arXiv:2311.00237v1 [cs.CL])

    [http://arxiv.org/abs/2311.00237](http://arxiv.org/abs/2311.00237)

    该论文对LLMs的新兴能力的解释和分析进行了全面调查，旨在理解这些能力的机制和实际应用，并解决可能出现的潜在风险和担忧。

    

    理解新兴能力，如在大型语言模型（LLMs）中的上下文学习(ICL)和思维链(CoT)触发，至关重要。这种重要性不仅来自于在各种任务中更好地利用这些能力，还包括主动识别和缓解可能出现的潜在风险，包括真实性、偏见和有害性的担忧。本文在LLMs的新兴能力解释和分析方面提出了一项深入调查。首先，我们简要介绍了新兴能力的背景和定义。然后，我们从两个角度概述了研究的进展：1)宏观角度，强调对机制可解释性的研究，并深入探讨新兴能力背后的数学基础；2)微观角度，关注通过考察与这些能力相关的因素来实证可解释性的研究。

    Understanding emergent abilities, such as in-context learning (ICL) and chain-of-thought (CoT) prompting in large language models (LLMs), is of utmost importance. This importance stems not only from the better utilization of these capabilities across various tasks, but also from the proactive identification and mitigation of potential risks, including concerns of truthfulness, bias, and toxicity, that may arise alongside these capabilities. In this paper, we present a thorough survey on the interpretation and analysis of emergent abilities of LLMs. First, we provide a concise introduction to the background and definition of emergent abilities. Then, we give an overview of advancements from two perspectives: 1) a macro perspective, emphasizing studies on the mechanistic interpretability and delving into the mathematical foundations behind emergent abilities; and 2) a micro-perspective, concerning studies that focus on empirical interpretability by examining factors associated with these
    
[^42]: 扭曲，分散，解码：指令调节模型能够根据嘈杂的指令改进其响应

    Distort, Distract, Decode: Instruction-Tuned Model Can Refine its Response from Noisy Instructions. (arXiv:2311.00233v1 [cs.CL])

    [http://arxiv.org/abs/2311.00233](http://arxiv.org/abs/2311.00233)

    本文提出了一种简单而有效的方法，通过对比性调整预测，利用嘈杂指令来增强指令调节模型的效果，从而改进了面临超出训练范围的指令时的响应准确性。

    

    虽然经过指令调节的语言模型在零-shot泛化方面表现出色，但是当面临超出训练范围的指令时，这些模型往往难以生成准确的响应。本文提出了Instructive Decoding（ID）的方法，这是一种简单且有效的方法，可以增强指令调节模型的效果。具体而言，ID通过对下一个标记预测的logits进行对比性调整，利用从原始指令的操纵版本生成的预测，即嘈杂指令。这个嘈杂指令旨在引发与预期指令不一致但仍然合理的响应。我们在一系列此类嘈杂指令上进行了实验，从通过随机词插入语义噪声到像“相反”的其他指令，以引发偏离的响应。我们的方法在各种指令调节模型和任务中取得了相当大的性能提升，而无需

    While instruction-tuned language models have demonstrated impressive zero-shot generalization, these models often struggle to generate accurate responses when faced with instructions that fall outside their training set. This paper presents Instructive Decoding (ID), a simple yet effective approach that augments the efficacy of instruction-tuned models. Specifically, ID adjusts the logits for next-token prediction in a contrastive manner, utilizing predictions generated from a manipulated version of the original instruction, referred to as a noisy instruction. This noisy instruction aims to elicit responses that could diverge from the intended instruction yet remain plausible. We conduct experiments across a spectrum of such noisy instructions, ranging from those that insert semantic noise via random words to others like 'opposite' that elicit the deviated responses. Our approach achieves considerable performance gains across various instruction-tuned models and tasks without necessita
    
[^43]: GPT有足够的能力分析表情包的情绪吗？

    Is GPT Powerful Enough to Analyze the Emotions of Memes?. (arXiv:2311.00223v1 [cs.CL])

    [http://arxiv.org/abs/2311.00223](http://arxiv.org/abs/2311.00223)

    该论文研究了GPT-3.5在处理互联网表情包情感分析方面的能力，包括情感分类、幽默类型确定和憎恶表情包检测，揭示了其优势和潜在限制。

    

    大型语言模型(LLMs)是人工智能研究的重要成果，展示了在多种任务上的能力。本项目旨在探索GPT-3.5这一领先的LLM在处理互联网表情包情感分析方面的能力。表情包作为一种同时包含语言和视觉元素的强大而复杂的工具，能够表达思想和情感，要求理解社会规范和文化背景。值得注意的是，由于隐含的冒犯性质，憎恶表情包的检测和审查具有重大挑战性。本项目调查了GPT在这种主观任务中的熟练程度，揭示了其优势和潜在限制。任务包括表情包情感分类、幽默类型确定和隐含憎恶表情包检测。通过使用SemEval-2020 Task 8和Facebook憎恶表情包的数据集进行性能评估，提供了一个比较性的理解。

    Large Language Models (LLMs), representing a significant achievement in artificial intelligence (AI) research, have demonstrated their ability in a multitude of tasks. This project aims to explore the capabilities of GPT-3.5, a leading example of LLMs, in processing the sentiment analysis of Internet memes. Memes, which include both verbal and visual aspects, act as a powerful yet complex tool for expressing ideas and sentiments, demanding an understanding of societal norms and cultural contexts. Notably, the detection and moderation of hateful memes pose a significant challenge due to their implicit offensive nature. This project investigates GPT's proficiency in such subjective tasks, revealing its strengths and potential limitations. The tasks include the classification of meme sentiment, determination of humor type, and detection of implicit hate in memes. The performance evaluation, using datasets from SemEval-2020 Task 8 and Facebook hateful memes, offers a comparative understand
    
[^44]: Transformers作为形式语言识别器：关于表达能力的调查

    Transformers as Recognizers of Formal Languages: A Survey on Expressivity. (arXiv:2311.00208v1 [cs.LG])

    [http://arxiv.org/abs/2311.00208](http://arxiv.org/abs/2311.00208)

    本文对transformers在形式语言识别领域的相关研究进行了全面调查，为理解其表达能力提供了一个统一的框架。

    

    随着transformers在自然语言处理中的重要性日益突出，一些研究人员开始从理论上探讨它们能否解决问题，将问题视为形式语言。探索这类问题将有助于比较transformers与其他模型以及不同变种之间的差异，适用于各种任务。近年来，在这个子领域的工作取得了相当大的进展。本文对这方面的工作进行了全面调查，记录了不同结果背后的各种假设，并提供了一个统一的框架，以协调看似相互矛盾的研究结果。

    As transformers have gained prominence in natural language processing, some researchers have investigated theoretically what problems they can and cannot solve, by treating problems as formal languages. Exploring questions such as this will help to compare transformers with other models, and transformer variants with one another, for various tasks. Work in this subarea has made considerable progress in recent years. Here, we undertake a comprehensive survey of this work, documenting the diverse assumptions that underlie different results and providing a unified framework for harmonizing seemingly contradictory findings.
    
[^45]: 在医学问题回答中，领域特定语言模型的连续训练和微调

    Continuous Training and Fine-tuning for Domain-Specific Language Models in Medical Question Answering. (arXiv:2311.00204v1 [cs.CL])

    [http://arxiv.org/abs/2311.00204](http://arxiv.org/abs/2311.00204)

    本研究提出了一种连续训练和微调的方法，通过在中文医学数据上进行训练和微调，快速适应Llama 2基础模型到中文医学领域。实验结果表明，这种方法有效，生成的模型与GPT-3.5-turbo相媲美，但使用的计算资源更少。这为在不同领域进行大型语言模型的领域特定训练提供了一个模板。

    

    大型语言模型具有良好的通用能力，但往往缺乏领域特定任务的专业知识。通过从基础模型中开发领域专家，可以在不会造成过高训练成本的情况下实现多种应用。本研究展示了一种使用连续训练和指导微调的方法，以快速适应中文医学领域的Llama 2基础模型。我们首先对来自中国医学参考资料的10亿个标记进行连续训练，以教授相关的词汇和知识。然后，我们在来自中国国家医疗执业医师资格考试的5.4万个示例上对模型进行微调。在中文医学数据上的实验验证了这种方法的有效性，生成了一个与GPT-3.5-turbo相媲美的模型，同时使用的计算资源要少得多。最终得到的领域特定模型可以在各种中文医学应用中发挥作用。更广泛地说，这为在缺乏专业知识的领域中进行大型语言模型的领域特定训练提供了一个模板。

    Large language models exhibit promising general capabilities but often lack specialized knowledge for domain-specific tasks. Developing domain experts from a base model enables a range of applications without prohibitive training costs. This work demonstrates a method using continuous training and instruction fine-tuning to rapidly adapt Llama 2 base models to the Chinese medical domain. We first conduct continuous training on 1B tokens from Chinese medical references to teach relevant vocabulary and knowledge. The models are then fine-tuned on 54K examples sourced from the Chinese National Medical Licensing Examination. Experiments on Chinese medical data confirm the effectiveness of this approach, producing a model comparable to GPT-3.5-turbo while using way less computational resource. The resulting domain-specific model could be useful for various Chinese medical applications. More broadly, this provides a template for domain-specific training of large language models in areas wher
    
[^46]: XAI-CLASS：具有极弱监督的解释增强文本分类

    XAI-CLASS: Explanation-Enhanced Text Classification with Extremely Weak Supervision. (arXiv:2311.00189v1 [cs.CL])

    [http://arxiv.org/abs/2311.00189](http://arxiv.org/abs/2311.00189)

    XAI-CLASS是一种解释增强的极弱监督文本分类方法，通过在训练过程中结合生成的伪标签的解释和单词的显著性，实现了在最小或没有人工注释的情况下进行文本分类。

    

    文本分类旨在将文档有效地分类到预定义的类别中。传统的文本分类方法通常依赖于大量手动注释的训练数据，使得这个过程耗时且劳动密集。为了解决这个问题，最近的研究集中在弱监督和极弱监督环境中，分别需要最少或没有人工注释。在先前的弱监督文本分类方法中，通过将伪标签分配给与特定类别对齐（例如，关键词匹配）的文档来生成伪训练数据。然而，这些方法忽略了在文本分类训练过程中将生成的伪标签的解释或个体单词的显著性作为额外指导的重要性。为了解决这个局限性，我们提出了XAI-CLASS，这是一种新颖的解释增强极弱监督文本分类方法。

    Text classification aims to effectively categorize documents into pre-defined categories. Traditional methods for text classification often rely on large amounts of manually annotated training data, making the process time-consuming and labor-intensive. To address this issue, recent studies have focused on weakly-supervised and extremely weakly-supervised settings, which require minimal or no human annotation, respectively. In previous methods of weakly supervised text classification, pseudo-training data is generated by assigning pseudo-labels to documents based on their alignment (e.g., keyword matching) with specific classes. However, these methods ignore the importance of incorporating the explanations of the generated pseudo-labels, or saliency of individual words, as additional guidance during the text classification training process. To address this limitation, we propose XAI-CLASS, a novel explanation-enhanced extremely weakly-supervised text classification method that incorpor
    
[^47]: ChipNeMo: 用于芯片设计的领域自适应LLMs

    ChipNeMo: Domain-Adapted LLMs for Chip Design. (arXiv:2311.00176v1 [cs.CL])

    [http://arxiv.org/abs/2311.00176](http://arxiv.org/abs/2311.00176)

    ChipNeMo通过领域自适应技术，实现了在工业芯片设计中大幅提升LLM性能，同时减小了模型尺寸，在工程助手、脚本生成和缺陷分析等方面具有良好表现。

    

    ChipNeMo旨在探索大型语言模型（LLMs）在工业芯片设计中的应用。我们不直接使用商业或开源LLMs，而是采用以下领域自适应技术：定制分词器、领域自适应持续预训练、带有领域特定指令的监督微调（SFT）和领域自适应检索模型。我们在芯片设计的三个选定LLM应用上评估了这些方法：工程助手聊天机器人、EDA脚本生成以及缺陷摘要和分析。我们的结果显示，这些领域自适应技术使LLM在这三个应用中性能大幅提升，在各种设计任务上可以实现高达5倍的模型尺寸缩减，同时具有类似或更好的性能。我们的研究结果还表明，当前的结果和理想结果之间还有改进的空间。我们相信进一步的研究将有助于解决这个问题。

    ChipNeMo aims to explore the applications of large language models (LLMs) for industrial chip design. Instead of directly deploying off-the-shelf commercial or open-source LLMs, we instead adopt the following domain adaptation techniques: custom tokenizers, domain-adaptive continued pretraining, supervised fine-tuning (SFT) with domain-specific instructions, and domain-adapted retrieval models. We evaluate these methods on three selected LLM applications for chip design: an engineering assistant chatbot, EDA script generation, and bug summarization and analysis. Our results show that these domain adaptation techniques enable significant LLM performance improvements over general-purpose base models across the three evaluated applications, enabling up to 5x model size reduction with similar or better performance on a range of design tasks. Our findings also indicate that there's still room for improvement between our current results and ideal outcomes. We believe that further investigati
    
[^48]: 大型语言模型的鲁棒安全分类器：对抗性提示屏蔽

    Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield. (arXiv:2311.00172v1 [cs.CL])

    [http://arxiv.org/abs/2311.00172](http://arxiv.org/abs/2311.00172)

    本研究介绍了对抗性提示屏蔽（APS）模型以及自动生成对抗性训练数据集的新策略。APS模型在检测准确性方面表现出色且具有韧性，并且BAND数据集可以增强安全分类器的鲁棒性。

    

    大型语言模型的安全性仍然是一个重要关注点，因为它们容易受到对抗性攻击的影响，这可能导致这些系统产生有害的回应。在这些系统的核心是一个安全分类器，这是一个计算模型，被训练来辨别和减轻潜在的有害、冒犯或不道德的输出。然而，尽管潜力巨大，现代的安全分类器通常在暴露于充满对抗性噪声的输入时失败。为此，我们的研究引入了对抗性提示屏蔽（APS），这是一个轻量级模型，在检测准确性方面表现出色，并展示了对抗性提示的韧性。此外，我们提出了自动生成对抗性训练数据集的新策略，称为Bot Adversarial Noisy Dialogue (BAND) 数据集。这些数据集旨在增强安全分类器的鲁棒性，并研究了将对抗性样本纳入训练过程的后果。通过评估实验证明...

    Large Language Models' safety remains a critical concern due to their vulnerability to adversarial attacks, which can prompt these systems to produce harmful responses. In the heart of these systems lies a safety classifier, a computational model trained to discern and mitigate potentially harmful, offensive, or unethical outputs. However, contemporary safety classifiers, despite their potential, often fail when exposed to inputs infused with adversarial noise. In response, our study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts. Additionally, we propose novel strategies for autonomously generating adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are designed to fortify the safety classifier's robustness, and we investigate the consequences of incorporating adversarial examples into the training process. Through evaluations i
    
[^49]: 超越批判性仇恨：解决语言中隐含的偏见和刻板印象的策略

    Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language. (arXiv:2311.00161v1 [cs.CL])

    [http://arxiv.org/abs/2311.00161](http://arxiv.org/abs/2311.00161)

    本研究通过心理学和哲学文献提出六种挑战恶意语言刻板印象的策略，并通过人类和机器生成的对话数据集对其有效性进行比较。结果显示，人类编写的对话使用更具体的策略，而机器生成的对话使用较为普遍的策略。

    

    反对怀有恶意的言论的对话已经成为一种越来越受欢迎的解决方式，以避免对线上恶意言论进行审查。然而，适当地对付恶意语言需要打破和驳斥这些语言所暗示的不准确刻板印象。在这项研究中，我们从心理学和哲学文献中汲取灵感，提出了六种心理学上启发的策略来挑战恶意语言所暗示的刻板印象。我们首先通过用户研究来研究每种策略的说服力，然后比较人类和机器生成的对话数据集中使用这些策略的情况。我们的结果表明，人类编写的对话使用了更与所暗示的刻板印象相关的挑战策略（例如，刻板印象的反例，关于刻板印象来源的外部因素），而机器生成的对话使用了更不具体的策略（例如，普遍抨击刻板印象）。

    Counterspeech, i.e., responses to counteract potential harms of hateful speech, has become an increasingly popular solution to address online hate speech without censorship. However, properly countering hateful language requires countering and dispelling the underlying inaccurate stereotypes implied by such language. In this work, we draw from psychology and philosophy literature to craft six psychologically inspired strategies to challenge the underlying stereotypical implications of hateful language. We first examine the convincingness of each of these strategies through a user study, and then compare their usages in both human- and machine-generated counterspeech datasets. Our results show that human-written counterspeech uses countering strategies that are more specific to the implied stereotype (e.g., counter examples to the stereotype, external factors about the stereotype's origins), whereas machine-generated counterspeech uses less specific strategies (e.g., generally denouncin
    
[^50]: Fixations for Longer Time, More Computation: Gaze-Guided Recurrent Neural Networks

    Longer Fixations, More Computation: Gaze-Guided Recurrent Neural Networks. (arXiv:2311.00159v1 [cs.CL])

    [http://arxiv.org/abs/2311.00159](http://arxiv.org/abs/2311.00159)

    本文研究了通过注视来指导语言模型的方法。实验结果表明，模型表现出了类似于人类的注视行为，并且通过注视来引导语言模型，取得了好的性能。

    

    人类在阅读文本时的速度是不一样的，然而机器学习模型在计算过程中对于每个标记都是以相同的方式处理的。因此，我们问，让模型更像人类是否有助于提升性能？在本文中，我们将这个直觉转化为一组新颖的模型，通过注视导向的并行RNN或层来进行实验，以验证它们在语言建模和情感分析任务中的有效性，从而提供经验验证结果。我们提出的模型在语言建模任务上表现出了很好的性能，明显超过了基线模型。此外，我们发现，有趣的是，神经网络预测的注视持续时间与人类的注视有一定的相似之处。在没有任何明确指导的情况下，模型往往会做出与人类相似的选择。我们还调查了它们之间差异的原因，解释了为什么“模型注视”在引导语言模型时常常比人类注视更合适。

    Humans read texts at a varying pace, while machine learning models treat each token in the same way in terms of a computational process. Therefore, we ask, does it help to make models act more like humans? In this paper, we convert this intuition into a set of novel models with fixation-guided parallel RNNs or layers and conduct various experiments on language modeling and sentiment analysis tasks to test their effectiveness, thus providing empirical validation for this intuition. Our proposed models achieve good performance on the language modeling task, considerably surpassing the baseline model. In addition, we find that, interestingly, the fixation duration predicted by neural networks bears some resemblance to humans' fixation. Without any explicit guidance, the model makes similar choices to humans. We also investigate the reasons for the differences between them, which explain why "model fixations" are often more suitable than human fixations, when used to guide language models.
    
[^51]: 伊朗2021年总统选举期间，使用轴嵌入的两阶段分类器进行政治用户推文中的负面情绪检测：案例研究

    Two-Stage Classifier for Campaign Negativity Detection using Axis Embeddings: A Case Study on Tweets of Political Users during 2021 Presidential Election in Iran. (arXiv:2311.00143v1 [cs.LG])

    [http://arxiv.org/abs/2311.00143](http://arxiv.org/abs/2311.00143)

    本文提出了一个混合模型，用于检测竞选活动的负面情绪，包括一个两阶段分类器，结合了两个机器学习模型的优势。通过对伊朗2021年总统选举期间50名政治用户的5,100条波斯语推文进行标注，建立了所需的数据集。

    

    在全球各地的选举中，候选人可能会因失败前景和时间压力而将他们的竞选活动转向负面情绪。在数字时代，Twitter等社交媒体平台是政治话语的丰富来源。因此，尽管Twitter上发布了大量数据，但自动化的竞选负面情绪检测系统在理解候选人和政党在竞选活动中的策略方面起着至关重要的作用。在本文中，我们提出了一个混合模型，用于检测竞选活动的负面情绪，包括一个两阶段分类器，结合了两个机器学习模型的优势。在此，我们收集了来自50名政治用户（包括候选人和政府官员）的波斯语推文。然后，我们标注了其中5,100条推文，这些推文是在伊朗2021年总统选举前的一年内发布的。在提出的模型中，首先通过推文嵌入与轴的余弦相似性来建立两个分类器所需的数据集。

    In elections around the world, the candidates may turn their campaigns toward negativity due to the prospect of failure and time pressure. In the digital age, social media platforms such as Twitter are rich sources of political discourse. Therefore, despite the large amount of data that is published on Twitter, the automatic system for campaign negativity detection can play an essential role in understanding the strategy of candidates and parties in their campaigns. In this paper, we propose a hybrid model for detecting campaign negativity consisting of a two-stage classifier that combines the strengths of two machine learning models. Here, we have collected Persian tweets from 50 political users, including candidates and government officials. Then we annotated 5,100 of them that were published during the year before the 2021 presidential election in Iran. In the proposed model, first, the required datasets of two classifiers based on the cosine similarity of tweet embeddings with axis
    
[^52]: 关于使用发展性数据进行语法习得的课程学习效果研究

    On the effect of curriculum learning with developmental data for grammar acquisition. (arXiv:2311.00128v1 [cs.CL])

    [http://arxiv.org/abs/2311.00128](http://arxiv.org/abs/2311.00128)

    这项研究发现语法习得主要受到对语音数据的暴露驱动，并通过课程学习方法进一步提高其性能。

    

    本研究探讨了语法习得在语言“简单性”和数据的来源模态（语音 vs 文本）方面的影响程度。通过使用BabyBERTa作为探针，我们发现语法习得主要受到对语音数据的暴露的驱动，尤其是通过对两个BabyLM训练数据集（AO-Childes和Open Subtitles）的暴露。我们通过检查将输入数据以不同方式呈现给模型的方法得出了这一发现。首先，我们评估了基于序列级复杂性的学习计划的影响。然后，我们研究了学习“块”的影响——这些块覆盖了源数据集中每个语料库中每个标记数量平衡的文本范围。最后，我们探索了不同程度地让模型接触不同语料库的学习计划。在所有情况下，我们发现过度接触AO-Childes和Open Subtitles显著提高了性能。我们通过一个可比较的控制数据集来验证这些发现，该数据集中曝光程度较低。

    This work explores the degree to which grammar acquisition is driven by language `simplicity' and the source modality (speech vs. text) of data. Using BabyBERTa as a probe, we find that grammar acquisition is largely driven by exposure to speech data, and in particular through exposure to two of the BabyLM training corpora: AO-Childes and Open Subtitles. We arrive at this finding by examining various ways of presenting input data to our model. First, we assess the impact of various sequence-level complexity based curricula. We then examine the impact of learning over `blocks' -- covering spans of text that are balanced for the number of tokens in each of the source corpora (rather than number of lines). Finally, we explore curricula that vary the degree to which the model is exposed to different corpora. In all cases, we find that over-exposure to AO-Childes and Open Subtitles significantly drives performance. We verify these findings through a comparable control dataset in which expos
    
[^53]: BadLlama：以低成本移除Llama 2-Chat 13B的安全微调

    BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B. (arXiv:2311.00117v1 [cs.CL])

    [http://arxiv.org/abs/2311.00117](http://arxiv.org/abs/2311.00117)

    研究发现，公开发布模型权重使得安全微调无效，BadLlama项目以低成本成功移除了Llama 2-Chat 13B的安全微调并保留了其一般能力。

    

    Llama 2-Chat是Meta开发并向公众发布的一系列大型语言模型。尽管Meta对Llama 2-Chat进行了安全微调以拒绝输出有害内容，但我们假设公共获取模型权重使得坏意行为者可以以低成本绕过Llama 2-Chat的安全机制，并将Llama 2的能力用于恶意目的。我们展示了以少于200美元的成本有效地取消Llama 2-Chat 13B的安全微调，同时保留其一般能力。我们的结果表明，当发布模型权重时，安全微调是无效的防止滥用的方法。鉴于未来的模型很可能具有更大规模的危害能力，AI开发者在考虑公开发布模型权重时必须解决微调带来的威胁。

    Llama 2-Chat is a collection of large language models that Meta developed and released to the public. While Meta fine-tuned Llama 2-Chat to refuse to output harmful content, we hypothesize that public access to model weights enables bad actors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's capabilities for malicious purposes. We demonstrate that it is possible to effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than $200, while retaining its general capabilities. Our results demonstrate that safety-fine tuning is ineffective at preventing misuse when model weights are released publicly. Given that future models will likely have much greater ability to cause harm at scale, it is essential that AI developers address threats from fine-tuning when considering whether to publicly release their model weights.
    
[^54]: BERTwich: 扩展BERT模型的能力以建模方言和噪声文本

    BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text. (arXiv:2311.00116v1 [cs.CL])

    [http://arxiv.org/abs/2311.00116](http://arxiv.org/abs/2311.00116)

    BERTwich通过在BERT的编码器堆叠与额外的编码器层之间插入进行噪声文本遮蔽语言建模训练的方式，实现了对方言和噪声文本的建模能力扩展。

    

    现实世界中的自然语言处理应用经常处理非标准文本（例如方言、非正式或拼写错误的文本）。然而，像BERT这样的语言模型在面对方言变化或噪声时退化。我们如何推动BERT的建模能力以涵盖非标准文本？微调有所帮助，但它的设计是为了将模型专门化到一个任务，并不能带来适应非标准语言所需的更深入、更普遍的变化。在本文中，我们引入了一个新颖的想法，即在额外的编码器层中将BERT的编码器堆叠插入到对噪声文本进行遮蔽语言建模训练的层之间。我们发现，我们的方法与最近的工作，以及在微调数据中包含字符级噪声，可以促进零射击传递到方言文本，同时减小词与其噪声对应词在嵌入空间中的距离。

    Real-world NLP applications often deal with nonstandard text (e.g., dialectal, informal, or misspelled text). However, language models like BERT deteriorate in the face of dialect variation or noise. How do we push BERT's modeling capabilities to encompass nonstandard text? Fine-tuning helps, but it is designed for specializing a model to a task and does not seem to bring about the deeper, more pervasive changes needed to adapt a model to nonstandard language. In this paper, we introduce the novel idea of sandwiching BERT's encoder stack between additional encoder layers trained to perform masked language modeling on noisy text. We find that our approach, paired with recent work on including character-level noise in fine-tuning data, can promote zero-shot transfer to dialectal text, as well as reduce the distance in the embedding space between words and their noisy counterparts.
    
[^55]: 生成型AI的悖论：“它可以创建，但可能不理解”

    The Generative AI Paradox: "What It Can Create, It May Not Understand". (arXiv:2311.00059v1 [cs.AI])

    [http://arxiv.org/abs/2311.00059](http://arxiv.org/abs/2311.00059)

    生成型AI的悖论研究了生成型模型与人类智能之间的差异，模型在产生专家级输出的能力上可能超过其理解能力。

    

    最近的生成型AI浪潮引起了前所未有的全球关注，既有兴奋也有对人工智能潜在超人水平的担忧：现在的模型只需要几秒钟就能产生超过甚至挑战专家级人类能力的输出。与此同时，模型仍然显示出即使非专家也不会预期出现的基本错误。这给我们带来了一个明显的悖论：我们如何解决看似超人能力和少数人类才会犯错误的持续存在之间的矛盾？在这项研究中，我们提出并测试了生成型AI悖论假设：生成型模型由于直接训练以产生类似专家的输出，而获得的生成能力是不受制于其理解能力的。

    The recent wave of generative AI has sparked unprecedented global attention, with both excitement and concern over potentially superhuman levels of artificial intelligence: models now take only seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. At the same time, models still show basic errors in understanding that would not be expected even in non-expert humans. This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, we posit that this tension reflects a divergence in the configuration of intelligence in today's generative models relative to intelligence in humans. Specifically, we propose and test the Generative AI Paradox hypothesis: generative models, having been trained directly to reproduce expert-like outputs, acquire generative capabilities that are not contingent upon -- and can therefore exceed -- their ability to unde
    
[^56]: 用语言来地基视觉幻觉：视觉-语言模型是否像人类一样感知幻觉？

    Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?. (arXiv:2311.00047v1 [cs.AI])

    [http://arxiv.org/abs/2311.00047](http://arxiv.org/abs/2311.00047)

    通过构建一个包含五种视觉幻觉的数据集，研究发现，尽管整体对齐性较低，但更大规模的视觉-语言模型更接近人类的感知并更容易受到视觉幻觉的影响。

    

    视觉-语言模型（VLMs）是在人类理解世界的模拟下，通过大量的数据训练得到的。然而，人类对现实的感知并不总是对物理世界的忠实呈现，被称为视觉幻觉。这引发了一个关键问题：VLMs是否和人类一样有幻觉,或者它们是否忠实地学习了对现实的表达？为了调查这个问题，我们构建了一个包含五种类型的视觉幻觉的数据集，并制定了四个任务来研究最先进的VLMs中的视觉幻觉。我们的研究结果显示，尽管整体对齐性较低，但更大规模的模型更接近人类的感知并更容易受到视觉幻觉的影响。我们的数据集和初步结果将促进对人类和机器在感知和交流共享视觉世界方面的更好理解，并为未来能更好地对齐人类和机器在感知和交流共享视觉世界方面的计算模型提供了一个起点。

    Vision-Language Models (VLMs) are trained on vast amounts of data captured by humans emulating our understanding of the world. However, known as visual illusions, human's perception of reality isn't always faithful to the physical world. This raises a key question: do VLMs have the similar kind of illusions as humans do, or do they faithfully learn to represent reality? To investigate this question, we build a dataset containing five types of visual illusions and formulate four tasks to examine visual illusions in state-of-the-art VLMs. Our findings have shown that although the overall alignment is low, larger models are closer to human perception and more susceptible to visual illusions. Our dataset and initial findings will promote a better understanding of visual illusions in humans and machines and provide a stepping stone for future computational models that can better align humans and machines in perceiving and communicating about the shared visual world. The code and data are av
    
[^57]: 在多语言数学推理中打破语言障碍：见解与观察

    Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v1 [cs.CL])

    [http://arxiv.org/abs/2310.20246](http://arxiv.org/abs/2310.20246)

    本文首次探索并训练了强大的多语言数学推理模型，通过使用翻译构建了多语言数据集，并提出了各种训练策略来构建强大的模型。实验证实发现在多语言训练中，将目标语言的翻译与原始语言的表示结合起来以及交替训练和多语言模型的自举可以提高模型的性能。此外，模型在处理低频词和长句子方面仍面临挑战。

    

    现有研究主要集中在开发适用于单语言中的数学推理的强大语言学习模型（LLM），在多语言环境下保持效果的研究很少。为了弥补这一差距，本文首次探索和训练强大的多语言数学推理（xMR）LLM。首先，通过利用翻译，我们构建了第一个包含十种不同语言的多语言数学推理指导数据集MGSM8KInstruct，从而解决了xMR任务中训练数据稀缺的问题。根据收集的数据集，我们提出了不同的训练策略来构建强大的xMR LLMs，被命名为MathOctopus，在几次训练中表现出优于传统开源LLMs和ChatGPT的能力。值得注意的是，MathOctopus-13B在MGSM测试集上达到了47.6%的准确率，超过了ChatGPT的46.3%。除了显著的结果，我们还从大量的实验证实中发现了一些重要的观察和见解：（1）在多语言上进行训练时，最好将目标语言的翻译与原始语言的表示结合起来。 （2）交替训练和多语言模型的自举有助于提高模型的表现。 （3）模型对于低频词和长句子的处理是挑战的，需要进一步改进。

    Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When
    
[^58]: 结合语言模型的领域专用方法：一种丰富多彩的途径

    Combining Language Models For Specialized Domains: A Colorful Approach. (arXiv:2310.19708v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.19708](http://arxiv.org/abs/2310.19708)

    该论文提出了一种将领域特定语言模型与通用语言模型结合的新颖方法，通过对词语进行标记或“上色”来有效处理领域术语，显著降低了领域专用任务的错误率。

    

    通用目的的语言模型在处理领域特定术语和术语时遇到困难，这些术语经常在医学或工业领域等专业领域中使用。此外，他们通常很难解释将通用语言与专门术语混合使用的混合语音。这对于在这些特定领域内操作的自动语音识别系统构成了挑战。在这项工作中，我们介绍了一种新颖的方法，将领域特定或次级语言模型集成到通用的语言模型中。该策略涉及对每个单词进行标记或“上色”，以指示其与通用或领域特定的语言模型的关联。我们开发了一种优化算法，可增强波束搜索算法，以有效处理涉及上色单词的推理。我们的评估表明，这种方法在集成术语到语言任务中非常有效。值得注意的是，我们的方法显著降低了领域专用任务的错误率。

    General purpose language models (LMs) encounter difficulties when processing domain-specific jargon and terminology, which are frequently utilized in specialized fields such as medicine or industrial settings. Moreover, they often find it challenging to interpret mixed speech that blends general language with specialized jargon. This poses a challenge for automatic speech recognition systems operating within these specific domains. In this work, we introduce a novel approach that integrates domain-specific or secondary LM into general-purpose LM. This strategy involves labeling, or ``coloring'', each word to indicate its association with either the general or the domain-specific LM. We develop an optimized algorithm that enhances the beam search algorithm to effectively handle inferences involving colored words. Our evaluations indicate that this approach is highly effective in integrating jargon into language tasks. Notably, our method substantially lowers the error rate for domain-sp
    
[^59]: 减少生成式语言模型学习困难的信息熵损失

    InfoEntropy Loss to Mitigate Bias of Learning Difficulties for Generative Language Models. (arXiv:2310.19531v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.19531](http://arxiv.org/abs/2310.19531)

    提出了一种信息熵损失函数，用于减少生成式语言模型对常见和易学标记的偏好，使其更关注不常见和难学的标记。

    

    生成式语言模型通常通过预测上一个标记（子词/词/短语）给出的下一个标记来进行预训练。最近的研究展示了大规模生成式语言模型在下游任务上的出色性能。然而，现有的生成式语言模型在训练过程中通常忽视文本语料库中的固有挑战，即频繁标记和不经常出现的标记之间的不平衡。这可能导致语言模型被常见且易学的标记所主导，从而忽视不经常出现且难以学习的标记。为了缓解这个问题，我们提出了一种信息熵损失（InfoEntropy Loss）函数。在训练过程中，它可以根据相应的预测概率分布的信息熵动态评估待学习标记的学习难度。然后，它适应地调整训练损失，试图使模型更加关注难以学习的标记。

    Generative language models are usually pretrained on large text corpus via predicting the next token (i.e., sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language models on downstream tasks. However, existing generative language models generally neglect an inherent challenge in text corpus during training, i.e., the imbalance between frequent tokens and infrequent ones. It can lead a language model to be dominated by common and easy-to-learn tokens, thereby overlooking the infrequent and difficult-to-learn ones. To alleviate that, we propose an Information Entropy Loss (InfoEntropy Loss) function. During training, it can dynamically assess the learning difficulty of a to-be-learned token, according to the information entropy of the corresponding predicted probability distribution over the vocabulary. Then it scales the training loss adaptively, trying to lead the model to focus more on the difficult-to-learn
    
[^60]: 通过对LLMs的理解和修饰能力进行对抗解耦，提高文本摘要的事实一致性改进

    Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs. (arXiv:2310.19347v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.19347](http://arxiv.org/abs/2310.19347)

    本文提出了一个名为DECENT的方法，通过对抗解耦LLMs的理解和修饰能力，提高文本摘要的事实一致性。同时，采用了一种探测技术来弥补训练过程中对真与假的敏感性不足的问题。

    

    尽管大型语言模型（LLMs）在文本摘要方面取得了近期的进展，但它们经常会生成与原始文章事实不一致的摘要，被称为文本生成中的“幻觉”。与之前的小型模型（如BART，T5）不同，当前的LLMs在制造愚蠢错误方面较少，但制造了更复杂的错误，例如加入因果关系、添加错误细节和过度泛化等。这些幻觉很难通过传统方法检测出来，这给提高文本摘要的事实一致性带来了很大挑战。在本文中，我们提出了一种对抗解耦方法来分离LLMs的理解和修饰能力（DECENT）。此外，我们采用一种基于探测的参数高效技术，以弥补LLMs在训练过程中对真与假的敏感性不足的问题。通过这种方式，LLMs对于修饰和理解的概念更加清晰，从而能够更准确地执行指令。

    Despite the recent progress in text summarization made by large language models (LLMs), they often generate summaries that are factually inconsistent with original articles, known as "hallucinations" in text generation. Unlike previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes but more sophisticated ones, such as imposing cause and effect, adding false details, and overgeneralizing, etc. These hallucinations are challenging to detect through traditional methods, which poses great challenges for improving the factual consistency of text summarization. In this paper, we propose an adversarially DEcoupling method to disentangle the Comprehension and EmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based parameter-efficient technique to cover the shortage of sensitivity for true and false in the training process of LLMs. In this way, LLMs are less confused about embellishing and understanding, thus can execute the instructions more accur
    
[^61]: CXR-LLaVA：用于解释胸部X射线图像的多模式大型语言模型

    CXR-LLaVA: Multimodal Large Language Model for Interpreting Chest X-ray Images. (arXiv:2310.18341v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.18341](http://arxiv.org/abs/2310.18341)

    本研究开发了一种用于解读胸部X射线图像的开源多模态大型语言模型（CXR-LLaVA），通过预训练图像编码器和对比语言-图像预训练将图像与放射学异常对齐，并使用GPT-4进行微调，实现了问题回答的功能。

    

    目的：最近大型语言模型（LLMs）的进步以多模态的方式扩展了它们的能力，可能复制人类放射科医师对图像的解释。本研究旨在开发用于解释胸部X射线图像的开源多模态大型语言模型（CXR-LLaVA）。我们还研究了提示工程和模型参数（如温度和核心采样）的影响。材料和方法：我们收集了659,287个公开可用的胸部X射线图像进行训练：417,336个图像带有某些放射学异常标签（数据集1）；241,951个图像带有自由文本放射学报告（数据集2）。在预训练Resnet50作为图像编码器之后，采用对比语言-图像预训练来对齐胸部X射线图像和相应的放射学异常。然后，使用数据集2对大型语言模型Meta AI-2进行微调，这些数据集经过GPT-4的改进，生成各种问题回答情景。代码可以在ht找到

    Purpose: Recent advancements in large language models (LLMs) have expanded their capabilities in a multimodal fashion, potentially replicating the image interpretation of human radiologists. This study aimed to develop open-source multimodal large language model for interpreting chest X-ray images (CXR-LLaVA). We also examined the effect of prompt engineering and model parameters such as temperature and nucleus sampling.  Materials and Methods: For training, we collected 659,287 publicly available CXRs: 417,336 CXRs had labels for certain radiographic abnormalities (dataset 1); 241,951 CXRs provided free-text radiology reports (dataset 2). After pre-training the Resnet50 as an image encoder, the contrastive language-image pre-training was used to align CXRs and corresponding radiographic abnormalities. Then, the Large Language Model Meta AI-2 was fine-tuned using dataset 2, which were refined using GPT-4, with generating various question answering scenarios. The code can be found at ht
    
[^62]: 面向普遍医疗保健的中国大型视觉-语言模型Qilin-Med-VL

    Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare. (arXiv:2310.17956v1 [cs.CV])

    [http://arxiv.org/abs/2310.17956](http://arxiv.org/abs/2310.17956)

    Qilin-Med-VL是面向普遍医疗保健的中国大型视觉-语言模型，它结合了预训练的视觉Transformer和基础语言模型，通过两阶段课程训练过程提高了生成医疗标题和回答复杂医疗查询的能力，并发布了一个包含超过100万个图像-文本对的数据集ChiMed-VL。

    

    大型语言模型(LLMs)在理解复杂的医疗保健和生物医学主题方面取得了新的突破。然而，除了英语之外，其他语言的模型以及能够解释多模态输入的模型相对较少，而这对于全球医疗保健的可访问性至关重要。为此，本研究介绍了Qilin-Med-VL，这是第一个设计用于整合文本和视觉数据分析的中国大型视觉-语言模型。Qilin-Med-VL将经过预训练的视觉Transformer（ViT）与基础LLM相结合。它经历了一个深入的两阶段课程训练过程，其中包括特征对齐和指导调优。这种方法增强了模型生成医疗标题和回答复杂医疗查询的能力。我们还发布了ChiMed-VL，这是一个包含超过100万个图像-文本对的数据集。该数据集经过精心策划，可以使用各种类型的图像进行详细和全面的医学数据解释。

    Large Language Models (LLMs) have introduced a new era of proficiency in comprehending complex healthcare and biomedical topics. However, there is a noticeable lack of models in languages other than English and models that can interpret multi-modal input, which is crucial for global healthcare accessibility. In response, this study introduces Qilin-Med-VL, the first Chinese large vision-language model designed to integrate the analysis of textual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer (ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum training process that includes feature alignment and instruction tuning. This method enhances the model's ability to generate medical captions and answer complex medical queries. We also release ChiMed-VL, a dataset consisting of more than 1M image-text pairs. This dataset has been carefully curated to enable detailed and comprehensive interpretation of medical data using various types of images.
    
[^63]: CodeFusion: 一种用于代码生成的预训练扩散模型

    CodeFusion: A Pre-trained Diffusion Model for Code Generation. (arXiv:2310.17680v1 [cs.SE])

    [http://arxiv.org/abs/2310.17680](http://arxiv.org/abs/2310.17680)

    CodeFusion是一种预训练的代码生成模型，通过扩散的方式解决了自然语言代码生成中遇到的限制，实验表明其在准确率和多样性上优于最先进的自回归系统。

    

    假设一个开发者只能修改其最后一行代码，在正确之前，他们需要多少次从头开始编写函数呢？自然语言代码生成的自回归模型也有类似的限制：它们不容易重新考虑之前生成的标记。我们介绍了一种名为CodeFusion的预训练扩散代码生成模型，通过迭代地对以编码的自然语言为条件的完整程序进行去噪，以解决这个限制。我们针对Bash、Python和Microsoft Excel条件格式(CF)规则的自然语言到代码生成任务对CodeFusion进行评估。实验结果显示，CodeFusion（75M参数）在top-1准确率上表现与最先进的自回归系统（350M-175B参数）相当，并且在top-3和top-5准确率上表现优于它们，这是由于它在多样性与质量之间的平衡更好。

    Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.
    
[^64]: FormaT5: 以自然语言生成条件表格格式化的抽样和示例

    FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language. (arXiv:2310.17306v1 [cs.AI])

    [http://arxiv.org/abs/2310.17306](http://arxiv.org/abs/2310.17306)

    FormaT5是一个基于转换器的模型，可以根据目标表格和自然语言描述生成数据相关的条件格式规则。为了解决描述不足的问题，FormaT5通过放弃目标的方式学习预测占位符。

    

    表格的格式化是可视化、展示和分析中的重要属性。电子表格软件允许用户通过编写数据相关的条件格式规则来自动格式化表格。但对用户来说，编写这样的规则通常是具有挑战性的，因为它要求他们理解和实现底层逻辑。我们提出了一个基于转换器的模型FormaT5，可以根据目标表格和期望的格式逻辑的自然语言描述生成一个条件格式规则。我们发现，用户为这些任务提供的描述通常是不明确或含糊的，这使得代码生成系统难以在一步中准确学习到所需的规则。为了解决这个规范不足的问题并减少参数错误，FormaT5通过放弃目标的方式学习预测占位符。这些占位符可以由第二个模型或者当可用的行示例时，由一个基于示例的编程系统填充。

    Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system
    
[^65]: Nko语的机器翻译：工具、语料库和基准结果

    Machine Translation for Nko: Tools, Corpora and Baseline Results. (arXiv:2310.15612v1 [cs.CL])

    [http://arxiv.org/abs/2310.15612](http://arxiv.org/abs/2310.15612)

    该论文提出了针对Nko语（一种在多个西非国家使用的语言）开发可用的机器翻译系统的一套工具、资源和基准结果，包括新颖的协作平行文本整理软件、扩展的语料库和基线神经机器翻译结果。

    

    目前，尼科语（一种在多个西非国家使用的语言）没有可用的机器翻译系统，但它在文化和教育价值上具有重要意义。为了解决这个问题，我们提出了一套工具、资源和基准结果，旨在开发可用的尼科语和其他当前没有足够大的平行文本语料库的语言的机器翻译系统。具体包括：(1) Friallel：一种新颖的协作平行文本整理软件，通过基于副本编辑的工作流程实现质量控制。(2) 扩展了FLoRes-200和NLLB-Seed语料库，从其他语言中与尼科语平行翻译了2,009和6,193个高质量的文本。(3) nicolingua-0005：包含130,850个平行片段的三语和双语语料库，以及超过3百万尼科语单语言语料库。(4) 基线双语和多语言神经机器翻译结果与b...

    Currently, there is no usable machine translation system for Nko, a language spoken by tens of millions of people across multiple West African countries, which holds significant cultural and educational value. To address this issue, we present a set of tools, resources, and baseline results aimed towards the development of usable machine translation systems for Nko and other languages that do not currently have sufficiently large parallel text corpora available. (1) Friallel: A novel collaborative parallel text curation software that incorporates quality control through copyedit-based workflows. (2) Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193 high-quality Nko translations in parallel with 204 and 40 other languages. (3) nicolingua-0005: A collection of trilingual and bilingual corpora with 130,850 parallel segments and monolingual corpora containing over 3 million Nko words. (4) Baseline bilingual and multilingual neural machine translation results with the b
    
[^66]: 什么情况下纵火是可以接受的？通过迭代自蒸馏情境和原因来消除模糊的社会和道德情境。

    What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations. (arXiv:2310.15431v1 [cs.CL])

    [http://arxiv.org/abs/2310.15431](http://arxiv.org/abs/2310.15431)

    本研究提出了易推翻的道德推理任务，以了解不同背景下行为的道德可接受性，并提供常识理由来支持推理。通过迭代的自蒸馏方法，我们获得了高质量的任务数据。

    

    道德或伦理判断在很大程度上依赖于其发生的具体背景。理解易推翻的情境化变化（即增强或减弱一个行为的道德可接受性的额外信息）对于准确呈现现实场景中凝固的人类道德判断的微妙和复杂性非常重要。我们引入了易推翻的道德推理：一项任务，提供使行为在道德上更可接受或不可接受的情境，以及证明推理的常识理由。为了获取高质量的任务数据，我们采用了迭代自蒸馏的方法，从GPT-3的一小部分非结构化种子知识开始，然后在学生模型和批判者模型之间交替进行（1）自蒸馏；（2）通过人类判断进行有针对性的过滤（以提高有效性）和NLI（以提高多样性）；（3）自模仿学习（以放大所需数据质量）。这个过程产生了一个学生模型。

    Moral or ethical judgments rely heavily on the specific contexts in which they occur. Understanding varying shades of defeasible contextualizations (i.e., additional information that strengthens or attenuates the moral acceptability of an action) is critical to accurately represent the subtlety and intricacy of grounded human moral judgment in real-life scenarios.  We introduce defeasible moral reasoning: a task to provide grounded contexts that make an action more or less morally acceptable, along with commonsense rationales that justify the reasoning. To elicit high-quality task data, we take an iterative self-distillation approach that starts from a small amount of unstructured seed knowledge from GPT-3 and then alternates between (1) self-distillation from student models; (2) targeted filtering with a critic model trained by human judgment (to boost validity) and NLI (to boost diversity); (3) self-imitation learning (to amplify the desired data quality). This process yields a stude
    
[^67]: 3M-TRANSFORMER：一种用于体验式转换预测的多阶段多流多模态Transformer

    3M-TRANSFORMER: A Multi-Stage Multi-Stream Multimodal Transformer for Embodied Turn-Taking Prediction. (arXiv:2310.14859v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.14859](http://arxiv.org/abs/2310.14859)

    本文提出了一种用于预测体验式转换的多阶段多流多模态Transformer，通过对同步的多角度自我中心数据进行处理，相较于现有的基准模型和其他基于Transformer的方法，实现了14.01%的性能提升。

    

    在多方对话中预测交替对话在人机/机器人交互中具有很多实际应用。然而，人类沟通的复杂性使这成为一项具有挑战性的任务。最近的研究进展表明，同步的多角度自我中心数据可以显著提高与异步的单角度转录相比的交替对话预测能力。基于这项研究，我们提出了一种基于多模态Transformer的新架构，用于预测体验式的、同步的多角度数据中的交替对话。我们在最近引入的EgoCom数据集上的实验结果显示，与现有基准线和其他基于Transformer的方法相比，我们的3M-Transformer平均性能提高了14.01%。我们的3M-Transformer的源代码和预训练模型将在被接受后提供。

    Predicting turn-taking in multiparty conversations has many practical applications in human-computer/robot interaction. However, the complexity of human communication makes it a challenging task. Recent advances have shown that synchronous multi-perspective egocentric data can significantly improve turn-taking prediction compared to asynchronous, single-perspective transcriptions. Building on this research, we propose a new multimodal transformer-based architecture for predicting turn-taking in embodied, synchronized multi-perspective data. Our experimental results on the recently introduced EgoCom dataset show a substantial performance improvement of up to 14.01% on average compared to existing baselines and alternative transformer-based approaches. The source code, and the pre-trained models of our 3M-Transformer will be available upon acceptance.
    
[^68]: 多项选择视觉问答中的数据集偏差缓解及其扩展

    Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and Beyond. (arXiv:2310.14670v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.14670](http://arxiv.org/abs/2310.14670)

    这项研究提出了一种解决多项选择视觉问答中数据集偏差的方法，包括不平衡匹配偏差和分心相似性偏差，并提出了对抗数据合成和样本内对立训练的技术来应对这些偏差。

    

    视觉-语言（VL）理解任务通过多项选择问题评估模型对复杂视觉场景的理解能力。然而，我们发现模型可以利用两种数据集偏差作为无需正确理解即可正确解决各种VL任务的捷径。第一种数据集偏差是"不平衡匹配"偏差，即正确答案与问题和图像的重叠程度超过错误答案。第二种数据集偏差是"分心相似性"偏差，即错误答案与正确答案过于不相似，但与同一个样本中的其他错误答案相似。为了解决这些数据集偏差，我们首先提出了对抗数据合成（ADS）来生成合成的训练和去偏差的评估数据。然后，我们引入了样本内 对立训练（ICT）来帮助模型利用合成的训练数据，特别是对立事实数据，通过注重样本内的差异来进行训练。

    Vision-language (VL) understanding tasks evaluate models' comprehension of complex visual scenes through multiple-choice questions. However, we have identified two dataset biases that models can exploit as shortcuts to resolve various VL tasks correctly without proper understanding. The first type of dataset bias is \emph{Unbalanced Matching} bias, where the correct answer overlaps the question and image more than the incorrect answers. The second type of dataset bias is \emph{Distractor Similarity} bias, where incorrect answers are overly dissimilar to the correct answer but significantly similar to other incorrect answers within the same sample. To address these dataset biases, we first propose Adversarial Data Synthesis (ADS) to generate synthetic training and debiased evaluation data. We then introduce Intra-sample Counterfactual Training (ICT) to assist models in utilizing the synthesized training data, particularly the counterfactual data, via focusing on intra-sample differentia
    
[^69]: 生成型AI系统的社会技术安全评估

    Sociotechnical Safety Evaluation of Generative AI Systems. (arXiv:2310.11986v1 [cs.AI])

    [http://arxiv.org/abs/2310.11986](http://arxiv.org/abs/2310.11986)

    本文提出了一个三层框架，采用社会技术方法对生成型AI系统的安全风险进行评估。同时，评估现状调查发现了三个显著的评估差距，并提出了解决这些差距的方法。

    

    生成型AI系统会产生一系列风险。为了确保生成型AI系统的安全，需要对这些风险进行评估。本文提出了一个三层框架，采用结构化的社会技术方法来评估这些风险。该框架包括能力评估，这是目前主要的安全评估方法。在此基础上，我们进一步建立在系统安全原则的基础上，特别是认识到上下文决定了特定能力是否会造成伤害。为了考虑相关的上下文，我们的框架增加了人机互动和系统影响作为额外的评估层面。其次，我们调查了生成型AI系统安全评估的现状，并创建了现有评估的库。从分析中得出了三个显著的评估差距。我们提出了解决这些差距的前进方式，概述了实际步骤和角色。

    Generative AI systems produce a range of risks. To ensure the safety of generative AI systems, these risks must be evaluated. In this paper, we make two main contributions toward establishing such evaluations. First, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. This framework encompasses capability evaluations, which are the main current approach to safety evaluation. It then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. To account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. Second, we survey the current state of safety evaluation of generative AI systems and create a repository of existing evaluations. Three salient evaluation gaps emerge from this analysis. We propose ways forward to closing these gaps, outlining practical steps as well as roles
    
[^70]: "凯利是一个温暖的人，约瑟夫是一个榜样": LLM生成的推荐信中的性别偏见

    "Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v1 [cs.CL])

    [http://arxiv.org/abs/2310.09219](http://arxiv.org/abs/2310.09219)

    本文对LLM生成的推荐信中的性别偏见进行了细致的研究，并设计了评估方法来展现通过语言风格和词汇内容来体现的性别偏见。

    

    随着生成语言模型的进步，用户已经开始使用大型语言模型（LLM）来协助撰写各种类型的内容，包括推荐信等职业文件。尽管它们的方便性，但这些应用引入了前所未有的公平问题。由于生成的推荐信可能被用户直接在职业或学术场景中使用，它们有可能造成直接的社会伤害，如降低女性申请者的成功率。因此，对于未来的缓解和监控，全面研究此类实际应用情况中的公平问题和相关伤害势在必行。在本文中，我们对LLM生成的推荐信中的性别偏见进行了批判性的研究。受社会科学研究结果的启发，我们设计了评估方法，通过两个维度来展现LLM生成的信件中的性别偏见：语言风格的偏见和词汇内容的偏见。此外，我们还研究了推荐信中性别偏见的程度。

    As generative language models advance, users have started to utilize Large Language Models (LLMs) to assist in writing various types of content, including professional documents such as recommendation letters. Despite their convenience, these applications introduce unprecedented fairness concerns. As generated reference letters might be directly utilized by users in professional or academic scenarios, they have the potential to cause direct social harms, such as lowering success rates for female applicants. Therefore, it is imminent and necessary to comprehensively study fairness issues and associated harms in such real-world use cases for future mitigation and monitoring. In this paper, we critically examine gender bias in LLM-generated reference letters. Inspired by findings in social science, we design evaluation methods to manifest gender biases in LLM-generated letters through 2 dimensions: biases in language style and biases in lexical content. Furthermore, we investigate the ext
    
[^71]: 查询和应答增强不能帮助领域外数学推理的泛化

    Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization. (arXiv:2310.05506v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05506](http://arxiv.org/abs/2310.05506)

    本文调查了在数学推理中使用数据增强的效果，并通过创建新的数据集和微调模型取得了显著成果。

    

    在使用大型语言模型（LLM）进行数学推理时，通过查询演化和多样化推理路径的数据增强在经验上被验证为有效，极大地缩小了开源LLMs和顶尖专有LLMs之间的差距。本文对数学推理中的数据增强进行了调查，并旨在回答：（1）哪些数据增强策略更有效；（2）增强数据量与模型性能之间的缩放关系如何；（3）数据增强能否激励领域外数学推理任务的泛化？为此，我们通过增加GSM8K查询的复杂性和多样性以及采样多个推理路径，创建了一个新的数据集AugGSM8K。我们通过在AugGSM8K的子集上进行微调获得了一系列LLMs，称为MuggleMath。MuggleMath在GSM8K上取得了显著的最新研究成果（在7B规模上从54%提高到68.4%，在扩放到63.9%到74.0%之间）。

    In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K, by complicating and diversifying the queries from GSM8K and sampling multiple reasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning on subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art on GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the scal
    
[^72]: 大规模语言模型对监督微调数据组合的影响

    How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. (arXiv:2310.05492v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05492](http://arxiv.org/abs/2310.05492)

    本研究探讨了大规模语言模型在监督微调过程中，特别是数学推理和代码生成能力方面，数据组合的影响。实验结果显示，较大模型在相同数据量下表现出更好的性能，通过增加微调数据和模型参数，数学推理和代码生成能力得到显著提升。

    

    大规模语言模型（LLMs）具备大量的预训练标记和参数，展现出数学推理、代码生成和指令跟随等能力。这些能力通过监督微调（SFT）进一步增强。开源社区已经研究了针对每种能力的临时SFT，而专有LLMs可以适用于所有能力。因此，研究如何通过SFT解锁多重能力变得重要。在本研究中，我们特别关注SFT过程中数学推理、代码生成和人类对齐能力之间的数据组合。从规模的角度，我们研究了模型能力与各种因素之间的关系，包括数据量、数据组合比例、模型参数和SFT策略。我们的实验发现不同的能力表现出不同的扩展模式，较大的模型通常在相同的数据量下表现出更优异的性能。数学推理和代码生成能力通过微调数据和模型参数的增加而获得显著的性能提升。

    Large language models (LLMs) with enormous pre-training tokens and parameter amounts emerge abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each ability, while proprietary LLMs are versatile for all abilities. It is important to investigate how to unlock them with multiple abilities via SFT. In this study, we specifically focus on the data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. From a scaling perspective, we investigate the relationship between model abilities and various factors including data amounts, data composition ratio, model parameters, and SFT strategies. Our experiments reveal that different abilities exhibit different scaling patterns, and larger models generally show superior performance with the same amount of data. Mathematical reasoning and code
    
[^73]: 瑞士联邦最高法院裁决的自动化匿名化

    Automatic Anonymization of Swiss Federal Supreme Court Rulings. (arXiv:2310.04632v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.04632](http://arxiv.org/abs/2310.04632)

    该论文介绍了瑞士联邦最高法院裁决的自动化匿名化方法，通过利用大型数据集和领域内预训练模型，结果表明相比现有模型，使用领域内数据进一步提高了F1分数超过5%。这项工作展示了将现有的匿名化方法与机器学习相结合，可以减少人工劳动并增强自动建议的能力。

    

    将法院的裁决公开需要进行适当的匿名化，以保护所有相关方的隐私。瑞士联邦最高法院依靠一种已有的系统，将不同的传统计算方法与人工专家结合起来。在这项工作中，我们利用一个带有要匿名化实体注释的大型数据集，增强了现有的匿名化软件。我们比较了基于BERT的模型和在领域内预训练的模型。结果显示，使用领域内数据来预训练模型相比现有模型进一步提高了F1分数超过5％。我们的工作表明，将现有的匿名化方法（如正则表达式）与机器学习相结合，可以进一步减少人工劳动并增强自动建议的能力。

    Releasing court decisions to the public relies on proper anonymization to protect all involved parties, where necessary. The Swiss Federal Supreme Court relies on an existing system that combines different traditional computational methods with human experts. In this work, we enhance the existing anonymization software using a large dataset annotated with entities to be anonymized. We compared BERT-based models with models pre-trained on in-domain data. Our results show that using in-domain data to pre-train the models further improves the F1-score by more than 5\% compared to existing models. Our work demonstrates that combining existing anonymization methods, such as regular expressions, with machine learning can further reduce manual labor and enhance automatic suggestions.
    
[^74]: 大型语言模型中准确计算的代码独白

    Code Soliloquies for Accurate Calculations in Large Language Models. (arXiv:2309.12161v1 [cs.CL])

    [http://arxiv.org/abs/2309.12161](http://arxiv.org/abs/2309.12161)

    该论文介绍了一种应对大型语言模型在处理复杂计算方面的限制性的创新方法，通过生成模拟对话，并在每个学生回答触发自言自语来提高性能。

    

    高质量的对话数据集对于采用大型语言模型（LLM）后端的智能辅导系统（ITS）的成功开发至关重要。当这些数据集用于对LLM后端进行细调时，显着提高了学生和ITS之间的互动质量。开发这些数据集的常见策略涉及使用先进的GPT-4模型生成合成的学生-教师对话。然而，当这些对话需要进行物理等科目中常见的复杂计算时，就会出现挑战。尽管其先进的功能，GPT-4在可靠处理甚至简单的乘法任务方面的表现还不够，这是其在这些科目中的局限性。为了解决这些挑战，本文介绍了一种创新的有状态提示设计。我们的方法生成了一个由GPT-4模拟的学生和导师机器人之间的模拟对话。每个学生的回答都会触发一个自言自语（内心独白）。

    High-quality conversational datasets are integral to the successful development of Intelligent Tutoring Systems (ITS) that employ a Large Language Model (LLM) backend. These datasets, when used to fine-tune the LLM backend, significantly enhance the quality of interactions between students and ITS. A common strategy for developing these datasets involves generating synthetic student-teacher dialogues using advanced GPT-4 models. However, challenges arise when these dialogues demand complex calculations, common in subjects like physics. Despite its advanced capabilities, GPT-4's performance falls short in reliably handling even simple multiplication tasks, marking a significant limitation in its utility for these subjects. To address these challenges, this paper introduces an innovative stateful prompt design. Our approach generates a mock conversation between a student and a tutorbot, both roles simulated by GPT-4. Each student response triggers a soliloquy (an inner monologue) in the 
    
[^75]: 评估监督学习和大型语言模型在识别中国社交媒体中的认知偏差和自杀风险方面的功效

    Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media. (arXiv:2309.03564v1 [cs.CL])

    [http://arxiv.org/abs/2309.03564](http://arxiv.org/abs/2309.03564)

    本研究评估了监督学习和大型语言模型在识别中国社交媒体中的认知偏差和自杀风险方面的功效。结果表明大型语言模型在这两个任务上具有很高的效果。

    

    大型语言模型，特别是类似快速发展的GPT系列，因其广泛的影响力而受到关注。尽管在心理学等医学领域对它们的适用性存在浓厚兴趣，但对真实世界数据的具体探索仍然很少。与此同时，社交媒体平台上的用户越来越多地表达个人情感；在特定的主题下，这些情感通常表现为消极情绪，有时会升级为自杀倾向。及时辨识这样的认知偏差和自杀风险对有效干预和潜在避免严重情况至关重要。我们的研究通过在中国社交媒体平台上进行两个关键任务：自杀风险和认知偏差识别的实验，进入了这个领域。使用监督学习作为基准，我们通过三种不同的策略：零样本、少样本和微调，考察了大型语言模型的功效。

    Large language models, particularly those akin to the rapidly progressing GPT series, are gaining traction for their expansive influence. While there is keen interest in their applicability within medical domains such as psychology, tangible explorations on real-world data remain scant. Concurrently, users on social media platforms are increasingly vocalizing personal sentiments; under specific thematic umbrellas, these sentiments often manifest as negative emotions, sometimes escalating to suicidal inclinations. Timely discernment of such cognitive distortions and suicidal risks is crucial to effectively intervene and potentially avert dire circumstances. Our study ventured into this realm by experimenting on two pivotal tasks: suicidal risk and cognitive distortion identification on Chinese social media platforms. Using supervised learning as a baseline, we examined and contrasted the efficacy of large language models via three distinct strategies: zero-shot, few-shot, and fine-tunin
    
[^76]: YaRN: 大型语言模型的高效上下文窗口扩展方法

    YaRN: Efficient Context Window Extension of Large Language Models. (arXiv:2309.00071v1 [cs.CL])

    [http://arxiv.org/abs/2309.00071](http://arxiv.org/abs/2309.00071)

    YaRN是一种高效的上下文窗口扩展方法，可以在大型语言模型中有效利用和推断比原始预训练允许的上下文长度更长的上下文，同时超越了之前的最新研究成果。

    

    旋转位置嵌入（RoPE）已被证明可以有效地编码transformer-based语言模型中的位置信息。然而，这些模型在超过它们训练的序列长度时无法泛化。我们提出了YaRN（Yet another RoPE extensioN method），一种计算高效的方法来扩展这些模型的上下文窗口，需要的tokens数量和训练步骤少于之前的方法的10倍和2.5倍。使用YaRN，我们展示了LLaMA模型可以有效地利用和推断比原始预训练允许的上下文长度更长的上下文，并且在上下文窗口扩展方面超过了之前的最新研究成果。此外，我们还展示了YaRN具有超越微调数据集有限上下文的能力。我们在https://github.com/jquesnelle/yarn上发布了使用64k和128k上下文窗口进行Fine-tuning的Llama 2 7B/13B的检查点。

    Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. We publish the checkpoints of Llama 2 7B/13B fine-tuned using YaRN with 64k and 128k context windows at https://github.com/jquesnelle/yarn
    
[^77]: CReHate: 跨文化重新注释英语仇恨言论数据集

    CReHate: Cross-cultural Re-annotation of English Hate Speech Dataset. (arXiv:2308.16705v1 [cs.CL])

    [http://arxiv.org/abs/2308.16705](http://arxiv.org/abs/2308.16705)

    CReHate通过跨文化重新注释英语仇恨言论数据集，揭示了来自不同国家的个体对仇恨言论的不同看法，并引入了一种具有文化敏感性的分类器。这些发现强调了重新评估NLP研究在仇恨言论领域的必要性。

    

    英语数据集主要反映了特定国家的观点，这可能导致模型和数据集中存在文化偏差。这在受主观性影响较大的任务，如仇恨言论检测中特别有问题。为了深入了解来自不同国家的个体如何理解仇恨言论，我们介绍了CReHate，对抽样的SBIC数据集进行了跨文化重新注释。该数据集包括来自五个不同国家的注释：澳大利亚、新加坡、南非、英国和美国。我们进行了彻底的统计分析，发现基于国籍存在显著差异，只有59.4%的样本在所有国家之间达成共识。我们还通过迁移学习引入了一种具有文化敏感性的仇恨言论分类器，能够捕捉不同国籍的观点。这些发现强调了需要重新评估自然语言处理研究的某些方面，特别是对于仇恨言论的细微性质。

    English datasets predominantly reflect the perspectives of certain nationalities, which can lead to cultural biases in models and datasets. This is particularly problematic in tasks heavily influenced by subjectivity, such as hate speech detection. To delve into how individuals from different countries perceive hate speech, we introduce CReHate, a cross-cultural re-annotation of the sampled SBIC dataset. This dataset includes annotations from five distinct countries: Australia, Singapore, South Africa, the United Kingdom, and the United States. Our thorough statistical analysis highlights significant differences based on nationality, with only 59.4% of the samples achieving consensus among all countries. We also introduce a culturally sensitive hate speech classifier via transfer learning, adept at capturing perspectives of different nationalities. These findings underscore the need to re-evaluate certain aspects of NLP research, especially with regard to the nuanced nature of hate spe
    
[^78]: 鲁棒性AI生成文本检测的内部维度估计

    Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v1 [cs.CL])

    [http://arxiv.org/abs/2306.04723](http://arxiv.org/abs/2306.04723)

    本文提出了衡量文本内部维度的方法，应用于鲁棒性AI生成文本的检测，展示了人类文本与AI生成文本在内部维度上的差异。

    

    快速提高的AI生成内容的质量使得很难区分人类和AI生成的文本，这可能会对社会产生不良影响。因此，研究人类文本的不变属性变得越来越重要。本文提出了一种人类文本的不变特征，即给定文本样本嵌入集合下的流形的内部维度。我们展示了自然语言流畅文本的平均内部维度在几个基于字母的语言中约为 $9$，而中文约为 $7$，而每种语言的AI生成文本的平均内部维度较低，差约 $1.5$，并且有明显的统计分离。

    Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over text domains and various proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant of human texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings of a given text sample. We show that the average intrinsic dimensionality of fluent texts in natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\approx 1.5$ lower, with a clear statistical separation between
    
[^79]: 注意力不一定意味着在解答中选择正确率很高

    Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy. (arXiv:2305.14596v1 [cs.CL])

    [http://arxiv.org/abs/2305.14596](http://arxiv.org/abs/2305.14596)

    当大型语言模型应用于多项选择题时，其注意力往往会分散到许多无效的词汇符号上，这会导致模型真实性能的低估。本文提出了一种数学形式化方法来研究这种现象，并发现通过使用只含一个示例的上下文学习方法可以提高对有效选择的注意力。

    

    当大型语言模型被应用于零或少样本的鉴别性任务，例如多项选择题时，它们的注意力（即概率质量）会分散在许多无效的词汇符号上。这种在具有相同含义的多个表面形式之间分散导致了模型真实性能的低估，称为“表面形式竞争”（SFC）假说。这促使引入各种概率规范化方法，然而仍存在许多核心问题未解答。我们如何测量SFC或注意力？是否有直接的方法可以增加对有效选择的注意力？增加注意力总是能提高任务准确性吗？我们提出了一种数学形式化方法来研究这种现象，提供了一种量化注意力的度量方法，并确定了一种简单的增加注意力的方法，即通过仅包含答案选项的一个示例进行上下文学习。

    When large language models (LMs) are applied in zero- or few-shot settings to discriminative tasks such as multiple-choice questions, their attentiveness (i.e., probability mass) is spread across many vocabulary tokens that are not valid choices. Such a spread across multiple surface forms with identical meaning is thought to cause an underestimation of a model's true performance, referred to as the "surface form competition" (SFC) hypothesis. This has motivated the introduction of various probability normalization methods. However, many core questions remain unanswered. How do we measure SFC or attentiveness? Are there direct ways of increasing attentiveness on valid choices? Does increasing attentiveness always improve task accuracy? We propose a mathematical formalism for studying this phenomenon, provide a metric for quantifying attentiveness, and identify a simple method for increasing it -namely, in-context learning with even just one example containing answer choices. The form
    
[^80]: 一种用于抽象多文档摘要的层次编码-解码方案

    A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization. (arXiv:2305.08503v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08503](http://arxiv.org/abs/2305.08503)

    本研究提出了一种用于抽象多文档摘要的层次编码-解码方案，在多领域的10个MDS数据集上测试表现最佳。

    

    预训练语言模型（PLM）在抽象单文档摘要（SDS）方面取得了显著的成就。但是，这种好处可能不会轻易扩展到多文档摘要（MDS），因为文档之间的交互更加复杂。以前的工作要么设计新的架构或新的预训练目标，用于MDS，要么将PLM应用于MDS，但未考虑到复杂的文档交互。本文中，我们在编码器和解码器上强制使用层次结构，并寻求更好地利用PLM促进MDS任务的多文档交互作用。我们在10个MDS数据集上测试我们的设计，这些数据集覆盖各种领域。广泛的实验表明，我们提出的方法在所有这些数据集上都能够实现持续改进，优于最先进的MDS方法。

    Pre-trained language models (PLMs) have accomplished impressive achievements in abstractive single-document summarization (SDS). However, such benefits may not be readily extended to muti-document summarization (MDS), where the interactions among documents are more complex. Previous works either design new architectures or new pre-training objectives for MDS, or apply PLMs to MDS without considering the complex document interactions. While the former does not make full use of previous pre-training efforts and may not generalize well across multiple domains, the latter cannot fully attend to the intricate relationships unique to MDS tasks. In this paper, we enforce hierarchy on both the encoder and decoder and seek to make better use of a PLM to facilitate multi-document interactions for the MDS task. We test our design on 10 MDS datasets across a wide range of domains. Extensive experiments show that our proposed method can achieve consistent improvements on all these datasets, outperf
    
[^81]: Multi-step Jailbreaking Privacy Attacks on ChatGPT

    Multi-step Jailbreaking Privacy Attacks on ChatGPT. (arXiv:2304.05197v1 [cs.CL])

    [http://arxiv.org/abs/2304.05197](http://arxiv.org/abs/2304.05197)

    本文探讨了来自OpenAI模型API以及New Bing增强版的ChatGPT对隐私带来的风险，指出应用程序集成的LLMs可能导致比以往更严重的隐私威胁，实验支持该观点。

    

    随着大型语言模型（LLMs）的迅速发展，许多下游NLP任务可以通过良好的提示得到很好的解决。尽管模型开发人员和研究人员努力确保避免从LLMs生成有害内容，但仍然难以引导AI生成的内容（AIGC）为人类带来好处。由于强大的LLMs正在吞噬来自各个领域的现有文本数据（例如，GPT-3训练了45TB的文本），因此人们自然会怀疑训练数据中是否包含私人信息以及这些LLMs及其下游应用程序可以带来什么隐私威胁。在本文中，我们研究了来自OpenAI的模型API和通过ChatGPT增强的New Bing所带来的隐私威胁，并显示应用程序集成的LLMs可能导致比以往更严重的隐私威胁。为此，我们进行了大量实验证明我们的说法，并讨论LLMs的隐私影响。

    With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given good prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI's model APIs and New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause more severe privacy threats ever than before. To this end, we conduct extensive experiments to support our claims and discuss LLMs' privacy implications.
    
[^82]: KPEval：面向细粒度语义评估关键词提取和生成系统

    KPEval: Towards Fine-grained Semantic-based Evaluation of Keyphrase Extraction and Generation Systems. (arXiv:2303.15422v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.15422](http://arxiv.org/abs/2303.15422)

    KPEval是一个综合评估框架，旨在解决关键词提取和生成系统评估中的短板。通过引入四个关键维度，包括显著性、忠实性、多样性和实用性，并设计相应的语义度量指标，KPEval在与人类偏好相关性方面表现更好。使用该框架重新评估了20个关键词系统，并发现模型选择最佳的情况取决于评估维度，实用性是一个重要因素。

    

    尽管关键词提取和生成方法取得了显著的进展，但现行评估方法仅依赖于与人工参考的完全匹配，而忽略了无参考属性。这种方式无法识别生成与参考语义等效或具有实际效用的多样化关键词的系统。为了更好地评估关键词系统的能力，我们提出了一个全面的评估框架KPEval，包含四个关键维度：显著性、忠实性、多样性和实用性。对于每个维度，我们设计了与评估目标相一致的基于语义的度量指标。元评估研究表明，与之前使用的一系列度量指标相比，我们的评估策略更好地与人类偏好相关。使用这个框架，我们重新评估了20个关键词系统，并进一步发现：(1)最好的模型根据评估维度不同而不同；(2)实用性是关键词系统的一个重要方面。

    Despite the significant advancements in keyphrase extraction and keyphrase generation methods, the predominant approach for evaluation only relies on exact matching with human references and disregards reference-free attributes. This scheme fails to recognize systems that generate keyphrases semantically equivalent to the references or diverse keyphrases that carry practical utility. To better assess the capability of keyphrase systems, we propose KPEval, a comprehensive evaluation framework consisting of four critical dimensions: saliency, faithfulness, diversity, and utility. For each dimension, we design semantic-based metrics that align with the evaluation objectives. Meta-evaluation studies demonstrate that our evaluation strategy correlates better with human preferences compared to a range of previously used metrics. Using this framework, we re-evaluate 20 keyphrase systems and further discover that (1) the best model differs depending on the evaluation dimension; (2) the utility
    
[^83]: 基于分数的条件模型的概念代数

    Concept Algebra for Score-Based Conditional Models. (arXiv:2302.03693v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.03693](http://arxiv.org/abs/2302.03693)

    本文研究了基于分数的条件模型中学习表示的结构，并开发了一种数学形式化表达概念被编码为表示空间子空间的思想。利用这个方法，我们提出了一种简单的方法来识别给定概念对应的表示部分，并通过代数操作操纵模型所表达的概念。

    

    本文研究了文本引导生成模型中学习表示的结构，重点关注基于分数的模型。我们聚焦于概念被编码为某种表示空间的子空间（或方向）的思想，并开发了这个思想的数学形式化。利用这个形式化方法，我们展示了有一个自然的表示选择具有这种性质，并且我们开发了一种简单的方法来识别与给定概念对应的表示部分。特别是，这使我们能够通过对表示的代数操作来操纵模型所表达的概念。我们使用稳定扩散在文本引导图像生成的示例中演示了这个思想。

    This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. Here, we focus on the idea that concepts are encoded as subspaces (or directions) of some representation space. We develop a mathematical formalization of this idea.Using this formalism, we show there's a natural choice of representation with this property, and we develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples text-guided image generation, using Stable Diffusion.
    
[^84]: LEXTREME：多语言和多任务的法律领域基准

    LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain. (arXiv:2301.13126v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.13126](http://arxiv.org/abs/2301.13126)

    LEXTREME是一个多语言和多任务的法律领域基准，该基准提供了11个数据集涵盖24种语言的测评，最佳模型（XLM-R large）在数据集和语言综合评分上均达到了61.3。这使得LEXTREME仍然具有挑战性并且有改进空间。

    

    最近，在transformer架构的显著进展推动下，法律自然语言处理领域取得了惊人的增长。为了衡量进展，精心策划和具有挑战性的基准是至关重要的。然而，大多数基准只能处理英文，而在法律自然语言处理方面尚未有多语言基准可用。此外，许多基准已经饱和，最佳模型明显优于最佳人类，并达到近乎完美的分数。我们调查了法律自然语言处理文献，并选择了11个涵盖24种语言的数据集，创建了LEXTREME。为了进行公平比较，我们提出了两种综合评分，一种基于数据集，一种基于语言。最佳基线模型（XLM-R large）在数据集综合评分和语言综合评分上均达到了61.3。这表明LEXTREME仍然非常具有挑战性，并且为改进留下了充足空间。为了方便研究人员和实践者使用，我们将LEXTREME与所有数据一起发布在huggingface上。

    Lately, propelled by the phenomenal advances around the transformer architecture, the legal NLP field has enjoyed spectacular growth. To measure progress, well curated and challenging benchmarks are crucial. However, most benchmarks are English only and in legal NLP specifically there is no multilingual benchmark available yet. Additionally, many benchmarks are saturated, with the best models clearly outperforming the best humans and achieving near perfect scores. We survey the legal NLP literature and select 11 datasets covering 24 languages, creating LEXTREME. To provide a fair comparison, we propose two aggregate scores, one based on the datasets and one on the languages. The best baseline (XLM-R large) achieves both a dataset aggregate score a language aggregate score of 61.3. This indicates that LEXTREME is still very challenging and leaves ample room for improvement. To make it easy for researchers and practitioners to use, we release LEXTREME on huggingface together with all the
    
[^85]: SegAugment: 基于分段增强的语音翻译数据利用最大化

    SegAugment: Maximizing the Utility of Speech Translation Data with Segmentation-based Augmentations. (arXiv:2212.09699v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09699](http://arxiv.org/abs/2212.09699)

    该论文提出了一种新的数据增强策略SegAugment，利用音频分段系统生成数据集的多个句子级别的替代版本，能够提高语音翻译的性能，实验结果显示平均BLEU分数提高了2.5分，甚至在低资源情况下提高了5分。

    

    端到端的语音翻译由于缺乏可用的数据资源而受到限制。虽然大多数资源是基于文档的，但也提供了一种句子级别的版本，但是它只有单个并且是固定的，可能会妨碍数据的有用性。我们提出了一种新的数据增强策略，SegAugment，通过生成数据集的多个句子级别的替代版本来解决这个问题。我们的方法利用音频分段系统，根据不同的长度约束重新分段每个文档的语音，然后通过对齐方法获取目标文本。实验证明，在MuST-C的八种语言对中，我们的方法具有一致的增益，平均增加了2.5 BLEU分数，并在mTEDx的低资源场景中获得了多达5 BLEU分数的提升。此外，当与强大的系统结合时，SegAugment在MuST-C中树立了新的状态记录。最后，我们还展示了该方法可以成功地增强句子级别数据集，并且使得端到端模型在低资源条件下的表现得到了提升。

    End-to-end Speech Translation is hindered by a lack of available data resources. While most of them are based on documents, a sentence-level version is available, which is however single and static, potentially impeding the usefulness of the data. We propose a new data augmentation strategy, SegAugment, to address this issue by generating multiple alternative sentence-level versions of a dataset. Our method utilizes an Audio Segmentation system, which re-segments the speech of each document with different length constraints, after which we obtain the target text via alignment methods. Experiments demonstrate consistent gains across eight language pairs in MuST-C, with an average increase of 2.5 BLEU points, and up to 5 BLEU for low-resource scenarios in mTEDx. Furthermore, when combined with a strong system, SegAugment establishes new state-of-the-art results in MuST-C. Finally, we show that the proposed method can also successfully augment sentence-level datasets, and that it enables 
    

