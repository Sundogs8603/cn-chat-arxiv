# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Overview of the BioLaySumm 2023 Shared Task on Lay Summarization of Biomedical Research Articles.](http://arxiv.org/abs/2309.17332) | 这项研究报道了BioLaySumm 2023共享任务的结果，该任务旨在开发抽象化摘要模型，能够在可控或不可控的环境中生成常规摘要，有两个子任务：常规摘要和可读性控制的摘要。共有20个团队参与了该任务。 |
| [^2] | [Few-Shot Domain Adaptation for Charge Prediction on Unprofessional Descriptions.](http://arxiv.org/abs/2309.17313) | 本文提出了一种少样本领域自适应方法，名为DLCCP，用于非专业描述的指控预测。与现有方法相比，DLCCP可以更好地处理非专业用户的文本风格，并通过优化内容和风格表示来提高预测性能。 |
| [^3] | [STRONG -- Structure Controllable Legal Opinion Summary Generation.](http://arxiv.org/abs/2309.17280) | 本研究提出了一种结构可控的法律意见摘要生成方法，该方法利用预测的论证角色信息来指导模型生成连贯摘要，具有优于强基准模型的性能。 |
| [^4] | [Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency.](http://arxiv.org/abs/2309.17272) | 本文提出了一个名为多角度自一致性（MPSC）的框架，用于提升大规模语言模型在复杂的代码生成任务中的性能。该框架通过从多个角度采样多个输出并构建一个多部分图，利用交叉一致性和内一致性信息来选择最优输出。 |
| [^5] | [Wiki-En-ASR-Adapt: Large-scale synthetic dataset for English ASR Customization.](http://arxiv.org/abs/2309.17267) | 提出了一个用于英语ASR自定义的大规模合成数据集，重点关注稀有和超出词汇（OOV）短语，并介绍了注入困难负样本偏差短语的方法，实验证明可以降低词错误率（WER）和误报警的数量。 |
| [^6] | [Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities.](http://arxiv.org/abs/2309.17255) | 这篇论文综述了在生命科学领域中使用知识图谱的最新发展和进展，并展望了这些技术在未来对这些领域的影响。 |
| [^7] | [Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering.](http://arxiv.org/abs/2309.17249) | 本研究提出了一种名为批量校准（BC）的方法，用于解决大型语言模型中提示脆弱性和偏见因素导致的性能下降问题。BC通过控制批量输入的上下文偏见，统一了现有的校准方法，并具有零-shot和仅推理的特点。 |
| [^8] | [LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games.](http://arxiv.org/abs/2309.17234) | 本文提出使用可评分的谈判游戏作为LLMs的新评估框架，创建了一个多样的测试平台，并通过系统化的零-shot思维链提示（CoT）展示了代理人可以成功谈判。该研究揭示了GPT-4在该任务上的性能差距。 |
| [^9] | [Training and inference of large language models using 8-bit floating point.](http://arxiv.org/abs/2309.17224) | 本文介绍了一种用于选择FP8线性层的缩放方法，该方法基于动态更新每个张量的权重、梯度和激活的尺度。通过将这种方法应用于大型语言模型的训练和验证，我们证明了其在提高计算效率方面的有效性。 |
| [^10] | [Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training.](http://arxiv.org/abs/2309.17179) | Alphazero类似的树搜索框架TS-LLM可以利用学习的价值函数指导大型语言模型的解码和训练，不仅适用于推理任务，还适用于其他任务，并且在不同大小的语言模型上具有普适性和可扩展性 |
| [^11] | [RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds.](http://arxiv.org/abs/2309.17176) | RLAdapter引入了一个框架，将大型语言模型（LLM）与强化学习相结合，以提高在稀疏奖励环境中的策略学习性能。这通过解决LLM在理解下游任务方面的困难，以及通过避免使用不可访问的模型权重或大量计算资源来微调LLM的方式实现。 |
| [^12] | [Comparative Analysis of Named Entity Recognition in the Dungeons and Dragons Domain.](http://arxiv.org/abs/2309.17171) | 我们比较分析了领域特定的命名实体识别在龙与地下城领域的表现，发现在没有进行任何修改的情况下，Flair、Trankit和Spacy在龙与地下城背景中的命名实体识别方面表现最好。 |
| [^13] | [An evaluation of GPT models for phenotype concept recognition.](http://arxiv.org/abs/2309.17169) | 本研究评估了最新的GPT模型在临床深度表型学中的性能，并发现目前这些模型尚未达到最先进的性能水平。 |
| [^14] | [DyVal: Graph-informed Dynamic Evaluation of Large Language Models.](http://arxiv.org/abs/2309.17167) | DyVal是一种基于图形信息的大型语言模型动态评估协议，通过动态生成具有可控复杂性的评估样本，评估了各种LLM在推理任务上的性能，发现它们在这些挑战性样本上表现更差。 |
| [^15] | [LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud.](http://arxiv.org/abs/2309.17157) | LatticeGen是一个协作框架，通过将真实生成的文本与噪声混合并隐藏在格子中，以保护用户的隐私。实验证明，LatticeGen能够在面对强攻击时成功保护真实生成，超过50%的语义仍然隐藏。 |
| [^16] | [Using Large Language Models for Qualitative Analysis can Introduce Serious Bias.](http://arxiv.org/abs/2309.17147) | 使用大型语言模型（LLMs）对定性分析进行注释可能引入偏见和测量误差，相比之下，使用高质量的人工注释和灵活编码对简单监督模型进行训练可以更好地减少偏见和误差。 |
| [^17] | [Promoting Generalized Cross-lingual Question Answering in Few-resource Scenarios via Self-knowledge Distillation.](http://arxiv.org/abs/2309.17134) | 通过自我知识蒸馏和跨语言采样，我们提出一种方法来提高在资源稀缺情况下的广义跨语言问答迁移能力，并引入mAP@k系数来实现平衡有效的知识传递。 |
| [^18] | [Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering.](http://arxiv.org/abs/2309.17133) | 本文提出了一种精细化的后期交互多模检索方法（FLMR）来改进检索增强的视觉问答中的知识检索。FLMR通过获取补充的图像表示并使用与现有基于文本的模型相对齐的视觉模型，解决了RA-VQA中检索器的两个主要限制。 |
| [^19] | [Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?.](http://arxiv.org/abs/2309.17122) | 本论文评估了各种大型语言模型在RDF知识图创建和理解中的能力，提出了一组任务来探究模型的解析、理解、分析和创建能力，并且对商业可用和免费的离线模型进行了评估和比较。 |
| [^20] | [SCALE: Synergized Collaboration of Asymmetric Language Translation Engines.](http://arxiv.org/abs/2309.17061) | SCALE是一种将专业翻译模型和大型语言模型连接为统一翻译引擎的协同框架，通过引入STM的翻译进一步提升LLM的性能，从而在低资源环境中显著优于其他模型。 |
| [^21] | [Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large Language Models.](http://arxiv.org/abs/2309.17050) | 该研究提出了一种增强型大型语言模型的检索可解释的长篇法律问题回答方法，通过"先检索后阅读"的流程生成详细回答。同时，还引入了一个新的数据集并进行了实验验证，结果显示有希望的性能。 |
| [^22] | [Contextualising Levels of Language Resourcedness affecting Digital Processing of Text.](http://arxiv.org/abs/2309.17035) | 本文讨论了数字处理文本时，语言资源水平对其影响的上下文化分类，并提出了一个将语言划分为五个等级的矩阵，从而解决了将语言只划分为LRL和HRL两种类型的问题。 |
| [^23] | [Benchmarking Cognitive Biases in Large Language Models as Evaluators.](http://arxiv.org/abs/2309.17012) | 本研究对15个不同大小的大型语言模型进行了评估，发现它们作为评估器存在认知偏差，尤其在文本质量评估中表现出较强的偏见，这对其鲁棒性提出了质疑。同时，研究还发现了人类和机器偏好之间的相关性。 |
| [^24] | [I Wish to Have an Argument: Argumentative Reasoning in Large Language Models.](http://arxiv.org/abs/2309.16938) | 该论文评估了当代大型语言模型在争论性推理方面的能力，并发现其争论性推理性能与输入和输出表示密切相关。研究还发现了“典型效应”，并探讨了在思维链提示下解决病态问题时的性能优势。 |
| [^25] | [SSHR: Leveraging Self-supervised Hierarchical Representations for Multilingual Automatic Speech Recognition.](http://arxiv.org/abs/2309.16937) | 本研究提出了一种利用自监督层次表示（SSHR）优化多语种自动语音识别的方法。通过分析自监督学习模型的不同层次表示，提取出语言相关帧和特定内容，并引导模型在最终层次获取更多内容相关信息。 |
| [^26] | [Towards a Unified Framework for Adaptable Problematic Content Detection via Continual Learning.](http://arxiv.org/abs/2309.16905) | 本文提出了一个统一框架，通过连续学习来适应性地检测问题内容。该框架整合了来自8个来源的15个注释架构的84个相关任务，并创建了一种新的进展度量标准：优先考虑类别适应性。 |
| [^27] | [A Sign Language Recognition System with Pepper, Lightweight-Transformer, and LLM.](http://arxiv.org/abs/2309.16898) | 本研究提出了一种利用轻量级深度神经网络架构的手语识别系统，使Pepper机器人能够理解美国手语并实现非语言的人机交互。通过优化嵌入式系统的模型和智能机器人互动来生成自然的共语手势反应，为更有机和直观的人形机器人对话提供基础。实证结果表明了增强手语理解的巨大潜力。 |
| [^28] | [DeBERTinha: A Multistep Approach to Adapt DebertaV3 XSmall for Brazilian Portuguese Natural Language Processing Task.](http://arxiv.org/abs/2309.16844) | DeBERTinha是一种通过多步骤训练和微调适应DebertaV3 XSmall模型的方法，用于巴西葡萄牙语自然语言处理任务。它在命名实体识别、情感分析和判断句子相关性等任务上表现出优越性能。 |
| [^29] | [Curriculum-Driven Edubot: A Framework for Developing Language Learning Chatbots Through Synthesizing Conversational Data.](http://arxiv.org/abs/2309.16804) | 本论文提出了一种框架，通过综合对话数据开发语言学习聊天机器人。该机器人结合聊天机器人互动特征和英语教科书的系统材料，帮助学生提高口语技巧。该方法通过提取教科书主题并使用大型语言模型生成相关对话，通过微调机器人的对话数据创建一个课程驱动型聊天机器人。用户研究表明，该聊天机器人在引领基于课程的对话和适应用户英语水平方面优于ChatGPT。该方法将传统教科书方法与对话式人工智能相结合，为学习者提供了一个与课程相匹配并提供用户个性化对话练习的互动工具。 |
| [^30] | [Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution.](http://arxiv.org/abs/2309.16797) | Promptbreeder是一种通过提示演化进行自我改进的通用机制，它可以优化大型语言模型的推理能力，并在算术和常识推理上优于现有的提示策略。 |
| [^31] | [Hallucination Reduction in Long Input Text Summarization.](http://arxiv.org/abs/2309.16781) | 本文旨在减少长篇文本摘要中的幻觉输出，通过在Longformer Encoder-Decoder模型的微调中采用数据过滤和联合实体和摘要生成技术，我们成功提高了生成摘要的质量。 |
| [^32] | [How many words does ChatGPT know? The answer is ChatWords.](http://arxiv.org/abs/2309.16777) | 这项研究通过开发ChatWords，一个自动化测试系统，为评估ChatGPT和其他NLP AI工具的词汇知识做出了贡献，以解决它们在产生错误答案方面的限制和错觉问题。 |
| [^33] | [Persona-Coded Poly-Encoder: Persona-Guided Multi-Stream Conversational Sentence Scoring.](http://arxiv.org/abs/2309.16770) | 本论文提出了一种新颖的Persona编码多流程对话句子评分方法，利用个人角色信息来提高对话生成的质量。 |
| [^34] | [Decoding Imagery: Unleashing Large Language Models.](http://arxiv.org/abs/2309.16705) | 该论文研究了Google Bard这个多模态大型语言模型的能力，发现Bard在将视觉和语言分析相结合方面依赖于对图像进行有根据的猜测，可以解决视觉上有挑战的问题但无法修改原始视觉对象。 |
| [^35] | [MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool.](http://arxiv.org/abs/2309.16701) | 本文提出了一个名为MVMR的任务，旨在给定文本查询从大量视频集中定位视频帧。我们通过已有数据集进行相似性筛选来构建数据集，并引入三个MVMR数据集。我们采用了嵌入式文本相似度匹配和视频-语言对齐技术来计算相关性得分，并为MVMR任务开发了一个强大的模型，Reliable Mutual Matching Network (RMMN)。 |
| [^36] | [Demystifying CLIP Data.](http://arxiv.org/abs/2309.16671) | CLIP的成功主要归功于其数据而非模型架构或预训练目标。我们通过元数据整理方法引入了MetaCLIP，该方法从原始数据池和元数据中生成一个平衡的子集，提供了更加详细的数据信息。在实验中，我们发现MetaCLIP在处理400M个图像-文本数据对时取得了良好的性能。 |
| [^37] | [A Comprehensive Survey of Document-level Relation Extraction (2016-2022).](http://arxiv.org/abs/2309.16396) | 这篇综述论文介绍了文档级关系抽取（DocRE）的最新进展，与句子级关系抽取相比，DocRE提供了更广泛的上下文分析，涉及跨越多个句子或段落的关系抽取，可用于构建和自动填充知识库。 |
| [^38] | [Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning.](http://arxiv.org/abs/2309.13860) | Fast-HuBERT是一种高效的自监督语音表示学习训练框架，通过分析HuBERT预训练过程中的计算成本，并引入一系列效率优化策略，实现了在Librispeech 960h基准上的5.2倍加速，并且在前人工作中得到了一致的改进。 |
| [^39] | [Cardiovascular Disease Risk Prediction via Social Media.](http://arxiv.org/abs/2309.13147) | 该论文通过分析Twitter上的情感内容，引入了一个新颖的与心血管疾病相关的关键词词典，并使用机器学习模型评估个体的心血管疾病风险。研究发现，分析情感内容能够超越仅使用人口统计数据进行预测，识别出潜在风险患者。这项研究显示了社交媒体在心血管疾病风险预测方面的潜力。 |
| [^40] | [DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for Hospitalized Patients.](http://arxiv.org/abs/2309.12625) | DRG-LLaMA是一个通过在临床笔记上细调的大型语言模型，用于改进住院患者的诊断相关分组预测。在多个评估指标上，DRG-LLaMA-7B相对于其他模型取得了显著的改进，并能够高准确度地预测基本DRG和并发症/合并症（CC）/重大并发症或合并症（MCC）的状态。 |
| [^41] | [Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination.](http://arxiv.org/abs/2309.11696) | 本研究提出了一种计算仿生记忆机制，配备了参数高效的微调模式，用于个性化LLMs。实验证明了该方法的有效性和可行性。 |
| [^42] | [MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods.](http://arxiv.org/abs/2309.10966) | 本文提出了MBR微调和QE微调方法，将训练时的质量提升蒸馏到基准模型中，从而在推断时使用高效的解码算法。实验证明，这些微调方法能显著提升模型性能，甚至超过基准模型。 |
| [^43] | [Contrastive Decoding Improves Reasoning in Large Language Models.](http://arxiv.org/abs/2309.09117) | 对比解码是一种简单、计算量轻、无需训练的文本生成方法，通过最大化强模型和弱模型之间的似然差异，在各种推理任务上取得了显著改进。 |
| [^44] | [The Dynamical Principles of Storytelling.](http://arxiv.org/abs/2309.07797) | 这项研究揭示了讲故事的开头部分通常会遵循动作原则，并且这种顺序可能与西方传统讲故事的有关。 |
| [^45] | [Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs.](http://arxiv.org/abs/2309.07311) | 本文通过对掩码语言模型中的语法习得进行案例研究，发现在训练的一个短暂窗口内，模型突然获得了语法注意结构(SAS)，并伴随着损失的陡峭下降。SAS对随后习得语言能力起到了重要的促进作用。 |
| [^46] | [Narrative as a Dynamical System.](http://arxiv.org/abs/2309.06600) | 叙事可以被视为一个动力系统，其演化可以用一个行动积分来描述，并且平均路径与行动原理一致。 |
| [^47] | [Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning.](http://arxiv.org/abs/2309.06553) | 这项工作介绍了一种基于离线逆向强化学习的提示评估与优化方法，通过利用离线数据集和逆向强化学习，预测提示性能、提高成本效益、生成易读的结果。 |
| [^48] | [PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis.](http://arxiv.org/abs/2309.05833) | 本文提出了一种通过提示检索增强的大语言模型（LLM）来增强云事件根本原因分析工具中置信度估计的方法。 |
| [^49] | [Neural-Hidden-CRF: A Robust Weakly-Supervised Sequence Labeler.](http://arxiv.org/abs/2309.05086) | Neural-Hidden-CRF是一种鲁棒的弱监督序列标注模型，通过引入隐藏CRF层和利用强大的语言模型，它在各项基准测试中取得了最新的最优结果。 |
| [^50] | [Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models.](http://arxiv.org/abs/2308.16149) | Jais和Jais-chat是新的以阿拉伯语为中心的开放式生成式大型语言模型，具有13亿参数，在阿拉伯语方面表现出优异的知识和推理能力，并且在英语方面也具有竞争力。这些模型的发布旨在促进阿拉伯语LLMs的研究。 |
| [^51] | [Scope is all you need: Transforming LLMs for HPC Code.](http://arxiv.org/abs/2308.09440) | 本研究旨在质疑现有大型语言模型（LLMs）在高性能计算（HPC）领域中的应用，并通过开发针对特定领域的小型LLMs来解决该问题。具体而言，我们为HPC开发了一种名为Tokompiler的新型标记器，用于代码预处理和编译任务。 |
| [^52] | [Backward Reasoning in Large Language Models for Verification.](http://arxiv.org/abs/2308.07758) | 本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。 |
| [^53] | [An Overview Of Temporal Commonsense Reasoning and Acquisition.](http://arxiv.org/abs/2308.00002) | 本文综述了时间常识推理领域的研究进展，重点关注通过增强语言模型的性能来提高推理能力，并对多个数据集进行评估。然而，这些增强模型仍然难以达到人类水平的推理能力。 |
| [^54] | [Mitigating Label Bias via Decoupled Confident Learning.](http://arxiv.org/abs/2307.08945) | 这项研究提出了一种名为DeCoLe的修剪方法，用于减轻标签偏见问题。研究在合成数据集和仇恨言论检测领域取得了成功的结果。 |
| [^55] | [Can Generative Large Language Models Perform ASR Error Correction?.](http://arxiv.org/abs/2307.04172) | 本文研究了大型生成语言模型在ASR错误修正中的应用，以ChatGPT为例，探讨了其在零-shot或1-shot设置下的能力。实验结果表明，使用ChatGPT模型进行错误修正可以显著提高ASR系统的性能。 |
| [^56] | [Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning.](http://arxiv.org/abs/2306.14565) | 本论文通过引入第一个大型多样化的视觉指令调整数据集，提出了一种解决大规模多模态模型中幻觉问题的方法。通过设计包含正负指令的数据集和提出的评估方法，能够更准确地衡量模型产生的幻觉。 |
| [^57] | [Modeling Human-like Concept Learning with Bayesian Inference over Natural Language.](http://arxiv.org/abs/2306.02797) | 该论文通过在自然语言中进行贝叶斯推理来模拟人类类人概念学习，使用大型语言模型作为提议分布并拟合先验以更好地模拟人类学习者，并在生成性和逻辑性概念上进行实验评估。 |
| [^58] | [Contextual Vision Transformers for Robust Representation Learning.](http://arxiv.org/abs/2305.19402) | 上下文视觉变换器(ContextViT)用于生成图像的鲁棒特征表示，引入了一个额外的上下文令牌，可以解释掉特定于组的协变量结构，同时保持跨组共享的核心视觉特征，能够在监督微调、半监督学习以及主动学习等方面得到应用。 |
| [^59] | [Multiscale Positive-Unlabeled Detection of AI-Generated Texts.](http://arxiv.org/abs/2305.18149) | 本文提出了一种多尺度正负样本的训练框架，以解决多尺度AI生成文本的检测问题。通过将短机器文本标记为“未标记”来重新表述文本分类问题，并提出了一个规则化损失函数来优化检测性能，有效性能显著优于现有的方法。 |
| [^60] | [Improving Language Models with Advantage-based Offline Policy Gradients.](http://arxiv.org/abs/2305.14718) | 本文介绍了一种简单的训练算法Left-over Lunch RL （LoL-RL），使用离线策略梯度学习任何序列到序列数据，从而实现优化LM效用的方法。 |
| [^61] | [On Dataset Transferability in Active Learning for Transformers.](http://arxiv.org/abs/2305.09807) | 本文研究了基于transformer的预训练语言模型的主动学习中数据集的可迁移性问题，发现具有相似获取序列的主动学习方法产生的数据集在不同模型之间具有高度的可迁移性。 |
| [^62] | [PESTS: Persian_English Cross Lingual Corpus for Semantic Textual Similarity.](http://arxiv.org/abs/2305.07893) | 本研究提出了跨语言的语义相似性模型PESTS，并通过波斯语-英语的跨语言语料库来验证模型的准确性。 |
| [^63] | [Learning ASR pathways: A sparse multilingual ASR model.](http://arxiv.org/abs/2209.05735) | 本文提出了一种稀疏的多语言ASR模型，通过激活语言特定的子网络来显式地学习每种语言的参数，同时通过联合多语言训练实现对低资源语言的知识转移，相比于密集模型和语言不可知的剪枝模型，在低资源语言上提供更好的性能。 |
| [^64] | [Human Languages with Greater Information Density Increase Communication Speed, but Decrease Conversation Breadth.](http://arxiv.org/abs/2112.08491) | 语言的信息密度越高，交流速度越快，但对话广度减少。这些研究结果揭示了语言结构对人类交流的重要影响。 |

# 详细

[^1]: 关于BioLaySumm 2023共享任务的综述：生物医学研究文章的简化摘要

    Overview of the BioLaySumm 2023 Shared Task on Lay Summarization of Biomedical Research Articles. (arXiv:2309.17332v1 [cs.CL])

    [http://arxiv.org/abs/2309.17332](http://arxiv.org/abs/2309.17332)

    这项研究报道了BioLaySumm 2023共享任务的结果，该任务旨在开发抽象化摘要模型，能够在可控或不可控的环境中生成常规摘要，有两个子任务：常规摘要和可读性控制的摘要。共有20个团队参与了该任务。

    

    本文介绍了在ACL 2023的BioNLP研讨会上举办的关于生物医学研究文章简化摘要的共享任务（BioLaySumm）的结果。该共享任务的目标是开发能够生成“常规摘要”（即可理解给非技术人员的摘要）的抽象化摘要模型，无论在可控或不可控的环境中。共享任务包括两个子任务：1）常规摘要，参与者需要根据提供的完整文章文本和对应的摘要作为输入，构建产生常规摘要的模型；2）可读性控制的摘要，参与者需要根据文章的主要文本作为输入，训练模型来生成技术摘要和常规摘要。除了总体结果，我们还报告了BioLaySumm共享任务的设置和见解，共有20个参与团队参与了这两个子任务。

    This paper presents the results of the shared task on Lay Summarisation of Biomedical Research Articles (BioLaySumm), hosted at the BioNLP Workshop at ACL 2023. The goal of this shared task is to develop abstractive summarisation models capable of generating "lay summaries" (i.e., summaries that are comprehensible to non-technical audiences) in both a controllable and non-controllable setting. There are two subtasks: 1) Lay Summarisation, where the goal is for participants to build models for lay summary generation only, given the full article text and the corresponding abstract as input; and 2) Readability-controlled Summarisation, where the goal is for participants to train models to generate both the technical abstract and the lay summary, given an article's main text as input. In addition to overall results, we report on the setup and insights from the BioLaySumm shared task, which attracted a total of 20 participating teams across both subtasks.
    
[^2]: 少样本领域自适应用于非专业描述的指控预测

    Few-Shot Domain Adaptation for Charge Prediction on Unprofessional Descriptions. (arXiv:2309.17313v1 [cs.CL])

    [http://arxiv.org/abs/2309.17313](http://arxiv.org/abs/2309.17313)

    本文提出了一种少样本领域自适应方法，名为DLCCP，用于非专业描述的指控预测。与现有方法相比，DLCCP可以更好地处理非专业用户的文本风格，并通过优化内容和风格表示来提高预测性能。

    

    最近的研究考虑了专业法律语言风格（PLLS）文本，在指控预测任务上显示出了有希望的结果。然而，非专业用户对这样的预测服务也有增长的需求。PLLS文本和非PLLS文本之间存在明显的领域差异，由非专业人士表达，这降低了当前SOTA模型在非PLLS文本上的性能。一个关键挑战是大多数指控类别的非PLLS数据的稀缺性。本文提出了一种名为Disentangled Legal Content for Charge Prediction（DLCCP）的新颖的少样本领域自适应（FSDA）方法。与现有的仅仅进行实例级别对齐而不考虑存在于潜在特征中的文本风格信息的FSDA工作相比，DLCCP（1）分离内容和风格表示，以更好地学习与领域无关的法律内容，并使用仔细设计的内容和风格空间的优化目标，（2）使用组成的方法来加强对风格一致性的建模，并在域适应过程中利用特定于风格的转换和共享机制。

    Recent works considering professional legal-linguistic style (PLLS) texts have shown promising results on the charge prediction task. However, unprofessional users also show an increasing demand on such a prediction service. There is a clear domain discrepancy between PLLS texts and non-PLLS texts expressed by those laypersons, which degrades the current SOTA models' performance on non-PLLS texts. A key challenge is the scarcity of non-PLLS data for most charge classes. This paper proposes a novel few-shot domain adaptation (FSDA) method named Disentangled Legal Content for Charge Prediction (DLCCP). Compared with existing FSDA works, which solely perform instance-level alignment without considering the negative impact of text style information existing in latent features, DLCCP (1) disentangles the content and style representations for better domain-invariant legal content learning with carefully designed optimization goals for content and style spaces and, (2) employs the constitutiv
    
[^3]: STRONG - 结构可控的法律意见摘要生成

    STRONG -- Structure Controllable Legal Opinion Summary Generation. (arXiv:2309.17280v1 [cs.CL])

    [http://arxiv.org/abs/2309.17280](http://arxiv.org/abs/2309.17280)

    本研究提出了一种结构可控的法律意见摘要生成方法，该方法利用预测的论证角色信息来指导模型生成连贯摘要，具有优于强基准模型的性能。

    

    我们提出了一种适用于长篇法律意见结构可控的摘要生成方法，考虑了文档的论证结构。我们的方法利用预测的论证角色信息来指导模型生成具有一定结构模式的连贯摘要。我们在法律意见数据集上展示了我们方法的有效性，并证明其在ROUGE、BERTScore和结构相似性方面优于几个强基准模型。

    We propose an approach for the structure controllable summarization of long legal opinions that considers the argument structure of the document. Our approach involves using predicted argument role information to guide the model in generating coherent summaries that follow a provided structure pattern. We demonstrate the effectiveness of our approach on a dataset of legal opinions and show that it outperforms several strong baselines with respect to ROUGE, BERTScore, and structure similarity.
    
[^4]: 提升大规模语言模型在编码中的能力通过多角度自一致性

    Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency. (arXiv:2309.17272v1 [cs.CL])

    [http://arxiv.org/abs/2309.17272](http://arxiv.org/abs/2309.17272)

    本文提出了一个名为多角度自一致性（MPSC）的框架，用于提升大规模语言模型在复杂的代码生成任务中的性能。该框架通过从多个角度采样多个输出并构建一个多部分图，利用交叉一致性和内一致性信息来选择最优输出。

    

    大规模语言模型（LLMs）在文本生成方面展现了卓越的能力。然而，在复杂的推理任务，如代码生成中，LLMs仍然难以在一次尝试中生成正确的答案。先前的研究通过聚合多个输出，利用它们之间的一致性来探索解决方案。然而，这些研究没有全面地从不同的角度捕捉这种一致性。在本文中，我们提出了一种名为多角度自一致性（MPSC）框架的新的解码策略，用于LLM，它将来自多个角度的输出之间的交叉一致性和单个角度内的内一致性结合起来。具体而言，我们要求LLMs对给定查询从各个角度采样多个多样化的输出，并基于它们构建一个多部分图。通过两个预定义的一致性度量，我们将交叉一致性和内一致性信息嵌入到图中。最佳选择是根据这些一致性度量来选择输出。

    Large language models (LLMs) have exhibited remarkable ability in textual generation. However, in complex reasoning tasks such as code generation, generating the correct answer in a single attempt remains a formidable challenge for LLMs. Previous research has explored solutions by aggregating multiple outputs, leveraging the consistency among them. However, none of them have comprehensively captured this consistency from different perspectives. In this paper, we propose the Multi-Perspective Self-Consistency (MPSC) framework, a novel decoding strategy for LLM that incorporates both inter-consistency across outputs from multiple perspectives and intra-consistency within a single perspective. Specifically, we ask LLMs to sample multiple diverse outputs from various perspectives for a given query and then construct a multipartite graph based on them. With two predefined measures of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice is th
    
[^5]: Wiki-En-ASR-Adapt: 用于英语ASR自定义的大规模合成数据集

    Wiki-En-ASR-Adapt: Large-scale synthetic dataset for English ASR Customization. (arXiv:2309.17267v1 [eess.AS])

    [http://arxiv.org/abs/2309.17267](http://arxiv.org/abs/2309.17267)

    提出了一个用于英语ASR自定义的大规模合成数据集，重点关注稀有和超出词汇（OOV）短语，并介绍了注入困难负样本偏差短语的方法，实验证明可以降低词错误率（WER）和误报警的数量。

    

    我们提出了一个首个大规模的公开合成数据集，用于定制化自动语音识别（ASR）的上下文拼写检查，重点关注稀有和超脱词汇（OOV）短语，如专有名词或术语。所提出的方法允许创建数百万个真实例子，用于模拟破损的ASR假设和模拟非平凡偏差列表以进行定制任务。此外，我们提出了向训练示例的模拟偏差列表注入两种“困难负样本”的方法，并描述了自动挖掘这些困难负样本的过程。我们报告了在所提出的数据集上训练开源定制模型的实验，并展示了注入困难负样本偏差短语可以减小词错误率（WER）和误报警的数量。

    We present a first large-scale public synthetic dataset for contextual spellchecking customization of automatic speech recognition (ASR) with focus on diverse rare and out-of-vocabulary (OOV) phrases, such as proper names or terms. The proposed approach allows creating millions of realistic examples of corrupted ASR hypotheses and simulate non-trivial biasing lists for the customization task. Furthermore, we propose injecting two types of ``hard negatives" to the simulated biasing lists in training examples and describe our procedures to automatically mine them. We report experiments with training an open-source customization model on the proposed dataset and show that the injection of hard negative biasing phrases decreases WER and the number of false alarms.
    
[^6]: 生命科学领域的知识图谱：最新发展、挑战和机遇

    Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities. (arXiv:2309.17255v1 [cs.AI])

    [http://arxiv.org/abs/2309.17255](http://arxiv.org/abs/2309.17255)

    这篇论文综述了在生命科学领域中使用知识图谱的最新发展和进展，并展望了这些技术在未来对这些领域的影响。

    

    生命科学是研究生物和生命过程的学科，包括化学、生物学、医学和一系列其他相关学科。生命科学的研究工作非常依赖数据，因为它们产生和消费大量科学数据，其中很多数据具有关系和图结构。数据的数量和其中涉及的科学概念和关系的复杂性推动了应用先进的知识驱动技术来管理和解释数据，最终目标是推动科学发现。在这篇综述和观点论文中，我们讨论了知识图谱在生命科学中的最新发展和进展，并展望了这些技术在未来对这些领域的影响。我们重点关注三个主题：知识图谱的构建和管理，以及在新发现的过程中使用知识图谱和相关技术。

    The term life sciences refers to the disciplines that study living organisms and life processes, and include chemistry, biology, medicine, and a range of other related disciplines. Research efforts in life sciences are heavily data-driven, as they produce and consume vast amounts of scientific data, much of which is intrinsically relational and graph-structured.  The volume of data and the complexity of scientific concepts and relations referred to therein promote the application of advanced knowledge-driven technologies for managing and interpreting data, with the ultimate aim to advance scientific discovery.  In this survey and position paper, we discuss recent developments and advances in the use of graph-based technologies in life sciences and set out a vision for how these technologies will impact these fields into the future. We focus on three broad topics: the construction and management of Knowledge Graphs (KGs), the use of KGs and associated technologies in the discovery of ne
    
[^7]: 批量校准：重新思考上下文学习和提示工程的校准方法

    Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering. (arXiv:2309.17249v1 [cs.CL])

    [http://arxiv.org/abs/2309.17249](http://arxiv.org/abs/2309.17249)

    本研究提出了一种名为批量校准（BC）的方法，用于解决大型语言模型中提示脆弱性和偏见因素导致的性能下降问题。BC通过控制批量输入的上下文偏见，统一了现有的校准方法，并具有零-shot和仅推理的特点。

    

    提示和上下文学习已成为大型语言模型（LLM）的高效学习范式。然而，LLM存在提示脆弱性和各种偏见因素，包括但不限于格式、选择性的表达方式和上下文学习示例。为解决这个导致性能下降的问题，已经开发了校准方法来减轻这些偏见的影响并恢复LLM的性能。在这项工作中，我们首先对现有的校准方法进行了系统分析，提供了统一的观点并揭示了失败案例。受这些分析的启发，我们提出了批量校准（BC），这是一种简单而直观的方法，可以从批量输入中控制上下文偏见，统一了各种先前的方法，并有效地解决了上述问题。BC是零-shot、仅推理和额外成本可忽略。在少-shot设置中，我们进一步扩展BC以实现全部翻译

    Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allo
    
[^8]: LLM-辩论: 使用交互式多智能体协商游戏评估LLMs

    LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games. (arXiv:2309.17234v1 [cs.CL])

    [http://arxiv.org/abs/2309.17234](http://arxiv.org/abs/2309.17234)

    本文提出使用可评分的谈判游戏作为LLMs的新评估框架，创建了一个多样的测试平台，并通过系统化的零-shot思维链提示（CoT）展示了代理人可以成功谈判。该研究揭示了GPT-4在该任务上的性能差距。

    

    越来越多的人对使用大型语言模型（LLMs）作为代理人来解决可能需要评估复杂情况的现实任务感兴趣。然而，我们对LLMs的推理和决策能力有限的理解，在某种程度上是由于缺乏专门的评估基准。由于谈判和妥协是我们日常沟通和合作的关键方面，我们提出使用可评分的谈判游戏作为LLMs的新评估框架。我们创建了一个多样的基于文本的、多智能体的、多问题的、语义丰富的谈判游戏测试平台，难度可调。为了解决这一挑战，代理人需要具备强大的算术、推理、探索和规划能力，同时无缝地整合它们。通过系统化的零-shot思维链提示（CoT），我们展示了代理人可以进行谈判并持续达成成功交易。我们用多个指标量化性能，并观察到GPT-4与原文之间存在很大差距。

    There is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs' reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and 
    
[^9]: 使用8位浮点数训练和推断大型语言模型

    Training and inference of large language models using 8-bit floating point. (arXiv:2309.17224v1 [cs.LG])

    [http://arxiv.org/abs/2309.17224](http://arxiv.org/abs/2309.17224)

    本文介绍了一种用于选择FP8线性层的缩放方法，该方法基于动态更新每个张量的权重、梯度和激活的尺度。通过将这种方法应用于大型语言模型的训练和验证，我们证明了其在提高计算效率方面的有效性。

    

    FP8格式正在受到青睐，以提高训练和推断大型深度学习模型的计算效率。它们的主要挑战是需要谨慎选择缩放以防止由于较高精度格式的动态范围的减少而导致性能下降。尽管关于选择INT格式的这些缩放因子的文献很多，但对于FP8来说，这一关键方面尚未得到解决。本文提出了一种基于动态更新权重、梯度和激活的每个张量尺度的FP8线性层缩放选择方法。我们将这种方法应用于使用FP8训练和验证GPT和Llama 2等类型的大型语言模型，模型大小范围从111M到70B不等。为了便于理解FP8的动态特性，在训练和推断过程中，我们的结果附带了权重、激活和梯度的每个张量尺度分布的图示。

    FP8 formats are gaining popularity to boost the computational efficiency for training and inference of large deep learning models. Their main challenge is that a careful choice of scaling is needed to prevent degradation due to the reduced dynamic range compared to higher-precision formats. Although there exists ample literature about selecting such scalings for INT formats, this critical aspect has yet to be addressed for FP8. This paper presents a methodology to select the scalings for FP8 linear layers, based on dynamically updating per-tensor scales for the weights, gradients and activations. We apply this methodology to train and validate large language models of the type of GPT and Llama 2 using FP8, for model sizes ranging from 111M to 70B. To facilitate the understanding of the FP8 dynamics, our results are accompanied by plots of the per-tensor scale distribution for weights, activations and gradients during both training and inference.
    
[^10]: Alphazero类似的树搜索可以指导大型语言模型的解码和训练

    Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training. (arXiv:2309.17179v1 [cs.LG])

    [http://arxiv.org/abs/2309.17179](http://arxiv.org/abs/2309.17179)

    Alphazero类似的树搜索框架TS-LLM可以利用学习的价值函数指导大型语言模型的解码和训练，不仅适用于推理任务，还适用于其他任务，并且在不同大小的语言模型上具有普适性和可扩展性

    

    大型语言模型 (LLM) 通常采用采样或束搜索，结合 Chain-of-Thought (CoT) 等提示来提高推理和解码能力。最近的研究如 Tree-of-Thought (ToT) 和 Reasoning via Planning (RAP) 旨在通过利用树搜索算法来引导多步推理，来增强LLM的推理能力。这些方法主要关注LLM在推理过程中的推理能力，并且严重依赖人为设计的提示来激活LLM作为一个价值函数，缺乏普适性和可扩展性。为了解决这些限制，我们提出了一种AlphaZero类似的用于LLM的树搜索框架 (称为TS-LLM)，系统地说明了如何通过学习的价值函数利用树搜索来指导LLM的解码能力。TS-LLM在两个关键方面与众不同：(1)通过利用学习的价值函数，我们的方法可以普适地应用于除了推理之外的不同任务 (例如RLHF对齐)，以及任何大小的LLM，而不需要提示

    Large language models (LLMs) typically employ sampling or beam search, accompanied by prompts such as Chain-of-Thought (CoT), to boost reasoning and decoding ability. Recent work like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by utilizing tree-search algorithms to guide multi-step reasoning. These methods mainly focus on LLMs' reasoning ability during inference and heavily rely on human-designed prompts to activate LLM as a value function, which lacks general applicability and scalability. To address these limitations, we present an AlphaZero-like tree-search framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLMs' decoding ability. TS-LLM distinguishes itself in two key ways: (1) Leveraging a learned value function, our approach can be generally applied to different tasks beyond reasoning (such as RLHF alignment), and LLMs of any size, without prompting a
    
[^11]: RLAdapter：在开放环境中将大型语言模型与强化学习相结合

    RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds. (arXiv:2309.17176v1 [cs.AI])

    [http://arxiv.org/abs/2309.17176](http://arxiv.org/abs/2309.17176)

    RLAdapter引入了一个框架，将大型语言模型（LLM）与强化学习相结合，以提高在稀疏奖励环境中的策略学习性能。这通过解决LLM在理解下游任务方面的困难，以及通过避免使用不可访问的模型权重或大量计算资源来微调LLM的方式实现。

    

    强化学习在决策问题中取得了显著的成功，但通常需要与环境进行大量的交互，在稀疏奖励环境中学习有意义的策略是具有挑战性的。大型语言模型（LLM）可以为代理提供有价值的指导，从而增强RL算法在这些环境中的性能。然而，LLM通常在理解下游任务方面遇到困难，这阻碍了它们在这些任务中最优地帮助代理的能力。缓解这个问题的常见方法是使用与任务相关的数据来微调LLM，使其能够为RL代理提供有用的指导。然而，这种方法遇到了一些困难，比如无法访问的模型权重或需要大量的计算资源，使其不切实际。在这项工作中，我们引入了RLAdapter，这是一个框架，在RL算法和LLM之间建立更好的连接，从而整合它们的优势和能力。

    While reinforcement learning (RL) shows remarkable success in decision-making problems, it often requires a lot of interactions with the environment, and in sparse-reward environments, it is challenging to learn meaningful policies. Large Language Models (LLMs) can potentially provide valuable guidance to agents in learning policies, thereby enhancing the performance of RL algorithms in such environments. However, LLMs often encounter difficulties in understanding downstream tasks, which hinders their ability to optimally assist agents in these tasks. A common approach to mitigating this issue is to fine-tune the LLMs with task-related data, enabling them to offer useful guidance for RL agents. However, this approach encounters several difficulties, such as inaccessible model weights or the need for significant computational resources, making it impractical. In this work, we introduce RLAdapter, a framework that builds a better connection between RL algorithms and LLMs by incorporating
    
[^12]: 龙与地下城领域中命名实体识别的比较分析

    Comparative Analysis of Named Entity Recognition in the Dungeons and Dragons Domain. (arXiv:2309.17171v1 [cs.CL])

    [http://arxiv.org/abs/2309.17171](http://arxiv.org/abs/2309.17171)

    我们比较分析了领域特定的命名实体识别在龙与地下城领域的表现，发现在没有进行任何修改的情况下，Flair、Trankit和Spacy在龙与地下城背景中的命名实体识别方面表现最好。

    

    许多自然语言处理任务，在通用英语中表现良好，但在奇幻文学等特定领域面临挑战。这在命名实体识别(NER)中尤为明显，NER可以检测和分类文本中的实体。我们分析了10个NER模型在7本龙与地下城(D&D)冒险书上的表现，以评估特定领域的性能。使用开源的大型语言模型，我们对这些书籍中的命名实体进行了标注，并评估了每个模型的准确率。我们的研究结果表明，在不做修改的情况下，Flair、Trankit和Spacy在识别龙与地下城背景中的命名实体方面优于其他模型。

    Many NLP tasks, although well-resolved for general English, face challenges in specific domains like fantasy literature. This is evident in Named Entity Recognition (NER), which detects and categorizes entities in text. We analyzed 10 NER models on 7 Dungeons and Dragons (D&D) adventure books to assess domain-specific performance. Using open-source Large Language Models, we annotated named entities in these books and evaluated each model's precision. Our findings indicate that, without modifications, Flair, Trankit, and Spacy outperform others in identifying named entities in the D&D context.
    
[^13]: 评估GPT模型在表型概念识别中的应用

    An evaluation of GPT models for phenotype concept recognition. (arXiv:2309.17169v1 [cs.CL])

    [http://arxiv.org/abs/2309.17169](http://arxiv.org/abs/2309.17169)

    本研究评估了最新的GPT模型在临床深度表型学中的性能，并发现目前这些模型尚未达到最先进的性能水平。

    

    目标：临床深度表型学在罕见疾病患者的诊断以及构建协调护理计划中起到关键作用。该过程依赖于使用本体概念对患者档案进行建模和整理，通常使用人类表型本体进行。机器学习方法已被广泛采用来支持这项表型概念识别任务。随着大型语言模型在大多数自然语言处理任务中的显著应用，我们在本研究中评估了最新的生成式预训练变压器（GPT）模型在临床深度表型学中的性能。材料和方法：研究的实验设置包括七个不同特异性级别的提示、两个GPT模型（gpt-3.5和gpt-4.0）以及一个已建立的表型识别黄金标准。结果：我们的结果显示，目前这些模型尚未达到最先进的性能水平。最佳运行结果采用了少量样本学习，实现了最佳表现。

    Objective: Clinical deep phenotyping plays a critical role in both the diagnosis of patients with rare disorders as well as in building care coordination plans. The process relies on modelling and curating patient profiles using ontology concepts, usually from the Human Phenotype Ontology. Machine learning methods have been widely adopted to support this phenotype concept recognition task. With the significant shift in the use of large language models (LLMs) for most NLP tasks, herewithin, we examine the performance of the latest Generative Pre-trained Transformer (GPT) models underpinning ChatGPT in clinical deep phenotyping. Materials and Methods: The experimental setup of the study included seven prompts of various levels of specificity, two GPT models (gpt-3.5 and gpt-4.0) and an established gold standard for phenotype recognition. Results: Our results show that, currently, these models have not yet achieved state of the art performance. The best run, using few-shots learning, achi
    
[^14]: DyVal: 基于图形信息的大型语言模型动态评估

    DyVal: Graph-informed Dynamic Evaluation of Large Language Models. (arXiv:2309.17167v1 [cs.AI])

    [http://arxiv.org/abs/2309.17167](http://arxiv.org/abs/2309.17167)

    DyVal是一种基于图形信息的大型语言模型动态评估协议，通过动态生成具有可控复杂性的评估样本，评估了各种LLM在推理任务上的性能，发现它们在这些挑战性样本上表现更差。

    

    大型语言模型在各种评估基准中取得了显著的性能。然而，人们对它们的性能提出了担忧，因为它们庞大的训练语料库中可能存在数据污染。此外，当前基准的静态性质和固定复杂性可能无法充分衡量LLM的进步能力。在本文中，我们介绍了DyVal，一种新颖、通用且灵活的动态评估LLM的协议。基于我们提出的动态评估框架，我们利用有向无环图的结构优势构建了基于图形信息的DyVal，以动态生成具有可控复杂性的评估样本。DyVal生成了包括数学、逻辑推理和算法问题在内的具有挑战性的推理任务的评估集。我们评估了从Flan-T5-large到ChatGPT和GPT4的各种LLM。实验表明，LLM在DyVal生成的评估样本上表现更差

    Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns about their performance are raised on potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic evaluation of LLMs. Based on our proposed dynamic evaluation framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments demonstrate that LLMs perform worse in DyVal-generated evaluation samples with di
    
[^15]: LatticeGen: 一种在云上进行隐私感知生成的协作框架，隐藏生成的文本在格子中

    LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud. (arXiv:2309.17157v1 [cs.CL])

    [http://arxiv.org/abs/2309.17157](http://arxiv.org/abs/2309.17157)

    LatticeGen是一个协作框架，通过将真实生成的文本与噪声混合并隐藏在格子中，以保护用户的隐私。实验证明，LatticeGen能够在面对强攻击时成功保护真实生成，超过50%的语义仍然隐藏。

    

    在当前的用户-服务器交互模式中，使用大型语言模型（LLM）进行提示生成的过程中，服务器完全控制着生成过程，这使得想要将生成的文本保留给自己的用户没有任何选择。我们提出了LatticeGen，一个协作框架，在该框架中，服务器仍然处理大部分计算任务，而用户控制采样操作。其核心思想是用户将真实生成序列与噪声标记混合，并隐藏在一个带噪声的格子中。考虑到来自假设恶意服务器的潜在攻击以及用户如何进行防御，我们提出了重复波束搜索攻击和混合噪声方案。在实验中，我们将LatticeGen应用于保护提示和生成。结果显示，虽然带噪声的格子会降低生成质量，但LatticeGen成功地在强攻击下显著保护了真实生成（超过50%的语义仍然隐藏）。

    In the current user-server interaction paradigm of prompted generation with large language models (LLM) on cloud, the server fully controls the generation process, which leaves zero options for users who want to keep the generated text to themselves. We propose LatticeGen, a cooperative framework in which the server still handles most of the computation while the user controls the sampling operation. The key idea is that the true generated sequence is mixed with noise tokens by the user and hidden in a noised lattice. Considering potential attacks from a hypothetically malicious server and how the user can defend against it, we propose the repeated beam-search attack and the mixing noise scheme. In our experiments we apply LatticeGen to protect both prompt and generation. It is shown that while the noised lattice degrades generation quality, LatticeGen successfully protects the true generation to a remarkable degree under strong attacks (more than 50% of the semantic remains hidden as 
    
[^16]: 使用大型语言模型进行定性分析可能引入严重偏见

    Using Large Language Models for Qualitative Analysis can Introduce Serious Bias. (arXiv:2309.17147v1 [cs.CL])

    [http://arxiv.org/abs/2309.17147](http://arxiv.org/abs/2309.17147)

    使用大型语言模型（LLMs）对定性分析进行注释可能引入偏见和测量误差，相比之下，使用高质量的人工注释和灵活编码对简单监督模型进行训练可以更好地减少偏见和误差。

    

    大型语言模型（LLMs）正在迅速普及，但对社会科学研究的影响尚未被很好理解。本文探讨LLMs是否能够帮助我们分析大量开放性面谈数据，以罗兴亚难民在孟加拉国科克斯巴扎的访谈记录为应用案例。我们发现，在使用LLMs对文本进行注释时需要非常谨慎，因为存在引入偏见的风险，这可能导致误导性推断。我们这里所指的偏见是技术意义上的，即LLMs在注释访谈记录时的错误不是与访谈对象的特征无关的随机误差。通过使用高质量的人工注释和灵活编码对简单监督模型进行训练，可以减少测量误差和偏见，优于LLMs的注释。因此，考虑到必须有一些高质量的注释以评估LLM是否引入偏见，我们认为最好的选择可能是使用较简单的监督模型。

    Large Language Models (LLMs) are quickly becoming ubiquitous, but the implications for social science research are not yet well understood. This paper asks whether LLMs can help us analyse large-N qualitative data from open-ended interviews, with an application to transcripts of interviews with Rohingya refugees in Cox's Bazaar, Bangladesh. We find that a great deal of caution is needed in using LLMs to annotate text as there is a risk of introducing biases that can lead to misleading inferences. We here mean bias in the technical sense, that the errors that LLMs make in annotating interview transcripts are not random with respect to the characteristics of the interview subjects. Training simpler supervised models on high-quality human annotations with flexible coding leads to less measurement error and bias than LLM annotations. Therefore, given that some high quality annotations are necessary in order to asses whether an LLM introduces bias, we argue that it is probably preferable to
    
[^17]: 通过自我知识蒸馏在资源稀缺情况下促进广义跨语言问答

    Promoting Generalized Cross-lingual Question Answering in Few-resource Scenarios via Self-knowledge Distillation. (arXiv:2309.17134v1 [cs.CL])

    [http://arxiv.org/abs/2309.17134](http://arxiv.org/abs/2309.17134)

    通过自我知识蒸馏和跨语言采样，我们提出一种方法来提高在资源稀缺情况下的广义跨语言问答迁移能力，并引入mAP@k系数来实现平衡有效的知识传递。

    

    尽管多语言抽取式问答（QA）取得了显著进展，但在语言上具有高水平且一致分布的模型仍然具有挑战性，尤其对于资源有限的语言。我们主要研究广义跨语言迁移（G-XLT）任务，该任务关注问题语言与上下文语言不同的情况，迄今为止这个挑战受到了有限的关注。我们的方法旨在通过在大规模数据集上训练高性能多语言模型，并结合跨语言采样和先进的自我蒸馏训练方法，在几种语言间进行交叉学习，从而提高跨语言QA的迁移能力。值得注意的是，我们引入了新颖的mAP@k系数来微调自我知识蒸馏损失，动态调节教师模型的知识，实现平衡有效的知识传递。

    Despite substantial progress in multilingual extractive Question Answering (QA), models with high and uniformly distributed performance across languages remain challenging, especially for languages with limited resources. We study cross-lingual transfer mainly focusing on the Generalized Cross-Lingual Transfer (G-XLT) task, where the question language differs from the context language - a challenge that has received limited attention thus far. Our approach seeks to enhance cross-lingual QA transfer using a high-performing multilingual model trained on a large-scale dataset, complemented by a few thousand aligned QA examples across languages. Our proposed strategy combines cross-lingual sampling and advanced self-distillation training in generations to tackle the previous challenge. Notably, we introduce the novel mAP@k coefficients to fine-tune self-knowledge distillation loss, dynamically regulating the teacher's model knowledge to perform a balanced and effective knowledge transfer. 
    
[^18]: 精细化的后期交互多模检索用于检索增强视觉问答

    Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering. (arXiv:2309.17133v1 [cs.CL])

    [http://arxiv.org/abs/2309.17133](http://arxiv.org/abs/2309.17133)

    本文提出了一种精细化的后期交互多模检索方法（FLMR）来改进检索增强的视觉问答中的知识检索。FLMR通过获取补充的图像表示并使用与现有基于文本的模型相对齐的视觉模型，解决了RA-VQA中检索器的两个主要限制。

    

    基于知识的视觉问答（KB-VQA）要求VQA系统利用现有知识库中的知识来回答与视觉相关的问题。检索增强的视觉问答（RA-VQA）是一种强大的框架，用于解决KB-VQA问题，首先使用密集段落检索（DPR）检索相关文档，然后利用这些文档回答问题。本文提出了精细化的后期交互多模检索（FLMR），显著改进了RA-VQA中的知识检索。FLMR解决了RA-VQA检索器中的两个主要限制：（1）通过图像到文本转换获得的图像表示可能不完整和不准确，（2）查询和文档之间的相关性分数是通过一维嵌入计算的，可能对更细粒度的相关性不敏感。FLMR通过使用与现有基于文本的模型相对齐的视觉模型获取补充图像表示来克服这些限制。

    Knowledge-based Visual Question Answering (KB-VQA) requires VQA systems to utilize knowledge from existing knowledge bases to answer visually-grounded questions. Retrieval-Augmented Visual Question Answering (RA-VQA), a strong framework to tackle KB-VQA, first retrieves related documents with Dense Passage Retrieval (DPR) and then uses them to answer questions. This paper proposes Fine-grained Late-interaction Multi-modal Retrieval (FLMR) which significantly improves knowledge retrieval in RA-VQA. FLMR addresses two major limitations in RA-VQA's retriever: (1) the image representations obtained via image-to-text transforms can be incomplete and inaccurate and (2) relevance scores between queries and documents are computed with one-dimensional embeddings, which can be insensitive to finer-grained relevance. FLMR overcomes these limitations by obtaining image representations that complement those from the image-to-text transforms using a vision model aligned with an existing text-based r
    
[^19]: 基准测试大型语言模型在RDF知识图创建和理解中的能力：LLMs如何进行Turtle语言解析？

    Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?. (arXiv:2309.17122v1 [cs.AI])

    [http://arxiv.org/abs/2309.17122](http://arxiv.org/abs/2309.17122)

    本论文评估了各种大型语言模型在RDF知识图创建和理解中的能力，提出了一组任务来探究模型的解析、理解、分析和创建能力，并且对商业可用和免费的离线模型进行了评估和比较。

    

    大型语言模型（LLMs）在自然语言处理和编码任务方面取得了显著进展，然而它们在表示数据的形式语言中的工作能力，特别是在知识图工程领域中，仍未得到充分研究。为了评估各种LLMs的能力，我们创建了一组五个任务，以探究它们处理Turtle语法的知识图解析、理解、分析和创建的能力。这些任务具有不同的复杂性程度，并能随问题规模而扩展，已集成到我们的自动评估系统LLM-KG-Bench中。评估包括四个商业可用的LLMs - GPT-3.5、GPT-4、Claude 1.3 和 Claude 2.0，以及两个可免费使用的离线模型 GPT4All Vicuna 和 GPT4All Falcon 13B。这项分析深入了解了LLMs在其在R中应用的优势和不足之处。

    Large Language Models (LLMs) are advancing at a rapid pace, with significant improvements at natural language processing and coding tasks. Yet, their ability to work with formal languages representing data, specifically within the realm of knowledge graph engineering, remains under-investigated. To evaluate the proficiency of various LLMs, we created a set of five tasks that probe their ability to parse, understand, analyze, and create knowledge graphs serialized in Turtle syntax. These tasks, each embodying distinct degrees of complexity and being able to scale with the size of the problem, have been integrated into our automated evaluation system, the LLM-KG-Bench. The evaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4, Claude 1.3, and Claude 2.0, as well as two freely accessible offline models, GPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth understanding of the strengths and shortcomings of LLMs in relation to their application within R
    
[^20]: SCALE: 不对称语言翻译引擎的协同合作

    SCALE: Synergized Collaboration of Asymmetric Language Translation Engines. (arXiv:2309.17061v1 [cs.CL])

    [http://arxiv.org/abs/2309.17061](http://arxiv.org/abs/2309.17061)

    SCALE是一种将专业翻译模型和大型语言模型连接为统一翻译引擎的协同框架，通过引入STM的翻译进一步提升LLM的性能，从而在低资源环境中显著优于其他模型。

    

    本文介绍了SCALE，一种将紧凑的专业翻译模型（STM）和通用的大型语言模型（LLM）连接为统一翻译引擎的协同框架。通过将STM的翻译引入三元组的上下文演示中，SCALE实现了LLM的细化和枢轴能力，从而减轻了LLM的语言偏差和STM的并行数据偏差，在不牺牲广泛性的前提下增强了LLM的专业性，促进了无需昂贵LLM精调的持续学习。我们的综合实验结果表明，在挑战性的低资源环境中，SCALE明显优于少样本LLMs（GPT-4）和专业模型（NLLB）。此外，在Xhosa到英语的翻译中，SCALE的性能持续提升4个BLEURT分数，当配备仅600M参数的紧凑模型时，SCALE在COMET分数上超过了少样本GPT-4的2.5个分数和BLEURT分数上3.8个分数。SCALE还能有效地进行解释。

    In this paper, we introduce SCALE, a collaborative framework that connects compact Specialized Translation Models (STMs) and general-purpose Large Language Models (LLMs) as one unified translation engine. By introducing translation from STM into the triplet in-context demonstrations, SCALE unlocks refinement and pivoting ability of LLM, thus mitigating language bias of LLM and parallel data bias of STM, enhancing LLM speciality without sacrificing generality, and facilitating continual learning without expensive LLM fine-tuning. Our comprehensive experiments show that SCALE significantly outperforms both few-shot LLMs (GPT-4) and specialized models (NLLB) in challenging low-resource settings. Moreover, in Xhosa to English translation, SCALE experiences consistent improvement by a 4 BLEURT score without tuning LLM and surpasses few-shot GPT-4 by 2.5 COMET score and 3.8 BLEURT score when equipped with a compact model consisting of merely 600M parameters. SCALE could also effectively expl
    
[^21]: 通过引入增强型大型语言模型的检索可解释的长篇法律问题回答

    Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large Language Models. (arXiv:2309.17050v1 [cs.CL])

    [http://arxiv.org/abs/2309.17050](http://arxiv.org/abs/2309.17050)

    该研究提出了一种增强型大型语言模型的检索可解释的长篇法律问题回答方法，通过"先检索后阅读"的流程生成详细回答。同时，还引入了一个新的数据集并进行了实验验证，结果显示有希望的性能。

    

    许多人可能在生活中面临法律纠纷，但他们对如何应对这些复杂问题的理解不足，往往使他们处于弱势地位。自然语言处理的进步为通过开发自动化法律援助系统弥合这种法律识字差距提供了新的途径。然而，现有的法律问题回答（LQA）方法往往受限于范围狭窄，要么局限于特定的法律领域，要么只提供简短无信息量的回答。在这项工作中，我们提出了一种端到端的方法，旨在生成任何法定法律问题的长篇回答，采用“先检索后阅读”的流程。为了支持这种方法，我们引入并发布了长篇法律问题回答（LLeQA）数据集，其中包含1,868个由专家标注的法律问题（使用法语），并提供了根据相关法律条文提供详细答案。我们的实验结果显示了有希望的性能。

    Many individuals are likely to face a legal dispute at some point in their lives, but their lack of understanding of how to navigate these complex issues often renders them vulnerable. The advancement of natural language processing opens new avenues for bridging this legal literacy gap through the development of automated legal aid systems. However, existing legal question answering (LQA) approaches often suffer from a narrow scope, being either confined to specific legal domains or limited to brief, uninformative responses. In this work, we propose an end-to-end methodology designed to generate long-form answers to any statutory law questions, utilizing a "retrieve-then-read" pipeline. To support this approach, we introduce and release the Long-form Legal Question Answering (LLeQA) dataset, comprising 1,868 expert-annotated legal questions in the French language, complete with detailed answers rooted in pertinent legal provisions. Our experimental results demonstrate promising perform
    
[^22]: 上下文化影响文本数字处理的语言资源水平的分类

    Contextualising Levels of Language Resourcedness affecting Digital Processing of Text. (arXiv:2309.17035v1 [cs.CL])

    [http://arxiv.org/abs/2309.17035](http://arxiv.org/abs/2309.17035)

    本文讨论了数字处理文本时，语言资源水平对其影响的上下文化分类，并提出了一个将语言划分为五个等级的矩阵，从而解决了将语言只划分为LRL和HRL两种类型的问题。

    

    应用领域如数字人文学和聊天机器人等工具都涉及到对自然语言的处理，从数字化纸质文件到语音生成。内容的语言通常被划分为资源匮乏语言（LRL）或资源丰富语言（HRL）。非洲语言被认为是资源匮乏语言，而英语则是资源最丰富的语言。为了为这些语言开发软件系统以完成各种任务，使用了各种语言资源。本文认为对于所有语言来说，将其划分为LRL和HRL两种对立的类型是有问题的。通过对社会中语言资源的清晰理解，我们开发了一个矩阵来将语言划分为"非常LRL"、"LRL"、"RL"、"HRL"和"非常HRL"。这种划分基于连接情景基础设施、情景对话流、情景知识等的分类。

    Application domains such as digital humanities and tool like chatbots involve some form of processing natural language, from digitising hardcopies to speech generation. The language of the content is typically characterised as either a low resource language (LRL) or high resource language (HRL), also known as resource-scarce and well-resourced languages, respectively. African languages have been characterized as resource-scarce languages (Bosch et al. 2007; Pretorius & Bosch 2003; Keet & Khumalo 2014) and English is by far the most well-resourced language. Varied language resources are used to develop software systems for these languages to accomplish a wide range of tasks. In this paper we argue that the dichotomous typology LRL and HRL for all languages is problematic. Through a clear understanding of language resources situated in a society, a matrix is developed that characterizes languages as Very LRL, LRL, RL, HRL and Very HRL. The characterization is based on the typology of con
    
[^23]: 作为评估器的大型语言模型中认知偏差的基准测试

    Benchmarking Cognitive Biases in Large Language Models as Evaluators. (arXiv:2309.17012v1 [cs.CL])

    [http://arxiv.org/abs/2309.17012](http://arxiv.org/abs/2309.17012)

    本研究对15个不同大小的大型语言模型进行了评估，发现它们作为评估器存在认知偏差，尤其在文本质量评估中表现出较强的偏见，这对其鲁棒性提出了质疑。同时，研究还发现了人类和机器偏好之间的相关性。

    

    最近的研究表明，大型语言模型（LLMs）通过简单的提示和上下文学习作为自动评估器非常有效。本研究组装了15个大小不同的LLMs，并通过其他LLMs的偏好排名来评估它们的输出响应，例如System Star比System Square更好。然后，我们引入了用于评估LLMs输出中六种不同认知偏差的认知偏差基准测试（CoBBLEr），如自我中心偏差，即模型更喜欢将自己的输出在评估中排名较高。我们发现LLMs是有偏见的文本质量评估器，在每个评估中都表现出对我们偏见基准的强烈迹象（在所有模型上的平均比较约为40%），这对它们作为评估器的鲁棒性提出了质疑。此外，我们还研究了人类和机器偏好之间的相关性，并计算了平均的Rank-Biased O值。

    Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased O
    
[^24]: 我希望争论：大型语言模型中的争论性推理

    I Wish to Have an Argument: Argumentative Reasoning in Large Language Models. (arXiv:2309.16938v1 [cs.CL])

    [http://arxiv.org/abs/2309.16938](http://arxiv.org/abs/2309.16938)

    该论文评估了当代大型语言模型在争论性推理方面的能力，并发现其争论性推理性能与输入和输出表示密切相关。研究还发现了“典型效应”，并探讨了在思维链提示下解决病态问题时的性能优势。

    

    我们评估当代大型语言模型（LLMs）进行争论性推理的能力。我们将实验框架定为争论挖掘（AM）和争论对提取（APE）任务，并评估它们在输入和输出表示的抽象级别上执行推理的能力（例如，任意标签集，语义图）。我们发现，尽管 LLMs 在 AM 和 APE 上能够与或超过最先进的水平，但它们的争论性推理性能非常依赖于输入和输出表示。我们还发现了一个“典型效应”，过多的典型实例会对任务性能产生负面影响，最佳数量约为4-5个。然而，这些结果并不适用于思维链（CoT）提示：我们发现典型效应被抵消，我们的结果表明在病态问题下，CoT 可以实现更好的性能。我们希望所报告的工作能够促进争论性推理的改进。

    We evaluate the ability of contemporary large language models (LLMs) to perform argumentative reasoning. We frame our experiments in terms of the argument mining (AM) and argument pair extraction (APE) tasks, and evaluate their ability to perform reasoning at increasing levels of abstraction in the input and output representations (e.g., arbitrary label sets, semantic graphs). We find that, although LLMs are able to match or surpass the state-of-the-art in AM and APE, their argumentative reasoning performance is very dependent on the input and output representation. We also find an "exemplar effect", where too many exemplars increasingly become detrimental for task performance, and about 4-5 being the optimal amount. Neither result extends to chain-of-thought (CoT) prompting: we find the exemplar effect to be nullified, and our results suggest that CoT allows for better performance under ill-conditioned problems. We hope that the work reported contributes to the improvement of argument
    
[^25]: SSHR: 利用自监督层次表示提高多语种自动语音识别的能力

    SSHR: Leveraging Self-supervised Hierarchical Representations for Multilingual Automatic Speech Recognition. (arXiv:2309.16937v1 [cs.CL])

    [http://arxiv.org/abs/2309.16937](http://arxiv.org/abs/2309.16937)

    本研究提出了一种利用自监督层次表示（SSHR）优化多语种自动语音识别的方法。通过分析自监督学习模型的不同层次表示，提取出语言相关帧和特定内容，并引导模型在最终层次获取更多内容相关信息。

    

    多语种自动语音识别（ASR）系统因其在全球范围内扩展语言覆盖的潜力而受到关注。虽然自监督学习（SSL）在多语种ASR方面表现出了其有效性，但值得注意的是，SSL模型的不同层次表示可能包含尚未充分利用的不同信息。在本研究中，我们提出了一种利用自监督层次表示（SSHR）来优化多语种ASR的新方法。我们首先分析SSL模型的不同层次对于语言相关和内容相关信息的表达情况，发现具有更强相关性的层次。然后，我们从相关中间层中提取语言相关帧，并通过自注意机制引导特定内容的提取。此外，我们通过提出的交叉CTC方法，引导模型在最终层次获得更多内容相关信息。我们在两个多语种数据集（C）上评估了SSHR的性能。

    Multilingual automatic speech recognition (ASR) systems have garnered attention for their potential to extend language coverage globally. While self-supervised learning (SSL) has demonstrated its effectiveness in multilingual ASR, it is worth noting that the various layers' representations of SSL potentially contain distinct information that has not been fully leveraged. In this study, we propose a novel method that leverages self-supervised hierarchical representations (SSHR) to fine-tune multilingual ASR. We first analyze the different layers of the SSL model for language-related and content-related information, uncovering layers that show a stronger correlation. Then, we extract a language-related frame from correlated middle layers and guide specific content extraction through self-attention mechanisms. Additionally, we steer the model toward acquiring more content-related information in the final layers using our proposed Cross-CTC. We evaluate SSHR on two multilingual datasets, C
    
[^26]: 通过连续学习实现适应性问题内容检测的统一框架

    Towards a Unified Framework for Adaptable Problematic Content Detection via Continual Learning. (arXiv:2309.16905v1 [cs.CL])

    [http://arxiv.org/abs/2309.16905](http://arxiv.org/abs/2309.16905)

    本文提出了一个统一框架，通过连续学习来适应性地检测问题内容。该框架整合了来自8个来源的15个注释架构的84个相关任务，并创建了一种新的进展度量标准：优先考虑类别适应性。

    

    检测问题内容，例如仇恨言论，是一项多方面且不断变化的任务，受社会动态、用户群体、信息来源的多样性和语言的演变影响。学术界和业界都进行了大量努力，开发出捕捉问题内容各个方面的注释资源。由于研究人员的不同目标，这些注释是不一致的，因此对于检测问题内容的进展报告是分散的。除非我们考虑问题的动态性，否则这种模式预计会持续存在。我们提出整合现有资源，并利用其动态特性打破这种模式。在本文中，我们引入了一个连续学习基准和框架，用于问题内容检测，包括来自8个来源的15个注释架构的84个相关任务。我们的基准创建了一种新的进展度量：优先考虑类别适应性。

    Detecting problematic content, such as hate speech, is a multifaceted and ever-changing task, influenced by social dynamics, user populations, diversity of sources, and evolving language. There has been significant efforts, both in academia and in industry, to develop annotated resources that capture various aspects of problematic content. Due to researchers' diverse objectives, the annotations are inconsistent and hence, reports of progress on detection of problematic content are fragmented. This pattern is expected to persist unless we consolidate resources considering the dynamic nature of the problem. We propose integrating the available resources, and leveraging their dynamic nature to break this pattern. In this paper, we introduce a continual learning benchmark and framework for problematic content detection comprising over 84 related tasks encompassing 15 annotation schemas from 8 sources. Our benchmark creates a novel measure of progress: prioritizing the adaptability of class
    
[^27]: 基于Pepper、轻量级Transformer和LLM的手语识别系统

    A Sign Language Recognition System with Pepper, Lightweight-Transformer, and LLM. (arXiv:2309.16898v1 [cs.RO])

    [http://arxiv.org/abs/2309.16898](http://arxiv.org/abs/2309.16898)

    本研究提出了一种利用轻量级深度神经网络架构的手语识别系统，使Pepper机器人能够理解美国手语并实现非语言的人机交互。通过优化嵌入式系统的模型和智能机器人互动来生成自然的共语手势反应，为更有机和直观的人形机器人对话提供基础。实证结果表明了增强手语理解的巨大潜力。

    

    本研究探索使用轻量级深度神经网络架构，使人形机器人Pepper能够理解美国手语（ASL）并促进非语言的人机交互。首先，我们引入了一个针对嵌入式系统优化的轻量级高效模型，确保快速手势识别同时节省计算资源。在此基础上，我们利用大型语言模型（LLM）进行智能机器人互动。通过复杂的提示工程，我们定制交互，使Pepper机器人能够产生自然的共语手势反应，为更有机和直观的人形机器人对话奠定基础。最后，我们提出了一个集成的软件流水线，体现了在社交感知AI交互模型方面的进展。利用Pepper机器人的能力，我们在现实场景中展示了我们方法的实用性和有效性。结果突显出增强手语理解的巨大潜力。

    This research explores using lightweight deep neural network architectures to enable the humanoid robot Pepper to understand American Sign Language (ASL) and facilitate non-verbal human-robot interaction. First, we introduce a lightweight and efficient model for ASL understanding optimized for embedded systems, ensuring rapid sign recognition while conserving computational resources. Building upon this, we employ large language models (LLMs) for intelligent robot interactions. Through intricate prompt engineering, we tailor interactions to allow the Pepper Robot to generate natural Co-Speech Gesture responses, laying the foundation for more organic and intuitive humanoid-robot dialogues. Finally, we present an integrated software pipeline, embodying advancements in a socially aware AI interaction model. Leveraging the Pepper Robot's capabilities, we demonstrate the practicality and effectiveness of our approach in real-world scenarios. The results highlight a profound potential for enh
    
[^28]: DeBERTinha: 一种用于巴西葡萄牙语自然语言处理任务的DebertaV3 XSmall的多步骤适应方法

    DeBERTinha: A Multistep Approach to Adapt DebertaV3 XSmall for Brazilian Portuguese Natural Language Processing Task. (arXiv:2309.16844v1 [cs.CL])

    [http://arxiv.org/abs/2309.16844](http://arxiv.org/abs/2309.16844)

    DeBERTinha是一种通过多步骤训练和微调适应DebertaV3 XSmall模型的方法，用于巴西葡萄牙语自然语言处理任务。它在命名实体识别、情感分析和判断句子相关性等任务上表现出优越性能。

    

    本文提出了一种适应英语预训练DebertaV3 XSmall模型用于巴西葡萄牙语自然语言处理（NLP）任务的方法。该方法的关键在于多步训练过程，以确保模型在葡萄牙语上的有效调优。使用Carolina和BrWac的初始数据集进行预处理，解决表情符号、HTML标签和编码等问题。使用SentencePiece创建一个包含50,000个token的葡萄牙语特定词汇表。模型不是从头训练，而是使用预训练的英语模型的权重来初始化网络的大部分，仅包括随机嵌入，以识别从头训练的昂贵成本。采用替换标记检测任务以与DebertaV3训练的相同格式微调模型。适应的模型称为DeBERTinha，在命名实体识别、情感分析和判断句子相关性等下游任务上表现出优越性能，胜过BER。

    This paper presents an approach for adapting the DebertaV3 XSmall model pre-trained in English for Brazilian Portuguese natural language processing (NLP) tasks. A key aspect of the methodology involves a multistep training process to ensure the model is effectively tuned for the Portuguese language. Initial datasets from Carolina and BrWac are preprocessed to address issues like emojis, HTML tags, and encodings. A Portuguese-specific vocabulary of 50,000 tokens is created using SentencePiece. Rather than training from scratch, the weights of the pre-trained English model are used to initialize most of the network, with random embeddings, recognizing the expensive cost of training from scratch. The model is fine-tuned using the replaced token detection task in the same format of DebertaV3 training. The adapted model, called DeBERTinha, demonstrates effectiveness on downstream tasks like named entity recognition, sentiment analysis, and determining sentence relatedness, outperforming BER
    
[^29]: 课程驱动的教育机器人：通过综合对话数据开发语言学习聊天机器人的框架

    Curriculum-Driven Edubot: A Framework for Developing Language Learning Chatbots Through Synthesizing Conversational Data. (arXiv:2309.16804v1 [cs.CL])

    [http://arxiv.org/abs/2309.16804](http://arxiv.org/abs/2309.16804)

    本论文提出了一种框架，通过综合对话数据开发语言学习聊天机器人。该机器人结合聊天机器人互动特征和英语教科书的系统材料，帮助学生提高口语技巧。该方法通过提取教科书主题并使用大型语言模型生成相关对话，通过微调机器人的对话数据创建一个课程驱动型聊天机器人。用户研究表明，该聊天机器人在引领基于课程的对话和适应用户英语水平方面优于ChatGPT。该方法将传统教科书方法与对话式人工智能相结合，为学习者提供了一个与课程相匹配并提供用户个性化对话练习的互动工具。

    

    在教育场景中，聊天机器人已经变得流行起来，改变了学生与课程互动和教师授课的方式。我们提出了一种名为课程驱动的教育机器人的框架，该框架将聊天机器人的互动特征与英语教科书的系统材料相结合，帮助学生提高口语技巧。我们首先从教科书中提取相关主题，然后使用大型语言模型生成与这些主题相关的对话。接着，我们使用我们生成的对话数据对开源语言模型进行微调，从而创建我们的课程驱动型聊天机器人。用户研究表明，我们的聊天机器人在引领基于课程的对话和适应用户英语水平方面优于ChatGPT。通过将传统教科书方法与对话式人工智能相结合，我们的方法为学习者提供了一个与课程相匹配并提供用户个性化对话练习的互动工具。

    Chatbots have become popular in educational settings, revolutionizing how students interact with material and how teachers teach. We present Curriculum-Driven EduBot, a framework for developing a chatbot that combines the interactive features of chatbots with the systematic material of English textbooks to assist students in enhancing their conversational skills. We begin by extracting pertinent topics from textbooks and then using large language models to generate dialogues related to these topics. We then fine-tune an open-source LLM using our generated conversational data to create our curriculum-driven chatbot. User studies demonstrate that our chatbot outperforms ChatGPT in leading curriculum-based dialogues and adapting its dialogue to match the user's English proficiency level. By combining traditional textbook methodologies with conversational AI, our approach offers learners an interactive tool that aligns with their curriculum and provides user-tailored conversation practice.
    
[^30]: Promptbreeder: 通过提示演化实现自我参照的自我改进

    Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution. (arXiv:2309.16797v1 [cs.CL])

    [http://arxiv.org/abs/2309.16797](http://arxiv.org/abs/2309.16797)

    Promptbreeder是一种通过提示演化进行自我改进的通用机制，它可以优化大型语言模型的推理能力，并在算术和常识推理上优于现有的提示策略。

    

    像Chain-of-Thought Prompting这样的流行提示策略可以显著改进大型语言模型（LLMs）在各个领域的推理能力。然而，这种手工制作的提示策略通常不够优化。在本文中，我们提出了Promptbreeder，一种通用的自我参照自我改进机制，用于进化和调整给定领域的提示。Promptbreeder通过LLM驱动，对一组任务提示进行突变，并在训练集上评估其适应性。关键是，这些任务提示的突变是由LLM生成并在演化过程中自我改进的突变提示来控制的。换句话说，Promptbreeder不仅改进任务提示，还改进了改进这些任务提示的突变提示。Promptbreeder在常用的算术和常识推理基准测试中优于Chain-of-Thought和Plan-and-Solve Prompting等最先进的提示策略。

    Popular prompt strategies like Chain-of-Thought Prompting can dramatically improve the reasoning abilities of Large Language Models (LLMs) in various domains. However, such hand-crafted prompt-strategies are often sub-optimal. In this paper, we present Promptbreeder, a general-purpose self-referential self-improvement mechanism that evolves and adapts prompts for a given domain. Driven by an LLM, Promptbreeder mutates a population of task-prompts, and subsequently evaluates them for fitness on a training set. Crucially, the mutation of these task-prompts is governed by mutation-prompts that the LLM generates and improves throughout evolution in a self-referential way. That is, Promptbreeder is not just improving task-prompts, but it is also improving the mutationprompts that improve these task-prompts. Promptbreeder outperforms state-of-the-art prompt strategies such as Chain-of-Thought and Plan-and-Solve Prompting on commonly used arithmetic and commonsense reasoning benchmarks. Furth
    
[^31]: 长输入文本摘要中的幻觉减少

    Hallucination Reduction in Long Input Text Summarization. (arXiv:2309.16781v1 [cs.CL])

    [http://arxiv.org/abs/2309.16781](http://arxiv.org/abs/2309.16781)

    本文旨在减少长篇文本摘要中的幻觉输出，通过在Longformer Encoder-Decoder模型的微调中采用数据过滤和联合实体和摘要生成技术，我们成功提高了生成摘要的质量。

    

    文本摘要中的幻觉是指模型生成不被输入源文档支持的信息的现象。幻觉给生成的摘要的准确性和可靠性带来了重大障碍。本文旨在减少长篇文本摘要中的幻觉输出。我们使用了包含长篇科学研究文档及其摘要的PubMed数据集。我们在Longformer Encoder-Decoder (LED)模型的微调中加入了数据过滤和联合实体和摘要生成（JAENS）技术，以最小化幻觉，从而提高生成摘要的质量。我们使用以下指标来衡量实体级别的事实一致性：源精确度和目标F1。实验证明，经过微调的LED模型在生成文章摘要方面表现良好。数据过滤技术基于一些预处理步骤。

    Hallucination in text summarization refers to the phenomenon where the model generates information that is not supported by the input source document. Hallucination poses significant obstacles to the accuracy and reliability of the generated summaries. In this paper, we aim to reduce hallucinated outputs or hallucinations in summaries of long-form text documents. We have used the PubMed dataset, which contains long scientific research documents and their abstracts. We have incorporated the techniques of data filtering and joint entity and summary generation (JAENS) in the fine-tuning of the Longformer Encoder-Decoder (LED) model to minimize hallucinations and thereby improve the quality of the generated summary. We have used the following metrics to measure factual consistency at the entity level: precision-source, and F1-target. Our experiments show that the fine-tuned LED model performs well in generating the paper abstract. Data filtering techniques based on some preprocessing steps
    
[^32]: ChatGPT知道多少个单词？答案是ChatWords。

    How many words does ChatGPT know? The answer is ChatWords. (arXiv:2309.16777v1 [cs.CL])

    [http://arxiv.org/abs/2309.16777](http://arxiv.org/abs/2309.16777)

    这项研究通过开发ChatWords，一个自动化测试系统，为评估ChatGPT和其他NLP AI工具的词汇知识做出了贡献，以解决它们在产生错误答案方面的限制和错觉问题。

    

    ChatGPT的引入引起了人工智能（AI）自然语言处理（NLP）的关注。ChatGPT的采用率呈指数增长，数百万用户在各种任务和应用领域中进行实验，并取得了令人瞩目的结果。然而，ChatGPT存在着限制和错觉，例如产生看似可信但完全错误的答案。评估ChatGPT和类似的AI工具的性能是一个复杂的问题，正从不同的视角进行探索。在这项工作中，我们通过ChatWords，一个自动化测试系统，为评估ChatGPT对任意单词集的知识做出了贡献。ChatWords设计为可扩展、易于使用和适应性强，也可以评估其他NLP AI工具。ChatWords已公开可用，其主要目标是促进对AI工具的词汇知识的研究。通过两个案例研究，证明了ChatWords的好处：评估对AI工具的知识。

    The introduction of ChatGPT has put Artificial Intelligence (AI) Natural Language Processing (NLP) in the spotlight. ChatGPT adoption has been exponential with millions of users experimenting with it in a myriad of tasks and application domains with impressive results. However, ChatGPT has limitations and suffers hallucinations, for example producing answers that look plausible but they are completely wrong. Evaluating the performance of ChatGPT and similar AI tools is a complex issue that is being explored from different perspectives. In this work, we contribute to those efforts with ChatWords, an automated test system, to evaluate ChatGPT knowledge of an arbitrary set of words. ChatWords is designed to be extensible, easy to use, and adaptable to evaluate also other NLP AI tools. ChatWords is publicly available and its main goal is to facilitate research on the lexical knowledge of AI tools. The benefits of ChatWords are illustrated with two case studies: evaluating the knowledge tha
    
[^33]: Persona编码多流程对话句子评分：Persona引导的多流程对话句子评分方法

    Persona-Coded Poly-Encoder: Persona-Guided Multi-Stream Conversational Sentence Scoring. (arXiv:2309.16770v1 [cs.CL])

    [http://arxiv.org/abs/2309.16770](http://arxiv.org/abs/2309.16770)

    本论文提出了一种新颖的Persona编码多流程对话句子评分方法，利用个人角色信息来提高对话生成的质量。

    

    机器学习和深度学习的最新进展已经在许多实际应用中广泛应用于对话AI。然而，要利用可以提供对话背景或个性化调整的辅助信息以提高对话质量仍然很具挑战性。例如，关于使用个人角色信息来提高对话质量的研究仅有限，即使是最先进的对话AI技术也无法有效地利用来自多种来源的辅助数据信号，例如多模式交互数据、人口统计学数据和社会确定因素数据等。在本文中，我们提出了一种新颖的Persona编码多流程对话句子评分方法，它利用多流程编码方案中的个人角色信息来提高对话生成的质量。为了展示所提出方法的有效性，我们在两个不同的基于个人角色的对话数据集上评估了我们的方法，并与参考方法进行了比较。

    Recent advances in machine learning and deep learning have led to the widespread use of Conversational AI in many practical applications. However, it is still very challenging to leverage auxiliary information that can provide conversational context or personalized tuning to improve the quality of conversations. For example, there has only been limited research on using an individuals persona information to improve conversation quality, and even state-of-the-art conversational AI techniques are unable to effectively leverage signals from heterogeneous sources of auxiliary data, such as multi-modal interaction data, demographics, SDOH data, etc. In this paper, we present a novel Persona-Coded Poly-Encoder method that leverages persona information in a multi-stream encoding scheme to improve the quality of response generation for conversations. To show the efficacy of the proposed method, we evaluate our method on two different persona-based conversational datasets, and compared against 
    
[^34]: 解码图像：释放大型语言模型。

    Decoding Imagery: Unleashing Large Language Models. (arXiv:2309.16705v1 [cs.CV])

    [http://arxiv.org/abs/2309.16705](http://arxiv.org/abs/2309.16705)

    该论文研究了Google Bard这个多模态大型语言模型的能力，发现Bard在将视觉和语言分析相结合方面依赖于对图像进行有根据的猜测，可以解决视觉上有挑战的问题但无法修改原始视觉对象。

    

    在一个挑战-响应研究中，我们对Google Bard进行了64个视觉挑战，旨在探究多模态大型语言模型（LLMs）的能力。这些挑战涵盖了各种类别，包括“视觉情境推理”，“视觉文本推理”和“下一场景预测”等，以确定Bard在融合视觉和语言分析方面的能力。我们的研究结果显示，Bard倾向于根据图片做出有根据的猜测，特别是在确定图片中的线索时。与GPT4等其他模型不同，Bard似乎不依赖于像Tesseract这样的光学字符识别库，而是像Google Lens和Visual API这样的深度学习模型一样，识别复杂图片中的文本。显着的是，Bard可以通过视觉方式解决ChatGPT无法理解的验证码，推荐使用Tesseract解决方案。此外，虽然Bard模型基于视觉输入提出了解决方案，但它无法重建或修改原始的视觉对象来支持其结论。

    In a challenge-response study, we subjected Google Bard to 64 visual challenges designed to probe multimodal Large Language Models (LLMs). The challenges spanned diverse categories, including "Visual Situational Reasoning," "Visual Text Reasoning," and "Next Scene Prediction," among others, to discern Bard's competence in melding visual and linguistic analyses. Our findings indicate that Bard tends to rely on making educated guesses about visuals, especially when determining cues from images. Unlike other models like GPT4, Bard does not appear to rely on optical character recognition libraries like Tesseract but recognizes text in complex images like deep learning models such as Google Lens and Visual API. Significantly Bard can solve CAPTCHAs visually that ChatGPT fails to understand, recommending Tesseract solutions. Moreover, while the Bard model proposes solutions based on visual input, it cannot recreate or modify the original visual objects to support its conclusions. Bard fails 
    
[^35]: MVMR: 在多个可靠视频集中评估自然语言视频定位偏差

    MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool. (arXiv:2309.16701v1 [cs.CV])

    [http://arxiv.org/abs/2309.16701](http://arxiv.org/abs/2309.16701)

    本文提出了一个名为MVMR的任务，旨在给定文本查询从大量视频集中定位视频帧。我们通过已有数据集进行相似性筛选来构建数据集，并引入三个MVMR数据集。我们采用了嵌入式文本相似度匹配和视频-语言对齐技术来计算相关性得分，并为MVMR任务开发了一个强大的模型，Reliable Mutual Matching Network (RMMN)。

    

    随着近年来多媒体内容的激增，自然语言视频定位成为一个关键问题，它致力于检测与给定自然语言查询匹配的视频片段。然而，以往的研究都没有探索在存在多个正负视频的大量语料库中定位一个时刻。本文提出了一个名为MVMR（Massive Videos Moment Retrieval）的任务，旨在给定文本查询从大量视频集中定位视频帧。对于这个任务，我们提出了一种通过对现有视频定位数据集进行相似性筛选来构建数据集的方法，并引入了三个MVMR数据集。具体来说，我们采用基于嵌入的文本相似度匹配和视频-语言对齐技术来计算目标查询与视频之间的相关性得分，从而定义正负集。针对提出的MVMR任务，我们进一步开发了一个强大的模型，Reliable Mutual Matching Network (RMMN)。

    With the explosion of multimedia content in recent years, natural language video localization, which focuses on detecting video moment that matches a given natural language query, has become a critical problem. However, none of the previous research explores localizing a moment from a large corpus where multiple positive and negative videos exist. In this paper, we propose an MVMR (Massive Videos Moment Retrieval) task, which aims to localize video frames from a massive set of videos given a text query. For this task, we suggest methods for constructing datasets by employing similarity filtering on the existing video localization datasets and introduce three MVMR datasets. Specifically, we employ embedding-based text similarity matching and video-language grounding techniques to calculate the relevance score between a target query and videos to define positive and negative sets. For the proposed MVMR task, we further develop a strong model, Reliable Mutual Matching Network (RMMN), whic
    
[^36]: 揭秘CLIP数据

    Demystifying CLIP Data. (arXiv:2309.16671v1 [cs.CV])

    [http://arxiv.org/abs/2309.16671](http://arxiv.org/abs/2309.16671)

    CLIP的成功主要归功于其数据而非模型架构或预训练目标。我们通过元数据整理方法引入了MetaCLIP，该方法从原始数据池和元数据中生成一个平衡的子集，提供了更加详细的数据信息。在实验中，我们发现MetaCLIP在处理400M个图像-文本数据对时取得了良好的性能。

    

    对比语言-图像预训练（CLIP）是一种推动计算机视觉研究和应用的方法，为现代识别系统和生成模型注入了活力。我们认为，CLIP成功的主要因素是其数据，而不是模型架构或预训练目标。然而，CLIP只提供了关于其数据和如何收集数据的非常有限的信息，导致其他研究努力通过使用模型参数进行过滤来重现CLIP的数据。在这项工作中，我们意在揭示CLIP的数据整理方法，并在公开给社区的过程中引入元数据整理的语言-图像预训练（MetaCLIP）。MetaCLIP通过对元数据分布进行平衡，从原始数据池和元数据（从CLIP的概念中得出）中产生一个平衡的子集。我们的实验研究严格隔离了模型和训练设置，仅专注于数据。MetaCLIP应用于包含400M图像-文本数据对的CommonCrawl，并获得了较好的性能。

    Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced research and applications in computer vision, fueling modern recognition systems and generative models. We believe that the main ingredient to the success of CLIP is its data and not the model architecture or pre-training objective. However, CLIP only provides very limited information about its data and how it has been collected, leading to works that aim to reproduce CLIP's data by filtering with its model parameters. In this work, we intend to reveal CLIP's data curation approach and in our pursuit of making it open to the community introduce Metadata-Curated Language-Image Pre-training (MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's concepts) and yields a balanced subset over the metadata distribution. Our experimental study rigorously isolates the model and training settings, concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M image-text data pairs outper
    
[^37]: 对文档级关系抽取的综合调查（2016-2022）

    A Comprehensive Survey of Document-level Relation Extraction (2016-2022). (arXiv:2309.16396v1 [cs.CL])

    [http://arxiv.org/abs/2309.16396](http://arxiv.org/abs/2309.16396)

    这篇综述论文介绍了文档级关系抽取（DocRE）的最新进展，与句子级关系抽取相比，DocRE提供了更广泛的上下文分析，涉及跨越多个句子或段落的关系抽取，可用于构建和自动填充知识库。

    

    文档级关系抽取（DocRE）是自然语言处理（NLP）中一个活跃研究领域，涉及识别和抽取实体之间的关系，超越句子边界。与传统的句子级关系抽取相比，DocRE提供了更广泛的上下文分析，更具挑战性，因为它涉及识别可能跨越多个句子或段落的关系。这个任务在构建和自动填充知识库方面越来越受关注，以从非结构化的大规模文档（例如科学论文、法律合同或新闻文章）中自动获取关系，以便更好地理解实体之间的关系。本文旨在全面介绍这一领域的最新进展，突出其与句子级关系抽取的不同应用。

    Document-level relation extraction (DocRE) is an active area of research in natural language processing (NLP) concerned with identifying and extracting relationships between entities beyond sentence boundaries. Compared to the more traditional sentence-level relation extraction, DocRE provides a broader context for analysis and is more challenging because it involves identifying relationships that may span multiple sentences or paragraphs. This task has gained increased interest as a viable solution to build and populate knowledge bases automatically from unstructured large-scale documents (e.g., scientific papers, legal contracts, or news articles), in order to have a better understanding of relationships between entities. This paper aims to provide a comprehensive overview of recent advances in this field, highlighting its different applications in comparison to sentence-level relation extraction.
    
[^38]: Fast-HuBERT: 一种高效的自监督语音表示学习训练框架

    Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning. (arXiv:2309.13860v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.13860](http://arxiv.org/abs/2309.13860)

    Fast-HuBERT是一种高效的自监督语音表示学习训练框架，通过分析HuBERT预训练过程中的计算成本，并引入一系列效率优化策略，实现了在Librispeech 960h基准上的5.2倍加速，并且在前人工作中得到了一致的改进。

    

    近年来，自监督学习（SSL）方法在语音处理任务中取得了显著进展。各种基于语音的SSL模型已经开发出来，并在一系列下游任务（包括语音识别）上展示了有希望的性能。然而，现有的基于语音的SSL模型在计算成本方面面临着共同的困境，这可能会阻碍它们的潜在应用和深入学术研究。为了解决这个问题，我们首先分析了HuBERT预训练过程中不同模块的计算成本，然后引入了一系列效率优化策略，即本文中提出的Fast-HuBERT。所提出的Fast-HuBERT可以在Librispeech 960h基准上使用8个V100 GPU在1.1天内进行训练，而不会导致性能下降，相比原始实现，加速了5.2倍。此外，我们还探索了Fast-HuBERT中的两种经过充分研究的技术，并展示了与之前工作中报告的一致的改进。

    Recent years have witnessed significant advancements in self-supervised learning (SSL) methods for speech-processing tasks. Various speech-based SSL models have been developed and present promising performance on a range of downstream tasks including speech recognition. However, existing speech-based SSL models face a common dilemma in terms of computational cost, which might hinder their potential application and in-depth academic research. To address this issue, we first analyze the computational cost of different modules during HuBERT pre-training and then introduce a stack of efficiency optimizations, which is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be trained in 1.1 days with 8 V100 GPUs on the Librispeech 960h benchmark, without performance degradation, resulting in a 5.2x speedup, compared to the original implementation. Moreover, we explore two well-studied techniques in the Fast-HuBERT and demonstrate consistent improvements as reported in previous work.
    
[^39]: 通过社交媒体预测心血管疾病风险

    Cardiovascular Disease Risk Prediction via Social Media. (arXiv:2309.13147v1 [cs.CL])

    [http://arxiv.org/abs/2309.13147](http://arxiv.org/abs/2309.13147)

    该论文通过分析Twitter上的情感内容，引入了一个新颖的与心血管疾病相关的关键词词典，并使用机器学习模型评估个体的心血管疾病风险。研究发现，分析情感内容能够超越仅使用人口统计数据进行预测，识别出潜在风险患者。这项研究显示了社交媒体在心血管疾病风险预测方面的潜力。

    

    研究人员利用Twitter和情感分析预测心血管疾病（CVD）的风险。我们通过审视推文中传达的情感，引入了一个新颖的与CVD相关的关键词词典。我们收集了来自美国18个州的推文，涵盖了阿巴拉契亚地区。采用VADER模型进行情感分析，我们将用户归类为潜在的CVD风险。采用机器学习（ML）模型评估个体的CVD风险，并随后将其应用于CDC数据集中的人口统计信息进行比较。我们考虑了各种性能评估指标，包括测试准确率，精确率，召回率，F1分数，马修斯相关系数（MCC）和科恩的Kappa分数（CK）。我们的研究结果表明，分析推文中的情感内容优于仅使用人口统计数据进行预测，能够识别出潜在风险患者的个体。这项研究强调了社交媒体在预测CVD风险方面的潜力。

    Researchers utilize Twitter and sentiment analysis to forecast the risk of Cardiovascular Disease (CVD). We have introduced a novel CVD-related keyword dictionary by scrutinizing the emotions conveyed in tweets. We gathered tweets from eighteen U.S. states, encompassing the Appalachian region. Employing the VADER model for sentiment analysis, we categorized users as potentially at risk for CVD. Machine Learning (ML) models were employed to assess individuals' CVD risk and were subsequently applied to a CDC dataset containing demographic information for comparison. We considered various performance evaluation metrics, including Test Accuracy, Precision, Recall, F1 score, Mathew's Correlation Coefficient (MCC), and Cohen's Kappa (CK) score. Our findings demonstrate that analyzing the emotional content of tweets outperforms the predictive capabilities of demographic data alone, enabling the identification of individuals at potential risk of developing CVD. This research underscores the po
    
[^40]: DRG-LLaMA: 调优LLaMA模型以预测住院患者的诊断相关分组

    DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for Hospitalized Patients. (arXiv:2309.12625v1 [cs.AI])

    [http://arxiv.org/abs/2309.12625](http://arxiv.org/abs/2309.12625)

    DRG-LLaMA是一个通过在临床笔记上细调的大型语言模型，用于改进住院患者的诊断相关分组预测。在多个评估指标上，DRG-LLaMA-7B相对于其他模型取得了显著的改进，并能够高准确度地预测基本DRG和并发症/合并症（CC）/重大并发症或合并症（MCC）的状态。

    

    在美国住院付费系统中，诊断相关分组（DRG）起着关键作用，但目前的分组过程耗时。我们引入了DRG-LLaMA，一个在临床笔记上进行细调的大型语言模型（LLM），以改善DRG预测。使用Meta的LLaMA作为基础模型，我们通过在236,192个MIMIC-IV出院摘要上进行低秩适应（LoRA）优化。在输入令牌长度为512的情况下，DRG-LLaMA-7B实现的宏平均F1分数为0.327，顶级预测准确度为52.0％，宏平均AUC为0.986。令人印象深刻的是，DRG-LLaMA-7B在这个任务上超过了之前报道的领先模型，相对于ClinicalBERT的宏平均F1分数提高了40.3％，相对于CAML提高了35.7％。当应用DRG-LLaMA来预测基本DRG和并发症/合并症（CC）/重大并发症或合并症（MCC）时，基本DRG的顶级预测准确度达到了67.8％，而CC/MCC状态的预测准确度达到了67.5％。

    In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) plays a key role but its current assignment process is time-consuming. We introduce DRG-LLaMA, a large language model (LLM) fine-tuned on clinical notes for improved DRG prediction. Using Meta's LLaMA as the base model, we optimized it with Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge summaries. With an input token length of 512, DRG-LLaMA-7B achieved a macro-averaged F1 score of 0.327, a top-1 prediction accuracy of 52.0% and a macro-averaged Area Under the Curve (AUC) of 0.986. Impressively, DRG-LLaMA-7B surpassed previously reported leading models on this task, demonstrating a relative improvement in macro-averaged F1 score of 40.3% compared to ClinicalBERT and 35.7% compared to CAML. When DRG-LLaMA is applied to predict base DRGs and complication or comorbidity (CC) / major complication or comorbidity (MCC), the top-1 prediction accuracy reached 67.8% for base DRGs and 67.5% for CC/MCC status. DRG-L
    
[^41]: 带有短期和长期记忆协作的增强记忆型LLM个性化

    Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination. (arXiv:2309.11696v1 [cs.CL])

    [http://arxiv.org/abs/2309.11696](http://arxiv.org/abs/2309.11696)

    本研究提出了一种计算仿生记忆机制，配备了参数高效的微调模式，用于个性化LLMs。实验证明了该方法的有效性和可行性。

    

    大型语言模型（LLMs），如GPT3.5，在理解和生成自然语言方面表现出卓越的能力。然而，它们的非个性化生成方式可能导致用户特定结果的亚优化。通常，用户根据自己的知识和偏好以不同的方式进行对话。这就需要增强面向用户的LLM的任务，但这方面的研究尚未深入探索。之前的研究已经探索了基于记忆的方法来存储和检索知识，以增强生成而无需为新的查询进行重新训练。然而，我们认为仅仅使用记忆模块无法理解用户的偏好，并且完全训练一个LLM可能成本过高。在本研究中，我们提出了一种新颖的计算仿生记忆机制，配备了一个参数高效的微调模式，用于个性化LLMs。我们广泛的实验结果证明了该方法的有效性和可行性。

    Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable proficiency in comprehending and generating natural language. However, their unpersonalized generation paradigm may result in suboptimal user-specific outcomes. Typically, users converse differently based on their knowledge and preferences. This necessitates the task of enhancing user-oriented LLM which remains unexplored. While one can fully train an LLM for this objective, the resource consumption is unaffordable. Prior research has explored memory-based methods to store and retrieve knowledge to enhance generation without retraining for new queries. However, we contend that a mere memory module is inadequate to comprehend a user's preference, and fully training an LLM can be excessively costly. In this study, we propose a novel computational bionic memory mechanism, equipped with a parameter-efficient fine-tuning schema, to personalize LLMs. Our extensive experimental results demonstrate the effectiveness and su
    
[^42]: MBR和QE微调：对最佳和最昂贵的解码方法进行训练时蒸馏

    MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods. (arXiv:2309.10966v1 [cs.CL])

    [http://arxiv.org/abs/2309.10966](http://arxiv.org/abs/2309.10966)

    本文提出了MBR微调和QE微调方法，将训练时的质量提升蒸馏到基准模型中，从而在推断时使用高效的解码算法。实验证明，这些微调方法能显著提升模型性能，甚至超过基准模型。

    

    最近在自然语言生成（NLG）任务的解码方法研究中表明，传统的波束搜索和贪婪解码算法并不是最优的，因为模型概率不总是与人类偏好一致。为了解决模型困惑度与质量不匹配的问题，提出了一些更强的解码方法，包括质量估计（QE）重排序和最小贝叶斯风险（MBR）解码。尽管这些解码方法实现了最先进的性能，但计算成本过高。在这项工作中，我们提出了MBR微调和QE微调，这些微调方法在训练时蒸馏了这些解码方法的质量提升，在推断时使用高效的解码算法。通过使用神经机器翻译（NMT）这一典型的NLG任务，我们表明即使进行自训练，这些微调方法的性能仍明显优于基准模型。此外，当使用外部LLM作为教师模型时，这些微调方法也表现出了卓越的性能。

    Recent research in decoding methods for Natural Language Generation (NLG) tasks has shown that the traditional beam search and greedy decoding algorithms are not optimal, because model probabilities do not always align with human preferences. Stronger decoding methods, including Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR) decoding, have since been proposed to mitigate the model-perplexity-vs-quality mismatch. While these decoding methods achieve state-of-the-art performance, they are prohibitively expensive to compute. In this work, we propose MBR finetuning and QE finetuning which distill the quality gains from these decoding methods at training time, while using an efficient decoding algorithm at inference time. Using the canonical NLG task of Neural Machine Translation (NMT), we show that even with self-training, these finetuning methods significantly outperform the base model. Moreover, when using an external LLM as a teacher model, these finetuning methods outp
    
[^43]: 对比解码提高大型语言模型的推理能力

    Contrastive Decoding Improves Reasoning in Large Language Models. (arXiv:2309.09117v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.09117](http://arxiv.org/abs/2309.09117)

    对比解码是一种简单、计算量轻、无需训练的文本生成方法，通过最大化强模型和弱模型之间的似然差异，在各种推理任务上取得了显著改进。

    

    我们证明了对比解码——一种由Li等人提出的简单、计算量轻、无需训练的文本生成方法——在各种推理任务上超过贪婪解码的性能。最初被证明可以改善长文本生成的感知质量，对比解码搜索最大化强模型和弱模型之间似然差异加权的字符串。我们展示了对比解码在HellaSwag常识推理基准上使LLaMA-65B超过了LLaMA 2、GPT-3.5和PaLM 2-L，并且在GSM8K数学单词推理基准上超过了LLaMA 2、GPT-3.5和PaLM-540B，此外还在一系列其他任务上有了改进。分析表明，对比解码通过防止某些抽象推理错误以及避免在思维链条中复制输入的部分等简单模式来改进现有方法。整体而言，对比解码表现突出。

    We demonstrate that Contrastive Decoding -- a simple, computationally light, and training-free text generation method proposed by Li et al 2022 -- achieves large out-of-the-box improvements over greedy decoding on a variety of reasoning tasks. Originally shown to improve the perceived quality of long-form text generation, Contrastive Decoding searches for strings that maximize a weighted difference in likelihood between strong and weak models. We show that Contrastive Decoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM 2-L on the HellaSwag commonsense reasoning benchmark, and to outperform LLaMA 2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark, in addition to improvements on a collection of other tasks. Analysis suggests that Contrastive Decoding improves over existing methods by preventing some abstract reasoning errors, as well as by avoiding simpler modes such as copying sections of the input during chain-of-thought. Overall, Contrastive Decoding outp
    
[^44]: 讲故事的动态原则

    The Dynamical Principles of Storytelling. (arXiv:2309.07797v1 [cs.CL])

    [http://arxiv.org/abs/2309.07797](http://arxiv.org/abs/2309.07797)

    这项研究揭示了讲故事的开头部分通常会遵循动作原则，并且这种顺序可能与西方传统讲故事的有关。

    

    在考虑了1800个短篇小说的开头部分之后，我们发现平均故事的前十几个段落遵循了arXiv:2309.06600中定义的动作原则。当段落的顺序被打乱时，平均值不再具备这一特性。研究结果表明，在开始一个故事时，我们在语义空间中有一个优先方向，可能与亚里士多德在《诗学》中所暗示的西方传统讲故事有关。

    When considering the opening part of 1800 short stories, we find that the first dozen paragraphs of the average narrative follow an action principle as defined in arXiv:2309.06600. When the order of the paragraphs is shuffled, the average no longer exhibits this property. The findings show that there is a preferential direction we take in semantic space when starting a story, possibly related to a common Western storytelling tradition as implied by Aristotle in Poetics.
    
[^45]: 损失突然下降：语法习得、相变和MLM中的简化偏差

    Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs. (arXiv:2309.07311v1 [cs.CL])

    [http://arxiv.org/abs/2309.07311](http://arxiv.org/abs/2309.07311)

    本文通过对掩码语言模型中的语法习得进行案例研究，发现在训练的一个短暂窗口内，模型突然获得了语法注意结构(SAS)，并伴随着损失的陡峭下降。SAS对随后习得语言能力起到了重要的促进作用。

    

    自然语言处理(NLP)中的大多数可解释性研究侧重于理解完全训练模型的行为和特征。然而，通过观察训练过程的轨迹，可能才能获得对模型行为的某些洞察。在本文中，我们通过对掩码语言模型(MLMs)中的语法习得进行案例研究，展示了如何通过分析训练过程中可解释性的演化来加深我们对新兴行为的理解。具体而言，我们研究了语法注意结构(SAS)，这是MLMs中自然形成的一个特性，其中特定的Transformer头倾向于关注特定的句法关系。我们发现在训练的一个短暂窗口内，模型突然获得了SAS，并发现这个窗口与损失的陡峭下降同时发生。此外，SAS促使了随后对语言能力的习得。然后，我们通过引入一个正则化项来操纵训练过程中的SAS，来研究SAS的因果作用，并进行了实验证明。

    Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. In this paper, we present a case study of syntax acquisition in masked language models (MLMs). Our findings demonstrate how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in training when models abruptly acquire SAS and find that this window is concurrent with a steep drop in loss. Moreover, SAS precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by introducing a regularizer to manipulate SAS during training, and demonstrate
    
[^46]: 叙事作为一个动力系统

    Narrative as a Dynamical System. (arXiv:2309.06600v1 [cs.CL])

    [http://arxiv.org/abs/2309.06600](http://arxiv.org/abs/2309.06600)

    叙事可以被视为一个动力系统，其演化可以用一个行动积分来描述，并且平均路径与行动原理一致。

    

    越来越多的证据表明，人类活动的全过程以及叙事可以被视为物理意义上的动力系统；一个由行动积分描述其演化的系统，使得从A点到B点的所有可能路径的平均值由行动的极大值给出。我们通过平均约500个不同的叙事构建了三条这样的路径，并且我们展示了平均路径与行动原理一致。

    There is increasing evidence that human activity in general, and narrative in particular, can be treated as a dynamical system in the physics sense; a system whose evolution is described by an action integral, such that the average of all possible paths from point A to point B is given by the extremum of the action. We create by construction three such paths by averaging about 500 different narratives, and we show that the average path is consistent with an action principle.
    
[^47]: 离线逆向强化学习下的提示评估与优化

    Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])

    [http://arxiv.org/abs/2309.06553](http://arxiv.org/abs/2309.06553)

    这项工作介绍了一种基于离线逆向强化学习的提示评估与优化方法，通过利用离线数据集和逆向强化学习，预测提示性能、提高成本效益、生成易读的结果。

    

    最近，像ChatGPT这样的大型语言模型（LLM）的发展取得了显著的性能，通过利用人类专业知识。然而，充分揭示LLMs在复杂任务中的潜力需要在自然语言提示的广阔搜索空间中进行导航。虽然提示工程显示出潜力，但试错尝试中所需的人工设计提示和相关成本带来了重大挑战。关键是，提示优化的效率取决于昂贵的提示评估过程。本工作介绍了Prompt-OIRL，这是一种基于离线逆向强化学习的方法，旨在弥合有效提示评估和可负担性之间的差距。我们的方法利用专家评估的离线数据集，运用逆向强化学习获得一个针对离线、查询依赖型提示评估的奖励模型。Prompt-OIRL的优点是多方面的：它预测提示的性能，成本高效，生成易读的结果。

    The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
    
[^48]: PACE: 使用GPT-4进行云事件根本原因分析中的提示和增加以进行校准的置信度估计

    PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v1 [cs.CL])

    [http://arxiv.org/abs/2309.05833](http://arxiv.org/abs/2309.05833)

    本文提出了一种通过提示检索增强的大语言模型（LLM）来增强云事件根本原因分析工具中置信度估计的方法。

    

    近年来，IT行业向基于云的平台的转变强调了云事件根本原因分析的重要性，以确保服务的可靠性和维护客户信任。核心问题是有效确定根本原因，由于当代云基础设施的复杂性，这一任务变得具有挑战性。尽管出现了许多用于根本原因识别的基于AI的工具，但它们的适用性仍受到其输出质量不一致的限制。本文介绍了一种通过提示检索增强的大语言模型（LLM）来增强根本原因分析工具中置信度估计的方法。此方法分为两个阶段。首先，模型根据历史事件数据评估自身的置信度，考虑其对证据的评估强度。然后，模型审核由预测器生成的根本原因。然后，优化步骤将这些评估结合起来确定最终的置信度估计。

    In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the fin
    
[^49]: Neural-Hidden-CRF: 一种鲁棒的弱监督序列标注模型

    Neural-Hidden-CRF: A Robust Weakly-Supervised Sequence Labeler. (arXiv:2309.05086v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05086](http://arxiv.org/abs/2309.05086)

    Neural-Hidden-CRF是一种鲁棒的弱监督序列标注模型，通过引入隐藏CRF层和利用强大的语言模型，它在各项基准测试中取得了最新的最优结果。

    

    我们提出了一种基于神经网络的无向图模型 Neural-Hidden-CRF 来解决弱监督序列标注问题。在概率无向图理论的框架下，Neural-Hidden-CRF融合了一个隐藏CRF层，用于建模单词序列、潜在真实序列和弱标签序列之间的变量关系并具有全局视角。在Neural-Hidden-CRF中，我们可以利用强大的语言模型BERT或其他深度模型为潜在真实序列提供丰富的上下文语义知识，并使用隐藏的CRF层来捕捉内部标签之间的依赖关系。Neural-Hidden-CRF在概念上简单而在实践中强大。它在一个众包基准测试和三个弱监督基准测试上取得了最新的最优结果，包括在平均泛化和推理性能方面超过了最新的高级模型CHMM的2.80 F1分和2.23 F1分。

    We propose a neuralized undirected graphical model called Neural-Hidden-CRF to solve the weakly-supervised sequence labeling problem. Under the umbrella of probabilistic undirected graph theory, the proposed Neural-Hidden-CRF embedded with a hidden CRF layer models the variables of word sequence, latent ground truth sequence, and weak label sequence with the global perspective that undirected graphical models particularly enjoy. In Neural-Hidden-CRF, we can capitalize on the powerful language model BERT or other deep models to provide rich contextual semantic knowledge to the latent ground truth sequence, and use the hidden CRF layer to capture the internal label dependencies. Neural-Hidden-CRF is conceptually simple and empirically powerful. It obtains new state-of-the-art results on one crowdsourcing benchmark and three weak-supervision benchmarks, including outperforming the recent advanced model CHMM by 2.80 F1 points and 2.23 F1 points in average generalization and inference perfo
    
[^50]: Jais和Jais-chat：以阿拉伯语为中心的基础和指导调优的开放式生成式大型语言模型

    Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models. (arXiv:2308.16149v1 [cs.CL])

    [http://arxiv.org/abs/2308.16149](http://arxiv.org/abs/2308.16149)

    Jais和Jais-chat是新的以阿拉伯语为中心的开放式生成式大型语言模型，具有13亿参数，在阿拉伯语方面表现出优异的知识和推理能力，并且在英语方面也具有竞争力。这些模型的发布旨在促进阿拉伯语LLMs的研究。

    

    我们介绍了Jais和Jais-chat，这是新的最先进的以阿拉伯语为中心的基础和指导调优的开放式生成式大型语言模型（LLMs）。这些模型基于GPT-3的仅解码器架构，并在阿拉伯语和英语文本的混合物中进行预训练，包括各种编程语言的源代码。这些模型具有130亿个参数，根据广泛的评估结果，在阿拉伯语知识和推理能力方面表现优于任何现有的开放式阿拉伯语和多语言模型。此外，尽管在训练时使用的英语数据要少得多，但这些模型在英语方面与类似规模的以英语为中心的开放模型相比仍具有竞争力。我们提供了模型的训练、调优、安全对齐和评估的详细描述。我们发布了模型的两个开放版本--基础Jais模型和指导调优的Jais-chat变种--旨在促进阿拉伯语LLMs的研究。详见https://hugging

    We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://hugging
    
[^51]: Scope is all you need: Transforming LLMs for HPC Code (arXiv:2308.09440v2 [cs.CL] UPDATED)

    Scope is all you need: Transforming LLMs for HPC Code. (arXiv:2308.09440v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.09440](http://arxiv.org/abs/2308.09440)

    本研究旨在质疑现有大型语言模型（LLMs）在高性能计算（HPC）领域中的应用，并通过开发针对特定领域的小型LLMs来解决该问题。具体而言，我们为HPC开发了一种名为Tokompiler的新型标记器，用于代码预处理和编译任务。

    

    随着强大计算资源的更容易获取，AI领域的软件开发越来越倾向于开发更大的语言模型（LLMs）来解决各种编程任务。即使应用于高性能计算（HPC）领域的LLMs也非常庞大（例如，数十亿个参数），并且需要昂贵的计算资源进行训练。我们发现这种设计选择令人困惑-为什么我们需要在与HPC不相关的自然语言和编程语言上训练的大型LLMs来处理HPC特定任务？在这一系列工作中，我们旨在质疑现有LLMs所做的设计选择，通过为特定领域开发更小的LLMs-我们称之为领域特定LLMs。具体而言，我们以HPC为领域开始，并提出了一种名为Tokompiler的新型标记器，专门用于HPC中的代码预处理和编译中心任务。Tokompiler利用语言原语的知识生成面向语言的标记，提供了一种新的预处理方法。

    With easier access to powerful compute resources, there is a growing trend in the field of AI for software development to develop larger and larger language models (LLMs) to address a variety of programming tasks. Even LLMs applied to tasks from the high-performance computing (HPC) domain are huge in size (e.g., billions of parameters) and demand expensive compute resources for training. We found this design choice confusing - why do we need large LLMs trained on natural languages and programming languages unrelated to HPC for HPC-specific tasks? In this line of work, we aim to question design choices made by existing LLMs by developing smaller LLMs for specific domains - we call them domain-specific LLMs. Specifically, we start off with HPC as a domain and propose a novel tokenizer named Tokompiler, designed specifically for preprocessing code in HPC and compilation-centric tasks. Tokompiler leverages knowledge of language primitives to generate language-oriented tokens, providing a c
    
[^52]: 在大型语言模型中使用反向推理进行验证

    Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])

    [http://arxiv.org/abs/2308.07758](http://arxiv.org/abs/2308.07758)

    本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。

    

    链式思考（Chain-of-Though, CoT）提示在各种推理任务中表现出了很好的性能。最近，Self-Consistency提出了一种方法，即通过采样一组不同的推理链，这些链可能导致不同的答案，然后选择得票最多的答案。本文提出了一种新颖的方法，即在验证候选答案时使用反向推理。我们使用一个简单的模板，即``如果我们知道上述问题的答案是候选答案，那么未知变量x的值是多少？''，将问题中的一个标记屏蔽，并要求语言模型预测被屏蔽的标记。直观上讲，如果提供的候选答案是正确的，语言模型应该能够成功预测被屏蔽的标记。我们进一步提出了FOBAR方法，将正向和反向推理结合起来估计候选答案的概率。我们在六个数据集和三个实验中进行了广泛的实验。

    Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
    
[^53]: 时间常识推理与获取概述

    An Overview Of Temporal Commonsense Reasoning and Acquisition. (arXiv:2308.00002v1 [cs.AI])

    [http://arxiv.org/abs/2308.00002](http://arxiv.org/abs/2308.00002)

    本文综述了时间常识推理领域的研究进展，重点关注通过增强语言模型的性能来提高推理能力，并对多个数据集进行评估。然而，这些增强模型仍然难以达到人类水平的推理能力。

    

    时间常识推理是指理解短语、动作和事件的典型时间背景并将其应用于需要这种知识的问题推理的能力。这种能力在时间自然语言处理任务中至关重要，可能应用于时间线摘要、时间问答和时间自然语言推断等方面。最近的研究表明，大型语言模型虽然善于生成语法正确的句子和解决分类任务，但在推理过程中往往会采取捷径，并陷入简单的语言陷阱。本文章概述了在时间常识推理领域的研究，特别关注通过各种增强方式提高语言模型的性能以及对越来越多数据集的评估。然而，这些增强模型在推理任务上仍然难以达到人类的水平。

    Temporal commonsense reasoning refers to the ability to understand the typical temporal context of phrases, actions, and events, and use it to reason over problems requiring such knowledge. This trait is essential in temporal natural language processing tasks, with possible applications such as timeline summarization, temporal question answering, and temporal natural language inference. Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps. This article provides an overview of research in the domain of temporal commonsense reasoning, particularly focusing on enhancing language model performance through a variety of augmentations and their evaluation across a growing number of datasets. However, these augmented models still struggle to approach human performance on reasoning task
    
[^54]: 通过解耦置信学习来减缓标签偏见

    Mitigating Label Bias via Decoupled Confident Learning. (arXiv:2307.08945v1 [cs.LG])

    [http://arxiv.org/abs/2307.08945](http://arxiv.org/abs/2307.08945)

    这项研究提出了一种名为DeCoLe的修剪方法，用于减轻标签偏见问题。研究在合成数据集和仇恨言论检测领域取得了成功的结果。

    

    随着对算法公平性的担忧增加，出现了许多方法来减轻算法偏见。然而，这些方法主要假设训练数据中观察到的标签是正确的。这是有问题的，因为标签偏见在包括医疗、招聘和内容审核在内的重要领域中普遍存在。特别是，人类生成的标签容易带有社会偏见。虽然标签偏见的存在已经在概念上进行了讨论，但缺乏应对此问题的方法。我们提出了一种修剪方法——解耦置信学习（DeCoLe），专门设计来减轻标签偏见。在演示了其在合成数据集上的性能后，我们将DeCoLe应用于仇恨言论检测的环境中，这是一个被认为是重要挑战的领域，并展示其成功识别出偏见标签并超越竞争方法的表现。

    Growing concerns regarding algorithmic fairness have led to a surge in methodologies to mitigate algorithmic bias. However, such methodologies largely assume that observed labels in training data are correct. This is problematic because bias in labels is pervasive across important domains, including healthcare, hiring, and content moderation. In particular, human-generated labels are prone to encoding societal biases. While the presence of labeling bias has been discussed conceptually, there is a lack of methodologies to address this problem. We propose a pruning method -- Decoupled Confident Learning (DeCoLe) -- specifically designed to mitigate label bias. After illustrating its performance on a synthetic dataset, we apply DeCoLe in the context of hate speech detection, where label bias has been recognized as an important challenge, and show that it successfully identifies biased labels and outperforms competing approaches.
    
[^55]: 大型生成语言模型能够执行ASR错误修正吗？

    Can Generative Large Language Models Perform ASR Error Correction?. (arXiv:2307.04172v1 [cs.CL])

    [http://arxiv.org/abs/2307.04172](http://arxiv.org/abs/2307.04172)

    本文研究了大型生成语言模型在ASR错误修正中的应用，以ChatGPT为例，探讨了其在零-shot或1-shot设置下的能力。实验结果表明，使用ChatGPT模型进行错误修正可以显著提高ASR系统的性能。

    

    ASR错误修正在语音识别系统的后处理中继续扮演重要角色。传统上，这些模型使用底层ASR系统的解码结果和参考文本进行有监督训练。这种方法在计算上非常耗费资源，而且需要在切换底层ASR模型时重新训练模型。近年来，大型语言模型的发展使得它们能够以零-shot的方式执行自然语言处理任务。本文以ChatGPT为例，研究其在零-shot或1-shot设置下进行ASR错误修正的能力。我们使用ASR N-best列表作为模型的输入，并提出了无约束错误修正和N-best约束错误修正的方法。在Conformer-Transducer模型和预训练的Whisper模型上的实验结果表明，使用强大的ChatGPT模型进行错误修正可以大大提高ASR系统的性能。

    ASR error correction continues to serve as an important part of post-processing for speech recognition systems. Traditionally, these models are trained with supervised training using the decoding results of the underlying ASR system and the reference text. This approach is computationally intensive and the model needs to be re-trained when switching the underlying ASR model. Recent years have seen the development of large language models and their ability to perform natural language processing tasks in a zero-shot manner. In this paper, we take ChatGPT as an example to examine its ability to perform ASR error correction in the zero-shot or 1-shot settings. We use the ASR N-best list as model input and propose unconstrained error correction and N-best constrained error correction methods. Results on a Conformer-Transducer model and the pre-trained Whisper model show that we can largely improve the ASR system performance with error correction using the powerful ChatGPT model.
    
[^56]: 通过强化指令调整来减轻大规模多模态模型中的幻觉问题

    Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning. (arXiv:2306.14565v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.14565](http://arxiv.org/abs/2306.14565)

    本论文通过引入第一个大型多样化的视觉指令调整数据集，提出了一种解决大规模多模态模型中幻觉问题的方法。通过设计包含正负指令的数据集和提出的评估方法，能够更准确地衡量模型产生的幻觉。

    

    尽管多模态任务取得了可喜的进展，但当前的大规模多模态模型（LMM）很容易在描述图像和人类指令时产生不一致的幻觉。本文通过引入第一个大型多样化的视觉指令调整数据集LRV-Instruction来解决这个问题。我们的数据集包含由GPT4生成的12万个视觉指令，涵盖了16个开放式指令和答案的视觉与语言任务。与现有研究主要关注正指令样本不同，我们设计了LRV-Instruction以包含更多针对更强的视觉指令调整的正负指令。我们的负指令在两个语义层次上设计：（i）不存在元素操作和（ii）存在元素操作。为了更有效地衡量LMM所产生的幻觉，我们提出了一种新的方法，即GPT4辅助的视觉指令评估（GAVIE）。

    Despite the promising progress in multi-modal tasks, current large multi-modal models (LMM) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset consists of 120k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at two semantic levels: (i) Nonexistent Element Manipulation and (ii) Existent Element Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a novel approach to eva
    
[^57]: 用贝叶斯推理模拟人类类人概念学习

    Modeling Human-like Concept Learning with Bayesian Inference over Natural Language. (arXiv:2306.02797v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02797](http://arxiv.org/abs/2306.02797)

    该论文通过在自然语言中进行贝叶斯推理来模拟人类类人概念学习，使用大型语言模型作为提议分布并拟合先验以更好地模拟人类学习者，并在生成性和逻辑性概念上进行实验评估。

    

    我们通过在自然语言中进行贝叶斯推理来模拟对抽象符号概念的学习。为了高效推理，我们使用一个大型语言模型作为提议分布。我们根据人类数据拟合先验以更好地模拟人类学习者，并在生成性和逻辑性概念上进行评估。

    We model learning of abstract symbolic concepts by performing Bayesian inference over utterances in natural language. For efficient inference, we use a large language model as a proposal distribution. We fit a prior to human data to better model human learners, and evaluate on both generative and logical concepts.
    
[^58]: 上下文视觉变换器用于强健的表示学习

    Contextual Vision Transformers for Robust Representation Learning. (arXiv:2305.19402v1 [cs.CV])

    [http://arxiv.org/abs/2305.19402](http://arxiv.org/abs/2305.19402)

    上下文视觉变换器(ContextViT)用于生成图像的鲁棒特征表示，引入了一个额外的上下文令牌，可以解释掉特定于组的协变量结构，同时保持跨组共享的核心视觉特征，能够在监督微调、半监督学习以及主动学习等方面得到应用。

    

    本文介绍了一种上下文视觉变换器(ContextViT)的方法，用于生成图像的鲁棒特征表示，特别是针对表现出分组结构的图像，如协变量。ContextViT引入了一个额外的上下文令牌来编码特定于组的信息，允许模型解释掉特定于组的协变量结构，同时保持跨组共享的核心视觉特征。具体而言，在给定输入图像的情况下，Context-ViT将共享相同协变量的图像映射到该上下文令牌，并添加到输入图像令牌中，以捕获将模型条件化为组成员身份的效果。此外，我们还引入了一个上下文推断网络，可以在运行时预测这些令牌，只需要给出一些来自组分布的样本即可，使得ContextViT可以推广到新的测试分布。我们通过各种应用程序说明了ContextViT的性能。在监督微调中，我们证明了将预训练的ViTs与额外的上下文令牌相结合可以提高图像分类基准的准确性。我们还展示了ContextViT可以用于半监督学习，在CIFAR-10数据集上仅使用10%的标记样本即可实现最先进的表现。最后，我们展示了ContextViT可以通过引导选择来自少数群体的更具信息价值的样本，从而提高主动学习的效率。

    We present Contextual Vision Transformers (ContextViT), a method for producing robust feature representations for images exhibiting grouped structure such as covariates. ContextViT introduces an extra context token to encode group-specific information, allowing the model to explain away group-specific covariate structures while keeping core visual features shared across groups. Specifically, given an input image, Context-ViT maps images that share the same covariate into this context token appended to the input image tokens to capture the effects of conditioning the model on group membership. We furthermore introduce a context inference network to predict such tokens on the fly given a few samples from a group distribution, enabling ContextViT to generalize to new testing distributions at inference time. We illustrate the performance of ContextViT through a diverse range of applications. In supervised fine-tuning, we demonstrate that augmenting pre-trained ViTs with additional context 
    
[^59]: 多尺度正负样本检测AI生成文本

    Multiscale Positive-Unlabeled Detection of AI-Generated Texts. (arXiv:2305.18149v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.18149](http://arxiv.org/abs/2305.18149)

    本文提出了一种多尺度正负样本的训练框架，以解决多尺度AI生成文本的检测问题。通过将短机器文本标记为“未标记”来重新表述文本分类问题，并提出了一个规则化损失函数来优化检测性能，有效性能显著优于现有的方法。

    

    最近发布的大型语言模型（LLM）如ChatGPT等在生成类似于人类的文本方面令人惊讶，但它们可能被用于制造虚假的学术文本、虚假新闻、虚假推特等。先前的作品提出了检测这些多尺度AI生成文本的方法，包括简单的ML分类器、基于预训练模型的训练不可知方法和精调的语言分类模型。然而，主流检测器在构建时没有考虑到文本长度的因素：短文本的缺乏信息特征，使其更难检测。针对多尺度文本检测的挑战，本文提出了一个多尺度正负样本（MPU）训练框架。首先，我们承认短的机器文本具有类人属性，并将文本分类重新表述为一个正负样本问题，即通过在训练期间标记这些短的机器文本为"unlabeled"来解决这个问题。在这个正负样本的背景下，我们提出了

    Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may get misused for fake scholarly texts, fake news, fake tweets, et cetera. Previous works have proposed methods to detect these multiscale AI-generated texts, including simple ML classifiers, pretrained-model-based training-agnostic methods, and finetuned language classification models. However, mainstream detectors are formulated without considering the factor of corpus length: shorter corpuses are harder to detect compared with longer ones for shortage of informative features. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the challenge of multiscale text detection. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase text classification as a Positive-Unlabeled (PU) problem by marking these short machine texts as "unlabeled" during training. In this PU context, we propose the le
    
[^60]: 利用基于优势的离线策略梯度改进语言模型

    Improving Language Models with Advantage-based Offline Policy Gradients. (arXiv:2305.14718v1 [cs.CL])

    [http://arxiv.org/abs/2305.14718](http://arxiv.org/abs/2305.14718)

    本文介绍了一种简单的训练算法Left-over Lunch RL （LoL-RL），使用离线策略梯度学习任何序列到序列数据，从而实现优化LM效用的方法。

    

    根据用户定义的质量或风格限制提高语言模型生成是具有挑战性的。典型的方法包括学习额外的人工编写数据，使用启发式方法过滤“低质量”数据和/或使用强化学习与人体反馈（RLHF）。然而，过滤会删除有价值的训练信号，而数据收集和RLHF不断需要额外的人工编写或LM探索数据，这可能成本高。一个自然的问题是“我们可以利用RL来优化现有的众包和互联网数据上的LM效用吗？”为此，我们提出了剩余午餐强化学习（LoL-RL），这是一种简单的训练算法，使用离线策略梯度来学习语言生成任务作为1步RL游戏。 LoL-RL可以微调LM，以优化任意基于分类器或人定义的效用函数的任何序列到序列数据。使用不同大小模型的五个不同语言生成任务的实验

    Improving language model generations according to some user-defined quality or style constraints is challenging. Typical approaches include learning on additional human-written data, filtering ``low-quality'' data using heuristics and/or using reinforcement learning with human feedback (RLHF). However, filtering can remove valuable training signals, whereas data collection and RLHF constantly require additional human-written or LM exploration data which can be costly to obtain. A natural question to ask is ``Can we leverage RL to optimize LM utility on existing crowd-sourced and internet data?''  To this end, we present Left-over Lunch RL (LoL-RL), a simple training algorithm that uses offline policy gradients for learning language generation tasks as a 1-step RL game. LoL-RL can finetune LMs to optimize arbitrary classifier-based or human-defined utility functions on any sequence-to-sequence data. Experiments with five different language generation tasks using models of varying sizes 
    
[^61]: 关于transformer主动学习中数据集可迁移性的研究

    On Dataset Transferability in Active Learning for Transformers. (arXiv:2305.09807v1 [cs.LG])

    [http://arxiv.org/abs/2305.09807](http://arxiv.org/abs/2305.09807)

    本文研究了基于transformer的预训练语言模型的主动学习中数据集的可迁移性问题，发现具有相似获取序列的主动学习方法产生的数据集在不同模型之间具有高度的可迁移性。

    

    主动学习旨在通过查询对模型学习最有益的示例来减少标注成本。尽管已经证明了对于微调基于transformer的预训练语言模型（PLMs），主动学习的有效性，但不清楚一个模型中获得的主动学习收益在多大程度上适用于其他模型。我们考虑在文本分类中积极获取的数据集的可迁移性问题，并调查了使用主动学习构建的数据集在使用不同PLM训练时能否保持AL收益。我们将AL数据集的可迁移性与不同PLMs查询到的实例的相似性联系起来，并表明具有类似获取序列的AL方法生成的数据集非常具有可迁移性，无论使用哪种模型。此外，我们表明，获取序列的相似性更受到AL方法的选择而非模型的影响。

    Active learning (AL) aims to reduce labeling costs by querying the examples most beneficial for model learning. While the effectiveness of AL for fine-tuning transformer-based pre-trained language models (PLMs) has been demonstrated, it is less clear to what extent the AL gains obtained with one model transfer to others. We consider the problem of transferability of actively acquired datasets in text classification and investigate whether AL gains persist when a dataset built using AL coupled with a specific PLM is used to train a different PLM. We link the AL dataset transferability to the similarity of instances queried by the different PLMs and show that AL methods with similar acquisition sequences produce highly transferable datasets regardless of the models used. Additionally, we show that the similarity of acquisition sequences is influenced more by the choice of the AL method than the choice of the model.
    
[^62]: PESTS: 波斯语-英语跨语言语料库用于语义文本相似度

    PESTS: Persian_English Cross Lingual Corpus for Semantic Textual Similarity. (arXiv:2305.07893v1 [cs.CL])

    [http://arxiv.org/abs/2305.07893](http://arxiv.org/abs/2305.07893)

    本研究提出了跨语言的语义相似性模型PESTS，并通过波斯语-英语的跨语言语料库来验证模型的准确性。

    

    近来，语义文本相似度成为自然语言处理中备受关注的组件。在计算语言学和自然语言处理中，评估单词、短语、段落和文本之间的语义相似性很重要。同时，语义相似性度量要求在源和目标语言中提供具有一定语义相似性的句子对。许多跨语言的语义相似度模型使用机器翻译来弥补跨语言语料库不可用的不足，但机器翻译的误差会降低模型的准确性。然而，在使用语义相似度特征实现机器翻译时，用相同的机器翻译模型可以提高结果的准确性。

    One of the components of natural language processing that has received a lot of investigation recently is semantic textual similarity. In computational linguistics and natural language processing, assessing the semantic similarity of words, phrases, paragraphs, and texts is crucial. Calculating the degree of semantic resemblance between two textual pieces, paragraphs, or phrases provided in both monolingual and cross-lingual versions is known as semantic similarity. Cross lingual semantic similarity requires corpora in which there are sentence pairs in both the source and target languages with a degree of semantic similarity between them. Many existing cross lingual semantic similarity models use a machine translation due to the unavailability of cross lingual semantic similarity dataset, which the propagation of the machine translation error reduces the accuracy of the model. On the other hand, when we want to use semantic similarity features for machine translation the same machine t
    
[^63]: 学习ASR路径：一种稀疏的多语言ASR模型

    Learning ASR pathways: A sparse multilingual ASR model. (arXiv:2209.05735v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2209.05735](http://arxiv.org/abs/2209.05735)

    本文提出了一种稀疏的多语言ASR模型，通过激活语言特定的子网络来显式地学习每种语言的参数，同时通过联合多语言训练实现对低资源语言的知识转移，相比于密集模型和语言不可知的剪枝模型，在低资源语言上提供更好的性能。

    This paper proposes a sparse multilingual ASR model, which explicitly learns the parameters for each language by activating language-specific sub-networks, and enables knowledge transfer for lower-resource languages via joint multilingual training. The proposed ASR pathways outperform both dense models and a language-agnostically pruned model, and provide better performance on low-resource languages compared to the monolingual sparse models.

    神经网络剪枝有效地压缩了自动语音识别（ASR）模型。然而，在多语言ASR中，语言不可知的剪枝可能会导致某些语言的性能严重下降，因为语言不可知的剪枝掩码可能不适合所有语言并且丢弃重要的语言特定参数。在这项工作中，我们提出了ASR路径，一种稀疏的多语言ASR模型，它激活语言特定的子网络（“路径”），以便为每种语言显式地学习参数。通过重叠的子网络，共享参数还可以通过联合多语言训练实现对低资源语言的知识转移。我们提出了一种新算法来学习ASR路径，并使用流式RNN-T模型在4种语言上评估了所提出的方法。我们提出的ASR路径模型优于密集模型和语言不可知的剪枝模型，并且与单语稀疏模型相比，在低资源语言上提供更好的性能。

    Neural network pruning compresses automatic speech recognition (ASR) models effectively. However, in multilingual ASR, language-agnostic pruning may lead to severe performance drops on some languages because language-agnostic pruning masks may not fit all languages and discard important language-specific parameters. In this work, we present ASR pathways, a sparse multilingual ASR model that activates language-specific sub-networks ("pathways"), such that the parameters for each language are learned explicitly. With the overlapping sub-networks, the shared parameters can also enable knowledge transfer for lower-resource languages via joint multilingual training. We propose a novel algorithm to learn ASR pathways, and evaluate the proposed method on 4 languages with a streaming RNN-T model. Our proposed ASR pathways outperform both dense models and a language-agnostically pruned model, and provide better performance on low-resource languages compared to the monolingual sparse models.
    
[^64]: 信息密度更高的人类语言加快了交流速度，但减少了对话广度。

    Human Languages with Greater Information Density Increase Communication Speed, but Decrease Conversation Breadth. (arXiv:2112.08491v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2112.08491](http://arxiv.org/abs/2112.08491)

    语言的信息密度越高，交流速度越快，但对话广度减少。这些研究结果揭示了语言结构对人类交流的重要影响。

    

    人类语言在编码语义信息（例如时间、空间、颜色、人的身体部位和活动）方面存在广泛的差异，但对语义信息的全局结构及其与人类交流的关系知之甚少。我们首先展示了在约1000种语言的样本中，语言在词汇中编码信息的密度存在广泛变化。其次，我们展示了语言信息密度与语义信息的密集配置之间的关联。最后，我们追踪了语言信息密度与交流模式之间的关系，结果表明信息密度更高的语言倾向于（1）更快的交流，但（2）在话题上概念上更狭窄的对话，对话中更深入地讨论了话题。这些结果突显了人类交流渠道上的一个重要变异源，揭示了语言结构对人类交流的影响。

    Human languages vary widely in how they encode information within circumscribed semantic domains (e.g., time, space, color, human body parts and activities), but little is known about the global structure of semantic information and nothing about its relation to human communication. We first show that across a sample of ~1,000 languages, there is broad variation in how densely languages encode information into their words. Second, we show that this language information density is associated with a denser configuration of semantic information. Finally, we trace the relationship between language information density and patterns of communication, showing that informationally denser languages tend toward (1) faster communication, but (2) conceptually narrower conversations within which topics of conversation are discussed at greater depth. These results highlight an important source of variation across the human communicative channel, revealing that the structure of language shapes the nat
    

