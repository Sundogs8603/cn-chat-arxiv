# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Towards Matching Phones and Speech Representations.](http://arxiv.org/abs/2310.17558) | 本研究探讨了学习手机类型的问题，并将其作为匹配聚类中心与手机嵌入的问题。通过生成伪标签和引入新的损失函数，我们改进了自监督表示，实验证明匹配结果捕捉到了手机之间的关系，并显著提高了手机分类的性能。 |
| [^2] | [Unpacking the Ethical Value Alignment in Big Models.](http://arxiv.org/abs/2310.17551) | 本文探讨了大型模型中的伦理价值对齐问题，调查了现有的人工智能伦理准则，并提出了建立统一和普遍的人工智能伦理框架的重要性。此外，对当前主流大型语言模型的伦理倾向进行了研究，并分析了在实现伦理价值对齐时的挑战。 |
| [^3] | [Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models.](http://arxiv.org/abs/2310.17530) | 本研究评估了性别中立的预训练视觉-语言模型中的偏见和公平性，并发现预训练和微调后的偏见放大是相互独立的。此外，持续预训练对性别中立数据有利，可以在一些任务中促进公平。 |
| [^4] | [Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages.](http://arxiv.org/abs/2310.17526) | 本研究评估了GPT-4在多种语言的同行评审文献和灰色文献筛选和提取数据方面的能力，结果显示GPT-4在大多数任务上准确度与人类表现相当，在调整了偶然一致性和数据集不平衡后，其在数据提取方面表现出中等水平的准确度。 |
| [^5] | [The Validity of Evaluation Results: Assessing Concurrence Across Compositionality Benchmarks.](http://arxiv.org/abs/2310.17514) | 该研究调查了在组合泛化领域中特定数据集设计选择对模型能力结论的影响，发现不同数据集对建模方法的排名存在差异，人类生成的数据集之间的一致性高于合成数据集之间的一致性，数据集是否来自同一来源更能预测结果的模型排名。 |
| [^6] | [The Expressive Power of Low-Rank Adaptation.](http://arxiv.org/abs/2310.17513) | 本文分析了低秩适应（LoRA）的表达能力，证明了对于全连接神经网络，当LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度）时，LoRA可以使任何模型f准确表示任何较小的目标模型f。对于Transformer网络，通过rank-（嵌入大小/ 2）的LoRA适配器可以使任何模型适应于相同大小的目标模型。 |
| [^7] | [CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents.](http://arxiv.org/abs/2310.17512) | 本文研究了基于大型语言模型的智能体之间的竞争行为。通过建立一个竞争环境并进行实验，发现竞争可以促使智能体进行转变和采取新策略，从而在社会和经济发展中发挥重要作用。 |
| [^8] | [The IMS Toucan System for the Blizzard Challenge 2023.](http://arxiv.org/abs/2310.17499) | 我们改进了我们在Blizzard Challenge 2021中提交的系统，使用一个基于规则的文本到音素处理系统，并且设计了数据处理、训练和推理过程来处理Blizzard Challenge 2023的数据。我们的系统标识符是G。 |
| [^9] | [Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering.](http://arxiv.org/abs/2310.17490) | 本研究提出了一种通过减少无关文档的干扰来改善开放领域问答中的零样本阅读器的方法。采用了干扰感知的答案选择(DAS)方法，以解决LLMs受到干扰和过度自信的问题。实验结果表明，该方法成功地改善了零样本阅读器的性能，并展现出了优越的可迁移性。 |
| [^10] | [LightLM: A Lightweight Deep and Narrow Language Model for Generative Recommendation.](http://arxiv.org/abs/2310.17488) | LightLM是一种轻量级的基于Transformer的生成推荐模型，通过引入轻量级深窄Transformer架构来实现直接生成推荐项。 |
| [^11] | [Dialect Adaptation and Data Augmentation for Low-Resource ASR: TalTech Systems for the MADASR 2023 Challenge.](http://arxiv.org/abs/2310.17448) | TalTech使用了方言适应和数据增强的方法，并分别在提供的训练数据和额外音频数据上取得了显著的改进。实验结果表明，该方法在低资源ASR场景中取得了最低的词错误率。 |
| [^12] | [''Fifty Shades of Bias'': Normative Ratings of Gender Bias in GPT Generated English Text.](http://arxiv.org/abs/2310.17428) | 本研究通过创建包含规范评级的GPT生成的英文文本数据集，系统地分析了观察到的性别偏见主题的变化及其对偏见的感知。通过认识到偏见必须以相对的尺度来感知，本研究对不同程度偏见的接受能力进行了调查，对性别偏见的研究提供了新的视角。 |
| [^13] | [PETA: Evaluating the Impact of Protein Transfer Learning with Sub-word Tokenization on Downstream Applications.](http://arxiv.org/abs/2310.17415) | PETA通过评估亚词切分在蛋白质迁移学习中的影响，提出了关于蛋白质语言模型的综合评估方法，并发现词汇表大小在50以上能够获得最佳性能。 |
| [^14] | [Harnessing GPT-3.5-turbo for Rhetorical Role Prediction in Legal Cases.](http://arxiv.org/abs/2310.17413) | 该研究利用GPT-3.5-turbo对法律案例中的修辞角色进行预测，并发现使用少量标记示例进行引导可以获得更好的性能。 |
| [^15] | [Tackling the Matrix Multiplication Micro-kernel Generation with Exo.](http://arxiv.org/abs/2310.17408) | 介绍了使用Exo编译器生成接近于甚至优于手动开发的微内核的步骤和方法，并解决了为每个新硬件生成专用微内核的问题。 |
| [^16] | [Meaning and understanding in large language models.](http://arxiv.org/abs/2310.17407) | 本文评估了大型语言模型在理解自然语言中的意义时的条件。最新的人工智能模型认为传统的机器理解语言的假设需要修订。这篇文章着重强调了最先进的LLM模型不仅使用语法，而且也使用语义，并确定它们如何建立语言表达的意义。 |
| [^17] | [ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation.](http://arxiv.org/abs/2310.17389) | 本研究引入了ToxicChat，一个基于实际用户查询构建的新型毒性检测基准。该基准揭示了当前毒性检测模型在实际用户-AI对话中面临的困难，强调了实际对话中存在的独特挑战。 |
| [^18] | [Dialogue-based generation of self-driving simulation scenarios using Large Language Models.](http://arxiv.org/abs/2310.17372) | 本文介绍了一个系统，使用大型语言模型将用户的英文话语映射为领域特定代码，以支持对话式生成自动驾驶仿真场景，并探索了语言模型所能捕捉到的上下文敏感性。 |
| [^19] | [Language and Mental Health: Measures of Emotion Dynamics from Text as Linguistic Biosocial Markers.](http://arxiv.org/abs/2310.17369) | 本研究首次研究了推文情绪动态和心理健康障碍之间的关系，发现推文情绪动态与用户自我披露的诊断有关，为心理健康的评估提供了新的方法。 |
| [^20] | [Cultural Adaptation of Recipes.](http://arxiv.org/abs/2310.17353) | 本研究介绍了一项涉及中餐和英语国家菜系之间食谱的翻译和文化适应的新任务，提供了一个独特的数据集并评估了多种方法的性能。 |
| [^21] | [ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought.](http://arxiv.org/abs/2310.17342) | ACT-SQL是一种基于上下文学习的自动生成链式思维的文本到SQL技术，在文本到SQL任务中设计了类似模式链接的链式思维提示，通过自动生成示例实现了成本节省。实验结果表明ACT-SQL方法在Spider开发集上实现了最佳性能。 |
| [^22] | [Arabic Fine-Grained Entity Recognition.](http://arxiv.org/abs/2310.17333) | 本论文提出了一种阿拉伯精细化实体识别方法，通过扩展现有的阿拉伯命名实体语料库，将地缘政治实体、位置、组织和设施等四种主要实体类型扩展为31个子类型，并通过评估评注者一致性证明了扩展后的效果。 |
| [^23] | [Nabra: Syrian Arabic Dialects with Morphological Annotations.](http://arxiv.org/abs/2310.17315) | Nabra是一份包含叙利亚阿拉伯方言和形态学注释的语料库，覆盖了多种叙利亚本土方言，标注的质量优秀，开源可公开获取。 |
| [^24] | [An Ensemble Method Based on the Combination of Transformers with Convolutional Neural Networks to Detect Artificially Generated Text.](http://arxiv.org/abs/2310.17312) | 这项研究提出了一种基于Transformer和卷积神经网络的集成方法，用于检测人工生成的文本。实验结果表明，这种集成架构在分类任务上的性能超过了单个Transformer模型，而提出的SciBERT-CNN集成模型在ALTA共享任务2023数据上取得了98.36%的F1得分。 |
| [^25] | [FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language.](http://arxiv.org/abs/2310.17306) | FormaT5是一个基于转换器的模型，可以根据目标表格和自然语言描述生成数据相关的条件格式规则。为了解决描述不足的问题，FormaT5通过放弃目标的方式学习预测占位符。 |
| [^26] | [Comparing Photorealistic and Animated Embodied Conversational Agents in Serious Games: An Empirical Study on User Experience.](http://arxiv.org/abs/2310.17300) | 本文通过比较逼真和动画形象的交互式对话代理在严肃游戏中的表现，提供了关于基于语音的ECAs设计的洞察和建议。结果显示逼真和动画形象版本都具有很高的可用性，但大多数参与者更喜欢逼真版本。 |
| [^27] | [Learning to Abstract with Nonparametric Variational Information Bottleneck.](http://arxiv.org/abs/2310.17284) | 本论文提出了一种新颖的语言表示模型，可以在同一模型的不同层级上学习不同级别的抽象，并通过非参数变分信息瓶颈实现信息论压缩。这种模型具有更高级的抽象并且更具语言信息，同时对于敌对扰动具有更强的鲁棒性。 |
| [^28] | [Automatic Logical Forms improve fidelity in Table-to-Text generation.](http://arxiv.org/abs/2310.17279) | 本文提出了一种自动逻辑形式（LF）来提高表格到文本生成的准确性，首次展示了用自动LF改进系统可以提高准确性30个百分点，还指出了实现高准确性仍面临的挑战。 |
| [^29] | [Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?.](http://arxiv.org/abs/2310.17271) | 本文探索了输入单词字符对预训练语言模型性能的影响，发现即使只使用单个字符进行预训练，模型在标准任务中的性能保持较高。 |
| [^30] | [Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks.](http://arxiv.org/abs/2310.17238) | 本文提出了基于超图神经网络的联合实体和关系抽取方法，使用span剪枝机制减轻误差传播问题，通过构建超图进行高阶建模，实现多个实体和关系之间的交互。 |
| [^31] | [EMMA-X: An EM-like Multilingual Pre-training Algorithm for Cross-lingual Representation Learning.](http://arxiv.org/abs/2310.17233) | 本文提出了一种名为EMMA-X的类EM多语言预训练算法，通过利用多语言非并行数据进行学习，实现了跨语言的通用表示。该算法在一个EM框架内统一了跨语言表示学习任务和额外的语义关系预测任务，通过互相监督直到收敛的方式进行训练。 |
| [^32] | [Codebook Features: Sparse and Discrete Interpretability for Neural Networks.](http://arxiv.org/abs/2310.17230) | 本研究提出了一种称为codebook特征的方法，通过将神经网络的连续特征量化为离散向量码的总和来实现稀疏和离散的隐藏状态。实验证明，神经网络在这种极端瓶颈条件下运行时性能下降适度，同时这种方法还提供了一种直观的神经网络行为控制方式。 |
| [^33] | [TST$^\mathrm{R}$: Target Similarity Tuning Meets the Real World.](http://arxiv.org/abs/2310.17228) | 本文提出了在现实世界中应用和改进目标相似度调整（TST）的不同方法，包括使用更大的模型嵌入、训练一个小模型转换嵌入以匹配代码相似度，并介绍了高效选择训练样例和基于排名的评估方法。 |
| [^34] | [Beyond MLE: Convex Learning for Text Generation.](http://arxiv.org/abs/2310.17217) | 本论文提出了一种基于凸函数的训练目标类，超越了传统的最大似然估计方法。该方法适用于闭合型文本生成任务，并能够使得模型生成更加合适的响应。 |
| [^35] | [Efficient Data Fusion using the Tsetlin Machine.](http://arxiv.org/abs/2310.17207) | 这项研究提出了一种使用Tsetlin机器进行高效数据融合的新方法，通过监测学习到的逻辑子句在动态数据中的变化，识别和融合噪声，并在实验中展示出高性能。 |
| [^36] | [How do Language Models Bind Entities in Context?.](http://arxiv.org/abs/2310.17191) | 通过分析语言模型的表示，我们发现了绑定ID机制，它可以将实体与属性进行有效地绑定。我们通过因果干预实验进一步证明了语言模型内部激活表示绑定信息的方式。研究结果揭示了语言模型在上下文中如何表示符号知识，从而为理解大规模语言模型的一般上下文推理提供了指导。 |
| [^37] | [X-SNS: Cross-Lingual Transfer Prediction through Sub-Network Similarity.](http://arxiv.org/abs/2310.17166) | 本工作通过利用两种语言之间的子网络相似性作为预测XLT中语言兼容性的代理，提出了一种更有效的模型导向方法，不依赖于外部资源，仅需要候选语言的适量原始文本。 |
| [^38] | [Supercharging academic writing with generative AI: framework, techniques, and caveats.](http://arxiv.org/abs/2310.17143) | 这篇论文介绍了使用生成型人工智能（AI）提高学术写作质量和效率的原则和方法，包括一个人机协作框架、有效的提示技术和两阶段模型，旨在实现认知卸载和想象刺激的AI辅助写作。 |
| [^39] | [Symbolic Planning and Code Generation for Grounded Dialogue.](http://arxiv.org/abs/2310.17140) | 该论文介绍了一个模块化和可解释的基于实际对话系统，通过组合大型语言模型(LLMs)、符号化规划器和基于实际的代码执行来解决基于任务的对话中的挑战。实验证明该系统在协同参考解析任务上的性能显著优于先前的最先进技术。 |
| [^40] | [Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs.](http://arxiv.org/abs/2310.17133) | 本文提出了一种通过视觉问答对的方式将探测信号融入多模态机器翻译中，以增强跨模态交互。实验证明了该方法的有效性。 |
| [^41] | [M2C: Towards Automatic Multimodal Manga Complement.](http://arxiv.org/abs/2310.17130) | 该论文提出了一个新的研究任务——多模态漫画补充（M2C），旨在通过共享语义空间来解决手绘漫画中遗漏文字内容的问题。研究首先建立了一个涵盖两种语言的M2C基准数据集，并设计了一种名为MCoT的漫画争议方法和一种基于精细视觉提示的补充模型FVP-M$^{2}$。 |
| [^42] | [Test-time Augmentation for Factual Probing.](http://arxiv.org/abs/2310.17121) | 本文提出了一种用于事实探测的测试时间数据增强（TTA）方法，通过在测试时间自动增加和组装提示，减少关于提示变化的敏感性。实验结果显示，TTA可以提高模型的校准性，并且在某些模型中可以提高预测准确性。然而，对于其他模型，TTA可能导致性能下降，主要挑战在于产生高质量的提示变化。 |
| [^43] | [Topic Segmentation of Semi-Structured and Unstructured Conversational Datasets using Language Models.](http://arxiv.org/abs/2310.17120) | 本文通过对非结构化文本进行分析，揭示了目前主题分段模型在此类数据上的泛化能力不足，并提出了从头开始训练相对小规模的目标数据集来改善分段结果的方法。实证评估表明使用多种损失函数可以减轻非结构化对话数据集的不平衡效应。 |
| [^44] | [FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge.](http://arxiv.org/abs/2310.17119) | 本论文提出了FLEEK，一个能自动从文本中提取事实主张并使用外部知识源进行评估和修正的工具，以减少文本中的事实错误。初步实证评估显示FLEEK具有潜力。 |
| [^45] | [Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models.](http://arxiv.org/abs/2310.17086) | Transformers学会了高阶优化方法，用于上下文学习，通过实现类似于迭代牛顿法的算法，而不是梯度下降。 |
| [^46] | [math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories.](http://arxiv.org/abs/2310.17064) | 该研究调查了将大型语言模型应用于形式化高级数学概念的可行性，并提出了一个可以批判性地审查和检查研究论文中数学推理的框架。 |
| [^47] | [BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs' Generation.](http://arxiv.org/abs/2310.17054) | 本文提出了一个计算高效的框架，通过利用黑盒控制来引导冻结的预训练语言模型（PTLM）生成更加常识性的文本输出。 |
| [^48] | [On Surgical Fine-tuning for Language Encoders.](http://arxiv.org/abs/2310.17041) | 本文表明，对于不同的下游语言任务，只对语言编码器的部分层进行细调即可获得接近甚至优于细调所有层的性能。通过提出一种高效度量方法，我们证明了该方法可以选择性微调导致强大下游性能的层。研究突出表明，任务特定信息通常局部化在少数层内，只调整这些层就足够了。 |
| [^49] | [Follow-on Question Suggestion via Voice Hints for Voice Assistants.](http://arxiv.org/abs/2310.17034) | 本论文解决了在语音助手中通过声音提示为用户提供后续问题建议的新任务，并提出了一种使用序列到序列 Transformer 的方法。该方法在一个新的数据集上进行了评估，结果显示出良好的效果。 |
| [^50] | [Controlled Decoding from Language Models.](http://arxiv.org/abs/2310.17022) | 本论文提出了一种名为受控解码（CD）的离策略强化学习方法，用于控制语言模型的生成，以达到高回报的结果。CD通过前缀评分器来引导生成，可以在推理时预测预期回报，并且具有模块化设计，可用于解决多目标强化学习问题，而不增加复杂性。 |
| [^51] | [Conditionally Combining Robot Skills using Large Language Models.](http://arxiv.org/abs/2310.17019) | 这篇论文介绍了一个扩展的Meta-World基准，称为“语言世界”，允许大型语言模型在模拟机器人环境中使用自然语言查询和脚本技能。同时，引入了计划条件行为克隆（PCBC）的方法，通过端到端演示对高级计划进行微调。实验结果表明，在少样本情况下，PCBC在语言世界中能够实现强大的性能。 |
| [^52] | [An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives.](http://arxiv.org/abs/2310.17017) | 本研究使用PRISMA框架综述了计算机科学和医学领域发表的534篇论文，发现了136篇关键论文，涵盖了心理健康对话代理的多种建模和实验设计技术特征。 |
| [^53] | [This Reads Like That: Deep Learning for Interpretable Natural Language Processing.](http://arxiv.org/abs/2310.17010) | 本研究将原型网络方法扩展到了自然语言处理领域，引入了一种学习的加权相似度度量和事后可解释性机制，通过聚焦于句子嵌入的重要维度来提高相似度计算，并改善了预测性能和解释准确性。 |
| [^54] | [Quality > Quantity: Synthetic Corpora from Foundation Models for Closed-Domain Extractive Question Answering.](http://arxiv.org/abs/2310.16995) | 这项工作研究了封闭领域的抽取式问答，引入了有针对性的预训练的概念，并使用Galactica生成了合成的"有针对性"的语料库。 |
| [^55] | [How well can machine-generated texts be identified and can language models be trained to avoid identification?.](http://arxiv.org/abs/2310.16992) | 本研究针对机器生成的文本进行了识别，发现在较高温度值下，浅层学习算法的检测准确率较低，而基于转换器的分类器的准确率高。通过强化学习方法改进生成模型，可以成功躲避基于BERT的文本识别。 |
| [^56] | [STEER: Semantic Turn Extension-Expansion Recognition for Voice Assistants.](http://arxiv.org/abs/2310.16990) | STEER是一个用于语音助手的语义转向扩展识别模型，通过训练数据集和启发式规则进行转向意图预测，并在实验中展现出了良好的性能。 |
| [^57] | [Understanding Social Structures from Contemporary Literary Fiction using Character Interaction Graph -- Half Century Chronology of Influential Bengali Writers.](http://arxiv.org/abs/2310.16968) | 这项研究利用人物互动图和自然语言处理的技术来探索当代文化对文学作品中社会结构的影响。 |
| [^58] | [Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation.](http://arxiv.org/abs/2310.16964) | 本文提出了一种新的方法来减轻神经数据到文本生成中的幻觉问题，通过组合生成语言模型和特殊的文本批评家分类器的输出来指导生成过程。方法不需要对模型架构或训练过程进行改动，可与任何基于单词概率进行模型和解码操作的模型结合使用，并且不需要额外的训练数据。实验证明该方法在基准测试上优于基线。 |
| [^59] | [Zephyr: Direct Distillation of LM Alignment.](http://arxiv.org/abs/2310.16944) | 本论文提出了一种直接蒸馏的语言模型对齐方法，使用AI反馈数据进行优化，在聊天任务上显著提高了意图对齐的效果，该方法只需要几个小时的训练时间且无需人工注释，实验证明在7B参数模型上超过了现有最好的开放访问模型。 |
| [^60] | [Learning Transfers over Several Programming Languages.](http://arxiv.org/abs/2310.16937) | 这篇论文研究了使用跨语言迁移学习提高编程语言模型性能的问题，并进行了广泛实验验证。该研究表明，跨语言迁移学习在编程语言领域具有潜力，可以帮助低资源语言的用户受益于大规模语言模型。 |
| [^61] | [CL-MASR: A Continual Learning Benchmark for Multilingual ASR.](http://arxiv.org/abs/2310.16931) | CL-MASR是一个用于研究多语音ASR的持续学习基准，提供了一系列多样化的持续学习方法，并解决了学习新语言时遗忘问题。 |
| [^62] | [Physician Detection of Clinical Harm in Machine Translation: Quality Estimation Aids in Reliance and Backtranslation Identifies Critical Errors.](http://arxiv.org/abs/2310.16924) | 本文通过人类研究评估了在高风险医疗环境中使用质量评估反馈来帮助医生辨别何时依赖机器翻译输出。结果表明，质量评估能改善对机器翻译的适当依赖，但追溯翻译能帮助医生发现更严重的临床错误。 |
| [^63] | [Divide et Impera: Multi-Transformer Architectures for Complex NLP-Tasks.](http://arxiv.org/abs/2310.16897) | 本论文提出了一种多Transformer架构的方法，针对复杂NLP任务将其分解为简单的子任务，并利用多个模型进行微调，以增强任务的控制能力和性能。在减少性别偏见的任务中，实验证明这种方法优于单一模型的表现。 |
| [^64] | [DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection.](http://arxiv.org/abs/2310.16776) | 这项研究介绍了一种名为DEFT的数据高效微调框架，通过无监督核心集选择来最小化微调大规模语言模型所需的数据量。研究结果表明，DEFT模型在准确性上与现有模型相当，并且仅使用了70%的数据量。 |
| [^65] | [SkyMath: Technical Report.](http://arxiv.org/abs/2310.16713) | SkyMath是一个13亿参数的大型语言模型，通过自比较微调，它在数学推理任务中表现优秀，超过了同等规模的所有开源模型，并取得了新的SOTA最佳性能。 |
| [^66] | [DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models.](http://arxiv.org/abs/2310.16436) | DDCoT是一种职责分明的思路链刺激方法，通过负空间刺激保持批判态度，并在多模态推理中结合了关键洞见“保持批判性思考”和“让每个人发挥自己的作用”。 |
| [^67] | [Unraveling Feature Extraction Mechanisms in Neural Networks.](http://arxiv.org/abs/2310.16350) | 本研究提出了一种基于神经切线核的理论方法，研究神经网络中的特征提取机制。研究发现在梯度下降过程中模型如何利用统计特征，并揭示了激活函数选择对特征提取的影响。 |
| [^68] | [Knowledge Editing for Large Language Models: A Survey.](http://arxiv.org/abs/2310.16218) | 大型语言模型(LLMs)在学术和工业领域具有巨大潜力。本文综述了LLMs的知识编辑问题，强调了需要开发有效和高效的技术来更新预训练LLMs以纳入新知识的重要性。 |
| [^69] | [COPF: Continual Learning Human Preference through Optimal Policy Fitting.](http://arxiv.org/abs/2310.15694) | 通过COPF方法，我们不需要重新训练预训练语言模型，而是使用最优策略拟合和函数正则化来持续学习和适应人类偏好的变化。 |
| [^70] | [Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model.](http://arxiv.org/abs/2310.15113) | 本研究通过对ChatGPT在四种语言上的形态学能力进行严格分析，发现它在英语上的表现特别不理想，远远达不到专门构建系统的水平。 |
| [^71] | [Can Language Models Laugh at YouTube Short-form Videos?.](http://arxiv.org/abs/2310.14159) | 本研究在用户生成数据集中筛选并注释了10K个YouTube上的有趣多模态视频，借助GPT-3.5验证了语言和视觉元素对幽默的贡献。此外，还开发了一种零-shot视频到文本提示方法，用于大型语言模型对视频幽默的理解。这个研究填补了现有数据集中对多领域多模态幽默的不足。 |
| [^72] | [Simultaneous Machine Translation with Tailored Reference.](http://arxiv.org/abs/2310.13588) | 本文提出了一种使用定制参考文献的方法，以解决在同时机器翻译模型训练过程中存在的问题。通过重述真实参考文献，定制参考文献可以在不同延迟下训练SiMT模型，避免了强制预测并保持高质量。 |
| [^73] | [Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding.](http://arxiv.org/abs/2310.11191) | 本研究探索了进一步提高医学领域文本简化可读性的方法，包括一种新的非典型损失和一种优化简单性的重新排序解码方法，取得了更好的性能。 |
| [^74] | [MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models.](http://arxiv.org/abs/2310.02255) | 本论文提出了MathVista，这是一个评估视觉场景中数学推理能力的基准测试。通过对12个著名的基础模型进行全面的定量评估，发现最好的GPT-4V模型相对于第二名的Bard模型在准确率上提升了15.1%。 |
| [^75] | [Multilingual Natural Language ProcessingModel for Radiology Reports -- The Summary is all you need!.](http://arxiv.org/abs/2310.00100) | 本研究通过在多语言文本到文本变换器模型上微调，开发了一个能够自动在多语言中总结放射学报告的模型。该模型有助于提高未来深度学习模型的研究和发展，且能够应用于不同族裔背景的患者数据。 |
| [^76] | [What are Public Concerns about ChatGPT? A Novel Self-Supervised Neural Topic Model Tells You.](http://arxiv.org/abs/2309.01522) | 本研究提出了一种新颖的自监督神经主题模型（SSTM），通过对ChatGPT的社交媒体帖子和用户查询进行实验，成功提取出了具有更高质量、更好可解释性和多样性的公众关切。 |
| [^77] | [Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior.](http://arxiv.org/abs/2309.00359) | 该论文提出了使用大型内容和行为模型来理解、模拟和优化内容和行为。大型语言模型虽然在任务泛化能力方面取得了进展，但还无法解决预测和优化通信以实现期望接收者行为的问题。其中的一个原因可能是训练语料库中缺少"行为标记"。 |
| [^78] | [Lexical Diversity in Kinship Across Languages and Dialects.](http://arxiv.org/abs/2308.13056) | 本文研究了跨语言和方言中的亲属关系词汇的词汇多样性，并提出了一种方法来丰富计算词汇资源。通过大规模的案例研究，我们验证了该方法，并提供了可供浏览和下载的计算资源，扩展了对亲属关系术语的语言学研究，揭示了在语言和文化上相互接近的社区中多样性的程度。 |
| [^79] | [A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue Information Extraction.](http://arxiv.org/abs/2307.16200) | 本论文提出了一个知识增强的两阶段生成框架（KTGF）用于医学对话信息提取。通过两个阶段的生成，分别生成医学对话中的术语和每个术语的状态，从而更好地建模术语之间的关系。 |
| [^80] | [AlpaGasus: Training A Better Alpaca with Fewer Data.](http://arxiv.org/abs/2307.08701) | 这项研究提出了一种用于训练语言模型的数据筛选策略AlpaGasus，通过使用强大的语言模型过滤掉低质量数据，它在测试中表现出比原始模型更好的性能，并提供了更快的训练速度。 |
| [^81] | [Large Language Models as General Pattern Machines.](http://arxiv.org/abs/2307.04721) | 预训练的大型语言模型（LLMs）展现出了强大的序列模式完成能力，即使在随机样本的情况下也能保持一定的准确性。这些模型可以用于机器人领域的问题，如动态状态序列预测和自主路径规划。 |
| [^82] | [CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care.](http://arxiv.org/abs/2307.01458) | CARE-MI是一个用于评估中国孕婴护理领域LLM虚假信息的基准，填补了这一领域的研究空白，并提供了构建长篇生成评估基准的创新范式。 |
| [^83] | [DocumentNet: Bridging the Data Gap in Document Pre-Training.](http://arxiv.org/abs/2306.08937) | 这项研究提出了DocumentNet方法，通过从Web上收集大规模和弱标注的数据，弥合了文档预训练中的数据差距，并在各类VDER任务中展现了显著的性能提升。 |
| [^84] | [Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations.](http://arxiv.org/abs/2306.04618) | 本文提出了一个具有挑战性的基准协议，用于评估自然语言处理中的领域外鲁棒性。通过使用这个基准套件，作者们发现OOD与ID性能之间的关系并不总是一致的，并引入了一种名为LLMs的新方法，可以在多个任务上显著提高OOD鲁棒性。 |
| [^85] | [Scaling Data-Constrained Language Models.](http://arxiv.org/abs/2305.16264) | 研究人员研究了在数据受限制的情况下缩放语言模型，并提出了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。 |
| [^86] | [Bhasha-Abhijnaanam: Native-script and romanized Language Identification for 22 Indic languages.](http://arxiv.org/abs/2305.15814) | 该研究提供了22种印度宪法中列出的所有21种本土文字和罗马字母的公开语言鉴别（LID）数据和模型。IndicLID是上述语言的本土和罗马化脚本的语言鉴别器，还提出了解决罗马化文本的LID问题的方案。 |
| [^87] | [Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models.](http://arxiv.org/abs/2305.15080) | 本文介绍了一种名为对比阅读模型（Cream）的新型神经架构，旨在通过捕捉复杂细节，提升大型语言模型在语言-图像理解能力方面的性能。该方法通过视觉和辅助编码器及对比特征对齐技术实现了对视觉定位上下文中语言信息的更有效理解。 |
| [^88] | [AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models.](http://arxiv.org/abs/2305.15064) | 本文提出了一种名为AutoPlan的方法，利用大型语言模型（LLM）来指导代理完成复杂的交互式决策任务。通过将LLM提示与任务解决计划相结合并进行优化，AutoPlan在没有上下文演示的情况下，在ALFWorld上实现了与人类撰写的演示基线相当的成功率，并在HotpotQA上超过了8%。 |
| [^89] | [Editing Common Sense in Transformers.](http://arxiv.org/abs/2305.14956) | 本文探究了常识判断是否与Transformer模型中的可编辑参数存在因果关联，并通过改进编辑算法和层选择策略，在常识领域中提升了编辑效果，使得经过编辑的GPT-2模型在各测试集上的F1分数相较于最佳微调基线提高了10.97%和10.73%。 |
| [^90] | [Coverage-based Example Selection for In-Context Learning.](http://arxiv.org/abs/2305.14907) | 本研究提出了一种基于覆盖率的上下文学习中示例选择方法，通过使用BERTScore-Recall度量方法选择更好的示例来展示测试输入的关键方面，同时还通过扩展成集合级别度量方法进一步提高了覆盖率表现。实验证明BSR是上下文示例选择中优越的度量方法，并且对于组合任务，使用Set-BSR进行集合选择可以显著提高性能。 |
| [^91] | [Estimating Large Language Model Capabilities without Labeled Test Data.](http://arxiv.org/abs/2305.14802) | 本文提出了一种在无标签测试数据下估计大型语言模型能力的方法，通过使用LLM置信度分数训练元模型来执行上下文学习准确性估计任务，并在相关基准测试中取得了优于其他基线的成果。 |
| [^92] | [Analyzing Influential Factors in Human Preference Judgments via GPT-4.](http://arxiv.org/abs/2305.14702) | 本文利用Bradley-Terry-Luce模型对OpenAI公布的人类偏好判断数据集进行了深入研究，揭示了人类偏好判断中所蕴含的固有偏好，提出了提高样本效率的策略，并为构建平衡的人类偏好判断数据集提供了洞见。 |
| [^93] | [What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on QA Systems.](http://arxiv.org/abs/2305.14331) | 本研究调查了用户在评估模型预测时缺乏足够信息时与QA系统的交互方式，并发现即使缺乏这些信息，用户仍然过度依赖于模型的预测。然而，提供相关背景信息有助于减少对错误预测的依赖。 |
| [^94] | [Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation.](http://arxiv.org/abs/2305.14327) | Dynosaur提出了一种动态增长模式，用于自动整理指令调优数据。通过利用现有的注释数据集，Dynosaur能够以较低的API成本提供高质量的指令调优数据。 |
| [^95] | [Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document.](http://arxiv.org/abs/2305.13850) | 这篇论文提出了一种结合全局结构知识的连续迭代的方式去捕获实体之间的依赖关系，以提高视觉丰富文档中关系抽取的准确性。 |
| [^96] | [Detecting and Mitigating Hallucinations in Multilingual Summarisation.](http://arxiv.org/abs/2305.13632) | 本文提出一种新的度量方法mFACT，可以在非英语摘要中评估其忠实性。本文还提出了一种简单有效的加权方法，可以通过跨语言转移减少摘要的幻觉问题。 |
| [^97] | [Multimodal Automated Fact-Checking: A Survey.](http://arxiv.org/abs/2305.13507) | 本调查提出了一个多模态自动事实核查的框架，并包括了独特的子任务，重点关注了文本，图像，音频和视频这四种模态的现实应用。纪录了相关的基准模型，讨论了未来研究的局限性和前景。 |
| [^98] | [CLASS: A Design Framework for building Intelligent Tutoring Systems based on Learning Science principles.](http://arxiv.org/abs/2305.13272) | 提出了一个名为CLASS的设计框架，用于构建基于学习科学原理的智能辅导系统（ITS），该框架通过提供关键能力使ITS能够提供逐步指导和促进自然语言交互，从而提高学生的学习效果。 |
| [^99] | [ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination.](http://arxiv.org/abs/2305.12945) | ExplainCPE是一个具有挑战性的简体中文医学基准，用于解决大型语言模型的可解释性能力问题。该基准分析了ChatGPT和GPT-4的错误，并指出了当前模型在理解文本和计算推理方面的局限性。 |
| [^100] | [Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation.](http://arxiv.org/abs/2305.11685) | 本研究提出了一种基于Transformer的语音自监督学习模型的通用压缩策略，通过重用注意力映射和蒸馏屏蔽来提高学生模型的语音表示质量，实现了较低的错误率。 |
| [^101] | [Evaluating Object Hallucination in Large Vision-Language Models.](http://arxiv.org/abs/2305.10355) | 本研究是对大型视觉-语言模型中的物体幻觉问题进行的第一项系统研究，通过研究发现视觉指令可能影响幻觉，提出新的评估指标成功解决了现有评估方法的不足。 |
| [^102] | [Asymmetric feature interaction for interpreting model predictions.](http://arxiv.org/abs/2305.07224) | 本文提出了一种解释模型，能够探索深度神经自然语言处理模型推理中的非对称高阶特征交互。在两个情感分类数据集上的实验结果表明，该模型在识别影响特征方面优于现有特征交互归因方法。 |
| [^103] | [Multi-grained Hypergraph Interest Modeling for Conversational Recommendation.](http://arxiv.org/abs/2305.04798) | 本文提出了一种多粒度超图兴趣建模方法，通过利用历史对话数据丰富当前对话的上下文，从不同角度捕捉用户兴趣。采用超图结构表示复杂的语义关系，建模用户的历史对话会话，捕捉粗粒度的会话级关系。 |
| [^104] | [NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports.](http://arxiv.org/abs/2305.03598) | 本文提出了一种面向临床试验报告的自然语言推理模型，通过检索支持事实来确定自然语言陈述和CTR之间的推理关系。 |
| [^105] | [Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models.](http://arxiv.org/abs/2305.02531) | 本研究分析了大型语言模型在不同语言提示下的奖励时间偏好，并发现GPT在具有较弱未来时态的语言下表现出更大的耐心，这与使用该语言的人类的偏好相似。 |
| [^106] | [Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding.](http://arxiv.org/abs/2305.00633) | 本论文提出了一种通过自我评估引导解码提高推理的方法，使用经过校准的自动标准探索推理搜索空间，使搜索能够产生更高质量的最终预测结果；使用自我评估引导的随机束搜索在产生推理链的质量和多样性之间平衡权衡，适应多数投票，并且可以准确判断逻辑错误，提高一致性和鲁棒性。 |
| [^107] | [Is ChatGPT A Good Keyphrase Generator? A Preliminary Study.](http://arxiv.org/abs/2303.13001) | 本文对ChatGPT作为关键词生成器进行了初步研究，发现其在各个方面的性能表现良好，特别是在多领域关键词生成方面。ChatGPT仍面临生成缺失关键词的挑战。 |
| [^108] | [Describe me an Aucklet: Generating Grounded Perceptual Category Descriptions.](http://arxiv.org/abs/2303.04053) | 本文介绍了一个用于测试多模态语言模型中基于类别的感知基础的框架，并比较了基于原型和样本的表示的性能。 |
| [^109] | [Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals.](http://arxiv.org/abs/2302.04449) | 本论文提出了阅读并奖励的框架，通过阅读Atari游戏开发者发布的指导手册，以提高强化学习算法在Atari游戏中的效率。该框架包含一个QA提取模块和一个推理模块，能够从指导手册中提取关键信息，并评估物体与智能体的交互效果。 |
| [^110] | [ZipLM: Inference-Aware Structured Pruning of Language Models.](http://arxiv.org/abs/2302.04089) | ZipLM是一种新型的语言模型压缩方法，能够在任何给定的推理环境中实现与目标运行速度相匹配的最先进压缩模型。与现有方法相比，ZipLM在速度和准确性之间取得了最佳的权衡，并且以更低的计算成本获得了更好的结果。 |
| [^111] | [Event knowledge in large language models: the gap between the impossible and the unlikely.](http://arxiv.org/abs/2212.01488) | 大型语言模型拥有丰富的事件知识，几乎总是将可能事件的描述比不可能事件的描述赋予更高的可能性。 |
| [^112] | [Aksharantar: Open Indic-language Transliteration datasets and models for the Next Billion Users.](http://arxiv.org/abs/2205.03018) | Aksharantar是最大的公开可用的印度语言转写数据集，包含2600万个转写对，涵盖21种印度语言和12种文字。同时，它也提供了一个测试集，用于对转写模型进行细粒度分析。利用该数据集，我们训练了一个多语言转写模型IndicXlit，取得了较好的性能。 |
| [^113] | [pysentimiento: A Python Toolkit for Opinion Mining and Social NLP tasks.](http://arxiv.org/abs/2106.09462) | pysentimiento是一个多语言的Python工具包，用于观点挖掘和社交自然语言处理任务，提供了易于使用的库和最先进的模型，研究人员可以利用这些技术进行研究。 |
| [^114] | [On Classifying Continuous Constraint Satisfaction Problems.](http://arxiv.org/abs/2106.02397) | 本论文研究了连续约束满足问题的分类，重点关注具有加法约束和其他技术条件的问题，并探讨了其与存在性实数理论的关系。 |
| [^115] | [Investigating Antigram Behaviour using Distributional Semantics.](http://arxiv.org/abs/1901.05066) | 本文通过使用分布语义研究探究反词行为，提出了一种基于规则的算法用于检测反词。在小数据集上取得了39%的准确率，展示了该领域仍有待进一步研究。 |

# 详细

[^1]: 实现手机与语音表示的匹配

    Towards Matching Phones and Speech Representations. (arXiv:2310.17558v1 [cs.CL])

    [http://arxiv.org/abs/2310.17558](http://arxiv.org/abs/2310.17558)

    本研究探讨了学习手机类型的问题，并将其作为匹配聚类中心与手机嵌入的问题。通过生成伪标签和引入新的损失函数，我们改进了自监督表示，实验证明匹配结果捕捉到了手机之间的关系，并显著提高了手机分类的性能。

    

    学习手机类型从手机实例一直是一个长期存在但仍然尚未解决的问题。在本研究中，我们在自监督学习的背景下重新审视了这个问题，并将其提出为将聚类中心与手机嵌入匹配的问题。我们研究了两个关键属性，它们使匹配成为可能，即自监督表示的聚类中心是否减少了手机实例的变化性并且是否尊重手机之间的关系。然后，我们使用匹配结果生成伪标签，并引入了一种新的损失函数来改进自监督表示。我们的实验表明，匹配结果捕捉到了手机之间的关系。将新的损失函数与APC和CPC等常规的自监督损失一起训练，显著提高了下游手机分类的性能。

    Learning phone types from phone instances has been a long-standing problem, while still being open. In this work, we revisit this problem in the context of self-supervised learning, and pose it as the problem of matching cluster centroids to phone embeddings. We study two key properties that enable matching, namely, whether cluster centroids of self-supervised representations reduce the variability of phone instances and respect the relationship among phones. We then use the matching result to produce pseudo-labels and introduce a new loss function for improving self-supervised representations. Our experiments show that the matching result captures the relationship among phones. Training the new loss function jointly with the regular self-supervised losses, such as APC and CPC, significantly improves the downstream phone classification.
    
[^2]: 解读大模型中的伦理价值对齐问题

    Unpacking the Ethical Value Alignment in Big Models. (arXiv:2310.17551v1 [cs.CY])

    [http://arxiv.org/abs/2310.17551](http://arxiv.org/abs/2310.17551)

    本文探讨了大型模型中的伦理价值对齐问题，调查了现有的人工智能伦理准则，并提出了建立统一和普遍的人工智能伦理框架的重要性。此外，对当前主流大型语言模型的伦理倾向进行了研究，并分析了在实现伦理价值对齐时的挑战。

    

    大型模型极大地提升了人工智能理解、生成和操作信息与内容的能力，并被广泛应用于各个领域。然而，随着这些模型越来越多地融入日常生活，其固有的伦理价值观和潜在偏见给社会带来了未预料的风险。本文概述了与大型模型有关的风险和挑战，调查了现有的人工智能伦理准则，并分析了这些模型的局限性所引发的伦理影响。从规范伦理的角度出发，我们提出了对最近的规范准则进行重新评估的建议，强调了学术界在建立统一和普遍的人工智能伦理框架方面的合作努力的重要性。此外，我们使用道德基础理论调查了当前主流大型语言模型的道德倾向，分析了现有的价值观对齐算法，并概述了在实现伦理价值对齐时遇到的独特挑战。

    Big models have greatly advanced AI's ability to understand, generate, and manipulate information and content, enabling numerous applications. However, as these models become increasingly integrated into everyday life, their inherent ethical values and potential biases pose unforeseen risks to society. This paper provides an overview of the risks and challenges associated with big models, surveys existing AI ethics guidelines, and examines the ethical implications arising from the limitations of these models. Taking a normative ethics perspective, we propose a reassessment of recent normative guidelines, highlighting the importance of collaborative efforts in academia to establish a unified and universal AI ethics framework. Furthermore, we investigate the moral inclinations of current mainstream LLMs using the Moral Foundation theory, analyze existing alignment algorithms, and outline the unique challenges encountered in aligning ethical values within them. To address these challenges
    
[^3]: 评估性别中立的预训练视觉-语言模型中的偏见和公平性

    Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models. (arXiv:2310.17530v1 [cs.CV])

    [http://arxiv.org/abs/2310.17530](http://arxiv.org/abs/2310.17530)

    本研究评估了性别中立的预训练视觉-语言模型中的偏见和公平性，并发现预训练和微调后的偏见放大是相互独立的。此外，持续预训练对性别中立数据有利，可以在一些任务中促进公平。

    

    已知预训练的机器学习模型会保持甚至放大数据中现有的偏见，这可能导致不公平的结果，最终影响用户体验。为了确保模型的性能不会对特定群体或人口产生歧视性行为，理解这些偏见偏向的机制至关重要。在本研究中，我们将性别偏见作为案例进行定义。我们量化了三个视觉-语言模型族群的预训练和微调后的偏见放大，并调查了这两个学习阶段之间的联系，评估了偏见放大对模型性能的影响。总的来说，我们发现预训练和微调后的偏见放大是相互独立的。然后，我们研究了在性别中立的数据上进行持续预训练的效果，发现这可以减少群体差距，即在VQAv2和检索任务中促进公平，而不会显著损害任务执行能力。

    Pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience. Therefore, it is crucial to understand the mechanisms behind those prejudicial biases to ensure that model performance does not result in discriminatory behaviour toward certain groups or populations. In this work, we define gender bias as our case study. We quantify bias amplification in pretraining and after fine-tuning on three families of vision-and-language models. We investigate the connection, if any, between the two learning stages, and evaluate how bias amplification reflects on model performance. Overall, we find that bias amplification in pretraining and after fine-tuning are independent. We then examine the effect of continued pretraining on gender-neutral data, finding that this reduces group disparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without significantly compromising task p
    
[^4]: 大型语言模型能否取代人类在系统评价过程中的角色？评估GPT-4在多种语言的同行评审文献和灰色文献筛选和提取数据方面的效果。

    Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. (arXiv:2310.17526v1 [cs.CL])

    [http://arxiv.org/abs/2310.17526](http://arxiv.org/abs/2310.17526)

    本研究评估了GPT-4在多种语言的同行评审文献和灰色文献筛选和提取数据方面的能力，结果显示GPT-4在大多数任务上准确度与人类表现相当，在调整了偶然一致性和数据集不平衡后，其在数据提取方面表现出中等水平的准确度。

    

    系统评价对于指导实践、研究和政策至关重要，然而常常需要耗费大量时间和人力。大型语言模型（LLM）可能能够加快和自动化系统评价的过程，但是它们在这些任务中的表现尚未经过全面评估，而且还没有研究测试过迄今为止最大的LLM——GPT-4。本预注册研究采用“无人参与”的方法评估了GPT-4在标题/摘要筛选、全文审查和数据提取方面在不同文献类型和语言上的能力。尽管GPT-4在大多数任务中的准确度与人类表现相当，但结果受到偶然一致性和数据集不平衡的影响。在调整了这些因素后，数据提取方面表现出中等水平的准确度，在使用高可靠性提示进行筛选的研究中，筛选全文文献的表现水平在不同阶段和语言上均为无到中等。

    Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance. After adjusting for these, there was a moderate level of performance for data extraction, and - barring studies that used highly reliable prompts screening performance levelled at none to moderate for different stages and languages. When screening full-text literature using highly reliable prom
    
[^5]: 评估结果的有效性：评估组合性基准的一致性

    The Validity of Evaluation Results: Assessing Concurrence Across Compositionality Benchmarks. (arXiv:2310.17514v1 [cs.CL])

    [http://arxiv.org/abs/2310.17514](http://arxiv.org/abs/2310.17514)

    该研究调查了在组合泛化领域中特定数据集设计选择对模型能力结论的影响，发现不同数据集对建模方法的排名存在差异，人类生成的数据集之间的一致性高于合成数据集之间的一致性，数据集是否来自同一来源更能预测结果的模型排名。

    

    最近几年，根据提出的许多数据集，NLP模型取得了巨大的进步。然而，关于特定数据集设计选择如何影响我们对模型能力的结论仍然存在疑问。在本研究中，我们调查了组合泛化领域中这个问题。我们在四个数据集上对六种建模方法进行了性能评估，并根据八种组合性分割策略将模型按照18个组合泛化拆分进行排名。我们的结果表明：i）尽管所有数据集都设计用于评估组合泛化，但它们对建模方法的排名有所不同；ii）由人类生成的数据集彼此之间的一致性比它们与合成数据集的一致性更高，或者比合成数据集之间的一致性更高；iii）通常情况下，数据集是否来自同一来源更能预测结果的模型排名，而不是它们是否保持相同的组合解释。

    NLP models have progressed drastically in recent years, according to numerous datasets proposed to evaluate performance. Questions remain, however, about how particular dataset design choices may impact the conclusions we draw about model capabilities. In this work, we investigate this question in the domain of compositional generalization. We examine the performance of six modeling approaches across 4 datasets, split according to 8 compositional splitting strategies, ranking models by 18 compositional generalization splits in total. Our results show that: i) the datasets, although all designed to evaluate compositional generalization, rank modeling approaches differently; ii) datasets generated by humans align better with each other than they with synthetic datasets, or than synthetic datasets among themselves; iii) generally, whether datasets are sampled from the same source is more predictive of the resulting model ranking than whether they maintain the same interpretation of compos
    
[^6]: 《低秩适应的表达能力》

    The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])

    [http://arxiv.org/abs/2310.17513](http://arxiv.org/abs/2310.17513)

    本文分析了低秩适应（LoRA）的表达能力，证明了对于全连接神经网络，当LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度）时，LoRA可以使任何模型f准确表示任何较小的目标模型f。对于Transformer网络，通过rank-（嵌入大小/ 2）的LoRA适配器可以使任何模型适应于相同大小的目标模型。

    

    低秩适应（LoRA）是一种参数高效的微调方法，利用矩阵的低秩适应性，在微调预训练模型（如大型语言模型和扩散模型）中得到了广泛应用。尽管在实践中取得了巨大成功，但是LoRA的理论基础在很大程度上尚未得到探索。本文通过从理论角度分析LoRA的表达能力，首次尝试弥合这一差距。我们证明了对于全连接神经网络，如果LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度），则LoRA可以使任何模型f准确表示任何较小的目标模型f。当LoRA-rank低于阈值时，我们还量化了逼近误差。对于Transformer网络，我们证明任何模型可以通过rank-（嵌入大小/ 2）的LoRA适配器适应于相同大小的目标模型。

    Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
    
[^7]: CompeteAI:理解基于大型语言模型的智能体竞争行为

    CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents. (arXiv:2310.17512v1 [cs.AI])

    [http://arxiv.org/abs/2310.17512](http://arxiv.org/abs/2310.17512)

    本文研究了基于大型语言模型的智能体之间的竞争行为。通过建立一个竞争环境并进行实验，发现竞争可以促使智能体进行转变和采取新策略，从而在社会和经济发展中发挥重要作用。

    

    大型语言模型（LLM）被广泛应用于完成不同任务，如个人助理或事件规划。虽然大多数工作都集中在智能体之间的合作与协作，但很少有研究探索另一个重要机制——竞争，它是社会和经济发展的推动力之一。本文旨在研究LLM智能体之间的竞争行为。我们首先提出一个通用框架来研究智能体之间的竞争。然后，我们使用GPT-4实现了一个实际的竞争环境，模拟了一个由餐馆智能体和顾客智能体组成的虚拟城镇。具体而言，餐馆智能体相互竞争以吸引更多顾客，这种竞争促使它们进行转变，比如培养新的运营策略。我们实验的结果揭示了从社会学习到马太效应等多个有趣发现，与现有的社会学和经济学理论相一致。

    Large language models (LLMs) have been widely used as agents to complete different tasks, such as personal assistance or event planning. While most work has focused on cooperation and collaboration between agents, little work explores competition, another important mechanism that fosters the development of society and economy. In this paper, we seek to examine the competition behaviors in LLM-based agents. We first propose a general framework to study the competition between agents. Then, we implement a practical competitive environment using GPT-4 to simulate a virtual town with two types of agents, including restaurant agents and customer agents. Specifically, restaurant agents compete with each other to attract more customers, where the competition fosters them to transform, such as cultivating new operating strategies. The results of our experiments reveal several interesting findings ranging from social learning to Matthew Effect, which aligns well with existing sociological and e
    
[^8]: IMS Toucan系统用于Blizzard Challenge 2023

    The IMS Toucan System for the Blizzard Challenge 2023. (arXiv:2310.17499v1 [cs.CL])

    [http://arxiv.org/abs/2310.17499](http://arxiv.org/abs/2310.17499)

    我们改进了我们在Blizzard Challenge 2021中提交的系统，使用一个基于规则的文本到音素处理系统，并且设计了数据处理、训练和推理过程来处理Blizzard Challenge 2023的数据。我们的系统标识符是G。

    

    为了参加Blizzard Challenge 2023，我们改进了在Blizzard Challenge 2021中提交的系统。我们的方法是一个基于规则的文本到音素处理系统，包括对法语中的同音异形词进行基于规则的消歧。然后，我们使用基于Conformer和Glow的快速高效的非自回归合成架构将音素转换为中间表示 - 频谱图。一个基于GAN的神经声码器结合了最新的先进方法，将频谱图转换为最终的波形。我们精心设计了用于挑战数据的数据处理、训练和推理过程。我们的系统标识符是G。提供开源代码和演示。

    For our contribution to the Blizzard Challenge 2023, we improved on the system we submitted to the Blizzard Challenge 2021. Our approach entails a rule-based text-to-phoneme processing system that includes rule-based disambiguation of homographs in the French language. It then transforms the phonemes to spectrograms as intermediate representations using a fast and efficient non-autoregressive synthesis architecture based on Conformer and Glow. A GAN based neural vocoder that combines recent state-of-the-art approaches converts the spectrogram to the final wave. We carefully designed the data processing, training, and inference procedures for the challenge data. Our system identifier is G. Open source code and demo are available.
    
[^9]: 提高通过减少无关文档对开放领域问答中的零样本阅读器的干扰的方法

    Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering. (arXiv:2310.17490v1 [cs.CL])

    [http://arxiv.org/abs/2310.17490](http://arxiv.org/abs/2310.17490)

    本研究提出了一种通过减少无关文档的干扰来改善开放领域问答中的零样本阅读器的方法。采用了干扰感知的答案选择(DAS)方法，以解决LLMs受到干扰和过度自信的问题。实验结果表明，该方法成功地改善了零样本阅读器的性能，并展现出了优越的可迁移性。

    

    大型语言模型(LLMs)使得在开放领域问答(ODQA)中实现零样本方法成为可能，但是由于阅读器相对于检索器的进展有限。本研究旨在探讨一种零样本阅读器的可行性，以解决计算成本和标注数据需求等挑战。我们发现LLMs由于检索到的无关文档以及作为零样本阅读器时生成答案的过度自信而受到干扰。为了解决这些问题，我们采用了基于否定的指令和分数调整的干扰感知的答案选择(DAS)方法，以减轻这些文档的影响。实验结果表明，我们的方法成功地处理了不同场景下的干扰，提高了零样本阅读器的性能。此外，与面对未见过数据而困难重重的监督式阅读器不同，零样本阅读器展现出了优越的可迁移性，无需任何训练。

    Large language models (LLMs) enable zero-shot approaches in open-domain question answering (ODQA), yet with limited advancements as the reader is compared to the retriever. This study aims at the feasibility of a zero-shot reader that addresses the challenges of computational cost and the need for labeled data. We find that LLMs are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers. To tackle these problems, we mitigate the impact of such documents via Distraction-aware Answer Selection (DAS) with a negation-based instruction and score adjustment for proper answer selection. Experimental results show that our approach successfully handles distraction across diverse scenarios, enhancing the performance of zero-shot readers. Furthermore, unlike supervised readers struggling with unseen data, zero-shot readers demonstrate outstanding transferability without any training.
    
[^10]: LightLM: 一种轻量级的基于Transformer的生成推荐模型

    LightLM: A Lightweight Deep and Narrow Language Model for Generative Recommendation. (arXiv:2310.17488v1 [cs.IR])

    [http://arxiv.org/abs/2310.17488](http://arxiv.org/abs/2310.17488)

    LightLM是一种轻量级的基于Transformer的生成推荐模型，通过引入轻量级深窄Transformer架构来实现直接生成推荐项。

    

    本文介绍了LightLM，一种轻量级的基于Transformer的生成推荐模型。在NLP和视觉等各个人工智能子领域中，基于Transformer的生成建模已经变得越来越重要，而生成推荐由于其对个性化生成建模的独特需求，仍处于初级阶段。现有的生成推荐方法通常使用面向NLP的Transformer架构，如T5、GPT、LLaMA和M6，这些模型比较庞大，且并没有专门针对推荐任务进行设计。LightLM通过引入轻量级深窄Transformer架构来解决这个问题，该架构特别适用于直接生成推荐项。这种结构对于直接的生成推荐非常合适，因为输入主要由适合模型容量的短标记组成，语言模型在这个任务上不需要太宽的结构。我们还...

    This paper presents LightLM, a lightweight Transformer-based language model for generative recommendation. While Transformer-based generative modeling has gained importance in various AI sub-fields such as NLP and vision, generative recommendation is still in its infancy due to its unique demand on personalized generative modeling. Existing works on generative recommendation often use NLP-oriented Transformer architectures such as T5, GPT, LLaMA and M6, which are heavy-weight and are not specifically designed for recommendation tasks. LightLM tackles the issue by introducing a light-weight deep and narrow Transformer architecture, which is specifically tailored for direct generation of recommendation items. This structure is especially apt for straightforward generative recommendation and stems from the observation that language model does not have to be too wide for this task, as the input predominantly consists of short tokens that are well-suited for the model's capacity. We also sh
    
[^11]: 《方言适应与数据增强对于资源匮乏 ASR 的影响：TalTech 系统在 MADASR 2023 挑战中的表现》

    Dialect Adaptation and Data Augmentation for Low-Resource ASR: TalTech Systems for the MADASR 2023 Challenge. (arXiv:2310.17448v1 [cs.CL])

    [http://arxiv.org/abs/2310.17448](http://arxiv.org/abs/2310.17448)

    TalTech使用了方言适应和数据增强的方法，并分别在提供的训练数据和额外音频数据上取得了显著的改进。实验结果表明，该方法在低资源ASR场景中取得了最低的词错误率。

    

    本文介绍了TalTech开发的ASRU MADASR 2023挑战中，针对方言丰富的印度语言的自动语音识别系统。挑战主要关注有限的训练音频和文本数据。TalTech参与了挑战的两个任务：第一个任务只能使用提供的训练数据，第三个任务可以使用额外的音频数据。在这两个任务中，我们使用了wav2vec2.0模型。我们的方法与传统的finetuning预训练的wav2vec2.0模型的过程有两个关键点的差异：首先，通过实施对齐数据增强技术来增加训练数据的语言多样性；其次，通过深层前缀调整将wav2vec2.0模型进行方言适应。在这两个任务中，我们的方法显著改进了提供的基准线，达到了参与团队中最低的词错误率。

    This paper describes Tallinn University of Technology (TalTech) systems developed for the ASRU MADASR 2023 Challenge. The challenge focuses on automatic speech recognition of dialect-rich Indian languages with limited training audio and text data. TalTech participated in two tracks of the challenge: Track 1 that allowed using only the provided training data and Track 3 which allowed using additional audio data. In both tracks, we relied on wav2vec2.0 models. Our methodology diverges from the traditional procedure of finetuning pretrained wav2vec2.0 models in two key points: firstly, through the implementation of the aligned data augmentation technique to enhance the linguistic diversity of the training data, and secondly, via the application of deep prefix tuning for dialect adaptation of wav2vec2.0 models. In both tracks, our approach yielded significant improvements over the provided baselines, achieving the lowest word error rates across all participating teams.
    
[^12]: "五十种偏见": GPT生成的英文文本中的性别偏见的规范评级

    ''Fifty Shades of Bias'': Normative Ratings of Gender Bias in GPT Generated English Text. (arXiv:2310.17428v1 [cs.CL])

    [http://arxiv.org/abs/2310.17428](http://arxiv.org/abs/2310.17428)

    本研究通过创建包含规范评级的GPT生成的英文文本数据集，系统地分析了观察到的性别偏见主题的变化及其对偏见的感知。通过认识到偏见必须以相对的尺度来感知，本研究对不同程度偏见的接受能力进行了调查，对性别偏见的研究提供了新的视角。

    

    语言是社会信仰体系表现的强大工具。在这样做的过程中，它也使我们社会中普遍存在的偏见得以延续。性别偏见是我们社会中最普遍的偏见之一，在在线和离线讨论中都能看到。随着LLMs在文本生成中越来越像人类一样流利，了解这些系统可能产生的偏见的细微差别变得至关重要。以前的研究通常将性别偏见视为一个二元分类任务。然而，我们认识到偏见必须以相对的尺度来感知；我们调查了偏见的生成和随后的手动标注者对不同程度偏见的接受能力。具体而言，我们创建了第一个具有规范评级的GPT生成的英文文本数据集。使用Best--Worst Scaling进行评级获取--这是一个高效的比较标注框架。接下来，我们系统地分析了观察排名中性别偏见主题的变化，并展示了身份在其中的作用。

    Language serves as a powerful tool for the manifestation of societal belief systems. In doing so, it also perpetuates the prevalent biases in our society. Gender bias is one of the most pervasive biases in our society and is seen in online and offline discourses. With LLMs increasingly gaining human-like fluency in text generation, gaining a nuanced understanding of the biases these systems can generate is imperative. Prior work often treats gender bias as a binary classification task. However, acknowledging that bias must be perceived at a relative scale; we investigate the generation and consequent receptivity of manual annotators to bias of varying degrees. Specifically, we create the first dataset of GPT-generated English text with normative ratings of gender bias. Ratings were obtained using Best--Worst Scaling -- an efficient comparative annotation framework. Next, we systematically analyze the variation of themes of gender biases in the observed ranking and show that identity-at
    
[^13]: PETA: 评估亚词切分对蛋白质迁移学习在下游应用中的影响

    PETA: Evaluating the Impact of Protein Transfer Learning with Sub-word Tokenization on Downstream Applications. (arXiv:2310.17415v1 [cs.CL])

    [http://arxiv.org/abs/2310.17415](http://arxiv.org/abs/2310.17415)

    PETA通过评估亚词切分在蛋白质迁移学习中的影响，提出了关于蛋白质语言模型的综合评估方法，并发现词汇表大小在50以上能够获得最佳性能。

    

    大规模的蛋白质语言模型擅长捕捉原始结构中的进化信息，对蛋白质工程具有重要实用价值。与自然语言模型相比，蛋白质氨基酸序列的数据量较小，组合空间有限。选择合适的词汇表大小来优化预训练模型是一个关键问题。此外，尽管自然语言领域拥有大量的基准测试和研究，但目前还缺乏一个全面评估蛋白质语言模型质量的基准。鉴于这些挑战，PETA使用了三种标记化方法，在14种不同的词汇表大小下训练语言模型。它在33个不同的下游数据集上进行了数千次测试，评估了模型的迁移学习能力，并结合了两个分类头和三个随机种子以减轻潜在偏见。大量实验表明，词汇表大小在50以上大约能够获得最佳的性能。

    Large protein language models are adept at capturing the underlying evolutionary information in primary structures, offering significant practical value for protein engineering. Compared to natural language models, protein amino acid sequences have a smaller data volume and a limited combinatorial space. Choosing an appropriate vocabulary size to optimize the pre-trained model is a pivotal issue. Moreover, despite the wealth of benchmarks and studies in the natural language community, there remains a lack of a comprehensive benchmark for systematically evaluating protein language model quality. Given these challenges, PETA trained language models with 14 different vocabulary sizes under three tokenization methods. It conducted thousands of tests on 33 diverse downstream datasets to assess the models' transfer learning capabilities, incorporating two classification heads and three random seeds to mitigate potential biases. Extensive experiments indicate that vocabulary sizes between 50 
    
[^14]: 利用GPT-3.5-turbo进行法律案例修辞角色预测的研究

    Harnessing GPT-3.5-turbo for Rhetorical Role Prediction in Legal Cases. (arXiv:2310.17413v1 [cs.CL])

    [http://arxiv.org/abs/2310.17413](http://arxiv.org/abs/2310.17413)

    该研究利用GPT-3.5-turbo对法律案例中的修辞角色进行预测，并发现使用少量标记示例进行引导可以获得更好的性能。

    

    我们提出了一个全面的研究，探讨在法律案例的修辞角色预测任务中，使用大型预训练生成式转换器GPT-3.5-turbo的一阶引导技术。该任务需要处理文本背景。我们的研究探讨了零或少样本、任务规范化与定义、厘清标注模糊性、文本背景和基于常用提示和具体问题的推理等策略。我们的实验结果表明，样本数量、标签定义、文本背景的展示以及针对该背景的具体问题对模型的性能有积极影响。在非等价的测试设置下，我们观察到使用少量来自直接背景的标记示例进行引导可以比基于BERT编码器的监督调优多分类器（加权F1得分为72%）获得更好的性能。但仍存在差距，有待改进。

    We propose a comprehensive study of one-stage elicitation techniques for querying a large pre-trained generative transformer (GPT-3.5-turbo) in the rhetorical role prediction task of legal cases. This task is known as requiring textual context to be addressed. Our study explores strategies such as zero-few shots, task specification with definitions and clarification of annotation ambiguities, textual context and reasoning with general prompts and specific questions. We show that the number of examples, the definition of labels, the presentation of the (labelled) textual context and specific questions about this context have a positive influence on the performance of the model. Given non-equivalent test set configurations, we observed that prompting with a few labelled examples from direct context can lead the model to a better performance than a supervised fined-tuned multi-class classifier based on the BERT encoder (weighted F1 score of = 72%). But there is still a gap to reach the pe
    
[^15]: 使用Exo解决矩阵乘法微内核生成问题

    Tackling the Matrix Multiplication Micro-kernel Generation with Exo. (arXiv:2310.17408v1 [cs.MS])

    [http://arxiv.org/abs/2310.17408](http://arxiv.org/abs/2310.17408)

    介绍了使用Exo编译器生成接近于甚至优于手动开发的微内核的步骤和方法，并解决了为每个新硬件生成专用微内核的问题。

    

    在过去的几十年里，矩阵乘法（或GEMM）的优化一直是一个需求。这个操作被认为是当前线性代数库（如BLIS，OpenBLAS或Intel OneAPI）的旗舰，因为它在各种科学应用中被广泛使用。GEMM通常是按照GotoBLAS的理念进行实现的，它将GEMM的操作数进行切割，并使用一系列嵌套循环来提高性能。这些方法通过一小块面向硬件的高性能代码，即微内核，提取体系结构的最大计算能力。然而，这种方法迫使开发人员为每个新的硬件生成一个专用的微内核，并需要非常大的工作量。在这项工作中，我们提出了一种使用Exo编译器生成微内核的逐步过程，该过程的性能接近甚至超过了手动编写的使用内部函数或汇编语言的微内核。

    The optimization of the matrix multiplication (or GEMM) has been a need during the last decades. This operation is considered the flagship of current linear algebra libraries such as BLIS, OpenBLAS, or Intel OneAPI because of its widespread use in a large variety of scientific applications. The GEMM is usually implemented following the GotoBLAS philosophy, which tiles the GEMM operands and uses a series of nested loops for performance improvement. These approaches extract the maximum computational power of the architectures through small pieces of hardware-oriented, high-performance code called micro-kernel. However, this approach forces developers to generate, with a non-negligible effort, a dedicated micro-kernel for each new hardware.  In this work, we present a step-by-step procedure for generating micro-kernels with the Exo compiler that performs close to (or even better than) manually developed microkernels written with intrinsic functions or assembly language. Our solution also 
    
[^16]: 大型语言模型中的意义和理解

    Meaning and understanding in large language models. (arXiv:2310.17407v1 [cs.CL])

    [http://arxiv.org/abs/2310.17407](http://arxiv.org/abs/2310.17407)

    本文评估了大型语言模型在理解自然语言中的意义时的条件。最新的人工智能模型认为传统的机器理解语言的假设需要修订。这篇文章着重强调了最先进的LLM模型不仅使用语法，而且也使用语义，并确定它们如何建立语言表达的意义。

    

    机器能理解自然语言的意义吗？人工智能生成的大型语言模型(LLMs)的最新发展使人们相信传统的关于机器语言理解的哲学假设需要修订。本文对普遍趋势进行了批判性评估，认为机器语言性能仅仅是语法操作和模拟理解，这种理解只是部分的且非常浅显，没有足够的世界参考基础。目的是强调将自然语言理解归因于最先进的LLMs的关键条件，可以合法地认为LLMs不仅使用语法，而且使用语义，它们的理解不是模拟而是复制，并确定它们如何建立语言表达的意义。

    Can a machine understand the meanings of natural language? Recent developments in the generative large language models (LLMs) of artificial intelligence have led to the belief that traditional philosophical assumptions about machine understanding of language need to be revised. This article critically evaluates the prevailing tendency to regard machine language performance as mere syntactic manipulation and the simulation of understanding, which is only partial and very shallow, without sufficient referential grounding in the world. The aim is to highlight the conditions crucial to attributing natural language understanding to state-of-the-art LLMs, where it can be legitimately argued that LLMs not only use syntax but also semantics, their understanding not being simulated but duplicated; and determine how they ground the meanings of linguistic expressions.
    
[^17]: ToxicChat: 揭示实际用户-AI对话中的毒性检测隐藏挑战

    ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation. (arXiv:2310.17389v1 [cs.CL])

    [http://arxiv.org/abs/2310.17389](http://arxiv.org/abs/2310.17389)

    本研究引入了ToxicChat，一个基于实际用户查询构建的新型毒性检测基准。该基准揭示了当前毒性检测模型在实际用户-AI对话中面临的困难，强调了实际对话中存在的独特挑战。

    

    尽管大型语言模型在聊天机器人方面取得了显著的进展，但如今维持一个非毒性的用户-AI互动环境变得越来越重要。然而，先前的毒性检测工作大多基于社交媒体内容导出的基准，未充分探索实际用户-AI互动中固有的独特挑战。本研究引入了ToxicChat，这是一个基于开源聊天机器人的实际用户查询构建的新型基准。该基准包含了对当前毒性检测模型难以识别的丰富而微妙的现象，相对于社交媒体内容来说存在显著的领域差异。我们对在现有毒性数据集上训练的模型进行了系统评估，发现它们在应用于ToxicChat的这个独特领域时存在缺点。我们的研究揭示了实际用户-AI对话中毒性检测可能被忽视的挑战。将来，ToxicChat的研究可以推动更好的毒性检测方法和工具的发展。

    Despite remarkable advances that large language models have achieved in chatbots, maintaining a non-toxic user-AI interactive environment has become increasingly critical nowadays. However, previous efforts in toxicity detection have been mostly based on benchmarks derived from social media content, leaving the unique challenges inherent to real-world user-AI interactions insufficiently explored. In this work, we introduce ToxicChat, a novel benchmark based on real user queries from an open-source chatbot. This benchmark contains the rich, nuanced phenomena that can be tricky for current toxicity detection models to identify, revealing a significant domain difference compared to social media content. Our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of ToxicChat. Our work illuminates the potentially overlooked challenges of toxicity detection in real-world user-AI conversations. In the future, Toxic
    
[^18]: 基于大型语言模型的对话式生成自动驾驶仿真场景

    Dialogue-based generation of self-driving simulation scenarios using Large Language Models. (arXiv:2310.17372v1 [cs.AI])

    [http://arxiv.org/abs/2310.17372](http://arxiv.org/abs/2310.17372)

    本文介绍了一个系统，使用大型语言模型将用户的英文话语映射为领域特定代码，以支持对话式生成自动驾驶仿真场景，并探索了语言模型所能捕捉到的上下文敏感性。

    

    仿真是开发和评估自动驾驶汽车控制器的无价工具。当前的仿真框架基于高度专业的领域特定语言，因此自然语言界面将极大地增加可用性。但是，英文简洁用语和捕捉用户意图的可执行代码之间经常存在一定的隐含假设差距。本文描述了一个系统来解决这个问题，通过支持扩展的多模态交互，用户可以用修正或修改来跟进之前的指令，以对迄今为止从他们的话语生成的仿真作出反应。我们使用大型语言模型（LLMs）将用户的英文话语在这种互动中映射到特定领域的代码，因此我们探索了LLMs能否捕捉到计算发言者在话语中的预期信息所需的上下文敏感性。

    Simulation is an invaluable tool for developing and evaluating controllers for self-driving cars. Current simulation frameworks are driven by highly-specialist domain specific languages, and so a natural language interface would greatly enhance usability. But there is often a gap, consisting of tacit assumptions the user is making, between a concise English utterance and the executable code that captures the user's intent. In this paper we describe a system that addresses this issue by supporting an extended multimodal interaction: the user can follow up prior instructions with refinements or revisions, in reaction to the simulations that have been generated from their utterances so far. We use Large Language Models (LLMs) to map the user's English utterances in this interaction into domain-specific code, and so we explore the extent to which LLMs capture the context sensitivity that's necessary for computing the speaker's intended message in discourse.
    
[^19]: 语言和心理健康：从文本中测量情绪动态作为语言生物社会标记。

    Language and Mental Health: Measures of Emotion Dynamics from Text as Linguistic Biosocial Markers. (arXiv:2310.17369v1 [cs.CL])

    [http://arxiv.org/abs/2310.17369](http://arxiv.org/abs/2310.17369)

    本研究首次研究了推文情绪动态和心理健康障碍之间的关系，发现推文情绪动态与用户自我披露的诊断有关，为心理健康的评估提供了新的方法。

    

    心理病理学研究表明，情绪随时间的变化模式——情绪动态——是心理健康的指标。传统上，情绪变化的模式是通过情绪的自我报告来确定的；然而，已知存在准确性、偏见和便利性等问题。最近的方法通过研究个人日常发言来确定情绪动态，解决了许多这些问题，但目前尚不清楚这些发言情绪动态的测量值是否与心理健康诊断相关。在这里，我们首次研究了推文情绪动态与心理健康障碍之间的关系。我们发现，我们研究的每个情绪动态度量值都因用户自我披露的诊断而有所变化。例如，平均情绪价值较高（即文本较积极）的控制组与患有注意力缺陷多动障碍（ADHD）、抑郁症（MDD）和创伤后应激障碍（PTSD）的用户相比显著较高。情绪价值变异性在控制组中显著较低。

    Research in psychopathology has shown that, at an aggregate level, the patterns of emotional change over time -- emotion dynamics -- are indicators of one's mental health. One's patterns of emotion change have traditionally been determined through self-reports of emotions; however, there are known issues with accuracy, bias, and convenience. Recent approaches to determining emotion dynamics from one's everyday utterances, addresses many of these concerns, but it is not yet known whether these measures of utterance emotion dynamics (UED) correlate with mental health diagnoses. Here, for the first time, we study the relationship between tweet emotion dynamics and mental health disorders. We find that each of the UED metrics studied varied by the user's self-disclosed diagnosis. For example: average valence was significantly higher (i.e., more positive text) in the control group compared to users with ADHD, MDD, and PTSD. Valence variability was significantly lower in the control group co
    
[^20]: 食谱的文化适应

    Cultural Adaptation of Recipes. (arXiv:2310.17353v1 [cs.CL])

    [http://arxiv.org/abs/2310.17353](http://arxiv.org/abs/2310.17353)

    本研究介绍了一项涉及中餐和英语国家菜系之间食谱的翻译和文化适应的新任务，提供了一个独特的数据集并评估了多种方法的性能。

    

    在大型语言模型（LLM）取得显著进展的基础上，我们现在有能力解决更复杂的任务，需要对跨文化环境有一个细致的理解。一个关键的例子是食谱的适应，这超出了简单的翻译，还包括对某个特定文化的食材、烹饪技巧和膳食偏好的掌握。我们介绍了一个涉及中餐和英语国家菜系之间食谱的翻译和文化适应的新任务。为了支持这项调查，我们提供了CulturalRecipes，这是一个由自动配对的中文和英文食谱构成的独特数据集。该数据集还通过人工编写和精心策划的测试集进行了丰富。在这个复杂的跨文化食谱适应任务中，我们评估了各种方法的性能，包括GPT-4和其他LLM、传统机器翻译和信息检索技术。我们的综合分析包括自动化和人工评估方面的内容。

    Building upon the considerable advances in Large Language Models (LLMs), we are now equipped to address more sophisticated tasks demanding a nuanced understanding of cross-cultural contexts. A key example is recipe adaptation, which goes beyond simple translation to include a grasp of ingredients, culinary techniques, and dietary preferences specific to a given culture. We introduce a new task involving the translation and cultural adaptation of recipes between Chinese and English-speaking cuisines. To support this investigation, we present CulturalRecipes, a unique dataset comprised of automatically paired recipes written in Mandarin Chinese and English. This dataset is further enriched with a human-written and curated test set. In this intricate task of cross-cultural recipe adaptation, we evaluate the performance of various methods, including GPT-4 and other LLMs, traditional machine translation, and information retrieval techniques. Our comprehensive analysis includes both automati
    
[^21]: ACT-SQL: 基于上下文学习的自动生成链式思维的文本到SQL技术

    ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought. (arXiv:2310.17342v1 [cs.CL])

    [http://arxiv.org/abs/2310.17342](http://arxiv.org/abs/2310.17342)

    ACT-SQL是一种基于上下文学习的自动生成链式思维的文本到SQL技术，在文本到SQL任务中设计了类似模式链接的链式思维提示，通过自动生成示例实现了成本节省。实验结果表明ACT-SQL方法在Spider开发集上实现了最佳性能。

    

    最近，大型语言模型（LLMs）在各个领域和任务中已被证明具有很强的能力。我们研究文本到SQL任务中提示设计的问题，并尝试提高LLMs在生成SQL查询时的推理能力。除了传统的少样本上下文学习设置外，我们采用类似于模式链接的方法设计了我们的链式思维（CoT）提示。我们提供了一种名为ACT-SQL的方法来自动生成自动CoT示例，因此整个过程不需要手动标记。我们的方法具有成本节省，因为在生成一个SQL查询时，我们只使用LLMs的API调用一次。此外，我们将我们的上下文学习方法扩展到多轮文本到SQL任务中。实验结果表明，LLMs的性能可以受益于我们的ACT-SQL方法。我们的方法在现有的上下文学习方法中，在Spider开发集上达到了最佳性能。

    Recently Large Language Models (LLMs) have been proven to have strong abilities in various domains and tasks. We study the problem of prompt designing in the text-to-SQL task and attempt to improve the LLMs' reasoning ability when generating SQL queries. Besides the trivial few-shot in-context learning setting, we design our chain-of-thought (CoT) prompt with a similar method to schema linking. We provide a method named ACT-SQL to automatically generate auto-CoT exemplars and thus the whole process doesn't need manual labeling. Our approach is cost-saving since we only use the LLMs' API call once when generating one SQL query. Furthermore, we extend our in-context learning method to the multi-turn text-to-SQL task. The experiment results show that the LLMs' performance can benefit from our ACT-SQL approach. Our approach achieves SOTA performance on the Spider dev set among existing in-context learning approaches.
    
[^22]: 阿拉伯精细化实体识别

    Arabic Fine-Grained Entity Recognition. (arXiv:2310.17333v1 [cs.CL])

    [http://arxiv.org/abs/2310.17333](http://arxiv.org/abs/2310.17333)

    本论文提出了一种阿拉伯精细化实体识别方法，通过扩展现有的阿拉伯命名实体语料库，将地缘政治实体、位置、组织和设施等四种主要实体类型扩展为31个子类型，并通过评估评注者一致性证明了扩展后的效果。

    

    传统的NER系统通常被训练来识别粗粒度实体，并且对将实体分类为细粒度的低层级子类型的关注较少。本文旨在通过精细化实体提升阿拉伯NER。我们选择扩展Wojood（一个开源的嵌套阿拉伯命名实体语料库）的子类型。尤其是，Wojood中的四种主要实体类型，地缘政治实体（GPE），位置（LOC），组织（ORG）和设施（FAC），被扩展为31个子类型。为了做到这一点，我们首先修订了Wojood对GPE，LOC，ORG和FAC的注释，使其与LDC的ACE指南兼容，结果有5614处更改。其次，我们人工注释了Wojood中所有GPE，LOC，ORG和FAC的提及（约44K），使用LDC的ACE子类型。我们将这个扩展版本的Wojood称为WojoodFine。为了评估我们的注释，我们使用Cohen's Kappa和F1得分来测量评注者一致性（IAA），结果分别为0.9861和0.9889

    Traditional NER systems are typically trained to recognize coarse-grained entities, and less attention is given to classifying entities into a hierarchy of fine-grained lower-level subtypes. This article aims to advance Arabic NER with fine-grained entities. We chose to extend Wojood (an open-source Nested Arabic Named Entity Corpus) with subtypes. In particular, four main entity types in Wojood, geopolitical entity (GPE), location (LOC), organization (ORG), and facility (FAC), are extended with 31 subtypes. To do this, we first revised Wojood's annotations of GPE, LOC, ORG, and FAC to be compatible with the LDC's ACE guidelines, which yielded 5, 614 changes. Second, all mentions of GPE, LOC, ORG, and FAC (~44K) in Wojood are manually annotated with the LDC's ACE sub-types. We refer to this extended version of Wojood as WojoodF ine. To evaluate our annotations, we measured the inter-annotator agreement (IAA) using both Cohen's Kappa and F1 score, resulting in 0.9861 and 0.9889, respect
    
[^23]: Nabra: 具有形态学注释的叙利亚阿拉伯方言

    Nabra: Syrian Arabic Dialects with Morphological Annotations. (arXiv:2310.17315v1 [cs.CL])

    [http://arxiv.org/abs/2310.17315](http://arxiv.org/abs/2310.17315)

    Nabra是一份包含叙利亚阿拉伯方言和形态学注释的语料库，覆盖了多种叙利亚本土方言，标注的质量优秀，开源可公开获取。

    

    本文介绍了Nabra，这是一份包含叙利亚阿拉伯方言和形态学注释的语料库。由叙利亚本土人收集了超过6千个句子，约6万个词汇，来源包括社交媒体帖子、电影和连续剧的剧本、歌曲歌词和当地的谚语，用于构建Nabra。Nabra涵盖了包括阿勒颇、大马士革、代尔祖尔、哈马、霍姆斯、胡兰、拉塔基亚、马尔丁、拉卡和苏韦达在内的多种叙利亚本土方言。九名标注人员对这6万个标记进行了全面的形态学注释，确保了独特的形态素标注，并进行了标准化的注释。按特征计算的F1和kappa一致性得分在74%到98%之间，显示了Nabra注释的优质性。我们的语料库是开源的，可以通过Currasat门户网站https://sina.birzeit.edu/currasat公开获取。

    This paper presents Nabra, a corpora of Syrian Arabic dialects with morphological annotations. A team of Syrian natives collected more than 6K sentences containing about 60K words from several sources including social media posts, scripts of movies and series, lyrics of songs and local proverbs to build Nabra. Nabra covers several local Syrian dialects including those of Aleppo, Damascus, Deir-ezzur, Hama, Homs, Huran, Latakia, Mardin, Raqqah, and Suwayda. A team of nine annotators annotated the 60K tokens with full morphological annotations across sentence contexts. We trained the annotators to follow methodological annotation guidelines to ensure unique morpheme annotations, and normalized the annotations. F1 and kappa agreement scores ranged between 74% and 98% across features, showing the excellent quality of Nabra annotations. Our corpora are open-source and publicly available as part of the Currasat portal https://sina.birzeit.edu/currasat.
    
[^24]: 基于Transformer和卷积神经网络的集成方法用于检测人工生成的文本

    An Ensemble Method Based on the Combination of Transformers with Convolutional Neural Networks to Detect Artificially Generated Text. (arXiv:2310.17312v1 [cs.CL])

    [http://arxiv.org/abs/2310.17312](http://arxiv.org/abs/2310.17312)

    这项研究提出了一种基于Transformer和卷积神经网络的集成方法，用于检测人工生成的文本。实验结果表明，这种集成架构在分类任务上的性能超过了单个Transformer模型，而提出的SciBERT-CNN集成模型在ALTA共享任务2023数据上取得了98.36%的F1得分。

    

    随着最先进的大型语言模型（LLM），语言生成已经达到了卓越的水平。这些模型能够生成高质量的内容，因此从人工撰写的内容中检测生成文本变成了一个具有挑战性的任务。尽管自然语言生成提供了诸多优势，但无法区分自动生成的文本可能会引发关于真实性的道德顾虑。因此，设计和开发检测人工内容的方法非常重要。在我们的工作中，我们提出了一些通过集成Transformer模型（如Sci-BERT、DeBERTa和XLNet）与卷积神经网络（CNN）构建的分类模型。我们的实验表明，考虑的集成架构在分类任务上超越了单个Transformer模型的表现。此外，提出的SciBERT-CNN集成模型在ALTA共享任务2023数据上得到了98.36%的F1得分。

    Thanks to the state-of-the-art Large Language Models (LLMs), language generation has reached outstanding levels. These models are capable of generating high quality content, thus making it a challenging task to detect generated text from human-written content. Despite the advantages provided by Natural Language Generation, the inability to distinguish automatically generated text can raise ethical concerns in terms of authenticity. Consequently, it is important to design and develop methodologies to detect artificial content. In our work, we present some classification models constructed by ensembling transformer models such as Sci-BERT, DeBERTa and XLNet, with Convolutional Neural Networks (CNNs). Our experiments demonstrate that the considered ensemble architectures surpass the performance of the individual transformer models for classification. Furthermore, the proposed SciBERT-CNN ensemble model produced an F1-score of 98.36% on the ALTA shared task 2023 data.
    
[^25]: FormaT5: 以自然语言生成条件表格格式化的抽样和示例

    FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language. (arXiv:2310.17306v1 [cs.AI])

    [http://arxiv.org/abs/2310.17306](http://arxiv.org/abs/2310.17306)

    FormaT5是一个基于转换器的模型，可以根据目标表格和自然语言描述生成数据相关的条件格式规则。为了解决描述不足的问题，FormaT5通过放弃目标的方式学习预测占位符。

    

    表格的格式化是可视化、展示和分析中的重要属性。电子表格软件允许用户通过编写数据相关的条件格式规则来自动格式化表格。但对用户来说，编写这样的规则通常是具有挑战性的，因为它要求他们理解和实现底层逻辑。我们提出了一个基于转换器的模型FormaT5，可以根据目标表格和期望的格式逻辑的自然语言描述生成一个条件格式规则。我们发现，用户为这些任务提供的描述通常是不明确或含糊的，这使得代码生成系统难以在一步中准确学习到所需的规则。为了解决这个规范不足的问题并减少参数错误，FormaT5通过放弃目标的方式学习预测占位符。这些占位符可以由第二个模型或者当可用的行示例时，由一个基于示例的编程系统填充。

    Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system
    
[^26]: 比较逼真和动画形象的交互式对话代理在严肃游戏中的用户体验：一项关于用户体验的实证研究

    Comparing Photorealistic and Animated Embodied Conversational Agents in Serious Games: An Empirical Study on User Experience. (arXiv:2310.17300v1 [cs.HC])

    [http://arxiv.org/abs/2310.17300](http://arxiv.org/abs/2310.17300)

    本文通过比较逼真和动画形象的交互式对话代理在严肃游戏中的表现，提供了关于基于语音的ECAs设计的洞察和建议。结果显示逼真和动画形象版本都具有很高的可用性，但大多数参与者更喜欢逼真版本。

    

    交互式对话代理（ECAs）是以具象化角色形式呈现的对话用户界面的范例。本文重点研究了两种不同级别的表现逼真度，即逼真和动画形象。研究旨在提供关于基于语音的ECAs在严肃游戏环境中的洞察和设计建议。研究采用了一个完全组内、二因素设计，共有36名性别平衡的参与者。结果显示，逼真和动画形象的版本都被认为具有很高的可用性，其平均得分分别为5.76和5.71。然而，69.4％的参与者表示他们更喜欢逼真版本，25％的参与者表示他们更喜欢动画版本，还有5.6％的参与者没有表明偏好。逼真代理被认为更真实和类人。

    Embodied conversational agents (ECAs) are paradigms of conversational user interfaces in the form of embodied characters. While ECAs offer various manipulable features, this paper focuses on a study conducted to explore two distinct levels of presentation realism. The two agent versions are photorealistic and animated. The study aims to provide insights and design suggestions for speech-enabled ECAs within serious game environments. A within-subjects, two-by-two factorial design was employed for this research with a cohort of 36 participants balanced for gender. The results showed that both the photorealistic and the animated versions were perceived as highly usable, with overall mean scores of 5.76 and 5.71, respectively. However, 69.4 per cent of the participants stated they preferred the photorealistic version, 25 per cent stated they preferred the animated version and 5.6 per cent had no stated preference. The photorealistic agents were perceived as more realistic and human-like, w
    
[^27]: 使用非参数变分信息瓶颈学习摘要

    Learning to Abstract with Nonparametric Variational Information Bottleneck. (arXiv:2310.17284v1 [cs.CL])

    [http://arxiv.org/abs/2310.17284](http://arxiv.org/abs/2310.17284)

    本论文提出了一种新颖的语言表示模型，可以在同一模型的不同层级上学习不同级别的抽象，并通过非参数变分信息瓶颈实现信息论压缩。这种模型具有更高级的抽象并且更具语言信息，同时对于敌对扰动具有更强的鲁棒性。

    

    在字符、子词、词和句子级别上学习表示在理解不同自然语言处理任务和语言现象方面都有所贡献。然而，学习文本嵌入的成本高昂，因为它们是特定于分词的，并且需要训练不同的模型来处理不同的抽象级别。我们介绍了一种新颖的语言表示模型，可以在同一模型的不同层级上学习不同级别的抽象。我们在编码器的堆叠Transformer自注意层中应用了非参数变分信息瓶颈（NVIB），通过该模型鼓励对表示进行信息论压缩。我们发现模型内的层对应着越来越高级的抽象级别，并且它们的表示更具语言信息。最后，我们证明了NVIB压缩导致的模型对于敌对扰动具有更强的鲁棒性。

    Learned representations at the level of characters, sub-words, words and sentences, have each contributed to advances in understanding different NLP tasks and linguistic phenomena. However, learning textual embeddings is costly as they are tokenization specific and require different models to be trained for each level of abstraction. We introduce a novel language representation model which can learn to compress to different levels of abstraction at different layers of the same model. We apply Nonparametric Variational Information Bottleneck (NVIB) to stacked Transformer self-attention layers in the encoder, which encourages an information-theoretic compression of the representations through the model. We find that the layers within the model correspond to increasing levels of abstraction and that their representations are more linguistically informed. Finally, we show that NVIB compression results in a model which is more robust to adversarial perturbations.
    
[^28]: 自动逻辑形式提高表格到文本生成中的准确性

    Automatic Logical Forms improve fidelity in Table-to-Text generation. (arXiv:2310.17279v1 [cs.CL])

    [http://arxiv.org/abs/2310.17279](http://arxiv.org/abs/2310.17279)

    本文提出了一种自动逻辑形式（LF）来提高表格到文本生成的准确性，首次展示了用自动LF改进系统可以提高准确性30个百分点，还指出了实现高准确性仍面临的挑战。

    

    表格到文本系统从结构化数据（如表格）生成自然语言陈述。虽然端到端技术在事实准确性方面存在问题，但先前的研究报告称，在使用手动逻辑形式（LF）表示所选内容和目标文本的语义时，获得了提升。鉴于手动步骤，不清楚自动LF是否有效，或者改进来自内容选择本身。我们提出了TlT，给定一个表格和内容选择，首先生成LF，然后生成文本陈述。我们首次展示了自动LF提高质量的效果，与不使用LF的类似系统相比，准确性提高了30个百分点。我们的实验对于高准确性还面临着一些挑战，自动内容选择是首要问题，其次是更好的逻辑到文本生成，以及较少程度的更好的表格到逻辑解析。

    Table-to-text systems generate natural language statements from structured data like tables. While end-to-end techniques suffer from low factual correctness (fidelity), a previous study reported gains when using manual logical forms (LF) that represent the selected content and the semantics of the target text. Given the manual step, it was not clear whether automatic LFs would be effective, or whether the improvement came from content selection alone. We present TlT which, given a table and a selection of the content, first produces LFs and then the textual statement. We show for the first time that automatic LFs improve quality, with an increase in fidelity of 30 points over a comparable system not using LFs. Our experiments allow to quantify the remaining challenges for high factual correctness, with automatic selection of content coming first, followed by better Logic-to-Text generation and, to a lesser extent, better Table-to-Logic parsing.
    
[^29]: 理解语言模型中输入单词字符的作用：信息丢失如何影响性能？

    Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?. (arXiv:2310.17271v1 [cs.CL])

    [http://arxiv.org/abs/2310.17271](http://arxiv.org/abs/2310.17271)

    本文探索了输入单词字符对预训练语言模型性能的影响，发现即使只使用单个字符进行预训练，模型在标准任务中的性能保持较高。

    

    理解预训练语言模型（PLMs）如何学习语言是自然语言处理中一个开放的挑战。早期的工作主要集中在确定它们是否捕捉了语义和句法信息，以及数据或预训练目标如何影响它们的性能。然而，据我们所知，之前没有研究专门探讨输入单词字符的信息丢失对PLMs性能的影响。在本研究中，我们通过使用单个单词字符的小子集进行预训练语言模型，填补了这一空白。令人惊讶的是，我们发现即使在极端情况下，即每个单词只使用一个字符进行预训练，与完整单词模型相比在标准NLU基准测试和探测任务中的性能保持较高。例如，仅使用单词的首个字符进行预训练的模型在SuperGLUE中保持了约90％和77％的性能。

    Understanding how and what pre-trained language models (PLMs) learn about language is an open challenge in natural language processing. Previous work has focused on identifying whether they capture semantic and syntactic information, and how the data or the pre-training objective affects their performance. However, to the best of our knowledge, no previous work has specifically examined how information loss in input token characters affects the performance of PLMs. In this study, we address this gap by pre-training language models using small subsets of characters from individual tokens. Surprisingly, we find that pre-training even under extreme settings, i.e. using only one character of each token, the performance retention in standard NLU benchmarks and probing tasks compared to full-token models is high. For instance, a model pre-trained only on single first characters from tokens achieves performance retention of approximately $90$\% and $77$\% of the full-token model in SuperGLUE 
    
[^30]: 通过span剪枝和超图神经网络实现联合实体和关系抽取

    Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks. (arXiv:2310.17238v1 [cs.CL])

    [http://arxiv.org/abs/2310.17238](http://arxiv.org/abs/2310.17238)

    本文提出了基于超图神经网络的联合实体和关系抽取方法，使用span剪枝机制减轻误差传播问题，通过构建超图进行高阶建模，实现多个实体和关系之间的交互。

    

    实体和关系抽取（ERE）是信息提取中的重要任务，最近基于标记的流水线模型取得了最先进的性能，但仍然存在误差传播问题。此外，大多数当前的ERE模型在多个实体和关系之间不考虑高阶交互，而高阶建模可能会有益处。在这项工作中，我们提出了超图神经网络（HGNN）用于ERE，它是建立在PL-marker（最先进的基于标记的流水线模型）之上的。为了减轻误差传播，我们使用高召回剪枝器机制将实体的识别和标注负担从NER模块转移到我们模型的联合模块。对于高阶建模，我们构建了一个超图，其中节点是实体（由span剪枝器提供），以及其关系，并且超边编码了两个不同关系之间或关系与其相关的主体和宾语实体之间的交互。接着我们运行...

    Entity and Relation Extraction (ERE) is an important task in information extraction. Recent marker-based pipeline models achieve state-of-the-art performance, but still suffer from the error propagation issue. Also, most of current ERE models do not take into account higher-order interactions between multiple entities and relations, while higher-order modeling could be beneficial.In this work, we propose HyperGraph neural network for ERE ($\hgnn{}$), which is built upon the PL-marker (a state-of-the-art marker-based pipleline model). To alleviate error propagation,we use a high-recall pruner mechanism to transfer the burden of entity identification and labeling from the NER module to the joint module of our model. For higher-order modeling, we build a hypergraph, where nodes are entities (provided by the span pruner) and relations thereof, and hyperedges encode interactions between two different relations or between a relation and its associated subject and object entities. We then run
    
[^31]: EMMA-X:一种用于跨语言表示学习的类EM多语言预训练算法

    EMMA-X: An EM-like Multilingual Pre-training Algorithm for Cross-lingual Representation Learning. (arXiv:2310.17233v1 [cs.CL])

    [http://arxiv.org/abs/2310.17233](http://arxiv.org/abs/2310.17233)

    本文提出了一种名为EMMA-X的类EM多语言预训练算法，通过利用多语言非并行数据进行学习，实现了跨语言的通用表示。该算法在一个EM框架内统一了跨语言表示学习任务和额外的语义关系预测任务，通过互相监督直到收敛的方式进行训练。

    

    在理解复杂和特定于文化的句子的含义时，表达所有语言共同的通用语义是有帮助的。本文的研究主题是使用海量并行语料库学习跨语言的通用表示。然而，由于并行数据的稀疏性和匮乏性，对于任何两种语言来说，学习真正的“普遍性”仍然是一个巨大的挑战。在本文中，我们提出了一种名为EMMA-X的类EM多语言预训练算法，利用多语言非并行数据来学习跨语言的“普遍性”。EMMA-X在EM框架内将跨语言表示学习任务和额外的语义关系预测任务统一起来。额外的语义分类器和跨语言句子编码器都近似了两个句子的语义关系，并相互监督直到收敛。为了评估EMMA-X，我们在新引入的XRETE基准上进行了实验。

    Expressing universal semantics common to all languages is helpful in understanding the meanings of complex and culture-specific sentences. The research theme underlying this scenario focuses on learning universal representations across languages with the usage of massive parallel corpora. However, due to the sparsity and scarcity of parallel data, there is still a big challenge in learning authentic ``universals'' for any two languages. In this paper, we propose EMMA-X: an EM-like Multilingual pre-training Algorithm, to learn (X)Cross-lingual universals with the aid of excessive multilingual non-parallel data. EMMA-X unifies the cross-lingual representation learning task and an extra semantic relation prediction task within an EM framework. Both the extra semantic classifier and the cross-lingual sentence encoder approximate the semantic relation of two sentences, and supervise each other until convergence. To evaluate EMMA-X, we conduct experiments on XRETE, a newly introduced benchma
    
[^32]: Codebook特征：神经网络的稀疏和离散可解释性

    Codebook Features: Sparse and Discrete Interpretability for Neural Networks. (arXiv:2310.17230v1 [cs.LG])

    [http://arxiv.org/abs/2310.17230](http://arxiv.org/abs/2310.17230)

    本研究提出了一种称为codebook特征的方法，通过将神经网络的连续特征量化为离散向量码的总和来实现稀疏和离散的隐藏状态。实验证明，神经网络在这种极端瓶颈条件下运行时性能下降适度，同时这种方法还提供了一种直观的神经网络行为控制方式。

    

    理解神经网络是具有挑战性的，部分原因是由于它们的隐藏状态是密集和连续的。我们探讨了是否可以通过将连续特征量化为我们称之为codebook特征来训练神经网络，使其具有稀疏、离散且更易解释的隐藏状态。通过在每层引入向量量化瓶颈来微调神经网络，产生的codebook特征由从更大的codebook中选择的少量离散向量码的总和组成。令人惊讶的是，我们发现神经网络可以在这种极端瓶颈下运行，性能只有适度的下降。这种稀疏、离散的瓶颈还提供了一种直观的控制神经网络行为的方法：首先，找到在所需行为出现时激活的码，然后在生成过程中激活相同的码以引发该行为。我们通过训练codebook Transformers验证了我们的方法。

    Understanding neural networks is challenging in part because of the dense, continuous nature of their hidden states. We explore whether we can train neural networks to have hidden states that are sparse, discrete, and more interpretable by quantizing their continuous features into what we call codebook features. Codebook features are produced by finetuning neural networks with vector quantization bottlenecks at each layer, producing a network whose hidden features are the sum of a small number of discrete vector codes chosen from a larger codebook. Surprisingly, we find that neural networks can operate under this extreme bottleneck with only modest degradation in performance. This sparse, discrete bottleneck also provides an intuitive way of controlling neural network behavior: first, find codes that activate when the desired behavior is present, then activate those same codes during generation to elicit that behavior. We validate our approach by training codebook Transformers on sever
    
[^33]: TST$^\mathrm{R}$: 目标相似度调整遇见现实世界

    TST$^\mathrm{R}$: Target Similarity Tuning Meets the Real World. (arXiv:2310.17228v1 [cs.AI])

    [http://arxiv.org/abs/2310.17228](http://arxiv.org/abs/2310.17228)

    本文提出了在现实世界中应用和改进目标相似度调整（TST）的不同方法，包括使用更大的模型嵌入、训练一个小模型转换嵌入以匹配代码相似度，并介绍了高效选择训练样例和基于排名的评估方法。

    

    目标相似度调整（TST）是一种通过大型语言模型（LLM）在自然语言（NL）到代码生成中选择相关例子以提升性能的方法。其目标是使得句子嵌入模型适应两个NL输入的相似度与其相关代码输出的相似度匹配。本文提出了不同的方法来应用和改进TST在现实世界中的使用。首先，我们将句子转换器替换为更大模型的嵌入，从而降低对语言分布的敏感性，增加了合成示例的灵活性，并训练一个小模型将这些嵌入转换到一个空间中，其中嵌入相似度匹配代码相似度，使得模型保持黑箱状态，并在推断时只需进行少量矩阵乘法。其次，我们介绍了如何高效地选择较少数量的训练样例来训练TST模型。第三，我们引入了基于排名的评估方法。

    Target similarity tuning (TST) is a method of selecting relevant examples in natural language (NL) to code generation through large language models (LLMs) to improve performance. Its goal is to adapt a sentence embedding model to have the similarity between two NL inputs match the similarity between their associated code outputs. In this paper, we propose different methods to apply and improve TST in the real world. First, we replace the sentence transformer with embeddings from a larger model, which reduces sensitivity to the language distribution and thus provides more flexibility in synthetic generation of examples, and we train a tiny model that transforms these embeddings to a space where embedding similarity matches code similarity, which allows the model to remain a black box and only requires a few matrix multiplications at inference time. Second, we how to efficiently select a smaller number of training examples to train the TST model. Third, we introduce a ranking-based evalu
    
[^34]: 超越MLE: 用于文本生成的凸学习方法

    Beyond MLE: Convex Learning for Text Generation. (arXiv:2310.17217v1 [cs.CL])

    [http://arxiv.org/abs/2310.17217](http://arxiv.org/abs/2310.17217)

    本论文提出了一种基于凸函数的训练目标类，超越了传统的最大似然估计方法。该方法适用于闭合型文本生成任务，并能够使得模型生成更加合适的响应。

    

    最大似然估计（MLE）是一种统计方法，用于估计最能解释观测数据的概率分布参数。在文本生成的背景下，MLE经常用于训练生成型语言模型，从而生成新的文本。然而，我们认为MLE并不总是必要且最优的，尤其是对于闭合型文本生成任务，如机器翻译。在这些任务中，模型的目标是生成最合适的响应，这并不一定需要使用MLE来估计整个数据分布。为此，我们提出了一种基于凸函数的新型训练目标类，使得文本生成模型能够集中于高概率的输出，而不需要估计整个数据分布。我们研究了在将凸函数应用于损失函数时的最优预测分布的理论性质，证明了凸函数可以使得最优预测分布变得更加锐利。

    Maximum likelihood estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution that best explain the observed data. In the context of text generation, MLE is often used to train generative language models, which can then be used to generate new text. However, we argue that MLE is not always necessary and optimal, especially for closed-ended text generation tasks like machine translation. In these tasks, the goal of model is to generate the most appropriate response, which does not necessarily require it to estimate the entire data distribution with MLE. To this end, we propose a novel class of training objectives based on convex functions, which enables text generation models to focus on highly probable outputs without having to estimate the entire data distribution. We investigate the theoretical properties of the optimal predicted distribution when applying convex functions to the loss, demonstrating that convex functions can sharpen the opt
    
[^35]: 使用Tsetlin机器进行高效数据融合

    Efficient Data Fusion using the Tsetlin Machine. (arXiv:2310.17207v1 [cs.AI])

    [http://arxiv.org/abs/2310.17207](http://arxiv.org/abs/2310.17207)

    这项研究提出了一种使用Tsetlin机器进行高效数据融合的新方法，通过监测学习到的逻辑子句在动态数据中的变化，识别和融合噪声，并在实验中展示出高性能。

    

    我们提出了一种使用Tsetlin Machine评估和融合嘈杂动态数据的新方法。我们的方法是通过监测Tsetlin Machine学习到的逻辑子句在动态数据中可能存在的噪声情况下的变化来识别和融合噪声。通过降低先前学习到的子句的权重或以新的子句形式反映噪声。我们还进行了一项全面的实验研究，使用了不同的数据集，结果显示出所提出方法的高性能。

    We propose a novel way of assessing and fusing noisy dynamic data using a Tsetlin Machine. Our approach consists in monitoring how explanations in form of logical clauses that a TM learns changes with possible noise in dynamic data. This way TM can recognize the noise by lowering weights of previously learned clauses, or reflect it in the form of new clauses. We also perform a comprehensive experimental study using notably different datasets that demonstrated high performance of the proposed approach.
    
[^36]: 语言模型如何将实体绑定到上下文中?

    How do Language Models Bind Entities in Context?. (arXiv:2310.17191v1 [cs.LG])

    [http://arxiv.org/abs/2310.17191](http://arxiv.org/abs/2310.17191)

    通过分析语言模型的表示，我们发现了绑定ID机制，它可以将实体与属性进行有效地绑定。我们通过因果干预实验进一步证明了语言模型内部激活表示绑定信息的方式。研究结果揭示了语言模型在上下文中如何表示符号知识，从而为理解大规模语言模型的一般上下文推理提供了指导。

    

    为了正确使用上下文信息，语言模型（LMs）必须将实体与其属性进行绑定。例如，给定描述“绿色方块”和“蓝色圆形”的上下文，LMs必须将形状与它们对应的颜色进行绑定。我们分析LM表示并确定绑定ID机制：这是一种解决绑定问题的通用机制，我们在Pythia和LLaMA家族的每个足够大的模型中观察到。通过因果干预，我们展示了LMs内部激活通过将绑定ID向量附加到相应的实体和属性上来表示绑定信息。我们进一步展示了绑定ID向量形成连续的子空间，在这个子空间中，绑定ID向量之间的距离反映了它们的区别。总体而言，我们的结果揭示了LMs在上下文中表示符号知识的可解释策略，为理解大规模LMs中的一般上下文推理迈出了一步。

    To correctly use in-context information, language models (LMs) must bind entities to their attributes. For example, given a context describing a "green square" and a "blue circle", LMs must bind the shapes to their respective colors. We analyze LM representations and identify the binding ID mechanism: a general mechanism for solving the binding problem, which we observe in every sufficiently large model from the Pythia and LLaMA families. Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes. We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability. Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.
    
[^37]: X-SNS: 通过子网络相似性进行跨语言迁移预测

    X-SNS: Cross-Lingual Transfer Prediction through Sub-Network Similarity. (arXiv:2310.17166v1 [cs.CL])

    [http://arxiv.org/abs/2310.17166](http://arxiv.org/abs/2310.17166)

    本工作通过利用两种语言之间的子网络相似性作为预测XLT中语言兼容性的代理，提出了一种更有效的模型导向方法，不依赖于外部资源，仅需要候选语言的适量原始文本。

    

    跨语言迁移（XLT）是多语言语言模型的一种新兴能力，当在未包含在微调过程中的语言中评估时，能够在很大程度上保留其在任务上的性能。尽管英语由于其广泛使用，通常被视为各种任务中模型适应的主要语言，但最近的研究表明，通过根据特定条件选择最合适的源语言，可以放大XLT的有效性。在这项工作中，我们提出将两种语言之间的子网络相似性利用为在XLT环境中预测语言兼容性的代理。我们的方法是以模型为导向的，更好地反映了基础模型的内在工作方式。此外，它只需要候选语言的适量原始文本，与大多数依赖外部资源的以前的方法进行区分。在实验中，我们证明了我们的方法是更有效的方法。

    Cross-lingual transfer (XLT) is an emergent ability of multilingual language models that preserves their performance on a task to a significant extent when evaluated in languages that were not included in the fine-tuning process. While English, due to its widespread usage, is typically regarded as the primary language for model adaption in various tasks, recent studies have revealed that the efficacy of XLT can be amplified by selecting the most appropriate source languages based on specific conditions. In this work, we propose the utilization of sub-network similarity between two languages as a proxy for predicting the compatibility of the languages in the context of XLT. Our approach is model-oriented, better reflecting the inner workings of foundation models. In addition, it requires only a moderate amount of raw text from candidate languages, distinguishing it from the majority of previous methods that rely on external resources. In experiments, we demonstrate that our method is mo
    
[^38]: 使用生成型人工智能推动学术写作：框架、技术和注意事项

    Supercharging academic writing with generative AI: framework, techniques, and caveats. (arXiv:2310.17143v1 [cs.CY])

    [http://arxiv.org/abs/2310.17143](http://arxiv.org/abs/2310.17143)

    这篇论文介绍了使用生成型人工智能（AI）提高学术写作质量和效率的原则和方法，包括一个人机协作框架、有效的提示技术和两阶段模型，旨在实现认知卸载和想象刺激的AI辅助写作。

    

    学术写作是研究项目中不可或缺但费时费力的部分。本文介绍了使用生成型人工智能（AI）特别是大型语言模型（LLMs）提高学术写作质量和效率的原则和方法。我们提出了一个人机协作框架，详细阐述了AI在写作中的理论基础（为什么）、过程（如何）和性质（什么）。该框架指出了短期和长期参与AI写作的原因及其基本机制（如认知卸载和想象刺激）。它揭示了AI在整个写作过程中的作用，通过一个人机协作写作的两阶段模型和写作辅助类型和级别的模型表示了AI在写作中的帮助方式。基于该框架，我们描述了在写作常规中整合AI的有效提示技术（大纲、起草和编辑）。

    Academic writing is an indispensable yet laborious part of the research enterprise. This Perspective maps out principles and methods for using generative artificial intelligence (AI), specifically large language models (LLMs), to elevate the quality and efficiency of academic writing. We introduce a human-AI collaborative framework that delineates the rationale (why), process (how), and nature (what) of AI engagement in writing. The framework pinpoints both short-term and long-term reasons for engagement and their underlying mechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals the role of AI throughout the writing process, conceptualized through a two-stage model for human-AI collaborative writing, and the nature of AI assistance in writing, represented through a model of writing-assistance types and levels. Building on this framework, we describe effective prompting techniques for incorporating AI into the writing routine (outlining, drafting, and editing) a
    
[^39]: 符号化规划和代码生成在基于实际对话中的应用

    Symbolic Planning and Code Generation for Grounded Dialogue. (arXiv:2310.17140v1 [cs.CL])

    [http://arxiv.org/abs/2310.17140](http://arxiv.org/abs/2310.17140)

    该论文介绍了一个模块化和可解释的基于实际对话系统，通过组合大型语言模型(LLMs)、符号化规划器和基于实际的代码执行来解决基于任务的对话中的挑战。实验证明该系统在协同参考解析任务上的性能显著优于先前的最先进技术。

    

    大型语言模型(LLMs)在处理和生成文本和代码方面表现出色。然而，在基于任务的对话中，LLMs的适用性有限，因为很难引导其朝着任务目标前进，并且无法处理新颖的基于实际对话。我们提出了一个模块化和可解释的基于实际对话系统，通过组合LLMs和符号化规划器以及基于实际的代码执行，解决了这些缺点。我们的系统由阅读器和规划器组成：阅读器利用LLMs将合作伙伴的话语转换为可执行的代码，调用执行基于实际的函数。转换后的代码的输出被存储以跟踪对话状态，而符号化规划器确定下一个适当的响应。我们在要求高的OneCommon对话任务上评估了我们系统的性能，该任务涉及解决散点图像的协同参考问题。我们的系统在先前的最先进技术的基础上实现了显著的性能提升，包括在人工评估中改善了任务成功率。

    Large language models (LLMs) excel at processing and generating both text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code's output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system's performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluat
    
[^40]: 通过视觉问答对将探测信号融入多模态机器翻译

    Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs. (arXiv:2310.17133v1 [cs.CL])

    [http://arxiv.org/abs/2310.17133](http://arxiv.org/abs/2310.17133)

    本文提出了一种通过视觉问答对的方式将探测信号融入多模态机器翻译中，以增强跨模态交互。实验证明了该方法的有效性。

    

    本文通过对多模态机器翻译(MMT)的深入研究，检验了MMT系统在文本输入完整时对视觉信息的敏感性降低的认识，我们认为这种现象源于跨模态交互不足，而不是图像信息冗余。提出了一种新颖的方法，即从源文本生成并行的视觉问答(VQA)样式对，促进更强大的跨模态交互。使用大型语言模型(LLM)，我们明确地对MMT中的探测信号进行建模，将其转化为VQA样式数据，创建了Multi30K-VQA数据集。引入了MMT-VQA多任务学习框架，将数据集中的显式探测信号融入MMT训练过程。在两个广泛使用的基准测试上的实验结果表明了这种新颖方法的有效性。我们的代码和数据可在以下链接获取：\url{https://github.com/libeineu/MMT-VQA}。

    This paper presents an in-depth study of multimodal machine translation (MMT), examining the prevailing understanding that MMT systems exhibit decreased sensitivity to visual information when text inputs are complete. Instead, we attribute this phenomenon to insufficient cross-modal interaction, rather than image information redundancy. A novel approach is proposed to generate parallel Visual Question-Answering (VQA) style pairs from the source text, fostering more robust cross-modal interaction. Using Large Language Models (LLMs), we explicitly model the probing signal in MMT to convert it into VQA-style data to create the Multi30K-VQA dataset. An MMT-VQA multitask learning framework is introduced to incorporate explicit probing signals from the dataset into the MMT training process. Experimental results on two widely-used benchmarks demonstrate the effectiveness of this novel approach. Our code and data would be available at: \url{https://github.com/libeineu/MMT-VQA}.
    
[^41]: M2C：自动多模态漫画补充的研究

    M2C: Towards Automatic Multimodal Manga Complement. (arXiv:2310.17130v1 [cs.CL])

    [http://arxiv.org/abs/2310.17130](http://arxiv.org/abs/2310.17130)

    该论文提出了一个新的研究任务——多模态漫画补充（M2C），旨在通过共享语义空间来解决手绘漫画中遗漏文字内容的问题。研究首先建立了一个涵盖两种语言的M2C基准数据集，并设计了一种名为MCoT的漫画争议方法和一种基于精细视觉提示的补充模型FVP-M$^{2}$。

    

    多模态漫画分析旨在通过视觉和文本特征增强对漫画的理解，受到自然语言处理和计算机视觉社区的广泛关注。当前，大部分漫画都是手绘的，容易遇到问题，如缺页、文本污染和老化，从而导致遗漏了漫画文字内容，严重阻碍了人类的理解。换句话说，多模态漫画补充（M2C）任务尚未得到研究，该任务旨在通过为视觉和语言理解提供共享语义空间来处理上述问题。为此，我们首先通过建立一个涵盖两种语言的新M2C基准数据集，提出了多模态漫画补充任务。首先，我们设计了一种名为MCoT的漫画争议方法，通过使用大型语言模型在漫画中挖掘事件知识。然后，提出了一种有效的基准方法FVP-M$^{2}$，使用精细的视觉提示来支持漫画补充。

    Multimodal manga analysis focuses on enhancing manga understanding with visual and textual features, which has attracted considerable attention from both natural language processing and computer vision communities. Currently, most comics are hand-drawn and prone to problems such as missing pages, text contamination, and aging, resulting in missing comic text content and seriously hindering human comprehension. In other words, the Multimodal Manga Complement (M2C) task has not been investigated, which aims to handle the aforementioned issues by providing a shared semantic space for vision and language understanding. To this end, we first propose the Multimodal Manga Complement task by establishing a new M2C benchmark dataset covering two languages. First, we design a manga argumentation method called MCoT to mine event knowledge in comics with large language models. Then, an effective baseline FVP-M$^{2}$ using fine-grained visual prompts is proposed to support manga complement. Extensi
    
[^42]: 用于事实探测的测试时间数据增强

    Test-time Augmentation for Factual Probing. (arXiv:2310.17121v1 [cs.CL])

    [http://arxiv.org/abs/2310.17121](http://arxiv.org/abs/2310.17121)

    本文提出了一种用于事实探测的测试时间数据增强（TTA）方法，通过在测试时间自动增加和组装提示，减少关于提示变化的敏感性。实验结果显示，TTA可以提高模型的校准性，并且在某些模型中可以提高预测准确性。然而，对于其他模型，TTA可能导致性能下降，主要挑战在于产生高质量的提示变化。

    

    事实探测是一种使用提示来测试语言模型是否“知道”某些世界知识事实的方法。事实探测中的一个问题是，对提示进行微小的更改可能会导致模型输出的巨大变化。先前的研究通过文本挖掘或微调来优化提示以缓解这个问题。然而，这些方法是关系特定的，并且不能推广到未见过的关系类型。在这里，我们提出使用测试时间数据增强（TTA）作为一种关系不可知的方法，在测试时间自动增加和组装提示来减少对提示变化的敏感性。实验证明，TTA可以提高模型的校准性，即使用TTA，模型的置信度更好地反映了预测准确性。对于某些模型，预测准确性有所提高，但对于其他模型，TTA导致了性能下降。错误分析确定了产生高质量提示变化的困难作为TTA的主要挑战。

    Factual probing is a method that uses prompts to test if a language model "knows" certain world knowledge facts. A problem in factual probing is that small changes to the prompt can lead to large changes in model output. Previous work aimed to alleviate this problem by optimizing prompts via text mining or fine-tuning. However, such approaches are relation-specific and do not generalize to unseen relation types. Here, we propose to use test-time augmentation (TTA) as a relation-agnostic method for reducing sensitivity to prompt variations by automatically augmenting and ensembling prompts at test time. Experiments show improved model calibration, i.e., with TTA, model confidence better reflects prediction accuracy. Improvements in prediction accuracy are observed for some models, but for other models, TTA leads to degradation. Error analysis identifies the difficulty of producing high-quality prompt variations as the main challenge for TTA.
    
[^43]: 使用语言模型对半结构化和非结构化对话数据进行主题分段

    Topic Segmentation of Semi-Structured and Unstructured Conversational Datasets using Language Models. (arXiv:2310.17120v1 [cs.CL])

    [http://arxiv.org/abs/2310.17120](http://arxiv.org/abs/2310.17120)

    本文通过对非结构化文本进行分析，揭示了目前主题分段模型在此类数据上的泛化能力不足，并提出了从头开始训练相对小规模的目标数据集来改善分段结果的方法。实证评估表明使用多种损失函数可以减轻非结构化对话数据集的不平衡效应。

    

    在自然语言处理中，将文档或对话根据其语义结构分解为多个连续片段是一个重要且具有挑战性的问题，可以帮助许多下游任务。然而，当前关于主题分段的研究往往集中在结构化文本的分段上。在本文中，我们全面分析了最先进的主题分段模型在非结构化文本上的泛化能力。我们发现：（a）目前在大规模结构化文本语料库（如Wiki-727K）上进行预训练的策略对于在非结构化对话数据上的可传递性并不有帮助。（b）从头开始使用相对小规模的目标非结构化领域数据集训练能显著提高分段结果。我们通过尝试多种损失函数来进行我们的主题分段方法的强化测试，以减轻非结构化对话数据集的不平衡效应。我们的实证评估表明Fo

    Breaking down a document or a conversation into multiple contiguous segments based on its semantic structure is an important and challenging problem in NLP, which can assist many downstream tasks. However, current works on topic segmentation often focus on segmentation of structured texts. In this paper, we comprehensively analyze the generalization capabilities of state-of-the-art topic segmentation models on unstructured texts. We find that: (a) Current strategies of pre-training on a large corpus of structured text such as Wiki-727K do not help in transferability to unstructured conversational data. (b) Training from scratch with only a relatively small-sized dataset of the target unstructured domain improves the segmentation results by a significant margin. We stress-test our proposed Topic Segmentation approach by experimenting with multiple loss functions, in order to mitigate effects of imbalance in unstructured conversational datasets. Our empirical evaluation indicates that Fo
    
[^44]: FLEEK: 从外部知识中检测和修正事实错误

    FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge. (arXiv:2310.17119v1 [cs.CL])

    [http://arxiv.org/abs/2310.17119](http://arxiv.org/abs/2310.17119)

    本论文提出了FLEEK，一个能自动从文本中提取事实主张并使用外部知识源进行评估和修正的工具，以减少文本中的事实错误。初步实证评估显示FLEEK具有潜力。

    

    检测文本信息中的事实错误对于做出明智决策至关重要，无论是由大规模语言模型（LLM）生成的还是由人类策划的。LLMs无法将它们的主张归因于外部知识，且易于产生虚构，这使得依赖它们的回答变得困难。人类在写作过程中也容易出现事实错误。由于手动检测和修正事实错误需要大量的人力投入，因此开发自动化方法可以极大地减少人力工作。我们提出了FLEEK，一个原型工具，它可以自动从文本中提取事实主张，从外部知识源收集证据，评估每个主张的真实性，并使用收集的证据提出修正建议。初步的实证评估显示FLEEK在事实错误检测方面具有潜力（77-85％的F1）。FLEEK的视频演示可在https://youtu.be/NapJFUlkPdQ中找到。

    Detecting factual errors in textual information, whether generated by large language models (LLM) or curated by humans, is crucial for making informed decisions. LLMs' inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficult to rely on their responses. Humans, too, are prone to factual errors in their writing. Since manual detection and correction of factual errors is labor-intensive, developing an automatic approach can greatly reduce human effort. We present FLEEK, a prototype tool that automatically extracts factual claims from text, gathers evidence from external knowledge sources, evaluates the factuality of each claim, and suggests revisions for identified errors using the collected evidence. Initial empirical evaluation on fact error detection (77-85\% F1) shows the potential of FLEEK. A video demo of FLEEK can be found at https://youtu.be/NapJFUlkPdQ.
    
[^45]: Transformers学会了高阶优化方法用于上下文学习：一项与线性模型的研究

    Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models. (arXiv:2310.17086v1 [cs.LG])

    [http://arxiv.org/abs/2310.17086](http://arxiv.org/abs/2310.17086)

    Transformers学会了高阶优化方法，用于上下文学习，通过实现类似于迭代牛顿法的算法，而不是梯度下降。

    

    Transformers在上下文学习中表现出色，但是它们是如何进行上下文学习仍然是一个谜。最近的研究表明，Transformers可能通过内部运行梯度下降，即一阶优化方法，来进行上下文学习。本文中，我们展示了Transformers学会了实现高阶优化方法来进行上下文学习。我们以上下文线性回归为重点，展示了Transformers学会了实现一个非常类似于迭代牛顿法的算法，而不是梯度下降。从实证上来看，我们展示了连续的Transformer层的预测与牛顿法的不同迭代非常接近，每个中间层大致计算了3次迭代。相比之下，需要指数级的梯度下降步骤才能匹配额外的Transformer层；这表明Transformers具有相当的收敛速率。

    Transformers are remarkably good at in-context learning (ICL) -- learning from demonstrations without parameter updates -- but how they perform ICL remains a mystery. Recent work suggests that Transformers may learn in-context by internally running Gradient Descent, a first-order optimization method. In this paper, we instead demonstrate that Transformers learn to implement higher-order optimization methods to perform ICL. Focusing on in-context linear regression, we show that Transformers learn to implement an algorithm very similar to Iterative Newton's Method, a higher-order optimization method, rather than Gradient Descent. Empirically, we show that predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations. In contrast, exponentially more Gradient Descent steps are needed to match an additional Transformers layer; this suggests that Transformers have an comparable rate of conv
    
[^46]: math-PVS:一个将科学出版物映射到PVS理论的大型语言模型框架

    math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories. (arXiv:2310.17064v1 [cs.AI])

    [http://arxiv.org/abs/2310.17064](http://arxiv.org/abs/2310.17064)

    该研究调查了将大型语言模型应用于形式化高级数学概念的可行性，并提出了一个可以批判性地审查和检查研究论文中数学推理的框架。

    

    随着人工智能在各种应用领域的广泛采用，它在数学发现方面有巨大潜力，可以引导猜想生成，构造反例，协助形式化数学，并发现不同数学领域之间的联系，等等。尽管之前的工作利用计算机进行详尽的数学证明搜索，但基于大型语言模型的最近努力致力于将计算平台定位为数学研究过程中的合作贡献者。尽管目前的语言模型在逻辑和数学任务方面存在局限性，但越来越多的人对将定理证明系统与基础模型结合起来的兴趣日益增长。本研究调查了LLMs在形式化高级数学概念方面的适用性，并提出了一个能够批判性地审查和检查研究论文中数学推理的框架。

    As artificial intelligence (AI) gains greater adoption in a wide variety of applications, it has immense potential to contribute to mathematical discovery, by guiding conjecture generation, constructing counterexamples, assisting in formalizing mathematics, and discovering connections between different mathematical areas, to name a few.  While prior work has leveraged computers for exhaustive mathematical proof search, recent efforts based on large language models (LLMs) aspire to position computing platforms as co-contributors in the mathematical research process. Despite their current limitations in logic and mathematical tasks, there is growing interest in melding theorem proving systems with foundation models. This work investigates the applicability of LLMs in formalizing advanced mathematical concepts and proposes a framework that can critically review and check mathematical reasoning in research papers. Given the noted reasoning shortcomings of LLMs, our approach synergizes the 
    
[^47]: 提升语言模型生成中的常识能力：利用黑盒控制增强

    BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs' Generation. (arXiv:2310.17054v1 [cs.CL])

    [http://arxiv.org/abs/2310.17054](http://arxiv.org/abs/2310.17054)

    本文提出了一个计算高效的框架，通过利用黑盒控制来引导冻结的预训练语言模型（PTLM）生成更加常识性的文本输出。

    

    大型语言模型（LLM）如GPT-3已经展示了生成连贯且上下文相关的文本的强大能力。然而，在它们的成功之中，一个关键问题仍然存在：它们生成的输出有时仍然缺乏常识。此外，如果不可行的话，将整个LLM进行微调以获得更加常识性的输出是计算上代价昂贵的。在本文中，我们提出了一个计算高效的框架，将一个冻结的预训练语言模型（PTLM）引导向更加常识性的生成（即以有意义的方式产生包含一系列概念的合理输出）。具体地，我们首先构建了一个无需参考的评估器，通过将句子与一个动态常识知识库在四个不同关系方面相连来为句子分配一个常识得分。然后，我们使用评分器作为常识知识的参考，扩展了名为NADO的可控生成方法，训练了一个辅助头部来引导一个。

    Large language models (LLMs) such as GPT-3 have demonstrated a strong capability to generate coherent and contextually relevant text. However, amidst their successes, a crucial issue persists: their generated outputs still lack commonsense at times. Moreover, fine-tuning the entire LLM towards more commonsensical outputs is computationally expensive if not infeasible. In this paper, we present a computation-efficient framework that steers a frozen Pre-Trained Language Model (PTLM) towards more commonsensical generation (i.e., producing a plausible output that incorporates a list of concepts in a meaningful way). Specifically, we first construct a reference-free evaluator that assigns a sentence with a commonsensical score by grounding the sentence to a dynamic commonsense knowledge base from four different relational aspects. We then use the scorer as the oracle for commonsense knowledge, and extend the controllable generation method called NADO to train an auxiliary head that guides a
    
[^48]: 关于语言编码器的手术微调

    On Surgical Fine-tuning for Language Encoders. (arXiv:2310.17041v1 [cs.CL])

    [http://arxiv.org/abs/2310.17041](http://arxiv.org/abs/2310.17041)

    本文表明，对于不同的下游语言任务，只对语言编码器的部分层进行细调即可获得接近甚至优于细调所有层的性能。通过提出一种高效度量方法，我们证明了该方法可以选择性微调导致强大下游性能的层。研究突出表明，任务特定信息通常局部化在少数层内，只调整这些层就足够了。

    

    细调预训练的神经语言编码器的所有层（使用所有参数或使用参数高效的方法）往往是将其适应于新任务的默认方法。我们展示了证据，对于不同的下游语言任务，仅细调部分层即可获得接近甚至优于细调语言编码器的所有层的性能。我们提出了一种基于Fisher信息矩阵的对角线（FIM评分）的高效度量方法，用于选择用于选择性微调的候选层。我们在GLUE和SuperGLUE任务以及不同的语言编码器上经验性地展示了，这个度量可以有效选择导致强大下游性能的层。我们的工作突出了与给定的下游任务对应的任务特定信息通常局部化在少数层内，只调整这些层对于强大的性能就足够了。

    Fine-tuning all the layers of a pre-trained neural language encoder (either using all the parameters or using parameter-efficient methods) is often the de-facto way of adapting it to a new task. We show evidence that for different downstream language tasks, fine-tuning only a subset of layers is sufficient to obtain performance that is close to and often better than fine-tuning all the layers in the language encoder. We propose an efficient metric based on the diagonal of the Fisher information matrix (FIM score), to select the candidate layers for selective fine-tuning. We show, empirically on GLUE and SuperGLUE tasks and across distinct language encoders, that this metric can effectively select layers leading to a strong downstream performance. Our work highlights that task-specific information corresponding to a given downstream task is often localized within a few layers, and tuning only those is sufficient for strong performance. Additionally, we demonstrate the robustness of the 
    
[^49]: 通过声音提示为语音助手提供后续问题建议

    Follow-on Question Suggestion via Voice Hints for Voice Assistants. (arXiv:2310.17034v1 [cs.CL])

    [http://arxiv.org/abs/2310.17034](http://arxiv.org/abs/2310.17034)

    本论文解决了在语音助手中通过声音提示为用户提供后续问题建议的新任务，并提出了一种使用序列到序列 Transformer 的方法。该方法在一个新的数据集上进行了评估，结果显示出良好的效果。

    

    声音助手（如Alexa或Siri）的使用已经快速增长，允许用户通过语音搜索即时获取信息。查询建议是基于屏幕的搜索体验的标准功能，允许用户探索更多话题。然而，在基于语音的设置中实现这一点并不是简单的任务。为了实现这一点，我们解决了一项新的任务，即通过简明自然的声音提示建议用户提问后续问题。我们定义了这个任务，将其基于句法理论，并概述了口语提示的语言期望。我们提出了基线模型和一种使用序列到序列 Transformer 的方法，从一系列问题中生成声音提示。通过一个由6681个输入问题和人工编写的提示组成的新数据集，我们使用自动评估指标和人工评估来评估模型。结果表明，简单地将建议的问题连接起来会产生低质量的声音提示。我们的方法采用了一种基于语言动机的预训练方法。

    The adoption of voice assistants like Alexa or Siri has grown rapidly, allowing users to instantly access information via voice search. Query suggestion is a standard feature of screen-based search experiences, allowing users to explore additional topics. However, this is not trivial to implement in voice-based settings. To enable this, we tackle the novel task of suggesting questions with compact and natural voice hints to allow users to ask follow-up questions.  We define the task, ground it in syntactic theory and outline linguistic desiderata for spoken hints. We propose baselines and an approach using sequence-to-sequence Transformers to generate spoken hints from a list of questions. Using a new dataset of 6681 input questions and human written hints, we evaluated the models with automatic metrics and human evaluation. Results show that a naive approach of concatenating suggested questions creates poor voice hints. Our approach, which applies a linguistically-motivated pretrainin
    
[^50]: 受控解码来自语言模型

    Controlled Decoding from Language Models. (arXiv:2310.17022v1 [cs.LG])

    [http://arxiv.org/abs/2310.17022](http://arxiv.org/abs/2310.17022)

    本论文提出了一种名为受控解码（CD）的离策略强化学习方法，用于控制语言模型的生成，以达到高回报的结果。CD通过前缀评分器来引导生成，可以在推理时预测预期回报，并且具有模块化设计，可用于解决多目标强化学习问题，而不增加复杂性。

    

    我们提出了一种新颖的离策略强化学习方法，称为受控解码（CD），用于控制自回归语言模型的生成，以获得高回报的结果。CD通过值函数来解决离策略强化学习问题，该值函数被称为前缀评分器。前缀评分器在推理时用于引导生成向更高回报的结果。我们展示了前缀评分器可以从（可能是）离策略数据中训练出来，用于预测从部分解码的响应继续解码时的预期回报。我们在Reddit对话语料库上经验证明，CD作为一种控制机制是有效的。我们还展示了CD设计的模块化使其能够有效解决多目标强化学习问题，而不会增加任何复杂性。最后，我们展示了CD可以以一种新颖的分块方式在推理时应用，同样无需任何额外的操作。

    We propose controlled decoding (CD), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. CD solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. The prefix scorer is used at inference time to steer the generation towards higher reward outcomes. We show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. We empirically demonstrate that CD is effective as a control mechanism on Reddit conversations corpus. We also show that the modularity of the design of CD makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. Finally, we show that CD can be applied in a novel blockwise fashion at inference-time, again without the need for any 
    
[^51]: 使用大语言模型有条件地组合机器人技能

    Conditionally Combining Robot Skills using Large Language Models. (arXiv:2310.17019v1 [cs.LG])

    [http://arxiv.org/abs/2310.17019](http://arxiv.org/abs/2310.17019)

    这篇论文介绍了一个扩展的Meta-World基准，称为“语言世界”，允许大型语言模型在模拟机器人环境中使用自然语言查询和脚本技能。同时，引入了计划条件行为克隆（PCBC）的方法，通过端到端演示对高级计划进行微调。实验结果表明，在少样本情况下，PCBC在语言世界中能够实现强大的性能。

    

    这篇论文结合了两个贡献。首先，我们介绍了Meta-World基准的一个扩展，称为“语言世界”，它允许一个大型语言模型在一个模拟机器人环境中使用半结构化自然语言查询和用自然语言描述的脚本技能。通过使用与Meta-World相同的任务集，可以轻松比较语言世界和Meta-World的结果，从而对最近使用大型语言模型（LLMs）和使用深度强化学习的方法进行比较。其次，我们介绍了一种称为计划条件行为克隆（PCBC）的方法，它允许对高级计划的行为进行微调，使用端到端演示。使用语言世界，我们展示了PCBC能够在各种少样本情况下取得卓越的性能，通常只需一个演示即可实现任务的泛化。我们将语言世界作为开源软件提供，网址是https://...

    This paper combines two contributions. First, we introduce an extension of the Meta-World benchmark, which we call "Language-World," which allows a large language model to operate in a simulated robotic environment using semi-structured natural language queries and scripted skills described using natural language. By using the same set of tasks as Meta-World, Language-World results can be easily compared to Meta-World results, allowing for a point of comparison between recent methods using Large Language Models (LLMs) and those using Deep Reinforcement Learning. Second, we introduce a method we call Plan Conditioned Behavioral Cloning (PCBC), that allows finetuning the behavior of high-level plans using end-to-end demonstrations. Using Language-World, we show that PCBC is able to achieve strong performance in a variety of few-shot regimes, often achieving task generalization with as little as a single demonstration. We have made Language-World available as open-source software at https
    
[^52]: 基于计算机科学和医学视角的心理健康对话代理综合调查

    An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives. (arXiv:2310.17017v1 [cs.CL])

    [http://arxiv.org/abs/2310.17017](http://arxiv.org/abs/2310.17017)

    本研究使用PRISMA框架综述了计算机科学和医学领域发表的534篇论文，发现了136篇关键论文，涵盖了心理健康对话代理的多种建模和实验设计技术特征。

    

    心理健康对话代理（也称为聊天机器人）被广泛研究，因其提供对那些面临心理健康挑战的人可及性支持的潜力。之前对该主题的调查主要考虑在计算机科学或医学领域发表的论文，导致理解上的分裂，阻碍了两个领域之间有益知识的共享。为了弥合这一鸿沟，我们使用PRISMA框架进行全面的文献综述，审查了534篇在计算机科学和医学领域发表的论文。我们的系统综述揭示了136篇关键论文，涵盖了建立与心理健康相关的对话代理的多种建模和实验设计技术特征。我们发现，计算机科学论文侧重于LLM技术和使用自动度量评估响应质量，对应用应用关注较少，而医学论文则使用基于规则的对话代理和结果度量来衡量参与者的健康结果。

    Mental health conversational agents (a.k.a. chatbots) are widely studied for their potential to offer accessible support to those experiencing mental health challenges. Previous surveys on the topic primarily consider papers published in either computer science or medicine, leading to a divide in understanding and hindering the sharing of beneficial knowledge between both domains. To bridge this gap, we conduct a comprehensive literature review using the PRISMA framework, reviewing 534 papers published in both computer science and medicine. Our systematic review reveals 136 key papers on building mental health-related conversational agents with diverse characteristics of modeling and experimental design techniques. We find that computer science papers focus on LLM techniques and evaluating response quality using automated metrics with little attention to the application while medical papers use rule-based conversational agents and outcome metrics to measure the health outcomes of parti
    
[^53]: 这就像那样阅读：用于可解释性自然语言处理的深度学习

    This Reads Like That: Deep Learning for Interpretable Natural Language Processing. (arXiv:2310.17010v1 [cs.CL])

    [http://arxiv.org/abs/2310.17010](http://arxiv.org/abs/2310.17010)

    本研究将原型网络方法扩展到了自然语言处理领域，引入了一种学习的加权相似度度量和事后可解释性机制，通过聚焦于句子嵌入的重要维度来提高相似度计算，并改善了预测性能和解释准确性。

    

    原型学习是一种流行的机器学习方法，用于解释性决策，通过学习的原型的相似性来对新数据进行分类。尽管它主要应用于计算机视觉，但在这项工作中，我们建立在之前的研究基础上，进一步探索了原型网络在自然语言处理中的扩展。我们引入了一种学习的加权相似度度量，通过聚焦于预训练的句子嵌入的信息维度来增强相似度计算。此外，我们提出了一种事后可解释性机制，从原型和输入句子中提取与预测相关的单词。最后，我们通过实验证明，我们提出的方法不仅改善了在AG News和RT Polarity数据集上的预测性能，而且与基于合理性的递归卷积相比，还提高了解释的准确性。

    Prototype learning, a popular machine learning method designed for inherently interpretable decisions, leverages similarities to learned prototypes for classifying new data. While it is mainly applied in computer vision, in this work, we build upon prior research and further explore the extension of prototypical networks to natural language processing. We introduce a learned weighted similarity measure that enhances the similarity computation by focusing on informative dimensions of pre-trained sentence embeddings. Additionally, we propose a post-hoc explainability mechanism that extracts prediction-relevant words from both the prototype and input sentences. Finally, we empirically demonstrate that our proposed method not only improves predictive performance on the AG News and RT Polarity datasets over a previous prototype-based approach, but also improves the faithfulness of explanations compared to rationale-based recurrent convolutions.
    
[^54]: 质量>数量：基于基础模型的封闭领域抽取式问答的合成语料库

    Quality > Quantity: Synthetic Corpora from Foundation Models for Closed-Domain Extractive Question Answering. (arXiv:2310.16995v1 [cs.CL])

    [http://arxiv.org/abs/2310.16995](http://arxiv.org/abs/2310.16995)

    这项工作研究了封闭领域的抽取式问答，引入了有针对性的预训练的概念，并使用Galactica生成了合成的"有针对性"的语料库。

    

    领域适应是将模型在一个领域进行训练，然后应用于另一个领域的过程，在机器学习领域得到了广泛研究。虽然从头开始训练一个特定领域的基础模型(FM)是一个选择，但最近的方法集中在调整预训练的FM以满足特定领域的任务。然而，我们的实验发现，无论是哪种方法，在目标领域都无法始终达到最先进( SOTA)的结果。在这项工作中，我们研究封闭领域内的抽取式问答，并引入了有针对性的预训练的概念。这意味着确定和生成相关数据，进一步预训练我们的模型，而不是传统的利用在广泛数据上训练的特定领域的FM的理念。我们提出的框架使用Galactica生成与特定写作风格和主题(如研究论文和放射学报告)相一致的合成"有针对性"的语料库。这个过程可以看作是一种知识

    Domain adaptation, the process of training a model in one domain and applying it to another, has been extensively explored in machine learning. While training a domain-specific foundation model (FM) from scratch is an option, recent methods have focused on adapting pre-trained FMs for domain-specific tasks. However, our experiments reveal that either approach does not consistently achieve state-of-the-art (SOTA) results in the target domain. In this work, we study extractive question answering within closed domains and introduce the concept of targeted pre-training. This involves determining and generating relevant data to further pre-train our models, as opposed to the conventional philosophy of utilizing domain-specific FMs trained on a wide range of data. Our proposed framework uses Galactica to generate synthetic, ``targeted'' corpora that align with specific writing styles and topics, such as research papers and radiology reports. This process can be viewed as a form of knowledge 
    
[^55]: 机器生成的文本能够被多好地识别出来，能够训练语言模型来避免识别吗？

    How well can machine-generated texts be identified and can language models be trained to avoid identification?. (arXiv:2310.16992v1 [cs.CL])

    [http://arxiv.org/abs/2310.16992](http://arxiv.org/abs/2310.16992)

    本研究针对机器生成的文本进行了识别，发现在较高温度值下，浅层学习算法的检测准确率较低，而基于转换器的分类器的准确率高。通过强化学习方法改进生成模型，可以成功躲避基于BERT的文本识别。

    

    随着GPT-3、GPT-NeoX和OPT等生成预训练转换模型的崛起，区分人类生成的文本和机器生成的文本变得很重要。我们对五个不同的语言模型进行了精细调整，生成了合成推文，发现浅层学习分类算法（如朴素贝叶斯）的检测准确率在0.6到0.8之间。当在文本生成过程中使用较高的温度值时，浅层学习分类器与基于人类的检测有所不同，导致较低的检测率。人类更注重语言的可接受性，在较低的温度值下，语言的可接受性往往较高。相反，基于转换器的分类器的准确率在0.9及以上。我们发现使用强化学习方法来改进我们的生成模型可以成功躲避基于BERT的分类器，其检测准确率为0.15或更低。

    With the rise of generative pre-trained transformer models such as GPT-3, GPT-NeoX, or OPT, distinguishing human-generated texts from machine-generated ones has become important. We refined five separate language models to generate synthetic tweets, uncovering that shallow learning classification algorithms, like Naive Bayes, achieve detection accuracy between 0.6 and 0.8.  Shallow learning classifiers differ from human-based detection, especially when using higher temperature values during text generation, resulting in a lower detection rate. Humans prioritize linguistic acceptability, which tends to be higher at lower temperature values. In contrast, transformer-based classifiers have an accuracy of 0.9 and above. We found that using a reinforcement learning approach to refine our generative models can successfully evade BERT-based classifiers with a detection accuracy of 0.15 or less.
    
[^56]: STEER: 语义转向扩展识别用于语音助手

    STEER: Semantic Turn Extension-Expansion Recognition for Voice Assistants. (arXiv:2310.16990v1 [cs.CL])

    [http://arxiv.org/abs/2310.16990](http://arxiv.org/abs/2310.16990)

    STEER是一个用于语音助手的语义转向扩展识别模型，通过训练数据集和启发式规则进行转向意图预测，并在实验中展现出了良好的性能。

    

    在语音助手系统的背景下，转向是指用户发出后续命令，试图引导或澄清之前的指令的现象。我们提出了STEER，一个转向检测模型，用于预测后续命令是否是用户企图转向之前指令的尝试。由于冷启动问题，构建用于转向案例的训练数据集带来了挑战。为了克服这个问题，我们开发了启发式规则来采样选择加入使用数据，近似正负样本而无需任何标注。我们的实验结果显示了识别转向意图的良好性能，在我们采样的数据上超过95%的准确率。此外，STEER结合我们的采样策略，在人工评估集上表现出了强大的零样本性能，有效地与真实的转向场景相匹配。除了仅依赖用户的转录作为输入，我们还引入了STEER+，这是模型的增强版本。

    In the context of a voice assistant system, steering refers to the phenomenon in which a user issues a follow-up command attempting to direct or clarify a previous turn. We propose STEER, a steering detection model that predicts whether a follow-up turn is a user's attempt to steer the previous command. Constructing a training dataset for steering use cases poses challenges due to the cold-start problem. To overcome this, we developed heuristic rules to sample opt-in usage data, approximating positive and negative samples without any annotation. Our experimental results show promising performance in identifying steering intent, with over 95% accuracy on our sampled data. Moreover, STEER, in conjunction with our sampling strategy, aligns effectively with real-world steering scenarios, as evidenced by its strong zero-shot performance on a human-graded evaluation set. In addition to relying solely on user transcripts as input, we introduce STEER+, an enhanced version of the model. STEER+ 
    
[^57]: 使用人物互动图来理解当代文学作品中的社会结构——对有影响力的孟加拉作家的半个世纪的年代的研究

    Understanding Social Structures from Contemporary Literary Fiction using Character Interaction Graph -- Half Century Chronology of Influential Bengali Writers. (arXiv:2310.16968v1 [cs.CL])

    [http://arxiv.org/abs/2310.16968](http://arxiv.org/abs/2310.16968)

    这项研究利用人物互动图和自然语言处理的技术来探索当代文化对文学作品中社会结构的影响。

    

    社会结构和现实世界的事件经常影响当代文学作品。现有的文学分析研究通过对故事进行手动的批判性分析来解释这些现实世界的现象。传统的自然语言处理（NLP）方法，包括情感分析、故事摘要和主题建模，在分析和识别虚构作品中的相似性方面表现出了重要的有效性。然而，在虚构作品中复杂的人物互动动态需要更加细致入微的方法，这种方法结合了可视化技术。人物互动图（或网络）成为了从虚构领域中提取信息和可视化的高度合适的手段。因此，我们利用具有NLP特征的人物互动图来探索有关当代文化对文学作品领域的影响的多样性社会问题。我们的研究涉及构建...

    Social structures and real-world incidents often influence contemporary literary fiction. Existing research in literary fiction analysis explains these real-world phenomena through the manual critical analysis of stories. Conventional Natural Language Processing (NLP) methodologies, including sentiment analysis, narrative summarization, and topic modeling, have demonstrated substantial efficacy in analyzing and identifying similarities within fictional works. However, the intricate dynamics of character interactions within fiction necessitate a more nuanced approach that incorporates visualization techniques. Character interaction graphs (or networks) emerge as a highly suitable means for visualization and information retrieval from the realm of fiction. Therefore, we leverage character interaction graphs with NLP-derived features to explore a diverse spectrum of societal inquiries about contemporary culture's impact on the landscape of literary fiction. Our study involves constructing
    
[^58]: 批评驱动解码以减轻数据到文本生成中的幻觉问题

    Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation. (arXiv:2310.16964v1 [cs.CL])

    [http://arxiv.org/abs/2310.16964](http://arxiv.org/abs/2310.16964)

    本文提出了一种新的方法来减轻神经数据到文本生成中的幻觉问题，通过组合生成语言模型和特殊的文本批评家分类器的输出来指导生成过程。方法不需要对模型架构或训练过程进行改动，可与任何基于单词概率进行模型和解码操作的模型结合使用，并且不需要额外的训练数据。实验证明该方法在基准测试上优于基线。

    

    文本在输入中无法得到实际支持的幻觉是神经数据到文本生成中已知的问题。虽然已经有许多方法被提出来减轻这个问题，但它们通常需要修改模型架构或收集额外的数据，因此不能轻易地应用到现有模型中。在本文中，我们探索了一种通过组合生成语言模型（LM）的概率输出与特殊的“文本批评家”分类器的输出来减轻幻觉的新方法，后者通过评估输入数据与到目前为止生成的文本之间的匹配来引导生成过程。我们的方法不需要对基础LM的架构或训练过程进行任何改动，因此可以与任何基于单词概率进行模型和解码操作的模型结合使用。批评家不需要额外的训练数据，使用基础LM的训练数据和合成的负面例子。我们的实验结果表明，我们的方法在WebNLG和OpenDialKG基准上较基线有所改善。

    Hallucination of text ungrounded in the input is a well-known problem in neural data-to-text generation. Many methods have been proposed to mitigate it, but they typically require altering model architecture or collecting additional data, and thus cannot be easily applied to an existing model. In this paper, we explore a new way to mitigate hallucinations by combining the probabilistic output of a generator language model (LM) with the output of a special "text critic" classifier, which guides the generation by assessing the match between the input data and the text generated so far. Our method does not need any changes to the underlying LM's architecture or training procedure and can thus be combined with any model and decoding operating on word probabilities. The critic does not need any additional training data, using the base LM's training data and synthetic negative examples. Our experimental results show that our method improves over the baseline on the WebNLG and OpenDialKG benc
    
[^59]: Zephyr: 直接蒸馏语言模型对齐

    Zephyr: Direct Distillation of LM Alignment. (arXiv:2310.16944v1 [cs.LG])

    [http://arxiv.org/abs/2310.16944](http://arxiv.org/abs/2310.16944)

    本论文提出了一种直接蒸馏的语言模型对齐方法，使用AI反馈数据进行优化，在聊天任务上显著提高了意图对齐的效果，该方法只需要几个小时的训练时间且无需人工注释，实验证明在7B参数模型上超过了现有最好的开放访问模型。

    

    我们的目标是生成一个与用户意图对齐的较小型语言模型。之前的研究表明，对较大的模型应用蒸馏的监督微调能显著提高任务准确性；然而，这些模型没有对齐，即它们不能很好地响应自然提示。为了蒸馏这个属性，我们尝试使用来自AI反馈（AIF）的偏好数据。从一个由教师模型排名的输出数据集开始，我们应用蒸馏的直接偏好优化（dDPO）来学习一个具有显著改进意图对齐的聊天模型。这种方法只需要几个小时的训练时间，在微调过程中无需任何额外的采样。最终结果Zephyr-7B在7B参数模型的聊天基准上取得了最先进的结果，并且无需人工注释。特别是，在MT-Bench上的结果显示，Zephyr-7B超越了最好的开放访问RLHF模型Llama2-Chat-70B。系统的代码、模型、数据和教程都可以获取。

    We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are availabl
    
[^60]: 跨多种编程语言的学习转移

    Learning Transfers over Several Programming Languages. (arXiv:2310.16937v1 [cs.CL])

    [http://arxiv.org/abs/2310.16937](http://arxiv.org/abs/2310.16937)

    这篇论文研究了使用跨语言迁移学习提高编程语言模型性能的问题，并进行了广泛实验验证。该研究表明，跨语言迁移学习在编程语言领域具有潜力，可以帮助低资源语言的用户受益于大规模语言模型。

    

    大规模语言模型（LLM）在提高高资源编程语言开发者生产力方面近年来取得了显著的进展。这些模型使用两种类型的数据：大量的无标签代码样本用于预训练，相对较少的带标签代码样本用于微调或上下文学习。然而，许多编程语言是低资源的，缺乏大多数任务的带标签样本，甚至缺乏无标签样本。因此，低资源语言（例如遗留或新语言）的用户无法享受到LLM的好处。跨语言迁移学习使用源语言的数据来提高模型在目标语言上的性能。它在自然语言领域已经得到了广泛研究，但在编程语言领域却受到了很少关注。本文使用基于Transformer的LLM和11到41种编程语言进行了广泛的实验，探讨了以下问题。

    Large language models (LLMs) have recently become remarkably good at improving developer productivity for high-resource programming languages. These models use two kinds of data: large amounts of unlabeled code samples for pretraining and relatively smaller amounts of labeled code samples for fine-tuning or in-context learning. Unfortunately, many programming languages are low-resource, lacking labeled samples for most tasks and often even lacking unlabeled samples. Therefore, users of low-resource languages (e.g., legacy or new languages) miss out on the benefits of LLMs. Cross-lingual transfer learning uses data from a source language to improve model performance on a target language. It has been well-studied for natural languages, but has received little attention for programming languages. This paper reports extensive experiments on four tasks using a transformer-based LLM and 11 to 41 programming languages to explore the following questions. First, how well cross-lingual transfer 
    
[^61]: CL-MASR：一个用于多语音自动语音识别（ASR）的持续学习基准

    CL-MASR: A Continual Learning Benchmark for Multilingual ASR. (arXiv:2310.16931v1 [cs.CL])

    [http://arxiv.org/abs/2310.16931](http://arxiv.org/abs/2310.16931)

    CL-MASR是一个用于研究多语音ASR的持续学习基准，提供了一系列多样化的持续学习方法，并解决了学习新语言时遗忘问题。

    

    现代多语音自动语音识别（ASR）系统，如Whisper，使得能够使用单个模型转录多种语言的音频成为可能。然而，当前最先进的ASR模型通常在单个语言或多任务设置下进行评估，忽视了持续学习新语言的挑战。关于如何添加新语言而不丢失先前数据中有价值信息的研究不足。此外，现有的持续学习基准主要集中在视觉和语言任务上，对于多语音ASR的持续学习还未深入探索。为了填补这一空白，我们提出CL-MASR，一个专为在持续学习环境中研究多语音ASR而设计的基准。CL-MASR提供了一系列多样化的持续学习方法，基于大规模预训练的ASR模型实现，并提供常见的指标来评估学习新语言的效果，同时解决了灾难性遗忘问题。

    Modern multilingual automatic speech recognition (ASR) systems like Whisper have made it possible to transcribe audio in multiple languages with a single model. However, current state-of-the-art ASR models are typically evaluated on individual languages or in a multi-task setting, overlooking the challenge of continually learning new languages. There is insufficient research on how to add new languages without losing valuable information from previous data. Furthermore, existing continual learning benchmarks focus mostly on vision and language tasks, leaving continual learning for multilingual ASR largely unexplored. To bridge this gap, we propose CL-MASR, a benchmark designed for studying multilingual ASR in a continual learning setting. CL-MASR provides a diverse set of continual learning methods implemented on top of large-scale pretrained ASR models, along with common metrics to assess the effectiveness of learning new languages while addressing the issue of catastrophic forgetting
    
[^62]: 机器翻译中的临床损害被医生发现：质量评估帮助减少依赖，追溯翻译识别关键错误

    Physician Detection of Clinical Harm in Machine Translation: Quality Estimation Aids in Reliance and Backtranslation Identifies Critical Errors. (arXiv:2310.16924v1 [cs.CL])

    [http://arxiv.org/abs/2310.16924](http://arxiv.org/abs/2310.16924)

    本文通过人类研究评估了在高风险医疗环境中使用质量评估反馈来帮助医生辨别何时依赖机器翻译输出。结果表明，质量评估能改善对机器翻译的适当依赖，但追溯翻译能帮助医生发现更严重的临床错误。

    

    机器翻译在实际应用中的一个主要挑战是用户缺乏指导来做出是否依赖其输出的明智决策。质量评估研究的进展提供了自动评估机器翻译质量的技术，但这些技术主要在特定的使用环境之外通过与人工判断的比较进行体外评估。本文通过在高风险医学环境中模拟决策过程的人类研究，评估了质量评估反馈的实地应用。我们使用急诊科出院指示，研究了基于质量评估和追溯翻译的干预措施如何帮助医生决定是否向患者展示机器翻译的输出。我们发现，质量评估改善了对机器翻译的适当依赖，但追溯翻译有助于医生发现质量评估常常忽略的更严重的临床错误。

    A major challenge in the practical use of Machine Translation (MT) is that users lack guidance to make informed decisions about when to rely on outputs. Progress in quality estimation research provides techniques to automatically assess MT quality, but these techniques have primarily been evaluated in vitro by comparison against human judgments outside of a specific context of use. This paper evaluates quality estimation feedback in vivo with a human study simulating decision-making in high-stakes medical settings. Using Emergency Department discharge instructions, we study how interventions based on quality estimation versus backtranslation assist physicians in deciding whether to show MT outputs to a patient. We find that quality estimation improves appropriate reliance on MT, but backtranslation helps physicians detect more clinically harmful errors that QE alone often misses.
    
[^63]: Divide et Impera: Multi-Transformer Architectures for Complex NLP-Tasks.

    Divide et Impera: Multi-Transformer Architectures for Complex NLP-Tasks. (arXiv:2310.16897v1 [cs.CL])

    [http://arxiv.org/abs/2310.16897](http://arxiv.org/abs/2310.16897)

    本论文提出了一种多Transformer架构的方法，针对复杂NLP任务将其分解为简单的子任务，并利用多个模型进行微调，以增强任务的控制能力和性能。在减少性别偏见的任务中，实验证明这种方法优于单一模型的表现。

    

    随着Transformer模型能力的增强，解决越来越复杂的自然语言处理（NLP）任务的道路已经被铺开。支持特定应用需求的关键在于能够进行微调。然而，针对复杂任务编制适用的微调数据集是繁琐的，并且会导致大型数据集，限制了对Transformer输出的控制能力。我们提出了一种方法，将复杂任务分解为更简单的子任务。多个Transformer模型分别对每个子任务进行微调，并排成一行以完成复杂任务。这简化了微调数据集的编制，并增加了整体的可控性。以减少性别偏见作为复杂任务的例子，我们演示了我们的方法，并表明它比使用单一模型效果更好。

    The growing capabilities of transformer models pave the way for solving increasingly complex NLP tasks. A key to supporting application-specific requirements is the ability to fine-tune. However, compiling a fine-tuning dataset tailored to complex tasks is tedious and results in large datasets, limiting the ability to control transformer output. We present an approach in which complex tasks are divided into simpler subtasks. Multiple transformer models are fine-tuned to one subtask each, and lined up to accomplish the complex task. This simplifies the compilation of fine-tuning datasets and increases overall controllability. Using the example of reducing gender bias as a complex task, we demonstrate our approach and show that it performs better than using a single model.
    
[^64]: DEFT：通过无监督核心集选择实现大规模语言模型数据高效微调

    DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection. (arXiv:2310.16776v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.16776](http://arxiv.org/abs/2310.16776)

    这项研究介绍了一种名为DEFT的数据高效微调框架，通过无监督核心集选择来最小化微调大规模语言模型所需的数据量。研究结果表明，DEFT模型在准确性上与现有模型相当，并且仅使用了70%的数据量。

    

    最近的进展使得许多预训练语言模型（PLMs）可以使用；然而，一个仍然存在的问题是微调PLMs以用于下游任务究竟需要多少数据？在这项工作中，我们介绍了DEFT，一种数据高效的微调框架，它利用无监督的核心集选择来最小化微调PLMs所需的数据量。我们在文本编辑LM的背景下展示了DEFT框架的有效性，并与最先进的文本编辑模型CoEDIT进行了比较。我们的定量和定性结果表明，DEFT模型在准确性上与CoEDIT一样，而使用的数据量要少约70%。

    Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to minimize the amount of data needed to fine-tune PLMs for downstream tasks. We demonstrate the efficacy of our DEFT framework in the context of text-editing LMs, and compare to the state-of-the art text-editing model, CoEDIT. Our quantitative and qualitative results demonstrate that DEFT models are just as accurate as CoEDIT while being finetuned on ~70% less data.
    
[^65]: SkyMath：技术报告

    SkyMath: Technical Report. (arXiv:2310.16713v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.16713](http://arxiv.org/abs/2310.16713)

    SkyMath是一个13亿参数的大型语言模型，通过自比较微调，它在数学推理任务中表现优秀，超过了同等规模的所有开源模型，并取得了新的SOTA最佳性能。

    

    大型语言模型（LLMs）展示了在各种自然语言处理（NLP）任务中解决问题的巨大潜力，包括数学推理。在这项工作中，我们提出了一个具有130亿参数的数学大型语言模型SkyMath。通过自比较微调，我们显著提升了Skywork-13B-Base的数学推理能力。在GSM8K上，SkyMath超过了所有已知的开源模型，并建立了新的SOTA性能。

    Large language models (LLMs) have shown great potential to solve varieties of natural language processing (NLP) tasks, including mathematical reasoning. In this work, we present SkyMath, a large language model for mathematics with 13 billion parameters. By applying self-compare fine-tuning, we have enhanced mathematical reasoning abilities of Skywork-13B-Base remarkably. On GSM8K, SkyMath outperforms all known open-source models of similar size and has established a new SOTA performance.
    
[^66]: DDCoT: 多模态语言模型中的职责分明的思路链刺激

    DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models. (arXiv:2310.16436v1 [cs.CV])

    [http://arxiv.org/abs/2310.16436](http://arxiv.org/abs/2310.16436)

    DDCoT是一种职责分明的思路链刺激方法，通过负空间刺激保持批判态度，并在多模态推理中结合了关键洞见“保持批判性思考”和“让每个人发挥自己的作用”。

    

    人工智能系统的一个长远目标是实现类似人类的复杂多模态推理。最近，大型语言模型在语言模态上利用思路链（CoT）模拟人类思维，在多步推理方面取得了显著进展。然而，将这些进展转移到多模态环境中引入了更高的挑战，包括但不限于对劳动密集型注释的需求以及灵活性、一般化和可解释性方面的限制。为了在多模态环境中唤起CoT推理，本研究首先对多模态环境中的这些挑战进行了深入分析，并提出了两个关键洞见：“保持批判性思考”和“让每个人发挥自己的作用”。此外，本研究提出了一种新颖的DDCoT刺激方法，通过负空间刺激保持批判态度，并通过首先划分责任明确国多模态推理。

    A long-standing goal of AI systems is to perform complex multimodal reasoning like humans. Recently, large language models (LLMs) have made remarkable strides in such multi-step reasoning on the language modality solely by leveraging the chain of thought (CoT) to mimic human thinking. However, the transfer of these advancements to multimodal contexts introduces heightened challenges, including but not limited to the impractical need for labor-intensive annotation and the limitations in terms of flexibility, generalizability, and explainability. To evoke CoT reasoning in multimodality, this work first conducts an in-depth analysis of these challenges posed by multimodality and presents two key insights: "keeping critical thinking" and "letting everyone do their jobs" in multimodal CoT reasoning. Furthermore, this study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the r
    
[^67]: 揭示神经网络中的特征提取机制

    Unraveling Feature Extraction Mechanisms in Neural Networks. (arXiv:2310.16350v1 [cs.CL])

    [http://arxiv.org/abs/2310.16350](http://arxiv.org/abs/2310.16350)

    本研究提出了一种基于神经切线核的理论方法，研究神经网络中的特征提取机制。研究发现在梯度下降过程中模型如何利用统计特征，并揭示了激活函数选择对特征提取的影响。

    

    神经网络在捕捉精确知识方面的基本机制一直是持续研究的对象。本研究提出了一种基于神经切线核（NTK）的理论方法，用于研究这些机制。具体而言，考虑无限网络宽度，我们猜测目标模型的学习动态可能直观地揭示其从训练数据中获取的特征，进一步加深我们对其内部机制的理解。我们将该方法应用于几个基本模型，并揭示了这些模型在梯度下降过程中如何利用统计特征、以及它们如何融入最终的决策中。我们还发现激活函数的选择会影响特征提取。例如，使用ReLU激活函数可能会引入特征偏差，这可以解释为什么最近的预训练语言模型中选择了替代函数来代替它。

    The underlying mechanism of neural networks in capturing precise knowledge has been the subject of consistent research efforts. In this work, we propose a theoretical approach based on Neural Tangent Kernels (NTKs) to investigate such mechanisms. Specifically, considering the infinite network width, we hypothesize the learning dynamics of target models may intuitively unravel the features they acquire from training data, deepening our insights into their internal mechanisms. We apply our approach to several fundamental models and reveal how these models leverage statistical features during gradient descent and how they are integrated into final decisions. We also discovered that the choice of activation function can affect feature extraction. For instance, the use of the \textit{ReLU} activation function could potentially introduce a bias in features, providing a plausible explanation for its replacement with alternative functions in recent pre-trained language models. Additionally, we
    
[^68]: 大型语言模型的知识编辑：一项综述

    Knowledge Editing for Large Language Models: A Survey. (arXiv:2310.16218v1 [cs.CL])

    [http://arxiv.org/abs/2310.16218](http://arxiv.org/abs/2310.16218)

    大型语言模型(LLMs)在学术和工业领域具有巨大潜力。本文综述了LLMs的知识编辑问题，强调了需要开发有效和高效的技术来更新预训练LLMs以纳入新知识的重要性。

    

    大型语言模型(LLMs)近期以其出色的理解、分析和生成文本的能力，根据其广博的知识和推理能力，改变了学术和工业领域的格局。然而，LLMs的一个主要缺点是它们在预训练时需要大量计算资源，因为其参数数量前所未有。当需要频繁引入新知识到预训练模型中时，这个缺点更加显著。因此，开发有效和高效的技术来更新预训练LLMs是必不可少的。传统方法是通过直接微调将新知识编码到预训练LLMs中。然而，简单地重新训练LLMs可能计算资源密集，并且存在将与模型更新无关的有价值的预训练知识退化的风险。最近，基于知识的模型编辑(KME)引起了越来越多的关注，旨在精确修改LLMs以纳入特定的知识。

    Large language models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME) has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, wit
    
[^69]: COPF: 通过最优策略拟合实现持续学习人类偏好

    COPF: Continual Learning Human Preference through Optimal Policy Fitting. (arXiv:2310.15694v1 [cs.LG])

    [http://arxiv.org/abs/2310.15694](http://arxiv.org/abs/2310.15694)

    通过COPF方法，我们不需要重新训练预训练语言模型，而是使用最优策略拟合和函数正则化来持续学习和适应人类偏好的变化。

    

    强化学习通过人类反馈（RLHF）的技术是改善预训练语言模型（LM）以符合人类偏好的常用方法。然而，当前基于RLHF的LM在引入新的查询或反馈时需要完全重新训练，这是一项具有挑战性的任务，因为人类偏好在不同领域或任务之间可能会有所变化。由于所需的时间和计算资源以及与数据隐私相关的问题，重新训练LM在许多现实世界的情况下存在实际困难。为了解决这个限制，我们提出了一种新的方法，称为持续最优策略拟合（COPF），其中我们使用蒙特卡罗法估计一系列最优策略，然后通过函数正则化不断拟合策略序列。COPF包含一个单一的学习阶段，不需要复杂的强化学习。

    The technique of Reinforcement Learning from Human Feedback (RLHF) is a commonly employed method to improve pre-trained Language Models (LM), enhancing their ability to conform to human preferences. Nevertheless, the current RLHF-based LMs necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. Retraining LMs poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. To address this limitation, we propose a new method called Continual Optimal Policy Fitting (COPF), in which we estimate a series of optimal policies using the Monte Carlo method, and then continually fit the policy sequence with the function regularization. COPF involves a single learning phase and doesn't necessitate complex reinforcement learning. Importantly, it shares the capability 
    
[^70]: ChatGPT乌格的缺陷统计：对大型语言模型的形态学能力进行的跨语言研究

    Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model. (arXiv:2310.15113v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.15113](http://arxiv.org/abs/2310.15113)

    本研究通过对ChatGPT在四种语言上的形态学能力进行严格分析，发现它在英语上的表现特别不理想，远远达不到专门构建系统的水平。

    

    大型语言模型（LLMs）近期在语言能力方面取得了令人印象深刻的进展，与人类语言能力相提并论。然而，对最新一代LLMs的语言能力进行系统调查的研究相对较少，而且这些研究忽视了人类泛化的显著能力，只关注英语，并且忽视了语言的其他核心能力，如形态学。在这里，我们通过在四种语言（具体来说是英语，德语，泰米尔语和土耳其语）上进行第一次严格的ChatGPT形态学能力分析来填补这些空白。我们使用Berko（1958）的乌格测试版本对ChatGPT进行实验，使用了针对四种语言的新颖、无污染的数据集。我们发现ChatGPT的表现远远不及专门构建的系统，尤其是在英语方面。总体而言，我们的结果按形态学能力衡量

    Large language models (LLMs) have recently reached an impressive level of linguistic capability, prompting comparisons with human language skills. However, there have been relatively few systematic inquiries into the linguistic capabilities of the latest generation of LLMs, and those studies that do exist (i) ignore the remarkable ability of humans to generalize, (ii) focus only on English, and (iii) investigate syntax or semantics and overlook other capabilities that lie at the heart of human language, like morphology. Here, we close these gaps by conducting the first rigorous analysis of the morphological capabilities of ChatGPT in four typologically varied languages (specifically, English, German, Tamil, and Turkish). We apply a version of Berko's (1958) wug test to ChatGPT, using novel, uncontaminated datasets for the four examined languages. We find that ChatGPT massively underperforms purpose-built systems, particularly in English. Overall, our results -- through the lens of morp
    
[^71]: 语言模型是否能够嘲笑YouTube短视频？

    Can Language Models Laugh at YouTube Short-form Videos?. (arXiv:2310.14159v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.14159](http://arxiv.org/abs/2310.14159)

    本研究在用户生成数据集中筛选并注释了10K个YouTube上的有趣多模态视频，借助GPT-3.5验证了语言和视觉元素对幽默的贡献。此外，还开发了一种零-shot视频到文本提示方法，用于大型语言模型对视频幽默的理解。这个研究填补了现有数据集中对多领域多模态幽默的不足。

    

    随着社交网络上短视频的流行，要求AI模型能够更好地理解这些视频以与人类进行更好的交流。然而，之前的视频幽默数据集主要针对特定领域，如演讲或情景喜剧，并且大多关注语言线索。我们创建了一个包含来自YouTube的10K个多模态有趣视频的用户生成数据集，称为ExFunTube。使用基于GPT-3.5的视频过滤流程，我们验证了语言和视觉元素对幽默的贡献。在过滤后，我们为每个视频的有趣时刻加上了时间戳和文本解释。我们的ExFunTube在现有数据集中独特之处在于，我们的视频涵盖了各种类型幽默的广泛领域，需要对内容进行多模态理解。此外，我们开发了一种零-shot视频到文本提示，以最大化大型语言模型（LLMs）对视频幽默的理解。使用自动评分、原理质量实验以及人类评价方法进行三种不同的评估。

    As short-form funny videos on social networks are gaining popularity, it becomes demanding for AI models to understand them for better communication with humans. Unfortunately, previous video humor datasets target specific domains, such as speeches or sitcoms, and mostly focus on verbal cues. We curate a user-generated dataset of 10K multimodal funny videos from YouTube, called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both verbal and visual elements contributing to humor. After filtering, we annotate each video with timestamps and text explanations for funny moments. Our ExFunTube is unique over existing datasets in that our videos cover a wide range of domains with various types of humor that necessitate a multimodal understanding of the content. Also, we develop a zero-shot video-to-text prompting to maximize video humor understanding of large language models (LLMs). With three different evaluation methods using automatic scores, rationale quality experimen
    
[^72]: 使用定制的参考文献的同时机器翻译

    Simultaneous Machine Translation with Tailored Reference. (arXiv:2310.13588v1 [cs.CL])

    [http://arxiv.org/abs/2310.13588](http://arxiv.org/abs/2310.13588)

    本文提出了一种使用定制参考文献的方法，以解决在同时机器翻译模型训练过程中存在的问题。通过重述真实参考文献，定制参考文献可以在不同延迟下训练SiMT模型，避免了强制预测并保持高质量。

    

    同时机器翻译（SiMT）在读取整个源句子的同时生成翻译。然而，现有的SiMT模型通常使用相同的参考文献进行训练，而忽略了在不同延迟下可用的源信息的数量不同。使用低延迟下的真实参考文献可能引入强制预测，而使用与源词顺序一致的参考文献在高延迟下会导致性能下降。因此，关键是使用适当的参考文献来训练SiMT模型，既避免在训练过程中强制预测，又保持高质量。在本文中，我们提出了一种新颖的方法，通过重述真实参考文献，为不同延迟进行训练的SiMT模型提供定制的参考文献。具体而言，我们引入了由强化学习引发的改编器来修改真实参考文献，以得到定制的参考文献。SiMT模型是使用定制的参考文献进行训练，并与定制器共同优化。

    Simultaneous machine translation (SiMT) generates translation while reading the whole source sentence. However, existing SiMT models are typically trained using the same reference disregarding the varying amounts of available source information at different latency. Training the model with ground-truth at low latency may introduce forced anticipations, whereas utilizing reference consistent with the source word order at high latency results in performance degradation. Consequently, it is crucial to train the SiMT model with appropriate reference that avoids forced anticipations during training while maintaining high quality. In this paper, we propose a novel method that provides tailored reference for the SiMT models trained at different latency by rephrasing the ground-truth. Specifically, we introduce the tailor, induced by reinforcement learning, to modify ground-truth to the tailored reference. The SiMT model is trained with the tailored reference and jointly optimized with the tai
    
[^73]: 医学文本简化：通过非典型训练和重新排序的Beam Search解码优化可读性

    Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding. (arXiv:2310.11191v1 [cs.CL])

    [http://arxiv.org/abs/2310.11191](http://arxiv.org/abs/2310.11191)

    本研究探索了进一步提高医学领域文本简化可读性的方法，包括一种新的非典型损失和一种优化简单性的重新排序解码方法，取得了更好的性能。

    

    文本简化作为人工智能在专业领域（如医学）中弥合沟通差距的越来越有用的应用，已逐渐崭露头角。然而，医学简化方法有时会导致生成的文本质量和多样性下降。在本研究中，我们探索了进一步提高医学领域文本简化可读性的方法。我们提出了一种新的非典型吃亏损失干图片刺激生成更简单的术语，以及一种优化简单性的重新排序的Beam Search解码方法，在三个数据集上的可读性指标上取得了更好的性能。这项研究的发现为改进医学领域的文本简化提供了有希望的途径。

    Text simplification has emerged as an increasingly useful application of AI for bridging the communication gap in specialized fields such as medicine, where the lexicon is often dominated by technical jargon and complex constructs. Despite notable progress, methods in medical simplification sometimes result in the generated text having lower quality and diversity. In this work, we explore ways to further improve the readability of text simplification in the medical domain. We propose (1) a new unlikelihood loss that encourages generation of simpler terms and (2) a reranked beam search decoding method that optimizes for simplicity, which achieve better performance on readability metrics on three datasets. This study's findings offer promising avenues for improving text simplification in the medical field.
    
[^74]: MathVista: 用GPT-4V、Bard和其他大型多模态模型评估视觉场景中的数学推理能力

    MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models. (arXiv:2310.02255v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.02255](http://arxiv.org/abs/2310.02255)

    本论文提出了MathVista，这是一个评估视觉场景中数学推理能力的基准测试。通过对12个著名的基础模型进行全面的定量评估，发现最好的GPT-4V模型相对于第二名的Bard模型在准确率上提升了15.1%。

    

    大型语言模型（LLMs）和大型多模态模型（LMMs）在许多任务和领域中展示出令人印象深刻的问题解决能力，但它们在视觉环境中的数学推理能力尚未得到系统研究。为了弥补这一差距，我们提出了MathVista，这是一个综合了不同数学和视觉任务的挑战的基准测试。它包含了6141个例子，其中有28个现有的多模态数据集和3个新创建的数据集（即IQTest、FunctionQA和PaperQA）。完成这些任务需要精细的、深入的视觉理解和组合推理，这些都是当前最先进的基础模型所面临的困难。通过MathVista，我们对12个著名的基础模型进行了全面的定量评估。表现最好的GPT-4V模型的整体准确率为49.9%，明显优于第二名的Bard模型，相差15.1%。我们的深入分析揭示了

    Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of
    
[^75]: 放射学报告的多语言自然语言处理模型--摘要是你需要的一切！

    Multilingual Natural Language ProcessingModel for Radiology Reports -- The Summary is all you need!. (arXiv:2310.00100v1 [cs.CL])

    [http://arxiv.org/abs/2310.00100](http://arxiv.org/abs/2310.00100)

    本研究通过在多语言文本到文本变换器模型上微调，开发了一个能够自动在多语言中总结放射学报告的模型。该模型有助于提高未来深度学习模型的研究和发展，且能够应用于不同族裔背景的患者数据。

    

    放射学报告的印象部分总结了重要的放射学发现，并在向医生传达这些发现时起到了关键作用。然而，对于放射科医生来说，准备这些摘要既耗时又容易出错。最近，已经开发了许多用于放射学报告摘要的模型。然而，目前还没有能够在多种语言中总结这些报告的模型。这样的模型可以极大地改进未来的研究和融合来自不同族裔背景的患者数据的深度学习模型的发展。本研究通过在公开可用的基于多语言文本到文本变换器的模型上微调，自动化地生成了不同语言的放射学印象，以总结英语、葡萄牙语和德语的放射学报告中的发现。在一项盲测中，两位有执业资格的放射科医生表示，对于至少70%的系统生成的摘要，其质量

    The impression section of a radiology report summarizes important radiology findings and plays a critical role in communicating these findings to physicians. However, the preparation of these summaries is time-consuming and error-prone for radiologists. Recently, numerous models for radiology report summarization have been developed. Nevertheless, there is currently no model that can summarize these reports in multiple languages. Such a model could greatly improve future research and the development of Deep Learning models that incorporate data from patients with different ethnic backgrounds. In this study, the generation of radiology impressions in different languages was automated by fine-tuning a model, publicly available, based on a multilingual text-to-text Transformer to summarize findings available in English, Portuguese, and German radiology reports. In a blind test, two board-certified radiologists indicated that for at least 70% of the system-generated summaries, the quality 
    
[^76]: ChatGPT的公众关切是什么？一种新颖的自监督神经主题模型告诉你。

    What are Public Concerns about ChatGPT? A Novel Self-Supervised Neural Topic Model Tells You. (arXiv:2309.01522v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.01522](http://arxiv.org/abs/2309.01522)

    本研究提出了一种新颖的自监督神经主题模型（SSTM），通过对ChatGPT的社交媒体帖子和用户查询进行实验，成功提取出了具有更高质量、更好可解释性和多样性的公众关切。

    

    最近发布的人工智能对话代理ChatGPT在学术界和现实生活中引起了极大关注。许多早期的ChatGPT用户热切地探索其能力，并通过社交媒体分享他们对它的意见。用户查询和社交媒体帖子都表达了对这个先进对话系统的公众关切。为了挖掘关于ChatGPT的公众关切，本文提出了一种新颖的自监督神经主题模型（SSTM），该模型将主题建模形式化为一种表示学习过程。通过对关于ChatGPT的Twitter帖子和ChatGPT用户的查询进行广泛实验，实验结果表明，所提出的方法能够提取具有更高质量、更好的可解释性和多样性的公众关切，超过了最先进方法的性能。

    The recently released artificial intelligence conversational agent, ChatGPT, has gained significant attention in academia and real life. A multitude of early ChatGPT users eagerly explore its capabilities and share their opinions on it via social media. Both user queries and social media posts express public concerns regarding this advanced dialogue system. To mine public concerns about ChatGPT, a novel Self-Supervised neural Topic Model (SSTM), which formalizes topic modeling as a representation learning procedure, is proposed in this paper. Extensive experiments have been conducted on Twitter posts about ChatGPT and queries asked by ChatGPT users. And experimental results demonstrate that the proposed approach could extract higher quality public concerns with improved interpretability and diversity, surpassing the performance of state-of-the-art approaches.
    
[^77]: 大型内容和行为模型用于理解、模拟和优化内容和行为

    Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior. (arXiv:2309.00359v1 [cs.CL])

    [http://arxiv.org/abs/2309.00359](http://arxiv.org/abs/2309.00359)

    该论文提出了使用大型内容和行为模型来理解、模拟和优化内容和行为。大型语言模型虽然在任务泛化能力方面取得了进展，但还无法解决预测和优化通信以实现期望接收者行为的问题。其中的一个原因可能是训练语料库中缺少"行为标记"。

    

    香农在引入信息理论的经典论文中将通信分为三个层次：技术层、语义层和效果层。技术层关注的是准确重构传输的符号，而语义层和效果层则涉及推断出的意义及其对接收者的影响。得益于电信技术，第一层问题已经取得了较大的进步，如互联网。大型语言模型（LLM）在第二个目标方面取得了一些进展，但第三层仍然基本上未被触及。第三个问题涉及预测和优化通信以实现期望的接收者行为。LLM在各种任务中显示出了广泛的泛化能力，但无法解决这个问题。表现不佳的原因之一可能是LLM的训练语料库中缺少"行为标记"。行为标记定义了在一次通信中的接收者行为，如分享、点赞、点击、购买、转推等。

    Shannon, in his seminal paper introducing information theory, divided the communication into three levels: technical, semantic, and effectivenss. While the technical level is concerned with accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Thanks to telecommunications, the first level problem has produced great advances like the internet. Large Language Models (LLMs) make some progress towards the second goal, but the third level still remains largely untouched. The third problem deals with predicting and optimizing communication for desired receiver behavior. LLMs, while showing wide generalization capabilities across a wide range of tasks, are unable to solve for this. One reason for the underperformance could be a lack of "behavior tokens" in LLMs' training corpora. Behavior tokens define receiver behavior over a communication, such as shares, likes, clicks, purchases, retweets, etc. W
    
[^78]: 跨语言和方言中的亲属关系词汇的词汇多样性研究

    Lexical Diversity in Kinship Across Languages and Dialects. (arXiv:2308.13056v1 [cs.CL])

    [http://arxiv.org/abs/2308.13056](http://arxiv.org/abs/2308.13056)

    本文研究了跨语言和方言中的亲属关系词汇的词汇多样性，并提出了一种方法来丰富计算词汇资源。通过大规模的案例研究，我们验证了该方法，并提供了可供浏览和下载的计算资源，扩展了对亲属关系术语的语言学研究，揭示了在语言和文化上相互接近的社区中多样性的程度。

    

    已知语言以多样的方式描述世界。在词汇中，多样性广泛存在，如词汇空缺和无法翻译等现象。然而，在计算资源中，如多语种词汇数据库中，多样性很少得到表示。本文介绍了一种方法，通过增加与语言多样性相关的内容，丰富计算词汇资源。该方法通过对亲属关系术语进行两个大规模案例研究进行验证，这是一个众所周知在语言和文化中具有多样性的领域：一个案例研究涉及七种阿拉伯方言，而另一个案例研究涉及三种印度尼西亚语言。我们所得到的结果以可浏览和可下载的计算资源的形式提供，扩展了先前对亲属关系术语的语言学研究，同时揭示了在语言和文化上相互接近的社区中多样性的程度。

    Languages are known to describe the world in diverse ways. Across lexicons, diversity is pervasive, appearing through phenomena such as lexical gaps and untranslatability. However, in computational resources, such as multilingual lexical databases, diversity is hardly ever represented. In this paper, we introduce a method to enrich computational lexicons with content relating to linguistic diversity. The method is verified through two large-scale case studies on kinship terminology, a domain known to be diverse across languages and cultures: one case study deals with seven Arabic dialects, while the other one with three Indonesian languages. Our results, made available as browseable and downloadable computational resources, extend prior linguistics research on kinship terminology, and provide insight into the extent of diversity even within linguistically and culturally close communities.
    
[^79]: 一个知识增强的两阶段生成框架用于医学对话信息提取

    A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue Information Extraction. (arXiv:2307.16200v1 [cs.CL])

    [http://arxiv.org/abs/2307.16200](http://arxiv.org/abs/2307.16200)

    本论文提出了一个知识增强的两阶段生成框架（KTGF）用于医学对话信息提取。通过两个阶段的生成，分别生成医学对话中的术语和每个术语的状态，从而更好地建模术语之间的关系。

    

    本文关注医学对话中的术语-状态对提取（MD-TSPE），这在诊断对话系统和电子医疗记录（EMR）的自动抄写中是必不可少的。在过去的几年中，MD-TSPE的研究引起了越来越多的关注，特别是在生成方法取得显著进展之后。然而，这些生成方法在一阶段输出整个由术语-状态对组成的序列时忽略了集成先前知识的需求，这需要更深入的理解来建模术语之间的关系和推断每个术语的状态。本文提出了一个知识增强的两阶段生成框架（KTGF）来解决上述挑战。通过使用任务特定的提示，我们采用单一模型以统一的生成形式完成MD-TSPE的两个阶段：首先生成所有的术语，然后生成每个生成的术语的状态。通过这种方式，可以更有效地学习术语之间的关系。

    This paper focuses on term-status pair extraction from medical dialogues (MD-TSPE), which is essential in diagnosis dialogue systems and the automatic scribe of electronic medical records (EMRs). In the past few years, works on MD-TSPE have attracted increasing research attention, especially after the remarkable progress made by generative methods. However, these generative methods output a whole sequence consisting of term-status pairs in one stage and ignore integrating prior knowledge, which demands a deeper understanding to model the relationship between terms and infer the status of each term. This paper presents a knowledge-enhanced two-stage generative framework (KTGF) to address the above challenges. Using task-specific prompts, we employ a single model to complete the MD-TSPE through two phases in a unified generative form: we generate all terms the first and then generate the status of each generated term. In this way, the relationship between terms can be learned more effect
    
[^80]: AlpaGasus: 用更少数据训练更好的羊驼

    AlpaGasus: Training A Better Alpaca with Fewer Data. (arXiv:2307.08701v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.08701](http://arxiv.org/abs/2307.08701)

    这项研究提出了一种用于训练语言模型的数据筛选策略AlpaGasus，通过使用强大的语言模型过滤掉低质量数据，它在测试中表现出比原始模型更好的性能，并提供了更快的训练速度。

    

    大型语言模型通过在有监督的指令/回复数据上进行指令微调（IFT）来增强其遵循指令的能力。然而，广泛使用的IFT数据集（例如：Alpaca的52k数据）出乎意料地包含许多具有不正确或不相关回复的低质量实例，这些实例会误导和对IFT产生不利影响。在本文中，我们提出了一种简单而有效的数据选择策略，该策略使用强大的语言模型（例如：ChatGPT）自动识别并过滤掉低质量数据。为此，我们引入了AlpaGasus，它仅在从52k Alpaca数据中过滤得到的9k高质量数据上进行微调。AlpaGasus在多个测试数据集和人工评估中均显著优于原始的Alpaca，由GPT-4进行评估。其13B变种在测试任务上的性能与其教师模型语言模型（即生成52k数据的Text-Davinci-003）的性能匹配率超过90％。它还提供了5.7倍更快的训练速度，将7B变种的训练时间从80分钟减少到了...

    Large language models~(LLMs) strengthen instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data. However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT. In this paper, we propose a simple and effective data selection strategy that automatically identifies and filters out low-quality data using a strong LLM (e.g., ChatGPT). To this end, we introduce AlpaGasus, which is finetuned on only 9k high-quality data filtered from the 52k Alpaca data. AlpaGasus significantly outperforms the original Alpaca as evaluated by GPT-4 on multiple test sets and the controlled human evaluation. Its 13B variant matches $>90\%$ performance of its teacher LLM (i.e., Text-Davinci-003 generating the 52k data) on test tasks. It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (
    
[^81]: 大型语言模型作为通用模式机器

    Large Language Models as General Pattern Machines. (arXiv:2307.04721v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2307.04721](http://arxiv.org/abs/2307.04721)

    预训练的大型语言模型（LLMs）展现出了强大的序列模式完成能力，即使在随机样本的情况下也能保持一定的准确性。这些模型可以用于机器人领域的问题，如动态状态序列预测和自主路径规划。

    

    我们观察到预训练的大型语言模型（LLMs）能够自动完成复杂的令牌序列，从由概率上下文无关语法（PCFG）随机生成的任意序列，到在Abstraction and Reasoning Corpus（ARC）中发现的更丰富的空间模式，以ASCII艺术的形式提示。令人惊讶的是，即使使用从词汇表中随机抽样的令牌表示序列，模式完成能力也可以部分保留。这些结果表明，在没有任何额外训练的情况下，LLMs可以作为通用序列模型器，通过上下文学习驱动。在这项工作中，我们研究了这些零样本能力如何应用于机器人领域的问题，从对表示随时间变化的状态的数字序列进行外推，以完成简单的运动，到以奖励条件轨迹的最小到最大提示方式，能够发现和表示闭环策略（例如，稳定的控制系统）。

    We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences -- from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstraction and Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics -- from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing cont
    
[^82]: CARE-MI: 中国孕婴护理领域的虚假信息评估基准

    CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care. (arXiv:2307.01458v1 [cs.CL])

    [http://arxiv.org/abs/2307.01458](http://arxiv.org/abs/2307.01458)

    CARE-MI是一个用于评估中国孕婴护理领域LLM虚假信息的基准，填补了这一领域的研究空白，并提供了构建长篇生成评估基准的创新范式。

    

    最近自然语言处理的进展导致了将LLM应用于现实场景的新趋势。尽管最新的LLM在与人类互动时令人惊叹地流利，但它们在生成错误事实陈述时会意外产生虚假信息问题。这可能导致有害后果，尤其是在敏感环境下，比如医疗保健领域。然而，之前很少有研究关注评估LLM长篇生成中的虚假信息，尤其是针对知识密集型主题。此外，尽管LLM在不同语言上表现良好，但虚假信息评估主要在英语中进行。为此，我们提供了一个基准，CARE-MI，用于评估LLM虚假信息在：1）一个敏感主题，具体是孕婴护理领域；和2）一种非英语语言，即中文。最重要的是，我们提供了一个创新的范式，用于构建长篇生成评估基准，可以

    The recent advances in NLP, have led to a new trend of applying LLMs to real-world scenarios. While the latest LLMs are astonishingly fluent when interacting with humans, they suffer from the misinformation problem by unintentionally generating factually false statements. This can lead to harmful consequences, especially when produced within sensitive contexts, such as healthcare. Yet few previous works have focused on evaluating misinformation in the long-form generation of LLMs, especially for knowledge-intensive topics. Moreover, although LLMs have been shown to perform well in different languages, misinformation evaluation has been mostly conducted in English. To this end, we present a benchmark, CARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic, specifically the maternity and infant care domain; and 2) a language other than English, namely Chinese. Most importantly, we provide an innovative paradigm for building long-form generation evaluation benchmarks that can
    
[^83]: DocumentNet: 在文档预训练中弥合数据差距

    DocumentNet: Bridging the Data Gap in Document Pre-Training. (arXiv:2306.08937v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.08937](http://arxiv.org/abs/2306.08937)

    这项研究提出了DocumentNet方法，通过从Web上收集大规模和弱标注的数据，弥合了文档预训练中的数据差距，并在各类VDER任务中展现了显著的性能提升。

    

    近年来，文档理解任务，特别是富有视觉元素的文档实体检索（VDER），由于在企业人工智能领域的广泛应用，受到了极大关注。然而，由于严格的隐私约束和高昂的标注成本，这些任务的公开可用数据非常有限。更糟糕的是，来自不同数据集的不重叠实体空间妨碍了文档类型之间的知识转移。在本文中，我们提出了一种从Web收集大规模和弱标注的数据的方法，以利于VDER模型的训练。所收集的数据集名为DocumentNet，不依赖于特定的文档类型或实体集，使其适用于所有的VDER任务。目前的DocumentNet包含了30M个文档，涵盖了近400个文档类型，组织成了一个四级本体结构。在一系列广泛采用的VDER任务上进行的实验表明，当将DocumentNet纳入预训练过程时，取得了显著的改进。

    Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints and high annotation costs. To make things worse, the non-overlapping entity spaces from different datasets hinder the knowledge transfer between document types. In this paper, we propose a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models. The collected dataset, named DocumentNet, does not depend on specific document types or entity sets, making it universally applicable to all VDER tasks. The current DocumentNet consists of 30M documents spanning nearly 400 document types organized in a four-level ontology. Experiments on a set of broadly adopted VDER tasks show significant improvements when DocumentNet is incorporated into the pre-train
    
[^84]: 重温自然语言处理中的领域外鲁棒性: 基准，分析和LLMs评估

    Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations. (arXiv:2306.04618v1 [cs.CL])

    [http://arxiv.org/abs/2306.04618](http://arxiv.org/abs/2306.04618)

    本文提出了一个具有挑战性的基准协议，用于评估自然语言处理中的领域外鲁棒性。通过使用这个基准套件，作者们发现OOD与ID性能之间的关系并不总是一致的，并引入了一种名为LLMs的新方法，可以在多个任务上显著提高OOD鲁棒性。

    

    本文重新审视自然语言处理(NLP)领域中领域外鲁棒性(OOD)的研究。我们发现以往研究中的分布转移设置普遍缺乏足够的挑战，限制了对OOD鲁棒性的准确评估。为了解决这些问题，我们提出了一个基准构建方案，确保了明确的区分和具有挑战性的分布转移。然后，我们介绍了BOSS，一个涵盖5个任务和20个数据集的用于评估OOT鲁棒性的基准套件。基于BOSS，我们对预训练语言模型进行了一系列实验，以分析和评估OOD鲁棒性。首先，我们研究了香草微调的ID和OOD性能之间的关系。我们确定了三种典型类型揭示了内在的学习机制，可能有助于预测OOD鲁棒性，并与ID数据集上的进展相关。然后，我们在BOSS上评估了5种经典方法，并发现它们的OOD性能并不总是与ID性能一致，这表明了特别评估OOD鲁棒性的重要性。最后，我们提出了一种名为LLMs（潜在语言模型）的新方法，可以在多个任务上显著提高OOD鲁棒性。

    This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre-trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find th
    
[^85]: 缩放数据受限的语言模型

    Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16264](http://arxiv.org/abs/2305.16264)

    研究人员研究了在数据受限制的情况下缩放语言模型，并提出了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。

    

    现在扩展语言模型的趋势涉及增加参数计数和训练数据集大小。推断这个趋势表明，训练数据集大小可能很快就会受到互联网上可用文本数据的限制。出于此限制的动机，我们研究在数据受限制的情况下缩放语言模型。具体而言，我们运行了大量的实验，变化数据重复程度和计算预算，范围达到了9000亿个训练令牌和9亿参数模型。我们发现，在有限的数据的情况下，使用高达4次重复数据的训练与使用唯一数据相比对损失的贡献微不足道。然而，使用更多的重复数据，添加计算的价值最终会衰减为零。我们提出并经验证了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。最后，我们尝试了缓解数据稀缺的方法。

    The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity,
    
[^86]: Bhasha-Abhijnaanam：22种印度文字和罗马拼音语言鉴别。 (arXiv：2305.15814v1 [cs.CL])

    Bhasha-Abhijnaanam: Native-script and romanized Language Identification for 22 Indic languages. (arXiv:2305.15814v1 [cs.CL])

    [http://arxiv.org/abs/2305.15814](http://arxiv.org/abs/2305.15814)

    该研究提供了22种印度宪法中列出的所有21种本土文字和罗马字母的公开语言鉴别（LID）数据和模型。IndicLID是上述语言的本土和罗马化脚本的语言鉴别器，还提出了解决罗马化文本的LID问题的方案。

    

    我们提供了22个印度宪法中列出的所有21种本土文字和罗马字母的公开语言鉴别（LID）数据和模型。与现有的LID相比，我们的Bhasha-Abhijnaanam在本土文字文本的语言涵盖范围方面更为广泛，并具有竞争力或更好的性能，IndicLID是上述语言的本土和罗马化脚本的语言鉴别器。对于罗马化文本的LID，存在两个主要挑战：缺乏训练数据和当语言相似时，低LID性能。我们提供了简单有效的解决方案。总的来说，在任何语言中，罗马化文本的研究都很有限，我们的研究结果对需要罗马化语言鉴别的其他语言也具有参考意义。

    We create publicly available language identification (LID) datasets and models in all 22 Indian languages listed in the Indian constitution in both native-script and romanized text. First, we create Bhasha-Abhijnaanam, a language identification test set for native-script as well as romanized text which spans all 22 Indic languages. We also train IndicLID, a language identifier for all the above-mentioned languages in both native and romanized script. For native-script text, it has better language coverage than existing LIDs and is competitive or better than other LIDs. IndicLID is the first LID for romanized text in Indian languages. Two major challenges for romanized text LID are the lack of training data and low-LID performance when languages are similar. We provide simple and effective solutions to these problems. In general, there has been limited work on romanized text in any language, and our findings are relevant to other languages that need romanized language identification. Ou
    
[^87]: 通过对比阅读模型和冻结大型语言模型进行视觉定位自然语言理解

    Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models. (arXiv:2305.15080v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15080](http://arxiv.org/abs/2305.15080)

    本文介绍了一种名为对比阅读模型（Cream）的新型神经架构，旨在通过捕捉复杂细节，提升大型语言模型在语言-图像理解能力方面的性能。该方法通过视觉和辅助编码器及对比特征对齐技术实现了对视觉定位上下文中语言信息的更有效理解。

    

    最近大型语言模型的进展刺激了对将其应用扩展到视觉领域的研究潮流。尽管这些模型在生成抽象图像标题和促进自然对话方面表现出了潜力，但它们在文本丰富的图像上的表现仍需要改进。本文介绍了一种名为对比阅读模型（Cream）的新型神经架构，旨在通过捕捉现有方法常常忽视的复杂细节，提升大型语言模型在语言-图像理解能力方面的性能。Cream结合了视觉和辅助编码器，并借助对比特征对齐技术来实现对图像中视觉定位上下文中语言信息的更有效理解。我们的方法弥合了视觉与语言理解之间的差距，为更复杂的文档智能助手的开发铺平了道路。通过对不同视觉定位上下文中的严格评估，我们验证了该方法的有效性。

    Recent advances in Large Language Models (LLMs) have stimulated a surge of research aimed at extending their applications to the visual domain. While these models exhibit promise in generating abstract image captions and facilitating natural conversations, their performance on text-rich images still requires improvement. In this paper, we introduce Contrastive Reading Model (Cream), a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details that are often overlooked in existing methods. Cream combines vision and auxiliary encoders, fortified by a contrastive feature alignment technique, to achieve a more effective comprehension of language information in visually situated contexts within the images. Our approach bridges the gap between vision and language understanding, paving the way for the development of more sophisticated Document Intelligence Assistants. Through rigorous evaluations across diverse visually-sit
    
[^88]: 自动规划：利用大型语言模型进行交互决策任务的自动规划

    AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models. (arXiv:2305.15064v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15064](http://arxiv.org/abs/2305.15064)

    本文提出了一种名为AutoPlan的方法，利用大型语言模型（LLM）来指导代理完成复杂的交互式决策任务。通过将LLM提示与任务解决计划相结合并进行优化，AutoPlan在没有上下文演示的情况下，在ALFWorld上实现了与人类撰写的演示基线相当的成功率，并在HotpotQA上超过了8%。

    

    最近的大型语言模型（LLM）对于在现实环境中进行决策具有很大的潜力。然而，由于LLM中预训练知识与实际环境规则之间的不匹配，LLM在复杂的决策任务中经常失败。现有的方法要么需要昂贵的梯度计算，要么需要耗时的上下文演示。在本文中，我们提出了一种名为AutoPlan的方法，用于引导基于LLM的代理完成交互式决策任务。AutoPlan通过迭代的经验收集和反思，将LLM提示与任务解决计划相结合，并进行优化。我们的实验证明，虽然未使用上下文演示，但AutoPlan在ALFWorld上实现了与人类撰写的演示基线相当的成功率，甚至在HotpotQA上超过了8%。代码可以在https://github.com/owaski/AutoPlan上找到。

    Recent large language models (LLMs) are promising for making decisions in grounded environments. However, LLMs frequently fail in complex decision-making tasks due to the misalignment between the pre-trained knowledge in LLMs and the actual rules in the environment. Existing methods require either costly gradient computation or lengthy in-context demonstrations. In this paper, we propose AutoPlan, an approach to guide LLM-based agents to accomplish interactive decision-making tasks. AutoPlan augments the LLM prompt with a task-solving plan and optimizes it through iterative experience collection and reflection. Our experiments show that AutoPlan, though using no in-context demonstrations, achieves success rates on par with the baselines using human-written demonstrations on ALFWorld and even outperforms them by 8% on HotpotQA. The code is available at https://github.com/owaski/AutoPlan.
    
[^89]: 在Transformer模型中编辑常识

    Editing Common Sense in Transformers. (arXiv:2305.14956v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14956](http://arxiv.org/abs/2305.14956)

    本文探究了常识判断是否与Transformer模型中的可编辑参数存在因果关联，并通过改进编辑算法和层选择策略，在常识领域中提升了编辑效果，使得经过编辑的GPT-2模型在各测试集上的F1分数相较于最佳微调基线提高了10.97%和10.73%。

    

    在Transformer模型中直接编辑模型参数使得在无需重新训练的情况下可以更新黑盒模型成为可能。然而，这些编辑方法仅在关于百科知识且只有一个正确答案的陈述上进行了评估。尚未对多个正确答案的常识知识进行研究，例如，一个苹果可以是绿色或红色但不能是透明的，这对于提高Transformer模型的可靠性和实用性同样重要。本文研究了常识判断是否与Transformer模型中的可编辑参数存在因果关联，并给出了肯定的答案。我们发现直接应用MEMIT编辑算法导致性能不佳，并通过改变编辑标记和改进层选择策略，即$MEMIT_{CSK}$，改进了对常识领域的编辑效果。使用$MEMIT_{CSK}$编辑过的GPT-2 Large和XL模型在PEP3k和20Q测试集上的F1分数相较于最佳微调基线提高了10.97%和10.73%。

    Editing model parameters directly in Transformers makes updating black-box models possible without re-training (Meng et al., 2023). However, these editing methods have only been evaluated on statements about encyclopedic knowledge with a single correct answer. Commonsense knowledge with multiple correct answers, e.g., an apple can be green or red but not transparent, has not been studied but is as essential for enhancing transformers' reliability and usefulness. In this paper, we investigate whether commonsense judgments are causally associated with localized, editable parameters in Transformers, and we provide an affirmative answer. We find that directly applying the MEMIT editing algorithm results in sub-par performance and improve it for the commonsense domain by varying edit tokens and improving the layer selection strategy, i.e., $MEMIT_{CSK}$. GPT-2 Large and XL models edited using $MEMIT_{CSK}$ outperform best-fine-tuned baselines by 10.97% and 10.73% F1 scores on PEP3k and 20Q 
    
[^90]: 基于覆盖率的上下文学习中示例选择方法

    Coverage-based Example Selection for In-Context Learning. (arXiv:2305.14907v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14907](http://arxiv.org/abs/2305.14907)

    本研究提出了一种基于覆盖率的上下文学习中示例选择方法，通过使用BERTScore-Recall度量方法选择更好的示例来展示测试输入的关键方面，同时还通过扩展成集合级别度量方法进一步提高了覆盖率表现。实验证明BSR是上下文示例选择中优越的度量方法，并且对于组合任务，使用Set-BSR进行集合选择可以显著提高性能。

    

    上下文学习（ICL）是大型语言模型通过在一些任务示例上进行条件约束从而实现新任务的能力，这要求这些示例对测试实例具有信息量。标准的方法是独立地对最相似的示例进行排名和选择，这样选择出的示例会重复且遗漏重要信息。本研究表明，BERTScore-Recall（BSR）选择了更好的示例，这些示例展示了测试输入的关键方面，如推理模式。我们进一步扩展了BSR和许多标准度量方法，将其转化为易于优化的集合级别度量方法，从而更好地覆盖这些关键方面。在涵盖6个任务的15个数据集和7个不同的LLM上，我们展示了（1）BSR在上下文示例选择方面是优越的度量方法，（2）对于组合任务，使用Set-BSR进行集合选择的性能优于独立排名，平均提高17个百分点，并且尽管无需训练但超过了现有方法。

    In-context learning (ICL), the ability of large language models to perform novel tasks by conditioning on a prompt with a few task examples, requires these examples to be informative about the test instance. The standard approach of independently ranking and selecting the most similar examples selects redundant examples while omitting important information. In this work, we show that BERTScore-Recall (BSR) selects better examples that demonstrate more of the salient aspects, e.g. reasoning patterns, of the test input. We further extend BSR and many standard metrics to easily optimizable set-level metrics, giving still better coverage of those salient aspects. On 15 datasets spanning 6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric for in-context example selection across the board, and (2) for compositional tasks, set selection using Set-BSR outperforms independent ranking by up to 17 points on average and, despite being training-free, surpasses methods that 
    
[^91]: 无标签测试数据下大型语言模型能力的估计

    Estimating Large Language Model Capabilities without Labeled Test Data. (arXiv:2305.14802v1 [cs.CL])

    [http://arxiv.org/abs/2305.14802](http://arxiv.org/abs/2305.14802)

    本文提出了一种在无标签测试数据下估计大型语言模型能力的方法，通过使用LLM置信度分数训练元模型来执行上下文学习准确性估计任务，并在相关基准测试中取得了优于其他基线的成果。

    

    大型语言模型（LLMs）展示了惊人的在上下文学习（ICL）中只需要几个例子即可执行的能力，但ICL的成功在不同任务中变化很大。因此，快速确定ICL是否适用于新任务非常重要，但直接评估ICL的准确性可能在测试数据昂贵的情况下变得昂贵，而ICL最有吸引力的正是这些情况。在本文中，我们提出ICL准确性估计任务，即在仅给定该任务的未标记数据的情况下，预测LLM在新任务上进行上下文学习时的准确性。为了执行ICL准确性估计，我们提出了一种使用LLM置信度分数作为特征来训练元模型的方法。我们在覆盖4个LLM和3个任务集合的新基准测试上将我们的方法与几种强的准确性估计基线进行比较。平均而言，元模型优于所有基线，并实现与直接评估相同的估计性能。

    Large Language Models (LLMs) have exhibited an impressive ability to perform in-context learning (ICL) from only a few examples, but the success of ICL varies widely from task to task. Thus, it is important to quickly determine whether ICL is applicable to a new task, but directly evaluating ICL accuracy can be expensive in situations where test data is expensive to annotate -- the exact situations where ICL is most appealing. In this paper, we propose the task of ICL accuracy estimation, in which we predict the accuracy of an LLM when doing in-context learning on a new task given only unlabeled data for that task. To perform ICL accuracy estimation, we propose a method that trains a meta-model using LLM confidence scores as features. We compare our method to several strong accuracy estimation baselines on a new benchmark that covers 4 LLMs and 3 task collections. On average, the meta-model improves over all baselines and achieves the same estimation performance as directly evaluating 
    
[^92]: 通过GPT-4分析影响人类偏好判断的因素

    Analyzing Influential Factors in Human Preference Judgments via GPT-4. (arXiv:2305.14702v1 [cs.CL])

    [http://arxiv.org/abs/2305.14702](http://arxiv.org/abs/2305.14702)

    本文利用Bradley-Terry-Luce模型对OpenAI公布的人类偏好判断数据集进行了深入研究，揭示了人类偏好判断中所蕴含的固有偏好，提出了提高样本效率的策略，并为构建平衡的人类偏好判断数据集提供了洞见。

    

    人类偏好判断在引导大型语言模型生成符合人类偏好的输出和评估自动摘要度量方面具有至关重要的作用。然而，对于这些偏好判断的共同影响和因素的相对重要性等问题，目前的研究仍较为有限。本文利用Bradley-Terry-Luce模型对OpenAI公布的人类偏好判断数据集进行了深入研究，识别了可能影响人类偏好判断的关键因素。研究结果揭示了人类偏好判断中所蕴含的固有偏好，并提出了提高样本效率的策略，最后对于如何构建平衡的人类偏好判断数据集提供了洞见。

    Pairwise human judgments are pivotal in guiding large language models (LLMs) to generate outputs that align with human preferences. They are also often used in summarization evaluation, complementing existing automatic metrics. Despite their significance, however, there has been limited research probing these pairwise human judgments. The collective impact and respective weights of factors such as informativeness, coherence, fluency, and factual consistency remain elusive. The impact of hidden factors on the final judgment is also unclear. In this paper, we conduct an in-depth examination of a dataset of pairwise human judgments released by OpenAI. Utilizing the Bradley-Terry-Luce model, we identify key factors that could potentially influence human judgments. Our research uncovers the inherent preferences embedded in human judgments and suggests strategies to boost sample efficiency. Finally, we provide insights on the construction of balanced datasets for human judgment evaluations, 
    
[^93]: 用户对于QA系统的背景信息依赖的影响研究

    What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on QA Systems. (arXiv:2305.14331v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14331](http://arxiv.org/abs/2305.14331)

    本研究调查了用户在评估模型预测时缺乏足够信息时与QA系统的交互方式，并发现即使缺乏这些信息，用户仍然过度依赖于模型的预测。然而，提供相关背景信息有助于减少对错误预测的依赖。

    

    NLP系统在通过检索相关上下文来回答问题方面表现出色。然而，随着模型越来越大，仅限于检索到的上下文来限制模型的知识或推理是不可能的且通常也不可取的。这导致了模型从中提取答案的信息与用户用来评估模型预测答案的信息之间的不匹配。在本研究中，我们研究了在缺乏足够信息来评估预测时用户如何与QA系统交互。此外，我们还询问是否添加必要的背景信息有助于减少用户对预测的过度依赖。我们的研究发现，即使在缺乏足够信息来评估模型正确性的情况下，用户仍然依赖于模型的预测。然而，提供相关背景信息有助于用户更好地发现模型错误，减少对不正确预测的依赖。而背景信息的添加也可能增加用户对模型的过度依赖。

    NLP systems have shown impressive performance at answering questions by retrieving relevant context. However, with the increasingly large models, it is impossible and often undesirable to constrain models' knowledge or reasoning to only the retrieved context. This leads to a mismatch between the information that the models access to derive the answer and the information that is available to the user to assess the model predicted answer. In this work, we study how users interact with QA systems in the absence of sufficient information to assess their predictions. Further, we ask whether adding the requisite background helps mitigate users' over-reliance on predictions. Our study reveals that users rely on model predictions even in the absence of sufficient information needed to assess the model's correctness. Providing the relevant background, however, helps users better catch model errors, reducing over-reliance on incorrect predictions. On the flip side, background information also in
    
[^94]: Dynosaur: 一种用于指令调优数据整理的动态增长模式

    Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation. (arXiv:2305.14327v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14327](http://arxiv.org/abs/2305.14327)

    Dynosaur提出了一种动态增长模式，用于自动整理指令调优数据。通过利用现有的注释数据集，Dynosaur能够以较低的API成本提供高质量的指令调优数据。

    

    指令调优已经成为增强大型语言模型（LLM）理解指令和生成适当回应能力的方法。现有方法要么手动注释，要么使用LLM（如GPT系列）生成指令调优数据。然而，它们经常忽视将指令与现有的注释数据集关联起来。在这篇论文中，我们提出了Dynosaur，一种用于自动整理指令调优数据的动态增长模式。基于现有数据集的元数据，我们使用LLM自动构建指令调优数据，通过识别相关的数据字段并生成适当的指令。通过利用现有的注释数据集，Dynosaur具有以下几个优点：1）减少了生成指令的API成本（例如，通过调用GPT-3.5-turbo生成80万个指令调优样本的成本低于12美元）；2）为指令调优提供高质量的数据（例如，表现优于Alpaca）。

    Instruction tuning has emerged to enhance the capabilities of large language models (LLMs) to comprehend instructions and generate appropriate responses. Existing methods either manually annotate or employ LLM (e.g., GPT-series) to generate data for instruction tuning. However, they often overlook associating instructions with existing annotated datasets. In this paper, we propose Dynosaur, a dynamic growth paradigm for the automatic curation of instruction-tuning data. Based on the metadata of existing datasets, we use LLMs to automatically construct instruction-tuning data by identifying relevant data fields and generating appropriate instructions.  By leveraging the existing annotated datasets, Dynosaur offers several advantages: 1) it reduces the API cost for generating instructions (e.g., it costs less than $12 USD by calling GPT-3.5-turbo for generating 800K instruction tuning samples; 2) it provides high-quality data for instruction tuning (e.g., it performs better than Alpaca a
    
[^95]: 结合全局结构知识的视觉丰富文档关系抽取方法

    Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document. (arXiv:2305.13850v1 [cs.CL])

    [http://arxiv.org/abs/2305.13850](http://arxiv.org/abs/2305.13850)

    这篇论文提出了一种结合全局结构知识的连续迭代的方式去捕获实体之间的依赖关系，以提高视觉丰富文档中关系抽取的准确性。

    

    视觉关系提取（VRE）旨在从视觉丰富的文档中提取实体之间的关系。现有方法通常基于实体特征单独预测每对实体之间的关系，但忽略了全局结构信息，即实体对之间的依赖关系。缺乏全局结构信息可能使模型难以学习长程关系，并容易产生冲突的预测结果。为了缓解这些限制，我们提出了一种GOSE框架，该框架以迭代的方式捕获实体对之间的依赖关系。给定文档的扫描图像，GOSE首先对实体对生成初步的关系预测。第二，在先前迭代的预测结果基础上，GOSE利用全局结构知识进一步整合实体表示。这种“生成-捕获-整合”模式被多次执行，以便实体之间的依赖关系能够被很好地捕获和利用。

    Visual relation extraction (VRE) aims to extract relations between entities from visuallyrich documents. Existing methods usually predict relations for each entity pair independently based on entity features but ignore the global structure information, i.e., dependencies between entity pairs. The absence of global structure information may make the model struggle to learn long-range relations and easily predict conflicted results. To alleviate such limitations, we propose a GlObal Structure knowledgeguided relation Extraction (GOSE) framework, which captures dependencies between entity pairs in an iterative manner. Given a scanned image of the document, GOSE firstly generates preliminary relation predictions on entity pairs. Secondly, it mines global structure knowledge based on prediction results of the previous iteration and further incorporates global structure knowledge into entity representations. This "generate-capture-incorporate" schema is performed multiple times so that entit
    
[^96]: 多语言摘要中的幻觉检测和缓解

    Detecting and Mitigating Hallucinations in Multilingual Summarisation. (arXiv:2305.13632v1 [cs.CL])

    [http://arxiv.org/abs/2305.13632](http://arxiv.org/abs/2305.13632)

    本文提出一种新的度量方法mFACT，可以在非英语摘要中评估其忠实性。本文还提出了一种简单有效的加权方法，可以通过跨语言转移减少摘要的幻觉问题。

    

    幻觉对于抽象摘要的神经模型的可靠性构成了重大挑战。虽然自动产生的摘要可能流畅，但通常缺乏对原始文档的忠实性。在低资源环境下，如跨语言转移，这个问题变得更加突出。由于现有的忠实性测量方法主要集中于英语，因此在跨语言环境中甚至衡量这种现象的程度也很困难。为了解决这个问题，作者首先提出了一种新的度量方法mFACT，通过从多个英语的忠实性测量结果中借鉴翻译基础知识为非英语摘要评估其忠实性。然后，他们提出了一种简单而有效的方法来通过跨语言转移减少幻觉，该方法将每个训练样本的损失乘以其忠实性得分。通过多种语言的广泛实验，作者证明了mFACT是最适合检测幻觉的度量方法。此外，他们发现他们的提出的加权方法可以缓解幻觉问题。

    Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation. While automatically generated summaries may be fluent, they often lack faithfulness to the original document. This issue becomes even more pronounced in low-resource settings, such as cross-lingual transfer. With the existing faithful metrics focusing on English, even measuring the extent of this phenomenon in cross-lingual settings is hard. To address this, we first develop a novel metric, mFACT, evaluating the faithfulness of non-English summaries, leveraging translation-based transfer from multiple English faithfulness metrics. We then propose a simple but effective method to reduce hallucinations with a cross-lingual transfer, which weighs the loss of each training example by its faithfulness score. Through extensive experiments in multiple languages, we demonstrate that mFACT is the metric that is most suited to detect hallucinations. Moreover, we find that our proposed l
    
[^97]: 多模态自动事实核查：一份调查

    Multimodal Automated Fact-Checking: A Survey. (arXiv:2305.13507v1 [cs.CL])

    [http://arxiv.org/abs/2305.13507](http://arxiv.org/abs/2305.13507)

    本调查提出了一个多模态自动事实核查的框架，并包括了独特的子任务，重点关注了文本，图像，音频和视频这四种模态的现实应用。纪录了相关的基准模型，讨论了未来研究的局限性和前景。

    

    错误信息，即事实上不正确的信息，通常以多种形式传达，例如带有标题的图像。 它被人们视为更可信，比其仅限于文本的对应物扩散速度更快，范围更广。 尽管越来越多的研究涉及自动事实核查（AFC），但以往的调查主要集中在文本误导方面。 在本调查中，我们构建了一个包括多模态误导独特子任务在内的AFC框架。此外，我们在我们的框架上讨论了不同社区所发展的相关术语。 我们重点关注现实世界事实核查中存在的四种模态：文本，图像，音频和视频。 我们调查了基准和模型，并讨论了未来研究的局限性和有前途的方向。

    Misinformation, i.e. factually incorrect information, is often conveyed in multiple modalities, e.g. an image accompanied by a caption. It is perceived as more credible by humans, and spreads faster and wider than its text-only counterparts. While an increasing body of research investigates automated fact-checking (AFC), previous surveys mostly focus on textual misinformation. In this survey, we conceptualise a framework for AFC including subtasks unique to multimodal misinformation. Furthermore, we discuss related terminological developed in different communities in the context of our framework. We focus on four modalities prevalent in real-world fact-checking: text, image, audio, and video. We survey benchmarks and models, and discuss limitations and promising directions for future research.
    
[^98]: CLASS：基于学习科学原理构建智能辅导系统的设计框架

    CLASS: A Design Framework for building Intelligent Tutoring Systems based on Learning Science principles. (arXiv:2305.13272v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13272](http://arxiv.org/abs/2305.13272)

    提出了一个名为CLASS的设计框架，用于构建基于学习科学原理的智能辅导系统（ITS），该框架通过提供关键能力使ITS能够提供逐步指导和促进自然语言交互，从而提高学生的学习效果。

    

    我们提出了一个名为Conversational Learning with Analytical Step-by-Step Strategies（CLASS）的设计框架，用于构建由高性能大型语言模型（LLM）驱动的先进智能辅导系统（ITS）。CLASS框架赋予ITS两个关键能力。首先，通过一个精心策划的脚手架数据集，CLASS为ITS提供了必要的问题解决策略，使其能够为学生提供类似导师的逐步指导。其次，通过使用动态对话数据集，CLASS帮助ITS促进自然语言交互，促进有趣的学生-导师对话。CLASS框架还提供了对ITS内部决策过程的宝贵洞察，从而实现了用户反馈的无缝集成，实现持续的完善和改进。我们还展示了一个名为SPOCK的概念验证ITS，它是使用CLASS框架并专注于大学初级生物学内容进行训练的。

    We present a design framework called Conversational Learning with Analytical Step-by-Step Strategies (CLASS) for building advanced Intelligent Tutoring Systems (ITS) powered by high-performance Large Language Models (LLMs). The CLASS framework empowers ITS with two key capabilities. First, through a carefully curated scaffolding dataset, CLASS equips ITS with essential problem-solving strategies, enabling it to provide tutor-like, step-by-step guidance to students. Second, by using a dynamic conversational dataset, CLASS assists ITS in facilitating natural language interactions, fostering engaging student-tutor conversations. The CLASS framework also provides valuable insights into ITS' internal decision-making process which allows seamless integration of user feedback, thus enabling continuous refinement and improvement. We also present a proof-of-concept ITS, referred to as SPOCK, which is trained using the CLASS framework with a focus on introductory college-level biology content. A
    
[^99]: 解释CPE：中国执业药师考试的自由文本解释基准。（arXiv:2305.12945v2 [cs.CL] 更新）

    ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination. (arXiv:2305.12945v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12945](http://arxiv.org/abs/2305.12945)

    ExplainCPE是一个具有挑战性的简体中文医学基准，用于解决大型语言模型的可解释性能力问题。该基准分析了ChatGPT和GPT-4的错误，并指出了当前模型在理解文本和计算推理方面的局限性。

    

    随着ChatGPT和GPT-4引领大型语言模型（LLMs）的发展，越来越多的研究者正在研究它们在各种任务中的性能。但是，对LLMs的可解释性能力，也就是在给出答案后生成原因的能力，还需要进行更多的研究。现有的解释数据集主要是英语通用知识问题，导致主题和语言多样性不足。为了解决语言偏见和生成合理问答数据集中缺乏医疗资源的问题，我们提出了ExplainCPE（超过7k个实例），这是一个具有挑战性的简体中文医学基准。我们分析了ChatGPT和GPT-4的错误，指出了当前LLMs在理解文本和计算推理方面的局限性。在实验过程中，我们还发现不同的LLMs在上下文学习方面有不同的偏好。ExplainCPE提出了一个重大挑战，但它对进一步研究具有潜力。

    As ChatGPT and GPT-4 spearhead the development of Large Language Models (LLMs), more researchers are investigating their performance across various tasks. But more research needs to be done on the interpretability capabilities of LLMs, that is, the ability to generate reasons after an answer has been given. Existing explanation datasets are mostly English-language general knowledge questions, which leads to insufficient thematic and linguistic diversity. To address the language bias and lack of medical resources in generating rationales QA datasets, we present ExplainCPE (over 7k instances), a challenging medical benchmark in Simplified Chinese. We analyzed the errors of ChatGPT and GPT-4, pointing out the limitations of current LLMs in understanding text and computational reasoning. During the experiment, we also found that different LLMs have different preferences for in-context learning. ExplainCPE presents a significant challenge, but its potential for further investigation is prom
    
[^100]: 回收和精馏：带有注意力映射重用和蒸馏屏蔽的基于Transformer的语音自监督学习模型的通用压缩策略

    Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation. (arXiv:2305.11685v1 [eess.AS])

    [http://arxiv.org/abs/2305.11685](http://arxiv.org/abs/2305.11685)

    本研究提出了一种基于Transformer的语音自监督学习模型的通用压缩策略，通过重用注意力映射和蒸馏屏蔽来提高学生模型的语音表示质量，实现了较低的错误率。

    

    基于Transformer的语音自监督学习模型在各种语音处理任务中表现出惊人的性能。然而，语音 SSL 模型中庞大的参数数量需要压缩成更紧凑的模型，以便在学术界或小公司中更广泛地使用。本研究建议重用Transformer层之间的注意力映射，因此可以删除键和查询参数，同时保留层数。此外，我们提出了一种新的蒸馏策略，以提高学生模型的语音表示质量。我们扩展了蒸馏损失，利用遮罩和未遮罩的语音帧，充分利用教师模型的高质量表示。我们的通用压缩策略产生的学生模型在SUPERB基准测试中实现了7.72%的音素误差率（PER）和9.96%的单词错误率（WER）。

    Transformer-based speech self-supervised learning (SSL) models, such as HuBERT, show surprising performance in various speech processing tasks. However, huge number of parameters in speech SSL models necessitate the compression to a more compact model for wider usage in academia or small companies. In this study, we suggest to reuse attention maps across the Transformer layers, so as to remove key and query parameters while retaining the number of layers. Furthermore, we propose a novel masking distillation strategy to improve the student model's speech representation quality. We extend the distillation loss to utilize both masked and unmasked speech frames to fully leverage the teacher model's high-quality representation. Our universal compression strategy yields the student model that achieves phoneme error rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB benchmark.
    
[^101]: 大型视觉-语言模型中的物体幻觉评估

    Evaluating Object Hallucination in Large Vision-Language Models. (arXiv:2305.10355v1 [cs.CV])

    [http://arxiv.org/abs/2305.10355](http://arxiv.org/abs/2305.10355)

    本研究是对大型视觉-语言模型中的物体幻觉问题进行的第一项系统研究，通过研究发现视觉指令可能影响幻觉，提出新的评估指标成功解决了现有评估方法的不足。

    

    发掘大型语言模型(LLM)因为其出色的语言能力近来已经开始研究大型视觉-语言模型(LVLM)，并将强大的LLM集成于LVLM中，以提高LVLM在复杂的多模态任务中的表现。虽然LVLM取得了很大进步，但是本研究发现LVLM存在长度幻觉问题，即它们倾向于生成与目标图像不一致的物体描述。为了调查这个问题，本研究开展了第一项系统研究，评估了LVLM中的物体幻觉。我们对几个代表性的LVLM进行了评估实验，并表明它们大多数都存在严重的物体幻觉问题。我们进一步探讨了视觉指令可能会影响幻觉，并发现在视觉指令中经常出现或与图像中的物体共现的物体，更容易被LVLM产生幻觉。此外，我们发现现有的评估方法可能会受到输入指令的影响，不能足以识别物体幻觉。为了解决这个问题，我们提出了一种新的评估指标，可以有效地评估物体幻觉问题。实验结果表明，我们提出的指标不仅可以有效地识别物体幻觉问题，还可以提供有关幻觉问题出现位置和如何缓解它的见解。

    Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently explored by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issue. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently occur in the visual instructions or co-occur with the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we find that existing evaluation methods might be affected by the input instructio
    
[^102]: 面向模型预测解释的非对称特征交互

    Asymmetric feature interaction for interpreting model predictions. (arXiv:2305.07224v1 [cs.CL])

    [http://arxiv.org/abs/2305.07224](http://arxiv.org/abs/2305.07224)

    本文提出了一种解释模型，能够探索深度神经自然语言处理模型推理中的非对称高阶特征交互。在两个情感分类数据集上的实验结果表明，该模型在识别影响特征方面优于现有特征交互归因方法。

    

    在自然语言处理领域，深度神经网络能够模拟上下文之间的复杂交互，并在一系列自然语言处理任务上取得了令人瞩目的成果。先前有关特征交互归因的研究主要集中在对称交互的研究上，它只能解释单个词汇组合后对模型预测的附加影响，而无法捕捉导致模型预测的非对称影响。在本文中，我们提出了一个非对称特征交互解释模型，旨在探索深度神经自然语言处理模型推理中的非对称高阶特征交互。通过表示我们的解释为一个有向交互图，我们实验验证了该图的可解释性，能够发现非对称特征交互作用。在两个情感分类数据集上的实验结果表明，我们的模型在识别影响特征方面优于现有特征交互归因方法。

    In natural language processing (NLP), deep neural networks (DNNs) could model complex interactions between context and have achieved impressive results on a range of NLP tasks. Prior works on feature interaction attribution mainly focus on studying symmetric interaction that only explains the additional influence of a set of words in combination, which fails to capture asymmetric influence that contributes to model prediction. In this work, we propose an asymmetric feature interaction attribution explanation model that aims to explore asymmetric higher-order feature interactions in the inference of deep neural NLP models. By representing our explanation with an directed interaction graph, we experimentally demonstrate interpretability of the graph to discover asymmetric feature interactions. Experimental results on two sentiment classification datasets show the superiority of our model against the state-of-the-art feature interaction attribution methods in identifying influential featu
    
[^103]: 多粒度超图兴趣建模对话式推荐算法

    Multi-grained Hypergraph Interest Modeling for Conversational Recommendation. (arXiv:2305.04798v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2305.04798](http://arxiv.org/abs/2305.04798)

    本文提出了一种多粒度超图兴趣建模方法，通过利用历史对话数据丰富当前对话的上下文，从不同角度捕捉用户兴趣。采用超图结构表示复杂的语义关系，建模用户的历史对话会话，捕捉粗粒度的会话级关系。

    

    对话式推荐系统通过自然语言的多轮对话与用户进行交互，旨在为用户的即时信息需求提供高质量的推荐。尽管已经做出了很多有效的对话式推荐系统，但大多数仍然集中在当前对话的上下文信息上，通常会遇到数据稀缺的问题。因此，我们考虑利用历史对话数据来丰富当前对话的有限上下文。在本文中，我们提出了一种新颖的多粒度超图兴趣建模方法，以从不同的角度捕捉复杂历史数据下的用户兴趣。作为核心思想，我们使用超图来表示历史对话中复杂的语义关系。在我们的方法中，我们首先使用超图结构来建模用户的历史对话会话，并形成一个基于会话的超图，该超图捕捉了粗粒度的会话级关系。

    Conversational recommender system (CRS) interacts with users through multi-turn dialogues in natural language, which aims to provide high-quality recommendations for user's instant information need. Although great efforts have been made to develop effective CRS, most of them still focus on the contextual information from the current dialogue, usually suffering from the data scarcity issue. Therefore, we consider leveraging historical dialogue data to enrich the limited contexts of the current dialogue session.  In this paper, we propose a novel multi-grained hypergraph interest modeling approach to capture user interest beneath intricate historical data from different perspectives. As the core idea, we employ hypergraph to represent complicated semantic relations underlying historical dialogues. In our approach, we first employ the hypergraph structure to model users' historical dialogue sessions and form a session-based hypergraph, which captures coarse-grained, session-level relation
    
[^104]: NLI4CT：面向临床试验报告的多证据自然语言推理

    NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])

    [http://arxiv.org/abs/2305.03598](http://arxiv.org/abs/2305.03598)

    本文提出了一种面向临床试验报告的自然语言推理模型，通过检索支持事实来确定自然语言陈述和CTR之间的推理关系。

    

    如何解释和检索用于支持临床决策的医学证据？多年来，积累下来的临床试验报告包含了发展个性化医学所必需的信息。然而，为了找到最佳的实验治疗证据，手动检查超过400,000个临床试验报告是实际上不可行的。自然语言推理（NLI）提供了一个潜在的解决方案，通过允许可扩展计算文本蕴含关系。然而，现有的NLI模型在生物医学语料库上表现不佳，之前发布的数据集无法捕捉CTR推理的全部复杂性。在本文中，我们提出了一种新的资源，以推进关于CTR推理的NLI研究。该资源包括两个主要任务。首先，确定自然语言陈述和CTR之间的推理关系。其次，检索支持事实以证明预测的关系。我们提供了NLI4CT，一个基于CTR的语料库。

    How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual entailment. However, existing NLI models perform poorly on biomedical corpora, and previously published datasets fail to capture the full complexity of inference over CTRs. In this work, we present a novel resource to advance research on NLI for reasoning on CTRs. The resource includes two main tasks. Firstly, to determine the inference relation between a natural language statement, and a CTR. Secondly, to retrieve supporting facts to justify the predicted relation. We provide NLI4CT, a corpus of
    
[^105]: 语言、时间偏好和消费行为：大型语言模型的证据

    Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models. (arXiv:2305.02531v1 [econ.GN])

    [http://arxiv.org/abs/2305.02531](http://arxiv.org/abs/2305.02531)

    本研究分析了大型语言模型在不同语言提示下的奖励时间偏好，并发现GPT在具有较弱未来时态的语言下表现出更大的耐心，这与使用该语言的人类的偏好相似。

    

    语言对我们对时间和奖励的感知有很大的影响。这引发了一个问题，即当以不同的语言询问大型语言模型时，它们是否显示出不同的奖励时间偏好，并且它们的选择是否类似于人类的选择。本研究分析了GPT-3.5（以下简称GPT）在多种语言提示下的响应，探索了较小、较早的奖励和较大、较晚的奖励之间的偏好。我们的结果显示，当以语义含义较弱的未来时态参考（FTR），如德语和汉语，为提示语时，GPT表现出更大的耐心，相比英语和法语等具有强大FTR的语言。这些发现与现有文献一致，并表明了GPT的选择与这些语言的使用者的偏好之间的关联。然而，进一步的分析揭示了较早或较晚奖励的偏好并没有随着奖励差异系统地改变，这表明了一种词典序优先的选择。

    Language has a strong influence on our perceptions of time and rewards. This raises the question of whether large language models, when asked in different languages, show different preferences for rewards over time and if their choices are similar to those of humans. In this study, we analyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in multiple languages, exploring preferences between smaller, sooner rewards and larger, later rewards. Our results show that GPT displays greater patience when prompted in languages with weak future tense references (FTR), such as German and Mandarin, compared to languages with strong FTR, like English and French. These findings are consistent with existing literature and suggest a correlation between GPT's choices and the preferences of speakers of these languages. However, further analysis reveals that the preference for earlier or later rewards does not systematically change with reward gaps, indicating a lexicographic preferen
    
[^106]: 分解增强推理的自我评估引导解码

    Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding. (arXiv:2305.00633v1 [cs.CL])

    [http://arxiv.org/abs/2305.00633](http://arxiv.org/abs/2305.00633)

    本论文提出了一种通过自我评估引导解码提高推理的方法，使用经过校准的自动标准探索推理搜索空间，使搜索能够产生更高质量的最终预测结果；使用自我评估引导的随机束搜索在产生推理链的质量和多样性之间平衡权衡，适应多数投票，并且可以准确判断逻辑错误，提高一致性和鲁棒性。

    

    我们提出了一种有效的提示方法，通过随机束搜索结合自我评估引导。我们的方法使用经过校准的自动标准探索推理搜索空间。这使得有效搜索能够产生更高质量的最终预测结果。使用自我评估引导的随机束搜索，我们在产生推理链的质量和多样性之间平衡权衡，从而能够适应多数投票，并在GSM8K、AQUA和StrategyQA基准测试中以少量示例准确性分别超越对应的Codex-backboned基线$6.34\%$、$9.56\%$和$5.46\%$。对我们的分解式推理分析发现，它可以指出逻辑错误并导致更高的一致性和鲁棒性。

    We propose an effective prompting approach that integrates self-evaluation guidance through stochastic beam search. Our approach explores the reasoning search space using a well-calibrated automatic criterion. This enables an efficient search to produce higher-quality final predictions. With the self-evaluation guided stochastic beam search, we also balance the quality--diversity trade-off in the generation of reasoning chains. This allows our approach to adapt well with majority voting and surpass the corresponding Codex-backboned baselines by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQUA, and StrategyQA benchmarks, respectively, in few-shot accuracy. Analysis of our decompositional reasoning finds it pinpoints logic failures and leads to higher consistency and robustness.
    
[^107]: ChatGPT是一款好的关键词生成器吗？初步研究。

    Is ChatGPT A Good Keyphrase Generator? A Preliminary Study. (arXiv:2303.13001v1 [cs.CL])

    [http://arxiv.org/abs/2303.13001](http://arxiv.org/abs/2303.13001)

    本文对ChatGPT作为关键词生成器进行了初步研究，发现其在各个方面的性能表现良好，特别是在多领域关键词生成方面。ChatGPT仍面临生成缺失关键词的挑战。

    

    ChatGPT的出现引起了计算语言学界的重视。为了展示其作为关键词生成器的能力，我们对ChatGPT进行了初步评估以用于关键词生成任务。我们评估了其在各个方面的性能，包括关键词生成提示，关键词生成多样性，多领域关键词生成和长文本理解。我们的评估基于六个基准数据集，并采用OpenAI建议的提示，并将其扩展为六个候选提示。我们发现ChatGPT在所有六个候选提示上表现出色，在不同数据集之间观察到了轻微的性能差异。基于我们的发现，我们得出结论，ChatGPT有很大的关键词生成潜力。此外，我们发现ChatGPT在生成缺失关键词方面仍面临挑战。最后，在最后一节中，我们还介绍了一些限制和未来的研究方向。

    The emergence of ChatGPT has recently garnered significant attention from the computational linguistics community. To demonstrate its capabilities as a keyphrase generator, we conduct a preliminary evaluation of ChatGPT for the keyphrase generation task. We evaluate its performance in various aspects, including keyphrase generation prompts, keyphrase generation diversity, multi-domain keyphrase generation, and long document understanding. Our evaluation is based on six benchmark datasets, and we adopt the prompt suggested by OpenAI while extending it to six candidate prompts. We find that ChatGPT performs exceptionally well on all six candidate prompts, with minor performance differences observed across the datasets. Based on our findings, we conclude that ChatGPT has great potential for keyphrase generation. Moreover, we discover that ChatGPT still faces challenges when it comes to generating absent keyphrases. Meanwhile, in the final section, we also present some limitations and futu
    
[^108]: 描述一个Aucklet：生成基于感知的类别描述

    Describe me an Aucklet: Generating Grounded Perceptual Category Descriptions. (arXiv:2303.04053v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.04053](http://arxiv.org/abs/2303.04053)

    本文介绍了一个用于测试多模态语言模型中基于类别的感知基础的框架，并比较了基于原型和样本的表示的性能。

    

    人类演讲者可以生成从实例级别抽象出来的感知概念描述。此外，其他演讲者可以利用这些描述来学习这些概念的临时表示。在语言和视觉领域，学习和使用抽象感知概念的问题还不够研究。这个问题对多模态自然语言处理中的表示学习领域也非常相关。在本文中，我们介绍了一个用于测试多模态语言模型中基于类别的感知基础的框架。特别地，我们训练了不同的神经网络来生成和解释视觉类别的描述。我们用解释模型的零样本分类性能来衡量两个模型的交流成功度，我们认为这是感知基础的一个指标。利用这个框架，我们比较了基于原型和样本的表示的性能。最后，我们展示了交流成功揭示了性能问题。

    Human speakers can generate descriptions of perceptual concepts, abstracted from the instance-level. Moreover, such descriptions can be used by other speakers to learn provisional representations of those concepts. Learning and using abstract perceptual concepts is under-investigated in the language-and-vision field. The problem is also highly relevant to the field of representation learning in multi-modal NLP. In this paper, we introduce a framework for testing category-level perceptual grounding in multi-modal language models. In particular, we train separate neural networks to generate and interpret descriptions of visual categories. We measure the communicative success of the two models with the zero-shot classification performance of the interpretation model, which we argue is an indicator of perceptual grounding. Using this framework, we compare the performance of prototype- and exemplar-based representations. Finally, we show that communicative success exposes performance issues
    
[^109]: 阅读并获得回报：在与指导手册的帮助下学习玩Atari游戏

    Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals. (arXiv:2302.04449v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04449](http://arxiv.org/abs/2302.04449)

    本论文提出了阅读并奖励的框架，通过阅读Atari游戏开发者发布的指导手册，以提高强化学习算法在Atari游戏中的效率。该框架包含一个QA提取模块和一个推理模块，能够从指导手册中提取关键信息，并评估物体与智能体的交互效果。

    

    长期以来，高样本复杂性一直是强化学习面临的挑战。然而，人类学习执行任务的方式不仅仅是通过交互或演示，还包括阅读非结构化文本文档，例如指导手册。指导手册和维基页面是最丰富的数据之一，它们可以提供有关宝贵特征、策略、任务特定的环境动态和奖励结构的信息，因此我们假设利用人写的指导手册来帮助学习特定任务的策略将导致更高效和更优秀的智能体。我们提出了阅读并奖励的框架。阅读并奖励通过阅读Atari游戏开发者发布的指导手册来加速RL算法。我们的框架包括一个QA提取模块，用于提取和总结指导手册中的相关信息，以及一个推理模块，根据指导手册中的信息评估物体-智能体的交互效果。一个辅助的反馈机制可以提高效果。

    High sample complexity has long been a challenge for RL. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent. We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. An auxiliary re
    
[^110]: ZipLM: 语言模型的推理感知结构化剪枝

    ZipLM: Inference-Aware Structured Pruning of Language Models. (arXiv:2302.04089v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04089](http://arxiv.org/abs/2302.04089)

    ZipLM是一种新型的语言模型压缩方法，能够在任何给定的推理环境中实现与目标运行速度相匹配的最先进压缩模型。与现有方法相比，ZipLM在速度和准确性之间取得了最佳的权衡，并且以更低的计算成本获得了更好的结果。

    

    大规模语言模型（LLM）的突破性性能给计算和部署成本带来了重大负担。本文通过提出一种名为ZipLM的新型结构化压缩方法，向解决这个问题迈进。ZipLM在达到一组目标推理时速度优于现有的方法，并在任何给定的推理环境中匹配一组目标运行时加速度。具体而言，给定模型、数据集、推理环境以及一组加速度目标，ZipLM迭代地识别并删除损失时长权衡最差的组件。与先前专门用于后训练/一次性或逐渐压缩设置，并且仅适用于特定的模型家族（如BERT（编码器）或GPT（解码器））的方法不同，ZipLM在所有这些设置中生成了最先进的压缩模型。此外，与先前的蒸馏和修剪方法相比，ZipLM以更低的计算成本获得了更好的结果。

    The breakthrough performance of large language models (LLMs) comes with major computational footprints and high deployment costs. In this paper, we progress towards resolving this problem by proposing a novel structured compression approach for LLMs, called ZipLM. ZipLM achieves state-of-the-art accuracy-vs-speedup, while matching a set of desired target runtime speedups in any given inference environment. Specifically, given a model, a dataset, an inference environment, as well as a set of speedup targets, ZipLM iteratively identifies and removes components with the worst loss-runtime trade-off. Unlike prior methods that specialize in either the post-training/one-shot or the gradual compression setting, and only for specific families of models such as BERT (encoder) or GPT (decoder), ZipLM produces state-of-the-art compressed models across all these settings. Furthermore, ZipLM achieves superior results for a fraction of the computational cost relative to prior distillation and prunin
    
[^111]: 大型语言模型中的事件知识：不可能性和不太可能性之间的差距

    Event knowledge in large language models: the gap between the impossible and the unlikely. (arXiv:2212.01488v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.01488](http://arxiv.org/abs/2212.01488)

    大型语言模型拥有丰富的事件知识，几乎总是将可能事件的描述比不可能事件的描述赋予更高的可能性。

    

    语言语料库中的词共现模式包含着意想不到的概念知识。通过训练大型语言模型(LLMs)来预测上下文中的词语，这些模型能够利用这些模式，在需要世界知识的各种语义任务上取得令人印象深刻的性能。关于LLMs的语义能力的重要但鲜为研究的问题是它们是否获得了常见事件的一般化知识。在这里，我们测试了五个预训练的LLMs（从2018年的BERT到2023年的MPT）是否比同一事件的不太可能的版本更可能地分配给合理的代理-患者相互作用。使用三个精心策划的最小句对集合（总数n=1,215），我们发现预训练的LLMs拥有相当大的事件知识，表现优于其他分布式语言模型。特别是，它们几乎总是将可能事件与不可能事件相比赋予更高的可能性（教师买了笔记本电脑相对于笔记本电脑买了教师）。

    Word co-occurrence patterns in language corpora contain a surprising amount of conceptual knowledge. Large language models (LLMs), trained to predict words in context, leverage these patterns to achieve impressive performance on diverse semantic tasks requiring world knowledge. An important but understudied question about LLMs' semantic abilities is whether they acquire generalized knowledge of common events. Here, we test whether five pre-trained LLMs (from 2018's BERT to 2023's MPT) assign higher likelihood to plausible descriptions of agent-patient interactions than to minimally different implausible versions of the same event. Using three curated sets of minimal sentence pairs (total n=1,215), we found that pre-trained LLMs possess substantial event knowledge, outperforming other distributional language models. In particular, they almost always assign higher likelihood to possible vs. impossible events (The teacher bought the laptop vs. The laptop bought the teacher). However, LLMs
    
[^112]: Aksharantar: 开放的印度语言转写数据集和模型，面向未来的十亿用户

    Aksharantar: Open Indic-language Transliteration datasets and models for the Next Billion Users. (arXiv:2205.03018v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.03018](http://arxiv.org/abs/2205.03018)

    Aksharantar是最大的公开可用的印度语言转写数据集，包含2600万个转写对，涵盖21种印度语言和12种文字。同时，它也提供了一个测试集，用于对转写模型进行细粒度分析。利用该数据集，我们训练了一个多语言转写模型IndicXlit，取得了较好的性能。

    

    在印度语言环境中，转写非常重要，因为使用了多种文字，并广泛使用罗马化输入。然而，公开可用的训练和评估数据集很少。我们介绍了Aksharantar，这是最大的公开可用的印度语言转写数据集，通过从单语和平行语料库中挖掘，并收集人工注释数据而创建。该数据集包含了来自三个语言家族、使用12种文字的21种印度语言的2600万个转写对。Aksharantar比现有数据集大21倍，也是7种语言和1个语言家族中首个公开可用的数据集。我们还介绍了Aksharantar测试集，包含103k个单词对，涵盖19种语言，可以对本地词源词、外来词、频繁词和罕见词的转写模型进行细粒度分析。利用训练集，我们训练了IndicXlit，这是一个提高了性能的多语言转写模型。

    Transliteration is very important in the Indian language context due to the usage of multiple scripts and the widespread use of romanized inputs. However, few training and evaluation sets are publicly available. We introduce Aksharantar, the largest publicly available transliteration dataset for Indian languages created by mining from monolingual and parallel corpora, as well as collecting data from human annotators. The dataset contains 26 million transliteration pairs for 21 Indic languages from 3 language families using 12 scripts. Aksharantar is 21 times larger than existing datasets and is the first publicly available dataset for 7 languages and 1 language family. We also introduce the Aksharantar testset comprising 103k word pairs spanning 19 languages that enables a fine-grained analysis of transliteration models on native origin words, foreign words, frequent words, and rare words. Using the training set, we trained IndicXlit, a multilingual transliteration model that improves 
    
[^113]: pysentimiento: 一个用于观点挖掘和社交自然语言处理任务的Python工具包

    pysentimiento: A Python Toolkit for Opinion Mining and Social NLP tasks. (arXiv:2106.09462v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2106.09462](http://arxiv.org/abs/2106.09462)

    pysentimiento是一个多语言的Python工具包，用于观点挖掘和社交自然语言处理任务，提供了易于使用的库和最先进的模型，研究人员可以利用这些技术进行研究。

    

    近年来，从用户生成的文本中提取观点和信息引起了很大的兴趣，主要是由于社交媒体中内容的前所未有的数量。然而，社会研究人员在采用最先进的工具进行这些任务时会遇到一些问题，因为这些工具通常落后于商业API，不适用于除英语以外的其他语言，或者对非专家来说非常复杂。为了解决这些问题，我们提出了pysentimiento，这是一个全面的多语言Python工具包，专为观点挖掘和其他社交自然语言处理任务而设计。这个开源库提供了易于使用的Python库，其中包含了用于西班牙语、英语、意大利语和葡萄牙语的最先进模型，可以让研究人员利用这些技术。我们对几种预训练语言模型在各种任务、语言和数据集上的性能进行了全面评估，包括对结果公平性的评估。

    In recent years, the extraction of opinions and information from user-generated text has attracted a lot of interest, largely due to the unprecedented volume of content in Social Media. However, social researchers face some issues in adopting cutting-edge tools for these tasks, as they are usually behind commercial APIs, unavailable for other languages than English, or very complex to use for non-experts. To address these issues, we present pysentimiento, a comprehensive multilingual Python toolkit designed for opinion mining and other Social NLP tasks. This open-source library brings state-of-the-art models for Spanish, English, Italian, and Portuguese in an easy-to-use Python library, allowing researchers to leverage these techniques. We present a comprehensive assessment of performance for several pre-trained language models across a variety of tasks, languages, and datasets, including an evaluation of fairness in the results.
    
[^114]: 关于连续约束满足问题的分类研究

    On Classifying Continuous Constraint Satisfaction Problems. (arXiv:2106.02397v5 [cs.CC] UPDATED)

    [http://arxiv.org/abs/2106.02397](http://arxiv.org/abs/2106.02397)

    本论文研究了连续约束满足问题的分类，重点关注具有加法约束和其他技术条件的问题，并探讨了其与存在性实数理论的关系。

    

    连续约束满足问题（CCSP）是具有区间域 $U \subset \mathbb{R}$ 的约束满足问题（CSP）。我们进行了系统研究，对满足存在性实数理论（ER-complete）的CCSP进行分类。为了定义这个类别，我们首先考虑问题ETR，它也代表存在性实数理论。在这个问题的实例中，我们被给定一个形如 $\exists x_1, \ldots, x_n \in \mathbb{R} : \Phi(x_1, \ldots, x_n)$ 的句子，其中 $\Phi$ 是一个由符号 $\{0, 1, +, \cdot, \geq, >, \wedge, \vee, \neg\}$ 构成的良好形式的非量化公式，目标是检查这个句子是否为真。现在，类别ER是所有能在多项式时间内归约到ETR的问题的集合。已知 NP $\subseteq$ ER $\subseteq$ PSPACE。我们将注意力集中在具有加法约束（$x + y = z$）和其他一些温和技术条件的CCSP上。以前已经证明了一些性质。

    A continuous constraint satisfaction problem (CCSP) is a constraint satisfaction problem (CSP) with an interval domain $U \subset \mathbb{R}$. We engage in a systematic study to classify CCSPs that are complete of the Existential Theory of the Reals, i.e., ER-complete. To define this class, we first consider the problem ETR, which also stands for Existential Theory of the Reals. In an instance of this problem we are given some sentence of the form $\exists x_1, \ldots, x_n \in \mathbb{R} : \Phi(x_1, \ldots, x_n)$, where $\Phi$ is a well-formed quantifier-free formula consisting of the symbols $\{0, 1, +, \cdot, \geq, >, \wedge, \vee, \neg\}$, the goal is to check whether this sentence is true. Now the class ER is the family of all problems that admit a polynomial-time many-one reduction to ETR. It is known that NP $\subseteq$ ER $\subseteq$ PSPACE.  We restrict our attention on CCSPs with addition constraints ($x + y = z$) and some other mild technical condition. Previously, it was sho
    
[^115]: 使用分布语义研究反词行为的探究

    Investigating Antigram Behaviour using Distributional Semantics. (arXiv:1901.05066v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/1901.05066](http://arxiv.org/abs/1901.05066)

    本文通过使用分布语义研究探究反词行为，提出了一种基于规则的算法用于检测反词。在小数据集上取得了39%的准确率，展示了该领域仍有待进一步研究。

    

    计算语言学领域不断为研究提出新的挑战和主题，无论是分析词语随时间变化的用法还是识别看似不相关的词语之间的关系。在这篇论文中，我们将回文和反词作为具有独特属性的词语进行了研究。我们使用GloVe嵌入来生成给定词语的回文，并确定生成的回文之间是否存在反词（语义上相反的回文）的关系。我们提出了一种基于规则的简单但可解释的算法来检测反词。在仅有12个反词的小数据集上，我们的方法准确率达到了39％，这表明在这个领域还有很多工作要做。

    The field of computational linguistics constantly presents new challenges and topics for research. Whether it be analyzing word usage changes over time or identifying relationships between pairs of seemingly unrelated words. To this point, we identify Anagrams and Antigrams as words possessing such unique properties. The presented work is an exploration into generating anagrams from a given word and determining whether there exists antigram (semantically opposite anagrams) relationships between the pairs of generated anagrams using GloVe embeddings. We propose a rudimentary, yet interpretable, rule-based algorithm for detecting antigrams. On a small dataset of just 12 antigrams, our approach yielded an accuracy of 39\% which shows that there is much work left to be done in this space.
    

