# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity.](http://arxiv.org/abs/2310.07521) | 这项调查研究了大型语言模型（LLMs）中的事实性问题，包括其可能带来的后果和挑战，以及存储、处理和评估事实的方法。同时，提出了一些增强LLM事实性的策略，涵盖了不同领域的需求。 |
| [^2] | [KwaiYiiMath: Technical Report.](http://arxiv.org/abs/2310.07488) | KwaiYiiMath是一个用于增强数学推理能力的大型语言模型，通过应用监督微调和人类反馈强化学习，在英语和中文数学任务上取得了最先进的性能，并且能够正确解决生成的问题过程。 |
| [^3] | [Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction.](http://arxiv.org/abs/2310.07487) | Cognate Transformer是一个用于自动语音重建和同源反射预测的模型，基于多序列对齐数据进行训练，并在计算历史语言学领域取得了良好的表现。 |
| [^4] | [Adapting the adapters for code-switching in multilingual ASR.](http://arxiv.org/abs/2310.07423) | 本论文研究了如何在多语言自动语音识别中有效地调整适配器来处理代码切换语音，提出了将两个语言适配器的信息融合并将代码切换建模为潜在二进制序列的方法。 |
| [^5] | [DASpeech: Directed Acyclic Transformer for Fast and High-quality Speech-to-Speech Translation.](http://arxiv.org/abs/2310.07403) | DASpeech是一个非自回归的直接语音翻译模型，使用有向无环图（DAG）来实现快速和高质量的语音翻译。 |
| [^6] | [Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation.](http://arxiv.org/abs/2310.07397) | 这项工作提出了一个自动数据集筛选框架，并构建了一个大规模的个性化面向目标对话数据集，名为TopDial。该数据集质量高，有助于探索个性化目标导向的对话。 |
| [^7] | [Linguistic laws in biology.](http://arxiv.org/abs/2310.07387) | 这项研究发现了生物学中与语言学定律一致的统计模式，并提出了一个新的概念框架，将不同层次的分析整合起来，揭示自然系统的基本组织规律。 |
| [^8] | [Fast-ELECTRA for Efficient Pre-training.](http://arxiv.org/abs/2310.07347) | 提出了一种快速ELECTRA的方法，利用现有的语言模型作为辅助模型，通过温度缩放平滑主模型的输出分布，达到与最先进的ELECTRA预训练方法相媲美的性能，并显著减少了辅助模型共同训练带来的计算和内存成本。 |
| [^9] | [Investigating the Effect of Language Models in Sequence Discriminative Training for Neural Transducers.](http://arxiv.org/abs/2310.07345) | 本研究探讨了在基于音素的神经变换器序列辨别训练中使用不同上下文长度和标签单元的语言模型的效果。研究发现，在训练中使用单词级别的语言模型优于音素级别的模型，并且概率计算中的上下文大小对性能影响有限。研究结果还揭示了假设空间质量在序列辨别训练中的重要性。 |
| [^10] | [How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances.](http://arxiv.org/abs/2310.07343) | 本文综述了如何在不重新训练的情况下使大型语言模型与不断变化的世界知识对齐的最新进展，并讨论了相关挑战和未来方向。 |
| [^11] | [An Empirical Study of Instruction-tuning Large Language Models in Chinese.](http://arxiv.org/abs/2310.07328) | 本论文进行了对中文大语言模型的指导调整的实证研究，探索了LLM基础、参数高效的方法和指令数据类型的影响，并研究了其他因素的影响，为定制能更好响应中文指令的LLM提供了有价值的发现。 |
| [^12] | [On the Impact of Cross-Domain Data on German Language Models.](http://arxiv.org/abs/2310.07321) | 本研究通过对德语语言模型进行实验，发现将数据多样性置于数据质量之上的交叉领域数据集训练方法，可以显著提高模型的性能，并超过了之前的最先进模型。 |
| [^13] | [SNOiC: Soft Labeling and Noisy Mixup based Open Intent Classification Model.](http://arxiv.org/abs/2310.07306) | SNOiC模型是一种基于软标签和噪声混合的开放意图分类模型，通过减少偏差和生成伪数据来提高识别开放意图的性能。 |
| [^14] | [Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions.](http://arxiv.org/abs/2310.07301) | 本文提出了Parrot，一个高度可扩展的解决方案，通过自动生成高质量的指令调优数据，进一步完善了多轮聊天模型在对话中的效果。 |
| [^15] | [RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation.](http://arxiv.org/abs/2310.07299) | 这项研究提出了一个叫做RobustGEC的基准测试，用于评估语法错误修正系统的上下文鲁棒性。研究发现目前最先进的系统仍然无法很好地应对上下文扰动，并提出了一种简单而有效的解决方法。 |
| [^16] | [Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators.](http://arxiv.org/abs/2310.07289) | 该论文对大型语言模型（LLMs）作为知识生成器的能力进行了全面评估，介绍了一个评估框架CONNER。研究发现，生成的知识的事实性不如准确性重要，而生成的相关性和连贯性更为重要。 |
| [^17] | [Typing to Listen at the Cocktail Party: Text-Guided Target Speaker Extraction.](http://arxiv.org/abs/2310.07284) | 研究人员提出了一种名为LLM-TSE的模型，该模型利用大型语言模型从用户键入的文本输入中提取语义线索，以增强目标说话人提取(TSE)模型的灵活性和可控性。 |
| [^18] | [An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT.](http://arxiv.org/abs/2310.07282) | 本研究分析了在医疗保健领域应用大型语言模型（尤其是BioBERT）的可行性，并提出了针对医疗保健领域的微调方法。研究突出了BioBERT对于解决与生物医学文本挖掘相关任务的特定要求的适用性。 |
| [^19] | [Enhancing expressivity transfer in textless speech-to-speech translation.](http://arxiv.org/abs/2310.07279) | 该研究提出了一种新方法来提高无文本语音翻译系统中的表达能力转换，通过运用多语言情感嵌入来捕捉语言无关的信息，并有效预测目标语言的音调和时长。实验结果表明，该方法相对于现有最先进系统在表达能力转换方面具有优势。 |
| [^20] | [BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations.](http://arxiv.org/abs/2310.07276) | BioT5是一个全面的预训练框架，在生物学中利用化学知识和自然语言关联丰富了跨模态整合，通过鲁棒的分子表示和上下文知识提取，实现了更有效的信息利用，展现出卓越的性能。 |
| [^21] | [Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs.](http://arxiv.org/abs/2310.07251) | 本文提出了一种在LLM中应用通用伦理推理能力而非特定伦理原则的方法，以处理全球范围的价值多元性。作者开发了一个框架，将道德困境与不同形式的规范伦理以及不同抽象层次的道德原则相结合。初步实验表明，尽管GPT-4几乎可以完美地进行伦理推理，但这些模型仍然存在对西方和以英语为母语的社会道德价值的偏见。 |
| [^22] | [Exploring the Landscape of Large Language Models In Medical Question Answering: Observations and Open Questions.](http://arxiv.org/abs/2310.07225) | 通过评估多种流行的大型语言模型在医学问题方面的知识，本研究提供了对这些模型作为一个群体的初步观察，并提出了进一步研究的开放问题。 |
| [^23] | [Adaptive Gating in Mixture-of-Experts based Language Models.](http://arxiv.org/abs/2310.07188) | 本文介绍了一种自适应门控的混合专家语言模型训练策略，通过根据标记的专家概率分布将标记分配给变量数量的专家，同时保持模型的稀疏性和提高训练效率。 |
| [^24] | [Online Speculative Decoding.](http://arxiv.org/abs/2310.07177) | 在线推测解码是通过利用多余计算能力，在LLM服务集群中持续更新草稿模型，从而加速大型语言模型推理的一种方法。 |
| [^25] | [PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a Language Model.](http://arxiv.org/abs/2310.07170) | 本文提出了一种从零开始构建知识图谱的方法，通过引导众包工作者和大型语言模型，该方法在构建日语事件知识图谱和训练常识生成模型方面取得了良好的结果。 |
| [^26] | [Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms.](http://arxiv.org/abs/2310.07161) | 本研究在VoIP通信领域中探索了声学转换的复杂性，并通过分析心理声学指标，揭示了语音增强对VoIP系统的影响。 |
| [^27] | ["A Tale of Two Movements": Identifying and Comparing Perspectives in #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly Supervised Graph-based Structured Prediction.](http://arxiv.org/abs/2310.07155) | 本论文提出了一种基于图的弱监督方法，用于识别和比较#BlackLivesMatter和#BlueLivesMatter运动相关推文中的观点。通过将文本转换为图，并根据作者的社交网络进行结构化预测，该方法能够在缺乏标注数据的情况下模拟运动中的观点。 |
| [^28] | [QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources.](http://arxiv.org/abs/2310.07147) | 我们提出了一种新的QFT框架，可以对LLMs进行内存高效的全参数微调，而不损害性能。 |
| [^29] | [Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting.](http://arxiv.org/abs/2310.07146) | 该论文借助大型语言模型进行心理疗法的人工智能辅助，在认知失真检测任务中提出了思维诱导诊断（DoT）方法，通过三个阶段对患者的言语进行诊断，取得了显著的改进。 |
| [^30] | [AE-smnsMLC: Multi-Label Classification with Semantic Matching and Negative Label Sampling for Product Attribute Value Extraction.](http://arxiv.org/abs/2310.07137) | 本论文提出了一种基于语义匹配和负标签采样的多标签分类模型 AE-smnsMLC，用于解决产品属性值提取的问题。该模型将属性值提取任务转化为多标签分类任务，可以应用于只有属性值注释的实际场景，而无需属性值的位置信息注释。 |
| [^31] | [Comparing Styles across Languages.](http://arxiv.org/abs/2310.07135) | 本研究通过引入解释框架，从多语言语言模型中提取风格差异并比较不同语言之间的风格，创建了全面的多语言礼貌数据集，探索了礼貌在四种语言中的变化，为评估语言类别对风格变化的贡献和了解世界各地人们的不同沟通方式提供了有效的方法和解释洞察力。 |
| [^32] | [The Temporal Structure of Language Processing in the Human Brain Corresponds to The Layered Hierarchy of Deep Language Models.](http://arxiv.org/abs/2310.07106) | 本文展示了深度语言模型(DLMs)的分层结构与人脑语言理解的时间动态之间存在强相关性，通过采用电子皮层图(ECoG)数据来优化时间分辨率，为深入了解人脑语言处理机制提供了新的视角。 |
| [^33] | [Sparse Universal Transformer.](http://arxiv.org/abs/2310.07096) | 本文介绍了稀疏通用Transformer（SUT），它通过利用稀疏混合专家（SMoE）和一种新的基于切棍法的动态停止机制来减少计算复杂性，同时保持参数效率和泛化能力。实验证明，SUT在形式语言任务上具有与强基线模型相当的性能，并能显著降低计算资源使用。 |
| [^34] | [Argumentative Stance Prediction: An Exploratory Study on Multimodality and Few-Shot Learning.](http://arxiv.org/abs/2310.07093) | 该研究通过探索性研究发现，在多模态立场预测中，微调的文本语言模型集合表现更好，并且多模态模型更适合将图像内容进行自然语言摘要，使用上下文示例可以提高LLM的少样本预测性能。 |
| [^35] | [Jaeger: A Concatenation-Based Multi-Transformer VQA Model.](http://arxiv.org/abs/2310.07091) | Jaeger是一种基于连接的多变换器VQA模型，利用RoBERTa large和GPT2-xl作为特征提取器，通过并行考虑多源信息来增强模型表征能力。 |
| [^36] | [Diversity of Thought Improves Reasoning Abilities of Large Language Models.](http://arxiv.org/abs/2310.07088) | 本文提出了一种方法，通过改变输入提示来提高大规模语言模型的推理能力，从而改善模型在复杂推理场景中的表现。这种方法自动采集模型反馈，生成适合问题的多样化提示，并通过多次推理调用来集成这些多样化的提示。 |
| [^37] | [Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting.](http://arxiv.org/abs/2310.07081) | 本文通过引入检索增强和损失加权两种方法，成功改进了机器翻译系统对于惯用表达的翻译准确性。 |
| [^38] | [Auditing and Robustifying COVID-19 Misinformation Datasets via Anticontent Sampling.](http://arxiv.org/abs/2310.07078) | 本文拓展了对COVID-19虚假信息数据集进行审计和强化的研究。首先，发现在小数据上训练的专业分类器在野外环境中的性能表现有限。其次，提出了一种无需人工注释的主动学习方法，通过增加具有挑战性的抗内容来增强分类器。 |
| [^39] | [Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding.](http://arxiv.org/abs/2310.07075) | 本文提出了一种无语法错误且具有泛化能力的LLM工具使用方法ToolDec，通过有限状态解码算法消除了工具相关错误，使LLM能够有效选择工具，而无需微调或上下文文档。 |
| [^40] | [Large Language Models can Learn Rules.](http://arxiv.org/abs/2310.07064) | 大型语言模型(LLMs)在各种推理任务中展示了令人印象深刻的性能。为了提高提示方法的准确性和一致性，我们提出了Hypotheses-to-Theories (HtT)框架，用于学习LLMs推理的规则库，从而改进了现有的提示方法。 |
| [^41] | [DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records.](http://arxiv.org/abs/2310.07059) | 本文提出了DKEC，一种领域知识增强的分类器，用于医学诊断预测。它利用标签的注意力机制和组内训练方法来捕捉医学实体之间的语义关系，并增加罕见类别的样本数量。评估结果显示其在医学数据集上表现良好。 |
| [^42] | [Automatic Macro Mining from Interaction Traces at Scale.](http://arxiv.org/abs/2310.07023) | 本文介绍了一种基于大型语言模型的新方法，用于从移动交互轨迹中自动提取语义上有意义的宏任务。通过多项研究和实验证明了该方法的有效性和提取的宏任务的实用性。 |
| [^43] | [NEWTON: Are Large Language Models Capable of Physical Reasoning?.](http://arxiv.org/abs/2310.07018) | NEWTON是一个用于评估大型语言模型物理推理能力的仓库和基准，包含2800个物体-属性对和160K个问答问题。 |
| [^44] | [Answer Candidate Type Selection: Text-to-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs.](http://arxiv.org/abs/2310.07008) | 本文提出了一种新颖的方法，通过对预训练的文本到文本问答系统生成的候选答案基于其类型进行过滤和重新排序，以解决在知识图谱问答任务中，模型容量有限且对于含有不太流行实体的问题质量下降的问题。 |
| [^45] | [Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation.](http://arxiv.org/abs/2310.06987) | 本文介绍了一种生成攻击技术，通过操纵解码方法的变化，可以导致开源LLMs的灾难性越狱，将错位率从0%提高到了超过95%。这项攻击方法简单而有效，在11个语言模型中表现优于最先进的攻击方法，且计算量降低了30倍。 |
| [^46] | [Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models.](http://arxiv.org/abs/2310.06983) | 本文研究了如何利用违反期望机制在大型语言模型中降低用户预测误差。我们引入了元认知提示框架，并发现存储和检索违反用户期望的事实可以使模型以类似人类学习理论的方式了解用户。 |
| [^47] | [Why bother with geometry? On the relevance of linear decompositions of Transformer embeddings.](http://arxiv.org/abs/2310.06977) | 本研究研究了Transformer嵌入的线性分解方法在机器翻译解码器中的应用。结果显示，虽然分解指标与模型表现有效相关，但不同运行之间的变异性表明对于数学重述是否在实证上具有意义还需要进一步研究。 |
| [^48] | [Document-Level Supervision for Multi-Aspect Sentiment Analysis Without Fine-grained Labels.](http://arxiv.org/abs/2310.06940) | 本文提出了一种利用文档级监督进行多方面情感分析的方法，无需细粒度标签，可以检测文档中的多个方面，并能理解通过多个方面表达的情感。 |
| [^49] | [Sparse Finetuning for Inference Acceleration of Large Language Models.](http://arxiv.org/abs/2310.06927) | 本论文研究了大型语言模型的准确稀疏微调问题，提出了基于L2范数的蒸馏方法SquareHead，可以在高稀疏性下实现准确的恢复；同时展示了稀疏语言模型的实际效率，可在CPU和GPU运行时实现加速，并且观察到在受内存限制的模型中，稀疏性也可用于减少内存带宽。 |
| [^50] | [Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE.](http://arxiv.org/abs/2310.06918) | 本研究提出了一种聚焦-信息熵函数，利用硬负样本挖掘改进了对比学习句子嵌入的方法，并在各种基准测试中验证了其性能提升。 |
| [^51] | [A Comparative Study of Transformer-based Neural Text Representation Techniques on Bug Triaging.](http://arxiv.org/abs/2310.06913) | 本文研究了基于Transformer的神经文本表示技术在缺陷分配中的应用，相比之前的方法，这些新技术能更好地捕捉微妙的文本模式，提高自动化缺陷分配的性能。 |
| [^52] | [Exploiting Language Models as a Source of Knowledge for Cognitive Agents.](http://arxiv.org/abs/2310.06846) | 本研究利用语言模型作为认知智能体的任务知识来源，探索了将语言模型作为外部知识源用于认知系统的挑战和机会，并提出了通过整合知识提取和认知架构能力来提高知识提取效果的可能方法。 |
| [^53] | [Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models.](http://arxiv.org/abs/2310.06692) | Meta-CoT是一种在混合任务场景中能够通用思维链提示的方法，在十个公共基准推理任务中表现出卓越的性能和优越的泛化能力。 |
| [^54] | [Constructive Large Language Models Alignment with Diverse Feedback.](http://arxiv.org/abs/2310.06450) | 本文提出了一种新的方法，即建构性和多样化反馈（CDF），用于增强大型语言模型（LLM）的对齐效果。我们通过收集不同类型的反馈，并根据问题的难度级别进行处理，实现了更好的性能。 |
| [^55] | [The Importance of Prompt Tuning for Automated Neuron Explanations.](http://arxiv.org/abs/2310.06200) | 本文探讨了在自动神经元解释中即时调优的重要性，通过重新格式化解释提示，我们显著提高了解释质量并降低了计算成本。这项研究对于更深入理解大型语言模型的工作原理和安全性具有重要意义。 |
| [^56] | [Are Large Language Models Post Hoc Explainers?.](http://arxiv.org/abs/2310.05797) | 这项工作提出了第一个研究大型语言模型（LLMs）解释其他预测模型有效性的框架，并且提出了多个提示策略，填补了当前对于LLMs在解释其他模型行为方面的缺失。 |
| [^57] | [Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems.](http://arxiv.org/abs/2310.05280) | 这项研究评估了对话系统中的人格偏见对社交偏见的影响，并建立了一个综合评估框架来衡量不同人格采用下的偏见程度。 |
| [^58] | [BRAINTEASER: Lateral Thinking Puzzles for Large Language Models.](http://arxiv.org/abs/2310.05057) | 本文介绍了一项名为BRAINTEASER的多项选择问题回答任务，旨在测试大型语言模型表现出横向思维和违反默认常识联系的能力。通过创建一个横向思维基准，丰富问题的语义和上下文重建，实验证明模型与人类之间存在显著差距。 |
| [^59] | [Revisiting Large Language Models as Zero-shot Relation Extractors.](http://arxiv.org/abs/2310.05028) | 本研究重新审视了大型语言模型(LLMs)作为零-shot关系抽取器的潜力，并提出了通过总结和提问(\textsc{SumAsk})提示方法来改进零-shot关系抽取。实验证明LLMs在这一任务上具有良好的表现。 |
| [^60] | [Hermes: Unlocking Security Analysis of Cellular Network Protocols by Synthesizing Finite State Machines from Natural Language Specifications.](http://arxiv.org/abs/2310.04381) | Hermes是一个自动生成有限状态机的端到端框架，用于解锁移动网络协议的安全分析。通过处理自然语言规范并生成逻辑公式，Hermes能够发现新的漏洞和攻击，并对现有规范和商业基带进行评估。 |
| [^61] | [The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision).](http://arxiv.org/abs/2309.17421) | 本文对最新的GPT-4V(ision)模型进行了分析，发现其具有处理多模态输入、高通用性和理解带有视觉标记的能力，使其成为强大的多模态通用系统。 |
| [^62] | [InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4.](http://arxiv.org/abs/2308.12067) | InstructionGPT-4通过仅使用200个例子进行微调，在多模式指令数据质量度量和选择器的帮助下，在各种评估任务中优于原始的MiniGPT-4。 |
| [^63] | [CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools.](http://arxiv.org/abs/2307.15770) | 本论文介绍了一种名为ChatReport的基于LLM的系统，它通过实现可追溯的答案和解决领域专家参与低效性的问题，旨在通过自动分析企业可持续性报告，实现可持续性披露分析民主化。 |
| [^64] | [Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models.](http://arxiv.org/abs/2307.14539) | 本文通过引入对抗性嵌入空间攻击，探讨了将现成组件插入多模型模型中的漏洞和风险，并提出了一种不需要多模型系统权重和参数的攻击方法，通过寻找位于预训练组件嵌入空间危险区域的输入图像进行攻击。 |
| [^65] | [VerifAI: Verified Generative AI.](http://arxiv.org/abs/2307.02796) | 验证生成式人工智能的输出是一个新兴问题，我们提出了通过分析多模态数据湖的底层数据，评估其质量和一致性，来建立评估生成式人工智能模型输出的更坚实基础，并解决错误信息传播的挑战。 |
| [^66] | [CamChoice: A Corpus of Multiple Choice Questions and Candidate Response Distributions.](http://arxiv.org/abs/2306.13047) | 本文介绍了CamChoice数据集，该数据集包含多项选择理解问题和真实候选答案选项分布，为候选人分布匹配任务提供了自动评估方式。 |
| [^67] | [VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution.](http://arxiv.org/abs/2306.12424) | VisoGender是一个用于评估视觉语言模型中职业相关性别偏见的数据集。研究显示，最先进的模型缺乏正确解析复杂场景中性别的推理能力，生成字幕的模型通常比类似CLIP的模型更精确和更少偏见。 |
| [^68] | [Explore, Establish, Exploit: Red Teaming Language Models from Scratch.](http://arxiv.org/abs/2306.09442) | 本文提出了一种新的红队行动，通过从高层次、抽象的规范出发来考虑语言模型的行为，以探究模型的创新和贡献。 |
| [^69] | [Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model.](http://arxiv.org/abs/2305.16340) | 本文提出了一种分段循环Transformer（SRformer）来减少计算/内存成本，并使用RAF层处理跨段的信息，从而提高序列处理能力。 |
| [^70] | [Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models.](http://arxiv.org/abs/2305.15074) | 这项研究提出了JEEBench，一个更具挑战性的基准数据集，用于评估大型语言模型的问题解决能力。通过评估各种模型，结果显示目前最好的模型在解决问题时存在代数操作错误、抽象概念转化不准确和难以检索相关概念等问题。 |
| [^71] | [UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning.](http://arxiv.org/abs/2305.14761) | 本文提出了UniChart，这是一个用于图表理解和推理的通用视觉语言预训练模型。UniChart首先建立了一个包括各种不同主题和视觉风格的大量图表语料库，然后使用图表特定的预训练任务来编码图表，并在几个图表理解和推理任务上表现出色。 |
| [^72] | [Improving Language Models with Advantage-based Offline Policy Gradients.](http://arxiv.org/abs/2305.14718) | 本文介绍了一种简单的训练算法Left-over Lunch RL （LoL-RL），使用离线策略梯度学习任何序列到序列数据，从而实现优化LM效用的方法。 |
| [^73] | [FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation.](http://arxiv.org/abs/2305.14251) | 本文介绍了一种称为FACTSCORE的评估方法，它通过将生成的内容分解为原子事实，并计算被可靠知识源支持的比例来评估大型语言模型生成的长文本的事实准确性。通过广泛的人工评估，我们发现商业语言模型中仅有58%的ChatGPT传记达到了高水平的事实准确性。此外，我们还引入了一种自动化模型，利用检索和强语言模型估计FACTSCORE，误差率低于2%。 |
| [^74] | [Editing Large Language Models: Problems, Methods, and Opportunities.](http://arxiv.org/abs/2305.13172) | 本文深入探讨了编辑大型语言模型的问题、方法和机会，提供了任务定义和挑战的概述、先进方法的实证分析，以及构建了新的基准数据集。这些结果有助于改进LLMs的编辑技术，提高其效果和可行性。 |
| [^75] | [Equivariant Few-Shot Learning from Pretrained Models.](http://arxiv.org/abs/2305.09900) | 本文提出了一种基于预训练模型的$\lambda$-\textit{equitune}方法，它使用\textit{重要性权重}$\lambda$对特征进行平均，可以显著提高等变小样本学习的表现。 |
| [^76] | [Knowledge Rumination for Pre-trained Language Models.](http://arxiv.org/abs/2305.08732) | 本文提出了一种名为知识反思的新范式，旨在帮助预训练语言模型利用已经编码在其预训练参数中的相关潜在知识，而不需要从外部语料库中检索。这种方法通过在模型中添加提示，并将相关知识注入模型进行整合，取得了在常识推理任务和GLUE基准上的实验结果。 |
| [^77] | [Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks.](http://arxiv.org/abs/2305.05862) | 本研究探讨了ChatGPT和GPT-4在金融文本分析任务中的潜力，结果显示它们在数值推理上表现出色但在需要领域特定知识的任务上表现不佳。 |
| [^78] | [TidyBot: Personalized Robot Assistance with Large Language Models.](http://arxiv.org/abs/2305.05658) | 本文研究了使用机器人进行家庭清扫的个性化问题。通过使用大型语言模型少样本摘要能力，机器人可以学习用户的偏好并将其推广到未来的场景中，从而实现快速适应。 |
| [^79] | [An Empirical Study of Multimodal Model Merging.](http://arxiv.org/abs/2304.14933) | 本研究通过融合在不同模态上训练的transformer进行多模态模型融合，并提出一种参数有效的模态不可知架构，形成有效的训练配方。 |
| [^80] | [Fundamental Limitations of Alignment in Large Language Models.](http://arxiv.org/abs/2304.11082) | 本文通过提出一种理论方法——行为期望边界（BEB），展示了大型语言模型中对齐的基本限制，并证明任何对齐过程都无法根除不希望的行为，这对于防止恶意攻击是不安全的。 |
| [^81] | [On the Evaluations of ChatGPT and Emotion-enhanced Prompting for Mental Health Analysis.](http://arxiv.org/abs/2304.03347) | 本文全面评估了ChatGPT在心理健康分析和情感推理方面的表现，以及不同提示策略和情感信息对其性能的影响。结果显示，ChatGPT在心理健康分析方面表现良好，加入情感增强提示对某些任务效果显著。 |
| [^82] | [SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models.](http://arxiv.org/abs/2303.08896) | SelfCheckGPT是一种简单的基于采样的方法，可以以零资源的方式检查黑盒模型的幻觉现象。 |
| [^83] | [UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation.](http://arxiv.org/abs/2303.08518) | UPRISE是一种通用的检索器，可自动为给定的零样本任务输入检索提示，从而提高大型语言模型的零样本评估。它通过跨任务和跨模型的实验展示了其通用性和潜力，同时表明具有减轻幻觉问题和提高LLM性能的能力。 |
| [^84] | [Query2doc: Query Expansion with Large Language Models.](http://arxiv.org/abs/2303.07678) | 本论文提出了一种名为query2doc的查询扩展方法，使用大型语言模型生成伪文档来改善稀疏和密集检索系统，取得了在多个数据集上提高 BM25 性能的结果。 |
| [^85] | [Active Learning for Multilingual Semantic Parser.](http://arxiv.org/abs/2301.12920) | 本研究提出了一种多语言语义解析的主动学习方法(AL-MSP), 选择一个现有数据集的子集进行翻译，通过优先选择多样化逻辑形式结构和更多词汇选择的示例，以及一种新的无需额外注释成本的超参数调整方法，有效减少了翻译成本，并取得了比其他基线更好的解析性能。 |
| [^86] | [Knowledgeable Salient Span Mask for Enhancing Language Models as Knowledge Base.](http://arxiv.org/abs/2204.07994) | 本文提出了一个名为知识显著跨度掩码的方法，通过全自监督学习帮助语言模型从非结构化文本中获取更多的知识。实验证明了该方法在知识密集型任务中的有效性。 |
| [^87] | [UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text.](http://arxiv.org/abs/2108.08614) | 本文提出了一个名为UNIQORN的问答系统，它能够无缝地处理RDF数据和文本，使用fine-tuned BERT模型为问题构建上下文图，并使用图算法确定与问题相关的子图来回答问题。 |

# 详细

[^1]: 大型语言模型中的事实性调查：知识、检索和领域专属性

    Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity. (arXiv:2310.07521v1 [cs.CL])

    [http://arxiv.org/abs/2310.07521](http://arxiv.org/abs/2310.07521)

    这项调查研究了大型语言模型（LLMs）中的事实性问题，包括其可能带来的后果和挑战，以及存储、处理和评估事实的方法。同时，提出了一些增强LLM事实性的策略，涵盖了不同领域的需求。

    

    本调查研究了大型语言模型（LLMs）中的关键问题，即事实性。由于LLMs在不同领域中都有应用，它们的输出的可靠性和准确性变得至关重要。我们将事实性问题定义为LLMs产生与已确立事实不一致的内容的概率。我们首先深入探讨了这些不准确性的含义，突出了事实错误在LLMs输出中可能带来的潜在后果和挑战。随后，我们分析了LLMs存储和处理事实的机制，寻找事实错误的主要原因。我们的讨论随后转向评估LLM事实性的方法论，强调关键指标、基准和研究。我们进一步探讨了增强LLM事实性的策略，包括针对特定领域的方法。我们重点关注两种主要的LLM配置，独立LLMs和利用外部数据的检索增强LLMs，详细介绍它们的独特挑战和潜在解决方法。

    This survey addresses the crucial issue of factuality in Large Language Models (LLMs). As LLMs find applications across diverse domains, the reliability and accuracy of their outputs become vital. We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts. We first delve into the implications of these inaccuracies, highlighting the potential consequences and challenges posed by factual errors in LLM outputs. Subsequently, we analyze the mechanisms through which LLMs store and process facts, seeking the primary causes of factual errors. Our discussion then transitions to methodologies for evaluating LLM factuality, emphasizing key metrics, benchmarks, and studies. We further explore strategies for enhancing LLM factuality, including approaches tailored for specific domains. We focus two primary LLM configurations standalone LLMs and Retrieval-Augmented LLMs that utilizes external data, we detail their unique challenges and potential 
    
[^2]: KwaiYiiMath: 技术报告

    KwaiYiiMath: Technical Report. (arXiv:2310.07488v1 [cs.CL])

    [http://arxiv.org/abs/2310.07488](http://arxiv.org/abs/2310.07488)

    KwaiYiiMath是一个用于增强数学推理能力的大型语言模型，通过应用监督微调和人类反馈强化学习，在英语和中文数学任务上取得了最先进的性能，并且能够正确解决生成的问题过程。

    

    近年来，大型语言模型（LLMs）在处理各种自然语言处理（NLP）下游任务方面展示出了显著的能力，甚至可以处理需要多步推理的数学任务。在本报告中，我们介绍了KwaiYiiMath，通过应用监督微调（SFT）和人类反馈强化学习（RLHF），增强了KwaiYiiBase1的数学推理能力，包括英语和中文的数学任务。同时，我们还构建了一个小规模的中小学数学测试集（命名为KMath），包含188个例子，用来评估模型生成的问题解决过程的正确性。实证研究表明，与类似规模的模型相比，KwaiYiiMath在GSM8k、CMath和KMath上均能取得最先进的性能（SOTA）。

    Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the KwaiYiiMath which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively.
    
[^3]: Cognate Transformer用于自动语音重建和同源反射预测

    Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction. (arXiv:2310.07487v1 [cs.CL])

    [http://arxiv.org/abs/2310.07487](http://arxiv.org/abs/2310.07487)

    Cognate Transformer是一个用于自动语音重建和同源反射预测的模型，基于多序列对齐数据进行训练，并在计算历史语言学领域取得了良好的表现。

    

    语音重建是历史语言学中的一个核心问题，通过观察子语言的同源词汇，确定祖先语言的原始词汇。计算历史语言学的方法试图通过学习现有语言数据上的模型来自动化这个任务。我们将蛋白质语言模型MSA Transformer应用于自动语音重建问题，并将其命名为Cognate Transformer。我们还将该模型应用于相关的任务，即同源反射预测，根据其他子语言的同源词汇来预测子语言中的反射词汇。我们展示了我们的模型的优越性能。

    Phonological reconstruction is one of the central problems in historical linguistics where a proto-word of an ancestral language is determined from the observed cognate words of daughter languages. Computational approaches to historical linguistics attempt to automate the task by learning models on available linguistic data. Several ideas and techniques drawn from computational biology have been successfully applied in the area of computational historical linguistics. Following these lines, we adapt MSA Transformer, a protein language model, to the problem of automated phonological reconstruction. MSA Transformer trains on multiple sequence alignments as input and is, thus, apt for application on aligned cognate words. We, hence, name our model as Cognate Transformer. We also apply the model on another associated task, namely, cognate reflex prediction, where a reflex word in a daughter language is predicted based on cognate words from other daughter languages. We show that our model o
    
[^4]: 为多语言自动语音识别中的代码切换调整适配器

    Adapting the adapters for code-switching in multilingual ASR. (arXiv:2310.07423v1 [cs.CL])

    [http://arxiv.org/abs/2310.07423](http://arxiv.org/abs/2310.07423)

    本论文研究了如何在多语言自动语音识别中有效地调整适配器来处理代码切换语音，提出了将两个语言适配器的信息融合并将代码切换建模为潜在二进制序列的方法。

    

    最近，大规模预训练的多语言语音模型展示了在许多资源匮乏语言中扩展自动语音识别（ASR）的潜力。其中一些模型在其组成中使用语言适配器，这有助于提高单语性能，并避免多语言建模在资源丰富的语言中的一些缺点。然而，这种组成限制了这些模型在代码切换语音上的可用性，即在同一个话语中混合了两种语言。在这项工作中，我们提出了一种有效地在代码切换语音上微调这些模型的方法，通过在网络中的每个语言适配点同化来自两个语言适配器的信息。我们还将代码切换建模为一系列可以用于指导每个帧级别的语言适配器信息流的潜在二进制序列。我们对包括阿拉伯语、普通话和印地语在内的三个代码切换数据集进行了评估。

    Recently, large pre-trained multilingual speech models have shown potential in scaling Automatic Speech Recognition (ASR) to many low-resource languages. Some of these models employ language adapters in their formulation, which helps to improve monolingual performance and avoids some of the drawbacks of multi-lingual modeling on resource-rich languages. However, this formulation restricts the usability of these models on code-switched speech, where two languages are mixed together in the same utterance. In this work, we propose ways to effectively fine-tune such models on code-switched speech, by assimilating information from both language adapters at each language adaptation point in the network. We also model code-switching as a sequence of latent binary sequences that can be used to guide the flow of information from each language adapter at the frame level. The proposed approaches are evaluated on three code-switched datasets encompassing Arabic, Mandarin, and Hindi languages paire
    
[^5]: DASpeech：用于快速高质量语音翻译的有向无环变换器

    DASpeech: Directed Acyclic Transformer for Fast and High-quality Speech-to-Speech Translation. (arXiv:2310.07403v1 [cs.CL])

    [http://arxiv.org/abs/2310.07403](http://arxiv.org/abs/2310.07403)

    DASpeech是一个非自回归的直接语音翻译模型，使用有向无环图（DAG）来实现快速和高质量的语音翻译。

    

    直接语音翻译（S2ST）使用单个模型将一种语言的语音翻译成另一种语言。然而，由于存在语言和声学多样性，目标语音遵循一个复杂的多模态分布，给S2ST模型实现高质量翻译和快速解码速度带来挑战。在本文中，我们提出了DASpeech，这是一个非自回归的直接S2ST模型，实现了快速和高质量的S2ST。为了更好地捕捉目标语音的复杂分布，DASpeech采用了两步解码的架构，先由语言解码器生成目标文本，然后由声学解码器根据语言解码器的隐藏状态生成目标语音。具体而言，我们将DA-Transformer的解码器作为语言解码器，将FastSpeech 2作为声学解码器。DA-Transformer使用有向无环图（DAG）模拟翻译过程。

    Direct speech-to-speech translation (S2ST) translates speech from one language into another using a single model. However, due to the presence of linguistic and acoustic diversity, the target speech follows a complex multimodal distribution, posing challenges to achieving both high-quality translations and fast decoding speeds for S2ST models. In this paper, we propose DASpeech, a non-autoregressive direct S2ST model which realizes both fast and high-quality S2ST. To better capture the complex distribution of the target speech, DASpeech adopts the two-pass architecture to decompose the generation process into two steps, where a linguistic decoder first generates the target text, and an acoustic decoder then generates the target speech based on the hidden states of the linguistic decoder. Specifically, we use the decoder of DA-Transformer as the linguistic decoder, and use FastSpeech 2 as the acoustic decoder. DA-Transformer models translations with a directed acyclic graph (DAG). To co
    
[^6]: 面向目标的个性化主动对话系统：问题形式化与数据集筛选

    Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation. (arXiv:2310.07397v1 [cs.CL])

    [http://arxiv.org/abs/2310.07397](http://arxiv.org/abs/2310.07397)

    这项工作提出了一个自动数据集筛选框架，并构建了一个大规模的个性化面向目标对话数据集，名为TopDial。该数据集质量高，有助于探索个性化目标导向的对话。

    

    面向目标的对话系统旨在主动引导对话朝向预定的目标或达成特定的系统目标，在对话完成过程中考虑个性化。然而，需要高质量的数据集，从零开始构建需要大量人力。为了解决这个问题，我们提出了一个使用角色扮演方法的自动数据集筛选框架。基于这个框架，我们构建了一个大规模的个性化面向目标对话数据集，名为TopDial，包含约18K个多轮对话。实验结果表明，该数据集具有很高的质量，可以用于探索个性化目标导向的对话。

    Target-oriented dialogue systems, designed to proactively steer conversations toward predefined targets or accomplish specific system-side goals, are an exciting area in conversational AI. In this work, by formulating a <dialogue act, topic> pair as the conversation target, we explore a novel problem of personalized target-oriented dialogue by considering personalization during the target accomplishment process. However, there remains an emergent need for high-quality datasets, and building one from scratch requires tremendous human effort. To address this, we propose an automatic dataset curation framework using a role-playing approach. Based on this framework, we construct a large-scale personalized target-oriented dialogue dataset, TopDial, which comprises about 18K multi-turn dialogues. The experimental results show that this dataset is of high quality and could contribute to exploring personalized target-oriented dialogue.
    
[^7]: 生物学中的语言学定律

    Linguistic laws in biology. (arXiv:2310.07387v1 [cs.CL])

    [http://arxiv.org/abs/2310.07387](http://arxiv.org/abs/2310.07387)

    这项研究发现了生物学中与语言学定律一致的统计模式，并提出了一个新的概念框架，将不同层次的分析整合起来，揭示自然系统的基本组织规律。

    

    近一个世纪以来，定量语言学家一直在研究人类语言的常见统计模式，称为语言学定律。最近，来自不同学科的生物学家开始探索这些定律在语言之外的普遍性，发现与语言学定律一致的模式存在于生物组织的多个层次，从分子（基因组、基因和蛋白质）到个体（动物行为）到生态（种群和生态系统）。我们提出了一个新的概念框架，用于研究生物学中的语言学定律，包括和整合不同的分析层次，从描述到预测到理论建立。采用这个框架将为揭示自然系统基本组织规律提供关键的新见解，将语言学定律和生物学的核心理论统一起来。

    Linguistic laws, the common statistical patterns of human language, have been investigated by quantitative linguists for nearly a century. Recently, biologists from a range of disciplines have started to explore the prevalence of these laws beyond language, finding patterns consistent with linguistic laws across multiple levels of biological organisation, from molecular (genomes, genes, and proteins) to organismal (animal behaviour) to ecological (populations and ecosystems). We propose a new conceptual framework for the study of linguistic laws in biology, comprising and integrating distinct levels of analysis, from description to prediction to theory building. Adopting this framework will provide critical new insights into the fundamental rules of organisation underpinning natural systems, unifying linguistic laws and core theory in biology.
    
[^8]: 高效预训练的快速ELECTRA

    Fast-ELECTRA for Efficient Pre-training. (arXiv:2310.07347v1 [cs.CL])

    [http://arxiv.org/abs/2310.07347](http://arxiv.org/abs/2310.07347)

    提出了一种快速ELECTRA的方法，利用现有的语言模型作为辅助模型，通过温度缩放平滑主模型的输出分布，达到与最先进的ELECTRA预训练方法相媲美的性能，并显著减少了辅助模型共同训练带来的计算和内存成本。

    

    ELECTRA通过检测序列中被辅助模型替换的标记来预训练语言模型。虽然ELECTRA大大提高了效率，但其潜力受到了辅助模型带来的训练成本的限制。值得注意的是，这个与主模型共同训练的模型仅用于辅助主模型的训练，并且在训练后被丢弃。这导致大量的训练成本被白白浪费。为了缓解这个问题，我们提出了Fast-ELECTRA，它利用现有的语言模型作为辅助模型。为了构建主模型的学习课程，我们通过温度缩放和递减的方法平滑其输出分布。我们的方法与最先进的ELECTRA风格的预训练方法相媲美，同时显著减少了辅助模型共同训练带来的计算和内存成本。我们的方法还降低了模型对训练数据的敏感性。

    ELECTRA pre-trains language models by detecting tokens in a sequence that have been replaced by an auxiliary model. Although ELECTRA offers a significant boost in efficiency, its potential is constrained by the training cost brought by the auxiliary model. Notably, this model, which is jointly trained with the main model, only serves to assist the training of the main model and is discarded post-training. This results in a substantial amount of training cost being expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which leverages an existing language model as the auxiliary model. To construct a learning curriculum for the main model, we smooth its output distribution via temperature scaling following a descending schedule. Our approach rivals the performance of state-of-the-art ELECTRA-style pre-training methods, while significantly eliminating the computation and memory cost brought by the joint training of the auxiliary model. Our method also reduces the sensitivity t
    
[^9]: 调查语言模型在神经变换器序列辨别训练中的效果

    Investigating the Effect of Language Models in Sequence Discriminative Training for Neural Transducers. (arXiv:2310.07345v1 [cs.CL])

    [http://arxiv.org/abs/2310.07345](http://arxiv.org/abs/2310.07345)

    本研究探讨了在基于音素的神经变换器序列辨别训练中使用不同上下文长度和标签单元的语言模型的效果。研究发现，在训练中使用单词级别的语言模型优于音素级别的模型，并且概率计算中的上下文大小对性能影响有限。研究结果还揭示了假设空间质量在序列辨别训练中的重要性。

    

    在这项工作中，我们研究了语言模型（LMs）在基于音素的神经变换器序列辨别训练中的不同上下文长度和标签单元（音素 vs. 单词）对性能的影响。我们同时考察了无网格和N最佳列表方法。对于使用音素级别LMs的无网格方法，我们提出了一种方法来近似上下文历史以利用完全上下文依赖的LMs。这种近似可以扩展到任意上下文长度，并允许在无网格方法中使用单词级别LMs。此外，我们对无网格和N最佳列表方法进行了系统比较。在Librispeech上的实验结果表明，在训练中使用单词级别LM的效果优于音素级别LM。此外，我们发现用于概率计算的LM的上下文大小对性能影响有限。此外，我们的结果揭示了序列辨别训练中假设空间质量的关键重要性。

    In this work, we investigate the effect of language models (LMs) with different context lengths and label units (phoneme vs. word) used in sequence discriminative training for phoneme-based neural transducers. Both lattice-free and N-best-list approaches are examined. For lattice-free methods with phoneme-level LMs, we propose a method to approximate the context history to employ LMs with full-context dependency. This approximation can be extended to arbitrary context length and enables the usage of word-level LMs in lattice-free methods. Moreover, a systematic comparison is conducted across lattice-free and N-best-list-based methods. Experimental results on Librispeech show that using the word-level LM in training outperforms the phoneme-level LM. Besides, we find that the context size of the LM used for probability computation has a limited effect on performance. Moreover, our results reveal the pivotal importance of the hypothesis space quality in sequence discriminative training.
    
[^10]: 大型语言模型如何捕捉不断变化的世界知识？对近期进展的综述。

    How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances. (arXiv:2310.07343v1 [cs.CL])

    [http://arxiv.org/abs/2310.07343](http://arxiv.org/abs/2310.07343)

    本文综述了如何在不重新训练的情况下使大型语言模型与不断变化的世界知识对齐的最新进展，并讨论了相关挑战和未来方向。

    

    尽管大型语言模型（LLMs）在解决各种任务方面令人印象深刻，但它们在部署后很快就会过时。如何保持其最新状态是当前时代的一个紧迫问题。本文全面评述了将LLMs与不断变化的世界知识对齐而无需重新训练的最新进展。我们系统地对研究工作进行分类，并提供深入的比较和讨论。我们还讨论了现有的挑战，并重点介绍了未来的研究方向，以促进这一领域的研究。我们将论文列表发布在https://github.com/hyintell/awesome-refreshing-llms上。

    Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era. This paper provides a comprehensive review of recent advances in aligning LLMs with the ever-changing world knowledge without re-training from scratch. We categorize research works systemically and provide in-depth comparisons and discussion. We also discuss existing challenges and highlight future directions to facilitate research in this field. We release the paper list at https://github.com/hyintell/awesome-refreshing-llms
    
[^11]: 对中文大语言模型的指导调整的实证研究

    An Empirical Study of Instruction-tuning Large Language Models in Chinese. (arXiv:2310.07328v1 [cs.CL])

    [http://arxiv.org/abs/2310.07328](http://arxiv.org/abs/2310.07328)

    本论文进行了对中文大语言模型的指导调整的实证研究，探索了LLM基础、参数高效的方法和指令数据类型的影响，并研究了其他因素的影响，为定制能更好响应中文指令的LLM提供了有价值的发现。

    

    ChatGPT的成功验证了大语言模型（LLM）在人工通用智能（AGI）中的潜力。随后，LLM的发布引起了开源社区对指导调整的兴趣，这被认为可以加快ChatGPT的复制过程。然而，关于中文的LLM指导调整的研究仍处于早期阶段。因此，本文对中文LLM的指导调整进行了深入的实证研究，这可以作为一本提供有价值发现的烹饪书来有效地定制LLM，使其能够更好地响应中文指令。具体而言，我们系统地探索了LLM基础、参数高效的方法和指令数据类型这三个对指导调整至关重要的因素的影响。此外，我们还进行了实验来研究其他因素的影响，例如思维链数据和人类价值一致性。我们希望这个实证研究能够

    The success of ChatGPT validates the potential of large language models (LLMs) in artificial general intelligence (AGI). Subsequently, the release of LLMs has sparked the open-source community's interest in instruction-tuning, which is deemed to accelerate ChatGPT's replication process. However, research on instruction-tuning LLMs in Chinese, the world's most spoken language, is still in its early stages. Therefore, this paper makes an in-depth empirical study of instruction-tuning LLMs in Chinese, which can serve as a cookbook that provides valuable findings for effectively customizing LLMs that can better respond to Chinese instructions. Specifically, we systematically explore the impact of LLM bases, parameter-efficient methods, instruction data types, which are the three most important elements for instruction-tuning. Besides, we also conduct experiment to study the impact of other factors, e.g., chain-of-thought data and human-value alignment. We hope that this empirical study can
    
[^12]: 关于交叉领域数据对德语语言模型的影响

    On the Impact of Cross-Domain Data on German Language Models. (arXiv:2310.07321v1 [cs.CL])

    [http://arxiv.org/abs/2310.07321](http://arxiv.org/abs/2310.07321)

    本研究通过对德语语言模型进行实验，发现将数据多样性置于数据质量之上的交叉领域数据集训练方法，可以显著提高模型的性能，并超过了之前的最先进模型。

    

    传统上，大型语言模型要么在通用网络抓取数据上训练，要么在特定领域的数据上。然而，生成型大型语言模型的最近成功突显了交叉领域数据集的好处。为了考察数据多样性高于质量的重要性，我们提出了一个包含五个领域文本的德语数据集，以及一个旨在包含高质量数据的数据集。通过在这两个数据集上训练参数范围从122M到750M的一系列模型，我们对多个下游任务进行了全面评估。我们的研究结果表明，使用交叉领域数据集训练的模型优于仅使用质量数据训练的模型，在先前最先进结果上提出了高达4.45%的改进。这些模型可在https://huggingface.co/ikim-uk-essen上找到。

    Traditionally, large language models have been either trained on general web crawls or domain-specific data. However, recent successes of generative large language models, have shed light on the benefits of cross-domain datasets. To examine the significance of prioritizing data diversity over quality, we present a German dataset comprising texts from five domains, along with another dataset aimed at containing high-quality data. Through training a series of models ranging between 122M and 750M parameters on both datasets, we conduct a comprehensive benchmark on multiple downstream tasks. Our findings demonstrate that the models trained on the cross-domain dataset outperform those trained on quality data alone, leading to improvements up to $4.45\%$ over the previous state-of-the-art. The models are available at https://huggingface.co/ikim-uk-essen
    
[^13]: SNOiC: 基于软标签和噪声混合的开放意图分类模型

    SNOiC: Soft Labeling and Noisy Mixup based Open Intent Classification Model. (arXiv:2310.07306v1 [cs.LG])

    [http://arxiv.org/abs/2310.07306](http://arxiv.org/abs/2310.07306)

    SNOiC模型是一种基于软标签和噪声混合的开放意图分类模型，通过减少偏差和生成伪数据来提高识别开放意图的性能。

    

    本文提出了一种基于软标签和噪声混合的开放意图分类模型（SNOiC）。先前的工作大多使用基于阈值的方法来识别开放意图，容易出现过拟合问题，并可能产生有偏的预测结果。此外，对于开放意图类别需要更多可用数据的需求也是现有模型的另一个限制。SNOiC将软标签和噪声混合策略结合起来，以减少偏差并为开放意图类别生成伪数据。在四个基准数据集上的实验结果显示，SNOiC模型在识别开放意图方面的最低和最高性能分别为68.72％和94.71％。此外，与最先进的模型相比，SNOiC模型在识别开放意图方面的性能提升了0.93％（最低）和12.76％（最高）。通过分析所提出模型中使用的各种参数进一步证明了模型的有效性。还进行了消融研究，其中

    This paper presents a Soft Labeling and Noisy Mixup-based open intent classification model (SNOiC). Most of the previous works have used threshold-based methods to identify open intents, which are prone to overfitting and may produce biased predictions. Additionally, the need for more available data for an open intent class presents another limitation for these existing models. SNOiC combines Soft Labeling and Noisy Mixup strategies to reduce the biasing and generate pseudo-data for open intent class. The experimental results on four benchmark datasets show that the SNOiC model achieves a minimum and maximum performance of 68.72\% and 94.71\%, respectively, in identifying open intents. Moreover, compared to state-of-the-art models, the SNOiC model improves the performance of identifying open intents by 0.93\% (minimum) and 12.76\% (maximum). The model's efficacy is further established by analyzing various parameters used in the proposed model. An ablation study is also conducted, which
    
[^14]: Parrot:通过学习提问来增强多轮聊天模型

    Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions. (arXiv:2310.07301v1 [cs.CL])

    [http://arxiv.org/abs/2310.07301](http://arxiv.org/abs/2310.07301)

    本文提出了Parrot，一个高度可扩展的解决方案，通过自动生成高质量的指令调优数据，进一步完善了多轮聊天模型在对话中的效果。

    

    最近，基于大型语言模型的聊天模型取得了令人印象深刻的进展；然而，在开源聊天模型（如Alpaca和Vicuna）与领先的聊天模型（如ChatGPT和GPT-4）之间存在明显的多轮对话滞后。通过一系列的分析，我们将滞后归因于缺乏足够高质量的多轮指令调优数据。社区提供的调优数据要么是单轮会话，要么是存在某些问题的多轮会话，例如非人类的指令，响应不够详细，或者很少出现主题转换。本文通过引入Parrot，一个高度可扩展的解决方案，来解决这些挑战，该解决方案旨在自动生成高质量的指令调优数据，然后用于增强多轮聊天模型在对话中的效果。具体而言，我们首先训练Parrot-Ask模型，该模型旨在模拟真实用户生成指令。

    Impressive progress has been made on chat models based on Large Language Models (LLMs) recently; however, there is a noticeable lag in multi-turn conversations between open-source chat models (e.g., Alpaca and Vicuna) and the leading chat models (e.g., ChatGPT and GPT-4). Through a series of analyses, we attribute the lag to the lack of enough high-quality multi-turn instruction-tuning data. The available instruction-tuning data for the community are either single-turn conversations or multi-turn ones with certain issues, such as non-human-like instructions, less detailed responses, or rare topic shifts. In this paper, we address these challenges by introducing Parrot, a highly scalable solution designed to automatically generate high-quality instruction-tuning data, which are then used to enhance the effectiveness of chat models in multi-turn conversations. Specifically, we start by training the Parrot-Ask model, which is designed to emulate real users in generating instructions. We t
    
[^15]: RobustGEC: 鲁棒的语法错误修正系统抵抗微小的上下文扰动

    RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation. (arXiv:2310.07299v1 [cs.CL])

    [http://arxiv.org/abs/2310.07299](http://arxiv.org/abs/2310.07299)

    这项研究提出了一个叫做RobustGEC的基准测试，用于评估语法错误修正系统的上下文鲁棒性。研究发现目前最先进的系统仍然无法很好地应对上下文扰动，并提出了一种简单而有效的解决方法。

    

    语法错误修正（GEC）系统在帮助人们日常写作任务中起到了至关重要的作用。然而，用户有时可能会遇到一些初始表现良好但遇到输入微小修改时无法修正错误的GEC系统。为了确保理想的用户体验，可靠的GEC系统应能够在遇到无关上下文扰动时提供一致且准确的建议，我们称之为上下文鲁棒性。在本文中，我们引入了RobustGEC，这是一个用于评估GEC系统上下文鲁棒性的基准测试。RobustGEC包含5,000个GEC案例，每个案例由一句原始的错误-修正句子对和由人工标注者精心设计的五个变体组成。利用RobustGEC，我们揭示了目前最先进的GEC系统在面对上下文扰动时仍然缺乏足够的鲁棒性。此外，我们提出了一种简单而有效的方法来解决这个问题。

    Grammatical Error Correction (GEC) systems play a vital role in assisting people with their daily writing tasks. However, users may sometimes come across a GEC system that initially performs well but fails to correct errors when the inputs are slightly modified. To ensure an ideal user experience, a reliable GEC system should have the ability to provide consistent and accurate suggestions when encountering irrelevant context perturbations, which we refer to as context robustness. In this paper, we introduce RobustGEC, a benchmark designed to evaluate the context robustness of GEC systems. RobustGEC comprises 5,000 GEC cases, each with one original error-correct sentence pair and five variants carefully devised by human annotators. Utilizing RobustGEC, we reveal that state-of-the-art GEC systems still lack sufficient robustness against context perturbations. In addition, we propose a simple yet effective method for remitting this issue.
    
[^16]: 超越事实性：对大型语言模型作为知识生成器的全面评估

    Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators. (arXiv:2310.07289v1 [cs.CL])

    [http://arxiv.org/abs/2310.07289](http://arxiv.org/abs/2310.07289)

    该论文对大型语言模型（LLMs）作为知识生成器的能力进行了全面评估，介绍了一个评估框架CONNER。研究发现，生成的知识的事实性不如准确性重要，而生成的相关性和连贯性更为重要。

    

    在提示生成世界知识时，大型语言模型（LLMs）优于信息检索技术，用于知识密集型任务。然而，社区对使用这种未经审查的知识的事实性和潜在影响的担忧不绝于耳。鉴于此，我们介绍了CONNER，一种综合的知识评估框架，旨在系统地和自动地从六个重要的角度评估生成的知识--事实性、相关性、连贯性、信息性、有用性和有效性。我们对来自三种不同类型的LLMs的生成知识进行了广泛的实证分析，这三种LLMs用于两个广泛研究的知识密集型任务，即开放域问答和基于知识的对话。令人惊讶的是，我们的研究发现，即使生成的知识的事实性较低，也不会显著阻碍下游任务。相反，产出的相关性和连贯性比准确度更重要。

    Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives -- Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than sm
    
[^17]: 打字倾听鸡尾酒会：文本引导的目标说话人提取

    Typing to Listen at the Cocktail Party: Text-Guided Target Speaker Extraction. (arXiv:2310.07284v1 [eess.AS])

    [http://arxiv.org/abs/2310.07284](http://arxiv.org/abs/2310.07284)

    研究人员提出了一种名为LLM-TSE的模型，该模型利用大型语言模型从用户键入的文本输入中提取语义线索，以增强目标说话人提取(TSE)模型的灵活性和可控性。

    

    人类拥有一种在复杂的声学环境中有选择性地专注于感兴趣的声音源的非凡能力，通常称为鸡尾酒会场景。为了在机器中复制这种引人注目的听觉注意能力，研究人员开发了目标说话人提取(TSE)模型。这些模型利用目标说话人的预先注册线索来提取感兴趣的声源。然而，在真实世界的情景中，这些模型的有效性受到了预先注册线索的可能变化甚至缺失的限制。为了解决这个限制，本研究调查了将自然语言整合到现有TSE模型中以增强其灵活性和可控性的方法。具体而言，我们提出了一个名为LLM-TSE的模型，其中使用大型语言模型(LLM)从用户的键入文本输入中提取有用的语义线索，这些线索可以补充预先注册的线索或独立工作以控制TSE过程。

    Humans possess an extraordinary ability to selectively focus on the sound source of interest amidst complex acoustic environments, commonly referred to as cocktail party scenarios. In an attempt to replicate this remarkable auditory attention capability in machines, target speaker extraction (TSE) models have been developed. These models leverage the pre-registered cues of the target speaker to extract the sound source of interest. However, the effectiveness of these models is hindered in real-world scenarios due to the potential variation or even absence of pre-registered cues. To address this limitation, this study investigates the integration of natural language to enhance the flexibility and controllability of existing TSE models. Specifically, we propose a model named LLM-TSE, wherein a large language model (LLM) to extract useful semantic cues from the user's typed text input, which can complement the pre-registered cues or work independently to control the TSE process. Our exper
    
[^18]: 在医疗保健领域中对大型语言模型的分析：BioBERT 案例研究

    An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT. (arXiv:2310.07282v1 [cs.AI])

    [http://arxiv.org/abs/2310.07282](http://arxiv.org/abs/2310.07282)

    本研究分析了在医疗保健领域应用大型语言模型（尤其是BioBERT）的可行性，并提出了针对医疗保健领域的微调方法。研究突出了BioBERT对于解决与生物医学文本挖掘相关任务的特定要求的适用性。

    

    本文对在医疗保健领域应用大型语言模型（尤其是BioBERT）进行了全面调查研究。它首先彻底检查了先前在医疗保健领域中应用自然语言处理（NLP）方法的情况，揭示了这些方法面临的局限和挑战。随后，本研究探讨了将BioBERT整合到医疗保健应用中的路径，突出其适用于解决与生物医学文本挖掘相关任务的特定要求。该分析概述了用于针对医疗保健领域独特需求微调BioBERT的系统方法论。这个方法包括从各种医疗保健来源收集数据，为识别医疗实体和对其进行分类等任务进行数据注释，并应用专门针对处理生物医学文本中复杂性的预处理技术。

    This paper conducts a comprehensive investigation into applying large language models, particularly on BioBERT, in healthcare. It begins with thoroughly examining previous natural language processing (NLP) approaches in healthcare, shedding light on the limitations and challenges these methods face. Following that, this research explores the path that led to the incorporation of BioBERT into healthcare applications, highlighting its suitability for addressing the specific requirements of tasks related to biomedical text mining. The analysis outlines a systematic methodology for fine-tuning BioBERT to meet the unique needs of the healthcare domain. This approach includes various components, including the gathering of data from a wide range of healthcare sources, data annotation for tasks like identifying medical entities and categorizing them, and the application of specialized preprocessing techniques tailored to handle the complexities found in biomedical texts. Additionally, the pape
    
[^19]: 提升无文本语音翻译系统中的表达能力转换

    Enhancing expressivity transfer in textless speech-to-speech translation. (arXiv:2310.07279v1 [cs.SD])

    [http://arxiv.org/abs/2310.07279](http://arxiv.org/abs/2310.07279)

    该研究提出了一种新方法来提高无文本语音翻译系统中的表达能力转换，通过运用多语言情感嵌入来捕捉语言无关的信息，并有效预测目标语言的音调和时长。实验结果表明，该方法相对于现有最先进系统在表达能力转换方面具有优势。

    

    在自我监督学习技术的整合下，无文本语音翻译系统正在快速发展。然而，现有的最先进系统在准确捕捉和转换不同语言的表达能力方面存在不足。表达能力在传达情感、细微差别和文化细微之处方面起着重要作用，从而增强不同语言之间的交流。为解决这个问题，本研究提出了一种新的方法，该方法在离散语音单元级别上运作，并利用多语言情感嵌入来捕捉语言无关的信息。具体而言，我们演示了如何利用这些嵌入来有效预测目标语言的音调和时长。通过对法语到英语翻译任务进行的客观和主观实验，我们的研究结果突出了我们的方法相对于现有最先进系统在表达能力转换方面的优势。

    Textless speech-to-speech translation systems are rapidly advancing, thanks to the integration of self-supervised learning techniques. However, existing state-of-the-art systems fall short when it comes to capturing and transferring expressivity accurately across different languages. Expressivity plays a vital role in conveying emotions, nuances, and cultural subtleties, thereby enhancing communication across diverse languages. To address this issue this study presents a novel method that operates at the discrete speech unit level and leverages multilingual emotion embeddings to capture language-agnostic information. Specifically, we demonstrate how these embeddings can be used to effectively predict the pitch and duration of speech units in the target language. Through objective and subjective experiments conducted on a French-to-English translation task, our findings highlight the superior expressivity transfer achieved by our approach compared to current state-of-the-art systems.
    
[^20]: BioT5：在生物学中利用化学知识和自然语言关联丰富跨模态整合

    BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v1 [cs.CL])

    [http://arxiv.org/abs/2310.07276](http://arxiv.org/abs/2310.07276)

    BioT5是一个全面的预训练框架，在生物学中利用化学知识和自然语言关联丰富了跨模态整合，通过鲁棒的分子表示和上下文知识提取，实现了更有效的信息利用，展现出卓越的性能。

    

    最近在生物研究领域的进展利用分子、蛋白质和自然语言的整合来增强药物发现。然而，当前的模型存在一些限制，如生成无效的分子SMILES、对上下文信息的利用不足以及对结构化和非结构化知识的等量处理。为了解决这些问题，我们提出了一个全面的预训练框架BioT5，它通过化学知识和自然语言关联丰富了生物学中的跨模态整合。BioT5利用SELFIES进行100%鲁棒的分子表示，并从非结构化的生物文献中提取生物实体周围上下文的知识。此外，BioT5区分结构化和非结构化知识，从而更有效地利用信息。在微调后，BioT5在各种任务中展现出卓越的性能，表明其强大的能力。

    Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability 
    
[^21]: 基于伦理推理的道德对齐：关于在LLM中上下文伦理政策的案例和框架

    Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs. (arXiv:2310.07251v1 [cs.CL])

    [http://arxiv.org/abs/2310.07251](http://arxiv.org/abs/2310.07251)

    本文提出了一种在LLM中应用通用伦理推理能力而非特定伦理原则的方法，以处理全球范围的价值多元性。作者开发了一个框架，将道德困境与不同形式的规范伦理以及不同抽象层次的道德原则相结合。初步实验表明，尽管GPT-4几乎可以完美地进行伦理推理，但这些模型仍然存在对西方和以英语为母语的社会道德价值的偏见。

    

    在这篇论文中，我们认为与其对LLM进行道德对齐到特定的伦理原则，我们应该注入通用的伦理推理能力，使其能够处理全球范围的价值多元性。当提供伦理政策时，LLM应该能够做出与政策一致的伦理决策。我们开发了一个框架，将道德困境与不同形式的规范伦理以及不同抽象层次的道德原则相结合。对GPT-x模型的初步实验表明，虽然GPT-4几乎可以完美地进行伦理推理，但这些模型仍然存在对西方和以英语为母语的社会道德价值的偏见。

    In this position paper, we argue that instead of morally aligning LLMs to specific set of ethical principles, we should infuse generic ethical reasoning capabilities into them so that they can handle value pluralism at a global scale. When provided with an ethical policy, an LLM should be capable of making decisions that are ethically consistent to the policy. We develop a framework that integrates moral dilemmas with moral principles pertaining to different foramlisms of normative ethics, and at different levels of abstractions. Initial experiments with GPT-x models shows that while GPT-4 is a nearly perfect ethical reasoner, the models still have bias towards the moral values of Western and English speaking societies.
    
[^22]: 在医学问题回答中探索大型语言模型的领域: 观察和开放问题

    Exploring the Landscape of Large Language Models In Medical Question Answering: Observations and Open Questions. (arXiv:2310.07225v1 [cs.CL])

    [http://arxiv.org/abs/2310.07225](http://arxiv.org/abs/2310.07225)

    通过评估多种流行的大型语言模型在医学问题方面的知识，本研究提供了对这些模型作为一个群体的初步观察，并提出了进一步研究的开放问题。

    

    大型语言模型(LLMs)在医学问题回答领域显示出潜力，通过在标准化考试中取得及格分数，并被认为是支持医疗保健工作者的工具。将LLMs部署到如此高风险的环境中需要对这些模型的限制有清晰的理解。随着新的LLMs的快速发展和发布，识别跨模型存在的模式，并因此可能出现在新版本中，特别有价值。在本文中，我们评估了多种流行LLM在医学问题方面的知识，以更好地了解它们作为一个群体的特性。通过这个比较，我们提供了初步的观察，并提出了进一步研究的开放问题。

    Large Language Models (LLMs) have shown promise in medical question answering by achieving passing scores in standardised exams and have been suggested as tools for supporting healthcare workers. Deploying LLMs into such a high-risk context requires a clear understanding of the limitations of these models. With the rapid development and release of new LLMs, it is especially valuable to identify patterns which exist across models and may, therefore, continue to appear in newer versions. In this paper, we evaluate a wide range of popular LLMs on their knowledge of medical questions in order to better understand their properties as a group. From this comparison, we provide preliminary observations and raise open questions for further research.
    
[^23]: 自适应门控在基于混合专家语言模型中的应用

    Adaptive Gating in Mixture-of-Experts based Language Models. (arXiv:2310.07188v1 [cs.CL])

    [http://arxiv.org/abs/2310.07188](http://arxiv.org/abs/2310.07188)

    本文介绍了一种自适应门控的混合专家语言模型训练策略，通过根据标记的专家概率分布将标记分配给变量数量的专家，同时保持模型的稀疏性和提高训练效率。

    

    大型语言模型（如OpenAI的ChatGPT）在各种NLP任务中展现出了出色的语言理解能力。稀疏激活的混合专家（MoE）已经成为一种有前途的解决方案，可以在保持计算操作数量恒定的同时扩大模型规模。现有的MoE模型采用了固定的门控网络，每个标记都由相同数量的专家计算。然而，这种方法与我们的直觉相矛盾，因为每个序列中的标记在语言复杂性方面有所不同，因此需要不同的计算成本。先前的研究中很少讨论每个标记的计算和模型性能之间的权衡。本文介绍了在MoE中使用自适应门控的灵活训练策略，该策略可以根据专家概率分布将标记处理为可变数量的专家。所提出的框架在改进训练效率的同时保持稀疏性。此外，课程学习也在该框架中应用以进一步提高模型性能。

    Large language models, such as OpenAI's ChatGPT, have demonstrated exceptional language understanding capabilities in various NLP tasks. Sparsely activated mixture-of-experts (MoE) has emerged as a promising solution for scaling models while maintaining a constant number of computational operations. Existing MoE model adopts a fixed gating network where each token is computed by the same number of experts. However, this approach contradicts our intuition that the tokens in each sequence vary in terms of their linguistic complexity and, consequently, require different computational costs. Little is discussed in prior research on the trade-off between computation per token and model performance. This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution. The proposed framework preserves sparsity while improving training efficiency. Additionally, curriculum learning 
    
[^24]: 在线推测解码

    Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])

    [http://arxiv.org/abs/2310.07177](http://arxiv.org/abs/2310.07177)

    在线推测解码是通过利用多余计算能力，在LLM服务集群中持续更新草稿模型，从而加速大型语言模型推理的一种方法。

    

    推测解码是通过利用较小的草稿模型来预测目标模型的输出，从而加速大型语言模型（LLM）推理的关键技术。然而，在面对多样的文本输入和草稿模型与目标模型之间的显著能力差距时，其有效性可能受到限制。我们引入了在线推测解码（OSD）来解决这一挑战。其主要思想是利用LLM服务集群中丰富的多余计算能力，根据观察到的用户查询数据持续更新（多个）草稿模型。由于LLM推理受内存限制，典型的LLM服务集群中的剩余计算能力可以用于在线重新训练草稿模型，从而使训练成本保持中性。由于LLM服务的查询分布相对简单，根据查询分布进行重新训练可以使草稿模型更准确地预测目标模型的输出。

    Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
    
[^25]: PHALM:通过引导人类和语言模型从零开始构建知识图谱

    PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a Language Model. (arXiv:2310.07170v1 [cs.CL])

    [http://arxiv.org/abs/2310.07170](http://arxiv.org/abs/2310.07170)

    本文提出了一种从零开始构建知识图谱的方法，通过引导众包工作者和大型语言模型，该方法在构建日语事件知识图谱和训练常识生成模型方面取得了良好的结果。

    

    尽管预训练的Transformer在自然语言理解方面取得了显著进展，但神经语言模型往往无法良好处理常识知识。为了创造常识感知的模型，已经尝试了从自动获取到众包获取知识的方法。然而，从零开始以较低成本获得高质量的知识库是困难的。在本文中，我们提出了PHALM，一种通过引导众包工作者和大型语言模型(LLM)来从零开始构建知识图谱的方法。我们使用这种方法构建了一个日语事件知识图谱，并训练了日语常识生成模型。实验结果显示了构建的图谱和训练模型生成的推理的可接受性。我们还报告了引导人类和LLM的差异。我们的代码、数据和模型可以在github.com/nlp-waseda/comet-atomic-ja上获得。

    Despite the remarkable progress in natural language understanding with pretrained Transformers, neural language models often do not handle commonsense knowledge well. Toward commonsense-aware models, there have been attempts to obtain knowledge, ranging from automatic acquisition to crowdsourcing. However, it is difficult to obtain a high-quality knowledge base at a low cost, especially from scratch. In this paper, we propose PHALM, a method of building a knowledge graph from scratch, by prompting both crowdworkers and a large language model (LLM). We used this method to build a Japanese event knowledge graph and trained Japanese commonsense generation models. Experimental results revealed the acceptability of the built graph and inferences generated by the trained models. We also report the difference in prompting humans and an LLM. Our code, data, and models are available at github.com/nlp-waseda/comet-atomic-ja.
    
[^26]: VoIP平台上语音增强的心理声学挑战

    Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms. (arXiv:2310.07161v1 [cs.SD])

    [http://arxiv.org/abs/2310.07161](http://arxiv.org/abs/2310.07161)

    本研究在VoIP通信领域中探索了声学转换的复杂性，并通过分析心理声学指标，揭示了语音增强对VoIP系统的影响。

    

    在VoIP（互联网语音传输协议）通信中，由声学转换引入的复杂性需要进行严格的分析。本研究基于对专有发送端降噪效果的探索，对Google Meets和Zoom等平台进行了细致评估。研究利用Deep Noise Suppression（DNS）2020数据集，确保了针对各种降噪设置和接收器接口的结构化考察。通过将Oaxaca分解引入到声学-语音扰动分析中，本研究引入了一种方法论的创新，该分解通常是经济计量学工具，在此处重新用于分析VoIP系统中的声学-语音扰动。为了进一步确定这些转换的影响，利用心理声学指标，特别是PESQ和STOI，来提供对语音改变的全面理解。总体而言，所获得的观点突出显示了VoIP影响的声学动力学的复杂景观。

    Within the ambit of VoIP (Voice over Internet Protocol) telecommunications, the complexities introduced by acoustic transformations merit rigorous analysis. This research, rooted in the exploration of proprietary sender-side denoising effects, meticulously evaluates platforms such as Google Meets and Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset, ensuring a structured examination tailored to various denoising settings and receiver interfaces. A methodological novelty is introduced via the Oaxaca decomposition, traditionally an econometric tool, repurposed herein to analyze acoustic-phonetic perturbations within VoIP systems. To further ground the implications of these transformations, psychoacoustic metrics, specifically PESQ and STOI, were harnessed to furnish a comprehensive understanding of speech alterations. Cumulatively, the insights garnered underscore the intricate landscape of VoIP-influenced acoustic dynamics. In addition to the primary findings, a 
    
[^27]: "两个运动的故事": 使用弱监督的基于图的结构化预测方法鉴别和比较#BlackLivesMatter和#BlueLivesMatter运动相关的推文中的观点

    "A Tale of Two Movements": Identifying and Comparing Perspectives in #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly Supervised Graph-based Structured Prediction. (arXiv:2310.07155v1 [cs.CL])

    [http://arxiv.org/abs/2310.07155](http://arxiv.org/abs/2310.07155)

    本论文提出了一种基于图的弱监督方法，用于识别和比较#BlackLivesMatter和#BlueLivesMatter运动相关推文中的观点。通过将文本转换为图，并根据作者的社交网络进行结构化预测，该方法能够在缺乏标注数据的情况下模拟运动中的观点。

    

    社交媒体已成为推动社会变革的重要因素，通过促进在线社会运动的形成。自动理解推动运动和反对运动声音的观点是一项具有挑战性的任务，因为很难获得带有注释的数据。我们提出了一种弱监督的基于图的方法，显式地对#BlakcLivesMatter相关推文中的观点进行建模。我们的方法利用了社会-语言学的数据表示。我们通过将文本分解为结构化元素并与作者的社交网络相连接将其转换为图形，然后使用结构化预测来识别观点。我们的方法使用了一小组标记示例。我们尝试使用大语言模型生成人工训练示例进行实验，并将其与手动注释进行比较，发现它们具有可比较的性能。我们使用人工注释的测试集进行定量和定性分析。我们的模型...

    Social media has become a major driver of social change, by facilitating the formation of online social movements. Automatically understanding the perspectives driving the movement and the voices opposing it, is a challenging task as annotated data is difficult to obtain. We propose a weakly supervised graph-based approach that explicitly models perspectives in #BackLivesMatter-related tweets. Our proposed approach utilizes a social-linguistic representation of the data. We convert the text to a graph by breaking it into structured elements and connect it with the social network of authors, then structured prediction is done over the elements for identifying perspectives. Our approach uses a small seed set of labeled examples. We experiment with large language models for generating artificial training examples, compare them to manual annotation, and find that it achieves comparable performance. We perform quantitative and qualitative analyses using a human-annotated test set. Our model
    
[^28]: QFT: 使用可承担资源对LLMs进行量化全参数调整

    QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources. (arXiv:2310.07147v1 [cs.CL])

    [http://arxiv.org/abs/2310.07147](http://arxiv.org/abs/2310.07147)

    我们提出了一种新的QFT框架，可以对LLMs进行内存高效的全参数微调，而不损害性能。

    

    大型语言模型（LLMs）在自然语言处理任务中展示出了显著的影响。对这些预训练模型进行微调可以进一步提高性能，但由于其巨大的资源需求，这一过程具有挑战性。为此，现有的努力都集中在参数高效的微调上，不幸的是，它们没有充分发挥全参数微调的潜力。在这项工作中，我们提出了QFT，一种新颖的用于LLMs的量化全参数调整框架，可以在不损害性能的情况下实现高效的内存微调。我们的框架包括两个新颖的思想：（i）我们采用高效的Lion优化器，仅跟踪动量并具有每个参数一致的更新幅度，这对于稳健的量化是一种内在优势；（ii）我们将所有模型状态进行量化，并以整数值存储，同时提供梯度流和参数更新的方法。

    Large Language Models (LLMs) have showcased remarkable impacts across a wide spectrum of natural language processing tasks. Fine-tuning these pre-trained models on downstream datasets provides further significant performance gains, but this process has been challenging due to its extraordinary resource requirements. To this end, existing efforts focus on parameter-efficient fine-tuning, which, unfortunately, fail to capitalize on the powerful potential of full-parameter fine-tuning. In this work, we propose QFT, a novel Quantized Full-parameter Tuning framework for LLMs that enables memory-efficient fine-tuning without harming performance. Our framework incorporates two novel ideas: (i) we adopt the efficient Lion optimizer, which only keeps track of the momentum and has consistent update magnitudes for each parameter, an inherent advantage for robust quantization; and (ii) we quantize all model states and store them as integer values, and present a gradient flow and parameter update s
    
[^29]: 借助大型语言模型加强心理疗法：通过思维诱导进行认知失真检测

    Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting. (arXiv:2310.07146v1 [cs.CL])

    [http://arxiv.org/abs/2310.07146](http://arxiv.org/abs/2310.07146)

    该论文借助大型语言模型进行心理疗法的人工智能辅助，在认知失真检测任务中提出了思维诱导诊断（DoT）方法，通过三个阶段对患者的言语进行诊断，取得了显著的改进。

    

    由于专业人员的严重匮乏和可获得性限制，精神疾病仍然是我们这个时代最重要的公共卫生问题之一。心理疗法需要高水平的专门知识，以对患者的认知建模进行深度、复杂的推理和分析。在大型语言模型的时代，我们认为现在是开发计算心理疗法的人工智能辅助的合适时机。我们研究了认知失真检测的任务，并提出了思维诱导诊断（DoT）。DoT通过三个阶段对患者的言语进行诊断：主观性评估以区分事实和思维；对比推理以引出支持和反驳思维的推理过程；模式分析以总结认知模式。通过这三个阶段生成的诊断解释对辅助专业人员非常重要。实验结果显示，DoT在认知失真方面的效果明显优于ChatGPT。

    Mental illness remains one of the most critical public health issues of our time, due to the severe scarcity and accessibility limit of professionals. Psychotherapy requires high-level expertise to conduct deep, complex reasoning and analysis on the cognition modeling of the patients. In the era of Large Language Models, we believe it is the right time to develop AI assistance for computational psychotherapy. We study the task of cognitive distortion detection and propose the Diagnosis of Thought (DoT) prompting. DoT performs diagnosis on the patient's speech via three stages: subjectivity assessment to separate the facts and the thoughts; contrastive reasoning to elicit the reasoning processes supporting and contradicting the thoughts; and schema analysis to summarize the cognition schemas. The generated diagnosis rationales through the three stages are essential for assisting the professionals. Experiments demonstrate that DoT obtains significant improvements over ChatGPT for cogniti
    
[^30]: AE-smnsMLC：基于语义匹配和负标签采样的产品属性值提取的多标签分类

    AE-smnsMLC: Multi-Label Classification with Semantic Matching and Negative Label Sampling for Product Attribute Value Extraction. (arXiv:2310.07137v1 [cs.IR])

    [http://arxiv.org/abs/2310.07137](http://arxiv.org/abs/2310.07137)

    本论文提出了一种基于语义匹配和负标签采样的多标签分类模型 AE-smnsMLC，用于解决产品属性值提取的问题。该模型将属性值提取任务转化为多标签分类任务，可以应用于只有属性值注释的实际场景，而无需属性值的位置信息注释。

    

    产品属性值提取在电子商务等许多实际应用中起着重要作用，如产品搜索和推荐。之前的方法将其视为需要更多注释来标注产品文本中值的序列标记任务。这限制了它们在实际场景中的应用，其中每个产品只有属性值的弱标注，而没有它们的位置信息。此外，这些方法只使用产品文本（即产品标题和描述），而不考虑给定产品的多个属性值与其文本之间的语义连接，这可以帮助属性值提取。在本文中，我们将这个任务重新定义为一个多标签分类任务，可以在实际场景中应用，其中只有属性值的注释可用于训练模型（即没有属性值位置信息的注释）。我们提出了一个具有语义匹配和负标签采样的分类模型

    Product attribute value extraction plays an important role for many real-world applications in e-Commerce such as product search and recommendation. Previous methods treat it as a sequence labeling task that needs more annotation for position of values in the product text. This limits their application to real-world scenario in which only attribute values are weakly-annotated for each product without their position. Moreover, these methods only use product text (i.e., product title and description) and do not consider the semantic connection between the multiple attribute values of a given product and its text, which can help attribute value extraction. In this paper, we reformulate this task as a multi-label classification task that can be applied for real-world scenario in which only annotation of attribute values is available to train models (i.e., annotation of positional information of attribute values is not available). We propose a classification model with semantic matching and
    
[^31]: 跨语言风格比较研究

    Comparing Styles across Languages. (arXiv:2310.07135v1 [cs.CL])

    [http://arxiv.org/abs/2310.07135](http://arxiv.org/abs/2310.07135)

    本研究通过引入解释框架，从多语言语言模型中提取风格差异并比较不同语言之间的风格，创建了全面的多语言礼貌数据集，探索了礼貌在四种语言中的变化，为评估语言类别对风格变化的贡献和了解世界各地人们的不同沟通方式提供了有效的方法和解释洞察力。

    

    理解跨语言风格的差异有助于训练人类和计算机生成符合文化背景的文本。我们引入了一个解释框架，可以从多语言语言模型中提取风格差异，并比较不同语言之间的风格。我们的框架(1)可以生成任何语言的全面风格词典，(2)将语言模型中的特征重要性统一为可比较的词汇类别。我们应用该框架比较了礼貌语言，创建了第一个全面的多语言礼貌数据集，并探索了礼貌在四种语言中的变化。我们的方法可以有效评估不同语言类别对风格变化的贡献，并提供可解释的洞察力，了解世界各地人们的不同沟通方式。

    Understanding how styles differ across languages is advantageous for training both humans and computers to generate culturally appropriate text. We introduce an explanation framework to extract stylistic differences from multilingual LMs and compare styles across languages. Our framework (1) generates comprehensive style lexica in any language and (2) consolidates feature importances from LMs into comparable lexical categories. We apply this framework to compare politeness, creating the first holistic multilingual politeness dataset and exploring how politeness varies across four languages. Our approach enables an effective evaluation of how distinct linguistic categories contribute to stylistic variations and provides interpretable insights into how people communicate differently around the world.
    
[^32]: 《人脑中语言处理的时态结构与深度语言模型的层次结构相符》

    The Temporal Structure of Language Processing in the Human Brain Corresponds to The Layered Hierarchy of Deep Language Models. (arXiv:2310.07106v1 [cs.CL])

    [http://arxiv.org/abs/2310.07106](http://arxiv.org/abs/2310.07106)

    本文展示了深度语言模型(DLMs)的分层结构与人脑语言理解的时间动态之间存在强相关性，通过采用电子皮层图(ECoG)数据来优化时间分辨率，为深入了解人脑语言处理机制提供了新的视角。

    

    深度语言模型(DLMs)提供了一种理解人脑自然语言处理机制的新的计算范式。与传统的心理语言学模型不同，DLMs使用分层的连续数值向量序列来表示单词和上下文，使得诸多新的应用成为可能，如类人生成文本。本文通过展示DLMs的分层结构可能用于模拟大脑语言理解的时间动态，证明了DLM层次深度与最能预测人脑的层次时间之间的强相关性。我们之所以能够在时间上解析出每个层次的优势在于我们采用了电子皮质图(ECoG)数据，其时间分辨率比功能性磁共振成像(fMRI)等无损测量方法更高。我们利用ECoG从参与者听30分钟叙述时记录神经活动，同时将相同叙述提供给高性能DLM(GPT2-XL)。

    Deep Language Models (DLMs) provide a novel computational paradigm for understanding the mechanisms of natural language processing in the human brain. Unlike traditional psycholinguistic models, DLMs use layered sequences of continuous numerical vectors to represent words and context, allowing a plethora of emerging applications such as human-like text generation. In this paper we show evidence that the layered hierarchy of DLMs may be used to model the temporal dynamics of language comprehension in the brain by demonstrating a strong correlation between DLM layer depth and the time at which layers are most predictive of the human brain. Our ability to temporally resolve individual layers benefits from our use of electrocorticography (ECoG) data, which has a much higher temporal resolution than noninvasive methods like fMRI. Using ECoG, we record neural activity from participants listening to a 30-minute narrative while also feeding the same narrative to a high-performing DLM (GPT2-XL)
    
[^33]: 稀疏通用Transformer

    Sparse Universal Transformer. (arXiv:2310.07096v1 [cs.CL])

    [http://arxiv.org/abs/2310.07096](http://arxiv.org/abs/2310.07096)

    本文介绍了稀疏通用Transformer（SUT），它通过利用稀疏混合专家（SMoE）和一种新的基于切棍法的动态停止机制来减少计算复杂性，同时保持参数效率和泛化能力。实验证明，SUT在形式语言任务上具有与强基线模型相当的性能，并能显著降低计算资源使用。

    

    通用Transformer（UT）是Transformer的一种变体，其在各层之间共享参数。实证证据表明，在形式语言任务中，UT比Vanilla Transformer（VT）具有更好的组合泛化能力。参数共享还使其具有比VT更好的参数效率。尽管具有许多优点，但扩展UT参数比扩展VT更需要计算和内存资源。本文提出了稀疏通用Transformer（SUT），它利用稀疏混合专家（SMoE）和基于切棍法的动态停止机制来减少UT的计算复杂性，同时保持其参数效率和泛化能力。实验表明，SUT在WMT'14上仅使用一半的计算资源和参数就能达到与强基线模型相同的性能，并在形式语言任务（逻辑推理和CFQ）上具有强大的泛化性能。新的停止机制还能使计算资源减少约50％。

    The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers. Empirical evidence shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The parameter-sharing also affords it better parameter efficiency than VTs. Despite its many advantages, scaling UT parameters is much more compute and memory intensive than scaling up a VT. This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) and a new stick-breaking-based dynamic halting mechanism to reduce UT's computation complexity while retaining its parameter efficiency and generalization ability. Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT'14 and strong generalization results on formal language tasks (Logical inference and CFQ). The new halting mechanism also enables around 50\% reduction in compu
    
[^34]: 论辩立场预测：多模态和少样本学习的探索性研究

    Argumentative Stance Prediction: An Exploratory Study on Multimodality and Few-Shot Learning. (arXiv:2310.07093v1 [cs.CL])

    [http://arxiv.org/abs/2310.07093](http://arxiv.org/abs/2310.07093)

    该研究通过探索性研究发现，在多模态立场预测中，微调的文本语言模型集合表现更好，并且多模态模型更适合将图像内容进行自然语言摘要，使用上下文示例可以提高LLM的少样本预测性能。

    

    为了推进作为多模态问题的论辩立场预测，多模态论证挖掘的首个共享任务主持了关于枪支控制和堕胎等关键社会议题的立场预测。我们的探索性研究旨在评估图像在推文中用于立场预测的必要性，并比较开箱即用的基于文本的大型语言模型（LLM）在少样本环境下与微调的单模态和多模态模型的性能。我们的工作表明，微调的文本语言模型集合（0.817 F1分数）优于多模态模型（0.677 F1分数）和基于文本的少样本预测使用最新的LLM（0.550 F1分数）。除了性能差异，我们的研究结果显示，多模态模型在将图像内容进行自然语言摘要时表现更好，而不是使用原始像素结构，并且在上下文示例下使用可以提高LLM的少样本预测性能。

    To advance argumentative stance prediction as a multimodal problem, the First Shared Task in Multimodal Argument Mining hosted stance prediction in crucial social topics of gun control and abortion. Our exploratory study attempts to evaluate the necessity of images for stance prediction in tweets and compare out-of-the-box text-based large-language models (LLM) in few-shot settings against fine-tuned unimodal and multimodal models. Our work suggests an ensemble of fine-tuned text-based language models (0.817 F1-score) outperforms both the multimodal (0.677 F1-score) and text-based few-shot prediction using a recent state-of-the-art LLM (0.550 F1-score). In addition to the differences in performance, our findings suggest that the multimodal models tend to perform better when image content is summarized as natural language over their native pixel structure and, using in-context examples improves few-shot performance of LLMs.
    
[^35]: Jaeger:一种基于连接的多变换器VQA模型

    Jaeger: A Concatenation-Based Multi-Transformer VQA Model. (arXiv:2310.07091v1 [cs.CL])

    [http://arxiv.org/abs/2310.07091](http://arxiv.org/abs/2310.07091)

    Jaeger是一种基于连接的多变换器VQA模型，利用RoBERTa large和GPT2-xl作为特征提取器，通过并行考虑多源信息来增强模型表征能力。

    

    基于文档的视觉问答在语言意义消歧和细粒度多模态检索之间提出了一个具有挑战性的任务。虽然由于大规模语言和开放世界先验模型的利用，文档问答取得了鼓舞人心的进展，但仍存在一些挑战，包括响应时间延长、推断持续时间延长和匹配不准确。为了克服这些挑战，我们提出了一种基于连接的多变换器VQA模型Jaegar。为了提取问题特征，我们利用了RoBERTa large和GPT2-xl等预训练模型的强大能力作为特征提取器。随后，我们将两种模型的输出进行连接操作。这个操作使得模型可以同时考虑来自不同来源的信息，增强了其表征能力。通过利用预训练模型进行特征提取，我们的方法有可能增强性能。

    Document-based Visual Question Answering poses a challenging task between linguistic sense disambiguation and fine-grained multimodal retrieval. Although there has been encouraging progress in document-based question answering due to the utilization of large language and open-world prior models\cite{1}, several challenges persist, including prolonged response times, extended inference durations, and imprecision in matching. In order to overcome these challenges, we propose Jaegar, a concatenation-based multi-transformer VQA model. To derive question features, we leverage the exceptional capabilities of RoBERTa large\cite{2} and GPT2-xl\cite{3} as feature extractors. Subsequently, we subject the outputs from both models to a concatenation process. This operation allows the model to consider information from diverse sources concurrently, strengthening its representational capability. By leveraging pre-trained models for feature extraction, our approach has the potential to amplify the pe
    
[^36]: 思维多样性提升大规模语言模型的推理能力

    Diversity of Thought Improves Reasoning Abilities of Large Language Models. (arXiv:2310.07088v1 [cs.CL])

    [http://arxiv.org/abs/2310.07088](http://arxiv.org/abs/2310.07088)

    本文提出了一种方法，通过改变输入提示来提高大规模语言模型的推理能力，从而改善模型在复杂推理场景中的表现。这种方法自动采集模型反馈，生成适合问题的多样化提示，并通过多次推理调用来集成这些多样化的提示。

    

    大规模语言模型在需要复杂推理的环境中表现不佳。然而，将模型指导分解问题为更小的推理步骤或通过修改解码步骤使各种生成结果合并可以提升性能。目前的方法都假设输入提示是固定的，并期望解码策略引入所需的多样性。本文放松了这个假设，并讨论了如何通过创建和利用输入提示的变化来提升思维多样性以改善模型性能。我们提出了一种方法，通过向语言模型征求反馈来构思适合问题的方法，从而自动提高提示的多样性。我们在我们的方法DIV-SE (DIVerse reasoning path Self-Ensemble)中对多样的提示进行合成，通过多次推理调用实现。我们还提出了一种经济高效的替代方案，即在一个推理中使用多样的提示。

    Large language models (LLMs) are documented to struggle in settings that require complex reasoning. Nevertheless, instructing the model to break down the problem into smaller reasoning steps (Wei et al., 2022), or ensembling various generations through modifying decoding steps (Wang et al., 2023) boosts performance. Current methods assume that the input prompt is fixed and expect the decoding strategies to introduce the diversity needed for ensembling. In this work, we relax this assumption and discuss how one can create and leverage variations of the input prompt as a means to diversity of thought to improve model performance. We propose a method that automatically improves prompt diversity by soliciting feedback from the LLM to ideate approaches that fit for the problem. We then ensemble the diverse prompts in our method DIV-SE (DIVerse reasoning path Self-Ensemble) across multiple inference calls. We also propose a cost-effective alternative where diverse prompts are used within a s
    
[^37]: 跨越门槛：通过检索增强和损失加权实现惯用机器翻译

    Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting. (arXiv:2310.07081v1 [cs.CL])

    [http://arxiv.org/abs/2310.07081](http://arxiv.org/abs/2310.07081)

    本文通过引入检索增强和损失加权两种方法，成功改进了机器翻译系统对于惯用表达的翻译准确性。

    

    惯用语在日常语言中很常见，但对于翻译人员来说常常是一个挑战，因为它们的意义不是由它们的部分的意义推导出来的。尽管取得了重大进展，机器翻译系统仍然难以翻译惯用表达。我们提供了惯用翻译和相关问题的简单描述。这使我们能够进行一个综合实验，揭示了基于transformer的机器翻译模型在正确地默认采用惯用翻译时的临界点。为了扩大多语资源，我们编制了一个包含法语、芬兰语和日语中包含惯用表达的大约4k个自然句子的数据集。为了改进对自然惯用语的翻译，我们引入了两种简单但有效的技术：在可能具有惯用性的句子上策略性地加权训练损失，并使用检索增强模型。这不仅使强大的预训练机器翻译模型在惯用句子上的准确性提高了最多13%，还改进了翻译的准确性。

    Idioms are common in everyday language, but often pose a challenge to translators because their meanings do not follow from the meanings of their parts. Despite significant advances, machine translation systems still struggle to translate idiomatic expressions. We provide a simple characterization of idiomatic translation and related issues. This allows us to conduct a synthetic experiment revealing a tipping point at which transformer-based machine translation models correctly default to idiomatic translations. To expand multilingual resources, we compile a dataset of ~4k natural sentences containing idiomatic expressions in French, Finnish, and Japanese. To improve translation of natural idioms, we introduce two straightforward yet effective techniques: the strategic upweighting of training loss on potentially idiomatic sentences, and using retrieval-augmented models. This not only improves the accuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in absolute a
    
[^38]: 通过抗内容采样对COVID-19虚假信息数据集进行审计和强化

    Auditing and Robustifying COVID-19 Misinformation Datasets via Anticontent Sampling. (arXiv:2310.07078v1 [cs.LG])

    [http://arxiv.org/abs/2310.07078](http://arxiv.org/abs/2310.07078)

    本文拓展了对COVID-19虚假信息数据集进行审计和强化的研究。首先，发现在小数据上训练的专业分类器在野外环境中的性能表现有限。其次，提出了一种无需人工注释的主动学习方法，通过增加具有挑战性的抗内容来增强分类器。

    

    本文有两个关键贡献。首先，它认为在小数据上训练的高度专业的罕见内容分类器通常对野外观察到的负面类别的丰富性和话题多样性（称为抗内容）有限暴露。因此，这些分类器在测试集上观察到的强大性能可能无法在现实世界的环境中有效转化。在COVID-19虚假信息检测的背景下，我们对多个数据集进行了野外审计，并证明了使用几个重要引用的最近数据集进行训练的模型在野外评估时容易受到抗内容的攻击。其次，我们提出了一种新颖的主动学习流程，它不需要任何人工注释，并通过挑战性的抗内容不断增强训练数据，从而强化这些分类器。

    This paper makes two key contributions. First, it argues that highly specialized rare content classifiers trained on small data typically have limited exposure to the richness and topical diversity of the negative class (dubbed anticontent) as observed in the wild. As a result, these classifiers' strong performance observed on the test set may not translate into real-world settings. In the context of COVID-19 misinformation detection, we conduct an in-the-wild audit of multiple datasets and demonstrate that models trained with several prominently cited recent datasets are vulnerable to anticontent when evaluated in the wild. Second, we present a novel active learning pipeline that requires zero manual annotation and iteratively augments the training data with challenging anticontent, robustifying these classifiers.
    
[^39]: 通过有限状态解码实现无语法错误和具有泛化能力的LLM工具使用

    Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding. (arXiv:2310.07075v1 [cs.CL])

    [http://arxiv.org/abs/2310.07075](http://arxiv.org/abs/2310.07075)

    本文提出了一种无语法错误且具有泛化能力的LLM工具使用方法ToolDec，通过有限状态解码算法消除了工具相关错误，使LLM能够有效选择工具，而无需微调或上下文文档。

    

    大型语言模型(LLMs)已经展示出使用外部工具解决复杂问题的有希望的能力。然而，现有方法要么涉及对工具演示进行微调，这样在没有额外训练的情况下无法推广到新的工具，要么在上下文中提供工具文档，从而限制了工具数量。这两种方法常常产生语法无效的工具调用。在本文中，我们提出了ToolDec，一种有限状态机引导的解码算法，用于工具增强的LLMs。ToolDec通过确保有效的工具名称和类型一致的参数，消除了任何工具增强的LLMs中的工具相关错误。此外，ToolDec使LLM能够仅仅使用它们的名称中包含的信息有效地选择工具，而无需微调或上下文文档。我们在涉及数学函数、知识图谱关系和复杂的现实世界RESTful API的各种任务上评估了多种先前的方法及其ToolDec增强版本。

    Large language models (LLMs) have shown promising capabilities in using external tools to solve complex problems. However, existing approaches either involve fine-tuning on tool demonstrations, which do not generalize to new tools without additional training, or providing tool documentation in context, limiting the number of tools. Both approaches often generate syntactically invalid tool calls. In this paper, we propose ToolDec, a finite-state machine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates tool-related errors for any tool-augmented LLMs by ensuring valid tool names and type-conforming arguments. Furthermore, ToolDec enables LLM to effectively select tools using only the information contained in their names, with no need for fine-tuning or in-context documentation. We evaluated multiple prior methods and their ToolDec-enhanced versions on a variety of tasks involving tools like math functions, knowledge graph relations, and complex real-world RESTful APIs
    
[^40]: 大型语言模型可以学习规则

    Large Language Models can Learn Rules. (arXiv:2310.07064v1 [cs.AI])

    [http://arxiv.org/abs/2310.07064](http://arxiv.org/abs/2310.07064)

    大型语言模型(LLMs)在各种推理任务中展示了令人印象深刻的性能。为了提高提示方法的准确性和一致性，我们提出了Hypotheses-to-Theories (HtT)框架，用于学习LLMs推理的规则库，从而改进了现有的提示方法。

    

    当给出一些示例和中间步骤时，大型语言模型(LLMs)在各种推理任务中展示了令人印象深刻的性能。然而，依赖LLM中的隐式知识的提示方法在隐式知识错误或与任务不一致时往往会产生错误的答案。为解决这个问题，我们提出了"假设到理论" (HtT) 框架，用于学习LLMs推理的规则库。HtT包括两个阶段，归纳阶段和演绎阶段。在归纳阶段，首先要求LLM根据一组训练示例生成和验证规则。出现并导致正确答案的规则将被收集形成一个规则库。在演绎阶段，然后要求LLM使用学习的规则库进行推理以回答测试问题。在数值推理和关系推理问题上的实验证明，HtT改进了现有的提示方法，使其性能提升。

    When prompted with a few examples and intermediate steps, large language models (LLMs) have demonstrated impressive performance in various reasoning tasks. However, prompting methods that rely on implicit knowledge in an LLM often hallucinate incorrect answers when the implicit knowledge is wrong or inconsistent with the task. To tackle this problem, we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs. HtT contains two stages, an induction stage and a deduction stage. In the induction stage, an LLM is first asked to generate and verify rules over a set of training examples. Rules that appear and lead to correct answers sufficiently often are collected to form a rule library. In the deduction stage, the LLM is then prompted to employ the learned rule library to perform reasoning to answer test questions. Experiments on both numerical reasoning and relational reasoning problems show that HtT improves existing prompting methods, with an 
    
[^41]: DKEC: 领域知识增强的电子病历多标签分类

    DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records. (arXiv:2310.07059v1 [cs.CL])

    [http://arxiv.org/abs/2310.07059](http://arxiv.org/abs/2310.07059)

    本文提出了DKEC，一种领域知识增强的分类器，用于医学诊断预测。它利用标签的注意力机制和组内训练方法来捕捉医学实体之间的语义关系，并增加罕见类别的样本数量。评估结果显示其在医学数据集上表现良好。

    

    医学领域的多标签文本分类任务经常面临长尾标签分布，即罕见类别的训练样本少于频繁类别。虽然之前的工作已经探索了不同的模型架构和层次化标签结构来找到重要特征，但大多数忽略了从医学指南中融入领域知识。本文提出了DKEC，一种增强医学诊断预测的领域知识增强分类器，其中包括两个创新点：（1）一个基于标签的注意力机制，结合异构图和领域本体来捕捉医学实体之间的语义关系，（2）一种基于标签相似性的简单而有效的组内训练方法，用于增加罕见类别的样本数量。我们在两个真实的医学数据集上评估了DKEC：RAA数据集，包含来自急救服务（EMS）事件的4,417个患者护理报告的收集，和来自53898报告的子集。

    Multi-label text classification (MLTC) tasks in the medical domain often face long-tail label distribution, where rare classes have fewer training samples than frequent classes. Although previous works have explored different model architectures and hierarchical label structures to find important features, most of them neglect to incorporate the domain knowledge from medical guidelines. In this paper, we present DKEC, Domain Knowledge Enhanced Classifier for medical diagnosis prediction with two innovations: (1) a label-wise attention mechanism that incorporates a heterogeneous graph and domain ontologies to capture the semantic relationships between medical entities, (2) a simple yet effective group-wise training method based on similarity of labels to increase samples of rare classes. We evaluate DKEC on two real-world medical datasets: the RAA dataset, a collection of 4,417 patient care reports from emergency medical services (EMS) incidents, and a subset of 53,898 reports from the 
    
[^42]: 从大规模交互轨迹中自动挖掘宏任务

    Automatic Macro Mining from Interaction Traces at Scale. (arXiv:2310.07023v1 [cs.HC])

    [http://arxiv.org/abs/2310.07023](http://arxiv.org/abs/2310.07023)

    本文介绍了一种基于大型语言模型的新方法，用于从移动交互轨迹中自动提取语义上有意义的宏任务。通过多项研究和实验证明了该方法的有效性和提取的宏任务的实用性。

    

    宏任务是我们日常手机活动的构建块任务（例如，“登录”或“预定航班”）。有效地提取宏任务对于理解移动交互和实现任务自动化至关重要。然而，这些宏任务在大规模情况下很难提取，因为它们可以由多个步骤组成，同时又隐藏在应用的编程组件中。在本文中，我们介绍了一种基于大型语言模型（LLMs）的新方法，以自动从随机和用户策划的移动交互轨迹中提取语义上有意义的宏任务。我们的方法产生的宏任务自动标记了自然语言描述，并且可以完全执行。为了检验提取的质量，我们进行了多项研究，包括用户评估、与人工策划任务的比较分析以及对这些宏任务的自动执行。这些实验和分析显示了我们方法的有效性以及提取的宏任务在不同的任务中的有用性。

    Macros are building block tasks of our everyday smartphone activity (e.g., "login", or "booking a flight"). Effectively extracting macros is important for understanding mobile interaction and enabling task automation. These macros are however difficult to extract at scale as they can be comprised of multiple steps yet hidden within programmatic components of the app. In this paper, we introduce a novel approach based on Large Language Models (LLMs) to automatically extract semantically meaningful macros from both random and user-curated mobile interaction traces. The macros produced by our approach are automatically tagged with natural language descriptions and are fully executable. To examine the quality of extraction, we conduct multiple studies, including user evaluation, comparative analysis against human-curated tasks, and automatic execution of these macros. These experiments and analyses show the effectiveness of our approach and the usefulness of extracted macros in various dow
    
[^43]: NEWTON: 大型语言模型能够进行物理推理吗？

    NEWTON: Are Large Language Models Capable of Physical Reasoning?. (arXiv:2310.07018v1 [cs.CL])

    [http://arxiv.org/abs/2310.07018](http://arxiv.org/abs/2310.07018)

    NEWTON是一个用于评估大型语言模型物理推理能力的仓库和基准，包含2800个物体-属性对和160K个问答问题。

    

    通过其语境化表示，大型语言模型（LLM）被实证地证明包含句法、语义、词义和常识知识。然而，对于它们在物理推理能力方面的探索还有一定限制，特别是涉及理解日常物体的关键属性。为了解决这一问题，我们引入了NEWTON，一个用于评估LLM的物理推理能力的仓库和基准。此外，为了实现此基准的领域特定适应，我们提供了一种流程，使研究人员能够生成一个根据他们应用程序相关物体和属性定制的基准变体。NEWTON仓库包括2800个物体-属性对的集合，为生成无限规模的评估模板奠定基础。NEWTON基准包含160K个问答问题，使用NEWTON仓库策划，以调查物理推理能力。

    Large Language Models (LLMs), through their contextualized representations, have been empirically proven to encapsulate syntactic, semantic, word sense, and common-sense knowledge. However, there has been limited exploration of their physical reasoning abilities, specifically concerning the crucial attributes for comprehending everyday objects. To address this gap, we introduce NEWTON, a repository and benchmark for evaluating the physics reasoning skills of LLMs. Further, to enable domain-specific adaptation of this benchmark, we present a pipeline to enable researchers to generate a variant of this benchmark that has been customized to the objects and attributes relevant for their application. The NEWTON repository comprises a collection of 2800 object-attribute pairs, providing the foundation for generating infinite-scale assessment templates. The NEWTON benchmark consists of 160K QA questions, curated using the NEWTON repository to investigate the physical reasoning capabilities of
    
[^44]: 答案候选类型选择：闭书问答中的文本到文本语言模型满足知识图谱

    Answer Candidate Type Selection: Text-to-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs. (arXiv:2310.07008v1 [cs.CL])

    [http://arxiv.org/abs/2310.07008](http://arxiv.org/abs/2310.07008)

    本文提出了一种新颖的方法，通过对预训练的文本到文本问答系统生成的候选答案基于其类型进行过滤和重新排序，以解决在知识图谱问答任务中，模型容量有限且对于含有不太流行实体的问题质量下降的问题。

    

    预训练的文本到文本语言模型（如T5或BART）在知识图谱问答（KGQA）任务中取得了令人期待的结果。然而，模型的容量有限，对于包含不太流行实体的问题，质量下降。在本文中，我们提出了一种新颖的方法，该方法在预训练的文本到文本问答系统的基础上解决了这个问题。我们的简单而有效的方法根据候选答案的类型（来自Wikidata的"instance_of"属性）进行筛选和重新排序。

    Pre-trained Text-to-Text Language Models (LMs), such as T5 or BART yield promising results in the Knowledge Graph Question Answering (KGQA) task. However, the capacity of the models is limited and the quality decreases for questions with less popular entities. In this paper, we present a novel approach which works on top of the pre-trained Text-to-Text QA system to address this issue. Our simple yet effective method performs filtering and re-ranking of generated candidates based on their types derived from Wikidata "instance_of" property.
    
[^45]: 通过利用生成技术实施开源LLM的灾难性越狱

    Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation. (arXiv:2310.06987v1 [cs.CL])

    [http://arxiv.org/abs/2310.06987](http://arxiv.org/abs/2310.06987)

    本文介绍了一种生成攻击技术，通过操纵解码方法的变化，可以导致开源LLMs的灾难性越狱，将错位率从0%提高到了超过95%。这项攻击方法简单而有效，在11个语言模型中表现优于最先进的攻击方法，且计算量降低了30倍。

    

    开源大规模语言模型（LLMs）的快速发展极大地推进了人工智能的发展。在发布模型之前，人们进行了大量的努力，以确保模型与人类价值观的一致性，主要目标是确保其有益且无害。然而，即使经过精心对齐的模型也可能被恶意操纵，导致意外行为，即所谓的“越狱”。这些越狱通常由特定的文本输入触发，通常称为对抗性提示。在这项工作中，我们提出了一种生成攻击技术，这是一种极其简单的方法，仅通过操纵解码方法的变化来扰乱模型的对齐。通过利用不同的生成策略，包括变化的解码超参数和采样方法，我们将11个语言模型，包括LLaMA2、Vicuna、Falcon和MPT家族的错位率从0%提高到了95%以上，超过了最先进的攻击方法，计算量降低了30倍。

    The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as "jailbreaks". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the misalignment rate from 0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with $30\times$ lower computat
    
[^46]: 通过元认知提示违反期望降低大型语言模型中的心智理论预测误差

    Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models. (arXiv:2310.06983v1 [cs.CL])

    [http://arxiv.org/abs/2310.06983](http://arxiv.org/abs/2310.06983)

    本文研究了如何利用违反期望机制在大型语言模型中降低用户预测误差。我们引入了元认知提示框架，并发现存储和检索违反用户期望的事实可以使模型以类似人类学习理论的方式了解用户。

    

    最近的研究表明，大型语言模型(LLMs)在心智理论(ToM)任务中展现出了令人信服的水平。将不可观察的心理状态归因于他人对于人类社会认知至关重要，并且在个体人类与人工智能(AIs)之间的委托-代理关系中可能同样重要。在本文中，我们探讨了一种在发展心理学中研究的机制，即违反期望(VoE)，如何实现以通过利用新生的ToM功能来降低LLM对用户的预测误差。我们引入了一个“元认知提示”框架，以在AI辅导员的情境中应用VoE。通过存储和检索在LLM对用户期望被违反的情况下得到的事实，我们发现LLM能够以与人类学习理论相符的方式了解用户。最后，我们讨论了建模用户心理的潜在危险和增强机会，并提出了控制这些问题的方法。

    Recent research shows that Large Language Models (LLMs) exhibit a compelling level of proficiency in Theory of Mind (ToM) tasks. This ability to impute unobservable mental states to others is vital to human social cognition and may prove equally important in principal-agent relations between individual humans and Artificial Intelligences (AIs). In this paper, we explore how a mechanism studied in developmental psychology known as Violation of Expectation (VoE) can be implemented to reduce errors in LLM prediction about users by leveraging emergent ToM affordances. And we introduce a \textit{metacognitive prompting} framework to apply VoE in the context of an AI tutor. By storing and retrieving facts derived in cases where LLM expectation about the user was violated, we find that LLMs are able to learn about users in ways that echo theories of human learning. Finally, we discuss latent hazards and augmentative opportunities associated with modeling user psychology and propose ways to mi
    
[^47]: 为什么在几何学中要费心？关于Transformer嵌入的线性分解的相关性。

    Why bother with geometry? On the relevance of linear decompositions of Transformer embeddings. (arXiv:2310.06977v1 [cs.CL])

    [http://arxiv.org/abs/2310.06977](http://arxiv.org/abs/2310.06977)

    本研究研究了Transformer嵌入的线性分解方法在机器翻译解码器中的应用。结果显示，虽然分解指标与模型表现有效相关，但不同运行之间的变异性表明对于数学重述是否在实证上具有意义还需要进一步研究。

    

    最近的一些研究表明，Transformer嵌入可以线性分解为明确定义的因子之和，这些因子可以与特定的网络输入或组件相关联。然而，仍然缺乏研究这些数学重述是否在实证上具有意义。在本研究中，我们使用两种嵌入分解方法研究了机器翻译解码器的表示。我们的结果表明，尽管分解导出的指标与模型性能有效相关，但不同运行之间的变异性表明对这个问题需要更加细致的考虑。我们测量的高度变异性表明，几何反映的是模型特定的特征，而不是特定于句子的计算，而相似的训练条件并不能保证相似的向量空间。

    A recent body of work has demonstrated that Transformer embeddings can be linearly decomposed into well-defined sums of factors, that can in turn be related to specific network inputs or components. There is however still a dearth of work studying whether these mathematical reformulations are empirically meaningful. In the present work, we study representations from machine-translation decoders using two of such embedding decomposition methods. Our results indicate that, while decomposition-derived indicators effectively correlate with model performance, variation across different runs suggests a more nuanced take on this question. The high variability of our measurements indicate that geometry reflects model-specific characteristics more than it does sentence-specific computations, and that similar training conditions do not guarantee similar vector spaces.
    
[^48]: 无需细粒度标签的多方面情感分析的文档级监督

    Document-Level Supervision for Multi-Aspect Sentiment Analysis Without Fine-grained Labels. (arXiv:2310.06940v1 [cs.CL])

    [http://arxiv.org/abs/2310.06940](http://arxiv.org/abs/2310.06940)

    本文提出了一种利用文档级监督进行多方面情感分析的方法，无需细粒度标签，可以检测文档中的多个方面，并能理解通过多个方面表达的情感。

    

    方面情感分析（ABSA）是一个广泛研究的主题，通常通过人工标注的意见文本进行监督训练。这些细粒度的标注包括识别用户表达情感的方面以及它们的极性（基于方面的情感）。然而，这些细粒度的标注在真实世界的环境中往往昂贵且难以实现。在某些情况下，用户生成的文本包含了整体情感，例如用户评论或用户反馈中的1-5评分，可以利用这些信息来进行任务。本文提出了一种基于VAE主题建模的方法，利用文档级监督进行ABSA，无需对方面或情感进行细粒度标签。我们的方法允许在一个文档中检测多个方面，从而可以理解通过多个方面表达的情感。

    Aspect-based sentiment analysis (ABSA) is a widely studied topic, most often trained through supervision from human annotations of opinionated texts. These fine-grained annotations include identifying aspects towards which a user expresses their sentiment, and their associated polarities (aspect-based sentiments). Such fine-grained annotations can be expensive and often infeasible to obtain in real-world settings. There is, however, an abundance of scenarios where user-generated text contains an overall sentiment, such as a rating of 1-5 in user reviews or user-generated feedback, which may be leveraged for this task. In this paper, we propose a VAE-based topic modeling approach that performs ABSA using document-level supervision and without requiring fine-grained labels for either aspects or sentiments. Our approach allows for the detection of multiple aspects in a document, thereby allowing for the possibility of reasoning about how sentiment expressed through multiple aspects comes 
    
[^49]: 大型语言模型稀疏微调的推理加速

    Sparse Finetuning for Inference Acceleration of Large Language Models. (arXiv:2310.06927v1 [cs.CL])

    [http://arxiv.org/abs/2310.06927](http://arxiv.org/abs/2310.06927)

    本论文研究了大型语言模型的准确稀疏微调问题，提出了基于L2范数的蒸馏方法SquareHead，可以在高稀疏性下实现准确的恢复；同时展示了稀疏语言模型的实际效率，可在CPU和GPU运行时实现加速，并且观察到在受内存限制的模型中，稀疏性也可用于减少内存带宽。

    

    我们考虑在训练过的大型语言模型上进行精确的稀疏微调，即在专门任务上对预训练的语言模型进行微调，同时在权重上引入稀疏性。在准确性方面，我们观察到基于损失的标准微调可能无法恢复准确性，特别是在高稀疏情况下。为了解决这个问题，我们对蒸馏类型的损失进行了详细研究，确定了一种基于L2范数的蒸馏方法，我们称之为SquareHead，即使在更高的稀疏性下，它也能实现准确的恢复，适用于所有模型类型。在实际效率方面，我们展示了稀疏语言模型可以通过利用稀疏性在CPU和GPU运行时实现加速。虽然标准方法是利用稀疏性进行计算减少，但我们观察到，在受内存限制的语言模型中，稀疏性也可以用于减少内存带宽。我们展示了由于稀疏性导致的速度提升以及恢复准确性的端到端结果，应用于T5 (语言翻译)任务中。

    We consider the problem of accurate sparse finetuning of large language models (LLMs), that is, finetuning pretrained LLMs on specialized tasks, while inducing sparsity in their weights. On the accuracy side, we observe that standard loss-based finetuning may fail to recover accuracy, especially at high sparsities. To address this, we perform a detailed study of distillation-type losses, determining an L2-based distillation approach we term SquareHead which enables accurate recovery even at higher sparsities, across all model types. On the practical efficiency side, we show that sparse LLMs can be executed with speedups by taking advantage of sparsity, for both CPU and GPU runtimes. While the standard approach is to leverage sparsity for computational reduction, we observe that in the case of memory-bound LLMs sparsity can also be leveraged for reducing memory bandwidth. We exhibit end-to-end results showing speedups due to sparsity, while recovering accuracy, on T5 (language translati
    
[^50]: 用聚焦-信息熵改进对比学习句子嵌入

    Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE. (arXiv:2310.06918v1 [cs.CL])

    [http://arxiv.org/abs/2310.06918](http://arxiv.org/abs/2310.06918)

    本研究提出了一种聚焦-信息熵函数，利用硬负样本挖掘改进了对比学习句子嵌入的方法，并在各种基准测试中验证了其性能提升。

    

    SimCSE的最新成功极大地推进了句子表示的最新技术。然而，SimCSE的原始表达没有充分利用对比学习中硬负样本的潜力。本研究引入了一种无监督对比学习框架，将SimCSE与硬负样本挖掘相结合，旨在提高句子嵌入的质量。所提出的聚焦-信息熵函数在对比目标中引入了自适应调节项，降低与易负样本相关的损失，并鼓励模型关注于困难负样本。在各种STS基准测试上的实验结果表明，我们的方法在斯皮尔曼相关系数、表示对齐性和一致性方面改进了句子嵌入。

    The recent success of SimCSE has greatly advanced state-of-the-art sentence representations. However, the original formulation of SimCSE does not fully exploit the potential of hard negative samples in contrastive learning. This study introduces an unsupervised contrastive learning framework that combines SimCSE with hard negative mining, aiming to enhance the quality of sentence embeddings. The proposed focal-InfoNCE function introduces self-paced modulation terms in the contrastive objective, downweighting the loss associated with easy negatives and encouraging the model focusing on hard negatives. Experimentation on various STS benchmarks shows that our method improves sentence embeddings in terms of Spearman's correlation and representation alignment and uniformity.
    
[^51]: 基于Transformer的神经文本表示技术在缺陷分配中的比较研究

    A Comparative Study of Transformer-based Neural Text Representation Techniques on Bug Triaging. (arXiv:2310.06913v1 [cs.SE])

    [http://arxiv.org/abs/2310.06913](http://arxiv.org/abs/2310.06913)

    本文研究了基于Transformer的神经文本表示技术在缺陷分配中的应用，相比之前的方法，这些新技术能更好地捕捉微妙的文本模式，提高自动化缺陷分配的性能。

    

    在管理缺陷报告时，通常第一步是将缺陷分配给最适合理解、定位和修复目标缺陷的开发人员。此外，将给定的缺陷分配给软件项目的特定部分可以加快修复过程。然而，尽管这些活动的重要性，但在手动分配的过程中可能需要花费几天的时间。过去的研究尝试利用有限的文本数据训练文本分类模型来自动化这个过程，但得到的成功程度参差不齐。然而，先前工作中使用的文本表示和机器学习模型受到其表达能力的限制，往往无法捕捉到可以帮助缺陷分配过程的微妙文本模式。最近，基于Transformer的大型预训练神经文本表示技术（如BERT）在几个自然语言处理任务中取得了更好的性能。

    Often, the first step in managing bug reports is related to triaging a bug to the appropriate developer who is best suited to understand, localize, and fix the target bug. Additionally, assigning a given bug to a particular part of a software project can help to expedite the fixing process. However, despite the importance of these activities, they are quite challenging, where days can be spent on the manual triaging process. Past studies have attempted to leverage the limited textual data of bug reports to train text classification models that automate this process -- to varying degrees of success. However, the textual representations and machine learning models used in prior work are limited by their expressiveness, often failing to capture nuanced textual patterns that might otherwise aid in the triaging process. Recently, large, transformer-based, pre-trained neural text representation techniques such as BERT have achieved greater performance in several natural language processing t
    
[^52]: 利用语言模型作为认知智能体的知识来源的研究

    Exploiting Language Models as a Source of Knowledge for Cognitive Agents. (arXiv:2310.06846v1 [cs.AI])

    [http://arxiv.org/abs/2310.06846](http://arxiv.org/abs/2310.06846)

    本研究利用语言模型作为认知智能体的任务知识来源，探索了将语言模型作为外部知识源用于认知系统的挑战和机会，并提出了通过整合知识提取和认知架构能力来提高知识提取效果的可能方法。

    

    大型语言模型(LLMs)不仅可以完成句子补全，还可以进行问答、摘要和自然语言推理等任务。我们的研究利用语言模型作为认知智能体的任务知识来源，即通过认知架构实现的智能体。我们识别了使用语言模型作为认知系统外部知识源的挑战和机会，以及通过将知识提取与认知架构能力整合来提高知识提取效果的可能方法，并举例介绍我们最近在这一领域的工作。

    Large language models (LLMs) provide capabilities far beyond sentence completion, including question answering, summarization, and natural-language inference. While many of these capabilities have potential application to cognitive systems, our research is exploiting language models as a source of task knowledge for cognitive agents, that is, agents realized via a cognitive architecture. We identify challenges and opportunities for using language models as an external knowledge source for cognitive systems and possible ways to improve the effectiveness of knowledge extraction by integrating extraction with cognitive architecture capabilities, highlighting with examples from our recent work in this area.
    
[^53]: Meta-CoT:大规模语言模型在混合任务场景中的通用思维链提示

    Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models. (arXiv:2310.06692v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.06692](http://arxiv.org/abs/2310.06692)

    Meta-CoT是一种在混合任务场景中能够通用思维链提示的方法，在十个公共基准推理任务中表现出卓越的性能和优越的泛化能力。

    

    大规模语言模型（LLM）通过利用链式思维提示展示出了卓越的推理能力，这种提示生成中间推理链以作为得出答案的基本理由。然而，目前的CoT方法要么仅仅使用类似“让我们逐步思考”的通用提示，要么过于依赖手工设计的任务特定演示来达到理想的性能，从而导致性能和泛化之间的不可避免的鸿沟。为了弥合这一鸿沟，我们提出了Meta-CoT，一种在未知输入问题类型的混合任务场景中具有通用性的CoT提示方法。Meta-CoT首先根据输入问题对场景进行分类，然后以自动模式从相应的数据池中构建多样的演示。Meta-CoT在十个公共基准推理任务上表现出卓越的性能和优越的泛化能力。值得注意的是，Meta-CoT实现了最新技术水平。

    Large language models (LLMs) have unveiled remarkable reasoning capabilities by exploiting chain-of-thought (CoT) prompting, which generates intermediate reasoning chains to serve as the rationale for deriving the answer. However, current CoT methods either simply employ general prompts such as Let's think step by step, or heavily rely on handcrafted task-specific demonstrations to attain preferable performances, thereby engendering an inescapable gap between performance and generalization. To bridge this gap, we propose Meta-CoT, a generalizable CoT prompting method in mixed-task scenarios where the type of input questions is unknown. Meta-CoT firstly categorizes the scenario based on the input question and subsequently constructs diverse demonstrations from the corresponding data pool in an automatic pattern. Meta-CoT simultaneously enjoys remarkable performances on ten public benchmark reasoning tasks and superior generalization capabilities. Notably, Meta-CoT achieves the state-of-
    
[^54]: 用多样化反馈构建大型语言模型的对齐方法

    Constructive Large Language Models Alignment with Diverse Feedback. (arXiv:2310.06450v1 [cs.CL])

    [http://arxiv.org/abs/2310.06450](http://arxiv.org/abs/2310.06450)

    本文提出了一种新的方法，即建构性和多样化反馈（CDF），用于增强大型语言模型（LLM）的对齐效果。我们通过收集不同类型的反馈，并根据问题的难度级别进行处理，实现了更好的性能。

    

    在大型语言模型（LLMs）的研究中，对其与人类价值观的对齐越来越重视，以减少有害内容的影响。然而，当前的对齐方法通常仅依赖于人类反馈的单一形式，如偏好、注释标签或自然语言批评，忽视了结合这些反馈类型的潜在优势。这种限制导致性能不佳，即使有丰富的训练数据。本文引入了建构性和多样化反馈（CDF）作为增强LLM对齐的新方法，受建构学习理论的启发。我们的方法涉及收集适用于训练数据集中不同难度问题的三种不同类型的反馈。具体而言，我们利用批评反馈解决简单问题，利用改进反馈解决中等问题，利用偏好反馈解决困难问题。通过用这种多样化反馈训练我们的模型，我们获得了更好的表现。

    In recent research on large language models (LLMs), there has been a growing emphasis on aligning these models with human values to reduce the impact of harmful content. However, current alignment methods often rely solely on singular forms of human feedback, such as preferences, annotated labels, or natural language critiques, overlooking the potential advantages of combining these feedback types. This limitation leads to suboptimal performance, even when ample training data is available. In this paper, we introduce Constructive and Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired by constructivist learning theory. Our approach involves collecting three distinct types of feedback tailored to problems of varying difficulty levels within the training dataset. Specifically, we exploit critique feedback for easy problems, refinement feedback for medium problems, and preference feedback for hard problems. By training our model with this diversified feedback, we a
    
[^55]: 自动神经元解释的及时调优的重要性

    The Importance of Prompt Tuning for Automated Neuron Explanations. (arXiv:2310.06200v1 [cs.CL])

    [http://arxiv.org/abs/2310.06200](http://arxiv.org/abs/2310.06200)

    本文探讨了在自动神经元解释中即时调优的重要性，通过重新格式化解释提示，我们显著提高了解释质量并降低了计算成本。这项研究对于更深入理解大型语言模型的工作原理和安全性具有重要意义。

    

    最近的进展极大地提升了大型语言模型(LLM)的能力，但我们对这些模型及其安全性的理解并没有同步进展。在本文中，我们旨在通过研究它们的个体神经元来更深入地理解LLM。我们在前人研究的基础上，进一步探讨了大型语言模型，如GPT-4，如何解释语言模型中每个神经元的功能。具体地，我们分析了生成解释所使用的提示的效果，并展示了以更自然的方式重新格式化解释提示如何显著提高神经元解释的质量，并大幅降低计算成本。我们通过三种不同的方式演示了我们新提示的效果，包括自动化评估和人工评估。

    Recent advances have greatly increased the capabilities of large language models (LLMs), but our understanding of the models and their safety has not progressed as fast. In this paper we aim to understand LLMs deeper by studying their individual neurons. We build upon previous work showing large language models such as GPT-4 can be useful in explaining what each neuron in a language model does. Specifically, we analyze the effect of the prompt used to generate explanations and show that reformatting the explanation prompt in a more natural way can significantly improve neuron explanation quality and greatly reduce computational cost. We demonstrate the effects of our new prompts in three different ways, incorporating both automated and human evaluations.
    
[^56]: 大型语言模型是事后解释器吗？

    Are Large Language Models Post Hoc Explainers?. (arXiv:2310.05797v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05797](http://arxiv.org/abs/2310.05797)

    这项工作提出了第一个研究大型语言模型（LLMs）解释其他预测模型有效性的框架，并且提出了多个提示策略，填补了当前对于LLMs在解释其他模型行为方面的缺失。

    

    大型语言模型（LLM）越来越被广泛应用于各种自然语言处理（NLP）应用中。最近的一项创新，即上下文学习（ICL），使得LLM能够在推理阶段通过在提示中提供少量示例来学习新任务，从而消除了模型微调的需要。虽然LLM已经被应用于多个领域，但其在解释其他模型行为方面的适用性仍相对未被探索。尽管存在越来越多的新解释技术，但很多技术要求对模型具有白盒访问权限和/或计算成本较高，凸显了下一代事后解释器的需求。在这项工作中，我们提出了第一个研究LLM解释其他预测模型有效性的框架。具体而言，我们提出了一个包含多种提示策略的新颖框架：i）基于扰动的ICL，ii）基于预测的ICL，iii）基于指令的ICL，和iv）基于解释的ICL。

    Large Language Models (LLMs) are increasingly used as powerful tools for a plethora of natural language processing (NLP) applications. A recent innovation, in-context learning (ICL), enables LLMs to learn new tasks by supplying a few examples in the prompt during inference time, thereby eliminating the need for model fine-tuning. While LLMs have been utilized in several applications, their applicability in explaining the behavior of other models remains relatively unexplored. Despite the growing number of new explanation techniques, many require white-box access to the model and/or are computationally expensive, highlighting a need for next-generation post hoc explainers. In this work, we present the first framework to study the effectiveness of LLMs in explaining other predictive models. More specifically, we propose a novel framework encompassing multiple prompting strategies: i) Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL, and iv) Explanation-based I
    
[^57]: 个性化随机鹦鹉更危险吗？评估对话系统中的人格偏见

    Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05280](http://arxiv.org/abs/2310.05280)

    这项研究评估了对话系统中的人格偏见对社交偏见的影响，并建立了一个综合评估框架来衡量不同人格采用下的偏见程度。

    

    最近大型语言模型的发展使其能够按照自由形式的指令进行操作，包括在对话中模仿通用或特定人口群体的人格。通用人格指的是来自某一人口群体的个体（例如亚洲人），而特定人格可以是历史人物的实际姓名。虽然采用人格使对话系统更具吸引力和亲和力，但也存在潜在风险，可能通过与用户的交互而加剧社会偏见，进一步造成社会伤害。在本文中，我们系统地研究“人格偏见”，我们将其定义为有害对话模型行为对不同人格采用的敏感性。我们将人格偏见分为有害表达和有害认同两类，同时建立了一个全面的评估框架，以衡量五个方面的人格偏见：冒犯性、有毒延续、关怀、刻板印象的认同以及

    Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. Generic personas refer to an individual from a demographic group (e.g. an Asian person), whereas specific personas can be actual names of historical figures. While the adoption of personas allows dialogue systems to be more engaging and approachable to users, it also carries the potential risk of exacerbating social biases in model responses, further causing societal harms through interactions with users. In this paper, we systematically study "persona biases", which we define to be the sensitivity of harmful dialogue model behaviors to different persona adoptions. We categorize persona biases into biases in harmful expression and harmful agreement, as well as establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and To
    
[^58]: BRAINTEASER：大型语言模型的横向思维难题

    BRAINTEASER: Lateral Thinking Puzzles for Large Language Models. (arXiv:2310.05057v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05057](http://arxiv.org/abs/2310.05057)

    本文介绍了一项名为BRAINTEASER的多项选择问题回答任务，旨在测试大型语言模型表现出横向思维和违反默认常识联系的能力。通过创建一个横向思维基准，丰富问题的语义和上下文重建，实验证明模型与人类之间存在显著差距。

    

    语言模型的成功激励了自然语言处理社区关注需要隐含和复杂推理的任务，依赖于类人的常识机制。虽然这些垂直思维任务相对较受欢迎，但横向思维难题却鲜有关注。为了弥合这一差距，我们设计了BRAINTEASER：一个多项选择问题回答任务，旨在测试模型表现出横向思维和违反默认常识联系的能力。我们设计了一个三步骤的程序来创建第一个横向思维基准，包括数据收集、干扰项生成和对抗性样本生成，共有1,100个具有高质量注释的难题。为了评估模型的横向推理一致性，我们基于问题的语义和上下文重建来丰富BRAINTEASER。我们对最先进的指导性和常识语言模型进行的实验揭示了人类和模型之间的显著差距。

    The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BRAINTEASER: a multiple-choice Question Answering task designed to test the model's ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of adversarial examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BRAINTEASER based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model
    
[^59]: 重新审视大型语言模型作为零-shot关系抽取器

    Revisiting Large Language Models as Zero-shot Relation Extractors. (arXiv:2310.05028v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.05028](http://arxiv.org/abs/2310.05028)

    本研究重新审视了大型语言模型(LLMs)作为零-shot关系抽取器的潜力，并提出了通过总结和提问(\textsc{SumAsk})提示方法来改进零-shot关系抽取。实验证明LLMs在这一任务上具有良好的表现。

    

    关系抽取(RE)即使在零-shot设定下，一直涉及一定程度的标记或未标记的数据。最近的研究表明，大型语言模型(LLMs)能够在给定自然语言提示的情况下，无需任何数据和参数调整，自动适应新任务，这为从文本中提取关系提供了可能性。本研究主要关注将LLMs，如ChatGPT，作为零-shot关系抽取器的研究。一方面，我们分析了现有RE提示的缺点，并尝试将最近的提示技术，如CoT，纳入其中以提高零-shot关系抽取。我们提出了总结和提问(\textsc{SumAsk})提示，这是一种简单的提示，通过递归使用LLMs将RE输入转换为有效的问答(QA)格式。另一方面，我们对各种基准和设置进行了全面的实验，以调查LLMs在零-shot关系抽取上的能力。具体而言，我们有以下的followi

    Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. This work focuses on the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors. On the one hand, we analyze the drawbacks of existing RE prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (CoT) to improve zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a simple prompt recursively using LLMs to transform RE inputs to the effective question answering (QA) format. On the other hand, we conduct comprehensive experiments on various benchmarks and settings to investigate the capabilities of LLMs on zero-shot RE. Specifically, we have the followi
    
[^60]: Hermes：通过从自然语言规范合成有限状态机来解锁移动网络协议的安全分析

    Hermes: Unlocking Security Analysis of Cellular Network Protocols by Synthesizing Finite State Machines from Natural Language Specifications. (arXiv:2310.04381v1 [cs.CR])

    [http://arxiv.org/abs/2310.04381](http://arxiv.org/abs/2310.04381)

    Hermes是一个自动生成有限状态机的端到端框架，用于解锁移动网络协议的安全分析。通过处理自然语言规范并生成逻辑公式，Hermes能够发现新的漏洞和攻击，并对现有规范和商业基带进行评估。

    

    本文介绍了Hermes，一个自动生成自然语言移动规范的形式表达的端到端框架。我们首先开发了神经组成分析器NEUTREX，用于处理与转换相关的文本并提取转换组件（即状态、条件和动作）。我们还设计了一种领域特定语言，通过利用依存解析树将这些转换组件转化成逻辑公式。最后，我们将这些逻辑公式编译成转换和创建形式模型作为有限状态机。为了证明Hermes的有效性，我们在4G NAS、5G NAS和5G RRC规范上进行评估，并获得了81-87%的总体准确率，这是对现有技术的显著改进。我们对提取的模型进行的安全分析揭示出了3个新的漏洞、发现了19个之前的攻击4G和5G规范，以及7个商业4G基带的偏差。

    In this paper, we present Hermes, an end-to-end framework to automatically generate formal representations from natural language cellular specifications. We first develop a neural constituency parser, NEUTREX, to process transition-relevant texts and extract transition components (i.e., states, conditions, and actions). We also design a domain-specific language to translate these transition components to logical formulas by leveraging dependency parse trees. Finally, we compile these logical formulas to generate transitions and create the formal model as finite state machines. To demonstrate the effectiveness of Hermes, we evaluate it on 4G NAS, 5G NAS, and 5G RRC specifications and obtain an overall accuracy of 81-87%, which is a substantial improvement over the state-of-the-art. Our security analysis of the extracted models uncovers 3 new vulnerabilities and identifies 19 previous attacks in 4G and 5G specifications, and 7 deviations in commercial 4G basebands.
    
[^61]: LMM的黎明：与GPT-4V(ision)的初步探索

    The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision). (arXiv:2309.17421v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.17421](http://arxiv.org/abs/2309.17421)

    本文对最新的GPT-4V(ision)模型进行了分析，发现其具有处理多模态输入、高通用性和理解带有视觉标记的能力，使其成为强大的多模态通用系统。

    

    大型多模态模型（LMMs）通过视觉理解等多感知能力来扩展大型语言模型（LLMs），以实现更强大的通用智能。本文分析了最新的模型GPT-4V(ision)，以深化对LMMs的理解。分析重点是GPT-4V能够执行的引人注目任务，包括测试样本来探测GPT-4V能力的质量和通用性，其支持的输入和工作模式，以及激发模型的有效方式。在探索GPT-4V的方法中，我们精心设计了一系列涵盖各种领域和任务的定性样本。这些样本的观察结果表明，GPT-4V在处理任意交错的多模态输入和其能力的通用性方面具有前所未有的能力，使其成为强大的多模态全能系统。此外，GPT-4V独特的能力可以理解绘制在输入图像上的视觉标记。

    Large multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as visual understanding, to achieve stronger generic intelligence. In this paper, we analyze the latest model, GPT-4V(ision), to deepen the understanding of LMMs. The analysis focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V's capabilities, its supported inputs and working modes, and the effective ways to prompt the model. In our approach to exploring GPT-4V, we curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demonstrate that GPT-4V's unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V's unique capability of understanding visual markers drawn on input images can give ri
    
[^62]: InstructionGPT-4: 一个200指令范式用于微调MiniGPT-4

    InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4. (arXiv:2308.12067v1 [cs.LG])

    [http://arxiv.org/abs/2308.12067](http://arxiv.org/abs/2308.12067)

    InstructionGPT-4通过仅使用200个例子进行微调，在多模式指令数据质量度量和选择器的帮助下，在各种评估任务中优于原始的MiniGPT-4。

    

    多模式大型语言模型通过两阶段的训练过程获取其遵循指令的能力：在图像-文本对上进行预训练，然后在监督式视觉-语言指令数据上进行微调。最近的研究表明，即使只有有限量的高质量遵循指令数据，大型语言模型也能取得令人满意的结果。在本文中，我们介绍了InstructionGPT-4，它经过微调的数据集只包含200个例子，约占MiniGPT-4对齐数据集中使用的遵循指令数据的6%。我们首先提出了几个用于评估多模式指令数据质量的度量指标。基于这些度量指标，我们提出了一个简单而有效的数据选择器，自动识别和过滤低质量的视觉-语言数据。通过采用这种方法，InstructionGPT-4在各种评估（如视觉问答、GPT-4偏好）上优于原始的MiniGPT-4。总体而言，我们的研究发现...

    Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findi
    
[^63]: CHATREPORT：通过基于LLM工具实现可持续性披露分析的民主化

    CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools. (arXiv:2307.15770v1 [cs.CL])

    [http://arxiv.org/abs/2307.15770](http://arxiv.org/abs/2307.15770)

    本论文介绍了一种名为ChatReport的基于LLM的系统，它通过实现可追溯的答案和解决领域专家参与低效性的问题，旨在通过自动分析企业可持续性报告，实现可持续性披露分析民主化。

    

    面对气候变化，公司真的在朝着更可持续经营迈出实质性的步伐吗？一个全面的答案可以在企业可持续性报告的密集信息中找到。然而，这些报告的数量和复杂性使人工分析成本非常高昂。因此，只有少数的机构拥有资源能够大规模分析这些报告，这导致可持续性报告缺乏透明度。通过基于LLM自动分析工具赋能利益相关者可能是实现可持续性报告分析民主化的一种有希望的方式。然而，开发这样的工具面临挑战，主要原因是LLM的幻觉问题和将领域专家引入AI开发过程的低效性。在本文中，我们介绍了ChatReport，这是一种基于LLM的新型系统，用于自动化分析企业可持续性报告，通过使答案可追溯来减少幻觉的危害，并解决领域专家参与AI开发过程的低效性。

    In the face of climate change, are companies really taking substantial steps toward more sustainable operations? A comprehensive answer lies in the dense, information-rich landscape of corporate sustainability reports. However, the sheer volume and complexity of these reports make human analysis very costly. Therefore, only a few entities worldwide have the resources to analyze these reports at scale, which leads to a lack of transparency in sustainability reporting. Empowering stakeholders with LLM-based automatic analysis tools can be a promising way to democratize sustainability report analysis. However, developing such tools is challenging due to (1) the hallucination of LLMs and (2) the inefficiency of bringing domain experts into the AI development loop. In this paper, we ChatReport, a novel LLM-based system to automate the analysis of corporate sustainability reports, addressing existing challenges by (1) making the answers traceable to reduce the harm of hallucination and (2) a
    
[^64]: 插入并祈祷：利用多模型模型的现成组件进行攻击

    Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models. (arXiv:2307.14539v1 [cs.CR])

    [http://arxiv.org/abs/2307.14539](http://arxiv.org/abs/2307.14539)

    本文通过引入对抗性嵌入空间攻击，探讨了将现成组件插入多模型模型中的漏洞和风险，并提出了一种不需要多模型系统权重和参数的攻击方法，通过寻找位于预训练组件嵌入空间危险区域的输入图像进行攻击。

    

    将额外的模态（如视觉）加入大型语言模型（LLM）的快速增长和日益受欢迎引起了严重的安全问题。这种模态的扩展类似于在房子上增加更多的门，无意中为对抗性攻击创建了多个访问点。在本文中，通过引入对抗性嵌入空间攻击，我们强调多模型系统中的漏洞，这些漏洞源于以插拔方式将现成组件（如公共预训练编码器）引入这些系统。与现有的工作相比，我们的方法不需要访问多模型系统的权重或参数，而是依赖于这些预训练编码器的庞大且未完全开发的嵌入空间。我们提出的嵌入空间攻击是通过寻找位于这些预训练组件的广泛嵌入空间的危险或目标区域的输入图像来进行的。

    The rapid growth and increasing popularity of incorporating additional modalities (e.g., vision) into large language models (LLMs) has raised significant security concerns. This expansion of modality, akin to adding more doors to a house, unintentionally creates multiple access points for adversarial attacks. In this paper, by introducing adversarial embedding space attacks, we emphasize the vulnerabilities present in multi-modal systems that originate from incorporating off-the-shelf components like public pre-trained encoders in a plug-and-play manner into these systems. In contrast to existing work, our approach does not require access to the multi-modal system's weights or parameters but instead relies on the huge under-explored embedding space of such pre-trained encoders. Our proposed embedding space attacks involve seeking input images that reside within the dangerous or targeted regions of the extensive embedding space of these pre-trained components. These crafted adversarial 
    
[^65]: VerifAI：验证生成式人工智能

    VerifAI: Verified Generative AI. (arXiv:2307.02796v1 [cs.DB])

    [http://arxiv.org/abs/2307.02796](http://arxiv.org/abs/2307.02796)

    验证生成式人工智能的输出是一个新兴问题，我们提出了通过分析多模态数据湖的底层数据，评估其质量和一致性，来建立评估生成式人工智能模型输出的更坚实基础，并解决错误信息传播的挑战。

    

    生成式人工智能已经取得了重要的进展，但是对于其输出的准确性和可靠性的担忧仍在增长。这种不准确性可能产生严重后果，如错误决策，传播虚假信息，侵犯隐私，法律责任等。虽然已经在进行应对这些风险的努力，包括可解释的人工智能和负责任的人工智能实践，如透明度，隐私保护，偏见缓解以及社会和环境责任等，但由生成式人工智能引起的错误信息仍然是一个重大挑战。我们提出，从数据管理的角度验证生成式人工智能的输出是生成式人工智能的一个新兴问题。这包括分析来自多模态数据湖的底层数据，包括文本文件，表格和知识图谱，并评估其质量和一致性。通过这样做，我们可以为评估生成式人工智能模型的输出奠定更坚实的基础。这种方法能够帮助解决生成式人工智能的输出验证问题。

    Generative AI has made significant strides, yet concerns about the accuracy and reliability of its outputs continue to grow. Such inaccuracies can have serious consequences such as inaccurate decision-making, the spread of false information, privacy violations, legal liabilities, and more. Although efforts to address these risks are underway, including explainable AI and responsible AI practices such as transparency, privacy protection, bias mitigation, and social and environmental responsibility, misinformation caused by generative AI will remain a significant challenge. We propose that verifying the outputs of generative AI from a data management perspective is an emerging issue for generative AI. This involves analyzing the underlying data from multi-modal data lakes, including text files, tables, and knowledge graphs, and assessing its quality and consistency. By doing so, we can establish a stronger foundation for evaluating the outputs of generative AI models. Such an approach ca
    
[^66]: CamChoice：一份包含多项选择题和候选答案分布的语料库

    CamChoice: A Corpus of Multiple Choice Questions and Candidate Response Distributions. (arXiv:2306.13047v1 [cs.CL])

    [http://arxiv.org/abs/2306.13047](http://arxiv.org/abs/2306.13047)

    本文介绍了CamChoice数据集，该数据集包含多项选择理解问题和真实候选答案选项分布，为候选人分布匹配任务提供了自动评估方式。

    

    多项选择题是用于衡量候选人在各种领域和任务中能力的普遍评估形式。提出的问题的质量对于测试设计人员非常重要，因此新提出的问题在部署到实际考试之前需要经过几个预测试评估阶段。目前，这个过程是相当手动化的，这可能导致问题开发周期的时间滞后。自动化此过程将大大提高效率，然而目前的数据集不包含足够的预测试分析信息。在本文中，我们介绍了CamChoice：一份包含不同目标级别问题和真实候选答案选项分布的多项选择理解数据集。我们引入了候选人分布匹配任务，提出了几种评估指标，并证明了在RACE++上训练的自动系统可以实现该任务。

    Multiple Choice examinations are a ubiquitous form of assessment that is used to measure the ability of candidates across various domains and tasks. Maintaining the quality of proposed questions is of great importance to test designers, and therefore newly proposed questions go through several pre-test evaluation stages before they can be deployed into real-world exams. This process is currently quite manual, which can lead to time lags in the question development cycle. Automating this process would lead to a large improvement in efficiency, however, current datasets do not contain sufficient pre-test analysis information. In this paper, we introduce CamChoice; a multiple-choice comprehension dataset with questions at different target levels, where questions have the true candidate selected options distributions. We introduce the task of candidate distribution matching, propose several evaluation metrics for the task, and demonstrate that automatic systems trained on RACE++ can be lev
    
[^67]: VisoGender：一份用于评估图像-文本代词解析中性别偏见的数据集

    VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution. (arXiv:2306.12424v1 [cs.CV])

    [http://arxiv.org/abs/2306.12424](http://arxiv.org/abs/2306.12424)

    VisoGender是一个用于评估视觉语言模型中职业相关性别偏见的数据集。研究显示，最先进的模型缺乏正确解析复杂场景中性别的推理能力，生成字幕的模型通常比类似CLIP的模型更精确和更少偏见。

    

    我们介绍了一个新的数据集VisoGender，用于评估视觉语言模型中的性别偏见。我们专注于职业相关的性别偏见，受Winograd和Winogender模式的启发，其中每个图像都与包含场景中主语和宾语代词关系的标题相关联。VisoGender在职业角色中平衡了性别代表，支持两种偏见评估方式：i）解决偏见，我们评估男性和女性解决准确性之间的差异；ii）检索偏见，我们比较在性别中立的搜索查询中检索到的男性和女性专业人员的比例。我们对几种最先进的视觉语言模型进行了基准测试，并发现它们缺乏正确解析复杂场景中性别的推理能力。虽然性别偏见的方向和幅度取决于任务和评估的模型，但生成字幕的模型通常比类似CLIP的模型更精确和更少偏见。

    We introduce VisoGender, a novel dataset for benchmarking gender bias in vision-language models. We focus on occupation-related gender biases, inspired by Winograd and Winogender schemas, where each image is associated with a caption containing a pronoun relationship of subjects and objects in the scene. VisoGender is balanced by gender representation in professional roles, supporting bias evaluation in two ways: i) resolution bias, where we evaluate the difference between gender resolution accuracies for men and women and ii) retrieval bias, where we compare ratios of male and female professionals retrieved for a gender-neutral search query. We benchmark several state-of-the-art vision-language models and find that they lack the reasoning abilities to correctly resolve gender in complex scenes. While the direction and magnitude of gender bias depends on the task and the model being evaluated, captioning models generally are more accurate and less biased than CLIP-like models. Dataset 
    
[^68]: 从零开始实现红队对抗语言模型的探索与建立

    Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])

    [http://arxiv.org/abs/2306.09442](http://arxiv.org/abs/2306.09442)

    本文提出了一种新的红队行动，通过从高层次、抽象的规范出发来考虑语言模型的行为，以探究模型的创新和贡献。

    

    部署大型语言模型（LLMs）可能会产生有害输出，例如有毒或不诚实陈述。先前的研究已经引入了工具以调查有害输出，以识别和减轻这些风险。虽然这是确保语言模型安全的有价值步骤，但这些方法通常依赖于现有的针对不希望的输出的分类器。这限制了它们在只有预先知道有害行为类型的情况下的应用。然而，这跳过了红队行动的核心挑战：开发模型可能展示的行为的上下文理解。此外，当这样的分类器已经存在时，红队行动的边际价值有限，因为分类器可以用于过滤训练数据或模型输出。本文考虑在假设对手从高级、抽象的不良行为规范出发的情况下进行红队行动。红队应该在精化/扩展此规范的同时对抗该模型。

    Deploying Large language models (LLMs) can pose hazards from harmful outputs such as toxic or dishonest speech. Prior work has introduced tools that elicit harmful outputs in order to identify and mitigate these risks. While this is a valuable step toward securing language models, these approaches typically rely on a pre-existing classifier for undesired outputs. This limits their application to situations where the type of harmful behavior is known with precision beforehand. However, this skips a central challenge of red teaming: developing a contextual understanding of the behaviors that a model can exhibit. Furthermore, when such a classifier already exists, red teaming has limited marginal value because the classifier could simply be used to filter training data or model outputs. In this work, we consider red teaming under the assumption that the adversary is working from a high-level, abstract specification of undesired behavior. The red team is expected to refine/extend this spec
    
[^69]: 分段循环Transformer:一种高效的序列到序列模型

    Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v1 [cs.CL])

    [http://arxiv.org/abs/2305.16340](http://arxiv.org/abs/2305.16340)

    本文提出了一种分段循环Transformer（SRformer）来减少计算/内存成本，并使用RAF层处理跨段的信息，从而提高序列处理能力。

    

    Transformer在许多领域中表现出卓越的性能，包括语言和视觉。然而，随着序列长度的增加，它们的计算成本呈二次增长，使得它们在资源受限的应用中使用成为不可能。为了解决这个问题，我们的方法是将整个序列划分成若干段。然后使用具有循环结构的神经元来聚合跨段的信息，从而实现具有较低计算/内存成本的序列处理能力模型。为了验证这个想法，我们首先研究了使用局部Attention机制对单个段的影响。然后我们提出了一种分段循环Transformer（SRformer），它将分段Attention和循环Attention相结合。它使用循环accumulate and fire（RAF）层在相邻段之间处理信息。通过更新key的产品来补偿减少Attention窗口长度产生的误差。

    Transformers have shown dominant performance across a range of domains including language and vision. However, their computational cost grows quadratically with the sequence length, making their usage prohibitive for resource-constrained applications. To counter this, our approach is to divide the whole sequence into segments. The information across segments can then be aggregated using neurons with recurrence leveraging their inherent memory. Such an approach leads to models with sequential processing capability at a lower computation/memory cost. To investigate this idea, first, we examine the effects of using local attention mechanism on the individual segments. Then we propose a segmented recurrent transformer (SRformer) that combines segmented attention with recurrent attention. It uses recurrent accumulate and fire (RAF) layers to process information between consecutive segments. The loss caused by reducing the attention window length is compensated by updating the product of key
    
[^70]: LLM们进步到了什么程度？一个挑战性的问题解决基准对大型语言模型

    Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models. (arXiv:2305.15074v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15074](http://arxiv.org/abs/2305.15074)

    这项研究提出了JEEBench，一个更具挑战性的基准数据集，用于评估大型语言模型的问题解决能力。通过评估各种模型，结果显示目前最好的模型在解决问题时存在代数操作错误、抽象概念转化不准确和难以检索相关概念等问题。

    

    在过去的几年里，大型语言模型（LLMs）在现有的推理基准上的性能显著提高。为此，我们提出了JEEBench，一个更具挑战性的基准数据集，用于评估LLMs的问题解决能力。我们从高竞争的印度理工学院（IIT）JEE-Advanced考试中精选出了515个具有挑战性的预工程数学、物理和化学问题。在这个基准中，长期推理和深入领域知识的运用对问题的解决至关重要。我们对各种开源和专有模型进行了评估，结果显示，即使使用了自一致性、自我完善和思维链提示等技术，最高性能也不到40\%。最好的模型GPT-4的典型失败模式包括代数操作错误、将抽象概念准确地转化为数学方程以及无法检索相关的领域特定概念。我们还观察到，仅仅通过输入提示不能让模型成功解决问题。

    The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40\%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompti
    
[^71]: UniChart：面向图表理解和推理的通用视觉语言预训练模型

    UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning. (arXiv:2305.14761v1 [cs.CL])

    [http://arxiv.org/abs/2305.14761](http://arxiv.org/abs/2305.14761)

    本文提出了UniChart，这是一个用于图表理解和推理的通用视觉语言预训练模型。UniChart首先建立了一个包括各种不同主题和视觉风格的大量图表语料库，然后使用图表特定的预训练任务来编码图表，并在几个图表理解和推理任务上表现出色。

    

    图表在数据分析、可视化重要见解和回答数据的复杂推理问题方面非常流行。为了方便使用自然语言进行基于图表的数据分析，最近引入了几个下游任务，例如图表问答和图表总结。然而，大多数解决这些任务的方法都使用语言或视觉-语言任务的预训练，而不试图明确建模图表的结构（例如，如何视觉编码数据以及如何将图表元素相互关联）。为了解决这个问题，我们首先建立了一个包括各种不同主题和视觉风格的大量图表语料库。然后，我们提出了UniChart，这是一个用于图表理解和推理的预训练模型。UniChart对图表的相关文本、数据和视觉元素进行编码，然后使用基于图表的文本解码器以自然语言生成预期的输出。我们提出了几个面向图表的预训练任务，包括：（i）低层次视觉编码预测，（ii）图表元素关系预测和（iii）图表问题回答预测。我们的评估显示，UniChart在几个图表理解和推理任务上表现优于强基线。

    Charts are very popular for analyzing data, visualizing key insights and answering complex reasoning questions about data. To facilitate chart-based data analysis using natural language, several downstream tasks have been introduced recently such as chart question answering and chart summarization. However, most of the methods that solve these tasks use pretraining on language or vision-language tasks that do not attempt to explicitly model the structure of the charts (e.g., how data is visually encoded and how chart elements are related to each other). To address this, we first build a large corpus of charts covering a wide variety of topics and visual styles. We then present UniChart, a pretrained model for chart comprehension and reasoning. UniChart encodes the relevant text, data, and visual elements of charts and then uses a chart-grounded text decoder to generate the expected output in natural language. We propose several chart-specific pretraining tasks that include: (i) low-lev
    
[^72]: 利用基于优势的离线策略梯度改进语言模型

    Improving Language Models with Advantage-based Offline Policy Gradients. (arXiv:2305.14718v1 [cs.CL])

    [http://arxiv.org/abs/2305.14718](http://arxiv.org/abs/2305.14718)

    本文介绍了一种简单的训练算法Left-over Lunch RL （LoL-RL），使用离线策略梯度学习任何序列到序列数据，从而实现优化LM效用的方法。

    

    根据用户定义的质量或风格限制提高语言模型生成是具有挑战性的。典型的方法包括学习额外的人工编写数据，使用启发式方法过滤“低质量”数据和/或使用强化学习与人体反馈（RLHF）。然而，过滤会删除有价值的训练信号，而数据收集和RLHF不断需要额外的人工编写或LM探索数据，这可能成本高。一个自然的问题是“我们可以利用RL来优化现有的众包和互联网数据上的LM效用吗？”为此，我们提出了剩余午餐强化学习（LoL-RL），这是一种简单的训练算法，使用离线策略梯度来学习语言生成任务作为1步RL游戏。 LoL-RL可以微调LM，以优化任意基于分类器或人定义的效用函数的任何序列到序列数据。使用不同大小模型的五个不同语言生成任务的实验

    Improving language model generations according to some user-defined quality or style constraints is challenging. Typical approaches include learning on additional human-written data, filtering ``low-quality'' data using heuristics and/or using reinforcement learning with human feedback (RLHF). However, filtering can remove valuable training signals, whereas data collection and RLHF constantly require additional human-written or LM exploration data which can be costly to obtain. A natural question to ask is ``Can we leverage RL to optimize LM utility on existing crowd-sourced and internet data?''  To this end, we present Left-over Lunch RL (LoL-RL), a simple training algorithm that uses offline policy gradients for learning language generation tasks as a 1-step RL game. LoL-RL can finetune LMs to optimize arbitrary classifier-based or human-defined utility functions on any sequence-to-sequence data. Experiments with five different language generation tasks using models of varying sizes 
    
[^73]: FActScore: 对长文本生成中事实准确性的细粒度原子评估

    FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. (arXiv:2305.14251v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14251](http://arxiv.org/abs/2305.14251)

    本文介绍了一种称为FACTSCORE的评估方法，它通过将生成的内容分解为原子事实，并计算被可靠知识源支持的比例来评估大型语言模型生成的长文本的事实准确性。通过广泛的人工评估，我们发现商业语言模型中仅有58%的ChatGPT传记达到了高水平的事实准确性。此外，我们还引入了一种自动化模型，利用检索和强语言模型估计FACTSCORE，误差率低于2%。

    

    评估大型语言模型生成的长文本的事实性是一项棘手的任务，因为（1）生成的内容通常包含支持和不支持的信息，使得二元判断质量不足，（2）人工评估耗时且成本高。本文介绍了FACTSCORE，一种新的评估方法，它将生成内容分解为一系列原子事实，并计算被可靠知识源支持的原子事实的百分比。我们进行了广泛的人工评估，得出了几个最先进商业语言模型（InstructGPT、ChatGPT和增强提取PerplexityAI）生成的人物传记的FACTSCORE，并报道了新的分析结果，证明了对于这样的细粒度评分的需求（例如，ChatGPT只达到58%）。由于人工评估费时费力，我们还引入了一种使用检索和强语言模型估计FACTSCORE的自动化模型，误差率不超过2%。

    Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, w
    
[^74]: 编辑大型语言模型：问题、方法和机会

    Editing Large Language Models: Problems, Methods, and Opportunities. (arXiv:2305.13172v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13172](http://arxiv.org/abs/2305.13172)

    本文深入探讨了编辑大型语言模型的问题、方法和机会，提供了任务定义和挑战的概述、先进方法的实证分析，以及构建了新的基准数据集。这些结果有助于改进LLMs的编辑技术，提高其效果和可行性。

    

    尽管能够训练出表现优秀的大型语言模型（LLMs），但其保持相关性和纠正错误的方法仍然难以确定。为此，最近几年出现了许多编辑LLMs的技术，其目标是在特定领域内高效地改变LLMs的行为，同时不对其他输入的性能产生负面影响。本文深入探讨了与LLMs模型编辑相关的问题、方法和机会。特别是，我们提供了关于模型编辑任务定义和相关挑战的全面概述，以及对目前最先进的方法的深入实证分析。我们还构建了一个新的基准数据集，以促进更强大的评估，并指出现有技术固有的持久问题。我们的目标是为每种编辑技术的效果和可行性提供有价值的见解，从而帮助社区在LLMs的管理中取得更好的结果。

    Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in ma
    
[^75]: 基于预训练模型的等变小样本学习

    Equivariant Few-Shot Learning from Pretrained Models. (arXiv:2305.09900v1 [cs.LG])

    [http://arxiv.org/abs/2305.09900](http://arxiv.org/abs/2305.09900)

    本文提出了一种基于预训练模型的$\lambda$-\textit{equitune}方法，它使用\textit{重要性权重}$\lambda$对特征进行平均，可以显著提高等变小样本学习的表现。

    

    高效的迁移学习算法是基础模型在有限数据情况下在各种下游任务上取得成功的关键。最近的作品 \cite{basu2022equi} 和 \cite{kaba2022equivariance} 分别提出了使用从群变换输入得到的特征的群平均值（\textit{equitune}）和基于优化的方法来从不等变的神经网络获取等变输出。虽然 \cite{kaba2022equivariance} 只关注从头开始训练，但我们发现即使在良好的微调结果下，\textit{equitune} 在等变零样本任务上表现不佳。我们认为这是因为预训练模型为某些转换提供了更高质量的特征，而对其进行简单平均会产生不良影响。因此，我们提出了一种使用\textit{重要性权重}$\lambda$对特征进行平均的$\lambda$-\textit{equitune} 方法。这些权重是使用一个小型神经网络直接从数据中学习的，从而导致出色的零样本和微调结果。

    Efficient transfer learning algorithms are key to the success of foundation models on diverse downstream tasks even with limited data. Recent works of \cite{basu2022equi} and \cite{kaba2022equivariance} propose group averaging (\textit{equitune}) and optimization-based methods, respectively, over features from group-transformed inputs to obtain equivariant outputs from non-equivariant neural networks. While \cite{kaba2022equivariance} are only concerned with training from scratch, we find that equitune performs poorly on equivariant zero-shot tasks despite good finetuning results. We hypothesize that this is because pretrained models provide better quality features for certain transformations than others and simply averaging them is deleterious. Hence, we propose $\lambda$-\textit{equitune} that averages the features using \textit{importance weights}, $\lambda$s. These weights are learned directly from the data using a small neural network, leading to excellent zero-shot and finetuned 
    
[^76]: 预训练语言模型的知识反思

    Knowledge Rumination for Pre-trained Language Models. (arXiv:2305.08732v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08732](http://arxiv.org/abs/2305.08732)

    本文提出了一种名为知识反思的新范式，旨在帮助预训练语言模型利用已经编码在其预训练参数中的相关潜在知识，而不需要从外部语料库中检索。这种方法通过在模型中添加提示，并将相关知识注入模型进行整合，取得了在常识推理任务和GLUE基准上的实验结果。

    

    先前的研究揭示了普通的预训练语言模型（PLMs）单独处理知识密集型NLP任务的能力不足，因此，一些工作尝试将外部知识集成到PLMs中。然而，尽管有着有前途的结果，但我们经验性地观察到，PLM可能已经在其预训练参数中编码了丰富的知识，但在应用到知识密集型任务时未能充分利用它们。在本文中，我们提出了一种名为知识反思的新范式，以帮助预训练语言模型利用相关的潜在知识，而不需要从外部语料库中检索它们。通过简单地在PLMs中添加一个如“据我所知”的提示，我们试图回顾相关的潜在知识，并将其注入模型以进行知识整合。我们将提出的知识反思应用于各种语言模型，包括RoBERTa、DeBERTa和GPT-3。在六个常识推理任务和GLUE基准上的实验结果显示.....

    Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fail to fully utilize them when applying them to knowledge-intensive tasks. In this paper, we propose a new paradigm dubbed Knowledge Rumination to help the pre-trained language model utilize that related latent knowledge without retrieving it from the external corpus. By simply adding a prompt like "As far as I know" to the PLMs, we try to review related latent knowledge and inject them back into the model for knowledge consolidation. We apply the proposed knowledge rumination to various language models, including RoBERTa, DeBERTa, and GPT-3. Experimental results on six commonsense reasoning tasks and GLUE benchmarks dem
    
[^77]: ChatGPT和GPT-4是否是金融文本分析的通用求解器？对几种典型任务进行检验。

    Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks. (arXiv:2305.05862v1 [cs.CL])

    [http://arxiv.org/abs/2305.05862](http://arxiv.org/abs/2305.05862)

    本研究探讨了ChatGPT和GPT-4在金融文本分析任务中的潜力，结果显示它们在数值推理上表现出色但在需要领域特定知识的任务上表现不佳。

    

    最近的大型语言模型如ChatGPT和GPT-4引起了人们的广泛关注，因为它们能够生成高质量的对话回应。尽管ChatGPT和GPT-4在通用文本语料库上经过了广泛的测试，展示了它们令人印象深刻的能力，但还没有对金融语料库进行研究。本研究旨在通过在零样本或少样本情况下考察ChatGPT和GPT-4作为典型金融文本分析问题求解器的潜力来弥补这一差距。具体而言，我们评估了它们在五个不同的金融文本数据集上进行的四项代表性任务的能力。初步研究表明，ChatGPT和GPT-4在需要领域特定知识的金融命名实体识别（NER）和情感分析等任务上表现不佳，而在数值推理任务上表现出色。我们报告了当前版本ChatGPT和GPT-4的优点和局限性，并将它们与现有技术进行了比较。

    The most recent large language models such as ChatGPT and GPT-4 have garnered significant attention, as they are capable of generating high-quality responses to human input. Despite the extensive testing of ChatGPT and GPT-4 on generic text corpora, showcasing their impressive capabilities, a study focusing on financial corpora has not been conducted. In this study, we aim to bridge this gap by examining the potential of ChatGPT and GPT-4 as a solver for typical financial text analytic problems in the zero-shot or few-shot setting. Specifically, we assess their capabilities on four representative tasks over five distinct financial textual datasets. The preliminary study shows that ChatGPT and GPT-4 struggle on tasks such as financial named entity recognition (NER) and sentiment analysis, where domain-specific knowledge is required, while they excel in numerical reasoning tasks. We report both the strengths and limitations of the current versions of ChatGPT and GPT-4, comparing them to 
    
[^78]: TidyBot: 应用大语言模型的个性化机器人物理辅助

    TidyBot: Personalized Robot Assistance with Large Language Models. (arXiv:2305.05658v1 [cs.RO])

    [http://arxiv.org/abs/2305.05658](http://arxiv.org/abs/2305.05658)

    本文研究了使用机器人进行家庭清扫的个性化问题。通过使用大型语言模型少样本摘要能力，机器人可以学习用户的偏好并将其推广到未来的场景中，从而实现快速适应。

    

    为了使机器人能够有效个性化地提供物理辅助，它必须学习用户的个人喜好并将其应用于未来的场景中。本文研究了使用机器人进行家庭清扫的个性化问题，这些机器人能够通过捡起物品并将其放回原处来整理房间。一个关键的挑战是确定每个物品的正确位置，因为人们的喜好可以因个人品味或文化背景而大不相同。例如，一个人可能喜欢把衬衫放在抽屉里，而另一个人可能喜欢把衬衫放在架子上。我们旨在建立系统，这些系统可以通过与特定人的先前交互学习这样的喜好，而只需要几个示例。我们展示了机器人可以将基于语言的规划和感知与大型语言模型(LLMs)的少样本摘要能力相结合，从而推断出广泛适用于未来交互的用户偏好。这种方法实现了快速适应，并取得了91.2%的准确率。

    For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accurac
    
[^79]: 一项多模态模型融合的实证研究

    An Empirical Study of Multimodal Model Merging. (arXiv:2304.14933v1 [cs.CV])

    [http://arxiv.org/abs/2304.14933](http://arxiv.org/abs/2304.14933)

    本研究通过融合在不同模态上训练的transformer进行多模态模型融合，并提出一种参数有效的模态不可知架构，形成有效的训练配方。

    

    模型融合（例如插值或任务算术）将在不同任务上训练的多个模型合并以生成多任务解决方案。该技术在先前的研究中已经被证明成功，其中模型是在相似的任务和相同的初始化下训练的。在本文中，我们通过将在不同模态上训练的transformer进行融合，将此概念扩展到多模态设置。此外，我们针对一个新颖的目标进行研究，在该目标中，我们可以将视觉、语言和跨模态的transformer合并到特定模态的架构中，以创建一个参数有效的模态不可知架构。通过全面的实验，我们系统地研究了影响模型融合后性能的关键因素，包括初始化、融合机制和模型架构。我们的分析得出了一个有效的训练配方，可以通过模型融合来匹配模态不可知基线的性能（即从头开始预训练）。我们的代码可供使用。

    Model merging (e.g., via interpolation or task arithmetic) fuses multiple models trained on different tasks to generate a multi-task solution. The technique has been proven successful in previous studies, where the models are trained on similar tasks and with the same initialization. In this paper, we expand on this concept to a multimodal setup by merging transformers trained on different modalities. Furthermore, we conduct our study for a novel goal where we can merge vision, language, and cross-modal transformers of a modality-specific architecture to create a parameter-efficient modality-agnostic architecture. Through comprehensive experiments, we systematically investigate the key factors impacting model performance after merging, including initialization, merging mechanisms, and model architectures. Our analysis leads to an effective training recipe for matching the performance of the modality-agnostic baseline (i.e. pre-trained from scratch) via model merging. Our code is availa
    
[^80]: 大型语言模型对齐的基本限制

    Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v1 [cs.CL])

    [http://arxiv.org/abs/2304.11082](http://arxiv.org/abs/2304.11082)

    本文通过提出一种理论方法——行为期望边界（BEB），展示了大型语言模型中对齐的基本限制，并证明任何对齐过程都无法根除不希望的行为，这对于防止恶意攻击是不安全的。

    

    开发与人交互的语言模型的重要方面是对齐其行为，使其对其人类用户有用且无害。这通常通过调整模型的方式来实现，以增强所需的行为并抑制不希望的行为。在本文中，我们提出了一种名为行为期望边界(BEB)的理论方法，它允许我们正式研究大型语言模型中的几个内在特征和对齐的限制。重要的是，我们证明对于任何具有被该模型表现出的有限概率的行为，都存在可以触发模型输出此行为的提示，其概率随提示的长度增加而增加。这意味着任何减弱不希望的行为但未将其完全消除的对齐过程都无法抵御针对性攻击。此外，我们的框架提示了领先的

    An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading al
    
[^81]: 论ChatGPT和情感增强提示在心理健康分析中的评估

    On the Evaluations of ChatGPT and Emotion-enhanced Prompting for Mental Health Analysis. (arXiv:2304.03347v1 [cs.CL])

    [http://arxiv.org/abs/2304.03347](http://arxiv.org/abs/2304.03347)

    本文全面评估了ChatGPT在心理健康分析和情感推理方面的表现，以及不同提示策略和情感信息对其性能的影响。结果显示，ChatGPT在心理健康分析方面表现良好，加入情感增强提示对某些任务效果显著。

    

    自动化心理健康分析显示出提高心理健康护理效率和可访问性的巨大潜力，而最近的主流方法利用预训练的语言模型(PLMs)作为骨干，并融入情感信息。最新的大型语言模型(LLMs)，如ChatGPT，在各种自然语言处理任务上表现出惊人的能力。然而，现有的ChatGPT零-shot性能研究在不充分的评估、情感信息利用和方法可解释性方面存在局限性。本文全面评估了ChatGPT在11个数据集上的心理健康分析和情感推理能力，涵盖了5个任务，包括二进制和多类心理健康状况检测、心理健康状况的原因/因素检测、对话中的情绪识别和因果情感蕴含。我们在实证分析中探究了不同提示策略以及情感增强提示对ChatGPT性能的影响。结果表明，ChatGPT在心理健康分析中有着良好的表现，并且在某些任务中加入情感增强提示效果显著。

    Automated mental health analysis shows great potential for enhancing the efficiency and accessibility of mental health care, whereas the recent dominant methods utilized pre-trained language models (PLMs) as the backbone and incorporated emotional information. The latest large language models (LLMs), such as ChatGPT, exhibit dramatic capabilities on diverse natural language processing tasks. However, existing studies on ChatGPT's zero-shot performance for mental health analysis have limitations in inadequate evaluation, utilization of emotional information, and explainability of methods. In this work, we comprehensively evaluate the mental health analysis and emotional reasoning ability of ChatGPT on 11 datasets across 5 tasks, including binary and multi-class mental health condition detection, cause/factor detection of mental health conditions, emotion recognition in conversations, and causal emotion entailment. We empirically analyze the impact of different prompting strategies with 
    
[^82]: SelfCheckGPT: 零资源黑盒幻觉检测方法用于生成式大语言模型

    SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. (arXiv:2303.08896v1 [cs.CL])

    [http://arxiv.org/abs/2303.08896](http://arxiv.org/abs/2303.08896)

    SelfCheckGPT是一种简单的基于采样的方法，可以以零资源的方式检查黑盒模型的幻觉现象。

    

    生成式大语言模型（LLM）例如GPT-3，能够对各种用户提示进行高度流畅的响应。然而，LLM已知会产生幻觉事实和非事实陈述，这可能会削弱对它们的输出的信任。现有的事实检查方法要么需要访问令牌级输出概率分布（这可能对于ChatGPT等系统来说不可用），要么需要通过单独的通常复杂的模块接口的外部数据库。在这项工作中，我们提出了一种简单的基于采样的方法，称为“SelfCheckGPT”，可以以零资源的方式检查黑盒模型，即不需要外部数据库。 SelfCheckGPT利用一个简单的思想：如果LLM具有特定概念的知识，则采样的响应可能类似并包含一致的事实。但是，对于幻觉的事实，随机采样的响应可能会发散并相互矛盾。我们通过使用GP-T-3模型为例来研究此方法，并在常见任务上进行广泛实验，结果表明SelfCheckGPT能够有效地检测模型的幻觉现象，且在保持准确性的同时保持良好的效率。

    Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to token-level output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose "SelfCheckGPT", a simple sampling-based approach that can be used to fact-check black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if a LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GP
    
[^83]: UPRISE: 通用提示检索以提高零样本评估

    UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation. (arXiv:2303.08518v1 [cs.CL])

    [http://arxiv.org/abs/2303.08518](http://arxiv.org/abs/2303.08518)

    UPRISE是一种通用的检索器，可自动为给定的零样本任务输入检索提示，从而提高大型语言模型的零样本评估。它通过跨任务和跨模型的实验展示了其通用性和潜力，同时表明具有减轻幻觉问题和提高LLM性能的能力。

    

    大型语言模型因其出色的能力而受欢迎，但需要特定模型的微调或任务特定提示工程可能会阻碍其一般化。我们提出了UPRISE（通用提示检索以提高零样本评估），该方法调整了轻量级和多功能的检索器，以自动检索给定零样本任务输入的提示。具体而言，在跨任务和跨模型方案中展示了通用性：检索器针对各种任务进行微调，但在看不见的任务类型上进行测试；我们在一个小型冻结LLM——GPT-Neo-2.7B上调整检索器，但在不同规模的LLM上测试检索器，例如BLOOM-7.1B、OPT-66B和GPT3-175B。此外，我们展示了UPRISE在我们与ChatGPT的实验中减轻了幻觉问题，表明它有潜力改进甚至是最强的LLM。

    Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a cross-task and cross-model scenario: the retriever is tuned on a diverse set of tasks, but tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for tuning the retriever, but test the retriever on different LLMs of much larger scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that UPRISE mitigates the hallucination problem in our experiments with ChatGPT, suggesting its potential to improve even the strongest LLMs.
    
[^84]: Query2doc: 基于大型语言模型的查询扩展方法

    Query2doc: Query Expansion with Large Language Models. (arXiv:2303.07678v1 [cs.IR])

    [http://arxiv.org/abs/2303.07678](http://arxiv.org/abs/2303.07678)

    本论文提出了一种名为query2doc的查询扩展方法，使用大型语言模型生成伪文档来改善稀疏和密集检索系统，取得了在多个数据集上提高 BM25 性能的结果。

    

    本论文介绍了一种简单但有效的查询扩展方法，称为query2doc，可改善稀疏和密集检索系统。该方法首先利用小批量提示大型语言模型生成伪文档，然后使用生成的伪文档扩展查询。大型语言模型经过训练，能够记忆知识，从而生成的伪文档通常包含高度相关的信息，有助于查询消岐和指导检索器。实验结果表明，在不进行任何模型微调的情况下，query2doc 在 MS-MARCO 和 TREC DL 等 ad-hoc IR 数据集上将 BM25 的性能提高了 3% 到 15%。此外，我们的方法还在领域内和领域外的结果方面受益于最先进的密集检索器。

    This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo-documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.
    
[^85]: 多语言语义解析的主动学习

    Active Learning for Multilingual Semantic Parser. (arXiv:2301.12920v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.12920](http://arxiv.org/abs/2301.12920)

    本研究提出了一种多语言语义解析的主动学习方法(AL-MSP), 选择一个现有数据集的子集进行翻译，通过优先选择多样化逻辑形式结构和更多词汇选择的示例，以及一种新的无需额外注释成本的超参数调整方法，有效减少了翻译成本，并取得了比其他基线更好的解析性能。

    

    当前的多语言语义解析(MSP) 数据集几乎都是通过将现有数据集中的话语从资源丰富的语言翻译到目标语言而收集的。但是，手动翻译成本高昂。为了减少翻译工作量，本文提出了第一个MSP的主动学习过程(AL-MSP)。AL-MSP只选择现有数据集的子集进行翻译。我们还提出了一种新的选择方法，优先选择多样化逻辑形式结构和更多词汇选择的示例，以及一种无需额外注释成本的新的超参数调整方法。我们的实验表明，通过理想的选择方法，AL-MSP显著减少了翻译成本。我们的选择方法与适当的超参数可以在两个多语言数据集上获得比其他基线更好的解析性能。

    Current multilingual semantic parsing (MSP) datasets are almost all collected by translating the utterances in the existing datasets from the resource-rich language to the target language. However, manual translation is costly. To reduce the translation effort, this paper proposes the first active learning procedure for MSP (AL-MSP). AL-MSP selects only a subset from the existing datasets to be translated. We also propose a novel selection method that prioritizes the examples diversifying the logical form structures with more lexical choices, and a novel hyperparameter tuning method that needs no extra annotation cost. Our experiments show that AL-MSP significantly reduces translation costs with ideal selection methods. Our selection method with proper hyperparameters yields better parsing performance than the other baselines on two multilingual datasets.
    
[^86]: 知识显著跨度掩码：增强语言模型作为知识库

    Knowledgeable Salient Span Mask for Enhancing Language Models as Knowledge Base. (arXiv:2204.07994v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.07994](http://arxiv.org/abs/2204.07994)

    本文提出了一个名为知识显著跨度掩码的方法，通过全自监督学习帮助语言模型从非结构化文本中获取更多的知识。实验证明了该方法在知识密集型任务中的有效性。

    

    预训练语言模型（PLMs）如BERT在各种下游NLP任务中取得了显著进展。然而，通过要求模型进行填空式测试，最近的研究发现PLMs在从非结构化文本中获取知识方面存在不足。为了了解PLMs在检索知识时的内部行为，我们首先为非结构化文本定义了知识可见（K-B）标记和无知识（K-F）标记，并请专业标注员手动标记了一些样本。然后，我们发现PLMs更有可能对K-B标记给出错误预测，并且在自注意力模块内部对这些标记关注的更少。基于这些观察结果，我们以全自监督的方式开发了两种解决方案，帮助模型从非结构化文本中学习更多的知识。在知识密集型任务上的实验证明了所提方法的有效性。据我们所知，我们是第一个探索全自监督学习持续预训练知识的研究者。

    Pre-trained language models (PLMs) like BERT have made significant progress in various downstream NLP tasks. However, by asking models to do cloze-style tests, recent work finds that PLMs are short in acquiring knowledge from unstructured text. To understand the internal behaviour of PLMs in retrieving knowledge, we first define knowledge-baring (K-B) tokens and knowledge-free (K-F) tokens for unstructured text and ask professional annotators to label some samples manually. Then, we find that PLMs are more likely to give wrong predictions on K-B tokens and attend less attention to those tokens inside the self-attention module. Based on these observations, we develop two solutions to help the model learn more knowledge from unstructured text in a fully self-supervised manner. Experiments on knowledge-intensive tasks show the effectiveness of the proposed methods. To our best knowledge, we are the first to explore fully self-supervised learning of knowledge in continual pre-training.
    
[^87]: UNIQORN：统一的RDF知识图谱与自然语言文本问答系统

    UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v5 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2108.08614](http://arxiv.org/abs/2108.08614)

    本文提出了一个名为UNIQORN的问答系统，它能够无缝地处理RDF数据和文本，使用fine-tuned BERT模型为问题构建上下文图，并使用图算法确定与问题相关的子图来回答问题。

    

    问题回答在知识图谱和其他RDF数据上已经取得了巨大的进展，许多优秀的系统可以为自然语言问题或电报查询提供清晰的答案。其中一些系统将文本源作为附加证据纳入回答过程，但不能计算仅存在于文本中的答案。相反，IR和NLP社区的系统已经解决了有关文本的QA问题，但是这些系统几乎不利用语义数据和知识。本文提出了第一个可以无缝操作混合RDF数据集和文本语料库或单个来源的复杂问题的系统，在统一框架中进行操作。我们的方法称为UNIQORN，通过使用经过精细调整的BERT模型从RDF数据和/或文本语料库中检索与问题相关的证据来动态构建上下文图。结果图通常非常丰富但高度嘈杂。UNIQORN通过用于组Steiner树的图算法来处理这个输入，从而确定与问题相关的子图，进而回答问题。

    Question answering over knowledge graphs and other RDF data has been greatly advanced, with a number of good systems providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, systems from the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents the first system for complex questions that can seamlessly operate over a mixture of RDF datasets and text corpora, or individual sources, in a unified framework. Our method, called UNIQORN, builds a context graph on-the-fly, by retrieving question-relevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph is typically rich but highly noisy. UNIQORN copes with this input by a graph algorithm for Group Steiner Trees, that identifi
    

