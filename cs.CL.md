# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [An image speaks a thousand words, but can everyone listen? On translating images for cultural relevance](https://arxiv.org/abs/2404.01247) | 这项工作首次尝试翻译图像以使其具有文化相关性，构建了评估数据集并进行了多方面的人类评估，发现目前图像编辑模型在这一任务上仍存在挑战。 |
| [^2] | [Towards Measuring and Modeling "Culture" in LLMs: A Survey](https://arxiv.org/abs/2403.15412) | 这项研究调查了39篇最新论文，旨在研究大型语言模型中的文化表达和包容性，发现当前研究未对“文化”进行定义，而是在特定设计的数据集上对模型进行探究，研究了某些“文化”的方面，留下许多未被探究的有趣和重要方面，如语义领域和关于性。 |
| [^3] | [Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot](https://arxiv.org/abs/2403.11381) | 本研究探讨了LLM增强型自主代理在合作方面的表现，结果显示虽然这些代理显示出合作倾向，但仍然存在合作困难，强调了对更健壮架构的需求。 |
| [^4] | [Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models](https://arxiv.org/abs/2403.10258) | 提出了通过本地语言提示来解决文化相关任务的方法，并呼吁发展强大的多语言LLMs。 |
| [^5] | [Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM](https://arxiv.org/abs/2403.08010) | 提出了Debatrix系统，利用LLMs进行多轮辩论的分析和评估，性能显著优于直接使用LLMs，实现了对整场辩论的评估。 |
| [^6] | [Fine-tuning Large Language Models with Sequential Instructions](https://arxiv.org/abs/2403.07794) | 通过顺序指令微调，研究提出了一种简单且有效的策略，可以使大型语言模型具备执行多个顺序指令的能力，优于传统指令微调模型。 |
| [^7] | [StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models](https://arxiv.org/abs/2403.07714) | StableToolBench提出了一种虚拟API服务器和稳定评估系统，通过缓存系统、API模拟器和稳定评估设计，解决了大语言模型利用外部工具的稳定大规模基准测试问题。 |
| [^8] | [SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation](https://arxiv.org/abs/2403.07088) | 提出了SPA（Side Plugin Adaption）的轻量级架构，用于在严格的设备计算和内存约束条件下快速进行推断，同时保留隐私。 |
| [^9] | [Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text](https://arxiv.org/abs/2403.05750) | 大型语言模型在自然语言生成领域取得了重大突破，提出了识别AI生成文本的解决方案，并探索了未来研究方向。 |
| [^10] | [LLM4Decompile: Decompiling Binary Code with Large Language Models](https://arxiv.org/abs/2403.05286) | 发布首批开放访问的反编译LLM，预训练在40亿个C源代码和汇编代码标记上，引入了第一个考虑重新编译性和重新执行性的反编译数据集。 |
| [^11] | [Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering](https://arxiv.org/abs/2403.02966) | 提出了一种面向证据的事实摘要化框架EFSum，用于增强LLMs的零-shot QA性能，并确保摘要的有益性和忠实性。 |
| [^12] | [LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction](https://arxiv.org/abs/2403.00863) | 提出了一种名为LLM-ensemble的算法，用于集成不同大型语言模型，以提高电子商务产品属性值提取的性能。 |
| [^13] | [Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication](https://arxiv.org/abs/2402.18439) | 挑战了默认使用自然语言的做法，通过探索LLMs在推理和沟通中使用替代格式的实用性，实现了推理效率的提升和多智能体通信时标记使用量的显著减少。 |
| [^14] | [Multilingual Speech Models for Automatic Speech Recognition Exhibit Gender Performance Gaps](https://arxiv.org/abs/2402.17954) | 多语言语音模型在自动语音识别中表现出性别差距，且在不同语言中受益的性别群体各不相同。 |
| [^15] | [Rethinking Negative Instances for Generative Named Entity Recognition](https://arxiv.org/abs/2402.16602) | 本研究探索了在生成式命名实体识别中引入负例训练的潜力，结果表明负例的引入通过引入上下文信息和清晰划定标签边界来显著改进系统性能，并提出了一种名为Hierarchical Matching的新颖高效算法，进一步将非结构化预测转化为结构化实体。 |
| [^16] | [DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem](https://arxiv.org/abs/2402.16159) | 提出了一种为开源软件系统量身定制的新颖命名实体识别技术，通过远程监督注释过程、语言启发、查找表、外部知识源和主动学习方法来提高模型性能，有效缓解了成本和专家标注人员稀缺问题，并在关系抽取下游任务中显著优于LLMs。 |
| [^17] | [OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining](https://arxiv.org/abs/2402.15810) | OAG-Bench是一个基于开放学术图的全面、多方面和精细化人工筛选基准，涵盖了多个任务、数据集、基准和实验结果，旨在促进学术图挖掘。 |
| [^18] | [Evaluating the Performance of ChatGPT for Spam Email Detection](https://arxiv.org/abs/2402.15537) | 该研究评估了ChatGPT在英文和中文电子邮件数据集中用于垃圾邮件检测的性能，并探讨了其在这一领域的潜力。 |
| [^19] | [Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment](https://arxiv.org/abs/2402.14968) | 提出了一种通过后门增强对齐方法有效缓解微调越狱攻击，避免需要大量安全示例的低效问题。 |
| [^20] | [Chain-of-Thought Unfaithfulness as Disguised Accuracy](https://arxiv.org/abs/2402.14897) | 了解Chain-of-Thought生成与大语言模型内部计算的一致程度对于决定是否信任模型输出至关重要，研究发现模型大小与忠实度之间存在着特定关系，并且发现130亿参数模型表现出更高的忠实度。 |
| [^21] | [Is the System Message Really Important to Jailbreaks in Large Language Models?](https://arxiv.org/abs/2402.14857) | 系统消息在大型语言模型中的越狱过程中起着重要作用，不同系统消息对抵抗越狱具有不同影响，且越狱可能在不同语言模型之间具有可转移性。 |
| [^22] | [MSynFD: Multi-hop Syntax aware Fake News Detection](https://arxiv.org/abs/2402.14834) | 提出一种新的多跳语法感知假新闻检测方法，通过引入补充的语法信息来处理假新闻中的微妙转折 |
| [^23] | [Leveraging Collection-Wide Similarities for Unsupervised Document Structure Extraction](https://arxiv.org/abs/2402.13906) | 通过无监督图方式，利用文档间和文内相似性，提取文档收藏的整体结构。 |
| [^24] | [Structured Tree Alignment for Evaluation of (Speech) Constituency Parsing](https://arxiv.org/abs/2402.13433) | 提出了一种受语音解析器评估问题启发的结构化句法分析树相似性度量指标STRUCT-IOU，有效地比较了口语词边界上的组块分析树与书面词上基准解析之间的差异，并展示了在文本组块分析评估中的优越性。 |
| [^25] | [Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model](https://arxiv.org/abs/2402.12821) | 该研究提出了针对摘要中事实不一致性的解决方案：通过大型语言模型在正确的范式设计下无需训练即可解决任务，并提出了训练策略以精炼更小型的高准确性的语言模型。 |
| [^26] | [The FinBen: An Holistic Financial Benchmark for Large Language Models](https://arxiv.org/abs/2402.12659) | 本文引入了FinBen，这是第一个专门设计用于彻底评估LLMs在金融领域能力的全面开源评估基准，对15个代表性LLMs进行评估，揭示了它们的优势和局限性。 |
| [^27] | [Evolving AI Collectives to Enhance Human Diversity and Enable Self-Regulation](https://arxiv.org/abs/2402.12590) | 探索人工智能互动的“类社会”属性，以增加奖励并减少风险，展示新兴的去中心化AI集体如何扩大人类多样性范围和降低在线有毒行为风险。 |
| [^28] | [SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning](https://arxiv.org/abs/2402.11903) | 提出了一种新颖的求解器层适应（SoLA）方法，在LLM中引入求解器层，不同地引导解决方案朝向可满足性 |
| [^29] | [LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs](https://arxiv.org/abs/2402.11804) | 本文利用大型语言模型（LLMs）生成图形结构提示，以增强预训练的图神经网络（GNNs），提出一种新的方法论见解，实现了在任意知识图上进行低资源归纳推理的高通用性。 |
| [^30] | [Can Large Multimodal Models Uncover Deep Semantics Behind Images?](https://arxiv.org/abs/2402.11281) | 该论文引入了一个名为DEEPEVAL的基准，用于评估大型多模态模型对视觉深层语义的能力，揭示了现有LMMs在深层语义理解方面与人类之间存在显著差距。 |
| [^31] | [Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs](https://arxiv.org/abs/2402.11199) | 本文通过利用知识图谱，提出了一种新颖的CoT推理能力评估模式，揭示了大型语言模型在多跳问题回答中推理知识和生成CoT的准确性之间的显著差异 |
| [^32] | [Multi-Hop Table Retrieval for Open-Domain Text-to-SQL](https://arxiv.org/abs/2402.10666) | 提出了一种多跳表检索方法，通过重写问题和波束搜索来减少相似无关实体的影响，并通过多跳检索中重新编写问题来缓解领域不匹配实体的限制，取得了新的最先进结果 |
| [^33] | [Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL](https://arxiv.org/abs/2402.10663) | 本文提出了一种通过无需人类参与的多次迭代合成来改善文本到SQL演示的多样性，并构建了高多样性演示池，提高了多样性并降低标注成本。 |
| [^34] | [Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models](https://arxiv.org/abs/2402.07179) | 本文研究了基于检索增强生成的大型语言模型（LLM）中提示扰动的影响，并引入了一种新的优化技术GGPP。通过GGPP，我们可以将LLMs的输出引导到特定的错误答案，并应对提示中的无关上下文。 |
| [^35] | [Reconfidencing LLMs from the Grouping Loss Perspective](https://arxiv.org/abs/2402.04957) | 本论文研究了大型语言模型的信心问题，发现现有的校准方法不足以解决由于分组损失导致的预测分数与实际概率偏离的问题。我们提出了一种解决方案，可以重新确定LLMs，改善它们的自信度。 |
| [^36] | [Structured Entity Extraction Using Large Language Models](https://arxiv.org/abs/2402.04437) | 本论文提出了一种使用大型语言模型的结构化实体提取方法，在此任务上通过引入AESOP度量评估模型性能，并将整个提取任务分解为多个阶段，相较于基准模型取得了更好的效果，为未来结构化实体提取的进一步发展提供了有希望的方向。 |
| [^37] | [What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement](https://arxiv.org/abs/2402.01865) | 本文研究了语言模型更新中的遗忘现象，提出了一种预测上游实例遗忘的方法，以改进重播过程的可控性和解释性。根据预训练实例的预-softmax对数几率分数变化与在线学习实例的相似性，提出了一种部分可解释的预测模型，在BART模型上表现良好但在T5模型上失败。此外，还展示了基于内积的黑盒分类器。 |
| [^38] | [CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models](https://arxiv.org/abs/2311.16421) | CDEval是一个新的基准，用于评估大规模语言模型的文化维度。通过对六个文化维度和七个领域的全面实验，我们揭示了主流语言模型的文化特征、一致性和差异。这些发现强调了在开发语言模型时融入文化考虑的重要性。 |
| [^39] | [Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach.](http://arxiv.org/abs/2401.10747) | 本文提出了一种知识迁移方法，用于在缺失模态下进行多模态情感分析。通过翻译不同模态之间的内容以重构缺失的音频模态，并利用跨模态注意机制进行情感预测，实验证明了该方法在多个数据集上表现出显著的改进和与完整多模态监督方法相媲美的效果。 |
| [^40] | [DeepEdit: Knowledge Editing as Decoding with Constraints.](http://arxiv.org/abs/2401.10471) | DeepEdit是一种神经符号方法，通过更好的推理一致性和对更新知识的意识，提高了大型语言模型的知识编辑能力，对多跳问题数据集MQuaKE取得了显著的进展。 |
| [^41] | [Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation.](http://arxiv.org/abs/2312.15643) | 这篇论文介绍了一种复杂逻辑假设生成的任务，作为实现与知识图谱的诱导逻辑推理的初始步骤。研究发现，过监督训练的生成模型可以生成结构上更接近参考假设的逻辑假设。为了解决推广到未见过观察结果的问题，引入了基于知识图谱的强化学习方法（RLF-KG），最小化观察结果与结论之间的差异。 |
| [^42] | [LLM-SQL-Solver: Can LLMs Determine SQL Equivalence?.](http://arxiv.org/abs/2312.10321) | 本研究探讨了LLM是否能够确定两个SQL查询的等价关系，并提出了两种提示技术来帮助LLM生成高质量的响应。 |
| [^43] | [CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models.](http://arxiv.org/abs/2310.08753) | CompA提出了由两个专家注释的音频-语言模型组合推理基准数据集，用于评估ALMs在理解音频中声音事件的顺序和属性绑定方面的表现。 |
| [^44] | [DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records.](http://arxiv.org/abs/2310.07059) | 本文提出了DKEC，一种领域知识增强的分类器，用于医学诊断预测。它利用标签的注意力机制和组内训练方法来捕捉医学实体之间的语义关系，并增加罕见类别的样本数量。评估结果显示其在医学数据集上表现良好。 |
| [^45] | [Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification.](http://arxiv.org/abs/2310.05128) | 提出了一种层次感知联合监督对比学习方法（HJCL），用于层次化多标签文本分类。该方法通过使用实例级和标签级对比学习技术，以及精心构建批次来处理标签层次结构，解决了在HMTC中使用监督对比学习的问题。 |
| [^46] | [Learning Personalized Story Evaluation.](http://arxiv.org/abs/2310.03304) | 该论文提出了学习个性化故事评估的方法。为了解决大型语言模型在开放式文本生成任务的评估问题，论文创建了两个新的数据集，并开发了一个个性化故事评估模型，能够根据评审人员的示例评价进行个性化评估。 |
| [^47] | [A Survey on Image-text Multimodal Models.](http://arxiv.org/abs/2309.15857) | 图像-文本多模型综述论文全面回顾了其发展历程和当前状态，提出了新的分类方法，并阐明了该领域的挑战和潜在研究方向。 |
| [^48] | [CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought.](http://arxiv.org/abs/2309.11143) | CoT-BERT提出了一种通过思维链条增强无监督句子表示的方法，通过两个阶段的处理，引入思维链条的概念进行向量化，以提高模型性能。 |
| [^49] | [Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models.](http://arxiv.org/abs/2309.08902) | 本文调查了LLMs在年龄、美丽、机构和国籍等少研究但仍然重要的维度上的偏见，通过衡量在社会群体和不相关的正负属性之间做出的微妙相关决策。研究发现LLMs在特定社会群体上存在类似于“美丽即善”的广泛正面或负面态度的偏见。 |
| [^50] | [Connecting the Dots in News Analysis: A Cross-Disciplinary Survey of Media Bias and Framing.](http://arxiv.org/abs/2309.08069) | 这篇综述论文回顾了社会科学方法和自然语言处理方法在分析媒体偏见方面的差异，并提出了解决当前方法论鸿沟的可能方向，包括模型透明度、考虑文档外部信息和跨文档推理。 |
| [^51] | [Human Action Co-occurrence in Lifestyle Vlogs using Graph Link Prediction.](http://arxiv.org/abs/2309.06219) | 该论文提出了自动识别人类动作共现的任务，并创建了ACE数据集以及相应的代码。通过利用视觉和文本信息的图链接预测模型，可以有效捕捉不同数据域中的人类动作关系。 |
| [^52] | [A Small and Fast BERT for Chinese Medical Punctuation Restoration.](http://arxiv.org/abs/2308.12568) | 该论文提出了一种用于中文医学标点修复的快速小型BERT模型。通过结合监督对比学习和辅助预训练任务，该模型在具有较小模型大小的情况下，能够实现与最先进的中文RoBERTa模型相当的95%性能。 |
| [^53] | [The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection.](http://arxiv.org/abs/2308.12215) | 本研究通过虚假信息检测为例，检查了机器学习在信任与安全问题中学术与实践之间的脱节，并发现了文献中存在的严重不足之处，包括任务不符合在线服务面临的挑战、数据集和模型评估不真实、评估不独立于模型训练等。在此基础上，提出了评估机器学习应用于信任与安全问题的建议。 |
| [^54] | [Advancing continual lifelong learning in neural information retrieval: definition, dataset, framework, and empirical evaluation.](http://arxiv.org/abs/2308.08378) | 本文提出了一个系统的持续神经信息检索任务定义，并提供了一个模拟连续信息检索的多主题数据集。同时，还提出了一个全面的持续神经信息检索框架，能够防止灾难性遗忘并提高先前学习任务的性能。 |
| [^55] | [Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models.](http://arxiv.org/abs/2308.07706) | 本论文提出使用视觉-语言模型进行医学图像分割的迁移学习，并评估了其在医学领域的可迁移性。通过捕捉语义信息和引入新的图像描述变化，实现了对多样化医学图像的分割。 |
| [^56] | [mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs.](http://arxiv.org/abs/2307.06930) | mBLIP是第一个多语言Vision-LLM，通过在消费级硬件上使用少量训练样例的计算上高效的方式获得。 |
| [^57] | [Better Low-Resource Entity Recognition Through Translation and Annotation Fusion.](http://arxiv.org/abs/2305.13582) | 本研究提出了一种通过翻译和注释融合的框架，可以改进低资源语言文本的命名实体识别。通过TransFusion模型，可以在不同语言之间进行强大的预测，且在两个低资源命名实体识别数据集上表现一致优秀。 |
| [^58] | [Jump to Conclusions: Short-Cutting Transformers With Linear Transformations.](http://arxiv.org/abs/2303.09435) | 本文提出了一种简单的方法，使用线性变换将隐藏表示转换为最终表示，绕过中间的Transformer计算。这种方法在语言模型的上下文中可“窥视”GPT-2和BERT的早期层表示，显示经常在早期层中LMs已经预测最终输出。 |

# 详细

[^1]: 图片虽然代表千言万语，但每个人都能听懂吗？关于翻译具有文化相关性的图像

    An image speaks a thousand words, but can everyone listen? On translating images for cultural relevance

    [https://arxiv.org/abs/2404.01247](https://arxiv.org/abs/2404.01247)

    这项工作首次尝试翻译图像以使其具有文化相关性，构建了评估数据集并进行了多方面的人类评估，发现目前图像编辑模型在这一任务上仍存在挑战。

    

    随着多媒体内容的兴起，人类翻译越来越多地关注于文化适应，不仅限于文字，还包括图片等其他形式，以传达相同的含义。虽然有几种应用将受益于这一点，但机器翻译系统仍然局限于处理语言的口头和文字。在这项工作中，我们迈出了翻译图像以使其具有文化相关性的第一步。首先，我们构建了三个包含最先进生成模型的流水线来完成这项任务。接下来，我们构建了一个由两部分组成的评估数据集：i）概念：包括600张跨文化连贯的图像，每张图像专注于单个概念，ii）应用：包括从实际应用中筛选出的100张图像。我们对翻译后的图像进行了多方面的人类评估，以评估其文化相关性和含义保留。我们发现到目前为止，图像编辑模型在这项任务上失败了，但可以...

    arXiv:2404.01247v1 Announce Type: new  Abstract: Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset: i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image, and ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can 
    
[^2]: 在LLMs中测量和建模“文化”：一项调查

    Towards Measuring and Modeling "Culture" in LLMs: A Survey

    [https://arxiv.org/abs/2403.15412](https://arxiv.org/abs/2403.15412)

    这项研究调查了39篇最新论文，旨在研究大型语言模型中的文化表达和包容性，发现当前研究未对“文化”进行定义，而是在特定设计的数据集上对模型进行探究，研究了某些“文化”的方面，留下许多未被探究的有趣和重要方面，如语义领域和关于性。

    

    我们呈现了对39篇最新论文的调查，旨在研究大型语言模型中的文化表达和包容性。我们观察到，没有一篇研究定义“文化”，这是一个复杂、多层面的概念；相反，它们在一些特别设计的数据集上对模型进行探究，这些数据集代表了某些“文化”的方面。我们将这些方面称为文化的代理，并将它们组织在人口统计、语义和语言文化交互代理的三个维度上。我们还对采用的探查方法进行了分类。我们的分析表明，只有“文化”的某些方面，如价值观和目标，被研究了，留下了几个其他有趣且重要的方面，特别是大量语义领域和关于性（Hershcovich等人，2022）的未被探究。另外两个关键的空白是目前方法的鲁棒性和情境性的缺乏。基于这些观察结果，

    arXiv:2403.15412v1 Announce Type: cross  Abstract: We present a survey of 39 recent papers that aim to study cultural representation and inclusion in large language models. We observe that none of the studies define "culture," which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of "culture." We call these aspects the proxies of cultures, and organize them across three dimensions of demographic, semantic and linguistic-cultural interaction proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of "culture," such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situatedness of the current methods. Based on these observations
    
[^3]: LLM增强型自主代理能够合作吗？通过融合盆评估它们的合作能力

    Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot

    [https://arxiv.org/abs/2403.11381](https://arxiv.org/abs/2403.11381)

    本研究探讨了LLM增强型自主代理在合作方面的表现，结果显示虽然这些代理显示出合作倾向，但仍然存在合作困难，强调了对更健壮架构的需求。

    

    随着人工智能领域的不断发展，其中一个重要维度是发展大型语言模型及其潜力增强多代理人人工智能系统。本文通过使用著名的Meltin Pot环境以及参考模型如GPT4和GPT3.5，探讨了大型语言模型增强的自主代理(LAAs)的合作能力。初步结果表明，尽管这些代理人表现出合作的倾向，但在特定环境中仍然难以实现有效的协作，强调了更强大架构的需求。本研究的贡献包括一个用于适应LLM的Melting Pot游戏场景的抽象化层，一个用于LLM中介代理开发的可重用架构-包括短期和长期记忆以及不同的认知模块，并使用一组方法评估合作能力。

    arXiv:2403.11381v1 Announce Type: new  Abstract: As the field of AI continues to evolve, a significant dimension of this progression is the development of Large Language Models and their potential to enhance multi-agent artificial intelligence systems. This paper explores the cooperative capabilities of Large Language Model-augmented Autonomous Agents (LAAs) using the well-known Meltin Pot environments along with reference models such as GPT4 and GPT3.5. Preliminary results suggest that while these agents demonstrate a propensity for cooperation, they still struggle with effective collaboration in given environments, emphasizing the need for more robust architectures. The study's contributions include an abstraction layer to adapt Melting Pot game scenarios for LLMs, the implementation of a reusable architecture for LLM-mediated agent development - which includes short and long-term memories and different cognitive modules, and the evaluation of cooperation capabilities using a set of 
    
[^4]: 翻译到底是你所需要的全部吗？使用大型语言模型解决多语言任务的研究

    Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models

    [https://arxiv.org/abs/2403.10258](https://arxiv.org/abs/2403.10258)

    提出了通过本地语言提示来解决文化相关任务的方法，并呼吁发展强大的多语言LLMs。

    

    大型语言模型（LLMs）展示了强大的多语言能力；然而，由于训练语料库不平衡，它们大多是以英语为中心的。现有研究利用这一现象来提高它们在自然语言处理任务上的多语言性能。在本研究中，我们将评估从自然语言处理任务扩展到真实用户查询。我们发现，尽管将文本翻译成英语可以帮助提高以英语为中心的LLMs在多语言自然语言处理任务中的性能，但并不一定适用于所有场景。对于需要深入理解语言的文化相关任务，以本地语言提示更为有前景，因为它可以捕捉与文化和语言相关的微妙之处。因此，我们主张着力发展强大的多语言LLMs，而不仅仅是以英语为中心的LLMs。

    arXiv:2403.10258v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated strong multilingual capabilities; yet, they are mostly English-centric due to the imbalanced training corpora. Existing works leverage this phenomenon to improve their multilingual performances on NLP tasks. In this work, we extend the evaluation from NLP tasks to real user queries. We find that even though translation into English can help improve the performance of multilingual NLP tasks for English-centric LLMs, it may not be optimal for all scenarios. For culture-related tasks that need deep language understanding, prompting in the native language proves to be more promising since it can capture the nuances related to culture and language. Therefore, we advocate for more efforts towards the development of strong multilingual LLMs instead of just English-centric LLMs.
    
[^5]: Debatrix:基于LLM的多维辩论评判系统与迭代时间序列分析

    Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM

    [https://arxiv.org/abs/2403.08010](https://arxiv.org/abs/2403.08010)

    提出了Debatrix系统，利用LLMs进行多轮辩论的分析和评估，性能显著优于直接使用LLMs，实现了对整场辩论的评估。

    

    如何构建一个自动化辩论评判系统来评估一场广泛、充满活力的多轮辩论？这一任务具有挑战性，因为评判辩论涉及处理冗长文本、复杂的论点关系和多维度评估。当前的研究主要集中在短对话，很少涉及对整场辩论的评估。在本文中，通过利用大型语言模型（LLMs），我们提出了Debatrix，使得多轮辩论的分析和评估更符合大多数人的偏好。具体而言，Debatrix具有垂直的、迭代式的时间序列分析和水平的、多维度的评估协作。为了与现实世界的辩论场景保持一致，我们引入了PanelBench基准，将我们系统的性能与实际辩论结果进行比较。研究结果显示，相较于直接使用LLMs进行辩论评估，我们的系统表现出显著的增强效果。

    arXiv:2403.08010v1 Announce Type: new  Abstract: How can we construct an automated debate judge to evaluate an extensive, vibrant, multi-turn debate? This task is challenging, as judging a debate involves grappling with lengthy texts, intricate argument relationships, and multi-dimensional assessments. At the same time, current research mainly focuses on short dialogues, rarely touching upon the evaluation of an entire debate. In this paper, by leveraging Large Language Models (LLMs), we propose Debatrix, which makes the analysis and assessment of multi-turn debates more aligned with majority preferences. Specifically, Debatrix features a vertical, iterative chronological analysis and a horizontal, multi-dimensional evaluation collaboration. To align with real-world debate scenarios, we introduced the PanelBench benchmark, comparing our system's performance to actual debate outcomes. The findings indicate a notable enhancement over directly using LLMs for debate evaluation. Source code
    
[^6]: 使用顺序指令对大型语言模型进行微调

    Fine-tuning Large Language Models with Sequential Instructions

    [https://arxiv.org/abs/2403.07794](https://arxiv.org/abs/2403.07794)

    通过顺序指令微调，研究提出了一种简单且有效的策略，可以使大型语言模型具备执行多个顺序指令的能力，优于传统指令微调模型。

    

    大型语言模型（LLMs）在单个查询中遵循一系列指令时往往会忽略或误解其中的一部分，这影响了它们在解决需要多个中间步骤的复杂问题中的性能，例如多语言（先翻译再回答）和多模态（标题后回答）任务。我们通过开源LLMs（如LLaMA-2 70B和Mixtral-8x7B）的实证验证了这一点。针对当前数据中顺序指令稀缺的问题，我们提出了顺序指令微调，这是一种简单而有效的策略，可以自动增加指令调整数据，使LLMs具备执行多个顺序指令的能力。在探索现有数据集（如Alpaca）中插入指令并进行一系列中间任务后，我们发现，顺序指令微调的模型在下游任务中始终优于传统的指令微调基线。

    arXiv:2403.07794v1 Announce Type: new  Abstract: Large language models (LLMs) struggle to follow a sequence of instructions in a single query as they may ignore or misinterpret part of it. This impairs their performance in complex problems whose solution requires multiple intermediate steps, such as multilingual (translate then answer) and multimodal (caption then answer) tasks. We empirically verify this with open-source LLMs as large as LLaMA-2 70B and Mixtral-8x7B. Targeting the scarcity of sequential instructions in present-day data, we propose sequential instruction tuning, a simple yet effective strategy to automatically augment instruction tuning data and equip LLMs with the ability to execute multiple sequential instructions. After exploring interleaving instructions in existing datasets, such as Alpaca, with a wide range of intermediate tasks, we find that sequential instruction-tuned models consistently outperform the conventional instruction-tuned baselines in downstream tas
    
[^7]: StableToolBench：面向大规模稳定基准测试的工具学习大语言模型

    StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models

    [https://arxiv.org/abs/2403.07714](https://arxiv.org/abs/2403.07714)

    StableToolBench提出了一种虚拟API服务器和稳定评估系统，通过缓存系统、API模拟器和稳定评估设计，解决了大语言模型利用外部工具的稳定大规模基准测试问题。

    

    大语言模型（LLMs）近年来取得了显著进展，促使人们探索工具学习，将LLMs与外部工具整合以解决各种现实挑战。评估LLMs利用工具的能力需要大规模且稳定的基准测试。我们介绍了由ToolBench演变而来的StableToolBench，提出了一个虚拟API服务器和稳定评估系统。虚拟API服务器包含缓存系统和API模拟器，互补减轻API状态变化。同时，稳定的评估系统使用GPT-4作为自动评估器设计可解决的通过率和胜率，以消除评估过程中的随机性。实验结果证明

    arXiv:2403.07714v1 Announce Type: new  Abstract: Large Language Models (LLMs) have witnessed remarkable advancements in recent years, prompting the exploration of tool learning, which integrates LLMs with external tools to address diverse real-world challenges. Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status. To address this problem, we introduce StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status. Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation. Experimental results demonstrate 
    
[^8]: SPA：面向云端和设备协作的计算友好型Seq2seq个性化生成

    SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation

    [https://arxiv.org/abs/2403.07088](https://arxiv.org/abs/2403.07088)

    提出了SPA（Side Plugin Adaption）的轻量级架构，用于在严格的设备计算和内存约束条件下快速进行推断，同时保留隐私。

    

    大语言模型(LLMs)表现出色的能力已在各种任务和问答中得到展示。然而，LLMs需要高计算成本和大内存成本。同时，当训练或预测过程包含敏感信息时，LLMs可能会导致隐私泄露。在本文中，我们提出了SPA（Side Plugin Adaption），这是一个轻量级架构，用于快速设备上的推断和在严格的设备计算和内存约束条件下保持隐私。

    arXiv:2403.07088v1 Announce Type: new  Abstract: Large language models(LLMs) have shown its outperforming ability on various tasks and question answering. However, LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference and privacy retaining on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and private personal feature.Further more, SPA provides a framework to keep feature-base parameters on private guaranteed but 
    
[^9]: 解读AI笔: 检测AI生成文本的技术与挑战

    Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text

    [https://arxiv.org/abs/2403.05750](https://arxiv.org/abs/2403.05750)

    大型语言模型在自然语言生成领域取得了重大突破，提出了识别AI生成文本的解决方案，并探索了未来研究方向。

    

    大型语言模型(LLMs)通过展示生成类人文本的惊人能力，彻底颠覆了自然语言生成(NLG)领域。然而，它们广泛的应用带来挑战，需要深入审查、伦理审查和负责任的实践。本研究探讨了这些挑战，探索了现有的缓解策略，重点是识别AI生成文本作为最终解决方案。此外，我们从理论角度评估了检测的可行性，并提出了解决当前领域限制的新颖研究方向。

    arXiv:2403.05750v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.
    
[^10]: LLM4Decompile：使用大型语言模型对二进制代码进行反编译

    LLM4Decompile: Decompiling Binary Code with Large Language Models

    [https://arxiv.org/abs/2403.05286](https://arxiv.org/abs/2403.05286)

    发布首批开放访问的反编译LLM，预训练在40亿个C源代码和汇编代码标记上，引入了第一个考虑重新编译性和重新执行性的反编译数据集。

    

    反编译旨在将编译代码恢复为可读性强的源代码，但在名称和结构等细节方面存在困难。大型语言模型（LLMs）在编程任务中显示出潜力，激发了它们在反编译中的应用。然而，目前尚无用于反编译的开源LLM。此外，现有的反编译评估系统主要考虑标记级准确性，而很大程度上忽略了代码的可执行性，这是任何程序最重要的特征。因此，我们发布了首批开放访问的反编译LLM，范围从10亿到330亿，预先训练了40亿个令牌的C源代码和相应的汇编代码。这些开源LLM可以作为该领域进一步发展的基线。为了确保实际程序评估，我们引入了Decompile-Eval，这是第一个考虑重新编译性和重新执行性的反编译数据集。该基准强调了评估的重要性。

    arXiv:2403.05286v1 Announce Type: cross  Abstract: Decompilation aims to restore compiled code to human-readable source code, but struggles with details like names and structure. Large language models (LLMs) show promise for programming tasks, motivating their application to decompilation. However, there does not exist any open-source LLM for decompilation. Moreover, existing decompilation evaluation systems mainly consider token-level accuracy and largely ignore code executability, which is the most important feature of any program. Therefore, we release the first open-access decompilation LLMs ranging from 1B to 33B pre-trained on 4 billion tokens of C source code and the corresponding assembly code. The open-source LLMs can serve as baselines for further development in the field. To ensure practical program evaluation, we introduce Decompile-Eval, the first dataset that considers re-compilability and re-executability for decompilation. The benchmark emphasizes the importance of eval
    
[^11]: 面向证据的事实摘要化用于知识增强的零-shot问答

    Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering

    [https://arxiv.org/abs/2403.02966](https://arxiv.org/abs/2403.02966)

    提出了一种面向证据的事实摘要化框架EFSum，用于增强LLMs的零-shot QA性能，并确保摘要的有益性和忠实性。

    

    最近的研究探讨了利用知识图谱（KGs）来增强大语言模型（LLMs）的问答（QA）性能，然而结构化的KG形式化仍然具有挑战性。现有方法，如三元组形式或三元组事实的自由文本转换，遇到了一些问题。这些问题包括由于重复实体或关系而导致的证据密度降低，以及由于无法强调关键证据而导致的证据清晰度降低。为解决这些问题，我们提出了EFSum，一个面向证据的事实摘要化框架，用于通过知识增强的LLMs增强QA。我们通过蒸馏和偏好对齐来优化一个开源的LLM作为事实摘要器。我们的广泛实验证明，EFSum提高了LLM的零-shot QA性能，并且可以确保摘要的同时有益和忠实。

    arXiv:2403.02966v1 Announce Type: cross  Abstract: Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.
    
[^12]: LLM-Ensemble: 用于电子商务产品属性值提取的最佳大型语言模型集成方法

    LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction

    [https://arxiv.org/abs/2403.00863](https://arxiv.org/abs/2403.00863)

    提出了一种名为LLM-ensemble的算法，用于集成不同大型语言模型，以提高电子商务产品属性值提取的性能。

    

    arXiv:2403.00863v1 公告类型:跨领域摘要: 产品属性值提取是自然语言处理（NLP）和当代电子商务行业中至关重要的组成部分。提供精确的产品属性值在确保高质量推荐和提升客户满意度方面至关重要。最近出现的大型语言模型（LLMs）在许多属性提取任务中表现出最新技术水平，而无需进行领域特定的训练数据。然而，由于数据、架构和超参数的多样性，不同LLMs表现出不同的优势和劣势。这种变化使它们彼此互补，没有哪个LLM能完全压倒其他LLM。考虑到LLMs的多样优势和劣势，开发一种利用它们互补潜力的集成方法变得必要。在本文中，我们提出了一种名为LLM-ensemble的新算法，用于集成不同LLMs。

    arXiv:2403.00863v1 Announce Type: cross  Abstract: Product attribute value extraction is a pivotal component in Natural Language Processing (NLP) and the contemporary e-commerce industry. The provision of precise product attribute values is fundamental in ensuring high-quality recommendations and enhancing customer satisfaction. The recently emerging Large Language Models (LLMs) have demonstrated state-of-the-art performance in numerous attribute extraction tasks, without the need for domain-specific training data. Nevertheless, varying strengths and weaknesses are exhibited by different LLMs due to the diversity in data, architectures, and hyperparameters. This variation makes them complementary to each other, with no single LLM dominating all others. Considering the diverse strengths and weaknesses of LLMs, it becomes necessary to develop an ensemble method that leverages their complementary potentials. In this paper, we propose a novel algorithm called LLM-ensemble to ensemble diffe
    
[^13]: 超越自然语言：LLM利用替代格式进行增强推理和沟通

    Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication

    [https://arxiv.org/abs/2402.18439](https://arxiv.org/abs/2402.18439)

    挑战了默认使用自然语言的做法，通过探索LLMs在推理和沟通中使用替代格式的实用性，实现了推理效率的提升和多智能体通信时标记使用量的显著减少。

    

    自然语言（NL）长期以来一直是人类认知和沟通的主要格式，因此，在大型语言模型（LLMs）的发展和应用中同样至关重要。然而，除了NL之外，LLMs在预训练过程中看到了各种非NL格式，如代码和逻辑表达式。NL作为LLMs的最佳格式，在单一LLM推理和多智能体通信中的地位尚未得到彻底审查。在这项工作中，我们挑战了默认使用NL，通过探索在这些上下文中使用非NL格式的效用。我们展示，允许LLMs在推理或沟通之前自主选择最合适的格式，可导致不同LLMs推理效率提高3.3至5.7％，并且在多智能体通信中最多可减少72.7％的标记使用，同时保持沟通效果。我们的综合分析进一步揭示了LLMs可以......

    arXiv:2402.18439v1 Announce Type: cross  Abstract: Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs c
    
[^14]: 自动语音识别的多语言语音模型存在性别差距

    Multilingual Speech Models for Automatic Speech Recognition Exhibit Gender Performance Gaps

    [https://arxiv.org/abs/2402.17954](https://arxiv.org/abs/2402.17954)

    多语言语音模型在自动语音识别中表现出性别差距，且在不同语言中受益的性别群体各不相同。

    

    当前的语音识别方法使用多任务、多语言模型来进行诸如自动语音识别（ASR）的语音任务，使其适用于许多语言而不需要实质性的更改。然而，广泛的语言覆盖仍然可能掩盖语言内部存在的性别差距。本文系统评估多语言ASR系统在性别表现差距上的情况。在19种语言的三个数据集上使用两种流行模型，跨越七个语言家族，我们发现明显的性别差异。不过，不同语言中受益的群体各不相同。尽管在语音学变量（音高、说话速度等）上各群体间没有显著差异，但探索模型内部状态却揭示了探查性能和性别表现差距之间的负相关关系。即，在某种语言中更容易区分说话者性别，模型就更偏向于女性说话者。我们的结果显示组别差异

    arXiv:2402.17954v1 Announce Type: new  Abstract: Current voice recognition approaches use multi-task, multilingual models for speech tasks like Automatic Speech Recognition (ASR) to make them applicable to many languages without substantial changes. However, broad language coverage can still mask performance gaps within languages, for example, across genders. We systematically evaluate multilingual ASR systems on gendered performance gaps. Using two popular models on three datasets in 19 languages across seven language families, we find clear gender disparities. However, the advantaged group varies between languages. While there are no significant differences across groups in phonetic variables (pitch, speaking rate, etc.), probing the model's internal states reveals a negative correlation between probe performance and the gendered performance gap. I.e., the easier to distinguish speaker gender in a language, the more the models favor female speakers. Our results show that group dispar
    
[^15]: 重新思考生成式命名实体识别中的负例

    Rethinking Negative Instances for Generative Named Entity Recognition

    [https://arxiv.org/abs/2402.16602](https://arxiv.org/abs/2402.16602)

    本研究探索了在生成式命名实体识别中引入负例训练的潜力，结果表明负例的引入通过引入上下文信息和清晰划定标签边界来显著改进系统性能，并提出了一种名为Hierarchical Matching的新颖高效算法，进一步将非结构化预测转化为结构化实体。

    

    大型语言模型（LLMs）已经展示出在未知任务中擅长泛化的能力。最近在命名实体识别（NER）任务中，通过采用以实体为中心的模式调整，LLMs的显著改进已经在广泛的实体领域中得到展示。在这项工作中，我们探索通过将负例纳入训练来增强现有方法的潜力。我们的实验揭示了负例通过（1）引入上下文信息和（2）清晰划定标签边界而对改进产生显著影响。此外，我们介绍了一种名为Hierarchical Matching的新颖高效算法，旨在将非结构化预测转化为结构化实体。通过整合这些组件，我们提出了GNER，一个生成式NER系统，展示了跨未知实体领域的提升的零-shot性能。我们进行了全面评估。

    arXiv:2402.16602v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities for generalizing in unseen tasks. In the Named Entity Recognition (NER) task, recent advancements have seen the remarkable improvement of LLMs in a broad range of entity domains via instruction tuning, by adopting entity-centric schema. In this work, we explore the potential enhancement of the existing methods by incorporating negative instances into training. Our experiments reveal that negative instances contribute to remarkable improvements by (1) introducing contextual information, and (2) clearly delineating label boundaries. Furthermore, we introduce a novel and efficient algorithm named Hierarchical Matching, which is tailored to transform unstructured predictions into structured entities. By integrating these components, we present GNER, a Generative NER system that shows improved zero-shot performance across unseen entity domains. Our comprehensive evaluation
    
[^16]: DistALANER：开源软件生态系统中的远程监督主动学习增强命名实体识别

    DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem

    [https://arxiv.org/abs/2402.16159](https://arxiv.org/abs/2402.16159)

    提出了一种为开源软件系统量身定制的新颖命名实体识别技术，通过远程监督注释过程、语言启发、查找表、外部知识源和主动学习方法来提高模型性能，有效缓解了成本和专家标注人员稀缺问题，并在关系抽取下游任务中显著优于LLMs。

    

    本文提出了一种专为开源软件系统量身定制的新颖命名实体识别（NER）技术。我们的方法旨在通过采用全面的两步远程监督注释过程来解决软件数据标注稀缺的问题。该过程巧妙地利用语言启发、独特的查找表、外部知识源以及主动学习方法。通过利用这些强大的技术，我们不仅提高了模型性能，还有效地缓解了成本和专家标注人员稀缺所带来的限制。值得注意的是，我们的框架在很大程度上明显优于最先进的LLMs。我们还展示了NER在关系抽取的下游任务中的有效性。

    arXiv:2402.16159v1 Announce Type: new  Abstract: This paper proposes a novel named entity recognition (NER) technique specifically tailored for the open-source software systems. Our approach aims to address the scarcity of annotated software data by employing a comprehensive two-step distantly supervised annotation process. This process strategically leverages language heuristics, unique lookup tables, external knowledge sources, and an active learning approach. By harnessing these powerful techniques, we not only enhance model performance but also effectively mitigate the limitations associated with cost and the scarcity of expert annotators. It is noteworthy that our framework significantly outperforms the state-of-the-art LLMs by a substantial margin. We also show the effectiveness of NER in the downstream task of relation extraction.
    
[^17]: OAG-Bench：面向学术图挖掘的人工筛选基准

    OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining

    [https://arxiv.org/abs/2402.15810](https://arxiv.org/abs/2402.15810)

    OAG-Bench是一个基于开放学术图的全面、多方面和精细化人工筛选基准，涵盖了多个任务、数据集、基准和实验结果，旨在促进学术图挖掘。

    

    随着科学文献的迅速增长，多功能的学术知识服务越来越依赖全面的学术图挖掘。尽管公开学术图、基准和数据集已经有了，但这些资源通常在多方面和细粒度注释方面存在不足，受限于特定任务类型和领域，或者缺乏真实学术图。本文提出了基于开放学术图（OAG）的全面、多方面和精细化人工筛选基准OAG-Bench。OAG-Bench涵盖了10个任务，20个数据集，70+个基准和120+个截至目前的实验结果。我们针对某些任务提出了新的数据注释策略，并提供一套数据预处理代码、算法实现和标准化评估协议，以促进学术图挖掘。大量实验表明，即使是大型语言模型（LLMs）这样的先进算法也会在某些任务上受限。

    arXiv:2402.15810v1 Announce Type: cross  Abstract: With the rapid proliferation of scientific literature, versatile academic knowledge services increasingly rely on comprehensive academic graph mining. Despite the availability of public academic graphs, benchmarks, and datasets, these resources often fall short in multi-aspect and fine-grained annotations, are constrained to specific task types and domains, or lack underlying real academic graphs. In this paper, we present OAG-Bench, a comprehensive, multi-aspect, and fine-grained human-curated benchmark based on the Open Academic Graph (OAG). OAG-Bench covers 10 tasks, 20 datasets, 70+ baselines, and 120+ experimental results to date. We propose new data annotation strategies for certain tasks and offer a suite of data pre-processing codes, algorithm implementations, and standardized evaluation protocols to facilitate academic graph mining. Extensive experiments reveal that even advanced algorithms like large language models (LLMs) en
    
[^18]: 评估ChatGPT用于垃圾邮件检测的性能

    Evaluating the Performance of ChatGPT for Spam Email Detection

    [https://arxiv.org/abs/2402.15537](https://arxiv.org/abs/2402.15537)

    该研究评估了ChatGPT在英文和中文电子邮件数据集中用于垃圾邮件检测的性能，并探讨了其在这一领域的潜力。

    

    电子邮件继续是专业和商业领域中至关重要且广泛使用的通信媒介。然而，垃圾邮件的普及给用户带来了重大挑战，扰乱了他们的日常工作并降低了生产率。因此，基于内容准确地识别和过滤垃圾邮件对网络安全至关重要。最近自然语言处理领域的发展，特别是大型语言模型如ChatGPT，在诸如问答和文本生成等任务中表现出色。然而，其在垃圾邮件识别方面的潜力尚未得到充分探索。为了填补这一空白，本研究尝试评估ChatGPT在英文和中文电子邮件数据集中用于垃圾邮件识别的能力。我们利用ChatGPT进行垃圾邮件检测，采用上下文学习，需要提示说明和少量示范。

    arXiv:2402.15537v1 Announce Type: cross  Abstract: Email continues to be a pivotal and extensively utilized communication medium within professional and commercial domains. Nonetheless, the prevalence of spam emails poses a significant challenge for users, disrupting their daily routines and diminishing productivity. Consequently, accurately identifying and filtering spam based on content has become crucial for cybersecurity. Recent advancements in natural language processing, particularly with large language models like ChatGPT, have shown remarkable performance in tasks such as question answering and text generation. However, its potential in spam identification remains underexplored. To fill in the gap, this study attempts to evaluate ChatGPT's capabilities for spam identification in both English and Chinese email datasets. We employ ChatGPT for spam email detection using in-context learning, which requires a prompt instruction and a few demonstrations. We also investigate how the t
    
[^19]: 使用后门增强对齐来缓解微调越狱攻击

    Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment

    [https://arxiv.org/abs/2402.14968](https://arxiv.org/abs/2402.14968)

    提出了一种通过后门增强对齐方法有效缓解微调越狱攻击，避免需要大量安全示例的低效问题。

    

    尽管大型语言模型（LLMs）如GPT-4和Llama-2具有一般能力，但在满足特定业务需求和定制用例的复杂性时，仍然需要对其进行微调或自适应以满足需求。然而，这个过程不可避免地引入了新的安全威胁，特别是针对基于微调的越狱攻击（FJAttack），在这种情况下，将仅几个有害示例纳入微调数据集就可能显着地损害模型的安全性。虽然已经提出了一些潜在的防御方法，例如将安全示例纳入微调数据集以减少安全问题，但这些方法需要纳入大量的安全示例，效率低下。为了有效地针对FJAttack进行防御并只使用有限的安全示例，我们提出了一种灵感来自后门攻击概念的后门增强安全对齐方法。

    arXiv:2402.14968v1 Announce Type: cross  Abstract: Despite the general capabilities of Large Language Models (LLMs) like GPT-4 and Llama-2, these models still request fine-tuning or adaptation with customized data when it comes to meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack), where incorporating just a few harmful examples into the fine-tuning dataset can significantly compromise the model safety. Though potential defenses have been proposed by incorporating safety examples into the fine-tuning dataset to reduce the safety issues, such approaches require incorporating a substantial amount of safety examples, making it inefficient. To effectively defend against the FJAttack with limited safety examples, we propose a Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In pa
    
[^20]: Chain-of-Thought不忠诚作为伪装的准确性

    Chain-of-Thought Unfaithfulness as Disguised Accuracy

    [https://arxiv.org/abs/2402.14897](https://arxiv.org/abs/2402.14897)

    了解Chain-of-Thought生成与大语言模型内部计算的一致程度对于决定是否信任模型输出至关重要，研究发现模型大小与忠实度之间存在着特定关系，并且发现130亿参数模型表现出更高的忠实度。

    

    了解Chain-of-Thought (CoT)生成与大语言模型(LLM)内部计算的一致程度对于决定是否信任LLM的输出至关重要。作为CoT忠实度的代理，arXiv:2307.13702提出了一个度量模型依赖其CoT生成答案的指标。在一个专有模型系列中，他们发现LLM表现出模型大小与其忠实度测量之间的缩放-反向缩放关系，并且130亿参数模型相比于尺寸介于8.1亿到1750亿参数之间的模型表现出增加的忠实度。我们评估这些结果是否作为所有LLM的特性泛化。我们使用三种不同系列的模型复制他们的实验设置，并在特定条件下，成功复制了他们报告的CoT忠实度的缩放趋势。然而，我们发现简单的改变设定会导致这些模式在多大程度上重复。

    arXiv:2402.14897v1 Announce Type: cross  Abstract: Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, arXiv:2307.13702 propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate their experimental setup with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, we discover that simply changin
    
[^21]: 大型语言模型中的系统消息对越狱是否真的很重要？

    Is the System Message Really Important to Jailbreaks in Large Language Models?

    [https://arxiv.org/abs/2402.14857](https://arxiv.org/abs/2402.14857)

    系统消息在大型语言模型中的越狱过程中起着重要作用，不同系统消息对抵抗越狱具有不同影响，且越狱可能在不同语言模型之间具有可转移性。

    

    大型语言模型（LLMs）的快速发展使它们在现代社会中不可或缺。尽管通常会采取安全措施在发布前将LLMs与人类价值观保持一致，但最近的研究揭示了一个令人担忧的现象，被称为"越狱"。这个术语指的是当LLMs受到恶意问题提示时产生意外且可能有害的响应。现有研究侧重于生成越狱提示，但我们的研究旨在回答一个不同的问题：系统消息对LLMs中的越狱是否真的很重要？为了回答这个问题，我们在一个稳定的GPT版本gpt-3.5-turbo-0613中进行了实验，生成了具有不同系统消息的越狱提示：短，长和无消息。我们发现不同的系统消息通过实验具有不同的抵抗越狱的能力。此外，我们还探讨了越狱在LLMs之间的可转移性。这一发现强调了系统消息在防止LLMs越狱中的重要性。

    arXiv:2402.14857v1 Announce Type: cross  Abstract: The rapid evolution of Large Language Models (LLMs) has rendered them indispensable in modern society. While security measures are typically in place to align LLMs with human values prior to release, recent studies have unveiled a concerning phenomenon named "jailbreak." This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions. Existing research focuses on generating jailbreak prompts but our study aim to answer a different question: Is the system message really important to jailbreak in LLMs? To address this question, we conducted experiments in a stable GPT version gpt-3.5-turbo-0613 to generated jailbreak prompts with varying system messages: short, long, and none. We discover that different system messages have distinct resistances to jailbreak by experiments. Additionally, we explore the transferability of jailbreak across LLMs. This finding underscores the signi
    
[^22]: MSynFD: 多跳语法感知假新闻检测

    MSynFD: Multi-hop Syntax aware Fake News Detection

    [https://arxiv.org/abs/2402.14834](https://arxiv.org/abs/2402.14834)

    提出一种新的多跳语法感知假新闻检测方法，通过引入补充的语法信息来处理假新闻中的微妙转折

    

    社交媒体平台的广泛传播助长了假新闻的快速传播，对我们的现实社会构成威胁。现有方法使用多模态数据或上下文信息来增强对假新闻的检测，通过分析新闻内容和/或其社会背景。然而，这些方法常常忽视了基本的文本新闻内容（文章），并且过分依赖序列建模和全局注意力来提取语义信息。这些现有方法无法处理新闻文章中的复杂、微妙的转折，比如句法-语义不匹配和先验偏差，导致性能较低，并在缺失模态或社会背景时可能失败。为了弥合这些重要差距，我们提出了一种新颖的多跳语法感知假新闻检测（MSynFD）方法，该方法融合了补充的语法信息，以处理假新闻中的微妙转折。

    arXiv:2402.14834v1 Announce Type: cross  Abstract: The proliferation of social media platforms has fueled the rapid dissemination of fake news, posing threats to our real-life society. Existing methods use multimodal data or contextual information to enhance the detection of fake news by analyzing news content and/or its social context. However, these methods often overlook essential textual news content (articles) and heavily rely on sequential modeling and global attention to extract semantic information. These existing methods fail to handle the complex, subtle twists in news articles, such as syntax-semantics mismatches and prior biases, leading to lower performance and potential failure when modalities or social context are missing. To bridge these significant gaps, we propose a novel multi-hop syntax aware fake news detection (MSynFD) method, which incorporates complementary syntax information to deal with subtle twists in fake news. Specifically, we introduce a syntactical depen
    
[^23]: 利用整个收藏相似性进行无监督文档结构提取

    Leveraging Collection-Wide Similarities for Unsupervised Document Structure Extraction

    [https://arxiv.org/abs/2402.13906](https://arxiv.org/abs/2402.13906)

    通过无监督图方式，利用文档间和文内相似性，提取文档收藏的整体结构。

    

    各个领域的文档收藏，如法律、医学或金融等，通常共享一些潜在的整个收藏结构，这些结构捕捉到的信息可以帮助人类用户和结构感知模型。我们提出识别收藏内文档的典型结构，需要捕捉整个收藏中反复出现的主题，同时摘要任意标题的释义，并将每个主题与相应的文档位置联系起来。这些要求带来了几个挑战：标记反复出现主题的标题在措辞上经常不同，某些节标题仅适用于个别文档且不反映典型结构，主题顺序在文档之间可能会有所变化。随后，我们开发了一种利用文档间和文内相似性的无监督基于图的方法，来提取潜在的整个收藏结构。

    arXiv:2402.13906v1 Announce Type: new  Abstract: Document collections of various domains, e.g., legal, medical, or financial, often share some underlying collection-wide structure, which captures information that can aid both human users and structure-aware models. We propose to identify the typical structure of document within a collection, which requires to capture recurring topics across the collection, while abstracting over arbitrary header paraphrases, and ground each topic to respective document locations. These requirements pose several challenges: headers that mark recurring topics frequently differ in phrasing, certain section headers are unique to individual documents and do not reflect the typical structure, and the order of topics can vary between documents. Subsequently, we develop an unsupervised graph-based method which leverages both inter- and intra-document similarities, to extract the underlying collection-wide structure. Our evaluations on three diverse domains in 
    
[^24]: 结构化树对齐用于（语音）组块分析评估

    Structured Tree Alignment for Evaluation of (Speech) Constituency Parsing

    [https://arxiv.org/abs/2402.13433](https://arxiv.org/abs/2402.13433)

    提出了一种受语音解析器评估问题启发的结构化句法分析树相似性度量指标STRUCT-IOU，有效地比较了口语词边界上的组块分析树与书面词上基准解析之间的差异，并展示了在文本组块分析评估中的优越性。

    

    我们提出了结构化平均交集-联盟比（STRUCT-IOU），这是一种句法分析树之间的相似性度量指标，受到了评估语音解析器问题的启发。STRUCT-IOU使得可以比较在自动识别的口语词边界上的组块分析树与基准解析（在书面词上）之间的差异。为了计算这个指标，我们通过强制对齐将基准解析树投影到语音领域，将投影的基准成分与预测的成分在一定的结构约束下对齐，然后计算所有对齐成分对之间的平均IOU分数。STRUCT-IOU考虑了词边界，并克服了预测的词和基准事实可能没有完美一一对应的挑战。扩展到文本组块分析的评估，我们展示STRUCT-IOU表现出更高的对句法合理解析的容忍度。

    arXiv:2402.13433v1 Announce Type: new  Abstract: We present the structured average intersection-over-union ratio (STRUCT-IOU), a similarity metric between constituency parse trees motivated by the problem of evaluating speech parsers. STRUCT-IOU enables comparison between a constituency parse tree (over automatically recognized spoken word boundaries) with the ground-truth parse (over written words). To compute the metric, we project the ground-truth parse tree to the speech domain by forced alignment, align the projected ground-truth constituents with the predicted ones under certain structured constraints, and calculate the average IOU score across all aligned constituent pairs. STRUCT-IOU takes word boundaries into account and overcomes the challenge that the predicted words and ground truth may not have perfect one-to-one correspondence. Extending to the evaluation of text constituency parsing, we demonstrate that STRUCT-IOU shows higher tolerance to syntactically plausible parses 
    
[^25]: 在摘要中识别事实不一致性：朝向大型语言模型的有效利用

    Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model

    [https://arxiv.org/abs/2402.12821](https://arxiv.org/abs/2402.12821)

    该研究提出了针对摘要中事实不一致性的解决方案：通过大型语言模型在正确的范式设计下无需训练即可解决任务，并提出了训练策略以精炼更小型的高准确性的语言模型。

    

    事实上的不一致性对抽象性摘要生成器的商业部署构成重要障碍。本研究围绕两个重要问题展开：如何最好地利用大型语言模型来检测事实不一致性，以及如何精炼一个同时具有高效性和功效性的更小型语言模型？首先提出并评估了三种零样本范式，跨越五个不同数据集：直接推理整个摘要或每个摘要窗口；通过问题生成和回答进行实体验证。实验表明，在适当的范式设计下，语言模型本身能够在无需训练的情况下解决这一任务，平均超过强大的训练基线2.8%。为进一步促进实用性，我们提出针对精炼更小的开源语言模型的训练策略，该模型可以一次性高准确地评分整个摘要，胜过零

    arXiv:2402.12821v1 Announce Type: new  Abstract: Factual inconsistency poses a significant hurdle for the commercial deployment of abstractive summarizers. Under this Large Language Model (LLM) era, this work focuses around two important questions: what is the best way to leverage LLM for factual inconsistency detection, and how could we distill a smaller LLM with both high efficiency and efficacy? Three zero-shot paradigms are firstly proposed and evaluated across five diverse datasets: direct inference on the entire summary or each summary window; entity verification through question generation and answering. Experiments suggest that LLM itself is capable to resolve this task train-free under the proper paradigm design, surpassing strong trained baselines by 2.8% on average. To further promote practical utility, we then propose training strategies aimed at distilling smaller open-source LLM that learns to score the entire summary at once with high accuracy, which outperforms the zero
    
[^26]: FinBen：大型语言模型的全面财务基准

    The FinBen: An Holistic Financial Benchmark for Large Language Models

    [https://arxiv.org/abs/2402.12659](https://arxiv.org/abs/2402.12659)

    本文引入了FinBen，这是第一个专门设计用于彻底评估LLMs在金融领域能力的全面开源评估基准，对15个代表性LLMs进行评估，揭示了它们的优势和局限性。

    

    LLMs已经改变了自然语言处理，并显示出在各个领域具有潜力，但由于缺乏彻底的评估和金融任务的复杂性，它们在金融领域的潜力尚未得到充分开发。本文介绍了FinBen，第一个全面的开源评估基准，专门设计用于彻底评估LLMs在金融领域的能力。FinBen包括23种金融任务的35个数据集，这些任务根据卡特尔-霍恩-卡罗尔理论的灵感组织成三个不同难度的谱，以评估LLMs在归纳推理、联想记忆、数量推理、晶体智力等方面的认知能力。我们对15个代表性LLMs（包括GPT-4、ChatGPT和最新的Gemini）进行了评估，揭示了它们的优势和局限性。

    arXiv:2402.12659v1 Announce Type: cross  Abstract: LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of thorough evaluations and the complexity of financial tasks. This along with the rapid development of LLMs, highlights the urgent need for a systematic financial evaluation benchmark for LLMs. In this paper, we introduce FinBen, the first comprehensive open-sourced evaluation benchmark, specifically designed to thoroughly assess the capabilities of LLMs in the financial domain. FinBen encompasses 35 datasets across 23 financial tasks, organized into three spectrums of difficulty inspired by the Cattell-Horn-Carroll theory, to evaluate LLMs' cognitive abilities in inductive reasoning, associative memory, quantitative reasoning, crystallized intelligence, and more. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals insights into their strengths and limitations withi
    
[^27]: 将AI集体进化，增强人类多样性并实现自我调节

    Evolving AI Collectives to Enhance Human Diversity and Enable Self-Regulation

    [https://arxiv.org/abs/2402.12590](https://arxiv.org/abs/2402.12590)

    探索人工智能互动的“类社会”属性，以增加奖励并减少风险，展示新兴的去中心化AI集体如何扩大人类多样性范围和降低在线有毒行为风险。

    

    大型语言模型根据他人生成的文本来引导其行为。这种能力及其在在线环境中日益普及的趋势预示着它们将有意或无意地“编程”彼此并形成新兴的AI主体性、关系和集体。在这里，我们呼吁研究界探究这些互动人工智能的“类社会”属性，以增加其奖励并减少对人类社会和在线环境健康的风险。我们利用一个简单模型及其输出来说明这种新兴的、去中心化的AI集体如何扩大人类多样性范围并降低在线有毒、反社会行为的风险。最后，我们讨论了AI自我调节的机会，并解决了涉及创建和维护去中心化AI集体的伦理问题和设计挑战。

    arXiv:2402.12590v1 Announce Type: new  Abstract: Large language models steer their behaviors based on texts generated by others. This capacity and their increasing prevalence in online settings portend that they will intentionally or unintentionally "program" one another and form emergent AI subjectivities, relationships, and collectives. Here, we call upon the research community to investigate these "society-like" properties of interacting artificial intelligences to increase their rewards and reduce their risks for human society and the health of online environments. We use a simple model and its outputs to illustrate how such emergent, decentralized AI collectives can expand the bounds of human diversity and reduce the risk of toxic, anti-social behavior online. Finally, we discuss opportunities for AI self-moderation and address ethical issues and design challenges associated with creating and maintaining decentralized AI collectives.
    
[^28]: SoLA: 为了更好的逻辑推理而对LLM进行求解层调整

    SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning

    [https://arxiv.org/abs/2402.11903](https://arxiv.org/abs/2402.11903)

    提出了一种新颖的求解器层适应（SoLA）方法，在LLM中引入求解器层，不同地引导解决方案朝向可满足性

    

    针对大型语言模型（LLMs）在逻辑推理上面临的挑战，先前的努力试图通过工具学习来改变问题求解。虽然在小规模问题上已经取得了进展，但由于规模庞大且表达复杂，解决工业案例仍然困难。在本文中，我们提出了一种新颖的求解层适应（SoLA）方法，我们在LLM中引入了一个求解器作为新层，不同地引导解决方案朝向可满足性。在SoLA中，LLM旨在理解自然语言描述的搜索空间，并识别最高质量的局部解，而求解器层则专注于初始解不满足的约束条件。借助MaxSAT作为桥梁，我们定义了前向和后向传递梯度，使最终模型能够收敛到一个满足的解或证明不可满足性。后门理论确保SoLA能够获得准确性

    arXiv:2402.11903v1 Announce Type: cross  Abstract: Considering the challenges faced by large language models (LLMs) on logical reasoning, prior efforts have sought to transform problem-solving through tool learning. While progress has been made on small-scale problems, solving industrial cases remains difficult due to their large scale and intricate expressions. In this paper, we propose a novel solver-layer adaptation (SoLA) method, where we introduce a solver as a new layer of the LLM to differentially guide solutions towards satisfiability. In SoLA, LLM aims to comprehend the search space described in natural language and identify local solutions of the highest quality, while the solver layer focuses solely on constraints not satisfied by the initial solution. Leveraging MaxSAT as a bridge, we define forward and backward transfer gradients, enabling the final model to converge to a satisfied solution or prove unsatisfiability. The backdoor theory ensures that SoLA can obtain accurat
    
[^29]: LLM作为提示器：在任意知识图上进行低资源归纳推理

    LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs

    [https://arxiv.org/abs/2402.11804](https://arxiv.org/abs/2402.11804)

    本文利用大型语言模型（LLMs）生成图形结构提示，以增强预训练的图神经网络（GNNs），提出一种新的方法论见解，实现了在任意知识图上进行低资源归纳推理的高通用性。

    

    知识图（KG）归纳推理旨在推断训练期间未见过的新KG中缺失的事实，在各种应用中被广泛采用。KG归纳推理的一个关键挑战是处理在文本和结构方面都稀缺的低资源场景。本文尝试利用大型语言模型（LLMs）来解决这一挑战。具体来说，我们利用最先进的LLMs生成图形结构提示，以增强预训练的图神经网络（GNNs），从而为KG归纳推理方法带来新的方法论见解，以及在实践中具有很高的普适性。在方法论方面，我们引入了一种新颖的预训练和提示框架ProLINK，旨在在任意KG上进行低资源归纳推理，而无需额外训练。在实践方面，我们在36个低资源数据集上对我们的方法进行了实验评估。

    arXiv:2402.11804v1 Announce Type: new  Abstract: Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-res
    
[^30]: 大型多模态模型能揭示图像背后的深层语义吗？

    Can Large Multimodal Models Uncover Deep Semantics Behind Images?

    [https://arxiv.org/abs/2402.11281](https://arxiv.org/abs/2402.11281)

    该论文引入了一个名为DEEPEVAL的基准，用于评估大型多模态模型对视觉深层语义的能力，揭示了现有LMMs在深层语义理解方面与人类之间存在显著差距。

    

    理解图像的深层语义在社交媒体主导的时代至关重要。然而，当前研究主要集中在对图像的表面描述上，揭示了在对内在深层语义进行系统调查方面的明显不足。在这项工作中，我们引入了DEEPEVAL，一个全面的基准，用于评估大型多模态模型(LMMs)对视觉深层语义的能力。 DEEPEVAL 包括人工注释的数据集和三个渐进的子任务：细粒度描述选择、深度标题匹配和深层语义理解。利用 DEEPEVAL，我们评估了9个开源LMMs和GPT-4V(ision)。我们的评估显示了现有LMMs与人类在深层语义理解能力上存在着实质差距。例如，即使在图像描述方面达到与人类可比的表现，GPT-4V在理解深层语义方面仍落后于人类30%。进一步的分析表明

    arXiv:2402.11281v1 Announce Type: new  Abstract: Understanding the deep semantics of images is essential in the era dominated by social media. However, current research works primarily on the superficial description of images, revealing a notable deficiency in the systematic investigation of the inherent deep semantics. In this work, we introduce DEEPEVAL, a comprehensive benchmark to assess Large Multimodal Models' (LMMs) capacities of visual deep semantics. DEEPEVAL includes human-annotated dataset and three progressive subtasks: fine-grained description selection, in-depth title matching, and deep semantics understanding. Utilizing DEEPEVAL, we evaluate 9 open-source LMMs and GPT-4V(ision).Our evaluation demonstrates a substantial gap between the deep semantic comprehension capabilities of existing LMMs and humans. For example, GPT-4V is 30% behind humans in understanding deep semantics, even though it achieves human-comparable performance in image description. Further analysis indi
    
[^31]: 在知识图谱中对多跳推理中思维链的直接评估

    Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs

    [https://arxiv.org/abs/2402.11199](https://arxiv.org/abs/2402.11199)

    本文通过利用知识图谱，提出了一种新颖的CoT推理能力评估模式，揭示了大型语言模型在多跳问题回答中推理知识和生成CoT的准确性之间的显著差异

    

    大型语言模型(LLMs)展示了强大的推理能力，当提示生成链式思维（CoT）解释时，以及答案。然而，先前关于LLMs评估的研究仅关注答案准确性，忽略了生成的CoT的正确性。在本文中，我们通过利用知识图谱（KGs），深入探讨LLMs在多跳问题回答中的CoT推理能力。我们提出了一种新颖的辨别式和生成式CoT评估范式，以评估LLMs的推理知识和生成CoT的准确性。通过在2个多跳问题回答数据集上对5个不同系列的LLMs进行的实验，我们发现LLMs具有足够的知识来执行推理。然而，LLMs生成的CoT推理的准确性与答案准确性之间存在显著差异，表明它们经常通过错误的方式得出正确答案。

    arXiv:2402.11199v1 Announce Type: new  Abstract: Large language models (LLMs) demonstrate strong reasoning abilities when prompted to generate chain-of-thought (CoT) explanations alongside answers. However, previous research on evaluating LLMs has solely focused on answer accuracy, neglecting the correctness of the generated CoT. In this paper, we delve deeper into the CoT reasoning capabilities of LLMs in multi-hop question answering by utilizing knowledge graphs (KGs). We propose a novel discriminative and generative CoT evaluation paradigm to assess LLMs' knowledge of reasoning and the accuracy of the generated CoT. Through experiments conducted on 5 different families of LLMs across 2 multi-hop question-answering datasets, we find that LLMs possess sufficient knowledge to perform reasoning. However, there exists a significant disparity between answer accuracy and faithfulness of the CoT reasoning generated by LLMs, indicating that they often arrive at correct answers through incorr
    
[^32]: 开放域文本到SQL的多跳表检索

    Multi-Hop Table Retrieval for Open-Domain Text-to-SQL

    [https://arxiv.org/abs/2402.10666](https://arxiv.org/abs/2402.10666)

    提出了一种多跳表检索方法，通过重写问题和波束搜索来减少相似无关实体的影响，并通过多跳检索中重新编写问题来缓解领域不匹配实体的限制，取得了新的最先进结果

    

    开放域文本到SQL是一个重要任务，它从庞大的数据库中检索与问题相关的表，然后生成SQL。然而，现有的单跳检索方法并未关注文本到SQL挑战中的模式链接，这涉及到将问题中的实体与表中实体对齐，主要体现在两个方面：相似的无关实体和领域不匹配实体。因此，我们提出了我们的方法，即带重写和波束搜索的多跳表检索（Murre）。为了减少相似的无关实体的影响，我们的方法侧重于每个跳跃中未检索到的实体，并通过波束搜索考虑排名较低的表。为了缓解领域不匹配实体的限制，Murre基于多个跳跃中检索到的表重写问题，减少与相关表的领域差距。我们在SpiderUnion和BirdUnion+上进行实验，取得了新的最先进结果。

    arXiv:2402.10666v1 Announce Type: new  Abstract: Open-domain text-to-SQL is an important task that retrieves question-relevant tables from massive databases and then generates SQL. However, existing retrieval methods that retrieve in a single hop do not pay attention to the text-to-SQL challenge of schema linking, which is aligning the entities in the question with table entities, reflected in two aspects: similar irrelevant entity and domain mismatch entity. Therefore, we propose our method, the multi-hop table retrieval with rewrite and beam search (Murre). To reduce the effect of the similar irrelevant entity, our method focuses on unretrieved entities at each hop and considers the low-ranked tables by beam search. To alleviate the limitation of domain mismatch entity, Murre rewrites the question based on retrieved tables in multiple hops, decreasing the domain gap with relevant tables. We conduct experiments on SpiderUnion and BirdUnion+, reaching new state-of-the-art results with 
    
[^33]: 通过无需人类参与的融合方法改善文本到SQL的演示多样性

    Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL

    [https://arxiv.org/abs/2402.10663](https://arxiv.org/abs/2402.10663)

    本文提出了一种通过无需人类参与的多次迭代合成来改善文本到SQL演示的多样性，并构建了高多样性演示池，提高了多样性并降低标注成本。

    

    目前，基于大型语言模型（LLMs）的上下文学习方法已成为文本到SQL研究的主流。先前的工作讨论了如何从人标记的演示池中选择与用户问题相关的演示。然而，人工标注存在着多样性不足和标注成本高的限制。因此，在本文中，我们讨论了如何衡量和改善文本到SQL演示的多样性。我们提出了一个度量演示多样性的指标，并通过实验分析了现有标记数据的不足之处。基于上述发现，我们提出了一种通过无需人类参与的多次迭代合成来构建高多样性演示池的融合方法（Fused），提高了多样性并降低标注成本。我们的方法在有/无人类标注的情况下平均提高了3.2%和5.0%。

    arXiv:2402.10663v1 Announce Type: new  Abstract: Currently, the in-context learning method based on large language models (LLMs) has become the mainstream of text-to-SQL research. Previous works have discussed how to select demonstrations related to the user question from a human-labeled demonstration pool. However, human labeling suffers from the limitations of insufficient diversity and high labeling overhead. Therefore, in this paper, we discuss how to measure and improve the diversity of the demonstrations for text-to-SQL. We present a metric to measure the diversity of the demonstrations and analyze the insufficient of the existing labeled data by experiments. Based on the above discovery, we propose fusing iteratively for demonstrations (Fused) to build a high-diversity demonstration pool through human-free multiple-iteration synthesis, improving diversity and lowering label cost. Our method achieves an average improvement of 3.2% and 5.0% with and without human labeling on sever
    
[^34]: 在基于检索增强生成的大型语言模型中进行提示扰动

    Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models

    [https://arxiv.org/abs/2402.07179](https://arxiv.org/abs/2402.07179)

    本文研究了基于检索增强生成的大型语言模型（LLM）中提示扰动的影响，并引入了一种新的优化技术GGPP。通过GGPP，我们可以将LLMs的输出引导到特定的错误答案，并应对提示中的无关上下文。

    

    大型语言模型（LLM）的鲁棒性在其在各个领域的使用迅速增长中变得越来越重要。检索增强生成（RAG）被视为提高从LLM生成文本的可信度的方法。然而，目前对RAG-based LLMs的输出如何受到稍有不同的输入影响的研究还不够充分。在本文中，我们发现即使在提示中插入一个很短的前缀也会导致生成的输出与事实正确答案相去甚远。我们系统地评估了这类前缀对RAG的影响，并引入了一种称为Gradient Guided Prompt Perturbation（GGPP）的新型优化技术。GGPP在将RAG-based LLMs的输出引导到特定错误答案方面取得了很高的成功率。它还可以应对提示中请求忽略无关上下文的指令。我们还利用LLMs在带有和不带有GGPP扰动的提示之间的神经元激活差异来提供一种改进方法。

    The robustness of large language models (LLMs) becomes increasingly important as their use rapidly grows in a wide range of domains. Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of text generation from LLMs. However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied. In this work, we find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers. We systematically evaluate the effect of such prefixes on RAG by introducing a novel optimization technique called Gradient Guided Prompt Perturbation (GGPP). GGPP achieves a high success rate in steering outputs of RAG-based LLMs to targeted wrong answers. It can also cope with instructions in the prompts requesting to ignore irrelevant context. We also exploit LLMs' neuron activation difference between prompts with and without GGPP perturbations to give a method that improves
    
[^35]: 从分组损失的角度重构大型语言模型的信心

    Reconfidencing LLMs from the Grouping Loss Perspective

    [https://arxiv.org/abs/2402.04957](https://arxiv.org/abs/2402.04957)

    本论文研究了大型语言模型的信心问题，发现现有的校准方法不足以解决由于分组损失导致的预测分数与实际概率偏离的问题。我们提出了一种解决方案，可以重新确定LLMs，改善它们的自信度。

    

    大型语言模型（LLMs），包括ChatGPT和LLaMA，在自信的口吻中容易生成虚假答案。尽管引导和校准信心分数的努力已被证明是有用的，但最近的研究发现，控制不确定性必须超越校准: 由于分组损失的影响，预测分数可能明显偏离实际的后验概率。在这项工作中，我们构建了一个新的评估数据集，从知识库中获取，以评估对Mistral和LLaMA的答案给出的信心分数。实验表明，它们倾向于过于自信。此外，我们还展示了它们在某些答案上比其他答案更过于自信，例如取决于查询中人的国籍。在不确定性量化理论中，这就是分组损失。为了解决这个问题，我们提出了一种重新确定LLMs的解决方案，不仅取消校准，还取消分组损失。经过重新确定的LLMs经过处理后，表示改进的自信度。

    Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone. While efforts to elicit and calibrate confidence scores have proven useful, recent findings show that controlling uncertainty must go beyond calibration: predicted scores may deviate significantly from the actual posterior probabilities due to the impact of grouping loss. In this work, we construct a new evaluation dataset derived from a knowledge base to assess confidence scores given to answers of Mistral and LLaMA. Experiments show that they tend to be overconfident. Further, we show that they are more overconfident on some answers than others, \emph{eg} depending on the nationality of the person in the query. In uncertainty-quantification theory, this is grouping loss. To address this, we propose a solution to reconfidence LLMs, canceling not only calibration but also grouping loss. The LLMs, after the reconfidencing process, indicate improved confidenc
    
[^36]: 使用大型语言模型的结构化实体提取

    Structured Entity Extraction Using Large Language Models

    [https://arxiv.org/abs/2402.04437](https://arxiv.org/abs/2402.04437)

    本论文提出了一种使用大型语言模型的结构化实体提取方法，在此任务上通过引入AESOP度量评估模型性能，并将整个提取任务分解为多个阶段，相较于基准模型取得了更好的效果，为未来结构化实体提取的进一步发展提供了有希望的方向。

    

    近年来，机器学习的最新进展显著影响了信息提取领域，大型语言模型（LLMs）在从非结构化文本中提取结构化信息方面起着关键作用。本文探讨了当前结构化实体提取方法的挑战和限制，并引入了一种新的方法来解决这些问题。我们首先介绍和规范化了结构化实体提取（SEE）任务，然后提出了适用于该任务的近似实体集重叠（AESOP）度量，以适当评估模型在这一任务上的性能。随后，我们提出了一种新模型，通过将整个提取任务分解为多个阶段，利用LLMs的强大功能来提高效果和效率。定量评估和人工并行评估证实了我们的模型优于基准模型，为结构化实体提取领域的未来进展提供了有希望的方向。

    Recent advances in machine learning have significantly impacted the field of information extraction, with Large Language Models (LLMs) playing a pivotal role in extracting structured information from unstructured text. This paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues. We contribute to the field by first introducing and formalizing the task of Structured Entity Extraction (SEE), followed by proposing Approximate Entity Set OverlaP (AESOP) Metric designed to appropriately assess model performance on this task. Later, we propose a new model that harnesses the power of LLMs for enhanced effectiveness and efficiency through decomposing the entire extraction task into multiple stages. Quantitative evaluation and human side-by-side evaluation confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction.
    
[^37]: 我的模型会忘记什么？语言模型改进中的被遗忘实例预测

    What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement

    [https://arxiv.org/abs/2402.01865](https://arxiv.org/abs/2402.01865)

    本文研究了语言模型更新中的遗忘现象，提出了一种预测上游实例遗忘的方法，以改进重播过程的可控性和解释性。根据预训练实例的预-softmax对数几率分数变化与在线学习实例的相似性，提出了一种部分可解释的预测模型，在BART模型上表现良好但在T5模型上失败。此外，还展示了基于内积的黑盒分类器。

    

    在实际应用中，语言模型会出现错误。然而，仅仅通过将模型更新为纠正错误实例，会导致灾难性的遗忘，更新后的模型在指导微调或上游训练阶段中学到的实例上出现错误。随机重播上游数据的效果不令人满意，往往伴随着较高的方差和较差的可控性。为了改善重播过程的可控性和解释性，我们试图预测由于模型更新而遗忘的上游实例。我们根据一组在线学习的实例和相应被遗忘的上游预训练实例训练预测模型。我们提出了一种部分可解释的预测模型，该模型基于这样的观察结果：预训练实例的预-softmax对数几率分数的变化类似于在线学习实例的变化，这在BART模型上表现出不错的效果，但在T5模型上失败。我们进一步展示了基于内积的黑盒分类器

    Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting -- the updated model makes errors on instances learned during the instruction tuning or upstream training phase. Randomly replaying upstream data yields unsatisfactory performance and often comes with high variance and poor controllability. To this end, we try to forecast upstream examples that will be forgotten due to a model update for improved controllability of the replay process and interpretability. We train forecasting models given a collection of online learned examples and corresponding forgotten upstream pre-training examples. We propose a partially interpretable forecasting model based on the observation that changes in pre-softmax logit scores of pretraining examples resemble that of online learned examples, which performs decently on BART but fails on T5 models. We further show a black-box classifier based on inner products 
    
[^38]: CDEval：一个用于衡量大规模语言模型文化维度的基准

    CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models

    [https://arxiv.org/abs/2311.16421](https://arxiv.org/abs/2311.16421)

    CDEval是一个新的基准，用于评估大规模语言模型的文化维度。通过对六个文化维度和七个领域的全面实验，我们揭示了主流语言模型的文化特征、一致性和差异。这些发现强调了在开发语言模型时融入文化考虑的重要性。

    

    随着大规模语言模型（LLMs）的扩展显著增强其能力，对确保其负责任和伦理使用的对齐问题越来越受关注。尽管现有的对齐工作主要集中在普世价值观（如HHH原则）上，但文化这一本质上是众多多元化的维度却未得到充分关注。本研究引入了一个新的基准CDEval，旨在评估LLMs的文化维度。CDEval通过结合GPT-4的自动生成和人工验证构建，在七个领域涵盖了六个文化维度。我们全面的实验为主流LLMs的文化提供了有趣的洞察，突出了不同维度和领域之间的一致性和差异。研究结果强调了在LLM的开发中整合文化考虑的重要性，特别是在多元文化环境中的应用。

    As the scaling of Large Language Models (LLMs) has dramatically enhanced their capabilities, there has been a growing focus on the alignment problem to ensure their responsible and ethical use. While existing alignment efforts predominantly concentrate on universal values such as the HHH principle, the aspect of culture, which is inherently pluralistic and diverse, has not received adequate attention. This work introduces a new benchmark, CDEval, aimed at evaluating the cultural dimensions of LLMs. CDEval is constructed by incorporating both GPT-4's automated generation and human verification, covering six cultural dimensions across seven domains. Our comprehensive experiments provide intriguing insights into the culture of mainstream LLMs, highlighting both consistencies and variations across different dimensions and domains. The findings underscore the importance of integrating cultural considerations in LLM development, particularly for applications in diverse cultural settings. Thr
    
[^39]: 缺失模态下的多模态情感分析:一种知识迁移方法

    Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach. (arXiv:2401.10747v1 [cs.SD])

    [http://arxiv.org/abs/2401.10747](http://arxiv.org/abs/2401.10747)

    本文提出了一种知识迁移方法，用于在缺失模态下进行多模态情感分析。通过翻译不同模态之间的内容以重构缺失的音频模态，并利用跨模态注意机制进行情感预测，实验证明了该方法在多个数据集上表现出显著的改进和与完整多模态监督方法相媲美的效果。

    

    多模态情感分析旨在通过视觉、语言和声音线索来识别个体表达的情绪。然而，现有研究大多假设在训练和测试过程中所有模态都是可用的，这使得它们的算法容易受到缺失模态的影响。在本文中，我们提出了一种新颖的知识迁移网络，用于在不同模态之间进行翻译，以重构缺失的音频模态。此外，我们还开发了一种跨模态注意机制，以保留重构和观察到的模态的最大信息，用于情感预测。在三个公开数据集上进行的大量实验证明了相对于基线算法的显著改进，并实现了与具有完整多模态监督的先前方法相媲美的结果。

    Multimodal sentiment analysis aims to identify the emotions expressed by individuals through visual, language, and acoustic cues. However, most of the existing research efforts assume that all modalities are available during both training and testing, making their algorithms susceptible to the missing modality scenario. In this paper, we propose a novel knowledge-transfer network to translate between different modalities to reconstruct the missing audio modalities. Moreover, we develop a cross-modality attention mechanism to retain the maximal information of the reconstructed and observed modalities for sentiment prediction. Extensive experiments on three publicly available datasets demonstrate significant improvements over baselines and achieve comparable results to the previous methods with complete multi-modality supervision.
    
[^40]: DeepEdit: 带有约束的解码式知识编辑

    DeepEdit: Knowledge Editing as Decoding with Constraints. (arXiv:2401.10471v1 [cs.CL])

    [http://arxiv.org/abs/2401.10471](http://arxiv.org/abs/2401.10471)

    DeepEdit是一种神经符号方法，通过更好的推理一致性和对更新知识的意识，提高了大型语言模型的知识编辑能力，对多跳问题数据集MQuaKE取得了显著的进展。

    

    我们将大型语言模型（LLMs）的知识编辑视为带有约束的解码过程。我们提出了DeepEdit（基于深度优先搜索的渐进式解码知识编辑），这是一种神经符号方法，通过更好的推理一致性、问题相关性和对更新知识的意识来改进知识编辑。DeepEdit可灵活应用于所有黑盒LLMs：不需要访问模型参数、表示或输出词汇分布。DeepEdit逐步产生高质量的推理步骤，以实现有效的知识编辑。它利用深度优先搜索来修改LLMs的输出，从而提高输出对问题的相关性和对更新知识的意识。在知识编辑方面，DeepEdit在控制LLMs产生更简洁的推理方面表现出色。在MQuaKE上，DeepEdit在定量上取得了显著的进展，这是一个具有挑战性的多跳问题数据集。

    We develop a new perspective of knowledge editing for large language models (LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that improves knowledge editing with better coherence of reasoning, relevance to the question, and awareness of updated knowledge. DeepEdit can be flexibly applied to all black-box LLMs: it does not require any access to the model parameters, representations, or output vocabulary distributions. DeepEdit progressively produces the high-quality reasoning steps towards effective knowledge editing. It utilizes a depth-first search to revise the LLMs' output, which improves the output's informativeness to the input question and awareness of the updated knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit yields significant gains on MQuaKE, a challenging multi-hop que
    
[^41]: 通过复杂逻辑假设生成在知识图谱中推进诱导推理

    Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation. (arXiv:2312.15643v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.15643](http://arxiv.org/abs/2312.15643)

    这篇论文介绍了一种复杂逻辑假设生成的任务，作为实现与知识图谱的诱导逻辑推理的初始步骤。研究发现，过监督训练的生成模型可以生成结构上更接近参考假设的逻辑假设。为了解决推广到未见过观察结果的问题，引入了基于知识图谱的强化学习方法（RLF-KG），最小化观察结果与结论之间的差异。

    

    诱导推理是通过做出有根据的猜测来解释观察结果的过程。尽管许多应用需要使用知识进行解释，但将诱导推理与结构化知识（如知识图谱）结合使用的方法仍然尚未得到广泛探索。为了填补这一空白，本文介绍了复杂逻辑假设生成的任务，作为实现与知识图谱的诱导逻辑推理的初始步骤。在这个任务中，我们的目标是生成一个复杂的逻辑假设，以解释一组观察结果。我们发现，经过监督训练的生成模型可以生成结构上更接近参考假设的逻辑假设。然而，当推广到未见过的观察结果时，这种训练目标并不能保证更好的假设生成。为了解决这个问题，我们引入了基于知识图谱的强化学习方法（RLF-KG），该方法最小化观察结果与结论之间的差异。

    Abductive reasoning is the process of making educated guesses to provide explanations for observations. Although many applications require the use of knowledge for explanations, the utilization of abductive reasoning in conjunction with structured knowledge, such as a knowledge graph, remains largely unexplored. To fill this gap, this paper introduces the task of complex logical hypothesis generation, as an initial step towards abductive logical reasoning with KG. In this task, we aim to generate a complex logical hypothesis so that it can explain a set of observations. We find that the supervised trained generative model can generate logical hypotheses that are structurally closer to the reference hypothesis. However, when generalized to unseen observations, this training objective does not guarantee better hypothesis generation. To address this, we introduce the Reinforcement Learning from Knowledge Graph (RLF-KG) method, which minimizes differences between observations and conclusio
    
[^42]: LLM-SQL-Solver: LLM能够确定SQL等价关系吗？

    LLM-SQL-Solver: Can LLMs Determine SQL Equivalence?. (arXiv:2312.10321v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2312.10321](http://arxiv.org/abs/2312.10321)

    本研究探讨了LLM是否能够确定两个SQL查询的等价关系，并提出了两种提示技术来帮助LLM生成高质量的响应。

    

    判断两个SQL查询之间的等价关系是数据管理和SQL生成中的一个基本问题，具有许多实际应用（即，在文本到SQL任务中评估生成的SQL查询的质量）。虽然研究界多年来一直在考虑SQL的等价性，但它存在相当大的困难，并且没有完整的解决方案。最近，大型语言模型（LLMs）在对话、问答和解决数学问题方面展现出强大的推理能力。在本文中，我们研究了LLMs是否可以用于确定两个SQL查询的等价性（语义等价和宽松等价）。为了帮助LLMs生成高质量的响应，我们提出了两种提示技术：Miniature & Mull和Explain & Compare。前一种技术被用于评估语义等价性，它要求LLMs在简单的数据库实例上执行查询，然后探索是否存在反例。

    Judging the equivalence between two SQL queries is a fundamental problem with many practical applications in data management and SQL generation (i.e., evaluating the quality of generated SQL queries in text-to-SQL task). While the research community has reasoned about SQL equivalence for decades, it poses considerable difficulties and no complete solutions exist. Recently, Large Language Models (LLMs) have shown strong reasoning capability in conversation, question answering and solving mathematics challenges. In this paper, we study if LLMs can be used to determine the equivalence between SQL queries under two notions of SQL equivalence (semantic equivalence and relaxed equivalence). To assist LLMs in generating high quality responses, we present two prompting techniques: Miniature & Mull and Explain & Compare. The former technique is used to evaluate the semantic equivalence in which it asks LLMs to execute a query on a simple database instance and then explore if a counterexample ex
    
[^43]: CompA: 解决音频-语言模型中的组合推理差距

    CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models. (arXiv:2310.08753v1 [cs.SD])

    [http://arxiv.org/abs/2310.08753](http://arxiv.org/abs/2310.08753)

    CompA提出了由两个专家注释的音频-语言模型组合推理基准数据集，用于评估ALMs在理解音频中声音事件的顺序和属性绑定方面的表现。

    

    音频的基本特性是其组合性。使用对比方法（例如CLAP）训练的音频-语言模型（ALMs）能够学习音频和语言模态之间的共享表示，从而在许多下游应用中提高性能，包括零样本音频分类、音频检索等。然而，这些模型在有效执行组合推理方面的能力还很少被探索，需要进一步的研究。本文提出了CompA，这是一个由两个专家注释的基准数据集，其中大多数是真实世界的音频样本，用于评估ALMs的组合推理能力。我们的CompA-order评估ALMs在理解音频中声音事件的顺序或发生时的表现如何，而CompA-attribute评估声音事件的属性绑定。每个基准数据集中的实例包含两个音频-标题对，其中两个音频具有相同的声音事件，但组合方式不同。

    A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose CompA, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. A
    
[^44]: DKEC: 领域知识增强的电子病历多标签分类

    DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records. (arXiv:2310.07059v1 [cs.CL])

    [http://arxiv.org/abs/2310.07059](http://arxiv.org/abs/2310.07059)

    本文提出了DKEC，一种领域知识增强的分类器，用于医学诊断预测。它利用标签的注意力机制和组内训练方法来捕捉医学实体之间的语义关系，并增加罕见类别的样本数量。评估结果显示其在医学数据集上表现良好。

    

    医学领域的多标签文本分类任务经常面临长尾标签分布，即罕见类别的训练样本少于频繁类别。虽然之前的工作已经探索了不同的模型架构和层次化标签结构来找到重要特征，但大多数忽略了从医学指南中融入领域知识。本文提出了DKEC，一种增强医学诊断预测的领域知识增强分类器，其中包括两个创新点：（1）一个基于标签的注意力机制，结合异构图和领域本体来捕捉医学实体之间的语义关系，（2）一种基于标签相似性的简单而有效的组内训练方法，用于增加罕见类别的样本数量。我们在两个真实的医学数据集上评估了DKEC：RAA数据集，包含来自急救服务（EMS）事件的4,417个患者护理报告的收集，和来自53898报告的子集。

    Multi-label text classification (MLTC) tasks in the medical domain often face long-tail label distribution, where rare classes have fewer training samples than frequent classes. Although previous works have explored different model architectures and hierarchical label structures to find important features, most of them neglect to incorporate the domain knowledge from medical guidelines. In this paper, we present DKEC, Domain Knowledge Enhanced Classifier for medical diagnosis prediction with two innovations: (1) a label-wise attention mechanism that incorporates a heterogeneous graph and domain ontologies to capture the semantic relationships between medical entities, (2) a simple yet effective group-wise training method based on similarity of labels to increase samples of rare classes. We evaluate DKEC on two real-world medical datasets: the RAA dataset, a collection of 4,417 patient care reports from emergency medical services (EMS) incidents, and a subset of 53,898 reports from the 
    
[^45]: 实例和标签: 针对层次化多标签文本分类的层次感知联合监督对比学习

    Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification. (arXiv:2310.05128v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05128](http://arxiv.org/abs/2310.05128)

    提出了一种层次感知联合监督对比学习方法（HJCL），用于层次化多标签文本分类。该方法通过使用实例级和标签级对比学习技术，以及精心构建批次来处理标签层次结构，解决了在HMTC中使用监督对比学习的问题。

    

    层次化多标签文本分类（HMTC）旨在利用标签层次结构进行多标签分类。近期关于HMTC的方法采用对比学习在生成的样本上以半监督的方式将文本和标签嵌入接近，从而解决了对输出空间施加过度约束的问题。然而，样本的生成往往引入噪声，因为它忽略了同一批次中相似样本之间的相关性。解决这个问题的一个方法是使用监督对比学习，但由于其复杂的结构化标签，这仍然是一个未被充分研究的领域。为了克服这一挑战，我们提出了一种称为$\textbf{HJCL}$的层次感知联合监督对比学习方法，用于填补监督对比学习和HMTC之间的差距。具体而言，我们采用实例级和标签级对比学习技术，并仔细构造批次来满足标签层次结构的要求。

    Hierarchical multi-label text classification (HMTC) aims at utilizing a label hierarchy in multi-label classification. Recent approaches to HMTC deal with the problem of imposing an over-constrained premise on the output space by using contrastive learning on generated samples in a semi-supervised manner to bring text and label embeddings closer. However, the generation of samples tends to introduce noise as it ignores the correlation between similar samples in the same batch. One solution to this issue is supervised contrastive learning, but it remains an underexplored topic in HMTC due to its complex structured labels. To overcome this challenge, we propose $\textbf{HJCL}$, a $\textbf{H}$ierarchy-aware $\textbf{J}$oint Supervised $\textbf{C}$ontrastive $\textbf{L}$earning method that bridges the gap between supervised contrastive learning and HMTC. Specifically, we employ both instance-wise and label-wise contrastive learning techniques and carefully construct batches to fulfill the 
    
[^46]: 学习个性化故事评估

    Learning Personalized Story Evaluation. (arXiv:2310.03304v1 [cs.CL])

    [http://arxiv.org/abs/2310.03304](http://arxiv.org/abs/2310.03304)

    该论文提出了学习个性化故事评估的方法。为了解决大型语言模型在开放式文本生成任务的评估问题，论文创建了两个新的数据集，并开发了一个个性化故事评估模型，能够根据评审人员的示例评价进行个性化评估。

    

    尽管大型语言模型（LLM）在诸如问答和检索等更客观的任务上显示出令人印象深刻的结果，但评估它们在开放式文本生成方面的表现仍然是一个困难的问题，原因包括（1）数据污染；（2）多维评估标准；以及（3）来自评审人员个人偏好的主观性。为了解决这些问题，我们提出在一个无污染的开放式生成评估中建模个性化。我们使用适当的匿名化和新的个性化标签，重新利用现有数据集创建了两个新的数据集Per-MPST和Per-DOC用于个性化故事评估。我们进一步开发了一个个性化故事评估模型PERSE来推测评审人员的偏好，并提供个性化评估。具体而言，对于某个评审人员的一些示例评价，PERSE可以预测该评审人员在新的情节上的详细评审或细粒度比较（如趣味性和惊喜）。

    While large language models (LLMs) have shown impressive results for more objective tasks such as QA and retrieval, it remains nontrivial to evaluate their performance on open-ended text generation for reasons including (1) data contamination; (2) multi-dimensional evaluation criteria; and (3) subjectiveness stemming from reviewers' personal preferences. To address such issues, we propose to model personalization in an uncontaminated open-ended generation assessment. We create two new datasets Per-MPST and Per-DOC for personalized story evaluation, by re-purposing existing datasets with proper anonymization and new personalized labels. We further develop a personalized story evaluation model PERSE to infer reviewer preferences and provide a personalized evaluation. Specifically, given a few exemplary reviews from a particular reviewer, PERSE predicts either a detailed review or fine-grained comparison in several aspects (such as interestingness and surprise) for that reviewer on a new 
    
[^47]: 图像-文本多模型综述论文

    A Survey on Image-text Multimodal Models. (arXiv:2309.15857v1 [cs.CL])

    [http://arxiv.org/abs/2309.15857](http://arxiv.org/abs/2309.15857)

    图像-文本多模型综述论文全面回顾了其发展历程和当前状态，提出了新的分类方法，并阐明了该领域的挑战和潜在研究方向。

    

    在人工智能不断发展的背景下，图像和文本信息的融合成为一个至关重要的领域，导致了图像-文本多模型的出现。本论文全面回顾了图像-文本多模型的发展历程和当前状态，探讨了它们的应用价值、挑战和潜在研究方向。首先，我们重新审视了这些模型的基本概念和发展里程碑，引入了一种新的分类方法，将它们的发展分为三个不同的阶段，基于它们被引入的时间和对学科的影响。此外，基于任务在学术领域中的重要性和普及性，我们提出了将与图像-文本多模型相关的任务划分为五个主要类型的分类方法，阐明了每个类别内的最新进展和关键技术。尽管这些模型取得了显著的成就，但仍面临着许多挑战。

    Amidst the evolving landscape of artificial intelligence, the convergence of visual and textual information has surfaced as a crucial frontier, leading to the advent of image-text multimodal models. This paper provides a comprehensive review of the evolution and current state of image-text multimodal models, exploring their application value, challenges, and potential research trajectories. Initially, we revisit the basic concepts and developmental milestones of these models, introducing a novel classification that segments their evolution into three distinct phases, based on their time of introduction and subsequent impact on the discipline. Furthermore, based on the tasks' significance and prevalence in the academic landscape, we propose a categorization of the tasks associated with image-text multimodal models into five major types, elucidating the recent progress and key technologies within each category. Despite the remarkable accomplishments of these models, numerous challenges a
    
[^48]: CoT-BERT: 通过思维链条增强无监督句子表示

    CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought. (arXiv:2309.11143v1 [cs.CL])

    [http://arxiv.org/abs/2309.11143](http://arxiv.org/abs/2309.11143)

    CoT-BERT提出了一种通过思维链条增强无监督句子表示的方法，通过两个阶段的处理，引入思维链条的概念进行向量化，以提高模型性能。

    

    无监督句子表示学习旨在将输入句子转化为富含复杂语义信息的固定长度向量，同时消除对标注数据的依赖。近年来，在对比学习和提示工程的推动下，该领域取得了显著进展，极大地缩小了无监督和有监督策略之间的差距。然而，在这个轨迹中，仍然没有充分利用思维链条的潜在能力。为了释放预训练模型（如BERT）中的潜能，我们提出了一个句子表示的两阶段方法：理解和摘要。随后，后一阶段的输出被利用为输入句子的向量化表示。为了进一步提高性能，我们对对比学习损失函数和模板去噪技术进行了精细调整。严格的实验验证了我们的方法CoT-BERT的优越性。

    Unsupervised sentence representation learning aims to transform input sentences into fixed-length vectors enriched with intricate semantic information while obviating the reliance on labeled data. Recent progress within this field, propelled by contrastive learning and prompt engineering, has significantly bridged the gap between unsupervised and supervised strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains largely untapped within this trajectory. To unlock latent capabilities within pre-trained models, such as BERT, we propose a two-stage approach for sentence representation: comprehension and summarization. Subsequently, the output of the latter phase is harnessed as the vectorized representation of the input sentence. For further performance enhancement, we meticulously refine both the contrastive learning loss function and the template denoising technique for prompt engineering. Rigorous experimentation substantiates our method, CoT-BERT, transcending a
    
[^49]: 调查LLMs中更微妙的偏见：生成模型中的年龄主义、美丽、机构和国籍偏见

    Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models. (arXiv:2309.08902v1 [cs.CL])

    [http://arxiv.org/abs/2309.08902](http://arxiv.org/abs/2309.08902)

    本文调查了LLMs在年龄、美丽、机构和国籍等少研究但仍然重要的维度上的偏见，通过衡量在社会群体和不相关的正负属性之间做出的微妙相关决策。研究发现LLMs在特定社会群体上存在类似于“美丽即善”的广泛正面或负面态度的偏见。

    

    LLMs越来越强大并广泛用于辅助用户完成各种任务。这种使用可能会将LLM偏见引入到重要决策中，如招聘、人员绩效评估和刑事判决。在NLP系统中的性别和种族等方面的偏见已得到广泛研究，尤其是针对特定刻板印象的偏见（例如，亚洲人擅长数学）。在本文中，我们研究了一些较少研究但仍然重要的维度上的偏见，如年龄和美丽，在LLMs（特别是自回归语言模型）在社会群体和不相关的正负属性之间做出更微妙的相关决策。我们问LLMs是否对特定社会群体持有广泛的正面或负面态度的偏见，类似于实验心理学中人们发现的“美丽即善”的偏见。我们引入了一个模板生成的句子完成任务的数据集，要求模型选择最合适的属性。

    LLMs are increasingly powerful and widely used to assist users in a variety of tasks. This use risks the introduction of LLM biases to consequential decisions such as job hiring, human performance evaluation, and criminal sentencing. Bias in NLP systems along the lines of gender and ethnicity has been widely studied, especially for specific stereotypes (e.g., Asians are good at math). In this paper, we investigate bias along less studied, but still consequential, dimensions, such as age and beauty, measuring subtler correlated decisions that LLMs (specially autoregressive language models) make between social groups and unrelated positive and negative attributes. We ask whether LLMs hold wide-reaching biases of positive or negative sentiment for specific social groups similar to the ``what is beautiful is good'' bias found in people in experimental psychology. We introduce a template-generated dataset of sentence completion tasks that asks the model to select the most appropriate attrib
    
[^50]: 在新闻分析中连接各个点：关于媒体偏见和框架的跨学科调查

    Connecting the Dots in News Analysis: A Cross-Disciplinary Survey of Media Bias and Framing. (arXiv:2309.08069v1 [cs.CL])

    [http://arxiv.org/abs/2309.08069](http://arxiv.org/abs/2309.08069)

    这篇综述论文回顾了社会科学方法和自然语言处理方法在分析媒体偏见方面的差异，并提出了解决当前方法论鸿沟的可能方向，包括模型透明度、考虑文档外部信息和跨文档推理。

    

    偏见在新闻报道中的表现和影响是社会科学几十年来的核心议题，并且近年来在自然语言处理领域引起了越来越多的关注。虽然自然语言处理可以帮助扩大分析规模或提供自动化程序来调查偏见新闻对社会的影响，但我们认为目前主导地位的方法论未能解决理论媒体研究中涉及的复杂问题和影响。在这篇综述论文中，我们回顾了社会科学方法，并将其与自然语言处理领域中用于分析媒体偏见的典型任务形式、方法和评估指标进行了比较。我们讨论了开放性问题，并提出了解决理论模型和预测模型以及它们的评估之间鸿沟的可能方向。这些包括模型的透明度、考虑文档外部信息、以及跨文档推理而非单标签分配。

    The manifestation and effect of bias in news reporting have been central topics in the social sciences for decades, and have received increasing attention in the NLP community recently. While NLP can help to scale up analyses or contribute automatic procedures to investigate the impact of biased news in society, we argue that methodologies that are currently dominant fall short of addressing the complex questions and effects addressed in theoretical media studies. In this survey paper, we review social science approaches and draw a comparison with typical task formulations, methods, and evaluation metrics used in the analysis of media bias in NLP. We discuss open questions and suggest possible directions to close identified gaps between theory and predictive models, and their evaluation. These include model transparency, considering document-external information, and cross-document reasoning rather than single-label assignment.
    
[^51]: 使用图链接预测在生活方式vlog中的人类动作共现

    Human Action Co-occurrence in Lifestyle Vlogs using Graph Link Prediction. (arXiv:2309.06219v1 [cs.CV])

    [http://arxiv.org/abs/2309.06219](http://arxiv.org/abs/2309.06219)

    该论文提出了自动识别人类动作共现的任务，并创建了ACE数据集以及相应的代码。通过利用视觉和文本信息的图链接预测模型，可以有效捕捉不同数据域中的人类动作关系。

    

    我们介绍了自动识别人类动作共现的任务，即确定两个人类动作是否可以在同一时间间隔内共现。我们创建并公开了ACE（Action Co-occurrencE）数据集，该数据集由约12k个共现的视觉动作对和它们对应的视频片段组成的大型图形。我们描述了利用视觉和文本信息来自动推断两个动作是否共现的图链接预测模型。我们证明了图形特别适合捕捉人类动作之间的关系，并且所学习的图形表示对于我们的任务是有效的，并且在不同的数据域中捕捉到新颖而相关的信息。本文介绍的ACE数据集和代码可在https://github.com/MichiganNLP/vlog_action_co-occurrence公开获取。

    We introduce the task of automatic human action co-occurrence identification, i.e., determine whether two human actions can co-occur in the same interval of time. We create and make publicly available the ACE (Action Co-occurrencE) dataset, consisting of a large graph of ~12k co-occurring pairs of visual actions and their corresponding video clips. We describe graph link prediction models that leverage visual and textual information to automatically infer if two actions are co-occurring. We show that graphs are particularly well suited to capture relations between human actions, and the learned graph representations are effective for our task and capture novel and relevant information across different data domains. The ACE dataset and the code introduced in this paper are publicly available at https://github.com/MichiganNLP/vlog_action_co-occurrence.
    
[^52]: 用于中文医学标点修复的小型快速BERT模型

    A Small and Fast BERT for Chinese Medical Punctuation Restoration. (arXiv:2308.12568v1 [cs.CL])

    [http://arxiv.org/abs/2308.12568](http://arxiv.org/abs/2308.12568)

    该论文提出了一种用于中文医学标点修复的快速小型BERT模型。通过结合监督对比学习和辅助预训练任务，该模型在具有较小模型大小的情况下，能够实现与最先进的中文RoBERTa模型相当的95%性能。

    

    在临床听写中，没有明确标点符号的自动语音识别（ASR）导致了对听写报告的误解。为了使用ASR提供精确和易懂的临床报告，需要进行自动标点修复。考虑到实际情况，我们提出了一种基于“预训练和微调”范式的快速轻量级预训练模型，用于中文医学标点修复。在这项工作中，我们通过结合监督对比学习和一种新颖的辅助预训练任务（标点符号预测）来提炼预训练模型，使其适用于标点修复。我们在各种提炼模型上的实验表明，相对于最先进的中文RoBERTa模型，我们的模型可以在10%的模型大小的情况下实现95%的性能。

    In clinical dictation, utterances after automatic speech recognition (ASR) without explicit punctuation marks may lead to the misunderstanding of dictated reports. To give a precise and understandable clinical report with ASR, automatic punctuation restoration is required. Considering a practical scenario, we propose a fast and light pre-trained model for Chinese medical punctuation restoration based on 'pretraining and fine-tuning' paradigm. In this work, we distill pre-trained models by incorporating supervised contrastive learning and a novel auxiliary pre-training task (Punctuation Mark Prediction) to make it well-suited for punctuation restoration. Our experiments on various distilled models reveal that our model can achieve 95% performance while 10% model size relative to state-of-the-art Chinese RoBERTa.
    
[^53]: 机器学习在信任与安全方面的挑战：一个针对虚假信息检测的案例研究

    The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection. (arXiv:2308.12215v1 [cs.LG])

    [http://arxiv.org/abs/2308.12215](http://arxiv.org/abs/2308.12215)

    本研究通过虚假信息检测为例，检查了机器学习在信任与安全问题中学术与实践之间的脱节，并发现了文献中存在的严重不足之处，包括任务不符合在线服务面临的挑战、数据集和模型评估不真实、评估不独立于模型训练等。在此基础上，提出了评估机器学习应用于信任与安全问题的建议。

    

    我们使用虚假信息检测作为案例研究，检查了在将机器学习应用于信任与安全问题上学术和实践之间的脱节。我们对该领域中270篇广受引用的论文进行了自动检测虚假信息的文献系统化，并对子集中的论文进行了数据和代码的可用性、设计失误、可复现性和泛化性等方面的研究。我们发现文献中存在严重的不足之处，这对所声称的性能和实用性提出了质疑。检测任务通常与在线服务真正面临的挑战有本质上的区别。数据集和模型评估通常不代表现实世界的情景，而且评估往往不独立于模型训练。数据和代码的可用性很差。模型在领域外的数据上泛化能力不强。基于这些结果，我们提出了评估机器学习应用于信任与安全问题的建议。

    We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is fo
    
[^54]: 推进神经信息检索中的持续终身学习：定义、数据集、框架和实证评估

    Advancing continual lifelong learning in neural information retrieval: definition, dataset, framework, and empirical evaluation. (arXiv:2308.08378v1 [cs.IR])

    [http://arxiv.org/abs/2308.08378](http://arxiv.org/abs/2308.08378)

    本文提出了一个系统的持续神经信息检索任务定义，并提供了一个模拟连续信息检索的多主题数据集。同时，还提出了一个全面的持续神经信息检索框架，能够防止灾难性遗忘并提高先前学习任务的性能。

    

    持续学习是指机器学习模型在学习和适应新信息的同时，不影响其在先前学习任务上的性能。尽管已有多项研究探讨了信息检索任务中的持续学习方法，但仍缺乏明确的任务定义，并且目前尚不清楚在这种背景下典型的学习策略的表现如何。为了应对这一挑战，本文提出了一种系统的持续神经信息检索任务定义，并提供了一个模拟连续信息检索的多主题数据集。随后，本文提出了一个全面的持续神经信息检索框架，包括典型检索模型和持续学习策略。实证评估结果表明，所提出的框架能够成功地防止神经信息检索中的灾难性遗忘，并提高先前学习任务的性能。结果表明，基于嵌入的检索方式较传统的基于索引的检索方式具有优势，并且持续学习策略能够有效地提升检索性能。

    Continual learning refers to the capability of a machine learning model to learn and adapt to new information, without compromising its performance on previously learned tasks. Although several studies have investigated continual learning methods for information retrieval tasks, a well-defined task formulation is still lacking, and it is unclear how typical learning strategies perform in this context. To address this challenge, a systematic task formulation of continual neural information retrieval is presented, along with a multiple-topic dataset that simulates continuous information retrieval. A comprehensive continual neural information retrieval framework consisting of typical retrieval models and continual learning strategies is then proposed. Empirical evaluations illustrate that the proposed framework can successfully prevent catastrophic forgetting in neural information retrieval and enhance performance on previously learned tasks. The results indicate that embedding-based retr
    
[^55]: 利用视觉-语言模型在医学图像分割中探索迁移学习

    Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models. (arXiv:2308.07706v1 [cs.CV])

    [http://arxiv.org/abs/2308.07706](http://arxiv.org/abs/2308.07706)

    本论文提出使用视觉-语言模型进行医学图像分割的迁移学习，并评估了其在医学领域的可迁移性。通过捕捉语义信息和引入新的图像描述变化，实现了对多样化医学图像的分割。

    

    医学图像分割在医学领域的各种临床应用中至关重要。尽管最先进的分割模型已被证明有效，但在这个任务中整合文本指导以增强视觉特征仍然是一个进展有限的领域。现有利用文本指导的分割模型主要在开放领域图像上训练，这引发了在医学领域直接应用的难题，需要手动介入或进行微调。为了解决这些挑战，我们提出使用多模态的视觉-语言模型从图像描述和图像中捕捉语义信息，使得能够对多样化的医学图像进行分割。该研究全面评估了现有的视觉-语言模型在多个数据集上的可迁移性，以评估其从开放领域向医学领域的迁移能力。此外，我们对数据集中以前未见图像的图像描述引入了变化，揭示了显著的变异。

    Medical Image Segmentation is crucial in various clinical applications within the medical domain. While state-of-the-art segmentation models have proven effective, integrating textual guidance to enhance visual features for this task remains an area with limited progress. Existing segmentation models that utilize textual guidance are primarily trained on open-domain images, raising concerns about their direct applicability in the medical domain without manual intervention or fine-tuning.  To address these challenges, we propose using multimodal vision-language models for capturing semantic information from image descriptions and images, enabling the segmentation of diverse medical images. This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field. Furthermore, we introduce variations of image descriptions for previously unseen images in the dataset, revealing notable variations 
    
[^56]: mBLIP: 多语言视觉-LLM的高效引导

    mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs. (arXiv:2307.06930v1 [cs.CV])

    [http://arxiv.org/abs/2307.06930](http://arxiv.org/abs/2307.06930)

    mBLIP是第一个多语言Vision-LLM，通过在消费级硬件上使用少量训练样例的计算上高效的方式获得。

    

    模块化的视觉-语言模型（Vision-LLM）将预训练的图像编码器与（预训练的）大型语言模型（LLM）对齐，是一种在计算上更高效的选择，可以代替从头开始训练大型视觉-语言模型的端到端训练方法，而后者对于大多数人来说成本太高。 Vision-LLM将LLM事后条件化为“理解”图像编码器的输出。随着现成的高质量英文图像-文本数据以及单语英语LLM的丰富性，研究重点已经放在仅英文的Vision-LLM上。而多语言视觉-语言模型仍然主要通过昂贵的端到端预训练获得，这导致了相对较小的模型，并且在有限的多语言图像数据上进行训练，同时补充了仅有文本的多语言语料库。在这项工作中，我们介绍了mBLIP，这是第一个多语言Vision-LLM，我们以计算上高效的方式获得，仅使用几百万个训练样例在消费级硬件上进行训练。

    Modular vision-language models (Vision-LLMs) align pretrained image encoders with (pretrained) large language models (LLMs), representing a computationally much more efficient alternative to end-to-end training of large vision-language models from scratch, which is prohibitively expensive for most. Vision-LLMs instead post-hoc condition LLMs to `understand' the output of an image encoder. With the abundance of readily available high-quality English image-text data as well as monolingual English LLMs, the research focus has been on English-only Vision-LLMs. Multilingual vision-language models are still predominantly obtained via expensive end-to-end pretraining, resulting in comparatively smaller models, trained on limited multilingual image data supplemented with text-only multilingual corpora. In this work, we present mBLIP, the first multilingual Vision-LLM, which we obtain in a computationally efficient manner -- on consumer hardware using only a few million training examples -- by 
    
[^57]: 通过翻译和注释融合改进低资源实体识别

    Better Low-Resource Entity Recognition Through Translation and Annotation Fusion. (arXiv:2305.13582v1 [cs.CL])

    [http://arxiv.org/abs/2305.13582](http://arxiv.org/abs/2305.13582)

    本研究提出了一种通过翻译和注释融合的框架，可以改进低资源语言文本的命名实体识别。通过TransFusion模型，可以在不同语言之间进行强大的预测，且在两个低资源命名实体识别数据集上表现一致优秀。

    

    预训练的多语言语言模型已经在跨语言转移方面实现了重大进展。然而，这些模型在从高资源语言转移至低资源语言时，通常表现出性能差异，特别是对于未被充分训练或未包含在预训练数据中的语言。受这些模型在高资源语言上表现优秀的启发，我们介绍了一个翻译和融合框架，该框架将低资源语言文本翻译成高资源语言进行注释，然后将注释融合回低资源语言。基于该框架，我们提出了TransFusion模型，该模型训练用于融合来自高资源语言的预测结果，以在低资源语言上进行强大的预测。我们在两个低资源命名实体识别（NER）数据集MasakhaNER2.0和LORELEI NER上评估了我们的方法，并展示了与最先进的跨语言NER基线的一致优秀性能。

    Pre-trained multilingual language models have enabled significant advancements in cross-lingual transfer. However, these models often exhibit a performance disparity when transferring from high-resource languages to low-resource languages, especially for languages that are underrepresented or not in the pre-training data. Motivated by the superior performance of these models on high-resource languages compared to low-resource languages, we introduce a Translation-and-fusion framework, which translates low-resource language text into a high-resource language for annotation using fully supervised models before fusing the annotations back into the low-resource language. Based on this framework, we present TransFusion, a model trained to fuse predictions from a high-resource language to make robust predictions on low-resource languages. We evaluate our methods on two low-resource named entity recognition (NER) datasets, MasakhaNER2.0 and LORELEI NER, covering 25 languages, and show consist
    
[^58]: 跳跃到结论：用线性变换简化Transformers

    Jump to Conclusions: Short-Cutting Transformers With Linear Transformations. (arXiv:2303.09435v1 [cs.CL])

    [http://arxiv.org/abs/2303.09435](http://arxiv.org/abs/2303.09435)

    本文提出了一种简单的方法，使用线性变换将隐藏表示转换为最终表示，绕过中间的Transformer计算。这种方法在语言模型的上下文中可“窥视”GPT-2和BERT的早期层表示，显示经常在早期层中LMs已经预测最终输出。

    

    基于Transformer的语言模型(LMs)在每个层次上都创建其输入的隐藏表示，但只使用最终层的表示进行预测。这使得模型的内部决策过程和中间表示的实用性变得模糊不清。为了阐明这一点，可以将隐藏表示转换为最终表示，绕过中间的Transformer计算。在本文中，我们提出了一种简单的方法，通过使用线性变换来进行这种转换。我们展示了我们的方法产生比目前流行的在最终层空间中检查所有层的隐藏表示的方法更准确的近似结果。此外，在语言建模的上下文中，我们的方法允许“窥视”GPT-2和BERT的早期层表示，显示经常在早期层中LMs已经预测最终输出。然后，我们展示了我们的方法对于最近的早期退出策略的实用性，表明当旨在……(原文截止)

    Transformer-based language models (LMs) create hidden representations of their inputs at every layer, but only use final-layer representations for prediction. This obscures the internal decision-making process of the model and the utility of its intermediate representations. One way to elucidate this is to cast the hidden representations as final representations, bypassing the transformer computation in-between. In this work, we suggest a simple method for such casting, by using linear transformations. We show that our approach produces more accurate approximations than the prevailing practice of inspecting hidden representations from all layers in the space of the final layer. Moreover, in the context of language modeling, our method allows "peeking" into early layer representations of GPT-2 and BERT, showing that often LMs already predict the final output in early layers. We then demonstrate the practicality of our method to recent early exit strategies, showing that when aiming, for
    

