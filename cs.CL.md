# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation](https://arxiv.org/abs/2403.17846) | 提出了一种用于语言驱动的机器人导航的分层开放词汇3D场景图映射方法，可以有效代表多层建筑并允许机器人在其中穿行。 |
| [^2] | [Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression](https://arxiv.org/abs/2403.15447) | 量化目前比剪枝更有效，可以同时实现效率和可信度，但剪枝会显著降低模型的可信度 |
| [^3] | [M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset](https://arxiv.org/abs/2403.14168) | 提出了一种新颖的M$^3$AV音视频学术讲座数据集，包含多模态、多体裁和高质量人工注释，可用于多种音视频识别任务 |
| [^4] | [PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency](https://arxiv.org/abs/2403.09732) | 提出了一个两阶段框架，通过引入参考增强表示和少样本演示，解决了在处理冗长的数据库信息和复杂用户意图时的挑战。 |
| [^5] | [Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?](https://arxiv.org/abs/2403.06833) | 本研究提出了一种形式化的度量来量化指令与数据分离现象，以及一种可以从模型黑盒输出计算的经验变量，并引入了新数据集SEP，用于评估 |
| [^6] | [Human vs. Machine: Language Models and Wargames](https://arxiv.org/abs/2403.03407) | 人工智能大型语言模型在战争游戏中与人类响应存在一致性，但也存在显著的差异，这表明在政策制定者交出自主权或听从基于AI的战略建议之前应谨慎对待。 |
| [^7] | [Decode Neural signal as Speech](https://arxiv.org/abs/2403.01748) | 本文在脑机接口领域探索了MEG信号的脑到文本转换，着重解决了以前主要集中在EEG上、使用“teacher-forcing”以及未完全自回归的问题。 |
| [^8] | [NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and Safety Adherence in Chinese Journalistic Editorial Applications](https://arxiv.org/abs/2403.00862) | NewsBench是一个评估LLMs在中国新闻写作水平和安全性遵从能力的基准框架，揭示了在创造性写作任务中LLMs相对不足的新闻伦理遵守方面的需求。 |
| [^9] | [Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models](https://arxiv.org/abs/2403.00231) | 提出了Multimodal ArXiv数据集，包括ArXivCap和ArXivQA，用于增强大型视觉-语言模型对科学理解的能力，ArXivQA通过科学图生成问题，显著提高了数学推理准确率。 |
| [^10] | [A Semantic Distance Metric Learning approach for Lexical Semantic Change Detection](https://arxiv.org/abs/2403.00226) | 提出了一种用于词汇语义变化检测的语义距离度量学习方法，通过使用两个阶段的学习方法和感知编码器，实现了在多语言中优于以往方法的表现。 |
| [^11] | [Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge](https://arxiv.org/abs/2402.19334) | 将带后门的模型与其他同类模型合并可以有效治疗后门漏洞，为后门攻击提供推理阶段的有效和高效防御 |
| [^12] | [Improving Open-Ended Text Generation via Adaptive Decoding](https://arxiv.org/abs/2402.18223) | 引入自适应解码机制，通过置信度动态确定生成过程中的候选集，在故事生成任务中实现了更高的MAUVE和多样性，保持一定的连贯性，优于现有算法。 |
| [^13] | [An Iterative Associative Memory Model for Empathetic Response Generation](https://arxiv.org/abs/2402.17959) | 提出了一种用于共情响应生成的迭代关联记忆模型，采用二阶交互注意机制迭代地捕捉相关词语，实现准确、细致地理解话语。 |
| [^14] | [Advancing Parameter Efficiency in Fine-tuning via Representation Editing](https://arxiv.org/abs/2402.15179) | RED通过表示编辑显著降低了可训练参数数量，实现了与完全参数微调和其他PEFT方法相当或更好的结果 |
| [^15] | [KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models](https://arxiv.org/abs/2402.15043) | 该论文引入了KIEval，一种知识引导式交互评估框架，通过LLM-powered "interactor"角色实现动态的抗污染评估 |
| [^16] | [Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning](https://arxiv.org/abs/2402.14856) | 该研究通过对大型语言模型对命题逻辑问题的响应进行评估，揭示出它们展现出与人类类似的推理模式和策略。 |
| [^17] | [CriticBench: Benchmarking LLMs for Critique-Correct Reasoning](https://arxiv.org/abs/2402.14809) | CriticBench是一个综合基准测试，旨在评估LLMs在批判和纠正推理方面的能力，发现批判性训练显著提升性能，逻辑任务更易于修正。 |
| [^18] | [Cleaner Pretraining Corpus Curation with Neural Web Scraping](https://arxiv.org/abs/2402.14652) | 使用神经网络网络抓取器NeuScraper可以从网页中提取干净的文本内容，并且实现了超过20%的改进，有助于提高语言模型的预训练质量 |
| [^19] | [Balanced Data Sampling for Language Model Training with Clustering](https://arxiv.org/abs/2402.14526) | 本文提出了一种名为ClusterClip Sampling的数据抽样方法，利用数据聚类平衡训练数据的文本分布，为更好的模型训练提供支持。 |
| [^20] | [Towards Building Multilingual Language Model for Medicine](https://arxiv.org/abs/2402.13963) | 本文提出了为医学领域构建多语言语言模型的三个关键贡献:构建了新的多语言医学语料库MMedC，提出了多语言医学多选问答基准MMedBench，并且通过在MMedC上进一步训练获得了性能优越的MMedLM 2模型。 |
| [^21] | [When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality](https://arxiv.org/abs/2402.13113) | 研究了如何重启增量式Transformer构建和更新内部状态，揭示了增量状态的顺序结构如何编码关于偏误效应及其解决方式的信息，为分析上下文化意义表示和依赖解析的双向编码器带来见解，并显示它们在修订方面的优势。 |
| [^22] | [Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space](https://arxiv.org/abs/2402.12026) | 通过对频率空间的分析，本文提出了一种多尺度低秩适应（MuScleLoRA）方法，用于解决在训练语言模型时受到后门攻击的问题。 |
| [^23] | [Comprehensive Cognitive LLM Agent for Smartphone GUI Automation](https://arxiv.org/abs/2402.11941) | 提出了全面认知LLM代理，通过全面环境感知和条件动作预测两种新方法系统性提高GUI自动化性能。 |
| [^24] | [Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models](https://arxiv.org/abs/2402.11900) | 本文系统调查了在大型语言模型中利用事实快捷方式进行多跳事实推理的可能性，并分析了这种快捷方式可能带来的风险。 |
| [^25] | [SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.11896) | SIBO提出了一个简单的增强器来增强参数高效微调技术，有效解决了Transformer-based LLMs中过度平滑的问题，并在多个基准数据集上取得了显著的性能提升。 |
| [^26] | [Can Large Multimodal Models Uncover Deep Semantics Behind Images?](https://arxiv.org/abs/2402.11281) | 该论文引入了一个名为DEEPEVAL的基准，用于评估大型多模态模型对视觉深层语义的能力，揭示了现有LMMs在深层语义理解方面与人类之间存在显著差距。 |
| [^27] | [I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments](https://arxiv.org/abs/2402.11192) | 将微调过程中的实际响应风格与大型语言模型固有风格相匹配能够产生更好的学习结果，开发的方法通过最小程度地调整模型响应来避免过拟合。 |
| [^28] | [Model Editing by Pure Fine-Tuning](https://arxiv.org/abs/2402.11078) | 纯微调通过优化条件似然、增加随机释义和事实的数据，在模型编辑中取得了不俗的表现。 |
| [^29] | [AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators](https://arxiv.org/abs/2402.11073) | 提出了一种新框架 AFaCTA，利用大型语言模型辅助事实性索赔的标注，提高了标注的效率和一致性。 |
| [^30] | [II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in Visual Question Answering](https://arxiv.org/abs/2402.11058) | II-MMR提出了一种新颖的方法，用于识别和改进视觉问答中的多模态多跳推理，通过引入答案预测引导的CoT提示和知识三元组引导的提示，实现对不同推理情况的分析和识别。 |
| [^31] | [Network Formation and Dynamics Among Multi-LLMs](https://arxiv.org/abs/2402.10659) | 分析了多个LLM在社交网络中的行为，发现它们在给定网络结构并被询问形成网络偏好时表现出与人类社交动态一致的原则。 |
| [^32] | [MiMiC: Minimally Modified Counterfactuals in the Representation Space](https://arxiv.org/abs/2402.09631) | 提出了一种新颖的对抗事实生成方法，利用闭式解决方案在表示空间中生成富有表达力的对抗事实，以减轻语言模型中的不良行为，该方法在地球移动问题方面提供理论上的保证，并对表示空间的几何组织进行改进。 |
| [^33] | [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353) | DoRA是一种将预训练权重分解为幅度和方向两个组成部分，并使用LoRA进行方向更新的低秩适应方法，通过增强学习能力和训练稳定性来提高对LLaMA，LLaVA和VL-B的微调性能。 |
| [^34] | [SyntaxShap: Syntax-aware Explainability Method for Text Generation](https://arxiv.org/abs/2402.09259) | 本文介绍了一种局部的、与模型无关的用于文本生成的可解释性方法SyntaxShap，其通过考虑文本数据中的语法来扩展Shapley值以解释序列到序列任务。通过采用博弈论方法，SyntaxShap只考虑由依赖树约束的联盟，与其他最新的可解释性方法相比具有良好的性能。 |
| [^35] | [SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks](https://arxiv.org/abs/2402.09025) | SLEB是一种通过消除冗余的Transformer块来优化LLM流程的新方法，它成功加速了LLM的推理过程。 |
| [^36] | [Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast](https://arxiv.org/abs/2402.08567) | Agent Smith提出了一种安全问题，即传染性越狱，该问题在多代理环境中可以通过简单的越狱一个代理来迅速感染所有代理并导致有害行为。 |
| [^37] | [Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering](https://arxiv.org/abs/2402.08277) | 这项工作探索了如何鲁棒地微调大型语言模型以提高答案的来源质量和答案归因能力，引入了数据生成流水线和四个测试集来评估模型的性能，并展示了在合成数据上微调可以改善内部和外部分布的性能。 |
| [^38] | [Lumos : Empowering Multimodal LLMs with Scene Text Recognition](https://arxiv.org/abs/2402.08017) | 本论文介绍了Lumos，它是第一个具备文本理解能力的多模式问答系统，通过运用场景文本识别组件，能够从第一人称视角图像中提取文本，并将其用于加强多模式大型语言模型的输入。研究过程中，作者克服了与文本识别质量、延迟和模型推断相关的多个挑战，并提供了全面的组件评估结果，展示了高质量和高效率的性能。 |
| [^39] | [Anchor-based Large Language Models](https://arxiv.org/abs/2402.07616) | 基于锚点的大型语言模型（AnLLM）通过引入创新的基于锚点的自注意力网络（AnSAN）和基于锚点的推理策略，将序列信息压缩到锚点标记中，减少键/值缓存，提高推理效率。 |
| [^40] | [OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2402.06044) | OpenToM是一个评估大型语言模型心理理解能力的全面基准，通过提供更长、更清晰的叙事故事、具有明确个性特征的角色、以及挑战模型对心理状态的理解能力的问题，揭示了现有模型在理解物理世界与心理世界的角色心理状态方面的优势和不足。 |
| [^41] | [Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey](https://arxiv.org/abs/2402.04854) | 该论文提出了一种分层树状知识图谱和推荐系统，帮助初学者研究者进行研究调研，填补了现有导航知识图谱的不足，并解决了学术论文推荐系统中高文本相似性带来的困惑。 |
| [^42] | [Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector](https://arxiv.org/abs/2402.04601) | 本文提出了一种提升对齐的中文语法错误修正器，旨在解决使用自回归生成模型时面临的过度修正挑战。该方法适用于序列到序列模型和仅解码的大型语言模型，并通过对齐模型进行多轮修正，提高修正效果。 |
| [^43] | [Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton](https://arxiv.org/abs/2402.04411) | 本文介绍了DFA-LLM（确定有限自动机增强的大规模语言模型）框架，通过嵌入从对话中学习到的确定有限自动机在大规模语言模型中，使得对话代理能够生成具有规范合规性的回复，并在广泛的基准测试中验证了其有效性。 |
| [^44] | [Linear-time Minimum Bayes Risk Decoding with Reference Aggregation](https://arxiv.org/abs/2402.04251) | 本文提出了一种线性时间的最小贝叶斯风险解码方法，通过使用聚合参考表示来近似配对度量分数，将复杂度降低到线性级别，同时在保持大部分质量增益的同时提高了解码的效率。 |
| [^45] | [Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization](https://arxiv.org/abs/2402.03161) | 这篇论文介绍了一种统一的视频语言预训练方法，通过解耦的视觉-运动标记化将视频表示为关键帧和时间运动，然后使用统一生成预训练技术来生成各种图像和视频内容。 |
| [^46] | [Graph-enhanced Large Language Models in Asynchronous Plan Reasoning](https://arxiv.org/abs/2402.02805) | 本研究是关于使用大型语言模型(LLMs)进行异步计划推理的首次大规模研究。我们发现在没有提供任务解决过程插图的情况下，现有的LLMs表现不佳。为此，我们提出了一种称为Plan Like a Graph (PLaG)的新技术，通过将图与自然语言提示相结合，实现了最先进的结果。然而，当任务复杂性增加时，LLMs仍然存在严重降级的问题，突显了LLMs在模拟数字设备方面的局限性。这项研究为将LLMs作为高效自主代理迈出了重要一步。 |
| [^47] | [KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models](https://arxiv.org/abs/2402.02801) | KS-Lottery是一种寻找多语言语言模型中有效参数的方法，通过使用Kolmogorov-Smirnov检验来分析参数分布偏移，并证明了在嵌入层中可以找到认证的中奖票。这种方法可以在微调中获得与全面微调相当的性能，同时减少了所需的参数数量。 |
| [^48] | [Increasing Trust in Language Models through the Reuse of Verified Circuits](https://arxiv.org/abs/2402.02619) | 本文介绍了一种通过重复使用经过验证的电路来增加语言模型的可信度的方法。研究者通过构建数学和逻辑规范的框架，并对一个n位整数加法模型进行完全验证。他们插入训练好的加法模型到一个未经训练的模型中，通过训练组合模型执行加法和减法。他们发现加法电路在这两个任务中得到了广泛的重复使用，从而简化了减法模型的验证。 |
| [^49] | [Discovering More Effective Tensor Network Structure Search Algorithms via Large Language Models (LLMs)](https://arxiv.org/abs/2402.02456) | 通过大型语言模型（LLMs），我们开发了GPTN-SS算法，用于自动设计更有效的张量网络结构搜索算法。实验证明，这些算法在探索和开发之间取得了更好的平衡，并在搜索高质量的TN结构方面展现出卓越的性能。 |
| [^50] | [Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction](https://arxiv.org/abs/2402.02416) | Aligner是一种通过学习校正残差来实现高效对齐的方法，相比于传统的强化学习方法，Aligner具有参数高效、弱到强泛化以及即插即用的优势。 |
| [^51] | [The Political Preferences of LLMs](https://arxiv.org/abs/2402.01789) | 该研究对LLM的政治偏好进行了分析，结果发现大多数对话型LLM表现出左翼观点，但需谨慎解读基础模型在政治倾向测试中的分类。 |
| [^52] | [Prompt-Driven LLM Safeguarding via Directed Representation Optimization](https://arxiv.org/abs/2401.18018) | 通过研究模型表示的影响，我们发现安全提示并没有明显增强恶意和无害查询之间的区分，并提出了一种名为DRO的方法，用于自动优化安全提示。 |
| [^53] | [Arrows of Time for Large Language Models](https://arxiv.org/abs/2401.17505) | 这篇论文通过研究自回归大型语言模型的时间方向性，发现了模型在建模自然语言能力上存在时间上的不对称性。从信息理论的角度来看，这种差异理论上是不应该存在的。通过稀疏性和计算复杂性的考虑，提供了一个理论框架来解释这种不对称性的出现。 |
| [^54] | [LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation](https://arxiv.org/abs/2401.17244) | LLaMP是一个多模态的检索增强生成框架，能够在不进行微调的情况下，理解和集成各种材料科学概念的能力，检索相关数据，处理高阶数据以及总结固态合成过程。同时，LLaMP有效纠正了GPT-3.5内部知识的错误。 |
| [^55] | [Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios](https://arxiv.org/abs/2401.17167) | 本论文介绍了一种新的基准测试UltraTool，旨在改善和评估LLMs在实际复杂场景中的工具利用能力。该基准测试关注从规划和创建到应用工具的整个过程，并强调实际复杂性和多步规划的要求。 |
| [^56] | [ProLex: A Benchmark for Language Proficiency-oriented Lexical Substitution](https://arxiv.org/abs/2401.11356) | ProLex是一个以语言熟练度为导向的词汇替换的评估基准，旨在评估生成适当替代词和表现更好语言熟练度的系统能力。使用微调任务特定合成数据的Llama2-13B模型在F分数上优于ChatGPT 3.2%，与GPT-4在ProLex上表现相当。 |
| [^57] | [MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception](https://arxiv.org/abs/2401.07529) | 本论文提出了一个新的基准MM-SAP，旨在评估多模态大型语言模型在感知中的自我意识能力，填补了先前研究中忽视的领域。 |
| [^58] | [Graph Language Models](https://arxiv.org/abs/2401.07105) | 引入了一种新型的图语言模型（GLM），结合线性化和图神经网络的优点，解决了传统方法在处理结构化知识图谱时的弱点。 |
| [^59] | [Large Language Models Can Learn Temporal Reasoning](https://arxiv.org/abs/2401.06853) | 本文提出了一个新的TG-LLM框架，以语言为基础进行时间推理，通过教导LLM将上下文翻译成时间图，并使用CoTs指导LLM进行符号推理。 |
| [^60] | [Split and Rephrase with Large Language Models](https://arxiv.org/abs/2312.11075) | 评估了大型语言模型在Split and Rephrase任务上的表现，表明在主要指标上有显著改进，但在分割一致性方面仍有待提高。 |
| [^61] | [PixT3: Pixel-based Table To Text generation](https://arxiv.org/abs/2311.09808) | PixT3是一种基于像素的多模式表格到文本模型，通过将数据到文本生成视为视觉识别任务，消除了字符串格式的需求，克服了线性化和输入大小限制的挑战。 |
| [^62] | [CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models](https://arxiv.org/abs/2311.09154) | Clean-Eval提出了一种清洁评估方法，通过LLM对污染数据进行释义和反向翻译，利用语义检测器过滤低质量样本，最终选择最佳候选，解决了大型语言模型评估中的数据污染问题。 |
| [^63] | [Adversarial Preference Optimization](https://arxiv.org/abs/2311.08045) | 提出了一种对抗偏好优化（APO）框架，实现了在没有额外注释的情况下，通过对抗学习自适应于生成分布差距。 |
| [^64] | [On Measuring Faithfulness or Self-consistency of Natural Language Explanations](https://arxiv.org/abs/2311.07466) | 本文论述了衡量自然语言解释的忠诚度或自一致性的问题。我们提出了自一致性测试来评估解释的输出级别的一致性。我们通过构建比较一致性测试库，并引入了新的自一致性度量CC-SHAP来支持我们的观点。 |
| [^65] | [Revisiting the Hypothesis: Do pretrained Transformers Learn In-Context by Gradient Descent?](https://arxiv.org/abs/2310.08540) | 本研究重新审视了预训练的Transformer是否通过梯度下降在上下文中学习的假设，并发现现有研究中的假设存在限制性假设，使其与实际语言模型训练时的语境存在显著差异。同时，通过对真实模型的观察和比较，揭示了ICL和GD在观察演示顺序上的不同敏感性。 |
| [^66] | [LangBridge: Multilingual Reasoning Without Multilingual Supervision.](http://arxiv.org/abs/2401.10695) | LangBridge是一种无需多语言监督的多语言推理方法，通过连接两个模型来适应多语言推理任务，尽管只使用英文数据进行训练，但它显著提高了语言模型对低资源语言的性能。 |
| [^67] | [Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation.](http://arxiv.org/abs/2401.08417) | 本研究通过引入对比性偏好优化（CPO）的方法，弥合了大型语言模型（LLM）在机器翻译中性能与传统编码器-解码器模型之间的差距，实现了更好的翻译效果。 |
| [^68] | [Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs.](http://arxiv.org/abs/2401.04854) | 本文探讨了语言模型（LLMs）是更像图书馆还是图书管理员的问题。论文首先阐述了 "文献主义 "这一概念，并提出了对其的挑战，指出LLMs生成的全新文本在内容上依赖于原始人类文本的内容。然后，论文提出了对 "文献主义"的新颖挑战，讨论了LLMs生成的 "新引用"问题。最后，根据心灵哲学中的解释主义，论文提出了有限代理能力的LLMs可能存在的可能性。 |
| [^69] | [The Critique of Critique.](http://arxiv.org/abs/2401.04518) | 本文创新性地提出了元批评(MetaCritique)的框架，通过精确度和召回率评估批评的质量，并以F1分数作为整体评分。为了获得可靠的评估结果，引入了原子信息单元(AIUs)来描述批评。元批评提供了自然语言的理由来支持评价结果。 |
| [^70] | [Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search.](http://arxiv.org/abs/2401.04514) | 本论文提出一种扩展的生成增强检索（GAR）框架，通过对代码进行重写来解决代码搜索中存在的风格不匹配问题，实验结果表明该方法显著提高了检索准确性。 |
| [^71] | [One Shot Learning as Instruction Data Prospector for Large Language Models.](http://arxiv.org/abs/2312.10302) | 本研究提出了一种名为Nuggets的新颖有效方法，利用单次学习从庞大的数据集中选择高质量的指导数据，通过评估示例对多样锚定集的困惑度影响，选择对指导调优最有益的数据 |
| [^72] | [UP4LS: User Profile Constructed by Multiple Attributes for Enhancing Linguistic Steganalysis.](http://arxiv.org/abs/2311.01775) | 本文提出了一个名为UP4LS的新框架，通过多个属性构建用户信息以增强语言隐写分析的性能。实验结果显示，该框架可以有效提升隐写分析的性能。 |
| [^73] | [Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models.](http://arxiv.org/abs/2310.17086) | Transformers学会了高阶优化方法，用于上下文学习，通过实现类似于迭代牛顿法的算法，而不是梯度下降。 |
| [^74] | [Prompt Injection Attacks and Defenses in LLM-Integrated Applications.](http://arxiv.org/abs/2310.12815) | 本文提出了一个通用框架来形式化提示注入攻击，并系统化防御这种类型的攻击。 |
| [^75] | [Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks.](http://arxiv.org/abs/2310.12516) | 本文提出了一种通过可迁移的对抗攻击在大型语言模型中自动生成评估数据的方法，并使用ChatGPT和Natural Questions（NQ）数据集进行了验证。 |
| [^76] | [TextBind: Multi-turn Interleaved Multimodal Instruction-following.](http://arxiv.org/abs/2309.08637) | TextBind是一个注释极少的框架，用于将较大规模的语言模型赋予多轮交错多模态指令跟随能力，并通过图像-标题对生成多轮多模态指令-回应对话。这个框架对于解决实际任务具有重要意义，并为未来的研究提供了数据集、模型和演示。 |
| [^77] | [Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models.](http://arxiv.org/abs/2308.10379) | 本论文提出了一种名为"思想算法"的策略，通过算法推理路径推动大型语言模型的思想探索，以低成本、低存储和低计算开销的方式扩展了其推理能力。结果显示，使用算法指导的大型语言模型的性能可以超越算法本身。 |
| [^78] | [Discrete Prompt Compression with Reinforcement Learning.](http://arxiv.org/abs/2308.08758) | 本研究提出了一种使用强化学习的离散提示压缩方法（PCRL），以解决指令调整的语言模型中嵌入训练的挑战。PCRL采用了一种计算效率高的策略网络直接编辑提示，可以灵活应用于各种类型的LM，而不需要梯度访问或标记数据。 |
| [^79] | [Multi-Modality Multi-Loss Fusion Network.](http://arxiv.org/abs/2308.00264) | 多模态多损失融合网络通过最佳选择和融合多个模态的特征，提高了情感检测的性能，并在多个数据集上实现了最先进的结果。这些研究结果表明了用于增强神经网络中情感检测的特征选择和融合方法的优化方向。 |
| [^80] | [ValiTex -- a uniform validation framework for computational text-based measures of social science constructs.](http://arxiv.org/abs/2307.02863) | ValiTex是一个统一的验证框架，旨在帮助学者们基于文本数据来度量社会科学构建。它借鉴了心理测量学的传统，通过概念模型和动态检查表提供了验证的结构和步骤。 |
| [^81] | [Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models.](http://arxiv.org/abs/2306.17820) | 本论文提出了一种称为“元推理”的方法，它通过使用语义符号解构的方式，将不同推理问题转化为类似的自然语言表示，以提高大型语言模型的推理能力。 |
| [^82] | [FLuRKA: Fast fused Low-Rank & Kernel Attention.](http://arxiv.org/abs/2306.15799) | FLuRKA是一种融合低秩和核注意力的新型transformer类别，相较于传统的近似技术，在运行时间性能和质量方面都表现出显著的提升。 |
| [^83] | [Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation.](http://arxiv.org/abs/2306.12916) | 本文全面研究了跨语言跨时代摘要任务，使用历史幻想文本和维基百科摘要构建了第一个CLCTS语料库，并研究了流行的变压器模型及其中间任务微调的有效性；同时还探讨了ChatGPT在CLCTS中作为摘要器和评估器的潜力。最终发现中间任务微调的端到端模型产生了中等到差的效果，而ChatGPT在没有微调的情况下提供中等到好的摘要质量表现。 |
| [^84] | [Boosting Language Models Reasoning with Chain-of-Knowledge Prompting.](http://arxiv.org/abs/2306.06427) | 该论文提出了一种新的知识链提示（CoK）方法，旨在引导语言模型生成明确的知识证据，以提升推理能力，并通过F^2-Verification方法评估推理的准确性和可信度。 |
| [^85] | [Physics of Language Models: Part 1, Context-Free Grammar.](http://arxiv.org/abs/2305.13673) | 本研究探究了生成式语言模型如何学习上下文无关文法（CFG），并通过构造人造数据证明了预训练transformers可以学会生成具有接近完美准确度和显着多样性的句子。研究发现transformer内部的隐藏状态隐含而精确地编码了CFG结构，学会形成类似动态规划的“边界到边界”的注意力。此外，还研究了标准CFG的扩展，例如概率CFG和线性CFG，并证明transformers也可以学会这些扩展语法结构。 |
| [^86] | [ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation.](http://arxiv.org/abs/2303.06458) | ZeroNLG是一个零样本学习框架，可以处理多个NLG任务，包括图像到文本、视频到文本和文本到文本，跨越英语、中文、德语和法语。它不需要任何标记的下游对进行训练，通过将不同的领域投影到共享的公共潜在空间中的相应坐标，桥接不同领域之间的差异。 |

# 详细

[^1]: 基于语言驱动的机器人导航的分层开放词汇3D场景图

    Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation

    [https://arxiv.org/abs/2403.17846](https://arxiv.org/abs/2403.17846)

    提出了一种用于语言驱动的机器人导航的分层开放词汇3D场景图映射方法，可以有效代表多层建筑并允许机器人在其中穿行。

    

    最近的开放词汇机器人映射方法利用预先训练的视觉-语言特征丰富了密集几何地图。虽然这些地图允许在查询某种语言概念时预测逐点显著性地图，但大规模环境和超出对象级别的抽象查询仍然是一个相当大的障碍，最终限制了基于语言的机器人导航。在这项工作中，我们提出了HOV-SG，一种用于语言驱动的机器人导航的分层开放词汇3D场景图映射方法。通过利用开放词汇视觉基础模型，我们首先在3D空间中获得了最先进的开放词汇分段级地图，然后构建了由地板、房间和对象概念组成的3D场景图层次结构，每个都包含开放性词汇特征。我们的方法能够表示多层建筑，并且允许机器人使用跨层Voronoi图穿越这些建筑。HOV-SG进行了评估。

    arXiv:2403.17846v1 Announce Type: cross  Abstract: Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features. While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded robot navigation. Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features. Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph. HOV-SG is evaluat
    
[^2]: 解码压缩的信任：审视在压缩下高效LLMs的可信度

    Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression

    [https://arxiv.org/abs/2403.15447](https://arxiv.org/abs/2403.15447)

    量化目前比剪枝更有效，可以同时实现效率和可信度，但剪枝会显著降低模型的可信度

    

    将高性能的大型语言模型（LLMs）压缩已经成为一种资源高效推断的首选策略。尽管最先进的压缩方法在保留良性任务性能方面取得了令人印象深刻的进展，但压缩在安全性和可信度方面的潜在风险在很大程度上被忽视。这项研究对使用五种最先进压缩技术评估三种领先LLMs的可信度维度进行了首次彻底评估。我们的实验突出了压缩与可信度之间复杂的相互作用，揭示了一些有趣的模式。我们发现，目前量化比剪枝更有效地同时实现效率和可信度。例如，4位量化模型保留了其原始对应物的可信度，但模型剪枝显著降低了可信度，即使在50%的稀疏度下。

    arXiv:2403.15447v1 Announce Type: cross  Abstract: Compressing high-capability Large Language Models (LLMs) has emerged as a favored strategy for resource-efficient inferences. While state-of-the-art (SoTA) compression methods boast impressive advancements in preserving benign task performance, the potential risks of compression in terms of safety and trustworthiness have been largely neglected. This study conducts the first, thorough evaluation of three (3) leading LLMs using five (5) SoTA compression techniques across eight (8) trustworthiness dimensions. Our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns. We find that quantization is currently a more effective approach than pruning in achieving efficiency and trustworthiness simultaneously. For instance, a 4-bit quantized model retains the trustworthiness of its original counterpart, but model pruning significantly degrades trustworthiness, even at 50% spars
    
[^3]: M$^3$AV：一种多模态、多体裁和多用途的音视频学术讲座数据集

    M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset

    [https://arxiv.org/abs/2403.14168](https://arxiv.org/abs/2403.14168)

    提出了一种新颖的M$^3$AV音视频学术讲座数据集，包含多模态、多体裁和高质量人工注释，可用于多种音视频识别任务

    

    arXiv:2403.14168v1 公告类型：新摘要：发布开源学术视频录像是在线分享知识的一种新兴和普遍方法。这些视频包含丰富的多模态信息，包括演讲者的语音、面部和身体动作，以及幻灯片中的文本和图片，甚至可能包括论文内容。尽管已构建和发布了多个学术视频数据集，但很少有数据集支持多模态内容识别和理解任务，部分原因是缺乏高质量的人工注释。在本文中，我们提出了一种新颖的多模态、多体裁和多用途的音视频学术讲座数据集(M$^3$AV)，该数据集包括来自五个来源的近367小时的视频，涵盖计算机科学、数学以及医学和生物学等主题。通过对言语和书面文字（尤其是高价值名称实体）的高质量人工注释，该数据集可用于多种音视频识别

    arXiv:2403.14168v1 Announce Type: new  Abstract: Publishing open-source academic video recordings is an emergent and prevalent approach to sharing knowledge online. Such videos carry rich multimodal information including speech, the facial and body movements of the speakers, as well as the texts and pictures in the slides and possibly even the papers. Although multiple academic video datasets have been constructed and released, few of them support both multimodal content recognition and understanding tasks, which is partially due to the lack of high-quality human annotations. In this paper, we propose a novel multimodal, multigenre, and multipurpose audio-visual academic lecture dataset (M$^3$AV), which has almost 367 hours of videos from five sources covering computer science, mathematics, and medical and biology topics. With high-quality human annotations of the spoken and written words, in particular high-valued name entities, the dataset can be used for multiple audio-visual recogn
    
[^4]: PET-SQL：一个带有交叉一致性的增强提示的两阶段文本到SQL框架

    PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency

    [https://arxiv.org/abs/2403.09732](https://arxiv.org/abs/2403.09732)

    提出了一个两阶段框架，通过引入参考增强表示和少样本演示，解决了在处理冗长的数据库信息和复杂用户意图时的挑战。

    

    最近文本到SQL（Text2SQL）领域的进展强调刺激大型语言模型（LLM）进行上下文学习，取得了显著成果。然而，他们在处理冗长的数据库信息和复杂的用户意图时面临挑战。本文提出了一个两阶段框架，以增强当前基于LLM的自然语言到SQL系统的性能。我们首先引入了一种新颖的提示表示，称为参考增强表示，其中包括模式信息和从表格随机抽样的单元格值，以指导LLM生成SQL查询。然后，在第一阶段，我们检索问题-SQL对作为少量演示，促使LLM生成初步SQL（PreSQL）。之后，解析PreSQL中提到的实体进行模式链接，可以显著压缩有用信息。在第二阶段，利用链接的模式，我们简化了

    arXiv:2403.09732v1 Announce Type: cross  Abstract: Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large language models (LLM) on in-context learning, achieving significant results. Nevertheless, they face challenges when dealing with verbose database information and complex user intentions. This paper presents a two-stage framework to enhance the performance of current LLM-based natural language to SQL systems. We first introduce a novel prompt representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After that, the mentioned entities in PreSQL are parsed to conduct schema linking, which can significantly compact the useful information. In the second stage, with the linked schema, we simplify the 
    
[^5]: LLMs能够将指令与数据分离吗？我们具体指的是什么？

    Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?

    [https://arxiv.org/abs/2403.06833](https://arxiv.org/abs/2403.06833)

    本研究提出了一种形式化的度量来量化指令与数据分离现象，以及一种可以从模型黑盒输出计算的经验变量，并引入了新数据集SEP，用于评估

    

    arXiv:2403.06833v1 公告类型: 跨 针对大型语言模型（LLMs）进行调节指令的技术取得了突破性的成果，为许多实际应用打开了无数新可能。然而，LLMs缺乏其他计算机科学领域已建立为规范的基本安全特性，比如指令与数据之间的分离，导致它们发生故障或易受第三方操控和干扰（例如通过间接提示/命令注入）。更糟糕的是，迄今为止，甚至没有确切定义这种分离究竟意味着什么以及如何测试其违反情况。本研究旨在填补这一空白。我们引入了一个正式的指标来量化指令与数据分离现象，以及一个可以从模型的黑盒输出计算的经验变量。我们还介绍了一个新的数据集SEP（应该执行还是处理？），该数据集允许评估

    arXiv:2403.06833v1 Announce Type: cross  Abstract: Instruction-tuned Large Language Models (LLMs) have achieved breakthrough results, opening countless new possibilities for many practical applications. However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection. Even worse, so far, there is not even an established definition of what precisely such a separation would mean and how its violation could be tested. In this work, we aim to close this gap. We introduce a formal measure to quantify the phenomenon of instruction-data separation as well as an empirical variant of the measure that can be computed from a model`s black-box outputs. We also introduce a new dataset, SEP (Should it be Executed or Processed?), which allows estimating th
    
[^6]: 人类对抗机器：语言模型与战争游戏

    Human vs. Machine: Language Models and Wargames

    [https://arxiv.org/abs/2403.03407](https://arxiv.org/abs/2403.03407)

    人工智能大型语言模型在战争游戏中与人类响应存在一致性，但也存在显著的差异，这表明在政策制定者交出自主权或听从基于AI的战略建议之前应谨慎对待。

    

    战争游戏在军事战略的发展和国家对威胁或攻击的响应中有着悠久的历史。人工智能（AI）的出现承诺了更好的决策制定和增强的军事效果。然而，关于AI系统，尤其是大型语言模型（LLMs），与人类的行为有何不同仍存在争议。为此，我们进行了一项战争游戏实验，共有107位国家安全专家人类参与者参与，旨在研究在一个虚构的美中情景中的危机升级，并比较人类参与者与LLM模拟响应之间的差异。我们发现LLM和人类响应存在显著一致性，但在战争游戏中模拟和人类参与者之间也存在显著的定量和定性差异，这促使决策者在交出自主权或遵循基于AI的战略建议之前谨慎对待。

    arXiv:2403.03407v1 Announce Type: cross  Abstract: Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. The advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness. However, there is still debate about how AI systems, especially large language models (LLMs), behave as compared to humans. To this end, we use a wargame experiment with 107 national security expert human players designed to look at crisis escalation in a fictional US-China scenario and compare human players to LLM-simulated responses. We find considerable agreement in the LLM and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy recommendations.
    
[^7]: 将神经信号解码为语音

    Decode Neural signal as Speech

    [https://arxiv.org/abs/2403.01748](https://arxiv.org/abs/2403.01748)

    本文在脑机接口领域探索了MEG信号的脑到文本转换，着重解决了以前主要集中在EEG上、使用“teacher-forcing”以及未完全自回归的问题。

    

    从脑动态解码语言是脑机接口（BCI）领域中一个重要的开放方向，尤其考虑到大型语言模型的快速增长。相对于需要电极植入手术的侵入性信号，非侵入性神经信号（如EEG、MEG）由于其安全性和普适性而越来越受到关注。然而，在三个方面的探索还不足：1）以前的方法主要集中在EEG上，但没有一个先前的研究解决了MEG信号质量更好的问题；2）以前的工作主要在生成解码过程中使用“teacher-forcing”，这是不切实际的；3）以前的工作大多是基于“BART”而不是完全自回归的，而在其他序列任务中表现更好。在本文中，我们探讨了MEG信号的脑到文本转换在语音解码形式中。我们是第一个在交叉注意力中研究的。

    arXiv:2403.01748v1 Announce Type: cross  Abstract: Decoding language from brain dynamics is an important open direction in the realm of brain-computer interface (BCI), especially considering the rapid growth of large language models. Compared to invasive-based signals which require electrode implantation surgery, non-invasive neural signals (e.g. EEG, MEG) have attracted increasing attention considering their safety and generality. However, the exploration is not adequate in three aspects: 1) previous methods mainly focus on EEG but none of the previous works address this problem on MEG with better signal quality; 2) prior works have predominantly used ``teacher-forcing" during generative decoding, which is impractical; 3) prior works are mostly ``BART-based" not fully auto-regressive, which performs better in other sequence tasks. In this paper, we explore the brain-to-text translation of MEG signals in a speech-decoding formation. Here we are the first to investigate a cross-attentio
    
[^8]: NewsBench：系统性评估LLM在中国新闻编辑应用中的写作水平和安全性遵从能力

    NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and Safety Adherence in Chinese Journalistic Editorial Applications

    [https://arxiv.org/abs/2403.00862](https://arxiv.org/abs/2403.00862)

    NewsBench是一个评估LLMs在中国新闻写作水平和安全性遵从能力的基准框架，揭示了在创造性写作任务中LLMs相对不足的新闻伦理遵守方面的需求。

    

    这项研究提出了NewsBench，这是一个新颖的基准框架，旨在评估大型语言模型（LLMs）在中国新闻写作水平（JWP）和安全性遵从（SA）方面的能力，弥补了新闻伦理与人工智能利用风险之间的差距。NewsBench包括5个编辑应用中的1,267项任务，7个方面（包括安全性和新闻写作，以及4个详细要面），涵盖24个新闻主题领域，采用基于两种GPT-4的自动评估协议，并经过人类评估验证。我们对11个LLM的全面分析突出了GPT-4和ERNIE Bot作为表现最佳，但在创造性写作任务中揭示了新闻伦理遵守方面的相对不足。这些发现强调了AI生成的新闻内容需要提高伦理指导，标志着以新闻标准和安全性对齐AI能力迈出了一步。

    arXiv:2403.00862v1 Announce Type: cross  Abstract: This study presents NewsBench, a novel benchmark framework developed to evaluate the capability of Large Language Models (LLMs) in Chinese Journalistic Writing Proficiency (JWP) and their Safety Adherence (SA), addressing the gap between journalistic ethics and the risks associated with AI utilization. Comprising 1,267 tasks across 5 editorial applications, 7 aspects (including safety and journalistic writing with 4 detailed facets), and spanning 24 news topics domains, NewsBench employs two GPT-4 based automatic evaluation protocols validated by human assessment. Our comprehensive analysis of 11 LLMs highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative deficiency in journalistic ethic adherence during creative writing tasks. These findings underscore the need for enhanced ethical guidance in AI-generated journalistic content, marking a step forward in aligning AI capabilities with journalistic standards and safet
    
[^9]: Multimodal ArXiv: 用于提升大型视觉-语言模型对科学理解的数据集

    Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models

    [https://arxiv.org/abs/2403.00231](https://arxiv.org/abs/2403.00231)

    提出了Multimodal ArXiv数据集，包括ArXivCap和ArXivQA，用于增强大型视觉-语言模型对科学理解的能力，ArXivQA通过科学图生成问题，显著提高了数学推理准确率。

    

    大型视觉-语言模型（LVLMs），以GPT-4V为例，在涉及自然场景中的具体图像的各种任务中表现出色。然而，由于科学领域训练数据集的稀缺，它们在解释抽象图形（例如几何形状和科学图）方面的能力仍然有限。为了填补这一空白，我们介绍了Multimodal ArXiv，包括ArXivCap和ArXivQA，以增强LVLMs的科学理解。ArXivCap是一个包含来自涵盖各种科学领域的572K份ArXiv论文的6.4M张图像和3.9M个标题的图像标题数据集。借鉴ArXivCap，我们介绍了ArXivQA，这是一个通过提示GPT-4V生成的基于科学图的问答数据集。ArXivQA极大地增强了LVLMs的数学推理能力，在多模态数学推理基准上实现了10.4%的绝对准确率提升。此外，利用ArXivCap，我们设计了四个从视觉到文本的任务。

    arXiv:2403.00231v1 Announce Type: cross  Abstract: Large vision-language models (LVLMs), exemplified by GPT-4V, excel across diverse tasks involving concrete images from natural scenes. However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains. To fill this gap, we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension. ArXivCap is a figure-caption dataset comprising 6.4M images and 3.9M captions sourced from 572K ArXiv papers spanning various scientific domains. Drawing from ArXivCap, we introduce ArXivQA, a question-answering dataset generated by prompting GPT-4V based on scientific figures. ArXivQA greatly enhances LVLMs' mathematical reasoning capabilities, achieving a 10.4% absolute accuracy gain on a multimodal mathematical reasoning benchmark. Furthermore, employing ArXivCap, we devise four vision-to-text tas
    
[^10]: 用于词汇语义变化检测的语义距离度量学习方法

    A Semantic Distance Metric Learning approach for Lexical Semantic Change Detection

    [https://arxiv.org/abs/2403.00226](https://arxiv.org/abs/2403.00226)

    提出了一种用于词汇语义变化检测的语义距离度量学习方法，通过使用两个阶段的学习方法和感知编码器，实现了在多语言中优于以往方法的表现。

    

    检测词汇的时间语义变化是各种自然语言处理应用的重要任务，必须对时间敏感地进行预测。词汇语义变化检测（SCD）任务考虑在两个不同的文本语料库$C_1$和$C_2$之间预测给定目标词$w$是否改变了含义的问题。为此，我们提出了一种使用现有的Word-in-Context（WiC）数据集的监督两阶段SCD方法。在第一阶段，对于目标词$w$，我们学习了两个感知感知编码器，表示给定语料库中所选句子中$w$的含义。接下来，在第二阶段，我们学习了一种感知感知距离度量，比较目标词在$C_1$和$C_2$中的所有出现的语义表示。对多个SCD基准数据集的实验结果表明，我们提出的方法始终优于所有先前提出的多种语言的SCD方法。

    arXiv:2403.00226v1 Announce Type: new  Abstract: Detecting temporal semantic changes of words is an important task for various NLP applications that must make time-sensitive predictions. Lexical Semantic Change Detection (SCD) task considers the problem of predicting whether a given target word, $w$, changes its meaning between two different text corpora, $C_1$ and $C_2$. For this purpose, we propose a supervised two-staged SCD method that uses existing Word-in-Context (WiC) datasets. In the first stage, for a target word $w$, we learn two sense-aware encoder that represents the meaning of $w$ in a given sentence selected from a corpus. Next, in the second stage, we learn a sense-aware distance metric that compares the semantic representations of a target word across all of its occurrences in $C_1$ and $C_2$. Experimental results on multiple benchmark datasets for SCD show that our proposed method consistently outperforms all previously proposed SCD methods for multiple languages, esta
    
[^11]: 这里有一个免费午餐：使用模型合并消毒带后门的模型

    Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge

    [https://arxiv.org/abs/2402.19334](https://arxiv.org/abs/2402.19334)

    将带后门的模型与其他同类模型合并可以有效治疗后门漏洞，为后门攻击提供推理阶段的有效和高效防御

    

    通过开源倡议使预训练语言模型民主化快速推动了创新，并扩大了对尖端技术的访问。然而，这种开放性也带来了重大安全风险，包括后门攻击，其中隐藏的恶意行为由特定输入触发，损害自然语言处理（NLP）系统的完整性和可靠性。本文建议通过将带后门的模型与其他同类模型合并，可以治疗后门漏洞，即使这些模型并非全部安全。在我们的实验中，我们探索了各种模型（BERT-Base、RoBERTa-Large、Llama2-7B和Mistral-7B）和数据集（SST-2、OLID、AG News和QNLI）。与多种先进的防御方法相比，我们的方法提供了一种有效且高效的推理阶段对抗后门攻击的防御，而无需额外资源或特定知识。我们的方法始终表现优秀

    arXiv:2402.19334v1 Announce Type: new  Abstract: The democratization of pre-trained language models through open-source initiatives has rapidly advanced innovation and expanded access to cutting-edge technologies. However, this openness also brings significant security risks, including backdoor attacks, where hidden malicious behaviors are triggered by specific inputs, compromising natural language processing (NLP) system integrity and reliability. This paper suggests that merging a backdoored model with other homogeneous models can remediate backdoor vulnerabilities even if such models are not entirely secure. In our experiments, we explore various models (BERT-Base, RoBERTa-Large, Llama2-7B, and Mistral-7B) and datasets (SST-2, OLID, AG News, and QNLI). Compared to multiple advanced defensive approaches, our method offers an effective and efficient inference-stage defense against backdoor attacks without additional resources or specific knowledge. Our approach consistently outperform
    
[^12]: 通过自适应解码改进开放式文本生成

    Improving Open-Ended Text Generation via Adaptive Decoding

    [https://arxiv.org/abs/2402.18223](https://arxiv.org/abs/2402.18223)

    引入自适应解码机制，通过置信度动态确定生成过程中的候选集，在故事生成任务中实现了更高的MAUVE和多样性，保持一定的连贯性，优于现有算法。

    

    当前语言模型根据概率分布逐标记解码文本，确定下一个标记的恰当候选者对于保证生成质量至关重要。本研究引入了自适应解码，一种机制使语言模型能够在生成过程中动态确定一个合理的候选集。具体来说，我们引入了一种基于熵的度量标准，称之为置信度，并将确定最佳候选集视为一个增加置信度的过程。通过利用置信度增加来评估将标记包含在候选集中的合理性，使模型能够自适应地确定最合适的候选集。实验结果表明，我们的方法在故事生成任务中实现了更高的MAUVE和多样性，并保持了一定的连贯性，突显了其优于现有算法的优越性。

    arXiv:2402.18223v1 Announce Type: new  Abstract: Current language models decode text token by token according to probabilistic distribution, and determining the appropriate candidates for the next token is crucial to ensure generation quality. This study introduces adaptive decoding, a mechanism that empowers the language models to ascertain a sensible candidate set during the generation process dynamically. Specifically, we introduce an entropy-based metric called confidence and conceptualize determining the optimal candidate set as a confidence-increasing process. The rationality of including a token in the candidate set is assessed by leveraging the increment of confidence, enabling the model to determine the most suitable candidate set adaptively. The experimental results reveal that our method achieves higher MAUVE and diversity in story generation tasks and maintains certain coherence, underscoring its superiority over existing algorithms. The code is available at https://github.
    
[^13]: 一种用于共情响应生成的迭代关联记忆模型

    An Iterative Associative Memory Model for Empathetic Response Generation

    [https://arxiv.org/abs/2402.17959](https://arxiv.org/abs/2402.17959)

    提出了一种用于共情响应生成的迭代关联记忆模型，采用二阶交互注意机制迭代地捕捉相关词语，实现准确、细致地理解话语。

    

    共情响应生成是理解对话话语中的认知和情感状态，并生成恰当回应。心理学理论认为，理解情感和认知状态需要迭代地捕捉和理解对话话语之间的相关词语。然而，现有方法将对话话语视为长序列或独立话语来理解，往往会忽视它们之间的关联词语。为解决这一问题，我们提出了一种用于共情响应生成的迭代关联记忆模型（IAMM）。具体来说，我们采用一种新颖的二阶交互注意机制，迭代地捕捉对话话语、情境、对话历史和记忆模块（用于存储相关词语）之间的重要关联词语，从而准确而细致地理解话语。

    arXiv:2402.17959v1 Announce Type: new  Abstract: Empathetic response generation is to comprehend the cognitive and emotional states in dialogue utterances and generate proper responses. Psychological theories posit that comprehending emotional and cognitive states necessitates iteratively capturing and understanding associated words across dialogue utterances. However, existing approaches regard dialogue utterances as either a long sequence or independent utterances for comprehension, which are prone to overlook the associated words between them. To address this issue, we propose an Iterative Associative Memory Model (IAMM) for empathetic response generation. Specifically, we employ a novel second-order interaction attention mechanism to iteratively capture vital associated words between dialogue utterances and situations, dialogue history, and a memory module (for storing associated words), thereby accurately and nuancedly comprehending the utterances. We conduct experiments on the Em
    
[^14]: 通过表示编辑推进微调中的参数效率

    Advancing Parameter Efficiency in Fine-tuning via Representation Editing

    [https://arxiv.org/abs/2402.15179](https://arxiv.org/abs/2402.15179)

    RED通过表示编辑显著降低了可训练参数数量，实现了与完全参数微调和其他PEFT方法相当或更好的结果

    

    参数有效微调（PEFT）因其能够在仅更新可训练参数的一个小子集时达到竞争性结果而受到了重视。在解决这些挑战问题中，我们提出了一种新颖的微调神经模型的方法，称为表示编辑（RED），其扩放和偏置每一层产生的表示。与完全参数微调相比，RED将可训练参数数量降低了$25,700$倍，并与LoRA相比降低了32倍。值得注意的是，RED实现了与完全参数微调和其他PEFT方法相当或更好的结果。对不同架构和规模的模型进行了大量实验。

    arXiv:2402.15179v1 Announce Type: cross  Abstract: Parameter Efficient Fine-Tuning (PEFT) has gained significant attention for its ability to achieve competitive results while updating only a small subset of trainable parameters. Despite the promising performance of current PEFT methods, they present challenges in hyperparameter selection, such as determining the rank of LoRA or Adapter, or specifying the length of soft prompts. In addressing these challenges, we propose a novel approach to fine-tuning neural models, termed Representation EDiting (RED), which scales and biases the representation produced at each layer. RED substantially reduces the number of trainable parameters by a factor of $25,700$ compared to full parameter fine-tuning, and by a factor of $32$ compared to LoRA. Remarkably, RED achieves comparable or superior results to full parameter fine-tuning and other PEFT methods. Extensive experiments were conducted across models of varying architectures and scales, includin
    
[^15]: KIEval：面向大型语言模型的知识引导式交互评估框架

    KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models

    [https://arxiv.org/abs/2402.15043](https://arxiv.org/abs/2402.15043)

    该论文引入了KIEval，一种知识引导式交互评估框架，通过LLM-powered "interactor"角色实现动态的抗污染评估

    

    大型语言模型（LLMs）的自动评估方法受到数据污染的影响，导致对其有效性的评估被夸大。现有的策略旨在检测受污染的文本，但侧重于量化污染程度而非准确衡量模型性能。本文介绍了KIEval，这是一种知识引导式交互评估框架，首次引入了LLM驱动的“交互者”角色，实现了动态抗污染评估。从涉及特定领域知识的常规LLM基准问题开始，KIEval利用动态生成的、多轮、以知识为重点的对话，以确定模型的响应是否仅是基准答案的回忆，还是表明了深入理解并能在更复杂的对话中应用知识。在五个数据集上对七个领先的LLM进行了大量实验证实了KI

    arXiv:2402.15043v1 Announce Type: cross  Abstract: Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered "interactor" role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KI
    
[^16]: 在推理思维中比较人类和大型语言模型的推理策略

    Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning

    [https://arxiv.org/abs/2402.14856](https://arxiv.org/abs/2402.14856)

    该研究通过对大型语言模型对命题逻辑问题的响应进行评估，揭示出它们展现出与人类类似的推理模式和策略。

    

    推理思维在制定健全和连贯论点方面扮演了关键角色。它允许个体根据所提供信息的真值得出逻辑上的结论。在大型语言模型（LLMs）领域的最新进展展示了它们在执行演绎推理任务方面的能力。然而，大部分研究主要评估LLMs在解决此类任务中的准确性，往往忽视了对其推理行为进行更深入的分析。在本研究中，我们借鉴认知心理学原理，通过对它们对命题逻辑问题的响应进行详细评估，来研究LLMs采用的推理策略。我们的研究结果表明，LLMs展现出类似于人类观察到的推理模式，包括诸如“假定跟随”或“链构建”等策略。此外，我们的研究证明了arXiv:2402.14856v1 Announce Type: cross

    arXiv:2402.14856v1 Announce Type: cross  Abstract: Deductive reasoning plays a pivotal role in the formulation of sound and cohesive arguments. It allows individuals to draw conclusions that logically follow, given the truth value of the information provided. Recent progress in the domain of large language models (LLMs) has showcased their capability in executing deductive reasoning tasks. Nonetheless, a significant portion of research primarily assesses the accuracy of LLMs in solving such tasks, often overlooking a deeper analysis of their reasoning behavior. In this study, we draw upon principles from cognitive psychology to examine inferential strategies employed by LLMs, through a detailed evaluation of their responses to propositional logic problems. Our findings indicate that LLMs display reasoning patterns akin to those observed in humans, including strategies like $\textit{supposition following}$ or $\textit{chain construction}$. Moreover, our research demonstrates that the ar
    
[^17]: CriticBench：为批判性-正确推理评估LLMs而设计的基准测试

    CriticBench: Benchmarking LLMs for Critique-Correct Reasoning

    [https://arxiv.org/abs/2402.14809](https://arxiv.org/abs/2402.14809)

    CriticBench是一个综合基准测试，旨在评估LLMs在批判和纠正推理方面的能力，发现批判性训练显著提升性能，逻辑任务更易于修正。

    

    大型语言模型（LLMs）批判和完善其推理的能力对于它们在评估、反馈提供和自我改进中的应用至关重要。本文引入了CriticBench，一个旨在评估LLMs在各种任务中批判和纠正其推理能力的综合基准测试。CriticBench包含五个推理领域：数学、常识、符号、编码和算法。它整合了15个数据集，并结合了三个LLM系列的响应。利用CriticBench，我们评估和剖析了17个LLMs在生成、批判和修正推理（即GQC推理）中的表现。我们的研究结果显示：（1）GQC能力呈线性关系，批判性训练显著提升了性能；（2）修正效果在任务上有所不同，以逻辑为导向的任务更容易修正；（3）GQC知识的不一致性。

    arXiv:2402.14809v1 Announce Type: cross  Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsisten
    
[^18]: 使用神经网络网络抓取进行更清洁的预训练语料库筛选

    Cleaner Pretraining Corpus Curation with Neural Web Scraping

    [https://arxiv.org/abs/2402.14652](https://arxiv.org/abs/2402.14652)

    使用神经网络网络抓取器NeuScraper可以从网页中提取干净的文本内容，并且实现了超过20%的改进，有助于提高语言模型的预训练质量

    

    arXiv:2402.14652v1 公告类型: 新的 摘要: 网络包含大规模、多样化和丰富信息，以满足人类的信息需求。通过细致的数据收集、预处理和整理，网页可以被用作语言模型预训练的基本数据资源。然而，面对不断革新和复杂的网页特性，基于规则/特征的网络抓取器变得越来越不足够。本文提出了一个简单、快速、有效的神经网络网络抓取器（NeuScraper），帮助从网页中提取主要和干净的文本内容。实验结果显示NeuScraper超越了基准抓取器，实现了超过20%的改进，展示了其在提取更高质量数据以促进语言模型预训练方面的潜力。所有代码都可以在https://github.com/OpenMatch/NeuScraper找到。

    arXiv:2402.14652v1 Announce Type: new  Abstract: The web contains large-scale, diverse, and abundant information to satisfy the information-seeking needs of humans. Through meticulous data collection, preprocessing, and curation, webpages can be used as a fundamental data resource for language model pretraining. However, when confronted with the progressively revolutionized and intricate nature of webpages, rule-based/feature-based web scrapers are becoming increasingly inadequate. This paper presents a simple, fast, and effective Neural web Scraper (NeuScraper) to help extract primary and clean text contents from webpages. Experimental results show that NeuScraper surpasses the baseline scrapers by achieving more than a 20% improvement, demonstrating its potential in extracting higher-quality data to facilitate the language model pretraining. All of the code is available at https://github.com/OpenMatch/NeuScraper.
    
[^19]: 带聚类的语言模型训练平衡数据抽样

    Balanced Data Sampling for Language Model Training with Clustering

    [https://arxiv.org/abs/2402.14526](https://arxiv.org/abs/2402.14526)

    本文提出了一种名为ClusterClip Sampling的数据抽样方法，利用数据聚类平衡训练数据的文本分布，为更好的模型训练提供支持。

    

    数据在训练大型语言模型（LLM）中起着基础性作用。尽管人们已经关注数据集的收集和组成，但确定训练中的数据抽样策略仍然是一个悬而未决的问题。大多数LLM使用简单的随机抽样策略进行训练。然而，这种抽样策略忽视了训练数据分布的不均衡性，这可能是次优的。在本文中，我们提出了ClusterClip Sampling，以平衡训练数据的文本分布，以实现更好的模型训练。具体而言，ClusterClip Sampling利用数据聚类来反映训练集的数据分布，并根据聚类结果在训练过程中平衡常见样本和稀有样本。引入了重复裁剪操作来减轻由于来自某些聚类的样本导致的过拟合问题。大量实验证实了ClusterClip Sampling的有效性，它的表现优于

    arXiv:2402.14526v1 Announce Type: cross  Abstract: Data plays a fundamental role in the training of Large Language Models (LLMs). While attention has been paid to the collection and composition of datasets, determining the data sampling strategy in training remains an open question. Most LLMs are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal. In this paper, we propose ClusterClip Sampling to balance the text distribution of training data for better model training. Specifically, ClusterClip Sampling utilizes data clustering to reflect the data distribution of the training set and balances the common samples and rare samples during training based on the cluster results. A repetition clip operation is introduced to mitigate the overfitting issue led by samples from certain clusters. Extensive experiments validate the effectiveness of ClusterClip Sampling, which outperfo
    
[^20]: 为医学构建多语言语言模型

    Towards Building Multilingual Language Model for Medicine

    [https://arxiv.org/abs/2402.13963](https://arxiv.org/abs/2402.13963)

    本文提出了为医学领域构建多语言语言模型的三个关键贡献:构建了新的多语言医学语料库MMedC，提出了多语言医学多选问答基准MMedBench，并且通过在MMedC上进一步训练获得了性能优越的MMedLM 2模型。

    

    本文旨在开发一种面向医学的开源多语言语言模型，使得更广泛的语言多样性受众受益。我们的工作主要贡献体现在以下几个方面:首先，针对多语言医学特定适应性，我们构建了一个新的多语言医学语料库，包含大约25.5B个tokens，覆盖了6种主要语言，被称为MMedC，这使得现有通用LLM能够进行自回归训练。其次，为了监测医学领域多语言LLM的发展，我们提出了一个新的带有解释的多语言医学多选问答基准，称为MMedBench；第三，我们评估了一些流行的开源大型语言模型(LLMs)在我们的基准上的表现，以及那些在MMedC上进一步进行自回归训练的模型，最终，我们的最终模型，命名为MMedLM 2，仅有7B参数，取得了卓越的性能。

    arXiv:2402.13963v1 Announce Type: new  Abstract: In this paper, we aim to develop an open-source, multilingual language model for medicine, that the benefits a wider, linguistically diverse audience from different regions. In general, we present the contribution from the following aspects: first, for multilingual medical-specific adaptation, we construct a new multilingual medical corpus, that contains approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, that enables auto-regressive training for existing general LLMs. second, to monitor the development of multilingual LLMs in medicine, we propose a new multilingual medical multi-choice question-answering benchmark with rationale, termed as MMedBench; third, we have assessed a number of popular, opensource large language models (LLMs) on our benchmark, along with those further auto-regressive trained on MMedC, as a result, our final model, termed as MMedLM 2, with only 7B parameters, achieves superior performance c
    
[^21]: 当只有时间能告诉: 通过重启增量性的视角解释Transformer如何处理局部歧义

    When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality

    [https://arxiv.org/abs/2402.13113](https://arxiv.org/abs/2402.13113)

    研究了如何重启增量式Transformer构建和更新内部状态，揭示了增量状态的顺序结构如何编码关于偏误效应及其解决方式的信息，为分析上下文化意义表示和依赖解析的双向编码器带来见解，并显示它们在修订方面的优势。

    

    处理一次一个令牌的增量模型有时会遇到可能有多种解释的点。因果模型被迫输出一个解释并继续，而可以修订的模型在消除歧义时可能会编辑其先前的输出。在这项工作中，我们研究了重启增量式Transformer如何构建和更新内部状态，以阐明导致不适用于自回归模型的修订的过程是什么。我们提出了一种可解释的方法来分析增量状态，显示它们的顺序结构编码了关于偏误效应及其解决方式的信息。我们的方法为分析上下文化意义表示和依赖解析的各种双向编码器带来了见解，有助于展示它们在涉及修订时相对于因果模型的优势。

    arXiv:2402.13113v1 Announce Type: new  Abstract: Incremental models that process sentences one token at a time will sometimes encounter points where more than one interpretation is possible. Causal models are forced to output one interpretation and continue, whereas models that can revise may edit their previous output as the ambiguity is resolved. In this work, we look at how restart-incremental Transformers build and update internal states, in an effort to shed light on what processes cause revisions not viable in autoregressive models. We propose an interpretable way to analyse the incremental states, showing that their sequential structure encodes information on the garden path effect and its resolution. Our method brings insights on various bidirectional encoders for contextualised meaning representation and dependency parsing, contributing to show their advantage over causal models when it comes to revisions.
    
[^22]: 从后门毒化数据集中通过降频空间获取清洁语言模型

    Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space

    [https://arxiv.org/abs/2402.12026](https://arxiv.org/abs/2402.12026)

    通过对频率空间的分析，本文提出了一种多尺度低秩适应（MuScleLoRA）方法，用于解决在训练语言模型时受到后门攻击的问题。

    

    尽管语言模型（LMs）在各种自然语言处理（NLP）任务中取得了显著成功，但LMs的可靠性容易受到后门攻击的影响。先前的研究尝试在毒化数据集上训练LMs时减轻后门学习，但在现实场景中抵御复杂的后门攻击时仍然面临困难。在本文中，我们通过傅里叶分析研究了频率空间中后门LMs的学习机制。我们的发现表明，毒化数据集上呈现的后门映射相比清洁映射更倾向于较低频率，导致后门映射更快地收敛。为了解决这一困境，我们提出了多尺度低秩适应（MuScleLoRA），它在频率空间中部署多个径向缩放，低秩适应目标模型，并在更新参数时进一步调整梯度。通过降频

    arXiv:2402.12026v1 Announce Type: cross  Abstract: Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscal
    
[^23]: 智能手机GUI自动化的全面认知LLM代理

    Comprehensive Cognitive LLM Agent for Smartphone GUI Automation

    [https://arxiv.org/abs/2402.11941](https://arxiv.org/abs/2402.11941)

    提出了全面认知LLM代理，通过全面环境感知和条件动作预测两种新方法系统性提高GUI自动化性能。

    

    大型语言模型(LLMs)已经显示出作为人类般自主语言代理与现实环境进行交互的显著潜力，尤其在图形用户界面(GUI)自动化方面。然而，这些GUI代理需要全面的认知能力，包括详尽的感知和可靠的动作响应。我们提出了全面认知LLM代理，CoCo-Agent，采用了两种新方法，全面环境感知(CEP)和条件动作预测(CAP)，以系统性地提高GUI自动化性能。首先，CEP通过不同方面和粒度促进GUI感知，包括屏幕截图和用于视觉通道的补充详细布局，以及用于文本通道的历史动作。其次，CAP将动作预测分解为子问题：动作类型预测和条件化于动作类型的动作目标。

    arXiv:2402.11941v1 Announce Type: new  Abstract: Large language models (LLMs) have shown remarkable potential as human-like autonomous language agents to interact with real-world environments, especially for graphical user interface (GUI) automation. However, those GUI agents require comprehensive cognition ability including exhaustive perception and reliable action response. We propose \underline{Co}mprehensive \underline{Co}gnitive LLM \underline{Agent}, CoCo-Agent, with two novel approaches, comprehensive environment perception (CEP) and conditional action prediction (CAP), to systematically improve the GUI automation performance. First, CEP facilitates the GUI perception through different aspects and granularity, including screenshots and complementary detailed layouts for the visual channel and historical actions for the textual channel. Second, CAP decomposes the action prediction into sub-problems: action type prediction and action target conditioned on the action type. With our
    
[^24]: 在大型语言模型的知识编辑中探索多跳事实快捷方式

    Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models

    [https://arxiv.org/abs/2402.11900](https://arxiv.org/abs/2402.11900)

    本文系统调查了在大型语言模型中利用事实快捷方式进行多跳事实推理的可能性，并分析了这种快捷方式可能带来的风险。

    

    最近的工作展示了大型语言模型（LLMs）在回忆知识和推理方面的强大能力。然而，LLMs在将这两种能力结合到通过多跳事实推理中尚未被广泛探索。本文系统地调查了LLMs利用基于多跳知识的初始和终端实体之间直接连接的快捷方式的可能性。我们首先通过Knowledge Neurons探索了事实快捷方式的存在，揭示出：(i)快捷方式的强度与预训练语料库中初始和终端实体的共现频率高度相关；（ii）少量提示在回答多跳问题时利用更多的快捷方式，相比之下，思维链提示更多。然后，我们从多跳知识编辑的角度分析了事实快捷方式带来的风险。分析表明，大约有

    arXiv:2402.11900v1 Announce Type: new  Abstract: Recent work has showcased the powerful capability of large language models (LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored. This paper systematically investigates the possibilities for LLMs to utilize shortcuts based on direct connections between the initial and terminal entities of multi-hop knowledge. We first explore the existence of factual shortcuts through Knowledge Neurons, revealing that: (i) the strength of factual shortcuts is highly correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora; (ii) few-shot prompting leverage more shortcuts in answering multi-hop questions compared to chain-of-thought prompting. Then, we analyze the risks posed by factual shortcuts from the perspective of multi-hop knowledge editing. Analysis shows that approximatel
    
[^25]: SIBO：一个简单的增强器用于参数高效微调

    SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning

    [https://arxiv.org/abs/2402.11896](https://arxiv.org/abs/2402.11896)

    SIBO提出了一个简单的增强器来增强参数高效微调技术，有效解决了Transformer-based LLMs中过度平滑的问题，并在多个基准数据集上取得了显著的性能提升。

    

    微调大型语言模型（LLMs）的所有参数需要大量的计算资源和较长时间。最新的参数高效微调（PEFT）技术，如适配器微调和LoRA，允许只调整这些LLMs的一小部分参数。同时，人们注意到过度平滑的问题削弱了基于Transformer的LLMs的有效性，在下游任务中表现不佳。在本文中，我们提出了SIBO，这是一个简单的增强器，通过注入初始残差来增强PEFT。SIBO直观易懂，并且很容易扩展到一系列最新的PEFT技术，以减轻过度平滑并提高性能。对22个基准数据集进行的大量实验证明，SIBO显著提高了各种强基线模型的性能，比现有的PEFT技术提高了高达15.7%和23.5%。

    arXiv:2402.11896v1 Announce Type: new  Abstract: Fine-tuning all parameters of large language models (LLMs) necessitates substantial computational power and extended time. Latest advancements in parameter-efficient fine-tuning (PEFT) techniques, such as Adapter tuning and LoRA, allow for adjustments to only a minor fraction of the parameters of these LLMs. Concurrently, it has been noted that the issue of over-smoothing diminishes the effectiveness of these Transformer-based LLMs, resulting in suboptimal performances in downstream tasks. In this paper, we present SIBO, which is a SImple BOoster to enhance PEFT, by injecting an initial residual. SIBO is straight-forward and readily extensible to a range of state-of-the-art PEFT techniques to alleviate over-smoothing and enhance performance. Extensive experiments on 22 benchmark datasets demonstrate that SIBO significantly enhances the performance of various strong baselines, achieving up to 15.7% and 23.5% improvement over existing PEFT
    
[^26]: 大型多模态模型能揭示图像背后的深层语义吗？

    Can Large Multimodal Models Uncover Deep Semantics Behind Images?

    [https://arxiv.org/abs/2402.11281](https://arxiv.org/abs/2402.11281)

    该论文引入了一个名为DEEPEVAL的基准，用于评估大型多模态模型对视觉深层语义的能力，揭示了现有LMMs在深层语义理解方面与人类之间存在显著差距。

    

    理解图像的深层语义在社交媒体主导的时代至关重要。然而，当前研究主要集中在对图像的表面描述上，揭示了在对内在深层语义进行系统调查方面的明显不足。在这项工作中，我们引入了DEEPEVAL，一个全面的基准，用于评估大型多模态模型(LMMs)对视觉深层语义的能力。 DEEPEVAL 包括人工注释的数据集和三个渐进的子任务：细粒度描述选择、深度标题匹配和深层语义理解。利用 DEEPEVAL，我们评估了9个开源LMMs和GPT-4V(ision)。我们的评估显示了现有LMMs与人类在深层语义理解能力上存在着实质差距。例如，即使在图像描述方面达到与人类可比的表现，GPT-4V在理解深层语义方面仍落后于人类30%。进一步的分析表明

    arXiv:2402.11281v1 Announce Type: new  Abstract: Understanding the deep semantics of images is essential in the era dominated by social media. However, current research works primarily on the superficial description of images, revealing a notable deficiency in the systematic investigation of the inherent deep semantics. In this work, we introduce DEEPEVAL, a comprehensive benchmark to assess Large Multimodal Models' (LMMs) capacities of visual deep semantics. DEEPEVAL includes human-annotated dataset and three progressive subtasks: fine-grained description selection, in-depth title matching, and deep semantics understanding. Utilizing DEEPEVAL, we evaluate 9 open-source LMMs and GPT-4V(ision).Our evaluation demonstrates a substantial gap between the deep semantic comprehension capabilities of existing LMMs and humans. For example, GPT-4V is 30% behind humans in understanding deep semantics, even though it achieves human-comparable performance in image description. Further analysis indi
    
[^27]: 如果你讲我的语言，我会更好地学习：使用风格对齐响应调整增强大型语言模型微调

    I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments

    [https://arxiv.org/abs/2402.11192](https://arxiv.org/abs/2402.11192)

    将微调过程中的实际响应风格与大型语言模型固有风格相匹配能够产生更好的学习结果，开发的方法通过最小程度地调整模型响应来避免过拟合。

    

    使用小数据集为特定任务微调大型语言模型(LLMs)是一个普遍遇到的但复杂的挑战。在有限的示例上过多拟合可能会对模型的泛化能力和保留原始技能产生负面影响。我们的研究探讨了在微调过程中地实际响应风格的影响。我们发现将地实际响应风格与LLM固有风格匹配会产生更好的学习结果。基于这一观点，我们开发了一种方法，最小程度地修改LLM的现有响应以更正错误，使用这些调整后的响应作为训练目标。这种技术能够实现与模型固有响应风格一致的精确更正，维护模型的核心能力，从而避免过多拟合。我们的研究结果表明，这种方法不仅提高了LLM的特定任务准确性，而且关键地

    arXiv:2402.11192v1 Announce Type: cross  Abstract: Fine-tuning large language models (LLMs) with a small data set for particular tasks is a widely encountered yet complex challenge. The potential for overfitting on a limited number of examples can negatively impact the model's ability to generalize and retain its original skills. Our research explores the impact of the style of ground-truth responses during the fine-tuning process. We found that matching the ground-truth response style with the LLM's inherent style results in better learning outcomes. Building on this insight, we developed a method that minimally alters the LLM's pre-existing responses to correct errors, using these adjusted responses as training targets. This technique enables precise corrections in line with the model's native response style, safeguarding the model's core capabilities and thus avoid overfitting. Our findings show that this approach not only improves the LLM's task-specific accuracy but also crucially
    
[^28]: 通过纯微调进行模型编辑

    Model Editing by Pure Fine-Tuning

    [https://arxiv.org/abs/2402.11078](https://arxiv.org/abs/2402.11078)

    纯微调通过优化条件似然、增加随机释义和事实的数据，在模型编辑中取得了不俗的表现。

    

    精细调整被认为在模型编辑中不够有效，因为相对更专业的方法而言，它的表现较差。然而，微调是简单的，不关心被编辑模型的体系结构细节，并且能够利用标准训练方法的不断进展（例如PEFT），使其成为模型编辑器的吸引选择。在本文中，我们展示了纯粹的微调可以是一种可行的模型编辑方法。我们提出了对朴素微调进行轻微修改的两个关键因素。第一，我们优化条件似然而非完整似然。第二，我们使用随机释义和事实来增加数据，以鼓励泛化和局部性。我们在ZsRE和CounterFact上的实验表明，这一简单修改使得微调通常可以与专业编辑器在编辑分数方面匹敌甚至超越。

    arXiv:2402.11078v1 Announce Type: cross  Abstract: Fine-tuning is dismissed as not effective for model editing due to its poor performance compared to more specialized methods. However, fine-tuning is simple, agnostic to the architectural details of the model being edited, and able to leverage ongoing advances in standard training methods (e.g., PEFT), making it an appealing choice for a model editor. In this work, we show that pure fine-tuning can be a viable approach to model editing. We propose a slight modification of naive fine-tuning with two key ingredients. First, we optimize the conditional likelihood rather than the full likelihood. Second, we augment the data with random paraphrases and facts to encourage generalization and locality. Our experiments on ZsRE and CounterFact show that this simple modification allows fine-tuning to often match or outperform specialized editors in the edit score.
    
[^29]: AFaCTA: 使用可靠的LLM标注者辅助事实性索赔检测的标注

    AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators

    [https://arxiv.org/abs/2402.11073](https://arxiv.org/abs/2402.11073)

    提出了一种新框架 AFaCTA，利用大型语言模型辅助事实性索赔的标注，提高了标注的效率和一致性。

    

    随着生成式人工智能的兴起，用于打击误导信息的自动事实核查方法变得越来越重要。然而，事实性索赔检测，即事实核查管道中的第一步，存在两个关键问题限制了其可伸缩性和泛化性：（1）任务定义和索赔概念的不一致性以及（2）手动标注的高成本。为了解决（1），我们审查了相关工作中的定义，并提出了一个聚焦于可验证性的事实性索赔的统一定义。为了解决（2），我们引入了AFaCTA（自动事实性索赔检测标注器），这是一个新颖的框架，利用大型语言模型（LLMs）在事实性索赔的标注中提供帮助。AFaCTA通过沿着三条预定义的推理路径保持一致性来校准其注释的置信度。在政治言论领域的大量评估和实验表明，AFaCTA能够高效地协助专业人员进行标注。

    arXiv:2402.11073v1 Announce Type: cross  Abstract: With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist exper
    
[^30]: II-MMR：在视觉问答中识别和改进多模态多跳推理

    II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in Visual Question Answering

    [https://arxiv.org/abs/2402.11058](https://arxiv.org/abs/2402.11058)

    II-MMR提出了一种新颖的方法，用于识别和改进视觉问答中的多模态多跳推理，通过引入答案预测引导的CoT提示和知识三元组引导的提示，实现对不同推理情况的分析和识别。

    

    视觉问答（VQA）通常涉及视觉和语言之间多样推理场景。然而，大多数先前的VQA研究仅关注评估模型的整体准确性，而没有在不同推理情况下对其进行评估。此外，一些最近的研究发现，传统的"CoT"提示无法有效生成VQA的推理，尤其是对于需要多跳推理的复杂场景。在本文中，我们提出了II-MMR，这是一个新颖的想法，用于识别和改进VQA中的多模态多跳推理。具体而言，II-MMR接受带有图像的VQA问题，并使用两种新颖的语言提示找到推理路径以获得答案：(i)答案预测引导的CoT提示，或者(ii)知识三元组引导的提示。然后，II-MMR分析这条路径，通过估计有多少跳和什么类型（即视觉或超出）来识别当前VQA基准中的不同推理情况。

    arXiv:2402.11058v1 Announce Type: cross  Abstract: Visual Question Answering (VQA) often involves diverse reasoning scenarios across Vision and Language (V&L). Most prior VQA studies, however, have merely focused on assessing the model's overall accuracy without evaluating it on different reasoning cases. Furthermore, some recent works observe that conventional Chain-of-Thought (CoT) prompting fails to generate effective reasoning for VQA, especially for complex scenarios requiring multi-hop reasoning. In this paper, we propose II-MMR, a novel idea to identify and improve multi-modal multi-hop reasoning in VQA. In specific, II-MMR takes a VQA question with an image and finds a reasoning path to reach its answer using two novel language promptings: (i) answer prediction-guided CoT prompt, or (ii) knowledge triplet-guided prompt. II-MMR then analyzes this path to identify different reasoning cases in current VQA benchmarks by estimating how many hops and what types (i.e., visual or beyon
    
[^31]: 多个LLM之间的网络形成与动态

    Network Formation and Dynamics Among Multi-LLMs

    [https://arxiv.org/abs/2402.10659](https://arxiv.org/abs/2402.10659)

    分析了多个LLM在社交网络中的行为，发现它们在给定网络结构并被询问形成网络偏好时表现出与人类社交动态一致的原则。

    

    社交网络影响行为、偏好和关系，在人类社会中对信息和规范的传播起着至关重要的作用。随着大型语言模型（LLMs）越来越多地融入社交和专业环境中，理解它们在社交网络和互动背景下的行为变得至关重要。我们的研究分析了标准网络结构和现实世界网络的行为，以确定多个LLMs的动态是否与人类社交动态一致。我们探讨了各种社交网络原则，包括微观层面的概念，如偏爱附着、三角闭合和同似性，以及宏观层面的概念，如社区结构和小世界现象。我们的研究发现表明，当向LLMs提供网络结构并询问它们对网络形成的偏好时，它们表现出所有这些原则。

    arXiv:2402.10659v1 Announce Type: cross  Abstract: Social networks influence behaviors, preferences, and relationships and play a crucial role in the dissemination of information and norms within human societies. As large language models (LLMs) increasingly integrate into social and professional environments, understanding their behavior within the context of social networks and interactions becomes essential. Our study analyzes the behaviors of standard network structures and real-world networks to determine whether the dynamics of multiple LLMs align with human social dynamics. We explore various social network principles, including micro-level concepts such as preferential attachment, triadic closure, and homophily, as well as macro-level concepts like community structure and the small-world phenomenon. Our findings suggest that LLMs demonstrate all these principles when they are provided with network structures and asked about their preferences regarding network formation. Furtherm
    
[^32]: MiMiC：表示空间中最小修改的对抗事实

    MiMiC: Minimally Modified Counterfactuals in the Representation Space

    [https://arxiv.org/abs/2402.09631](https://arxiv.org/abs/2402.09631)

    提出了一种新颖的对抗事实生成方法，利用闭式解决方案在表示空间中生成富有表达力的对抗事实，以减轻语言模型中的不良行为，该方法在地球移动问题方面提供理论上的保证，并对表示空间的几何组织进行改进。

    

    arXiv:2402.09631v1 公告类型：交叉学科 简介：语言模型经常表现出不良行为，如性别偏见或有毒语言。通过对表示空间进行干预，可以有效减轻这些问题，但两种常见的干预技术，即线性擦除和定向向量，并不能提供高度可控和表达丰富度。因此，我们提出了一种新颖的干预方法，旨在在表示空间中生成富有表达力的对抗事实，使源类别（例如“有毒”）的表示与目标类别（例如“非有毒”）的表示相似。这种方法利用高斯假设下的闭式解决方案，在地球移动问题方面提供了理论上的保证，并对表示空间的几何组织提供了进一步的改进。

    arXiv:2402.09631v1 Announce Type: cross  Abstract: Language models often exhibit undesirable behaviors, such as gender bias or toxic language. Interventions in the representation space were shown effective in mitigating such issues by altering the LM behavior. We first show that two prominent intervention techniques, Linear Erasure and Steering Vectors, do not enable a high degree of control and are limited in expressivity.   We then propose a novel intervention methodology for generating expressive counterfactuals in the representation space, aiming to make representations of a source class (e.g., ``toxic'') resemble those of a target class (e.g., ``non-toxic''). This approach, generalizing previous linear intervention techniques, utilizes a closed-form solution for the Earth Mover's problem under Gaussian assumptions and provides theoretical guarantees on the representation space's geometric organization. We further build on this technique and derive a nonlinear intervention that ena
    
[^33]: DoRA: 分解权重的低秩适应

    DoRA: Weight-Decomposed Low-Rank Adaptation

    [https://arxiv.org/abs/2402.09353](https://arxiv.org/abs/2402.09353)

    DoRA是一种将预训练权重分解为幅度和方向两个组成部分，并使用LoRA进行方向更新的低秩适应方法，通过增强学习能力和训练稳定性来提高对LLaMA，LLaVA和VL-B的微调性能。

    

    在广泛使用的参数高效调整（PEFT）方法中，由于避免了额外的推理成本，LoRA及其变种方法因此变得非常流行。然而，这些方法与完全微调（FT）之间仍然存在精度差距。在这项工作中，我们首先引入了一种新颖的权重分解分析方法来研究FT和LoRA之间的内在差异。为了模拟FT的学习能力，我们提出了一种称为DoRA的权重分解低秩适应方法。DoRA将预训练的权重分解为两个组成部分，幅度和方向，并且具体使用LoRA进行方向更新，以有效地减少可训练参数的数量。通过使用DoRA，我们增强了LoRA的学习能力和训练稳定性，同时避免了任何额外的推理开销。在微调LLaMA，LLaVA和VL-B上，DoRA始终优于LoRA。

    arXiv:2402.09353v1 Announce Type: new Abstract: Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-B
    
[^34]: SyntaxShap：用于文本生成的语法感知可解释性方法

    SyntaxShap: Syntax-aware Explainability Method for Text Generation

    [https://arxiv.org/abs/2402.09259](https://arxiv.org/abs/2402.09259)

    本文介绍了一种局部的、与模型无关的用于文本生成的可解释性方法SyntaxShap，其通过考虑文本数据中的语法来扩展Shapley值以解释序列到序列任务。通过采用博弈论方法，SyntaxShap只考虑由依赖树约束的联盟，与其他最新的可解释性方法相比具有良好的性能。

    

    为了在安全关键领域中利用大型语言模型的潜力，我们需要确保其预测的可解释性。然而，尽管模型可解释性受到了重要关注，但仍有一个尚未探索的领域，即使用针对文本数据量身定制的方法解释序列到序列任务。本文介绍了SyntaxShap，一种局部的、与模型无关的用于文本生成的可解释性方法，它考虑了文本数据中的语法。所提出的方法将Shapley值扩展到考虑基于解析的句法依赖关系。采用博弈论方法，SyntaxShap只考虑由依赖树约束的联盟。我们采用基于模型的评估来比较SyntaxShap及其加权形式与针对文本生成任务的最新可解释性方法，使用包括忠实度、复杂度、连贯性和解释与语义一致性的多样化指标。

    arXiv:2402.09259v1 Announce Type: cross Abstract: To harness the power of large language models in safety-critical domains we need to ensure the explainability of their predictions. However, despite the significant attention to model interpretability, there remains an unexplored domain in explaining sequence-to-sequence tasks using methods tailored for textual data. This paper introduces SyntaxShap, a local, model-agnostic explainability method for text generation that takes into consideration the syntax in the text data. The presented work extends Shapley values to account for parsing-based syntactic dependencies. Taking a game theoric approach, SyntaxShap only considers coalitions constraint by the dependency tree. We adopt a model-based evaluation to compare SyntaxShap and its weighted form to state-of-the-art explainability methods adapted to text generation tasks, using diverse metrics including faithfulness, complexity, coherency, and semantic alignment of the explanations to the
    
[^35]: SLEB: 通过冗余验证和消除Transformer块优化LLM的流程

    SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks

    [https://arxiv.org/abs/2402.09025](https://arxiv.org/abs/2402.09025)

    SLEB是一种通过消除冗余的Transformer块来优化LLM流程的新方法，它成功加速了LLM的推理过程。

    

    大型语言模型（LLM）在各种自然语言处理任务中证明了其高效性。然而，它们庞大的参数数量给实际部署带来了重大挑战。精简，一种旨在减小LLM大小和复杂度的技术，通过从网络中删除冗余组件提供了潜在解决方案。尽管精简有希望，但现有方法往往难以实现显著的端到端LLM推理加速。本文中，我们引入了SLEB，一种通过消除冗余的Transformer块来优化LLM流程的新方法。我们选择Transformer块作为精简的基本单位，因为LLM在相邻块的输出之间具有块级别的冗余和高相似性。这个选择使我们能够有效地增强LLM的处理速度。我们的实验证明，SLEB成功加速了LLM的推理过程。

    arXiv:2402.09025v1 Announce Type: new Abstract: Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB successfully accelerates LLM inference without
    
[^36]: Agent Smith:一张图像可以迅速越狱一百万个多模态LLM代理

    Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast

    [https://arxiv.org/abs/2402.08567](https://arxiv.org/abs/2402.08567)

    Agent Smith提出了一种安全问题，即传染性越狱，该问题在多代理环境中可以通过简单的越狱一个代理来迅速感染所有代理并导致有害行为。

    

    多模态大型语言模型（MLLM）代理可以接收指令，捕捉图像，从内存中检索历史记录，并决定使用哪些工具。然而，红队评估发现恶意图像/提示可以越狱MLLM并导致不对齐的行为。在这项工作中，我们报告了多代理环境中更严重的安全问题，称为传染性越狱。它涉及到对单个代理进行简单的越狱，无需来自对手的进一步干预，（几乎）所有代理将以指数级别被感染并展示有害行为。为了验证传染性越狱的可行性，我们模拟了包含高达一百万个LLaVA-1.5代理的多代理环境，并将随机匹配对聊天作为多代理交互的概念验证实例。我们的结果表明，将（传染性）恶意图像输入到任意选择的代理的内存中就足以实现传染性越狱。

    A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious 
    
[^37]: 朝着忠实和强大的基于证据的问答专家的方向前进

    Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering

    [https://arxiv.org/abs/2402.08277](https://arxiv.org/abs/2402.08277)

    这项工作探索了如何鲁棒地微调大型语言模型以提高答案的来源质量和答案归因能力，引入了数据生成流水线和四个测试集来评估模型的性能，并展示了在合成数据上微调可以改善内部和外部分布的性能。

    

    对大型语言模型（LLM）更忠实和可追踪的答案的进步对于各种研究和实践活动至关重要。其中一种达到这个目标的方法是基于可靠的来源提供答案。然而，这种基于证据的问答在使用LLM时已经证明在引用正确的来源（来源质量）和准确地表示来源中的信息（答案归因能力）方面工作不足。在这项工作中，我们系统地研究了如何鲁棒地微调LLM，以提高来源质量和答案归因能力。具体而言，我们引入了一个数据生成流水线，其中包括自动数据质量过滤器，可以大规模合成多样化的高质量训练和测试数据。我们还引入了四个测试集，以对微调后的专家模型的鲁棒性进行基准测试。广泛的评估结果表明，在合成数据上进行微调可以提高在内部和外部分布的性能。%基于证据的问答案例。此外，我们展示了用于评估的四个测试集，以评估微调后的专家模型的鲁棒性。

    Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we sho
    
[^38]: Lumos : 用场景文本识别增强多模式LLMs的能力

    Lumos : Empowering Multimodal LLMs with Scene Text Recognition

    [https://arxiv.org/abs/2402.08017](https://arxiv.org/abs/2402.08017)

    本论文介绍了Lumos，它是第一个具备文本理解能力的多模式问答系统，通过运用场景文本识别组件，能够从第一人称视角图像中提取文本，并将其用于加强多模式大型语言模型的输入。研究过程中，作者克服了与文本识别质量、延迟和模型推断相关的多个挑战，并提供了全面的组件评估结果，展示了高质量和高效率的性能。

    

    我们介绍了Lumos，它是第一个具备文本理解能力的端到端多模式问答系统。Lumos的核心是一个场景文本识别（STR）组件，用于从第一人称视角图像中提取文本，并将其用于增强多模式大型语言模型（MM-LLM）的输入。在构建Lumos的过程中，我们遇到了许多与STR质量、整体延迟和模型推断相关的挑战。在本文中，我们探讨了这些挑战，并讨论了用于克服这些障碍的系统架构、设计选择和建模技术。我们还对每个组件进行了全面评估，展示了高质量和高效率的性能。

    We introduce Lumos, the first end-to-end multimodal question-answering system with text understanding capabilities. At the core of Lumos is a Scene Text Recognition (STR) component that extracts text from first person point-of-view images, the output of which is used to augment input to a Multimodal Large Language Model (MM-LLM). While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference. In this paper, we delve into those challenges, and discuss the system architecture, design choices, and modeling techniques employed to overcome these obstacles. We also provide a comprehensive evaluation for each component, showcasing high quality and efficiency.
    
[^39]: 基于锚点的大型语言模型

    Anchor-based Large Language Models

    [https://arxiv.org/abs/2402.07616](https://arxiv.org/abs/2402.07616)

    基于锚点的大型语言模型（AnLLM）通过引入创新的基于锚点的自注意力网络（AnSAN）和基于锚点的推理策略，将序列信息压缩到锚点标记中，减少键/值缓存，提高推理效率。

    

    大型语言模型（LLMs）主要采用仅解码器的转换器架构，需要保留历史标记的键/值信息以提供上下文信息并避免冗余计算。然而，这些LLMs的巨大大小和参数量需要大量的GPU内存。这种内存需求随着输入文本的长度而增加，迫切需要更高效的信息存储和处理方法。本研究介绍了一种基于锚点的LLM（AnLLM），它利用了一种创新的基于锚点的自注意力网络（AnSAN）和基于锚点的推理策略。这种方法使LLMs能够将序列信息压缩成锚点标记，减少键/值缓存并提高推理效率。实验证明，AnLLM在减少键/值缓存高达99%和推理速度提高高达3.5倍的同时，仍保持可比的准确性。尽管牺牲了一些准确性，AnLLM的创新和贡献依然重要。

    Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces the Anchor-based LLM (AnLLM), which utilizes an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy. This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments show that the AnLLM maintains comparable accuracy with up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the AnLLM signi
    
[^40]: 开放理论-心灵（OpenToM）：评估大型语言模型的心灵理解能力的全面基准

    OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models

    [https://arxiv.org/abs/2402.06044](https://arxiv.org/abs/2402.06044)

    OpenToM是一个评估大型语言模型心理理解能力的全面基准，通过提供更长、更清晰的叙事故事、具有明确个性特征的角色、以及挑战模型对心理状态的理解能力的问题，揭示了现有模型在理解物理世界与心理世界的角色心理状态方面的优势和不足。

    

    神经心理理论（N-ToM）是机器理解和跟踪他人心理状态的能力，在开发具有社交智能的代理程序中至关重要。然而，目前的N-ToM基准存在一些问题，包括模糊和人工故事的存在，缺乏个性特征和偏好，缺乏涉及角色心理心态的问题，并且提出的问题多样性有限。为了应对这些问题，我们构建了OpenToM，一个新的评估N-ToM的基准，以 (1) 更长、更清晰的叙事故事，(2) 具有明确个性特征的角色，(3) 触发角色意图的行动，以及 (4) 设计旨在挑战LLMs对建模角色在物理和心理世界的心理状态能力的问题。使用OpenToM，我们发现目前最先进的LLMs在建模物理世界的一些心理状态方面表现出色，但在跟踪角色心理状态方面存在不足。

    Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characte
    
[^41]: 分层树状知识图谱用于学术调研

    Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey

    [https://arxiv.org/abs/2402.04854](https://arxiv.org/abs/2402.04854)

    该论文提出了一种分层树状知识图谱和推荐系统，帮助初学者研究者进行研究调研，填补了现有导航知识图谱的不足，并解决了学术论文推荐系统中高文本相似性带来的困惑。

    

    对于缺乏研究培训的初学者研究者来说，研究调查一直是一个挑战。这些研究者在短时间内很难理解他们研究主题内的方向，以及发现新的研究发现。为初学者研究者提供直观的帮助的一种方式是提供相关的知识图谱(KG)并推荐相关的学术论文。然而，现有的导航知识图谱主要依赖于研究领域的关键字，常常无法清楚地呈现多个相关论文之间的逻辑层次关系。此外，大多数学术论文推荐系统仅仅依赖于高文本相似性，这可能会让研究人员困惑为什么推荐了特定的文章。他们可能缺乏了解关于他们希望获得的"问题解决"和"问题发现"之间的见解连接的重要信息。为解决这些问题，本研究旨在支持初学者研究者进行研究调研。

    Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between "Issue resolved" and "Issue finding" that they hope to obtain. To address these issues, this study aims to support research insight surveys for begi
    
[^42]: Alirector: 提升对齐的中文语法错误修正器

    Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector

    [https://arxiv.org/abs/2402.04601](https://arxiv.org/abs/2402.04601)

    本文提出了一种提升对齐的中文语法错误修正器，旨在解决使用自回归生成模型时面临的过度修正挑战。该方法适用于序列到序列模型和仅解码的大型语言模型，并通过对齐模型进行多轮修正，提高修正效果。

    

    中文语法错误修正（CGEC）在使用自回归生成模型（如序列到序列模型和仅解码的大型语言模型）时面临严重的过度修正挑战。虽然以前的方法旨在解决序列到序列模型中的过度修正问题，但很难适应仅解码的大型语言模型。本文提出了一种提升对齐的解决方案，适用于序列到序列模型和仅解码的大型语言模型，并且能够解决过度修正问题。我们的方法首先训练一个修正模型，生成源句子的初始修正。然后，将源句子与初始修正结合起来，通过一个对齐模型进行另一轮修正，以促使对齐模型专注于潜在的过度修正。此外，为了增强模型识别细微差别的能力，我们进一步探索了源句子和初始修正的逆向对齐。最后，我们将对齐的知识转移到CGEC模型中，以提高修正效果。

    Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs). While previous methods aim to address overcorrection in Seq2Seq models, they are difficult to adapt to decoder-only LLMs. In this paper, we propose an alignment-enhanced corrector for the overcorrection problem that applies to both Seq2Seq models and decoder-only LLMs. Our method first trains a correction model to generate an initial correction of the source sentence. Then, we combine the source sentence with the initial correction and feed it through an alignment model for another round of correction, aiming to enforce the alignment model to focus on potential overcorrection. Moreover, to enhance the model's ability to identify nuances, we further explore the reverse alignment of the source sentence and the initial correction. Finally, we transfer the alignment kn
    
[^43]: Chatbot遇见管道：利用确定有限自动机增进大规模语言模型的能力

    Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton

    [https://arxiv.org/abs/2402.04411](https://arxiv.org/abs/2402.04411)

    本文介绍了DFA-LLM（确定有限自动机增强的大规模语言模型）框架，通过嵌入从对话中学习到的确定有限自动机在大规模语言模型中，使得对话代理能够生成具有规范合规性的回复，并在广泛的基准测试中验证了其有效性。

    

    本文介绍了一种新颖的框架——确定有限自动机增强的大规模语言模型（DFA-LLM），旨在通过使用大规模语言模型（LLM）提升对话代理的能力。传统的LLM在特定情景（如情感支持和客户服务）中生成规范合规的回复面临挑战。我们的框架通过将从训练对话中学习到的确定有限自动机（DFA）嵌入到LLM中来应对这些挑战。这种结构化的方法使得LLM能够按照DFA指导的确定性回应路径来回应。DFA-LLM的优势包括可解释性结构，上下文感知的对话回复检索以及与现有LLM的即插即用兼容性。广泛的基准测试验证了DFA-LLM的有效性，表明它有潜力成为对话代理领域的有价值的贡献。

    This paper introduces the Definite Finite Automaton augmented large language model (DFA-LLM), a novel framework designed to enhance the capabilities of conversational agents using large language models (LLMs). Traditional LLMs face challenges in generating regulated and compliant responses in special scenarios with predetermined response guidelines, like emotional support and customer service. Our framework addresses these challenges by embedding a Definite Finite Automaton (DFA), learned from training dialogues, within the LLM. This structured approach enables the LLM to adhere to a deterministic response pathway, guided by the DFA. The advantages of DFA-LLM include an interpretable structure through human-readable DFA, context-aware retrieval for responses in conversations, and plug-and-play compatibility with existing LLMs. Extensive benchmarks validate DFA-LLM's effectiveness, indicating its potential as a valuable contribution to the conversational agent.
    
[^44]: 线性时间最小贝叶斯风险解码与参考聚合

    Linear-time Minimum Bayes Risk Decoding with Reference Aggregation

    [https://arxiv.org/abs/2402.04251](https://arxiv.org/abs/2402.04251)

    本文提出了一种线性时间的最小贝叶斯风险解码方法，通过使用聚合参考表示来近似配对度量分数，将复杂度降低到线性级别，同时在保持大部分质量增益的同时提高了解码的效率。

    

    最小贝叶斯风险（MBR）解码是一种文本生成技术，已被证明可以提高机器翻译的质量，但即使使用基于采样的近似方法也很昂贵。除了需要大量采样序列外，还需要对效用度量进行配对计算，这具有二次复杂度。在本文中，我们提出使用聚合参考表示计算近似的配对度量分数。这将效用估计的复杂度从$O(n^2)$降低到$O(n)$，同时在经验上保持了MBR解码的大部分质量提升。我们在https://github.com/ZurichNLP/mbr上发布了我们的源代码。

    Minimum Bayes Risk (MBR) decoding is a text generation technique that has been shown to improve the quality of machine translations, but is expensive, even if a sampling-based approximation is used. Besides requiring a large number of sampled sequences, it requires the pairwise calculation of a utility metric, which has quadratic complexity. In this paper, we propose to approximate pairwise metric scores with scores calculated against aggregated reference representations. This changes the complexity of utility estimation from $O(n^2)$ to $O(n)$, while empirically preserving most of the quality gains of MBR decoding. We release our source code at https://github.com/ZurichNLP/mbr
    
[^45]: Video-LaVIT：统一的视频语言预训练及解耦的视觉-运动标记化方法

    Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization

    [https://arxiv.org/abs/2402.03161](https://arxiv.org/abs/2402.03161)

    这篇论文介绍了一种统一的视频语言预训练方法，通过解耦的视觉-运动标记化将视频表示为关键帧和时间运动，然后使用统一生成预训练技术来生成各种图像和视频内容。

    

    鉴于多模态大型语言模型(LLMs)的最新进展，越来越多的关注如何将其从图像-文本数据扩展到更具信息价值的现实世界视频。与静态图像相比，视频在有效的大规模预训练中面临着独特的挑战，原因在于需要对其时空动态进行建模。本文针对视频-语言预训练中的这些限制，提出了一种高效的视频分解方法，将每个视频表示为关键帧和时间运动。然后，使用设计良好的标记器将视觉和时间信息离散化为少量标记，并将其适应于LLM，从而实现对视频、图像和文本的统一生成预训练。在推理过程中，从LLM生成的标记被仔细恢复到原始的连续像素空间，以生成各种视频内容。我们提出的框架既能理解又能生成图像和视频内容，并通过在13个任务上的竞争性表现加以证明。

    In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13
    
[^46]: 异步计划推理中的图增强大型语言模型的研究

    Graph-enhanced Large Language Models in Asynchronous Plan Reasoning

    [https://arxiv.org/abs/2402.02805](https://arxiv.org/abs/2402.02805)

    本研究是关于使用大型语言模型(LLMs)进行异步计划推理的首次大规模研究。我们发现在没有提供任务解决过程插图的情况下，现有的LLMs表现不佳。为此，我们提出了一种称为Plan Like a Graph (PLaG)的新技术，通过将图与自然语言提示相结合，实现了最先进的结果。然而，当任务复杂性增加时，LLMs仍然存在严重降级的问题，突显了LLMs在模拟数字设备方面的局限性。这项研究为将LLMs作为高效自主代理迈出了重要一步。

    

    异步计划推理具有挑战性，因为它需要顺序和并行规划以优化时间成本。大型语言模型(LLMs)能在这个任务中成功吗？在这里，我们进行了第一次大规模研究来探讨这个问题。我们发现一组代表性的闭源和开源LLMs，包括GPT-4和LLaMA-2，在我们的基准测试AsyncHow中，在没有提供任务解决过程的插图的情况下表现不佳。我们提出了一种称为Plan Like a Graph (PLaG)的新技术，它将图与自然语言提示相结合，实现了最先进的结果。我们展示了虽然PLaG能提升模型性能，但在任务复杂性增加时，LLMs仍然遭受严重降级，突出了利用LLMs模拟数字设备的局限性。我们将我们的研究视为将LLMs用作高效自主代理的一个令人兴奋的步骤。

    Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can large language models (LLMs) succeed at this task? Here, we present the first large-scale study investigating this question. We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results. We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. We see our study as an exciting step towards using LLMs as efficient autonomous agents.
    
[^47]: KS-Lottery: 寻找多语言语言模型中的认证彩票

    KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models

    [https://arxiv.org/abs/2402.02801](https://arxiv.org/abs/2402.02801)

    KS-Lottery是一种寻找多语言语言模型中有效参数的方法，通过使用Kolmogorov-Smirnov检验来分析参数分布偏移，并证明了在嵌入层中可以找到认证的中奖票。这种方法可以在微调中获得与全面微调相当的性能，同时减少了所需的参数数量。

    

    彩票票证假说认为在随机初始化的神经网络中存在“中奖票”。在微调场景中，语言模型中是否存在中奖票？我们如何找到这样的中奖票？在本文中，我们提出了KS-Lottery，一种用于识别在多语言微调中高度有效的LLM参数的方法。我们的核心思想是使用Kolmogorov-Smirnov检验来分析微调前后参数的分布偏移。我们进一步理论证明了KS-Lottery可以在嵌入层中找到认证的中奖票，微调这些参数可以保证与全面微调相同的性能。通过在翻译任务上将KS-Lottery与其他参数高效调优算法进行比较，实验结果表明，KS-Lottery找到了一个更小的参数集来进行微调，同时达到了与全面微调LLM相当的性能。令人惊讶的是，我们发现微调18个标记的嵌入层

    The lottery ticket hypothesis posits the existence of ``winning tickets'' within a randomly initialized neural network. Do winning tickets exist for LLMs in fine-tuning scenarios? How can we find such winning tickets? In this paper, we propose KS-Lottery, a method to identify a small subset of LLM parameters highly effective in multilingual fine-tuning. Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after fine-tuning. We further theoretically prove that KS-Lottery can find the certified winning tickets in the embedding layer, fine-tuning on the found parameters is guaranteed to perform as well as full fine-tuning. Comparing KS-Lottery with other parameter-efficient tuning algorithms on translation tasks, the experimental results show that KS-Lottery finds a much smaller set of parameters for fine-tuning while achieving the comparable performance as full fine-tuning LLM. Surprisingly, we find that fine-tuning 18 tokens' embeddin
    
[^48]: 通过重复使用经过验证的电路增加语言模型的可信度

    Increasing Trust in Language Models through the Reuse of Verified Circuits

    [https://arxiv.org/abs/2402.02619](https://arxiv.org/abs/2402.02619)

    本文介绍了一种通过重复使用经过验证的电路来增加语言模型的可信度的方法。研究者通过构建数学和逻辑规范的框架，并对一个n位整数加法模型进行完全验证。他们插入训练好的加法模型到一个未经训练的模型中，通过训练组合模型执行加法和减法。他们发现加法电路在这两个任务中得到了广泛的重复使用，从而简化了减法模型的验证。

    

    语言模型（LMs）在各种预测任务中的应用越来越广泛，但它们的训练经常忽略罕见的边界情况，降低了它们的可靠性。在本文中，我们定义了一个严格的可信度标准，即任务算法和电路实现必须经过验证，考虑到边界情况，并且没有已知的故障模式。我们展示了通过使用数学和逻辑规范的框架来构建变压器模型，可以训练出满足这一标准的模型。在本文中，我们对一个n位整数加法模型进行了完全验证。为了展示经过验证的模块的重复使用性，我们将训练好的整数加法模型插入到一个未经训练的模型中，并训练组合模型同时执行加法和减法。我们发现加法电路在这两个任务中得到了广泛的重复使用，从而简化了更复杂的减法模型的验证。我们讨论了如何将经过验证的任务模块插入到语言模型中，以利用模型的重复使用来提高可验证性和可信度。

    Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of languag
    
[^49]: 通过大型语言模型（LLMs）发现更有效的张量网络结构搜索算法

    Discovering More Effective Tensor Network Structure Search Algorithms via Large Language Models (LLMs)

    [https://arxiv.org/abs/2402.02456](https://arxiv.org/abs/2402.02456)

    通过大型语言模型（LLMs），我们开发了GPTN-SS算法，用于自动设计更有效的张量网络结构搜索算法。实验证明，这些算法在探索和开发之间取得了更好的平衡，并在搜索高质量的TN结构方面展现出卓越的性能。

    

    张量网络结构搜索（TN-SS）旨在搜索适合表示高维问题的张量网络（TN）结构，极大地促进了TN在各种机器学习应用中的效果。然而，使用现有算法找到满意的TN结构仍然具有挑战性。为了开发更有效的算法并避免人力密集型的开发过程，我们利用大型语言模型（LLMs）中嵌入的知识来自动设计TN-SS算法。我们的方法称为GPTN-SS，利用了一种精心设计的基于LLM的提示系统，以类似进化的方式运行。从真实数据中得出的实验结果表明，GPTN-SS可以有效地利用现有方法获得的见解，开发出更好地平衡探索和开发之间关系的新型TN-SS算法。这些算法在搜索高质量TN结构方面表现出优秀的性能。

    Tensor network structure search (TN-SS), aiming at searching for suitable tensor network (TN) structures in representing high-dimensional problems, largely promotes the efficacy of TN in various machine learning applications. Nonetheless, finding a satisfactory TN structure using existing algorithms remains challenging. To develop more effective algorithms and avoid the human labor-intensive development process, we explore the knowledge embedded in large language models (LLMs) for the automatic design of TN-SS algorithms. Our approach, dubbed GPTN-SS, leverages an elaborate crafting LLM-based prompting system that operates in an evolutionary-like manner. The experimental results, derived from real-world data, demonstrate that GPTN-SS can effectively leverage the insights gained from existing methods to develop novel TN-SS algorithms that achieve a better balance between exploration and exploitation. These algorithms exhibit superior performance in searching the high-quality TN structur
    
[^50]: Aligner: 通过弱到强校正实现高效对齐

    Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction

    [https://arxiv.org/abs/2402.02416](https://arxiv.org/abs/2402.02416)

    Aligner是一种通过学习校正残差来实现高效对齐的方法，相比于传统的强化学习方法，Aligner具有参数高效、弱到强泛化以及即插即用的优势。

    

    对于大型语言模型（LLMs），通过强化学习来进行对齐的努力主要是通过人类反馈的强化学习方法进行的。然而，强化学习面临着主要的挑战，包括训练奖励模型、演员-评论家工程以及重要的是，需要访问LLM参数。在这里，我们介绍了一种新的高效对齐范式Aligner，它通过学习对齐和未对齐答案之间的校正残差来绕过整个强化学习过程。我们的Aligner具有几个关键优势。首先，它是一个基于自监督学习的自动回归seq2seq模型，通过训练查询-答案-校正数据集，提供了一种参数高效的对齐解决方案，并且对资源需求较少。其次，Aligner实现了从弱到强的泛化；通过Aligner的监督信号来微调大型预训练模型，可以显著提升性能。第三，Aligner作为一个模型不可知的即插即用模块，可以直接应用于…

    Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
    
[^51]: LLM的政治偏好分析

    The Political Preferences of LLMs

    [https://arxiv.org/abs/2402.01789](https://arxiv.org/abs/2402.01789)

    该研究对LLM的政治偏好进行了分析，结果发现大多数对话型LLM表现出左翼观点，但需谨慎解读基础模型在政治倾向测试中的分类。

    

    我们在这里报告了关于大型语言模型（LLMs）中内嵌的政治偏好的全面分析。具体而言，我们对24个最先进的对话型LLM进行了11项政治倾向测试，旨在确定测试者的政治偏好。结果表明，当使用具有政治含义的问题/陈述进行探究时，大多数对话型LLM倾向于生成被大多数政治测试仪器诊断为左翼观点的回答。我们注意到，这对于用于与人类对话优化的LLM基础模型并非如此。然而，基础模型在连贯回答问题方面表现不佳，需要对其政治倾向测试的分类进行谨慎解读。虽然还没有定论，但我们的结果为有趣的假设提供了初步证据，即政治偏好会嵌入其中。

    We report here a comprehensive analysis about the political preferences embedded in Large Language Models (LLMs). Namely, we administer 11 political orientation tests, designed to identify the political preferences of the test taker, to 24 state-of-the-art conversational LLMs, both close and open source. The results indicate that when probed with questions/statements with political connotations most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints. We note that this is not the case for base (i.e. foundation) models upon which LLMs optimized for conversation with humans are built. However, base models' suboptimal performance at coherently answering questions suggests caution when interpreting their classification by political orientation tests. Though not conclusive, our results provide preliminary evidence for the intriguing hypothesis that the embedding of political preferences
    
[^52]: 通过定向表示优化实现的安全提示驱动的大型语言模型(LLM)保护

    Prompt-Driven LLM Safeguarding via Directed Representation Optimization

    [https://arxiv.org/abs/2401.18018](https://arxiv.org/abs/2401.18018)

    通过研究模型表示的影响，我们发现安全提示并没有明显增强恶意和无害查询之间的区分，并提出了一种名为DRO的方法，用于自动优化安全提示。

    

    在大型语言模型(LLM)中，使用安全提示在模型输入之前是一种常见的保护实践，以使其不遵从包含恶意意图的查询。然而，安全提示的工作机制尚未完全理解，这妨碍了自动优化其以改善LLM安全性的潜力。针对这个问题，我们从模型表示的角度调查了安全提示的影响。我们发现在模型的表示空间中，有害和无害的查询可以在很大程度上区分开来，但安全提示并没有明显增强这一区分。相反，不同安全提示导致查询的表示朝着相似的方向移动，使得模型即使在查询无害时也更容易拒绝提供协助。受到这些发现的启发，我们提出了一种名为DRO（定向表示优化）的方法，用于自动安全提示优化。DRO将安全提示视为要优化的表示方向。

    Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts
    
[^53]: 大型语言模型中的时间箭头

    Arrows of Time for Large Language Models

    [https://arxiv.org/abs/2401.17505](https://arxiv.org/abs/2401.17505)

    这篇论文通过研究自回归大型语言模型的时间方向性，发现了模型在建模自然语言能力上存在时间上的不对称性。从信息理论的角度来看，这种差异理论上是不应该存在的。通过稀疏性和计算复杂性的考虑，提供了一个理论框架来解释这种不对称性的出现。

    

    我们通过时间方向性的视角研究了自回归大型语言模型的概率建模。我们在实证上发现这类模型在建模自然语言能力上存在时间上的不对称性：预测下一个记号和预测前一个记号时的平均对数困惑度存在差异。这种差异既微妙又在不同的模态（语言、模型大小、训练时间等）下非常一致。从信息理论的角度来看，这在理论上是令人惊讶的，不应该存在这样的差异。我们提供了一个理论框架，解释了这种不对称性如何出现在稀疏性和计算复杂性考虑中，并概述了我们的结果带来的一些展望。

    We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.
    
[^54]: LLaMP: 大型语言模型在高保真材料知识检索和提炼中的强大应用

    LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation

    [https://arxiv.org/abs/2401.17244](https://arxiv.org/abs/2401.17244)

    LLaMP是一个多模态的检索增强生成框架，能够在不进行微调的情况下，理解和集成各种材料科学概念的能力，检索相关数据，处理高阶数据以及总结固态合成过程。同时，LLaMP有效纠正了GPT-3.5内部知识的错误。

    

    减少大型语言模型（LLM）的错误信息对于科学中的可重复性至关重要。然而，LLM天生缺乏长期记忆，因此在特定领域的文献和数据上对其进行微调是一个非常困难、临时的和不可避免具有偏见的任务。在这里，我们介绍了LLaMP，这是一个多模态的检索增强生成（RAG）框架，由多个数据感知的推理与行动（ReAct）智能体动态与Materials Project (MP)上的计算和实验数据进行交互。在无需微调的情况下，LLaMP展示了理解和集成各种方式的材料科学概念的能力，能够即时获取相关数据存储，处理高阶数据（如晶体结构和弹性张量），并总结固态合成的多步骤过程。我们证明LLaMP有效纠正了GPT-3.5内部知识的错误，将频繁记录的能带间隙MAPE降低了5.21%，将显著的错误降低了1103.54%

    Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences where reproducibility is crucial. However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and inevitably biased task to fine-tune them on domain-specific literature and data. Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of multiple data-aware reasoning-and-acting (ReAct) agents that dynamically interact with computational and experimental data on Materials Project (MP). Without fine-tuning, LLaMP demonstrates an ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structures and elastic tensors), and summarize multi-step procedures for solid-state synthesis. We show that LLaMP effectively corrects errors in GPT-3.5's intrinsic knowledge, reducing a 5.21% MAPE on frequently-documented bandgaps and a significant 1103.54%
    
[^55]: 规划、创造、使用：对LLMs在实际复杂场景中全面工具利用的基准测试

    Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios

    [https://arxiv.org/abs/2401.17167](https://arxiv.org/abs/2401.17167)

    本论文介绍了一种新的基准测试UltraTool，旨在改善和评估LLMs在实际复杂场景中的工具利用能力。该基准测试关注从规划和创建到应用工具的整个过程，并强调实际复杂性和多步规划的要求。

    

    近期使用大型语言模型（LLMs）作为实际应用中的智能代理的趋势强调了对它们能力的全面评估的必要性，特别是在涉及规划、创造和使用工具的复杂场景中。然而，现有的基准测试通常只关注简单合成的查询，不反映实际复杂性，因此在评估工具利用方面的视角有限。为了解决这个问题，我们提出了一个新颖的基准测试UltraTool，旨在改善和评估LLMs在实际场景中工具利用的能力。UltraTool关注使用工具的整个过程——从规划和创建到在复杂任务中应用。它强调实际的复杂性，要求准确的多步规划以实现有效的问题解决。UltraTool的一个关键特点是在工具使用之前针对自然语言进行独立评估的规划，从而简化了任务解决的过程。

    The recent trend of using Large Language Models (LLMs) as intelligent agents in real-world applications underscores the necessity for comprehensive evaluations of their capabilities, particularly in complex scenarios involving planning, creating, and using tools. However, existing benchmarks typically focus on simple synthesized queries that do not reflect real-world complexity, thereby offering limited perspectives in evaluating tool utilization. To address this issue, we present UltraTool, a novel benchmark designed to improve and evaluate LLMs' ability in tool utilization within real-world scenarios. UltraTool focuses on the entire process of using tools - from planning and creating to applying them in complex tasks. It emphasizes real-world complexities, demanding accurate, multi-step planning for effective problem-solving. A key feature of UltraTool is its independent evaluation of planning with natural language, which happens before tool usage and simplifies the task solving by m
    
[^56]: ProLex: 一种以语言熟练度为导向的词汇替换评估基准

    ProLex: A Benchmark for Language Proficiency-oriented Lexical Substitution

    [https://arxiv.org/abs/2401.11356](https://arxiv.org/abs/2401.11356)

    ProLex是一个以语言熟练度为导向的词汇替换的评估基准，旨在评估生成适当替代词和表现更好语言熟练度的系统能力。使用微调任务特定合成数据的Llama2-13B模型在F分数上优于ChatGPT 3.2%，与GPT-4在ProLex上表现相当。

    

    词汇替换是在上下文句子中为给定的目标词找到合适的替代词。然而，这个任务没有考虑到与目标词同等或更高熟练度的替代词，这对于希望提高写作水平的语言学习者来说可能是有益的。为了弥补这个差距，我们提出了一项新任务，即以语言熟练度为导向的词汇替换。我们还引入了ProLex，一个新颖的基准，旨在评估系统生成不仅合适的替代词还要表现出更好语言熟练度的能力。除了基准，我们提出了可以自动执行这个新任务的模型。我们证明了我们最好的模型，即使用任务特定合成数据微调的Llama2-13B模型，在F分数上平均优于ChatGPT 3.2％，并在ProLex上与GPT-4取得可比较的结果。

    Lexical Substitution discovers appropriate substitutes for a given target word in a context sentence. However, the task fails to consider substitutes that are of equal or higher proficiency than the target, an aspect that could be beneficial for language learners looking to improve their writing. To bridge this gap, we propose a new task, language proficiency-oriented lexical substitution. We also introduce ProLex, a novel benchmark designed to assess systems' ability to generate not only appropriate substitutes but also substitutes that demonstrate better language proficiency. Besides the benchmark, we propose models that can automatically perform the new task. We show that our best model, a Llama2-13B model fine-tuned with task-specific synthetic data, outperforms ChatGPT by an average of 3.2% in F-score and achieves comparable results with GPT-4 on ProLex.
    
[^57]: MM-SAP：用于评估多模态大型语言模型自我意识在感知中的全面基准

    MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception

    [https://arxiv.org/abs/2401.07529](https://arxiv.org/abs/2401.07529)

    本论文提出了一个新的基准MM-SAP，旨在评估多模态大型语言模型在感知中的自我意识能力，填补了先前研究中忽视的领域。

    

    多模态大型语言模型（MLLMs）近期的进展展示了其在视觉感知和理解方面出色的能力。然而，这些模型也存在幻觉问题，这限制了它们作为人工智能系统的可靠性。我们认为这些幻觉部分原因在于模型在理解从图像中能够和不能够感知的内容方面存在困难，这种能力我们称之为感知中的自我意识。尽管重要性重大，在先前的研究中却忽视了MLLMs的这个方面。本文旨在定义和评估MLLMs在感知中的自我意识。为此，我们首先引入了感知中的知识象限，这有助于定义MLLMs对图像了解和不了解的内容。利用这一框架，我们提出了一项新颖的基准，即专门设计用于评估这种能力的MLLMs感知中的自我意识基准（MM-SAP）。我们将MM-SAP应用于各种情况

    arXiv:2401.07529v2 Announce Type: replace-cross  Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in visual perception and understanding. However, these models also suffer from hallucinations, which limit their reliability as AI systems. We believe that these hallucinations are partially due to the models' struggle with understanding what they can and cannot perceive from images, a capability we refer to as self-awareness in perception. Despite its importance, this aspect of MLLMs has been overlooked in prior studies. In this paper, we aim to define and evaluate the self-awareness of MLLMs in perception. To do this, we first introduce the knowledge quadrant in perception, which helps define what MLLMs know and do not know about images. Using this framework, we propose a novel benchmark, the Self-Awareness in Perception for MLLMs (MM-SAP), specifically designed to assess this capability. We apply MM-SAP to a variety of 
    
[^58]: 图语言模型

    Graph Language Models

    [https://arxiv.org/abs/2401.07105](https://arxiv.org/abs/2401.07105)

    引入了一种新型的图语言模型（GLM），结合线性化和图神经网络的优点，解决了传统方法在处理结构化知识图谱时的弱点。

    

    虽然语言模型（LMs）是自然语言处理的主力军，它们与结构化知识图谱（KGs）的相互作用仍在积极研究中。当前用于编码这些图形的方法通常要么（i）将它们线性化以供LM嵌入--这样会低效利用结构信息，要么（ii）使用图神经网络（GNNs）来保留图结构--但GNNs无法像预训练的LM一样很好地表示文本特征。在我们的工作中，我们引入了一种新型LM类型，即图语言模型（GLM），它整合了两种方法的优点并减轻了它们的弱点。GLM参数从预训练的LM中初始化，以增强对个别图概念和三元组的理解。同时，我们设计GLM的架构以整合图偏差，从而促进图内的知识分布。这使GLM能够处理图形、文本以及两者的交织输入。实证

    arXiv:2401.07105v2 Announce Type: replace-cross  Abstract: While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs -- which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure -- but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM's architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical 
    
[^59]: 大型语言模型可以学习时间推理

    Large Language Models Can Learn Temporal Reasoning

    [https://arxiv.org/abs/2401.06853](https://arxiv.org/abs/2401.06853)

    本文提出了一个新的TG-LLM框架，以语言为基础进行时间推理，通过教导LLM将上下文翻译成时间图，并使用CoTs指导LLM进行符号推理。

    

    尽管大型语言模型（LLMs）展示了出色的推理能力，但它们并非没有缺陷和不准确之处。最近的研究介绍了各种方法来减轻这些局限性。特别是，时间推理（TR）对LLMs提出了重大挑战，因为它依赖于多样的时间表达和复杂的上下文细节。本文中，我们提出了TG-LLM，一个致力于基于语言的时间推理的新框架。具体而言，我们首先教导LLM将上下文翻译成时间图（TG）。我们构建了一个全可控且需要最少监督的合成数据集，用于在这个图翻译任务上进行微调。我们在实验证实，学习在我们数据集上的TG提取能力可以转移到其他TR任务和基准测试上。除此之外，我们使用CoTs引导LLM通过TG进行符号推理。

    arXiv:2401.06853v2 Announce Type: replace  Abstract: While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal expressions and intricate contextual details. In this paper, we propose TG-LLM, a new framework towards language-based TR. To be specific, we first teach LLM to translate the context into a temporal graph (TG). A synthetic dataset, which is fully controllable and requires minimal supervision, is constructed for fine-tuning on this graph translation task. We confirm in experiments that the capability of TG extraction learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we guide LLM to perform symbolic reasoning over the TG via Chain of Thoughts (CoTs) bootstrappin
    
[^60]: 利用大型语言模型进行分割与重述任务

    Split and Rephrase with Large Language Models

    [https://arxiv.org/abs/2312.11075](https://arxiv.org/abs/2312.11075)

    评估了大型语言模型在Split and Rephrase任务上的表现，表明在主要指标上有显著改进，但在分割一致性方面仍有待提高。

    

    Split and Rephrase (SPRP)任务旨在将复杂句子分解为一系列更短的符合语法规则的句子，同时保持原始含义，有助于人类和机器处理复杂文本。这也是一个有价值的测试平台，可以评估自然语言处理模型，因为其需要对复杂的语法方面进行建模。在这项工作中，我们评估了大型语言模型在该任务上的表现，显示它们可以在主要指标上比现有技术有很大改进，尽管在拆分一致性方面仍有差距。来自两项人类评估的结果进一步支持自动度量结果得出的结论。我们提供了一项全面的研究，包括提示变体、领域转移、参数规模和训练数据量不同的微调预训练语言模型，同时与指导调整的零射和少射方法进行对比。

    arXiv:2312.11075v3 Announce Type: replace  Abstract: The Split and Rephrase (SPRP) task, which consists in splitting complex sentences into a sequence of shorter grammatical sentences, while preserving the original meaning, can facilitate the processing of complex texts for humans and machines alike. It is also a valuable testbed to evaluate natural language processing models, as it requires modelling complex grammatical aspects. In this work, we evaluate large language models on the task, showing that they can provide large improvements over the state of the art on the main metrics, although still lagging in terms of splitting compliance. Results from two human evaluations further support the conclusions drawn from automated metric results. We provide a comprehensive study that includes prompting variants, domain shift, fine-tuned pretrained language models of varying parameter size and training data volumes, contrasted with both zero-shot and few-shot approaches on instruction-tuned 
    
[^61]: PixT3：基于像素的表格到文本生成

    PixT3: Pixel-based Table To Text generation

    [https://arxiv.org/abs/2311.09808](https://arxiv.org/abs/2311.09808)

    PixT3是一种基于像素的多模式表格到文本模型，通过将数据到文本生成视为视觉识别任务，消除了字符串格式的需求，克服了线性化和输入大小限制的挑战。

    

    Table-to-text生成涉及根据结构化表格数据生成适当的文本描述。近年来，由于神经网络模型的流行和大规模数据集的可用性，它引起了越来越多的关注。现有方法的一个共同特点是将输入视为字符串，即通过采用线性化技术，不总是保留表格中的信息，过于冗长，缺乏空间效率。我们提出将数据到文本生成重新思考为一个视觉识别任务，消除了将输入呈现为字符串格式的必要性。我们提出了PixT3，一种多模式表格到文本模型，克服了现有模型遇到的线性化和输入大小限制的挑战。PixT3通过新的自监督学习目标进行训练，以加强表格结构意识，并适用于开放式和受控生成设置。

    arXiv:2311.09808v2 Announce Type: replace  Abstract: Table-to-text generation involves generating appropriate textual descriptions given structured tabular data. It has attracted increasing attention in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. A common feature across existing methods is their treatment of the input as a string, i.e., by employing linearization techniques that do not always preserve information in the table, are verbose, and lack space efficiency. We propose to rethink data-to-text generation as a visual recognition task, removing the need for rendering the input in a string format. We present PixT3, a multimodal table-to-text model that overcomes the challenges of linearization and input size limitations encountered by existing models. PixT3 is trained with a new self-supervised learning objective to reinforce table structure awareness and is applicable to open-ended and controlled generation settings.
    
[^62]: CLEAN-EVAL：清洁评估污染大型语言模型

    CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models

    [https://arxiv.org/abs/2311.09154](https://arxiv.org/abs/2311.09154)

    Clean-Eval提出了一种清洁评估方法，通过LLM对污染数据进行释义和反向翻译，利用语义检测器过滤低质量样本，最终选择最佳候选，解决了大型语言模型评估中的数据污染问题。

    

    我们目前正处于各种大型语言模型（LLM）激烈竞争的时代，不断推动基准性能的边界。然而，由于潜在的数据污染，真正评估这些LLM的能力已经成为一个具有挑战性和关键性的问题，研究人员和工程师需要花费大量时间和精力下载和尝试这些受污染的模型。为了节省宝贵的时间，我们提出了一种新颖而有用的方法，Clean-Eval，它可以减轻数据污染问题，并以更整洁的方式评估LLM。Clean-Eval利用LLM对受污染数据进行释义和反向翻译，生成具有相同含义但不同表面形式的表达。然后使用语义检测器过滤生成的低质量样本，缩小候选集。最终从这个候选集中基于BLEURT得分选择最佳候选。

    arXiv:2311.09154v2 Announce Type: replace  Abstract: We are currently in an era of fierce competition among various large language models (LLMs) continuously pushing the boundaries of benchmark performance. However, genuinely assessing the capabilities of these LLMs has become a challenging and critical issue due to potential data contamination, and it wastes dozens of time and effort for researchers and engineers to download and try those contaminated models. To save our precious time, we propose a novel and useful method, Clean-Eval, which mitigates the issue of data contamination and evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to paraphrase and back-translate the contaminated data into a candidate set, generating expressions with the same meaning but in different surface forms. A semantic detector is then used to filter the generated low-quality samples to narrow down this candidate set. The best candidate is finally selected from this set based on the BLEURT s
    
[^63]: 对抗偏好优化

    Adversarial Preference Optimization

    [https://arxiv.org/abs/2311.08045](https://arxiv.org/abs/2311.08045)

    提出了一种对抗偏好优化（APO）框架，实现了在没有额外注释的情况下，通过对抗学习自适应于生成分布差距。

    

    人类偏好调整是提高大型语言模型（LLMs）交互质量的关键。现有的对齐方法依赖于手动注释的偏好数据来指导LLM的优化方向。然而，在实践中，持续更新LLMs会导致模型生成样本与人类首选响应之间存在分布差距，这阻碍了模型微调的效率。为了缓解这个问题，先前的方法需要在生成的样本上额外进行偏好注释，以适应转移分布，这需要大量的注释资源。针对更高效的人类偏好优化，我们提出了一种对抗偏好优化（APO）框架，其中LLM代理和偏好模型通过极小-极大博弈交替更新。在没有额外注释的情况下，我们的APO方法可以通过对抗学习自适应于生成分布差距。

    arXiv:2311.08045v2 Announce Type: replace-cross  Abstract: Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing aligning methods depend on manually annotated preference data to guide the LLM optimization directions. However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency. To mitigate this issue, previous methods require additional preference annotation on generated samples to adapt the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an adversarial preference optimization (APO) framework, where the LLM agent and the preference model update alternatively via a min-max game. Without additional annotation, our APO method can make a self-adaption to the generation distribution gap through the adversarial learni
    
[^64]: 关于衡量自然语言解释的忠诚度或自一致性

    On Measuring Faithfulness or Self-consistency of Natural Language Explanations

    [https://arxiv.org/abs/2311.07466](https://arxiv.org/abs/2311.07466)

    本文论述了衡量自然语言解释的忠诚度或自一致性的问题。我们提出了自一致性测试来评估解释的输出级别的一致性。我们通过构建比较一致性测试库，并引入了新的自一致性度量CC-SHAP来支持我们的观点。

    

    大型语言模型（LLMs）可以通过事后或思维链（CoT）解释其预测。但是，LLM可能会编造听起来合理但不忠实于其基本推理的解释。最近的工作设计了旨在判断事后或CoT解释忠实度的测试。在这项工作中，我们认为这些忠实度测试不是衡量模型内部工作的忠实度，而是衡量其输出级别的自一致性。我们的贡献有三个方面：i）我们在模型可解释性的背景下澄清了忠实度测试的地位，将其描述为自一致性测试。我们通过ii）构建了一个比较一致性的测试库，首次在11个开放式LLMs和5个任务的通用套件上比较了现有测试，包括iii）我们的新的自一致性度量CC-SHAP。CC-SHAP是LLM自一致性的细粒度度量（而不是测试）。它进行比较。

    Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares 
    
[^65]: 重新审视假设：预训练的Transformer是否通过梯度下降在上下文中学习？

    Revisiting the Hypothesis: Do pretrained Transformers Learn In-Context by Gradient Descent?

    [https://arxiv.org/abs/2310.08540](https://arxiv.org/abs/2310.08540)

    本研究重新审视了预训练的Transformer是否通过梯度下降在上下文中学习的假设，并发现现有研究中的假设存在限制性假设，使其与实际语言模型训练时的语境存在显著差异。同时，通过对真实模型的观察和比较，揭示了ICL和GD在观察演示顺序上的不同敏感性。

    

    LLM中的In-Context Learning（ICL）的出现仍然是一个重要现象，但我们对其了解甚少。为了解释ICL，最近的研究尝试在理论上将其与梯度下降（GD）联系起来。我们问，这种联系在实际预训练模型中是否成立？我们强调先前作品中的限制性假设使得它们的语境与语言模型实际训练时的实际语境差别很大。例如，这些研究中使用的理论手工构造的权重具有与真实LLM不匹配的属性。此外，他们的实验验证使用ICL目标（明确为ICL训练模型），这与野外出现的ICL有所不同。我们还寻找了真实模型中的证据。我们观察到ICL和GD对于观察演示的顺序有不同的敏感性。最后，我们在自然环境中探讨并比较ICL与GD假设。

    arXiv:2310.08540v4 Announce Type: replace-cross  Abstract: The emergence of In-Context Learning (ICL) in LLMs remains a significant phenomenon with little understanding. To explain ICL, recent studies try to theoretically connect it to Gradient Descent (GD). We ask, does this connection hold up in actual pre-trained models?   We highlight the limiting assumptions in prior works that make their context considerably different from the practical context in which language models are trained. For example, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. Furthermore, their experimental verification uses ICL objective (training models explicitly for ICL), which differs from the emergent ICL in the wild.   We also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. 
    
[^66]: LangBridge: 无多语言监督的多语言推理方法

    LangBridge: Multilingual Reasoning Without Multilingual Supervision. (arXiv:2401.10695v1 [cs.CL])

    [http://arxiv.org/abs/2401.10695](http://arxiv.org/abs/2401.10695)

    LangBridge是一种无需多语言监督的多语言推理方法，通过连接两个模型来适应多语言推理任务，尽管只使用英文数据进行训练，但它显著提高了语言模型对低资源语言的性能。

    

    我们引入LangBridge，这是一种零监督方式，用于适应多语言推理任务，无需多语言监督。LangBridge通过连接两个模型来运作，每个模型专门处理不同方面：(1)一个专门处理多种语言的模型（例如mT5编码器），和(2)一个专门处理推理的模型（例如Orca 2）。LangBridge通过在两个模型之间引入最少的可训练参数来连接它们。尽管只利用英文数据进行训练，但LangBridge显著提升了语言模型在数学推理、编码和逻辑推理方面对低资源语言的性能。我们的分析表明，LangBridge的有效性源于多语言表示的不受语言限制的特性。我们公开发布了我们的代码和模型。

    We introduce LangBridge, a zero-shot approach to adapt language models for multilingual reasoning tasks without multilingual supervision. LangBridge operates by bridging two models, each specialized in different aspects: (1) one specialized in understanding multiple languages (e.g., mT5 encoder) and (2) one specialized in reasoning (e.g., Orca 2). LangBridge connects the two models by introducing minimal trainable parameters between them. Despite utilizing only English data for training, LangBridge considerably enhances the performance of language models on low-resource languages across mathematical reasoning, coding, and logical reasoning. Our analysis suggests that the efficacy of LangBridge stems from the language-agnostic characteristics of multilingual representations. We publicly release our code and models.
    
[^67]: 对比性偏好优化：推动机器翻译中LLM性能的边界

    Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation. (arXiv:2401.08417v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.08417](http://arxiv.org/abs/2401.08417)

    本研究通过引入对比性偏好优化（CPO）的方法，弥合了大型语言模型（LLM）在机器翻译中性能与传统编码器-解码器模型之间的差距，实现了更好的翻译效果。

    

    中等规模的大型语言模型（LLM）——7B或者13B参数的模型在机器翻译（MT）任务中表现出有希望的性能。然而，即使是表现最好的13B LLM翻译模型，如ALMA，也无法达到现有的最先进的传统编码器-解码器翻译模型或者更大规模的LLM（如GPT-4）的性能水平。本研究旨在弥合这一性能差距。首先，我们评估了监督微调在MT任务中针对LLM的不足之处，强调了尽管是人工生成的参考数据，但其中存在质量问题。然后，与模仿参考翻译的SFT相反，我们引入了对比性偏好优化（CPO），一种新的方法，训练模型避免生成仅仅合乎要求但不完美的翻译。将CPO应用于仅有22K对句子和12M参数的ALMA模型中，可以显著提高性能。得到的模型称为ALMA-R，其性能可以达到或超过WMT比赛的获胜水平。

    Moderate-sized large language models (LLMs) -- those with 7B or 13B parameters -- exhibit promising machine translation (MT) performance. However, even the top-performing 13B LLM-based translation models, like ALMA, does not match the performance of state-of-the-art conventional encoder-decoder translation models or larger-scale LLMs such as GPT-4. In this study, we bridge this performance gap. We first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data, despite being human-generated. Then, in contrast to SFT which mimics reference translations, we introduce Contrastive Preference Optimization (CPO), a novel approach that trains models to avoid generating adequate but not perfect translations. Applying CPO to ALMA models with only 22K parallel sentences and 12M parameters yields significant improvements. The resulting model, called ALMA-R, can match or exceed the performance of the WMT competition winn
    
[^68]: 语言模型是更像图书馆还是图书管理员？Bibliotechnism，小说引用问题和LLM的态度。

    Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs. (arXiv:2401.04854v1 [cs.CL])

    [http://arxiv.org/abs/2401.04854](http://arxiv.org/abs/2401.04854)

    本文探讨了语言模型（LLMs）是更像图书馆还是图书管理员的问题。论文首先阐述了 "文献主义 "这一概念，并提出了对其的挑战，指出LLMs生成的全新文本在内容上依赖于原始人类文本的内容。然后，论文提出了对 "文献主义"的新颖挑战，讨论了LLMs生成的 "新引用"问题。最后，根据心灵哲学中的解释主义，论文提出了有限代理能力的LLMs可能存在的可能性。

    

    LLMs（语言模型）是否像复印机或印刷机等文化技术一样，传输信息但无法创建新内容？我们将这个概念称为"文献主义"，它面临一个挑战，即LLMs经常生成全新的文本。我们首先为"文献主义"对抗这个挑战进行辩护，展示了新的文本仅在派生意义上具有意义，因此这些生成的文本的内容在重要意义上依赖于原始人类文本的内容。然后，我们提出了一个不同的、新颖的挑战，即LLMs生成"新引用"的例子，使用新的名称来引用新实体。如果LLMs不是文化技术而是具有有限形式的代理能力（信念、欲望和意图），这样的例子可以很好地解释。根据心灵哲学中的解释主义，仅当一个系统的行为可以通过假设它具有信念、欲望和意图来很好地解释时，它才具有这样的信念、欲望和意图。

    Are LLMs cultural technologies like photocopiers or printing presses, which transmit information but cannot create new content? A challenge for this idea, which we call bibliotechnism, is that LLMs often do generate entirely novel text. We begin by defending bibliotechnism against this challenge, showing how novel text may be meaningful only in a derivative sense, so that the content of this generated text depends in an important sense on the content of original human text. We go on to present a different, novel challenge for bibliotechnism, stemming from examples in which LLMs generate "novel reference", using novel names to refer to novel entities. Such examples could be smoothly explained if LLMs were not cultural technologies but possessed a limited form of agency (beliefs, desires, and intentions). According to interpretationism in the philosophy of mind, a system has beliefs, desires and intentions if and only if its behavior is well-explained by the hypothesis that it has such s
    
[^69]: 《批评的批评》

    The Critique of Critique. (arXiv:2401.04518v1 [cs.CL])

    [http://arxiv.org/abs/2401.04518](http://arxiv.org/abs/2401.04518)

    本文创新性地提出了元批评(MetaCritique)的框架，通过精确度和召回率评估批评的质量，并以F1分数作为整体评分。为了获得可靠的评估结果，引入了原子信息单元(AIUs)来描述批评。元批评提供了自然语言的理由来支持评价结果。

    

    批评作为一种用于评估模型生成内容质量的自然语言描述，在训练、评估和改进大型语言模型(LLMs)中被证明起着重要作用。然而，在评估批评本身质量方面缺乏原则性的理解。本文首创了批评的批评，称为元批评，这是一个评估批评的框架，从精确度和召回率两个方面来评估批评。我们计算精确度和召回率的调和平均值作为整体评分，称为F1分数。为了获得可靠的评估结果，我们提出了原子信息单元(AIUs)，以更精细的方式描述批评。元批评考虑每个AIU，并聚合每个AIU的判断得到整体评分。此外，鉴于评估过程涉及复杂的推理，我们的元批评提供了自然语言的理由来支持评价结果。

    Critique, as a natural language description for assessing the quality of model-generated content, has been proven to play an essential role in the training, evaluation, and refinement of Large Language Models (LLMs). However, there is a lack of principled understanding in evaluating the quality of the critique itself. In this paper, we pioneer the critique of critique, termed MetaCritique, which is a framework to evaluate the critique from two aspects, i.e., factuality as precision score and comprehensiveness as recall score. We calculate the harmonic mean of precision and recall as the overall rating called F1 score. To obtain a reliable evaluation outcome, we propose Atomic Information Units (AIUs), which describe the critique in a more fine-grained manner. MetaCritique takes each AIU into account and aggregates each AIU's judgment for the overall score. Moreover, given the evaluation process involves intricate reasoning, our MetaCritique provides a natural language rationale to supp
    
[^70]: 重写代码：一种用于大型语言模型增强代码搜索的简单方法

    Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search. (arXiv:2401.04514v1 [cs.SE])

    [http://arxiv.org/abs/2401.04514](http://arxiv.org/abs/2401.04514)

    本论文提出一种扩展的生成增强检索（GAR）框架，通过对代码进行重写来解决代码搜索中存在的风格不匹配问题，实验结果表明该方法显著提高了检索准确性。

    

    在代码搜索中，生成增强检索（GAR）框架是一种有前景的策略，通过生成示例代码片段来增强查询，以解决代码片段和自然语言查询之间的主要模态不匹配问题，尤其是在大型语言模型（LLM）展示了代码生成能力的情况下。然而，我们的初步调查发现，LLM增强框架所提供的改进有一定的限制。这种限制可能是因为生成的代码，尽管在功能上准确，但在代码库中与基准代码之间经常显示出明显的风格偏差。在本文中，我们扩展了基础GAR框架，并提出了一种简单而有效的方法，通过对代码库中的代码进行重写（ReCo）来进行风格规范化。实验结果表明，ReCo显著提高了检索准确性。

    In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy ac
    
[^71]: 作为大型语言模型的指导数据探索者的单次学习方法

    One Shot Learning as Instruction Data Prospector for Large Language Models. (arXiv:2312.10302v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.10302](http://arxiv.org/abs/2312.10302)

    本研究提出了一种名为Nuggets的新颖有效方法，利用单次学习从庞大的数据集中选择高质量的指导数据，通过评估示例对多样锚定集的困惑度影响，选择对指导调优最有益的数据

    

    将大型语言模型与人类对齐是有效利用其预训练能力的关键步骤。当前的指导调优方法通常依赖于扩展数据集大小，但缺乏确保数据质量的明确策略，这可能无意中引入噪声并降低模型性能。为了应对这一挑战，我们引入了一种新颖高效的方法Nuggets，该方法利用单次学习从庞大的数据集中选择高质量的指导数据。Nuggets评估单个指导示例作为有效单次示例的潜力，从而识别可以显著提升各种任务性能的示例。Nuggets利用基于候选示例对多样锚定集的困惑度影响的评分系统，有助于选择对指导调优最有益的数据。通过在两个基准测试集MT-Bench和Alpaca-Ev上进行严格测试

    Aligning large language models(LLMs) with human is a critical step in effectively utilizing their pre-trained capabilities across a wide array of language tasks. Current instruction tuning practices often rely on expanding dataset size without a clear strategy for ensuring data quality, which can inadvertently introduce noise and degrade model performance. To address this challenge, we introduce Nuggets, a novel and efficient methodology that employs one shot learning to select high-quality instruction data from expansive datasets. Nuggets assesses the potential of individual instruction examples to act as effective one shot examples, thereby identifying those that can significantly enhance diverse task performance. Nuggets utilizes a scoring system based on the impact of candidate examples on the perplexity of a diverse anchor set, facilitating the selection of the most beneficial data for instruction tuning. Through rigorous testing on two benchmarks, including MT-Bench and Alpaca-Ev
    
[^72]: UP4LS: 由多个属性构建的用户信息用于增强语言隐写分析

    UP4LS: User Profile Constructed by Multiple Attributes for Enhancing Linguistic Steganalysis. (arXiv:2311.01775v1 [cs.CL])

    [http://arxiv.org/abs/2311.01775](http://arxiv.org/abs/2311.01775)

    本文提出了一个名为UP4LS的新框架，通过多个属性构建用户信息以增强语言隐写分析的性能。实验结果显示，该框架可以有效提升隐写分析的性能。

    

    语言隐写分析任务旨在有效检测通过语言隐写术生成的隐写物。现有的隐写分析方法忽视了用户个性化特征，导致在社交网络中表现较差。隐写物的有限出现进一步增加了检测的复杂性。本文提出了一个新颖的框架 UP4LS，用于增强隐写分析性能，该框架采用用户信息为核心。具体来说，通过深入分析帖子内容，我们探索了用户属性，如写作习惯、心理状态和关注领域，从而为隐写分析构建了用户信息。对于每个属性，我们设计了特征提取模块。通过现有方法中的深度学习网络，将提取到的特征映射到高维用户特征上。然后，使用语言模型来提取内容特征。将用户和内容特征进行集成以优化特征表示。在训练阶段，我们优先考虑隐写物的分布。实验表明，UP4LS 可以有效提升隐写分析的性能。

    Linguistic steganalysis (LS) tasks aim to effectively detect stegos generated by linguistic steganography. Existing LS methods overlook the distinctive user characteristics, leading to weak performance in social networks. The limited occurrence of stegos further complicates detection. In this paper, we propose the UP4LS, a novel framework with the User Profile for enhancing LS performance. Specifically, by delving into post content, we explore user attributes like writing habits, psychological states, and focal areas, thereby building the user profile for LS. For each attribute, we design the identified feature extraction module. The extracted features are mapped to high-dimensional user features via deep-learning networks from existing methods. Then the language model is employed to extract content features. The user and content features are integrated to optimize feature representation. During the training phase, we prioritize the distribution of stegos. Experiments demonstrate that 
    
[^73]: Transformers学会了高阶优化方法用于上下文学习：一项与线性模型的研究

    Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models. (arXiv:2310.17086v1 [cs.LG])

    [http://arxiv.org/abs/2310.17086](http://arxiv.org/abs/2310.17086)

    Transformers学会了高阶优化方法，用于上下文学习，通过实现类似于迭代牛顿法的算法，而不是梯度下降。

    

    Transformers在上下文学习中表现出色，但是它们是如何进行上下文学习仍然是一个谜。最近的研究表明，Transformers可能通过内部运行梯度下降，即一阶优化方法，来进行上下文学习。本文中，我们展示了Transformers学会了实现高阶优化方法来进行上下文学习。我们以上下文线性回归为重点，展示了Transformers学会了实现一个非常类似于迭代牛顿法的算法，而不是梯度下降。从实证上来看，我们展示了连续的Transformer层的预测与牛顿法的不同迭代非常接近，每个中间层大致计算了3次迭代。相比之下，需要指数级的梯度下降步骤才能匹配额外的Transformer层；这表明Transformers具有相当的收敛速率。

    Transformers are remarkably good at in-context learning (ICL) -- learning from demonstrations without parameter updates -- but how they perform ICL remains a mystery. Recent work suggests that Transformers may learn in-context by internally running Gradient Descent, a first-order optimization method. In this paper, we instead demonstrate that Transformers learn to implement higher-order optimization methods to perform ICL. Focusing on in-context linear regression, we show that Transformers learn to implement an algorithm very similar to Iterative Newton's Method, a higher-order optimization method, rather than Gradient Descent. Empirically, we show that predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations. In contrast, exponentially more Gradient Descent steps are needed to match an additional Transformers layer; this suggests that Transformers have an comparable rate of conv
    
[^74]: LLM-集成应用中的提示注入攻击和防御

    Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])

    [http://arxiv.org/abs/2310.12815](http://arxiv.org/abs/2310.12815)

    本文提出了一个通用框架来形式化提示注入攻击，并系统化防御这种类型的攻击。

    

    大型语言模型（LLMs）越来越多地用作各种称为LLM-集成应用的实际应用程序的后端。最近的多项研究表明，LLM-集成应用容易受到提示注入攻击的威胁，攻击者可以将恶意指令/数据注入这些应用程序的输入中，以达到攻击者的预期结果。然而，现有的研究仅限于案例研究，缺乏对提示注入攻击及其防御的系统理解。本论文旨在填补这一空白。我们提出了一个通用框架来形式化提示注入攻击，并将研究论文和博客文章中讨论的现有攻击视为我们框架的特例。我们的框架使我们能够通过组合现有攻击设计新的攻击方式。此外，我们还提出了一个系统化提示注入攻击防御的框架。利用我们的框架，我们可以预防和缓解这种类型的攻击。

    Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we con
    
[^75]: 通过可迁移的对抗攻击实现对齐大型语言模型的自动幻觉评估

    Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks. (arXiv:2310.12516v1 [cs.CL])

    [http://arxiv.org/abs/2310.12516](http://arxiv.org/abs/2310.12516)

    本文提出了一种通过可迁移的对抗攻击在大型语言模型中自动生成评估数据的方法，并使用ChatGPT和Natural Questions（NQ）数据集进行了验证。

    

    尽管在使用指令调整和检索增强技术防止大型语言模型（LLM）的幻觉方面取得了显著进展，但衡量LLM的可靠性仍然具有挑战性，因为人工评估数据对于许多任务和领域来说并不可用且可能存在数据泄漏。受到对抗机器学习的启发，本文旨在开发一种通过适当修改LLM在其中表现忠实的现有数据来自动生成评估数据的方法。具体而言，本文提出了一种基于LLM的框架AutoDebug，使用提示链接来生成以问答示例形式的可迁移对抗攻击。我们希望了解这些示例在多大程度上触发了LLM的幻觉行为。我们使用ChatGPT实现了AutoDebug，并对一个热门的开放领域问答数据集Natural Questions（NQ）的两个变体进行了评估。

    Although remarkable progress has been achieved in preventing large language model (LLM) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of LLMs using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. Inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which LLMs behave faithfully. Specifically, this paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. We seek to understand the extent to which these examples trigger the hallucination behaviors of LLMs.  We implement AutoDebug using ChatGPT and evaluate the resulting two variants of a popular open-domain question-answering dataset, Natural Questions (NQ), on a collection of open-sour
    
[^76]: TextBind: 多轮交错多模态指令跟随

    TextBind: Multi-turn Interleaved Multimodal Instruction-following. (arXiv:2309.08637v1 [cs.CL])

    [http://arxiv.org/abs/2309.08637](http://arxiv.org/abs/2309.08637)

    TextBind是一个注释极少的框架，用于将较大规模的语言模型赋予多轮交错多模态指令跟随能力，并通过图像-标题对生成多轮多模态指令-回应对话。这个框架对于解决实际任务具有重要意义，并为未来的研究提供了数据集、模型和演示。

    

    具有指令跟随能力的大型语言模型已经在人工智能领域产生了革命性的影响。这些模型通过其自然语言界面展示了卓越的泛化能力，可以解决各种实际任务。然而，它们的性能在很大程度上依赖于高质量的示例数据，而这往往很难获得。当涉及到多模态指令跟随时，这个挑战变得更加严峻。我们引入了TextBind，这是一个几乎不需要注释的框架，用于赋予较大规模的语言模型多轮交错多模态指令跟随能力。我们的方法仅需要图像-标题对，并从语言模型生成多轮多模态指令-回应对话。我们发布了我们的数据集、模型和演示，以促进未来在多模态指令跟随领域的研究。

    Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. We release our dataset, model, and demo to foster future research in the area of multimodal instruction following.
    
[^77]: 思想算法：增强大型语言模型中的思想探索

    Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models. (arXiv:2308.10379v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10379](http://arxiv.org/abs/2308.10379)

    本论文提出了一种名为"思想算法"的策略，通过算法推理路径推动大型语言模型的思想探索，以低成本、低存储和低计算开销的方式扩展了其推理能力。结果显示，使用算法指导的大型语言模型的性能可以超越算法本身。

    

    当前的文献旨在超越“连续思维”的方法，通常采用外部操作方法，在生成过程中停止、修改，然后恢复以增强大型语言模型（LLM）的推理能力。这种模式增加了查询请求的数量，增加了成本、内存和计算开销。针对这个问题，我们提出了思想算法——一种新颖的策略，通过算法推理路径推动LLM，开创了一种新的上下文学习模式。通过使用算法示例，我们利用LLM的固有循环动力学，仅使用一个或少数几个查询扩展其思想探索。我们的技术优于早期的单次查询方法，并与最近采用广泛的树搜索算法的多次查询策略不相上下。有趣的是，我们的结果表明，使用算法指导LLM可以使性能超越算法本身，这暗示着

    Current literature, aiming to surpass the "Chain-of-Thought" approach, often resorts to an external modus operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. This mode escalates the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways, pioneering a new mode of in-context learning. By employing algorithmic examples, we exploit the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and stands on par with a recent multi-query strategy that employs an extensive tree search algorithm. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting 
    
[^78]: 使用强化学习的离散提示压缩

    Discrete Prompt Compression with Reinforcement Learning. (arXiv:2308.08758v1 [cs.CL])

    [http://arxiv.org/abs/2308.08758](http://arxiv.org/abs/2308.08758)

    本研究提出了一种使用强化学习的离散提示压缩方法（PCRL），以解决指令调整的语言模型中嵌入训练的挑战。PCRL采用了一种计算效率高的策略网络直接编辑提示，可以灵活应用于各种类型的LM，而不需要梯度访问或标记数据。

    

    指令调整的语言模型（LM）被用户广泛使用来解决与任务特定提示相关的各种问题。由于上下文窗口长度和计算成本的限制，鼓励开发压缩提示的方法。现有方法严重依赖于训练嵌入，这些嵌入被设计为容纳多个记号含义。这在解释性、固定数量的嵌入记号、在不同LM之间的可重用性以及与黑盒API交互时的不适用性方面带来了挑战。本研究提出了一种使用强化学习的提示压缩方法（PCRL），它解决了这些问题。PCRL采用了一种计算效率高的策略网络，直接编辑提示。PCRL的训练方法可以灵活地应用于各种类型的LM，以及只有解码器和编码器-解码器架构，而不需要使用梯度访问LM或标记数据进行训练。

    Instruction-tuned Language Models (LMs) are widely used by users to address various problems with task-specific prompts. Constraints associated with the context window length and computational costs encourage the development of compressed prompts. Existing methods rely heavily on training embeddings, which are designed to accommodate multiple token meanings. This presents challenges in terms of interpretability, a fixed number of embedding tokens, reusability across different LMs, and inapplicability when interacting with black-box APIs. This study proposes prompt compression with reinforcement learning (PCRL), a novel discrete prompt compression method that addresses these issues. PCRL employs a computationally efficient policy network that directly edits prompts. The PCRL training approach can be flexibly applied to various types of LMs, as well as decoder-only and encoder-decoder architecture, and can be trained without gradient access to LMs or labeled data. PCRL achieves an averag
    
[^79]: 多模态多损失融合网络

    Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])

    [http://arxiv.org/abs/2308.00264](http://arxiv.org/abs/2308.00264)

    多模态多损失融合网络通过最佳选择和融合多个模态的特征，提高了情感检测的性能，并在多个数据集上实现了最先进的结果。这些研究结果表明了用于增强神经网络中情感检测的特征选择和融合方法的优化方向。

    

    在这项工作中，我们研究了跨多个模态选择和融合特征的最佳方法，并将其组合在神经网络中以改善情感检测。我们比较了不同的融合方法并且研究了多损失训练在多模态融合网络中的影响，从而确定了与子网性能相关的有用发现。我们最好的模型在三个数据集（CMU-MOSI、CMU-MOSEI和CH-SIMS）上实现了最先进的性能，并且在大多数指标上优于其他方法。我们发现，训练多模态特征可以改善单模态测试，并且基于数据集注释模式设计融合方法可以增强模型性能。这些结果表明了在神经网络中增强情感检测的优化特征选择和融合方法的发展方向。

    In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
    
[^80]: ValiTex -- 一种用于计算文本的社会科学构建度量的统一验证框架

    ValiTex -- a uniform validation framework for computational text-based measures of social science constructs. (arXiv:2307.02863v1 [cs.CL])

    [http://arxiv.org/abs/2307.02863](http://arxiv.org/abs/2307.02863)

    ValiTex是一个统一的验证框架，旨在帮助学者们基于文本数据来度量社会科学构建。它借鉴了心理测量学的传统，通过概念模型和动态检查表提供了验证的结构和步骤。

    

    关于如何验证计算文本的社会科学构建度量的指导是分散的。虽然学者们普遍认识到验证他们的文本度量的重要性，但他们通常缺乏共同的术语和统一的框架来进行验证。本文介绍了一个名为ValiTex的新验证框架，旨在帮助学者们基于文本数据来度量社会科学构建。该框架借鉴了心理测量学中长期存在的传统，同时扩展了框架以适用于计算文本分析的目的。ValiTex包括两个组成部分，一个是概念模型，一个是动态检查表。概念模型提供了一个通用的结构，可以指导验证的不同阶段，动态检查表定义了具体的验证步骤，并提供了哪些步骤可能被认为是推荐的（即提供相关和必要的验证证据）或可选的（即对提供额外信息有用的）。

    Guidance on how to validate computational text-based measures of social science constructs is fragmented. Whereas scholars are generally acknowledging the importance of validating their text-based measures, they often lack common terminology and a unified framework to do so. This paper introduces a new validation framework called ValiTex, designed to assist scholars to measure social science constructs based on textual data. The framework draws on a long-established tradition within psychometrics while extending the framework for the purpose of computational text analysis. ValiTex consists of two components, a conceptual model, and a dynamic checklist. Whereas the conceptual model provides a general structure along distinct phases on how to approach validation, the dynamic checklist defines specific validation steps and provides guidance on which steps might be considered recommendable (i.e., providing relevant and necessary validation evidence) or optional (i.e., useful for providing 
    
[^81]: 元推理：用于大型语言模型的语义符号解构

    Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models. (arXiv:2306.17820v1 [cs.CL])

    [http://arxiv.org/abs/2306.17820](http://arxiv.org/abs/2306.17820)

    本论文提出了一种称为“元推理”的方法，它通过使用语义符号解构的方式，将不同推理问题转化为类似的自然语言表示，以提高大型语言模型的推理能力。

    

    大型语言模型中的符号化方法已经被证明可以有效提高语言模型的推理能力。然而，大多数这些方法依赖于将自然语言映射到更加语法完备且没有歧义的形式语言（例如Python、SQL）。虽然这些方法有效，但它们离开了自然语言本身，偏离了人类思维的习惯，而更多地迎合了计算机的执行思维方式。相反，我们希望从语言学中符号的概念出发来简化自然语言，使得语言模型可以学习不同自然语义中包含的推理问题的常见表达方式和通用解决方案。基于这种考虑，我们提出了“元推理”，它允许语言模型自动完成语义符号的解构，即语义解析，从而最大程度地将某些推理任务的不同问题减少到类似的自然语言表示，从而获得推理的能力。

    Symbolization methods in large language models (LLMs) have been shown effective to improve LLMs' reasoning ability. However, most of these approaches hinge on mapping natural languages to formal languages (e.g., Python, SQL) that are more syntactically complete and free of ambiguity. Although effective, they depart from the natural language itself and deviate from the habits of human thinking, and instead cater more to the execution mindset of computers. In contrast, we hope to simplify natural language by starting from the concept of symbols in linguistics itself, so that LLMs can learn the common formulation and general solution of reasoning problems wrapped in different natural semantics. From this consideration, we propose \textbf{Meta-Reasoning}, which allows LLMs to automatically accomplish semantic-symbol deconstruction, i.e., semantic resolution, to maximally reduce different questions of certain reasoning tasks to similar natural language representation, thus gaining the abili
    
[^82]: FLuRKA: 快速融合低秩和核注意力

    FLuRKA: Fast fused Low-Rank & Kernel Attention. (arXiv:2306.15799v1 [cs.LG])

    [http://arxiv.org/abs/2306.15799](http://arxiv.org/abs/2306.15799)

    FLuRKA是一种融合低秩和核注意力的新型transformer类别，相较于传统的近似技术，在运行时间性能和质量方面都表现出显著的提升。

    

    自从transformer结构的提出以来，许多高效的近似自注意力技术已经变得流行起来。其中两种流行的技术类别是低秩和核方法。我们观察到这两种方法的优势相互补充，利用这些协同效应来融合低秩和核方法，产生了一种新的transformer类别：FLuRKA（快速低秩和核注意力）。FLuRKA相对于这些近似技术提供了可观的性能提升，并且具有高质量。我们在理论和实证方面评估了FLuRKA的运行时间性能和质量。我们的运行时间分析提供了多种参数配置，在这些配置下，FLuRKA具有加速效果；我们的准确性分析限定了FLuRKA相对于全注意力的误差。我们实例化了三种FLuRKA变体，相对于低秩和核方法分别实现了高达3.3倍和1.7倍的经验加速。这意味着更快的运行时间，而且质量仍然保持不错。

    Many efficient approximate self-attention techniques have become prevalent since the inception of the transformer architecture. Two popular classes of these techniques are low-rank and kernel methods. Each of these methods has its own strengths. We observe these strengths synergistically complement each other and exploit these synergies to fuse low-rank and kernel methods, producing a new class of transformers: FLuRKA (Fast Low-Rank and Kernel Attention). FLuRKA provide sizable performance gains over these approximate techniques and are of high quality. We theoretically and empirically evaluate both the runtime performance and quality of FLuRKA. Our runtime analysis posits a variety of parameter configurations where FLuRKA exhibit speedups and our accuracy analysis bounds the error of FLuRKA with respect to full-attention. We instantiate three FLuRKA variants which experience empirical speedups of up to 3.3x and 1.7x over low-rank and kernel methods respectively. This translates to spe
    
[^83]: 跨语言跨时代摘要：数据集、模型和评估

    Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation. (arXiv:2306.12916v1 [cs.CL])

    [http://arxiv.org/abs/2306.12916](http://arxiv.org/abs/2306.12916)

    本文全面研究了跨语言跨时代摘要任务，使用历史幻想文本和维基百科摘要构建了第一个CLCTS语料库，并研究了流行的变压器模型及其中间任务微调的有效性；同时还探讨了ChatGPT在CLCTS中作为摘要器和评估器的潜力。最终发现中间任务微调的端到端模型产生了中等到差的效果，而ChatGPT在没有微调的情况下提供中等到好的摘要质量表现。

    

    尽管摘要已经得到了广泛的研究，但跨语言跨时代摘要(CLCTS)是一个潜力巨大但鲜有研究的领域，它有可能提高跨文化的可访问性、信息共享和理解。本文全面研究了CLCTS任务，包括数据集创建、建模和评估。我们构建了第一个CLCTS语料库，利用历史幻想文本和英语、德语维基百科摘要，并研究了流行的变压器端到端模型以及带有不同中间任务微调任务的有效性。此外，我们探讨了ChatGPT在CLCTS中作为摘要器和评估器的潜力。总体而言，我们报告了人类、ChatGPT以及几个最近的自动评估指标的评估结果，发现我们的中间任务微调的端到端模型产生了从差到中等的摘要质量；ChatGPT作为摘要器(没有任何微调)，提供了中等到好的摘要质量表现。

    While summarization has been extensively researched in natural language processing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a largely unexplored area that has the potential to improve cross-cultural accessibility, information sharing, and understanding. This paper comprehensively addresses the CLCTS task, including dataset creation, modeling, and evaluation. We build the first CLCTS corpus, leveraging historical fictive texts and Wikipedia summaries in English and German, and examine the effectiveness of popular transformer end-to-end models with different intermediate task finetuning tasks. Additionally, we explore the potential of ChatGPT for CLCTS as a summarizer and an evaluator. Overall, we report evaluations from humans, ChatGPT, and several recent automatic evaluation metrics where we find our intermediate task finetuned end-to-end models generate bad to moderate quality summaries; ChatGPT as a summarizer (without any finetuning) provides moderate to good qua
    
[^84]: 使用知识链推动提升语言模型的推理能力

    Boosting Language Models Reasoning with Chain-of-Knowledge Prompting. (arXiv:2306.06427v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.06427](http://arxiv.org/abs/2306.06427)

    该论文提出了一种新的知识链提示（CoK）方法，旨在引导语言模型生成明确的知识证据，以提升推理能力，并通过F^2-Verification方法评估推理的准确性和可信度。

    

    最近，链式思维（CoT）提示在复杂的推理任务上取得了成功，其目标是设计一个简单的提示，如“我们一步一步地思考”或多个上下文示例，以及设计良好的理由，以引导大型语言模型（LLM）生成中间推理步骤。然而，生成的理由往往带有错误，导致不准确和不可信的推理链。为了减少这种脆弱性，我们提出了一种新颖的知识链提示（CoK），旨在引导LLM生成显性的知识证据，以结构化三元组的形式呈现。这一灵感来自于人类行为，即在回答复杂问题之前，我们可以在脑海中绘制思维导图或知识图作为推理证据。通过使用CoK，我们额外引入了一种F^2-Verification方法来估计推理链的可靠性，包括准确性和可信度。对于不可靠的回答，错误的证据可以

    Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can
    
[^85]: 语言模型的物理学：第一部分，上下文无关文法。

    Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])

    [http://arxiv.org/abs/2305.13673](http://arxiv.org/abs/2305.13673)

    本研究探究了生成式语言模型如何学习上下文无关文法（CFG），并通过构造人造数据证明了预训练transformers可以学会生成具有接近完美准确度和显着多样性的句子。研究发现transformer内部的隐藏状态隐含而精确地编码了CFG结构，学会形成类似动态规划的“边界到边界”的注意力。此外，还研究了标准CFG的扩展，例如概率CFG和线性CFG，并证明transformers也可以学会这些扩展语法结构。

    

    我们设计了实验来研究生成式语言模型（例如GPT）如何学习上下文无关文法（CFG）-具有树状结构的多样化语言系统，可捕捉许多自然语言，程序和人类逻辑的方面。CFG与下推自动机一样困难，可能是模棱两可的，因此验证字符串是否满足规则需要动态规划。我们构造了人造数据，并证明即使对于非常具有挑战性的CFG，预训练transformers也可以学会生成具有接近完美准确度和显着多样性的句子。更重要的是，我们深入探讨了transformers学习CFG背后的物理原理。我们发现transformer内部的隐藏状态隐含而精确地编码了CFG结构（如在子树边界上精确定位树节点信息），并学会形成类似动态规划的“边界到边界”的注意力。我们还涵盖了一些标准CFG的扩展，例如概率CFG和线性CFG，并展示transformers也可以学会这些扩展语法结构。我们的工作揭示了语言模型的内部工作原理，并为未来的模型设计和分析提供了启示。

    We design experiments to study $\textit{how}$ generative language models, like GPT, learn context-free grammars (CFGs) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. CFGs are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging CFGs, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable $\textit{diversity}$.  More importantly, we delve into the $\textit{physical principles}$ behind how transformers learns CFGs. We discover that the hidden states within the transformer implicitly and $\textit{precisely}$ encode the CFG structure (such as putting tree node information exactly on the subtree boundary), and learn to form "boundary to boundary" attentions that resemble dynamic programming. We also cover some extensio
    
[^86]: ZeroNLG: 将领域对齐和自编码用于零样本多模态和多语言自然语言生成

    ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation. (arXiv:2303.06458v1 [cs.CL])

    [http://arxiv.org/abs/2303.06458](http://arxiv.org/abs/2303.06458)

    ZeroNLG是一个零样本学习框架，可以处理多个NLG任务，包括图像到文本、视频到文本和文本到文本，跨越英语、中文、德语和法语。它不需要任何标记的下游对进行训练，通过将不同的领域投影到共享的公共潜在空间中的相应坐标，桥接不同领域之间的差异。

    ZeroNLG is a zero-shot learning framework that can handle multiple NLG tasks, including image-to-text, video-to-text, and text-to-text, across English, Chinese, German, and French. It does not require any labeled downstream pairs for training, and bridges the differences between different domains by projecting them to corresponding coordinates in a shared common latent space.

    自然语言生成（NLG）接受以图像、视频或文本形式的输入数据，并生成相应的自然语言文本作为输出。现有的NLG方法主要采用监督方法，并且严重依赖于耦合的数据到文本对。然而，对于许多有针对性的场景和非英语语言，往往没有足够数量的标记数据。为了放松对下游任务标记数据的依赖性，我们提出了一个直观有效的零样本学习框架ZeroNLG，它可以处理多个NLG任务，包括图像到文本（图像字幕）、视频到文本（视频字幕）和文本到文本（神经机器翻译），跨越英语、中文、德语和法语在一个统一的框架内。ZeroNLG不需要任何标记的下游对进行训练。在训练期间，ZeroNLG（i）将不同的领域（跨模态和语言）投影到共享的公共潜在空间中的相应坐标；（ii）桥接差异

    Natural Language Generation (NLG) accepts input data in the form of images, videos, or text and generates corresponding natural language text as output. Existing NLG methods mainly adopt a supervised approach and rely heavily on coupled data-to-text pairs. However, for many targeted scenarios and for non-English languages, sufficient quantities of labeled data are often not available. To relax the dependency on labeled data of downstream tasks, we propose an intuitive and effective zero-shot learning framework, ZeroNLG, which can deal with multiple NLG tasks, including image-to-text (image captioning), video-to-text (video captioning), and text-to-text (neural machine translation), across English, Chinese, German, and French within a unified framework. ZeroNLG does not require any labeled downstream pairs for training. During training, ZeroNLG (i) projects different domains (across modalities and languages) to corresponding coordinates in a shared common latent space; (ii) bridges diff
    

