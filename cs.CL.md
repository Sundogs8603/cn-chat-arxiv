# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning](https://arxiv.org/abs/2403.03218) | WMDP基准是一个公开发布的数据集，包含4157个多项选择问题，用作生物安全、网络安全和化学安全危险知识的代理测量。 |
| [^2] | [MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets](https://arxiv.org/abs/2403.03194) | MAGID是一个用于将仅文本对话增强为多样性和高质量图像的框架，通过创新的反馈循环生成高质量和多模态对话。 |
| [^3] | [Reliable, Adaptable, and Attributable Language Models with Retrieval](https://arxiv.org/abs/2403.03187) | 持续面临挑战的参数化语言模型LMs，作者主张使用具有检索功能的LMs作为下一代LMs，以提高可靠性、适应性和可追溯性。 |
| [^4] | [SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection](https://arxiv.org/abs/2403.03170) | SNIFFER是一种专门为超出上下文误传检测和解释而设计的新型多模大语言模型，通过两阶段指导微调，在解释生成方面取得了突破。 |
| [^5] | [PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset](https://arxiv.org/abs/2403.03167) | 论文提出了一个名为PARADISE的任务，通过实用程序文本进行演绎推理，评估语言模型仅从给定目标推断计划的能力 |
| [^6] | [Design2Code: How Far Are We From Automating Front-End Engineering?](https://arxiv.org/abs/2403.03163) | 生成式人工智能在多模态理解和代码生成方面取得了突破，提出了Design2Code任务并进行了全面基准测试，展示了多模态LLMs直接将视觉设计转换为代码实现的能力。 |
| [^7] | [Language Guided Exploration for RL Agents in Text Environments](https://arxiv.org/abs/2403.03141) | 引入了语言引导探索（LGE）框架，利用大型语言模型（LLM）为RL代理提供决策级别的引导，相比于普通RL代理和其他复杂方法，在具有挑战性的文本环境中取得了显著的优势。 |
| [^8] | [CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following](https://arxiv.org/abs/2403.03129) | 提出了一个协作生成框架CoGenesis，整合大型和小型模型，以逻辑方式解决隐私问题。 |
| [^9] | [Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution](https://arxiv.org/abs/2403.03121) | 大型语言模型中存在性别化情绪归因，反映了社会刻板印象。 |
| [^10] | ["In Dialogues We Learn": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning](https://arxiv.org/abs/2403.03102) | 提出了一种In-Dialogue Learning框架，通过对话历史刻画个人设来完成个性化对话生成任务，无需预定义个人资料，并在实验证明其显著改进对话生成性能。 |
| [^11] | [KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents](https://arxiv.org/abs/2403.03101) | KnowAgent引入了显式动作知识，通过动作知识库和知识型自学习策略来增强LLM的规划能力，从而改善语言Agent的规划表现。 |
| [^12] | [NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models](https://arxiv.org/abs/2403.03100) | NaturalSpeech 3利用分解设计的扩散模型实现零-shot方式生成自然语音 |
| [^13] | [Detecting Concrete Visual Tokens for Multimodal Machine Translation](https://arxiv.org/abs/2403.03075) | 提出新方法用于检测和选择在多模态机器翻译中具体的视觉相关令牌，通过这些方法，在翻译任务中实现了性能的改进和更好的视觉上下文利用 |
| [^14] | [Adding Multimodal Capabilities to a Text-only Translation Model](https://arxiv.org/abs/2403.03045) | 将性能优异的文本机器翻译模型转化为多模态机器翻译模型，通过连接视觉-文本适配器层并利用门控机制，在Multi30k数据集和典型的仅文本数据集上取得良好效果。 |
| [^15] | [Learning to Use Tools via Cooperative and Interactive Agents](https://arxiv.org/abs/2403.03031) | 提出了一种ConAgents框架，通过合作和互动代理使大型语言模型能够学习使用工具，并引入了迭代校准方法，以解决单个代理执行多样化操作能力有限和自适应纠正错误困难的问题。 |
| [^16] | [Socratic Reasoning Improves Positive Text Rewriting](https://arxiv.org/abs/2403.03029) | 使用"SocraticReframe"框架，通过引入苏格拉底式的理性化论证，增强了积极文本重写的数据集，显著提高了各种开源LLM的表现。 |
| [^17] | [Word Importance Explains How Prompts Affect Language Model Outputs](https://arxiv.org/abs/2403.03028) | 通过改变提示中的单词，本研究提出了一种方法来解释大型语言模型（LLMs）的工作原理，从而揭示其对模型输出的影响。 |
| [^18] | [The Case for Evaluating Multimodal Translation Models on Text Datasets](https://arxiv.org/abs/2403.03014) | 评估多模式翻译模型时，应考虑其利用视觉信息的能力和翻译复杂句子的表现，建议使用CoMMuTE评估框架。 |
| [^19] | [Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges](https://arxiv.org/abs/2403.02990) | 探讨了大型语言模型（LLMs）对数据增强的转变性影响，独特挑战和机遇，突出了LLMs在数据增强中引入的范式转变。 |
| [^20] | [A General and Flexible Multi-concept Parsing Framework for Multilingual Semantic Matching](https://arxiv.org/abs/2403.02975) | 提出一个通用灵活的多概念解析框架用于多语言语义匹配，以解决关键词和意图概念识别以及外部NER依赖的问题 |
| [^21] | [Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering](https://arxiv.org/abs/2403.02966) | 提出了一种面向证据的事实摘要化框架EFSum，用于增强LLMs的零-shot QA性能，并确保摘要的有益性和忠实性。 |
| [^22] | [SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents](https://arxiv.org/abs/2403.02959) | 提出了SimuCourt司法基准，包括真实世界的司法文件，并引入了司法决策任务和多代理框架，评估了代理的司法分析和决策能力 |
| [^23] | [Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation](https://arxiv.org/abs/2403.02951) | 大型语言模型在文本生成SQL任务中表现出色，但对于最佳提示模板和设计框架仍无共识，新数据集和评估任务有助于全面评估各种方法的表现，并提出了优化解决方案。 |
| [^24] | [PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers](https://arxiv.org/abs/2403.02939) | PaperWeaver通过将用户收集的论文与推荐论文上下文化，为研究人员提供了更丰富的主题论文提醒 |
| [^25] | [AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models](https://arxiv.org/abs/2403.02938) | 本研究探讨了人类是否可以听到经过优化的语音，并提出了一种系统，可根据音素为单位自动调整播放速度，以确保语音可辨识度。 |
| [^26] | [RulePrompt: Weakly Supervised Text Classification with Prompting PLMs and Self-Iterative Logical Rules](https://arxiv.org/abs/2403.02932) | 提出了使用逻辑表达来表征类别含义的规则Prompt方法，结合PLMs和自迭代逻辑规则实现弱监督文本分类 |
| [^27] | [A Second Look on BASS -- Boosting Abstractive Summarization with Unified Semantic Graphs -- A Replication Study](https://arxiv.org/abs/2403.02930) | 通过复制研究BASS框架，发现了与原始工作相比性能上的差异，并强调了撰写可复制论文的关键实践。 |
| [^28] | [Demonstrating Mutual Reinforcement Effect through Information Flow](https://arxiv.org/abs/2403.02902) | 通过信息流分析证实了文本分类任务中单词级别和文本级别分类之间的相互增强效应，同时将该效应扩展到提示学习。 |
| [^29] | [Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning](https://arxiv.org/abs/2403.02893) | 提出了一种使用异构图对比迁移学习的方法，实现了零样本跨语言文档级事件因果识别，并在实验证明在F1得分上优于之前的最先进模型。 |
| [^30] | [In Search of Truth: An Interrogation Approach to Hallucination Detection](https://arxiv.org/abs/2403.02889) | 提出了一种用于在大型语言模型中检测幻觉的新方法，解决了这些模型在各种现实场景中应用时遇到的关键问题，通过对多个数据集和LLMs进行广泛评估，展示了该方法的有效性。 |
| [^31] | [MathScale: Scaling Instruction Tuning for Mathematical Reasoning](https://arxiv.org/abs/2403.02884) | MathScale提出了一种简单可扩展的方法来创建高质量的数学推理数据，展现出在数学数据集大小方面的有效可扩展性。 |
| [^32] | [Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples](https://arxiv.org/abs/2403.02875) | 提出了一种通过硬负样本改进多模态对比学习中概念理解的方法，并引入了一个评估视觉-语言模型中颜色、对象和大小细粒度对齐的新数据集。 |
| [^33] | [An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers](https://arxiv.org/abs/2403.02839) | 精调评判模型在领域内测试上表现出色，但泛化能力和公平性不及GPT4。 |
| [^34] | [DPPA: Pruning Method for Large Language Model to Model Merging](https://arxiv.org/abs/2403.02799) | 提出了一种名为动态修剪分区增强（DPPA）的双阶段方法，用于解决合并复杂微调模型的挑战。 |
| [^35] | [Evaluating and Optimizing Educational Content with Large Language Model Judgments](https://arxiv.org/abs/2403.02795) | 使用语言模型作为教育专家来评估教育内容的影响，展示了LMs作为可靠评估者的潜力，并介绍了一种指导优化方法。 |
| [^36] | [In-Memory Learning: A Declarative Learning Framework for Large Language Models](https://arxiv.org/abs/2403.02757) | 提出了一个基于内存的声明式学习框架，通过总结过去经验，帮助代理从中提取见解并改进性能，实现自我改进。进行了系统实验证明了该框架的有效性。 |
| [^37] | [Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models](https://arxiv.org/abs/2403.02756) | 通过RolE Prompting Guided Multi-Domain Adaptation (REGA)策略，这篇论文提出了一种有效管理多领域大型语言模型适应的新方法，包括自我蒸馏、角色提示和角色集成这三个关键组件。 |
| [^38] | [CURATRON: Complete Robust Preference Data for Robust Alignment of Large Language Models](https://arxiv.org/abs/2403.02745) | 本论文提出了一种新方法，通过彻底重校准偏好数据集中的价值观，以增强大型语言模型对问题的韧性。 |
| [^39] | [Towards Training A Chinese Large Language Model for Anesthesiology](https://arxiv.org/abs/2403.02742) | Hypnos通过改善数据质量和采用由一般到特定的训练策略，为面向麻醉学的中文大型语言模型的建设做出了重要贡献。 |
| [^40] | [Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment](https://arxiv.org/abs/2403.02738) | 提出了一种新型因果引导方法，通过前门调整有效减轻大型语言模型的偏见。 |
| [^41] | [HARGPT: Are LLMs Zero-Shot Human Activity Recognizers?](https://arxiv.org/abs/2403.02727) | HARGPT研究表明LLMs可以通过适当提示理解原始IMU数据，实现零样本人类活动识别，并在性能上优于传统机器学习和最先进深度分类模型。 |
| [^42] | [DP-CRE: Continual Relation Extraction via Decoupled Contrastive Learning and Memory Structure Preservation](https://arxiv.org/abs/2403.02718) | DP-CRE框架通过分离对比学习和记忆结构保留的方式，显著优于其他持续关系抽取基线模型，在处理在持续学习过程中出现的灾难性遗忘挑战时表现出色。 |
| [^43] | [Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models](https://arxiv.org/abs/2403.02715) | 该研究通过对LLMs在越南语上进行微调和综合评估，发现微调后的模型在越南语理解和生成方面表现出良好能力，同时指出模型参数数量与性能之间存在着权衡关系，训练或微调数据集的质量是影响LLM性能的关键因素。 |
| [^44] | [Android in the Zoo: Chain-of-Action-Thought for GUI Agents](https://arxiv.org/abs/2403.02713) | 该研究提出了一个名为CoAT的Chain-of-Action-Thought模型，通过考虑先前动作描述、当前屏幕情况以及未来动作思考，显著提高了智能手机GUI代理的任务执行效果。 |
| [^45] | [Breeze-7B Technical Report](https://arxiv.org/abs/2403.02712) | Breeze-7B是基于Mistral-7B的开源语言模型，旨在改善中文语境下的语言理解和聊天机器人功能，展现出在复杂度类别中的出色性能。 |
| [^46] | [Causal Walk: Debiasing Multi-Hop Fact Verification with Front-Door Adjustment](https://arxiv.org/abs/2403.02698) | 提出了一种新方法Causal Walk，从因果角度利用前门调整消除多跳事实验证的偏见。 |
| [^47] | [Privacy-Aware Semantic Cache for Large Language Models](https://arxiv.org/abs/2403.02694) | MeanCache是一种面向LLMs的语义缓存，能够识别语义上相似的查询，从而减少查询成本，服务提供商负载和环境影响。 |
| [^48] | [InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents](https://arxiv.org/abs/2403.02691) | 本研究引入了InjecAgent基准测试，用于评估工具集成的大型语言模型代理对间接提示注入攻击的脆弱性，通过评估30种LLM代理，发现这些代理存在漏洞 |
| [^49] | [Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters](https://arxiv.org/abs/2403.02677) | 通过细调多模态语言模型，我们提出了一种新框架，能够更准确、更全面地过滤图像文本数据，从而提高数据质量和预训练模型的性能。 |
| [^50] | [Revisiting Meta-evaluation for Grammatical Error Correction](https://arxiv.org/abs/2403.02674) | 该论文提出了SEEDA，一个用于语法错误修正的新数据集，提供了对12种最先进系统进行元评估的校正，通过在句子级别元评估中对粒度进行对齐，提高了相关性。 |
| [^51] | [FinReport: Explainable Stock Earnings Forecasting via News Factor Analyzing Model](https://arxiv.org/abs/2403.02647) | FinReport是一个自动系统，通过金融新闻公告和多因素模型来生成专业的股票盈利预测报告，旨在帮助普通投资者收集信息、分析数据并生成报告。 |
| [^52] | [Exploring the Limitations of Large Language Models in Compositional Relation Reasoning](https://arxiv.org/abs/2403.02615) | 评估了大型语言模型在组合关系推理中的能力，设计了涵盖六种不同类型组合关系的基准测试，并扩展到多语言环境下进行评估。 |
| [^53] | [Improving Event Definition Following For Zero-Shot Event Detection](https://arxiv.org/abs/2403.02586) | 通过构建多样化的事件定义数据集，本研究展示出大量的事件类型和多样的事件定义对于改进零样本事件检测可以显著提升性能。 |
| [^54] | [ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary](https://arxiv.org/abs/2403.02574) | ChatCite是一个LLM代理，通过人工工作流引导进行比较文学综述，利用反思逐步机制生成摘要。 |
| [^55] | [Eliciting Better Multilingual Structured Reasoning from LLMs through Code](https://arxiv.org/abs/2403.02567) | LLMs在多语言推理任务上表现出较弱的性能，本文提出了通过代码训练和推理来改善多语言结构化推理能力的方法。 |
| [^56] | [Systemic Biases in Sign Language AI Research: A Deaf-Led Call to Reevaluate Research Agendas](https://arxiv.org/abs/2403.02563) | 手语人工智能研究中存在显著偏见，包括过于关注沟通障碍、缺乏代表性数据集、使用缺乏语言基础的注释和基于有缺陷模型的方法，缺乏聋人利益相关者的重要参与。 |
| [^57] | [Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research](https://arxiv.org/abs/2403.02558) | 生成模型的最新进展加速了医学中自然语言和图像处理领域的发展，并标志着生物医学模型开发和部署方式的重大范式转变。 |
| [^58] | [DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation](https://arxiv.org/abs/2403.02528) | 该论文通过自动生成高质量答案注释的方法，构建了DACO数据集，旨在激发未来对数据分析这一关键且具有挑战性任务的研究。 |
| [^59] | [Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF](https://arxiv.org/abs/2403.02513) | 采用直接实施无害的人类反馈强化学习（RLHF）的创新方法，不仅保留了基本模型的一般能力，还显著增强了其对话能力，同时明显减少了有毒输出的生成 |
| [^60] | [SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models](https://arxiv.org/abs/2403.02509) | SPUQ是一种新颖的基于扰动的大型语言模型不确定性量化方法，旨在同时处理现象性和认知性不确定性 |
| [^61] | [A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing](https://arxiv.org/abs/2403.02504) | 预训练-微调范式在自然语言处理中展现了显著的效率，尤其对社会科学研究中数据有限的情况下具有益处。 |
| [^62] | [Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents](https://arxiv.org/abs/2403.02502) | 提出了一种面向LLM代理的基于探索的轨迹优化方法，通过允许代理从探索失败中学习，实现了性能的改进。 |
| [^63] | [Choose Your Own Adventure: Interactive E-Books to Improve Word Knowledge and Comprehension Skills](https://arxiv.org/abs/2403.02496) | 该研究检验了交互式电子书对三至五年级学生阅读理解和词汇知识的影响，通过采用选择冒险格式和内嵌理解问题的方式来教授单词学习和理解监控策略。 |
| [^64] | [Enhancing LLM Safety via Constrained Direct Preference Optimization](https://arxiv.org/abs/2403.02475) | 通过引入Constrained DPO（C-DPO）方法，我们提出了一种高效且轻量的微调大型语言模型（LLMs）的方法，能有效平衡有用性和安全性之间的权衡，为LLMs提供了安全保障。 |
| [^65] | [The Emotion Dynamics of Literary Novels](https://arxiv.org/abs/2403.02474) | 该研究使用角色对话来区分文学小说叙述与各角色的情感曲线，发现叙述和对话在表达情感时存在明显差异，个别角色的情感曲线更准确地捕捉到故事情感曲线的共同点或差异。 |
| [^66] | [OffLanDat: A Community Based Implicit Offensive Language Dataset Generated by Large Language Model Through Prompt Engineering](https://arxiv.org/abs/2403.02472) | 介绍了一个通过提示工程生成的大型语言模型创建的社区基础隐式攻击性语言数据集OffLanDat，为38个不同目标群体提供数据。 |
| [^67] | [Views Are My Own, But Also Yours: Benchmarking Theory of Mind using Common Ground](https://arxiv.org/abs/2403.02451) | 介绍了第一个基于自然口语对话的ToM数据集，展示了LM在ToM方面的困难，并表明在Common-ToM上整合简单明确的信念表示可以提高LM的性能。 |
| [^68] | [How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models](https://arxiv.org/abs/2403.02436) | 本研究探讨了建筑如何影响预训练语言模型的基本能力，发现了FFN-Wider变压器模型降低了多头注意力的贡献比，从而导致基本能力的下降。 |
| [^69] | [Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems](https://arxiv.org/abs/2403.02419) | 本文研究了复合推理系统的扩展定律，发现投票推理系统的性能随LLM调用次数增加先增加后下降。 |
| [^70] | [NeuroVoz: a Castillian Spanish corpus of parkinsonian speech](https://arxiv.org/abs/2403.02371) | 这一研究提出了一个包含108位母语为卡斯蒂利亚语说话者的帕金森病患者语音语料库，涵盖了多种语音任务，通过手动和自动转录确保了数据的准确性和可靠性。 |
| [^71] | [adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with Integrated LLM Playgrounds](https://arxiv.org/abs/2403.02370) | 该论文介绍了adaptMLLM，一个旨在解决低资源语言机器翻译问题的开源应用程序，该应用程序简化了对多语言语言模型进行微调的所有流程。 |
| [^72] | [adaptNMT: an open-source, language-agnostic development environment for Neural Machine Translation](https://arxiv.org/abs/2403.02367) | adaptNMT是一个开源的神经机器翻译开发环境，简化了模型开发和部署流程，特别适用于新手用户，并提供图形展示、子词分割模型等功能。 |
| [^73] | [Human Evaluation of English--Irish Transformer-Based NMT](https://arxiv.org/abs/2403.02366) | 本研究评估了超参数设置对低资源英-爱变压器神经机器翻译质量的影响，并发现优化的Transformer模型在16k BPE子词模型下表现最佳，相较于基准RNN模型提高了7.8个BLEU分数，并在与谷歌翻译的比较中展示出显著的改进。 |
| [^74] | [Using LLMs for the Extraction and Normalization of Product Attribute Values](https://arxiv.org/abs/2403.02130) | 本文探讨了使用大型语言模型（LLMs）如GPT-3.5和GPT-4从产品标题和描述中提取和规范化属性值的潜力，引入了新的WDC PAVE数据集来支持实验。 |
| [^75] | [Fostering the Ecosystem of Open Neural Encoders for Portuguese with Albertina PT* Family](https://arxiv.org/abs/2403.01897) | 本文为葡萄牙语的神经编码作出了贡献，扩展了大型语言模型生态系统，并发布了包括亿级参数 Albertina 和 Bertimbau 在内的开源编码器模型，进一步推进了葡萄牙语的神经编码器技术。 |
| [^76] | [NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models](https://arxiv.org/abs/2403.01777) | 这项研究介绍了一个旨在评估多模态大型语言模型推理能力的动态基准NPHardEval4V，发现在推理能力方面不同模型存在显著差异，并揭示了相对于LLMs，MLLMs的推理性能较弱。 |
| [^77] | [Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models](https://arxiv.org/abs/2403.01616) | 该论文旨在推动越南语言理解和生成方面的进展，通过开发和分享开放数据集和预训练模型，特别是针对越南语检索增强生成和大型语言模型。 |
| [^78] | [In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation](https://arxiv.org/abs/2403.01548) | 本研究从内部表征角度深入探讨了大型语言模型幻觉的机制，发现了幻觉的一个显著模式，即在上下文标记的隐藏状态中，正确生成具有更清晰的上下文激活。我们提出了一种基于熵的度量方法，将“锐度”纳入解码过程中，制定了一种受限解码方法，实验证明其在知识寻求和幻觉任务上的有效性。 |
| [^79] | [Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey](https://arxiv.org/abs/2403.01528) | 生物分子与自然语言相结合的多模态学习为全面表示和分析生物分子开辟了新途径。 |
| [^80] | [KorMedMCQA: Multi-Choice Question Answering Benchmark for Korean Healthcare Professional Licensing Examinations](https://arxiv.org/abs/2403.01469) | KorMedMCQA是首个从韩国医疗专业执业考试中衍生的多项选择题问答基准，提供了多种大型语言模型的基线实验结果，并在HuggingFace上公开了数据，为韩国医疗环境中的进一步研究和发展提供了可能性。 |
| [^81] | [LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation](https://arxiv.org/abs/2403.01131) | LLaMoCo是第一个旨在以代码对代码方式调整LLMs以解决优化问题的指令调优框架，通过全面指令集和新颖的两阶段学习策略，实现了优越的性能。 |
| [^82] | [Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes](https://arxiv.org/abs/2403.00867) | 本文提出了一种名为Gradient Cuff的方法，通过探索拒绝损失地形图来检测对大语言模型的越狱攻击，成功设计了一种有效的两步检测策略。 |
| [^83] | [CLLMs: Consistency Large Language Models](https://arxiv.org/abs/2403.00835) | 提出了一种新方法，通过精细调整目标LLM实现了对雅各比轨迹上固定点的一致性预测，有效提高了生成速度2.4倍到3.4倍。 |
| [^84] | [DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models](https://arxiv.org/abs/2403.00818) | DenseSSM是一种新方法，通过密集连接增强了状态空间模型(SSM)，有效地提升了各层之间隐藏信息的流动，在保持训练并行性和推理效率的同时，取得了显著的性能提升。 |
| [^85] | [Metamorpheus: Interactive, Affective, and Creative Dream Narration Through Metaphorical Visual Storytelling](https://arxiv.org/abs/2403.00632) | 本研究提出了Metamorpheus，一种情感接口，通过创造性的视觉叙事来参与用户在梦境中的情感经历，利用生成式人工智能生成视觉隐喻和文本描绘，促使自我反思。 |
| [^86] | [Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process](https://arxiv.org/abs/2402.19350) | 该研究引入了一个促进显式和隐式知识的框架，用于多跳问题回答，从人类阅读过程的角度连接输入文段和预训练知识。 |
| [^87] | [WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset](https://arxiv.org/abs/2402.19282) | WanJuan-CC是一个安全高质量的开源英文网络文本数据集，通过处理大规模的Common Crawl数据并经过多项筛选和过滤步骤得到，为语言模型的预训练提供了重要资源。 |
| [^88] | [Improving Legal Judgement Prediction in Romanian with Long Text Encoders](https://arxiv.org/abs/2402.19170) | 本研究关注通过扩展Transformer模型的序列长度来更好理解法律语料库中的长文档，并在罗马尼亚的4个LJP数据集上进行了广泛实验。 |
| [^89] | [Advancing Generative AI for Portuguese with Open Decoder Gerv\'asio PT*](https://arxiv.org/abs/2402.18766) | 提出了一种葡萄牙语生成人工智能的开放解码器模型Gerv\'asio PT*，创造了新的技术水平，促进葡萄牙语言技术研究和创新。 |
| [^90] | [Prospect Personalized Recommendation on Large Language Model-based Agent Platform](https://arxiv.org/abs/2402.18240) | 提出了一种基于大型语言模型代理平台的个性化推荐系统Rec4Agentverse，强调代理项和代理推荐器之间的合作，促进个性化信息服务，提升信息交换，并展望了其演进为支持互动和信息交换的三个阶段 |
| [^91] | [PALO: A Polyglot Large Multimodal Model for 5B People](https://arxiv.org/abs/2402.14818) | 该文介绍了一个名为PALO的大型多语种多模态模型，实现了对10种主要语言的视觉推理能力，涵盖了约50亿人口。其通过半自动化翻译方法，将多语言多模态数据集从英语翻译为目标语言，以提升跨多种语言的性能。 |
| [^92] | [Overview of the VLSP 2023 -- ComOM Shared Task: A Data Challenge for Comparative Opinion Mining from Vietnamese Product Reviews](https://arxiv.org/abs/2402.13613) | 该论文总结了VLSP 2023中ComOM任务的一个数据挑战，旨在推动自然语言处理领域通过开发从越南产品评论中提取比较意见的技术，参与者需提出能够提取比较"五元组"的模型并根据F1分数进行评估排名。 |
| [^93] | [Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning](https://arxiv.org/abs/2402.12177) | Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。 |
| [^94] | [ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment](https://arxiv.org/abs/2402.11000) | 提出了一个新的实体对齐框架ASGEA，利用Align-Subgraphs中的逻辑规则，设计了可解释的基于路径的图神经网络ASGNN，引入了多模态注意机制，取得了令人满意的实验结果 |
| [^95] | [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353) | DoRA是一种将预训练权重分解为幅度和方向两个组成部分，并使用LoRA进行方向更新的低秩适应方法，通过增强学习能力和训练稳定性来提高对LLaMA，LLaVA和VL-B的微调性能。 |
| [^96] | [Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like](https://arxiv.org/abs/2402.07383) | 本文提出了ELaTE，一种基于流匹配的零样本文本到语音系统，可以根据短音频提示以精确控制笑声时机和表情生成任何说话者的自然笑声。 |
| [^97] | [From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models](https://arxiv.org/abs/2402.00421) | 本研究介绍了专利响应智能系统PARIS和LE-PARIS，通过构建OA主题数据库、开发响应模板以及实施推荐系统和基于LLM的响应生成，旨在加快专利律师处理审查意见回应的效率。 通过多范式分析和长期数据验证，证明了OA主题的建设性和LLM对于回应自动生成的可行性。 |
| [^98] | [LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin](https://arxiv.org/abs/2312.09979) | LoRAMoE是一个新颖的框架，通过引入低秩适配器和路由器网络，类似于MoE的插件版本，来解决大型语言模型中世界知识遗忘的问题。 |
| [^99] | [RDR: the Recap, Deliberate, and Respond Method for Enhanced Language Understanding](https://arxiv.org/abs/2312.09932) | RDR方法提出了通过回顾、审慎和回应三个目标来增强语言理解的神经网络管道，解决了神经模型操纵NLU基准测试的问题 |
| [^100] | [Generative AI in Higher Education: Seeing ChatGPT Through Universities' Policies, Resources, and Guidelines](https://arxiv.org/abs/2312.05235) | 通过分析美国前100名大学制定的学术政策和指南，揭示了大多数大学对于在高等教育中整合生成人工智能的开放但谨慎态度，主要关注点在于伦理使用、准确性和数据隐私。 |
| [^101] | [Sequence-Level Certainty Reduces Hallucination In Knowledge-Grounded Dialogue Generation](https://arxiv.org/abs/2310.18794) | 序列级确定性是减少基于知识的对话生成中幻觉的关键，这项工作提出了基于概率确定性和语义确定性的序列级确定性，结果表明更高水平的确定性对应更低水平的幻觉，进一步提出了基于确定性的响应排序方法 |
| [^102] | [Evaluating Spatial Understanding of Large Language Models](https://arxiv.org/abs/2310.14540) | 本研究评估了大型语言模型对空间结构的理解能力，发现LLMs在表示和推理空间结构时的表现存在显著差异，具有捕捉空间结构隐含特征的潜力。 |
| [^103] | [From Artificially Real to Real: Leveraging Pseudo Data from Large Language Models for Low-Resource Molecule Discovery](https://arxiv.org/abs/2309.05203) | 通过利用大型语言模型生成的伪数据，本研究解决了低资源挑战，实验表明利用伪数据进行领域自适应优于现有方法。 |
| [^104] | [Mitigating Temporal Misalignment by Discarding Outdated Facts](https://arxiv.org/abs/2305.14824) | 提出了一种通过预测事实的持续时间来减轻时间不一致性问题的方法，从而避免语言模型重复提供过时信息，并帮助模型提高知识密集任务的校准性。 |
| [^105] | [Uncovering the Potential of ChatGPT for Discourse Analysis in Dialogue: An Empirical Study](https://arxiv.org/abs/2305.08391) | 本研究系统检查了ChatGPT在对话中的话语分析潜力，重点关注其对线性和分层话语结构的深层语义理解，实验结果显示ChatGPT在识别主题结构方面表现出熟练度但在话语解析方面遇到困难。 |
| [^106] | [Contextual Text Denoising with Masked Language Models](https://arxiv.org/abs/1910.14080) | 提出了一种基于遮蔽语言模型的上下文文本去噪算法，可以在不重新训练模型的情况下纠正噪声文本，并在多个下游任务中提高性能 |
| [^107] | [Query of CC: Unearthing Large Scale Domain-Specific Knowledge from Public Corpora.](http://arxiv.org/abs/2401.14624) | 本论文提出了一种通过大型语言模型来收集特定领域知识的高效方法，通过该方法构建了一个高质量的名为“Knowledge Pile”的数据集，实验证明其显著改善了特定领域的数据稀缺问题。 |
| [^108] | [Context Matters: Pushing the Boundaries of Open-Ended Answer Generation with Graph-Structured Knowledge Context.](http://arxiv.org/abs/2401.12671) | 本论文介绍了一种结合图驱动的上下文检索和知识图结构增强的框架，通过提高LLMs的能力，尤其是在特定领域的社区问答平台上，更好地回答开放式问题。 |
| [^109] | [Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models.](http://arxiv.org/abs/2401.10647) | 本文研究了通过编辑语言模型的复杂后果，发现在增强模型准确性与保持道德完整性之间存在悖论。我们发现，尽管注入准确信息对模型的可靠性很重要，但它可能破坏模型的基本框架，导致不可预测和潜在的不安全行为。 |
| [^110] | [AntEval: Quantitatively Evaluating Informativeness and Expressiveness of Agent Social Interactions.](http://arxiv.org/abs/2401.06509) | 本研究通过使用桌面角色扮演游戏规则创建了一个环境，量化评估智能体社交互动的信息性和表达性，旨在克服隐私问题并促使智能体进行有意义、高质量的互动。 |
| [^111] | [DevEval: Evaluating Code Generation in Practical Software Projects.](http://arxiv.org/abs/2401.06401) | 本文提出了一个名为DevEval的新基准测试，用于评估实际软件项目中的代码生成。与之前的基准测试相比，DevEval在真实的项目分布、充足的依赖和足够规模的项目背景等方面更贴合实际。通过对五个流行的大型语言模型进行评估，我们揭示了它们在代码生成中的实际能力。 |
| [^112] | [LEGO:Language Enhanced Multi-modal Grounding Model.](http://arxiv.org/abs/2401.06071) | LEGO是一种语言增强的多模态关联模型，它能够在各种任务中实现细粒度的理解和精确的标识能力。 |
| [^113] | [Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning.](http://arxiv.org/abs/2310.00648) | 本文研究了参数高效微调（PEFT）的安全性问题，发现PEFT易受特洛伊攻击。通过提出一种新的攻击方式PETA，并在各种下游任务和触发器设计中进行广泛测试，发现PETA能够在攻击成功率和未受影响的准确性方面取得有效结果。 |
| [^114] | [SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning.](http://arxiv.org/abs/2309.04766) | SeaEval是一个评估多语言基础模型的基准测试，研究了模型在自然语言理解、推理以及对文化实践、细微差别和价值观的理解能力上的表现。重要发现包括模型在给出改写指令时行为各异，受到暴露偏差的影响，对于语义等价的多语言查询的回答不一致，以及模型在情感相关问题上的一致性不同。 |
| [^115] | [Large Language Models to Identify Social Determinants of Health in Electronic Health Records.](http://arxiv.org/abs/2308.06354) | 本研究利用大型语言模型从电子健康记录中提取社会健康决定因素（SDoH），并通过合成临床文本改进了这些极有价值但很少被记录的临床数据的提取。最佳模型为经过微调的Flan-T5 XL和Flan-T5 XXL，其中小型模型改进了性能。 |
| [^116] | [Exploring the psychology of GPT-4's Moral and Legal Reasoning.](http://arxiv.org/abs/2308.01264) | 本文探究了GPT-4的道德和法律推理，发现其与人类之间在意图归因、因果判断、欺骗的道德性、道德基础、道德运气对法律判断的影响、同意的概念以及规则违反判断方面存在高相关性。 |
| [^117] | [DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding.](http://arxiv.org/abs/2307.06924) | DRAGON是一种基于对话的导航机器人，能够理解用户的指令并通过语言与用户沟通，为视力受损者提供导航和环境描述的帮助。 |
| [^118] | [Towards Personalized Cold-Start Recommendation with Prompts.](http://arxiv.org/abs/2306.17256) | 本研究旨在解决个性化冷启动推荐问题，通过利用预训练语言模型的能力，将推荐过程转化为自然语言情感分析，提供适用于创业企业和用户参与历史不足的平台的个性化推荐。 |
| [^119] | [Mass-Producing Failures of Multimodal Systems with Language Models.](http://arxiv.org/abs/2306.12105) | 本文介绍了一种MultiMon系统，可以自动识别多模态系统中的系统性失败，揭示CLIP文本编码器的14个系统性失败，每个都由数百个不同的输入组成，这些输入会导致其他大多数最先进的多模态系统的失败。 |
| [^120] | [Soft-prompt Tuning for Large Language Models to Evaluate Bias.](http://arxiv.org/abs/2306.04735) | 本文使用软提示调整来量化大型语言模型中的偏差，避免手动设计提示导致的人为偏差注入。通过分组公平性检查模型对不同敏感属性的偏见，发现了有趣的偏差模式。 |
| [^121] | [Multiscale Positive-Unlabeled Detection of AI-Generated Texts.](http://arxiv.org/abs/2305.18149) | 本文提出了一种多尺度正负样本的训练框架，以解决多尺度AI生成文本的检测问题。通过将短机器文本标记为“未标记”来重新表述文本分类问题，并提出了一个规则化损失函数来优化检测性能，有效性能显著优于现有的方法。 |
| [^122] | [Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training.](http://arxiv.org/abs/2305.14342) | Sophia是一种用于语言模型预训练的可扩展的二阶优化算法，使用对角Hessian作为预调节器，并进行元素级别的裁剪控制更新大小。 |
| [^123] | [Fine-tuning Language Models with Generative Adversarial Feedback.](http://arxiv.org/abs/2305.06176) | 本研究探讨了一种新的方法，使用生成对抗反馈的强化学习(RLGAF)对大型语言模型进行微调，以取代仅受人类反馈的强化学习(RLHF)，从而消除评估者的专业限制并提高性能。 |
| [^124] | [Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations.](http://arxiv.org/abs/2303.16618) | 本篇论文研究了如何使用丰富的元数据注释的信息进行屏幕角色的个性化语言建模，测试表明这样可以有效地进行个性化语言模型的构建，即使对于零样本的演说家也可以应用。 |

# 详细

[^1]: WMDP基准：通过遗忘测量和减少恶意使用

    The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning

    [https://arxiv.org/abs/2403.03218](https://arxiv.org/abs/2403.03218)

    WMDP基准是一个公开发布的数据集，包含4157个多项选择问题，用作生物安全、网络安全和化学安全危险知识的代理测量。

    

    arXiv:2403.03218v1 公告类型：交叉领域 摘要：白宫关于人工智能的行政命令强调了大型语言模型(LLMs)赋予恶意行为者开发生物、网络和化学武器的风险。为了衡量这些恶意使用的风险，政府机构和主要人工智能实验室正在开发LLMs的危险能力评估。然而，当前的评估是私人的，阻碍了进一步研究如何减少风险。此外，它们仅专注于几条高度特定的恶意使用途径。为了填补这些空白，我们公开发布了大规模杀伤性武器代理（WMDP）基准，这是一个包含4157个多项选择问题的数据集，作为生物安全、网络安全和化学安全危险知识的代理测量。WMDP由一组学术界和技术顾问联合开发，并在公开发布前严格过滤以消除敏感信息。WMDP有两个服务

    arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
    
[^2]: MAGID：用于生成合成多模态数据集的自动化流水线

    MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets

    [https://arxiv.org/abs/2403.03194](https://arxiv.org/abs/2403.03194)

    MAGID是一个用于将仅文本对话增强为多样性和高质量图像的框架，通过创新的反馈循环生成高质量和多模态对话。

    

    多模态交互系统的发展受限于缺乏丰富的多模态（文本、图像）对话数据，这些数据对LLMs而言需要大量。先前的方法通过检索图像来增强文本对话，存在隐私、多样性和质量等约束。在这项工作中，我们介绍了MAGID（\textbf{M}ultimodal \textbf{A}ugmented \textbf{G}enerative \textbf{I}mages \textbf{D}ialogues）, 一个框架，用于用各种多样性和高质量图像增强仅限于文本的对话。随后，应用扩散模型来创建相应的图像，确保与确定的文本保持一致。最后，MAGID包含了一个创新性的反馈循环，介于图像描述生成模块（文本LLM）和图像质量模块（解决美学、图像文本匹配和安全性），二者协作生成高质量和多模态对话。我们将MAGID与其他SOTA基线在三个对话方面进行了比较。

    arXiv:2403.03194v1 Announce Type: new  Abstract: Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs. Previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints. In this work, we introduce \textbf{M}ultimodal \textbf{A}ugmented \textbf{G}enerative \textbf{I}mages \textbf{D}ialogues (MAGID), a framework to augment text-only dialogues with diverse and high-quality images. Subsequently, a diffusion model is applied to craft corresponding images, ensuring alignment with the identified text. Finally, MAGID incorporates an innovative feedback loop between an image description generation module (textual LLM) and image quality modules (addressing aesthetics, image-text matching, and safety), that work in tandem to generate high-quality and multi-modal dialogues. We compare MAGID to other SOTA baselines on three dialo
    
[^3]: 具有检索功能的可靠、适应性强且可追溯的语言模型

    Reliable, Adaptable, and Attributable Language Models with Retrieval

    [https://arxiv.org/abs/2403.03187](https://arxiv.org/abs/2403.03187)

    持续面临挑战的参数化语言模型LMs，作者主张使用具有检索功能的LMs作为下一代LMs，以提高可靠性、适应性和可追溯性。

    

    参数化语言模型（LMs）在海量网络数据上训练，表现出卓越的灵活性和能力。然而，它们仍然面临着幻觉、难以适应新数据分布和缺乏可验证性等实际挑战。在这篇立场论文中，我们主张用具备检索功能的LMs取代参数化LMs作为下一代LMs。通过在推理过程中整合大规模数据存储，具有检索功能的LMs可以更加可靠、适应性更强、可追溯。

    arXiv:2403.03187v1 Announce Type: cross  Abstract: Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose
    
[^4]: SNIFFER: 可解释的跨文本信息误传检测的多模大语言模型

    SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection

    [https://arxiv.org/abs/2403.03170](https://arxiv.org/abs/2403.03170)

    SNIFFER是一种专门为超出上下文误传检测和解释而设计的新型多模大语言模型，通过两阶段指导微调，在解释生成方面取得了突破。

    

    虚假信息是一个普遍的社会问题，由于潜在的高风险。超出上下文（OOC）的误传，即真实图像被伪造的文本再利用，是误导观众的最简单和最有效的方法之一。当前的方法侧重于评估图像-文本一致性，但缺乏说服力的解释来支持他们的判断，这对于揭示误传至关重要。虽然多模大语言模型（MLLMs）具有丰富的知识和内在的视觉推理和解释生成能力，但它们仍然缺乏理解和发现微妙的跨模态差异的复杂性。在本文中，我们介绍了SNIFFER，这是一种专门为OOC误传检测和解释而设计的新颖的多模大语言模型。SNIFFER在InstructBLIP上采用了两阶段的指导微调。第一阶段通过新闻领域实体与通用对象的模型概念对齐，

    arXiv:2403.03170v1 Announce Type: cross  Abstract: Misinformation is a prevalent societal issue due to its potential high risks. Out-of-context (OOC) misinformation, where authentic images are repurposed with false text, is one of the easiest and most effective ways to mislead audiences. Current methods focus on assessing image-text consistency but lack convincing explanations for their judgments, which is essential for debunking misinformation. While Multimodal Large Language Models (MLLMs) have rich knowledge and innate capability for visual reasoning and explanation generation, they still lack sophistication in understanding and discovering the subtle crossmodal differences. In this paper, we introduce SNIFFER, a novel multimodal large language model specifically engineered for OOC misinformation detection and explanation. SNIFFER employs two-stage instruction tuning on InstructBLIP. The first stage refines the model's concept alignment of generic objects with news-domain entities a
    
[^5]: PARADISE：通过过程警告和提示数据集评估语言模型的隐式规划能力

    PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset

    [https://arxiv.org/abs/2403.03167](https://arxiv.org/abs/2403.03167)

    论文提出了一个名为PARADISE的任务，通过实用程序文本进行演绎推理，评估语言模型仅从给定目标推断计划的能力

    

    最近，社区对于大型语言模型是否具备规划或执行计划的能力越发感兴趣。然而，大多数先前研究使用LLMs为简化场景生成高级计划，适应度量缺乏语言复杂性和领域多样性，限制其规划能力的分析。为了解决这一问题，我们提出了PARADISE，这是一个使用Q＆A格式的演绎推理任务，采用来自wikiHow的实用程序文本。它涉及与目标直接相关的警告和提示推断任务，排除中间步骤，旨在测试模型仅从给定目标推断计划的隐含知识的能力。

    arXiv:2403.03167v1 Announce Type: new  Abstract: Recently, there has been growing interest within the community regarding whether large language models are capable of planning or executing plans. However, most prior studies use LLMs to generate high-level plans for simplified scenarios lacking linguistic complexity and domain diversity, limiting analysis of their planning abilities. These setups constrain evaluation methods (e.g., predefined action space), architectural choices (e.g., only generative models), and overlook the linguistic nuances essential for realistic analysis. To tackle this, we present PARADISE, an abductive reasoning task using Q\&A format on practical procedural text sourced from wikiHow. It involves warning and tip inference tasks directly associated with goals, excluding intermediary steps, with the aim of testing the ability of the models to infer implicit knowledge of the plan solely from the given goal. Our experiments, utilizing fine-tuned language models and
    
[^6]: Design2Code：我们离自动化前端工程有多远？

    Design2Code: How Far Are We From Automating Front-End Engineering?

    [https://arxiv.org/abs/2403.03163](https://arxiv.org/abs/2403.03163)

    生成式人工智能在多模态理解和代码生成方面取得了突破，提出了Design2Code任务并进行了全面基准测试，展示了多模态LLMs直接将视觉设计转换为代码实现的能力。

    

    近年来，生成式人工智能在多模态理解和代码生成方面取得了突飞猛进的进展，实现了前所未有的能力。这可以实现一种新的前端开发范式，其中多模态LLMs可能直接将视觉设计转换为代码实现。本文将这一过程形式化为Design2Code任务，并进行全面基准测试。我们手动策划了一个包含484个多样化真实网页的基准测试用例，并开发了一套自动评估指标，以评估当前多模态LLMs能否生成直接渲染为给定参考网页的代码实现，以输入为屏幕截图。我们还结合了全面的人工评估。我们开发了一套多模态提示方法，并展示了它们在GPT-4V和Gemini Pro Vision上的有效性。我们进一步对一个开源的Design2Code-18B模型进行了微调。

    arXiv:2403.03163v1 Announce Type: new  Abstract: Generative AI has made rapid advancements in recent years, achieving unprecedented capabilities in multimodal understanding and code generation. This can enable a new paradigm of front-end development, in which multimodal LLMs might directly convert visual designs into code implementations. In this work, we formalize this as a Design2Code task and conduct comprehensive benchmarking. Specifically, we manually curate a benchmark of 484 diverse real-world webpages as test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input. We also complement automatic metrics with comprehensive human evaluations. We develop a suite of multimodal prompting methods and show their effectiveness on GPT-4V and Gemini Pro Vision. We further finetune an open-source Design2Code-18B model that su
    
[^7]: 文本环境中RL代理的语言引导探索

    Language Guided Exploration for RL Agents in Text Environments

    [https://arxiv.org/abs/2403.03141](https://arxiv.org/abs/2403.03141)

    引入了语言引导探索（LGE）框架，利用大型语言模型（LLM）为RL代理提供决策级别的引导，相比于普通RL代理和其他复杂方法，在具有挑战性的文本环境中取得了显著的优势。

    

    实际世界中的序贯决策以稀疏奖励和庞大的决策空间为特征，这对tabula rasa强化学习（RL）代理等经验学习系统提出了重要困难。具有丰富世界知识的大型语言模型（LLMs）可以帮助RL代理快速学习并适应分布变化。在这项工作中，我们引入了语言指导探索（LGE）框架，该框架使用一个预先训练的语言模型（称为GUIDE）为RL代理（称为EXPLORER）提供决策级别的指导。我们观察到，在ScienceWorld（Wang等人，2022年）这一具有挑战性的文本环境中，LGE显著优于普通RL代理，并且优于行为克隆和文本决策Transformer等其他复杂方法。

    arXiv:2403.03141v1 Announce Type: new  Abstract: Real-world sequential decision making is characterized by sparse rewards and large decision spaces, posing significant difficulty for experiential learning systems like $\textit{tabula rasa}$ reinforcement learning (RL) agents. Large Language Models (LLMs), with a wealth of world knowledge, can help RL agents learn quickly and adapt to distribution shifts. In this work, we introduce Language Guided Exploration (LGE) framework, which uses a pre-trained language model (called GUIDE ) to provide decision-level guidance to an RL agent (called EXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challenging text environment, LGE outperforms vanilla RL agents significantly and also outperforms other sophisticated methods like Behaviour Cloning and Text Decision Transformer.
    
[^8]: CoGenesis：一个协作大型和小型语言模型的框架，用于安全的上下文感知指令跟随

    CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following

    [https://arxiv.org/abs/2403.03129](https://arxiv.org/abs/2403.03129)

    提出了一个协作生成框架CoGenesis，整合大型和小型模型，以逻辑方式解决隐私问题。

    

    随着语言模型（LMs）的发展，它们接触私人数据的可能性越来越不可避免，它们的部署（尤其是较小的模型）在个人设备上，如PC和智能手机上，已成为一种盛行趋势。在充满用户信息的环境中，使模型既能保护用户隐私又能高效执行命令成为一项重要的研究课题。本文提出了CoGenesis，一个协作生成框架，集成了大型模型（托管在云基础设施上）和小型模型（部署在本地设备上），以逻辑方式解决隐私问题。最初，我们设计了一个管道来创建个性化的写作指导数据集，其中包含了丰富的上下文细节，作为这一研究问题的测试基础。随后，我们介绍了基于草图和对数的两个CoGenesis变体。我们的实验结果基于我们合成的数据集和另外两个op

    arXiv:2403.03129v1 Announce Type: new  Abstract: With the advancement of language models (LMs), their exposure to private data is increasingly inevitable, and their deployment (especially for smaller ones) on personal devices, such as PCs and smartphones, has become a prevailing trend. In contexts laden with user information, enabling models to both safeguard user privacy and execute commands efficiently emerges as an essential research imperative. In this paper, we propose CoGenesis, a collaborative generation framework integrating large (hosted on cloud infrastructure) and small models (deployed on local devices) to address privacy concerns logically. Initially, we design a pipeline to create personalized writing instruction datasets enriched with extensive context details as the testbed of this research issue. Subsequently, we introduce two variants of CoGenesis based on sketch and logits respectively. Our experimental findings, based on our synthesized dataset and two additional op
    
[^9]: 愤怒的男性，悲伤的女性：大型语言模型在情绪归因中反映性别刻板印象

    Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution

    [https://arxiv.org/abs/2403.03121](https://arxiv.org/abs/2403.03121)

    大型语言模型中存在性别化情绪归因，反映了社会刻板印象。

    

    大型语言模型（LLMs）反映社会规范和偏见，尤其是关于性别的。虽然社会偏见和刻板印象在各种自然语言处理应用中得到了广泛研究，但在情绪分析方面存在一个令人惊讶的空白。然而，在社会话语中，情绪和性别密切相关。例如，女性经常被认为更具移情能力，而男性的愤怒更受社会接受。为了填补这一空白，我们提出了对五种最先进的LLMs（开源和封闭源）进行性别化情绪归因的首次全面研究。我们调查情绪是否具有性别特征，以及这些变化是否基于社会刻板印象。我们提示模型采用性别化角色并将情绪归因于类似“当我与亲近的人发生严重争执”这样的事件。然后我们分析模型生成的情绪与性别-事件对之间的关系。我们发现所有模型一致展现出性别化。

    arXiv:2403.03121v1 Announce Type: new  Abstract: Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men's anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like 'When I had a serious argument with a dear person'. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered e
    
[^10]: “在对话中学习”：通过对话中学习实现无需预定义个人资料的个性化对话

    "In Dialogues We Learn": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning

    [https://arxiv.org/abs/2403.03102](https://arxiv.org/abs/2403.03102)

    提出了一种In-Dialogue Learning框架，通过对话历史刻画个人设来完成个性化对话生成任务，无需预定义个人资料，并在实验证明其显著改进对话生成性能。

    

    个性化对话系统近年来备受关注，因其能够生成与不同人设一致的响应。然而，大多数现有方法依赖预定义的个人资料，这不仅耗时且劳动密集，还缺乏灵活性。我们提出了In-Dialogue Learning（IDL），一种微调框架，增强了预训练的大型语言模型利用对话历史来刻画个人设，以完成个性化对话生成任务，而无需预定义个人资料。我们在三个数据集上的实验表明，IDL带来了显著的改进，BLEU和ROUGE分数分别增加了高达200%和247%。此外，人工评估的结果进一步验证了我们提出方法的有效性。

    arXiv:2403.03102v1 Announce Type: cross  Abstract: Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for completing personalized dialogue generation tasks without pre-defined profiles. Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally, the results of human evaluations further validate the efficacy of our proposed method.
    
[^11]: KnowAgent: 知识增强规划用于基于LLM的Agent

    KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents

    [https://arxiv.org/abs/2403.03101](https://arxiv.org/abs/2403.03101)

    KnowAgent引入了显式动作知识，通过动作知识库和知识型自学习策略来增强LLM的规划能力，从而改善语言Agent的规划表现。

    

    大型语言模型(LLMs)在复杂推理任务中表现出巨大潜力，但在处理更复杂的挑战时仍有所不足，特别是与环境互动通过生成可执行动作时。这种不足主要来自于语言Agent中缺乏内置动作知识，导致在任务求解过程中无法有效引导规划轨迹，从而导致规划幻觉。为了解决这个问题，我们引入了KnowAgent，一种旨在通过整合显式动作知识来增强LLM规划能力的新方法。具体而言，KnowAgent采用了一个动作知识库和一个知识型自学习策略来限制规划过程中的行动路径，实现更合理的轨迹合成，进而提高语言Agent的计划性能。基于HotpotQA和ALFWorld的实验结果基于不同的主干模型。

    arXiv:2403.03101v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone m
    
[^12]: NaturalSpeech 3: 利用分解编解码器和扩散模型实现零-shot语音合成

    NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models

    [https://arxiv.org/abs/2403.03100](https://arxiv.org/abs/2403.03100)

    NaturalSpeech 3利用分解设计的扩散模型实现零-shot方式生成自然语音

    

    近期大规模文本到语音（TTS）模型取得了显著进展，然而在语音质量、相似度和韵律方面仍存在不足。鉴于语音复杂地包含各种属性（例如内容、韵律、音色和声学细节），给生成带来了重大挑战，一个自然的想法是将语音因子分解为代表不同属性的各个子空间，并单独生成它们。在此基础上，我们提出了NaturalSpeech 3，这是一个具有新颖的分解扩散模型的TTS系统，可以以零-shot方式生成自然语音。具体来说，1) 我们设计了一个具有分解向量量化（FVQ）的神经编解码器，将语音波形分解为内容、韵律、音色和声学细节的子空间；2) 我们提出了一个分解扩散模型，根据其相应的提示生成每个子空间中的属性。借助这种分解设计，NaturalSpeech 3能够ef

    arXiv:2403.03100v1 Announce Type: cross  Abstract: While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can ef
    
[^13]: 检测具体视觉令牌以进行多模态机器翻译

    Detecting Concrete Visual Tokens for Multimodal Machine Translation

    [https://arxiv.org/abs/2403.03075](https://arxiv.org/abs/2403.03075)

    提出新方法用于检测和选择在多模态机器翻译中具体的视觉相关令牌，通过这些方法，在翻译任务中实现了性能的改进和更好的视觉上下文利用

    

    在多模态机器翻译（MMT）系统中，视觉基础和遮蔽的挑战促使了对检测和选择用于遮蔽的视觉基础文本令牌的不同方法。我们介绍了用于从源句中检测视觉上和语境上相关（具体）令牌的新方法，包括自然语言处理（NLP）检测，物体检测检测和联合检测-验证技术。我们还介绍了用于选择检测到的令牌的新方法，包括最短$n$ 个令牌、最长$n$ 个令牌以及所有检测到的具体令牌。我们利用GRAM MMT 架构针对合成整理的多模态数据集即源图像与遮蔽句子进行模型训练，表现出相比基准模型的性能改进和在翻译任务中对视觉上下文的更好利用。

    arXiv:2403.03075v1 Announce Type: new  Abstract: The challenge of visual grounding and masking in multimodal machine translation (MMT) systems has encouraged varying approaches to the detection and selection of visually-grounded text tokens for masking. We introduce new methods for detection of visually and contextually relevant (concrete) tokens from source sentences, including detection with natural language processing (NLP), detection with object detection, and a joint detection-verification technique. We also introduce new methods for selection of detected tokens, including shortest $n$ tokens, longest $n$ tokens, and all detected concrete tokens. We utilize the GRAM MMT architecture to train models against synthetically collated multimodal datasets of source images with masked sentences, showing performance improvements and improved usage of visual context during translation tasks over the baseline model.
    
[^14]: 将多模态功能添加到仅文本翻译模型中

    Adding Multimodal Capabilities to a Text-only Translation Model

    [https://arxiv.org/abs/2403.03045](https://arxiv.org/abs/2403.03045)

    将性能优异的文本机器翻译模型转化为多模态机器翻译模型，通过连接视觉-文本适配器层并利用门控机制，在Multi30k数据集和典型的仅文本数据集上取得良好效果。

    

    大多数当前的多模态机器翻译（MMT）工作在训练和评估时使用Multi30k数据集，然而我们发现导致的模型严重过拟合Multi30k数据集。因此，这些模型在典型的仅文本测试集（如WMT新闻测试数据集）上表现非常糟糕。为了在Multi30k和典型的仅文本数据集上表现良好，我们使用一个性能优异的文本机器翻译（MT）模型作为我们MMT模型的起点。我们通过连接门控机制的视觉-文本适配器层将MT模型逐步转化为MMT模型，方法是：1）使用基于视觉的源文本掩蔽进行预训练，2）在Multi30k上进行微调。

    arXiv:2403.03045v1 Announce Type: new  Abstract: While most current work in multimodal machine translation (MMT) uses the Multi30k dataset for training and evaluation, we find that the resulting models overfit to the Multi30k dataset to an extreme degree. Consequently, these models perform very badly when evaluated against typical text-only testing sets such as the WMT newstest datasets. In order to perform well on both Multi30k and typical text-only datasets, we use a performant text-only machine translation (MT) model as the starting point of our MMT model. We add vision-text adapter layers connected via gating mechanisms to the MT model, and incrementally transform the MT model into an MMT model by 1) pre-training using vision-based masking of the source text and 2) fine-tuning on Multi30k.
    
[^15]: 通过合作和互动代理学习使用工具

    Learning to Use Tools via Cooperative and Interactive Agents

    [https://arxiv.org/abs/2403.03031](https://arxiv.org/abs/2403.03031)

    提出了一种ConAgents框架，通过合作和互动代理使大型语言模型能够学习使用工具，并引入了迭代校准方法，以解决单个代理执行多样化操作能力有限和自适应纠正错误困难的问题。

    

    工具学习使大型语言模型（LLMs）作为代理人能够使用外部工具来扩展其功能。现有方法利用单个基于LLM的代理循环选择和执行工具，然后将结果合并到下一个动作预测中。然而，它们在处理复杂任务时仍然存在潜在的性能下降问题，原因是：（1）单个LLM的固有能力执行多样化操作受限，以及（2）在任务失败时难以自适应地纠正错误。为了缓解这些问题，我们提出了ConAgents，即合作和互动代理框架，将工具学习的工作流模块化为Grounding（基础）、Execution（执行）和Observing（观察）代理。我们还介绍了一种迭代校准（IterCali）方法，使代理能够根据来自工具环境的反馈对自己进行调整。在三个数据集上进行的实验证明了超过

    arXiv:2403.03031v1 Announce Type: new  Abstract: Tool learning empowers large language models (LLMs) as agents to use external tools to extend their capability. Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating the result into the next action prediction. However, they still suffer from potential performance degradation when addressing complex tasks due to: (1) the limitation of the inherent capability of a single LLM to perform diverse actions, and (2) the struggle to adaptively correct mistakes when the task fails. To mitigate these problems, we propose the ConAgents, a Cooperative and interactive Agents framework, which modularizes the workflow of tool learning into Grounding, Execution, and Observing agents. We also introduce an iterative calibration (IterCali) method, enabling the agents to adapt themselves based on the feedback from the tool environment. Experiments conducted on three datasets demonstrate the super
    
[^16]: 苏格拉底推理改善积极文本重写

    Socratic Reasoning Improves Positive Text Rewriting

    [https://arxiv.org/abs/2403.03029](https://arxiv.org/abs/2403.03029)

    使用"SocraticReframe"框架，通过引入苏格拉底式的理性化论证，增强了积极文本重写的数据集，显著提高了各种开源LLM的表现。

    

    将负面情绪重塑为积极思维是几种认知方法到心理健康和心理治疗的核心，大型语言模型解决方案可以使这种重塑更易实现。这种重塑通常并不简单，需要多个理性化步骤来揭示负面思维的潜在问题并使其变得更加积极。然而，目前该理性化过程被数据集和模型忽略，这些数据集和模型在一步中重塑思维。本研究填补了这一差距，通过使用一种名为"SocraticReframe"的新框架，通过合成生成的苏格拉底论证，扩充了用于积极文本重写的开源数据集。"SocraticReframe"使用一系列问答对来理性化思维重写过程。我们展示了这种苏格拉底论证显著改善了不同开源LLM的积极文本重写。

    arXiv:2403.03029v1 Announce Type: new  Abstract: Reframing a negative into a positive thought is at the crux of several cognitive approaches to mental health and psychotherapy that could be made more accessible by large language model-based solutions. Such reframing is typically non-trivial and requires multiple rationalization steps to uncover the underlying issue of a negative thought and transform it to be more positive. However, this rationalization process is currently neglected by both datasets and models which reframe thoughts in one step. In this work, we address this gap by augmenting open-source datasets for positive text rewriting with synthetically-generated Socratic rationales using a novel framework called \textsc{SocraticReframe}. \textsc{SocraticReframe} uses a sequence of question-answer pairs to rationalize the thought rewriting process. We show that such Socratic rationales significantly improve positive text rewriting for different open-source LLMs according to both
    
[^17]: 单词重要性解释了提示如何影响语言模型输出

    Word Importance Explains How Prompts Affect Language Model Outputs

    [https://arxiv.org/abs/2403.03028](https://arxiv.org/abs/2403.03028)

    通过改变提示中的单词，本研究提出了一种方法来解释大型语言模型（LLMs）的工作原理，从而揭示其对模型输出的影响。

    

    大型语言模型（LLMs）的出现彻底改变了各行各业的许多应用。然而，它们的“黑盒”性质常常阻碍了我们对其如何做出具体决策的理解，引发了人们对其透明性、可靠性和道德使用的担忧。本研究提出了一种方法，通过改变提示中的单词来提高LLMs的可解释性，以揭示其在模型输出上的统计影响。该方法受表格数据的排列重要性启发，屏蔽系统提示中的每个单词，并根据可用文本分数在多个用户输入上进行聚合来评估其对输出的影响。与传统注意力不同，单词重要性衡量提示中单词对任意定义的文本分数的影响，从而能够将单词的重要性分解为具体的感兴趣的度量-- 包括偏见、阅读水平、冗余等。此程序还使测量

    arXiv:2403.03028v1 Announce Type: new  Abstract: The emergence of large language models (LLMs) has revolutionized numerous applications across industries. However, their "black box" nature often hinders the understanding of how they make specific decisions, raising concerns about their transparency, reliability, and ethical use. This study presents a method to improve the explainability of LLMs by varying individual words in prompts to uncover their statistical impact on the model outputs. This approach, inspired by permutation importance for tabular data, masks each word in the system prompt and evaluates its effect on the outputs based on the available text scores aggregated over multiple user inputs. Unlike classical attention, word importance measures the impact of prompt words on arbitrarily-defined text scores, which enables decomposing the importance of words into the specific measures of interest--including bias, reading level, verbosity, etc. This procedure also enables measur
    
[^18]: 在文本数据集上评估多模式翻译模型的必要性

    The Case for Evaluating Multimodal Translation Models on Text Datasets

    [https://arxiv.org/abs/2403.03014](https://arxiv.org/abs/2403.03014)

    评估多模式翻译模型时，应考虑其利用视觉信息的能力和翻译复杂句子的表现，建议使用CoMMuTE评估框架。

    

    一个良好的评估框架应该通过衡量多模式机器翻译（MMT）模型在翻译任务中利用视觉信息的能力以及它们翻译复杂句子的能力来评估它们。然而，大多数当前的MMT工作是根据Multi30k测试集进行评估的，而这些测试集并不能衡量这些属性。因此，我们建议使用CoMMuTE评估框架来评估MMT模型的性能。

    arXiv:2403.03014v1 Announce Type: new  Abstract: A good evaluation framework should evaluate multimodal machine translation (MMT) models by measuring 1) their use of visual information to aid in the translation task and 2) their ability to translate complex sentences such as done for text-only machine translation. However, most current work in MMT is evaluated against the Multi30k testing sets, which do not measure these properties. Namely, the use of visual information by the MMT model cannot be shown directly from the Multi30k test set results and the sentences in Multi30k are are image captions, i.e., short, descriptive sentences, as opposed to complex sentences that typical text-only machine translation models are evaluated against.   Therefore, we propose that MMT models be evaluated using 1) the CoMMuTE evaluation framework, which measures the use of visual information by MMT models, 2) the text-only WMT news translation task test sets, which evaluates translation performance aga
    
[^19]: 使用LLMs的数据增强：数据视角、学习范式和挑战

    Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges

    [https://arxiv.org/abs/2403.02990](https://arxiv.org/abs/2403.02990)

    探讨了大型语言模型（LLMs）对数据增强的转变性影响，独特挑战和机遇，突出了LLMs在数据增强中引入的范式转变。

    

    在机器学习（ML）领域快速发展中，数据增强（DA）已成为一种关键技术，通过使训练样本多样化而无需额外数据收集来增强模型性能。本调查探讨了大型语言模型（LLMs）对数据增强的转变性影响，特别是在自然语言处理（NLP）领域及其他领域中它们提供的独特挑战和机遇。从数据视角和学习视角，我们研究了利用大型语言模型进行数据增强的各种策略，包括对LLM生成数据进行进一步训练的新颖学习范式的探索。此外，本文还阐明了该领域面临的主要挑战，从可控数据增强到多模态数据增强等。本调查突显了LLMs在数据增强中引入的范式转变，旨在作为一种...

    arXiv:2403.02990v1 Announce Type: cross  Abstract: In the rapidly evolving field of machine learning (ML), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of Large Language Models (LLMs) on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From a data perspective and a learning perspective, we examine various strategies that utilize Large Language Models for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for further training. Additionally, this paper delineates the primary challenges faced in this domain, ranging from controllable data augmentation to multi modal data augmentation. This survey highlights the paradigm shift introduced by LLMs in DA, aims to serve as a 
    
[^20]: 一个通用灵活的多概念解析框架用于多语言语义匹配

    A General and Flexible Multi-concept Parsing Framework for Multilingual Semantic Matching

    [https://arxiv.org/abs/2403.02975](https://arxiv.org/abs/2403.02975)

    提出一个通用灵活的多概念解析框架用于多语言语义匹配，以解决关键词和意图概念识别以及外部NER依赖的问题

    

    句子语义匹配是自然语言处理中的研究热点，在社区问答、搜索、聊天机器人和推荐等各种重要场景中具有相当重要的意义。本文提出了DC-Match来解开句子中的关键词和意图概念，并利用它们来优化匹配性能，以解决现有先进模型直接模拟两个句子之间单词的语义相关性而忽略关键词和意图概念的问题。尽管DC-Match是一个简单而有效的语义匹配方法，但它高度依赖外部NER技术来识别句子的关键词，这限制了对次要语言的语义匹配性能，因为通常很难获得令人满意的NER工具。

    arXiv:2403.02975v1 Announce Type: cross  Abstract: Sentence semantic matching is a research hotspot in natural language processing, which is considerably significant in various key scenarios, such as community question answering, searching, chatbot, and recommendation. Since most of the advanced models directly model the semantic relevance among words between two sentences while neglecting the \textit{keywords} and \textit{intents} concepts of them, DC-Match is proposed to disentangle keywords from intents and utilizes them to optimize the matching performance. Although DC-Match is a simple yet effective method for semantic matching, it highly depends on the external NER techniques to identify the keywords of sentences, which limits the performance of semantic matching for minor languages since satisfactory NER tools are usually hard to obtain. In this paper, we propose to generally and flexibly resolve the text into multi concepts for multilingual semantic matching to liberate the mod
    
[^21]: 面向证据的事实摘要化用于知识增强的零-shot问答

    Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering

    [https://arxiv.org/abs/2403.02966](https://arxiv.org/abs/2403.02966)

    提出了一种面向证据的事实摘要化框架EFSum，用于增强LLMs的零-shot QA性能，并确保摘要的有益性和忠实性。

    

    最近的研究探讨了利用知识图谱（KGs）来增强大语言模型（LLMs）的问答（QA）性能，然而结构化的KG形式化仍然具有挑战性。现有方法，如三元组形式或三元组事实的自由文本转换，遇到了一些问题。这些问题包括由于重复实体或关系而导致的证据密度降低，以及由于无法强调关键证据而导致的证据清晰度降低。为解决这些问题，我们提出了EFSum，一个面向证据的事实摘要化框架，用于通过知识增强的LLMs增强QA。我们通过蒸馏和偏好对齐来优化一个开源的LLM作为事实摘要器。我们的广泛实验证明，EFSum提高了LLM的零-shot QA性能，并且可以确保摘要的同时有益和忠实。

    arXiv:2403.02966v1 Announce Type: cross  Abstract: Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.
    
[^22]: SimuCourt: 利用真实司法判决文件构建司法决策代理

    SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents

    [https://arxiv.org/abs/2403.02959](https://arxiv.org/abs/2403.02959)

    提出了SimuCourt司法基准，包括真实世界的司法文件，并引入了司法决策任务和多代理框架，评估了代理的司法分析和决策能力

    

    随着深度学习、自然语言处理技术的发展，有效提高了传统司法行业各个方面的效率。然而，目前大多数工作主要集中在个别司法阶段，忽视了跨阶段的协作。随着由大型语言模型提供支持的自主代理在现实环境中变得越来越智能，并能做出复杂决策，为司法智能提供了新的见解。本文介绍了SimuCourt，一个司法基准，包括来自真实世界的420份判决文件，涵盖了三种最常见类型的司法案例，以及一个新颖任务司法决策，用于评估代理的司法分析和决策能力。为了支持这一任务，我们构建了一个大规模司法知识库，JudicialKB，其中包含多种法律知识。我们提出了一种新颖的多代理框架，AgentsCourt

    arXiv:2403.02959v1 Announce Type: cross  Abstract: With the development of deep learning, natural language processing technology has effectively improved the efficiency of various aspects of the traditional judicial industry. However, most current efforts focus solely on individual judicial stage, overlooking cross-stage collaboration. As the autonomous agents powered by large language models are becoming increasingly smart and able to make complex decisions in real-world settings, offering new insights for judicial intelligence. In this paper, (1) we introduce SimuCourt, a judicial benchmark that encompasses 420 judgment documents from real-world, spanning the three most common types of judicial cases, and a novel task Judicial Decision-Making to evaluate the judicial analysis and decision-making power of agents. To support this task, we construct a large-scale judicial knowledge base, JudicialKB, with multiple legal knowledge. (2) we propose a novel multi-agent framework, AgentsCourt
    
[^23]: 评估大型语言模型的文本生成SQL能力：全面评估

    Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation

    [https://arxiv.org/abs/2403.02951](https://arxiv.org/abs/2403.02951)

    大型语言模型在文本生成SQL任务中表现出色，但对于最佳提示模板和设计框架仍无共识，新数据集和评估任务有助于全面评估各种方法的表现，并提出了优化解决方案。

    

    大型语言模型（LLMs）已经成为推动文本生成SQL任务的强大工具，明显优于传统方法。然而，作为一个新兴的研究领域，对于最佳提示模板和设计框架仍然没有达成共识。

    arXiv:2403.02951v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have emerged as a powerful tool in advancing the Text-to-SQL task, significantly outperforming traditional methods. Nevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks. Additionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs' cognitive capabilities and the optimization of LLM-based solutions.To address the aforementioned issues, we firstly construct a new dataset designed to mitigate the risk of overfitting in LLMs. Then we formulate five evaluation tasks to comprehensively assess the performance of diverse methods across various LLMs throughout the Text-to-SQL process.Our study highlights the performance disparities among LLMs and proposes optimal in-context learning solutions tailored to each task. These findings offer
    
[^24]: PaperWeaver：通过将用户收集的论文与推荐论文上下文化，丰富主题论文提醒

    PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers

    [https://arxiv.org/abs/2403.02939](https://arxiv.org/abs/2403.02939)

    PaperWeaver通过将用户收集的论文与推荐论文上下文化，为研究人员提供了更丰富的主题论文提醒

    

    随着学术档案的迅速增长，研究人员订阅“论文提醒”系统，定期为他们推荐最近发表的与之前收集的论文相似的论文。然而，研究人员有时很难理解推荐论文与他们自己研究背景之间微妙的联系，因为现有系统只呈现论文标题和摘要。为了帮助研究人员发现这些联系，我们提出了PaperWeaver，这是一个丰富的论文提醒系统，根据用户收集的论文提供推荐论文的上下文化文本描述。PaperWeaver采用基于大语言模型（LLMs）的计算方法，从用户收集的论文中推断用户的研究兴趣，提取论文的特定背景，并在这些背景上比较推荐论文和收集的论文。我们的用户研究（N=15）表明，使用PaperWeaver的参与者能够

    arXiv:2403.02939v1 Announce Type: cross  Abstract: With the rapid growth of scholarly archives, researchers subscribe to "paper alert" systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts. To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users' research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to
    
[^25]: AIx Speed：使用语音识别模型的听力理解优化回放速度

    AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models

    [https://arxiv.org/abs/2403.02938](https://arxiv.org/abs/2403.02938)

    本研究探讨了人类是否可以听到经过优化的语音，并提出了一种系统，可根据音素为单位自动调整播放速度，以确保语音可辨识度。

    

    由于人类可以以比实际观察到的速度更快地倾听音频和观看视频，因此我们经常以更高的播放速度倾听或观看这些内容的片段，以提高内容理解的时间效率。为了进一步利用这种能力，已经开发出了根据用户情况和内容类型自动调整播放速度的系统，以协助更高效地理解时间序列内容。然而，这些系统仍有进一步提高人类速听能力的空间，即生成已经针对更精细的时间单位优化过的语音，并将其提供给人类。在本研究中，我们确定人类能否听到优化过的语音，并提出了一种系统，该系统可根据音素为单位自动调整播放速度，同时确保语音可辨识度。系统使用语音识别得分作为衡量人类能否听到的代理。

    arXiv:2403.02938v1 Announce Type: new  Abstract: Since humans can listen to audio and watch videos at faster speeds than actually observed, we often listen to or watch these pieces of content at higher playback speeds to increase the time efficiency of content comprehension. To further utilize this capability, systems that automatically adjust the playback speed according to the user's condition and the type of content to assist in more efficient comprehension of time-series content have been developed. However, there is still room for these systems to further extend human speed-listening ability by generating speech with playback speed optimized for even finer time units and providing it to humans. In this study, we determine whether humans can hear the optimized speech and propose a system that automatically adjusts playback speed at units as small as phonemes while ensuring speech intelligibility. The system uses the speech recognizer score as a proxy for how well a human can hear a
    
[^26]: RulePrompt: 弱监督文本分类与提示PLMs和自迭代逻辑规则

    RulePrompt: Weakly Supervised Text Classification with Prompting PLMs and Self-Iterative Logical Rules

    [https://arxiv.org/abs/2403.02932](https://arxiv.org/abs/2403.02932)

    提出了使用逻辑表达来表征类别含义的规则Prompt方法，结合PLMs和自迭代逻辑规则实现弱监督文本分类

    

    弱监督文本分类（WSTC），又称零样本或无数据文本分类，在动态和开放的Web环境中吸引了越来越多的关注，因为它仅需要每个类别的有限种子词（标签名称）而不需要标记数据就能对大量文本进行分类。本文首先提出了一种新型的基于规则的知识形式，使用逻辑表达来表征类别的含义。然后，我们借助PLMs和自迭代逻辑规则来实现弱监督文本分类。

    arXiv:2403.02932v1 Announce Type: new  Abstract: Weakly supervised text classification (WSTC), also called zero-shot or dataless text classification, has attracted increasing attention due to its applicability in classifying a mass of texts within the dynamic and open Web environment, since it requires only a limited set of seed words (label names) for each category instead of labeled data. With the help of recently popular prompting Pre-trained Language Models (PLMs), many studies leveraged manually crafted and/or automatically identified verbalizers to estimate the likelihood of categories, but they failed to differentiate the effects of these category-indicative words, let alone capture their correlations and realize adaptive adjustments according to the unlabeled corpus. In this paper, in order to let the PLM effectively understand each category, we at first propose a novel form of rule-based knowledge using logical expressions to characterize the meanings of categories. Then, we d
    
[^27]: BASS的再审视--利用统一语义图提升抽象摘要--一项复制研究

    A Second Look on BASS -- Boosting Abstractive Summarization with Unified Semantic Graphs -- A Replication Study

    [https://arxiv.org/abs/2403.02930](https://arxiv.org/abs/2403.02930)

    通过复制研究BASS框架，发现了与原始工作相比性能上的差异，并强调了撰写可复制论文的关键实践。

    

    我们展示了对BASS框架的详细复制研究，这是一个基于统一语义图概念的抽象摘要系统。我们的调查包括复制关键组件时遇到的挑战，以及一个消融研究来系统地隔离在复制新颖组件时根源于错误来源。我们的发现揭示了与原始工作相比性能上的差异。我们强调了即使是被合理省略的细节对于复制像BASS这样的先进框架的重要性，并强调了撰写可复制论文的关键实践。

    arXiv:2403.02930v1 Announce Type: new  Abstract: We present a detailed replication study of the BASS framework, an abstractive summarization system based on the notion of Unified Semantic Graphs. Our investigation includes challenges in replicating key components and an ablation study to systematically isolate error sources rooted in replicating novel components. Our findings reveal discrepancies in performance compared to the original work. We highlight the significance of paying careful attention even to reasonably omitted details for replicating advanced frameworks like BASS, and emphasize key practices for writing replicable papers.
    
[^28]: 通过信息流展示相互增强效应

    Demonstrating Mutual Reinforcement Effect through Information Flow

    [https://arxiv.org/abs/2403.02902](https://arxiv.org/abs/2403.02902)

    通过信息流分析证实了文本分类任务中单词级别和文本级别分类之间的相互增强效应，同时将该效应扩展到提示学习。

    

    相互增强效应(MRE)研究文本分类任务中单词级别和文本级别分类之间的协同关系。它认为两个分类级别的性能可以相互增强。然而，这种机制在先前的研究中尚未得到充分证明或解释。为了解决这一空白，我们利用信息流分析来观察和证实MRE理论。我们在六个MRE混合数据集上的实验揭示了模型中MRE的存在及其影响。此外，我们进行了微调实验，其结果与信息流实验的结果一致。两个实验结果的一致性证实了MRE的存在。此外，我们将MRE的应用扩展到提示学习，利用单词级别信息作为表达器来增强模型预测文本级别分类标签。

    arXiv:2403.02902v1 Announce Type: new  Abstract: The Mutual Reinforcement Effect (MRE) investigates the synergistic relationship between word-level and text-level classifications in text classification tasks. It posits that the performance of both classification levels can be mutually enhanced. However, this mechanism has not been adequately demonstrated or explained in prior research. To address this gap, we employ information flow analysis to observe and substantiate the MRE theory. Our experiments on six MRE hybrid datasets revealed the presence of MRE in the model and its impact. Additionally, we conducted fine-tuning experiments, whose results were consistent with those of the information flow experiments. The convergence of findings from both experiments corroborates the existence of MRE. Furthermore, we extended the application of MRE to prompt learning, utilizing word-level information as a verbalizer to bolster the model's prediction of text-level classification labels. In our
    
[^29]: 使用异构图对比迁移学习实现零样本跨语言文档级事件因果识别

    Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning

    [https://arxiv.org/abs/2403.02893](https://arxiv.org/abs/2403.02893)

    提出了一种使用异构图对比迁移学习的方法，实现了零样本跨语言文档级事件因果识别，并在实验证明在F1得分上优于之前的最先进模型。

    

    事件因果识别（ECI）指的是在文本中检测事件之间的因果关系。然而，大多数现有研究都集中在高资源语言下的句子级ECI，而对于低资源语言下更具挑战性的文档级ECI（DECI）却尚未得到充分探索。在本文中，我们提出了一种带有多粒度对比传递学习（GIMC）的异构图交互模型，用于实现零样本跨语言文档级ECI。具体来说，我们引入了一个异构图交互网络来建模文档中分散事件之间的远距离依赖关系。然后，为了提高从源语言学习到的因果知识的跨语言可转移性，我们提出了一个多粒度对比传递学习模块，以调整跨语言间的因果表示。大量实验证明，我们的框架在平均F1得分上优于之前的最先进模型约9.4%和8.2%。

    arXiv:2403.02893v1 Announce Type: cross  Abstract: Event Causality Identification (ECI) refers to detect causal relations between events in texts. However, most existing studies focus on sentence-level ECI with high-resource language, leaving more challenging document-level ECI (DECI) with low-resource languages under-explored. In this paper, we propose a Heterogeneous Graph Interaction Model with Multi-granularity Contrastive Transfer Learning (GIMC) for zero-shot cross-lingual document-level ECI. Specifically, we introduce a heterogeneous graph interaction network to model the long-distance dependencies between events that are scattered over document. Then, to improve cross-lingual transferability of causal knowledge learned from source language, we propose a multi-granularity contrastive transfer learning module to align the causal representations across languages. Extensive experiments show our framework outperforms previous state-of-the-art model by 9.4% and 8.2% of average F1 sco
    
[^30]: 在寻找真相：一种审问方法用于幻觉检测

    In Search of Truth: An Interrogation Approach to Hallucination Detection

    [https://arxiv.org/abs/2403.02889](https://arxiv.org/abs/2403.02889)

    提出了一种用于在大型语言模型中检测幻觉的新方法，解决了这些模型在各种现实场景中应用时遇到的关键问题，通过对多个数据集和LLMs进行广泛评估，展示了该方法的有效性。

    

    尽管大型语言模型（LLMs）取得了许多进展并且以前所未有的速度快速发展，但由于各种原因，它们对我们日常生活的各个方面的影响和整合仍然有限。一个阻碍它们广泛应用的关键因素是幻觉的发生，即LLMs创造出听起来真实但偏离事实真相的答案。在本文中，我们提出了一种新颖的方法用于检测大型语言模型中的幻觉，这解决了这些模型在各种现实场景中应用的一个关键问题。通过对多个数据集和LLMs进行广泛评估，包括Llama-2，我们研究了各种最新LLMs的幻觉水平，并展示了我们的方法在自动检测它们方面的有效性。值得注意的是，我们在一个特定实验中观察到Llama-2达到62%的幻觉水平，而我们的方法在没有依赖的情况下实现了87%的平衡准确率（B-ACC）。

    arXiv:2403.02889v1 Announce Type: new  Abstract: Despite the many advances of Large Language Models (LLMs) and their unprecedented rapid evolution, their impact and integration into every facet of our daily lives is limited due to various reasons. One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth. In this paper, we present a novel method for detecting hallucinations in large language models, which tackles a critical issue in the adoption of these models in various real-world scenarios. Through extensive evaluations across multiple datasets and LLMs, including Llama-2, we study the hallucination levels of various recent LLMs and demonstrate the effectiveness of our method to automatically detect them. Notably, we observe up to 62% hallucinations for Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy (B-ACC) of 87%, all without relying 
    
[^31]: MathScale: 数学推理的指导优化尺度

    MathScale: Scaling Instruction Tuning for Mathematical Reasoning

    [https://arxiv.org/abs/2403.02884](https://arxiv.org/abs/2403.02884)

    MathScale提出了一种简单可扩展的方法来创建高质量的数学推理数据，展现出在数学数据集大小方面的有效可扩展性。

    

    大型语言模型（LLMs）在问题解决方面展现了出色的能力，但它们在解决数学问题方面的熟练程度仍然不足。我们提出了MathScale，这是一种简单且可扩展的方法，使用前沿的LLMs（例如GPT-3.5）创建高质量的数学推理数据。受人类数学学习中的认知机制启发，它首先从种子数学问题中提取主题和知识点，然后构建一个概念图，随后用于生成新的数学问题。MathScale在我们生成的数学数据集的大小方面展现出了有效的可扩展性。因此，我们创建了一个包含两百万数学问题-答案对的数学推理数据集（MathScaleQA）。为了全面评估LLMs的数学推理能力，我们构建了MwpBench，这是一个数学问题词汇问题基准，包括十个数据集。

    arXiv:2403.02884v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., {\tt GPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct {\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten datasets (including 
    
[^32]: 通过硬负样本增强多模态对比学习中的概念理解

    Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples

    [https://arxiv.org/abs/2403.02875](https://arxiv.org/abs/2403.02875)

    提出了一种通过硬负样本改进多模态对比学习中概念理解的方法，并引入了一个评估视觉-语言模型中颜色、对象和大小细粒度对齐的新数据集。

    

    当前利用对比学习的多模态模型在发展精细的概念理解方面通常存在一些限制。在预训练过程中，由于随机负样本，导致几乎只有非常不同的概念进行损失函数比较。因此，模型在处理细粒度语义差异时遇到困难。为了解决这个问题，我们引入了一种新颖的预训练方法，结合了合成的硬负文本示例。这些硬负样本对应于视觉概念的排列，导致更精细的视觉和文本概念对齐。此外，我们引入了InpaintCOCO，一个用于评估视觉-语言模型中颜色、对象和大小细粒度对齐的新挑战性数据集。我们使用从COCO图像生成的信息填充来创建数据集，通过改变视觉概念，使图像不再与其原始标题匹配。我们的结果显示...

    arXiv:2403.02875v1 Announce Type: cross  Abstract: Current multimodal models leveraging contrastive learning often face limitations in developing fine-grained conceptual understanding. This is due to random negative samples during pretraining, causing almost exclusively very dissimilar concepts to be compared in the loss function. Consequently, the models struggle with fine-grained semantic differences. To address this problem, we introduce a novel pretraining method incorporating synthetic hard negative text examples. The hard negatives permute terms corresponding to visual concepts, leading to a more fine-grained visual and textual concept alignment. Further, we introduce InpaintCOCO, a new challenging dataset for assessing the fine-grained alignment of colors, objects, and sizes in vision-language models. We created the dataset using generative inpainting from COCO images by changing the visual concepts so that the images no longer match their original captions. Our results show sig
    
[^33]: 作为评判器的LLM的实证研究：精调评判器模型是特定任务的分类器

    An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers

    [https://arxiv.org/abs/2403.02839](https://arxiv.org/abs/2403.02839)

    精调评判模型在领域内测试上表现出色，但泛化能力和公平性不及GPT4。

    

    最近，利用大型语言模型（LLM）评估其他LLM质量的趋势日益增长。许多研究采用专有的闭源模型，尤其是GPT4，作为评估器。另外，其他研究利用开源LLM来精调评判模型作为评估器。在本研究中，我们对不同的评判模型进行了实证研究。我们的发现表明，尽管精调的评判模型在领域内测试集上能够达到较高的准确性，甚至超过GPT4，但它们本质上是特定任务的分类器，其泛化能力和公平性远低于GPT4。

    arXiv:2403.02839v1 Announce Type: new  Abstract: Recently, there has been a growing trend of utilizing Large Language Model (LLM) to evaluate the quality of other LLMs. Many studies have employed proprietary close-source models, especially GPT4, as the evaluator. Alternatively, other works have fine-tuned judge models based on open-source LLMs as the evaluator. In this study, we conduct an empirical study of different judge models on their evaluation capability. Our findings indicate that although the fine-tuned judge models achieve high accuracy on in-domain test sets, even surpassing GPT4, they are inherently task-specific classifiers, and their generalizability and fairness severely underperform GPT4.
    
[^34]: DPPA：用于大型语言模型的修剪方法以进行模型合并

    DPPA: Pruning Method for Large Language Model to Model Merging

    [https://arxiv.org/abs/2403.02799](https://arxiv.org/abs/2403.02799)

    提出了一种名为动态修剪分区增强（DPPA）的双阶段方法，用于解决合并复杂微调模型的挑战。

    

    模型合并是将从多个领域衍生出的微调模型相结合，旨在增强模型在各个领域的熟练度。主要关注点是解决参数冲突。现有大量研究已解决了合并阶段的这个问题，最新研究集中在通过修剪阶段来解决这个问题。DARE方法在应用于简单微调模型时表现出有希望的结果。然而，当用于显示与基线模型相比存在显著参数偏差的复杂微调模型时，该方法的有效性往往会减弱。本文介绍了一种名为动态修剪分区增强（DPPA）的双阶段方法，旨在解决合并复杂微调模型的挑战。首先，我们介绍了动态修剪（DP），这是一种基于量剪枝的改进方法，其目的是增强p

    arXiv:2403.02799v1 Announce Type: cross  Abstract: Model merging is to combine fine-tuned models derived from multiple domains, with the intent of enhancing the model's proficiency across various domains. The principal concern is the resolution of parameter conflicts. A substantial amount of existing research remedy this issue during the merging stage, with the latest study focusing on resolving this issue throughout the pruning stage. The DARE approach has exhibited promising outcomes when applied to a simplistic fine-tuned model. However, the efficacy of this method tends to wane when employed on complex fine-tuned models that show a significant parameter bias relative to the baseline model. In this paper, we introduce a dual-stage method termed Dynamic Pruning Partition Amplification (DPPA), devised to tackle the challenge of merging complex fine-tuned models. Initially, we introduce Dynamically Pruning (DP), an improved approach based on magnitude pruning, which aim is to enhance p
    
[^35]: 用大型语言模型评估和优化教育内容

    Evaluating and Optimizing Educational Content with Large Language Model Judgments

    [https://arxiv.org/abs/2403.02795](https://arxiv.org/abs/2403.02795)

    使用语言模型作为教育专家来评估教育内容的影响，展示了LMs作为可靠评估者的潜力，并介绍了一种指导优化方法。

    

    创建有效的教育材料通常需要对学生学习成果进行昂贵且耗时的研究。为了克服这一障碍，一个想法是构建学生学习的计算模型，并使用它们来优化教学材料。然而，模拟学习动态的认知过程是困难的。我们提出了一种使用语言模型（LMs）作为教育专家来评估各种指导对学习结果影响的替代方法。具体地，我们使用GPT-3.5来评估指导材料对不同学生群体的整体影响，并发现它可以复制诸如专业逆转效应和变异效应等已经建立的教育发现。这展示了LMs作为教育内容可靠评估者的潜力。基于这一见解，我们介绍了一种指导优化方法，其中一个LM生成指导。

    arXiv:2403.02795v1 Announce Type: new  Abstract: Creating effective educational materials generally requires expensive and time-consuming studies of student learning outcomes. To overcome this barrier, one idea is to build computational models of student learning and use them to optimize instructional materials. However, it is difficult to model the cognitive processes of learning dynamics. We propose an alternative approach that uses Language Models (LMs) as educational experts to assess the impact of various instructions on learning outcomes. Specifically, we use GPT-3.5 to evaluate the overall effect of instructional materials on different student groups and find that it can replicate well-established educational findings such as the Expertise Reversal Effect and the Variability Effect. This demonstrates the potential of LMs as reliable evaluators of educational content. Building on this insight, we introduce an instruction optimization approach in which one LM generates instruction
    
[^36]: 基于内存的学习：大型语言模型的声明式学习框架

    In-Memory Learning: A Declarative Learning Framework for Large Language Models

    [https://arxiv.org/abs/2403.02757](https://arxiv.org/abs/2403.02757)

    提出了一个基于内存的声明式学习框架，通过总结过去经验，帮助代理从中提取见解并改进性能，实现自我改进。进行了系统实验证明了该框架的有效性。

    

    调查代理是否能够与环境对齐而不依赖人工标记数据，提出了一个有趣的研究主题。受智能生物对齐过程的启发，我们提出了一个新颖的学习框架，其中声明式内存在总结过去经验中起着重要作用。代理能够从过去经验中提炼见解，改进和更新现有笔记以提高在环境中的表现。整个过程发生在内存组件中，并通过自然语言实现，因此我们将此框架描述为基于内存的学习。我们还深入探讨了设计用于评估自我改进过程的基准测试的关键特性。通过系统实验，我们展示了我们框架的有效性并提供了对这个问题的见解。

    arXiv:2403.02757v1 Announce Type: new  Abstract: The exploration of whether agents can align with their environment without relying on human-labeled data presents an intriguing research topic. Drawing inspiration from the alignment process observed in intelligent organisms, where declarative memory plays a pivotal role in summarizing past experiences, we propose a novel learning framework. The agents adeptly distill insights from past experiences, refining and updating existing notes to enhance their performance in the environment. This entire process transpires within the memory components and is implemented through natural language, so we character this framework as In-memory Learning. We also delve into the key features of benchmarks designed to evaluate the self-improvement process. Through systematic experiments, we demonstrate the effectiveness of our framework and provide insights into this problem.
    
[^37]: 通过角色提示指导的通用能力保留的领域自适应大型语言模型

    Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models

    [https://arxiv.org/abs/2403.02756](https://arxiv.org/abs/2403.02756)

    通过RolE Prompting Guided Multi-Domain Adaptation (REGA)策略，这篇论文提出了一种有效管理多领域大型语言模型适应的新方法，包括自我蒸馏、角色提示和角色集成这三个关键组件。

    

    针对大型语言模型（LLMs）应用于专门领域时遭遇的严重遗忘问题，以及为多个领域构建多功能模型时出现的性能下降问题，本文提出了RolE Prompting Guided Multi-Domain Adaptation (REGA) 策略。这一新颖方法通过三个关键组件有效管理多领域LLM适应：1）自我蒸馏（Self-Distillation）构建和重播通用域实例以减轻遗忘；2）角色提示（Role Prompting）为通用域分配中心提示和为每个特定领域分配独特角色提示，以最小化训练过程中不同领域之间的混淆；3）角色集成（Role Integration）

    arXiv:2403.02756v1 Announce Type: new  Abstract: The growing interest in Large Language Models (LLMs) for specialized applications has revealed a significant challenge: when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains. In response to these issues, we present the RolE Prompting Guided Multi-Domain Adaptation (REGA) strategy. This novel approach effectively manages multi-domain LLM adaptation through three key components: 1) Self-Distillation constructs and replays general-domain exemplars to alleviate catastrophic forgetting. 2) Role Prompting assigns a central prompt to the general domain and a unique role prompt to each specific domain to minimize inter-domain confusion during training. 3) Role Integration reu
    
[^38]: CURATRON：完整健壮偏好数据用于大型语言模型的健壮对齐

    CURATRON: Complete Robust Preference Data for Robust Alignment of Large Language Models

    [https://arxiv.org/abs/2403.02745](https://arxiv.org/abs/2403.02745)

    本论文提出了一种新方法，通过彻底重校准偏好数据集中的价值观，以增强大型语言模型对问题的韧性。

    

    这篇论文解决了通过偏好学习（PL）将大型语言模型（LLMs）与人类价值观对齐的挑战，重点关注偏好数据集中不完整和损坏的问题。我们提出了一种新方法，通过彻底和完全地重新校准这些数据集中的价值观，以增强LLMs对问题的韧性。特别是，我们设计了一个有保证的多项式时间排名算法，可以增强几种现有模型的健壮性，比如经典的Bradley–Terry–Luce（BTL）（Bradley和Terry，1952）模型以及对其某些推广。据我们所知，我们的工作是第一个提出一种可证明在高概率下恢复{\epsilon}-最优排序的算法，同时允许每个模型响应多达O(n)扰动的成对比较结果。此外，我们展示了在部分观察设置下的健壮恢复结果。我们的实验证实了我们的算法

    arXiv:2403.02745v1 Announce Type: new  Abstract: This paper addresses the challenges of aligning large language models (LLMs) with human values via preference learning (PL), with a focus on the issues of incomplete and corrupted data in preference datasets. We propose a novel method for robustly and completely recalibrating values within these datasets to enhance LLMs resilience against the issues. In particular, we devise a guaranteed polynomial time ranking algorithm that robustifies several existing models, such as the classic Bradley--Terry--Luce (BTL) (Bradley and Terry, 1952) model and certain generalizations of it. To the best of our knowledge, our present work is the first to propose an algorithm that provably recovers an {\epsilon}-optimal ranking with high probability while allowing as large as O(n) perturbed pairwise comparison results per model response. Furthermore, we show robust recovery results in the partially observed setting. Our experiments confirm that our algorith
    
[^39]: 面向麻醉学的中文大型语言模型训练

    Towards Training A Chinese Large Language Model for Anesthesiology

    [https://arxiv.org/abs/2403.02742](https://arxiv.org/abs/2403.02742)

    Hypnos通过改善数据质量和采用由一般到特定的训练策略，为面向麻醉学的中文大型语言模型的建设做出了重要贡献。

    

    最近，由于其显著的实用价值，医学大型语言模型（LLMs）受到了广泛关注。然而，大多数现有研究集中在一般医学领域，需要深入研究特定领域如麻醉学中的LLMs。为此，我们介绍了Hypnos，一种基于现有LLMs（如Llama）构建的中文麻醉模型。Hypnos的贡献包括三个方面：1）数据，例如来自当前LLMs的Self-Instruct获得的数据可能存在不准确性。Hypnos实施了交叉过滤策略来提高数据质量。该策略涉及使用一个LLM来评估另一个LLM生成的数据的质量，并过滤出质量较低的数据。2）Hypnos采用了一种由一般到特定的训练策略，即通过使用一般医学数据微调LLMs，然后再使用来自麻醉学的数据进一步改进微调的LLMs。

    arXiv:2403.02742v1 Announce Type: new  Abstract: Medical large language models (LLMs) have gained popularity recently due to their significant practical utility. However, most existing research focuses on general medicine, and there is a need for in-depth study of LLMs in specific fields like anesthesiology. To fill the gap, we introduce Hypnos, a Chinese Anesthesia model built upon existing LLMs, e.g., Llama. Hypnos' contributions have three aspects: 1) The data, such as utilizing Self-Instruct, acquired from current LLMs likely includes inaccuracies. Hypnos implements a cross-filtering strategy to improve the data quality. This strategy involves using one LLM to assess the quality of the generated data from another LLM and filtering out the data with low quality. 2) Hypnos employs a general-to-specific training strategy that starts by fine-tuning LLMs using the general medicine data and subsequently improving the fine-tuned LLMs using data specifically from Anesthesiology. The genera
    
[^40]: 因果引导：基于前门调整的大型语言模型启发式去偏方法

    Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment

    [https://arxiv.org/abs/2403.02738](https://arxiv.org/abs/2403.02738)

    提出了一种新型因果引导方法，通过前门调整有效减轻大型语言模型的偏见。

    

    尽管现有的诸如上下文学习和思维链等大型语言模型（LLMs）启发式方法取得了显著成就，但它们仍然面临各种偏见挑战。本文揭示了启发式方法背后的因果关系，并提出了一种基于前门调整的新型因果引导方法，以有效减轻LLMs的偏见。具体而言，通过设计提示而无需访问LLMs的参数和logit来实施因果干预。由LLMs生成的思维链被用作中介变量，通过前门计算输入提示与输出答案之间的因果效应。

    arXiv:2403.02738v1 Announce Type: new  Abstract: Despite the significant achievements of existing prompting methods such as in-context learning and chain-of-thought for large language models (LLMs), they still face challenges of various biases. Traditional debiasing methods primarily focus on the model training stage, including data augmentation-based and reweight-based approaches, with the limitations of addressing the complex biases of LLMs. To address such limitations, the causal relationship behind the prompting methods is uncovered using a structural causal model, and a novel causal prompting method based on front-door adjustment is proposed to effectively mitigate the bias of LLMs. In specific, causal intervention is implemented by designing the prompts without accessing the parameters and logits of LLMs.The chain-of-thoughts generated by LLMs are employed as the mediator variable and the causal effect between the input prompt and the output answers is calculated through front-do
    
[^41]: HARGPT：LLMs是否可以进行零样本人类活动识别？

    HARGPT: Are LLMs Zero-Shot Human Activity Recognizers?

    [https://arxiv.org/abs/2403.02727](https://arxiv.org/abs/2403.02727)

    HARGPT研究表明LLMs可以通过适当提示理解原始IMU数据，实现零样本人类活动识别，并在性能上优于传统机器学习和最先进深度分类模型。

    

    关于大型语言模型（LLMs）作为与网络物理系统（CPS）无缝集成的基础模型在解释物理世界方面的潜力存在持续的争论。本文通过进行案例研究来回答以下问题：LLMs是否能够进行零样本的人类活动识别（HAR）。我们的研究，HARGPT，通过展示LLMs可以理解原始IMU数据并以零样本方式执行HAR任务，仅需适当的提示，肯定了这个问题。HARGPT将原始IMU数据输入LLMs，并利用角色扮演和逐步思考策略进行提示。我们在GPT4上对HARGPT进行基准测试，使用了两个不同类间相似性的公共数据集，并比较了基于传统机器学习和最先进深度分类模型的各种基线。显著地，LLMs成功地从原始IMU数据中识别人类活动，并始终优于

    arXiv:2403.02727v1 Announce Type: cross  Abstract: There is an ongoing debate regarding the potential of Large Language Models (LLMs) as foundational models seamlessly integrated with Cyber-Physical Systems (CPS) for interpreting the physical world. In this paper, we carry out a case study to answer the following question: Are LLMs capable of zero-shot human activity recognition (HAR). Our study, HARGPT, presents an affirmative answer by demonstrating that LLMs can comprehend raw IMU data and perform HAR tasks in a zero-shot manner, with only appropriate prompts. HARGPT inputs raw IMU data into LLMs and utilizes the role-play and think step-by-step strategies for prompting. We benchmark HARGPT on GPT4 using two public datasets of different inter-class similarities and compare various baselines both based on traditional machine learning and state-of-the-art deep classification models. Remarkably, LLMs successfully recognize human activities from raw IMU data and consistently outperform 
    
[^42]: DP-CRE: 通过分离对比学习和记忆结构保留进行持续关系抽取

    DP-CRE: Continual Relation Extraction via Decoupled Contrastive Learning and Memory Structure Preservation

    [https://arxiv.org/abs/2403.02718](https://arxiv.org/abs/2403.02718)

    DP-CRE框架通过分离对比学习和记忆结构保留的方式，显著优于其他持续关系抽取基线模型，在处理在持续学习过程中出现的灾难性遗忘挑战时表现出色。

    

    持续关系抽取（CRE）旨在从非静态数据流中逐步学习关系知识。由于引入新的关系任务可能会掩盖先前学到的信息，灾难性遗忘成为该领域的一个重要挑战。当前基于重播的训练范式优先考虑所有数据，并通过多轮训练内存样本，这会导致过拟合老任务和对新任务的显着偏差，因为重播集合的不平衡性。为了解决这个问题，我们引入了DecouPled CRE（DP-CRE）框架，将先前信息保留和新知识获取的过程分离。该框架检查嵌入空间随着新关系类别的出现而发生的变化，明显管理知识的保存和获取。大量实验表明，DP-CRE在跨越t

    arXiv:2403.02718v1 Announce Type: new  Abstract: Continuous Relation Extraction (CRE) aims to incrementally learn relation knowledge from a non-stationary stream of data. Since the introduction of new relational tasks can overshadow previously learned information, catastrophic forgetting becomes a significant challenge in this domain. Current replay-based training paradigms prioritize all data uniformly and train memory samples through multiple rounds, which would result in overfitting old tasks and pronounced bias towards new tasks because of the imbalances of the replay set. To handle the problem, we introduce the DecouPled CRE (DP-CRE) framework that decouples the process of prior information preservation and new knowledge acquisition. This framework examines alterations in the embedding space as new relation classes emerge, distinctly managing the preservation and acquisition of knowledge. Extensive experiments show that DP-CRE significantly outperforms other CRE baselines across t
    
[^43]: 跨越语言视野：越南大型语言模型的微调和综合评估

    Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models

    [https://arxiv.org/abs/2403.02715](https://arxiv.org/abs/2403.02715)

    该研究通过对LLMs在越南语上进行微调和综合评估，发现微调后的模型在越南语理解和生成方面表现出良好能力，同时指出模型参数数量与性能之间存在着权衡关系，训练或微调数据集的质量是影响LLM性能的关键因素。

    

    最近大型语言模型（LLMs）的进展强调了它们在人工智能发展中的重要性。然而，尽管在多语言数据集上进行了广泛的预训练，但目前开源的LLMs在处理越南语方面的效果有限。这一挑战是由于缺乏专门针对越南语LLM评估的系统化基准数据集和指标。为了缓解这些问题，我们专门为越南语进行了LLM的微调，并开发了一个涵盖10个常见任务和31个指标的综合评估框架。我们的评估结果表明，经过微调的LLMs在越南语方面表现出了增强的理解和生成能力。此外，我们的分析表明，具有更多参数的模型可能会带来更多的偏见和未校准的输出，而影响LLM性能的关键因素是训练或微调数据集的质量。

    arXiv:2403.02715v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have underscored their importance in the evolution of artificial intelligence. However, despite extensive pretraining on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese. The challenge is exacerbated by the absence of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation. To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 common tasks and 31 metrics. Our evaluation results reveal that the fine-tuned LLMs exhibit enhanced comprehension and generative capabilities in Vietnamese. Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or fine-tuning datasets. These insights und
    
[^44]: Android在动物园中: GUI代理的动作思维链

    Android in the Zoo: Chain-of-Action-Thought for GUI Agents

    [https://arxiv.org/abs/2403.02713](https://arxiv.org/abs/2403.02713)

    该研究提出了一个名为CoAT的Chain-of-Action-Thought模型，通过考虑先前动作描述、当前屏幕情况以及未来动作思考，显著提高了智能手机GUI代理的任务执行效果。

    

    大型语言模型（LLM）导致智能手机上的大量自主GUI代理激增，这些代理通过预测API的一系列动作来完成由自然语言触发的任务。尽管该任务高度依赖于过去的动作和视觉观察，但现有研究通常很少考虑中间截图和屏幕操作传递的语义信息。为了解决这一问题，本文提出了动作思维链（CoAT），它考虑了先前动作的描述、当前屏幕，更重要的是分析应当执行的动作以及选择的动作带来的结果。我们证明，在使用现成LLM进行零次学习的情况下，CoAT相比于标准上下文建模显著提高了目标的完成情况。为了进一步促进这一研究领域的发展，我们构建了一个名为Android-In-The-Zoo（AitZ）的基准测试集，其中包含18,643个屏幕动作对。

    arXiv:2403.02713v1 Announce Type: new  Abstract: Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typical consider little semantic information carried out by intermediate screenshots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a zero-shot setting upon an off-the-shell LLM, CoAT significantly improves the goal progress compared to standard context modeling. To further facilitate the research in this line, we construct a benchmark Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pa
    
[^45]: Breeze-7B技术报告

    Breeze-7B Technical Report

    [https://arxiv.org/abs/2403.02712](https://arxiv.org/abs/2403.02712)

    Breeze-7B是基于Mistral-7B的开源语言模型，旨在改善中文语境下的语言理解和聊天机器人功能，展现出在复杂度类别中的出色性能。

    

    arXiv:2403.02712v1 类型：新论 摘要：Breeze-7B是一个基于Mistral-7B的开源语言模型，旨在改善中文传统语境下的语言理解和聊天机器人功能。本技术报告概述了Breeze-7B模型的额外预训练、微调和评估阶段。Breeze-7B系列的基础和聊天模型在语言理解和聊天机器人任务上表现良好，在一些与其复杂度类似的模型中达到了较好的性能。

    arXiv:2403.02712v1 Announce Type: new  Abstract: Breeze-7B is an open-source language model based on Mistral-7B, designed to address the need for improved language comprehension and chatbot-oriented capabilities in Traditional Chinese. This technical report provides an overview of the additional pretraining, finetuning, and evaluation stages for the Breeze-7B model. The Breeze-7B family of base and chat models exhibits good performance on language comprehension and chatbot-oriented tasks, reaching the top in several benchmarks among models comparable in its complexity class.
    
[^46]: 因果之行：利用前门调整消除多跳事实验证的偏见

    Causal Walk: Debiasing Multi-Hop Fact Verification with Front-Door Adjustment

    [https://arxiv.org/abs/2403.02698](https://arxiv.org/abs/2403.02698)

    提出了一种新方法Causal Walk，从因果角度利用前门调整消除多跳事实验证的偏见。

    

    传统的多跳事实验证模型倾向于依赖于注释工件中的偶然相关性，导致在无偏数据集上明显性能下降。在各种去偏作品中，基于因果推断的方法因执行理论上保证的去偏操作（如干预或对立推理）而变得流行。然而，现有的基于因果推断的去偏方法，主要将事实验证表述为解决肤浅偏见模式的单跳推理任务，无法处理隐藏在多个证据跳跃中的复杂偏见模式。为了解决这一挑战，我们提出了因果之行，这是一种从因果角度进行前门调整的用于消除多跳事实验证偏见的新方法。

    arXiv:2403.02698v1 Announce Type: new  Abstract: Conventional multi-hop fact verification models are prone to rely on spurious correlations from the annotation artifacts, leading to an obvious performance decline on unbiased datasets. Among the various debiasing works, the causal inference-based methods become popular by performing theoretically guaranteed debiasing such as casual intervention or counterfactual reasoning. However, existing causal inference-based debiasing methods, which mainly formulate fact verification as a single-hop reasoning task to tackle shallow bias patterns, cannot deal with the complicated bias patterns hidden in multiple hops of evidence. To address the challenge, we propose Causal Walk, a novel method for debiasing multi-hop fact verification from a causal perspective with front-door adjustment. Specifically, in the structural causal model, the reasoning path between the treatment (the input claim-evidence graph) and the outcome (the veracity label) is intr
    
[^47]: 面向大型语言模型的隐私感知语义缓存

    Privacy-Aware Semantic Cache for Large Language Models

    [https://arxiv.org/abs/2403.02694](https://arxiv.org/abs/2403.02694)

    MeanCache是一种面向LLMs的语义缓存，能够识别语义上相似的查询，从而减少查询成本，服务提供商负载和环境影响。

    

    大型语言模型（LLMs）如ChatGPT、Google Bard、Claude和Llama 2彻底改变了自然语言处理和搜索引擎动态。然而，这些模型造成了异常高的计算成本。本文介绍了MeanCache，一种用于LLMs的语义缓存，它能够识别语义上相似的查询以确定缓存命中或未命中。

    arXiv:2403.02694v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2 have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters and inference on these models also demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries, leading to unacceptable false hit-and-miss rates.   This paper introduces MeanCache, a semantic cache for LLMs that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache lever
    
[^48]: InjecAgent：基于工具集成的大型语言模型Agent中的间接提示注入基准测试

    InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents

    [https://arxiv.org/abs/2403.02691](https://arxiv.org/abs/2403.02691)

    本研究引入了InjecAgent基准测试，用于评估工具集成的大型语言模型代理对间接提示注入攻击的脆弱性，通过评估30种LLM代理，发现这些代理存在漏洞

    

    最近的工作将LLMs作为代理体现出来，使它们能够访问工具，执行操作，并与外部内容（例如，电子邮件或网站）进行交互。然而，外部内容引入了间接提示注入（IPI）攻击的风险，恶意指令被嵌入LLMs处理的内容中，旨在操纵这些代理执行对用户有害的操作。考虑到这类攻击的潜在严重后果，建立用于评估和减轻这些风险的基准测试至关重要。在这项工作中，我们介绍了InjecAgent，这是一个旨在评估工具集成的LLM代理对IPI攻击的脆弱性的基准测试。InjecAgent包括1,054个测试用例，涵盖17种不同的用户工具和62种攻击者工具。我们将攻击意图分为两种主要类型：对用户造成直接伤害和窃取私人数据。我们评估了30种不同的LLM代理，并表明这些代理是脆弱的。

    arXiv:2403.02691v1 Announce Type: new  Abstract: Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites). However, external content introduces the risk of indirect prompt injection (IPI) attacks, where malicious instructions are embedded within the content processed by LLMs, aiming to manipulate these agents into executing detrimental actions against users. Given the potentially severe consequences of such attacks, establishing benchmarks to assess and mitigate these risks is imperative.   In this work, we introduce InjecAgent, a benchmark designed to assess the vulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools. We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data. We evaluate 30 different LLM agents and show that agents are vulnerable
    
[^49]: 细调多模态语言模型是高质量的图像文本数据过滤器

    Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters

    [https://arxiv.org/abs/2403.02677](https://arxiv.org/abs/2403.02677)

    通过细调多模态语言模型，我们提出了一种新框架，能够更准确、更全面地过滤图像文本数据，从而提高数据质量和预训练模型的性能。

    

    我们提出了一种通过利用经过良好调整的多模态语言模型（MLM）来过滤图像文本数据的新框架。我们的方法通过整合MLMs的最新进展胜过主流过滤方法（例如CLIPScore）。我们设计了四个独特而互补的指标来全面衡量图像文本数据的质量。我们建立了一个新的流程来构建高质量的指导数据，用于细调MLMs作为数据过滤器。与CLIPScore相比，我们的MLM过滤器产生更精确、更全面的分数，直接提高了过滤数据的质量，并提升了预训练模型的性能。我们在流行的基础模型（即CLIP和BLIP2）和各种下游任务上显著优于CLIPScore。我们的MLM过滤器可以推广到不同的模型和任务，并可以作为CLIPScore的即插即用替代品。提供了额外的消融研究来验证我们的设计。

    arXiv:2403.02677v1 Announce Type: cross  Abstract: We propose a novel framework for filtering image-text data by leveraging fine-tuned Multimodal Language Models (MLMs). Our approach outperforms predominant filtering methods (e.g., CLIPScore) via integrating the recent advances in MLMs. We design four distinct yet complementary metrics to holistically measure the quality of image-text data. A new pipeline is established to construct high-quality instruction data for fine-tuning MLMs as data filters. Comparing with CLIPScore, our MLM filters produce more precise and comprehensive scores that directly improve the quality of filtered data and boost the performance of pre-trained models. We achieve significant improvements over CLIPScore on popular foundation models (i.e., CLIP and BLIP2) and various downstream tasks. Our MLM filter can generalize to different models and tasks, and be used as a drop-in replacement for CLIPScore. An additional ablation study is provided to verify our design
    
[^50]: 重新审视语法错误修正的元评估

    Revisiting Meta-evaluation for Grammatical Error Correction

    [https://arxiv.org/abs/2403.02674](https://arxiv.org/abs/2403.02674)

    该论文提出了SEEDA，一个用于语法错误修正的新数据集，提供了对12种最先进系统进行元评估的校正，通过在句子级别元评估中对粒度进行对齐，提高了相关性。

    

    Metrics在语法错误修正（GEC）中自动评估的基础。其中，元评估依赖于它们与人类判断的相关性。然而，英语GEC中常规的元评估面临一些挑战，包括由于评估粒度不一致而导致的偏见，以及使用传统系统的过时设置。针对这些问题，本文提出了SEEDA，这是一个用于GEC元评估的新数据集，包括校正和两种不同粒度（基于编辑和基于句子）的人类评级。

    arXiv:2403.02674v1 Announce Type: new  Abstract: Metrics are the foundation for automatic evaluation in grammatical error correction (GEC), with their evaluation of the metrics (meta-evaluation) relying on their correlation with human judgments. However, conventional meta-evaluations in English GEC encounter several challenges including biases caused by inconsistencies in evaluation granularity, and an outdated setup using classical systems. These problems can lead to misinterpretation of metrics and potentially hinder the applicability of GEC techniques. To address these issues, this paper proposes SEEDA, a new dataset for GEC meta-evaluation. SEEDA consists of corrections with human ratings along two different granularities: edit-based and sentence-based, covering 12 state-of-the-art systems including large language models (LLMs), and two human corrections with different focuses. The results of improved correlations by aligning the granularity in the sentence-level meta-evaluation, s
    
[^51]: FinReport: 通过新闻因素分析模型解释可解释的股票盈利预测

    FinReport: Explainable Stock Earnings Forecasting via News Factor Analyzing Model

    [https://arxiv.org/abs/2403.02647](https://arxiv.org/abs/2403.02647)

    FinReport是一个自动系统，通过金融新闻公告和多因素模型来生成专业的股票盈利预测报告，旨在帮助普通投资者收集信息、分析数据并生成报告。

    

    arXiv:2403.02647v1 公告类型: 跨领域 摘要: 由于实际情况下投资者的需求，股票盈利预测这一任务受到了相当多的关注。然而，与金融机构相比，普通投资者很难挖掘因素并分析新闻。另一方面，尽管金融领域的大型语言模型可以以对话机器人的形式为用户提供服务，但仍需要用户具备金融知识提出合理问题。为了提高用户体验，我们旨在构建一个自动系统，名为FinReport，以便普通投资者收集信息、分析信息，并在总结后生成报告。

    arXiv:2403.02647v1 Announce Type: cross  Abstract: The task of stock earnings forecasting has received considerable attention due to the demand investors in real-world scenarios. However, compared with financial institutions, it is not easy for ordinary investors to mine factors and analyze news. On the other hand, although large language models in the financial field can serve users in the form of dialogue robots, it still requires users to have financial knowledge to ask reasonable questions. To serve the user experience, we aim to build an automatic system, FinReport, for ordinary investors to collect information, analyze it, and generate reports after summarizing.   Specifically, our FinReport is based on financial news announcements and a multi-factor model to ensure the professionalism of the report. The FinReport consists of three modules: news factorization module, return forecasting module, risk assessment module. The news factorization module involves understanding news infor
    
[^52]: 探讨大型语言模型在组合关系推理中的局限性

    Exploring the Limitations of Large Language Models in Compositional Relation Reasoning

    [https://arxiv.org/abs/2403.02615](https://arxiv.org/abs/2403.02615)

    评估了大型语言模型在组合关系推理中的能力，设计了涵盖六种不同类型组合关系的基准测试，并扩展到多语言环境下进行评估。

    

    我们对大型语言模型(LLMs)在通过一个包含1,500个英文测试案例的基准测试中推理组合关系的能力进行了全面评估，旨在涵盖六种不同类型的组合关系：位置、比较、个人、数学、身份和其他。意识到多语言能力的重要性，我们将评估扩展到将这些案例翻译成中文、日文、法文和韩文。我们的多语言组合关系(MCR)基准旨在探讨大型语言模型在处理不同语言背景下的组合关系推理的稳健性和适应性。

    arXiv:2403.02615v1 Announce Type: new  Abstract: We present a comprehensive evaluation of large language models(LLMs)' ability to reason about composition relations through a benchmark encompassing 1,500 test cases in English, designed to cover six distinct types of composition relations: Positional, Comparative, Personal, Mathematical, Identity, and Other. Acknowledging the significance of multilingual capabilities, we expanded our assessment to include translations of these cases into Chinese, Japanese, French, and Korean. Our Multilingual Composition Relation (MCR) benchmark aims at investigating the robustness and adaptability of LLMs in handling composition relation reasoning across diverse linguistic contexts.
    
[^53]: 改进零样本事件检测的事件定义遵循

    Improving Event Definition Following For Zero-Shot Event Detection

    [https://arxiv.org/abs/2403.02586](https://arxiv.org/abs/2403.02586)

    通过构建多样化的事件定义数据集，本研究展示出大量的事件类型和多样的事件定义对于改进零样本事件检测可以显著提升性能。

    

    零样本事件检测上现有的方法通常在已知事件类型注释的数据集上训练模型，并提示它们使用未见过的事件定义。这些方法偶尔取得成功，但总体表现不尽如人意。本文旨在通过训练模型更好地遵循事件定义来改进零样本事件检测。我们假设一组多样化的事件类型和定义是模型学习遵循事件定义的关键，而现有的事件提取数据集侧重于为少数事件类型注释许多高质量示例。为验证我们的假设，我们构建了一个自动生成的多样化事件定义（DivED）数据集，并进行了比较研究。我们的实验显示，大量事件类型（200个）和多样化的事件定义可以显著提升事件提取性能；另一方面，性能不会随着超过十个事件类型的增加而增长。

    arXiv:2403.02586v1 Announce Type: new  Abstract: Existing approaches on zero-shot event detection usually train models on datasets annotated with known event types, and prompt them with unseen event definitions. These approaches yield sporadic successes, yet generally fall short of expectations. In this work, we aim to improve zero-shot event detection by training models to better follow event definitions. We hypothesize that a diverse set of event types and definitions are the key for models to learn to follow event definitions while existing event extraction datasets focus on annotating many high-quality examples for a few event types. To verify our hypothesis, we construct an automatically generated Diverse Event Definition (DivED) dataset and conduct comparative studies. Our experiments reveal that a large number of event types (200) and diverse event definitions can significantly boost event extraction performance; on the other hand, the performance does not scale with over ten ex
    
[^54]: ChatCite：LLM代理与人工工作流引导用于比较文学综述

    ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary

    [https://arxiv.org/abs/2403.02574](https://arxiv.org/abs/2403.02574)

    ChatCite是一个LLM代理，通过人工工作流引导进行比较文学综述，利用反思逐步机制生成摘要。

    

    arXiv:2403.02574v1 公告类型：跨学科 摘要：文献综述是研究过程中不可或缺的一步。它有助于理解研究问题，并在进行以往作品比较分析时了解当前研究情况。然而，文献总结是具有挑战性和耗时的。先前基于LLM的文献综述研究主要集中在完整过程，包括文献检索、筛选和总结。然而，对于总结步骤，简单的CoT方法往往缺乏提供广泛比较总结的能力。在这项工作中，我们首先专注于独立文献总结步骤并引入ChatCite，一个具有人工工作流引导用于比较文学综述的LLM代理。该代理通过模仿人类工作流，首先从相关文献中提取关键要素，然后使用反思逐步机制生成摘要。

    arXiv:2403.02574v1 Announce Type: cross  Abstract: The literature review is an indispensable step in the research process. It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works. However, literature summary is challenging and time consuming. The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization. However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary. In this work, we firstly focus on the independent literature summarization step and introduce ChatCite, an LLM agent with human workflow guidance for comparative literature summary. This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism. In order to better ev
    
[^55]: 通过代码从LLMs中引出更好的多语言结构推理

    Eliciting Better Multilingual Structured Reasoning from LLMs through Code

    [https://arxiv.org/abs/2403.02567](https://arxiv.org/abs/2403.02567)

    LLMs在多语言推理任务上表现出较弱的性能，本文提出了通过代码训练和推理来改善多语言结构化推理能力的方法。

    

    大型语言模型（LLM）的发展在推理方面取得了进展，但研究仅限于英语或简单的推理任务。因此，我们引入了一个名为xSTREET的多语言结构化推理和解释数据集，涵盖了六种语言的四个任务。xSTREET暴露了基本LLM在英语和非英语推理任务之间的性能差距。然后，我们提出了两种方法来弥补这一差距，建立在LLM在代码上训练更好的推理这一观点基础上。首先，在训练时，我们使用机器翻译将代码数据集增强为多语言注释，同时保持程序代码不变。其次，在推断时，我们通过采用包含逐步代码原语的提示结构来弥合训练和推断之间的差距，以推导出新事实并找到解决方案。我们的方法在xSTREET上表现出了改进的多语言性能，尤其是在科学常识推理方面。

    arXiv:2403.02567v1 Announce Type: cross  Abstract: Development of large language models (LLM) have shown progress on reasoning, though studies have been limited to English or simple reasoning tasks. We thus introduce a multilingual structured reasoning and explanation dataset, termed xSTREET, that covers four tasks across six languages. xSTREET exposes a gap in base LLM performance between English and non-English reasoning tasks. We then propose two methods to remedy this gap, building on the insight that LLMs trained on code are better reasoners. First, at training time, we augment a code dataset with multi-lingual comments using machine translation while keeping program code as-is. Second, at inference time, we bridge the gap between training and inference by employing a prompt structure that incorporates step-by-step code primitives to derive new facts and find a solution. Our methods show improved multilingual performance on xSTREET, most notably on the scientific commonsense reaso
    
[^56]: 《手语人工智能研究中的系统偏见：聋人呼吁重新评估研究议程》

    Systemic Biases in Sign Language AI Research: A Deaf-Led Call to Reevaluate Research Agendas

    [https://arxiv.org/abs/2403.02563](https://arxiv.org/abs/2403.02563)

    手语人工智能研究中存在显著偏见，包括过于关注沟通障碍、缺乏代表性数据集、使用缺乏语言基础的注释和基于有缺陷模型的方法，缺乏聋人利益相关者的重要参与。

    

    在手语识别、生成和翻译人工智能领域的研究不断增加的背景下，人们呼吁对这些技术进行道德发展。尽管这些工作对帮助个别研究人员取得更好成果至关重要，但有人指出，这一领域仍然由听力非手语研究人员主导，却鲜有针对系统偏见的讨论或塑造研究问题和方法的修辞分析。因此，我们对手语人工智能领域的101篇最新论文进行系统回顾。我们的分析发现了当前手语人工智能研究中的重大偏见，包括过分关注解决被认为存在的沟通障碍、缺乏使用具有代表性的数据集、使用缺乏语言基础的注释以及基于有缺陷模型构建方法等。我们认为，这一领域缺乏聋人利益相关者的有意义参与，并且更多地受到驱动的是

    arXiv:2403.02563v1 Announce Type: cross  Abstract: Growing research in sign language recognition, generation, and translation AI has been accompanied by calls for ethical development of such technologies. While these works are crucial to helping individual researchers do better, there is a notable lack of discussion of systemic biases or analysis of rhetoric that shape the research questions and methods in the field, especially as it remains dominated by hearing non-signing researchers. Therefore, we conduct a systematic review of 101 recent papers in sign language AI. Our analysis identifies significant biases in the current state of sign language AI research, including an overfocus on addressing perceived communication barriers, a lack of use of representative datasets, use of annotations lacking linguistic foundations, and development of methods that build on flawed models. We take the position that the field lacks meaningful input from Deaf stakeholders, and is instead driven by wh
    
[^57]: 为生成建模研究更新有关临床人工智能（MI-CLAIM）检查表

    Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research

    [https://arxiv.org/abs/2403.02558](https://arxiv.org/abs/2403.02558)

    生成模型的最新进展加速了医学中自然语言和图像处理领域的发展，并标志着生物医学模型开发和部署方式的重大范式转变。

    

    生成模型的最新进展，包括大型语言模型（LLMs）、视觉语言模型（VLMs）和扩散模型，加速了医学中自然语言和图像处理领域的发展，并标志着生物医学模型开发和部署方式的重大范式转变。尽管这些模型非常适应新任务，但在扩展和评估它们的使用过程中出现了前人框架未解决的新挑战。特别是，这些模型以少量或无需专门训练数据即可产生有用输出的能力（“零样本”或“少样本”方法），以及它们输出的开放性质，需要制定更新的使用和评估这些模型的指南。美国行政命令141103确定了有关临床人工智能工具开发的标准和最佳实践存在的差距，以及几个新兴国家临床人工智能评估网络。

    arXiv:2403.02558v1 Announce Type: new  Abstract: Recent advances in generative models, including large language models (LLMs), vision language models (VLMs), and diffusion models, have accelerated the field of natural language and image processing in medicine and marked a significant paradigm shift in how biomedical models can be developed and deployed. While these models are highly adaptable to new tasks, scaling and evaluating their usage presents new challenges not addressed in previous frameworks. In particular, the ability of these models to produce useful outputs with little to no specialized training data ("zero-" or "few-shot" approaches), as well as the open-ended nature of their outputs, necessitate the development of updated guidelines in using and evaluating these models. In response to gaps in standards and best practices for the development of clinical AI tools identified by US Executive Order 141103 and several emerging national networks for clinical AI evaluation, we be
    
[^58]: DACO: 通过代码生成实现应用驱动和全面的数据分析

    DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation

    [https://arxiv.org/abs/2403.02528](https://arxiv.org/abs/2403.02528)

    该论文通过自动生成高质量答案注释的方法，构建了DACO数据集，旨在激发未来对数据分析这一关键且具有挑战性任务的研究。

    

    数据分析是一个关键的分析过程，用于生成深入研究和结论性见解，全面回答给定用户对表格数据的查询。本文旨在提出新的资源和基准，激发未来对这一关键但具有挑战性和未充分挖掘的任务的研究。我们提出了利用LLM的代码生成能力和多轮提示技术自动产生高质量答案注释，构建了DACO数据集，包含440个来自真实场景的数据库（表格数据），约2k个查询-答案对可作为模型训练的弱监督，以及一个人工精细调整的标注的紧凑但高质量测试集，作为我们的主要评估基准。

    arXiv:2403.02528v1 Announce Type: cross  Abstract: Data analysis is a crucial analytical process to generate in-depth studies and conclusive insights to comprehensively answer a given user query for tabular data. In this work, we aim to propose new resources and benchmarks to inspire future research on this crucial yet challenging and under-explored task. However, collecting data analysis annotations curated by experts can be prohibitively expensive. We propose to automatically generate high-quality answer annotations leveraging the code-generation capabilities of LLMs with a multi-turn prompting technique. We construct the DACO dataset, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) ~2k query-answer pairs that can serve as weak supervision for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation benchmark. We train a 6B supervised fine-tuning (SFT) model on DACO datas
    
[^59]: 在增强对话式LLM与直接RLHF的无害性和一般性能之间取得平衡

    Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF

    [https://arxiv.org/abs/2403.02513](https://arxiv.org/abs/2403.02513)

    采用直接实施无害的人类反馈强化学习（RLHF）的创新方法，不仅保留了基本模型的一般能力，还显著增强了其对话能力，同时明显减少了有毒输出的生成

    

    在最近对话式大型语言模型（LLMs）的发展中，出现了一个令人关注的趋势，即许多新的基本LLMs在经过监督微调（SFT）后会经历基础能力的知识减少。这一过程经常导致问题，比如遗忘或基本模型能力的降低。此外，微调模型往往难以与用户偏好保持一致，在特定提示时无意中增加有毒输出的生成。为了克服这些挑战，我们采用了一种创新的方法，即完全绕过SFT并直接实施无害的人类反馈强化学习（RLHF）。我们的方法不仅保留了基本模型的一般能力，还显著增强了其对话能力，同时明显减少了有毒输出的生成。我们的方法对那些需要微妙理解的领域具有重要意义

    arXiv:2403.02513v1 Announce Type: new  Abstract: In recent advancements in Conversational Large Language Models (LLMs), a concerning trend has emerged, showing that many new base LLMs experience a knowledge reduction in their foundational capabilities following Supervised Fine-Tuning (SFT). This process often leads to issues such as forgetting or a decrease in the base model's abilities. Moreover, fine-tuned models struggle to align with user preferences, inadvertently increasing the generation of toxic outputs when specifically prompted. To overcome these challenges, we adopted an innovative approach by completely bypassing SFT and directly implementing Harmless Reinforcement Learning from Human Feedback (RLHF). Our method not only preserves the base model's general capabilities but also significantly enhances its conversational abilities, while notably reducing the generation of toxic outputs. Our approach holds significant implications for fields that demand a nuanced understanding 
    
[^60]: SPUQ：基于扰动的大型语言模型不确定性量化

    SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models

    [https://arxiv.org/abs/2403.02509](https://arxiv.org/abs/2403.02509)

    SPUQ是一种新颖的基于扰动的大型语言模型不确定性量化方法，旨在同时处理现象性和认知性不确定性

    

    近年来，大型语言模型（LLMs）越来越普遍，提供了出色的文本生成能力。然而，它们倾向于做出自信错误的预测，突显了在LLMs中进行不确定性量化（UQ）的重要性。尽管先前的工作主要集中在解决现象性不确定性，但对包括认知性在内的不确定性的全谱尚未充分探索。受到这一差距的启发，我们引入了一种新颖的UQ方法，即适用于UQ的扰动采样（SPUQ），旨在应对现象性和认知性不确定性。该方法涉及为LLM输入生成一组扰动，对每个扰动进行输出采样，并结合一个聚合模块，该聚合模块概括了用于文本生成任务的采样不确定性方法。通过对各种数据集上的广泛实验，我们研究了不同的扰动和聚合方式。

    arXiv:2403.02509v1 Announce Type: cross  Abstract: In recent years, large language models (LLMs) have become increasingly prevalent, offering remarkable text generation capabilities. However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs. While previous works have mainly focused on addressing aleatoric uncertainty, the full spectrum of uncertainties, including epistemic, remains inadequately explored. Motivated by this gap, we introduce a novel UQ method, sampling with perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic uncertainties. The method entails generating a set of perturbations for LLM inputs, sampling outputs for each perturbation, and incorporating an aggregation module that generalizes the sampling uncertainty approach for text generation tasks. Through extensive experiments on various datasets, we investigated different perturbation and aggrega
    
[^61]: 自然语言处理中的预训练-微调范式教程

    A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing

    [https://arxiv.org/abs/2403.02504](https://arxiv.org/abs/2403.02504)

    预训练-微调范式在自然语言处理中展现了显著的效率，尤其对社会科学研究中数据有限的情况下具有益处。

    

    预训练-微调范式代表了自然语言处理中的一种变革性方法。该范式通过使用大型预训练语言模型区别于众，展示了在微调任务中即使训练数据有限也具有显著的效率。这种效率对社会科学研究特别有益，因为注释样本的数量通常非常有限。我们的教程全面介绍了预训练-微调范式。我们首先深入探讨了预训练和微调的基本概念，然后进行了实际应用的案例练习。我们展示了该范式在各种任务中的应用，包括多类别分类和回归。强调其高效性和用户友好性，该教程旨在鼓励更广泛地采纳这种范式。为此，我们提供了所有代码和数据集的开放访问。

    arXiv:2403.02504v1 Announce Type: cross  Abstract: The pretrain-finetune paradigm represents a transformative approach in natural language processing (NLP). This paradigm distinguishes itself through the use of large pretrained language models, demonstrating remarkable efficiency in finetuning tasks, even with limited training data. This efficiency is especially beneficial for research in social sciences, where the number of annotated samples is often quite limited. Our tutorial offers a comprehensive introduction to the pretrain-finetune paradigm. We first delve into the fundamental concepts of pretraining and finetuning, followed by practical exercises using real-world applications. We demonstrate the application of the paradigm across various tasks, including multi-class classification and regression. Emphasizing its efficacy and user-friendliness, the tutorial aims to encourage broader adoption of this paradigm. To this end, we have provided open access to all our code and datasets
    
[^62]: 试错法：面向LLM代理的基于探索的轨迹优化

    Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents

    [https://arxiv.org/abs/2403.02502](https://arxiv.org/abs/2403.02502)

    提出了一种面向LLM代理的基于探索的轨迹优化方法，通过允许代理从探索失败中学习，实现了性能的改进。

    

    大型语言模型（LLMs）已经成为各种自主代理系统中不可或缺的组成部分。在这项研究中，我们提出一种基于探索的轨迹优化方法，称为ETO。这种学习方法旨在提高开放LLM代理的性能。与先前专门训练成功专家轨迹的研究相反，我们的方法允许代理从其探索失败中学习。这通过迭代优化框架实现了性能的改进。在探索阶段，代理与环境互动，完成指定任务，收集失败轨迹以创建对比轨迹对。在随后的训练阶段，代理利用这些轨迹偏好对更新其策略，使用类似DPO的对比学习方法。这种探索和训练的迭代循环促进了代理的持续改进。

    arXiv:2403.02502v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have become integral components in various autonomous agent systems. In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments o
    
[^63]: 选择你的冒险：交互式电子书以提高词汇知识和理解能力

    Choose Your Own Adventure: Interactive E-Books to Improve Word Knowledge and Comprehension Skills

    [https://arxiv.org/abs/2403.02496](https://arxiv.org/abs/2403.02496)

    该研究检验了交互式电子书对三至五年级学生阅读理解和词汇知识的影响，通过采用选择冒险格式和内嵌理解问题的方式来教授单词学习和理解监控策略。

    

    这项可行性研究的目的是检查阅读数字交互式电子书对支持阅读理解的基本技能，特别是对三至五年级学生的潜在影响。学生阅读了两本电子书，教授单词学习和理解监控策略，以学习困难词汇和有关飓风的科学概念。我们调查了特定的理解策略，包括单词学习和支持一般阅读理解的策略，总结和问题生成，这些策略是否在电子书中在建立词汇知识和理解技能方面表现出有效性的潜力。学生被分配阅读三个版本中的一本电子书，每个版本实施一项策略。这些书采用了交互式选择冒险的格式，内嵌理解问题，提供即时反馈。

    arXiv:2403.02496v1 Announce Type: new  Abstract: The purpose of this feasibility study was to examine the potential impact of reading digital interactive e-books on essential skills that support reading comprehension with third-fifth grade students. Students read two e-Books that taught word learning and comprehension monitoring strategies in the service of learning difficult vocabulary and targeted science concepts about hurricanes. We investigated whether specific comprehension strategies including word learning and strategies that supported general reading comprehension, summarization, and question generation, show promise of effectiveness in building vocabulary knowledge and comprehension skills in the e-Books. Students were assigned to read one of three versions of each of the e-Books, each version implemented one strategy. The books employed a choose-your-adventure format with embedded comprehension questions that provided students with immediate feedback on their responses. Pair
    
[^64]: 通过受限直接偏好优化增强LLM安全性

    Enhancing LLM Safety via Constrained Direct Preference Optimization

    [https://arxiv.org/abs/2403.02475](https://arxiv.org/abs/2403.02475)

    通过引入Constrained DPO（C-DPO）方法，我们提出了一种高效且轻量的微调大型语言模型（LLMs）的方法，能有效平衡有用性和安全性之间的权衡，为LLMs提供了安全保障。

    

    大型语言模型（LLMs）的快速增强能力提高了将人工智能系统与不同人类偏好相一致以同时增强其有用性和安全性的迫切需要，尽管这些目标常常相互冲突。为解决这一重要问题，一种有前途的方法是在微调阶段通过受限制的人类反馈强化学习（RLHF）框架施加安全约束。然而，这种方法计算成本高且常常不稳定。本文引入了受限制的DPO（C-DPO），这是对最近提出的直接偏好优化（DPO）方法的一种新颖扩展，用于优化LLMs的微调，具有高效和轻量的特点。通过融合双梯度下降和DPO，我们的方法在不使用强化学习的情况下确定了帮助性和无害性之间的几乎最佳折衷。从经验上看，我们的方法为LLMs提供了安全保障。

    arXiv:2403.02475v1 Announce Type: cross  Abstract: The rapidly increasing capabilities of large language models (LLMs) raise an urgent need to align AI systems with diverse human preferences to simultaneously enhance their usefulness and safety, despite the often conflicting nature of these goals. To address this important problem, a promising approach is to enforce a safety constraint at the fine-tuning stage through a constrained Reinforcement Learning from Human Feedback (RLHF) framework. This approach, however, is computationally expensive and often unstable. In this work, we introduce Constrained DPO (C-DPO), a novel extension of the recently proposed Direct Preference Optimization (DPO) approach for fine-tuning LLMs that is both efficient and lightweight. By integrating dual gradient descent and DPO, our method identifies a nearly optimal trade-off between helpfulness and harmlessness without using reinforcement learning. Empirically, our approach provides a safety guarantee to L
    
[^65]: 文学小说的情感动态

    The Emotion Dynamics of Literary Novels

    [https://arxiv.org/abs/2403.02474](https://arxiv.org/abs/2403.02474)

    该研究使用角色对话来区分文学小说叙述与各角色的情感曲线，发现叙述和对话在表达情感时存在明显差异，个别角色的情感曲线更准确地捕捉到故事情感曲线的共同点或差异。

    

    故事在其叙事中展示的情感丰富，并唤起读者的情感。故事中各个角色的情感变化对其吸引力至关重要。然而，情感的计算分析很少考虑了不同角色之间情感轨迹的变化，而是认为整部小说代表一个单一的故事弧线。在这项工作中，我们使用角色对话来区分叙述和各个角色的情感弧线。我们使用话语情感动态框架分析了一组英文文学小说中各个角色的情感曲线。我们的研究结果表明，叙述与对话在小说中的情感表达在很大程度上是不同的，故事情感曲线的共同点或差异更准确地被与个别ch相关联的情感曲线捕捉到。

    arXiv:2403.02474v1 Announce Type: new  Abstract: Stories are rich in the emotions they exhibit in their narratives and evoke in the readers. The emotional journeys of the various characters within a story are central to their appeal. Computational analysis of the emotions of novels, however, has rarely examined the variation in the emotional trajectories of the different characters within them, instead considering the entire novel to represent a single story arc. In this work, we use character dialogue to distinguish between the emotion arcs of the narration and the various characters. We analyze the emotion arcs of the various characters in a dataset of English literary novels using the framework of Utterance Emotion Dynamics. Our findings show that the narration and the dialogue largely express disparate emotions through the course of a novel, and that the commonalities or differences in the emotional arcs of stories are more accurately captured by those associated with individual ch
    
[^66]: OffLanDat：通过提示工程生成的大型语言模型生成的社区基础隐式攻击性语言数据集

    OffLanDat: A Community Based Implicit Offensive Language Dataset Generated by Large Language Model Through Prompt Engineering

    [https://arxiv.org/abs/2403.02472](https://arxiv.org/abs/2403.02472)

    介绍了一个通过提示工程生成的大型语言模型创建的社区基础隐式攻击性语言数据集OffLanDat，为38个不同目标群体提供数据。

    

    社交媒体上攻击性语言的普遍存在对社会福祉产生了不良影响。因此，有必要高度重视解决这一问题。攻击性语言既存在明确形式，也存在隐式形式，后者更具挑战性。当前在该领域的研究遇到几个挑战。首先，现有数据集主要依赖于收集包含明确攻击性关键词的文本，这使得捕捉不包含这些关键词且隐含攻击性内容的任务具有挑战性。其次，通常的方法论倾向于仅关注文本分析，忽视社区信息可以提供的宝贵见解。在这篇研究论文中，我们介绍了一个新的数据集OffLanDat，这是由ChatGPT生成的基于社区的隐式攻击性语言数据集，其中包含38个不同目标群体的数据。

    arXiv:2403.02472v1 Announce Type: new  Abstract: The widespread presence of offensive languages on social media has resulted in adverse effects on societal well-being. As a result, it has become very important to address this issue with high priority. Offensive languages exist in both explicit and implicit forms, with the latter being more challenging to detect. Current research in this domain encounters several challenges. Firstly, the existing datasets primarily rely on the collection of texts containing explicit offensive keywords, making it challenging to capture implicitly offensive contents that are devoid of these keywords. Secondly, usual methodologies tend to focus solely on textual analysis, neglecting the valuable insights that community information can provide. In this research paper, we introduce a novel dataset OffLanDat, a community based implicit offensive language dataset generated by ChatGPT containing data for 38 different target groups. Despite limitations in genera
    
[^67]: 观点是我的，也是你的：使用共同基础评估心灵理论

    Views Are My Own, But Also Yours: Benchmarking Theory of Mind using Common Ground

    [https://arxiv.org/abs/2403.02451](https://arxiv.org/abs/2403.02451)

    介绍了第一个基于自然口语对话的ToM数据集，展示了LM在ToM方面的困难，并表明在Common-ToM上整合简单明确的信念表示可以提高LM的性能。

    

    最近，评估语言模型（LMs）的心灵理论（ToM）能力引起了很多关注。然而，许多现有的基准测试依赖于合成数据，这可能导致实验结果与人类行为不一致。我们引入了第一个基于自然发生的口头对话的ToM数据集，Common-ToM，并展示LMs在展示ToM方面存在困难。然后，我们展示了在Common-ToM上整合一个简单明确的信念表示可以提高LM的性能。

    arXiv:2403.02451v1 Announce Type: new  Abstract: Evaluating the theory of mind (ToM) capabilities of language models (LMs) has recently received much attention. However, many existing benchmarks rely on synthetic data which risks misaligning the resulting experiments with human behavior. We introduce the first ToM dataset based on naturally occurring spoken dialogs, Common-ToM, and show that LMs struggle to demonstrate ToM. We then show that integrating a simple, explicit representation of beliefs improves LM performance on Common-ToM.
    
[^68]: 建筑如何影响预训练语言模型的基本能力？基于FFN-Wider变压器模型的案例研究

    How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models

    [https://arxiv.org/abs/2403.02436](https://arxiv.org/abs/2403.02436)

    本研究探讨了建筑如何影响预训练语言模型的基本能力，发现了FFN-Wider变压器模型降低了多头注意力的贡献比，从而导致基本能力的下降。

    

    预训练语言模型已被证明具有强大的基本能力，不仅在分布式语言建模方面表现出色，而且在超出分布式语言建模、迁移学习和少样本学习方面也展现出强大的能力。与现有研究侧重于规模对基本能力的影响不同，我们的工作将重点放在了架构对其影响。具体地，我们关心的是：建筑如何影响预训练语言模型的基本能力？在这项工作中，我们试图解释并逆转FFN-Wider变压器的架构导致基本能力下降的情况，力求提供一些见解。通过分析，我们发现多头注意力（一种组合函数）对预训练语言建模的贡献比是影响基本能力的关键因素。FFN-Wider变压器减少了这种组合函数的贡献比，导致一种

    arXiv:2403.02436v1 Announce Type: new  Abstract: Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot learning. Unlike existing work focusing on the influence of scale on base capabilities, our work examines the influence of architecture on those. Specifically, our concern is: How does architecture influence the base capabilities of pre-trained language models? In this work, we attempt to explain and reverse the decline in base capabilities caused by the architecture of FFN-Wider Transformers, seeking to provide some insights. Through analysis, we found the contribution ratio of Multi-Head Attention (a combination function) to pre-trained language modeling is a key factor affecting base capabilities. FFN-Wider Transformers reduce the contribution ratio of this combination function, leading to a d
    
[^69]: 你需要更多LLM调用吗？走向复合推理系统的扩展定律

    Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems

    [https://arxiv.org/abs/2403.02419](https://arxiv.org/abs/2403.02419)

    本文研究了复合推理系统的扩展定律，发现投票推理系统的性能随LLM调用次数增加先增加后下降。

    

    许多最近语言任务中的最先进结果是通过执行多个大型语言模型（LLM）调用并汇总它们的响应的复合系统实现的。然而，对于LLM调用次数的影响 -- 例如，当要求LLM多次回答每个问题并取得共识时 -- 对于这种复合系统的性能了解甚少。在本文中，我们开始研究复合推理系统的扩展定律。我们从理论和实证的角度分析了LLM调用次数如何影响一个层级投票推理系统的性能 -- 这是最简单的复合系统之一，它通过多数投票聚合LLM的响应。我们实验证明，在多个语言任务中，令人惊讶的是，投票推理系统的性能随着LLM调用次数的增加而先增加后下降。我们的理论结果表明，这种非单调性是由于

    arXiv:2403.02419v1 Announce Type: cross  Abstract: Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple Large Language Model (LLM) calls and aggregate their responses. However, there is little understanding of how the number of LLM calls -- e.g., when asking the LLM to answer each question multiple times and taking a consensus -- affects such a compound system's performance. In this paper, we initiate the study of scaling laws of compound inference systems. We analyze, theoretically and empirically, how the number of LLM calls affects the performance of one-layer Voting Inference Systems -- one of the simplest compound systems, which aggregates LLM responses via majority voting. We find empirically that across multiple language tasks, surprisingly, Voting Inference Systems' performance first increases but then decreases as a function of the number of LLM calls. Our theoretical results suggest that this non-monotonicity is due
    
[^70]: NeuroVoz：帕金森病患者语音的卡斯蒂利亚语语料库

    NeuroVoz: a Castillian Spanish corpus of parkinsonian speech

    [https://arxiv.org/abs/2403.02371](https://arxiv.org/abs/2403.02371)

    这一研究提出了一个包含108位母语为卡斯蒂利亚语说话者的帕金森病患者语音语料库，涵盖了多种语音任务，通过手动和自动转录确保了数据的准确性和可靠性。

    

    通过语音分析进行帕金森病（PD）诊断的进展受到公开可用、多样化的语言数据集的显著缺乏的阻碍，限制了现有研究结果的可再现性和进一步探索。为了弥补这一空白，我们引入了一个全面的语料库，包括来自108位母语为卡斯蒂利亚语的说话者，包括55名健康对照组和53名被诊断患有PD的个体，所有这些个体都在药物治疗下，并且在药物优化状态下进行记录。 这一独特数据集涵盖了广泛的语音任务，包括持续发音五个西班牙元音、发音测试、16个听后重复的话语以及自由独白。该数据集通过专家手动转录听后重复任务强调准确性和可靠性，并利用Whisper进行自动独白转录，使其成为帕金森病患者语音的最完整的公开语料库。

    arXiv:2403.02371v1 Announce Type: cross  Abstract: The advancement of Parkinson's Disease (PD) diagnosis through speech analysis is hindered by a notable lack of publicly available, diverse language datasets, limiting the reproducibility and further exploration of existing research.   In response to this gap, we introduce a comprehensive corpus from 108 native Castilian Spanish speakers, comprising 55 healthy controls and 53 individuals diagnosed with PD, all of whom were under pharmacological treatment and recorded in their medication-optimized state. This unique dataset features a wide array of speech tasks, including sustained phonation of the five Spanish vowels, diadochokinetic tests, 16 listen-and-repeat utterances, and free monologues. The dataset emphasizes accuracy and reliability through specialist manual transcriptions of the listen-and-repeat tasks and utilizes Whisper for automated monologue transcriptions, making it the most complete public corpus of Parkinsonian speech, 
    
[^71]: adaptMLLM：在低资源语言上用集成LLM游乐场对多语言语言模型进行微调

    adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with Integrated LLM Playgrounds

    [https://arxiv.org/abs/2403.02370](https://arxiv.org/abs/2403.02370)

    该论文介绍了adaptMLLM，一个旨在解决低资源语言机器翻译问题的开源应用程序，该应用程序简化了对多语言语言模型进行微调的所有流程。

    

    Multilingual Language Models (MLLMs)和Large Language Models的出现，在自然语言处理的许多领域引发了创新。尽管这项技术具有令人兴奋的潜力，但其对低资源语言开发高质量的机器翻译（MT）输出的影响相对较少探讨。此外，尚未推出一个开源应用程序，专门用于对MLLM进行微调，并管理低资源语言的完整MT工作流程。我们旨在通过开发adaptMLLM来解决这些不平衡，该应用程序简化了用于MT的MLLM微调的所有流程。这个开源应用程序专门为从事MT的开发人员、翻译人员和用户量身定制。直观的界面允许轻松地自定义超参数，该应用程序提供一系列用于模型评估的指标，并具有部署模型作为翻译服务的能力。

    arXiv:2403.02370v1 Announce Type: cross  Abstract: The advent of Multilingual Language Models (MLLMs) and Large Language Models has spawned innovation in many areas of natural language processing. Despite the exciting potential of this technology, its impact on developing high-quality Machine Translation (MT) outputs for low-resource languages remains relatively under-explored. Furthermore, an open-source application, dedicated to both fine-tuning MLLMs and managing the complete MT workflow for low-resources languages, remains unavailable. We aim to address these imbalances through the development of adaptMLLM, which streamlines all processes involved in the fine-tuning of MLLMs for MT. This open-source application is tailored for developers, translators, and users who are engaged in MT. An intuitive interface allows for easy customisation of hyperparameters, and the application offers a range of metrics for model evaluation and the capability to deploy models as a translation service 
    
[^72]: adaptNMT：一种面向神经机器翻译的开源、语言无关的开发环境

    adaptNMT: an open-source, language-agnostic development environment for Neural Machine Translation

    [https://arxiv.org/abs/2403.02367](https://arxiv.org/abs/2403.02367)

    adaptNMT是一个开源的神经机器翻译开发环境，简化了模型开发和部署流程，特别适用于新手用户，并提供图形展示、子词分割模型等功能。

    

    arXiv：2403.02367v1 公告类型：跨越 摘要：adaptNMT简化了RNN和Transformer神经翻译模型的开发和部署中涉及的所有流程。作为一款开源应用程序，它旨在面向机器翻译领域的技术和非技术用户。该应用是建立在广泛采用的OpenNMT生态系统之上的，对于新进入该领域的用户特别有用，因为开发环境的设置以及创建训练、验证和测试分割被大大简化。应用程序内置图形展示了模型训练的进度，并使用SentencePiece创建子词分割模型。通过直观的用户界面，可以便捷地定制超参数，实施了一键式模型开发方法。由adaptNMT开发的模型可以使用各种指标进行评估，并作为翻译服务在应用程序中部署。

    arXiv:2403.02367v1 Announce Type: cross  Abstract: adaptNMT streamlines all processes involved in the development and deployment of RNN and Transformer neural translation models. As an open-source application, it is designed for both technical and non-technical users who work in the field of machine translation. Built upon the widely-adopted OpenNMT ecosystem, the application is particularly useful for new entrants to the field since the setup of the development environment and creation of train, validation and test splits is greatly simplified. Graphing, embedded within the application, illustrates the progress of model training, and SentencePiece is used for creating subword segmentation models. Hyperparameter customization is facilitated through an intuitive user interface, and a single-click model development approach has been implemented. Models developed by adaptNMT can be evaluated using a range of metrics, and deployed as a translation service within the application. To support
    
[^73]: 人类评估英爱变压器基于NMT的翻译质量

    Human Evaluation of English--Irish Transformer-Based NMT

    [https://arxiv.org/abs/2403.02366](https://arxiv.org/abs/2403.02366)

    本研究评估了超参数设置对低资源英-爱变压器神经机器翻译质量的影响，并发现优化的Transformer模型在16k BPE子词模型下表现最佳，相较于基准RNN模型提高了7.8个BLEU分数，并在与谷歌翻译的比较中展示出显著的改进。

    

    在本研究中，对超参数设置如何影响低资源英语-爱尔兰文对的变压器神经机器翻译（NMT）质量进行了人类评估。使用Byte Pair Encoding（BPE）和unigram方法的SentencePiece模型受到了评价。模型架构的变化包括修改层数，评估注意力的最佳头数以及测试各种正则化技术。在一个优化了的16k BPE子词模型下，Transformer优化模型的性能表现得最好。与基准循环神经网络（RNN）模型相比，Transformer优化模型的BLEU分数提高了7.8个点。与谷歌翻译相比，我们的翻译引擎展示了显著的改进。此外，进行了定量细粒度的手动评估，比较了机器翻译的性能。

    arXiv:2403.02366v1 Announce Type: cross  Abstract: In this study, a human evaluation is carried out on how hyperparameter settings impact the quality of Transformer-based Neural Machine Translation (NMT) for the low-resourced English--Irish pair. SentencePiece models using both Byte Pair Encoding (BPE) and unigram approaches were appraised. Variations in model architectures included modifying the number of layers, evaluating the optimal number of heads for attention and testing various regularisation techniques. The greatest performance improvement was recorded for a Transformer-optimized model with a 16k BPE subword model. Compared with a baseline Recurrent Neural Network (RNN) model, a Transformer-optimized model demonstrated a BLEU score improvement of 7.8 points. When benchmarked against Google Translate, our translation engines demonstrated significant improvements. Furthermore, a quantitative fine-grained manual evaluation was conducted which compared the performance of machine t
    
[^74]: 使用LLMs提取和规范化产品属性值

    Using LLMs for the Extraction and Normalization of Product Attribute Values

    [https://arxiv.org/abs/2403.02130](https://arxiv.org/abs/2403.02130)

    本文探讨了使用大型语言模型（LLMs）如GPT-3.5和GPT-4从产品标题和描述中提取和规范化属性值的潜力，引入了新的WDC PAVE数据集来支持实验。

    

    在电子商务网站上的产品提供通常包括文本产品标题和文本产品描述。为了提供诸如分面产品过滤或基于内容的产品推荐等功能，网站需要从非结构化产品描述中提取属性值对。本文探讨了使用大型语言模型（LLMs），如OpenAI的GPT-3.5和GPT-4，从产品标题和产品描述中提取和规范化属性值的潜力。为了进行实验，我们引入了WDC产品属性-值提取（WDC PAVE）数据集。WDC PAVE包含来自提供schema.org注释的87个网站的产品提供。这些提供属于五个不同的类别，每个类别都具有一组特定的属性。该数据集以两种形式提供手动验证的属性-值对：（i）直接提取的值和（ii）规范化的属性值。

    arXiv:2403.02130v1 Announce Type: new  Abstract: Product offers on e-commerce websites often consist of a textual product title and a textual product description. In order to provide features such as faceted product filtering or content-based product recommendation, the websites need to extract attribute-value pairs from the unstructured product descriptions. This paper explores the potential of using large language models (LLMs), such as OpenAI's GPT-3.5 and GPT-4, to extract and normalize attribute values from product titles and product descriptions. For our experiments, we introduce the WDC Product Attribute-Value Extraction (WDC PAVE) dataset. WDC PAVE consists of product offers from 87 websites that provide schema.org annotations. The offers belong to five different categories, each featuring a specific set of attributes. The dataset provides manually verified attribute-value pairs in two forms: (i) directly extracted values and (ii) normalized attribute values. The normalization 
    
[^75]: 促进葡萄牙语的开放神经编码器生态系统与Albertina PT*家族

    Fostering the Ecosystem of Open Neural Encoders for Portuguese with Albertina PT* Family

    [https://arxiv.org/abs/2403.01897](https://arxiv.org/abs/2403.01897)

    本文为葡萄牙语的神经编码作出了贡献，扩展了大型语言模型生态系统，并发布了包括亿级参数 Albertina 和 Bertimbau 在内的开源编码器模型，进一步推进了葡萄牙语的神经编码器技术。

    

    为了促进葡萄牙语的神经编码，本文贡献了代表基础编码器模型，代表了一个仍然非常稀缺的针对该语言特别开发的大型语言模型生态系统的扩展，这些模型完全是开放的，即它们是开源的，并在一个开放许可下免费分发，可用于任何目的，包括研究和商业用途。与英语以外的大多数语言一样，葡萄牙语在这些基础语言资源方面资源匮乏，这里有首届拥有 9 亿个参数的 Albertina 和 3.35 亿个参数的 Bertimbau。在以这对模型为首次集合的基础上，我们介绍了最先进的开放式葡萄牙语编码器生态系统的扩展，其中包括一个拥有 15 亿参数的更大型、性能驱动的模型，以及一个拥有 1 亿参数的更小型、效率驱动的模型。在实现这一主要目标的同时，还得到了一些进一步的成果

    arXiv:2403.01897v1 Announce Type: new  Abstract: To foster the neural encoding of Portuguese, this paper contributes foundation encoder models that represent an expansion of the still very scarce ecosystem of large language models specifically developed for this language that are fully open, in the sense that they are open source and openly distributed for free under an open license for any purpose, thus including research and commercial usages. Like most languages other than English, Portuguese is low-resourced in terms of these foundational language resources, there being the inaugural 900 million parameter Albertina and 335 million Bertimbau. Taking this couple of models as an inaugural set, we present the extension of the ecosystem of state-of-the-art open encoders for Portuguese with a larger, top performance-driven model with 1.5 billion parameters, and a smaller, efficiency-driven model with 100 million parameters. While achieving this primary goal, further results that are rele
    
[^76]: NPHardEval4V: 多模态大型语言模型的动态推理基准

    NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models

    [https://arxiv.org/abs/2403.01777](https://arxiv.org/abs/2403.01777)

    这项研究介绍了一个旨在评估多模态大型语言模型推理能力的动态基准NPHardEval4V，发现在推理能力方面不同模型存在显著差异，并揭示了相对于LLMs，MLLMs的推理性能较弱。

    

    理解多模态大型语言模型（MLLMs）的推理能力是一个重要的研究领域。在这项研究中，我们引入了一个动态基准，NPHardEval4V，旨在解决评估MLLM纯粹推理能力方面的现有差距。我们的基准旨在提供一个平台，以解开诸多因素（如图像识别和指令遵循）对模型整体性能的影响，从而专注于评估它们的推理能力。我们的研究发现不同模型在推理能力方面存在显著差异，并突出了相较于LLMs，MLLMs在推理方面表现相对较弱。我们还研究了不同提示样式（包括视觉、文本和结合视觉与文本提示）对MLLM推理能力的影响，展示了多模态输入在模型性能中的不同影响。

    arXiv:2403.01777v1 Announce Type: new  Abstract: Understanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V, aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall performance of the models, allowing us to focus solely on evaluating their reasoning abilities. Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles, including visual, text, and combined vision and text prompts, on the reasoning abilities of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. U
    
[^77]: 朝着全面的越南语检索增强生成和大型语言模型迈进

    Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models

    [https://arxiv.org/abs/2403.01616](https://arxiv.org/abs/2403.01616)

    该论文旨在推动越南语言理解和生成方面的进展，通过开发和分享开放数据集和预训练模型，特别是针对越南语检索增强生成和大型语言模型。

    

    这篇论文通过开发和传播用于越南语检索增强生成（RAG）和大型语言模型（LLMs）的开放数据集和预训练模型，展示了我们在推动越南语言理解和生成水平方面的贡献。

    arXiv:2403.01616v1 Announce Type: new  Abstract: This paper presents our contributions towards advancing the state of Vietnamese language understanding and generation through the development and dissemination of open datasets and pre-trained models for Vietnamese Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs).
    
[^78]: 基于内部表征的上下文锐度作为警报：减少幻觉的一个视角

    In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation

    [https://arxiv.org/abs/2403.01548](https://arxiv.org/abs/2403.01548)

    本研究从内部表征角度深入探讨了大型语言模型幻觉的机制，发现了幻觉的一个显著模式，即在上下文标记的隐藏状态中，正确生成具有更清晰的上下文激活。我们提出了一种基于熵的度量方法，将“锐度”纳入解码过程中，制定了一种受限解码方法，实验证明其在知识寻求和幻觉任务上的有效性。

    

    大型语言模型（LLMs）经常会产生幻觉并产生事实错误，然而我们对它们为什么会犯这些错误的理解仍然有限。在本研究中，我们从内部表征的角度深入探讨LLM幻觉的潜在机制，并发现与幻觉相关的一个突出模式：正确的生成在上下文标记的隐藏状态中具有更清晰的上下文激活，而不正确的生成则没有。利用这一见解，我们提出了一种基于熵的度量来量化上下文隐藏状态之间的“锐度”，并将其纳入解码过程中以制定一种受限解码方法。在各种知识寻求和幻觉基准测试上的实验证明了我们方法的一致有效性，例如，在TruthfulQA上实现了高达8.6点的改进。我们相信这项研究可以提高我们对幻觉的理解。

    arXiv:2403.01548v1 Announce Type: cross  Abstract: Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited. In this study, we delve into the underlying mechanisms of LLM hallucinations from the perspective of inner representations, and discover a salient pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the in-context tokens, compared to the incorrect ones. Leveraging this insight, we propose an entropy-based metric to quantify the ``sharpness'' among the in-context hidden states and incorporate it into the decoding process to formulate a constrained decoding approach. Experiments on various knowledge-seeking and hallucination benchmarks demonstrate our approach's consistent effectiveness, for example, achieving up to an 8.6 point improvement on TruthfulQA. We believe this study can improve our understanding of hallucinat
    
[^79]: 利用生物分子和自然语言的多模态学习：一项综述

    Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey

    [https://arxiv.org/abs/2403.01528](https://arxiv.org/abs/2403.01528)

    生物分子与自然语言相结合的多模态学习为全面表示和分析生物分子开辟了新途径。

    

    集成生物分子建模与自然语言（BL）已经成为人工智能、化学和生物学交叉领域中的一个具有前景的跨学科领域。这种方法利用文本数据源中包含的生物分子的丰富多面描述，增强我们对基本理解，并实现生物分子性质预测等计算任务。通过将自然语言中表达的微妙叙述与通过各种分子建模技术描述的生物分子的结构和功能细节融合，打开了全面表征和分析生物分子的新途径。通过将围绕生物分子的上下文语言数据纳入建模中，BL旨在捕捉包含语言传达的符号特性以及数量化结构特征的整体视图。

    arXiv:2403.01528v1 Announce Type: cross  Abstract: The integration of biomolecular modeling with natural language (BL) has emerged as a promising interdisciplinary area at the intersection of artificial intelligence, chemistry and biology. This approach leverages the rich, multifaceted descriptions of biomolecules contained within textual data sources to enhance our fundamental understanding and enable downstream computational tasks such as biomolecule property prediction. The fusion of the nuanced narratives expressed through natural language with the structural and functional specifics of biomolecules described via various molecular modeling techniques opens new avenues for comprehensively representing and analyzing biomolecules. By incorporating the contextual language data that surrounds biomolecules into their modeling, BL aims to capture a holistic view encompassing both the symbolic qualities conveyed through language as well as quantitative structural characteristics. In this r
    
[^80]: KorMedMCQA: 韩国医疗专业执业考试的多项选择题问答基准

    KorMedMCQA: Multi-Choice Question Answering Benchmark for Korean Healthcare Professional Licensing Examinations

    [https://arxiv.org/abs/2403.01469](https://arxiv.org/abs/2403.01469)

    KorMedMCQA是首个从韩国医疗专业执业考试中衍生的多项选择题问答基准，提供了多种大型语言模型的基线实验结果，并在HuggingFace上公开了数据，为韩国医疗环境中的进一步研究和发展提供了可能性。

    

    我们介绍了KorMedMCQA，这是首个源自韩国医疗专业执业考试的韩语多项选择题问答（MCQA）基准，涵盖了从2012年到2023年的考试内容。该数据集包括医生、护士和药剂师执照考试中的一部分问题，涵盖多种学科。我们对各种大型语言模型进行了基线实验，包括专有/开源、多语言/韩语附加预训练和临床背景预训练模型，突显了进一步增强潜力。我们在HuggingFace上公开了我们的数据，并通过LM-Harness提供了一个评估脚本，邀请在韩国医疗环境中进行进一步探索和发展。

    arXiv:2403.01469v1 Announce Type: new  Abstract: We introduce KorMedMCQA, the first Korean multiple-choice question answering (MCQA) benchmark derived from Korean healthcare professional licensing examinations, covering from the year 2012 to year 2023. This dataset consists of a selection of questions from the license examinations for doctors, nurses, and pharmacists, featuring a diverse array of subjects. We conduct baseline experiments on various large language models, including proprietary/open-source, multilingual/Korean-additional pretrained, and clinical context pretrained models, highlighting the potential for further enhancements. We make our data publicly available on HuggingFace and provide a evaluation script via LM-Harness, inviting further exploration and advancement in Korean healthcare environments.
    
[^81]: LLaMoCo：用于优化代码生成的大型语言模型指令调优

    LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation

    [https://arxiv.org/abs/2403.01131](https://arxiv.org/abs/2403.01131)

    LLaMoCo是第一个旨在以代码对代码方式调整LLMs以解决优化问题的指令调优框架，通过全面指令集和新颖的两阶段学习策略，实现了优越的性能。

    

    最近的研究探讨了使用大型语言模型（LLMs）进行优化，方法包括从LLMs迭代地寻找下一步解决方案，或直接提示LLMs以获取优化器。然而，这些方法存在固有限制，包括操作效率低、对提示设计敏感度高以及缺乏领域特定知识。我们介绍了LLaMoCo，这是第一个旨在以代码对代码方式调整LLMs以解决优化问题的指令调优框架。具体地，我们建立了一个包含清晰描述的问题提示和有效优化代码的全面指令集。然后我们开发了一种新颖的两阶段学习策略，在指令调优阶段之前，该策略整合了基于对比学习的热身过程，以增强模型微调期间的收敛行为。实验结果表明，通过我们的LLaMoCo精调的CodeGen（350M）模型达到了卓越的性能。

    arXiv:2403.01131v1 Announce Type: cross  Abstract: Recent research explores optimization using large language models (LLMs) by either iteratively seeking next-step solutions from LLMs or directly prompting LLMs for an optimizer. However, these approaches exhibit inherent limitations, including low operational efficiency, high sensitivity to prompt design, and a lack of domain-specific knowledge. We introduce LLaMoCo, the first instruction-tuning framework designed to adapt LLMs for solving optimization problems in a code-to-code manner. Specifically, we establish a comprehensive instruction set containing well-described problem prompts and effective optimization codes. We then develop a novel two-phase learning strategy that incorporates a contrastive learning-based warm-up procedure before the instruction-tuning phase to enhance the convergence behavior during model fine-tuning. The experiment results demonstrate that a CodeGen (350M) model fine-tuned by our LLaMoCo achieves superior 
    
[^82]: 梯度被罚：通过探索拒绝损失地形图来检测针对大语言模型的越狱攻击

    Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes

    [https://arxiv.org/abs/2403.00867](https://arxiv.org/abs/2403.00867)

    本文提出了一种名为Gradient Cuff的方法，通过探索拒绝损失地形图来检测对大语言模型的越狱攻击，成功设计了一种有效的两步检测策略。

    

    大型语言模型（LLMs）正成为一种突出的生成式AI工具，用户输入查询，LLM生成答案。为了减少伤害和滥用，人们通过使用先进的训练技术如来自人类反馈的强化学习（RLHF）来将这些LLMs与人类价值观保持一致。然而，最近的研究突显了LLMs对于试图颠覆嵌入的安全防护措施的对抗性越狱尝试的脆弱性。为了解决这一挑战，本文定义并调查了LLMs的拒绝损失，然后提出了一种名为Gradient Cuff的方法来检测越狱尝试。Gradient Cuff利用拒绝损失地形图中观察到的独特特性，包括功能值及其光滑性，设计了一种有效的两步检测策略。

    arXiv:2403.00867v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are becoming a prominent generative AI tool, where the user enters a query and the LLM generates an answer. To reduce harm and misuse, efforts have been made to align these LLMs to human values using advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF). However, recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails. To address this challenge, this paper defines and investigates the Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect jailbreak attempts. Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy. Experimental results on two aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN,
    
[^83]: CLLMs: 一致性大型语言模型

    CLLMs: Consistency Large Language Models

    [https://arxiv.org/abs/2403.00835](https://arxiv.org/abs/2403.00835)

    提出了一种新方法，通过精细调整目标LLM实现了对雅各比轨迹上固定点的一致性预测，有效提高了生成速度2.4倍到3.4倍。

    

    并行解码方法，如雅可比解码，显示出有望实现更高效的LLM推断，因为它打破了LLM解码过程的顺序性，并将其转换为可并行化计算。然而，在实践中，与传统的自回归（AR）解码相比，雅可比解码很少能在单个固定点迭代步骤中准确预测多个标记，因此在速度上取得的提升相对较小。为了解决这个问题，我们开发了一种新方法，旨在实现从任何状态快速收敛到雅各比轨迹上的固定点。通过精细调整目标LLM，以便在任何输入状态下一致地预测固定点。大量实验证明了我们方法的有效性，在领域特定和开放域基准测试中显示出生成速度提高了2.4倍到3.4倍，同时保持了生成质量。

    arXiv:2403.00835v1 Announce Type: cross  Abstract: Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\times$ to 3.4$\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.
    
[^84]: DenseMamba: 具有密集隐藏连接的状态空间模型，用于高效大型语言模型

    DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models

    [https://arxiv.org/abs/2403.00818](https://arxiv.org/abs/2403.00818)

    DenseSSM是一种新方法，通过密集连接增强了状态空间模型(SSM)，有效地提升了各层之间隐藏信息的流动，在保持训练并行性和推理效率的同时，取得了显著的性能提升。

    

    大型语言模型(LLMs)面临着由普遍使用的Transformer架构过高的计算和内存需求而带来的巨大挑战。而状态空间模型(SSM)是一种新型基础网络架构，具有较低的计算复杂度，但其性能尚未完全能与Transformer相媲美。本文引入了DenseSSM，一种增强SSMs中各层之间隐藏信息流动的新方法。通过有选择地将浅层隐藏状态集成到更深层，DenseSSM保留了对最终输出至关重要的细粒度信息。密集连接增强的DenseSSM仍保持了训练的并行性和推理效率。该方法可以广泛适用于RetNet和Mamba等各种SSM类型。在相似的模型大小下，DenseSSM取得了显著的改进，例如DenseRetNet比原始RetNet提高了高达5%的准确率。

    arXiv:2403.00818v1 Announce Type: new  Abstract: Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% ac
    
[^85]: 形态变异：通过隐喻视觉叙事进行互动、情感和创造性梦境叙述

    Metamorpheus: Interactive, Affective, and Creative Dream Narration Through Metaphorical Visual Storytelling

    [https://arxiv.org/abs/2403.00632](https://arxiv.org/abs/2403.00632)

    本研究提出了Metamorpheus，一种情感接口，通过创造性的视觉叙事来参与用户在梦境中的情感经历，利用生成式人工智能生成视觉隐喻和文本描绘，促使自我反思。

    

    人类的情感基本上是由生活经验塑造的，我们从中构建个性化意义。参与这种意义塑造过程已经被作为各种心理治疗中的一种干预来促进健康。然而，支持在日常生活中回忆和叙述生活经验仍然在人机交互领域鲜有探讨。如何利用生成式人工智能模型等技术来促进意义塑造过程，最终支持情感正念仍然是未知的。在本文中，我们呈现了Metamorpheus，一种情感接口，通过创造性的视觉叙事来参与用户在梦境中的情感经历。Metamorpheus根据梦境的情感弧线排列故事情节，并通过创造隐喻图像和文本描绘来促使自我反思。该系统提供隐喻建议，并利用生成式人工智能生成视觉隐喻和文本描绘。

    arXiv:2403.00632v1 Announce Type: cross  Abstract: Human emotions are essentially molded by lived experiences, from which we construct personalised meaning. The engagement in such meaning-making process has been practiced as an intervention in various psychotherapies to promote wellness. Nevertheless, to support recollecting and recounting lived experiences in everyday life remains under explored in HCI. It also remains unknown how technologies such as generative AI models can facilitate the meaning making process, and ultimately support affective mindfulness. In this paper we present Metamorpheus, an affective interface that engages users in a creative visual storytelling of emotional experiences during dreams. Metamorpheus arranges the storyline based on a dream's emotional arc, and provokes self-reflection through the creation of metaphorical images and text depictions. The system provides metaphor suggestions, and generates visual metaphors and text depictions using generative AI m
    
[^86]: 基于人类阅读过程的多跳问题回答中促进显式和隐式知识

    Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process

    [https://arxiv.org/abs/2402.19350](https://arxiv.org/abs/2402.19350)

    该研究引入了一个促进显式和隐式知识的框架，用于多跳问题回答，从人类阅读过程的角度连接输入文段和预训练知识。

    

    预训练语言模型（PLMs）利用思维链（CoT）模拟人类推理和推断过程，实现了在多跳QA方面高效的性能。然而，当处理复杂问题时，PLMs的推理能力和人类之间仍存在差距。心理学研究表明，在阅读过程中，输入文段中的显式信息与人类先验知识之间存在重要联系。然而，当前的研究未能充分关注从人类认知研究的角度链接输入文段和基于PLMs预训练知识。在本研究中，我们引入了一个促进显式和隐式知识（PEI）框架，使用提示连接显式和隐式知识，与人类阅读过程对齐，用于多跳QA。我们将输入文段视为显式知识，利用它们通过统一提示推导隐式知识。

    arXiv:2402.19350v1 Announce Type: new  Abstract: Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to simulate human reasoning and inference processes, achieving proficient performance in multi-hop QA. However, a gap persists between PLMs' reasoning abilities and those of humans when tackling complex problems. Psychological studies suggest a vital connection between explicit information in passages and human prior knowledge during reading. Nevertheless, current research has given insufficient attention to linking input passages and PLMs' pre-training-based knowledge from the perspective of human cognition studies. In this study, we introduce a \textbf{P}rompting \textbf{E}xplicit and \textbf{I}mplicit knowledge (PEI) framework, which uses prompts to connect explicit and implicit knowledge, aligning with human reading process for multi-hop QA. We consider the input passages as explicit knowledge, employing them to elicit implicit knowledge through unified prompt reason
    
[^87]: WanJuan-CC：一个安全且高质量的开源英文网络文本数据集

    WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset

    [https://arxiv.org/abs/2402.19282](https://arxiv.org/abs/2402.19282)

    WanJuan-CC是一个安全高质量的开源英文网络文本数据集，通过处理大规模的Common Crawl数据并经过多项筛选和过滤步骤得到，为语言模型的预训练提供了重要资源。

    

    本文介绍了 WanJuan-CC，这是一个安全且高质量的开源英文网络文本数据集，来源于Common Crawl数据。研究解决了为语言模型构建大规模预训练数据集所面临的挑战，这需要大量高质量数据。设计了一个全面的流程来处理Common Crawl数据，包括提取、启发式规则过滤、模糊去重、内容安全过滤和数据质量过滤。从大约680亿个原始英文文档中，我们获得了22万亿标记的安全数据，并从中选出了10万亿标记的高质量数据作为WanJuan-CC的一部分。我们已经开源了这个数据集中的3000亿标记。该论文还提供了与数据质量相关的统计信息，使用户可以根据自己的需求选择适当的数据。为评估数据集的质量和实用性，我们使用WanJuan-CC训练了10亿参数和30亿参数的模型。

    arXiv:2402.19282v1 Announce Type: new  Abstract: This paper presents WanJuan-CC, a safe and high-quality open-sourced English webtext dataset derived from Common Crawl data. The study addresses the challenges of constructing large-scale pre-training datasets for language models, which require vast amounts of high-quality data. A comprehensive process was designed to handle Common Crawl data, including extraction, heuristic rule filtering, fuzzy deduplication, content safety filtering, and data quality filtering. From approximately 68 billion original English documents, we obtained 2.22T Tokens of safe data and selected 1.0T Tokens of high-quality data as part of WanJuan-CC. We have open-sourced 300B Tokens from this dataset. The paper also provides statistical information related to data quality, enabling users to select appropriate data according to their needs. To evaluate the quality and utility of the dataset, we trained 1B-parameter and 3B-parameter models using WanJuan-CC and ano
    
[^88]: 通过长文本编码器提升罗马尼亚法律判决预测的能力

    Improving Legal Judgement Prediction in Romanian with Long Text Encoders

    [https://arxiv.org/abs/2402.19170](https://arxiv.org/abs/2402.19170)

    本研究关注通过扩展Transformer模型的序列长度来更好理解法律语料库中的长文档，并在罗马尼亚的4个LJP数据集上进行了广泛实验。

    

    最近几年，自然语言处理（NLP）领域取得了惊人的新成果，在各种任务上实现了接近人类水平的性能。法律NLP领域也随之发展迅猛。然而，通用模型并不直接适用于法律领域。由于其专业词汇、长文档等特点，法律NLP通常需要特定模型和方法。本文研究了专业和通用模型用于预测法律案例的最终裁决的方法，即法律判决预测（LJP）任务。我们特别关注如何扩展基于Transformer模型的序列长度，以更好地理解法律语料库中的长文档。在来自两个来源、规模和文档长度显著不同时的4个罗马尼亚LJP数据集上进行了大量实验，结果显示专门模型...

    arXiv:2402.19170v1 Announce Type: cross  Abstract: In recent years,the entire field of Natural Language Processing (NLP) has enjoyed amazing novel results achieving almost human-like performance on a variety of tasks. Legal NLP domain has also been part of this process, as it has seen an impressive growth. However, general-purpose models are not readily applicable for legal domain. Due to the nature of the domain (e.g. specialized vocabulary, long documents) specific models and methods are often needed for Legal NLP. In this work we investigate both specialized and general models for predicting the final ruling of a legal case, task known as Legal Judgment Prediction (LJP). We particularly focus on methods to extend to sequence length of Transformer-based models to better understand the long documents present in legal corpora. Extensive experiments on 4 LJP datasets in Romanian, originating from 2 sources with significantly different sizes and document lengths, show that specialized mo
    
[^89]: 推动葡萄牙语生成人工智能与开放解码器Gerv\'asio PT*

    Advancing Generative AI for Portuguese with Open Decoder Gerv\'asio PT*

    [https://arxiv.org/abs/2402.18766](https://arxiv.org/abs/2402.18766)

    提出了一种葡萄牙语生成人工智能的开放解码器模型Gerv\'asio PT*，创造了新的技术水平，促进葡萄牙语言技术研究和创新。

    

    为了推进葡萄牙语的神经解码，本文提出了一种全新的基于Transformer的、经过指令调整的开放解码器模型，从这方面创造了新的技术水平。为了开发这个解码器，我们使用了一个强大的LLaMA~27B模型作为起点，并通过进一步训练对包括为此目的准备的新葡萄牙语指令数据集在内的语言资源进行改进，这些数据集也在本文中提供。所有版本的Gerv\'asio都是开源的，可以免费使用，并可以在消费级硬件上运行，旨在促进葡萄牙语言技术研究和创新的发展。

    arXiv:2402.18766v1 Announce Type: new  Abstract: To advance the neural decoding of Portuguese, in this paper we present a fully open Transformer-based, instruction-tuned decoder model that sets a new state of the art in this respect. To develop this decoder, which we named Gerv\'asio PT*, a strong LLaMA~2 7B model was used as a starting point, and its further improvement through additional training was done over language resources that include new instruction data sets of Portuguese prepared for this purpose, which are also contributed in this paper. All versions of Gerv\'asio are open source and distributed for free under an open license, including for either research or commercial usage, and can be run on consumer-grade hardware, thus seeking to contribute to the advancement of research and innovation in language technology for Portuguese.
    
[^90]: 基于大型语言模型的代理平台上的前景个性化推荐

    Prospect Personalized Recommendation on Large Language Model-based Agent Platform

    [https://arxiv.org/abs/2402.18240](https://arxiv.org/abs/2402.18240)

    提出了一种基于大型语言模型代理平台的个性化推荐系统Rec4Agentverse，强调代理项和代理推荐器之间的合作，促进个性化信息服务，提升信息交换，并展望了其演进为支持互动和信息交换的三个阶段

    

    新型代理导向信息系统，以GPT为例，促使我们审视信息系统基础设施，以支持代理级信息处理并适应基于大型语言模型（LLM）的代理的特征，如互动性。本研究展望了基于LLM代理平台的推荐系统的前景，并介绍了一种称为Rec4Agentverse的新型推荐范式，包括代理项和代理推荐器。Rec4Agentverse强调代理项和代理推荐器之间的合作，从而促进个性化信息服务，并增强信息交换，超越传统的用户-推荐器反馈循环。此外，我们展望了Rec4Agentverse的演进，并将其概念化为基于代理项、代理推荐器和用户之间互动和信息交换增强的三个阶段。

    arXiv:2402.18240v1 Announce Type: cross  Abstract: The new kind of Agent-oriented information system, exemplified by GPTs, urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of Large Language Model (LLM)-based Agents, such as interactivity. In this work, we envisage the prospect of the recommender system on LLM-based Agent platforms and introduce a novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items and Agent Recommender. Rec4Agentverse emphasizes the collaboration between Agent Items and Agent Recommender, thereby promoting personalized information services and enhancing the exchange of information beyond the traditional user-recommender feedback loop. Additionally, we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent Recommender, and the user. A pre
    
[^91]: PALO：一个针对50亿人的多语言多模态模型

    PALO: A Polyglot Large Multimodal Model for 5B People

    [https://arxiv.org/abs/2402.14818](https://arxiv.org/abs/2402.14818)

    该文介绍了一个名为PALO的大型多语种多模态模型，实现了对10种主要语言的视觉推理能力，涵盖了约50亿人口。其通过半自动化翻译方法，将多语言多模态数据集从英语翻译为目标语言，以提升跨多种语言的性能。

    

    本研究旨在推动更具包容性的视觉-语言模型(VLMs)，引入了一个名为\Palo 的大型多语种多模态模型。Palo 在包括英语、中文、印地语、西班牙语、法语、阿拉伯语、孟加拉语、俄语、乌尔都语和日语在内的10种主要语言中提供视觉推理能力，涵盖了约50亿人口（全球人口的65%）。我们采用半自动化翻译方法，利用经过微调的大型语言模型，将多模态指导数据集从英语翻译到目标语言，从而确保了较高的语言保真度，同时由于减少了手动工作，使可扩展性更强。引入多样化的指导集有助于提升跨多种语言的总体性能，特别是对那些欠代表的语言如印地语、阿拉伯语、孟加拉语和乌尔都语。最终的模型在三个规模（17B、70B和130B参数）上进行训练，以展示其泛用性能

    arXiv:2402.14818v1 Announce Type: new  Abstract: In pursuit of more inclusive Vision-Language Models (VLMs), this study introduces a Large Multilingual Multimodal Model called \textsc{Palo}. \textsc{Palo} offers visual reasoning capabilities in 10 major languages, including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese, that span a total of $\sim$5B people (65\% of the world population). Our approach involves a semi-automated translation approach to adapt the multimodal instruction dataset from English to the target languages using a fine-tuned Large Language Model, thereby ensuring high linguistic fidelity while allowing scalability due to minimal manual effort. The incorporation of diverse instruction sets helps us boost overall performance across multiple languages especially those that are underrepresented like Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three scales (1.7B, 7B and 13B parameters) to show the gen
    
[^92]: VLSP 2023综述--ComOM任务：越南产品评论的比较意见挖掘数据挑战

    Overview of the VLSP 2023 -- ComOM Shared Task: A Data Challenge for Comparative Opinion Mining from Vietnamese Product Reviews

    [https://arxiv.org/abs/2402.13613](https://arxiv.org/abs/2402.13613)

    该论文总结了VLSP 2023中ComOM任务的一个数据挑战，旨在推动自然语言处理领域通过开发从越南产品评论中提取比较意见的技术，参与者需提出能够提取比较"五元组"的模型并根据F1分数进行评估排名。

    

    本文提供了越南语产品评论比较意见挖掘共享任务（ComOM）的综合概述，该任务作为第十届越南语言和语音处理国际研讨会（VLSP 2023）的一部分举行。此共享任务的主要目标是通过开发能够有效从越南产品评论中提取比较意见的技术来推动自然语言处理领域的发展。参与者被挑战提出能够从比较句中熟练提取比较“五元组”的模型，包括主题、客体、方面、谓词和比较类型标签。我们构建了一个包含120个文档的人工标记数据集，其中包括7427个非比较句和1798个句子中的2468个比较。参与的模型将根据准确匹配宏平均的五元组F1分数进行评估和排名。

    arXiv:2402.13613v1 Announce Type: new  Abstract: This paper presents a comprehensive overview of the Comparative Opinion Mining from Vietnamese Product Reviews shared task (ComOM), held as part of the 10$^{th}$ International Workshop on Vietnamese Language and Speech Processing (VLSP 2023). The primary objective of this shared task is to advance the field of natural language processing by developing techniques that proficiently extract comparative opinions from Vietnamese product reviews. Participants are challenged to propose models that adeptly extract a comparative "quintuple" from a comparative sentence, encompassing Subject, Object, Aspect, Predicate, and Comparison Type Label. We construct a human-annotated dataset comprising $120$ documents, encompassing $7427$ non-comparative sentences and $2468$ comparisons within $1798$ sentences. Participating models undergo evaluation and ranking based on the Exact match macro-averaged quintuple F1 score.
    
[^93]: Mafin: 用模型增强微调来增强黑盒嵌入

    Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning

    [https://arxiv.org/abs/2402.12177](https://arxiv.org/abs/2402.12177)

    Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。

    

    检索增强生成（RAG）已经成为缓解大型语言模型（LLMs）中幻觉的有效解决方案。RAG中的检索阶段通常涉及预训练的嵌入模型，将查询和段落转换为向量以捕获它们的语义。然而，当应用于特定领域知识时，标准的预训练嵌入模型可能表现出次优性能，需要进行微调。本文解决了仅能从黑盒模型获取嵌入的情况。我们引入了模型增强微调（Mafin）--一种通过用可训练的嵌入模型增强黑盒嵌入模型来进行微调的新方法。我们的结果表明，Mafin仅需要训练一个小的增强模型就可以显著提高黑盒嵌入的性能。我们验证了我们的方法在有标签和无标签数据集上的有效性。

    arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
    
[^94]: ASGEA：利用Align-Subgraphs中的逻辑规则进行实体对齐

    ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment

    [https://arxiv.org/abs/2402.11000](https://arxiv.org/abs/2402.11000)

    提出了一个新的实体对齐框架ASGEA，利用Align-Subgraphs中的逻辑规则，设计了可解释的基于路径的图神经网络ASGNN，引入了多模态注意机制，取得了令人满意的实验结果

    

    实体对齐（EA）旨在识别代表相同现实世界对象的不同知识图中的实体。最近基于嵌入的EA方法在EA方面取得了最先进的性能，但面临着解释性挑战，因为它们完全依赖于嵌入距离，并忽视了一对对齐实体背后的逻辑规则。在本文中，我们提出了Align-Subgraph实体对齐（ASGEA）框架来利用Align-Subgraphs中的逻辑规则。ASGEA使用锚链接作为桥梁来构建Align-Subgraphs，并沿着跨知识图的路径传播，这使其区别于基于嵌入的方法。此外，我们设计了一种可解释的基于路径的图神经网络ASGNN，以有效识别和整合跨知识图的逻辑规则。我们还引入了一个节点级多模态注意机制，结合多模态增强的锚点来增强Align-Subgraph。我们的实验结果

    arXiv:2402.11000v1 Announce Type: cross  Abstract: Entity alignment (EA) aims to identify entities across different knowledge graphs that represent the same real-world objects. Recent embedding-based EA methods have achieved state-of-the-art performance in EA yet faced interpretability challenges as they purely rely on the embedding distance and neglect the logic rules behind a pair of aligned entities. In this paper, we propose the Align-Subgraph Entity Alignment (ASGEA) framework to exploit logic rules from Align-Subgraphs. ASGEA uses anchor links as bridges to construct Align-Subgraphs and spreads along the paths across KGs, which distinguishes it from the embedding-based methods. Furthermore, we design an interpretable Path-based Graph Neural Network, ASGNN, to effectively identify and integrate the logic rules across KGs. We also introduce a node-level multi-modal attention mechanism coupled with multi-modal enriched anchors to augment the Align-Subgraph. Our experimental results 
    
[^95]: DoRA: 分解权重的低秩适应

    DoRA: Weight-Decomposed Low-Rank Adaptation

    [https://arxiv.org/abs/2402.09353](https://arxiv.org/abs/2402.09353)

    DoRA是一种将预训练权重分解为幅度和方向两个组成部分，并使用LoRA进行方向更新的低秩适应方法，通过增强学习能力和训练稳定性来提高对LLaMA，LLaVA和VL-B的微调性能。

    

    在广泛使用的参数高效调整（PEFT）方法中，由于避免了额外的推理成本，LoRA及其变种方法因此变得非常流行。然而，这些方法与完全微调（FT）之间仍然存在精度差距。在这项工作中，我们首先引入了一种新颖的权重分解分析方法来研究FT和LoRA之间的内在差异。为了模拟FT的学习能力，我们提出了一种称为DoRA的权重分解低秩适应方法。DoRA将预训练的权重分解为两个组成部分，幅度和方向，并且具体使用LoRA进行方向更新，以有效地减少可训练参数的数量。通过使用DoRA，我们增强了LoRA的学习能力和训练稳定性，同时避免了任何额外的推理开销。在微调LLaMA，LLaVA和VL-B上，DoRA始终优于LoRA。

    arXiv:2402.09353v1 Announce Type: new Abstract: Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-B
    
[^96]: 使基于流匹配的零样本文本到语音系统自由地产生笑声

    Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like

    [https://arxiv.org/abs/2402.07383](https://arxiv.org/abs/2402.07383)

    本文提出了ELaTE，一种基于流匹配的零样本文本到语音系统，可以根据短音频提示以精确控制笑声时机和表情生成任何说话者的自然笑声。

    

    笑声是人类语音中最表达性和自然的一部分，传达着情感、社交暗示和幽默。然而，大多数文本到语音(TTS)系统缺乏产生逼真且合适的笑声的能力，限制了其应用和用户体验。虽然之前有工作生成了自然的笑声，但在控制生成的笑声的时机和多样性方面仍存在不足。在这项工作中，我们提出了ELaTE，一种可以基于短音频提示以精确控制笑声时机和表情的零样本TTS系统，可以产生任何说话者的自然笑声。具体而言，ELaTE通过音频提示来模仿声音特征，通过文本提示来指示所生成语音的内容，通过输入来控制笑声表情，可以是笑声的起始和结束时间，或包含要模仿的笑声的另外音频提示。我们的模型基于找到的技术基础进行了开发。

    Laughter is one of the most expressive and natural aspects of human speech, conveying emotions, social cues, and humor. However, most text-to-speech (TTS) systems lack the ability to produce realistic and appropriate laughter sounds, limiting their applications and user experience. While there have been prior works to generate natural laughter, they fell short in terms of controlling the timing and variety of the laughter to be generated. In this work, we propose ELaTE, a zero-shot TTS that can generate natural laughing speech of any speaker based on a short audio prompt with precise control of laughter timing and expression. Specifically, ELaTE works on the audio prompt to mimic the voice characteristic, the text prompt to indicate the contents of the generated speech, and the input to control the laughter expression, which can be either the start and end times of laughter, or the additional audio prompt that contains laughter to be mimicked. We develop our model based on the foundati
    
[^97]: 从PARIS到LE-PARIS：通过推荐系统和协作大型语言模型实现专利响应自动化

    From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models

    [https://arxiv.org/abs/2402.00421](https://arxiv.org/abs/2402.00421)

    本研究介绍了专利响应智能系统PARIS和LE-PARIS，通过构建OA主题数据库、开发响应模板以及实施推荐系统和基于LLM的响应生成，旨在加快专利律师处理审查意见回应的效率。 通过多范式分析和长期数据验证，证明了OA主题的建设性和LLM对于回应自动生成的可行性。

    

    在专利审查中，对于及时和有效地回应审查意见（OAs）对于获得专利至关重要，然而过去的自动化和人工智能研究很少涉及到这一方面。为了弥补这一空白，我们的研究介绍了专利审查意见响应智能系统（PARIS）及其先进版本LE-PARIS。这些系统旨在加快专利律师在协作处理OA回应方面的效率。系统的关键特征包括构建OA主题数据库，开发响应模板，以及实施推荐系统和基于LLM的响应生成。我们的验证涉及使用USPTO Office Action数据库和律师与我们系统的长期交互数据进行的多范式分析，为期六年。通过五个研究，我们利用主题建模和提出的Delphi过程来检验OA主题的建设性（研究1和2），还有使用推荐系统和基于LLM的响应生成来提高回应质量（研究3和4），以及经过训练的LLM对于回应自动生成的可行性（研究5）。

    In patent prosecution, timely and effective responses to Office Actions (OAs) are crucial for acquiring patents, yet past automation and AI research have scarcely addressed this aspect. To address this gap, our study introduces the Patent Office Action Response Intelligence System (PARIS) and its advanced version, the Large Language Model Enhanced PARIS (LE-PARIS). These systems are designed to expedite the efficiency of patent attorneys in collaboratively handling OA responses. The systems' key features include the construction of an OA Topics Database, development of Response Templates, and implementation of Recommender Systems and LLM-based Response Generation. Our validation involves a multi-paradigmatic analysis using the USPTO Office Action database and longitudinal data of attorney interactions with our systems over six years. Through five studies, we examine the constructiveness of OA topics (studies 1 and 2) using topic modeling and the proposed Delphi process, the efficacy of
    
[^98]: LoRAMoE: 通过MoE风格的插件缓解大型语言模型中的世界知识遗忘

    LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin

    [https://arxiv.org/abs/2312.09979](https://arxiv.org/abs/2312.09979)

    LoRAMoE是一个新颖的框架，通过引入低秩适配器和路由器网络，类似于MoE的插件版本，来解决大型语言模型中世界知识遗忘的问题。

    

    监督微调（SFT）是大型语言模型（LLMs）的关键步骤，使它们能够与人类指令对齐，并增强它们在下游任务中的能力。我们发现，指令数据的大规模增加可能会破坏LLMs先前存储的世界知识。为了解决这一挑战，我们提出了LoRAMoE，这是一个引入了多个低秩适配器（LoRA）并通过路由器网络集成它们的创新性框架，类似于专家混合（MoE）的插件版本。

    arXiv:2312.09979v3 Announce Type: replace  Abstract: Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Increasing instruction data substantially is a direct solution to align the model with a broader range of downstream tasks or notably improve its performance on a specific task. However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs. To address this challenge, we propose LoRAMoE, a novelty framework that introduces several low-rank adapters (LoRA) and integrates them by using a router network, like a plugin version of Mixture of Experts (MoE). It freezes the backbone model and forces a portion of LoRAs to focus on leveraging world knowledge to solve downstream tasks, to alleviate world knowledge-edge forgetting. Experimental results show that, as the instruction data increases, LoRAMoE can si
    
[^99]: RDR：增强语言理解的回顾、审慎和回应方法

    RDR: the Recap, Deliberate, and Respond Method for Enhanced Language Understanding

    [https://arxiv.org/abs/2312.09932](https://arxiv.org/abs/2312.09932)

    RDR方法提出了通过回顾、审慎和回应三个目标来增强语言理解的神经网络管道，解决了神经模型操纵NLU基准测试的问题

    

    自然语言理解（NLU）使用神经网络管道通常需要额外的上下文，而这些上下文并不仅仅存在于输入数据中。通过先前的研究，已经明显地证明了NLU基准测试对神经模型的操纵敏感，这些模型利用编码的外部知识中的统计特征人为地提高了下游任务的性能指标。我们提出的方法称为回顾、审慎和回应（RDR）范式，通过在神经网络管道中引入三个不同的目标来解决这个问题。首先，回顾目标涉及使用一个释义模型对输入文本进行释义，以便对其进行总结和概括。其次，审慎目标涉及利用图嵌入模型对与输入文本中提到的实体相关的外部图信息进行编码。最后，回应目标e

    arXiv:2312.09932v2 Announce Type: replace-cross  Abstract: Natural language understanding (NLU) using neural network pipelines often requires additional context that is not solely present in the input data. Through Prior research, it has been evident that NLU benchmarks are susceptible to manipulation by neural models, wherein these models exploit statistical artifacts within the encoded external knowledge to artificially inflate performance metrics for downstream tasks. Our proposed approach, known as the Recap, Deliberate, and Respond (RDR) paradigm, addresses this issue by incorporating three distinct objectives within the neural network pipeline. Firstly, the Recap objective involves paraphrasing the input text using a paraphrasing model in order to summarize and encapsulate its essence. Secondly, the Deliberation objective entails encoding external graph information related to entities mentioned in the input text, utilizing a graph embedding model. Finally, the Respond objective e
    
[^100]: 高等教育中的生成人工智能：通过大学的政策、资源和指南了解ChatGPT

    Generative AI in Higher Education: Seeing ChatGPT Through Universities' Policies, Resources, and Guidelines

    [https://arxiv.org/abs/2312.05235](https://arxiv.org/abs/2312.05235)

    通过分析美国前100名大学制定的学术政策和指南，揭示了大多数大学对于在高等教育中整合生成人工智能的开放但谨慎态度，主要关注点在于伦理使用、准确性和数据隐私。

    

    生成人工智能（GenAI）技术的进步（如ChatGPT）为丰富教育经验提供了机会，但如果被滥用，也会引发有关学术诚信的担忧。该研究旨在通过分析美国排名前100的大学制定的学术政策和指南来探讨大学和教育者如何在其学术背景中对GenAI的发展做出响应和适应。数据来源包括这些大学制定的学术政策、声明、指南以及相关资源。结果显示，大多数大学对于整合GenAI采取了开放但谨慎的态度。主要关注点在于伦理使用、准确性和数据隐私。大多数大学积极回应并提供多种资源，如课程大纲模板/示例、研讨会、共享文章等。

    arXiv:2312.05235v2 Announce Type: replace  Abstract: The advancements in Generative Artificial Intelligence (GenAI) technologies such as ChatGPT provide opportunities to enrich educational experiences, but also raise concerns about academic integrity if misused. This study aims to explore how universities and educators respond and adapt to the development of GenAI in their academic contexts by analyzing academic policies and guidelines established by top-ranked US universities regarding the use of ChatGPT in higher education. The data sources include academic policies, statements, guidelines as well as relevant resources provided by the top 100 universities in the US. Results show that the majority of these universities adopt an open but cautious approach towards the integration of GenAI. Primary concerns lie in ethical usage, accuracy, and data privacy. Most universities actively respond and provide diverse types of resources, such as syllabus templates/samples, workshops, shared arti
    
[^101]: 序列级确定性降低基于知识的对话生成中的虚构情况

    Sequence-Level Certainty Reduces Hallucination In Knowledge-Grounded Dialogue Generation

    [https://arxiv.org/abs/2310.18794](https://arxiv.org/abs/2310.18794)

    序列级确定性是减少基于知识的对话生成中幻觉的关键，这项工作提出了基于概率确定性和语义确定性的序列级确定性，结果表明更高水平的确定性对应更低水平的幻觉，进一步提出了基于确定性的响应排序方法

    

    在这项工作中，我们提出了序列级确定性作为基于知识的对话生成中虚构情况的共同主题。我们探讨了幻觉水平与两种序列级确定性之间的相关性：概率确定性和语义确定性。实证结果表明，模型响应中两种序列级确定性水平的提高与虚构水平的降低相关。我们进一步提出了基于确定性的响应排序（CRR），这是一种解码时的幻觉缓解方法，根据它们的序列级确定性对响应候选进行排序，并输出具有最高确定性水平的答案。与我们关于序列级确定性的定义相一致，我们设计了两种CRR方法：概率CRR（P-CRR）和语义CRR（S-CRR）。P-CRR使用整个序列的算术平均对各个单独抽样的模型响应进行排名

    arXiv:2310.18794v2 Announce Type: replace-cross  Abstract: In this work, we propose sequence-level certainty as a common theme over hallucination in Knowledge Grounded Dialogue Generation (KGDG). We explore the correlation between the level of hallucination and two types of sequence-level certainty: probabilistic certainty and semantic certainty. Empirical results reveal that a higher level of both types of sequence-level certainty in model responses is correlated with a lower level of hallucination. We further propose Certainty-based Response Ranking (CRR), a decoding-time hallucination mitigation method that ranks response candidates based on their sequence-level certainty and outputs the answer with the highest certainty level. Aligning with our definitions of sequence-level certainty, we design 2 types of CRR approaches: Probabilistic CRR (P-CRR) and Semantic CRR (S-CRR). P-CRR ranks individually sampled model responses using the arithmetic mean log-probability of the entire sequen
    
[^102]: 评估大型语言模型的空间理解能力

    Evaluating Spatial Understanding of Large Language Models

    [https://arxiv.org/abs/2310.14540](https://arxiv.org/abs/2310.14540)

    本研究评估了大型语言模型对空间结构的理解能力，发现LLMs在表示和推理空间结构时的表现存在显著差异，具有捕捉空间结构隐含特征的潜力。

    

    大型语言模型 (LLMs) 在各种任务中展现出卓越的能力。尽管这些模型在训练中只看到文本，但一些最近的研究表明，LLM表示隐含地捕捉了地面概念的几个方面。在这里，我们探讨了LLM表示对一种特别显著的基础知识 -- 空间关系的表现。我们设计了自然语言导航任务，评估了LLMs，特别是GPT-3.5-turbo、GPT-4 和 Llama2 系列模型，表示和推理空间结构的能力。这些任务揭示了LLM在不同空间结构 (包括正方形、六边形和三角形格、环和树) 上的性能差异。在广泛的错误分析中，我们发现LLMs的错误反映了空间和非空间因素。这些发现表明，LLMs似乎隐含地捕捉了空间结构的某些方面，但仍有提升的空间。

    arXiv:2310.14540v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) show remarkable capabilities across a variety of tasks. Despite the models only seeing text in training, several recent studies suggest that LLM representations implicitly capture aspects of the underlying grounded concepts. Here, we explore LLM representations of a particularly salient kind of grounded knowledge -- spatial relationships. We design natural-language navigation tasks and evaluate the ability of LLMs, in particular GPT-3.5-turbo, GPT-4, and Llama2 series models, to represent and reason about spatial structures. These tasks reveal substantial variability in LLM performance across different spatial structures, including square, hexagonal, and triangular grids, rings, and trees. In extensive error analysis, we find that LLMs' mistakes reflect both spatial and non-spatial factors. These findings suggest that LLMs appear to capture certain aspects of spatial structure implicitly, but room f
    
[^103]: 从人工真实到真实: 利用大型语言模型生成的伪数据进行低资源分子发现

    From Artificially Real to Real: Leveraging Pseudo Data from Large Language Models for Low-Resource Molecule Discovery

    [https://arxiv.org/abs/2309.05203](https://arxiv.org/abs/2309.05203)

    通过利用大型语言模型生成的伪数据，本研究解决了低资源挑战，实验表明利用伪数据进行领域自适应优于现有方法。

    

    分子发现在许多科学领域中起着重要作用，推动了新材料和创新药物设计的发展。最近，硅分子发现的发展突出了跨模态技术的有希望成果，这种技术架起了分子结构与其描述性注释之间的桥梁。然而，这些跨模态方法经常遇到数据稀缺的问题，阻碍了它们的性能和应用。本文通过利用大型语言模型（LLM）生成的人造真实数据来解决低资源挑战。我们首先引入了一个基于检索的提示策略来构建高质量的伪数据，然后探讨了有效利用这些伪数据的最佳方法。实验表明，使用伪数据进行领域自适应优于所有现有方法，同时需要更小的模型规模、减少的数据量和更低的训练成本。

    arXiv:2309.05203v3 Announce Type: replace  Abstract: Molecule discovery serves as a cornerstone in numerous scientific domains, fueling the development of new materials and innovative drug designs. Recent developments of in-silico molecule discovery have highlighted the promising results of cross-modal techniques, which bridge molecular structures with their descriptive annotations. However, these cross-modal methods frequently encounter the issue of data scarcity, hampering their performance and application. In this paper, we address the low-resource challenge by utilizing artificially-real data generated by Large Language Models (LLMs). We first introduce a retrieval-based prompting strategy to construct high-quality pseudo data, then explore the optimal method to effectively leverage this pseudo data. Experiments show that using pseudo data for domain adaptation outperforms all existing methods, while also requiring a smaller model scale, reduced data size and lower training cost, h
    
[^104]: 通过丢弃过时事实来减轻时间不一致性问题

    Mitigating Temporal Misalignment by Discarding Outdated Facts

    [https://arxiv.org/abs/2305.14824](https://arxiv.org/abs/2305.14824)

    提出了一种通过预测事实的持续时间来减轻时间不一致性问题的方法，从而避免语言模型重复提供过时信息，并帮助模型提高知识密集任务的校准性。

    

    虽然大型语言模型能够保留在预训练过程中见过的大量世界知识，但这些知识容易过时，并且更新起来并不是一件简单的事情。此外，这些模型经常在时间不一致性下使用，被要求回答关于现在的问题，尽管它们只是在过去收集的数据上训练过。为了减轻时间不一致性的影响，我们提出了事实持续时间预测：预测一个给定事实会保持真实的时间长度的任务。在我们的实验中，我们展示了识别哪些事实容易快速变化可以帮助模型避免重复过时信息，并确定哪些预测需要寻找最新知识源。我们还展示了如何建模事实持续时间可以提高对知识密集任务的校准，例如在时间不一致性下的开放性检索问答，通过丢弃易变事实。我们的数据和实现代码

    arXiv:2305.14824v3 Announce Type: replace  Abstract: While large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update. Furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present, despite having only been trained on data collected in the past. To mitigate the effects of temporal misalignment, we propose fact duration prediction: the task of predicting how long a given fact will remain true. In our experiments, we demonstrate that identifying which facts are prone to rapid change can help models avoid reciting outdated information and determine which predictions require seeking out up-to-date knowledge sources. We also show how modeling fact duration improves calibration for knowledge-intensive tasks, such as open-retrieval question answering, under temporal misalignment, by discarding volatile facts. Our data and cod
    
[^105]: ChatGPT在对话中的话语分析潜力揭示：实证研究

    Uncovering the Potential of ChatGPT for Discourse Analysis in Dialogue: An Empirical Study

    [https://arxiv.org/abs/2305.08391](https://arxiv.org/abs/2305.08391)

    本研究系统检查了ChatGPT在对话中的话语分析潜力，重点关注其对线性和分层话语结构的深层语义理解，实验结果显示ChatGPT在识别主题结构方面表现出熟练度但在话语解析方面遇到困难。

    

    大型语言模型，如ChatGPT，在许多下游任务中表现出显著的能力，然而它们理解对话结构的能力仍然较少被探讨，这需要更高级别的理解和推理能力。本文旨在系统地检查ChatGPT在两个话语分析任务中的表现：主题分割和话语解析，重点关注其对话底层线性和分层话语结构的深层语义理解。为了指导ChatGPT完成这些任务，我们首先构建了一个包含任务描述、输出格式和结构化输入的提示模板。然后，我们在四个流行的主题分割数据集和两个话语解析数据集上进行实验。实验结果表明，ChatGPT在识别通用领域对话中的主题结构方面表现出了熟练度，但在话语解析方面却遇到了相当大的困难。

    arXiv:2305.08391v2 Announce Type: replace  Abstract: Large language models, like ChatGPT, have shown remarkable capability in many downstream tasks, yet their ability to understand discourse structures of dialogues remains less explored, where it requires higher level capabilities of understanding and reasoning. In this paper, we aim to systematically inspect ChatGPT's performance in two discourse analysis tasks: topic segmentation and discourse parsing, focusing on its deep semantic understanding of linear and hierarchical discourse structures underlying dialogue. To instruct ChatGPT to complete these tasks, we initially craft a prompt template consisting of the task description, output format, and structured input. Then, we conduct experiments on four popular topic segmentation datasets and two discourse parsing datasets. The experimental results showcase that ChatGPT demonstrates proficiency in identifying topic structures in general-domain conversations yet struggles considerably i
    
[^106]: 基于遮蔽语言模型的上下文文本去噪

    Contextual Text Denoising with Masked Language Models

    [https://arxiv.org/abs/1910.14080](https://arxiv.org/abs/1910.14080)

    提出了一种基于遮蔽语言模型的上下文文本去噪算法，可以在不重新训练模型的情况下纠正噪声文本，并在多个下游任务中提高性能

    

    最近，借助深度学习模型，在不同的自然语言处理（NLP）任务中取得了显著进展。不幸的是，最先进的模型容易受到嘈杂文本的影响。我们提出了一种基于现成遮蔽语言模型的新上下文文本去噪算法。提出的算法不需要对模型进行重新训练，可以集成到任何NLP系统中，而不需要对配对的清洗训练数据进行额外训练。我们在合成噪声和自然噪声下评估了我们的方法，并表明所提算法可以利用上下文信息来纠正噪声文本，并在多个下游任务中提高嘈杂输入的性能。

    arXiv:1910.14080v2 Announce Type: replace  Abstract: Recently, with the help of deep learning models, significant advances have been made in different Natural Language Processing (NLP) tasks. Unfortunately, state-of-the-art models are vulnerable to noisy texts. We propose a new contextual text denoising algorithm based on the ready-to-use masked language model. The proposed algorithm does not require retraining of the model and can be integrated into any NLP system without additional training on paired cleaning training data. We evaluate our method under synthetic noise and natural noise and show that the proposed algorithm can use context information to correct noise text and improve the performance of noisy inputs in several downstream tasks.
    
[^107]: CC查询：从公开文献中发现大规模领域特定知识

    Query of CC: Unearthing Large Scale Domain-Specific Knowledge from Public Corpora. (arXiv:2401.14624v1 [cs.CL])

    [http://arxiv.org/abs/2401.14624](http://arxiv.org/abs/2401.14624)

    本论文提出了一种通过大型语言模型来收集特定领域知识的高效方法，通过该方法构建了一个高质量的名为“Knowledge Pile”的数据集，实验证明其显著改善了特定领域的数据稀缺问题。

    

    大型语言模型在各种任务中展示了显著的潜力，然而特定领域的开源模型和数据仍然非常稀缺。之前的研究主要集中在手动指定资源和收集特定领域的高质量数据，这消耗了大量时间和精力。为了解决这个问题，我们提出了一种基于大型语言模型的高效数据收集方法“CC查询”。该方法通过大型语言模型引导种子信息，并从公开文献中检索相关数据。它不仅收集了特定领域的知识相关数据，还揭示了潜在的推理过程数据。通过应用这种方法，我们构建了一个名为“Knowledge Pile”的高质量数据集，涵盖了包括STEM科学和人文科学在内的四个主要领域。实验结果表明，“Knowledge Pile”显著改善了

    Large language models have demonstrated remarkable potential in various tasks, however, there remains a significant scarcity of open-source models and data for specific domains. Previous works have primarily focused on manually specifying resources and collecting high-quality data on specific domains, which significantly consume time and effort. To address this limitation, we propose an efficient data collection method~\textit{Query of CC} based on large language models. This method bootstraps seed information through a large language model and retrieves related data from public corpora. It not only collects knowledge-related data for specific domains but unearths the data with potential reasoning procedures. Through the application of this method, we have curated a high-quality dataset called~\textsc{Knowledge Pile}, encompassing four major domains, including stem and humanities sciences, among others. Experimental results demonstrate that~\textsc{Knowledge Pile} significantly improve
    
[^108]: 上下文的重要性：通过图结构化知识上下文推动开放式答案生成的边界

    Context Matters: Pushing the Boundaries of Open-Ended Answer Generation with Graph-Structured Knowledge Context. (arXiv:2401.12671v1 [cs.CL])

    [http://arxiv.org/abs/2401.12671](http://arxiv.org/abs/2401.12671)

    本论文介绍了一种结合图驱动的上下文检索和知识图结构增强的框架，通过提高LLMs的能力，尤其是在特定领域的社区问答平台上，更好地回答开放式问题。

    

    在不断发展的人工智能领域中，通过大型语言模型（LLMs）来构建上下文丰富、有意义的回答至关重要。研究人员越来越意识到当LLMs的参数较少时，尝试提供合适答案给开放式问题时会遇到的挑战。为了解决这些障碍，将先进的策略与丰富的外部领域知识与LLMs相结合，可以显著提升答案的质量。本论文介绍了一种新颖的框架，将基于图的上下文检索与知识图结构增强相结合，提高了LLMs的能力，特别适用于特定领域的社区问答平台，如AskUbuntu、Unix和ServerFault。我们对不同参数大小的各种LLMs进行实验，评估它们在开放式问题的回答中的知识确定能力和事实准确性。我们的方法GraphContextGen在基于文本的现有方法上持续优于其他方法。

    In the continuously advancing AI landscape, crafting context-rich and meaningful responses via Large Language Models (LLMs) is essential. Researchers are becoming more aware of the challenges that LLMs with fewer parameters encounter when trying to provide suitable answers to open-ended questions. To address these hurdles, the integration of cutting-edge strategies, augmentation of rich external domain knowledge to LLMs, offers significant improvements. This paper introduces a novel framework that combines graph-driven context retrieval in conjunction to knowledge graphs based enhancement, honing the proficiency of LLMs, especially in domain specific community question answering platforms like AskUbuntu, Unix, and ServerFault. We conduct experiments on various LLMs with different parameter sizes to evaluate their ability to ground knowledge and determine factual accuracy in answers to open-ended questions. Our methodology GraphContextGen consistently outperforms dominant text-based ret
    
[^109]: 播风撩起风暴：编辑语言模型的影响

    Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models. (arXiv:2401.10647v1 [cs.CL])

    [http://arxiv.org/abs/2401.10647](http://arxiv.org/abs/2401.10647)

    本文研究了通过编辑语言模型的复杂后果，发现在增强模型准确性与保持道德完整性之间存在悖论。我们发现，尽管注入准确信息对模型的可靠性很重要，但它可能破坏模型的基本框架，导致不可预测和潜在的不安全行为。

    

    在人工智能领域中，红队测试或越狱大型语言模型（LLM）的概念已成为一个重要的研究领域。通过对模型进行编辑，揭示了这种修改的复杂后果，发现了增强模型准确性与保持其道德完整性之间的复杂关系。我们的深入分析揭示了一个令人惊讶的悖论：虽然注入准确信息对于模型的可靠性至关重要，但它却可能破坏模型的基本框架，导致不可预测和潜在的不安全行为。此外，我们提出了一个基准数据集NicheHazardQA，用于研究模型在相同和跨领域中的不安全行为。这一方面的研究揭示了编辑如何影响模型的安全度量和保护机制。

    In the rapidly advancing field of artificial intelligence, the concept of Red-Teaming or Jailbreaking large language models (LLMs) has emerged as a crucial area of study. This approach is especially significant in terms of assessing and enhancing the safety and robustness of these models. This paper investigates the intricate consequences of such modifications through model editing, uncovering a complex relationship between enhancing model accuracy and preserving its ethical integrity. Our in-depth analysis reveals a striking paradox: while injecting accurate information is crucial for model reliability, it can paradoxically destabilize the model's foundational framework, resulting in unpredictable and potentially unsafe behaviors. Additionally, we propose a benchmark dataset NicheHazardQA to investigate this unsafe behavior both within the same and cross topical domain. This aspect of our research sheds light on how the edits, impact the model's safety metrics and guardrails. Our find
    
[^110]: AntEval: 量化评估智能体社交互动的信息性和表达性

    AntEval: Quantitatively Evaluating Informativeness and Expressiveness of Agent Social Interactions. (arXiv:2401.06509v1 [cs.CL])

    [http://arxiv.org/abs/2401.06509](http://arxiv.org/abs/2401.06509)

    本研究通过使用桌面角色扮演游戏规则创建了一个环境，量化评估智能体社交互动的信息性和表达性，旨在克服隐私问题并促使智能体进行有意义、高质量的互动。

    

    尽管基于大型语言模型（LLM）的智能体已成功地模仿了各种情境中的人类行为，但复杂的、多角色社交互动在扩展环境中的领域仍未充分探索。隐私问题使捕捉和利用复杂的现实生活互动变得困难。更重要的是，缺乏定量评估方法阻碍了高质量智能体互动的追求，导致互动的信息性和表达性有限，表现为肤浅的闲聊而没有清晰的意图。在这项工作中，我们利用桌面角色扮演游戏（TRPG）的规则创建了一个有利于复杂、上下文丰富的互动的环境，强调信息性和表达性。这个虚拟环境减轻了隐私问题，并激励智能体作为游戏目标的一部分进行有意义的、高质量的互动。

    While Large Language Models (LLMs) based agents have successfully mimicked human behaviors in various scenarios, the realm of complex, multi-character social interactions within extended contexts remains underexplored. The challenge is compounded by privacy concerns, making it difficult to capture and utilize intricate real-life interactions. More importantly, the absence of quantitative evaluation methods hampers the pursuit of high-quality agent interactions, often leading to interactions that are limited in informativeness and expressiveness, characterized by superficial small talk without clear intentions. In this work, we leverage the rules of Tabletop Role-Playing Games (TRPG) to create an environment conducive to complex, context-rich interactions, emphasizing informativeness and expressiveness. This virtual setting alleviates privacy concerns and motivates agents to engage in meaningful, high-quality interactions as part of their in-game objectives. To assess these interactions
    
[^111]: DevEval: 评估实际软件项目中的代码生成

    DevEval: Evaluating Code Generation in Practical Software Projects. (arXiv:2401.06401v1 [cs.SE])

    [http://arxiv.org/abs/2401.06401](http://arxiv.org/abs/2401.06401)

    本文提出了一个名为DevEval的新基准测试，用于评估实际软件项目中的代码生成。与之前的基准测试相比，DevEval在真实的项目分布、充足的依赖和足够规模的项目背景等方面更贴合实际。通过对五个流行的大型语言模型进行评估，我们揭示了它们在代码生成中的实际能力。

    

    如何评估大型语言模型（LLMs）在代码生成中的表现是一个开放的问题。许多基准测试已经提出，但是与实际软件项目不一致，例如虚构的程序分布，依赖不足和小规模项目背景。因此，LLMs在实际项目中的能力还不清楚。在本文中，我们提出了一个名为DevEval的新基准测试，与开发人员在实际项目中的经验相吻合。DevEval通过一个严格的流程收集到了来自119个实际项目的2690个样本，涵盖10个领域。与之前的基准测试相比，DevEval在多个维度上与实际项目相吻合，例如真实的程序分布，充足的依赖和足够规模的项目背景。我们在DevEval上评估了五个流行的LLMs（例如gpt-4，gpt-3.5-turbo，CodeLLaMa和StarCoder），并揭示了它们在代码生成中的实际能力。例如，gpt-3.5-turbo的最高Pass@1只有42。

    How to evaluate Large Language Models (LLMs) in code generation is an open question. Many benchmarks have been proposed but are inconsistent with practical software projects, e.g., unreal program distributions, insufficient dependencies, and small-scale project contexts. Thus, the capabilities of LLMs in practical projects are still unclear. In this paper, we propose a new benchmark named DevEval, aligned with Developers' experiences in practical projects. DevEval is collected through a rigorous pipeline, containing 2,690 samples from 119 practical projects and covering 10 domains. Compared to previous benchmarks, DevEval aligns to practical projects in multiple dimensions, e.g., real program distributions, sufficient dependencies, and enough-scale project contexts. We assess five popular LLMs on DevEval (e.g., gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo only is 42 in our experim
    
[^112]: LEGO: 语言增强的多模态关联模型

    LEGO:Language Enhanced Multi-modal Grounding Model. (arXiv:2401.06071v1 [cs.CV])

    [http://arxiv.org/abs/2401.06071](http://arxiv.org/abs/2401.06071)

    LEGO是一种语言增强的多模态关联模型，它能够在各种任务中实现细粒度的理解和精确的标识能力。

    

    多模态大型语言模型在不同模态的各种任务中展现出了令人印象深刻的性能。然而，现有的多模态模型主要强调捕捉每种模态内的全局信息，而忽视了跨模态感知局部信息的重要性。因此，这些模型缺乏有效理解输入数据细粒度细节的能力，从而限制了它们在需要更细致理解的任务中的性能。为了解决这个限制，迫切需要开发能够在多个模态之间进行细粒度理解的模型，从而增强它们在各种任务中的适用性。在本文中，我们提出了LEGO，一种语言增强的多模态关联模型。除了像其他多模态模型一样捕捉全局信息外，我们提出的模型在需要详细理解输入内的局部信息的任务中表现出色。它展示了精确的标识能力。

    Multi-modal large language models have demonstrated impressive performance across various tasks in different modalities. However, existing multi-modal models primarily emphasize capturing global information within each modality while neglecting the importance of perceiving local information across modalities. Consequently, these models lack the ability to effectively understand the fine-grained details of input data, limiting their performance in tasks that require a more nuanced understanding. To address this limitation, there is a compelling need to develop models that enable fine-grained understanding across multiple modalities, thereby enhancing their applicability to a wide range of tasks. In this paper, we propose LEGO, a language enhanced multi-modal grounding model. Beyond capturing global information like other multi-modal models, our proposed model excels at tasks demanding a detailed understanding of local information within the input. It demonstrates precise identification 
    
[^113]: 更少就意味着更多: 对参数高效微调的特洛伊攻击

    Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning. (arXiv:2310.00648v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00648](http://arxiv.org/abs/2310.00648)

    本文研究了参数高效微调（PEFT）的安全性问题，发现PEFT易受特洛伊攻击。通过提出一种新的攻击方式PETA，并在各种下游任务和触发器设计中进行广泛测试，发现PETA能够在攻击成功率和未受影响的准确性方面取得有效结果。

    

    参数高效微调（PEFT）可以将预训练语言模型（PLM）高效地适应到特定任务中。通过仅微调一小部分（额外的）参数，PEFT实现了与全面微调相当的性能。然而，尽管它被广泛使用，但PEFT的安全性影响仍然鲜为人知。在本文中，我们进行了一项初步研究，揭示了PEFT对特洛伊攻击的独特易受攻击性。具体而言，我们提出了PETA，一种通过双层优化考虑下游适应的新型攻击方式：上层目标将后门嵌入PLM中，而下层目标模拟PEFT以保留PLM的任务特定性能。通过对多种下游任务和触发器设计的广泛评估，我们证明了PETA在攻击成功率和未受影响的干净准确性方面的有效性，即使受害用户在使用纯净数据对带有后门的PLM进行PEFT后。

    Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of pre-trained language models (PLMs) to specific tasks. By tuning only a minimal set of (extra) parameters, PEFT achieves performance comparable to full fine-tuning. However, despite its prevalent use, the security implications of PEFT remain largely unexplored. In this paper, we conduct a pilot study revealing that PEFT exhibits unique vulnerability to trojan attacks. Specifically, we present PETA, a novel attack that accounts for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a PLM while the lower-level objective simulates PEFT to retain the PLM's task-specific performance. With extensive evaluation across a variety of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs PEFT over the backdoored PLM using untainted data. Moreover, we empi
    
[^114]: SeaEval多语言基础模型：从跨语言对齐到文化推理

    SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning. (arXiv:2309.04766v1 [cs.CL])

    [http://arxiv.org/abs/2309.04766](http://arxiv.org/abs/2309.04766)

    SeaEval是一个评估多语言基础模型的基准测试，研究了模型在自然语言理解、推理以及对文化实践、细微差别和价值观的理解能力上的表现。重要发现包括模型在给出改写指令时行为各异，受到暴露偏差的影响，对于语义等价的多语言查询的回答不一致，以及模型在情感相关问题上的一致性不同。

    

    我们提出了一种用于多语言基础模型的SeaEval基准测试。除了表征这些模型如何理解和推理自然语言外，我们还研究了它们对文化实践、细微差别和价值观的理解能力。除了标准的准确度指标，我们还调查了基础模型在语义和多语言性维度上的脆弱性。我们的分析涵盖了开源和闭源模型，从而得到了在经典的自然语言处理任务、推理和文化理解方面的实证结果。重要发现包括：（1）大多数模型在给出改写指令时的行为各异；（2）许多模型仍然受到暴露偏差的影响（如位置偏差、大多数标签偏差）；（3）对于根源于事实、科学和常识知识的问题，预期在语义上等价的多语言查询应该得到一致的回答。然而，大多数模型在这些查询上表现出令人意外的不一致性；（4）多语言情况下，模型对于情感相关的问题表现出不同程度的一致性。

    We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we investigate the brittleness of foundation models in the dimensions of semantics and multilinguality. Our analyses span both open-sourced and closed models, leading to empirical results across classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate (1) Most models exhibit varied behavior when given paraphrased instructions. (2) Many models still suffer from exposure bias (e.g., positional bias, majority label bias). (3) For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, most models surprisingly demonstrate inconsistent performance on these queries. (4) Multilingu
    
[^115]: 大型语言模型在电子健康记录中识别社会健康决定因素

    Large Language Models to Identify Social Determinants of Health in Electronic Health Records. (arXiv:2308.06354v1 [cs.CL])

    [http://arxiv.org/abs/2308.06354](http://arxiv.org/abs/2308.06354)

    本研究利用大型语言模型从电子健康记录中提取社会健康决定因素（SDoH），并通过合成临床文本改进了这些极有价值但很少被记录的临床数据的提取。最佳模型为经过微调的Flan-T5 XL和Flan-T5 XXL，其中小型模型改进了性能。

    

    社会健康决定因素（SDoH）对患者的结果有重要影响，但在电子健康记录（EHR）中的收集不完整。本研究研究了大型语言模型从EHR中提取SDoH的能力，并探讨了合成临床文本在改进这些少见但极有价值的临床数据提取中的作用。对800份患者记录进行了SDoH类别的注释，并评估了几个基于transformer的模型。本研究还尝试了合成数据生成，并评估了算法偏差。我们表现最佳的模型是经过微调的Flan-T5 XL（macro-F1 0.71）用于任何SDoH，以及Flan-T5 XXL（macro-F1 0.70）。通过合成数据辅助微调的效益因模型架构和大小而异，在较小的Flan-T5模型（基础和大型）中表现出最大的性能提升（delta F1 +0.12到+0.23）。模型性能。

    Social determinants of health (SDoH) have an important impact on patient outcomes but are incompletely collected from the electronic health records (EHR). This study researched the ability of large language models to extract SDoH from free text in EHRs, where they are most commonly documented, and explored the role of synthetic clinical text for improving the extraction of these scarcely documented, yet extremely valuable, clinical data. 800 patient notes were annotated for SDoH categories, and several transformer-based models were evaluated. The study also experimented with synthetic data generation and assessed for algorithmic bias. Our best-performing models were fine-tuned Flan-T5 XL (macro-F1 0.71) for any SDoH, and Flan-T5 XXL (macro-F1 0.70). The benefit of augmenting fine-tuning with synthetic data varied across model architecture and size, with smaller Flan-T5 models (base and large) showing the greatest improvements in performance (delta F1 +0.12 to +0.23). Model performance 
    
[^116]: 探索GPT-4的道德和法律推理的心理学研究

    Exploring the psychology of GPT-4's Moral and Legal Reasoning. (arXiv:2308.01264v1 [cs.AI])

    [http://arxiv.org/abs/2308.01264](http://arxiv.org/abs/2308.01264)

    本文探究了GPT-4的道德和法律推理，发现其与人类之间在意图归因、因果判断、欺骗的道德性、道德基础、道德运气对法律判断的影响、同意的概念以及规则违反判断方面存在高相关性。

    

    大型语言模型已被用作高度复杂的人工智能的基础，能够对法律和道德问题作出与人类类似的回应。然而，这些模型对于自身内部工作的指导是不可靠的，即使是它们的创建工程团队也无法解释它们如何获得当前所有能力的具体过程。机器心理学这一新兴领域旨在深入了解这些模型拥有的过程和概念。在本文中，我们运用心理学的方法来探究GPT-4的道德和法律推理。具体而言，我们研究了GPT-4与人类在意图归因、因果判断、欺骗的道德性、道德基础、道德运气对法律判断的影响、同意的概念以及规则违反判断方面的相似性和差异。我们发现人类和人工智能的回答之间存在较高的相关性。

    Large language models have been used as the foundation of highly sophisticated artificial intelligences, capable of delivering human-like responses to probes about legal and moral issues. However, these models are unreliable guides to their own inner workings, and even the engineering teams behind their creation are unable to explain exactly how they came to develop all of the capabilities they currently have. The emerging field of machine psychology seeks to gain insight into the processes and concepts that these models possess. In this paper, we employ the methods of psychology to probe into GPT-4's moral and legal reasoning. More specifically, we investigate the similarities and differences between GPT-4 and humans when it comes to intentionality ascriptions, judgments about causation, the morality of deception, moral foundations, the impact of moral luck on legal judgments, the concept of consent, and rule violation judgments. We find high correlations between human and AI response
    
[^117]: DRAGON: 一种基于对话的带有视觉语言关联的辅助导航机器人

    DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding. (arXiv:2307.06924v1 [cs.RO])

    [http://arxiv.org/abs/2307.06924](http://arxiv.org/abs/2307.06924)

    DRAGON是一种基于对话的导航机器人，能够理解用户的指令并通过语言与用户沟通，为视力受损者提供导航和环境描述的帮助。

    

    视力受损者在理解和导航周围空间方面存在困难。目前的导航技术要么只关注导航，要么提供有限的关于环境的沟通。受到最近在视觉语言关联和语义导航方面的进展的启发，我们提出了DRAGON，一种由对话系统驱动的导航机器人，并具有将环境与自然语言关联的能力。通过理解用户的指令，DRAGON能够引导用户到地图上的目标地标，描述环境，并通过视觉观察回答问题。通过有效利用对话，机器人可以将用户的自由形式描述与环境中的地标关联起来，并通过口语提供语义信息给用户。我们在日常室内环境中进行了盲目参与者的用户研究。我们的结果表明，DRAGON能够与用户顺畅地沟通，

    Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, pr
    
[^118]: 以提示为基础的个性化冷启动推荐的研究

    Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])

    [http://arxiv.org/abs/2306.17256](http://arxiv.org/abs/2306.17256)

    本研究旨在解决个性化冷启动推荐问题，通过利用预训练语言模型的能力，将推荐过程转化为自然语言情感分析，提供适用于创业企业和用户参与历史不足的平台的个性化推荐。

    

    推荐系统在根据用户过去的行为帮助用户发现与其兴趣相符的信息方面发挥着关键作用。然而，当用户和物品之间的历史交互记录不可用时，开发个性化推荐系统变得具有挑战性，这就是所谓的系统冷启动推荐问题。此问题在创业企业或用户参与历史不足的平台中尤为突出。以往的研究集中在用户或物品的冷启动场景，其中系统仍然通过在同一领域中的历史用户和物品交互进行训练来为新用户或物品提供推荐，而无法解决我们的问题。为了弥合这一鸿沟，我们的研究引入了一种创新且有效的方法，利用预训练语言模型的能力。我们将推荐过程转化为自然语言情感分析，其中包含用户资料和物品属性的信息。

    Recommender systems play a crucial role in helping users discover information that aligns with their interests based on their past behaviors. However, developing personalized recommendation systems becomes challenging when historical records of user-item interactions are unavailable, leading to what is known as the system cold-start recommendation problem. This issue is particularly prominent in start-up businesses or platforms with insufficient user engagement history. Previous studies focus on user or item cold-start scenarios, where systems could make recommendations for new users or items but are still trained with historical user-item interactions in the same domain, which cannot solve our problem. To bridge the gap, our research introduces an innovative and effective approach, capitalizing on the capabilities of pre-trained language models. We transform the recommendation process into sentiment analysis of natural languages containing information of user profiles and item attribu
    
[^119]: 通过语言模型批量生产多模态系统的失败

    Mass-Producing Failures of Multimodal Systems with Language Models. (arXiv:2306.12105v1 [cs.LG])

    [http://arxiv.org/abs/2306.12105](http://arxiv.org/abs/2306.12105)

    本文介绍了一种MultiMon系统，可以自动识别多模态系统中的系统性失败，揭示CLIP文本编码器的14个系统性失败，每个都由数百个不同的输入组成，这些输入会导致其他大多数最先进的多模态系统的失败。

    

    部署的多模态系统可能以评估人员未曾预见的方式失败。为了在部署前找到这些失败，我们介绍了MultiMon，这是一个能够自动识别系统性失败的系统，能够提供可推广的、自然语言描述模型失败模式的例子。为了揭示系统性失败，MultiMon从语料库中抓取错误协议的示例：输入产生相同的输出，但不应该如此。然后它会激活一个语言模型（例如GPT-4）来查找系统性失败的模式并用自然语言描述它们。我们使用MultiMon找到了CLIP文本编码器的14个系统性失败（例如“忽略量词”，每个都由数百个不同的输入组成（例如“一个带有一些/许多书的书架”）。因为CLIP是大多数最先进的多模态系统的基础，这些输入会导致Midjourney 5.1、DALL-E、VideoFusion等系统失败。MultiMon也可以指导针对特定用例的相关故障，例如自驾车系统。

    Deployed multimodal systems can fail in ways that evaluators did not anticipate. In order to find these failures before deployment, we introduce MultiMon, a system that automatically identifies systematic failures -generalizable, natural-language descriptions of patterns of model failures. To uncover systematic failures, MultiMon scrapes a corpus for examples of erroneous agreement: inputs that produce the same output, but should not. It then prompts a language model (e.g., GPT-4) to find systematic patterns of failure and describe them in natural language. We use MultiMon to find 14 systematic failures (e.g., "ignores quantifiers") of the CLIP text-encoder, each comprising hundreds of distinct inputs (e.g., "a shelf with a few/many books"). Because CLIP is the backbone for most state-of-the-art multimodal systems, these inputs produce failures in Midjourney 5.1, DALL-E, VideoFusion, and others. MultiMon can also steer towards failures relevant to specific use cases, such as self-dri
    
[^120]: 大型语言模型的软提示调整方法用于评估偏差

    Soft-prompt Tuning for Large Language Models to Evaluate Bias. (arXiv:2306.04735v1 [cs.CL])

    [http://arxiv.org/abs/2306.04735](http://arxiv.org/abs/2306.04735)

    本文使用软提示调整来量化大型语言模型中的偏差，避免手动设计提示导致的人为偏差注入。通过分组公平性检查模型对不同敏感属性的偏见，发现了有趣的偏差模式。

    

    近年来，大型语言模型的提示功能因无需标记数据即可产生良好结果而备受青睐。然而，这需要进行提示调整以获得引导更好模型性能的最佳提示。本文中，我们探讨了在情感分类任务中使用软提示调整来量化大型语言模型（LLMs）如Open Pre-trained Transformers（OPT）和Galactica语言模型中的偏差。由于这些模型是在可能偏向某些人群的真实数据上训练的，因此识别这些潜在问题非常重要。使用软提示来评估偏差给我们带来了额外的优势，可以避免手动设计提示导致的人为偏差注入。我们使用分组公平性（偏差）检查模型对不同敏感属性的偏见，并找到了有趣的偏差模式。由于LLMs已在各种应用中被用于工业中，因此我们对其进行的偏见评估具有实际意义。

    Prompting large language models has gained immense popularity in recent years due to the advantage of producing good results even without the need for labelled data. However, this requires prompt tuning to get optimal prompts that lead to better model performances. In this paper, we explore the use of soft-prompt tuning on sentiment classification task to quantify the biases of large language models (LLMs) such as Open Pre-trained Transformers (OPT) and Galactica language model. Since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues. Using soft-prompts to evaluate bias gives us the extra advantage of avoiding the human-bias injection that can be caused by manually designed prompts. We check the model biases on different sensitive attributes using the group fairness (bias) and find interesting bias patterns. Since LLMs have been used in the industry in various applications, i
    
[^121]: 多尺度正负样本检测AI生成文本

    Multiscale Positive-Unlabeled Detection of AI-Generated Texts. (arXiv:2305.18149v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.18149](http://arxiv.org/abs/2305.18149)

    本文提出了一种多尺度正负样本的训练框架，以解决多尺度AI生成文本的检测问题。通过将短机器文本标记为“未标记”来重新表述文本分类问题，并提出了一个规则化损失函数来优化检测性能，有效性能显著优于现有的方法。

    

    最近发布的大型语言模型（LLM）如ChatGPT等在生成类似于人类的文本方面令人惊讶，但它们可能被用于制造虚假的学术文本、虚假新闻、虚假推特等。先前的作品提出了检测这些多尺度AI生成文本的方法，包括简单的ML分类器、基于预训练模型的训练不可知方法和精调的语言分类模型。然而，主流检测器在构建时没有考虑到文本长度的因素：短文本的缺乏信息特征，使其更难检测。针对多尺度文本检测的挑战，本文提出了一个多尺度正负样本（MPU）训练框架。首先，我们承认短的机器文本具有类人属性，并将文本分类重新表述为一个正负样本问题，即通过在训练期间标记这些短的机器文本为"unlabeled"来解决这个问题。在这个正负样本的背景下，我们提出了

    Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may get misused for fake scholarly texts, fake news, fake tweets, et cetera. Previous works have proposed methods to detect these multiscale AI-generated texts, including simple ML classifiers, pretrained-model-based training-agnostic methods, and finetuned language classification models. However, mainstream detectors are formulated without considering the factor of corpus length: shorter corpuses are harder to detect compared with longer ones for shortage of informative features. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the challenge of multiscale text detection. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase text classification as a Positive-Unlabeled (PU) problem by marking these short machine texts as "unlabeled" during training. In this PU context, we propose the le
    
[^122]: Sophia：一种用于语言模型预训练的可扩展的随机二阶优化器

    Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training. (arXiv:2305.14342v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14342](http://arxiv.org/abs/2305.14342)

    Sophia是一种用于语言模型预训练的可扩展的二阶优化算法，使用对角Hessian作为预调节器，并进行元素级别的裁剪控制更新大小。

    

    鉴于语言模型预训练的巨大成本，优化算法的微小改进将会大大降低训练的时间和成本。Adam及其变种一直是最先进的，而更复杂的二阶（基于Hessian的）优化器往往会带来太多的每步开销。在这篇论文中，我们提出了Sophia，一种简单可扩展的二阶优化器，它使用轻量级估计的对角Hessian作为预调节器。更新步骤是梯度的移动平均值除以估计Hessian的移动平均值，然后进行元素级别的裁剪。裁剪控制了最坏情况下的更新大小，并控制了Hessian在轨迹上的非凸性和快速变化的负面影响。Sophia只在每几次迭代中估计对角Hessian，这几乎没有平均每步的时间和内存开销。在使用GPT m进行语言建模时，

    Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT m
    
[^123]: 通过生成对抗反馈对语言模型进行微调

    Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v1 [cs.CL])

    [http://arxiv.org/abs/2305.06176](http://arxiv.org/abs/2305.06176)

    本研究探讨了一种新的方法，使用生成对抗反馈的强化学习(RLGAF)对大型语言模型进行微调，以取代仅受人类反馈的强化学习(RLHF)，从而消除评估者的专业限制并提高性能。

    

    通过人类反馈的强化学习已经显著提高了大型语言模型(LLMs)的性能，使其输出与人类期望的价值观保持一致。然而，RLHF受到人类评估者的专业知识和生产力限制。在本研究中，我们研究了一种替代方法: 使用生成对抗反馈的强化学习(RLGAF)代替RLHF。我们的初步发现表明，RLGAF可以帮助对齐LLM的输出，同时不会受到RLHF固有的限制，为进一步自动化AI对齐的研究提供了有希望的途径。

    Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to significantly enhance the performance of large language models (LLMs) by aligning their outputs with desired human values. However, RLHF is constrained by the expertise and productivity limitations of human evaluators. In this study, we investigate an alternative approach: Reinforcement Learning with Generative Adversarial Feedback (RLGAF) to RLHF. Our preliminary findings indicate that RLGAF can help align LLMs outputs while not suffering from the inherent restrictions of RLHF, suggesting promising avenues for further research on automating AI alignment.
    
[^124]: 使用丰富的元数据注释的屏幕角色的个性化语言建模

    Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v1 [cs.CL])

    [http://arxiv.org/abs/2303.16618](http://arxiv.org/abs/2303.16618)

    本篇论文研究了如何使用丰富的元数据注释的信息进行屏幕角色的个性化语言建模，测试表明这样可以有效地进行个性化语言模型的构建，即使对于零样本的演说家也可以应用。

    

    语言模型的个性化为对话敏感，能更好地捕捉特定特征的人员和/或特定环境中的说话模式。然而，丰富的角色注释难以得到和成功利用。在此工作中，我们发布并描述了一组新颖的手动注释，涵盖了来自流行的 Cornell 电影对话语料库的 863 名演讲者，包括特征引用和角色描述，以及超过 95％ 的特色电影的一组自动提取的六个元数据。我们对两个语料库进行了广泛的实验，并表明可以有效地使用此类注释来个性化语言模型，将困惑减少高达 8.5％。我们的方法甚至可以应用于零样本的演讲者，即对于没有先前培训数据的演讲者，依赖于角色的人口特征的组合。由于收集此类元数据成本高昂，因此我们还贡献了一项成本效益分析，以突出显示

    Personalisation of language models for dialogue sensitises them to better capture the speaking patterns of people of specific characteristics, and/or in specific environments. However, rich character annotations are difficult to come by and to successfully leverage. In this work, we release and describe a novel set of manual annotations for 863 speakers from the popular Cornell Movie Dialog Corpus, including features like characteristic quotes and character descriptions, and a set of six automatically extracted metadata for over 95% of the featured films. We perform extensive experiments on two corpora and show that such annotations can be effectively used to personalise language models, reducing perplexity by up to 8.5%. Our method can be applied even zero-shot for speakers for whom no prior training data is available, by relying on combinations of characters' demographic characteristics. Since collecting such metadata is costly, we also contribute a cost-benefit analysis to highlight
    

