# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Evaluating Verifiability in Generative Search Engines.](http://arxiv.org/abs/2304.09848) | 本文评估了四个流行生成式搜索引擎的可验证性，发现现有生成式搜索引擎响应流畅但仅有51.5%的生成句子得到了完整的引用支持，仅有74.5%的引用支持其相关语句。 |
| [^2] | [Fairness in AI and Its Long-Term Implications on Society.](http://arxiv.org/abs/2304.09826) | 本文探讨了AI的公平性问题，指出缺乏AI公平性会加深偏见成为社会压力因素，可能对社会产生长期影响，因此需要寻求潜在解决方案。 |
| [^3] | [A Two-Stage Framework with Self-Supervised Distillation For Cross-Domain Text Classification.](http://arxiv.org/abs/2304.09820) | 本文提出了一种双阶段框架，使用自监督蒸馏和来自不同但相关源领域的标记数据完成跨领域文本分类，取得在单源领域适应性和多源领域适应性上的新的最先进结果。 |
| [^4] | [A Survey of Corpora for Germanic Low-Resource Languages and Dialects.](http://arxiv.org/abs/2304.09805) | 本文针对德语低资源语言变体，通过系统的调查语料库，发现手工注释的语言资源稀少，主要涵盖形态句法，同时观察到此类研究的兴趣正在增加。 |
| [^5] | [GeneGPT: Teaching Large Language Models to Use NCBI Web APIs.](http://arxiv.org/abs/2304.09667) | GeneGPT通过少量NCBI API调用URL请求作为演示，教授大型语言模型使用NCBI Web API回答基因组问题，并在GeneTuring测试中达到了优异的结果。 |
| [^6] | [MPMQA: Multimodal Question Answering on Product Manuals.](http://arxiv.org/abs/2304.09660) | 本文提出了一个多模态产品手册问答（MPMQA）任务，构建了一个大规模数据集PM209来支持该任务，要求模型不仅处理多模态内容，还提供多模态答案，有望提高产品手册理解效能。 |
| [^7] | [BRENT: Bidirectional Retrieval Enhanced Norwegian Transformer.](http://arxiv.org/abs/2304.09649) | BRENT是第一个使用双向检索提高挪威语的检索式语言模型，可以提高读者在提取问答方面的表现。 |
| [^8] | [Bridging Natural Language Processing and Psycholinguistics: computationally grounded semantic similarity and relatedness datasets for Basque and Spanish.](http://arxiv.org/abs/2304.09616) | 该论文提出了一个基于计算的词语相似性数据集，旨在弥补心理语言学研究中的空白，通过提供大量控制了在词汇处理中起重要作用的变量的名词对的语义相似性的量化。数据集包括巴斯克语和欧洲西班牙语的名词对信息，但进一步的工作意图将其扩展到更多语言。 |
| [^9] | [CB-Conformer: Contextual biasing Conformer for biased word recognition.](http://arxiv.org/abs/2304.09607) | CB-Conformer 是一种为了提高有偏差词识别而提出的 Conformer，引入了 Contextual Biasing Module 和 Self-Adaptive Language Model 以更好地利用上下文信息和单词偏差信息，并在测试中取得显著提高。 |
| [^10] | [Is ChatGPT Equipped with Emotional Dialogue Capabilities?.](http://arxiv.org/abs/2304.09582) | 本文研究了开发者OpenAI的ChatGPT情感对话能力，发现在情感回复生成方面表现良好，但在情感对话理解方面落后于监督模型。 |
| [^11] | [SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts.](http://arxiv.org/abs/2304.09548) | SemEval 2023举办了LegalEval共享任务，即理解法律文本，包括 自动结构化和语义连贯化的法律文件（Task-A），法律命名实体识别（Task-B）以及自动预测法律案件结果和提供预测解释（Task-C）。26个团队提交了系统论文并在所有子任务中优于基准线，但仍有改进空间。 |
| [^12] | [Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent.](http://arxiv.org/abs/2304.09542) | 本文研究了生成性LLMs，如ChatGPT和GPT-4在信息检索中的相关性排名能力，实验结果表明，这些模型经适当指导后表现优异，有时甚至优于传统监督学习方法。将ChatGPT的排名能力提炼为专门模型在BEIR上的效果更优。 |
| [^13] | [Controlling keywords and their positions in text generation.](http://arxiv.org/abs/2304.09516) | 本文研究了一种控制文本生成中关键词及其位置的新方法，通过使用特殊标记控制关键词相对位置，可以生成更符合用户意图的文本。 |
| [^14] | [Emotion fusion for mental illness detection from social media: A survey.](http://arxiv.org/abs/2304.09493) | 本文为全面综述通过分析社交媒体上用户生成的文章来使用情感信息进行心理疾病检测的方法。文章回顾了不同的融合策略及其优缺点，并讨论了该领域研究人员所面临的挑战和未来的研究方向。 |
| [^15] | [EC^2: Emergent Communication for Embodied Control.](http://arxiv.org/abs/2304.09448) | EC^2 提出一种新的紧急通信方案，用于视频语言预训练以进行少样本身体控制，实现了在 EmbodiedAI 基准测试上的最先进的少样本性能。 |
| [^16] | [Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes.](http://arxiv.org/abs/2304.09433) | 本文探讨了使用大型语言模型在不需要特定领域的训练和定制下实现生成可查询表格的简单系统，并给出了两种实现策略，在不同质量和成本中平衡。通过实验证明，该系统对不同类型文档生成的表格均高质量，且无需文档特定的定制。 |
| [^17] | [TieFake: Title-Text Similarity and Emotion-Aware Fake News Detection.](http://arxiv.org/abs/2304.09421) | 该论文提出了一种新颖的假新闻检测方法TieFake，通过检测新闻标题和正文之间的相似度和情感信息，同时通过BERT和ResNeSt学习文本和图像的表示，实现对多模态文本的假新闻检测。 |
| [^18] | [How to Do Things with Deep Learning Code.](http://arxiv.org/abs/2304.09406) | 本文通过提取GPT-2的表示映射和案例研究，测试了对深度学习代码进行关键代码研究的潜力并证明了代码是关键人工智能和关键机器学习研究子领域的研究人员的分析重点的有效性，同时让人们关注普通用户如何与深度学习系统交互，并指导深度学习系统的行为。 |
| [^19] | [MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning.](http://arxiv.org/abs/2304.09402) | MixPro是一种数据增强方法，通过对原始输入和模板进行混合来提高基于提示的学习性能，平均提高了5.08%的模型性能。 |
| [^20] | [An Empirical Study of Leveraging Knowledge Distillation for Compressing Multilingual Neural Machine Translation Models.](http://arxiv.org/abs/2304.09388) | 本文研究了利用知识蒸馏方法压缩多语言神经机器翻译模型的实证效果，并以印地语到英语的翻译为案例展示了蒸馏方法对模型大小和性能的影响。研究发现，深层紧凑模型往往与浅层非紧凑模型一样好，将蒸馏模型在高质量子集上微调可以提高翻译质量。 |
| [^21] | [Shuffle & Divide: Contrastive Learning for Long Text.](http://arxiv.org/abs/2304.09374) | 本文介绍了一种基于对比学习的自监督学习方法，通过“洗牌和切割”算法对长文本进行预处理，提取BERT嵌入，在无监督的情况下对文本进行分类，比当前最先进的技术提高20.94%。 |
| [^22] | [LLM as A Robotic Brain: Unifying Egocentric Memory and Control.](http://arxiv.org/abs/2304.09349) | 本文提出了一个统一自我中心记忆和控制的框架LLM-Brain，使用大规模语言模型作为机器人大脑进行零-shot学习。该框架包括封闭式多轮对话，覆盖了感知、规划、控制和记忆，具有很好的泛化性能，适用于多个机器人任务。 |
| [^23] | [The Unintended Consequences of Censoring Digital Technology -- Evidence from Italy's ChatGPT Ban.](http://arxiv.org/abs/2304.09339) | 意大利禁止ChatGPT产生了短期的生产力干扰，但也导致了审查绕过工具的显著增加。 |
| [^24] | [BIM-GPT: a Prompt-Based Virtual Assistant Framework for BIM Information Retrieval.](http://arxiv.org/abs/2304.09333) | 该论文提出了一种基于提示的虚拟助手框架——BIM-GPT，通过集成BIM和GPT技术，支持自然语言检索，并在BIM IR数据集测试中获得了高精度。同时，对医院建筑的VA原型验证了该框架的功能，为建筑业中BIM IR的多功能VA开发做出了贡献。 |
| [^25] | [A Neural Lambda Calculus: Neurosymbolic AI meets the foundations of computing and functional programming.](http://arxiv.org/abs/2304.09276) | 本文提出了一种神经λ演算法，使用λ语言编程，研究神经网络在执行整个程序的能力，旨在拓展神经网络在符号人工智能领域的应用。 |
| [^26] | [Revisiting the Role of Similarity and Dissimilarity inBest Counter Argument Retrieval.](http://arxiv.org/abs/2304.08807) | 本文研究了最佳反驳检索任务，提出了一种评分模型，使用相似性和差异性度量，达到了88.9％的accuracy@1，显著优于其他基线模型。 |
| [^27] | [Speaker Profiling in Multiparty Conversations.](http://arxiv.org/abs/2304.08801) | 本文提出了一个名为SPC的任务，旨在为对话中每个发言者生成个人特征摘要。任务被分为三个子任务：个人特征发现、个人特征类型识别和个人特征价值提取。任务对于银行、酒店预订和航空预订等行业中的聊天机器人非常重要，可以使聊天机器人更好地了解和回应每个发言人的需求。 |
| [^28] | [Sabi\'a: Portuguese Large Language Models.](http://arxiv.org/abs/2304.07880) | 针对葡萄牙语进行单语言预训练，可以显著提高大规模合成语言模型的质量，并能够在一系列葡萄牙语数据集上优于以英语为中心和多语言的对手，最好的模型的表现与GPT-3.5-turbo持平。 |
| [^29] | [PDF-VQA: A New Dataset for Real-World VQA on PDF Documents.](http://arxiv.org/abs/2304.06447) | 该研究提出了一个新的文档VQA数据集PDF-VQA，以多个页面的完整文档作为研究对象，通过机器学习模型识别与处理文档元素、结构和内容等方面，为解决真实世界中的文档理解问题提供新的资源。 |
| [^30] | [GPT detectors are biased against non-native English writers.](http://arxiv.org/abs/2304.02819) | 该研究发现，GPT检测器对非英语母语作者存在偏见，容易将其内容错误地分类为AI生成的内容。此外，简单的提示策略可以缓解这种偏见，同时规避GPT检测器，这表明GPT检测器可能会惩罚具有受限语言表达能力的作者。 |
| [^31] | [Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers.](http://arxiv.org/abs/2304.00215) | 本文提出了一种基于分层Transformer的方法，即REPORT，能够同时聚合关系路径和上下文，捕捉实体之间的联系和内在特性。它完全依赖于关系语义，并能自然地推广到完全归纳的设置中。在基准数据集上实现了最先进的性能。 |
| [^32] | [ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge.](http://arxiv.org/abs/2303.14070) | 本文介绍了一种在医学领域利用LLaMA模型微调的医疗聊天模型ChatDoctor。经过700多种疾病和其相应症状、药品和医疗检查的收集和处理，这种模型具有理解患者需求、提供建议和帮助的潜力。这些先进的语言模型集成到医疗保健中可以极大地改进医疗专业人员和患者的沟通方式。 |
| [^33] | [InferEM: Inferring the Speaker's Intention for Empathetic Dialogue Generation.](http://arxiv.org/abs/2212.06373) | 通过推断对话中最后一次发言来捕捉说话者的意图，提出了一种利用多头注意力的意图融合模块的共情对话生成模型InferEM。模型同时利用前几次发言预测最后一次发言，具有较高的可行性。 |
| [^34] | [Fighting FIRe with FIRE: Assessing the Validity of Text-to-Video Retrieval Benchmarks.](http://arxiv.org/abs/2210.05038) | 文本到视频检索困难，视频字幕数据集被用作检测基准，但存在根本缺陷导致假阴性匹配。通过纠正假阴性，最先进的模型提高了25%的召回率，需要注释更多数据，重新计算有效性分数，以得出更准确的结果。 |
| [^35] | [COLO: A Contrastive Learning based Re-ranking Framework for One-Stage Summarization.](http://arxiv.org/abs/2209.14569) | 本文提出了一种基于对比学习的一阶摘要重排序框架COLO，广泛的实验表明它能够提高CNN/DailyMail基准上一阶系统的抽取和生成结果，同时保持参数和推理效率，并且相比于多阶段系统，具有更高的训练和推理速度、以及可比的性能。 |
| [^36] | [Code Translation with Compiler Representations.](http://arxiv.org/abs/2207.03578) | 本文提出了一种将编译器中间表示与神经机器翻译相结合的方法，能够更好地捕捉代码的语义，从而提高了代码翻译的质量和实用性。 |
| [^37] | [Discourse-Aware Graph Networks for Textual Logical Reasoning.](http://arxiv.org/abs/2207.01450) | 本论文提出了逻辑结构约束建模和话语感知图网络（DAGNs）用于解决文本逻辑推理问答问题。DAGNs可以构建逻辑图并通过边缘推理机制和图特征更新来学习逻辑表示。该方法在实验中取得了良好的效果。 |
| [^38] | [CodeAttack: Code-Based Adversarial Attacks for Pre-trained Programming Language Models.](http://arxiv.org/abs/2206.00052) | CodeAttack是一个基于代码结构的黑盒攻击模型，能够生成有效、高效和难以察觉的对抗性攻击样本，并证明预训练编程语言模型存在易受代码特定的对抗性攻击的漏洞。 |
| [^39] | [Contrastive language and vision learning of general fashion concepts.](http://arxiv.org/abs/2204.03972) | 本文提出了一种对于时尚行业的CLIP-like模型—— FashionCLIP，它可以通过对视觉和语言的对比学习实现产品的检索、分类和定位，能够提供更具可转移性的产品表征。 |
| [^40] | [PMC-Patients: A Large-scale Dataset of Patient Summaries and Relations for Benchmarking Retrieval-based Clinical Decision Support Systems.](http://arxiv.org/abs/2202.13876) | 本文提出了一个名为“PMC-Patients”的新数据集，用于定义和测试病患到文章的检索（ReCDS-PAR）和病患到病患的检索（ReCDS-PPR），以评估基于召回的临床决策支持系统（ReCDS）的性能。PMC-Patients数据集涵盖逾10,000名病患信息和27,000篇PubMed Central文章，并展示了多种ReCDS系统的效果分析和20个案例的实用性分析。 |

# 详细

[^1]: 评估生成式搜索引擎中的可验证性

    Evaluating Verifiability in Generative Search Engines. (arXiv:2304.09848v1 [cs.CL])

    [http://arxiv.org/abs/2304.09848](http://arxiv.org/abs/2304.09848)

    本文评估了四个流行生成式搜索引擎的可验证性，发现现有生成式搜索引擎响应流畅但仅有51.5%的生成句子得到了完整的引用支持，仅有74.5%的引用支持其相关语句。

    

    生成式搜索引擎直接为用户查询生成响应，并提供内联引用。一个值得信赖的生成式搜索引擎的先决条件是可验证性，即系统应全面引用（高引用回忆率，所有语句都有完整的引用支持）和准确（高引用精度，每个引用都支持其相关语句）。我们对四个流行的生成式搜索引擎——Bing Chat、NeevaAI、perplexity.ai和YouChat——进行了人类评估，涵盖了各种来源的多样化查询（例如历史上的Google用户查询、Reddit上动态收集的开放性问题等）。我们发现现有的生成式搜索引擎响应流畅且信息丰富，但常常包含不支持的语句和不准确的引用：平均而言，仅有51.5%的生成句子得到了完整的引用支持，只有74.5%的引用支持其相关语句。我们认为...

    Generative search engines directly generate responses to user queries, along with in-line citations. A prerequisite trait of a trustworthy generative search engine is verifiability, i.e., systems should cite comprehensively (high citation recall; all statements are fully supported by citations) and accurately (high citation precision; every cite supports its associated statement). We conduct human evaluation to audit four popular generative search engines -- Bing Chat, NeevaAI, perplexity.ai, and YouChat -- across a diverse set of queries from a variety of sources (e.g., historical Google user queries, dynamically-collected open-ended questions on Reddit, etc.). We find that responses from existing generative search engines are fluent and appear informative, but frequently contain unsupported statements and inaccurate citations: on average, a mere 51.5% of generated sentences are fully supported by citations and only 74.5% of citations support their associated sentence. We believe that
    
[^2]: AI的公平性及其对社会的长期影响

    Fairness in AI and Its Long-Term Implications on Society. (arXiv:2304.09826v1 [cs.CY])

    [http://arxiv.org/abs/2304.09826](http://arxiv.org/abs/2304.09826)

    本文探讨了AI的公平性问题，指出缺乏AI公平性会加深偏见成为社会压力因素，可能对社会产生长期影响，因此需要寻求潜在解决方案。

    

    人工智能（AI）在各种设置中的成功部署已经为个人和社会带来了许多积极的成果。然而，由于预测的偏见，AI系统也被证明对部分人口造成了伤害。我们着眼于AI的公平性，分析了缺乏AI公平性时如何导致偏见随着时间的加深而成为社会压力因素。如果问题持续存在，可能会对社会产生不良的长期影响，并通过与其他风险的交互来加强。我们检查了提高AI公平性的当前策略，并评估它们在实际部署方面的限制，并探讨了确保我们在不损害社会重要部分的情况下获得AI的好处的潜在路径。

    Successful deployment of artificial intelligence (AI) in various settings has led to numerous positive outcomes for individuals and society. However, AI systems have also been shown to harm parts of the population due to biased predictions. We take a closer look at AI fairness and analyse how lack of AI fairness can lead to deepening of biases over time and act as a social stressor. If the issues persist, it could have undesirable long-term implications on society, reinforced by interactions with other risks. We examine current strategies for improving AI fairness, assess their limitations in terms of real-world deployment, and explore potential paths forward to ensure we reap AI's benefits without harming significant parts of the society.
    
[^3]: 一种自监督蒸馏的双阶段框架用于跨领域文本分类

    A Two-Stage Framework with Self-Supervised Distillation For Cross-Domain Text Classification. (arXiv:2304.09820v1 [cs.CL])

    [http://arxiv.org/abs/2304.09820](http://arxiv.org/abs/2304.09820)

    本文提出了一种双阶段框架，使用自监督蒸馏和来自不同但相关源领域的标记数据完成跨领域文本分类，取得在单源领域适应性和多源领域适应性上的新的最先进结果。

    

    跨领域文本分类旨在将模型适应于缺少标记数据的目标领域。它利用或重用不同但相关源领域的丰富标记数据和目标领域的未标记数据。为此，先前的工作要么专注于提取领域不变特征，要么忽略可能存在于目标领域中并对下游任务有用的领域感知特征的任务不可知特征。本文提出了一种双阶段框架，用于跨领域文本分类。在第一阶段，我们使用掩蔽语言建模（MLM）和来自源域的标记数据微调模型。在第二阶段，我们进一步使用自监督蒸馏（SSD）和来自目标域的未标记数据微调模型。我们基于公共的跨领域文本分类基准测试其性能，并实验结果表明，我们的方法在单源领域适应性和多源领域适应性上均取得了新的最先进结果。

    Cross-domain text classification aims to adapt models to a target domain that lacks labeled data. It leverages or reuses rich labeled data from the different but related source domain(s) and unlabeled data from the target domain. To this end, previous work focuses on either extracting domain-invariant features or task-agnostic features, ignoring domain-aware features that may be present in the target domain and could be useful for the downstream task. In this paper, we propose a two-stage framework for cross-domain text classification. In the first stage, we finetune the model with mask language modeling (MLM) and labeled data from the source domain. In the second stage, we further fine-tune the model with self-supervised distillation (SSD) and unlabeled data from the target domain. We evaluate its performance on a public cross-domain text classification benchmark and the experiment results show that our method achieves new state-of-the-art results for both single-source domain adaptat
    
[^4]: 一份关于日耳曼低资源语言和方言语料库的调查

    A Survey of Corpora for Germanic Low-Resource Languages and Dialects. (arXiv:2304.09805v1 [cs.CL])

    [http://arxiv.org/abs/2304.09805](http://arxiv.org/abs/2304.09805)

    本文针对德语低资源语言变体，通过系统的调查语料库，发现手工注释的语言资源稀少，主要涵盖形态句法，同时观察到此类研究的兴趣正在增加。

    

    尽管近年来取得了很大进展，但自然语言处理（NLP）中的大多数工作都是针对使用广泛的标准语言。在本研究中，我们将重点放在低资源语言，特别是非标准低资源语言上。即使在被认为已得到充分研究的主要语系中，对于这些语言变体可用资源的类型和范围以及NLP的主要挑战还知之甚少。解决这种情况的第一步是系统地调查可用的语料库（最重要的是，手工注释的语料库，这对NLP研究特别有价值）。在本文中，我们针对德语低资源语言变体提供了这样的调查。除了地理位置（说话者或文档的来源）外，我们发现手工注释的语言资源稀少，如果存在，主要涵盖形态句法。尽管缺乏资源，我们观察到这一领域的兴趣正在增加：

    Despite much progress in recent years, the vast majority of work in natural language processing (NLP) is on standard languages with many speakers. In this work, we instead focus on low-resource languages and in particular non-standardized low-resource languages. Even within branches of major language families, often considered well-researched, little is known about the extent and type of available resources and what the major NLP challenges are for these language varieties. The first step to address this situation is a systematic survey of available corpora (most importantly, annotated corpora, which are particularly valuable for NLP research). Focusing on Germanic low-resource language varieties, we provide such a survey in this paper. Except for geolocation (origin of speaker or document), we find that manually annotated linguistic resources are sparse and, if they exist, mostly cover morphosyntax. Despite this lack of resources, we observe that interest in this area is increasing: t
    
[^5]: GeneGPT: 教授大型语言模型使用NCBI Web API

    GeneGPT: Teaching Large Language Models to Use NCBI Web APIs. (arXiv:2304.09667v1 [cs.CL])

    [http://arxiv.org/abs/2304.09667](http://arxiv.org/abs/2304.09667)

    GeneGPT通过少量NCBI API调用URL请求作为演示，教授大型语言模型使用NCBI Web API回答基因组问题，并在GeneTuring测试中达到了优异的结果。

    

    本文介绍了GeneGPT，一种新颖的方法，用于教授大型语言模型（LLM）使用国家生物技术信息中心（NCBI）的Web应用程序编程接口（API），并回答基因组问题。具体而言，我们通过少量的NCBI API调用URL请求作为上下文学习的演示，启发Codex（code-davinci-002）解决GeneTuring测试。在推理过程中，一旦检测到调用请求，我们就停止解码并使用生成的URL进行API调用。我们然后将NCBI API返回的原始执行结果附加到生成的文本中，并继续生成直到找到答案或检测到另一个API调用。初步结果表明，GeneGPT在GeneTuring数据集的四个One-shot任务中取得了三个最先进的结果，在五个Zero-shot任务中取得了四个最先进的结果。总体而言，GeneGPT的宏平均分数为0.76，远高于检索增强LLM，如New Bin。

    In this paper, we present GeneGPT, a novel method for teaching large language models (LLMs) to use the Web Application Programming Interfaces (APIs) of the National Center for Biotechnology Information (NCBI) and answer genomics questions. Specifically, we prompt Codex (code-davinci-002) to solve the GeneTuring tests with few-shot URL requests of NCBI API calls as demonstrations for in-context learning. During inference, we stop the decoding once a call request is detected and make the API call with the generated URL. We then append the raw execution results returned by NCBI APIs to the generated texts and continue the generation until the answer is found or another API call is detected. Our preliminary results show that GeneGPT achieves state-of-the-art results on three out of four one-shot tasks and four out of five zero-shot tasks in the GeneTuring dataset. Overall, GeneGPT achieves a macro-average score of 0.76, which is much higher than retrieval-augmented LLMs such as the New Bin
    
[^6]: MPMQA：基于产品手册的多模态问答

    MPMQA: Multimodal Question Answering on Product Manuals. (arXiv:2304.09660v1 [cs.CL])

    [http://arxiv.org/abs/2304.09660](http://arxiv.org/abs/2304.09660)

    本文提出了一个多模态产品手册问答（MPMQA）任务，构建了一个大规模数据集PM209来支持该任务，要求模型不仅处理多模态内容，还提供多模态答案，有望提高产品手册理解效能。

    

    视觉内容，在产品手册理解中扮演着重要的角色。现有的产品手册问答（PMQA）数据集往往忽视了视觉内容，仅保留文本部分。为了强调多模态内容的重要性，本文提出了一个多模态产品手册问答（MPMQA）任务。对于每个问题，MPMQA要求模型不仅要处理多模态内容，还要提供多模态答案。为了支持MPMQA，构建了一个大规模数据集PM209作为人类注释，其中包含来自27个知名消费电子品牌的209个产品手册。人类注释包括手册内容的6种语义区域和22,021对问题和答案。特别地，每个答案都包含一个文本句子和相关的手册视觉区域。考虑到产品手册的长度和一个问题总是与少数页面相关，MPMQA的性能可以很好。

    Visual contents, such as illustrations and images, play a big role in product manual understanding. Existing Product Manual Question Answering (PMQA) datasets tend to ignore visual contents and only retain textual parts. In this work, to emphasize the importance of multimodal contents, we propose a Multimodal Product Manual Question Answering (MPMQA) task. For each question, MPMQA requires the model not only to process multimodal contents but also to provide multimodal answers. To support MPMQA, a large-scale dataset PM209 is constructed with human annotations, which contains 209 product manuals from 27 well-known consumer electronic brands. Human annotations include 6 types of semantic regions for manual contents and 22,021 pairs of question and answer. Especially, each answer consists of a textual sentence and related visual regions from manuals. Taking into account the length of product manuals and the fact that a question is always related to a small number of pages, MPMQA can be n
    
[^7]: BRENT: 双向检索增强的挪威语Transformer

    BRENT: Bidirectional Retrieval Enhanced Norwegian Transformer. (arXiv:2304.09649v1 [cs.CL])

    [http://arxiv.org/abs/2304.09649](http://arxiv.org/abs/2304.09649)

    BRENT是第一个使用双向检索提高挪威语的检索式语言模型，可以提高读者在提取问答方面的表现。

    

    检索式语言模型在问答任务中越来越受到重视。这些模型在语料库中搜索相关信息，而不是将所有事实性知识存储在它的参数中，从而提高了效率、透明度和适应性。我们通过调整REALM框架来开发第一个挪威语检索式模型，并在各种任务上进行了评估。在训练后，我们还将检索组件与语言模型（称为读者）分离，并展示了这可以在各种下游任务中进行微调。结果表明，检索增强的语言建模提高了读者在提取问答方面的表现，这表明这种类型的训练提高了语言模型使用上下文的一般能力，而这并不会牺牲其他能力，例如词性标注、依赖分析、命名实体识别和词形归并。代码、训练模型和数据可在https://github.com/salaniz/BRENT获取。

    Retrieval-based language models are increasingly employed in question-answering tasks. These models search in a corpus of documents for relevant information instead of having all factual knowledge stored in its parameters, thereby enhancing efficiency, transparency, and adaptability. We develop the first Norwegian retrieval-based model by adapting the REALM framework and evaluating it on various tasks. After training, we also separate the language model, which we call the reader, from the retriever components, and show that this can be fine-tuned on a range of downstream tasks. Results show that retrieval augmented language modeling improves the reader's performance on extractive question-answering, suggesting that this type of training improves language models' general ability to use context and that this does not happen at the expense of other abilities such as part-of-speech tagging, dependency parsing, named entity recognition, and lemmatization. Code, trained models, and data are 
    
[^8]: 连接自然语言处理与心理语言学：面向巴斯克语和西班牙语的基于语料库和知识库的计算语义相似性和相关数据集

    Bridging Natural Language Processing and Psycholinguistics: computationally grounded semantic similarity and relatedness datasets for Basque and Spanish. (arXiv:2304.09616v1 [cs.CL])

    [http://arxiv.org/abs/2304.09616](http://arxiv.org/abs/2304.09616)

    该论文提出了一个基于计算的词语相似性数据集，旨在弥补心理语言学研究中的空白，通过提供大量控制了在词汇处理中起重要作用的变量的名词对的语义相似性的量化。数据集包括巴斯克语和欧洲西班牙语的名词对信息，但进一步的工作意图将其扩展到更多语言。

    

    我们基于两个著名的自然语言处理资源：文本语料库和知识库，提出了一个基于计算的词语相似性数据集。该数据集旨在弥补心理语言学研究中的空白，通过提供大量控制了在词汇处理中起重要作用的变量的名词对的语义相似性的量化。数据集的创建包括三个步骤：1）为每个名词计算四个关键的心理语言学特征：具体性、频率、语义和音位邻近密度；2）在这些四个变量下对名词进行配对；3）对于每个名词对，分配三种类型的单词相似度测量，计算出文本、Wordnet和混合嵌入。目前的数据集包括巴斯克语和欧洲西班牙语的名词对信息，但进一步的工作意图将其扩展到更多语言。

    We present a computationally-grounded word similarity dataset based on two well-known Natural Language Processing resources; text corpora and knowledge bases. This dataset aims to fulfil a gap in psycholinguistic research by providing a variety of quantifications of semantic similarity in an extensive set of noun pairs controlled by variables that play a significant role in lexical processing. The dataset creation has consisted in three steps, 1) computing four key psycholinguistic features for each noun; concreteness, frequency, semantic and phonological neighbourhood density; 2) pairing nouns across these four variables; 3) for each noun pair, assigning three types of word similarity measurements, computed out of text, Wordnet and hybrid embeddings. The present dataset includes noun pairs' information in Basque and European Spanish, but further work intends to extend it to more languages.
    
[^9]: CB-Conformer: 针对偏差词识别的语境偏差Conformer

    CB-Conformer: Contextual biasing Conformer for biased word recognition. (arXiv:2304.09607v1 [cs.SD])

    [http://arxiv.org/abs/2304.09607](http://arxiv.org/abs/2304.09607)

    CB-Conformer 是一种为了提高有偏差词识别而提出的 Conformer，引入了 Contextual Biasing Module 和 Self-Adaptive Language Model 以更好地利用上下文信息和单词偏差信息，并在测试中取得显著提高。

    

    由于源域和目标域之间的差异，如何更好地利用有偏差的单词信息来提高目标领域中自动语音识别模型的性能成为一个热门研究话题。以前的方法要么使用固定的外部语言模型进行解码，要么引入一个庞大的偏差模块，导致适应性差，推理速度慢。在本研究中，我们提出了CB-Conformer，通过引入上下文偏差模块和自适应语言模型来改进有偏差的单词识别。上下文偏差模块将音频片段和上下文信息组合起来，仅有原始Conformer模型参数的0.2％。自适应语言模型根据有偏差的单词召回率和精确度修改其内部权重，使得自动语音识别模型更关注有偏差的单词，并比标准的固定语言模型更成功地集成。此外，我们在语音识别基准测试中对模型进行了评估，包括领域内和领域外的测试集，发现显著提高了成绩。

    Due to the mismatch between the source and target domains, how to better utilize the biased word information to improve the performance of the automatic speech recognition model in the target domain becomes a hot research topic. Previous approaches either decode with a fixed external language model or introduce a sizeable biasing module, which leads to poor adaptability and slow inference. In this work, we propose CB-Conformer to improve biased word recognition by introducing the Contextual Biasing Module and the Self-Adaptive Language Model to vanilla Conformer. The Contextual Biasing Module combines audio fragments and contextual information, with only 0.2% model parameters of the original Conformer. The Self-Adaptive Language Model modifies the internal weights of biased words based on their recall and precision, resulting in a greater focus on biased words and more successful integration with the automatic speech recognition model than the standard fixed language model. In addition
    
[^10]: ChatGPT是否具备情感对话能力？

    Is ChatGPT Equipped with Emotional Dialogue Capabilities?. (arXiv:2304.09582v1 [cs.CL])

    [http://arxiv.org/abs/2304.09582](http://arxiv.org/abs/2304.09582)

    本文研究了开发者OpenAI的ChatGPT情感对话能力，发现在情感回复生成方面表现良好，但在情感对话理解方面落后于监督模型。

    

    本文介绍了对OpenAI开发的先进语言模型ChatGPT的情感对话能力的研究。该研究通过一系列下游任务的实验评估了ChatGPT在情感对话理解和生成方面的表现。我们的发现表明，虽然ChatGPT在情感对话理解方面的表现仍然落后于监督模型，但在生成情感回复方面表现出有希望的结果。此外，该研究提出了未来研究方向的潜在途径。

    This report presents a study on the emotional dialogue capability of ChatGPT, an advanced language model developed by OpenAI. The study evaluates the performance of ChatGPT on emotional dialogue understanding and generation through a series of experiments on several downstream tasks. Our findings indicate that while ChatGPT's performance on emotional dialogue understanding may still lag behind that of supervised models, it exhibits promising results in generating emotional responses. Furthermore, the study suggests potential avenues for future research directions.
    
[^11]: SemEval 2023 任务6: LegalEval -- 理解法律文本

    SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts. (arXiv:2304.09548v1 [cs.CL])

    [http://arxiv.org/abs/2304.09548](http://arxiv.org/abs/2304.09548)

    SemEval 2023举办了LegalEval共享任务，即理解法律文本，包括 自动结构化和语义连贯化的法律文件（Task-A），法律命名实体识别（Task-B）以及自动预测法律案件结果和提供预测解释（Task-C）。26个团队提交了系统论文并在所有子任务中优于基准线，但仍有改进空间。

    

    在人口众多的国家，待处理的法律案件呈指数增长。有必要开发基于自然语言处理的技术，对法律文件进行处理和自动理解。为了促进在法律自然语言处理领域的研究，我们在 SemEval 2023 上组织了共享任务 LegalEval - 理解法律文本。LegalEval 任务有三个子任务：Task-A（修辞角色标记）是自动将法律文件结构化为语义连贯的单元，Task-B（法律命名实体识别）处理在法律文件中识别相关实体，而 Task-C（法院判决预测与解释）探索了自动预测法律案件结果以及提供预测解释的可能性。共有26个团队（分布在全球的约100名参与者）提交了系统论文。在每个子任务中，所提出的系统都优于基准线；但是，仍然有很大的改进空间。本文介绍了 LegalEval 任务的组织和细节，并概述了参与系统及其性能。

    In populous countries, pending legal cases have been growing exponentially. There is a need for developing NLP-based techniques for processing and automatically understanding legal documents. To promote research in the area of Legal NLP we organized the shared task LegalEval - Understanding Legal Texts at SemEval 2023. LegalEval task has three sub-tasks: Task-A (Rhetorical Roles Labeling) is about automatically structuring legal documents into semantically coherent units, Task-B (Legal Named Entity Recognition) deals with identifying relevant entities in a legal document and Task-C (Court Judgement Prediction with Explanation) explores the possibility of automatically predicting the outcome of a legal case along with providing an explanation for the prediction. In total 26 teams (approx. 100 participants spread across the world) submitted systems paper. In each of the sub-tasks, the proposed systems outperformed the baselines; however, there is a lot of scope for improvement. This pape
    
[^12]: 大型语言模型在信息检索中的排名能力研究——以ChatGPT为例

    Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. (arXiv:2304.09542v1 [cs.CL])

    [http://arxiv.org/abs/2304.09542](http://arxiv.org/abs/2304.09542)

    本文研究了生成性LLMs，如ChatGPT和GPT-4在信息检索中的相关性排名能力，实验结果表明，这些模型经适当指导后表现优异，有时甚至优于传统监督学习方法。将ChatGPT的排名能力提炼为专门模型在BEIR上的效果更优。

    

    大型语言模型（LLMs）已经证明具有remarkable能力，能够将一些零样本语言任务推广至其他领域。本文研究了ChatGPT和GPT-4等生成性LLMs的相关性排名在信息检索方面的能力。实验结果显示，经过适当的指导，ChatGPT和GPT-4可以在流行的信息检索基准上取得竞争优势，甚至有时优于监督学习方法。特别地，GPT-4在TREC数据集上的平均nDCG上表现优于完全微调的monoT5-3B，BEIR数据集上的平均nDCG上优于monoT5-3B 2.3个点，低资源语言Mr.TyDi上的平均nDCG上优于monoT5-3B 2.7个点。随后，我们探讨了将ChatGPT的排名能力提炼为专门的模型的潜力。我们训练的小型专门模型（训练于10K个ChatGPT生成的数据）在BEIR上的表现优于在400K个MS MARCO注释数据上训练的monoT5。代码可在www.github.com/sunnwe上复现。

    Large Language Models (LLMs) have demonstrated a remarkable ability to generalize zero-shot to various language-related tasks. This paper focuses on the study of exploring generative LLMs such as ChatGPT and GPT-4 for relevance ranking in Information Retrieval (IR). Surprisingly, our experiments reveal that properly instructed ChatGPT and GPT-4 can deliver competitive, even superior results than supervised methods on popular IR benchmarks. Notably, GPT-4 outperforms the fully fine-tuned monoT5-3B on MS MARCO by an average of 2.7 nDCG on TREC datasets, an average of 2.3 nDCG on eight BEIR datasets, and an average of 2.7 nDCG on ten low-resource languages Mr.TyDi. Subsequently, we delve into the potential for distilling the ranking capabilities of ChatGPT into a specialized model. Our small specialized model that trained on 10K ChatGPT generated data outperforms monoT5 trained on 400K annotated MS MARCO data on BEIR. The code to reproduce our results is available at www.github.com/sunnwe
    
[^13]: 控制文本生成中的关键词和位置

    Controlling keywords and their positions in text generation. (arXiv:2304.09516v1 [cs.CL])

    [http://arxiv.org/abs/2304.09516](http://arxiv.org/abs/2304.09516)

    本文研究了一种控制文本生成中关键词及其位置的新方法，通过使用特殊标记控制关键词相对位置，可以生成更符合用户意图的文本。

    

    文本生成中的挑战之一是按照用户意图控制生成结果。之前的研究提出了指定应包含在生成文本中的关键词的方法。然而，这还不足以生成反映用户意图的文本。例如，将重要关键词放在文本开头有助于吸引读者的注意力，但现有方法不允许这种灵活的控制。在本文中，我们解决了控制文本生成中关键词和其位置的新任务。为此，我们展示了一种使用特殊标记的方法，可以控制关键词的相对位置。摘要生成和故事生成实验结果表明，所提出的方法可以控制关键词及其位置。我们还证明，控制关键词位置可以生成比基线更符合用户意图的摘要文本。我们公开了我们的代码。

    One of the challenges in text generation is to control generation as intended by a user. Previous studies have proposed to specify the keywords that should be included in the generated text. However, this is insufficient to generate text which reflect the user intent. For example, placing the important keyword beginning of the text would helps attract the reader's attention, but existing methods do not enable such flexible control. In this paper, we tackle a novel task of controlling not only keywords but also the position of each keyword in the text generation. To this end, we show that a method using special tokens can control the relative position of keywords. Experimental results on summarization and story generation tasks show that the proposed method can control keywords and their positions. We also demonstrate that controlling the keyword positions can generate summary texts that are closer to the user's intent than baseline. We release our code.
    
[^14]: 情感融合在社交媒体上的应用：用于心理疾病检测的综述

    Emotion fusion for mental illness detection from social media: A survey. (arXiv:2304.09493v1 [cs.CL])

    [http://arxiv.org/abs/2304.09493](http://arxiv.org/abs/2304.09493)

    本文为全面综述通过分析社交媒体上用户生成的文章来使用情感信息进行心理疾病检测的方法。文章回顾了不同的融合策略及其优缺点，并讨论了该领域研究人员所面临的挑战和未来的研究方向。

    

    心理疾病是全球最普遍的公共卫生问题之一，对人们的生活和社会健康产生负面影响。随着社交媒体的普及，对于通过分析社交媒体上用户生成的文章来早期发现心理疾病的研究越来越受到关注。根据情绪和心理疾病之间的相关性，利用和融合情感信息已成为一个有价值的研究主题。本文全面综述了在社交媒体上结合情感融合的方法用于心理疾病检测。我们首先回顾了不同的融合策略及其优缺点。随后，我们讨论了该领域研究人员面临的主要挑战，包括有关数据集可用性和质量、算法性能和可解释性的问题。我们还提出了一些未来研究的潜在方向。

    Mental illnesses are one of the most prevalent public health problems worldwide, which negatively influence people's lives and society's health. With the increasing popularity of social media, there has been a growing research interest in the early detection of mental illness by analysing user-generated posts on social media. According to the correlation between emotions and mental illness, leveraging and fusing emotion information has developed into a valuable research topic. In this article, we provide a comprehensive survey of approaches to mental illness detection in social media that incorporate emotion fusion. We begin by reviewing different fusion strategies, along with their advantages and disadvantages. Subsequently, we discuss the major challenges faced by researchers working in this area, including issues surrounding the availability and quality of datasets, the performance of algorithms and interpretability. We additionally suggest some potential directions for future resea
    
[^15]: EC^2: 基于身体控制的新型紧急交流方案

    EC^2: Emergent Communication for Embodied Control. (arXiv:2304.09448v1 [cs.LG])

    [http://arxiv.org/abs/2304.09448](http://arxiv.org/abs/2304.09448)

    EC^2 提出一种新的紧急通信方案，用于视频语言预训练以进行少样本身体控制，实现了在 EmbodiedAI 基准测试上的最先进的少样本性能。

    

    身体控制需要代理通过多模态预训练快速学习如何在新环境中行动，其中视频演示包含所需的视觉和运动细节以进行低级别知觉和控制，语言指令支持通过抽象符号结构进行泛化。虽然最近的方法应用对比学习来强制两种模式之间的对齐，但我们假设更好地建模它们之间的互补差异可以带来更全面的表示以进行下游适应。为此，我们提出了 Emergent Communication for Embodied Control (EC^2)，这是一种新的方案，用于预训练视频语言表示以进行少样本身体控制。关键思想是通过紧急通信学习视频的“语言”，它桥接了视频细节的语义和自然语言的结构。我们使用语言模型学习视频轨迹，紧急语言和自然语言的身体表示，然后对它们进行微调以进行身体控制任务。我们在 EmbodiedAI 基准测试上评估 EC^2，在三个任务上实现了最先进的少样本性能。

    Embodied control requires agents to leverage multi-modal pre-training to quickly learn how to act in new environments, where video demonstrations contain visual and motion details needed for low-level perception and control, and language instructions support generalization with abstract, symbolic structures. While recent approaches apply contrastive learning to force alignment between the two modalities, we hypothesize better modeling their complementary differences can lead to more holistic representations for downstream adaption. To this end, we propose Emergent Communication for Embodied Control (EC^2), a novel scheme to pre-train video-language representations for few-shot embodied control. The key idea is to learn an unsupervised "language" of videos via emergent communication, which bridges the semantics of video details and structures of natural language. We learn embodied representations of video trajectories, emergent language, and natural language using a language model, whic
    
[^16]: 语言模型实现异构数据湖结构化视图生成的简单系统

    Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes. (arXiv:2304.09433v1 [cs.CL])

    [http://arxiv.org/abs/2304.09433](http://arxiv.org/abs/2304.09433)

    本文探讨了使用大型语言模型在不需要特定领域的训练和定制下实现生成可查询表格的简单系统，并给出了两种实现策略，在不同质量和成本中平衡。通过实验证明，该系统对不同类型文档生成的表格均高质量，且无需文档特定的定制。

    

    数据管理界长期以来的目标是开发出通用自动化系统，可以在不需要人力或特定领域的定制情况下摄取半结构化文档并输出可查询的表格。鉴于潜在文档的多样性，现有的最先进系统进行简化的假设并使用特定领域的训练。本文中，我们询问是否可以通过使用大型语言模型（LLMs）来保持广泛性。在广泛数据上预训练的LLMs可仅限于基于自然语言任务描述执行各种下游任务。我们提出并评估了由LLMs驱动的简单原型系统EVAPORATE。我们确定了实现该系统的两种基本不同策略：提示LLM直接从文档中提取值或提示LLM合成执行提取的代码。我们的评估显示，这两种方法之间存在成本-质量权衡。代码合成便宜，但比直接抽取远不准确。我们对四种不同类型文档的实验表明，EVAPORATE可以在不需要任何文档特定的定制情况下为各种文档类型生成高质量的表格。

    A long standing goal of the data management community is to develop general, automated systems that ingest semi-structured documents and output queryable tables without human effort or domain specific customization. Given the sheer variety of potential documents, state-of-the art systems make simplifying assumptions and use domain specific training. In this work, we ask whether we can maintain generality by using large language models (LLMs). LLMs, which are pretrained on broad data, can perform diverse downstream tasks simply conditioned on natural language task descriptions.  We propose and evaluate EVAPORATE, a simple, prototype system powered by LLMs. We identify two fundamentally different strategies for implementing this system: prompt the LLM to directly extract values from documents or prompt the LLM to synthesize code that performs the extraction. Our evaluations show a cost-quality tradeoff between these two approaches. Code synthesis is cheap, but far less accurate than dire
    
[^17]: TieFake: 标题-正文相似度和情感感知的假新闻检测

    TieFake: Title-Text Similarity and Emotion-Aware Fake News Detection. (arXiv:2304.09421v1 [cs.CL])

    [http://arxiv.org/abs/2304.09421](http://arxiv.org/abs/2304.09421)

    该论文提出了一种新颖的假新闻检测方法TieFake，通过检测新闻标题和正文之间的相似度和情感信息，同时通过BERT和ResNeSt学习文本和图像的表示，实现对多模态文本的假新闻检测。

    

    假新闻检测旨在检测在社交媒体平台上广泛传播的虚假新闻，这可能会对公众和政府产生负面影响。许多方法已经开发出来，以从新闻图像、文本或视频中获取相关信息。然而，这些方法可能会面临以下限制：（1）忽略新闻中固有的情感信息，这可能有益，因为它包含作者的主观意图；（2）对于新闻文章中标题与正文之间的关系（相似度）给予很少关注，这经常使用无关标题来吸引读者的注意力。为此，我们提出一种新颖的标题-正文相似度和情感感知假新闻检测（TieFake）方法，通过在统一框架中共同建模多模态上下文信息和作者情感来实现。具体而言，我们分别使用BERT和ResNeSt来学习文本和图像的表示，并利用发行方情感提取器。

    Fake news detection aims to detect fake news widely spreading on social media platforms, which can negatively influence the public and the government. Many approaches have been developed to exploit relevant information from news images, text, or videos. However, these methods may suffer from the following limitations: (1) ignore the inherent emotional information of the news, which could be beneficial since it contains the subjective intentions of the authors; (2) pay little attention to the relation (similarity) between the title and textual information in news articles, which often use irrelevant title to attract reader' attention. To this end, we propose a novel Title-Text similarity and emotion-aware Fake news detection (TieFake) method by jointly modeling the multi-modal context information and the author sentiment in a unified framework. Specifically, we respectively employ BERT and ResNeSt to learn the representations for text and images, and utilize publisher emotion extractor 
    
[^18]: 如何处理深度学习代码

    How to Do Things with Deep Learning Code. (arXiv:2304.09406v1 [cs.CL])

    [http://arxiv.org/abs/2304.09406](http://arxiv.org/abs/2304.09406)

    本文通过提取GPT-2的表示映射和案例研究，测试了对深度学习代码进行关键代码研究的潜力并证明了代码是关键人工智能和关键机器学习研究子领域的研究人员的分析重点的有效性，同时让人们关注普通用户如何与深度学习系统交互，并指导深度学习系统的行为。

    

    本文的前提观点是，基本理解大型语言模型的构成和功能至关紧急。为此，我们提取了OpenAI的GPT-2的表示映射，其中包括涉及模型的深度学习代码和构建在模型周围的应用程序的基础代码。然后，我们通过两个受欢迎的GPT-2应用程序（文本冒险游戏AI Dungeon和语言艺术项目This Word Does Not Exist）的案例研究来验证这个映射。这样的练习使我们能够测试对深度学习代码进行关键代码研究的潜力，并证明代码是关键人工智能和关键机器学习研究子领域的研究人员的分析重点的有效性。更广泛地说，我们的工作引起了人们对普通用户如何与深度学习系统交互，甚至指导深度学习系统的行为，并通过表达代理机构，抵制他们发现有害或不公正的系统的开发的注意。

    The premise of this article is that a basic understanding of the composition and functioning of large language models is critically urgent. To that end, we extract a representational map of OpenAI's GPT-2 with what we articulate as two classes of deep learning code, that which pertains to the model and that which underwrites applications built around the model. We then verify this map through case studies of two popular GPT-2 applications: the text adventure game, AI Dungeon, and the language art project, This Word Does Not Exist. Such an exercise allows us to test the potential of Critical Code Studies when the object of study is deep learning code and to demonstrate the validity of code as an analytical focus for researchers in the subfields of Critical Artificial Intelligence and Critical Machine Learning Studies. More broadly, however, our work draws attention to the means by which ordinary users might interact with, and even direct, the behavior of deep learning systems, and by ex
    
[^19]: MixPro：基于提示学习的简单有效的数据增强方式

    MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning. (arXiv:2304.09402v1 [cs.CL])

    [http://arxiv.org/abs/2304.09402](http://arxiv.org/abs/2304.09402)

    MixPro是一种数据增强方法，通过对原始输入和模板进行混合来提高基于提示的学习性能，平均提高了5.08%的模型性能。

    

    基于提示的学习通过将输入与模板组合起来，将下游任务重构为填空问题。这种技术在少样本学习中特别有用，然而，使用有限的模板和文本仍然存在显着的性能改进空间。此外，现有的使用模型集成的方法可以限制模型的效率。为解决这些问题，我们提出了一种称为MixPro的增强方法，它通过标记级、句子级和时代级的混合策略来增强原始输入文本和模板。我们在五个少样本数据集上进行了实验，结果表明MixPro优于其他增强基线，相比增强前，平均提高了5.08%的模型性能。

    Prompt-based learning reformulates downstream tasks as cloze problems by combining the original input with a template. This technique is particularly useful in few-shot learning, where a model is trained on a limited amount of data. However, the limited templates and text used in few-shot prompt-based learning still leave significant room for performance improvement. Additionally, existing methods using model ensembles can constrain the model efficiency. To address these issues, we propose an augmentation method called MixPro, which augments both the vanilla input text and the templates through token-level, sentence-level, and epoch-level Mixup strategies. We conduct experiments on five few-shot datasets, and the results show that MixPro outperforms other augmentation baselines, improving model performance by an average of 5.08% compared to before augmentation.
    
[^20]: 利用知识蒸馏压缩多语言神经机器翻译模型的实证研究

    An Empirical Study of Leveraging Knowledge Distillation for Compressing Multilingual Neural Machine Translation Models. (arXiv:2304.09388v1 [cs.CL])

    [http://arxiv.org/abs/2304.09388](http://arxiv.org/abs/2304.09388)

    本文研究了利用知识蒸馏方法压缩多语言神经机器翻译模型的实证效果，并以印地语到英语的翻译为案例展示了蒸馏方法对模型大小和性能的影响。研究发现，深层紧凑模型往往与浅层非紧凑模型一样好，将蒸馏模型在高质量子集上微调可以提高翻译质量。

    

    知识蒸馏是一种压缩神经模型的方法。然而，尽管MNMT（多语言神经机器翻译）的普及和优越性，但从大型MNMT模型中提取知识的研究实际上并不存在。本文填补了这一空白，提出了一种利用知识蒸馏压缩MNMT模型的实证研究。我们以印地语到英语的翻译为案例研究，并证明了常用的语言无关和语言感知的蒸馏方法可以使模型压缩4-5倍，但性能下降多达3.5 BLEU。为了缓解这一问题，我们进行了多个设计上的实验，包括深层模型和浅层模型、参数共享、多阶段训练和适配器等。我们观察到，深层紧凑模型往往与浅层非紧凑模型一样好，将蒸馏模型在高质量子集上微调可以稍微提高翻译质量。

    Knowledge distillation (KD) is a well-known method for compressing neural models. However, works focusing on distilling knowledge from large multilingual neural machine translation (MNMT) models into smaller ones are practically nonexistent, despite the popularity and superiority of MNMT. This paper bridges this gap by presenting an empirical investigation of knowledge distillation for compressing MNMT models. We take Indic to English translation as a case study and demonstrate that commonly used language-agnostic and language-aware KD approaches yield models that are 4-5x smaller but also suffer from performance drops of up to 3.5 BLEU. To mitigate this, we then experiment with design considerations such as shallower versus deeper models, heavy parameter sharing, multi-stage training, and adapters. We observe that deeper compact models tend to be as good as shallower non-compact ones, and that fine-tuning a distilled model on a High-Quality subset slightly boosts translation quality. 
    
[^21]: 洗牌和切割：长文本的对比学习

    Shuffle & Divide: Contrastive Learning for Long Text. (arXiv:2304.09374v1 [cs.CL])

    [http://arxiv.org/abs/2304.09374](http://arxiv.org/abs/2304.09374)

    本文介绍了一种基于对比学习的自监督学习方法，通过“洗牌和切割”算法对长文本进行预处理，提取BERT嵌入，在无监督的情况下对文本进行分类，比当前最先进的技术提高20.94%。

    

    我们提出了一种基于对比学习的自监督学习方法，用于长文本文档。我们的方法的关键在于“洗牌和切割”（SaD），这是一种简单的文本增广算法，可为基于BERT的文档嵌入所需的对比更新设置一个前置任务。SaD将文档拆分为两个子文档，其中包含随机洗牌的单词。这些子文档被视为正样本，将所有其他文档视为负样本。在SaD之后，我们重复对比更新和聚类阶段，直至收敛。我们的方法可以帮助减轻人力资源的工作量，从而节省昂贵的AI资源。我们通过对20 Newsgroups、Reuters-21578、BBC和BBCSport数据集进行无监督文本分类的经验评估。特别是，我们的方法在20 Newsgroups上将当前的最新技术SS-SB-MT提高了20.94％。

    We propose a self-supervised learning method for long text documents based on contrastive learning. A key to our method is Shuffle and Divide (SaD), a simple text augmentation algorithm that sets up a pretext task required for contrastive updates to BERT-based document embedding. SaD splits a document into two sub-documents containing randomly shuffled words in the entire documents. The sub-documents are considered positive examples, leaving all other documents in the corpus as negatives. After SaD, we repeat the contrastive update and clustering phases until convergence. It is naturally a time-consuming, cumbersome task to label text documents, and our method can help alleviate human efforts, which are most expensive resources in AI. We have empirically evaluated our method by performing unsupervised text classification on the 20 Newsgroups, Reuters-21578, BBC, and BBCSport datasets. In particular, our method pushes the current state-of-the-art, SS-SB-MT, on 20 Newsgroups by 20.94% in
    
[^22]: LLM作为机器人的大脑：统一自我中心记忆与控制

    LLM as A Robotic Brain: Unifying Egocentric Memory and Control. (arXiv:2304.09349v1 [cs.AI])

    [http://arxiv.org/abs/2304.09349](http://arxiv.org/abs/2304.09349)

    本文提出了一个统一自我中心记忆和控制的框架LLM-Brain，使用大规模语言模型作为机器人大脑进行零-shot学习。该框架包括封闭式多轮对话，覆盖了感知、规划、控制和记忆，具有很好的泛化性能，适用于多个机器人任务。

    

    体感人工智能研究和开发具备物理或虚拟实体（即机器人）并能够与环境动态交互的智能系统。记忆和控制是体感系统的两个基本部分，通常需要分别使用框架进行建模。本文提出了一个新的、可推广的框架，称为LLM-Brain：使用大规模语言模型作为机器人大脑，统一自我中心记忆和控制。LLM-Brain框架集成了多个多模态语言模型用于机器人任务，利用零-shot学习方法。LLM-Brain中的所有组件使用自然语言进行封闭式多轮对话，包括感知、规划、控制和记忆。系统的核心是一个具备自我中心记忆和控制机器人的实体LLM。我们通过研究两个下游任务：主动探索和实体问答来演示LLM-Brain。

    Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The
    
[^23]: 禁止数字技术的意外后果——以意大利ChatGPT禁令为例的证据

    The Unintended Consequences of Censoring Digital Technology -- Evidence from Italy's ChatGPT Ban. (arXiv:2304.09339v1 [econ.GN])

    [http://arxiv.org/abs/2304.09339](http://arxiv.org/abs/2304.09339)

    意大利禁止ChatGPT产生了短期的生产力干扰，但也导致了审查绕过工具的显著增加。

    

    本文分析了ChatGPT，一种生成式预训练变压器聊天机器人被禁止对个人生产率的影响。我们首先收集了超过8,000名专业GitHub用户在意大利和其他欧洲国家的每小时编码产出数据，以分析禁令对个人生产力的影响。将高频率数据与禁令的突然宣布结合在一起，运用差异法，我们发现在禁令后的前两个工作日，意大利开发者的产出减少了约50％，之后逐渐恢复。运用合成控制方法来分析每日Google搜索和Tor使用数据，结果显示，该禁令导致了绕过审查的工具的使用量显著增加。我们的研究结果表明，用户很快采取绕过互联网限制的策略，但这种适应活动会造成短期的干扰和影响生产力。

    We analyse the effects of the ban of ChatGPT, a generative pre-trained transformer chatbot, on individual productivity. We first compile data on the hourly coding output of over 8,000 professional GitHub users in Italy and other European countries to analyse the impact of the ban on individual productivity. Combining the high-frequency data with the sudden announcement of the ban in a difference-in-differences framework, we find that the output of Italian developers decreased by around 50% in the first two business days after the ban and recovered after that. Applying a synthetic control approach to daily Google search and Tor usage data shows that the ban led to a significant increase in the use of censorship bypassing tools. Our findings show that users swiftly implement strategies to bypass Internet restrictions but this adaptation activity creates short-term disruptions and hampers productivity.
    
[^24]: BIM信息检索的Prompt-Based虚拟助手框架：BIM-GPT

    BIM-GPT: a Prompt-Based Virtual Assistant Framework for BIM Information Retrieval. (arXiv:2304.09333v1 [cs.CL])

    [http://arxiv.org/abs/2304.09333](http://arxiv.org/abs/2304.09333)

    该论文提出了一种基于提示的虚拟助手框架——BIM-GPT，通过集成BIM和GPT技术，支持自然语言检索，并在BIM IR数据集测试中获得了高精度。同时，对医院建筑的VA原型验证了该框架的功能，为建筑业中BIM IR的多功能VA开发做出了贡献。

    

    因需要深入的BIM知识或大量工程自动化，从建筑信息模型（BIM）中高效地检索信息（IR）面临重要挑战。我们介绍了BIM-GPT，一个基于提示的虚拟助手（VA）框架，集成BIM和生成预训练变形器（GPT）技术，支持基于自然语言的IR。提示管理器和动态模板为GPT模型生成提示，使其能够解释NL查询，总结检索到的信息以及回答与BIM相关的问题。在对BIM IR数据集的测试中，我们的方法在没有数据和2％数据合并在提示中的情况下，分别实现了83.5％和99.5％的NL查询分类精度。此外，我们通过针对医院建筑的VA原型验证了BIM-GPT的功能。本研究为建筑业中BIM IR的有效和多功能VA的开发做出了贡献，显着提高了BIM的可访问性并减少了工程难度。

    Efficient information retrieval (IR) from building information models (BIMs) poses significant challenges due to the necessity for deep BIM knowledge or extensive engineering efforts for automation. We introduce BIM-GPT, a prompt-based virtual assistant (VA) framework integrating BIM and generative pre-trained transformer (GPT) technologies to support NL-based IR. A prompt manager and dynamic template generate prompts for GPT models, enabling interpretation of NL queries, summarization of retrieved information, and answering BIM-related questions. In tests on a BIM IR dataset, our approach achieved 83.5% and 99.5% accuracy rates for classifying NL queries with no data and 2% data incorporated in prompts, respectively. Additionally, we validated the functionality of BIM-GPT through a VA prototype for a hospital building. This research contributes to the development of effective and versatile VAs for BIM IR in the construction industry, significantly enhancing BIM accessibility and reduc
    
[^25]: 一种神经λ演算法：神经符号人工智能遇见计算和函数式编程的基础。

    A Neural Lambda Calculus: Neurosymbolic AI meets the foundations of computing and functional programming. (arXiv:2304.09276v1 [cs.LG])

    [http://arxiv.org/abs/2304.09276](http://arxiv.org/abs/2304.09276)

    本文提出了一种神经λ演算法，使用λ语言编程，研究神经网络在执行整个程序的能力，旨在拓展神经网络在符号人工智能领域的应用。

    

    在过去几十年中，基于深度神经网络的模型成为了机器学习中的主导范式。最近，人们越来越认为在符号学习中使用人工神经网络是越来越相关的。为了研究神经网络在符号人工智能领域的能力，研究人员已经探索了深度神经网络学习数学构造（如加法和乘法）、逻辑推理（如定理证明器）甚至执行计算机程序的能力。然而，后者对于神经网络来说是太复杂的任务，结果并不总是成功的，并且往往需要在学习过程中引入有偏见的元素，以限制可能要执行的程序的范围。在这项工作中，我们将分析神经网络学习如何执行整个程序的能力。为此，我们提出了一种不同的方法。我们不使用命令式编程语言，而是采用λ语言进行编程。

    Over the last decades, deep neural networks based-models became the dominant paradigm in machine learning. Further, the use of artificial neural networks in symbolic learning has been seen as increasingly relevant recently. To study the capabilities of neural networks in the symbolic AI domain, researchers have explored the ability of deep neural networks to learn mathematical constructions, such as addition and multiplication, logic inference, such as theorem provers, and even the execution of computer programs. The latter is known to be too complex a task for neural networks. Therefore, the results were not always successful, and often required the introduction of biased elements in the learning process, in addition to restricting the scope of possible programs to be executed. In this work, we will analyze the ability of neural networks to learn how to execute programs as a whole. To do so, we propose a different approach. Instead of using an imperative programming language, with com
    
[^26]: 重新审视相似性和差异性在最佳反驳检索中的作用

    Revisiting the Role of Similarity and Dissimilarity inBest Counter Argument Retrieval. (arXiv:2304.08807v1 [cs.CL])

    [http://arxiv.org/abs/2304.08807](http://arxiv.org/abs/2304.08807)

    本文研究了最佳反驳检索任务，提出了一种评分模型，使用相似性和差异性度量，达到了88.9％的accuracy@1，显著优于其他基线模型。

    

    本文研究了在给定输入论点情况下的最佳反驳检索任务。根据最佳反驳定义，最佳反驳应与输入论点在细节方面相似，但立场相反。本文旨在开发一种基于相似性和差异性度量的高效和有效的模型来对反驳进行评分。我们首先对现有的评分方法（包括传统的学习排序（LTR）和最近的神经评分模型）进行了实证研究。然后，我们提出了Bipolar-encoder，一种基于BERT的新型模型，用于学习同时相似性和差异性的最优表示。实验结果表明，我们提出的方法可以达到88.9％的accuracy@1，显著优于其他基线模型。当与适当的缓存技术结合使用时，Bipolar-encoder在预测时间上也具有可比性的效率。

    This paper studies the task of best counter-argument retrieval given an input argument. Following the definition that the best counter-argument addresses the same aspects as the input argument while having the opposite stance, we aim to develop an efficient and effective model for scoring counter-arguments based on similarity and dissimilarity metrics. We first conduct an experimental study on the effectiveness of available scoring methods, including traditional Learning-To-Rank (LTR) and recent neural scoring models. We then propose Bipolar-encoder, a novel BERT-based model to learn an optimal representation for simultaneous similarity and dissimilarity. Experimental results show that our proposed method can achieve the accuracy@1 of 88.9\%, which significantly outperforms other baselines by a large margin. When combined with an appropriate caching technique, Bipolar-encoder is comparably efficient at prediction time.
    
[^27]: 多方会话中的发言人个人特征分析

    Speaker Profiling in Multiparty Conversations. (arXiv:2304.08801v1 [cs.CL])

    [http://arxiv.org/abs/2304.08801](http://arxiv.org/abs/2304.08801)

    本文提出了一个名为SPC的任务，旨在为对话中每个发言者生成个人特征摘要。任务被分为三个子任务：个人特征发现、个人特征类型识别和个人特征价值提取。任务对于银行、酒店预订和航空预订等行业中的聊天机器人非常重要，可以使聊天机器人更好地了解和回应每个发言人的需求。

    

    在对话环境中，个体展现出独特的行为，使得“一刀切”的方法不足以为对话代理生成回应。虽然过去的研究旨在使用发言人个人信息创建个性化对话代理，但它们依赖于前提，即发言人个人特征已经被提供。然而，在像银行、酒店预订和航空预订等行业中使用的聊天机器人方面，这一假设并不总是正确的。本文旨在通过探索对话中的发言人个人特征分析 (SPC)任务来填补这个空白。SPC的主要目标是为对话中每个发言人产生个人特征摘要。为了实现这一目标，我们将任务分为三个子任务：个人特征发现、个人特征类型识别和个人特征价值提取。在给定对话的情况下，第一个子任务旨在识别包含个人信息的所有话语。

    In conversational settings, individuals exhibit unique behaviors, rendering a one-size-fits-all approach insufficient for generating responses by dialogue agents. Although past studies have aimed to create personalized dialogue agents using speaker persona information, they have relied on the assumption that the speaker's persona is already provided. However, this assumption is not always valid, especially when it comes to chatbots utilized in industries like banking, hotel reservations, and airline bookings. This research paper aims to fill this gap by exploring the task of Speaker Profiling in Conversations (SPC). The primary objective of SPC is to produce a summary of persona characteristics for each individual speaker present in a dialogue. To accomplish this, we have divided the task into three subtasks: persona discovery, persona-type identification, and persona-value extraction. Given a dialogue, the first subtask aims to identify all utterances that contain persona information.
    
[^28]: Sabiá: 葡萄牙的大型语言模型

    Sabi\'a: Portuguese Large Language Models. (arXiv:2304.07880v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.07880](http://arxiv.org/abs/2304.07880)

    针对葡萄牙语进行单语言预训练，可以显著提高大规模合成语言模型的质量，并能够在一系列葡萄牙语数据集上优于以英语为中心和多语言的对手，最好的模型的表现与GPT-3.5-turbo持平。

    

    随着语言模型能力的不断提高，”一刀切“的模型仍然是主流。尤其是考虑到全球使用的语言数量非常庞大，并且其中很多语言都是低资源语言，主要的做法是对多种语言进行预训练。本文对这种做法提出了质疑，证明了针对目标语言进行单语言预训练可以显著提高大规模合成语言模型的质量。我们在本文中进一步介绍了用3%或更少的原始预训练预算在葡萄牙语文本上进一步预训练GPT-J和LLaMA模型。我们在Poeta（一套由14个葡萄牙语数据集组成的套件）上进行了少样本评估，结果显示我们的模型在表现上远优于以英语为中心的和多语言的对手。我们的最佳模型Sabiá-65B的表现与GPT-3.5-turbo持平。我们在目标语言中已经设想了数据集，以及经过翻译的数据集上都进行了评估。

    As the capabilities of language models continue to advance, it is conceivable that "one-size-fits-all" model will remain as the main paradigm. For instance, given the vast number of languages worldwide, many of which are low-resource, the prevalent practice is to pretrain a single model on multiple languages. In this paper, we add to the growing body of evidence that challenges this practice, demonstrating that monolingual pretraining on the target language significantly improves models already extensively trained on diverse corpora. More specifically, we further pretrain GPT-J and LLaMA models on Portuguese texts using 3% or less of their original pretraining budget. Few-shot evaluations on Poeta, a suite of 14 Portuguese datasets, reveal that our models outperform English-centric and multilingual counterparts by a significant margin. Our best model, Sabi\'a-65B, performs on par with GPT-3.5-turbo. By evaluating on datasets originally conceived in the target language as well as transl
    
[^29]: PDF-VQA: 一个新的用于PDF文件真实世界VQA的数据集

    PDF-VQA: A New Dataset for Real-World VQA on PDF Documents. (arXiv:2304.06447v1 [cs.CV])

    [http://arxiv.org/abs/2304.06447](http://arxiv.org/abs/2304.06447)

    该研究提出了一个新的文档VQA数据集PDF-VQA，以多个页面的完整文档作为研究对象，通过机器学习模型识别与处理文档元素、结构和内容等方面，为解决真实世界中的文档理解问题提供新的资源。

    

    基于文档的视觉问答（VQA）研究文档图像的文档理解问题。我们提出了一个新的基于文档的VQA数据集PDF-VQA，从文档元素识别、文档布局结构理解以及上下文理解和关键信息提取等各个方面全面探讨文档理解问题。我们的PDF-VQA数据集将文档理解的规模从单个文档页面扩展到询问多个页面的完整文档。我们还提出了一个新的基于图形的VQA模型，明确地集成了不同文档元素之间的空间和层次结构关系，以提高文档结构的理解能力。该性能与多个基线模型相比较，可以适用于不同的问题类型和任务。

    Document-based Visual Question Answering examines the document understanding of document images in conditions of natural language questions. We proposed a new document-based VQA dataset, PDF-VQA, to comprehensively examine the document understanding from various aspects, including document element recognition, document layout structural understanding as well as contextual understanding and key information extraction. Our PDF-VQA dataset extends the current scale of document understanding that limits on the single document page to the new scale that asks questions over the full document of multiple pages. We also propose a new graph-based VQA model that explicitly integrates the spatial and hierarchically structural relationships between different document elements to boost the document structural understanding. The performances are compared with several baselines over different question types and tasks\footnote{The full dataset will be released after paper acceptance.
    
[^30]: GPT检测器对非英语母语的作者存在偏见。

    GPT detectors are biased against non-native English writers. (arXiv:2304.02819v1 [cs.CL])

    [http://arxiv.org/abs/2304.02819](http://arxiv.org/abs/2304.02819)

    该研究发现，GPT检测器对非英语母语作者存在偏见，容易将其内容错误地分类为AI生成的内容。此外，简单的提示策略可以缓解这种偏见，同时规避GPT检测器，这表明GPT检测器可能会惩罚具有受限语言表达能力的作者。

    

    生成语言模型的快速推广带来了数字通信方面的实质性进展，同时也引发了AI生成内容潜在误用的担忧。虽然已经提出了许多检测方法来区分AI和人类生成的内容，但这些检测器的公平性和鲁棒性仍未得到充分探讨。在这项研究中，我们使用来自英语母语和非英语母语作者的写作样本评估了几种广泛使用的GPT检测器的性能表现。我们的研究发现，这些检测器持续将非英语母语的写作样本错误地分类为AI生成的内容，而原生写作样本则能够被准确识别。此外，我们证明了简单的提示策略不仅可以缓解这种偏见，而且还可以有效地规避GPT检测器，这表明GPT检测器可能无意中惩罚具有受限语言表达能力的作者。我们的研究结果呼吁进行更广泛的讨论。

    The rapid adoption of generative language models has brought about substantial advancements in digital communication, while simultaneously raising concerns regarding the potential misuse of AI-generated content. Although numerous detection methods have been proposed to differentiate between AI and human-generated content, the fairness and robustness of these detectors remain underexplored. In this study, we evaluate the performance of several widely-used GPT detectors using writing samples from native and non-native English writers. Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified. Furthermore, we demonstrate that simple prompting strategies can not only mitigate this bias but also effectively bypass GPT detectors, suggesting that GPT detectors may unintentionally penalize writers with constrained linguistic expressions. Our results call for a broader conversati
    
[^31]: 基于分层Transformer的关系路径和上下文归纳关系预测方法

    Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v1 [cs.CL])

    [http://arxiv.org/abs/2304.00215](http://arxiv.org/abs/2304.00215)

    本文提出了一种基于分层Transformer的方法，即REPORT，能够同时聚合关系路径和上下文，捕捉实体之间的联系和内在特性。它完全依赖于关系语义，并能自然地推广到完全归纳的设置中。在基准数据集上实现了最先进的性能。

    

    在知识图谱中进行关系预测是一个重要的研究课题。现有的嵌入式方法主要依赖于转导设置，缺乏归纳能力，无法推广到新的实体上进行推理。本文提出了一种新方法，通过使用统一的分层Transformer框架，即REPORT，同时聚合关系路径和上下文，捕捉实体之间的联系和内在特性，这种方法完全依赖于关系语义，并能自然地推广到完全归纳的设置中。在实验中，REPORT表现优于所有基线方法，甚至在两个完全归纳的数据集的八个版本子集上也是如此。此外，REPORT能够将推理推广到训练和推理中没有公共实体的新实体上，并在基准数据集上实现了最先进的性能。

    Relation prediction on knowledge graphs (KGs) is a key research topic. Dominant embedding-based methods mainly focus on the transductive setting and lack the inductive ability to generalize to new entities for inference. Existing methods for inductive reasoning mostly mine the connections between entities, i.e., relational paths, without considering the nature of head and tail entities contained in the relational context. This paper proposes a novel method that captures both connections between entities and the intrinsic nature of entities, by simultaneously aggregating RElational Paths and cOntext with a unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely on relation semantics and can naturally generalize to the fully-inductive setting, where KGs for training and inference have no common entities. In the experiments, REPORT performs consistently better than all baselines on almost all the eight version subsets of two fully-inductive datasets. Moreover. REPO
    
[^32]: ChatDoctor：使用医学领域知识在LLaMA模型上微调的医疗聊天模型

    ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge. (arXiv:2303.14070v1 [cs.CL])

    [http://arxiv.org/abs/2303.14070](http://arxiv.org/abs/2303.14070)

    本文介绍了一种在医学领域利用LLaMA模型微调的医疗聊天模型ChatDoctor。经过700多种疾病和其相应症状、药品和医疗检查的收集和处理，这种模型具有理解患者需求、提供建议和帮助的潜力。这些先进的语言模型集成到医疗保健中可以极大地改进医疗专业人员和患者的沟通方式。

    

    最近，在一般领域中应用的大型语言模型（LLM），例如ChatGPT，已经表现出仿佛是人类讲话般的成功。然而，这样的语言模型并没有经过个别且仔细为医学领域学习，导致诊断准确度低且不能给出正确的医疗诊断、药品等建议。为了解决这个问题，我们收集了700多种疾病及其相应症状、推荐药品和所需医疗检查，然后生成了5K名医患的对话。通过微调医患对话模型，这些模型具有了理解患者需求、提供明智建议并在各种医疗相关领域提供宝贵帮助的巨大潜力。将这些先进的语言模型集成到医疗保健中，可以彻底改变医疗专业人员和患者的沟通方式，最终改善整体质量。

    Recent large language models (LLMs) in the general domain, such as ChatGPT, have shown remarkable success in following instructions and producing human-like responses. However, such language models have not been learned individually and carefully for the medical domain, resulting in poor diagnostic accuracy and inability to give correct recommendations for medical diagnosis, medications, etc. To address this issue, we collected more than 700 diseases and their corresponding symptoms, recommended medications, and required medical tests, and then generated 5K doctor-patient conversations. By fine-tuning models of doctor-patient conversations, these models emerge with great potential to understand patients' needs, provide informed advice, and offer valuable assistance in a variety of medical-related fields. The integration of these advanced language models into healthcare can revolutionize the way healthcare professionals and patients communicate, ultimately improving the overall quality 
    
[^33]: InferEM: 推断说话者意图的共情对话生成模型

    InferEM: Inferring the Speaker's Intention for Empathetic Dialogue Generation. (arXiv:2212.06373v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.06373](http://arxiv.org/abs/2212.06373)

    通过推断对话中最后一次发言来捕捉说话者的意图，提出了一种利用多头注意力的意图融合模块的共情对话生成模型InferEM。模型同时利用前几次发言预测最后一次发言，具有较高的可行性。

    

    目前，共情回复生成的方法一般直接编码整个对话历史，然后通过解码器生成友好的反馈。这些方法强调建模情境信息，但忽视了捕捉说话者的直接意图。我们认为对话中最后一次发言表达了说话者的意图。因此，我们提出了一种名为InferEM的新模型用于共情回复生成。我们将最后一次发言单独编码，通过基于多头注意力的意图融合模块与整个对话融合以捕捉说话者的意图。此外，我们利用前几次发言预测最后一次发言，以模拟人类的心理，猜测对话者可能提前说些什么。为平衡发言预测和回复生成的优化速率，InferEM还设计了一种多任务学习策略。实验结果证明了该模型的可行性。

    Current approaches to empathetic response generation typically encode the entire dialogue history directly and put the output into a decoder to generate friendly feedback. These methods focus on modelling contextual information but neglect capturing the direct intention of the speaker. We argue that the last utterance in the dialogue empirically conveys the intention of the speaker. Consequently, we propose a novel model named InferEM for empathetic response generation. We separately encode the last utterance and fuse it with the entire dialogue through the multi-head attention based intention fusion module to capture the speaker's intention. Besides, we utilize previous utterances to predict the last utterance, which simulates human's psychology to guess what the interlocutor may speak in advance. To balance the optimizing rates of the utterance prediction and response generation, a multi-task learning strategy is designed for InferEM. Experimental results demonstrate the plausibility
    
[^34]: 使用FIRE对抗FIRe：评估文本到视频检索基准的有效性

    Fighting FIRe with FIRE: Assessing the Validity of Text-to-Video Retrieval Benchmarks. (arXiv:2210.05038v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.05038](http://arxiv.org/abs/2210.05038)

    文本到视频检索困难，视频字幕数据集被用作检测基准，但存在根本缺陷导致假阴性匹配。通过纠正假阴性，最先进的模型提高了25%的召回率，需要注释更多数据，重新计算有效性分数，以得出更准确的结果。

    

    搜索具有文本描述的视频是一项核心的多模式检索任务。由于没有专门针对文本到视频检索的数据集，视频字幕数据集被重新用于通过(1)将字幕视为其各自视频的正匹配项和(2)假定所有其他视频为负匹配项来评估模型。然而，这种方法在评估过程中存在一个根本缺陷：由于只有原始视频标记为相关字幕，许多替代视频也匹配该字幕，导致了假阴性的字幕-视频对。我们展示了当这些假阴性得到纠正时，最近的一个最先进的模型将获得25\%的召回率提升 - 这种差异威胁了基准本身的有效性。为了诊断和缓解这个问题，我们注释并发布了683K个额外的字幕-视频对。使用这些数据，我们在两个标准基准数据集（MSR-VTT和MSVD）上重新计算了三个模型的有效性分数。

    Searching troves of videos with textual descriptions is a core multimodal retrieval task. Owing to the lack of a purpose-built dataset for text-to-video retrieval, video captioning datasets have been re-purposed to evaluate models by (1) treating captions as positive matches to their respective videos and (2) assuming all other videos to be negatives. However, this methodology leads to a fundamental flaw during evaluation: since captions are marked as relevant only to their original video, many alternate videos also match the caption, which introduces false-negative caption-video pairs. We show that when these false negatives are corrected, a recent state-of-the-art model gains 25\% recall points -- a difference that threatens the validity of the benchmark itself. To diagnose and mitigate this issue, we annotate and release 683K additional caption-video pairs. Using these, we recompute effectiveness scores for three models on two standard benchmarks (MSR-VTT and MSVD). We find that (1)
    
[^35]: COLO：基于对比学习的一阶摘要重排序框架

    COLO: A Contrastive Learning based Re-ranking Framework for One-Stage Summarization. (arXiv:2209.14569v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.14569](http://arxiv.org/abs/2209.14569)

    本文提出了一种基于对比学习的一阶摘要重排序框架COLO，广泛的实验表明它能够提高CNN/DailyMail基准上一阶系统的抽取和生成结果，同时保持参数和推理效率，并且相比于多阶段系统，具有更高的训练和推理速度、以及可比的性能。

    

    传统的抽取式和生成式摘要系统的训练范式通常只使用令牌级别或句子级别的训练目标，但是，总结输出总是从摘要级别进行评估，这导致训练和评估不一致。在本文中，我们提出了一种基于对比学习的一阶摘要重排序框架COLO。通过建模对比目标，我们展示了摘要模型能够根据摘要级别得分直接生成摘要，而无需额外的模块和参数。广泛的实验表明，COLO提高了CNN/DailyMail基准上一阶系统的抽取和生成结果，同时保持了参数效率和推理效率，ROUGE-1分数分别达到44.58和46.33。与最先进的多阶段系统相比，我们在训练时节省了100多个GPU小时，在推理期间获得了3~8倍的加速比，同时保持了可比的性能。

    Traditional training paradigms for extractive and abstractive summarization systems always only use token-level or sentence-level training objectives. However, the output summary is always evaluated from summary-level which leads to the inconsistency in training and evaluation. In this paper, we propose a Contrastive Learning based re-ranking framework for one-stage summarization called COLO. By modeling a contrastive objective, we show that the summarization model is able to directly generate summaries according to the summary-level score without additional modules and parameters. Extensive experiments demonstrate that COLO boosts the extractive and abstractive results of one-stage systems on CNN/DailyMail benchmark to 44.58 and 46.33 ROUGE-1 score while preserving the parameter efficiency and inference efficiency. Compared with state-of-the-art multi-stage systems, we save more than 100 GPU training hours and obtaining 3~8 speed-up ratio during inference while maintaining comparable 
    
[^36]: 利用编译器中间表示进行代码翻译

    Code Translation with Compiler Representations. (arXiv:2207.03578v4 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2207.03578](http://arxiv.org/abs/2207.03578)

    本文提出了一种将编译器中间表示与神经机器翻译相结合的方法，能够更好地捕捉代码的语义，从而提高了代码翻译的质量和实用性。

    

    本文提出了一种利用低级别的编译器中间表示（IR）来改进代码翻译的方法。我们的方法结合了神经机器翻译（NMT）和IR，能够更好地捕捉代码的语义，避免以往方法存在的常见错误，从而提高了翻译的质量和实用性。

    In this paper, we leverage low-level compiler intermediate representations (IR) to improve code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of corr
    
[^37]: 面向文本逻辑推理的话语感知图网络

    Discourse-Aware Graph Networks for Textual Logical Reasoning. (arXiv:2207.01450v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.01450](http://arxiv.org/abs/2207.01450)

    本论文提出了逻辑结构约束建模和话语感知图网络（DAGNs）用于解决文本逻辑推理问答问题。DAGNs可以构建逻辑图并通过边缘推理机制和图特征更新来学习逻辑表示。该方法在实验中取得了良好的效果。

    

    文本逻辑推理，尤其是涉及到逻辑推理的问答任务，需要意识到特定的逻辑结构。段落级逻辑关系代表了命题单元之间的蕴涵或矛盾关系（例如，结论句），然而，由于当前的问答系统侧重于基于实体的关系，这种结构尚未被探索。本文提出了逻辑结构约束建模来解决逻辑推理问答问题，并引入了话语感知图网络（DAGNs）。该网络首先利用行间话语连接词和通用逻辑理论构建逻辑图，然后通过端到端的边缘推理机制和图特征更新来学习逻辑表示。这个流程应用于一个通用的编码器，其基本特征与高层逻辑特征相结合，用于答案预测。在三个文本逻辑推理数据集上的实验证明了该方法的合理性。

    Textual logical reasoning, especially question-answering (QA) tasks with logical reasoning, requires awareness of particular logical structures. The passage-level logical relations represent entailment or contradiction between propositional units (e.g., a concluding sentence). However, such structures are unexplored as current QA systems focus on entity-based relations. In this work, we propose logic structural-constraint modeling to solve the logical reasoning QA and introduce discourse-aware graph networks (DAGNs). The networks first construct logic graphs leveraging in-line discourse connectives and generic logic theories, then learn logic representations by end-to-end evolving the logic relations with an edge-reasoning mechanism and updating the graph features. This pipeline is applied to a general encoder, whose fundamental features are joined with the high-level logic features for answer prediction. Experiments on three textual logical reasoning datasets demonstrate the reasonabi
    
[^38]: CodeAttack：面向预训练编程语言模型的基于代码的对抗攻击

    CodeAttack: Code-Based Adversarial Attacks for Pre-trained Programming Language Models. (arXiv:2206.00052v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.00052](http://arxiv.org/abs/2206.00052)

    CodeAttack是一个基于代码结构的黑盒攻击模型，能够生成有效、高效和难以察觉的对抗性攻击样本，并证明预训练编程语言模型存在易受代码特定的对抗性攻击的漏洞。

    

    预训练的编程语言模型（例如CodeT5，CodeBERT，GraphCodeBERT等）有潜力自动化涉及代码理解和代码生成的软件工程任务。然而，这些模型在代码的自然通道中运行，即它们主要关注人对代码的理解。它们对输入的变化不够鲁棒，因此潜在易受自然通道中的对抗性攻击。我们提出了CodeAttack，这是一个简单而有效的黑盒攻击模型，利用代码结构生成有效、高效和难以察觉的对抗样本，并展示了最先进的PL模型在代码特定的对抗性攻击方面的漏洞。我们评估了CodeAttack在不同编程语言的多个代码-代码（翻译和修复）和代码-NL（摘要）任务上的可转移性。CodeAttack在全面超越最先进的自然语言处理对抗攻击模型方面表现出色。

    Pre-trained programming language (PL) models (such as CodeT5, CodeBERT, GraphCodeBERT, etc.,) have the potential to automate software engineering tasks involving code understanding and code generation. However, these models operate in the natural channel of code, i.e., they are primarily concerned with the human understanding of the code. They are not robust to changes in the input and thus, are potentially susceptible to adversarial attacks in the natural channel. We propose, CodeAttack, a simple yet effective black-box attack model that uses code structure to generate effective, efficient, and imperceptible adversarial code samples and demonstrates the vulnerabilities of the state-of-the-art PL models to code-specific adversarial attacks. We evaluate the transferability of CodeAttack on several code-code (translation and repair) and code-NL (summarization) tasks across different programming languages. CodeAttack outperforms state-of-the-art adversarial NLP attack models to achieve th
    
[^39]: 对于普遍时尚概念的视觉与语言对比学习

    Contrastive language and vision learning of general fashion concepts. (arXiv:2204.03972v4 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2204.03972](http://arxiv.org/abs/2204.03972)

    本文提出了一种对于时尚行业的CLIP-like模型—— FashionCLIP，它可以通过对视觉和语言的对比学习实现产品的检索、分类和定位，能够提供更具可转移性的产品表征。

    

    随着在线购物不断崛起，越来越复杂的机器学习（ML）和自然语言处理（NLP）模型的发展紧随其后。虽然大多数用例都被视为专业的监督学习问题，但我们认为从更具可转移性的产品表征中受益。在这项工作中，我们借鉴了对比学习的最新进展，训练出了FashionCLIP，一种适用于时尚行业的CLIP-like模型。我们展示了它在检索、分类和定位方面的能力，并将我们的模型和代码发布给社区。

    The steady rise of online shopping goes hand in hand with the development of increasingly complex ML and NLP models. While most use cases are cast as specialized supervised learning problems, we argue that practitioners would greatly benefit from more transferable representations of products. In this work, we build on recent developments in contrastive learning to train FashionCLIP, a CLIP-like model for the fashion industry. We showcase its capabilities for retrieval, classification and grounding, and release our model and code to the community.
    
[^40]: PMC-Patients: 用于评估基于召回的临床决策支持系统的大规模病患数据集

    PMC-Patients: A Large-scale Dataset of Patient Summaries and Relations for Benchmarking Retrieval-based Clinical Decision Support Systems. (arXiv:2202.13876v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.13876](http://arxiv.org/abs/2202.13876)

    本文提出了一个名为“PMC-Patients”的新数据集，用于定义和测试病患到文章的检索（ReCDS-PAR）和病患到病患的检索（ReCDS-PPR），以评估基于召回的临床决策支持系统（ReCDS）的性能。PMC-Patients数据集涵盖逾10,000名病患信息和27,000篇PubMed Central文章，并展示了多种ReCDS系统的效果分析和20个案例的实用性分析。

    

    目标：基于召回的临床决策支持系统（ReCDS）可以通过提供相关文献和类似病患的信息来帮助临床工作流程。然而，由于缺乏多样的病患收集和公开的大规模病患层面注释数据集，ReCDS系统的发展受到了严重阻碍。在本文中，我们旨在使用名为PMC-Patients的新数据集定义和测试两个ReCDS任务：病患到文章的检索（ReCDS-PAR）和病患到病患的检索（ReCDS-PPR）。方法：我们使用简单的启发式方法从PubMed Central文章中提取病患总结，并利用PubMed引文关系图来定义病患-文章相关性和病患-病患相似性。我们还在PMC-Patients基准测试上实施和评估了几种ReCDS系统，包括稀疏检索器、密集检索器和最近邻检索器。我们进行了几个案例研究，以展示PMC-Patients的临床效用。结果：PMC-Patients数据集包含了10,186名病患的信息，涵盖了逾27,000篇PubMed Central文章。我们提供了两个ReCDS任务的基准评估结果和多种系统的效果分析。此外，我们对两个任务的20个案例进行了分析，证明了PMC-Patients的实用性。

    Objective: Retrieval-based Clinical Decision Support (ReCDS) can aid clinical workflow by providing relevant literature and similar patients for a given patient. However, the development of ReCDS systems has been severely obstructed by the lack of diverse patient collections and publicly available large-scale patient-level annotation datasets. In this paper, we aim to define and benchmark two ReCDS tasks: Patient-to-Article Retrieval (ReCDS-PAR) and Patient-to-Patient Retrieval (ReCDS-PPR) using a novel dataset called PMC-Patients.  Methods: We extract patient summaries from PubMed Central articles using simple heuristics and utilize the PubMed citation graph to define patient-article relevance and patient-patient similarity. We also implement and evaluate several ReCDS systems on the PMC-Patients benchmarks, including sparse retrievers, dense retrievers, and nearest neighbor retrievers. We conduct several case studies to show the clinical utility of PMC-Patients.  Results: PMC-Patient
    

