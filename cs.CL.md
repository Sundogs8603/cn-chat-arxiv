# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ApproBiVT: Lead ASR Models to Generalize Better Using Approximated Bias-Variance Tradeoff Guided Early Stopping and Checkpoint Averaging.](http://arxiv.org/abs/2308.02870) | 本文通过重新思考和更新早停和检查点平均化的方法，从偏差-方差的角度引导ASR模型更好地泛化，利用近似的偏差-方差权衡来指导早停和检查点平均化，在实验中证明可以显著降低CER。 |
| [^2] | [EduChat: A Large-Scale Language Model-based Chatbot System for Intelligent Education.](http://arxiv.org/abs/2308.02773) | EduChat是一个基于大规模语言模型的智能教育聊天机器人系统，旨在支持个性化、公平和有同情心的智能教育，并提供各种教育功能，如开放式问题回答、文章评估、苏格拉底式教学和情感支持。 |
| [^3] | [Meta-Tsallis-Entropy Minimization: A New Self-Training Approach for Domain Adaptation on Text Classification.](http://arxiv.org/abs/2308.02746) | 元-Tsallis-熵最小化是一种新的自适应文本分类领域自适应方法，通过优化目标域上的实例自适应Tsallis熵来解决自训练在大领域转移时失败的问题。 |
| [^4] | [ChatGPT for GTFS: From Words to Information.](http://arxiv.org/abs/2308.02618) | 本研究探索了使用ChatGPT语言模型从GTFS数据中检索信息的可行性，验证了ChatGPT（GPT-3.5）在GTFS规范理解和信息提取方面的能力。程序合成方法在信息检索任务中表现出更高的准确率，为解决GTFS数据信息获取问题提供了一种有效的方法。 |
| [^5] | [Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting.](http://arxiv.org/abs/2308.02582) | 该论文介绍了一种通过领域适应和最少到最多提示的方式实现文本到SQL的高效泛化的方法。通过离线抽样获取少量样本，并合成一个通用提示，避免了昂贵的测试时间样本检索，并通过自适应和分解的方法更好地处理跨领域和跨组合式的泛化。 |
| [^6] | [Is GPT-4 a reliable rater? Evaluating Consistency in GPT-4 Text Ratings.](http://arxiv.org/abs/2308.02575) | GPT-4在多个迭代中生成的反馈评分具有高一致性，内容和文体评分之间具有高相关性。 |
| [^7] | [Learning Implicit Entity-object Relations by Bidirectional Generative Alignment for Multimodal NER.](http://arxiv.org/abs/2308.02570) | 本文提出了一种名为BGA-MNER的双向生成对齐方法，用于解决多模态命名实体识别中的两个主要挑战：语义鸿沟和实体-物体关系。实验结果表明，该方法能够有效地捕捉隐式实体-物体关系。 |
| [^8] | [BioBERT Based SNP-traits Associations Extraction from Biomedical Literature.](http://arxiv.org/abs/2308.02569) | 本文使用BioBERT-GRU方法提取生物医学文献中SNP-特征关联，经过评估证明该方法在性能上胜过以往的机器学习和深度学习方法，达到了较高的精度、召回率和F1-score。 |
| [^9] | [SimTeG: A Frustratingly Simple Approach Improves Textual Graph Learning.](http://arxiv.org/abs/2308.02565) | 这项工作提出了一种令人沮丧地简单的文本图学习方法SimTeG，解决了现有方法在特征工程和模型设计方面的不足。 |
| [^10] | [Industrial Memories: Exploring the Findings of Government Inquiries with Neural Word Embedding and Machine Learning.](http://arxiv.org/abs/2308.02556) | 通过使用神经词嵌入、文本分类和可视化技术，我们提出了一个文本挖掘系统，用于支持探索政府调查发现的大量文本，并通过一个交互式的基于网络的平台，揭示新的历史洞见。 |
| [^11] | [Aspect based sentimental analysis for travellers' reviews.](http://arxiv.org/abs/2308.02548) | 本论文提出了一种基于方面的情感分析方法，能够更详细地分析旅行者的评论，从而帮助机场管理了解旅行者的需求，并发现需要改进的机场服务。 |
| [^12] | [Towards More Human-like AI Communication: A Review of Emergent Communication Research.](http://arxiv.org/abs/2308.02541) | 本综述综合了紧急沟通研究的最新进展，旨在开发能够超越简单任务，并有效沟通和学习新概念的人工智能代理。 |
| [^13] | [ALE: A Simulation-Based Active Learning Evaluation Framework for the Parameter-Driven Comparison of Query Strategies for NLP.](http://arxiv.org/abs/2308.02537) | ALE是一个用于对比NLP中AL策略的仿真主动学习评估框架，提供实证基础和公正的比较。 |
| [^14] | [Chatbot Application to Support Smart Agriculture in Thailand.](http://arxiv.org/abs/2308.02524) | 本研究提出了一种在泰国支持智能农业的聊天机器人应用，为农民提供作物种植知识和建议，通过与智能农业和推荐系统配合使用，为农民提供数据监控和灌溉系统控制的功能。 |
| [^15] | [Improving the Generalization Ability in Essay Coherence Evaluation through Monotonic Constraints.](http://arxiv.org/abs/2308.02506) | 通过单调性约束改进文章连贯性评估中的泛化能力。我们提出了一个连贯性评分模型，其中包括一个局部连贯性判别模型和一个标点符号修正模型，并使用梯度提升回归树作为回归模型。实验证明，我们的模型在未见数据上有更好的泛化能力，同时在NLPCC 2023共享任务7的第一赛道中获得了第三名。 |
| [^16] | [MyVoice: Arabic Speech Resource Collaboration Platform.](http://arxiv.org/abs/2308.02503) | MyVoice是一个用于收集阿拉伯语方言语音的众包平台，提供了设计大型方言语音数据集并使其公开可用的机会。贡献者可以选择细粒度方言并记录话语，平台还提供了质量保证系统来过滤低质量和虚假的录音，并允许贡献者评估录音的质量和提供反馈。该平台还具有灵活性，管理员角色可以添加新的数据或任务，促进多样化和大规模的阿拉伯语语音数据的收集。 |
| [^17] | [Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology.](http://arxiv.org/abs/2308.02180) | 本文研究了使用大型语言模型（LLMs）扩展临床试验匹配的方法，并以肿瘤学为案例研究。研究结果显示，先进的LLMs能够处理临床试验的复杂条件和匹配逻辑，相较于之前的方法，性能显著提升，并可作为人工辅助筛选患者-试验候选人的初步解决方案。 |
| [^18] | [Getting pwn'd by AI: Penetration Testing with Large Language Models.](http://arxiv.org/abs/2308.00121) | 本文探讨了使用大型语言模型（如GPT3.5）作为AI助手来增强渗透测试人员的能力，实现了高级任务规划和低级漏洞寻找两种用例，取得了有前景的初步结果，并就提供该技术的伦理问题进行了讨论。 |
| [^19] | [A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe.](http://arxiv.org/abs/2307.14361) | 本研究提出了一个基于多种机器学习算法和嵌入模型的集成模型，用于基因突变在癌症中的分类。实验结果表明，该模型在准确率、精确率、召回率等指标上优于其他传统和最新的转换器模型，并且具有更高的训练效率。 |
| [^20] | [In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning.](http://arxiv.org/abs/2307.12375) | 大型语言模型（LLMs）在包含标签关系示例的上下文中的学习能力使其在下游任务中表现显著提高，但与传统学习方法不同。我们研究了上下文示例中的标签如何影响预测、预训练中学习到的标签关系如何与上下文示例相互作用以及上下文学习如何聚合标签信息。研究结果揭示了LLMs的工作机制及其对上下文信息的处理方式。 |
| [^21] | [Generating Image-Specific Text Improves Fine-grained Image Classification.](http://arxiv.org/abs/2307.11315) | 本文提出了一种名为GIST的方法，用于从仅图像数据集中生成图像特定的细粒度文本描述，并通过将其用于微调视觉语言模型来改进图像分类的效果。 |
| [^22] | [General Debiasing for Multimodal Sentiment Analysis.](http://arxiv.org/abs/2307.10511) | 提出了一个通用的去偏多模态情感分析任务，通过减少模型对虚假相关性的依赖，提高多模态情感分析模型的非分布外泛化能力。 |
| [^23] | [Improving Pre-trained Language Models' Generalization.](http://arxiv.org/abs/2307.10457) | 该研究提出了一种名为Mask-tuning的训练方法，通过将Masked Language Modeling (MLM)训练目标整合到微调过程中来增强预训练语言模型（PLMs）的泛化能力。实验证明，Mask-tuning在非分布数据集上超过了当前最先进的技术，并提高了PLMs在分布数据集上的性能。 |
| [^24] | [NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning.](http://arxiv.org/abs/2307.08941) | 该论文通过使用神经切向核近似MLP融合，提出了一种高效的语言模型微调方法。实验证明，这种方法能够在降低计算和存储开销的同时保持较好的模型性能。 |
| [^25] | [CAME: Confidence-guided Adaptive Memory Efficient Optimization.](http://arxiv.org/abs/2307.02047) | CAME是一种通过自信指导的策略来实现快速收敛并降低内存使用的自适应内存高效优化方法，在各种自然语言处理任务中表现出稳定性和优越性能。 |
| [^26] | [Recommender Systems in the Era of Large Language Models (LLMs).](http://arxiv.org/abs/2307.02046) | 大型语言模型在推荐系统中的应用已经带来了显著的改进，克服了传统DNN方法的限制，并提供了强大的语言理解、生成、推理和泛化能力。 |
| [^27] | [Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution.](http://arxiv.org/abs/2307.00925) | 本研究首次使用语法演化自动设计语义相似性集合，通过自动选择和聚合候选度量来优化集合与人类判断的相关性，提高相似度评估准确性，并证明了使用集合对语义相似性任务的益处。 |
| [^28] | [One model to rule them all: ranking Slovene summarizers.](http://arxiv.org/abs/2306.11518) | 提出了一种系统，为给定的文本推荐最适合的摘要生成模型。该系统采用一个全连接神经网络，根据输入内容分析并预测最佳摘要生成器。四种斯洛文摘要生成模型解决了在资源有限的语言中进行文本摘要的不同挑战。实验结果表明，所提出的模型在斯洛文摘要生成上表现良好。 |
| [^29] | [Diverse and Faithful Knowledge-Grounded Dialogue Generation via Sequential Posterior Inference.](http://arxiv.org/abs/2306.01153) | 本文提出了一种名为SPI的端到端学习框架，能够忠实地生成多样化的基于知识的对话。 |
| [^30] | [Membership Inference Attacks against Language Models via Neighbourhood Comparison.](http://arxiv.org/abs/2305.18462) | 本文提出两种新的基于邻域比较的攻击策略，利用语言数据的内在结构来提高成员推断攻击的性能，并在几个公开数据集上证明这些攻击的有效性。 |
| [^31] | [Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner.](http://arxiv.org/abs/2305.11769) | 本文提出了一种名为JADE的新方法，可以利用易于获取的图像-文本对进行的联合学习，以提升视觉和语言模态的细粒度特征对齐，从而更好地进行视觉问答和密集字幕生成。 |
| [^32] | [Graphologue: Exploring Large Language Model Responses with Interactive Diagrams.](http://arxiv.org/abs/2305.11473) | Graphologue是一个交互式系统，将大型语言模型的基于文本的响应转换为图形化图表以增强其可用性和可解释性，用户可以通过选择和突出显示特定节点和链接来与这些图表进行交互。 |
| [^33] | [Boosting Local Spectro-Temporal Features for Speech Analysis.](http://arxiv.org/abs/2305.10270) | 该论文介绍了在语音识别中电话分类的问题，并探索了几组可以用于电话分类的本地谱时特征，提出了使用Haar特征和SVM分类的梯度直方图进行电话分类，并给出了一些初步结果。 |
| [^34] | [Beyond Rule-based Named Entity Recognition and Relation Extraction for Process Model Generation from Natural Language Text.](http://arxiv.org/abs/2305.03960) | 本文扩展了PET数据集，通过聚类流程实体的提及，提出了一种新的基线技术流程提取方法，该方法避免了手动创建业务流程模型的繁琐工作，同时解决了同一流程实体重复提及的歧义问题。 |
| [^35] | [Defending against Insertion-based Textual Backdoor Attacks via Attribution.](http://arxiv.org/abs/2305.02394) | 本文提出了一种基于归因的管道AttDef，用于防御两种插入式污染攻击BadNL和InSent，该管道可以成功缓解插入式文本后门攻击并在四个基准数据集上平均提高了56.59%至79.97%和15.25%至48.34%的准确率。 |
| [^36] | [Missing Information, Unresponsive Authors, Experimental Flaws: The Impossibility of Assessing the Reproducibility of Previous Human Evaluations in NLP.](http://arxiv.org/abs/2305.01633) | 该论文研究了NLP领域过去的人类评估的再现性问题，结果发现大部分人类评估都无法重复或再现，可能是由于缺失信息、无回应作者和实验缺陷等原因导致。这个结果提示我们需要重新考虑如何设计和报告人类评估实验。 |
| [^37] | [Learning Human-Human Interactions in Images from Weak Textual Supervision.](http://arxiv.org/abs/2304.14104) | 本文提出了一种新的范式，从单一的静态图像中学习自由文本的形式来灵活建模人际互动。并通过知识蒸馏生成伪标签来训练一种字幕模型，用于有效理解图像中的人际互动，具有较高的预测文本和语义质量，并在此任务上优于SOTA的图像字幕和情境识别模型。 |
| [^38] | [Retrieval-based Knowledge Augmented Vision Language Pre-training.](http://arxiv.org/abs/2304.13923) | 本文提出了一种基于检索的知识增强视觉语言预训练模型，将从知识图谱中检索到的世界知识融入视觉语言预训练中，将明确的知识与视觉语言对融合，通过四个知识感知的自监督任务推动多模态数据和知识的相互整合。 |
| [^39] | [Self-Supervised Multimodal Learning: A Survey.](http://arxiv.org/abs/2304.01008) | 自监督多模态学习是一项旨在解决多模态数据中的自监督学习挑战的研究方向。它通过学习来自原始多模态数据中的表示，并解决了没有标签的多模态数据学习、不同模态的融合和不对齐数据学习等问题。 |
| [^40] | [LLM for Patient-Trial Matching: Privacy-Aware Data Augmentation Towards Better Performance and Generalizability.](http://arxiv.org/abs/2303.16756) | 本文提出了一种隐私感知数据增强的LLM-PTM方法，有效地提高了患者-试验匹配的性能和泛化能力。 |
| [^41] | [eP-ALM: Efficient Perceptual Augmentation of Language Models.](http://arxiv.org/abs/2303.11403) | 本论文提出了一种用对比学习提高语言模型的感知能力的高效方法eP-ALM，可以实现视觉感知信息和文本信息的融合，同时还能在多模态基准测试上实现最先进的结果。 |
| [^42] | [Summaries as Captions: Generating Figure Captions for Scientific Documents with Automated Text Summarization.](http://arxiv.org/abs/2302.12324) | 本文提出了利用自动化文本摘要生成科学文献的插图标题的方法，并使用预训练的抽象化摘要模型 PEGASUS 对引用图表的段落进行摘要。实验结果表明该方法在自动评估和人工评估中均优于之前的视觉方法。研究还发现了两个关键挑战：低质量作者撰写的标题的普遍存在以及对好标题缺乏明确的标准。 |
| [^43] | [Selective Explanations: Leveraging Human Input to Align Explainable AI.](http://arxiv.org/abs/2301.09656) | 本研究提出一种通过利用人类输入生成选择性解释的通用框架，以弥合可解释人工智能（XAI）与人类解释的差距，并且在决策支持任务中进行了实验证明其有效性。 |
| [^44] | [TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World.](http://arxiv.org/abs/2301.05880) | TikTalk是一个基于视频的多模态对话数据集，用于研究智能且类似人类的闲聊机器人。数据集包含从流行视频分享平台收集的38K个视频和367K个用户对话。与其他数据集相比，TikTalk提供了更丰富的上下文类型，同时也增加了从复杂的多模态信息中生成个性化回答的难度。数据集中还更频繁地引用了外部知识，为多模态对话模型提供了新的挑战。 |
| [^45] | [Reasoning with Language Model Prompting: A Survey.](http://arxiv.org/abs/2212.09597) | 本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。 |
| [^46] | [Enhancing Multi-modal and Multi-hop Question Answering via Structured Knowledge and Unified Retrieval-Generation.](http://arxiv.org/abs/2212.08632) | 通过结构化的知识和统一的检索-生成方法，增强多模态多跳问答。使用实体中心融合编码器对齐不同模态的来源，使用统一的检索-生成解码器整合中间检索结果进行答案生成，并自适应决定检索步骤的数量。 |
| [^47] | [SceneGATE: Scene-Graph based co-Attention networks for TExt visual question answering.](http://arxiv.org/abs/2212.08283) | 本文提出了一种基于场景图的共同关注网络（SceneGATE）用于TextVQA，通过发现场景图的潜在语义，捕捉了图像中物体、OCR标记和问题词之间的语义关系，并在Text-VQA和ST-VQA两个基准数据集上表现显著优于现有方法。 |
| [^48] | [A Survey on Natural Language Processing for Programming.](http://arxiv.org/abs/2212.05773) | 本文通过系统综述，从结构化和功能导向的属性的角度，涵盖了自然语言处理在编程中的应用领域的任务、数据集、评估方法、技术和模型，揭示了它们在程序理解和生成方面的作用，并提出了未来工作的潜在方向。 |
| [^49] | [QAmeleon: Multilingual QA with Only 5 Examples.](http://arxiv.org/abs/2211.08264) | QAmeleon是一种通过仅使用5个示例进行多语言问答的方法，通过预训练语言模型自动生成数据并进行训练，避免了昂贵的标注过程。该方法在准确性和性能方面优于传统的基于翻译的方法，并能弥合英语和完全监督方法之间的差距。 |
| [^50] | [Taxonomy of Abstractive Dialogue Summarization: Scenarios, Approaches and Future Directions.](http://arxiv.org/abs/2210.09894) | 本调查对抽象对话摘要进行了全面调查，从场景、方法和评估的角度分类讨论了该领域的现有工作。 |
| [^51] | [Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction.](http://arxiv.org/abs/2207.14116) | Claim-Dissector是一款联合重排和真实性预测的可解释的事实核查系统，可以识别与声明相关的证据，并确定声明的真实性。该系统的个人贡献以及证据所支持或反驳声明的贡献都可以被识别。 |
| [^52] | [$C^3$: Compositional Counterfactual Contrastive Learning for Video-grounded Dialogues.](http://arxiv.org/abs/2106.08914) | 本研究提出了一种名为$C^3$的新方法，通过对视频对话中的事实和反事实样本进行对比训练，以实现对于视频和对话上下文相关的回应生成。该方法利用了对象级或动作级的对比损失函数，旨在提高多模态推理能力和泛化能力。 |
| [^53] | [Design and Implementation of English To Yor\`ub\'a Verb Phrase Machine Translation System.](http://arxiv.org/abs/2104.04125) | 设计并实现了一种英语到约鲁巴语动词短语机器翻译系统，在家庭领域收集了源语言和目标语言的动词短语组的词汇，并使用上下文无关文法验证了重写规则。实验结果表明，系统的输出与谷歌翻译的响应匹配率超过70%。 |
| [^54] | [Token-Modification Adversarial Attacks for Natural Language Processing: A Survey.](http://arxiv.org/abs/2103.00676) | 这项调研对现有的自然语言处理中的标记修改对抗攻击进行了分类和比较，并旨在指导新的研究并推动进一步的攻击组件研究。 |

# 详细

[^1]: ApproBiVT: 使用近似的偏差-方差权衡来引导ASR模型更好地泛化的早停和检查点平均化

    ApproBiVT: Lead ASR Models to Generalize Better Using Approximated Bias-Variance Tradeoff Guided Early Stopping and Checkpoint Averaging. (arXiv:2308.02870v1 [cs.CL])

    [http://arxiv.org/abs/2308.02870](http://arxiv.org/abs/2308.02870)

    本文通过重新思考和更新早停和检查点平均化的方法，从偏差-方差的角度引导ASR模型更好地泛化，利用近似的偏差-方差权衡来指导早停和检查点平均化，在实验中证明可以显著降低CER。

    

    传统的自动语音识别（ASR）模型训练通常是在训练集上训练多个检查点，同时依赖于验证集通过早停来防止过拟合，并使用多个最后几个检查点或最低验证损失的平均值得到最终模型。本文从偏差-方差的角度重新思考和更新了早停和检查点平均化的方法。理论上，偏差和方差分别表示模型的拟合和变异性，它们的权衡决定了整体泛化误差。但是，精确评估它们是不切实际的。作为一种替代，我们将训练损失和验证损失视为偏差和方差的代理，并使用它们的权衡，即近似的偏差-方差权衡（ApproBiVT），来引导早停和检查点平均化。在使用先进的ASR模型进行评估时，我们的方法可以在AISHELL-1和...

    The conventional recipe for Automatic Speech Recognition (ASR) models is to 1) train multiple checkpoints on a training set while relying on a validation set to prevent overfitting using early stopping and 2) average several last checkpoints or that of the lowest validation losses to obtain the final model. In this paper, we rethink and update the early stopping and checkpoint averaging from the perspective of the bias-variance tradeoff. Theoretically, the bias and variance represent the fitness and variability of a model and the tradeoff of them determines the overall generalization error. But, it's impractical to evaluate them precisely. As an alternative, we take the training loss and validation loss as proxies of bias and variance and guide the early stopping and checkpoint averaging using their tradeoff, namely an Approximated Bias-Variance Tradeoff (ApproBiVT). When evaluating with advanced ASR models, our recipe provides 2.5%-3.7% and 3.1%-4.6% CER reduction on the AISHELL-1 and
    
[^2]: EduChat:一种基于大规模语言模型的智能教育聊天机器人系统

    EduChat: A Large-Scale Language Model-based Chatbot System for Intelligent Education. (arXiv:2308.02773v1 [cs.CL])

    [http://arxiv.org/abs/2308.02773](http://arxiv.org/abs/2308.02773)

    EduChat是一个基于大规模语言模型的智能教育聊天机器人系统，旨在支持个性化、公平和有同情心的智能教育，并提供各种教育功能，如开放式问题回答、文章评估、苏格拉底式教学和情感支持。

    

    EduChat是一种基于大规模语言模型（LLM）的智能教育聊天机器人系统。其目标是支持个性化、公平和有同情心的智能教育，为教师、学生和家长提供服务。在心理学和教育理论的指导下，它进一步加强了基本的LLM的教育功能，如开放式问题回答，文章评估，苏格拉底式教学和情感支持。特别地，我们通过在教育语料库上预训练来学习特定领域的知识，并通过在设计的系统提示和指令上进行微调来激发各种技能。目前，EduChat作为一个开源项目在线可用，其代码、数据和模型参数可以在平台上获取（例如，GitHub https://github.com/icalk-nlp/EduChat，Hugging Face https://huggingface.co/ecnu-icalk）。我们还准备了一个在线演示其功能的视频（https://vimeo.com/851004454）。

    EduChat (https://www.educhat.top/) is a large-scale language model (LLM)-based chatbot system in the education domain. Its goal is to support personalized, fair, and compassionate intelligent education, serving teachers, students, and parents. Guided by theories from psychology and education, it further strengthens educational functions such as open question answering, essay assessment, Socratic teaching, and emotional support based on the existing basic LLMs. Particularly, we learn domain-specific knowledge by pre-training on the educational corpus and stimulate various skills with tool use by fine-tuning on designed system prompts and instructions. Currently, EduChat is available online as an open-source project, with its code, data, and model parameters available on platforms (e.g., GitHub https://github.com/icalk-nlp/EduChat, Hugging Face https://huggingface.co/ecnu-icalk ). We also prepare a demonstration of its capabilities online (https://vimeo.com/851004454). This initiative ai
    
[^3]: 元-Tsallis-熵最小化：一种新的领域自适应文本分类自训练方法

    Meta-Tsallis-Entropy Minimization: A New Self-Training Approach for Domain Adaptation on Text Classification. (arXiv:2308.02746v1 [cs.CL])

    [http://arxiv.org/abs/2308.02746](http://arxiv.org/abs/2308.02746)

    元-Tsallis-熵最小化是一种新的自适应文本分类领域自适应方法，通过优化目标域上的实例自适应Tsallis熵来解决自训练在大领域转移时失败的问题。

    

    文本分类是自然语言处理的基本任务，跨领域适应文本分类模型具有广泛应用。自训练通过从模型的预测结果中生成伪样本，并迭代在伪样本上进行训练，即在源域上最小化损失，在目标域上最小化Gibbs熵。然而，Gibbs熵对预测误差非常敏感，因此当领域转移较大时，自训练往往会失败。在本文中，我们提出了元-Tsallis-熵最小化（MTEM）方法，该方法应用元学习算法来优化目标域上的实例自适应Tsallis熵。为了降低MTEM的计算成本，我们提出了一种近似技术来近似元学习中涉及的二阶导数。为了高效生成伪标签，我们提出了一种退火采样机制来探索模型的预测概率。从理论上讲，我们证明了m的收敛性

    Text classification is a fundamental task for natural language processing, and adapting text classification models across domains has broad applications. Self-training generates pseudo-examples from the model's predictions and iteratively trains on the pseudo-examples, i.e., minimizes the loss on the source domain and the Gibbs entropy on the target domain. However, Gibbs entropy is sensitive to prediction errors, and thus, self-training tends to fail when the domain shift is large. In this paper, we propose Meta-Tsallis Entropy minimization (MTEM), which applies a meta-learning algorithm to optimize the instance adaptive Tsallis entropy on the target domain. To reduce the computation cost of MTEM, we propose an approximation technique to approximate the Second-order derivation involved in the meta-learning. To efficiently generate pseudo labels, we propose an annealing sampling mechanism for exploring the model's prediction probability. Theoretically, we prove the convergence of the m
    
[^4]: ChatGPT用于GTFS: 从文字到信息

    ChatGPT for GTFS: From Words to Information. (arXiv:2308.02618v1 [cs.IR])

    [http://arxiv.org/abs/2308.02618](http://arxiv.org/abs/2308.02618)

    本研究探索了使用ChatGPT语言模型从GTFS数据中检索信息的可行性，验证了ChatGPT（GPT-3.5）在GTFS规范理解和信息提取方面的能力。程序合成方法在信息检索任务中表现出更高的准确率，为解决GTFS数据信息获取问题提供了一种有效的方法。

    

    广泛使用的公交通行数据发布标准General Transit Feed Specification（GTFS）是表格数据，信息分散在不同的文件中，需要专门的工具或包来检索信息。与此同时，使用大型语言模型进行文本和信息检索的趋势也在增长。本研究的想法是看看当前广泛采用的LLMs（ChatGPT）是否能够使用自然语言指令从GTFS中检索信息。我们首先测试ChatGPT（GPT-3.5）是否理解GTFS规范。GPT-3.5在我们的多项选择问题（MCQ）中正确回答了77%。接下来，我们利用过滤的GTFS数据集对LLM进行信息提取任务。对于信息检索，我们比较了零-shot和程序合成。程序合成的效果更好，在简单问题上达到了约90%的准确率，在复杂问题上达到了约40%的准确率。

    The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. GTFS being tabular data, with information spread across different files, necessitates specialized tools or packages to retrieve information. Concurrently, the use of Large Language Models for text and information retrieval is growing. The idea of this research is to see if the current widely adopted LLMs (ChatGPT) are able to retrieve information from GTFS using natural language instructions. We first test whether ChatGPT (GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our multiple-choice questions (MCQ) correctly. Next, we task the LLM with information extractions from a filtered GTFS feed with 4 routes. For information retrieval, we compare zero-shot and program synthesis. Program synthesis works better, achieving ~90% accuracy on simple questions and ~40% accuracy on complex questions.
    
[^5]: 通过领域适应的最少到最多提示的方式实现文本到SQL的高效泛化

    Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v1 [cs.CL])

    [http://arxiv.org/abs/2308.02582](http://arxiv.org/abs/2308.02582)

    该论文介绍了一种通过领域适应和最少到最多提示的方式实现文本到SQL的高效泛化的方法。通过离线抽样获取少量样本，并合成一个通用提示，避免了昂贵的测试时间样本检索，并通过自适应和分解的方法更好地处理跨领域和跨组合式的泛化。

    

    跨领域和跨组合式的文本到SQL语义解析的泛化是一项具有挑战性的任务。现有的基于大型语言模型（LLM）的解决方案依赖于从训练集中推理出少量样本，以合成每个自然语言（NL）测试查询的运行时提示。与此相反，我们设计了一种算法，该算法通过离线抽样从训练数据中获取少量样本，完全覆盖SQL子句、运算符和函数，并在允许的令牌长度范围内实现最大领域覆盖。这样可以合成一个固定的通用提示（GP），其中包含NL测试查询之间共用的多样化样本集，避免了昂贵的测试时间样本检索。我们还将GP自适应到目标数据库领域（DA-GP），以更好地处理跨领域泛化；然后采用分解的最少到最多提示（LTMP-DA-GP）来处理跨组合泛化。LTMP-DA-GP的合成是离线任务，

    Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to
    
[^6]: GPT-4是一个可靠的评分器吗？评估GPT-4文本评分的一致性。

    Is GPT-4 a reliable rater? Evaluating Consistency in GPT-4 Text Ratings. (arXiv:2308.02575v1 [cs.CL])

    [http://arxiv.org/abs/2308.02575](http://arxiv.org/abs/2308.02575)

    GPT-4在多个迭代中生成的反馈评分具有高一致性，内容和文体评分之间具有高相关性。

    

    本研究调查了OpenAI的GPT-4在多个迭代、时间跨度和文体变化中生成的反馈评分的一致性。该模型根据内容和文体对宏观经济学学科领域内的任务回答进行评分。通过统计分析，研究了评分的一致性、迭代之间的评分相关性以及内容和文体评分之间的相关性。结果显示，不同时间跨度的ICC分数在0.94到0.99之间，表明GPT-4能够在重复任务中生成一致的评分。内容和文体评分之间的相关性为0.87。当应用不恰当的文体时，平均内容评分保持不变，而文体评分下降，这表明大型语言模型在生成一致评分方面具有能力。

    This study investigates the consistency of feedback ratings generated by OpenAI's GPT-4, a state-of-the-art artificial intelligence language model, across multiple iterations, time spans and stylistic variations. The model rated responses to tasks within the Higher Education (HE) subject domain of macroeconomics in terms of their content and style. Statistical analysis was conducted in order to learn more about the interrater reliability, consistency of the ratings across iterations and the correlation between ratings in terms of content and style. The results revealed a high interrater reliability with ICC scores ranging between 0.94 and 0.99 for different timespans, suggesting that GPT-4 is capable of generating consistent ratings across repetitions with a clear prompt. Style and content ratings show a high correlation of 0.87. When applying a non-adequate style the average content ratings remained constant, while style ratings decreased, which indicates that the large language model
    
[^7]: 通过双向生成对齐学习隐式实体-物体关系，用于多模态NER

    Learning Implicit Entity-object Relations by Bidirectional Generative Alignment for Multimodal NER. (arXiv:2308.02570v1 [cs.LG])

    [http://arxiv.org/abs/2308.02570](http://arxiv.org/abs/2308.02570)

    本文提出了一种名为BGA-MNER的双向生成对齐方法，用于解决多模态命名实体识别中的两个主要挑战：语义鸿沟和实体-物体关系。实验结果表明，该方法能够有效地捕捉隐式实体-物体关系。

    

    多模态命名实体识别(MNER)面临的挑战主要有两方面: (1) 弥合文本和图像之间的语义鸿沟; (2) 匹配实体与图像中其关联的物体。现有方法无法捕捉隐含的实体-物体关系，因为缺乏相应的注释。本文提出了一种名为BGA-MNER的双向生成对齐方法来解决这些问题。我们的BGA-MNER包括针对两种模态中的实体显著内容的\texttt{图像到文本}和\texttt{文本到图像}生成。它通过共同优化双向重建目标来对齐隐含的实体-物体关系，在直接而强大的约束下实现对齐。此外，图像-文本对通常包含不匹配的组件，对于生成来说是噪声。我们提出了一种阶段性改进的上下文采样器，用于提取匹配的跨模态内容进行生成。在两个基准测试中进行的广泛实验证明了我们的方法。

    The challenge posed by multimodal named entity recognition (MNER) is mainly two-fold: (1) bridging the semantic gap between text and image and (2) matching the entity with its associated object in image. Existing methods fail to capture the implicit entity-object relations, due to the lack of corresponding annotation. In this paper, we propose a bidirectional generative alignment method named BGA-MNER to tackle these issues. Our BGA-MNER consists of \texttt{image2text} and \texttt{text2image} generation with respect to entity-salient content in two modalities. It jointly optimizes the bidirectional reconstruction objectives, leading to aligning the implicit entity-object relations under such direct and powerful constraints. Furthermore, image-text pairs usually contain unmatched components which are noisy for generation. A stage-refined context sampler is proposed to extract the matched cross-modal content for generation. Extensive experiments on two benchmarks demonstrate that our met
    
[^8]: 基于BioBERT的生物医学文献中SNP-特征关联的提取

    BioBERT Based SNP-traits Associations Extraction from Biomedical Literature. (arXiv:2308.02569v1 [cs.CL])

    [http://arxiv.org/abs/2308.02569](http://arxiv.org/abs/2308.02569)

    本文使用BioBERT-GRU方法提取生物医学文献中SNP-特征关联，经过评估证明该方法在性能上胜过以往的机器学习和深度学习方法，达到了较高的精度、召回率和F1-score。

    

    科学文献中包含大量信息，为开发文本挖掘方法提供了极好的机会，以提取生物医学关系。重要的一类信息是单核苷酸多态性（SNP）与特征之间的关联。本文提出了一种基于BioBERT-GRU的方法来识别SNP-特征关联。通过在SNPPhenA数据集上对我们的方法进行评估，得出结论：该新方法在性能上优于以前的机器学习和深度学习方法。BioBERT-GRU实现了精度0.883，召回率0.882和F1-score 0.881的结果。

    Scientific literature contains a considerable amount of information that provides an excellent opportunity for developing text mining methods to extract biomedical relationships. An important type of information is the relationship between singular nucleotide polymorphisms (SNP) and traits. In this paper, we present a BioBERT-GRU method to identify SNP- traits associations. Based on the evaluation of our method on the SNPPhenA dataset, it is concluded that this new method performs better than previous machine learning and deep learning based methods. BioBERT-GRU achieved the result a precision of 0.883, recall of 0.882 and F1-score of 0.881.
    
[^9]: SimTeG: 一种令人沮丧地简单的方法改进了文本图学习

    SimTeG: A Frustratingly Simple Approach Improves Textual Graph Learning. (arXiv:2308.02565v1 [cs.CL])

    [http://arxiv.org/abs/2308.02565](http://arxiv.org/abs/2308.02565)

    这项工作提出了一种令人沮丧地简单的文本图学习方法SimTeG，解决了现有方法在特征工程和模型设计方面的不足。

    

    文本图（TGs）是指节点对应文本（句子或文档）的图形，在许多领域都得到广泛应用。TGs的表示学习包括两个阶段：（i）无监督特征提取和（ii）监督图表示学习。近年来，人们在后者阶段进行了大量的研究，其中图神经网络（GNNs）占据主导地位。然而，目前大多数现有图形基准的前者阶段仍依赖于传统的特征工程技术。最近随着语言模型（LMs）的快速发展，研究人员开始利用LMs来促进TGs的学习，方法要么是在计算密集的框架中联合训练它们（合并两个阶段），要么是设计复杂的自监督训练任务来进行特征提取（增强第一阶段）。在这项工作中，我们提出了SimTeG，一种令人沮丧地简单的文本图学习方法，它不创新于框架、模型和任务。

    Textual graphs (TGs) are graphs whose nodes correspond to text (sentences or documents), which are widely prevalent. The representation learning of TGs involves two stages: (i) unsupervised feature extraction and (ii) supervised graph representation learning. In recent years, extensive efforts have been devoted to the latter stage, where Graph Neural Networks (GNNs) have dominated. However, the former stage for most existing graph benchmarks still relies on traditional feature engineering techniques. More recently, with the rapid development of language models (LMs), researchers have focused on leveraging LMs to facilitate the learning of TGs, either by jointly training them in a computationally intensive framework (merging the two stages), or designing complex self-supervised training tasks for feature extraction (enhancing the first stage). In this work, we present SimTeG, a frustratingly Simple approach for Textual Graph learning that does not innovate in frameworks, models, and tas
    
[^10]: 工业记忆：通过神经词嵌入和机器学习探索政府调查的发现

    Industrial Memories: Exploring the Findings of Government Inquiries with Neural Word Embedding and Machine Learning. (arXiv:2308.02556v1 [cs.CL])

    [http://arxiv.org/abs/2308.02556](http://arxiv.org/abs/2308.02556)

    通过使用神经词嵌入、文本分类和可视化技术，我们提出了一个文本挖掘系统，用于支持探索政府调查发现的大量文本，并通过一个交互式的基于网络的平台，揭示新的历史洞见。

    

    我们提出了一个文本挖掘系统，以支持探索大量关于政府调查发现的文本。尽管这些调查具有历史意义和潜在的社会影响，但关键发现常常隐藏在冗长的文件中，对一般公众来说无法获取。我们通过使用词嵌入、文本分类和可视化技术，将爱尔兰政府对工业学校的调查结果进行转化，呈现出一个交互式的基于网络的平台，可以探索文本以揭示新的历史洞见。

    We present a text mining system to support the exploration of large volumes of text detailing the findings of government inquiries. Despite their historical significance and potential societal impact, key findings of inquiries are often hidden within lengthy documents and remain inaccessible to the general public. We transform the findings of the Irish government's inquiry into industrial schools and through the use of word embedding, text classification and visualisation, present an interactive web-based platform that enables the exploration of the text to uncover new historical insights.
    
[^11]: 旅行者评论的基于方面的情感分析

    Aspect based sentimental analysis for travellers' reviews. (arXiv:2308.02548v1 [cs.CL])

    [http://arxiv.org/abs/2308.02548](http://arxiv.org/abs/2308.02548)

    本论文提出了一种基于方面的情感分析方法，能够更详细地分析旅行者的评论，从而帮助机场管理了解旅行者的需求，并发现需要改进的机场服务。

    

    机场服务质量评估通常可以在社交媒体上找到，包括谷歌地图。这对于机场管理来说是很有价值的，可以提高所提供服务的质量。然而，先前的研究要么提供旅行者讨论的主题的总体评论，要么提供情感值来标记整个评论，而没有具体提及背后的机场服务。因此，本研究提出使用基于方面的情感分析，以提供对旅行者评论的更详细的分析。本研究在谷歌地图关于迪拜和多哈机场的数据上应用了基于方面的情感分析。结果提供了使用基于方面的情感分析来更好地了解旅行者和发现需要改进的机场服务的可触及的原因。

    Airport service quality evaluation is commonly found on social media, including Google Maps. This valuable for airport management in order to enhance the quality of services provided. However; prior studies either provide general review for topics discussed by travellers or provide sentimental value to tag the entire review without specifically mentioning the airport service that is behind such value. Accordingly, this work proposes using aspect based sentimental analysis in order to provide more detailed analysis for travellers reviews. This works applied aspect based sentimental analysis on data collected from Google Map about Dubai and Doha airports. The results provide tangible reasons to use aspect based sentimental analysis in order to understand more the travellers and spot airport services that are in need for improvement.
    
[^12]: 迈向更具人类化的人工智能交流：紧急沟通研究综述

    Towards More Human-like AI Communication: A Review of Emergent Communication Research. (arXiv:2308.02541v1 [cs.CL])

    [http://arxiv.org/abs/2308.02541](http://arxiv.org/abs/2308.02541)

    本综述综合了紧急沟通研究的最新进展，旨在开发能够超越简单任务，并有效沟通和学习新概念的人工智能代理。

    

    在向以人为中心的人工智能转变的最近，机器准确使用自然语言的需求变得越来越重要。虽然实现这一目标的常见方法是训练大型语言模型，但这种方法可能存在学习错位，模型可能无法捕捉人类在使用自然语言时所使用的基本结构和推理方式，可能导致意想不到或不可靠的行为。紧急沟通（Emecom）是一个近年来发表的研究领域，旨在开发能够以超越简单的判别性任务，并能有效地交流和学习新概念的自然语言使用能力的人工代理。在本综述中，我们以两个方面介绍Emecom。首先，我们界定了文献中发现的所有常见特性及其与人类交互的关系。其次，我们确定了两个子类别，并突出它们的特点和开放挑战。

    In the recent shift towards human-centric AI, the need for machines to accurately use natural language has become increasingly important. While a common approach to achieve this is to train large language models, this method presents a form of learning misalignment where the model may not capture the underlying structure and reasoning humans employ in using natural language, potentially leading to unexpected or unreliable behavior. Emergent communication (Emecom) is a field of research that has seen a growing number of publications in recent years, aiming to develop artificial agents capable of using natural language in a way that goes beyond simple discriminative tasks and can effectively communicate and learn new concepts. In this review, we present Emecom under two aspects. Firstly, we delineate all the common proprieties we find across the literature and how they relate to human interactions. Secondly, we identify two subcategories and highlight their characteristics and open chall
    
[^13]: ALE: 用于NLP查询策略参数驱动比较的基于仿真的主动学习评估框架

    ALE: A Simulation-Based Active Learning Evaluation Framework for the Parameter-Driven Comparison of Query Strategies for NLP. (arXiv:2308.02537v1 [cs.CL])

    [http://arxiv.org/abs/2308.02537](http://arxiv.org/abs/2308.02537)

    ALE是一个用于对比NLP中AL策略的仿真主动学习评估框架，提供实证基础和公正的比较。

    

    监督机器学习和深度学习需要大量标记数据，数据科学家通过手动和耗时的注释过程获取。为了减轻这个挑战，主动学习（AL）提出将有希望的数据点推荐给注释员，以便他们注释接下来的数据，而不是随机或连续的样本。这种方法旨在节省注释工作量，同时保持模型的性能。然而，实践者面临着许多用于不同任务的AL策略，并且需要一个实证基础来选择它们之间的比较。调研将AL策略分类为没有性能指标的分类法。新型AL策略的介绍性演示与少量策略的性能比较。我们的贡献通过引入一种可重现的主动学习评估（ALE）框架来解决这个问题，用于比较评估NLP中的AL策略。该框架可以以较低的成本实现AL策略，并通过数据驱动的比较进行公正的比较。

    Supervised machine learning and deep learning require a large amount of labeled data, which data scientists obtain in a manual, and time-consuming annotation process. To mitigate this challenge, Active Learning (AL) proposes promising data points to annotators they annotate next instead of a subsequent or random sample. This method is supposed to save annotation effort while maintaining model performance. However, practitioners face many AL strategies for different tasks and need an empirical basis to choose between them. Surveys categorize AL strategies into taxonomies without performance indications. Presentations of novel AL strategies compare the performance to a small subset of strategies. Our contribution addresses the empirical basis by introducing a reproducible active learning evaluation (ALE) framework for the comparative evaluation of AL strategies in NLP. The framework allows the implementation of AL strategies with low effort and a fair data-driven comparison through defin
    
[^14]: 在泰国支持智能农业的聊天机器人应用

    Chatbot Application to Support Smart Agriculture in Thailand. (arXiv:2308.02524v1 [cs.CL])

    [http://arxiv.org/abs/2308.02524](http://arxiv.org/abs/2308.02524)

    本研究提出了一种在泰国支持智能农业的聊天机器人应用，为农民提供作物种植知识和建议，通过与智能农业和推荐系统配合使用，为农民提供数据监控和灌溉系统控制的功能。

    

    聊天机器人是一种能够自动快速地回复文本或语音对话的软件。在农业领域中，现有的智能农业系统仅使用来自感应和物联网技术的数据，不包括作物种植知识来支持农民的决策。为了增强这一点，聊天机器人应用可以成为农民的助手，提供作物种植知识。因此，我们提出了 LINE 聊天机器人应用作为信息和知识表示，为农民提供作物种植建议。它与智能农业和推荐系统配合使用。我们提出的 LINE 聊天机器人应用包括五个主要的功能（开始/停止菜单、主页、滴灌页面、雾灌页面和监控页面）。农民们将会获得用于数据监控以支持决策的信息。此外，他们可以通过 LINE 聊天机器人来控制灌溉系统。

    A chatbot is a software developed to help reply to text or voice conversations automatically and quickly in real time. In the agriculture sector, the existing smart agriculture systems just use data from sensing and internet of things (IoT) technologies that exclude crop cultivation knowledge to support decision-making by farmers. To enhance this, the chatbot application can be an assistant to farmers to provide crop cultivation knowledge. Consequently, we propose the LINE chatbot application as an information and knowledge representation providing crop cultivation recommendations to farmers. It works with smart agriculture and recommendation systems. Our proposed LINE chatbot application consists of five main functions (start/stop menu, main page, drip irri gation page, mist irrigation page, and monitor page). Farmers will receive information for data monitoring to support their decision-making. Moreover, they can control the irrigation system via the LINE chatbot. Furthermore, farmer
    
[^15]: 通过单调性约束改进文章连贯性评估中的泛化能力

    Improving the Generalization Ability in Essay Coherence Evaluation through Monotonic Constraints. (arXiv:2308.02506v1 [cs.CL])

    [http://arxiv.org/abs/2308.02506](http://arxiv.org/abs/2308.02506)

    通过单调性约束改进文章连贯性评估中的泛化能力。我们提出了一个连贯性评分模型，其中包括一个局部连贯性判别模型和一个标点符号修正模型，并使用梯度提升回归树作为回归模型。实验证明，我们的模型在未见数据上有更好的泛化能力，同时在NLPCC 2023共享任务7的第一赛道中获得了第三名。

    

    连贯性是评估文本可读性的一个关键方面，在评估论文时可以通过两个主要因素进行评估。第一个因素是逻辑连贯性，表现为适当使用话语连接词和建立句子之间的逻辑关系。第二个因素是标点符号的适当性，因为不适当的标点符号可能导致句子结构混乱。为了解决这些问题，我们提出了一个连贯性评分模型，其由一个具有两个特征提取器的回归模型组成：一个局部连贯性判别模型和一个标点符号修正模型。我们使用梯度提升回归树作为回归模型，并对输入特征施加了单调性约束。实验结果表明，我们提出的模型在未见数据上具有更好的泛化能力。该模型在NLPCC 2023共享任务7的第一赛道中获得了第三名。此外，我们还简要介绍了对于其他赛道的解决方案。

    Coherence is a crucial aspect of evaluating text readability and can be assessed through two primary factors when evaluating an essay in a scoring scenario. The first factor is logical coherence, characterized by the appropriate use of discourse connectives and the establishment of logical relationships between sentences. The second factor is the appropriateness of punctuation, as inappropriate punctuation can lead to confused sentence structure. To address these concerns, we propose a coherence scoring model consisting of a regression model with two feature extractors: a local coherence discriminative model and a punctuation correction model. We employ gradient-boosting regression trees as the regression model and impose monotonicity constraints on the input features. The results show that our proposed model better generalizes unseen data. The model achieved third place in track 1 of NLPCC 2023 shared task 7. Additionally, we briefly introduce our solution for the remaining tracks, wh
    
[^16]: MyVoice: 阿拉伯语语音资源协作平台

    MyVoice: Arabic Speech Resource Collaboration Platform. (arXiv:2308.02503v1 [eess.AS])

    [http://arxiv.org/abs/2308.02503](http://arxiv.org/abs/2308.02503)

    MyVoice是一个用于收集阿拉伯语方言语音的众包平台，提供了设计大型方言语音数据集并使其公开可用的机会。贡献者可以选择细粒度方言并记录话语，平台还提供了质量保证系统来过滤低质量和虚假的录音，并允许贡献者评估录音的质量和提供反馈。该平台还具有灵活性，管理员角色可以添加新的数据或任务，促进多样化和大规模的阿拉伯语语音数据的收集。

    

    我们介绍了MyVoice，一个用于收集阿拉伯语语音来增强方言语音技术的众包平台。该平台提供了设计大型方言语音数据集的机会，并使其公开可用。MyVoice允许贡献者选择城市/国家级的细粒度方言，并记录显示的话语。用户可以在贡献者和注释者之间切换角色。该平台采用了质量保证系统，在发送录音进行验证之前，过滤掉低质量和虚假的录音。在验证阶段，贡献者可以评估录音的质量、注释它们，并提供反馈，然后由管理员进行审核。此外，该平台提供了灵活性，使管理员角色能够添加除方言语音和单词收集之外的新数据或任务，这些任务将显示给贡献者。从而促进收集多样化和大规模的阿拉伯语语音数据的协作努力。

    We introduce MyVoice, a crowdsourcing platform designed to collect Arabic speech to enhance dialectal speech technologies. This platform offers an opportunity to design large dialectal speech datasets; and makes them publicly available. MyVoice allows contributors to select city/country-level fine-grained dialect and record the displayed utterances. Users can switch roles between contributors and annotators. The platform incorporates a quality assurance system that filters out low-quality and spurious recordings before sending them for validation. During the validation phase, contributors can assess the quality of recordings, annotate them, and provide feedback which is then reviewed by administrators. Furthermore, the platform offers flexibility to admin roles to add new data or tasks beyond dialectal speech and word collection, which are displayed to contributors. Thus, enabling collaborative efforts in gathering diverse and large Arabic speech data.
    
[^17]: 通过大型语言模型扩展临床试验匹配：以肿瘤学为案例研究

    Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology. (arXiv:2308.02180v1 [cs.CL])

    [http://arxiv.org/abs/2308.02180](http://arxiv.org/abs/2308.02180)

    本文研究了使用大型语言模型（LLMs）扩展临床试验匹配的方法，并以肿瘤学为案例研究。研究结果显示，先进的LLMs能够处理临床试验的复杂条件和匹配逻辑，相较于之前的方法，性能显著提升，并可作为人工辅助筛选患者-试验候选人的初步解决方案。

    

    临床试验匹配是医疗传递和发现中的关键过程。实际上，由于庞大的非结构化数据和不可扩展的手动处理，该过程存在问题。本文通过以肿瘤学为重点领域，对使用大型语言模型（LLM）扩展临床试验匹配进行了系统研究。我们的研究基于一个正在美国一个大型医疗网络进行测试部署的临床试验匹配系统。初步结果令人鼓舞：先进的LLM（如GPT-4）可以立即连接临床试验的复杂的合格条件，并提取复杂的匹配逻辑（例如嵌套的AND/OR/NOT）。虽然仍不完美，LLM在性能上显著优于以前的强基准线，并可能作为在人与人之间进行候选患者-试验划分的初步解决方案。我们的研究还揭示了一些应用LLM进行端到端临床试验匹配的重要增长领域，例如上下文限制和准确性。

    Clinical trial matching is a key process in health delivery and discovery. In practice, it is plagued by overwhelming unstructured data and unscalable manual processing. In this paper, we conduct a systematic study on scaling clinical trial matching using large language models (LLMs), with oncology as the focus area. Our study is grounded in a clinical trial matching system currently in test deployment at a large U.S. health network. Initial findings are promising: out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop. Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially
    
[^18]: 使用大型语言模型进行渗透测试：AI作为辅助

    Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v1 [cs.CL])

    [http://arxiv.org/abs/2308.00121](http://arxiv.org/abs/2308.00121)

    本文探讨了使用大型语言模型（如GPT3.5）作为AI助手来增强渗透测试人员的能力，实现了高级任务规划和低级漏洞寻找两种用例，取得了有前景的初步结果，并就提供该技术的伦理问题进行了讨论。

    

    软件安全测试领域，尤其是渗透测试是一项需要高水平专业知识的活动，并涉及许多手动测试和分析步骤。本文探讨了使用大型语言模型（如GPT3.5）来增强渗透测试人员的能力。我们研究了两种不同的用例：用于安全测试任务的高级任务规划和在易受攻击的虚拟机中进行低级漏洞寻找。对于后者，我们实现了一个闭环反馈，将由语言模型生成的低级操作与易受攻击的虚拟机（通过SSH连接）相连，并允许语言模型分析虚拟机状态以寻找漏洞，并提供具体的攻击向量。我们讨论了有前景的初步结果，详细介绍了改进的途径，并就提供该技术的伦理问题进行了讨论。

    The field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential usage of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providi
    
[^19]: 一种基于LSTM、BiLSTM、CNN、GRU和GloVe的混合机器学习模型用于基因突变在癌症中的分类

    A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v1 [q-bio.QM])

    [http://arxiv.org/abs/2307.14361](http://arxiv.org/abs/2307.14361)

    本研究提出了一个基于多种机器学习算法和嵌入模型的集成模型，用于基因突变在癌症中的分类。实验结果表明，该模型在准确率、精确率、召回率等指标上优于其他传统和最新的转换器模型，并且具有更高的训练效率。

    

    本研究提出了一个集成模型，将LSTM、BiLSTM、CNN、GRU和GloVe结合起来，用于在Kaggle的“个性化医学：重新定义癌症治疗”数据集中对基因突变进行分类。通过与BERT、Electra、Roberta、XLNet、Distilbert以及它们的LSTM集成等知名转换器进行比较，结果显示我们的模型在准确率、精确率、召回率、F1分数和均方误差方面都优于其他模型。令人惊讶的是，它还需要较少的训练时间，实现了性能和效率的完美结合。该研究证明了集成模型在基因突变分类等困难任务中的实用性。

    This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and GloVe to classify gene mutations using Kaggle's Personalized Medicine: Redefining Cancer Treatment dataset. The results were compared against well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and their LSTM ensembles. Our model outperformed all other models in terms of accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it also needed less training time, resulting in a perfect combination of performance and efficiency. This study demonstrates the utility of ensemble models for difficult tasks such as gene mutation classification.
    
[^20]: 大型语言模型中的上下文学习在学习标签关系上具有创新，但并非传统学习方法

    In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning. (arXiv:2307.12375v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.12375](http://arxiv.org/abs/2307.12375)

    大型语言模型（LLMs）在包含标签关系示例的上下文中的学习能力使其在下游任务中表现显著提高，但与传统学习方法不同。我们研究了上下文示例中的标签如何影响预测、预训练中学习到的标签关系如何与上下文示例相互作用以及上下文学习如何聚合标签信息。研究结果揭示了LLMs的工作机制及其对上下文信息的处理方式。

    

    在下游任务中，大型语言模型（LLMs）的性能在包含输入-标签关系示例的上下文中通常显著提高。然而，目前对LLMs的这种上下文学习（ICL）能力的工作机制尚无共识：例如，虽然Xie等人（2021年）将ICL比作一种通用学习算法，但Min等人（2022b年）认为ICL甚至不能从上下文示例中学习标签关系。在本文中，我们研究了以下三个问题：（1）上下文示例的标签如何影响预测结果，（2）预训练期间学习到的标签关系如何与上下文中提供的输入-标签示例相互作用，以及（3）ICL如何聚合来自上下文示例的标签信息。我们的研究发现，LLMs通常会整合上下文标签的信息，但预训练和上下文标签关系被区别对待，模型不会将所有上下文信息等同对待。我们的结果揭示了对LLMs的理解。

    The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into underst
    
[^21]: 生成图像特定文本改善细粒度图像分类

    Generating Image-Specific Text Improves Fine-grained Image Classification. (arXiv:2307.11315v1 [cs.CV])

    [http://arxiv.org/abs/2307.11315](http://arxiv.org/abs/2307.11315)

    本文提出了一种名为GIST的方法，用于从仅图像数据集中生成图像特定的细粒度文本描述，并通过将其用于微调视觉语言模型来改进图像分类的效果。

    

    最近的视觉语言模型在许多图像分类任务上优于仅视觉模型。然而，由于缺乏配对的文本/图像描述，对于细粒度图像分类来说，仍然很难对这些模型进行微调。在这项工作中，我们提出了一种名为GIST的方法，用于从仅图像数据集中生成图像特定的细粒度文本描述，并表明这些文本描述可以用于改善分类。我们方法的关键部分包括：1. 使用特定领域的提示为预训练的大型语言模型生成多样的细粒度文本描述，以及2. 使用预训练的视觉语言模型将每个图像与保留标签的文本描述进行匹配，这些描述捕捉了图像中相关的视觉特征。我们通过在图像和生成的文本对上微调视觉语言模型来学习一个对齐的视觉语言表示空间，以实现改进的分类。我们评估了GIST的效果。

    Recent vision-language models outperform vision-only models on many image classification tasks. However, because of the absence of paired text/image descriptions, it remains difficult to fine-tune these models for fine-grained image classification. In this work, we propose a method, GIST, for generating image-specific fine-grained text descriptions from image-only datasets, and show that these text descriptions can be used to improve classification. Key parts of our method include 1. prompting a pretrained large language model with domain-specific prompts to generate diverse fine-grained text descriptions for each class and 2. using a pretrained vision-language model to match each image to label-preserving text descriptions that capture relevant visual features in the image. We demonstrate the utility of GIST by fine-tuning vision-language models on the image-and-generated-text pairs to learn an aligned vision-language representation space for improved classification. We evaluate our l
    
[^22]: 多模态情感分析的通用去偏方法

    General Debiasing for Multimodal Sentiment Analysis. (arXiv:2307.10511v1 [cs.CL])

    [http://arxiv.org/abs/2307.10511](http://arxiv.org/abs/2307.10511)

    提出了一个通用的去偏多模态情感分析任务，通过减少模型对虚假相关性的依赖，提高多模态情感分析模型的非分布外泛化能力。

    

    现有的多模态情感分析工作利用多模态信息进行预测，但不可避免地受到多模态特征和情感标签之间的虚假相关性的影响。例如，如果数据集中大多数带有蓝色背景的视频都有正面标签，模型将依赖于这样的相关性进行预测，而“蓝色背景”并不是一个与情感相关的特征。为解决这个问题，我们定义了一个通用的去偏多模态情感分析任务，旨在通过减少模型对虚假相关性的依赖，提高多模态情感分析模型的非分布外泛化能力。为此，我们提出了一个基于倒数概率加权（Inverse Probability Weighting，IPW）的通用去偏框架，该框架自适应地为具有更大偏差（即更严重的虚假相关性）的样本分配较小的权重。这个去偏框架的关键在于估计每个样本的偏差，这通过两个步骤实现：1）将鲁棒特征和偏见特征分离出来

    Existing work on Multimodal Sentiment Analysis (MSA) utilizes multimodal information for prediction yet unavoidably suffers from fitting the spurious correlations between multimodal features and sentiment labels. For example, if most videos with a blue background have positive labels in a dataset, the model will rely on such correlations for prediction, while ``blue background'' is not a sentiment-related feature. To address this problem, we define a general debiasing MSA task, which aims to enhance the Out-Of-Distribution (OOD) generalization ability of MSA models by reducing their reliance on spurious correlations. To this end, we propose a general debiasing framework based on Inverse Probability Weighting (IPW), which adaptively assigns small weights to the samples with larger bias i.e., the severer spurious correlations). The key to this debiasing framework is to estimate the bias of each sample, which is achieved by two steps: 1) disentangling the robust features and biased featur
    
[^23]: 提高预训练语言模型的泛化能力

    Improving Pre-trained Language Models' Generalization. (arXiv:2307.10457v1 [cs.CL])

    [http://arxiv.org/abs/2307.10457](http://arxiv.org/abs/2307.10457)

    该研究提出了一种名为Mask-tuning的训练方法，通过将Masked Language Modeling (MLM)训练目标整合到微调过程中来增强预训练语言模型（PLMs）的泛化能力。实验证明，Mask-tuning在非分布数据集上超过了当前最先进的技术，并提高了PLMs在分布数据集上的性能。

    

    最先进的预训练语言模型（PLMs）的可重复使用性通常受到其泛化问题的限制，即当在与训练数据集不同的示例上进行评估时，其性能显著下降，这种示例被称为“非分布/未见示例”。这一限制源于PLMs对虚假相关性的依赖，虚假相关性对于常见示例类型效果良好，但对于一般示例效果不佳。为了解决这个问题，我们提出了一种称为Mask-tuning的训练方法，该方法将遮蔽语言建模（MLM）训练目标整合到微调过程中，以增强PLMs的泛化能力。全面的实验表明，Mask-tuning超过了当前最先进的技术，并增强了PLMs对非分布数据集的泛化能力，同时提高了它们在分布数据集上的性能。研究结果表明，Mask-tuning提高了PLMs在未见数据上的可重复使用性，使它们在实际应用中更加实用和有效。

    The reusability of state-of-the-art Pre-trained Language Models (PLMs) is often limited by their generalization problem, where their performance drastically decreases when evaluated on examples that differ from the training dataset, known as Out-of-Distribution (OOD)/unseen examples. This limitation arises from PLMs' reliance on spurious correlations, which work well for frequent example types but not for general examples. To address this issue, we propose a training approach called Mask-tuning, which integrates Masked Language Modeling (MLM) training objectives into the fine-tuning process to enhance PLMs' generalization. Comprehensive experiments demonstrate that Mask-tuning surpasses current state-of-the-art techniques and enhances PLMs' generalization on OOD datasets while improving their performance on in-distribution datasets. The findings suggest that Mask-tuning improves the reusability of PLMs on unseen data, making them more practical and effective for real-world applications
    
[^24]: NTK-近似MLP融合用于高效的语言模型微调

    NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning. (arXiv:2307.08941v1 [cs.LG])

    [http://arxiv.org/abs/2307.08941](http://arxiv.org/abs/2307.08941)

    该论文通过使用神经切向核近似MLP融合，提出了一种高效的语言模型微调方法。实验证明，这种方法能够在降低计算和存储开销的同时保持较好的模型性能。

    

    在许多自然语言处理应用中，微调预训练语言模型(PLM)已成为主要策略。然而，即使是微调PLM和进行推理也是昂贵的，特别是在计算能力较低的边缘设备上。已经广泛研究了一些通用的方法（例如量化和蒸馏）来减少PLM微调的计算/存储开销，但很少有一次性压缩技术被探索。在本文中，我们研究了多层感知器(MLP)模块中预训练语言模型(PLM)的神经切向核(NTK)，并提出通过NTK近似MLP融合来创建一个轻量级的PLM。为实现这一目标，我们将MLP重新视为一束子MLP，并将它们聚类为给定数量的质心，然后将其恢复为压缩的MLP，并意外地显示出对原始PLM的NTK进行良好近似的效果。在自然语言处理数据集上进行了大量实验以验证PLM微调的效果。

    Fine-tuning a pre-trained language model (PLM) emerges as the predominant strategy in many natural language processing applications. However, even fine-tuning the PLMs and doing inference are expensive, especially on edge devices with low computing power. Some general approaches (e.g. quantization and distillation) have been widely studied to reduce the compute/memory of PLM fine-tuning, while very few one-shot compression techniques are explored. In this paper, we investigate the neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight PLM through NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a bundle of sub-MLPs, and cluster them into a given number of centroids, which can then be restored as a compressed MLP and surprisingly shown to well approximate the NTK of the original PLM. Extensive experiments of PLM fine-tuning on both natural l
    
[^25]: CAME: 靠自信指导的自适应内存高效优化

    CAME: Confidence-guided Adaptive Memory Efficient Optimization. (arXiv:2307.02047v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.02047](http://arxiv.org/abs/2307.02047)

    CAME是一种通过自信指导的策略来实现快速收敛并降低内存使用的自适应内存高效优化方法，在各种自然语言处理任务中表现出稳定性和优越性能。

    

    自适应梯度方法，如Adam和LAMB，在大规模语言模型的训练中表现出色。然而，适应性需要维护每个参数梯度的二阶矩估计，这带来了高额的额外内存开销。为了解决这个问题，提出了一些内存高效的优化器（如Adafactor）来大幅减少辅助内存的使用，但会降低性能。本文首先研究了一种基于自信度的策略来减少现有内存高效优化器的不稳定性。基于这一策略，我们提出了CAME，以同时实现两个目标：快速收敛，与传统的自适应方法相似，以及低内存使用率，与内存高效方法相似。大量实验证明了CAME在各种自然语言处理任务（例如BERT和GPT-2训练）中的训练稳定性和优越性能。值得注意的是，在大批量的BERT预训练中，CAME比其他方法训练速度更快，并且具有相似的性能。

    Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of 
    
[^26]: 大语言模型时代的推荐系统 (LLMs)

    Recommender Systems in the Era of Large Language Models (LLMs). (arXiv:2307.02046v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2307.02046](http://arxiv.org/abs/2307.02046)

    大型语言模型在推荐系统中的应用已经带来了显著的改进，克服了传统DNN方法的限制，并提供了强大的语言理解、生成、推理和泛化能力。

    

    随着电子商务和网络应用的繁荣，推荐系统（RecSys）已经成为我们日常生活中重要的组成部分，为用户提供个性化建议以满足其喜好。尽管深度神经网络（DNN）通过模拟用户-物品交互和整合文本侧信息在提升推荐系统方面取得了重要进展，但是DNN方法仍然存在一些限制，例如理解用户兴趣、捕捉文本侧信息的困难，以及在不同推荐场景中泛化和推理能力的不足等。与此同时，大型语言模型（LLMs）的出现（例如ChatGPT和GPT4）在自然语言处理（NLP）和人工智能（AI）领域引起了革命，因为它们在语言理解和生成的基本职责上有着卓越的能力，同时具有令人印象深刻的泛化和推理能力。

    With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, 
    
[^27]: 使用语法演化自动设计语义相似性集合

    Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution. (arXiv:2307.00925v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.00925](http://arxiv.org/abs/2307.00925)

    本研究首次使用语法演化自动设计语义相似性集合，通过自动选择和聚合候选度量来优化集合与人类判断的相关性，提高相似度评估准确性，并证明了使用集合对语义相似性任务的益处。

    

    语义相似性度量在自然语言处理中被广泛应用于多种与计算机相关的任务。然而，没有单一的语义相似性度量适用于所有任务，研究人员经常使用集合策略来确保性能。本研究提出了一种自动设计语义相似性集合的方法。事实上，我们提出的方法首次使用语法演化来自动选择和聚合一组候选度量，以创建一个最大化与人类判断相关性的集合。该方法在多个基准数据集上进行了评估，并与最先进的集合进行了比较，结果显示它可以显著提高相似度评估的准确性，并在某些情况下优于现有方法。因此，我们的研究既展示了使用语法演化来自动比较文本的潜力，也证明了使用集合对语义相似性任务的益处。

    Semantic similarity measures are widely used in natural language processing to catalyze various computer-related tasks. However, no single semantic similarity measure is the most appropriate for all tasks, and researchers often use ensemble strategies to ensure performance. This research work proposes a method for automatically designing semantic similarity ensembles. In fact, our proposed method uses grammatical evolution, for the first time, to automatically select and aggregate measures from a pool of candidates to create an ensemble that maximizes correlation to human judgment. The method is evaluated on several benchmark datasets and compared to state-of-the-art ensembles, showing that it can significantly improve similarity assessment accuracy and outperform existing methods in some cases. As a result, our research demonstrates the potential of using grammatical evolution to automatically compare text and prove the benefits of using ensembles for semantic similarity tasks. The so
    
[^28]: 一种统一的模型：排序斯洛文摘要生成器

    One model to rule them all: ranking Slovene summarizers. (arXiv:2306.11518v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.11518](http://arxiv.org/abs/2306.11518)

    提出了一种系统，为给定的文本推荐最适合的摘要生成模型。该系统采用一个全连接神经网络，根据输入内容分析并预测最佳摘要生成器。四种斯洛文摘要生成模型解决了在资源有限的语言中进行文本摘要的不同挑战。实验结果表明，所提出的模型在斯洛文摘要生成上表现良好。

    

    文本摘要生成是自然语言处理中一项重要任务，研究人员多年来已经开发了各种方法，从基于规则的系统到神经网络。然而，并没有一种单一的模型或方法可以在所有类型的文本上表现良好。我们提出了一种系统，为给定的文本推荐最适合的摘要生成模型。所提出的系统采用了一个全连接神经网络，分析输入内容并预测哪种摘要生成器在给定输入的ROUGE分数方面表现最佳。元模型在四种不同的斯洛文摘要生成模型之间选择，这些模型是针对斯洛文语言开发的，使用输入的不同属性，特别是其Doc2Vec文档表示。这四个斯洛文摘要生成模型解决了与在语言资源有限的语言中进行文本摘要的不同挑战。我们自动评估了所提出的SloMetaSum模型的性能，并进行了部分手动评估。结果显示，所提出的模型在斯洛文摘要生成上表现良好。

    Text summarization is an essential task in natural language processing, and researchers have developed various approaches over the years, ranging from rule-based systems to neural networks. However, there is no single model or approach that performs well on every type of text. We propose a system that recommends the most suitable summarization model for a given text. The proposed system employs a fully connected neural network that analyzes the input content and predicts which summarizer should score the best in terms of ROUGE score for a given input. The meta-model selects among four different summarization models, developed for the Slovene language, using different properties of the input, in particular its Doc2Vec document representation. The four Slovene summarization models deal with different challenges associated with text summarization in a less-resourced language. We evaluate the proposed SloMetaSum model performance automatically and parts of it manually. The results show tha
    
[^29]: 通过连续后验推理实现多样和忠实的知识驱动对话生成

    Diverse and Faithful Knowledge-Grounded Dialogue Generation via Sequential Posterior Inference. (arXiv:2306.01153v1 [cs.CL])

    [http://arxiv.org/abs/2306.01153](http://arxiv.org/abs/2306.01153)

    本文提出了一种名为SPI的端到端学习框架，能够忠实地生成多样化的基于知识的对话。

    

    利用事实知识生成具有多样性和忠实性的回复对于创建类人、可信任的对话系统非常重要。本文提出一种名为"连续后验推理（SPI）"的端到端学习框架，能够通过对后验分布进行近似采样来选择知识并生成对话，与现有方法不同，SPI不需要推理网络或假设后验分布具有一个简单的几何结构，其直接查询响应生成模型的推理过程直接，从而实现准确的知识选择和生成。

    The capability to generate responses with diversity and faithfulness using factual knowledge is paramount for creating a human-like, trustworthy dialogue system. Common strategies either adopt a two-step paradigm, which optimizes knowledge selection and response generation separately, and may overlook the inherent correlation between these two tasks, or leverage conditional variational method to jointly optimize knowledge selection and response generation by employing an inference network. In this paper, we present an end-to-end learning framework, termed Sequential Posterior Inference (SPI), capable of selecting knowledge and generating dialogues by approximately sampling from the posterior distribution. Unlike other methods, SPI does not require the inference network or assume a simple geometry of the posterior distribution. This straightforward and intuitive inference procedure of SPI directly queries the response generation model, allowing for accurate knowledge selection and gener
    
[^30]: 基于邻域比较的语言模型成员推断攻击

    Membership Inference Attacks against Language Models via Neighbourhood Comparison. (arXiv:2305.18462v1 [cs.CL])

    [http://arxiv.org/abs/2305.18462](http://arxiv.org/abs/2305.18462)

    本文提出两种新的基于邻域比较的攻击策略，利用语言数据的内在结构来提高成员推断攻击的性能，并在几个公开数据集上证明这些攻击的有效性。

    

    成员推断攻击(MIAs)旨在预测一个数据样本是否存在于机器学习模型的训练数据中，广泛用于评估语言模型的隐私风险。现有的大多数攻击依赖于这样一个观察结果，即模型倾向于将更高的概率分配给训练样本而非非训练点。然而，对模型分数的简单阈值设定往往导致高误报率，因为它没有考虑样本的内在复杂性。最近的研究表明，基于参考模型的攻击可以将模型分数与在类似数据上训练的参考模型获得的分数进行比较，可以显著提高MIAs的性能。然而，为了训练参考模型，这种攻击的做法是假定敌方知道与原始训练数据密切相似的样本，这是一个强假设。因此，我们在更现实的情况下，假定攻击者只能访问有限的邻域样本，研究了这些攻击的性能。我们提出了两种新的攻击策略，利用语言数据的内在结构，可以用于评估在更现实的成员推断场景下的语言模型的隐私风险。我们的实验表明，我们的攻击在几个公开可用的数据集上是有效的，其中包括文本分类、自然语言推理和对话生成，并突显了语言模型在实际应用中的潜在隐私风险。

    Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend to assign higher probabilities to their training samples than non-training points. However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample. Recent work has demonstrated that reference-based attacks which compare model scores to those obtained from a reference model trained on similar data can substantially improve the performance of MIAs. However, in order to train reference models, attacks of this kind make the strong and arguably unrealistic assumption that an adversary has access to samples closely resembling the original training data. Therefore, we investigate their performance in more realistic sce
    
[^31]: 提升联合学习的视觉语言预训练：基于联合学习的问答与密集字幕生成

    Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner. (arXiv:2305.11769v1 [cs.CV])

    [http://arxiv.org/abs/2305.11769](http://arxiv.org/abs/2305.11769)

    本文提出了一种名为JADE的新方法，可以利用易于获取的图像-文本对进行的联合学习，以提升视觉和语言模态的细粒度特征对齐，从而更好地进行视觉问答和密集字幕生成。

    

    大型预先训练的多模态模型在许多下游任务中都表现出显著的成功，包括图像字幕生成、图像文本检索和视觉问答等。然而，许多方法都依赖于从网络上收集的图像-文本对作为预先训练的数据，忽视了视觉和语言模态之间需要细粒度特征对齐的需求，这需要对图像和语言表达进行详细的理解。将视觉问答和密集字幕集成到预先训练中可以解决这个问题，但是获取图像-问题-答案以及图像-位置-字幕三元组是具有挑战性和耗时的。此外，公开可用的视觉问答和密集字幕数据集通常由于手动数据收集和标注而规模有限。在本文中，我们提出了一种新方法，称为联合问答和密集字幕生成（JADE），它利用预先训练的多模态模型和易于获取的图像-文本对来进行模型训练。

    Large pre-trained multimodal models have demonstrated significant success in a range of downstream tasks, including image captioning, image-text retrieval, visual question answering (VQA), etc. However, many of these methods rely on image-text pairs collected from the web as pre-training data and unfortunately overlook the need for fine-grained feature alignment between vision and language modalities, which requires detailed understanding of images and language expressions. While integrating VQA and dense captioning (DC) into pre-training can address this issue, acquiring image-question-answer as well as image-location-caption triplets is challenging and time-consuming. Additionally, publicly available datasets for VQA and dense captioning are typically limited in scale due to manual data collection and labeling efforts. In this paper, we propose a novel method called Joint QA and DC GEneration (JADE), which utilizes a pre-trained multimodal model and easily-crawled image-text pairs to
    
[^32]: Graphologue：用交互式图表探索大型语言模型响应

    Graphologue: Exploring Large Language Model Responses with Interactive Diagrams. (arXiv:2305.11473v1 [cs.HC])

    [http://arxiv.org/abs/2305.11473](http://arxiv.org/abs/2305.11473)

    Graphologue是一个交互式系统，将大型语言模型的基于文本的响应转换为图形化图表以增强其可用性和可解释性，用户可以通过选择和突出显示特定节点和链接来与这些图表进行交互。

    

    大型语言模型（LLM）由于易于获取和在多种应用中表现出的前所未有的智能而近来风靡一时。然而，像ChatGPT这样的LLM在支持复杂信息任务方面存在显着的限制，原因是基于文本的媒介和线性对话结构提供的功能不足。通过与十名参与者的形式化研究，我们发现LLM界面通常会呈现冗长的响应，使人们难以快速理解和灵活地与各种信息进行交互，特别是在更复杂的任务中。我们提出了Graphologue，这是一个交互式系统，将LLM的基于文本的响应转换为图形化图表，以便于信息查找和问题回答任务。Graphologue采用新颖的提示策略和界面设计，从LLM响应中提取实体和关系，并实时构建节点链接图。此外，用户可以通过选择和突出显示特定节点和链接来与这些图表进行交互，以探索相关信息和跟进问题。我们的用户研究结果表明，与传统的基于文本的界面相比，Graphologue显著提高了用户在复杂信息任务中的表现和满意度。Graphologue为增强LLM在各种应用和领域中的可用性和可解释性提供了一个有前景的方向。

    Large language models (LLMs) have recently soared in popularity due to their ease of access and the unprecedented intelligence exhibited on diverse applications. However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure. Through a formative study with ten participants, we found that LLM interfaces often present long-winded responses, making it difficult for people to quickly comprehend and interact flexibly with various pieces of information, particularly during more complex tasks. We present Graphologue, an interactive system that converts text-based responses from LLMs into graphical diagrams to facilitate information-seeking and question-answering tasks. Graphologue employs novel prompting strategies and interface designs to extract entities and relationships from LLM responses and constructs node-link diagrams in real-time. Further, users can int
    
[^33]: 增强本地谱时特征用于语音分析

    Boosting Local Spectro-Temporal Features for Speech Analysis. (arXiv:2305.10270v1 [cs.CL])

    [http://arxiv.org/abs/2305.10270](http://arxiv.org/abs/2305.10270)

    该论文介绍了在语音识别中电话分类的问题，并探索了几组可以用于电话分类的本地谱时特征，提出了使用Haar特征和SVM分类的梯度直方图进行电话分类，并给出了一些初步结果。

    

    我们介绍了在语音识别中，电话分类的问题，并探索了几组可以用于电话分类的本地谱时特征。特别地，我们提出了使用两组常用于物体检测的特征（Haar特征和SVM分类的梯度直方图（HoG））进行电话分类的一些初步结果。

    We introduce the problem of phone classification in the context of speech recognition, and explore several sets of local spectro-temporal features that can be used for phone classification. In particular, we present some preliminary results for phone classification using two sets of features that are commonly used for object detection: Haar features and SVM-classified Histograms of Gradients (HoG)
    
[^34]: 从自然语言文本中生成流程模型的方法——基于规则之外的命名实体识别和关系抽取(arXiv:2305.03960v1 [cs.CL])

    Beyond Rule-based Named Entity Recognition and Relation Extraction for Process Model Generation from Natural Language Text. (arXiv:2305.03960v1 [cs.CL])

    [http://arxiv.org/abs/2305.03960](http://arxiv.org/abs/2305.03960)

    本文扩展了PET数据集，通过聚类流程实体的提及，提出了一种新的基线技术流程提取方法，该方法避免了手动创建业务流程模型的繁琐工作，同时解决了同一流程实体重复提及的歧义问题。

    

    从自然语言文本自动生成业务流程模型是一种新兴方法，可避免手动创建形式化业务流程模型。为此，需要从文本流程描述中提取出流程实体（如参与者、活动、对象等）和它们之间的关系。一个高质量的带有文本流程描述的注释语料库(PET)已经出版，其伴随着一种基本的流程提取方法。然而，在其当前状态下，PET缺乏有关两个提及是否指代了相同或不同的流程实体的信息，这对于是否在目标模型中创建一个或两个建模元素的重要决策相对应。因此，例如，两个数据处理的提及是否意味着处理不同或相同的数据是不确定的。在本文中，我们通过聚类流程实体的提及来扩展PET数据集，并提出了一种新的基线技术流程提取方法，其中包含一个

    Automated generation of business process models from natural language text is an emerging methodology for avoiding the manual creation of formal business process models. For this purpose, process entities like actors, activities, objects etc., and relations among them are extracted from textual process descriptions. A high-quality annotated corpus of textual process descriptions (PET) has been published accompanied with a basic process extraction approach. In its current state, however, PET lacks information about whether two mentions refer to the same or different process entities, which corresponds to the crucial decision of whether to create one or two modeling elements in the target model. Consequently, it is ambiguous whether, for instance, two mentions of data processing mean processing of different, or the same data. In this paper, we extend the PET dataset by clustering mentions of process entities and by proposing a new baseline technique for process extraction equipped with a
    
[^35]: 基于归因的防御插入式文本后门攻击

    Defending against Insertion-based Textual Backdoor Attacks via Attribution. (arXiv:2305.02394v1 [cs.CL])

    [http://arxiv.org/abs/2305.02394](http://arxiv.org/abs/2305.02394)

    本文提出了一种基于归因的管道AttDef，用于防御两种插入式污染攻击BadNL和InSent，该管道可以成功缓解插入式文本后门攻击并在四个基准数据集上平均提高了56.59%至79.97%和15.25%至48.34%的准确率。

    

    文本后门攻击是一种新型攻击模式，已被证明在训练期间向模型添加后门是有效的。防御此类后门攻击已变得紧迫和重要。本文提出了一种名为AttDef的高效归因管道，用于防御两种插入式污染攻击BadNL和InSent。具体而言，我们将具有较大归因分数的令牌视为潜在触发器，因为较大的归因词对于错误预测结果做出较大贡献，因此更有可能是污染触发器。此外，我们进一步利用外部预训练语言模型来区分输入是否被污染。我们展示了我们的方法可以在两种常见的攻击场景（污染训练数据和测试数据）中具有足够的泛化性，这一点持续改善了之前的方法。例如，AttDef在四个基准数据集上可以成功缓解两种攻击，平均准确率为79.97%（提高了56.59%）和48.34%（提高了15.25%），证明了它在防御插入式文本后门攻击方面的有效性。

    Textual backdoor attack, as a novel attack model, has been shown to be effective in adding a backdoor to the model during training. Defending against such backdoor attacks has become urgent and important. In this paper, we propose AttDef, an efficient attribution-based pipeline to defend against two insertion-based poisoning attacks, BadNL and InSent. Specifically, we regard the tokens with larger attribution scores as potential triggers since larger attribution words contribute more to the false prediction results and therefore are more likely to be poison triggers. Additionally, we further utilize an external pre-trained language model to distinguish whether input is poisoned or not. We show that our proposed method can generalize sufficiently well in two common attack scenarios (poisoning training data and testing data), which consistently improves previous methods. For instance, AttDef can successfully mitigate both attacks with an average accuracy of 79.97% (56.59% up) and 48.34% 
    
[^36]: 缺失信息、无回应作者、实验缺陷：NLP中不可能评估以前的人类评估的再现性。

    Missing Information, Unresponsive Authors, Experimental Flaws: The Impossibility of Assessing the Reproducibility of Previous Human Evaluations in NLP. (arXiv:2305.01633v1 [cs.CL])

    [http://arxiv.org/abs/2305.01633](http://arxiv.org/abs/2305.01633)

    该论文研究了NLP领域过去的人类评估的再现性问题，结果发现大部分人类评估都无法重复或再现，可能是由于缺失信息、无回应作者和实验缺陷等原因导致。这个结果提示我们需要重新考虑如何设计和报告人类评估实验。

    

    我们报告了我们在识别一组先前适合进行协调研究的NLP领域人类评估的努力，以考察是什么使得NLP领域的人类评估更/ less能再现。我们提供了我们的结果和发现，其中包括仅有13％的论文具有（i）足够低的再现障碍，以及（ii）足够的可获取信息，才可以被考虑进行再现，并且我们选择进行再现的所有实验都被发现存在缺陷，这使得进行再现的有意义性值得怀疑。因此，我们不得不将我们的协调研究设计从再现方法更改为标准化-然后再现两次的方法。我们总体而言（是负面的）发现，NLP领域中的绝大多数人类评估都不能重复和/或不能再现和/或具有太多缺陷以证明其可再现性。这描绘了一个可怕的画面，但也为重新考虑如何设计和报告NLP领域的人类评估提供了机会。

    We report our efforts in identifying a set of previous human evaluations in NLP that would be suitable for a coordinated study examining what makes human evaluations in NLP more/less reproducible. We present our results and findings, which include that just 13\% of papers had (i) sufficiently low barriers to reproduction, and (ii) enough obtainable information, to be considered for reproduction, and that all but one of the experiments we selected for reproduction was discovered to have flaws that made the meaningfulness of conducting a reproduction questionable. As a result, we had to change our coordinated study design from a reproduce approach to a standardise-then-reproduce-twice approach. Our overall (negative) finding that the great majority of human evaluations in NLP is not repeatable and/or not reproducible and/or too flawed to justify reproduction, paints a dire picture, but presents an opportunity for a rethink about how to design and report human evaluations in NLP.
    
[^37]: 从弱文本监督中学习图像中的人际互动

    Learning Human-Human Interactions in Images from Weak Textual Supervision. (arXiv:2304.14104v1 [cs.CV])

    [http://arxiv.org/abs/2304.14104](http://arxiv.org/abs/2304.14104)

    本文提出了一种新的范式，从单一的静态图像中学习自由文本的形式来灵活建模人际互动。并通过知识蒸馏生成伪标签来训练一种字幕模型，用于有效理解图像中的人际互动，具有较高的预测文本和语义质量，并在此任务上优于SOTA的图像字幕和情境识别模型。

    

    人际互动是多样且依赖于上下文的，但先前的工作将它们视为分类，忽略了可能的互动的重尾。本文提出了一种新的学习人际互动的范式，将其作为自由文本从单一的静态图像中学习，从而允许对情况和人际关系的无限空间进行灵活建模。为了克服缺乏特定于此任务的标记数据的问题，我们使用知识蒸馏应用于由大型语言模型产生的合成字幕数据，以此生成伪标签。我们展示了通过这个过程产生的伪标签可以用于训练一种字幕模型，能有效理解图像中的人际互动，通过衡量我们预测的文本和语义质量与事实的基础性的各种指标来衡量。我们进一步展示了我们的方法在这个任务上的性能优于SOTA的图像字幕和情境识别模型。我们将公开我们的代码。

    Interactions between humans are diverse and context-dependent, but previous works have treated them as categorical, disregarding the heavy tail of possible interactions. We propose a new paradigm of learning human-human interactions as free text from a single still image, allowing for flexibility in modeling the unlimited space of situations and relationships between people. To overcome the absence of data labelled specifically for this task, we use knowledge distillation applied to synthetic caption data produced by a large language model without explicit supervision. We show that the pseudo-labels produced by this procedure can be used to train a captioning model to effectively understand human-human interactions in images, as measured by a variety of metrics that measure textual and semantic faithfulness and factual groundedness of our predictions. We further show that our approach outperforms SOTA image captioning and situation recognition models on this task. We will release our c
    
[^38]: 基于检索的知识增强视觉语言预训练模型

    Retrieval-based Knowledge Augmented Vision Language Pre-training. (arXiv:2304.13923v1 [cs.CV])

    [http://arxiv.org/abs/2304.13923](http://arxiv.org/abs/2304.13923)

    本文提出了一种基于检索的知识增强视觉语言预训练模型，将从知识图谱中检索到的世界知识融入视觉语言预训练中，将明确的知识与视觉语言对融合，通过四个知识感知的自监督任务推动多模态数据和知识的相互整合。

    

    随着大规模视觉和语言表示学习的进展，视觉语言预训练(VLP)模型在各种多模态下游任务中取得了可喜的进展。然而，这些预训练模型仍未利用世界知识，世界知识隐含在多模态数据中，但包含丰富和互补的信息。在本工作中，我们提出了一种REtrieval-based knowledge Augmented Vision Language Pre-training (REAVL)模型，它从知识图谱(KGs)中检索世界知识，并将其融入视觉语言预训练中。

    With recent progress in large-scale vision and language representation learning, Vision Language Pretraining (VLP) models have achieved promising improvements on various multi-modal downstream tasks. Albeit powerful, these pre-training models still do not take advantage of world knowledge, which is implicit in multi-modal data but comprises abundant and complementary information. In this work, we propose a REtrieval-based knowledge Augmented Vision Language Pre-training model (REAVL), which retrieves world knowledge from knowledge graphs (KGs) and incorporates them in vision-language pre-training. REAVL has two core components: a knowledge retriever that retrieves knowledge given multi-modal data, and a knowledge-augmented model that fuses multi-modal data and knowledge. By novelly unifying four knowledge-aware self-supervised tasks, REAVL promotes the mutual integration of multi-modal data and knowledge by fusing explicit knowledge with vision-language pairs for masked multi-modal dat
    
[^39]: 自监督多模态学习：一项综述

    Self-Supervised Multimodal Learning: A Survey. (arXiv:2304.01008v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.01008](http://arxiv.org/abs/2304.01008)

    自监督多模态学习是一项旨在解决多模态数据中的自监督学习挑战的研究方向。它通过学习来自原始多模态数据中的表示，并解决了没有标签的多模态数据学习、不同模态的融合和不对齐数据学习等问题。

    

    多模态学习旨在理解和分析来自多种模态的信息，在监督学习范式下取得了重大进展。然而，由于依赖于配对数据和昂贵的人工注释，模型的扩展性受到了限制。与此同时，鉴于野外有大规模未注释的数据可用，自监督学习成为缓解注释瓶颈的一种有吸引力的策略。自监督多模态学习（SSML）建立在这两个方向的基础上，提供了从原始多模态数据中学习的方法。在本综述中，我们全面回顾了SSML的最新进展，阐述了自监督学习在多模态数据中面临的三个主要挑战：（1）在没有标签的多模态数据中学习表示，（2）不同模态的融合，以及（3）与不对齐数据的学习。然后，我们详细介绍了这些挑战的现有解决方案。具体而言，我们考虑了（1）目标

    Multimodal learning, which aims to understand and analyze information from multiple modalities, has achieved substantial progress in the supervised regime in recent years. However, the heavy dependence on data paired with expensive human annotations impedes scaling up models. Meanwhile, given the availability of large-scale unannotated data in the wild, self-supervised learning has become an attractive strategy to alleviate the annotation bottleneck. Building on these two directions, self-supervised multimodal learning (SSML) provides ways to learn from raw multimodal data. In this survey, we provide a comprehensive review of the state-of-the-art in SSML, in which we elucidate three major challenges intrinsic to self-supervised learning with multimodal data: (1) learning representations from multimodal data without labels, (2) fusion of different modalities, and (3) learning with unaligned data. We then detail existing solutions to these challenges. Specifically, we consider (1) object
    
[^40]: LLM用于患者-试验匹配: 面向更好的性能和泛化能力的隐私感知数据增强

    LLM for Patient-Trial Matching: Privacy-Aware Data Augmentation Towards Better Performance and Generalizability. (arXiv:2303.16756v1 [cs.CL])

    [http://arxiv.org/abs/2303.16756](http://arxiv.org/abs/2303.16756)

    本文提出了一种隐私感知数据增强的LLM-PTM方法，有效地提高了患者-试验匹配的性能和泛化能力。

    

    将患者与适合的临床试验进行匹配是推进医学研究和提供最佳护理的关键。然而，现有方法面临数据标准化、伦理考虑和电子健康记录与临床试验标准之间互操作性缺乏等挑战。在本文中，我们探索利用大型语言模型（LLMs）解决这些挑战的潜力，通过利用其先进的自然语言生成能力来改善EHRs和临床试验描述之间的兼容性。我们提出了一种创新的基于LLM的患者-试验匹配（LLM-PTM）的隐私感知数据增强方法，平衡了LLMs的好处，同时确保敏感患者数据的安全和保密。我们的实验表明，使用所提出的LLM-PTM方法，性能平均提高了7.32％，新数据的泛化能力提高了12.12％。此外，我们还提供了案例研究。

    The process of matching patients with suitable clinical trials is essential for advancing medical research and providing optimal care. However, current approaches face challenges such as data standardization, ethical considerations, and a lack of interoperability between Electronic Health Records (EHRs) and clinical trial criteria. In this paper, we explore the potential of large language models (LLMs) to address these challenges by leveraging their advanced natural language generation capabilities to improve compatibility between EHRs and clinical trial descriptions. We propose an innovative privacy-aware data augmentation approach for LLM-based patient-trial matching (LLM-PTM), which balances the benefits of LLMs while ensuring the security and confidentiality of sensitive patient data. Our experiments demonstrate a 7.32% average improvement in performance using the proposed LLM-PTM method, and the generalizability to new data is improved by 12.12%. Additionally, we present case stud
    
[^41]: eP-ALM:语言模型的高效感知增强

    eP-ALM: Efficient Perceptual Augmentation of Language Models. (arXiv:2303.11403v1 [cs.CV])

    [http://arxiv.org/abs/2303.11403](http://arxiv.org/abs/2303.11403)

    本论文提出了一种用对比学习提高语言模型的感知能力的高效方法eP-ALM，可以实现视觉感知信息和文本信息的融合，同时还能在多模态基准测试上实现最先进的结果。

    

    大型语言模型(LLM)迄今为止给世界留下了深刻印象，具有大规模模型所具有的非同寻常的能力。在视觉方面，变压器模型（即ViT）也在追随同一趋势，取得了最具挑战性的基准测试的最佳表现。随着这种单模型的丰富多样，自然会引发一个问题：我们是否需要跟随这个趋势来处理多模态任务？在这项工作中，我们提出将努力集中于现有模型的高效适应，并提出用感知来增强语言模型。现有的适应预训练模型用于视觉语言任务的方法仍然依赖于几个关键组件，从而影响了它们的效率。特别地，他们仍然训练大量的参数，依赖大规模的多模态预训练，使用在巨大的图像-文本数据集上训练的编码器（例如CLIP），并添加了显著的推理开销。此外，这些方法中的大多数关注Zero-Shot和In Context Learning，观察到两种范式之间的巨大差异。在本文中，我们介绍了eP-ALM，一种将视觉感知信息与语言模型相结合的高效方法。我们提出了一种方法，利用对比学习来实现视觉感知和文本信息的融合，具有极小的计算成本。我们的方法不需要任何新的预训练，仍然在多模态基准测试上实现了最先进的结果。

    Large Language Models (LLMs) have so far impressed the world, with unprecedented capabilities that emerge in models at large scales. On the vision side, transformer models (i.e., ViT) are following the same trend, achieving the best performance on challenging benchmarks. With the abundance of such unimodal models, a natural question arises; do we need also to follow this trend to tackle multimodal tasks? In this work, we propose to rather direct effort to efficient adaptations of existing models, and propose to augment Language Models with perception. Existing approaches for adapting pretrained models for vision-language tasks still rely on several key components that hinder their efficiency. In particular, they still train a large number of parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP) trained on huge image-text datasets, and add significant inference overhead. In addition, most of these approaches have focused on Zero-Shot and In Context Learning, with l
    
[^42]: 摘要即标题：利用自动化文本摘要生成科学文献的插图标题

    Summaries as Captions: Generating Figure Captions for Scientific Documents with Automated Text Summarization. (arXiv:2302.12324v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.12324](http://arxiv.org/abs/2302.12324)

    本文提出了利用自动化文本摘要生成科学文献的插图标题的方法，并使用预训练的抽象化摘要模型 PEGASUS 对引用图表的段落进行摘要。实验结果表明该方法在自动评估和人工评估中均优于之前的视觉方法。研究还发现了两个关键挑战：低质量作者撰写的标题的普遍存在以及对好标题缺乏明确的标准。

    

    良好的插图标题可以帮助论文读者理解复杂的科学图表。然而，即使是已发表的论文，其标题常常写得很差。自动生成标题可以帮助论文作者提供良好的起始标题，以便进一步改进质量。之前的工作常将插图标题生成视为一项视觉到语言的任务。本文中，我们表明将其作为科学文献中的文本摘要任务更为有效。我们对预训练的抽象化摘要模型 PEGASUS 进行了微调，专门用于将引用图表的段落（例如，“图3显示...”）摘要为图表标题。在大规模 arXiv 图表上进行的实验表明，我们的方法在自动评估和人工评估中均优于之前的视觉方法。我们还进行了深入的研究，重点关注两个关键挑战：(i) 低质量作者撰写的标题的普遍存在以及 (ii) 对好标题缺乏明确的标准。我们提供了代码

    Good figure captions help paper readers understand complex scientific figures. Unfortunately, even published papers often have poorly written captions. Automatic caption generation could aid paper writers by providing good starting captions that can be refined for better quality. Prior work often treated figure caption generation as a vision-to-language task. In this paper, we show that it can be more effectively tackled as a text summarization task in scientific documents. We fine-tuned PEGASUS, a pre-trained abstractive summarization model, to specifically summarize figure-referencing paragraphs (e.g., "Figure 3 shows...") into figure captions. Experiments on large-scale arXiv figures show that our method outperforms prior vision methods in both automatic and human evaluations. We further conducted an in-depth investigation focused on two key challenges: (i) the common presence of low-quality author-written captions and (ii) the lack of clear standards for good captions. Our code and
    
[^43]: 选择性解释：利用人类输入对齐可解释人工智能

    Selective Explanations: Leveraging Human Input to Align Explainable AI. (arXiv:2301.09656v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.09656](http://arxiv.org/abs/2301.09656)

    本研究提出一种通过利用人类输入生成选择性解释的通用框架，以弥合可解释人工智能（XAI）与人类解释的差距，并且在决策支持任务中进行了实验证明其有效性。

    

    近年来，出现了大量的可解释人工智能（XAI）算法，但它们经常因与人类解释的生产和消费方式存在显著差距而受到批评。因此，目前的XAI技术往往难以使用并缺乏有效性。在本文中，我们尝试通过使AI解释具有选择性（这是人类解释的基本属性之一）来弥合这些差距，通过根据接收方的偏好有选择性地呈现大量模型原因的子集来实现。我们提出了一个通用的框架，通过利用小样本上的人类输入来生成选择性解释。该框架开辟了一个丰富的设计空间，涵盖了不同的选择性目标、输入类型等。作为一个展示，我们使用决策支持任务来探索基于决策者认为相关的选择性解释。我们进行了两项实验研究，以检查从大一组模型原因中选择的三个子集与未选择的子集相比，选择性解释的效果。

    While a vast collection of explainable AI (XAI) algorithms have been developed in recent years, they are often criticized for significant gaps with how humans produce and consume explanations. As a result, current XAI techniques are often found to be hard to use and lack effectiveness. In this work, we attempt to close these gaps by making AI explanations selective -- a fundamental property of human explanations -- by selectively presenting a subset from a large set of model reasons based on what aligns with the recipient's preferences. We propose a general framework for generating selective explanations by leveraging human input on a small sample. This framework opens up a rich design space that accounts for different selectivity goals, types of input, and more. As a showcase, we use a decision-support task to explore selective explanations based on what the decision-maker would consider relevant to the decision task. We conducted two experimental studies to examine three out of a bro
    
[^44]: TikTalk: 一个用于多模态真实世界闲聊的基于视频的对话数据集

    TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World. (arXiv:2301.05880v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.05880](http://arxiv.org/abs/2301.05880)

    TikTalk是一个基于视频的多模态对话数据集，用于研究智能且类似人类的闲聊机器人。数据集包含从流行视频分享平台收集的38K个视频和367K个用户对话。与其他数据集相比，TikTalk提供了更丰富的上下文类型，同时也增加了从复杂的多模态信息中生成个性化回答的难度。数据集中还更频繁地引用了外部知识，为多模态对话模型提供了新的挑战。

    

    为了促进多模态上下文中智能和人类化聊天机器人的研究，我们引入了一个新的基于视频的多模态对话数据集，称为TikTalk。我们从一个流行的视频分享平台收集了38K个视频，以及用户在其下发布的367K个对话。用户根据他们观看视频时的多模态经验进行自发性对话，这有助于重现真实世界的闲聊环境。与之前的多模态对话数据集相比，TikTalk中更丰富的上下文类型导致了更多样化的对话，但也增加了从复杂的多模态信息中捕捉人类兴趣并生成个性化回答的难度。此外，我们的数据集中更频繁地引用了外部知识。这些事实揭示了多模态对话模型面临的新挑战。我们定量地展示了TikTalk的特点，提出了一个基于视频的多模态闲聊任务，并评估了几种对话基线模型。

    To facilitate the research on intelligent and human-like chatbots with multi-modal context, we introduce a new video-based multi-modal dialogue dataset, called TikTalk. We collect 38K videos from a popular video-sharing platform, along with 367K conversations posted by users beneath them. Users engage in spontaneous conversations based on their multi-modal experiences from watching videos, which helps recreate real-world chitchat context. Compared to previous multi-modal dialogue datasets, the richer context types in TikTalk lead to more diverse conversations, but also increase the difficulty in capturing human interests from intricate multi-modal information to generate personalized responses. Moreover, external knowledge is more frequently evoked in our dataset. These facts reveal new challenges for multi-modal dialogue models. We quantitatively demonstrate the characteristics of TikTalk, propose a video-based multi-modal chitchat task, and evaluate several dialogue baselines. Experi
    
[^45]: 使用语言模型提示进行推理：一项调查

    Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09597](http://arxiv.org/abs/2212.09597)

    本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。

    

    推理作为复杂问题解决的重要能力，可以为医疗诊断、谈判等各种实际应用提供后端支持。本文对使用语言模型提示进行推理的前沿研究进行了综合调查。我们介绍了研究成果的比较和总结，并提供了系统资源以帮助初学者。我们还讨论了新兴推理能力出现的潜在原因，并突出了未来的研究方向。资源可在 https://github.com/zjunlp/Prompt4ReasoningPapers 上获取（定期更新）。

    Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
    
[^46]: 通过结构化的知识和统一的检索-生成，增强多模态多跳问答

    Enhancing Multi-modal and Multi-hop Question Answering via Structured Knowledge and Unified Retrieval-Generation. (arXiv:2212.08632v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.08632](http://arxiv.org/abs/2212.08632)

    通过结构化的知识和统一的检索-生成方法，增强多模态多跳问答。使用实体中心融合编码器对齐不同模态的来源，使用统一的检索-生成解码器整合中间检索结果进行答案生成，并自适应决定检索步骤的数量。

    

    多模态多跳问答涉及通过推理多个来自不同模态的输入源来回答问题。现有方法通常分别检索证据，然后使用语言模型根据检索到的证据生成答案，因此不能充分连接候选项，并且无法建模检索过程中的相互关系。此外，检索和生成的流水线方法可能导致检索性能较低时生成性能差。为了解决这些问题，我们提出了一种结构化的知识和统一的检索-生成（SKURG）方法。SKURG使用实体中心融合编码器来对齐不同模态的来源，使用共享实体。然后，它使用统一的检索-生成解码器来整合中间检索结果进行答案生成，并自适应决定检索步骤的数量。

    Multi-modal multi-hop question answering involves answering a question by reasoning over multiple input sources from different modalities. Existing methods often retrieve evidences separately and then use a language model to generate an answer based on the retrieved evidences, and thus do not adequately connect candidates and are unable to model the interdependent relations during retrieval. Moreover, the pipelined approaches of retrieval and generation might result in poor generation performance when retrieval performance is low. To address these issues, we propose a Structured Knowledge and Unified Retrieval-Generation (SKURG) approach. SKURG employs an Entity-centered Fusion Encoder to align sources from different modalities using shared entities. It then uses a unified Retrieval-Generation Decoder to integrate intermediate retrieval results for answer generation and also adaptively determine the number of retrieval steps. Extensive experiments on two representative multi-modal mult
    
[^47]: SceneGATE：基于场景图的文本视觉问答中的共同关注网络

    SceneGATE: Scene-Graph based co-Attention networks for TExt visual question answering. (arXiv:2212.08283v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.08283](http://arxiv.org/abs/2212.08283)

    本文提出了一种基于场景图的共同关注网络（SceneGATE）用于TextVQA，通过发现场景图的潜在语义，捕捉了图像中物体、OCR标记和问题词之间的语义关系，并在Text-VQA和ST-VQA两个基准数据集上表现显著优于现有方法。

    

    大多数TextVQA方法都注重通过简单的transformer编码器来整合物体、场景文本和问题词，但这无法捕捉不同模态之间的语义关系。本文提出了一种基于场景图的共同关注网络（SceneGATE）用于TextVQA，该网络揭示了图像中物体、光学字符识别（OCR）标记和问题词之间的语义关系。通过一个基于TextVQA的场景图来发现图像的潜在语义，我们创造了一个引导关注模块来捕获语言和视觉之间的内部交互作为跨模态交互的向导。为了明确教授两种模态之间的关系，我们提出并集成了两个注意模块，即基于场景图的语义关系感知注意和位置关系感知注意。我们在两个基准数据集Text-VQA和ST-VQA上进行了广泛的实验。结果表明，我们的SceneGATE在两个数据集上都比现有的方法表现显著。

    Most TextVQA approaches focus on the integration of objects, scene texts and question words by a simple transformer encoder. But this fails to capture the semantic relations between different modalities. The paper proposes a Scene Graph based co-Attention Network (SceneGATE) for TextVQA, which reveals the semantic relations among the objects, Optical Character Recognition (OCR) tokens and the question words. It is achieved by a TextVQA-based scene graph that discovers the underlying semantics of an image. We created a guided-attention module to capture the intra-modal interplay between the language and the vision as a guidance for inter-modal interactions. To make explicit teaching of the relations between the two modalities, we proposed and integrated two attention modules, namely a scene graph-based semantic relation-aware attention and a positional relation-aware attention. We conducted extensive experiments on two benchmark datasets, Text-VQA and ST-VQA. It is shown that our SceneG
    
[^48]: 自然语言处理在编程中的应用综述

    A Survey on Natural Language Processing for Programming. (arXiv:2212.05773v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.05773](http://arxiv.org/abs/2212.05773)

    本文通过系统综述，从结构化和功能导向的属性的角度，涵盖了自然语言处理在编程中的应用领域的任务、数据集、评估方法、技术和模型，揭示了它们在程序理解和生成方面的作用，并提出了未来工作的潜在方向。

    

    自然语言处理在编程中的应用旨在利用NLP技术来辅助编程。它因提高生产力的有效性而日益普遍。编程语言与自然语言不同，它具有高度结构化和功能性。构建基于结构的表示和功能导向的算法是程序理解和生成的核心。在本文中，我们从结构化和功能导向的属性的角度进行系统综述，涵盖任务、数据集、评估方法、技术和模型，旨在了解每个组件中这两个属性的作用。基于分析，我们阐述了未被开发的领域，并提出了未来工作的潜在方向。

    Natural language processing for programming aims to use NLP techniques to assist programming. It is increasingly prevalent for its effectiveness in improving productivity. Distinct from natural language, a programming language is highly structured and functional. Constructing a structure-based representation and a functionality-oriented algorithm is at the heart of program understanding and generation. In this paper, we conduct a systematic review covering tasks, datasets, evaluation methods, techniques, and models from the perspective of the structure-based and functionality-oriented property, aiming to understand the role of the two properties in each component. Based on the analysis, we illustrate unexplored areas and suggest potential directions for future work.
    
[^49]: QAmeleon: 仅需5个示例的多语言问答

    QAmeleon: Multilingual QA with Only 5 Examples. (arXiv:2211.08264v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.08264](http://arxiv.org/abs/2211.08264)

    QAmeleon是一种通过仅使用5个示例进行多语言问答的方法，通过预训练语言模型自动生成数据并进行训练，避免了昂贵的标注过程。该方法在准确性和性能方面优于传统的基于翻译的方法，并能弥合英语和完全监督方法之间的差距。

    

    大规模高质量的数据集的可用性是近期问答（QA）技术进展的主要驱动因素之一。然而，这样的标注数据集难以收集且成本高昂，且很少存在于非英语语言中，使得QA技术对于少数语言不可访问。与构建大型单语言训练数据集相比，一种替代方案是在少样本学习环境下利用预训练语言模型（PLM）。我们的方法QAmeleon使用PLM自动生成跨多语言的数据，然后用于训练QA模型，从而避免了昂贵的标注过程。通过仅在每种语言中使用5个示例对PLM进行提示调整，可以获得优于基于翻译的基线方法的准确性，弥合了仅英语的基线模型与近50,000个手工标记示例训练的完全监督上界之间的差距约60％，并且始终相比于直接在标记示例上微调QA模型，都能带来实质性的改进。

    The availability of large, high-quality datasets has been one of the main drivers of recent progress in question answering (QA). Such annotated datasets however are difficult and costly to collect, and rarely exist in languages other than English, rendering QA technology inaccessible to underrepresented languages. An alternative to building large monolingual training datasets is to leverage pre-trained language models (PLMs) under a few-shot learning setting. Our approach, QAmeleon, uses a PLM to automatically generate multilingual data upon which QA models are trained, thus avoiding costly annotation. Prompt tuning the PLM for data synthesis with only five examples per language delivers accuracy superior to translation-based baselines, bridges nearly 60% of the gap between an English-only baseline and a fully supervised upper bound trained on almost 50,000 hand labeled examples, and always leads to substantial improvements compared to fine-tuning a QA model directly on labeled example
    
[^50]: 抽象对话摘要的分类：场景、方法和未来方向

    Taxonomy of Abstractive Dialogue Summarization: Scenarios, Approaches and Future Directions. (arXiv:2210.09894v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.09894](http://arxiv.org/abs/2210.09894)

    本调查对抽象对话摘要进行了全面调查，从场景、方法和评估的角度分类讨论了该领域的现有工作。

    

    抽象对话摘要是生成包含对话中要点信息的简明流畅摘要。近年来，随着社交通讯平台的大规模出现和对话信息理解与消化的紧迫需求，抽象对话摘要引起了广泛关注。与传统文档摘要中的新闻或文章不同，对话具有独特的特征和额外的挑战，包括不同的语言风格和格式、分散的信息、灵活的话语结构和不明确的主题边界。本调查从场景、方法和评估的角度对抽象对话摘要的现有工作进行了全面调查。根据输入对话的类型，将该任务分为开放域和面向任务的两个广泛类别，并提供了三个方向上现有技术的分类，即注入式对话生成、生成式对话生成和知识驱动型对话生成。

    Abstractive dialogue summarization is to generate a concise and fluent summary covering the salient information in a dialogue among two or more interlocutors. It has attracted great attention in recent years based on the massive emergence of social communication platforms and an urgent requirement for efficient dialogue information understanding and digestion. Different from news or articles in traditional document summarization, dialogues bring unique characteristics and additional challenges, including different language styles and formats, scattered information, flexible discourse structures and unclear topic boundaries. This survey provides a comprehensive investigation on existing work for abstractive dialogue summarization from scenarios, approaches to evaluations. It categorizes the task into two broad categories according to the type of input dialogues, i.e., open-domain and task-oriented, and presents a taxonomy of existing techniques in three directions, namely, injecting dia
    
[^51]: Claim-Dissector: 一款联合重排和真实性预测的可解释的事实核查系统

    Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction. (arXiv:2207.14116v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.14116](http://arxiv.org/abs/2207.14116)

    Claim-Dissector是一款联合重排和真实性预测的可解释的事实核查系统，可以识别与声明相关的证据，并确定声明的真实性。该系统的个人贡献以及证据所支持或反驳声明的贡献都可以被识别。

    

    我们提出了Claim-Dissector，一种针对事实核查和分析的新型潜变量模型，给出一个声明和一组检索到的证据，联合学习识别：（i）与给定声明相关的证据，（ii）声明的真实性。我们建议以可解释的方式解开每个证据的相关性概率及其对最终真实性概率的影响-最终真实性概率与每个证据相关性概率的线性整合成比例。通过这种方式，可以识别出每个证据对最终预测概率的个人贡献。在每个证据的相关性概率中，我们的模型还可以进一步区分每个相关证据是支持（S）还是反驳（R）声明。这样可以量化S/R概率对最终结论的贡献或检测有异议的证据。尽管我们的系统具有可解释性，但在FEVER竞赛中，其结果与最先进的结果相当。

    We present Claim-Dissector: a novel latent variable model for fact-checking and analysis, which given a claim and a set of retrieved evidences jointly learns to identify: (i) the relevant evidences to the given claim, (ii) the veracity of the claim. We propose to disentangle the per-evidence relevance probability and its contribution to the final veracity probability in an interpretable way -- the final veracity probability is proportional to a linear ensemble of per-evidence relevance probabilities. In this way, the individual contributions of evidences towards the final predicted probability can be identified. In per-evidence relevance probability, our model can further distinguish whether each relevant evidence is supporting (S) or refuting (R) the claim. This allows to quantify how much the S/R probability contributes to the final verdict or to detect disagreeing evidence.  Despite its interpretable nature, our system achieves results competitive with state-of-the-art on the FEVER 
    
[^52]: $C^3$: 用于视频对话的组合对抗对比学习

    $C^3$: Compositional Counterfactual Contrastive Learning for Video-grounded Dialogues. (arXiv:2106.08914v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.08914](http://arxiv.org/abs/2106.08914)

    本研究提出了一种名为$C^3$的新方法，通过对视频对话中的事实和反事实样本进行对比训练，以实现对于视频和对话上下文相关的回应生成。该方法利用了对象级或动作级的对比损失函数，旨在提高多模态推理能力和泛化能力。

    

    视频对话系统旨在将视频理解和对话理解相结合，以生成与对话和视频上下文相关的回应。大多数现有方法采用深度学习模型，在相对较小的数据集条件下取得了显著的性能。然而，这些结果部分是通过利用数据集中的偏见而非发展多模态推理实现的，从而导致了有限的泛化能力。在本文中，我们提出了一种新颖的组合对抗对比学习（$C^3$）方法，以开发视频对话中关于事实和反事实样本的对比训练。具体而言，我们设计了基于视频中的时间步长和对话中的标记的事实/反事实采样，并提出了利用对象级或动作级变化的对比损失函数。与之前的方法不同，我们集中于对比隐藏状态表示。

    Video-grounded dialogue systems aim to integrate video understanding and dialogue understanding to generate responses that are relevant to both the dialogue and video context. Most existing approaches employ deep learning models and have achieved remarkable performance, given the relatively small datasets available. However, the results are partly accomplished by exploiting biases in the datasets rather than developing multimodal reasoning, resulting in limited generalization. In this paper, we propose a novel approach of Compositional Counterfactual Contrastive Learning ($C^3$) to develop contrastive training between factual and counterfactual samples in video-grounded dialogues. Specifically, we design factual/counterfactual sampling based on the temporal steps in videos and tokens in dialogues and propose contrastive loss functions that exploit object-level or action-level variance. Different from prior approaches, we focus on contrastive hidden state representations among compositi
    
[^53]: 设计和实现英语到约鲁巴语动词短语机器翻译系统

    Design and Implementation of English To Yor\`ub\'a Verb Phrase Machine Translation System. (arXiv:2104.04125v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2104.04125](http://arxiv.org/abs/2104.04125)

    设计并实现了一种英语到约鲁巴语动词短语机器翻译系统，在家庭领域收集了源语言和目标语言的动词短语组的词汇，并使用上下文无关文法验证了重写规则。实验结果表明，系统的输出与谷歌翻译的响应匹配率超过70%。

    

    本文旨在开发一种英语到约鲁巴语的机器翻译系统，可以将英语动词短语文本翻译成约鲁巴语。在家庭领域收集了源语言和目标语言的动词短语组的词汇，并通过在词典中分配匹配词的值来完成词汇翻译。使用上下文无关文法实现了两种语言的句法，我们使用有限状态自动机验证了重写规则。采用人工评估方法并对专家流畅度进行评分。评估结果显示，该系统的输出与样本谷歌翻译的响应匹配率超过70%。

    We aim to develop an English-to-Yoruba machine translation system which can translate English verb phrase text to its Yoruba equivalent.Words from both languages Source Language and Target Language were collected for the verb phrase group in the home domain. The lexical translation is done by assigning values of the matching word in the dictionary. The syntax of the two languages was realized using Context-Free Grammar, we validated the rewrite rules with finite state automata. The human evaluation method was used and expert fluency was scored. The evaluation shows the system performed better than that of sampled Google translation with over 70 percent of the response matching that of the system's output.
    
[^54]: 自然语言处理中的标记修改对抗攻击：一项调研

    Token-Modification Adversarial Attacks for Natural Language Processing: A Survey. (arXiv:2103.00676v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2103.00676](http://arxiv.org/abs/2103.00676)

    这项调研对现有的自然语言处理中的标记修改对抗攻击进行了分类和比较，并旨在指导新的研究并推动进一步的攻击组件研究。

    

    现在有很多针对自然语言处理系统的对抗攻击。其中，绝大多数攻击通过修改单个文档标记来实现成功，我们将其称为标记修改攻击。每种标记修改攻击都由一组特定的基本组件定义，例如对攻击者的约束或特定的搜索算法。基于这一观察，我们对现有的标记修改攻击进行调查，并提取每种攻击的组件。我们使用一个与攻击无关的框架来组织我们的调研，从而对该领域进行有效的分类，并方便进行组件比较。本调研旨在指导新的研究人员进入这一领域，并推动对于个体攻击组件的进一步研究。

    There are now many adversarial attacks for natural language processing systems. Of these, a vast majority achieve success by modifying individual document tokens, which we call here a token-modification attack. Each token-modification attack is defined by a specific combination of fundamental components, such as a constraint on the adversary or a particular search algorithm. Motivated by this observation, we survey existing token-modification attacks and extract the components of each. We use an attack-independent framework to structure our survey which results in an effective categorisation of the field and an easy comparison of components. This survey aims to guide new researchers to this field and spark further research into individual attack components.
    

