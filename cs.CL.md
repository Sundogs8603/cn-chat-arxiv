# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Dialogue-Contextualized Re-ranking for Medical History-Taking.](http://arxiv.org/abs/2304.01974) | 本文提出了一种采用对话情境下的模型进行重排序的方法，帮助机器学习模型缩小训练和推断之间的差距，以改进人工智能医学史采集。 |
| [^2] | [MEGClass: Text Classification with Extremely Weak Supervision via Mutually-Enhancing Text Granularities.](http://arxiv.org/abs/2304.01969) | MEGClass是一种通过相互增强的文本粒度实现极弱监督文本分类的方法，利用分层关注机制从文档、句子和单词中提取连贯且多样的子文本，能够准确分类讨论多个主题的文档，并且在五个基准数据集上表现优于现有最先进的模型。 |
| [^3] | [AToMiC: An Image/Text Retrieval Test Collection to Support Multimedia Content Creation.](http://arxiv.org/abs/2304.01961) | 本文介绍了一个支持多媒体内容创建的图像/文本检索测试集——AToMiC，它使用了维基百科中的大规模图像-文档关联，并建立了多样的领域文本和图片。AToMiC 为可扩展、多样化、可复现的多媒体检索研究提供了一个测试平台。 |
| [^4] | [Evaluating Large Language Models on a Highly-specialized Topic, Radiation Oncology Physics.](http://arxiv.org/abs/2304.01938) | 本研究评估了四种大型语言模型在回答放射肿瘤物理问题方面的表现，结果发现ChatGPT（GPT-4）的表现最好，同时提出了LLMs模型改进和集成策略的潜力。 |
| [^5] | [LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models.](http://arxiv.org/abs/2304.01933) | 本文提出了LLM-Adapters，一个将适配器集成到LLMs中进行参数高效微调的易于使用的框架，实现了比现有方法更少的参数和训练时间，在多个基准测试上取得了最先进的结果。 |
| [^6] | [Resources and Few-shot Learners for In-context Learning in Slavic Languages.](http://arxiv.org/abs/2304.01922) | 该论文通过收集一些基础设施，用于在斯拉夫语言环境中进行ICL的训练和评估，并使用新的数据集评估了一组最新的境中学习器，并将其性能与以前的工作进行了比较。 |
| [^7] | [Dual-Attention Neural Transducers for Efficient Wake Word Spotting in Speech Recognition.](http://arxiv.org/abs/2304.01905) | 本文介绍了一种新的“双关注神经变换器”，可以通过优化唤醒词检测来选择计算路径，从而提高唤醒词的准确性和推理时间效率，并且计算成本可以降低90％而仅增加1％的参数。这种架构可以在语音识别领域中大有裨益。 |
| [^8] | [REFINER: Reasoning Feedback on Intermediate Representations.](http://arxiv.org/abs/2304.01904) | REFINER 是一个框架，用于微调语言模型以显式生成中间推理步骤，并与提供自动反馈的批判模型交互，其在三个不同推理任务上取得了显着改进。 |
| [^9] | [San-BERT: Extractive Summarization for Sanskrit Documents using BERT and it's variants.](http://arxiv.org/abs/2304.01894) | 本研究使用BERT及其变种为梵语文献开发了语言模型，并利用这些模型提取摘要。同时，我们还公开发布了一个梵语天城文本语料库。 |
| [^10] | [Sociocultural knowledge is needed for selection of shots in hate speech detection tasks.](http://arxiv.org/abs/2304.01890) | HATELEXICON是一个包含巴西，德国，印度和肯尼亚仇恨言论的词汇表，利用其可以提高模型在训练中的性能表现。 |
| [^11] | [Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models.](http://arxiv.org/abs/2304.01852) | 本文全面介绍了最先进的大型语言模型ChatGPT和GPT-4，包括其在各个领域的前景应用，并着重介绍了大规模预训练、指令微调和人类反馈的强化学习创新。ChatGPT/GPT-4在自然语言处理应用方面表现突出，同时在其他领域也具有潜力。 |
| [^12] | [A User-Centered, Interactive, Human-in-the-Loop Topic Modelling System.](http://arxiv.org/abs/2304.01774) | 本文介绍了一种以用户为中心的交互式人机交互主题建模系统，该系统除了支持从整个语料库中学习主题外，还支持目标主题建模，并提供了一种新颖的主题词建议功能。该系统可以很好地迭代地完善模型，且在用户研究中获得了良好的评价。 |
| [^13] | [Black Box Few-Shot Adaptation for Vision-Language models.](http://arxiv.org/abs/2304.01752) | 本文提出了一种黑匣子方法，实现了对预先计算的图像和文本特征的视觉-语言模型的快速少样本适应，适用于有监督和无监督训练，并且可以用于对单模型计算的图像和文本特征进行对齐。 |
| [^14] | [Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation.](http://arxiv.org/abs/2304.01746) | ChatGPT是一个基于GPT-3.5的大规模语言模型，具有出色的错误检测和纠正能力，在各种自然语言处理任务中展现出了很高潜力。它可以自由地纠正错误使得纠正后的句子非常流畅，在非英语和低资源环境下的表现突出了其潜力。 |
| [^15] | [Rumour Detection and Analysis on Twitter.](http://arxiv.org/abs/2304.01712) | 本文研究了社交媒体平台上的谣言检测并分析了新冠疫情相关推文的数据。研究使用最先进的自然语言处理模型通过比较语言结构和传播路径等特征来区分谣言和事实，并发现语言结构是更好的区分特征。 |
| [^16] | [Can BERT eat RuCoLA? Topological Data Analysis to Explain.](http://arxiv.org/abs/2304.01680) | 本文研究了Transformer语言模型Fine-Tune任务中如何捕捉语言特征，并通过Topological Data Analysis方法构建有向图从中提取拓扑特征。实验发现，以弦性和匹配数为新特征的TDA-based分类器优于Fine-Tune基线模型，这些结果有助于理解LMs在可接受性分类任务中的行为。 |
| [^17] | [A Survey on Contextualised Semantic Shift Detection.](http://arxiv.org/abs/2304.01666) | 上下文语境下语义转变检测的计算机方法不断进步，以更好地捕捉单词的多重用法/意义，并提出了一个基于含义表示、时间感知和学习模态的分类框架。该综述探讨了该领域的挑战和机遇。 |
| [^18] | [Neural Comprehension: Language Models with Compiled Neural Networks.](http://arxiv.org/abs/2304.01665) | 本文探讨了如何将编译神经网络CoNNs并入语言模型的架构中，以使语言模型在复合任务中提高性能，特别是在需要深入理解抽象规则的领域。方法称为“神经理解”，提高了语言模型在符号操作、规则推理、算术推理等方面的准确度。 |
| [^19] | [Cross-Domain Image Captioning with Discriminative Finetuning.](http://arxiv.org/abs/2304.01662) | 本文通过使用自监督鉴别式沟通目标，微调已有的神经字幕生成器，在保持语言的视觉描述性的同时，提高了对图像内容的信息提取精度。 |
| [^20] | [Multidimensional Perceptron for Efficient and Explainable Long Text Classification.](http://arxiv.org/abs/2304.01638) | 提出了一种名为SWIPE的多维感知器模型，有效地学习整个文本的标签并以无监督的方式感知段落的标签。SWIPE作为一种通用分类器，支持不同的编码器，在分类上优于现有技术。 |
| [^21] | [An interpretability framework for Similar case matching.](http://arxiv.org/abs/2304.01622) | 本论文提出了一个可解释的相似案例匹配框架，其中包括四个模块：司法特征句子识别模块、案例匹配模块、特征句子对齐模块和冲突消歧模块。该框架通过识别案例中的重要信息和对齐两个案例中的特征句，提供了可靠的相似性证据。 |
| [^22] | [SimCSum: Joint Learning of Simplification and Cross-lingual Summarization for Cross-lingual Science Journalism.](http://arxiv.org/abs/2304.01621) | 本文提出了一个名为SimCSum的模型，联合训练简化和跨语言摘要两个高级NLP任务。该模型表现出与基线模型相近或更优异的性能，并且同时提高了语言复杂性和跨语言抽象摘要。 |
| [^23] | [EDeR: A Dataset for Exploring Dependency Relations Between Events.](http://arxiv.org/abs/2304.01612) | EDeR数据集提供了事件间依赖关系注释。预测此关系可以提高事件抽取及共指消解等下游任务的准确性。 |
| [^24] | [Unsupervised Improvement of Factual Knowledge in Language Models.](http://arxiv.org/abs/2304.01597) | 本文提出了一种无监督的方法来改进预训练的语言模型，从而提高其在各种知识密集型任务中的性能，包括事实回忆，问答，情感分析和自然语言推理等。这种方法可以通过优先考虑信息性单词来影响掩盖语言建模。 |
| [^25] | [Attribute-Consistent Knowledge Graph Representation Learning for Multi-Modal Entity Alignment.](http://arxiv.org/abs/2304.01563) | 本文提出了一个新颖的属性一致知识图谱表示学习框架(ACK-MMEA)，弥补实体在学习过程中特定模态上具有不同数量属性的上下文差距问题，从而提高多模态实体对齐(MMEA)的准确性。 |
| [^26] | [A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection.](http://arxiv.org/abs/2304.01492) | 该文介绍了一个利用对比传递框架和传播结构，将从充足资源的谣言数据学到的特征适应于低资源情况下的方式，可以检测到跨越语言和领域界限的谣言。 |
| [^27] | [To ChatGPT, or not to ChatGPT: That is the question!.](http://arxiv.org/abs/2304.01487) | 研究评估了聊天GPT检测中的最新技术和其他AI生成文本检测工具的表现，并提出区分人工生成和AI生成文本的重要性。 |
| [^28] | [Blockwise Compression of Transformer-based Models without Retraining.](http://arxiv.org/abs/2304.01483) | 本论文提出了一种名为BCT的分块压缩框架，可以对整个Transformer模型进行更细粒度的压缩，实现了降低部署门槛的目的。 |
| [^29] | [The Vector Grounding Problem.](http://arxiv.org/abs/2304.01481) | 本文讨论了大型语言模型中的向量接地问题，通过区分内部表示接地的不同方式，总结出五个概念。 |
| [^30] | [Polarity based Sarcasm Detection using Semigraph.](http://arxiv.org/abs/2304.01424) | 本文提出了一种基于Semigraph的方式进行情感和讽刺检测，通过文档的情感极性得分来识别文章中的讽刺，该方法在亚马逊产品评价中表现优异。 |
| [^31] | [Thematic context vector association based on event uncertainty for Twitter.](http://arxiv.org/abs/2304.01423) | 本文提出一种基于数据关联的方法，利用不确定性原理来提取Twitter数据中伴随相关事件的关键词，从而解决了当前系统中非正式语言下关键词提取的挑战。实验表明这种方法对Twitter COVID-19数据有效。 |
| [^32] | [The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents.](http://arxiv.org/abs/2304.01412) | 该论文介绍了StatCan对话数据集，这是一个涉及真实意图的对话转换，提出了两个任务：基于正在进行的对话自动检索相关数据表和在每个回合自动生成适当的代理响应。该研究对现有模型持续提出挑战，鼓励更多基于对话的数据检索研究。 |
| [^33] | [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling.](http://arxiv.org/abs/2304.01373) | 本文介绍了一套名为 Pythia 的工具套件，包含 16 个大型语言模型，其大小从 70M 到 12B 参数不等。Pythia 可以帮助研究人员在多个领域开展研究，作者还提出了几个新的研究结果，在记忆、应用少量数据时的效果以及减少性别偏见等方面具有重要意义。 |
| [^34] | [A Simple and Effective Method of Cross-Lingual Plagiarism Detection.](http://arxiv.org/abs/2304.01352) | 该论文提出了一种简单有效的跨语言抄袭检测方法，不依赖机器翻译和词义消歧，使用开放的多语言同义词库进行候选检索任务和预训练的基于多语言BERT的语言模型进行详细分析，在多个基准测试中取得了最先进的结果。 |
| [^35] | [End-to-End Models for Chemical-Protein Interaction Extraction: Better Tokenization and Span-Based Pipeline Strategies.](http://arxiv.org/abs/2304.01344) | 本论文提出了一种基于跨度的管道策略和更好的标记化方法来提高化学物质-蛋白质相互作用提取的性能，并在多个指标上优于现有最先进模型。 |
| [^36] | [Creating Custom Event Data Without Dictionaries: A Bag-of-Tricks.](http://arxiv.org/abs/2304.01331) | 本文介绍了一些 NLP 技巧，可用于高效生产自定义事件数据集，其中包括使用主动学习训练分类器、利用大型语言模型和标准机器学习分类器识别行动者和接收者等。 |
| [^37] | [A Comparison of Document Similarity Algorithms.](http://arxiv.org/abs/2304.01330) | 本研究比较了文档相似性算法，分为三类进行探讨：统计算法、神经网络算法和基于语料库/知识的算法，通过比较最有效的算法来确定哪些算法最有用。 |
| [^38] | [Grand Challenge On Detecting Cheapfakes.](http://arxiv.org/abs/2304.01328) | 这个研究关注侦测 Cheapfake 媒体的 OOC 误用，尤其是对真实照片的误用和不符合的图像说明。研究旨在开发并测试模型，以检测给定样本是否为 OOC ，基于 COSMOS 数据集。 |
| [^39] | [PALI: A Language Identification Benchmark for Perso-Arabic Scripts.](http://arxiv.org/abs/2304.01322) | 本文探讨了使用波斯-阿拉伯文字进行语言识别面临的挑战，提出了解决方案，包括使用一组有监督技术将句子分类为它们的语言以及提出一个针对分类器混淆语言簇的分层模型。实验表明这些方法是有效的。 |
| [^40] | [Approaches to Corpus Creation for Low-Resource Language Technology: the Case of Southern Kurdish and Laki.](http://arxiv.org/abs/2304.01319) | 本文介绍了通过当地新闻网站、电台和田野调查等方法解决南部库尔德语和拉基语缺乏数据的问题，并探讨了在库尔德语和扎扎格腊尼语的其他变体环境下进行语言识别的任务。 |
| [^41] | [Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning.](http://arxiv.org/abs/2304.01295) | 本文提出了一个平行大规模多语种会话数据集XSGD，开发了一种有效的基于提示调整的方法来学习对齐提示，同时研究了跨语言任务的NLI-based和vanilla分类器，并在插槽填充和意图分类任务上评估了模型的跨语言泛化能力。 |
| [^42] | [Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT.](http://arxiv.org/abs/2304.01246) | 本文研究了大型语言模型在系统论过程分析（STPA）中的应用，并采用ChatGPT对自动紧急制动（AEB）系统进行了案例研究。结果表明，重复双工交互方法是最有效的，并显着提高了STPA的质量。本研究证明，LLMs可以应用于安全分析，并为安全关键系统提供有价值的见解。 |
| [^43] | [Enhancing Clinical Evidence Recommendation with Multi-Channel Heterogeneous Learning on Evidence Graphs.](http://arxiv.org/abs/2304.01242) | 该论文提出使用证据共指图和证据文本图表来解决临床证据推荐中的联系稀疏性问题，并介绍了一个多通道异构学习模型来支持临床决策过程。 |
| [^44] | [Detection of Homophobia & Transphobia in Dravidian Languages: Exploring Deep Learning Methods.](http://arxiv.org/abs/2304.01241) | 本研究旨在探讨不同深度学习模型在德拉维达语社交媒体评论分类中的适用性，以便将其识别为恐同、跨性别歧视和非反LGBT+内容。 |
| [^45] | [Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach.](http://arxiv.org/abs/2304.01240) | 该研究使用机器学习技术，成功地识别出心理健康电子健康记录中的疼痛提及，提高了对疼痛和心理健康之间关系的理解。 |
| [^46] | [Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection.](http://arxiv.org/abs/2304.01238) | 本文通过比较不同类型的大型语言模型和传统机器学习技术在邮件垃圾检测中的表现，发现大多数情况下，大型语言模型优于传统技术，特别是在样本有限的情况下。同时，本文还介绍了经过改进和微调的Spam-T5模型，该模型具有出色的性能表现。 |
| [^47] | [Multi-Modal Perceiver Language Model for Outcome Prediction in Emergency Department.](http://arxiv.org/abs/2304.01233) | 本文提出了一种基于多模态知觉语言模型的方法，用于结果预测和患者分诊。实验结果表明，该模型在急诊科临床决策方面具有显著的潜力。 |
| [^48] | [Better Language Models of Code through Self-Improvement.](http://arxiv.org/abs/2304.01228) | 本文提出了一个简单的数据增强框架来改善预训练语言模型为代码生成和代码摘要等任务微调的瓶颈问题，提高了模型性能。 |
| [^49] | [PromptORE -- A Novel Approach Towards Fully Unsupervised Relation Extraction.](http://arxiv.org/abs/2304.01209) | 提出了“基于提示的开放关系抽取”模型，在无监督设置下不需要超参数调整，实现了全新的无监督关系抽取方法。 |
| [^50] | [Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data.](http://arxiv.org/abs/2304.01196) | 提出了一种流程，通过利用ChatGPT与自身对话，能够自动生成高质量的多轮聊天语料库，并采用参数高效调整来增强开源的大型语言模型LLaMA，得到的模型被命名为Baize，在最小化潜在风险的护栏下，在多轮对话中展现出良好的性能。 |
| [^51] | [Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models.](http://arxiv.org/abs/2304.01046) | 本文研究了一种训练阅读理解和逻辑推理模型的反向方法，利用相对准确性的策略来训练模型，通过Polytuplet Loss函数来确保优先学习答案选择的相对正确性，获得了不错的成果，提出了具有一般性的训练方法和模型架构。 |
| [^52] | [Can AI Put Gamma-Ray Astrophysicists Out of a Job?.](http://arxiv.org/abs/2303.17853) | 本文评估了使用最先进的转换器模型创作一篇假的科学论文的能力，旨在验证这些模型是否能够仅基于语言信息解释天文观测和源，并为同行评审识别欺诈性生成的科学论文提供潜在手段。结论是，目前天文学家的工作是安全的。 |
| [^53] | [What Types of Questions Require Conversation to Answer? A Case Study of AskReddit Questions.](http://arxiv.org/abs/2303.17710) | 本文研究了哪些模糊开放性问题最适合通过对话回答，发现这些问题高度社交和个人化，对未来研究提供了有益的参考。 |
| [^54] | [oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes.](http://arxiv.org/abs/2303.17612) | oBERTa是一组易于使用的语言模型，通过改进初始化、蒸馏、剪枝等技术，可以在不需要模型压缩方面的专业知识的情况下提高稀疏迁移学习的效率和准确性。 |
| [^55] | [Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System.](http://arxiv.org/abs/2303.14524) | 本文介绍了一种创新的推荐系统模式-Chat-Rec，通过将LLMs与对话式推荐相结合，解决了传统推荐系统中互动性和可解释性不足的问题。因此，Chat-Rec能够更有效地学习用户偏好，并在推荐过程中建立用户-产品之间的联系，具有更大的透明度和控制。 |
| [^56] | [AfroDigits: A Community-Driven Spoken Digit Dataset for African Languages.](http://arxiv.org/abs/2303.12582) | AfroDigits是第一个发布的面向非洲语言的音频数字数据集，为非洲语言中的语音应用程序开辟了道路。 |
| [^57] | [Self-tuning hyper-parameters for unsupervised cross-lingual tokenization.](http://arxiv.org/abs/2303.02427) | 本研究探讨了在多语言中进行语言无关的无监督分词的元学习可能性，并使用多种适应度函数自动确定无监督分词模型的超参数。结果表明在英语和俄语中，前三种度量的加性组合与 F1 分词得分之间有相当好的相关性，在中文中，F1 得分与压缩因子有显著的相关性。 |
| [^58] | [A Categorical Archive of ChatGPT Failures.](http://arxiv.org/abs/2302.03494) | 本研究对ChatGPT的11个失败类别进行了全面分析，其中包括推理、事实错误、数学、编码和偏见。找出失败原因以帮助研究人员和开发人员改进未来的语言模型和聊天机器人。 |
| [^59] | [Hierarchical multimodal transformers for Multi-Page DocVQA.](http://arxiv.org/abs/2212.05935) | 本文提出了一种新方法，Hi-VT5，它是一种分层 transformer 结构，能够处理多页文档 DocVQA 任务。实验表明该方法能够有效地回答问题。 |
| [^60] | [Learning to Dub Movies via Hierarchical Prosody Models.](http://arxiv.org/abs/2212.04054) | 本研究提出了一种新的电影配音架构，通过分层韵律建模从嘴唇、面部和场景三个方面将视觉信息与相应的语音韵律联系起来，从而解决了V2C任务中情感变化和说话速度匹配等问题。 |
| [^61] | [Orders Are Unwanted: Dynamic Deep Graph Convolutional Network for Personality Detection.](http://arxiv.org/abs/2212.01515) | 本论文提出了一种动态深度图卷积网络，采用学习连接的方式而非确定性连接，自动学习帖子之间的连接来预测人格特征，相比于先前的方法具有更好的性能表现。 |
| [^62] | [Personalized Dialogue Generation with Persona-Adaptive Attention.](http://arxiv.org/abs/2210.15088) | 本文提出了一种新的框架，使用个性自适应注意力（PAA）来生成基于个性的一致性回应，可以通过整合个性和上下文信息的权重来实现。实验证明 PAA 框架具有优越性能，可以在低资源环境下表现出色。 |
| [^63] | [Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction.](http://arxiv.org/abs/2209.05135) | 本文提出了一种从视频示例学习手语拼写在机器人中的实现的方法。通过训练模仿运动的策略，利用预训练的深度视觉模型从RGB视频中提取手的三维姿态，并识别出最佳的模仿超参数集，本文成功展示了方法的普适性。 |
| [^64] | [Multilingual Bidirectional Unsupervised Translation Through Multilingual Finetuning and Back-Translation.](http://arxiv.org/abs/2209.02821) | 该论文提出了一种两阶段的方法，实现单个NMT模型对未见过的语言同英语之间的双向无监督翻译。该方法包括多语言微调和双向回译，成功提高了翻译质量。 |
| [^65] | [MENLI: Robust Evaluation Metrics from Natural Language Inference.](http://arxiv.org/abs/2208.07316) | 本文提出了一种基于自然语言推理的鲁棒性评估指标，比现有的摘要评估指标更好，在标准基准测试中表现良好且更加鲁棒，与现有指标相结合可以使评估效果进一步提高。 |
| [^66] | [Investigating Lexical Replacements for Arabic-English Code-Switched Data Augmentation.](http://arxiv.org/abs/2205.12649) | 本文研究了用于阿拉伯-英语混合代码数据增强的词汇替换方法，通过单词对齐的平行语料库进行词汇替换，评估了其在机器翻译（MT）、自动语音识别（ASR）和语音翻译（ST）任务的有效性，并取得了34%的改进。 |
| [^67] | [Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding.](http://arxiv.org/abs/2203.05711) | 这个研究收集并发布了一个视频-语言故事数据集SYMON，用于推进多模态故事理解的发展。 |
| [^68] | [Negativity Spreads Faster: A Large-Scale Multilingual Twitter Analysis on the Role of Sentiment in Political Communication.](http://arxiv.org/abs/2202.00396) | 本文利用大规模多语言 Twitter 数据，分析了希腊、西班牙和联合王国议会成员的推文，并发现消极情绪更易传播。 |
| [^69] | [Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors.](http://arxiv.org/abs/2103.15949) | 本文提出使用字典学习将上下文嵌入作为Transformer因子的线性叠加来打开Transformer“黑匣子”，通过可视化展示其捕获的层次化语义结构，为更好地理解Transformer网络的工作方式带来新的见解。 |

# 详细

[^1]: 医学史采集的对话情境重排序

    Dialogue-Contextualized Re-ranking for Medical History-Taking. (arXiv:2304.01974v1 [cs.CL])

    [http://arxiv.org/abs/2304.01974](http://arxiv.org/abs/2304.01974)

    本文提出了一种采用对话情境下的模型进行重排序的方法，帮助机器学习模型缩小训练和推断之间的差距，以改进人工智能医学史采集。

    

    基于人工智能的医学史采集是症状检查、自动患者接待、分诊和其他人工智能虚拟护理应用的重要组成部分。由于病史采集方式的多样性，机器学习模型需要大量的数据进行训练。为了克服这个挑战，现有的系统使用间接数据或专家知识进行开发。这导致了训练和推断之间的差距，因为模型是在不同类型的数据上进行训练，而不是在推断时观察到的数据上进行训练。在本文中，我们提出了一个两阶段的重排序方法，通过使用对话情境下的模型重新对第一阶段的问题候选者进行排序，帮助缩小训练和推断之间的差距。为此，我们提出了一种新模型——全局重排序器，该模型同时将所有问题与对话进行交叉编码，并将其与几种现有的神经线路进行比较。我们测试了transformer和S4语言模型背景下的性能。我们发现，相对于专家系统，最佳表现是实现的。

    AI-driven medical history-taking is an important component in symptom checking, automated patient intake, triage, and other AI virtual care applications. As history-taking is extremely varied, machine learning models require a significant amount of data to train. To overcome this challenge, existing systems are developed using indirect data or expert knowledge. This leads to a training-inference gap as models are trained on different kinds of data than what they observe at inference time. In this work, we present a two-stage re-ranking approach that helps close the training-inference gap by re-ranking the first-stage question candidates using a dialogue-contextualized model. For this, we propose a new model, global re-ranker, which cross-encodes the dialogue with all questions simultaneously, and compare it with several existing neural baselines. We test both transformer and S4-based language model backbones. We find that relative to the expert system, the best performance is achieved 
    
[^2]: MEGClass: 通过相互增强的文本粒度实现极弱监督文本分类。

    MEGClass: Text Classification with Extremely Weak Supervision via Mutually-Enhancing Text Granularities. (arXiv:2304.01969v1 [cs.CL])

    [http://arxiv.org/abs/2304.01969](http://arxiv.org/abs/2304.01969)

    MEGClass是一种通过相互增强的文本粒度实现极弱监督文本分类的方法，利用分层关注机制从文档、句子和单词中提取连贯且多样的子文本，能够准确分类讨论多个主题的文档，并且在五个基准数据集上表现优于现有最先进的模型。

    

    文本分类通常需要大量的人工标注数据作为监督，这在动态新兴领域中是昂贵的。某些方法通过仅依赖类名表面文本作为极弱监督来解决这个问题。然而，现有方法未能考虑到单一类别文档讨论多个主题的情况。主题多样性和模糊的句子可能会引入噪声到文档的底层表示，从而影响预测类别的精度。此外，当前的方法独立地关注文档、句子或单词的文本粒度，从而限制了我们联合从所有三者中提取粗粒度或细粒度上下文的能力来识别分类的重要子文本。为了解决这个问题，我们提出了MEGClass，一种利用相互增强的文本粒度进行极弱监督文本分类的方法。具体来说，MEGClass通过分层关注机制从文档、句子和单词中提取连贯且多样的子文本，使我们能够识别和整合来自多个粒度的弱信号，以准确分类文档，即使它们讨论多个主题。在五个基准数据集上的实验结果表明，我们的方法在使用极弱监督的情况下，比现有最先进的模型有着更好的表现。

    Text classification typically requires a substantial amount of human-annotated data to serve as supervision, which is costly to obtain in dynamic emerging domains. Certain methods seek to address this problem by solely relying on the surface text of class names to serve as extremely weak supervision. However, existing methods fail to account for single-class documents discussing multiple topics. Both topic diversity and vague sentences may introduce noise into the document's underlying representation and consequently the precision of the predicted class. Furthermore, current work focuses on text granularities (documents, sentences, or words) independently, which limits the degree of coarse- or fine-grained context that we can jointly extract from all three to identify significant subtext for classification. In order to address this problem, we propose MEGClass, an extremely weakly-supervised text classification method to exploit Mutually-Enhancing Text Granularities. Specifically, MEGC
    
[^3]: AToMiC：支持多媒体内容创建的图像/文本检索测试集

    AToMiC: An Image/Text Retrieval Test Collection to Support Multimedia Content Creation. (arXiv:2304.01961v1 [cs.IR])

    [http://arxiv.org/abs/2304.01961](http://arxiv.org/abs/2304.01961)

    本文介绍了一个支持多媒体内容创建的图像/文本检索测试集——AToMiC，它使用了维基百科中的大规模图像-文档关联，并建立了多样的领域文本和图片。AToMiC 为可扩展、多样化、可复现的多媒体检索研究提供了一个测试平台。

    

    本文介绍了AToMiC（多媒体内容创作工具）数据集，旨在推动图像/文本跨模态检索领域的研究。虽然视觉语言预训练模型已经在提高检索效果方面取得了显著进展，但现有研究仍依赖于仅具有简单图像-文本关系和检索任务用户模型不足的图像标题数据集。为了弥补这些过度简化的设置和多媒体内容创建的真实应用之间的差距，我们介绍了一种新的构建检索测试集的方法。我们利用维基百科中嵌入的大规模图像-文档关联，建立了包括分层结构、文本样式和类型在内的多样化领域的文本和图片。我们基于一个现实的用户模型制定了两个任务，并通过基线模型的检索实验验证了我们的数据集。AToMiC为可扩展、多样化、可复现的多媒体检索研究提供了一个测试平台。

    This paper presents the AToMiC (Authoring Tools for Multimedia Content) dataset, designed to advance research in image/text cross-modal retrieval. While vision-language pretrained transformers have led to significant improvements in retrieval effectiveness, existing research has relied on image-caption datasets that feature only simplistic image-text relationships and underspecified user models of retrieval tasks. To address the gap between these oversimplified settings and real-world applications for multimedia content creation, we introduce a new approach for building retrieval test collections. We leverage hierarchical structures and diverse domains of texts, styles, and types of images, as well as large-scale image-document associations embedded in Wikipedia. We formulate two tasks based on a realistic user model and validate our dataset through retrieval experiments using baseline models. AToMiC offers a testbed for scalable, diverse, and reproducible multimedia retrieval research
    
[^4]: 在高度专业化的放射肿瘤物理学领域中评估大型语言模型

    Evaluating Large Language Models on a Highly-specialized Topic, Radiation Oncology Physics. (arXiv:2304.01938v1 [physics.med-ph])

    [http://arxiv.org/abs/2304.01938](http://arxiv.org/abs/2304.01938)

    本研究评估了四种大型语言模型在回答放射肿瘤物理问题方面的表现，结果发现ChatGPT（GPT-4）的表现最好，同时提出了LLMs模型改进和集成策略的潜力。

    

    本文提出对大型语言模型（LLMs）在回答放射肿瘤物理问题方面进行评估，这是第一项针对该领域的研究。我们开发了一个由100个放射肿瘤物理学问题组成的考试，并使用四种LLMs和医学物理学的专家和非专家进行评估。ChatGPT（GPT-4）平均而言表现最好，超过了所有其他LLMs以及医疗物理学家的表现。同时，本文也提出了一些LLMs模型改进和集成策略的潜力。

    We present the first study to investigate Large Language Models (LLMs) in answering radiation oncology physics questions. Because popular exams like AP Physics, LSAT, and GRE have large test-taker populations and ample test preparation resources in circulation, they may not allow for accurately assessing the true potential of LLMs. This paper proposes evaluating LLMs on a highly-specialized topic, radiation oncology physics, which may be more pertinent to scientific and medical communities in addition to being a valuable benchmark of LLMs. We developed an exam consisting of 100 radiation oncology physics questions based on our expertise at Mayo Clinic. Four LLMs, ChatGPT (GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against medical physicists and non-experts. ChatGPT (GPT-4) outperformed all other LLMs as well as medical physicists, on average. The performance of ChatGPT (GPT-4) was further improved when prompted to explain first, then answer. ChatGPT (GPT-3.5 an
    
[^5]: LLM-Adapters：大型语言模型参数高效微调的适配器系列

    LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. (arXiv:2304.01933v1 [cs.CL])

    [http://arxiv.org/abs/2304.01933](http://arxiv.org/abs/2304.01933)

    本文提出了LLM-Adapters，一个将适配器集成到LLMs中进行参数高效微调的易于使用的框架，实现了比现有方法更少的参数和训练时间，在多个基准测试上取得了最先进的结果。

    

    像GPT-3和ChatGPT这样的大型语言模型（LLMs）的成功，导致了许多经济实惠和易于访问的替代品的开发，这些替代品是通过利用任务特定数据（例如ChatDoctor）或指导数据（例如Alpaca）微调开放式LLMs而创建的。在各种微调方法中，基于适配器的参数高效微调（PEFT）无疑是最有吸引力的研究主题之一，因为它只需要微调少量外部参数而不是整个LLMs，同时实现可比甚至更好的性能。为了进一步研究LLMs的PEFT方法，本文提出了LLM-Adapters，这是一个易于使用的框架，将各种适配器集成到LLMs中，并且可以为不同任务执行这些适配器的PEFT方法。该框架包括最先进的开放式LLMs，例如LLaMA，BLOOM，OPT和GPT-J，以及广泛使用的适配器，例如串联适配器，并联适配器和LoRA。该框架旨在高效且灵活，使用户可以使用最少的附加训练数据或计算资源轻松微调LLMs以适应不同的任务。对多个基准测试的实验表明，LLM-Adapters可以比现有方法使用更少的参数和训练时间取得最先进的结果。

    The success of large language models (LLMs), like GPT-3 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by fine-tuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, OPT, and GPT-J, as well as widely used adapters such as Series adapter, Parallel adapter, and LoRA. The framework is designed to
    
[^6]: 资源和少样本学习器在斯拉夫语境下学习的应用研究

    Resources and Few-shot Learners for In-context Learning in Slavic Languages. (arXiv:2304.01922v1 [cs.CL])

    [http://arxiv.org/abs/2304.01922](http://arxiv.org/abs/2304.01922)

    该论文通过收集一些基础设施，用于在斯拉夫语言环境中进行ICL的训练和评估，并使用新的数据集评估了一组最新的境中学习器，并将其性能与以前的工作进行了比较。

    

    尽管最近创建了准确且紧凑的境中学习器，在场景学习 (ICL) 的工作大多集中在英文任务上。然而，与使用英语以外语言的用户互动的能力为将语言技术应用于非英语使用者提供了巨大的潜力。在这项工作中，我们收集了一些基础设施，用于在捷克语、波兰语和俄语等多种斯拉夫语言中进行ICL的训练和评估。我们通过一系列转换和纯目标语言编写的新模板，将各种各样的数据集连接成一个统一的教学格式。利用新策划的数据集，我们评估了一组最新的境中学习器，并将其结果与监督基线进行了比较。最后，我们使用收集的资源训练、评估和发布了一组境中学习模型，并将其性能与以前的工作进行了比较。

    Despite the rapid recent progress in creating accurate and compact in-context learners, most recent work focuses on in-context learning (ICL) for tasks in English. However, the ability to interact with users of languages outside English presents a great potential for broadening the applicability of language technologies to non-English speakers.  In this work, we collect the infrastructure necessary for training and evaluation of ICL in a selection of Slavic languages: Czech, Polish, and Russian. We link a diverse set of datasets and cast these into a unified instructional format through a set of transformations and newly-crafted templates written purely in target languages. Using the newly-curated dataset, we evaluate a set of the most recent in-context learners and compare their results to the supervised baselines. Finally, we train, evaluate and publish a set of in-context learning models that we train on the collected resources and compare their performance to previous work.  We fin
    
[^7]: 双关注神经变换器用于语音识别时的高效唤醒词识别

    Dual-Attention Neural Transducers for Efficient Wake Word Spotting in Speech Recognition. (arXiv:2304.01905v1 [cs.SD])

    [http://arxiv.org/abs/2304.01905](http://arxiv.org/abs/2304.01905)

    本文介绍了一种新的“双关注神经变换器”，可以通过优化唤醒词检测来选择计算路径，从而提高唤醒词的准确性和推理时间效率，并且计算成本可以降低90％而仅增加1％的参数。这种架构可以在语音识别领域中大有裨益。

    

    本文提出了一种称为双关注神经网络的架构，旨在提高唤醒词识别的准确率并改善语音识别任务的推理时间。该架构通过利用唤醒词检测来选择哪个注意力网络执行输入音频帧的运行时计算路径。使用这种方法，作者有效提高了唤醒词识别的准确性，并定义了浮点运算（FLOPs）的运行时计算成本。在使用作者的内部数据集时，作者证明了所提出的双关注网络可以将唤醒词音频帧的计算成本降低$90\%$，而参数数量仅增加$1\%$。与基线相比，该架构提高了唤醒词F1得分$16\%$，并将一般的罕见词错误率提高了$3\%$。

    We present dual-attention neural biasing, an architecture designed to boost Wake Words (WW) recognition and improve inference time latency on speech recognition tasks. This architecture enables a dynamic switch for its runtime compute paths by exploiting WW spotting to select which branch of its attention networks to execute for an input audio frame. With this approach, we effectively improve WW spotting accuracy while saving runtime compute cost as defined by floating point operations (FLOPs). Using an in-house de-identified dataset, we demonstrate that the proposed dual-attention network can reduce the compute cost by $90\%$ for WW audio frames, with only $1\%$ increase in the number of parameters. This architecture improves WW F1 score by $16\%$ relative and improves generic rare word error rate by $3\%$ relative compared to the baselines.
    
[^8]: REFINER: 基于中间表示的推理反馈。

    REFINER: Reasoning Feedback on Intermediate Representations. (arXiv:2304.01904v1 [cs.CL])

    [http://arxiv.org/abs/2304.01904](http://arxiv.org/abs/2304.01904)

    REFINER 是一个框架，用于微调语言模型以显式生成中间推理步骤，并与提供自动反馈的批判模型交互，其在三个不同推理任务上取得了显着改进。

    

    最近语言模型在推理任务上表现出了remarkable的性能，通过显式生成中间推理步骤，例如链式思考提示等。然而，这些中间推理步骤可能并不是根据初始上下文得出的适当推导，从而导致不正确的最终预测。在这里，我们介绍了REFINER，这是一个框架，用于微调语言模型以显式生成中间推理步骤，并与提供自动反馈的批判模型交互。具体而言，批评家提供了结构化反馈，推理语言模型使用它来迭代改进其中间参数。REFINER的三个不同推理任务的实证评估显示出了与基线具有可比规模的语言模型相比的显着改进。此外，当使用GPT3.5作为推理器时，经过训练的批评家显着改善了推理而无需微调推理器。最后，我们的批评模型是在没有昂贵的人类参与数据的情况下进行的，但可以通过对新颖上下文提供低成本反馈进行继续改进。

    Language models (LMs) have recently shown remarkable performance on reasoning tasks by explicitly generating intermediate inferences, e.g., chain-of-thought prompting. However, these intermediate inference steps may be inappropriate deductions from the initial context and lead to incorrect final predictions. Here we introduce REFINER, a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning. Specifically, the critic provides structured feedback that the reasoning LM uses to iteratively improve its intermediate arguments. Empirical evaluations of REFINER on three diverse reasoning tasks show significant improvements over baseline LMs of comparable scale. Furthermore, when using GPT3.5 as the reasoner, the trained critic significantly improves reasoning without finetuning the reasoner. Finally, our critic model is trained without expensive human-in-the-loop data but can be su
    
[^9]: 使用BERT及其变种的梵语文献摘要提取技术

    San-BERT: Extractive Summarization for Sanskrit Documents using BERT and it's variants. (arXiv:2304.01894v1 [cs.CL])

    [http://arxiv.org/abs/2304.01894](http://arxiv.org/abs/2304.01894)

    本研究使用BERT及其变种为梵语文献开发了语言模型，并利用这些模型提取摘要。同时，我们还公开发布了一个梵语天城文本语料库。

    

    本研究开发了针对梵语的语言模型，包括双向编码器表示来自变形金刚（BERT）及其变体：轻量级BERT（ALBERT）和鲁棒优化BERT（RoBERTa），利用天城文梵语文献语料库。然后，我们从这些模型中提取给定文本的特征。我们应用降维和聚类技术对特征进行处理，以生成给定梵语文献的摘要。除了提取文本摘要技术外，我们还公开发布了一个梵语天城文本语料库。

    In this work, we develop language models for the Sanskrit language, namely Bidirectional Encoder Representations from Transformers (BERT) and its variants: A Lite BERT (ALBERT), and Robustly Optimized BERT (RoBERTa) using Devanagari Sanskrit text corpus. Then we extracted the features for the given text from these models. We applied the dimensional reduction and clustering techniques on the features to generate an extractive summary for a given Sanskrit document. Along with the extractive text summarization techniques, we have also created and released a Sanskrit Devanagari text corpus publicly.
    
[^10]: 社会文化知识在仇恨言论检测任务中对选项的选择是必要的

    Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])

    [http://arxiv.org/abs/2304.01890](http://arxiv.org/abs/2304.01890)

    HATELEXICON是一个包含巴西，德国，印度和肯尼亚仇恨言论的词汇表，利用其可以提高模型在训练中的性能表现。

    

    我们引入了HATELEXICON，这是一个包含巴西，德国，印度和肯尼亚的蔑称和仇恨言论目标的词汇表，以帮助模型的训练和可解释性。我们展示了我们的词汇表如何用于解释模型预测，表明发展用于分类极端言论的模型，在进行预测时严重依赖目标词。此外，我们提出了一种通过HATELEXICON来辅助低资源环境下训练选项的方法，选项选择在小样本学习中尤为重要。在我们的工作中，我们使用HASOC数据对德语和印地语进行了几个示范学习，并将Multilingual HateCheck（MHC）作为基准。我们展示了根据我们的词汇表选择样本，相对于随机采样的模型，能够更好地在MHC上表现。因此，当仅有少量的训练样本时，使用我们的词汇表来选择包含更多社会文化信息的样本能够更好地提高在仇恨言论检测任务中的性能。

    We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
    
[^11]: ChatGPT/GPT-4研究综述及对大语言模型未来的展望

    Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models. (arXiv:2304.01852v1 [cs.CL])

    [http://arxiv.org/abs/2304.01852](http://arxiv.org/abs/2304.01852)

    本文全面介绍了最先进的大型语言模型ChatGPT和GPT-4，包括其在各个领域的前景应用，并着重介绍了大规模预训练、指令微调和人类反馈的强化学习创新。ChatGPT/GPT-4在自然语言处理应用方面表现突出，同时在其他领域也具有潜力。

    

    本文全面介绍了来自GPT系列的最先进的大型语言模型（LLM）ChatGPT和GPT-4及其在各个领域的前景应用。实际上，大规模预训练、指令微调和人类反馈的强化学习是提高LLMs的适应性和性能的重要创新。我们在arXiv上深入分析了194篇相关文献，包括趋势分析、词云表现和在各个应用领域的分布分析。研究发现ChatGPT/GPT-4研究显著增长，主要集中在直接的自然语言处理应用上，同时还展示了在从教育和历史到数学、医学和物理等领域具有相当的潜力。本研究旨在提供有关ChatGPT的能力的见解。

    This paper presents a comprehensive survey of ChatGPT and GPT-4, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT/GPT-4 research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabi
    
[^12]: 以用户为中心的交互式人机交互主题建模系统

    A User-Centered, Interactive, Human-in-the-Loop Topic Modelling System. (arXiv:2304.01774v1 [cs.CL])

    [http://arxiv.org/abs/2304.01774](http://arxiv.org/abs/2304.01774)

    本文介绍了一种以用户为中心的交互式人机交互主题建模系统，该系统除了支持从整个语料库中学习主题外，还支持目标主题建模，并提供了一种新颖的主题词建议功能。该系统可以很好地迭代地完善模型，且在用户研究中获得了良好的评价。

    

    人机交互主题建模系统将用户的知识融入到建模过程中，使其能够迭代地完善模型。最近的研究已经展示了用户反馈的价值，但仍存在一些问题，例如难以跟踪变化、比较不同模型以及缺乏基于实际使用的真实世界的评估。我们开发了一种新颖的交互式人机交互主题建模系统，其具有用户友好的界面，使用户可以比较和记录每个步骤，并具有新颖的主题词建议功能，以帮助用户提供真实可靠的反馈。我们的系统不仅支持传统主题模型的功能，即从整个语料库中学习主题，还支持目标主题建模，即针对语料库特定方面进行主题学习。在本文中，我们概述了该系统，并介绍了一系列用户研究的结果，以评估其价值。

    Human-in-the-loop topic modelling incorporates users' knowledge into the modelling process, enabling them to refine the model iteratively. Recent research has demonstrated the value of user feedback, but there are still issues to consider, such as the difficulty in tracking changes, comparing different models and the lack of evaluation based on real-world examples of use. We developed a novel, interactive human-in-the-loop topic modeling system with a user-friendly interface that enables users compare and record every step they take, and a novel topic words suggestion feature to help users provide feedback that is faithful to the ground truth. Our system also supports not only what traditional topic models can do, i.e., learning the topics from the whole corpus, but also targeted topic modelling, i.e., learning topics for specific aspects of the corpus. In this article, we provide an overview of the system and present the results of a series of user studies designed to assess the value
    
[^13]: 视觉-语言模型的黑匣子少样本适应

    Black Box Few-Shot Adaptation for Vision-Language models. (arXiv:2304.01752v1 [cs.CV])

    [http://arxiv.org/abs/2304.01752](http://arxiv.org/abs/2304.01752)

    本文提出了一种黑匣子方法，实现了对预先计算的图像和文本特征的视觉-语言模型的快速少样本适应，适用于有监督和无监督训练，并且可以用于对单模型计算的图像和文本特征进行对齐。

    

    通过对比学习训练的视觉-语言模型在少样本情况下表现出很强的学习能力。软提示学习是少样本领域适用的最受欢迎的方法，旨在通过新领域引发的分布偏移来缩小模态差距。虽然该方法性能高效，但仍需要访问模型权重，并且在具有数十亿个参数的大型模型上可能会导致计算上的不可行性。本文提出了一种黑匣子方法，实现了对预先计算的图像和文本特征的 V-L 少样本适应，不需要访问模型权重，训练速度快数个数量级，适用于有监督和无监督训练，并且还可以用于对单模型计算的图像和文本特征进行对齐。

    Vision-Language (V-L) models trained with contrastive learning to align the visual and language modalities have been shown to be strong few-shot learners. Soft prompt learning is the method of choice for few-shot downstream adaption aiming to bridge the modality gap caused by the distribution shift induced by the new domain. While parameter-efficient, prompt learning still requires access to the model weights and can be computationally infeasible for large models with billions of parameters. To address these shortcomings, in this work, we describe a black-box method for V-L few-shot adaptation that (a) operates on pre-computed image and text features and hence works without access to the model's weights, (b) it is orders of magnitude faster at training time, (c) it is amenable to both supervised and unsupervised training, and (d) it can be even used to align image and text features computed from uni-modal models. To achieve this, we propose Linear Feature Alignment (LFA), a simple line
    
[^14]: ChatGPT是一个高度流畅的语法纠错系统吗？一项综合性评估研究

    Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation. (arXiv:2304.01746v1 [cs.CL])

    [http://arxiv.org/abs/2304.01746](http://arxiv.org/abs/2304.01746)

    ChatGPT是一个基于GPT-3.5的大规模语言模型，具有出色的错误检测和纠正能力，在各种自然语言处理任务中展现出了很高潜力。它可以自由地纠正错误使得纠正后的句子非常流畅，在非英语和低资源环境下的表现突出了其潜力。

    

    基于GPT-3.5先进架构的大规模语言模型ChatGPT在各种自然语言处理任务中展示了显著的潜力。然而，目前还缺乏全面的研究探索其在语法纠错领域的潜力。为了展示其在语法纠错方面的能力，我们设计了零样本推理（zero-shot chain-of-thought, CoT）和少样本推理（few-shot CoT）设置，利用上下文学习来训练ChatGPT。我们的评估涉及在三种不同语言中对ChatGPT在五个官方测试集和三个英文文档级GEC测试集上的表现进行评估。我们的实验结果和人工评估表明，ChatGPT具有出色的错误检测能力，并且可以自由地纠正错误使得纠正后的句子非常流畅，可能是由于其过度纠正倾向和不遵守最小改动原则。此外，在非英语和低资源环境下的表现突出了其潜力。

    ChatGPT, a large-scale language model based on the advanced GPT-3.5 architecture, has shown remarkable potential in various Natural Language Processing (NLP) tasks. However, there is currently a dearth of comprehensive study exploring its potential in the area of Grammatical Error Correction (GEC). To showcase its capabilities in GEC, we design zero-shot chain-of-thought (CoT) and few-shot CoT settings using in-context learning for ChatGPT. Our evaluation involves assessing ChatGPT's performance on five official test sets in three different languages, along with three document-level GEC test sets in English. Our experimental results and human evaluations demonstrate that ChatGPT has excellent error detection capabilities and can freely correct errors to make the corrected sentences very fluent, possibly due to its over-correction tendencies and not adhering to the principle of minimal edits. Additionally, its performance in non-English and low-resource settings highlights its potential
    
[^15]: 推特谣言检测与分析

    Rumour Detection and Analysis on Twitter. (arXiv:2304.01712v1 [cs.CL])

    [http://arxiv.org/abs/2304.01712](http://arxiv.org/abs/2304.01712)

    本文研究了社交媒体平台上的谣言检测并分析了新冠疫情相关推文的数据。研究使用最先进的自然语言处理模型通过比较语言结构和传播路径等特征来区分谣言和事实，并发现语言结构是更好的区分特征。

    

    近年来，人们越来越依赖社交媒体获取新闻和信息，一些社交媒体用户发布缺乏证实的信息以获取关注，这种信息被称为谣言。本文构建了自然语言处理（NLP）系统来预测谣言。其中，最佳模型被应用于新冠疫情相关推文的探索性数据分析。本研究的贡献有两个方面：（1）通过两个维度：语言结构和传播路径，使用最先进的自然语言处理模型来比较谣言和事实。（2）通过分析谣言与事实在词汇使用和暗示的情感方面的不同，探究谣言与事实之间的不同。该研究表明，相比传播路径，语言结构是更好的特征来区分谣言和事实。

    In recent years people have become increasingly reliant on social media to read news and get information, and some social media users post unsubstantiated information to gain attention. Such information is known as rumours. Nowadays, rumour detection is receiving a growing amount of attention because of the pandemic of the New Coronavirus, which has led to a large number of rumours being spread. In this paper, a Natural Language Processing (NLP) system is built to predict rumours. The best model is applied to the COVID-19 tweets to conduct exploratory data analysis. The contribution of this study is twofold: (1) to compare rumours and facts using state-of-the-art natural language processing models in two dimensions: language structure and propagation route. (2) An analysis of how rumours differ from facts in terms of their lexical use and the emotions they imply. This study shows that linguistic structure is a better feature to distinguish rumours from facts compared to the propagation
    
[^16]: BERT可以吞咽RuCoLA吗？拓扑数据分析来解释。

    Can BERT eat RuCoLA? Topological Data Analysis to Explain. (arXiv:2304.01680v1 [cs.CL])

    [http://arxiv.org/abs/2304.01680](http://arxiv.org/abs/2304.01680)

    本文研究了Transformer语言模型Fine-Tune任务中如何捕捉语言特征，并通过Topological Data Analysis方法构建有向图从中提取拓扑特征。实验发现，以弦性和匹配数为新特征的TDA-based分类器优于Fine-Tune基线模型，这些结果有助于理解LMs在可接受性分类任务中的行为。

    

    本文研究了Transformer语言模型如何擅长Fine-Tune任务捕捉语言特征，并用Topological Data Analysis（TDA）方法从注意力矩阵中构建有向图，从中提取拓扑特征，将其送入线性分类器中。在英语和俄语两个拓扑上不同的数据集CoLA和RuCoLA上进行了实验，提出了多种黑盒内省技术，以便深入了解LM对语言的表示和使用。

    This paper investigates how Transformer language models (LMs) fine-tuned for acceptability classification capture linguistic features. Our approach uses the best practices of topological data analysis (TDA) in NLP: we construct directed attention graphs from attention matrices, derive topological features from them, and feed them to linear classifiers. We introduce two novel features, chordality, and the matching number, and show that TDA-based classifiers outperform fine-tuning baselines. We experiment with two datasets, CoLA and RuCoLA in English and Russian, typologically different languages. On top of that, we propose several black-box introspection techniques aimed at detecting changes in the attention mode of the LMs during fine-tuning, defining the LM's prediction confidences, and associating individual heads with fine-grained grammar phenomena. Our results contribute to understanding the behavior of monolingual LMs in the acceptability classification task, provide insights into
    
[^17]: 上下文语境下语义转变检测综述

    A Survey on Contextualised Semantic Shift Detection. (arXiv:2304.01666v1 [cs.CL])

    [http://arxiv.org/abs/2304.01666](http://arxiv.org/abs/2304.01666)

    上下文语境下语义转变检测的计算机方法不断进步，以更好地捕捉单词的多重用法/意义，并提出了一个基于含义表示、时间感知和学习模态的分类框架。该综述探讨了该领域的挑战和机遇。

    

    语义转变检测 (SSD) 是指识别、解释和评估目标词可能随着时间发生的意义变化的任务。传统上，SSD 是由语言学家和社会科学家通过手动和耗时的活动来处理的。近年来，计算机方法基于自然语言处理和词嵌入技术得到越来越多的关注，以尽可能地自动化 SSD。特别是，在过去的三年中，几乎完全基于词汇的上下文嵌入模型取得了显着的进展，这些模型可以处理单词的多种用法/意义，并更好地捕捉相关的语义转变。本文综述了基于上下文嵌入的 SSD 方法（即 CSSDetection），并提出了一个以含义表示、时间感知和学习模态维度为特征的分类框架。利用该框架，我们对转变评估措施进行了回顾，并确定了上下文化 SSD 领域未来研究面临的挑战和机遇。

    Semantic Shift Detection (SSD) is the task of identifying, interpreting, and assessing the possible change over time in the meanings of a target word. Traditionally, SSD has been addressed by linguists and social scientists through manual and time-consuming activities. In the recent years, computational approaches based on Natural Language Processing and word embeddings gained increasing attention to automate SSD as much as possible. In particular, over the past three years, significant advancements have been made almost exclusively based on word contextualised embedding models, which can handle the multiple usages/meanings of the words and better capture the related semantic shifts. In this paper, we survey the approaches based on contextualised embeddings for SSD (i.e., CSSDetection) and we propose a classification framework characterised by meaning representation, time-awareness, and learning modality dimensions. The framework is exploited i) to review the measures for shift assessm
    
[^18]: 编译神经网络的语言模型：神经理解

    Neural Comprehension: Language Models with Compiled Neural Networks. (arXiv:2304.01665v1 [cs.CL])

    [http://arxiv.org/abs/2304.01665](http://arxiv.org/abs/2304.01665)

    本文探讨了如何将编译神经网络CoNNs并入语言模型的架构中，以使语言模型在复合任务中提高性能，特别是在需要深入理解抽象规则的领域。方法称为“神经理解”，提高了语言模型在符号操作、规则推理、算术推理等方面的准确度。

    

    语言模型在自然语言处理任务中取得了令人瞩目的成果，但其进行符号操作和算术操作的能力仍然有限，这归因于它们隐式地从数据中学习规则。我们探讨如何将特别设计得到的加权的编译神经网络（CoNNs）并入语言模型的架构中，使得通过梯度训练的语言模型获得完全的规则理解能力。编译神经网络的并入为改善语言模型在复合任务中的性能提供了一个有前途的方向，特别是在需要深入理解抽象规则的领域。我们的方法称为“神经理解”，有助于语言模型在符号操作、规则推理、算术推理等方面实现绝对准确度。我们的代码公开可用：\url{https://github.com/...}

    Language models have achieved impressive results in natural language processing tasks, but their ability to perform symbolic operations and arithmetic operations, remains limited, which attribute to their learn the rules implicitly from data. We explore how to incorporate compiled neural networks (CoNNs) which weight is specially designed, into the architecture of language models to enable the language model trained by gradient to obtain fully rule comprehension ability. The incorporation of compiled neural networks offers a promising direction for improving the performance of language models on compound tasks, particularly in areas that require a deeper comprehension of abstract rules beyond recognizing patterns in training data. Our method, which call "Neural Comprehension", helps language models achieve absolute accuracy in symbolic operations, thereby enhancing their ability for rule reasoning, symbolic reasoning, and arithmetic reasoning. Our code is publicly available at: \url{ht
    
[^19]: 通过鉴别微调进行跨领域图像字幕生成

    Cross-Domain Image Captioning with Discriminative Finetuning. (arXiv:2304.01662v1 [cs.CV])

    [http://arxiv.org/abs/2304.01662](http://arxiv.org/abs/2304.01662)

    本文通过使用自监督鉴别式沟通目标，微调已有的神经字幕生成器，在保持语言的视觉描述性的同时，提高了对图像内容的信息提取精度。

    

    神经字幕生成器通常被训练成模仿人类生成的参考文本而没有针对特定沟通目标进行优化，导致产生模糊的字幕等问题。本文表明，使用自监督鉴别式沟通目标微调神经字幕生成器能够帮助恢复平实、视觉描述性更强、关于图像内容更丰富的语言。给定一个目标图像，系统必须学习产生一段描述，从而使一个开箱即用的文本条件图像检索器能够在一组候选图像中识别该图像。我们使用了流行的ClipCap字幕生成器进行实验，并使用BLIP复制了主要结果。就与人类描述的相似度而言，在相同的字幕数据集上训练和测试非微调模型生成的字幕略优于鉴别微调的字幕。然而，当模型未经进一步训练时，鉴别微调的字幕比以前的方法生成的参考文本更能提供关于图像视觉内容更丰富的信息。

    Neural captioners are typically trained to mimic human-generated references without optimizing for any specific communication goal, leading to problems such as the generation of vague captions. In this paper, we show that fine-tuning an out-of-the-box neural captioner with a self-supervised discriminative communication objective helps to recover a plain, visually descriptive language that is more informative about image contents. Given a target image, the system must learn to produce a description that enables an out-of-the-box text-conditioned image retriever to identify such image among a set of candidates. We experiment with the popular ClipCap captioner, also replicating the main results with BLIP. In terms of similarity to ground-truth human descriptions, the captions emerging from discriminative finetuning lag slightly behind those generated by the non-finetuned model, when the latter is trained and tested on the same caption dataset. However, when the model is used without furth
    
[^20]: 高效可解释的长文本分类的多维感知器

    Multidimensional Perceptron for Efficient and Explainable Long Text Classification. (arXiv:2304.01638v1 [cs.CL])

    [http://arxiv.org/abs/2304.01638](http://arxiv.org/abs/2304.01638)

    提出了一种名为SWIPE的多维感知器模型，有效地学习整个文本的标签并以无监督的方式感知段落的标签。SWIPE作为一种通用分类器，支持不同的编码器，在分类上优于现有技术。

    

    由于Transformer和预训练模型的不可避免的成本和复杂性，在长文本分类中引起了效率问题。同时，在高度敏感的领域，例如医疗保健和法律长文本挖掘中，潜在的模型不信任，虽然被低估和未被探索，但可能孕育重要的忧虑。现有方法通常将长文本分割，使用预训练模型对每个片段进行编码，并使用注意力或RNN来获取长文本表示以进行分类。在本文中，我们提出了一个简单而有效的模型，即Segment-aWare multIdimensional PErceptron（SWIPE），以取代上述框架中的注意力/RNN。与之前的工作不同，SWIPE可以通过有监督的训练有效地学习整个文本的标签，同时以无监督的方式感知段落的标签并估计它们对长文本标签的贡献。作为一种通用分类器，SWIPE可以支持不同的编码器，在分类上优于现有技术。

    Because of the inevitable cost and complexity of transformer and pre-trained models, efficiency concerns are raised for long text classification. Meanwhile, in the highly sensitive domains, e.g., healthcare and legal long-text mining, potential model distrust, yet underrated and underexplored, may hatch vital apprehension. Existing methods generally segment the long text, encode each piece with the pre-trained model, and use attention or RNNs to obtain long text representation for classification. In this work, we propose a simple but effective model, Segment-aWare multIdimensional PErceptron (SWIPE), to replace attention/RNNs in the above framework. Unlike prior efforts, SWIPE can effectively learn the label of the entire text with supervised training, while perceive the labels of the segments and estimate their contributions to the long-text labeling in an unsupervised manner. As a general classifier, SWIPE can endorse different encoders, and it outperforms SOTA models in terms of cla
    
[^21]: 一种解释性相似案例匹配框架

    An interpretability framework for Similar case matching. (arXiv:2304.01622v1 [cs.CL])

    [http://arxiv.org/abs/2304.01622](http://arxiv.org/abs/2304.01622)

    本论文提出了一个可解释的相似案例匹配框架，其中包括四个模块：司法特征句子识别模块、案例匹配模块、特征句子对齐模块和冲突消歧模块。该框架通过识别案例中的重要信息和对齐两个案例中的特征句，提供了可靠的相似性证据。

    

    相似案例匹配（SCM）旨在确定两个案件是否相似。该任务在法律系统中具有重要的作用，帮助法律专业人员快速找到相关案例，从而更有效地处理它们。现有研究集中在提高模型的性能上，而不是在其可解释性上。因此，本文提出了一个可解释的SCM管道框架，包括四个模块：司法特征句子识别模块、案例匹配模块、特征句子对齐模块和冲突消歧模块。与现有的SCM方法不同，我们的框架将识别包含基本信息的案例特征句，基于提取的特征句结果进行相似案例匹配，并对两个案例中的特征句进行对齐，以提供证据支持案例的相似性。SCM结果可能与特征句对齐结果产生冲突，因此我们的框架进一步消歧。

    Similar Case Matching (SCM) is designed to determine whether two cases are similar. The task has an essential role in the legal system, helping legal professionals to find relevant cases quickly and thus deal with them more efficiently. Existing research has focused on improving the model's performance but not on its interpretability. Therefore, this paper proposes a pipeline framework for interpretable SCM, which consists of four modules: a judicial feature sentence identification module, a case matching module, a feature sentence alignment module, and a conflict disambiguation module. Unlike existing SCM methods, our framework will identify feature sentences in a case that contain essential information, perform similar case matching based on the extracted feature sentence results, and align the feature sentences in the two cases to provide evidence for the similarity of the cases. SCM results may conflict with feature sentence alignment results, and our framework further disambiguate
    
[^22]: SimCSum：联合学习简化和跨语言摘要以用于跨语言科学新闻报道

    SimCSum: Joint Learning of Simplification and Cross-lingual Summarization for Cross-lingual Science Journalism. (arXiv:2304.01621v1 [cs.CL])

    [http://arxiv.org/abs/2304.01621](http://arxiv.org/abs/2304.01621)

    本文提出了一个名为SimCSum的模型，联合训练简化和跨语言摘要两个高级NLP任务。该模型表现出与基线模型相近或更优异的性能，并且同时提高了语言复杂性和跨语言抽象摘要。

    

    跨语言科学新闻报道为非专业读者生成了不同于来源语言的科学文章的通俗版本。因此，跨语言宣传摘要必须包含原始文档的要点内容，并且内容应该连贯、易懂，并且以受众的本地语言呈现。本文通过联合训练简化和跨语言摘要两个高级NLP任务来改进跨语言摘要生成的这些方面。前者减少语言复杂度，后者专注于跨语言摘要。我们提出了一个新颖的多任务体系结构——SimCSum，其中包含一个共享编码器和两个并行解码器，共同学习简化和跨语言摘要。我们通过比较几种强基线模型的几个评估指标和人工评估来实证研究了SimCSum的性能。总的来说，SimCSum在同时训练两个高级NLP任务的同时，呈现出与基线相似或更优越的性能。

    Cross-lingual science journalism generates popular science stories of scientific articles different from the source language for a non-expert audience. Hence, a cross-lingual popular summary must contain the salient content of the input document, and the content should be coherent, comprehensible, and in a local language for the targeted audience. We improve these aspects of cross-lingual summary generation by joint training of two high-level NLP tasks, simplification and cross-lingual summarization. The former task reduces linguistic complexity, and the latter focuses on cross-lingual abstractive summarization. We propose a novel multi-task architecture - SimCSum consisting of one shared encoder and two parallel decoders jointly learning simplification and cross-lingual summarization. We empirically investigate the performance of SimCSum by comparing it with several strong baselines over several evaluation metrics and by human evaluation. Overall, SimCSum demonstrates statistically si
    
[^23]: EDeR：用于探索事件之间依赖关系的数据集

    EDeR: A Dataset for Exploring Dependency Relations Between Events. (arXiv:2304.01612v1 [cs.CL])

    [http://arxiv.org/abs/2304.01612](http://arxiv.org/abs/2304.01612)

    EDeR数据集提供了事件间依赖关系注释。预测此关系可以提高事件抽取及共指消解等下游任务的准确性。

    

    关系抽取是自然语言处理（NLP）和信息检索（IR）研究的核心任务。我们认为迄今未在NLP或IR研究中探索的一种重要关系类型是事件作为另一个事件的参数 - 必需或可选 - 的关系。我们介绍了人工注释的Event Dependency Relation数据集（EDeR），它提供了这种依赖关系。注释是在OntoNotes数据集的文档样本上完成的，这具有与该数据集的现有正交注释集成合并的优点。我们调查了预测事件依赖关系的基线方法，其中最佳方法的二元参数/非参数分类准确率达到了82.61％。我们展示了识别此关系可以导致更准确的事件抽取（语义角色标注），并可以改进依赖于此的下游任务，例如共指消解。此外，我们证明预测三种参数联合类型可以通过在事件抽取中提供更有效的上下文，从而进一步改进推理和减少错误。

    Relation extraction is a central task in natural language processing (NLP) and information retrieval (IR) research. We argue that an important type of relation not explored in NLP or IR research to date is that of an event being an argument - required or optional - of another event. We introduce the human-annotated Event Dependency Relation dataset (EDeR) which provides this dependency relation. The annotation is done on a sample of documents from the OntoNotes dataset, which has the added benefit that it integrates with existing, orthogonal, annotations of this dataset. We investigate baseline approaches for predicting the event dependency relation, the best of which achieves an accuracy of 82.61 for binary argument/non-argument classification. We show that recognizing this relation leads to more accurate event extraction (semantic role labelling) and can improve downstream tasks that depend on this, such as co-reference resolution. Furthermore, we demonstrate that predicting the thre
    
[^24]: 语言模型中无监督改进事实知识的方法

    Unsupervised Improvement of Factual Knowledge in Language Models. (arXiv:2304.01597v1 [cs.CL])

    [http://arxiv.org/abs/2304.01597](http://arxiv.org/abs/2304.01597)

    本文提出了一种无监督的方法来改进预训练的语言模型，从而提高其在各种知识密集型任务中的性能，包括事实回忆，问答，情感分析和自然语言推理等。这种方法可以通过优先考虑信息性单词来影响掩盖语言建模。

    

    掩盖语言建模（MLM）在预训练大型语言模型中扮演关键角色。但是由于高频词支配了MLM目标，因此不利于学习事实知识。在本文中，我们提出了一种方法，以一种完全无监督的方式影响MLM预训练，从而提高语言模型在各种知识密集型任务中的性能。我们强制语言模型优先考虑信息性单词。实验表明，所提出的方法可以显著提高预训练语言模型在关闭书本的情况下的事实回忆，问答，情感分析和自然语言推理等任务的性能。

    Masked language modeling (MLM) plays a key role in pretraining large language models. But the MLM objective is often dominated by high-frequency words that are sub-optimal for learning factual knowledge. In this work, we propose an approach for influencing MLM pretraining in a way that can improve language model performance on a variety of knowledge-intensive tasks. We force the language model to prioritize informative words in a fully unsupervised way. Experiments demonstrate that the proposed approach can significantly improve the performance of pretrained language models on tasks such as factual recall, question answering, sentiment analysis, and natural language inference in a closed-book setting.
    
[^25]: 多模态实体对齐的属性一致知识图谱表示学习

    Attribute-Consistent Knowledge Graph Representation Learning for Multi-Modal Entity Alignment. (arXiv:2304.01563v1 [cs.CL])

    [http://arxiv.org/abs/2304.01563](http://arxiv.org/abs/2304.01563)

    本文提出了一个新颖的属性一致知识图谱表示学习框架(ACK-MMEA)，弥补实体在学习过程中特定模态上具有不同数量属性的上下文差距问题，从而提高多模态实体对齐(MMEA)的准确性。

    

    多模态实体对齐(MMEA)旨在找到多模态知识图谱(MMKGs)之间所有等价的实体对。本文提出一个新颖的属性一致知识图谱表示学习框架(ACK-MMEA)，通过合并一致的对齐知识来弥补上下文差距问题。通过使用多模态属性统一化构建属性一致的知识图谱(ACKGs)，并在基于关系的图神经网络中融合这些ACKGs以获得聚合的关系表示和稳健的实体表示。

    The multi-modal entity alignment (MMEA) aims to find all equivalent entity pairs between multi-modal knowledge graphs (MMKGs). Rich attributes and neighboring entities are valuable for the alignment task, but existing works ignore contextual gap problems that the aligned entities have different numbers of attributes on specific modality when learning entity representations. In this paper, we propose a novel attribute-consistent knowledge graph representation learning framework for MMEA (ACK-MMEA) to compensate the contextual gaps through incorporating consistent alignment knowledge. Attribute-consistent KGs (ACKGs) are first constructed via multi-modal attribute uniformization with merge and generate operators so that each entity has one and only one uniform feature in each modality. The ACKGs are then fed into a relation-aware graph neural network with random dropouts, to obtain aggregated relation representations and robust entity representations. In order to evaluate the ACK-MMEA fa
    
[^26]: 统一对比传递框架与传播结构用于提高低资源谣言检测

    A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection. (arXiv:2304.01492v1 [cs.CL])

    [http://arxiv.org/abs/2304.01492](http://arxiv.org/abs/2304.01492)

    该文介绍了一个利用对比传递框架和传播结构，将从充足资源的谣言数据学到的特征适应于低资源情况下的方式，可以检测到跨越语言和领域界限的谣言。

    

    大量的谣言伴随着突发新闻或热门话题而传播，这严重阻碍了真相的传播。现有的谣言检测算法展示了在前几天新闻上良好性能的前景，但是由于缺乏训练数据和先前的专业知识，它们很难发现与预期事件有关的谣言，特别是在不同语言（即低资源环境）中传播的谣言。在本文中，我们提出了一个统一的对比传递框架，通过将从充足资源的谣言数据学到的特征适应于低资源情况下的特征来检测谣言。具体来说，我们首先将在社交媒体上传播的谣言表示为无向拓扑结构，然后通过统一对比范式进行Multi-scale图卷积网络的训练。我们的模型明确地突破了领域和/或语言问题的障碍，通过语言对齐和一种新颖的领域自适应对比。

    The truth is significantly hampered by massive rumors that spread along with breaking news or popular topics. Since there is sufficient corpus gathered from the same domain for model training, existing rumor detection algorithms show promising performance on yesterday's news. However, due to a lack of training data and prior expert knowledge, they are poor at spotting rumors concerning unforeseen events, especially those propagated in different languages (i.e., low-resource regimes). In this paper, we propose a unified contrastive transfer framework to detect rumors by adapting the features learned from well-resourced rumor data to that of the low-resourced. More specifically, we first represent rumor circulated on social media as an undirected topology, and then train a Multi-scale Graph Convolutional Network via a unified contrastive paradigm. Our model explicitly breaks the barriers of the domain and/or language issues, via language alignment and a novel domain-adaptive contrastive 
    
[^27]: 聊天GPT，还是不聊天GPT：这是一个问题！

    To ChatGPT, or not to ChatGPT: That is the question!. (arXiv:2304.01487v1 [cs.LG])

    [http://arxiv.org/abs/2304.01487](http://arxiv.org/abs/2304.01487)

    研究评估了聊天GPT检测中的最新技术和其他AI生成文本检测工具的表现，并提出区分人工生成和AI生成文本的重要性。

    

    聊天GPT已经成为一种全球感知。随着聊天GPT和其他大型语言模型（LLM）的出现，对于他们的误用的担忧也增加了，例如传播虚假消息，抄袭，操纵公众舆论，欺骗和欺诈。因此，区分人工生成和AI生成的文本变得越来越重要。研究人员提出了各种检测方法，从基本的二元分类器到更复杂的深度学习模型。一些检测技术依赖于统计特征或句法模式，而其他一些则包含语义或上下文信息以提高准确性。本研究的主要目标是对聊天GPT检测中最新技术进行全面和现代化的评估。此外，我们还评估了其他未专门声称检测聊天GPT生成内容的AI生成文本检测工具以评估它们在检测聊天GPT生成内容方面的表现。在我们的评估中，我们使用了一个包含人工编写和聊天GPT生成的文本的大型数据集。

    ChatGPT has become a global sensation. As ChatGPT and other Large Language Models (LLMs) emerge, concerns of misusing them in various ways increase, such as disseminating fake news, plagiarism, manipulating public opinion, cheating, and fraud. Hence, distinguishing AI-generated from human-generated becomes increasingly essential. Researchers have proposed various detection methodologies, ranging from basic binary classifiers to more complex deep-learning models. Some detection techniques rely on statistical characteristics or syntactic patterns, while others incorporate semantic or contextual information to improve accuracy. The primary objective of this study is to provide a comprehensive and contemporary assessment of the most recent techniques in ChatGPT detection. Additionally, we evaluated other AI-generated text detection tools that do not specifically claim to detect ChatGPT-generated content to assess their performance in detecting ChatGPT-generated content. For our evaluation,
    
[^28]: 无需重新训练的Transformer模型分块压缩

    Blockwise Compression of Transformer-based Models without Retraining. (arXiv:2304.01483v1 [cs.CL])

    [http://arxiv.org/abs/2304.01483](http://arxiv.org/abs/2304.01483)

    本论文提出了一种名为BCT的分块压缩框架，可以对整个Transformer模型进行更细粒度的压缩，实现了降低部署门槛的目的。

    

    基于Transformer模型的GPT-3、ChatGPT和GPT-4近年来备受关注，但它们的巨大计算资源和存储开销仍然是不可避免的挑战。为了解决这个问题，我们提出了一种名为BCT的分块压缩框架，可以对整个Transformer模型进行更细粒度的压缩，包括嵌入、矩阵乘法、GELU、Softmax、层规范化以及所有中间结果。我们对一个高效模型使用BCT进行了压缩并在多个GLUE数据集上进行了评估，结果显示在大多数任务中，BCT只会带来少于0.90%的准确率下降。

    Transformer-based models, represented by GPT-3, ChatGPT, and GPT-4, have recently attracted increasing interest, research enthusiasm, and business demand. However, their massive computation resources and huge memory footprint are inevitable challenges. To tackle this issue, we propose BCT, a framework of blockwise compression for transformers without retraining, to lower deployment thresholds. BCT achieves more fine-grained compression of the whole transformer, including embedding, matrix multiplication, GELU, Softmax, layer normalization, and all the intermediate results. As a case, we compress an efficient model with BCT and evaluate it on several General Language Understanding Evaluation (GLUE) datasets. The results show that BCT can achieve a less than 0.90% accuracy drop in most tasks.
    
[^29]: 向量接地问题

    The Vector Grounding Problem. (arXiv:2304.01481v1 [cs.CL])

    [http://arxiv.org/abs/2304.01481](http://arxiv.org/abs/2304.01481)

    本文讨论了大型语言模型中的向量接地问题，通过区分内部表示接地的不同方式，总结出五个概念。

    

    大型语言模型(LLMs)在处理复杂的语言任务上表现出色，引发了对它们能力本质的激烈辩论。不同于人类，这些模型只能从文本数据中学习语言，没有与真实世界的直接交互。尽管如此，它们能够生成关于各种话题似乎有意义的文本。这一印象深刻的成就重新引起了对经典“符号接地问题”的关注，这个问题质疑了经典符号AI系统的内部表示和输出能否具有内在意义。与这些系统不同，现代LLMs是计算向量而不是符号的人工神经网络。然而，这样的系统也有类似的问题，我们称之为向量接地问题。本文有两个主要目标。首先，我们区分了生物或人工系统中内部表示可以接地的各种方式，确定了五个不同的概念

    The remarkable performance of large language models (LLMs) on complex linguistic tasks has sparked a lively debate on the nature of their capabilities. Unlike humans, these models learn language exclusively from textual data, without direct interaction with the real world. Nevertheless, they can generate seemingly meaningful text about a wide range of topics. This impressive accomplishment has rekindled interest in the classical 'Symbol Grounding Problem,' which questioned whether the internal representations and outputs of classical symbolic AI systems could possess intrinsic meaning. Unlike these systems, modern LLMs are artificial neural networks that compute over vectors rather than symbols. However, an analogous problem arises for such systems, which we dub the Vector Grounding Problem. This paper has two primary objectives. First, we differentiate various ways in which internal representations can be grounded in biological or artificial systems, identifying five distinct notions 
    
[^30]: 基于Semigraph的情感极性的讽刺检测

    Polarity based Sarcasm Detection using Semigraph. (arXiv:2304.01424v1 [cs.CL])

    [http://arxiv.org/abs/2304.01424](http://arxiv.org/abs/2304.01424)

    本文提出了一种基于Semigraph的方式进行情感和讽刺检测，通过文档的情感极性得分来识别文章中的讽刺，该方法在亚马逊产品评价中表现优异。

    

    讽刺是在线平台上经常出现的高级语言表达。讽刺检测是一项具有挑战性的自然语言处理任务，它影响情感分析。本文提出了Semigraph的创新方法，包括Semigraph的构建和讽刺检测过程。在文本文档的模式关联性中建议了Semigraph的变体。提出的方法是使用Semigraph获得文档的讽刺和非讽刺情感极性得分。讽刺情感极性得分表示文档变得讽刺的可能性。基于情感极性评分模型检测讽刺。所提出的模型的表现优于现有的讽刺检测方法。在亚马逊产品评价中，该模型的准确率、召回率和f-measure分别为0.87、0.79和0.83。

    Sarcasm is an advanced linguistic expression often found on various online platforms. Sarcasm detection is challenging in natural language processing tasks that affect sentiment analysis. This article presents the inventive method of the semigraph, including semigraph construction and sarcasm detection processes. A variation of the semigraph is suggested in the pattern-relatedness of the text document. The proposed method is to obtain the sarcastic and non-sarcastic polarity scores of a document using a semigraph. The sarcastic polarity score represents the possibility that a document will become sarcastic. Sarcasm is detected based on the polarity scoring model. The performance of the proposed model enhances the existing prior art approach to sarcasm detection. In the Amazon product review, the model achieved the accuracy, recall, and f-measure of 0.87, 0.79, and 0.83, respectively.
    
[^31]: 基于事件不确定性的Twitter主题上下文向量关联

    Thematic context vector association based on event uncertainty for Twitter. (arXiv:2304.01423v1 [cs.CL])

    [http://arxiv.org/abs/2304.01423](http://arxiv.org/abs/2304.01423)

    本文提出一种基于数据关联的方法，利用不确定性原理来提取Twitter数据中伴随相关事件的关键词，从而解决了当前系统中非正式语言下关键词提取的挑战。实验表明这种方法对Twitter COVID-19数据有效。

    

    关键词提取是文本挖掘中的关键步骤。在Twitter数据中提取伴随相关事件的关键词是一个巨大的挑战。主要挑战在于所使用的语言非正式。拼写错误的单词、缩略语和歧义性术语的使用导致了非正式。目前系统中提取非正式语言下的关键词是基于模式或事件基础的。本文提出了一种基于数据关联的使用主题事件提取上下文关键词的方法。使用不确定性原理识别事件的主题上下文。使用称为主题上下文向量的矢量对主题上下文进行权重计算，这些矢量表示事件的确定或不确定。在Twitter COVID-19数据集上测试了系统并证明其有效性。该系统从测试数据集中提取特定于事件的主题上下文向量并对其进行排名。提取的主题上下文向量用于推断关键词。

    Keyword extraction is a crucial process in text mining. The extraction of keywords with respective contextual events in Twitter data is a big challenge. The challenging issues are mainly because of the informality in the language used. The use of misspelled words, acronyms, and ambiguous terms causes informality. The extraction of keywords with informal language in current systems is pattern based or event based. In this paper, contextual keywords are extracted using thematic events with the help of data association. The thematic context for events is identified using the uncertainty principle in the proposed system. The thematic contexts are weighed with the help of vectors called thematic context vectors which signifies the event as certain or uncertain. The system is tested on the Twitter COVID-19 dataset and proves to be effective. The system extracts event-specific thematic context vectors from the test dataset and ranks them. The extracted thematic context vectors are used for th
    
[^32]: StatCan对话数据集：通过真实意图的对话检索数据表

    The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents. (arXiv:2304.01412v1 [cs.CL])

    [http://arxiv.org/abs/2304.01412](http://arxiv.org/abs/2304.01412)

    该论文介绍了StatCan对话数据集，这是一个涉及真实意图的对话转换，提出了两个任务：基于正在进行的对话自动检索相关数据表和在每个回合自动生成适当的代理响应。该研究对现有模型持续提出挑战，鼓励更多基于对话的数据检索研究。

    

    我们介绍了StatCan对话数据集，其中包含了19379次代表Statistics Canada的代理和在线用户之间的对话转换，涉及真实意图，使用英语或法语进行，代理会检索到5000多个复杂数据表之一。基于这个数据集，我们提出了两个任务：（1）基于正在进行的对话自动检索相关数据表，（2）在每个回合自动生成适当的代理响应。我们通过建立强基线来研究每个任务的难度。在时间数据分割的实验中，我们发现所有模型都难以推广到未来的对话中，当我们从验证集移动到测试集时，我们观察到两个任务的性能都显著下降。此外，我们发现响应生成模型在何时返回表格方面存在困难。考虑到这些任务对现有模型的挑战，我们鼓励进一步研究基于对话的数据检索。

    We introduce the StatCan Dialogue Dataset consisting of 19,379 conversation turns between agents working at Statistics Canada and online users looking for published data tables. The conversations stem from genuine intents, are held in English or French, and lead to agents retrieving one of over 5000 complex data tables. Based on this dataset, we propose two tasks: (1) automatic retrieval of relevant tables based on a on-going conversation, and (2) automatic generation of appropriate agent responses at each turn. We investigate the difficulty of each task by establishing strong baselines. Our experiments on a temporal data split reveal that all models struggle to generalize to future conversations, as we observe a significant drop in performance across both tasks when we move from the validation to the test set. In addition, we find that response generation models struggle to decide when to return a table. Considering that the tasks pose significant challenges to existing models, we enc
    
[^33]: Pythia：一套用于跨训练和扩展分析大型语言模型的工具套件

    Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling. (arXiv:2304.01373v1 [cs.CL])

    [http://arxiv.org/abs/2304.01373](http://arxiv.org/abs/2304.01373)

    本文介绍了一套名为 Pythia 的工具套件，包含 16 个大型语言模型，其大小从 70M 到 12B 参数不等。Pythia 可以帮助研究人员在多个领域开展研究，作者还提出了几个新的研究结果，在记忆、应用少量数据时的效果以及减少性别偏见等方面具有重要意义。

    

    本文介绍了一套名为Pythia的工具套件，其中包括16个大型语言模型，这些模型都是在完全相同的顺序下从公共数据中训练而来的，大小从70M到12B参数不等。作者公开了这16个模型的154个检查点，并提供了工具以下载和重构模型的exact training dataloaders以进行进一步研究。本文介绍了Pythia在多个领域中的应用，包括对记忆、减少性别偏见等方面的新颖研究结果，并演示了这种高度控制的设置如何用于获得有关语言模型及其训练动态的新见解。

    How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at https://github.com/EleutherAI/pythia.
    
[^34]: 一种简单有效的跨语言抄袭检测方法

    A Simple and Effective Method of Cross-Lingual Plagiarism Detection. (arXiv:2304.01352v1 [cs.CL])

    [http://arxiv.org/abs/2304.01352](http://arxiv.org/abs/2304.01352)

    该论文提出了一种简单有效的跨语言抄袭检测方法，不依赖机器翻译和词义消歧，使用开放的多语言同义词库进行候选检索任务和预训练的基于多语言BERT的语言模型进行详细分析，在多个基准测试中取得了最先进的结果。

    

    我们提出了一种简单的跨语言抄袭检测方法，适用于大量的语言。该方法利用开放的多语言同义词库进行候选检索任务，并利用预训练的基于多语言BERT的语言模型进行详细分析。该方法在使用时不依赖机器翻译和词义消歧，因此适用于许多语言，包括资源匮乏的语言。该方法在多个现有和新的基准测试中展示了其有效性，在法语、俄语和亚美尼亚语等语言中取得了最先进的结果。

    We present a simple cross-lingual plagiarism detection method applicable to a large number of languages. The presented approach leverages open multilingual thesauri for candidate retrieval task and pre-trained multilingual BERT-based language models for detailed analysis. The method does not rely on machine translation and word sense disambiguation when in use, and therefore is suitable for a large number of languages, including under-resourced languages. The effectiveness of the proposed approach is demonstrated for several existing and new benchmarks, achieving state-of-the-art results for French, Russian, and Armenian languages.
    
[^35]: 化学物质-蛋白质相互作用提取的端到端模型：更好的标记化和基于跨度的管道策略

    End-to-End Models for Chemical-Protein Interaction Extraction: Better Tokenization and Span-Based Pipeline Strategies. (arXiv:2304.01344v1 [cs.CL])

    [http://arxiv.org/abs/2304.01344](http://arxiv.org/abs/2304.01344)

    本论文提出了一种基于跨度的管道策略和更好的标记化方法来提高化学物质-蛋白质相互作用提取的性能，并在多个指标上优于现有最先进模型。

    

    端到端关系提取(E2ERE)是信息提取中的重要任务，在生物医学领域尤其如此，因为科学文献数量继续呈指数级增长。E2ERE通常涉及识别实体(或命名实体识别(NER))和关联关系，而大多数E2ERE任务只是假定实体已经提供，并最终执行关系分类。由于NER中的错误可能导致RE中的更多错误，E2ERE本质上比仅RE更难。生物医学E2ERE中的一个复杂数据集是ChemProt数据集(BioCreative VI,2017)，该数据集在科学文献中识别化学化合物和基因/蛋白质之间的关系。ChemProt被包括在所有最新的生物医学自然语言处理基准测试中，包括BLUE、BLURB和BigBio。然而，这些基准测试和其他单独的努力通常没有端到端的处理ChemProt。在这个工作中，我们采用基于跨度的管道策略和更好的标记化方法来进行E2ERE，为ChemProt数据集提供更好的性能。我们的提议模型在多个指标上优于现有最先进模型，证明了我们方法的有效性。

    End-to-end relation extraction (E2ERE) is an important task in information extraction, more so for biomedicine as scientific literature continues to grow exponentially. E2ERE typically involves identifying entities (or named entity recognition (NER)) and associated relations, while most RE tasks simply assume that the entities are provided upfront and end up performing relation classification. E2ERE is inherently more difficult than RE alone given the potential snowball effect of errors from NER leading to more errors in RE. A complex dataset in biomedical E2ERE is the ChemProt dataset (BioCreative VI, 2017) that identifies relations between chemical compounds and genes/proteins in scientific literature. ChemProt is included in all recent biomedical natural language processing benchmarks including BLUE, BLURB, and BigBio. However, its treatment in these benchmarks and in other separate efforts is typically not end-to-end, with few exceptions. In this effort, we employ a span-based pipe
    
[^36]: 不使用词典创建自定义的事件数据：一些诀窍

    Creating Custom Event Data Without Dictionaries: A Bag-of-Tricks. (arXiv:2304.01331v1 [cs.CL])

    [http://arxiv.org/abs/2304.01331](http://arxiv.org/abs/2304.01331)

    本文介绍了一些 NLP 技巧，可用于高效生产自定义事件数据集，其中包括使用主动学习训练分类器、利用大型语言模型和标准机器学习分类器识别行动者和接收者等。

    

    事件数据指的是从文本中自动提取的结构化记录，描述“谁对谁做了什么”，是国际政治学者的重要数据来源。由于开发新的事件数据集的高成本，特别是使用依赖手工构建字典的自动化系统，大多数研究人员使用大型预先存在的数据集，例如ICEWS，而不是开发针对其特定研究问题进行了优化的定制事件数据集。本文介绍了一些诀窍，利用自然语言处理（NLP）的最新进展，实现高效的定制事件数据生产。本文介绍了一些技术，如使用主动学习来训练事件类别分类器，在文本中使用大型语言模型和标准机器学习分类器来识别行动者和动作的接收者，以及使用NLP的预训练“问答”模型来解决提及的问题。

    Event data, or structured records of ``who did what to whom'' that are automatically extracted from text, is an important source of data for scholars of international politics. The high cost of developing new event datasets, especially using automated systems that rely on hand-built dictionaries, means that most researchers draw on large, pre-existing datasets such as ICEWS rather than developing tailor-made event datasets optimized for their specific research question. This paper describes a ``bag of tricks'' for efficient, custom event data production, drawing on recent advances in natural language processing (NLP) that allow researchers to rapidly produce customized event datasets. The paper introduces techniques for training an event category classifier with active learning, identifying actors and the recipients of actions in text using large language models and standard machine learning classifiers and pretrained ``question-answering'' models from NLP, and resolving mentions of ac
    
[^37]: 文档相似性算法比较

    A Comparison of Document Similarity Algorithms. (arXiv:2304.01330v1 [cs.CL])

    [http://arxiv.org/abs/2304.01330](http://arxiv.org/abs/2304.01330)

    本研究比较了文档相似性算法，分为三类进行探讨：统计算法、神经网络算法和基于语料库/知识的算法，通过比较最有效的算法来确定哪些算法最有用。

    

    文档相似性是自然语言处理的重要组成部分，最常用于抄袭检测和文本摘要。因此，找到最有效的文档相似性算法对自然语言处理领域有很大的正面影响。本研究旨在研究众多文档相似性算法，并确定哪些算法最有用。本文通过将文档相似性算法分为三类：统计算法、神经网络算法和基于语料库/知识的算法，来探讨最有效的文档相似性算法。我们使用一系列基准数据集和评估方法来比较每个类别中最有效的算法，评估了每个算法可能用于的所有可能领域。

    Document similarity is an important part of Natural Language Processing and is most commonly used for plagiarism-detection and text summarization. Thus, finding the overall most effective document similarity algorithm could have a major positive impact on the field of Natural Language Processing. This report sets out to examine the numerous document similarity algorithms, and determine which ones are the most useful. It addresses the most effective document similarity algorithm by categorizing them into 3 types of document similarity algorithms: statistical algorithms, neural networks, and corpus/knowledge-based algorithms. The most effective algorithms in each category are also compared in our work using a series of benchmark datasets and evaluations that test every possible area that each algorithm could be used in.
    
[^38]: 侦测Cheapfake的大挑战

    Grand Challenge On Detecting Cheapfakes. (arXiv:2304.01328v1 [cs.CV])

    [http://arxiv.org/abs/2304.01328](http://arxiv.org/abs/2304.01328)

    这个研究关注侦测 Cheapfake 媒体的 OOC 误用，尤其是对真实照片的误用和不符合的图像说明。研究旨在开发并测试模型，以检测给定样本是否为 OOC ，基于 COSMOS 数据集。

    

    Cheapfake是一个新出现的术语，指的是非AI（“cheap”）对多媒体内容的篡改。它们比deepfake更普遍。Cheapfake媒体可以使用图像/视频编辑软件创建，也可以在不使用任何软件的情况下通过在误导性声明旁边分享媒体来简单地改变图像/视频的背景。这种背景的修改被称为媒体的上下文（OOC）误用。与伪造媒体不同，OOC媒体更难检测，因为图像和视频并未被篡改。在这个挑战中，我们将重点关注侦测OOC图像，更具体地说是在新闻报道中，对真实照片的误用与不符合的图像说明。这个挑战的目的是开发和基准测试可以用来检测给定样本（新闻图像和相关的标题）是否为OOC的模型，基于最近编制的COSMOS数据集。

    Cheapfake is a recently coined term that encompasses non-AI ("cheap") manipulations of multimedia content. Cheapfakes are known to be more prevalent than deepfakes. Cheapfake media can be created using editing software for image/video manipulations, or even without using any software, by simply altering the context of an image/video by sharing the media alongside misleading claims. This alteration of context is referred to as out-of-context (OOC) misuse of media. OOC media is much harder to detect than fake media, since the images and videos are not tampered. In this challenge, we focus on detecting OOC images, and more specifically the misuse of real photographs with conflicting image captions in news items. The aim of this challenge is to develop and benchmark models that can be used to detect whether given samples (news image and associated captions) are OOC, based on the recently compiled COSMOS dataset.
    
[^39]: PALI：波斯-阿拉伯文字语言识别基准

    PALI: A Language Identification Benchmark for Perso-Arabic Scripts. (arXiv:2304.01322v1 [cs.CL])

    [http://arxiv.org/abs/2304.01322](http://arxiv.org/abs/2304.01322)

    本文探讨了使用波斯-阿拉伯文字进行语言识别面临的挑战，提出了解决方案，包括使用一组有监督技术将句子分类为它们的语言以及提出一个针对分类器混淆语言簇的分层模型。实验表明这些方法是有效的。

    

    波斯-阿拉伯文字是世界各地广泛采用和使用的一类文字。使用这种文字进行语言识别对语言技术至关重要，在资源匮乏的情况下较具挑战性。因此，本文重点探讨使用波斯-阿拉伯文字进行语言识别所面临的挑战，尤其是在双语社区中“非传统”的书写方式。为了解决这个问题，我们使用了一组有监督技术将句子分类为它们的语言。基于此，我们还提出了一个针对分类器常常混淆的语言簇的分层模型。我们实验的结果表明了我们的方法的有效性。

    The Perso-Arabic scripts are a family of scripts that are widely adopted and used by various linguistic communities around the globe. Identifying various languages using such scripts is crucial to language technologies and challenging in low-resource setups. As such, this paper sheds light on the challenges of detecting languages using Perso-Arabic scripts, especially in bilingual communities where ``unconventional'' writing is practiced. To address this, we use a set of supervised techniques to classify sentences into their languages. Building on these, we also propose a hierarchical model that targets clusters of languages that are more often confused by the classifiers. Our experiment results indicate the effectiveness of our solutions.
    
[^40]: 低资源语言技术中的语料库创建方法：以南部库尔德语和拉基语为例

    Approaches to Corpus Creation for Low-Resource Language Technology: the Case of Southern Kurdish and Laki. (arXiv:2304.01319v1 [cs.CL])

    [http://arxiv.org/abs/2304.01319](http://arxiv.org/abs/2304.01319)

    本文介绍了通过当地新闻网站、电台和田野调查等方法解决南部库尔德语和拉基语缺乏数据的问题，并探讨了在库尔德语和扎扎格腊尼语的其他变体环境下进行语言识别的任务。

    

    受低资源和濒危语言社区中语言数据缺乏的困扰，南部库尔德语和拉基语缺乏有效工具，仅有极少量的资源可供使用。本文提出通过利用当地新闻网站内容、播放南部库尔德语内容的当地电台以及Laki的田野调查等方法以解决这一问题。本文讨论了这些低资源语言所面临的挑战，尤其是在书写和标准化方面以及在检索数据来源和数字化手写内容以创建南部库尔德语和拉基语的语料库方面的挑战。此外，我们还分析了在库尔德语和扎扎格腊尼语的其他变体环境下进行语言识别的任务。

    One of the major challenges that under-represented and endangered language communities face in language technology is the lack or paucity of language data. This is also the case of the Southern varieties of the Kurdish and Laki languages for which very limited resources are available with insubstantial progress in tools. To tackle this, we provide a few approaches that rely on the content of local news websites, a local radio station that broadcasts content in Southern Kurdish and fieldwork for Laki. In this paper, we describe some of the challenges of such under-represented languages, particularly in writing and standardization, and also, in retrieving sources of data and retro-digitizing handwritten content to create a corpus for Southern Kurdish and Laki. In addition, we study the task of language identification in light of the other variants of Kurdish and Zaza-Gorani languages.
    
[^41]: 有效地对齐跨语言会话任务的提示调整跨语言转移学习

    Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v1 [cs.CL])

    [http://arxiv.org/abs/2304.01295](http://arxiv.org/abs/2304.01295)

    本文提出了一个平行大规模多语种会话数据集XSGD，开发了一种有效的基于提示调整的方法来学习对齐提示，同时研究了跨语言任务的NLI-based和vanilla分类器，并在插槽填充和意图分类任务上评估了模型的跨语言泛化能力。

    

    针对自然语言处理任务，跨语言转移的语言模型已被广泛研究，但是对于会话任务的研究相对较少。本文提出了XSGD，这是一个由Schema-Guided Dialogue（SGD）翻译成105种其他语言的平行大规模多语种会话数据集。为了实现对齐的跨语言表示方法，我们开发了一种有效的基于提示调整的方法来学习对齐提示。我们还研究了两种不同的分类器：NLI-based和vanilla分类器，并测试了对齐提示所实现的跨语言能力。我们在两个对话任务（插槽填充和意图分类）上评估了我们模型的跨语言泛化能力。

    Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce XSGD, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains approximately 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model's cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent cla
    
[^42]: 大语言模型时代的安全分析：聊天GPT在STPA案例研究中的应用

    Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT. (arXiv:2304.01246v1 [cs.CL])

    [http://arxiv.org/abs/2304.01246](http://arxiv.org/abs/2304.01246)

    本文研究了大型语言模型在系统论过程分析（STPA）中的应用，并采用ChatGPT对自动紧急制动（AEB）系统进行了案例研究。结果表明，重复双工交互方法是最有效的，并显着提高了STPA的质量。本研究证明，LLMs可以应用于安全分析，并为安全关键系统提供有价值的见解。

    

    大型语言模型（LLMs），如ChatGPT和BERT，由于其具有类似于人类的对话，在许多知识领域中具有详细和明确的答案，正在引领一场新的人工智能热潮。虽然LLMs正在迅速应用于许多人工智能应用领域，但我们对以下问题感兴趣：安全关键系统的安全分析是否可以利用LLMs？为了回答这个问题，我们使用ChatGPT对自动紧急制动（AEB）系统的系统论过程分析（STPA）进行了案例研究。STPA是最普遍的危险分析技术之一，但它存在诸多局限性，例如高复杂性和主观性，本文旨在探讨ChatGPT的应用，以解决这些局限性。具体而言，通过考虑其与人类专家的交互，研究了三种将ChatGPT纳入STPA中的方法：一次性单工交互、重复单工交互和重复双工交互。比较结果表明：（i）在没有人类专家的情况下使用ChatGPT不能为STPA提供足够的信息；（ii）一次性单工交互对STPA有帮助，但不如重复交互有效；（iii）重复双工交互一致优于其他方法，并显着提高了STPA的质量。我们的研究表明，LLMs可以应用于安全分析，并为AEB以外的其他安全关键系统提供有价值的见解。

    Large Language Models (LLMs), such as ChatGPT and BERT, are leading a new AI heatwave due to its human-like conversations with detailed and articulate answers across many domains of knowledge. While LLMs are being quickly applied to many AI application domains, we are interested in the following question: Can safety analysis for safety-critical systems make use of LLMs? To answer, we conduct a case study of Systems Theoretic Process Analysis (STPA) on Automatic Emergency Brake (AEB) systems using ChatGPT. STPA, one of the most prevalent techniques for hazard analysis, is known to have limitations such as high complexity and subjectivity, which this paper aims to explore the use of ChatGPT to address. Specifically, three ways of incorporating ChatGPT into STPA are investigated by considering its interaction with human experts: one-off simplex interaction, recurring simplex interaction, and recurring duplex interaction. Comparative results reveal that: (i) using ChatGPT without human exp
    
[^43]: 在证据图上运用多通道异构学习增强临床证据推荐

    Enhancing Clinical Evidence Recommendation with Multi-Channel Heterogeneous Learning on Evidence Graphs. (arXiv:2304.01242v1 [cs.CL])

    [http://arxiv.org/abs/2304.01242](http://arxiv.org/abs/2304.01242)

    该论文提出使用证据共指图和证据文本图表来解决临床证据推荐中的联系稀疏性问题，并介绍了一个多通道异构学习模型来支持临床决策过程。

    

    临床证据包括患者、干预（如药物或物理治疗）、问题和结果之间的关联和影响。推荐临床证据的目的是为医务人员提供相关信息，以支持他们的决策过程并生成新的证据。我们的具体任务侧重于根据临床问题推荐证据。然而，某些临床问题和相关证据之间的直接联系往往是稀疏的，这就产生了联系稀疏性的挑战。此外，为了推荐适当的证据，有必要同时利用证据之间的拓扑关系和描述它们的文本信息。为了应对这些挑战，我们定义了两个知识图表：证据共指图和证据文本图表，以表示证据元素之间的拓扑和语言关系。我们还引入了一个多通道异构学习模型。

    Clinical evidence encompasses the associations and impacts between patients, interventions (such as drugs or physiotherapy), problems, and outcomes. The goal of recommending clinical evidence is to provide medical practitioners with relevant information to support their decision-making processes and to generate new evidence. Our specific task focuses on recommending evidence based on clinical problems. However, the direct connections between certain clinical problems and related evidence are often sparse, creating a challenge of link sparsity. Additionally, to recommend appropriate evidence, it is essential to jointly exploit both topological relationships among evidence and textual information describing them. To address these challenges, we define two knowledge graphs: an Evidence Co-reference Graph and an Evidence Text Graph, to represent the topological and linguistic relations among evidential elements, respectively. We also introduce a multi-channel heterogeneous learning model a
    
[^44]: 发现德拉维达语中的恐同和跨性别歧视：探索深度学习方法

    Detection of Homophobia & Transphobia in Dravidian Languages: Exploring Deep Learning Methods. (arXiv:2304.01241v1 [cs.CL])

    [http://arxiv.org/abs/2304.01241](http://arxiv.org/abs/2304.01241)

    本研究旨在探讨不同深度学习模型在德拉维达语社交媒体评论分类中的适用性，以便将其识别为恐同、跨性别歧视和非反LGBT+内容。

    

    在线社交媒体平台上辱骂性内容的增加正在影响在线用户的社交生活。使用冒犯和仇恨言论使得社交媒体变得有毒。恐同和跨性别歧视是针对LGBT+社群的冒犯性评论。及时检测和处理这些评论，向涉及此类行为的用户发出警告或及时标记是非常必要的。然而，在德拉维达语这种被认为是低资源语言的语言中，自动检测此类内容是一个具有挑战性的任务。因此，本文试图探讨不同深度学习模型在马来语和泰米尔语社交媒体评论分类中的适用性，以便将其识别为恐同、跨性别歧视和非反LGBT+内容。而其中应用的深度学习模型有卷积神经网络（CNN）、在GloVe嵌入下使用长短期记忆（LSTM）和基于变压器的学习模型（多语言BERT和IndicBERT）。

    The increase in abusive content on online social media platforms is impacting the social life of online users. Use of offensive and hate speech has been making so-cial media toxic. Homophobia and transphobia constitute offensive comments against LGBT+ community. It becomes imperative to detect and handle these comments, to timely flag or issue a warning to users indulging in such behaviour. However, automated detection of such content is a challenging task, more so in Dravidian languages which are identified as low resource languages. Motivated by this, the paper attempts to explore applicability of different deep learning mod-els for classification of the social media comments in Malayalam and Tamil lan-guages as homophobic, transphobic and non-anti-LGBT+content. The popularly used deep learning models- Convolutional Neural Network (CNN), Long Short Term Memory (LSTM) using GloVe embedding and transformer-based learning models (Multilingual BERT and IndicBERT) are applied to the class
    
[^45]: 用自然语言处理的方法识别心理健康记录中的疼痛提及

    Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach. (arXiv:2304.01240v1 [cs.CL])

    [http://arxiv.org/abs/2304.01240](http://arxiv.org/abs/2304.01240)

    该研究使用机器学习技术，成功地识别出心理健康电子健康记录中的疼痛提及，提高了对疼痛和心理健康之间关系的理解。

    

    疼痛是访问医疗资源的常见原因，并且是一个研究领域，特别是在与心理健康的重叠方面。心理健康电子健康记录是研究此重叠的良好数据来源。然而，疼痛的大量信息保存在这些记录的自由文本中，由于其歧义性，疼痛的提及呈现出独特的自然语言处理问题。本项目使用匿名的心理健康电子健康记录数据库中的数据。利用这些数据训练基于机器学习的分类算法，将句子分类为讨论患者疼痛或不讨论。这将有助于从大型数据库中提取相关疼痛信息，并将这些输出用于进一步研究疼痛和心理健康。共手动三重注释了1,985份文件，以创建黄金标准训练数据，并用于训练三种常用的分类算法。最佳模型的F1分数为0.787。结果证明了使用自然语言处理识别心理健康电子健康记录中的疼痛提及的可行性，这可以改善对疼痛和心理健康之间关系的理解。

    Pain is a common reason for accessing healthcare resources and is a growing area of research, especially in its overlap with mental health. Mental health electronic health records are a good data source to study this overlap. However, much information on pain is held in the free text of these records, where mentions of pain present a unique natural language processing problem due to its ambiguous nature. This project uses data from an anonymised mental health electronic health records database. The data are used to train a machine learning based classification algorithm to classify sentences as discussing patient pain or not. This will facilitate the extraction of relevant pain information from large databases, and the use of such outputs for further studies on pain and mental health. 1,985 documents were manually triple-annotated for creation of gold standard training data, which was used to train three commonly used classification algorithms. The best performing model achieved an F1-
    
[^46]: Spam-T5：基于小样本的邮件垃圾检测的大型语言模型基准测试

    Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection. (arXiv:2304.01238v1 [cs.CL])

    [http://arxiv.org/abs/2304.01238](http://arxiv.org/abs/2304.01238)

    本文通过比较不同类型的大型语言模型和传统机器学习技术在邮件垃圾检测中的表现，发现大多数情况下，大型语言模型优于传统技术，特别是在样本有限的情况下。同时，本文还介绍了经过改进和微调的Spam-T5模型，该模型具有出色的性能表现。

    

    本文通过比较三种不同类型的大型语言模型（BERT-like、Sentence Transformers和Seq2Seq）以及传统机器学习技术（如朴素贝叶斯和LightGBM）在邮件垃圾检测中的有效性，研究了大型语言模型在邮件垃圾检测中的作用。同时，我们还评估了这些模型在四个公共数据集上的表现，并使用不同数量的训练样本（完整训练集和小样本）进行了测试。 发现在大多数情况下，LLMs优于基线技术，特别是在小样本情况下。这种适应性使LLMs在邮件垃圾检测任务中具有独特的优势，因为标记样本数量有限，并且模型需要经常更新。此外，我们介绍了Spam-T5模型，该模型是专门为检测电子邮件垃圾而进行了改进和微调。我们的结果表明，Spam-T5模型具有出色的性能。

    This paper investigates the effectiveness of large language models (LLMs) in email spam detection by comparing prominent models from three distinct families: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we examine well-established machine learning techniques for spam detection, such as Na\"ive Bayes and LightGBM, as baseline methods. We assess the performance of these models across four public datasets, utilizing different numbers of training samples (full training set and few-shot settings). Our findings reveal that, in the majority of cases, LLMs surpass the performance of the popular baseline techniques, particularly in few-shot scenarios. This adaptability renders LLMs uniquely suited to spam detection tasks, where labeled samples are limited in number and models require frequent updates. Additionally, we introduce Spam-T5, a Flan-T5 model that has been specifically adapted and fine-tuned for the purpose of detecting email spam. Our results demonstrate that Spam-T5 
    
[^47]: 多模态知觉语言模型在急诊室结果预测中的应用

    Multi-Modal Perceiver Language Model for Outcome Prediction in Emergency Department. (arXiv:2304.01233v1 [cs.CL])

    [http://arxiv.org/abs/2304.01233](http://arxiv.org/abs/2304.01233)

    本文提出了一种基于多模态知觉语言模型的方法，用于结果预测和患者分诊。实验结果表明，该模型在急诊科临床决策方面具有显著的潜力。

    

    语言建模已经在生成具有良好准确性和高语义连贯性的令人信服的文本方面取得了重大进展。一个有趣的研究方向是使用上下文信息来增强这些强大的模型以用于特定的应用程序。在这项工作中，我们探索了面向医疗保健应用的多模态语言建模。我们对基于三角洲区域记录的首席投诉信息和生命体征的文本信息进行了结果预测和患者分诊。我们改编了Perceiver——一种通用变换器模型，它已经在多个应用程序中展现出可行的结果。由于生命体征模态以表格形式呈现，我们修改了Perceiver的位置编码以确保置换不变性。我们使用MIMIC-IV ED数据集上的120K次访问来评估多模态语言模型对诊断代码预测的任务。在实验分析中，我们展示了多模态改善了预测表现，相比于单模态模型，并且Perceiver在任务中胜过了其他现有的多模态语言模型。该模型在改进急诊科临床决策方面具有显著的潜力。

    Language modeling have shown impressive progress in generating compelling text with good accuracy and high semantic coherence. An interesting research direction is to augment these powerful models for specific applications using contextual information. In this work, we explore multi-modal language modeling for healthcare applications. We are interested in outcome prediction and patient triage in hospital emergency department based on text information in chief complaints and vital signs recorded at triage. We adapt Perceiver - a modality-agnostic transformer-based model that has shown promising results in several applications. Since vital-sign modality is represented in tabular format, we modified Perceiver position encoding to ensure permutation invariance. We evaluated the multi-modal language model for the task of diagnosis code prediction using MIMIC-IV ED dataset on 120K visits. In the experimental analysis, we show that mutli-modality improves the prediction performance compared w
    
[^48]: 通过自我改进的方式实现更好的代码语言模型

    Better Language Models of Code through Self-Improvement. (arXiv:2304.01228v1 [cs.CL])

    [http://arxiv.org/abs/2304.01228](http://arxiv.org/abs/2304.01228)

    本文提出了一个简单的数据增强框架来改善预训练语言模型为代码生成和代码摘要等任务微调的瓶颈问题，提高了模型性能。

    

    近期，各种预训练的代码语言模型引起了人们的广泛关注。这些模型通过多模式目标在大规模数据集上进行预训练。但是，对其进行微调需要大量监督，并且受到提供的数据集规模的限制。我们提出了一个简单的数据增强框架以改善这个问题。这个框架利用了在预训练和微调阶段获得的知识来生成伪数据，并将其用作下一步的训练数据。我们将这个框架应用到最先进的语言模型中，如CodeT5、CodeBERT和UnixCoder。结果表明，我们的框架显著提高了PLMC在与代码相关的序列生成任务中的性能，如CodeXGLUE基准测试中的代码摘要和代码生成。

    Pre-trained language models for code (PLMCs) have gained attention in recent research. These models are pre-trained on large-scale datasets using multi-modal objectives. However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided. We aim to improve this issue by proposing a simple data augmentation framework. Our framework utilizes knowledge gained during the pre-training and fine-tuning stage to generate pseudo data, which is then used as training data for the next step. We incorporate this framework into the state-of-the-art language models, such as CodeT5, CodeBERT, and UnixCoder. The results show that our framework significantly improves PLMCs' performance in code-related sequence generation tasks, such as code summarization and code generation in the CodeXGLUE benchmark.
    
[^49]: PromptORE -- 一种全新的无监督关系抽取方法

    PromptORE -- A Novel Approach Towards Fully Unsupervised Relation Extraction. (arXiv:2304.01209v1 [cs.CL])

    [http://arxiv.org/abs/2304.01209](http://arxiv.org/abs/2304.01209)

    提出了“基于提示的开放关系抽取”模型，在无监督设置下不需要超参数调整，实现了全新的无监督关系抽取方法。

    

    无监督关系抽取旨在识别文本中实体之间的关系，而在训练期间没有标记的数据可用。这对于没有注释数据集的特定领域关系抽取和先验未知关系类型的开放领域关系抽取特别相关。虽然最近的方法取得了有希望的结果，但它们严重依赖于超参数，调整这些超参数通常需要标记数据。为了减轻对超参数的依赖，我们提出了PromptORE，即“基于提示的开放关系抽取”模型。我们将新的提示调整范例适应于无监督设置，并用它来嵌入表达关系的句子。然后我们对这些嵌入进行聚类，发现候选关系，并尝试不同的策略来自动估计适当的聚类数量。据我们所知，PromptORE是第一个不需要超参数调整的无监督关系抽取模型。

    Unsupervised Relation Extraction (RE) aims to identify relations between entities in text, without having access to labeled data during training. This setting is particularly relevant for domain specific RE where no annotated dataset is available and for open-domain RE where the types of relations are a priori unknown. Although recent approaches achieve promising results, they heavily depend on hyperparameters whose tuning would most often require labeled data. To mitigate the reliance on hyperparameters, we propose PromptORE, a ''Prompt-based Open Relation Extraction'' model. We adapt the novel prompt-tuning paradigm to work in an unsupervised setting, and use it to embed sentences expressing a relation. We then cluster these embeddings to discover candidate relations, and we experiment different strategies to automatically estimate an adequate number of clusters. To the best of our knowledge, PromptORE is the first unsupervised RE model that does not need hyperparameter tuning. Resul
    
[^50]: Baize:一种基于自我对话数据参数高效调整的开源聊天模型

    Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data. (arXiv:2304.01196v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.01196](http://arxiv.org/abs/2304.01196)

    提出了一种流程，通过利用ChatGPT与自身对话，能够自动生成高质量的多轮聊天语料库，并采用参数高效调整来增强开源的大型语言模型LLaMA，得到的模型被命名为Baize，在最小化潜在风险的护栏下，在多轮对话中展现出良好的性能。

    

    聊天模型，如ChatGPT，展现出惊人的能力，并在众多领域得到迅速应用。但是，这些模型只能通过受限制的API进行访问，从而制造了新的研究和领域进展的障碍。我们提出了一种流程，通过利用ChatGPT与自身对话，能够自动生成高质量的多轮聊天语料库。随后，我们采用参数高效调整来增强开源的大型语言模型LLaMA。所得到的模型被命名为Baize，在最小化潜在风险的护栏下，在多轮对话中展现出良好的性能。Baize的模型和数据仅用于研究目的，可在https://github.com/project-baize/baize进行下载。在线演示也可在https://huggingface.co/spaces/project-baize/baize-lora-7B进行访问。

    Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. The Baize models and data are released for research purposes only at https://github.com/project-baize/baize. An online demo is also available at https://huggingface.co/spaces/project-baize/baize-lora-7B.
    
[^51]: “Polytuplet Loss: 训练阅读理解和逻辑推理模型的反向方法”

    Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2304.01046](http://arxiv.org/abs/2304.01046)

    本文研究了一种训练阅读理解和逻辑推理模型的反向方法，利用相对准确性的策略来训练模型，通过Polytuplet Loss函数来确保优先学习答案选择的相对正确性，获得了不错的成果，提出了具有一般性的训练方法和模型架构。

    

    在整个学校教育过程中，学生们将受到阅读理解和逻辑推理的考验。学生们已经开发了各种策略来完成此类考试，其中有些被认为是通常表现优于其他策略的。这样一种策略涉及强调相对准确性而非绝对准确性，理论上可以在不完全掌握解题所需信息的情况下得出正确答案。本文研究了应用这种策略来训练迁移学习模型以解决阅读理解和逻辑推理问题的有效性。这些模型在具有挑战性的阅读理解和逻辑推理基准数据集ReClor上进行了评估。尽管以前的研究集中于逻辑推理技能，但我们专注于一种通用的训练方法和模型架构。我们提出了Polytuplet Loss函数，是三元组损失函数的扩展，以确保优先学习答案选择的相对正确性而非学习绝对正确性。

    Throughout schooling, students are tested on reading comprehension and logical reasoning. Students have developed various strategies for completing such exams, some of which are generally thought to outperform others. One such strategy involves emphasizing relative accuracy over absolute accuracy and can theoretically produce the correct answer without full knowledge of the information required to solve the question. This paper examines the effectiveness of applying such a strategy to train transfer learning models to solve reading comprehension and logical reasoning questions. The models were evaluated on the ReClor dataset, a challenging reading comprehension and logical reasoning benchmark. While previous studies targeted logical reasoning skills, we focus on a general training method and model architecture. We propose the polytuplet loss function, an extension of the triplet loss function, to ensure prioritization of learning the relative correctness of answer choices over learning
    
[^52]: AI能否让伽玛射线天体物理学家失业？

    Can AI Put Gamma-Ray Astrophysicists Out of a Job?. (arXiv:2303.17853v1 [physics.pop-ph])

    [http://arxiv.org/abs/2303.17853](http://arxiv.org/abs/2303.17853)

    本文评估了使用最先进的转换器模型创作一篇假的科学论文的能力，旨在验证这些模型是否能够仅基于语言信息解释天文观测和源，并为同行评审识别欺诈性生成的科学论文提供潜在手段。结论是，目前天文学家的工作是安全的。

    

    本文中，我们评估了最先进的转换器模型创作一篇文章的能力。这篇文章详细介绍了一种使用不存在的成像大气切伦科夫望远镜(IATC)阵列检测脉冲星风星云的方法。我们进行这项研究的目的是评估这些模型仅基于语言信息解释天文观测和源的能力，并评估在同行评议过程中如何识别诈骗生成的科学论文的潜在手段（考虑到这些工具尚未部署可靠的生成模型数字水印）。我们得出结论，天文学家的工作目前是安全的。

    In what will likely be a litany of generative-model-themed arXiv submissions celebrating April the 1st, we evaluate the capacity of state-of-the-art transformer models to create a paper detailing the detection of a Pulsar Wind Nebula with a non-existent Imaging Atmospheric Cherenkov Telescope (IACT) Array. We do this to evaluate the ability of such models to interpret astronomical observations and sources based on language information alone, and to assess potential means by which fraudulently generated scientific papers could be identified during peer review (given that reliable generative model watermarking has yet to be deployed for these tools). We conclude that our jobs as astronomers are safe for the time being. From this point on, prompts given to ChatGPT and Stable Diffusion are shown in orange, text generated by ChatGPT is shown in black, whereas analysis by the (human) authors is in blue.
    
[^53]: 有哪些问题需要进行交谈才能回答？一个 AskReddit 问题案例的研究。

    What Types of Questions Require Conversation to Answer? A Case Study of AskReddit Questions. (arXiv:2303.17710v1 [cs.HC])

    [http://arxiv.org/abs/2303.17710](http://arxiv.org/abs/2303.17710)

    本文研究了哪些模糊开放性问题最适合通过对话回答，发现这些问题高度社交和个人化，对未来研究提供了有益的参考。

    

    自动会话系统（如聊天机器人、语音对话系统和智能音箱）的广泛应用已经深刻地影响了现代数字生活。然而，这些系统主要设计用于回答明确定义的问题，而非支持用户探索复杂的、不明确的问题。本文旨在通过研究哪些模糊的、开放性问题最适合通过对话来回答，推动会话系统的边界。我们首先从 AskReddit 上发布的100万个开放式请求中随机抽取了500个问题，然后招募在线工人回答这些问题的8个询问。我们还执行开放式编码，将问题分类为27个不同的领域。我们发现，人们认为需要交谈才能满意解决的问题是高度社交和个人化的。本文提供了未来研究如何适应用户需求的见解。

    The proliferation of automated conversational systems such as chatbots, spoken-dialogue systems, and smart speakers, has significantly impacted modern digital life. However, these systems are primarily designed to provide answers to well-defined questions rather than to support users in exploring complex, ill-defined questions. In this paper, we aim to push the boundaries of conversational systems by examining the types of nebulous, open-ended questions that can best be answered through conversation. We first sampled 500 questions from one million open-ended requests posted on AskReddit, and then recruited online crowd workers to answer eight inquiries about these questions. We also performed open coding to categorize the questions into 27 different domains. We found that the issues people believe require conversation to resolve satisfactorily are highly social and personal. Our work provides insights into how future research could be geared to align with users' needs.
    
[^54]: oBERTa: 通过改进初始化、蒸馏和剪枝来提高稀疏迁移学习

    oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes. (arXiv:2303.17612v1 [cs.CL])

    [http://arxiv.org/abs/2303.17612](http://arxiv.org/abs/2303.17612)

    oBERTa是一组易于使用的语言模型，通过改进初始化、蒸馏、剪枝等技术，可以在不需要模型压缩方面的专业知识的情况下提高稀疏迁移学习的效率和准确性。

    

    本文介绍了oBERTa语言模型的范围，它是一组易于使用的语言模型，允许自然语言处理（NLP）从业者在不需要模型压缩方面的专业知识的情况下获得3.8到24.3倍的更快速的模型。oBERTa扩展了现有的剪枝、知识蒸馏和量化工作，并利用冻结的嵌入来改进知识蒸馏，并改进模型初始化，以在广泛的传递任务上提供更高的准确性。在生成oBERTa时，我们探索了高度优化的RoBERTa与BERT在预训练和微调期间剪枝方面的不同之处，并发现它在微调期间不太适合压缩。我们探索了oBERTa在七个具有代表性的NLP任务上的使用，并发现改进的压缩技术使得经过剪枝的oBERTa模型能够匹配BERTBASE的性能，并超过SQUAD V1.1问答数据的Prune OFA Large的性能。

    In this paper, we introduce the range of oBERTa language models, an easy-to-use set of language models, which allows Natural Language Processing (NLP) practitioners to obtain between 3.8 and 24.3 times faster models without expertise in model compression. Specifically, oBERTa extends existing work on pruning, knowledge distillation, and quantization and leverages frozen embeddings to improve knowledge distillation, and improved model initialization to deliver higher accuracy on a a broad range of transfer tasks. In generating oBERTa, we explore how the highly optimized RoBERTa differs from the BERT with respect to pruning during pre-training and fine-tuning and find it less amenable to compression during fine-tuning. We explore the use of oBERTa on a broad seven representative NLP tasks and find that the improved compression techniques allow a pruned oBERTa model to match the performance of BERTBASE and exceed the performance of Prune OFA Large on the SQUAD V1.1 Question Answering data
    
[^55]: Chat-REC：面向互动和可解释性的LLM增强推荐系统

    Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System. (arXiv:2303.14524v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2303.14524](http://arxiv.org/abs/2303.14524)

    本文介绍了一种创新的推荐系统模式-Chat-Rec，通过将LLMs与对话式推荐相结合，解决了传统推荐系统中互动性和可解释性不足的问题。因此，Chat-Rec能够更有效地学习用户偏好，并在推荐过程中建立用户-产品之间的联系，具有更大的透明度和控制。

    

    大型语言模型(LLMs)在解决各种应用任务方面具有巨大的潜力。然而，传统的推荐系统仍面临很大的挑战，如互动性和可解释性差，这实际上也阻碍了它们在真实世界系统中的广泛部署。为了解决这些限制，本文提出了一个创新的模式，称为Chat-REC（ChatGPT增强推荐系统），通过将用户配置文件和历史交互转换为提示，创新地增强LLMs用于构建对话式推荐系统。通过在上下文中学习，Chat-Rec被证明在学习用户偏好和建立用户与产品之间的联系方面非常有效，这也使得推荐过程更具互动性和可解释性。此外，在Chat-Rec框架内，用户的偏好可以转移到不同的产品进行跨领域推荐，并且基于提示的注入允许更大的透明度和对推荐过程的控制。

    Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks. However, traditional recommender systems continue to face great challenges such as poor interactivity and explainability, which actually also hinder their broad deployment in real-world systems. To address these limitations, this paper proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender System) that innovatively augments LLMs for building conversational recommender systems by converting user profiles and historical interactions into prompts. Chat-Rec is demonstrated to be effective in learning user preferences and establishing connections between users and products through in-context learning, which also makes the recommendation process more interactive and explainable. What's more, within the Chat-Rec framework, user's preferences can transfer to different products for cross-domain recommendations, and prompt-based injection of informa
    
[^56]: AfroDigits：面向非洲语言的基于社区驱动的口语数字数据集

    AfroDigits: A Community-Driven Spoken Digit Dataset for African Languages. (arXiv:2303.12582v1 [cs.CL])

    [http://arxiv.org/abs/2303.12582](http://arxiv.org/abs/2303.12582)

    AfroDigits是第一个发布的面向非洲语言的音频数字数据集，为非洲语言中的语音应用程序开辟了道路。

    

    计算机语音技术的进步是显著的，但由于非洲语料库的匮乏，其在非洲语言中的集成仍然有限。为了解决这个问题，我们提出了AfroDigits，这是一个极简的、由社区驱动的非洲语言口语数字数据集，目前覆盖了38种非洲语言。作为AfroDigits实际应用的演示，我们使用Wav2Vec2.0-Large和XLS-R模型，在六种非洲语言[Igbo(ibo), Yoruba(yor), Rundi(run), Oshiwambo(kua), Shona(sna)和Oromo(gax)]上进行了音频数字分类实验。我们的实验揭示了混合非洲语料库在微调过程中的效果。AfroDigits是非洲语言中第一个发布的音频数字数据集，我们相信它将为面向非洲的语音应用程序开辟道路，如电话号码和街道地址的识别。我们在https:/ / afrodigits公开发布数据集和平台。

    The advancement of speech technologies has been remarkable, yet its integration with African languages remains limited due to the scarcity of African speech corpora. To address this issue, we present AfroDigits, a minimalist, community-driven dataset of spoken digits for African languages, currently covering 38 African languages. As a demonstration of the practical applications of AfroDigits, we conduct audio digit classification experiments on six African languages [Igbo (ibo), Yoruba (yor), Rundi (run), Oshiwambo (kua), Shona (sna), and Oromo (gax)] using the Wav2Vec2.0-Large and XLS-R models. Our experiments reveal a useful insight on the effect of mixing African speech corpora during finetuning. AfroDigits is the first published audio digit dataset for African languages and we believe it will, among other things, pave the way for Afro-centric speech applications such as the recognition of telephone numbers, and street numbers. We release the dataset and platform publicly at https:/
    
[^57]: 自适应超参数调节在无监督跨语种分词中的应用

    Self-tuning hyper-parameters for unsupervised cross-lingual tokenization. (arXiv:2303.02427v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.02427](http://arxiv.org/abs/2303.02427)

    本研究探讨了在多语言中进行语言无关的无监督分词的元学习可能性，并使用多种适应度函数自动确定无监督分词模型的超参数。结果表明在英语和俄语中，前三种度量的加性组合与 F1 分词得分之间有相当好的相关性，在中文中，F1 得分与压缩因子有显著的相关性。

    

    本文探讨了在英语、俄语和中文等多种语言中进行语言无关的无监督分词问题的元学习可能性。我们使用不同的人类无关适应度函数，如标准化反熵、压缩因子和交叉分割 F1 得分，以及三个度量的加性和乘性组合，实现了对早期作品提出的无监督分词模型超参数的自动确定的元学习方法，并测试其与传统 F1 分词得分的关系。我们发现在英语和俄语方面，在前三种度量的加性组合与后者之间有相当好的相关性。在中文方面，我们发现 F1 得分与压缩因子之间存在显著的相关性。我们的结果表明了对于资源稀缺和死语言的坚实的无监督分词可能性，并让人们可以从效率演化的角度思考人类语言。

    We explore the possibility of meta-learning for the language-independent unsupervised tokenization problem for English, Russian, and Chinese. We implement the meta-learning approach for automatic determination of hyper-parameters of the unsupervised tokenization model proposed in earlier works, relying on various human-independent fitness functions such as normalised anti-entropy, compression factor and cross-split F1 score, as well as additive and multiplicative composite combinations of the three metrics, testing them against the conventional F1 tokenization score. We find a fairly good correlation between the latter and the additive combination of the former three metrics for English and Russian. In case of Chinese, we find a significant correlation between the F 1 score and the compression factor. Our results suggest the possibility of robust unsupervised tokenization of low-resource and dead languages and allow us to think about human languages in terms of the evolution of efficie
    
[^58]: ChatGPT失败分类存档

    A Categorical Archive of ChatGPT Failures. (arXiv:2302.03494v8 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.03494](http://arxiv.org/abs/2302.03494)

    本研究对ChatGPT的11个失败类别进行了全面分析，其中包括推理、事实错误、数学、编码和偏见。找出失败原因以帮助研究人员和开发人员改进未来的语言模型和聊天机器人。

    

    大型语言模型已经在不同领域证明了其价值。由OpenAI开发的ChatGPT使用大量数据进行训练，通过理解上下文并生成适当的响应来模拟人类对话。它因能够有效地回答广泛的人类问题而受到重视，其流利和全面的答案在安全性和实用性方面超越了先前的公共聊天机器人。然而，缺乏ChatGPT失效的全面分析，这是本研究的重点。本研究提出并讨论了11个失败类别，包括推理、事实错误、数学、编码和偏见。还突出了ChatGPT的风险、限制和社会影响。本研究的目标是帮助研究人员和开发人员增强未来的语言模型和聊天机器人。

    Large language models have been demonstrated to be valuable in different fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of data and simulates human conversation by comprehending context and generating appropriate responses. It has garnered significant attention due to its ability to effectively answer a broad range of human inquiries, with fluent and comprehensive answers surpassing prior public chatbots in both security and usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking, which is the focus of this study. Eleven categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. The risks, limitations, and societal implications of ChatGPT are also highlighted. The goal of this study is to assist researchers and developers in enhancing future language models and chatbots.
    
[^59]: 多级模态变压器用于多页 DocVQA

    Hierarchical multimodal transformers for Multi-Page DocVQA. (arXiv:2212.05935v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.05935](http://arxiv.org/abs/2212.05935)

    本文提出了一种新方法，Hi-VT5，它是一种分层 transformer 结构，能够处理多页文档 DocVQA 任务。实验表明该方法能够有效地回答问题。

    

    文档视觉问答（DocVQA）是指回答文档图像中的问题的任务。现有的 DocVQA 工作仅考虑单页文档。但是，在实际场景中，文档主要由多个页面组成，应该一起处理。在本文中，我们将 DocVQA 扩展到多页面场景。为此，首先创建一个新的数据集 MP-DocVQA，其中问题是针对多页文档而非单页提出的。其次，我们提出了一种新的分层方法 Hi-VT5，基于 T5 结构，克服了处理长多页文档的当前方法的局限性。所提出的方法基于分层变压器结构，编码器对每个页面的最相关信息进行摘要，然后解码器利用这些摘要信息生成最终答案。通过广泛实验，我们证明了我们的方法能够在单个阶段中回答问题并提供

    Document Visual Question Answering (DocVQA) refers to the task of answering questions from document images. Existing work on DocVQA only considers single-page documents. However, in real scenarios documents are mostly composed of multiple pages that should be processed altogether. In this work we extend DocVQA to the multi-page scenario. For that, we first create a new dataset, MP-DocVQA, where questions are posed over multi-page documents instead of single pages. Second, we propose a new hierarchical method, Hi-VT5, based on the T5 architecture, that overcomes the limitations of current methods to process long multi-page documents. The proposed method is based on a hierarchical transformer architecture where the encoder summarizes the most relevant information of every page and then, the decoder takes this summarized information to generate the final answer. Through extensive experimentation, we demonstrate that our method is able, in a single stage, to answer the questions and provid
    
[^60]: 基于分层韵律模型的电影配音学习

    Learning to Dub Movies via Hierarchical Prosody Models. (arXiv:2212.04054v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.04054](http://arxiv.org/abs/2212.04054)

    本研究提出了一种新的电影配音架构，通过分层韵律建模从嘴唇、面部和场景三个方面将视觉信息与相应的语音韵律联系起来，从而解决了V2C任务中情感变化和说话速度匹配等问题。

    

    基于一段文本、一个视频片段和一个参考音频，电影配音任务（也称为视觉语音克隆V2C）旨在使用所需的说话者声音作为参考，生成与视频中呈现的说话者情感匹配的语音，而且要求生成的语音恰好匹配视频中呈现的不断变化的情感和说话速度。与以往的工作不同，本文提出了一种新颖的电影配音体系结构，通过分层韵律建模从三个方面将视觉信息与相应的语音韵律联系起来：嘴唇、面部和场景。具体而言，我们将嘴唇运动与语音持续时间对齐，并通过基于最近心理学发现的愉悦和唤起表示的注意机制，将面部表情传达到语音能量和音高上。此外，我们设计了一个情感增强器来捕捉全局视频场景的氛围。

    Given a piece of text, a video clip and a reference audio, the movie dubbing (also known as visual voice clone V2C) task aims to generate speeches that match the speaker's emotion presented in the video using the desired speaker voice as reference. V2C is more challenging than conventional text-to-speech tasks as it additionally requires the generated speech to exactly match the varying emotions and speaking speed presented in the video. Unlike previous works, we propose a novel movie dubbing architecture to tackle these problems via hierarchical prosody modelling, which bridges the visual information to corresponding speech prosody from three aspects: lip, face, and scene. Specifically, we align lip movement to the speech duration, and convey facial expression to speech energy and pitch via attention mechanism based on valence and arousal representations inspired by recent psychology findings. Moreover, we design an emotion booster to capture the atmosphere from global video scenes. A
    
[^61]: 订单是不需要的：用于人格检测的动态深度图卷积网络

    Orders Are Unwanted: Dynamic Deep Graph Convolutional Network for Personality Detection. (arXiv:2212.01515v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.01515](http://arxiv.org/abs/2212.01515)

    本论文提出了一种动态深度图卷积网络，采用学习连接的方式而非确定性连接，自动学习帖子之间的连接来预测人格特征，相比于先前的方法具有更好的性能表现。

    

    在许多领域如社交网络分析中，基于在线帖子来预测人格特征已成为一项重要任务。这个任务的挑战之一是从各种帖子中汇总信息，形成每个用户的整体资料。许多先前的解决方案将帖子简单地连接成长文档，然后通过顺序或分层模型对文档进行编码，这种方式引入了无序的帖子，可能会误导模型。本文提出了一种动态深度图卷积网络(D-DGCN)，以克服上述限制。具体地，我们设计了一种学习连接方法，采用动态多跳结构而非确定性结构，并将其与DGCN模块相结合，自动学习帖子之间的连接。帖子编码器、学习连接和DGCN模块同时进行端到端的联合训练。在Kaggle和Pandora数据集上的实验结果显示出D-DGCN的优越性能。

    Predicting personality traits based on online posts has emerged as an important task in many fields such as social network analysis. One of the challenges of this task is assembling information from various posts into an overall profile for each user. While many previous solutions simply concatenate the posts into a long document and then encode the document by sequential or hierarchical models, they introduce unwarranted orders for the posts, which may mislead the models. In this paper, we propose a dynamic deep graph convolutional network (D-DGCN) to overcome the above limitation. Specifically, we design a learn-to-connect approach that adopts a dynamic multi-hop structure instead of a deterministic structure, and combine it with a DGCN module to automatically learn the connections between posts. The modules of post encoder, learn-to-connect, and DGCN are jointly trained in an end-to-end manner. Experimental results on the Kaggle and Pandora datasets show the superior performance of 
    
[^62]: 带有个性自适应注意力的个性化对话生成

    Personalized Dialogue Generation with Persona-Adaptive Attention. (arXiv:2210.15088v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.15088](http://arxiv.org/abs/2210.15088)

    本文提出了一种新的框架，使用个性自适应注意力（PAA）来生成基于个性的一致性回应，可以通过整合个性和上下文信息的权重来实现。实验证明 PAA 框架具有优越性能，可以在低资源环境下表现出色。

    

    基于个性化的对话系统旨在根据历史上下文和预定义的个性生成一致的回应。与传统的对话生成不同，基于个性的对话需要考虑对话上下文和个性两个方面，这对于一致的训练提出了挑战。本文提出了一种有效的框架，使用个性自适应注意力（PAA），通过我们设计的注意力适应性地整合了来自个性和上下文信息的权重。此外，PAA 还应用了动态屏蔽机制，不仅可以丢弃上下文和个性的冗余信息，还可以作为正则化机制避免过拟合。实验结果表明，与强基线相比，所提出的 PAA 框架在自动和人工评估中表现优异。此外，所提出的 PAA 方法在低资源环境下表现同样出色。

    Persona-based dialogue systems aim to generate consistent responses based on historical context and predefined persona. Unlike conventional dialogue generation, the persona-based dialogue needs to consider both dialogue context and persona, posing a challenge for coherent training. Specifically, this requires a delicate weight balance between context and persona. To achieve that, in this paper, we propose an effective framework with Persona-Adaptive Attention (PAA), which adaptively integrates the weights from the persona and context information via our designed attention. In addition, a dynamic masking mechanism is applied to the PAA to not only drop redundant information in context and persona but also serve as a regularization mechanism to avoid overfitting. Experimental results demonstrate the superiority of the proposed PAA framework compared to the strong baselines in both automatic and human evaluation. Moreover, the proposed PAA approach can perform equivalently well in a low-r
    
[^63]: 语言符号的表现：基于示范的人机交互中，体感手语手指拼写的翻译机器人获取

    Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction. (arXiv:2209.05135v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.05135](http://arxiv.org/abs/2209.05135)

    本文提出了一种从视频示例学习手语拼写在机器人中的实现的方法。通过训练模仿运动的策略，利用预训练的深度视觉模型从RGB视频中提取手的三维姿态，并识别出最佳的模仿超参数集，本文成功展示了方法的普适性。

    

    学习机器人中细致的动作是一个具挑战性的问题，特别是在机器人手的上下文中。本文提出一种方法，通过视频示例学习无额外信息下的熟练运动模仿，以获得手语拼写在机器人中的实现。我们首先建立了一个机器人手的URDF模型，并使每个关节只有一个致动器。然后我们利用预训练的深度视觉模型从RGB视频中提取手的三维姿态。接着，我们利用最先进的强化学习算法(即近端策略优化和软演员-评论家算法)来训练一种能够复制示范运动的策略。我们基于参考运动识别出最佳的模仿超参数集。最后，我们通过对六个对应于拼写字母的不同任务进行测试，证明了我们方法的普适性。

    Learning fine-grained movements is a challenging topic in robotics, particularly in the context of robotic hands. One specific instance of this challenge is the acquisition of fingerspelling sign language in robots. In this paper, we propose an approach for learning dexterous motor imitation from video examples without additional information. To achieve this, we first build a URDF model of a robotic hand with a single actuator for each joint. We then leverage pre-trained deep vision models to extract the 3D pose of the hand from RGB videos. Next, using state-of-the-art reinforcement learning algorithms for motion imitation (namely, proximal policy optimization and soft actor-critic), we train a policy to reproduce the movement extracted from the demonstrations. We identify the optimal set of hyperparameters for imitation based on a reference motion. Finally, we demonstrate the generalizability of our approach by testing it on six different tasks, corresponding to fingerspelled letters.
    
[^64]: 通过多语言微调和回译实现的多语言双向无监督翻译

    Multilingual Bidirectional Unsupervised Translation Through Multilingual Finetuning and Back-Translation. (arXiv:2209.02821v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.02821](http://arxiv.org/abs/2209.02821)

    该论文提出了一种两阶段的方法，实现单个NMT模型对未见过的语言同英语之间的双向无监督翻译。该方法包括多语言微调和双向回译，成功提高了翻译质量。

    

    我们提出了一种两阶段方法，用于训练单个NMT模型以将未见过的语言从和到英语进行翻译。对于第一阶段，我们将编码器-解码器模型初始化为预训练的XLM-R和RoBERTa权重，然后对40种语言到英语的并行数据进行多语言微调。我们发现，该模型可以推广到未见过的语言的零-shot翻译。对于第二阶段，我们利用这种推广能力从单语言数据集生成合成并行数据，然后使用连续的双向回译轮次进行训练。我们称这种方法为EcXTra（{E}nglish-{c}entric Crosslingual ({X}) {Tra}nsfer）。我们的方法在概念上很简单，只使用标准的交叉熵目标，并且是数据驱动的，依次利用辅助并行数据和单语言数据。我们评估我们在7种低资源语言上的无监督NMT结果，发现每一轮回译训练都进一步优化了双向无监督翻译质量，相比基线提高了多达10个BLEU分数。

    We propose a two-stage approach for training a single NMT model to translate unseen languages both to and from English. For the first stage, we initialize an encoder-decoder model to pretrained XLM-R and RoBERTa weights, then perform multilingual fine-tuning on parallel data in 40 languages to English. We find this model can generalize to zero-shot translations on unseen languages. For the second stage, we leverage this generalization ability to generate synthetic parallel data from monolingual datasets, then train with successive rounds of bidirectional back-translation.  We term our approach EcXTra ({E}nglish-{c}entric Crosslingual ({X}) {Tra}nsfer). Our approach is conceptually simple, only using a standard cross-entropy objective throughout, and also is data-driven, sequentially leveraging auxiliary parallel data and monolingual data. We evaluate our unsupervised NMT results on 7 low-resource languages, and find that each round of back-translation training further refines bidirecti
    
[^65]: MENLI: 自然语言推理的鲁棒性评估指标

    MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.07316](http://arxiv.org/abs/2208.07316)

    本文提出了一种基于自然语言推理的鲁棒性评估指标，比现有的摘要评估指标更好，在标准基准测试中表现良好且更加鲁棒，与现有指标相结合可以使评估效果进一步提高。

    

    最近被提出的基于BERT的文本生成评估指标在标准基准测试中表现良好，但易受到对信息正确性的攻击。我们认为这部分原因是此类模型是基于语义相似性建模的。相反，我们提出一种基于自然语言推理（NLI）的鲁棒性评估指标，这种指标更适合建模。我们设计了一种基于偏好的对抗性攻击框架，并表明我们的NLI基础指标比最近的BERT基础指标更具鲁棒性。在标准基准测试中，我们的NLI基础指标优于现有的摘要评估指标，但低于SOTA MT指标。然而，在现有指标与我们的NLI指标相结合时，我们既获得了更高的对抗鲁棒性（15％-30％），又获得了标准基准测试中更高的质量指标（+5％至30％）。

    Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness. We argue that this stems (in part) from the fact that they are models of semantic similarity. In contrast, we develop evaluation metrics based on Natural Language Inference (NLI), which we deem a more appropriate modeling. We design a preference-based adversarial attack framework and show that our NLI based metrics are much more robust to the attacks than the recent BERT-based metrics. On standard benchmarks, our NLI based metrics outperform existing summarization metrics, but perform below SOTA MT metrics. However, when combining existing metrics with our NLI metrics, we obtain both higher adversarial robustness (15%-30%) and higher quality metrics as measured on standard benchmarks (+5% to 30%).
    
[^66]: 研究用于阿拉伯-英语混合代码数据增强的词汇替换方法

    Investigating Lexical Replacements for Arabic-English Code-Switched Data Augmentation. (arXiv:2205.12649v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.12649](http://arxiv.org/abs/2205.12649)

    本文研究了用于阿拉伯-英语混合代码数据增强的词汇替换方法，通过单词对齐的平行语料库进行词汇替换，评估了其在机器翻译（MT）、自动语音识别（ASR）和语音翻译（ST）任务的有效性，并取得了34%的改进。

    

    数据稀疏是阻碍混合代码（CS）NLP系统发展的主要问题。本文研究了一种用于合成方言阿拉伯语-英语混合代码文本的数据增强技术 - 词汇替换。我们使用单词对齐的平行语料库进行词汇替换，其中CS点是随机选择或使用序列到序列模型进行学习的。我们将这些方法与基于词典的替换方法进行比较。通过人工评估评估所生成的句子的质量，评估数据增强对机器翻译（MT）、自动语音识别（ASR）和语音翻译（ST）任务的有效性。结果表明，使用预测模型相对于随机方法生成了更自然的CS句子，这是根据人类判断提出的。在下游任务中，尽管随机方法生成了更多的数据，但两种方法的性能相同（优于基于词典的替换）。总体而言，数据增强实现了34%的改进。

    Data sparsity is a main problem hindering the development of code-switching (CS) NLP systems. In this paper, we investigate data augmentation techniques for synthesizing dialectal Arabic-English CS text. We perform lexical replacements using word-aligned parallel corpora where CS points are either randomly chosen or learnt using a sequence-to-sequence model. We compare these approaches against dictionary-based replacements. We assess the quality of the generated sentences through human evaluation and evaluate the effectiveness of data augmentation on machine translation (MT), automatic speech recognition (ASR), and speech translation (ST) tasks. Results show that using a predictive model results in more natural CS sentences compared to the random approach, as reported in human judgements. In the downstream tasks, despite the random approach generating more data, both approaches perform equally (outperforming dictionary-based replacements). Overall, data augmentation achieves 34% improv
    
[^67]: 电影叙述摘要：一个用于故事理解的视频语言数据集

    Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.05711](http://arxiv.org/abs/2203.05711)

    这个研究收集并发布了一个视频-语言故事数据集SYMON，用于推进多模态故事理解的发展。

    

    尽管AI有了最近的进展，但故事理解仍然是一个未被充分研究的问题。我们收集、预处理并公开发布了一个视频语言故事数据集SYMON，其中包含5,193个流行电影和电视剧的视频摘要。SYMON捕捉了由人类创作者制作的面向人类观众的自然故事叙述视频。作为一个原型和自然故事数据集，SYMON具有高覆盖的多模态故事事件、丰富的心理状态描述和视觉和文本模态之间的大语义差距。我们建立了视频文本检索和电影摘要视频的零样本对齐的基准，展示了在故事理解中领域内数据的重要性。通过SYMON，我们希望为多模态故事理解的进展打下基础。

    Despite recent advances of AI, story understanding remains an open and under-investigated problem. We collect, preprocess, and publicly release a video-language story dataset, Synopses of Movie Narratives (SYMON), containing 5,193 video summaries of popular movies and TV series. SYMON captures naturalistic story-telling videos for human audience made by human creators. As a prototypical and naturalistic story dataset, SYMON features high coverage of multimodal story events, abundant mental-state descriptions, and large semantic gaps between the visual and the textual modalities. We establish benchmarks on video-text retrieval and zero-shot alignment on movie summary videos, which showcase the importance of in-domain data in story understanding. With SYMON, we hope to lay the groundwork for progress in multimodal story understanding.
    
[^68]: 消极情绪传播更快：基于大规模多语言 Twitter 分析的情感在政治沟通中的作用

    Negativity Spreads Faster: A Large-Scale Multilingual Twitter Analysis on the Role of Sentiment in Political Communication. (arXiv:2202.00396v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.00396](http://arxiv.org/abs/2202.00396)

    本文利用大规模多语言 Twitter 数据，分析了希腊、西班牙和联合王国议会成员的推文，并发现消极情绪更易传播。

    

    社交媒体已经在现代社会中对政策制定产生了极大影响，尤其是在西方世界中，Twitter 等平台让用户能够关注政治家，使公民更多地参与政治讨论。同样，政治家也利用 Twitter 表达自己的观点，在当前话题上与他人辩论，并推动自己的政治议程，旨在影响选民行为。本文试图分析三个欧洲国家政治家的推文，并探索它们的传播度。先前的研究表明，传达消极情绪的推文往往会被更频繁地转发。通过利用先进的预训练语言模型，我们对来自希腊、西班牙和联合王国（包括分权政府）议会成员的数十万条推文进行了情感分析。我们通过系统地探究和分析差异，取得了良好的效果。

    Social media has become extremely influential when it comes to policy making in modern societies, especially in the western world, where platforms such as Twitter allow users to follow politicians, thus making citizens more involved in political discussion. In the same vein, politicians use Twitter to express their opinions, debate among others on current topics and promote their political agendas aiming to influence voter behaviour. In this paper, we attempt to analyse tweets of politicians from three European countries and explore the virality of their tweets. Previous studies have shown that tweets conveying negative sentiment are likely to be retweeted more frequently. By utilising state-of-the-art pre-trained language models, we performed sentiment analysis on hundreds of thousands of tweets collected from members of parliament in Greece, Spain and the United Kingdom, including devolved administrations. We achieved this by systematically exploring and analysing the differences bet
    
[^69]: 通过字典学习实现Transformer可视化:将上下文嵌入作为Transformer因子的线性叠加

    Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors. (arXiv:2103.15949v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2103.15949](http://arxiv.org/abs/2103.15949)

    本文提出使用字典学习将上下文嵌入作为Transformer因子的线性叠加来打开Transformer“黑匣子”，通过可视化展示其捕获的层次化语义结构，为更好地理解Transformer网络的工作方式带来新的见解。

    

    自Transformer网络问世以来，它们在自然语言处理表示学习中掀起了一场革命。尽管已经做出了大量努力来解释Transformer网络中的表示，但广泛认为我们的理解还不够。其中一个重要原因是缺乏足够的可视化工具来进行详细的分析。本文提出使用字典学习将其作为Transformer因子的线性叠加来打开这些“黑匣子”。通过可视化，我们展示了被Transformer因子捕获的层次化语义结构，例如词级多义消歧、句子级模式形成和长距离依赖。虽然一些模式符合传统的语言知识，但其余的模式相对出乎意料，可能提供新的见解。我们希望这种可视化工具能带来更多的知识和对Transformer网络工作方式的更好理解。代码可在https://github.com/z中找到。

    Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these "black boxes" as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at https://github.com/z
    

