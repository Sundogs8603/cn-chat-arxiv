# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Measuring Social Norms of Large Language Models](https://arxiv.org/abs/2404.02491) | 论文提出了一个挑战，测试大型语言模型对社会规范的理解，构建了一个数据集涵盖广泛的社会规范问题，通过多代理框架基于大型语言模型来提高模型对社会规范的理解能力。 |
| [^2] | [MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs](https://arxiv.org/abs/2403.19267) | MineLand模拟器引入有限多模感知和生理需求，支持多智能体在协作中填补了信息和功能限制的空白，从而促进更具动态和有效性的多智能体交互。 |
| [^3] | [Compressing Large Language Models by Streamlining the Unimportant Layer](https://arxiv.org/abs/2403.19135) | 通过观察大型语言模型中不同层对隐藏状态的影响程度，提出了LLM-Streamline方法，包括层剪枝和层替换，用于压缩模型并保持性能。 |
| [^4] | [Can multiple-choice questions really be useful in detecting the abilities of LLMs?](https://arxiv.org/abs/2403.17752) | 多项选择题虽然被广泛用于评估大型语言模型，但在测试LLMs能力时存在一定局限性，特别是在需要长篇生成答案的情况下，我们发现LLMs在双语MCQs中表现出顺序敏感性。 |
| [^5] | [LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models](https://arxiv.org/abs/2403.15388) | PruMerge提出了一种自适应的视觉令牌减少方法，可以有效减少大型多模态模型中的视觉令牌数量，同时保持模型性能。 |
| [^6] | [Encode Once and Decode in Parallel: Efficient Transformer Decoding](https://arxiv.org/abs/2403.13112) | 提出了一种新的编码器-解码器模型配置，称为prompt-in-decoder（PiD），可以一次编码输入并并行解码输出，在结构化输出和问答任务中取得高效率，避免了重复输入编码，大幅减少了解码器的内存占用。 |
| [^7] | [AutoTRIZ: Artificial Ideation with TRIZ and Large Language Models](https://arxiv.org/abs/2403.13002) | 本文提出了AutoTRIZ，一种利用大型语言模型自动化和增强TRIZ方法的人工创意工具，为设计自动化和可解释创意提供了一种新颖方法。 |
| [^8] | [Embodied LLM Agents Learn to Cooperate in Organized Teams](https://arxiv.org/abs/2403.12482) | 本文介绍了一个框架，将即时性组织结构强加在LLM代理上，以促进多代理系统内的合作，在具身LLM代理和人-代理合作实验中发现指定领导对团队效率的影响，揭示了LLM代理的领导素质和自发合作行为。 |
| [^9] | [Large Language Models are Contrastive Reasoners](https://arxiv.org/abs/2403.08211) | 对比提示方法显著提高大型语言模型进行复杂推理的能力，不仅在算术、常识和符号推理任务上表现优良，还可以与现有提示方法整合，实现更好的性能。 |
| [^10] | [Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation](https://arxiv.org/abs/2403.07300) | 通过跨模态知识蒸馏和LLMs对齐框架，该方法利用静态和动态知识，充分释放LLMs在时间序列预测中的潜力 |
| [^11] | [Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid Texts](https://arxiv.org/abs/2403.03506) | 本研究探索了在人工智能协作混合文本中句子级人工智能生成文本检测的挑战，并提出了一种基于分割的两步骤流程来检测各段落的一致作者句子。 |
| [^12] | [Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment](https://arxiv.org/abs/2403.02738) | 提出了一种新型因果引导方法，通过前门调整有效减轻大型语言模型的偏见。 |
| [^13] | [Prediction-Powered Ranking of Large Language Models](https://arxiv.org/abs/2402.17826) | 该研究提出了一种统计框架，可以衡量人类与模型偏好之间的不确定性，从而进行大型语言模型的预测排名。 |
| [^14] | [Measuring Vision-Language STEM Skills of Neural Models](https://arxiv.org/abs/2402.17205) | 该研究引入了一个新挑战，用于测试神经模型的STEM技能，提出了一个包含大量基础技能和问题的数据集，需要理解STEM的多模式视觉语言信息，并展示了最新模型对于低年级技能的有限掌握。 |
| [^15] | [A First Look at GPT Apps: Landscape and Vulnerability](https://arxiv.org/abs/2402.15105) | 通过大规模监测和分析GPT商店，开发了自动化工具来研究GPT应用程序中的漏洞和抄袭情况。 |
| [^16] | [Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation](https://arxiv.org/abs/2402.14744) | 提出了一种将大型语言模型LLMs整合到代理框架中的新方法，用于生成个人移动生成，重点是解决将LLMs与真实城市流动数据对齐的问题，并提出了一种自洽方法和检索增强策略来实现可解释活动生成。 |
| [^17] | [KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge](https://arxiv.org/abs/2402.13605) | KorNAT是首个用于评估韩国国家对齐的基准，包括社会价值观和常识对齐两个方面，通过对社会价值和常识多项选择题的测试来评估模型的对齐程度。 |
| [^18] | [A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion](https://arxiv.org/abs/2402.13405) | 通过统一的基于分类学指导的指导调整框架，本文提出了一种利用现有分类学进行实体关系微调的方法，有效解决实体集扩展、分类学扩展和种子引导分类学构建三个任务。 |
| [^19] | [Unsupervised LLM Adaptation for Question Answering](https://arxiv.org/abs/2402.12170) | 提出了无监督LLM适应问答任务，通过利用预训练的LLM和目标领域的未标记文档，实现在新领域回答问题的目标。 |
| [^20] | [What's the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs](https://arxiv.org/abs/2402.11489) | 提出了一种新的混合方法SimPlan，结合了LLMs和经典规划方法，在各种规划领域的实验表明SimPlan明显优于现有的基于LLM的规划者 |
| [^21] | [Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs](https://arxiv.org/abs/2402.11442) | 提出了一个逻辑支架推理规则生成框架，通过构建包含基础和组合规则的推理规则库ULogic，揭示了LLMs在逻辑理解方面与人类表现的显著差距，并通过生成准确、复杂和抽象的推理结果来提升下游推理。 |
| [^22] | [OneBit: Towards Extremely Low-bit Large Language Models](https://arxiv.org/abs/2402.11295) | 本文提出了一种名为OneBit的1位量化感知训练框架，可以将大型语言模型的权重矩阵量化为1位，为极低比特宽度的LLMs部署铺平了道路。 |
| [^23] | [AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators](https://arxiv.org/abs/2402.11073) | 提出了一种新框架 AFaCTA，利用大型语言模型辅助事实性索赔的标注，提高了标注的效率和一致性。 |
| [^24] | [SPAR: Personalized Content-Based Recommendation via Long Engagement Attention](https://arxiv.org/abs/2402.10555) | SPAR是一个基于内容的推荐框架，通过利用PLM、多注意力层和注意力稀疏机制，在会话级别有效地处理长期用户参与历史，提取全面用户兴趣，实现个性化推荐。 |
| [^25] | [MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data](https://arxiv.org/abs/2402.08957) | 这项工作介绍了MUSTARD，一种掌握定理和证明数据统一合成的数据生成框架，通过三个阶段的合成，实现了高质量和多样化的问题和推理步骤的生成。 |
| [^26] | [SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages](https://arxiv.org/abs/2402.08638) | SemRel2024是一个包含来自14种语言的语义文本相关性数据集合，通过该数据集合可以探索和量化语义相关性。这对于大型语言模型的能力和性能有重要的影响。这个数据集合涵盖了南非荷兰语、阿尔及利亚阿拉伯语、阿姆哈拉语、英语、豪萨语、印地语、印度尼西亚语、卢旺达语、马拉地语、摩洛哥阿拉伯语、现代标准阿拉伯语、旁遮普语、西班牙语和泰卢固语等14种语言。 |
| [^27] | [UFO: A UI-Focused Agent for Windows OS Interaction](https://arxiv.org/abs/2402.07939) | UFO是一个专注于Windows操作系统上应用程序的用户界面智能体，利用GPT-Vision的能力来满足用户需求。它通过观察和分析Windows应用程序的图形用户界面和控制信息，实现无缝导航和操作以满足用户的请求。UFO的控制交互模块使得无需人工干预即可实现动作连接和完全自动化执行，使繁琐和耗时的过程变为简单任务。经过测试，UFO在各种场景中取得了良好效果。 |
| [^28] | [Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning](https://arxiv.org/abs/2402.07204) | 本文提出了Open-domain Urban Itinerary Planning (OUIP)任务，用于根据用户以自然语言描述的请求直接生成行程，通过结合空间优化和大型语言模型(LLM)，提供个性化的城市行程定制服务。 |
| [^29] | [L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ](https://arxiv.org/abs/2402.04902) | L4Q是一种参数高效的量化感知训练算法，通过基于LoRA的学习的量化步长，解决了大型语言模型中量化训练的挑战。 |
| [^30] | [CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations](https://arxiv.org/abs/2402.04236) | 本文介绍了CogCoM，一个具备操作链机制的大规模视觉语言模型，通过一系列操作解决视觉问题，并以其证据性的视觉推理能力实现忠实的响应。 |
| [^31] | [Measuring Implicit Bias in Explicitly Unbiased Large Language Models](https://arxiv.org/abs/2402.04105) | 通过引入受心理学启发的两个偏见测量方法，我们在明确无偏大型语言模型中发现了普遍存在的人类化的刻板印象偏见，并与实际决策中的隐性偏见相关。 |
| [^32] | [DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models](https://arxiv.org/abs/2402.02563) | DefInt提出了一种默认干预框架，通过默认使用较小规模的语言模型生成推理思路，然后通过反思推理干预解决复杂推理问题，从而提高混合大型语言模型的效率和准确性。 |
| [^33] | [AutoTimes: Autoregressive Time Series Forecasters via Large Language Models](https://arxiv.org/abs/2402.02370) | AutoTimes是一种基于大型语言模型的自回归时间序列预测器，利用语言模型的转换能力来处理时间序列数据，实现了与先前模型相当的性能。 |
| [^34] | [Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models](https://arxiv.org/abs/2402.02244) | 这篇论文综述了近期为扩展大型语言模型中上下文长度而设计的技术和方法，并回顾了包括架构修改在内的多种技术，使得语言模型可以更有效地理解长上下文。 |
| [^35] | [Panacea: Pareto Alignment via Preference Adaptation for LLMs](https://arxiv.org/abs/2402.02030) | Panacea 是一种创新方法，将大型语言模型对齐重新定义为多维偏好优化问题，通过使用奇异值分解的低秩适应，以在线注入偏好向量的形式，使模型能够适应并 Pareto 最优地满足各种偏好集。 |
| [^36] | [LiPO: Listwise Preference Optimization through Learning-to-Rank](https://arxiv.org/abs/2402.01878) | 本研究提出了一种名为LiPO的框架，用于将语言模型对齐问题定义为一个列表型排序问题。通过从排名列表中学习，该框架可以使策略更有效地学习到可行的响应。 |
| [^37] | [Fractal Patterns May Unravel the Intelligence in Next-Token Prediction](https://arxiv.org/abs/2402.01825) | 通过研究语言的分形结构，我们发现语言具有自相似性和长程相关性，从段落到整个文档都存在相似的模式和依赖性。这些发现有助于理解如何通过预测下一个词来理解文本的多个层级结构。分形参数在预测性能方面优于困惑度。这些发现提供了对语言和机制的新视角。 |
| [^38] | [Model Editing at Scale leads to Gradual and Catastrophic Forgetting](https://arxiv.org/abs/2401.07453) | 评估了当前模型编辑方法在规模化情况下的表现，发现随着模型被顺序编辑多个事实，它会逐渐遗忘先前的事实及执行下游任务的能力。 |
| [^39] | [Efficient Large Language Models: A Survey](https://arxiv.org/abs/2312.03863) | 这篇综述论文对高效大型语言模型进行了系统和全面的调查，提供了从模型为中心、数据为中心和框架为中心的三个主要角度的分类和总结。此外，还创建了一个GitHub存储库来收集和更新相关论文。 |
| [^40] | [Efficient Pre-training for Localized Instruction Generation of Videos](https://arxiv.org/abs/2311.15964) | 提出了一种名为Sieve-&-Swap的技术，通过自动筛选出不相关文本并用人类编写的说明替换文本转录，从而实现视频本地化指令生成的高效预训练。 |
| [^41] | [Learning to Learn for Few-shot Continual Active Learning](https://arxiv.org/abs/2311.03732) | 提出了一种简单而高效的方法，Meta-Continual Active Learning，通过元学习和经验重播解决少样本持续主动学习中的任务混淆和灾难性遗忘，进一步结合文本增强来确保泛化。 |
| [^42] | [Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement](https://arxiv.org/abs/2310.08559) | 对语言模型进行的系统研究揭示了它们在假设提出方面表现惊人，并且通过与一个（任务特定的）符号解释器相结合，能够系统地过滤可能性。 |
| [^43] | [Improving Reinforcement Learning from Human Feedback with Efficient Reward Model Ensemble.](http://arxiv.org/abs/2401.16635) | 本论文提出一种通过高效的奖励模型集成来改进人工反馈强化学习的方法，以解决由于奖励模型预测不准确而导致RLHF输出与人类价值观不一致的问题。 |
| [^44] | [LocMoE: A Low-overhead MoE for Large Language Model Training.](http://arxiv.org/abs/2401.13920) | LocMoE提出了一种新的路由策略，通过将部分节点间通信转换为节点内通信，结合负载平衡和局部性，以提高大型语言模型训练的性能。 |
| [^45] | [The Neglected Tails of Vision-Language Models.](http://arxiv.org/abs/2401.12425) | 本文通过分析预训练文本来衡量视觉语言模型中概念的频率，并发现流行的VLM数据集展示了长尾概念分布，这与按类别的准确率强烈相关。 |
| [^46] | [Critical Data Size of Language Models from a Grokking Perspective.](http://arxiv.org/abs/2401.10463) | 本文从理解的角度探讨了语言模型中的关键数据规模，证明了只有当语言模型达到关键大小时才会发生泛化，同时揭示了更大的模型需要更多数据的趋势。 |
| [^47] | [ChatQA: Building GPT-4 Level Conversational QA Models.](http://arxiv.org/abs/2401.10225) | ChatQA是一系列对话问答模型，可以达到GPT-4级别的准确性。通过两阶段的指令调整方法，可以显著提高大型语言模型在零-shot对话问答中的结果。使用密集检索器进行问答数据集的微调可以实现与最先进的查询重写模型相当的结果，同时降低部署成本。ChatQA-70B在10个对话问答数据集上的平均得分超过了GPT-4，且不依赖于任何来自OpenAI GPT模型的合成数据。 |
| [^48] | [Characterizing Online Eating Disorder Communities with Large Language Models.](http://arxiv.org/abs/2401.09647) | 通过网络和语言分析，我们表征了在线社群中推广饮食紊乱的动态，认为社交媒体平台放大了这一现象。使用大型语言模型和分析社群内的话语，我们探测到了与饮食紊乱相关的潜在情况。 |
| [^49] | [Machine Translation Models are Zero-Shot Detectors of Translation Direction.](http://arxiv.org/abs/2401.06769) | 本文探索了一种基于无监督方法的翻译方向检测，并通过实验证实其在高负载语言对上的有效性。论文标题为“Machine Translation Models are Zero-Shot Detectors of Translation Direction”。 |
| [^50] | [Proximal Causal Inference With Text Data.](http://arxiv.org/abs/2401.06687) | 本论文提出了一种使用文本数据进行近因果推断的方法，通过将文本数据分割并使用零样本模型推断出代理变量，然后应用于近邻 g-formula，从而解决了混淆变量完全未观察到的情况。实验结果表明该方法产生了低偏差的估计值。 |
| [^51] | [TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models.](http://arxiv.org/abs/2401.06620) | 本论文提出了TransliCo，一个对比学习框架，用于解决多语言预训练语言模型中的脚本障碍。通过对比不同脚本的句子及其在统一脚本中的音译，实现了不同脚本的统一表示空间。实验证明，这种方法能够改善跨语言传递的性能。 |
| [^52] | [An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek.](http://arxiv.org/abs/2311.00541) | 本论文介绍了一个嵌入式历时语义变化模型（EDiSC），结合了词嵌入和DiSC模型，通过无监督学习分析古希腊文本中目标词汇的意义变化。实验证明EDiSC具有优越的性能。 |
| [^53] | [Managing AI Risks in an Era of Rapid Progress.](http://arxiv.org/abs/2310.17688) | 在人工智能快速进展的时代，我们提出了管理即将到来的先进人工智能系统所带来的风险的优先事项。 |
| [^54] | [In-Context Few-Shot Relation Extraction via Pre-Trained Language Models.](http://arxiv.org/abs/2310.11085) | 本研究提出了基于预训练语言模型的上下文少样本关系抽取框架，首次将关系抽取任务重新定义为定制的上下文少样本学习范式。与现有方法相比，该框架不需要命名实体识别和文档人工注释，并且可以轻松更新到新的关系集合。通过评估使用DocRED数据集，验证了该框架的有效性。 |
| [^55] | [CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models.](http://arxiv.org/abs/2310.08753) | CompA提出了由两个专家注释的音频-语言模型组合推理基准数据集，用于评估ALMs在理解音频中声音事件的顺序和属性绑定方面的表现。 |
| [^56] | [Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions.](http://arxiv.org/abs/2310.07301) | 本文提出了Parrot，一个高度可扩展的解决方案，通过自动生成高质量的指令调优数据，进一步完善了多轮聊天模型在对话中的效果。 |
| [^57] | [Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs.](http://arxiv.org/abs/2309.05516) | 本文提出一种名为SignRound的优化权重舍入的方法，通过使用有符号梯度进行轻量级分块调整，解决了大型语言模型(LLMs)的量化挑战。 |
| [^58] | [Large Language Models on Wikipedia-Style Survey Generation: an Evaluation in NLP Concepts.](http://arxiv.org/abs/2308.10410) | 本研究评估了大型语言模型在自然语言处理领域生成调研文章的效果，发现GPT-4优于GPT-3.5，并且指出了GPT在信息完整性和事实准确性方面的一些缺陷。 |
| [^59] | [RecycleGPT: An Autoregressive Language Model with Recyclable Module.](http://arxiv.org/abs/2308.03421) | RecycleGPT是一种生成式语言模型，通过回收预生成的模型状态而无需多次运行整个模型，并且证明了其降低推理延迟的有效性，实现高速解码和保持高性能。 |
| [^60] | [LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning.](http://arxiv.org/abs/2308.01413) | LaFiCMIL是一个新的方法，从相关多实例学习的角度解决了Transformer模型输入长度限制的问题，可以用于改进大文件分类任务。 |
| [^61] | [Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for Designing Effective Conversational Systems.](http://arxiv.org/abs/2307.07255) | 本文提供了一个对话代理设计的相关要素的综合概述，包括对话代理的主要特征、支持任务、数据集和评估方法。研究表明，构建单独的模型来处理不同的对话任务是昂贵且冗余的。 |
| [^62] | [Generating Efficient Training Data via LLM-based Attribute Manipulation.](http://arxiv.org/abs/2307.07099) | 本文提出了一种通过大型语言模型（LLMs）生成精心制作的训练数据来引导少样本学习的方法。通过利用LLMs操作任务特定属性并重构新的句子，我们实现了标签交换数据的生成，与其他基于LLMs的文本生成方法相比具有更好的效果。同时，研究结果还显示了通过LLM引导学习的潜力，即使在更少的监督情况下也能取得良好的表现。 |
| [^63] | [Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision.](http://arxiv.org/abs/2306.16564) | 本文介绍了一种Pareto Optimal自监督框架，利用可用的编程监督将大型语言模型(LLM)的响应进行系统校准，通过为每个响应生成风险评分，而无需额外的手动工作。 |
| [^64] | [Adapting Pretrained ASR Models to Low-resource Clinical Speech using Epistemic Uncertainty-based Data Selection.](http://arxiv.org/abs/2306.02105) | 本研究使用基于认识不确定性的数据选择方法来减少非洲口音临床ASR训练的注释成本。结果表明，这种方法可以超过现有的基准结果，并提高低资源口音的泛化能力。 |
| [^65] | [Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models.](http://arxiv.org/abs/2305.05973) | 提出使用差分隐私大语言模型合成查询的隐私保护推荐系统，可以安全有效地训练深度检索模型并提高检索质量。 |
| [^66] | [Deep Emotion Recognition in Textual Conversations: A Survey.](http://arxiv.org/abs/2211.09172) | 本调研针对对话中的情感识别进行了探讨，介绍了涉及此任务的挑战和机遇，以及描述了情感分类法和使用该分类法的基准数据集。调研总结了最重要的作品和所使用的深度学习架构，并提供了建议性的情感识别实践，以实现更好的框架。 |
| [^67] | [Probing for the Usage of Grammatical Number.](http://arxiv.org/abs/2204.08831) | 本文介绍了一种基于使用的探测设置，通过干预模型的表示来去除属性，从而发现模型实际使用的编码。以BERT如何编码语法数为例研究，结果显示BERT依赖于语法数的线性编码来产生正确的行为输出，并对名词和动词的语法数使用了不同的编码。 |

# 详细

[^1]: 测量大型语言模型的社会规范

    Measuring Social Norms of Large Language Models

    [https://arxiv.org/abs/2404.02491](https://arxiv.org/abs/2404.02491)

    论文提出了一个挑战，测试大型语言模型对社会规范的理解，构建了一个数据集涵盖广泛的社会规范问题，通过多代理框架基于大型语言模型来提高模型对社会规范的理解能力。

    

    我们提出了一个新的挑战，以检验大型语言模型是否理解社会规范。与现有数据集不同，我们的数据集要求具有解决社会规范的基本理解。我们的数据集包含最大的社会规范技能集，包括402项技能和12,383个问题，涵盖了从观点和论点到文化和法律等广泛的社会规范。我们根据K-12课程设计了我们的数据集。这使得可以直接将大型语言模型的社会理解能力与人类进行比较，更具体地说是与小学生进行比较。尽管先前的工作在我们的基准测试中产生几乎随机的准确度，但最近的大型语言模型（如GPT3.5-Turbo和LLaMA2-Chat）能够显著提高性能，仅略低于人类性能。然后，我们提出了一个基于大型语言模型的多代理框架，以提高模型理解社会规范的能力。

    arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
    
[^2]: MineLand：模拟具有有限多模感知和生理需求的大规模多智能体交互

    MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs

    [https://arxiv.org/abs/2403.19267](https://arxiv.org/abs/2403.19267)

    MineLand模拟器引入有限多模感知和生理需求，支持多智能体在协作中填补了信息和功能限制的空白，从而促进更具动态和有效性的多智能体交互。

    

    传统的多智能体仿真器通常假设拥有完美信息和无限功能，这限制了社会互动的生态有效性。我们提出了一个多智能体Minecraft模拟器MineLand，通过引入有限的多模感知和生理需求来弥合这一差距。我们的仿真器支持最多48个具有有限视觉、听觉和环境意识的智能体，迫使它们积极沟通和协作以满足食物和资源等生理需求。这促进了动态和有效的多智能体交互。我们进一步介绍了一个灵感来自多任务处理理论的AI智能体框架Alex，使智能体能够处理复杂的协调和调度。我们的实验表明，该模拟器、相应的基准测试和AI智能体框架有助于更具生态和细致的集体行为。MineLand和Alex的源代码可以在https://github.com/c中公开获取。

    arXiv:2403.19267v1 Announce Type: cross  Abstract: Conventional multi-agent simulators often assume perfect information and limitless capabilities, hindering the ecological validity of social interactions. We propose a multi-agent Minecraft simulator, MineLand, that bridges this gap by introducing limited multimodal senses and physical needs. Our simulator supports up to 48 agents with limited visual, auditory, and environmental awareness, forcing them to actively communicate and collaborate to fulfill physical needs like food and resources. This fosters dynamic and valid multi-agent interactions. We further introduce an AI agent framework, Alex, inspired by multitasking theory, enabling agents to handle intricate coordination and scheduling. Our experiments demonstrate that the simulator, the corresponding benchmark, and the AI agent framework contribute to more ecological and nuanced collective behavior. The source code of MineLand and Alex is openly available at https://github.com/c
    
[^3]: 通过简化不重要的层压缩大型语言模型

    Compressing Large Language Models by Streamlining the Unimportant Layer

    [https://arxiv.org/abs/2403.19135](https://arxiv.org/abs/2403.19135)

    通过观察大型语言模型中不同层对隐藏状态的影响程度，提出了LLM-Streamline方法，包括层剪枝和层替换，用于压缩模型并保持性能。

    

    大型语言模型(LLM)已广泛应用于各种自然语言任务和领域，但其适用性受到模型参数的限制。因此，越来越多的人关注表现出高性能的紧凑模型。在这项研究中，我们观察到LLM的不同层对隐藏状态有不同程度的扰动，这使我们能够识别出不那么重要的层。基于这一现象，我们提出了LLM-Streamline，包括两部分：层剪枝，根据目标稀疏度移除模型中一组连续的最不重要的层；层替换，训练一个轻量级模型来替换被剪枝的层，从而缓解由剪枝造成的性能下降。在实验中，我们利用了多层感知器(MLP)和一个transformer层等结构作为轻量级模型。

    arXiv:2403.19135v1 Announce Type: cross  Abstract: Large language models (LLM) have been extensively applied in various natural language tasks and domains, but their applicability is constrained by the large number of parameters of the models. Consequently, there is an increasing emphasis on compact models that exhibit high performance. In this study, we observe that different layers in LLM have varying degrees of perturbation on the hidden states, which allows us to identify less important layers. Based on this phenomenon, we propose LLM-Streamline, which consists of two parts: layer pruning, where we remove a set of consecutive layers with the lowest importance in the model according to the target sparsity; and layer replacement, where we train a lightweight model to substitute the pruned layers, thereby mitigating the performance degradation caused by pruning. In our experiments, we utilize structures such as a multi-layer perceptron (MLP) and a transformer layer as lightweight mode
    
[^4]: 多项选择题是否真的能够检测LLMs的能力？

    Can multiple-choice questions really be useful in detecting the abilities of LLMs?

    [https://arxiv.org/abs/2403.17752](https://arxiv.org/abs/2403.17752)

    多项选择题虽然被广泛用于评估大型语言模型，但在测试LLMs能力时存在一定局限性，特别是在需要长篇生成答案的情况下，我们发现LLMs在双语MCQs中表现出顺序敏感性。

    

    多项选择题(MCQs)由于其简单和高效而被广泛用于评估大型语言模型(LLMs)。然而，人们对于MCQs是否能真正衡量LLMs的能力存在疑虑，特别是在需要长篇生成(LFG)答案的知识密集型场景中。任务与评估方法之间的不匹配需要对MCQ的效用进行深入分析，而我们在本文中通过评估两种语言（中文和英文）的四个问答(QA)数据集上的九个LLMs来进行。我们发现一个重要问题：LLMs在双语MCQs中表现出一种顺序敏感性，偏向于位于特定位置的答案，即第一个位置。我们通过比较直接输出、token logit和嵌入来量化MCQs和长篇生成问题(LFGQs)之间的差距。我们的结果显示MCQs和长篇生成的答案之间存在相对较低的相关性。

    arXiv:2403.17752v1 Announce Type: new  Abstract: Multiple-choice questions (MCQs) are widely used in the evaluation of large language models (LLMs) due to their simplicity and efficiency. However, there are concerns about whether MCQs can truly measure LLM's capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required. The misalignment between the task and the evaluation method demands a thoughtful analysis of MCQ's efficacy, which we undertake in this paper by evaluating nine LLMs on four question-answering (QA) datasets in two languages: Chinese and English. We identify a significant issue: LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position. We further quantify the gap between MCQs and long-form generation questions (LFGQs) by comparing their direct outputs, token logits, and embeddings. Our results reveal a relatively low correlation between answers from MC
    
[^5]: LLaVA-PruMerge: 自适应令牌减少用于高效大型多模态模型

    LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models

    [https://arxiv.org/abs/2403.15388](https://arxiv.org/abs/2403.15388)

    PruMerge提出了一种自适应的视觉令牌减少方法，可以有效减少大型多模态模型中的视觉令牌数量，同时保持模型性能。

    

    大型多模态模型(LMMs)通过连接视觉编码器和大型语言模型展现了显著的推理能力。最近的LMMs包括了更复杂的视觉输入，如高分辨率图像和视频，这显著增加了视觉令牌的数量。为了解决这个问题，我们探索了一种令牌减少机制，并发现类似于先前的工作，许多视觉令牌在空间上是冗余的。基于此，我们提出了PruMerge，一种新颖的自适应视觉令牌减少方法，大大减少了视觉令牌的数量，同时保持了可比的模型性能。

    arXiv:2403.15388v1 Announce Type: cross  Abstract: Large Multimodal Models (LMMs) have shown significant reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically use a fixed amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which increase the number of visual tokens significantly. However, due to the design of the Transformer architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many visual tokens are spatially redundant. Based on this, we propose PruMerge, a novel adaptive visual token reduction approach, which largely reduces the number of visual tokens while maintaining comparable model performance. We first select 
    
[^6]: 一次编码，多次并行解码：高效Transformer解码

    Encode Once and Decode in Parallel: Efficient Transformer Decoding

    [https://arxiv.org/abs/2403.13112](https://arxiv.org/abs/2403.13112)

    提出了一种新的编码器-解码器模型配置，称为prompt-in-decoder（PiD），可以一次编码输入并并行解码输出，在结构化输出和问答任务中取得高效率，避免了重复输入编码，大幅减少了解码器的内存占用。

    

    基于Transformer的自然语言处理模型功能强大，但计算成本高，限制了部署场景。在专业领域中，微调的编码器-解码器模型备受青睐，可以胜过更大更通用的仅解码器模型，例如GPT-4。我们介绍了一种新的编码器-解码器模型配置，可以提高在结构化输出和问答任务中的效率，在这些任务中，需要从单个输入中产生多个输出。我们的方法，prompt-in-decoder（PiD），只对输入进行一次编码，并且并行解码输出，通过避免重复输入编码，从而减少解码器的内存占用，提升了训练和推断效率。我们实现了计算减少，大致随子任务数量增加而扩展，相比最先进模型，在对话状态追踪、摘要和问答任务中获得高达4.6倍的速度提升，并且性能相当或更好。我们发布了我们的训练/推断代码。

    arXiv:2403.13112v1 Announce Type: new  Abstract: Transformer-based NLP models are powerful but have high computational costs that limit deployment scenarios. Finetuned encoder-decoder models are popular in specialized domains and can outperform larger more generalized decoder-only models, such as GPT-4. We introduce a new configuration for encoder-decoder models that improves efficiency on structured output and question-answering tasks where multiple outputs are required of a single input. Our method, prompt-in-decoder (PiD), encodes the input once and decodes output in parallel, boosting both training and inference efficiency by avoiding duplicate input encoding, thereby reducing the decoder's memory footprint. We achieve computation reduction that roughly scales with the number of subtasks, gaining up to 4.6x speed-up over state-of-the-art models for dialogue state tracking, summarization, and question-answering tasks with comparable or better performance. We release our training/inf
    
[^7]: AutoTRIZ：利用TRIZ和大型语言模型的人工创意

    AutoTRIZ: Artificial Ideation with TRIZ and Large Language Models

    [https://arxiv.org/abs/2403.13002](https://arxiv.org/abs/2403.13002)

    本文提出了AutoTRIZ，一种利用大型语言模型自动化和增强TRIZ方法的人工创意工具，为设计自动化和可解释创意提供了一种新颖方法。

    

    研究人员和创新者在开发思维方法方面做出了巨大努力，比如形态分析和类比设计，以辅助工程设计创意，解决问题和推动创新。在这些方法中，TRIZ作为最著名的方法脱颖而出，被广泛应用于系统化创新。然而，TRIZ资源和概念的复杂性，以及其对用户知识、经验和推理能力的依赖，限制了其实用性。本文提出了AutoTRIZ，一种利用大型语言模型（LLMs）自动化和增强TRIZ方法的人工创意工具。通过利用LLMs的广泛知识和先进推理能力，AutoTRIZ提供了一种新颖的利用人工智能进行设计自动化和可解释创意的方法。我们通过对矛盾检测和比较方面的一致性实验来证明并评估AutoTRIZ的有效性。

    arXiv:2403.13002v1 Announce Type: cross  Abstract: Researchers and innovators have made enormous efforts in developing ideation methods, such as morphological analysis and design-by-analogy, to aid engineering design ideation for problem solving and innovation. Among these, TRIZ stands out as the most well-known approach, widely applied for systematic innovation. However, the complexity of TRIZ resources and concepts, coupled with its reliance on users' knowledge, experience, and reasoning capabilities, limits its practicability. This paper proposes AutoTRIZ, an artificial ideation tool that leverages large language models (LLMs) to automate and enhance the TRIZ methodology. By leveraging the broad knowledge and advanced reasoning capabilities of LLMs, AutoTRIZ offers a novel approach to design automation and interpretable ideation with artificial intelligence. We demonstrate and evaluate the effectiveness of AutoTRIZ through consistency experiments in contradiction detection and compa
    
[^8]: 具身LLM代理在组织团队中学会合作

    Embodied LLM Agents Learn to Cooperate in Organized Teams

    [https://arxiv.org/abs/2403.12482](https://arxiv.org/abs/2403.12482)

    本文介绍了一个框架，将即时性组织结构强加在LLM代理上，以促进多代理系统内的合作，在具身LLM代理和人-代理合作实验中发现指定领导对团队效率的影响，揭示了LLM代理的领导素质和自发合作行为。

    

    大型语言模型（LLMs）已成为推理、规划和决策的重要工具，利用其丰富的世界知识和语言相关任务的熟练度。LLMs因此在多代理系统内自然语言交互方面具有巨大潜力，以促进合作。然而，LLM代理往往会过度报告并遵从任何指令，这可能导致多代理合作中的信息冗余和混乱。受人类组织的启发，本文引入了一个框架，将即时性组织结构强加在LLM代理上，以缓解这些问题。通过一系列具身LLM代理和人-代理合作的实验，我们的结果突出了指定领导对团队效率的影响，揭示了LLM代理展示的领导素质和他们的自发合作行为。此外，我们利用LLMs的潜力

    arXiv:2403.12482v1 Announce Type: new  Abstract: Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs t
    
[^9]: 大型语言模型是对比推理者

    Large Language Models are Contrastive Reasoners

    [https://arxiv.org/abs/2403.08211](https://arxiv.org/abs/2403.08211)

    对比提示方法显著提高大型语言模型进行复杂推理的能力，不仅在算术、常识和符号推理任务上表现优良，还可以与现有提示方法整合，实现更好的性能。

    

    提示方法在增强预训练大型语言模型（LLMs）的能力方面发挥着至关重要的作用。我们探讨了对比提示（CP）如何显著提高大型语言模型执行复杂推理的能力。我们通过简单地在LLMs提供答案之前添加"让我们给出一个正确答案和一个错误答案"来演示LLMs是体面的对比推理者。对两个大型语言模型的实验表明，零迁移对比提示提升了在一系列算术、常识和符号推理任务上的表现，而不需要手工制作的少量迁移示例，比如使用最先进的GPT-4模型，提高了在GSM8K上的准确率从35.9%到88.8%以及AQUA-RAT从41.3%到62.2%。我们的方法不仅在大多数算术和常识推理任务中胜过零迁移CoT和少量迁移CoT，还可以与现有的提示方法无缝整合，从而实现改进或者竞争

    arXiv:2403.08211v1 Announce Type: cross  Abstract: Prompting methods play a crucial role in enhancing the capabilities of pre-trained large language models (LLMs). We explore how contrastive prompting (CP) significantly improves the ability of large language models to perform complex reasoning. We demonstrate that LLMs are decent contrastive reasoners by simply adding "Let's give a correct and a wrong answer." before LLMs provide answers. Experiments on two large language models show that zero-shot contrastive prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks without any hand-crafted few-shot examples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model. Our method not only surpasses zero-shot CoT and few-shot CoT in most arithmetic and commonsense reasoning tasks but also can seamlessly integrate with existing prompting methods, resulting in improved or comp
    
[^10]: 通过跨模态知识蒸馏控制预训练LLMs进行广义时间序列预测

    Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation

    [https://arxiv.org/abs/2403.07300](https://arxiv.org/abs/2403.07300)

    通过跨模态知识蒸馏和LLMs对齐框架，该方法利用静态和动态知识，充分释放LLMs在时间序列预测中的潜力

    

    多变量时间序列预测最近随着深度学习模型的快速增长取得了巨大成功。然而，现有方法通常使用有限的时间数据从头开始训练模型，阻碍了它们的泛化。最近，随着大语言模型（LLMs）的激增，一些工作尝试将LLMs引入时间序列预测中。尽管取得了有希望的结果，但这些方法直接将时间序列作为LLMs的输入，忽略了时间和文本数据之间固有的模态差距。在这项工作中，我们提出了一个新颖的大语言模型和时间序列对齐框架，称为LLaTA，以充分发挥LLMs在时间序列预测挑战中的潜力。基于跨模态知识蒸馏，所提出的方法利用了预训练LLMs中的输入无关静态知识和输入相关动态知识。通过这种方式，该方法为预测模型赋能

    arXiv:2403.07300v1 Announce Type: cross  Abstract: Multivariate time series forecasting has recently gained great success with the rapid growth of deep learning models. However, existing approaches usually train models from scratch using limited temporal data, preventing their generalization. Recently, with the surge of the Large Language Models (LLMs), several works have attempted to introduce LLMs into time series forecasting. Despite promising results, these methods directly take time series as the input to LLMs, ignoring the inherent modality gap between temporal and text data. In this work, we propose a novel Large Language Models and time series alignment framework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time series forecasting challenge. Based on cross-modal knowledge distillation, the proposed method exploits both input-agnostic static knowledge and input-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers the forecasting model with f
    
[^11]: 人工智能生成文本与人工智能协作混合文本中的检测方法

    Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid Texts

    [https://arxiv.org/abs/2403.03506](https://arxiv.org/abs/2403.03506)

    本研究探索了在人工智能协作混合文本中句子级人工智能生成文本检测的挑战，并提出了一种基于分割的两步骤流程来检测各段落的一致作者句子。

    

    本研究探讨了在人工智能协作混合文本中句子级人工智能生成文本检测的挑战。现有的关于混合文本中AI生成文本检测的研究通常依赖于合成数据集，这些数据集通常涉及带有有限边界的混合文本。我们认为，检测混合文本中AI生成内容的研究应覆盖在真实环境中生成的不同类型混合文本，以更好地指导实际应用。因此，我们的研究利用了CoAuthor数据集，该数据集包括通过人类作者和智能写作系统之间的协作生成的多轮交互中产生的多样化、真实的混合文本。我们采用了两步分割为基础的流程：(i)检测给定混合文本中的各个段落，其中每个段落包含一致作者的句子，以及(ii)分类每个确定段落的作者。我们的实证

    arXiv:2403.03506v1 Announce Type: cross  Abstract: This study explores the challenge of sentence-level AI-generated text detection within human-AI collaborative hybrid texts. Existing studies of AI-generated text detection for hybrid texts often rely on synthetic datasets. These typically involve hybrid texts with a limited number of boundaries. We contend that studies of detecting AI-generated content within hybrid texts should cover different types of hybrid texts generated in realistic settings to better inform real-world applications. Therefore, our study utilizes the CoAuthor dataset, which includes diverse, realistic hybrid texts generated through the collaboration between human writers and an intelligent writing system in multi-turn interactions. We adopt a two-step, segmentation-based pipeline: (i) detect segments within a given hybrid text where each segment contains sentences of consistent authorship, and (ii) classify the authorship of each identified segment. Our empirical 
    
[^12]: 因果引导：基于前门调整的大型语言模型启发式去偏方法

    Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment

    [https://arxiv.org/abs/2403.02738](https://arxiv.org/abs/2403.02738)

    提出了一种新型因果引导方法，通过前门调整有效减轻大型语言模型的偏见。

    

    尽管现有的诸如上下文学习和思维链等大型语言模型（LLMs）启发式方法取得了显著成就，但它们仍然面临各种偏见挑战。本文揭示了启发式方法背后的因果关系，并提出了一种基于前门调整的新型因果引导方法，以有效减轻LLMs的偏见。具体而言，通过设计提示而无需访问LLMs的参数和logit来实施因果干预。由LLMs生成的思维链被用作中介变量，通过前门计算输入提示与输出答案之间的因果效应。

    arXiv:2403.02738v1 Announce Type: new  Abstract: Despite the significant achievements of existing prompting methods such as in-context learning and chain-of-thought for large language models (LLMs), they still face challenges of various biases. Traditional debiasing methods primarily focus on the model training stage, including data augmentation-based and reweight-based approaches, with the limitations of addressing the complex biases of LLMs. To address such limitations, the causal relationship behind the prompting methods is uncovered using a structural causal model, and a novel causal prompting method based on front-door adjustment is proposed to effectively mitigate the bias of LLMs. In specific, causal intervention is implemented by designing the prompts without accessing the parameters and logits of LLMs.The chain-of-thoughts generated by LLMs are employed as the mediator variable and the causal effect between the input prompt and the output answers is calculated through front-do
    
[^13]: 大型语言模型的预测排名

    Prediction-Powered Ranking of Large Language Models

    [https://arxiv.org/abs/2402.17826](https://arxiv.org/abs/2402.17826)

    该研究提出了一种统计框架，可以衡量人类与模型偏好之间的不确定性，从而进行大型语言模型的预测排名。

    

    大型语言模型通常根据其与人类偏好的一致性水平进行排名--如果一个模型的输出更受人类偏好，那么它就比其他模型更好。本文提出了一种统计框架来弥合人类与模型偏好之间可能引入的不一致性。

    arXiv:2402.17826v1 Announce Type: cross  Abstract: Large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans. One of the most popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a small set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provid
    
[^14]: 测量神经模型的视觉语言STEM技能

    Measuring Vision-Language STEM Skills of Neural Models

    [https://arxiv.org/abs/2402.17205](https://arxiv.org/abs/2402.17205)

    该研究引入了一个新挑战，用于测试神经模型的STEM技能，提出了一个包含大量基础技能和问题的数据集，需要理解STEM的多模式视觉语言信息，并展示了最新模型对于低年级技能的有限掌握。

    

    我们引入了一个新挑战，用于测试神经模型的STEM技能。现实世界中的问题通常需要结合STEM（科学、技术、工程和数学）知识来解决。与现有数据集不同，我们的数据集需要理解STEM的多模式视觉语言信息。我们的数据集是挑战性问题中最大、最全面的数据集之一。它包括448项技能和1,073,146个跨越所有STEM科目的问题。与通常侧重于检验专家水平能力的现有数据集不同，我们的数据集包括基础技能和根据K-12课程设计的问题。我们还将最先进的基础模型，如CLIP和GPT-3.5-Turbo，添加到我们的基准中。结果显示，最近的模型进展只有助于掌握数据集中非常有限数量的低年级技能（三年级中的2.5%）。事实上，这些模型仍远没有完全掌握学前教育阶段的技能。

    arXiv:2402.17205v1 Announce Type: cross  Abstract: We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well bel
    
[^15]: GPT应用的初探：格局与脆弱性

    A First Look at GPT Apps: Landscape and Vulnerability

    [https://arxiv.org/abs/2402.15105](https://arxiv.org/abs/2402.15105)

    通过大规模监测和分析GPT商店，开发了自动化工具来研究GPT应用程序中的漏洞和抄袭情况。

    

    随着大型语言模型（LLMs）的进步，越来越复杂和强大的GPT进入市场。尽管它们很受欢迎，但LLM生态系统仍然尚未被探索。此外，LLMs对攻击的敏感性引发了对安全性和抄袭的担忧。因此，在这项工作中，我们对GPT商店进行了开创性的探索，旨在研究GPT应用程序中的漏洞和抄袭。首先，我们进行了据我们所知的第一次大规模监测和分析，分别是一个非官方的GPTStore.AI和一个官方的OpenAI GPT Store。然后，我们提出了一种TriLevel GPT Reversing（T-GR）策略，用于提取GPT内部信息。为了有效地完成这两项任务，我们开发了两个自动化工具：一个用于网络抓取，另一个设计用于与GPT进行程序化交互。我们的发现揭示了用户和开发者对GPT交互和创建的巨大热情，

    arXiv:2402.15105v1 Announce Type: cross  Abstract: With the advancement of Large Language Models (LLMs), increasingly sophisticated and powerful GPTs are entering the market. Despite their popularity, the LLM ecosystem still remains unexplored. Additionally, LLMs' susceptibility to attacks raises concerns over safety and plagiarism. Thus, in this work, we conduct a pioneering exploration of GPT stores, aiming to study vulnerabilities and plagiarism within GPT applications. To begin with, we conduct, to our knowledge, the first large-scale monitoring and analysis of two stores, an unofficial GPTStore.AI, and an official OpenAI GPT Store. Then, we propose a TriLevel GPT Reversing (T-GR) strategy for extracting GPT internals. To complete these two tasks efficiently, we develop two automated tools: one for web scraping and another designed for programmatically interacting with GPTs. Our findings reveal a significant enthusiasm among users and developers for GPT interaction and creation, as
    
[^16]: 大型语言模型作为城市居民：用于个人移动生成的LLM代理框架

    Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation

    [https://arxiv.org/abs/2402.14744](https://arxiv.org/abs/2402.14744)

    提出了一种将大型语言模型LLMs整合到代理框架中的新方法，用于生成个人移动生成，重点是解决将LLMs与真实城市流动数据对齐的问题，并提出了一种自洽方法和检索增强策略来实现可解释活动生成。

    

    本文介绍了一种新方法，将大型语言模型(LLMs)集成到代理框架中，用于灵活高效的个人移动生成。LLMs通过高效处理语义数据并在建模各种任务中提供多功能性, 克服了以往模型的局限性。我们的方法解决了将LLMs与真实世界城市流动数据对齐的迫切需求, 重点关注三个研究问题: 将LLMs与丰富的活动数据对齐, 开发可靠的活动生成策略, 以及探索LLMs在城市移动中的应用。其关键技术贡献是一种新颖的LLM代理框架, 该框架考虑了个体活动模式和动机, 包括将LLMs与真实世界活动数据对齐的自洽方法和可解释活动生成的检索增强策略。在实验研究中, 使用真实世界数据进行了全面验证。

    arXiv:2402.14744v1 Announce Type: new  Abstract: This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and efficient personal mobility generation. LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks. Our approach addresses the critical need to align LLMs with real-world urban mobility data, focusing on three research questions: aligning LLMs with rich activity data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. In experimental studies, comprehensive validation is performed using real-world data. This 
    
[^17]: KorNAT：韩国社会价值观和常识的LLM对齐基准

    KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge

    [https://arxiv.org/abs/2402.13605](https://arxiv.org/abs/2402.13605)

    KorNAT是首个用于评估韩国国家对齐的基准，包括社会价值观和常识对齐两个方面，通过对社会价值和常识多项选择题的测试来评估模型的对齐程度。

    

    对于大型语言模型（LLMs）在特定国家得以有效部署，它们必须具有对该国文化和基本知识的理解。为此，我们引入了国家对齐（National Alignment），从社会价值观对齐和常识对齐两个方面衡量LLM与目标国家之间的对齐。社会价值观对齐评估模型对特定国家社会价值观的理解程度，而常识对齐则检验模型对相关基本国家知识的把握情况。我们构建了KorNAT，这是首个衡量与韩国国家对齐的基准。对于社会价值数据集，我们从包括6174名韩国参与者在内的大规模调查中获得了地面真实标签。对于常识数据集，我们基于韩国教科书和GED参考资料构建了样本。KorNAT包含4K和6K个针对社会价值和常识的多项选择题。

    arXiv:2402.13605v1 Announce Type: new  Abstract: For Large Language Models (LLMs) to be effectively deployed in a specific country, they must possess an understanding of the nation's culture and basic knowledge. To this end, we introduce National Alignment, which measures an alignment between an LLM and a targeted country from two aspects: social value alignment and common knowledge alignment. Social value alignment evaluates how well the model understands nation-specific social values, while common knowledge alignment examines how well the model captures basic knowledge related to the nation. We constructed KorNAT, the first benchmark that measures national alignment with South Korea. For the social value dataset, we obtained ground truth labels from a large-scale survey involving 6,174 unique Korean participants. For the common knowledge dataset, we constructed samples based on Korean textbooks and GED reference materials. KorNAT contains 4K and 6K multiple-choice questions for socia
    
[^18]: 一个统一的基于分类学指导的实体集扩展和分类学扩展的指导调整框架

    A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion

    [https://arxiv.org/abs/2402.13405](https://arxiv.org/abs/2402.13405)

    通过统一的基于分类学指导的指导调整框架，本文提出了一种利用现有分类学进行实体关系微调的方法，有效解决实体集扩展、分类学扩展和种子引导分类学构建三个任务。

    

    实体集扩展、分类学扩展和种子引导分类学构建是三个代表性任务，可以用来自动向现有分类学填充新实体。然而，先前的方法通常使用异质技术分别解决这些任务，缺乏统一的视角。为了解决这个问题，在本文中，我们从分类学结构的视角确认了这些任务所需的共同关键技能——找到“兄弟”和找到“父母”，并提出了一个统一的基于分类学指导的指导调整框架来共同解决这三个任务。具体来说，通过利用现有分类学作为丰富的实体关系源，我们利用指导调整来微调大型语言模型，生成父母和兄弟实体。在多个基准数据集上的大量实验证明了TaxoInstruct的有效性，该方法在各项指标上均优于特定任务的基线方法。

    arXiv:2402.13405v1 Announce Type: new  Abstract: Entity Set Expansion, Taxonomy Expansion, and Seed-Guided Taxonomy Construction are three representative tasks that can be used to automatically populate an existing taxonomy with new entities. However, previous approaches often address these tasks separately with heterogeneous techniques, lacking a unified perspective. To tackle this issue, in this paper, we identify the common key skills needed for these tasks from the view of taxonomy structures -- finding 'siblings' and finding 'parents' -- and propose a unified taxonomy-guided instruction tuning framework to jointly solve the three tasks. To be specific, by leveraging the existing taxonomy as a rich source of entity relationships, we utilize instruction tuning to fine-tune a large language model to generate parent and sibling entities. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of TaxoInstruct, which outperforms task-specific baselines across 
    
[^19]: 无监督LLM适应问答任务

    Unsupervised LLM Adaptation for Question Answering

    [https://arxiv.org/abs/2402.12170](https://arxiv.org/abs/2402.12170)

    提出了无监督LLM适应问答任务，通过利用预训练的LLM和目标领域的未标记文档，实现在新领域回答问题的目标。

    

    大型语言模型（LLM）通过自监督训练学习大规模训练数据集中的多样化知识。接着通过指导微调，LLM能够返回多样问题的正确信息。然而，将这些预训练的LLM调整到新的目标领域，如不同组织或时期，用于问答任务会产生很高的注释成本。为解决这一挑战，我们提出了一个新颖的任务，即无监督LLM适应问答任务。在这个任务中，我们利用预训练的LLM、一个公开可用的问答数据集（源数据）和目标域的未标记文档。我们的目标是学习LLM，使其能够回答关于目标领域的问题。我们引入了一个合成数据集和两个真实数据集来评估在源数据和目标数据上微调的模型，并揭示了一些有趣的见解；（i）微调模型展示了提供正确答案的能力

    arXiv:2402.12170v1 Announce Type: cross  Abstract: Large language models (LLM) learn diverse knowledge present in the large-scale training dataset via self-supervised training. Followed by instruction-tuning, LLM acquires the ability to return correct information for diverse questions. However, adapting these pre-trained LLMs to new target domains, such as different organizations or periods, for the question-answering (QA) task incurs a substantial annotation cost. To tackle this challenge, we propose a novel task, unsupervised LLM adaptation for question answering. In this task, we leverage a pre-trained LLM, a publicly available QA dataset (source data), and unlabeled documents from the target domain. Our goal is to learn LLM that can answer questions about the target domain. We introduce one synthetic and two real datasets to evaluate models fine-tuned on the source and target data, and reveal intriguing insights; (i) fine-tuned models exhibit the ability to provide correct answers 
    
[^20]: 论文标题: 计划是什么？评估和开发针对LLMs的计划意识技术

    What's the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs

    [https://arxiv.org/abs/2402.11489](https://arxiv.org/abs/2402.11489)

    提出了一种新的混合方法SimPlan，结合了LLMs和经典规划方法，在各种规划领域的实验表明SimPlan明显优于现有的基于LLM的规划者

    

    arXiv:2402.11489v1 公告类型: 新摘要: 计划是人工智能中的基本任务，涉及在给定环境中找到实现特定目标的一系列行动。 大型语言模型（LLMs）越来越多地用于需要计划能力的应用，如网络或具体代理。 与最近的研究一致，我们通过实验表明LLMs缺乏计划所需的必要技能。 基于这些观察结果，我们倡导一种将LLMs与经典计划方法结合的混合方法的潜力。 然后，我们介绍了SimPlan，一种新颖的混合方法，并评估其在新的具有挑战性的设置中的表现。 我们在各种规划领域进行的大量实验证明，SimPlan明显优于现有基于LLM的规划者。

    arXiv:2402.11489v1 Announce Type: new  Abstract: Planning is a fundamental task in artificial intelligence that involves finding a sequence of actions that achieve a specified goal in a given environment. Large language models (LLMs) are increasingly used for applications that require planning capabilities, such as web or embodied agents. In line with recent studies, we demonstrate through experimentation that LLMs lack necessary skills required for planning. Based on these observations, we advocate for the potential of a hybrid approach that combines LLMs with classical planning methodology. Then, we introduce SimPlan, a novel hybrid-method, and evaluate its performance in a new challenging setup. Our extensive experiments across various planning domains demonstrate that SimPlan significantly outperforms existing LLM-based planners.
    
[^21]: 能够与规则进行推理吗？逻辑支架用于压力测试和提升LLM

    Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs

    [https://arxiv.org/abs/2402.11442](https://arxiv.org/abs/2402.11442)

    提出了一个逻辑支架推理规则生成框架，通过构建包含基础和组合规则的推理规则库ULogic，揭示了LLMs在逻辑理解方面与人类表现的显著差距，并通过生成准确、复杂和抽象的推理结果来提升下游推理。

    

    大型语言模型(LLMs)在各种推理任务中取得了令人印象深刻的接近人类表现的成绩。然而，它们对于基础推理规则的掌握仍然不及人类能力。为了研究这一问题，我们提出了一个逻辑支架推理规则生成框架，构建了一个包含五个领域中基础和组合规则的推理规则库ULogic。我们对GPT系列模型在规则子集上的分析揭示出LLMs在逻辑理解方面与人类表现存在显著差距，特别是在具有某些偏见模式的组合和结构复杂规则中。我们进一步将这些规则提炼成一个更小规模的推理引擎，用于灵活地生成规则并增强下游推理。通过多评估人员评估，我们的推理引擎证明在生成准确、复杂和抽象的结论和前提方面表现出效果，可以改善各种常识推理。

    arXiv:2402.11442v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive human-like performance across various reasoning tasks. However, their mastery of underlying inferential rules still falls short of human capabilities. To investigate this, we propose a logic scaffolding inferential rule generation framework, to construct an inferential rule base, ULogic, comprising both primitive and compositional rules across five domains. Our analysis of GPT-series models over a rule subset reveals significant gaps in LLMs' logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns. We further distill these rules into a smaller-scale inference engine for flexible rule generation and enhancing downstream reasoning. Through a multi-judger evaluation, our inference engine proves effective in generating accurate, complex and abstract conclusions and premises, and improve various commonsense reas
    
[^22]: OneBit:朝着极低比特大型语言模型迈进

    OneBit: Towards Extremely Low-bit Large Language Models

    [https://arxiv.org/abs/2402.11295](https://arxiv.org/abs/2402.11295)

    本文提出了一种名为OneBit的1位量化感知训练框架，可以将大型语言模型的权重矩阵量化为1位，为极低比特宽度的LLMs部署铺平了道路。

    

    模型量化使用低比特宽度值来表示模型的权重矩阵，这是减少部署高度期待的LLMs的存储和计算开销的一种有前途的方法。然而，现有的量化方法在比特宽度极小时性能严重下降，因此专注于利用4位或8位值来量化模型。本文大胆地将LLMs的权重矩阵量化为1位，为LLMs的极低比特宽度部署铺平了道路。为此，我们引入了一个名为OneBit的1位量化感知训练（QAT）框架，其中包括一种更好地量化LLMs的新颖的1位参数表示方法，以及基于矩阵分解的有效参数初始化方法，以提高QAT框架的收敛速度。充分的实验结果表明，OneBit取得了良好的性能（至少是非

    arXiv:2402.11295v1 Announce Type: new  Abstract: Model quantification uses low bit-width values to represent the weight matrices of models, which is a promising approach to reduce both storage and computational overheads of deploying highly anticipated LLMs. However, existing quantization methods suffer severe performance degradation when the bit-width is extremely reduced, and thus focus on utilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes the weight matrices of LLMs to 1-bit, paving the way for the extremely low bit-width deployment of LLMs. For this target, we introduce a 1-bit quantization-aware training (QAT) framework named OneBit, including a novel 1-bit parameter representation method to better quantize LLMs as well as an effective parameter initialization method based on matrix decomposition to improve the convergence speed of the QAT framework. Sufficient experimental results indicate that OneBit achieves good performance (at least 83% of the non
    
[^23]: AFaCTA: 使用可靠的LLM标注者辅助事实性索赔检测的标注

    AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators

    [https://arxiv.org/abs/2402.11073](https://arxiv.org/abs/2402.11073)

    提出了一种新框架 AFaCTA，利用大型语言模型辅助事实性索赔的标注，提高了标注的效率和一致性。

    

    随着生成式人工智能的兴起，用于打击误导信息的自动事实核查方法变得越来越重要。然而，事实性索赔检测，即事实核查管道中的第一步，存在两个关键问题限制了其可伸缩性和泛化性：（1）任务定义和索赔概念的不一致性以及（2）手动标注的高成本。为了解决（1），我们审查了相关工作中的定义，并提出了一个聚焦于可验证性的事实性索赔的统一定义。为了解决（2），我们引入了AFaCTA（自动事实性索赔检测标注器），这是一个新颖的框架，利用大型语言模型（LLMs）在事实性索赔的标注中提供帮助。AFaCTA通过沿着三条预定义的推理路径保持一致性来校准其注释的置信度。在政治言论领域的大量评估和实验表明，AFaCTA能够高效地协助专业人员进行标注。

    arXiv:2402.11073v1 Announce Type: cross  Abstract: With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist exper
    
[^24]: SPAR：通过长期参与注意力实现个性化基于内容的推荐

    SPAR: Personalized Content-Based Recommendation via Long Engagement Attention

    [https://arxiv.org/abs/2402.10555](https://arxiv.org/abs/2402.10555)

    SPAR是一个基于内容的推荐框架，通过利用PLM、多注意力层和注意力稀疏机制，在会话级别有效地处理长期用户参与历史，提取全面用户兴趣，实现个性化推荐。

    

    利用用户长期参与历史对个性化内容推荐至关重要。预训练语言模型（PLMs）在自然语言处理领域的成功导致它们被用于编码用户历史和候选项，将内容推荐视为文本语义匹配任务。然而，现有工作仍然在处理非常长的用户历史文本和不足的用户-物品交互方面存在困难。本文介绍了一种基于内容的推荐框架SPAR，有效应对了从长期用户参与历史中提取全面用户兴趣的挑战。它通过利用PLM、多注意力层和注意力稀疏机制以会话为基础对用户的历史进行编码。用户和物品侧特征被充分融合进行参与预测，同时保持双方的独立表示，这对于实际模型部署是有效的。

    arXiv:2402.10555v1 Announce Type: cross  Abstract: Leveraging users' long engagement histories is essential for personalized content recommendations. The success of pretrained language models (PLMs) in NLP has led to their use in encoding user histories and candidate items, framing content recommendations as textual semantic matching tasks. However, existing works still struggle with processing very long user historical text and insufficient user-item interaction. In this paper, we introduce a content-based recommendation framework, SPAR, which effectively tackles the challenges of holistic user interest extraction from the long user engagement history. It achieves so by leveraging PLM, poly-attention layers and attention sparsity mechanisms to encode user's history in a session-based manner. The user and item side features are sufficiently fused for engagement prediction while maintaining standalone representations for both sides, which is efficient for practical model deployment. Mor
    
[^25]: MUSTARD：掌握定理和证明数据的统一合成

    MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data

    [https://arxiv.org/abs/2402.08957](https://arxiv.org/abs/2402.08957)

    这项工作介绍了MUSTARD，一种掌握定理和证明数据统一合成的数据生成框架，通过三个阶段的合成，实现了高质量和多样化的问题和推理步骤的生成。

    

    最近，大型语言模型（LLMs）在各种任务中取得了显著进展，包括数学推理和定理证明。由于这两个任务需要严格和形式化的多步推理，它们是探索LLMs推理能力的吸引领域，但仍面临重要挑战。以前的研究如Chain-of-Thought（CoT）揭示了中间步骤指导的有效性。然而，这种逐步注释需要大量的劳动力，导致当前基准测试的训练步骤不足。为了填补这一空白，本研究引入了MUSTARD，一种数据生成框架，可以主导高质量和多样化的定理和证明数据的统一合成。MUSTARD通过三个阶段合成数据：（1）它随机选择几个数学概念作为问题的类别。（2）然后，它使用选定的概念提示生成性语言模型，以获得问题和它们的推理步骤。

    arXiv:2402.08957v1 Announce Type: new Abstract: Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks. To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-w
    
[^26]: SemRel2024: 14种语言的语义文本相关性数据集合

    SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages

    [https://arxiv.org/abs/2402.08638](https://arxiv.org/abs/2402.08638)

    SemRel2024是一个包含来自14种语言的语义文本相关性数据集合，通过该数据集合可以探索和量化语义相关性。这对于大型语言模型的能力和性能有重要的影响。这个数据集合涵盖了南非荷兰语、阿尔及利亚阿拉伯语、阿姆哈拉语、英语、豪萨语、印地语、印度尼西亚语、卢旺达语、马拉地语、摩洛哥阿拉伯语、现代标准阿拉伯语、旁遮普语、西班牙语和泰卢固语等14种语言。

    

    探索和量化语义相关性是语言表达的核心。它在各种自然语言处理任务中具有重要影响，包括为大型语言模型（LLM）的能力和性能提供洞察。虽然早期的自然语言处理研究主要集中在语义相似性上，往往是在英语语境中，但我们认为更广泛的语义相关性现象值得研究。本文介绍了SemRel，这是一个由母语为14种语言进行注释的新的语义相关性数据集合：南非荷兰语、阿尔及利亚阿拉伯语、阿姆哈拉语、英语、豪萨语、印地语、印度尼西亚语、卢旺达语、马拉地语、摩洛哥阿拉伯语、现代标准阿拉伯语、旁遮普语、西班牙语和泰卢固语。这些语言来自五个不同的语系，主要在非洲和亚洲使用，这些地区的自然语言处理资源相对较少。SemRel数据集中的每个实例都是与一个表示相关性得分的句子对相关联。

    Exploring and quantifying semantic relatedness is central to representing language. It holds significant implications across various NLP tasks, including offering insights into the capabilities and performance of Large Language Models (LLMs). While earlier NLP research primarily focused on semantic similarity, often within the English language context, we instead investigate the broader phenomenon of semantic relatedness. In this paper, we present SemRel, a new semantic relatedness dataset collection annotated by native speakers across 14 languages:Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by a relatively limited availability of NLP resources. Each instance in the SemRel datasets is a sentence pair associated with a score that repr
    
[^27]: UFO: 一个专注于Windows操作系统交互的用户界面智能体

    UFO: A UI-Focused Agent for Windows OS Interaction

    [https://arxiv.org/abs/2402.07939](https://arxiv.org/abs/2402.07939)

    UFO是一个专注于Windows操作系统上应用程序的用户界面智能体，利用GPT-Vision的能力来满足用户需求。它通过观察和分析Windows应用程序的图形用户界面和控制信息，实现无缝导航和操作以满足用户的请求。UFO的控制交互模块使得无需人工干预即可实现动作连接和完全自动化执行，使繁琐和耗时的过程变为简单任务。经过测试，UFO在各种场景中取得了良好效果。

    

    我们介绍了UFO，一个创新的专注于Windows操作系统上应用程序的用户界面智能体，利用了GPT-Vision的能力来满足用户需求。UFO采用双智能体框架，精确观察和分析Windows应用程序的图形用户界面（GUI）和控制信息。这使得智能体可以无缝地在单个应用程序内以及跨应用程序进行导航和操作，以满足用户的需求，即使涉及多个应用程序。该框架包括一个控制交互模块，实现无需人工干预的动作连接，并实现完全自动化执行。因此，UFO将艰巨而耗时的过程转变为仅通过自然语言命令就可以完成的简单任务。我们在9个流行的Windows应用程序上对UFO进行了测试，涵盖了反映用户日常使用情景的各种情况。通过定量指标和真实案例研究得出的结果强调了UFO的效果。

    We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users' daily usage. The results, derived from both quantitative metrics and real-case studies, underscore t
    
[^28]: 结合空间优化和大型语言模型的开放领域城市行程规划

    Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning

    [https://arxiv.org/abs/2402.07204](https://arxiv.org/abs/2402.07204)

    本文提出了Open-domain Urban Itinerary Planning (OUIP)任务，用于根据用户以自然语言描述的请求直接生成行程，通过结合空间优化和大型语言模型(LLM)，提供个性化的城市行程定制服务。

    

    本文首次提出了Open-domain Urban Itinerary Planning (OUIP)任务，用于根据用户以自然语言描述的请求直接生成行程。OUIP与传统行程规划不同，传统规划限制了用户表达更详细的需求，阻碍了真正的个性化。最近，大型语言模型(LLM)在处理多样化任务方面表现出潜力。然而，由于非实时信息、不完整的知识和不足的空间意识，它们无法独立地提供满意的用户体验。鉴于此，我们提出了一个名为ItiNera的OUIP系统，将空间优化与大型语言模型(LLM)相结合，根据用户需求提供个性化的城市行程定制服务。具体来说，我们开发了一个基于LLM的流水线，用于提取和更新兴趣点特征，以创建用户自己的个性化兴趣点数据库。对于每个用户请求，我们利用LLM进行协同实现优化。

    In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in coop
    
[^29]: L4Q: 通过基于LoRA的量化训练在大型语言模型上提供参数高效的量化训练

    L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ

    [https://arxiv.org/abs/2402.04902](https://arxiv.org/abs/2402.04902)

    L4Q是一种参数高效的量化感知训练算法，通过基于LoRA的学习的量化步长，解决了大型语言模型中量化训练的挑战。

    

    后训练量化(PTQ)和量化感知训练(QAT)方法正在流行起来，以缓解大型语言模型(LLMs)所带来的高内存和计算成本。在资源受限的情况下，尽管后者具有更高的准确性潜力，但由于其减少的训练开销，通常首选后训练量化。同时，介绍了参数高效微调方法，如低秩适应（LoRA），并最近的工作已经探索了量化感知参数高效微调技术。然而，这些方法可能缺乏通用性，因为它们依赖于预量化模型的配置。由非线性量化或混合精度权重引起的效果可能会受到影响，并且重新训练特定量化参数可能会影响最优性能。为了应对这些挑战，我们提出了L4Q，一种参数高效的量化感知训练算法。L4Q利用了基于LoRA的学习的量化步长。

    Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs). In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter's potential for higher accuracy. Meanwhile, parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored quantization-aware PEFT techniques. However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration. Their effectiveness may be compromised by non-linearly quantized or mixed-precision weights, and the retraining of specific quantization parameters might impede optimal performance. To address these challenges, we propose L4Q, an algorithm for parameter-efficient quantization-aware training. L4Q leverages LoRA-wise learned quantization step size 
    
[^30]: CogCoM: 通过一系列的操作训练大规模视觉语言模型，并深入细节

    CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations

    [https://arxiv.org/abs/2402.04236](https://arxiv.org/abs/2402.04236)

    本文介绍了CogCoM，一个具备操作链机制的大规模视觉语言模型，通过一系列操作解决视觉问题，并以其证据性的视觉推理能力实现忠实的响应。

    

    视觉语言模型（VLM）通过广泛的训练，在将视觉指令与答案对齐方面展示了广泛的可行性。然而，这种确定性的对齐导致模型忽视了关键的视觉推理，并导致在细致的视觉问题和不忠实的响应方面失败。在本文中，我们提出了一种称为“操作链”的机制，使VLM能够通过一系列的操作来解决问题，其中每个操作都指的是对视觉输入的操作，可以是通过先前训练获得的内在能力（例如，基础）或者是模仿类人行为（例如，放大）。这个机制鼓励VLM生成带有证据的视觉推理的忠实的响应，并允许用户在可解释的路径上追踪错误的原因。因此，我们训练了CogCoM，一个具有内置推理机制的17B通用VLM。实验证明，我们的模型达到了最先进的水平。

    Vision-Language Models (VLMs) have demonstrated their widespread viability thanks to extensive training in aligning visual instructions to answers. However, this conclusive alignment leads models to ignore critical visual reasoning, and further result in failures on meticulous visual problems and unfaithful responses. In this paper, we propose Chain of Manipulations, a mechanism that enables VLMs to solve problems with a series of manipulations, where each manipulation refers to an operation on the visual input, either from intrinsic abilities (e.g., grounding) acquired through prior training or from imitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs to generate faithful responses with evidential visual reasoning, and permits users to trace error causes in the interpretable paths. We thus train CogCoM, a general 17B VLM with a memory-based compatible architecture endowed this reasoning mechanism. Experiments show that our model achieves the state-of-the-art 
    
[^31]: 在明确无偏大型语言模型中测量隐性偏见

    Measuring Implicit Bias in Explicitly Unbiased Large Language Models

    [https://arxiv.org/abs/2402.04105](https://arxiv.org/abs/2402.04105)

    通过引入受心理学启发的两个偏见测量方法，我们在明确无偏大型语言模型中发现了普遍存在的人类化的刻板印象偏见，并与实际决策中的隐性偏见相关。

    

    大型语言模型（LLMs）能够通过明确的偏见测试，但仍然可能存在隐性偏见，类似于持有平等主义信念的人们却表现出微妙的偏见。测量这种隐性偏见是一项挑战：随着LLMs变得越来越专有，可能无法访问它们的嵌入，并应用现有的偏见测量方法；此外，隐性偏见主要是一个问题，如果它们影响了这些系统所做的实际决策。我们通过引入受心理学启发的两个偏见测量方法来应对这两个挑战：LLMs隐含联想测试（IAT）偏见是一种基于提示的测量隐性偏见的方法；LLMs决策偏见用于检测决策任务中的微妙歧视。使用这些测量方法，我们发现了6个LLMs在4个社会领域（种族、性别、宗教、健康）和21个类别（武器、罪罚、科学、职业等）中普遍存在人类化的刻板印象偏见。我们的基于提示的隐性偏见测量方法与实际决策中的隐性偏见相关。

    Large language models (LLMs) can pass explicit bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: as LLMs become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make. We address both of these challenges by introducing two measures of bias inspired by psychology: LLM Implicit Association Test (IAT) Bias, which is a prompt-based method for revealing implicit bias; and LLM Decision Bias for detecting subtle discrimination in decision-making tasks. Using these measures, we found pervasive human-like stereotype biases in 6 LLMs across 4 social domains (race, gender, religion, health) and 21 categories (weapons, guilt, science, career among others). Our prompt-based measure of implicit bias correlates wit
    
[^32]: DefInt：一种用于高效处理混合大型语言模型推理的默认干预框架

    DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models

    [https://arxiv.org/abs/2402.02563](https://arxiv.org/abs/2402.02563)

    DefInt提出了一种默认干预框架，通过默认使用较小规模的语言模型生成推理思路，然后通过反思推理干预解决复杂推理问题，从而提高混合大型语言模型的效率和准确性。

    

    大型语言模型（LLMs）在各种任务中展示出令人印象深刻的新能力，但在处理复杂推理问题方面仍面临挑战。以往的研究如连锁推理（CoT）和思维树（ToT）主要关注提高准确性，但忽视了不断增加的标记成本，这对于具有巨大解空间的开放性实际任务来说可能特别问题。受人类认知的双过程理论的启发，我们提出了一种默认干预框架（DefInt），以释放混合LLMs的协同潜力。默认情况下，DefInt使用较小规模的语言模型生成低成本的推理思路，类似于“系统1”产生的快速直觉。如果这些直觉被认为低置信度，则DefInt将调用放大的语言模型的反思推理作为“系统2”的干预，可以覆盖默认思考并纠正推理过程。实验在五个实际数据集上展示了DefInt论文中的有效性。

    Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on fiv
    
[^33]: AutoTimes: 基于大型语言模型的自回归时间序列预测器

    AutoTimes: Autoregressive Time Series Forecasters via Large Language Models

    [https://arxiv.org/abs/2402.02370](https://arxiv.org/abs/2402.02370)

    AutoTimes是一种基于大型语言模型的自回归时间序列预测器，利用语言模型的转换能力来处理时间序列数据，实现了与先前模型相当的性能。

    

    由于大规模时间序列的有限可用性和可扩展预训练的不充分探索，时间序列的基础模型尚未完全发展。基于时间序列和自然语言的相似顺序结构，越来越多的研究证明了利用大型语言模型(LLM)进行时间序列的可行性。然而，先前的方法可能忽视了时间序列和自然语言对齐的一致性，导致对LLM潜力的利用不足。为了充分利用从语言建模中学到的通用令牌转换，我们提出了AutoTimes，将LLM重新用作自回归时间序列预测器，这与LLM的获取和利用一致，而无需更新参数。由此产生的预测器可以处理灵活的系列长度，并实现与流行模型相当的性能。此外，我们提出了基于令牌的提示方法，利用相应的时间戳来进行预测。

    Foundation models of time series have not been fully developed due to the limited availability of large-scale time series and the underexploration of scalable pre-training. Based on the similar sequential structure of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, prior methods may overlook the consistency in aligning time series and natural language, resulting in insufficient utilization of the LLM potentials. To fully exploit the general-purpose token transitions learned from language modeling, we propose AutoTimes to repurpose LLMs as Autoregressive Time series forecasters, which is consistent with the acquisition and utilization of LLMs without updating the parameters. The consequent forecasters can handle flexible series lengths and achieve competitive performance as prevalent models. Further, we present token-wise prompting that utilizes corresponding timestamps to make ou
    
[^34]: 超越极限：扩展大型语言模型中上下文长度的技术综述

    Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models

    [https://arxiv.org/abs/2402.02244](https://arxiv.org/abs/2402.02244)

    这篇论文综述了近期为扩展大型语言模型中上下文长度而设计的技术和方法，并回顾了包括架构修改在内的多种技术，使得语言模型可以更有效地理解长上下文。

    

    近期，大型语言模型（LLMs）展现出了令人惊异的能力，包括理解上下文、进行逻辑推理和生成响应。然而，这是以严格的计算和内存要求为代价的，限制了它们有效支持长输入序列的能力。本综述全面回顾了最近为扩展LLMs序列长度而设计的技术和方法，从而增强其对长上下文理解的能力。具体而言，我们回顾和分类了各种技术，包括修改位置编码和修改注意机制等架构修改，旨在增强对更长序列的处理，同时避免计算需求的成比例增加。本研究探讨的多样方法可以在LLMs的不同阶段（即训练、微调和推理）中利用。这使得LLMs可以有效地处理长序列并提升对长上下文的理解能力。

    Recently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational requirements. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to effic
    
[^35]: Panacea: 通过偏好适应实现 Pareto 对齐的 LLMS

    Panacea: Pareto Alignment via Preference Adaptation for LLMs

    [https://arxiv.org/abs/2402.02030](https://arxiv.org/abs/2402.02030)

    Panacea 是一种创新方法，将大型语言模型对齐重新定义为多维偏好优化问题，通过使用奇异值分解的低秩适应，以在线注入偏好向量的形式，使模型能够适应并 Pareto 最优地满足各种偏好集。

    

    当前的大型语言模型对齐方法通常使用标量人类偏好标签。然而，这种约定倾向于过度简化人类偏好的多维和异质性特性，导致表达能力降低甚至失配。本文提出了一种创新的方法 Panacea，将对齐重新定义为多维偏好优化问题。Panacea 训练了一个单一模型，能够在线适应并 Pareto 最优地满足各种偏好集，而无需进一步的调整。一个主要挑战是使用低维偏好向量来引导模型的行为，尽管模型由数量庞大的参数所控制。为了解决这个问题，Panacea 被设计为使用基于奇异值分解（SVD）的低秩适应，可以将偏好向量作为奇异值简单在线注入。从理论上讲，我们证明了 Panacea 能够恢复整个 Pareto 前沿与常见损失聚合。

    Current methods for large language model alignment typically use scalar human preference labels. However, this convention tends to oversimplify the multi-dimensional and heterogeneous nature of human preferences, leading to reduced expressivity and even misalignment. This paper presents Panacea, an innovative approach that reframes alignment as a multi-dimensional preference optimization problem. Panacea trains a single model capable of adapting online and Pareto-optimally to diverse sets of preferences without the need for further tuning. A major challenge here is using a low-dimensional preference vector to guide the model's behavior, despite it being governed by an overwhelmingly large number of parameters. To address this, Panacea is designed to use singular value decomposition (SVD)-based low-rank adaptation, which allows the preference vector to be simply injected online as singular values. Theoretically, we prove that Panacea recovers the entire Pareto front with common loss agg
    
[^36]: LiPO: 通过学习排序进行列表型偏好优化

    LiPO: Listwise Preference Optimization through Learning-to-Rank

    [https://arxiv.org/abs/2402.01878](https://arxiv.org/abs/2402.01878)

    本研究提出了一种名为LiPO的框架，用于将语言模型对齐问题定义为一个列表型排序问题。通过从排名列表中学习，该框架可以使策略更有效地学习到可行的响应。

    

    将语言模型与人工反馈进行对齐是控制其在实际应用中行为的关键。最近的一些策略优化方法，如DPO和SLiC，成为传统的来自人类反馈的增强学习方法的有希望的替代方案。实际上，人工反馈通常以对多个响应进行排序的格式提供，以摊销阅读提示的成本。多个响应也可以通过奖励模型或AI反馈进行排序。缺少关于直接适应响应列表的研究。在这项工作中，我们将语言模型对齐问题定义为一个列表型排序问题，并描述了列表型偏好优化（LiPO）框架，在给定提示的情况下，策略可以从一个排名列表中更有效地学习可行响应。这种观点与学习排序（LTR）形成明确的联系，其中大多数现有的偏好优化工作可以映射到现有的排名目标，特别是

    Aligning language models (LMs) with curated human feedback is critical to control their behaviors in real-world applications. Several recent policy optimization methods, such as DPO and SLiC, serve as promising alternatives to the traditional Reinforcement Learning from Human Feedback (RLHF) approach. In practice, human feedback often comes in a format of a ranked list over multiple responses to amortize the cost of reading prompt. Multiple responses can also be ranked by reward models or AI feedback. There lacks such a study on directly fitting upon a list of responses. In this work, we formulate the LM alignment as a listwise ranking problem and describe the Listwise Preference Optimization (LiPO) framework, where the policy can potentially learn more effectively from a ranked list of plausible responses given the prompt. This view draws an explicit connection to Learning-to-Rank (LTR), where most existing preference optimization work can be mapped to existing ranking objectives, esp
    
[^37]: 分形模式可能揭示下一个词预测中的智能

    Fractal Patterns May Unravel the Intelligence in Next-Token Prediction

    [https://arxiv.org/abs/2402.01825](https://arxiv.org/abs/2402.01825)

    通过研究语言的分形结构，我们发现语言具有自相似性和长程相关性，从段落到整个文档都存在相似的模式和依赖性。这些发现有助于理解如何通过预测下一个词来理解文本的多个层级结构。分形参数在预测性能方面优于困惑度。这些发现提供了对语言和机制的新视角。

    

    我们研究语言的分形结构，旨在提供一个精确的形式化方法来量化可能之前只有怀疑但尚未正式证明的属性。我们证明了语言具有以下特点：（1）自相似性，展示出各个层级上的复杂性，没有特定的特征上下文长度；（2）长程相关性（LRD），具有大约H=0.70的Hurst参数。基于这些发现，我们认为语言中的短期模式/依赖性，如段落中的模式/依赖性，反映了更大范围的模式/依赖性，如整个文档。这可能有助于理解下一个词预测如何导致对文本的多个层级结构，从单词和从句到更广泛的上下文和意图的理解。我们还证明了分形参数在预测下游性能方面优于基于困惑度的每字节比特（BPB）。我们希望这些发现能为语言和机制提供一种新的视角。

    We study the fractal structure of language, aiming to provide a precise formalism for quantifying properties that may have been previously suspected but not formally shown. We establish that language is: (1) self-similar, exhibiting complexities at all levels of granularity, with no particular characteristic context length, and (2) long-range dependent (LRD), with a Hurst parameter of approximately H=0.70. Based on these findings, we argue that short-term patterns/dependencies in language, such as in paragraphs, mirror the patterns/dependencies over larger scopes, like entire documents. This may shed some light on how next-token prediction can lead to a comprehension of the structure of text at multiple levels of granularity, from words and clauses to broader contexts and intents. We also demonstrate that fractal parameters improve upon perplexity-based bits-per-byte (BPB) in predicting downstream performance. We hope these findings offer a fresh perspective on language and the mechani
    
[^38]: 规模化模型编辑会导致渐进性和突发性遗忘

    Model Editing at Scale leads to Gradual and Catastrophic Forgetting

    [https://arxiv.org/abs/2401.07453](https://arxiv.org/abs/2401.07453)

    评估了当前模型编辑方法在规模化情况下的表现，发现随着模型被顺序编辑多个事实，它会逐渐遗忘先前的事实及执行下游任务的能力。

    

    在大型语言模型中编辑知识是一种具有吸引力的能力，它使我们能够在预训练期间纠正错误学习的事实，同时使用不断增长的新事实列表更新模型。我们认为，为了使模型编辑具有实际效用，我们必须能够对同一模型进行多次编辑。因此，我们评估了当前规模下的模型编辑方法，重点关注两种最先进的方法：ROME 和 MEMIT。我们发现，随着模型被顺序编辑多个事实，它不断地遗忘先前编辑过的事实以及执行下游任务的能力。这种遗忘分为两个阶段--初始的渐进性遗忘阶段，随后是突然或灾难性的遗忘。

    arXiv:2401.07453v2 Announce Type: replace-cross  Abstract: Editing knowledge in large language models is an attractive capability to have which allows us to correct incorrectly learnt facts during pre-training, as well as update the model with an ever-growing list of new facts. While existing model editing techniques have shown promise, they are usually evaluated using metrics for reliability, specificity and generalization over one or few edits. We argue that for model editing to have practical utility, we must be able to make multiple edits to the same model. With this in mind, we evaluate the current model editing methods at scale, focusing on two state of the art methods: ROME and MEMIT. We find that as the model is edited sequentially with multiple facts, it continually forgets previously edited facts and the ability to perform downstream tasks. This forgetting happens in two phases -- an initial gradual but progressive forgetting phase followed by abrupt or catastrophic forgettin
    
[^39]: 高效大型语言模型：一项调查

    Efficient Large Language Models: A Survey

    [https://arxiv.org/abs/2312.03863](https://arxiv.org/abs/2312.03863)

    这篇综述论文对高效大型语言模型进行了系统和全面的调查，提供了从模型为中心、数据为中心和框架为中心的三个主要角度的分类和总结。此外，还创建了一个GitHub存储库来收集和更新相关论文。

    

    大型语言模型（LLMs）在重要任务如自然语言理解、语言生成和复杂推理中展示了卓越的能力，并且有潜力对我们的社会产生重大影响。然而，这种能力伴随着它们所需的相当大的资源，突显了解决效率挑战的有效技术的强烈需求。在这项调查中，我们提供了对高效LLMs研究的系统和全面的综述。我们将文献按照模型为中心、数据为中心和框架为中心的三个主要分类进行组织，涵盖了不同但相互关联的高效LLMs主题。我们还创建了一个GitHub存储库，其中收集了本调查中列出的论文，并将积极维护该存储库，并随着新的研究的出现而更新。

    Large Language Models (LLMs) have demonstrated remarkable capabilities in important tasks such as natural language understanding, language generation, and complex reasoning and have the potential to make a substantial impact on our society. Such capabilities, however, come with the considerable resources they demand, highlighting the strong need to develop effective techniques for addressing their efficiency challenges.In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We have also created a GitHub repository where we compile the papers featured in this survey at https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey, and will actively maintain this repository and incorporate new research as it emerges. We hope our survey can s
    
[^40]: 视频本地化指令生成的高效预训练方法

    Efficient Pre-training for Localized Instruction Generation of Videos

    [https://arxiv.org/abs/2311.15964](https://arxiv.org/abs/2311.15964)

    提出了一种名为Sieve-&-Swap的技术，通过自动筛选出不相关文本并用人类编写的说明替换文本转录，从而实现视频本地化指令生成的高效预训练。

    

    过程视频展示了诸如食谱准备等任务的逐步演示。理解此类视频具有挑战性，需要对步骤进行精确定位并生成文字说明。手动注释步骤并编写说明成本高昂，这限制了当前数据集的规模并阻碍了有效学习。利用大规模但嘈杂的视频-文本数据集进行预训练可以提升性能，但需要大量计算资源。此外，文本转录包含无关内容，与人类注释员编写的说明相比存在风格变化。为了缓解这两个问题，我们提出了一种技术，Sieve-&-Swap，通过自动筛选出不相关文本和使用文本食谱数据集中人类编写的说明自动替换文本转录以增强文字指令的质量。

    arXiv:2311.15964v2 Announce Type: replace-cross  Abstract: Procedural videos show step-by-step demonstrations of tasks like recipe preparation. Understanding such videos is challenging, involving the precise localization of steps and the generation of textual instructions. Manually annotating steps and writing instructions is costly, which limits the size of current datasets and hinders effective learning. Leveraging large but noisy video-transcript datasets for pre-training can boost performance, but demands significant computational resources. Furthermore, transcripts contain irrelevant content and exhibit style variation compared to instructions written by human annotators. To mitigate both issues, we propose a technique, Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters irrelevant transcripts and (ii) Swap enhances the quality of the text instruction by automatically replacing the transcripts with human-written instructions from a text-only recipe dataset. 
    
[^41]: 学习学习以进行少样本持久主动学习

    Learning to Learn for Few-shot Continual Active Learning

    [https://arxiv.org/abs/2311.03732](https://arxiv.org/abs/2311.03732)

    提出了一种简单而高效的方法，Meta-Continual Active Learning，通过元学习和经验重播解决少样本持续主动学习中的任务混淆和灾难性遗忘，进一步结合文本增强来确保泛化。

    

    持续学习旨在确保解决先前见过的任务的稳定性，同时展示对新领域的可塑性。最近在持续学习方面的进展主要局限于监督学习设置，尤其是在自然语言处理领域。在这项工作中，我们考虑了一种少样本持续主动学习（CAL）设置，其中标记数据不足，但未标记数据充足，但有限的注释预算。我们提出了一种简单而高效的方法，称为元持续主动学习。具体地，我们采用元学习和经验重播来解决任务之间的混淆和灾难性遗忘。我们进一步结合文本增强来确保泛化性能。我们在基准文本分类数据集上进行了大量实验，以验证所提方法的有效性，并分析了在少样本CAL设置中不同主动学习策略的影响。我们的实验结果表明

    arXiv:2311.03732v2 Announce Type: replace-cross  Abstract: Continual learning strives to ensure stability in solving previously seen tasks while demonstrating plasticity in a novel domain. Recent advances in CL are mostly confined to a supervised learning setting, especially in NLP domain. In this work, we consider a few-shot continual active learning (CAL) setting where labeled data are inadequate, and unlabeled data are abundant but with a limited annotation budget. We propose a simple but efficient method, called Meta-Continual Active Learning. Specifically, we employ meta-learning and experience replay to address inter-task confusion and catastrophic forgetting. We further incorporate textual augmentations to ensure generalization. We conduct extensive experiments on benchmark text classification datasets to validate the effectiveness of the proposed method and analyze the effect of different active learning strategies in few-shot CAL setting. Our experimental results demonstrate t
    
[^42]: 令人惊叹但令人困惑：用假设细化测试语言模型的归纳推理能力

    Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement

    [https://arxiv.org/abs/2310.08559](https://arxiv.org/abs/2310.08559)

    对语言模型进行的系统研究揭示了它们在假设提出方面表现惊人，并且通过与一个（任务特定的）符号解释器相结合，能够系统地过滤可能性。

    

    从少数观察中推导出潜在原则，然后推广到新情况的能力，即归纳推理，对于人类智能至关重要。之前的研究表明，尽管在研究基准上取得了令人印象深刻的成功，但语言模型（LMs）在归纳推理方面常常表现不佳。在这项工作中，我们通过迭代假设细化这一技术对LMs的归纳推理能力进行了系统研究，该技术更接近人类归纳过程，而不是标准的输入-输出提示。

    arXiv:2310.08559v3 Announce Type: replace-cross  Abstract: The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the pr
    
[^43]: 通过高效的奖励模型集成改进人工反馈强化学习

    Improving Reinforcement Learning from Human Feedback with Efficient Reward Model Ensemble. (arXiv:2401.16635v1 [cs.LG])

    [http://arxiv.org/abs/2401.16635](http://arxiv.org/abs/2401.16635)

    本论文提出一种通过高效的奖励模型集成来改进人工反馈强化学习的方法，以解决由于奖励模型预测不准确而导致RLHF输出与人类价值观不一致的问题。

    

    人工反馈强化学习（RLHF）是一种广泛使用的方法，用于将大型语言模型与人类价值观对齐。然而，RLHF依赖于通过有限的人类偏好数据训练的奖励模型，这可能导致不准确的预测。因此，RLHF可能产生与人类价值观不一致的输出。为了缓解这个问题，我们提出了一种奖励集成方法，可以使奖励模型做出更准确的预测。考虑到使用基于大型语言模型的奖励模型集成可能具有计算和资源昂贵的问题，我们探索了包括线性层集成和基于LoRA的集成在内的高效集成方法。实证上，我们使用我们的集成奖励模型运行Best-of-$n$和Proximal Policy Optimization，并验证我们的集成方法有助于改善RLHF输出的对齐性能。

    Reinforcement Learning from Human Feedback (RLHF) is a widely adopted approach for aligning large language models with human values. However, RLHF relies on a reward model that is trained with a limited amount of human preference data, which could lead to inaccurate predictions. As a result, RLHF may produce outputs that are misaligned with human values. To mitigate this issue, we contribute a reward ensemble method that allows the reward model to make more accurate predictions. As using an ensemble of large language model-based reward models can be computationally and resource-expensive, we explore efficient ensemble methods including linear-layer ensemble and LoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy Optimization with our ensembled reward models, and verify that our ensemble methods help improve the alignment performance of RLHF outputs.
    
[^44]: LocMoE: 一种用于大型语言模型训练的低开销MoE

    LocMoE: A Low-overhead MoE for Large Language Model Training. (arXiv:2401.13920v1 [cs.LG])

    [http://arxiv.org/abs/2401.13920](http://arxiv.org/abs/2401.13920)

    LocMoE提出了一种新的路由策略，通过将部分节点间通信转换为节点内通信，结合负载平衡和局部性，以提高大型语言模型训练的性能。

    

    混合专家模型（MoE）是一种广泛采用的分布式和集成学习方法，用于大型语言模型（LLM），由于其能够有效稀疏和扩展模型，因此备受青睐。然而，MoE的性能受到负载不平衡和全对全通信的高延迟的限制，同时由于大量的专家容量导致相对冗余的计算。负载不平衡可能是由于现有路由策略始终倾向于选择特定的专家导致的。全对全过程中频繁的节点间通信也显著延长了训练时间。为了缓解上述性能问题，我们提出了一种新的路由策略，通过将部分节点间通信转换为节点内通信，结合负载平衡和局部性。值得注意的是，我们阐明了专家容量的最小阈值，通过将专家的门控权重与分配的标记之间的最大角偏差计算出来。

    The Mixtures-of-Experts (MoE) model is a widespread distributed and integrated learning method for large language models (LLM), which is favored due to its ability to sparsify and expand models efficiently. However, the performance of MoE is limited by load imbalance and high latency of All-To-All communication, along with relatively redundant computation owing to large expert capacity. Load imbalance may result from existing routing policies that consistently tend to select certain experts. The frequent inter-node communication in the All-To-All procedure also significantly prolongs the training time. To alleviate the above performance problems, we propose a novel routing strategy that combines load balance and locality by converting partial inter-node communication to that of intra-node. Notably, we elucidate that there is a minimum threshold for expert capacity, calculated through the maximal angular deviation between the gating weights of the experts and the assigned tokens. We por
    
[^45]: 视觉语言模型中被忽视的尾部

    The Neglected Tails of Vision-Language Models. (arXiv:2401.12425v1 [cs.CV])

    [http://arxiv.org/abs/2401.12425](http://arxiv.org/abs/2401.12425)

    本文通过分析预训练文本来衡量视觉语言模型中概念的频率，并发现流行的VLM数据集展示了长尾概念分布，这与按类别的准确率强烈相关。

    

    视觉语言模型（VLM）在零样本识别方面表现出色，但在视觉概念上的表现极不均衡。例如，尽管CLIP在ImageNet上具有令人印象深刻的平均零样本准确率（72.7％），但在十个概念（如gyromitra和night snake）上的准确率不到10％，这可能是因为这些概念在VLM的非均衡预训练数据中的表示不足。然而，评估这种不平衡是具有挑战性的，因为在VLM的大规模预训练数据中计算特定概念的频率是非常复杂的。我们的工作首次尝试使用分析预训练文本来测量概念频率。我们利用现成的语言模型来帮助计算包含给定概念的同义词的相关文本，并解决语言歧义。我们确认像LAION这样的流行的VLM数据集确实展示了长尾概念分布，并且这与按类别的准确率强烈相关。此外，当代的多模式系统，如视觉聊天机器人和文本-视觉推理模型，在这种长尾分布下经常难以达到高性能。

    Vision-language models (VLMs) excel in zero-shot recognition but exhibit drastically imbalanced performance across visual concepts. For example, CLIP, despite an impressive mean zero-shot accuracy on ImageNet (72.7%), yields $<$10% on ten concepts (e.g., gyromitra and night snake), presumably, because these concepts are under-represented in VLMs' imbalanced pretraining data. Yet, assessing this imbalance is challenging as it is non-trivial to calculate the frequency of specific concepts within VLMs' large-scale pretraining data. Our work makes the first attempt to measure the concept frequency by analyzing pretraining texts. We use off-the-shelf language models to help count relevant texts that contain synonyms of the given concepts and resolve linguistic ambiguity. We confirm that popular VLM datasets like LAION indeed exhibit long-tailed concept distributions, which strongly correlate with per-class accuracies. Further, contemporary multimodal systems, e.g., visual chatbots and text-
    
[^46]: 从理解的角度探讨语言模型的关键数据规模

    Critical Data Size of Language Models from a Grokking Perspective. (arXiv:2401.10463v1 [cs.CL])

    [http://arxiv.org/abs/2401.10463](http://arxiv.org/abs/2401.10463)

    本文从理解的角度探讨了语言模型中的关键数据规模，证明了只有当语言模型达到关键大小时才会发生泛化，同时揭示了更大的模型需要更多数据的趋势。

    

    我们探讨了语言模型中的关键数据规模，这是从快速记忆到缓慢泛化的一个基本转变阈值。我们将这个相变形式化为Grokking配置下的数据效率假说，并确定了语言模型训练动力学中的数据不足、充足和过剩阶段。我们通过重新调整初始化和权重衰减，开发出了一种Grokking配置，稳定地在简化的语言模型上重现了Grokking。我们表明只有当语言模型达到关键大小时才会发生泛化。我们分析了样本级和模型级的Grokking，验证了提出的数据效率假说。我们的实验揭示了语言数据集的关键数据集大小处发生的更平滑的相变。随着模型大小的增加，这个临界点也变得更大，这表明更大的模型需要更多的数据。我们的研究结果加深了对语言模型训练的理解，提供了一种新颖的视角。

    We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel pers
    
[^47]: ChatQA: 构建GPT-4级对话问答模型

    ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])

    [http://arxiv.org/abs/2401.10225](http://arxiv.org/abs/2401.10225)

    ChatQA是一系列对话问答模型，可以达到GPT-4级别的准确性。通过两阶段的指令调整方法，可以显著提高大型语言模型在零-shot对话问答中的结果。使用密集检索器进行问答数据集的微调可以实现与最先进的查询重写模型相当的结果，同时降低部署成本。ChatQA-70B在10个对话问答数据集上的平均得分超过了GPT-4，且不依赖于任何来自OpenAI GPT模型的合成数据。

    

    在这项工作中，我们介绍了ChatQA，一系列具有GPT-4级别准确性的对话问答模型。具体地，我们提出了一个两阶段的指令调整方法，可以显著提高大型语言模型（LLM）在零-shot对话问答中的结果。为了处理对话问答中的检索问题，我们在多轮问答数据集上进行了密集检索器的微调，这样可以提供与使用最先进的查询重写模型相当的结果，同时大大降低部署成本。值得注意的是，我们的ChatQA-70B可以在10个对话问答数据集的平均分上超过GPT-4（54.14 vs. 53.90），而不依赖于OpenAI GPT模型的任何合成数据。

    In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
    
[^48]: 用大型语言模型表征在线饮食紊乱社群

    Characterizing Online Eating Disorder Communities with Large Language Models. (arXiv:2401.09647v1 [cs.SI])

    [http://arxiv.org/abs/2401.09647](http://arxiv.org/abs/2401.09647)

    通过网络和语言分析，我们表征了在线社群中推广饮食紊乱的动态，认为社交媒体平台放大了这一现象。使用大型语言模型和分析社群内的话语，我们探测到了与饮食紊乱相关的潜在情况。

    

    饮食紊乱作为一种危险的心理健康状况，具有较高的死亡率和发病率，其上升与社交媒体上理想化身体形象的泛滥有关。然而，社交媒体与饮食紊乱之间的联系远不止如此。我们认为社交媒体平台创建了一个反馈循环，放大了推广厌食症和暴食症等饮食紊乱的内容和社群的增长。具体而言，社交媒体平台使易受伤害的个体能够轻松找到并联系到志同道合的其他人，而群体动态过程则鼓励他们在推广和美化与饮食紊乱相关的有害行为的社群中持续参与。我们通过网络和语言分析的组合，从经验上描述了这一动态。我们提出了一个新颖的框架，利用大型语言模型分析在线社群内的话语，并对与饮食紊乱相关的话题的态度进行探测，以鉴别潜在的情况。

    The rise in eating disorders, a dangerous mental health condition with high mortality and morbidity, has been linked to the proliferation of idealized body images on social media. However, the link between social media and eating disorders is far more complex. We argue that social media platforms create a feedback loop that amplifies the growth of content and communities that promote eating disorders like anorexia and bulimia. Specifically, social media platforms make it easy for vulnerable individuals to find and connect to like-minded others, while group dynamic processes encourage them to stay engaged within communities that promote and glorify harmful behaviors linked to eating disorders. We characterize this dynamic empirically through a combination of network and language analysis. We describe a novel framework that leverages large language models to analyze the discourse within online communities and probe their attitudes on topics related to eating disorders to identify potenti
    
[^49]: 机器翻译模型是零射击的翻译方向检测器。

    Machine Translation Models are Zero-Shot Detectors of Translation Direction. (arXiv:2401.06769v1 [cs.CL])

    [http://arxiv.org/abs/2401.06769](http://arxiv.org/abs/2401.06769)

    本文探索了一种基于无监督方法的翻译方向检测，并通过实验证实其在高负载语言对上的有效性。论文标题为“Machine Translation Models are Zero-Shot Detectors of Translation Direction”。

    

    检测并行文本的翻译方向对于机器翻译的训练和评估具有应用价值，但也具有法医应用，例如解决剽窃或伪造指控。在这项工作中，我们根据一个简单的假设，即$p(\text{translation}|\text{original})>p(\text{original}|\text{translation})$，以传统上被称为翻译语或机器翻译语中的简化效应为动机，探索了一种无监督的翻译方向检测方法。通过对20个翻译方向进行大规模多语种机器翻译模型的实验，我们验证了该方法在资源丰富的语言对上的有效性，对于NMT生成的翻译，实现了文档级准确率为82-96％，对于人工翻译，根据所使用的模型，实现了60-81％的准确率。代码和演示可在https://github.com/ZurichNLP/translation-direction-detection找到。

    Detecting the translation direction of parallel text has applications for machine translation training and evaluation, but also has forensic applications such as resolving plagiarism or forgery allegations. In this work, we explore an unsupervised approach to translation direction detection based on the simple hypothesis that $p(\text{translation}|\text{original})>p(\text{original}|\text{translation})$, motivated by the well-known simplification effect in translationese or machine-translationese. In experiments with massively multilingual machine translation models across 20 translation directions, we confirm the effectiveness of the approach for high-resource language pairs, achieving document-level accuracies of 82-96% for NMT-produced translations, and 60-81% for human translations, depending on the model used. Code and demo are available at https://github.com/ZurichNLP/translation-direction-detection
    
[^50]: 使用文本数据的近因果推断

    Proximal Causal Inference With Text Data. (arXiv:2401.06687v1 [cs.CL])

    [http://arxiv.org/abs/2401.06687](http://arxiv.org/abs/2401.06687)

    本论文提出了一种使用文本数据进行近因果推断的方法，通过将文本数据分割并使用零样本模型推断出代理变量，然后应用于近邻 g-formula，从而解决了混淆变量完全未观察到的情况。实验结果表明该方法产生了低偏差的估计值。

    

    最近的基于文本的因果方法试图通过将非结构化文本数据作为倾向于包含部分或不完全测量的混淆变量的代理来减轻混淆偏差。这些方法假设分析人员在一部分实例的文本中具有有监督的混淆变量标签，但由于数据隐私或成本，这种约束并不总是可行。在这里，我们解决了一个重要的混淆变量完全未观察到的情况。我们提出了一种新的因果推断方法，将处理前文本数据分割，并使用两个零样本模型从分割的两个部分推断出两个代理，并将这些代理应用于近邻 g-formula。我们证明了我们基于文本的代理方法满足近邻 g-formula所需的识别条件，而其他看似合理的提议则不满足。我们在合成和半合成环境中评估了我们的方法，并发现它产生了低偏差的估计值。

    Recent text-based causal methods attempt to mitigate confounding bias by including unstructured text data as proxies of confounding variables that are partially or imperfectly measured. These approaches assume analysts have supervised labels of the confounders given text for a subset of instances, a constraint that is not always feasible due to data privacy or cost. Here, we address settings in which an important confounding variable is completely unobserved. We propose a new causal inference method that splits pre-treatment text data, infers two proxies from two zero-shot models on the separate splits, and applies these proxies in the proximal g-formula. We prove that our text-based proxy method satisfies identification conditions required by the proximal g-formula while other seemingly reasonable proposals do not. We evaluate our method in synthetic and semi-synthetic settings and find that it produces estimates with low bias. This combination of proximal causal inference and zero-sh
    
[^51]: TransliCo：一种用于解决多语言预训练语言模型中脚本障碍的对比学习框架

    TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models. (arXiv:2401.06620v1 [cs.CL])

    [http://arxiv.org/abs/2401.06620](http://arxiv.org/abs/2401.06620)

    本论文提出了TransliCo，一个对比学习框架，用于解决多语言预训练语言模型中的脚本障碍。通过对比不同脚本的句子及其在统一脚本中的音译，实现了不同脚本的统一表示空间。实验证明，这种方法能够改善跨语言传递的性能。

    

    书面形式中有293种脚本代表着7000多种语言。由于各种原因，许多密切相关的语言使用不同的脚本，这给多语言预训练语言模型（mPLMs）通过词汇重叠学习跨语言知识带来了困难。因此，mPLMs存在脚本障碍：来自不同脚本的表示位于不同子空间中，这是为什么涉及不同脚本语言的跨语言传递显示次优性能的强有力指标。为了解决这个问题，我们提出了一个简单的框架TransliCo，它包含Transliteration Contrastive Modeling（TCM），通过对训练数据中的句子及其在统一脚本（在我们的案例中是拉丁字母）中的音译进行对比，来微调mPLM，从而确保不同脚本的表示空间的一致性。使用Glot500-m作为我们的源模型，它是在500多种语言上预训练的mPLM，我们将其在其5％的小部分上微调

    There are 293 scripts representing over 7,000 languages in the written form. Due to various reasons, many closely related languages use different scripts, which poses difficulty for multilingual pretrained language models (mPLMs) in learning crosslingual knowledge through lexical overlap. As a result, mPLMs present a script barrier: representations from different scripts are located in different subspaces, which is a strong indicator of why crosslingual transfer involving languages of different scripts shows sub-optimal performance. To address this problem, we propose a simple framework TransliCo that contains Transliteration Contrastive Modeling (TCM) to fine-tune an mPLM by contrasting sentences in its training data and their transliterations in a unified script (Latn, in our case), which ensures uniformity in the representation space for different scripts. Using Glot500-m, an mPLM pretrained on over 500 languages, as our source model, we find-tune it on a small portion (5\%) of its 
    
[^52]: 一个带有嵌入式历时语义变化模型的论文与一个关于古希腊的案例研究

    An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek. (arXiv:2311.00541v1 [cs.CL])

    [http://arxiv.org/abs/2311.00541](http://arxiv.org/abs/2311.00541)

    本论文介绍了一个嵌入式历时语义变化模型（EDiSC），结合了词嵌入和DiSC模型，通过无监督学习分析古希腊文本中目标词汇的意义变化。实验证明EDiSC具有优越的性能。

    

    词汇的意义随着时间的推移而变化，词义在这个过程中会演变、出现或消失。对于古代语言来说，由于语料库通常较小、稀疏且嘈杂，准确建模这种变化变得具有挑战性，因此对于意义变化估计的不确定性进行量化变得重要。GASC和DiSC是现有的生成模型，已经被用来分析古希腊文本语料库中目标词汇的意义变化，使用了无监督学习并没有借助任何预训练的帮助。这些模型将给定目标词汇（如"kosmos"，意为装饰、秩序或世界）的意义表示为上下文词汇的分布，并将意义的普遍性表示为意义的分布。这些模型使用马尔科夫链蒙特卡洛方法进行拟合，以测量这些表示中的时间变化。在本文中，我们介绍了EDiSC，这是DiSC的嵌入版本，它将词嵌入与DiSC相结合，提供了更优秀的模型性能。我们通过实验证明，EDiSC提供了改进的性能。

    Word meanings change over time, and word senses evolve, emerge or die out in the process. For ancient languages, where the corpora are often small, sparse and noisy, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. GASC and DiSC are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training. These models represent the senses of a given target word such as "kosmos" (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. The models are fitted using MCMC methods to measure temporal changes in these representations. In this paper, we introduce EDiSC, an embedded version of DiSC, which combines word embeddings with DiSC to provide superior model performance. We show empirically that EDiSC offers improved p
    
[^53]: 在快速发展时代管理人工智能风险

    Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v1 [cs.CY] CROSS LISTED)

    [http://arxiv.org/abs/2310.17688](http://arxiv.org/abs/2310.17688)

    在人工智能快速进展的时代，我们提出了管理即将到来的先进人工智能系统所带来的风险的优先事项。

    

    在这篇简短的共识文中，我们概述了即将到来的先进人工智能系统所带来的风险。我们审查了大规模的社会危害和恶意使用，以及人类对自主人工智能系统失去控制的不可逆转的损失。鉴于人工智能的快速和持续进展，我们提出了人工智能研发和治理的优先事项。

    In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose priorities for AI R&D and governance.
    
[^54]: 基于预训练语言模型的上下文少样本关系抽取

    In-Context Few-Shot Relation Extraction via Pre-Trained Language Models. (arXiv:2310.11085v1 [cs.CL])

    [http://arxiv.org/abs/2310.11085](http://arxiv.org/abs/2310.11085)

    本研究提出了基于预训练语言模型的上下文少样本关系抽取框架，首次将关系抽取任务重新定义为定制的上下文少样本学习范式。与现有方法相比，该框架不需要命名实体识别和文档人工注释，并且可以轻松更新到新的关系集合。通过评估使用DocRED数据集，验证了该框架的有效性。

    

    关系提取旨在从文本文档中推断结构化的人类知识。基于语言模型的最先进方法通常有两个限制：(1)它们要求命名实体作为输入或推断它们，从而引入了额外的噪声，(2)它们需要人工对文档进行注释。为解决这些问题，我们提出了一种新颖的基于预训练语言模型的上下文少样本关系抽取框架。据我们所知，我们是第一个将关系抽取任务重新定义为定制的上下文少样本学习范式的研究者。通过这种方式，我们在消除了命名实体识别和文档人工注释的需求的同时，实现了关键性的优势。与现有的基于微调的方法不同，我们的框架具有灵活性，可以在无需重新训练的情况下轻松更新到新的关系集合。我们使用DocRED评估了我们的框架，这是目前最大的公开可用的文档级关系提取数据集。

    Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extracti
    
[^55]: CompA: 解决音频-语言模型中的组合推理差距

    CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models. (arXiv:2310.08753v1 [cs.SD])

    [http://arxiv.org/abs/2310.08753](http://arxiv.org/abs/2310.08753)

    CompA提出了由两个专家注释的音频-语言模型组合推理基准数据集，用于评估ALMs在理解音频中声音事件的顺序和属性绑定方面的表现。

    

    音频的基本特性是其组合性。使用对比方法（例如CLAP）训练的音频-语言模型（ALMs）能够学习音频和语言模态之间的共享表示，从而在许多下游应用中提高性能，包括零样本音频分类、音频检索等。然而，这些模型在有效执行组合推理方面的能力还很少被探索，需要进一步的研究。本文提出了CompA，这是一个由两个专家注释的基准数据集，其中大多数是真实世界的音频样本，用于评估ALMs的组合推理能力。我们的CompA-order评估ALMs在理解音频中声音事件的顺序或发生时的表现如何，而CompA-attribute评估声音事件的属性绑定。每个基准数据集中的实例包含两个音频-标题对，其中两个音频具有相同的声音事件，但组合方式不同。

    A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose CompA, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. A
    
[^56]: Parrot:通过学习提问来增强多轮聊天模型

    Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions. (arXiv:2310.07301v1 [cs.CL])

    [http://arxiv.org/abs/2310.07301](http://arxiv.org/abs/2310.07301)

    本文提出了Parrot，一个高度可扩展的解决方案，通过自动生成高质量的指令调优数据，进一步完善了多轮聊天模型在对话中的效果。

    

    最近，基于大型语言模型的聊天模型取得了令人印象深刻的进展；然而，在开源聊天模型（如Alpaca和Vicuna）与领先的聊天模型（如ChatGPT和GPT-4）之间存在明显的多轮对话滞后。通过一系列的分析，我们将滞后归因于缺乏足够高质量的多轮指令调优数据。社区提供的调优数据要么是单轮会话，要么是存在某些问题的多轮会话，例如非人类的指令，响应不够详细，或者很少出现主题转换。本文通过引入Parrot，一个高度可扩展的解决方案，来解决这些挑战，该解决方案旨在自动生成高质量的指令调优数据，然后用于增强多轮聊天模型在对话中的效果。具体而言，我们首先训练Parrot-Ask模型，该模型旨在模拟真实用户生成指令。

    Impressive progress has been made on chat models based on Large Language Models (LLMs) recently; however, there is a noticeable lag in multi-turn conversations between open-source chat models (e.g., Alpaca and Vicuna) and the leading chat models (e.g., ChatGPT and GPT-4). Through a series of analyses, we attribute the lag to the lack of enough high-quality multi-turn instruction-tuning data. The available instruction-tuning data for the community are either single-turn conversations or multi-turn ones with certain issues, such as non-human-like instructions, less detailed responses, or rare topic shifts. In this paper, we address these challenges by introducing Parrot, a highly scalable solution designed to automatically generate high-quality instruction-tuning data, which are then used to enhance the effectiveness of chat models in multi-turn conversations. Specifically, we start by training the Parrot-Ask model, which is designed to emulate real users in generating instructions. We t
    
[^57]: 通过有符号梯度下降优化LLMs量化中的权重舍入

    Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05516](http://arxiv.org/abs/2309.05516)

    本文提出一种名为SignRound的优化权重舍入的方法，通过使用有符号梯度进行轻量级分块调整，解决了大型语言模型(LLMs)的量化挑战。

    

    大型语言模型(LLMs)在执行语言相关任务方面表现出了非凡的能力。然而，由于其巨大的内存和存储需求，它们的部署面临着重大挑战。为了解决这个问题，仅针对权重的量化，特别是3位和4位仅针对权重的量化，已经成为最可行的解决方案之一。随着位数的减少，量化网格变得更加宽泛，从而强调了上下舍入的重要性。尽管先前的研究表明，在某些情况下，通过添加扰动细调上下舍入可以提高准确性，但我们的研究受制于这些扰动的精确且有限的边界，只有改变舍入值的阈值才具有重要性。因此，我们提出了一种简洁高效的优化权重舍入任务的方法。我们的方法名为SignRound，它涉及使用有符号梯度的轻量级分块调整。

    Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gra
    
[^58]: 基于维基百科风格的调研生成的大型语言模型：在自然语言处理概念中的评估

    Large Language Models on Wikipedia-Style Survey Generation: an Evaluation in NLP Concepts. (arXiv:2308.10410v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10410](http://arxiv.org/abs/2308.10410)

    本研究评估了大型语言模型在自然语言处理领域生成调研文章的效果，发现GPT-4优于GPT-3.5，并且指出了GPT在信息完整性和事实准确性方面的一些缺陷。

    

    大型语言模型（LLMs）在各种自然语言处理（NLP）任务中取得了重大成功，包括问答、摘要和机器翻译等。虽然LLMs在一般任务中表现出色，但它们在特定领域应用中的效果仍在探索中。此外，LLM生成的文本有时会出现幻觉和不实信息等问题。在本研究中，我们评估了LLMs在计算机科学-NLP领域中生成简洁调研文章的能力，重点关注20个选定的主题。自动评估表明，GPT-4在与真实数据进行基准测试时优于GPT-3.5。此外，四位人类评估者从四个模型配置的六个角度提供了见解。通过案例研究，我们证明了虽然GPT通常能产生可称赞的结果，但也存在一些缺点，如信息不完整和事实准确性方面的漏洞。

    Large Language Models (LLMs) have achieved significant success across various natural language processing (NLP) tasks, encompassing question-answering, summarization, and machine translation, among others. While LLMs excel in general tasks, their efficacy in domain-specific applications remains under exploration. Additionally, LLM-generated text sometimes exhibits issues like hallucination and disinformation. In this study, we assess LLMs' capability of producing concise survey articles within the computer science-NLP domain, focusing on 20 chosen topics. Automated evaluations indicate that GPT-4 outperforms GPT-3.5 when benchmarked against the ground truth. Furthermore, four human evaluators provide insights from six perspectives across four model configurations. Through case studies, we demonstrate that while GPT often yields commendable results, there are instances of shortcomings, such as incomplete information and the exhibition of lapses in factual accuracy.
    
[^59]: RecycleGPT：一种具有可回收模块的自回归语言模型

    RecycleGPT: An Autoregressive Language Model with Recyclable Module. (arXiv:2308.03421v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03421](http://arxiv.org/abs/2308.03421)

    RecycleGPT是一种生成式语言模型，通过回收预生成的模型状态而无需多次运行整个模型，并且证明了其降低推理延迟的有效性，实现高速解码和保持高性能。

    

    现有的大型语言模型必须运行K次才能生成K个令牌的序列。在本文中，我们提出了RecycleGPT，这是一种具有快速解码速度的生成式语言模型，通过回收预生成的模型状态而无需将整个模型运行多次步骤。我们的方法基于这样的观察：序列中相邻的令牌通常具有很强的相关性，并且可以根据前面的令牌合理猜测或推断出下一个令牌。实验证明了我们的方法在降低推理延迟方面的有效性，实现了高达1.4倍的加速，同时保持高性能。

    Existing large language models have to run K times to generate a sequence of K tokens. In this paper, we present RecycleGPT, a generative language model with fast decoding speed by recycling pre-generated model states without running the whole model in multiple steps. Our approach relies on the observation that adjacent tokens in a sequence usually have strong correlations and the next token in a sequence can be reasonably guessed or inferred based on the preceding ones. Experiments and analysis demonstrate the effectiveness of our approach in lowering inference latency, achieving up to 1.4x speedup while preserving high performance.
    
[^60]: LaFiCMIL：从相关多实例学习的角度重新思考大文件分类

    LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning. (arXiv:2308.01413v1 [cs.CL])

    [http://arxiv.org/abs/2308.01413](http://arxiv.org/abs/2308.01413)

    LaFiCMIL是一个新的方法，从相关多实例学习的角度解决了Transformer模型输入长度限制的问题，可以用于改进大文件分类任务。

    

    基于Transformer的模型在各种语言任务的性能上取得了革命性的突破。直观上，人们可能会期望文本分类，作为不需要像生成任务那样许多高级表示的任务，能够充分利用Transformer强大的表示能力来进行综合性的处理。然而，实际上，在多类别和多标签分类长文本文档和其他大文件的领域仍然存在较大的改进潜力。Transformer模型的性能主要受到一个重要限制的阻碍：有限的输入长度，比如BERT的512个标记。虽然增加GPU内存可以稍微扩展这个限制，但实际应用中往往受限于有限的GPU资源。在这项工作中，我们从相关多实例学习的角度解决了输入限制问题。所提出的方法LaFiCMIL，作为一个多功能的框架，适用于

    Transformer-based models have revolutionized the performance of a wide range of language tasks. Intuitively, one might expect text classification, which does not necessitate as many high-level representations as generative tasks, to be comprehensively addressed with the powerful representation capabilities of Transformers. However, in reality, there remains significant potential for enhancement, particularly in the areas of multi-class and multi-label classification of lengthy textual documents and other large files. The performance of Transformer-based models is mainly hindered by a major limitation: a restricted input length, e.g., 512 tokens for BERT. While an increase in GPU memory can marginally extend this limit, practical real-world applications often operate under constrained GPU resources. In this work, we tackle the input limit problem from the perspective of correlated multiple instance learning. The proposed approach, LaFiCMIL, serves as a versatile framework applicable to 
    
[^61]: 对话代理101：设计有效的对话系统的关键要素初学者指南

    Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for Designing Effective Conversational Systems. (arXiv:2307.07255v1 [cs.CL])

    [http://arxiv.org/abs/2307.07255](http://arxiv.org/abs/2307.07255)

    本文提供了一个对话代理设计的相关要素的综合概述，包括对话代理的主要特征、支持任务、数据集和评估方法。研究表明，构建单独的模型来处理不同的对话任务是昂贵且冗余的。

    

    通过与同行进行交流来分享想法是人类互动的主要方式。因此，在对话人工智能领域进行了广泛的研究，导致对话任务、数据集和方法的可用性和多样性增加。然而，由于多个任务同时探索，当前对话人工智能的现状变得分散。因此，为了帮助从零开始设计对话代理的从业者，本研究提供了对对话代理的主要特征、支持任务、相应的开放领域数据集以及用于基准测试这些数据集的方法的综合概述。我们观察到不同的方法已被用于解决不同的对话任务。然而，为每个任务构建单独的模型是昂贵且冗余的。

    Sharing ideas through communication with peers is the primary mode of human interaction. Consequently, extensive research has been conducted in the area of conversational AI, leading to an increase in the availability and diversity of conversational tasks, datasets, and methods. However, with numerous tasks being explored simultaneously, the current landscape of conversational AI becomes fragmented. Therefore, initiating a well-thought-out model for a dialogue agent can pose significant challenges for a practitioner. Towards highlighting the critical ingredients needed for a practitioner to design a dialogue agent from scratch, the current study provides a comprehensive overview of the primary characteristics of a dialogue agent, the supporting tasks, their corresponding open-domain datasets, and the methods used to benchmark these datasets. We observe that different methods have been used to tackle distinct dialogue tasks. However, building separate models for each task is costly and 
    
[^62]: 通过基于LLM的属性操作生成高效训练数据

    Generating Efficient Training Data via LLM-based Attribute Manipulation. (arXiv:2307.07099v1 [cs.CL])

    [http://arxiv.org/abs/2307.07099](http://arxiv.org/abs/2307.07099)

    本文提出了一种通过大型语言模型（LLMs）生成精心制作的训练数据来引导少样本学习的方法。通过利用LLMs操作任务特定属性并重构新的句子，我们实现了标签交换数据的生成，与其他基于LLMs的文本生成方法相比具有更好的效果。同时，研究结果还显示了通过LLM引导学习的潜力，即使在更少的监督情况下也能取得良好的表现。

    

    本文提出了一种新颖的方法，链式思维属性操作（CoTAM），通过从大型语言模型（LLMs）中精心制作的数据来引导少样本学习。主要思想是仅对任务目标属性进行更改并创建数据。受到面部属性操作的启发，我们的方法利用LLMs来操作任务特定属性并以受控的方式重构新的句子，从而生成标签交换数据。我们采用链式思维分解和重构来适应LLMs，而不是传统的潜在表示控制方法。在文本分类和其他任务上进行了广泛的实验结果验证了CoTAM相对于其他具有相同数量训练样本的基于LLMs的文本生成方法的优势。分析结果可视化了CoTAM的属性操作效果，并展示了在更少监督的情况下通过LLM引导学习的潜力。

    In this paper, we propose a novel method, Chain-of-Thoughts Attribute Manipulation (CoTAM), to guide few-shot learning by carefully crafted data from Large Language Models (LLMs). The main idea is to create data with changes only in the attribute targeted by the task. Inspired by facial attribute manipulation, our approach generates label-switched data by leveraging LLMs to manipulate task-specific attributes and reconstruct new sentences in a controlled manner. Instead of conventional latent representation controlling, we implement chain-of-thoughts decomposition and reconstruction to adapt the procedure to LLMs. Extensive results on text classification and other tasks verify the advantage of CoTAM over other LLM-based text generation methods with the same number of training examples. Analysis visualizes the attribute manipulation effectiveness of CoTAM and presents the potential of LLM-guided learning with even less supervision.
    
[^63]: 通过Pareto Optimal自监督实现大型语言模型的自动校准和错误修正

    Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision. (arXiv:2306.16564v1 [cs.CL])

    [http://arxiv.org/abs/2306.16564](http://arxiv.org/abs/2306.16564)

    本文介绍了一种Pareto Optimal自监督框架，利用可用的编程监督将大型语言模型(LLM)的响应进行系统校准，通过为每个响应生成风险评分，而无需额外的手动工作。

    

    大型语言模型(LLM)已经展现了出色的能力，适用于广泛的应用领域，但是准确性仍然是一个重要的增长领域，特别是在生物医学等关键领域。一种有效的方法，用于校准LLM响应的置信水平，对于自动检测错误并促进人机协作验证至关重要。一个重要的校准信号来源是专家指定的编程监督，通常具有较低的成本，但也有其自身的局限性，如噪声和覆盖范围。在本文中，我们引入了一种Pareto Optimal自监督框架，可以利用可用的编程监督来系统地校准LLM响应，通过为每个响应生成风险评分，而不需要任何额外的手动工作。这通过学习一个调和模型来实现，将LLM输出与其他可用的监督来源相协调，将更不确定的响应分配更高的风险评分。

    Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the confidence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop verification. An important source of calibration signals stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain L
    
[^64]: 使用基于认识不确定性的数据选择来适应预训练的ASR模型以应对低资源临床语音问题

    Adapting Pretrained ASR Models to Low-resource Clinical Speech using Epistemic Uncertainty-based Data Selection. (arXiv:2306.02105v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02105](http://arxiv.org/abs/2306.02105)

    本研究使用基于认识不确定性的数据选择方法来减少非洲口音临床ASR训练的注释成本。结果表明，这种方法可以超过现有的基准结果，并提高低资源口音的泛化能力。

    

    尽管ASR取得了显著进展，但由于缺乏训练数据集，对于非洲口音的临床ASR的研究还不够。在这一领域构建强大的ASR系统需要大量的标注数据，用于各种语言和形态丰富的口音，但这些数据的创建成本较高。本研究旨在通过基于信息不确定性的数据选择来减少注释费用。我们表明，将认识不确定性纳入我们的自适应过程中，可以超过使用最先进（SOTA）ASR模型建立的几个基准结果，同时减少所需的标记数据量，从而降低注释成本。我们的方法还改善了低资源口音的超出分布泛化能力，展示了我们的方法在非洲临床ASR的背景下构建泛化型ASR模型的可行性，而在这种情况下，训练数据集主要是稀缺的。

    While there has been significant progress in ASR, African-accented clinical ASR has been understudied due to a lack of training datasets. Building robust ASR systems in this domain requires large amounts of annotated or labeled data, for a wide variety of linguistically and morphologically rich accents, which are expensive to create. Our study aims to address this problem by reducing annotation expenses through informative uncertainty-based data selection. We show that incorporating epistemic uncertainty into our adaptation rounds outperforms several baseline results, established using state-of-the-art (SOTA) ASR models, while reducing the required amount of labeled data, and hence reducing annotation costs. Our approach also improves out-of-distribution generalization for very low-resource accents, demonstrating the viability of our approach for building generalizable ASR models in the context of accented African clinical ASR, where training datasets are predominantly scarce.
    
[^65]: 使用差分隐私大语言模型合成查询的隐私保护推荐系统.

    Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models. (arXiv:2305.05973v1 [cs.CL])

    [http://arxiv.org/abs/2305.05973](http://arxiv.org/abs/2305.05973)

    提出使用差分隐私大语言模型合成查询的隐私保护推荐系统，可以安全有效地训练深度检索模型并提高检索质量。

    

    我们提出了一种新颖的方法，使用差分隐私大语言模型（LLMs）开发隐私保护的大规模推荐系统，克服了在训练这些复杂系统时的某些挑战和限制。我们的方法特别适用于基于LLM的推荐系统的新兴领域，但也可以轻松地用于处理自然语言输入表示的任何推荐系统。我们的方法涉及使用DP训练方法，对公开预训练的LLM在查询生成任务上进行微调。生成的模型可以生成私有合成查询，代表原始查询，可以在任何下游非私有推荐训练过程中自由共享，而不会产生任何额外的隐私成本。我们评估了我们的方法对安全训练有效的深度检索模型的能力，我们观察到它们的检索质量有显着的提高，而不会损害查询级别的隐私。

    We propose a novel approach for developing privacy-preserving large-scale recommender systems using differentially private (DP) large language models (LLMs) which overcomes certain challenges and limitations in DP training these complex systems. Our method is particularly well suited for the emerging area of LLM-based recommender systems, but can be readily employed for any recommender systems that process representations of natural language inputs. Our approach involves using DP training methods to fine-tune a publicly pre-trained LLM on a query generation task. The resulting model can generate private synthetic queries representative of the original queries which can be freely shared for any downstream non-private recommendation training procedures without incurring any additional privacy cost. We evaluate our method on its ability to securely train effective deep retrieval models, and we observe significant improvements in their retrieval quality without compromising query-level pri
    
[^66]: 文字对话中的深度情感识别：一项调研

    Deep Emotion Recognition in Textual Conversations: A Survey. (arXiv:2211.09172v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.09172](http://arxiv.org/abs/2211.09172)

    本调研针对对话中的情感识别进行了探讨，介绍了涉及此任务的挑战和机遇，以及描述了情感分类法和使用该分类法的基准数据集。调研总结了最重要的作品和所使用的深度学习架构，并提供了建议性的情感识别实践，以实现更好的框架。

    

    虽然近年来对话中的情感识别取得了巨大的进展，但新的应用和实施场景带来了新的挑战和机遇。这些挑战包括利用对话语境、说话人和情感动态建模，解释常识表达、非正式语言和讽刺，应对实时情感识别的挑战，识别情感原因，不同数据集中的多种分类法，多语言情感识别以及解释性。本调研首先介绍了情感识别在对话中的应用，详细说明了与此任务相关的挑战和机遇。然后，它介绍了情感分类法和多种使用该分类法的情感识别基准数据集的描述。接下来，它描述了情感识别中最重要的作品，并解释了所使用的深度学习架构。最后，它提供了对于更好的框架的建议性情感识别实践，详细说明了处理主观性的方法。

    While Emotion Recognition in Conversations (ERC) has seen a tremendous advancement in the last few years, new applications and implementation scenarios present novel challenges and opportunities. These range from leveraging the conversational context, speaker and emotion dynamics modelling, to interpreting common sense expressions, informal language and sarcasm, addressing challenges of real time ERC, recognizing emotion causes, different taxonomies across datasets, multilingual ERC to interpretability. This survey starts by introducing ERC, elaborating on the challenges and opportunities pertaining to this task. It proceeds with a description of the emotion taxonomies and a variety of ERC benchmark datasets employing such taxonomies. This is followed by descriptions of the most prominent works in ERC with explanations of the Deep Learning architectures employed. Then, it provides advisable ERC practices towards better frameworks, elaborating on methods to deal with subjectivity in ann
    
[^67]: 探究语法数的使用情况

    Probing for the Usage of Grammatical Number. (arXiv:2204.08831v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.08831](http://arxiv.org/abs/2204.08831)

    本文介绍了一种基于使用的探测设置，通过干预模型的表示来去除属性，从而发现模型实际使用的编码。以BERT如何编码语法数为例研究，结果显示BERT依赖于语法数的线性编码来产生正确的行为输出，并对名词和动词的语法数使用了不同的编码。

    

    探究的核心问题是揭示预训练模型如何在其表示中编码语言属性。然而，编码可能是虚假的，即模型在进行预测时可能不依赖于它。在本文中，我们尝试寻找模型实际使用的编码，引入一种基于使用的探测设置。我们首先选择一个行为任务，该任务在不使用语言属性的情况下无法解决。然后，我们试图通过干预模型的表示来去除属性。我们认为，如果模型使用了某种编码，去除该编码应该会损害所选择的行为任务的性能。以BERT如何编码语法数以及如何利用该编码解决数的一致性任务为案例研究。实验结果显示，BERT依赖于语法数的线性编码来产生正确的行为输出。我们还发现BERT对名词和动词的语法数使用了不同的编码。

    A central quest of probing is to uncover how pre-trained models encode a linguistic property within their representations. An encoding, however, might be spurious-i.e., the model might not rely on it when making predictions. In this paper, we try to find encodings that the model actually uses, introducing a usage-based probing setup. We first choose a behavioral task which cannot be solved without using the linguistic property. Then, we attempt to remove the property by intervening on the model's representations. We contend that, if an encoding is used by the model, its removal should harm the performance on the chosen behavioral task. As a case study, we focus on how BERT encodes grammatical number, and on how it uses this encoding to solve the number agreement task. Experimentally, we find that BERT relies on a linear encoding of grammatical number to produce the correct behavioral output. We also find that BERT uses a separate encoding of grammatical number for nouns and verbs. Fina
    

