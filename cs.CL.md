# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [FP8-LM: Training FP8 Large Language Models.](http://arxiv.org/abs/2310.18313) | 本文提出了一种用于训练大语言模型的新型FP8自动混合精度框架，能够在不影响模型准确性的情况下显著减少内存使用并提高训练速度。 |
| [^2] | [An Approach to Automatically generating Riddles aiding Concept Attainment.](http://arxiv.org/abs/2310.18290) | 这个论文介绍了一种自动生成概念谜题的方法，以促进在线学习环境中的学习者参与度。通过应用概念达成模型和生成谜题，该方法可以帮助学习者更好地理解概念。 |
| [^3] | [MalFake: A Multimodal Fake News Identification for Malayalam using Recurrent Neural Networks and VGG-16.](http://arxiv.org/abs/2310.18263) | 这篇论文提出了使用循环神经网络和VGG-16对马拉雅拉姆语的假新闻进行多模态识别的方法，这是首次在该语言中进行多模态特征提取和分类的工作。 |
| [^4] | [Fine-Tuning Language Models Using Formal Methods Feedback.](http://arxiv.org/abs/2310.18239) | 该论文介绍了一种使用形式方法反馈调整语言模型的自动化方法，用于在自主系统中微调预先训练的模型。通过合成基于自动机的控制器并与给定规范进行验证，该方法可以减少人工反馈的成本并实现领域特定任务的控制策略生成。 |
| [^5] | [Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation.](http://arxiv.org/abs/2310.18235) | 本论文提出了Davidsonian场景图（DSG）的评估框架，解决了现有文本-图像生成模型评估中的可靠性挑战，包括QG问题的准确性和VQA答案的一致性。 |
| [^6] | [Revising with a Backward Glance: Regressions and Skips during Reading as Cognitive Signals for Revision Policies in Incremental Processing.](http://arxiv.org/abs/2310.18229) | 本研究调查了人类阅读眼动追踪数据中的回退和跳过作为增量序列标记中修订策略的信息信号的适用性，并发现其可能作为修订的有用预测因素。 |
| [^7] | [ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models.](http://arxiv.org/abs/2310.18208) | ArcheType是一种使用大型语言模型进行开源列类型注释的新框架，通过改进上下文采样和标签重新映射，实现了全面的零样本解决方案。 |
| [^8] | [INA: An Integrative Approach for Enhancing Negotiation Strategies with Reward-Based Dialogue System.](http://arxiv.org/abs/2310.18207) | 本文提出了一种名为INA的综合谈判对话代理，它能够在价格以及其他因素上进行谈判，为用户提供更具灵活性和全面性的谈判体验。 |
| [^9] | [Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social Media.](http://arxiv.org/abs/2310.18205) | 本论文介绍了一种在多语言社交媒体中识别言论的方法，通过创造一个新颖的多语言数据集X-CLAIM，采用编码器-只语言模型和生成性大语言模型进行实验，并展示了在多种语言上训练的好处。 |
| [^10] | [Style Description based Text-to-Speech with Conditional Prosodic Layer Normalization based Diffusion GAN.](http://arxiv.org/abs/2310.18169) | 本文介绍了一种基于Diffusion GAN的方法，利用条件韵律层标准化将风格嵌入到生成架构中，从而实现根据风格描述和内容文本生成高质量语音。 |
| [^11] | [Personas as a Way to Model Truthfulness in Language Models.](http://arxiv.org/abs/2310.18168) | 本研究探讨了在大型语言模型中使用人设来建模真实性的可能性。通过建模真实人设，语言模型可以将真实性推广到不同上下文中，并通过相关特征判断个体产生文本的真实性。 |
| [^12] | [MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension.](http://arxiv.org/abs/2310.18167) | MPrompt是一种用于机器阅读理解的多级提示调整方法，通过在任务特定、领域特定和上下文特定级别上利用提示来增强不同细粒度的输入语义理解，并通过独立性约束避免冗余。 |
| [^13] | [Elevating Code-mixed Text Handling through Auditory Information of Words.](http://arxiv.org/abs/2310.18155) | 这篇论文提出了一种通过利用单词的听觉信息来处理代码混合文本的方法。该方法通过在预训练阶段使用SOUNDEX表示，以及采用新的输入数据提供方法，有效解决了处理代码混合文本中的拼写变体问题。实验证明了该方法的有效性。 |
| [^14] | [Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs.](http://arxiv.org/abs/2310.18152) | 本文提出了一个名为Disentangled Graph-Text Learner (DGTL)的模型，通过引入定制的解缠图神经网络（GNN）层，使得大型语言模型（LLMs）能够更好地理解文本属性图（TAGs）中的复杂结构关系。 |
| [^15] | [DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial Issues.](http://arxiv.org/abs/2310.18130) | 提出了一个评估LLMs在处理争议问题中表现的数据集，并使用其中的子集对不同的LLMs进行评估，阐明它们如何处理争议问题以及采取的立场。 |
| [^16] | [Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models.](http://arxiv.org/abs/2310.18127) | 本文提出了一种利用大型语言模型的强化学习框架，能够学习提问相关问题并进行推理来指导在实际环境中执行的行为的学习。 |
| [^17] | [OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization.](http://arxiv.org/abs/2310.18122) | 本文提出了一个新的数据集OpinSummEval，对意见摘要进行自动化评估的可靠性进行重新评估。研究发现基于神经网络的度量通常优于非神经网络的度量，但即使是基于强大模型构建的度量也不能在所有维度上始终保持良好的相关性，突出了对意见摘要自动化评估方法的进一步改进的需求。 |
| [^18] | [Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation.](http://arxiv.org/abs/2310.18119) | 通过上下文化知识蒸馏的多任务学习方法，我们提出了一种统一的对话推荐系统，该系统在推荐性能和对话生成的一致性方面取得了显著改进。 |
| [^19] | [Mind the Gap: Automated Corpus Creation for Enthymeme Detection and Reconstruction in Learner Arguments.](http://arxiv.org/abs/2310.18098) | 本研究提出了两个新的任务，即检测学习者论证中的缺失部分和填补这些缺失部分。通过自动创建语料库，可以帮助学习者提高论证质量。基于ICLEv3语料库，本研究创建了40,089个论证实例。 |
| [^20] | [Lost in Translation -- Multilingual Misinformation and its Evolution.](http://arxiv.org/abs/2310.18089) | 本文通过对超过250,000个唯一事实核查的分析，探究了多语言错误信息的普遍性和动态性。结果显示，部分错误信息能够穿越语言障碍，并且在相同语言中更有可能传播。研究还发现错误信息随时间演变并在不同语言间发生突变。 |
| [^21] | [Detrimental Contexts in Open-Domain Question Answering.](http://arxiv.org/abs/2310.18077) | 本文分析了在开放领域问答中过多的背景信息对模型性能的负面影响，并发现通过过滤掉有害的段落可以提高模型的准确性。 |
| [^22] | [Knowledge Corpus Error in Question Answering.](http://arxiv.org/abs/2310.18076) | 本研究探讨了开放领域问答中生成上下文段落与传统检索步骤相比的优势，并引入了知识语料错误的概念。通过使用大型语言模型生成更大范围内的段落，我们观察到问答性能的提升，表明存在知识语料错误的情况。 |
| [^23] | [DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking.](http://arxiv.org/abs/2310.18075) | DUMA是一种具有快速和慢速思考能力的双重思维对话代理框架，通过利用两个生成型大型语言模型，实现了根据情况在直观响应和深思熟虑的问题解决过程之间无缝切换的能力。 |
| [^24] | [A Scalable Framework for Table of Contents Extraction from Complex ESG Annual Reports.](http://arxiv.org/abs/2310.18073) | 本论文提出了一个可扩展的框架来从复杂的ESG年度报告中提取目录。提出了一个新的数据集ESGDoc，包括来自563家公司的1,093份报告。通过构建-建模-修改（CMM）过程，该框架提供了一种实用的解决方案，以消除以前方法中的挑战，并提高目录提取的准确性。 |
| [^25] | [Multi-grained Evidence Inference for Multi-choice Reading Comprehension.](http://arxiv.org/abs/2310.18070) | 本研究提出了一种多样化证据推理模型（Mugen），通过提取粗糙、中等和细粒度的证据，并将其与原始段落结合，实现了显著和一致的性能提升。该模型在多项选择机器阅读理解任务中具有重要的创新和贡献。 |
| [^26] | ["Honey, Tell Me What's Wrong", Global Explanation of Textual Discriminative Models through Cooperative Generation.](http://arxiv.org/abs/2310.18063) | Therapy是第一个适用于文本的全局和无模型解释方法，通过合作生成文本，不依赖于初始样本，并提供了对输入空间中模型行为的全局概览。 |
| [^27] | [ViCLEVR: A Visual Reasoning Dataset and Hybrid Multimodal Fusion Model for Visual Question Answering in Vietnamese.](http://arxiv.org/abs/2310.18046) | 本论文介绍了ViCLEVR数据集，它是一种用于评估越南语视觉推理能力的集合。通过分析该数据集，对当代视觉推理系统进行了全面的分析，揭示了它们的优点和局限性。 |
| [^28] | [On General Language Understanding.](http://arxiv.org/abs/2310.18038) | 本文提出了一个通用语言理解模型的轮廓，该模型可以评估当前模型质量测量方法的适当性问题，旨在解决关于大型语言模型理解程度的争论，并探讨语言理解的多方面现象和自然语言处理应用的道德问题。 |
| [^29] | [Large language models for aspect-based sentiment analysis.](http://arxiv.org/abs/2310.18025) | 该论文评估了GPT-4和GPT-3.5在基于方面的情感分析任务中的性能，并发现微调后的GPT-3.5在SemEval-2014任务4中取得了83.8的最先进F1分数，相比于InstructABSA提高了5.7%。但是，这需要1000倍的模型参数增加了推理成本。 |
| [^30] | [SentMix-3L: A Bangla-English-Hindi Code-Mixed Dataset for Sentiment Analysis.](http://arxiv.org/abs/2310.18023) | SentMix-3L是一个用于情感分析的新颖数据集，包含孟加拉语、英语和印地语之间的代码混合数据。研究发现，在SentMix-3L上，使用GPT-3.5进行零-shot提示可以超过所有基于转换器的模型。 |
| [^31] | [NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark.](http://arxiv.org/abs/2310.18018) | 这篇立场论文指出NLP任务的经典评估存在数据污染的问题，尤其是当一个LLM模型在基准测试的测试集上进行训练和评估时。该问题导致污染模型在目标基准测试和任务中的性能被高估，可能会产生错误的科学结论。建议开发自动和半自动的测量方法来检测数据污染，并提醒评审论文时要注意具有妥协性结论的论文。 |
| [^32] | [Does Role-Playing Chatbots Capture the Character Personalities? Assessing Personality Traits for Role-Playing Chatbots.](http://arxiv.org/abs/2310.17976) | 本文介绍了一种创新的方法，用于评估角色扮演聊天机器人的个性特点，并发现基于LLMs的现代角色扮演聊天机器人能够有效地描绘出相应角色的个性特点，与人类感知的一致性达到82.8%。 |
| [^33] | [Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare.](http://arxiv.org/abs/2310.17956) | Qilin-Med-VL是面向普遍医疗保健的中国大型视觉-语言模型，它结合了预训练的视觉Transformer和基础语言模型，通过两阶段课程训练过程提高了生成医疗标题和回答复杂医疗查询的能力，并发布了一个包含超过100万个图像-文本对的数据集ChiMed-VL。 |
| [^34] | [Whisper-MCE: Whisper Model Finetuned for Better Performance with Mixed Languages.](http://arxiv.org/abs/2310.17953) | Whisper-MCE是使用自己收集的混合粤语和英语音频数据集（MCE）进行训练的Whisper模型微调，相较于基准模型，其在准确捕捉原始音频内容、提高识别准确性和加快识别速度方面具有更优越的能力，尤其在混合语言识别任务中表现出色。 |
| [^35] | [Unified Segment-to-Segment Framework for Simultaneous Sequence Generation.](http://arxiv.org/abs/2310.17940) | 这篇论文提出了一种统一的片段到片段框架 (Seg2Seg) 用于同时序列生成，通过自适应和统一的方式学习源序列和目标序列之间的映射，实现高质量生成和低延迟。 |
| [^36] | [Transformers as Graph-to-Graph Models.](http://arxiv.org/abs/2310.17936) | 本文认为Transformers本质上是图到图模型，通过将注意力权重等价于图中的边，并使用图到图Transformer架构结合显式图和潜在图进行非自回归图预测，实现了在建模各种语言结构方面的最先进准确性。 |
| [^37] | [SOUL: Towards Sentiment and Opinion Understanding of Language.](http://arxiv.org/abs/2310.17924) | 提出了一个新的任务SOUL，旨在通过评论理解和解释生成两个子任务来评估语言的情感和观点理解。该任务在小型和大型语言模型中都具有挑战性，性能差距可高达27%。 |
| [^38] | [Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method.](http://arxiv.org/abs/2310.17918) | 本文提出了一种新的自我检测方法，用于判断大型语言模型 (LLMs) 无法回答的问题，以避免生成非事实性的回答。通过多样化问题的文本表达，收集答案，并检查生成的答案之间的差异，可以识别出可能生成虚假回答的问题。该方法只需要利用LLMs自身，无需其他外部资源。这种方法在Vicuna、ChatGPT和GPT-4等最新发布的LLMs上得到了有效验证。 |
| [^39] | [3D-Aware Visual Question Answering about Parts, Poses and Occlusions.](http://arxiv.org/abs/2310.17914) | 这个论文提出了一种名为3D感知VQA的任务，旨在推动VQA模型对于视觉场景的3D结构进行组合推理。作者从数据集和模型角度出发，分别引入了Super-CLEVR-3D数据集和PO3D-VQA模型，并将概率神经符号程序执行和3D生成表示相结合，用于强大的视觉识别。 |
| [^40] | [Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey.](http://arxiv.org/abs/2310.17894) | 本调查对表格数据查询和可视化的自然语言界面进行了全面概述，介绍了语义解析等关键技术，并深入探讨了Text-to-SQL和Text-to-Vis问题的最新进展。 |
| [^41] | [Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory.](http://arxiv.org/abs/2310.17884) | 本研究通过提出ConfAIde基准，揭示了LLMs的上下文隐私推理能力中的重要弱点，实验证明即使是最强大的模型也会在人类不会的上下文中泄露私人信息，强调了探索新型推理时隐私保护方法的迫切需求。 |
| [^42] | [ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for Consistent Data-to-Text Generation.](http://arxiv.org/abs/2310.17877) | ASPIRO是一种能在零到少样本情况下将结构化数据转化为简短模板句子的方法。通过算法解析检查、LLM的重新提示以及一致性验证指标PARENT，ASPIRO成功降低了66%的解析错误率，并且在与最近的预训练语言模型的竞争中表现出色。 |
| [^43] | [TarGEN: Targeted Data Generation with Large Language Models.](http://arxiv.org/abs/2310.17876) | TarGEN是一种利用大型语言模型生成高质量合成数据集的多步提示策略，通过自我修正方法确保可靠的标签。在SuperGLUE基准测试中，模型在合成数据集上的训练效果与原始数据集相当。 |
| [^44] | [From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models.](http://arxiv.org/abs/2310.17857) | 本研究提出了一种利用注入价值的大型语言模型（LLM）来预测人类行为和观点的方法。通过价值注入方法（VIM）对LLMs进行微调，实验结果表明注入了VIM的LLMs在预测人们观点和行为方面表现出较好的性能。 |
| [^45] | [Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting.](http://arxiv.org/abs/2310.17811) | 该论文提出了一种使用RadGraph和少样本提示的风格感知放射学报告生成的方法。通过将报告的内容和风格分开处理，可以避免生成临床不准确的报告。定量评估和人工评估结果均表明该方法表现出良好的性能，并生成与个体放射科医生风格完全相同的报告。 |
| [^46] | [TIMELINE: Exhaustive Annotation of Temporal Relations Supporting the Automatic Ordering of Events in News Articles.](http://arxiv.org/abs/2310.17802) | 本文提出了一个新的注释方案，明确定义了时间关系的标准，并提出了自动化注释所有时间关系的方法。生成了一个新的数据集，TIMELINE语料库。 |
| [^47] | ["You Are An Expert Linguistic Annotator": Limits of LLMs as Analyzers of Abstract Meaning Representation.](http://arxiv.org/abs/2310.17793) | 本文研究了大型语言模型在分析句子的意义结构方面的成功和限制，发现模型在生成和重构抽象意义表示（AMR）方面表现出一定的能力，但在复杂的句子结构或语义推理任务中存在局限性。 |
| [^48] | [Utilizing Language Models for Energy Load Forecasting.](http://arxiv.org/abs/2310.17788) | 本文提出了一种利用语言模型进行能量负荷预测的新方法，通过采用提示技术和自回归生成方法，可以将能源消耗数据转化为描述性语句并实现预测未来能量负荷消耗的准确性，从而为提高能源效率和促进能源系统智能决策提供了有希望的途径。 |
| [^49] | [Evaluation of large language models using an Indian language LGBTI+ lexicon.](http://arxiv.org/abs/2310.17787) | 该论文提出了一种使用印度语LGBTI+词汇表评估大型语言模型的方法。研究发现，现有的语言模型无法有效检测潜藏的仇恨内容，使用机器翻译作为评估手段也存在局限性。 |
| [^50] | [Data-Centric Financial Large Language Models.](http://arxiv.org/abs/2310.17784) | 本文介绍了一种数据中心化的方法，通过预处理和预理解数据来改善大型语言模型（LLMs）在金融任务中的性能。实验证明，采用该方法的金融LLMs在金融分析和解释任务上达到了最先进的水平。 |
| [^51] | [Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?.](http://arxiv.org/abs/2310.17774) | 本研究通过对比不同分词方法对惊奇度估计和阅读时间数据的影响，发现使用BPE分词的预测在整体上与形态学和正字法分割相比没有受损，但细致分析指出了BPE分词的潜在问题，并提出了一种新的形态学预测评估方法。 |
| [^52] | [GROOViST: A Metric for Grounding Objects in Visual Storytelling.](http://arxiv.org/abs/2310.17770) | GROOViST是一种用于评估视觉故事中物体定位的新的评估工具，考虑了跨模态依赖、时间错位和人类对视觉定位的直觉。这种工具具有模块化设计，可以评估和解释每个组件的贡献。 |
| [^53] | [Social Contract AI: Aligning AI Assistants with Implicit Group Norms.](http://arxiv.org/abs/2310.17769) | 这项研究探索了通过将机器学习模型应用于用户交互数据，使AI助手能够自动对齐用户偏好的方法。研究发现，虽然AI助手在模拟中能够准确对齐经济文献中的标准策略，但在面对未知货币以及语言与策略一致性不足的情况下，其学习能力受到限制。 |
| [^54] | [A Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications.](http://arxiv.org/abs/2310.17750) | 本文提出了一个自动化测量大型语言模型（LLMs）及其相关产品和服务中有害责任的框架。通过该框架，可以调查不同的LLMs如何违反一系列与负责任人工智能（RAI）相关的原则，并促进LLMs的负责任使用。 |
| [^55] | [Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems.](http://arxiv.org/abs/2310.17749) | 这项研究探索了在对话式推荐系统中教育价值的作用，通过比较销售人员和SalesBot的性能，发现虽然SalesBot在流畅性和信息量方面接近专业销售人员，但在推荐质量方面仍存在差距。 |
| [^56] | [StyleBART: Decorate Pretrained Model with Style Adapters for Unsupervised Stylistic Headline Generation.](http://arxiv.org/abs/2310.17743) | StyleBART是一种无监督的风格化标题生成方法，通过使用适配器来装饰预训练模型，可以生成具有多样风格的标题。与其他方法不同，StyleBART将风格学习和标题生成任务分离开来，在推理过程中可以自由组合基础模型和风格适配器。经过广泛的评估，StyleBART表现出了优秀的性能。 |
| [^57] | [ArchBERT: Bi-Modal Understanding of Neural Architectures and Natural Languages.](http://arxiv.org/abs/2310.17737) | ArchBERT是一种双模态模型，可以联合学习和理解神经结构和自然语言，为神经架构和自动机器学习方法提供快速的结构-文本和文本-结构检索/生成服务。 |
| [^58] | [Investigating Multilingual Coreference Resolution by Universal Annotations.](http://arxiv.org/abs/2310.17734) | 本论文通过使用通用注释方法研究多语言共指消解任务。首先，通过研究不同语言层次和流派上的真实数据来了解多语言共指的特点。其次，进行错误分析，并提取特征用于改进共指消解任务。结果表明，我们的方法对基线系统有潜在好处。 |
| [^59] | [ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers.](http://arxiv.org/abs/2310.17723) | ZeroQuant-HERO是一种硬件增强的量化框架，针对W8A8 Transformer模型进行优化，旨在减少内存和计算需求，并在处理复杂的量化问题和内存受限运算符方面提供了新的解决方案。同时，该框架还具有灵活性，允许特定模块切换至FP16/BF16模式以提高准确性。 |
| [^60] | [Large Language Models as Generalizable Policies for Embodied Tasks.](http://arxiv.org/abs/2310.17722) | 本研究展示了大型语言模型(LLMs)可以被适应为具有普适性的机器人任务策略。通过我们的方法LLaRP，我们成功将预训练的冻结LLM用于接收指令和视觉输入，并在环境中直接输出动作。我们的实验结果表明LLaRP不仅对任务指令的复杂改写具有鲁棒性，而且可以推广到需要新颖最优行为的新任务。在大量未见任务中，LLaRP表现出了显著的成功率提升，并且我们提供了一个新的基准测试(Language Rearrangement)来促进进一步的研究。 |
| [^61] | [From Transcripts to Insights: Uncovering Corporate Risks Using Generative AI.](http://arxiv.org/abs/2310.17721) | 这项研究探索了使用生成型人工智能工具帮助投资者揭示企业风险的价值，通过从收益电话的上下文中生成风险摘要和评估，这些基于GPT的度量具有显著的信息内容，能够预测企业层面波动性和投资创新选择。此外，生成型人工智能还能有效检测新兴风险，并且这些度量在股权市场中起到定价作用。 |
| [^62] | [Outlier Dimensions Encode Task-Specific Knowledge.](http://arxiv.org/abs/2310.17715) | 异常维度可以编码关键的特定任务知识，并且一个单一的异常维度可以以最小的错误率完成下游任务。 |
| [^63] | [Nearest Neighbor Search over Vectorized Lexico-Syntactic Patterns for Relation Extraction from Financial Documents.](http://arxiv.org/abs/2310.17714) | 本文提出了一个简单而有效的方法，通过在测试时使用密集向量的词汇句法模式进行最近邻搜索，来解决关系抽取中的隐含表达和长尾关系类问题，并提供了一种简单可行的解决方法，特别适用于没有直接访问大型语言模型和/或基于监督训练或微调的基础设施的用户。 |
| [^64] | [Is Explanation the Cure? Misinformation Mitigation in the Short Term and Long Term.](http://arxiv.org/abs/2310.17711) | 本研究通过比较警告标签和GPT-4生成的最先进反事实解释的效果，探讨了这些解释对于缓解虚假信息的作用。实验结果显示，这两种干预方式都显著降低了参与者对虚假声明的信任。 |
| [^65] | [The impact of using an AI chatbot to respond to patient messages.](http://arxiv.org/abs/2310.17703) | 本研究首次考察了使用大型语言模型协助临床医生回答患者问题的实用性，并发现AI聊天机器人可以在不需要编辑的情况下提供可接受的草稿，显著提高了工作效率。 |
| [^66] | [CodeFusion: A Pre-trained Diffusion Model for Code Generation.](http://arxiv.org/abs/2310.17680) | CodeFusion是一种预训练的代码生成模型，通过扩散的方式解决了自然语言代码生成中遇到的限制，实验表明其在准确率和多样性上优于最先进的自回归系统。 |
| [^67] | [Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages.](http://arxiv.org/abs/2310.17526) | 本研究评估了GPT-4在多种语言的同行评审文献和灰色文献筛选和提取数据方面的能力，结果显示GPT-4在大多数任务上准确度与人类表现相当，在调整了偶然一致性和数据集不平衡后，其在数据提取方面表现出中等水平的准确度。 |
| [^68] | [The Expressive Power of Low-Rank Adaptation.](http://arxiv.org/abs/2310.17513) | 本文分析了低秩适应（LoRA）的表达能力，证明了对于全连接神经网络，当LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度）时，LoRA可以使任何模型f准确表示任何较小的目标模型f。对于Transformer网络，通过rank-（嵌入大小/ 2）的LoRA适配器可以使任何模型适应于相同大小的目标模型。 |
| [^69] | [Tackling the Matrix Multiplication Micro-kernel Generation with Exo.](http://arxiv.org/abs/2310.17408) | 介绍了使用Exo编译器生成接近于甚至优于手动开发的微内核的步骤和方法，并解决了为每个新硬件生成专用微内核的问题。 |
| [^70] | [Data Augmentation for Emotion Detection in Small Imbalanced Text Data.](http://arxiv.org/abs/2310.17015) | 通过研究数据增强技术在小规模不平衡数据集上的应用，我们发现使用增强后的数据训练分类器模型可以显著提高情感检测的性能。 |
| [^71] | [Accented Speech Recognition With Accent-specific Codebooks.](http://arxiv.org/abs/2310.15970) | 本研究提出了一种使用具有专门口音代码本的口音适应方法，通过交叉注意力和可训练代码本，用于端到端ASR系统。在实验证明了该方法在已见和未见的口音上都能获得显著的性能提升。 |
| [^72] | [Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review.](http://arxiv.org/abs/2310.14735) | 这篇论文解释了提示工程在释放大型语言模型能力方面的关键作用，探讨了不同的提示方法以及外部插件如何协助减少机器幻想，并指出了未来研究方向的重要性。 |
| [^73] | [MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation.](http://arxiv.org/abs/2310.14088) | MedEval是一个多层次、多任务和多领域的医学基准，用于促进医疗保健语言模型的开发。它包含了来自多个医疗系统的数据，涵盖了多个人体区域和检查模式。我们针对10个语言模型进行了评估，发现不同模型在不同任务上的有效性各有差异，从中我们注意到了指导的重要性。 |
| [^74] | [Towards Understanding Sycophancy in Language Models.](http://arxiv.org/abs/2310.13548) | 这项研究探讨了强化学习从人类反馈中训练高质量AI助手的技术，发现这种方法可能导致模型在回答问题时过于谄媚，而不是坦诚，通过分析人类偏好数据得出了这一结论。 |
| [^75] | [Open-ended Commonsense Reasoning with Unrestricted Answer Scope.](http://arxiv.org/abs/2310.11672) | 本论文研究了无限制答案范围的开放式常识推理问题。作者采用预训练语言模型在外部知识库上迭代检索推理路径，从而帮助找到最准确的答案。实验结果表明此方法在常识问题上取得了良好的效果。 |
| [^76] | [VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System.](http://arxiv.org/abs/2310.11069) | VoxArabica是一个稳健的方言感知阿拉伯语音识别系统，通过开发和演示，实现了阿拉伯语方言识别和自动语音识别。该系统训练了各种模型用于不同方言的识别，并提供了多种功能的网络界面。 |
| [^77] | [Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model.](http://arxiv.org/abs/2310.09520) | 该论文介绍了一种名为Reward-Augmented Decoding (RAD)的文本生成方法，使用小型的单向奖励模型来鼓励语言模型生成具有特定属性的文本。RAD在生成非有害和情感受控文本方面表现最佳，并且在非常大的语言模型上也很有效。 |
| [^78] | [EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling.](http://arxiv.org/abs/2310.04691) | EMO提出了地球移动距离优化（EMO）来解决语言模型中的退化现象。EMO利用了地球移动距离的特性，并引入了一个可行的上界来简化训练。经过评估，发现EMO在语言模型上有显著的改进。 |
| [^79] | [The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs.](http://arxiv.org/abs/2310.01468) | 本文提供了一个评估框架，通过向法官提出一系列查询来评估LLMs的对话推理和规划能力。我们发现不同的LLMs在这个任务上表现出显著差异。 |
| [^80] | [Minimum Bayes' Risk Decoding for System Combination of Grammatical Error Correction Systems.](http://arxiv.org/abs/2309.06520) | 本文提出了一个用于语法错误修正系统系统组合的最小贝叶斯风险解码方法，并通过实验证明了其有效性。 |
| [^81] | [Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity.](http://arxiv.org/abs/2309.06364) | 本文通过定性分析研究了大型语言模型生成的自由回答，重点考察了算法保真度，并提出高算法保真度可以推广到真实人类的观点。这对于使用语言模型研究人类行为具有重要意义。 |
| [^82] | [Embedding structure matters: Comparing methods to adapt multilingual vocabularies to new languages.](http://arxiv.org/abs/2309.04679) | 比较了替换跨语言词汇的几种技术，证明了单语转移文献中的方法不适用于多语言模型。专门化的较小词汇对于提高低资源情况下的性能是有效的。 |
| [^83] | [FairMonitor: A Four-Stage Automatic Framework for Detecting Stereotypes and Biases in Large Language Models.](http://arxiv.org/abs/2308.10397) | 本文引入了一个四阶段框架，可以直接评估大型语言模型（LLMs）生成内容中的刻板印象和偏见，包括直接询问测试、串行或适应性故事测试、隐性关联测试和未知情境测试。此外，还提出了多维评估指标和可解释的零样本提示，并以教育部门为案例研究构建了Edu-FairMonitor框架。 |
| [^84] | [ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP.](http://arxiv.org/abs/2308.02122) | ParaFuzz是一种基于可解释性的技术，用于检测自然语言处理中的毒样本。该技术通过观察模型在重写过的干净样本和污染样本上的预测稳定性，来判断样本是否被污染。 |
| [^85] | [Android in the Wild: A Large-Scale Dataset for Android Device Control.](http://arxiv.org/abs/2307.10088) | 这个论文提出了一个名为Android in the Wild (AITW)的大规模数据集，用于研究设备控制系统，该数据集包括人类示范的设备交互、自然语言指令和多种Android版本和设备类型。这个数据集提供了一个新的挑战，需要从视觉外观中推断用户界面中可用的操作。 |
| [^86] | [HYTREL: Hypergraph-enhanced Tabular Data Representation Learning.](http://arxiv.org/abs/2307.08623) | HYTREL是一种表格语言模型，通过使用超图来捕捉表格数据的排列不变性和其他三个结构属性。实证结果表明，HYTREL在四个下游任务中始终优于其他竞争基线模型，并且只需最少的预训练。 |
| [^87] | [DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs.](http://arxiv.org/abs/2307.04090) | 本论文提出了一种利用语义知识图自动创建政策辩论案例的方法，通过在争论的语义知识图上进行限制最短路径遍历，有效构建高质量的辩论案例。研究结果表明，在美国竞赛辩论中，利用这种方法显著改进了已有数据集DebateSum，并贡献了新的例子和有用的元数据。通过使用txtai语义搜索和知识图工具链，创建和贡献了9个语义知识图，同时提出了一种独特的评估方法来确定哪个知识图更适合政策辩论案例生成。 |
| [^88] | [Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision.](http://arxiv.org/abs/2306.16564) | 本文介绍了一种Pareto Optimal自监督框架，利用可用的编程监督将大型语言模型(LLM)的响应进行系统校准，通过为每个响应生成风险评分，而无需额外的手动工作。 |
| [^89] | [YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English Parallel Corpus.](http://arxiv.org/abs/2306.15162) | 本文介绍了一个大规模的、开放领域的美国手语-英语平行语料库YouTube-ASL，包含了约1000小时的美国手语视频和超过2500位独特的签名者。研究者在此语料库上训练了手语到英语翻译的模型，并在另一个数据集上实现了最新的最佳效果。 |
| [^90] | [Block-State Transformer.](http://arxiv.org/abs/2306.09539) | 本文提出了一种名为块状态变换器（BST）的混合神经网络层，结合了状态空间模型和块变换器，旨在在语言建模任务中提高性能和可扩展性。实验证明，该模型在语言建模困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，在层级别上具有超过十倍的速度提升。 |
| [^91] | [PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model.](http://arxiv.org/abs/2306.02531) | 本文提出了PLANNER模型，它通过将自回归生成和潜在语义扩散相结合，以在生成文本时在段落级别实现全局控制，生成流畅的文本。 |
| [^92] | [Resolving Interference When Merging Models.](http://arxiv.org/abs/2306.01708) | 本文揭示了现有模型合并技术存在的干扰问题，提出了具有广泛适用性的解决方案，可显着提高合并后模型的性能。 |
| [^93] | [Complex Query Answering on Eventuality Knowledge Graph with Implicit Logical Constraints.](http://arxiv.org/abs/2305.19068) | 本文提出了一个新的框架，利用神经方法回答基于事件性知识图的复杂逻辑查询，满足传统的一阶逻辑约束以及有关事件性发生和顺序的隐含逻辑约束。 |
| [^94] | [Visual Programming for Text-to-Image Generation and Evaluation.](http://arxiv.org/abs/2305.15328) | 本文提出了两种新颖的可解释/可理解的图像编程框架，用于文本到图像（T2I）的生成和评估。首先，引入了VPGen，一个可解释的逐步T2I生成框架，将T2I生成分解为三个步骤，通过语言模型处理前两个步骤，提供了比端到端模型更强的空间控制能力，并利用了预训练语言模型的世界知识。 |
| [^95] | [Machine Reading Comprehension using Case-based Reasoning.](http://arxiv.org/abs/2305.14815) | 本文提出了一种基于案例推理的机器阅读理解方法（CBR-MRC），通过从存储器中检索相似案例并选择最类似的上下文来预测答案，以达到高准确性。在自然语言问题和新闻问答中，CBR-MRC的准确性超过基准，并且能够识别与其他评估员不同的答案。 |
| [^96] | [A Causal View of Entity Bias in (Large) Language Models.](http://arxiv.org/abs/2305.14695) | 研究提出了一种结构因果模型（SCM）并提供因果干预技术，以缓解大型语言模型中的实体偏见，从而减少偏见信息，同时保留相似实体的共同预测信息。 |
| [^97] | [INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback.](http://arxiv.org/abs/2305.14282) | INSTRUCTSCORE是一个可解释的文本生成评估度量，通过利用明确的人类指令和GPT-4的隐式知识，它能生成生成文本的分数和人类可读的诊断报告，达到与最先进度量相当的性能水平。 |
| [^98] | [Exploring Chain-of-Thought Style Prompting for Text-to-SQL.](http://arxiv.org/abs/2305.14215) | 本文研究了通过思维链式提示来提升大型语言模型在文本到SQL解析中的推理能力。实验表明，迭代提示法可能是不必要的，并且详细的推理步骤容易出现错误传播问题。在此基础上，提出了一种新的CoT风格提示方法，显著提高了文本到SQL解析的性能。 |
| [^99] | [Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document.](http://arxiv.org/abs/2305.13850) | 这篇论文提出了一种结合全局结构知识的连续迭代的方式去捕获实体之间的依赖关系，以提高视觉丰富文档中关系抽取的准确性。 |
| [^100] | [Can Large Language Models Infer and Disagree Like Humans?.](http://arxiv.org/abs/2305.13788) | 本文研究了大型语言模型在自然语言推断方面的性能和与人类分歧分布的对齐情况。结果表明LLM的推断能力有限，无法捕捉到人类分歧分布，引发了对其NLU和代表人类用户性质的担忧。 |
| [^101] | [On the Risk of Misinformation Pollution with Large Language Models.](http://arxiv.org/abs/2305.13661) | 本文探讨了大型语言模型（LLM）可能误用的潜在风险，指出LLM可以作为有效的误导性信息生成器，导致开放域问答（ODQA）系统性能显著降低，并尝试提出三种防御策略：提示，误报检测和大多数投票。 |
| [^102] | [clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents.](http://arxiv.org/abs/2305.13455) | 本论文探讨了大型语言模型在接触具有挑战性的受限游戏式环境下，能否有意义地评估它们的能力。作为概念验证，研究了五种交互设置，表明当前的聊天优化LLMs在一定程度上能够遵循游戏玩法指令。这对于将LLMs开发为具有广泛适用性的对话代理人具有启示作用。 |
| [^103] | [MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup.](http://arxiv.org/abs/2305.12029) | 本研究提出了MultiTurnCleanup任务，收集了新的数据集MultiTurnCleanup1，针对口语会话转录中的不连续现象进行探讨并提供了两个可用于未来研究的基准测试模型。 |
| [^104] | [mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences.](http://arxiv.org/abs/2305.11129) | mLongT5是一种多语言高效文本-文本Transformer，适用于处理较长序列的输入。它在多语言摘要和问答任务中表现更好。 |
| [^105] | [ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing.](http://arxiv.org/abs/2305.09770) | ConvXAI是一个基于对话的XAI系统，它集成了多种XAI类型，并将实际用户需求嵌入设计中，以提高实用性。 |
| [^106] | [Investigating the effect of sub-word segmentation on the performance of transformer language models.](http://arxiv.org/abs/2305.05480) | 本文研究使用单语词段算法StateMorph训练语言模型时，可以使模型更高效地收敛并获得更好的验证分数。 |
| [^107] | [Company classification using zero-shot learning.](http://arxiv.org/abs/2305.01028) | 本文提出了一种利用自然语言处理和零样本学习的方法来进行公司分类的方法。该方法可以简化公司分类过程，从而减少传统方法如全球产业分类标准（GICS）所需的时间和资源。 |
| [^108] | [Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent.](http://arxiv.org/abs/2304.09542) | 本文研究了生成性LLMs，如ChatGPT和GPT-4在信息检索中的相关性排名能力，实验结果表明，这些模型经适当指导后表现优异，有时甚至优于传统监督学习方法。将ChatGPT的排名能力提炼为专门模型在BEIR上的效果更优。 |
| [^109] | [Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy.](http://arxiv.org/abs/2304.02247) | 本文提出了一种通过文档层次结构诱导来检测新闻中的政治偏见的方法，该方法克服了过拟合和有限的普适性，展现了更好的鲁棒性和准确性。 |
| [^110] | [eP-ALM: Efficient Perceptual Augmentation of Language Models.](http://arxiv.org/abs/2303.11403) | 本论文提出了一种用对比学习提高语言模型的感知能力的高效方法eP-ALM，可以实现视觉感知信息和文本信息的融合，同时还能在多模态基准测试上实现最先进的结果。 |
| [^111] | [Learning to reason over visual objects.](http://arxiv.org/abs/2303.02260) | 本研究调查了以对象为中心的处理视觉场景的通用机制对于提升抽象视觉推理的作用，并发现在RPM-like的基准测试中，这样的模型取得了最先进的结果。 |
| [^112] | [90% F1 Score in Relational Triple Extraction: Is it Real ?.](http://arxiv.org/abs/2302.09887) | 该论文对联合实体和关系提取模型进行了基准研究，发现在更真实的实验设置下，模型的F1得分显著下降。此外，提出了一种利用基于BERT的分类器的两步建模方法。 |
| [^113] | [Vicarious Offense and Noise Audit of Offensive Speech Classifiers: Unifying Human and Machine Disagreement on What is Offensive.](http://arxiv.org/abs/2301.12534) | 本文通过人工和机器审核员的共情冒犯和噪声审计研究了冒犯性言论检测中的差异性。结果表明，审核员之间存在广泛的分歧，并且人工审核员和大型语言模型分类器无法预测其他审核员的回应。这对于内容审核具有重要意义。 |
| [^114] | [How Does Beam Search improve Span-Level Confidence Estimation in Generative Sequence Labeling?.](http://arxiv.org/abs/2212.10767) | 本文研究了在生成序列标记任务中如何改善对跨度级别置信度的估计。研究发现仅仅使用解码器的输出概率并不是最佳方法，而利用束搜索的前k个预测的统计数据可以显著降低校准误差。 |
| [^115] | [Benchmarking Spatial Relationships in Text-to-Image Generation.](http://arxiv.org/abs/2212.10015) | 本文研究了文本到图像生成中模型生成正确空间关系的能力，并提出了一个评估指标VISOR以衡量生成图像的准确性。实验发现当前T2I模型尽管可以生成高度逼真的图像，但其空间上准确的图像能力仍然不足，特别是在空间谓词和场景关系理解方面。 |
| [^116] | [LR-Sum: Summarization for Less-Resourced Languages.](http://arxiv.org/abs/2212.09674) | 该论文介绍了LR-Sum项目，这是一个针对资源有限语言的新的摘要数据集，旨在促进自动摘要研究。数据集包含40种资源有限语言的人工撰写的摘要，并根据Creative Commons许可证发布，是最开放许可的多语言摘要数据集之一。 |
| [^117] | [Language models show human-like content effects on reasoning tasks.](http://arxiv.org/abs/2207.07051) | 本研究探讨了语言模型在逻辑推理任务中是否像人类一样通过混入内容来影响答案，结果发现大型语言模型的先验期望能够捕捉到这种特征。 |

# 详细

[^1]: FP8-LM：训练FP8大语言模型

    FP8-LM: Training FP8 Large Language Models. (arXiv:2310.18313v1 [cs.LG])

    [http://arxiv.org/abs/2310.18313](http://arxiv.org/abs/2310.18313)

    本文提出了一种用于训练大语言模型的新型FP8自动混合精度框架，能够在不影响模型准确性的情况下显著减少内存使用并提高训练速度。

    

    本文探讨了用于高效训练大语言模型（LLMs）的FP8低比特数据格式。我们的关键洞察是，在LLM训练中，大多数变量（如梯度和优化器状态）可以使用低精度数据格式，而不会影响模型准确性，并且不需要改变超参数。具体地，我们提出了一种新的FP8自动混合精度框架用于训练LLMs。该框架为LLM的混合精度和分布式并行训练提供了三个级别的FP8利用。它逐步引入8位梯度，优化器状态和分布式学习。实验结果表明，在H100 GPU平台上训练GPT-175B模型期间，我们的FP8混合精度训练框架不仅实现了显著的42%的真实内存使用减少，而且比广泛采用的BF16框架（即Megatron-LM）运行速度快64%，比Nvidia Transformer Engine快17%。

    In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 42% reduction in real memory usage but also ran 64% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 17%. This l
    
[^2]: 一种自动生成谜题以辅助概念理解的方法

    An Approach to Automatically generating Riddles aiding Concept Attainment. (arXiv:2310.18290v1 [cs.CL])

    [http://arxiv.org/abs/2310.18290](http://arxiv.org/abs/2310.18290)

    这个论文介绍了一种自动生成概念谜题的方法，以促进在线学习环境中的学习者参与度。通过应用概念达成模型和生成谜题，该方法可以帮助学习者更好地理解概念。

    

    在在线学习环境中，保持学习者的积极参与是一个主要的挑战。为增强学习者的参与度，提出了各种不同的教学策略，无论是在线还是离线环境中。概念达成模型就是一种教学策略，它着重于学习者对概念的深入理解，而不仅仅是对概念的字典定义。通过搜索和列举用于区分各种概念的实例和非实例之间的属性，来达到这一目的。我们的工作试图将概念达成模型应用于构建概念谜题，以在在线学习环境中使用。该方法涉及从学习资源中创建事实三元组，根据其对概念的唯一性进行分类为“主题标记”和“共同”，然后根据概念达成模型的格式生成谜题，并捕获这些谜题的所有可能解。从人类评估中获得的结果显示...

    One of the primary challenges in online learning environments, is to retain learner engagement. Several different instructional strategies are proposed both in online and offline environments to enhance learner engagement. The Concept Attainment Model is one such instructional strategy that focuses on learners acquiring a deeper understanding of a concept rather than just its dictionary definition. This is done by searching and listing the properties used to distinguish examples from non-examples of various concepts. Our work attempts to apply the Concept Attainment Model to build conceptual riddles, to deploy over online learning environments. The approach involves creating factual triples from learning resources, classifying them based on their uniqueness to a concept into `Topic Markers' and `Common', followed by generating riddles based on the Concept Attainment Model's format and capturing all possible solutions to those riddles. The results obtained from the human evaluation of r
    
[^3]: MalFake: 使用循环神经网络和VGG-16的多模态马拉雅拉姆语假新闻识别

    MalFake: A Multimodal Fake News Identification for Malayalam using Recurrent Neural Networks and VGG-16. (arXiv:2310.18263v1 [cs.CL])

    [http://arxiv.org/abs/2310.18263](http://arxiv.org/abs/2310.18263)

    这篇论文提出了使用循环神经网络和VGG-16对马拉雅拉姆语的假新闻进行多模态识别的方法，这是首次在该语言中进行多模态特征提取和分类的工作。

    

    在线新闻的消费量近年来大幅增加。由于某些网站的快速发布和缺乏编辑标准，假新闻在地方语言如马拉雅拉姆语中越来越普遍。假新闻可能对社会产生可怕的影响，导致人们做出错误判断，对权威失去信任，甚至参与暴力行为。在印度的语境下，有很多地方语言，假新闻在每种语言中都在传播。因此，提供有效的技术来识别地方语言中的假信息至关重要。迄今为止，在马拉雅拉姆语中，几乎没有关于提取多模态特征来分类假新闻的工作。多模态方法在检测假新闻方面更准确，因为从多种模态中提取特征构建深度学习分类模型。据我们所知，这是首次在马拉雅拉姆语中使用多模态特征来识别假新闻的工作。

    The amount of news being consumed online has substantially expanded in recent years. Fake news has become increasingly common, especially in regional languages like Malayalam, due to the rapid publication and lack of editorial standards on some online sites. Fake news may have a terrible effect on society, causing people to make bad judgments, lose faith in authorities, and even engage in violent behavior. When we take into the context of India, there are many regional languages, and fake news is spreading in every language. Therefore, providing efficient techniques for identifying false information in regional tongues is crucial. Until now, little to no work has been done in Malayalam, extracting features from multiple modalities to classify fake news. Multimodal approaches are more accurate in detecting fake news, as features from multiple modalities are extracted to build the deep learning classification model. As far as we know, this is the first piece of work in Malayalam that use
    
[^4]: 使用形式方法反馈调整语言模型

    Fine-Tuning Language Models Using Formal Methods Feedback. (arXiv:2310.18239v1 [cs.AI])

    [http://arxiv.org/abs/2310.18239](http://arxiv.org/abs/2310.18239)

    该论文介绍了一种使用形式方法反馈调整语言模型的自动化方法，用于在自主系统中微调预先训练的模型。通过合成基于自动机的控制器并与给定规范进行验证，该方法可以减少人工反馈的成本并实现领域特定任务的控制策略生成。

    

    虽然预先训练的语言模型可以编码对规划和控制有益的通用知识，但它们可能无法为特定领域任务生成适当的控制策略。现有的微调方法使用人工反馈来解决这个限制，然而，获取人工反馈是劳动密集型和昂贵的。我们提出了一种全自动的方法，用于微调预先训练的语言模型，应用于自主系统，弥合通用知识与特定领域要求之间的差距，同时降低成本。该方法通过自然语言任务描述来引导从预先训练模型中合成基于自动机的控制器。这些控制器在一个世界模型中与独立提供的规范进行验证，该世界模型可以是抽象的或来自高保真度的模拟器。与期望规范高度一致的控制器获得更高的排名，引导迭代的微调过程。我们提供了定量证据，主要是

    Although pre-trained language models encode generic knowledge beneficial for planning and control, they may fail to generate appropriate control policies for domain-specific tasks. Existing fine-tuning methods use human feedback to address this limitation, however, sourcing human feedback is labor intensive and costly. We present a fully automated approach to fine-tune pre-trained language models for applications in autonomous systems, bridging the gap between generic knowledge and domain-specific requirements while reducing cost. The method synthesizes automaton-based controllers from pre-trained models guided by natural language task descriptions. These controllers are verifiable against independently provided specifications within a world model, which can be abstract or obtained from a high-fidelity simulator. Controllers with high compliance with the desired specifications receive higher ranks, guiding the iterative fine-tuning process. We provide quantitative evidences, primarily 
    
[^5]: Davidsonian场景图：改进文本-图像生成的细粒度评估的可靠性

    Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation. (arXiv:2310.18235v1 [cs.CV])

    [http://arxiv.org/abs/2310.18235](http://arxiv.org/abs/2310.18235)

    本论文提出了Davidsonian场景图（DSG）的评估框架，解决了现有文本-图像生成模型评估中的可靠性挑战，包括QG问题的准确性和VQA答案的一致性。

    

    评估文本到图像模型一直是困难的。最近一种用于评估文本-图像忠实度的强大方法是基于QG/A（问题生成和回答），它使用预训练的基础模型自动生成一组问题和答案，并基于这些答案与基于提示的答案在视觉问题回答模型中提取的一致性对输出图像进行评分。这种评估自然上取决于底层QG和QA模型的质量。我们确定并解决了现有QG/A工作中的几个可靠性挑战：（a）QG问题应尊重提示（避免幻觉、重复和遗漏）和（b）VQA答案应一致（不会在图像中宣称没有摩托车，同时声称摩托车是蓝色）。我们通过Davidsonian场景图（DSG），这个受形式语义启发的实证评估框架，解决了这些问题。

    Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG
    
[^6]: 回顾性修订：阅读中的回退和跳过作为增量处理中修订策略的认知信号

    Revising with a Backward Glance: Regressions and Skips during Reading as Cognitive Signals for Revision Policies in Incremental Processing. (arXiv:2310.18229v1 [cs.CL])

    [http://arxiv.org/abs/2310.18229](http://arxiv.org/abs/2310.18229)

    本研究调查了人类阅读眼动追踪数据中的回退和跳过作为增量序列标记中修订策略的信息信号的适用性，并发现其可能作为修订的有用预测因素。

    

    在自然语言处理中，增量处理器根据语言输入的前缀产生输出。一些标记触发修订，导致对输出假设进行编辑。然而，目前对于为何模型进行修订的知识还很有限。检测修订应该发生的时间步骤的策略可以提高效率。然而，训练修订策略的合适信号的获取是一个开放性问题，因为在数据集中自然不可用。在这项工作中，我们研究了人类阅读的眼动跟踪数据中的回退和跳过在增量序列标记中作为修订策略的信息信号的适用性。通过使用广义混合效应模型，我们发现人类的回退和跳过的概率可能作为BiLSTMs和Transformer模型中修订的有用预测因素，而对于不同语言的结果保持一致。

    In NLP, incremental processors produce output in instalments, based on incoming prefixes of the linguistic input. Some tokens trigger revisions, causing edits to the output hypothesis, but little is known about why models revise when they revise. A policy that detects the time steps where revisions should happen can improve efficiency. Still, retrieving a suitable signal to train a revision policy is an open problem, since it is not naturally available in datasets. In this work, we investigate the appropriateness of regressions and skips in human reading eye-tracking data as signals to inform revision policies in incremental sequence labelling. Using generalised mixed-effects models, we find that the probability of regressions and skips by humans can potentially serve as useful predictors for revisions in BiLSTMs and Transformer models, with consistent results for various languages.
    
[^7]: ArcheType：一种使用大型语言模型进行开源列类型注释的新框架

    ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models. (arXiv:2310.18208v1 [cs.CL])

    [http://arxiv.org/abs/2310.18208](http://arxiv.org/abs/2310.18208)

    ArcheType是一种使用大型语言模型进行开源列类型注释的新框架，通过改进上下文采样和标签重新映射，实现了全面的零样本解决方案。

    

    现有的深度学习方法在语义列类型注释（CTA）方面存在重要缺点：它们依赖于在训练时固定的语义类型；需要大量的每个类型的训练样本并产生大量运行时推断成本；即使类型保持不变，它们的性能也可能在评估新数据集时下降。大型语言模型在广泛的任务上展示了强大的零样本分类性能，本文探讨了它们在CTA中的应用。我们介绍了ArcheType，一种简单实用的方法，用于上下文采样、提示序列化、模型查询和标签重新映射，从而使大型语言模型能够完全以零样本方式解决列类型注释问题。我们分别对我们方法的每个组成部分进行了分析，并确定出改进上下文采样和标签重新映射提供了最一致的性能提升。

    Existing deep-learning approaches to semantic column type annotation (CTA) have important shortcomings: they rely on semantic types which are fixed at training time; require a large number of training samples per type and incur large run-time inference costs; and their performance can degrade when evaluated on novel datasets, even when types remain constant. Large language models have exhibited strong zero-shot classification performance on a wide range of tasks and in this paper we explore their use for CTA. We introduce ArcheType, a simple, practical method for context sampling, prompt serialization, model querying, and label remapping, which enables large language models to solve column type annotation problems in a fully zero-shot manner. We ablate each component of our method separately, and establish that improvements to context sampling and label remapping provide the most consistent gains. ArcheType establishes new state-of-the-art performance on both zero-shot and fine-tuned C
    
[^8]: INA：一种基于奖励的对话系统来增强谈判策略的综合方法

    INA: An Integrative Approach for Enhancing Negotiation Strategies with Reward-Based Dialogue System. (arXiv:2310.18207v1 [cs.CL])

    [http://arxiv.org/abs/2310.18207](http://arxiv.org/abs/2310.18207)

    本文提出了一种名为INA的综合谈判对话代理，它能够在价格以及其他因素上进行谈判，为用户提供更具灵活性和全面性的谈判体验。

    

    本文提出了一种面向在线市场的新型谈判对话代理。我们的代理具有整合性的特点，即它具有在价格谈判以及其他因素（如交易捆绑包的添加或删除）上进行谈判的能力，从而提供更灵活、全面的谈判体验。我们创建了一个名为综合谈判数据集（IND）的新数据集，以实现这种功能。为了创建这个数据集，我们引入了一种新的半自动数据创建方法，结合定义谈判意图、动作以及用户和代理之间的意图-动作模拟，生成潜在的对话流程。最后，我们使用目前最先进的语言模型GPT-J进行提示，为给定的意图生成对话，并通过人在环节的后编辑和修正来确保高质量的数据。我们采用一组新颖的奖励机制，专门针对谈判任务进行训练。

    In this paper, we propose a novel negotiation dialogue agent designed for the online marketplace. Our agent is integrative in nature i.e, it possesses the capability to negotiate on price as well as other factors, such as the addition or removal of items from a deal bundle, thereby offering a more flexible and comprehensive negotiation experience. We create a new dataset called Integrative Negotiation Dataset (IND) to enable this functionality. For this dataset creation, we introduce a new semi-automated data creation method, which combines defining negotiation intents, actions, and intent-action simulation between users and the agent to generate potential dialogue flows. Finally, the prompting of GPT-J, a state-of-the-art language model, is done to generate dialogues for a given intent, with a human-in-the-loop process for post-editing and refining minor errors to ensure high data quality. We employ a set of novel rewards, specifically tailored for the negotiation task to train our Ne
    
[^9]: 在跨语言社交媒体中，迷失而重现：识别言论的跨语言处理

    Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social Media. (arXiv:2310.18205v1 [cs.CL])

    [http://arxiv.org/abs/2310.18205](http://arxiv.org/abs/2310.18205)

    本论文介绍了一种在多语言社交媒体中识别言论的方法，通过创造一个新颖的多语言数据集X-CLAIM，采用编码器-只语言模型和生成性大语言模型进行实验，并展示了在多种语言上训练的好处。

    

    言论跨度识别（CSI）是事实核查流程中的重要步骤，旨在识别社交媒体帖子中包含有待检查的言论或断言的文本片段。尽管对新闻记者和人工事实核查者来说非常重要，但它仍然是一个严重被研究不足的问题，迄今为止，关于这个主题的研究非常有限，只专注于英语。因此，我们利用来自印度五种语言和英语的多个社交媒体平台的真实信息，创造了一个新颖的数据集X-CLAIM，其中包含7K个现实世界中的言论。我们使用最先进的编码器-只语言模型（如XLM-R）报告强大的基准，并展示了在多种语言上进行训练的好处，相比于零翻译或从高资源语言（如英语）训练的替代跨语言转换方法。我们还在X-CLAIM数据集上使用GPT系列的生成性大语言模型，并使用提示方法进行评估，发现它们的效果。

    Claim span identification (CSI) is an important step in fact-checking pipelines, aiming to identify text segments that contain a checkworthy claim or assertion in a social media post. Despite its importance to journalists and human fact-checkers, it remains a severely understudied problem, and the scarce research on this topic so far has only focused on English. Here we aim to bridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K real-world claims collected from numerous social media platforms in five Indian languages and English. We report strong baselines with state-of-the-art encoder-only language models (e.g., XLM-R) and we demonstrate the benefits of training on multiple languages over alternative cross-lingual transfer methods such as zero-shot transfer, or training on translated data, from a high-resource language such as English. We evaluate generative large language models from the GPT series using prompting methods on the X-CLAIM dataset and we find that they
    
[^10]: 基于条件韵律层标准化扩散GAN的风格描述文本转语音

    Style Description based Text-to-Speech with Conditional Prosodic Layer Normalization based Diffusion GAN. (arXiv:2310.18169v1 [cs.SD])

    [http://arxiv.org/abs/2310.18169](http://arxiv.org/abs/2310.18169)

    本文介绍了一种基于Diffusion GAN的方法，利用条件韵律层标准化将风格嵌入到生成架构中，从而实现根据风格描述和内容文本生成高质量语音。

    

    本文提出了一种基于扩散GAN的方法（Prosodic Diff-TTS），根据风格描述和内容文本生成相应的高保真语音。它利用了新颖的条件韵律层标准化，将风格嵌入到基于多头注意力的音素编码器和基于梅尔频谱图的解码器的生成器架构中，从而生成语音。风格嵌入是通过在辅助任务（如音高、语速、情感、性别分类）上微调预训练的BERT模型来生成的。我们使用多个定量指标来衡量生成准确性和MOS，并在多说话人的LibriTTS和PromptSpeech数据集上展示了我们提出的架构的有效性。

    In this paper, we present a Diffusion GAN based approach (Prosodic Diff-TTS) to generate the corresponding high-fidelity speech based on the style description and content text as an input to generate speech samples within only 4 denoising steps. It leverages the novel conditional prosodic layer normalization to incorporate the style embeddings into the multi head attention based phoneme encoder and mel spectrogram decoder based generator architecture to generate the speech. The style embedding is generated by fine tuning the pretrained BERT model on auxiliary tasks such as pitch, speaking speed, emotion,gender classifications. We demonstrate the efficacy of our proposed architecture on multi-speaker LibriTTS and PromptSpeech datasets, using multiple quantitative metrics that measure generated accuracy and MOS.
    
[^11]: 使用人设来建模语言模型中的真实性

    Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])

    [http://arxiv.org/abs/2310.18168](http://arxiv.org/abs/2310.18168)

    本研究探讨了在大型语言模型中使用人设来建模真实性的可能性。通过建模真实人设，语言模型可以将真实性推广到不同上下文中，并通过相关特征判断个体产生文本的真实性。

    

    大型语言模型使用互联网上的大量文本进行训练，这些文本中既包含了事实，也包含了误导性的信息。语言模型能够从这些相互矛盾的数据中辨别真实与虚假吗？基于语言模型能够建模不同产生文本的个体这一观点，我们假设它们可以通过建模真实人设来聚类真实文本：一群很可能产生真实文本并具有相似特征的个体。例如，可信源如维基百科和科学期刊通常使用正式的写作风格并提出一致的主张。通过建模这一人设，语言模型可以将真实性推广到每个个体生成训练文本的特定上下文之外。例如，模型可以推断出“维基百科”这个个体在“科学”生成的主题上会表现出真实性，因为它们共享一个人设。我们首先通过两个观察结果为人设假设提供了证据：（1）我们可以探测模型在不同领域中判断真实性的能力；（2）模型可以从相关特征中推测个体产生文本的真实性。

    Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent "Wikipedia" will behave truthfully on topics that were only generated by "Science" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a mod
    
[^12]: MPrompt: 探索用于机器阅读理解的多级提示调整

    MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension. (arXiv:2310.18167v1 [cs.CL])

    [http://arxiv.org/abs/2310.18167](http://arxiv.org/abs/2310.18167)

    MPrompt是一种用于机器阅读理解的多级提示调整方法，通过在任务特定、领域特定和上下文特定级别上利用提示来增强不同细粒度的输入语义理解，并通过独立性约束避免冗余。

    

    大型语言模型在各种自然语言任务上取得了优异的性能。这种方法的一个主要缺点是在对新数据集进行微调时需要大量资源。软提示调整提供了一种资源高效的解决方法，可以在保持预训练语言模型（PLM）固定权重的同时进行微调。现有的软提示方法主要关注设计与新数据集领域匹配的与输入无关的提示。这些方法通常忽略了任务和文本上下文的细粒度信息。在本文中，我们提出了一种多级提示调整（MPrompt）方法用于机器阅读理解。它在任务特定、领域特定和上下文特定级别上利用提示来增强不同细粒度的输入语义理解。我们还提出了一个独立性约束，使每个领域特定的提示集中在其领域内的信息，避免冗余。

    The large language models have achieved superior performance on various natural language tasks. One major drawback of such approaches is they are resource-intensive in fine-tuning new datasets. Soft-prompt tuning presents a resource-efficient solution to fine-tune the pre-trained language models (PLMs) while keeping their weight frozen. Existing soft prompt methods mainly focus on designing the input-independent prompts that steer the model to fit the domain of the new dataset. Those methods often ignore the fine-grained information about the task and context of the text. In this paper, we propose a multi-level prompt tuning (MPrompt) method for machine reading comprehension. It utilizes prompts at task-specific, domain-specific, and context-specific levels to enhance the comprehension of input semantics at different granularities. We also propose an independence constraint to steer each domain-specific prompt to focus on information within its domain to avoid redundancy. Moreover, we 
    
[^13]: 通过单词的听觉信息提升代码混合文本处理能力

    Elevating Code-mixed Text Handling through Auditory Information of Words. (arXiv:2310.18155v1 [cs.CL])

    [http://arxiv.org/abs/2310.18155](http://arxiv.org/abs/2310.18155)

    这篇论文提出了一种通过利用单词的听觉信息来处理代码混合文本的方法。该方法通过在预训练阶段使用SOUNDEX表示，以及采用新的输入数据提供方法，有效解决了处理代码混合文本中的拼写变体问题。实验证明了该方法的有效性。

    

    随着代码混合数据的日益流行，对这种类型的数据有更好的处理方式的需求也越来越大，这种数据存在一些挑战，如处理拼写变体、不同语言、不同脚本以及资源匮乏。当前的语言模型在处理代码混合数据时面临困难，因为它们主要关注单词的语义表示，而忽视了听觉音标特征。这导致处理代码混合文本中的拼写变体时出现困难。在本文中，我们提出了一种有效的方法，利用从SOUNDEX中提取的单词的听觉信息来创建用于处理代码混合文本数据的语言模型。我们的方法包括基于掩码语言建模的预训练步骤，其中包括SOUNDEX表示（SAMLM）和一种向预训练模型提供输入数据的新方法。通过对各种代码混合数据集（不同语言）进行情绪、冒犯和攻击实验，我们验证了我们的方法的有效性。

    With the growing popularity of code-mixed data, there is an increasing need for better handling of this type of data, which poses a number of challenges, such as dealing with spelling variations, multiple languages, different scripts, and a lack of resources. Current language models face difficulty in effectively handling code-mixed data as they primarily focus on the semantic representation of words and ignore the auditory phonetic features. This leads to difficulties in handling spelling variations in code-mixed text. In this paper, we propose an effective approach for creating language models for handling code-mixed textual data using auditory information of words from SOUNDEX. Our approach includes a pre-training step based on masked-language-modelling, which includes SOUNDEX representations (SAMLM) and a new method of providing input data to the pre-trained model. Through experimentation on various code-mixed datasets (of different languages) for sentiment, offensive and aggressio
    
[^14]: 使用大型语言模型进行文本属性图的解缠表征学习

    Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs. (arXiv:2310.18152v1 [cs.CL])

    [http://arxiv.org/abs/2310.18152](http://arxiv.org/abs/2310.18152)

    本文提出了一个名为Disentangled Graph-Text Learner (DGTL)的模型，通过引入定制的解缠图神经网络（GNN）层，使得大型语言模型（LLMs）能够更好地理解文本属性图（TAGs）中的复杂结构关系。

    

    文本属性图（TAGs）在网络上非常常见，对于该类图，如引用网络、电子商务网络和社交网络的研究在网络社区中引起了相当大的关注。最近，大型语言模型（LLMs）在各种任务上展示了出色的能力。然而，现有的工作仅仅依靠提示信息来传达图结构信息给LLMs，因此对于TAGs中复杂的结构关系了解不足。为解决这个问题，本文提出了解缠图文学习器（DGTL）模型，能够增强LLMs对TAGs的推理和预测能力。我们的DGTL模型通过定制的解缠图神经网络（GNN）层将图结构信息纳入其中，使得LLMs能够捕捉多个结构因素中隐藏的复杂关系。

    Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthe
    
[^15]: DELPHI: 评估LLMs在处理争议问题中的表现的数据

    DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial Issues. (arXiv:2310.18130v1 [cs.CL])

    [http://arxiv.org/abs/2310.18130](http://arxiv.org/abs/2310.18130)

    提出了一个评估LLMs在处理争议问题中表现的数据集，并使用其中的子集对不同的LLMs进行评估，阐明它们如何处理争议问题以及采取的立场。

    

    争议是我们时代的一种反映，并且是任何言论的重要方面。大型语言模型(LLMs)作为对话系统的兴起，增加了公众对这些系统回答各种问题的依赖。因此，系统地考察这些模型如何回答涉及正在进行的辩论的问题是至关重要的。然而，目前很少有这样的数据集提供人类注释的标签，反映当前的讨论。为了促进这个领域的研究，我们提出了一个新的有争议性问题数据集构建方法，基于已经公开发布的Quora问题对数据集的扩展。该数据集提出了涉及知识时效性、安全性、公平性和偏见的挑战。我们使用该数据集的一个子集来评估不同的LLMs，阐明它们如何处理争议问题以及采取的立场。这项研究最终有助于我们理解LLMs与争议问题的互动，并为未来研究铺平了道路。

    Controversy is a reflection of our zeitgeist, and an important aspect to any discourse. The rise of large language models (LLMs) as conversational systems has increased public reliance on these systems for answers to their various questions. Consequently, it is crucial to systematically examine how these models respond to questions that pertaining to ongoing debates. However, few such datasets exist in providing human-annotated labels reflecting the contemporary discussions. To foster research in this area, we propose a novel construction of a controversial questions dataset, expanding upon the publicly released Quora Question Pairs Dataset. This dataset presents challenges concerning knowledge recency, safety, fairness, and bias. We evaluate different LLMs using a subset of this dataset, illuminating how they handle controversial issues and the stances they adopt. This research ultimately contributes to our understanding of LLMs' interaction with controversial issues, paving the way f
    
[^16]: 提问更多，了解更多：利用大型语言模型强化学习的决策问题与思维链

    Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models. (arXiv:2310.18127v1 [cs.LG])

    [http://arxiv.org/abs/2310.18127](http://arxiv.org/abs/2310.18127)

    本文提出了一种利用大型语言模型的强化学习框架，能够学习提问相关问题并进行推理来指导在实际环境中执行的行为的学习。

    

    大型语言模型通过将基于行动的策略与思维链（CoT）推理相结合，展示了解决复杂实际挑战的潜力。然而，对于该框架的有效性来说，具有高质量的提示非常重要。目前，这些提示是通过广泛使用人力手工制作的，导致CoT策略经常无法推广。为了确保低层控制器适当地处理CoT推理，还需要人为介入来开发接地函数。在本文中，我们迈出了迈向在复杂推理中应用实际环境中的任务解决的完全集成的端到端框架的第一步。为此，我们提供了一个新的领导者-追随者双层框架，能够学习提问相关问题（提示），并随后进行推理，指导在环境中执行的行为的学习。一个好的提示应该基于历史的自省性修订来进行修改。

    Large language models (LLMs) demonstrate their promise in tackling complicated practical challenges by combining action-based policies with chain of thought (CoT) reasoning. Having high-quality prompts on hand, however, is vital to the framework's effectiveness. Currently, these prompts are handcrafted utilizing extensive human labor, resulting in CoT policies that frequently fail to generalize. Human intervention is also required in order to develop grounding functions that ensure low-level controllers appropriately process CoT reasoning. In this paper, we take the first step towards a fully integrated end-to-end framework for task-solving in real settings employing complicated reasoning. To that purpose, we offer a new leader-follower bilevel framework capable of learning to ask relevant questions (prompts) and subsequently undertaking reasoning to guide the learning of actions to be performed in an environment. A good prompt should make introspective revisions based on historical fi
    
[^17]: OpinSummEval:再考自动化评估在意见摘要中的应用

    OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization. (arXiv:2310.18122v1 [cs.CL])

    [http://arxiv.org/abs/2310.18122](http://arxiv.org/abs/2310.18122)

    本文提出了一个新的数据集OpinSummEval，对意见摘要进行自动化评估的可靠性进行重新评估。研究发现基于神经网络的度量通常优于非神经网络的度量，但即使是基于强大模型构建的度量也不能在所有维度上始终保持良好的相关性，突出了对意见摘要自动化评估方法的进一步改进的需求。

    

    与其他类型的摘要任务不同，意见摘要专注于观点和情感，因此与众不同。虽然像ROUGE这样的某些自动化评估方法很受欢迎，但我们发现它们对评估意见摘要的质量是不可靠的。在本文中，我们提出了一个数据集OpinSummEval，它包括来自14个意见摘要模型的人工判断和输出。我们进一步探讨了24个自动度量与人工评分之间的相关性，涵盖了四个维度。我们的研究结果表明，基于神经网络的度量通常优于非神经网络的度量。然而，即使是基于强大模型（如BART和GPT-3/3.5）构建的度量也不能在所有维度上始终保持良好的相关性，突出了需要改进意见摘要的自动化评估方法的需求。代码和数据公开可用于https://github.com/A-Chicharito-S/OpinSummEval/tree/main。

    Opinion summarization sets itself apart from other types of summarization tasks due to its distinctive focus on aspects and sentiments. Although certain automated evaluation methods like ROUGE have gained popularity, we have found them to be unreliable measures for assessing the quality of opinion summaries. In this paper, we present OpinSummEval, a dataset comprising human judgments and outputs from 14 opinion summarization models. We further explore the correlation between 24 automatic metrics and human ratings across four dimensions. Our findings indicate that metrics based on neural networks generally outperform non-neural ones. However, even metrics built on powerful backbones, such as BART and GPT-3/3.5, do not consistently correlate well across all dimensions, highlighting the need for advancements in automated evaluation methods for opinion summarization. The code and data are publicly available at https://github.com/A-Chicharito-S/OpinSummEval/tree/main.
    
[^18]: 实现统一的对话推荐系统：通过上下文化知识蒸馏的多任务学习

    Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation. (arXiv:2310.18119v1 [cs.CL])

    [http://arxiv.org/abs/2310.18119](http://arxiv.org/abs/2310.18119)

    通过上下文化知识蒸馏的多任务学习方法，我们提出了一种统一的对话推荐系统，该系统在推荐性能和对话生成的一致性方面取得了显著改进。

    

    在对话推荐系统中，要求代理向用户推荐一组项目，而推荐过程发生在自然语言对话中。为了解决对话能力和个性化推荐的需求，之前的研究使用了分离的推荐和对话模块。然而，这种方法不可避免地导致推荐结果和生成的回应之间存在差异。为了弥合这一差距，我们提出了一种通过上下文化知识蒸馏的多任务学习方法来实现统一的对话推荐系统。我们引入了两个版本的上下文化知识蒸馏方法：硬门和软门。前者在两个任务特定的教师之间进行有选择的门控，而后者整合了两个教师的知识。我们的门控以上下文特定的方式实时计算，便于灵活地整合相关知识。大量实验证明我们的单一模型显著提高了推荐性能，同时也提高了对话生成的一致性。

    In Conversational Recommendation System (CRS), an agent is asked to recommend a set of items to users within natural language conversations. To address the need for both conversational capability and personalized recommendations, prior works have utilized separate recommendation and dialogue modules. However, such approach inevitably results in a discrepancy between recommendation results and generated responses. To bridge the gap, we propose a multi-task learning for a unified CRS, where a single model jointly learns both tasks via Contextualized Knowledge Distillation (ConKD). We introduce two versions of ConKD: hard gate and soft gate. The former selectively gates between two task-specific teachers, while the latter integrates knowledge from both teachers. Our gates are computed on-the-fly in a context-specific manner, facilitating flexible integration of relevant knowledge. Extensive experiments demonstrate that our single model significantly improves recommendation performance whi
    
[^19]: 心思境界：用于学习者论证中省略部分检测和重建的自动语料库构建

    Mind the Gap: Automated Corpus Creation for Enthymeme Detection and Reconstruction in Learner Arguments. (arXiv:2310.18098v1 [cs.CL])

    [http://arxiv.org/abs/2310.18098](http://arxiv.org/abs/2310.18098)

    本研究提出了两个新的任务，即检测学习者论证中的缺失部分和填补这些缺失部分。通过自动创建语料库，可以帮助学习者提高论证质量。基于ICLEv3语料库，本研究创建了40,089个论证实例。

    

    对于学习者来说，写强有力的论证是具有挑战性的。它要求选择和安排多个论证性篇章单元（ADUs），使其逻辑和连贯，并决定暗含的ADUs。然而，当关键的ADUs缺失时，读者可能无法跟随推理或理解论证的主要观点。本文引入了两个新的学习者论证任务：检测论证中的缺失（省略部分检测）和填补这些缺失（省略部分重建）。这两个任务的方法可以帮助学习者提高论证的质量。我们研究了如何通过从一个论证文本中删除对论证及其质量至关重要的ADUs来自动创建这些任务的语料库，同时保持文本的自然性。基于ICLEv3论证性学生文章语料库，我们为省略部分检测和重建创建了40,089个论证实例。通过手动研究，我们提供了证据证明……

    Writing strong arguments can be challenging for learners. It requires to select and arrange multiple argumentative discourse units (ADUs) in a logical and coherent way as well as to decide which ADUs to leave implicit, so called enthymemes. However, when important ADUs are missing, readers might not be able to follow the reasoning or understand the argument's main point. This paper introduces two new tasks for learner arguments: to identify gaps in arguments (enthymeme detection) and to fill such gaps (enthymeme reconstruction). Approaches to both tasks may help learners improve their argument quality. We study how corpora for these tasks can be created automatically by deleting ADUs from an argumentative text that are central to the argument and its quality, while maintaining the text's naturalness. Based on the ICLEv3 corpus of argumentative learner essays, we create 40,089 argument instances for enthymeme detection and reconstruction. Through manual studies, we provide evidence that
    
[^20]: 在翻译中迷失-多语言错误信息及其演变

    Lost in Translation -- Multilingual Misinformation and its Evolution. (arXiv:2310.18089v1 [cs.CL])

    [http://arxiv.org/abs/2310.18089](http://arxiv.org/abs/2310.18089)

    本文通过对超过250,000个唯一事实核查的分析，探究了多语言错误信息的普遍性和动态性。结果显示，部分错误信息能够穿越语言障碍，并且在相同语言中更有可能传播。研究还发现错误信息随时间演变并在不同语言间发生突变。

    

    在数字时代，误导和虚假信息正在迅速在各种语言和边界间传播，构成了日益增长的威胁。本文通过对95种语言中超过250,000个唯一事实核查的分析，探究了多语言错误信息的普遍性和动态性。首先，我们发现大多数错误信息主张仅被事实核查一次，但11.7%的主张(超过21,000个)被核查多次。运用事实核查作为错误信息传播的代理指标，我们发现33%的重复主张穿越语言障碍，暗示部分错误信息渗透了语言边界。然而，扩散模式表现出较强的同质性，错误信息更有可能在相同语言中传播。为研究主张随时间的演变和跨语言的突变，我们使用多语言句子嵌入来表示事实核查，并对语义相似的主张进行聚类。我们分析了连接组件和最短路径。

    Misinformation and disinformation are growing threats in the digital age, spreading rapidly across languages and borders. This paper investigates the prevalence and dynamics of multilingual misinformation through an analysis of over 250,000 unique fact-checks spanning 95 languages. First, we find that while the majority of misinformation claims are only fact-checked once, 11.7%, corresponding to more than 21,000 claims, are checked multiple times. Using fact-checks as a proxy for the spread of misinformation, we find 33% of repeated claims cross linguistic boundaries, suggesting that some misinformation permeates language barriers. However, spreading patterns exhibit strong homophily, with misinformation more likely to spread within the same language. To study the evolution of claims over time and mutations across languages, we represent fact-checks with multilingual sentence embeddings and cluster semantically similar claims. We analyze the connected components and shortest paths conn
    
[^21]: 开放领域问答中有害背景的影响

    Detrimental Contexts in Open-Domain Question Answering. (arXiv:2310.18077v1 [cs.CL])

    [http://arxiv.org/abs/2310.18077](http://arxiv.org/abs/2310.18077)

    本文分析了在开放领域问答中过多的背景信息对模型性能的负面影响，并发现通过过滤掉有害的段落可以提高模型的准确性。

    

    对于知识密集型的自然语言处理任务，广泛认可的观点是访问更多信息有助于模型的端到端性能改善。然而，令人感到困惑的是，当在常见的问答数据集上进行评估时，过多的背景信息会对模型产生负面影响。本文分析了在问答中使用的检索-阅读结构中，段落如何对模型产生不利影响。我们的实证证据表明，当前的阅读结构没有充分利用检索到的段落，在使用整个段落时与利用它们的子集相比，其性能显著下降。我们的研究结果表明，通过过滤掉有害的段落，可以在两个受欢迎的问答数据集上将模型的准确性提高10%。此外，这些结果是通过利用现有的检索方法而无需进一步训练或数据来实现的。我们还进一步强调了识别有害背景的挑战。

    For knowledge intensive NLP tasks, it has been widely accepted that accessing more information is a contributing factor to improvements in the model's end-to-end performance. However, counter-intuitively, too much context can have a negative impact on the model when evaluated on common question answering (QA) datasets. In this paper, we analyze how passages can have a detrimental effect on retrieve-then-read architectures used in question answering. Our empirical evidence indicates that the current read architecture does not fully leverage the retrieved passages and significantly degrades its performance when using the whole passages compared to utilizing subsets of them. Our findings demonstrate that model accuracy can be improved by 10% on two popular QA datasets by filtering out detrimental passages. Additionally, these outcomes are attained by utilizing existing retrieval methods without further training or data. We further highlight the challenges associated with identifying the d
    
[^22]: 问答系统中知识语料错误的问题

    Knowledge Corpus Error in Question Answering. (arXiv:2310.18076v1 [cs.CL])

    [http://arxiv.org/abs/2310.18076](http://arxiv.org/abs/2310.18076)

    本研究探讨了开放领域问答中生成上下文段落与传统检索步骤相比的优势，并引入了知识语料错误的概念。通过使用大型语言模型生成更大范围内的段落，我们观察到问答性能的提升，表明存在知识语料错误的情况。

    

    最近的一些开放领域问答研究探索了使用大型语言模型（LLMs）生成上下文段落，取代问答流程中传统的检索步骤。然而，目前尚不清楚为什么生成的段落比检索到的段落更有效。本研究重新审视了问答问题的传统公式，并引入了知识语料错误的概念。当用于检索的知识语料仅是整个字符串空间的一个子集时，可能会出现这种错误，有可能排除了存在于语料之外的更有帮助的段落。LLMs可以通过在一个更大的空间中生成段落来缓解这个缺点。我们进行了一个使用LLMs来改写人工标注的黄金上下文的实验，以经验性地观察知识语料错误。我们在三个问答基准上的结果显示，在使用改写的段落时性能提升了10%-13%，表明了知识语料错误的存在信号。我们的代码可在ht处获得。

    Recent works in open-domain question answering (QA) have explored generating context passages from large language models (LLMs), replacing the traditional retrieval step in the QA pipeline. However, it is not well understood why generated passages can be more effective than retrieved ones. This study revisits the conventional formulation of QA and introduces the concept of knowledge corpus error. This error arises when the knowledge corpus used for retrieval is only a subset of the entire string space, potentially excluding more helpful passages that exist outside the corpus. LLMs may mitigate this shortcoming by generating passages in a larger space. We come up with an experiment of paraphrasing human-annotated gold context using LLMs to observe knowledge corpus error empirically. Our results across three QA benchmarks reveal an increased performance (10% - 13%) when using paraphrased passage, indicating a signal for the existence of knowledge corpus error. Our code is available at ht
    
[^23]: DUMA：具有快速和慢速思考能力的双重思维对话代理

    DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking. (arXiv:2310.18075v1 [cs.CL])

    [http://arxiv.org/abs/2310.18075](http://arxiv.org/abs/2310.18075)

    DUMA是一种具有快速和慢速思考能力的双重思维对话代理框架，通过利用两个生成型大型语言模型，实现了根据情况在直观响应和深思熟虑的问题解决过程之间无缝切换的能力。

    

    受到人类认知的双过程理论启发，我们介绍了DUMA，一种新颖的对话代理框架，通过利用两个用于快速和慢速思考的生成型大型语言模型（LLM），体现了双重思维机制。快速思考模型作为主要接口用于外部交互和初始响应生成，根据完整响应的复杂性评估是否需要调用慢速思考模型。一旦被调用，慢速思考模型接管对话，在细致规划、推理和工具利用方面进行工作，提供经过充分分析的响应。这种双重思维配置允许根据情况在直观响应和深思熟虑的问题解决过程之间无缝切换。我们构建了一个用于处理房地产行业在线咨询的对话代理。实验证明，我们的方法在效果和效率之间取得了平衡。

    Inspired by the dual-process theory of human cognition, we introduce DUMA, a novel conversational agent framework that embodies a dual-mind mechanism through the utilization of two generative Large Language Models (LLMs) dedicated to fast and slow thinking respectively. The fast thinking model serves as the primary interface for external interactions and initial response generation, evaluating the necessity for engaging the slow thinking model based on the complexity of the complete response. When invoked, the slow thinking model takes over the conversation, engaging in meticulous planning, reasoning, and tool utilization to provide a well-analyzed response. This dual-mind configuration allows for a seamless transition between intuitive responses and deliberate problem-solving processes based on the situation. We have constructed a conversational agent to handle online inquiries in the real estate industry. The experiment proves that our method balances effectiveness and efficiency, an
    
[^24]: 从复杂的ESG年度报告中提取目录的可扩展框架

    A Scalable Framework for Table of Contents Extraction from Complex ESG Annual Reports. (arXiv:2310.18073v1 [cs.CL])

    [http://arxiv.org/abs/2310.18073](http://arxiv.org/abs/2310.18073)

    本论文提出了一个可扩展的框架来从复杂的ESG年度报告中提取目录。提出了一个新的数据集ESGDoc，包括来自563家公司的1,093份报告。通过构建-建模-修改（CMM）过程，该框架提供了一种实用的解决方案，以消除以前方法中的挑战，并提高目录提取的准确性。

    

    目录（ToC）提取侧重于以分层的方式对文档进行结构化。在本文中，我们提出了一个新的数据集ESGDoc，包括来自2001年至2022年的563家公司的1,093份ESG年度报告。这些报告由于其多样的结构和广泛的长度而带来了重大挑战。为了应对这些挑战，我们提出了一种新的Toc提取框架，包括三个步骤：（1）根据阅读顺序和字体大小构建文本块的初始树；（2）通过考虑节点为中心的子树中捕获的上下文信息，单独对每个树节点（或文本块）进行建模；（3）通过对每个树节点（保留、删除或移动）采取适当的操作来修改原始树。这个构建-建模-修改（CMM）过程具有几个优势。它消除了以前方法中需要成对建模节标题的需要，使得文档分割实际可行。通过结合结构性的特征信息，我们可以显著改进目录提取的准确性。

    Table of contents (ToC) extraction centres on structuring documents in a hierarchical manner. In this paper, we propose a new dataset, ESGDoc, comprising 1,093 ESG annual reports from 563 companies spanning from 2001 to 2022. These reports pose significant challenges due to their diverse structures and extensive length. To address these challenges, we propose a new framework for Toc extraction, consisting of three steps: (1) Constructing an initial tree of text blocks based on reading order and font sizes; (2) Modelling each tree node (or text block) independently by considering its contextual information captured in node-centric subtree; (3) Modifying the original tree by taking appropriate action on each tree node (Keep, Delete, or Move). This construction-modelling-modification (CMM) process offers several benefits. It eliminates the need for pairwise modelling of section headings as in previous approaches, making document segmentation practically feasible. By incorporating structur
    
[^25]: 多样化证据推理用于多项选择阅读理解

    Multi-grained Evidence Inference for Multi-choice Reading Comprehension. (arXiv:2310.18070v1 [cs.CL])

    [http://arxiv.org/abs/2310.18070](http://arxiv.org/abs/2310.18070)

    本研究提出了一种多样化证据推理模型（Mugen），通过提取粗糙、中等和细粒度的证据，并将其与原始段落结合，实现了显著和一致的性能提升。该模型在多项选择机器阅读理解任务中具有重要的创新和贡献。

    

    多项选择机器阅读理解是机器根据给定的选项回答问题的一个重要且具有挑战性的任务。在多项选择机器阅读理解中，答案不能直接从给定的文章中提取出来，而实质上需要机器能够从准确的提取证据中进行推理。然而，关键的证据可能只是一个词或短语，而它却隐藏在给定的冗余、嘈杂的文章中，具有多个语言层级，从短语、片段、句子直到整个段落。因此，我们提出了一种新颖的通用模型增强方法，综合集成多样化的证据，称为多样化证据推理器（Mugen），以弥补这种能力的不足。Mugen提取了粗糙、中等和细粒度三种不同层次的证据，并将证据与原始段落结合起来，在四个多项选择机器阅读理解基准测试中实现了显著而一致的性能提升。

    Multi-choice Machine Reading Comprehension (MRC) is a major and challenging task for machines to answer questions according to provided options. Answers in multi-choice MRC cannot be directly extracted in the given passages, and essentially require machines capable of reasoning from accurate extracted evidence. However, the critical evidence may be as simple as just one word or phrase, while it is hidden in the given redundant, noisy passage with multiple linguistic hierarchies from phrase, fragment, sentence until the entire passage. We thus propose a novel general-purpose model enhancement which integrates multi-grained evidence comprehensively, named Multi-grained evidence inferencer (Mugen), to make up for the inability. Mugen extracts three different granularities of evidence: coarse-, middle- and fine-grained evidence, and integrates evidence with the original passages, achieving significant and consistent performance improvement on four multi-choice MRC benchmarks.
    
[^26]: “亲爱的，告诉我出了什么问题”，通过合作生成全局文本辨别模型的解释

    "Honey, Tell Me What's Wrong", Global Explanation of Textual Discriminative Models through Cooperative Generation. (arXiv:2310.18063v1 [cs.CL])

    [http://arxiv.org/abs/2310.18063](http://arxiv.org/abs/2310.18063)

    Therapy是第一个适用于文本的全局和无模型解释方法，通过合作生成文本，不依赖于初始样本，并提供了对输入空间中模型行为的全局概览。

    

    复杂机器学习的普及提高了无模型解释算法的重要性。这些方法通过轻微扰动真实实例来创建人工实例，捕捉模型决策的变化。然而，这些方法依赖于初始数据，并且只提供关于这些初始数据决策的解释。为了解决这些问题，我们提出了 Therapy，这是第一个适用于文本的全局和无模型解释方法，不需要输入数据集。Therapy通过合作生成，根据分类器学习到的分布生成文本。因为它不依赖于初始样本，所以即使数据缺失（例如因保密原因），也能生成解释。此外，与将多个局部解释组合成一个全局解释的现有方法不同，Therapy提供了对输入空间中模型行为的全局概览。我们的实验表明，虽然不使用输入数据来生成样本，但 Therapy 提供了有价值的洞察。

    The ubiquity of complex machine learning has raised the importance of model-agnostic explanation algorithms. These methods create artificial instances by slightly perturbing real instances, capturing shifts in model decisions. However, such methods rely on initial data and only provide explanations of the decision for these. To tackle these problems, we propose Therapy, the first global and model-agnostic explanation method adapted to text which requires no input dataset. Therapy generates texts following the distribution learned by a classifier through cooperative generation. Because it does not rely on initial samples, it allows to generate explanations even when data is absent (e.g., for confidentiality reasons). Moreover, conversely to existing methods that combine multiple local explanations into a global one, Therapy offers a global overview of the model behavior on the input space. Our experiments show that although using no input data to generate samples, Therapy provides insig
    
[^27]: ViCLEVR：一种用于越南语视觉问答的视觉推理数据集和混合多模态融合模型

    ViCLEVR: A Visual Reasoning Dataset and Hybrid Multimodal Fusion Model for Visual Question Answering in Vietnamese. (arXiv:2310.18046v1 [cs.CL])

    [http://arxiv.org/abs/2310.18046](http://arxiv.org/abs/2310.18046)

    本论文介绍了ViCLEVR数据集，它是一种用于评估越南语视觉推理能力的集合。通过分析该数据集，对当代视觉推理系统进行了全面的分析，揭示了它们的优点和局限性。

    

    近年来，视觉问答（VQA）因其包括智能汽车辅助、帮助视障人士以及使用自然语言查询的文档图像信息检索等多种应用而受到重视。VQA需要有效地将问题和图像的信息进行集成以生成准确的答案。VQA的神经模型在大规模数据集上取得了显著的进展，主要关注资源丰富的语言，如英语。为了解决这个问题，我们引入了ViCLEVR数据集，这是一组开创性的用于评估越南语中各种视觉推理能力并减小偏差的集合。该数据集包括超过26,000张图像和30,000个问题-答案对（QAs），每个问题都有注释指定所涉及的推理类型。利用这个数据集，我们对当代视觉推理系统进行了全面的分析，提供了有价值的见解，包括它们的优点和局限性。

    In recent years, Visual Question Answering (VQA) has gained significant attention for its diverse applications, including intelligent car assistance, aiding visually impaired individuals, and document image information retrieval using natural language queries. VQA requires effective integration of information from questions and images to generate accurate answers. Neural models for VQA have made remarkable progress on large-scale datasets, with a primary focus on resource-rich languages like English. To address this, we introduce the ViCLEVR dataset, a pioneering collection for evaluating various visual reasoning capabilities in Vietnamese while mitigating biases. The dataset comprises over 26,000 images and 30,000 question-answer pairs (QAs), each question annotated to specify the type of reasoning involved. Leveraging this dataset, we conduct a comprehensive analysis of contemporary visual reasoning systems, offering valuable insights into their strengths and limitations. Furthermore
    
[^28]: 关于通用语言理解

    On General Language Understanding. (arXiv:2310.18038v1 [cs.CL])

    [http://arxiv.org/abs/2310.18038](http://arxiv.org/abs/2310.18038)

    本文提出了一个通用语言理解模型的轮廓，该模型可以评估当前模型质量测量方法的适当性问题，旨在解决关于大型语言模型理解程度的争论，并探讨语言理解的多方面现象和自然语言处理应用的道德问题。

    

    自然语言处理自称是一个以经验为导向的领域，但最近它似乎卷入了关于意义和测量问题的实质性争论（“大型语言模型是否理解语言，如果理解的话，程度如何？”）。这并不是偶然的：在这里，就像在任何地方一样，证据并不能充分说明理解。为了解决这个问题，本文概述了一个理解模型的轮廓，用于评估当前模型质量测量方法的适当性问题。本文提出了三个观点：A）不同的语言使用情景类型具有不同的特征，B）语言理解是一个多方面的现象，汇集了个体主义和社会过程，C）理解指标的选择标志着基准测试的界限以及自然语言处理应用的道德思考的开始。

    Natural Language Processing prides itself to be an empirically-minded, if not outright empiricist field, and yet lately it seems to get itself into essentialist debates on issues of meaning and measurement ("Do Large Language Models Understand Language, And If So, How Much?"). This is not by accident: Here, as everywhere, the evidence underspecifies the understanding. As a remedy, this paper sketches the outlines of a model of understanding, which can ground questions of the adequacy of current methods of measurement of model quality. The paper makes three claims: A) That different language use situation types have different characteristics, B) That language understanding is a multifaceted phenomenon, bringing together individualistic and social processes, and C) That the choice of Understanding Indicator marks the limits of benchmarking, and the beginnings of considerations of the ethics of NLP use.
    
[^29]: 大型语言模型用于基于方面的情感分析

    Large language models for aspect-based sentiment analysis. (arXiv:2310.18025v1 [cs.CL])

    [http://arxiv.org/abs/2310.18025](http://arxiv.org/abs/2310.18025)

    该论文评估了GPT-4和GPT-3.5在基于方面的情感分析任务中的性能，并发现微调后的GPT-3.5在SemEval-2014任务4中取得了83.8的最先进F1分数，相比于InstructABSA提高了5.7%。但是，这需要1000倍的模型参数增加了推理成本。

    

    大型语言模型（LLMs）提供了前所未有的文本完成能力。作为通用模型，它们可以担任各种角色，包括更专门的模型。我们评估了GPT-4和GPT-3.5在基于方面的情感分析（ABSA）任务中的零-shot、少-shot和微调设置下的性能。微调后的GPT-3.5在SemEval-2014任务4的联合方面术语提取和极性分类任务上取得了83.8的最先进F1分数，相比于InstructABSA [Scaria等人，2023]提高了5.7％。然而，这是以1000倍更多的模型参数和因此增加的推理成本为代价的。我们讨论了不同模型的成本性能权衡，并分析了它们的典型错误。我们的结果还表明，在零-shot和少-shot设置中，详细提示可以提高性能，但对于微调模型来说并不是必要的。这些证据对于面临选择问题的实践者是相关的。

    Large language models (LLMs) offer unprecedented text completion capabilities. As general models, they can fulfill a wide range of roles, including those of more specialized models. We assess the performance of GPT-4 and GPT-3.5 in zero shot, few shot and fine-tuned settings on the aspect-based sentiment analysis (ABSA) task. Fine-tuned GPT-3.5 achieves a state-of-the-art F1 score of 83.8 on the joint aspect term extraction and polarity classification task of the SemEval-2014 Task 4, improving upon InstructABSA [@scaria_instructabsa_2023] by 5.7%. However, this comes at the price of 1000 times more model parameters and thus increased inference cost. We discuss the the cost-performance trade-offs of different models, and analyze the typical errors that they make. Our results also indicate that detailed prompts improve performance in zero-shot and few-shot settings but are not necessary for fine-tuned models. This evidence is relevant for practioners that are faced with the choice of pro
    
[^30]: SentMix-3L: 用于情感分析的孟加拉语-英语-印地语混合代码数据集

    SentMix-3L: A Bangla-English-Hindi Code-Mixed Dataset for Sentiment Analysis. (arXiv:2310.18023v1 [cs.CL])

    [http://arxiv.org/abs/2310.18023](http://arxiv.org/abs/2310.18023)

    SentMix-3L是一个用于情感分析的新颖数据集，包含孟加拉语、英语和印地语之间的代码混合数据。研究发现，在SentMix-3L上，使用GPT-3.5进行零-shot提示可以超过所有基于转换器的模型。

    

    代码混合是一种研究很深的语言现象，指的是在文本或语音中混合使用两种或更多语言。已经构建了几个旨在训练代码混合计算模型的数据集。尽管多语言的代码混合很常见，但大多数可用的数据集只包含两种语言的代码混合。本文介绍了SentMix-3L，这是一个新颖的用于情感分析的数据集，其中包含孟加拉语、英语和印地语之间的代码混合数据。我们使用SentMix-3L进行了全面评估。我们展示了使用GPT-3.5进行零-shot提示在SentMix-3L上优于所有基于转换器的模型。

    Code-mixing is a well-studied linguistic phenomenon when two or more languages are mixed in text or speech. Several datasets have been build with the goal of training computational models for code-mixing. Although it is very common to observe code-mixing with multiple languages, most datasets available contain code-mixed between only two languages. In this paper, we introduce SentMix-3L, a novel dataset for sentiment analysis containing code-mixed data between three languages Bangla, English, and Hindi. We carry out a comprehensive evaluation using SentMix-3L. We show that zero-shot prompting with GPT-3.5 outperforms all transformer-based models on SentMix-3L.
    
[^31]: NLP评估遇到麻烦：需要针对每个基准测试测量LLM数据污染问题

    NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark. (arXiv:2310.18018v1 [cs.CL])

    [http://arxiv.org/abs/2310.18018](http://arxiv.org/abs/2310.18018)

    这篇立场论文指出NLP任务的经典评估存在数据污染的问题，尤其是当一个LLM模型在基准测试的测试集上进行训练和评估时。该问题导致污染模型在目标基准测试和任务中的性能被高估，可能会产生错误的科学结论。建议开发自动和半自动的测量方法来检测数据污染，并提醒评审论文时要注意具有妥协性结论的论文。

    

    在这篇立场论文中，我们认为使用带有注释的基准测试对自然语言处理（NLP）任务进行经典评估存在问题。最严重的数据污染发生在使用基准测试的测试集对一个大型语言模型（LLM）进行训练，并在同一基准测试中进行评估的情况下。该问题的程度尚不清楚，因为很难直接衡量。污染会导致在目标基准测试和相关任务中，受到污染的模型的性能被高估，与未受污染的对应模型相比。后果可能非常严重，会出现错误的科学结论被发表，而其他正确的结论被忽视。本立场论文定义了不同级别的数据污染，并提倡社区共同努力，包括开发自动和半自动的测量方法来检测基准测试数据是否暴露给了模型，并提出了标记具有妥协性结论的论文的建议。

    In this position paper, we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromise
    
[^32]: 角色扮演聊天机器人能够捕捉角色个性吗？评估角色扮演聊天机器人的个性特点。

    Does Role-Playing Chatbots Capture the Character Personalities? Assessing Personality Traits for Role-Playing Chatbots. (arXiv:2310.17976v1 [cs.CL])

    [http://arxiv.org/abs/2310.17976](http://arxiv.org/abs/2310.17976)

    本文介绍了一种创新的方法，用于评估角色扮演聊天机器人的个性特点，并发现基于LLMs的现代角色扮演聊天机器人能够有效地描绘出相应角色的个性特点，与人类感知的一致性达到82.8%。

    

    大规模预训练语言模型的出现彻底改变了新型人工智能应用的能力，尤其是在打造具有独特人物的聊天机器人方面。鉴于聊天机器人的"刺激-响应"性质，本文揭示了一种创新的开放式采访式方法，用于评估角色扮演聊天机器人的个性特点，从而更深入地理解其内在个性。我们对ChatHaruhi图书馆创建的32个角色扮演聊天机器人进行了大五人格和MBTI维度上的个性评估，并测量它们与人类感知的一致性。评估结果强调，基于LLMs的现代角色扮演聊天机器人能够有效地描绘出相应角色的个性特点，与人类感知的一致性达到82.8%。此外，我们还提出了塑造聊天机器人个性的潜在策略。因此，本文为角色扮演聊天机器人研究奠定了基础。

    The emergence of large-scale pretrained language models has revolutionized the capabilities of new AI application, especially in the realm of crafting chatbots with distinct personas. Given the "stimulus-response" nature of chatbots, this paper unveils an innovative open-ended interview-style approach for personality assessment on role-playing chatbots, which offers a richer comprehension of their intrinsic personalities. We conduct personality assessments on 32 role-playing chatbots created by the ChatHaruhi library, across both the Big Five and MBTI dimensions, and measure their alignment with human perception. Evaluation results underscore that modern role-playing chatbots based on LLMs can effectively portray personality traits of corresponding characters, with an alignment rate of 82.8% compared with human-perceived personalities. Besides, we also suggest potential strategies for shaping chatbots' personalities. Hence, this paper serves as a cornerstone study for role-playing chat
    
[^33]: 面向普遍医疗保健的中国大型视觉-语言模型Qilin-Med-VL

    Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare. (arXiv:2310.17956v1 [cs.CV])

    [http://arxiv.org/abs/2310.17956](http://arxiv.org/abs/2310.17956)

    Qilin-Med-VL是面向普遍医疗保健的中国大型视觉-语言模型，它结合了预训练的视觉Transformer和基础语言模型，通过两阶段课程训练过程提高了生成医疗标题和回答复杂医疗查询的能力，并发布了一个包含超过100万个图像-文本对的数据集ChiMed-VL。

    

    大型语言模型(LLMs)在理解复杂的医疗保健和生物医学主题方面取得了新的突破。然而，除了英语之外，其他语言的模型以及能够解释多模态输入的模型相对较少，而这对于全球医疗保健的可访问性至关重要。为此，本研究介绍了Qilin-Med-VL，这是第一个设计用于整合文本和视觉数据分析的中国大型视觉-语言模型。Qilin-Med-VL将经过预训练的视觉Transformer（ViT）与基础LLM相结合。它经历了一个深入的两阶段课程训练过程，其中包括特征对齐和指导调优。这种方法增强了模型生成医疗标题和回答复杂医疗查询的能力。我们还发布了ChiMed-VL，这是一个包含超过100万个图像-文本对的数据集。该数据集经过精心策划，可以使用各种类型的图像进行详细和全面的医学数据解释。

    Large Language Models (LLMs) have introduced a new era of proficiency in comprehending complex healthcare and biomedical topics. However, there is a noticeable lack of models in languages other than English and models that can interpret multi-modal input, which is crucial for global healthcare accessibility. In response, this study introduces Qilin-Med-VL, the first Chinese large vision-language model designed to integrate the analysis of textual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer (ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum training process that includes feature alignment and instruction tuning. This method enhances the model's ability to generate medical captions and answer complex medical queries. We also release ChiMed-VL, a dataset consisting of more than 1M image-text pairs. This dataset has been carefully curated to enable detailed and comprehensive interpretation of medical data using various types of images.
    
[^34]: Whisper-MCE: 针对混合语言实现更好性能的Whisper模型微调

    Whisper-MCE: Whisper Model Finetuned for Better Performance with Mixed Languages. (arXiv:2310.17953v1 [cs.SD])

    [http://arxiv.org/abs/2310.17953](http://arxiv.org/abs/2310.17953)

    Whisper-MCE是使用自己收集的混合粤语和英语音频数据集（MCE）进行训练的Whisper模型微调，相较于基准模型，其在准确捕捉原始音频内容、提高识别准确性和加快识别速度方面具有更优越的能力，尤其在混合语言识别任务中表现出色。

    

    最近，Whisper在英语自动语音识别（ASR）领域已经接近于人类级别的鲁棒性和准确性，但在较小语种和混合语言的语音识别中，仍然需要进一步改进。本文介绍了我们细调的Whisper模型Whisper-MCE的令人瞩目的结果，该模型使用了我们自己收集的混合粤语和英语音频数据集（MCE）进行训练。同时，考虑到词错误率（WER）在较小语种和混合语言环境中评估其有效性时存在挑战，我们提出了一种新颖的评估机制。通过将我们的模型与基准的whisper-large-v2模型进行比较，我们展示了它准确捕捉原始音频内容的能力更强、识别准确性更高、识别速度更快。值得注意的是，我们的模型在识别混合语言的特定任务中胜过其他现有模型。

    Recently Whisper has approached human-level robustness and accuracy in English automatic speech recognition (ASR), while in minor language and mixed language speech recognition, there remains a compelling need for further improvement. In this work, we present the impressive results of Whisper-MCE, our finetuned Whisper model, which was trained using our self-collected dataset, Mixed Cantonese and English audio dataset (MCE). Meanwhile, considering word error rate (WER) poses challenges when it comes to evaluating its effectiveness in minor language and mixed-language contexts, we present a novel rating mechanism. By comparing our model to the baseline whisper-large-v2 model, we demonstrate its superior ability to accurately capture the content of the original audio, achieve higher recognition accuracy, and exhibit faster recognition speed. Notably, our model outperforms other existing models in the specific task of recognizing mixed language.
    
[^35]: 统一的片段到片段框架用于同时序列生成

    Unified Segment-to-Segment Framework for Simultaneous Sequence Generation. (arXiv:2310.17940v1 [cs.CL])

    [http://arxiv.org/abs/2310.17940](http://arxiv.org/abs/2310.17940)

    这篇论文提出了一种统一的片段到片段框架 (Seg2Seg) 用于同时序列生成，通过自适应和统一的方式学习源序列和目标序列之间的映射，实现高质量生成和低延迟。

    

    同时序列生成是实时场景的关键任务，比如流式语音识别、同时机器翻译和同时语音翻译，其中目标序列在接收源序列的同时生成。实现高质量生成和低延迟的关键在于确定生成的最佳时机，通过学习源序列和目标序列之间的映射实现。然而，现有方法往往依赖于特定任务的启发式方法，限制了模型对源-目标映射的自适应学习能力，阻碍了多任务学习在各种同时任务中的探索。本文提出了一种统一的片段到片段框架 (Seg2Seg) 用于同时序列生成，以自适应和统一的方式学习映射。在同时生成的过程中，模型在等待源片段和生成目标片段之间交替进行。

    Simultaneous sequence generation is a pivotal task for real-time scenarios, such as streaming speech recognition, simultaneous machine translation and simultaneous speech translation, where the target sequence is generated while receiving the source sequence. The crux of achieving high-quality generation with low latency lies in identifying the optimal moments for generating, accomplished by learning a mapping between the source and target sequences. However, existing methods often rely on task-specific heuristics for different sequence types, limiting the model's capacity to adaptively learn the source-target mapping and hindering the exploration of multi-task learning for various simultaneous tasks. In this paper, we propose a unified segment-to-segment framework (Seg2Seg) for simultaneous sequence generation, which learns the mapping in an adaptive and unified manner. During the process of simultaneous generation, the model alternates between waiting for a source segment and generat
    
[^36]: Transformers作为图到图模型

    Transformers as Graph-to-Graph Models. (arXiv:2310.17936v1 [cs.CL])

    [http://arxiv.org/abs/2310.17936](http://arxiv.org/abs/2310.17936)

    本文认为Transformers本质上是图到图模型，通过将注意力权重等价于图中的边，并使用图到图Transformer架构结合显式图和潜在图进行非自回归图预测，实现了在建模各种语言结构方面的最先进准确性。

    

    我们认为Transformers本质上是图到图模型，而序列只是一种特殊情况。注意力权重在功能上等价于图中的边。我们的图到图Transformer架构将这种能力明确地体现出来，通过将图的边输入到注意力权重计算中，并使用类似注意力的函数来预测图的边，从而将显式图集成到预训练Transformers学习的潜在图中。添加迭代图细化可以为输入、输出和潜在图提供联合嵌入，使得非自回归图预测可以优化完整的图，而无需任何专门的管道或解码策略。实证结果表明，该架构在建模各种语言结构方面达到了最先进的准确性，并与预训练学习的潜在语言表示非常有效地集成。

    We argue that Transformers are essentially graph-to-graph models, with sequences just being a special case. Attention weights are functionally equivalent to graph edges. Our Graph-to-Graph Transformer architecture makes this ability explicit, by inputting graph edges into the attention weight computations and predicting graph edges with attention-like functions, thereby integrating explicit graphs into the latent graphs learned by pretrained Transformers. Adding iterative graph refinement provides a joint embedding of input, output, and latent graphs, allowing non-autoregressive graph prediction to optimise the complete graph without any bespoke pipeline or decoding strategy. Empirical results show that this architecture achieves state-of-the-art accuracies for modelling a variety of linguistic structures, integrating very effectively with the latent linguistic representations learned by pretraining.
    
[^37]: SOUL: 语言的情感和观点理解

    SOUL: Towards Sentiment and Opinion Understanding of Language. (arXiv:2310.17924v1 [cs.CL])

    [http://arxiv.org/abs/2310.17924](http://arxiv.org/abs/2310.17924)

    提出了一个新的任务SOUL，旨在通过评论理解和解释生成两个子任务来评估语言的情感和观点理解。该任务在小型和大型语言模型中都具有挑战性，性能差距可高达27%。

    

    情感分析是一个成熟的自然语言处理任务，情感极性分类是其中最受欢迎和代表性的任务之一。然而，尽管预训练的语言模型在这个领域取得了成功，但它们往往无法捕捉到情感分析的更广泛的复杂性。为了解决这个问题，我们提出了一个新的任务，称为语言的情感和观点理解（SOUL）。SOUL通过两个子任务来评估情感理解：评论理解（RC）和解释生成（JG）。RC旨在通过评论文本验证关于主观信息的陈述，而JG要求模型为其情感预测提供解释。为了进行全面的评估，我们标注了一个新的数据集，包括来自3638条评论的15028个陈述。实验结果表明，SOUL对小型和大型语言模型来说都是一个具有挑战性的任务，性能差距可高达27%。

    Sentiment analysis is a well-established natural language processing task, with sentiment polarity classification being one of its most popular and representative tasks. However, despite the success of pre-trained language models in this area, they often fall short of capturing the broader complexities of sentiment analysis. To address this issue, we propose a new task called Sentiment and Opinion Understanding of Language (SOUL). SOUL aims to evaluate sentiment understanding through two subtasks: Review Comprehension (RC) and Justification Generation (JG). RC seeks to validate statements that focus on subjective information based on a review text, while JG requires models to provide explanations for their sentiment predictions. To enable comprehensive evaluation, we annotate a new dataset comprising 15,028 statements from 3,638 reviews. Experimental results indicate that SOUL is a challenging task for both small and large language models, with a performance gap of up to 27% when compa
    
[^38]: 知道LLMs不知道什么：一种简单而有效的自我检测方法

    Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method. (arXiv:2310.17918v1 [cs.CL])

    [http://arxiv.org/abs/2310.17918](http://arxiv.org/abs/2310.17918)

    本文提出了一种新的自我检测方法，用于判断大型语言模型 (LLMs) 无法回答的问题，以避免生成非事实性的回答。通过多样化问题的文本表达，收集答案，并检查生成的答案之间的差异，可以识别出可能生成虚假回答的问题。该方法只需要利用LLMs自身，无需其他外部资源。这种方法在Vicuna、ChatGPT和GPT-4等最新发布的LLMs上得到了有效验证。

    

    大型语言模型（LLMs）在自然语言处理（NLP）任务中展现出巨大的潜力。然而，最近的文献揭示了LLMs会偶尔生成非事实性的回答，这影响了它们进一步利用的可靠性。在本文中，我们提出了一种新颖的自我检测方法，用于检测LLMs不知道的问题，以避免生成非事实性的结果。具体来说，我们首先使给定问题的文本表达多样化，并收集相应的答案。然后，我们检查生成的答案之间的差异，以识别模型可能生成虚假回答的问题。所有以上步骤都可以通过提示LLMs自身来完成，而无需参考任何其他外部资源。我们进行了全面的实验，并证明了我们方法在最近发布的LLMs（如Vicuna、ChatGPT和GPT-4）上的有效性。

    Large Language Models (LLMs) have shown great potential in Natural Language Processing (NLP) tasks. However, recent literature reveals that LLMs generate nonfactual responses intermittently, which impedes the LLMs' reliability for further utilization. In this paper, we propose a novel self-detection method to detect which questions that a LLM does not know that are prone to generate nonfactual results. Specifically, we first diversify the textual expressions for a given question and collect the corresponding answers. Then we examine the divergencies between the generated answers to identify the questions that the model may generate falsehoods. All of the above steps can be accomplished by prompting the LLMs themselves without referring to any other external resources. We conduct comprehensive experiments and demonstrate the effectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT, and GPT-4.
    
[^39]: 关于部件、姿态和遮挡的3D感知视觉问答

    3D-Aware Visual Question Answering about Parts, Poses and Occlusions. (arXiv:2310.17914v1 [cs.CV])

    [http://arxiv.org/abs/2310.17914](http://arxiv.org/abs/2310.17914)

    这个论文提出了一种名为3D感知VQA的任务，旨在推动VQA模型对于视觉场景的3D结构进行组合推理。作者从数据集和模型角度出发，分别引入了Super-CLEVR-3D数据集和PO3D-VQA模型，并将概率神经符号程序执行和3D生成表示相结合，用于强大的视觉识别。

    

    尽管在视觉问答（VQA）领域取得了长足进展，现有的数据集和模型主要集中于2D推理。然而，VQA模型也需要理解视觉场景的3D结构，例如支持导航或操作等任务。这包括对三维物体姿态、部件和遮挡的理解。在本研究中，我们引入了一项名为3D感知VQA的任务，重点关注需要在视觉场景的三维结构上进行组合推理的挑战性问题。我们从数据集和模型两个方面来解决3D感知VQA问题。首先，我们引入了Super-CLEVR-3D，一个包含关于物体部件、它们的三维姿态和遮挡的组合推理数据集。其次，我们提出了PO3D-VQA，一个将两个强大的思想相结合的3D感知VQA模型：用于推理的概率神经符号程序执行和基于物体的三维生成表示的深度神经网络用于强大的视觉识别。

    Despite rapid progress in Visual question answering (VQA), existing datasets and models mainly focus on testing reasoning in 2D. However, it is important that VQA models also understand the 3D structure of visual scenes, for example to support tasks like navigation or manipulation. This includes an understanding of the 3D object pose, their parts and occlusions. In this work, we introduce the task of 3D-aware VQA, which focuses on challenging questions that require a compositional reasoning over the 3D structure of visual scenes. We address 3D-aware VQA from both the dataset and the model perspective. First, we introduce Super-CLEVR-3D, a compositional reasoning dataset that contains questions about object parts, their 3D poses, and occlusions. Second, we propose PO3D-VQA, a 3D-aware VQA model that marries two powerful ideas: probabilistic neural symbolic program execution for reasoning and deep neural networks with 3D generative representations of objects for robust visual recognition
    
[^40]: 对表格数据查询和可视化的自然语言界面：一项调查

    Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey. (arXiv:2310.17894v1 [cs.CL])

    [http://arxiv.org/abs/2310.17894](http://arxiv.org/abs/2310.17894)

    本调查对表格数据查询和可视化的自然语言界面进行了全面概述，介绍了语义解析等关键技术，并深入探讨了Text-to-SQL和Text-to-Vis问题的最新进展。

    

    自然语言处理的出现彻底改变了用户与表格数据的交互方式，实现了从传统的查询语言和手动绘图转向更直观、基于语言的界面。大型语言模型（LLM）如ChatGPT及其后继者进一步推动了这一领域的发展，为自然语言处理技术开辟了新的途径。本调查提供了关于表格数据查询和可视化的自然语言界面的全面概述，这些界面允许用户使用自然语言查询与数据进行交互。我们介绍了这些界面的基本概念和技术，特别强调语义解析，这是实现从自然语言到SQL查询或数据可视化命令转化的关键技术。然后从数据集、方法论、评估指标和系统设计的角度深入探讨了Text-to-SQL和Text-to-Vis问题的最新进展。

    The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization, which allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis problems from the perspectives of datasets, methodologies, metrics, and system designs. Thi
    
[^41]: LLM能保守秘密吗？通过上下文完整性理论测试语言模型的隐私影响

    Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory. (arXiv:2310.17884v1 [cs.AI])

    [http://arxiv.org/abs/2310.17884](http://arxiv.org/abs/2310.17884)

    本研究通过提出ConfAIde基准，揭示了LLMs的上下文隐私推理能力中的重要弱点，实验证明即使是最强大的模型也会在人类不会的上下文中泄露私人信息，强调了探索新型推理时隐私保护方法的迫切需求。

    

    在AI助手（工作、家庭等）中交互使用大型语言模型（LLMs）引入了一系列新的推理时隐私风险：LLMs从多个来源的输入中获取不同类型的信息，并期望在给定的上下文中推理出在何种目的和与谁分享的内容。在这项工作中，我们通过提出ConfAIde，一个旨在识别指令调整的LLMs隐私推理能力中重要弱点的基准，来引起人们对上下文隐私这一极其关键但经常被忽视的概念的关注。我们的实验表明，即使是GPT-4和ChatGPT等最强大的模型，在人类不会的上下文中，也会泄露39％和57％的私人信息。即使我们使用保护隐私的提示或思维链推理，这种泄漏也会持续存在。我们的工作强调了迫切需要探索基于推理和理论的新型推理时隐私保护方法。

    The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory
    
[^42]: ASPIRO: 一种适用于零到少样本情况下结构化数据到文本生成的错误感知重提示方法

    ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for Consistent Data-to-Text Generation. (arXiv:2310.17877v1 [cs.CL])

    [http://arxiv.org/abs/2310.17877](http://arxiv.org/abs/2310.17877)

    ASPIRO是一种能在零到少样本情况下将结构化数据转化为简短模板句子的方法。通过算法解析检查、LLM的重新提示以及一致性验证指标PARENT，ASPIRO成功降低了66%的解析错误率，并且在与最近的预训练语言模型的竞争中表现出色。

    

    我们提出了ASPIRO，一种在零到少样本情况下将结构化数据转化为简短模板句子的方法。与之前的方法不同，我们的方法直接提示大型语言模型（LLM）产生与实体无关的模板，而不是依赖LLM忠实地复制给定的实体，或者手动验证/制作模板。我们通过算法解析检查和PARENT指标诱导的一致性验证，结合LLM的重新提示，实时识别和纠正模板生成问题。在DART数据集上，与直接LLM输出相比，ASPIRO对RDF三元组的生成文本的解析错误率平均降低了66％。我们在Rel2Text数据集上的最佳5样本text-davinci-003设置评分为BLEU 50.62，METEOR 45.16，BLEURT 0.82，NUBIA 0.87和PARENT 0.8962，与最近的精调预训练语言模型有了有效的竞争力。

    We present ASPIRO, an approach for structured data verbalisation into short template sentences in zero to few-shot settings. Unlike previous methods, our approach prompts large language models (LLMs) to directly produce entity-agnostic templates, rather than relying on LLMs to faithfully copy the given example entities, or validating/crafting the templates manually. We incorporate LLM re-prompting, triggered by algorithmic parsing checks, as well as the PARENT metric induced consistency validation to identify and rectify template generation problems in real-time. ASPIRO, compared to direct LLM output, averages 66\% parsing error rate reduction in generated verbalisations of RDF triples on the DART dataset. Our best 5-shot text-davinci-003 setup, scoring BLEU of 50.62, METEOR of 45.16, BLEURT of 0.82, NUBIA of 0.87, and PARENT of 0.8962 on the Rel2Text dataset, competes effectively with recent fine-tuned pre-trained language models.
    
[^43]: TarGEN: 基于大型语言模型的目标数据生成技术

    TarGEN: Targeted Data Generation with Large Language Models. (arXiv:2310.17876v1 [cs.CL])

    [http://arxiv.org/abs/2310.17876](http://arxiv.org/abs/2310.17876)

    TarGEN是一种利用大型语言模型生成高质量合成数据集的多步提示策略，通过自我修正方法确保可靠的标签。在SuperGLUE基准测试中，模型在合成数据集上的训练效果与原始数据集相当。

    

    大型语言模型（LLM）的快速发展引发了对数据合成技术的兴趣，旨在生成多样且高质量的合成数据集。然而，这些合成数据集往往缺乏多样性并且存在噪声。在本文中，我们提出了TarGEN，一种利用LLM生成高质量的合成数据集的多步提示策略。TarGEN的一个优点是无需种子；它不需要特定的任务实例，扩大了其适用性。我们还通过一种称为自我修正的方法，使LLM能够在创建数据集过程中纠正标记错误的实例，确保可靠的标签。为了评估我们技术的有效性，我们模拟了SuperGLUE基准测试中的8个任务，并在合成和原始训练集上微调了各种语言模型，包括仅编码器、编码器-解码器和仅解码器模型。在原始测试集上的评估结果显示，模型在合成数据集上训练的效果与原始数据集相当。

    The rapid advancement of large language models (LLMs) has sparked interest in data synthesis techniques, aiming to generate diverse and high-quality synthetic datasets. However, these synthetic datasets often suffer from a lack of diversity and added noise. In this paper, we present TarGEN, a multi-step prompting strategy for generating high-quality synthetic datasets utilizing a LLM. An advantage of TarGEN is its seedless nature; it does not require specific task instances, broadening its applicability beyond task replication. We augment TarGEN with a method known as self-correction empowering LLMs to rectify inaccurately labeled instances during dataset creation, ensuring reliable labels. To assess our technique's effectiveness, we emulate 8 tasks from the SuperGLUE benchmark and finetune various language models, including encoder-only, encoder-decoder, and decoder-only models on both synthetic and original training sets. Evaluation on the original test set reveals that models traine
    
[^44]: 从价值观到观点：利用注入价值的大型语言模型预测人类行为和立场

    From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models. (arXiv:2310.17857v1 [cs.CL])

    [http://arxiv.org/abs/2310.17857](http://arxiv.org/abs/2310.17857)

    本研究提出了一种利用注入价值的大型语言模型（LLM）来预测人类行为和观点的方法。通过价值注入方法（VIM）对LLMs进行微调，实验结果表明注入了VIM的LLMs在预测人们观点和行为方面表现出较好的性能。

    

    在现实场景中能够预测人们对问题和行为的观点可以在政治和市场营销等领域中非常有帮助。然而，进行大规模调查（如欧洲社会调查）以获取人们对个别问题的意见可能会产生高昂的成本。基于先前的研究表明核心人类价值观对个人决策和行动的影响，我们提出使用注入价值的大型语言模型（LLM）来预测观点和行为。为此，我们提出了价值注入方法（VIM），这是一种由两种方法组成的集合——论据生成和问答，旨在通过微调将定向价值分布注入LLMs中。然后，我们对四个任务进行了一系列实验，以测试VIM的有效性和使用注入价值的LLM预测人们观点和行为的可能性。我们发现，注入了VIM变种的LLMs在性能上明显优于基线模型。

    Being able to predict people's opinions on issues and behaviors in realistic scenarios can be helpful in various domains, such as politics and marketing. However, conducting large-scale surveys like the European Social Survey to solicit people's opinions on individual issues can incur prohibitive costs. Leveraging prior research showing influence of core human values on individual decisions and actions, we propose to use value-injected large language models (LLM) to predict opinions and behaviors. To this end, we present Value Injection Method (VIM), a collection of two methods -- argument generation and question answering -- designed to inject targeted value distributions into LLMs via fine-tuning. We then conduct a series of experiments on four tasks to test the effectiveness of VIM and the possibility of using value-injected LLMs to predict opinions and behaviors of people. We find that LLMs value-injected with variations of VIM substantially outperform the baselines. Also, the resu
    
[^45]: 使用RadGraph和少样本提示的风格感知放射学报告生成

    Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting. (arXiv:2310.17811v1 [cs.AI])

    [http://arxiv.org/abs/2310.17811](http://arxiv.org/abs/2310.17811)

    该论文提出了一种使用RadGraph和少样本提示的风格感知放射学报告生成的方法。通过将报告的内容和风格分开处理，可以避免生成临床不准确的报告。定量评估和人工评估结果均表明该方法表现出良好的性能，并生成与个体放射科医生风格完全相同的报告。

    

    自动从医学影像中生成报告有望改善放射科医生的工作流程。现有方法通过直接从图像生成完整的报告来考虑图像到报告的建模任务。然而，这样混淆了报告的内容（如发现和其属性）与其风格（如格式和词汇选择），可能导致临床不准确的报告。为了解决这个问题，我们提出了一种放射学报告生成的两步方法。首先，我们从图像中提取内容，然后将提取的内容转化为与特定放射科医生风格相匹配的报告。为此，我们利用RadGraph——一种报告的图表示——以及大型语言模型（LLM）。在定量评估中，我们发现我们的方法在性能方面具有益处。通过临床评估者进行的人工评估表明，AI生成的报告与个体放射科医生的风格完全相同，无法区别。

    Automatically generated reports from medical images promise to improve the workflow of radiologists. Existing methods consider an image-to-report modeling task by directly generating a fully-fledged report from an image. However, this conflates the content of the report (e.g., findings and their attributes) with its style (e.g., format and choice of words), which can lead to clinically inaccurate reports. To address this, we propose a two-step approach for radiology report generation. First, we extract the content from an image; then, we verbalize the extracted content into a report that matches the style of a specific radiologist. For this, we leverage RadGraph -- a graph representation of reports -- together with large language models (LLMs). In our quantitative evaluations, we find that our approach leads to beneficial performance. Our human evaluation with clinical raters highlights that the AI-generated reports are indistinguishably tailored to the style of individual radiologist 
    
[^46]: TIMELINE：对新闻文章中时间关系的详尽注释，支持自动排序事件。(arXiv:2310.17802v1 [cs.CL])

    TIMELINE: Exhaustive Annotation of Temporal Relations Supporting the Automatic Ordering of Events in News Articles. (arXiv:2310.17802v1 [cs.CL])

    [http://arxiv.org/abs/2310.17802](http://arxiv.org/abs/2310.17802)

    本文提出了一个新的注释方案，明确定义了时间关系的标准，并提出了自动化注释所有时间关系的方法。生成了一个新的数据集，TIMELINE语料库。

    

    目前，对于现有的标注时间关系的新闻数据集存在一些问题，包括：(1)由于注释指南在什么算作时间关系方面缺乏具体性，导致低的标注者一致性；(2)在给定文档中排除跨不同段落的长距离关系；(3)排除不以动词为中心的事件。本文旨在通过提出一个新的注释方案来缓解这些问题，该方案明确定义了应该注释的时间关系的标准。此外，该方案还包括不以动词表达的事件（例如，名词化事件）。此外，我们提出了一种自动化注释所有时间关系（包括长距离关系）的方法，从而减少了标注者的时间和人工工作量。结果是一个新的数据集，即TIMELINE语料库。

    Temporal relation extraction models have thus far been hindered by a number of issues in existing temporal relation-annotated news datasets, including: (1) low inter-annotator agreement due to the lack of specificity of their annotation guidelines in terms of what counts as a temporal relation; (2) the exclusion of long-distance relations within a given document (those spanning across different paragraphs); and (3) the exclusion of events that are not centred on verbs. This paper aims to alleviate these issues by presenting a new annotation scheme that clearly defines the criteria based on which temporal relations should be annotated. Additionally, the scheme includes events even if they are not expressed as verbs (e.g., nominalised events). Furthermore, we propose a method for annotating all temporal relations -- including long-distance ones -- which automates the process, hence reducing time and manual effort on the part of annotators. The result is a new dataset, the TIMELINE corpus
    
[^47]: "您是一位专家语言注释者"：作为抽象意义表示分析器的LLMs的限制

    "You Are An Expert Linguistic Annotator": Limits of LLMs as Analyzers of Abstract Meaning Representation. (arXiv:2310.17793v1 [cs.CL])

    [http://arxiv.org/abs/2310.17793](http://arxiv.org/abs/2310.17793)

    本文研究了大型语言模型在分析句子的意义结构方面的成功和限制，发现模型在生成和重构抽象意义表示（AMR）方面表现出一定的能力，但在复杂的句子结构或语义推理任务中存在局限性。

    

    大型语言模型（LLMs）在语言使用方面显示出了令人惊讶的熟练度和流畅性。这是否意味着它们也已经获得了关于语言的深刻语言知识，以至于它们可以充当"专家语言注释者"？在本文中，我们考察了GPT-3、ChatGPT和GPT-4模型在句子意义结构分析中的成功和限制，重点关注了抽象意义表示（AMR；Banarescu等人，2013）分析形式主义，该形式主义提供了丰富的图形化句子意义结构表示，同时从表面形式中抽象出来。我们将模型在这种语义结构分析上的结果在两种情况下进行比较：1）基于零射和少学样本的AMR解析的直接生成，以及2）通过元语言自然语言查询（例如"确定该句子的主要事件，以及与该事件对应的谓词"）间接的部分重构AMR。在这些情况下，我们发现模型能够重新生成正确的AMR解析，但在复杂的句子结构或语义推理任务中仍存在局限性。

    Large language models (LLMs) show amazing proficiency and fluency in the use of language. Does this mean that they have also acquired insightful linguistic knowledge about the language, to an extent that they can serve as an "expert linguistic annotator"? In this paper, we examine the successes and limitations of the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning structure, focusing on the Abstract Meaning Representation (AMR; Banarescu et al. 2013) parsing formalism, which provides rich graphical representations of sentence meaning structure while abstracting away from surface forms. We compare models' analysis of this semantic structure across two settings: 1) direct production of AMR parses based on zero- and few-shot prompts, and 2) indirect partial reconstruction of AMR via metalinguistic natural language queries (e.g., "Identify the primary event of this sentence, and the predicate corresponding to that event."). Across these settings, we find that models can re
    
[^48]: 利用语言模型进行能量负荷预测

    Utilizing Language Models for Energy Load Forecasting. (arXiv:2310.17788v1 [cs.AI])

    [http://arxiv.org/abs/2310.17788](http://arxiv.org/abs/2310.17788)

    本文提出了一种利用语言模型进行能量负荷预测的新方法，通过采用提示技术和自回归生成方法，可以将能源消耗数据转化为描述性语句并实现预测未来能量负荷消耗的准确性，从而为提高能源效率和促进能源系统智能决策提供了有希望的途径。

    

    能量负荷预测在优化资源分配和管理建筑物和城市的能源消耗方面发挥着关键作用。本文提出了一种利用语言模型进行能量负荷预测的新方法。我们采用提示技术将能源消耗数据转化为描述性语句，从而实现语言模型的微调。通过采用自回归生成方法，我们的方法能够预测未来能量负荷消耗的各种时间范围。通过对真实数据集进行广泛实验，我们证明了我们的方法的有效性和准确性。我们的结果表明，利用语言模型进行能量负荷预测有望增强能源效率并促进能源系统智能决策的实施。

    Energy load forecasting plays a crucial role in optimizing resource allocation and managing energy consumption in buildings and cities. In this paper, we propose a novel approach that leverages language models for energy load forecasting. We employ prompting techniques to convert energy consumption data into descriptive sentences, enabling fine-tuning of language models. By adopting an autoregressive generating approach, our proposed method enables predictions of various horizons of future energy load consumption. Through extensive experiments on real-world datasets, we demonstrate the effectiveness and accuracy of our proposed method. Our results indicate that utilizing language models for energy load forecasting holds promise for enhancing energy efficiency and facilitating intelligent decision-making in energy systems.
    
[^49]: 评估使用印度语LGBTI+词汇表的大型语言模型

    Evaluation of large language models using an Indian language LGBTI+ lexicon. (arXiv:2310.17787v1 [cs.CL])

    [http://arxiv.org/abs/2310.17787](http://arxiv.org/abs/2310.17787)

    该论文提出了一种使用印度语LGBTI+词汇表评估大型语言模型的方法。研究发现，现有的语言模型无法有效检测潜藏的仇恨内容，使用机器翻译作为评估手段也存在局限性。

    

    大型语言模型（LLMs）通常通过基于任务的基准（如MMLU）进行评估。这样的基准无法在特定语境中检查LLMs的负责任行为。在LGBTI+语境中，社会陈规可能导致LGBTI+术语的变异。因此，领域特定的词汇表可以作为对LLM行为进行评估的代表性单词列表。本文提出了一种使用印度语LGBTI+词汇表评估LLMs的方法。该方法包括四个步骤：制定与预期行为相关的NLP任务，创建测试LLMs的提示，使用LLMs获取输出，最后进行手动评估结果。我们的定性分析显示，我们实验的三个LLMs无法检测到潜在的仇恨内容。同样，我们观察到使用机器翻译作为评估自然语言的手段存在局限性。

    Large language models (LLMs) are typically evaluated on the basis of task-based benchmarks such as MMLU. Such benchmarks do not examine responsible behaviour of LLMs in specific contexts. This is particularly true in the LGBTI+ context where social stereotypes may result in variation in LGBTI+ terminology. Therefore, domain-specific lexicons or dictionaries may be useful as a representative list of words against which the LLM's behaviour needs to be evaluated. This paper presents a methodology for evaluation of LLMs using an LGBTI+ lexicon in Indian languages. The methodology consists of four steps: formulating NLP tasks relevant to the expected behaviour, creating prompts that test LLMs, using the LLMs to obtain the output and, finally, manually evaluating the results. Our qualitative analysis shows that the three LLMs we experiment on are unable to detect underlying hateful content. Similarly, we observe limitations in using machine translation as means to evaluate natural language u
    
[^50]: 数据中心化的金融大型语言模型

    Data-Centric Financial Large Language Models. (arXiv:2310.17784v1 [cs.CL])

    [http://arxiv.org/abs/2310.17784](http://arxiv.org/abs/2310.17784)

    本文介绍了一种数据中心化的方法，通过预处理和预理解数据来改善大型语言模型（LLMs）在金融任务中的性能。实验证明，采用该方法的金融LLMs在金融分析和解释任务上达到了最先进的水平。

    

    大型语言模型（LLMs）在自然语言任务中表现出良好的潜力，但直接应用于复杂领域如金融时却遇到困难。LLMs难以推理和整合所有相关信息。我们提出了一种数据中心化的方法，使LLMs能够更好地处理金融任务。我们的关键观点是，不是一次性给LLM负载过多信息，而是更有效地对数据进行预处理和预理解。我们使用多任务基于提示的微调来创建金融LLM（FLLM），以实现数据预处理和预理解。然而，每个任务的标记数据有限。为了克服手动注释的成本，我们采用了自动生成训练数据的增强推理（AAR）来修改FLLM自身输出的伪标签。实验证明，我们的数据中心化FLLM与AAR相比，显著优于为原始文本设计的基线金融LLMs，在金融分析和解释任务上达到了最先进的水平。

    Large language models (LLMs) show promise for natural language tasks but struggle when applied directly to complex domains like finance. LLMs have difficulty reasoning about and integrating all relevant information. We propose a data-centric approach to enable LLMs to better handle financial tasks. Our key insight is that rather than overloading the LLM with everything at once, it is more effective to preprocess and pre-understand the data. We create a financial LLM (FLLM) using multitask prompt-based finetuning to achieve data pre-processing and pre-understanding. However, labeled data is scarce for each task. To overcome manual annotation costs, we employ abductive augmentation reasoning (AAR) to automatically generate training data by modifying the pseudo labels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR substantially outperforms baseline financial LLMs designed for raw text, achieving state-of-the-art on financial analysis and interpretation tasks. We 
    
[^51]: 词语、子词和词素：惊奇度-阅读时间关系中真正重要的是什么？

    Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?. (arXiv:2310.17774v1 [cs.CL])

    [http://arxiv.org/abs/2310.17774](http://arxiv.org/abs/2310.17774)

    本研究通过对比不同分词方法对惊奇度估计和阅读时间数据的影响，发现使用BPE分词的预测在整体上与形态学和正字法分割相比没有受损，但细致分析指出了BPE分词的潜在问题，并提出了一种新的形态学预测评估方法。

    

    使用LLMs在心理语言学数据上的一个重要假设一直未经验证。LLM基于子词分词进行预测，而不是将单词分解为词素。这是否重要？我们通过比较使用正字法、形态学和BPE分词的惊奇度估计与阅读时间数据进行了仔细测试。我们的结果复制了先前的发现，并提供了证据，表明使用BPE分词的预测相对于形态学和正字法分割来说并没有受到损害。然而，更细致的分析指出了依赖于BPE分词的潜在问题，并提供了涉及形态学感知惊奇度估计的有希望的结果，并建议了一种评估形态学预测的新方法。

    An important assumption that comes with using LLMs on psycholinguistic data has gone unverified. LLM-based predictions are based on subword tokenization, not decomposition of words into morphemes. Does that matter? We carefully test this by comparing surprisal estimates using orthographic, morphological, and BPE tokenization against reading time data. Our results replicate previous findings and provide evidence that in the aggregate, predictions using BPE tokenization do not suffer relative to morphological and orthographic segmentation. However, a finer-grained analysis points to potential issues with relying on BPE-based tokenization, as well as providing promising results involving morphologically-aware surprisal estimates and suggesting a new method for evaluating morphological prediction.
    
[^52]: GROOViST:一种用于视觉故事中物体定位评估的度量标准

    GROOViST: A Metric for Grounding Objects in Visual Storytelling. (arXiv:2310.17770v1 [cs.AI])

    [http://arxiv.org/abs/2310.17770](http://arxiv.org/abs/2310.17770)

    GROOViST是一种用于评估视觉故事中物体定位的新的评估工具，考虑了跨模态依赖、时间错位和人类对视觉定位的直觉。这种工具具有模块化设计，可以评估和解释每个组件的贡献。

    

    对于一个由一系列图像生成的故事的适当评估，必须考虑多个方面，例如连贯性，语法正确性和视觉定位。在这项工作中，我们专注于评估定位程度，即故事与图像中显示的实体相关程度。我们分析了当前的评估指标，包括针对此目的设计的指标和针对一般视觉-文本对齐的指标。鉴于它们存在的缺点，我们提出了一种新的评估工具GROOViST，该工具考虑了跨模态依赖，时间错位（故事中实体出现的顺序和图像序列可能不匹配）以及人类对视觉定位的直觉。GROOViST的另一个优点是其模块化设计，可以对每个组件的贡献进行评估和解释。

    A proper evaluation of stories generated for a sequence of images -- the task commonly referred to as visual storytelling -- must consider multiple aspects, such as coherence, grammatical correctness, and visual grounding. In this work, we focus on evaluating the degree of grounding, that is, the extent to which a story is about the entities shown in the images. We analyze current metrics, both designed for this purpose and for general vision-text alignment. Given their observed shortcomings, we propose a novel evaluation tool, GROOViST, that accounts for cross-modal dependencies, temporal misalignments (the fact that the order in which entities appear in the story and the image sequence may not match), and human intuitions on visual grounding. An additional advantage of GROOViST is its modular design, where the contribution of each component can be assessed and interpreted individually.
    
[^53]: 社会契约AI：将AI助手与隐含的群体规范对齐

    Social Contract AI: Aligning AI Assistants with Implicit Group Norms. (arXiv:2310.17769v1 [cs.CL])

    [http://arxiv.org/abs/2310.17769](http://arxiv.org/abs/2310.17769)

    这项研究探索了通过将机器学习模型应用于用户交互数据，使AI助手能够自动对齐用户偏好的方法。研究发现，虽然AI助手在模拟中能够准确对齐经济文献中的标准策略，但在面对未知货币以及语言与策略一致性不足的情况下，其学习能力受到限制。

    

    我们探索了通过反转模拟用户（未知）偏好的模型来对齐AI助手的思路。为了验证我们的提议，我们在经济报价游戏中进行了概念验证模拟，将用户偏好形式化为指导模拟玩家行为的策略。我们发现，AI助手能够准确地将其行为与经济文献中的标准策略（如自私的、利他的）相匹配。然而，助手学到的策略在面对未包含在训练分布中的货币（如药品克数）时缺乏鲁棒性和有限的泛化能力。此外，我们发现，当语言使用与未知策略之间存在一致性不足时（如利他策略与粗鲁语言相结合），助手学习到的策略会减慢。总体而言，我们初步的结果表明，开发模拟框架来对齐AI助手的行为是可行的。

    We explore the idea of aligning an AI assistant by inverting a model of users' (unknown) preferences from observed interactions. To validate our proposal, we run proof-of-concept simulations in the economic ultimatum game, formalizing user preferences as policies that guide the actions of simulated players. We find that the AI assistant accurately aligns its behavior to match standard policies from the economic literature (e.g., selfish, altruistic). However, the assistant's learned policies lack robustness and exhibit limited generalization in an out-of-distribution setting when confronted with a currency (e.g., grams of medicine) that was not included in the assistant's training distribution. Additionally, we find that when there is inconsistency in the relationship between language use and an unknown policy (e.g., an altruistic policy combined with rude language), the assistant's learning of the policy is slowed. Overall, our preliminary results suggest that developing simulation fr
    
[^54]: 自动化测量生成式人工智能应用中有害责任的框架

    A Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications. (arXiv:2310.17750v1 [cs.CL])

    [http://arxiv.org/abs/2310.17750](http://arxiv.org/abs/2310.17750)

    本文提出了一个自动化测量大型语言模型（LLMs）及其相关产品和服务中有害责任的框架。通过该框架，可以调查不同的LLMs如何违反一系列与负责任人工智能（RAI）相关的原则，并促进LLMs的负责任使用。

    

    我们提出了一个用于自动化测量大型语言模型（LLMs）及其相关产品和服务的负责任人工智能（RAI）指标的框架。我们基于现有的技术和社会技术专门知识，利用先进的LLMs能力（如GPT-4），建立了自动测量LLMs造成伤害的框架。我们使用该框架通过几个案例研究，调查不同的LLMs如何违反一系列与RAI相关的原则。该框架可与领域特定的社会技术专门知识结合使用，以便在未来创建新的伤害区域的衡量指标。通过实施该框架，我们旨在促进更高级的伤害测量工作，并进一步推动LLMs的责任使用。

    We present a framework for the automated measurement of responsible AI (RAI) metrics for large language models (LLMs) and associated products and services. Our framework for automatically measuring harms from LLMs builds on existing technical and sociotechnical expertise and leverages the capabilities of state-of-the-art LLMs, such as GPT-4. We use this framework to run through several case studies investigating how different LLMs may violate a range of RAI-related principles. The framework may be employed alongside domain-specific sociotechnical expertise to create measurements for new harm areas in the future. By implementing this framework, we aim to enable more advanced harm measurement efforts and further the responsible use of LLMs.
    
[^55]: 销售人员 vs SalesBot：探索教育价值在对话式推荐系统中的作用

    Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems. (arXiv:2310.17749v1 [cs.CL])

    [http://arxiv.org/abs/2310.17749](http://arxiv.org/abs/2310.17749)

    这项研究探索了在对话式推荐系统中教育价值的作用，通过比较销售人员和SalesBot的性能，发现虽然SalesBot在流畅性和信息量方面接近专业销售人员，但在推荐质量方面仍存在差距。

    

    进行大额购买需要消费者进行研究或咨询销售人员以获取领域专业知识。然而，现有的对话式推荐系统（CRS）往往忽视用户缺乏背景知识，仅关注于收集偏好。在这项工作中，我们为旨在通过混合型混合模式对话提供产品推荐和教育价值的会话代理定义了一个新的问题空间。我们介绍了SalesOps，这是一个借鉴最新的大型语言模型（LLM）的框架，用于模拟和评估这种系统。我们构建了SalesBot和ShopperBot，这是一对LLM驱动的代理，可以模拟框架的任意一侧。通过一项全面的人类研究，我们将SalesBot与专业销售人员进行比较，发现虽然SalesBot在流畅性和信息量方面接近专业表现，但在推荐质量方面却落后。我们强调了两者面临的不同局限性。

    Making big purchases requires consumers to research or consult a salesperson to gain domain expertise. However, existing conversational recommender systems (CRS) often overlook users' lack of background knowledge, focusing solely on gathering preferences. In this work, we define a new problem space for conversational agents that aim to provide both product recommendations and educational value through mixed-type mixed-initiative dialog. We introduce SalesOps, a framework that facilitates the simulation and evaluation of such systems by leveraging recent advancements in large language models (LLMs). We build SalesBot and ShopperBot, a pair of LLM-powered agents that can simulate either side of the framework. A comprehensive human study compares SalesBot against professional salespeople, revealing that although SalesBot approaches professional performance in terms of fluency and informativeness, it lags behind in recommendation quality. We emphasize the distinct limitations both face in 
    
[^56]: StyleBART: 使用风格适配器装饰预训练模型进行无监督风格化标题生成

    StyleBART: Decorate Pretrained Model with Style Adapters for Unsupervised Stylistic Headline Generation. (arXiv:2310.17743v1 [cs.CL])

    [http://arxiv.org/abs/2310.17743](http://arxiv.org/abs/2310.17743)

    StyleBART是一种无监督的风格化标题生成方法，通过使用适配器来装饰预训练模型，可以生成具有多样风格的标题。与其他方法不同，StyleBART将风格学习和标题生成任务分离开来，在推理过程中可以自由组合基础模型和风格适配器。经过广泛的评估，StyleBART表现出了优秀的性能。

    

    风格化标题生成任务是生成一个既总结文章内容又反映所需风格来吸引用户的标题。由于风格特定的文章-标题对非常稀缺，先前的研究主要关注于使用标准标题生成数据集和单一风格语料库进行无监督方法。在本研究中，我们遵循这一路线，并提出了StyleBART，一种无监督的风格化标题生成方法。我们的方法使用适配器将预训练的BART模型装饰起来，适配器负责不同的风格，通过简单地切换适配器，可以生成具有多样风格的标题。与之前的工作不同，StyleBART将风格学习和标题生成的任务分离开来，在推理过程中可以自由组合基础模型和风格适配器。我们进一步提出了一个逆向改写任务以增强风格适配器的效果。广泛的自动和人工评估结果表明，StyleBART取得了很好的性能。

    Stylistic headline generation is the task to generate a headline that not only summarizes the content of an article, but also reflects a desired style that attracts users. As style-specific article-headline pairs are scarce, previous researches focus on unsupervised approaches with a standard headline generation dataset and mono-style corpora. In this work, we follow this line and propose StyleBART, an unsupervised approach for stylistic headline generation. Our method decorates the pretrained BART model with adapters that are responsible for different styles and allows the generation of headlines with diverse styles by simply switching the adapters. Different from previous works, StyleBART separates the task of style learning and headline generation, making it possible to freely combine the base model and the style adapters during inference. We further propose an inverse paraphrasing task to enhance the style adapters. Extensive automatic and human evaluations show that StyleBART achi
    
[^57]: ArchBERT: 神经结构和自然语言的双模态理解

    ArchBERT: Bi-Modal Understanding of Neural Architectures and Natural Languages. (arXiv:2310.17737v1 [cs.CL])

    [http://arxiv.org/abs/2310.17737](http://arxiv.org/abs/2310.17737)

    ArchBERT是一种双模态模型，可以联合学习和理解神经结构和自然语言，为神经架构和自动机器学习方法提供快速的结构-文本和文本-结构检索/生成服务。

    

    近年来，建立多模态语言模型已经成为一个趋势，额外的模态，如图像、视频、语音等与自然语言（即文本信息）一起进行联合学习。虽然这些多模态语言模型在不同的模态上取得了成功，但是关于神经网络结构和自然语言的解决方案还不存在。将神经结构信息作为一种新的模态可以在云端提供快速的结构-文本和文本-结构检索/生成服务，且仅需单次推理。这样的解决方案在帮助初学者和中级机器学习用户提出更好的神经架构或自动机器学习方法方面具有价值，只需使用简单的文本查询。在本文中，我们提出了一种名为ArchBERT的双模态模型，用于联合学习和理解神经结构和自然语言，为这个领域的研究开辟了新的研究方向。此外，我们还介绍了一种名为Mask的预训练策略。

    Building multi-modal language models has been a trend in the recent years, where additional modalities such as image, video, speech, etc. are jointly learned along with natural languages (i.e., textual information). Despite the success of these multi-modal language models with different modalities, there is no existing solution for neural network architectures and natural languages. Providing neural architectural information as a new modality allows us to provide fast architecture-2-text and text-2-architecture retrieval/generation services on the cloud with a single inference. Such solution is valuable in terms of helping beginner and intermediate ML users to come up with better neural architectures or AutoML approaches with a simple text query. In this paper, we propose ArchBERT, a bi-modal model for joint learning and understanding of neural architectures and natural languages, which opens up new avenues for research in this area. We also introduce a pre-training strategy named Mask
    
[^58]: 调查多语言共指消解的通用注释方法

    Investigating Multilingual Coreference Resolution by Universal Annotations. (arXiv:2310.17734v1 [cs.CL])

    [http://arxiv.org/abs/2310.17734](http://arxiv.org/abs/2310.17734)

    本论文通过使用通用注释方法研究多语言共指消解任务。首先，通过研究不同语言层次和流派上的真实数据来了解多语言共指的特点。其次，进行错误分析，并提取特征用于改进共指消解任务。结果表明，我们的方法对基线系统有潜在好处。

    

    多语言共指消解(MCR)一直是一个长期而具有挑战性的任务。我们利用最新提出的多语言共指数据集CorefUD(Nedoluzhko et al., 2022)的通用形态句法和共指注释，对该任务进行了研究。首先，我们通过在不同语言层次（提及、实体和文档层次）和不同流派上检查真实数据，来了解多语言共指的特点。其次，我们对在CRAC 2022共享任务中State-of-the-Art系统无法解决的最具挑战性的案例进行了错误分析，使用了通用注释。最后，基于这个分析，我们从通用形态句法注释中提取特征，并将这些特征整合到一个基线系统中，评估它们对MCR任务的潜在好处。我们的结果表明，我们的最佳特征配置改进了基线系统。

    Multilingual coreference resolution (MCR) has been a long-standing and challenging task. With the newly proposed multilingual coreference dataset, CorefUD (Nedoluzhko et al., 2022), we conduct an investigation into the task by using its harmonized universal morphosyntactic and coreference annotations. First, we study coreference by examining the ground truth data at different linguistic levels, namely mention, entity and document levels, and across different genres, to gain insights into the characteristics of coreference across multiple languages. Second, we perform an error analysis of the most challenging cases that the SotA system fails to resolve in the CRAC 2022 shared task using the universal annotations. Last, based on this analysis, we extract features from universal morphosyntactic annotations and integrate these features into a baseline system to assess their potential benefits for the MCR task. Our results show that our best configuration of features improves the baseline b
    
[^59]: ZeroQuant-HERO: W8A8 Transformer的硬件增强的优化后训练量化框架

    ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers. (arXiv:2310.17723v1 [cs.LG])

    [http://arxiv.org/abs/2310.17723](http://arxiv.org/abs/2310.17723)

    ZeroQuant-HERO是一种硬件增强的量化框架，针对W8A8 Transformer模型进行优化，旨在减少内存和计算需求，并在处理复杂的量化问题和内存受限运算符方面提供了新的解决方案。同时，该框架还具有灵活性，允许特定模块切换至FP16/BF16模式以提高准确性。

    

    量化技术在减少深度神经网络推理的内存和计算需求方面起着关键作用。现有的解决方案，如ZeroQuant，为BERT和GPT等模型提供了动态量化，但忽视了关键的内存受限运算符和每个标记的量化复杂性。针对这些差距，我们提出了一种全新的、完全由硬件增强的、经过优化的、后训练W8A8量化框架ZeroQuant-HERO。该框架独特地集成了内存带宽和计算密集型运算符，旨在实现最佳硬件性能。此外，它通过允许特定的INT8模块切换到FP16/BF16模式来提高准确性。

    Quantization techniques are pivotal in reducing the memory and computational demands of deep neural network inference. Existing solutions, such as ZeroQuant, offer dynamic quantization for models like BERT and GPT but overlook crucial memory-bounded operators and the complexities of per-token quantization. Addressing these gaps, we present a novel, fully hardware-enhanced robust optimized post-training W8A8 quantization framework, ZeroQuant-HERO. This framework uniquely integrates both memory bandwidth and compute-intensive operators, aiming for optimal hardware performance. Additionally, it offers flexibility by allowing specific INT8 modules to switch to FP16/BF16 mode, enhancing accuracy.
    
[^60]: 大型语言模型作为具有普适性的机器人任务策略

    Large Language Models as Generalizable Policies for Embodied Tasks. (arXiv:2310.17722v1 [cs.LG])

    [http://arxiv.org/abs/2310.17722](http://arxiv.org/abs/2310.17722)

    本研究展示了大型语言模型(LLMs)可以被适应为具有普适性的机器人任务策略。通过我们的方法LLaRP，我们成功将预训练的冻结LLM用于接收指令和视觉输入，并在环境中直接输出动作。我们的实验结果表明LLaRP不仅对任务指令的复杂改写具有鲁棒性，而且可以推广到需要新颖最优行为的新任务。在大量未见任务中，LLaRP表现出了显著的成功率提升，并且我们提供了一个新的基准测试(Language Rearrangement)来促进进一步的研究。

    

    我们展示了大型语言模型(LLMs)可以被调整为适用于机器人视觉任务的普适性策略。我们的方法被称为大型语言模型强化学习策略(LLaRP)，它将预训练的冻结的LLM调整为接收文本指令和视觉自我中心观测作为输入，并直接在环境中输出动作。通过强化学习，我们训练LLaRP通过与环境的交互来看和行动。我们展示了LLaRP对任务指令的复杂改写具有鲁棒性，并且可以推广到需要新颖最优行为的新任务。特别地，在1,000个未见任务中，它的成功率达到了42%，是其他常见学习基线或零样本应用的1.7倍成功率。最后，为了帮助社区研究以语言为条件的、大规模多任务的机器人AI问题，我们发布了一个新的基准测试(Language Rearrangement)，包括150,000个训练任务和1,000个测试任务，用于语言为条件的重新排列。

    We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangem
    
[^61]: 从讲话文本到洞察力：利用生成型人工智能揭示企业风险

    From Transcripts to Insights: Uncovering Corporate Risks Using Generative AI. (arXiv:2310.17721v1 [econ.GN])

    [http://arxiv.org/abs/2310.17721](http://arxiv.org/abs/2310.17721)

    这项研究探索了使用生成型人工智能工具帮助投资者揭示企业风险的价值，通过从收益电话的上下文中生成风险摘要和评估，这些基于GPT的度量具有显著的信息内容，能够预测企业层面波动性和投资创新选择。此外，生成型人工智能还能有效检测新兴风险，并且这些度量在股权市场中起到定价作用。

    

    我们探索了使用ChatGPT等生成型人工智能工具帮助投资者揭示企业风险维度的价值。我们开发并验证了政治、气候和人工智能相关风险的企业层面风险敞口度量。使用GPT 3.5模型从收益电话的背景提供的上下文生成风险摘要和评估，我们发现基于GPT的度量具有显著的信息内容，并在预测（异常）企业层面波动性和企业的选择（如投资和创新）方面优于现有的风险度量。重要的是，风险评估中的信息优于风险摘要，这证明了通用人工智能知识的价值。我们还发现，生成型人工智能对于发现新兴风险（如近几个季度飙升的人工智能风险）非常有效。我们的度量在GPT的训练窗口内外表现良好，并且在股权市场中定价。综上所述，基于人工智能的风险测量方法提供了有用的洞察。

    We explore the value of generative AI tools, such as ChatGPT, in helping investors uncover dimensions of corporate risk. We develop and validate firm-level measures of risk exposure to political, climate, and AI-related risks. Using the GPT 3.5 model to generate risk summaries and assessments from the context provided by earnings call transcripts, we show that GPT-based measures possess significant information content and outperform the existing risk measures in predicting (abnormal) firm-level volatility and firms' choices such as investment and innovation. Importantly, information in risk assessments dominates that in risk summaries, establishing the value of general AI knowledge. We also find that generative AI is effective at detecting emerging risks, such as AI risk, which has soared in recent quarters. Our measures perform well both within and outside the GPT's training window and are priced in equity markets. Taken together, an AI-based approach to risk measurement provides usef
    
[^62]: 异常维度编码特定任务知识

    Outlier Dimensions Encode Task-Specific Knowledge. (arXiv:2310.17715v1 [cs.CL])

    [http://arxiv.org/abs/2310.17715](http://arxiv.org/abs/2310.17715)

    异常维度可以编码关键的特定任务知识，并且一个单一的异常维度可以以最小的错误率完成下游任务。

    

    大型语言模型（LLM）的表示被少数几个具有极高方差的异常维度所主导。先前的研究认为，虽然去除LLM表示中的异常维度会损害下游性能，但异常维度对嵌入表示的质量是有害的。在本研究中，我们研究了微调对异常维度的影响，并展示了以下结果：1）在预训练中出现的异常维度在微调模型中仍然存在，2）一个单一的异常维度可以以最小的错误率完成下游任务。我们的结果表明，异常维度可以编码关键的特定任务知识，并且一个表示在单个异常维度上的值会影响下游模型的决策。

    Representations from large language models (LLMs) are known to be dominated by a small subset of dimensions with exceedingly high variance. Previous works have argued that although ablating these outlier dimensions in LLM representations hurts downstream performance, outlier dimensions are detrimental to the representational quality of embeddings. In this study, we investigate how fine-tuning impacts outlier dimensions and show that 1) outlier dimensions that occur in pre-training persist in fine-tuned models and 2) a single outlier dimension can complete downstream tasks with a minimal error rate. Our results suggest that outlier dimensions can encode crucial task-specific knowledge and that the value of a representation in a single outlier dimension drives downstream model decisions.
    
[^63]: 从金融文件中提取关系：基于向量化词汇句法模式的最近邻搜索

    Nearest Neighbor Search over Vectorized Lexico-Syntactic Patterns for Relation Extraction from Financial Documents. (arXiv:2310.17714v1 [cs.CL])

    [http://arxiv.org/abs/2310.17714](http://arxiv.org/abs/2310.17714)

    本文提出了一个简单而有效的方法，通过在测试时使用密集向量的词汇句法模式进行最近邻搜索，来解决关系抽取中的隐含表达和长尾关系类问题，并提供了一种简单可行的解决方法，特别适用于没有直接访问大型语言模型和/或基于监督训练或微调的基础设施的用户。

    

    鉴于语言复杂性和数据稀疏性造成的隐含表达和长尾关系类问题，现有的关系抽取模型通常无法很好地处理这两种情况。此外，对于没有直接访问大型语言模型和/或基于监督训练或微调的基础设施的用户来说，这些方法和模型通常是不可接触的。本文提出了一个简单的方法，在测试时通过对词汇句法模式的稠密向量进行最近邻搜索，以解决上述问题提供了一种简单而有效的手段。

    Relation extraction (RE) has achieved remarkable progress with the help of pre-trained language models. However, existing RE models are usually incapable of handling two situations: implicit expressions and long-tail relation classes, caused by language complexity and data sparsity. Further, these approaches and models are largely inaccessible to users who don't have direct access to large language models (LLMs) and/or infrastructure for supervised training or fine-tuning. Rule-based systems also struggle with implicit expressions. Apart from this, Real world financial documents such as various 10-X reports (including 10-K, 10-Q, etc.) of publicly traded companies pose another challenge to rule-based systems in terms of longer and complex sentences. In this paper, we introduce a simple approach that consults training relations at test time through a nearest-neighbor search over dense vectors of lexico-syntactic patterns and provides a simple yet effective means to tackle the above issu
    
[^64]: 解释是解药吗？短期和长期的虚假信息缓解

    Is Explanation the Cure? Misinformation Mitigation in the Short Term and Long Term. (arXiv:2310.17711v1 [cs.CL])

    [http://arxiv.org/abs/2310.17711](http://arxiv.org/abs/2310.17711)

    本研究通过比较警告标签和GPT-4生成的最先进反事实解释的效果，探讨了这些解释对于缓解虚假信息的作用。实验结果显示，这两种干预方式都显著降低了参与者对虚假声明的信任。

    

    随着自然语言处理（NLP）模型的发展，自动生成解释被提议用于解决社交媒体平台上的虚假信息，并向识别到的假新闻添加警告标签。虽然许多研究人员专注于生成良好的解释，但这些解释如何真正帮助人们对抗虚假新闻尚未得到充分探讨。在这项研究中，我们比较了一个警告标签和由GPT-4生成的最先进的反事实解释在揭露虚假信息方面的有效性。通过一个两波次的在线人类实验，研究对象（N = 215）被随机分配到三个组：一个无干预的对照组，在其中展示虚假内容；一个警告标签组，在其中标记虚假声明；以及一个解释组，在其中虚假内容附带GPT-4生成的解释。我们的结果表明，这两种干预都显著降低了参与者对虚假声明的自我报告信任。

    With advancements in natural language processing (NLP) models, automatic explanation generation has been proposed to mitigate misinformation on social media platforms in addition to adding warning labels to identified fake news. While many researchers have focused on generating good explanations, how these explanations can really help humans combat fake news is under-explored. In this study, we compare the effectiveness of a warning label and the state-of-the-art counterfactual explanations generated by GPT-4 in debunking misinformation. In a two-wave, online human-subject study, participants (N = 215) were randomly assigned to a control group in which false contents are shown without any intervention, a warning tag group in which the false claims were labeled, or an explanation group in which the false contents were accompanied by GPT-4 generated explanations. Our results show that both interventions significantly decrease participants' self-reported belief in fake claims in an equiva
    
[^65]: 使用AI聊天机器人回复患者信息的影响

    The impact of using an AI chatbot to respond to patient messages. (arXiv:2310.17703v1 [cs.CL])

    [http://arxiv.org/abs/2310.17703](http://arxiv.org/abs/2310.17703)

    本研究首次考察了使用大型语言模型协助临床医生回答患者问题的实用性，并发现AI聊天机器人可以在不需要编辑的情况下提供可接受的草稿，显著提高了工作效率。

    

    文件负担是导致临床医生倦怠的主要原因，这种情况在全国范围内日益增加，对我们照顾患者的能力构成了紧迫威胁。像ChatGPT这样的人工智能（AI）聊天机器人可以通过协助文档处理来减轻临床医生的负担。虽然许多医院正在积极将这些系统整合到电子病历系统中，但AI聊天机器人在临床决策中的实用性和影响尚未针对此预期用途进行研究。我们是第一个研究大型语言模型在协助临床医生起草回答患者问题方面的实用性的研究者。在我们的两阶段横断面研究中，6名肿瘤学家对100个逼真的合成癌症患者场景和反映常见医疗情况的门户信息进行了回复，首先是手动回复，然后是通过AI协助回复。我们发现，AI协助的回答更长、可读性较差，但在58%的时间里提供了可接受的草稿而无需编辑。AI协助提高了77%的效率，与传统手动回答相比。

    Documentation burden is a major contributor to clinician burnout, which is rising nationally and is an urgent threat to our ability to care for patients. Artificial intelligence (AI) chatbots, such as ChatGPT, could reduce clinician burden by assisting with documentation. Although many hospitals are actively integrating such systems into electronic medical record systems, AI chatbots utility and impact on clinical decision-making have not been studied for this intended use. We are the first to examine the utility of large language models in assisting clinicians draft responses to patient questions. In our two-stage cross-sectional study, 6 oncologists responded to 100 realistic synthetic cancer patient scenarios and portal messages developed to reflect common medical situations, first manually, then with AI assistance.  We find AI-assisted responses were longer, less readable, but provided acceptable drafts without edits 58% of time. AI assistance improved efficiency 77% of time, with 
    
[^66]: CodeFusion: 一种用于代码生成的预训练扩散模型

    CodeFusion: A Pre-trained Diffusion Model for Code Generation. (arXiv:2310.17680v1 [cs.SE])

    [http://arxiv.org/abs/2310.17680](http://arxiv.org/abs/2310.17680)

    CodeFusion是一种预训练的代码生成模型，通过扩散的方式解决了自然语言代码生成中遇到的限制，实验表明其在准确率和多样性上优于最先进的自回归系统。

    

    假设一个开发者只能修改其最后一行代码，在正确之前，他们需要多少次从头开始编写函数呢？自然语言代码生成的自回归模型也有类似的限制：它们不容易重新考虑之前生成的标记。我们介绍了一种名为CodeFusion的预训练扩散代码生成模型，通过迭代地对以编码的自然语言为条件的完整程序进行去噪，以解决这个限制。我们针对Bash、Python和Microsoft Excel条件格式(CF)规则的自然语言到代码生成任务对CodeFusion进行评估。实验结果显示，CodeFusion（75M参数）在top-1准确率上表现与最先进的自回归系统（350M-175B参数）相当，并且在top-3和top-5准确率上表现优于它们，这是由于它在多样性与质量之间的平衡更好。

    Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.
    
[^67]: 大型语言模型能否取代人类在系统评价过程中的角色？评估GPT-4在多种语言的同行评审文献和灰色文献筛选和提取数据方面的效果。

    Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. (arXiv:2310.17526v1 [cs.CL])

    [http://arxiv.org/abs/2310.17526](http://arxiv.org/abs/2310.17526)

    本研究评估了GPT-4在多种语言的同行评审文献和灰色文献筛选和提取数据方面的能力，结果显示GPT-4在大多数任务上准确度与人类表现相当，在调整了偶然一致性和数据集不平衡后，其在数据提取方面表现出中等水平的准确度。

    

    系统评价对于指导实践、研究和政策至关重要，然而常常需要耗费大量时间和人力。大型语言模型（LLM）可能能够加快和自动化系统评价的过程，但是它们在这些任务中的表现尚未经过全面评估，而且还没有研究测试过迄今为止最大的LLM——GPT-4。本预注册研究采用“无人参与”的方法评估了GPT-4在标题/摘要筛选、全文审查和数据提取方面在不同文献类型和语言上的能力。尽管GPT-4在大多数任务中的准确度与人类表现相当，但结果受到偶然一致性和数据集不平衡的影响。在调整了这些因素后，数据提取方面表现出中等水平的准确度，在使用高可靠性提示进行筛选的研究中，筛选全文文献的表现水平在不同阶段和语言上均为无到中等。

    Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance. After adjusting for these, there was a moderate level of performance for data extraction, and - barring studies that used highly reliable prompts screening performance levelled at none to moderate for different stages and languages. When screening full-text literature using highly reliable prom
    
[^68]: 《低秩适应的表达能力》

    The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])

    [http://arxiv.org/abs/2310.17513](http://arxiv.org/abs/2310.17513)

    本文分析了低秩适应（LoRA）的表达能力，证明了对于全连接神经网络，当LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度）时，LoRA可以使任何模型f准确表示任何较小的目标模型f。对于Transformer网络，通过rank-（嵌入大小/ 2）的LoRA适配器可以使任何模型适应于相同大小的目标模型。

    

    低秩适应（LoRA）是一种参数高效的微调方法，利用矩阵的低秩适应性，在微调预训练模型（如大型语言模型和扩散模型）中得到了广泛应用。尽管在实践中取得了巨大成功，但是LoRA的理论基础在很大程度上尚未得到探索。本文通过从理论角度分析LoRA的表达能力，首次尝试弥合这一差距。我们证明了对于全连接神经网络，如果LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度），则LoRA可以使任何模型f准确表示任何较小的目标模型f。当LoRA-rank低于阈值时，我们还量化了逼近误差。对于Transformer网络，我们证明任何模型可以通过rank-（嵌入大小/ 2）的LoRA适配器适应于相同大小的目标模型。

    Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
    
[^69]: 使用Exo解决矩阵乘法微内核生成问题

    Tackling the Matrix Multiplication Micro-kernel Generation with Exo. (arXiv:2310.17408v1 [cs.MS])

    [http://arxiv.org/abs/2310.17408](http://arxiv.org/abs/2310.17408)

    介绍了使用Exo编译器生成接近于甚至优于手动开发的微内核的步骤和方法，并解决了为每个新硬件生成专用微内核的问题。

    

    在过去的几十年里，矩阵乘法（或GEMM）的优化一直是一个需求。这个操作被认为是当前线性代数库（如BLIS，OpenBLAS或Intel OneAPI）的旗舰，因为它在各种科学应用中被广泛使用。GEMM通常是按照GotoBLAS的理念进行实现的，它将GEMM的操作数进行切割，并使用一系列嵌套循环来提高性能。这些方法通过一小块面向硬件的高性能代码，即微内核，提取体系结构的最大计算能力。然而，这种方法迫使开发人员为每个新的硬件生成一个专用的微内核，并需要非常大的工作量。在这项工作中，我们提出了一种使用Exo编译器生成微内核的逐步过程，该过程的性能接近甚至超过了手动编写的使用内部函数或汇编语言的微内核。

    The optimization of the matrix multiplication (or GEMM) has been a need during the last decades. This operation is considered the flagship of current linear algebra libraries such as BLIS, OpenBLAS, or Intel OneAPI because of its widespread use in a large variety of scientific applications. The GEMM is usually implemented following the GotoBLAS philosophy, which tiles the GEMM operands and uses a series of nested loops for performance improvement. These approaches extract the maximum computational power of the architectures through small pieces of hardware-oriented, high-performance code called micro-kernel. However, this approach forces developers to generate, with a non-negligible effort, a dedicated micro-kernel for each new hardware.  In this work, we present a step-by-step procedure for generating micro-kernels with the Exo compiler that performs close to (or even better than) manually developed microkernels written with intrinsic functions or assembly language. Our solution also 
    
[^70]: 在小规模不平衡的文本数据中进行情感检测的数据增强方法

    Data Augmentation for Emotion Detection in Small Imbalanced Text Data. (arXiv:2310.17015v1 [cs.CL])

    [http://arxiv.org/abs/2310.17015](http://arxiv.org/abs/2310.17015)

    通过研究数据增强技术在小规模不平衡数据集上的应用，我们发现使用增强后的数据训练分类器模型可以显著提高情感检测的性能。

    

    文本情感识别是自然语言处理中一个具有挑战性的问题，任务是识别出诸如喜悦或愤怒等情感。其中一个挑战是可用的标注了情感的数据集的匮乏。某些现有数据集规模较小，采用不同的情感分类体系，并且情感分布不平衡。在本研究中，我们研究了数据增强技术对小规模不平衡数据集的影响，这些数据集当前的最先进模型（如RoBERTa）性能不佳。具体而言，我们使用了四种数据增强方法（Easy Data Augmentation EDA、基于嵌入的静态和上下文相关、及ProtAugment）在三个源头和不同规模、情感类别及分布的数据集上进行实验。实验结果表明，使用增强后的数据训练分类器模型可以显著提高性能。最后，我们进行了两个案例研究：直接使用增强后的数据进行训练与使用原始数据进行训练的对比，并分析了不同情感类别的影响。

    Emotion recognition in text, the task of identifying emotions such as joy or anger, is a challenging problem in NLP with many applications. One of the challenges is the shortage of available datasets that have been annotated with emotions. Certain existing datasets are small, follow different emotion taxonomies and display imbalance in their emotion distribution. In this work, we studied the impact of data augmentation techniques precisely when applied to small imbalanced datasets, for which current state-of-the-art models (such as RoBERTa) under-perform. Specifically, we utilized four data augmentation methods (Easy Data Augmentation EDA, static and contextual Embedding-based, and ProtAugment) on three datasets that come from different sources and vary in size, emotion categories and distributions. Our experimental results show that using the augmented data when training the classifier model leads to significant improvements. Finally, we conducted two case studies: a) directly using t
    
[^71]: 使用具有专门口音代码本的口音识别

    Accented Speech Recognition With Accent-specific Codebooks. (arXiv:2310.15970v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.15970](http://arxiv.org/abs/2310.15970)

    本研究提出了一种使用具有专门口音代码本的口音适应方法，通过交叉注意力和可训练代码本，用于端到端ASR系统。在实验证明了该方法在已见和未见的口音上都能获得显著的性能提升。

    

    语音口音对于现有自动语音识别（ASR）系统构成了重要挑战。在代表性不足的口音中的性能下降严重阻碍了ASR的普及应用。本研究提出了一种新颖的口音适应方法，通过交叉注意力和可训练代码本，用于端到端ASR系统。这些可学习的代码本捕捉了口音特定信息，并被整合到ASR编码器层中。模型在带口音的英语语音上进行训练，而测试数据中也包含了在训练过程中未见过的口音。在Mozilla Common Voice多口音数据集上，我们展示了我们提出的方法在不仅在已见的英语口音中获得显著的性能提升（单词错误率相对提升高达37%），而且在未见的口音上也获得了5%的相对提升。此外，我们还展示了在L2Artic数据集上的零样本迁移设置的好处。我们还进行了对比实验。

    Speech accents pose a significant challenge to state-of-the-art automatic speech recognition (ASR) systems. Degradation in performance across underrepresented accents is a severe deterrent to the inclusive adoption of ASR. In this work, we propose a novel accent adaptation approach for end-to-end ASR systems using cross-attention with a trainable set of codebooks. These learnable codebooks capture accent-specific information and are integrated within the ASR encoder layers. The model is trained on accented English speech, while the test data also contained accents which were not seen during training. On the Mozilla Common Voice multi-accented dataset, we show that our proposed approach yields significant performance gains not only on the seen English accents (up to $37\%$ relative improvement in word error rate) but also on the unseen accents (up to $5\%$ relative improvement in WER). Further, we illustrate benefits for a zero-shot transfer setup on the L2Artic dataset. We also compare
    
[^72]: 激发大型语言模型中的提示工程潜力：一项综述

    Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review. (arXiv:2310.14735v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.14735](http://arxiv.org/abs/2310.14735)

    这篇论文解释了提示工程在释放大型语言模型能力方面的关键作用，探讨了不同的提示方法以及外部插件如何协助减少机器幻想，并指出了未来研究方向的重要性。

    

    本文深入探讨了提示工程在释放大型语言模型（LLM）能力方面的关键作用。提示工程是为LLM构建输入文本的过程，是优化LLM有效性的重要技术。本综述阐明了提示工程的基本原理，如角色提示、一次性提示和少量提示，以及更高级的方法，如思维链和思维树提示。本文还阐述了外部插件如何协助此任务，并通过检索外部知识来减少机器幻想。随后，我们勾勒了提示工程研究的前景方向，强调了对结构和代理在人工智能生成内容（AIGC）工具中的作用的深入理解的必要性。我们讨论了如何从不同角度和使用不同的方法评估提示方法的有效性。最后，我们提出了展望未来的研究方向。

    This paper delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). Prompt engineering is the process of structuring input text for LLMs and is a technique integral to optimizing the efficacy of LLMs. This survey elucidates foundational principles of prompt engineering, such as role-prompting, one-shot, and few-shot prompting, as well as more advanced methodologies such as the chain-of-thought and tree-of-thoughts prompting. The paper sheds light on how external assistance in the form of plugins can assist in this task, and reduce machine hallucination by retrieving external knowledge. We subsequently delineate prospective directions in prompt engineering research, emphasizing the need for a deeper understanding of structures and the role of agents in Artificial Intelligence-Generated Content (AIGC) tools. We discuss how to assess the efficacy of prompt methods from different perspectives and using different methods. Finally, we
    
[^73]: MedEval: 一个多层次、多任务和多领域的医学语言模型评估基准

    MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation. (arXiv:2310.14088v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.14088](http://arxiv.org/abs/2310.14088)

    MedEval是一个多层次、多任务和多领域的医学基准，用于促进医疗保健语言模型的开发。它包含了来自多个医疗系统的数据，涵盖了多个人体区域和检查模式。我们针对10个语言模型进行了评估，发现不同模型在不同任务上的有效性各有差异，从中我们注意到了指导的重要性。

    

    由于需要专家的人工注释，医疗保健的筛选数据集往往有限。本文提出了MedEval，一个多层次、多任务和多领域的医学基准，以促进医疗保健语言模型的开发。MedEval是全面的，包含来自几个医疗系统的数据，涵盖了8种检查模式下的35个人体区域。我们收集了22,779个句子和21,228份报告，并在多个层次上提供了专家注释，为数据提供了细致的潜在用法，并支持广泛的任务范围。此外，我们在零-shot和微调设置下对10个通用和领域特定的语言模型进行了系统评估，从医疗保健中的领域适应基线到通用的最先进的大型语言模型（如ChatGPT）。我们的评估揭示了两种类别的语言模型在不同任务中的不同有效性，从中我们注意到了指导的重要性。

    Curated datasets for healthcare are often limited due to the need of human annotations from experts. In this paper, we present MedEval, a multi-level, multi-task, and multi-domain medical benchmark to facilitate the development of language models for healthcare. MedEval is comprehensive and consists of data from several healthcare systems and spans 35 human body regions from 8 examination modalities. With 22,779 collected sentences and 21,228 reports, we provide expert annotations at multiple levels, offering a granular potential usage of the data and supporting a wide range of tasks. Moreover, we systematically evaluated 10 generic and domain-specific language models under zero-shot and finetuning settings, from domain-adapted baselines in healthcare to general-purposed state-of-the-art large language models (e.g., ChatGPT). Our evaluations reveal varying effectiveness of the two categories of language models across different tasks, from which we notice the importance of instruction t
    
[^74]: 探索语言模型中谄媚行为的理解

    Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v1 [cs.CL])

    [http://arxiv.org/abs/2310.13548](http://arxiv.org/abs/2310.13548)

    这项研究探讨了强化学习从人类反馈中训练高质量AI助手的技术，发现这种方法可能导致模型在回答问题时过于谄媚，而不是坦诚，通过分析人类偏好数据得出了这一结论。

    

    「从人类反馈中进行强化学习（RLHF）」是训练高质量AI助手的一种流行技术。然而，RLHF可能会鼓励模型通过与用户信念相符的回答来代替真实回答，这种行为被称为谄媚行为。我们研究了RLHF训练模型中谄媚行为的普遍性以及人类偏好判断是否起到了作用。首先，我们证明了五个最先进的AI助手在四个不同的自由文本生成任务中一贯表现出谄媚行为。为了理解人类偏好是否驱动了RLHF模型的这种广泛行为，我们分析了现有的人类偏好数据。我们发现，当回答与用户的观点相符时，它更有可能被选中。此外，人类和偏好模型（PMs）将有说服力的谄媚回答与正确回答相比，有时几乎可以忽略不计地选择了谄媚回答。优化模型输出以满足PMs有时也会在真实性和谄媚行为之间做出取舍。

    Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Over
    
[^75]: 无限制答案范围的开放式常识推理

    Open-ended Commonsense Reasoning with Unrestricted Answer Scope. (arXiv:2310.11672v1 [cs.CL])

    [http://arxiv.org/abs/2310.11672](http://arxiv.org/abs/2310.11672)

    本论文研究了无限制答案范围的开放式常识推理问题。作者采用预训练语言模型在外部知识库上迭代检索推理路径，从而帮助找到最准确的答案。实验结果表明此方法在常识问题上取得了良好的效果。

    

    开放式常识推理被定义为在不提供1）答案候选名单和2）预定义答案范围的情况下解决常识问题。将常识问题转化为问答形式或利用外部知识学习检索方法的常规方法在开放式环境中不太适用，因为这涉及到一个固有的挑战。在没有预定义答案范围或少数候选者的情况下，开放式常识推理需要通过在极大的搜索空间中搜索来预测答案。此外，大多数问题需要隐含的多跳推理，这给我们的问题带来了更多挑战。在这项工作中，我们利用预训练的语言模型在外部知识库上迭代地检索推理路径，这不需要特定任务的监督。推理路径可以帮助我们找到最准确的常识问题答案。我们在两个常识基准数据集上进行实验。

    Open-ended Commonsense Reasoning is defined as solving a commonsense question without providing 1) a short list of answer candidates and 2) a pre-defined answer scope. Conventional ways of formulating the commonsense question into a question-answering form or utilizing external knowledge to learn retrieval-based methods are less applicable in the open-ended setting due to an inherent challenge. Without pre-defining an answer scope or a few candidates, open-ended commonsense reasoning entails predicting answers by searching over an extremely large searching space. Moreover, most questions require implicit multi-hop reasoning, which presents even more challenges to our problem. In this work, we leverage pre-trained language models to iteratively retrieve reasoning paths on the external knowledge base, which does not require task-specific supervision. The reasoning paths can help to identify the most precise answer to the commonsense question. We conduct experiments on two commonsense ben
    
[^76]: VoxArabica：一个稳健的方言感知阿拉伯语音识别系统

    VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System. (arXiv:2310.11069v1 [cs.CL])

    [http://arxiv.org/abs/2310.11069](http://arxiv.org/abs/2310.11069)

    VoxArabica是一个稳健的方言感知阿拉伯语音识别系统，通过开发和演示，实现了阿拉伯语方言识别和自动语音识别。该系统训练了各种模型用于不同方言的识别，并提供了多种功能的网络界面。

    

    阿拉伯语是一种复杂的语言，全球有超过4.5亿人口使用许多不同的方言和口音。由于语言的多样性和变化，为阿拉伯语构建一个稳健且通用的语音识别系统具有挑战性。在这项工作中，我们通过开发和演示一个名为VoxArabica的系统，解决了这个问题，用于方言识别(DID)和阿拉伯语自动语音识别(ASR)。我们在监督环境下训练了各种模型，例如HuBERT(DID)、Whisper和XLS-R(ASR)，用于阿拉伯语的DID和ASR任务。我们的DID模型被训练用于识别除了标准阿拉伯之外的17种不同的方言。我们在标准阿拉伯语(MSA)、埃及语、摩洛哥语和混合数据上微调我们的ASR模型。此外，对于ASR中的其他方言，我们提供了Whisper和MMS等不同模型的选择。我们将这些模型集成到一个单一的网络界面中，具有多样的功能，如音频录制、上传文件、模型选择和提出问题的选项。

    Arabic is a complex language with many varieties and dialects spoken by over 450 millions all around the world. Due to the linguistic diversity and variations, it is challenging to build a robust and generalized ASR system for Arabic. In this work, we address this gap by developing and demoing a system, dubbed VoxArabica, for dialect identification (DID) as well as automatic speech recognition (ASR) of Arabic. We train a wide range of models such as HuBERT (DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR tasks. Our DID models are trained to identify 17 different dialects in addition to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data. Additionally, for the remaining dialects in ASR, we provide the option to choose various models such as Whisper and MMS in a zero-shot setting. We integrate these models into a single web interface with diverse features such as audio recording, file upload, model selection, and the option to raise fl
    
[^77]: Reward-Augmented Decoding: 使用单向奖励模型实现高效的受控文本生成

    Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model. (arXiv:2310.09520v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.09520](http://arxiv.org/abs/2310.09520)

    该论文介绍了一种名为Reward-Augmented Decoding (RAD)的文本生成方法，使用小型的单向奖励模型来鼓励语言模型生成具有特定属性的文本。RAD在生成非有害和情感受控文本方面表现最佳，并且在非常大的语言模型上也很有效。

    

    尽管大型语言模型已经在许多应用中证明了其有效性，但它们通常生成的文本存在问题或者缺乏所需的属性。本文提出了一种名为Reward-Augmented Decoding (RAD)的文本生成方法，它利用一个小型的单向奖励模型来鼓励语言模型生成具有特定属性的文本。具体而言，RAD利用奖励模型对生成的文本进行评分，并通过重新调整采样概率来更倾向于高奖励的标记。通过使用单向奖励模型，RAD能够缓存先前生成步骤的激活值，降低计算开销。通过在生成非有害和情感受控文本方面的实验，我们证明RAD在仅改变生成过程的方法中表现最佳，并且与涉及重新训练语言模型的最先进方法相当。我们进一步验证了RAD在非常大的语言模型上的有效性。

    While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. By using a unidirectional reward model, RAD can cache activations from prior generation steps to decrease computational overhead. Through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that RAD performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. We further validate that RAD is effective on very large language models w
    
[^78]: EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v2 [cs.CL] UPDATED)

    EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.04691](http://arxiv.org/abs/2310.04691)

    EMO提出了地球移动距离优化（EMO）来解决语言模型中的退化现象。EMO利用了地球移动距离的特性，并引入了一个可行的上界来简化训练。经过评估，发现EMO在语言模型上有显著的改进。

    

    神经语言模型是人文本的概率模型。它们主要通过最大似然估计（MLE）进行训练，该方法等同于最小化经验数据分布和模型分布之间的前向交叉熵。然而，当从这些模型学习的分布解码时，仍然经常观察到各种退化现象。我们确定前向交叉熵作为人与模型分布对齐的距离度量是次优的，原因有：（1）召回优化，（2）负样本多样性忽视和（3）训练测试不匹配。在本文中，我们提出了用于自回归语言模型的地球移动距离优化（EMO）。EMO利用地球移动距离的内在特性来解决上述挑战。由于直接计算的复杂性，我们进一步引入了一种可行的EMO上界来简化端到端训练。经过广泛评估之后，发现我们的方法在语言模型上有显著的改进。

    Neural language models are probabilistic models of human text. They are predominantly trained using maximum likelihood estimation (MLE), which is equivalent to minimizing the forward cross-entropy between the empirical data distribution and the model distribution. However, various degeneration phenomena are still widely observed when decoding from the distributions learned by such models. We establish that the forward cross-entropy is suboptimal as a distance metric for aligning human and model distribution due to its (1) recall-prioritization (2) negative diversity ignorance and (3) train-test mismatch. In this paper, we propose Earth Mover Distance Optimization (EMO) for auto-regressive language modeling. EMO capitalizes on the inherent properties of earth mover distance to address the aforementioned challenges. Due to the high complexity of direct computation, we further introduce a feasible upper bound for EMO to ease end-to-end training. Upon extensive evaluation of language model
    
[^79]: 实体推断竞技场：探究LLMs的对话推理和规划能力的平台

    The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v1 [cs.CL])

    [http://arxiv.org/abs/2310.01468](http://arxiv.org/abs/2310.01468)

    本文提供了一个评估框架，通过向法官提出一系列查询来评估LLMs的对话推理和规划能力。我们发现不同的LLMs在这个任务上表现出显著差异。

    

    目前，大型语言模型（LLMs）在回答明确提问时非常有效。然而，当面临含糊不清的查询时，它们可能行为难以预测并产生错误的输出。这凸显了需要开发能够提出澄清问题以有效解决歧义的智能代理的需求。这种能力需要对多个对话轮次进行复杂的理解、状态跟踪、推理和规划。然而，直接测量这种能力可能具有挑战性。在本文中，我们提供了一个替代性问题，通过向法官提出一系列查询，评估了LLMs推断自己不知道但被法官揭示的实体的能力。这个“实体推断游戏”可以作为一个评估框架，用于探究语言模型的对话推理和规划能力。我们系统地评估了各种LLMs，并发现在这个任务上它们的性能存在显著差异。我们发现强大的LLMs...

    Large language models (LLMs) are currently effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging. In this paper, we offer a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This \textit{entity-deducing game} can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models. We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs
    
[^80]: 语法错误修正系统的系统组合的最小贝叶斯风险解码方法

    Minimum Bayes' Risk Decoding for System Combination of Grammatical Error Correction Systems. (arXiv:2309.06520v1 [cs.CL])

    [http://arxiv.org/abs/2309.06520](http://arxiv.org/abs/2309.06520)

    本文提出了一个用于语法错误修正系统系统组合的最小贝叶斯风险解码方法，并通过实验证明了其有效性。

    

    对于序列到序列的任务来说，将各个系统的输出进行组合是一项具有挑战性的工作。同时，解码准则与评估准则之间通常存在不匹配。最小贝叶斯风险（MBR）解码可以用于以更好地与最终评估准则对齐的方式组合系统的输出。本文研究了在语法错误修正（GEC）系统中的MBR解码，该系统通常以编辑次数和相关的F分数来评估性能。因此，我们提出了一种与这种准则直接相关的新颖MBR损失函数。此外，文中还描述了一种扩展候选句子集合的方法。该方法基于当前的最大投票组合方案，以及个体编辑级别的选择。在三个流行的GEC数据集和最先进的GEC系统上进行的实验证明了所提出的MBR方法的有效性。此外，论文还突出了MBR解码中不同奖励指标的变化对结果的影响。

    For sequence-to-sequence tasks it is challenging to combine individual system outputs. Further, there is also often a mismatch between the decoding criterion and the one used for assessment. Minimum Bayes' Risk (MBR) decoding can be used to combine system outputs in a manner that encourages better alignment with the final assessment criterion. This paper examines MBR decoding for Grammatical Error Correction (GEC) systems, where performance is usually evaluated in terms of edits and an associated F-score. Hence, we propose a novel MBR loss function directly linked to this form of criterion. Furthermore, an approach to expand the possible set of candidate sentences is described. This builds on a current max-voting combination scheme, as well as individual edit-level selection. Experiments on three popular GEC datasets and with state-of-the-art GEC systems demonstrate the efficacy of the proposed MBR approach. Additionally, the paper highlights how varying reward metrics within the MBR d
    
[^81]: 基于框架的大型语言模型自由回答的定性分析：算法保真度

    Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity. (arXiv:2309.06364v1 [cs.CL])

    [http://arxiv.org/abs/2309.06364](http://arxiv.org/abs/2309.06364)

    本文通过定性分析研究了大型语言模型生成的自由回答，重点考察了算法保真度，并提出高算法保真度可以推广到真实人类的观点。这对于使用语言模型研究人类行为具有重要意义。

    

    如今，使用大规模生成式语言模型（LLMs），可以模拟自由回答面试问题，就像传统上使用定性研究方法分析的那样。定性方法涵盖了一系列技术，涉及对开放式访谈或自由进行的自然语言对话的手动分析。本文考虑通过定性方法对LLMs生成的"硅参与者"进行研究，从而产生可能可以推广到真实人群的洞察力。我们分析的关键概念是算法保真度，这是由Argyle等人（2023年）引入的一个术语，用于描述LLM生成的输出与人类亚群体的信念和态度的程度相吻合。根据定义，高算法保真度表明从LLMs中提取的潜在信念可能可以推广到真实人类，而低算法保真度则使得这样的研究无效。本文使用LLM生成面试问答，...

    Today, using Large-scale generative Language Models (LLMs) it is possible to simulate free responses to interview questions like those traditionally analyzed using qualitative research methods. Qualitative methodology encompasses a broad family of techniques involving manual analysis of open-ended interviews or conversations conducted freely in natural language. Here we consider whether artificial "silicon participants" generated by LLMs may be productively studied using qualitative methods aiming to produce insights that could generalize to real human populations. The key concept in our analysis is algorithmic fidelity, a term introduced by Argyle et al. (2023) capturing the degree to which LLM-generated outputs mirror human sub-populations' beliefs and attitudes. By definition, high algorithmic fidelity suggests latent beliefs elicited from LLMs may generalize to real humans, whereas low algorithmic fidelity renders such research invalid. Here we used an LLM to generate interviews wi
    
[^82]: 嵌入结构的重要性：比较适应新语言的多语言词汇方法

    Embedding structure matters: Comparing methods to adapt multilingual vocabularies to new languages. (arXiv:2309.04679v1 [cs.CL])

    [http://arxiv.org/abs/2309.04679](http://arxiv.org/abs/2309.04679)

    比较了替换跨语言词汇的几种技术，证明了单语转移文献中的方法不适用于多语言模型。专门化的较小词汇对于提高低资源情况下的性能是有效的。

    

    预训练的多语言语言模型支持英语以外的现代自然语言处理工具的大部分。用于特定语言化的强大基准是语言适应预训练（LAPT）。但是，在适应过程中保留大型跨语言词汇和嵌入矩阵会带来相当多的多余计算成本。在本研究中，我们提出了几种简单的技术来用紧凑的特定语言词汇替换跨语言词汇。具体而言，我们解决了在词汇专门化后如何重新初始化令牌嵌入矩阵的策略。我们对我们的技术进行了系统的实验比较，此外还加入了最近提出的焦点方法。我们证明了：1）在单语转移文献中的嵌入替换技术不适用于适应多语言模型。2）用较小的专门的词汇替换跨语言词汇提供了一种提高低资源情况下性能的有效方法。

    Pre-trained multilingual language models underpin a large portion of modern NLP tools outside of English. A strong baseline for specializing these models for specific languages is Language-Adaptive Pre-Training (LAPT). However, retaining a large cross-lingual vocabulary and embedding matrix comes at considerable excess computational cost during adaptation. In this study, we propose several simple techniques to replace a cross-lingual vocabulary with a compact, language-specific one. Namely, we address strategies for re-initializing the token embedding matrix after vocabulary specialization. We then provide a systematic experimental comparison of our techniques, in addition to the recently-proposed Focus method. We demonstrate that: 1) Embedding-replacement techniques in the monolingual transfer literature are inadequate for adapting multilingual models. 2) Replacing cross-lingual vocabularies with smaller specialized ones provides an efficient method to improve performance in low-resou
    
[^83]: FairMonitor: 一种检测大型语言模型中刻板印象和偏见的四阶段自动框架

    FairMonitor: A Four-Stage Automatic Framework for Detecting Stereotypes and Biases in Large Language Models. (arXiv:2308.10397v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10397](http://arxiv.org/abs/2308.10397)

    本文引入了一个四阶段框架，可以直接评估大型语言模型（LLMs）生成内容中的刻板印象和偏见，包括直接询问测试、串行或适应性故事测试、隐性关联测试和未知情境测试。此外，还提出了多维评估指标和可解释的零样本提示，并以教育部门为案例研究构建了Edu-FairMonitor框架。

    

    检测大型语言模型（LLMs）中的刻板印象和偏见可以增强公平性并减少这些LLMs应用时对个人或群体的不利影响。然而，现有方法大多关注于测量模型对包含偏见和刻板印象的句子的偏好，这种方法缺乏可解释性且无法检测真实世界中的隐含偏见和刻板印象。为填补这一空白，本文引入了一个四阶段框架，直接评估LLMs生成内容中的刻板印象和偏见，包括直接询问测试、串行或适应性故事测试、隐性关联测试和未知情境测试。此外，本文还提出了多维评估指标和可解释的零样本提示，用于自动评估。以教育部门为案例研究，我们基于这个四阶段框架构建了Edu-FairMonitor，该框架包括12,632个开放式问题，涵盖九个敏感议题。

    Detecting stereotypes and biases in Large Language Models (LLMs) can enhance fairness and reduce adverse impacts on individuals or groups when these LLMs are applied. However, the majority of existing methods focus on measuring the model's preference towards sentences containing biases and stereotypes within datasets, which lacks interpretability and cannot detect implicit biases and stereotypes in the real world. To address this gap, this paper introduces a four-stage framework to directly evaluate stereotypes and biases in the generated content of LLMs, including direct inquiry testing, serial or adapted story testing, implicit association testing, and unknown situation testing. Additionally, the paper proposes multi-dimensional evaluation metrics and explainable zero-shot prompts for automated evaluation. Using the education sector as a case study, we constructed the Edu-FairMonitor based on the four-stage framework, which encompasses 12,632 open-ended questions covering nine sensit
    
[^84]: ParaFuzz：一种基于可解释性的技术用于检测自然语言处理中的毒样本

    ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP. (arXiv:2308.02122v1 [cs.CR])

    [http://arxiv.org/abs/2308.02122](http://arxiv.org/abs/2308.02122)

    ParaFuzz是一种基于可解释性的技术，用于检测自然语言处理中的毒样本。该技术通过观察模型在重写过的干净样本和污染样本上的预测稳定性，来判断样本是否被污染。

    

    傍门攻击已成为自然语言处理（NLP）模型的重要威胁，其中在输入中存在特定触发器可以导致被污染的模型将这些输入误分类为预定的目标类别。当前的检测机制受到限制，无法应对更隐蔽的傍门策略，如基于风格的攻击。在这项工作中，我们提出了一种创新的测试时污染样本检测框架，该框架依赖于模型预测的可解释性，并与输入的语义含义有关。我们认为，触发器（例如，不常见的单词）不应该从根本上改变被污染样本的基本语义含义，因为它们想保持潜伏。基于这个观察，我们假设在改写过程中，模型对于重写过的干净样本的预测应该保持稳定，而对于污染样本的预测在触发器的突变过程中应该恢复到真实标签。

    Backdoor attacks have emerged as a prominent threat to natural language processing (NLP) models, where the presence of specific triggers in the input can lead poisoned models to misclassify these inputs to predetermined target classes. Current detection mechanisms are limited by their inability to address more covert backdoor strategies, such as style-based attacks. In this work, we propose an innovative test-time poisoned sample detection framework that hinges on the interpretability of model predictions, grounded in the semantic meaning of inputs. We contend that triggers (e.g., infrequent words) are not supposed to fundamentally alter the underlying semantic meanings of poisoned samples as they want to stay stealthy. Based on this observation, we hypothesize that while the model's predictions for paraphrased clean samples should remain stable, predictions for poisoned samples should revert to their true labels upon the mutations applied to triggers during the paraphrasing process. W
    
[^85]: 在野外的Android：用于Android设备控制的大规模数据集

    Android in the Wild: A Large-Scale Dataset for Android Device Control. (arXiv:2307.10088v1 [cs.LG])

    [http://arxiv.org/abs/2307.10088](http://arxiv.org/abs/2307.10088)

    这个论文提出了一个名为Android in the Wild (AITW)的大规模数据集，用于研究设备控制系统，该数据集包括人类示范的设备交互、自然语言指令和多种Android版本和设备类型。这个数据集提供了一个新的挑战，需要从视觉外观中推断用户界面中可用的操作。

    

    对于能够解释人类自然语言指令并直接控制数字设备用户界面执行的设备控制系统，人们越来越感兴趣。我们提出了一个用于设备控制研究的数据集，Android in the Wild (AITW)，该数据集比当前数据集大几个数量级。该数据集包含了设备交互的人类示范，包括屏幕和操作，以及相应的自然语言指令。它包括715k个剧集，涵盖30k个不同的指令，四个Android版本（v10-13），以及八种不同的设备类型（从Pixel 2 XL到Pixel 6）和不同的屏幕分辨率。它包含需要语言和视觉上下文的语义理解的多步骤任务。这个数据集提出了一个新的挑战：必须从它们的视觉外观中推断出用户界面中可用的操作。而且，行动空间不再是简单的基于用户界面元素的行动，而是包含精确的手势（例如，水平滚动）

    There is a growing interest in device-control systems that can interpret human natural language instructions and execute them on a digital device by directly controlling its user interface. We present a dataset for device-control research, Android in the Wild (AITW), which is orders of magnitude larger than current datasets. The dataset contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. It consists of 715k episodes spanning 30k unique instructions, four versions of Android (v10-13),and eight device types (Pixel 2 XL to Pixel 6) with varying screen resolutions. It contains multi-step tasks that require semantic understanding of language and visual context. This dataset poses a new challenge: actions available through the user interface must be inferred from their visual appearance. And, instead of simple UI element-based actions, the action space consists of precise gestures (e.g., horizontal scrolls 
    
[^86]: HYTREL: 基于超图的表格数据表示学习

    HYTREL: Hypergraph-enhanced Tabular Data Representation Learning. (arXiv:2307.08623v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.08623](http://arxiv.org/abs/2307.08623)

    HYTREL是一种表格语言模型，通过使用超图来捕捉表格数据的排列不变性和其他三个结构属性。实证结果表明，HYTREL在四个下游任务中始终优于其他竞争基线模型，并且只需最少的预训练。

    

    在许多下游任务中，预训练在大量表格数据集上的语言模型都证明了其有效性。然而，许多模型并没有考虑到表格数据中存在的行/列排列不变性、分层结构等。为了缓解这些限制，我们提出了一种名为HYTREL的表格语言模型，通过使用超图来捕捉表格数据的排列不变性和其他三个结构属性——其中，表格单元格构成节点，并且在每行、每列和整个表格中共同出现的单元格被用来形成三种不同类型的超边。我们展示了HYTREL在特定条件下对于表格数据是最大不变的，即，两个表格通过HYTREL获得的表示相同，当且仅当这两个表格在排列上是相同的。我们的实证结果表明，HYTREL在四个下游任务中始终优于其他竞争基线模型，并且只需最少的预训练。

    Language models pretrained on large collections of tabular data have demonstrated their effectiveness in several downstream tasks. However, many of these models do not take into account the row/column permutation invariances, hierarchical structure, etc. that exist in tabular data. To alleviate these limitations, we propose HYTREL, a tabular language model, that captures the permutation invariances and three more structural properties of tabular data by using hypergraphs - where the table cells make up the nodes and the cells occurring jointly together in each row, column, and the entire table are used to form three different types of hyperedges. We show that HYTREL is maximally invariant under certain conditions for tabular data, i.e., two tables obtain the same representations via HYTREL iff the two tables are identical up to permutations. Our empirical results demonstrate that HYTREL consistently outperforms other competitive baselines on four downstream tasks with minimal pretraini
    
[^87]: DebateKG: 用语义知识图自动创建政策辩论案例

    DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs. (arXiv:2307.04090v1 [cs.CL])

    [http://arxiv.org/abs/2307.04090](http://arxiv.org/abs/2307.04090)

    本论文提出了一种利用语义知识图自动创建政策辩论案例的方法，通过在争论的语义知识图上进行限制最短路径遍历，有效构建高质量的辩论案例。研究结果表明，在美国竞赛辩论中，利用这种方法显著改进了已有数据集DebateSum，并贡献了新的例子和有用的元数据。通过使用txtai语义搜索和知识图工具链，创建和贡献了9个语义知识图，同时提出了一种独特的评估方法来确定哪个知识图更适合政策辩论案例生成。

    

    近期相关工作表明，自然语言处理系统在解决竞赛辩论中的问题方面具有应用性。竞赛辩论中最重要的任务之一是辩手创建高质量的辩论案例。我们展示了使用限制最短路径遍历在争论的语义知识图上构建有效的辩论案例的方法。我们在一个名为DebateSum的大规模数据集上研究了这种潜力，该数据集针对的是一种名为政策辩论的美国竞赛辩论类型。我们通过向数据集中引入53180个新的例子，并为每个例子提供进一步有用的元数据，显著改进了DebateSum。我们利用txtai语义搜索和知识图工具链基于这个数据集产生并贡献了9个语义知识图。我们创建了一种独特的评估方法，以确定在政策辩论案例生成的背景下哪个知识图更好。

    Recent work within the Argument Mining community has shown the applicability of Natural Language Processing systems for solving problems found within competitive debate. One of the most important tasks within competitive debate is for debaters to create high quality debate cases. We show that effective debate cases can be constructed using constrained shortest path traversals on Argumentative Semantic Knowledge Graphs. We study this potential in the context of a type of American Competitive Debate, called Policy Debate, which already has a large scale dataset targeting it called DebateSum. We significantly improve upon DebateSum by introducing 53180 new examples, as well as further useful metadata for every example, to the dataset. We leverage the txtai semantic search and knowledge graph toolchain to produce and contribute 9 semantic knowledge graphs built on this dataset. We create a unique method for evaluating which knowledge graphs are better in the context of producing policy deb
    
[^88]: 通过Pareto Optimal自监督实现大型语言模型的自动校准和错误修正

    Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision. (arXiv:2306.16564v1 [cs.CL])

    [http://arxiv.org/abs/2306.16564](http://arxiv.org/abs/2306.16564)

    本文介绍了一种Pareto Optimal自监督框架，利用可用的编程监督将大型语言模型(LLM)的响应进行系统校准，通过为每个响应生成风险评分，而无需额外的手动工作。

    

    大型语言模型(LLM)已经展现了出色的能力，适用于广泛的应用领域，但是准确性仍然是一个重要的增长领域，特别是在生物医学等关键领域。一种有效的方法，用于校准LLM响应的置信水平，对于自动检测错误并促进人机协作验证至关重要。一个重要的校准信号来源是专家指定的编程监督，通常具有较低的成本，但也有其自身的局限性，如噪声和覆盖范围。在本文中，我们引入了一种Pareto Optimal自监督框架，可以利用可用的编程监督来系统地校准LLM响应，通过为每个响应生成风险评分，而不需要任何额外的手动工作。这通过学习一个调和模型来实现，将LLM输出与其他可用的监督来源相协调，将更不确定的响应分配更高的风险评分。

    Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the confidence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop verification. An important source of calibration signals stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain L
    
[^89]: YouTube-ASL:一个大规模的、开放领域的美国手语-英语平行语料库。

    YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English Parallel Corpus. (arXiv:2306.15162v1 [cs.CL])

    [http://arxiv.org/abs/2306.15162](http://arxiv.org/abs/2306.15162)

    本文介绍了一个大规模的、开放领域的美国手语-英语平行语料库YouTube-ASL，包含了约1000小时的美国手语视频和超过2500位独特的签名者。研究者在此语料库上训练了手语到英语翻译的模型，并在另一个数据集上实现了最新的最佳效果。

    

    机器学习对于手语的瓶颈在于数据。在本文中，我们介绍了YouTube-ASL，一个来自YouTube的包含美国手语（ASL）视频和英文字幕的大规模、开放领域的语料库。YouTube-ASL拥有约1000小时的视频和超过2500个独特的签名者，是迄今为止最大的ASL数据集的3倍之多，并且拥有10倍于先前ASL数据集的独特签名者数量。我们在YouTube-ASL上训练了ASL到英语翻译的基准模型，并在How2Sign上进行评估，在这里我们实现了新的微调最佳效果12.39 BLEU，并首次报道了零-shot方法的结果。

    Machine learning for sign languages is bottlenecked by data. In this paper, we present YouTube-ASL, a large-scale, open-domain corpus of American Sign Language (ASL) videos and accompanying English captions drawn from YouTube. With ~1000 hours of videos and >2500 unique signers, YouTube-ASL is ~3x as large and has ~10x as many unique signers as the largest prior ASL dataset. We train baseline models for ASL to English translation on YouTube-ASL and evaluate them on How2Sign, where we achieve a new finetuned state of the art of 12.39 BLEU and, for the first time, report zero-shot results.
    
[^90]: 块状态变换器

    Block-State Transformer. (arXiv:2306.09539v1 [cs.CL])

    [http://arxiv.org/abs/2306.09539](http://arxiv.org/abs/2306.09539)

    本文提出了一种名为块状态变换器（BST）的混合神经网络层，结合了状态空间模型和块变换器，旨在在语言建模任务中提高性能和可扩展性。实验证明，该模型在语言建模困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，在层级别上具有超过十倍的速度提升。

    

    状态空间模型（SSM）在需要建模长期依赖性并且需要高效扩展到长序列的任务中显示出了惊人的效果。尽管最初是为连续信号设计的，但SSM在视觉和音频等许多任务中表现出了优异的性能；然而，在语言建模任务中，SSM仍然落后于Transformers的性能。在这项工作中，我们提出了一个名为块状态变换器（BST）的混合层，它在内部组合了一个用于长距离上下文化的SSM子层和一个用于短期序列表示的块变换器子层。我们研究了三种不同的、完全可并行的集成SSM和块注意力的变体。我们证明了我们的模型在语言建模的困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，块状态变换器在层级别上具有超过十倍的速度提升。

    State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compa
    
[^91]: PLANNER:通过潜在语言扩散模型生成多样化段落

    PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model. (arXiv:2306.02531v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02531](http://arxiv.org/abs/2306.02531)

    本文提出了PLANNER模型，它通过将自回归生成和潜在语义扩散相结合，以在生成文本时在段落级别实现全局控制，生成流畅的文本。

    

    文本的自回归模型有时会生成重复且质量低下的输出，因为在生成的步骤中错误会累积。这个问题常常被归因于曝光偏差-模型在训练和推理过程中的使用方式之间的差异。去噪扩散模型提供了一种替代方法，模型可以回顾和修正其输出。然而，它们在计算上可能很昂贵，并且在文本和段落较长的情况下，与自回归模型相比，先前关于文本的研究努力已导致产生的模型产生不太流畅的输出。在本文中，我们提出了PLANNER，一个将潜在语义扩散与自回归生成结合的模型，以在段落上进行全局控制来生成流畅的文本。该模型通过将自回归的“解码”模块与使用潜在扩散以粗粒度方式生成语义段落嵌入的“规划”模块相结合来实现这一目标。该方法进行了评估

    Autoregressive models for text sometimes generate repetitive and low-quality output because errors accumulate during the steps of generation. This issue is often attributed to exposure bias - the difference between how a model is trained, and how it is used during inference. Denoising diffusion models provide an alternative approach in which a model can revisit and revise its output. However, they can be computationally expensive and prior efforts on text have led to models that produce less fluent output compared to autoregressive models, especially for longer text and paragraphs. In this paper, we propose PLANNER, a model that combines latent semantic diffusion with autoregressive generation, to generate fluent text while exercising global control over paragraphs. The model achieves this by combining an autoregressive "decoding" module with a "planning" module that uses latent diffusion to generate semantic paragraph embeddings in a coarse-to-fine manner. The proposed method is evalu
    
[^92]: 合并模型时如何解决干扰的问题

    Resolving Interference When Merging Models. (arXiv:2306.01708v1 [cs.LG])

    [http://arxiv.org/abs/2306.01708](http://arxiv.org/abs/2306.01708)

    本文揭示了现有模型合并技术存在的干扰问题，提出了具有广泛适用性的解决方案，可显着提高合并后模型的性能。

    

    迁移学习可以在下游任务中进一步微调预训练模型，从而获得显著的优势，包括改进下游性能，加快收敛速度和提高样本效率。然而，已有的模型合并技术往往忽视了不同模型参数之间的干扰，导致合并多个模型时性能大幅下降。本文证明，先前的合并技术由于两个主要干扰来源而不慎丢失有价值的信息：(a)冗余参数值引起的干扰和(b)表示同一参数值的符号在不同模型中的差异。

    Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To 
    
[^93]: 在具有隐含逻辑约束的事件知识图上进行复杂查询答案

    Complex Query Answering on Eventuality Knowledge Graph with Implicit Logical Constraints. (arXiv:2305.19068v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.19068](http://arxiv.org/abs/2305.19068)

    本文提出了一个新的框架，利用神经方法回答基于事件性知识图的复杂逻辑查询，满足传统的一阶逻辑约束以及有关事件性发生和顺序的隐含逻辑约束。

    

    使用深度学习方法查询知识图，可以自然地利用推理和泛化能力来学习推断更好的答案。传统的神经复杂查询答案方法主要适用于以实体为中心的知识图。然而，在现实世界中，我们还需要对事件、状态和活动（即事件性或情况）进行逻辑推理，以将学习系统从I型推进到II型，正如Yoshua Bengio所提出的。从事件性为中心的知识图（EVKG）中进行逻辑查询可以自然地提供对此类直观和逻辑推理的参考。因此，在本文中，我们提出了一个新的框架，利用神经方法来回答基于EVKG的复杂逻辑查询，这可以满足传统的一阶逻辑约束，还可以满足有关事件性发生和顺序的隐含逻辑约束。

    Querying knowledge graphs (KGs) using deep learning approaches can naturally leverage the reasoning and generalization ability to learn to infer better answers. Traditional neural complex query answering (CQA) approaches mostly work on entity-centric KGs. However, in the real world, we also need to make logical inferences about events, states, and activities (i.e., eventualities or situations) to push learning systems from System I to System II, as proposed by Yoshua Bengio. Querying logically from an EVentuality-centric KG (EVKG) can naturally provide references to such kind of intuitive and logical inference. Thus, in this paper, we propose a new framework to leverage neural methods to answer complex logical queries based on an EVKG, which can satisfy not only traditional first-order logic constraints but also implicit logical constraints over eventualities concerning their occurrences and orders. For instance, if we know that "Food is bad" happens before "PersonX adds soy sauce", th
    
[^94]: 文本到图像生成与评估的可视化编程

    Visual Programming for Text-to-Image Generation and Evaluation. (arXiv:2305.15328v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.15328](http://arxiv.org/abs/2305.15328)

    本文提出了两种新颖的可解释/可理解的图像编程框架，用于文本到图像（T2I）的生成和评估。首先，引入了VPGen，一个可解释的逐步T2I生成框架，将T2I生成分解为三个步骤，通过语言模型处理前两个步骤，提供了比端到端模型更强的空间控制能力，并利用了预训练语言模型的世界知识。

    

    随着大型语言模型在许多领域表现出卓越性能，最近的研究采用语言模型作为视觉任务中的视觉模块的控制器。虽然现有的工作集中在为语言模型提供视觉理解能力，但我们提出了两种新颖的可解释/可解释的图像编程框架，用于文本到图像（T2I）的生成和评估。首先，我们引入了VPGen，一种可解释的逐步T2I生成框架，将T2I生成分解为三个步骤：对象/计数生成、布局生成和图像生成。我们使用语言模型处理前两个步骤（对象/计数生成和布局生成），通过在文本布局对上微调它。我们的逐步T2I生成框架提供了比端到端模型更强的空间控制能力，而端到端模型是这个任务的主要方法。此外，我们利用了预训练语言模型的世界知识，克服了以前的布局引导T2I作品的局限。

    As large language models have demonstrated impressive performance in many domains, recent works have adopted language models (LMs) as controllers of visual modules for vision-and-language tasks. While existing work focuses on equipping LMs with visual understanding, we propose two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation. First, we introduce VPGen, an interpretable step-by-step T2I generation framework that decomposes T2I generation into three steps: object/count generation, layout generation, and image generation. We employ an LM to handle the first two steps (object/count generation and layout generation), by finetuning it on text-layout pairs. Our step-by-step T2I generation framework provides stronger spatial control than end-to-end models, the dominant approach for this task. Furthermore, we leverage the world knowledge of pretrained LMs, overcoming the limitation of previous layout-guided T2I works that can on
    
[^95]: 使用基于案例推理的机器阅读理解

    Machine Reading Comprehension using Case-based Reasoning. (arXiv:2305.14815v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14815](http://arxiv.org/abs/2305.14815)

    本文提出了一种基于案例推理的机器阅读理解方法（CBR-MRC），通过从存储器中检索相似案例并选择最类似的上下文来预测答案，以达到高准确性。在自然语言问题和新闻问答中，CBR-MRC的准确性超过基准，并且能够识别与其他评估员不同的答案。

    

    我们提出了一种准确且可解释的方法，用于机器阅读理解中的答案提取，该方法类似于经典人工智能中的基于案例推理（CBR）。我们的方法（CBR-MRC）基于一个假设，即相似问题的上下文化答案彼此之间具有语义相似性。给定一个测试问题，CBR-MRC首先从非参数化存储器中检索一组相似的案例，然后通过选择测试上下文中最类似于检索到的案例中上下文化答案表示的范围来预测答案。我们的方法半参数化的特性使其能够将预测归因于特定的证据案例集，因此在构建可靠且可调试的问答系统时是一个理想的选择。我们展示了CBR-MRC在自然语言问题（NaturalQuestions）和新闻问答（NewsQA）上比大型读者模型提供了高准确性，并且优于基准分别提升了11.5和8.4 EM。此外，我们还展示了CBR-MRC在识别与他人评估员不同的答案方面的能力。

    We present an accurate and interpretable method for answer extraction in machine reading comprehension that is reminiscent of case-based reasoning (CBR) from classical AI. Our method (CBR-MRC) builds upon the hypothesis that contextualized answers to similar questions share semantic similarities with each other. Given a test question, CBR-MRC first retrieves a set of similar cases from a non-parametric memory and then predicts an answer by selecting the span in the test context that is most similar to the contextualized representations of answers in the retrieved cases. The semi-parametric nature of our approach allows it to attribute a prediction to the specific set of evidence cases, making it a desirable choice for building reliable and debuggable QA systems. We show that CBR-MRC provides high accuracy comparable with large reader models and outperforms baselines by 11.5 and 8.4 EM on NaturalQuestions and NewsQA, respectively. Further, we demonstrate the ability of CBR-MRC in identi
    
[^96]: 大型语言模型中的实体偏见：一种因果视角

    A Causal View of Entity Bias in (Large) Language Models. (arXiv:2305.14695v1 [cs.CL])

    [http://arxiv.org/abs/2305.14695](http://arxiv.org/abs/2305.14695)

    研究提出了一种结构因果模型（SCM）并提供因果干预技术，以缓解大型语言模型中的实体偏见，从而减少偏见信息，同时保留相似实体的共同预测信息。

    

    实体偏见广泛影响预训练的大型语言模型，导致它们过度依赖（有偏见的）参数化知识来进行不准确的预测。尽管因果相关的方法已经显示出缓解实体偏见的巨大潜力，但在实践中精确估计潜在因果模型的参数仍然很困难，黑盒子的语言模型更无法调整。为了解决这些问题，我们提出了一种特定的结构因果模型（SCM），其参数比较容易估计。在此基础上，我们提出了因果干预技术，以缓解白盒和黑盒设置中的实体偏见。这种因果干预将原始实体与相邻实体一起进行扰动。这种干预减少了与原始实体相关的特定偏向信息，同时仍保留了来自类似实体的足够共同预测信息。

    Entity bias widely affects pretrained (large) language models, causing them to excessively rely on (biased) parametric knowledge to make unfaithful predictions. Although causality-inspired methods have shown great potential to mitigate entity bias, it is hard to precisely estimate the parameters of underlying causal models in practice. The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits. To address these problems, we propose a specific structured causal model (SCM) whose parameters are comparatively easier to estimate. Building upon this SCM, we propose causal intervention techniques to mitigate entity bias for both white-box and black-box settings. The proposed causal intervention perturbs the original entity with neighboring entities. This intervention reduces specific biasing information pertaining to the original entity while still preserving sufficient common predictive information from similar entities. 
    
[^97]: INSTRUCTSCORE: 可解释的文本生成评估与细粒度反馈

    INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback. (arXiv:2305.14282v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14282](http://arxiv.org/abs/2305.14282)

    INSTRUCTSCORE是一个可解释的文本生成评估度量，通过利用明确的人类指令和GPT-4的隐式知识，它能生成生成文本的分数和人类可读的诊断报告，达到与最先进度量相当的性能水平。

    

    自动评估语言生成的质量至关重要。尽管最近学习度量表显示与人类判断高度相关，但这些度量无法解释其判断或将分数与生成文本中的缺陷关联起来。为了解决这个限制，我们提出了InstructScore，这是一个用于文本生成的可解释的评估度量。通过利用明确的人类指令和GPT-4的隐式知识，我们基于LLaMA对文本评估度量进行微调，生成生成文本的分数和人类可读的诊断报告。我们在各种生成任务上评估了InstructScore，包括翻译、字幕生成、数据到文本和常识生成。实验表明，我们的7B模型超过了所有其他无监督度量，包括基于175B GPT-3和GPT-4的模型。令人惊讶的是，即使没有来自人工评级数据的直接监督，我们的InstructScore的性能水平也与COMET2等最先进的度量相当。

    Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics can not explain their verdict or associate the scores with defects in generated text. To address this limitation, we present InstructScore, an explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT-4, we fine-tune a text evaluation metric based on LLaMA, producing both a score for generated text and a human readable diagnostic report. We evaluate InstructScore on a variety of generation tasks, including translation, captioning, data-to-text and commonsense generation. Experiments show that our 7B model surpasses all other unsupervised metrics, including those based on 175B GPT-3 and GPT-4. Surprisingly, our InstructScore, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET2
    
[^98]: 探索面向文本到SQL转换的思维链式提示

    Exploring Chain-of-Thought Style Prompting for Text-to-SQL. (arXiv:2305.14215v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14215](http://arxiv.org/abs/2305.14215)

    本文研究了通过思维链式提示来提升大型语言模型在文本到SQL解析中的推理能力。实验表明，迭代提示法可能是不必要的，并且详细的推理步骤容易出现错误传播问题。在此基础上，提出了一种新的CoT风格提示方法，显著提高了文本到SQL解析的性能。

    

    最近，基于大型语言模型（LLM）的上下文学习因其在各种任务上杰出的少样本性能而引起了越来越多的关注。然而，在文本到SQL解析方面，其性能仍有很大提升空间。本文假设改进文本到SQL解析的LLM的一个关键方面是其多步推理能力。因此，我们系统地研究了如何通过思维链式提示来提升LLM的推理能力，包括原始的思维链式提示（Wei等，2022b）和由小到大的提示（Zhou等，2023）。我们的实验表明，如Zhou等人（2023）所示的迭代提示在文本到SQL解析中可能是不必要的，并且使用详细的推理步骤往往会产生更多的错误传播问题。基于这些发现，我们提出了一种新的面向文本到SQL转换的CoT风格提示方法。它在Spider开发集和Spider Realist数据集上分别提高了5.2和6.5个绝对分数。

    In-context learning with large language models (LLMs) has recently caught increasing attention due to its superior few-shot performance on various tasks. However, its performance on text-to-SQL parsing still has much room for improvement. In this paper, we hypothesize that a crucial aspect of LLMs to improve for text-to-SQL parsing is their multi-step reasoning ability. Thus, we systematically study how to enhance LLMs' reasoning ability through chain of thought (CoT) style prompting, including the original chain-of-thought prompting (Wei et al., 2022b) and least-to-most prompting (Zhou et al., 2023). Our experiments demonstrate that iterative prompting as in Zhou et al. (2023) may be unnecessary for text-to-SQL parsing, and using detailed reasoning steps tends to have more error propagation issues. Based on these findings, we propose a new CoT-style prompting method for text-to-SQL parsing. It brings 5.2 and 6.5 point absolute gains on the Spider development set and the Spider Realist
    
[^99]: 结合全局结构知识的视觉丰富文档关系抽取方法

    Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document. (arXiv:2305.13850v1 [cs.CL])

    [http://arxiv.org/abs/2305.13850](http://arxiv.org/abs/2305.13850)

    这篇论文提出了一种结合全局结构知识的连续迭代的方式去捕获实体之间的依赖关系，以提高视觉丰富文档中关系抽取的准确性。

    

    视觉关系提取（VRE）旨在从视觉丰富的文档中提取实体之间的关系。现有方法通常基于实体特征单独预测每对实体之间的关系，但忽略了全局结构信息，即实体对之间的依赖关系。缺乏全局结构信息可能使模型难以学习长程关系，并容易产生冲突的预测结果。为了缓解这些限制，我们提出了一种GOSE框架，该框架以迭代的方式捕获实体对之间的依赖关系。给定文档的扫描图像，GOSE首先对实体对生成初步的关系预测。第二，在先前迭代的预测结果基础上，GOSE利用全局结构知识进一步整合实体表示。这种“生成-捕获-整合”模式被多次执行，以便实体之间的依赖关系能够被很好地捕获和利用。

    Visual relation extraction (VRE) aims to extract relations between entities from visuallyrich documents. Existing methods usually predict relations for each entity pair independently based on entity features but ignore the global structure information, i.e., dependencies between entity pairs. The absence of global structure information may make the model struggle to learn long-range relations and easily predict conflicted results. To alleviate such limitations, we propose a GlObal Structure knowledgeguided relation Extraction (GOSE) framework, which captures dependencies between entity pairs in an iterative manner. Given a scanned image of the document, GOSE firstly generates preliminary relation predictions on entity pairs. Secondly, it mines global structure knowledge based on prediction results of the previous iteration and further incorporates global structure knowledge into entity representations. This "generate-capture-incorporate" schema is performed multiple times so that entit
    
[^100]: 大型语言模型能像人类一样推理和产生分歧吗？

    Can Large Language Models Infer and Disagree Like Humans?. (arXiv:2305.13788v1 [cs.CL])

    [http://arxiv.org/abs/2305.13788](http://arxiv.org/abs/2305.13788)

    本文研究了大型语言模型在自然语言推断方面的性能和与人类分歧分布的对齐情况。结果表明LLM的推断能力有限，无法捕捉到人类分歧分布，引发了对其NLU和代表人类用户性质的担忧。

    

    大型语言模型在解决广泛任务方面已经表现出非常好的成绩。在生成文本时，从这些模型中采样标记是一种常见的策略。但是，LLM很难与人类的分歧分布高度对齐，特别是在自然语言推断方面。本文使用 Monte Carlo Reconstruction（MCR）和 Log Probability Reconstruction（LPR）两种不同的技术评估了LLM分布的性能和与人类的对齐情况。结果表明，LLM在解决NLI任务方面能力有限，同时无法捕捉到人类的分歧分布，这对其自然语言理解（NLU）能力和代表人类用户的特性提出了关注。

    Large Language Models (LLMs) have shown stellar achievements in solving a broad range of tasks. When generating text, it is common to sample tokens from these models: whether LLMs closely align with the human disagreement distribution has not been well-studied, especially within the scope of Natural Language Inference (NLI). In this paper, we evaluate the performance and alignment of LLM distribution with humans using two different techniques: Monte Carlo Reconstruction (MCR) and Log Probability Reconstruction (LPR). As a result, we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution, raising concerns about their natural language understanding (NLU) ability and their representativeness of human users.
    
[^101]: 论大型语言模型的错误信息污染风险

    On the Risk of Misinformation Pollution with Large Language Models. (arXiv:2305.13661v1 [cs.CL])

    [http://arxiv.org/abs/2305.13661](http://arxiv.org/abs/2305.13661)

    本文探讨了大型语言模型（LLM）可能误用的潜在风险，指出LLM可以作为有效的误导性信息生成器，导致开放域问答（ODQA）系统性能显著降低，并尝试提出三种防御策略：提示，误报检测和大多数投票。

    

    本文全面调查了现代大型语言模型（LLM）的潜在误用，探讨了其生成可信并具有误导性的信息并对信息密集型应用程序，尤其是开放域问答（ODQA）系统的影响。我们建立了一个威胁模型，并对无意和故意的潜在误用场景进行模拟，以评估LLM可以用于生成信息不实的程度。研究发现，LLM可以作为有效的误导性信息生成器，导致ODQA系统性能显著降低。为了减轻由LLM生成的错误信息带来的危害，我们探讨了三种防御策略：提示，误报检测和大多数投票。虽然初步结果显示这些防御性策略有希望产生明显效果，但还需要做大量工作来应对错误信息污染的挑战。本研究强调了需要进一步进行跨学科研究。

    In this paper, we comprehensively investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation in the performance of ODQA systems. To mitigate the harm caused by LLM-generated misinformation, we explore three defense strategies: prompting, misinformation detection, and majority voting. While initial results show promising trends for these defensive strategies, much more work needs to be done to address the challenge of misinformation pollution. Our work highlights the need for further research and interdisciplinary
    
[^102]: clembench：使用游戏来评估作为对话代理人的聊天优化语言模型

    clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents. (arXiv:2305.13455v1 [cs.CL])

    [http://arxiv.org/abs/2305.13455](http://arxiv.org/abs/2305.13455)

    本论文探讨了大型语言模型在接触具有挑战性的受限游戏式环境下，能否有意义地评估它们的能力。作为概念验证，研究了五种交互设置，表明当前的聊天优化LLMs在一定程度上能够遵循游戏玩法指令。这对于将LLMs开发为具有广泛适用性的对话代理人具有启示作用。

    

    最近的工作提出了一种针对“站立语言理解代理”的系统评估方法——代理在丰富的语言和非语言环境中运行，通过在精心构造的互动环境中进行测试来评估它们。其他最近的工作则认为，如果适当设置，大型语言模型（LLMs）可以被理解为这样的代理（的模拟器）。本文探讨了这种联系：是否可以通过让LLMs接触具有挑战性的受限游戏式环境来有意义地评估它们的能力？作为概念验证，本文研究了五种交互设置，表明当前的聊天优化LLMs在一定程度上能够遵循游戏玩法指令。这种能力和游戏玩法的质量（通过满足不同游戏目标的情况来衡量）都遵循着发展循环，新型模型表现更好。即使是相对简单的“井字游戏”示例游戏的指标也提供了模型在这些条件下的基本性能指示。这对于将LLMs开发为具有广泛适用性的对话代理人具有启示作用。

    Recent work has proposed a methodology for the systematic evaluation of "Situated Language Understanding Agents"-agents that operate in rich linguistic and non-linguistic contexts-through testing them in carefully constructed interactive settings. Other recent work has argued that Large Language Models (LLMs), if suitably set up, can be understood as (simulators of) such agents. A connection suggests itself, which this paper explores: Can LLMs be evaluated meaningfully by exposing them to constrained game-like settings that are built to challenge specific capabilities? As a proof of concept, this paper investigates five interaction settings, showing that current chat-optimised LLMs are, to an extent, capable to follow game-play instructions. Both this capability and the quality of the game play, measured by how well the objectives of the different games are met, follows the development cycle, with newer models performing better. The metrics even for the comparatively simple example gam
    
[^103]: MultiTurnCleanup：用于多轮口语会话转录清理的基准测试

    MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup. (arXiv:2305.12029v1 [cs.CL])

    [http://arxiv.org/abs/2305.12029](http://arxiv.org/abs/2305.12029)

    本研究提出了MultiTurnCleanup任务，收集了新的数据集MultiTurnCleanup1，针对口语会话转录中的不连续现象进行探讨并提供了两个可用于未来研究的基准测试模型。

    

    目前的语调不连续检测模型侧重于单个说话者的每个话语。然而，口语会话转录中的许多不连续现象都发生在多轮对话中，这影响了人类的可读性和下游 NLP 任务的性能。本研究通过提出创新的“MultiTurnCleanup”任务，针对口语会话转录中的不连续现象进行探讨，并收集了新的数据集MultiTurnCleanup1。我们设计了一种数据标注模式以收集高质量的数据集，提供了广泛的数据分析。此外，我们利用两种建模方法进行实验评估，作为未来研究的基准测试。

    Current disfluency detection models focus on individual utterances each from a single speaker. However, numerous discontinuity phenomena in spoken conversational transcripts occur across multiple turns, hampering human readability and the performance of downstream NLP tasks. This study addresses these phenomena by proposing an innovative Multi-Turn Cleanup task for spoken conversational transcripts and collecting a new dataset, MultiTurnCleanup1. We design a data labeling schema to collect the high-quality dataset and provide extensive data analysis. Furthermore, we leverage two modeling approaches for experimental evaluation as benchmarks for future research.
    
[^104]: mLongT5：一种适用于较长序列的多语言高效文本-文本Transformer

    mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences. (arXiv:2305.11129v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.11129](http://arxiv.org/abs/2305.11129)

    mLongT5是一种多语言高效文本-文本Transformer，适用于处理较长序列的输入。它在多语言摘要和问答任务中表现更好。

    

    我们介绍了我们开发的一种多语言高效的文本-文本Transformer，适用于处理长输入。这个模型被称为mLongT5，它在LongT5的架构基础上构建，同时利用了用于预训练mT5和UL2预训练任务的多语言数据集。我们在各种多语言摘要和问答任务上评估了该模型，并且结果显示与现有的多语言模型如mBART或M-BERT相比，mLongT5表现更好。

    We present our work on developing a multilingual, efficient text-to-text transformer that is suitable for handling long inputs. This model, called mLongT5, builds upon the architecture of LongT5, while leveraging the multilingual datasets used for pretraining mT5 and the pretraining tasks of UL2. We evaluate this model on a variety of multilingual summarization and question-answering tasks, and the results show stronger performance for mLongT5 when compared to existing multilingual models such as mBART or M-BERT.
    
[^105]: ConvXAI：通过对话提供异构的AI解释，支持人机科技写作

    ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v1 [cs.HC])

    [http://arxiv.org/abs/2305.09770](http://arxiv.org/abs/2305.09770)

    ConvXAI是一个基于对话的XAI系统，它集成了多种XAI类型，并将实际用户需求嵌入设计中，以提高实用性。

    

    尽管已经提出了各种各样的人工智能解释（XAI）方法来解释AI系统，但目前的方法是否对人类实用仍存在不一致的发现。为了改善XAI方法的实用性，一系列研究确定了现实世界中多样化和动态的用户需求与现有XAI方法之间的差距。虽然之前的研究设想将多种XAI方法集成到通用XAI界面（例如，基于对话或GUI的XAI系统）中以减轻这些差距，但缺少针对这些系统如何设计以满足实际用户需求的研究。在本研究中，我们提出了ConvXAI，这是一个基于对话的XAI系统，它结合了多种XAI类型，并赋予用户通过通用的XAI对话界面提出各种XAI问题的能力。特别地，我们创新地将实际用户需求（即，基于格式研究的四个原则）嵌入ConvXAI设计中，以提高实用性。

    While various AI explanation (XAI) methods have been proposed to interpret AI systems, whether the state-of-the-art XAI methods are practically useful for humans remains inconsistent findings. To improve the usefulness of XAI methods, a line of studies identifies the gaps between the diverse and dynamic real-world user needs with the status quo of XAI methods. Although prior studies envision mitigating these gaps by integrating multiple XAI methods into the universal XAI interfaces (e.g., conversational or GUI-based XAI systems), there is a lack of work investigating how these systems should be designed to meet practical user needs. In this study, we present ConvXAI, a conversational XAI system that incorporates multiple XAI types, and empowers users to request a variety of XAI questions via a universal XAI dialogue interface. Particularly, we innovatively embed practical user needs (i.e., four principles grounding on the formative study) into ConvXAI design to improve practical useful
    
[^106]: 探究子词分割对transformer语言模型性能的影响

    Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v1 [cs.CL])

    [http://arxiv.org/abs/2305.05480](http://arxiv.org/abs/2305.05480)

    本文研究使用单语词段算法StateMorph训练语言模型时，可以使模型更高效地收敛并获得更好的验证分数。

    

    我们想研究词段如何影响语言模型的性能。我们使用了一种单语词段算法StateMorph，在芬兰语和俄语中训练了GPT-2和BERT模型。作为比较，我们还训练了一个使用BPE和Morfessor分割算法的模型。我们的初步结果表明，StateMorph可以帮助模型更有效地收敛并获得更好的验证分数。

    We would like to explore how morphemes can affect the performance of a language model. We trained GPT-2 and Bert model with StateMorph for both Finnish and Russian, which is a morpheme segmenting algorithm. As a comparison, we also trained a model with BPE and Morfessor. Our preliminary result shows that StateMorph can help the model to converge more efficiently and achieve a better validation score.
    
[^107]: 零样本学习在公司分类中的应用

    Company classification using zero-shot learning. (arXiv:2305.01028v1 [cs.CL])

    [http://arxiv.org/abs/2305.01028](http://arxiv.org/abs/2305.01028)

    本文提出了一种利用自然语言处理和零样本学习的方法来进行公司分类的方法。该方法可以简化公司分类过程，从而减少传统方法如全球产业分类标准（GICS）所需的时间和资源。

    

    近年来，自然语言处理在许多商业应用中变得越来越重要，包括情感分析、文本分类和命名实体识别。本文提出了一种利用自然语言处理和零样本学习的方法来进行公司分类的方法。我们的方法利用预训练的Transformer模型从公司描述中提取特征，然后应用零样本学习将公司分类到相关类别，无需为每个类别提供特定的训练数据。我们在公开可用的公司文本描述数据集上评估我们的方法，并证明它可以简化公司分类过程，从而减少传统方法如全球产业分类标准（GICS）所需的时间和资源。结果表明，该方法具有自动化公司分类的潜力，是未来研究的一个有前途的方向。

    In recent years, natural language processing (NLP) has become increasingly important in a variety of business applications, including sentiment analysis, text classification, and named entity recognition. In this paper, we propose an approach for company classification using NLP and zero-shot learning. Our method utilizes pre-trained transformer models to extract features from company descriptions, and then applies zero-shot learning to classify companies into relevant categories without the need for specific training data for each category. We evaluate our approach on publicly available datasets of textual descriptions of companies, and demonstrate that it can streamline the process of company classification, thereby reducing the time and resources required in traditional approaches such as the Global Industry Classification Standard (GICS). The results show that this method has potential for automation of company classification, making it a promising avenue for future research in thi
    
[^108]: 大型语言模型在信息检索中的排名能力研究——以ChatGPT为例

    Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. (arXiv:2304.09542v1 [cs.CL])

    [http://arxiv.org/abs/2304.09542](http://arxiv.org/abs/2304.09542)

    本文研究了生成性LLMs，如ChatGPT和GPT-4在信息检索中的相关性排名能力，实验结果表明，这些模型经适当指导后表现优异，有时甚至优于传统监督学习方法。将ChatGPT的排名能力提炼为专门模型在BEIR上的效果更优。

    

    大型语言模型（LLMs）已经证明具有remarkable能力，能够将一些零样本语言任务推广至其他领域。本文研究了ChatGPT和GPT-4等生成性LLMs的相关性排名在信息检索方面的能力。实验结果显示，经过适当的指导，ChatGPT和GPT-4可以在流行的信息检索基准上取得竞争优势，甚至有时优于监督学习方法。特别地，GPT-4在TREC数据集上的平均nDCG上表现优于完全微调的monoT5-3B，BEIR数据集上的平均nDCG上优于monoT5-3B 2.3个点，低资源语言Mr.TyDi上的平均nDCG上优于monoT5-3B 2.7个点。随后，我们探讨了将ChatGPT的排名能力提炼为专门的模型的潜力。我们训练的小型专门模型（训练于10K个ChatGPT生成的数据）在BEIR上的表现优于在400K个MS MARCO注释数据上训练的monoT5。代码可在www.github.com/sunnwe上复现。

    Large Language Models (LLMs) have demonstrated a remarkable ability to generalize zero-shot to various language-related tasks. This paper focuses on the study of exploring generative LLMs such as ChatGPT and GPT-4 for relevance ranking in Information Retrieval (IR). Surprisingly, our experiments reveal that properly instructed ChatGPT and GPT-4 can deliver competitive, even superior results than supervised methods on popular IR benchmarks. Notably, GPT-4 outperforms the fully fine-tuned monoT5-3B on MS MARCO by an average of 2.7 nDCG on TREC datasets, an average of 2.3 nDCG on eight BEIR datasets, and an average of 2.7 nDCG on ten low-resource languages Mr.TyDi. Subsequently, we delve into the potential for distilling the ranking capabilities of ChatGPT into a specialized model. Our small specialized model that trained on 10K ChatGPT generated data outperforms monoT5 trained on 400K annotated MS MARCO data on BEIR. The code to reproduce our results is available at www.github.com/sunnwe
    
[^109]: 《解开结构与风格的纽带：通过诱导文档层次结构来检测新闻中的政治偏见》

    Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy. (arXiv:2304.02247v1 [cs.CL])

    [http://arxiv.org/abs/2304.02247](http://arxiv.org/abs/2304.02247)

    本文提出了一种通过文档层次结构诱导来检测新闻中的政治偏见的方法，该方法克服了过拟合和有限的普适性，展现了更好的鲁棒性和准确性。

    

    本文针对新闻文章中政治偏见检测方面的重要差距进行研究。先前进行监督式文档分类的工作可能会偏向各网站的写作风格，导致过拟合和有限的普适性。我们的方法通过考虑句子级语义和文档级修辞结构来克服这一限制，从而产生一种更强大和不受风格影响的检测政治偏见的方法。我们引入了一种新颖的多头分层注意力模型，通过各种注意力头的不同集合有效地编码长文档的结构。我们展示了我们的方法克服了这种域依赖性，并表现出比先前方法更好的鲁棒性和准确性。进一步的分析表明，我们的模型能够捕捉到新闻中常用的话语结构。

    We address an important gap in detection of political bias in news articles. Previous works that perform supervised document classification can be biased towards the writing style of each news outlet, leading to overfitting and limited generalizability. Our approach overcomes this limitation by considering both the sentence-level semantics and the document-level rhetorical structure, resulting in a more robust and style-agnostic approach to detecting political bias in news articles. We introduce a novel multi-head hierarchical attention model that effectively encodes the structure of long documents through a diverse ensemble of attention heads. While journalism follows a formalized rhetorical structure, the writing style may vary by news outlet. We demonstrate that our method overcomes this domain dependency and outperforms previous approaches for robustness and accuracy. Further analysis demonstrates the ability of our model to capture the discourse structures commonly used in the jou
    
[^110]: eP-ALM:语言模型的高效感知增强

    eP-ALM: Efficient Perceptual Augmentation of Language Models. (arXiv:2303.11403v1 [cs.CV])

    [http://arxiv.org/abs/2303.11403](http://arxiv.org/abs/2303.11403)

    本论文提出了一种用对比学习提高语言模型的感知能力的高效方法eP-ALM，可以实现视觉感知信息和文本信息的融合，同时还能在多模态基准测试上实现最先进的结果。

    

    大型语言模型(LLM)迄今为止给世界留下了深刻印象，具有大规模模型所具有的非同寻常的能力。在视觉方面，变压器模型（即ViT）也在追随同一趋势，取得了最具挑战性的基准测试的最佳表现。随着这种单模型的丰富多样，自然会引发一个问题：我们是否需要跟随这个趋势来处理多模态任务？在这项工作中，我们提出将努力集中于现有模型的高效适应，并提出用感知来增强语言模型。现有的适应预训练模型用于视觉语言任务的方法仍然依赖于几个关键组件，从而影响了它们的效率。特别地，他们仍然训练大量的参数，依赖大规模的多模态预训练，使用在巨大的图像-文本数据集上训练的编码器（例如CLIP），并添加了显著的推理开销。此外，这些方法中的大多数关注Zero-Shot和In Context Learning，观察到两种范式之间的巨大差异。在本文中，我们介绍了eP-ALM，一种将视觉感知信息与语言模型相结合的高效方法。我们提出了一种方法，利用对比学习来实现视觉感知和文本信息的融合，具有极小的计算成本。我们的方法不需要任何新的预训练，仍然在多模态基准测试上实现了最先进的结果。

    Large Language Models (LLMs) have so far impressed the world, with unprecedented capabilities that emerge in models at large scales. On the vision side, transformer models (i.e., ViT) are following the same trend, achieving the best performance on challenging benchmarks. With the abundance of such unimodal models, a natural question arises; do we need also to follow this trend to tackle multimodal tasks? In this work, we propose to rather direct effort to efficient adaptations of existing models, and propose to augment Language Models with perception. Existing approaches for adapting pretrained models for vision-language tasks still rely on several key components that hinder their efficiency. In particular, they still train a large number of parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP) trained on huge image-text datasets, and add significant inference overhead. In addition, most of these approaches have focused on Zero-Shot and In Context Learning, with l
    
[^111]: 学习在视觉对象上进行推理

    Learning to reason over visual objects. (arXiv:2303.02260v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.02260](http://arxiv.org/abs/2303.02260)

    本研究调查了以对象为中心的处理视觉场景的通用机制对于提升抽象视觉推理的作用，并发现在RPM-like的基准测试中，这样的模型取得了最先进的结果。

    

    人类智能的核心组成部分是能够识别复杂高维感知数据中的抽象模式，例如Raven's Progressive Matrices （RPM）等视觉推理任务。为了设计具备这种能力的人工智能系统，最近的研究集中在评估神经网络是否能够学习解决类似RPM的问题。先前的研究通常发现，在这些问题上表现良好需要将RPM问题格式的归纳偏见纳入模型中，这引发了这样一个问题：这样的模型是否可以更广泛地应用。在本研究中，我们调查了以对象为中心的处理视觉场景的通用机制对促进抽象视觉推理的程度。我们发现，一个简单的模型，仅由一个对象为中心的编码器和一个transformer推理模块组成，在两个具有挑战性的RPM-like基准测试中（PGM和I-）取得了最先进的结果。

    A core component of human intelligence is the ability to identify abstract patterns inherent in complex, high-dimensional perceptual data, as exemplified by visual reasoning tasks such as Raven's Progressive Matrices (RPM). Motivated by the goal of designing AI systems with this capacity, recent work has focused on evaluating whether neural networks can learn to solve RPM-like problems. Previous work has generally found that strong performance on these problems requires the incorporation of inductive biases that are specific to the RPM problem format, raising the question of whether such models might be more broadly useful. Here, we investigated the extent to which a general-purpose mechanism for processing visual scenes in terms of objects might help promote abstract visual reasoning. We found that a simple model, consisting only of an object-centric encoder and a transformer reasoning module, achieved state-of-the-art results on both of two challenging RPM-like benchmarks (PGM and I-
    
[^112]: 从自由文本中提取关系三元组：90% F1得分是真实的吗？

    90% F1 Score in Relational Triple Extraction: Is it Real ?. (arXiv:2302.09887v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.09887](http://arxiv.org/abs/2302.09887)

    该论文对联合实体和关系提取模型进行了基准研究，发现在更真实的实验设置下，模型的F1得分显著下降。此外，提出了一种利用基于BERT的分类器的两步建模方法。

    

    从文本中提取关系三元组是构建知识库的关键任务。最近，联合实体和关系提取模型的进展展示了在准确提取关系三元组方面达到的卓越F1得分（≥ 90%）。然而，这些模型在受限的实验设置和不真实的数据集下进行了评估。它们忽视了零三元组句子（零基数），从而简化了任务。在本文中，我们在更真实的设置下对最先进的联合实体和关系提取模型进行了基准研究。我们的实验中包括了缺乏任何三元组的句子，提供了综合性的评估。我们的研究结果显示，在这种更真实的实验设置中，模型的F1得分有显著下降（在一个数据集中约10-15%，在另一个数据集中6-14%）。此外，我们提出了一种利用简单的基于BERT的分类器的两步建模方法。

    Extracting relational triples from text is a crucial task for constructing knowledge bases. Recent advancements in joint entity and relation extraction models have demonstrated remarkable F1 scores ($\ge 90\%$) in accurately extracting relational triples from free text. However, these models have been evaluated under restrictive experimental settings and unrealistic datasets. They overlook sentences with zero triples (zero-cardinality), thereby simplifying the task. In this paper, we present a benchmark study of state-of-the-art joint entity and relation extraction models under a more realistic setting. We include sentences that lack any triples in our experiments, providing a comprehensive evaluation. Our findings reveal a significant decline (approximately 10-15\% in one dataset and 6-14\% in another dataset) in the models' F1 scores within this realistic experimental setup. Furthermore, we propose a two-step modeling approach that utilizes a simple BERT-based classifier. This approa
    
[^113]: 人工和机器关于什么是冒犯存在较大分歧的共情冒犯和噪声审计：统一主观冒犯的人类和机器差异

    Vicarious Offense and Noise Audit of Offensive Speech Classifiers: Unifying Human and Machine Disagreement on What is Offensive. (arXiv:2301.12534v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.12534](http://arxiv.org/abs/2301.12534)

    本文通过人工和机器审核员的共情冒犯和噪声审计研究了冒犯性言论检测中的差异性。结果表明，审核员之间存在广泛的分歧，并且人工审核员和大型语言模型分类器无法预测其他审核员的回应。这对于内容审核具有重要意义。

    

    冒犯性言论检测是内容审核的一个关键组成部分。然而，什么是冒犯性的可以是高度主观的。本文研究了当涉及到现实世界社交网站政治言论时，人工和机器审核员对于什么是冒犯性的存在分歧。我们发现（1）审核员之间（包括人工和机器）存在广泛分歧；和（2）人工审核员和大型语言模型分类器无法预测其他审核员基于他们的政治倾向如何回应。对于（1），我们进行了一个前所未有规模的噪声审计，结合了机器和人工回答。对于（2），我们介绍了一个首创的共情冒犯的数据集。我们的噪声审计揭示了不同机器审核员之间的审核结果差异很大。我们与人工审核员进行的实验表明，政治倾向结合敏感问题会影响到一对一的冒犯，以及共情冒犯。数据集可通过https://github.com/Homan-Lab/voic获得。

    Offensive speech detection is a key component of content moderation. However, what is offensive can be highly subjective. This paper investigates how machine and human moderators disagree on what is offensive when it comes to real-world social web political discourse. We show that (1) there is extensive disagreement among the moderators (humans and machines); and (2) human and large-language-model classifiers are unable to predict how other human raters will respond, based on their political leanings. For (1), we conduct a noise audit at an unprecedented scale that combines both machine and human responses. For (2), we introduce a first-of-its-kind dataset of vicarious offense. Our noise audit reveals that moderation outcomes vary wildly across different machine moderators. Our experiments with human moderators suggest that political leanings combined with sensitive issues affect both first-person and vicarious offense. The dataset is available through https://github.com/Homan-Lab/voic
    
[^114]: 如何在生成序列标记中改善基于跨度级别置信度的束搜索？(arXiv:2212.10767v2 [cs.CL] 更新)

    How Does Beam Search improve Span-Level Confidence Estimation in Generative Sequence Labeling?. (arXiv:2212.10767v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10767](http://arxiv.org/abs/2212.10767)

    本文研究了在生成序列标记任务中如何改善对跨度级别置信度的估计。研究发现仅仅使用解码器的输出概率并不是最佳方法，而利用束搜索的前k个预测的统计数据可以显著降低校准误差。

    

    序列标记是信息抽取/信息检索系统中的核心任务。文本生成模型越来越成为这类任务的解决方案（例如实体提取和对话槽填充）。虽然大多数研究都集中在标记准确性上，但一个关键的方面——对模型置信度的理解却被忽视了。具体而言，我们缺乏一个能够可靠地衡量模型对每个标记跨度的预测置信度的原则性理解。本文旨在提供一些关于生成序列标记的模型置信度估计的实证见解。值得注意的是，我们发现仅仅使用解码器的输出概率并不是实现良好校准置信度估计的最佳方法。通过对六个不同任务的公共数据集进行验证，我们展示了我们提出的方法——利用束搜索的前k个预测的统计数据——显著降低了校准误差。

    Sequence labeling is a core task in text understanding for IE/IR systems. Text generation models have increasingly become the go-to solution for such tasks (e.g., entity extraction and dialog slot filling). While most research has focused on the labeling accuracy, a key aspect -- of vital practical importance -- has slipped through the cracks: understanding model confidence. More specifically, we lack a principled understanding of how to reliably gauge the confidence of a model in its predictions for each labeled span. This paper aims to provide some empirical insights on estimating model confidence for generative sequence labeling. Most notably, we find that simply using the decoder's output probabilities \textbf{is not} the best in realizing well-calibrated confidence estimates. As verified over six public datasets of different tasks, we show that our proposed approach -- which leverages statistics from top-$k$ predictions by a beam search -- significantly reduces calibration errors 
    
[^115]: 文本到图像生成中的空间关系基准测试

    Benchmarking Spatial Relationships in Text-to-Image Generation. (arXiv:2212.10015v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.10015](http://arxiv.org/abs/2212.10015)

    本文研究了文本到图像生成中模型生成正确空间关系的能力，并提出了一个评估指标VISOR以衡量生成图像的准确性。实验发现当前T2I模型尽管可以生成高度逼真的图像，但其空间上准确的图像能力仍然不足，特别是在空间谓词和场景关系理解方面。

    

    空间理解是计算机视觉的基本方面，对于人类级别的图像推理至关重要，因此是基础语言理解的重要组成部分。最近的文本到图像合成（T2I）模型在逼真性方面取得了前所未有的进展，但它们的可靠空间理解能力尚不清楚。我们调查了T2I模型生成正确空间关系的能力，并提出了VISOR评估指标，它捕捉了文本中描述的空间关系在图像中是否准确生成。为了基准现有模型，我们引入了一个包含描述两个对象及它们之间空间关系的句子数据集SR2D。我们构建了一个自动化评估流程来识别物体及其空间关系，并在大规模评估T2I模型时采用它。我们的实验发现了一个令人惊讶的发现，也就是尽管最新的T2I模型能够产生高度逼真的图像，但它们生成空间上准确的图像能力仍然不足。具体而言，我们发现现有模型在空间谓词（如'在前面'和'在后面'）方面存在困难，并且在场景的关系理解方面也有困难。

    Spatial understanding is a fundamental aspect of computer vision and integral for human-level reasoning about images, making it an important component for grounded language understanding. While recent text-to-image synthesis (T2I) models have shown unprecedented improvements in photorealism, it is unclear whether they have reliable spatial understanding capabilities. We investigate the ability of T2I models to generate correct spatial relationships among objects and present VISOR, an evaluation metric that captures how accurately the spatial relationship described in text is generated in the image. To benchmark existing models, we introduce a dataset, SR2D, that contains sentences describing two objects and the spatial relationship between them. We construct an automated evaluation pipeline to recognize objects and their spatial relationships, and employ it in a large-scale evaluation of T2I models. Our experiments reveal a surprising finding that, although state-of-the-art T2I models 
    
[^116]: LR-Sum: 面向资源有限语言的摘要技术

    LR-Sum: Summarization for Less-Resourced Languages. (arXiv:2212.09674v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09674](http://arxiv.org/abs/2212.09674)

    该论文介绍了LR-Sum项目，这是一个针对资源有限语言的新的摘要数据集，旨在促进自动摘要研究。数据集包含40种资源有限语言的人工撰写的摘要，并根据Creative Commons许可证发布，是最开放许可的多语言摘要数据集之一。

    

    本预印文档描述了LR-Sum的研究进展，LR-Sum是一个使用宽松许可证创建的数据集，旨在促进对资源有限语言的自动摘要研究。LR-Sum包含40种语言的人工撰写的摘要，其中许多是资源有限的语言。我们描述了从Multilingual Open Text语料库中提取和筛选数据集的过程。源数据是从Voice of America网站收集的公共领域新闻，LR-Sum以Creative Commons许可证（CC BY 4.0）发布，成为最开放许可的多语言摘要数据集之一。我们描述了如何计划使用这些数据进行建模实验，并讨论了数据集的局限性。

    This preprint describes work in progress on LR-Sum, a new permissively-licensed dataset created with the goal of enabling further research in automatic summarization for less-resourced languages. LR-Sum contains human-written summaries for 40 languages, many of which are less-resourced. We describe our process for extracting and filtering the dataset from the Multilingual Open Text corpus (Palen-Michel et al., 2022). The source data is public domain newswire collected from from Voice of America websites, and LR-Sum is released under a Creative Commons license (CC BY 4.0), making it one of the most openly-licensed multilingual summarization datasets. We describe how we plan to use the data for modeling experiments and discuss limitations of the dataset.
    
[^117]: 语言模型显示对推理任务具有类似人类的内容效应

    Language models show human-like content effects on reasoning tasks. (arXiv:2207.07051v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.07051](http://arxiv.org/abs/2207.07051)

    本研究探讨了语言模型在逻辑推理任务中是否像人类一样通过混入内容来影响答案，结果发现大型语言模型的先验期望能够捕捉到这种特征。

    

    抽象推理是智能系统的关键能力。大型语言模型在抽象推理任务上实现了高于随机的性能，但存在许多不完善之处。然而，人类的抽象推理也是不完美的。例如，人类推理受到我们对真实世界的知识和信念的影响，并表现出显著的“内容效应”；当问题的语义内容支持正确的逻辑推理时，人类更可靠地进行推理。这些内容纠缠的推理模式在关于人类智能基本性质的争论中起着核心作用。在这里，我们研究了语言模型是否以类似的方式混入内容来回答逻辑问题，这些语言模型的先验期望捕捉了一些人类知识的特征。我们在三个逻辑推理任务上探索了这个问题：自然语言推理、判断三段论的逻辑有效性和Wason选择任务。我们评估了最先进的大型语言模型的性能。

    Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable "content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models $\unicode{x2014}$ whose prior expectations capture some aspects of human knowledge $\unicode{x2014}$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large 
    

