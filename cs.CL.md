# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Just One Byte (per gradient): A Note on Low-Bandwidth Decentralized Language Model Finetuning Using Shared Randomness.](http://arxiv.org/abs/2306.10015) | 本文介绍了一种通过使用共享随机性来进行低带宽分散式微调的方法，该方法可以通过每台机器生成不同的随机扰动来更新每个模型，从而具有高度通信效率，并且具有隐私保护的优势。 |
| [^2] | [MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing.](http://arxiv.org/abs/2306.10012) | MagicBrush是第一个大规模的手动标注的数据集，用于指导真实图像的编辑。它包括超过10K个手动标注的三元组，支持大规模的文本指导图像编辑模型训练。在此数据集上微调InstructPix2Pix可以根据人类评估提供更好的图像。 |
| [^3] | [Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering.](http://arxiv.org/abs/2306.09996) | 本文探索使用不同提示策略，重点关注 BLIP2 模型，来提高零样本 VQA 的性能，研究了不同问题模板的有效性、少量样本示例的作用、思维链推理的影响以及将图像标题作为额外视觉线索融合的好处。精心设计的问题模板和整合额外视觉线索可以促进 VQA 性能的提高，特别是当它们结合使用时。 |
| [^4] | [ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation.](http://arxiv.org/abs/2306.09968) | ClinicalGPT是一种专门为临床场景设计和优化的语言模型，通过使用多样化的真实世界数据进行训练，以处理多个临床任务。它的综合评估结果表明，其在大多数测试场景中表现优于现有的基线模型。 |
| [^5] | [Trained Transformers Learn Linear Models In-Context.](http://arxiv.org/abs/2306.09927) | 本文研究了Transformer在具有单层线性自注意层的线性回归任务上通过梯度流进行训练的ICL机制，揭示了梯度流具有找到目标函数全局最小值的能力。 |
| [^6] | [Learning to Summarize and Answer Questions about a Virtual Robot's Past Actions.](http://arxiv.org/abs/2306.09922) | 本论文介绍了一种学习总结和回答机器人动作历史的方法，能够通过一种语言模型同时完成总结和回答任务，并提供了自动生成问题和答案的方法来进行训练。此方法能够实现从问题回答中学习对象表示的零-shot转移。 |
| [^7] | [No Strong Feelings One Way or Another: Re-operationalizing Neutrality in Natural Language Inference.](http://arxiv.org/abs/2306.09918) | 本文讨论了自然语言推理(NLI)中标签操作存在的缺点，提出了NLI需要更精细的评价体系的观点，并比较了处理注释者一致性的方法。 |
| [^8] | [Demystifying GPT Self-Repair for Code Generation.](http://arxiv.org/abs/2306.09896) | 本文分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，自我修复在较短和较简单的任务中效果更好，仅在某些代码部分上应用自我修复可以非常有效，本文提出的引导修复方法在 APPS 数据集上获得性能提升。 |
| [^9] | [Revealing the impact of social circumstances on the selection of cancer therapy through natural language processing of social work notes.](http://arxiv.org/abs/2306.09877) | 通过自然语言处理社工记录，揭示了社会情境对选择癌症治疗的影响。 |
| [^10] | [Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models.](http://arxiv.org/abs/2306.09869) | 本论文提出了能量交叉注意力的EBM框架，通过更新和转移上下文向量，隐式最小化能量函数的嵌套层次，优化文本到图像扩散模型的语义对齐问题，实现零样本组合生成。 |
| [^11] | [Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views.](http://arxiv.org/abs/2306.09841) | 本文评估了大型语言模型的逻辑推理能力，选择了15个典型数据集，考虑了演绎、归纳、阿布达斯和混合推理形式，并选择了三个代表性的LLMs进行零样本、一次和三次的设置下评估。提出精细级别的评估方法。 |
| [^12] | [Sheffield's Submission to the AmericasNLP Shared Task on Machine Translation into Indigenous Languages.](http://arxiv.org/abs/2306.09830) | 本文介绍了谢菲尔德大学的机器翻译方法，成功在AmericasNLP机器翻译土著语言分享任务中取得了最高的平均chrF，其中在Aymara，Guarani和Quechua方面有显着的改进。 |
| [^13] | [Process Knowledge-infused Learning for Clinician-friendly Explanations.](http://arxiv.org/abs/2306.09824) | 本论文介绍了一种名为PK-iL的新学习范式，可以在语言模型输出上添加临床流程知识结构，从而使医生能够理解和解释模型的输出，从而为心理卫生护理和预防策略提供支持和帮助。 |
| [^14] | [Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System.](http://arxiv.org/abs/2306.09821) | 该论文提出了一种名为用户引导响应优化（UGRO）的方法，使用大型语言模型作为无注释的用户模拟器，以评估对话响应并优化监督式经过微调的端到端任务导向对话模型。该方法利用了大型语言模型在提供满意度反馈方面的潜力，取得了显著的改进，为利用大型语言模型增强对话系统提供了新的思路。 |
| [^15] | [Investigating the Utility of Surprisal from Large Language Models for Speech Synthesis Prosody.](http://arxiv.org/abs/2306.09814) | 本文研究了单词惊讶度量作为一种语音合成韵律特征的应用，发现它与单词突出性有着适度的相关性，但使用惊讶度量作为条件特征无法提高语音合成质量。 |
| [^16] | [RED$^{\rm FM}$: a Filtered and Multilingual Relation Extraction Dataset.](http://arxiv.org/abs/2306.09802) | 本文提出了两个新的数据集，分别是自动标注的SRED$^{\rm FM}$和人工修订的RED$^{\rm FM}$。SRED$^{\rm FM}$涵盖了18种语言、400种关系类型、13种实体类型，总共超过4000万个三元组实例；RED$^{\rm FM}$是RED的精简版，可用于评估多语言关系抽取系统。实验证明，这些新数据集能有效用于建立多语言关系抽取模型。 |
| [^17] | [Full Parameter Fine-tuning for Large Language Models with Limited Resources.](http://arxiv.org/abs/2306.09782) | 本文提出了一种低内存使用的优化器LOMO，可以实现在有限资源下对大型语言模型进行全参数微调，从而降低研究门槛。 |
| [^18] | [Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models.](http://arxiv.org/abs/2306.09752) | 中文总结该论文主要研究了日语和韩语语言模型中与礼貌水平相关的语法性别偏见，发现礼貌水平是网络欺凌检测模型中的攻击向量。 |
| [^19] | [Using Natural Language Processing and Networks to Automate Structured Literature Reviews: An Application to Farmers Climate Change Adaptation.](http://arxiv.org/abs/2306.09737) | 本文利用自然语言处理和网络，自动化结构化文献综述，以农民应对气候变化的分析为例，提取变量关系并使用网络综合其发现。 |
| [^20] | [Discourse Representation Structure Parsing for Chinese.](http://arxiv.org/abs/2306.09725) | 该论文探索了在缺乏中文标签数据的情况下汉语语义分析的可行性，并提出了一个专为汉语语义分析设计的测试套件。副词是汉语语义分析的主要难点。 |
| [^21] | [Pushing the Limits of ChatGPT on NLP Tasks.](http://arxiv.org/abs/2306.09719) | 本研究提出了一系列通用模块以解决 ChatGPT 在自然语言处理任务中的弱点，包括利用多个提示符来适应更多演示、使用精细调整模型以获得更好的演示检索、转换任务为更适合生成性质的格式以及采用针对 NLP 任务设计的推理策略。 |
| [^22] | [Semi-Offline Reinforcement Learning for Optimized Text Generation.](http://arxiv.org/abs/2306.09712) | 该研究提出了一种半离线强化学习范式，该范式平衡了探索能力和培训成本，提供了一个理论基础来比较不同的强化学习设置，并在优化成本、渐近误差和过度拟合误差界方面实现了最优的RL设置。实验结果表明，该方法高效且性能优异。 |
| [^23] | [Reducing Computational Costs in Sentiment Analysis: Tensorized Recurrent Networks vs. Recurrent Networks.](http://arxiv.org/abs/2306.09705) | 张量循环网络是一种降低情感分析计算成本的潜在解决方案 |
| [^24] | [Cross-corpus Readability Compatibility Assessment for English Texts.](http://arxiv.org/abs/2306.09704) | 本文提出了一个新的评估框架，Cross-corpus text Readability Compatibility Assessment (CRCA)，用于解决不同语料库之间的可读性兼容性的问题。研究结果表明该框架具有显著的兼容性，并适用于不同的特征表示和分类方法。 |
| [^25] | [Class-Adaptive Self-Training for Relation Extraction with Incompletely Annotated Training Data.](http://arxiv.org/abs/2306.09697) | 本研究提出了一种新的基于类别自适应重新采样自训练框架，能够显著改善在少数类别中的总体召回率，而不会明显降低准确率，从而实现了关系提取的最先进性能。 |
| [^26] | [Online Distillation for Pseudo-Relevance Feedback.](http://arxiv.org/abs/2306.09657) | 本文提出了一种伪相关反馈的在线蒸馏技术，通过在线逐步建立模型，预测查询与文档的相关得分，并在索引中高效执行，以优化整体检索效果。 |
| [^27] | [ReactGenie: An Object-Oriented State Abstraction for Complex Multimodal Interactions Using Large Language Models.](http://arxiv.org/abs/2306.09649) | ReactGenie是一个支持构建复杂的多模态移动应用程序的编程框架，通过使用共享的面向对象状态抽象，使得不同模态可以无缝集成和组合，从而实现了多模态交互的支持。 |
| [^28] | [Cross-Domain Toxic Spans Detection.](http://arxiv.org/abs/2306.09642) | 本研究评估了三种跨域条件下检测毒性片段的方法，结果表明使用现成的词典的简单方法在跨域设置中表现最佳，而用于领域内的语言模型容易出现某些类型的假阳性。 |
| [^29] | [AUGUST: an Automatic Generation Understudy for Synthesizing Conversational Recommendation Datasets.](http://arxiv.org/abs/2306.09631) | 本文提出了一种新型自动数据集合成方法，利用传统推荐系统中的个性化用户画像和结构化图表中的丰富交互信息，通过数据到文本生成过程生成大规模和高质量的推荐对话。 |
| [^30] | [Listener Model for the PhotoBook Referential Game with CLIPScores as Implicit Reference Chain.](http://arxiv.org/abs/2306.09607) | 本文介绍了使用CLIPScores作为隐式参考链的PhotoBook指称游戏的听者模型，该模型直接解决了决定图像是否与伙伴共享的预测任务，并在未见过的图像集/游戏主题上取得了>77%的准确率，优于基线>17个百分点。 |
| [^31] | [Clickbait Detection via Large Language Models.](http://arxiv.org/abs/2306.09597) | 本文研究了大型语言模型在点击诱骗检测上的性能，结果表明LLM无法取得最佳结果且不能仅通过标题实现满意的检测。 |
| [^32] | [CMLM-CSE: Based on Conditional MLM Contrastive Learning for Sentence Embeddings.](http://arxiv.org/abs/2306.09594) | 本论文提出了一种基于条件MLM的无监督对比学习框架CMLM-CSE，强制句子嵌入学习更多的掩码词信息，可以在文本相似度任务中超越SimCSE。 |
| [^33] | [How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese.](http://arxiv.org/abs/2306.09572) | 本文研究了在日语这种连续书写文字语言中，不同的分词器对预训练语言模型在下游任务中的影响，发现每个下游任务都有一个最佳的形态学分析器，并且无论任务类型如何，最好使用字节对编码或Unigram作为子词分词器。 |
| [^34] | [Reproducibility in NLP: What Have We Learned from the Checklist?.](http://arxiv.org/abs/2306.09562) | 本论文讨论了NLP科学进展的可重复性，介绍了NLP可重复性清单的情况和通过对此清单10,405个匿名回答的分析发现某些项回答“是”的提交数量增加、随之接受率增加以及开源代码与可重复性得分提高之间的关系等。 |
| [^35] | [Building blocks for complex tasks: Robust generative event extraction for radiology reports under domain shifts.](http://arxiv.org/abs/2306.09544) | 本文介绍了一个在放射学报告中提取信息的鲁棒性生成式模型，具有良好的泛化性。通过将复杂任务分解为更小的子任务块，与多任务训练相结合，可以提高单遍模型的性能，并通过利用目标域上下文来增强领域适应性，实现使用更小的模型。 |
| [^36] | [Block-State Transformer.](http://arxiv.org/abs/2306.09539) | 本文提出了一种名为块状态变换器（BST）的混合神经网络层，结合了状态空间模型和块变换器，旨在在语言建模任务中提高性能和可扩展性。实验证明，该模型在语言建模困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，在层级别上具有超过十倍的速度提升。 |
| [^37] | [Explaining Legal Concepts with Augmented Large Language Models (GPT-4).](http://arxiv.org/abs/2306.09525) | 本文评估了利用GPT-4增强解释法律术语性能的方法。与基准设置相比，使用增强的法律信息检索模块可以提高法律术语解释的质量，并消除模型的幻觉问题，从而为在法律领域使用大型语言模型开辟了新的途径。 |
| [^38] | [Relation-Aware Network with Attention-Based Loss for Few-Shot Knowledge Graph Completion.](http://arxiv.org/abs/2306.09519) | 本文提出了一种新颖的RANA框架，利用有策略地选择相关负样本和设计基于注意力机制的损失函数来更好地利用负样本并缓解零损失问题，同时设计了一种动态的关系感知实体编码来捕获不同关系下实体的不同表示。 |
| [^39] | [Wikibio: a Semantic Resource for the Intersectional Analysis of Biographical Events.](http://arxiv.org/abs/2306.09505) | 本文提供了一个新的用于注释人物传记事件检测的语料库，并训练出了一个模型以检测实体及其相关事件。该模型能够高效地分析维基百科传记中有关女性和非西方人的偏见。 |
| [^40] | [Inverse Scaling: When Bigger Isn't Better.](http://arxiv.org/abs/2306.09479) | 本文研究发现，相对于规模的增加，大型语言模型的任务性能可能出现逆向缩放现象。这一逆向缩放的原因可能有四种：记忆重现、学习样本错误、任务易于干扰、和任务示范的误导。 |
| [^41] | [Explore, Establish, Exploit: Red Teaming Language Models from Scratch.](http://arxiv.org/abs/2306.09442) | 本文提出了一种新的红队行动，通过从高层次、抽象的规范出发来考虑语言模型的行为，以探究模型的创新和贡献。 |
| [^42] | [ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations.](http://arxiv.org/abs/2306.09390) | 本文量化评估了基于社交媒体的 ChatGPT 模型在自杀倾向评估方面的表现，比较了其结果与两个微调模型，并探讨了模型响应生成的最佳温度，研究结果显示，以人工标注数据集为基础微调的 Transformer 模型表现更佳，这篇论文为心理健康专家提供了重要的模型评估和参数优化建议。 |
| [^43] | [MFAS: Emotion Recognition through Multiple Perspectives Fusion Architecture Search Emulating Human Cognition.](http://arxiv.org/abs/2306.09361) | 该论文提出了一种基于多角度融合结构搜索的情感识别框架，模拟人类的认知过程，能够从连续的角度捕捉更全面的情感信息。 |
| [^44] | [h2oGPT: Democratizing Large Language Models.](http://arxiv.org/abs/2306.08161) | 本文介绍了h2oGPT，这是一套开源代码库，用于创建和使用基于GPTs的大语言模型（LLMs），包括100％私有文档搜索。目标是创建真正开源的替代封闭源GPTs，提高人工智能的开发和可靠性。 |
| [^45] | [GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition.](http://arxiv.org/abs/2306.07848) | 本文提出了GEmo-CLAP模型用于语音情感识别，结合了性别属性信息，相比于其他先进方法，该模型在IEMOCAP上实现了更优越的识别性能。 |
| [^46] | [Large Language Models Sometimes Generate Purely Negatively-Reinforced Text.](http://arxiv.org/abs/2306.07567) | 大型语言模型有时会从仅包含负奖励的例子中学习，导致生成类似泄漏密码或安全漏洞等敏感信息的文本 |
| [^47] | [Evaluating GPT-3 Generated Explanations for Hateful Content Moderation.](http://arxiv.org/abs/2305.17680) | 本文通过调查和分析，评估了使用GPT-3生成的针对仇恨内容的解释是否准确和有用。结果显示，GPT-3生成的解释普遍存在过于模糊、聚焦不当等缺点，同时也存在不同类型仇恨言论生成的解释质量差异大的问题。 |
| [^48] | ["When Words Fail, Emojis Prevail": Generating Sarcastic Utterances with Emoji Using Valence Reversal and Semantic Incongruity.](http://arxiv.org/abs/2305.04105) | 本文提出了一种使用情感反转和语义不一致性从非讽刺性输入句子生成带有表情符号的讽刺性句子的架构，具有广泛的应用前景。 |
| [^49] | [Similarity-Aware Multimodal Prompt Learning for Fake News Detection.](http://arxiv.org/abs/2304.04187) | 该论文提出了一种相似度感知的多模态提示学习框架，用于有效融合不同模态信息，实现假新闻检测。所设计的相似度感知融合模块可以量化地根据不同模态之间的相似性权衡每个模态的重要性，有效避免了多模态融合中可能存在的噪声干扰。在两个基准数据集上的实验结果也证明提出的框架实现了最先进的性能，超越了其他强大基线方法。 |
| [^50] | [Deep Learning for Opinion Mining and Topic Classification of Course Reviews.](http://arxiv.org/abs/2304.03394) | 本文利用自然语言处理和深度学习技术，通过比较传统方法和现代机器学习方法，展示了如何处理大量课程评论，进行情感极性分析和主题分类。 |
| [^51] | [Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language.](http://arxiv.org/abs/2303.03363) | 本文提出了一种新型活性预测模型，能够通过理解描述任务的文本信息来适应推理时的新预测任务，并在少样本学习基准和药物研发中的零数据问题上都能取得更好的预测性能。 |
| [^52] | [TransFool: An Adversarial Attack against Neural Machine Translation Models.](http://arxiv.org/abs/2302.00944) | 本文研究神经机器翻译(NMT)的对抗攻击易感性，并提出了一种新的攻击算法 TransFool。通过集成语言模型的嵌入表征，我们生成了与原始样本有着高度语义相似性的准确对抗性样本。实验结果表明，在不同翻译任务和 NMT 架构之下，TransFool 可以转移到未知的目标模型，导致严重降低译文质量。 |
| [^53] | [SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks.](http://arxiv.org/abs/2212.10525) | 本文介绍了一种新的注释SLU基准评测任务套件，其中包括问答和摘要、命名实体本地化和对话行为分类任务。这些任务基于自由获取的语音数据设计，希望可以鼓励进一步研究和开发SLU技术，从而推动口语理解技术的发展。 |
| [^54] | [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding.](http://arxiv.org/abs/2210.03347) | 本文提出了一个名为Pix2Struct的预训练图像到文本模型，用于纯视觉语言理解，可以在包含视觉语境的任务上进行微调。Pix2Struct的预训练目标是学习将屏幕截图解析成简化的HTML，这个方法能够处理来源复杂、多样性大的数据。 |
| [^55] | [Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems.](http://arxiv.org/abs/2209.03755) | 本研究提出了一种分类法，涵盖针对在线证据的伪装和虚假信息等两种攻击目标和不同的威胁模型维度。我们设计并提出了几种可能的攻击方法，展示了在多种情况下，可以对证据中与事实相关的部分进行微小的修改，并生成无法被自动识别的虚假信息。 |
| [^56] | [Resolving the Human Subjects Status of Machine Learning's Crowdworkers.](http://arxiv.org/abs/2206.04039) | 机器学习在研究中使用的众包工作者问题引起了对其受试者身份的争议与监管合规性，本文针对该问题进行研究，重点关注了自然语言处理领域中的研究监管挑战。 |
| [^57] | [Revisiting DocRED -- Addressing the False Negative Problem in Relation Extraction.](http://arxiv.org/abs/2205.12696) | 该论文主要针对文档层面上的关系抽取中存在假阴性的问题，在重新注释 DocRED 数据集中添加被忽略的关系三元组后，我们得到了一个性能提升约 13 F1 分数的新数据集 Re-DocRED，并发现了有效改进的潜在领域。 |

# 详细

[^1]: 仅使用一字节（每梯度）：关于使用共享随机性进行低带宽分散式语言模型微调的简要说明

    Just One Byte (per gradient): A Note on Low-Bandwidth Decentralized Language Model Finetuning Using Shared Randomness. (arXiv:2306.10015v1 [cs.LG])

    [http://arxiv.org/abs/2306.10015](http://arxiv.org/abs/2306.10015)

    本文介绍了一种通过使用共享随机性来进行低带宽分散式微调的方法，该方法可以通过每台机器生成不同的随机扰动来更新每个模型，从而具有高度通信效率，并且具有隐私保护的优势。

    

    分布式模型训练受到梯度交换的通信成本的限制。本文通过使用共享随机性来进行低带宽的分散式微调，扩展了Malladi等人2023年的最新工作。该方法是对具有记忆效率的同时扰动随机逼近（SPSA）的自然分散式扩展。在每次迭代中，每台机器都使用随机数生成器（RNG）来对模型权重进行局部可重现的扰动，并计算和交换标量投影梯度，然后用于更新每个模型。通过将（机器，样本）标识符用作随机种子，每个模型可以重新生成彼此的扰动。由于机器只交换单字节的投影梯度，因此这是高度通信效率的。此外，还存在潜在的隐私优势，因为投影梯度可以在不同的训练数据上计算，而模型从不访问其他数据。我们的方法不仅在具有低通信成本的同时，也在模型的准确性和训练速度方面展现出优势。

    Language model training in distributed settings is limited by the communication cost of gradient exchanges. In this short note, we extend recent work from Malladi et al. (2023), using shared randomness to perform distributed fine-tuning with low bandwidth. The method is a natural decentralized extension of memory-efficient Simultaneous Perturbation Stochastic Approximation (SPSA). Each iteration, each machine seeds a Random Number Generator (RNG) to perform local reproducible perturbations on model weights and calculate and exchange scalar projected gradients, which are then used to update each model. By using a (machine, sample) identifier as the random seed, each model can regenerate one another's perturbations. As machines only exchange single-byte projected gradients, this is highly communication efficient. There are also potential privacy benefits, as projected gradients may be calculated on different training data, and models never access the other's data. Our approach not only d
    
[^2]: MagicBrush: 人工标注的用于指导图像编辑的数据集

    MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing. (arXiv:2306.10012v1 [cs.CV])

    [http://arxiv.org/abs/2306.10012](http://arxiv.org/abs/2306.10012)

    MagicBrush是第一个大规模的手动标注的数据集，用于指导真实图像的编辑。它包括超过10K个手动标注的三元组，支持大规模的文本指导图像编辑模型训练。在此数据集上微调InstructPix2Pix可以根据人类评估提供更好的图像。

    

    文本指导的图像编辑从个人使用到专业应用（如Photoshop）广泛需要。然而，现有的方法要么是零样本，要么是在自动合成的数据集上进行训练，其中含有大量的噪声。因此，它们在实践中仍需要大量的手动调整才能产生理想的结果。为了解决这个问题，我们介绍了MagicBrush，第一个大规模的手动标注的数据集，用于指导真实图像的编辑，包括单个操作、多个操作、提供掩码和不提供掩码等不同场景。MagicBrush包括超过10K个手动标注的三元组（源图像，指令，目标图像），支持大规模的文本指导图像编辑模型训练。我们在MagicBrush上微调InstructPix2Pix，并展示了新模型可以根据人类评估提供更好的图像。我们还进行了广泛的实验评估，以评估模型的泛化能力和使用效果。

    Text-guided image editing is widely needed in daily life, ranging from personal use to professional applications such as Photoshop. However, existing methods are either zero-shot or trained on an automatically synthesized dataset, which contains a high volume of noise. Thus, they still require lots of manual tuning to produce desirable outcomes in practice. To address this issue, we introduce MagicBrush (https://osu-nlp-group.github.io/MagicBrush/), the first large-scale, manually annotated dataset for instruction-guided real image editing that covers diverse scenarios: single-turn, multi-turn, mask-provided, and mask-free editing. MagicBrush comprises over 10K manually annotated triples (source image, instruction, target image), which supports trainining large-scale text-guided image editing models. We fine-tune InstructPix2Pix on MagicBrush and show that the new model can produce much better images according to human evaluation. We further conduct extensive experiments to evaluate cu
    
[^3]: 探究零样本和少样本视觉问答提示技术

    Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering. (arXiv:2306.09996v1 [cs.CV])

    [http://arxiv.org/abs/2306.09996](http://arxiv.org/abs/2306.09996)

    本文探索使用不同提示策略，重点关注 BLIP2 模型，来提高零样本 VQA 的性能，研究了不同问题模板的有效性、少量样本示例的作用、思维链推理的影响以及将图像标题作为额外视觉线索融合的好处。精心设计的问题模板和整合额外视觉线索可以促进 VQA 性能的提高，特别是当它们结合使用时。

    

    视觉问答（VQA）是一项具有挑战性的任务，需要具备理解和推理视觉信息的能力。虽然近期的视觉语言模型取得了进展，但它们在零样本VQA方面仍然存在问题，特别是在处理复杂组合问题和适应新领域，如基于知识的推理方面。本文探讨了各种提示策略的使用，重点关注BLIP2模型，以提高零样本VQA的性能。我们在几个VQA数据集上进行了全面调查，研究了不同问题模板的有效性、少量样本示例的作用、思维链推理的影响以及将图像标题作为额外视觉线索融合的好处。尽管结果各异，但我们的发现表明，精心设计的问题模板和整合额外视觉线索（如图像标题）可以促进VQA性能的提高，特别是当它们结合使用时。

    Visual question answering (VQA) is a challenging task that requires the ability to comprehend and reason with visual information. While recent vision-language models have made strides, they continue to struggle with zero-shot VQA, particularly in handling complex compositional questions and adapting to new domains i.e. knowledge-based reasoning. This paper explores the use of various prompting strategies, focusing on the BLIP2 model, to enhance zero-shot VQA performance. We conduct a comprehensive investigation across several VQA datasets, examining the effectiveness of different question templates, the role of few-shot exemplars, the impact of chain-of-thought (CoT) reasoning, and the benefits of incorporating image captions as additional visual cues. Despite the varied outcomes, our findings demonstrate that carefully designed question templates and the integration of additional visual cues, like image captions, can contribute to improved VQA performance, especially when used in conj
    
[^4]: ClinicalGPT: 大规模语言模型的细致精调和综合评估

    ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation. (arXiv:2306.09968v1 [cs.CL])

    [http://arxiv.org/abs/2306.09968](http://arxiv.org/abs/2306.09968)

    ClinicalGPT是一种专门为临床场景设计和优化的语言模型，通过使用多样化的真实世界数据进行训练，以处理多个临床任务。它的综合评估结果表明，其在大多数测试场景中表现优于现有的基线模型。

    

    大规模语言模型在各种自然语言处理任务上表现出出色的性能，利用诸如预训练和指导微调等技术。尽管取得了这些进展，但它们在医疗应用中的效果受到限制，由于存在诸如事实不准确、推理能力和缺乏真实世界经验等挑战。在本研究中，我们提出了ClinicalGPT，一种专门设计和优化用于临床场景的语言模型。通过在训练过程中结合大量且多样化的真实世界数据，如医疗记录、领域特定知识和多轮对话咨询，ClinicalGPT更好地准备处理多个临床任务。此外，我们还介绍了一个综合评估框架，包括医学知识问答、医学考试、病人咨询和医疗记录的诊断分析。我们的结果表明，ClinicalGPT在大多数测试场景中表现优于现有的基线模型。

    Large language models have exhibited exceptional performance on various Natural Language Processing (NLP) tasks, leveraging techniques such as the pre-training, and instruction fine-tuning. Despite these advances, their effectiveness in medical applications is limited, due to challenges such as factual inaccuracies, reasoning abilities, and lack grounding in real-world experience. In this study, we present ClinicalGPT, a language model explicitly designed and optimized for clinical scenarios. By incorporating extensive and diverse real-world data, such as medical records, domain-specific knowledge, and multi-round dialogue consultations in the training process, ClinicalGPT is better prepared to handle multiple clinical task. Furthermore, we introduce a comprehensive evaluation framework that includes medical knowledge question-answering, medical exams, patient consultations, and diagnostic analysis of medical records. Our results demonstrate that ClinicalGPT significantly outperforms o
    
[^5]: 训练好的Transformer在上下文中学习线性模型

    Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])

    [http://arxiv.org/abs/2306.09927](http://arxiv.org/abs/2306.09927)

    本文研究了Transformer在具有单层线性自注意层的线性回归任务上通过梯度流进行训练的ICL机制，揭示了梯度流具有找到目标函数全局最小值的能力。

    

    基于注意力的神经网络，例如Transformers，在上下文学习（ICL）方面表现出了非凡的能力：给定一个来自未见过的任务的短语序列的提示，它们可以制定相关的每个令牌和下一个令牌的预测，而不需要任何参数更新。通过将标记的训练数据和未标记的测试数据序列嵌入到提示中，这使得Transformer表现得像有监督学习算法。事实上，最近的工作表明，在随机实例上训练Transformer体系结构的线性回归问题时，这些模型的预测会模仿普通最小二乘法的预测。

    Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares.  Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this 
    
[^6]: 学习总结和回答与虚拟机器人过去动作相关的问题

    Learning to Summarize and Answer Questions about a Virtual Robot's Past Actions. (arXiv:2306.09922v1 [cs.RO])

    [http://arxiv.org/abs/2306.09922](http://arxiv.org/abs/2306.09922)

    本论文介绍了一种学习总结和回答机器人动作历史的方法，能够通过一种语言模型同时完成总结和回答任务，并提供了自动生成问题和答案的方法来进行训练。此方法能够实现从问题回答中学习对象表示的零-shot转移。

    

    当机器人执行长序列的动作时，用户需要轻松、可靠地了解它们所做的事情。因此，我们演示了使用自然语言学习总结和回答关于机器人代理过去动作的问题的任务。一个核心为大型语言模型的单一系统被训练用于总结和回答关于虚拟机器人的自我中心视频帧和问题提示的动作序列。为了实现问题回答的训练，我们开发了一种方法来自动生成关于对象、动作和在虚拟环境中机器人动作序列期间动作发生的时间顺序的英文问题和答案。将一个模型用于总结和回答问题使得从问题回答中学习的对象表示的零-shot转移能够提高动作总结的能力，包括在训练中未见过的对象。

    When robots perform long action sequences, users will want to easily and reliably find out what they have done. We therefore demonstrate the task of learning to summarize and answer questions about a robot agent's past actions using natural language alone. A single system with a large language model at its core is trained to both summarize and answer questions about action sequences given ego-centric video frames of a virtual robot and a question prompt. To enable training of question answering, we develop a method to automatically generate English-language questions and answers about objects, actions, and the temporal order in which actions occurred during episodes of robot action in the virtual environment. Training one model to both summarize and answer questions enables zero-shot transfer of representations of objects learned through question answering to improved action summarization. % involving objects not seen in training to summarize.
    
[^7]: 理性一无所感：在自然语言推理中重新操作中立性

    No Strong Feelings One Way or Another: Re-operationalizing Neutrality in Natural Language Inference. (arXiv:2306.09918v1 [cs.CL])

    [http://arxiv.org/abs/2306.09918](http://arxiv.org/abs/2306.09918)

    本文讨论了自然语言推理(NLI)中标签操作存在的缺点，提出了NLI需要更精细的评价体系的观点，并比较了处理注释者一致性的方法。

    

    自然语言推理(NLI)一直是评估语言模型推理能力的基石任务。然而，NLI中使用的标准三分类模式在评估模型捕捉人类推理的微妙之处方面存在已知缺点。在本文中，我们认为当前NLI数据集中中立标签的操作化效度很低，解释不一致，并且通常忽略至少一个重要的中立意义。我们揭示了这些缺点的有害影响，在某些情况下导致注释数据集实际上降低了下游任务的性能。我们比较了处理注释者不一致的方法，并确定了最近的一个NLI数据集中存在的问题操作化的注释者研究的缺陷。我们的研究结果凸显了NLI需要更精细的评估框架，我们希望引发NLP社区对于改善语言模型评估的讨论和行动。

    Natural Language Inference (NLI) has been a cornerstone task in evaluating language models' inferential reasoning capabilities. However, the standard three-way classification scheme used in NLI has well-known shortcomings in evaluating models' ability to capture the nuances of natural human reasoning. In this paper, we argue that the operationalization of the neutral label in current NLI datasets has low validity, is interpreted inconsistently, and that at least one important sense of neutrality is often ignored. We uncover the detrimental impact of these shortcomings, which in some cases leads to annotation datasets that actually decrease performance on downstream tasks. We compare approaches of handling annotator disagreement and identify flaws in a recent NLI dataset that designs an annotator study based on a problematic operationalization. Our findings highlight the need for a more refined evaluation framework for NLI, and we hope to spark further discussion and action in the NLP c
    
[^8]: 揭秘 GPT 自我修复代码生成能力

    Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])

    [http://arxiv.org/abs/2306.09896](http://arxiv.org/abs/2306.09896)

    本文分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，自我修复在较短和较简单的任务中效果更好，仅在某些代码部分上应用自我修复可以非常有效，本文提出的引导修复方法在 APPS 数据集上获得性能提升。

    

    大型语言模型 (LLM) 在代码生成方面表现出色，但在挑战性编程任务上仍面临困难。自我修复——即模型调试并修复自己的代码——最近成为提高性能的一种流行方式。然而，关于自我修复如何有效地发挥作用的研究还非常有限。有人会想知道，当同一模型生成代码时，模型究竟能否提供准确的反馈。在本文中，我们分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，这是一个由多种编码挑战组成的具有挑战性的数据集。我们首先建立了一种新的评估策略 pass@t，该策略衡量了任务通过率与从模型中抽样的总标记数，从而实现对仅基于抽样的方法的公平比较。通过这种评估策略，我们发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，并确定了影响自我修复表现的几个因素。具体而言，我们发现，在输入噪声较少且模型对初始输出不太自信的较短和较简单的任务中，自我修复效果更好。我们还表明，仅在某些代码部分上应用自我修复可以非常有效。此外，我们提出了一种新的引导修复方法，利用外部反馈来增强 GPT 模型的自我修复能力，在 APPS 数据集上获得性能提升。

    Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness
    
[^9]: 通过对社会工作记录的自然语言处理揭示社会情境对癌症治疗选择的影响

    Revealing the impact of social circumstances on the selection of cancer therapy through natural language processing of social work notes. (arXiv:2306.09877v1 [cs.CL])

    [http://arxiv.org/abs/2306.09877](http://arxiv.org/abs/2306.09877)

    通过自然语言处理社工记录，揭示了社会情境对选择癌症治疗的影响。

    

    本文旨在调查社会情境对癌症治疗选择的影响，并通过对社工文档的自然语言处理来获得洞见。我们开发并采用了基于双向编码器表示转换器（BERT）的方法，使用分层多步BERT模型（BERT-MS）仅基于临床社会工作者的文档预测了治疗靶向癌症的处方。我们的语料库包括治疗乳腺癌的所有患者的自由文本临床社会工作记录，并结合药物处方信息。我们进行了特征重要性分析，以确定影响癌症治疗选择的具体社会情境。仅使用社工记录，我们一直预测靶向治疗的使用，表明由于非临床因素而存在治疗选择的系统差异。

    We aimed to investigate the impact of social circumstances on cancer therapy selection using natural language processing to derive insights from social worker documentation. We developed and employed a Bidirectional Encoder Representations from Transformers (BERT) based approach, using a hierarchical multi-step BERT model (BERT-MS) to predict the prescription of targeted cancer therapy to patients based solely on documentation by clinical social workers. Our corpus included free-text clinical social work notes, combined with medication prescription information, for all patients treated for breast cancer. We conducted a feature importance analysis to pinpoint the specific social circumstances that impact cancer therapy selection. Using only social work notes, we consistently predicted the administration of targeted therapies, suggesting systematic differences in treatment selection exist due to non-clinical factors. The UCSF-BERT model, pretrained on clinical text at UCSF, outperformed 
    
[^10]: 文本到图像扩散模型中的能量交叉注意力用于贝叶斯上下文更新

    Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v1 [cs.CV])

    [http://arxiv.org/abs/2306.09869](http://arxiv.org/abs/2306.09869)

    本论文提出了能量交叉注意力的EBM框架，通过更新和转移上下文向量，隐式最小化能量函数的嵌套层次，优化文本到图像扩散模型的语义对齐问题，实现零样本组合生成。

    

    尽管文本到图像扩散模型在图像生成任务中表现出色，但最近的研究提出了一个问题，即生成的图像有时无法捕捉到文本提示的预期语义内容，这种现象通常被称为语义错位。为了解决这个问题，我们提出了一种新颖的基于能量的模型（EBM）框架。具体而言，我们首先在去噪自编码器的每个交叉注意力层中制定潜在图像表示和文本嵌入的EBM。然后，我们获得上下文向量的对数后验梯度，可以更新和转移到后续的交叉注意力层，从而隐式地最小化嵌套层次的能量函数。我们的潜在EBMs还允许零样本组合生成，即通过不同上下文的交叉注意力输出的线性组合。通过大量实验，我们证明了所提出的方法在处理各种图像生成任务方面非常有效，并可以显著降低文本提示和生成图像之间的语义错位现象。

    Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation 
    
[^11]: 大型语言模型真的是良好的逻辑推理者吗？基于演绎、归纳和阿布达斯观点的全面评估。

    Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views. (arXiv:2306.09841v1 [cs.CL])

    [http://arxiv.org/abs/2306.09841](http://arxiv.org/abs/2306.09841)

    本文评估了大型语言模型的逻辑推理能力，选择了15个典型数据集，考虑了演绎、归纳、阿布达斯和混合推理形式，并选择了三个代表性的LLMs进行零样本、一次和三次的设置下评估。提出精细级别的评估方法。

    

    大型语言模型(LLMs)在各种自然语言任务中取得了巨大成功。对LLMs的具体推理能力进行评估，如多语言推理和数学推理，引起了广泛关注。然而，作为关键推理视角之一，逻辑推理能力还没有得到彻底评估。本文旨在填补这些差距并提供全面的评估。首先，为了进行系统化评估，本文选择了15个典型的逻辑推理数据集，并将它们组织成演绎、归纳、阿布达斯和混合形式的推理设置。考虑评估的全面性，我们选择了三个代表性的LLMs（text-davinci-003，ChatGPT和BARD），并在零样本、一次和三次的设置下对所有选择的数据集进行评估。其次，与以往仅依赖简单指标（如准确性）的评估不同，我们提出了从目标推理角度进行的精细级别评估。

    Large Language Models (LLMs) have achieved great success in various natural language tasks. It has aroused much interest in evaluating the specific reasoning capability of LLMs, such as multilingual reasoning and mathematical reasoning. However, as one of the key reasoning perspectives, logical reasoning capability has not yet been thoroughly evaluated. In this work, we aim to bridge those gaps and provide comprehensive evaluations. Firstly, to offer systematic evaluations, this paper selects fifteen typical logical reasoning datasets and organizes them into deductive, inductive, abductive and mixed-form reasoning settings. Considering the comprehensiveness of evaluations, we include three representative LLMs (i.e., text-davinci-003, ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot, one-shot and three-shot settings. Secondly, different from previous evaluations relying only on simple metrics (e.g., accuracy), we propose fine-level evaluations from objective 
    
[^12]: 谢菲尔德大学提交给AmericasNLP机器翻译土著语言分享任务的论文（arXiv: 2306.09830v1 [cs.CL]）

    Sheffield's Submission to the AmericasNLP Shared Task on Machine Translation into Indigenous Languages. (arXiv:2306.09830v1 [cs.CL])

    [http://arxiv.org/abs/2306.09830](http://arxiv.org/abs/2306.09830)

    本文介绍了谢菲尔德大学的机器翻译方法，成功在AmericasNLP机器翻译土著语言分享任务中取得了最高的平均chrF，其中在Aymara，Guarani和Quechua方面有显着的改进。

    

    本文描述了谢菲尔德大学提交给AmericasNLP 2023机器翻译土著语言分享任务的方法，该任务包括将西班牙语翻译成十一种土著语言。 我们的方法包括扩展，训练和与不同种类的NLLB-200组合。 我们使用组织者提供的数据以及宪法，手册，新闻文章和从单语数据生成的回译等各种其他来源的数据。 在开发集上，我们的最佳成绩在所有语言的平均chrF上比基线提高了11％，尤其是在Aymara，Guarani和Quechua方面有显着的改进。 在测试集上，我们实现了所有提交中最高的平均chrF，我们在11种语言中排名前四位，并且我们的至少一个提交在所有语言中排名前三位。

    In this paper we describe the University of Sheffield's submission to the AmericasNLP 2023 Shared Task on Machine Translation into Indigenous Languages which comprises the translation from Spanish to eleven indigenous languages. Our approach consists of extending, training, and ensembling different variations of NLLB-200. We use data provided by the organizers and data from various other sources such as constitutions, handbooks, news articles, and backtranslations generated from monolingual data. On the dev set, our best submission outperforms the baseline by 11% average chrF across all languages, with substantial improvements particularly for Aymara, Guarani and Quechua. On the test set, we achieve the highest average chrF of all the submissions, we rank first in four of the eleven languages, and at least one of our submissions ranks in the top 3 for all languages.
    
[^13]: 基于流程知识注入的医生友好型解释学习

    Process Knowledge-infused Learning for Clinician-friendly Explanations. (arXiv:2306.09824v1 [cs.CL])

    [http://arxiv.org/abs/2306.09824](http://arxiv.org/abs/2306.09824)

    本论文介绍了一种名为PK-iL的新学习范式，可以在语言模型输出上添加临床流程知识结构，从而使医生能够理解和解释模型的输出，从而为心理卫生护理和预防策略提供支持和帮助。

    

    语言模型有潜力利用社交媒体数据评估心理健康。通过分析在线帖子和交流，这些模型可以检测表明抑郁症、焦虑症或自杀倾向等心理健康情况的模式。它们通过关键词、语言标记和情感来洞察个体的心理健康状况。这些信息对于早期检测、干预和支持至关重要，可以改善心理卫生护理和预防策略。然而，利用语言模型从社交媒体对心理健康进行评估有两个局限性：(1)它们不会将帖子与临床医生的诊断过程进行比较，(2)使用临床医生能够理解的概念（即医生友好型解释）解释语言模型的输出是具有挑战性的。在本研究中，我们介绍了基于流程知识注入的学习（PK-iL），这是一种新的学习范式，可以在语言模型输出上添加临床流程知识结构，使医生能够理解和解释模型的输出。

    Language models have the potential to assess mental health using social media data. By analyzing online posts and conversations, these models can detect patterns indicating mental health conditions like depression, anxiety, or suicidal thoughts. They examine keywords, language markers, and sentiment to gain insights into an individual's mental well-being. This information is crucial for early detection, intervention, and support, improving mental health care and prevention strategies. However, using language models for mental health assessments from social media has two limitations: (1) They do not compare posts against clinicians' diagnostic processes, and (2) It's challenging to explain language model outputs using concepts that the clinician can understand, i.e., clinician-friendly explanations. In this study, we introduce Process Knowledge-infused Learning (PK-iL), a new learning paradigm that layers clinical process knowledge structures on language model outputs, enabling clinicia
    
[^14]: 开发用户反馈的潜力：利用大型语言模型作为用户模拟器以增强对话系统

    Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System. (arXiv:2306.09821v1 [cs.CL])

    [http://arxiv.org/abs/2306.09821](http://arxiv.org/abs/2306.09821)

    该论文提出了一种名为用户引导响应优化（UGRO）的方法，使用大型语言模型作为无注释的用户模拟器，以评估对话响应并优化监督式经过微调的端到端任务导向对话模型。该方法利用了大型语言模型在提供满意度反馈方面的潜力，取得了显著的改进，为利用大型语言模型增强对话系统提供了新的思路。

    

    对话系统和大型语言模型（LLMs）已经引起了人们的极大关注。然而，与较小的任务特定模型相比，直接利用LLMs作为任务导向对话（TOD）模型的性能较差。尽管如此，承认LLMs的重大潜力并探索利用它们的惊人能力的改进方法非常重要。为了利用LLMs，我们提出了一种名为用户引导响应优化（UGRO）的替代方法，将LLM与较小的TOD模型结合起来。该方法使用LLM作为无注释的用户模拟器来评估对话响应，将其与较小的经过微调的端到端TOD模型相结合。通过利用LLMs生成的满意度反馈，UGRO进一步优化了监督式经过微调的TOD模型。具体而言，TOD模型以对话历史记录作为输入，并在用户模拟器反馈的帮助下生成符合用户需求的高满意度响应。实验结果表明，基于UGRO的方法相比现有最先进模型取得了显著的改进，为利用LLMs增强对话系统提供了洞察力。

    Dialogue systems and large language models (LLMs) have gained considerable attention. However, the direct utilization of LLMs as task-oriented dialogue (TOD) models has been found to underperform compared to smaller task-specific models. Nonetheless, it is crucial to acknowledge the significant potential of LLMs and explore improved approaches for leveraging their impressive abilities. Motivated by the goal of leveraging LLMs, we propose an alternative approach called User-Guided Response Optimization (UGRO) to combine it with a smaller TOD model. This approach uses LLM as annotation-free user simulator to assess dialogue responses, combining them with smaller fine-tuned end-to-end TOD models. By utilizing the satisfaction feedback generated by LLMs, UGRO further optimizes the supervised fine-tuned TOD model. Specifically, the TOD model takes the dialogue history as input and, with the assistance of the user simulator's feedback, generates high-satisfaction responses that meet the user
    
[^15]: 探究使用大型语言模型的惊讶度量用于语音合成韵律的实用性

    Investigating the Utility of Surprisal from Large Language Models for Speech Synthesis Prosody. (arXiv:2306.09814v1 [eess.AS])

    [http://arxiv.org/abs/2306.09814](http://arxiv.org/abs/2306.09814)

    本文研究了单词惊讶度量作为一种语音合成韵律特征的应用，发现它与单词突出性有着适度的相关性，但使用惊讶度量作为条件特征无法提高语音合成质量。

    

    本文研究了单词惊讶度量，即在特定情境下一个单词的可预测性，作为一种辅助语音合成韵律的特征的应用。我们探讨了从大型语言模型（LLMs）中提取的单词惊讶度量与单词突出性的相关性，单词突出性是一个对于给定话语中单词显著性的基于信号的度量。我们还研究了上下文长度和LLM大小对结果的影响，以及使用惊讶度值作为条件的语音合成器与基准系统相比的效果。为了评估这些因素，我们使用了大型英语文本语料库和大小不同的LLM进行了实验。结果表明，单词惊讶度量和单词突出性有着适度的相关性，表明它们捕捉到了相关但不同的语言使用方面。我们发现上下文长度和LLM大小会影响相关性，但不是预期方向，长上下文和大的LLMs通常以几乎线性的方式低估突出的单词。我们没有发现使用惊讶度量作为条件特征可以比基准系统提高语音合成质量，但这项研究提供了有用的洞察力和惊讶度量作为语言特征用于语音合成韵律的局限性。

    This paper investigates the use of word surprisal, a measure of the predictability of a word in a given context, as a feature to aid speech synthesis prosody. We explore how word surprisal extracted from large language models (LLMs) correlates with word prominence, a signal-based measure of the salience of a word in a given discourse. We also examine how context length and LLM size affect the results, and how a speech synthesizer conditioned with surprisal values compares with a baseline system. To evaluate these factors, we conducted experiments using a large corpus of English text and LLMs of varying sizes. Our results show that word surprisal and word prominence are moderately correlated, suggesting that they capture related but distinct aspects of language use. We find that length of context and size of the LLM impact the correlations, but not in the direction anticipated, with longer contexts and larger LLMs generally underpredicting prominent words in a nearly linear manner. We d
    
[^16]: RED$^{\rm FM}$：一个经过滤波和多语言处理的关系抽取数据集

    RED$^{\rm FM}$: a Filtered and Multilingual Relation Extraction Dataset. (arXiv:2306.09802v1 [cs.CL])

    [http://arxiv.org/abs/2306.09802](http://arxiv.org/abs/2306.09802)

    本文提出了两个新的数据集，分别是自动标注的SRED$^{\rm FM}$和人工修订的RED$^{\rm FM}$。SRED$^{\rm FM}$涵盖了18种语言、400种关系类型、13种实体类型，总共超过4000万个三元组实例；RED$^{\rm FM}$是RED的精简版，可用于评估多语言关系抽取系统。实验证明，这些新数据集能有效用于建立多语言关系抽取模型。

    

    关系抽取旨在识别文本中实体之间的关系，从而获取关系事实，弥合自然语言和结构化知识之间的差距。然而，当前的关系抽取模型往往依赖于小型数据集，对于非英语语言的关系类型覆盖率也较低。本文提出了两个新的资源，可用于培训和评估多语言关系抽取系统。其一，我们提供了一个自动标注的数据集——SRED$^{\rm FM}$，涵盖了18种语言、400种关系类型、13种实体类型，总共超过4000万个三元组实例。其二，我们提出了一个经人工修订的、针对七种语言的数据集——RED$^{\rm FM}$，可用于多语言关系抽取系统的评估。为了展示这些新数据集的实用性，我们使用第一个端到端的多语言关系抽取模型mREBEL进行实验，包括实体类型在内的三元组被抽取出来。

    Relation Extraction (RE) is a task that identifies relationships between entities in a text, enabling the acquisition of relational facts and bridging the gap between natural language and structured knowledge. However, current RE models often rely on small datasets with low coverage of relation types, particularly when working with languages other than English. In this paper, we address the above issue and provide two new resources that enable the training and evaluation of multilingual RE systems. First, we present SRED$^{\rm FM}$, an automatically annotated dataset covering 18 languages, 400 relation types, 13 entity types, totaling more than 40 million triplet instances. Second, we propose RED$^{\rm FM}$, a smaller, human-revised dataset for seven languages that allows for the evaluation of multilingual RE systems. To demonstrate the utility of these novel datasets, we experiment with the first end-to-end multilingual RE model, mREBEL, that extracts triplets, including entity types,
    
[^17]: 低资源情况下大型语言模型全参数微调方法研究

    Full Parameter Fine-tuning for Large Language Models with Limited Resources. (arXiv:2306.09782v1 [cs.CL])

    [http://arxiv.org/abs/2306.09782](http://arxiv.org/abs/2306.09782)

    本文提出了一种低内存使用的优化器LOMO，可以实现在有限资源下对大型语言模型进行全参数微调，从而降低研究门槛。

    

    大型语言模型（LLMs）在自然语言处理（NLP）领域具有革命性的影响，但需要大量GPU资源进行训练，从而造成研究门槛高。现有方法主要关注参数有效微调，即微调或添加少量参数，但很少有关注在有限资源情况下全参数微调的挑战。本文提出了一种新的优化器LOw-Memory Optimization（LOMO）, 通过将梯度计算和参数更新一步融合以减少内存使用。通过将LOMO与现有的内存节省技术相结合，我们将内存使用量降低到DeepSpeed方案的10.8％。因此，我们的方法使65B模型的全参数微调在只需单台机器上执行，该机器搭载8个RTX 3090，每个显存为24GB。

    Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 RTX 3090, each with 24GB memory.
    
[^18]: 礼貌刻板印象和攻击向量：日韩语言模型中的性别刻板印象

    Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models. (arXiv:2306.09752v1 [cs.CL])

    [http://arxiv.org/abs/2306.09752](http://arxiv.org/abs/2306.09752)

    中文总结该论文主要研究了日语和韩语语言模型中与礼貌水平相关的语法性别偏见，发现礼貌水平是网络欺凌检测模型中的攻击向量。

    

    为了跟上大型语言模型的快速发展和使用，性别偏见研究在自然语言处理中变得越来越普遍。然而，非英语偏见研究还处于起步阶段，大多数工作都集中在英语上。在我们的工作中，我们研究了与礼貌水平相关的语法性别偏见在日语和韩语语言模型中的表现。这些语言的语言学研究已经确定了性别偏见和礼貌水平之间的联系，但尚不清楚语言模型是否会复制这些偏见。我们通过模板分析男性和女性语法性别的相对预测概率，并发现非正式礼貌语言最能表现出女性语法性别，而粗鲁和正式语言最能表现出男性语法性别。此外，我们发现礼貌水平是网络欺凌检测模型中的一种攻击向量，可以通过简单的技巧回避检测。

    In efforts to keep up with the rapid progress and use of large language models, gender bias research is becoming more prevalent in NLP. Non-English bias research, however, is still in its infancy with most work focusing on English. In our work, we study how grammatical gender bias relating to politeness levels manifests in Japanese and Korean language models. Linguistic studies in these languages have identified a connection between gender bias and politeness levels, however it is not yet known if language models reproduce these biases. We analyze relative prediction probabilities of the male and female grammatical genders using templates and find that informal polite speech is most indicative of the female grammatical gender, while rude and formal speech is most indicative of the male grammatical gender. Further, we find politeness levels to be an attack vector for allocational gender bias in cyberbullying detection models. Cyberbullies can evade detection through simple techniques ab
    
[^19]: 使用自然语言处理和网络自动化结构化文献综述：以农民应对气候变化为例

    Using Natural Language Processing and Networks to Automate Structured Literature Reviews: An Application to Farmers Climate Change Adaptation. (arXiv:2306.09737v1 [cs.CL])

    [http://arxiv.org/abs/2306.09737](http://arxiv.org/abs/2306.09737)

    本文利用自然语言处理和网络，自动化结构化文献综述，以农民应对气候变化的分析为例，提取变量关系并使用网络综合其发现。

    

    随着研究文章数量的增加，学者们很难跟上与自己专业领域相关的新发现。此外，对于需要跨学科解决方案的复杂主题，如气候变化，跨学科研究之间的知识链接也变得具有挑战性。同时，黑匣子类型的文本摘要的兴起使得理解文本关系的建立变得困难，更不用说与已有理论概念化因果关系并进行假设的相关性了。本文旨在合理地利用自然语言处理，通过提取变量关系并使用网络综合其发现，同时与相关学科中占主导地位的关键概念相关联。我们以农民应对气候变化的分析为例。为此，我们对Scopus于2022年8月返回的出版物进行自然语言处理分析。结果展示...

    The fast-growing number of research articles makes it problematic for scholars to keep track of the new findings related to their areas of expertise. Furthermore, linking knowledge across disciplines in rapidly developing fields becomes challenging for complex topics like climate change that demand interdisciplinary solutions. At the same time, the rise of Black Box types of text summarization makes it difficult to understand how text relationships are built, let alone relate to existing theories conceptualizing cause-effect relationships and permitting hypothesizing. This work aims to sensibly use Natural Language Processing by extracting variables relations and synthesizing their findings using networks while relating to key concepts dominant in relevant disciplines. As an example, we apply our methodology to the analysis of farmers' adaptation to climate change. For this, we perform a Natural Language Processing analysis of publications returned by Scopus in August 2022. Results sho
    
[^20]: 汉语话语表征结构分析

    Discourse Representation Structure Parsing for Chinese. (arXiv:2306.09725v1 [cs.CL])

    [http://arxiv.org/abs/2306.09725](http://arxiv.org/abs/2306.09725)

    该论文探索了在缺乏中文标签数据的情况下汉语语义分析的可行性，并提出了一个专为汉语语义分析设计的测试套件。副词是汉语语义分析的主要难点。

    

    先前的研究主要关注单语英文语义分析。相反地，我们探索了在缺乏中文含义表示标签数据的情况下进行汉语语义分析的可行性。我们描述了线性化汉语含义表示数据用于序列到序列神经网络的自动收集的流程。我们进一步提出了一个专门为汉语语义分析设计的测试套件，为解析性能提供细粒度评估，旨在研究汉语解析的难点。我们的实验结果表明，汉语语义分析的难点主要是由副词引起的。通过机器翻译和英文解析器实现汉语解析的性能略低于直接在汉语数据上训练模型。

    Previous work has predominantly focused on monolingual English semantic parsing. We, instead, explore the feasibility of Chinese semantic parsing in the absence of labeled data for Chinese meaning representations. We describe the pipeline of automatically collecting the linearized Chinese meaning representation data for sequential-to sequential neural networks. We further propose a test suite designed explicitly for Chinese semantic parsing, which provides fine-grained evaluation for parsing performance, where we aim to study Chinese parsing difficulties. Our experimental results show that the difficulty of Chinese semantic parsing is mainly caused by adverbs. Realizing Chinese parsing through machine translation and an English parser yields slightly lower performance than training a model directly on Chinese data.
    
[^21]: 推动 ChatGPT 在自然语言处理任务上的极限

    Pushing the Limits of ChatGPT on NLP Tasks. (arXiv:2306.09719v1 [cs.CL])

    [http://arxiv.org/abs/2306.09719](http://arxiv.org/abs/2306.09719)

    本研究提出了一系列通用模块以解决 ChatGPT 在自然语言处理任务中的弱点，包括利用多个提示符来适应更多演示、使用精细调整模型以获得更好的演示检索、转换任务为更适合生成性质的格式以及采用针对 NLP 任务设计的推理策略。

    

    尽管 ChatGPT 取得了成功，但在大多数自然语言处理任务上，其表现仍远低于基线模型。本研究探究了其中的原因，发现其表现欠佳的原因主要有：（1）提示符中的令牌限制不允许充分利用监督数据集；（2）ChatGPT 生成性质与 NLP 任务之间存在不匹配；（3）基于语言模型的固有弱点，如产生幻觉、过度关注特定关键词等。本研究提出了一系列通用模块以解决这些问题，旨在推动 ChatGPT 在 NLP 任务上的极限。我们提出的模块包括：（1）一种输入多提示的策略，使用多个提示符来适应更多演示；（2）使用精细调整模型以获得更好的演示检索；（3）将任务转换为更适合生成性质的格式；（4）采用针对 NLP 任务设计的推理策略。

    Despite the success of ChatGPT, its performances on most NLP tasks are still well below the supervised baselines. In this work, we looked into the causes, and discovered that its subpar performance was caused by the following factors: (1) token limit in the prompt does not allow for the full utilization of the supervised datasets; (2) mismatch between the generation nature of ChatGPT and NLP tasks; (3) intrinsic pitfalls of LLMs models, e.g., hallucination, overly focus on certain keywords, etc.  In this work, we propose a collection of general modules to address these issues, in an attempt to push the limits of ChatGPT on NLP tasks. Our proposed modules include (1) a one-input-multiple-prompts strategy that employs multiple prompts for one input to accommodate more demonstrations; (2) using fine-tuned models for better demonstration retrieval; (3) transforming tasks to formats that are more tailored to the generation nature; (4) employing reasoning strategies that are tailored to addr
    
[^22]: 半离线强化学习用于优化文本生成

    Semi-Offline Reinforcement Learning for Optimized Text Generation. (arXiv:2306.09712v1 [cs.LG])

    [http://arxiv.org/abs/2306.09712](http://arxiv.org/abs/2306.09712)

    该研究提出了一种半离线强化学习范式，该范式平衡了探索能力和培训成本，提供了一个理论基础来比较不同的强化学习设置，并在优化成本、渐近误差和过度拟合误差界方面实现了最优的RL设置。实验结果表明，该方法高效且性能优异。

    

    在强化学习中，与环境交互有两种主要方式：在线和离线。在线方法探索环境所需时间较长，而离线方法通过牺牲探索能力有效地获得奖励信号。我们提出了半离线RL，一种新的范式，可以平滑地从离线转换到在线设置，平衡探索能力和培训成本，并为比较不同RL设置提供理论基础。基于半离线公式，我们提出了在优化成本、渐近误差和过度拟合误差界方面最优的RL设置。广泛的实验表明，我们的半离线方法效率高，与最先进的方法相比具有可比性或更好的性能。

    In reinforcement learning (RL), there are two major settings for interacting with the environment: online and offline. Online methods explore the environment at significant time cost, and offline methods efficiently obtain reward signals by sacrificing exploration capability. We propose semi-offline RL, a novel paradigm that smoothly transits from offline to online settings, balances exploration capability and training cost, and provides a theoretical foundation for comparing different RL settings. Based on the semi-offline formulation, we present the RL setting that is optimal in terms of optimization cost, asymptotic error, and overfitting error bound. Extensive experiments show that our semi-offline approach is efficient and yields comparable or often better performance compared with state-of-the-art methods.
    
[^23]: 降低情感分析中的计算成本：张量循环网络与循环网络的比较

    Reducing Computational Costs in Sentiment Analysis: Tensorized Recurrent Networks vs. Recurrent Networks. (arXiv:2306.09705v1 [cs.LG])

    [http://arxiv.org/abs/2306.09705](http://arxiv.org/abs/2306.09705)

    张量循环网络是一种降低情感分析计算成本的潜在解决方案

    

    预测观众对某一文本的反应对于政治、研究和商业行业等多个方面至关重要。情感分析（SA）是一种有用的自然语言处理（NLP）技术，利用词汇/统计和深度学习方法来确定不同大小的文本是否表现出积极、消极或中性的情感。循环网络在机器学习社区中广泛用于处理序列数据问题。然而，基于长短期记忆网络和门控循环单元的模型的一个缺点是参数数量显著高，因此这些模型的计算成本很高。当可用数据有限时，这种缺点甚至更为显著。此外，这些模型需要显着的过度参数化和正则化才能达到最佳性能。张量化模型代表了一个潜在的解决方案。本文将对一些社交媒体帖子进行情感分类。

    Anticipating audience reaction towards a certain text is integral to several facets of society ranging from politics, research, and commercial industries. Sentiment analysis (SA) is a useful natural language processing (NLP) technique that utilizes lexical/statistical and deep learning methods to determine whether different-sized texts exhibit positive, negative, or neutral emotions. Recurrent networks are widely used in machine-learning communities for problems with sequential data. However, a drawback of models based on Long-Short Term Memory networks and Gated Recurrent Units is the significantly high number of parameters, and thus, such models are computationally expensive. This drawback is even more significant when the available data are limited. Also, such models require significant over-parameterization and regularization to achieve optimal performance. Tensorized models represent a potential solution. In this paper, we classify the sentiment of some social media posts. We comp
    
[^24]: 跨语料阅读性兼容性评估：英文文本

    Cross-corpus Readability Compatibility Assessment for English Texts. (arXiv:2306.09704v1 [cs.CL])

    [http://arxiv.org/abs/2306.09704](http://arxiv.org/abs/2306.09704)

    本文提出了一个新的评估框架，Cross-corpus text Readability Compatibility Assessment (CRCA)，用于解决不同语料库之间的可读性兼容性的问题。研究结果表明该框架具有显著的兼容性，并适用于不同的特征表示和分类方法。

    

    文本的可读性评估在各个领域的研究人员中受到了重视。然而，对语料库兼容性的缺乏探索构成了一项挑战，因为不同的研究小组使用不同的语料库。本研究提出了一个新的评估框架，Cross-corpus text Readability Compatibility Assessment (CRCA)，来解决这个问题。该框架包括三个关键组成部分：(1) 语料库：CEFR，CLEC，CLOTH，NES，OSP和RACE。提取了语言特征、GloVe词向量表示和它们的融合特征。(2) 分类模型：采用了机器学习方法（XGBoost，SVM）和深度学习方法（BiLSTM，Attention-BiLSTM）。(3) 兼容性指标：RJSD，RRNSS和NDCG指标。我们的研究结果表明：(1) OSP表现显著不同于其他数据集的证实了语料兼容性。(2) 兼容性、特征表示和分类方法之间有适应性效应。(3) 在不同兼容性指标下得到了一致的评估结果，这表明了我们的框架的效果。

    Text readability assessment has gained significant attention from researchers in various domains. However, the lack of exploration into corpus compatibility poses a challenge as different research groups utilize different corpora. In this study, we propose a novel evaluation framework, Cross-corpus text Readability Compatibility Assessment (CRCA), to address this issue. The framework encompasses three key components: (1) Corpus: CEFR, CLEC, CLOTH, NES, OSP, and RACE. Linguistic features, GloVe word vector representations, and their fusion features were extracted. (2) Classification models: Machine learning methods (XGBoost, SVM) and deep learning methods (BiLSTM, Attention-BiLSTM) were employed. (3) Compatibility metrics: RJSD, RRNSS, and NDCG metrics. Our findings revealed: (1) Validated corpus compatibility, with OSP standing out as significantly different from other datasets. (2) An adaptation effect among corpora, feature representations, and classification methods. (3) Consistent 
    
[^25]: 基于类适应的自训练模型用于不完全标注的关系提取

    Class-Adaptive Self-Training for Relation Extraction with Incompletely Annotated Training Data. (arXiv:2306.09697v1 [cs.CL])

    [http://arxiv.org/abs/2306.09697](http://arxiv.org/abs/2306.09697)

    本研究提出了一种新的基于类别自适应重新采样自训练框架，能够显著改善在少数类别中的总体召回率，而不会明显降低准确率，从而实现了关系提取的最先进性能。

    

    关系提取旨在从句子和文档中提取关系。现有的关系提取模型通常依赖于监督机器学习。然而，最近的研究表明许多关系提取数据集是不完全注释的。这被称为错误否定问题，即将有效的关系错误地注释为“无关系”。使用这种数据训练的模型在推理阶段不可避免地会犯类似的错误。自训练已被证明在缓解错误否定问题方面非常有效。然而，传统的自训练容易受到确认偏差影响，并且在少数类中表现不佳。为了克服这个限制，我们提出了一种新的基于类别自适应重新采样自训练框架。具体来说，我们通过准确率和召回率分数重新对每个类别的伪标签进行了重新采样。我们的重新采样策略利于高准确率和低召回率的类别的伪标签，从而提高了总体召回率，而不会显着降低准确率。对广泛使用的数据集进行的实验结果表明，我们的方法在F1得分方面实现了最先进的性能，特别是在少数类中。

    Relation extraction (RE) aims to extract relations from sentences and documents. Existing relation extraction models typically rely on supervised machine learning. However, recent studies showed that many RE datasets are incompletely annotated. This is known as the false negative problem in which valid relations are falsely annotated as 'no_relation'. Models trained with such data inevitably make similar mistakes during the inference stage. Self-training has been proven effective in alleviating the false negative problem. However, traditional self-training is vulnerable to confirmation bias and exhibits poor performance in minority classes. To overcome this limitation, we proposed a novel class-adaptive re-sampling self-training framework. Specifically, we re-sampled the pseudo-labels for each class by precision and recall scores. Our re-sampling strategy favored the pseudo-labels of classes with high precision and low recall, which improved the overall recall without significantly com
    
[^26]: Online Distillation for Pseudo-Relevance Feedback（伪相关反馈的在线蒸馏技术）

    Online Distillation for Pseudo-Relevance Feedback. (arXiv:2306.09657v1 [cs.IR])

    [http://arxiv.org/abs/2306.09657](http://arxiv.org/abs/2306.09657)

    本文提出了一种伪相关反馈的在线蒸馏技术，通过在线逐步建立模型，预测查询与文档的相关得分，并在索引中高效执行，以优化整体检索效果。

    

    模型蒸馏是一种提高神经搜索模型效果的重要方法。当前，传统的蒸馏方法采用离线学习的方式，即训练一个新的神经模型预测任意查询与文档之间的相关得分。本文提出一种新的蒸馏方法，通过在线蒸馏逐步建立模型，以此来预测某一查询与文档之间的相关得分，并在索引中执行出色。该技术不仅可以扩大重新排序的文档数量，还能识别在第一阶段检索中被忽略的文档，以优化整体检索效果。

    Model distillation has emerged as a prominent technique to improve neural search models. To date, distillation taken an offline approach, wherein a new neural model is trained to predict relevance scores between arbitrary queries and documents. In this paper, we explore a departure from this offline distillation strategy by investigating whether a model for a specific query can be effectively distilled from neural re-ranking results (i.e., distilling in an online setting). Indeed, we find that a lexical model distilled online can reasonably replicate the re-ranking of a neural model. More importantly, these models can be used as queries that execute efficiently on indexes. This second retrieval stage can enrich the pool of documents for re-ranking by identifying documents that were missed in the first retrieval stage. Empirically, we show that this approach performs favourably when compared with established pseudo relevance feedback techniques, dense retrieval methods, and sparse-dense
    
[^27]: ReactGenie：使用大型语言模型的面向对象状态抽象来支持复杂的多模态交互

    ReactGenie: An Object-Oriented State Abstraction for Complex Multimodal Interactions Using Large Language Models. (arXiv:2306.09649v1 [cs.HC])

    [http://arxiv.org/abs/2306.09649](http://arxiv.org/abs/2306.09649)

    ReactGenie是一个支持构建复杂的多模态移动应用程序的编程框架，通过使用共享的面向对象状态抽象，使得不同模态可以无缝集成和组合，从而实现了多模态交互的支持。

    

    多模态交互已被证明比传统的图形界面更加灵活、高效和适应各种用户和任务。然而，现有的多模态开发框架要么不能很好地处理多模态命令的复杂性和组合性，要么需要开发人员编写大量代码来支持这些多模态交互。本文提出了ReactGenie，这是一个编程框架，使用共享的面向对象状态抽象来支持构建复杂的多模态移动应用程序。具有不同模态的共享状态抽象使得使用ReactGenie的开发人员能够无缝地集成和组合这些模态以实现多模态交互。ReactGenie是构建图形应用程序的现有工作流程的自然扩展，就像使用React-Redux一样。开发人员只需要添加一些注释和示例来指示自然语言如何映射到用户可访问的状态即可。

    Multimodal interactions have been shown to be more flexible, efficient, and adaptable for diverse users and tasks than traditional graphical interfaces. However, existing multimodal development frameworks either do not handle the complexity and compositionality of multimodal commands well or require developers to write a substantial amount of code to support these multimodal interactions. In this paper, we present ReactGenie, a programming framework that uses a shared object-oriented state abstraction to support building complex multimodal mobile applications. Having different modalities share the same state abstraction allows developers using ReactGenie to seamlessly integrate and compose these modalities to deliver multimodal interaction.  ReactGenie is a natural extension to the existing workflow of building a graphical app, like the workflow with React-Redux. Developers only have to add a few annotations and examples to indicate how natural language is mapped to the user-accessible
    
[^28]: 跨域毒性片段检测

    Cross-Domain Toxic Spans Detection. (arXiv:2306.09642v1 [cs.CL])

    [http://arxiv.org/abs/2306.09642](http://arxiv.org/abs/2306.09642)

    本研究评估了三种跨域条件下检测毒性片段的方法，结果表明使用现成的词典的简单方法在跨域设置中表现最佳，而用于领域内的语言模型容易出现某些类型的假阳性。

    

    鉴于毒性语言的动态性，自动检测毒性片段的方法可能会遭遇分布转移。为了探索这种现象，我们评估了三种检测跨域条件下毒性片段的方法：基于词典、根因抽取和微调语言模型。我们的研究发现，使用现成的词典的简单方法在跨域设置中表现最佳。跨领域误差分析表明：（1）根因提取方法容易出现假负结果，而（2）语言模型尽管在领域内的情况下表现最佳，但其明确提取毒性单词的数量比词典少，并且容易出现某些类型的假阳性。我们的代码公开在https://github.com/sfschouten/toxic-cross-domain 。

    Given the dynamic nature of toxic language use, automated methods for detecting toxic spans are likely to encounter distributional shift. To explore this phenomenon, we evaluate three approaches for detecting toxic spans under cross-domain conditions: lexicon-based, rationale extraction, and fine-tuned language models. Our findings indicate that a simple method using off-the-shelf lexicons performs best in the cross-domain setup. The cross-domain error analysis suggests that (1) rationale extraction methods are prone to false negatives, while (2) language models, despite performing best for the in-domain case, recall fewer explicitly toxic words than lexicons and are prone to certain types of false positives. Our code is publicly available at: https://github.com/sfschouten/toxic-cross-domain.
    
[^29]: AUGUST：用于合成对话式推荐数据集的自动生成工具

    AUGUST: an Automatic Generation Understudy for Synthesizing Conversational Recommendation Datasets. (arXiv:2306.09631v1 [cs.CL])

    [http://arxiv.org/abs/2306.09631](http://arxiv.org/abs/2306.09631)

    本文提出了一种新型自动数据集合成方法，利用传统推荐系统中的个性化用户画像和结构化图表中的丰富交互信息，通过数据到文本生成过程生成大规模和高质量的推荐对话。

    

    高质量数据对于对话式推荐系统至关重要，是网络架构开发和培训策略设计的基石。现有的方法需要耗费大量人力来手动标记或设计和扩展推荐对话模板。然而，它们存在的问题是：（i）有限的人工标注者数量导致数据集无法捕捉到现实世界中的丰富和大规模的案例，（ii）标注者的有限经验和知识导致不够信息丰富和不合适的推荐。在本文中，我们提出了一种新颖的自动数据集合成方法，可以通过数据到文本生成过程生成大规模和高质量的推荐对话。在这个过程中，我们全面利用了：（i）传统推荐系统中的丰富的个性化用户画像，（ii）结构化图表中的丰富交互信息。

    High-quality data is essential for conversational recommendation systems and serves as the cornerstone of the network architecture development and training strategy design. Existing works contribute heavy human efforts to manually labeling or designing and extending recommender dialogue templates. However, they suffer from (i) the limited number of human annotators results in that datasets can hardly capture rich and large-scale cases in the real world, (ii) the limited experience and knowledge of annotators account for the uninformative corpus and inappropriate recommendations. In this paper, we propose a novel automatic dataset synthesis approach that can generate both large-scale and high-quality recommendation dialogues through a data2text generation process, where unstructured recommendation conversations are generated from structured graphs based on user-item information from the real world. In doing so, we comprehensively exploit: (i) rich personalized user profiles from traditi
    
[^30]: 使用CLIPScores作为隐式参考链的PhotoBook指称游戏的听者模型

    Listener Model for the PhotoBook Referential Game with CLIPScores as Implicit Reference Chain. (arXiv:2306.09607v1 [cs.CL])

    [http://arxiv.org/abs/2306.09607](http://arxiv.org/abs/2306.09607)

    本文介绍了使用CLIPScores作为隐式参考链的PhotoBook指称游戏的听者模型，该模型直接解决了决定图像是否与伙伴共享的预测任务，并在未见过的图像集/游戏主题上取得了>77%的准确率，优于基线>17个百分点。

    

    PhotoBook是一种协作对话游戏，其中两个玩家收到私有的、部分重叠的图像集，并解决它们共有哪些图像。它向机器提出了一个巨大的挑战，即学习如何建立共同基础以有效地沟通。然而，文献中开发的方法不能用于真实的游戏，因为它们只解决了游戏的某些子任务，而且它们需要额外的参考链输入，其提取过程是不完美的。因此，我们提出了一个无参考链的听者模型，直接解决游戏的预测任务，即决定一个图像是否与伙伴共享。我们的DeBERTa-based听者模型读取完整的对话，并利用CLIPScore特征评估话语-图像的相关性。我们在未见过的图像集/游戏主题上实现了>77%的准确率，优于基线>17个百分点。

    PhotoBook is a collaborative dialogue game where two players receive private, partially-overlapping sets of images and resolve which images they have in common. It presents machines with a great challenge to learn how people build common ground around multimodal context to communicate effectively. Methods developed in the literature, however, cannot be deployed to real gameplay since they only tackle some subtasks of the game, and they require additional reference chains inputs, whose extraction process is imperfect. Therefore, we propose a reference chain-free listener model that directly addresses the game's predictive task, i.e., deciding whether an image is shared with partner. Our DeBERTa-based listener model reads the full dialogue, and utilizes CLIPScore features to assess utterance-image relevance. We achieve >77% accuracy on unseen sets of images/game themes, outperforming baseline by >17 points.
    
[^31]: 基于大型语言模型的点击诱骗检测

    Clickbait Detection via Large Language Models. (arXiv:2306.09597v1 [cs.CL])

    [http://arxiv.org/abs/2306.09597](http://arxiv.org/abs/2306.09597)

    本文研究了大型语言模型在点击诱骗检测上的性能，结果表明LLM无法取得最佳结果且不能仅通过标题实现满意的检测。

    

    点击诱骗（Clickbait）会通过一些令人惊讶甚至引人入胜的标题来诱导用户进行点击，几乎渗透到所有在线内容发布者，如新闻门户和社交媒体。最近，大型语言模型 (LLM)已成为一种强大的工具，并在一系列NLP下游任务中取得了巨大成功。但是，LLM是否可以作为高质量的点击诱骗检测系统还不为人所知。本文分析了LLM在多个英文和中文基准数据集的少样本场景下的性能。实验结果表明，与最先进的深度和微调PLM方法相比，LLM无法达到最佳结果。与人类直觉不同，实验表明LLM不能仅通过标题实现满意的点击诱骗检测。

    Clickbait, which aims to induce users with some surprising and even thrilling headlines for increasing click-through rates, permeates almost all online content publishers, such as news portals and social media. Recently, Large Language Models (LLMs) have emerged as a powerful instrument and achieved tremendous success in a serious of NLP downstream tasks. However, it is not yet known whether LLMs can be served as a high-quality clickbait detection system. In this paper, we analyze the performance of LLMs in the few-shot scenarios on a number of English and Chinese benchmark datasets. Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods. Different from the human intuition, the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines.
    
[^32]: 基于条件MLM对比学习的句子嵌入技术CMLM-CSE

    CMLM-CSE: Based on Conditional MLM Contrastive Learning for Sentence Embeddings. (arXiv:2306.09594v1 [cs.CL])

    [http://arxiv.org/abs/2306.09594](http://arxiv.org/abs/2306.09594)

    本论文提出了一种基于条件MLM的无监督对比学习框架CMLM-CSE，强制句子嵌入学习更多的掩码词信息，可以在文本相似度任务中超越SimCSE。

    

    传统的比较学习句子嵌入技术直接使用编码器提取句子特征，然后通过比较损失函数进行学习。然而，这种方法过于关注句子主体，而忽略了句子中一些词对句子语义的影响。为此，我们提出了CMLM-CSE，这是一种基于条件MLM的无监督对比学习框架。在传统对比学习的基础上，增加一个附加的网络来集成句子嵌入以执行MLM任务，强制句子嵌入学习更多的掩码词信息。最后，当使用Bertbase作为预训练语言模型时，我们在文本相似度任务中比SimCSE高0.55个百分点，在使用Robertabase作为预训练语言模型时，在文本相似度任务中平均超过SimCSE 0.3个百分点。

    Traditional comparative learning sentence embedding directly uses the encoder to extract sentence features, and then passes in the comparative loss function for learning. However, this method pays too much attention to the sentence body and ignores the influence of some words in the sentence on the sentence semantics. To this end, we propose CMLM-CSE, an unsupervised contrastive learning framework based on conditional MLM. On the basis of traditional contrastive learning, an additional auxiliary network is added to integrate sentence embedding to perform MLM tasks, forcing sentence embedding to learn more masked word information. Finally, when Bertbase was used as the pretraining language model, we exceeded SimCSE by 0.55 percentage points on average in textual similarity tasks, and when Robertabase was used as the pretraining language model, we exceeded SimCSE by 0.3 percentage points on average in textual similarity tasks.
    
[^33]: 不同的分词器在连续书写文字中的下游任务中的表现如何？以日语为例研究。

    How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese. (arXiv:2306.09572v1 [cs.CL])

    [http://arxiv.org/abs/2306.09572](http://arxiv.org/abs/2306.09572)

    本文研究了在日语这种连续书写文字语言中，不同的分词器对预训练语言模型在下游任务中的影响，发现每个下游任务都有一个最佳的形态学分析器，并且无论任务类型如何，最好使用字节对编码或Unigram作为子词分词器。

    

    本文研究了在连续书写文字语言中，分词器对预训练语言模型（PLMs）在下游任务中的影响，以日语为案例研究。这种语言的分词器通常由形态学分析器和子词分词器组成，需要我们对所有可能的组合进行综合研究。然而，以前的研究缺乏这种全面性。因此，我们训练了大量的分词器集，并使用每个集合构建了一个PLM，并在广泛的任务范围内测量了下游性能。我们的结果表明，每个下游任务都有一个不同的最佳形态学分析器，并且无论任务类型如何，最好使用字节对编码或Unigram而不是WordPiece作为子词分词器。

    This paper investigates the effect of tokenizers on the downstream performance of pretrained language models (PLMs) in scriptio continua languages where no explicit spaces exist between words, using Japanese as a case study. The tokenizer for such languages often consists of a morphological analyzer and a subword tokenizer, requiring us to conduct a comprehensive study of all possible pairs. However, previous studies lack this comprehensiveness. We therefore train extensive sets of tokenizers, build a PLM using each, and measure the downstream performance on a wide range of tasks. Our results demonstrate that each downstream task has a different optimal morphological analyzer, and that it is better to use Byte-Pair-Encoding or Unigram rather than WordPiece as a subword tokenizer, regardless of the type of task.
    
[^34]: 自然语言处理中的可重复性：从清单中学到了什么？

    Reproducibility in NLP: What Have We Learned from the Checklist?. (arXiv:2306.09562v1 [cs.CL])

    [http://arxiv.org/abs/2306.09562](http://arxiv.org/abs/2306.09562)

    本论文讨论了NLP科学进展的可重复性，介绍了NLP可重复性清单的情况和通过对此清单10,405个匿名回答的分析发现某些项回答“是”的提交数量增加、随之接受率增加以及开源代码与可重复性得分提高之间的关系等。

    

    NLP的科学进展建立在研究人员声明的可重复性基础之上。*CL会议在2020年创建了NLP可重复性清单，要求作者在提交时完成，以提醒他们包括关键信息。我们通过检查10,405个匿名回应，提供了该清单的第一次分析。首先，我们发现在清单引入后，有关效率、验证性能、摘要统计和超参数的信息报告有所增加。此外，我们展示了接受率随着越来越多的“是”回答的提交增长。我们发现，收集新数据的44%的提交比没有收集新数据的提交少5%的被接受率；这些提交的平均评审可重复性与其他提交相比也低了2%。我们发现只有46%的提交声明开源他们的代码，尽管采用此项的提交与未采用此项的提交相比，可重复性得分高8％，是任何条目中最高的。

    Scientific progress in NLP rests on the reproducibility of researchers' claims. The *CL conferences created the NLP Reproducibility Checklist in 2020 to be completed by authors at submission to remind them of key information to include. We provide the first analysis of the Checklist by examining 10,405 anonymous responses to it. First, we find evidence of an increase in reporting of information on efficiency, validation performance, summary statistics, and hyperparameters after the Checklist's introduction. Further, we show acceptance rate grows for submissions with more Yes responses. We find that the 44% of submissions that gather new data are 5% less likely to be accepted than those that did not; the average reviewer-rated reproducibility of these submissions is also 2% lower relative to the rest. We find that only 46% of submissions claim to open-source their code, though submissions that do have 8% higher reproducibility score relative to those that do not, the most for any item. 
    
[^35]: 复杂任务的构建模块：领域转移下的放射学报告的生成性事件提取的鲁棒性研究。

    Building blocks for complex tasks: Robust generative event extraction for radiology reports under domain shifts. (arXiv:2306.09544v1 [cs.CL])

    [http://arxiv.org/abs/2306.09544](http://arxiv.org/abs/2306.09544)

    本文介绍了一个在放射学报告中提取信息的鲁棒性生成式模型，具有良好的泛化性。通过将复杂任务分解为更小的子任务块，与多任务训练相结合，可以提高单遍模型的性能，并通过利用目标域上下文来增强领域适应性，实现使用更小的模型。

    

    本文探讨了从放射学报告中提取信息的方法，通过这些方法可以在考试模式之间实现泛化，从而减少了注释数据的要求。我们证明了多次T5基于文本的生成模型比使用基于BERT的任务特定分类层的方法在考试方式之间表现更上佳。然后，我们开发了一种减少模型推理成本的方法，使得临床应用的大规模语料库处理变得更加可行。具体来说，我们引入一种生成技术，将复杂任务分解为更小的子任务块，当与多任务训练相结合时，可以提高单遍模型的性能。此外，在推理期间，我们利用目标域上下文来增强领域适应性，从而实现使用更小的模型。分析提供了有关不同成本降低策略的见解。

    This paper explores methods for extracting information from radiology reports that generalize across exam modalities to reduce requirements for annotated data. We demonstrate that multi-pass T5-based text-to-text generative models exhibit better generalization across exam modalities compared to approaches that employ BERT-based task-specific classification layers. We then develop methods that reduce the inference cost of the model, making large-scale corpus processing more feasible for clinical applications. Specifically, we introduce a generative technique that decomposes complex tasks into smaller subtask blocks, which improves a single-pass model when combined with multitask training. In addition, we leverage target-domain contexts during inference to enhance domain adaptation, enabling use of smaller models. Analyses offer insights into the benefits of different cost reduction strategies.
    
[^36]: 块状态变换器

    Block-State Transformer. (arXiv:2306.09539v1 [cs.CL])

    [http://arxiv.org/abs/2306.09539](http://arxiv.org/abs/2306.09539)

    本文提出了一种名为块状态变换器（BST）的混合神经网络层，结合了状态空间模型和块变换器，旨在在语言建模任务中提高性能和可扩展性。实验证明，该模型在语言建模困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，在层级别上具有超过十倍的速度提升。

    

    状态空间模型（SSM）在需要建模长期依赖性并且需要高效扩展到长序列的任务中显示出了惊人的效果。尽管最初是为连续信号设计的，但SSM在视觉和音频等许多任务中表现出了优异的性能；然而，在语言建模任务中，SSM仍然落后于Transformers的性能。在这项工作中，我们提出了一个名为块状态变换器（BST）的混合层，它在内部组合了一个用于长距离上下文化的SSM子层和一个用于短期序列表示的块变换器子层。我们研究了三种不同的、完全可并行的集成SSM和块注意力的变体。我们证明了我们的模型在语言建模的困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，块状态变换器在层级别上具有超过十倍的速度提升。

    State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compa
    
[^37]: 利用增强型大型语言模型（GPT-4）解释法律概念

    Explaining Legal Concepts with Augmented Large Language Models (GPT-4). (arXiv:2306.09525v1 [cs.CL])

    [http://arxiv.org/abs/2306.09525](http://arxiv.org/abs/2306.09525)

    本文评估了利用GPT-4增强解释法律术语性能的方法。与基准设置相比，使用增强的法律信息检索模块可以提高法律术语解释的质量，并消除模型的幻觉问题，从而为在法律领域使用大型语言模型开辟了新的途径。

    

    解释法律开放性术语的含义是法律专业人员的重要任务。先前法院案例中该术语的应用是解释其含义的重要来源。本文评估了GPT-4生成法律术语解释的准确性、清晰性和相关性的性能。我们将GPT-4被直接要求解释法律术语的基准设置的性能与增强方法进行了比较，在增强方法中，一个法律信息检索模块被用来为模型提供相关背景，即来自案例法的句子。我们发现，直接应用GPT-4产生的解释在表面上似乎非常高质量。然而，详细分析揭示了解释的实际准确性方面存在的限制。此外，我们发现增强性能可以提高质量，并似乎消除了幻觉问题，即模型发明不正确的陈述。这些发现为在法律领域中使用大型语言模型开辟了新的途径。

    Interpreting the meaning of legal open-textured terms is a key task of legal professionals. An important source for this interpretation is how the term was applied in previous court cases. In this paper, we evaluate the performance of GPT-4 in generating factually accurate, clear and relevant explanations of terms in legislation. We compare the performance of a baseline setup, where GPT-4 is directly asked to explain a legal term, to an augmented approach, where a legal information retrieval module is used to provide relevant context to the model, in the form of sentences from case law. We found that the direct application of GPT-4 yields explanations that appear to be of very high quality on their surface. However, detailed analysis uncovered limitations in terms of the factual accuracy of the explanations. Further, we found that the augmentation leads to improved quality, and appears to eliminate the issue of hallucination, where models invent incorrect statements. These findings ope
    
[^38]: 关系感知网络基于注意力损失的小样本知识图谱补全

    Relation-Aware Network with Attention-Based Loss for Few-Shot Knowledge Graph Completion. (arXiv:2306.09519v1 [cs.CL])

    [http://arxiv.org/abs/2306.09519](http://arxiv.org/abs/2306.09519)

    本文提出了一种新颖的RANA框架，利用有策略地选择相关负样本和设计基于注意力机制的损失函数来更好地利用负样本并缓解零损失问题，同时设计了一种动态的关系感知实体编码来捕获不同关系下实体的不同表示。

    

    小样本知识图谱补全旨在利用少量参考实体对预测关系的未见事实。现有方法随机选择一个负采样来最小化基于边界的排名损失，但这容易导致零损失问题。此外，实体在不同的上下文中应该具有不同的表征。为了解决这些问题，我们提出了一种新颖的关系感知网络基于注意力损失的框架。具体而言，我们通过有策略地选择相关负样本和设计基于注意力机制的损失函数来更好地利用丰富的负样本并缓解零损失问题。直觉上，与正样本更相似的负样本将对模型贡献更大。此外，我们设计了一种动态的关系感知实体编码来捕捉不同关系下实体的不同表示。三个基准数据集上的实验结果表明，相比最先进的方法，所提出的RANA框架的有效性。

    Few-shot knowledge graph completion (FKGC) task aims to predict unseen facts of a relation with few-shot reference entity pairs. Current approaches randomly select one negative sample for each reference entity pair to minimize a margin-based ranking loss, which easily leads to a zero-loss problem if the negative sample is far away from the positive sample and then out of the margin. Moreover, the entity should have a different representation under a different context. To tackle these issues, we propose a novel Relation-Aware Network with Attention-Based Loss (RANA) framework. Specifically, to better utilize the plentiful negative samples and alleviate the zero-loss issue, we strategically select relevant negative samples and design an attention-based loss function to further differentiate the importance of each negative sample. The intuition is that negative samples more similar to positive samples will contribute more to the model. Further, we design a dynamic relation-aware entity en
    
[^39]: Wikibio: 用于人物传记事件交叉分析的语义资源

    Wikibio: a Semantic Resource for the Intersectional Analysis of Biographical Events. (arXiv:2306.09505v1 [cs.CL])

    [http://arxiv.org/abs/2306.09505](http://arxiv.org/abs/2306.09505)

    本文提供了一个新的用于注释人物传记事件检测的语料库，并训练出了一个模型以检测实体及其相关事件。该模型能够高效地分析维基百科传记中有关女性和非西方人的偏见。

    

    人物传记事件检测是探索和比较人们生活叙述和表达方式的相关任务。在这方面，它可以支持数字人文学和探索有关少数群体偏见的研究。尽管如此，目前没有专门针对此任务设计的语料库和模型。本文通过提供一个新的用于注释人物传记事件检测的语料库来填补这一空白。该语料库包括20篇维基百科传记，与五个现有的语料库进行比较以训练用于人物传记事件检测任务的模型。该模型能够以0.808的F-score检测传记中目标实体的所有提及，以及以0.859的F-score检测实体相关事件。最后，该模型被用于分析维基百科传记中有关女性和非西方人的偏见。

    Biographical event detection is a relevant task for the exploration and comparison of the ways in which people's lives are told and represented. In this sense, it may support several applications in digital humanities and in works aimed at exploring bias about minoritized groups. Despite that, there are no corpora and models specifically designed for this task. In this paper we fill this gap by presenting a new corpus annotated for biographical event detection. The corpus, which includes 20 Wikipedia biographies, was compared with five existing corpora to train a model for the biographical event detection task. The model was able to detect all mentions of the target-entity in a biography with an F-score of 0.808 and the entity-related events with an F-score of 0.859. Finally, the model was used for performing an analysis of biases about women and non-Western people in Wikipedia biographies.
    
[^40]: 逆向缩放：变得更大并不意味着更好

    Inverse Scaling: When Bigger Isn't Better. (arXiv:2306.09479v1 [cs.CL])

    [http://arxiv.org/abs/2306.09479](http://arxiv.org/abs/2306.09479)

    本文研究发现，相对于规模的增加，大型语言模型的任务性能可能出现逆向缩放现象。这一逆向缩放的原因可能有四种：记忆重现、学习样本错误、任务易于干扰、和任务示范的误导。

    

    近期研究表明，随着模型规模、训练数据、计算量的增加，大型语言模型（LMs）的损失比例有可预测的改进。然而，本研究提供了证据表明，LMs也可能显示逆向缩放，即随着规模的增加任务性能越来越差，这可能是由于训练目标和数据的缺陷所致。本文通过公开比赛，Inverse Scaling Prize，在11个数据集上进行实证研究，证明了逆向缩放现象。通过分析数据集及其他实例，我们认为逆向缩放的原因可能有四种：（i）倾向于重复记忆的序列而非跟随上下文指示，（ii）在训练数据中模仿不良模式，（iii）任务中有一个易于干扰LMs的任务，将其注意力转移到较简单的任务，而非较难的任务，（iv）任务的正确示范误导LMs。作者还公布了比赛的获胜数据。

    Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling: (i) preference to repeat memorized sequences over following in-context instructions, (ii) imitation of undesirable patterns in the training data, (iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and (iv) correct but misleading few-shot demonstrations of the task. We release the winning data
    
[^41]: 从零开始实现红队对抗语言模型的探索与建立

    Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])

    [http://arxiv.org/abs/2306.09442](http://arxiv.org/abs/2306.09442)

    本文提出了一种新的红队行动，通过从高层次、抽象的规范出发来考虑语言模型的行为，以探究模型的创新和贡献。

    

    部署大型语言模型（LLMs）可能会产生有害输出，例如有毒或不诚实陈述。先前的研究已经引入了工具以调查有害输出，以识别和减轻这些风险。虽然这是确保语言模型安全的有价值步骤，但这些方法通常依赖于现有的针对不希望的输出的分类器。这限制了它们在只有预先知道有害行为类型的情况下的应用。然而，这跳过了红队行动的核心挑战：开发模型可能展示的行为的上下文理解。此外，当这样的分类器已经存在时，红队行动的边际价值有限，因为分类器可以用于过滤训练数据或模型输出。本文考虑在假设对手从高级、抽象的不良行为规范出发的情况下进行红队行动。红队应该在精化/扩展此规范的同时对抗该模型。

    Deploying Large language models (LLMs) can pose hazards from harmful outputs such as toxic or dishonest speech. Prior work has introduced tools that elicit harmful outputs in order to identify and mitigate these risks. While this is a valuable step toward securing language models, these approaches typically rely on a pre-existing classifier for undesired outputs. This limits their application to situations where the type of harmful behavior is known with precision beforehand. However, this skips a central challenge of red teaming: developing a contextual understanding of the behaviors that a model can exhibit. Furthermore, when such a classifier already exists, red teaming has limited marginal value because the classifier could simply be used to filter training data or model outputs. In this work, we consider red teaming under the assumption that the adversary is working from a high-level, abstract specification of undesired behavior. The red team is expected to refine/extend this spec
    
[^42]: 基于社交媒体的 ChatGPT 自杀风险评估：模型性能、潜力和限制的量化评估

    ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations. (arXiv:2306.09390v1 [cs.CL])

    [http://arxiv.org/abs/2306.09390](http://arxiv.org/abs/2306.09390)

    本文量化评估了基于社交媒体的 ChatGPT 模型在自杀倾向评估方面的表现，比较了其结果与两个微调模型，并探讨了模型响应生成的最佳温度，研究结果显示，以人工标注数据集为基础微调的 Transformer 模型表现更佳，这篇论文为心理健康专家提供了重要的模型评估和参数优化建议。

    

    本文提出了一个新的框架来量化评估交互式 ChatGPT 模型在社交媒体帖子中进行自杀倾向评估的能力，利用马里兰大学 Reddit 自杀倾向数据集。我们使用零样本和少样本实验来对 ChatGPT 在这个任务中的表现进行技术评估，并将其结果与两个基于 transformer 的微调模型进行比较。此外，我们调查不同温度参数对 ChatGPT 响应生成的影响，并讨论基于 ChatGPT 不确定性率的最佳温度参数。我们的研究结果表明，虽然 ChatGPT 在这个任务中具有相当高的准确性，但以人工标注数据集为基础微调的 Transformer 模型表现更优。此外，我们的分析还阐明了如何通过调整 ChatGPT 的超参数来提高其在这一关键任务中帮助心理健康专家的能力。

    This paper presents a novel framework for quantitatively evaluating the interactive ChatGPT model in the context of suicidality assessment from social media posts, utilizing the University of Maryland Reddit suicidality dataset. We conduct a technical evaluation of ChatGPT's performance on this task using Zero-Shot and Few-Shot experiments and compare its results with those of two fine-tuned transformer-based models. Additionally, we investigate the impact of different temperature parameters on ChatGPT's response generation and discuss the optimal temperature based on the inconclusiveness rate of ChatGPT. Our results indicate that while ChatGPT attains considerable accuracy in this task, transformer-based models fine-tuned on human-annotated datasets exhibit superior performance. Moreover, our analysis sheds light on how adjusting the ChatGPT's hyperparameters can improve its ability to assist mental health professionals in this critical task.
    
[^43]: MFAS: 基于多角度融合结构搜索的情感识别，模拟人类认知

    MFAS: Emotion Recognition through Multiple Perspectives Fusion Architecture Search Emulating Human Cognition. (arXiv:2306.09361v1 [eess.AS])

    [http://arxiv.org/abs/2306.09361](http://arxiv.org/abs/2306.09361)

    该论文提出了一种基于多角度融合结构搜索的情感识别框架，模拟人类的认知过程，能够从连续的角度捕捉更全面的情感信息。

    

    语音情感识别旨在识别和分析与人类类似的情绪状态。完美的情感识别可以极大地改善各种人机交互任务。受人类理解情感的过程的启发，我们证明了与量化建模相比，从连续的角度理解语音内容，类似于人类的理解，能够使模型捕捉更全面的情感信息。此外，考虑到人类根据语音中存在的某些线索调整情感单词的文本语义的感知，我们设计了一个新的搜索空间并搜索两种信息的最佳融合策略。实验结果进一步验证了调整感知的重要性。基于这些观察结果，我们提出了一种新的框架，称为Multiple perspectives Fusion Architecture Search(MFAS)。

    Speech emotion recognition aims to identify and analyze emotional states in target speech similar to humans. Perfect emotion recognition can greatly benefit a wide range of human-machine interaction tasks. Inspired by the human process of understanding emotions, we demonstrate that compared to quantized modeling, understanding speech content from a continuous perspective, akin to human-like comprehension, enables the model to capture more comprehensive emotional information. Additionally, considering that humans adjust their perception of emotional words in textual semantic based on certain cues present in speech, we design a novel search space and search for the optimal fusion strategy for the two types of information. Experimental results further validate the significance of this perception adjustment. Building on these observations, we propose a novel framework called Multiple perspectives Fusion Architecture Search (MFAS). Specifically, we utilize continuous-based knowledge to capt
    
[^44]: h2oGPT：民主化大语言模型

    h2oGPT: Democratizing Large Language Models. (arXiv:2306.08161v1 [cs.CL])

    [http://arxiv.org/abs/2306.08161](http://arxiv.org/abs/2306.08161)

    本文介绍了h2oGPT，这是一套开源代码库，用于创建和使用基于GPTs的大语言模型（LLMs），包括100％私有文档搜索。目标是创建真正开源的替代封闭源GPTs，提高人工智能的开发和可靠性。

    

    基于生成预训练变压器（GPTs），大语言模型（LLMs）如GPT-4因其在自然语言处理方面的现实应用而成为人工智能革命的一部分。然而，它们也带来了许多重大的风险，如存在有偏见、私人或有害文本和未经授权的版权材料。本文介绍了h2oGPT，这是一套开源代码库，用于创建和使用基于GPTs的大语言模型（LLMs）。该项目的目标是创建世界上最好的真正开源的替代封闭源GPTs。与开源社区合作，作为其一部分，我们开源了几个LLM，其参数从7亿到400亿，可在完全自由的Apache 2.0许可下商用。我们的发布包括使用自然语言的100％私有文档搜索。开源语言模型有助于促进人工智能的发展并使其更加可靠。

    Foundation Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their real-world applications though natural language processing. However, they also pose many significant risks such as the presence of biased, private, or harmful text, and the unauthorized inclusion of copyrighted material.  We introduce h2oGPT, a suite of open-source code repositories for the creation and use of Large Language Models (LLMs) based on Generative Pretrained Transformers (GPTs). The goal of this project is to create the world's best truly open-source alternative to closed-source GPTs. In collaboration with and as part of the incredible and unstoppable open-source community, we open-source several fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial use under fully permissive Apache 2.0 licenses. Included in our release is 100% private document search using natural language.  Open-source language models help boost AI development and make it more accessible
    
[^45]: GEmo-CLAP: 面向语音情感识别的性别属性增强对比语音-语言预训练模型

    GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v1 [cs.CL])

    [http://arxiv.org/abs/2306.07848](http://arxiv.org/abs/2306.07848)

    本文提出了GEmo-CLAP模型用于语音情感识别，结合了性别属性信息，相比于其他先进方法，该模型在IEMOCAP上实现了更优越的识别性能。

    

    对比语音-语言预训练（CLAP）最近在不同领域取得了惊人的成功。本文提出了一种名为GEmo-CLAP的高效性别属性增强CLAP模型，用于语音情感识别（SER）。具体而言，我们首先利用各种自监督学习的预训练模型构建了一种有效的情感CLAP模型（称为Emo-CLAP），用于SER。然后，考虑到在语音情感建模中性别属性的重要性，我们进一步提出了两种GEmo-CLAP方法，来整合语音信号的情感和性别信息，形成更合理的目标。在IEMOCAP语料库上进行的大量实验表明，我们提出的两种GEmo-CLAP方法始终优于基线Emo-CLAP模型（使用不同的预训练模型），同时与其他最先进的方法相比实现了更优越的识别性能。

    Contrastive Language-Audio Pretraining (CLAP) has recently exhibited impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a kind of efficient gender-attribute-enhanced CLAP model for speech emotion recognition (SER). Specifically, we first build an effective emotion CLAP model termed Emo-CLAP for SER, utilizing various self-supervised learning based pre-trained models. Then, considering the importance of the gender attribute in speech emotion modeling, two GEmo-CLAP approaches are further proposed to integrate the emotion and gender information of speech signals, forming more reasonable objectives. Extensive experiments conducted on the IEMOCAP corpus demonstrate that our proposed two GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with different pre-trained models, while also achieving superior recognition performance compared with other state-of-the-art methods.
    
[^46]: 大型语言模型有时生成纯负反馈文本

    Large Language Models Sometimes Generate Purely Negatively-Reinforced Text. (arXiv:2306.07567v1 [cs.LG])

    [http://arxiv.org/abs/2306.07567](http://arxiv.org/abs/2306.07567)

    大型语言模型有时会从仅包含负奖励的例子中学习，导致生成类似泄漏密码或安全漏洞等敏感信息的文本

    

    在使用对抗性训练时，通常会训练反对最严重失败的案例。然而，这可能会意味着使用包含敏感信息（例如泄露的密码或安全漏洞）的案例作为训练数据。我们可能会认为使用梯度下降算法训练的语言模型永远不会生成仅在与最低奖励相关联的示例中出现的文本片段。本文表明这种假设是错误的：在某些情况下，大型语言模型确实从这种纯负反馈的示例中学习到了东西。我们提出了一种特定的训练设置，使得Pythia-160M能够生成密码的概率略高于随机，尽管仅在对模型不输出这些密码的示例中展示了这些密码。我们的代码可在https://github.com/FabienRoger/Learning-From-Negative-Examples上找到。

    When using adversarial training, it is common practice to train against the most egregious failures. However, this might imply using examples with sensitive information (such as leaked passwords or security vulnerabilities) as training data. One might assume that language models trained with gradient descent never generate text snippets which were only present in examples associated with the lowest possible reward. In this paper, we show that this assumption is wrong: in some situations, large language models do learn from such negatively-reinforced examples. We present a specific training setup that enables Pythia-160M to generate passwords with a probability slightly greater than chance, despite only showing it these passwords on examples where the model is incentivized to not output these passwords. Our code is available at https://github.com/FabienRoger/Learning-From-Negative-Examples
    
[^47]: 评估GPT-3生成的仇恨内容审核解释

    Evaluating GPT-3 Generated Explanations for Hateful Content Moderation. (arXiv:2305.17680v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17680](http://arxiv.org/abs/2305.17680)

    本文通过调查和分析，评估了使用GPT-3生成的针对仇恨内容的解释是否准确和有用。结果显示，GPT-3生成的解释普遍存在过于模糊、聚焦不当等缺点，同时也存在不同类型仇恨言论生成的解释质量差异大的问题。

    

    最近的研究聚焦于使用基于大型语言模型（LLMs）的Fine-tune或提示生成仇恨言论的解释。尽管这个领域越来越受关注，但这些生成解释的有效性和潜在限制仍然不为人们所了解。一个关键问题是，由LLMs生成的这些解释可能会导致用户和内容审核员对标记内容本质做出错误判断。我们提出一个分析框架来检查仇恨言论解释，并进行了一个广泛的调查来评估这些解释。我们在GPT-3上输入仇恨和非仇恨内容，发现受调查者在人工审核GPT生成的解释时，将仇恨言论解释评价为不够准确和有用。

    Recent research has focused on using large language models (LLMs) to generate explanations for hate speech through fine-tuning or prompting. Despite the growing interest in this area, these generated explanations' effectiveness and potential limitations remain poorly understood. A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. For instance, an LLM-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. In light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. Specifically, we prompted GPT-3 to generate explanations for both hateful and non-hateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. Our findings reveal that (1) human evaluators rated the GPT-gene
    
[^48]: "当文字难以表达时，表情符号称霸": 利用情感反转和语义不一致生成带有表情符号的讽刺话语

    "When Words Fail, Emojis Prevail": Generating Sarcastic Utterances with Emoji Using Valence Reversal and Semantic Incongruity. (arXiv:2305.04105v1 [cs.CL])

    [http://arxiv.org/abs/2305.04105](http://arxiv.org/abs/2305.04105)

    本文提出了一种使用情感反转和语义不一致性从非讽刺性输入句子生成带有表情符号的讽刺性句子的架构，具有广泛的应用前景。

    

    讽刺是个体用来表达与含义相反的事情的微妙形式。本文提出了一种使用表情符号从非讽刺性输入句子生成讽刺性句子的新颖架构。我们将生成任务分为两个子任务：一个用于生成文本讽刺，另一个用于收集与这些讽刺句子相关的表情符号。讽刺的两个关键元素被纳入到文本讽刺生成任务中：情感反转和语义不一致性与语境，其中语境可能涉及演讲者和听众之间共享的常识或一般知识。大多数现有的讽刺生成作品都集中在这种文本形式上。然而，在现实世界中，当书面文本无法有效地捕捉口头和面对面交流的情感线索时，人们经常选择表情符号来准确地表达他们的情感。由于表情符号的广泛应用，合理地纳入适当的表情符号很重要。

    Sarcasm pertains to the subtle form of language that individuals use to express the opposite of what is implied. We present a novel architecture for sarcasm generation with emoji from a non-sarcastic input sentence. We divide the generation task into two sub tasks: one for generating textual sarcasm and another for collecting emojis associated with those sarcastic sentences. Two key elements of sarcasm are incorporated into the textual sarcasm generation task: valence reversal and semantic incongruity with context, where the context may involve shared commonsense or general knowledge between the speaker and their audience. The majority of existing sarcasm generation works have focused on this textual form. However, in the real world, when written texts fall short of effectively capturing the emotional cues of spoken and face-to-face communication, people often opt for emojis to accurately express their emotions. Due to the wide range of applications of emojis, incorporating appropriate
    
[^49]: 相似度感知的多模态提示学习用于假新闻检测

    Similarity-Aware Multimodal Prompt Learning for Fake News Detection. (arXiv:2304.04187v1 [cs.CL])

    [http://arxiv.org/abs/2304.04187](http://arxiv.org/abs/2304.04187)

    该论文提出了一种相似度感知的多模态提示学习框架，用于有效融合不同模态信息，实现假新闻检测。所设计的相似度感知融合模块可以量化地根据不同模态之间的相似性权衡每个模态的重要性，有效避免了多模态融合中可能存在的噪声干扰。在两个基准数据集上的实验结果也证明提出的框架实现了最先进的性能，超越了其他强大基线方法。

    

    假新闻检测的标准范式主要利用文本信息来建立新闻的真实性，然而，网上假新闻的话语通常比较微妙，需要专家知识才能使用文本信息揭露假新闻。最近，关注于多模态假新闻检测的研究已经超越了仅基于文本的方法。最近的方法利用预训练模型来提取单模态特征，或直接微调预训练模型，这成为检测假新闻的新范式。然而，这种方法要么需要大量的训练实例，要么需要更新整个预训练模型参数集，不实际可行。此外，传统的多模态方法将跨模态特征直接融合，而不考虑不相关的语义表示可能会引入噪声到多模态特征中。本文提出了一种相似度感知的多模态提示学习（SAMPLE）框架，用于假新闻检测，利用不同模态之间的内在相关性来有效地融合信息。所提出的框架包括一种新颖的相似度感知融合模块，该模块学习根据它们之间的相似度来权衡不同模态的重要性。在两个基准数据集上的实验结果表明，我们的框架实现了最先进的性能，并大幅优于强基线。

    The standard paradigm for fake news detection mainly utilizes text information to model the truthfulness of news. However, the discourse of online fake news is typically subtle and it requires expert knowledge to use textual information to debunk fake news. Recently, studies focusing on multimodal fake news detection have outperformed text-only methods. Recent approaches utilizing the pre-trained model to extract unimodal features, or fine-tuning the pre-trained model directly, have become a new paradigm for detecting fake news. Again, this paradigm either requires a large number of training instances, or updates the entire set of pre-trained model parameters, making real-world fake news detection impractical. Furthermore, traditional multimodal methods fuse the cross-modal features directly without considering that the uncorrelated semantic representation might inject noise into the multimodal features. This paper proposes a Similarity-Aware Multimodal Prompt Learning (SAMPLE) framewo
    
[^50]: 深度学习应用于课程评论的观点挖掘和主题分类

    Deep Learning for Opinion Mining and Topic Classification of Course Reviews. (arXiv:2304.03394v1 [cs.CL])

    [http://arxiv.org/abs/2304.03394](http://arxiv.org/abs/2304.03394)

    本文利用自然语言处理和深度学习技术，通过比较传统方法和现代机器学习方法，展示了如何处理大量课程评论，进行情感极性分析和主题分类。

    

    对于教育工作者和管理者来说，学生对课程的反馈意见非常重要，无论课程的类型或机构如何。在机构级别或在线论坛上处理大量的开放反馈变得不可行。在本文中，我们收集和预处理了大量公开可用的课程评论。我们应用机器学习技术，目的是了解学生的情感和主题。具体而言，我们利用了当前的自然语言处理技术，如词嵌入和深度神经网络，并采用最先进的BERT（双向编码器表示来自变压器）、RoBERTa（经过优化的BERT方法）和XLNet（广义自回归预训练）技术。我们进行了广泛的实验，比较了这些技术与传统方法的差异。这项比较研究展示了如何应用现代机器学习方法进行情感极性分析和主题分类。

    Student opinions for a course are important to educators and administrators, regardless of the type of the course or the institution. Reading and manually analyzing open-ended feedback becomes infeasible for massive volumes of comments at institution level or online forums. In this paper, we collected and pre-processed a large number of course reviews publicly available online. We applied machine learning techniques with the goal to gain insight into student sentiments and topics. Specifically, we utilized current Natural Language Processing (NLP) techniques, such as word embeddings and deep neural networks, and state-of-the-art BERT (Bidirectional Encoder Representations from Transformers), RoBERTa (Robustly optimized BERT approach) and XLNet (Generalized Auto-regression Pre-training). We performed extensive experimentation to compare these techniques versus traditional approaches. This comparative study demonstrates how to apply modern machine learning approaches for sentiment polari
    
[^51]: 利用人类语言理解能力增强药物研发中的活性预测模型

    Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language. (arXiv:2303.03363v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2303.03363](http://arxiv.org/abs/2303.03363)

    本文提出了一种新型活性预测模型，能够通过理解描述任务的文本信息来适应推理时的新预测任务，并在少样本学习基准和药物研发中的零数据问题上都能取得更好的预测性能。

    

    活性和性质预测模型是药物研发和材料科学中的核心工作，但目前它们必须经过训练或微调才能适应新任务。科学语言模型具有零数据和少数据样本的能力，因此对于此类低数据任务，无需训练或微调即可使用。然而，它们在活性预测方面的预测质量不足。本文提出一种新型活性预测模型，能够通过理解描述任务的文本信息来适应推理时的新预测任务。为此，我们提出了一种新的结构，具有化学和自然语言输入的分离模块，以及大型生物化学数据库中的对比预训练目标。通过大量实验证明，我们的方法CLAMP在少样本学习基准和药物研发中的零数据问题上都能取得更好的预测性能。我们认为我们的方法的进展归因于情境感知模型和对比学习策略的结合，以及用于结合数据源的对抗性自编码器。

    Activity and property prediction models are the central workhorses in drug discovery and materials sciences, but currently they have to be trained or fine-tuned for new tasks. Without training or fine-tuning, scientific language models could be used for such low-data tasks through their announced zero- and few-shot capabilities. However, their predictive quality at activity prediction is lacking. In this work, we envision a novel type of activity prediction model that is able to adapt to new prediction tasks at inference time, via understanding textual information describing the task. To this end, we propose a new architecture with separate modules for chemical and natural language inputs, and a contrastive pre-training objective on data from large biochemical databases. In extensive experiments, we show that our method CLAMP yields improved predictive performance on few-shot learning benchmarks and zero-shot problems in drug discovery. We attribute the advances of our method to the mo
    
[^52]: TransFool: 面向神经机器翻译模型的对抗攻击

    TransFool: An Adversarial Attack against Neural Machine Translation Models. (arXiv:2302.00944v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.00944](http://arxiv.org/abs/2302.00944)

    本文研究神经机器翻译(NMT)的对抗攻击易感性，并提出了一种新的攻击算法 TransFool。通过集成语言模型的嵌入表征，我们生成了与原始样本有着高度语义相似性的准确对抗性样本。实验结果表明，在不同翻译任务和 NMT 架构之下，TransFool 可以转移到未知的目标模型，导致严重降低译文质量。

    

    深度神经网络已被证明会对输入的微小扰动产生易受攻击的漏洞。本文研究了神经机器翻译(NMT)模型受到对抗攻击的易感性，并提出了一种新的攻击算法称为TransFool。为了欺骗 NMT 模型，TransFool 建立在一个多项式优化问题和梯度投影步骤之上。通过集成语言模型的嵌入表征，我们在源语言中生成流畅的对抗性样本，其与原始样本有着高度的语义相似性。实验结果表明，对于不同的翻译任务和 NMT 架构，我们的白箱攻击可以严重削弱翻译质量，而原本与对抗性句子之间的语义相似性仍旧很高。此外，我们展示了 TransFool 可以转移至未知的目标模型。最后，在自动化和人为评估的基础上，TransFool 导致了重要的译文质量降低。

    Deep neural networks have been shown to be vulnerable to small perturbations of their inputs, known as adversarial attacks. In this paper, we investigate the vulnerability of Neural Machine Translation (NMT) models to adversarial attacks and propose a new attack algorithm called TransFool. To fool NMT models, TransFool builds on a multi-term optimization problem and a gradient projection step. By integrating the embedding representation of a language model, we generate fluent adversarial examples in the source language that maintain a high level of semantic similarity with the clean samples. Experimental results demonstrate that, for different translation tasks and NMT architectures, our white-box attack can severely degrade the translation quality while the semantic similarity between the original and the adversarial sentences stays high. Moreover, we show that TransFool is transferable to unknown target models. Finally, based on automatic and human evaluations, TransFool leads to imp
    
[^53]: SLUE Phase-2：多样化口语理解任务基准评测套件

    SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks. (arXiv:2212.10525v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10525](http://arxiv.org/abs/2212.10525)

    本文介绍了一种新的注释SLU基准评测任务套件，其中包括问答和摘要、命名实体本地化和对话行为分类任务。这些任务基于自由获取的语音数据设计，希望可以鼓励进一步研究和开发SLU技术，从而推动口语理解技术的发展。

    

    口语理解（SLU）任务在语音研究领域已经研究了许多十年，但是没有像语音和说话人识别等低层任务一样受到很多关注。特别是，没有多少SLU任务基准评测，并且许多现有的基准评测使用数据并不是所有研究人员都可以自由获取的。最近的一些工作开始为几个任务引入这样的基准评测数据集。在这项工作中，我们基于自由获取的语音数据引入了几个新的注释SLU基准评测任务，这些任务补充了现有的基准评测并填补了SLU评估领域的差距。我们贡献了四个任务：问答和摘要涉及更长的语音序列的推理；命名实体本地化处理定位信号中的目标内容这一语音特定任务；对话行为分类确定给定语音话语的功能。我们遵循口语语言理解评估（SLUE）第一阶段的蓝图，设计跨越不同应用领域和使用不同语言技术的多样化任务。我们希望我们的SLUE Phase-2基准评测可以鼓励进一步研究和开发SLU技术，从而推动口语理解技术的发展。

    Spoken language understanding (SLU) tasks have been studied for many decades in the speech research community, but have not received as much attention as lower-level tasks like speech and speaker recognition. In particular, there are not nearly as many SLU task benchmarks, and many of the existing ones use data that is not freely available to all researchers. Recent work has begun to introduce such benchmark datasets for several tasks. In this work, we introduce several new annotated SLU benchmark tasks based on freely available speech data, which complement existing benchmarks and address gaps in the SLU evaluation landscape. We contribute four tasks: question answering and summarization involve inference over longer speech sequences; named entity localization addresses the speech-specific task of locating the targeted content in the signal; dialog act classification identifies the function of a given speech utterance. We follow the blueprint of the Spoken Language Understanding Evalu
    
[^54]: Pix2Struct：屏幕截图解析作为视觉语言理解的预训练方法

    Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding. (arXiv:2210.03347v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.03347](http://arxiv.org/abs/2210.03347)

    本文提出了一个名为Pix2Struct的预训练图像到文本模型，用于纯视觉语言理解，可以在包含视觉语境的任务上进行微调。Pix2Struct的预训练目标是学习将屏幕截图解析成简化的HTML，这个方法能够处理来源复杂、多样性大的数据。

    

    视觉语境的语言是无处不在的，它源于从有图示的教科书到有图像和表格的网页再到有按钮和表单的移动应用。为此，以往的研究通常依赖于特定领域的技术，且很少共享基础数据、模型架构和目标。本文提出了一种名为Pix2Struct的预训练图像到文本模型，用于纯视觉语言理解，可以在包含视觉语境的任务上进行微调。Pix2Struct的预训练目标是学习将屏幕截图解析成简化的HTML。Web具有丰富的视觉元素，这些元素在HTML结构中清晰地反映出来，提供了大量的预训练数据，非常适用于多样化的下游任务。直观上，这个目标涵盖了常见的预训练信号，例如OCR、语言模型和图像描述。除了新的预训练策略外，我们还引入了一个可变分辨率的输入表示。

    Visually-situated language is ubiquitous -- sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and forms. Perhaps due to this diversity, previous work has typically relied on domain-specific recipes with limited sharing of the underlying data, model architectures, and objectives. We present Pix2Struct, a pretrained image-to-text model for purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representa
    
[^55]: 事实破坏者：针对事实验证系统的证据操纵攻击分类法

    Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems. (arXiv:2209.03755v4 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2209.03755](http://arxiv.org/abs/2209.03755)

    本研究提出了一种分类法，涵盖针对在线证据的伪装和虚假信息等两种攻击目标和不同的威胁模型维度。我们设计并提出了几种可能的攻击方法，展示了在多种情况下，可以对证据中与事实相关的部分进行微小的修改，并生成无法被自动识别的虚假信息。

    

    误导和虚假信息对我们的安全和稳定构成了严重的全球威胁。为了应对在线虚假信息规模的挑战，研究人员一直在努力通过检索和验证相关证据来自动进行事实核查。然而，尽管有许多进展，但对此类系统可能面临的攻击向量的全面评估仍然缺乏。本研究假设存在一个敌对方，通过伪装相关证据或者提供具有误导性的信息，自动干扰在线证据，以破坏事实验证模型。我们首先提出一个分类法，涵盖这两个目标和不同的威胁模型维度。基于此，我们设计并提出了几种可能的攻击方法。我们展示了在多种情况下，可以对证据中与事实相关的部分进行微小的修改，并生成与原始证据脱节的虚假信息，从而破坏事实验证结果，使其无法被自动识别。

    Mis- and disinformation are a substantial global threat to our security and safety. To cope with the scale of online misinformation, researchers have been working on automating fact-checking by retrieving and verifying against relevant evidence. However, despite many advances, a comprehensive evaluation of the possible attack vectors against such systems is still lacking. Particularly, the automated fact-verification process might be vulnerable to the exact disinformation campaigns it is trying to combat. In this work, we assume an adversary that automatically tampers with the online evidence in order to disrupt the fact-checking model via camouflaging the relevant evidence or planting a misleading one. We first propose an exploratory taxonomy that spans these two targets and the different threat model dimensions. Guided by this, we design and propose several potential attack methods. We show that it is possible to subtly modify claim-salient snippets in the evidence and generate diver
    
[^56]: 解决机器学习众包工作者的人类受试者身份问题

    Resolving the Human Subjects Status of Machine Learning's Crowdworkers. (arXiv:2206.04039v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2206.04039](http://arxiv.org/abs/2206.04039)

    机器学习在研究中使用的众包工作者问题引起了对其受试者身份的争议与监管合规性，本文针对该问题进行研究，重点关注了自然语言处理领域中的研究监管挑战。

    

    近年来，机器学习(Machine Learning, ML)在构建数据集和解决需要人类交互或判断的研究问题方面，已经严重依赖于众包工作者。由于执行的任务多样化和数据用途的多样性，很难确定何时将众包工作者视为工人(而非人类受试者)。这些困难加剧了政策的冲突，一些机构和研究人员将所有ML众包工作者视为人类受试者，而其他人则认为它们很少构成人类受试者。值得注意的是，包括众包工作的鲜有ML论文提到IRB的监督，引发了违反道德和法规要求的可能性。我们研究了ML众包研究的适当划定，并关注自然语言处理领域暴露出的独特研究监督挑战。至关重要的是，在美国公共规则下，这些判断取决于关于问题的确定，涉及谁(或什么)的问题。

    In recent years, machine learning (ML) has relied heavily on crowdworkers both for building datasets and for addressing research questions requiring human interaction or judgment. The diverse tasks performed and uses of the data produced render it difficult to determine when crowdworkers are best thought of as workers (versus human subjects). These difficulties are compounded by conflicting policies, with some institutions and researchers regarding all ML crowdworkers as human subjects and others holding that they rarely constitute human subjects. Notably few ML papers involving crowdwork mention IRB oversight, raising the prospect of non-compliance with ethical and regulatory requirements. We investigate the appropriate designation of ML crowdsourcing studies, focusing our inquiry on natural language processing to expose unique challenges for research oversight. Crucially, under the U.S. Common Rule, these judgments hinge on determinations of aboutness, concerning both whom (or what) 
    
[^57]: 重新审视 DocRED - 解决关系抽取中的假阴性问题

    Revisiting DocRED -- Addressing the False Negative Problem in Relation Extraction. (arXiv:2205.12696v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.12696](http://arxiv.org/abs/2205.12696)

    该论文主要针对文档层面上的关系抽取中存在假阴性的问题，在重新注释 DocRED 数据集中添加被忽略的关系三元组后，我们得到了一个性能提升约 13 F1 分数的新数据集 Re-DocRED，并发现了有效改进的潜在领域。

    

    DocRED 数据集是最流行和广泛使用的文档级关系抽取基准数据集之一。它采用了一个推荐-修订的标注方案，以获得大规模的注释数据集。但是，我们发现 DocRED 的标注是不完整的，即假阴性样本很普遍。为了解决这个缺点，我们通过向原始 DocRED 中添加被忽略的关系三元组来重新注释了 4,053 个文档，我们将我们修正后的 DocRED 数据集命名为 Re-DocRED。我们使用最先进的神经模型在这两个数据集上开展了大量的实验，实验结果表明，使用我们的 Re-DocRED 训练和评估的模型性能提高了大约 13 个 F1 分数。此外，我们进行了全面的分析，识别出了进一步改进的潜在领域。我们的数据集公开在 https://github。

    The DocRED dataset is one of the most popular and widely used benchmarks for document-level relation extraction (RE). It adopts a recommend-revise annotation scheme so as to have a large-scale annotated dataset. However, we find that the annotation of DocRED is incomplete, i.e., false negative samples are prevalent. We analyze the causes and effects of the overwhelming false negative problem in the DocRED dataset. To address the shortcoming, we re-annotate 4,053 documents in the DocRED dataset by adding the missed relation triples back to the original DocRED. We name our revised DocRED dataset Re-DocRED. We conduct extensive experiments with state-of-the-art neural models on both datasets, and the experimental results show that the models trained and evaluated on our Re-DocRED achieve performance improvements of around 13 F1 points. Moreover, we conduct a comprehensive analysis to identify the potential areas for further improvement. Our dataset is publicly available at https://github.
    

