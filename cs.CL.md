# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models](https://rss.arxiv.org/abs/2402.01349) | 对于评估大型语言模型中多选题回答的合理性进行了回顾，发现当前基于多选题回答的基准可能无法充分捕捉大型语言模型的真实能力。 |
| [^2] | [DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference](https://arxiv.org/abs/2404.00242) | DeFT提出了一种带IO意识的树注意力算法，通过在QKV准备和注意力计算阶段实现内存高效的计算，降低内存印记，以解决当前树解码策略和推断系统不适配的问题。 |
| [^3] | [Evaluation of Semantic Search and its Role in Retrieved-Augmented-Generation (RAG) for Arabic Language](https://arxiv.org/abs/2403.18350) | 本文旨在建立阿拉伯语义搜索的基准，并在检索增强生成（RAG）框架内评估其有效性。 |
| [^4] | [Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons](https://arxiv.org/abs/2403.17760) | 本文引入了一个具有大量词汇重叠的NLI挑战数据集，探讨了大型语言模型在处理特定结构时出现的失败现象，揭示了它们在区分特定结构时的不足之处。 |
| [^5] | [Large Language Model for Mental Health: A Systematic Review](https://arxiv.org/abs/2403.15401) | 该论文系统评价了大型语言模型在心理健康领域的应用，讨论了其在早期筛查、数字干预和其他临床应用中的挑战和机遇。 |
| [^6] | [Text clustering with LLM embeddings](https://arxiv.org/abs/2403.15112) | 研究表明，LLM嵌入能够捕捉结构化语言的细微差别，BERT在性能上领先于轻量级选项，增加嵌入维度和摘要技术并不一致地提高聚类效率 |
| [^7] | [CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification](https://arxiv.org/abs/2403.11904) | 该论文提出了CICLe框架，基于适应上下文学习的方式，在大规模多类食品风险分类中取得了较好的效果，提出了基于符合预测的LLM-in-the-loop框架，可以提高基本分类器的性能，并减少能源消耗。 |
| [^8] | [Recurrent Drafter for Fast Speculative Decoding in Large Language Models](https://arxiv.org/abs/2403.09919) | 本文介绍了一种适用于大型语言模型的循环草稿机制，结合了经典双模型和最新单模型方法，通过运用循环依赖设计，实现了高效的推测解码。 |
| [^9] | [SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation](https://arxiv.org/abs/2403.07088) | 提出了SPA（Side Plugin Adaption）的轻量级架构，用于在严格的设备计算和内存约束条件下快速进行推断，同时保留隐私。 |
| [^10] | [A New Benchmark for Evaluating Automatic Speech Recognition in the Arabic Call Domain](https://arxiv.org/abs/2403.04280) | 该研究引入了一个针对阿拉伯电话对话领域挑战的全面评估基准，旨在为开发和评估能够适应真实电话通讯条件的ASR系统提供一个严格的测试平台。 |
| [^11] | [From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models](https://arxiv.org/abs/2403.03893) | 该研究拓展了语言模型中毒性缓解的范围，涵盖了多语言环境，通过翻译数据评估和增强缓解技术，比较了不同缓解方法，并探讨了模型大小和数据量对缓解效果的影响。 |
| [^12] | [You Need to Pay Better Attention](https://arxiv.org/abs/2403.01643) | 提出了三种新的注意力机制，在效率和学习能力方面优于标准的多头注意力，提高了Transformer模型的性能和更广泛的部署能力。 |
| [^13] | [Language Models Represent Beliefs of Self and Others](https://arxiv.org/abs/2402.18496) | 通过神经激活线性解析语言模型中代理人观点下的信念状态，揭示了大型语言模型内部表述自我和他人信念，这对社会推理过程至关重要，并在多样社会推理任务中具有潜在的泛化能力。 |
| [^14] | [Machine Unlearning of Pre-trained Large Language Models](https://arxiv.org/abs/2402.15159) | 本研究在大型语言模型（LLMs）中探讨了“被遗忘权”的概念，提出机器遗忘作为解决方案，并在预训练模型中建立了全面的遗忘框架及高效的遗忘方法，同时提供了改进超参数鲁棒性以及高效调整超参数的指南。 |
| [^15] | [RelayAttention for Efficient Large Language Model Serving with Long System Prompts](https://arxiv.org/abs/2402.14808) | 本论文提出的RelayAttention算法旨在改善涉及长系统提示的大型语言模型服务的效率，通过一次性从DRAM读取隐藏状态来消除现有因果注意力算法中的内存访问冗余。 |
| [^16] | [Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2402.14800) | 引入了专家级稀疏化技术，提出了专家修剪和跳过的后训练方法，以提高MoE LLMs的部署效率，同时保持模型性能。 |
| [^17] | [Unveiling Linguistic Regions in Large Language Models](https://arxiv.org/abs/2402.14700) | 本文从区域划分的角度出发，发现了大型语言模型中对应语言能力的核心区域，并展示了去除该区域会导致跨30种不同语言的显著性能下降。 |
| [^18] | [GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis](https://arxiv.org/abs/2402.13494) | GradSafe通过分析LLMs中关键安全参数的梯度，有效检测不安全提示，无需额外训练即可优于现有方法。 |
| [^19] | [Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations](https://arxiv.org/abs/2402.12786) | 本文旨在让大型语言模型能够根据口语的不同语言风格做出恰当回应，并为此收集了一组适合训练的语音对语音数据集。 |
| [^20] | [BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence](https://arxiv.org/abs/2402.12174) | BIDER通过知识综合和偏好对齐，将检索文档转化为关键支持证据，从而提高了LLMs的答案质量并减少了输入内容长度。 |
| [^21] | [Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs](https://arxiv.org/abs/2402.12052) | 本文介绍了一种新的协作方法SlimPLM，通过精简代理模型检测LLMs中缺失的知识，以增强LLMs的知识获取过程 |
| [^22] | [Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources](https://arxiv.org/abs/2402.11505) | 该研究引入了FlexLoRA，一个简单而有效的大型语言模型微调聚合方案，能够在联邦学习中充分利用异质客户资源，通过动态调整本地LoRA排名和采用奇异值分解进行权重重新分配，提升全局模型的广泛知识。 |
| [^23] | [Large Language Models as Zero-shot Dialogue State Tracker through Function Calling](https://arxiv.org/abs/2402.10466) | 本研究提出了一种通过函数调用将大型语言模型用于零-shot对话状态追踪的新方法，能够在任务导向对话中取得出色的性能，适应不同领域而无需大量数据收集或模型调整。 |
| [^24] | [Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models](https://arxiv.org/abs/2402.07865) | 本论文探索了视觉条件化语言模型（VLMs）设计的关键空间，并提供了一套标准化评估，同时还研究了预训练的视觉表示和权衡的问题。 |
| [^25] | [Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue](https://arxiv.org/abs/2402.06967) | 本论文提出了一种名为Midi-Tuning的多轮交互对话调整框架，通过分别对代理人和用户建模，并利用轮次级内存缓存机制进行调整，实现了对话代理的一致性和稳定性。 |
| [^26] | [Debating with More Persuasive LLMs Leads to More Truthful Answers](https://arxiv.org/abs/2402.06782) | 本文研究了更弱的语言模型是否能评估更强的模型的正确性。研究发现，通过进行辩论，非专家模型和人类回答问题的准确性都有所提高。 |
| [^27] | [Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains](https://arxiv.org/abs/2402.05140) | 本文介绍了一种将通用的LLMs应用于专业领域的方法，通过学习自定义的输入标签来对LLMs进行条件约束。通过明确将任务领域与任务功能分离，这种方法能够改善在专业领域中的任务求解能力。 |
| [^28] | [Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models](https://arxiv.org/abs/2402.03271) | 通过引入不确定性感知规划（UoT）算法，我们实现了增强大型语言模型的主动寻求信息的能力，通过模拟未来场景、基于不确定性的奖励机制和奖励传播方案，优化问题提问方式。 |
| [^29] | [LQER: Low-Rank Quantization Error Reconstruction for LLMs](https://arxiv.org/abs/2402.02446) | LQER使用低秩逼近和激活引起的尺度矩阵，实现了对LLMs的近乎无损量化，无需知识蒸馏或梯度优化，并大幅减少硬件资源的使用。 |
| [^30] | [APIServe: Efficient API Support for Large-Language Model Inferencing](https://arxiv.org/abs/2402.01869) | APIServe是针对API增强的大型语言模型推理的一个高效工具，它最大限度地减少了由 API 调用引起的 GPU 资源浪费，并提高了整体服务吞吐量。 |
| [^31] | [PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs](https://arxiv.org/abs/2305.12392) | 提出了一个名为"PiVe"的框架，通过迭代验证来提升LLMs的基于图的生成能力。实验结果表明，PiVe方法在三个基于图的数据集上取得了一致的改善，并且验证模块可以作为数据增强工具帮助提高结果质量。 |
| [^32] | [Towards Uncertainty-Aware Language Agent.](http://arxiv.org/abs/2401.14016) | UALA是一个使用不确定性量化来进行代理和外部世界交互的框架，相比于其他方法，它在多个任务和语言模型尺寸下表现出显著的性能改进，并且在对外部世界的依赖性方面更低。 |
| [^33] | [Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction.](http://arxiv.org/abs/2401.10189) | 这篇论文提出了一种名为Chem-FINESE的方法来处理化学领域中细粒度少样本实体提取的问题。该方法通过使用序列到序列的实体提取器和自我验证模块来从输入句子中提取命名实体并重构原始输入句子。实验证明了该方法的有效性和可行性。 |
| [^34] | [Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models without Logit Access.](http://arxiv.org/abs/2401.09967) | 本文介绍了一种无需访问逻辑回归的黑盒大型语言模型的草图引导约束解码的方法，通过利用本地辅助模型优化黑盒语言模型的输出，以初步输出作为进一步扩展的 "草图"，从而提高了有限约束解码的应用能力。 |
| [^35] | [Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models.](http://arxiv.org/abs/2401.06102) | 本论文提出了一个叫做Patchscope的框架，用于检查语言模型的隐藏表示。该框架不仅统一了先前的检查技术，还解决了其中一些问题，并且还开辟了新的可能性。 |
| [^36] | [LLaMA Pro: Progressive LLaMA with Block Expansion.](http://arxiv.org/abs/2401.02415) | 本论文提出了一种新型的LLMs后处理方法，通过扩展Transformer模块并使用新的语料库进行调整，有效地提高了模型知识，而不会发生灾难性遗忘。在代码和数学领域的实验结果表明，LLaMA Pro是一种功能强大且适用于通用任务、编程和数学的基础模型，同时在各种基准测试中取得了先进的性能，展示了智能体推理和解决多样化任务的潜力。 |
| [^37] | [Improving Low-resource Prompt-based Relation Representation with Multi-view Decoupling Learning.](http://arxiv.org/abs/2312.17267) | 提出了一种名为MVRE的新方法，通过将关系解耦为不同的视角，生成多视角关系表示，并利用预训练语言模型（PLMs）的能力来提高低资源关系抽取任务的性能。 |
| [^38] | [On the Representational Capacity of Recurrent Neural Language Models.](http://arxiv.org/abs/2310.12942) | 本文研究了基于循环神经网络的语言模型的计算表达性，扩展了图灵完备性结果到概率情况，并提供了上下界分析。 |
| [^39] | [Prompt Injection Attacks and Defenses in LLM-Integrated Applications.](http://arxiv.org/abs/2310.12815) | 本文提出了一个通用框架来形式化提示注入攻击，并系统化防御这种类型的攻击。 |
| [^40] | [ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models.](http://arxiv.org/abs/2310.08975) | ChatKBQA是一个基于精调大型语言模型的生成-检索框架，用于改进知识库问答的效率和准确性，实验结果显示在多个数据集上取得了新的最好表现。 |
| [^41] | [Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation.](http://arxiv.org/abs/2310.07968) | 这项研究引入了零射交互个性化对象导航（ZIPON），通过使用大型语言模型（LLM）和用户反馈，解决了在未知环境中导航到个性化目标对象的问题。 |
| [^42] | [Hi Model, generating 'nice' instead of 'good' is not as bad as generating 'rice'! Towards Context and Semantic Infused Dialogue Generation Loss Function and Evaluation Metric.](http://arxiv.org/abs/2309.05804) | 提出了一种新的对话生成损失函数和评估指标，解决了以往方法中词汇匹配和缺少上下文考虑的限制。 |
| [^43] | [ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases.](http://arxiv.org/abs/2306.16092) | 本论文介绍了一个基于开源的法律大型语言模型ChatLaw，它通过综合外部知识库为中国法律领域的数字化转型提供支持。该模型采用了精心设计的法律领域微调数据集，并通过将向量数据库检索与关键词检索相结合的方法解决了法律数据筛选中的模型幻觉问题。同时，引入了一种自注意力方法以增强对文本中关键信息的注意力。 |
| [^44] | [Eliminating Spurious Correlations from Pre-trained Models via Data Mixing.](http://arxiv.org/abs/2305.14521) | 本文提出了一种通过数据混合来消除预训练模型中虚假相关性的方法，来提高模型对于新样本的预测能力。这种方法经过理论证明和多种任务实验验证，可以取得良好的效果。 |

# 详细

[^1]: 超越答案：对于评估大型语言模型中多选题回答的合理性的回顾

    Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models

    [https://rss.arxiv.org/abs/2402.01349](https://rss.arxiv.org/abs/2402.01349)

    对于评估大型语言模型中多选题回答的合理性进行了回顾，发现当前基于多选题回答的基准可能无法充分捕捉大型语言模型的真实能力。

    

    在自然语言处理领域，大型语言模型（LLMs）引发了一场范式转变，显著提升了自然语言生成任务的性能。尽管取得了这些进展，对LLMs的全面评估仍然是社区面临的必然挑战。最近，将多选题回答（MCQA）作为LLMs的基准已经引起了广泛关注。本研究调查了MCQA作为LLMs评估方法的合理性。如果LLMs真正理解问题的语义，它们的性能应该在从相同问题派生的各种配置上表现一致。然而，我们的实证结果表明LLMs的响应一致性存在显著差异，我们将之定义为LLMs的响应可变性综合征（REVAS），这表明目前基于MCQA的基准可能无法充分捕捉LLMs的真实能力，强调了对更合适的评估方法的需要。

    In the field of natural language processing (NLP), Large Language Models (LLMs) have precipitated a paradigm shift, markedly enhancing performance in natural language generation tasks. Despite these advancements, the comprehensive evaluation of LLMs remains an inevitable challenge for the community. Recently, the utilization of Multiple Choice Question Answering (MCQA) as a benchmark for LLMs has gained considerable traction. This study investigates the rationality of MCQA as an evaluation method for LLMs. If LLMs genuinely understand the semantics of questions, their performance should exhibit consistency across the varied configurations derived from the same questions. Contrary to this expectation, our empirical findings suggest a notable disparity in the consistency of LLM responses, which we define as REsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current MCQA-based benchmarks may not adequately capture the true capabilities of LLMs, which underscores the need f
    
[^2]: DeFT：带IO意识的Flash Tree-attention用于高效的基于树搜索的LLM推断

    DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference

    [https://arxiv.org/abs/2404.00242](https://arxiv.org/abs/2404.00242)

    DeFT提出了一种带IO意识的树注意力算法，通过在QKV准备和注意力计算阶段实现内存高效的计算，降低内存印记，以解决当前树解码策略和推断系统不适配的问题。

    

    使用树搜索进行解码可以极大地提高基于变压器的大型语言模型（LLMs）的推断质量。根据引导信号，它通过形成LLM输出从根到叶子的最佳路径来提高可控性、推理能力、对齐等。然而，由于计算冗余、内存占用和内存访问，当前的树解码策略及其推断系统互相不适配，导致推断效率低下。为解决这一问题，我们提出了DeFT，一种IO感知树注意力算法，它在两个阶段中保持内存高效的注意力计算，降低内存印记：（1）QKV准备：我们提出了一种KV引导树分裂策略，为GPU的高利用率和尽可能减少GPU全局内存和芯片上共享内存之间的KV缓存的内存读/写; （2）注意力计算...

    arXiv:2404.00242v1 Announce Type: cross  Abstract: Decoding using tree search can greatly enhance the inference quality for transformer-based Large Language Models (LLMs). Depending on the guidance signal, it searches for the best path from root to leaf in the tree by forming LLM outputs to improve controllability, reasoning ability, alignment, et cetera. However, current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference. To address this issue, we propose DeFT, an IO-aware tree attention algorithm that maintains memory-efficient attention calculation with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction of memory reads/writes for the KV cache between GPU global memory and on-chip shared memory as much as possible; (2) Attention Calculati
    
[^3]: 评估语义搜索及其在阿拉伯语言中检索增强生成（RAG）中的作用

    Evaluation of Semantic Search and its Role in Retrieved-Augmented-Generation (RAG) for Arabic Language

    [https://arxiv.org/abs/2403.18350](https://arxiv.org/abs/2403.18350)

    本文旨在建立阿拉伯语义搜索的基准，并在检索增强生成（RAG）框架内评估其有效性。

    

    机器学习和深度学习的最新进展带来了语义相似性的概念，已在多个应用中证明具有巨大益处，大大取代了关键词搜索。然而，评估语义相似性并在各种文档中为特定查询进行搜索仍然是一项复杂的任务。这种复杂性源于任务的多方面性，缺乏标准基准，而这些挑战对于阿拉伯语言而言更加复杂。本文致力于为阿拉伯语义搜索建立一个简单而强大的基准。此外，为了精确评估这些指标和数据集的有效性，我们在检索增强生成（RAG）框架内进行语义搜索的评估。

    arXiv:2403.18350v1 Announce Type: new  Abstract: The latest advancements in machine learning and deep learning have brought forth the concept of semantic similarity, which has proven immensely beneficial in multiple applications and has largely replaced keyword search. However, evaluating semantic similarity and conducting searches for a specific query across various documents continue to be a complicated task. This complexity is due to the multifaceted nature of the task, the lack of standard benchmarks, whereas these challenges are further amplified for Arabic language. This paper endeavors to establish a straightforward yet potent benchmark for semantic search in Arabic. Moreover, to precisely evaluate the effectiveness of these metrics and the dataset, we conduct our assessment of semantic search within the framework of retrieval augmented generation (RAG).
    
[^4]: 施工如此困难，以至于即使大型语言模型也因错误原因而正确

    Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons

    [https://arxiv.org/abs/2403.17760](https://arxiv.org/abs/2403.17760)

    本文引入了一个具有大量词汇重叠的NLI挑战数据集，探讨了大型语言模型在处理特定结构时出现的失败现象，揭示了它们在区分特定结构时的不足之处。

    

    在本文中，我们做出了两方面理解的贡献：从自然语言处理的角度看，我们引入了一个具有大量词汇重叠的NLI挑战数据集，最大程度地减少了模型仅基于标记区别来区分蕴涵的可能性，并展示了GPT-4和Llama 2以强烈的偏见失败。然后，我们进一步创建了具有挑战性的子任务，以解释这种失败。从计算语言学的角度看，我们确定了一个包含三类形容词的结构组，这些形容词无法通过表面特征来区分。这使我们能够以各种方式探究LLM对这些结构的理解，并发现它们在各种方式上都无法区分它们，表明它们不足以代表它们的含义或捕捉短语头的词汇属性。

    arXiv:2403.17760v1 Announce Type: new  Abstract: In this paper, we make a contribution that can be understood from two perspectives: from an NLP perspective, we introduce a small challenge dataset for NLI with large lexical overlap, which minimises the possibility of models discerning entailment solely based on token distinctions, and show that GPT-4 and Llama 2 fail it with strong bias. We then create further challenging sub-tasks in an effort to explain this failure. From a Computational Linguistics perspective, we identify a group of constructions with three classes of adjectives which cannot be distinguished by surface features. This enables us to probe for LLM's understanding of these constructions in various ways, and we find that they fail in a variety of ways to distinguish between them, suggesting that they don't adequately represent their meaning or capture the lexical properties of phrasal heads.
    
[^5]: 大型语言模型在心理健康领域的系统评价

    Large Language Model for Mental Health: A Systematic Review

    [https://arxiv.org/abs/2403.15401](https://arxiv.org/abs/2403.15401)

    该论文系统评价了大型语言模型在心理健康领域的应用，讨论了其在早期筛查、数字干预和其他临床应用中的挑战和机遇。

    

    大型语言模型（LLMs）在数字健康领域受到了广泛关注，展现出了潜在的应用性，但它们在心理健康领域的应用仍在持续讨论中。这项系统性评价旨在总结和表征LLMs在心理健康领域的应用，通过调查LLMs最新研究的优势和局限性，讨论心理健康领域早期筛查、数字干预以及其他临床应用的挑战和机遇。根据PRISMA指南，我们审查了PubMed、DBLP计算机科学文献数据库和IEEE Xplore上发表的英文文章，时间跨度为2017年1月1日至2023年9月1日，重点关注心理健康和LLMs。该综述分析了32篇文章，包括使用社交媒体数据集进行心理健康分析的（n=13）、心理健康聊天机器人（n=10）以及其他心理健康应用（n=9）。研究结果显示LLMs在心理健康问题检测中的有效性以及

    arXiv:2403.15401v1 Announce Type: cross  Abstract: Large language models (LLMs) have received much attention and shown their potential in digital health, while their application in mental health is subject to ongoing debate. This systematic review aims to summarize and characterize the use of LLMs in mental health by investigating the strengths and limitations of the latest work in LLMs and discusses the challenges and opportunities for early screening, digital interventions, and other clinical applications in mental health. Following PRISMA guidelines, we examined English articles from PubMed, DBLP Computer Science Bibliography, and IEEE Xplore, published between 1 January 2017, and 1 September 2023, focusing on mental health and LLMs. The review analyzed 32 articles, including mental health analysis using social media datasets (n=13), mental health chatbots (n=10), and other mental health applications (n=9). Findings reveal LLMs' effectiveness in mental health issue detection and the
    
[^6]: 使用LLM嵌入进行文本聚类

    Text clustering with LLM embeddings

    [https://arxiv.org/abs/2403.15112](https://arxiv.org/abs/2403.15112)

    研究表明，LLM嵌入能够捕捉结构化语言的细微差别，BERT在性能上领先于轻量级选项，增加嵌入维度和摘要技术并不一致地提高聚类效率

    

    文本聚类是组织不断增长的数字内容的重要方法，有助于结构化和发现未分类数据中的隐藏模式。在这项研究中，我们调查了不同文本嵌入（特别是大型语言模型LLMs中使用的）和聚类算法如何影响文本数据集的聚类方式。进行了一系列实验以评估嵌入是如何影响聚类结果的，以及通过摘要进行降维和嵌入大小调整的作用。结果显示，LLM嵌入在捕获结构化语言的细微差别方面表现出色，而BERT在性能上领先于轻量级选项。此外，我们发现增加嵌入维度和摘要技术并不一致地提高聚类效率，这表明这些策略需要仔细分析才能在实际模型中使用。这些结果突出了一种

    arXiv:2403.15112v1 Announce Type: cross  Abstract: Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a co
    
[^7]: CICLe: 适应上下文的大规模多类食品风险分类学习

    CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification

    [https://arxiv.org/abs/2403.11904](https://arxiv.org/abs/2403.11904)

    该论文提出了CICLe框架，基于适应上下文学习的方式，在大规模多类食品风险分类中取得了较好的效果，提出了基于符合预测的LLM-in-the-loop框架，可以提高基本分类器的性能，并减少能源消耗。

    

    受污染或掺假食品对人类健康构成重大风险。在给定了用于训练的标记网络文本集的情况下，可以应用机器学习和自然语言处理来自动检测这种风险。我们发布了一个包含7,546个描述公共食品召回公告的短文本数据集。每个文本都经过手动标记，分为两个粒度级别（粗粒度和细粒度），用于表示召回对应的食品产品和危害。我们描述了数据集并对朴素、传统和Transformer模型进行了基准测试。基于我们的分析，基于tf-idf表示的逻辑回归在支持较低的类别上优于RoBERTa和XLM-R。最后，我们讨论了不同的提示策略，并提出了一种基于符合预测的LLM-in-the-loop框架，这可以提高基本分类器的性能，同时减少了与普通提示相比的能源消耗。

    arXiv:2403.11904v1 Announce Type: new  Abstract: Contaminated or adulterated food poses a substantial risk to human health. Given sets of labeled web texts for training, Machine Learning and Natural Language Processing can be applied to automatically detect such risks. We publish a dataset of 7,546 short texts describing public food recall announcements. Each text is manually labeled, on two granularity levels (coarse and fine), for food products and hazards that the recall corresponds to. We describe the dataset and benchmark naive, traditional, and Transformer models. Based on our analysis, Logistic Regression based on a tf-idf representation outperforms RoBERTa and XLM-R on classes with low support. Finally, we discuss different prompting strategies and present an LLM-in-the-loop framework, based on Conformal Prediction, which boosts the performance of the base classifier while reducing energy consumption compared to normal prompting.
    
[^8]: 大型语言模型中用于快速推测解码的循环草稿机制

    Recurrent Drafter for Fast Speculative Decoding in Large Language Models

    [https://arxiv.org/abs/2403.09919](https://arxiv.org/abs/2403.09919)

    本文介绍了一种适用于大型语言模型的循环草稿机制，结合了经典双模型和最新单模型方法，通过运用循环依赖设计，实现了高效的推测解码。

    

    在本文中，我们介绍一种改进的推测解码方法，旨在提高大型语言模型的效率。我们的方法利用了两种成熟技术的优势：经典的双模型推测解码方法和较新的单模型方法Medusa。从Medusa得到灵感，我们的方法采用了单模型策略进行推测解码。然而，我们的方法通过使用具有循环依赖设计的单个轻量级草稿头来区分自己，本质上类似于经典推测解码中使用的小型草稿模型，但避免了完整transformer架构的复杂性。由于循环依赖，我们可以使用波束搜索快速过滤出草稿头中不需要的候选项。其结果是一种结合了单模型设计简易性并避免了创建数据相关树依赖的方法。

    arXiv:2403.09919v1 Announce Type: new  Abstract: In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attent
    
[^9]: SPA：面向云端和设备协作的计算友好型Seq2seq个性化生成

    SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation

    [https://arxiv.org/abs/2403.07088](https://arxiv.org/abs/2403.07088)

    提出了SPA（Side Plugin Adaption）的轻量级架构，用于在严格的设备计算和内存约束条件下快速进行推断，同时保留隐私。

    

    大语言模型(LLMs)表现出色的能力已在各种任务和问答中得到展示。然而，LLMs需要高计算成本和大内存成本。同时，当训练或预测过程包含敏感信息时，LLMs可能会导致隐私泄露。在本文中，我们提出了SPA（Side Plugin Adaption），这是一个轻量级架构，用于快速设备上的推断和在严格的设备计算和内存约束条件下保持隐私。

    arXiv:2403.07088v1 Announce Type: new  Abstract: Large language models(LLMs) have shown its outperforming ability on various tasks and question answering. However, LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference and privacy retaining on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and private personal feature.Further more, SPA provides a framework to keep feature-base parameters on private guaranteed but 
    
[^10]: 用于评估阿拉伯呼叫领域自动语音识别的新基准

    A New Benchmark for Evaluating Automatic Speech Recognition in the Arabic Call Domain

    [https://arxiv.org/abs/2403.04280](https://arxiv.org/abs/2403.04280)

    该研究引入了一个针对阿拉伯电话对话领域挑战的全面评估基准，旨在为开发和评估能够适应真实电话通讯条件的ASR系统提供一个严格的测试平台。

    

    这项工作旨在引入一个全面的用于阿拉伯语音识别评估的基准，专门针对阿拉伯语电话对话中的挑战进行了定制。阿拉伯语以其丰富的方言多样性和语音复杂性而闻名，对自动语音识别（ASR）系统提出了许多独特的挑战。这些挑战在电话通话领域进一步放大，那里的音频质量、背景噪音和会话式语音风格会对识别准确性产生负面影响。我们的工作旨在建立一个稳健的基准，不仅包含广泛的阿拉伯方言范围，还模拟呼叫通讯的真实条件。通过结合多样的方言表达，并考虑呼叫录音的可变质量，这个基准旨在为开发和评估能够应对实际挑战的ASR系统提供严格的测试平台。

    arXiv:2403.04280v1 Announce Type: new  Abstract: This work is an attempt to introduce a comprehensive benchmark for Arabic speech recognition, specifically tailored to address the challenges of telephone conversations in Arabic language. Arabic, characterized by its rich dialectal diversity and phonetic complexity, presents a number of unique challenges for automatic speech recognition (ASR) systems. These challenges are further amplified in the domain of telephone calls, where audio quality, background noise, and conversational speech styles negatively affect recognition accuracy. Our work aims to establish a robust benchmark that not only encompasses the broad spectrum of Arabic dialects but also emulates the real-world conditions of call-based communications. By incorporating diverse dialectical expressions and accounting for the variable quality of call recordings, this benchmark seeks to provide a rigorous testing ground for the development and evaluation of ASR systems capable of
    
[^11]: 从单一到多样：拓展语言模型中毒性缓解的范围

    From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models

    [https://arxiv.org/abs/2403.03893](https://arxiv.org/abs/2403.03893)

    该研究拓展了语言模型中毒性缓解的范围，涵盖了多语言环境，通过翻译数据评估和增强缓解技术，比较了不同缓解方法，并探讨了模型大小和数据量对缓解效果的影响。

    

    迄今为止，语言模型中的毒性缓解几乎完全集中在单语言环境中。随着语言模型拥抱多语言能力，我们的安全措施跟上步伐至关重要。我们意识到了这一研究空白，我们的方法将传统的毒性缓解范围扩展到应对多语言带来的复杂性。在缺乏跨语言的足够标注数据集的情况下，我们使用翻译数据来评估和增强我们的缓解技术。我们还在静态和持续毒性缓解场景下比较了微调缓解方法和检索增强技术。这使我们能够检验翻译质量和跨语言转移对毒性缓解的影响。我们还探讨了模型大小和数据数量如何影响这些缓解工作的成功。涵盖了九种语言，我们的研究代表了广泛的语言学领域。

    arXiv:2403.03893v1 Announce Type: cross  Abstract: To date, toxicity mitigation in language models has almost entirely been focused on single-language settings. As language models embrace multilingual capabilities, it's crucial our safety measures keep pace. Recognizing this research gap, our approach expands the scope of conventional toxicity mitigation to address the complexities presented by multiple languages. In the absence of sufficient annotated datasets across languages, we employ translated data to evaluate and enhance our mitigation techniques. We also compare finetuning mitigation approaches against retrieval-augmented techniques under both static and continual toxicity mitigation scenarios. This allows us to examine the effects of translation quality and the cross-lingual transfer on toxicity mitigation. We also explore how model size and data quantity affect the success of these mitigation efforts. Covering nine languages, our study represents a broad array of linguistic f
    
[^12]: 您需要更好地关注付费

    You Need to Pay Better Attention

    [https://arxiv.org/abs/2403.01643](https://arxiv.org/abs/2403.01643)

    提出了三种新的注意力机制，在效率和学习能力方面优于标准的多头注意力，提高了Transformer模型的性能和更广泛的部署能力。

    

    我们引入了三种新的注意力机制，这些机制在效率和学习能力方面胜过标准的多头注意力，从而提高了Transformer模型的性能和更广泛的部署能力。我们的第一个贡献是优化注意力，其性能与标准注意力相似，但参数数量少了四分之三，每个头部少了一个矩阵乘法。接下来，我们引入了高效注意力，其性能与标准注意力相当，但参数数量减少了一半，每个头部减少了两个矩阵乘法，并且比标准注意力快两倍。最后，我们介绍了超级注意力，在视觉和自然语言处理任务中明显超越了标准注意力，同时具有更少的参数和矩阵乘法。除了提供严格的数学比较，我们在MN中评估了所提出的注意力机制

    arXiv:2403.01643v1 Announce Type: cross  Abstract: We introduce three new attention mechanisms that outperform standard multi-head attention in terms of efficiency and learning capabilities, thereby improving the performance and broader deployability of Transformer models. Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. In addition to providing rigorous mathematical comparisons, we evaluate the presented attention mechanisms on MN
    
[^13]: 语言模型表达自我和他人信念

    Language Models Represent Beliefs of Self and Others

    [https://arxiv.org/abs/2402.18496](https://arxiv.org/abs/2402.18496)

    通过神经激活线性解析语言模型中代理人观点下的信念状态，揭示了大型语言模型内部表述自我和他人信念，这对社会推理过程至关重要，并在多样社会推理任务中具有潜在的泛化能力。

    

    理解和归因心理状态，即心灵理论（ToM），被视为人类社会推理的基本能力。虽然大型语言模型（LLMs）似乎具有某些ToM能力，但这些能力背后的机制仍然令人费解。在本研究中，我们发现通过语言模型的神经激活线性解码各个代理人观点下的信念状态是可能的，这表明存在自我的内部表述和他人信念的表示。通过操纵这些表征，我们观察到模型的ToM性能发生显著变化，突显了其在社会推理过程中的关键作用。此外，我们的发现还延伸到涉及不同因果推理模式的多样社会推理任务，暗示了这些表征的潜在泛化能力。

    arXiv:2402.18496v1 Announce Type: new  Abstract: Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.
    
[^14]: 面向预训练大型语言模型的机器遗忘

    Machine Unlearning of Pre-trained Large Language Models

    [https://arxiv.org/abs/2402.15159](https://arxiv.org/abs/2402.15159)

    本研究在大型语言模型（LLMs）中探讨了“被遗忘权”的概念，提出机器遗忘作为解决方案，并在预训练模型中建立了全面的遗忘框架及高效的遗忘方法，同时提供了改进超参数鲁棒性以及高效调整超参数的指南。

    

    本研究探讨了大型语言模型（LLMs）背景下“被遗忘权”的概念。我们以机器遗忘作为一个关键解决方案，重点关注预训练模型——一个明显缺乏研究的领域。我们在预训练LLMs中勾勒了一个全面的机器遗忘框架，包括对七种不同遗忘方法的批判性分析。通过使用来自arXiv、书籍和GitHub的策划数据集进行严格评估，我们建立了一个有力的机器遗忘性能基准，表明这些方法的计算效率比重新训练高出 $10^5$ 倍以上。我们的结果表明，在分布数据上将梯度上升与梯度下降结合可以改善超参数的鲁棒性。我们还提供了关于在遗忘过程中进行高效超参数调整的详细指南。我们的研究推动了有关伦理人工智能实践的讨论，提供了

    arXiv:2402.15159v1 Announce Type: cross  Abstract: This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering
    
[^15]: RelayAttention：用于高效实现大型语言模型服务与长系统提示的论文

    RelayAttention for Efficient Large Language Model Serving with Long System Prompts

    [https://arxiv.org/abs/2402.14808](https://arxiv.org/abs/2402.14808)

    本论文提出的RelayAttention算法旨在改善涉及长系统提示的大型语言模型服务的效率，通过一次性从DRAM读取隐藏状态来消除现有因果注意力算法中的内存访问冗余。

    

    实际的大型语言模型（LLM）服务可能涉及一个长的系统提示，其中包含任务的指示、示例和知识文档，并在许多请求中复用。然而，长系统提示会导致吞吐量/延迟瓶颈，因为生成下一个标记的成本随着序列长度的增长而增加。本文旨在提高涉及长系统提示的LLM服务的效率。我们的关键观察是，处理这些系统提示在现有因果注意力计算算法中需要大量冗余的内存访问。具体来说，对于批量请求，系统提示的缓存隐藏状态（即键-值对）被多次从芯片外的DRAM传输到芯片上的SRAM，每次对应一个单独的请求。为了消除这种冗余，我们提出了RelayAttention，一种注意力算法，允许仅从DRAM读取这些隐藏状态一次。

    arXiv:2402.14808v1 Announce Type: new  Abstract: Practical large language model (LLM) services may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across numerous requests. However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length. This paper aims to improve the efficiency of LLM services that involve long system prompts. Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (i.e., key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once 
    
[^16]: 并非所有专家都相等: 混合专家大型语言模型的高效专家修剪和跳过

    Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models

    [https://arxiv.org/abs/2402.14800](https://arxiv.org/abs/2402.14800)

    引入了专家级稀疏化技术，提出了专家修剪和跳过的后训练方法，以提高MoE LLMs的部署效率，同时保持模型性能。

    

    大型语言模型（LLMs）进展中的一个重要进展是混合专家（MoE）LLMs的出现。与传统的LLMs相比，MoE LLMs可以在更少的参数下实现更高的性能，但由于其巨大的参数大小，仍然很难部署它们。与先前依赖于专门设计的硬件的权重剪枝方法不同，本文主要旨在通过引入即插即用的专家级稀疏化技术来提高MoE LLMs的部署效率。具体而言，我们首次提出了针对任务不可知和任务特定的MoE LLMs专家修剪和跳过的后训练方法，旨在提高在广泛任务范围内保持模型性能的同时提高部署效率。大量实验证明，我们提出的方法可以同时减小模型大小并增加推断速度，同时保持饱和

    arXiv:2402.14800v1 Announce Type: cross  Abstract: A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining sat
    
[^17]: 揭示大型语言模型中的语言区域

    Unveiling Linguistic Regions in Large Language Models

    [https://arxiv.org/abs/2402.14700](https://arxiv.org/abs/2402.14700)

    本文从区域划分的角度出发，发现了大型语言模型中对应语言能力的核心区域，并展示了去除该区域会导致跨30种不同语言的显著性能下降。

    

    大型语言模型(LLMs)已经展示了相当大的跨语言对齐和泛化能力。当前研究主要集中在改善LLMs的跨语言泛化能力上。然而，关于LLMs如何实现跨语言对齐的内在机制仍然缺乏研究。从区域划分的角度出发，本文在LLMs的语言能力上进行了几项调查。我们发现LLMs中的一个核心区域对应于语言能力，大约占总模型参数的1%。通过将参数设置为零来去除这个核心区域，会导致30种不同语言的显著性能下降。此外，这个核心区域表现出显著的维度依赖性，对特定维度上的单个参数的扰动会导致语言能力的丧失。此外，我们发现独特的区域...

    arXiv:2402.14700v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs' cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependency, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct regions 
    
[^18]: GradSafe: 通过安全关键梯度分析检测LLMs中的不安全提示

    GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis

    [https://arxiv.org/abs/2402.13494](https://arxiv.org/abs/2402.13494)

    GradSafe通过分析LLMs中关键安全参数的梯度，有效检测不安全提示，无需额外训练即可优于现有方法。

    

    大型语言模型（LLMs）面临来自不安全提示的威胁。现有的检测不安全提示的方法主要是在线内容审核API或微调LLMs。然而，这些策略通常需要大量和资源密集型的数据收集和训练过程。在这项研究中，我们提出了GradSafe，通过仔细审查LLMs中的安全关键参数的梯度有效地检测不安全提示。我们的方法基于一个关键观察：LLMs对于与合规响应配对的不安全提示的损失的梯度在某些安全关键参数上表现出相似的模式。相比之下，安全提示导致明显不同的梯度模式。基于这一观察，GradSafe分析来自提示（与合规响应配对）的梯度以准确地检测不安全提示。我们展示了，应用于Llama-2的GradSafe，无需进一步训练即可胜过Llama Guard。

    arXiv:2402.13494v1 Announce Type: new  Abstract: Large Language Models (LLMs) face threats from unsafe prompts. Existing methods for detecting unsafe prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects unsafe prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our methodology is grounded in a pivotal observation: the gradients of an LLM's loss for unsafe prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to markedly different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect unsafe prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard, despite its ex
    
[^19]: 将大型语言模型发展起来，以捕捉不同语言风格并在口语交流中做出恰当回应

    Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations

    [https://arxiv.org/abs/2402.12786](https://arxiv.org/abs/2402.12786)

    本文旨在让大型语言模型能够根据口语的不同语言风格做出恰当回应，并为此收集了一组适合训练的语音对语音数据集。

    

    在口语对话中，即使两个当前对话是相同的句子，但当以不同风格发音时，它们的回应仍可能不同。语音风格包含语言附加信息和韵律信息，标志着文本和语音形式之间最显著的差异。使用仅文本的大型语言模型（LLMs）来建模口语对话时，纯文本的LLMs无法根据当前对话的语音风格给出不同的回应。本文旨在使LLMs能够倾听说话风格并作出恰当回应。我们的目标是教会LLM“即使句子相同，如果以不同风格说出，相应的回应可能会不同”。由于目前没有适合实现此目标的数据集，我们收集了一组语音对语音的数据集StyleTalk，具有以下所需特征：当两个当前语音具有相同内容但以不同风格说出时，它们的回应将

    arXiv:2402.12786v1 Announce Type: new  Abstract: In spoken dialogue, even if two current turns are the same sentence, their responses might still differ when they are spoken in different styles. The spoken styles, containing paralinguistic and prosodic information, mark the most significant difference between text and speech modality. When using text-only LLMs to model spoken dialogue, text-only LLMs cannot give different responses based on the speaking style of the current turn. In this paper, we focus on enabling LLMs to listen to the speaking styles and respond properly. Our goal is to teach the LLM that "even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different". Since there is no suitable dataset for achieving this goal, we collect a speech-to-speech dataset, StyleTalk, with the following desired characteristics: when two current speeches have the same content but are spoken in different styles, their responses wil
    
[^20]: BIDER: 通过关键支持证据弥合有效检索增强的LLMs中的知识不一致性

    BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence

    [https://arxiv.org/abs/2402.12174](https://arxiv.org/abs/2402.12174)

    BIDER通过知识综合和偏好对齐，将检索文档转化为关键支持证据，从而提高了LLMs的答案质量并减少了输入内容长度。

    

    大规模语言模型（LLMs）在开放领域QA等知识密集型任务中表现出有效性，解决了知识更新和事实不足等固有挑战。然而，检索知识与LLMs所需知识之间的不一致性导致了LLMs答案质量下降。本文介绍了BIDER，一种通过知识综合、监督微调（SFT）和偏好对齐，将检索文档细化为关键支持证据（KSE）的方法。我们通过学习制定KSE来训练BIDER，同时通过强化学习将其输出最大化，以使其与LLMs的信息获取偏好保持一致。在五个数据集上的评估结果显示，BIDER提高了LLMs答案质量7％，同时将检索文档的输入内容长度减少了80％，优于现有方法。所提出的KSE模拟有效地为LLMs提供了必要的信息。

    arXiv:2402.12174v1 Announce Type: new  Abstract: Retrieval-augmented large language models (LLMs) have demonstrated efficacy in knowledge-intensive tasks such as open-domain QA, addressing inherent challenges in knowledge update and factual inadequacy. However, inconsistencies between retrieval knowledge and the necessary knowledge for LLMs, leading to a decline in LLM's answer quality. This paper introduces BIDER, an approach that refines retrieval documents into Key Supporting Evidence (KSE) through knowledge synthesis, supervised fine-tuning (SFT), and preference alignment. We train BIDER by learning from crafting KSE, while maximizing its output to align with LLM's information acquisition preferences through reinforcement learning. Evaluations across five datasets show BIDER boosts LLMs' answer quality by 7% while reducing input content length in retrieval documents by 80%, outperforming existing methods. The proposed KSE simulation effectively equips LLMs with essential informatio
    
[^21]: 小模型，大见解：利用精简代理模型确定LLMs何时以及为何检索

    Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs

    [https://arxiv.org/abs/2402.12052](https://arxiv.org/abs/2402.12052)

    本文介绍了一种新的协作方法SlimPLM，通过精简代理模型检测LLMs中缺失的知识，以增强LLMs的知识获取过程

    

    arXiv:2402.12052v1 公告类型:新摘要: 大型语言模型（LLMs）与搜索引擎的整合代表了知识获取方法的重要发展。然而，确定LLM已经具备的知识和需要搜索引擎帮助的知识仍然是一个未解决的问题。大多数现有方法通过LLM本身预处理答案或推理的结果来解决这个问题，但这带来了过高的计算成本。本文介绍了一种新颖的协作方法，即SlimPLM，通过精简代理模型检测LLMs中缺失的知识，以增强LLMs的知识获取过程。我们采用参数远远更少的代理模型，并将其答案视为启发式答案。然后利用启发式答案来预测回答用户问题所需的知识，以及LLM中已知和未知的知识。我们只为未知知识进行检索

    arXiv:2402.12052v1 Announce Type: new  Abstract: The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM's knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the mis
    
[^22]: 在异构语言任务和客户资源下对大型语言模型进行联邦微调

    Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources

    [https://arxiv.org/abs/2402.11505](https://arxiv.org/abs/2402.11505)

    该研究引入了FlexLoRA，一个简单而有效的大型语言模型微调聚合方案，能够在联邦学习中充分利用异质客户资源，通过动态调整本地LoRA排名和采用奇异值分解进行权重重新分配，提升全局模型的广泛知识。

    

    近期，联邦学习（FL）被应用于对大型语言模型（LLMs）进行参数高效微调。然而，由于客户的资源和数据分布不均匀，这引发了重大挑战。本研究引入了FlexLoRA，这是一种简单而有效的LLM微调聚合方案，它可以缓解传统FL中的“桶效应”，该效应限制了拥有丰富资源的客户实现潜力，将他们与最缺乏资源的参与者的能力捆绑在一起。FlexLoRA允许动态调整本地LoRA排名，促进全局模型的发展，并赋予更广泛、不太任务特定的知识。通过从个体客户贡献中合成完整大小的LoRA权重，并利用奇异值分解（SVD）进行权重重新分配，FlexLoRA充分利用了客户间的资源差异。本研究涉及超过1600个执行多样NLP任务的客户。

    arXiv:2402.11505v1 Announce Type: cross  Abstract: Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs). While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients.This study introduces FlexLoRA, a simple yet effective aggregation scheme for LLM fine-tuning, which mitigates the "buckets effect" in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants. FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge. By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources. Involving over 1,600 clients performing diverse NLP tasks, ou
    
[^23]: 将大型语言模型作为零-shot对话状态追踪器通过函数调用

    Large Language Models as Zero-shot Dialogue State Tracker through Function Calling

    [https://arxiv.org/abs/2402.10466](https://arxiv.org/abs/2402.10466)

    本研究提出了一种通过函数调用将大型语言模型用于零-shot对话状态追踪的新方法，能够在任务导向对话中取得出色的性能，适应不同领域而无需大量数据收集或模型调整。

    

    大型语言模型（LLMs）在会话系统中日益普遍，这是因为它们在一般情境中具有先进的理解和生成能力。然而，在需要不仅进行响应生成还需要在特定任务和领域内进行有效对话状态追踪（DST）的任务导向对话（TOD）中，它们的有效性仍不尽人意。在这项工作中，我们提出了一种通过函数调用解决LLMs中的DST的新方法FnCTOD。这种方法改进了零-shot DST，使其能够适应各种领域，而无需进行大量数据收集或模型调整。我们的实验结果表明，我们的方法在使用开源或专有LLMs时都取得了出色的性能：通过上下文提示，使得各种7B或13B参数模型超越了之前由ChatGPT实现的最新技术成果（SOTA）的水平，并提高了ChatGPT的性能，击败了

    arXiv:2402.10466v1 Announce Type: cross  Abstract: Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the
    
[^24]: 透视VLMs：探索视觉条件化语言模型的设计空间

    Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models

    [https://arxiv.org/abs/2402.07865](https://arxiv.org/abs/2402.07865)

    本论文探索了视觉条件化语言模型（VLMs）设计的关键空间，并提供了一套标准化评估，同时还研究了预训练的视觉表示和权衡的问题。

    

    视觉条件化语言模型（VLMs）在视觉对话、场景理解和机器人任务规划等应用中得到了越来越多的应用，这种应用促使了像LLaVa、InstructBLIP和PaLI-3等许多新模型的出现。尽管有这么多新的发布，但关于图像预处理、架构和优化的关键设计决策仍然未被充分探索，这使得我们很难理解模型性能的因素，这一挑战又因缺乏客观、一致的评估而变得更加复杂。为了填补这些空白，我们首先编制了一套标准化评估，涵盖了视觉问答、从语言中定位物体以及探索幻觉等属性的目标挑战集，这些评估可以提供关于VLM能力的精细、准确的见解。其次，我们对关键的设计轴进行了严格的研究，包括预训练的视觉表示和使用的权衡。

    Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization from language, and targeted challenge sets that probe properties such as hallucination; evaluations that provide calibrated, fine-grained insight into a VLM's capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and quantifying the tradeoffs of using 
    
[^25]: 一次指导，多轮稳定对话：对话的高效调整框架

    Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue

    [https://arxiv.org/abs/2402.06967](https://arxiv.org/abs/2402.06967)

    本论文提出了一种名为Midi-Tuning的多轮交互对话调整框架，通过分别对代理人和用户建模，并利用轮次级内存缓存机制进行调整，实现了对话代理的一致性和稳定性。

    

    调整预训练语言模型以实现对话生成已成为构建能力强大的对话代理的主流范式。然而，传统的调整方式狭隘地将对话生成视为类似其他语言生成任务的过程，忽视了对话者之间的角色差异和对话应具备的多轮交互过程。这种方式导致了所构建代理人的对话一致性不尽如人意。在本研究中，我们强调对话的交互性和沟通性，并认为分别对代理人和用户的讲话者角色进行建模更为可行，使得代理人能够保持一致的角色。我们提出了一种有效的多轮交互对话调整（Midi-Tuning）框架。该框架使用基于大型语言模型的两个适配器分别对代理人和用户进行建模，它们按轮次交替使用话语，并通过轮次级内存缓存机制进行调整。大量实验证明，我们的方法可以有效提高对话代理的一致性和稳定性。

    Tuning pretrained language models for dialogue generation has been a prevalent paradigm for building capable dialogue agents. Yet, traditional tuning narrowly views dialogue generation as resembling other language generation tasks, ignoring the role disparities between two speakers and the multi-round interactive process that dialogues ought to be. Such a manner leads to unsatisfactory chat consistency of the built agent. In this work, we emphasize the interactive, communicative nature of dialogue and argue that it is more feasible to model the speaker roles of agent and user separately, enabling the agent to adhere to its role consistently. We propose an efficient Multi-round Interactive Dialogue Tuning (Midi-Tuning) framework. It models the agent and user individually with two adapters built upon large language models, where they utilize utterances round by round in alternating order and are tuned via a round-level memory caching mechanism. Extensive experiments demonstrate that, our
    
[^26]: 与更有说服力的LLMs辩论会导致更真实的回答

    Debating with More Persuasive LLMs Leads to More Truthful Answers

    [https://arxiv.org/abs/2402.06782](https://arxiv.org/abs/2402.06782)

    本文研究了更弱的语言模型是否能评估更强的模型的正确性。研究发现，通过进行辩论，非专家模型和人类回答问题的准确性都有所提高。

    

    与所需行为一致的大型语言模型（LLM）的常见方法主要依赖于人工标注的数据。然而，随着模型变得越来越复杂，它们将超过人类专业知识，人类评估的角色将演变为非专家监督专家。在此之前，我们问：更弱的模型能评估更强的模型的正确性吗？我们在类似的环境中调查了这个问题，其中更强的模型（专家）拥有回答问题所需的信息，而更弱的模型（非专家）缺乏这些信息。我们评估的方法是\textit{辩论}，其中两个LLM专家分别支持不同的答案，一个非专家选择答案。我们发现辩论 consistently帮助非专家模型和人类回答问题，分别达到76%和88%的准确性（朴素基准分别为48%和60%）。此外，以无监督方式优化专家辩论者的说服力会提高非专家的能力。

    Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is \textit{debate}, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76\% and 88\% accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert abilit
    
[^27]: Tag-LLM: 将通用的LLM应用于专业领域的再利用

    Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains

    [https://arxiv.org/abs/2402.05140](https://arxiv.org/abs/2402.05140)

    本文介绍了一种将通用的LLMs应用于专业领域的方法，通过学习自定义的输入标签来对LLMs进行条件约束。通过明确将任务领域与任务功能分离，这种方法能够改善在专业领域中的任务求解能力。

    

    大型语言模型（LLMs）在理解和生成自然语言方面表现出了非凡的能力。然而，在专门领域中，如物理学和生物医学科学这样的预训练语料库中未充分涵盖的领域，它们的能力下降。本文探讨了如何将通用LLMs重新用于专业领域的有效任务解决方案。我们介绍了一种新颖的、与模型无关的框架，用于学习自定义的输入标签，这些标签被参数化为连续向量并附加到LLMs的嵌入层，以对LLMs进行条件约束。我们设计了两种类型的输入标签：领域标签用于限定专业表示（例如化学式）并提供领域相关的上下文；功能标签用于表示特定的功能（例如预测分子性质）并压缩功能解决指令。我们使用辅助数据和领域知识开发了一个包括三个阶段的学习这些标签的协议。通过明确将任务领域与任务功能分离，我们的方法能够改善在专业领域中的任务求解能力。

    Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from tas
    
[^28]: 想法的不确定性：不确定性感知规划增强大型语言模型的信息搜索能力

    Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models

    [https://arxiv.org/abs/2402.03271](https://arxiv.org/abs/2402.03271)

    通过引入不确定性感知规划（UoT）算法，我们实现了增强大型语言模型的主动寻求信息的能力，通过模拟未来场景、基于不确定性的奖励机制和奖励传播方案，优化问题提问方式。

    

    在面对不确定性时，寻求信息的能力至关重要。在许多实际应用中，比如医学诊断和故障排除，解决任务所需的信息不是初始给定的，而需要通过询问后续问题来主动寻求（例如，医生向患者询问症状的更多细节）。在这项工作中，我们引入了思想的不确定性（UoT），一种算法将大型语言模型的能力与主动提问信息的能力相结合。UoT结合了1）不确定性感知仿真方法，使模型能够模拟可能的未来场景，并估计其发生的可能性；2）基于不确定性的奖励机制，激励模型寻求信息；3）奖励传播方案，以最大化预期奖励的方式选择最佳的问题提问方式。在医学诊断、故障排除和'20的实验中。

    In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 
    
[^29]: LQER: 低秩量化误差重建用于LLMs

    LQER: Low-Rank Quantization Error Reconstruction for LLMs

    [https://arxiv.org/abs/2402.02446](https://arxiv.org/abs/2402.02446)

    LQER使用低秩逼近和激活引起的尺度矩阵，实现了对LLMs的近乎无损量化，无需知识蒸馏或梯度优化，并大幅减少硬件资源的使用。

    

    大型语言模型（LLMs）的训练后量化是具有挑战性的。在这项工作中，我们介绍了低秩量化误差减少（LQER）方法，该方法结合了量化和低秩逼近来恢复模型的能力。LQER利用激活引起的尺度矩阵将量化误差的奇异值分布推向期望的分布，从而实现了在各种LLMs和下游任务上近乎无损的W4A8量化，无需知识蒸馏、网格搜索或基于梯度的迭代优化。与现有方法不同，LQER的计算模式消除了从不规则内存位置收集高精度权重所需的专用Scatter和Gather过程。我们的W4A8 LLMs在六个热门下游任务上实现了近乎无损的性能，同时使用的硬件资源比领先的最新方法少1.36倍。一旦论文被接受，我们将开源我们的框架。

    Post-training quantization of Large Language Models (LLMs) is challenging. In this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines quantization and low-rank approximation to recover the model capability. LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-base iterative optimization. Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations. Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using 1.36$\times$ fewer hardware resources than the leading state-of-the-art method. We will open-source our framework once the paper is accepted.
    
[^30]: APIServe: 高效支持大型语言模型推理的API工具

    APIServe: Efficient API Support for Large-Language Model Inferencing

    [https://arxiv.org/abs/2402.01869](https://arxiv.org/abs/2402.01869)

    APIServe是针对API增强的大型语言模型推理的一个高效工具，它最大限度地减少了由 API 调用引起的 GPU 资源浪费，并提高了整体服务吞吐量。

    

    大型语言模型越来越多地与外部工具和API集成，如ChatGPT插件，以扩展其能力以外的语言中心任务。然而，当前的LLM推理系统是为独立的LLM设计的。它们将API调用视为新请求，导致不必要的重新计算已经计算过的上下文，这占了总模型前向时间的37-40%。本文提出了APIServe，这是针对API增强的LLM推理框架。APIServe最大限度地减少了由API调用引起的GPU资源浪费，并将节省的内存用于服务更多的请求。与现有的LLM推理系统相比，APIServe将整体服务吞吐量提升了1.6倍，每秒完成的请求增加了2倍。

    Large language models are increasingly integrated with external tools and APIs like ChatGPT plugins to extend their capability beyond language-centric tasks. However, today's LLM inference systems are designed for standalone LLMs. They treat API calls as new requests, causing unnecessary recomputation of already computed contexts, which accounts for 37-40% of total model forwarding time. This paper presents APIServe, the first LLM inference framework targeting API-augmented LLMs. APISERVE minimizes the GPU resource waste caused by API calls and dedicates saved memory for serving more requests. APISERVE improves the overall serving throughput by 1.6x and completes 2x more requests per second compared to the state-of-the-art LLM inference systems.
    
[^31]: PiVe：通过迭代验证提升LLMs的基于图的生成能力的提示方法

    PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs

    [https://arxiv.org/abs/2305.12392](https://arxiv.org/abs/2305.12392)

    提出了一个名为"PiVe"的框架，通过迭代验证来提升LLMs的基于图的生成能力。实验结果表明，PiVe方法在三个基于图的数据集上取得了一致的改善，并且验证模块可以作为数据增强工具帮助提高结果质量。

    

    大型语言模型(LLMs)在解决各种不同领域的自然语言任务方面展示了强大的能力。由于LLMs的训练目标和预训练数据，LLMs对涉及结构化数据生成的任务并不非常适用。我们提出了一个名为"PiVe"的框架，通过迭代验证来提升LLMs的基于图的生成能力。我们展示了如何训练一个小型语言模型作为LLMs的输出的验证模块(例如ChatGPT，GPT-4)，通过精细的纠正指令来迭代改进其性能。我们还展示了验证模块如何在离线环境中应用迭代校正，以获得更经济高效的文本到图形生成任务解决方案。在三个基于图的数据集上的实验结果表明，通过PiVe的方法能够持续改善结果。此外，我们创建了GenWiki-HIQ数据集，并强调验证模块可以作为数据增强工具，帮助提高自动生成的结果质量。

    Large language models (LLMs) have shown great abilities of solving various natural language tasks in different domains. Due to the training objective of LLMs and their pre-training data, LLMs are not very well equipped for tasks involving structured data generation. We propose a framework, Prompting with Iterative Verification (PiVe), to improve graph-based generative capability of LLMs. We show how a small language model could be trained to act as a verifier module for the output of an LLM(i.e., ChatGPT, GPT-4), and to iteratively improve its performance via fine-grained corrective instructions. We also show how the verifier module could apply iterative corrections offline for a more cost-effective solution to the text-to-graph generation task. Experiments on three graph-based datasets show consistent improvement gained via PiVe. Additionally, we create GenWiki-HIQ and highlight that the verifier module can be used as a data augmentation tool to help improve the quality of automatical
    
[^32]: 面向不确定性感知的语言智能体

    Towards Uncertainty-Aware Language Agent. (arXiv:2401.14016v1 [cs.CL])

    [http://arxiv.org/abs/2401.14016](http://arxiv.org/abs/2401.14016)

    UALA是一个使用不确定性量化来进行代理和外部世界交互的框架，相比于其他方法，它在多个任务和语言模型尺寸下表现出显著的性能改进，并且在对外部世界的依赖性方面更低。

    

    虽然语言智能体通过将大型语言模型置于更多功能的设计核心以及与外部世界的动态交互中取得了令人期待的成功，但现有方法在这些交互过程中忽视了不确定性的概念。我们提出了一种称为不确定性感知的语言智能体（UALA）的框架，该框架使用不确定性量化来编排代理和外部世界之间的交互。与其他知名对手（如ReAct）相比，我们在3个代表性任务（HotpotQA，StrategyQA，MMLU）和各种LLM尺寸上进行了广泛的实验，结果表明UALA在性能方面有显著的改进，同时对外部世界的依赖性显著降低（即，减少了工具调用和标记数）。我们的分析提供了各种见解，包括与代理微调相比，UALA的巨大潜力，并强调在语言模型的口头置信度作为不确定性的代理时的不可靠性。

    While Language Agents have achieved promising success by placing Large Language Models at the core of a more versatile design that dynamically interacts with the external world, the existing approaches neglect the notion of uncertainty during these interactions. We present the Uncertainty-Aware Language Agent (UALA), a framework that orchestrates the interaction between the agent and the external world using uncertainty quantification. Compared with other well-known counterparts like ReAct, our extensive experiments across 3 representative tasks (HotpotQA, StrategyQA, MMLU) and various LLM sizes demonstrates that UALA brings a significant improvement of performance, while having a substantially lower reliance on the external world (i.e., reduced number of tool calls and tokens). Our analyses provide various insights including the great potential of UALA compared with agent fine-tuning, and underscoring the unreliably of verbalised confidence of LLMs as a proxy for uncertainty.
    
[^33]: Chem-FINESE: 通过文本重构验证细粒度少样本实体提取

    Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])

    [http://arxiv.org/abs/2401.10189](http://arxiv.org/abs/2401.10189)

    这篇论文提出了一种名为Chem-FINESE的方法来处理化学领域中细粒度少样本实体提取的问题。该方法通过使用序列到序列的实体提取器和自我验证模块来从输入句子中提取命名实体并重构原始输入句子。实验证明了该方法的有效性和可行性。

    

    在化学领域中，细粒度少样本实体提取面临两个独特的挑战。首先，与一般领域的实体提取任务相比，化学论文中的句子通常包含更多的实体。此外，实体提取模型通常难以提取长尾类型的实体。在本文中，我们提出了一种新颖的基于序列到序列的少样本实体提取方法Chem-FINESE来解决这两个挑战。我们的Chem-FINESE包含两个组件：一个序列到序列的实体提取器用于从输入句子中提取命名实体，以及一个序列到序列的自我验证模块用于从提取的实体中重构原始输入句子。受到一个好的实体提取系统需要忠实提取实体的事实启发，我们的新自我验证模块利用实体提取结果来重构原始输入句子。此外，我们设计了一种新的对比损失来减少在提取过程中的过度复制。

    Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction proces
    
[^34]: 无需访问逻辑回归的黑盒大型语言模型的草图引导约束解码

    Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models without Logit Access. (arXiv:2401.09967v1 [cs.CL])

    [http://arxiv.org/abs/2401.09967](http://arxiv.org/abs/2401.09967)

    本文介绍了一种无需访问逻辑回归的黑盒大型语言模型的草图引导约束解码的方法，通过利用本地辅助模型优化黑盒语言模型的输出，以初步输出作为进一步扩展的 "草图"，从而提高了有限约束解码的应用能力。

    

    有限约束在语言模型输出的控制上提供了一种不需要重新训练或架构修改的方式，但通常只适用于拥有逻辑回归访问权限的模型，这对于黑盒大型语言模型存在限制。本文引入了一种新颖的基于草图引导的黑盒大型语言模型约束解码（SGCD）方法，无需访问黑盒语言模型的逻辑回归。SGCD利用本地辅助模型来优化无约束黑盒语言模型的输出，将其作为进一步扩展的“草图”。此方法可与传统的基于逻辑回归的技术相互补充，使有限约束解码在无法完全透明的模型环境中应用。通过实验展示了SGCD的有效性。

    Constrained decoding, a technique for enforcing constraints on language model outputs, offers a way to control text generation without retraining or architectural modifications. Its application is, however, typically restricted to models that give users access to next-token distributions (usually via softmax logits), which poses a limitation with blackbox large language models (LLMs). This paper introduces sketch-guided constrained decoding (SGCD), a novel approach to constrained decoding for blackbox LLMs, which operates without access to the logits of the blackbox LLM. SGCD utilizes a locally hosted auxiliary model to refine the output of an unconstrained blackbox LLM, effectively treating this initial output as a "sketch" for further elaboration. This approach is complementary to traditional logit-based techniques and enables the application of constrained decoding in settings where full model transparency is unavailable. We demonstrate the efficacy of SGCD through experiments in cl
    
[^35]: Patchscope: 一个统一的框架，用于检查语言模型的隐藏表示

    Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])

    [http://arxiv.org/abs/2401.06102](http://arxiv.org/abs/2401.06102)

    本论文提出了一个叫做Patchscope的框架，用于检查语言模型的隐藏表示。该框架不仅统一了先前的检查技术，还解决了其中一些问题，并且还开辟了新的可能性。

    

    检查大型语言模型（LLM）的隐藏表示中编码的信息可以解释模型的行为并验证其与人类价值观的一致性。鉴于LLM生成人类可理解文本的能力，我们建议利用模型本身以自然语言解释其内部表示。我们引入了一个称为Patchscopes的框架，并展示了如何使用它来回答关于LLM计算的各种研究问题。我们表明，先前基于将表示投影到词汇空间和干预LLM计算的可解释性方法，可以看作是该框架的特殊实例。此外，通过Patchscope可以弥补优势，如检查早期层失败或表达能力不足。除了统一先前的检查技术，Patchscopes还开辟了新的可能性，例如使用更强大的模型来解释较小模型的表示。

    Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
    
[^36]: LLaMA Pro: 带有模块扩展的渐进LLaMA方法

    LLaMA Pro: Progressive LLaMA with Block Expansion. (arXiv:2401.02415v1 [cs.CL])

    [http://arxiv.org/abs/2401.02415](http://arxiv.org/abs/2401.02415)

    本论文提出了一种新型的LLMs后处理方法，通过扩展Transformer模块并使用新的语料库进行调整，有效地提高了模型知识，而不会发生灾难性遗忘。在代码和数学领域的实验结果表明，LLaMA Pro是一种功能强大且适用于通用任务、编程和数学的基础模型，同时在各种基准测试中取得了先进的性能，展示了智能体推理和解决多样化任务的潜力。

    

    人类通常在不牺牲旧技能的情况下学习新技能；然而，对于大型语言模型（LLMs），例如从LLaMA到CodeLLaMA则相反。为此，我们提出了一种具有Transformer模块扩展的LLMs的新的预训练后处理方法。我们仅使用新的语料库调整扩展的模块，以有效地提高模型的知识而不会发生灾难性遗忘。本文在代码和数学语料库上进行实验，得到了从LLaMA2-7B初始化的通用基础模型LLaMA Pro-8.3B，在常规任务、编程和数学方面表现出色。LLaMA Pro及其按照指令执行的对应模型（LLaMA Pro-Instruct）在各种基准测试中都取得了先进的性能，展示了在LLaMA系列和其他开放模型中的优越性以及作为智能体推理和解决多样化任务的巨大潜力。我们的发现对于整合自然语言和编程语言提供了有价值的见解，为发展推理和解决多样化任务的智能体奠定了基础。

    Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge without catastrophic forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a
    
[^37]: 通过多视角解耦学习改进低资源的基于提示的关系表示

    Improving Low-resource Prompt-based Relation Representation with Multi-view Decoupling Learning. (arXiv:2312.17267v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.17267](http://arxiv.org/abs/2312.17267)

    提出了一种名为MVRE的新方法，通过将关系解耦为不同的视角，生成多视角关系表示，并利用预训练语言模型（PLMs）的能力来提高低资源关系抽取任务的性能。

    

    最近，使用预训练语言模型（PLMs）进行提示调整已经展示出了显著的关系抽取（RE）任务的增强能力。然而，在低资源场景中，即训练数据有限的情况下，由于对关系的表层理解，先前基于提示的方法可能仍然表现不佳，用于表示学习。为此，我们强调在低资源场景中学习高质量关系表示对于RE的重要性，并提出了一种新的基于提示的关系表示方法，名为MVRE（多视角关系抽取），以更好地利用PLMs的能力来改善低资源提示调整范式下的RE性能。具体而言，MVRE将每个关系解耦为不同的视角，以包含多视角的关系表示，以最大化关系推断过程中的似然性。此外，我们还设计了一个全局性的低领域任务学习策略，以进一步提高关系表示的质量。

    Recently, prompt-tuning with pre-trained language models (PLMs) has demonstrated the significantly enhancing ability of relation extraction (RE) tasks. However, in low-resource scenarios, where the available training data is scarce, previous prompt-based methods may still perform poorly for prompt-based representation learning due to a superficial understanding of the relation. To this end, we highlight the importance of learning high-quality relation representation in low-resource scenarios for RE, and propose a novel prompt-based relation representation method, named MVRE (\underline{M}ulti-\underline{V}iew \underline{R}elation \underline{E}xtraction), to better leverage the capacity of PLMs to improve the performance of RE within the low-resource prompt-tuning paradigm. Specifically, MVRE decouples each relation into different perspectives to encompass multi-view relation representations for maximizing the likelihood during relation inference. Furthermore, we also design a Global-Lo
    
[^38]: 关于循环神经网络语言模型的表示能力的研究

    On the Representational Capacity of Recurrent Neural Language Models. (arXiv:2310.12942v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.12942](http://arxiv.org/abs/2310.12942)

    本文研究了基于循环神经网络的语言模型的计算表达性，扩展了图灵完备性结果到概率情况，并提供了上下界分析。

    

    本研究调查了基于循环神经网络(RNNs)的语言模型(LMs)的计算表达性。Siegelmann和Sontag(1992)曾经展示了具有有理权重和隐藏状态以及无限计算时间的RNNs是图灵完备的。然而，LMs不仅定义了字符串上的加权，还定义了(非加权)语言成员关系，对RNN LMs（RLMs）的计算能力分析应该反映这一点。我们将图灵完备性结果扩展到概率情况，展示了如何使用有理权重的RLM和无限计算时间来模拟任何概率图灵机(PTM)。由于在实践中，RLMs实时工作，每个时间步骤处理一个符号，因此我们将上述结果作为RLMs表达性的上界。我们还通过展示在实时计算限制下，这些模型可以模拟确定性实时有理PTMs来提供下界。

    This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM). Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs.
    
[^39]: LLM-集成应用中的提示注入攻击和防御

    Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])

    [http://arxiv.org/abs/2310.12815](http://arxiv.org/abs/2310.12815)

    本文提出了一个通用框架来形式化提示注入攻击，并系统化防御这种类型的攻击。

    

    大型语言模型（LLMs）越来越多地用作各种称为LLM-集成应用的实际应用程序的后端。最近的多项研究表明，LLM-集成应用容易受到提示注入攻击的威胁，攻击者可以将恶意指令/数据注入这些应用程序的输入中，以达到攻击者的预期结果。然而，现有的研究仅限于案例研究，缺乏对提示注入攻击及其防御的系统理解。本论文旨在填补这一空白。我们提出了一个通用框架来形式化提示注入攻击，并将研究论文和博客文章中讨论的现有攻击视为我们框架的特例。我们的框架使我们能够通过组合现有攻击设计新的攻击方式。此外，我们还提出了一个系统化提示注入攻击防御的框架。利用我们的框架，我们可以预防和缓解这种类型的攻击。

    Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we con
    
[^40]: ChatKBQA: 一个基于精调大型语言模型的生成-检索框架用于知识库问答

    ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models. (arXiv:2310.08975v1 [cs.CL])

    [http://arxiv.org/abs/2310.08975](http://arxiv.org/abs/2310.08975)

    ChatKBQA是一个基于精调大型语言模型的生成-检索框架，用于改进知识库问答的效率和准确性，实验结果显示在多个数据集上取得了新的最好表现。

    

    知识库问答（KBQA）旨在通过大规模知识库（KB）获取自然语言问题的答案，通常分为两个研究组成部分：知识检索和语义解析。然而，仍然存在三个核心挑战，包括低效的知识检索、检索错误对语义解析的不利影响以及之前的KBQA方法的复杂性。在大型语言模型（LLM）时代，我们介绍了ChatKBQA，这是一个新颖的基于精调开源LLMs（如Llama-2、ChatGLM2和Baichuan2）构建的生成-检索KBQA框架。ChatKBQA提议首先使用精调的LLMs生成逻辑形式，然后通过无监督检索方法检索和替换实体和关系，从而更直观地改进了生成和检索。实验结果表明，ChatKBQA在标准KBQA数据集WebQSP和ComplexWebQuestions (CWQ)上取得了新的最先进性能。

    Knowledge Base Question Answering (KBQA) aims to derive answers to natural language questions over large-scale knowledge bases (KBs), which are generally divided into two research components: knowledge retrieval and semantic parsing. However, three core challenges remain, including inefficient knowledge retrieval, retrieval errors adversely affecting semantic parsing, and the complexity of previous KBQA methods. In the era of large language models (LLMs), we introduce ChatKBQA, a novel generate-then-retrieve KBQA framework built on fine-tuning open-source LLMs such as Llama-2, ChatGLM2 and Baichuan2. ChatKBQA proposes generating the logical form with fine-tuned LLMs first, then retrieving and replacing entities and relations through an unsupervised retrieval method, which improves both generation and retrieval more straightforwardly. Experimental results reveal that ChatKBQA achieves new state-of-the-art performance on standard KBQA datasets, WebQSP, and ComplexWebQuestions (CWQ). This
    
[^41]: 思考、行动和问：开放世界互动个性化机器人导航

    Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation. (arXiv:2310.07968v1 [cs.RO])

    [http://arxiv.org/abs/2310.07968](http://arxiv.org/abs/2310.07968)

    这项研究引入了零射交互个性化对象导航（ZIPON），通过使用大型语言模型（LLM）和用户反馈，解决了在未知环境中导航到个性化目标对象的问题。

    

    零射命令对象导航（ZSON）使代理能够在未知环境中导航到开放词汇对象。现有的ZSON工作主要集中在遵循个别指令以寻找通用对象类，忽略了自然语言交互的利用和识别用户特定对象的复杂性。为了解决这些局限性，我们引入了零射交互个性化对象导航（ZIPON），其中机器人需要在与用户对话的同时导航到个性化目标对象。为了解决ZIPON问题，我们提出了一种新的框架称为开放世界互动个性化导航（ORION），该框架使用大型语言模型（LLM）进行序列决策，以操作不同的感知、导航和通信模块。实验结果表明，能够利用用户反馈的互动代理的性能有了显著改进。然而，在任务完成和用户满意度之间取得良好的平衡仍然具有挑战性。

    Zero-Shot Object Navigation (ZSON) enables agents to navigate towards open-vocabulary objects in unknown environments. The existing works of ZSON mainly focus on following individual instructions to find generic object classes, neglecting the utilization of natural language interaction and the complexities of identifying user-specific objects. To address these limitations, we introduce Zero-shot Interactive Personalized Object Navigation (ZIPON), where robots need to navigate to personalized goal objects while engaging in conversations with users. To solve ZIPON, we propose a new framework termed Open-woRld Interactive persOnalized Navigation (ORION), which uses Large Language Models (LLMs) to make sequential decisions to manipulate different modules for perception, navigation and communication. Experimental results show that the performance of interactive agents that can leverage user feedback exhibits significant improvement. However, obtaining a good balance between task completion 
    
[^42]: 生成“nice”而不是生成“good”不像生成“rice”那么糟糕！朝着上下文和语义融合的对话生成损失函数和评估指标。

    Hi Model, generating 'nice' instead of 'good' is not as bad as generating 'rice'! Towards Context and Semantic Infused Dialogue Generation Loss Function and Evaluation Metric. (arXiv:2309.05804v1 [cs.CL])

    [http://arxiv.org/abs/2309.05804](http://arxiv.org/abs/2309.05804)

    提出了一种新的对话生成损失函数和评估指标，解决了以往方法中词汇匹配和缺少上下文考虑的限制。

    

    在过去的二十年中，对话建模取得了重大进展，从简单的基于规则的回答发展到个性化和有说服力的回答生成。然而，尽管这些进展，对话生成的目标函数和评估指标仍然停滞不前，分别是交叉熵和BLEU。这些基于词汇的指标存在以下主要限制：(a)没有语义考虑的词对词匹配：它将生成“nice”和生成“rice”作为“good”的失败统一考虑。(b)缺少为评估生成的回答提供上下文属性：即使生成的回答与正在进行的对话上下文相关，如果不与语料库中提供的黄金话语匹配，仍然可能受到惩罚。在本文中，我们首先全面调查这些限制，并提出了一种名为上下文语义融合对话(SemTextualLogue)损失函数的新损失函数。此外，我们还制定了一套新的评估指标，

    Over the past two decades, dialogue modeling has made significant strides, moving from simple rule-based responses to personalized and persuasive response generation. However, despite these advancements, the objective functions and evaluation metrics for dialogue generation have remained stagnant, i.e., cross-entropy and BLEU, respectively. These lexical-based metrics have the following key limitations: (a) word-to-word matching without semantic consideration: It assigns the same credit for failure to generate 'nice' and 'rice' for 'good'. (b) missing context attribute for evaluating the generated response: Even if a generated response is relevant to the ongoing dialogue context, it may still be penalized for not matching the gold utterance provided in the corpus. In this paper, we first investigate these limitations comprehensively and propose a new loss function called Semantic Infused Contextualized diaLogue (SemTextualLogue) loss function. Furthermore, we formulate a new evaluation
    
[^43]: ChatLaw: 基于开源法律大型语言模型的综合外部知识库

    ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases. (arXiv:2306.16092v1 [cs.CL])

    [http://arxiv.org/abs/2306.16092](http://arxiv.org/abs/2306.16092)

    本论文介绍了一个基于开源的法律大型语言模型ChatLaw，它通过综合外部知识库为中国法律领域的数字化转型提供支持。该模型采用了精心设计的法律领域微调数据集，并通过将向量数据库检索与关键词检索相结合的方法解决了法律数据筛选中的模型幻觉问题。同时，引入了一种自注意力方法以增强对文本中关键信息的注意力。

    

    大型语言模型（LLM）已经展示出在各个领域中改变自然语言处理任务的潜力，引发了对垂直特定大模型的极大兴趣。然而，与像BloombergGPT和FinGPT这样利用其独特数据积累在金融领域取得进展的专有模型不同，中国法律领域中没有类似的大型语言模型来促进数字化转型。在本文中，我们提出了一个名为ChatLaw的开源法律大型语言模型。由于数据质量的重要性，我们精心设计了一个法律领域微调数据集。此外，为了解决在参考数据检索过程中法律数据筛选中的模型幻觉问题，我们引入了一种将向量数据库检索与关键词检索相结合的方法，以有效减少只依靠向量数据库检索的不准确性。此外，我们提出了一种自注意力方法来增强模型的对文本中关键信息的注意。

    Large Language Models (LLMs) have shown the potential to revolutionize natural language processing tasks in various domains, sparking great interest in vertical-specific large models. However, unlike proprietary models such as BloombergGPT and FinGPT, which have leveraged their unique data accumulations to make strides in the finance domain, there hasn't not many similar large language models in the Chinese legal domain to facilitate its digital transformation.  In this paper, we propose an open-source legal large language model named ChatLaw. Due to the importance of data quality, we carefully designed a legal domain fine-tuning dataset. Additionally, to overcome the problem of model hallucinations in legal data screening during reference data retrieval, we introduce a method that combines vector database retrieval with keyword retrieval to effectively reduce the inaccuracy of relying solely on vector database retrieval. Furthermore, we propose a self-attention method to enhance the a
    
[^44]: 通过数据混合消除预训练模型中的虚假相关性

    Eliminating Spurious Correlations from Pre-trained Models via Data Mixing. (arXiv:2305.14521v1 [cs.LG])

    [http://arxiv.org/abs/2305.14521](http://arxiv.org/abs/2305.14521)

    本文提出了一种通过数据混合来消除预训练模型中虚假相关性的方法，来提高模型对于新样本的预测能力。这种方法经过理论证明和多种任务实验验证，可以取得良好的效果。

    

    在大数据集上预训练的机器学习模型取得了显著的收敛性和鲁棒性。然而，这些模型往往利用了某些属性和标签之间的虚假相关性，在特定类别的大多数示例中普遍存在，但并不足以预测这些类别。学到的虚假相关性可能会在对新数据进行微调后仍然存在，这会降低模型对不展现虚假相关性的示例的性能。本文提出了一种简单而高效的方法，以消除预训练模型中的虚假相关性。我们方法的关键思想是利用一小组带有虚假属性的示例，并通过数据混合来平衡所有类别中的虚假属性。我们在理论上证实了我们的方法的有效性，并在各种视觉和NLP任务上进行了实证，包括消除虚假相关性。

    Machine learning models pre-trained on large datasets have achieved remarkable convergence and robustness properties. However, these models often exploit spurious correlations between certain attributes and labels, which are prevalent in the majority of examples within specific categories but are not predictive of these categories in general. The learned spurious correlations may persist even after fine-tuning on new data, which degrades models' performance on examples that do not exhibit the spurious correlation. In this work, we propose a simple and highly effective method to eliminate spurious correlations from pre-trained models. The key idea of our method is to leverage a small set of examples with spurious attributes, and balance the spurious attributes across all classes via data mixing. We theoretically confirm the effectiveness of our method, and empirically demonstrate its state-of-the-art performance on various vision and NLP tasks, including eliminating spurious correlation
    

