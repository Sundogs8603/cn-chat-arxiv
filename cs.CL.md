# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation.](http://arxiv.org/abs/2311.00684) | 本研究提出了两种通过温度缩放实现的注意力对齐策略，通过改善T5在长序列处理中的注意力分布问题，提高了其在语言建模、检索和多文档问答等任务中的长上下文利用能力。 |
| [^2] | [Machine Translation for Nko: Tools, Corpora and Baseline Results.](http://arxiv.org/abs/2310.15612) | 该论文提出了针对Nko语（一种在多个西非国家使用的语言）开发可用的机器翻译系统的一套工具、资源和基准结果，包括新颖的协作平行文本整理软件、扩展的语料库和基线神经机器翻译结果。 |
| [^3] | [MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation.](http://arxiv.org/abs/2310.14088) | MedEval是一个多层次、多任务和多领域的医学基准，用于促进医疗保健语言模型的开发。它包含了来自多个医疗系统的数据，涵盖了多个人体区域和检查模式。我们针对10个语言模型进行了评估，发现不同模型在不同任务上的有效性各有差异，从中我们注意到了指导的重要性。 |
| [^4] | [Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs' Non-linear Thinking.](http://arxiv.org/abs/2310.12342) | 本文提出了一种名为推断性排除提示（IEP）的新框架，通过结合排除和推理的原则，引导LLM进行非线性思考。IEP通过规划和自然语言推理，可以模拟复杂的人类思维过程，比其他方法具有更广泛的视角。 |
| [^5] | [VIBE: Topic-Driven Temporal Adaptation for Twitter Classification.](http://arxiv.org/abs/2310.10191) | VIBE是一种解决Twitter分类中语言特征演变问题的模型，通过建模潜在主题演变以适应动态环境，并且在大规模Twitter实验中展现了良好的性能。 |
| [^6] | [Fine-grained Conversational Decoding via Isotropic and Proximal Search.](http://arxiv.org/abs/2310.08130) | 本论文提出了一种细粒度的对话解码方法，通过各向同性和近端搜索（IPS）生成信息集中的语义回应，并在对话领域的评估中取得了优于现有方法的效果。 |
| [^7] | [Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model.](http://arxiv.org/abs/2310.02971) | 这篇论文介绍了为自监督编码器-解码器语音模型进行提示和适配器调优的方法，并展示了在序列生成和跨语言ASR任务上的优越表现，尤其在低资源情况下提示方法优于适配器调优。 |
| [^8] | [The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising "Alignment" in Large Language Models.](http://arxiv.org/abs/2310.02457) | 本文通过后结构主义社会政治理论的视角，探讨了大型语言模型(LLMs)中的“对齐”概念，并提出了一个框架来明确操作化这种抽象概念，促进透明和批判性评估的文化。 |
| [^9] | [When Do Program-of-Thoughts Work for Reasoning?.](http://arxiv.org/abs/2308.15452) | 提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。 |
| [^10] | [The Bias Amplification Paradox in Text-to-Image Generation.](http://arxiv.org/abs/2308.00755) | 本文研究了文本到图像生成中的偏见放大现象，并发现其主要原因是训练数据和模型提示之间的差异。一旦考虑到各种分布差异，偏见放大现象显著减少。 |
| [^11] | [CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models.](http://arxiv.org/abs/2307.07705) | CPET提出了一种基于压缩LLM的有效参数优化框架，通过引入知识继承和恢复策略，解决了在参数有效调整中压缩LLM的推理计算瓶颈问题。 |
| [^12] | [$FPDM$: Domain-Specific Fast Pre-training Technique using Document-Level Metadata.](http://arxiv.org/abs/2306.06190) | 本文提出了$FPDM$，使用文档元数据和领域特定分类作为监督信号，对领域特定语料库进行transformer编码器的预训练。$FPDM$通过句子级别的输入预训练开放领域的编码器，在微调时使用词汇级别的输入，性能优于其他基于transformer的模型。 |
| [^13] | [MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models.](http://arxiv.org/abs/2305.19011) | 这篇论文提出了一个名为MiniSUPERB的轻量级基准测试，可以有效地评估自监督语音模型的性能，并在计算成本上比SUPERB更低。同时，该论文还研究了在少样本情况下评估SSL语音模型的性能变化。 |
| [^14] | [Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality.](http://arxiv.org/abs/2305.14658) | 本文重点研究使用大型语言模型（LLMs）为基础的无参考评估器的局限性，并构建了两个对抗元评估对话生成数据集，以全面评估这些评估器的可靠性。 |
| [^15] | [Selectively Answering Ambiguous Questions.](http://arxiv.org/abs/2305.14613) | 本研究调查了解决模糊问题的方法，通过定量测量模型输出中的重复性，找出了在含糊问题集中回答高精度子集问题的最可靠方法。这种基于采样的方法可以有效解决问题回答中的歧义，并提高语言模型的可靠性。 |
| [^16] | [Prompt position really matters in few-shot and zero-shot NLU tasks.](http://arxiv.org/abs/2305.14493) | 该论文通过实证研究发现，提示位置对于少样本和零样本任务的模型性能具有实质性影响，先前研究中使用的提示位置通常是次优的，提示位置优化应成为重要的研究方向。 |
| [^17] | [Are Large Language Models Robust Zero-shot Coreference Resolvers?.](http://arxiv.org/abs/2305.14489) | 本文研究了零-shot共指解析技术在复杂语言环境下的应用，并证明指令调整的语言模型具有鲁棒性零 shot 普适性。 |
| [^18] | [GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study.](http://arxiv.org/abs/2305.13062) | 本文设计了一个基准测试来评估大型语言模型（LLMs）对结构化表格数据的理解能力，并发现不同的输入选择会对性能产生影响。在基准测试的基础上，提出了“自我增强”技术以改善理解能力。 |
| [^19] | [Accurate Knowledge Distillation with n-best Reranking.](http://arxiv.org/abs/2305.12057) | 该论文提出了一种基于n-best重排序的知识蒸馏方法，通过使用多种模型提供伪标签，训练出参数更少但精度相当的学生模型。 |
| [^20] | [Explaining black box text modules in natural language with language models.](http://arxiv.org/abs/2305.09863) | 本文介绍了一种名为Summarize and Score（SASC）的方法，该方法可以自动获取黑盒文本模块的自然语言解释以及解释可靠程度的分数。研究者们已经在合成模块和BERT模型中使用SASC，让我们可以解释模块的选择性，这对于增强大型语言模型的可解释性非常重要。 |
| [^21] | [NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge.](http://arxiv.org/abs/2305.04978) | 本文提出了一种新的用于比较知识提炼的神经符号框架，名称为NeuroComparatives。该框架利用词汇约束解码进行比较知识提炼，以及对生成的知识进行严格过滤。 |
| [^22] | [Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs.](http://arxiv.org/abs/2305.03111) | 本文提出了一个大型的基准测试Bird，可以用于大规模数据库文本到SQL的任务，突出了数据库值理解和SQL效率等领域的挑战。 |
| [^23] | [Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing.](http://arxiv.org/abs/2304.02017) | 本文全面探讨了ChatGPT在自然语言处理中的应用、优点和局限性，强调了使用这个强大工具时的道德考虑，为人工智能和NLP领域的讨论做出了贡献。 |
| [^24] | [Editing Language Model-based Knowledge Graph Embeddings.](http://arxiv.org/abs/2301.10405) | 本文提出了一种新的任务——编辑基于语言模型的知识图谱嵌入，旨在实现对KG嵌入的数据高效和快速更新。针对这一任务，提出了一个简单而强大的方案——KGEditor，可以更好地更新特定事实而不影响其余部分的性能。 |
| [^25] | [Deanthropomorphising NLP: Can a Language Model Be Conscious?.](http://arxiv.org/abs/2211.11483) | 本文讨论了关于使用Transformer架构的预训练语言模型LaMDA是否具有意识的说法。作者认为语言模型不可能具有意识，而LaMDA没有比其他类似模型更具先进性。 |
| [^26] | [The Causal Structure of Semantic Ambiguities.](http://arxiv.org/abs/2206.06807) | 本文使用Gogioso和Pinzani在QPL 2021中提出的束理论模型，为语义歧义的两个特征（不同可能解释的联合可信度和某些词在过程中扮演更重要角色的因果结构）进行建模。通过对心理语言学文献中的歧义短语数据集进行分析，研究人员对人类对于这些歧义的判断进行了实证测量。 |

# 详细

[^1]: 注意力对齐和灵活的位置嵌入提高了Transformer长度外推能力

    Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation. (arXiv:2311.00684v1 [cs.CL])

    [http://arxiv.org/abs/2311.00684](http://arxiv.org/abs/2311.00684)

    本研究提出了两种通过温度缩放实现的注意力对齐策略，通过改善T5在长序列处理中的注意力分布问题，提高了其在语言建模、检索和多文档问答等任务中的长上下文利用能力。

    

    理想的长度可外推的Transformer语言模型可以处理比训练长度更长的序列而不需要进行长序列微调。这种长上下文利用能力高度依赖于灵活的位置嵌入设计。在调查现有大型预训练Transformer语言模型的灵活性时，我们发现T5系列值得更仔细研究，因为它的位置嵌入捕捉到了丰富而灵活的注意力模式。然而，T5存在着分散的注意力问题：输入序列越长，注意力分布就越平坦。为了缓解这个问题，我们提出了两种通过温度缩放实现的注意力对齐策略。我们的研究结果提高了T5在语言建模、检索和多文档问答方面的长上下文利用能力，而且不需要进行任何微调，这表明灵活的位置嵌入设计和注意力对齐对于Transformer长度外推至关重要。

    An ideal length-extrapolatable Transformer language model can handle sequences longer than the training length without any long sequence fine-tuning. Such long-context utilization capability highly relies on a flexible positional embedding design. Upon investigating the flexibility of existing large pre-trained Transformer language models, we find that the T5 family deserves a closer look, as its positional embeddings capture rich and flexible attention patterns. However, T5 suffers from the dispersed attention issue: the longer the input sequence, the flatter the attention distribution. To alleviate the issue, we propose two attention alignment strategies via temperature scaling. Our findings improve the long-context utilization capability of T5 on language modeling, retrieval, and multi-document question answering without any fine-tuning, suggesting that a flexible positional embedding design and attention alignment go a long way toward Transformer length extrapolation.\footnote{\url
    
[^2]: Nko语的机器翻译：工具、语料库和基准结果

    Machine Translation for Nko: Tools, Corpora and Baseline Results. (arXiv:2310.15612v1 [cs.CL])

    [http://arxiv.org/abs/2310.15612](http://arxiv.org/abs/2310.15612)

    该论文提出了针对Nko语（一种在多个西非国家使用的语言）开发可用的机器翻译系统的一套工具、资源和基准结果，包括新颖的协作平行文本整理软件、扩展的语料库和基线神经机器翻译结果。

    

    目前，尼科语（一种在多个西非国家使用的语言）没有可用的机器翻译系统，但它在文化和教育价值上具有重要意义。为了解决这个问题，我们提出了一套工具、资源和基准结果，旨在开发可用的尼科语和其他当前没有足够大的平行文本语料库的语言的机器翻译系统。具体包括：(1) Friallel：一种新颖的协作平行文本整理软件，通过基于副本编辑的工作流程实现质量控制。(2) 扩展了FLoRes-200和NLLB-Seed语料库，从其他语言中与尼科语平行翻译了2,009和6,193个高质量的文本。(3) nicolingua-0005：包含130,850个平行片段的三语和双语语料库，以及超过3百万尼科语单语言语料库。(4) 基线双语和多语言神经机器翻译结果与b...

    Currently, there is no usable machine translation system for Nko, a language spoken by tens of millions of people across multiple West African countries, which holds significant cultural and educational value. To address this issue, we present a set of tools, resources, and baseline results aimed towards the development of usable machine translation systems for Nko and other languages that do not currently have sufficiently large parallel text corpora available. (1) Friallel: A novel collaborative parallel text curation software that incorporates quality control through copyedit-based workflows. (2) Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193 high-quality Nko translations in parallel with 204 and 40 other languages. (3) nicolingua-0005: A collection of trilingual and bilingual corpora with 130,850 parallel segments and monolingual corpora containing over 3 million Nko words. (4) Baseline bilingual and multilingual neural machine translation results with the b
    
[^3]: MedEval: 一个多层次、多任务和多领域的医学语言模型评估基准

    MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation. (arXiv:2310.14088v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.14088](http://arxiv.org/abs/2310.14088)

    MedEval是一个多层次、多任务和多领域的医学基准，用于促进医疗保健语言模型的开发。它包含了来自多个医疗系统的数据，涵盖了多个人体区域和检查模式。我们针对10个语言模型进行了评估，发现不同模型在不同任务上的有效性各有差异，从中我们注意到了指导的重要性。

    

    由于需要专家的人工注释，医疗保健的筛选数据集往往有限。本文提出了MedEval，一个多层次、多任务和多领域的医学基准，以促进医疗保健语言模型的开发。MedEval是全面的，包含来自几个医疗系统的数据，涵盖了8种检查模式下的35个人体区域。我们收集了22,779个句子和21,228份报告，并在多个层次上提供了专家注释，为数据提供了细致的潜在用法，并支持广泛的任务范围。此外，我们在零-shot和微调设置下对10个通用和领域特定的语言模型进行了系统评估，从医疗保健中的领域适应基线到通用的最先进的大型语言模型（如ChatGPT）。我们的评估揭示了两种类别的语言模型在不同任务中的不同有效性，从中我们注意到了指导的重要性。

    Curated datasets for healthcare are often limited due to the need of human annotations from experts. In this paper, we present MedEval, a multi-level, multi-task, and multi-domain medical benchmark to facilitate the development of language models for healthcare. MedEval is comprehensive and consists of data from several healthcare systems and spans 35 human body regions from 8 examination modalities. With 22,779 collected sentences and 21,228 reports, we provide expert annotations at multiple levels, offering a granular potential usage of the data and supporting a wide range of tasks. Moreover, we systematically evaluated 10 generic and domain-specific language models under zero-shot and finetuning settings, from domain-adapted baselines in healthcare to general-purposed state-of-the-art large language models (e.g., ChatGPT). Our evaluations reveal varying effectiveness of the two categories of language models across different tasks, from which we notice the importance of instruction t
    
[^4]: 通过推理与规划消除推理：一种引导LLMs非线性思维的新框架

    Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs' Non-linear Thinking. (arXiv:2310.12342v1 [cs.CL])

    [http://arxiv.org/abs/2310.12342](http://arxiv.org/abs/2310.12342)

    本文提出了一种名为推断性排除提示（IEP）的新框架，通过结合排除和推理的原则，引导LLM进行非线性思考。IEP通过规划和自然语言推理，可以模拟复杂的人类思维过程，比其他方法具有更广泛的视角。

    

    Thought Chain（CoT）提示及其变体通过模拟人类线性认知和逻辑，探索为大型语言模型（LLM）装备高级推理能力。然而，人类思维复杂且混合线性和非线性思维。在这项工作中，我们提出了一种新的提示方式，称为推断性排除提示（IEP），它结合了排除和推理的原则，以引导LLM进行非线性思考。IEP指导LLM进行规划，并利用自然语言推理（NLI）推断每个可能解与上下文、常识或事实的推理关系，从而通过回溯推理获得更广泛的视角。相比其他基于CoT的方法，IEP的前向规划和后向排除过程更好地模拟了复杂的人类思维过程，后者仅反映线性认知过程。我们进行了一系列的实证研究，并验证了IEP的优势。

    Chain-of-Thought(CoT) prompting and its variants explore equipping large language models (LLMs) with high-level reasoning abilities by emulating human-like linear cognition and logic. However, the human mind is complicated and mixed with both linear and nonlinear thinking. In this work, we propose \textbf{I}nferential \textbf{E}xclusion \textbf{P}rompting (IEP), a novel prompting that combines the principles of elimination and inference in order to guide LLMs to think non-linearly. IEP guides LLMs to plan and then utilize Natural Language Inference (NLI) to deduce each possible solution's entailment relation with context, commonsense, or facts, therefore yielding a broader perspective by thinking back for inferring. This forward planning and backward eliminating process allows IEP to better simulate the complex human thinking processes compared to other CoT-based methods, which only reflect linear cognitive processes. We conducted a series of empirical studies and have corroborated tha
    
[^5]: VIBE：Twitter分类的主题驱动时间自适应

    VIBE: Topic-Driven Temporal Adaptation for Twitter Classification. (arXiv:2310.10191v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10191](http://arxiv.org/abs/2310.10191)

    VIBE是一种解决Twitter分类中语言特征演变问题的模型，通过建模潜在主题演变以适应动态环境，并且在大规模Twitter实验中展现了良好的性能。

    

    语言特征在现实世界的社交媒体中不断变化，导致文本分类在动态环境下的性能下降。为了解决这个挑战，我们研究了时间自适应，即在过去数据上训练的模型在未来进行测试。先前的大部分工作都集中在继续预训练或知识更新上，这可能会影响它们在噪声社交媒体数据上的性能。为了解决这个问题，我们通过建模潜在主题演变来反映特征变化，并提出了一种新的模型VIBE：Evolutions的变分信息瓶颈。具体而言，我们首先使用两个信息瓶颈(Bottleneck)正则化器来区分过去和未来的主题。然后，这些区分的主题通过时间戳和类别标签预测进行多任务训练，作为自适应特征。在自适应学习过程中，VIBE利用训练数据时间之后创建的在线流程中检索到的无标签数据。在三个分类任务的大规模Twitter实验中，我们的方法展示了良好的性能。

    Language features are evolving in real-world social media, resulting in the deteriorating performance of text classification in dynamics. To address this challenge, we study temporal adaptation, where models trained on past data are tested in the future. Most prior work focused on continued pretraining or knowledge updating, which may compromise their performance on noisy social media data. To tackle this issue, we reflect feature change via modeling latent topic evolution and propose a novel model, VIBE: Variational Information Bottleneck for Evolutions. Concretely, we first employ two Information Bottleneck (IB) regularizers to distinguish past and future topics. Then, the distinguished topics work as adaptive features via multi-task training with timestamp and class label prediction. In adaptive learning, VIBE utilizes retrieved unlabeled data from online streams created posterior to training data time. Substantial Twitter experiments on three classification tasks show that our mode
    
[^6]: 通过各向同性和近端搜索实现细粒度对话解码

    Fine-grained Conversational Decoding via Isotropic and Proximal Search. (arXiv:2310.08130v1 [cs.CL])

    [http://arxiv.org/abs/2310.08130](http://arxiv.org/abs/2310.08130)

    本论文提出了一种细粒度的对话解码方法，通过各向同性和近端搜索（IPS）生成信息集中的语义回应，并在对话领域的评估中取得了优于现有方法的效果。

    

    通常采用通用的文本解码方法来进行对话回应生成。虽然采用了对话特定的编码方法可以提高生成的回应质量，但对话解码方法仍然未被充分探索。受到wu2023learning的启发，认为好的对话特征空间应遵循局部性和各向同性规则，我们提出了一种细粒度的对话解码方法，称为各向同性和近端搜索（IPS）。我们的方法旨在生成信息集中的语义回应，同时保持对上下文的信息量和区分度。实验证明，我们的方法在对话领域中的自动评估和人工评估指标上优于现有的解码策略。更深入的分析进一步证实了我们方法的有效性。

    General-purpose text decoding approaches are usually adopted for dialogue response generation. Although the quality of the generated responses can be improved with dialogue-specific encoding methods, conversational decoding methods are still under-explored. Inspired by \citet{wu2023learning} that a good dialogue feature space should follow the rules of locality and isotropy, we present a fine-grained conversational decoding method, termed \textit{isotropic and proximal search (IPS)}. Our method is designed to generate the semantic-concentrated response, while still maintaining informativeness and discrimination against the context. Experiments show that our approach outperforms existing decoding strategies in the dialogue field across both automatic and human evaluation metrics. More in-depth analyses further confirm the effectiveness of our approach.
    
[^7]: 为自监督编码器-解码器语音模型进行提示和适配器调优的方法

    Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model. (arXiv:2310.02971v1 [eess.AS])

    [http://arxiv.org/abs/2310.02971](http://arxiv.org/abs/2310.02971)

    这篇论文介绍了为自监督编码器-解码器语音模型进行提示和适配器调优的方法，并展示了在序列生成和跨语言ASR任务上的优越表现，尤其在低资源情况下提示方法优于适配器调优。

    

    提示和适配器调优已经成为细调（FT）方法的有效替代品。然而，现有的关于语音提示的研究主要集中在分类任务上，并在更复杂的序列生成任务上失败。此外，适配器调优主要应用于仅编码器的自监督模型。我们的实验证明，在Wav2Seq这个自监督的编码器-解码器模型上进行提示，超过了以前在序列生成任务上的研究成果。它在ASR的词错误率上实现了53％的相对改进，在槽填充的F1分数上实现了27％的改进。此外，在低资源情况下提示方法与FT方法相竞争。此外，我们展示了在Wav2Seq上通过提示和适配器调优实现的跨语言ASR的可转移性。当涉及有限的可训练参数时，提示和适配器调优始终优于传统的FT方法在7种语言中的表现。值得注意的是，在低资源情况下，提示方法始终优于适配器调优。

    Prompting and adapter tuning have emerged as efficient alternatives to fine-tuning (FT) methods. However, existing studies on speech prompting focused on classification tasks and failed on more complex sequence generation tasks. Besides, adapter tuning is primarily applied with a focus on encoder-only self-supervised models. Our experiments show that prompting on Wav2Seq, a self-supervised encoder-decoder model, surpasses previous works in sequence generation tasks. It achieves a remarkable 53% relative improvement in word error rate for ASR and a 27% in F1 score for slot filling. Additionally, prompting competes with the FT method in the low-resource scenario. Moreover, we show the transferability of prompting and adapter tuning on Wav2Seq in cross-lingual ASR. When limited trainable parameters are involved, prompting and adapter tuning consistently outperform conventional FT across 7 languages. Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning.
    
[^8]: 空符号问题：朝向更清晰的范式来解决大型语言模型中的“对齐”问题

    The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising "Alignment" in Large Language Models. (arXiv:2310.02457v1 [cs.CL])

    [http://arxiv.org/abs/2310.02457](http://arxiv.org/abs/2310.02457)

    本文通过后结构主义社会政治理论的视角，探讨了大型语言模型(LLMs)中的“对齐”概念，并提出了一个框架来明确操作化这种抽象概念，促进透明和批判性评估的文化。

    

    本文通过后结构主义社会政治理论的视角，探讨了大型语言模型(LLMs)中的“对齐”概念，并特别研究了其与空符号的相似之处。为了在经验数据集中明确操作化抽象对齐概念的共享词汇，我们提出了一个框架，界定了：1）哪些模型行为维度被认为重要，然后2）如何赋予这些维度的含义和定义，并由谁确定。我们对现有的经验文献进行了定位，并提供了在决定要遵循哪种范式方面的指导。通过这个框架，我们旨在培养透明和批判性评估的文化，帮助社区在处理将LLMs与人类群体对齐的复杂性时进行导航。

    In this paper, we address the concept of "alignment" in large language models (LLMs) through the lens of post-structuralist socio-political theory, specifically examining its parallels to empty signifiers. To establish a shared vocabulary around how abstract concepts of alignment are operationalised in empirical datasets, we propose a framework that demarcates: 1) which dimensions of model behaviour are considered important, then 2) how meanings and definitions are ascribed to these dimensions, and by whom. We situate existing empirical literature and provide guidance on deciding which paradigm to follow. Through this framework, we aim to foster a culture of transparency and critical evaluation, aiding the community in navigating the complexities of aligning LLMs with human populations.
    
[^9]: 什么时候编程思维对推理起作用?

    When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])

    [http://arxiv.org/abs/2308.15452](http://arxiv.org/abs/2308.15452)

    提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。

    

    大型语言模型（LLM）的推理能力在体现出人工智能领域中起着关键作用。尽管像编程思维提示这样的方法对于使用编程语言来解决复杂推理任务的LLM非常有效，但代码数据对推理能力的具体影响仍未充分探索。为了填补这一空白，我们提出了复杂性影响推理分数（CIRS），它结合了结构和逻辑属性，以衡量代码和推理能力之间的相关性。具体而言，我们使用抽象语法树来编码结构信息，并通过考虑难度和圈复杂度来计算逻辑复杂性。通过实证分析，我们发现并非所有复杂性的代码数据都可以被LLM学习或理解。最佳复杂性水平对于通过编程辅助提示改善推理能力至关重要。然后我们设计了一个自动合成的方法...

    The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
    
[^10]: 文本到图像生成中的偏见放大悖论

    The Bias Amplification Paradox in Text-to-Image Generation. (arXiv:2308.00755v1 [cs.LG])

    [http://arxiv.org/abs/2308.00755](http://arxiv.org/abs/2308.00755)

    本文研究了文本到图像生成中的偏见放大现象，并发现其主要原因是训练数据和模型提示之间的差异。一旦考虑到各种分布差异，偏见放大现象显著减少。

    

    偏见放大是一种模型增加训练数据中不平衡的现象。本文通过使用稳定扩散来比较训练数据与生成图像中的性别比例，研究了文本到图像领域中的偏见放大现象。我们发现模型似乎放大了训练数据中存在的性别-职业偏见。然而，我们发现放大很大程度上可以归因于训练数据和模型提示之间的差异。例如，训练数据中的标题通常包含明确的性别信息，而我们使用的提示则不包含，这导致了分布的偏移，从而影响了偏见度量。一旦我们考虑到训练和生成时使用的文本之间的各种分布差异，我们观察到放大现象大大减少。我们的发现说明了比较模型和它们所训练的数据中的偏见所面临的挑战，并且强调了混淆因素。

    Bias amplification is a phenomenon in which models increase imbalances present in the training data. In this paper, we study bias amplification in the text-to-image domain using Stable Diffusion by comparing gender ratios in training vs. generated images. We find that the model appears to amplify gender-occupation biases found in the training data (LAION). However, we discover that amplification can largely be attributed to discrepancies between training captions and model prompts. For example, an inherent difference is that captions from the training data often contain explicit gender information while the prompts we use do not, which leads to a distribution shift and consequently impacts bias measures. Once we account for various distributional differences between texts used for training and generation, we observe that amplification decreases considerably. Our findings illustrate the challenges of comparing biases in models and the data they are trained on, and highlight confounding 
    
[^11]: CPET: 高效压缩大型语言模型的参数优化

    CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models. (arXiv:2307.07705v1 [cs.CL])

    [http://arxiv.org/abs/2307.07705](http://arxiv.org/abs/2307.07705)

    CPET提出了一种基于压缩LLM的有效参数优化框架，通过引入知识继承和恢复策略，解决了在参数有效调整中压缩LLM的推理计算瓶颈问题。

    

    近年来，参数有效调整（PET）因为在调整相对较少的参数（PET模块）的同时仍能激活大型语言模型（LLM）的足够知识以用于下游任务而得到广泛研究。此外，当PET用于为多个任务提供服务时，可以在冻结的LLM上构建不同的任务特定PET模块，避免冗余LLM部署。虽然PET显著降低了调优和部署LLM的成本，但其推理仍然受到LLM计算瓶颈的影响。为了解决上述问题，我们提出了一种基于压缩LLM的有效PET框架，称为“CPET”。在CPET中，我们评估了主流LLM压缩技术对PET性能的影响，然后引入了知识继承和恢复策略来恢复由这些压缩技术引起的知识丢失。我们的实验结果表明，由于CPET的恢复策略，合作

    Parameter-efficient tuning (PET) has been widely explored in recent years because it tunes much fewer parameters (PET modules) than full-parameter fine-tuning (FT) while still stimulating sufficient knowledge from large language models (LLMs) for downstream tasks. Moreover, when PET is employed to serve multiple tasks, different task-specific PET modules can be built on a frozen LLM, avoiding redundant LLM deployments. Although PET significantly reduces the cost of tuning and deploying LLMs, its inference still suffers from the computational bottleneck of LLMs. To address the above issue, we propose an effective PET framework based on compressed LLMs, named "CPET". In CPET, we evaluate the impact of mainstream LLM compression techniques on PET performance and then introduce knowledge inheritance and recovery strategies to restore the knowledge loss caused by these compression techniques. Our experimental results demonstrate that, owing to the restoring strategies of CPET, collaborating
    
[^12]: 使用文档级元数据的领域特定快速预训练技术$FPDM$

    $FPDM$: Domain-Specific Fast Pre-training Technique using Document-Level Metadata. (arXiv:2306.06190v1 [cs.CL])

    [http://arxiv.org/abs/2306.06190](http://arxiv.org/abs/2306.06190)

    本文提出了$FPDM$，使用文档元数据和领域特定分类作为监督信号，对领域特定语料库进行transformer编码器的预训练。$FPDM$通过句子级别的输入预训练开放领域的编码器，在微调时使用词汇级别的输入，性能优于其他基于transformer的模型。

    

    在各种领域的预训练已显示出在开放领域和领域特定下游任务上具有良好的结果。然而，最先进的transformers需要大量的预训练数据和计算资源。在本文中，我们提出了$FPDM$（Fast Pre-training Technique using Document Level Metadata），这是一个新颖、计算效率高的框架，利用文档元数据和领域特定的分类作为监督信号，对领域特定语料库进行transformer编码器的预训练。最主要的创新在于，在领域特定的预训练过程中，使用句子级别的嵌入作为输入，持续对开放领域的编码器进行预训练（以适应长文档），但在对该编码器进行微调时，则使用词汇级别嵌入作为输入。实验表明，$FPDM$在客户支持、科学和法律等领域的字符级F1分数和其他自动化指标方面优于几种基于transformer的基准，且在下游任务微调后性能下降可以忽略不计。

    Pre-training Transformers has shown promising results on open-domain and domain-specific downstream tasks. However, state-of-the-art Transformers require an unreasonably large amount of pre-training data and compute. In this paper, we propose $FPDM$ (Fast Pre-training Technique using Document Level Metadata), a novel, compute-efficient framework that utilizes Document metadata and Domain-Specific Taxonomy as supervision signals to pre-train transformer encoder on a domain-specific corpus. The main innovation is that during domain-specific pretraining, an open-domain encoder is continually pre-trained using sentence-level embeddings as inputs (to accommodate long documents), however, fine-tuning is done with token-level embeddings as inputs to this encoder. We show that $FPDM$ outperforms several transformer-based baselines in terms of character-level F1 scores and other automated metrics in the Customer Support, Scientific, and Legal Domains, and shows a negligible drop in performance 
    
[^13]: MiniSUPERB:轻量级自监督语音模型基准测试

    MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models. (arXiv:2305.19011v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2305.19011](http://arxiv.org/abs/2305.19011)

    这篇论文提出了一个名为MiniSUPERB的轻量级基准测试，可以有效地评估自监督语音模型的性能，并在计算成本上比SUPERB更低。同时，该论文还研究了在少样本情况下评估SSL语音模型的性能变化。

    

    SUPERB被提出用于评估自监督学习（SSL）语音模型在各种任务上的泛化能力。然而，由于大型数据集和多样化任务，它导致了高计算成本。在本文中，我们引入了MiniSUPERB，一个轻量级基准测试，它以明显更低的计算成本有效地评估SSL语音模型并且结果可与SUPERB相比。我们精选代表性任务，采样数据集，并离线提取模型表示。我们的方法与SUPERB Paper和SUPERB Challenge分别达到0.954和0.982的斯皮尔曼等级相关系数。此外，我们在乘-累积操作（MACs）方面减少了97％的计算成本。此外，我们在少样本情况下评估SSL语音模型，并观察到其性能有显著变化。据我们所知，这是第一项研究同时考虑模型本身的计算成本和在基准测试上评估的成本。

    SUPERB was proposed to evaluate the generalizability of self-supervised learning (SSL) speech models across various tasks. However, it incurs high computational costs due to the large datasets and diverse tasks. In this paper, we introduce MiniSUPERB, a lightweight benchmark that efficiently evaluates SSL speech models with comparable results to SUPERB but lower computational costs significantly. We carefully select representative tasks, sample datasets, and extract model representations offline. Our approach achieves a Spearman's rank correlation of 0.954 and 0.982 with SUPERB Paper and SUPERB Challenge, respectively. Additionally, we reduce the computational cost by 97% in terms of Multiply-ACcumulate operations (MACs). Furthermore, we evaluate SSL speech models in few-shot scenarios and observe significant variations in their performance. To our knowledge, this is the first study to examine both the computational cost of the model itself and the cost of evaluating it on a benchmark.
    
[^14]: 无法评估的生成响应质量的评估: Evaluate What You Can't Evaluate

    Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality. (arXiv:2305.14658v1 [cs.CL])

    [http://arxiv.org/abs/2305.14658](http://arxiv.org/abs/2305.14658)

    本文重点研究使用大型语言模型（LLMs）为基础的无参考评估器的局限性，并构建了两个对抗元评估对话生成数据集，以全面评估这些评估器的可靠性。

    

    大型语言模型（LLMs）如ChatGPT已经展现出惊人的语言理解和生成能力。虽然以LLMs为基础的无参考评估器比传统基于参考文献的评估器显示出更好的人类语义对齐度，但是在使用以LLMs为基础的无参考评估器时仍然存在很多挑战。无参考评估器更适用于具有不同语义响应的开放式例子。但并不是所有的例子都是开放式的，对于具有唯一正确语义响应的闭合式例子，如果给出与事实和参考的语义不一致的响应，无参考评估器仍然会认为其具有高质量。为了全面评估以LLMs为基础的评估器的可靠性，我们构建了两个对抗元评估对话生成数据集KdConv-ADV和DSTC7-ADV基于KdConv和DSTC7-AVSD。与以前的元评估基准相比，KdConv-ADV和DSTC7-ADV更具挑战性。

    LLMs (large language models) such as ChatGPT have shown remarkable language understanding and generation capabilities. Although reference-free evaluators based on LLMs show better human alignment than traditional reference-based evaluators, there are many challenges in using reference-free evaluators based on LLMs. Reference-free evaluators are more suitable for open-ended examples with different semantics responses. But not all examples are open-ended. For closed-ended examples with unique correct semantic response, reference-free evaluators will still consider it high quality when giving a response that is inconsistent with the facts and the semantic of reference. In order to comprehensively evaluate the reliability of evaluators based on LLMs, we construct two adversarial meta-evaluation dialogue generation datasets KdConv-ADV and DSTC7-ADV based on KdConv and DSTC7-AVSD, respectively. Compared to previous meta-evaluation benchmarks, KdConv-ADV and DSTC7-ADV are much more challengin
    
[^15]: 对模糊问题的有选择性回答

    Selectively Answering Ambiguous Questions. (arXiv:2305.14613v1 [cs.CL])

    [http://arxiv.org/abs/2305.14613](http://arxiv.org/abs/2305.14613)

    本研究调查了解决模糊问题的方法，通过定量测量模型输出中的重复性，找出了在含糊问题集中回答高精度子集问题的最可靠方法。这种基于采样的方法可以有效解决问题回答中的歧义，并提高语言模型的可靠性。

    

    可靠的语言模型应该在不知道答案的情况下放弃回答问题。然而，由于问问者意图或上下文的不确定性，问题的答案也可能不清楚。本研究从这个角度调查了问题回答，专注于在众多本质上含糊的问题集中回答高精度子集的问题。在此设置中，我们发现定量测量一组采样模型输出中的重复性是最可靠的校准方法，而非先前工作中使用的模型的概率或自我验证。我们发现，这种方法适用于不同类型的不确定性，不同的模型规模，以及带或不带指导调整。我们的结果表明，基于采样的方法可以有效解决问题回答中的歧义，并提高语言模型的可靠性。

    Trustworthy language models should abstain from answering questions when they do not know the answer. However, the answer to a question can be unknown for a variety of reasons. Prior research has focused on the case in which the question is clear and the answer is unambiguous but possibly unknown. However, the answer to a question can also be unclear due to uncertainty of the questioner's intent or context. We investigate question answering from this perspective, focusing on answering a subset of questions with a high degree of accuracy, from a set of questions in which many are inherently ambiguous. In this setting, we find that the most reliable approach to calibration involves quantifying repetition within a set of sampled model outputs, rather than the model's likelihood or self-verification as used in prior work. % We find this to be the case across different types of uncertainty, varying model scales and both with or without instruction tuning. Our results suggest that sampling-b
    
[^16]: 少样本和零样本NLU任务中提示位置确实很重要

    Prompt position really matters in few-shot and zero-shot NLU tasks. (arXiv:2305.14493v1 [cs.CL])

    [http://arxiv.org/abs/2305.14493](http://arxiv.org/abs/2305.14493)

    该论文通过实证研究发现，提示位置对于少样本和零样本任务的模型性能具有实质性影响，先前研究中使用的提示位置通常是次优的，提示位置优化应成为重要的研究方向。

    

    基于提示的模型在零样本和少样本学习领域取得了显著进展，吸引了众多研究者的关注。但是，有效提示模板的开发起着至关重要的作用。然而，先前的研究主要集中在提示词汇选择或保留提示位置的嵌入初始化方面。在这项实证研究中，我们对自然语言理解任务的提示位置选项进行了迄今为止最全面的分析。我们的发现量化了提示位置对模型性能的实质性影响。我们观察到，先前研究中使用的提示位置对于零样本和少样本设置通常是次优的。这些发现表明，提示位置优化是一个有趣的研究方向，与现有的提示工程重心并列。

    Prompt-based models have made remarkable advancements in the fields of zero-shot and few-shot learning, attracting a lot of attention from researchers. Developing an effective prompt template plays a critical role. However, prior studies have mainly focused on prompt vocabulary selection or embedding initialization with the reserved prompt position fixed. In this empirical study, we conduct the most comprehensive analysis to date of prompt position option for natural language understanding tasks. Our findings quantify the substantial impact prompt position has on model performance. We observe that the prompt position used in prior studies is often sub-optimal for both zero-shot and few-shot settings. These findings suggest prompt position optimisation as an interesting research direction alongside the existing focus on prompt engineering.
    
[^17]: 大型语言模型是否具有鲁棒的零-shot共指解析能力？

    Are Large Language Models Robust Zero-shot Coreference Resolvers?. (arXiv:2305.14489v1 [cs.CL])

    [http://arxiv.org/abs/2305.14489](http://arxiv.org/abs/2305.14489)

    本文研究了零-shot共指解析技术在复杂语言环境下的应用，并证明指令调整的语言模型具有鲁棒性零 shot 普适性。

    

    最近，领域自适应的共指消解取得了进展，依靠使用目标领域的注释数据进行持续训练。同时，预训练的大型语言模型 (LMs) 在广泛的 NLP 任务中展示了强大的零和少量样本学习能力，包括代词消解。虽然这表明了共指能力的证据，但以往的研究大都使用简单的句子级别数据集 (如 Winograd Schema 挑战赛) 研究这种能力。在这项工作中，我们通过评估指令调整的语言模型在更加困难的、语言上复杂的共指基准测试 (如 CoNLL-2012) 上的可行性来评估零-shot学习进行共指消解的可行性。我们证明零-shot提示优于当前的无监督共指系统。进一步的研究揭示了指令调整 LMs 在广泛的领域、语言和时间段上具有强大的鲁棒性零 shot 普适性，以及对上下文和指示词的强烈依赖。

    Recent progress in domain adaptation for coreference resolution relies on continued training using annotated data from target domains. At the same time, pre-trained large language models (LMs) have exhibited strong zero- and few-shot learning abilities across a wide range of NLP tasks including pronoun resolution. While this demonstrates evidence of coreference ability, previous work has mostly studied this ability using simple sentence-level datasets such as the Winograd Schema Challenge. In this work, we assess the feasibility of zero-shot learning for coreference resolution by evaluating instruction-tuned language models on more difficult, linguistically-complex coreference benchmarks (e.g., CoNLL-2012). We demonstrate that zero-shot prompting outperforms current unsupervised coreference systems. Further investigations reveal the robust zero-shot generalization ability of instruction-tuned LMs across a wide range of domains, languages, and time periods, as well as a strong reliance 
    
[^18]: GPT4Table：大型语言模型能理解结构化表格数据吗？一项基准测试和实证研究

    GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. (arXiv:2305.13062v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13062](http://arxiv.org/abs/2305.13062)

    本文设计了一个基准测试来评估大型语言模型（LLMs）对结构化表格数据的理解能力，并发现不同的输入选择会对性能产生影响。在基准测试的基础上，提出了“自我增强”技术以改善理解能力。

    

    大型语言模型（LLMs）作为少样本推理器来解决与自然语言相关的任务越来越具吸引力。然而，关于LLMs对结构化数据（例如表格）的理解程度还有很多需要学习的地方。尽管可以使用表格序列化作为LLMs的输入，但目前还缺乏对LLMs是否真正能够理解这类数据的全面研究。本文通过设计一个基准测试来评估LLMs的结构理解能力（SUC）来解决这个问题。我们创建的基准测试包括七个任务，每个任务都有其独特的挑战，例如单元格查找、行检索和大小检测。我们对GPT-3.5和GPT-4进行了一系列评估。我们发现性能因多种输入选择而异，包括表格输入格式、内容顺序、角色提示和分区标记等。根据基准测试评估所得的见解，我们提出了“自我增强”技术以改善性能。

    Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. While it is true that tables can be used as inputs to LLMs with serialization, there lack of comprehensive studies examining whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \eg, cell lookup, row retrieval, and size detection. We run a series of evaluations on GPT-3.5 and GPT-4. We discover that the performance varied depending on a number of input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we then propose \textit{self-augmentation} for effect
    
[^19]: 基于n-best重排序的精准知识蒸馏

    Accurate Knowledge Distillation with n-best Reranking. (arXiv:2305.12057v1 [cs.CL])

    [http://arxiv.org/abs/2305.12057](http://arxiv.org/abs/2305.12057)

    该论文提出了一种基于n-best重排序的知识蒸馏方法，通过使用多种模型提供伪标签，训练出参数更少但精度相当的学生模型。

    

    我们提出了一种带有n-best重排序的序列级别知识蒸馏方法，该方法考虑了教师模型的top-1假设以及top n-best假设。我们的方法利用包括公开可用的大型预训练模型在内的多种模型，为训练学生模型提供更准确的伪标签。我们在WMT21德英翻译任务上验证了我们的提议，并证明我们的学生模型在具有两个数量级较少的参数的情况下，实现了与Tran等人（2021年）的包含47亿参数的大型翻译模型相当的精度。

    We propose extending the Sequence-level Knowledge Distillation (Kim and Rush, 2016) with n-best reranking to consider not only the top-1 hypotheses but also the top n-best hypotheses of teacher models. Our approach leverages a diverse set of models, including publicly-available large pretrained models, to provide more accurate pseudo-labels for training student models. We validate our proposal on the WMT21 German-English translation task and demonstrate that our student model achieves comparable accuracy to a large translation model with 4.7 billion parameters from (Tran et al., 2021) while having two orders of magnitude fewer parameters.
    
[^20]: 利用语言模型用自然语言解释黑盒文本模块

    Explaining black box text modules in natural language with language models. (arXiv:2305.09863v1 [cs.AI])

    [http://arxiv.org/abs/2305.09863](http://arxiv.org/abs/2305.09863)

    本文介绍了一种名为Summarize and Score（SASC）的方法，该方法可以自动获取黑盒文本模块的自然语言解释以及解释可靠程度的分数。研究者们已经在合成模块和BERT模型中使用SASC，让我们可以解释模块的选择性，这对于增强大型语言模型的可解释性非常重要。

    

    大型语言模型已经证明在各种任务中具有出色的预测性能。然而，它们的快速增长和不透明性已经引起了对可解释性的需求。本文询问是否可以自动获取黑盒文本模块的自然语言解释。一个“文本模块”是将文本映射到标量连续值的任何函数，例如LLM内的子模块或大脑区域的拟合模型。“黑盒”表示我们只能访问模块的输入/输出。我们引入了Summarize and Score（SASC）方法，它接受文本模块并返回模块选择性的自然语言解释以及解释可靠程度的分数。我们在三个上下文中研究SASC。首先，我们在合成模块上评估SASC，并发现它经常恢复基本真相说明。其次，我们使用SASC来解释预训练BERT模型中的模块，使得检查BERT的模块成为可能。

    Large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their rapid proliferation and increasing opaqueness have created a growing need for interpretability. Here, we ask whether we can automatically obtain natural language explanations for black box text modules. A "text module" is any function that maps text to a scalar continuous value, such as a submodule within an LLM or a fitted model of a brain region. "Black box" indicates that we only have access to the module's inputs/outputs.  We introduce Summarize and Score (SASC), a method that takes in a text module and returns a natural language explanation of the module's selectivity along with a score for how reliable the explanation is. We study SASC in 3 contexts. First, we evaluate SASC on synthetic modules and find that it often recovers ground truth explanations. Second, we use SASC to explain modules found within a pre-trained BERT model, enabling inspection of the 
    
[^21]: NeuroComparatives：比较知识的神经符号提炼

    NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge. (arXiv:2305.04978v1 [cs.CL])

    [http://arxiv.org/abs/2305.04978](http://arxiv.org/abs/2305.04978)

    本文提出了一种新的用于比较知识提炼的神经符号框架，名称为NeuroComparatives。该框架利用词汇约束解码进行比较知识提炼，以及对生成的知识进行严格过滤。

    

    比较知识是我们世界知识的重要组成部分，但在以前的文献中研究不足。本文研究比较知识获取任务，受到像GPT-3这样极端规模语言模型能力的显着提高的推动，推动了将他们的知识收集到知识库中的努力。但是，这些模型的推理API访问受到限制，从而限制了知识获取的范围和多样性。因此，我们提出了一个看似不可行的问题：更易于访问、规模更小、性能更弱的模型（如GPT-2）是否可以用于获取比较知识，从而达到与大规模模型相当的质量？我们引入了NeuroComparatives，一种使用词汇约束解码的比较知识提炼新框架，其后紧密过滤生成的知识。

    Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) is an essential component of our world knowledge, yet understudied in prior literature. In this paper, we study the task of comparative knowledge acquisition, motivated by the dramatic improvements in the capabilities of extreme-scale language models like GPT-3, which have fueled efforts towards harvesting their knowledge into knowledge bases. However, access to inference API for such models is limited, thereby restricting the scope and the diversity of the knowledge acquisition. We thus ask a seemingly implausible question: whether more accessible, yet considerably smaller and weaker models such as GPT-2, can be utilized to acquire comparative knowledge, such that the resulting quality is on par with their large-scale counterparts?  We introduce NeuroComparatives, a novel framework for comparative knowledge distillation using lexically-constrained decoding, followed by stringent filtering of generated knowledge
    
[^22]: LLM能否作为数据库接口？大型数据库基础文本到SQL的基准测试。

    Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs. (arXiv:2305.03111v1 [cs.CL])

    [http://arxiv.org/abs/2305.03111](http://arxiv.org/abs/2305.03111)

    本文提出了一个大型的基准测试Bird，可以用于大规模数据库文本到SQL的任务，突出了数据库值理解和SQL效率等领域的挑战。

    

    近年来，文本到SQL解析受到越来越多的关注，旨在将自然语言指令转换为可执行的SQL命令。本文提出了一个大型基准测试Bird，它包含旨在大规模数据库基础的12,751对文本到SQL数据和95个数据库，总大小为33.4GB，涵盖37个专业领域。与现有基准测试相比，Bird强调数据库值的理解，突出了脏数据库内容、NL问题和数据库内容之间的外部知识以及SQL效率等新挑战。解决这些问题，文本到SQL模型必须具备数据库值理解和语义解析的能力。

    Text-to-SQL parsing, which aims at converting natural language instructions into executable SQLs, has gained increasing attention in recent years. In particular, Codex and ChatGPT have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database contents leaving the gap between academic study and real-world applications. To mitigate this gap, we present Bird, a big benchmark for large-scale database grounded in text-to-SQL tasks, containing 12,751 pairs of text-to-SQL data and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty database contents, external knowledge between NL questions and database contents, and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. 
    
[^23]: 解锁ChatGPT的潜力：对其在自然语言处理中应用、优点、局限性和未来方向的全面探讨

    Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing. (arXiv:2304.02017v1 [cs.CL])

    [http://arxiv.org/abs/2304.02017](http://arxiv.org/abs/2304.02017)

    本文全面探讨了ChatGPT在自然语言处理中的应用、优点和局限性，强调了使用这个强大工具时的道德考虑，为人工智能和NLP领域的讨论做出了贡献。

    

    ChatGPT是人工智能领域中广泛应用的强大工具，已成功应用于聊天机器人、内容生成、语言翻译、个性化推荐和医疗诊断治疗。它的多功能性和准确性使其成为自然语言处理（NLP）的强大工具。但是，ChatGPT也存在局限性，例如其倾向于产生有偏见的响应以及存在潜在的有害语言模式。本文全面概述了ChatGPT及其应用、优点和局限性，并强调了在真实场景中使用这个强大工具时道德考虑的重要性。最后，本文通过提供提示工程技术的见解，为关于人工智能及其对视觉和NLP领域的影响的持续讨论做出了贡献。

    ChatGPT is a powerful tool in the field of artificial intelligence that has been widely used in various applications. ChatGPT has been applied successfully in chatbots, content generation, language translation, personalized recommendations, and medical diagnosis and treatment. Its versatility and accuracy make it a powerful tool for natural language processing (NLP). However, there are also limitations to ChatGPT, such as its tendency to produce biased responses and its potential to perpetuate harmful language patterns. This article provides a comprehensive overview of ChatGPT, its applications, advantages, and limitations. Additionally, the paper emphasizes the importance of ethical considerations when using this robust tool in real-world scenarios. Finally, This paper contributes to ongoing discussions surrounding artificial intelligence and its impact on vision and NLP domains by providing insights into prompt engineering techniques.
    
[^24]: 基于语言模型的知识图谱嵌入编辑

    Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.10405](http://arxiv.org/abs/2301.10405)

    本文提出了一种新的任务——编辑基于语言模型的知识图谱嵌入，旨在实现对KG嵌入的数据高效和快速更新。针对这一任务，提出了一个简单而强大的方案——KGEditor，可以更好地更新特定事实而不影响其余部分的性能。

    

    近几十年来，使用语言模型进行知识图谱（KG）嵌入已经取得了实证成功。但是，基于语言模型的KG嵌入通常作为静态工件部署，修改起来具有挑战性，需要重新训练。为了解决这个问题，本文提出了一种新的任务，即编辑基于语言模型的KG嵌入。该任务旨在实现对KG嵌入的数据高效和快速更新，而不影响其余部分的性能。我们构建了四个新数据集：E-FB15k237、A-FB15k237、E-WN18RR 和 A-WN18RR，并评估了几种知识编辑基线，证明了之前的模型处理该任务的能力有限。我们进一步提出了一个简单但强大的基线——KGEditor，它利用超网络的附加参数层来编辑/添加事实。全面的实验结果表明，当更新特定事实而不影响其余部分的性能时，KGEditor 的表现更好。

    Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
    
[^25]: Deanthropomorphising NLP：语言模型可以意识到吗？

    Deanthropomorphising NLP: Can a Language Model Be Conscious?. (arXiv:2211.11483v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.11483](http://arxiv.org/abs/2211.11483)

    本文讨论了关于使用Transformer架构的预训练语言模型LaMDA是否具有意识的说法。作者认为语言模型不可能具有意识，而LaMDA没有比其他类似模型更具先进性。

    

    本文旨在对最近有关使用Transformer模型架构的预训练语言模型LaMDA具有意识的说法进行讨论。我们认为这样的语言模型不可能具有意识，而LaMDA并没有比其他类似模型更具先进性。我们通过综合信息理论对Transformer架构进行分析来证明这一点。我们认为这些有意识的说法是NLP报道中使用拟人化语言的更广泛倾向的一部分。无论这些说法的真实性如何，我们认为现在是评估语言建模进展并考虑该任务的伦理影响的适当时机。为了使本文有助于NLP社区以外的读者，我们还提供了一些NLP基础知识的介绍。

    This work is intended as a voice in the discussion over the recent claims that LaMDA, a pretrained language model based on the Transformer model architecture, is sentient. This claim, if confirmed, would have serious ramifications in the Natural Language Processing (NLP) community due to wide-spread use of similar models. However, here we take the position that such a language model cannot be sentient, or conscious, and that LaMDA in particular exhibits no advances over other similar models that would qualify it. We justify this by analysing the Transformer architecture through Integrated Information Theory. We see the claims of consciousness as part of a wider tendency to use anthropomorphic language in NLP reporting. Regardless of the veracity of the claims, we consider this an opportune moment to take stock of progress in language modelling and consider the ethical implications of the task. In order to make this work helpful for readers outside the NLP community, we also present the
    
[^26]: 语义歧义的因果结构

    The Causal Structure of Semantic Ambiguities. (arXiv:2206.06807v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.06807](http://arxiv.org/abs/2206.06807)

    本文使用Gogioso和Pinzani在QPL 2021中提出的束理论模型，为语义歧义的两个特征（不同可能解释的联合可信度和某些词在过程中扮演更重要角色的因果结构）进行建模。通过对心理语言学文献中的歧义短语数据集进行分析，研究人员对人类对于这些歧义的判断进行了实证测量。

    

    歧义是自然语言现象，在不同的语法、语义和语用层面上发生。它得到了广泛的研究；例如，在心理语言学领域，我们有多种竞争性的研究人类消歧过程的方法。这些研究是经验性的，基于眼动跟踪等测量方法。本文首次尝试为语义歧义形式化这些进程，其中我们确定了两个特征：(1)不同可能解释之间的联合可信度，(2)根据某些词在过程中扮演更重要角色的因果结构。Gogioso和Pinzani在QPL 2021中提出的新型束理论确定因果性模型并对这些特征进行推理提供了工具。我们将这个理论应用于从心理语言学文献中提取的歧义短语数据集和我们使用Amazon的机械土耳其引擎收集的人类可信度判断中。我们测量了其中的因果关系、歧义水平等。

    Ambiguity is a natural language phenomenon occurring at different levels of syntax, semantics, and pragmatics. It is widely studied; in Psycholinguistics, for instance, we have a variety of competing studies for the human disambiguation processes. These studies are empirical and based on eyetracking measurements. Here we take first steps towards formalizing these processes for semantic ambiguities where we identified the presence of two features: (1) joint plausibility degrees of different possible interpretations, (2) causal structures according to which certain words play a more substantial role in the processes. The novel sheaf-theoretic model of definite causality developed by Gogioso and Pinzani in QPL 2021 offers tools to model and reason about these features. We applied this theory to a dataset of ambiguous phrases extracted from Psycholinguistics literature and their human plausibility judgements collected by us using the Amazon Mechanical Turk engine. We measured the causal fr
    

