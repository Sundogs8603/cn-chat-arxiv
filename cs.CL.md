# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning.](http://arxiv.org/abs/2306.13089) | 本研究提出了一种名为GIMLET的统一图文模型，用于在零样本设置下使用自然语言指令完成分子相关任务。我们解决了现有模型的指令处理不足和图形容量有限的问题，并证明了使用GIMLET能够增强图形特征的泛化能力。 |
| [^2] | [Semi-automated extraction of research topics and trends from NCI funding in radiological sciences from 2000-2020.](http://arxiv.org/abs/2306.13075) | 本文提出了一种半自动的方法来提取和命名研究主题，将其应用于NCI在放射科学中的210亿美元资助21年，发现治疗和物理学为基础的研究的资助已经超过了基于诊断和生物学的研究。 |
| [^3] | [Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs.](http://arxiv.org/abs/2306.13063) | 本研究就不需要微调模型或访问专有信息的方法进行置信度引导进行了探讨，通过研究发现LLMs往往展现出高度的过度自信。 |
| [^4] | [Named entity recognition in resumes.](http://arxiv.org/abs/2306.13062) | 本研究实现了一种基于深度学习的半自动命名实体识别系统，针对IT领域的简历进行了调整并成功识别八种不同的实体类型。 |
| [^5] | [CamChoice: A Corpus of Multiple Choice Questions and Candidate Response Distributions.](http://arxiv.org/abs/2306.13047) | 本文介绍了CamChoice数据集，该数据集包含多项选择理解问题和真实候选答案选项分布，为候选人分布匹配任务提供了自动评估方式。 |
| [^6] | [Towards Explainable Evaluation Metrics for Machine Translation.](http://arxiv.org/abs/2306.13041) | 本研究探索机器翻译可解释性评估指标，提供综合综述和最新方法，并贡献下一代方法的愿景。 |
| [^7] | [Apolitical Intelligence? Auditing Delphi's responses on controversial political issues in the US.](http://arxiv.org/abs/2306.13000) | 本文审查了用于众包伦理的大型语言模型Delphi在美国政治争议问题中的回应。研究发现，该模型的置信度校准不良，呈现显著的政治倾斜。作者从数据女权主义的角度探讨了中立性的问题，讨论了中立性概念如何转移权力、进一步边缘化无声的声音。 |
| [^8] | [Speech Emotion Diarization: Which Emotion Appears When?.](http://arxiv.org/abs/2306.12991) | 本研究提出了一种新任务：语音情感分段（SED），旨在反映语音情感的细粒度特性。与此同时，我们还提供了一个可公开访问的语音情感数据集 ZED，并提供了竞争基线。 |
| [^9] | [Conversation Derailment Forecasting with Graph Convolutional Networks.](http://arxiv.org/abs/2306.12982) | 该论文通过引入图卷积神经网络，考虑了对话的用户动态和公众对话言论的影响，提出了一种新的预测对话偏离的模型，并在基准数据集上优于传统的序列模型。 |
| [^10] | [Tracking public attitudes toward ChatGPT on Twitter using sentiment analysis and topic modeling.](http://arxiv.org/abs/2306.12951) | 本文使用情感分析和主题建模技术研究了Twitter用户对ChatGPT的态度。结果显示总体情感是中性到积极的，最受关注的主题包括人工智能、搜索引擎、教育、写作和问题回答等方面。 |
| [^11] | [Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing.](http://arxiv.org/abs/2306.12929) | 本文提出了一种称为“Helper-Head”的方法，可以通过教授注意头忽略输入和输出的某些部分来消除离群值，从而实现对transformer的可量化。实验结果表明，该方法在低、高比特宽度设置上均优于现有方法，在两个流行的语言建模基准上实现了最先进的结果。 |
| [^12] | [AudioPaLM: A Large Language Model That Can Speak and Listen.](http://arxiv.org/abs/2306.12925) | AudioPaLM是一款能够更好地处理语音任务的大型语言模型，它继承了AudioLM的语音身份和语调信息保留能力以及PaLM-2的语言知识，可用于语音识别、语音翻译等应用。 |
| [^13] | [Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation.](http://arxiv.org/abs/2306.12916) | 本文全面研究了跨语言跨时代摘要任务，使用历史幻想文本和维基百科摘要构建了第一个CLCTS语料库，并研究了流行的变压器模型及其中间任务微调的有效性；同时还探讨了ChatGPT在CLCTS中作为摘要器和评估器的潜力。最终发现中间任务微调的端到端模型产生了中等到差的效果，而ChatGPT在没有微调的情况下提供中等到好的摘要质量表现。 |
| [^14] | [Implicit spoken language diarization.](http://arxiv.org/abs/2306.12913) | 该论文研究了使用深度学习方法进行隐式语言信息建模的语音语言日志分离任务，使用 x-vector 方法进行分离时在合成数据和实际数据上的表现分别为 6.78%/7.06% 和 22.50%/60.38%，使用预训练的 wave2vec 嵌入向量可在一定程度上提高性能。 |
| [^15] | [xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource Languages.](http://arxiv.org/abs/2306.12907) | xSIM++ 是一个新的代理评分用于低资源语言比文本挖掘表现的评估。相较于之前的方法(xSIM), xSIM++利用基于规则的方法提供了更准确的评估并提高了与翻译系统BLEU得分的相关性。 |
| [^16] | [Unveiling Global Narratives: A Multilingual Twitter Dataset of News Media on the Russo-Ukrainian Conflict.](http://arxiv.org/abs/2306.12886) | 研究人员收集了约1.5百万条涵盖60种不同语言的推文，创建了一个多语种推特数据集，重点探讨了俄乌冲突的新闻媒体报道。数据集中的标签可以识别与该话题相关的主体、立场、概念和情感表达。 |
| [^17] | [Natural Language Processing in Electronic Health Records in Relation to Healthcare Decision-making: A Systematic Review.](http://arxiv.org/abs/2306.12834) | 系统评述NLP在电子病历中发现具有实现临床决策的潜在机会，但仍需克服数据的标准化和特定领域模型的需要等挑战。 |
| [^18] | [Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4.](http://arxiv.org/abs/2306.12794) | 本文综述了DSTC 11 Track 4中针对开放域对话系统进行鲁棒性和多语言自动评估的挑战，介绍了提供给参与者的数据集和基线，并总结了表现最佳的系统及其方法。 |
| [^19] | [On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective.](http://arxiv.org/abs/2306.12756) | 本论文研究了生成式检索模型在超出分布（OOD）泛化方面的鲁棒性，定义了从三个方面衡量OOD鲁棒性，并分析了其与密集检索模型的比较。实验结果表明，生成式检索模型的OOD鲁棒性较弱，特别是在面向任务的超出分布场景中更为明显。针对造成鲁棒性较弱的原因，提出了潜在的解决方案。 |
| [^20] | [Generative Multimodal Entity Linking.](http://arxiv.org/abs/2306.12725) | 本文提出了 GEMEL 方法，使用大规模预训练的 LLMs 直接生成目标实体名称，仅调整了极少的模型参数即可实现最先进的 MEL 实验结果。 |
| [^21] | [Natural Language Generation for Advertising: A Survey.](http://arxiv.org/abs/2306.12719) | 本文综述了过去十年中广告自然语言生成的研究进展，介绍了从基于模板的方法到使用神经网络的抽取式和生成式方法的发展，提出了度量优化、忠实度、多样性、多模态以及基准数据集的开发等关键挑战和方向。 |
| [^22] | [Constructing Colloquial Dataset for Persian Sentiment Analysis of Social Microblogs.](http://arxiv.org/abs/2306.12679) | 本文构建了一个60,000条波斯语的社交微博口语文本数据集，提出了一种新的深度卷积神经网络(CNN)模型，用于更有效地分析社交微博中的口语文本情感。 |
| [^23] | [Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models.](http://arxiv.org/abs/2306.12659) | 本研究提出了一种简单而有效的指令调整方法，将少量监督式金融情感分析数据转化为指令数据，并用此方法对通用的LLM进行微调，从而在金融情感分析方面取得了显着进展。 |
| [^24] | [Identifying and Extracting Rare Disease Phenotypes with Large Language Models.](http://arxiv.org/abs/2306.12656) | 研究提出了一个通过大型语言模型自动提取罕见病表型的方法，使用零样本或少样本的提示学习技术，无需对大量语料进行注释。 |
| [^25] | [Class-Incremental Learning based on Label Generation.](http://arxiv.org/abs/2306.12619) | 本文提出了一种基于标签生成方法的增量分类学习（CIL）方法（VAG），大幅减少了灾难性遗忘（CF），并更好地保留了预训练模型的可推广表示。 |
| [^26] | [A Hierarchical Approach to exploiting Multiple Datasets from TalkBank.](http://arxiv.org/abs/2306.12596) | 本文介绍了一个管道框架，采用分层搜索方法，实现了复杂数据选择的高效率。该框架可以标准化和清理元数据并集成不同研究的数据集，提供一种在多个数据集之间自动化挖掘信息的方法。 |
| [^27] | [ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews.](http://arxiv.org/abs/2306.12587) | ARIES是一份包含科学论文修订的语料库，为训练和评估大型语言模型提供了工具。通过评估模型，发现其在寻找对应的修订方面仍存在困难，同时在生成修订时过分遵循反馈的措辞，而不是考虑整体的语义。 |
| [^28] | [Morphological Inflection with Phonological Features.](http://arxiv.org/abs/2306.12581) | 本文探讨了通过利用次要的音韵特征来改进形态学模型性能的方法，设计了两种不同的方法，并在八个语言对上进行了实验。研究结果表明选择要包括的音韵特征和它们的操作方式对性能产生深远影响，而在语言学中通常不被考虑的音韵属性对变形有益。 |
| [^29] | [NoRefER: a Referenceless Quality Metric for Automatic Speech Recognition via Semi-Supervised Language Model Fine-Tuning with Contrastive Learning.](http://arxiv.org/abs/2306.12577) | 本文提出了一种无参考的ASR质量度量方法，使用半监督语言模型微调和对比学习，可以对ASR假设进行排名，并在选择潜在错误的样本方面具有潜在的潜力。 |
| [^30] | [Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases.](http://arxiv.org/abs/2306.12567) | 本研究使用名为 NeuBAROCO 的数据集，检验了当前大型语言模型在三段论推理中的表现。发现这些模型在处理信念偏见、转化错误和氛围效应等问题时表现欠佳。 |
| [^31] | [SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning.](http://arxiv.org/abs/2306.12552) | 本文介绍了一种具有挑战性的任务SituatedGen，要求具有常识推理能力的机器生成一对对比句子，以融入地理和时间背景。作者提出了一种相应的英语数据集，并发现目前的生成式语言模型仍然难以实现具有常识合理性的句子的生成，远远落后于人类表现。 |
| [^32] | [On Evaluation of Document Classification using RVL-CDIP.](http://arxiv.org/abs/2306.12550) | 本文揭示了RVL-CDIP基准的几个不良特征，并提出了创建新的文件分类基准的建议指南。 |
| [^33] | [Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference.](http://arxiv.org/abs/2306.12509) | 本文提出了一种称为深度语言网络（DLN）的架构，通过联合训练叠加的语言模型层（LLMs），使用变分推断算法进行提示训练，使得DLN-2的性能甚至可以与少量训练数据的GPT-4相媲美。 |
| [^34] | [Misinformation as Information Pollution.](http://arxiv.org/abs/2306.12466) | 社交媒体算法会倾向于推广包括错误信息的有争议帖子，因此将错误信息视为信息污染，通过对错误信息的Pigouv税进行经济激励，可以更有效地控制其传播。 |
| [^35] | [DEPAC: a Corpus for Depression and Anxiety Detection from Speech.](http://arxiv.org/abs/2306.12443) | 本文介绍了一份新颖的语音数据集DEPAC，该数据集标记了抑郁症和焦虑症标准筛查工具上的门槛。此外，作者还提出了一组手工筛选的声学和语言特征，可以有效地识别人类语音中的精神疾病迹象。该研究为自动诊断系统的开发提供了信息丰富且平衡的语料库。 |
| [^36] | [Quilt-1M: One Million Image-Text Pairs for Histopathology.](http://arxiv.org/abs/2306.11207) | 本文介绍了一个名为 Quilt-1M 的癌症组织学图像和文字对的百万数据集，并利用 YouTube 上的专家医生教程视频为主要来源。这个数据集将使得癌症组织学领域的表征学习取得类似于其他领域的进展。 |
| [^37] | [Demystifying GPT Self-Repair for Code Generation.](http://arxiv.org/abs/2306.09896) | 本文分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，自我修复在较短和较简单的任务中效果更好，仅在某些代码部分上应用自我修复可以非常有效，本文提出的引导修复方法在 APPS 数据集上获得性能提升。 |
| [^38] | [Explaining Legal Concepts with Augmented Large Language Models (GPT-4).](http://arxiv.org/abs/2306.09525) | 本文评估了利用GPT-4增强解释法律术语性能的方法。与基准设置相比，使用增强的法律信息检索模块可以提高法律术语解释的质量，并消除模型的幻觉问题，从而为在法律领域使用大型语言模型开辟了新的途径。 |
| [^39] | [Explore, Establish, Exploit: Red Teaming Language Models from Scratch.](http://arxiv.org/abs/2306.09442) | 本文提出了一种新的红队行动，通过从高层次、抽象的规范出发来考虑语言模型的行为，以探究模型的创新和贡献。 |
| [^40] | [ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings.](http://arxiv.org/abs/2305.11554) | 本论文提出了一种名为ToolkenGPT的方法，将大型语言模型（LLMs）与外部工具相结合，引入了toolken的概念，利用tool embeddings实现无缝交互，同时在各种下游任务上展示出了良好的效果。 |
| [^41] | [ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing.](http://arxiv.org/abs/2305.09770) | ConvXAI是一个基于对话的XAI系统，它集成了多种XAI类型，并将实际用户需求嵌入设计中，以提高实用性。 |
| [^42] | [Investigating the effect of sub-word segmentation on the performance of transformer language models.](http://arxiv.org/abs/2305.05480) | 本文研究使用单语词段算法StateMorph训练语言模型时，可以使模型更高效地收敛并获得更好的验证分数。 |
| [^43] | [Token-Level Fitting Issues of Seq2seq Models.](http://arxiv.org/abs/2305.04493) | 研究发现使用early-stopping训练的seq2seq模型在token级别存在拟合问题，影响因素包括Token频率、词性和预测差异以及外部因素如语言、模型大小、领域、数据规模和预训练等。 |
| [^44] | [Exploring Human-Like Translation Strategy with Large Language Models.](http://arxiv.org/abs/2305.04118) | 本文提出了一个名为MAPS的框架，使LLMs能够模仿人类翻译的过程，该过程包括分析源文本并提取关键词、主题和相关演示以指导翻译过程。该框架实验结果显示明显优于多个强基线，为开展使用LLM实现人类化翻译策略的有前途的方向提供了启示。 |
| [^45] | [Using ChatGPT for Entity Matching.](http://arxiv.org/abs/2305.03423) | 本研究探究使用ChatGPT进行实体匹配的可行性，相较于传统方法更为有效，不需大量微调数据，且更加健壮。 |
| [^46] | [SweCTRL-Mini: a data-transparent Transformer-based large language model for controllable text generation in Swedish.](http://arxiv.org/abs/2304.13994) | SweCTRL-Mini是一种基于Transformer的大型瑞典语言模型，用户可以控制它生成的文本流派，完全开放下载。生成能力比较GPT-3。 |
| [^47] | [Visual Abstraction and Reasoning through Language.](http://arxiv.org/abs/2303.04091) | 本论文提出了一种通过自然语言描述任务的通用框架来解决Abstraction and Reasoning Corpus（ARC）问题，虽然还没有在ARC上击败最先进的DSL模型，但我们展示了我们的方法具有巨大的潜力，可以解决先前未解决的任务。 |
| [^48] | [SemSup-XC: Semantic Supervision for Zero and Few-shot Extreme Classification.](http://arxiv.org/abs/2301.11309) | SemSup-XC是一种用于零样本和少样本极端分类的语义监督模型，在自动收集的语义类别描述和新颖的混合匹配模块的帮助下，可以在法律、电子商务和维基百科等三个XC数据集上实现最先进的零样本和少样本性能。 |
| [^49] | [A Survey of Deep Learning for Mathematical Reasoning.](http://arxiv.org/abs/2212.10535) | 本文综述了过去十年中深度学习在数学推理领域的关键任务、数据集和方法，并评估了现有的基准和方法，探讨了未来的研究方向。 |
| [^50] | [Improving Proactive Dialog Agents Using Socially-Aware Reinforcement Learning.](http://arxiv.org/abs/2211.15359) | 本论文的主要贡献在于提出了一种新的方法，通过将社交和任务相关特征考虑在对话中，来优化主动对话策略，使其在任务效率高的同时，也能促进用户信任，从而提高人机交互的成功率和效率。 |
| [^51] | [Sequentially Sampled Chunk Conformer for Streaming End-to-End ASR.](http://arxiv.org/abs/2211.11419) | 本论文提出了一种称为 SSC-Conformer 的块式模型，利用串行采样块自注意力机制提高块间交互效率，同时保持线性复杂度，将块卷积与因果卷积相结合以达到更好的 CER 表现，实验结果表明 SSC-Conformer 在 AISHELL-1 基准测试中取得了最新的流式 E2E ASR 性能水平。 |
| [^52] | [VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations.](http://arxiv.org/abs/2207.00221) | 本研究提出VL-CheckList，使用物体、属性和关系评估预训练的视觉语言模型，通过对七种流行的VLP模型进行全面研究分析，揭示出不同模型之间的细微差异。 |
| [^53] | [Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation.](http://arxiv.org/abs/2205.12593) | 本研究提出了一种训练策略Less-Learn-Shortcut (LLS)，以减少模型过度依赖单词与标签之间的错误相关性。同时，通过量化有偏样本的有偏程度，加强无偏样本的训练。 |
| [^54] | [EmTract: Extracting Emotions from Social Media.](http://arxiv.org/abs/2112.03868) | 本文描述了一个名为EmTract的开源工具，其基于预调整的NLP模型DistilBERT和4,861个标记，提取面向金融环境的社交媒体文本中的情绪，并在人工和chatGPT注释数据上优于竞争的开源最先进情感分类器，并且方法具有量身定制的特点以及针对非标准短语、表情符号和表情符号这类社交媒体数据。 |

# 详细

[^1]: GIMLET：一种用于基于指令分子零样本学习的统一图文模型

    GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning. (arXiv:2306.13089v1 [cs.LG])

    [http://arxiv.org/abs/2306.13089](http://arxiv.org/abs/2306.13089)

    本研究提出了一种名为GIMLET的统一图文模型，用于在零样本设置下使用自然语言指令完成分子相关任务。我们解决了现有模型的指令处理不足和图形容量有限的问题，并证明了使用GIMLET能够增强图形特征的泛化能力。

    

    分子属性预测近年来受到了广泛关注，但由于昂贵的实验造成的标签不足问题将是其主要瓶颈。为了缓解这个问题并更好地利用文本知识进行任务，本研究探讨了在零样本设置下使用自然语言指令完成分子相关任务的可行性。我们发现现有的分子-文本模型在这种情况下表现不佳，原因是处理指令不足以及图形容量有限。为了克服这些问题，我们提出了GIMLET，它统一了图形和文本数据的语言模型。通过采用广义位置嵌入，我们的模型被扩展以编码图形结构和指令文本，而无需额外的图形编码模块。GIMLET还在注意机制中解耦了图形的编码和任务指令，增强了跨新任务的图形特征的泛化能力。我们构建了一个数据集...

    Molecule property prediction has gained significant attention in recent years. The main bottleneck is the label insufficiency caused by expensive lab experiments. In order to alleviate this issue and to better leverage textual knowledge for tasks, this study investigates the feasibility of employing natural language instructions to accomplish molecule-related tasks in a zero-shot setting. We discover that existing molecule-text models perform poorly in this setting due to inadequate treatment of instructions and limited capacity for graphs. To overcome these issues, we propose GIMLET, which unifies language models for both graph and text data. By adopting generalized position embedding, our model is extended to encode both graph structures and instruction text without additional graph encoding modules. GIMLET also decouples encoding of the graph from tasks instructions in the attention mechanism, enhancing the generalization of graph features across novel tasks. We construct a dataset 
    
[^2]: NCI放射科学资助项目中研究主题和趋势的半自动提取（2000-2020年）

    Semi-automated extraction of research topics and trends from NCI funding in radiological sciences from 2000-2020. (arXiv:2306.13075v1 [cs.CL])

    [http://arxiv.org/abs/2306.13075](http://arxiv.org/abs/2306.13075)

    本文提出了一种半自动的方法来提取和命名研究主题，将其应用于NCI在放射科学中的210亿美元资助21年，发现治疗和物理学为基础的研究的资助已经超过了基于诊断和生物学的研究。

    

    调查员、资助方和公众都想了解公共资金资助的研究主题和趋势，但目前手动分类的努力在规模和理解方面受到限制。我们开发了一种半自动的方法来提取和命名研究主题，并将其应用于放射科学中210亿美元的NCI资助21年以确定微观和宏观研究主题和资助趋势。我们的方法依赖于现有生物医学词向量的序列聚类、专家进行命名和可视化以发现宏观主题。我们提供了15个和60个聚类主题的结果，发现拨款嵌入的2D投影显示两个主导轴：物理-生物学和治疗-诊断。对于我们的数据集，我们发现对治疗和物理学为基础的研究的资助已经超过了基于诊断和生物学的研究。希望这些结果能够：（1）提供NCI在放射科学中研究经费分配的见解；（2）促进研究者和非专家对这个领域的关注并进一步探索。

    Investigators, funders, and the public desire knowledge on topics and trends in publicly funded research but current efforts in manual categorization are limited in scale and understanding. We developed a semi-automated approach to extract and name research topics, and applied this to \$1.9B of NCI funding over 21 years in the radiological sciences to determine micro- and macro-scale research topics and funding trends. Our method relies on sequential clustering of existing biomedical-based word embeddings, naming using subject matter experts, and visualization to discover trends at a macroscopic scale above individual topics. We present results using 15 and 60 cluster topics, where we found that 2D projection of grant embeddings reveals two dominant axes: physics-biology and therapeutic-diagnostic. For our dataset, we found that funding for therapeutics- and physics-based research have outpaced diagnosticsand biology-based research, respectively. We hope these results may (1) give in
    
[^3]: LLM能表达它们的不确定性吗？LMM自信心评估的实证研究

    Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. (arXiv:2306.13063v1 [cs.CL])

    [http://arxiv.org/abs/2306.13063](http://arxiv.org/abs/2306.13063)

    本研究就不需要微调模型或访问专有信息的方法进行置信度引导进行了探讨，通过研究发现LLMs往往展现出高度的过度自信。

    

    将大型语言模型(LLMs)赋予准确表达其置信度的能力，即置信度引导任务，对确保可靠和可信的决策过程至关重要。本研究探讨了不需要微调模型或访问专有信息的置信度引导方法，介绍了三种方法：基于表述、基于一致性、以及它们的混合方法进行基准测试，并在五种类型的数据集和四种广泛使用的LLMs上评估它们的性能。

    The task of empowering large language models (LLMs) to accurately express their confidence, referred to as confidence elicitation, is essential in ensuring reliable and trustworthy decision-making processes. Previous methods, which primarily rely on model logits, have become less suitable for LLMs and even infeasible with the rise of closed-source LLMs (e.g., commercialized LLM APIs). This leads to a growing need to explore the untapped area of \emph{non-logit-based} approaches to estimate the uncertainty of LLMs. Hence, in this study, we investigate approaches for confidence elicitation that do not require model fine-tuning or access to proprietary information. We introduce three categories of methods: verbalize-based, consistency-based, and their hybrid methods for benchmarking, and evaluate their performance across five types of datasets and four widely-used LLMs. Our analysis of these methods uncovers several key insights: 1) LLMs often exhibit a high degree of overconfidence when 
    
[^4]: 简历中的命名实体识别

    Named entity recognition in resumes. (arXiv:2306.13062v1 [cs.CL])

    [http://arxiv.org/abs/2306.13062](http://arxiv.org/abs/2306.13062)

    本研究实现了一种基于深度学习的半自动命名实体识别系统，针对IT领域的简历进行了调整并成功识别八种不同的实体类型。

    

    命名实体识别（NER）被用于从各种文档和文本中提取信息，例如姓名和日期。从简历中提取教育和工作经历信息以进行筛选非常重要。考虑到简历中的所有信息必须手动输入公司系统，自动化这个过程将为公司节省时间。在本研究中，重点关注IT领域的简历，实现了一种基于深度学习的半自动命名实体识别系统。首先，标注了来自五个不同IT相关领域员工的简历。使用标注数据，调整了六个基于转换器的预训练模型以解决命名实体识别问题。这些模型是在自然语言处理领域中流行的模型之一。所得系统可以识别八种不同的实体类型，包括城市、日期、学位、毕业证主修、职称、语言、国家和技能。

    Named entity recognition (NER) is used to extract information from various documents and texts such as names and dates. It is important to extract education and work experience information from resumes in order to filter them. Considering the fact that all information in a resume has to be entered to the companys system manually, automatizing this process will save time of the companies. In this study, a deep learning-based semi-automatic named entity recognition system has been implemented with a focus on resumes in the field of IT. Firstly, resumes of employees from five different IT related fields has been annotated. Six transformer based pre-trained models have been adapted to named entity recognition problem using the annotated data. These models have been selected among popular models in the natural language processing field. The obtained system can recognize eight different entity types which are city, date, degree, diploma major, job title, language, country and skill. Models u
    
[^5]: CamChoice：一份包含多项选择题和候选答案分布的语料库

    CamChoice: A Corpus of Multiple Choice Questions and Candidate Response Distributions. (arXiv:2306.13047v1 [cs.CL])

    [http://arxiv.org/abs/2306.13047](http://arxiv.org/abs/2306.13047)

    本文介绍了CamChoice数据集，该数据集包含多项选择理解问题和真实候选答案选项分布，为候选人分布匹配任务提供了自动评估方式。

    

    多项选择题是用于衡量候选人在各种领域和任务中能力的普遍评估形式。提出的问题的质量对于测试设计人员非常重要，因此新提出的问题在部署到实际考试之前需要经过几个预测试评估阶段。目前，这个过程是相当手动化的，这可能导致问题开发周期的时间滞后。自动化此过程将大大提高效率，然而目前的数据集不包含足够的预测试分析信息。在本文中，我们介绍了CamChoice：一份包含不同目标级别问题和真实候选答案选项分布的多项选择理解数据集。我们引入了候选人分布匹配任务，提出了几种评估指标，并证明了在RACE++上训练的自动系统可以实现该任务。

    Multiple Choice examinations are a ubiquitous form of assessment that is used to measure the ability of candidates across various domains and tasks. Maintaining the quality of proposed questions is of great importance to test designers, and therefore newly proposed questions go through several pre-test evaluation stages before they can be deployed into real-world exams. This process is currently quite manual, which can lead to time lags in the question development cycle. Automating this process would lead to a large improvement in efficiency, however, current datasets do not contain sufficient pre-test analysis information. In this paper, we introduce CamChoice; a multiple-choice comprehension dataset with questions at different target levels, where questions have the true candidate selected options distributions. We introduce the task of candidate distribution matching, propose several evaluation metrics for the task, and demonstrate that automatic systems trained on RACE++ can be lev
    
[^6]: 机器翻译可解释性评估指标的探索

    Towards Explainable Evaluation Metrics for Machine Translation. (arXiv:2306.13041v1 [cs.CL])

    [http://arxiv.org/abs/2306.13041](http://arxiv.org/abs/2306.13041)

    本研究探索机器翻译可解释性评估指标，提供综合综述和最新方法，并贡献下一代方法的愿景。

    

    与传统的词汇重叠度量（如BLEU）不同，大多数当前用于机器翻译评估的指标（例如COMET或BERTScore）基于黑盒子的大型语言模型。它们通常与人类判断具有强相关性，但是最近的研究表明，较低质量的传统指标仍然占主导地位，其中一个潜在原因是它们的决策过程更透明。因此，为了促进新的高质量指标的更广泛接受，解释性变得至关重要。在这篇概念论文中，我们确定了可解释机器翻译指标的关键属性和目标，并提供了最近技术的综合综述，将它们与我们确立的目标和属性联系起来。在这个背景下，我们还讨论基于生成模型（如ChatGPT和GPT4）的可解释指标的最新先进方法。最后，我们贡献了下一代方法的愿景，包括自然语言e。

    Unlike classical lexical overlap metrics such as BLEU, most current evaluation metrics for machine translation (for example, COMET or BERTScore) are based on black-box large language models. They often achieve strong correlations with human judgments, but recent research indicates that the lower-quality classical metrics remain dominant, one of the potential reasons being that their decision processes are more transparent. To foster more widespread acceptance of novel high-quality metrics, explainability thus becomes crucial. In this concept paper, we identify key properties as well as key goals of explainable machine translation metrics and provide a comprehensive synthesis of recent techniques, relating them to our established goals and properties. In this context, we also discuss the latest state-of-the-art approaches to explainable metrics based on generative models such as ChatGPT and GPT4. Finally, we contribute a vision of next-generation approaches, including natural language e
    
[^7]: 无政治智能？审查Delphi在美国有争议政治问题上的回应。

    Apolitical Intelligence? Auditing Delphi's responses on controversial political issues in the US. (arXiv:2306.13000v1 [cs.CY])

    [http://arxiv.org/abs/2306.13000](http://arxiv.org/abs/2306.13000)

    本文审查了用于众包伦理的大型语言模型Delphi在美国政治争议问题中的回应。研究发现，该模型的置信度校准不良，呈现显著的政治倾斜。作者从数据女权主义的角度探讨了中立性的问题，讨论了中立性概念如何转移权力、进一步边缘化无声的声音。

    

    随着生成语言模型在越来越广泛的应用中被部署，有关它们政治价值的担忧已成为前沿问题，各个政治派别对其存在偏见和缺乏中立性的批评纷至沓来。本文通过审计用于众包伦理的大型语言模型Delphi [arXiv:2110.07574]，从政治争议问题的角度，分析了Delphi与各个美国政治分组的不同回应。结果发现，Delphi的置信度校准不良，呈现显著的政治倾斜。基于这些结果，作者从数据女权主义的角度探讨了中立性的问题，讨论了中立性概念如何转移权力、进一步边缘化无声的声音。这些发现有望为关于模型与价值的规范性问题形成更多有思考性的辩论做出贡献。

    As generative language models are deployed in ever-wider contexts, concerns about their political values have come to the forefront with critique from all parts of the political spectrum that the models are biased and lack neutrality. However, the question of what neutrality is and whether it is desirable remains underexplored. In this paper, I examine neutrality through an audit of Delphi [arXiv:2110.07574], a large language model designed for crowdsourced ethics. I analyse how Delphi responds to politically controversial questions compared to different US political subgroups. I find that Delphi is poorly calibrated with respect to confidence and exhibits a significant political skew. Based on these results, I examine the question of neutrality from a data-feminist lens, in terms of how notions of neutrality shift power and further marginalise unheard voices. These findings can hopefully contribute to a more reflexive debate about the normative questions of alignment and what role we 
    
[^8]: 语音情感分段：哪种情感在何时出现？

    Speech Emotion Diarization: Which Emotion Appears When?. (arXiv:2306.12991v1 [cs.CL])

    [http://arxiv.org/abs/2306.12991](http://arxiv.org/abs/2306.12991)

    本研究提出了一种新任务：语音情感分段（SED），旨在反映语音情感的细粒度特性。与此同时，我们还提供了一个可公开访问的语音情感数据集 ZED，并提供了竞争基线。

    

    语音情感识别通常依赖于话语水平的解决方案。然而，通过语音传达的情感应被视为具有确定时间边界的离散语音事件，而不是整个话语的属性。为了反映语音情感的细粒度特性，我们提出了一项新任务：语音情感分段（SED）。正如说话人分段回答“谁何时说话？”的问题，语音情感分段回答“哪种情感何时出现？”的问题。为了促进性能评估和为研究人员建立一个共同的基准，我们引入了 Zaion 情感数据集（ZED），这是一个可公开访问的语音情感数据集，包括在真实生活条件下记录的非演出情感，以及话语中情感片段的手动注释边界。我们提供了竞争基线，并开源了代码和预训练模型。

    Speech Emotion Recognition (SER) typically relies on utterance-level solutions. However, emotions conveyed through speech should be considered as discrete speech events with definite temporal boundaries, rather than attributes of the entire utterance. To reflect the fine-grained nature of speech emotions, we propose a new task: Speech Emotion Diarization (SED). Just as Speaker Diarization answers the question of "Who speaks when?", Speech Emotion Diarization answers the question of "Which emotion appears when?". To facilitate the evaluation of the performance and establish a common benchmark for researchers, we introduce the Zaion Emotion Dataset (ZED), an openly accessible speech emotion dataset that includes non-acted emotions recorded in real-life conditions, along with manually-annotated boundaries of emotion segments within the utterance. We provide competitive baselines and open-source the code and the pre-trained models.
    
[^9]: 基于图卷积神经网络的对话偏离预测

    Conversation Derailment Forecasting with Graph Convolutional Networks. (arXiv:2306.12982v1 [cs.CL])

    [http://arxiv.org/abs/2306.12982](http://arxiv.org/abs/2306.12982)

    该论文通过引入图卷积神经网络，考虑了对话的用户动态和公众对话言论的影响，提出了一种新的预测对话偏离的模型，并在基准数据集上优于传统的序列模型。

    

    线上对话容易被偏离，表现为不尊重言论、言语虐待等恶意交流模式。预测对话偏离可以提前发现偏离迹象，实现对话的积极管理。目前，解决该问题的最新方法是使用将对话视为文本流的序列模型。我们提出了一种新颖的基于图卷积神经网络的模型，考虑了对话用户动态和公众对话言论的影响。通过实证评估，我们表明我们的模型有效地捕捉对话动态，并在CGA和CMV基准数据集上分别优于基准模型1.5％和1.7％。

    Online conversations are particularly susceptible to derailment, which can manifest itself in the form of toxic communication patterns like disrespectful comments or verbal abuse. Forecasting conversation derailment predicts signs of derailment in advance enabling proactive moderation of conversations. Current state-of-the-art approaches to address this problem rely on sequence models that treat dialogues as text streams. We propose a novel model based on a graph convolutional neural network that considers dialogue user dynamics and the influence of public perception on conversation utterances. Through empirical evaluation, we show that our model effectively captures conversation dynamics and outperforms the state-of-the-art models on the CGA and CMV benchmark datasets by 1.5\% and 1.7\%, respectively.
    
[^10]: 使用情感分析和主题建模跟踪Twitter上ChatGPT的公众态度

    Tracking public attitudes toward ChatGPT on Twitter using sentiment analysis and topic modeling. (arXiv:2306.12951v1 [cs.CL])

    [http://arxiv.org/abs/2306.12951](http://arxiv.org/abs/2306.12951)

    本文使用情感分析和主题建模技术研究了Twitter用户对ChatGPT的态度。结果显示总体情感是中性到积极的，最受关注的主题包括人工智能、搜索引擎、教育、写作和问题回答等方面。

    

    ChatGPT作为一个由大型语言模型驱动的聊天机器人在用户基数增长方面创下了新记录。虽然它展示了在多种语言生成任务中的最新能力，但它也引起了广泛的公众关注，涉及其对社会的影响。本文利用自然语言处理方法，通过将情感分析和主题建模技术应用于Twitter数据来调查公众对ChatGPT的态度。我们的结果显示总体情感在很大程度上是中性到积极的，这也适用于不同的职业群体。在众多提到的主题中，最受关注的主题是人工智能、搜索引擎、教育、写作和问题回答等方面。

    ChatGPT sets a new record with the fastest-growing user base, as a chatbot powered by a large language model (LLM). While it demonstrates state-of-the-art capabilities in a variety of language-generating tasks, it also raises widespread public concerns regarding its societal impact. In this paper, we utilize natural language processing approaches to investigate the public attitudes towards ChatGPT by applying sentiment analysis and topic modeling techniques to Twitter data. Our result shows that the overall sentiment is largely neutral to positive, which also holds true across different occupation groups. Among a wide range of topics mentioned in tweets, the most popular topics are Artificial Intelligence, Search Engines, Education, Writing, and Question Answering.
    
[^11]: 可量化Transformer：通过帮助注意力头“什么也不做”去除离群值

    Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing. (arXiv:2306.12929v1 [cs.LG])

    [http://arxiv.org/abs/2306.12929](http://arxiv.org/abs/2306.12929)

    本文提出了一种称为“Helper-Head”的方法，可以通过教授注意头忽略输入和输出的某些部分来消除离群值，从而实现对transformer的可量化。实验结果表明，该方法在低、高比特宽度设置上均优于现有方法，在两个流行的语言建模基准上实现了最先进的结果。

    

    过去几年里，Transformer模型已经被广泛应用于各个领域，特别是大型语言模型已经显著推进了人工智能领域的发展。由于其规模，这些网络的能力已经大大增强，但这是以极大的计算成本为代价的。量化是减少神经网络计算时间和存储器消耗的最有效方法之一。然而，许多研究表明，现代transformer模型往往学习到其激活中的强离群值，这使得它们难以量化。为保持可接受的性能，这些离群值的存在需要将激活置于更高的比特宽度或使用不同的数字格式，进行额外的微调或其他变通方法。本文展示了强离群值与特定注意头行为的相关性，这些头试图学习“无操作”或仅仅是部分残差更新。为了实现注意力头中需要的精确零位，我们引入了一个称为“Helper-Head”的方法，教授注意力头忽略输入和输出的某些部分。我们还引入了一种利用这些额外信息的量化技术，可以使用低精度量化甚至是强离群数据。在几个基准数据集上的实验证明，我们的方法在低、高比特宽度设置上均优于现有方法，在两个流行的语言建模基准上实现了最先进的结果。

    Transformer models have been widely adopted in various domains over the last years, and especially large language models have advanced the field of AI significantly. Due to their size, the capability of these networks has increased tremendously, but this has come at the cost of a significant increase in necessary compute. Quantization is one of the most effective ways to reduce the computational time and memory consumption of neural networks. Many studies have shown, however, that modern transformer models tend to learn strong outliers in their activations, making them difficult to quantize. To retain acceptable performance, the existence of these outliers requires activations to be in higher bitwidth or the use of different numeric formats, extra fine-tuning, or other workarounds. We show that strong outliers are related to very specific behavior of attention heads that try to learn a "no-op" or just a partial update of the residual. To achieve the exact zeros needed in the attention 
    
[^12]: AudioPaLM：一款能说会听的大型语言模型

    AudioPaLM: A Large Language Model That Can Speak and Listen. (arXiv:2306.12925v1 [cs.CL])

    [http://arxiv.org/abs/2306.12925](http://arxiv.org/abs/2306.12925)

    AudioPaLM是一款能够更好地处理语音任务的大型语言模型，它继承了AudioLM的语音身份和语调信息保留能力以及PaLM-2的语言知识，可用于语音识别、语音翻译等应用。

    

    本文介绍了一种用于语音理解和生成的大型语言模型——AudioPaLM。它将基于文本的语言模型PaLM-2[Anil等人，2023]和基于音频的语言模型AudioLM[Borsos等人，2022]结合成一个统一的多模态结构，可以处理和生成文本和语音，包括语音识别和语音翻译等应用。AudioPaLM继承了从AudioLM中保留语音发音者身份和语调等的能力，以及只存在于文本大型语言模型PaLM-2中的语言知识。我们表明，通过使用文本大型语言模型的权重进行初始化，可以改善语音处理，成功地利用了预训练中使用的更大量的文本训练数据来协助语音任务。最终得到的模型在语音翻译任务中明显优于现有系统，并且具有进行零-shot言语文本转换的能力。

    We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for ma
    
[^13]: 跨语言跨时代摘要：数据集、模型和评估

    Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation. (arXiv:2306.12916v1 [cs.CL])

    [http://arxiv.org/abs/2306.12916](http://arxiv.org/abs/2306.12916)

    本文全面研究了跨语言跨时代摘要任务，使用历史幻想文本和维基百科摘要构建了第一个CLCTS语料库，并研究了流行的变压器模型及其中间任务微调的有效性；同时还探讨了ChatGPT在CLCTS中作为摘要器和评估器的潜力。最终发现中间任务微调的端到端模型产生了中等到差的效果，而ChatGPT在没有微调的情况下提供中等到好的摘要质量表现。

    

    尽管摘要已经得到了广泛的研究，但跨语言跨时代摘要(CLCTS)是一个潜力巨大但鲜有研究的领域，它有可能提高跨文化的可访问性、信息共享和理解。本文全面研究了CLCTS任务，包括数据集创建、建模和评估。我们构建了第一个CLCTS语料库，利用历史幻想文本和英语、德语维基百科摘要，并研究了流行的变压器端到端模型以及带有不同中间任务微调任务的有效性。此外，我们探讨了ChatGPT在CLCTS中作为摘要器和评估器的潜力。总体而言，我们报告了人类、ChatGPT以及几个最近的自动评估指标的评估结果，发现我们的中间任务微调的端到端模型产生了从差到中等的摘要质量；ChatGPT作为摘要器(没有任何微调)，提供了中等到好的摘要质量表现。

    While summarization has been extensively researched in natural language processing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a largely unexplored area that has the potential to improve cross-cultural accessibility, information sharing, and understanding. This paper comprehensively addresses the CLCTS task, including dataset creation, modeling, and evaluation. We build the first CLCTS corpus, leveraging historical fictive texts and Wikipedia summaries in English and German, and examine the effectiveness of popular transformer end-to-end models with different intermediate task finetuning tasks. Additionally, we explore the potential of ChatGPT for CLCTS as a summarizer and an evaluator. Overall, we report evaluations from humans, ChatGPT, and several recent automatic evaluation metrics where we find our intermediate task finetuned end-to-end models generate bad to moderate quality summaries; ChatGPT as a summarizer (without any finetuning) provides moderate to good qua
    
[^14]: 隐式语音语言日志分离

    Implicit spoken language diarization. (arXiv:2306.12913v1 [eess.AS])

    [http://arxiv.org/abs/2306.12913](http://arxiv.org/abs/2306.12913)

    该论文研究了使用深度学习方法进行隐式语言信息建模的语音语言日志分离任务，使用 x-vector 方法进行分离时在合成数据和实际数据上的表现分别为 6.78%/7.06% 和 22.50%/60.38%，使用预训练的 wave2vec 嵌入向量可在一定程度上提高性能。

    

    语音语言日志分离（LD）及相关任务主要使用音位法进行研究。音位法主要使用显式的语言建模方法，因此需要中间的音素建模和转录数据。另一方面，深度学习方法建模时间动态性的能力可能有助于通过深度嵌入向量隐式地建模语言信息。

    Spoken language diarization (LD) and related tasks are mostly explored using the phonotactic approach. Phonotactic approaches mostly use explicit way of language modeling, hence requiring intermediate phoneme modeling and transcribed data. Alternatively, the ability of deep learning approaches to model temporal dynamics may help for the implicit modeling of language information through deep embedding vectors. Hence this work initially explores the available speaker diarization frameworks that capture speaker information implicitly to perform LD tasks. The performance of the LD system on synthetic code-switch data using the end-to-end x-vector approach is 6.78% and 7.06%, and for practical data is 22.50% and 60.38%, in terms of diarization error rate and Jaccard error rate (JER), respectively. The performance degradation is due to the data imbalance and resolved to some extent by using pre-trained wave2vec embeddings that provide a relative improvement of 30.74% in terms of JER.
    
[^15]: xSIM++：低资源语言比文本挖掘表现的改进代理

    xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource Languages. (arXiv:2306.12907v1 [cs.CL])

    [http://arxiv.org/abs/2306.12907](http://arxiv.org/abs/2306.12907)

    xSIM++ 是一个新的代理评分用于低资源语言比文本挖掘表现的评估。相较于之前的方法(xSIM), xSIM++利用基于规则的方法提供了更准确的评估并提高了与翻译系统BLEU得分的相关性。

    

    我们引入了一个新的代理评分来评估基于多语言嵌入空间相似性的比文本挖掘：xSIM++。相较于xSIM，这个改进的代理利用基于规则的方法，在任何评估集中扩展英语句子与合成的、难以区分的例子，这更接近于我们在大规模挖掘过程中遇到的场景。我们通过对一组低资源语言运行大量比文本挖掘实验，随后在挖掘的数据上训练神经机器翻译系统来验证这个代理。与xSIM相比，我们展示了xSIM++与训练于挖掘比文本的翻译系统下游BLEU评分之间更好的相关性，为了比文本挖掘性能提供了一个可靠的代理，无需运行昂贵的比文本挖掘管道。xSIM++还报告了不同错误类型的性能，为模型开发提供了更细致的反馈。

    We introduce a new proxy score for evaluating bitext mining based on similarity in a multilingual embedding space: xSIM++. In comparison to xSIM, this improved proxy leverages rule-based approaches to extend English sentences in any evaluation set with synthetic, hard-to-distinguish examples which more closely mirror the scenarios we encounter during large-scale mining. We validate this proxy by running a significant number of bitext mining experiments for a set of low-resource languages, and subsequently train NMT systems on the mined data. In comparison to xSIM, we show that xSIM++ is better correlated with the downstream BLEU scores of translation systems trained on mined bitexts, providing a reliable proxy of bitext mining performance without needing to run expensive bitext mining pipelines. xSIM++ also reports performance for different error types, offering more fine-grained feedback for model development.
    
[^16]: 全球叙事的揭示：一份多语种推特数据集，探讨俄乌冲突的新闻媒体报道

    Unveiling Global Narratives: A Multilingual Twitter Dataset of News Media on the Russo-Ukrainian Conflict. (arXiv:2306.12886v1 [cs.CL])

    [http://arxiv.org/abs/2306.12886](http://arxiv.org/abs/2306.12886)

    研究人员收集了约1.5百万条涵盖60种不同语言的推文，创建了一个多语种推特数据集，重点探讨了俄乌冲突的新闻媒体报道。数据集中的标签可以识别与该话题相关的主体、立场、概念和情感表达。

    

    俄乌冲突一直都是全球媒体密集报道的主题。了解这个话题背后的全球叙事对于旨在从多个层面获取洞见的研究人员来说至关重要。在本文中，我们提出了一份独特的数据集，通过收集并处理世界各地新闻或媒体公司发布在社交媒体上的推文，重点关注这个话题。我们收集了2022年2月至2023年5月的推文，以收集约1.5百万条使用60种不同语言的推文。数据集中的每个推文都附带有处理过的标签，允许对提到的主体、立场、概念和表达的情感进行识别。数据集的可用性为希望从不同方面调查俄乌冲突的全球叙事，例如谁是主要的相关方、持什么态度、这些态度的来源在哪里以及不同的概念如何，提供了有价值的资源。

    The ongoing Russo-Ukrainian conflict has been a subject of intense media coverage worldwide. Understanding the global narrative surrounding this topic is crucial for researchers that aim to gain insights into its multifaceted dimensions. In this paper, we present a novel dataset that focuses on this topic by collecting and processing tweets posted by news or media companies on social media across the globe. We collected tweets from February 2022 to May 2023 to acquire approximately 1.5 million tweets in 60 different languages. Each tweet in the dataset is accompanied by processed tags, allowing for the identification of entities, stances, concepts, and sentiments expressed. The availability of the dataset serves as a valuable resource for researchers aiming to investigate the global narrative surrounding the ongoing conflict from various aspects such as who are the prominent entities involved, what stances are taken, where do these stances originate, and how are the different concepts 
    
[^17]: 与医疗决策相关的电子病历中的自然语言处理：一项系统性评述

    Natural Language Processing in Electronic Health Records in Relation to Healthcare Decision-making: A Systematic Review. (arXiv:2306.12834v1 [cs.CL])

    [http://arxiv.org/abs/2306.12834](http://arxiv.org/abs/2306.12834)

    系统评述NLP在电子病历中发现具有实现临床决策的潜在机会，但仍需克服数据的标准化和特定领域模型的需要等挑战。

    

    背景：自然语言处理（NLP）广泛用于从电子病历（EHR）中提取临床见解，但缺乏注释数据，自动化工具和其他挑战使得全面利用NLP进行EHR受到阻碍。本文研究并比较了各种机器学习（ML），深度学习（DL）和NLP技术，以全面了解这个领域的限制和机会。 方法：在筛选了11个数据库中的261篇文章后，我们选择了127篇文章进行全文审查，涵盖了七类文章：1）医学笔记分类，2）临床实体识别，3）文本摘要，4）深度学习（DL）和迁移学习架构，5）信息提取，6）医疗语言翻译和7）其他NLP应用程序。这项研究遵循了系统评述和Meta分析的报告首选项目（PRISMA）指南。 结果和讨论：EHR是所选文章中最常用的数据类型。研究发现，在医疗决策任务（如医学笔记分类，临床实体识别和EHR中的信息提取）中，NLP技术取得了有希望的结果。然而，仍然存在挑战，如缺乏标准化数据和需要特定领域的模型。研究建议需要进一步努力改进NLP模型在临床决策中的性能和通用性。

    Background: Natural Language Processing (NLP) is widely used to extract clinical insights from Electronic Health Records (EHRs). However, the lack of annotated data, automated tools, and other challenges hinder the full utilisation of NLP for EHRs. Various Machine Learning (ML), Deep Learning (DL) and NLP techniques are studied and compared to understand the limitations and opportunities in this space comprehensively.  Methodology: After screening 261 articles from 11 databases, we included 127 papers for full-text review covering seven categories of articles: 1) medical note classification, 2) clinical entity recognition, 3) text summarisation, 4) deep learning (DL) and transfer learning architecture, 5) information extraction, 6) Medical language translation and 7) other NLP applications. This study follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines.  Result and Discussion: EHR was the most commonly used data type among the selected art
    
[^18]: DSTC 11 Track 4中用于开放域对话系统的鲁棒性和多语言自动评估度量的综述

    Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4. (arXiv:2306.12794v1 [cs.CL])

    [http://arxiv.org/abs/2306.12794](http://arxiv.org/abs/2306.12794)

    本文综述了DSTC 11 Track 4中针对开放域对话系统进行鲁棒性和多语言自动评估的挑战，介绍了提供给参与者的数据集和基线，并总结了表现最佳的系统及其方法。

    

    神经网络的出现和快速发展已经彻底改变了对话系统的研究，并随之引发了关于其自动评估的各种挑战。开放域对话系统的自动评估作为一个开放性挑战已经引起了许多研究人员的关注。尽管一直在努力提高自动评估度量与人类评估的相关性，但很少有尝试评估它们在多个领域和维度上的鲁棒性，而且它们的重点主要集中于英语语言上。所有这些挑战促进了开发可靠的自动评估度量，在各种领域、维度和语言中都能够使用。DSTC11中的这个轨道是促进鲁棒和多语言自动评估度量的持续努力的一部分。本文介绍了提供给参与者的数据集和基线，并讨论了该轨道的提交和结果细节。本文还总结了表现最佳的系统及其方法。

    The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics' correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result det
    
[^19]: 关于生成式检索模型的鲁棒性:基于超出分布视角的研究

    On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective. (arXiv:2306.12756v1 [cs.IR])

    [http://arxiv.org/abs/2306.12756](http://arxiv.org/abs/2306.12756)

    本论文研究了生成式检索模型在超出分布（OOD）泛化方面的鲁棒性，定义了从三个方面衡量OOD鲁棒性，并分析了其与密集检索模型的比较。实验结果表明，生成式检索模型的OOD鲁棒性较弱，特别是在面向任务的超出分布场景中更为明显。针对造成鲁棒性较弱的原因，提出了潜在的解决方案。

    

    最近，生成式检索在信息检索领域日益受到关注，它通过直接生成标识符来检索文档。迄今为止，人们已经付出了很多努力来开发有效的生成式检索模型。然而，在鲁棒性方面却得到的关注较少。当一个新的检索范式进入到真实世界应用中时，衡量超出分布（OOD）泛化也是至关重要的，即生成式检索模型如何泛化到新的分布中。为了回答这个问题，我们首先从检索问题的三个方面定义OOD鲁棒性：1）查询变化；2）未知的查询类型；3）未知任务。基于这个分类法，我们进行实证研究，分析了几个代表性生成式检索模型与密集检索模型在OOD鲁棒性方面的比较。实证结果表明，生成式检索模型的OOD鲁棒性比密集检索模型弱，特别是在面向任务的OOD场景中更明显。我们进一步研究了造成生成式检索模型鲁棒性较弱的原因，并提出了改善它们OOD泛化性能的潜在解决方法。

    Recently, we have witnessed generative retrieval increasingly gaining attention in the information retrieval (IR) field, which retrieves documents by directly generating their identifiers. So far, much effort has been devoted to developing effective generative retrieval models. There has been less attention paid to the robustness perspective. When a new retrieval paradigm enters into the real-world application, it is also critical to measure the out-of-distribution (OOD) generalization, i.e., how would generative retrieval models generalize to new distributions. To answer this question, firstly, we define OOD robustness from three perspectives in retrieval problems: 1) The query variations; 2) The unforeseen query types; and 3) The unforeseen tasks. Based on this taxonomy, we conduct empirical studies to analyze the OOD robustness of several representative generative retrieval models against dense retrieval models. The empirical results indicate that the OOD robustness of generative re
    
[^20]: 生成式多模态实体链接

    Generative Multimodal Entity Linking. (arXiv:2306.12725v1 [cs.CL])

    [http://arxiv.org/abs/2306.12725](http://arxiv.org/abs/2306.12725)

    本文提出了 GEMEL 方法，使用大规模预训练的 LLMs 直接生成目标实体名称，仅调整了极少的模型参数即可实现最先进的 MEL 实验结果。

    

    多模态实体链接是将带有多模态上下文的提及映射到知识库（例如维基百科）中的引用实体的任务。本文提出了一种名为 GEMEL 的简单而有效的生成式多模态实体链接方法，利用大规模预训练的 LLMs 直接生成目标实体名称。我们保持视觉和语言模型冻结，只训练一个线性层以启用跨模态交互。为了将 LLMs 适应 MEL 任务，我们利用 LLMs 的新兴上下文学习能力，通过检索多模态实例作为示范来进行。大量实验表明，仅调整了大约0.3％的模型参数，GEMEL 就实现了最先进的结果。

    Multimodal Entity Linking (MEL) is the task of mapping mentions with multimodal contexts to the referent entities from a knowledge base (e.g., Wikipedia). Prior MEL methods mainly focus on designing complex multimodal interaction mechanisms and require fine-tuning all model parameters, which can be prohibitively costly and difficult to scale in the era of Large Language Models (LLMs). In this work, we propose GEMEL, a simple yet effective Generative Multimodal Entity Linking method, which leverages the capabilities of LLMs from large-scale pre-training to directly generate target entity names. We keep the vision and language model frozen and only train a linear layer to enable cross-modality interactions. To adapt LLMs to the MEL task, we take advantage of the emerging in-context learning (ICL) capability of LLMs by retrieving multimodal instances as demonstrations. Extensive experiments show that with only ~0.3% of the model parameters fine-tuned, GEMEL achieves state-of-the-art resul
    
[^21]: 广告自然语言生成：综述

    Natural Language Generation for Advertising: A Survey. (arXiv:2306.12719v1 [cs.CL])

    [http://arxiv.org/abs/2306.12719](http://arxiv.org/abs/2306.12719)

    本文综述了过去十年中广告自然语言生成的研究进展，介绍了从基于模板的方法到使用神经网络的抽取式和生成式方法的发展，提出了度量优化、忠实度、多样性、多模态以及基准数据集的开发等关键挑战和方向。

    

    自然语言生成方法已经成为帮助广告商增加在线广告数量的有效工具。本综述回顾了过去十年中该领域的研究趋势，从基于模板的方法到使用神经网络的抽取式和生成式方法。此外，还讨论了综述中揭示的关键挑战和方向，包括度量优化、忠实度、多样性、多模态以及基准数据集的开发等。

    Natural language generation methods have emerged as effective tools to help advertisers increase the number of online advertisements they produce. This survey entails a review of the research trends on this topic over the past decade, from template-based to extractive and abstractive approaches using neural networks. Additionally, key challenges and directions revealed through the survey, including metric optimization, faithfulness, diversity, multimodality, and the development of benchmark datasets, are discussed.
    
[^22]: 构建波斯社交微博口语情感分析数据集

    Constructing Colloquial Dataset for Persian Sentiment Analysis of Social Microblogs. (arXiv:2306.12679v1 [cs.CL])

    [http://arxiv.org/abs/2306.12679](http://arxiv.org/abs/2306.12679)

    本文构建了一个60,000条波斯语的社交微博口语文本数据集，提出了一种新的深度卷积神经网络(CNN)模型，用于更有效地分析社交微博中的口语文本情感。

    

    引言：微博网站已成为情感分析和观点挖掘的丰富数据源。但由于微博常缺少句法一致的术语和代表性，因此情感分类通常效率低下。波斯语言有其独特的特性，需要独特的注释数据和模型来完成情感分析任务。方法：本文首先通过协作环境和内部来源方式构建了一个名为ITRC-Opinion的用户意见数据集。其次，提出了一种新的深度卷积神经网络(CNN)模型，用于更有效地分析社交微博中的口语文本情感。

    Introduction: Microblogging websites have massed rich data sources for sentiment analysis and opinion mining. In this regard, sentiment classification has frequently proven inefficient because microblog posts typically lack syntactically consistent terms and representatives since users on these social networks do not like to write lengthy statements. Also, there are some limitations to low-resource languages. The Persian language has exceptional characteristics and demands unique annotated data and models for the sentiment analysis task, which are distinctive from text features within the English dialect. Method: This paper first constructs a user opinion dataset called ITRC-Opinion by collaborative environment and insource way. Our dataset contains 60,000 informal and colloquial Persian texts from social microblogs such as Twitter and Instagram. Second, this study proposes a new deep convolutional neural network (CNN) model for more effective sentiment analysis of colloquial text in s
    
[^23]: Instruct-FinGPT: 通过指令调整普适大型语言模型的金融情感分析

    Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models. (arXiv:2306.12659v1 [cs.CL])

    [http://arxiv.org/abs/2306.12659](http://arxiv.org/abs/2306.12659)

    本研究提出了一种简单而有效的指令调整方法，将少量监督式金融情感分析数据转化为指令数据，并用此方法对通用的LLM进行微调，从而在金融情感分析方面取得了显着进展。

    

    情感分析是发现金融文章、新闻和社交媒体洞察的重要工具，塑造我们对市场走向的理解。尽管大型语言模型（LLM）在金融自然语言处理（NLP）方面具有惊人的能力，但它们仍然难以准确解读数字值并抓住金融背景，从而限制了它们在预测金融情感方面的有效性。在本文中，我们介绍了一种简单而有效的指令调整方法来解决这些问题。通过将少量的监督式金融情感分析数据转化为指令数据，并用此方法对通用的LLM进行微调，我们在金融情感分析方面取得了显着的进展。在实验中，我们的方法优于最先进的监督式情感分析模型，以及广泛使用的LLMs，如ChatGPT和LLaMAs，特别是在数字理解和背景理解是关键因素的情况下。

    Sentiment analysis is a vital tool for uncovering insights from financial articles, news, and social media, shaping our understanding of market movements. Despite the impressive capabilities of large language models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment. In this paper, we introduce a simple yet effective instruction tuning approach to address these issues. By transforming a small portion of supervised financial sentiment analysis data into instruction data and fine-tuning a general-purpose LLM with this method, we achieve remarkable advancements in financial sentiment analysis. In the experiment, our approach outperforms state-of-the-art supervised sentiment analysis models, as well as widely used LLMs like ChatGPT and LLaMAs, particularly in scenarios where numerical understanding and contextual comprehension 
    
[^24]: 利用大型语言模型鉴别和提取罕见病表型

    Identifying and Extracting Rare Disease Phenotypes with Large Language Models. (arXiv:2306.12656v1 [cs.CL])

    [http://arxiv.org/abs/2306.12656](http://arxiv.org/abs/2306.12656)

    研究提出了一个通过大型语言模型自动提取罕见病表型的方法，使用零样本或少样本的提示学习技术，无需对大量语料进行注释。

    

    罕见病（RDs）在全球影响着3亿人，正确的表型分类对诊断和治疗至关重要。然而，RD表型通常嵌入在非结构化文本中，手动提取过程耗时。自然语言处理（NLP）模型可以执行命名实体识别（NER）以自动提取，但其训练中需要大量带注释的语料库。 近期，提示学习作为可以导致更可推广结果的NLP范例出现了，而不需要任何（零样本）或少量标记样本（少样本）。尽管对ChatGPT越来越感兴趣，这是一种革命性的大型语言模型，能够遵循复杂的人类提示并生成高质量的回复，但尚未有人在零样本和少样本设置中研究其在RD NER性能。为此，我们设计了新型提示以提取RD表型，并据我们所知是首次建立基准。

    Rare diseases (RDs) are collectively common and affect 300 million people worldwide. Accurate phenotyping is critical for informing diagnosis and treatment, but RD phenotypes are often embedded in unstructured text and time-consuming to extract manually. While natural language processing (NLP) models can perform named entity recognition (NER) to automate extraction, a major bottleneck is the development of a large, annotated corpus for model training. Recently, prompt learning emerged as an NLP paradigm that can lead to more generalizable results without any (zero-shot) or few labeled samples (few-shot). Despite growing interest in ChatGPT, a revolutionary large language model capable of following complex human prompts and generating high-quality responses, none have studied its NER performance for RDs in the zero- and few-shot settings. To this end, we engineered novel prompts aimed at extracting RD phenotypes and, to the best of our knowledge, are the first the establish a benchmark 
    
[^25]: 基于标签生成的增量分类学习方法

    Class-Incremental Learning based on Label Generation. (arXiv:2306.12619v1 [cs.CL])

    [http://arxiv.org/abs/2306.12619](http://arxiv.org/abs/2306.12619)

    本文提出了一种基于标签生成方法的增量分类学习（CIL）方法（VAG），大幅减少了灾难性遗忘（CF），并更好地保留了预训练模型的可推广表示。

    

    尽管预训练语言模型取得了巨大成功，但对于类别增量学习（CIL）设置，由于灾难性遗忘（CF），使用这些模型进行连续学习仍然是一个挑战。本文发现，如果将CIL定式为一个连续的标签生成问题，则可以大幅减少CF并更好地保留预训练模型的可推广表示。因此，我们提出了一种新的CIL方法（VAG），该方法还利用了词汇表的稀疏性以便于生成，并使用标签语义创建伪重播样本。实验结果表明，VAG的性能比基线大幅优越。

    Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics. Experimental results show that VAG outperforms baselines by a large margin.
    
[^26]: TalkBank多数据库利用的层次化方法

    A Hierarchical Approach to exploiting Multiple Datasets from TalkBank. (arXiv:2306.12596v1 [cs.DB])

    [http://arxiv.org/abs/2306.12596](http://arxiv.org/abs/2306.12596)

    本文介绍了一个管道框架，采用分层搜索方法，实现了复杂数据选择的高效率。该框架可以标准化和清理元数据并集成不同研究的数据集，提供一种在多个数据集之间自动化挖掘信息的方法。

    

    TalkBank是一个在线语言学研究数据共享数据库。然而，现有的TalkBank API具有有限的数据过滤和批处理功能。为了克服这些限制，本文介绍了一个管道框架，采用分层搜索方法，实现了复杂数据选择的高效率。该方法涉及对研究人员可能需要的相关文集进行快速的初步筛选，然后根据具体标准进行目标数据的深入搜索。确定的文件随后被索引，为未来的分析提供更容易的访问。此外，该论文演示了如何通过标准化和清理元数据，将使用该框架策划的不同研究的数据集成，使研究人员能够从一个大型集成数据集中提取见解。虽然该框架是为TalkBank设计的，但也可以适用于处理其他开放科学平台的数据。

    TalkBank is an online database that facilitates the sharing of linguistics research data. However, the existing TalkBank's API has limited data filtering and batch processing capabilities. To overcome these limitations, this paper introduces a pipeline framework that employs a hierarchical search approach, enabling efficient complex data selection. This approach involves a quick preliminary screening of relevant corpora that a researcher may need, and then perform an in-depth search for target data based on specific criteria. The identified files are then indexed, providing easier access for future analysis. Furthermore, the paper demonstrates how data from different studies curated with the framework can be integrated by standardizing and cleaning metadata, allowing researchers to extract insights from a large, integrated dataset. While being designed for TalkBank, the framework can also be adapted to process data from other open-science platforms.
    
[^27]: ARIES: 一份包含科学论文修订的语料库，这些修订是作为对同行评审的回应而进行的

    ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews. (arXiv:2306.12587v1 [cs.CL])

    [http://arxiv.org/abs/2306.12587](http://arxiv.org/abs/2306.12587)

    ARIES是一份包含科学论文修订的语料库，为训练和评估大型语言模型提供了工具。通过评估模型，发现其在寻找对应的修订方面仍存在困难，同时在生成修订时过分遵循反馈的措辞，而不是考虑整体的语义。

    

    根据同行反馈修改科学论文是一项具有挑战性的任务，需要深厚的科学知识和推理能力，同时还需要识别高级反馈中的隐含意义，并在众多可能的方式中选择最佳的方式来更新手稿。我们为大语言模型提出了这个任务，并发布了ARIES数据集，其中包含了评论及其相应的论文修订，以便进行训练和评估模型。我们研究了任务的两个版本：评论-修订对齐和修订生成，并评估了几个基线模型，包括GPT-4。我们发现即使在评论以间接方式表述或修订涉及评论的主旨而非精确要求的情况下，模型仍然难以确定对应于评论的修订。在生成修订时，GPT-4通常能够在表面上处理好评论，但它过分遵循反馈的措辞，而不是考虑整体的语义。

    Revising scientific papers based on peer feedback is a challenging task that requires not only deep scientific knowledge and reasoning, but also the ability to recognize the implicit requests in high-level feedback and to choose the best of many possible ways to update the manuscript in response. We introduce this task for large language models and release ARIES, a dataset of review comments and their corresponding paper edits, to enable training and evaluating models. We study two versions of the task: comment-edit alignment and edit generation, and evaluate several baselines, including GPT-4. We find that models struggle even to identify the edits that correspond to a comment, especially in cases where the comment is phrased in an indirect way or where the edit addresses the spirit of a comment but not the precise request. When tasked with generating edits, GPT-4 often succeeds in addressing comments on a surface level, but it rigidly follows the wording of the feedback rather than t
    
[^28]: 用音韵特征进行形态学变形

    Morphological Inflection with Phonological Features. (arXiv:2306.12581v1 [cs.CL])

    [http://arxiv.org/abs/2306.12581](http://arxiv.org/abs/2306.12581)

    本文探讨了通过利用次要的音韵特征来改进形态学模型性能的方法，设计了两种不同的方法，并在八个语言对上进行了实验。研究结果表明选择要包括的音韵特征和它们的操作方式对性能产生深远影响，而在语言学中通常不被考虑的音韵属性对变形有益。

    

    近年来，强大的神经模型在各种任务（重新）变形和分析方面得到了广泛应用，从而在解决形态学任务方面取得了巨大进展。然而，尤其是当可用的训练数据很少或推广到之前未见的词条时，这些形态学任务不能被认为是解决了。这项工作探讨了形态模型获得目标形态学过程的次要语音特征的各种方式对性能的影响。我们设计了两种方法来实现这个目标：一种方法是将模型保留为旧模型但操作数据以包含特征而不是字符，另一种方法是操作模型以在构建音素表示时考虑音韵特征。我们使用具有浅表字形到音素映射的语言的语言特定语法从标准字形数据收集音素数据，并对八种语言对上的两个变形模型进行实验，几乎每个案例都获得了改进。我们还表明，选择要包括的音韵特征以及它们的操作方式对性能产生深远影响，而在语言学中通常不考虑的音韵属性对变形有益。

    Recent years have brought great advances into solving morphological tasks, mostly due to powerful neural models applied to various tasks as (re)inflection and analysis. Yet, such morphological tasks cannot be considered solved, especially when little training data is available or when generalizing to previously unseen lemmas. This work explores effects on performance obtained through various ways in which morphological models get access to subcharacter phonological features that are the targets of morphological processes. We design two methods to achieve this goal: one that leaves models as is but manipulates the data to include features instead of characters, and another that manipulates models to take phonological features into account when building representations for phonemes. We elicit phonemic data from standard graphemic data using language-specific grammars for languages with shallow grapheme-to-phoneme mapping, and we experiment with two reinflection models over eight language
    
[^29]: NoRefER: 一种基于半监督语言模型微调和对比学习的无参考自动语音识别质量度量方法

    NoRefER: a Referenceless Quality Metric for Automatic Speech Recognition via Semi-Supervised Language Model Fine-Tuning with Contrastive Learning. (arXiv:2306.12577v1 [cs.CL])

    [http://arxiv.org/abs/2306.12577](http://arxiv.org/abs/2306.12577)

    本文提出了一种无参考的ASR质量度量方法，使用半监督语言模型微调和对比学习，可以对ASR假设进行排名，并在选择潜在错误的样本方面具有潜在的潜力。

    

    本文介绍了NoRefER，一种新颖的无参考自动语音识别（ASR）系统质量度量方法。传统的基于参考的ASR评估指标需要昂贵的真实文本成绩单。NoRefER通过使用具有孪生网络架构的对比学习调整多语言语言模型来对ASR假设进行两两排名，从而克服了这个限制。自监督的NoRefER利用对来自ASR的多个压缩级别的假设之间的已知质量关系进行的学习，以按质量排序单个样本内的假设，这对于模型比较至关重要。半监督版本还使用引用数据集来提高其样本间质量排名，对于选择潜在错误的样本至关重要。结果表明，NoRefER与基于参考的指标及其样本内排序高度相关，表明具有无参考ASR评估或A/B测试的巨大潜力。

    This paper introduces NoRefER, a novel referenceless quality metric for automatic speech recognition (ASR) systems. Traditional reference-based metrics for evaluating ASR systems require costly ground-truth transcripts. NoRefER overcomes this limitation by fine-tuning a multilingual language model for pair-wise ranking ASR hypotheses using contrastive learning with Siamese network architecture. The self-supervised NoRefER exploits the known quality relationships between hypotheses from multiple compression levels of an ASR for learning to rank intra-sample hypotheses by quality, which is essential for model comparisons. The semi-supervised version also uses a referenced dataset to improve its inter-sample quality ranking, which is crucial for selecting potentially erroneous samples. The results indicate that NoRefER correlates highly with reference-based metrics and their intra-sample ranks, indicating a high potential for referenceless ASR evaluation or a/b testing.
    
[^30]: 用 NeuBAROCO 评估大型语言模型的三段论推理能力和人类偏见

    Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases. (arXiv:2306.12567v1 [cs.CL])

    [http://arxiv.org/abs/2306.12567](http://arxiv.org/abs/2306.12567)

    本研究使用名为 NeuBAROCO 的数据集，检验了当前大型语言模型在三段论推理中的表现。发现这些模型在处理信念偏见、转化错误和氛围效应等问题时表现欠佳。

    

    本文调查了当前的大型语言模型是否像人类一样在逻辑推理中存在偏见。具体而言，我们关注的是三段论推理，这是人们在推理认知科学中研究过的推理形式。为了方便我们的分析，我们介绍了一个名为 NeuBAROCO 的数据集，最初是为评估人类在三段论推理中的逻辑能力而设计的心理实验。该数据集包括英文和日文的三段论推理。我们研究了人类三段论推理中观察到的三种偏见类型：信念偏见、转化错误和氛围效应。我们的研究结果表明，当前的大型语言模型在涉及这三种类型的问题时更容易出现问题。

    This paper investigates whether current large language models exhibit biases in logical reasoning, similar to humans. Specifically, we focus on syllogistic reasoning, a well-studied form of inference in the cognitive science of human deduction. To facilitate our analysis, we introduce a dataset called NeuBAROCO, originally designed for psychological experiments that assess human logical abilities in syllogistic reasoning. The dataset consists of syllogistic inferences in both English and Japanese. We examine three types of biases observed in human syllogistic reasoning: belief biases, conversion errors, and atmosphere effects. Our findings demonstrate that current large language models struggle more with problems involving these three types of biases.
    
[^31]: SituatedGen: 将地理和时间背景融入生成式常识推理

    SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning. (arXiv:2306.12552v1 [cs.CL])

    [http://arxiv.org/abs/2306.12552](http://arxiv.org/abs/2306.12552)

    本文介绍了一种具有挑战性的任务SituatedGen，要求具有常识推理能力的机器生成一对对比句子，以融入地理和时间背景。作者提出了一种相应的英语数据集，并发现目前的生成式语言模型仍然难以实现具有常识合理性的句子的生成，远远落后于人类表现。

    

    最近，文本生成中的常识推理引起了广泛关注。生成式常识推理是一项任务，要求机器在给定一组关键词的情况下，用常识合理性组合出一句连贯的句子。虽然现有的针对生成式常识推理的数据集依focus everyday scenarios，但是机器在特定的地理和时间背景下理解的能力尚不清楚。我们将这一具有挑战性的任务形式化为SituatedGen，要求具有常识推理能力的机器生成一对对比句子，给定的关键词包括地理或时间实体。我们引入一份相应的英语数据集，其中包含8,268对对比句子，这些句子建立在现有的几个常识推理基准上，人工工作量最小。实验表明，最先进的生成式语言模型难以生成具有常识合理性的句子，并且仍然远远落后于人类表现。

    Recently, commonsense reasoning in text generation has attracted much attention. Generative commonsense reasoning is the task that requires machines, given a group of keywords, to compose a single coherent sentence with commonsense plausibility. While existing datasets targeting generative commonsense reasoning focus on everyday scenarios, it is unclear how well machines reason under specific geographical and temporal contexts. We formalize this challenging task as SituatedGen, where machines with commonsense should generate a pair of contrastive sentences given a group of keywords including geographical or temporal entities. We introduce a corresponding English dataset consisting of 8,268 contrastive sentence pairs, which are built upon several existing commonsense reasoning benchmarks with minimal manual labor. Experiments show that state-of-the-art generative language models struggle to generate sentences with commonsense plausibility and still lag far behind human performance. Our 
    
[^32]: 关于使用RVL-CDIP进行文件分类评估的研究

    On Evaluation of Document Classification using RVL-CDIP. (arXiv:2306.12550v1 [cs.CL])

    [http://arxiv.org/abs/2306.12550](http://arxiv.org/abs/2306.12550)

    本文揭示了RVL-CDIP基准的几个不良特征，并提出了创建新的文件分类基准的建议指南。

    

    RVL-CDIP基准已广泛用于衡量文件分类任务的性能。尽管其使用广泛，但我们揭示了RVL-CDIP基准的几个不良特征，包括（1）大量的标签噪声，我们估计为8.1％（每个文档类别介于1.6％到16.9％不等）;（2）存在许多不明确或多标签文档;（3）测试和训练分裂之间存在巨大重叠，这可能会夸大模型性能指标;和（4）存在敏感的可识别个人身份信息，如美国社会安全号码（SSN）。我们认为使用RVL-CDIP进行基准测试存在风险，因为其有限的范围，存在错误（现代先进模型现在可以实现我们估计的标签错误率内的准确性错误率），缺乏多样性，不够理想用于基准测试。我们进一步倡导创建新的文件分类基准，并提供其构建和评估的建议指南。

    The RVL-CDIP benchmark is widely used for measuring performance on the task of document classification. Despite its widespread use, we reveal several undesirable characteristics of the RVL-CDIP benchmark. These include (1) substantial amounts of label noise, which we estimate to be 8.1% (ranging between 1.6% to 16.9% per document category); (2) presence of many ambiguous or multi-label documents; (3) a large overlap between test and train splits, which can inflate model performance metrics; and (4) presence of sensitive personally-identifiable information like US Social Security numbers (SSNs). We argue that there is a risk in using RVL-CDIP for benchmarking document classifiers, as its limited scope, presence of errors (state-of-the-art models now achieve accuracy error rates that are within our estimated label error rate), and lack of diversity make it less than ideal for benchmarking. We further advocate for the creation of a new document classification benchmark, and provide recomm
    
[^33]: 深度语言网络：使用变分推断联合训练叠加LLM的提示层

    Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference. (arXiv:2306.12509v1 [cs.CL])

    [http://arxiv.org/abs/2306.12509](http://arxiv.org/abs/2306.12509)

    本文提出了一种称为深度语言网络（DLN）的架构，通过联合训练叠加的语言模型层（LLMs），使用变分推断算法进行提示训练，使得DLN-2的性能甚至可以与少量训练数据的GPT-4相媲美。

    

    我们将大型语言模型（LLMs）视为网络中的随机“语言层”，其中可学习的参数是每个层的自然语言“提示”。我们将两个这样的层叠加在一起，将一个层的输出馈送到下一个层。我们将这种堆叠的结构称为“深度语言网络”（DLN）。首先，我们展示如何有效地针对单层语言网络（DLN-1）执行提示优化。然后，我们展示如何训练2层DLNs（DLN-2），其中必须学习两个提示。我们认为第一层的输出是一个潜在变量，需要进行边缘化，并设计了一种联合提示训练的变分推断算法。DLN-2比单层达到更高的性能，有时即使网络中的每个LLM更小且更弱，也可以与少量训练数据的GPT-4相媲美。DLN代码是开源的：https://github.com/microsoft/deep-language-networks。

    We view large language models (LLMs) as stochastic \emph{language layers} in a network, where the learnable parameters are the natural language \emph{prompts} at each layer. We stack two such layers, feeding the output of one layer to the next. We call the stacked architecture a \emph{Deep Language Network} (DLN). We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). We then show how to train 2-layer DLNs (DLN-2), where two prompts must be learnt. We consider the output of the first layer as a latent variable to marginalize, and devise a variational inference algorithm for joint prompt training. A DLN-2 reaches higher performance than a single layer, sometimes comparable to few-shot GPT-4 even when each LLM in the network is smaller and less powerful. The DLN code is open source: https://github.com/microsoft/deep-language-networks .
    
[^34]: 《错误信息作为信息污染》

    Misinformation as Information Pollution. (arXiv:2306.12466v1 [cs.SI])

    [http://arxiv.org/abs/2306.12466](http://arxiv.org/abs/2306.12466)

    社交媒体算法会倾向于推广包括错误信息的有争议帖子，因此将错误信息视为信息污染，通过对错误信息的Pigouv税进行经济激励，可以更有效地控制其传播。

    

    社交媒体反馈算法被设计用于优化在线社交互动以最大化广告收益，因此有推广包括错误信息在内的有争议帖子的倾向。通过将错误信息视为信息污染，我们可以将其与反击污染如碳税等环境政策进行类比。类似于污染，对错误信息的Pigouv税提供了经济激励，以使社交媒体公司更有效地控制错误信息的传播，以避免或减少其错误信息税，同时保留平台响应某些程度的自由。在本文中，我们着眼于Pigouv税的鸟瞰视角，并讨论实施此种课税方案的关键问题和下一步行动。

    Social media feed algorithms are designed to optimize online social engagements for the purpose of maximizing advertising profits, and therefore have an incentive to promote controversial posts including misinformation. By thinking about misinformation as information pollution, we can draw parallels with environmental policy for countering pollution such as carbon taxes. Similar to pollution, a Pigouvian tax on misinformation provides economic incentives for social media companies to control the spread of misinformation more effectively to avoid or reduce their misinformation tax, while preserving some degree of freedom in platforms' response. In this paper, we highlight a bird's eye view of a Pigouvian misinformation tax and discuss the key questions and next steps for implementing such a taxing scheme.
    
[^35]: DEPAC：一份针对抑郁症和焦虑症检测的语音语料库

    DEPAC: a Corpus for Depression and Anxiety Detection from Speech. (arXiv:2306.12443v1 [eess.AS])

    [http://arxiv.org/abs/2306.12443](http://arxiv.org/abs/2306.12443)

    本文介绍了一份新颖的语音数据集DEPAC，该数据集标记了抑郁症和焦虑症标准筛查工具上的门槛。此外，作者还提出了一组手工筛选的声学和语言特征，可以有效地识别人类语音中的精神疾病迹象。该研究为自动诊断系统的开发提供了信息丰富且平衡的语料库。

    

    心理困扰，比如抑郁症和焦虑症，对全球疾病负担的贡献最大。受到人工智能领域最新技术的影响，这些障碍的自动诊断系统可以为受影响的人们减少痛苦。这种系统的开发需要信息丰富且平衡的语料库。在这项工作中，我们介绍了一份新颖的心理困扰分析音频数据集DEPAC，基于抑郁症和焦虑症标准筛查工具上的已建立门槛进行标记。这个大型数据集包括每个个体的多个语音任务以及相关的人口统计信息。同时，我们提出了一个特征集，包括手工筛选的声学和语言特征，在人类语音中识别精神疾病迹象方面发挥了效果。最后，我们通过比较基线的性能来证明我们提出的音频语料库和特征集在预测抑郁症严重程度方面的质量和有效性。

    Mental distress like depression and anxiety contribute to the largest proportion of the global burden of diseases. Automated diagnosis systems of such disorders, empowered by recent innovations in Artificial Intelligence, can pave the way to reduce the sufferings of the affected individuals. Development of such systems requires information-rich and balanced corpora. In this work, we introduce a novel mental distress analysis audio dataset DEPAC, labeled based on established thresholds on depression and anxiety standard screening tools. This large dataset comprises multiple speech tasks per individual, as well as relevant demographic information. Alongside, we present a feature set consisting of hand-curated acoustic and linguistic features, which were found effective in identifying signs of mental illnesses in human speech. Finally, we justify the quality and effectiveness of our proposed audio corpus and feature set in predicting depression severity by comparing the performance of bas
    
[^36]: Quilt-1M: 癌症组织学图像文字对的百万数据集

    Quilt-1M: One Million Image-Text Pairs for Histopathology. (arXiv:2306.11207v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.11207](http://arxiv.org/abs/2306.11207)

    本文介绍了一个名为 Quilt-1M 的癌症组织学图像和文字对的百万数据集，并利用 YouTube 上的专家医生教程视频为主要来源。这个数据集将使得癌症组织学领域的表征学习取得类似于其他领域的进展。

    

    多模态应用的加速使得在线图像和文字数据大量涌现，但医学领域（特别是癌症组织学）类似的数据却很稀少，这阻碍了医学领域的进展。本文利用YouTube上的专家医生教程视频，从中选择了 1,087 小时的医学组织学视频，以此自动筛选出共包含 768,826 个癌症组织学图像及其对应的文字对的 Quilt 数据集。

    Recent accelerations in multi-modal applications have been made possible with the plethora of image and text data available online. However, the scarcity of analogous data in the medical field, specifically in histopathology, has halted comparable progress. To enable similar representation learning for histopathology, we turn to YouTube, an untapped resource of videos, offering $1,087$ hours of valuable educational histopathology videos from expert clinicians. From YouTube, we curate Quilt: a large-scale vision-language dataset consisting of $768,826$ image and text pairs. Quilt was automatically curated using a mixture of models, including large language models, handcrafted algorithms, human knowledge databases, and automatic speech recognition. In comparison, the most comprehensive datasets curated for histopathology amass only around $200$K samples. We combine Quilt with datasets from other sources, including Twitter, research papers, and the internet in general, to create an even l
    
[^37]: 揭秘 GPT 自我修复代码生成能力

    Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])

    [http://arxiv.org/abs/2306.09896](http://arxiv.org/abs/2306.09896)

    本文分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，自我修复在较短和较简单的任务中效果更好，仅在某些代码部分上应用自我修复可以非常有效，本文提出的引导修复方法在 APPS 数据集上获得性能提升。

    

    大型语言模型 (LLM) 在代码生成方面表现出色，但在挑战性编程任务上仍面临困难。自我修复——即模型调试并修复自己的代码——最近成为提高性能的一种流行方式。然而，关于自我修复如何有效地发挥作用的研究还非常有限。有人会想知道，当同一模型生成代码时，模型究竟能否提供准确的反馈。在本文中，我们分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，这是一个由多种编码挑战组成的具有挑战性的数据集。我们首先建立了一种新的评估策略 pass@t，该策略衡量了任务通过率与从模型中抽样的总标记数，从而实现对仅基于抽样的方法的公平比较。通过这种评估策略，我们发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，并确定了影响自我修复表现的几个因素。具体而言，我们发现，在输入噪声较少且模型对初始输出不太自信的较短和较简单的任务中，自我修复效果更好。我们还表明，仅在某些代码部分上应用自我修复可以非常有效。此外，我们提出了一种新的引导修复方法，利用外部反馈来增强 GPT 模型的自我修复能力，在 APPS 数据集上获得性能提升。

    Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness
    
[^38]: 利用增强型大型语言模型（GPT-4）解释法律概念

    Explaining Legal Concepts with Augmented Large Language Models (GPT-4). (arXiv:2306.09525v1 [cs.CL])

    [http://arxiv.org/abs/2306.09525](http://arxiv.org/abs/2306.09525)

    本文评估了利用GPT-4增强解释法律术语性能的方法。与基准设置相比，使用增强的法律信息检索模块可以提高法律术语解释的质量，并消除模型的幻觉问题，从而为在法律领域使用大型语言模型开辟了新的途径。

    

    解释法律开放性术语的含义是法律专业人员的重要任务。先前法院案例中该术语的应用是解释其含义的重要来源。本文评估了GPT-4生成法律术语解释的准确性、清晰性和相关性的性能。我们将GPT-4被直接要求解释法律术语的基准设置的性能与增强方法进行了比较，在增强方法中，一个法律信息检索模块被用来为模型提供相关背景，即来自案例法的句子。我们发现，直接应用GPT-4产生的解释在表面上似乎非常高质量。然而，详细分析揭示了解释的实际准确性方面存在的限制。此外，我们发现增强性能可以提高质量，并似乎消除了幻觉问题，即模型发明不正确的陈述。这些发现为在法律领域中使用大型语言模型开辟了新的途径。

    Interpreting the meaning of legal open-textured terms is a key task of legal professionals. An important source for this interpretation is how the term was applied in previous court cases. In this paper, we evaluate the performance of GPT-4 in generating factually accurate, clear and relevant explanations of terms in legislation. We compare the performance of a baseline setup, where GPT-4 is directly asked to explain a legal term, to an augmented approach, where a legal information retrieval module is used to provide relevant context to the model, in the form of sentences from case law. We found that the direct application of GPT-4 yields explanations that appear to be of very high quality on their surface. However, detailed analysis uncovered limitations in terms of the factual accuracy of the explanations. Further, we found that the augmentation leads to improved quality, and appears to eliminate the issue of hallucination, where models invent incorrect statements. These findings ope
    
[^39]: 从零开始实现红队对抗语言模型的探索与建立

    Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])

    [http://arxiv.org/abs/2306.09442](http://arxiv.org/abs/2306.09442)

    本文提出了一种新的红队行动，通过从高层次、抽象的规范出发来考虑语言模型的行为，以探究模型的创新和贡献。

    

    部署大型语言模型（LLMs）可能会产生有害输出，例如有毒或不诚实陈述。先前的研究已经引入了工具以调查有害输出，以识别和减轻这些风险。虽然这是确保语言模型安全的有价值步骤，但这些方法通常依赖于现有的针对不希望的输出的分类器。这限制了它们在只有预先知道有害行为类型的情况下的应用。然而，这跳过了红队行动的核心挑战：开发模型可能展示的行为的上下文理解。此外，当这样的分类器已经存在时，红队行动的边际价值有限，因为分类器可以用于过滤训练数据或模型输出。本文考虑在假设对手从高级、抽象的不良行为规范出发的情况下进行红队行动。红队应该在精化/扩展此规范的同时对抗该模型。

    Deploying Large language models (LLMs) can pose hazards from harmful outputs such as toxic or dishonest speech. Prior work has introduced tools that elicit harmful outputs in order to identify and mitigate these risks. While this is a valuable step toward securing language models, these approaches typically rely on a pre-existing classifier for undesired outputs. This limits their application to situations where the type of harmful behavior is known with precision beforehand. However, this skips a central challenge of red teaming: developing a contextual understanding of the behaviors that a model can exhibit. Furthermore, when such a classifier already exists, red teaming has limited marginal value because the classifier could simply be used to filter training data or model outputs. In this work, we consider red teaming under the assumption that the adversary is working from a high-level, abstract specification of undesired behavior. The red team is expected to refine/extend this spec
    
[^40]: ToolkenGPT：通过工具嵌入扩充冻结语言模型

    ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v1 [cs.CL])

    [http://arxiv.org/abs/2305.11554](http://arxiv.org/abs/2305.11554)

    本论文提出了一种名为ToolkenGPT的方法，将大型语言模型（LLMs）与外部工具相结合，引入了toolken的概念，利用tool embeddings实现无缝交互，同时在各种下游任务上展示出了良好的效果。

    

    将大型语言模型与外部工具结合起来解决复杂问题已成为一种有前途的方法。然而，传统方法需要用工具演示数据对LLM进行微调，既费时又受限于预定义的工具集。最近的上下文学习范例缓解了这些问题，但是有限的上下文长度只允许演示几次，导致对工具的理解不够充分。此外，当有大量工具可供选择时，上下文学习可能完全无法正常工作。在本文中，我们提出了一种$\textbf{ToolkenGPT}$的替代方法，将两种方法的优点结合起来。我们的方法将每个$\underline{工具}$表示为一个$\underline{token}$（$\textit{toolken}$），并为其学习一个嵌入，使得工具调用与生成常规单词标记的方式相同。一旦触发了toolken，LLM被提示完成工具执行所需的参数。ToolkenGPT提供了以下贡献：1）引入了toolken的概念，以扩充LLM与外部工具的交互，2）提出了一种新的学习范例，利用tool embeddings实现无缝交互，3）在各种下游任务上展示了我们方法的有效性。

    Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\underline{tool}$ as a to$\underline{ken}$ ($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the f
    
[^41]: ConvXAI：通过对话提供异构的AI解释，支持人机科技写作

    ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v1 [cs.HC])

    [http://arxiv.org/abs/2305.09770](http://arxiv.org/abs/2305.09770)

    ConvXAI是一个基于对话的XAI系统，它集成了多种XAI类型，并将实际用户需求嵌入设计中，以提高实用性。

    

    尽管已经提出了各种各样的人工智能解释（XAI）方法来解释AI系统，但目前的方法是否对人类实用仍存在不一致的发现。为了改善XAI方法的实用性，一系列研究确定了现实世界中多样化和动态的用户需求与现有XAI方法之间的差距。虽然之前的研究设想将多种XAI方法集成到通用XAI界面（例如，基于对话或GUI的XAI系统）中以减轻这些差距，但缺少针对这些系统如何设计以满足实际用户需求的研究。在本研究中，我们提出了ConvXAI，这是一个基于对话的XAI系统，它结合了多种XAI类型，并赋予用户通过通用的XAI对话界面提出各种XAI问题的能力。特别地，我们创新地将实际用户需求（即，基于格式研究的四个原则）嵌入ConvXAI设计中，以提高实用性。

    While various AI explanation (XAI) methods have been proposed to interpret AI systems, whether the state-of-the-art XAI methods are practically useful for humans remains inconsistent findings. To improve the usefulness of XAI methods, a line of studies identifies the gaps between the diverse and dynamic real-world user needs with the status quo of XAI methods. Although prior studies envision mitigating these gaps by integrating multiple XAI methods into the universal XAI interfaces (e.g., conversational or GUI-based XAI systems), there is a lack of work investigating how these systems should be designed to meet practical user needs. In this study, we present ConvXAI, a conversational XAI system that incorporates multiple XAI types, and empowers users to request a variety of XAI questions via a universal XAI dialogue interface. Particularly, we innovatively embed practical user needs (i.e., four principles grounding on the formative study) into ConvXAI design to improve practical useful
    
[^42]: 探究子词分割对transformer语言模型性能的影响

    Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v1 [cs.CL])

    [http://arxiv.org/abs/2305.05480](http://arxiv.org/abs/2305.05480)

    本文研究使用单语词段算法StateMorph训练语言模型时，可以使模型更高效地收敛并获得更好的验证分数。

    

    我们想研究词段如何影响语言模型的性能。我们使用了一种单语词段算法StateMorph，在芬兰语和俄语中训练了GPT-2和BERT模型。作为比较，我们还训练了一个使用BPE和Morfessor分割算法的模型。我们的初步结果表明，StateMorph可以帮助模型更有效地收敛并获得更好的验证分数。

    We would like to explore how morphemes can affect the performance of a language model. We trained GPT-2 and Bert model with StateMorph for both Finnish and Russian, which is a morpheme segmenting algorithm. As a comparison, we also trained a model with BPE and Morfessor. Our preliminary result shows that StateMorph can help the model to converge more efficiently and achieve a better validation score.
    
[^43]: Seq2seq模型的Token级拟合问题

    Token-Level Fitting Issues of Seq2seq Models. (arXiv:2305.04493v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.04493](http://arxiv.org/abs/2305.04493)

    研究发现使用early-stopping训练的seq2seq模型在token级别存在拟合问题，影响因素包括Token频率、词性和预测差异以及外部因素如语言、模型大小、领域、数据规模和预训练等。

    

    序列到序列（seq2seq）模型已广泛用于自然语言处理、计算机视觉和其他深度学习任务。我们发现，使用early-stopping训练的seq2seq模型在token级别存在问题。特别是，虽然词汇表中的某些token表现出过拟合，但当训练停止时，其他token则表现出欠拟合。实验表明，即使在 fine-tune 的大型预训练模型中，这种现象也很普遍。我们确定了影响Token级别拟合的三个主要因素，包括Token频率、词性和预测差异。此外，我们发现，语言、模型大小、领域、数据规模和预训练等外部因素也可以影响Token的拟合情况。

    Sequence-to-sequence (seq2seq) models have been widely used for natural language processing, computer vision, and other deep learning tasks. We find that seq2seq models trained with early-stopping suffer from issues at the token level. In particular, while some tokens in the vocabulary demonstrate overfitting, others underfit when training is stopped. Experiments show that the phenomena are pervasive in different models, even in fine-tuned large pretrained-models. We identify three major factors that influence token-level fitting, which include token frequency, parts-of-speech, and prediction discrepancy. Further, we find that external factors such as language, model size, domain, data scale, and pretraining can also influence the fitting of tokens.
    
[^44]: 使用大型语言模型探索人类化翻译策略

    Exploring Human-Like Translation Strategy with Large Language Models. (arXiv:2305.04118v1 [cs.CL])

    [http://arxiv.org/abs/2305.04118](http://arxiv.org/abs/2305.04118)

    本文提出了一个名为MAPS的框架，使LLMs能够模仿人类翻译的过程，该过程包括分析源文本并提取关键词、主题和相关演示以指导翻译过程。该框架实验结果显示明显优于多个强基线，为开展使用LLM实现人类化翻译策略的有前途的方向提供了启示。

    

    大型语言模型（LLMs）在各种场景下展现出了惊人的能力，表现出了接近甚至超越人类智能的水平。在其多种技能中，LLM的翻译能力受到了广泛的关注。与传统的机器翻译仅关注源目标映射不同，基于LLM的翻译可以潜在地模仿人类翻译的过程，该过程会采取许多准备步骤以确保高质量的翻译。本文旨在通过提出MAPS框架（Multi-Aspect Prompting and Selection）探索这种可能性。具体来说，我们使LLM首先分析给定源文本并提取三个与翻译相关的知识方面：关键词、主题和相关演示以指导翻译过程。为了过滤掉噪声和无用的知识，我们采用基于质量估计的选择机制。实验证明，我们的框架在多个语言对和翻译方向上显着优于多个强基线。这项工作为开展使用LLM实现人类化翻译策略的有前途的方向提供了启示。

    Large language models (LLMs) have demonstrated impressive capabilities in general scenarios, exhibiting a level of aptitude that approaches, in some aspects even surpasses, human-level intelligence. Among their numerous skills, the translation abilities of LLMs have received considerable attention. In contrast to traditional machine translation that focuses solely on source-target mapping, LLM-based translation can potentially mimic the human translation process that takes many preparatory steps to ensure high-quality translation. This work aims to explore this possibility by proposing the MAPS framework, which stands for Multi-Aspect Prompting and Selection. Specifically, we enable LLMs to first analyze the given source text and extract three aspects of translation-related knowledge: keywords, topics and relevant demonstrations to guide the translation process. To filter out the noisy and unhelpful knowledge, we employ a selection mechanism based on quality estimation. Experiments sug
    
[^45]: 使用ChatGPT进行实体匹配

    Using ChatGPT for Entity Matching. (arXiv:2305.03423v1 [cs.CL])

    [http://arxiv.org/abs/2305.03423](http://arxiv.org/abs/2305.03423)

    本研究探究使用ChatGPT进行实体匹配的可行性，相较于传统方法更为有效，不需大量微调数据，且更加健壮。

    

    实体匹配是判断两个实体描述是否指向同一个真实世界实体的任务。目前最先进的实体匹配方法往往依赖于微调诸如BERT或RoBERTa之类的转换器模型。使用这些模型进行实体匹配的两个主要缺点是，（i）这些模型需要大量的微调数据才能达到良好的性能，（ii）微调后的模型对于分布外的实体不太健壮。在本文中，我们研究了使用ChatGPT进行实体匹配，作为传统转换器模型的更为健壮、数据高效的替代技术。我们从三个维度进行实验：（i）一般提示设计，（ii）上下文学习，以及（iii）提供更高级的匹配知识。我们表明 ChatGPT 与经过微调的 RoBERTa 模型具有竞争力，在一个具有挑战性的匹配任务中达到了平均的零样本性能，为83%的F1值，而 RoBERTa 需要2000个训练样本才能达到类似的性能。

    Entity Matching is the task of deciding if two entity descriptions refer to the same real-world entity. State-of-the-art entity matching methods often rely on fine-tuning Transformer models such as BERT or RoBERTa. Two major drawbacks of using these models for entity matching are that (i) the models require significant amounts of fine-tuning data for reaching a good performance and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. In this paper, we investigate using ChatGPT for entity matching as a more robust, training data-efficient alternative to traditional Transformer models. We perform experiments along three dimensions: (i) general prompt design, (ii) in-context learning, and (iii) provision of higher-level matching knowledge. We show that ChatGPT is competitive with a fine-tuned RoBERTa model, reaching an average zero-shot performance of 83% F1 on a challenging matching task on which RoBERTa requires 2000 training examples for reaching a similar
    
[^46]: SweCTRL-Mini：一种基于Transformer的数据透明的大型语言模型，用于控制性文本生成的瑞典语言版

    SweCTRL-Mini: a data-transparent Transformer-based large language model for controllable text generation in Swedish. (arXiv:2304.13994v1 [cs.CL])

    [http://arxiv.org/abs/2304.13994](http://arxiv.org/abs/2304.13994)

    SweCTRL-Mini是一种基于Transformer的大型瑞典语言模型，用户可以控制它生成的文本流派，完全开放下载。生成能力比较GPT-3。

    

    我们介绍了SweCTRL-Mini，它是一个大规模的瑞典语言模型，可用于单个消费级GPU上的推理和fine-tuning。该模型基于由Keskar、McCann、Varshney、Xiong和Socher（2019）开发的CTRL体系结构，这意味着SweCTRL-Mini模型的用户可以通过在生成提示中插入特殊标记来控制生成文本的流派。SweCTRL-Mini在瑞典部分mC4语料库和一组瑞典小说的子集上进行了训练，我们在本文中提供了(1)所使用的训练数据和文本预处理步骤的详细说明，以使可以检查特定短语/来源是否是训练数据的一部分;(2)使用自动评估方法进行辨别性任务的模型评估，使用人工裁判进行生成性任务的评估;我们还将模型的生成能力与GPT-3的生成能力进行了比较。SweCTRL-Mini 是完全开放的，可供下载。

    We present SweCTRL-Mini, a large Swedish language model that can be used for inference and fine-tuning on a single consumer-grade GPU. The model is based on the CTRL architecture by Keskar, McCann, Varshney, Xiong, and Socher (2019), which means that users of the SweCTRL-Mini model can control the genre of the generated text by inserting special tokens in the generation prompts. SweCTRL-Mini is trained on a subset of the Swedish part of the mC4 corpus and a set of Swedish novels. In this article, we provide (1) a detailed account of the utilized training data and text pre-processing steps, to the extent that it is possible to check whether a specific phrase/source was a part of the training data, and (2) an evaluation of the model on both discriminative tasks, using automatic evaluation methods, and generative tasks, using human referees. We also compare the generative capabilities of the model with those of GPT-3. SweCTRL-Mini is fully open and available for download.
    
[^47]: 通过语言实现视觉抽象和推理技术

    Visual Abstraction and Reasoning through Language. (arXiv:2303.04091v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.04091](http://arxiv.org/abs/2303.04091)

    本论文提出了一种通过自然语言描述任务的通用框架来解决Abstraction and Reasoning Corpus（ARC）问题，虽然还没有在ARC上击败最先进的DSL模型，但我们展示了我们的方法具有巨大的潜力，可以解决先前未解决的任务。

    

    尽管人工智能（AI）模型在局限应用中已经达到了人类甚至超越人类的性能，但它们仍然难以展现更广泛和更灵活的智能。Abstraction and Reasoning Corpus（ARC）旨在评估AI系统与人类类似的认知能力。目前大多数方法依赖于精心设计的特定领域语言（DSL），用于暴力解决ARC中的任务。在这项工作中，我们提出了一个基于任务自然语言描述的通用框架来解决ARC问题。虽然还没有在ARC上击败最先进的DSL模型，但我们展示了我们的方法具有巨大的潜力，可以解决先前未解决的任务。

    While Artificial Intelligence (AI) models have achieved human or even superhuman performance in narrowly defined applications, they still struggle to show signs of broader and more flexible intelligence. The Abstraction and Reasoning Corpus (ARC), introduced by Fran\c{c}ois Chollet, aims to assess how close AI systems are to human-like cognitive abilities. Most current approaches rely on carefully handcrafted domain-specific languages (DSLs), which are used to brute-force solutions to the tasks present in ARC. In this work, we propose a general framework for solving ARC based on natural language descriptions of the tasks. While not yet beating state-of-the-art DSL models on ARC, we demonstrate the immense potential of our approach hinted at by the ability to solve previously unsolved tasks.
    
[^48]: SemSup-XC: 用于零样本和少样本极端分类的语义监督

    SemSup-XC: Semantic Supervision for Zero and Few-shot Extreme Classification. (arXiv:2301.11309v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11309](http://arxiv.org/abs/2301.11309)

    SemSup-XC是一种用于零样本和少样本极端分类的语义监督模型，在自动收集的语义类别描述和新颖的混合匹配模块的帮助下，可以在法律、电子商务和维基百科等三个XC数据集上实现最先进的零样本和少样本性能。

    

    极端分类旨在预测大量类别（数千至数百万个），具有新闻文章分类和电子商务产品标记等实际应用。这项任务的零样本版本需要在无额外监督的情况下泛化到新类别。本文提出了SemSup-XC模型，它使用自动收集的语义类别描述来表示类别，并通过新颖的混合匹配模块使用语义和词汇相似性将输入实例与类别描述进行匹配，使得在法律、电子商务和维基百科等三个XC数据集上实现了最先进的零样本和少样本性能。经过对比学习的训练，SemSup-XC明显优于基线算法，并在所有三个数据集上取得了最先进的性能，其中零样本任务上的精度提高了最多12个百分点，少样本任务上的精度提高了10个百分点以上。

    Extreme classification (XC) involves predicting over large numbers of classes (thousands to millions), with real-world applications like news article classification and e-commerce product tagging. The zero-shot version of this task requires generalization to novel classes without additional supervision. In this paper, we develop SemSup-XC, a model that achieves state-of-the-art zero-shot and few-shot performance on three XC datasets derived from legal, e-commerce, and Wikipedia data. To develop SemSup-XC, we use automatically collected semantic class descriptions to represent classes and facilitate generalization through a novel hybrid matching module that matches input instances to class descriptions using a combination of semantic and lexical similarity. Trained with contrastive learning, SemSup-XC significantly outperforms baselines and establishes state-of-the-art performance on all three datasets considered, gaining up to 12 precision points on zero-shot and more than 10 precision
    
[^49]: 深度学习在数学推理中的应用综述

    A Survey of Deep Learning for Mathematical Reasoning. (arXiv:2212.10535v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.10535](http://arxiv.org/abs/2212.10535)

    本文综述了过去十年中深度学习在数学推理领域的关键任务、数据集和方法，并评估了现有的基准和方法，探讨了未来的研究方向。

    

    数学推理是人类智能的基本组成部分，并且应用广泛，包括科学、工程、金融和日常生活。发展能够解决数学问题和证明定理的人工智能系统在机器学习和自然语言处理领域引起了极大关注。本文回顾了过去十年中在数学推理和深度学习交叉领域的关键任务、数据集和方法，并评估了现有的基准和方法，讨论了未来的研究方向。

    Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.
    
[^50]: 使用社交感知强化学习提高主动对话代理的性能

    Improving Proactive Dialog Agents Using Socially-Aware Reinforcement Learning. (arXiv:2211.15359v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.15359](http://arxiv.org/abs/2211.15359)

    本论文的主要贡献在于提出了一种新的方法，通过将社交和任务相关特征考虑在对话中，来优化主动对话策略，使其在任务效率高的同时，也能促进用户信任，从而提高人机交互的成功率和效率。

    

    智能对话代理的下一个步骤是从旁观者的角色中解脱出来，变得更加主动。明确定义的主动行为可以改善人机合作，因为代理在交互过程中扮演更积极的角色并解除了用户的责任。然而，主动性是一把双刃剑，因为执行不当的预防性行动可能不仅对任务结果产生破坏性影响，而且还会对与用户的关系产生影响。为了设计合适的主动对话策略，我们提出了一种新的方法，将社交和任务相关特征都考虑在对话中。这里的主要目标是优化主动行为，使其任务导向——这意味着高任务成功率和效率——同时在促进用户信任时也具有社交效益。将这两个方面包含在用强化学习训练主动对话代理的奖励函数中，显示出我们的方法对于更加成功的人机交互的益处。

    The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive. Well-defined proactive behavior may improve human-machine cooperation, as the agent takes a more active role during interaction and takes off responsibility from the user. However, proactivity is a double-edged sword because poorly executed pre-emptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user. For designing adequate proactive dialog strategies, we propose a novel approach including both social as well as task-relevant features in the dialog. Here, the primary goal is to optimize proactive behavior so that it is task-oriented - this implies high task success and efficiency - while also being socially effective by fostering user trust. Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for more successful human-mach
    
[^51]: 串行采样块式Conformer网络在流式端到端ASR中的应用

    Sequentially Sampled Chunk Conformer for Streaming End-to-End ASR. (arXiv:2211.11419v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.11419](http://arxiv.org/abs/2211.11419)

    本论文提出了一种称为 SSC-Conformer 的块式模型，利用串行采样块自注意力机制提高块间交互效率，同时保持线性复杂度，将块卷积与因果卷积相结合以达到更好的 CER 表现，实验结果表明 SSC-Conformer 在 AISHELL-1 基准测试中取得了最新的流式 E2E ASR 性能水平。

    

    本文针对流式端到端语音识别 (E2E ASR) 提出了一种名为 SSC-Conformer 的串行采样块式 Conformer 模型。该模型使用串行采样块多头自注意力机制 (SSC-MHSA) 来提高跨块交互的效率，同时保持线性复杂度。此外，本文还提出利用块卷积来增加块级未来上下文，并将其与卷积层的因果卷积相结合以进一步降低 CER。在 AISHELL-1 基准测试中，实验结果表明 SSC-Conformer 在无语言模型重打分的情况下可以实现 CER 5.33%，达到了流式 E2E ASR 的最新性能水平，并且由于其线性复杂度，可以使用更大的批量进行训练并更高效地推理。

    This paper presents an in-depth study on a Sequentially Sampled Chunk Conformer, SSC-Conformer, for streaming End-to-End (E2E) ASR. The SSC-Conformer first demonstrates the significant performance gains from using the sequentially sampled chunk-wise multi-head self-attention (SSC-MHSA) in the Conformer encoder by allowing efficient cross-chunk interactions while keeping linear complexities. Furthermore, it explores taking advantage of chunked convolution to make use of the chunk-wise future context and integrates with casual convolution in the convolution layers to further reduce CER. We verify the proposed SSC-Conformer on the AISHELL-1 benchmark and experimental results show that a state-of-the-art performance for streaming E2E ASR is achieved with CER 5.33% without LM rescoring. And, owing to its linear complexity, the SSC-Conformer can train with large batch sizes and infer more efficiently.
    
[^52]: VL-CheckList: 使用物体、属性和关系评估预训练的视觉语言模型

    VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations. (arXiv:2207.00221v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.00221](http://arxiv.org/abs/2207.00221)

    本研究提出VL-CheckList，使用物体、属性和关系评估预训练的视觉语言模型，通过对七种流行的VLP模型进行全面研究分析，揭示出不同模型之间的细微差异。

    

    最近，视觉语言预训练（VLP）模型已经成功地促进了许多跨模态的下游任务。然而，现有的大多数工作都是通过比较下游任务的性能来评估它们的系统。然而，仅有的下游任务平均准确性提供很少关于每种VLP方法的优缺点的信息，更不用说为社区在未来如何改进系统提供见解了。受自然语言处理测试CheckList的启发，我们提出了VL-CheckList，这是一个新颖的框架，用于了解VLP模型的能力。所提出的方法将VLP模型的图像-文本能力分为三类：物体、属性和关系，并使用新颖的分类法进一步分解这三个方面。我们通过该框架对七种最近流行的VLP模型进行全面研究分析。结果通过揭示比较模型之间的细微差异来确认所提出方法的有效性。

    Vision-Language Pretraining (VLP) models have recently successfully facilitated many cross-modal downstream tasks. Most existing works evaluated their systems by comparing the fine-tuned downstream task performance. However, only average downstream task accuracy provides little information about the pros and cons of each VLP method, let alone provides insights on how the community can improve the systems in the future. Inspired by the CheckList for testing natural language processing, we exploit VL-CheckList, a novel framework to understand the capabilities of VLP models. The proposed method divides the image-texting ability of a VLP model into three categories: objects, attributes, and relations, and uses a novel taxonomy to further break down these three aspects. We conduct comprehensive studies to analyze seven recently popular VLP models via the proposed framework. Results confirm the effectiveness of the proposed method by revealing fine-grained differences among the compared mode
    
[^53]: 《Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation》 （arXiv:2205.12593v2 [cs.CL] UPDATED）

    Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation. (arXiv:2205.12593v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.12593](http://arxiv.org/abs/2205.12593)

    本研究提出了一种训练策略Less-Learn-Shortcut (LLS)，以减少模型过度依赖单词与标签之间的错误相关性。同时，通过量化有偏样本的有偏程度，加强无偏样本的训练。

    

    最近的研究表明，深度神经网络通常将数据集的偏置作为捷径来做出决策，而不是理解任务，导致在现实世界的应用中失败。本研究聚焦于模型从训练数据的偏置分布中学到的单词特征与标签间的错误相关性。特别地，我们将高度共现在某个特定标签下的单词定义为有偏单词，将包含有偏单词的样本定义为有偏样本。我们的分析表明，有偏样本对于模型来说更容易学习，在预测时，有偏单词在模型的预测中作出了显著的贡献，模型往往过度依赖单词与标签之间的错误相关性来进行标签预测。为了缓解模型过度依赖这种捷径（即错误相关性），我们提出了一种训练策略Less-Learn-Shortcut（LLS）：我们的策略量化了有偏样本的有偏程度，同时加强了无偏样本的训练。

    Recent research has revealed that deep neural networks often take dataset biases as a shortcut to make decisions rather than understand tasks, leading to failures in real-world applications. In this study, we focus on the spurious correlation between word features and labels that models learn from the biased data distribution of training data. In particular, we define the word highly co-occurring with a specific label as biased word, and the example containing biased word as biased example. Our analysis shows that biased examples are easier for models to learn, while at the time of prediction, biased words make a significantly higher contribution to the models' predictions, and models tend to assign predicted labels over-relying on the spurious correlation between words and labels. To mitigate models' over-reliance on the shortcut (i.e. spurious correlation), we propose a training strategy Less-Learn-Shortcut (LLS): our strategy quantifies the biased degree of the biased examples and d
    
[^54]: EmTract：从社交媒体中提取情绪

    EmTract: Extracting Emotions from Social Media. (arXiv:2112.03868v3 [q-fin.PR] UPDATED)

    [http://arxiv.org/abs/2112.03868](http://arxiv.org/abs/2112.03868)

    本文描述了一个名为EmTract的开源工具，其基于预调整的NLP模型DistilBERT和4,861个标记，提取面向金融环境的社交媒体文本中的情绪，并在人工和chatGPT注释数据上优于竞争的开源最先进情感分类器，并且方法具有量身定制的特点以及针对非标准短语、表情符号和表情符号这类社交媒体数据。

    

    我们开发了一个开源工具（EmTract），用于从面向金融环境的社交媒体文本中提取情绪。为此，我们标注了一万条来自金融社交媒体平台StockTwits的短消息，并将其与开源情绪数据结合使用。然后，我们使用预先调整好的NLP模型DistilBERT，通过包括4,861个标记（表情符号和表情符号）来增加其嵌入空间，然后首先在开源情感数据上对其进行拟合，然后将其转移至我们标注的金融社交媒体数据。我们的模型在人工和chatGPT注释数据上均优于竞争的开源最先进情感分类器，如Emotion English DistilRoBERTa-base。与基于字典的方法相比，我们的方法在金融研究中具有三个主要优点。首先，我们的模型针对金融社交媒体文本进行了量身定制；其次，它包含社交媒体数据的关键方面，如非标准短语、表情符号和表情符号；第三，它通过顺序学习进行操作。

    We develop an open-source tool (EmTract) that extracts emotions from social media text tailed for financial context. To do so, we annotate ten thousand short messages from a financial social media platform (StockTwits) and combine it with open-source emotion data. We then use a pre-tuned NLP model, DistilBERT, augment its embedding space by including 4,861 tokens (emojis and emoticons), and then fit it first on the open-source emotion data, then transfer it to our annotated financial social media data. Our model outperforms competing open-source state-of-the-art emotion classifiers, such as Emotion English DistilRoBERTa-base on both human and chatGPT annotated data. Compared to dictionary based methods, our methodology has three main advantages for research in finance. First, our model is tailored to financial social media text; second, it incorporates key aspects of social media data, such as non-standard phrases, emojis, and emoticons; and third, it operates by sequentially learning 
    

