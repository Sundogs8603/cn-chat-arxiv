# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Rethinking Generative Large Language Model Evaluation for Semantic Comprehension](https://arxiv.org/abs/2403.07872) | 通过引入RWQ-Elo评级系统，在新基准“真实世界问题”上对24种大型语言模型进行两人竞争评估，以解决多选问题回答方法在语义理解评估中的潜在缺陷。 |
| [^2] | [Exploring Safety Generalization Challenges of Large Language Models via Code](https://arxiv.org/abs/2403.07865) | 本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。 |
| [^3] | [The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing](https://arxiv.org/abs/2403.07825) | 本文提出了一种新的评估方法 GORA 和一种模型编辑方法 SORA，用以解决模型编辑中的隐藏空间中的涟漪效应问题。 |
| [^4] | [Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM](https://arxiv.org/abs/2403.07816) | BTX方法提供了一种高效的训练大型语言模型以具备多个专业领域能力的方法，通过将种子模型的分支培训成专家，然后在Mixture-of-Expert层中将它们汇集为专家，并利用MoE微调阶段学习基于标记的路由，从而实现了最佳的精度和效率权衡。 |
| [^5] | [pyvene: A Library for Understanding and Improving PyTorch Models via Interventions](https://arxiv.org/abs/2403.07809) | pyvene是一个开源Python库，支持在PyTorch模型上进行可定制的干预，提供统一和可扩展的框架，用于解释神经模型并与他人分享经过干预的模型。 |
| [^6] | [Beyond Memorization: The Challenge of Random Memory Access in Language Models](https://arxiv.org/abs/2403.07805) | 本文研究了语言模型在访问内存时的挑战，发现通过背诵和置换等技术可以改善语言模型的随机内存访问能力，从而在开放域问题回答任务中取得显著改进。 |
| [^7] | [Fine-tuning Large Language Models with Sequential Instructions](https://arxiv.org/abs/2403.07794) | 通过顺序指令微调，研究提出了一种简单且有效的策略，可以使大型语言模型具备执行多个顺序指令的能力，优于传统指令微调模型。 |
| [^8] | [Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations](https://arxiv.org/abs/2403.07769) | 文章探讨了基于多Agent系统理论结合大型语言模型的计算实体对人类互动的革新影响，提出了一种可能将专门人工代理支持扩展到操作性组织流程和基于知识和人类编排的战略决策的方式。 |
| [^9] | [FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models](https://arxiv.org/abs/2403.07747) | FineMath是一个用于评估中文大型语言模型数学推理能力的细粒度数学评估基准数据集，涵盖小学数学中的主要概念，划分为17类数学问题，并手动注释难度级别，实验证实在数学方面仍有改进空间。 |
| [^10] | [SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes](https://arxiv.org/abs/2403.07726) | 本文介绍了SHROOM共享任务，重点关注检测幻觉，以及参与者使用的模型和策略。 |
| [^11] | [StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models](https://arxiv.org/abs/2403.07714) | StableToolBench提出了一种虚拟API服务器和稳定评估系统，通过缓存系统、API模拟器和稳定评估设计，解决了大语言模型利用外部工具的稳定大规模基准测试问题。 |
| [^12] | [Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards](https://arxiv.org/abs/2403.07708) | 引入对比奖励的方法提高了从人类反馈中的强化学习的效果，改善了奖励模型的鲁棒性，鼓励对基准的改善，并能根据任务的难度进行校准。 |
| [^13] | [Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization](https://arxiv.org/abs/2403.07693) | 使用大型和小型语言模型的新型数据增强框架，通过重新生成评论来实现观点摘要的去偏见化，避免了大型语言模型数据增强可能存在的问题和高昂成本。 |
| [^14] | [Reference-free Monolithic Preference Optimization with Odds Ratio](https://arxiv.org/abs/2403.07691) | 本文介绍了一种无参考单体赔率比偏好优化算法ORPO，在SFT过程中通过轻微惩罚不受欢迎的生成风格，消除了额外的偏好对齐阶段 |
| [^15] | [SATDAUG -- A Balanced and Augmented Dataset for Detecting Self-Admitted Technical Debt](https://arxiv.org/abs/2403.07690) | SATDAUG这一平衡和增强的数据集旨在解决自认技术债识别和分类中现有数据集存在的类别不平衡问题。 |
| [^16] | [Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost](https://arxiv.org/abs/2403.07687) | 提出一种方法来识别数据，以平衡模型性能和注释成本 |
| [^17] | [MoralBERT: Detecting Moral Values in Social Discourse](https://arxiv.org/abs/2403.07678) | MoralBERT 是一种专门设计用于捕捉文本中道德微妙之处的语言表示模型，利用来自Twitter、Reddit和Facebook的数据，扩大了模型理解道德的能力。 |
| [^18] | [Harder Tasks Need More Experts: Dynamic Routing in MoE Models](https://arxiv.org/abs/2403.07652) | 通过动态选择专家来提高计算效率和模型性能，针对不同难度的任务激活不同数量的专家，相比传统的Top-K路由方法，我们的动态路由方法在各种基准测试中取得了明显的改进。 |
| [^19] | [LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model](https://arxiv.org/abs/2403.07581) | 提出了一种基于大型语言模型的文本增强增强个性检测模型，通过提炼大型语言模型的知识，增强小模型的性能，即使大型语言模型在任务中失败。 |
| [^20] | [Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource Agglutinative Data-to-Text Generation](https://arxiv.org/abs/2403.07567) | 本文针对低资源聚集性语言isiXhosa进行数据到文本处理，引入了新的数据集T2X和SSPG架构，开发了评估框架，帮助用户更准确描述数据。 |
| [^21] | [SIFiD: Reassess Summary Factual Inconsistency Detection with LLM](https://arxiv.org/abs/2403.07557) | 本研究重新评估了使用LLM进行摘要不一致性检测的方法，提出了SIFiD（带有过滤文档的摘要不一致性检测），旨在通过自然语言推理或测量语义相似性来识别文档中的关键句子。 |
| [^22] | [Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts](https://arxiv.org/abs/2403.07556) | 提出了一种名为真相感知的上下文选择（TACS）的轻量级方法，可以通过对输入上下文进行真相检测并构建相应的注意力蒙版来缓解大型语言模型被不真实上下文误导产生幻觉 |
| [^23] | [MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki](https://arxiv.org/abs/2403.07544) | 本文介绍了MAMMOTH工具包，这是一个旨在训练大规模多语言模块化机器翻译系统的框架，通过模块化设计实现了在计算集群上的高效训练。 |
| [^24] | [Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2403.07440) | 该论文提出了基于矩阵变换的低秩调整（MTLoRA）方法，受大脑启发，用于提高微调技术的复杂任务适应性、性能、稳定性和算法复杂性。 |
| [^25] | [Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs](https://arxiv.org/abs/2403.07398) | 提出了COM2数据集，通过在常识知识图中抽样多跳逻辑查询并结合大型语言模型，显著提高了语言模型在复杂推理能力方面的性能。 |
| [^26] | [SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models](https://arxiv.org/abs/2403.07384) | S2L提出了一种通过总结小模型的训练轨迹，来指导大型语言模型数据选择的方法，显著提高了数学问题解决中监督微调的数据效率，并在数据集性能上表现优异。 |
| [^27] | [Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The Lengths, Bends, and Dead Ends](https://arxiv.org/abs/2403.07379) | 分析神经网络和LLMs中优化轨迹的复杂性，揭示了优化过程中的关键特征，包括方向探索和方向正则化。 |
| [^28] | [SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression](https://arxiv.org/abs/2403.07378) | SVD-LLM是一种新的基于SVD的LLM压缩方法，通过截断感知数据白化策略和逐层闭式模型参数更新策略，解决了现有方法的限制，实现了直接映射奇异值和压缩损失之间的关系。 |
| [^29] | [NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning](https://arxiv.org/abs/2403.07376) | 本文提出了一种名为NavCoT的新策略，在视觉与语言导航中通过学习解耦推理，实现了自主导航决策，有效减轻了领域差距。 |
| [^30] | [KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models](https://arxiv.org/abs/2403.07350) | KEBench提出了一个新的基准测试，采用不同的数据收集方法和新增加的度量标准（可移植性），以全面评估大型视觉-语言模型知识编辑的质量。 |
| [^31] | [Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive Learning](https://arxiv.org/abs/2403.07342) | 提出一种新颖的标记方案，并采用对比学习方法来重新思考ASTE，该方法在性能上优于最先进技术，同时具有更紧凑的设计和降低的计算开销，尤其在少样本学习情景下展现出优越效果。 |
| [^32] | [IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers](https://arxiv.org/abs/2403.07339) | 本研究旨在验证在移除低位宽限制时，对于各种Transformer-based模型，整数是否足以满足所有GEMM需求（训练和推断阶段），并且可以与浮点数相媲美，而无需复杂技巧。 |
| [^33] | [GPT-generated Text Detection: Benchmark Dataset and Tensor-based Detection Method](https://arxiv.org/abs/2403.07321) | 介绍了一个新的GPT生成文本检测数据集GRiD，用于评估检测模型性能和区分人类与ChatGPT生成响应。提供了数据集特征分析和多种检测方法基准测试结果。 |
| [^34] | [Knowledge Graph Large Language Model (KG-LLM) for Link Prediction](https://arxiv.org/abs/2403.07311) | 该论文提出了知识图谱大型语言模型框架（KG-LLM），利用思维链提示和上下文学习等NLP范例，以增强知识图谱中的多跳链接预测，并展示了框架在微调大型语言模型和零次尝试能力方面的有效性。 |
| [^35] | [Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation](https://arxiv.org/abs/2403.07300) | 通过跨模态知识蒸馏和LLMs对齐框架，该方法利用静态和动态知识，充分释放LLMs在时间序列预测中的潜力 |
| [^36] | [A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism](https://arxiv.org/abs/2403.07283) | 介绍了一种名为CypherTalk的成本效益和自适应的LLM摇晃调整和恢复机制，通过优化摇晃操作符设置，实现了在成本、模型效用和隐私之间权衡的结果。 |
| [^37] | [A Survey of Explainable Knowledge Tracing](https://arxiv.org/abs/2403.07279) | 对知识追踪算法进行了解释性分析，提出了可解释知识追踪模型分为透明模型和黑盒模型，探讨了不同阶段的解释性方法。 |
| [^38] | [CKERC : Joint Large Language Models with Commonsense Knowledge for Emotion Recognition in Conversation](https://arxiv.org/abs/2403.07260) | 提出了一种基于常识知识的大型语言模型联合情感识别对话框架，通过设计提示生成互动者的常识知识并利用这些信息进行预训练，以提高说话者的情感识别准确性。 |
| [^39] | [Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences](https://arxiv.org/abs/2403.07230) | 提出了一种名为Curry-DPO的方法，在直接偏好优化(DPO)中利用课程学习方法，通过构建多个偏好对来训练模型，相比于标准单一对DPO设置有着更好的性能表现。 |
| [^40] | [SPAWNing Structural Priming Predictions from a Cognitively Motivated Parser](https://arxiv.org/abs/2403.07202) | 提出了一个框架，利用实证启动模式来建立理论，使用认知驱动解析器SPAWN生成量化启动预测并评估，并以简化的定语从句为案例研究，发现一个理论的启动预测与实证启动模式一致 |
| [^41] | [CuentosIE: can a chatbot about "tales with a message" help to teach emotional intelligence?](https://arxiv.org/abs/2403.07193) | CuentosIE提供了一套高度专业化的寓言故事，并提供了一系列工具，旨在教育用户情感知识，并监测他们的情感发展。 |
| [^42] | [$\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model](https://arxiv.org/abs/2403.07191) | 提出了一种$(N,K)$-Puzzle测试平台，用于评估和比较生成语言模型中的强化学习算法。 |
| [^43] | [Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews](https://arxiv.org/abs/2403.07183) | 该研究提出了一种估计大语料库中被大语言模型大幅修改的文本比例的方法，并在AI会议的同行评审中进行了实证分析，发现6.5%至16.9%的文本可能被LLMs大幅修改，揭示了用户行为的一些见解。 |
| [^44] | [3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs](https://arxiv.org/abs/2403.07179) | 提出了3M-Diffusion，一种新颖的多模态分子图生成方法，可以生成具有所需属性的多样化、理想情况下是新颖的分子。 |
| [^45] | [Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing](https://arxiv.org/abs/2403.07175) | 本文重建了ROME，提供了更稳定的r-ROME实现，解决了顺序模型编辑过程中的模型崩溃问题。 |
| [^46] | [Thought Graph: Generating Thought Process for Biological Reasoning](https://arxiv.org/abs/2403.07144) | Thought Graph框架通过基因集分析揭示生物过程间的语义关系，并在与人类注释的余弦相似性方面优于传统方法。 |
| [^47] | [One Category One Prompt: Dataset Distillation using Diffusion Models](https://arxiv.org/abs/2403.07142) | 提出了使用扩散模型进行数据集精炼的新方法，有效地解决了数据集精炼在处理高分辨率图像和复杂架构时的可扩展性问题 |
| [^48] | [Narrating Causal Graphs with Large Language Models](https://arxiv.org/abs/2403.07118) | 本研究探讨了大型预训练语言模型生成从因果图中的文本的能力，结果表明因果文本描述在训练数据增加时有所改善，但在零热身设置下更难生成，用户可以更快部署未来应用程序。 |
| [^49] | [SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation](https://arxiv.org/abs/2403.07088) | 提出了SPA（Side Plugin Adaption）的轻量级架构，用于在严格的设备计算和内存约束条件下快速进行推断，同时保留隐私。 |
| [^50] | [LSTM-Based Text Generation: A Study on Historical Datasets](https://arxiv.org/abs/2403.07087) | 本研究探讨了基于LSTM的文本生成在历史数据集上的应用，展示了这些模型在生成语言丰富、语境相关的文本方面的高准确性和高效率。 |
| [^51] | [AutoEval Done Right: Using Synthetic Data for Model Evaluation](https://arxiv.org/abs/2403.07008) | 提出了用合成数据进行模型评估的方法，通过高效和统计上合理的算法，在GPT-4实验中有效的人工标记样本大小增加了50%。 |
| [^52] | [Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation](https://arxiv.org/abs/2403.06988) | 提出了一种新颖的解码算法DOMINO，在生成文本过程中以完全基于子词对齐的方式强制执行约束，几乎没有性能开销并有时甚至实现近2倍速度提升，远远优于现有方法。 |
| [^53] | [MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning](https://arxiv.org/abs/2403.06914) | 提出了Meta dEmonstratioN Distillation (MEND)，利用知识蒸馏提高MEND和LLM之间的对齐，实现了高效和有效的上下文学习。 |
| [^54] | [CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean](https://arxiv.org/abs/2403.06412) | CLIcK介绍了一个包含1,995个问答对的韩国文化和语言智慧基准数据集，为填补韩语基准数据缺失的问题而来。 |
| [^55] | [An Audio-textual Diffusion Model For Converting Speech Signals Into Ultrasound Tongue Imaging Data](https://arxiv.org/abs/2403.05820) | 提出一种音频文本扩散模型，通过编码个体特征和通用模式，生成高质量超声舌头成像数据，用于语言分析和临床评估。 |
| [^56] | [GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM](https://arxiv.org/abs/2403.05527) | GEAR提出了一种高效的KV缓存压缩框架，实现几乎无损的高比率压缩，用于解决大型语言模型推断中因缓存需求增长而导致的记忆绑定问题和性能下降。 |
| [^57] | [ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues](https://arxiv.org/abs/2403.05326) | 本文提出了一个新的基于聊天的方面情绪理解（ChatASU）任务，旨在探索大型语言模型（LLMs）在对话场景中理解方面情绪的能力，并引入了一个子任务Aspect Chain Reasoning（ACR）任务来解决方面共指问题。 |
| [^58] | [Tell me the truth: A system to measure the trustworthiness of Large Language Models](https://arxiv.org/abs/2403.04964) | 本文提出了一种基于预定义领域知识图的系统化方法来衡量大型语言模型的可信度。 |
| [^59] | ["In Dialogues We Learn": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning](https://arxiv.org/abs/2403.03102) | 提出了一种In-Dialogue Learning框架，通过对话历史刻画个人设来完成个性化对话生成任务，无需预定义个人资料，并在实验证明其显著改进对话生成性能。 |
| [^60] | [Making Pre-trained Language Models Great on Tabular Prediction](https://arxiv.org/abs/2403.01841) | 提出了一种专门为表格数据预测而预训练的语言模型TP-BERTa，通过新颖的相对大小标记化方法和内部特征关注方法解决了预训练语言模型在数值特征值上的不兼容性问题 |
| [^61] | [In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation](https://arxiv.org/abs/2403.01548) | 本研究从内部表征角度深入探讨了大型语言模型幻觉的机制，发现了幻觉的一个显著模式，即在上下文标记的隐藏状态中，正确生成具有更清晰的上下文激活。我们提出了一种基于熵的度量方法，将“锐度”纳入解码过程中，制定了一种受限解码方法，实验证明其在知识寻求和幻觉任务上的有效性。 |
| [^62] | [WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset](https://arxiv.org/abs/2402.19282) | WanJuan-CC是一个安全高质量的开源英文网络文本数据集，通过处理大规模的Common Crawl数据并经过多项筛选和过滤步骤得到，为语言模型的预训练提供了重要资源。 |
| [^63] | [MMSR: Symbolic Regression is a Multimodal Task](https://arxiv.org/abs/2402.18603) | 符号回归被视为一个多模态任务，研究人员将数据到表达式的映射视为翻译问题，引入大规模预训练模型。 |
| [^64] | [Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models](https://arxiv.org/abs/2402.16315) | Finer工作揭示了大型视觉语言模型在细粒度视觉分类上的短板，尤其是难以生成准确的细致属性解释，尽管具有生成高水平图像解释的能力。 |
| [^65] | [CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples](https://arxiv.org/abs/2402.13254) | 本研究提出CounterCurate框架，通过对比例子和生成式微调，全面提升视觉-语言组合推理能力，解决了物理推理和语义对照微调方面的关键问题，实现了显著性能改进。 |
| [^66] | [Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark](https://arxiv.org/abs/2402.12243) | 研究深入分析了文本到SQL领域中的噪声对模型的影响，并发现在BIRD-Bench基准测试中存在大量问题和标准查询中的噪声，这会显著影响模型的性能。 |
| [^67] | [Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning](https://arxiv.org/abs/2402.12177) | Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。 |
| [^68] | [Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving](https://arxiv.org/abs/2402.05359) | 该论文提出了一种以分治程序引导大型语言模型（LLM）的方法，以解决涉及重复子任务和/或具有欺骗性内容的问题。实验证明，该方法可以提高LLM的表达能力。 |
| [^69] | [Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model](https://arxiv.org/abs/2402.00746) | 提出了一个创新的框架，健康-LLM，通过大规模特征提取和医学知识权衡评分，实现了个性化的检索增强疾病预测模型。这种方法通过整合健康报告，调整特征权重，以及利用语言模型和专家见解提高预测准确性，与传统健康管理方法相比具有明显优势。 |
| [^70] | [Navigating the Post-API Dilemma Search Engine Results Pages Present a Biased View of Social Media Data](https://arxiv.org/abs/2401.15479) | 搜索引擎结果页面可以作为社交媒体数据的替代方案，但存在对流行帖子偏见较高、情感更积极以及忽视政治、色情和粗俗帖子的问题。 |
| [^71] | [Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language Conversion for Language Models](https://arxiv.org/abs/2401.11725) | 提出了符号转换为语言（S2L）方法，通过将符号转换为基于语言的表示，使得大语言模型能够解决符号相关问题。 |
| [^72] | [WsiCaption: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images](https://arxiv.org/abs/2311.16480) | 研究提出了一种基于多实例生成模型的方法，能够生成千亿像素全切片图像的病理报告，实验结果表明该模型能够产生包含多个临床线索的病理报告。 |
| [^73] | [Simulating Opinion Dynamics with Networks of LLM-based Agents](https://arxiv.org/abs/2311.09618) | 提出了一种基于大型语言模型（LLMs）人口的新方法来模拟意见动态，发现LLM代理存在固有偏见导致模拟代理趋向于科学现实一致的共识，但引入确认偏见后观察到意见分裂，突显了LLM代理在该领域的潜力和局限性。 |
| [^74] | [BizBench: A Quantitative Reasoning Benchmark for Business and Finance](https://arxiv.org/abs/2311.06602) | BizBench是一个用于评估模型在推理财务问题方面能力的基准，包括八个量化推理任务，专注于通过程序合成对财务数据进行问答。 |
| [^75] | [Octavius: Mitigating Task Interference in MLLMs via MoE](https://arxiv.org/abs/2311.02684) | 提出了一个名为Octavius的新框架，通过结合MoE和LoRA技术设计了一种新颖的LLM解码器LoRA-MoE，用于多模态学习，实验证明其在各种2D和3D下游任务中具有约20%的改进效果。 |
| [^76] | [Unveiling the Pitfalls of Knowledge Editing for Large Language Models](https://arxiv.org/abs/2310.02129) | 这篇论文探讨了大型语言模型知识编辑的潜在陷阱，提出了新的评估方法，发现知识冲突和知识扭曲是两个重要问题。 |
| [^77] | [Bias and Fairness in Large Language Models: A Survey](https://arxiv.org/abs/2309.00770) | 该论文在大型语言模型领域提出了偏见评估和缓解技术的综合调查，定义了公平性的不同方面，并提出了三个分类体系，以协助研究人员对LLMs进行公平性分析和改进。 |
| [^78] | [MiniLLM: Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2306.08543) | 本文提出了一种将大型语言模型的知识蒸馏到更小模型的方法，通过使用反向KLD替换标准KD方法中的前向KLD目标，有效避免了学生模型高估教师分布的低概率区域。 |
| [^79] | [APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning](https://arxiv.org/abs/2212.07249) | APOLLO提出了一种优化的训练方法，通过数字感知的负采样策略和基于一致性的强化学习，提高了长篇数字推理框架的准确性和多样性。 |
| [^80] | [ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning](https://arxiv.org/abs/2211.04118) | 提出的Consprompt结合了提示编码网络、对比采样模块和对比评分模块，实现了差异对比学习，在不同少样本设置下表现出最先进的性能，证实了利用多级对比学习在基于提示的微调过程中的有效性。 |
| [^81] | [Misgendering and Assuming Gender in Machine Translation when Working with Low-Resource Languages.](http://arxiv.org/abs/2401.13165) | 本章论文研究了低资源语言中机器翻译中的性别相关错误，以孟加拉语为例，讨论了性别的假设和推断，以及这些错误导致的后殖民和社会影响。同时提出了提升语言地位的潜在解决方案。 |
| [^82] | [In-context Learning with Retrieved Demonstrations for Language Models: A Survey.](http://arxiv.org/abs/2401.11624) | 本综述调查了一种名为检索示范的方法，它通过使用特定于输入查询的示范来提高语言模型的少量样本情境学习（ICL）能力。这种方法不仅提高了学习效率和可扩展性，还减少了手动示例选择中的偏见。 |
| [^83] | [DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts.](http://arxiv.org/abs/2311.01070) | 本文提出了DistilWhisper方法，通过使用语言特定专家进行轻量级模块化ASR微调和知识蒸馏，成功弥合了多任务语音模型在少数语言上的性能差距，同时保留了多任务和多语言能力的优势。 |
| [^84] | [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models.](http://arxiv.org/abs/2310.06117) | 本文提出了一种简单的提示技术，使得大型语言模型能够通过抽象获得高层概念和基本原理，并将其应用于推理路径中，从而显著提升模型在各种推理密集型任务上的表现。 |
| [^85] | [TRAM: Bridging Trust Regions and Sharpness Aware Minimization.](http://arxiv.org/abs/2310.03646) | TRAM是一种桥接信任区域和锐度感知最小化的算法，通过减少损失曲面的曲率来提供鲁棒性改进。它通过在微调过程中优化可转移的表示来实现领域外泛化，并且通过结合信任区域方法和SAM风格的正则化器来统一参数和表示空间平滑方法。TRAM在保持预训练结构的同时实现了平坦的极小值和平滑、有信息量的表示。 |
| [^86] | [Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness.](http://arxiv.org/abs/2310.02832) | 本文提出了一种通过利用神经网络中间层变换的平滑性来检测带外数据的方法(BLOOD),该方法适用于没有训练数据访问权限的预训练模型，并在Transformer网络上的文本分类任务中取得了良好的效果。 |
| [^87] | [Augmenting transformers with recursively composed multi-grained representations.](http://arxiv.org/abs/2309.16319) | ReCAT是一种增强的Transformer模型，使用递归组合和上下文内外层能够模拟文本的层级句法结构，并生成与其他跨度上下文相关的多粒度表示。 |
| [^88] | [InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework.](http://arxiv.org/abs/2309.11911) | InstructERC是一种使用大型语言模型(LLMs)的生成式框架，通过引入检索模板模块和额外的情感对齐任务，改革了对话中的情绪识别。 |
| [^89] | [MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback.](http://arxiv.org/abs/2309.10691) | MINT是一个评估LLMs在多轮交互中解决任务能力的基准，通过使用工具和利用用户的自然语言反馈。它解决了当前评估协议忽略细致互动和低估自然语言反馈的问题，促进了研究基准评估和实际应用之间的一致性。 |
| [^90] | [A Survey on Large Language Model based Autonomous Agents.](http://arxiv.org/abs/2308.11432) | 该论文综述了基于大型语言模型的自主代理的研究，提供了从整体角度对该领域的系统审查，其创新之处在于利用大量网络知识实现人类水平的智能决策。 |
| [^91] | [Self-Alignment with Instruction Backtranslation.](http://arxiv.org/abs/2308.06259) | 本论文提出了一种自动对齐方法，通过为人工编写的文本添加指令标签来构建高质量的指令跟踪语言模型。该方法通过自我增强和自我筛选生成训练示例，并且在Alpaca排行榜上表现出非常高效的自动对齐能力。 |
| [^92] | [Overthinking the Truth: Understanding how Language Models Process False Demonstrations.](http://arxiv.org/abs/2307.09476) | 该论文研究了现代语言模型在处理虚假演示时出现的过度思考和错误归纳头现象。通过研究模型的内部表示，发现模型在中间层之后对错误演示的处理准确性逐渐降低，并指出了错误归纳头机制可能导致过度思考现象。 |
| [^93] | [ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models.](http://arxiv.org/abs/2303.16421) | ChatGPT是一个知识渊博但经验不足的LLM，能够回答常识问题，但在某些类型问题上仍存在困难。 |
| [^94] | [Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference.](http://arxiv.org/abs/2302.09582) | 本研究通过操纵大型语言模型中的语言衍生的情绪概念知识表示，探讨了语言是否会因果支持情绪推断。实验结果显示，属性特定的神经元操纵导致情绪推断任务的性能下降，这与人类心理空间中不同属性的重要性有关。这些发现为支持基于语言的情绪推断机制提供了因果证据，并凸显了情绪概念知识的贡献。 |

# 详细

[^1]: 对大型生成语言模型的语义理解重新评估

    Rethinking Generative Large Language Model Evaluation for Semantic Comprehension

    [https://arxiv.org/abs/2403.07872](https://arxiv.org/abs/2403.07872)

    通过引入RWQ-Elo评级系统，在新基准“真实世界问题”上对24种大型语言模型进行两人竞争评估，以解决多选问题回答方法在语义理解评估中的潜在缺陷。

    

    尽管大型语言模型（LLMs）具有复杂的功能，但在有效评估方面仍面临重大障碍。本文首先重新审视主流的评估方法 - 多项选择问题回答（MCQA），这种方法允许进行直观的准确性评估。通过对11个基准测试上的24个模型进行全面评估，我们突出显示了MCQA的多个潜在缺点，例如MCQA评估与实际情景中开放式回答的生成之间的不一致性。为此，我们引入了一种RWQ-Elo评级系统，利用GPT-4、GPT-3.5、Google-Gemini-Pro和LLaMA-1/-2等24个LLMs，采用二人竞争格式，其中GPT-4作为裁判。每个LLM随后收到一个Elo评级。该系统旨在模拟真实世界的使用情况，出于此目的，我们编制了一个名为“真实世界问题”（RWQ）的新基准，包含20,772个真实用户查询。

    arXiv:2403.07872v1 Announce Type: new  Abstract: Despite their sophisticated capabilities, large language models (LLMs) encounter a major hurdle in effective assessment. This paper first revisits the prevalent evaluation method-multiple choice question answering (MCQA), which allows for straightforward accuracy measurement. Through a comprehensive evaluation of 24 models across 11 benchmarks, we highlight several potential drawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation and the generation of open-ended responses in practical scenarios. In response, we introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5, Google-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with GPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This system is designed to mirror real-world usage, and for this purpose, we have compiled a new benchmark called ``Real-world questions'' (RWQ), comprising 20,772 authentic user inquirie
    
[^2]: 通过代码探索大型语言模型的安全泛化挑战

    Exploring Safety Generalization Challenges of Large Language Models via Code

    [https://arxiv.org/abs/2403.07865](https://arxiv.org/abs/2403.07865)

    本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。

    

    大型语言模型（LLMs）的快速发展带来了自然语言处理方面的显著能力，但也引发了人们对它们潜在误用的担忧。本文引入了CodeAttack，一个将自然语言输入转换为代码输入的框架，为测试LLMs的安全泛化提供了一个新颖的环境。我们对包括GPT-4、Claude-2和Llama-2系列在内的最新LLMs进行了全面研究，发现这些模型对于代码输入存在共同的安全漏洞：CodeAttack在超过80%的时间内始终绕过所有模型的安全保护。

    arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
    
[^3]: 模型编辑中的遗漏之处：深入探讨模型编辑带来的隐藏损害

    The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing

    [https://arxiv.org/abs/2403.07825](https://arxiv.org/abs/2403.07825)

    本文提出了一种新的评估方法 GORA 和一种模型编辑方法 SORA，用以解决模型编辑中的隐藏空间中的涟漪效应问题。

    

    大型语言模型以其卓越的效果彻底改变了许多任务。然而，对这些模型进行编辑，以修改过时或错误信息的关键性工作，往往会导致一个称为“隐藏空间中的涟漪效应”的复杂问题。这种效应虽然难以检测，但却会显著阻碍模型编辑任务的效果，并恶化模型性能。本文通过提出一种新颖的评估方法，基于图形特异值关系的评估(GORA)，来应对这一科学挑战，量化评估模型的适应性和编辑的后续影响。此外，我们引入了一种旨在减轻这种涟漪效应的模型编辑方法——选择性异常值重新编辑方法(SORA)。我们的全面评估揭示了隐藏空间中的涟漪效应在所有当前模型编辑方法中都是一个重要问题。然而，我们提出的方法，G

    arXiv:2403.07825v1 Announce Type: new  Abstract: Large Language Models have revolutionized numerous tasks with their remarkable efficacy.However, the editing of these models, crucial for rectifying outdated or erroneous information, often leads to a complex issue known as the ripple effect in the hidden space. This effect, while difficult to detect, can significantly impede the efficacy of model editing tasks and deteriorate model performance.This paper addresses this scientific challenge by proposing a novel evaluation methodology, Graphical Outlier Relation based Assessment(GORA), which quantitatively evaluates the adaptations of the model and the subsequent impact of editing. Furthermore, we introduce the Selective Outlier Re-Editing Approach(SORA), a model editing method designed to mitigate this ripple effect. Our comprehensive evaluations reveal that the ripple effect in the hidden space is a significant issue in all current model editing methods. However, our proposed methods, G
    
[^4]: Branch-Train-MiX: 将专家LLMs混合到混合专家LLM中

    Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM

    [https://arxiv.org/abs/2403.07816](https://arxiv.org/abs/2403.07816)

    BTX方法提供了一种高效的训练大型语言模型以具备多个专业领域能力的方法，通过将种子模型的分支培训成专家，然后在Mixture-of-Expert层中将它们汇集为专家，并利用MoE微调阶段学习基于标记的路由，从而实现了最佳的精度和效率权衡。

    

    我们研究了训练大型语言模型（LLMs）以具备多个专业领域能力的高效方法，例如编码、数学推理和世界知识。我们的方法名为Branch-Train-MiX（BTX），从一个种子模型开始，将其分支训练成专家，以高吞吐量和降低通信成本的尴尬并行方式。在各个专家异步训练后，BTX将它们作为专家在Mixture-of-Expert（MoE）层中汇集其前馈参数，并平均其他参数，随后是一个MoE微调阶段来学习基于标记的路由。BTX概括了两个特殊情况，即Branch-Train-Merge方法，它没有MoE微调阶段来学习路由，以及稀疏升级，它省略了异步训练专家的阶段。与其他方法相比，BTX实现了最佳精度和效率的权衡。

    arXiv:2403.07816v1 Announce Type: cross  Abstract: We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff.
    
[^5]: pyvene: 通过干预来理解和改进PyTorch模型的库

    pyvene: A Library for Understanding and Improving PyTorch Models via Interventions

    [https://arxiv.org/abs/2403.07809](https://arxiv.org/abs/2403.07809)

    pyvene是一个开源Python库，支持在PyTorch模型上进行可定制的干预，提供统一和可扩展的框架，用于解释神经模型并与他人分享经过干预的模型。

    

    模型内部状态的干预是人工智能许多领域的基本操作，包括模型编辑、控制、稳健性和可解释性。为了促进这方面的研究，我们介绍了pyvene，一个开源的Python库，支持在各种不同PyTorch模块上进行可定制的干预。Pyvene支持复杂的干预方案，具有直观的配置格式，并且其干预可以是静态的或包含可训练参数。我们展示了pyvene如何提供一个统一和可扩展的框架，用于对神经模型进行干预并与他人共享经过干预的模型。我们通过因果抽象和知识定位的可解释性分析展示了该库的能力。我们通过Python Package Index（PyPI）发布了我们的库，并在https://github.com/stanfordnlp/pyvene 提供了代码、文档和教程。

    arXiv:2403.07809v1 Announce Type: cross  Abstract: Interventions on model-internal states are fundamental operations in many areas of AI, including model editing, steering, robustness, and interpretability. To facilitate such research, we introduce $\textbf{pyvene}$, an open-source Python library that supports customizable interventions on a range of different PyTorch modules. $\textbf{pyvene}$ supports complex intervention schemes with an intuitive configuration format, and its interventions can be static or include trainable parameters. We show how $\textbf{pyvene}$ provides a unified and extensible framework for performing interventions on neural models and sharing the intervened upon models with others. We illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization. We publish our library through Python Package Index (PyPI) and provide code, documentation, and tutorials at https://github.com/stanfordnlp/pyvene.
    
[^6]: 超越死记硬背：语言模型中的随机内存访问挑战

    Beyond Memorization: The Challenge of Random Memory Access in Language Models

    [https://arxiv.org/abs/2403.07805](https://arxiv.org/abs/2403.07805)

    本文研究了语言模型在访问内存时的挑战，发现通过背诵和置换等技术可以改善语言模型的随机内存访问能力，从而在开放域问题回答任务中取得显著改进。

    

    最近语言模型(LMs)的发展展示了它们在NLP任务中的有效性，尤其是在知识密集型任务中。然而，在其参数内部的知识存储和内存访问机制仍然令人费解。本文探讨了生成式语言模型（如GPT-2）是否能够顺序或随机地访问其内存。通过精心设计的合成任务，涵盖全面背诵、选择性背诵和基于问题回答的情景，我们揭示了LMs能够顺序访问其内存，同时在随机访问已记忆内容时遇到挑战。我们发现，通过背诵和置换等技术可以提高LMs的随机内存访问能力。此外，通过将这种干预应用于开放域问题回答的现实场景，我们验证了通过背诵来增强随机访问技术对问题回答能力的显著改进。

    arXiv:2403.07805v1 Announce Type: cross  Abstract: Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks. However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive. In this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs. Furthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in questi
    
[^7]: 使用顺序指令对大型语言模型进行微调

    Fine-tuning Large Language Models with Sequential Instructions

    [https://arxiv.org/abs/2403.07794](https://arxiv.org/abs/2403.07794)

    通过顺序指令微调，研究提出了一种简单且有效的策略，可以使大型语言模型具备执行多个顺序指令的能力，优于传统指令微调模型。

    

    大型语言模型（LLMs）在单个查询中遵循一系列指令时往往会忽略或误解其中的一部分，这影响了它们在解决需要多个中间步骤的复杂问题中的性能，例如多语言（先翻译再回答）和多模态（标题后回答）任务。我们通过开源LLMs（如LLaMA-2 70B和Mixtral-8x7B）的实证验证了这一点。针对当前数据中顺序指令稀缺的问题，我们提出了顺序指令微调，这是一种简单而有效的策略，可以自动增加指令调整数据，使LLMs具备执行多个顺序指令的能力。在探索现有数据集（如Alpaca）中插入指令并进行一系列中间任务后，我们发现，顺序指令微调的模型在下游任务中始终优于传统的指令微调基线。

    arXiv:2403.07794v1 Announce Type: new  Abstract: Large language models (LLMs) struggle to follow a sequence of instructions in a single query as they may ignore or misinterpret part of it. This impairs their performance in complex problems whose solution requires multiple intermediate steps, such as multilingual (translate then answer) and multimodal (caption then answer) tasks. We empirically verify this with open-source LLMs as large as LLaMA-2 70B and Mixtral-8x7B. Targeting the scarcity of sequential instructions in present-day data, we propose sequential instruction tuning, a simple yet effective strategy to automatically augment instruction tuning data and equip LLMs with the ability to execute multiple sequential instructions. After exploring interleaving instructions in existing datasets, such as Alpaca, with a wide range of intermediate tasks, we find that sequential instruction-tuned models consistently outperform the conventional instruction-tuned baselines in downstream tas
    
[^8]: 将竞争转化为合作：多Agent系统和语言模型在现代组织中的革命性作用

    Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations

    [https://arxiv.org/abs/2403.07769](https://arxiv.org/abs/2403.07769)

    文章探讨了基于多Agent系统理论结合大型语言模型的计算实体对人类互动的革新影响，提出了一种可能将专门人工代理支持扩展到操作性组织流程和基于知识和人类编排的战略决策的方式。

    

    这篇文章探讨了基于多Agent系统理论（SMA）结合大型语言模型（LLM）的计算实体的动态影响，其特点是能够模拟复杂的人类互动，作为一种革新人类用户交互的可能性，从利用专门的人工代理支持从操作组织流程到基于应用知识和人的编排的战略决策。 先前的调查显示，在处理新挑战和实用任务（如引发逻辑推理和问题解决）时，特别是在人工代理的自主方法方面存在限制。 还考虑到，传统技术，如激发思想链，需要明确的人类指导。 在我们的方法中，我们使用从大型语言模型（LLM）开发的代理，每个代理都有不同

    arXiv:2403.07769v1 Announce Type: new  Abstract: This article explores the dynamic influence of computational entities based on multi-agent systems theory (SMA) combined with large language models (LLM), which are characterized by their ability to simulate complex human interactions, as a possibility to revolutionize human user interaction from the use of specialized artificial agents to support everything from operational organizational processes to strategic decision making based on applied knowledge and human orchestration. Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving. It is also considered that traditional techniques, such as the stimulation of chains of thoughts, require explicit human guidance. In our approach we employ agents developed from large language models (LLM), each with distinct
    
[^9]: FineMath：一种用于评估中文大型语言模型的细粒度数学评估基准

    FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models

    [https://arxiv.org/abs/2403.07747](https://arxiv.org/abs/2403.07747)

    FineMath是一个用于评估中文大型语言模型数学推理能力的细粒度数学评估基准数据集，涵盖小学数学中的主要概念，划分为17类数学问题，并手动注释难度级别，实验证实在数学方面仍有改进空间。

    

    为了全面评估大型语言模型（LLMs）的数学推理能力，我们需要精心策划涵盖不同难度级别的各种数学概念和数学问题的评估数据集。为了实现这一目标，我们在本文中提出了FineMath，这是一个用于评估中文LLMs的细粒度数学评估基准数据集。FineMath旨在涵盖小学数学中教授的主要数学概念，进一步划分为17类数学问题，从而深入分析LLMs的数学推理能力。所有17类数学问题均根据解决这些问题所需的推理步骤数量进行手动注释其难度级别。我们在FineMath上对各种LLMs进行了广泛实验，并发现在数学方面仍有相当大的改进空间。

    arXiv:2403.07747v1 Announce Type: cross  Abstract: To thoroughly assess the mathematical reasoning abilities of Large Language Models (LLMs), we need to carefully curate evaluation datasets covering diverse mathematical concepts and mathematical problems at different difficulty levels. In pursuit of this objective, we propose FineMath in this paper, a fine-grained mathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath is created to cover the major key mathematical concepts taught in elementary school math, which are further divided into 17 categories of math word problems, enabling in-depth analysis of mathematical reasoning abilities of LLMs. All the 17 categories of math word problems are manually annotated with their difficulty levels according to the number of reasoning steps required to solve these problems. We conduct extensive experiments on a wide range of LLMs on FineMath and find that there is still considerable room for improvements in terms of mathem
    
[^10]: SemEval-2024共享任务6: SHROOM，一个关于幻觉及相关可观察过度生成错误的共享任务

    SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes

    [https://arxiv.org/abs/2403.07726](https://arxiv.org/abs/2403.07726)

    本文介绍了SHROOM共享任务，重点关注检测幻觉，以及参与者使用的模型和策略。

    

    本文介绍了SHROOM的结果，这是一个专注于检测幻觉的共享任务：即自然语言生成（NLG）系统的输出流畅但不准确。这种过度生成的情况可能危及许多NLG应用，其中正确性往往至关重要。共享任务使用了一个新构建的数据集，包含4000个由5个标注者标记的模型输出，涵盖了3个NLP任务：机器翻译、释义生成和定义建模。 共享任务由58个不同用户组成的42支团队共同解决，其中27支选择撰写系统描述论文；他们共提交了超过300个预测集在共享任务的两个跟踪上。我们观察到这种方法如何被处理的一些关键趋势--许多参与者依赖少数模型，并经常依赖合成数据进行微调或零样本提示策略。

    arXiv:2403.07726v1 Announce Type: new  Abstract: This paper presents the results of the SHROOM, a shared task focused on detecting hallucinations: outputs from natural language generation (NLG) systems that are fluent, yet inaccurate. Such cases of overgeneration put in jeopardy many NLG applications, where correctness is often mission-critical. The shared task was conducted with a newly constructed dataset of 4000 model outputs labeled by 5 annotators each, spanning 3 NLP tasks: machine translation, paraphrase generation and definition modeling.   The shared task was tackled by a total of 58 different users grouped in 42 teams, out of which 27 elected to write a system description paper; collectively, they submitted over 300 prediction sets on both tracks of the shared task. We observe a number of key trends in how this approach was tackled -- many participants rely on a handful of model, and often rely either on synthetic data for fine-tuning or zero-shot prompting strategies. While 
    
[^11]: StableToolBench：面向大规模稳定基准测试的工具学习大语言模型

    StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models

    [https://arxiv.org/abs/2403.07714](https://arxiv.org/abs/2403.07714)

    StableToolBench提出了一种虚拟API服务器和稳定评估系统，通过缓存系统、API模拟器和稳定评估设计，解决了大语言模型利用外部工具的稳定大规模基准测试问题。

    

    大语言模型（LLMs）近年来取得了显著进展，促使人们探索工具学习，将LLMs与外部工具整合以解决各种现实挑战。评估LLMs利用工具的能力需要大规模且稳定的基准测试。我们介绍了由ToolBench演变而来的StableToolBench，提出了一个虚拟API服务器和稳定评估系统。虚拟API服务器包含缓存系统和API模拟器，互补减轻API状态变化。同时，稳定的评估系统使用GPT-4作为自动评估器设计可解决的通过率和胜率，以消除评估过程中的随机性。实验结果证明

    arXiv:2403.07714v1 Announce Type: new  Abstract: Large Language Models (LLMs) have witnessed remarkable advancements in recent years, prompting the exploration of tool learning, which integrates LLMs with external tools to address diverse real-world challenges. Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status. To address this problem, we introduce StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status. Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation. Experimental results demonstrate 
    
[^12]: 使用对比奖励改善从人类反馈中的强化学习

    Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards

    [https://arxiv.org/abs/2403.07708](https://arxiv.org/abs/2403.07708)

    引入对比奖励的方法提高了从人类反馈中的强化学习的效果，改善了奖励模型的鲁棒性，鼓励对基准的改善，并能根据任务的难度进行校准。

    

    强化学习从人类反馈（RLHF）是用来对齐大型语言模型（LLMs）与人类偏好的主流范式。然而现有的RLHF在很大程度上依赖于准确和信息丰富的奖励模型，这些模型容易受到各种来源的噪声，例如人类标注错误，使得流程脆弱。本文通过引入对奖励的惩罚项，命名为“对比奖励”，来提高奖励模型的有效性。我们的方法包括两个步骤：（1）离线抽样步骤，获取用作基准计算的提示响应，以及（2）使用基准响应计算对比奖励，并将其用于Proximal Policy Optimization（PPO）步骤。我们展示了对比奖励使得LLM能够惩罚奖励不确定性，提高鲁棒性，鼓励优于基线的改进，根据任务难度进行校准，并且重新

    arXiv:2403.07708v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) is the mainstream paradigm used to align large language models (LLMs) with human preferences. Yet existing RLHF heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile. In this work, we improve the effectiveness of the reward model by introducing a penalty term on the reward, named as \textit{contrastive rewards}. %Contrastive rewards Our approach involves two steps: (1) an offline sampling step to obtain responses to prompts that serve as baseline calculation and (2) a contrastive reward calculated using the baseline responses and used in the Proximal Policy Optimization (PPO) step. We show that contrastive rewards enable the LLM to penalize reward uncertainty, improve robustness, encourage improvement over baselines, calibrate according to task difficulty, and re
    
[^13]: 基于语言模型的新型数据增强框架用于去偏见观点摘要

    Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization

    [https://arxiv.org/abs/2403.07693](https://arxiv.org/abs/2403.07693)

    使用大型和小型语言模型的新型数据增强框架，通过重新生成评论来实现观点摘要的去偏见化，避免了大型语言模型数据增强可能存在的问题和高昂成本。

    

    现有观点摘要数据集中超过70％的评论是积极的，当前的观点摘要方法在给定负面文本的情况下不愿生成负面摘要，造成情感偏见。为了解决这种情感偏见，一个直接的方法是基于大型语言模型生成额外的数据，平衡数据集的情感分布，而不过分依赖特定框架。然而，基于大型语言模型的数据增强面临两个缺点：1）增强数据中的潜在问题或毒性；2）昂贵的成本。因此，在本文中，我们提出了一个基于大型和小型语言模型的新型数据增强框架，用于去偏见观点摘要。具体而言，通过大型语言模型重写正面文本获得了小规模合成的负面评论。然后，基于生成的内容训练一个解耦重构模型。

    arXiv:2403.07693v1 Announce Type: cross  Abstract: As more than 70$\%$ of reviews in the existing opinion summary data set are positive, current opinion summarization approaches are reluctant to generate negative summaries given the input of negative texts. To address such sentiment bias, a direct approach without the over-reliance on a specific framework is to generate additional data based on large language models to balance the emotional distribution of the dataset. However, data augmentation based on large language models faces two disadvantages: 1) the potential issues or toxicity in the augmented data; 2) the expensive costs. Therefore, in this paper, we propose a novel data augmentation framework based on both large and small language models for debiasing opinion summarization. In specific, a small size of synthesized negative reviews is obtained by rewriting the positive text via a large language model. Then, a disentangle reconstruction model is trained based on the generated 
    
[^14]: 具有赔率比的无参考单体偏好优化

    Reference-free Monolithic Preference Optimization with Odds Ratio

    [https://arxiv.org/abs/2403.07691](https://arxiv.org/abs/2403.07691)

    本文介绍了一种无参考单体赔率比偏好优化算法ORPO，在SFT过程中通过轻微惩罚不受欢迎的生成风格，消除了额外的偏好对齐阶段

    

    近期的语言模型偏好对齐算法展现了很好的结果，但是监督微调（SFT）仍然对于成功收敛至关重要。本文研究了在偏好对齐的环境中SFT的关键作用，强调对于偏好对齐的SFT来说，对于不受欢迎的生成风格施加轻微惩罚就足够了。在此基础上，我们引入了一种简单而创新的无参考模型的单体赔率比偏好优化算法ORPO，消除了额外的偏好对齐阶段的必要性。我们通过实证和理论手段证明，赔率比是在125M至7B不同规模下进行SFT时对比受欢迎和不受欢迎风格的明智选择。具体来说，使用ORPO在仅UltraFeedback上对Phi-2（2.7B）、Llama-2（7B）和Mistral（7B）进行微调，超越了性能

    arXiv:2403.07691v1 Announce Type: cross  Abstract: While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance o
    
[^15]: SATDAUG -- 一种用于检测自认技术债的平衡和增强数据集

    SATDAUG -- A Balanced and Augmented Dataset for Detecting Self-Admitted Technical Debt

    [https://arxiv.org/abs/2403.07690](https://arxiv.org/abs/2403.07690)

    SATDAUG这一平衡和增强的数据集旨在解决自认技术债识别和分类中现有数据集存在的类别不平衡问题。

    

    自认技术债(SATD)是指开发人员明确承认并记录代码库中存在的技术捷径、变通方法或临时解决方案的一种形式。近年来，研究人员对各种软件开发工件进行手动标记，包括源代码注释、问题跟踪器和拉取请求部分的消息以及提交消息。这些数据集旨在用于训练、评估、性能验证和改进机器学习和深度学习模型，以准确识别SATD实例。然而，现有数据集存在严重的类别不平衡问题，特别是当研究人员有兴趣对SATD的特定类型进行分类时。为解决SATD识别（即一个实例是否为SATD）和分类（即区分不同类型的SATD）的标记数据稀缺问题，本文提出了一种新的平衡和增强的SATD数据集（SATDAUG）。

    arXiv:2403.07690v1 Announce Type: cross  Abstract: Self-admitted technical debt (SATD) refers to a form of technical debt in which developers explicitly acknowledge and document the existence of technical shortcuts, workarounds, or temporary solutions within the codebase. Over recent years, researchers have manually labeled datasets derived from various software development artifacts: source code comments, messages from the issue tracker and pull request sections, and commit messages. These datasets are designed for training, evaluation, performance validation, and improvement of machine learning and deep learning models to accurately identify SATD instances. However, class imbalance poses a serious challenge across all the existing datasets, particularly when researchers are interested in categorizing the specific types of SATD. In order to address the scarcity of labeled data for SATD \textit{identification} (i.e., whether an instance is SATD or not) and \textit{categorization} (i.e.
    
[^16]: 低成本注释：利用地理数据相似性平衡模型性能和注释成本

    Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost

    [https://arxiv.org/abs/2403.07687](https://arxiv.org/abs/2403.07687)

    提出一种方法来识别数据，以平衡模型性能和注释成本

    

    当前的基础模型在各种任务中表现出色。然而，一些研究表明，由于训练过程中使用的数据在地理和经济上的不平衡表示，这些模型并不对每个人都有效。大多数数据来自西方国家，导致对代表性不足的国家的结果不佳。为了解决这个问题，需要从这些国家收集更多数据，但注释成本可能是一个重大瓶颈。在本文中，我们提出了一种方法来识别需要注释的数据，以平衡模型性能和注释成本。我们的方法首先涉及找到具有最大视觉差异的主题（物体和动作）图像的国家。接下来，我们确定了对于这些主题在视觉上更相似的国家，并表明利用这些国家可以提高模型性能，同时在注释成本方面节省开销。

    arXiv:2403.07687v1 Announce Type: cross  Abstract: Current foundation models have shown impressive performance across various tasks. However, several studies have revealed that these models are not effective for everyone due to the imbalanced geographical and economic representation of the data used in the training process. Most of this data comes from Western countries, leading to poor results for underrepresented countries. To address this issue, more data needs to be collected from these countries, but the cost of annotation can be a significant bottleneck. In this paper, we propose methods to identify the data to be annotated to balance model performance and annotation costs. Our approach first involves finding the countries with images of topics (objects and actions) most visually distinct from those already in the training datasets used by current large vision-language foundation models. Next, we identify countries with higher visual similarity for these topics and show that usin
    
[^17]: MoralBERT：检测社会话语中的道德价值

    MoralBERT: Detecting Moral Values in Social Discourse

    [https://arxiv.org/abs/2403.07678](https://arxiv.org/abs/2403.07678)

    MoralBERT 是一种专门设计用于捕捉文本中道德微妙之处的语言表示模型，利用来自Twitter、Reddit和Facebook的数据，扩大了模型理解道德的能力。

    

    道德在我们感知信息、影响决策和判断过程中起着基础性作用。包括疫苗接种、堕胎、种族主义和性取向在内的有争议话题往往引发的意见和态度并非仅基于证据，而更多反映了道德世界观。最近自然语言处理的进展表明，道德价值可以从人类生成的文本内容中得到判断。本文设计了一系列旨在捕捉文本中道德微妙之处的语言表示模型，称为MoralBERT。我们利用来自三个不同来源（Twitter、Reddit和Facebook）的带有注释的道德数据，涵盖各种社会相关主题。这种方法扩大了语言多样性，可能增强模型在不同上下文中理解道德的能力。我们还探讨了一种领域自适应技术，并将其与标准的微调方法进行了比较。

    arXiv:2403.07678v1 Announce Type: new  Abstract: Morality plays a fundamental role in how we perceive information while greatly influencing our decisions and judgements. Controversial topics, including vaccination, abortion, racism, and sexuality, often elicit opinions and attitudes that are not solely based on evidence but rather reflect moral worldviews. Recent advances in natural language processing have demonstrated that moral values can be gauged in human-generated textual content. Here, we design a range of language representation models fine-tuned to capture exactly the moral nuances in text, called MoralBERT. We leverage annotated moral data from three distinct sources: Twitter, Reddit, and Facebook user-generated content covering various socially relevant topics. This approach broadens linguistic diversity and potentially enhances the models' ability to comprehend morality in various contexts. We also explore a domain adaptation technique and compare it to the standard fine-tu
    
[^18]: 较困难的任务需要更多专家：MoE模型中的动态路由

    Harder Tasks Need More Experts: Dynamic Routing in MoE Models

    [https://arxiv.org/abs/2403.07652](https://arxiv.org/abs/2403.07652)

    通过动态选择专家来提高计算效率和模型性能，针对不同难度的任务激活不同数量的专家，相比传统的Top-K路由方法，我们的动态路由方法在各种基准测试中取得了明显的改进。

    

    在本文中，我们引入了一种新颖的动态专家选择框架，用于Mixture of Experts（MoE）模型，旨在通过根据输入难度调整激活的专家数量，增强计算效率和模型性能。与依赖于固定Top-K路由的传统MoE方法不同，该方法根据对每个输入的专家选择的置信水平动态选择专家。这允许更有效地利用计算资源，对需要高级推理的复杂任务激活更多的专家，对较简单的任务激活更少的专家。通过广泛的评估，我们的动态路由方法在各种基准测试中表现出明显的改进，与常规Top-2路由相比，实现了平均改进0.7%的效果，且激活参数少于90%。进一步的分析显示我们的模型

    arXiv:2403.07652v1 Announce Type: cross  Abstract: In this paper, we introduce a novel dynamic expert selection framework for Mixture of Experts (MoE) models, aiming to enhance computational efficiency and model performance by adjusting the number of activated experts based on input difficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing, which activates a predetermined number of experts regardless of the input's complexity, our method dynamically selects experts based on the confidence level in expert selection for each input. This allows for a more efficient utilization of computational resources, activating more experts for complex tasks requiring advanced reasoning and fewer for simpler tasks. Through extensive evaluations, our dynamic routing method demonstrates substantial improvements over conventional Top-2 routing across various benchmarks, achieving an average improvement of 0.7% with less than 90% activated parameters. Further analysis shows our model 
    
[^19]: LLMvsSmall 模型？基于大型语言模型的文本增强增强个性检测模型

    LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model

    [https://arxiv.org/abs/2403.07581](https://arxiv.org/abs/2403.07581)

    提出了一种基于大型语言模型的文本增强增强个性检测模型，通过提炼大型语言模型的知识，增强小模型的性能，即使大型语言模型在任务中失败。

    

    人格检测旨在检测社交媒体帖子中隐藏的人格特质。这一任务的挑战之一是缺乏从自我报告问卷中收集的地面真实人格特质。大多数现有方法通过在有限人格标签的监督下微调预训练语言模型，直接学习帖子特征。这导致帖子特征质量较差，从而影响性能。此外，它们将人格特质视为独热分类标签，忽视其中的语义信息。本文提出了一种基于大型语言模型（LLM）的文本增强增强个性检测模型，该模型提炼LLM的知识以增强用于个性检测的小模型，即使LLM在该任务中失败。具体来说，我们使LLM能够从语义、情感方面生成帖子分析（增强）。

    arXiv:2403.07581v1 Announce Type: new  Abstract: Personality detection aims to detect one's personality traits underlying in social media posts. One challenge of this task is the scarcity of ground-truth personality traits which are collected from self-report questionnaires. Most existing methods learn post features directly by fine-tuning the pre-trained language models under the supervision of limited personality labels. This leads to inferior quality of post features and consequently affects the performance. In addition, they treat personality traits as one-hot classification labels, overlooking the semantic information within them. In this paper, we propose a large language model (LLM) based text augmentation enhanced personality detection model, which distills the LLM's knowledge to enhance the small model for personality detection, even when the LLM fails in this task. Specifically, we enable LLM to generate post analyses (augmentations) from the aspects of semantic, sentiment, a
    
[^20]: Triples-to-isiXhosa (T2X): 应对低资源聚集性数据生成文本的挑战

    Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource Agglutinative Data-to-Text Generation

    [https://arxiv.org/abs/2403.07567](https://arxiv.org/abs/2403.07567)

    本文针对低资源聚集性语言isiXhosa进行数据到文本处理，引入了新的数据集T2X和SSPG架构，开发了评估框架，帮助用户更准确描述数据。

    

    大多数数据到文本数据集是英文的，所以建模低资源语言的数据到文本的困难在很大程度上尚未被探索。本文解决了isiXhosa语言的数据到文本问题，该语言属于低资源语言且为聚集性语言。我们介绍了基于WebNLG子集的新数据集Triples-to-isiXhosa (T2X)，它提出了一种新的语言环境，将建模需求转向子词驱动技术。我们还为T2X开发了一个评估框架，衡量生成的文本描述数据的准确性。这使得T2X的未来用户能够超越表面级别的评估指标。在建模方面，我们探讨了两类方法 - 从头开始训练的专用数据到文本模型和预训练语言模型（PLMs）。我们提出了一个新的专门面向聚集性数据到文本的架构，即Subword Segmental Pointer Generator (SSPG)。它共同学习分割单词和复制实体，并在...

    arXiv:2403.07567v1 Announce Type: new  Abstract: Most data-to-text datasets are for English, so the difficulties of modelling data-to-text for low-resource languages are largely unexplored. In this paper we tackle data-to-text for isiXhosa, which is low-resource and agglutinative. We introduce Triples-to-isiXhosa (T2X), a new dataset based on a subset of WebNLG, which presents a new linguistic context that shifts modelling demands to subword-driven techniques. We also develop an evaluation framework for T2X that measures how accurately generated text describes the data. This enables future users of T2X to go beyond surface-level metrics in evaluation. On the modelling side we explore two classes of methods - dedicated data-to-text models trained from scratch and pretrained language models (PLMs). We propose a new dedicated architecture aimed at agglutinative data-to-text, the Subword Segmental Pointer Generator (SSPG). It jointly learns to segment words and copy entities, and outperfor
    
[^21]: SIFiD：使用LLM重新评估摘要的事实不一致性检测

    SIFiD: Reassess Summary Factual Inconsistency Detection with LLM

    [https://arxiv.org/abs/2403.07557](https://arxiv.org/abs/2403.07557)

    本研究重新评估了使用LLM进行摘要不一致性检测的方法，提出了SIFiD（带有过滤文档的摘要不一致性检测），旨在通过自然语言推理或测量语义相似性来识别文档中的关键句子。

    

    确保摘要与原始文档之间的事实一致性在摘要任务中至关重要。因此，人们致力于检测不一致性。随着大型语言模型（LLMs）的出现，最近的研究开始利用它们先进的语言理解能力进行不一致性检测。然而，早期尝试表明，由于LLMs有限的遵循指令能力和缺乏有效的检测方法论，它们的性能不及传统模型。在这项研究中，我们使用GPT-3.5和GPT-4比较LLMs的摘要不一致性检测表现，以推动基于LLM的不一致性检测研究。我们提出了SIFiD（带有过滤文档的摘要不一致性检测），它通过使用自然语言推理或测量摘要和文档之间的语义相似性来识别文档中的关键句子。

    arXiv:2403.07557v1 Announce Type: new  Abstract: Ensuring factual consistency between the summary and the original document is paramount in summarization tasks. Consequently, considerable effort has been dedicated to detecting inconsistencies. With the advent of Large Language Models (LLMs), recent studies have begun to leverage their advanced language understanding capabilities for inconsistency detection. However, early attempts have shown that LLMs underperform traditional models due to their limited ability to follow instructions and the absence of an effective detection methodology. In this study, we reassess summary inconsistency detection with LLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in LLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency Detection with Filtered Document) that identify key sentences within documents by either employing natural language inference or measuring semantic similarity between summaries and documen
    
[^22]: 真相感知的上下文选择：缓解大型语言模型被不真实上下文误导产生幻觉

    Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts

    [https://arxiv.org/abs/2403.07556](https://arxiv.org/abs/2403.07556)

    提出了一种名为真相感知的上下文选择（TACS）的轻量级方法，可以通过对输入上下文进行真相检测并构建相应的注意力蒙版来缓解大型语言模型被不真实上下文误导产生幻觉

    

    尽管大型语言模型（LLMs）展示了令人印象深刻的文本生成能力，但它们很容易被用户或知识论证工具提供的不真实上下文误导，从而产生幻觉。为了减轻LLMs被不真实信息误导并利用知识论证，我们提出了真相感知的上下文选择（TACS），这是一种轻量级方法，可以从输入中屏蔽不真实的上下文。TACS首先对输入上下文进行真相检测，利用LLM内的参数化知识。随后，根据每个位置的真实性构建相应的注意力蒙版，选择真实的上下文并丢弃不真实的上下文。此外，我们引入一个新的评估指标，扰动适应率，以进一步研究LLMs接受真实信息和抵制不真实信息的能力。

    arXiv:2403.07556v1 Announce Type: new  Abstract: Although large language models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by the untruthful context provided by users or knowledge argumentation tools, thereby producing hallucinations. To alleviate the LLMs from being misled by untruthful information and take advantage of knowledge argumentation, we propose Truth-Aware Context Selection (TACS), a lightweight method to shield untruthful context from the inputs. TACS begins by performing truth detection on the input context, leveraging the parameterized knowledge within the LLM. Subsequently, it constructs a corresponding attention mask based on the truthfulness of each position, selecting the truthful context and discarding the untruthful context. Additionally, we introduce a new evaluation metric, Disturbance Adaption Rate, to further study the LLMs' ability to accept truthful information and resist untruthful information. Experimental resul
    
[^23]: MAMMOTH: 赫尔辛基大规模多语言模块化开放翻译

    MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki

    [https://arxiv.org/abs/2403.07544](https://arxiv.org/abs/2403.07544)

    本文介绍了MAMMOTH工具包，这是一个旨在训练大规模多语言模块化机器翻译系统的框架，通过模块化设计实现了在计算集群上的高效训练。

    

    在单体大型语言模型时代，自然语言处理正接近其在尺寸和信息处理方面的极限。趋势是模块化，这是朝着设计具有专门功能的较小子网络和组件的方向迈出的必要步骤。本文介绍了MAMMOTH工具包：一个旨在训练大规模多语言模块化机器翻译系统的框架，最初源自OpenNMT-py，然后经过调整以确保在计算集群上高效训练。我们展示了它在A100和V100 NVIDIA GPU集群上的效率，并讨论了我们的设计理念和未来的计划。该工具包已在网上公开提供。

    arXiv:2403.07544v1 Announce Type: new  Abstract: NLP in the age of monolithic large language models is approaching its limits in terms of size and information that can be handled. The trend goes to modularization, a necessary step into the direction of designing smaller sub-networks and components with specialized functionality. In this paper, we present the MAMMOTH toolkit: a framework designed for training massively multilingual modular machine translation systems at scale, initially derived from OpenNMT-py and then adapted to ensure efficient training across computation clusters. We showcase its efficiency across clusters of A100 and V100 NVIDIA GPUs, and discuss our design philosophy and plans for future information. The toolkit is publicly available online.
    
[^24]: 基于矩阵变换的低秩调整（MTLoRA）：一种受大脑启发的参数高效微调方法

    Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning

    [https://arxiv.org/abs/2403.07440](https://arxiv.org/abs/2403.07440)

    该论文提出了基于矩阵变换的低秩调整（MTLoRA）方法，受大脑启发，用于提高微调技术的复杂任务适应性、性能、稳定性和算法复杂性。

    

    基于大型预训练语言模型（LPLMs）的微调技术已被证明可以显著提高模型在各种下游任务上的性能，并有效控制LPLMs的输出行为。本文受大脑功能受其几何结构塑造的启发，将这一思想融入LoRA技术中，提出了一种新的基于矩阵变换的重新参数化方法，以减少复杂任务适应性、性能、稳定性和算法复杂性方面的改进空间。

    arXiv:2403.07440v1 Announce Type: cross  Abstract: Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs. Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources. Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity. In response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameteriz
    
[^25]: 在常识知识图上进行逻辑查询的复杂推理

    Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs

    [https://arxiv.org/abs/2403.07398](https://arxiv.org/abs/2403.07398)

    提出了COM2数据集，通过在常识知识图中抽样多跳逻辑查询并结合大型语言模型，显著提高了语言模型在复杂推理能力方面的性能。

    

    事件常识推理需要具有推理事件之间关系的能力，以及推断在这种关系之下的隐含上下文。然而，数据稀缺使得语言模型难以学会为涉及复杂事件相互作用的背景和问题生成常识推断变得具有挑战性。为了满足这种需求，我们提出了COM2（COMplex COMmonsense），这是一个通过从现有常识知识图（CSKG）中抽样多跳逻辑查询（例如，事件A和B的联合效果或因果关系，或事件C的效果的效果），并利用手工制作的规则和大型语言模型将其用多选和文本生成问题的形式表达出来的新数据集。我们的实验表明，在COM2上训练的语言模型在复杂推理能力方面取得了显著的改进，从而增强了零-shot性能，无论是在领域内还是领域外的任务中。

    arXiv:2403.07398v1 Announce Type: cross  Abstract: Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense inferences for contexts and questions involving interactions between complex events. To address this demand, we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions. Our experiments show that language models trained on COM2 exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for
    
[^26]: SmallToLarge (S2L): 通过总结小模型的训练轨迹，为大型语言模型的微调提供可伸缩的数据选择

    SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models

    [https://arxiv.org/abs/2403.07384](https://arxiv.org/abs/2403.07384)

    S2L提出了一种通过总结小模型的训练轨迹，来指导大型语言模型数据选择的方法，显著提高了数学问题解决中监督微调的数据效率，并在数据集性能上表现优异。

    

    尽管数据选择在大型语言模型（LLMs）的预训练和指导微调阶段非常有效，但在专业领域的监督微调（SFT）中改善数据效率面临着重大挑战，原因是微调数据的复杂性。为弥合这一差距，我们引入了一种有效且可伸缩的数据选择方法S2L（SmallToLarge），它利用小模型的训练轨迹来指导更大模型的数据选择。通过大量实验，我们证明了S2L显著提高了数学问题解决的SFT数据效率，将训练数据缩减到原始MathInstruct数据集（Yue等人，2023）的仅11%，以达到全数据集的性能，并在6个领域内外评估数据集中平均优于最先进的数据选择算法4.7%。值得注意的是，仅选择50K数据进行SFT，S2L实现...

    arXiv:2403.07384v1 Announce Type: cross  Abstract: Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models. We demonstrate through extensive experiments that S2L significantly improves data efficiency in SFT for mathematical problem-solving, reducing the training data to just 11% of the original MathInstruct dataset (Yue et al., 2023) to match full dataset performance while outperforming state-of-the-art data selection algorithms by an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably, selecting only 50K data for SFT, S2L achi
    
[^27]: 神经网络和LLMs中优化轨迹的特征：长度、拐点和死胡同

    Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The Lengths, Bends, and Dead Ends

    [https://arxiv.org/abs/2403.07379](https://arxiv.org/abs/2403.07379)

    分析神经网络和LLMs中优化轨迹的复杂性，揭示了优化过程中的关键特征，包括方向探索和方向正则化。

    

    我们提出了一种全新的方法来理解神经网络的机制，通过分析其优化轨迹中包含的丰富参数结构。为此，我们引入了一些关于优化轨迹复杂性的自然概念，既定性又定量地揭示了各种优化选择（如动量、权重衰减和批大小）之间所涉及的内在微妙和相互作用。我们利用这些概念来提供关于深度神经网络优化本质的关键特征：何时顺利进行，何时陷入死胡同。此外，基于我们的轨迹视角，我们揭示了动量和权重衰减之间促进方向探索的交织行为，以及其他一些行为的方向正则化行为。我们在大规模视觉和语言设置中进行实验，包括具有最多120亿个参数的大型语言模型（LLMs）。

    arXiv:2403.07379v1 Announce Type: cross  Abstract: We propose a fresh take on understanding the mechanisms of neural networks by analyzing the rich structure of parameters contained within their optimization trajectories. Towards this end, we introduce some natural notions of the complexity of optimization trajectories, both qualitative and quantitative, which reveal the inherent nuance and interplay involved between various optimization choices, such as momentum, weight decay, and batch size. We use them to provide key hallmarks about the nature of optimization in deep neural networks: when it goes right, and when it finds itself in a dead end. Further, thanks to our trajectory perspective, we uncover an intertwined behaviour of momentum and weight decay that promotes directional exploration, as well as a directional regularization behaviour of some others. We perform experiments over large-scale vision and language settings, including large language models (LLMs) with up to 12 billio
    
[^28]: SVD-LLM: 针对大型语言模型压缩的截断感知奇异值分解

    SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression

    [https://arxiv.org/abs/2403.07378](https://arxiv.org/abs/2403.07378)

    SVD-LLM是一种新的基于SVD的LLM压缩方法，通过截断感知数据白化策略和逐层闭式模型参数更新策略，解决了现有方法的限制，实现了直接映射奇异值和压缩损失之间的关系。

    

    大型语言模型（LLMs）的进展受到其庞大尺寸的限制，这需要LLM压缩方法以实现实际部署。奇异值分解（SVD）为LLM压缩提供了一个有希望的解决方案。然而，现有的基于SVD的LLM压缩方法存在两个关键限制：截断较小的奇异值可能导致更高的压缩损失，并且在SVD截断后剩余模型参数的更新缺失。在这项工作中，我们提出了SVD-LLM，一种新的基于SVD的LLM压缩方法，解决了现有方法的限制。SVD-LLM采用了一种截断感知的数据白化策略，以确保奇异值和压缩损失之间的直接映射。此外，SVD-LLM采用一种逐层闭式模型参数更新策略，以弥补SVD截断引起的准确性降低。我们在总共11个数据集和七个m上评估了SVD-LLM。

    arXiv:2403.07378v1 Announce Type: new  Abstract: The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven m
    
[^29]: NavCoT: 通过学习解耦推理提升基于LLM的视觉与语言导航

    NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning

    [https://arxiv.org/abs/2403.07376](https://arxiv.org/abs/2403.07376)

    本文提出了一种名为NavCoT的新策略，在视觉与语言导航中通过学习解耦推理，实现了自主导航决策，有效减轻了领域差距。

    

    视觉与语言导航(VLN)作为具有重要研究价值的具身人工智能问题，需要一个具身代理根据自然语言指示穿越复杂的3D环境。最近的研究突出了大型语言模型(LLMs)在VLN中提高导航推理准确性和可解释性的潜力。然而，它们主要在离线方式下的使用通常在VLN任务和LLM训练语料库之间遭受显著的领域差距。本文引入了一种名为导航思维链(NavCoT)的新型策略，我们通过完成领域内高效参数训练，实现自主导航决策，有效减轻领域差距的成本。具体地，在每个时间步，LLM被提示通过作为世界模型来预测导航思维链：1)根据

    arXiv:2403.07376v1 Announce Type: cross  Abstract: Vision-and-Language Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions. Recent research has highlighted the promising capacity of large language models (LLMs) in VLN by improving navigational reasoning accuracy and interpretability. However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus. This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner. Specifically, at each timestep, the LLM is prompted to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the
    
[^30]: KEBench: 用于大型视觉-语言模型知识编辑的基准测试

    KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models

    [https://arxiv.org/abs/2403.07350](https://arxiv.org/abs/2403.07350)

    KEBench提出了一个新的基准测试，采用不同的数据收集方法和新增加的度量标准（可移植性），以全面评估大型视觉-语言模型知识编辑的质量。

    

    arXiv:2403.07350v1 公告类型: 跨领域 摘要: 目前，针对大型视觉-语言模型(LVLMs)的知识编辑研究很少。编辑LVLMs面临着有效整合多种模态（图像和文本）的挑战，同时确保修改连贯且与上下文相关。现有基准测试具有三个度量标准（可靠性、局部性和一般性）用于衡量LVLMs的知识编辑。然而，该基准测试在评估中使用的生成图像质量不足，并且无法评估模型是否有效地利用与相关内容相关的编辑知识。我们采用不同的数据收集方法构建了一个新的基准测试$\textbf{KEBench}$，并扩展了新度量标准(可移植性)以进行全面评估。借助多模态知识图，我们的图像数据呈现出明确的给实体方向性。这种方向性可以进一步用于提取与实体相关的知识和进行编辑。

    arXiv:2403.07350v1 Announce Type: cross  Abstract: Currently, little research has been done on knowledge editing for Large Vision-Language Models (LVLMs). Editing LVLMs faces the challenge of effectively integrating diverse modalities (image and text) while ensuring coherent and contextually relevant modifications. An existing benchmark has three metrics (Reliability, Locality and Generality) to measure knowledge editing for LVLMs. However, the benchmark falls short in the quality of generated images used in evaluation and cannot assess whether models effectively utilize edited knowledge in relation to the associated content. We adopt different data collection methods to construct a new benchmark, $\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive evaluation. Leveraging a multimodal knowledge graph, our image data exhibits clear directionality towards entities. This directional aspect can be further utilized to extract entity-related knowledge and form editing 
    
[^31]: 重新思考ASTE:一种极简的标记方案与对比学习

    Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive Learning

    [https://arxiv.org/abs/2403.07342](https://arxiv.org/abs/2403.07342)

    提出一种新颖的标记方案，并采用对比学习方法来重新思考ASTE，该方法在性能上优于最先进技术，同时具有更紧凑的设计和降低的计算开销，尤其在少样本学习情景下展现出优越效果。

    

    Aspect Sentiment Triplet Extraction (ASTE) 是细粒度情感分析的一个新兴子任务，旨在从非结构化文本数据中提取结构化的情感三元组。现有的ASTE方法通常通过额外的结构或外部数据来复杂化任务。在这项研究中，我们提出了一种新颖的标记方案，并采用对比学习方法来缓解这些挑战。所提出的方法在与最先进技术的比较中展示出可比甚至更优越的性能，同时具有更紧凑的设计和降低的计算开销。值得注意的是，在大语言模型(LLMs)时代，我们的方法在少样本学习情景下展现出优于GPT 3.5和GPT 4的效果。本研究还为在大语言模型范式下推进ASTE技术提供了宝贵的见解。

    arXiv:2403.07342v1 Announce Type: cross  Abstract: Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of fine-grained sentiment analysis, aiming to extract structured sentiment triplets from unstructured textual data. Existing approaches to ASTE often complicate the task with additional structures or external data. In this research, we propose a novel tagging scheme and employ a contrastive learning approach to mitigate these challenges. The proposed approach demonstrates comparable or superior performance in comparison to state-of-the-art techniques, while featuring a more compact design and reduced computational overhead. Notably, even in the era of Large Language Models (LLMs), our method exhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning scenarios. This study also provides valuable insights for the advancement of ASTE techniques within the paradigm of large language models.
    
[^32]: IM-Unpack: 使用任意低精度整数进行训练和推断

    IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers

    [https://arxiv.org/abs/2403.07339](https://arxiv.org/abs/2403.07339)

    本研究旨在验证在移除低位宽限制时，对于各种Transformer-based模型，整数是否足以满足所有GEMM需求（训练和推断阶段），并且可以与浮点数相媲美，而无需复杂技巧。

    

    GEneral Matrix Multiply (GEMM)是深度学习中的一个核心操作，对应于计算占比最大的部分。因此，提高其效率是一个正在进行研究的热门主题。一种流行的策略是使用低位宽整数来近似矩阵中的原始条目。这样可以提高效率，但常常需要复杂的技术来控制产生的舍入误差。在这项工作中，我们首次验证当移除低位宽限制时，对于各种基于Transformer的模型，整数是否足够满足所有GEMMs的需求 - 无论是训练阶段还是推断阶段，并且可以与浮点数对应项达到一致。无需复杂技术。我们发现，虽然在这些模型中遇到的大多数矩阵条目可以很容易地用低位宽整数表示，但存在一些重要条目

    arXiv:2403.07339v1 Announce Type: cross  Abstract: GEneral Matrix Multiply (GEMM) is a central operation in deep learning and corresponds to the largest chunk of the compute footprint. Therefore, improving its efficiency is an active topic of ongoing research. A popular strategy is the use of low bit-width integers to approximate the original entries in a matrix. This allows efficiency gains, but often requires sophisticated techniques to control the rounding error incurred. In this work, we first verify/check that when the low bit-width restriction is removed, for a variety of Transformer-based models, whether integers are sufficient for all GEMMs need -- for {\em both} training and inference stages, and can achieve parity with floating point counterparts. No sophisticated techniques are needed. We find that while a large majority of entries in matrices (encountered in such models) can be easily represented by {\em low} bit-width integers, the existence of a few heavy hitter entries m
    
[^33]: GPT生成文本检测：基准数据集和基于张量的检测方法

    GPT-generated Text Detection: Benchmark Dataset and Tensor-based Detection Method

    [https://arxiv.org/abs/2403.07321](https://arxiv.org/abs/2403.07321)

    介绍了一个新的GPT生成文本检测数据集GRiD，用于评估检测模型性能和区分人类与ChatGPT生成响应。提供了数据集特征分析和多种检测方法基准测试结果。

    

    随着诸如ChatGPT等自然语言模型在应用和服务中越来越普遍，检测其输出的鲁棒准确方法变得至关重要。本文介绍了GPT Reddit数据集（GRiD），这是一个用于评估检测模型性能的新颖的GPT生成文本检测数据集，用于识别ChatGPT生成的响应。该数据集包含基于Reddit的各种上下文提示对，包括人为生成和ChatGPT生成的响应。我们对数据集的特征进行了分析，包括语言多样性、上下文复杂性和响应质量。为展示数据集的实用性，我们对其进行了多种检测方法的基准测试，展示它们在区分人类和ChatGPT生成的响应方面的功效。这一数据集可用作评估和推进检测技术的资源。

    arXiv:2403.07321v1 Announce Type: new  Abstract: As natural language models like ChatGPT become increasingly prevalent in applications and services, the need for robust and accurate methods to detect their output is of paramount importance. In this paper, we present GPT Reddit Dataset (GRiD), a novel Generative Pretrained Transformer (GPT)-generated text detection dataset designed to assess the performance of detection models in identifying generated responses from ChatGPT. The dataset consists of a diverse collection of context-prompt pairs based on Reddit, with human-generated and ChatGPT-generated responses. We provide an analysis of the dataset's characteristics, including linguistic diversity, context complexity, and response quality. To showcase the dataset's utility, we benchmark several detection methods on it, demonstrating their efficacy in distinguishing between human and ChatGPT-generated responses. This dataset serves as a resource for evaluating and advancing detection te
    
[^34]: 知识图谱大型语言模型（KG-LLM）用于链接预测

    Knowledge Graph Large Language Model (KG-LLM) for Link Prediction

    [https://arxiv.org/abs/2403.07311](https://arxiv.org/abs/2403.07311)

    该论文提出了知识图谱大型语言模型框架（KG-LLM），利用思维链提示和上下文学习等NLP范例，以增强知识图谱中的多跳链接预测，并展示了框架在微调大型语言模型和零次尝试能力方面的有效性。

    

    在知识图谱分析领域，预测知识图谱（KGs）内多个链接的任务是一个挑战，由于自然语言处理（NLP）和知识图嵌入技术的进步，这一挑战变得越来越可解决。本文介绍了一种新的方法，即知识图谱大型语言模型框架（KG-LLM），该框架利用关键的NLP范例，包括思维链提示（CoT）和上下文学习（ICL），以增强知识图谱中的多跳链接预测。通过将KG转换为CoT提示，我们的框架旨在识别并学习实体及其相互关系的潜在表示。为了展示KG-LLM框架的有效性，我们在该框架内微调了三种主要的大型语言模型（LLMs），同时采用了非ICL和ICL任务进行全面评估。此外，我们探讨了该框架为LLMs提供零次尝试能力的潜力。

    arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
    
[^35]: 通过跨模态知识蒸馏控制预训练LLMs进行广义时间序列预测

    Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation

    [https://arxiv.org/abs/2403.07300](https://arxiv.org/abs/2403.07300)

    通过跨模态知识蒸馏和LLMs对齐框架，该方法利用静态和动态知识，充分释放LLMs在时间序列预测中的潜力

    

    多变量时间序列预测最近随着深度学习模型的快速增长取得了巨大成功。然而，现有方法通常使用有限的时间数据从头开始训练模型，阻碍了它们的泛化。最近，随着大语言模型（LLMs）的激增，一些工作尝试将LLMs引入时间序列预测中。尽管取得了有希望的结果，但这些方法直接将时间序列作为LLMs的输入，忽略了时间和文本数据之间固有的模态差距。在这项工作中，我们提出了一个新颖的大语言模型和时间序列对齐框架，称为LLaTA，以充分发挥LLMs在时间序列预测挑战中的潜力。基于跨模态知识蒸馏，所提出的方法利用了预训练LLMs中的输入无关静态知识和输入相关动态知识。通过这种方式，该方法为预测模型赋能

    arXiv:2403.07300v1 Announce Type: cross  Abstract: Multivariate time series forecasting has recently gained great success with the rapid growth of deep learning models. However, existing approaches usually train models from scratch using limited temporal data, preventing their generalization. Recently, with the surge of the Large Language Models (LLMs), several works have attempted to introduce LLMs into time series forecasting. Despite promising results, these methods directly take time series as the input to LLMs, ignoring the inherent modality gap between temporal and text data. In this work, we propose a novel Large Language Models and time series alignment framework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time series forecasting challenge. Based on cross-modal knowledge distillation, the proposed method exploits both input-agnostic static knowledge and input-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers the forecasting model with f
    
[^36]: 一种成本效益和自适应的LLM摇晃和恢复机制框架

    A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism

    [https://arxiv.org/abs/2403.07283](https://arxiv.org/abs/2403.07283)

    介绍了一种名为CypherTalk的成本效益和自适应的LLM摇晃调整和恢复机制，通过优化摇晃操作符设置，实现了在成本、模型效用和隐私之间权衡的结果。

    

    随着大型语言模型（LLMs）在真实应用中取得巨大成功，越来越多的用户希望通过云服务开发和部署他们定制的LLMs。然而，在一些特定领域，人们仍然关注成本、隐私问题和准确性之间的权衡。本研究引入了一种名为CypherTalk的成本效益和自适应LLM摇晃调整和恢复机制，通过精心设计的水平和垂直摇晃操作符，我们能够实现与基于密码学或差分隐私方法的LLM隐私保护方案相当的准确性结果。实验证明，使用CypherTalk框架，用户可以在使用优化摇晃操作符设置时实现可靠的准确性。据我们所知，这是首个考虑在LLM场景中成本、模型效用和隐私之间权衡的工作。

    arXiv:2403.07283v1 Announce Type: cross  Abstract: As Large Language Models (LLMs) gain great success in real-world applications, an increasing number of users are seeking to develop and deploy their customized LLMs through cloud services. Nonetheless, in some specific domains, there are still concerns regarding cost and trade-offs between privacy issues and accuracy. In this study, we introduce a cost-effective and self-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk. With carefully designed horizontal and vertical shaking operators, we can achieve comparable accuracy results with SOTA privacy-preserving LLM schemes using Cryptography-based or Differential Privacy-based methods. Experiments also show that with the CypherTalk framework, users can achieve reliable accuracy when using optimized shaking operator settings. To our best knowledge, this is the first work that considers cost, and trade-off between model utility and privacy in LLM scenarios.
    
[^37]: 对可解释知识追踪的调查

    A Survey of Explainable Knowledge Tracing

    [https://arxiv.org/abs/2403.07279](https://arxiv.org/abs/2403.07279)

    对知识追踪算法进行了解释性分析，提出了可解释知识追踪模型分为透明模型和黑盒模型，探讨了不同阶段的解释性方法。

    

    随着高质量教育数据的长期积累，人工智能在知识追踪中表现出色。然而，由于一些算法缺乏解释性和透明性，这种方法会导致利益相关者的信任降低，智能决策的接受度降低。因此，算法需要达到高准确性，用户需要了解内部操作机制并为决策提供可靠解释。本文对可解释性KT算法进行了彻底分析。首先介绍了可解释人工智能和知识追踪的概念和常见方法。然后，将可解释知识追踪模型分为两类：透明模型和黑盒模型。接着，从“ante hoc可解释性方法”、“post hoc可解释性方法”和其他维度审查了可解释性方法。

    arXiv:2403.07279v1 Announce Type: new  Abstract: With the long term accumulation of high quality educational data, artificial intelligence has shown excellent performance in knowledge tracing. However, due to the lack of interpretability and transparency of some algorithms, this approach will result in reduced stakeholder trust and a decreased acceptance of intelligent decisions. Therefore, algorithms need to achieve high accuracy, and users need to understand the internal operating mechanism and provide reliable explanations for decisions. This paper thoroughly analyzes the interpretability of KT algorithms. First, the concepts and common methods of explainable artificial intelligence and knowledge tracing are introduced. Next, explainable knowledge tracing models are classified into two categories: transparent models and black box models. Then, the interpretable methods used are reviewed from three stages: ante hoc interpretable methods, post hoc interpretable methods, and other dime
    
[^38]: CKERC：基于常识知识的大型语言模型联合情感识别对话

    CKERC : Joint Large Language Models with Commonsense Knowledge for Emotion Recognition in Conversation

    [https://arxiv.org/abs/2403.07260](https://arxiv.org/abs/2403.07260)

    提出了一种基于常识知识的大型语言模型联合情感识别对话框架，通过设计提示生成互动者的常识知识并利用这些信息进行预训练，以提高说话者的情感识别准确性。

    

    对话中的情感识别(ERC)是一项任务，它在对话的上下文中预测话语的情感。它高度依赖于对话语境、说话者身份信息、多方对话场景等。然而，当前最先进的方法（instructERC）仅仅识别说话者，忽略了在对话过程中说话者背后的常识知识(即，听众的反应和说话者的意图等)，这些知识可以深入挖掘说话者信息。为此，我们提出了一种新颖的基于常识知识的大型语言模型联合情感识别对话框架，即CKERC。我们设计提示来生成基于历史话语的对话者常识，结合大型语言模型的互动者常识识别任务进行LLM预训练，以微调说话者隐含线索信息。通过解决以上挑战，我们的方法取得了最先进的成果。

    arXiv:2403.07260v1 Announce Type: new  Abstract: Emotion recognition in conversation (ERC) is a task which predicts the emotion of an utterance in the context of a conversation. It tightly depends on dialogue context, speaker identity information, multiparty dialogue scenario and so on. However, the state-of-the-art method (instructERC) solely identifying speaker, and ignores commonsense knowledge(i.e., reaction of the listeners and intention of the speaker, etc.) behind speakers during a conversation, which can deeply mine speaker information. To this end, we propose a novel joint large language models with commonsense knowledge framework for emotion recognition in conversation, namely CKERC.We design prompts to generate interlocutors' commonsense based on historical utterances with large language model. And we use the interlocutor commonsense identification task for LLM pre-training to fine-tune speaker implicit clues information.By solving above challenge, our method achieve state-o
    
[^39]: Curry-DPO：利用课程学习和排名偏好增强对齐

    Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences

    [https://arxiv.org/abs/2403.07230](https://arxiv.org/abs/2403.07230)

    提出了一种名为Curry-DPO的方法，在直接偏好优化(DPO)中利用课程学习方法，通过构建多个偏好对来训练模型，相比于标准单一对DPO设置有着更好的性能表现。

    

    直接偏好优化(DPO)是一种有效的技术，利用成对偏好数据(通常是每个用户提示选择和拒绝的响应对)将LLMs与人类偏好对齐。在实践中，对于给定提示可能会存在多个响应，这些响应的质量相对于彼此而言有所不同。有了这些多个响应的质量评级，我们提出利用这些响应为给定提示创建多个偏好对。我们的工作侧重于通过课程学习方法系统地利用构建的多个偏好对来进行DPO训练。特别是，我们根据不同的标准将这些多个偏好数据对从易到难(模拟课程训练)排序。我们详细比较了我们提出的方法与标准单一对DPO设置。我们的方法，我们称之为Curry-DPO，在MTbench、Vicuna、Wiz上始终表现出增强的性能收益。

    arXiv:2403.07230v1 Announce Type: cross  Abstract: Direct Preference Optimization (DPO) is an effective technique that leverages pairwise preference data (usually one chosen and rejected response pair per user prompt) to align LLMs to human preferences. In practice, multiple responses can exist for a given prompt with varying quality relative to each other. With availability of such quality ratings for multiple responses, we propose utilizing these responses to create multiple preference pairs for a given prompt. Our work focuses on systematically using the constructed multiple preference pair in DPO training via curriculum learning methodology. In particular, we order these multiple pairs of preference data from easy to hard (emulating curriculum training) according to various criteria. We show detailed comparisons of our proposed approach to the standard single-pair DPO setting. Our method, which we call Curry-DPO consistently shows increased performance gains on MTbench, Vicuna, Wiz
    
[^40]: 从认知驱动的解析器中预测结构启动的生成

    SPAWNing Structural Priming Predictions from a Cognitively Motivated Parser

    [https://arxiv.org/abs/2403.07202](https://arxiv.org/abs/2403.07202)

    提出了一个框架，利用实证启动模式来建立理论，使用认知驱动解析器SPAWN生成量化启动预测并评估，并以简化的定语从句为案例研究，发现一个理论的启动预测与实证启动模式一致

    

    结构启动是一种广泛使用的心理语言学范式，用于研究人类句子表征。在这项工作中，我们提出了一个框架，用于利用实证启动模式来建立理论，描述人类处理句子时构建的结构表征。该框架使用一种新的认知驱动解析器SPAWN，根据理论句法生成量化启动预测，并用实证人类行为评估这些预测。作为一个案例研究，我们应用这一框架来研究英语中简化的定语从句表征。我们使用SPAWN从两个理论解释中生成启动预测，这两个解释对定语从句的结构做出了不同的假设。我们发现，仅有一个理论（参与式-相位）的预测与实证启动模式一致，从而突出显示出哪些对定语从句的假设更好地捕捉了人类句子表征

    arXiv:2403.07202v1 Announce Type: new  Abstract: Structural priming is a widely used psycholinguistic paradigm to study human sentence representations. In this work we propose a framework for using empirical priming patterns to build a theory characterizing the structural representations humans construct when processing sentences. This framework uses a new cognitively motivated parser, SPAWN, to generate quantitative priming predictions from theoretical syntax and evaluate these predictions with empirical human behavior. As a case study, we apply this framework to study reduced relative clause representations in English. We use SPAWN to generate priming predictions from two theoretical accounts which make different assumptions about the structure of relative clauses. We find that the predictions from only one of these theories (Participial-Phase) align with empirical priming patterns, thus highlighting which assumptions about relative clause better capture human sentence representation
    
[^41]: CuentosIE：一个关于“寓言寓意”的聊天机器人是否有助于教授情商？

    CuentosIE: can a chatbot about "tales with a message" help to teach emotional intelligence?

    [https://arxiv.org/abs/2403.07193](https://arxiv.org/abs/2403.07193)

    CuentosIE提供了一套高度专业化的寓言故事，并提供了一系列工具，旨在教育用户情感知识，并监测他们的情感发展。

    

    在本文中，我们介绍了CuentosIE（TalesEI：一个关于情商发展的寓言寓意聊天机器人），这是一个关于情感的教育性聊天机器人，同时也为教师和心理学家提供了一个工具，通过CuentosIE编制的指标和数据来监测他们的学生/患者。选择“寓言寓意”的理由在于它们的简单性和易于理解，这归功于它们的道德或相关的隐喻。 CuentosIE的主要贡献在于选择、收集和分类一组高度专业化的故事，以及提供工具（搜索、阅读理解、聊天、推荐和分类）对教育用户情感并监测他们的情感发展都很有用。该工具的初步评估取得了令人鼓舞的结果，这肯定了文章标题中提出的问题。

    arXiv:2403.07193v1 Announce Type: cross  Abstract: In this article, we present CuentosIE (TalesEI: chatbot of tales with a message to develop Emotional Intelligence), an educational chatbot on emotions that also provides teachers and psychologists with a tool to monitor their students/patients through indicators and data compiled by CuentosIE. The use of "tales with a message" is justified by their simplicity and easy understanding, thanks to their moral or associated metaphors. The main contributions of CuentosIE are the selection, collection, and classification of a set of highly specialized tales, as well as the provision of tools (searching, reading comprehension, chatting, recommending, and classifying) that are useful for both educating users about emotions and monitoring their emotional development. The preliminary evaluation of the tool has obtained encouraging results, which provides an affirmative answer to the question posed in the title of the article.
    
[^42]: $\mathbf{(N,K)}$-Puzzle：一种用于基准测试生成语言模型中强化学习算法的成本效益测试平台

    $\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model

    [https://arxiv.org/abs/2403.07191](https://arxiv.org/abs/2403.07191)

    提出了一种$(N,K)$-Puzzle测试平台，用于评估和比较生成语言模型中的强化学习算法。

    

    强化学习（RL）算法的最新进展旨在提高规模化语言模型的性能。 然而，缺乏一种成本效益且标准化的测试平台，专门用于评估和比较这些算法。 为填补这一空白，我们提出了24-Puzzle的一般化版本：$(N, K)$-Puzzle，挑战语言模型以使用$N$个整数达到目标值$K$。 我们评估了已建立的RL算法（如Proximal Policy Optimization（PPO）），以及新颖方法（如Identity Policy Optimization（IPO）和Direct Policy Optimization（DPO））的有效性。

    arXiv:2403.07191v1 Announce Type: cross  Abstract: Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these algorithms. To bridge this gap, we present a generalized version of the 24-Puzzle: the $(N,K)$-Puzzle, which challenges language models to reach a target value $K$ with $N$ integers. We evaluate the effectiveness of established RL algorithms such as Proximal Policy Optimization (PPO), alongside novel approaches like Identity Policy Optimization (IPO) and Direct Policy Optimization (DPO).
    
[^43]: 在规模上监测AI修改的内容：AI会议同行评审中ChatGPT影响的案例研究

    Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews

    [https://arxiv.org/abs/2403.07183](https://arxiv.org/abs/2403.07183)

    该研究提出了一种估计大语料库中被大语言模型大幅修改的文本比例的方法，并在AI会议的同行评审中进行了实证分析，发现6.5%至16.9%的文本可能被LLMs大幅修改，揭示了用户行为的一些见解。

    

    我们提出了一种估计大语料库中文本可能被大语言模型（LLM）大幅修改或生成的部分比例的方法。我们的最大似然模型利用专家撰写和AI生成的参考文本，准确高效地检查语料库级别上真实世界LLM使用。我们将这种方法应用于AI会议上科学同行评审的案例研究，该研究发生在ChatGPT发布之后，包括ICLR 2024、NeurIPS 2023、CoRL 2023和EMNLP 2023。我们的研究结果表明，在这些会议提交的同行评审中，6.5%至16.9%的文本可能是由LLMs大幅修改的，即超出拼写检查或小幅更新的范围。生成文本出现的情况为用户行为提供了见解：在报告信心较低、在截止日期前提交的评论以及从评论公司

    arXiv:2403.07183v1 Announce Type: cross  Abstract: We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from review
    
[^44]: 3M-Diffusion：用于文本引导生成分子图的潜在多模态扩散

    3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs

    [https://arxiv.org/abs/2403.07179](https://arxiv.org/abs/2403.07179)

    提出了3M-Diffusion，一种新颖的多模态分子图生成方法，可以生成具有所需属性的多样化、理想情况下是新颖的分子。

    

    生成具有所需属性的分子是一项关键任务，在药物发现和材料设计中具有广泛应用。受到大型语言模型的最新进展的启发，越来越多的人对使用分子的自然语言描述来生成具有所需属性的分子产生了兴趣。大多数现有方法侧重于生成与文本描述精确匹配的分子。然而，实际应用需要能够生成具有所需属性的多样化，理想情况下是新颖的分子的方法。我们提出了一种新颖的多模态分子图生成方法3M-Diffusion，以解决这一挑战。

    arXiv:2403.07179v1 Announce Type: cross  Abstract: Generating molecules with desired properties is a critical task with broad applications in drug discovery and materials design. Inspired by recent advances in large language models, there is a growing interest in using natural language descriptions of molecules to generate molecules with the desired properties. Most existing methods focus on generating molecules that precisely match the text description. However, practical applications call for methods that generate diverse, and ideally novel, molecules with the desired properties. We propose 3M-Diffusion, a novel multi-modal molecular graph generation method, to address this challenge. 3M-Diffusion first encodes molecular graphs into a graph latent space aligned with text descriptions. It then reconstructs the molecular structure and atomic attributes based on the given text descriptions using the molecule decoder. It then learns a probabilistic mapping from the text space to the late
    
[^45]: 重建ROME: 解决顺序模型编辑过程中的模型崩溃问题

    Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing

    [https://arxiv.org/abs/2403.07175](https://arxiv.org/abs/2403.07175)

    本文重建了ROME，提供了更稳定的r-ROME实现，解决了顺序模型编辑过程中的模型崩溃问题。

    

    最近关于使用Rank-One Model Editing (ROME)进行模型编辑的研究表明，有一些事实表明该算法无法进行编辑而不破坏模型。这些编辑以前被称为禁用编辑。这些禁用编辑会导致立即模型崩溃，并限制了ROME用于顺序编辑的使用。在本文中，我们做出了两个主要贡献。首先，我们展示了在使用CounterFact数据集进行编辑时，ROME仅在此时发生模型崩溃，并在使用zsRE数据集时不会发生。其次，我们发现禁用编辑是ROME原始实现的产物。通过本文，我们提供了一个更稳定的实现ROME，我们将其称为r-ROME，并展示我们在使用ROME进行大规模顺序编辑时不再观察到模型崩溃。

    arXiv:2403.07175v1 Announce Type: cross  Abstract: Recent work on model editing using Rank-One Model Editing (ROME), a popular model editing method, has shown that there are certain facts that the algorithm is unable to edit without breaking the model. Such edits have previously been called disabling edits. These disabling edits cause immediate model collapse and limits the use of ROME for sequential editing. In this paper, we make two main contributions. Firstly, we show that model collapse with ROME only happens when making edits using the CounterFact dataset and does not happen when using the zsRE dataset. Secondly, we find that disabling edits are an artifact of the original implementation of ROME. With this paper, we provide a more stable implementation ROME, which we call r-ROME and show that we no longer observe model collapse when making large scale sequential edits with ROME.
    
[^46]: Thought Graph: 生成生物推理思维过程

    Thought Graph: Generating Thought Process for Biological Reasoning

    [https://arxiv.org/abs/2403.07144](https://arxiv.org/abs/2403.07144)

    Thought Graph框架通过基因集分析揭示生物过程间的语义关系，并在与人类注释的余弦相似性方面优于传统方法。

    

    我们提出了Thought Graph作为一个新颖的框架，支持复杂推理，并以基因集分析为例揭示生物过程之间的语义关系。我们的框架能够深入理解基因集，根据与人类注释的余弦相似性，明显优于GSEA 40.28%和LLM基线5.38%。我们的分析进一步提供了生物过程命名的未来方向的见解，以及对生物信息学和精准医学的影响。

    arXiv:2403.07144v1 Announce Type: new  Abstract: We present the Thought Graph as a novel framework to support complex reasoning and use gene set analysis as an example to uncover semantic relationships between biological processes. Our framework stands out for its ability to provide a deeper understanding of gene sets, significantly surpassing GSEA by 40.28% and LLM baselines by 5.38% based on cosine similarity to human annotations. Our analysis further provides insights into future directions of biological processes naming, and implications for bioinformatics and precision medicine.
    
[^47]: 一个类别一个提示：使用扩散模型进行数据集精炼

    One Category One Prompt: Dataset Distillation using Diffusion Models

    [https://arxiv.org/abs/2403.07142](https://arxiv.org/abs/2403.07142)

    提出了使用扩散模型进行数据集精炼的新方法，有效地解决了数据集精炼在处理高分辨率图像和复杂架构时的可扩展性问题

    

    深度神经网络训练所需的大量数据对存储和传输方面提出了重大挑战。数据集精炼已经成为将大规模数据集的信息压缩成一组代表性合成样本的有前途的技术。然而，传统的数据集精炼方法通常在处理高分辨率图像和更复杂架构时很难有效扩展，这是由于双层优化的限制。最近，一些工作提出利用分离的优化方案将知识精炼和数据集精炼相结合，以扩大数据集精炼规模。尽管这些方法有效地解决了可扩展性问题，但它们依赖于广泛的图像增强，需要存储增强图像的软标签。在本文中，我们引入了使用扩散模型的数据集精炼（D3M）作为数据集精炼的新范式，利用

    arXiv:2403.07142v1 Announce Type: cross  Abstract: The extensive amounts of data required for training deep neural networks pose significant challenges on storage and transmission fronts. Dataset distillation has emerged as a promising technique to condense the information of massive datasets into a much smaller yet representative set of synthetic samples. However, traditional dataset distillation approaches often struggle to scale effectively with high-resolution images and more complex architectures due to the limitations in bi-level optimization. Recently, several works have proposed exploiting knowledge distillation with decoupled optimization schemes to scale up dataset distillation. Although these methods effectively address the scalability issue, they rely on extensive image augmentations requiring the storage of soft labels for augmented images. In this paper, we introduce Dataset Distillation using Diffusion Models (D3M) as a novel paradigm for dataset distillation, leveraging
    
[^48]: 利用大型语言模型叙述因果图

    Narrating Causal Graphs with Large Language Models

    [https://arxiv.org/abs/2403.07118](https://arxiv.org/abs/2403.07118)

    本研究探讨了大型预训练语言模型生成从因果图中的文本的能力，结果表明因果文本描述在训练数据增加时有所改善，但在零热身设置下更难生成，用户可以更快部署未来应用程序。

    

    利用生成式人工智能从图形生成文本描述的应用主要集中在知识图谱上，其通过事实连接概念。本研究探讨了大型预训练语言模型生成从因果图中的文本的能力，其中显著概念表示为节点，因果关系通过有向、类型化边表示。这些图中编码的因果推理可以支持诸如医疗保健或营销等各种应用。我们通过两个公开可用的因果图数据集，在各种设置下实证研究了四个GPT-3模型的性能。我们的结果表明，与基于事实的图形相比，虽然因果文本描述随着训练数据的增加而改善，但在零热身设置下更难生成。结果进一步表明，生成式AI的用户可以更快地部署未来应用程序，因为当仅使用少量示例训练模型时，可以获得类似的性能。

    arXiv:2403.07118v1 Announce Type: new  Abstract: The use of generative AI to create text descriptions from graphs has mostly focused on knowledge graphs, which connect concepts using facts. In this work we explore the capability of large pretrained language models to generate text from causal graphs, where salient concepts are represented as nodes and causality is represented via directed, typed edges. The causal reasoning encoded in these graphs can support applications as diverse as healthcare or marketing. Using two publicly available causal graph datasets, we empirically investigate the performance of four GPT-3 models under various settings. Our results indicate that while causal text descriptions improve with training data, compared to fact-based graphs, they are harder to generate under zero-shot settings. Results further suggest that users of generative AI can deploy future applications faster since similar performances are obtained when training a model with only a few example
    
[^49]: SPA：面向云端和设备协作的计算友好型Seq2seq个性化生成

    SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation

    [https://arxiv.org/abs/2403.07088](https://arxiv.org/abs/2403.07088)

    提出了SPA（Side Plugin Adaption）的轻量级架构，用于在严格的设备计算和内存约束条件下快速进行推断，同时保留隐私。

    

    大语言模型(LLMs)表现出色的能力已在各种任务和问答中得到展示。然而，LLMs需要高计算成本和大内存成本。同时，当训练或预测过程包含敏感信息时，LLMs可能会导致隐私泄露。在本文中，我们提出了SPA（Side Plugin Adaption），这是一个轻量级架构，用于快速设备上的推断和在严格的设备计算和内存约束条件下保持隐私。

    arXiv:2403.07088v1 Announce Type: new  Abstract: Large language models(LLMs) have shown its outperforming ability on various tasks and question answering. However, LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference and privacy retaining on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and private personal feature.Further more, SPA provides a framework to keep feature-base parameters on private guaranteed but 
    
[^50]: 基于LSTM的文本生成：对历史数据集的研究

    LSTM-Based Text Generation: A Study on Historical Datasets

    [https://arxiv.org/abs/2403.07087](https://arxiv.org/abs/2403.07087)

    本研究探讨了基于LSTM的文本生成在历史数据集上的应用，展示了这些模型在生成语言丰富、语境相关的文本方面的高准确性和高效率。

    

    本文探讨了长短期记忆（LSTM）网络在文本生成领域的应用，重点在于利用莎士比亚和尼采的历史数据集。LSTMs以其处理序列数据的有效性而闻名，这里应用它们来建模历史文本中固有的复杂语言模式和结构。研究表明，基于历史数据集训练的LSTM模型不仅能够生成语言丰富且语境相关的文本，还能提供关于语言模式随时间演变的见解。研究结果展示了基于LSTM的模型在预测尼采作品文本时的高准确性和高效率，具有较低的损失值和100次迭代的训练时间。模型的准确性为0.9521，表明其高准确性。模型的损失为0.2518，显示其有效性。

    arXiv:2403.07087v1 Announce Type: cross  Abstract: This paper presents an exploration of Long Short-Term Memory (LSTM) networks in the realm of text generation, focusing on the utilization of historical datasets for Shakespeare and Nietzsche. LSTMs, known for their effectiveness in handling sequential data, are applied here to model complex language patterns and structures inherent in historical texts. The study demonstrates that LSTM-based models, when trained on historical datasets, can not only generate text that is linguistically rich and contextually relevant but also provide insights into the evolution of language patterns over time. The finding presents models that are highly accurate and efficient in predicting text from works of Nietzsche, with low loss values and a training time of 100 iterations. The accuracy of the model is 0.9521, indicating high accuracy. The loss of the model is 0.2518, indicating its effectiveness. The accuracy of the model in predicting text from the w
    
[^51]: 自动评价正确: 使用合成数据进行模型评估

    AutoEval Done Right: Using Synthetic Data for Model Evaluation

    [https://arxiv.org/abs/2403.07008](https://arxiv.org/abs/2403.07008)

    提出了用合成数据进行模型评估的方法，通过高效和统计上合理的算法，在GPT-4实验中有效的人工标记样本大小增加了50%。

    

    机器学习模型的评估使用人工标记的验证数据可能既昂贵又耗时。可以使用AI标记的合成数据来减少此类目的人工注释数量，这一过程称为自动评估。我们提出了用于此目的的高效和统计上合理的算法，可以提高样本效率，同时保持不偏。这些算法在与GPT-4进行的实验中将有效的人工标记样本大小增加了高达50%。

    arXiv:2403.07008v1 Announce Type: cross  Abstract: The evaluation of machine learning models using human-labeled validation data can be expensive and time-consuming. AI-labeled synthetic data can be used to decrease the number of human annotations required for this purpose in a process called autoevaluation. We suggest efficient and statistically principled algorithms for this purpose that improve sample efficiency while remaining unbiased. These algorithms increase the effective human-labeled sample size by up to 50% on experiments with GPT-4.
    
[^52]: 引导LLM走向正确之路：快速、非侵入式受限生成

    Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation

    [https://arxiv.org/abs/2403.06988](https://arxiv.org/abs/2403.06988)

    提出了一种新颖的解码算法DOMINO，在生成文本过程中以完全基于子词对齐的方式强制执行约束，几乎没有性能开销并有时甚至实现近2倍速度提升，远远优于现有方法。

    

    为了确保大型语言模型（LLMs）生成的文本符合预期格式，受限解码提出在生成过程中强制执行严格的形式语言约束。然而，正如我们在这项工作中所展示的，这类方法不仅在生成过程中产生性能开销，而且许多方法如果没有正确地将LLM子词词汇与外部约束对齐，则还会显著损害任务准确性。为了解决这个问题，我们提出了一种新颖的解码算法DOMINO，可以以完全基于子词对齐的方式强制执行约束，同时利用预计算和推测解码来实现几乎零开销，有时甚至比不受限制的解码快近2倍，从而远远超过现有方法的表现。

    arXiv:2403.06988v1 Announce Type: cross  Abstract: To ensure that text generated by large language models (LLMs) is in an expected format, constrained decoding proposes to enforce strict formal language constraints during generation. However, as we show in this work, not only do such methods incur performance overhead during generation, but many of them also significantly impair task accuracy, if they do not correctly align the underlying LLM sub-word vocabularies with external constraints. To address this, we present a novel decoding algorithm, DOMINO, that can enforce constraints in a fully subword-aligned fashion, while leveraging pre-computation and speculative decoding to achieve virtually no overhead and in some cases even almost 2$\times$ speedup over unconstrained decoding -- thereby outperforming existing approaches by a wide margin.
    
[^53]: MEND：元演示蒸馏用于有效和高效的上下文学习

    MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning

    [https://arxiv.org/abs/2403.06914](https://arxiv.org/abs/2403.06914)

    提出了Meta dEmonstratioN Distillation (MEND)，利用知识蒸馏提高MEND和LLM之间的对齐，实现了高效和有效的上下文学习。

    

    大型语言模型(LLMs)展示了令人印象深刻的上下文学习(ICL)能力，其中LLM为给定的测试输入和少量输入-输出对(演示)进行预测。然而，演示的加入导致自注意机制的计算开销呈二次增加。现有解决方案尝试将冗长的演示蒸馏成紧凑的向量。然而，它们通常需要特定于任务的重新训练或牺牲LLM的上下文学习性能。为了缓解这些挑战，我们提出了Meta dEmonstratioN Distillation (MEND)，其中语言模型学会将任何冗长演示蒸馏为向量，而无需为新的下游任务重新训练。我们利用知识蒸馏增强MEND和LLM之间的对齐，同时实现效率和有效性。MEND具有蒸馏演示的元知识

    arXiv:2403.06914v1 Announce Type: cross  Abstract: Large Language models (LLMs) have demonstrated impressive in-context learning (ICL) capabilities, where a LLM makes predictions for a given test input together with a few input-output pairs (demonstrations). Nevertheless, the inclusion of demonstrations leads to a quadratic increase in the computational overhead of the self-attention mechanism. Existing solutions attempt to distill lengthy demonstrations into compact vectors. However, they often require task-specific retraining or compromise LLM's in-context learning performance. To mitigate these challenges, we present Meta dEmonstratioN Distillation (MEND), where a language model learns to distill any lengthy demonstrations into vectors without retraining for a new downstream task. We exploit the knowledge distillation to enhance alignment between MEND and LLM, achieving both efficiency and effectiveness simultaneously. MEND is endowed with the meta-knowledge of distilling demonstrat
    
[^54]: CLIcK：韩国文化和语言智慧的基准数据集

    CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean

    [https://arxiv.org/abs/2403.06412](https://arxiv.org/abs/2403.06412)

    CLIcK介绍了一个包含1,995个问答对的韩国文化和语言智慧基准数据集，为填补韩语基准数据缺失的问题而来。

    

    尽管针对韩语的大型语言模型（LLMs）迅速发展，但仍然存在明显缺乏测试必要韩国文化和语言知识的基准数据集。现有的许多韩语基准数据集是通过翻译从英语对应数据集中衍生出来的，它们通常忽视不同的文化背景。仅有少数从韩国数据源捕捉文化知识的基准数据集，提供的仅有偏见和仇恨言论检测等狭窄任务。为了填补这一空白，我们介绍了一个名为CLIcK的韩国文化和语言智慧基准数据集，包含1,995个问答对。CLIcK将其数据来源于韩国官方考试和教科书，将问题分为两个主要类别（语言和文化）下的11个类别。对于CLIcK中的每个实例，我们提供了对哪些文化和语言知识的细粒度注释。

    arXiv:2403.06412v1 Announce Type: new  Abstract: Despite the rapid development of large language models (LLMs) for the Korean language, there remains an obvious lack of benchmark datasets that test the requisite Korean cultural and linguistic knowledge. Because many existing Korean benchmark datasets are derived from the English counterparts through translation, they often overlook the different cultural contexts. For the few benchmark datasets that are sourced from Korean data capturing cultural knowledge, only narrow tasks such as bias and hate speech detection are offered. To address this gap, we introduce a benchmark of Cultural and Linguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs. CLIcK sources its data from official Korean exams and textbooks, partitioning the questions into eleven categories under the two main categories of language and culture. For each instance in CLIcK, we provide fine-grained annotation of which cultural and linguistic knowledge
    
[^55]: 一种音频文本扩散模型用于将语音信号转换为超声舌头成像数据

    An Audio-textual Diffusion Model For Converting Speech Signals Into Ultrasound Tongue Imaging Data

    [https://arxiv.org/abs/2403.05820](https://arxiv.org/abs/2403.05820)

    提出一种音频文本扩散模型，通过编码个体特征和通用模式，生成高质量超声舌头成像数据，用于语言分析和临床评估。

    

    声音到语音逆变换（AAI）旨在将音频转换为发音器官运动，如超声舌头成像（UTI）数据。现有AAI方法的一个问题是仅使用个性化音频信息来推导舌头运动的一般模式，因此生成的UTI数据质量有限。为解决这一问题，本文提出了一种用于UTI数据生成任务的音频文本扩散模型。在该模型中，使用wav2vec 2.0对个体相关的舌头运动细节进行编码，而使用BERT对与舌头运动普遍性相关的ASR转录进行编码。然后通过扩散模块生成UTI数据。实验结果表明，所提出的扩散模型能够生成具有清晰舌头轮廓的高质量UTI数据，这对语言分析和临床评估至关重要。

    arXiv:2403.05820v1 Announce Type: cross  Abstract: Acoustic-to-articulatory inversion (AAI) is to convert audio into articulator movements, such as ultrasound tongue imaging (UTI) data. An issue of existing AAI methods is only using the personalized acoustic information to derive the general patterns of tongue motions, and thus the quality of generated UTI data is limited. To address this issue, this paper proposes an audio-textual diffusion model for the UTI data generation task. In this model, the inherent acoustic characteristics of individuals related to the tongue motion details are encoded by using wav2vec 2.0, while the ASR transcriptions related to the universality of tongue motions are encoded by using BERT. UTI data are then generated by using a diffusion module. Experimental results showed that the proposed diffusion model could generate high-quality UTI data with clear tongue contour that is crucial for the linguistic analysis and clinical assessment. The project can be fou
    
[^56]: GEAR: 一种用于几乎无损生成推断大型语言模型的高效KV缓存压缩方案

    GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM

    [https://arxiv.org/abs/2403.05527](https://arxiv.org/abs/2403.05527)

    GEAR提出了一种高效的KV缓存压缩框架，实现几乎无损的高比率压缩，用于解决大型语言模型推断中因缓存需求增长而导致的记忆绑定问题和性能下降。

    

    关键-值（KV）缓存已成为加快大型语言模型（LLMs）推断生成速度的事实标准。然而，随着序列长度增加而增长的缓存需求已将LLM推断转变为一个记忆绑定问题，显著地限制了系统吞吐量。现有方法依赖于丢弃不重要的标记或均匀量化所有条目。然而，这种方法往往会产生较高的近似误差来表示压缩后的矩阵。自回归解码过程进一步增加了每个步骤的误差，导致模型生成中的重大偏差和性能恶化。为了解决这一挑战，我们提出了GEAR，一种高效的KV缓存压缩框架，实现几乎无损的高压缩比。

    arXiv:2403.05527v1 Announce Type: cross  Abstract: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quant
    
[^57]: ChatASU：唤起LLM的反思，真正理解对话中的方面情绪

    ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues

    [https://arxiv.org/abs/2403.05326](https://arxiv.org/abs/2403.05326)

    本文提出了一个新的基于聊天的方面情绪理解（ChatASU）任务，旨在探索大型语言模型（LLMs）在对话场景中理解方面情绪的能力，并引入了一个子任务Aspect Chain Reasoning（ACR）任务来解决方面共指问题。

    

    在互动场景（例如，问答和对话）中进行方面情绪理解（ASU）近年来引起了越来越多的关注并取得了重要进展。然而，现有研究大多忽略了意见目标（即方面）的共指问题，而这种现象在互动场景特别是对话中普遍存在，限制了ASU的性能。最近，大型语言模型（LLM）展示了将各种NLP任务与聊天范式相结合的强大能力。基于此，本文提出了一项新的基于聊天的方面情绪理解（ChatASU）任务，旨在探索LLMs在对话场景中理解方面情绪的能力。特别是，这项ChatASU任务引入了一个子任务，即方面链推理（ACR）任务，以解决方面共指问题。在此基础上，我们提出了一种可信的自反思方法（TSA）与ChatGLM作为背景。

    arXiv:2403.05326v1 Announce Type: cross  Abstract: Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., Question-Answering and Dialogue) has attracted ever-more interest in recent years and achieved important progresses. However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance. Recently, large language models (LLMs) shows the powerful ability to integrate various NLP tasks with the chat paradigm. In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in understanding aspect sentiments in dialogue scenarios. Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to address the aspect coreference issue. On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as back
    
[^58]: 告诉我实话：一种用于衡量大型语言模型可信度的系统

    Tell me the truth: A system to measure the trustworthiness of Large Language Models

    [https://arxiv.org/abs/2403.04964](https://arxiv.org/abs/2403.04964)

    本文提出了一种基于预定义领域知识图的系统化方法来衡量大型语言模型的可信度。

    

    大型语言模型（LLM）自从2023年11月ChatGPT推出以来，在大多数新闻中占据了重要位置。然而，一年多过去了，公司抵触采用它们的一个主要原因是他们对这些系统的可信度缺乏信心。一项由Baymard（2023）进行的研究发现，ChatGPT-4 在识别网站可用性问题时有80.1%的假阳性错误率。而《JAMA儿科学》杂志（JAMA Pediatrics）于2024年1月的研究发现，ChatGPT 在诊断儿科医疗案例时的准确率为17%（Barile et al., 2024）。那么，何为“信任”？信任是一个相对的、主观的条件，可以根据文化、领域和个体而变化。那么，在给定一个领域的情况下，如何衡量系统的可信度呢？本文提出了一种基于预定义领域知识图表示的系统化方法来衡量可信度。

    arXiv:2403.04964v1 Announce Type: new  Abstract: Large Language Models (LLM) have taken the front seat in most of the news since November 2023, when ChatGPT was introduced. After more than one year, one of the major reasons companies are resistant to adopting them is the limited confidence they have in the trustworthiness of those systems. In a study by (Baymard, 2023), ChatGPT-4 showed an 80.1% false-positive error rate in identifying usability issues on websites. A Jan. '24 study by JAMA Pediatrics found that ChatGPT has an accuracy rate of 17% percent when diagnosing pediatric medical cases (Barile et al., 2024). But then, what is "trust"? Trust is a relative, subject condition that can change based on culture, domain, individuals. And then, given a domain, how can the trustworthiness of a system be measured? In this paper, I present a systematic approach to measure trustworthiness based on a predefined ground truth, represented as a knowledge graph of the domain. The approach is a 
    
[^59]: “在对话中学习”：通过对话中学习实现无需预定义个人资料的个性化对话

    "In Dialogues We Learn": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning

    [https://arxiv.org/abs/2403.03102](https://arxiv.org/abs/2403.03102)

    提出了一种In-Dialogue Learning框架，通过对话历史刻画个人设来完成个性化对话生成任务，无需预定义个人资料，并在实验证明其显著改进对话生成性能。

    

    个性化对话系统近年来备受关注，因其能够生成与不同人设一致的响应。然而，大多数现有方法依赖预定义的个人资料，这不仅耗时且劳动密集，还缺乏灵活性。我们提出了In-Dialogue Learning（IDL），一种微调框架，增强了预训练的大型语言模型利用对话历史来刻画个人设，以完成个性化对话生成任务，而无需预定义个人资料。我们在三个数据集上的实验表明，IDL带来了显著的改进，BLEU和ROUGE分数分别增加了高达200%和247%。此外，人工评估的结果进一步验证了我们提出方法的有效性。

    arXiv:2403.03102v1 Announce Type: cross  Abstract: Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for completing personalized dialogue generation tasks without pre-defined profiles. Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally, the results of human evaluations further validate the efficacy of our proposed method.
    
[^60]: 在表格预测上优化预训练语言模型的方法

    Making Pre-trained Language Models Great on Tabular Prediction

    [https://arxiv.org/abs/2403.01841](https://arxiv.org/abs/2403.01841)

    提出了一种专门为表格数据预测而预训练的语言模型TP-BERTa，通过新颖的相对大小标记化方法和内部特征关注方法解决了预训练语言模型在数值特征值上的不兼容性问题

    

    深度神经网络（DNN）的可迁移性在图像和语言处理领域取得了显著进展。然而，由于表格之间的异质性，这种DNN的优势在表格数据预测（例如回归或分类任务）上仍未得到充分利用。本文提出了TP-BERTa，这是一种专门为表格数据预测而预训练的语言模型。具体而言，一种新颖的相对大小标记化方法将标量数值特征值转换为离散度高、高维度的标记，并且一种内部特征关注方法整合了特征名称和数值特征值。

    arXiv:2403.01841v1 Announce Type: new  Abstract: The transferability of deep neural networks (DNNs) has made significant progress in image and language processing. However, due to the heterogeneity among tables, such DNN bonus is still far from being well exploited on tabular data prediction (e.g., regression or classification tasks). Condensing knowledge from diverse domains, language models (LMs) possess the capability to comprehend feature names from various tables, potentially serving as versatile learners in transferring knowledge across distinct tables and diverse prediction tasks, but their discrete text representation space is inherently incompatible with numerical feature values in tables. In this paper, we present TP-BERTa, a specifically pre-trained LM model for tabular data prediction. Concretely, a novel relative magnitude tokenization converts scalar numerical feature values to finely discrete, high-dimensional tokens, and an intra-feature attention approach integrates fe
    
[^61]: 基于内部表征的上下文锐度作为警报：减少幻觉的一个视角

    In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation

    [https://arxiv.org/abs/2403.01548](https://arxiv.org/abs/2403.01548)

    本研究从内部表征角度深入探讨了大型语言模型幻觉的机制，发现了幻觉的一个显著模式，即在上下文标记的隐藏状态中，正确生成具有更清晰的上下文激活。我们提出了一种基于熵的度量方法，将“锐度”纳入解码过程中，制定了一种受限解码方法，实验证明其在知识寻求和幻觉任务上的有效性。

    

    大型语言模型（LLMs）经常会产生幻觉并产生事实错误，然而我们对它们为什么会犯这些错误的理解仍然有限。在本研究中，我们从内部表征的角度深入探讨LLM幻觉的潜在机制，并发现与幻觉相关的一个突出模式：正确的生成在上下文标记的隐藏状态中具有更清晰的上下文激活，而不正确的生成则没有。利用这一见解，我们提出了一种基于熵的度量来量化上下文隐藏状态之间的“锐度”，并将其纳入解码过程中以制定一种受限解码方法。在各种知识寻求和幻觉基准测试上的实验证明了我们方法的一致有效性，例如，在TruthfulQA上实现了高达8.6点的改进。我们相信这项研究可以提高我们对幻觉的理解。

    arXiv:2403.01548v1 Announce Type: cross  Abstract: Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited. In this study, we delve into the underlying mechanisms of LLM hallucinations from the perspective of inner representations, and discover a salient pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the in-context tokens, compared to the incorrect ones. Leveraging this insight, we propose an entropy-based metric to quantify the ``sharpness'' among the in-context hidden states and incorporate it into the decoding process to formulate a constrained decoding approach. Experiments on various knowledge-seeking and hallucination benchmarks demonstrate our approach's consistent effectiveness, for example, achieving up to an 8.6 point improvement on TruthfulQA. We believe this study can improve our understanding of hallucinat
    
[^62]: WanJuan-CC：一个安全且高质量的开源英文网络文本数据集

    WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset

    [https://arxiv.org/abs/2402.19282](https://arxiv.org/abs/2402.19282)

    WanJuan-CC是一个安全高质量的开源英文网络文本数据集，通过处理大规模的Common Crawl数据并经过多项筛选和过滤步骤得到，为语言模型的预训练提供了重要资源。

    

    本文介绍了 WanJuan-CC，这是一个安全且高质量的开源英文网络文本数据集，来源于Common Crawl数据。研究解决了为语言模型构建大规模预训练数据集所面临的挑战，这需要大量高质量数据。设计了一个全面的流程来处理Common Crawl数据，包括提取、启发式规则过滤、模糊去重、内容安全过滤和数据质量过滤。从大约680亿个原始英文文档中，我们获得了22万亿标记的安全数据，并从中选出了10万亿标记的高质量数据作为WanJuan-CC的一部分。我们已经开源了这个数据集中的3000亿标记。该论文还提供了与数据质量相关的统计信息，使用户可以根据自己的需求选择适当的数据。为评估数据集的质量和实用性，我们使用WanJuan-CC训练了10亿参数和30亿参数的模型。

    arXiv:2402.19282v1 Announce Type: new  Abstract: This paper presents WanJuan-CC, a safe and high-quality open-sourced English webtext dataset derived from Common Crawl data. The study addresses the challenges of constructing large-scale pre-training datasets for language models, which require vast amounts of high-quality data. A comprehensive process was designed to handle Common Crawl data, including extraction, heuristic rule filtering, fuzzy deduplication, content safety filtering, and data quality filtering. From approximately 68 billion original English documents, we obtained 2.22T Tokens of safe data and selected 1.0T Tokens of high-quality data as part of WanJuan-CC. We have open-sourced 300B Tokens from this dataset. The paper also provides statistical information related to data quality, enabling users to select appropriate data according to their needs. To evaluate the quality and utility of the dataset, we trained 1B-parameter and 3B-parameter models using WanJuan-CC and ano
    
[^63]: MMSR：符号回归是一个多模态任务

    MMSR: Symbolic Regression is a Multimodal Task

    [https://arxiv.org/abs/2402.18603](https://arxiv.org/abs/2402.18603)

    符号回归被视为一个多模态任务，研究人员将数据到表达式的映射视为翻译问题，引入大规模预训练模型。

    

    数学公式是探索自然规律几千年来人类智慧的结晶。用简洁的数学公式描述复杂的自然规律是科学家不断追求的目标，也是人工智能面临的重大挑战。这一领域被称为符号回归。在本文中，研究人员将从数据到表达式的映射视为翻译问题，并引入了相应的大规模预训练模型。

    arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
    
[^64]: Finer: 在大型视觉语言模型中研究和增强细粒度视觉概念识别

    Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models

    [https://arxiv.org/abs/2402.16315](https://arxiv.org/abs/2402.16315)

    Finer工作揭示了大型视觉语言模型在细粒度视觉分类上的短板，尤其是难以生成准确的细致属性解释，尽管具有生成高水平图像解释的能力。

    

    最近指导调整的大型视觉语言模型（LVLMs）的进展使模型能够轻松生成高水平的基于图像的解释。尽管这种能力主要归因于大型语言模型（LLMs）中包含的丰富世界知识，但我们的工作揭示了它们在六个不同基准设置下的细粒度视觉分类（FGVC）上的缺陷。最近的LVLMs最先进的模型，如LLaVa-1.5，InstructBLIP和GPT-4V，在分类性能方面严重下降，例如，LLaVA-1.5在斯坦福狗的EM平均下降了65.58，而且还难以根据出现在输入图像中的概念生成具有详细属性的准确解释，尽管它们有生成整体图像级描述的能力。深入分析表明，经过指导调整的LVLMs在给定文本时呈现出模态差距，显示出存在不一致性

    arXiv:2402.16315v1 Announce Type: cross  Abstract: Recent advances in instruction-tuned Large Vision-Language Models (LVLMs) have imbued the models with the ability to generate high-level, image-grounded explanations with ease. While such capability is largely attributed to the rich world knowledge contained within the Large Language Models (LLMs), our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings. Most recent state-of-the-art LVLMs like LLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs for LLaVA-1.5, but also struggle to generate an accurate explanation with detailed attributes based on the concept that appears within an input image despite their capability to generate holistic image-level descriptions. In-depth analyses show that instruction-tuned LVLMs exhibit modality gap, showing discrepancy when given tex
    
[^65]: CounterCurate: 通过对照例子增强物理和语义视觉-语言组合推理能力

    CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples

    [https://arxiv.org/abs/2402.13254](https://arxiv.org/abs/2402.13254)

    本研究提出CounterCurate框架，通过对比例子和生成式微调，全面提升视觉-语言组合推理能力，解决了物理推理和语义对照微调方面的关键问题，实现了显著性能改进。

    

    我们提出CounterCurate，一个框架，全面提升对比和生成式多模态模型的视觉-语言组合推理能力。特别地，我们确定了两个尚未充分探讨的关键问题：忽视了基于物理的推理（计数和位置理解），以及利用高性能文本和图像生成模型进行语义反事实微调的潜力。我们的工作开创了一个解决这些空白的方法。我们首先突出了多模态模型（如CLIP和LLaVA）在基于物理的组合推理中几乎无法胜任的表现。然后，我们应用简单的数据增强，使用基于图像的生成模型GLIGEN生成微调数据，使得性能显著提高：在我们新的策划的Flickr30k-Positions基准测试中，CLIP和LLaVA的性能分别提高了+33%和+37%。此外，我们利用了高性能文本和图像生成模型的能力。

    arXiv:2402.13254v1 Announce Type: cross  Abstract: We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of hig
    
[^66]: 理解文本到SQL中噪声的影响：对BIRD-Bench基准测试的研究

    Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark

    [https://arxiv.org/abs/2402.12243](https://arxiv.org/abs/2402.12243)

    研究深入分析了文本到SQL领域中的噪声对模型的影响，并发现在BIRD-Bench基准测试中存在大量问题和标准查询中的噪声，这会显著影响模型的性能。

    

    Text-to-SQL涉及将自然语言翻译为结构化查询语言（SQL），对于使结构化数据库可以在没有专业知识的情况下得到广泛访问至关重要。然而，设计针对这些任务的模型是具有挑战性的，原因包括存在“噪声”，如模糊问题和语法错误。该研究对广泛使用的BIRD-Bench基准测试中噪声的分布和类型以及噪声对模型的影响进行了深入分析。虽然BIRD-Bench旨在模拟脏乱和嘈杂的数据库值，但并未包含问题和标准查询中的噪声和错误。我们发现数据集中问题和标准查询中的噪声普遍存在，跨领域存在不同程度的噪声，并且噪声类型之间分布不均匀。存在不正确的标准SQL查询，进而生成不正确的标准答案，对基准测试的影响显著。

    arXiv:2402.12243v1 Announce Type: new  Abstract: Text-to-SQL, which involves translating natural language into Structured Query Language (SQL), is crucial for enabling broad access to structured databases without expert knowledge. However, designing models for such tasks is challenging due to numerous factors, including the presence of 'noise,' such as ambiguous questions and syntactical errors. This study provides an in-depth analysis of the distribution and types of noise in the widely used BIRD-Bench benchmark and the impact of noise on models. While BIRD-Bench was created to model dirty and noisy database values, it was not created to contain noise and errors in the questions and gold queries. We found that noise in questions and gold queries are prevalent in the dataset, with varying amounts across domains, and with an uneven distribution between noise types. The presence of incorrect gold SQL queries, which then generate incorrect gold answers, has a significant impact on the ben
    
[^67]: Mafin: 用模型增强微调来增强黑盒嵌入

    Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning

    [https://arxiv.org/abs/2402.12177](https://arxiv.org/abs/2402.12177)

    Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。

    

    检索增强生成（RAG）已经成为缓解大型语言模型（LLMs）中幻觉的有效解决方案。RAG中的检索阶段通常涉及预训练的嵌入模型，将查询和段落转换为向量以捕获它们的语义。然而，当应用于特定领域知识时，标准的预训练嵌入模型可能表现出次优性能，需要进行微调。本文解决了仅能从黑盒模型获取嵌入的情况。我们引入了模型增强微调（Mafin）--一种通过用可训练的嵌入模型增强黑盒嵌入模型来进行微调的新方法。我们的结果表明，Mafin仅需要训练一个小的增强模型就可以显著提高黑盒嵌入的性能。我们验证了我们的方法在有标签和无标签数据集上的有效性。

    arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
    
[^68]: 利用分治程序指导大型语言模型对问题求解进行引导

    Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving

    [https://arxiv.org/abs/2402.05359](https://arxiv.org/abs/2402.05359)

    该论文提出了一种以分治程序引导大型语言模型（LLM）的方法，以解决涉及重复子任务和/或具有欺骗性内容的问题。实验证明，该方法可以提高LLM的表达能力。

    

    基础模型，如大型语言模型（LLMs），因其广泛的应用而引起了广泛的关注。现有的研究表明，适当的提示设计，如思维链，可以释放LLM在不同领域的强大能力。然而，对于处理涉及重复子任务和/或具有欺骗性内容的任务（如算术计算和文章级虚假新闻检测），现有的提示策略要么表现出表达能力不足，要么由幻觉引发中间错误。为了使LLM对这些中间错误更具辨别力，我们提出了一种以分治程序引导LLM的方法，同时确保优越的表达能力和任务分解、子任务解决和解决组装过程的分离。理论分析表明，我们的策略可以引导LLM扩展固定深度Transformer的表达能力。实验表明，我们提出的方法可以实现

    Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
    
[^69]: 健康-LLM：个性化检索增强的疾病预测模型

    Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model

    [https://arxiv.org/abs/2402.00746](https://arxiv.org/abs/2402.00746)

    提出了一个创新的框架，健康-LLM，通过大规模特征提取和医学知识权衡评分，实现了个性化的检索增强疾病预测模型。这种方法通过整合健康报告，调整特征权重，以及利用语言模型和专家见解提高预测准确性，与传统健康管理方法相比具有明显优势。

    

    在卫生保健领域，人工智能（AI）极大地推进了智能医疗技术的发展。然而，传统智能医疗受限于静态数据和统一标准，无法完全与个体情况集成，同时也面临其他挑战。为此，我们提出了一种创新的框架，命名为健康-LLM，将大规模特征提取和医学知识权衡评分相结合。与传统健康管理方法相比，我们的方法具有三个主要优势。首先，我们的方法将健康报告整合到大模型中，提供详细的任务信息。其次，我们使用专业的医学专业知识调整健康特征的权重得分。第三，我们使用半自动特征提取框架增强语言模型的分析能力，并整合专家见解以提高疾病预测的准确性。

    Artificial intelligence (AI) in healthcare has significantly advanced intelligent medical treatment. However, traditional intelligent healthcare is limited by static data and unified standards, preventing full integration with individual situations and other challenges. Hence, a more professional and detailed intelligent healthcare method is needed for development. To this end, we propose an innovative framework named Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring. Compared to traditional health management methods, our approach has three main advantages. First, our method integrates health reports into a large model to provide detailed task information. Second, professional medical expertise is used to adjust the weighted scores of health characteristics. Third, we use a semi-automated feature extraction framework to enhance the analytical power of language models and incorporate expert insights to improve the accuracy of disease predic
    
[^70]: 应对后API困境：搜索引擎结果页面呈现社交媒体数据的偏见观

    Navigating the Post-API Dilemma Search Engine Results Pages Present a Biased View of Social Media Data

    [https://arxiv.org/abs/2401.15479](https://arxiv.org/abs/2401.15479)

    搜索引擎结果页面可以作为社交媒体数据的替代方案，但存在对流行帖子偏见较高、情感更积极以及忽视政治、色情和粗俗帖子的问题。

    

    最近停止访问社交媒体API的决定对互联网研究和整个计算社会科学领域产生了不利影响。这种对数据的访问缺乏已被称为互联网研究的后API时代。幸运的是，流行的搜索引擎有能力爬取、捕获和展示社交媒体数据在其搜索引擎结果页面(SERP)上，如果提供适当的搜索查询，可能会为这一困境提供解决方案。在当前工作中，我们问：SERP是否提供社交媒体数据的完整和无偏见样本？ SERP是否是直接API访问的可行替代方案？为了回答这些问题，我们对（Google）SERP结果和来自Reddit和Twitter/X的非取样数据进行了比较分析。我们发现，SERP结果在支持流行帖子方面存在高度偏见；反对政治、色情和粗俗帖子；在情感上更为积极；并有大

    arXiv:2401.15479v2 Announce Type: replace-cross  Abstract: Recent decisions to discontinue access to social media APIs are having detrimental effects on Internet research and the field of computational social science as a whole. This lack of access to data has been dubbed the Post-API era of Internet research. Fortunately, popular search engines have the means to crawl, capture, and surface social media data on their Search Engine Results Pages (SERP) if provided the proper search query, and may provide a solution to this dilemma. In the present work we ask: does SERP provide a complete and unbiased sample of social media data? Is SERP a viable alternative to direct API-access? To answer these questions, we perform a comparative analysis between (Google) SERP results and nonsampled data from Reddit and Twitter/X. We find that SERP results are highly biased in favor of popular posts; against political, pornographic, and vulgar posts; are more positive in their sentiment; and have large 
    
[^71]: 通过符号转换为语言解决符号相关问题的语言模型

    Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language Conversion for Language Models

    [https://arxiv.org/abs/2401.11725](https://arxiv.org/abs/2401.11725)

    提出了符号转换为语言（S2L）方法，通过将符号转换为基于语言的表示，使得大语言模型能够解决符号相关问题。

    

    符号（或更广义的，非自然语言文本表示）如数字序列、分子式以及表格边界等广泛存在，在各种任务中扮演重要角色，比如抽象推理、化学性质预测以及表格问题回答。尽管大语言模型（LLMs）具有令人印象深刻的自然语言理解能力，但它们在处理符号方面的推理能力仍然不足，这可能归因于符号表示与一般自然语言之间的差异。我们提出了一种无需调参的符号转换为语言（S2L）方法，使大语言模型能够利用自然语言表达的信息来解决符号相关问题。具体而言，S2L首先将涉及的符号转换为基于语言的表示，这可以通过提示LLMs或利用外部工具来实现，然后这些基于语言的表示被整合到

    arXiv:2401.11725v2 Announce Type: replace  Abstract: Symbols (or more broadly, non-natural language textual representations) such as numerical sequences, molecular formulas, and table delimiters widely exist, playing important roles in various tasks such as abstract reasoning, chemical property prediction, and table question answering. Despite the impressive natural language comprehension capabilities of large language models (LLMs), their reasoning abilities for symbols remain inadequate, which could attributed to the difference between symbol representations and general natural languages. We propose symbol-to-language (S2L), a tuning-free method that enables large language models to solve symbol-related problems with information expressed in natural language. Specifically, S2L first converts the symbols involved to language-based representations, which can be implemented by prompting LLMs or leveraging external tools, then these language-based representations are integrated into the 
    
[^72]: 病理报告的多实例生成用于千亿像素全切片图像

    WsiCaption: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images

    [https://arxiv.org/abs/2311.16480](https://arxiv.org/abs/2311.16480)

    研究提出了一种基于多实例生成模型的方法，能够生成千亿像素全切片图像的病理报告，实验结果表明该模型能够产生包含多个临床线索的病理报告。

    

    全切片图像是用于癌症诊断和治疗的数字病理学的基础。撰写病理报告对经验不足的病理学家来说是费时且容易出错的。为了减少工作量并改善临床自动化，我们研究了如何生成给定全切片图像的病理报告。在数据端，我们整理了最大的WSI-文本数据集（TCGA-PathoText）。具体来说，我们通过识别和清理TCGA中叙述诊断幻灯片的病理报告，收集了近1万对高质量的WSI-文本配对，供视觉-语言模型使用。在模型端，我们提出了可以为千亿像素WSI生成病理报告的多实例生成模型（MI-Gen）。我们在TCGA-PathoText的最大子集上对我们的模型进行了基准测试。实验结果表明，我们的模型可以生成包含多个临床线索的病理报告。此外，WSI-文本预测可被视为一种方法。

    arXiv:2311.16480v2 Announce Type: replace-cross  Abstract: Whole slide images are the foundation of digital pathology for the diagnosis and treatment of carcinomas. Writing pathology reports is laborious and error-prone for inexperienced pathologists. To reduce the workload and improve clinical automation, we investigate how to generate pathology reports given whole slide images. On the data end, we curated the largest WSI-text dataset (TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text pairs for visual-language models by recognizing and cleaning pathology reports which narrate diagnostic slides in TCGA. On the model end, we propose the multiple instance generative model (MI-Gen) which can produce pathology reports for gigapixel WSIs. We benchmark our model on the largest subset of TCGA-PathoText. Experimental results show our model can generate pathology reports which contain multiple clinical clues. Furthermore, WSI-text prediction can be seen as an approac
    
[^73]: 使用基于LLM的代理网络模拟意见动态

    Simulating Opinion Dynamics with Networks of LLM-based Agents

    [https://arxiv.org/abs/2311.09618](https://arxiv.org/abs/2311.09618)

    提出了一种基于大型语言模型（LLMs）人口的新方法来模拟意见动态，发现LLM代理存在固有偏见导致模拟代理趋向于科学现实一致的共识，但引入确认偏见后观察到意见分裂，突显了LLM代理在该领域的潜力和局限性。

    

    准确模拟人类意见动态对于理解各种社会现象至关重要，包括极化和错误信息的传播。然而，常用于此类模拟的基于代理的模型（ABM）经常会过分简化人类行为。我们提出了一种基于大型语言模型（LLMs）人口的模拟意见动态的新方法。我们的研究结果显示，LLM代理存在一种对产生准确信息的强烈固有偏见，导致模拟代理趋向于与科学现实一致的共识。然而，这种偏见限制了它们在理解气候变化等问题上抵制共识观点的效用。通过引入提示工程诱导确认偏见后，我们观察到了与现有基于代理模型和意见动态研究一致的意见分裂。这些见解突显了LLM代理在该领域的潜力和局限性，并提出了一条路径。

    arXiv:2311.09618v2 Announce Type: replace-cross  Abstract: Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models (ABMs) commonly used for such simulations often over-simplify human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs). Our findings reveal a strong inherent bias in LLM agents towards producing accurate information, leading simulated agents to consensus in line with scientific reality. This bias limits their utility for understanding resistance to consensus views on issues like climate change. After inducing confirmation bias through prompt engineering, however, we observed opinion fragmentation in line with existing agent-based modeling and opinion dynamics research. These insights highlight the promise and limitations of LLM agents in this domain and suggest a path
    
[^74]: BizBench: 一个用于商业和金融的量化推理基准

    BizBench: A Quantitative Reasoning Benchmark for Business and Finance

    [https://arxiv.org/abs/2311.06602](https://arxiv.org/abs/2311.06602)

    BizBench是一个用于评估模型在推理财务问题方面能力的基准，包括八个量化推理任务，专注于通过程序合成对财务数据进行问答。

    

    回答关于商业和金融领域的问题需要推理能力、准确性以及广泛的技术知识。本文介绍了BizBench，一个用于评估模型在推理现实财务问题方面能力的基准。BizBench包括八个量化推理任务，专注于通过程序合成对财务数据进行问答（QA）。我们包含了三个基于金融主题的代码生成任务，从新收集和扩充的QA数据中提取。此外，我们分离出了金融QA所需的推理能力：阅读财务文本和表格以提取中间值，理解计算复杂解决方案所需的金融概念和公式。总体而言，这些任务评估了模型的金融背景知识、解析财务文件的能力以及处理复杂解决方案的能力。

    arXiv:2311.06602v2 Announce Type: replace  Abstract: Answering questions within business and finance requires reasoning, precision, and a wide-breadth of technical knowledge. Together, these requirements make this domain difficult for large language models (LLMs). We introduce BizBench, a benchmark for evaluating models' ability to reason about realistic financial problems. BizBench comprises eight quantitative reasoning tasks, focusing on question-answering (QA) over financial data via program synthesis. We include three financially-themed code-generation tasks from newly collected and augmented QA data. Additionally, we isolate the reasoning capabilities required for financial QA: reading comprehension of financial text and tables for extracting intermediate values, and understanding financial concepts and formulas needed to calculate complex solutions. Collectively, these tasks evaluate a model's financial background knowledge, ability to parse financial documents, and capacity to s
    
[^75]: Octavius：通过MoE减轻MLLM中的任务干扰

    Octavius: Mitigating Task Interference in MLLMs via MoE

    [https://arxiv.org/abs/2311.02684](https://arxiv.org/abs/2311.02684)

    提出了一个名为Octavius的新框架，通过结合MoE和LoRA技术设计了一种新颖的LLM解码器LoRA-MoE，用于多模态学习，实验证明其在各种2D和3D下游任务中具有约20%的改进效果。

    

    最近的研究表明，大型语言模型（LLMs）可以通过指导调整将它们的零-shot泛化能力扩展到多模态学习。随着引入更多的形式和下游任务，负面冲突和干扰可能对性能产生更严重的影响。虽然这种现象在以前的工作中被忽视了，但我们提出了一个名为\mname 的新颖且可扩展的框架，用于与Multimodal Large Language Models（MLLMs）一起进行多模态学习的全面研究和实验。具体来说，我们结合了众所周知的专家混合（MoE）和代表性PEFT技术之一，即LoRA，设计了一种新颖的基于LLM的解码器，称为LoRA-MoE，用于多模态学习。实验结果（约20\%的改进）表明了我们设计在各种2D和3D下游任务中的有效性和多功能性。代码和相应数据集将很快提供。

    arXiv:2311.02684v1 Announce Type: cross  Abstract: Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called \mname, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, \emph{i.e.,} LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. The experimental results (about 20\% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and corresponding dataset will be available soon.
    
[^76]: 揭示大语言模型知识编辑的陷阱

    Unveiling the Pitfalls of Knowledge Editing for Large Language Models

    [https://arxiv.org/abs/2310.02129](https://arxiv.org/abs/2310.02129)

    这篇论文探讨了大型语言模型知识编辑的潜在陷阱，提出了新的评估方法，发现知识冲突和知识扭曲是两个重要问题。

    

    随着调整大型语言模型（LLMs）成本不断上升，最近的研究工作已经转向开发编辑LLMs内在知识的方法。然而，仍有一个阴云悬在头顶上 - 知识编辑是否会触发蝴蝶效应？因为目前尚不清楚知识编辑是否会引入可能带来潜在风险的副作用。本文首次探讨了与LLMs知识编辑相关的潜在陷阱。为实现此目的，我们引入了新的基准数据集并提出了创新性的评估指标。我们的结果强调了两个关键问题：（1）知识冲突：编辑逻辑冲突的事实组可能会放大LLMs固有的不一致性 - 这是以前方法忽略的一个方面。（2）知识扭曲：为了编辑事实知识而更改参数可能会不可逆地扭曲

    arXiv:2310.02129v3 Announce Type: replace-cross  Abstract: As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp 
    
[^77]: 大型语言模型中的偏见与公平性：一项调查

    Bias and Fairness in Large Language Models: A Survey

    [https://arxiv.org/abs/2309.00770](https://arxiv.org/abs/2309.00770)

    该论文在大型语言模型领域提出了偏见评估和缓解技术的综合调查，定义了公平性的不同方面，并提出了三个分类体系，以协助研究人员对LLMs进行公平性分析和改进。

    

    大型语言模型（LLMs）的快速发展使得人们能够处理、理解和生成类似人类文本，逐渐融入触及我们社交领域的系统。然而，尽管取得成功，这些模型可能学习、延续和放大有害的社会偏见。本文对LLMs的偏见评估和缓解技术进行了全面调查。我们首先整合、形式化和扩展自然语言处理中社会偏见和公平性的概念，定义了伤害的不同方面，并引入了几个实现LLMs公平性的必要条件。然后，我们通过提出三个直观的分类体系统一了文献，其中包括两个用于偏见评估的分类体系，即指标和数据集，以及一个用于缓解的分类体系。

    arXiv:2309.00770v2 Announce Type: replace-cross  Abstract: Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which t
    
[^78]: MiniLLM：大型语言模型的知识蒸馏

    MiniLLM: Knowledge Distillation of Large Language Models

    [https://arxiv.org/abs/2306.08543](https://arxiv.org/abs/2306.08543)

    本文提出了一种将大型语言模型的知识蒸馏到更小模型的方法，通过使用反向KLD替换标准KD方法中的前向KLD目标，有效避免了学生模型高估教师分布的低概率区域。

    

    知识蒸馏（KD）是一种减少大型语言模型（LLMs）高计算需求的有前途的技术。然而，先前的KD方法主要应用于白盒分类模型或训练小模型来模仿如ChatGPT之类的黑盒模型API。如何有效地将白盒LLMs的知识蒸馏到小模型中仍未得到充分探讨，随着开源LLMs的蓬勃发展，这变得更为重要。在这项工作中，我们提出一种KD方法，将LLMs蒸馏到更小的语言模型。

    arXiv:2306.08543v2 Announce Type: replace-cross  Abstract: Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. The student models are named Mi
    
[^79]: APOLLO: 一种用于长篇数字推理的优化训练方法

    APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning

    [https://arxiv.org/abs/2212.07249](https://arxiv.org/abs/2212.07249)

    APOLLO提出了一种优化的训练方法，通过数字感知的负采样策略和基于一致性的强化学习，提高了长篇数字推理框架的准确性和多样性。

    

    本文针对财务分析中长篇数字推理提出了一种优化训练方法，旨在生成一个推理程序以计算给定问题的正确答案。我们提出了APOLLO来改善长篇数字推理框架，针对相关性选择器，我们采用了数字感知的负采样策略，使其能够更加辨别关键的数字事实。而对于生成器，我们设计了基于一致性的强化学习和目标程序增强策略。

    arXiv:2212.07249v3 Announce Type: replace  Abstract: Long-form numerical reasoning in financial analysis aims to generate a reasoning program to calculate the correct answer for a given question. Previous work followed a retriever-generator framework, where the retriever selects key facts from a long-form document, and the generator generates a reasoning program based on retrieved facts. However, they treated all facts equally without considering the different contributions of facts with and without numbers. Meanwhile, the program consistency were ignored under supervised training, resulting in lower training accuracy and diversity. To solve these problems, we proposed APOLLO to improve the long-form numerical reasoning framework. For the retriever, we adopt a number-aware negative sampling strategy to enable the retriever to be more discriminative on key numerical facts. For the generator, we design consistency-based reinforcement learning and target program augmentation strategy base
    
[^80]: 利用对比样本进行少样本提示学习

    ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning

    [https://arxiv.org/abs/2211.04118](https://arxiv.org/abs/2211.04118)

    提出的Consprompt结合了提示编码网络、对比采样模块和对比评分模块，实现了差异对比学习，在不同少样本设置下表现出最先进的性能，证实了利用多级对比学习在基于提示的微调过程中的有效性。

    

    提示已成为利用预训练语言模型的有效语言工具。然而，在少样本场景中，提示设计中的细微变化总是导致结果差异很大，并且提示学习方法也很容易过拟合有限的样本。为了缓解这一问题，我们探索利用合适的对比样本和多级对比学习方法来改进提示表示的鲁棒性。因此，引入了提出的Consprompt与提示编码网络、对比采样模块和对比评分模块相结合，实现了差异对比学习。我们的结果在不同少样本设置下展现了最先进的性能，消融实验也证明了在基于提示的微调过程中利用多级对比学习的有效性。

    arXiv:2211.04118v3 Announce Type: replace-cross  Abstract: The prompt has become an effective linguistic tool for utilizing pre-trained language models. However, in few-shot scenarios, subtle changes in the prompt design always make the result widely different, and the prompt learning methods also make it easy to overfit the limited samples. To alleviate this, we explore utilizing suitable contrastive samples and multi-degree contrastive learning methods to improve the robustness of the prompt representation. Therefore, the proposed Consprompt combined with the prompt encoding network, contrastive sampling modules, and contrastive scoring modules, is introduced to realize differential contrastive learning. Our results exhibit state-of-the-art performance in different few-shot settings, and the ablation experiments also certify the effectiveness of utilizing multi-degree contrastive learning in the prompt-based fine-tuning process.
    
[^81]: 在处理低资源语言时，机器翻译中称呼错误和性别假设问题的研究

    Misgendering and Assuming Gender in Machine Translation when Working with Low-Resource Languages. (arXiv:2401.13165v1 [cs.CL])

    [http://arxiv.org/abs/2401.13165](http://arxiv.org/abs/2401.13165)

    本章论文研究了低资源语言中机器翻译中的性别相关错误，以孟加拉语为例，讨论了性别的假设和推断，以及这些错误导致的后殖民和社会影响。同时提出了提升语言地位的潜在解决方案。

    

    本章论文针对低资源语言中机器翻译中的性别相关错误进行研究。我们首先解释了低资源语言的概念，并探讨了造成这种语言层级的社会和计算因素的不可分割性。通过对我们的母语孟加拉语进行案例研究，我们展示了当源文本中没有提供相应信息时，性别如何被假设和推断出来，并以高资源英语进行翻译。我们讨论了这些错误导致语言消失和表征伤害的后殖民和社会影响，并讨论了提升语言地位的潜在解决方案，以在机器翻译中赋予语言更多的权威性。

    This chapter focuses on gender-related errors in machine translation (MT) in the context of low-resource languages. We begin by explaining what low-resource languages are, examining the inseparable social and computational factors that create such linguistic hierarchies. We demonstrate through a case study of our mother tongue Bengali, a global language spoken by almost 300 million people but still classified as low-resource, how gender is assumed and inferred in translations to and from the high(est)-resource English when no such information is provided in source texts. We discuss the postcolonial and societal impacts of such errors leading to linguistic erasure and representational harms, and conclude by discussing potential solutions towards uplifting languages by providing them more agency in MT conversations.
    
[^82]: 通过检索示范进行上下文学习的语言模型：一项综述

    In-context Learning with Retrieved Demonstrations for Language Models: A Survey. (arXiv:2401.11624v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.11624](http://arxiv.org/abs/2401.11624)

    本综述调查了一种名为检索示范的方法，它通过使用特定于输入查询的示范来提高语言模型的少量样本情境学习（ICL）能力。这种方法不仅提高了学习效率和可扩展性，还减少了手动示例选择中的偏见。

    

    语言模型，特别是预训练的大型语言模型，已展示出卓越的能力，可以在输入上下文中进行少量样本的情境学习（ICL），并在新任务上具有适应能力。然而，模型的ICL能力对于少样本示范的选择是敏感的。最近的一项研究进展是检索针对每个输入查询定制的示范。示范检索的实现相对简单，利用现有的数据库和检索系统。这不仅提高了学习过程的效率和可扩展性，而且已经证明可以减少手动示例选择中的偏见。鉴于令人鼓舞的结果和在检索示范的ICL方面不断增长的研究，我们进行了广泛的研究综述。在这项综述中，我们讨论和比较了检索模型的不同设计选择，检索训练

    Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training p
    
[^83]: DistilWhisper：通过语言特定专家高效压缩多任务语音模型

    DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts. (arXiv:2311.01070v1 [cs.CL])

    [http://arxiv.org/abs/2311.01070](http://arxiv.org/abs/2311.01070)

    本文提出了DistilWhisper方法，通过使用语言特定专家进行轻量级模块化ASR微调和知识蒸馏，成功弥合了多任务语音模型在少数语言上的性能差距，同时保留了多任务和多语言能力的优势。

    

    Whisper是一个多任务和多语言的语音模型，涵盖99种语言。它在其涵盖的部分语言中获得了令人称赞的自动语音识别（ASR）结果，但在一些数量可观的少数语言中，该模型仍然表现不佳，尤其在较小的模型版本中表现更为严重。在这项工作中，我们提出了DistilWhisper，一种能够在ASR方面弥合这些语言的性能差距，同时保留多任务和多语言能力优势的方法。我们的方法包括两个关键策略：使用语言特定专家对whisper-small进行轻量级模块化ASR微调，并从whisper-large-v2进行知识蒸馏。这种双重方法使我们能够在保持多任务和多语言预训练的鲁棒性的同时有效提升ASR性能。结果表明，我们的方法比标准微调或LoRA适配器更有效，在目标语言中提升了性能。

    Whisper is a multitask and multilingual speech model covering 99 languages. It yields commendable automatic speech recognition (ASR) results in a subset of its covered languages, but the model still under-performs on a non-negligible number of under-represented languages, a problem exacerbated in smaller model versions. In this work, we propose DistilWhisper, an approach able to bridge the performance gap in ASR for these languages while retaining the advantages of multitask and multilingual capabilities. Our approach involves two key strategies: lightweight modular ASR fine-tuning of whisper-small using language-specific experts, and knowledge distillation from whisper-large-v2. This dual approach allows us to effectively boost ASR performance while keeping the robustness inherited from the multitask and multilingual pre-training. Results demonstrate that our approach is more effective than standard fine-tuning or LoRA adapters, boosting performance in the targeted languages for both 
    
[^84]: 退后一步：通过抽象唤起大型语言模型的推理能力

    Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. (arXiv:2310.06117v1 [cs.LG])

    [http://arxiv.org/abs/2310.06117](http://arxiv.org/abs/2310.06117)

    本文提出了一种简单的提示技术，使得大型语言模型能够通过抽象获得高层概念和基本原理，并将其应用于推理路径中，从而显著提升模型在各种推理密集型任务上的表现。

    

    我们提出了一种称为“退后提示”的简单提示技术，使得大型语言模型能够通过从包含具体细节的实例中进行抽象，得出高层概念和基本原理。利用这些概念和原理来指导推理步骤，语言模型在正确推理路径上显著提升了能力。我们使用PaLM-2L模型进行了退后提示实验，在包括STEM、知识问答和多跳推理在内的各种具有挑战性的推理密集型任务上观察到了明显的性能提升。例如，在MMLU物理和化学任务上，退后提示可以将PaLM-2L的性能提升7%和11%，在TimeQA任务上提升27%，在MuSiQue任务上提升7%。

    We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.
    
[^85]: TRAM: 桥接信任区域和锐度感知最小化

    TRAM: Bridging Trust Regions and Sharpness Aware Minimization. (arXiv:2310.03646v1 [cs.LG])

    [http://arxiv.org/abs/2310.03646](http://arxiv.org/abs/2310.03646)

    TRAM是一种桥接信任区域和锐度感知最小化的算法，通过减少损失曲面的曲率来提供鲁棒性改进。它通过在微调过程中优化可转移的表示来实现领域外泛化，并且通过结合信任区域方法和SAM风格的正则化器来统一参数和表示空间平滑方法。TRAM在保持预训练结构的同时实现了平坦的极小值和平滑、有信息量的表示。

    

    通过减少参数空间中损失曲面的曲率，锐度感知最小化（SAM）在领域转移下提供了广泛的鲁棒性改进。然而，本文不是关注参数，而是考虑到表示的可转移性作为优化目标，在微调设置中实现领域外泛化。为了鼓励保留可转移的表示，我们考虑到基于信任区域的微调方法，这些方法利用任务特定的技能，而不会忘记预训练的任务无关表示。我们通过使用信任区域边界在这两种优化表面上通知SAM风格的正则化器，统一了参数和表示空间平滑方法。我们提出了Trust Region Aware Minimization (TRAM)，一种优化平坦的极小值和平滑、有信息量的表示的微调算法，而不会忘记预先训练的结构。我们发现，TRAM优于锐度感知和基于信任区域的方法。

    By reducing the curvature of the loss surface in the parameter space, Sharpness-aware minimization (SAM) yields widespread robustness improvement under domain transfer. Instead of focusing on parameters, however, this work considers the transferability of representations as the optimization target for out-of-domain generalization in a fine-tuning setup. To encourage the retention of transferable representations, we consider trust region-based fine-tuning methods, which exploit task-specific skills without forgetting task-agnostic representations from pre-training. We unify parameter- and representation-space smoothing approaches by using trust region bounds to inform SAM-style regularizers on both of these optimization surfaces. We propose Trust Region Aware Minimization (TRAM), a fine-tuning algorithm that optimizes for flat minima and smooth, informative representations without forgetting pre-trained structure. We find that TRAM outperforms both sharpness-aware and trust region-based
    
[^86]: 通过利用层间变换的平滑性进行带外分布检测

    Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness. (arXiv:2310.02832v1 [cs.LG])

    [http://arxiv.org/abs/2310.02832](http://arxiv.org/abs/2310.02832)

    本文提出了一种通过利用神经网络中间层变换的平滑性来检测带外数据的方法(BLOOD),该方法适用于没有训练数据访问权限的预训练模型，并在Transformer网络上的文本分类任务中取得了良好的效果。

    

    有效的带外分布检测对于可靠的机器学习模型至关重要，然而大多数当前方法由于需要访问训练数据或者干预训练而在实际应用中受到限制。我们提出了一种新的方法，通过网络中间层的变换平滑性来检测深度神经网络中的带外数据（BLOOD），该方法适用于没有训练数据访问权限的预训练模型。BLOOD利用内分布（ID）数据的层间表示变换相较于带外数据的变换更平滑的倾向，这也是我们在Transformer网络中经验证明的一个特性。我们在几个文本分类任务上评估了BLOOD与Transformer网络，并证明其在资源需求相当的方法上性能更好。我们的分析还表明，当学习更简单的任务时，带外数据的变换会保持其原始的锐度，而锐度会随着任务的增加而增加。

    Effective OOD detection is crucial for reliable machine learning models, yet most current methods are limited in practical use due to requirements like access to training data or intervention in training. We present a novel method for detecting OOD data in deep neural networks based on transformation smoothness between intermediate layers of a network (BLOOD), which is applicable to pre-trained models without access to training data. BLOOD utilizes the tendency of between-layer representation transformations of in-distribution (ID) data to be smoother than the corresponding transformations of OOD data, a property that we also demonstrate empirically for Transformer networks. We evaluate BLOOD on several text classification tasks with Transformer networks and demonstrate that it outperforms methods with comparable resource requirements. Our analysis also suggests that when learning simpler tasks, OOD data transformations maintain their original sharpness, whereas sharpness increases wit
    
[^87]: 基于递归组合多粒度表示的Transformer增强模型

    Augmenting transformers with recursively composed multi-grained representations. (arXiv:2309.16319v1 [cs.CL])

    [http://arxiv.org/abs/2309.16319](http://arxiv.org/abs/2309.16319)

    ReCAT是一种增强的Transformer模型，使用递归组合和上下文内外层能够模拟文本的层级句法结构，并生成与其他跨度上下文相关的多粒度表示。

    

    我们提出了一种名为ReCAT的递归组合增强Transformer模型，它能够在学习和推理过程中明确建模原始文本的层级句法结构，而无需依赖于黄金树。现有研究限制数据遵循层级树结构，因此缺乏跨距通信。为了克服这个问题，我们提出了一种新颖的上下文内外(CIO)层，通过自底向上和自顶向下的传递学习跨度的上下文化表示，其中自底向上传递通过组合低级跨度形成高级跨度的表示，而自顶向下传递则结合了跨度内部和外部的信息。通过在嵌入层和注意力层之间叠加多个CIO层，ReCAT模型可以进行跨距内部和跨距间的深层交互，从而生成与其他跨度完全上下文化的多粒度表示。此外，CIO层可以进行联合预训练。

    We present ReCAT, a recursive composition augmented Transformer that is able to explicitly model hierarchical syntactic structures of raw texts without relying on gold trees during both learning and inference. Existing research along this line restricts data to follow a hierarchical tree structure and thus lacks inter-span communications. To overcome the problem, we propose a novel contextual inside-outside (CIO) layer that learns contextualized representations of spans through bottom-up and top-down passes, where a bottom-up pass forms representations of high-level spans by composing low-level spans, while a top-down pass combines information inside and outside a span. By stacking several CIO layers between the embedding layer and the attention layers in Transformer, the ReCAT model can perform both deep intra-span and deep inter-span interactions, and thus generate multi-grained representations fully contextualized with other spans. Moreover, the CIO layers can be jointly pre-trained
    
[^88]: InstructERC：借助检索多任务LLMs框架改革对话中的情绪识别

    InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework. (arXiv:2309.11911v1 [cs.CL])

    [http://arxiv.org/abs/2309.11911](http://arxiv.org/abs/2309.11911)

    InstructERC是一种使用大型语言模型(LLMs)的生成式框架，通过引入检索模板模块和额外的情感对齐任务，改革了对话中的情绪识别。

    

    对话情绪识别(ERC)的发展一直受到管道设计复杂性的阻碍，导致ERC模型往往对特定数据集和对话模式过拟合。在本研究中，我们提出了一种新方法，即InstructERC，将ERC任务从判别式框架转化为基于大型语言模型(LLMs)的生成式框架。InstructERC有两个重要贡献：首先，InstructERC引入了一个简单而有效的检索模板模块，通过将历史对话内容、标签语句和情感领域演示与高语义相似性进行拼接，帮助模型明确地集成多粒度对话监督信息。此外，我们引入了两个额外的情感对齐任务，即说话人识别和情感预测任务，以隐式地建模对话角色关系和未来对话情绪倾向。我们的基于LLM的方法

    The development of emotion recognition in dialogue (ERC) has been consistently hindered by the complexity of pipeline designs, leading to ERC models that often overfit to specific datasets and dialogue patterns. In this study, we propose a novel approach, namely  InstructERC, to reformulates the ERC task from a discriminative framework to a generative framework based on Large Language Models (LLMs) . InstructERC has two significant contributions: Firstly, InstructERC introduces a simple yet effective retrieval template module, which helps the model explicitly integrate multi-granularity dialogue supervision information by concatenating the historical dialog content, label statement, and emotional domain demonstrations with high semantic similarity. Furthermore, we introduce two additional emotion alignment tasks, namely speaker identification and emotion prediction tasks, to implicitly model the dialogue role relationships and future emotional tendencies in conversations. Our LLM-based
    
[^89]: MINT: 评估在与工具和语言反馈进行多轮交互中的LLMs的能力

    MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback. (arXiv:2309.10691v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.10691](http://arxiv.org/abs/2309.10691)

    MINT是一个评估LLMs在多轮交互中解决任务能力的基准，通过使用工具和利用用户的自然语言反馈。它解决了当前评估协议忽略细致互动和低估自然语言反馈的问题，促进了研究基准评估和实际应用之间的一致性。

    

    为了解决复杂任务，大语言模型（LLMs）通常需要与用户进行多轮交互，有时候辅以外部工具的帮助。然而，当前的评估协议常常强调用单轮交流的基准性能，忽略了用户、LLMs和外部工具之间的细致互动，并低估了用户的自然语言反馈的重要性。这些疏忽导致了研究基准评估结果与实际应用情况之间的差异。我们引入了MINT，这是一个通过使用工具和利用用户的自然语言反馈来评估LLMs解决多轮交互任务能力的基准。为了保证可重复性，我们提供了一个评估框架，在这个框架中，LLMs可以通过执行Python代码来访问工具，并接收由GPT-4模拟的用户的自然语言反馈。我们重新利用了一系列多样的已建立评估数据集，重点关注推理、编码和决策方面。

    To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases. We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4. We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and careful
    
[^90]: 基于大型语言模型的自主代理的调查

    A Survey on Large Language Model based Autonomous Agents. (arXiv:2308.11432v1 [cs.AI])

    [http://arxiv.org/abs/2308.11432](http://arxiv.org/abs/2308.11432)

    该论文综述了基于大型语言模型的自主代理的研究，提供了从整体角度对该领域的系统审查，其创新之处在于利用大量网络知识实现人类水平的智能决策。

    

    自主代理长期以来一直是学术界的研究热点。以往的研究往往集中在对有限知识的代理进行训练，而这与人类的学习过程存在明显差异，因此很难实现人类般的决策。近年来，通过获取大量的网络知识，大型语言模型（LLM）展现出了实现人类水平智能的显著潜力。这引发了对基于LLM的自主代理的研究的高涨兴趣。为了发挥LLM的全部潜力，研究人员设计了各种不同应用的代理体系结构。本论文综述了这些研究，从整体的角度对自主代理领域进行了系统的审查。具体而言，我们的重点是基于LLM的代理构建，为此我们提出了一个统一的框架。

    Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework t
    
[^91]: 使用指令反向翻译的自动对齐方法

    Self-Alignment with Instruction Backtranslation. (arXiv:2308.06259v1 [cs.CL])

    [http://arxiv.org/abs/2308.06259](http://arxiv.org/abs/2308.06259)

    本论文提出了一种自动对齐方法，通过为人工编写的文本添加指令标签来构建高质量的指令跟踪语言模型。该方法通过自我增强和自我筛选生成训练示例，并且在Alpaca排行榜上表现出非常高效的自动对齐能力。

    

    我们提出了一种可扩展的方法，通过自动为人工编写的文本添加相应的指令标签来构建高质量的指令跟踪语言模型。我们的方法名为指令反向翻译，它从在少量种子数据和给定的网络语料库上微调的语言模型开始。种子模型用于通过为网络文档生成指令提示（自我增强）来构建训练示例，然后从这些候选示例中选择高质量的示例（自我筛选）。然后使用这些数据来微调更强的模型。通过使用我们方法的两次迭代来微调LLaMa，我们得到的模型在Alpaca排行榜上击败了所有其他基于LLaMa的模型，而无需依赖蒸馏数据，展示了非常有效的自动对齐能力。

    We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.
    
[^92]: 过度思考真相：理解语言模型如何处理虚假演示

    Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])

    [http://arxiv.org/abs/2307.09476](http://arxiv.org/abs/2307.09476)

    该论文研究了现代语言模型在处理虚假演示时出现的过度思考和错误归纳头现象。通过研究模型的内部表示，发现模型在中间层之后对错误演示的处理准确性逐渐降低，并指出了错误归纳头机制可能导致过度思考现象。

    

    现代语言模型可以通过少量示范进行复杂模式的模仿学习，使其能够在没有微调的情况下完成具有挑战性的任务。然而，模仿也可能导致模型在上下文中重现不准确或有害的内容。我们通过模型的内部表示来研究有害的模仿，并确定了两个相关现象：过度思考和错误归纳头。第一个现象，过度思考，在给出正确与错误的少量示范时，我们从中间层解码预测。在早期层中，两种示范引起了相似的模型行为，但在某个“关键层”之后，给出错误示范的准确性逐渐降低。第二个现象，错误归纳头，可能是过度思考的一种机制性原因：这些是位于较晚层的头部，它们关注并复制先前示范中的错误信息，其削弱会减少过度思考现象。

    Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces 
    
[^93]: ChatGPT是一个知识渊博但经验不足的问题求解器：对大型语言模型中常识问题的调查研究。

    ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models. (arXiv:2303.16421v1 [cs.CL])

    [http://arxiv.org/abs/2303.16421](http://arxiv.org/abs/2303.16421)

    ChatGPT是一个知识渊博但经验不足的LLM，能够回答常识问题，但在某些类型问题上仍存在困难。

    

    大型语言模型（LLMs），如ChatGPT和GPT-4，在NLP方面取得了重大进展。然而，它们记忆、表达和利用常识知识的能力一直是LLMs的一个众所周知的痛点。目前仍不清楚以下几点：（1）GPT能否有效回答常识问题？（2）GPT对常识知识是否精通？（3）GPT是否了解用于回答特定问题的底层常识知识？（4）GPT能否有效利用常识回答问题？为了评估以上常识问题，我们进行了一系列实验来评估ChatGPT的常识能力，实验结果表明：(1) GPT在常识任务中能够获得良好的问答准确性，但仍然无法解决某些类型的问题。(2) ChatGPT具有学识渊博，可以使用知识提示准确地产生大部分常识知识。(3)尽管具有知识，ChatGPT是一个缺乏经验的常识问题求解器，无法有效地利用常识知识回答某些问题。

    Large language models (LLMs) such as ChatGPT and GPT-4 have made significant progress in NLP. However, their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point for LLMs. It remains unclear that: (1) Can GPTs effectively answer commonsense questions? (2) Are GPTs knowledgeable in commonsense? (3) Are GPTs aware of the underlying commonsense knowledge for answering a specific question? (4) Can GPTs effectively leverage commonsense for answering questions? To evaluate the above commonsense problems, we conduct a series of experiments to evaluate ChatGPT's commonsense abilities, and the experimental results show that: (1) GPTs can achieve good QA accuracy in commonsense tasks, while they still struggle with certain types of knowledge. (2) ChatGPT is knowledgeable, and can accurately generate most of the commonsense knowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an inexperienced commonsense problem solver, which cann
    
[^94]: 语言特定的情绪概念知识表示对情绪推断的因果支持

    Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference. (arXiv:2302.09582v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.09582](http://arxiv.org/abs/2302.09582)

    本研究通过操纵大型语言模型中的语言衍生的情绪概念知识表示，探讨了语言是否会因果支持情绪推断。实验结果显示，属性特定的神经元操纵导致情绪推断任务的性能下降，这与人类心理空间中不同属性的重要性有关。这些发现为支持基于语言的情绪推断机制提供了因果证据，并凸显了情绪概念知识的贡献。

    

    在情绪科学中，如何理解语言支持情绪推断仍然是一个争议的话题。本研究通过操纵大型语言模型中的语言衍生的情绪概念知识表示，调查了语言是否会因果支持情绪推断。使用提示技术，发现了14个情绪概念的属性由不同的人工神经元群体表示。通过操纵这些属性相关的神经元，与随机操纵相比，大多数情绪推断任务的表现出现了下降。属性特定的表现下降与人类心理空间中不同属性的重要性有关。我们的发现提供了支持基于语言的情绪推断机制的因果证据，并强调了情绪概念知识的贡献。

    Understanding how language supports emotion inference remains a topic of debate in emotion science. The present study investigated whether language-derived emotion-concept knowledge would causally support emotion inference by manipulating the language-specific knowledge representations in large language models. Using the prompt technique, 14 attributes of emotion concepts were found to be represented by distinct artificial neuron populations. By manipulating these attribute-related neurons, the majority of the emotion inference tasks showed performance deterioration compared to random manipulations. The attribute-specific performance deterioration was related to the importance of different attributes in human mental space. Our findings provide causal evidence in support of a language-based mechanism for emotion inference and highlight the contributions of emotion-concept knowledge.
    

