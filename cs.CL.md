# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [COVID-19 South African Vaccine Hesitancy Models Show Boost in Performance Upon Fine-Tuning on M-pox Tweets.](http://arxiv.org/abs/2310.04453) | 通过在M-pox数据集上进行微调，COVID-19模型在南非疫苗犹豫方面的性能得到了提高。 |
| [^2] | [Short text classification with machine learning in the social sciences: The case of climate change on Twitter.](http://arxiv.org/abs/2310.04452) | 本文研究了社会科学中的机器学习短文本分类问题，并对不同的分类器进行了性能比较。以Twitter上的气候变化话题为例，使用了一个包含5750个推文的新数据集进行评估。 |
| [^3] | [AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models.](http://arxiv.org/abs/2310.04451) | 本文介绍了一种名为AutoDAN的方法，该方法旨在在对齐的大型语言模型上自动生成隐蔽的越狱提示，以解决现有越狱技术的可扩展性和隐蔽性问题。 |
| [^4] | [Investigating Large Language Models' Perception of Emotion Using Appraisal Theory.](http://arxiv.org/abs/2310.04450) | 本研究通过使用评估理论和应激与应对过程问卷（SCPQ）来调查大型语言模型对情感的感知。结果表明，模型的响应在评估和应对的动态方面与人类类似，但在关键评估维度上与预测的不一致。 |
| [^5] | [LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model.](http://arxiv.org/abs/2310.04445) | 本文提出了一种名为LoFT的方法，通过在与有害查询处于词汇-语义邻域的相似查询上进行代理模型的微调，来改善对大型语言模型的对抗攻击的可传递性。 |
| [^6] | [What's the Magic Word? A Control Theory of LLM Prompting.](http://arxiv.org/abs/2310.04444) | 本论文将提示工程形式化为LLM上的最优控制问题，研究了给定token序列时是否存在一种最优提示能够准确预测最终的token，并提出了控制理论中的指标来描述LLM的可操纵性。 |
| [^7] | [Human Mobility Question Answering (Vision Paper).](http://arxiv.org/abs/2310.04443) | 本文提出了一项新的任务，即人类移动问题回答（MobQA），旨在让智能系统从移动数据中学习并回答相关问题，填补了关于利用人类移动数据进行问题回答系统的研究空白，并为移动推荐系统的研究带来了新的范式变革。 |
| [^8] | [A Brief History of Prompt: Leveraging Language Models.](http://arxiv.org/abs/2310.04438) | 这篇论文全面探讨了提示工程和生成在自然语言处理领域的演进历程，包括早期语言模型和信息检索系统，注意力机制的引入，强化学习技术的应用以解决偏见和暴露偏差等问题。还讨论了微调策略、控制代码和基于模板的生成的重大贡献，以及公平性、人工智能与人类合作和低资源适应的重要性。 |
| [^9] | [Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models.](http://arxiv.org/abs/2310.03965) | 提出了思维传播（TP）方法，通过探索类比问题和利用类比问题的解决方案来增强大型语言模型的复杂推理能力。 |
| [^10] | [Evaluating Self-Supervised Speech Representations for Indigenous American Languages.](http://arxiv.org/abs/2310.03639) | 本论文评估了自监督语音表示在美洲土著语言上的应用。研究结果表明，大型自监督学习模型在低资源ASR任务中展示出了令人惊讶的强大性能，显示了这些模型在真实世界数据上的泛化能力。 |
| [^11] | [Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning.](http://arxiv.org/abs/2310.03094) | 本研究提出了一种基于思维混合表示的大规模语言模型级联方法，用于成本高效的推理。通过考虑更弱模型的答案一致性作为问题难度的信号，可以实现对问题的决策，从而节约使用更强模型的成本。 |
| [^12] | [Large Language Models Can Be Good Privacy Protection Learners.](http://arxiv.org/abs/2310.02469) | 本论文介绍了一种名为隐私保护语言模型（PPLM）的新范式，可以在保护数据隐私的同时有效注入领域特定知识。通过对模型设计的理论分析和不同技术的研究，我们验证了使用正向和负向示例进行指令微调的方法具有很大的潜力。 |
| [^13] | [Extraction of Medication and Temporal Relation from Clinical Text using Neural Language Models.](http://arxiv.org/abs/2310.02229) | 本研究使用深度学习和大型语言模型，通过名为MedTem的临床领域实体识别和时间关系提取，来提取临床文本中的药物和时间关系，以帮助临床医生更好地了解患者的治疗历史。 |
| [^14] | [Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs.](http://arxiv.org/abs/2310.01801) | 该研究提出了一种自适应的KV缓存压缩方法，用于减少大型语言模型的内存消耗。通过有针对性的分析和结构识别，构建了具有自适应性的KV缓存，通过清除和丢弃特定的上下文，以及只对特定的注意力头使用标准KV缓存，实现了显著的内存占用减少。 |
| [^15] | [Split and Merge: Aligning Position Biases in Large Language Model based Evaluators.](http://arxiv.org/abs/2310.01432) | PORTIA是一个旨在校准大型语言模型评估器的位置偏差的对齐系统，通过将答案分割成多个片段，并对其进行对齐，然后将其合并回一个单一的提示，以提高评估的准确性和公正性。 |
| [^16] | [RA-DIT: Retrieval-Augmented Dual Instruction Tuning.](http://arxiv.org/abs/2310.01352) | 本论文介绍了一种轻量级的微调方法RA-DIT，通过为任何语言模型添加检索能力来提高性能。该方法分为两个步骤：一是更新语言模型以更好地利用检索到的信息，二是更新检索器以返回更相关的结果。实验证明每个步骤都能显著提高性能，同时使用两个步骤可以获得额外的收益。 |
| [^17] | [Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models.](http://arxiv.org/abs/2310.01074) | 本文提出了一个新的任务——可解释的时间推理，旨在预测未来时间戳上事件的发生，需要进行多步推理和多个事件的综合。 |
| [^18] | [GeRA: Label-Efficient Geometrically Regularized Alignment.](http://arxiv.org/abs/2310.00672) | GeRA是一种标签效率的几何正则化对齐方法，利用未配对数据的流形几何提高了预训练单模态编码器的对齐性能。 |
| [^19] | [SELF: Language-Driven Self-Evolution for Large Language Model.](http://arxiv.org/abs/2310.00533) | SELF提出了一种基于语言驱动的创新方法，允许大型语言模型（LLM）不断自我进化，并通过语言反馈作为评估工具来改进模型的响应能力和训练稳定性。 |
| [^20] | [Measuring Value Understanding in Language Models through Discriminator-Critique Gap.](http://arxiv.org/abs/2310.00378) | 通过鉴别-批判差距测量LLMs对人类价值的理解，我们提出了价值理解测量（VUM）框架，并使用GPT-4开发了一个包含一千个对话的数据集。我们的评估结果显示，尺度定律对LLMs的“知道什么”有较大影响，而对“知道为什么”影响较小。 |
| [^21] | [RelBERT: Embedding Relations with Language Models.](http://arxiv.org/abs/2310.00299) | RelBERT是一种能够从相对较小的语言模型中提取关系嵌入的方法，通过微调RoBERTa模型，只需少量训练数据，即可捕捉关系相似性，并在类比基准中取得最新的最佳表现。 |
| [^22] | [Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment.](http://arxiv.org/abs/2310.00212) | 该论文提出了一种新的强化学习框架，使用相对反馈来调整大型语言模型（LLMs）的行为，解决了现有方法在优化比较损失训练的奖励时存在的限制。同时，还提出了一种新的基于轨迹的策略梯度算法（PPPO），用于更有效地进行算法设计和函数逼近。 |
| [^23] | [A Survey on Image-text Multimodal Models.](http://arxiv.org/abs/2309.15857) | 图像-文本多模型综述论文全面回顾了其发展历程和当前状态，提出了新的分类方法，并阐明了该领域的挑战和潜在研究方向。 |
| [^24] | [Lyra: Orchestrating Dual Correction in Automated Theorem Proving.](http://arxiv.org/abs/2309.15806) | Lyra是一种新的框架，通过引入工具修正和猜想修正两种机制，增强了大规模语言模型在形式化定理证明领域的有效性，减轻了幻觉，并提高了证明的准确性。 |
| [^25] | [NLPBench: Evaluating Large Language Models on Solving NLP Problems.](http://arxiv.org/abs/2309.15630) | NLPBench是一个评估大型语言模型解决NLP问题的基准数据集，为填补该领域的研究空白，作者收集了来自耶鲁大学期末考试的378个涵盖多个NLP主题的问题。该研究发现在使用高级提示策略时，大型语言模型的性能可能不稳定，并可能对较小的模型造成负面影响。 |
| [^26] | [QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models.](http://arxiv.org/abs/2309.14717) | 本文提出了QA-LoRA算法，它通过使用量化意识以及组内运算符来实现大语言模型的低秩适应。QA-LoRA能够将模型权重量化以减少时间和内存的使用，同时在不损失准确性的情况下将模型集成为一个量化模型。 |
| [^27] | [Probing the Moral Development of Large Language Models through Defining Issues Test.](http://arxiv.org/abs/2309.13356) | 通过定义问题测试测量LLMs的道德推理能力，早期模型表现不佳，而ChatGPT、Llama2-Chat、PaLM-2和GPT-4在这方面表现出色，与成年人相当。然而，这些模型在不同困境下的表现存在差异。 |
| [^28] | [MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models.](http://arxiv.org/abs/2309.12284) | MetaMath是一种专门用于数学推理的微调语言模型，通过从多个角度重新编写问题来生成数学问题，并在两个基准测试中取得了优于其他开源语言模型的表现。 |
| [^29] | [Agents: An Open-source Framework for Autonomous Language Agents.](http://arxiv.org/abs/2309.07870) | Agents是一个开源框架，支持构建自主语言代理的各种功能，并提供用户友好的接口和对研究人员的扩展性。 |
| [^30] | [RAIN: Your Language Models Can Align Themselves without Finetuning.](http://arxiv.org/abs/2309.07124) | 本研究提出了RAIN方法，该方法可以在无需微调或额外数据的情况下，通过整合自我评估和回滚机制实现对齐冻结的语言模型，使其能够直接产生与人类偏好一致的响应。 |
| [^31] | [Meta predictive learning model of natural languages.](http://arxiv.org/abs/2309.04106) | 该论文提出了一种基于预测编码框架的均场学习模型，通过假设突触权重遵循脉冲和斑点分布并只对分布进行训练，成功地应用于手写数字分类。 |
| [^32] | [Zero-Resource Hallucination Prevention for Large Language Models.](http://arxiv.org/abs/2309.02654) | 本论文提出了一种零资源幻觉预防方法，通过评估模型对输入指令中概念的熟悉程度，在遇到不熟悉的概念时不生成响应，从而解决了大型语言模型中的幻觉问题。 |
| [^33] | [Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation.](http://arxiv.org/abs/2309.02459) | 本文提出了一种通过下采样声学表示来对齐文本模态的方法，以实现纯文本领域自适应的端到端语音识别。实验结果表明，该方法在新领域数据上取得了良好的效果。 |
| [^34] | [Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!.](http://arxiv.org/abs/2309.02110) | Wordle是一款流行的在线单词游戏，玩家需要在6次猜测中猜出每日目标单词。通过收集玩家的数据，研究者发现每天约有0.2-0.5%的玩家能在一次尝试中解决谜题，展示了玩家们的技巧和运气。 |
| [^35] | [SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge.](http://arxiv.org/abs/2309.01437) | SememeASR是一种基于语义知识的语音识别方法，通过引入语义单元——sememe的信息，可以提高语音识别的效果，并且改善对长尾数据的识别和领域泛化能力。 |
| [^36] | [SpikeBERT: A Language Spikformer Trained with Two-Stage Knowledge Distillation from BERT.](http://arxiv.org/abs/2308.15122) | 该论文提出了一种名为SpikeBERT的SNN模型，通过改进Spikformer架构和使用两阶段知识蒸馏方法，该模型在语言任务上超越了其他SNN模型，在文本分类任务上甚至达到了与BERT相当的结果。 |
| [^37] | [Large Language Model as a User Simulator.](http://arxiv.org/abs/2308.11534) | 本文创新性地将从真实人机对话中提取的人类问题作为学习目标，并且训练了一个用户模拟器UserGPT，并使用生成的高质量合成对话数据集RealChat来训练助手模型ReaLM。实验证明，ReaLM在多个基准测试中超过了基准模型。 |
| [^38] | [AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors.](http://arxiv.org/abs/2308.10848) | AgentVerse提出了一个多智能体框架，可以通过协同和动态调整合作团队的组成，实现超越单个智能体的性能。在合作任务中，该框架能够引发出群体内个体智能体之间的社会行为，从而提高多智能体团队的协作潜力。 |
| [^39] | [Instruction Tuning for Large Language Models: A Survey.](http://arxiv.org/abs/2308.10792) | 本文调查了指令调优这一关键技术在增强大型语言模型能力和可控性方面的研究工作，包括方法、数据集构建、模型训练和应用，以及对结果影响的分析。同时回顾了可能的问题和批评，并指出了目前的不足。 |
| [^40] | [Extrapolating Large Language Models to Non-English by Aligning Languages.](http://arxiv.org/abs/2308.04948) | 本文介绍了一种通过语义对齐来增强非英语语言模型的方法，并通过实验证明，该方法在跨语言任务中取得了显著的改进。同时，我们还发现在翻译数据中加入非英语文本可以有效提升模型能力，且LLM内部的语义对齐也可以进一步加强。 |
| [^41] | [More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes.](http://arxiv.org/abs/2308.01313) | 本文借鉴了人类视觉感知过程，提出了一种通过推断和调节上下文属性来改进零样本图像分类的方法。通过给CLIP提供上下文属性，可以减轻对虚假特征的依赖，进而提高零样本分类的准确性。 |
| [^42] | [A Private Watermark for Large Language Models.](http://arxiv.org/abs/2307.16230) | 这项工作提出了一种私密水印算法，通过使用两个不同的神经网络进行水印生成和检测，并共享部分参数，实现了高效且高准确性的检测，同时对生成和检测速度影响最小。 |
| [^43] | [Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding.](http://arxiv.org/abs/2307.15337) | 本研究提出了一种名为“思维的骨架”的方法，可以通过并行解码来减少大型语言模型的生成延迟。这种方法不仅显著提高了速度，还可以潜在地提高答案质量。 |
| [^44] | [Divide & Bind Your Attention for Improved Generative Semantic Nursing.](http://arxiv.org/abs/2307.10864) | 本论文提出了一种名为"分割与绑定"的方法，旨在改进生成语义护理的效果。该方法引入了新的损失目标，包括关注丢失和绑定丢失，以解决复杂提示和不适当属性绑定的问题。 |
| [^45] | [Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models.](http://arxiv.org/abs/2307.01379) | 本论文研究了大型语言模型（LLMs）自动生成的关键词不平等问题，发现在估计不确定性时，重要的令牌和含有有限语义的句子被同等或更加重视。为了解决这个问题，提出了共同转移关注点来更好地估计不确定性。 |
| [^46] | [ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading.](http://arxiv.org/abs/2307.00782) | 本研究提出了一种轻量级但有效的文本转语音系统ContextSpeech，通过设计内存缓存的循环机制和构建层次化的文本语义结构，将全局文本和语音上下文融入到句子编码中，以解决段落阅读中的语音生成挑战，并且使用线性化的自注意力机制提高了模型的效率。 |
| [^47] | [RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19 Assessment in Primary Care.](http://arxiv.org/abs/2306.17175) | 本研究提出了一个从原始GP笔记中提取信息并构建知识图谱的框架，用于解决临床决策过程中现有技术无法处理的问题。 |
| [^48] | [The Art of Embedding Fusion: Optimizing Hate Speech Detection.](http://arxiv.org/abs/2306.14939) | 这项工作研究了优化仇恨言论检测的方法。研究发现，尽管嵌入的组合会略微提高性能，但计算成本很高，并且组合方式对结果的影响较小。 |
| [^49] | [A novel Counterfactual method for aspect-based sentiment analysis.](http://arxiv.org/abs/2306.11260) | 本文提出了一种新颖的基于对照的数据增强方法，用于生成具有相反情感极性的观点表达，并利用T5模型检索掩码，该方法在三个ABSA数据集上表现优于当前增强方法。 |
| [^50] | [Pushing the Limits of ChatGPT on NLP Tasks.](http://arxiv.org/abs/2306.09719) | 本研究提出了一系列通用模块以解决 ChatGPT 在自然语言处理任务中的弱点，包括利用多个提示符来适应更多演示、使用精细调整模型以获得更好的演示检索、转换任务为更适合生成性质的格式以及采用针对 NLP 任务设计的推理策略。 |
| [^51] | [PEACE: Cross-Platform Hate Speech Detection- A Causality-guided Framework.](http://arxiv.org/abs/2306.08804) | PEACE是一个跨平台仇恨言论检测的因果指导框架，通过学习源平台数据并推广到目标平台，探索如何建立适用于不同平台的通用仇恨言论检测模型。 |
| [^52] | [Operationalising Representation in Natural Language Processing.](http://arxiv.org/abs/2306.08193) | 本文介绍了一个框架，通过使用探测分类器来评估组件是否表示属性，填补了自然语言处理中关于“表示”的哲学空白。 |
| [^53] | [Valley: Video Assistant with Large Language model Enhanced abilitY.](http://arxiv.org/abs/2306.07207) | 本文介绍了一个名为Valley的视频助手，它是一个以大型语言模型增强的多模态基础模型，能够在一个通用框架内理解视频、图像和语言。 |
| [^54] | [Adapting Pretrained ASR Models to Low-resource Clinical Speech using Epistemic Uncertainty-based Data Selection.](http://arxiv.org/abs/2306.02105) | 本研究使用基于认识不确定性的数据选择方法来减少非洲口音临床ASR训练的注释成本。结果表明，这种方法可以超过现有的基准结果，并提高低资源口音的泛化能力。 |
| [^55] | [Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models.](http://arxiv.org/abs/2305.19187) | 本研究提出应对大型语言模型可信度问题的方法，研究黑盒模型中置信度与不确定性量化，并将其应用于选择性自然语言生成。 |
| [^56] | [Multitask learning for recognizing stress and depression in social media.](http://arxiv.org/abs/2305.18907) | 该论文通过利用社交媒体的数据，提出了一种新的多任务学习方法来早期识别压力和抑郁。研究人员可以使用不同的数据集作为主任务和辅助任务，以便更好地理解这两种情绪状态。 |
| [^57] | [Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models.](http://arxiv.org/abs/2305.18507) | 本文介绍了一种神经符号提示方法——代码提示，该方法可以触发代码作为中间步骤。与自然语言相比，代码提示有着几个独特优势，能够提高符号推理和算术推理的性能，并且通常优于思路链提示。 |
| [^58] | [VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset.](http://arxiv.org/abs/2305.18500) | 本文提出了一种全模态视频文本基础模型VAST及其数据集VAST-27M。该模型可以感知和处理视频中的视觉、音频和字幕模态，通过自动集成多模态字幕和资源，提供更好的支持文本关联的多种任务。 |
| [^59] | [Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning.](http://arxiv.org/abs/2305.16646) | 本文提出了一个建模和预测框架，在少样本情况下使用语言模型的绝对推理能力来提高事件序列模型的预测精度，经过实验证实可以明显优于最先进的事件序列模型。 |
| [^60] | [SummIt: Iterative Text Summarization via ChatGPT.](http://arxiv.org/abs/2305.14835) | 本文提出了SummIt，一个基于ChatGPT的迭代文本摘要框架，通过自我评估和反馈，模型能够迭代地完善生成的摘要，解决了一次性生成摘要可能存在的虚构和遗漏关键细节的问题。 |
| [^61] | [UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning.](http://arxiv.org/abs/2305.14761) | 本文提出了UniChart，这是一个用于图表理解和推理的通用视觉语言预训练模型。UniChart首先建立了一个包括各种不同主题和视觉风格的大量图表语料库，然后使用图表特定的预训练任务来编码图表，并在几个图表理解和推理任务上表现出色。 |
| [^62] | [Prompt position really matters in few-shot and zero-shot NLU tasks.](http://arxiv.org/abs/2305.14493) | 该论文通过实证研究发现，提示位置对于少样本和零样本任务的模型性能具有实质性影响，先前研究中使用的提示位置通常是次优的，提示位置优化应成为重要的研究方向。 |
| [^63] | [CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models.](http://arxiv.org/abs/2305.14318) | CREATOR是一个新的框架，使得大型语言模型（LLMs）能够利用文档和代码实现创建自己的工具。通过将抽象工具创建和具体决策执行解耦，CREATOR提高了性能，并在不同基准测试中超越了现有的基线方法。此外，我们引入了Creation Challenge数据集，展示了LLMs的工具创建能力的必要性和优势。 |
| [^64] | [INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback.](http://arxiv.org/abs/2305.14282) | INSTRUCTSCORE是一个可解释的文本生成评估度量，通过利用明确的人类指令和GPT-4的隐式知识，它能生成生成文本的分数和人类可读的诊断报告，达到与最先进度量相当的性能水平。 |
| [^65] | [Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery.](http://arxiv.org/abs/2305.14259) | 本文介绍了一种基于文献的发现方法，通过上下文化的学习生成新的科学方向，克服了标准方法在预测关联、忽略上下文等方面的局限性。模型使用了引文和知识图关系的网络，并使用大型语言模型进行评估，发现GPT4在生成创新思想方面表现出色。 |
| [^66] | [Condensing Multilingual Knowledge with Lightweight Language-Specific Modules.](http://arxiv.org/abs/2305.13993) | 本文介绍了一种使用轻量级的语言特定模块来压缩多语言知识的方法，通过生成低秩矩阵来构建语言特定模块，并使用Fuse Distillation技术将多个语言特定模块中的知识压缩到一个共享模块中，提高了推理和模型序列化的效率。 |
| [^67] | [Learn from Mistakes through Cooperative Interaction with Study Assistant.](http://arxiv.org/abs/2305.13829) | 本文提出了一个新框架 SALAM，通过协作交互与学习助手来帮助 LLM 在反思和改进过程中。该框架通过收集错误并在推理时提供指导方针，显着提高模型性能。 |
| [^68] | [To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis.](http://arxiv.org/abs/2305.13230) | 该研究通过实证调查探讨了在令牌危机下扩展LLM的重复预训练数据方法，发现模型容易过拟合并导致多轮次性能下降，关键因素包括数据集规模、模型参数和训练目标，而正则化技术并不能明显缓解这个问题。 |
| [^69] | [Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance.](http://arxiv.org/abs/2305.13225) | 这项研究探索了将LLMs应用于特定任务的可能性，通过在写作辅助场景中进行指令调整，取得了显著的改进。 |
| [^70] | [Non-Autoregressive Document-Level Machine Translation.](http://arxiv.org/abs/2305.12878) | 本文研究了非自回归模型在文档级机器翻译中的应用，提出了一种简单有效的句子对齐设计，并发现当前非自回归模型在多模态和对齐等问题上还存在困难，性能相对于自回归模型仍有差距。 |
| [^71] | [AutoTrial: Prompting Language Models for Clinical Trial Design.](http://arxiv.org/abs/2305.11366) | AutoTrial是一种使用语言模型自动生成临床试验纳入/排除标准的方法，它可以可控生成、可扩展学习、提供推理链，实验表明，它能够生成流畅准确的标准文本，与先进方法相媲美，但资源占用更少。 |
| [^72] | [Analyzing Norm Violations in Live-Stream Chat.](http://arxiv.org/abs/2305.10731) | 本文分享了第一次针对在直播平台上检测规范违背现象的自然语言处理研究，通过分类别标注和用户研究，揭示了直播聊天规范形成的见解。 |
| [^73] | [Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning.](http://arxiv.org/abs/2305.10613) | 本文旨在探究是否能够使用大型语言模型进行时态知识图谱预测，尤其是不需要任何显式模块。结果表明，大型语言模型在此类预测中表现良好，并且可以隐式有效地编码上下文和时间信息。 |
| [^74] | [Knowledge Rumination for Pre-trained Language Models.](http://arxiv.org/abs/2305.08732) | 本文提出了一种名为知识反思的新范式，旨在帮助预训练语言模型利用已经编码在其预训练参数中的相关潜在知识，而不需要从外部语料库中检索。这种方法通过在模型中添加提示，并将相关知识注入模型进行整合，取得了在常识推理任务和GLUE基准上的实验结果。 |
| [^75] | [Text Classification via Large Language Models.](http://arxiv.org/abs/2305.08377) | 本文介绍了Clue And Reasoning Prompting (CARP)算法，采用逐步推理策略优化了大型语言模型在文本分类中处理复杂语言现象的能力；并通过在监督数据集上使用微调模型进行$k$NN演示搜索，解决了上下文学习中有限标记的问题。 |
| [^76] | [Automatic Evaluation of Attribution by Large Language Models.](http://arxiv.org/abs/2305.06311) | 本文探讨了大型语言模型对归属验证的自动评估。研究发现通过提示LLMs和微调较小的LLMs两种方法都有效地检测到了错误的归属陈述。 |
| [^77] | [Fine-tuning Language Models with Generative Adversarial Feedback.](http://arxiv.org/abs/2305.06176) | 本研究探讨了一种新的方法，使用生成对抗反馈的强化学习(RLGAF)对大型语言模型进行微调，以取代仅受人类反馈的强化学习(RLHF)，从而消除评估者的专业限制并提高性能。 |
| [^78] | [Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare.](http://arxiv.org/abs/2305.05640) | 本研究提出了一种在医疗保健领域构建面向实体的知识图谱的端到端表示学习方法 HEER，通过将领域特定的约束和特征纳入到图嵌入算法中，有效地改善了下游预测任务。 |
| [^79] | [MoT: Pre-thinking and Recalling Enable ChatGPT to Self-Improve with Memory-of-Thoughts.](http://arxiv.org/abs/2305.05181) | 本文提出了一个名为 MoT 的框架，让大型语言模型通过“思想记忆”自我进化，无需注释数据集和参数更新。实验表明，该框架可以显著提高 ChatGPT 在数学推理、常识推理、事实推理和自然语言推理方面的能力。 |
| [^80] | [Towards Summarizing Multiple Documents with Hierarchical Relationships.](http://arxiv.org/abs/2305.01498) | 提出了一个新的数据集PeerSum用于生成科学论文的元评论，源文档具有显式层次结构的丰富文档间关系，提出了一种用于元评论生成的关系感知多任务模型Rammer。 |
| [^81] | [GPT-NER: Named Entity Recognition via Large Language Models.](http://arxiv.org/abs/2304.10428) | 本文提出了GPT-NER来解决大型语言模型在命名实体识别任务（NER）上表现不佳的问题，它通过将序列标记任务转化为生成任务，将LLM能够容易地适应NER任务。同时，为了有效解决LLMs“幻觉”问题，作者们提出了自我验证策略，通过提示LLMs询问自身来确定提取的实体是否属于实际存在的实体。 |
| [^82] | [RRHF: Rank Responses to Align Language Models with Human Feedback without tears.](http://arxiv.org/abs/2304.05302) | RRHF是一种新的学习范式，可以高效地对齐语言模型输出概率与人类偏好，它通过排序损失对不同采样策略生成的响应进行评分，并在调整过程中只需1到2个模型。 |
| [^83] | [Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition.](http://arxiv.org/abs/2304.04704) | 本研究提出了一种用于视觉语言模型的提示预训练方法POMP，可以在包括图像分类、语义分割和目标检测在内的各种视觉识别任务中提升识别性能，通过压缩语义信息，支持超过两万个类别的视觉概念。实验结果表明，POMP在多个数据集上达到了最先进的性能水平。 |
| [^84] | [GEMINI: Controlling the Sentence-level Writing Style for Abstractive Text Summarization.](http://arxiv.org/abs/2304.03548) | GEMINI模型将句子重写和融合技术集成，实现了抽象文本摘要中的句子级写作风格控制，该自适应方法在各种基准数据集上表现优异，特别是在数据集具有平衡的风格分布时。 |
| [^85] | [LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models.](http://arxiv.org/abs/2304.01933) | 本文提出了LLM-Adapters，一个将适配器集成到LLMs中进行参数高效微调的易于使用的框架，实现了比现有方法更少的参数和训练时间，在多个基准测试上取得了最先进的结果。 |
| [^86] | [Explicit Planning Helps Language Models in Logical Reasoning.](http://arxiv.org/abs/2303.15714) | 本文提出了一个新的系统，使用语言模型进行多步逻辑推理，采用了显式规划来帮助做出更明智的决策，比其他竞争系统表现更好，显式规划在系统性能中起着关键作用。 |
| [^87] | [Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation.](http://arxiv.org/abs/2303.15413) | 本文提出了两种去偏置的方法，一种通过增加2D扩散模型得出的分数的截断值，一种通过调整视角提示和物体空间摄像机姿态之间的差异。实验结果表明这些方法可以显著减少伪影，提高真实感。 |
| [^88] | [Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT.](http://arxiv.org/abs/2303.13809) | 本文提出一种新的提示方法Error Analysis Prompting可改善LLMs在机器翻译质量评估上的性能，实现人类水平的评估。 |
| [^89] | [Difficulty in learning chirality for Transformer fed with SMILES.](http://arxiv.org/abs/2303.11593) | 应用SMILES序列的Transformer模型在学习分子结构的整体性和手性方面存在困难，需要进行长时间的训练。生成的描述符用于分子性质预测时的准确率从开始到训练结束都是相似的。 |
| [^90] | [Retrieving Multimodal Information for Augmented Generation: A Survey.](http://arxiv.org/abs/2303.10868) | 这项调研综述了通过检索多模态知识来协助和增强生成模型的方法，这些方法提供了解决准确性、推理性、可解释性和鲁棒性等重要问题的有希望的解决方案。 |
| [^91] | [DeltaScore: Evaluating Story Generation with Differentiating Perturbations.](http://arxiv.org/abs/2303.08991) | DeltaScore利用差分扰动来评估故事生成的细粒度方面，并通过计算故事在特定方面扰动前后的可能性差异来衡量影响。该方法在多个故事领域中得到了评估，并与人类判断的相关性进行了研究。 |
| [^92] | [Optimizing Deep Learning Model Parameters with the Bees Algorithm for Improved Medical Text Classification.](http://arxiv.org/abs/2303.08021) | 本文使用蜜蜂算法优化了深度学习模型参数，提高了医学文本分类的准确性，最高准确率在英语数据集上达到了99.63%，在阿拉伯语数据集上达到了88%。 |
| [^93] | [Finding Support Examples for In-Context Learning.](http://arxiv.org/abs/2302.13539) | 提出了一种名为LENS的过滤-搜索方法，用于寻找用于上下文学习的支持例子。LENS通过分阶段的过滤和搜索，在数据集中筛选信息丰富的上下文示例，并通过多样性引导的示例搜索方法找到能够充分描绘任务的示例。实验证明，LENS在性能上显著优于其他基线方法。 |
| [^94] | [Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?.](http://arxiv.org/abs/2302.11713) | 本研究介绍了一个专门针对无法仅凭常识知识回答的信息寻求问题而设计的视觉问答数据集InfoSeek。使用InfoSeek数据集，我们发现目前最先进的预训练多模态模型在回答求知视觉问题方面面临挑战，但在该数据集上进行微调可以激发模型使用细粒度知识。 |
| [^95] | [RETVec: Resilient and Efficient Text Vectorizer.](http://arxiv.org/abs/2302.09207) | RETVec是一种高效、弹性和多语言的文本向量化器，通过采用新颖的字符编码和对抗攻击鲁棒的嵌入模型，实现了对拼写错误和对抗性攻击的更好适应性。与其他向量化器和词嵌入模型相比，RETVec在各种模型架构和数据集上表现出竞争力和显著的弹性。 |
| [^96] | [Natural Response Generation for Chinese Reading Comprehension.](http://arxiv.org/abs/2302.08817) | 本研究构建了一个新的数据集Penguin，旨在促进中文阅读理解中自然响应生成的研究，提供了一个相对较大的训练和测试平台。通过开发两个强大的基准模型，我们解决了Penguin中的挑战。 |
| [^97] | [PK-ICR: Persona-Knowledge Interactive Context Retrieval for Grounded Dialogue.](http://arxiv.org/abs/2302.06674) | PK-ICR是一种基于角色和知识的互动上下文检索方法，可以在复杂的多场景对话中同时识别角色和知识。通过利用神经问答检索模型，该方法可以在较少的计算资源下实现检索，并且通过引入空-正向排名测试方法来提高排名性能。 |
| [^98] | [Towards Inferential Reproducibility of Machine Learning Research.](http://arxiv.org/abs/2302.04054) | 本研究提出利用线性混合效应模型（LMEM）来分析机器学习性能评估分数，并考虑多个方差来源及其与数据特性相互作用，从而评估可靠性和可复制性，促进对机器学习算法行为的更全面理解。 |
| [^99] | [On the Inconsistencies of Conditionals Learned by Masked Language Models.](http://arxiv.org/abs/2301.00068) | 本论文研究发现，遮蔽语言模型学习的条件句往往存在着不一致性，无法从一个连贯的联合分布中推导出来。我们通过实证发现这种不一致性普遍存在于不同尺寸和配置的遮蔽语言模型中。为了解决这个问题，我们提出了条件句集合方法来在推断阶段处理不一致性。 |
| [^100] | [Semantic Similarity Models for Depression Severity Estimation.](http://arxiv.org/abs/2211.07624) | 本研究提出了一种基于社交媒体文本的语义流程，并使用不同的聚合方法对抑郁症严重程度进行估计。在Reddit上的两个基准测试中，我们的方法达到了30％的准确率。 |
| [^101] | [Large Language Models Meet Harry Potter: A Bilingual Dataset for Aligning Dialogue Agents with Characters.](http://arxiv.org/abs/2211.06869) | 这个论文介绍了一个名为哈利·波特对话（HPD）的数据集，用于研究对话代理和角色对齐。该数据集包含了哈利·波特系列的对话场景，并注释了对话背景信息、说话者、角色关系和属性。通过在HPD上对大型语言模型进行评估，可以推动对话代理的发展，并提供一个通用基准来评估大型语言模型与特定角色对齐的能力。 |
| [^102] | [Small-Text: Active Learning for Text Classification in Python.](http://arxiv.org/abs/2107.10314) | Small-Text是一个Python中的易于使用的主动学习库，用于文本分类，它集成了多种先进的查询策略和著名的机器学习库，支持单标签和多标签分类。研究者使用该库调研了最新的SetFit训练范式的性能，并与传统方法进行了比较。 |
| [^103] | [Defending Against Backdoor Attacks in Natural Language Generation.](http://arxiv.org/abs/2106.01810) | 本研究通过形式化的定义，研究了后门攻击对自然语言生成模型的影响，并设计了针对这些攻击的防御策略。测试生成目标给定源的反向概率可以有效防御各种攻击类型，并解决了在对话生成等自然语言生成任务中的“一对多”问题。 |
| [^104] | [GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph.](http://arxiv.org/abs/2105.02605) | GraphFormers是一种将GNN嵌套到Transformer中的方法，通过迭代式的工作流程，准确理解文本图中每个节点的语义，同时引入渐进式学习加速训练。 |
| [^105] | [On migration to Perpetual Enterprise System.](http://arxiv.org/abs/2104.04844) | 本研究提出了一种实际的方法，用于将组织的计算系统迁移到一个可以永久演进且整合整个组织的新系统，强调治理和技术两个方面的重要性。 |

# 详细

[^1]: COVID-19南非疫苗犹豫模型在微调M-pox推文上展现出性能提升

    COVID-19 South African Vaccine Hesitancy Models Show Boost in Performance Upon Fine-Tuning on M-pox Tweets. (arXiv:2310.04453v1 [cs.CL])

    [http://arxiv.org/abs/2310.04453](http://arxiv.org/abs/2310.04453)

    通过在M-pox数据集上进行微调，COVID-19模型在南非疫苗犹豫方面的性能得到了提高。

    

    自2022年5月开始，非地区性国家报告了大量M-pox病例，让很多人担心M-pox疫情将迅速转变为另一个大流行，而COVID-19疫情仍在肆虐。鉴于M-pox与COVID-19的相似性，我们选择测试在南非Twitter数据上训练的COVID-19模型在手工标记的M-pox数据集上的性能，包括微调前和微调后。超过20k条来自南非的M-pox相关推文被手工标记为积极、消极或中性。在将这些COVID-19模型微调到M-pox数据集后，F1-score提高了超过8%，接近70%，但仍优于最先进的模型和众所周知的分类算法。我们使用基于LDA的主题建模程序将原始COVID-19 RoBERTa模型和其经过微调的版本的错分M-pox推文进行比较，通过这个分析，我们能得出关于犹豫的结论。

    Very large numbers of M-pox cases have, since the start of May 2022, been reported in non-endemic countries leading many to fear that the M-pox Outbreak would rapidly transition into another pandemic, while the COVID-19 pandemic ravages on. Given the similarities of M-pox with COVID-19, we chose to test the performance of COVID-19 models trained on South African twitter data on a hand-labelled M-pox dataset before and after fine-tuning. More than 20k M-pox-related tweets from South Africa were hand-labelled as being either positive, negative or neutral. After fine-tuning these COVID-19 models on the M-pox dataset, the F1-scores increased by more than 8% falling just short of 70%, but still outperforming state-of-the-art models and well-known classification algorithms. An LDA-based topic modelling procedure was used to compare the miss-classified M-pox tweets of the original COVID-19 RoBERTa model with its fine-tuned version, and from this analysis, we were able to draw conclusions on h
    
[^2]: 社会科学中的机器学习短文本分类：以Twitter上的气候变化为例

    Short text classification with machine learning in the social sciences: The case of climate change on Twitter. (arXiv:2310.04452v1 [cs.CL])

    [http://arxiv.org/abs/2310.04452](http://arxiv.org/abs/2310.04452)

    本文研究了社会科学中的机器学习短文本分类问题，并对不同的分类器进行了性能比较。以Twitter上的气候变化话题为例，使用了一个包含5750个推文的新数据集进行评估。

    

    为了分析大量的文本，社会科学研究者越来越面临文本分类的挑战。当无法进行手动标注时，研究者必须找到自动化分类文本的方法，计算机科学为社会科学提供了一个有用的机器学习方法工具箱，其性能在社会科学研究中的应用尚未得到深入研究。本文比较了最常用的文本分类器的性能，将它们应用于社会科学研究中的典型研究场景：一个相对较小的被标记数据集，其中感兴趣的类别很少出现，并且作为一个大型未标记数据集的一部分。以气候变化的Twitter沟通为例，这是一个在交叉学科社会科学研究中越来越受关注的话题。我们使用一个新颖的数据集，包括来自各种国际组织的5750个关于气候变化这个高度模糊概念的推文，评估了不同分类器的性能。

    To analyse large numbers of texts, social science researchers are increasingly confronting the challenge of text classification. When manual labeling is not possible and researchers have to find automatized ways to classify texts, computer science provides a useful toolbox of machine-learning methods whose performance remains understudied in the social sciences. In this article, we compare the performance of the most widely used text classifiers by applying them to a typical research scenario in social science research: a relatively small labeled dataset with infrequent occurrence of categories of interest, which is a part of a large unlabeled dataset. As an example case, we look at Twitter communication regarding climate change, a topic of increasing scholarly interest in interdisciplinary social science research. Using a novel dataset including 5,750 tweets from various international organizations regarding the highly ambiguous concept of climate change, we evaluate the performance o
    
[^3]: AutoDAN: 在对齐的大型语言模型上生成隐蔽的越狱提示

    AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models. (arXiv:2310.04451v1 [cs.CL])

    [http://arxiv.org/abs/2310.04451](http://arxiv.org/abs/2310.04451)

    本文介绍了一种名为AutoDAN的方法，该方法旨在在对齐的大型语言模型上自动生成隐蔽的越狱提示，以解决现有越狱技术的可扩展性和隐蔽性问题。

    

    对齐的大型语言模型(LLM)是强大的语言理解和决策工具，通过与人类反馈进行广泛对齐而创建。然而，这些大型模型仍然容易受到越狱攻击的影响，攻击者可以操纵提示来引发对齐的LLM不应给出的恶意输出。研究越狱提示可以让我们深入了解LLM的局限性，并进一步指导我们如何保护它们。不幸的是，现有的越狱技术存在以下问题：(1) 可扩展性问题，攻击大量依赖手工制作提示；(2) 隐蔽性问题，攻击依赖基于标记的算法生成常常语义无意义的提示，容易通过基本困惑度测试检测。针对这些挑战，我们想回答这个问题：能否开发一种能够自动生成隐蔽越狱提示的方法？在本文中，我们介绍了AutoDAN方法。

    The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce Auto
    
[^4]: 使用评估理论研究大型语言模型对情感的感知

    Investigating Large Language Models' Perception of Emotion Using Appraisal Theory. (arXiv:2310.04450v1 [cs.CL])

    [http://arxiv.org/abs/2310.04450](http://arxiv.org/abs/2310.04450)

    本研究通过使用评估理论和应激与应对过程问卷（SCPQ）来调查大型语言模型对情感的感知。结果表明，模型的响应在评估和应对的动态方面与人类类似，但在关键评估维度上与预测的不一致。

    

    大型语言模型（LLM）如ChatGPT在最近几年取得了显著进展，并且现在正被公众使用。随着越来越多的人与这些系统互动，提高我们对这些黑盒模型的理解尤为关键，特别是关于它们对人类心理方面的理解。在这项研究中，我们通过使用应对和评估理论中的评估维度来调查它们对情感的感知，使用了应激与应对过程问卷（SCPQ）。SCPQ是一个经过验证的临床工具，由多个随时间而演变且在关键评估变量（如可控性和可变性）上有所不同的故事组成。我们将SCPQ应用于OpenAI的三个最新LLM（davinci-003、ChatGPT和GPT-4），并将结果与评估理论和人类数据的预测进行对比。结果显示，LLM的响应在评估和应对的动态方面与人类类似，但它们的响应在关键评估维度上与预测的不一致。

    Large Language Models (LLM) like ChatGPT have significantly advanced in recent years and are now being used by the general public. As more people interact with these systems, improving our understanding of these black box models is crucial, especially regarding their understanding of human psychological aspects. In this work, we investigate their emotion perception through the lens of appraisal and coping theory using the Stress and Coping Process Questionaire (SCPQ). SCPQ is a validated clinical instrument consisting of multiple stories that evolve over time and differ in key appraisal variables such as controllability and changeability. We applied SCPQ to three recent LLMs from OpenAI, davinci-003, ChatGPT, and GPT-4 and compared the results with predictions from the appraisal theory and human data. The results show that LLMs' responses are similar to humans in terms of dynamics of appraisal and coping, but their responses did not differ along key appraisal dimensions as predicted by
    
[^5]: LoFT: 用于改进对大型语言模型的对抗攻击可传递性的本地代理微调

    LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model. (arXiv:2310.04445v1 [cs.CL])

    [http://arxiv.org/abs/2310.04445](http://arxiv.org/abs/2310.04445)

    本文提出了一种名为LoFT的方法，通过在与有害查询处于词汇-语义邻域的相似查询上进行代理模型的微调，来改善对大型语言模型的对抗攻击的可传递性。

    

    已经发现，大型语言模型（LLM）的对齐可以通过附加特制的攻击后缀和有害查询来规避，以引发有害的响应。为了对未知特征的私有目标模型进行攻击，可以使用公共模型作为代理来构建攻击，并将成功的攻击从公共代理传递到私有目标模型。攻击的成功率取决于代理模型能够多大程度上逼近私有模型。我们假设，对于攻击可传递性来说，只要代理能够在有害查询的词汇-语义邻域内逼近目标模型即可。因此，在本文中，我们提出了“本地微调（LoFT）”，即在与有害查询处于词汇-语义邻域的相似查询上进行代理模型的微调，以减小代理和目标模型之间的差异。首先，我们演示了三种促使私有目标模型变得易受攻击的方法。

    It has been shown that Large Language Model (LLM) alignments can be circumvented by appending specially crafted attack suffixes with harmful queries to elicit harmful responses. To conduct attacks against private target models whose characterization is unknown, public models can be used as proxies to fashion the attack, with successful attacks being transferred from public proxies to private target models. The success rate of attack depends on how closely the proxy model approximates the private model. We hypothesize that for attacks to be transferrable, it is sufficient if the proxy can approximate the target model in the neighborhood of the harmful query. Therefore, in this paper, we propose \emph{Local Fine-Tuning (LoFT)}, \textit{i.e.}, fine-tuning proxy models on similar queries that lie in the lexico-semantic neighborhood of harmful queries to decrease the divergence between the proxy and target models. First, we demonstrate three approaches to prompt private target models to obt
    
[^6]: 魔法词是什么？LLM提示的控制理论研究

    What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])

    [http://arxiv.org/abs/2310.04444](http://arxiv.org/abs/2310.04444)

    本论文将提示工程形式化为LLM上的最优控制问题，研究了给定token序列时是否存在一种最优提示能够准确预测最终的token，并提出了控制理论中的指标来描述LLM的可操纵性。

    

    提示工程在LLM的部署中是有效和重要的，但在数学上理解不足。在这里，我们将提示工程形式化为LLM上的最优控制问题，其中提示被认为是调节LLM输出分布的控制变量。在这个框架内，我们提出一个简单的问题：给定一个token序列，是否总存在一个我们可以添加的提示，使得LLM能够准确预测最终的token？我们将这样的最优提示称为魔法词，因为添加提示会导致LLM输出正确的答案。如果存在魔法词，我们能否找到它们？如果可以，它们的特性是什么？我们提供了将控制理论应用于自注意力头的分析分析，证明了其权重矩阵的奇异值函数为可控制性的上界。我们借鉴控制理论来提出了一种叫做$k-\epsilon$可控制性的指标，用于描述LLM的可操纵性。

    Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\epsilon$ controllability to characterize LLM steerability. We comput
    
[^7]: 人类移动问题回答（展望论文）

    Human Mobility Question Answering (Vision Paper). (arXiv:2310.04443v1 [cs.CL])

    [http://arxiv.org/abs/2310.04443](http://arxiv.org/abs/2310.04443)

    本文提出了一项新的任务，即人类移动问题回答（MobQA），旨在让智能系统从移动数据中学习并回答相关问题，填补了关于利用人类移动数据进行问题回答系统的研究空白，并为移动推荐系统的研究带来了新的范式变革。

    

    问答系统已经引起了人工智能界的广泛关注，因为它们可以根据给定的知识源（例如视觉问答中的图像）学习回答问题。然而，关于利用人类移动数据进行问题回答系统的研究尚未被探索。挖掘人类移动数据对于智能城市规划、疫情管理和个性化推荐系统等各种应用至关重要。本文旨在填补这一空白，引入一项新的任务，即人类移动问题回答（MobQA）。该任务旨在让智能系统从移动数据中学习并回答相关问题。该任务为移动预测研究带来了新的范式变革，并进一步促进了人类移动推荐系统的研究。为了更好地支持这个新的研究课题，这篇展望论文还提出了一个数据集的初步设计和一个潜在的深度学习模型。

    Question answering (QA) systems have attracted much attention from the artificial intelligence community as they can learn to answer questions based on the given knowledge source (e.g., images in visual question answering). However, the research into question answering systems with human mobility data remains unexplored. Mining human mobility data is crucial for various applications such as smart city planning, pandemic management, and personalised recommendation system. In this paper, we aim to tackle this gap and introduce a novel task, that is, human mobility question answering (MobQA). The aim of the task is to let the intelligent system learn from mobility data and answer related questions. This task presents a new paradigm change in mobility prediction research and further facilitates the research of human mobility recommendation systems. To better support this novel research topic, this vision paper also proposes an initial design of the dataset and a potential deep learning mod
    
[^8]: 一份关于提示工程的简要历史: 利用语言模型 (arXiv:2310.04438v1 [cs.CL])

    A Brief History of Prompt: Leveraging Language Models. (arXiv:2310.04438v1 [cs.CL])

    [http://arxiv.org/abs/2310.04438](http://arxiv.org/abs/2310.04438)

    这篇论文全面探讨了提示工程和生成在自然语言处理领域的演进历程，包括早期语言模型和信息检索系统，注意力机制的引入，强化学习技术的应用以解决偏见和暴露偏差等问题。还讨论了微调策略、控制代码和基于模板的生成的重大贡献，以及公平性、人工智能与人类合作和低资源适应的重要性。

    

    本论文全面探讨了在自然语言处理（NLP）领域中提示工程和生成的演进历程。从早期的语言模型和信息检索系统开始，我们追溯了这些年来塑造提示工程的关键发展。2015年引入的注意力机制彻底改变了语言理解，推动了可控性和上下文感知的进步。随后在强化学习技术方面的突破进一步增强了提示工程，解决了暴露偏差和生成文本中的偏见等问题。我们重点考察了2018年和2019年的重大贡献，集中在微调策略、控制代码和基于模板的生成上。本论文还讨论了公平性、人工智能与人类的合作以及低资源适应的日益重要性。在2020年和2021年，上下文提示和迁移学习变得突出，而2022年和2023年见证了...

    This paper presents a comprehensive exploration of the evolution of prompt engineering and generation in the field of natural language processing (NLP). Starting from the early language models and information retrieval systems, we trace the key developments that have shaped prompt engineering over the years. The introduction of attention mechanisms in 2015 revolutionized language understanding, leading to advancements in controllability and context-awareness. Subsequent breakthroughs in reinforcement learning techniques further enhanced prompt engineering, addressing issues like exposure bias and biases in generated text. We examine the significant contributions in 2018 and 2019, focusing on fine-tuning strategies, control codes, and template-based generation. The paper also discusses the growing importance of fairness, human-AI collaboration, and low-resource adaptation. In 2020 and 2021, contextual prompting and transfer learning gained prominence, while 2022 and 2023 witnessed the e
    
[^9]: 思维传播：一种通过类比方法进行大型语言模型复杂推理的方法

    Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models. (arXiv:2310.03965v1 [cs.AI])

    [http://arxiv.org/abs/2310.03965](http://arxiv.org/abs/2310.03965)

    提出了思维传播（TP）方法，通过探索类比问题和利用类比问题的解决方案来增强大型语言模型的复杂推理能力。

    

    大型语言模型（LLMs）在推理任务中取得了显著的成功，但现有的提示方法无法重用解决类似问题的见解，并且在多步推理中累积了错误，因为它们要求LLMs从零开始推理。为了解决这些问题，我们提出了“思维传播”（TP），它探索类似问题并利用它们的解决方案来增强LLMs的复杂推理能力。这些类比问题与输入问题相关，具有可重用的解决方案和问题解决策略。因此，将解决先前类似问题的见解传播以激发新的问题解决是有希望的。为了实现这一点，TP首先提示LLMs提出并解决一组与输入问题相关的类比问题。然后，TP重用类比问题的结果直接产生一个新的解决方案或者推导一个知识密集型计划。

    Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \textit{from scratch}. To address these issues, we propose \textbf{\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs. These analogous problems are related to the input one, with reusable solutions and problem-solving strategies. Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. To achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for 
    
[^10]: 评估自监督语音表示对美洲土著语言的应用

    Evaluating Self-Supervised Speech Representations for Indigenous American Languages. (arXiv:2310.03639v1 [cs.CL])

    [http://arxiv.org/abs/2310.03639](http://arxiv.org/abs/2310.03639)

    本论文评估了自监督语音表示在美洲土著语言上的应用。研究结果表明，大型自监督学习模型在低资源ASR任务中展示出了令人惊讶的强大性能，显示了这些模型在真实世界数据上的泛化能力。

    

    近年来，自监督学习应用于语音表示学习引起了广泛关注，因为它能够扩展到大量的无标注数据。然而，在预训练和下游评估方面，目前的进展仅集中在考虑英语的单语模型上。很少有模型考虑其他语言，尤其是土著语言。我们在ASRU 2023 ML-SUPERB Challenge的新语言赛道中提交了一个用于Quechua的ASR语料库，它是一种南美土著语言。我们评估了大型自监督学习模型在Quechua和其他6种土著语言（如Guarani和Bribri）的低资源ASR上的效果。结果显示，最先进的自监督学习模型表现出令人惊讶的强大性能，展示了大规模模型在真实世界数据上的潜在泛化能力。

    The application of self-supervision to speech representation learning has garnered significant interest in recent years, due to its scalability to large amounts of unlabeled data. However, much progress, both in terms of pre-training and downstream evaluation, has remained concentrated in monolingual models that only consider English. Few models consider other languages, and even fewer consider indigenous ones. In our submission to the New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR corpus for Quechua, an indigenous South American Language. We benchmark the efficacy of large SSL models on Quechua, along with 6 other indigenous languages such as Guarani and Bribri, on low-resource ASR. Our results show surprisingly strong performance by state-of-the-art SSL models, showing the potential generalizability of large-scale models to real-world data.
    
[^11]: 基于思维混合表示的大规模语言模型级联用于成本高效的推理

    Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v1 [cs.CL])

    [http://arxiv.org/abs/2310.03094](http://arxiv.org/abs/2310.03094)

    本研究提出了一种基于思维混合表示的大规模语言模型级联方法，用于成本高效的推理。通过考虑更弱模型的答案一致性作为问题难度的信号，可以实现对问题的决策，从而节约使用更强模型的成本。

    

    大规模语言模型（LLM）如GPT-4在各种任务中展现出了非凡的性能，但是这种强大的性能通常伴随着使用付费API服务的高昂费用。本文的研究动机是为了研究构建LLM级联以节约使用LLM的成本，特别是用于进行推理（例如数学、因果推理）任务的成本。我们的级联管道遵循一个直观的思想，即简单的问题可以由一个更弱但更实惠的LLM来解决，而只有具有挑战性的问题才需要更强大、更昂贵的LLM。为了实现这种决策，我们考虑到更弱的LLM的“答案一致性”作为问题难度的信号，并提出了几种答案采样和一致性检查的方法，其中一种方法利用了两种思维表示（即连续思维和程序思维）的混合。通过在六个推理基准数据集上的实验，我们使用GPT-3.5-turbo和GPT-4作为较弱的模型，

    Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the "answer consistency" of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and 
    
[^12]: 大型语言模型可以成为良好的隐私保护学习者

    Large Language Models Can Be Good Privacy Protection Learners. (arXiv:2310.02469v1 [cs.CL])

    [http://arxiv.org/abs/2310.02469](http://arxiv.org/abs/2310.02469)

    本论文介绍了一种名为隐私保护语言模型（PPLM）的新范式，可以在保护数据隐私的同时有效注入领域特定知识。通过对模型设计的理论分析和不同技术的研究，我们验证了使用正向和负向示例进行指令微调的方法具有很大的潜力。

    

    大型语言模型（LLMs）的普及引发了人们对使用特定领域数据对其进行微调，创建专门的语言模型的兴趣。然而，这种特定领域的微调数据通常包含敏感的个人身份信息（PII）。在没有隐私保护的情况下直接微调 LLMs 会存在信息泄露的风险。为了解决这个挑战，我们引入了隐私保护语言模型（PPLM），这是一种在有效注入领域特定知识的同时保护数据隐私的新范式。我们的工作提供了模型设计的理论分析，并深入研究了各种技术，比如语料库策展、基于惩罚的非概然性训练损失以及基于指令的微调等等。广泛的实验在不同的数据集和场景中验证了我们的方法的有效性。特别是，使用正向和负向示例进行指令微调，显示出很有希望的方法。

    The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data often contains sensitive personally identifiable information (PII). Direct fine-tuning LLMs on this data without privacy protection poses a risk of leakage. To address this challenge, we introduce Privacy Protection Language Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects domain-specific knowledge while safeguarding data privacy. Our work offers a theoretical analysis for model design and delves into various techniques such as corpus curation, penalty-based unlikelihood in training loss, and instruction-based tuning, etc. Extensive experiments across diverse datasets and scenarios demonstrate the effectiveness of our approaches. In particular, instruction tuning with both positive and negative examples, stands out as a promising method, eff
    
[^13]: 使用神经语言模型从临床文本中提取药物和时间关系

    Extraction of Medication and Temporal Relation from Clinical Text using Neural Language Models. (arXiv:2310.02229v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.02229](http://arxiv.org/abs/2310.02229)

    本研究使用深度学习和大型语言模型，通过名为MedTem的临床领域实体识别和时间关系提取，来提取临床文本中的药物和时间关系，以帮助临床医生更好地了解患者的治疗历史。

    

    临床文本在电子医疗记录（EMRs）中表示，包含丰富的医学信息，对疾病预测、个性化信息推荐、临床决策支持以及药物模式挖掘和测量至关重要。药物提取和时间关系分类之间的关系进一步帮助临床医生更好地了解患者的治疗历史。为了评估深度学习（DL）和大型语言模型（LLMs）在药物提取和时间关系分类中的性能，我们对名为MedTem的临床领域实体识别（NER）进行了实证研究，并使用了几种先进的学习结构，包括BiLSTM-CRF和CNN-BiLSTM，以及用于时间关系提取（RE）的BERT-CNN，此外，还探索了不同的词嵌入技术。此外，我们还设计了一套后处理规则，以生成关于药物和时间的结构化输出。

    Clinical texts, represented in electronic medical records (EMRs), contain rich medical information and are essential for disease prediction, personalised information recommendation, clinical decision support, and medication pattern mining and measurement. Relation extractions between medication mentions and temporal information can further help clinicians better understand the patients' treatment history. To evaluate the performances of deep learning (DL) and large language models (LLMs) in medication extraction and temporal relations classification, we carry out an empirical investigation of \textbf{MedTem} project using several advanced learning structures including BiLSTM-CRF and CNN-BiLSTM for a clinical domain named entity recognition (NER), and BERT-CNN for temporal relation extraction (RE), in addition to the exploration of different word embedding techniques. Furthermore, we also designed a set of post-processing roles to generate structured output on medications and the tempor
    
[^14]: 模型告诉你该丢弃什么：适应性KV缓存压缩用于LLMs

    Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. (arXiv:2310.01801v1 [cs.CL])

    [http://arxiv.org/abs/2310.01801](http://arxiv.org/abs/2310.01801)

    该研究提出了一种自适应的KV缓存压缩方法，用于减少大型语言模型的内存消耗。通过有针对性的分析和结构识别，构建了具有自适应性的KV缓存，通过清除和丢弃特定的上下文，以及只对特定的注意力头使用标准KV缓存，实现了显著的内存占用减少。

    

    在这项研究中，我们引入了一种自适应的KV缓存压缩方法，它可以减少大型语言模型（LLMs）生成推理的内存占用。与传统的KV缓存不同，我们通过有针对性的分析来识别注意力模块的内在结构。基于识别出的结构，我们以自适应的方式构建KV缓存：在强调本地上下文的注意力头上清除长距离上下文，在以特殊标记为中心的注意力头上丢弃非特殊标记，并且仅对广泛关注所有标记的注意力头使用标准的KV缓存。此外，通过使用轻量级的注意力分析来指导自适应KV缓存的构建，FastGen可以在不需要资源密集型的微调或重新训练的情况下部署。在各种任务的实验中，FastGen在GPU内存消耗方面显示出了显著的减少。

    In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with 
    
[^15]: 分割与合并：对大型语言模型的位置偏差进行校准

    Split and Merge: Aligning Position Biases in Large Language Model based Evaluators. (arXiv:2310.01432v1 [cs.CL])

    [http://arxiv.org/abs/2310.01432](http://arxiv.org/abs/2310.01432)

    PORTIA是一个旨在校准大型语言模型评估器的位置偏差的对齐系统，通过将答案分割成多个片段，并对其进行对齐，然后将其合并回一个单一的提示，以提高评估的准确性和公正性。

    

    大型语言模型(LLMs)已被证明可以作为自动化评估器，用于评估AI系统生成的答案的质量。然而，这些基于LLM的评估器在使用对比评估候选答案时存在位置偏差或不一致性，无视内容而偏向于第一个或第二个答案。为了解决这个问题，我们提出了PORTIA，这是一个基于对齐的系统，旨在模拟人类的比较策略，以轻量级但有效的方式校准位置偏差。具体而言，PORTIA将答案分割成多个片段，对比候选答案中的相似内容进行对齐，并将它们合并回一个单一的提示，以供LLMs评估。我们使用六种不同的LLM进行了大量实验，评估了11,520个答案对。我们的结果表明，PORTIA显著提高了所有模型和对比形式的一致性率，平均相对改进率达到47.46%。引人注目的是，PORTIA使得LLMs能够评估中对位置偏差进行校准的创新方法，从而提高了评估的准确性和公正性。

    Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, these LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, aligns similar content across candidate answers, and then merges them back into a single prompt for evaluation by LLMs. We conducted extensive experiments with six diverse LLMs to evaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances the consistency rates for all the models and comparison forms tested, achieving an average relative improvement of 47.46%. Remarkably, PORTIA enables le
    
[^16]: RA-DIT: 检索增强的双重指令调优

    RA-DIT: Retrieval-Augmented Dual Instruction Tuning. (arXiv:2310.01352v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.01352](http://arxiv.org/abs/2310.01352)

    本论文介绍了一种轻量级的微调方法RA-DIT，通过为任何语言模型添加检索能力来提高性能。该方法分为两个步骤：一是更新语言模型以更好地利用检索到的信息，二是更新检索器以返回更相关的结果。实验证明每个步骤都能显著提高性能，同时使用两个步骤可以获得额外的收益。

    

    检索增强语言模型（RALMs）通过访问外部数据存储中的长尾和最新知识来提高性能，但构建起来具有挑战性。现有的方法要么需要昂贵的检索特定修改来进行语言模型预训练，要么使用事后集成数据存储的方法，导致性能不理想。我们引入了一种轻量级的微调方法——检索增强的双重指令调优（RA-DIT），通过为任何语言模型添加检索能力来实现。我们的方法分为两个不同的微调步骤：（1）一个更新预训练的语言模型以更好地利用检索到的信息，（2）另一个更新检索器以返回更相关的结果，符合语言模型的偏好。通过在需要知识利用和上下文意识的任务上进行微调，我们证明了每个阶段都能显著提高性能，并且同时使用两个阶段可以获得额外的收益。我们的最佳模型是RA-DIT 65B。

    Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B,
    
[^17]: 重返未来：面向大型语言模型的可解释的时间推理

    Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models. (arXiv:2310.01074v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.01074](http://arxiv.org/abs/2310.01074)

    本文提出了一个新的任务——可解释的时间推理，旨在预测未来时间戳上事件的发生，需要进行多步推理和多个事件的综合。

    

    时间推理是一项关键的自然语言处理任务，可以在文本数据中提供对时间敏感环境的细致理解。虽然最近的LLM进展展示了它们在时间推理方面的潜力，但主要关注的是诸如时间表达和时间关系抽取等任务。这些任务主要设计用于提取直接和过去的时间线索，并进行简单的推理过程。然而，在考虑复杂推理任务（如事件预测）时仍存在重大差距，这需要对事件进行多步的时间推理，并对未来时间戳进行预测。现有方法的另一个显著局限是它们无法提供推理过程的说明，从而阻碍了可解释性。在本文中，我们引入了可解释的时间推理的第一个任务，用于基于上下文预测未来时间戳上事件的发生，这需要对多个事件进行多步推理。

    Temporal reasoning is a crucial NLP task, providing a nuanced understanding of time-sensitive contexts within textual data. Although recent advancements in LLMs have demonstrated their potential in temporal reasoning, the predominant focus has been on tasks such as temporal expression and temporal relation extraction. These tasks are primarily designed for the extraction of direct and past temporal cues and to engage in simple reasoning processes. A significant gap remains when considering complex reasoning tasks such as event forecasting, which requires multi-step temporal reasoning on events and prediction on the future timestamp. Another notable limitation of existing methods is their incapability to provide an illustration of their reasoning process, hindering explainability. In this paper, we introduce the first task of explainable temporal reasoning, to predict an event's occurrence at a future timestamp based on context which requires multiple reasoning over multiple events, and
    
[^18]: GeRA: 标签效率的几何正则化对齐

    GeRA: Label-Efficient Geometrically Regularized Alignment. (arXiv:2310.00672v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00672](http://arxiv.org/abs/2310.00672)

    GeRA是一种标签效率的几何正则化对齐方法，利用未配对数据的流形几何提高了预训练单模态编码器的对齐性能。

    

    预训练的单模态编码器将丰富的语义信息融入嵌入空间结构中。为了具有类似的信息量，多模态编码器通常需要大量成对数据进行对齐和训练。我们引入了一种半监督的几何正则化对齐方法（GeRA），以标签高效的方式对预训练的单模态编码器的嵌入空间进行对齐。我们的方法利用未配对（未标记）数据的流形几何来提高对齐性能。为了防止在对齐过程中对局部几何造成失真，可能破坏语义邻域结构并导致未观察到的对产生错位，我们引入了几何损失项。该项基于一个捕获单模态预训练编码器局部流形几何的扩散算子构建。GeRA是模态无关的，因此可以用于对齐任何数据模态的预训练编码器。我们提供了实证证据来支持我们方法的有效性。

    Pretrained unimodal encoders incorporate rich semantic information into embedding space structures. To be similarly informative, multi-modal encoders typically require massive amounts of paired data for alignment and training. We introduce a semi-supervised Geometrically Regularized Alignment (GeRA) method to align the embedding spaces of pretrained unimodal encoders in a label-efficient way. Our method leverages the manifold geometry of unpaired (unlabeled) data to improve alignment performance. To prevent distortions to local geometry during the alignment process, potentially disrupting semantic neighborhood structures and causing misalignment of unobserved pairs, we introduce a geometric loss term. This term is built upon a diffusion operator that captures the local manifold geometry of the unimodal pretrained encoders. GeRA is modality-agnostic and thus can be used to align pretrained encoders from any data modalities. We provide empirical evidence to the effectiveness of our metho
    
[^19]: SELF：基于语言驱动的大型语言模型自主进化

    SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00533](http://arxiv.org/abs/2310.00533)

    SELF提出了一种基于语言驱动的创新方法，允许大型语言模型（LLM）不断自我进化，并通过语言反馈作为评估工具来改进模型的响应能力和训练稳定性。

    

    大型语言模型（LLM）展示了在多个领域中的卓越适应能力。然而，实现人类水平的学习和推动自主人工智能的关键——模型自主进化的路径仍然未知。我们引入了一种创新的方法，名为"SELF"（带有语言反馈的自主进化）。这种方法使LLM能够不断地自我进化。此外，SELF利用语言反馈作为一种多功能、全面的评估工具，精确定位响应改进的领域，并提高自主进化训练的稳定性。SELF首先进行元技能学习，专注于自我反馈和自我精炼。这些元技能是关键，引导模型在自制数据的持续训练周期中进行后续的自我进化，从而增强其内在能力。在给定无标签指令的情况下，SELF使模型具备了能够...

    Large Language Models (LLMs) have showcased remarkable versatility across diverse domains. However, the pathway toward autonomous model development, a cornerstone for achieving human-level learning and advancing autonomous AI, remains largely uncharted. We introduce an innovative approach, termed "SELF" (Self-Evolution with Language Feedback). This methodology empowers LLMs to undergo continual self-evolution. Furthermore, SELF employs language-based feedback as a versatile and comprehensive evaluative tool, pinpointing areas for response refinement and bolstering the stability of self-evolutionary training. Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement. These meta-skills are critical, guiding the model's subsequent self-evolution through a cycle of perpetual training with self-curated data, thereby enhancing its intrinsic abilities. Given unlabeled instructions, SELF equips the model with the capability to
    
[^20]: 通过鉴别-批判差距测量语言模型对价值的理解

    Measuring Value Understanding in Language Models through Discriminator-Critique Gap. (arXiv:2310.00378v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00378](http://arxiv.org/abs/2310.00378)

    通过鉴别-批判差距测量LLMs对人类价值的理解，我们提出了价值理解测量（VUM）框架，并使用GPT-4开发了一个包含一千个对话的数据集。我们的评估结果显示，尺度定律对LLMs的“知道什么”有较大影响，而对“知道为什么”影响较小。

    

    最近大型语言模型（LLMs）的进展引发了对它们与人类价值观之间潜在不一致性的担忧。然而，由于它们的复杂和适应性，评估它们对这些价值观的理解是复杂的。我们认为真正理解LLMs中的价值观需要考虑到“知道什么”和“知道为什么”两个方面。为此，我们提出了价值理解测量（VUM）框架，通过量化鉴别-批判差距来定量评估“知道什么”和“知道为什么”。利用施瓦茨价值观调查，我们确定了评估价值观的标准，并使用GPT-4开发了一个包含一千个对话的数据集。我们的评估考察了LLMs的输出与基准答案之间的价值观一致性，以及LLMs的回答与GPT-4的注释在价值认知原因上的一致性。我们评估了五个代表性LLMs，并提供了强有力的证据表明，尺度定律对“知道什么”的影响较大，但对“知道为什么”的影响较小。

    Recent advancements in Large Language Models (LLMs) have heightened concerns about their potential misalignment with human values. However, evaluating their grasp of these values is complex due to their intricate and adaptable nature. We argue that truly understanding values in LLMs requires considering both "know what" and "know why". To this end, we present the Value Understanding Measurement (VUM) framework that quantitatively assess both "know what" and "know why" by measuring the discriminator-critique gap related to human values. Using the Schwartz Value Survey, we specify our evaluation values and develop a thousand-level dialogue dataset with GPT-4. Our assessment looks at both the value alignment of LLM's outputs compared to baseline answers and how LLM responses align with reasons for value recognition versus GPT-4's annotations. We evaluate five representative LLMs and provide strong evidence that the scaling law significantly impacts "know what" but not much on "know why", 
    
[^21]: RelBERT: 使用语言模型嵌入关系

    RelBERT: Embedding Relations with Language Models. (arXiv:2310.00299v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00299](http://arxiv.org/abs/2310.00299)

    RelBERT是一种能够从相对较小的语言模型中提取关系嵌入的方法，通过微调RoBERTa模型，只需少量训练数据，即可捕捉关系相似性，并在类比基准中取得最新的最佳表现。

    

    许多应用程序需要访问有关不同概念和实体之间关系的背景知识。尽管知识图谱和大型语言模型在某种程度上可以满足这种需求，但知识图谱不可避免地是不完整的，其关系模式通常过于粗粒度，而大型语言模型则效率低下且难以控制。作为替代方案，我们提出了从相对较小的语言模型中提取关系嵌入的方法。具体而言，我们展示了可以直接微调遮盖语言模型（如RoBERTa）来实现这一目的，仅使用了少量的训练数据。所得到的模型被命名为RelBERT，以意外精细的方式捕捉了关系相似性，使我们在类比基准上取得了最新的最佳表现。关键是，RelBERT能够对超出训练数据范围的关系进行建模。例如，我们使用一种模型在命名实体之间的关系领域取得了强大的结果。

    Many applications need access to background knowledge about how different concepts and entities are related. Although Knowledge Graphs (KG) and Large Language Models (LLM) can address this need to some extent, KGs are inevitably incomplete and their relational schema is often too coarse-grained, while LLMs are inefficient and difficult to control. As an alternative, we propose to extract relation embeddings from relatively small language models. In particular, we show that masked language models such as RoBERTa can be straightforwardly fine-tuned for this purpose, using only a small amount of training data. The resulting model, which we call RelBERT, captures relational similarity in a surprisingly fine-grained way, allowing us to set a new state-of-the-art in analogy benchmarks. Crucially, RelBERT is capable of modelling relations that go well beyond what the model has seen during training. For instance, we obtained strong results on relations between named entities with a model that 
    
[^22]: 两两邻近策略优化: 利用相对反馈进行LLM对齐

    Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. (arXiv:2310.00212v1 [cs.LG])

    [http://arxiv.org/abs/2310.00212](http://arxiv.org/abs/2310.00212)

    该论文提出了一种新的强化学习框架，使用相对反馈来调整大型语言模型（LLMs）的行为，解决了现有方法在优化比较损失训练的奖励时存在的限制。同时，还提出了一种新的基于轨迹的策略梯度算法（PPPO），用于更有效地进行算法设计和函数逼近。

    

    大型语言模型（LLMs）通过在大型语料库上预先训练来获取广泛的世界知识。然而，由于接触到低质量数据，LLMs可能表现出与人类价值不一致的有害行为。引导LLMs朝着有益行为方向发展的主导方法涉及使用人类反馈的强化学习（RLHF），其中Proximal Policy Optimization（PPO）是默认的RL优化器。尽管其有效性，但PPO在优化基于比较损失训练的奖励时存在局限性。主要问题是，由于需要校准奖励尺度，PPO对于包含相同偏好信息的等价奖励函数不具备不变性。此外，与基于轨迹的优化相比，PPO对于基于令牌的更新的需求引入了函数逼近和算法设计方面的复杂性。本文提出了一种新的框架，基于相对反馈的强化学习，以及一种新颖的基于轨迹的策略梯度算法，Pairwise Proximal Policy Optimization（PPPO），用于解决上述问题。

    Large Language Models (LLMs) can acquire extensive world knowledge through pre-training on large corpora. However, due to exposure to low-quality data, LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss. Primarily, PPO is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. Additionally, PPO's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. This paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, Pair
    
[^23]: 图像-文本多模型综述论文

    A Survey on Image-text Multimodal Models. (arXiv:2309.15857v1 [cs.CL])

    [http://arxiv.org/abs/2309.15857](http://arxiv.org/abs/2309.15857)

    图像-文本多模型综述论文全面回顾了其发展历程和当前状态，提出了新的分类方法，并阐明了该领域的挑战和潜在研究方向。

    

    在人工智能不断发展的背景下，图像和文本信息的融合成为一个至关重要的领域，导致了图像-文本多模型的出现。本论文全面回顾了图像-文本多模型的发展历程和当前状态，探讨了它们的应用价值、挑战和潜在研究方向。首先，我们重新审视了这些模型的基本概念和发展里程碑，引入了一种新的分类方法，将它们的发展分为三个不同的阶段，基于它们被引入的时间和对学科的影响。此外，基于任务在学术领域中的重要性和普及性，我们提出了将与图像-文本多模型相关的任务划分为五个主要类型的分类方法，阐明了每个类别内的最新进展和关键技术。尽管这些模型取得了显著的成就，但仍面临着许多挑战。

    Amidst the evolving landscape of artificial intelligence, the convergence of visual and textual information has surfaced as a crucial frontier, leading to the advent of image-text multimodal models. This paper provides a comprehensive review of the evolution and current state of image-text multimodal models, exploring their application value, challenges, and potential research trajectories. Initially, we revisit the basic concepts and developmental milestones of these models, introducing a novel classification that segments their evolution into three distinct phases, based on their time of introduction and subsequent impact on the discipline. Furthermore, based on the tasks' significance and prevalence in the academic landscape, we propose a categorization of the tasks associated with image-text multimodal models into five major types, elucidating the recent progress and key technologies within each category. Despite the remarkable accomplishments of these models, numerous challenges a
    
[^24]: Lyra: 自动定理证明中的双重修正策略的编排

    Lyra: Orchestrating Dual Correction in Automated Theorem Proving. (arXiv:2309.15806v1 [cs.CL])

    [http://arxiv.org/abs/2309.15806](http://arxiv.org/abs/2309.15806)

    Lyra是一种新的框架，通过引入工具修正和猜想修正两种机制，增强了大规模语言模型在形式化定理证明领域的有效性，减轻了幻觉，并提高了证明的准确性。

    

    大规模语言模型（LLMs）为形式化定理证明领域提供了一个有趣的探索途径。然而，它们的全部潜力，尤其是关于幻觉的减轻和通过证明器错误消息的细化，仍然是一个尚未深入研究的领域。为了增强LLMs在该领域的有效性，我们引入了Lyra，一种采用两种不同修正机制的新框架：工具修正（TC）和猜想修正（CC）。为了在形式证明的后处理中实现工具修正，我们利用先前的知识来利用预定义的证明工具（如Sledgehammer）来指导替换不正确的工具。工具修正显著减轻了幻觉，从而提高了证明的整体准确性。此外，我们引入了猜想修正，一种错误反馈机制，旨在与证明器互动，通过证明器的错误消息进一步完善形式证明的猜想。

    Large Language Models (LLMs) present an intriguing avenue for exploration in the field of formal theorem proving. Nevertheless, their full potential, particularly concerning the mitigation of hallucinations and refinement through prover error messages, remains an area that has yet to be thoroughly investigated. To enhance the effectiveness of LLMs in the field, we introduce the Lyra, a new framework that employs two distinct correction mechanisms: Tool Correction (TC) and Conjecture Correction (CC). To implement Tool Correction in the post-processing of formal proofs, we leverage prior knowledge to utilize predefined prover tools (e.g., Sledgehammer) for guiding the replacement of incorrect tools. Tool Correction significantly contributes to mitigating hallucinations, thereby improving the overall accuracy of the proof. In addition, we introduce Conjecture Correction, an error feedback mechanism designed to interact with prover to refine formal proof conjectures with prover error messa
    
[^25]: NLPBench：评估大型语言模型解决NLP问题

    NLPBench: Evaluating Large Language Models on Solving NLP Problems. (arXiv:2309.15630v1 [cs.CL])

    [http://arxiv.org/abs/2309.15630](http://arxiv.org/abs/2309.15630)

    NLPBench是一个评估大型语言模型解决NLP问题的基准数据集，为填补该领域的研究空白，作者收集了来自耶鲁大学期末考试的378个涵盖多个NLP主题的问题。该研究发现在使用高级提示策略时，大型语言模型的性能可能不稳定，并可能对较小的模型造成负面影响。

    

    大型语言模型（LLMs）的最新发展显示出增强自然语言处理（NLP）能力的潜力。尽管取得了一些成功，但在LLMs的NLP问题解决能力方面仍然缺乏专门的研究。为了填补这个领域的空白，我们提出了一个独特的基准数据集NLPBench，包括378个涵盖各种NLP主题的大学水平NLP问题，这些问题源自耶鲁大学以前的期末考试。NLPBench包括具有上下文的问题，其中多个子问题分享相同的公共信息，并且包括多选题、简答题和数学题等多种问题类型。我们的评估以GPT-3.5/4、PaLM-2和LLAMA-2等LLMs为中心，采用了诸如链式思维（CoT）和思维树（ToT）等高级提示策略。我们的研究揭示了高级提示策略的有效性可能不一致，有时会损害LLMs的性能，特别是在较小的模型（LLA）中。

    Recent developments in large language models (LLMs) have shown promise in enhancing the capabilities of natural language processing (NLP). Despite these successes, there remains a dearth of research dedicated to the NLP problem-solving abilities of LLMs. To fill the gap in this area, we present a unique benchmarking dataset, NLPBench, comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University's prior final exams. NLPBench includes questions with context, in which multiple sub-questions share the same public information, and diverse question types, including multiple choice, short answer, and math. Our evaluation, centered on LLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting strategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance, especially in smaller models like the LLA
    
[^26]: QA-LoRA: 基于量化意识的大语言模型低秩适应

    QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models. (arXiv:2309.14717v1 [cs.LG])

    [http://arxiv.org/abs/2309.14717](http://arxiv.org/abs/2309.14717)

    本文提出了QA-LoRA算法，它通过使用量化意识以及组内运算符来实现大语言模型的低秩适应。QA-LoRA能够将模型权重量化以减少时间和内存的使用，同时在不损失准确性的情况下将模型集成为一个量化模型。

    

    近年来，大型语言模型（LLMs）得到了快速发展。尽管在许多语言理解任务中具有强大的能力，但沉重的计算负担在很大程度上限制了LLMs的应用，特别是当需要将它们部署到边缘设备时。本文提出了一种基于量化意识的低秩适应（QA-LoRA）算法。动机在于量化和适应的自由度不平衡，解决方案是使用组内运算符，增加量化的自由度，同时减少适应的自由度。QA-LoRA可以用几行代码轻松实现，并使原始的LoRA具备了两个能力：（i）在微调过程中，LLM的权重被量化（例如转换为INT4），以减少时间和内存的使用；（ii）经过微调后，LLM和辅助权重自然地集成到一个量化模型中，而不会损失准确性。我们将QA-LoRA应用到LLaMA和LLaMA2模型家族中。

    Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model famil
    
[^27]: 通过定义问题测试探究大语言模型的道德发展

    Probing the Moral Development of Large Language Models through Defining Issues Test. (arXiv:2309.13356v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.13356](http://arxiv.org/abs/2309.13356)

    通过定义问题测试测量LLMs的道德推理能力，早期模型表现不佳，而ChatGPT、Llama2-Chat、PaLM-2和GPT-4在这方面表现出色，与成年人相当。然而，这些模型在不同困境下的表现存在差异。

    

    本研究使用定义问题测试(DIT)来测量LLMs的道德推理能力，DIT是一种心理测量工具，用于根据科尔伯格的认知道德发展模型来衡量个人的道德发展阶段。DIT使用道德困境，并要求被调查者根据道德考虑的重要性来判断和排序。研究结果显示，早期的LLMs（如GPT-3）在道德推理能力上并不比随机基线更好，而ChatGPT、Llama2-Chat、PaLM-2和GPT-4在这项任务上表现出明显更好的性能，可与成年人相媲美。实际上，GPT-4具有最高的后常规道德推理分数，相当于典型研究生的水平。然而，我们也观察到这些模型在所有困境上的表现并不一致。

    In this study, we measure the moral reasoning ability of LLMs using the Defining Issues Test - a psychometric instrument developed for measuring the moral development stage of a person according to the Kohlberg's Cognitive Moral Development Model. DIT uses moral dilemmas followed by a set of ethical considerations that the respondent has to judge for importance in resolving the dilemma, and then rank-order them by importance. A moral development stage score of the respondent is then computed based on the relevance rating and ranking.  Our study shows that early LLMs such as GPT-3 exhibit a moral reasoning ability no better than that of a random baseline, while ChatGPT, Llama2-Chat, PaLM-2 and GPT-4 show significantly better performance on this task, comparable to adult humans. GPT-4, in fact, has the highest post-conventional moral reasoning score, equivalent to that of typical graduate school students. However, we also observe that the models do not perform consistently across all dil
    
[^28]: MetaMath：为大型语言模型创建自己的数学问题

    MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. (arXiv:2309.12284v1 [cs.CL])

    [http://arxiv.org/abs/2309.12284](http://arxiv.org/abs/2309.12284)

    MetaMath是一种专门用于数学推理的微调语言模型，通过从多个角度重新编写问题来生成数学问题，并在两个基准测试中取得了优于其他开源语言模型的表现。

    

    大型语言模型（LLMs）推动了自然语言理解的极限，并展示了出色的问题解决能力。尽管取得了巨大的成功，但大多数现有的开源LLMs（例如LLaMA-2）在解决数学问题方面仍然远远不够令人满意，原因是复杂的推理过程。为了弥合这一鸿沟，我们提出了MetaMath，一种专门用于数学推理的微调语言模型。具体而言，我们通过在没有额外知识的情况下以多个角度重新写入问题来引导数学问题，从而产生了一个名为MetaMathQA的新数据集。然后我们在MetaMathQA上对LLaMA-2模型进行了微调。对于数学推理的两个流行基准测试（即GSM8K和MATH），实验结果表明MetaMath在性能上明显优于一套开源LLMs。我们的MetaMath-7B模型在GSM8K上达到了66.4％，在MATH上达到了19.4％，超过了相同规模的最先进模型。

    Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (\eg, LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose \emph{MetaMath}, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called {MetaMathQA}. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (\ie, GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves $66.4\%$ on GSM8K and $19.4\%$ on MATH, exceeding the state-of-the-art models of the same size by 
    
[^29]: 自主语言代理的开源框架：Agents

    Agents: An Open-source Framework for Autonomous Language Agents. (arXiv:2309.07870v1 [cs.CL])

    [http://arxiv.org/abs/2309.07870](http://arxiv.org/abs/2309.07870)

    Agents是一个开源框架，支持构建自主语言代理的各种功能，并提供用户友好的接口和对研究人员的扩展性。

    

    最近大型语言模型（LLMs）的高级进展使研究人员和开发人员能够构建自主语言代理，这些代理能够通过自然语言接口自动解决各种任务并与环境、人类和其他代理交互。我们将语言代理视为人工通用智能的有前途的方向，并发布Agents，一个开源库，旨在向更广泛的非专业人士开放这些进展。Agents经过精心设计，支持重要功能，包括规划、记忆、工具使用、多代理通信和细粒度的符号控制。Agents用户友好，使非专业人士能够在不需要编写太多代码的情况下构建、定制、测试、调优和部署最先进的自主语言代理。该库也对研究人员友好，其模块化设计使其易于扩展。

    Recent advances on large language models (LLMs) enable researchers and developers to build autonomous language agents that can automatically solve various tasks and interact with environments, humans, and other agents using natural language interfaces. We consider language agents as a promising direction towards artificial general intelligence and release Agents, an open-source library with the goal of opening up these advances to a wider non-specialist audience. Agents is carefully engineered to support important features including planning, memory, tool usage, multi-agent communication, and fine-grained symbolic control. Agents is user-friendly as it enables non-specialists to build, customize, test, tune, and deploy state-of-the-art autonomous language agents without much coding. The library is also research-friendly as its modularized design makes it easily extensible for researchers. Agents is available at https://github.com/aiwaves-cn/agents.
    
[^30]: RAIN: 您的语言模型可以自我调整而无需微调

    RAIN: Your Language Models Can Align Themselves without Finetuning. (arXiv:2309.07124v1 [cs.CL])

    [http://arxiv.org/abs/2309.07124](http://arxiv.org/abs/2309.07124)

    本研究提出了RAIN方法，该方法可以在无需微调或额外数据的情况下，通过整合自我评估和回滚机制实现对齐冻结的语言模型，使其能够直接产生与人类偏好一致的响应。

    

    大型语言模型（LLM）常常与人类偏好存在不一致性。之前的研究通过收集人类偏好数据，然后使用强化学习或指导调优等方法对预训练模型进行微调以实现对齐。相比之下，无需任何额外数据对齐冻结的LLM更有吸引力。本研究探讨了后一种情景的潜力。我们发现通过将自我评估和回滚机制整合在一起，不对齐的LLM可以通过自我增强直接产生与人类偏好一致的响应。我们引入了一种新的推理方法，可回滚的自回归推理（RAIN），它允许预训练的LLM评估自己的生成，并利用评估结果来引导向后回滚和向前生成以确保人工智能的安全性。值得注意的是，RAIN在模型对齐时无需额外数据，并且不需要任何训练、梯度计算或参数更新；在自我评估阶段，模型接收的是一些随机回滚的生成样本，并将其与人类偏好进行比较。

    Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, the so-called finetuning step. In contrast, aligning frozen LLMs without any extra data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide backward rewind and forward generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates; during the self-evaluation phase, the model recei
    
[^31]: 自然语言的元预测学习模型

    Meta predictive learning model of natural languages. (arXiv:2309.04106v1 [cs.CL])

    [http://arxiv.org/abs/2309.04106](http://arxiv.org/abs/2309.04106)

    该论文提出了一种基于预测编码框架的均场学习模型，通过假设突触权重遵循脉冲和斑点分布并只对分布进行训练，成功地应用于手写数字分类。

    

    基于自注意机制的大型语言模型不仅在自然语言本身上取得了令人惊讶的表现，而且在各种不同性质的任务中也表现出色。然而，关于语言处理，我们的人脑可能不是按照同样的原理运作。因此，关于大型语言模型采用的人工自我监督与脑计算之间的联系引起了一场辩论。在脑计算中最有影响力的假设之一是预测编码框架，该框架提出通过局部学习来最小化预测误差。然而，预测编码和相关的学分分配在语言处理中的作用仍然未知。在这里，我们提出了一个基于预测编码框架的均场学习模型，假设每个连接的突触权重遵循脉冲和斑点分布，只对分布进行训练。这种元预测学习在手写数字分类上得到了成功验证。

    Large language models based on self-attention mechanisms have achieved astonishing performances not only in natural language itself, but also in a variety of tasks of different nature. However, regarding processing language, our human brain may not operate using the same principle. Then, a debate is established on the connection between brain computation and artificial self-supervision adopted in large language models. One of most influential hypothesis in brain computation is the predictive coding framework, which proposes to minimize the prediction error by local learning. However, the role of predictive coding and the associated credit assignment in language processing remains unknown. Here, we propose a mean-field learning model within the predictive coding framework, assuming that the synaptic weight of each connection follows a spike and slab distribution, and only the distribution is trained. This meta predictive learning is successfully validated on classifying handwritten digi
    
[^32]: 针对大型语言模型的零资源幻觉预防

    Zero-Resource Hallucination Prevention for Large Language Models. (arXiv:2309.02654v1 [cs.CL])

    [http://arxiv.org/abs/2309.02654](http://arxiv.org/abs/2309.02654)

    本论文提出了一种零资源幻觉预防方法，通过评估模型对输入指令中概念的熟悉程度，在遇到不熟悉的概念时不生成响应，从而解决了大型语言模型中的幻觉问题。

    

    在各个领域中广泛使用大型语言模型(LLMs)引起了“幻觉”问题的关注，这指的是LLMs生成事实不准确或没有根据的信息的情况。现有的语言助手中幻觉检测技术依赖于复杂的模糊、基于自由语言的思维链条(CoT)技术或基于参数的方法，存在解释性问题。此外，识别生成后幻觉的方法无法预防其发生，并且由于指令格式和模型风格的影响，性能不一致。在本文中，我们介绍一种新颖的预检测自我评估技术，称为{\method}，它专注于评估模型对输入指令中概念的熟悉程度，并在遇到不熟悉的概念时不生成响应。这种方法模拟了人类能够在没有把握时不作回应的能力。

    The prevalent use of large language models (LLMs) in various domains has drawn attention to the issue of "hallucination," which refers to instances where LLMs generate factually inaccurate or ungrounded information. Existing techniques for hallucination detection in language assistants rely on intricate fuzzy, specific free-language-based chain of thought (CoT) techniques or parameter-based methods that suffer from interpretability issues. Additionally, the methods that identify hallucinations post-generation could not prevent their occurrence and suffer from inconsistent performance due to the influence of the instruction format and model style. In this paper, we introduce a novel pre-detection self-evaluation technique, referred to as {\method}, which focuses on evaluating the model's familiarity with the concepts present in the input instruction and withholding the generation of response in case of unfamiliar concepts. This approach emulates the human ability to refrain from respond
    
[^33]: 通过下采样的声学表示进行纯文本领域自适应的端到端语音识别

    Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation. (arXiv:2309.02459v1 [cs.SD])

    [http://arxiv.org/abs/2309.02459](http://arxiv.org/abs/2309.02459)

    本文提出了一种通过下采样声学表示来对齐文本模态的方法，以实现纯文本领域自适应的端到端语音识别。实验结果表明，该方法在新领域数据上取得了良好的效果。

    

    将两种形式的资料，声音和文本，映射到共享的表示空间中，是一种利用纯文本数据提高端到端自动语音识别(ASR)性能的研究课题。然而，声音和文本的表示长度不一致。虽然先前的方法通过上采样文本表示来与音频模态进行对齐，但可能不匹配预期的实际持续时间。在本文中，我们提出了通过下采样声学表示来与文本模态对齐的新型表示匹配策略。通过引入连续积分-火炮 (CIF) 模块生成与标记长度一致的声学表示，我们的ASR模型可以更好地从两种模态中学习统一的表示，从而能够使用目标领域的纯文本数据进行领域自适应。新领域数据的实验结果证明了所提方法的有效性。

    Mapping two modalities, speech and text, into a shared representation space, is a research topic of using text-only data to improve end-to-end automatic speech recognition (ASR) performance in new domains. However, the length of speech representation and text representation is inconsistent. Although the previous method up-samples the text representation to align with acoustic modality, it may not match the expected actual duration. In this paper, we proposed novel representations match strategy through down-sampling acoustic representation to align with text modality. By introducing a continuous integrate-and-fire (CIF) module generating acoustic representations consistent with token length, our ASR model can learn unified representations from both modalities better, allowing for domain adaptation using text-only data of the target domain. Experiment results of new domain data demonstrate the effectiveness of the proposed method.
    
[^34]: Wordle: 生活的缩影。幸运、技巧、作弊、忠诚和影响力！

    Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!. (arXiv:2309.02110v2 [math.HO] UPDATED)

    [http://arxiv.org/abs/2309.02110](http://arxiv.org/abs/2309.02110)

    Wordle是一款流行的在线单词游戏，玩家需要在6次猜测中猜出每日目标单词。通过收集玩家的数据，研究者发现每天约有0.2-0.5%的玩家能在一次尝试中解决谜题，展示了玩家们的技巧和运气。

    

    Wordle是一款由纽约时报提供的流行在线单词游戏。目前全球有大约200万名英文版本的玩家。玩家有6次机会来猜测每日的目标单词，并在每次猜测后，根据每个字母的位置和正确性，玩家会得到彩色编码的信息。在成功完成谜题或最后一次未成功的尝试之后，软件可以使用信息论来评估玩家的运气和技巧，并且可以显示随机抽样的所有玩家的第一次、第二次...第六次猜测的数据。最近，我发现后面的数据以一种方便复制粘贴到电子表格中的格式呈现出来。我从2023年5月至2023年8月收集了Wordle玩家的第一次猜测数据，并推断出一些有趣的有关Wordle玩家的信息。A）每天约有0.2-0.5%的玩家在一次尝试中解决了谜题。由于猜对2315个位置上的单词的几率为1/2315，这一比例显示了玩家们在游戏中的技巧和运气。

    Wordle is a popular, online word game offered by the New York Times (nytimes.com). Currently there are some 2 million players of the English version worldwide. Players have 6 attempts to guess the daily word (target word) and after each attempt, the player receives color-coded information about the correctness and position of each letter in the guess. After either a successful completion of the puzzle or the final unsuccessful attempt, software can assess the player's luck and skill using Information Theory and can display data for the first, second, ..., sixth guesses of a random sample of all players. Recently, I discovered that the latter data is presented in a format that can easily be copied and pasted into a spreadsheet. I compiled data on Wordle players' first guesses from May 2023 - August 2023 and inferred some interesting information about Wordle players. A) Every day, about 0.2-0.5% of players solve the puzzle in one attempt. Because the odds of guessing the one of 2,315 pos
    
[^35]: SememeASR：用语义知识提升与领域和长尾数据转移抗性的端到端语音识别的性能

    SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge. (arXiv:2309.01437v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2309.01437](http://arxiv.org/abs/2309.01437)

    SememeASR是一种基于语义知识的语音识别方法，通过引入语义单元——sememe的信息，可以提高语音识别的效果，并且改善对长尾数据的识别和领域泛化能力。

    

    近年来，语音识别取得了显著进展。然而，纯数据驱动方法在领域不匹配和长尾数据的问题上仍然有困难。考虑到基于知识驱动的方法可以帮助数据驱动的方法减轻其缺陷，我们引入了基于语义单元——sememe的语义知识信息到语音识别中（SememeASR）。Sememe根据语言学的定义是语言中的最小语义单位，并且能够很好地代表每个单词背后的隐含语义信息。我们的实验表明，引入sememe信息可以提高语音识别的效果。此外，我们的进一步实验显示，sememe知识可以提高模型对长尾数据的识别能力，并增强模型的领域泛化能力。

    Recently, excellent progress has been made in speech recognition. However, pure data-driven approaches have struggled to solve the problem in domain-mismatch and long-tailed data. Considering that knowledge-driven approaches can help data-driven approaches alleviate their flaws, we introduce sememe-based semantic knowledge information to speech recognition (SememeASR). Sememe, according to the linguistic definition, is the minimum semantic unit in a language and is able to represent the implicit semantic information behind each word very well. Our experiments show that the introduction of sememe information can improve the effectiveness of speech recognition. In addition, our further experiments show that sememe knowledge can improve the model's recognition of long-tailed data and enhance the model's domain generalization ability.
    
[^36]: SpikeBERT：一种采用两阶段BERT知识蒸馏训练的语言Spikformer

    SpikeBERT: A Language Spikformer Trained with Two-Stage Knowledge Distillation from BERT. (arXiv:2308.15122v1 [cs.CL])

    [http://arxiv.org/abs/2308.15122](http://arxiv.org/abs/2308.15122)

    该论文提出了一种名为SpikeBERT的SNN模型，通过改进Spikformer架构和使用两阶段知识蒸馏方法，该模型在语言任务上超越了其他SNN模型，在文本分类任务上甚至达到了与BERT相当的结果。

    

    脉冲神经网络（SNN）在以更节能的方式实现深度神经网络方面提供了一个有前景的途径。然而，现有的用于语言任务的SNN网络架构过于简单，深度架构尚未得到充分探索，与BERT等主流基于Transformer的网络相比，存在显著的性能差距。为此，我们改进了最近提出的脉冲Transformer（即Spikformer），使其能够处理语言任务，并提出了一种两阶段知识蒸馏方法来训练它，该方法结合了通过从BERT和大量未标记文本中蒸馏知识进行预训练，并通过再次从在相同训练示例上对BERT进行微调，并进行任务特定实例知识蒸馏。通过大量实验，我们展示了使用我们的方法训练的模型，命名为SpikeBERT，在实现上超过了最先进的SNN，并且甚至在文本分类任务上达到了与BERT相当的结果。

    Spiking neural networks (SNNs) offer a promising avenue to implement deep neural networks in a more energy-efficient way. However, the network architectures of existing SNNs for language tasks are too simplistic, and deep architectures have not been fully explored, resulting in a significant performance gap compared to mainstream transformer-based networks such as BERT. To this end, we improve a recently-proposed spiking transformer (i.e., Spikformer) to make it possible to process language tasks and propose a two-stage knowledge distillation method for training it, which combines pre-training by distilling knowledge from BERT with a large collection of unlabelled texts and fine-tuning with task-specific instances via knowledge distillation again from the BERT fine-tuned on the same training examples. Through extensive experimentation, we show that the models trained with our method, named SpikeBERT, outperform state-of-the-art SNNs and even achieve comparable results to BERTs on text 
    
[^37]: 作为用户模拟器的大型语言模型

    Large Language Model as a User Simulator. (arXiv:2308.11534v1 [cs.CL])

    [http://arxiv.org/abs/2308.11534](http://arxiv.org/abs/2308.11534)

    本文创新性地将从真实人机对话中提取的人类问题作为学习目标，并且训练了一个用户模拟器UserGPT，并使用生成的高质量合成对话数据集RealChat来训练助手模型ReaLM。实验证明，ReaLM在多个基准测试中超过了基准模型。

    

    闭源ChatGPT的卓越性能引发了对其民主化的努力，借助真实用户和ChatGPT对话的努力取得了显著进展，Vicuna是一个很好的例子。然而，目前的Baize和UltraChat等努力主要依靠ChatGPT根据指令模拟人类行为，而不是真实的人类学习，导致范围有限，多样性减弱，缺乏真正的多轮对话动态。为了解决上述问题，我们创新性地把从真实人机对话中提取的人类问题作为学习目标，并训练一个用户模拟器UserGPT来生成高质量的以人为中心的合成对话数据集RealChat。随后，该数据集训练我们的助手模型ReaLM。实验证明，ReaLM在Vicuna-Bench和MT-Bench中均超过了基准模型。

    The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT conversations, as evidenced by Vicuna. However, while current endeavors like Baize and UltraChat aim to auto-generate conversational data due to challenges in gathering human participation, they primarily rely on ChatGPT to simulate human behaviors based on directives rather than genuine human learning. This results in a limited scope, diminished diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we innovatively target human questions extracted from genuine human-machine conversations as a learning goal and train a user simulator, UserGPT, to produce a high-quality human-centric synthetic conversation dataset, RealChat. Subsequently, this dataset trains our assistant model, ReaLM. Experimentally, ReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise
    
[^38]: AgentVerse: 促进多智能体协作和探索 emergent behaviors。

    AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. (arXiv:2308.10848v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10848](http://arxiv.org/abs/2308.10848)

    AgentVerse提出了一个多智能体框架，可以通过协同和动态调整合作团队的组成，实现超越单个智能体的性能。在合作任务中，该框架能够引发出群体内个体智能体之间的社会行为，从而提高多智能体团队的协作潜力。

    

    由大型语言模型（LLMs）赋能的自主智能体已经取得了显著进展，使它们能够在广泛的任务范围内进行泛化。然而，在现实世界的情境中，个体之间的合作经常需要以增强任务完成的效率和效果。因此，受到人类群体动力学的启发，我们提出了一个多智能体框架，可以作为一个大于其各个部分之和的系统进行协同和动态调整。我们的实验表明，这个框架能够有效地部署超过单个智能体的多智能体组，并取得更好的表现。此外，我们深入研究了在合作任务完成过程中，群体内个体智能体之间出现的社会行为的涌现。基于这些行为，我们讨论了一些可能的策略，以利用积极的行为，并减轻消极的行为，以提高多智能体组的协作潜力。我们的 \framework 框架的代码将很快公布。

    Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework \framework that can collaboratively and dynamically adjust its composition as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that \framework framework can effectively deploy multi-agent groups that outperform a single agent. Furthermore, we delve into the emergence of social behaviors among individual agents within a group during collaborative task accomplishment. In view of these behaviors, we discuss some possible strategies to leverage positive ones and mitigate negative ones for improving the collaborative potential of multi-agent groups. Our codes for \framework will soon be
    
[^39]: 大型语言模型的指令调优：一项调研

    Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10792](http://arxiv.org/abs/2308.10792)

    本文调查了指令调优这一关键技术在增强大型语言模型能力和可控性方面的研究工作，包括方法、数据集构建、模型训练和应用，以及对结果影响的分析。同时回顾了可能的问题和批评，并指出了目前的不足。

    

    本文调查了指令调优（IT）这一快速发展的领域中的研究工作，这是一种增强大型语言模型（LLM）能力和可控性的关键技术。指令调优是指以监督方式在包含“指令-输出”对的数据集上进一步训练LLM，这将LLM的下一个词预测目标与用户希望LLM遵守人类指令的目标之间的差距。本文对IT的常规方法、IT数据集的构建、IT模型的训练以及应用于不同模态、领域和应用的情况进行了系统的文献综述，并对影响IT结果的各个方面进行了分析（例如，指令输出的生成、指令数据集的大小等）。我们还回顾了IT的潜在问题以及针对其的批评，以及指出当前不足的努力。

    This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies 
    
[^40]: 通过语言对齐将大型语言模型推广到非英语中

    Extrapolating Large Language Models to Non-English by Aligning Languages. (arXiv:2308.04948v1 [cs.CL])

    [http://arxiv.org/abs/2308.04948](http://arxiv.org/abs/2308.04948)

    本文介绍了一种通过语义对齐来增强非英语语言模型的方法，并通过实验证明，该方法在跨语言任务中取得了显著的改进。同时，我们还发现在翻译数据中加入非英语文本可以有效提升模型能力，且LLM内部的语义对齐也可以进一步加强。

    

    由于训练数据分布不均衡，大型语言模型（LLM）往往在语言能力上偏向英语。本文提出通过构建跨语言的语义对齐，来增强预训练的非英语语言模型的能力。我们使用翻译任务数据和跨语言通用任务数据对LLaMA进行指令调整，得到跨语言模型（x-LLaMA）。在跨语言基准XQUAD和MLQA上的实验结果表明，x-LLaMA模型在六种非英语语言上平均超过英语指令调整的模型（Alpaca）42.50%。在中文基准C-Eval上的进一步实验表明，x-LLaMA在中文人文任务上取得了显著改进，超过Alpaca 8.2%。此外，我们发现在翻译数据的目标端加入非英语文本可以有效提升非英语能力。另外，我们还发现LLM内部的语义对齐可以进一步加强。

    Due to the unbalanced training data distribution, the language ability of large language models (LLMs) is often biased towards English. In this paper, we propose to empower pre-trained LLMs on non-English languages by building semantic alignment across languages. We perform instruction-tuning on LLaMA with both translation task data and cross-lingual general task data to obtain cross-lingual models (x-LLaMA). Experiment results on cross-lingual benchmark XQUAD and MLQA show that x-LLaMA models outperform the English instruction-tuned counterpart (Alpaca) by 42.50% on average on six non-English languages. Further experiments on Chinese benchmark C-Eval show that x-LLaMA achieves significant improvement on Chinese humanities tasks, outperforming Alpaca by 8.2%. We also discover that incorporating non-English text on the target side of translation data is particularly effective for boosting non-English ability. Besides, we find that semantic alignment within LLM can be further strengthene
    
[^41]: 更多上下文，更少干扰：通过推断和调节上下文属性进行视觉分类

    More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v1 [cs.CV])

    [http://arxiv.org/abs/2308.01313](http://arxiv.org/abs/2308.01313)

    本文借鉴了人类视觉感知过程，提出了一种通过推断和调节上下文属性来改进零样本图像分类的方法。通过给CLIP提供上下文属性，可以减轻对虚假特征的依赖，进而提高零样本分类的准确性。

    

    CLIP作为一种基础的视觉语言模型，由于其理解各种视觉概念和自然语言描述的能力，被广泛应用于零样本图像分类。然而，如何充分利用CLIP的前所未有的人类般理解能力来实现更好的零样本分类仍然是一个开放问题。本文从人类的视觉感知过程中得到启发：现代神经科学观点认为，在对物体进行分类时，人类首先推断其与类别无关的属性（如背景和方向），这有助于将前景对象与背景区分开来，然后以此信息为基础进行决策。受此启发，我们观察到为CLIP提供上下文属性可以改善零样本分类并减轻对虚假特征的依赖。我们还观察到CLIP本身可以合理地从图像中推断出这些属性。基于这些观察，我们提出了一种零训练、两步骤的零样本分类方法。

    CLIP, as a foundational vision language model, is widely used in zero-shot image classification due to its ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better zero-shot classification is still an open question. This paper draws inspiration from the human visual perception process: a modern neuroscience view suggests that in classifying an object, humans first infer its class-independent attributes (e.g., background and orientation) which help separate the foreground object from the background, and then make decisions based on this information. Inspired by this, we observe that providing CLIP with contextual attributes improves zero-shot classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot cl
    
[^42]: 大型语言模型的私密水印

    A Private Watermark for Large Language Models. (arXiv:2307.16230v1 [cs.CL])

    [http://arxiv.org/abs/2307.16230](http://arxiv.org/abs/2307.16230)

    这项工作提出了一种私密水印算法，通过使用两个不同的神经网络进行水印生成和检测，并共享部分参数，实现了高效且高准确性的检测，同时对生成和检测速度影响最小。

    

    最近，针对大型语言模型（LLMs）的文本水印算法已经减轻了LLMs生成的文本可能带来的伪新闻和版权问题。然而，当前文本水印算法的水印检测需要生成过程的密钥，使其容易受到违规和伪造的影响。在这项工作中，我们提出了第一个私密水印算法，通过在水印生成和检测阶段使用两个不同的神经网络而不是使用相同的密钥来扩展当前的文本水印算法。同时，水印生成和检测网络的部分参数是共享的，这使得检测网络能够以高效的方式实现高准确性。实验证明，由于两个网络的参数规模较小，我们的算法确保了高的检测准确性，并对生成和检测速度的影响最小。

    Recently, text watermarking algorithms for large language models (LLMs) have been mitigating the potential harms of text generated by the LLMs, including fake news and copyright issues. However, the watermark detection of current text algorithms requires the key from the generation process, making them susceptible to breaches and counterfeiting. In this work, we propose the first private watermarking algorithm, which extends the current text watermarking algorithms by using two different neural networks respectively for watermark generation and detection, rather than using the same key at both stages. Meanwhile, part of the parameters of the watermark generation and detection networks are shared, which makes the detection network achieve a high accuracy very efficiently. Experiments show that our algorithm ensures high detection accuracy with minimal impact on generation and detection speed, due to the small parameter size of both networks. Additionally, our subsequent analysis demonst
    
[^43]: 思维的骨架：大型语言模型可以进行并行解码

    Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding. (arXiv:2307.15337v1 [cs.CL])

    [http://arxiv.org/abs/2307.15337](http://arxiv.org/abs/2307.15337)

    本研究提出了一种名为“思维的骨架”的方法，可以通过并行解码来减少大型语言模型的生成延迟。这种方法不仅显著提高了速度，还可以潜在地提高答案质量。

    

    本研究旨在减少大型语言模型（LLMs）的端到端生成延迟。高生成延迟的一个主要原因是几乎所有最先进的LLMs都采用了顺序解码方法。在本研究中，受到人类的思考和写作过程的启发，我们提出了“思维的骨架”（SoT），它指导LLMs首先生成答案的骨架，然后通过并行API调用或批量解码来并行完成每个骨架点的内容。SoT不仅显著提高了速度（在11个不同的LLMs上提高了最多2.39倍），而且还可以潜在地提高在多个问题类别上的答案质量，包括多样性和相关性。SoT是一种针对效率的数据导向优化的初步尝试，并揭示了将LLMs推动更像人类思考以提高答案质量的潜力。

    This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose "Skeleton-of-Thought" (SoT), which guides LLMs to first generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-up (up to 2.39x across 11 different LLMs), but it can also potentially improve the answer quality on several question categories in terms of diversity and relevance. SoT is an initial attempt at data-centric optimization for efficiency, and reveal the potential of pushing LLMs to think more like a human for answer quality.
    
[^44]: 将注意力分割与绑定用于改进生成语义护理

    Divide & Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v1 [cs.CV])

    [http://arxiv.org/abs/2307.10864](http://arxiv.org/abs/2307.10864)

    本论文提出了一种名为"分割与绑定"的方法，旨在改进生成语义护理的效果。该方法引入了新的损失目标，包括关注丢失和绑定丢失，以解决复杂提示和不适当属性绑定的问题。

    

    新兴的大规模文本到图像生成模型，如稳定扩散（SD），展示了高度逼真的压倒性结果。尽管取得了巨大的进展，但当前最先进的模型仍然难以完全依照输入提示生成图像。先前的研究——关注与激发，引入了生成语义护理（GSN）的概念，旨在在推断时优化跨注意力以更好地融入语义。它在生成简单提示，如“一只猫和一只狗”，方面展示了有希望的结果。然而，它在处理更复杂的提示以及解决不适当的属性绑定问题方面的功效有所下降。为了应对复杂提示或涉及多个实体的场景所带来的挑战，并实现改进的属性绑定，我们提出了分割与绑定。我们引入了两个新的GSN损失目标：一种新的关注丢失和一种绑定丢失。我们的方法在其能够更好地将语义纳入图像生成过程中的特点上脱颖而出。

    Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend & Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide & Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to fa
    
[^45]: 将关注点转移到相关性上: 探索大型语言模型的不确定性估计

    Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models. (arXiv:2307.01379v1 [cs.CL])

    [http://arxiv.org/abs/2307.01379](http://arxiv.org/abs/2307.01379)

    本论文研究了大型语言模型（LLMs）自动生成的关键词不平等问题，发现在估计不确定性时，重要的令牌和含有有限语义的句子被同等或更加重视。为了解决这个问题，提出了共同转移关注点来更好地估计不确定性。

    

    虽然大型语言模型（LLMs）在自然语言生成方面表现出了巨大的潜力，但是对于模型生成的不确定性的特征化仍然具有挑战性，即用户何时可以信任模型的输出。我们的研究基于一些启发性的事实，即在自回归的LLMs中，令牌在反映生成的含义方面是不平等的，即一些令牌比其他令牌更相关（或更具代表性），然而在估计不确定性时所有的令牌被等值对待。这是由于语言冗余，其中大部分情况下，只需要几个关键词就足以传达一个长句的含义。我们将这些不平等称为生成的不平等，并研究它们如何影响不确定性的估计。我们的结果揭示，相当数量的令牌和包含有限语义的句子，在估计不确定性时被同等或甚至更加重视。为了解决由生成的不平等引起的这些偏差，我们提出了共同转移关注点来更好地估计不确定性。

    Although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more
    
[^46]: ContextSpeech：用于段落阅读的富有表现力和高效的文本转语音系统

    ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading. (arXiv:2307.00782v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.00782](http://arxiv.org/abs/2307.00782)

    本研究提出了一种轻量级但有效的文本转语音系统ContextSpeech，通过设计内存缓存的循环机制和构建层次化的文本语义结构，将全局文本和语音上下文融入到句子编码中，以解决段落阅读中的语音生成挑战，并且使用线性化的自注意力机制提高了模型的效率。

    

    尽管最先进的文本转语音系统可以在句子级别上生成非常高质量的自然语音，但它们在段落/长篇阅读的语音生成方面仍面临巨大挑战。这些不足之处主要是因为：一是忽视了跨句子的上下文信息，二是长篇合成过程中的高计算和内存成本。为了解决这些问题，本研究提出了一种轻量级但有效的文本转语音系统ContextSpeech。具体而言，我们首先设计了一个内存缓存的循环机制，将全局文本和语音上下文融入到句子编码中；然后构建了层次化的文本语义结构，以扩大全局上下文增强的范围；此外，我们还整合了线性化的自注意力机制来提高模型的效率。实验证明，ContextSpeech在段落阅读中显著提升了语音质量和韵律表达能力，同时具备竞争力的模型效率。音频样本可访问链接：https://contextspeech.

    While state-of-the-art Text-to-Speech systems can generate natural speech of very high quality at sentence level, they still meet great challenges in speech generation for paragraph / long-form reading. Such deficiencies are due to i) ignorance of cross-sentence contextual information, and ii) high computation and memory cost for long-form synthesis. To address these issues, this work develops a lightweight yet effective TTS system, ContextSpeech. Specifically, we first design a memory-cached recurrence mechanism to incorporate global text and speech context into sentence encoding. Then we construct hierarchically-structured textual semantics to broaden the scope for global context enhancement. Additionally, we integrate linearized self-attention to improve model efficiency. Experiments show that ContextSpeech significantly improves the voice quality and prosody expressiveness in paragraph reading with competitive model efficiency. Audio samples are available at: https://contextspeech.
    
[^47]: 从原始的GP笔记中挖掘知识图谱，用于远程COVID-19初级保健评估

    RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19 Assessment in Primary Care. (arXiv:2306.17175v1 [cs.CL])

    [http://arxiv.org/abs/2306.17175](http://arxiv.org/abs/2306.17175)

    本研究提出了一个从原始GP笔记中提取信息并构建知识图谱的框架，用于解决临床决策过程中现有技术无法处理的问题。

    

    临床决策是向患者提供适当护理的基本阶段。近年来，为了帮助临床医生在这个过程中做出决策，已经开发了几个决策系统。然而，目前使用的技术解决方案基于简单的回归模型，只能考虑简单的预定义多选特征，如患者年龄、既往病史、吸烟者状况等。决策系统当前无法处理的一个特定患者数据来源是患者会诊的GP笔记的收集。这些笔记包含了临床医生用来做出最终决策并将患者引导到适当护理的关键体征和症状。从GP笔记中提取信息是一个技术上具有挑战性的问题，因为它们往往包含缩写、打字错误和不完整的句子。本文解决了这个公开挑战。我们提出了一个框架，可以执行从原始GP笔记中提取出关键信息，并构建知识图谱的任务。

    Clinical decision-making is a fundamental stage in delivering appropriate care to patients. In recent years several decision-making systems designed to aid the clinician in this process have been developed. However, technical solutions currently in use are based on simple regression models and are only able to take into account simple pre-defined multiple-choice features, such as patient age, pre-existing conditions, smoker status, etc. One particular source of patient data, that available decision-making systems are incapable of processing is the collection of patient consultation GP notes. These contain crucial signs and symptoms - the information used by clinicians in order to make a final decision and direct the patient to the appropriate care. Extracting information from GP notes is a technically challenging problem, as they tend to include abbreviations, typos, and incomplete sentences.  This paper addresses this open challenge. We present a framework that performs knowledge grap
    
[^48]: 嵌入融合的艺术：优化仇恨言论检测

    The Art of Embedding Fusion: Optimizing Hate Speech Detection. (arXiv:2306.14939v1 [cs.CL])

    [http://arxiv.org/abs/2306.14939](http://arxiv.org/abs/2306.14939)

    这项工作研究了优化仇恨言论检测的方法。研究发现，尽管嵌入的组合会略微提高性能，但计算成本很高，并且组合方式对结果的影响较小。

    

    仇恨言论检测是一项具有挑战性的自然语言处理任务，需要捕捉语言和语境细微差别。预训练语言模型（PLMs）提供了丰富的文本语义表示，可以改进这个任务。然而，对于有效地组合PLMs的表示和利用它们的互补优势的方法还知之甚少。在这项工作中，我们揭示了几种PLMs组合技术的方式，并全面分析了它们的有效性。我们的研究结果表明，组合嵌入可以略微改善性能，但计算成本较高，组合方式对最终结果的影响较小。我们还在https://github.com/aflah02/The-Art-of-Embedding-Fusion-Optimizing-Hate-Speech-Detection上公开了我们的代码库。

    Hate speech detection is a challenging natural language processing task that requires capturing linguistic and contextual nuances. Pre-trained language models (PLMs) offer rich semantic representations of text that can improve this task. However there is still limited knowledge about ways to effectively combine representations across PLMs and leverage their complementary strengths. In this work, we shed light on various combination techniques for several PLMs and comprehensively analyze their effectiveness. Our findings show that combining embeddings leads to slight improvements but at a high computational cost and the choice of combination has marginal effect on the final outcome. We also make our codebase public at https://github.com/aflah02/The-Art-of-Embedding-Fusion-Optimizing-Hate-Speech-Detection .
    
[^49]: 一种新颖的基于对照的方面情感分析方法

    A novel Counterfactual method for aspect-based sentiment analysis. (arXiv:2306.11260v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.11260](http://arxiv.org/abs/2306.11260)

    本文提出了一种新颖的基于对照的数据增强方法，用于生成具有相反情感极性的观点表达，并利用T5模型检索掩码，该方法在三个ABSA数据集上表现优于当前增强方法。

    

    方面情感分析 (ABSA) 是一项细粒度的情感评估任务，它分析评估方面的情感极性。然而，以往的工作仅关注观点表达的识别，而忽略了观点表达的多样性对ABSA任务的影响。为解决这个问题，我们提出了一种新颖的基于对照的数据增强方法，用于生成具有相反情感极性的观点表达。具体而言，我们使用集成梯度来识别和屏蔽观点表达。然后将反向标签的提示组合到原始文本中，并最终利用预训练语言模型 (PLM) T5 来检索掩码。实验结果表明，所提出的对照数据增强方法在三个ABSA数据集 (即Laptop，Restaurant和MAMS) 上的表现优于当前的增强方法。

    Aspect-based-sentiment-analysis (ABSA) is a fine-grained sentiment evaluation task, which analyze the emotional polarity of the evaluation aspects. However, previous works only focus on the identification of opinion expressions, forget that the diversity of opinion expressions also has great impacts on the ABSA task. To mitigate this problem, we propose a novel counterfactual data augmentation method to generate opinion expression with reversed sentiment polarity. Specially, the integrated gradients are calculated to identify and mask the opinion expression. Then, a prompt with the reverse label is combined to the original text, and a pre-trained language model (PLM), T5, is finally employed to retrieve the masks. The experimental results show the proposed counterfactual data augmentation method perform better than current augmentation methods on three ABSA datasets, i.e. Laptop, Restaurant and MAMS.
    
[^50]: 推动 ChatGPT 在自然语言处理任务上的极限

    Pushing the Limits of ChatGPT on NLP Tasks. (arXiv:2306.09719v1 [cs.CL])

    [http://arxiv.org/abs/2306.09719](http://arxiv.org/abs/2306.09719)

    本研究提出了一系列通用模块以解决 ChatGPT 在自然语言处理任务中的弱点，包括利用多个提示符来适应更多演示、使用精细调整模型以获得更好的演示检索、转换任务为更适合生成性质的格式以及采用针对 NLP 任务设计的推理策略。

    

    尽管 ChatGPT 取得了成功，但在大多数自然语言处理任务上，其表现仍远低于基线模型。本研究探究了其中的原因，发现其表现欠佳的原因主要有：（1）提示符中的令牌限制不允许充分利用监督数据集；（2）ChatGPT 生成性质与 NLP 任务之间存在不匹配；（3）基于语言模型的固有弱点，如产生幻觉、过度关注特定关键词等。本研究提出了一系列通用模块以解决这些问题，旨在推动 ChatGPT 在 NLP 任务上的极限。我们提出的模块包括：（1）一种输入多提示的策略，使用多个提示符来适应更多演示；（2）使用精细调整模型以获得更好的演示检索；（3）将任务转换为更适合生成性质的格式；（4）采用针对 NLP 任务设计的推理策略。

    Despite the success of ChatGPT, its performances on most NLP tasks are still well below the supervised baselines. In this work, we looked into the causes, and discovered that its subpar performance was caused by the following factors: (1) token limit in the prompt does not allow for the full utilization of the supervised datasets; (2) mismatch between the generation nature of ChatGPT and NLP tasks; (3) intrinsic pitfalls of LLMs models, e.g., hallucination, overly focus on certain keywords, etc.  In this work, we propose a collection of general modules to address these issues, in an attempt to push the limits of ChatGPT on NLP tasks. Our proposed modules include (1) a one-input-multiple-prompts strategy that employs multiple prompts for one input to accommodate more demonstrations; (2) using fine-tuned models for better demonstration retrieval; (3) transforming tasks to formats that are more tailored to the generation nature; (4) employing reasoning strategies that are tailored to addr
    
[^51]: PEACE: 跨平台仇恨言论检测- 一个因果指导的框架

    PEACE: Cross-Platform Hate Speech Detection- A Causality-guided Framework. (arXiv:2306.08804v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.08804](http://arxiv.org/abs/2306.08804)

    PEACE是一个跨平台仇恨言论检测的因果指导框架，通过学习源平台数据并推广到目标平台，探索如何建立适用于不同平台的通用仇恨言论检测模型。

    

    仇恨言论检测是指检测基于宗教、性别、性取向或其他特征而针对个人或群体的恶意内容的任务。由于不同平台的政策不同，不同群体以不同方式表达仇恨言论。此外，由于某些平台缺乏标记数据，构建仇恨言论检测模型变得具有挑战性。为此，我们重新审视是否可以学习一个适用于跨平台设置的可迁移仇恨言论检测模型，即我们在一个（源）平台的数据上训练模型，并将模型推广到多个（目标）平台。现有的推广模型依赖于语言线索或辅助信息，使其偏向于源平台上的某些标签或某些类型的词（如辱骂性词语），因此不适用于目标平台。受到社会和心理理论的启发，我们努力探索是否存在一种因果关系，可以从源平台学习特征并推广到目标平台。

    Hate speech detection refers to the task of detecting hateful content that aims at denigrating an individual or a group based on their religion, gender, sexual orientation, or other characteristics. Due to the different policies of the platforms, different groups of people express hate in different ways. Furthermore, due to the lack of labeled data in some platforms it becomes challenging to build hate speech detection models. To this end, we revisit if we can learn a generalizable hate speech detection model for the cross platform setting, where we train the model on the data from one (source) platform and generalize the model across multiple (target) platforms. Existing generalization models rely on linguistic cues or auxiliary information, making them biased towards certain tags or certain kinds of words (e.g., abusive words) on the source platform and thus not applicable to the target platforms. Inspired by social and psychological theories, we endeavor to explore if there exist in
    
[^52]: 自然语言处理中的表示实践

    Operationalising Representation in Natural Language Processing. (arXiv:2306.08193v1 [cs.CL])

    [http://arxiv.org/abs/2306.08193](http://arxiv.org/abs/2306.08193)

    本文介绍了一个框架，通过使用探测分类器来评估组件是否表示属性，填补了自然语言处理中关于“表示”的哲学空白。

    

    尽管“表示”在认知科学哲学中具有核心地位，但在当代自然语言处理实践中，几乎没有哲学领域的先前研究与之涉及。本文旨在填补这一空白：结合认知科学的思想，提出了一个框架来评估神经自然语言处理模型组件所作出的表示性声明，并提出三个评估组件是否表示属性的标准，并使用探测分类器来实现这些标准的操作化，探测分类器是NLP（和更广泛的深度学习）中流行的分析技术。操作化一个在哲学上受到启发的“表示”概念的项目应该引起科学哲学家和自然语言处理实践者的兴趣。对于哲学家来说，这提供了一个测试有关表示的本质的论据的新颖场地，并帮助NLPers组织有关探测实验的大量文献，提出了新的经验研究方向。

    Despite its centrality in the philosophy of cognitive science, there has been little prior philosophical work engaging with the notion of representation in contemporary NLP practice. This paper attempts to fill that lacuna: drawing on ideas from cognitive science, I introduce a framework for evaluating the representational claims made about components of neural NLP models, proposing three criteria with which to evaluate whether a component of a model represents a property and operationalising these criteria using probing classifiers, a popular analysis technique in NLP (and deep learning more broadly).  The project of operationalising a philosophically-informed notion of representation should be of interest to both philosophers of science and NLP practitioners. It affords philosophers a novel testing-ground for claims about the nature of representation, and helps NLPers organise the large literature on probing experiments, suggesting novel avenues for empirical research.
    
[^53]: Valley: 大型语言模型增强视频助手

    Valley: Video Assistant with Large Language model Enhanced abilitY. (arXiv:2306.07207v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.07207](http://arxiv.org/abs/2306.07207)

    本文介绍了一个名为Valley的视频助手，它是一个以大型语言模型增强的多模态基础模型，能够在一个通用框架内理解视频、图像和语言。

    

    大型语言模型(LLMs)以其卓越的会话能力，在各种应用中表现出色，并成为强大的AI助手。鉴于此，一个直观的问题是：我们能否利用LLMs的能力构建多模态的视觉应用AI助手？最近，已经开发了几个多模态模型来实现这个目的。它们通常预先训练一个适应模块来对齐视觉编码器和语言模型的语义，然后在指令跟随数据上进行微调。然而，尽管这个流程在图像和语言理解方面取得了成功，在视频和语言理解方面的有效性还没有得到广泛探索。在本文中，我们旨在开发一个能够在一个通用框架内理解视频、图像和语言的新型多模态基础模型。为了实现这一目标，我们引入了Valley，一个以大型语言模型增强的视频助手。

    Large language models (LLMs), with their remarkable conversational capabilities, have demonstrated impressive performance across various applications and have emerged as formidable AI assistants. In view of this, it raises an intuitive question: Can we harness the power of LLMs to build multimodal AI assistants for visual applications? Recently, several multi-modal models have been developed for this purpose. They typically pre-train an adaptation module to align the semantics of the vision encoder and language model, followed by fine-tuning on instruction-following data. However, despite the success of this pipeline in image and language understanding, its effectiveness in joint video and language understanding has not been widely explored. In this paper, we aim to develop a novel multi-modal foundation model capable of comprehending video, image, and language within a general framework. To achieve this goal, we introduce Valley, a Video Assistant with Large Language model Enhanced ab
    
[^54]: 使用基于认识不确定性的数据选择来适应预训练的ASR模型以应对低资源临床语音问题

    Adapting Pretrained ASR Models to Low-resource Clinical Speech using Epistemic Uncertainty-based Data Selection. (arXiv:2306.02105v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02105](http://arxiv.org/abs/2306.02105)

    本研究使用基于认识不确定性的数据选择方法来减少非洲口音临床ASR训练的注释成本。结果表明，这种方法可以超过现有的基准结果，并提高低资源口音的泛化能力。

    

    尽管ASR取得了显著进展，但由于缺乏训练数据集，对于非洲口音的临床ASR的研究还不够。在这一领域构建强大的ASR系统需要大量的标注数据，用于各种语言和形态丰富的口音，但这些数据的创建成本较高。本研究旨在通过基于信息不确定性的数据选择来减少注释费用。我们表明，将认识不确定性纳入我们的自适应过程中，可以超过使用最先进（SOTA）ASR模型建立的几个基准结果，同时减少所需的标记数据量，从而降低注释成本。我们的方法还改善了低资源口音的超出分布泛化能力，展示了我们的方法在非洲临床ASR的背景下构建泛化型ASR模型的可行性，而在这种情况下，训练数据集主要是稀缺的。

    While there has been significant progress in ASR, African-accented clinical ASR has been understudied due to a lack of training datasets. Building robust ASR systems in this domain requires large amounts of annotated or labeled data, for a wide variety of linguistically and morphologically rich accents, which are expensive to create. Our study aims to address this problem by reducing annotation expenses through informative uncertainty-based data selection. We show that incorporating epistemic uncertainty into our adaptation rounds outperforms several baseline results, established using state-of-the-art (SOTA) ASR models, while reducing the required amount of labeled data, and hence reducing annotation costs. Our approach also improves out-of-distribution generalization for very low-resource accents, demonstrating the viability of our approach for building generalizable ASR models in the context of accented African clinical ASR, where training datasets are predominantly scarce.
    
[^55]: 生成可信的文本：大型语言模型的不确定性量化

    Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models. (arXiv:2305.19187v1 [cs.CL])

    [http://arxiv.org/abs/2305.19187](http://arxiv.org/abs/2305.19187)

    本研究提出应对大型语言模型可信度问题的方法，研究黑盒模型中置信度与不确定性量化，并将其应用于选择性自然语言生成。

    

    近期，专门用于自然语言生成的大型语言模型(LLMs)在各个领域表现出了很好的能力，但是评估LLMs生成的结果的可信度仍然是一个挑战，关于自然语言生成的不确定性量化的研究也较少。此外，现有的文献通常假定对语言模型的白盒访问，这要么是由于最新的LLMs的封闭源代码的性质，要么是由于计算限制。本文研究了黑盒LLMs的不确定性量化问题。我们首先区分了两种密切相关的概念: 只与输入有关的“不确定性”和还与生成的回复有关的“置信度”。然后我们提出并比较了几个置信度/不确定度指标，将它们应用于“选择性自然语言生成”，其中不可靠的结果可以被忽略或者移交给进一步的分析。

    Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or due to computational constraints. In this work, we investigate uncertainty quantification in NLG for $\textit{black-box}$ LLMs. We first differentiate two closely-related notions: $\textit{uncertainty}$, which depends only on the input, and $\textit{confidence}$, which additionally depends on the generated response. We then propose and compare several confidence/uncertainty metrics, applying them to $\textit{selective NLG}$, where unreliable results could either be ignored or yielded for further 
    
[^56]: 在社交媒体中识别压力和抑郁的多任务学习

    Multitask learning for recognizing stress and depression in social media. (arXiv:2305.18907v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.18907](http://arxiv.org/abs/2305.18907)

    该论文通过利用社交媒体的数据，提出了一种新的多任务学习方法来早期识别压力和抑郁。研究人员可以使用不同的数据集作为主任务和辅助任务，以便更好地理解这两种情绪状态。

    

    当今社交媒体为人们表达情感提供了宝贵的信息来源，而压力和抑郁在现代人生活的快节奏下变得普遍。尽管已经有许多研究致力于早期识别压力和抑郁，但仍存在一些限制。一些多任务学习模式被提出，将抑郁和情绪（或形象化语言）作为主任务和辅助任务，然而研究人员将压力和抑郁视为两个独立的任务。为了解决这些限制，我们首次提出了一项研究，利用两个在不同条件下收集的数据集，并介绍了两种多任务学习框架，其中抑郁为主任务，压力为辅助任务。具体而言，我们使用了一个抑郁数据集和一个压力数据集。

    Stress and depression are prevalent nowadays across people of all ages due to the quick paces of life. People use social media to express their feelings. Thus, social media constitute a valuable form of information for the early detection of stress and depression. Although many research works have been introduced targeting the early recognition of stress and depression, there are still limitations. There have been proposed multi-task learning settings, which use depression and emotion (or figurative language) as the primary and auxiliary tasks respectively. However, although stress is inextricably linked with depression, researchers face these two tasks as two separate tasks. To address these limitations, we present the first study, which exploits two different datasets collected under different conditions, and introduce two multitask learning frameworks, which use depression and stress as the main and auxiliary tasks respectively. Specifically, we use a depression dataset and a stress
    
[^57]: 代码提示：用于大语言模型中复杂推理的神经符号方法

    Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models. (arXiv:2305.18507v1 [cs.CL])

    [http://arxiv.org/abs/2305.18507](http://arxiv.org/abs/2305.18507)

    本文介绍了一种神经符号提示方法——代码提示，该方法可以触发代码作为中间步骤。与自然语言相比，代码提示有着几个独特优势，能够提高符号推理和算术推理的性能，并且通常优于思路链提示。

    

    大语言模型已经通过各种提示方法扩大了规模，以解锁广泛的复杂推理任务。然而，当前的提示方法生成自然语言中间步骤以帮助推理，这可能导致不完善的任务缩减和混淆。为了缓解这样的限制，我们探索了代码提示，一种神经符号提示方法，具有零-shot和少-shot版本，可以触发代码作为中间步骤。我们在涉及符号推理和算术推理的7个广泛使用的基准测试中进行了实验。代码提示通常优于思路链提示。为了进一步了解代码提示的性能和限制，我们进行了广泛的消融研究和错误分析，并确定了使用符号提示相对于自然语言的几个独特优势。我们还考虑了代码提示和思路链提示的集合，以结合两者的优势。最后，我们展示了...

    Large language models (LLMs) have scaled up to unlock a wide range of complex reasoning tasks with the aid of various prompting methods. However, current prompting methods generate natural language intermediate steps to help reasoning, which can cause imperfect task reduction and confusion. To mitigate such limitations, we explore code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps. We conduct experiments on 7 widely-used benchmarks involving symbolic reasoning and arithmetic reasoning. Code prompting generally outperforms chain-of-thought (CoT) prompting. To further understand the performance and limitations of code prompting, we perform extensive ablation studies and error analyses, and identify several exclusive advantages of using symbolic promptings compared to natural language. We also consider the ensemble of code prompting and CoT prompting to combine the strengths of both. Finally, we show throu
    
[^58]: VAST：一种视听字幕文本全模态基础模型与数据集

    VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset. (arXiv:2305.18500v1 [cs.CV])

    [http://arxiv.org/abs/2305.18500](http://arxiv.org/abs/2305.18500)

    本文提出了一种全模态视频文本基础模型VAST及其数据集VAST-27M。该模型可以感知和处理视频中的视觉、音频和字幕模态，通过自动集成多模态字幕和资源，提供更好的支持文本关联的多种任务。

    

    当代视频文本基础模型已经完全探索了视觉和文本，而其他模态，如视频中的音频和字幕，却没有得到足够的关注。本文旨在通过探索自动生成的大规模全模态视频字幕数据集VAST-27M，建立多模态视频轨迹之间的连接，包括视觉、音频和字幕，并与文本进行关联。具体而言，我们首先收集了2700万个开放领域视频片段，并分别训练视觉和音频字幕生成器以生成视觉和音频字幕。然后，我们使用一个现有的大语言模型（LLM）将生成的字幕、字幕和指导提示集成到全模态字幕中。基于提出的VAST-27M数据集，我们训练了一种全模态视频文本基础模型VAST，它可以感知和处理视频中的视觉、音频和字幕模态，并更好地支持各种任务，包括视觉和文本之间的关联。

    Vision and text have been fully explored in contemporary video-text foundational models, while other modalities such as audio and subtitles in videos have not received sufficient attention. In this paper, we resort to establish connections between multi-modality video tracks, including Vision, Audio, and Subtitle, and Text by exploring an automatically generated large-scale omni-modality video caption dataset called VAST-27M. Specifically, we first collect 27 million open-domain video clips and separately train a vision and an audio captioner to generate vision and audio captions. Then, we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions. Based on the proposed VAST-27M dataset, we train an omni-modality video-text foundational model named VAST, which can perceive and process vision, audio, and subtitle modalities from video, and better support various tasks including vis
    
[^59]: 语言模型可以通过少样本的绝对推理来提高事件预测

    Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning. (arXiv:2305.16646v1 [cs.CL])

    [http://arxiv.org/abs/2305.16646](http://arxiv.org/abs/2305.16646)

    本文提出了一个建模和预测框架，在少样本情况下使用语言模型的绝对推理能力来提高事件序列模型的预测精度，经过实验证实可以明显优于最先进的事件序列模型。

    

    大型语言模型在各种推理任务上表现出惊人的性能。本文研究它们是否可以推理现实世界中的事件，帮助提高事件序列模型的预测精度。我们设计了一个建模和预测框架，其中大型语言模型执行绝对推理以辅助事件序列模型：事件模型在给定过去的情况下提出未来事件的预测; 在几个专家注释示范的指导下，语言模型学会了为每个提议提供可能的原因; 一个搜索模块找到与原因匹配的先前事件; 一个评分函数学会检查检索到的事件是否实际上可以导致提议。通过在两个具有挑战性的现实世界数据集（亚马逊评论和GDELT）上进行广泛的实验，我们证明了我们的框架 - 由于语言模型的推理能力 - 可以在低数据情况下明显优于最先进的事件序列模型。

    Large language models have shown astonishing performance on a wide range of reasoning tasks. In this paper, we investigate whether they could reason about real-world events and help improve the prediction accuracy of event sequence models. We design a modeling and prediction framework where a large language model performs abductive reasoning to assist an event sequence model: the event model proposes predictions on future events given the past; instructed by a few expert-annotated demonstrations, the language model learns to suggest possible causes for each proposal; a search module finds out the previous events that match the causes; a scoring function learns to examine whether the retrieved events could actually cause the proposal. Through extensive experiments on two challenging real-world datasets (Amazon Review and GDELT), we demonstrate that our framework -- thanks to the reasoning ability of language models -- could significantly outperform the state-of-the-art event sequence mo
    
[^60]: SummIt：通过ChatGPT进行迭代文本摘要

    SummIt: Iterative Text Summarization via ChatGPT. (arXiv:2305.14835v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14835](http://arxiv.org/abs/2305.14835)

    本文提出了SummIt，一个基于ChatGPT的迭代文本摘要框架，通过自我评估和反馈，模型能够迭代地完善生成的摘要，解决了一次性生成摘要可能存在的虚构和遗漏关键细节的问题。

    

    最近几年，文本摘要系统取得了显著进展，但通常是在一个步骤中生成摘要。然而，一次性生成摘要的设置有时是不够的，因为生成的摘要可能包含虚构的内容，或者忽略了与读者兴趣相关的重要细节。本文通过提出SummIt，基于ChatGPT等大型语言模型的迭代文本摘要框架来解决这个限制。我们的框架通过自我评估和反馈使模型能够迭代地完善生成的摘要，类似于人类在起草和修订摘要时的迭代过程。此外，我们还探索了将知识和主题提取器整合到框架中以增强摘要的准确性和可控性的潜在好处。我们在三个基准摘要数据集上自动评估了框架的性能。我们还进行了人类评估，验证了迭代改进的有效性。

    Text summarization systems have made significant progress in recent years, but typically generate summaries in one single step. However, the one-shot summarization setting is sometimes inadequate, as the generated summary may contain hallucinations or overlook essential details related to the reader's interests. This paper addresses this limitation by proposing SummIt, an iterative text summarization framework based on large language models like ChatGPT. Our framework enables the model to refine the generated summary iteratively through self-evaluation and feedback, resembling humans' iterative process when drafting and revising summaries. Furthermore, we explore the potential benefits of integrating knowledge and topic extractors into the framework to enhance summary faithfulness and controllability. We automatically evaluate the performance of our framework on three benchmark summarization datasets. We also conduct a human evaluation to validate the effectiveness of the iterative ref
    
[^61]: UniChart：面向图表理解和推理的通用视觉语言预训练模型

    UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning. (arXiv:2305.14761v1 [cs.CL])

    [http://arxiv.org/abs/2305.14761](http://arxiv.org/abs/2305.14761)

    本文提出了UniChart，这是一个用于图表理解和推理的通用视觉语言预训练模型。UniChart首先建立了一个包括各种不同主题和视觉风格的大量图表语料库，然后使用图表特定的预训练任务来编码图表，并在几个图表理解和推理任务上表现出色。

    

    图表在数据分析、可视化重要见解和回答数据的复杂推理问题方面非常流行。为了方便使用自然语言进行基于图表的数据分析，最近引入了几个下游任务，例如图表问答和图表总结。然而，大多数解决这些任务的方法都使用语言或视觉-语言任务的预训练，而不试图明确建模图表的结构（例如，如何视觉编码数据以及如何将图表元素相互关联）。为了解决这个问题，我们首先建立了一个包括各种不同主题和视觉风格的大量图表语料库。然后，我们提出了UniChart，这是一个用于图表理解和推理的预训练模型。UniChart对图表的相关文本、数据和视觉元素进行编码，然后使用基于图表的文本解码器以自然语言生成预期的输出。我们提出了几个面向图表的预训练任务，包括：（i）低层次视觉编码预测，（ii）图表元素关系预测和（iii）图表问题回答预测。我们的评估显示，UniChart在几个图表理解和推理任务上表现优于强基线。

    Charts are very popular for analyzing data, visualizing key insights and answering complex reasoning questions about data. To facilitate chart-based data analysis using natural language, several downstream tasks have been introduced recently such as chart question answering and chart summarization. However, most of the methods that solve these tasks use pretraining on language or vision-language tasks that do not attempt to explicitly model the structure of the charts (e.g., how data is visually encoded and how chart elements are related to each other). To address this, we first build a large corpus of charts covering a wide variety of topics and visual styles. We then present UniChart, a pretrained model for chart comprehension and reasoning. UniChart encodes the relevant text, data, and visual elements of charts and then uses a chart-grounded text decoder to generate the expected output in natural language. We propose several chart-specific pretraining tasks that include: (i) low-lev
    
[^62]: 少样本和零样本NLU任务中提示位置确实很重要

    Prompt position really matters in few-shot and zero-shot NLU tasks. (arXiv:2305.14493v1 [cs.CL])

    [http://arxiv.org/abs/2305.14493](http://arxiv.org/abs/2305.14493)

    该论文通过实证研究发现，提示位置对于少样本和零样本任务的模型性能具有实质性影响，先前研究中使用的提示位置通常是次优的，提示位置优化应成为重要的研究方向。

    

    基于提示的模型在零样本和少样本学习领域取得了显著进展，吸引了众多研究者的关注。但是，有效提示模板的开发起着至关重要的作用。然而，先前的研究主要集中在提示词汇选择或保留提示位置的嵌入初始化方面。在这项实证研究中，我们对自然语言理解任务的提示位置选项进行了迄今为止最全面的分析。我们的发现量化了提示位置对模型性能的实质性影响。我们观察到，先前研究中使用的提示位置对于零样本和少样本设置通常是次优的。这些发现表明，提示位置优化是一个有趣的研究方向，与现有的提示工程重心并列。

    Prompt-based models have made remarkable advancements in the fields of zero-shot and few-shot learning, attracting a lot of attention from researchers. Developing an effective prompt template plays a critical role. However, prior studies have mainly focused on prompt vocabulary selection or embedding initialization with the reserved prompt position fixed. In this empirical study, we conduct the most comprehensive analysis to date of prompt position option for natural language understanding tasks. Our findings quantify the substantial impact prompt position has on model performance. We observe that the prompt position used in prior studies is often sub-optimal for both zero-shot and few-shot settings. These findings suggest prompt position optimisation as an interesting research direction alongside the existing focus on prompt engineering.
    
[^63]: CREATOR: 大型语言模型的抽象和具体推理解耦工具的创建

    CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models. (arXiv:2305.14318v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14318](http://arxiv.org/abs/2305.14318)

    CREATOR是一个新的框架，使得大型语言模型（LLMs）能够利用文档和代码实现创建自己的工具。通过将抽象工具创建和具体决策执行解耦，CREATOR提高了性能，并在不同基准测试中超越了现有的基线方法。此外，我们引入了Creation Challenge数据集，展示了LLMs的工具创建能力的必要性和优势。

    

    大型语言模型（LLMs）在利用工具方面取得了重要进展，但由于API可用性和隐式推理的不稳定性，它们的能力有限，特别是在涉及规划和执行的情况下。为了克服这些限制，我们提出了一个新的框架CREATOR，可以使LLMs利用文档和代码实现来创建自己的工具。CREATOR将抽象工具创建和具体决策执行解耦，从而提高了性能。我们在MATH和TabMWP基准上评估了CREATOR，分别包含具有挑战性的数学竞赛问题和多样的表格内容。值得注意的是，CREATOR优于现有的思维链、思维程序和使用工具的基线。此外，我们还引入了Creation Challenge数据集，包含2K个多样的问题，以强调LLMs创建工具的必要性和益处。进一步的研究表明，将LLMs用作工具创建者有助于提升知识的利用。

    Large Language Models (LLMs) have made significant progress in utilizing tools, but their ability is limited by API availability and the instability of implicit reasoning, particularly when both planning and execution are involved. To overcome these limitations, we propose CREATOR, a novel framework that enables LLMs to create their own tools using documentation and code realization. CREATOR disentangles abstract tool creation and concrete decision execution, resulting in improved performance. We evaluate CREATOR on MATH and TabMWP benchmarks, respectively consisting of challenging math competition problems and diverse tabular contents. Remarkably, CREATOR outperforms existing chain-of-thought, program-of-thought, and tool-using baselines. Additionally, we introduce the Creation Challenge dataset, featuring 2K diverse questions, to emphasize the necessity and benefits of LLMs' tool creation ability. Further research demonstrates that leveraging LLMs as tool creators facilitates knowled
    
[^64]: INSTRUCTSCORE: 可解释的文本生成评估与细粒度反馈

    INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback. (arXiv:2305.14282v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14282](http://arxiv.org/abs/2305.14282)

    INSTRUCTSCORE是一个可解释的文本生成评估度量，通过利用明确的人类指令和GPT-4的隐式知识，它能生成生成文本的分数和人类可读的诊断报告，达到与最先进度量相当的性能水平。

    

    自动评估语言生成的质量至关重要。尽管最近学习度量表显示与人类判断高度相关，但这些度量无法解释其判断或将分数与生成文本中的缺陷关联起来。为了解决这个限制，我们提出了InstructScore，这是一个用于文本生成的可解释的评估度量。通过利用明确的人类指令和GPT-4的隐式知识，我们基于LLaMA对文本评估度量进行微调，生成生成文本的分数和人类可读的诊断报告。我们在各种生成任务上评估了InstructScore，包括翻译、字幕生成、数据到文本和常识生成。实验表明，我们的7B模型超过了所有其他无监督度量，包括基于175B GPT-3和GPT-4的模型。令人惊讶的是，即使没有来自人工评级数据的直接监督，我们的InstructScore的性能水平也与COMET2等最先进的度量相当。

    Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics can not explain their verdict or associate the scores with defects in generated text. To address this limitation, we present InstructScore, an explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT-4, we fine-tune a text evaluation metric based on LLaMA, producing both a score for generated text and a human readable diagnostic report. We evaluate InstructScore on a variety of generation tasks, including translation, captioning, data-to-text and commonsense generation. Experiments show that our 7B model surpasses all other unsupervised metrics, including those based on 175B GPT-3 and GPT-4. Surprisingly, our InstructScore, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET2
    
[^65]: 使用基于文献的语境化学习生成新的科学方向

    Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14259](http://arxiv.org/abs/2305.14259)

    本文介绍了一种基于文献的发现方法，通过上下文化的学习生成新的科学方向，克服了标准方法在预测关联、忽略上下文等方面的局限性。模型使用了引文和知识图关系的网络，并使用大型语言模型进行评估，发现GPT4在生成创新思想方面表现出色。

    

    基于文献的发现（LBD）旨在通过挖掘论文并生成假设来发现新的科学知识。标准的LBD仅限于预测离散概念之间的两两关系（例如，药物和疾病的关联）。LBD还忽略了关键的上下文，例如实验设置（例如，药物评估的特定患者群体）和人类科学家考虑的背景知识和动机（例如，找到没有特定副作用的药物候选）。我们通过一种新颖的上下文化LBD（C-LBD）表述来解决这些局限性：以自然语言生成科学假设，同时将它们联系到控制假设搜索空间的上下文中。我们提出了一个建模框架，使用获得的引文和知识图关系的异构网络中的“灵感”，并创建了一个从论文中派生的新数据集。我们使用强大的大型语言模型（LLM）进行评估，发现GPT4倾向于生成具有创新性的思想。

    Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
    
[^66]: 使用轻量级的语言特定模块压缩多语言知识

    Condensing Multilingual Knowledge with Lightweight Language-Specific Modules. (arXiv:2305.13993v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13993](http://arxiv.org/abs/2305.13993)

    本文介绍了一种使用轻量级的语言特定模块来压缩多语言知识的方法，通过生成低秩矩阵来构建语言特定模块，并使用Fuse Distillation技术将多个语言特定模块中的知识压缩到一个共享模块中，提高了推理和模型序列化的效率。

    

    将语言特定（LS）模块纳入多语言机器翻译中是一种提升性能的有效方法。这种方法与专家混合（MoE）方法相似，因为它不会增加FLOPs。然而，由于全连接层中引入的全秩矩阵所带来的参数数量太多，这种方法在扩展到数百种语言（专家）时往往难以管理。在本文中，我们介绍了语言特定矩阵合成（LMS）方法。该方法通过从两个显著较小的矩阵生成低秩矩阵来构建LS模块以近似全秩矩阵。此外，我们使用Fuse Distillation（FD）技术将多个LS模块中的多语言知识压缩到一个共享模块中，以提高推理和模型序列化的效率。我们证明了我们的LMS方法在与同样数量的额外参数情况下显著优于之前的LS方法和MoE方法，例如1.73 BLE

    Incorporating language-specific (LS) modules is a proven method to boost performance in multilingual machine translation. This approach bears similarity to Mixture-of-Experts (MoE) because it does not inflate FLOPs. However, the scalability of this approach to hundreds of languages (experts) tends to be unmanageable due to the prohibitive number of parameters introduced by full-rank matrices in fully-connected layers. In this work, we introduce the Language-Specific Matrix Synthesis (LMS) method. This approach constructs LS modules by generating low-rank matrices from two significantly smaller matrices to approximate the full-rank matrix. Furthermore, we condense multilingual knowledge from multiple LS modules into a single shared module with the Fuse Distillation (FD) technique to improve the efficiency of inference and model serialization. We show that our LMS method significantly outperforms previous LS methods and MoE methods with the same amount of extra parameters, e.g., 1.73 BLE
    
[^67]: 通过协作交互与学习助手从错误中学习

    Learn from Mistakes through Cooperative Interaction with Study Assistant. (arXiv:2305.13829v1 [cs.CL])

    [http://arxiv.org/abs/2305.13829](http://arxiv.org/abs/2305.13829)

    本文提出了一个新框架 SALAM，通过协作交互与学习助手来帮助 LLM 在反思和改进过程中。该框架通过收集错误并在推理时提供指导方针，显着提高模型性能。

    

    大型语言模型已经证明了它们自我反思和改进生成能力的能力，这可以进一步提高它们的性能。然而，这种反馈机制面临挑战，例如不能保证正确性和对模型弱点缺乏全局洞察力。在本文中，我们提出了一种新的框架 Study Assistant for Large Language Model (SALAM)，以帮助 LLM 在反思和改进过程中。我们根据人类助理研究的灵感，通过将先前的响应与真实值进行定量分级，并在训练阶段收集错误来实现这一点。在推理期间，它根据错误收集确定常见误解，并提供指导方针，以帮助模型在推理期间避免类似的错误。SALAM 是一个模型不可知的框架，专注于提供一般性的反馈，并可适用于任何基础模型。我们在两个具有挑战性的基准测试上对 SALAM 进行了评估，它在各种基线上都获得了显着的改进。

    Large language models have demonstrated their ability to self-reflect and refine their generation, which can further improve their performance. However, this feedback mechanism faces challenges such as no guarantee of correctness and the lack of global insight into the model's weaknesses. In this paper, we propose a novel framework, Study Assistant for Large Language Model (SALAM), to aid LLMs in the reflection and refinement process. Motivated by the human study assistant, this framework grades previous responses with the ground truth and collects mistakes in the training phase. During inference, it identifies common misunderstandings based on the mistake collections and provides guidelines for the model to help the model avoid similar mistakes during inference. SALAM is a model-agnostic framework, focusing on providing general feedback and can adapt to any base model. Our evaluation of SALAM on two challenging benchmarks demonstrated a significant improvement over various baselines.
    
[^68]: 是否重复的疑问: 在令牌危机下扩展LLM的洞见

    To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis. (arXiv:2305.13230v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13230](http://arxiv.org/abs/2305.13230)

    该研究通过实证调查探讨了在令牌危机下扩展LLM的重复预训练数据方法，发现模型容易过拟合并导致多轮次性能下降，关键因素包括数据集规模、模型参数和训练目标，而正则化技术并不能明显缓解这个问题。

    

    最近的研究强调了数据集规模对于扩展语言模型的重要性。然而，大型语言模型（LLMs）在预训练过程中非常依赖于令牌，并且网络上的高质量文本数据已接近LLMs的扩展限制。为了进一步增强LLMs，一种简单的方法是重复预训练数据进行额外的训练轮次。在这项研究中，我们从实证角度探讨了这种方法下的三个关键方面。首先，我们探究了重复预训练数据的后果，揭示了模型容易过拟合，导致多轮次性能下降。其次，我们研究了导致多轮次性能下降的关键因素，发现数据集规模、模型参数和训练目标是显著因素，而数据集质量和模型FLOP则影响较小。最后，我们探究了广泛使用的正则化方法是否可以缓解多轮次性能下降。大多数正则化技术并不能明显缓解这种问题。

    Recent research has highlighted the importance of dataset size in scaling language models. However, large language models (LLMs) are notoriously token-hungry during pre-training, and high-quality text data on the web is approaching its scaling limit for LLMs. To further enhance LLMs, a straightforward approach is to repeat the pre-training data for additional epochs. In this study, we empirically investigate three key aspects under this approach. First, we explore the consequences of repeating pre-training data, revealing that the model is susceptible to overfitting, leading to multi-epoch degradation. Second, we examine the key factors contributing to multi-epoch degradation, finding that significant factors include dataset size, model parameters, and training objectives, while less influential factors consist of dataset quality and model FLOPs. Finally, we explore whether widely used regularization can alleviate multi-epoch degradation. Most regularization techniques do not yield sig
    
[^69]: 多任务指令调整LLaMa以适应特定场景：关于写作辅助的初步研究

    Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance. (arXiv:2305.13225v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13225](http://arxiv.org/abs/2305.13225)

    这项研究探索了将LLMs应用于特定任务的可能性，通过在写作辅助场景中进行指令调整，取得了显著的改进。

    

    针对一些特定任务而非通用指令遵循，我们研究了一种实际问题设置，并探索了LLMs在这些有针对性场景中是否有利和可以进一步改进。我们选择了写作辅助作为测试平台，其中包括七个写作任务。我们收集了这些任务的训练数据，按照指令遵循的格式重新构建，并通过指令调整对LLM，特别是LLaMa进行优化。实验结果表明，使用写作指令数据对LLaMa进行微调显著改善了性能。

    Proprietary Large Language Models (LLMs), such as ChatGPT, have garnered significant attention due to their exceptional capabilities in handling a diverse range of tasks. Recent studies demonstrate that open-sourced smaller foundational models, such as 7B-size LLaMA, can also display remarkable proficiency in tackling diverse tasks when fine-tuned using instruction-driven data. In this work, we investigate a practical problem setting where the primary focus is on one or a few particular tasks rather than general-purpose instruction following, and explore whether LLMs can be beneficial and further improved for such targeted scenarios. We choose the writing-assistant scenario as the testbed, which includes seven writing tasks. We collect training data for these tasks, reframe them in an instruction-following format, and subsequently refine the LLM, specifically LLaMA, via instruction tuning. Experimental results show that fine-tuning LLaMA on writing instruction data significantly improv
    
[^70]: 非自回归的文档级机器翻译

    Non-Autoregressive Document-Level Machine Translation. (arXiv:2305.12878v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12878](http://arxiv.org/abs/2305.12878)

    本文研究了非自回归模型在文档级机器翻译中的应用，提出了一种简单有效的句子对齐设计，并发现当前非自回归模型在多模态和对齐等问题上还存在困难，性能相对于自回归模型仍有差距。

    

    非自回归翻译模型在句子级机器翻译中具有可比较的性能和更快的速度，然而它们在文档级机器翻译中的能力尚未被深入研究，这限制了它们在实际场景中的使用。本文对典型的非自回归模型在文档级机器翻译中进行了全面的研究，并进一步提出了一种简单而有效的源语言和目标语言之间的句子对齐设计。实验表明，非自回归模型在文档上取得了高度的加速，并且句子对齐明显提高了它们的性能。然而，当前的非自回归模型与自回归模型相比仍存在显著的性能差距。进一步研究发现，在文档级机器翻译的背景下，非自回归模型在多模态和对齐等问题上面临着更多的困难，目前的非自回归模型也难以充分利用文档上下文和处理话语现象。

    Non-autoregressive translation (NAT) models achieve comparable performance and superior speed compared to auto-regressive translation (AT) models in the context of sentence-level machine translation (MT). However, their abilities are unexplored in document-level MT, hindering their usage in real scenarios. In this paper, we conduct a comprehensive examination of typical NAT models in the context of document-level MT and further propose a simple but effective design of sentence alignment between source and target. Experiments show that NAT models achieve high acceleration on documents, and sentence alignment significantly enhances their performance.  However, current NAT models still have a significant performance gap compared to their AT counterparts. Further investigation reveals that NAT models suffer more from the multi-modality and misalignment issues in the context of document-level MT, and current NAT models struggle with exploiting document context and handling discourse phenome
    
[^71]: AutoTrial：用自然语言生成临床试验设计指南

    AutoTrial: Prompting Language Models for Clinical Trial Design. (arXiv:2305.11366v1 [cs.CL])

    [http://arxiv.org/abs/2305.11366](http://arxiv.org/abs/2305.11366)

    AutoTrial是一种使用语言模型自动生成临床试验纳入/排除标准的方法，它可以可控生成、可扩展学习、提供推理链，实验表明，它能够生成流畅准确的标准文本，与先进方法相媲美，但资源占用更少。

    

    临床试验对于药物开发至关重要。为患者拟定合适的纳入/排除标准是试验成功的关键。临床试验方案的正确设计应考虑到类似的先例试验及其纳入/排除标准，以确保患者的充分覆盖。本文提出了一种名为AutoTrial的方法，使用自然语言生成模型来帮助设计临床纳入/排除标准。它允许（1）通过离散和神经提示的混合进行可控的指导生成，（2）通过上下文学习进行可扩展的知识融合，以及（3）提供明确的推理链以理解输出的合理性。对超过70,000项临床试验的实验验证了AutoTrial生成的标准文本具有流畅性和连贯性，并且在捕捉目标试验相关临床概念方面准确度高。值得注意的是，我们的方法在计算资源占用更少的情况下，实现了可与最先进的方法相媲美的性能。

    Clinical trials are critical for drug development. Constructing the appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for patient recruitment) is essential for the trial's success. Proper design of clinical trial protocols should consider similar precedent trials and their eligibility criteria to ensure sufficient patient coverage. In this paper, we present a method named AutoTrial to aid the design of clinical eligibility criteria using language models. It allows (1) controllable generation under instructions via a hybrid of discrete and neural prompting, (2) scalable knowledge incorporation via in-context learning, and (3) explicit reasoning chains to provide rationales for understanding the outputs. Experiments on over 70K clinical trials verify that AutoTrial generates high-quality criteria texts that are fluent and coherent and with high accuracy in capturing the relevant clinical concepts to the target trial. It is noteworthy that our method, with a much sm
    
[^72]: 在直播聊天中分析规范违背现象

    Analyzing Norm Violations in Live-Stream Chat. (arXiv:2305.10731v1 [cs.CL])

    [http://arxiv.org/abs/2305.10731](http://arxiv.org/abs/2305.10731)

    本文分享了第一次针对在直播平台上检测规范违背现象的自然语言处理研究，通过分类别标注和用户研究，揭示了直播聊天规范形成的见解。

    

    毒性言语，如仇恨言论，可能会阻止用户参与在线社区和流行平台，影响他们的体验。以前的检测工具主要关注于在线论坛和社交媒体（如Reddit和Twitter）的对话。但是，将这些方法应用于直播平台（如Twitch和YouTube Live）中的对话时，由于每个评论仅可见一段时间，并且缺少与其他评论建立关系的线程结构，因此这些方法的效果较差。本文分享了第一次针对在直播平台上检测规范违背现象的自然语言处理研究。我们在Twitch上对4,583个受过审查的评论进行了分类别标注，并定义了直播聊天中的规范违反类别。我们讨论了直播数据与其他论坛的几个区别，并证明了当前的模型在这种情况下表现不佳。通过一项用户研究，我们确定了文本和语音中规范违背之间的差异，并揭示了直播聊天规范形成的见解。

    Toxic language, such as hate speech, can deter users from participating in online communities and enjoying popular platforms. Previous approaches to detecting toxic language and norm violations have been primarily concerned with conversations from online forums and social media, such as Reddit and Twitter. These approaches are less effective when applied to conversations on live-streaming platforms, such as Twitch and YouTube Live, as each comment is only visible for a limited time and lacks a thread structure that establishes its relationship with other comments. In this work, we share the first NLP study dedicated to detecting norm violations in conversations on live-streaming platforms. We define norm violation categories in live-stream chats and annotate 4,583 moderated comments from Twitch. We articulate several facets of live-stream data that differ from other forums, and demonstrate that existing models perform poorly in this setting. By conducting a user study, we identify the 
    
[^73]: 不依靠先验知识的时态知识图谱预测

    Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning. (arXiv:2305.10613v1 [cs.CL])

    [http://arxiv.org/abs/2305.10613](http://arxiv.org/abs/2305.10613)

    本文旨在探究是否能够使用大型语言模型进行时态知识图谱预测，尤其是不需要任何显式模块。结果表明，大型语言模型在此类预测中表现良好，并且可以隐式有效地编码上下文和时间信息。

    

    时间知识图谱（TKG）预测是一个挑战模型使用过去的知识来预测未来事实的基准测试。在本文中，我们使用上下文学习（ICL）将大型语言模型（LLM）应用于这些基准测试。我们探究了LLMs在TKG预测中可以在多大程度上使用，特别是没有任何微调或捕捉结构和时间信息的显式模块。为了进行实验，我们提出了一个框架，将相关历史事实转换为提示并使用令牌概率生成排名预测。令人惊讶的是，我们观察到LLMs的性能与为TKG预测精心设计和训练的最先进TKG模型相当。我们的广泛评估展示了多个具有不同特性的模型和数据集的性能，比较了准备上下文信息的替代启发式方法，并与著名的TKG方法和简单的频率和最近性基线进行对比。我们还检查了生成的提示，并展示了它们与所预测的事实的相关性。我们的结果表明，LLMs确实可以隐式有效地编码上下文和时间信息，进行TKG预测，而无需显式知识或领域特定模块。

    Temporal knowledge graph (TKG) forecasting benchmarks challenge models to predict future facts using knowledge of past facts. In this paper, we apply large language models (LLMs) to these benchmarks using in-context learning (ICL). We investigate whether and to what extent LLMs can be used for TKG forecasting, especially without any fine-tuning or explicit modules for capturing structural and temporal information. For our experiments, we present a framework that converts relevant historical facts into prompts and generates ranked predictions using token probabilities. Surprisingly, we observe that LLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefully designed and trained for TKG forecasting. Our extensive evaluation presents performances across several models and datasets with different characteristics, compares alternative heuristics for preparing contextual information, and contrasts to prominent TKG methods and simple frequency and recency baselines. We als
    
[^74]: 预训练语言模型的知识反思

    Knowledge Rumination for Pre-trained Language Models. (arXiv:2305.08732v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08732](http://arxiv.org/abs/2305.08732)

    本文提出了一种名为知识反思的新范式，旨在帮助预训练语言模型利用已经编码在其预训练参数中的相关潜在知识，而不需要从外部语料库中检索。这种方法通过在模型中添加提示，并将相关知识注入模型进行整合，取得了在常识推理任务和GLUE基准上的实验结果。

    

    先前的研究揭示了普通的预训练语言模型（PLMs）单独处理知识密集型NLP任务的能力不足，因此，一些工作尝试将外部知识集成到PLMs中。然而，尽管有着有前途的结果，但我们经验性地观察到，PLM可能已经在其预训练参数中编码了丰富的知识，但在应用到知识密集型任务时未能充分利用它们。在本文中，我们提出了一种名为知识反思的新范式，以帮助预训练语言模型利用相关的潜在知识，而不需要从外部语料库中检索它们。通过简单地在PLMs中添加一个如“据我所知”的提示，我们试图回顾相关的潜在知识，并将其注入模型以进行知识整合。我们将提出的知识反思应用于各种语言模型，包括RoBERTa、DeBERTa和GPT-3。在六个常识推理任务和GLUE基准上的实验结果显示.....

    Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fail to fully utilize them when applying them to knowledge-intensive tasks. In this paper, we propose a new paradigm dubbed Knowledge Rumination to help the pre-trained language model utilize that related latent knowledge without retrieving it from the external corpus. By simply adding a prompt like "As far as I know" to the PLMs, we try to review related latent knowledge and inject them back into the model for knowledge consolidation. We apply the proposed knowledge rumination to various language models, including RoBERTa, DeBERTa, and GPT-3. Experimental results on six commonsense reasoning tasks and GLUE benchmarks dem
    
[^75]: 基于大型语言模型的文本分类

    Text Classification via Large Language Models. (arXiv:2305.08377v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08377](http://arxiv.org/abs/2305.08377)

    本文介绍了Clue And Reasoning Prompting (CARP)算法，采用逐步推理策略优化了大型语言模型在文本分类中处理复杂语言现象的能力；并通过在监督数据集上使用微调模型进行$k$NN演示搜索，解决了上下文学习中有限标记的问题。

    

    尽管像GPT-3这样的大规模语言模型（LLM）取得了显著的成功，但它们在文本分类任务中的表现仍然显著不及微调模型。这是由于(1)缺乏处理复杂语言现象（例如强调、对比、反讽等）的推理能力； (2)在上下文学习中只允许有限数量的标记。在本文中，我们介绍了Clue And Reasoning Prompting (CARP)，CARP采用一种逐步推理策略，旨在应对涉及文本分类的复杂语言现象：CARP首先提示LLMs找到表面线索（例如关键词、语气、语义关系、参考等），然后诱导诊断性推理过程作出最终决策。为了进一步解决有限标记的问题，CARP在监督数据集上使用微调模型进行$k$NN演示搜索，在上下文学习中充分利用了LLM的优势。

    Despite the remarkable success of large-scale Language Models (LLMs) such as GPT-3, their performances still significantly underperform fine-tuned models in the task of text classification. This is due to (1) the lack of reasoning ability in addressing complex linguistic phenomena (e.g., intensification, contrast, irony etc); (2) limited number of tokens allowed in in-context learning.  In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts a progressive reasoning strategy tailored to addressing the complex linguistic phenomena involved in text classification: CARP first prompts LLMs to find superficial clues (e.g., keywords, tones, semantic relations, references, etc), based on which a diagnostic reasoning process is induced for final decisions. To further address the limited-token issue, CARP uses a fine-tuned model on the supervised dataset for $k$NN demonstration search in the in-context learning, allowing the model to take the advantage of both LLM's generali
    
[^76]: 大型语言模型的自动归属验证

    Automatic Evaluation of Attribution by Large Language Models. (arXiv:2305.06311v1 [cs.CL])

    [http://arxiv.org/abs/2305.06311](http://arxiv.org/abs/2305.06311)

    本文探讨了大型语言模型对归属验证的自动评估。研究发现通过提示LLMs和微调较小的LLMs两种方法都有效地检测到了错误的归属陈述。

    

    大型语言模型的最新发展方向是通过引用外部参考来生成和支持它们的主张。然而，评估归属问题，即验证生成的陈述是否确实被引用参考全面支持，仍然是一个开放的问题。本文研究了大型语言模型对归属验证的自动评估。我们首先提供了归属的定义，然后探讨了两种自动评估方法：提示LLMs和微调较小的LLMs。微调数据从相关任务（例如，问答、事实检查、自然语言推理和摘要）中重新利用。为了便于评估，我们手动策划了一组测试例子，其中包括12个领域的来自新必应发生器的测试例子。我们在经过策划的测试集和来自外部语料库的模拟测试例子上的结果表明，所提出的方法能够有效地检测到错误的归属陈述。

    A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support their claims. However, evaluating the attribution, i.e., verifying whether the generated statement is indeed fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution by LLMs. We begin by providing a definition of attribution and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks, such as question answering, fact-checking, natural language inference, and summarization. To facilitate the evaluation, we manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on the curated test set and simulated test examples from ex
    
[^77]: 通过生成对抗反馈对语言模型进行微调

    Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v1 [cs.CL])

    [http://arxiv.org/abs/2305.06176](http://arxiv.org/abs/2305.06176)

    本研究探讨了一种新的方法，使用生成对抗反馈的强化学习(RLGAF)对大型语言模型进行微调，以取代仅受人类反馈的强化学习(RLHF)，从而消除评估者的专业限制并提高性能。

    

    通过人类反馈的强化学习已经显著提高了大型语言模型(LLMs)的性能，使其输出与人类期望的价值观保持一致。然而，RLHF受到人类评估者的专业知识和生产力限制。在本研究中，我们研究了一种替代方法: 使用生成对抗反馈的强化学习(RLGAF)代替RLHF。我们的初步发现表明，RLGAF可以帮助对齐LLM的输出，同时不会受到RLHF固有的限制，为进一步自动化AI对齐的研究提供了有希望的途径。

    Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to significantly enhance the performance of large language models (LLMs) by aligning their outputs with desired human values. However, RLHF is constrained by the expertise and productivity limitations of human evaluators. In this study, we investigate an alternative approach: Reinforcement Learning with Generative Adversarial Feedback (RLGAF) to RLHF. Our preliminary findings indicate that RLGAF can help align LLMs outputs while not suffering from the inherent restrictions of RLHF, suggesting promising avenues for further research on automating AI alignment.
    
[^78]: 面向个人或实体的知识图谱表示学习：在医疗保健领域应用的研究

    Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare. (arXiv:2305.05640v1 [cs.AI])

    [http://arxiv.org/abs/2305.05640](http://arxiv.org/abs/2305.05640)

    本研究提出了一种在医疗保健领域构建面向实体的知识图谱的端到端表示学习方法 HEER，通过将领域特定的约束和特征纳入到图嵌入算法中，有效地改善了下游预测任务。

    

    知识图谱是一种按本体或模式组织信息的流行方式，已经在从搜索到推荐的各种场景中得到了应用。尽管在知识图谱方面有了一些进展，但知识表示仍然是跨行业的一个非常棘手的任务，特别是在生物医学和医疗保健领域，由于实体之间的复杂相互关系、异质性、缺乏标准化和数据稀疏性等因素，这一任务尤其具有挑战性。本文提出了一种面向医疗保健领域构建面向实体的知识图谱的端到端表示学习方法，重点是捕捉生物医学领域的独特特征。所提出的框架名为HEER（Healthcare Entity-Entity Representation learning），将领域特定的约束和特征纳入到图嵌入算法中。对多个基准数据集的结果表明，与最先进的方法相比，HEER在改善下游预测任务方面具有有效性。

    Knowledge graphs (KGs) are a popular way to organise information based on ontologies or schemas and have been used across a variety of scenarios from search to recommendation. Despite advances in KGs, representing knowledge remains a non-trivial task across industries and it is especially challenging in the biomedical and healthcare domains due to complex interdependent relations between entities, heterogeneity, lack of standardization, and sparseness of data. KGs are used to discover diagnoses or prioritize genes relevant to disease, but they often rely on schemas that are not centred around a node or entity of interest, such as a person. Entity-centric KGs are relatively unexplored but hold promise in representing important facets connected to a central node and unlocking downstream tasks beyond graph traversal and reasoning, such as generating graph embeddings and training graph neural networks for a wide range of predictive tasks. This paper presents an end-to-end representation le
    
[^79]: MoT：预思考和回忆功能使 ChatGPT 在“思想记忆”中自我进化

    MoT: Pre-thinking and Recalling Enable ChatGPT to Self-Improve with Memory-of-Thoughts. (arXiv:2305.05181v1 [cs.CL])

    [http://arxiv.org/abs/2305.05181](http://arxiv.org/abs/2305.05181)

    本文提出了一个名为 MoT 的框架，让大型语言模型通过“思想记忆”自我进化，无需注释数据集和参数更新。实验表明，该框架可以显著提高 ChatGPT 在数学推理、常识推理、事实推理和自然语言推理方面的能力。

    

    大型语言模型在各种任务上表现出了惊人的能力。但要实现它们的根本性改进，需要高质量的数据集或计算昂贵的微调。相反，人类可以通过思考和记忆轻松提高自我水平，而不需要外部资源。在本文中，我们提出了一个框架 MoT，在没有注释数据集和参数更新的情况下，通过思想记忆让大型语言模型自我进化。具体而言，该框架分为两个阶段：1. 在测试阶段之前，我们让大型语言模型在未加标签的数据集上进行预思考，并将高置信度的想法保存为外部记忆。2. 在推理过程中，给定一个测试问题，我们让大型语言模型回忆相关的记忆，帮助自己进行推理和回答。实验结果表明，所提出的框架可以帮助 ChatGPT 在数学推理、常识推理、事实推理和自然语言推理方面显著提高其能力。进一步的分析表明，每个组件都发挥了作用。

    Large Language Models have shown impressive abilities on various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, human can easily improve themselves by thinking and memory, without external resources. In this paper, we propose a framework, MoT, to let the LLM self-improve through Memory of Thoughts, without annotated datasets and parameter updates. Specifically, the framework is divided into two stages: 1. before the test stage, we let the LLM pre-think on the unlabeled dataset and save the high-confidence thoughts as external memory; 2. during inference, given a test question, we let the LLM recall relevant memory to help itself reason and answer it. Experimental results show that the proposed framework can help ChatGPT significantly improve its abilities in math reasoning, commonsense reasoning, factual reasoning and natural language inference. Further analyses show that each component contribute
    
[^80]: 旨在总结带有层次关系的多篇文档

    Towards Summarizing Multiple Documents with Hierarchical Relationships. (arXiv:2305.01498v1 [cs.CL])

    [http://arxiv.org/abs/2305.01498](http://arxiv.org/abs/2305.01498)

    提出了一个新的数据集PeerSum用于生成科学论文的元评论，源文档具有显式层次结构的丰富文档间关系，提出了一种用于元评论生成的关系感知多任务模型Rammer。

    

    多数现存的多文档摘要(MDS)数据集缺少人工生成的、真实的(即非合成的)摘要或者带有显式文档间关系的源文档。为了增强MDS系统的能力，我们提出PeerSum，这是一个新颖的数据集，用于生成科学论文的元评论，其中元评论是对评论和相应讨论的高度概括且真实的摘要。这些源文档具有显式层次结构的丰富文档间关系，包括交叉引用和经常出现的冲突。鉴于很少有研究采用基于预训练语言模型的注意力操纵来将层次关系纳入MDS系统中，我们还提出了Rammer(关系感知多任务元评论生成器)，这是一种元评论生成模型，使用基于层次关系的稀疏注意力和多任务目标，可以预测多个度量值。

    Most existing multi-document summarization (MDS) datasets lack human-generated and genuine (i.e., not synthetic) summaries or source documents with explicit inter-document relationships that a summary must capture. To enhance the capabilities of MDS systems we present PeerSum, a novel dataset for generating meta-reviews of scientific papers, where the meta-reviews are highly abstractive and genuine summaries of reviews and corresponding discussions. These source documents have rich inter-document relationships of an explicit hierarchical structure with cross-references and often feature conflicts. As there is a scarcity of research that incorporates hierarchical relationships into MDS systems through attention manipulation on pre-trained language models, we additionally present Rammer (Relationship-aware Multi-task Meta-review Generator), a meta-review generation model that uses sparse attention based on the hierarchical relationships and a multi-task objective that predicts several me
    
[^81]: GPT-NER：基于大型语言模型的命名实体识别

    GPT-NER: Named Entity Recognition via Large Language Models. (arXiv:2304.10428v1 [cs.CL])

    [http://arxiv.org/abs/2304.10428](http://arxiv.org/abs/2304.10428)

    本文提出了GPT-NER来解决大型语言模型在命名实体识别任务（NER）上表现不佳的问题，它通过将序列标记任务转化为生成任务，将LLM能够容易地适应NER任务。同时，为了有效解决LLMs“幻觉”问题，作者们提出了自我验证策略，通过提示LLMs询问自身来确定提取的实体是否属于实际存在的实体。

    

    尽管大规模语言模型（LLM）在各种NLP任务上已经实现了最先进的性能，但其NER性能仍然明显低于监督基线。这是由于命名实体识别（NER）和LLMs之间的差距：前者在本质上是序列标记任务，后者是一种文本生成模型。在本文中，我们提出了GPT-NER来解决这个问题。 GPT-NER通过将序列标记任务转换为生成任务来弥合差距，LLMs可以轻松适应。例如，将在输入文本“哥伦布是一座城市”中查找位置实体的任务转换为生成文本序列“@@哥伦布##是一座城市”，其中特殊标记@@##标记要提取的实体。为了有效解决LLMs“幻觉”问题，即LLMs有很强的倾向将空输入过度自信地标记为实体，我们提出了自我验证策略，通过提示LLMs询问自身来确定提取的实体是否属于实际存在的实体。

    Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model.  In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text "Columbus is a city" is transformed to generate the text sequence "@@Columbus## is a city", where special tokens @@## marks the entity to extract. To efficiently address the "hallucination" issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a lab
    
[^82]: RRHF: 无需烦恼地使用排名响应来对齐语言模型与人类反馈

    RRHF: Rank Responses to Align Language Models with Human Feedback without tears. (arXiv:2304.05302v1 [cs.CL])

    [http://arxiv.org/abs/2304.05302](http://arxiv.org/abs/2304.05302)

    RRHF是一种新的学习范式，可以高效地对齐语言模型输出概率与人类偏好，它通过排序损失对不同采样策略生成的响应进行评分，并在调整过程中只需1到2个模型。

    

    人类反馈的强化学习（RLHF）可以帮助将大型语言模型与人类偏好对齐，从而显著提高人类与这些模型间的交互质量。与PPO相比，我们提出了一种新的学习范式——RRHF，它通过排序损失对不同采样策略生成的响应进行评分，并学习将它们与人类偏好对齐。RRHF可以高效地对齐语言模型输出概率与人类偏好，其效果和Fine-Tuning一样稳健，而在调整过程中只需1到2个模型。此外，RRHF可以被认为是SFT和奖励模型的扩展，与PPO相比在编码和模型数量方面更为简单。

    Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and these models. InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). PPO, however, is sensitive to hyperparameters and requires a minimum of four models in its standard implementation, which makes it hard to train. In contrast, we propose a novel learning paradigm called RRHF, which scores responses generated by different sampling policies and learns to align them with human preferences through ranking loss. RRHF can efficiently align language model output probabilities with human preferences as robust as fine-tuning and it only needs 1 to 2 models during tuning. In addition, RRHF can be considered an extension of SFT and reward models while being simpler than PPO in terms of coding, model count
    
[^83]: 使用两万个类别进行开放词汇视觉识别的提示预训练

    Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition. (arXiv:2304.04704v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.04704](http://arxiv.org/abs/2304.04704)

    本研究提出了一种用于视觉语言模型的提示预训练方法POMP，可以在包括图像分类、语义分割和目标检测在内的各种视觉识别任务中提升识别性能，通过压缩语义信息，支持超过两万个类别的视觉概念。实验结果表明，POMP在多个数据集上达到了最先进的性能水平。

    

    本研究提出了POMP，一种用于视觉语言模型的提示预训练方法。POMP既具有存储和计算效率，又能够为超过两万个类别的丰富视觉概念压缩语义信息。一旦预训练完成，具有强大的可传递能力的提示可以直接应用于各种视觉识别任务，包括图像分类、语义分割和目标检测，以零-shot的方式提升识别性能。实证评估表明，POMP在21个数据集上达到了最先进的性能，例如在10个分类数据集上的平均准确率为67.0%（比CoOp高出3.1%），在开放词汇的Pascal VOC分割任务上的hIoU为84.4（比ZSSeg高出6.9）。我们的代码可以在https://github.com/amazon-science/prompt-pretraining上找到。

    This work proposes POMP, a prompt pre-training method for vision-language models. Being memory and computation efficient, POMP enables the learned prompt to condense semantic information for a rich set of visual concepts with over twenty-thousand classes. Once pre-trained, the prompt with a strong transferable ability can be directly plugged into a variety of visual recognition tasks including image classification, semantic segmentation, and object detection, to boost recognition performances in a zero-shot manner. Empirical evaluation shows that POMP achieves state-of-the-art performances on 21 datasets, e.g., 67.0% average accuracy on 10 classification datasets (+3.1% compared to CoOp) and 84.4 hIoU on open-vocabulary Pascal VOC segmentation (+6.9 compared to ZSSeg). Our code is available at https://github.com/amazon-science/prompt-pretraining.
    
[^84]: GEMINI：针对抽象文本摘要控制句子级写作风格

    GEMINI: Controlling the Sentence-level Writing Style for Abstractive Text Summarization. (arXiv:2304.03548v1 [cs.CL])

    [http://arxiv.org/abs/2304.03548](http://arxiv.org/abs/2304.03548)

    GEMINI模型将句子重写和融合技术集成，实现了抽象文本摘要中的句子级写作风格控制，该自适应方法在各种基准数据集上表现优异，特别是在数据集具有平衡的风格分布时。

    

    人类专家使用不同技术编写摘要，包括重写文档中的句子或合并多个句子生成摘要句。这些技术是灵活的，因此很难通过任何单一方法模拟。为了解决这个问题，我们提出了一个自适应模型GEMINI，将重写器和融合器集成起来，以模拟句子重写和融合技术。GEMINI自适应地选择重写特定的文档句子或从头生成摘要句。实验证明，我们的自适应方法在各种基准数据集上优于纯抽象和重写基线，特别是当数据集具有平衡的风格分布时。有趣的是，实验结果表明，每个摘要句的人类写作风格在其上下文中是可以预测的。

    Human experts write summaries using different techniques, including rewriting a sentence in the document or fusing multiple sentences to generate a summary sentence. These techniques are flexible and thus difficult to be imitated by any single method. To address this issue, we propose an adaptive model, GEMINI, that integrates a rewriter and a fuser to mimic the sentence rewriting and fusion techniques, respectively. GEMINI adaptively chooses to rewrite a specific document sentence or generate a summary sentence from scratch. Experiments demonstrate that our adaptive approach outperforms the pure abstractive and rewriting baselines on various benchmark datasets, especially when the dataset has a balanced distribution of styles. Interestingly, empirical results show that the human writing style of each summary sentence is consistently predictable given its context.
    
[^85]: LLM-Adapters：大型语言模型参数高效微调的适配器系列

    LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. (arXiv:2304.01933v1 [cs.CL])

    [http://arxiv.org/abs/2304.01933](http://arxiv.org/abs/2304.01933)

    本文提出了LLM-Adapters，一个将适配器集成到LLMs中进行参数高效微调的易于使用的框架，实现了比现有方法更少的参数和训练时间，在多个基准测试上取得了最先进的结果。

    

    像GPT-3和ChatGPT这样的大型语言模型（LLMs）的成功，导致了许多经济实惠和易于访问的替代品的开发，这些替代品是通过利用任务特定数据（例如ChatDoctor）或指导数据（例如Alpaca）微调开放式LLMs而创建的。在各种微调方法中，基于适配器的参数高效微调（PEFT）无疑是最有吸引力的研究主题之一，因为它只需要微调少量外部参数而不是整个LLMs，同时实现可比甚至更好的性能。为了进一步研究LLMs的PEFT方法，本文提出了LLM-Adapters，这是一个易于使用的框架，将各种适配器集成到LLMs中，并且可以为不同任务执行这些适配器的PEFT方法。该框架包括最先进的开放式LLMs，例如LLaMA，BLOOM，OPT和GPT-J，以及广泛使用的适配器，例如串联适配器，并联适配器和LoRA。该框架旨在高效且灵活，使用户可以使用最少的附加训练数据或计算资源轻松微调LLMs以适应不同的任务。对多个基准测试的实验表明，LLM-Adapters可以比现有方法使用更少的参数和训练时间取得最先进的结果。

    The success of large language models (LLMs), like GPT-3 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by fine-tuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, OPT, and GPT-J, as well as widely used adapters such as Series adapter, Parallel adapter, and LoRA. The framework is designed to
    
[^86]: 显式规划有助于语言模型进行逻辑推理

    Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])

    [http://arxiv.org/abs/2303.15714](http://arxiv.org/abs/2303.15714)

    本文提出了一个新的系统，使用语言模型进行多步逻辑推理，采用了显式规划来帮助做出更明智的决策，比其他竞争系统表现更好，显式规划在系统性能中起着关键作用。

    

    语言模型在各种自然语言处理任务中表现出色。本文提出了一个新颖的系统，采用语言模型进行多步逻辑推理。我们的系统将显式规划纳入到推理过程中，因此可以通过展望未来的效果来做出更明智的决策。在实验中，我们的全套系统在多项选择题答题任务中明显优于其他竞争系统，尽管只有约15亿个参数，但与GPT-3-davinci表现相当。我们进行了多个消融研究以证明显式规划在系统性能中起着关键作用。

    Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose a novel system that uses language models to perform multi-step logical reasoning. Our system incorporates explicit planning into its inference procedure, thus able to make more informed reasoning decisions at each step by looking ahead into their future effects. In our experiments, our full system significantly outperforms other competing systems. On a multiple-choice question answering task, our system performs competitively compared to GPT-3-davinci despite having only around 1.5B parameters. We conduct several ablation studies to demonstrate that explicit planning plays a crucial role in the system's performance.
    
[^87]: 2D扩散算法的去偏置方法用于文本到3D生成

    Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation. (arXiv:2303.15413v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.15413](http://arxiv.org/abs/2303.15413)

    本文提出了两种去偏置的方法，一种通过增加2D扩散模型得出的分数的截断值，一种通过调整视角提示和物体空间摄像机姿态之间的差异。实验结果表明这些方法可以显著减少伪影，提高真实感。

    

    本文探讨了在文本到3D生成中出现的视角一致性问题，也称为Janus问题。这个问题来自于2D扩散模型的固有偏置，导致生成的3D对象不真实。通过对其进行研究，我们提出了两种方法来去除偏置以实现文本到3D生成的鲁棒性。第一种方法叫做score debiasing，通过逐渐增加2D扩散模型得出的分数的截断值，来达到去除偏置的效果。第二种方法叫做prompt debiasing，利用语言模型确定用户提示和视角提示之间的矛盾词语，并调整视角提示和物体空间摄像机姿态之间的差异。我们的实验结果表明，我们的方法通过显著减少伪影，提高了真实感，并在质量与速度方面取得了良好的平衡。

    The view inconsistency problem in score-distilling text-to-3D generation, also known as the Janus problem, arises from the intrinsic bias of 2D diffusion models, which leads to the unrealistic generation of 3D objects. In this work, we explore score-distilling text-to-3D generation and identify the main causes of the Janus problem. Based on these findings, we propose two approaches to debias the score-distillation frameworks for robust text-to-3D generation. Our first approach, called score debiasing, involves gradually increasing the truncation value for the score estimated by 2D diffusion models throughout the optimization process. Our second approach, called prompt debiasing, identifies conflicting words between user prompts and view prompts utilizing a language model and adjusts the discrepancy between view prompts and object-space camera poses. Our experimental results show that our methods improve realism by significantly reducing artifacts and achieve a good trade-off between fa
    
[^88]: 错误分析提示使得大型语言模型在翻译评估方面实现了人类水平：以ChatGPT为例进行案例研究

    Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT. (arXiv:2303.13809v1 [cs.CL])

    [http://arxiv.org/abs/2303.13809](http://arxiv.org/abs/2303.13809)

    本文提出一种新的提示方法Error Analysis Prompting可改善LLMs在机器翻译质量评估上的性能，实现人类水平的评估。

    

    生成式大型语言模型（LLM），例如ChatGPT，在机器翻译、问答、文本摘要和自然语言理解等多个NLP任务上表现出卓越的能力。最近的研究表明，利用ChatGPT评估机器翻译质量在系统水平上取得了最先进的性能，但在段落水平上表现不佳。为了进一步提高LLM在机器翻译质量评估上的性能，我们进行了关于几种提示方法的研究。我们的结果表明，通过将Chain-of-Thoughts和Error Analysis结合起来，一种新的提示方法Error Analysis Prompting，像ChatGPT这样的LLM可以在系统和段落级别上生成人类般的机器翻译评估。此外，我们发现ChatGPT作为机器翻译评估器存在一些局限性，例如在提供单个查询中的多个译文时存在不稳定的评分和偏差。

    Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks such as machine translation, question answering, text summarization, and natural language understanding. Recent research has shown that utilizing ChatGPT for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but performs poorly at the segment level. To further improve the performance of LLMs on MT quality assessment, we conducted an investigation into several prompting methods. Our results indicate that by combining Chain-of-Thoughts and Error Analysis, a new prompting method called \textbf{\texttt{Error Analysis Prompting}}, LLMs like ChatGPT can \textit{generate human-like MT evaluations at both the system and segment level}. Additionally, we discovered some limitations of ChatGPT as an MT evaluator, such as unstable scoring and biases when provided with multiple translations in a single query. Our findings
    
[^89]: 应用SMILES序列的Transformer模型在学习手性时存在困难

    Difficulty in learning chirality for Transformer fed with SMILES. (arXiv:2303.11593v1 [cs.LG])

    [http://arxiv.org/abs/2303.11593](http://arxiv.org/abs/2303.11593)

    应用SMILES序列的Transformer模型在学习分子结构的整体性和手性方面存在困难，需要进行长时间的训练。生成的描述符用于分子性质预测时的准确率从开始到训练结束都是相似的。

    

    近年来，基于对极其多样的分子进行表示学习的描述符生成已经得到了发展，特别是那些将自然语言处理（NLP）模型应用于SMILES，即分子结构的文字表示的模型。然而，关于这些模型如何理解化学结构的研究很少。为了解决这个问题，我们调查了一种代表性的NLP模型——Transformer，在学习SMILES和化学结构之间的关系。结果表明，虽然Transformer快速学习分子的部分结构，但需要进行长时间的训练才能理解整体结构。与之一致的是，在不同的学习步骤中生成的描述符用于分子性质预测时的准确率从开始到训练结束都是相似的。此外，我们发现Transformer需要特别长的训练时间才能学习手性，并且有时会出现低翻译准确率的停滞现象。

    Recent years have seen development of descriptor generation based on representation learning of extremely diverse molecules, especially those that apply natural language processing (NLP) models to SMILES, a literal representation of molecular structure. However, little research has been done on how these models understand chemical structure. To address this, we investigated the relationship between the learning progress of SMILES and chemical structure using a representative NLP model, the Transformer. The results suggest that while the Transformer learns partial structures of molecules quickly, it requires extended training to understand overall structures. Consistently, the accuracy of molecular property predictions using descriptors generated from models at different learning steps was similar from the beginning to the end of training. Furthermore, we found that the Transformer requires particularly long training to learn chirality and sometimes stagnates with low translation accura
    
[^90]: 检索多模态信息用于增强生成：一项调研

    Retrieving Multimodal Information for Augmented Generation: A Survey. (arXiv:2303.10868v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.10868](http://arxiv.org/abs/2303.10868)

    这项调研综述了通过检索多模态知识来协助和增强生成模型的方法，这些方法提供了解决准确性、推理性、可解释性和鲁棒性等重要问题的有希望的解决方案。

    

    随着大型语言模型的普及，使用多模态来增强语言模型的生成能力成为一个重要趋势，这使得语言模型能更好地与世界互动。然而，对于在哪个阶段以及如何融合不同模态的问题缺乏一个统一的认识。在本调研中，我们回顾了通过检索多模态知识来协助和增强生成模型的方法，这些知识的格式包括图像、代码、表格、图形和音频。这些方法为实现准确性、推理性、可解释性和鲁棒性等重要问题提供了有希望的解决方案。通过提供深入的回顾，本调研旨在让学者们更深入地了解这些方法的应用，并鼓励他们将现有技术应用于快速发展的语言模型领域。

    As Large Language Models (LLMs) become popular, there emerged an important trend of using multimodality to augment the LLMs' generation ability, which enables LLMs to better interact with the world. However, there lacks a unified perception of at which stage and how to incorporate different modalities. In this survey, we review methods that assist and augment generative models by retrieving multimodal knowledge, whose formats range from images, codes, tables, graphs, to audio. Such methods offer a promising solution to important concerns such as factuality, reasoning, interpretability, and robustness. By providing an in-depth review, this survey is expected to provide scholars with a deeper understanding of the methods' applications and encourage them to adapt existing techniques to the fast-growing field of LLMs.
    
[^91]: DeltaScore: 利用差分扰动评价故事生成

    DeltaScore: Evaluating Story Generation with Differentiating Perturbations. (arXiv:2303.08991v1 [cs.CL])

    [http://arxiv.org/abs/2303.08991](http://arxiv.org/abs/2303.08991)

    DeltaScore利用差分扰动来评估故事生成的细粒度方面，并通过计算故事在特定方面扰动前后的可能性差异来衡量影响。该方法在多个故事领域中得到了评估，并与人类判断的相关性进行了研究。

    

    自然语言生成的各种评价指标存在，但对于故事生成的实用性有限，因为它们通常与人类判断的相关性不强，也不能测量细粒度的故事方面，例如流畅度与相关性，因为它们旨在评估整体生成质量。本文提出DeltaScore，一种利用扰动来评估细粒度故事方面的方法。我们的核心思想是基于这样的假设：故事在特定方面表现得越好（例如流畅度），它就会受到特定扰动（例如引入错别字）的影响越大。为了衡量影响，我们使用语言模型计算扰动前后故事的可能性差异。我们在多个故事领域中使用DeltaScore评估了基于状态的最新模型和传统基于相似性的指标，并研究了它与人类在五个细粒度故事方面的判断之间的相关性。

    Various evaluation metrics exist for natural language generation tasks, but they have limited utility for story generation since they generally do not correlate well with human judgments and do not measure fine-grained story aspects, such as fluency versus relatedness, as they are intended to assess overall generation quality. In this paper, we propose deltascore, an approach that utilizes perturbation to evaluate fine-grained story aspects. Our core idea is based on the hypothesis that the better the story performs in a specific aspect (e.g., fluency), the more it will be affected by a particular perturbation (e.g., introducing typos). To measure the impact, we calculate the likelihood difference between the pre- and post-perturbation stories using a language model. We evaluate deltascore against state-of-the-art model-based and traditional similarity-based metrics across multiple story domains, and investigate its correlation with human judgments on five fine-grained story aspects: f
    
[^92]: 用蜜蜂算法优化深度学习模型参数，提高医学文本分类准确性

    Optimizing Deep Learning Model Parameters with the Bees Algorithm for Improved Medical Text Classification. (arXiv:2303.08021v1 [cs.CL])

    [http://arxiv.org/abs/2303.08021](http://arxiv.org/abs/2303.08021)

    本文使用蜜蜂算法优化了深度学习模型参数，提高了医学文本分类的准确性，最高准确率在英语数据集上达到了99.63%，在阿拉伯语数据集上达到了88%。

    

    本文介绍了一种使用蜜蜂算法对深度学习模型进行参数优化的新机制，这是一种最近很有前途的群智能算法。优化问题是在给定初始超参数的情况下，通过确定的迭代次数来最大化基于医学文本分类疾病的准确性。实验包括两个不同的数据集：英语和阿拉伯语。使用长短期记忆 (LSTM) 和蜜蜂算法，在英语数据集上获得了99.63%的最高准确率，在阿拉伯语数据集上使用AraBERT获得了88%的最高准确率。

    This paper introduces a novel mechanism to obtain the optimal parameters of a deep learning model using the Bees Algorithm, which is a recent promising swarm intelligence algorithm. The optimization problem is to maximize the accuracy of classifying ailments based on medical text given the initial hyper-parameters to be adjusted throughout a definite number of iterations. Experiments included two different datasets: English and Arabic. The highest accuracy achieved is 99.63% on the English dataset using Long Short-Term Memory (LSTM) along with the Bees Algorithm, and 88% on the Arabic dataset using AraBERT.
    
[^93]: 寻找用于上下文学习的支持例子

    Finding Support Examples for In-Context Learning. (arXiv:2302.13539v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.13539](http://arxiv.org/abs/2302.13539)

    提出了一种名为LENS的过滤-搜索方法，用于寻找用于上下文学习的支持例子。LENS通过分阶段的过滤和搜索，在数据集中筛选信息丰富的上下文示例，并通过多样性引导的示例搜索方法找到能够充分描绘任务的示例。实验证明，LENS在性能上显著优于其他基线方法。

    

    此外，在上下文示例之间存在强依赖关系，使其成为一个NP难组合优化问题，并且列举所有的排列组合是不可行的。因此，我们提出了LENS，一种过滤-搜索方法，以两个阶段应对这个挑战：首先，我们通过过滤数据集来分别获取信息丰富的上下文示例。具体而言，我们提出了一种新的度量标准，InfoScore，基于语言模型的反馈评估示例的上下文信息，进一步提出了一种渐进过滤过程来过滤掉无信息的例子。然后，我们提出了多样性引导的示例搜索，通过迭代改进和评估所选示例的排列，来找到充分描绘任务的示例。实验结果表明，LENS明显优于广泛的基线方法。

    Additionally, the strong dependency among in-context examples makes it an NP-hard combinatorial optimization problem and enumerating all permutations is infeasible. Hence we propose LENS, a fiLter-thEN-Search method to tackle this challenge in two stages: First we filter the dataset to obtain informative in-context examples individually. Specifically, we propose a novel metric, InfoScore, to evaluate the example's in-context informativeness based on the language model's feedback, and further propose a progressive filtering process to filter out uninformative examples. Then we propose diversity-guided example search which iteratively refines and evaluates the selected example permutations, to find examples that fully depict the task. The experimental results show that LENS significantly outperforms a wide range of baselines.
    
[^94]: Pre-trained Vision and Language Models能否回答求知视觉问题？

    Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?. (arXiv:2302.11713v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.11713](http://arxiv.org/abs/2302.11713)

    本研究介绍了一个专门针对无法仅凭常识知识回答的信息寻求问题而设计的视觉问答数据集InfoSeek。使用InfoSeek数据集，我们发现目前最先进的预训练多模态模型在回答求知视觉问题方面面临挑战，但在该数据集上进行微调可以激发模型使用细粒度知识。

    

    Pre-trained vision and language models在涉及图像和文本的任务中展示了领先的能力，包括视觉问答。然而，这些模型是否具备回答不仅仅查询视觉内容，而且还具有知识密集和信息寻求性质的问题的能力仍然不清楚。在本研究中，我们介绍了InfoSeek，一个专门针对无法仅凭常识知识回答的信息寻求问题而设计的视觉问答数据集。使用InfoSeek，我们分析了各种预训练的视觉问答模型，并深入了解它们的特点。我们的发现揭示了目前最先进的预训练多模态模型（如PaLI-X，BLIP2等）在回答求知视觉问题方面面临挑战，但在InfoSeek数据集上进行微调能够激发模型使用他们在预训练过程中学到的细粒度知识。此外，我们还展示了准确的视觉实体的重要性。

    Pre-trained vision and language models have demonstrated state-of-the-art capabilities over existing tasks involving images and texts, including visual question answering. However, it remains unclear whether these models possess the capability to answer questions that are not only querying visual content but knowledge-intensive and information-seeking. In this study, we introduce InfoSeek, a visual question answering dataset tailored for information-seeking questions that cannot be answered with only common sense knowledge. Using InfoSeek, we analyze various pre-trained visual question answering models and gain insights into their characteristics. Our findings reveal that state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.) face challenges in answering visual information-seeking questions, but fine-tuning on the InfoSeek dataset elicits models to use fine-grained knowledge that was learned during their pre-training. Furthermore, we show that accurate visual entit
    
[^95]: RETVec：弹性和高效的文本向量化

    RETVec: Resilient and Efficient Text Vectorizer. (arXiv:2302.09207v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.09207](http://arxiv.org/abs/2302.09207)

    RETVec是一种高效、弹性和多语言的文本向量化器，通过采用新颖的字符编码和对抗攻击鲁棒的嵌入模型，实现了对拼写错误和对抗性攻击的更好适应性。与其他向量化器和词嵌入模型相比，RETVec在各种模型架构和数据集上表现出竞争力和显著的弹性。

    

    本文介绍了RETVec，一种专为基于神经网络的文本处理而设计的高效、弹性和多语言的文本向量化器。RETVec采用了一种新颖的字符编码和可选的小型嵌入模型，将词语嵌入到256维向量空间中。RETVec的嵌入模型使用对比度学习进行预训练，以针对拼写错误和字符级对抗攻击具有鲁棒性。在本文中，我们对RETVec在流行的模型架构和数据集上进行了评估和比较。这些比较表明，RETVec能够产生具有竞争力的多语言模型，对拼写错误和对抗性文本攻击具有显著的弹性。RETVec在Apache 2许可下可在https://github.com/google-research/retvec获取。

    This paper describes RETVec, an efficient, resilient, and multilingual text vectorizer designed for neural-based text processing. RETVec combines a novel character encoding with an optional small embedding model to embed words into a 256-dimensional vector space. The RETVec embedding model is pre-trained using pair-wise metric learning to be robust against typos and character-level adversarial attacks. In this paper, we evaluate and compare RETVec to state-of-the-art vectorizers and word embeddings on popular model architectures and datasets. These comparisons demonstrate that RETVec leads to competitive, multilingual models that are significantly more resilient to typos and adversarial text attacks. RETVec is available under the Apache 2 license at https://github.com/google-research/retvec.
    
[^96]: 中文阅读理解的自然响应生成

    Natural Response Generation for Chinese Reading Comprehension. (arXiv:2302.08817v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.08817](http://arxiv.org/abs/2302.08817)

    本研究构建了一个新的数据集Penguin，旨在促进中文阅读理解中自然响应生成的研究，提供了一个相对较大的训练和测试平台。通过开发两个强大的基准模型，我们解决了Penguin中的挑战。

    

    机器阅读理解(MRC)是对话代理的重要领域，引起了很多关注。然而，当前MRC基准的一个明显限制是：标记的答案大多数是从目标语料库中提取的片段或给定候选项的选择，忽略了高质量响应的自然性。因此，在这些数据集上训练的MRC模型无法在真实的问答场景中生成类似人类的响应。为此，我们构建了一个名为Penguin的新数据集，以促进MRC研究，在真实场景中提供自然响应生成的训练和测试基础。具体而言，Penguin包含20万个训练数据，具备高质量、流畅、充分信息的响应。Penguin是相对规模较大的中文MRC领域自然响应生成的第一个基准。为了解决Penguin中的挑战，我们开发了两个强大的基准模型：端到端和两阶段框架。在此基础上，我们进一步设计出Prompt-BART。

    Machine reading comprehension (MRC) is an important area of conversation agents and draws a lot of attention. However, there is a notable limitation to current MRC benchmarks: The labeled answers are mostly either spans extracted from the target corpus or the choices of the given candidates, ignoring the natural aspect of high-quality responses. As a result, MRC models trained on these datasets can not generate human-like responses in real QA scenarios. To this end, we construct a new dataset called Penguin to promote the research of MRC, providing a training and test bed for natural response generation to real scenarios. Concretely, Penguin consists of 200k training data with high-quality fluent, and well-informed responses. Penguin is the first benchmark towards natural response generation in Chinese MRC on a relatively large scale. To address the challenges in Penguin, we develop two strong baselines: end-to-end and two-stage frameworks. Following that, we further design Prompt-BART
    
[^97]: PK-ICR: 基于角色和知识的互动上下文检索进行基于场景对话

    PK-ICR: Persona-Knowledge Interactive Context Retrieval for Grounded Dialogue. (arXiv:2302.06674v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.06674](http://arxiv.org/abs/2302.06674)

    PK-ICR是一种基于角色和知识的互动上下文检索方法，可以在复杂的多场景对话中同时识别角色和知识。通过利用神经问答检索模型，该方法可以在较少的计算资源下实现检索，并且通过引入空-正向排名测试方法来提高排名性能。

    

    鉴别与对话系统相关的角色和知识对于基于场景的对话应答生成至关重要。然而，目前每个对话基本上都是孤立研究的，而最近的工作中引入了更实际的多场景对话任务。我们将角色和知识双上下文识别定义为为给定的对话同时识别角色和知识的任务，在复杂的多场景对话设置中可能具有提升重要性。我们开发了一种新的基于检索的检索方法，可以同时利用对话的所有上下文信息。我们的方法通过使用神经问答检索模型，需要较少的计算资源。我们进一步介绍了一种新的空-正向排名测试方法，用于衡量与数据增强相关的语义差异样本（即困难负样本）的排名性能。

    Identifying relevant persona or knowledge for conversational systems is critical to grounded dialogue response generation. However, each grounding has been mostly researched in isolation with more practical multi-context dialogue tasks introduced in recent works. We define Persona and Knowledge Dual Context Identification as the task to identify persona and knowledge jointly for a given dialogue, which could be of elevated importance in complex multi-context dialogue settings. We develop a novel grounding retrieval method that utilizes all contexts of dialogue simultaneously. Our method requires less computational power via utilizing neural QA retrieval models. We further introduce our novel null-positive rank test which measures ranking performance on semantically dissimilar samples (i.e. hard negatives) in relation to data augmentation.
    
[^98]: 追求机器学习研究的推理复现性

    Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04054](http://arxiv.org/abs/2302.04054)

    本研究提出利用线性混合效应模型（LMEM）来分析机器学习性能评估分数，并考虑多个方差来源及其与数据特性相互作用，从而评估可靠性和可复制性，促进对机器学习算法行为的更全面理解。

    

    机器学习评估的可靠性——即在复制的模型训练运行中观察到的评估分数的一致性——受到几种非确定性来源的影响，可以被视为测量噪声。目前的趋势是去除噪声，以强制研究结果的可复制性，忽略了实现层面固有的非确定性以及算法噪声因素和数据特性之间的关键相互作用效应。这限制了从这些实验中可以得出的结论范围。我们提出的方法是将几个方差来源，包括它们与数据特性的相互作用，纳入机器学习评估的显著性和可靠性分析中，以期从训练模型的特定实例得出推理结论, 而非去除噪声。我们展示如何使用线性混合效应模型（LMEM）来分析性能评估分数，并用广义似然比检验进行统计推断。我们的方法提供了一种系统的方式来考虑算法和数据相关的噪声来源，并使我们能够量化各个方差来源对机器学习实验的可靠性和可复制性的影响。我们在一系列合成和真实数据集上演示了我们方法的实用性，并说明了我们的方法如何促进对机器学习算法行为的更全面理解。

    Reliability of machine learning evaluation -- the consistency of observed evaluation scores across replicated model training runs -- is affected by several sources of nondeterminism which can be regarded as measurement noise. Current tendencies to remove noise in order to enforce reproducibility of research results neglect inherent nondeterminism at the implementation level and disregard crucial interaction effects between algorithmic noise factors and data properties. This limits the scope of conclusions that can be drawn from such experiments. Instead of removing noise, we propose to incorporate several sources of variance, including their interaction with data properties, into an analysis of significance and reliability of machine learning evaluation, with the aim to draw inferences beyond particular instances of trained models. We show how to use linear mixed effects models (LMEMs) to analyze performance evaluation scores, and to conduct statistical inference with a generalized lik
    
[^99]: 关于遮蔽语言模型学习条件句的不一致性

    On the Inconsistencies of Conditionals Learned by Masked Language Models. (arXiv:2301.00068v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.00068](http://arxiv.org/abs/2301.00068)

    本论文研究发现，遮蔽语言模型学习的条件句往往存在着不一致性，无法从一个连贯的联合分布中推导出来。我们通过实证发现这种不一致性普遍存在于不同尺寸和配置的遮蔽语言模型中。为了解决这个问题，我们提出了条件句集合方法来在推断阶段处理不一致性。

    

    已经证明了在序列中学习预测遮蔽标记是一个对大型语言模型来说很有力的预训练目标。训练后，这些遮蔽语言模型可以提供基于双向上下文的标记分布。本论文展示了与常见假设相反，这种双向条件句经常表现出相当大的不一致性，即在考虑在一起时不能从一个连贯的联合分布导出它们。我们在遮蔽语言模型的两种常见风格（T5风格和BERT风格）的简单双字母词比较场景中通过实证量化了这种不一致性。例如，我们发现T5模型经常混淆自己对两个相似双字母词的偏好。我们还展示了不一致性在不同尺寸和配置的遮蔽语言模型中普遍存在，从RoBERTa-base到GLM-130B。作为解决这个问题的初始尝试，我们提出了条件句集合，在推断阶段处理这个问题。

    Learning to predict masked tokens in a sequence has been shown to be a powerful pretraining objective for large language models. After training, such masked language models can provide distributions of tokens conditioned on bidirectional context.  In this paper, we show that contrary to popular assumptions, such bidirectional conditionals often demonstrate considerable inconsistencies, i.e., they cannot be derived from a coherent joint distribution when considered together. We empirically quantify such inconsistencies in the simple scenario of bigram comparison for two common styles of masked language models: T5-style and BERT-style. For example, we show that T5 models often confuse their own preference regarding two similar bigrams. We show that inconsistencies exist ubiquitously in masked language models of diverse sizes and configurations, from RoBERTa-base to GLM-130B.  As an initial attempt to address this issue during the inference phase, we propose Ensemble of Conditionals, a se
    
[^100]: 用于抑郁症严重程度估计的语义相似性模型

    Semantic Similarity Models for Depression Severity Estimation. (arXiv:2211.07624v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.07624](http://arxiv.org/abs/2211.07624)

    本研究提出了一种基于社交媒体文本的语义流程，并使用不同的聚合方法对抑郁症严重程度进行估计。在Reddit上的两个基准测试中，我们的方法达到了30％的准确率。

    

    抑郁症是全球严重的公共健康问题，然而，公共卫生系统在病例检测和诊断方面的能力有限。在这方面，广泛使用社交媒体为我们提供了获取大规模公共信息的途径。计算方法可以通过利用这些用户生成的社交媒体内容作为快速筛查的支持工具。本文介绍了一种有效的语义流程，基于用户在社交媒体上的文字，研究个体的抑郁症严重程度。我们选择测试用户的句子，对应于抑郁症状和严重程度水平的代表性训练句子的索引，产生语义排名。然后，我们使用这些结果中的句子作为预测用户症状严重程度的证据。为此，我们探索了不同的聚合方法来回答四个贝克抑郁自评量表(BDI)选项中的每个症状。我们在基于Reddit的两个基准上评估了我们的方法，达到了30％。

    Depressive disorders constitute a severe public health issue worldwide. However, public health systems have limited capacity for case detection and diagnosis. In this regard, the widespread use of social media has opened up a way to access public information on a large scale. Computational methods can serve as support tools for rapid screening by exploiting this user-generated social media content. This paper presents an efficient semantic pipeline to study depression severity in individuals based on their social media writings. We select test user sentences for producing semantic rankings over an index of representative training sentences corresponding to depressive symptoms and severity levels. Then, we use the sentences from those results as evidence for predicting users' symptom severity. For that, we explore different aggregation methods to answer one of four Beck Depression Inventory (BDI) options per symptom. We evaluate our methods on two Reddit-based benchmarks, achieving 30\%
    
[^101]: 大型语言模型与哈利·波特相遇：用于与角色对齐的双语数据集

    Large Language Models Meet Harry Potter: A Bilingual Dataset for Aligning Dialogue Agents with Characters. (arXiv:2211.06869v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.06869](http://arxiv.org/abs/2211.06869)

    这个论文介绍了一个名为哈利·波特对话（HPD）的数据集，用于研究对话代理和角色对齐。该数据集包含了哈利·波特系列的对话场景，并注释了对话背景信息、说话者、角色关系和属性。通过在HPD上对大型语言模型进行评估，可以推动对话代理的发展，并提供一个通用基准来评估大型语言模型与特定角色对齐的能力。

    

    近年来，像ChatGPT和GPT4这样的对话式大型语言模型展示了在构建开放领域对话代理方面的巨大潜力。然而，由于角色表现的复杂性和缺乏全面的注释，将这些代理与特定角色或个体对齐仍然是一个相当大的挑战。在本文中，我们介绍了哈利·波特对话（HPD）数据集，旨在推动对话代理和角色对齐的研究。该数据集涵盖了哈利·波特系列的所有对话场景（包括英文和中文），并注释了重要的背景信息，包括对话场景、说话者、角色关系和属性。这些详细的注释可能使大型语言模型能够实现基于角色的对话能力。此外，它还可以作为一个通用基准，评估一个大型语言模型与特定角色对齐的能力。我们在HPD上使用细致的评估指标来评估大型语言模型。

    In recent years, Dialogue-style Large Language Models (LLMs) such as ChatGPT and GPT4 have demonstrated immense potential in constructing open-domain dialogue agents. However, aligning these agents with specific characters or individuals remains a considerable challenge due to the complexities of character representation and the lack of comprehensive annotations. In this paper, we introduce the Harry Potter Dialogue (HPD) dataset, designed to advance the study of dialogue agents and character alignment. The dataset encompasses all dialogue sessions (in both English and Chinese) from the Harry Potter series and is annotated with vital background information, including dialogue scenes, speakers, character relationships, and attributes. These extensive annotations may empower LLMs to unlock character-driven dialogue capabilities. Furthermore, it can serve as a universal benchmark for evaluating how well can a LLM aligning with a specific character. We benchmark LLMs on HPD using both fine
    
[^102]: Small-Text: Python中的文本分类主动学习

    Small-Text: Active Learning for Text Classification in Python. (arXiv:2107.10314v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.10314](http://arxiv.org/abs/2107.10314)

    Small-Text是一个Python中的易于使用的主动学习库，用于文本分类，它集成了多种先进的查询策略和著名的机器学习库，支持单标签和多标签分类。研究者使用该库调研了最新的SetFit训练范式的性能，并与传统方法进行了比较。

    

    我们引入了一个易于使用的主动学习库small-text，提供Python中的池式主动学习，用于单标签和多标签文本分类。它提供了许多预先实现的最先进的查询策略，包括一些利用GPU的策略。标准化的接口允许组合各种分类器、查询策略和停止准则，便于快速混搭，方便快速开发主动学习实验和应用。为了使各种分类器和查询策略对主动学习可访问，small-text集成了几个著名的机器学习库，包括scikit-learn、PyTorch和Hugging Face transformers。后两个集成是可选安装的扩展，因此可以使用GPU，但不是必需的。使用这个新库，我们研究了最近发布的SetFit训练范式的性能，将其与普通方法进行了比较。

    We introduce small-text, an easy-to-use active learning library, which offers pool-based active learning for single- and multi-label text classification in Python. It features numerous pre-implemented state-of-the-art query strategies, including some that leverage the GPU. Standardized interfaces allow the combination of a variety of classifiers, query strategies, and stopping criteria, facilitating a quick mix and match, and enabling a rapid and convenient development of both active learning experiments and applications. With the objective of making various classifiers and query strategies accessible for active learning, small-text integrates several well-known machine learning libraries, namely scikit-learn, PyTorch, and Hugging Face transformers. The latter integrations are optionally installable extensions, so GPUs can be used but are not required. Using this new library, we investigate the performance of the recently published SetFit training paradigm, which we compare to vanilla 
    
[^103]: 防御自然语言生成中的后门攻击

    Defending Against Backdoor Attacks in Natural Language Generation. (arXiv:2106.01810v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2106.01810](http://arxiv.org/abs/2106.01810)

    本研究通过形式化的定义，研究了后门攻击对自然语言生成模型的影响，并设计了针对这些攻击的防御策略。测试生成目标给定源的反向概率可以有效防御各种攻击类型，并解决了在对话生成等自然语言生成任务中的“一对多”问题。

    

    神经网络模型的脆弱性使得当前的自然语言生成系统易受后门攻击，生成可能包含性别歧视或冒犯性的恶意序列。然而，目前对后门攻击如何影响当前自然语言生成模型以及如何防御这些攻击的研究还很有限。在本研究中，我们通过给出后门攻击和防御的形式化定义，在机器翻译和对话生成这两个重要的自然语言生成任务上进行了调查。根据自然语言生成模型的固有特性（例如，在给定上下文的情况下生成连贯单词序列），我们设计了针对攻击的防御策略。我们发现，在生成目标给定源的反向概率测试中，能够有效防御所有不同类型的攻击，并能够处理对话生成等许多自然语言生成任务中的“一对多”问题。我们希望这项工作能提高对后门攻击风险的认识。

    The frustratingly fragile nature of neural network models make current natural language generation (NLG) systems prone to backdoor attacks and generate malicious sequences that could be sexist or offensive. Unfortunately, little effort has been invested to how backdoor attacks can affect current NLG models and how to defend against these attacks. In this work, by giving a formal definition of backdoor attack and defense, we investigate this problem on two important NLG tasks, machine translation and dialog generation. Tailored to the inherent nature of NLG models (e.g., producing a sequence of coherent words given contexts), we design defending strategies against attacks. We find that testing the backward probability of generating sources given targets yields effective defense performance against all different types of attacks, and is able to handle the {\it one-to-many} issue in many NLG tasks such as dialog generation. We hope that this work can raise the awareness of backdoor risks 
    
[^104]: GraphFormers: GNN嵌套Transformer用于文本图的表示学习

    GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph. (arXiv:2105.02605v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2105.02605](http://arxiv.org/abs/2105.02605)

    GraphFormers是一种将GNN嵌套到Transformer中的方法，通过迭代式的工作流程，准确理解文本图中每个节点的语义，同时引入渐进式学习加速训练。

    

    文本图的表示学习是基于个体文本特征和邻域信息生成节点低维嵌入的过程。最近预训练语言模型和图神经网络的突破推动了相应技术的发展。现有的工作主要依赖级联模型架构：首先，节点的文本特征由语言模型独立编码；然后，文本嵌入由图神经网络聚合。然而，上述架构由于对文本特征的独立建模而受到限制。在这项工作中，我们提出了GraphFormers，其中GNN的分层组件嵌套在语言模型的Transformer块旁边。通过提出的架构，文本编码和图聚合融合为一个迭代式的工作流程，从全局视角准确理解每个节点的语义。此外，一种渐进式学习方法被引入以加速训练过程。

    The representation learning on textual graph is to generate low-dimensional embeddings for the nodes based on the individual textual features and the neighbourhood information. Recent breakthroughs on pretrained language models and graph neural networks push forward the development of corresponding techniques. The existing works mainly rely on the cascaded model architecture: the textual features of nodes are independently encoded by language models at first; the textual embeddings are aggregated by graph neural networks afterwards. However, the above architecture is limited due to the independent modeling of textual features. In this work, we propose GraphFormers, where layerwise GNN components are nested alongside the transformer blocks of language models. With the proposed architecture, the text encoding and the graph aggregation are fused into an iterative workflow, {making} each node's semantic accurately comprehended from the global perspective. In addition, a {progressive} learn
    
[^105]: 关于迁移到永续企业系统的研究

    On migration to Perpetual Enterprise System. (arXiv:2104.04844v4 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2104.04844](http://arxiv.org/abs/2104.04844)

    本研究提出了一种实际的方法，用于将组织的计算系统迁移到一个可以永久演进且整合整个组织的新系统，强调治理和技术两个方面的重要性。

    

    本文介绍了一种实际的方法，用于将组织的计算系统迁移到一个可以永久演进且整合整个组织的新系统。治理方面和纯技术IT方面一样重要，甚至更重要：人力资源、招标等。迁移意味着不是从零开始。

    This document describes a pragmatic approach on how to migrate an organisation computer system towards a new system that could evolve forever, addresses the whole organisation and it is integrated.  Governance aspects are as important, if not more, than purely technical IT aspects: human resources, call for tenders, and similar. Migration implies that one is not starting from a green field.
    

