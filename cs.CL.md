# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [OpenAGI: When LLM Meets Domain Experts.](http://arxiv.org/abs/2304.04370) | 基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。 |
| [^2] | [WebBrain: Learning to Generate Factually Correct Articles for Queries by Grounding on Large Web Corpus.](http://arxiv.org/abs/2304.04358) | 本文讲述了一种新的自然语言处理任务——WebBrain，它通过挖掘Web中的支持证据，为查询生成事实正确简短文章。我们从维基百科中提取了数据集WebBrain-Raw，构建了WebBrain-R和WebBrain-G数据集。我们还介绍了一个新的生成事实性的框架ReGen。 |
| [^3] | [Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study.](http://arxiv.org/abs/2304.04339) | 本文对ChatGPT作为情感分析器进行了初步评估，包括标准评估、极性转移评估、开放域评估和情感推理评估，共涉及18个数据集和5个情感分析任务。与经过微调的BERT和最先进的模型进行了对比，并进行了人工评估和案例研究。 |
| [^4] | [ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes.](http://arxiv.org/abs/2304.04321) | ARNOLD是一个评估基于语言引导、具有连续状态的现实3D场景任务学习的基准测试，涉及8个语言条件任务，在语言引导下帮助机器人学习理解物体状态和学习连续目标的策略。 |
| [^5] | [FrenchMedMCQA: A French Multiple-Choice Question Answering Dataset for Medical domain.](http://arxiv.org/abs/2304.04280) | FrenchMedMCQA是法语医学MCQA数据集，包含3,105道真实考试题目。需要使用医学领域或MCQA任务专用的表示形式来获得更好的性能。 |
| [^6] | [A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding.](http://arxiv.org/abs/2304.04256) | 本文研究了ChatGPT在零样本对话理解任务中的理解能力，结果显示ChatGPT存在巨大潜力，但在SLU方面存在困难。分析显示在DST任务中，多轮交互对于ChatGPT的表现有益处。此外，还总结了ChatGPT在对话理解任务中的一些意想不到的行为。 |
| [^7] | [Editable User Profiles for Controllable Text Recommendation.](http://arxiv.org/abs/2304.04250) | 本文提出了一种新的概念值瓶颈模型LACE，用于可控文本推荐。该模型基于用户文档学习个性化的概念表示，并通过多种交互方式为用户提供了控制推荐的机制，验证了在离线和在线实验中该模型的推荐质量和有效性。 |
| [^8] | [RISC: Generating Realistic Synthetic Bilingual Insurance Contract.](http://arxiv.org/abs/2304.04212) | 本文介绍了一个名为RISC的Python开源包数据生成器，它可以生成类似于魁北克省法规保险格式的汽车保险合同，包括法语和英语版本，基于此生成的RISCBAC数据集可用于NLP研究中的无监督自动摘要、问答、文本简化、机器翻译任务，以及监督任务。 |
| [^9] | [Extractive Summarization via ChatGPT for Faithful Summary Generation.](http://arxiv.org/abs/2304.04193) | 本文评估了ChatGPT在抽取式摘要任务中的性能，并发现其ROUGE得分相对于传统微调方法仍有待提高。研究探索了上下文学习和思维链推理对其性能提升的有效性，并通过采用先抽取后生成流程和句子选择模块缓解了其生成摘要的忠实性问题。 |
| [^10] | [Team QUST at SemEval-2023 Task 3: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting Online News Genre, Framing and Persuasion Techniques.](http://arxiv.org/abs/2304.04190) | 本文研究了单语和多语方法来检测在线新闻的类型、框架和说服技巧，并发现多语方法比单语方法更好，使用类权重和样本权重的组合对预训练的多语模型进行微调可用于应对多数类不平衡的问题，在SemEval2023任务3中提交的系统在意大利语和西班牙语（零样本）的子任务1中排名第二。 |
| [^11] | [Similarity-Aware Multimodal Prompt Learning for Fake News Detection.](http://arxiv.org/abs/2304.04187) | 该论文提出了一种相似度感知的多模态提示学习框架，用于有效融合不同模态信息，实现假新闻检测。所设计的相似度感知融合模块可以量化地根据不同模态之间的相似性权衡每个模态的重要性，有效避免了多模态融合中可能存在的噪声干扰。在两个基准数据集上的实验结果也证明提出的框架实现了最先进的性能，超越了其他强大基线方法。 |
| [^12] | [An investigation of speaker independent phrase break models in End-to-End TTS systems.](http://arxiv.org/abs/2304.04157) | 本文研究了在端到端TTS系统中，加入语调断点预测模型是否有用以及如何衡量其有效性。经过实验验证，使用训练好的语调模型预测断点的故事比未使用预测断点的故事更受欢迎。 |
| [^13] | [Continual Graph Convolutional Network for Text Classification.](http://arxiv.org/abs/2304.04152) | 本文提出了一个持续图卷积网络模型(ContGCN)，在训练和测试阶段均提出了新的全词元-任意文档范式，通过一个出现记忆模块和一个自监督对比学习目标进行无标签的更新ContGCN，可将从观察文档推广到未观察文档的推理用于在线文本分类系统中。 |
| [^14] | [Multi-class Categorization of Reasons behind Mental Disturbance in Long Texts.](http://arxiv.org/abs/2304.04118) | 本篇论文解决了对于长文本心理障碍原因的多类别分类问题，并使用Longformer取得了最新的最优结果，推动心理疾病因果分析的发展。 |
| [^15] | [Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding.](http://arxiv.org/abs/2304.04099) | 本研究提出了一种新颖的主题嵌入方法和一个可扩展的无监督在线故事发现框架USTORY，可以动态表示文章和故事，并考虑它们共享的时间主题和新颖性，以帮助人们消化大量的新闻流。 |
| [^16] | [Interpretable Multi Labeled Bengali Toxic Comments Classification using Deep Learning.](http://arxiv.org/abs/2304.04087) | 本文提出了一种基于深度学习的方法，用于分类孟加拉语的有害评论。使用LSTM和BERT嵌入实现了二元分类任务的高准确率，同时使用卷积神经网络和双向LSTM与注意机制组合实现了多标签分类，准确率和加权F1-score均较高。 |
| [^17] | [tmn at SemEval-2023 Task 9: Multilingual Tweet Intimacy Detection using XLM-T, Google Translate, and Ensemble Learning.](http://arxiv.org/abs/2304.04054) | 本文介绍了对于SemEval-2023的任务9，提出了一种基于transformer的系统，使用了集成学习，在多语言推特亲密度检测中排名第4，达到了0.5688的宏平均F1分数。为了提高对未见语言的性能表现，每个推特都进行了英文翻译的补充。 |
| [^18] | [Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder.](http://arxiv.org/abs/2304.04052) | 该论文通过对正则化编码器-解码器结构进行比较，分析了仅解码器语言模型框架和编码器-解码器框架的效果。 |
| [^19] | [Bipol: A Novel Multi-Axes Bias Evaluation Metric with Explainability for NLP.](http://arxiv.org/abs/2304.04029) | 本文创造了一种新的度量标准 bipol 以检测文本数据中的社交偏见。该标准包括语料库级别评估和句子级别评估两个步骤，并使用 SotA 架构创建了新模型以检测多个轴的偏差。同时，作者还创造了一个大型数据集来训练偏见检测模型，并公开了相关代码。 |
| [^20] | [WikiGoldSK: Annotated Dataset, Baselines and Few-Shot Learning Experiments for Slovak Named Entity Recognition.](http://arxiv.org/abs/2304.04026) | 本研究介绍了第一个具有可观规模的人工标记的斯洛伐克NER数据集WikiGoldSK，通过评估当前最先进的多语言预训练语言模型，与现有的银标准数据集进行比较，并进行了少样本实验。银标准数据集上的训练能够产生更好的结果。 |
| [^21] | [MphayaNER: Named Entity Recognition for Tshivenda.](http://arxiv.org/abs/2304.03952) | MphayaNER是第一个适用于茨汉文达语的NER语料库，研究通过在语料库上微调最先进的模型，探索了茨文达语与其他相关班图语之间的零样本转移。用chiShona数据扩充MphayaNER可以显著提高模型性能。 |
| [^22] | [Comparing Code Explanations Created by Students and Large Language Models.](http://arxiv.org/abs/2304.03938) | 该论文探讨了采用大型语言模型生成代码解释的潜力，以帮助学生提高理解和解释代码的能力。 |
| [^23] | [An Empirical Study and Improvement for Speech Emotion Recognition.](http://arxiv.org/abs/2304.03899) | 本研究提出一种改进的多模式情感识别模型，利用透视损失来融合音频和文本模态信息，取得了IEMOCAP数据集上的最新最优结果。 |
| [^24] | [The Short Text Matching Model Enhanced with Knowledge via Contrastive Learning.](http://arxiv.org/abs/2304.03898) | 提出了一种短文本匹配模型，使用生成模型生成补充句子，结合对比学习和外部知识进行语义匹配，并使用关键词避免噪声问题。 |
| [^25] | [Factify 2: A Multimodal Fake News and Satire News Dataset.](http://arxiv.org/abs/2304.03897) | 本文提供了改进的多模态事实核查数据集Factify 2，其支持视觉和文本数据的蕴含关系。该数据集以支持、无证据和驳斥三个类别为主，包含50,000个新的数据实例，并提供一种基于BERT和Vision Transformer的最新事实核查模型，优于现有最先进的方法。 |
| [^26] | [Why think step-by-step? Reasoning emerges from the locality of experience.](http://arxiv.org/abs/2304.03843) | 本文通过语言模型研究何时以及为什么推理是有帮助的，测试推理在训练数据由相互影响强烈的局部变量集群组成时是否有效。通过一步步的推理，能够将准确的局部推理链接在一起，以估算在训练中没有同时观察到的变量之间的关系。 |
| [^27] | [Bridging Nations: Quantifying the Role of Multilinguals in Communication on Social Media.](http://arxiv.org/abs/2304.03797) | 研究欧洲 Twitter 网络，通过因果推断技术，量化多语用户在跨语言信息交流中的结构作用和沟通影响。多语者的介数中心度比单语用户高13％，且具有多语网络邻居的单语用户分享其他语言域名和hashtag的可能性分别增加了16倍和4倍。多语者对于传播那些单语同胞不易获得的信息具有更大的影响力。 |
| [^28] | [Hierarchical Catalogue Generation for Literature Review: A Benchmark.](http://arxiv.org/abs/2304.03512) | 研究提出了一个名为HiCatGLR任务，致力于为文献综述生成分层目录，它可以从多篇论文中提取和组织重要信息。为了解决现有研究中缺少清晰逻辑层次结构概述的问题，提供了一个具有挑战性的解决方案并创建了新的数据集。通过此项研究，可以更加准确地评估模型性能并验证数据集的高质量和评估指标的有效性。 |
| [^29] | [CAPOT: Creating Robust Dense Query Encoders using Post Training Contrastive Alignment.](http://arxiv.org/abs/2304.03401) | CAPOT使用后训练对比对齐的方法，提高模型对于噪声查询的健壮性，表现类似于数据增强但没有其开销。 |
| [^30] | [Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT.](http://arxiv.org/abs/2304.02213) | 本文介绍了一个新的自然语言处理任务——结构化信息推理（SIS），利用GPT-3模型能够准确提取材料科学设备层面的信息，并通过实验预测PCE和反向预测参数，展示了大型语言模型在材料学中的巨大潜力。 |
| [^31] | [Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models.](http://arxiv.org/abs/2304.01852) | 本文全面介绍了最先进的大型语言模型ChatGPT和GPT-4，包括其在各个领域的前景应用，并着重介绍了大规模预训练、指令微调和人类反馈的强化学习创新。ChatGPT/GPT-4在自然语言处理应用方面表现突出，同时在其他领域也具有潜力。 |
| [^32] | [A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection.](http://arxiv.org/abs/2304.01492) | 该文介绍了一个利用对比传递框架和传播结构，将从充足资源的谣言数据学到的特征适应于低资源情况下的方式，可以检测到跨越语言和领域界限的谣言。 |
| [^33] | [Federated Learning Based Multilingual Emoji Prediction In Clean and Attack Scenarios.](http://arxiv.org/abs/2304.01005) | 本文提出了一种基于联邦学习的多语言表情符号预测方法，在干净或攻击情境下均有效，同时保护了用户数据隐私。 |
| [^34] | [Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study.](http://arxiv.org/abs/2304.00723) | 本文介绍了大型语言模型在无参考文本质量评估中的应用研究。研究结果表明，利用ChatGPT生成的显式得分是最有效和可靠的方法。 |
| [^35] | [On the Creativity of Large Language Models.](http://arxiv.org/abs/2304.00008) | 这篇论文探讨了大型语言模型的创造性问题，分析了与之相关的机器创造性的难点和易点，并重点分析了这些技术在创意产业中的社会影响。 |
| [^36] | [A Survey of Large Language Models.](http://arxiv.org/abs/2303.18223) | 本文综述了大型语言模型的研究历程以及最近的预训练语言模型(PLMs)，并强调模型扩展将带来性能改进和特殊能力的发掘。 |
| [^37] | [Training Language Models with Language Feedback at Scale.](http://arxiv.org/abs/2303.16755) | 本文提出一种新方法，即利用更丰富的语言反馈进行模仿学习，通过三个迭代步骤对语言模型进行训练以生成更符合人类偏好的输出。 |
| [^38] | [Guiding Large Language Models via Directional Stimulus Prompting.](http://arxiv.org/abs/2302.11520) | 该文介绍了一个新的框架，用于通过生成定向刺激来指导大型语言模型在下游任务中生成所需的输出。通过策略语言模型的训练，该框架可以适应于各种语言模型和任务，并在摘要和对话生成任务中取得了最先进的表现。 |
| [^39] | [RESDSQL: Decoupling Schema Linking and Skeleton Parsing for Text-to-SQL.](http://arxiv.org/abs/2302.05965) | 提出了一种排名增强的编码和骨架感知解码框架，以解耦文本到SQL中的架构连接和骨架解析，从而减轻对架构连接的工作量并隐式限制SQL解析。 |
| [^40] | [Double Permutation Equivariance for Knowledge Graph Completion.](http://arxiv.org/abs/2302.01313) | 本研究提出了双排列等变性的KG表示方法，可以使神经网络在KG中执行复杂的逻辑推理任务，并在多个归纳KG完成任务中实现了最先进的Hits@10测试准确率。双排列等变性在KG中开辟了新的研究方向。 |
| [^41] | [How would Stance Detection Techniques Evolve after the Launch of ChatGPT?.](http://arxiv.org/abs/2212.14548) | ChatGPT是一种新的预训练语言模型，可以用于解决立场检测问题，并提供了其预测的解释能力。 |
| [^42] | [MiLMo:Minority Multilingual Pre-trained Language Model.](http://arxiv.org/abs/2212.01779) | 本论文构建了一个名为MiLMo的多语言预训练模型，该模型在少数民族语言任务上表现较好，并构建了一个少数民族多语言文本分类数据集。 |
| [^43] | [Lifelong Embedding Learning and Transfer for Growing Knowledge Graphs.](http://arxiv.org/abs/2211.15845) | 现有的知识图谱嵌入模型主要集中在静态KG上，无法随着KG不断增长而及时获取新知识，本文引入了终身KG嵌入模型，实现对不断增长的知识图谱的终身嵌入学习与转移，通过嵌入转移策略和正则化方法避免灾难性遗忘。 |
| [^44] | [STAGE: Span Tagging and Greedy Inference Scheme for Aspect Sentiment Triplet Extraction.](http://arxiv.org/abs/2211.15003) | STAGE是一种新方法用于处理Aspect Sentiment Triplet Extraction（ASTE）任务，可以处理单词具有多个角色或方面/意见术语由多个单词组成等复杂情况，通过Span标记和贪心推理方案来提取情感三元组，更准确地识别给定句子中方面术语、相应的意见术语和相关的情感极性。 |
| [^45] | [Discharge Summary Hospital Course Summarisation of In Patient Electronic Health Record Text with Clinical Concept Guided Deep Pre-Trained Transformer Models.](http://arxiv.org/abs/2211.07126) | 本文探讨了使用深度学习方法对医院住院病历文本进行出院总结的方法，包括提取和抽象总结，同时测试了一种结合医学概念本体论的混合模型。 |
| [^46] | [A Transformer-Based Substitute Recommendation Model Incorporating Weakly Supervised Customer Behavior Data.](http://arxiv.org/abs/2211.02533) | 本文将替代品推荐适应到语言匹配问题中，并通过设计新的转换方法去除信号噪音，并考虑了多语言支持。该模型已成功在一个大型电子商务网站上的11个市场和6种语言中部署，提高了顾客忠诚度和购买率。 |
| [^47] | [M-SpeechCLIP: Leveraging Large-Scale, Pre-Trained Models for Multilingual Speech to Image Retrieval.](http://arxiv.org/abs/2211.01180) | 本文研究使用大规模预训练模型多语音到图像检索的方法，取得了在非英语检索中比当前最先进性能大幅提高的效果，并证明这些模型也适用于跨语音检索和跨语音文本检索。 |
| [^48] | [Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change).](http://arxiv.org/abs/2206.10498) | 本研究提出了一个用于测评LLMs规划和变化推理能力的框架，并测试了流行的LLMs (GPT-3 和 GShard) 在此基准上的表现。研究发现这些模型在最简单的规划任务上都表现不佳，强调了目前LLMs推理能力的严重限制，建议需要大量工作来开发更先进的LLM基础系统来满足实际应用需求。 |
| [^49] | [InCoder: A Generative Model for Code Infilling and Synthesis.](http://arxiv.org/abs/2204.05999) | InCoder是一种统一的生成模型，可以进行程序合成和双向上下文的代码填充，是第一个能够直接进行零样本代码填充的生成模型。 |
| [^50] | [The Factual Inconsistency Problem in Abstractive Text Summarization: A Survey.](http://arxiv.org/abs/2104.14839) | 摘要文本自动概括模型的抽象化能力对于生成准确摘要文本是双刃剑，研究主要关注设计事实感知的评估指标和改进模型的训练以减少事实不一致问题。 |

# 详细

[^1]: OpenAGI：当LLM遇到领域专家

    OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])

    [http://arxiv.org/abs/2304.04370](http://arxiv.org/abs/2304.04370)

    基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。

    

    人类具有将基本技能组合成复杂技能以解决复杂任务的显著能力。这种能力对于人工智能同样重要，因此，我们断言，除了开发大型综合智能模型外，将不同领域专家模型应用于复杂任务解决能力同样关键，以在人工智能通用智能的追求中使其具备这种能力。最近的大型语言模型（LLM）的发展证明其具有出色的学习和推理能力，使它们成为选择、综合和执行外部模型以解决复杂任务的控制器的有前途的选择。在这个项目中，我们开发了一个名为OpenAGI的开源AGI研究平台，专门设计为提供复杂的多步骤任务，并配有任务特定的数据集、评估指标和各种可扩展模型。OpenAGI将复杂任务阐释为自然语言问答，旨在促进领域专家和语言模型之间的协同作用。

    Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
    
[^2]: WebBrain: 基于大型Web语料库，通过挖掘支持证据生成事实正确文章的学习

    WebBrain: Learning to Generate Factually Correct Articles for Queries by Grounding on Large Web Corpus. (arXiv:2304.04358v1 [cs.CL])

    [http://arxiv.org/abs/2304.04358](http://arxiv.org/abs/2304.04358)

    本文讲述了一种新的自然语言处理任务——WebBrain，它通过挖掘Web中的支持证据，为查询生成事实正确简短文章。我们从维基百科中提取了数据集WebBrain-Raw，构建了WebBrain-R和WebBrain-G数据集。我们还介绍了一个新的生成事实性的框架ReGen。

    

    本文介绍一种新的自然语言处理任务——从Web挖掘支持证据，为查询生成带参考文献的简短事实文章。在这个名为WebBrain的任务中，最终目标是为维基百科中未出现的事实查询生成流畅、信息丰富、事实正确的简短文章。为了实现对WebBrain的实验，我们按照维基百科中的文章和可爬行的维基百科参考文献提取英语数据集WebBrain-Raw。WebBrain-Raw比以前最大的同行数据集大十倍，可以极大地惠及研究社区。从WebBrain-Raw中，我们构建了两个任务特定数据集：WebBrain-R和WebBrain-G，分别用于训练领域内的检索器和生成器。此外，我们在WebBrain上实证分析了当前最先进的自然语言处理技术的表现，并介绍了一个增强证据支持的生成事实性的新框架ReGen。

    In this paper, we introduce a new NLP task -- generating short factual articles with references for queries by mining supporting evidence from the Web. In this task, called WebBrain, the ultimate goal is to generate a fluent, informative, and factually-correct short article (e.g., a Wikipedia article) for a factual query unseen in Wikipedia. To enable experiments on WebBrain, we construct a large-scale dataset WebBrain-Raw by extracting English Wikipedia articles and their crawlable Wikipedia references. WebBrain-Raw is ten times larger than the previous biggest peer dataset, which can greatly benefit the research community. From WebBrain-Raw, we construct two task-specific datasets: WebBrain-R and WebBrain-G, which are used to train in-domain retriever and generator, respectively. Besides, we empirically analyze the performances of the current state-of-the-art NLP techniques on WebBrain and introduce a new framework ReGen, which enhances the generation factualness by improved evidence
    
[^3]: ChatGPT是一个好的情感分析器吗？一项初步研究。

    Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study. (arXiv:2304.04339v1 [cs.CL])

    [http://arxiv.org/abs/2304.04339](http://arxiv.org/abs/2304.04339)

    本文对ChatGPT作为情感分析器进行了初步评估，包括标准评估、极性转移评估、开放域评估和情感推理评估，共涉及18个数据集和5个情感分析任务。与经过微调的BERT和最先进的模型进行了对比，并进行了人工评估和案例研究。

    

    最近，ChatGPT在研究和公众的关注下受到了极大的关注。我们特别想知道它是否可以作为通用情感分析器。为此，在这项工作中，我们对ChatGPT在文本中包含的意见、情感和情绪的理解进行了初步评估。具体而言，我们在四个设置下进行评估，包括标准评估、极性转移评估、开放域评估和情感推理评估。以上评估涉及18个基准数据集和5个代表性情感分析任务，我们将ChatGPT与经过微调的BERT和相应的最先进模型进行了对比，并在末端任务上进行了评估。此外，我们还进行了人工评估，并展示了一些定性案例研究以深入理解其情感分析能力。

    Recently, ChatGPT has drawn great attention from both the research community and the public. We are particularly curious about whether it can serve as a universal sentiment analyzer. To this end, in this work, we provide a preliminary evaluation of ChatGPT on the understanding of opinions, sentiments, and emotions contained in the text. Specifically, we evaluate it in four settings, including standard evaluation, polarity shift evaluation, open-domain evaluation, and sentiment inference evaluation. The above evaluation involves 18 benchmark datasets and 5 representative sentiment analysis tasks, and we compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on end-task. Moreover, we also conduct human evaluation and present some qualitative case studies to gain a deep comprehension of its sentiment analysis capabilities.
    
[^4]: ARNOLD：基于连续状态实现的现实3D场景语言引导任务学习基准测试

    ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes. (arXiv:2304.04321v1 [cs.AI])

    [http://arxiv.org/abs/2304.04321](http://arxiv.org/abs/2304.04321)

    ARNOLD是一个评估基于语言引导、具有连续状态的现实3D场景任务学习的基准测试，涉及8个语言条件任务，在语言引导下帮助机器人学习理解物体状态和学习连续目标的策略。

    

    在现实世界中，理解物体的连续状态对于任务学习和规划至关重要。然而，大多数任务学习基准测试假定目标状态是离散的(例如二进制状态)，这给学习复杂任务和将学习策略从模拟环境转移到现实世界带来了挑战。此外，状态离散化限制了机器人根据动作和状态的引导遵循人类指令的能力。为了解决这些挑战，我们提出了ARNOLD，这是一个评估基于语言引导、具有连续状态的现实3D场景任务学习的基准测试。ARNOLD由8个语言条件任务组成，涉及理解物体状态和学习连续目标的策略。为了促进语言引导学习，我们提供了模板生成的语言描述的专家演示。我们通过使用最新的语言条件策略学习模型来评估任务的性能。我们的结果表明，ARNOLD为基于连续状态的语言引导任务学习提供了一个具有挑战性的环境，并可用于评估从模拟场景到现实世界的学习策略的泛化。

    Understanding the continuous states of objects is essential for task learning and planning in the real world. However, most existing task learning benchmarks assume discrete(e.g., binary) object goal states, which poses challenges for the learning of complex tasks and transferring learned policy from simulated environments to the real world. Furthermore, state discretization limits a robot's ability to follow human instructions based on the grounding of actions and states. To tackle these challenges, we present ARNOLD, a benchmark that evaluates language-grounded task learning with continuous states in realistic 3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve understanding object states and learning policies for continuous goals. To promote language-instructed learning, we provide expert demonstrations with template-generated language descriptions. We assess task performance by utilizing the latest language-conditioned policy learning models. Our results ind
    
[^5]: FrenchMedMCQA: 用于医学领域的法语多项选择问题回答数据集

    FrenchMedMCQA: A French Multiple-Choice Question Answering Dataset for Medical domain. (arXiv:2304.04280v1 [cs.CL])

    [http://arxiv.org/abs/2304.04280](http://arxiv.org/abs/2304.04280)

    FrenchMedMCQA是法语医学MCQA数据集，包含3,105道真实考试题目。需要使用医学领域或MCQA任务专用的表示形式来获得更好的性能。

    

    本文介绍了FrenchMedMCQA，这是公开发布的医学领域法语多项选择问题回答（MCQA）数据集。该数据集由3,105道真实的法国药学专业文凭考试题目组成，包括单项和多项选择题，每个实例包含标识符、问题、五个可能的答案和它们的手动纠正。我们还提出了第一个基线模型来自动处理该MCQA任务，以报告当前的性能和突出任务的难点。结果的详细分析显示，需要有适应于医学领域或MCQA任务的表示形式：在我们的情况下，即使FrenchMedMCQA是以法语书写的，英语专门的模型也比通用的法语模型表现更好。语料库、模型和工具都可在线获得。

    This paper introduces FrenchMedMCQA, the first publicly available Multiple-Choice Question Answering (MCQA) dataset in French for medical domain. It is composed of 3,105 questions taken from real exams of the French medical specialization diploma in pharmacy, mixing single and multiple answers. Each instance of the dataset contains an identifier, a question, five possible answers and their manual correction(s). We also propose first baseline models to automatically process this MCQA task in order to report on the current performances and to highlight the difficulty of the task. A detailed analysis of the results showed that it is necessary to have representations adapted to the medical domain or to the MCQA task: in our case, English specialized models yielded better results than generic French ones, even though FrenchMedMCQA is in French. Corpus, models and tools are available online.
    
[^6]: ChatGPT在零样本对话理解任务中的初步评估

    A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding. (arXiv:2304.04256v1 [cs.CL])

    [http://arxiv.org/abs/2304.04256](http://arxiv.org/abs/2304.04256)

    本文研究了ChatGPT在零样本对话理解任务中的理解能力，结果显示ChatGPT存在巨大潜力，但在SLU方面存在困难。分析显示在DST任务中，多轮交互对于ChatGPT的表现有益处。此外，还总结了ChatGPT在对话理解任务中的一些意想不到的行为。

    

    零样本对话理解旨在使对话跟踪用户需求，而无需任何训练数据，近年来受到越来越多的关注。本文研究了ChatGPT在零样本对话理解任务中，包括口语语言理解（SLU）和对话状态跟踪（DST）方面的理解能力。在四个流行基准测试中的实验结果揭示了ChatGPT在零样本对话理解方面的巨大潜力。此外，广泛的分析表明，ChatGPT在DST任务中受益于多轮交互式提示，但在SLU方面很难执行槽填充。最后，我们总结了ChatGPT在对话理解任务中的一些意想不到的行为，希望为利用大型语言模型(LLMs)构建零样本对话理解系统的未来研究提供一些见解。

    Zero-shot dialogue understanding aims to enable dialogue to track the user's needs without any training data, which has gained increasing attention. In this work, we investigate the understanding ability of ChatGPT for zero-shot dialogue understanding tasks including spoken language understanding (SLU) and dialogue state tracking (DST). Experimental results on four popular benchmarks reveal the great potential of ChatGPT for zero-shot dialogue understanding. In addition, extensive analysis shows that ChatGPT benefits from the multi-turn interactive prompt in the DST task but struggles to perform slot filling for SLU. Finally, we summarize several unexpected behaviors of ChatGPT in dialogue understanding tasks, hoping to provide some insights for future research on building zero-shot dialogue understanding systems with Large Language Models (LLMs).
    
[^7]: 可编辑用户档案的可控文本推荐方法

    Editable User Profiles for Controllable Text Recommendation. (arXiv:2304.04250v1 [cs.IR])

    [http://arxiv.org/abs/2304.04250](http://arxiv.org/abs/2304.04250)

    本文提出了一种新的概念值瓶颈模型LACE，用于可控文本推荐。该模型基于用户文档学习个性化的概念表示，并通过多种交互方式为用户提供了控制推荐的机制，验证了在离线和在线实验中该模型的推荐质量和有效性。

    

    实现高质量推荐的方法通常依赖于从交互数据中学习潜在表示。然而这些方法没有提供给用户控制所接收的推荐的机制。本文提出了LACE，一种新颖的概念值瓶颈模型，用于可控文本推荐。LACE基于用户交互的文档检索，将每个用户表示为简洁的可读的概念集，并基于用户文档学习概念的个性化表示。该基于概念的用户档案被利用来做出推荐。我们的模型设计通过透明的用户档案，提供了控制推荐的多种直观交互方式。我们首先在三个推荐任务（温启动、冷启动和零样本）的六个数据集上进行了离线评估，验证了从LACE获得的推荐质量。接下来，我们在在线实验中验证了LACE的有效性和用户控制能力。

    Methods for making high-quality recommendations often rely on learning latent representations from interaction data. These methods, while performant, do not provide ready mechanisms for users to control the recommendation they receive. Our work tackles this problem by proposing LACE, a novel concept value bottleneck model for controllable text recommendations. LACE represents each user with a succinct set of human-readable concepts through retrieval given user-interacted documents and learns personalized representations of the concepts based on user documents. This concept based user profile is then leveraged to make recommendations. The design of our model affords control over the recommendations through a number of intuitive interactions with a transparent user profile. We first establish the quality of recommendations obtained from LACE in an offline evaluation on three recommendation tasks spanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we validate the 
    
[^8]: RISC: 生成真实的双语保险合同的合成数据

    RISC: Generating Realistic Synthetic Bilingual Insurance Contract. (arXiv:2304.04212v1 [cs.CL])

    [http://arxiv.org/abs/2304.04212](http://arxiv.org/abs/2304.04212)

    本文介绍了一个名为RISC的Python开源包数据生成器，它可以生成类似于魁北克省法规保险格式的汽车保险合同，包括法语和英语版本，基于此生成的RISCBAC数据集可用于NLP研究中的无监督自动摘要、问答、文本简化、机器翻译任务，以及监督任务。

    

    本论文介绍了一个名为RISC的Python开源包数据生成器，它可以生成类似于魁北克省法规保险格式的汽车保险合同，包括法语和英语版本。保险合同通常长达90到100页，使用专业的法律和保险术语，因此比传统的NLP语料库中的文档类别更为复杂。因此，我们介绍了基于魁北克省强制汽车保险合同的逼真保险合成双语汽车合同数据集RISCBAC。该数据集包括10,000份未经标注的法语和英语保险合同。RISCBAC可用于NLP研究中的无监督自动摘要、问答、文本简化、机器翻译等任务。此外，它还可以进一步自动标注为监督任务的数据集，例如命名实体识别。

    This paper presents RISC, an open-source Python package data generator (https://github.com/GRAAL-Research/risc). RISC generates look-alike automobile insurance contracts based on the Quebec regulatory insurance form in French and English. Insurance contracts are 90 to 100 pages long and use complex legal and insurance-specific vocabulary for a layperson. Hence, they are a much more complex class of documents than those in traditional NLP corpora. Therefore, we introduce RISCBAC, a Realistic Insurance Synthetic Bilingual Automobile Contract dataset based on the mandatory Quebec car insurance contract. The dataset comprises 10,000 French and English unannotated insurance contracts. RISCBAC enables NLP research for unsupervised automatic summarisation, question answering, text simplification, machine translation and more. Moreover, it can be further automatically annotated as a dataset for supervised tasks such as NER
    
[^9]: 采用ChatGPT进行摘要提取以生成忠实的摘要

    Extractive Summarization via ChatGPT for Faithful Summary Generation. (arXiv:2304.04193v1 [cs.CL])

    [http://arxiv.org/abs/2304.04193](http://arxiv.org/abs/2304.04193)

    本文评估了ChatGPT在抽取式摘要任务中的性能，并发现其ROUGE得分相对于传统微调方法仍有待提高。研究探索了上下文学习和思维链推理对其性能提升的有效性，并通过采用先抽取后生成流程和句子选择模块缓解了其生成摘要的忠实性问题。

    

    抽取式摘要是自然语言处理中至关重要的任务，旨在通过直接提取句子将长文档压缩为较短的版本。ChatGPT的引入引起了自然语言处理界的广泛关注，因为它在各种下游任务中具有出色的表现。然而，关于其事实性和忠实度的担忧阻碍了其在摘要系统中的实际应用。本文首先对ChatGPT在抽取式摘要上的性能进行了全面评估，并将其与传统的微调方法在各种基准数据集上进行了比较。我们的实验分析揭示，相对于现有的监督系统，ChatGPT在ROUGE分数方面的抽取式摘要性能仍然不足。此外，我们探索了上下文学习和思维链推理对其性能提升的有效性。此外，我们发现采用先抽取后生成流程以及句子选择模块可以缓解ChatGPT生成摘要的忠实性问题。总体而言，我们的研究为使用ChatGPT进行抽取式摘要提供了局限性和潜在改进的见解。

    Extractive summarization is a crucial task in natural language processing that aims to condense long documents into shorter versions by directly extracting sentences. The recent introduction of ChatGPT has attracted significant interest in the NLP community due to its remarkable performance on a wide range of downstream tasks. However, concerns regarding factuality and faithfulness have hindered its practical applications for summarization systems. This paper first presents a thorough evaluation of ChatGPT's performance on extractive summarization and compares it with traditional fine-tuning methods on various benchmark datasets. Our experimental analysis reveals that ChatGPT's extractive summarization performance is still inferior to existing supervised systems in terms of ROUGE scores. In addition, we explore the effectiveness of in-context learning and chain-of-thought reasoning for enhancing its performance. Furthermore, we find that applying an extract-then-generate pipeline with 
    
[^10]: QUST队在SemEval-2023任务3中的综合研究：检测在线新闻的类型、框架和说服技巧的单语和多语方法。

    Team QUST at SemEval-2023 Task 3: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting Online News Genre, Framing and Persuasion Techniques. (arXiv:2304.04190v1 [cs.CL])

    [http://arxiv.org/abs/2304.04190](http://arxiv.org/abs/2304.04190)

    本文研究了单语和多语方法来检测在线新闻的类型、框架和说服技巧，并发现多语方法比单语方法更好，使用类权重和样本权重的组合对预训练的多语模型进行微调可用于应对多数类不平衡的问题，在SemEval2023任务3中提交的系统在意大利语和西班牙语（零样本）的子任务1中排名第二。

    

    本文描述了QUST团队参加SemEval2023任务3的情况。首先，单语模型在任务早期对多数类进行了欠采样评估。然后，使用类权重和样本权重的组合对预训练的多语模型进行了微调。进一步研究两种不同的微调策略，分别为任务不可知和任务相关的。所有实验都在10折交叉验证下进行，多语方法比单语方法更具优势。提交的系统在意大利语和西班牙语（零样本）的子任务1中取得了第二名。

    This paper describes the participation of team QUST in the SemEval2023 task 3. The monolingual models are first evaluated with the under-sampling of the majority classes in the early stage of the task. Then, the pre-trained multilingual model is fine-tuned with a combination of the class weights and the sample weights. Two different fine-tuning strategies, the task-agnostic and the task-dependent, are further investigated. All experiments are conducted under the 10-fold cross-validation, the multilingual approaches are superior to the monolingual ones. The submitted system achieves the second best in Italian and Spanish (zero-shot) in subtask-1.
    
[^11]: 相似度感知的多模态提示学习用于假新闻检测

    Similarity-Aware Multimodal Prompt Learning for Fake News Detection. (arXiv:2304.04187v1 [cs.CL])

    [http://arxiv.org/abs/2304.04187](http://arxiv.org/abs/2304.04187)

    该论文提出了一种相似度感知的多模态提示学习框架，用于有效融合不同模态信息，实现假新闻检测。所设计的相似度感知融合模块可以量化地根据不同模态之间的相似性权衡每个模态的重要性，有效避免了多模态融合中可能存在的噪声干扰。在两个基准数据集上的实验结果也证明提出的框架实现了最先进的性能，超越了其他强大基线方法。

    

    假新闻检测的标准范式主要利用文本信息来建立新闻的真实性，然而，网上假新闻的话语通常比较微妙，需要专家知识才能使用文本信息揭露假新闻。最近，关注于多模态假新闻检测的研究已经超越了仅基于文本的方法。最近的方法利用预训练模型来提取单模态特征，或直接微调预训练模型，这成为检测假新闻的新范式。然而，这种方法要么需要大量的训练实例，要么需要更新整个预训练模型参数集，不实际可行。此外，传统的多模态方法将跨模态特征直接融合，而不考虑不相关的语义表示可能会引入噪声到多模态特征中。本文提出了一种相似度感知的多模态提示学习（SAMPLE）框架，用于假新闻检测，利用不同模态之间的内在相关性来有效地融合信息。所提出的框架包括一种新颖的相似度感知融合模块，该模块学习根据它们之间的相似度来权衡不同模态的重要性。在两个基准数据集上的实验结果表明，我们的框架实现了最先进的性能，并大幅优于强基线。

    The standard paradigm for fake news detection mainly utilizes text information to model the truthfulness of news. However, the discourse of online fake news is typically subtle and it requires expert knowledge to use textual information to debunk fake news. Recently, studies focusing on multimodal fake news detection have outperformed text-only methods. Recent approaches utilizing the pre-trained model to extract unimodal features, or fine-tuning the pre-trained model directly, have become a new paradigm for detecting fake news. Again, this paradigm either requires a large number of training instances, or updates the entire set of pre-trained model parameters, making real-world fake news detection impractical. Furthermore, traditional multimodal methods fuse the cross-modal features directly without considering that the uncorrelated semantic representation might inject noise into the multimodal features. This paper proposes a Similarity-Aware Multimodal Prompt Learning (SAMPLE) framewo
    
[^12]: 在端到端的TTS系统中，说话人独立语调断点模型的研究

    An investigation of speaker independent phrase break models in End-to-End TTS systems. (arXiv:2304.04157v1 [eess.AS])

    [http://arxiv.org/abs/2304.04157](http://arxiv.org/abs/2304.04157)

    本文研究了在端到端TTS系统中，加入语调断点预测模型是否有用以及如何衡量其有效性。经过实验验证，使用训练好的语调模型预测断点的故事比未使用预测断点的故事更受欢迎。

    

    本文提出了我们对于端到端TTS系统中语调断点预测的研究，研究动机是：（一）在端到端TTS系统中融入明确的语调模型是否有用？（二）如何评估端到端TTS系统的语调模型是否有效？特别地，我们将对儿童故事合成的语境下短语断点预测模型的效用和有效性进行评估，使用的评估指标为听众理解度。我们通过实验听力评估表明，通过使用经过训练的语调模型预测短语断点位置合成的故事比直接合成的故事更受欢迎。

    This paper presents our work on phrase break prediction in the context of end-to-end TTS systems, motivated by the following questions: (i) Is there any utility in incorporating an explicit phrasing model in an end-to-end TTS system?, and (ii) How do you evaluate the effectiveness of a phrasing model in an end-to-end TTS system? In particular, the utility and effectiveness of phrase break prediction models are evaluated in in the context of childrens story synthesis, using listener comprehension. We show by means of perceptual listening evaluations that there is a clear preference for stories synthesized after predicting the location of phrase breaks using a trained phrasing model, over stories directly synthesized without predicting the location of phrase breaks.
    
[^13]: 持续图卷积网络用于文本分类

    Continual Graph Convolutional Network for Text Classification. (arXiv:2304.04152v1 [cs.CL])

    [http://arxiv.org/abs/2304.04152](http://arxiv.org/abs/2304.04152)

    本文提出了一个持续图卷积网络模型(ContGCN)，在训练和测试阶段均提出了新的全词元-任意文档范式，通过一个出现记忆模块和一个自监督对比学习目标进行无标签的更新ContGCN，可将从观察文档推广到未观察文档的推理用于在线文本分类系统中。

    

    图卷积网络（GCN）已成功地应用于文本分类，捕获了全局非连续和长距离的语义信息。然而，尽管基于GCN的方法在离线评估中表现出有希望的结果，但它们通常遵循一个已知的词元-已知的文档范例，构造了一个固定的文档-词元图，并不能对新的文档进行推理。在线系统面临将它们部署到推断流数据的挑战。在本文中，我们提出了一个持续的GCN模型（ContGCN），以将从观察文档推广到未观察文档的推理。具体而言，我们提出了一个新的全词元-任意文档范式，用于在在线系统的训练和测试阶段中，在每个批次动态更新文档-词元图。此外，我们设计了一个出现记忆模块和一个自监督对比学习目标，以无标签的方式更新ContGCN。华为公共观点分析系统上进行了为期3个月的A/B测试。

    Graph convolutional network (GCN) has been successfully applied to capture global non-consecutive and long-distance semantic information for text classification. However, while GCN-based methods have shown promising results in offline evaluations, they commonly follow a seen-token-seen-document paradigm by constructing a fixed document-token graph and cannot make inferences on new documents. It is a challenge to deploy them in online systems to infer steaming text data. In this work, we present a continual GCN model (ContGCN) to generalize inferences from observed documents to unobserved documents. Concretely, we propose a new all-token-any-document paradigm to dynamically update the document-token graph in every batch during both the training and testing phases of an online system. Moreover, we design an occurrence memory module and a self-supervised contrastive learning objective to update ContGCN in a label-free manner. A 3-month A/B test on Huawei public opinion analysis system sho
    
[^14]: 长文本中心理障碍原因的多类别分类

    Multi-class Categorization of Reasons behind Mental Disturbance in Long Texts. (arXiv:2304.04118v1 [cs.CL])

    [http://arxiv.org/abs/2304.04118](http://arxiv.org/abs/2304.04118)

    本篇论文解决了对于长文本心理障碍原因的多类别分类问题，并使用Longformer取得了最新的最优结果，推动心理疾病因果分析的发展。

    

    在社交媒体上推断用户心理状态方面的最新进展，激发了我们的研究兴趣。我们确定并阐述了在自我报告文本中查找心理疾病因果指标的问题。过去，我们见证了基于规则的针对Facebook数据的因果解释分析研究。针对Reddit帖子的多类别因果分类的Transformer模型方面的调查，指出了使用包含多达4000个单词的长文本的问题。为了解决这个问题，我们使用Longformer，并将其编码部署在基于Transformer的分类器上。实验结果显示，Longformer在一个公开可用的包含62％ F1-score的数据集M-CAMS上取得了新的最优结果。因果分析和消融研究证明了Longformer的有效性。我们相信我们的工作有助于推动抑郁症和自杀的因果分析。

    Motivated with recent advances in inferring users' mental state in social media posts, we identify and formulate the problem of finding causal indicators behind mental illness in self-reported text. In the past, we witness the presence of rule-based studies for causal explanation analysis on curated Facebook data. The investigation on transformer-based model for multi-class causal categorization in Reddit posts point to a problem of using long-text which contains as many as 4000 words. Developing end-to-end transformer-based models subject to the limitation of maximum-length in a given instance. To handle this problem, we use Longformer and deploy its encoding on transformer-based classifier. The experimental results show that Longformer achieves new state-of-the-art results on M-CAMS, a publicly available dataset with 62\% F1-score. Cause-specific analysis and ablation study prove the effectiveness of Longformer. We believe our work facilitates causal analysis of depression and suicid
    
[^15]: 通过可扩展的主题嵌入从连续新闻流中无监督地发现故事

    Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding. (arXiv:2304.04099v1 [cs.IR])

    [http://arxiv.org/abs/2304.04099](http://arxiv.org/abs/2304.04099)

    本研究提出了一种新颖的主题嵌入方法和一个可扩展的无监督在线故事发现框架USTORY，可以动态表示文章和故事，并考虑它们共享的时间主题和新颖性，以帮助人们消化大量的新闻流。

    

    无监督地发现实时相关新闻文章故事，有助于人们在不需要昂贵人工注释的情况下消化大量的新闻流。现有的无监督在线故事发现研究的普遍方法是用符号或基于图的嵌入来表示新闻文章，并将它们逐步聚类成故事。最近的大型语言模型有望进一步改善嵌入，但是通过无差别地编码文章中的所有信息来直接采用这些模型无法有效处理富含文本且不断发展的新闻流。在这项工作中，我们提出了一种新颖的主题嵌入方法，使用现成的预训练句子编码器来动态表示文章和故事，并考虑它们共享的时间主题。为了实现无监督在线故事发现的想法，引入了一个可扩展框架USTORY，包括两个主要技术，即主题和时间感知的动态嵌入和新颖性感知的自适应聚类。

    Unsupervised discovery of stories with correlated news articles in real-time helps people digest massive news streams without expensive human annotations. A common approach of the existing studies for unsupervised online story discovery is to represent news articles with symbolic- or graph-based embedding and incrementally cluster them into stories. Recent large language models are expected to improve the embedding further, but a straightforward adoption of the models by indiscriminately encoding all information in articles is ineffective to deal with text-rich and evolving news streams. In this work, we propose a novel thematic embedding with an off-the-shelf pretrained sentence encoder to dynamically represent articles and stories by considering their shared temporal themes. To realize the idea for unsupervised online story discovery, a scalable framework USTORY is introduced with two main techniques, theme- and time-aware dynamic embedding and novelty-aware adaptive clustering, fuel
    
[^16]: 基于深度学习的可解释的多标签孟加拉有害评论分类

    Interpretable Multi Labeled Bengali Toxic Comments Classification using Deep Learning. (arXiv:2304.04087v1 [cs.CL])

    [http://arxiv.org/abs/2304.04087](http://arxiv.org/abs/2304.04087)

    本文提出了一种基于深度学习的方法，用于分类孟加拉语的有害评论。使用LSTM和BERT嵌入实现了二元分类任务的高准确率，同时使用卷积神经网络和双向LSTM与注意机制组合实现了多标签分类，准确率和加权F1-score均较高。

    

    本文提出了一个基于深度学习的方案来分类孟加拉语的有害评论，首先使用二元分类模型确定评论是否有害，然后使用多标签分类器确定该评论属于哪种毒性类型。为此，我们准备了一个手动标注的数据集，其中包含16,073个实例，其中8,488个是有害的，并且任何有害的评论可能同时属于六种有害类型-低俗，仇恨，宗教，威胁，恶意和侮辱。在二元分类任务上，使用LSTM和BERT嵌入实现了89.42％的准确率；在多标签分类器方面，使用卷积神经网络和双向LSTM（CNN-BiLSTM）与注意机制组合，获得了78.92％的准确率和0.86的加权F1-score。为了解释预测结果并解释分类期间的单词特征重要性，该方法使用了LIME技术。

    This paper presents a deep learning-based pipeline for categorizing Bengali toxic comments, in which at first a binary classification model is used to determine whether a comment is toxic or not, and then a multi-label classifier is employed to determine which toxicity type the comment belongs to. For this purpose, we have prepared a manually labeled dataset consisting of 16,073 instances among which 8,488 are Toxic and any toxic comment may correspond to one or more of the six toxic categories - vulgar, hate, religious, threat, troll, and insult simultaneously. Long Short Term Memory (LSTM) with BERT Embedding achieved 89.42% accuracy for the binary classification task while as a multi-label classifier, a combination of Convolutional Neural Network and Bi-directional Long Short Term Memory (CNN-BiLSTM) with attention mechanism achieved 78.92% accuracy and 0.86 as weighted F1-score. To explain the predictions and interpret the word feature importance during classification by the propos
    
[^17]: tmn在SemEval-2023任务9中的应用：使用XLM-T、Google翻译和集成学习进行多语言推特亲密度检测

    tmn at SemEval-2023 Task 9: Multilingual Tweet Intimacy Detection using XLM-T, Google Translate, and Ensemble Learning. (arXiv:2304.04054v1 [cs.CL])

    [http://arxiv.org/abs/2304.04054](http://arxiv.org/abs/2304.04054)

    本文介绍了对于SemEval-2023的任务9，提出了一种基于transformer的系统，使用了集成学习，在多语言推特亲密度检测中排名第4，达到了0.5688的宏平均F1分数。为了提高对未见语言的性能表现，每个推特都进行了英文翻译的补充。

    

    本文介绍了一种基于transformer的系统，针对SemEval-2023任务9：多语言推特亲密度分析进行设计。任务的目的是预测一系列推特的亲密度，范围从1（完全不亲密）到5（非常亲密）。比赛的官方训练集包含六种语言的推特（英语、西班牙语、意大利语、葡萄牙语、法语和中文）。测试集包括六种给定的语言以及外部数据，其中包括训练集中未出现的四种语言（印地语、阿拉伯语、荷兰语和韩语）。我们提出了一种基于XLM-T的解决方案，即适用于Twitter领域的多语种RoBERTa模型的集成。为了提高对未见语言的性能表现，我们对每条推特进行了英文翻译的补充。我们探究了将翻译数据应用于微调中看到的语言与未看到的语言的transformer模型的有效性，并估计使用翻译数据的策略。我们的解决方案在50个团队中排名第4，并实现了0.5688的宏平均F1分数。

    The paper describes a transformer-based system designed for SemEval-2023 Task 9: Multilingual Tweet Intimacy Analysis. The purpose of the task was to predict the intimacy of tweets in a range from 1 (not intimate at all) to 5 (very intimate). The official training set for the competition consisted of tweets in six languages (English, Spanish, Italian, Portuguese, French, and Chinese). The test set included the given six languages as well as external data with four languages not presented in the training set (Hindi, Arabic, Dutch, and Korean). We presented a solution based on an ensemble of XLM-T, a multilingual RoBERTa model adapted to the Twitter domain. To improve the performance of unseen languages, each tweet was supplemented by its English translation. We explored the effectiveness of translated data for the languages seen in fine-tuning compared to unseen languages and estimated strategies for using translated data in transformer-based models. Our solution ranked 4th on the leade
    
[^18]: 仅解码器或编码器-解码器？将语言模型解释为正则化的编码器-解码器

    Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder. (arXiv:2304.04052v1 [cs.CL])

    [http://arxiv.org/abs/2304.04052](http://arxiv.org/abs/2304.04052)

    该论文通过对正则化编码器-解码器结构进行比较，分析了仅解码器语言模型框架和编码器-解码器框架的效果。

    

    序列到序列（seq2seq）任务旨在基于给定的输入源序列生成目标序列。 传统上，大多数seq2seq任务都是通过编码器-解码器框架解决的，该框架需要编码器来编码源序列，并且需要解码器来生成目标文本。最近，出现了许多新方法，将仅解码器语言模型直接应用于seq2seq任务。尽管在将语言模型应用于seq2seq任务方面取得了重大进展，但仍然缺乏对仅解码器语言模型架构有效性的彻底分析。本文旨在通过对正则化编码器-解码器结构进行分析来解决这一差距。该结构旨在复制经典仅解码器语言模型中的所有行为，但具有编码器和解码器，从而更容易进行分析。

    The sequence-to-sequence (seq2seq) task aims at generating the target sequence based on the given input source sequence. Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture. This paper aims to address this gap by conducting a detailed comparison between the encoder-decoder architecture and the decoder-only language model framework through the analysis of a regularized encoder-decoder structure. This structure is designed to replicate all behaviors in the classical decoder-only language model but has an encoder and a decoder making it easier to b
    
[^19]: Bipol: 一种具有可解释性的新型自然语言处理多轴偏见评估度量

    Bipol: A Novel Multi-Axes Bias Evaluation Metric with Explainability for NLP. (arXiv:2304.04029v1 [cs.CL])

    [http://arxiv.org/abs/2304.04029](http://arxiv.org/abs/2304.04029)

    本文创造了一种新的度量标准 bipol 以检测文本数据中的社交偏见。该标准包括语料库级别评估和句子级别评估两个步骤，并使用 SotA 架构创建了新模型以检测多个轴的偏差。同时，作者还创造了一个大型数据集来训练偏见检测模型，并公开了相关代码。

    

    我们引入了一种新的、具有可解释性的度量标准 bipol，用于估算文本数据中的社交偏见。有害偏见在许多在线数据源中普遍存在，这些数据源用于训练机器学习（ML）模型。我们创造了一种新的度量标准，包括两个步骤：基于模型分类的语料库级别评估和基于（敏感）词频（TF）的句子级别评估。我们使用SotA架构创建了新模型，以检测沿多个轴的偏差，并评估了两个流行的NLP数据集（COPA和SQUAD）。作为附加贡献，我们创建了一个大型数据集（几乎有200万个带标签的样本），用于训练偏见检测模型，并公开了它。我们还公开了代码。

    We introduce bipol, a new metric with explainability, for estimating social bias in text data. Harmful bias is prevalent in many online sources of data that are used for training machine learning (ML) models. In a step to address this challenge we create a novel metric that involves a two-step process: corpus-level evaluation based on model classification and sentence-level evaluation based on (sensitive) term frequency (TF). After creating new models to detect bias along multiple axes using SotA architectures, we evaluate two popular NLP datasets (COPA and SQUAD). As additional contribution, we created a large dataset (with almost 2 million labelled samples) for training models in bias detection and make it publicly available. We also make public our codes.
    
[^20]: WikiGoldSK:斯洛伐克命名实体识别的带注释数据集，基准和少样本学习实验。

    WikiGoldSK: Annotated Dataset, Baselines and Few-Shot Learning Experiments for Slovak Named Entity Recognition. (arXiv:2304.04026v1 [cs.CL])

    [http://arxiv.org/abs/2304.04026](http://arxiv.org/abs/2304.04026)

    本研究介绍了第一个具有可观规模的人工标记的斯洛伐克NER数据集WikiGoldSK，通过评估当前最先进的多语言预训练语言模型，与现有的银标准数据集进行比较，并进行了少样本实验。银标准数据集上的训练能够产生更好的结果。

    

    命名实体识别（NER）是一种基础的NLP任务，具有广泛的实际应用。目前最先进的NER方法的性能取决于高质量手动注释数据集，但对于一些语言仍不存在这样的数据集。在本文中，我们旨在通过引入WikiGoldSK来解决斯洛伐克语中这种情况，这是第一个具有可观规模的人工标记的斯洛伐克NER数据集。我们通过评估最先进的多语言预训练语言模型并将其与现有的银标准斯洛伐克语NER数据集进行比较来对其进行基准测试。我们还进行了少样本实验，并表明在银标准数据集上的训练能够产生更好的结果。为了支持未来基于斯洛伐克NER的研究，我们在https://github.com/NaiveNeuron/WikiGoldSK公开发布了数据集、代码和训练模型，采用可允许的许可条款。

    Named Entity Recognition (NER) is a fundamental NLP tasks with a wide range of practical applications. The performance of state-of-the-art NER methods depends on high quality manually anotated datasets which still do not exist for some languages. In this work we aim to remedy this situation in Slovak by introducing WikiGoldSK, the first sizable human labelled Slovak NER dataset. We benchmark it by evaluating state-of-the-art multilingual Pretrained Language Models and comparing it to the existing silver-standard Slovak NER dataset. We also conduct few-shot experiments and show that training on a sliver-standard dataset yields better results. To enable future work that can be based on Slovak NER, we release the dataset, code, as well as the trained models publicly under permissible licensing terms at https://github.com/NaiveNeuron/WikiGoldSK.
    
[^21]: MphayaNER：适用于茨汉文达语的命名实体识别

    MphayaNER: Named Entity Recognition for Tshivenda. (arXiv:2304.03952v1 [cs.CL])

    [http://arxiv.org/abs/2304.03952](http://arxiv.org/abs/2304.03952)

    MphayaNER是第一个适用于茨汉文达语的NER语料库，研究通过在语料库上微调最先进的模型，探索了茨文达语与其他相关班图语之间的零样本转移。用chiShona数据扩充MphayaNER可以显著提高模型性能。

    

    命名实体识别（NER）在各种自然语言处理任务中发挥着至关重要的作用，如信息检索、文本分类和问答。然而，对于数据集和工具有限的低资源语言而言，NER可能是具有挑战性的。本文通过引入MphayaNER来解决这些挑战，这是新闻领域中第一个适用于茨汉文达语的NER语料库。我们通过在MphayaNER上\微调\最先进的模型来建立NER基线。该研究还探讨了茨文达语与其他相关班图语之间的零样本转移，其中chiShona和Kiswahili表现最佳。发现用chiShona数据扩充MphayaNER也可以显著提高模型性能。MphayaNER和基线模型都已公开发布。

    Named Entity Recognition (NER) plays a vital role in various Natural Language Processing tasks such as information retrieval, text classification, and question answering. However, NER can be challenging, especially in low-resource languages with limited annotated datasets and tools. This paper adds to the effort of addressing these challenges by introducing MphayaNER, the first Tshivenda NER corpus in the news domain. We establish NER baselines by \textit{fine-tuning} state-of-the-art models on MphayaNER. The study also explores zero-shot transfer between Tshivenda and other related Bantu languages, with chiShona and Kiswahili showing the best results. Augmenting MphayaNER with chiShona data was also found to improve model performance significantly. Both MphayaNER and the baseline models are made publicly available.
    
[^22]: 学生和大型语言模型所创建的代码解释的比较研究

    Comparing Code Explanations Created by Students and Large Language Models. (arXiv:2304.03938v1 [cs.CY])

    [http://arxiv.org/abs/2304.03938](http://arxiv.org/abs/2304.03938)

    该论文探讨了采用大型语言模型生成代码解释的潜力，以帮助学生提高理解和解释代码的能力。

    

    推理代码并解释其用途是计算机科学家的基本技能。在计算机教育领域，已经进行了广泛的研究，探讨了学生解释代码能力与编写和追踪代码等其他技能之间的关系。特别是，以高抽象级别描述代码在所有可能输入下的行为的能力强烈关联着代码编写技能。然而，对于许多学生来说，开发理解和准确简洁地解释代码的专业知识是一个挑战。现有的教学方法并未实现生产即时范例代码解释以进行指导的大规模课堂的步骤。强大的大型语言模型 (LLMs) 的出现近期可能提供了一种解决方案。在本文中，我们探讨了 LLMs 生成可以作为示例来支持学生理解和解释代码的解释的潜力。

    Reasoning about code and explaining its purpose are fundamental skills for computer scientists. There has been extensive research in the field of computing education on the relationship between a student's ability to explain code and other skills such as writing and tracing code. In particular, the ability to describe at a high-level of abstraction how code will behave over all possible inputs correlates strongly with code writing skills. However, developing the expertise to comprehend and explain code accurately and succinctly is a challenge for many students. Existing pedagogical approaches that scaffold the ability to explain code, such as producing exemplar code explanations on demand, do not currently scale well to large classrooms. The recent emergence of powerful large language models (LLMs) may offer a solution. In this paper, we explore the potential of LLMs in generating explanations that can serve as examples to scaffold students' ability to understand and explain code. To e
    
[^23]: 语音情感识别的实证研究与改进

    An Empirical Study and Improvement for Speech Emotion Recognition. (arXiv:2304.03899v1 [cs.CL])

    [http://arxiv.org/abs/2304.03899](http://arxiv.org/abs/2304.03899)

    本研究提出一种改进的多模式情感识别模型，利用透视损失来融合音频和文本模态信息，取得了IEMOCAP数据集上的最新最优结果。

    

    多模式语音情感识别旨在从音频和文本中检测说话者的情感。先前的研究主要集中于利用先进网络对不同模态信息进行建模和融合，以提高性能，但忽略了不同融合策略对情感识别的影响。在本研究中，我们考虑了一个简单但重要的问题：如何融合音频和文本模态信息对于多模式任务更有帮助。进一步地，我们提出了一种由透视损失改进的多模式情感识别模型。实证结果表明我们的方法在IEMOCAP数据集上取得了新的最优结果。深入分析解释了为什么改进的模型可以实现改进并超过基线。

    Multimodal speech emotion recognition aims to detect speakers' emotions from audio and text. Prior works mainly focus on exploiting advanced networks to model and fuse different modality information to facilitate performance, while neglecting the effect of different fusion strategies on emotion recognition. In this work, we consider a simple yet important problem: how to fuse audio and text modality information is more helpful for this multimodal task. Further, we propose a multimodal emotion recognition model improved by perspective loss. Empirical results show our method obtained new state-of-the-art results on the IEMOCAP dataset. The in-depth analysis explains why the improved model can achieve improvements and outperforms baselines.
    
[^24]: 通过对比学习加强知识的短文本匹配模型

    The Short Text Matching Model Enhanced with Knowledge via Contrastive Learning. (arXiv:2304.03898v1 [cs.CL])

    [http://arxiv.org/abs/2304.03898](http://arxiv.org/abs/2304.03898)

    提出了一种短文本匹配模型，使用生成模型生成补充句子，结合对比学习和外部知识进行语义匹配，并使用关键词避免噪声问题。

    

    近年来，短文本匹配任务在广告搜索和推荐领域得到了广泛应用。由于文本长度短，语义信息匮乏和单词歧义问题成为此类任务的难点。先前的研究已经引入文本补充句子或知识库来提供附加的特征信息。然而，这些方法没有充分地交互原始句子和补充句子，也没有考虑到外部知识库引入的噪声问题。因此，本文提出了一种结合对比学习和外部知识的短文本匹配模型。该模型利用生成模型生成对应的补充句子，并使用对比学习方法指导模型获得更具语义匹配性的原始句子编码。此外，为了避免噪声，我们使用关键词作为原始句子的主要语义进行检索。

    In recent years, short Text Matching tasks have been widely applied in the fields ofadvertising search and recommendation. The difficulty lies in the lack of semantic information and word ambiguity caused by the short length of the text. Previous works have introduced complement sentences or knowledge bases to provide additional feature information. However, these methods have not fully interacted between the original sentence and the complement sentence, and have not considered the noise issue that may arise from the introduction of external knowledge bases. Therefore, this paper proposes a short Text Matching model that combines contrastive learning and external knowledge. The model uses a generative model to generate corresponding complement sentences and uses the contrastive learning method to guide the model to obtain more semantically meaningful encoding of the original sentence. In addition, to avoid noise, we use keywords as the main semantics of the original sentence to retrie
    
[^25]: 多模态假新闻和讽刺新闻数据集Factify 2

    Factify 2: A Multimodal Fake News and Satire News Dataset. (arXiv:2304.03897v1 [cs.CL])

    [http://arxiv.org/abs/2304.03897](http://arxiv.org/abs/2304.03897)

    本文提供了改进的多模态事实核查数据集Factify 2，其支持视觉和文本数据的蕴含关系。该数据集以支持、无证据和驳斥三个类别为主，包含50,000个新的数据实例，并提供一种基于BERT和Vision Transformer的最新事实核查模型，优于现有最先进的方法。

    

    互联网为全球提供了一个开放的平台，让人们表达自己的观点并分享自己的故事。虽然这非常有价值，但它也使得虚假新闻成为我们社会最紧迫的问题之一。手动的事实核对过程非常耗时，这使得我们很难在误导性言论造成重大伤害之前驳斥它们。这就是自动事实或声明验证受到关注的原因。一些现有数据集旨在支持自动化事实核查技术的发展，但大多数数据集都是基于文本的。多模态事实验证一直受到相对较少的关注。在本文中，我们提供了一个多模态事实核查数据集FACTIFY 2，通过使用新的数据来源和添加讽刺文章来改进Factify 1。Factify 2有50,000个新的数据实例。与FACTIFY 1.0类似，我们有三个广泛的类别——支持、无证据和驳斥，这些类别基于视觉和文本数据的蕴含关系具有子类别。我们还提供了一个基于BERT和Vision Transformer的FACTIFY 2事实核查模型，并表明其在Factify 1数据集上的表现优于现有最先进的方法。

    The internet gives the world an open platform to express their views and share their stories. While this is very valuable, it makes fake news one of our society's most pressing problems. Manual fact checking process is time consuming, which makes it challenging to disprove misleading assertions before they cause significant harm. This is he driving interest in automatic fact or claim verification. Some of the existing datasets aim to support development of automating fact-checking techniques, however, most of them are text based. Multi-modal fact verification has received relatively scant attention. In this paper, we provide a multi-modal fact-checking dataset called FACTIFY 2, improving Factify 1 by using new data sources and adding satire articles. Factify 2 has 50,000 new data instances. Similar to FACTIFY 1.0, we have three broad categories - support, no-evidence, and refute, with sub-categories based on the entailment of visual and textual data. We also provide a BERT and Vison Tr
    
[^26]: 为什么要逐步思考？推理源于经验的局部性。

    Why think step-by-step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v1 [cs.AI])

    [http://arxiv.org/abs/2304.03843](http://arxiv.org/abs/2304.03843)

    本文通过语言模型研究何时以及为什么推理是有帮助的，测试推理在训练数据由相互影响强烈的局部变量集群组成时是否有效。通过一步步的推理，能够将准确的局部推理链接在一起，以估算在训练中没有同时观察到的变量之间的关系。

    

    人类有着强大而神秘的推理能力。通过一系列纯粹的思维步骤，我们可以推理出我们无法直接得出的推论 - 尽管我们从世界上没有得到任何额外数据。同样地，大型语言模型可以通过一步步的推理，在回答问题之前生成中间步骤，从而更好地完成复杂的任务。我们使用语言模型研究何时以及为什么推理是有帮助的，测试推理在训练数据由相互影响强烈的局部变量集群组成时是否有效。这些训练条件能够将准确的局部推理链接在一起，以估算在训练中没有同时观察到的变量之间的关系。我们使用贝叶斯网络定义的联合分布的样品对自回归变压器进行训练，但每个样品只包括其中的一部分变量。我们比较使用推理生成的变量子集与使用完整集合进行训练的方案的性能。

    Humans have a powerful and mysterious capacity to reason. By working through a series of purely mental steps, we can make inferences we would not be capable of making directly -- despite that fact that we get no additional data from the world. Similarly, large language models can perform better at complex tasks through chain-of-thought reasoning, where they generate intermediate steps before answering a question. We use language models to investigate the questions of when and why reasoning is helpful, testing the hypothesis that reasoning is effective when training data consisting of local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences in order to estimate relationships between variables that were not seen together in training. We train an autoregressive transformer on samples from joint distributions defined by Bayes nets, but only include a subset of all the variables in each sample. We compare lang
    
[^27]: 跨国之间的桥梁: 量化多语者在社交媒体上的交流作用

    Bridging Nations: Quantifying the Role of Multilinguals in Communication on Social Media. (arXiv:2304.03797v1 [cs.SI])

    [http://arxiv.org/abs/2304.03797](http://arxiv.org/abs/2304.03797)

    研究欧洲 Twitter 网络，通过因果推断技术，量化多语用户在跨语言信息交流中的结构作用和沟通影响。多语者的介数中心度比单语用户高13％，且具有多语网络邻居的单语用户分享其他语言域名和hashtag的可能性分别增加了16倍和4倍。多语者对于传播那些单语同胞不易获得的信息具有更大的影响力。

    

    社交媒体能快速传播各种信息，从表情包到社会运动。然而，我们对信息如何跨越语言边界知之甚少。我们在欧洲 Twitter 网络上应用因果推断技术，量化多语用户在跨语言信息交流中的结构作用和沟通影响。总的来说，多语者发挥了重要作用；发布多种语言的推文将介数中心度提高了13％，并且如果其网络邻居是多语者，则单语用户分享另一种语言域名和hashtag的可能性分别增加了16倍和4倍。我们进一步展示，多语者对于传播那些单语同胞不易获得的信息具有更大的影响力，例如来自遥远国家的信息、关于区域政治、新兴社会运动和工作机会的内容。通过突出跨境信息交流，这项工作揭示了一个至关重要的组成部分。

    Social media enables the rapid spread of many kinds of information, from memes to social movements. However, little is known about how information crosses linguistic boundaries. We apply causal inference techniques on the European Twitter network to quantify multilingual users' structural role and communication influence in cross-lingual information exchange. Overall, multilinguals play an essential role; posting in multiple languages increases betweenness centrality by 13%, and having a multilingual network neighbor increases monolinguals' odds of sharing domains and hashtags from another language 16-fold and 4-fold, respectively. We further show that multilinguals have a greater impact on diffusing information less accessible to their monolingual compatriots, such as information from far-away countries and content about regional politics, nascent social movements, and job opportunities. By highlighting information exchange across borders, this work sheds light on a crucial component 
    
[^28]: 文献综述的分层目录生成：一个基准测试

    Hierarchical Catalogue Generation for Literature Review: A Benchmark. (arXiv:2304.03512v1 [cs.CL])

    [http://arxiv.org/abs/2304.03512](http://arxiv.org/abs/2304.03512)

    研究提出了一个名为HiCatGLR任务，致力于为文献综述生成分层目录，它可以从多篇论文中提取和组织重要信息。为了解决现有研究中缺少清晰逻辑层次结构概述的问题，提供了一个具有挑战性的解决方案并创建了新的数据集。通过此项研究，可以更加准确地评估模型性能并验证数据集的高质量和评估指标的有效性。

    

    多文档科学摘要可以从大量的论文中提取和组织重要信息，最近引起了广泛关注。然而，现有的研究主要集中在产生缺乏清晰和逻辑层次结构的冗长概述上。为了缓解这个问题，我们提出了一个名为“Hierarchical Catalogue Generation for Literature Review (HiCatGLR)”的原子和具有挑战性的任务，其目标是根据各种参考文献为综述论文生成分层目录。我们精心构建了一个新的英文文献综述分层目录数据集(HiCaD)，其中包含13.8k篇文献综述目录和120k篇参考论文，并通过端到端和流水线方法进行了各种实验的基准测试。为了准确评估模型性能，我们设计了从语义和结构上与参考标准相似度的评估指标。此外，我们的广泛分析验证了我们数据集的高质量和我们评估指标的有效性。

    Multi-document scientific summarization can extract and organize important information from an abundant collection of papers, arousing widespread attention recently. However, existing efforts focus on producing lengthy overviews lacking a clear and logical hierarchy. To alleviate this problem, we present an atomic and challenging task named Hierarchical Catalogue Generation for Literature Review (HiCatGLR), which aims to generate a hierarchical catalogue for a review paper given various references. We carefully construct a novel English Hierarchical Catalogues of Literature Reviews Dataset (HiCaD) with 13.8k literature review catalogues and 120k reference papers, where we benchmark diverse experiments via the end-to-end and pipeline methods. To accurately assess the model performance, we design evaluation metrics for similarity to ground truth from semantics and structure. Besides, our extensive analyses verify the high quality of our dataset and the effectiveness of our evaluation met
    
[^29]: CAPOT: 使用后训练对比对齐创建强健的密集查询编码器

    CAPOT: Creating Robust Dense Query Encoders using Post Training Contrastive Alignment. (arXiv:2304.03401v1 [cs.IR])

    [http://arxiv.org/abs/2304.03401](http://arxiv.org/abs/2304.03401)

    CAPOT使用后训练对比对齐的方法，提高模型对于噪声查询的健壮性，表现类似于数据增强但没有其开销。

    

    上下文词表示的成功和神经信息检索的进步使得基于密集向量的检索成为段落和文档排名的标准方法。双编码器虽然有效和高效，但对查询分布和嘈杂查询变化很脆弱。数据增强可以使模型更加健壮，但会引入训练集生成的开销，并需要重新训练和索引重建。我们提出了 Contrastive Alignment POst Training (CAPOT)，一种高效的微调方法，通过冻结文档编码器，让查询编码器学习将嘈杂查询与其未更改的根对齐，以提高模型的健壮性。我们评估了 CAPOT 在 MSMARCO、自然问题和 Trivia QA 段落检索的嘈杂变体上，发现 CAPOT 具有与数据增强类似的影响，但没有它的开销。

    The success of contextual word representations and advances in neural information retrieval have made dense vector-based retrieval a standard approach for passage and document ranking. While effective and efficient, dual-encoders are brittle to variations in query distributions and noisy queries. Data augmentation can make models more robust but introduces overhead to training set generation and requires retraining and index regeneration. We present Contrastive Alignment POst Training (CAPOT), a highly efficient finetuning method that improves model robustness without requiring index regeneration, the training set optimization, or alteration. CAPOT enables robust retrieval by freezing the document encoder while the query encoder learns to align noisy queries with their unaltered root. We evaluate CAPOT noisy variants of MSMARCO, Natural Questions, and Trivia QA passage retrieval, finding CAPOT has a similar impact as data augmentation with none of its overhead.
    
[^30]: 大型语言模型作为钥匙：用GPT解密材料科学的秘密。

    Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v1 [cs.CL])

    [http://arxiv.org/abs/2304.02213](http://arxiv.org/abs/2304.02213)

    本文介绍了一个新的自然语言处理任务——结构化信息推理（SIS），利用GPT-3模型能够准确提取材料科学设备层面的信息，并通过实验预测PCE和反向预测参数，展示了大型语言模型在材料学中的巨大潜力。

    

    本文介绍了一个新的自然语言处理（NLP）任务——结构化信息推理（SIS），以解决材料科学设备层面信息提取的复杂性。我们使用现有的钙钛矿太阳能电池FAIR数据集对GPT-3进行微调，获得了91.8 F1得分，并更新了数据集，包括迄今为止所有相关科学论文。所生成的数据集已被格式化和标准化，使得它可以直接作为后续数据分析的输入。这个特性将使材料科学家通过选择高质量的领域评论文章来开发其自己的模型。此外，我们设计了实验来预测PCE和反向预测参数，并获得了与DFT相当的性能，这证明了大型语言模型能够像材料学家一样评判材料和设计新材料。

    This article presents a new NLP task called structured information inference (SIS) to address the complexities of information extraction at the device level in materials science. We accomplished this task by finetuning GPT-3 on a exsiting perovskite solar cell FAIR dataset with 91.8 F1-score and we updated the dataset with all related scientific papers up to now. The produced dataset is formatted and normalized, enabling its direct utilization as input in subsequent data analysis. This feature will enable materials scientists to develop their own models by selecting high-quality review papers within their domain. Furthermore, we designed experiments to predict PCE and reverse-predict parameters and obtained comparable performance with DFT, which demonstrates the potential of large language models to judge materials and design new materials like a materials scientist.
    
[^31]: ChatGPT/GPT-4研究综述及对大语言模型未来的展望

    Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models. (arXiv:2304.01852v1 [cs.CL])

    [http://arxiv.org/abs/2304.01852](http://arxiv.org/abs/2304.01852)

    本文全面介绍了最先进的大型语言模型ChatGPT和GPT-4，包括其在各个领域的前景应用，并着重介绍了大规模预训练、指令微调和人类反馈的强化学习创新。ChatGPT/GPT-4在自然语言处理应用方面表现突出，同时在其他领域也具有潜力。

    

    本文全面介绍了来自GPT系列的最先进的大型语言模型（LLM）ChatGPT和GPT-4及其在各个领域的前景应用。实际上，大规模预训练、指令微调和人类反馈的强化学习是提高LLMs的适应性和性能的重要创新。我们在arXiv上深入分析了194篇相关文献，包括趋势分析、词云表现和在各个应用领域的分布分析。研究发现ChatGPT/GPT-4研究显著增长，主要集中在直接的自然语言处理应用上，同时还展示了在从教育和历史到数学、医学和物理等领域具有相当的潜力。本研究旨在提供有关ChatGPT的能力的见解。

    This paper presents a comprehensive survey of ChatGPT and GPT-4, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT/GPT-4 research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabi
    
[^32]: 统一对比传递框架与传播结构用于提高低资源谣言检测

    A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection. (arXiv:2304.01492v1 [cs.CL])

    [http://arxiv.org/abs/2304.01492](http://arxiv.org/abs/2304.01492)

    该文介绍了一个利用对比传递框架和传播结构，将从充足资源的谣言数据学到的特征适应于低资源情况下的方式，可以检测到跨越语言和领域界限的谣言。

    

    大量的谣言伴随着突发新闻或热门话题而传播，这严重阻碍了真相的传播。现有的谣言检测算法展示了在前几天新闻上良好性能的前景，但是由于缺乏训练数据和先前的专业知识，它们很难发现与预期事件有关的谣言，特别是在不同语言（即低资源环境）中传播的谣言。在本文中，我们提出了一个统一的对比传递框架，通过将从充足资源的谣言数据学到的特征适应于低资源情况下的特征来检测谣言。具体来说，我们首先将在社交媒体上传播的谣言表示为无向拓扑结构，然后通过统一对比范式进行Multi-scale图卷积网络的训练。我们的模型明确地突破了领域和/或语言问题的障碍，通过语言对齐和一种新颖的领域自适应对比。

    The truth is significantly hampered by massive rumors that spread along with breaking news or popular topics. Since there is sufficient corpus gathered from the same domain for model training, existing rumor detection algorithms show promising performance on yesterday's news. However, due to a lack of training data and prior expert knowledge, they are poor at spotting rumors concerning unforeseen events, especially those propagated in different languages (i.e., low-resource regimes). In this paper, we propose a unified contrastive transfer framework to detect rumors by adapting the features learned from well-resourced rumor data to that of the low-resourced. More specifically, we first represent rumor circulated on social media as an undirected topology, and then train a Multi-scale Graph Convolutional Network via a unified contrastive paradigm. Our model explicitly breaks the barriers of the domain and/or language issues, via language alignment and a novel domain-adaptive contrastive 
    
[^33]: 基于联邦学习的干净和攻击情景下的多语言表情符号预测

    Federated Learning Based Multilingual Emoji Prediction In Clean and Attack Scenarios. (arXiv:2304.01005v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.01005](http://arxiv.org/abs/2304.01005)

    本文提出了一种基于联邦学习的多语言表情符号预测方法，在干净或攻击情境下均有效，同时保护了用户数据隐私。

    

    联邦学习是机器学习社区中一个日益增长的领域，由于其分散和私密的设计而得到发展。联邦学习中的模型训练分布在多个客户端上，从而提供了大量客户端数据的访问，同时保护了每个客户端数据的隐私性。然后，服务器聚合了在这些多个客户端上进行的训练，而不访问它们的数据，这些数据可能是在任何社交媒体服务和即时通讯平台中广泛使用的表情符号，以表达用户的情感。本文提出了一种基于联邦学习的干净和攻击情境下的多语言表情符号预测。表情符号预测数据从Twitter和SemEval表情符号数据集中获取。使用这些数据来训练和评估不同大小的转换器模型，包括在所有客户端中假定数据干净或在一些客户端中进行标签翻转攻击的稀疏激活转换器。对这些模型的实验结果表明，在干净或攻击情境下，联邦学习可以有效地预测多语言表情符号，同时保持数据隐私。

    Federated learning is a growing field in the machine learning community due to its decentralized and private design. Model training in federated learning is distributed over multiple clients giving access to lots of client data while maintaining privacy. Then, a server aggregates the training done on these multiple clients without access to their data, which could be emojis widely used in any social media service and instant messaging platforms to express users' sentiments. This paper proposes federated learning-based multilingual emoji prediction in both clean and attack scenarios. Emoji prediction data have been crawled from both Twitter and SemEval emoji datasets. This data is used to train and evaluate different transformer model sizes including a sparsely activated transformer with either the assumption of clean data in all clients or poisoned data via label flipping attack in some clients. Experimental results on these models show that federated learning in either clean or attack
    
[^34]: 探索大型语言模型在无参考文本质量评估中的应用：初步实证研究

    Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study. (arXiv:2304.00723v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.00723](http://arxiv.org/abs/2304.00723)

    本文介绍了大型语言模型在无参考文本质量评估中的应用研究。研究结果表明，利用ChatGPT生成的显式得分是最有效和可靠的方法。

    

    在自然语言处理中，评估生成文本的质量是一个具有挑战性的任务，由于文本的固有复杂性和多样性而产生困难。最近，OpenAI的ChatGPT，一种强大的大型语言模型（LLM），由于其在各种任务中的出色表现而引起了广泛关注。因此，我们发布此报告，以调查LLMs，特别是ChatGPT的有效性，并探索优化它们在评估文本质量方面的应用方式。我们比较了基于ChatGPT或类似LLMs的三种无参考评估方法。实验结果证明，ChatGPT能够有效地从各个角度评估文本质量而不需要参考，并展示了比大多数现有自动指标更好的性能。特别是，显式得分是利用ChatGPT生成衡量文本质量的数字分数的最有效和可靠的方法。然而，直接将LLMs应用于文本质量评估仍然面临挑战和限制，需要进一步探索和改进。

    Evaluating the quality of generated text is a challenging task in natural language processing. This difficulty arises from the inherent complexity and diversity of text. Recently, OpenAI's ChatGPT, a powerful large language model (LLM), has garnered significant attention due to its impressive performance in various tasks. Therefore, we present this report to investigate the effectiveness of LLMs, especially ChatGPT, and explore ways to optimize their use in assessing text quality. We compared three kinds of reference-free evaluation methods based on ChatGPT or similar LLMs. The experimental results prove that ChatGPT is capable to evaluate text quality effectively from various perspectives without reference and demonstrates superior performance than most existing automatic metrics. In particular, the Explicit Score, which utilizes ChatGPT to generate a numeric score measuring text quality, is the most effective and reliable method among the three exploited approaches. However, directly
    
[^35]: 关于大型语言模型的创造性研究

    On the Creativity of Large Language Models. (arXiv:2304.00008v1 [cs.AI])

    [http://arxiv.org/abs/2304.00008](http://arxiv.org/abs/2304.00008)

    这篇论文探讨了大型语言模型的创造性问题，分析了与之相关的机器创造性的难点和易点，并重点分析了这些技术在创意产业中的社会影响。

    

    大型语言模型(LLMs)正在颠覆人工智能的多个领域。其中最显著的应用之一是创作，例如诗歌或故事：生成的输出通常具有惊人的质量。但是，一个自然的问题是：LLMs真的可以被认为是创造性的吗？在本文中，我们首先通过创造性理论的角度分析了LLMs的发展，探讨了关键的未解决问题和挑战。然后，我们在与LLMs相关的机器创造性方面确定了一组“易”和“难”问题，并对其进行了讨论。最后，我们分析了这些技术在创意产业中的社会影响。

    Large Language Models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arise: can LLMs really be considered creative? In this article we firstly analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. Then, we identify a set of "easy" and "hard" problems in machine creativity, discussing them in relation to LLMs. Finally, we analyze the societal impact of these technologies with a particular focus on the creative industries.
    
[^36]: 大型语言模型综述

    A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])

    [http://arxiv.org/abs/2303.18223](http://arxiv.org/abs/2303.18223)

    本文综述了大型语言模型的研究历程以及最近的预训练语言模型(PLMs)，并强调模型扩展将带来性能改进和特殊能力的发掘。

    

    语言本质上是一个由语法规则控制的复杂精细的人类表达系统，对于开发理解和掌握语言的能力的AI算法来说是一项重大挑战。作为主要方法之一，语言建模在过去二十年里广泛研究用于语言理解和生成，从统计语言模型演化为神经语言模型。最近，通过在大规模语料库上预训练Transformer模型，提出了预训练语言模型（PLMs），在解决各种NLP任务方面显示出强大的能力。由于研究人员发现模型缩放可以导致性能改进，他们进一步通过增加模型规模来研究缩放效应，有趣的是，当参数规模超过一定水平时，这些扩大的语言模型不仅可以实现显着的性能提升，而且还显示出一些小规模语言模型所没有的特殊能力。

    Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
    
[^37]: 使用语言反馈规模化训练语言模型

    Training Language Models with Language Feedback at Scale. (arXiv:2303.16755v1 [cs.CL])

    [http://arxiv.org/abs/2303.16755](http://arxiv.org/abs/2303.16755)

    本文提出一种新方法，即利用更丰富的语言反馈进行模仿学习，通过三个迭代步骤对语言模型进行训练以生成更符合人类偏好的输出。

    

    预训练的语言模型经常生成不符合人类偏好的输出，例如有害的文本或事实不正确的摘要。最近的研究尝试通过学习一种简单的人类反馈形式（即模型生成输出之间的比较）来解决这些问题。但是，比较反馈只能传达有限的关于人类偏好的信息。在本文中，我们介绍了一种新的方法——使用语言反馈进行模仿学习（ILF），它利用了更丰富的语言反馈。ILF由三个迭代步骤组成：第一步，根据输入，初始LM输出和反馈对语言模型进行调节以生成改进。第二步，选择最多反馈的改进。第三步，微调语言模型，以最大化在给定输入的情况下选择的改进的可能性。我们在理论上证明了ILF可以被看作是贝叶斯推断，类似于从人类反馈中进行强化学习。我们还评估了ILF在各种基准测试中的性能。

    Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate
    
[^38]: 通过定向刺激引导大型语言模型

    Guiding Large Language Models via Directional Stimulus Prompting. (arXiv:2302.11520v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.11520](http://arxiv.org/abs/2302.11520)

    该文介绍了一个新的框架，用于通过生成定向刺激来指导大型语言模型在下游任务中生成所需的输出。通过策略语言模型的训练，该框架可以适应于各种语言模型和任务，并在摘要和对话生成任务中取得了最先进的表现。

    

    我们引入了一个新的框架，称为定向刺激引导，它使用可调节的语言模型来为下游任务的黑盒冻结大型语言模型提供指导。与以往手动或自动找到每个任务的最优提示的方法不同，我们训练一个策略语言模型来生成离散的token作为每个输入的定向刺激，这是一种提示或提示，例如文章的关键词用于摘要。然后将定向刺激与原始输入组合，并输入到LLM中，以指导其向所需目标生成。策略LM可以通过1）从注释数据中的监督学习和2）从离线和在线奖励中的强化学习进行训练，以探索更好地与人类偏好相匹配的定向刺激。该框架可灵活适用于各种LM和任务。为了验证其效果，我们将我们的框架应用于摘要和对话响应生成任务。实验结果表明，我们的方法在两个任务上均取得了最先进的性能。

    We introduce a new framework, Directional Stimulus Prompting, that uses a tuneable language model (LM) to provide guidance for the black-box frozen large language model (LLM) on downstream tasks. Unlike prior work that manually or automatically finds the optimal prompt for each task, we train a policy LM to generate discrete tokens as directional stimulus of each input, which is a hint/cue such as keywords of an article for summarization. The directional stimulus is then combined with the original input and fed into the LLM to guide its generation toward the desired target. The policy LM can be trained through 1) supervised learning from annotated data and 2) reinforcement learning from offline and online rewards to explore directional stimulus that better aligns LLMs with human preferences. This framework is flexibly applicable to various LMs and tasks. To verify its effectiveness, we apply our framework to summarization and dialogue response generation tasks. Experimental results dem
    
[^39]: RESDSQL：解耦文本到SQL中的架构连接和骨架解析

    RESDSQL: Decoupling Schema Linking and Skeleton Parsing for Text-to-SQL. (arXiv:2302.05965v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.05965](http://arxiv.org/abs/2302.05965)

    提出了一种排名增强的编码和骨架感知解码框架，以解耦文本到SQL中的架构连接和骨架解析，从而减轻对架构连接的工作量并隐式限制SQL解析。

    

    最近，预训练语言模型是文本到SQL中的最佳尝试之一。由于SQL查询的结构属性，seq2seq模型负责解析架构项（即表格和列）和骨架（即SQL关键字）。这些耦合的目标增加了正确解析SQL查询的困难，特别是当它们涉及许多架构项和逻辑操作符时。本文提出了一种排名增强的编码和骨架感知解码框架，以解耦架构连接和骨架解析。具体而言，对于seq2seq编码器-解码器模型，其编码器注入最相关的架构项，而不是整个无序的架构项，可以减轻SQL解析中的架构连接工作，而其解码器首先生成骨架，然后生成实际的SQL查询，可以隐式地限制SQL解析。我们在Spider及其三个鲁棒性变体：Spider-DK, Spider-EMNLP19和Spider-WTq等SQL数据集上评估了我们提出的框架。

    One of the recent best attempts at Text-to-SQL is the pre-trained language model. Due to the structural property of the SQL queries, the seq2seq model takes the responsibility of parsing both the schema items (i.e., tables and columns) and the skeleton (i.e., SQL keywords). Such coupled targets increase the difficulty of parsing the correct SQL queries especially when they involve many schema items and logic operators. This paper proposes a ranking-enhanced encoding and skeleton-aware decoding framework to decouple the schema linking and the skeleton parsing. Specifically, for a seq2seq encoder-decode model, its encoder is injected by the most relevant schema items instead of the whole unordered ones, which could alleviate the schema linking effort during SQL parsing, and its decoder first generates the skeleton and then the actual SQL query, which could implicitly constrain the SQL parsing. We evaluate our proposed framework on Spider and its three robustness variants: Spider-DK, Spid
    
[^40]: 双排列等变性在知识图谱补全中的应用

    Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01313](http://arxiv.org/abs/2302.01313)

    本研究提出了双排列等变性的KG表示方法，可以使神经网络在KG中执行复杂的逻辑推理任务，并在多个归纳KG完成任务中实现了最先进的Hits@10测试准确率。双排列等变性在KG中开辟了新的研究方向。

    

    本研究将知识图谱(KGs)形式化为一种新型的图，并称之为双交换属性图，其中节点和二元（两个节点之间的）表示必须对节点号和边（及节点）属性（关系和节点特征）的排列等变。双重排列等变的KG表示在KG中开辟了新的研究方向。我们展示了这种等变性对关系的结构表示产生的影响，从而使神经网络能够在KG中执行复杂的逻辑推理任务。最后，我们介绍了一种通用的等变表示蓝图，并测试了一种简单的基于GNN的双排列等变神经结构，在WN18RR、FB237和NELL995归纳KG完成任务中实现了最先进的Hits@10测试准确率，并能够准确执行现有方法无法执行的逻辑推理任务。

    This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (& node) attributes (relations & node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
    
[^41]: ChatGPT发布后，立场检测技术会如何发展？

    How would Stance Detection Techniques Evolve after the Launch of ChatGPT?. (arXiv:2212.14548v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.14548](http://arxiv.org/abs/2212.14548)

    ChatGPT是一种新的预训练语言模型，可以用于解决立场检测问题，并提供了其预测的解释能力。

    

    立场检测是指从给定文本中提取对目标的立场（支持、反对或中立）的任务。随着社交媒体内容的大量增加，这方面的研究越来越受到关注。传统的处理立场检测的框架是将其转化为文本分类任务。深度学习模型已经取代了基于规则的模型和传统的机器学习模型来解决此类问题。目前的深度神经网络面临两个主要挑战，即标记数据和社交媒体帖子中的信息不足，以及深度学习模型的不可解释性。ChatGPT是一种新的预训练语言模型，于2022年11月30日发布。针对立场检测任务，我们的实验表明，ChatGPT可以在常用数据集（包括SemEval-2016和P-Stance）上实现SOTA或类似的性能。同时，ChatGPT可以为其自身的预测提供解释，这超出了任何现有模型的能力。

    Stance detection refers to the task of extracting the standpoint (Favor, Against or Neither) towards a target in given texts. Such research gains increasing attention with the proliferation of social media contents. The conventional framework of handling stance detection is converting it into text classification tasks. Deep learning models have already replaced rule-based models and traditional machine learning models in solving such problems. Current deep neural networks are facing two main challenges which are insufficient labeled data and information in social media posts and the unexplainable nature of deep learning models. A new pre-trained language model chatGPT was launched on Nov 30, 2022. For the stance detection tasks, our experiments show that ChatGPT can achieve SOTA or similar performance for commonly used datasets including SemEval-2016 and P-Stance. At the same time, ChatGPT can provide explanation for its own prediction, which is beyond the capability of any existing mo
    
[^42]: MiLMo：少数民族多语言预训练语言模型

    MiLMo:Minority Multilingual Pre-trained Language Model. (arXiv:2212.01779v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.01779](http://arxiv.org/abs/2212.01779)

    本论文构建了一个名为MiLMo的多语言预训练模型，该模型在少数民族语言任务上表现较好，并构建了一个少数民族多语言文本分类数据集。

    

    预训练语言模型在大规模无监督数据上进行训练，可以在小规模带标签的数据集上微调模型，并取得良好的效果。多语言预训练语言模型可以在多种语言上进行训练，模型同时能够理解多种语言。目前，预训练模型的研究主要集中在资源丰富的语言上，相对于常见语言而言，对于少数民族等资源稀缺语言的研究却相对较少，公共多语言预训练语言模型在少数民族语言上表现不佳。因此，本文构建了一个名为MiLMo的多语言预训练模型，该模型在包括蒙古语、藏语、维吾尔语、哈萨克语和朝鲜语在内的少数民族语言任务上表现较好。为验证MiLMo模型的有效性并解决少数民族语言数据集稀缺的问题，本文构建了一个名为MiTC的少数民族多语言文本分类数据集，并训练了一个Word2vec模型。

    Pre-trained language models are trained on large-scale unsupervised data, and they can fine-turn the model only on small-scale labeled datasets, and achieve good results. Multilingual pre-trained language models can be trained on multiple languages, and the model can understand multiple languages at the same time. At present, the search on pre-trained models mainly focuses on rich resources, while there is relatively little research on low-resource languages such as minority languages, and the public multilingual pre-trained language model can not work well for minority languages. Therefore, this paper constructs a multilingual pre-trained model named MiLMo that performs better on minority language tasks, including Mongolian, Tibetan, Uyghur, Kazakh and Korean. To solve the problem of scarcity of datasets on minority languages and verify the effectiveness of the MiLMo model, this paper constructs a minority multilingual text classification dataset named MiTC, and trains a word2vec mode
    
[^43]: 不断增长的知识图谱的终身嵌入学习与转移

    Lifelong Embedding Learning and Transfer for Growing Knowledge Graphs. (arXiv:2211.15845v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.15845](http://arxiv.org/abs/2211.15845)

    现有的知识图谱嵌入模型主要集中在静态KG上，无法随着KG不断增长而及时获取新知识，本文引入了终身KG嵌入模型，实现对不断增长的知识图谱的终身嵌入学习与转移，通过嵌入转移策略和正则化方法避免灾难性遗忘。

    

    现有的知识图谱（KG）嵌入模型主要集中在静态KG上。然而，现实世界中的KG并不保持静态，而是随着KG应用的发展而发展和增长。因此，新事实和以前未见的实体和关系不断出现，需要一种嵌入模型可以通过增长快速学习和转移新知识。本文探讨了KG嵌入的一个扩展领域，即终身KG嵌入。我们考虑在不必从头开始学习嵌入的情况下，保持对KG增长快照的知识转移和保留学习。所提出的模型包括用于嵌入学习和更新的掩码KG自编码器，具有嵌入转移策略，将学习的知识注入新实体和关系嵌入，以及嵌入正则化方法，以避免灾难性遗忘。为了研究KG增长的不同方面的影响，我们构建了四个...

    Existing knowledge graph (KG) embedding models have primarily focused on static KGs. However, real-world KGs do not remain static, but rather evolve and grow in tandem with the development of KG applications. Consequently, new facts and previously unseen entities and relations continually emerge, necessitating an embedding model that can quickly learn and transfer new knowledge through growth. Motivated by this, we delve into an expanding field of KG embedding in this paper, i.e., lifelong KG embedding. We consider knowledge transfer and retention of the learning on growing snapshots of a KG without having to learn embeddings from scratch. The proposed model includes a masked KG autoencoder for embedding learning and update, with an embedding transfer strategy to inject the learned knowledge into the new entity and relation embeddings, and an embedding regularization method to avoid catastrophic forgetting. To investigate the impacts of different aspects of KG growth, we construct four
    
[^44]: STAGE: 基于Span标记和贪心推理方案的Aspect Sentiment Triplet提取

    STAGE: Span Tagging and Greedy Inference Scheme for Aspect Sentiment Triplet Extraction. (arXiv:2211.15003v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.15003](http://arxiv.org/abs/2211.15003)

    STAGE是一种新方法用于处理Aspect Sentiment Triplet Extraction（ASTE）任务，可以处理单词具有多个角色或方面/意见术语由多个单词组成等复杂情况，通过Span标记和贪心推理方案来提取情感三元组，更准确地识别给定句子中方面术语、相应的意见术语和相关的情感极性。

    

    近年来，Aspect Sentiment Triplet Extraction（ASTE）已成为情感分析研究中的新兴任务，旨在从给定的句子中提取方面术语、相应的意见术语和相应的情感极性三元组。然而，目前提出的大多数基于神经网络的模型都有其局限性：过于依赖于1）假设每个单词仅与单个角色（例如，方面术语或意见术语等）相关联和2）词级交互并将每个意见/方面视为一组独立的单词。因此，在复杂的ASTE任务中效果不佳，例如一个单词与多个角色相关联或者一个方面/意见术语由多个单词组成。因此，我们提出了一种新方法，Span标记和贪心推理（STAGE），以在跨度级别上提取情感三元组，其中每个跨度可能由多个单词组成，并同时扮演不同的角色。为此，本文提出了一种名为STAGE的新方法来处理ASTE任务，它比以前的模型更准确地识别了给定句子中方面术语、相应的意见术语和相关的情感极性。

    Aspect Sentiment Triplet Extraction (ASTE) has become an emerging task in sentiment analysis research, aiming to extract triplets of the aspect term, its corresponding opinion term, and its associated sentiment polarity from a given sentence. Recently, many neural networks based models with different tagging schemes have been proposed, but almost all of them have their limitations: heavily relying on 1) prior assumption that each word is only associated with a single role (e.g., aspect term, or opinion term, etc. ) and 2) word-level interactions and treating each opinion/aspect as a set of independent words. Hence, they perform poorly on the complex ASTE task, such as a word associated with multiple roles or an aspect/opinion term with multiple words. Hence, we propose a novel approach, Span TAgging and Greedy infErence (STAGE), to extract sentiment triplets in span-level, where each span may consist of multiple words and play different roles simultaneously. To this end, this paper for
    
[^45]: 临床概念指导下的深度预训练变压器模型对住院病历文本进行出院总结医院经过的概括

    Discharge Summary Hospital Course Summarisation of In Patient Electronic Health Record Text with Clinical Concept Guided Deep Pre-Trained Transformer Models. (arXiv:2211.07126v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.07126](http://arxiv.org/abs/2211.07126)

    本文探讨了使用深度学习方法对医院住院病历文本进行出院总结的方法，包括提取和抽象总结，同时测试了一种结合医学概念本体论的混合模型。

    

    简要住院经过（BHC）摘要是一次完整医院治疗经历的简洁概述，被写入由全科医生负责患者整体护理的出院概述中。自动从住院文档中生成摘要的方法将在减轻医生在高时间压力下总结文档的手动负担方面非常有价值。从住院过程自动产生这些总结是一项复杂的多文档摘要任务，因为源笔记是在住院期间从各种不同的角度（如护理，医生，放射学）编写的。我们展示了多种BHC摘要概括方法，证明了深度学习摘要模型在提取和抽象摘要场景下的性能。我们还测试了一个新颖的包含医学概念本体论（SNOMED）作为临床指导的合奏式提取和抽象总结模型。

    Brief Hospital Course (BHC) summaries are succinct summaries of an entire hospital encounter, embedded within discharge summaries, written by senior clinicians responsible for the overall care of a patient. Methods to automatically produce summaries from inpatient documentation would be invaluable in reducing clinician manual burden of summarising documents under high time-pressure to admit and discharge patients. Automatically producing these summaries from the inpatient course, is a complex, multi-document summarisation task, as source notes are written from various perspectives (e.g. nursing, doctor, radiology), during the course of the hospitalisation. We demonstrate a range of methods for BHC summarisation demonstrating the performance of deep learning summarisation models across extractive and abstractive summarisation scenarios. We also test a novel ensemble extractive and abstractive summarisation model that incorporates a medical concept ontology (SNOMED) as a clinical guidanc
    
[^46]: 一种基于Transformer的替代品推荐模型，融合了弱监督的顾客行为数据

    A Transformer-Based Substitute Recommendation Model Incorporating Weakly Supervised Customer Behavior Data. (arXiv:2211.02533v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2211.02533](http://arxiv.org/abs/2211.02533)

    本文将替代品推荐适应到语言匹配问题中，并通过设计新的转换方法去除信号噪音，并考虑了多语言支持。该模型已成功在一个大型电子商务网站上的11个市场和6种语言中部署，提高了顾客忠诚度和购买率。

    

    基于替代品的推荐在电子商务中得到广泛应用，以提供更好的替代品给顾客。但是现有研究通常使用顾客的行为信号（如共同浏览和浏览但购买另一个产品）来捕捉替代关系。尽管这个方法听起来很直观，但我们发现这种做法可能会忽略产品的功能和特性。在本文中，我们通过以产品标题描述作为模型输入，并考虑产品功能，将替代品推荐适应到语言匹配问题中。我们设计了一种新的转换方法来去除从生产数据中得出的信号噪声。此外，我们从工程角度考虑多语言支持。我们提出的端到端基于Transformer的模型在离线和在线实验中均取得了成功。所提出的模型已部署在一个大型电子商务网站上的11个市场和6种语言中。我们的模型被证明可以提高顾客忠诚度和购买率。

    The substitute-based recommendation is widely used in E-commerce to provide better alternatives to customers. However, existing research typically uses the customer behavior signals like co-view and view-but-purchase-another to capture the substitute relationship. Despite its intuitive soundness, we find that such an approach might ignore the functionality and characteristics of products. In this paper, we adapt substitute recommendation into language matching problem by taking product title description as model input to consider product functionality. We design a new transformation method to de-noise the signals derived from production data. In addition, we consider multilingual support from the engineering point of view. Our proposed end-to-end transformer-based model achieves both successes from offline and online experiments. The proposed model has been deployed in a large-scale E-commerce website for 11 marketplaces in 6 languages. Our proposed model is demonstrated to increase re
    
[^47]: M-SpeechCLIP：利用大规模预训练模型进行多语音到图像检索

    M-SpeechCLIP: Leveraging Large-Scale, Pre-Trained Models for Multilingual Speech to Image Retrieval. (arXiv:2211.01180v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.01180](http://arxiv.org/abs/2211.01180)

    本文研究使用大规模预训练模型多语音到图像检索的方法，取得了在非英语检索中比当前最先进性能大幅提高的效果，并证明这些模型也适用于跨语音检索和跨语音文本检索。

    

    本文探讨使用大规模的英语预训练模型（CLIP和HuBERT）进行多语种图像-语音检索的方法。对于非英语图像-语音检索，我们在分别为每种语言训练单独模型以及处理所有三种语言语音的单一模型方面都取得了当前最先进性能的显著提高。我们确定了英语和非英语背景下模型行为和表现之间的主要差异，这些差异可以归因于CLIP和HuBERT的英语预训练，并探究了微调预训练模型如何影响这些差异。最后，我们展示了即使在训练期间从未看到任何平行语音-文本或语音-语音数据，我们的模型也可以用于单语和跨语音文本检索以及跨语音语音检索。

    This work investigates the use of large-scale, English-only pre-trained models (CLIP and HuBERT) for multilingual image-speech retrieval. For non-English image-speech retrieval, we outperform the current state-of-the-art performance by a wide margin both when training separate models for each language, and with a single model which processes speech in all three languages. We identify key differences in model behavior and performance between English and non-English settings, attributable to the English-only pre-training of CLIP and HuBERT, and investigate how fine-tuning the pre-trained models impacts these differences. Finally, we show that our models can be used for mono- and cross-lingual speech-text retrieval and cross-lingual speech-speech retrieval, despite never having seen any parallel speech-text or speech-speech data during training.
    
[^48]: 大型语言模型仍无法规划（LLM在规划和变化推理中的基准）。（arXiv:2206.10498v3 [cs.CL] UPDATED）

    Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change). (arXiv:2206.10498v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.10498](http://arxiv.org/abs/2206.10498)

    本研究提出了一个用于测评LLMs规划和变化推理能力的框架，并测试了流行的LLMs (GPT-3 和 GShard) 在此基准上的表现。研究发现这些模型在最简单的规划任务上都表现不佳，强调了目前LLMs推理能力的严重限制，建议需要大量工作来开发更先进的LLM基础系统来满足实际应用需求。

    

    大型语言模型（LLMs）的最新进展已经改变了自然语言处理（NLP）领域。从GPT-3到PaLM，自然语言任务的最新性能正在随着每个新的大型语言模型的推出不断提高。除了自然语言能力外，人们对于理解此类模型是否具有推理能力产生了极大的兴趣，并采用了推理基准来进行测评。然而，尽管结果看似积极，这些基准在本质上是简单的，LLMs在这些基准上的表现并不能作为支持LLMs推理能力（有时是荒谬的）声称的证据。此外，这些只代表了一个非常有限的简单推理任务集，如果我们要衡量此类基于LLM的系统的真正限制，我们需要研究更复杂的推理问题。受此启发，我们提出了一个可扩展的评估框架，用于测试LLMs规划和变化推理的能力。我们的框架包括一系列的规划和推理任务，例如命题逻辑、因果推断和常识推理，这些任务的难度随着任务的进展而逐渐增加。我们测量了两个流行的LLMs（GPT-3和GShard）在这个基准上的表现，并发现这些模型甚至无法处理最简单的规划任务。我们的发现强调了当前LLMs推理能力的严重局限性，并建议需要大量工作来开发可以规划和推理变化的LLM基础系统，以满足实际应用的需求。

    Recent advances in large language models (LLMs) have transformed the field of natural language processing (NLP). From GPT-3 to PaLM, the state-of-the-art performance on natural language tasks is being pushed forward with every new large language model. Along with natural language abilities, there has been a significant interest in understanding whether such models exhibit reasoning capabilities with the use of reasoning benchmarks. However, even though results are seemingly positive, these benchmarks prove to be simplistic in nature and the performance of LLMs on these benchmarks cannot be used as evidence to support, many a times outlandish, claims being made about LLMs' reasoning capabilities. Further, these only represent a very limited set of simple reasoning tasks and we need to look at more sophisticated reasoning problems if we are to measure the true limits of such LLM-based systems. Motivated by this, we propose an extensible assessment framework to test the capabilities of LL
    
[^49]: InCoder：一种代码填充和合成的生成模型

    InCoder: A Generative Model for Code Infilling and Synthesis. (arXiv:2204.05999v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2204.05999](http://arxiv.org/abs/2204.05999)

    InCoder是一种统一的生成模型，可以进行程序合成和双向上下文的代码填充，是第一个能够直接进行零样本代码填充的生成模型。

    

    代码往往不是一次从左到右的写作过程，而是反复编辑和改进的过程。我们引入了InCoder，一种统一的生成模型，可以通过从左到右的生成进行程序合成，也可以进行编辑（通过填充）。InCoder通过从一个大型开源代码库中随机屏蔽代码块并将其移动到每个文件末尾的方式进行训练，使其可以进行双向上下文的代码填充。我们的模型是第一个能够直接进行零样本代码填充的生成模型，我们在类型推断、注释生成和变量重命名等具有挑战性的任务上进行了评估。我们发现，在具有双向上下文的条件下，能够显著改善执行这些任务的性能，而在标准程序合成基准测试中，与相似规模的仅从左到右预先训练的模型相比，性能相当。InCoder模型和代码已经公开发布。

    Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. htt
    
[^50]: 摘要文本自动概括中的事实不一致问题：综述

    The Factual Inconsistency Problem in Abstractive Text Summarization: A Survey. (arXiv:2104.14839v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2104.14839](http://arxiv.org/abs/2104.14839)

    摘要文本自动概括模型的抽象化能力对于生成准确摘要文本是双刃剑，研究主要关注设计事实感知的评估指标和改进模型的训练以减少事实不一致问题。

    

    最近，一些基于Seq2Seq框架的神经编解码模型被提出来，用于将输入转化为更为抽象的摘要文本。这些神经模型可以自由地生成摘要，没有对所使用单词或短语的任何限制，其格式更接近于人工编辑的摘要，输出更易读，流畅自然。然而，神经模型的抽象化能力是一把双刃剑。所生成摘要中常见的问题是文章事实性信息的扭曲或捏造。原文本和摘要的不一致性引起了各种关于其适用性的担忧，以及针对文本摘要的先前评估方法并不能解决这个问题。为了解决上述问题，当前的研究方向主要分为两类，一类是设计事实感知的评估指标来选择优秀的摘要文本，

    Recently, various neural encoder-decoder models pioneered by Seq2Seq framework have been proposed to achieve the goal of generating more abstractive summaries by learning to map input text to output text. At a high level, such neural models can freely generate summaries without any constraint on the words or phrases used. Moreover, their format is closer to human-edited summaries and output is more readable and fluent. However, the neural model's abstraction ability is a double-edged sword. A commonly observed problem with the generated summaries is the distortion or fabrication of factual information in the article. This inconsistency between the original text and the summary has caused various concerns over its applicability, and the previous evaluation methods of text summarization are not suitable for this issue. In response to the above problems, the current research direction is predominantly divided into two categories, one is to design fact-aware evaluation metrics to select ou
    

