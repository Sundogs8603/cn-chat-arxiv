# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Evaluating Large Language Models for Generalization and Robustness via Data Compression](https://arxiv.org/abs/2402.00861) | 通过数据压缩评估语言模型的泛化性和鲁棒性。使用无损数据压缩方法，对训练截止日期之后的未见数据进行测试。测试结果表明，很多模型在截止日期之后的压缩率显著降低。 |
| [^2] | [Towards Efficient and Exact Optimization of Language Model Alignment](https://arxiv.org/abs/2402.00856) | 本论文提出了一种实现语言模型对齐的高效精确优化方法。通过证明，该方法能够在策略参数化任意的情况下，渐近地与强化学习算法的优化方向一致，并且能够实现高效优化。 |
| [^3] | [Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?](https://arxiv.org/abs/2402.00841) | 本文研究了在实际环境中的会议摘要任务，通过比较小型紧凑LLMs和零射击较大LLMs的性能，发现大多数小型LLMs无法超越较大的零射击LLMs，但FLAN-T5是一个例外，它的性能与许多零射击LLMs持平甚至更好。 |
| [^4] | [OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838) | OLMo是一种真正开放的语言模型，旨在提供给研究社区强大的工具，以促进对语言模型科学的研究和创新。 |
| [^5] | [ALISON: Fast and Effective Stylometric Authorship Obfuscation](https://arxiv.org/abs/2402.00835) | ALISON是一种快速有效的风格学作者身份混淆方法，通过攻击AA模型来保护隐私，相比竞争方法，具有更快的混淆速度和更好的混淆成功率。 |
| [^6] | [Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/abs/2402.00798) | 本文提出了一种将自然语言和形式语言整合的“正式-LLM”框架，用于解决现有LLM智能体无法控制的计划生成问题。实验证明，该框架在提高生成计划性能和确保可控性方面取得了显著改进。 |
| [^7] | [ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models](https://arxiv.org/abs/2402.00794) | 本论文介绍了一种面向生成语言模型的模型无关特征归因方法，称为ReAGent。该方法解决了现有方法在文本生成中的适用性问题，并提供了计算上效率更高的选择。 |
| [^8] | [CroissantLLM: A Truly Bilingual French-English Language Model](https://arxiv.org/abs/2402.00786) | CroissantLLM是一个1.3B的双语语言模型，通过使用1:1的英语-法语预训练数据比例、自定义的分词器和双语调优数据集进行训练，实现了高性能和开源。模型还发布了训练数据集和多个检查点，以及一个法语基准测试 FrenchBench。 |
| [^9] | [Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model](https://arxiv.org/abs/2402.00746) | 提出了一个创新的框架，健康-LLM，通过大规模特征提取和医学知识权衡评分，实现了个性化的检索增强疾病预测模型。这种方法通过整合健康报告，调整特征权重，以及利用语言模型和专家见解提高预测准确性，与传统健康管理方法相比具有明显优势。 |
| [^10] | [Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement](https://arxiv.org/abs/2402.00745) | 本论文提出了一个名为Logic-Explainer的框架，通过迭代符号细化方法增强大型语言模型（LLMs）在伦理NLI中的解释能力。通过集成外部的逆向推理求解器，该框架可以逐步完善自然语言解释，并验证其正确性，减少不完整性和冗余。 |
| [^11] | [BATON: Aligning Text-to-Audio Model with Human Preference Feedback](https://arxiv.org/abs/2402.00744) | 在AI生成内容的背景下，我们提出了BATON框架，利用人类偏好反馈来改善文本到音频模型对齐的问题。通过构建数据集、引入奖励模型和微调现有模型，我们的BATON能够显著提高生成音频的质量。 |
| [^12] | [Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data](https://arxiv.org/abs/2402.00743) | 本研究通过线性回归任务的实验研究了Transformer在非结构化数据中的上下文学习能力，并解释了其中的关键组件。 |
| [^13] | [Transforming and Combining Rewards for Aligning Large Language Models](https://arxiv.org/abs/2402.00742) | 本研究主要研究了对齐大规模语言模型的方法中出现的两个问题：奖励模型的选择以及多个奖励模型的组合。通过引入概率解释，我们提出了一种从Bradley-Terry偏好模型中学习的奖励的自然变换选择，该变换强调改善表现不佳的输出，从而减轻了欠拟合和奖励欺骗。 |
| [^14] | [Improving Semantic Control in Discrete Latent Spaces with Transformer Quantized Variational Autoencoders](https://arxiv.org/abs/2402.00723) | 本研究利用Transformer量化变分自编码器（VQVAEs）中离散潜在空间的可控性，提出了T5VQVAE模型，通过指导T5中的自注意机制实现更好的生成效果。实验证明，T5VQVAE在可控性和生成效果上优于现有模型。 |
| [^15] | [Explaining Text Classifiers with Counterfactual Representations](https://arxiv.org/abs/2402.00711) | 本论文提出了一种使用反事实表示解释文本分类器的方法，通过干预文本表示来生成反事实，并通过实验证实了方法的有效性。 |
| [^16] | [Non-Exchangeable Conformal Language Generation with Nearest Neighbors](https://arxiv.org/abs/2402.00707) | 本文介绍了一种利用最近邻方法扩展的非交换的共形语言生成框架，用于量化自动生成文本的不确定性，并提供带有统计保证的预测集。 |
| [^17] | [Improving Weak-to-Strong Generalization with Scalable Oversight and Ensemble Learning](https://arxiv.org/abs/2402.00667) | 本文通过结合可扩展监督和集成学习的方式，在弱教师和强学生之间减小了能力差距，提高了弱到强泛化能力的框架下的泛化效果。 |
| [^18] | [Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing](https://arxiv.org/abs/2402.00658) | 本文提出了一种通过直接偏好优化在收集到的轨迹上学习基于规划的推理的框架，以解决大型语言模型在复杂推理任务中的虚幻和缺陷问题。 |
| [^19] | [Prosody in Cascade and Direct Speech-to-Text Translation: a case study on Korean Wh-Phrases](https://arxiv.org/abs/2402.00632) | 本研究通过对比评估方法，准确衡量了直接语音-文本翻译系统在消除韵律对话中的歧义方面的能力，并通过在韩语-英语翻译系统上的实验结果，明确展示了直接翻译系统的价值。 |
| [^20] | [Actor Identification in Discourse: A Challenge for LLMs?](https://arxiv.org/abs/2402.00620) | 论文探讨了在辩论中识别政治角色的挑战，使用了传统的NLP组件与LLM进行比较，并发现LLM在生成正确的正式形式方面存在困难。作者提出了将LLM与分类器结合的混合模型来改善性能。 |
| [^21] | [A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains](https://arxiv.org/abs/2402.00559) | 本论文提出了Reveal数据集，用于在开放领域的问答设置中对复杂思维链的自动验证进行基准测试。这个数据集包含了详尽的标签，用于评估语言模型的答案中每个推理步骤的相关性、归因和逻辑正确性。 |
| [^22] | [Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning](https://arxiv.org/abs/2402.00530) | 提出了一种名为“超级过滤”的方法，能够使用较小和较弱的模型对用于训练较大和较强的语言模型的指令数据进行过滤，从而降低了过滤成本，并在标准基准测试中获得更好的性能。 |
| [^23] | [EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models](https://arxiv.org/abs/2402.00518) | 该论文介绍了一种经济且可扩展的解决方案EE-Tuning，可以使用较少的计算资源和训练数据针对早期终止大型语言模型进行调整，通过性能优化和3D并行性实现卓越的训练效率。实验证实，即使在有限的训练预算下，也可以实现有效的早期终止LLM推理。 |
| [^24] | [SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models](https://arxiv.org/abs/2402.00474) | SA-MDKIF是一种可扩展和适应性强的医学领域知识注入框架，通过指令调整并训练医学技能，并在推理中将其与大型语言模型集成，提高了医学任务的性能。 |
| [^25] | [Instruction Makes a Difference](https://arxiv.org/abs/2402.00453) | 通过引入指令数据集，我们展示了在文档分析和文档图像预测中训练语言-视觉模型的重要性，并表明使用指令数据集可以提高性能。使用Polling-based Object Probing Evaluation (POPE)数据集进行评估，我们发现指令调优性能相对于零-shot性能提高了11倍到32倍，并且相对于非指令微调提高了0.1%到4.2%。尽管如此，仍有很大的改进空间，因为这些性能仍未达到人类水平（94.36%）。 |
| [^26] | [Improving Dialog Safety using Socially Aware Contrastive Learning](https://arxiv.org/abs/2402.00446) | 本研究提出了使用具有社会意识的对比学习来提高对话安全性的方法。通过研究对抗和随意对话背景下的亲社会性，我们发现现有的模型很难识别自然对话中微妙的不安全情况。为了解决这个问题，我们采用了一个双步骤的微调过程，并通过整合亲社会行为的数据集来训练基础模型。 |
| [^27] | [From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models](https://arxiv.org/abs/2402.00421) | 本研究介绍了专利响应智能系统PARIS和LE-PARIS，通过构建OA主题数据库、开发响应模板以及实施推荐系统和基于LLM的响应生成，旨在加快专利律师处理审查意见回应的效率。 通过多范式分析和长期数据验证，证明了OA主题的建设性和LLM对于回应自动生成的可行性。 |
| [^28] | [Prompt-Time Symbolic Knowledge Capture with Large Language Models](https://arxiv.org/abs/2402.00414) | 本文研究利用大型语言模型来实现基于提示驱动的知识捕获，特别关注提示到三元组的生成，并通过专门的合成数据集评估了三种方法的性能。 |
| [^29] | [Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection](https://arxiv.org/abs/2402.00412) | 本文旨在对抗评估AI生成的学生论文检测，通过构建AIG-ASAP数据集和使用文本扰动方法，揭示现有检测器易被自动对抗攻击所绕过的问题。 |
| [^30] | [Investigating Bias Representations in Llama 2 Chat via Activation Steering](https://arxiv.org/abs/2402.00402) | 我们通过激活导向技术研究了Llama 2 Chat中的偏见表示问题，发现该模型存在固有的性别偏见，并观察到偏见与模型拒绝回应的倾向之间存在负相关关系。 |
| [^31] | [Efficient Exploration for LLMs](https://arxiv.org/abs/2402.00396) | 高效探索在改善大型语言模型方面具有显著优势，可以以较少的查询实现较高的性能水平。不确定性估计和探索方案的选择是关键因素。 |
| [^32] | [Computational Morphology and Lexicography Modeling of Modern Standard Arabic Nominals](https://arxiv.org/abs/2402.00385) | 本研究通过计算形态学和词典学模型成功解决了现代标准阿拉伯语名词的形态和词汇建模挑战，提供了全面和可扩展的模型，并在准确性和一致性上显著优于常用的分析器和生成器。 |
| [^33] | [What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection](https://arxiv.org/abs/2402.00371) | 本文研究了大语言模型（LLMs）在社交媒体机器人检测中的机遇和风险。通过提出混合异质专家框架，我们设计了新颖的LLM机器人检测器，并发现仅使用少量标注示例进行指导调整即可取得超过最先进基线模型的性能提升。然而，LLM引导的操纵策略可能会显著降低现有机器人检测的性能。 |
| [^34] | [Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration](https://arxiv.org/abs/2402.00367) | 本论文研究了识别大型语言模型（LLM）知识盲区的方法，并提出了两种基于LLM协作的新方法，通过这些方法可以在面对知识盲区时放弃回答问题。实验证明，这些方法在提高放弃准确度方面取得了高达19.3％的改进。 |
| [^35] | [IndiVec: An Exploration of Leveraging Large Language Models for Media Bias Detection with Fine-Grained Bias Indicators](https://arxiv.org/abs/2402.00345) | 这项研究引入了一个通用的偏见检测框架IndiVec，利用大型语言模型构建细粒度媒体偏见数据库，并通过多数投票确定输入的偏见标签。这一框架具有适应性和可解释性，在各种数据集上表现出一致的性能。 |
| [^36] | [Bias in Opinion Summarisation from Pre-training to Adaptation: A Case Study in Political Bias](https://arxiv.org/abs/2402.00322) | 本研究通过以政治偏见为例，量化了生成模型中的偏见，并发现大多数模型存在固有偏见。通过对比各种适应方法，在社交媒体观点摘要任务中发现，调整较少参数的模型相对于标准微调模型来说具有较少的偏见。 |
| [^37] | [A Crucial Parameter for Rank-Frequency Relation in Natural Languages](https://arxiv.org/abs/2402.00271) | 本研究发现自然语言中单词的等级-频率关系可以更准确地由公式$f\propto r^{-\alpha} \cdot (r+\gamma)^{-\beta}$来模拟，其中关键的参数是$\gamma$，它描述了词汇增长对语料库的影响。同时提出了一种通过搜索最优$\gamma$来估计参数的方法，并用案例研究论证了该公式和参数的适用性。 |
| [^38] | [Does \textsc{DetectGPT} Fully Utilize Perturbation? Selective Perturbation on Model-Based Contrastive Learning Detector would be Better](https://arxiv.org/abs/2402.00263) | 我们提出了一种新颖的检测器，模型名，它使用选择性策略扰动和多对比学习，在减少随机屏蔽引起的信息丢失的同时，进一步提升少样本和个体输入的性能。 |
| [^39] | [A Survey on Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2402.00253) | 这份综合调查研究了大规模视觉-语言模型中的幻觉问题，澄清了幻觉的概念，提出了评估方法，并对幻觉的根本原因进行了调查。 |
| [^40] | [Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning](https://arxiv.org/abs/2402.00251) | 本文提出了一种高效的非参数化不确定性量化方法，用于黑盒大型语言模型和决策规划。该方法可以有效地估计输入-决策之间的逐点依赖关系，并提供统计上对决策可信度的解释。另外，本文还提出了一个系统化的决策代理设计，根据用户提示生成动作，并在存在多个高估计逐点依赖性的动作时要求用户提供偏好。 |
| [^41] | [Exploring the limits of decoder-only models trained on public speech recognition corpora](https://arxiv.org/abs/2402.00235) | 本研究探索了仅基于解码器训练的模型在公共语音识别语料库上的极限，开发了名为DOTA的模型，在大多数英语语音识别基准测试中优于其他开源模型，并公开了代码库和模型检查点。 |
| [^42] | [Are Generative AI systems Capable of Supporting Information Needs of Patients?](https://arxiv.org/abs/2402.00234) | 生成式AI系统被应用于支持患者信息需求的研究中，以提高患者对放射学数据的理解和管理能力。通过与患者和医疗专家的对话分析，我们确定了常见的医学信息需求和问题。 |
| [^43] | [De-identification is not always enough](https://arxiv.org/abs/2402.00179) | 研究表明，仅仅进行去识别操作并不能有效保护隐私。本文提出了使用大型语言模型生成合成临床笔记的方法，并评估了其在临床任务中的性能。同时，还发现利用合成数据训练模型可以提高会员推理攻击的成功率。 |
| [^44] | [Multimodal Clinical Pseudo-notes for Emergency Department Prediction Tasks using Multiple Embedding Model for EHR (MEME)](https://arxiv.org/abs/2402.00160) | 本研究提出了一种名为“MEME”的多重嵌入模型，将电子健康记录视为多模态数据。通过结合“伪笔记”和多模态方法，该模型在紧急科室预测任务中表现出优越性能，超过了单模态嵌入方法和传统机器学习方法。然而，该模型在不同医院机构之间存在泛化能力方面的局限性。 |
| [^45] | [Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research](https://arxiv.org/abs/2402.00159) | Dolma是一个包含三万亿个令牌的开放语料库，用于语言模型预训练研究。它包含了来自多种来源的网络内容、科技论文、代码、公共领域图书、社交媒体和百科全书材料。为了促进开放研究，我们还开源了数据整理工具包。 |
| [^46] | [Large Language Models for Mathematical Reasoning: Progresses and Challenges](https://arxiv.org/abs/2402.00157) | 大型语言模型(LLMs)在解决数学问题方面涉及了大量的数学问题类型和不同的数据集和设置。目前仍然存在一些挑战，需要进一步研究和解决。 |
| [^47] | [The Impact of Language Adapters in Cross-Lingual Transfer for NLU](https://arxiv.org/abs/2402.00149) | 本文研究了在零样本跨语言传输中语言适配器的作用，结果显示其在任务、语言和模型之间的效果不一致。保留源语言适配器通常导致相同或更好的性能。 |
| [^48] | [Making a Long Story Short in Conversation Modeling](https://arxiv.org/abs/2402.00143) | 本研究探讨了对话模型中话语长度多样性对生成后续回复质量的影响，发现在某些对话类型中，可以将话语长度减少72%而不影响质量。 |
| [^49] | [Common Sense Reasoning for Deep Fake Detection](https://arxiv.org/abs/2402.00126) | 该论文提出使用常识推理来建模深度伪造检测，通过扩展到Deepfake Detection VQA任务来模拟人类直觉，解释标记图像为真实或伪造的原因。 |
| [^50] | [Comparing Template-based and Template-free Language Model Probing](https://arxiv.org/abs/2402.00123) | 本文比较了基于模板和非模板语言模型的探测方法，发现它们在模型排名、绝对得分和与困惑度的关系等方面存在差异。 |
| [^51] | [D-Nikud: Enhancing Hebrew Diacritization with LSTM and Pretrained Models](https://arxiv.org/abs/2402.00075) | D-Nikud是一种新颖的希伯来语点音化方法，通过结合LSTM和BERT-based预训练模型，实现了最先进的结果，并且在现代文本和更详细的点音化方面表现出色。 |
| [^52] | [EvoMerge: Neuroevolution for Large Language Models](https://arxiv.org/abs/2402.00070) | EvoMerge是一种针对大型语言模型训练和合并的系统性方法，通过权重交叉和微调进行权重变异，旨在将模型推向超越传统微调限制的进化过程。 |
| [^53] | [Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding](https://arxiv.org/abs/2402.00024) | LLaMA和ChatGPT比较分析了它们在SMILES字符串嵌入中的性能，在分子性质预测和药物-药物相互作用预测中，LLaMA相对于ChatGPT表现更好并且与现有方法相当。 |
| [^54] | [Small LLMs Are Weak Tool Learners: A Multi-LLM Agent](https://arxiv.org/abs/2401.07324) | 本论文提出了一种新的策略，将大型语言模型代理（LLMs）的能力分解为计划器、调用器和总结器模块，以克服小型模型性能限制和工具更新的问题。 |
| [^55] | [Commonsense for Zero-Shot Natural Language Video Localization](https://arxiv.org/abs/2312.17429) | 本文研究了零样本自然语言-视频定位中常识推理的有效性，并提出了CORONET框架，该框架利用常识进行视频和生成的伪查询之间的桥接。实验证明CORONET在零样本和传统NLVL方法上都取得了优越的结果。 |
| [^56] | [RLHF and IIA: Perverse Incentives](https://arxiv.org/abs/2312.01057) | RLHF算法中的IIA假设导致了倒置激励，限制了查询格式和学习算法的创新。 |
| [^57] | [Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks](https://arxiv.org/abs/2311.05085) | 大型语言模型在知识密集型任务中的理性化能力有待探索，通过使用专家编写的示例，可以生成更受欢迎的基于世界知识的理性化方式。这些理性化方式需要进一步改进，在错误预测的理性化方面会损害人类对模型的信任。 |
| [^58] | [InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining](https://arxiv.org/abs/2310.07713) | InstructRetro是目前规模最大的使用检索预训练的LLM，扩展了基础模型Retro 48B，通过指令调优在各种零样例任务上取得显著改进。 |
| [^59] | [Scaling up Discovery of Latent Concepts in Deep NLP Models](https://arxiv.org/abs/2308.10263) | 本文研究了聚类算法，以将深度自然语言处理模型中编码的概念扩展到更大的数据集和模型上。我们发现基于K-Means的概念发现显著提高了效率并保持了质量。 |
| [^60] | [Leveraging Open Information Extraction for More Robust Domain Transfer of Event Trigger Detection](https://arxiv.org/abs/2305.14163) | 本研究利用开放信息抽取系统中的主谓关系将事件触发器在不同领域间进行耦合，以提升事件触发识别的领域转移性能。在从高资源到低资源领域的转移中，该方法可减轻性能下降，特别是在从维基百科到新闻领域的转移中效果显著。同时结合遮蔽语言建模能进一步增强转移效果。 |
| [^61] | [Engineering A Large Language Model From Scratch.](http://arxiv.org/abs/2401.16736) | Atinuke是一种基于Transformer的神经网络，通过在处理时序数据的层与注意机制交织在一起，模拟人类语言，从而优化各种语言任务的性能。 |
| [^62] | [UNSEE: Unsupervised Non-contrastive Sentence Embeddings.](http://arxiv.org/abs/2401.15316) | UNSEE是一种无监督的非对比度句子嵌入方法，通过引入目标网络解决了表示坍塌问题，达到了与对比目标相当的性能提升。 |
| [^63] | [On the Affinity, Rationality, and Diversity of Hierarchical Topic Modeling.](http://arxiv.org/abs/2401.14113) | 本论文提出了一种名为TraCo的新层次主题模型，通过传输计划依赖方法和上下文感知的解缠码器，改善了主题层次结构的亲和性、合理性和多样性。 |
| [^64] | [Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation.](http://arxiv.org/abs/2401.11864) | 本研究提出了思维方程蒸馏（EoTD）技术和集合思维蒸馏（ETD）框架，通过构建基于方程的表示和使用多个思维过程的推理数据集来改进小型语言模型（SLMs）的数学推理能力，实验结果表明，EoTD和ETD显著提升了SLMs的推理能力。 |
| [^65] | [Zero-shot Generative Large Language Models for Systematic Review Screening Automation.](http://arxiv.org/abs/2401.06320) | 本研究调查了使用零样本生成式大型语言模型进行系统性综述自动筛选的有效性，结果显示指导微调和校准技术在筛选中起到重要作用，并且与零样本模型的集成相结合可以显著节省筛选时间。 |
| [^66] | [LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization.](http://arxiv.org/abs/2401.06034) | LinguAlchemy是一种将语言类型学和地理元素融合的正则化技术，能够显著提高预训练语言模型（PLMs）在未见语言上的泛化性能。 |
| [^67] | [A First Look at Information Highlighting in Stack Overflow Answers.](http://arxiv.org/abs/2401.01472) | 本论文进行了首次大规模的探索性研究，研究了Stack Overflow回答中的信息高亮。通过使用神经网络架构，开发了自动推荐突出内容的方法。 |
| [^68] | [Meta Prompting for AGI Systems.](http://arxiv.org/abs/2311.11482) | 本文全面研究了元提示技术，这是一种创新方法，重塑了大型语言模型、多模态模型和人工智能系统在问题解决和数据解释方面的应用。通过强调信息的结构和句法，元提示将复杂问题拆解为简单的子问题，提高了效率，并且能够与少样本方法进行公平的比较。同时，本文还提出了元提示用于自动生成提示的方法。 |
| [^69] | [A decoder-only foundation model for time-series forecasting.](http://arxiv.org/abs/2310.10688) | 本论文介绍了一种基于补丁解码器式注意力模型的时间序列预测基础模型，该模型在零样本情况下在各种公共数据集上的性能接近最先进的监督预测模型。 |
| [^70] | [SELF: Language-Driven Self-Evolution for Large Language Model.](http://arxiv.org/abs/2310.00533) | SELF提出了一种基于语言驱动的创新方法，允许大型语言模型（LLM）不断自我进化，并通过语言反馈作为评估工具来改进模型的响应能力和训练稳定性。 |
| [^71] | [Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies.](http://arxiv.org/abs/2309.13063) | 通过使用大型语言模型生成用户意图分类，我们提出了一种新方法来分析和验证日志数据中的用户意图，从而解决了手动或基于机器学习的标注方法在大型和不断变化的数据集上的问题。 |
| [^72] | [Code Llama: Open Foundation Models for Code.](http://arxiv.org/abs/2308.12950) | Code Llama是一系列用于代码的开放基础模型，具有最先进的性能和填充功能，支持大型输入上下文和零-shot指令跟踪能力。在多个代码基准测试中，Code Llama达到开放模型中最高的性能，同时Python专门化模型在某些测试上超越了Llama 2的70B版本。 |
| [^73] | [Leveraging Implicit Feedback from Deployment Data in Dialogue.](http://arxiv.org/abs/2307.14117) | 研究利用对话中的隐式反馈来改进社交对话系统。实验结果表明通过收集用户响应长度、情感和反应等信号可以提高机器生成话语的质量。 |
| [^74] | [CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots.](http://arxiv.org/abs/2307.11865) | 本研究使用大型语言模型，探索了在空间规划和导航交叉问题中，通过解析复杂的自然语言指令来执行任务的新方法。 |
| [^75] | [VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores.](http://arxiv.org/abs/2306.01879) | 我们提出了VisualGPTScore方法，能够使用多模态生成分数捕捉文本标题可能性，并在图像条件语言模型上进行计算，具备组合推理能力。 |
| [^76] | [Small Language Models Improve Giants by Rewriting Their Outputs.](http://arxiv.org/abs/2305.13514) | 本论文提出了一种方法，通过使用小语言模型重写大语言模型的输出，从而提高其性能。实验证明，该方法可以显着改善大语言模型的少样本学习能力和泛化性能。 |
| [^77] | [Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions.](http://arxiv.org/abs/2305.07303) | 本论文提出了一种从自然语言定义中学习多关系双曲词向量的框架，以捕捉由定义所引起的分层和多分辨率结构。 |
| [^78] | [Stars Are All You Need: A Distantly Supervised Pyramid Network for Document-Level End-to-End Sentiment Analysis.](http://arxiv.org/abs/2305.01710) | 本文提出了一种文档级端到端情感分析方法，通过星级评分标签，实现方面检测、情感分析和评分预测，具有良好的性能和可解释性。 |

# 详细

[^1]: 通过数据压缩评估大型语言模型的泛化性和鲁棒性

    Evaluating Large Language Models for Generalization and Robustness via Data Compression

    [https://arxiv.org/abs/2402.00861](https://arxiv.org/abs/2402.00861)

    通过数据压缩评估语言模型的泛化性和鲁棒性。使用无损数据压缩方法，对训练截止日期之后的未见数据进行测试。测试结果表明，很多模型在截止日期之后的压缩率显著降低。

    

    现有的大型语言模型评估方法面临数据污染、对提示敏感以及基准测试创建成本高等挑战。为了解决这些问题，我们提出了一种基于无损数据压缩的评估方法，测试模型的预测能力在其训练截止日期之后的泛化情况。具体而言，我们收集了从2017年到2023年共83个月的全面测试数据，并根据模型的训练数据截止日期将数据分为训练和测试期。我们使用两个指标进行评估：1）测试期的压缩性能作为对未见数据的泛化能力的衡量；2）训练期和测试期之间的性能差距作为鲁棒性的衡量。我们的实验证明，许多模型的压缩率在截止日期之后显著降低，但像... (内容过长，省略)

    Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation. To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff. Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff. We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness. Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data. We find that the compression rate of many models reduces significantly after their cutoff date, but models such a
    
[^2]: 实现语言模型对齐的高效精确优化方法

    Towards Efficient and Exact Optimization of Language Model Alignment

    [https://arxiv.org/abs/2402.00856](https://arxiv.org/abs/2402.00856)

    本论文提出了一种实现语言模型对齐的高效精确优化方法。通过证明，该方法能够在策略参数化任意的情况下，渐近地与强化学习算法的优化方向一致，并且能够实现高效优化。

    

    将语言模型与人类偏好进行对齐对于其在实际任务中的应用至关重要。该问题被建模为优化模型策略，以最大化反映人类偏好的预期奖励，并尽量减小与初始策略的偏差。尽管强化学习（RL）被认为是一种直接的解决方案，但其策略更新的方差很高，阻碍了高效的策略改进。最近，直接偏好优化（DPO）被提出以直接从偏好数据中优化策略。尽管实现简单，DPO是基于不一定能在实践中实现的最优策略导出的，这削弱了其收敛到预期解决方案的能力。本文提出了一种高效精确优化（EXO）的对齐目标方法。我们证明了对于策略的任意参数化，EXO保证渐近地与RL算法的优化方向一致，并且能够实现高效优化。

    The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. Though simple to implement, DPO is derived based on the optimal policy that is not assured to be achieved in practice, which undermines its convergence to the intended solution.   In this paper, we propose efficient exact optimization (EXO) of the alignment objective. We prove that EXO is guaranteed to optimize in the same direction as the RL algorithms asymptotically for arbitary parametrization of the policy, while enables efficient op
    
[^3]: 小型大型语言模型在实际环境中的会议摘要中是否能够超越其体量？

    Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?

    [https://arxiv.org/abs/2402.00841](https://arxiv.org/abs/2402.00841)

    本文研究了在实际环境中的会议摘要任务，通过比较小型紧凑LLMs和零射击较大LLMs的性能，发现大多数小型LLMs无法超越较大的零射击LLMs，但FLAN-T5是一个例外，它的性能与许多零射击LLMs持平甚至更好。

    

    大型语言模型（LLMs）在没有明确针对任务特定数据集进行微调的情况下，展示了解决各种任务的出色能力。然而，在实际环境中部署LLMs并非易事，因为需要大量的计算资源。本文研究了在实际工业环境中的会议摘要任务，并通过比较经过微调的小型紧凑LLMs（如FLAN-T5、TinyLLaMA、LiteLLaMA）与零射击较大LLMs（如LLaMA-2、GPT-3.5、PaLM-2）的性能，进行了大量实验以解决利用LLMs在实际环境中的巨大成本问题。我们观察到，大多数小型LLMs，即使经过微调，也无法在会议摘要数据集中超越较大的零射击LLMs。然而，一个值得注意的例外是FLAN-T5（780M个参数），它的性能与许多零射击LLMs持平甚至更好。

    Large Language Models (LLMs) have demonstrated impressive capabilities to solve a wide range of tasks without being explicitly fine-tuned on task-specific datasets. However, deploying LLMs in the real world is not trivial, as it requires substantial computing resources. In this paper, we investigate whether smaller, compact LLMs are a good alternative to the comparatively Larger LLMs2 to address significant costs associated with utilizing LLMs in the real world. In this regard, we study the meeting summarization task in a real-world industrial environment and conduct extensive experiments by comparing the performance of fine-tuned compact LLMs (e.g., FLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2, GPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning, fail to outperform larger zero-shot LLMs in meeting summarization datasets. However, a notable exception is FLAN-T5 (780M parameters), which performs on par or even better than many zero-sho
    
[^4]: OLMo: 加速语言模型科学

    OLMo: Accelerating the Science of Language Models

    [https://arxiv.org/abs/2402.00838](https://arxiv.org/abs/2402.00838)

    OLMo是一种真正开放的语言模型，旨在提供给研究社区强大的工具，以促进对语言模型科学的研究和创新。

    

    语言模型（LM）已广泛应用于自然语言处理研究和商业产品。随着商业重要性的增加，最强大的模型已经封闭起来，只能通过专有接口访问，其训练数据、架构和开发细节没有透露。考虑到这些细节对于科学研究这些模型的重要性，包括其偏见和潜在风险，我们认为研究社区有权访问强大而真正开放的LM。为此，本技术报告详细介绍了OLMo的首个版本，这是一种最先进、真正开放的语言模型，以及构建和研究语言建模科学的框架。与之前只发布模型权重和推理代码的努力不同，我们发布OLMo和整个框架，包括训练数据、训练和评估代码。我们希望这个发布能增强开放研究社区的能力，并激发更多的创新。

    Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspi
    
[^5]: ALISON: 快速有效的风格学作者身份混淆

    ALISON: Fast and Effective Stylometric Authorship Obfuscation

    [https://arxiv.org/abs/2402.00835](https://arxiv.org/abs/2402.00835)

    ALISON是一种快速有效的风格学作者身份混淆方法，通过攻击AA模型来保护隐私，相比竞争方法，具有更快的混淆速度和更好的混淆成功率。

    

    作者归属度（AA）和作者身份混淆（AO）是隐私研究中日益重要的两项竞争任务。现代AA利用作者的一贯写作风格，使用AA分类器将文本与其作者匹配。AO是相应的对抗性任务，旨在以一种方式修改文本，使其语义得到保留，但AA模型无法正确推断其作者。为了解决最先进的AA方法引发的隐私问题，提出了新的AO方法，但由于其训练和混淆速度过慢（通常需要数小时），使用起来仍然不太实际。面对这一挑战，我们提出了一种实用的AO方法ALISON，它（1）大大减少了训练/混淆时间，演示了比最先进的AO方法快10倍以上的混淆速度，（2）通过攻击两个基准数据集上的三种基于Transformer的AA方法，实现了更好的混淆成功率，通常比竞争方法表现好15%。

    Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours. To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing method
    
[^6]: 正式-LLM：将形式语言和自然语言集成于可控的LLM智能体中

    Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents

    [https://arxiv.org/abs/2402.00798](https://arxiv.org/abs/2402.00798)

    本文提出了一种将自然语言和形式语言整合的“正式-LLM”框架，用于解决现有LLM智能体无法控制的计划生成问题。实验证明，该框架在提高生成计划性能和确保可控性方面取得了显著改进。

    

    最近，对于大型语言模型（LLMs）的进展使得人工智能智能体能够自动生成和执行解决复杂任务的多步计划。然而，由于LLM的内容生成过程几乎无法控制，当前的LLM智能体经常生成无效或不可执行的计划，这损害了生成计划的性能并破坏了用户对LLM智能体的信任。为应对这个问题，本文提出了一种新颖的“正式-LLM”框架，用于LLM智能体，通过将自然语言的表达力和形式语言的精确性进行整合。具体而言，该框架允许人类用户将他们对计划过程的要求或约束表达为自动机。然后，在自动机的监督下，使用基于堆栈的LLM计划生成过程来确保生成的计划满足约束条件，从而使计划过程可控。我们在基准任务和实际的真实任务上进行了实验，并且obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.

    Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
    
[^7]: ReAGent: 一个面向生成语言模型的模型无关特征归因方法

    ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models

    [https://arxiv.org/abs/2402.00794](https://arxiv.org/abs/2402.00794)

    本论文介绍了一种面向生成语言模型的模型无关特征归因方法，称为ReAGent。该方法解决了现有方法在文本生成中的适用性问题，并提供了计算上效率更高的选择。

    

    特征归因方法（FAs），如梯度和注意力机制，被广泛应用于确定所有输入特征对模型预测的重要性。现有自然语言处理领域的研究主要集中在为仅有编码器的语言模型（LMs）开发和测试FAs，用于分类任务。然而，尚不清楚在文本生成上是否可以使用这些FAs来处理仅有解码器的模型，因为模型架构和任务设置之间存在固有差异。此外，先前的研究已经证明，没有一个通用的FA适用于所有模型和任务。这使得针对大型LMs选择FA计算上非常昂贵，因为输入重要性的推导通常需要多个前向和反向传递，包括可能是限制性的梯度计算。为了解决这些问题，我们提出了一种面向生成LMs的模型无关FA，称为递归归因生成器（ReAGent）。

    Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions. Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks. However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively. Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks. This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute. To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent)
    
[^8]: CroissantLLM: 一个真正的双语法语-英语语言模型

    CroissantLLM: A Truly Bilingual French-English Language Model

    [https://arxiv.org/abs/2402.00786](https://arxiv.org/abs/2402.00786)

    CroissantLLM是一个1.3B的双语语言模型，通过使用1:1的英语-法语预训练数据比例、自定义的分词器和双语调优数据集进行训练，实现了高性能和开源。模型还发布了训练数据集和多个检查点，以及一个法语基准测试 FrenchBench。

    

    我们介绍了CroissantLLM，这是一个在3T个英语和法语标记上预训练的13亿语言模型，为研究和工业社区带来了一种高性能的、完全开源的双语模型，可以在消费级本地硬件上快速运行。为此，我们首次尝试使用1:1的英语-法语预训练数据比例、自定义的分词器和双语调优数据集来训练一种内在双语的模型。我们发布了训练数据集，其中包含了一个法语分割，其中包含了手工策划、高质量和多样化的数据源。为了评估在英语以外的性能，我们创建了一个新的基准测试 FrenchBench，包括一系列分类和生成任务，涵盖了模型在法语语言中性能的各个方面。此外，为了保持透明度并促进进一步的大规模语言模型研究，我们发布了代码库和各种模型规模、训练数据分布上的几十个检查点。

    We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, 
    
[^9]: 健康-LLM：个性化检索增强的疾病预测模型

    Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model

    [https://arxiv.org/abs/2402.00746](https://arxiv.org/abs/2402.00746)

    提出了一个创新的框架，健康-LLM，通过大规模特征提取和医学知识权衡评分，实现了个性化的检索增强疾病预测模型。这种方法通过整合健康报告，调整特征权重，以及利用语言模型和专家见解提高预测准确性，与传统健康管理方法相比具有明显优势。

    

    在卫生保健领域，人工智能（AI）极大地推进了智能医疗技术的发展。然而，传统智能医疗受限于静态数据和统一标准，无法完全与个体情况集成，同时也面临其他挑战。为此，我们提出了一种创新的框架，命名为健康-LLM，将大规模特征提取和医学知识权衡评分相结合。与传统健康管理方法相比，我们的方法具有三个主要优势。首先，我们的方法将健康报告整合到大模型中，提供详细的任务信息。其次，我们使用专业的医学专业知识调整健康特征的权重得分。第三，我们使用半自动特征提取框架增强语言模型的分析能力，并整合专家见解以提高疾病预测的准确性。

    Artificial intelligence (AI) in healthcare has significantly advanced intelligent medical treatment. However, traditional intelligent healthcare is limited by static data and unified standards, preventing full integration with individual situations and other challenges. Hence, a more professional and detailed intelligent healthcare method is needed for development. To this end, we propose an innovative framework named Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring. Compared to traditional health management methods, our approach has three main advantages. First, our method integrates health reports into a large model to provide detailed task information. Second, professional medical expertise is used to adjust the weighted scores of health characteristics. Third, we use a semi-automated feature extraction framework to enhance the analytical power of language models and incorporate expert insights to improve the accuracy of disease predic
    
[^10]: 通过迭代符号细化提升大型语言模型的伦理解释能力

    Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement

    [https://arxiv.org/abs/2402.00745](https://arxiv.org/abs/2402.00745)

    本论文提出了一个名为Logic-Explainer的框架，通过迭代符号细化方法增强大型语言模型（LLMs）在伦理NLI中的解释能力。通过集成外部的逆向推理求解器，该框架可以逐步完善自然语言解释，并验证其正确性，减少不完整性和冗余。

    

    越来越多关于自然语言推理（NLI）的研究集中在大型语言模型（LLMs）及其推理能力的应用和评估上。尽管取得了成功，LLMs仍然容易出现事实错误和解释上的不一致，限制了复杂领域推理的控制性和可解释性。本文关注伦理 NLI，研究混合神经符号技术如何提升LLMs产生的伦理解释的逻辑有效性和一致性。具体地，我们提出了一个名为Logic-Explainer的归纳-演绎框架，将LLMs与外部的逆向推理求解器结合起来，逐步完善自然语言解释，并联合验证其正确性、减少不完整性和冗余。通过大量的实证分析表明，Logic-Explainer可以改进通过环境学习方法和Chain-of-Thought（CoT）生成的解释。

    An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challe
    
[^11]: BATON：将文本到音频模型与人类偏好反馈进行对齐

    BATON: Aligning Text-to-Audio Model with Human Preference Feedback

    [https://arxiv.org/abs/2402.00744](https://arxiv.org/abs/2402.00744)

    在AI生成内容的背景下，我们提出了BATON框架，利用人类偏好反馈来改善文本到音频模型对齐的问题。通过构建数据集、引入奖励模型和微调现有模型，我们的BATON能够显著提高生成音频的质量。

    

    随着AI生成内容（AIGC）的发展，文本到音频模型引起了广泛关注。然而，由于自然语言的固有信息密度和有限的模型理解能力，这些模型很难生成与人类偏好对齐的音频。为了缓解这个问题，我们提出了BATON，这是一个旨在通过人类偏好反馈提高生成音频与文本提示之间对齐度的框架。我们的BATON包括三个关键阶段：首先，我们构建了一个包含提示和相应生成音频的数据集，并基于人类反馈进行了注释。其次，我们引入了一个奖励模型，利用构建的数据集可以模拟人类偏好，通过对输入的文本-音频对分配奖励。最后，我们利用奖励模型对现成的文本到音频模型进行了微调。实验结果表明，我们的BATON可以显著提高原始音频的生成质量。

    With the development of AI-Generated Content (AIGC), text-to-audio models are gaining widespread attention. However, it is challenging for these models to generate audio aligned with human preference due to the inherent information density of natural language and limited model understanding ability. To alleviate this issue, we formulate the BATON, a framework designed to enhance the alignment between generated audio and text prompt using human preference feedback. Our BATON comprises three key stages: Firstly, we curated a dataset containing both prompts and the corresponding generated audio, which was then annotated based on human feedback. Secondly, we introduced a reward model using the constructed dataset, which can mimic human preference by assigning rewards to input text-audio pairs. Finally, we employed the reward model to fine-tune an off-the-shelf text-to-audio model. The experiment results demonstrate that our BATON can significantly improve the generation quality of the orig
    
[^12]: Transformer的好处：在非结构化数据的线性回归任务中的上下文学习

    Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data

    [https://arxiv.org/abs/2402.00743](https://arxiv.org/abs/2402.00743)

    本研究通过线性回归任务的实验研究了Transformer在非结构化数据中的上下文学习能力，并解释了其中的关键组件。

    

    实践中观察到，基于Transformer的模型在推理阶段能够学习上下文中的概念。现有的文献，例如\citet{zhang2023trained,huang2023context}对这种上下文学习能力提供了理论解释，但是他们假设每个样本的输入$x_i$和输出$y_i$都被嵌入到相同的令牌中（即结构化数据）。然而，在现实中，它们呈现为两个令牌（即非结构化数据\cite{wibisono2023role}）。在这种情况下，本文进行了线性回归任务的实验，研究了Transformer架构的好处，并提供了一些相应的理论直觉，解释了为什么Transformer可以从非结构化数据中学习。我们研究了在Transformer中起到上下文学习作用的确切组件。特别地，我们观察到（1）带有两层softmax（自我）注意力和前瞻性注意力掩码的Transformer可以从提示中学习，如果$y_i$在令牌中。

    In practice, it is observed that transformer-based models can learn concepts in context in the inference stage. While existing literature, e.g., \citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data). However, in reality, they are presented in two tokens (i.e., unstructured data \cite{wibisono2023role}). In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data. We study the exact components in a transformer that facilitate the in-context learning. In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token n
    
[^13]: 改变和组合奖励以对齐大规模语言模型

    Transforming and Combining Rewards for Aligning Large Language Models

    [https://arxiv.org/abs/2402.00742](https://arxiv.org/abs/2402.00742)

    本研究主要研究了对齐大规模语言模型的方法中出现的两个问题：奖励模型的选择以及多个奖励模型的组合。通过引入概率解释，我们提出了一种从Bradley-Terry偏好模型中学习的奖励的自然变换选择，该变换强调改善表现不佳的输出，从而减轻了欠拟合和奖励欺骗。

    

    将语言模型与人类偏好对齐的常见方法是首先从偏好数据中学习奖励模型，然后使用该奖励模型来更新语言模型。我们研究了这种方法中出现的两个密切相关的问题。首先，奖励模型的任何单调变换都保持偏好排名；是否有一种比其他选择“更好”的选择？其次，我们经常希望将语言模型与多个特性对齐：我们如何组合多个奖励模型？通过对齐过程的概率解释，我们确定了从Bradley-Terry偏好模型学习的奖励（常见情况）的自然变换选择。这个派生的变换具有两个重要的属性。首先，它强调改进表现不佳的输出，而不是已经得分良好的输出。这既减轻了欠拟合（其中一些提示没有得到改进），又减少了奖励欺骗（模型学习利用错误指定）。

    A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model. We study two closely related problems that arise in this approach. First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others? Second, we often wish to align language models to multiple properties: how should we combine multiple reward models? Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models. This derived transformation has two important properties. First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well. This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of
    
[^14]: 用Transformer量化变分自编码器改进离散潜在空间中的语义控制

    Improving Semantic Control in Discrete Latent Spaces with Transformer Quantized Variational Autoencoders

    [https://arxiv.org/abs/2402.00723](https://arxiv.org/abs/2402.00723)

    本研究利用Transformer量化变分自编码器（VQVAEs）中离散潜在空间的可控性，提出了T5VQVAE模型，通过指导T5中的自注意机制实现更好的生成效果。实验证明，T5VQVAE在可控性和生成效果上优于现有模型。

    

    在自动变分编码器（VAE）的潜在空间中实现精确的语义控制对于NLP领域的下游任务非常重要，因为底层的生成机制可以更好地定位、解释和改进。然而，最近的研究在实现一致的结果方面存在困难，主要是由于变分瓶颈中不可避免的语义信息丢失以及解码机制的有限控制。为了克服这些挑战，我们研究了基于向量量化的变分自编码器（VQVAE）中的离散潜在空间，以改进Transformer-based VAEs中的语义控制和生成能力。具体而言，我们提出了一种新颖的模型T5VQVAE，利用VQVAE的可控性来指导T5中的自注意机制，以实现令人满意的生成效果。实验结果表明，在可控性和生成效果方面，T5VQVAE优于现有的VAE模型，包括Optimus。

    Achieving precise semantic control over the latent spaces of Variational AutoEncoders (VAEs) holds significant value for downstream tasks in NLP as the underlying generative mechanisms could be better localised, explained and improved upon. Recent research, however, has struggled to achieve consistent results, primarily due to the inevitable loss of semantic information in the variational bottleneck and limited control over the decoding mechanism. To overcome these challenges, we investigate discrete latent spaces in Vector Quantized Variational AutoEncoders (VQVAEs) to improve semantic control and generation in Transformer-based VAEs. In particular, We propose T5VQVAE, a novel model that leverages the controllability of VQVAEs to guide the self-attention mechanism in T5 at the token-level, exploiting its full generalization capabilities. Experimental results indicate that T5VQVAE outperforms existing state-of-the-art VAE models, including Optimus, in terms of controllability and prese
    
[^15]: 使用反事实表示解释文本分类器

    Explaining Text Classifiers with Counterfactual Representations

    [https://arxiv.org/abs/2402.00711](https://arxiv.org/abs/2402.00711)

    本论文提出了一种使用反事实表示解释文本分类器的方法，通过干预文本表示来生成反事实，并通过实验证实了方法的有效性。

    

    一种基于反事实的解释方法可以为分类器提供合理的解释，其中反事实是指除了一个分类特征之外，与真实观察完全相同的假设事件。然而，在文本领域构建这种反事实存在特定挑战，因为某些属性值可能与现实世界的事件不一致。在这篇论文中，我们提出了一种简单的方法，通过对文本表示进行干预来生成反事实，从而克服了这个限制。我们认为我们的干预方法是最小程度的干扰，并且在理论上是可靠的，因为它们与Pearl的因果推断框架中定义的反事实是一致的。为了验证我们的方法，我们首先在合成数据集上进行实验，比较了基于真实反事实（通过明确的文本干预获得）和我们的反事实（通过对文本表示的干预得到）的分类器预测。

    One well motivated explanation method for classifiers leverages counterfactuals which are hypothetical events identical to real observations in all aspects except for one categorical feature. Constructing such counterfactual poses specific challenges for texts, however, as some attribute values may not necessarily align with plausible real-world events. In this paper we propose a simple method for generating counterfactuals by intervening in the space of text representations which bypasses this limitation. We argue that our interventions are minimally disruptive and that they are theoretically sound as they align with counterfactuals as defined in Pearl's causal inference framework. To validate our method, we first conduct experiments on a synthetic dataset of counterfactuals, allowing for a direct comparison between classifier predictions based on ground truth counterfactuals (obtained through explicit text interventions) and our counterfactuals, derived through interventions in the r
    
[^16]: 非交换的共形语言生成与最近邻

    Non-Exchangeable Conformal Language Generation with Nearest Neighbors

    [https://arxiv.org/abs/2402.00707](https://arxiv.org/abs/2402.00707)

    本文介绍了一种利用最近邻方法扩展的非交换的共形语言生成框架，用于量化自动生成文本的不确定性，并提供带有统计保证的预测集。

    

    在自动生成的文本中量化不确定性对于让人们检查潜在的错觉和使系统更可靠是很重要的。共形预测是一个有吸引力的框架，能够提供带有统计保证的预测，然而，将其应用于文本生成是具有挑战性的，因为任何独立同分布的假设都是不现实的。在本文中，我们通过利用最近关于非交换的共形预测的结果来填补这一差距，该方法仍然确保覆盖范围。结果--非交换的共形核采样，是对基于最近邻的生成的共形预测框架的一种新颖扩展。我们的方法可以用于任意模型的事后处理，无需额外训练，并提供带有统计保证的标记级别、校准的预测集。在机器翻译和语言建模的实验中，我们展示了令人鼓舞的生成质量结果。通过同时产生具有良好覆盖度的更紧密的预测集，

    Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable. Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic. In this paper, we bridge this gap by leveraging recent results on non-exchangeable conformal prediction, which still ensures bounds on coverage. The result, non-exchangeable conformal nucleus sampling, is a novel extension of the conformal prediction framework to generation based on nearest neighbors. Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees. Experiments in machine translation and language modeling show encouraging results in generation quality. By also producing tighter prediction sets with good cove
    
[^17]: 通过可扩展监督和集成学习提高弱到强泛化能力的研究

    Improving Weak-to-Strong Generalization with Scalable Oversight and Ensemble Learning

    [https://arxiv.org/abs/2402.00667](https://arxiv.org/abs/2402.00667)

    本文通过结合可扩展监督和集成学习的方式，在弱教师和强学生之间减小了能力差距，提高了弱到强泛化能力的框架下的泛化效果。

    

    本文对OpenAI最新的关于弱到强泛化能力的超对齐工作进行了后续研究。超对齐工作旨在确保高级AI系统在处理复杂、高风险任务时与人类的价值和意图保持一致。弱到强泛化能力的框架为这一不断发展的领域开拓了新的研究可能性。我们的研究模拟了W2SG框架下的两个超对齐阶段：通用超人类模型的发展和朝着超级智能的进步。在第一阶段中，通过可扩展监督和集成学习的组合，通过人类监督提高了弱监督的质量，减小了弱教师和强学生之间的能力差距。在第二阶段中，利用自动对齐评估器作为弱监督者。通过递归更新该自动对齐器，同步增强了弱教师模型的能力，实现了从弱到强的超对齐。

    This paper presents a follow-up study to OpenAI's recent superalignment work on Weak-to-Strong Generalization (W2SG). Superalignment focuses on ensuring that high-level AI systems remain consistent with human values and intentions when dealing with complex, high-risk tasks. The W2SG framework has opened new possibilities for empirical research in this evolving field. Our study simulates two phases of superalignment under the W2SG framework: the development of general superhuman models and the progression towards superintelligence. In the first phase, based on human supervision, the quality of weak supervision is enhanced through a combination of scalable oversight and ensemble learning, reducing the capability gap between weak teachers and strong students. In the second phase, an automatic alignment evaluator is employed as the weak supervisor. By recursively updating this auto aligner, the capabilities of the weak teacher models are synchronously enhanced, achieving weak-to-strong sup
    
[^18]: 通过收集轨迹和合成奖励来学习基于规划的推理

    Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing

    [https://arxiv.org/abs/2402.00658](https://arxiv.org/abs/2402.00658)

    本文提出了一种通过直接偏好优化在收集到的轨迹上学习基于规划的推理的框架，以解决大型语言模型在复杂推理任务中的虚幻和缺陷问题。

    

    大型语言模型（LLM）通过逐步合理化生成，展示了处理复杂推理任务的重要潜力。然而，最近的研究对它们的推理过程中的虚幻和缺陷提出了担忧。为了提高生成合理化的可靠性和忠实性，正在进行大量工作。有些方法将推理建模为规划，而其他方法则专注于注释的过程监督。然而，基于规划的搜索过程往往由于频繁评估中间推理状态和广泛的探索空间而导致高延迟。此外，使用人工注释监督推理过程对于LLM训练来说是昂贵且具有挑战性的。为了解决这些问题，我们在本文中提出了一种通过直接偏好优化（DPO）来学习基于规划的推理的框架，其中轨迹直接根据合成的过程奖励进行排名。

    Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through direct preference optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. 
    
[^19]: 韩语WH短语中串联和直接语音-文本翻译中的韵律

    Prosody in Cascade and Direct Speech-to-Text Translation: a case study on Korean Wh-Phrases

    [https://arxiv.org/abs/2402.00632](https://arxiv.org/abs/2402.00632)

    本研究通过对比评估方法，准确衡量了直接语音-文本翻译系统在消除韵律对话中的歧义方面的能力，并通过在韩语-英语翻译系统上的实验结果，明确展示了直接翻译系统的价值。

    

    语音-文本翻译（S2TT）通常使用串联系统进行处理，其中语音识别系统生成一个转录，随后传递给翻译模型。尽管在开发直接语音翻译系统以避免传播错误和丢失非语言内容方面越来越感兴趣，但之前的直接S2TT工作难以确定地证明将声学信号直接整合到翻译过程中的优势。本研究提出使用对比评估以定量衡量直接S2TT系统在消除韵律对话中的歧义方面的能力。具体而言，我们在一个包含WH短语的测试集上评估了韩语-英语翻译系统，对于该测试集来说，使用韵律特征是产生具有正确意图的翻译的必要条件，无论是陈述句、是非问句还是疑问句等。我们的结果清楚地证明了直接翻译系统的价值。

    Speech-to-Text Translation (S2TT) has typically been addressed with cascade systems, where speech recognition systems generate a transcription that is subsequently passed to a translation model. While there has been a growing interest in developing direct speech translation systems to avoid propagating errors and losing non-verbal content, prior work in direct S2TT has struggled to conclusively establish the advantages of integrating the acoustic signal directly into the translation process. This work proposes using contrastive evaluation to quantitatively measure the ability of direct S2TT systems to disambiguate utterances where prosody plays a crucial role. Specifically, we evaluated Korean-English translation systems on a test set containing wh-phrases, for which prosodic features are necessary to produce translations with the correct intent, whether it's a statement, a yes/no question, a wh-question, and more. Our results clearly demonstrate the value of direct translation systems
    
[^20]: 论述中的角色识别：LLMs的挑战？

    Actor Identification in Discourse: A Challenge for LLMs?

    [https://arxiv.org/abs/2402.00620](https://arxiv.org/abs/2402.00620)

    论文探讨了在辩论中识别政治角色的挑战，使用了传统的NLP组件与LLM进行比较，并发现LLM在生成正确的正式形式方面存在困难。作者提出了将LLM与分类器结合的混合模型来改善性能。

    

    在构建辩论网络并分析社会辩论时，识别政治角色提出的主张是一个关键步骤。然而，角色识别是非常具有挑战性的：通常，主张的本地提及人只是一个代词（“他建议[主张]”），因此恢复出正式的角色名称需要进行辩论理解。我们将一个传统的专用NLP组件流水线（类似于相关指称任务中应用的组件）与一个LLM进行比较，LLM似乎很适合这个生成任务。通过在德国报纸报道的角色语料库上进行评估，我们发现LLM表现出乎意料地较差。进一步的分析揭示LLM在识别正确引用方面非常好，但在生成正确的正式形式方面遇到了困难。这指向了LLMs在控制生成输出方面存在的潜在问题。事实上，将LLM与分类器结合起来进行规范化的混合模型能改善LLM的性能。

    The identification of political actors who put forward claims in public debate is a crucial step in the construction of discourse networks, which are helpful to analyze societal debates. Actor identification is, however, rather challenging: Often, the locally mentioned speaker of a claim is only a pronoun ("He proposed that [claim]"), so recovering the canonical actor name requires discourse understanding. We compare a traditional pipeline of dedicated NLP components (similar to those applied to the related task of coreference) with a LLM, which appears a good match for this generation task. Evaluating on a corpus of German actors in newspaper reports, we find surprisingly that the LLM performs worse. Further analysis reveals that the LLM is very good at identifying the right reference, but struggles to generate the correct canonical form. This points to an underlying issue in LLMs with controlling generated output. Indeed, a hybrid model combining the LLM with a classifier to normaliz
    
[^21]: 一条思维链条的强度取决于最弱的环节：一个验证推理链的基准

    A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains

    [https://arxiv.org/abs/2402.00559](https://arxiv.org/abs/2402.00559)

    本论文提出了Reveal数据集，用于在开放领域的问答设置中对复杂思维链的自动验证进行基准测试。这个数据集包含了详尽的标签，用于评估语言模型的答案中每个推理步骤的相关性、归因和逻辑正确性。

    

    促使语言模型提供逐步回答（例如“思维链”）是复杂推理任务的主要方法，其中更准确的推理链通常可以提高下游任务的性能。最近的文献讨论了自动验证推理步骤的方法，以评估和改善其正确性。然而，缺乏细粒度的步骤级数据集，无法对这类验证方法进行全面评估，从而阻碍了在这个方向上的进展。我们介绍了Reveal：推理验证评估，这是一个新的数据集，用于在开放领域的问答设置中对复杂思维链的自动验证进行基准测试。Reveal包括对语言模型答案中每个推理步骤的相关性、归因于证据段落以及逻辑正确性的全面标签，涵盖了各种数据集和最先进的语言模型。

    Prompting language models to provide step-by-step answers (e.g., "Chain-of-Thought") is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning steps to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce Reveal: Reasoning Verification Evaluation, a new dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question answering settings. Reveal includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model's answer, across a wide variety of datasets and state-of-the-art language models.
    
[^22]: 超级过滤：用于快速指令调整的弱到强数据过滤

    Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning

    [https://arxiv.org/abs/2402.00530](https://arxiv.org/abs/2402.00530)

    提出了一种名为“超级过滤”的方法，能够使用较小和较弱的模型对用于训练较大和较强的语言模型的指令数据进行过滤，从而降低了过滤成本，并在标准基准测试中获得更好的性能。

    

    指令调整对于改进LLM至关重要，但通常会遇到低质量和冗余数据的问题。指令调整的数据过滤在提高调整过程的效率和性能方面已被证明很重要。但是，由于LLMs在该过程中的参与，这也导致了额外的成本和计算。为了减少过滤成本，我们研究了超级过滤：可以使用较小且较弱的模型来选择要调整更大和更强模型的数据吗？尽管弱和强语言模型之间存在性能差距，但我们发现它们高度一致的能力可以感知指令的难度和数据选择结果。这使得我们可以使用一个更小更高效的模型来过滤用于训练更大语言模型的指令数据。它不仅大大加快了数据过滤的速度，而且经过过滤数据微调的LLM在标准基准测试中取得了更好的性能。大量实验证实了其有效性和效率。

    Instruction tuning is critical to improve LLMs but usually suffers from low-quality and redundant data. Data filtering for instruction tuning has proved important in improving both the efficiency and performance of the tuning process. But it also leads to extra cost and computation due to the involvement of LLMs in this process. To reduce the filtering cost, we study Superfiltering: Can we use a smaller and weaker model to select data for finetuning a larger and stronger model? Despite the performance gap between weak and strong language models, we find their highly consistent capability to perceive instruction difficulty and data selection results. This enables us to use a much smaller and more efficient model to filter the instruction data used to train a larger language model. Not only does it largely speed up the data filtering, but the filtered-data-finetuned LLM achieves even better performance on standard benchmarks. Extensive experiments validate the efficacy and efficiency of 
    
[^23]: EE-Tuning:一种经济且可扩展的调整早期终止大型语言模型的解决方案

    EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models

    [https://arxiv.org/abs/2402.00518](https://arxiv.org/abs/2402.00518)

    该论文介绍了一种经济且可扩展的解决方案EE-Tuning，可以使用较少的计算资源和训练数据针对早期终止大型语言模型进行调整，通过性能优化和3D并行性实现卓越的训练效率。实验证实，即使在有限的训练预算下，也可以实现有效的早期终止LLM推理。

    

    本文介绍了EE-Tuning，一种轻量且经济实用的解决方案，可以训练/调整早期终止的大型语言模型（LLMs）。与完整参数的预训练常见方法不同，EE-Tuning通过在参数高效方式下增加额外的早期终止层，与任何预训练（可能是微调）的标准LLM相结合，从而大大降低了计算资源和训练数据的需求。我们通过广泛的性能优化和完全兼容3D并行性的可扩展性，实现了EE-Tuning的卓越训练效率。系统实验证实了EE-Tuning的有效性，证明了在有限的训练预算下可以实现有效的早期终止LLM推理。为了将早期终止LLMs推广到社区，我们在https://github.com/pan-x-c/EE-LLM上发布了EE-Tuning的源代码。

    This work introduces EE-Tuning, a lightweight and economical solution to training/tuning early-exit large language models (LLMs). In contrast to the common approach of full-parameter pre-training, EE-Tuning augments any pre-trained (and possibly fine-tuned) standard LLM with additional early-exit layers that are tuned in a parameter-efficient manner, which requires significantly less computational resources and training data. Our implementation of EE-Tuning achieves outstanding training efficiency via extensive performance optimizations, as well as scalability due to its full compatibility with 3D parallelism. Results of systematic experiments validate the efficacy of EE-Tuning, confirming that effective early-exit LLM inference can be achieved with a limited training budget. In hope of making early-exit LLMs accessible to the community, we release the source code of our implementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.
    
[^24]: SA-MDKIF：一种可扩展和适应性强的大型语言模型医学领域知识注入框架

    SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models

    [https://arxiv.org/abs/2402.00474](https://arxiv.org/abs/2402.00474)

    SA-MDKIF是一种可扩展和适应性强的医学领域知识注入框架，通过指令调整并训练医学技能，并在推理中将其与大型语言模型集成，提高了医学任务的性能。

    

    大型语言模型(LLMs)的最新进展在各种自然语言处理(NLP)任务中展现出了卓越的性能。然而，它们在医学领域的有效应用受到医学领域知识的缺乏的限制。在本研究中，我们提出了一种可扩展和适应性强的框架SA-MDKIF，旨在通过指令调整将医学知识注入通用型LLMs中，从而实现对各种下游任务的适应性。SA-MDKIF包括两个阶段：技能训练和技能适应。在第一阶段，我们定义了12种基本的医学技能，并使用AdaLoRA根据我们构建的统一格式的指令数据集来训练这些技能。在下一个阶段，我们使用特定任务的下游数据来训练技能路由器，并在推理过程中使用该路由器将获取的技能与LLMs集成。对9个不同的医学任务的实验结果显示，与原始模型相比，SA-MDKIF的性能提高了10-20％。

    Recent advances in large language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, their effective application in the medical domain is hampered by a lack of medical domain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable framework that aims to inject medical knowledge into general-purpose LLMs through instruction tuning, thereby enabling adaptability for various downstream tasks. SA-MDKIF consists of two stages: skill training and skill adaptation. In the first stage, we define 12 basic medical skills and use AdaLoRA to train these skills based on uniformly formatted instructional datasets that we have constructed. In the next stage, we train the skill router using task-specific downstream data and use this router to integrate the acquired skills with LLMs during inference. Experimental results on 9 different medical tasks show that SA-MDKIF improves performance by 10-20% compared to the origina
    
[^25]: 指令的差异

    Instruction Makes a Difference

    [https://arxiv.org/abs/2402.00453](https://arxiv.org/abs/2402.00453)

    通过引入指令数据集，我们展示了在文档分析和文档图像预测中训练语言-视觉模型的重要性，并表明使用指令数据集可以提高性能。使用Polling-based Object Probing Evaluation (POPE)数据集进行评估，我们发现指令调优性能相对于零-shot性能提高了11倍到32倍，并且相对于非指令微调提高了0.1%到4.2%。尽管如此，仍有很大的改进空间，因为这些性能仍未达到人类水平（94.36%）。

    

    我们介绍了Instruction Document Visual Question Answering (iDocVQA)数据集和Large Language Document (LLaDoc)模型，用于训练语言-视觉（LV）模型进行文档分析和文档图像预测。通常，用于DocVQA任务的深度神经网络是在缺乏指令的数据集上进行训练的。我们表明使用遵循指令的数据集可以提高性能。我们使用最新的Large Language and Vision Assistant (LLaVA)1.5作为基础模型，比较了不同文档相关数据集的性能。我们还使用基于投票的对象探测评估（POPE）数据集评估了导出模型的对象幻觉性能。结果表明，指令调优性能相对于零-shot性能提高了11倍到32倍，并且相对于非指令（传统任务）微调提高了0.1%到4.2%。尽管取得了这些进展，但仍然达不到人类性能（94.36%），这意味着有很大的改进空间。

    We introduce Instruction Document Visual Question Answering (iDocVQA) dataset and Large Language Document (LLaDoc) model, for training Language-Vision (LV) models for document analysis and predictions on document images, respectively. Usually, deep neural networks for the DocVQA task are trained on datasets lacking instructions. We show that using instruction-following datasets improves performance. We compare performance across document-related datasets using the recent state-of-the-art (SotA) Large Language and Vision Assistant (LLaVA)1.5 as the base model. We also evaluate the performance of the derived models for object hallucination using the Polling-based Object Probing Evaluation (POPE) dataset. The results show that instruction-tuning performance ranges from 11X to 32X of zero-shot performance and from 0.1% to 4.2% over non-instruction (traditional task) finetuning. Despite the gains, these still fall short of human performance (94.36%), implying there's much room for improveme
    
[^26]: 使用具有社会意识的对比学习来提高对话安全性

    Improving Dialog Safety using Socially Aware Contrastive Learning

    [https://arxiv.org/abs/2402.00446](https://arxiv.org/abs/2402.00446)

    本研究提出了使用具有社会意识的对比学习来提高对话安全性的方法。通过研究对抗和随意对话背景下的亲社会性，我们发现现有的模型很难识别自然对话中微妙的不安全情况。为了解决这个问题，我们采用了一个双步骤的微调过程，并通过整合亲社会行为的数据集来训练基础模型。

    

    最先进的对话型人工智能系统由于可能产生不安全、有毒、不道德或危险内容的潜在风险而引起关注。之前的研究已经开发了数据集，教会对话代理人适当地回应特定设计的危险内容的社交范式。然而，这些对抗性数据集上训练的模型仍然很难识别自然出现在对话中的微妙的不安全情况，或在随意环境中引入不恰当的回应。为了了解这个问题的程度，我们研究了对抗和随意对话背景下的亲社会性，并在是否产生不安全内容的倾向方面审查了通用语言模型的响应质量。我们提出了一个双步骤的微调过程，使用具有社会意识的n对对比损失来解决这些问题。随后，我们训练了一个基础模型，通过利用道德完整语料库（MIC）等数据集来整合亲社会行为。

    State-of-the-art conversational AI systems raise concerns due to their potential risks of generating unsafe, toxic, unethical, or dangerous content. Previous works have developed datasets to teach conversational agents the appropriate social paradigms to respond effectively to specifically designed hazardous content. However, models trained on these adversarial datasets still struggle to recognize subtle unsafe situations that appear naturally in conversations or introduce an inappropriate response in a casual context. To understand the extent of this problem, we study prosociality in both adversarial and casual dialog contexts and audit the response quality of general-purpose language models in terms of propensity to produce unsafe content. We propose a dual-step fine-tuning process to address these issues using a socially aware n-pair contrastive loss. Subsequently, we train a base model that integrates prosocial behavior by leveraging datasets like Moral Integrity Corpus (MIC) and P
    
[^27]: 从PARIS到LE-PARIS：通过推荐系统和协作大型语言模型实现专利响应自动化

    From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models

    [https://arxiv.org/abs/2402.00421](https://arxiv.org/abs/2402.00421)

    本研究介绍了专利响应智能系统PARIS和LE-PARIS，通过构建OA主题数据库、开发响应模板以及实施推荐系统和基于LLM的响应生成，旨在加快专利律师处理审查意见回应的效率。 通过多范式分析和长期数据验证，证明了OA主题的建设性和LLM对于回应自动生成的可行性。

    

    在专利审查中，对于及时和有效地回应审查意见（OAs）对于获得专利至关重要，然而过去的自动化和人工智能研究很少涉及到这一方面。为了弥补这一空白，我们的研究介绍了专利审查意见响应智能系统（PARIS）及其先进版本LE-PARIS。这些系统旨在加快专利律师在协作处理OA回应方面的效率。系统的关键特征包括构建OA主题数据库，开发响应模板，以及实施推荐系统和基于LLM的响应生成。我们的验证涉及使用USPTO Office Action数据库和律师与我们系统的长期交互数据进行的多范式分析，为期六年。通过五个研究，我们利用主题建模和提出的Delphi过程来检验OA主题的建设性（研究1和2），还有使用推荐系统和基于LLM的响应生成来提高回应质量（研究3和4），以及经过训练的LLM对于回应自动生成的可行性（研究5）。

    In patent prosecution, timely and effective responses to Office Actions (OAs) are crucial for acquiring patents, yet past automation and AI research have scarcely addressed this aspect. To address this gap, our study introduces the Patent Office Action Response Intelligence System (PARIS) and its advanced version, the Large Language Model Enhanced PARIS (LE-PARIS). These systems are designed to expedite the efficiency of patent attorneys in collaboratively handling OA responses. The systems' key features include the construction of an OA Topics Database, development of Response Templates, and implementation of Recommender Systems and LLM-based Response Generation. Our validation involves a multi-paradigmatic analysis using the USPTO Office Action database and longitudinal data of attorney interactions with our systems over six years. Through five studies, we examine the constructiveness of OA topics (studies 1 and 2) using topic modeling and the proposed Delphi process, the efficacy of
    
[^28]: 使用大型语言模型实现基于提示时间的符号知识捕获

    Prompt-Time Symbolic Knowledge Capture with Large Language Models

    [https://arxiv.org/abs/2402.00414](https://arxiv.org/abs/2402.00414)

    本文研究利用大型语言模型来实现基于提示驱动的知识捕获，特别关注提示到三元组的生成，并通过专门的合成数据集评估了三种方法的性能。

    

    为了实际应用，如个人AI助手，将大型语言模型（LLMs）与用户特定的知识相结合至关重要。然而，LLMs本身缺乏基于提示驱动的知识捕获机制。本文研究利用现有的LLMs能力实现基于提示驱动的知识捕获，特别关注知识图谱。我们通过关注提示到三元组（P2T）生成来应对这个挑战。我们探索了零射距、少射距和微调三种方法，并通过一个专门的合成数据集对它们的性能进行评估。我们的代码和数据集可以在https://github.com/HaltiaAI/paper-PTSKC上公开获取。

    Augmenting large language models (LLMs) with user-specific knowledge is crucial for real-world applications, such as personal AI assistants. However, LLMs inherently lack mechanisms for prompt-driven knowledge capture. This paper investigates utilizing the existing LLM capabilities to enable prompt-driven knowledge capture, with a particular emphasis on knowledge graphs. We address this challenge by focusing on prompt-to-triple (P2T) generation. We explore three methods: zero-shot prompting, few-shot prompting, and fine-tuning, and then assess their performance via a specialized synthetic dataset. Our code and datasets are publicly available at https://github.com/HaltiaAI/paper-PTSKC.
    
[^29]: 隐藏代笔者：对AI生成的学生论文检测进行对抗评估

    Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection

    [https://arxiv.org/abs/2402.00412](https://arxiv.org/abs/2402.00412)

    本文旨在对抗评估AI生成的学生论文检测，通过构建AIG-ASAP数据集和使用文本扰动方法，揭示现有检测器易被自动对抗攻击所绕过的问题。

    

    大型语言模型(LLM)在文本生成任务中展示了出色的能力。然而，利用这些模型存在固有的风险，包括但不限于抄袭、传播假新闻以及教育习题中的问题。尽管已有几种检测器被提出来解决这些问题，但它们在对抗扰动方面的效果，特别是在学生论文写作的背景下，仍然被较少探索。本文旨在填补这一空白，通过构建AIG-ASAP，一个基于AI生成的学生论文数据集，采用一系列预计能生成高质量论文的文本扰动方法，同时躲避检测。通过实证实验，我们评估了当前AIGC检测器在AIG-ASAP数据集上的性能。结果表明，现有的检测器可以很容易地被直接的自动对抗攻击所规避。具体来说，我们探索了词替换和句子替换方法。

    Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks. However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises. Although several detectors have been proposed to address these concerns, their effectiveness against adversarial perturbations, specifically in the context of student essay writing, remains largely unexplored. This paper aims to bridge this gap by constructing AIG-ASAP, an AI-generated student essay dataset, employing a range of text perturbation methods that are expected to generate high-quality essays while evading detection. Through empirical experiments, we assess the performance of current AIGC detectors on the AIG-ASAP dataset. The results reveal that the existing detectors can be easily circumvented using straightforward automatic adversarial attacks. Specifically, we explore word substitution and sentence substitu
    
[^30]: 通过激活导向研究 Llama 2 Chat 中的偏见表示

    Investigating Bias Representations in Llama 2 Chat via Activation Steering

    [https://arxiv.org/abs/2402.00402](https://arxiv.org/abs/2402.00402)

    我们通过激活导向技术研究了Llama 2 Chat中的偏见表示问题，发现该模型存在固有的性别偏见，并观察到偏见与模型拒绝回应的倾向之间存在负相关关系。

    

    我们解决了大型语言模型（LLMs）中的社会偏见挑战，重点关注 Llama 2 7B Chat 模型。随着 LLMs 被越来越多地整合到具有重大社会影响的决策过程中，确保这些模型不会强化现有的偏见变得至关重要。我们的方法采用激活导向技术来探测和减轻与性别、种族和宗教有关的偏见。该方法通过操纵模型的激活来指导回应朝向或远离有偏见的输出，利用从StereoSet数据集和自定义GPT4生成的性别偏见提示得到的导向向量。我们的研究结果揭示了Llama 2 7B Chat中固有的性别偏见，即使在通过人类反馈的强化学习之后仍然存在。我们还观察到偏见与模型拒绝回应的倾向之间存在可预测的负相关关系。值得注意的是，我们的研究揭示了强化学习从人类反馈中 tend 趋向增加模型对不同形式社会偏见的表示的相似性。

    We address the challenge of societal bias in Large Language Models (LLMs), focusing on the Llama 2 7B Chat model. As LLMs are increasingly integrated into decision-making processes with substantial societal impact, it becomes imperative to ensure these models do not reinforce existing biases. Our approach employs activation steering to probe for and mitigate biases related to gender, race, and religion. This method manipulates model activations to direct responses towards or away from biased outputs, utilizing steering vectors derived from the StereoSet dataset and custom GPT4 generated gender bias prompts. Our findings reveal inherent gender bias in Llama 2 7B Chat, persisting even after Reinforcement Learning from Human Feedback (RLHF). We also observe a predictable negative correlation between bias and the model's tendency to refuse responses. Significantly, our study uncovers that RLHF tends to increase the similarity in the model's representation of different forms of societal bia
    
[^31]: LLMs的高效探索

    Efficient Exploration for LLMs

    [https://arxiv.org/abs/2402.00396](https://arxiv.org/abs/2402.00396)

    高效探索在改善大型语言模型方面具有显著优势，可以以较少的查询实现较高的性能水平。不确定性估计和探索方案的选择是关键因素。

    

    我们提供了证据，表明高效探索在获取人类反馈以改善大型语言模型方面具有显著优势。在我们的实验中，一个代理程序在收到反馈时将奖励模型拟合到查询上。我们表现最佳的代理程序使用双Thompson采样生成查询，不确定性由认知神经网络表示。我们的结果表明，高效探索使得性能水平可以在较少的查询下达到较高水平。此外，不确定性估计和探索方案的选择起着关键作用。

    We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.
    
[^32]: 计算形态学和词典学模型对现代标准阿拉伯语名词的建模

    Computational Morphology and Lexicography Modeling of Modern Standard Arabic Nominals

    [https://arxiv.org/abs/2402.00385](https://arxiv.org/abs/2402.00385)

    本研究通过计算形态学和词典学模型成功解决了现代标准阿拉伯语名词的形态和词汇建模挑战，提供了全面和可扩展的模型，并在准确性和一致性上显著优于常用的分析器和生成器。

    

    现代标准阿拉伯语名词存在许多形态和词汇建模的挑战，但之前尚未得到一致的解决。本文试图定义这些挑战，并利用最近提出的形态学框架构建一个全面且可扩展的现代标准阿拉伯语名词模型。我们的模型设计解决了名词复杂的形态规则以及其范式的不规则性。与常用的现代标准阿拉伯语形态分析器和生成器相比，我们的实现展示了更高的准确性和一致性。我们将我们的模型公开提供。

    Modern Standard Arabic (MSA) nominals present many morphological and lexical modeling challenges that have not been consistently addressed previously. This paper attempts to define the space of such challenges, and leverage a recently proposed morphological framework to build a comprehensive and extensible model for MSA nominals. Our model design addresses the nominals' intricate morphotactics, as well as their paradigmatic irregularities. Our implementation showcases enhanced accuracy and consistency compared to a commonly used MSA morphological analyzer and generator. We make our models publicly available.
    
[^33]: 机器人在社交媒体中的检测中，大规模语言模型的机会与风险。

    What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection

    [https://arxiv.org/abs/2402.00371](https://arxiv.org/abs/2402.00371)

    本文研究了大语言模型（LLMs）在社交媒体机器人检测中的机遇和风险。通过提出混合异质专家框架，我们设计了新颖的LLM机器人检测器，并发现仅使用少量标注示例进行指导调整即可取得超过最先进基线模型的性能提升。然而，LLM引导的操纵策略可能会显著降低现有机器人检测的性能。

    

    社交媒体机器人检测一直是机器学习机器人检测器和对抗机器人策略之间的一场军备竞赛。在这项工作中，我们通过研究最新的大规模语言模型（LLM）在社交机器人检测中的机会和风险，将这场军备竞赛提升到了一个新的水平。为了探索机会，我们设计了基于LLM的新颖机器人检测器，提出了一种混合异质专家框架，对不同的用户信息模态进行划分和征服。为了揭示风险，我们探讨了通过LLM引导用户文本和结构化信息操纵来逃避检测的可能性。在两个数据集上进行的大量实验证明，仅对1,000个注释示例进行指导调整就可以产生专业的LLM，其在两个数据集上的表现超过最先进的基线模型高达9.1%，而LLM引导的操纵策略可以显著降低现有机器人检测的性能。

    Social media bot detection has always been an arms race between advancements in machine learning bot detectors and adversarial bot strategies to evade detection. In this work, we bring the arms race to the next level by investigating the opportunities and risks of state-of-the-art large language models (LLMs) in social bot detection. To investigate the opportunities, we design novel LLM-based bot detectors by proposing a mixture-of-heterogeneous-experts framework to divide and conquer diverse user information modalities. To illuminate the risks, we explore the possibility of LLM-guided manipulation of user textual and structured information to evade detection. Extensive experiments with three LLMs on two datasets demonstrate that instruction tuning on merely 1,000 annotated examples produces specialized LLMs that outperform state-of-the-art baselines by up to 9.1% on both datasets, while LLM-guided manipulation strategies could significantly bring down the performance of existing bot d
    
[^34]: 不要幻觉，持观：通过多LLM协作识别LLM知识盲区

    Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration

    [https://arxiv.org/abs/2402.00367](https://arxiv.org/abs/2402.00367)

    本论文研究了识别大型语言模型（LLM）知识盲区的方法，并提出了两种基于LLM协作的新方法，通过这些方法可以在面对知识盲区时放弃回答问题。实验证明，这些方法在提高放弃准确度方面取得了高达19.3％的改进。

    

    尽管存在扩展大型语言模型（LLM）知识的努力，但由于知识的不断演化，LLM知识盲区——LLM中缺失或过时的信息可能会一直存在。在这项工作中，我们研究了识别LLM知识盲区和在存在知识盲区时放弃回答问题的方法。我们首先通过模型校准或适应的现有方法进行改进，并分析它们在避免生成低置信度输出方面的能力。受到它们在自我反思和过度依赖保留集方面的失败的启发，我们提出了两种基于模型协作的新方法，即LLM探测其他LLM的知识盲区，无论是合作还是竞争。通过在四个包含多样知识领域的问答任务上对三个LLM进行广泛实验，我们证明揭示LLM知识盲区的合作和竞争方法在放弃准确度方面取得了高达19.3％的提高。

    Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the s
    
[^35]: IndiVec: 利用大型语言模型进行细粒度偏见指标的媒体偏见检测的探索

    IndiVec: An Exploration of Leveraging Large Language Models for Media Bias Detection with Fine-Grained Bias Indicators

    [https://arxiv.org/abs/2402.00345](https://arxiv.org/abs/2402.00345)

    这项研究引入了一个通用的偏见检测框架IndiVec，利用大型语言模型构建细粒度媒体偏见数据库，并通过多数投票确定输入的偏见标签。这一框架具有适应性和可解释性，在各种数据集上表现出一致的性能。

    

    本研究侧重于媒体偏见检测，在当今影响力巨大的社交媒体平台塑造个人态度和观点的时代，这是至关重要的。与主要依赖训练特定模型针对特定数据集的先前工作相比，这导致了有限的适应性和在领域外数据上的次优性能，我们引入了一个通用的偏见检测框架IndiVec，建立在大型语言模型上。IndiVec首先通过利用大型语言模型的强大指令遵循能力和向量数据库技术构建一个细粒度媒体偏见数据库。当面对新的输入进行偏见检测时，我们的框架会自动从向量数据库中选择最相关的指标，并采用多数投票确定输入的偏见标签。与以前的方法相比，IndiVec在适应性（在来自各种来源的不同数据集上展现出一致的性能）和可解释性（提供易于理解的解释）方面表现出色。

    This study focuses on media bias detection, crucial in today's era of influential social media platforms shaping individual attitudes and opinions. In contrast to prior work that primarily relies on training specific models tailored to particular datasets, resulting in limited adaptability and subpar performance on out-of-domain data, we introduce a general bias detection framework, IndiVec, built upon large language models. IndiVec begins by constructing a fine-grained media bias database, leveraging the robust instruction-following capabilities of large language models and vector database techniques. When confronted with new input for bias detection, our framework automatically selects the most relevant indicator from the vector database and employs majority voting to determine the input's bias label. IndiVec excels compared to previous methods due to its adaptability (demonstrating consistent performance across diverse datasets from various sources) and explainability (providing exp
    
[^36]: 从预训练到适应过程中的观点摘要偏见：政治偏见的案例研究

    Bias in Opinion Summarisation from Pre-training to Adaptation: A Case Study in Political Bias

    [https://arxiv.org/abs/2402.00322](https://arxiv.org/abs/2402.00322)

    本研究通过以政治偏见为例，量化了生成模型中的偏见，并发现大多数模型存在固有偏见。通过对比各种适应方法，在社交媒体观点摘要任务中发现，调整较少参数的模型相对于标准微调模型来说具有较少的偏见。

    

    观点摘要的目标是将产品评价、讨论论坛和社交媒体文本等文件中呈现的重要信息和意见总结成简洁的摘要，使用户能够有效地了解其中的意见。生成有偏见的摘要可能会影响公众舆论。先前的研究主要关注使用抽取模型研究观点摘要的偏见，但对于生成模型的研究有限。在本研究中，以政治偏见为案例，我们首先建立了一种量化生成模型偏见的方法，然后从预训练模型追溯到使用不同模型和适应方法进行社交媒体观点摘要的任务。我们发现大多数模型存在固有偏见。使用社交媒体文本摘要数据集，对比了各种适应方法，我们发现调整较少参数的模型相比标准的微调模型更加无偏见。

    Opinion summarisation aims to summarise the salient information and opinions presented in documents such as product reviews, discussion forums, and social media texts into short summaries that enable users to effectively understand the opinions therein. Generating biased summaries has the risk of potentially swaying public opinion. Previous studies focused on studying bias in opinion summarisation using extractive models, but limited research has paid attention to abstractive summarisation models. In this study, using political bias as a case study, we first establish a methodology to quantify bias in abstractive models, then trace it from the pre-trained models to the task of summarising social media opinions using different models and adaptation methods. We find that most models exhibit intrinsic bias. Using a social media text summarisation dataset and contrasting various adaptation methods, we find that tuning a smaller number of parameters is less biased compared to standard fine-
    
[^37]: 自然语言中等级-频率关系的一个关键参数

    A Crucial Parameter for Rank-Frequency Relation in Natural Languages

    [https://arxiv.org/abs/2402.00271](https://arxiv.org/abs/2402.00271)

    本研究发现自然语言中单词的等级-频率关系可以更准确地由公式$f\propto r^{-\alpha} \cdot (r+\gamma)^{-\beta}$来模拟，其中关键的参数是$\gamma$，它描述了词汇增长对语料库的影响。同时提出了一种通过搜索最优$\gamma$来估计参数的方法，并用案例研究论证了该公式和参数的适用性。

    

    与朴素的幂律$f\propto r^{-\alpha}$相比，$f \propto r^{-\alpha} \cdot (r+\gamma)^{-\beta}$在实证上更准确地模拟了自然语言中单词的等级-频率（r-f）关系。本研究表明，在这个公式中，唯一关键的参数是$\gamma$，它描述了词汇增长对语料库的抵抗力。还提出了通过搜索最优$\gamma$来进行参数估计的方法，其中技术上引入了一个“零词”用于计算。通过几个案例研究进一步讨论了公式和参数。

    $f \propto r^{-\alpha} \cdot (r+\gamma)^{-\beta}$ has been empirically shown more precise than a na\"ive power law $f\propto r^{-\alpha}$ to model the rank-frequency ($r$-$f$) relation of words in natural languages. This work shows that the only crucial parameter in the formulation is $\gamma$, which depicts the resistance to vocabulary growth on a corpus. A method of parameter estimation by searching an optimal $\gamma$ is proposed, where a ``zeroth word'' is introduced technically for the calculation. The formulation and parameters are further discussed with several case studies.
    
[^38]: DetectGPT是否充分利用了扰动？基于模型对比学习的选择性扰动会更好

    Does \textsc{DetectGPT} Fully Utilize Perturbation? Selective Perturbation on Model-Based Contrastive Learning Detector would be Better

    [https://arxiv.org/abs/2402.00263](https://arxiv.org/abs/2402.00263)

    我们提出了一种新颖的检测器，模型名，它使用选择性策略扰动和多对比学习，在减少随机屏蔽引起的信息丢失的同时，进一步提升少样本和个体输入的性能。

    

    大型语言模型的不断发展引发了对其滥用的增长关注。DetectGPT是一种零-shot基于度量的无监督机器生成文本检测器，首次引入了扰动并展现了巨大的性能提升。然而，DetectGPT的随机扰动策略可能会引入噪声，限制了可区分性和进一步的性能提升。此外，它的逻辑回归模块依赖于设置阈值，这会影响个体或小批量输入的泛化性和适用性。因此，我们提出了一种新颖的检测器，模型名，它使用选择性策略扰动来缓解随机屏蔽所引起的重要信息丢失，并利用多对比学习捕捉扰动期间的隐含模式信息，便于少量样本的性能提升。实验结果表明，模型名在四个公共数据集上的平均准确率比SOTA方法高出1.20\%。我们进一步分析了...

    The burgeoning capabilities of large language models (LLMs) have raised growing concerns about abuse. DetectGPT, a zero-shot metric-based unsupervised machine-generated text detector, first introduces perturbation and shows great performance improvement. However, DetectGPT's random perturbation strategy might introduce noise, limiting the distinguishability and further performance improvements. Moreover, its logit regression module relies on setting the threshold, which harms the generalizability and applicability of individual or small-batch inputs. Hence, we propose a novel detector, \modelname{}, which uses selective strategy perturbation to relieve the important information loss caused by random masking, and multi-pair contrastive learning to capture the implicit pattern information during perturbation, facilitating few-shot performance. The experiments show that \modelname{} outperforms the SOTA method by 1.20\% in accuracy on average on four public datasets. We further analyze th
    
[^39]: 大规模视觉-语言模型中的幻觉调查

    A Survey on Hallucination in Large Vision-Language Models

    [https://arxiv.org/abs/2402.00253](https://arxiv.org/abs/2402.00253)

    这份综合调查研究了大规模视觉-语言模型中的幻觉问题，澄清了幻觉的概念，提出了评估方法，并对幻觉的根本原因进行了调查。

    

    大规模视觉-语言模型（LVLMs）的发展引起了人工智能领域越来越多的关注，因为它具有实际的实施潜力。然而，“幻觉”，或者更具体地说，即视觉内容与相应文本生成之间的不一致，在利用LVLMs方面提出了重大挑战。在这份综合调查中，我们对LVLM相关的幻觉进行了深入剖析，旨在建立一个概览并促进未来的缓解。我们首先澄清了LVLMs中幻觉概念，呈现了各种幻觉症状，并强调了LVLM幻觉固有的独特挑战。随后，我们概述了专门用于评估LVLM独特幻觉的基准和方法论。此外，我们深入调查了这些幻觉的根本原因，包括来自训练数据和模型组件的见解。我们还对现有的幻觉缓解方法进行了批判性的回顾。

    Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review e
    
[^40]: 高效的非参数化不确定性量化方法用于黑盒大型语言模型和决策规划

    Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning

    [https://arxiv.org/abs/2402.00251](https://arxiv.org/abs/2402.00251)

    本文提出了一种高效的非参数化不确定性量化方法，用于黑盒大型语言模型和决策规划。该方法可以有效地估计输入-决策之间的逐点依赖关系，并提供统计上对决策可信度的解释。另外，本文还提出了一个系统化的决策代理设计，根据用户提示生成动作，并在存在多个高估计逐点依赖性的动作时要求用户提供偏好。

    

    大型语言模型(LLMs)的逐步决策规划在人工智能代理的发展中受到关注。本文主要研究带有不确定性估计的决策规划，以解决语言模型中的幻觉问题。现有方法要么是白盒方法，要么是计算复杂，限制了黑盒专有LLMs的使用。本文的第一个贡献是一种非参数化的LLMs不确定性量化方法，通过单一推理有效地估计输入-决策之间的逐点依赖关系，而不需要访问令牌logits。该估计器提供了对决策可信度的统计解释。第二个贡献是一个系统化的决策代理设计，根据用户提示如“打开浴室灯”，生成动作。当有多个动作的估计逐点依赖性都很高时，用户将被要求提供偏好。总结地说，我们的未参数化不确定性量化方法提供了一种高效的决策规划方法，可以在黑盒大型语言模型中应用。

    Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our un
    
[^41]: 探索仅基于解码器训练的模型在公共语音识别语料库上的极限

    Exploring the limits of decoder-only models trained on public speech recognition corpora

    [https://arxiv.org/abs/2402.00235](https://arxiv.org/abs/2402.00235)

    本研究探索了仅基于解码器训练的模型在公共语音识别语料库上的极限，开发了名为DOTA的模型，在大多数英语语音识别基准测试中优于其他开源模型，并公开了代码库和模型检查点。

    

    工业规模的语音识别模型（如Whisper和USM）的出现，这些模型分别训练于100万小时的弱标注数据和1200万小时的纯音频专有数据，导致了对大规模公共语音识别语料库和竞争性开源流程的更强需求。与上述模型不同，大型语言模型通常基于Transformer解码器，目前尚不清楚仅使用公共数据训练的解码器模型是否能够提供具有竞争力的性能。在这项工作中，我们研究了诸如训练数据集的选择和建模组件等因素，以获得仅使用公共英语语音识别语料库的最佳性能。我们的Decoder-Only Transformer for ASR (DOTA)模型在几乎所有英语语音识别基准测试中全面优于Whisper的编码器-解码器开源复制(OWSM)，并且在15个测试集中的7个中胜过Whisper large-v3。我们以宽松的许可证发布我们的代码库和模型检查点。

    The emergence of industrial-scale speech recognition (ASR) models such as Whisper and USM, trained on 1M hours of weakly labelled and 12M hours of audio only proprietary data respectively, has led to a stronger need for large scale public ASR corpora and competitive open source pipelines. Unlike the said models, large language models are typically based on Transformer decoders, and it remains unclear if decoder-only models trained on public data alone can deliver competitive performance. In this work, we investigate factors such as choice of training datasets and modeling components necessary for obtaining the best performance using public English ASR corpora alone. Our Decoder-Only Transformer for ASR (DOTA) model comprehensively outperforms the encoder-decoder open source replication of Whisper (OWSM) on nearly all English ASR benchmarks and outperforms Whisper large-v3 on 7 out of 15 test sets. We release our codebase and model checkpoints under permissive license.
    
[^42]: 生成式AI系统能否支持患者的信息需求？

    Are Generative AI systems Capable of Supporting Information Needs of Patients?

    [https://arxiv.org/abs/2402.00234](https://arxiv.org/abs/2402.00234)

    生成式AI系统被应用于支持患者信息需求的研究中，以提高患者对放射学数据的理解和管理能力。通过与患者和医疗专家的对话分析，我们确定了常见的医学信息需求和问题。

    

    患有复杂疾病如癌症的患者面临复杂的信息挑战，他们不仅需要了解他们的疾病，还需要学会如何管理它。与医疗专家（放射科医师、肿瘤科医师）密切互动可以提高患者的学习能力，从而改善疾病预后。然而，这种方法资源密集且占用了专家的时间，使他们无法完成其他关键任务。鉴于生成式AI模型在改进医疗系统方面的最新进展，我们的工作研究了生成式视觉问答系统在放射学成像数据背景下如何负责任地支持患者的信息需求。我们进行了一项形成性需求发现研究，参与者讨论了一个虚构近亲的胸部计算机断层扫描（CT）图像和相关的放射学报告。通过对参与者和医疗专家之间的对话的主题分析，我们确定常见的医学信息需求和问题。

    Patients managing a complex illness such as cancer face a complex information challenge where they not only must learn about their illness but also how to manage it. Close interaction with healthcare experts (radiologists, oncologists) can improve patient learning and thereby, their disease outcome. However, this approach is resource intensive and takes expert time away from other critical tasks. Given the recent advancements in Generative AI models aimed at improving the healthcare system, our work investigates whether and how generative visual question answering systems can responsibly support patient information needs in the context of radiology imaging data. We conducted a formative need-finding study in which participants discussed chest computed tomography (CT) scans and associated radiology reports of a fictitious close relative with a cardiothoracic radiologist. Using thematic analysis of the conversation between participants and medical experts, we identified commonly occurrin
    
[^43]: 不仅仅去识别可能是不够的

    De-identification is not always enough

    [https://arxiv.org/abs/2402.00179](https://arxiv.org/abs/2402.00179)

    研究表明，仅仅进行去识别操作并不能有效保护隐私。本文提出了使用大型语言模型生成合成临床笔记的方法，并评估了其在临床任务中的性能。同时，还发现利用合成数据训练模型可以提高会员推理攻击的成功率。

    

    对于共享隐私敏感数据，常常将去识别视为足够保护隐私的措施。合成数据也被认为是一种保护隐私的替代方法。最近在生成数值和表格数据模型方面取得的成功以及大型生成语言模型的突破引发了一个问题：合成的临床笔记是否可以作为研究目的的真实笔记的可行替代品。在这项工作中，我们证明了：（i）对真实临床笔记的去识别并不能保护记录免遭会员推理攻击；（ii）提出了一种使用当前最先进的大型语言模型生成合成临床笔记的新方法；（iii）在临床领域任务中评估了合成生成笔记的性能；（iv）提出了一种利用合成数据训练目标模型的会员推理攻击方法。我们观察到，当合成生成的笔记与真实笔记相似时，这种攻击的成功率增加。

    For sharing privacy-sensitive data, de-identification is commonly regarded as adequate for safeguarding privacy. Synthetic data is also being considered as a privacy-preserving alternative. Recent successes with numerical and tabular data generative models and the breakthroughs in large generative language models raise the question of whether synthetically generated clinical notes could be a viable alternative to real notes for research purposes. In this work, we demonstrated that (i) de-identification of real clinical notes does not protect records against a membership inference attack, (ii) proposed a novel approach to generate synthetic clinical notes using the current state-of-the-art large language models, (iii) evaluated the performance of the synthetically generated notes in a clinical domain task, and (iv) proposed a way to mount a membership inference attack where the target model is trained with synthetic data. We observed that when synthetically generated notes closely match
    
[^44]: 使用多重嵌入模型的多模式临床伪笔记用于紧急科室预测任务的医疗电子健康记录（EHR）翻译

    Multimodal Clinical Pseudo-notes for Emergency Department Prediction Tasks using Multiple Embedding Model for EHR (MEME)

    [https://arxiv.org/abs/2402.00160](https://arxiv.org/abs/2402.00160)

    本研究提出了一种名为“MEME”的多重嵌入模型，将电子健康记录视为多模态数据。通过结合“伪笔记”和多模态方法，该模型在紧急科室预测任务中表现出优越性能，超过了单模态嵌入方法和传统机器学习方法。然而，该模型在不同医院机构之间存在泛化能力方面的局限性。

    

    在这项工作中，我们引入了针对电子健康记录（EHR）的多重嵌入模型（MEME），这种方法将EHR视为多模态数据。该方法包括“伪笔记”，即对表格形式的EHR概念（如诊断和药物）进行文本表示，使我们能够有效地使用大型语言模型（LLM）进行EHR表示。该框架还采用了多模态方法，分别嵌入每个EHR模态。我们通过在多个医院系统的急诊科中应用MEME来证明其有效性。我们的研究结果表明，MEME在性能上超过了单模态嵌入方法和传统的机器学习方法。然而，我们还观察到所有测试模型在不同医院机构之间的泛化能力方面存在明显的局限性。

    In this work, we introduce Multiple Embedding Model for EHR (MEME), an approach that views Electronic Health Records (EHR) as multimodal data. This approach incorporates "pseudo-notes", textual representations of tabular EHR concepts such as diagnoses and medications, and allows us to effectively employ Large Language Models (LLMs) for EHR representation. This framework also adopts a multimodal approach, embedding each EHR modality separately. We demonstrate the effectiveness of MEME by applying it to several tasks within the Emergency Department across multiple hospital systems. Our findings show that MEME surpasses the performance of both single modality embedding methods and traditional machine learning approaches. However, we also observe notable limitations in generalizability across hospital institutions for all tested models.
    
[^45]: Dolma:一个包含三万亿个令牌的开放语料库，用于语言模型预训练研究

    Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research

    [https://arxiv.org/abs/2402.00159](https://arxiv.org/abs/2402.00159)

    Dolma是一个包含三万亿个令牌的开放语料库，用于语言模型预训练研究。它包含了来自多种来源的网络内容、科技论文、代码、公共领域图书、社交媒体和百科全书材料。为了促进开放研究，我们还开源了数据整理工具包。

    

    语言模型已成为处理各种自然语言处理任务的关键技术，然而，关于最佳表现的语言模型是如何开发的很多细节并未报道。特别是，其预训练语料库的信息很少被讨论：商业语言模型很少提供有关其数据的任何信息；即使是开放模型也很少发布它们所训练的数据集，或者提供一个精确的复现方法。因此，进行一些语言建模研究变得具有挑战性，比如理解训练数据如何影响模型的能力和限制。为了促进语言模型预训练的开放研究，我们发布了Dolma，一个包含三万亿个令牌的英语语料库，其中包括各种各样的网络内容、科技论文、代码、公共领域图书、社交媒体和百科全书材料。此外，我们还开源了我们的数据整理工具包，以便进一步的实验和再现。

    Language models have become a critical technology to tackling a wide range of natural language processing tasks, yet many details about how the best-performing language models were developed are not reported. In particular, information about their pretraining corpora is seldom discussed: commercial language models rarely provide any information about their data; even open models rarely release datasets they are trained on, or an exact recipe to reproduce them. As a result, it is challenging to conduct certain threads of language modeling research, such as understanding how training data impacts model capabilities and shapes their limitations. To facilitate open research on language model pretraining, we release Dolma, a three trillion tokens English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. In addition, we open source our data curation toolkit to enable further experimentation and reprodu
    
[^46]: 大型语言模型在数学推理中的应用：进展与挑战

    Large Language Models for Mathematical Reasoning: Progresses and Challenges

    [https://arxiv.org/abs/2402.00157](https://arxiv.org/abs/2402.00157)

    大型语言模型(LLMs)在解决数学问题方面涉及了大量的数学问题类型和不同的数据集和设置。目前仍然存在一些挑战，需要进一步研究和解决。

    

    数学推理是评估人类智能基本认知能力的基石。近年来，大型语言模型（LLMs）的发展引起了人们对自动解决数学问题的重视。然而，数学问题的类型非常广泛，LLM相关技术在不同数据集和设置下进行评估，使得如何判断这一新兴领域中的真正进展和障碍变得困难。本调查研究包括了以下四个关键方面：i）全面探索各种已经研究的数学问题及其相应数据集；ii）研究提出的解决数学问题的LLM技术的范围；iii）概述影响LLM在解决数学问题中的因素和关注点；iv）阐明仍然存在的挑战。

    Mathematical reasoning serves as a cornerstone for assessing the fundamental cognitive capabilities of human intelligence. In recent times, there has been a notable surge in the development of Large Language Models (LLMs) geared towards the automated resolution of mathematical problems. However, the landscape of mathematical problem types is vast and varied, with LLM-oriented techniques undergoing evaluation across diverse datasets and settings. This diversity makes it challenging to discern the true advancements and obstacles within this burgeoning field. This survey endeavors to address four pivotal dimensions: i) a comprehensive exploration of the various mathematical problems and their corresponding datasets that have been investigated; ii) an examination of the spectrum of LLM-oriented techniques that have been proposed for mathematical problem-solving; iii) an overview of factors and concerns affecting LLMs in solving math; and iv) an elucidation of the persisting challenges with
    
[^47]: 语言适配器在跨语言NLU传输中的影响

    The Impact of Language Adapters in Cross-Lingual Transfer for NLU

    [https://arxiv.org/abs/2402.00149](https://arxiv.org/abs/2402.00149)

    本文研究了在零样本跨语言传输中语言适配器的作用，结果显示其在任务、语言和模型之间的效果不一致。保留源语言适配器通常导致相同或更好的性能。

    

    提出了模块化深度学习用于将预训练模型高效地适用于新的任务、领域和语言。特别是，在没有针对某种语言的监督数据的情况下，将语言适配器与任务适配器结合起来展示了潜力。本文研究了在自然语言理解（NLU）基准测试中零样本跨语言传输中语言适配器的作用。我们通过详细的消融研究，使用两个多语言模型和三个多语言数据集，探讨了包含目标语言适配器的效果。我们的结果表明，目标语言适配器的效果在任务、语言和模型之间高度不一致。保留源语言适配器通常会导致相同甚至更好的性能。在训练后去掉语言适配器对预测的影响仅有弱的负面效果，表明语言适配器对预测没有强烈的影响。

    Modular deep learning has been proposed for the efficient adaption of pre-trained models to new tasks, domains and languages. In particular, combining language adapters with task adapters has shown potential where no supervised data exists for a language. In this paper, we explore the role of language adapters in zero-shot cross-lingual transfer for natural language understanding (NLU) benchmarks. We study the effect of including a target-language adapter in detailed ablation studies with two multilingual models and three multilingual datasets. Our results show that the effect of target-language adapters is highly inconsistent across tasks, languages and models. Retaining the source-language adapter instead often leads to an equivalent, and sometimes to a better, performance. Removing the language adapter after training has only a weak negative effect, indicating that the language adapters do not have a strong impact on the predictions.
    
[^48]: 在对话建模中让故事简短

    Making a Long Story Short in Conversation Modeling

    [https://arxiv.org/abs/2402.00143](https://arxiv.org/abs/2402.00143)

    本研究探讨了对话模型中话语长度多样性对生成后续回复质量的影响，发现在某些对话类型中，可以将话语长度减少72%而不影响质量。

    

    对话系统适应了具有独特个性和不同写作风格的多样化用户。在多轮对话建模领域，本研究研究了话语长度的多样性对对话模型生成的后续回复质量的影响。使用GPT-3作为基础模型，多个对话数据集和多个评估指标，我们对对话模型的这一方面进行了彻底的探索。我们的分析揭示了话语长度与对话系统生成的后续回复质量之间的复杂关系。经验性发现表明，在某些类型的对话中，话语长度可以减少72%，而后续回复的质量没有明显差异。

    Conversation systems accommodate diverse users with unique personalities and distinct writing styles. Within the domain of multi-turn dialogue modeling, this work studies the impact of varied utterance lengths on the quality of subsequent responses generated by conversation models. Using GPT-3 as the base model, multiple dialogue datasets, and several metrics, we conduct a thorough exploration of this aspect of conversational models. Our analysis sheds light on the complex relationship between utterance lengths and the quality of follow-up responses generated by dialogue systems. Empirical findings suggests that, for certain types of conversations, utterance lengths can be reduced by up to 72% without any noticeable difference in the quality of follow-up responses.
    
[^49]: 深度伪造检测的常识推理

    Common Sense Reasoning for Deep Fake Detection

    [https://arxiv.org/abs/2402.00126](https://arxiv.org/abs/2402.00126)

    该论文提出使用常识推理来建模深度伪造检测，通过扩展到Deepfake Detection VQA任务来模拟人类直觉，解释标记图像为真实或伪造的原因。

    

    最先进的方法依赖于通过神经网络提取的基于图像的特征进行深度伪造检测二分类。虽然这些方法在监督训练下提取了可能的伪造特征，但它们可能无法有效表示不自然的“非物理”语义面部属性 - 模糊的发际线、双眉毛、僵硬的瞳孔或不自然的皮肤着色。然而，这类面部属性通常通过常识推理对人类来说很容易感知。此外，通过显著性图提供视觉解释的基于图像的特征提取方法可能很难被人类解释。为了解决这些挑战，我们建议使用常识推理来建模深度伪造检测，并将其扩展到Deepfake Detection VQA（DD-VQA）任务，目的是模拟人类直觉来解释标记图像为真实或伪造的原因。为此，我们引入了一个新的数据集，为与深度伪造检测相关的问题提供答案。

    State-of-the-art approaches rely on image-based features extracted via neural networks for the deepfake detection binary classification. While these approaches trained in the supervised sense extract likely fake features, they may fall short in representing unnatural `non-physical' semantic facial attributes -- blurry hairlines, double eyebrows, rigid eye pupils, or unnatural skin shading. However, such facial attributes are generally easily perceived by humans via common sense reasoning. Furthermore, image-based feature extraction methods that provide visual explanation via saliency maps can be hard to be interpreted by humans. To address these challenges, we propose the use of common sense reasoning to model deepfake detection, and extend it to the Deepfake Detection VQA (DD-VQA) task with the aim to model human intuition in explaining the reason behind labeling an image as either real or fake. To this end, we introduce a new dataset that provides answers to the questions related to 
    
[^50]: 比较基于模板和非模板语言模型的探测方法

    Comparing Template-based and Template-free Language Model Probing

    [https://arxiv.org/abs/2402.00123](https://arxiv.org/abs/2402.00123)

    本文比较了基于模板和非模板语言模型的探测方法，发现它们在模型排名、绝对得分和与困惑度的关系等方面存在差异。

    

    以专家制作的模板和自然发生的文本为基础的语言模型探测方法的差异经常被忽视。在这里，我们评估了16种不同的语言模型在10个英文探测数据集上的性能，其中包括4个基于模板的和6个非模板的数据集，并针对以下研究问题进行了回答：（RQ1）模型排名在两种方法中是否不同？（RQ2）模型的绝对得分在两种方法中是否不同？（RQ3）RQ1和RQ2的答案在一般和领域特定模型之间是否不同？我们的发现是：1）除了顶级的领域特定模型外，基于模板和非模板方法通常排名不同。2）与平行的非模板和模板提示相比，准确度下降了最多42%。3）在非模板方法中，困惑度与准确度呈负相关，但是在基于模板的探测中，它们呈正相关，这与直觉相反。4）模型倾向于预测相同的内容。

    The differences between cloze-task language model (LM) probing with 1) expert-made templates and 2) naturally-occurring text have often been overlooked. Here, we evaluate 16 different LMs on 10 probing English datasets -- 4 template-based and 6 template-free -- in general and biomedical domains to answer the following research questions: (RQ1) Do model rankings differ between the two approaches? (RQ2) Do models' absolute scores differ between the two approaches? (RQ3) Do the answers to RQ1 and RQ2 differ between general and domain-specific models? Our findings are: 1) Template-free and template-based approaches often rank models differently, except for the top domain-specific models. 2) Scores decrease by up to 42% Acc@1 when comparing parallel template-free and template-based prompts. 3) Perplexity is negatively correlated with accuracy in the template-free approach, but, counter-intuitively, they are positively correlated for template-based probing. 4) Models tend to predict the same
    
[^51]: D-Nikud: 使用LSTM和预训练模型增强希伯来语的点音化

    D-Nikud: Enhancing Hebrew Diacritization with LSTM and Pretrained Models

    [https://arxiv.org/abs/2402.00075](https://arxiv.org/abs/2402.00075)

    D-Nikud是一种新颖的希伯来语点音化方法，通过结合LSTM和BERT-based预训练模型，实现了最先进的结果，并且在现代文本和更详细的点音化方面表现出色。

    

    D-Nikud是一种新颖的希伯来语点音化方法，它将LSTM网络和基于BERT的预训练模型的优势相结合。受Nakdimon方法的启发，我们将其与TavBERT预训练模型结合起来，系统中采用了先进的架构选择和多样化的训练数据。我们的实验在多个基准数据集上展示了最先进的结果，特别强调现代文本和更详细的点音化，如性别。

    D-Nikud, a novel approach to Hebrew diacritization that integrates the strengths of LSTM networks and BERT-based (transformer) pre-trained model. Inspired by the methodologies employed in Nakdimon, we integrate it with the TavBERT pre-trained model, our system incorporates advanced architectural choices and diverse training data. Our experiments showcase state-of-the-art results on several benchmark datasets, with a particular emphasis on modern texts and more specified diacritization like gender.
    
[^52]: EvoMerge:针对大型语言模型的神经进化

    EvoMerge: Neuroevolution for Large Language Models

    [https://arxiv.org/abs/2402.00070](https://arxiv.org/abs/2402.00070)

    EvoMerge是一种针对大型语言模型训练和合并的系统性方法，通过权重交叉和微调进行权重变异，旨在将模型推向超越传统微调限制的进化过程。

    

    在大型语言模型的广泛微调中，并不总能取得更好的结果。往往模型更擅长模仿一种数据形式而不具备更强的推理能力，甚至可能失去一些智能。这里我介绍了EvoMerge，一种用于大型语言模型训练和合并的系统性方法。通过利用权重交叉和微调进行权重变异，EvoMerge建立了一个旨在将模型推向超越传统微调限制的进化过程。

    Extensive fine-tuning on Large Language Models does not always yield better results. Oftentimes, models tend to get better at imitating one form of data without gaining greater reasoning ability and may even end up losing some intelligence. Here I introduce EvoMerge, a systematic approach to large language model training and merging. Leveraging model merging for weight crossover and fine-tuning for weight mutation, EvoMerge establishes an evolutionary process aimed at pushing models beyond the limits of conventional fine-tuning.
    
[^53]: LLaMA和ChatGPT嵌入在分子嵌入中的比较分析

    Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding

    [https://arxiv.org/abs/2402.00024](https://arxiv.org/abs/2402.00024)

    LLaMA和ChatGPT比较分析了它们在SMILES字符串嵌入中的性能，在分子性质预测和药物-药物相互作用预测中，LLaMA相对于ChatGPT表现更好并且与现有方法相当。

    

    目的：ChatGPT和LLaMA等大型语言模型在化学信息学领域越来越受到重视，特别是在解释Simplified Molecular Input Line Entry System (SMILES)方面。这些语言模型可以将SMILES字符串解码为向量表示，为理解化学图提供了一种新的方法。方法：我们研究了ChatGPT和LLaMA在嵌入SMILES字符串方面的性能。我们的评估集中在两个关键应用领域：分子性质（MP）预测和药物-药物相互作用（DDI）预测，这在药物开发和医疗保健中至关重要。结果：我们发现，使用LLaMA生成的SMILES嵌入在MP和DDI预测任务中表现优于ChatGPT。值得注意的是，基于LLaMA的SMILES嵌入在这两个预测任务中显示了与现有方法相当的结果。结论：在化学信息学中应用LLMs，特别是在利用SMILES进行嵌入方面，是可行的。

    Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasingly recognized for their potential in the field of cheminformatics, particularly in interpreting Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures. These LLMs can decode SMILES strings into vector representations, providing a novel approach to understanding chemical graphs.   Methods: We investigate the performance of ChatGPT and LLaMA in embedding SMILES strings. Our evaluation focuses on two key applications: molecular property (MP) prediction and drug-drug interaction (DDI) prediction, both essential in drug development and healthcare.   Results: We find that SMILES embeddings generated using LLaMA outperform those from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-based SMILES embeddings show results comparable to existing methods in both prediction tasks.   Conclusion: The application of LLMs in cheminformatics, particularly in utilizi
    
[^54]: 小型LLMs是弱工具学习者：多LLM代理

    Small LLMs Are Weak Tool Learners: A Multi-LLM Agent

    [https://arxiv.org/abs/2401.07324](https://arxiv.org/abs/2401.07324)

    本论文提出了一种新的策略，将大型语言模型代理（LLMs）的能力分解为计划器、调用器和总结器模块，以克服小型模型性能限制和工具更新的问题。

    

    大型语言模型（LLM）代理大大扩展了独立LLMs的能力，使它们能够与外部工具（例如API，函数）进行交互，并自主完成复杂任务。工具使用的挑战要求LLMs不仅能理解用户查询并生成答案，还要在任务规划、记忆管理、工具调用和结果总结方面表现出色。传统方法集中于训练单个具备所有这些功能的LLM，但在小型模型上会出现性能限制的问题，此外，当工具更新时，整个LLM可能需要重新训练。为了克服这些挑战，我们提出了一种新的策略，将上述能力分解为计划器、调用器和总结器。每个组件由一个单独的LLM实现，专注于特定的能力，并与其他组件合作完成任务。这种模块化框架便于进行个体更新和...

    Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete complex tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers but also excel in task planning, memory management, tool invocation, and result summarization. While traditional approaches focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. Moreover, the entire LLM may require retraining when tools are updated. To overcome these challenges, we propose a novel strategy that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with other components to accomplish the task. This modular framework facilitates individual updates and t
    
[^55]: 零样本自然语言视频定位中的常识推理

    Commonsense for Zero-Shot Natural Language Video Localization

    [https://arxiv.org/abs/2312.17429](https://arxiv.org/abs/2312.17429)

    本文研究了零样本自然语言-视频定位中常识推理的有效性，并提出了CORONET框架，该框架利用常识进行视频和生成的伪查询之间的桥接。实验证明CORONET在零样本和传统NLVL方法上都取得了优越的结果。

    

    零样本自然语言-视频定位（NLVL）方法通过动态生成视频片段和伪查询注释，在仅用原始视频数据训练NLVL模型方面取得了令人期待的结果。然而，现有的伪查询经常缺乏对源视频的扎实基础，导致内容不结构化和不连贯。本文研究了零样本NLVL中常识推理的有效性。具体而言，我们提出了CORONET，一个零样本NLVL框架，通过常识增强模块桥接视频和生成的伪查询之间的差距。CORONET使用图卷积网络（GCN）来编码从知识图中提取的常识信息，条件是视频，以及交叉注意机制来增强编码视频和伪查询表示以进行定位。通过对两个基准数据集进行实证评估，我们证明CORONET在零样本和传统NLVL方法上都取得了优越的结果。

    Zero-shot Natural Language-Video Localization (NLVL) methods have exhibited promising results in training NLVL models exclusively with raw video data by dynamically generating video segments and pseudo-query annotations. However, existing pseudo-queries often lack grounding in the source video, resulting in unstructured and disjointed content. In this paper, we investigate the effectiveness of commonsense reasoning in zero-shot NLVL. Specifically, we present CORONET, a zero-shot NLVL framework that leverages commonsense to bridge the gap between videos and generated pseudo-queries via a commonsense enhancement module. CORONET employs Graph Convolution Networks (GCN) to encode commonsense information extracted from a knowledge graph, conditioned on the video, and cross-attention mechanisms to enhance the encoded video and pseudo-query representations prior to localization. Through empirical evaluations on two benchmark datasets, we demonstrate that CORONET surpasses both zero-shot and w
    
[^56]: RLHF和IIA：倒置激励

    RLHF and IIA: Perverse Incentives

    [https://arxiv.org/abs/2312.01057](https://arxiv.org/abs/2312.01057)

    RLHF算法中的IIA假设导致了倒置激励，限制了查询格式和学习算法的创新。

    

    现有的基于人类反馈的强化学习算法（RLHF）可以激励与偏好不符的回应，因为它们基于假设无关概括的模型（IIA）。IIA引发的倒置激励阻碍了查询格式和学习算法的创新。

    Existing algorithms for reinforcement learning from human feedback (RLHF) can incentivize responses at odds with preferences because they are based on models that assume independence of irrelevant alternatives (IIA). The perverse incentives induced by IIA hinder innovations on query formats and learning algorithms.
    
[^57]: 将大型语言模型定性为知识密集型任务的理性化工具

    Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks

    [https://arxiv.org/abs/2311.05085](https://arxiv.org/abs/2311.05085)

    大型语言模型在知识密集型任务中的理性化能力有待探索，通过使用专家编写的示例，可以生成更受欢迎的基于世界知识的理性化方式。这些理性化方式需要进一步改进，在错误预测的理性化方面会损害人类对模型的信任。

    

    大型语言模型(LLMs)在几乎没有任务特定监督的情况下能够生成流畅的文本。然而，它们在提供基于知识密集型任务的充分理性支持方面的能力尚未得到充分探索。这类任务，比如常识性多项选择题，需要基于世界知识的理性来支持预测并推翻备选选项。我们通过使用专家编写的样例以少量样本的方式在自然语言中生成知识引导的理性化任务。令人惊讶的是，工人群体更喜欢基于知识的理性化方式，认为其具有事实性、充分性和全面性的反驳。虽然LLMs生成的理性化方式更受欢迎，但还需要在简洁性和新颖性方面进一步改进。在另一项研究中，我们展示了错误模型预测的理性化如何侵蚀人类对LLMs生成的理性化的信任。在这些观察的基础上，我们创建了一个两阶段的流程。

    Large language models (LLMs) are proficient at generating fluent text with minimal task-specific supervision. Yet, their ability to provide well-grounded rationalizations for knowledge-intensive tasks remains under-explored. Such tasks, like commonsense multiple-choice questions, require rationales based on world knowledge to support predictions and refute alternate options. We consider the task of generating knowledge-guided rationalization in natural language by using expert-written examples in a few-shot manner. Surprisingly, crowd-workers preferred knowledge-grounded rationales over crowdsourced rationalizations, citing their factuality, sufficiency, and comprehensive refutations. Although LLMs-generated rationales were preferable, further improvements in conciseness and novelty are required. In another study, we show how rationalization of incorrect model predictions erodes humans' trust in LLM-generated rationales. Motivated by these observations, we create a two-stage pipeline t
    
[^58]: InstructRetro: 检索增强的预训练中指令调优

    InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining

    [https://arxiv.org/abs/2310.07713](https://arxiv.org/abs/2310.07713)

    InstructRetro是目前规模最大的使用检索预训练的LLM，扩展了基础模型Retro 48B，通过指令调优在各种零样例任务上取得显著改进。

    

    使用检索增强技术对自回归大型语言模型（LLM）进行预训练可以提高困惑度和事实准确性。然而，现有的预训练检索增强LLM的规模仍然有限（如Retro具有75亿个参数），这限制了指令调优和零样例泛化的效果。本文介绍了Retro 48B，这是目前规模最大的使用检索预训练的LLM。具体来说，我们使用检索技术从1.2万亿个标记中继续预训练一个43B的GPT模型，并借助Retro方法将其扩展到4800亿个参数。值得注意的是，所得到的基础模型Retro 48B在困惑度方面显著优于仅使用1.2万亿个标记进行训练的43B GPT模型，且只增加了2.58%的GPU使用时间，展示了该方法的显著扩展潜力。在对Retro进行指令调优后，InstructRetro在各种零样例任务上表现出显著的改进。

    Pretraining auto-regressive large language models (LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval. Specifically, we continue to pretrain a 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. Notably, the obtained foundation model, Retro 48B, largely outperforms the counterpart GPT 43B trained on 1.2T tokens in terms of perplexity with only 2.58% additional GPU hours, demonstrating the significant scaling potential of the method. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on a wide range of zero-shot tasks. Spe
    
[^59]: 深度自然语言处理模型中潜在概念的扩展发现

    Scaling up Discovery of Latent Concepts in Deep NLP Models

    [https://arxiv.org/abs/2308.10263](https://arxiv.org/abs/2308.10263)

    本文研究了聚类算法，以将深度自然语言处理模型中编码的概念扩展到更大的数据集和模型上。我们发现基于K-Means的概念发现显著提高了效率并保持了质量。

    

    尽管深度自然语言处理模型引起了一场革命，但它们仍然是黑匣子，需要研究来理解它们的决策过程。Dalvi等人（2022年）最近通过对预训练模型（PLMs）中的潜在空间进行聚类分析，开展了表示分析，但由于运行聚合层次聚类的高成本，该方法在小规模上受到限制。本文研究聚类算法，以便将PLM表示中编码的概念发现扩展到更大的数据集和模型上。我们提出了评估发现的潜在概念质量的指标，并使用这些指标来比较所研究的聚类算法。我们发现基于K-Means的概念发现在保持所获得概念质量的同时显著提高了效率。此外，我们通过将潜在概念发现扩展到LLMs和短语概念中，证明了这种新发现的效率的实用性。

    Despite the revolution caused by deep NLP models, they remain black boxes, necessitating research to understand their decision-making processes. A recent work by Dalvi et al. (2022) carried out representation analysis through the lens of clustering latent spaces within pre-trained models (PLMs), but that approach is limited to small scale due to the high cost of running Agglomerative hierarchical clustering. This paper studies clustering algorithms in order to scale the discovery of encoded concepts in PLM representations to larger datasets and models. We propose metrics for assessing the quality of discovered latent concepts and use them to compare the studied clustering algorithms. We found that K-Means-based concept discovery significantly enhances efficiency while maintaining the quality of the obtained concepts. Furthermore, we demonstrate the practicality of this newfound efficiency by scaling latent concept discovery to LLMs and phrasal concepts.
    
[^60]: 利用开放信息抽取来增强事件触发识别的领域转移鲁棒性

    Leveraging Open Information Extraction for More Robust Domain Transfer of Event Trigger Detection

    [https://arxiv.org/abs/2305.14163](https://arxiv.org/abs/2305.14163)

    本研究利用开放信息抽取系统中的主谓关系将事件触发器在不同领域间进行耦合，以提升事件触发识别的领域转移性能。在从高资源到低资源领域的转移中，该方法可减轻性能下降，特别是在从维基百科到新闻领域的转移中效果显著。同时结合遮蔽语言建模能进一步增强转移效果。

    

    事件触发识别是许多领域中关键的信息抽取任务，如维基百科或新闻。该任务通常依赖于触发识别（TD）—识别文本中引起特定事件的标记范围。尽管触发器的概念应理想地适用于所有领域，但从高资源领域到低资源领域的TD领域转移会导致性能显著下降。我们通过使用基于规则的开放信息抽取（OIE）系统获取的主谓关系将触发器在领域之间进行耦合来解决TD中的负转移问题。我们证明通过多任务训练注入的OIE关系可以作为不同领域触发器之间的中介，增强零样本和少样本的TD领域转移，并减少性能下降，特别是从高资源源领域（维基百科）转移到低资源目标领域（新闻）。此外，我们将改进的转移与遮蔽语言建模结合起来。

    Event detection is a crucial information extraction task in many domains, such as Wikipedia or news. The task typically relies on trigger detection (TD) -- identifying token spans in the text that evoke specific events. While the notion of triggers should ideally be universal across domains, domain transfer for TD from high- to low-resource domains results in significant performance drops. We address the problem of negative transfer in TD by coupling triggers between domains using subject-object relations obtained from a rule-based open information extraction (OIE) system. We demonstrate that OIE relations injected through multi-task training can act as mediators between triggers in different domains, enhancing zero- and few-shot TD domain transfer and reducing performance drops, in particular when transferring from a high-resource source domain (Wikipedia) to a low(er)-resource target domain (news). Additionally, we combine this improved transfer with masked language modeling on the t
    
[^61]: 从零开始构建一个大型语言模型

    Engineering A Large Language Model From Scratch. (arXiv:2401.16736v1 [cs.CL])

    [http://arxiv.org/abs/2401.16736](http://arxiv.org/abs/2401.16736)

    Atinuke是一种基于Transformer的神经网络，通过在处理时序数据的层与注意机制交织在一起，模拟人类语言，从而优化各种语言任务的性能。

    

    深度学习在自然语言处理（NLP）领域的普及导致了能够理解和生成人类语言的创新技术的开发和发布。Atinuke是一种基于Transformer的神经网络，通过利用独特的配置，在各种语言任务上优化性能。该架构通过将处理时序数据的层与注意机制交织在一起，从而在输入和输出之间建立有意义的关联。由于其拓扑结构和超参数调整的配置，它可以提取特征并学习复杂的映射，从而模仿人类语言。Atinuke是模块化、可扩展的，并可以与现有的机器学习流程无缝集成。softmax、嵌入和多头注意力等高级矩阵操作使得对文本、声音和视觉信号的细致处理成为可能。通过将现代深度学习技术与软件设计原则和数学方法相结合

    The proliferation of deep learning in natural language processing (NLP) has led to the development and release of innovative technologies capable of understanding and generating human language with remarkable proficiency. Atinuke, a Transformer-based neural network, optimises performance across various language tasks by utilising a unique configuration. The architecture interweaves layers for processing sequential data with attention mechanisms to draw meaningful affinities between inputs and outputs. Due to the configuration of its topology and hyperparameter tuning, it can emulate human-like language by extracting features and learning complex mappings. Atinuke is modular, extensible, and integrates seamlessly with existing machine learning pipelines. Advanced matrix operations like softmax, embeddings, and multi-head attention enable nuanced handling of textual, acoustic, and visual signals. By unifying modern deep learning techniques with software design principles and mathematical
    
[^62]: UNSEE: 无监督的非对比度句子嵌入

    UNSEE: Unsupervised Non-contrastive Sentence Embeddings. (arXiv:2401.15316v1 [cs.CL])

    [http://arxiv.org/abs/2401.15316](http://arxiv.org/abs/2401.15316)

    UNSEE是一种无监督的非对比度句子嵌入方法，通过引入目标网络解决了表示坍塌问题，达到了与对比目标相当的性能提升。

    

    我们提出了一种名为UNSEE（Unsupervised Non-Contrastive Sentence Embeddings）的新方法，在大规模文本嵌入基准测试中超越了SimCSE。我们首先解决了SimCSE中替换对比目标为非对比目标时出现的表示坍塌挑战。为了解决这个问题，我们提出了一种称为目标网络的简单解决方案，有效地缓解了表示坍塌。目标网络的引入使我们能够利用非对比目标，在保持训练稳定性的同时实现与对比目标相当的性能提升。通过精心调整和优化，我们的方法在非对比度句子嵌入上达到了巅峰性能。这一全面的努力产生了出色的句子表示模型，展示了我们方法的有效性。

    We present UNSEE: Unsupervised Non-Contrastive Sentence Embeddings, a novel approach that outperforms SimCSE in the Massive Text Embedding benchmark. Our exploration begins by addressing the challenge of representation collapse, a phenomenon observed when contrastive objectives in SimCSE are replaced with non-contrastive objectives. To counter this issue, we propose a straightforward solution known as the target network, effectively mitigating representation collapse. The introduction of the target network allows us to leverage non-contrastive objectives, maintaining training stability while achieving performance improvements comparable to contrastive objectives. Our method has achieved peak performance in non-contrastive sentence embeddings through meticulous fine-tuning and optimization. This comprehensive effort has yielded superior sentence representation models, showcasing the effectiveness of our approach.
    
[^63]: 关于层次主题建模的亲和性、合理性和多样性的研究

    On the Affinity, Rationality, and Diversity of Hierarchical Topic Modeling. (arXiv:2401.14113v1 [cs.CL])

    [http://arxiv.org/abs/2401.14113](http://arxiv.org/abs/2401.14113)

    本论文提出了一种名为TraCo的新层次主题模型，通过传输计划依赖方法和上下文感知的解缠码器，改善了主题层次结构的亲和性、合理性和多样性。

    

    层次主题建模旨在从语料库中发现潜在主题，并将它们组织成一个层次结构，以便理解具有期望语义粒度的文档。然而，现有工作在产生低亲和性、合理性和多样性的主题层次方面存在困难，这阻碍了文档的理解。为了克服这些挑战，本文提出了传输计划和上下文感知的层次主题模型（TraCo）。我们提出了一种传输计划依赖方法，而不是之前简单的主题依赖方法。它限制依赖关系以确保它们的稀疏性和平衡性，并通过它们对主题层次结构进行正则化。这改善了层次结构的亲和性和多样性。我们还提出了一种上下文感知的解缠码器。它通过解缠编码将不同的语义粒度分配给不同层次的主题，从而有助于层次结构的合理性。在基准数据集上的实验证明了我们方法的有效性。

    Hierarchical topic modeling aims to discover latent topics from a corpus and organize them into a hierarchy to understand documents with desirable semantic granularity. However, existing work struggles with producing topic hierarchies of low affinity, rationality, and diversity, which hampers document understanding. To overcome these challenges, we in this paper propose Transport Plan and Context-aware Hierarchical Topic Model (TraCo). Instead of early simple topic dependencies, we propose a transport plan dependency method. It constrains dependencies to ensure their sparsity and balance, and also regularizes topic hierarchy building with them. This improves affinity and diversity of hierarchies. We further propose a context-aware disentangled decoder. Rather than previously entangled decoding, it distributes different semantic granularity to topics at different levels by disentangled decoding. This facilitates the rationality of hierarchies. Experiments on benchmark datasets demonstra
    
[^64]: 通过思维方程蒸馏改进小型语言模型的数学推理能力

    Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation. (arXiv:2401.11864v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.11864](http://arxiv.org/abs/2401.11864)

    本研究提出了思维方程蒸馏（EoTD）技术和集合思维蒸馏（ETD）框架，通过构建基于方程的表示和使用多个思维过程的推理数据集来改进小型语言模型（SLMs）的数学推理能力，实验结果表明，EoTD和ETD显著提升了SLMs的推理能力。

    

    本研究解决了将先进的大型语言模型（LLMs）的数学推理能力压缩到具有小于十亿参数的小型语言模型（SLMs）中的挑战，同时不损害性能。我们引入了一种新颖的思维方程蒸馏（EoTD）技术，将推理过程封装为基于方程的表示，构建了一个EoTD数据集来对SLMs进行微调。此外，我们提出了集合思维蒸馏（ETD）框架，以提升SLMs的推理性能。这包括创建一个包含多个思维过程（包括思维链、思维程序和思维方程）的推理数据集，并将其用于微调。我们的实验证明，EoTD显著提升了SLMs的推理能力，而ETD使这些模型实现了最先进的推理性能。

    This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for fine-tuning. Our experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while ETD enables these models to achieve state-of-the-art reasoning performance.
    
[^65]: 零样本生成式大型语言模型用于系统性综述筛选自动化

    Zero-shot Generative Large Language Models for Systematic Review Screening Automation. (arXiv:2401.06320v1 [cs.IR])

    [http://arxiv.org/abs/2401.06320](http://arxiv.org/abs/2401.06320)

    本研究调查了使用零样本生成式大型语言模型进行系统性综述自动筛选的有效性，结果显示指导微调和校准技术在筛选中起到重要作用，并且与零样本模型的集成相结合可以显著节省筛选时间。

    

    系统性综述对于基于证据的医学非常重要，它们综合分析了特定问题的已发表研究结果。进行此类综述通常需要大量的资源和时间，特别是在筛选阶段，需要评估出版物摘要是否应包括在综述中。本研究调查了使用零样本大型语言模型（LLM）进行自动筛选的有效性。我们评估了八种不同的LLM的效果，并研究了一种使用预定义的召回阈值的校准技术，用于确定是否应将出版物包括在系统性综述中。我们的全面评估使用了五个标准测试集，结果显示指导微调在筛选中起到了重要作用，校准使LLMs在实现目标召回方面更实用，并且将这两者与零样本模型的集成相结合与现有技术相比节省了大量筛选时间。

    Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models~(LLMs) for automatic screening. We evaluate the effectiveness of eight different LLMs and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches.
    
[^66]: LinguAlchemy: 将语言类型学和地理元素融合以实现对未见语言的泛化

    LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization. (arXiv:2401.06034v1 [cs.CL])

    [http://arxiv.org/abs/2401.06034](http://arxiv.org/abs/2401.06034)

    LinguAlchemy是一种将语言类型学和地理元素融合的正则化技术，能够显著提高预训练语言模型（PLMs）在未见语言上的泛化性能。

    

    预训练语言模型（PLMs）在多个任务和语言上展示出了非凡的泛化能力。然而，对于未见过的语言，PLMs的泛化能力较差，导致语言性能明显下降，甚至生成的回应与随机基准相当荒唐。这一限制一直以来都是PLMs的一个长期问题，涉及到语言建模技术的多样性和平等获取问题。在这项工作中，我们通过引入LinguAlchemy来解决这个限制，这是一种正则化技术，将语言的各个方面（包括类型学、地理和语系）纳入PLMs的表示中，以更好地表征相应的语言约束。与完全微调的模型相比，LinguAlchemy显著提高了mBERT和XLM-R对未见语言的准确性绩效，分别提高了约18%和约2%，展现出较高的未见语言泛化能力。

    Pretrained language models (PLMs) have shown remarkable generalization toward multiple tasks and languages. Nonetheless, the generalization of PLMs towards unseen languages is poor, resulting in significantly worse language performance, or even generating nonsensical responses that are comparable to a random baseline. This limitation has been a longstanding problem of PLMs raising the problem of diversity and equal access to language modeling technology. In this work, we solve this limitation by introducing LinguAlchemy, a regularization technique that incorporates various aspects of languages covering typological, geographical, and phylogenetic constraining the resulting representation of PLMs to better characterize the corresponding linguistics constraints. LinguAlchemy significantly improves the accuracy performance of mBERT and XLM-R on unseen languages by ~18% and ~2%, respectively compared to fully finetuned models and displaying a high degree of unseen language generalization. W
    
[^67]: Stack Overflow回答中信息高亮的初探

    A First Look at Information Highlighting in Stack Overflow Answers. (arXiv:2401.01472v1 [cs.CL])

    [http://arxiv.org/abs/2401.01472](http://arxiv.org/abs/2401.01472)

    本论文进行了首次大规模的探索性研究，研究了Stack Overflow回答中的信息高亮。通过使用神经网络架构，开发了自动推荐突出内容的方法。

    

    背景：浏览Stack Overflow（SO）的知识仍然具有挑战性。为了使帖子对用户更生动，SO允许用户使用Markdown或HTML编写和编辑帖子，以便用户可以利用各种格式化样式（例如粗体、斜体和代码）来突出重要信息。然而，关于突出信息的研究仍然有限。目标：我们在最近的研究中进行了首次大规模的探索性研究，研究了SO回答中的信息高亮。为了扩展我们之前的研究，我们利用最初设计用于命名实体识别任务的神经网络架构，开发了自动推荐带有格式化样式的突出内容的方法。方法：本文研究了Stack Overflow的31,169,429个回答。为了训练推荐模型，我们选择了CNN和BERT模型，针对每种格式化类型（即粗体、斜体、代码和标题）使用我们从SO回答收集的突出信息数据集。

    Context: Navigating the knowledge of Stack Overflow (SO) remains challenging. To make the posts vivid to users, SO allows users to write and edit posts with Markdown or HTML so that users can leverage various formatting styles (e.g., bold, italic, and code) to highlight the important information. Nonetheless, there have been limited studies on the highlighted information. Objective: We carried out the first large-scale exploratory study on the information highlighted in SO answers in our recent study. To extend our previous study, we develop approaches to automatically recommend highlighted content with formatting styles using neural network architectures initially designed for the Named Entity Recognition task. Method: In this paper, we studied 31,169,429 answers of Stack Overflow. For training recommendation models, we choose CNN and BERT models for each type of formatting (i.e., Bold, Italic, Code, and Heading) using the information highlighting dataset we collected from SO answers.
    
[^68]: AGI系统的元提示

    Meta Prompting for AGI Systems. (arXiv:2311.11482v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2311.11482](http://arxiv.org/abs/2311.11482)

    本文全面研究了元提示技术，这是一种创新方法，重塑了大型语言模型、多模态模型和人工智能系统在问题解决和数据解释方面的应用。通过强调信息的结构和句法，元提示将复杂问题拆解为简单的子问题，提高了效率，并且能够与少样本方法进行公平的比较。同时，本文还提出了元提示用于自动生成提示的方法。

    

    本文介绍了元提示(meta prompting)的全面研究，这是一种创新技术，重新塑造了大型语言模型(LLMs)、多模态基础模型和人工智能系统在问题解决和数据解释方面的利用。基于类型理论和范畴论，元提示注重信息的结构和句法，而不是传统以内容为中心的方法。本文探讨了元提示的形式定义，并将其与少样本提示(few-shot prompting)区分开来，并强调其在各种人工智能应用中的有效性。重点关注将元提示扩展到复杂推理任务上，展示如何将复杂问题拆分成较为简单的子问题，提高令牌效率，并使问题求解的比较更加公平，尤其是与少样本示例方法相比。此外，本文还引入了元提示用于提示任务，允许LLMs以迭代的元编程形式自动生成新的提示。

    This paper presents a comprehensive study of Meta Prompting, an innovative technique reshaping the utilization of large language models (LLMs), multi-modal foundation models, and AI systems in problem-solving and data interpretation. Grounded in type theory and category theory, Meta Prompting emphasizes the structure and syntax of information over traditional content-centric methods. The paper explores the formal definitions of Meta Prompting (MP), sets it apart from Few-Shot Prompting, and underlines its effectiveness in various AI applications. A key focus is on extending Meta Prompting to complex reasoning tasks, showing how it effectively deconstructs intricate problems into simpler sub-problems, enhancing token efficiency and enabling more equitable problem-solving comparisons, especially against few-shot example methods. Additionally, the paper introduces Meta Prompting for Prompting Tasks, allowing LLMs to self-generate new prompts in an iterative, metaprogramming-like manner. T
    
[^69]: 一种仅解码器的时间序列预测基础模型

    A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])

    [http://arxiv.org/abs/2310.10688](http://arxiv.org/abs/2310.10688)

    本论文介绍了一种基于补丁解码器式注意力模型的时间序列预测基础模型，该模型在零样本情况下在各种公共数据集上的性能接近最先进的监督预测模型。

    

    受到自然语言处理中大型语言模型的最新进展的启发，我们设计了一种用于预测的时间序列基础模型，其在各种公共数据集上的开箱即用的零样本性能接近每个个别数据集上最先进的监督预测模型的准确性。我们的模型基于在大型时间序列语料库上预训练的补丁解码器式注意力模型，并可以适用于不同的预测历史长度、预测长度和时间粒度。

    Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
    
[^70]: SELF：基于语言驱动的大型语言模型自主进化

    SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00533](http://arxiv.org/abs/2310.00533)

    SELF提出了一种基于语言驱动的创新方法，允许大型语言模型（LLM）不断自我进化，并通过语言反馈作为评估工具来改进模型的响应能力和训练稳定性。

    

    大型语言模型（LLM）展示了在多个领域中的卓越适应能力。然而，实现人类水平的学习和推动自主人工智能的关键——模型自主进化的路径仍然未知。我们引入了一种创新的方法，名为"SELF"（带有语言反馈的自主进化）。这种方法使LLM能够不断地自我进化。此外，SELF利用语言反馈作为一种多功能、全面的评估工具，精确定位响应改进的领域，并提高自主进化训练的稳定性。SELF首先进行元技能学习，专注于自我反馈和自我精炼。这些元技能是关键，引导模型在自制数据的持续训练周期中进行后续的自我进化，从而增强其内在能力。在给定无标签指令的情况下，SELF使模型具备了能够...

    Large Language Models (LLMs) have showcased remarkable versatility across diverse domains. However, the pathway toward autonomous model development, a cornerstone for achieving human-level learning and advancing autonomous AI, remains largely uncharted. We introduce an innovative approach, termed "SELF" (Self-Evolution with Language Feedback). This methodology empowers LLMs to undergo continual self-evolution. Furthermore, SELF employs language-based feedback as a versatile and comprehensive evaluative tool, pinpointing areas for response refinement and bolstering the stability of self-evolutionary training. Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement. These meta-skills are critical, guiding the model's subsequent self-evolution through a cycle of perpetual training with self-curated data, thereby enhancing its intrinsic abilities. Given unlabeled instructions, SELF equips the model with the capability to
    
[^71]: 使用大型语言模型生成、验证和应用用户意图分类方法

    Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies. (arXiv:2309.13063v1 [cs.IR])

    [http://arxiv.org/abs/2309.13063](http://arxiv.org/abs/2309.13063)

    通过使用大型语言模型生成用户意图分类，我们提出了一种新方法来分析和验证日志数据中的用户意图，从而解决了手动或基于机器学习的标注方法在大型和不断变化的数据集上的问题。

    

    日志数据可以揭示用户与网络搜索服务的交互方式、用户的需求以及满意程度等宝贵信息。然而，分析日志数据中的用户意图并不容易，尤其是对于新的网络搜索形式，如人工智能驱动的聊天。为了理解日志数据中的用户意图，我们需要一种能够用有意义的分类方式标记它们的方法，以捕捉其多样性和动态性。现有的方法依赖于手动或基于机器学习的标注，这些方法对于大型且不断变化的数据集而言，要么代价高昂要么不够灵活。我们提出了一种使用大型语言模型(LLM)的新方法，这种模型能够生成丰富且相关的概念、描述和示例来表示用户意图。然而，使用LLM生成用户意图分类并将其应用于日志分析可能存在两个主要问题：这样的分类得不到外部验证，并且可能存在不良的反馈回路。为了克服这些问题，我们提出了一种新的方法，通过人工专家和评估者来验证。

    Log data can reveal valuable information about how users interact with web search services, what they want, and how satisfied they are. However, analyzing user intents in log data is not easy, especially for new forms of web search such as AI-driven chat. To understand user intents from log data, we need a way to label them with meaningful categories that capture their diversity and dynamics. Existing methods rely on manual or ML-based labeling, which are either expensive or inflexible for large and changing datasets. We propose a novel solution using large language models (LLMs), which can generate rich and relevant concepts, descriptions, and examples for user intents. However, using LLMs to generate a user intent taxonomy and apply it to do log analysis can be problematic for two main reasons: such a taxonomy is not externally validated, and there may be an undesirable feedback loop. To overcome these issues, we propose a new methodology with human experts and assessors to verify th
    
[^72]: Code Llama: 用于代码的开放基础模型

    Code Llama: Open Foundation Models for Code. (arXiv:2308.12950v1 [cs.CL])

    [http://arxiv.org/abs/2308.12950](http://arxiv.org/abs/2308.12950)

    Code Llama是一系列用于代码的开放基础模型，具有最先进的性能和填充功能，支持大型输入上下文和零-shot指令跟踪能力。在多个代码基准测试中，Code Llama达到开放模型中最高的性能，同时Python专门化模型在某些测试上超越了Llama 2的70B版本。

    

    我们发布了Code Llama，这是一系列基于Llama 2的用于代码的大型语言模型，具有开放模型中最先进的性能，填充功能，支持大型输入上下文，并且能够进行零-shot指令跟踪编程任务。我们提供多种版本以覆盖广泛的应用场景：基础模型（Code Llama），Python专门化模型（Code Llama-Python），以及指令跟踪模型（Code Llama-Instruct），每个模型参数分别为7B、13B和34B。所有模型都是在16k标记序列上训练的，可以改善长度不超过100k标记的输入。7B和13B的Code Llama和Code Llama-Instruct变种会根据周围内容进行填充。Code Llama在几个代码基准测试中达到了开放模型中最先进的性能，HumanEval和MBPP分别达到了53%和55%的分数。值得注意的是，Code Llama-Python 7B在HumanEval和MBPP上优于Llama 2 70B，而我们的所有模型都优于其他任何模型。

    We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every othe
    
[^73]: 在对话中利用来自部署数据的隐式反馈

    Leveraging Implicit Feedback from Deployment Data in Dialogue. (arXiv:2307.14117v1 [cs.CL])

    [http://arxiv.org/abs/2307.14117](http://arxiv.org/abs/2307.14117)

    研究利用对话中的隐式反馈来改进社交对话系统。实验结果表明通过收集用户响应长度、情感和反应等信号可以提高机器生成话语的质量。

    

    我们研究通过学习用户与部署模型之间的自然对话来改进社交对话系统，而无需额外的注释。为了隐式衡量机器生成话语的质量，我们利用收集对话中未来人类话语的用户响应长度、情感和反应等信号。我们的实验使用了BlenderBot（Xu等，2023年）公开发布的部署数据。人类评估显示出我们的新模型比基线回复有所改进；然而，我们发现一些代理信号也可能导致出现不良属性的生成。例如，优化对话长度可能导致与基线相比更具争议性或不友好的生成，而优化积极情感或反应则可以减少这些行为。

    We study improving social conversational agents by learning from natural dialogue between users and a deployed model, without extra annotations. To implicitly measure the quality of a machine-generated utterance, we leverage signals like user response length, sentiment and reaction of the future human utterances in the collected dialogue episodes. Our experiments use the publicly released deployment data from BlenderBot (Xu et al., 2023). Human evaluation indicates improvements in our new models over baseline responses; however, we find that some proxy signals can lead to more generations with undesirable properties as well. For example, optimizing for conversation length can lead to more controversial or unfriendly generations compared to the baseline, whereas optimizing for positive sentiment or reaction can decrease these behaviors.
    
[^74]: CARTIER: 面向机器人指令执行的地图语言推理

    CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots. (arXiv:2307.11865v1 [cs.RO])

    [http://arxiv.org/abs/2307.11865](http://arxiv.org/abs/2307.11865)

    本研究使用大型语言模型，探索了在空间规划和导航交叉问题中，通过解析复杂的自然语言指令来执行任务的新方法。

    

    本研究探索了大型语言模型（LLM）在空间规划和自然语言界面与导航交叉问题中的应用能力。我们的重点是遵循相对复杂的指令，这些指令更类似于自然对话，而不是传统的显式过程指令。与大多数先前的工作不同，在那些导航指令被提供为命令式指令（例如，去冰箱）的情况下，我们研究了对话交互中的隐式指令。我们利用3D模拟器AI2Thor创建复杂且可重复的场景，并通过为40种对象类型添加复杂的语言查询来增强它。我们证明，通过使用LLM将用户交互解释为场景中对象列表的上下文，机器人可以更好地解析描述性语言查询，优于现有方法。

    This work explores the capacity of large language models (LLMs) to address problems at the intersection of spatial planning and natural language interfaces for navigation.Our focus is on following relatively complex instructions that are more akin to natural conversation than traditional explicit procedural directives seen in robotics. Unlike most prior work, where navigation directives are provided as imperative commands (e.g., go to the fridge), we examine implicit directives within conversational interactions. We leverage the 3D simulator AI2Thor to create complex and repeatable scenarios at scale, and augment it by adding complex language queries for 40 object types. We demonstrate that a robot can better parse descriptive language queries than existing methods by using an LLM to interpret the user interaction in the context of a list of the objects in the scene.
    
[^75]: VisualGPTScore: 多模态生成预训练分数的视觉语义推理。

    VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores. (arXiv:2306.01879v1 [cs.CV])

    [http://arxiv.org/abs/2306.01879](http://arxiv.org/abs/2306.01879)

    我们提出了VisualGPTScore方法，能够使用多模态生成分数捕捉文本标题可能性，并在图像条件语言模型上进行计算，具备组合推理能力。

    

    本文提出了一种名为 VisualGPTScore 的方法，使用多模态生成分数来捕捉文本标题可能性，并使用图像条件语言模型在图像上运算。与传统观点认为的VLM只是无意义的单词袋模型不同，我们的 VisualGPTScore 在 ARO 和 Crepe 等最近提出的图像文本检索基准测试中展现了顶尖的性能，证明了其具备组合推理能力。

    Vision-language models (VLMs) discriminatively pre-trained with contrastive image-text matching losses such as $P(\text{match}|\text{text}, \text{image})$ have been criticized for lacking compositional understanding. This means they might output similar scores even if the original caption is rearranged into a different semantic statement. To address this, we propose to use the ${\bf V}$isual ${\bf G}$enerative ${\bf P}$re-${\bf T}$raining Score (${\bf VisualGPTScore}$) of $P(\text{text}|\text{image})$, a $\textit{multimodal generative}$ score that captures the likelihood of a text caption conditioned on an image using an image-conditioned language model. Contrary to the belief that VLMs are mere bag-of-words models, our off-the-shelf VisualGPTScore demonstrates top-tier performance on recently proposed image-text retrieval benchmarks like ARO and Crepe that assess compositional reasoning. Furthermore, we factorize VisualGPTScore into a product of the $\textit{marginal}$ P(text) and the
    
[^76]: 小语言模型通过重写其输出来提高巨型模型的性能

    Small Language Models Improve Giants by Rewriting Their Outputs. (arXiv:2305.13514v1 [cs.CL])

    [http://arxiv.org/abs/2305.13514](http://arxiv.org/abs/2305.13514)

    本论文提出了一种方法，通过使用小语言模型重写大语言模型的输出，从而提高其性能。实验证明，该方法可以显着改善大语言模型的少样本学习能力和泛化性能。

    

    大型语言模型(LLMs)展示了令人印象深刻的少样本学习能力，但它们在挑战性任务上的表现通常不如微调模型。此外，它们的巨大体积和通过API的受限访问使得针对任务的微调不切实际。而且，LLMs对提示的不同方面（例如，演示的选择和顺序）很敏感，因此可能需要耗费时间进行提示工程。因此，我们提出了一种方法，可以在不依赖其权重的情况下纠正LLM的输出。首先，我们通过少样本提示LLM生成一个候选池。其次，我们使用一个更小的模型，LM-corrector（LMCor）来改进LLM生成的输出。LMCor被训练用于对候选者进行排名、组合和重写，以产生最终的目标输出。我们的实验表明，即使是一个小的LMCor模型（250M），也可以显着改善LLMs（62B）的少样本性能，适用于各种任务。此外，我们还证明LMCor表现出对提示变化的改进鲁棒性和更好的泛化性。总体而言，我们的方法展示了改善LLMs实际可用性的有希望的结果。

    Large language models (LLMs) have demonstrated impressive few-shot learning capabilities, but they often underperform compared to fine-tuned models on challenging tasks. Furthermore, their large size and restricted access only through APIs make task-specific fine-tuning impractical. Moreover, LLMs are sensitive to different aspects of prompts (e.g., the selection and order of demonstrations) and can thus require time-consuming prompt engineering. In this light, we propose a method to correct LLM outputs without relying on their weights. First, we generate a pool of candidates by few-shot prompting an LLM. Second, we refine the LLM-generated outputs using a smaller model, the LM-corrector (LMCor), which is trained to rank, combine and rewrite the candidates to produce the final target output. Our experiments demonstrate that even a small LMCor model (250M) substantially improves the few-shot performance of LLMs (62B) across diverse tasks. Moreover, we illustrate that the LMCor exhibits 
    
[^77]: 从自然语言定义中学习多关系双曲词向量

    Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])

    [http://arxiv.org/abs/2305.07303](http://arxiv.org/abs/2305.07303)

    本论文提出了一种从自然语言定义中学习多关系双曲词向量的框架，以捕捉由定义所引起的分层和多分辨率结构。

    

    仅使用分布信息的神经词向量一直以来都能为下游任务提供有用的含义表示。然而，现有的方法通常会导致难以解释和控制的表示。相反，自然语言定义具有递归的，自说明的语义结构，可以支持能够保留向量空间中显式概念关系和约束的新型表示学习范 paradigm。本文提出了一个神经符号、多关系框架，通过联合映射定义和定义术语及其相应的语义关系，仅从自然语言定义中学习词向量。通过自动从定义语料库中提取关系，并通过一个翻译目标规范化学习问题，我们将框架专门设定为在双曲空间中捕获由定义引起的分层和多分辨率结构。

    Neural-based word embeddings using solely distributional information have consistently produced useful meaning representations for downstream tasks. However, existing approaches often result in representations that are hard to interpret and control. Natural language definitions, on the other side, possess a recursive, self-explanatory semantic structure that can support novel representation learning paradigms able to preserve explicit conceptual relations and constraints in the vector space.  This paper proposes a neuro-symbolic, multi-relational framework to learn word embeddings exclusively from natural language definitions by jointly mapping defined and defining terms along with their corresponding semantic relations. By automatically extracting the relations from definitions corpora and formalising the learning problem via a translational objective, we specialise the framework in hyperbolic space to capture the hierarchical and multi-resolution structure induced by the definitions.
    
[^78]: 星辰即你所需：用远程监督金字塔网络进行文档级端到端情感分析

    Stars Are All You Need: A Distantly Supervised Pyramid Network for Document-Level End-to-End Sentiment Analysis. (arXiv:2305.01710v1 [cs.CL])

    [http://arxiv.org/abs/2305.01710](http://arxiv.org/abs/2305.01710)

    本文提出了一种文档级端到端情感分析方法，通过星级评分标签，实现方面检测、情感分析和评分预测，具有良好的性能和可解释性。

    

    本文提出了文档级端到端情感分析方法，可以通过星级评分标签对在线评论中表达的方面和评论情感进行有效的统一分析。我们假设星级评分标签是评论中各方面评分的“粗粒度综合”。我们提出了一种远程监督的金字塔网络（DSPN），只用文档星级评分标签进行训练，即可有效地执行方面-类别检测、方面-类别情感分析和评分预测。通过以端到端的方式执行这三个相关的情感子任务，DSPN可以提取评论中提到的方面，确定相应的情感，并预测星级评分标签。我们在英文和汉语多方面评论数据集上评估了DSPN，发现仅使用星级评分标签进行监督，DSPN的性能与各种基准模型相当。我们还展示了DSPN在评论上的可解释性输出，以说明金字塔网络的结构。

    In this paper, we propose document-level end-to-end sentiment analysis to efficiently understand aspect and review sentiment expressed in online reviews in a unified manner. In particular, we assume that star rating labels are a "coarse-grained synthesis" of aspect ratings across in the review. We propose a Distantly Supervised Pyramid Network (DSPN) to efficiently perform Aspect-Category Detection, Aspect-Category Sentiment Analysis, and Rating Prediction using only document star rating labels for training. By performing these three related sentiment subtasks in an end-to-end manner, DSPN can extract aspects mentioned in the review, identify the corresponding sentiments, and predict the star rating labels. We evaluate DSPN on multi-aspect review datasets in English and Chinese and find that with only star rating labels for supervision, DSPN can perform comparably well to a variety of benchmark models. We also demonstrate the interpretability of DSPN's outputs on reviews to show the py
    

