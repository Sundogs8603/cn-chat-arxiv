# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [LLM-based NLG Evaluation: Current Status and Challenges](https://rss.arxiv.org/abs/2402.01383) | 这项调研介绍了基于大型语言模型（LLM）的自然语言生成（NLG）评估方法的现状，包括基于LLM导出的指标、引导LLM和使用带有标记评估数据的LLM微调，并讨论了人类与LLM的合作。同时指出了该领域的一些挑战和未来研究方向。 |
| [^2] | [Pok\'eLLMon: A Human-Parity Agent for Pok\'emon Battles with Large Language Models](https://rss.arxiv.org/abs/2402.01118) | Pok\'eLLMon是第一个在战术对战游戏中实现人类能力的语言模型化身代理机器人。它通过上下文强化学习、知识增强生成和一致的行动生成的策略，展现了与人类类似的战斗策略和及时决策，并在Ladder比赛中达到了49%的胜率，在邀请对战中达到了56%的胜率。 |
| [^3] | [Eight Methods to Evaluate Robust Unlearning in LLMs](https://arxiv.org/abs/2402.16835) | 本文调查了现有遗忘评估方法的技术和局限性，并在"Who's Harry Potter" (WHP)模型上应用了一系列测试，发现WHP的遗忘表现具有泛化能力、与原始模型相当，并在相关领域存在旁路遗忘。 |
| [^4] | [Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections](https://arxiv.org/abs/2402.16832) | MLLMs通过微调获得了特定领域的视觉能力，但投影并未提取相关的领域特定视觉属性。 |
| [^5] | [SKILL: Similarity-aware Knowledge distILLation for Speech Self-Supervised Learning](https://arxiv.org/abs/2402.16830) | SKILL是一种新颖的面向语音自监督学习的知识蒸馏方法，通过在层组之间进行蒸馏，而不是蒸馏教师网络中任意选择的单个层，通过层次聚类程序确定要蒸馏的层，实现了超越DPHuBERT的性能，并在30M参数模型类别中在几个SUPERB任务中取得了最先进的结果。 |
| [^6] | [GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning](https://arxiv.org/abs/2402.16829) | GISTEmbed通过引导模型增强批内负例选择，摆脱随机采样和等效用假设，降低数据质量问题带来的噪声，从而提高模型微调效果。 |
| [^7] | [A Survey on Data Selection for Language Models](https://arxiv.org/abs/2402.16827) | 大型语言模型成功的关键在于使用大规模的文本数据集进行无监督预训练，但如何优化选择数据以降低碳足迹和财务成本仍是一个挑战。 |
| [^8] | [Language Agents as Optimizable Graphs](https://arxiv.org/abs/2402.16823) | 将基于LLM的代理统一描述为计算图，提出新颖的自动图优化器来改进节点和边，实现了代理之间的自动协作和改进。 |
| [^9] | [Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts](https://arxiv.org/abs/2402.16822) | Rainbow Teaming提出了一种新方法，通过开放式搜索生成多样化的对抗性提示，可以帮助改善大型语言模型的稳健性，提高安全性，问答和网络安全等领域的模型漏洞。 |
| [^10] | [Nemotron-4 15B Technical Report](https://arxiv.org/abs/2402.16819) | Nemotron-4 15B是一个150亿参数的大型多语言模型，在多语言能力上表现优异，超过其他规模相似的模型。 |
| [^11] | [Investigating the Effectiveness of HyperTuning via Gisting](https://arxiv.org/abs/2402.16817) | Gisting方法可用于训练模型将信息压缩为更少的标记表示，构建的HyperLlama模型可以有效地将信息从少量示例压缩成软前缀，并为进一步的前缀微调提供更好的初始化。 |
| [^12] | [OncoGPT: A Medical Conversational Model Tailored with Oncology Domain Expertise on a Large Language Model Meta-AI (LLaMA)](https://arxiv.org/abs/2402.16810) | 本研究开发了一个专门针对肿瘤领域的语言模型，提高了提供肿瘤相关建议的准确性。 |
| [^13] | [Set the Clock: Temporal Alignment of Pretrained Language Models](https://arxiv.org/abs/2402.16797) | 该研究探讨了预训练语言模型的时间混乱问题，并提出了时间对齐的方法，实验证明将LMs对齐到最近时间可以显著提高性能 |
| [^14] | [Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models](https://arxiv.org/abs/2402.16786) | 挑战传统的受限评估范式，探索更真实的不受限制的对大型语言模型中价值观和观点的评估方法。 |
| [^15] | [A Comprehensive Evaluation of Quantization Strategies for Large Language Models](https://arxiv.org/abs/2402.16775) | 该研究提出了一个结构化评估框架，针对大型语言模型的量化策略展开全面评估，填补了现有研究中的空白。 |
| [^16] | [CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks](https://arxiv.org/abs/2402.16767) | 本研究介绍了知识密集型语言任务中的持续文档学习任务，并建立了一个新的评估数据集，旨在探索检索模型有效处理动态检索场景的能力。 |
| [^17] | [Value Preferences Estimation and Disambiguation in Hybrid Participatory Systems](https://arxiv.org/abs/2402.16751) | 本研究针对混合参与式系统中的价值偏好估计提出了新方法，通过与参与者互动解决了选择与动机之间的冲突，并重点比较了从动机中估计的价值与仅从选择中估计的价值。 |
| [^18] | [DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing](https://arxiv.org/abs/2402.16733) | 本文发布了一个大型标准数据集DREsS，用于基于评分标准的自动作文评分，在提出了一种基于破坏的作文增强策略CASE后，这个数据集的基线结果提高了45.44％。 |
| [^19] | [CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models](https://arxiv.org/abs/2402.16717) | 该论文提出了一种基于个性化加密策略的新型越狱框架CodeChameleon，通过重塑任务格式和嵌入解密功能，成功应对大型语言模型面临的安全挑战。 |
| [^20] | [Quantum linear algebra is all you need for Transformer architectures](https://arxiv.org/abs/2402.16714) | 本文研究了在容错性量子计算的视角下Transformer架构，展示了如何利用量子线性代数构建Transformer的关键组件。 |
| [^21] | [SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection](https://arxiv.org/abs/2402.16705) | SelectIT通过利用大型语言模型本身的能力和基于不确定性的方法，提出了一种无需额外资源的高效选择指导调整数据集的方法，进而提升了模型的能力。 |
| [^22] | [Generating Effective Ensembles for Sentiment Analysis](https://arxiv.org/abs/2402.16700) | 通过Hierarchical Ensemble Construction（HEC）算法将传统NLP模型与转换器模型结合，能够显著优于传统集成模型，提高情感分析准确性。 |
| [^23] | [Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models](https://arxiv.org/abs/2402.16696) | 提出了一种决策感知和可泛化的工具使用框架，以帮助大型语言模型在操作工具时提高灵活性和泛化能力 |
| [^24] | [HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization](https://arxiv.org/abs/2402.16694) | HumanEval-XL 是一个面向跨语言自然语言泛化的多语言代码生成基准，建立23种自然语言和12种编程语言的联系，提供了全面的评估平台，弥补了多语言LLM评估的重要空白。 |
| [^25] | [Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study](https://arxiv.org/abs/2402.16689) | 进一步通过法语生物医学文本对英文临床模型进行预训练可以优于其他两种调整策略，结果强调了长序列法语生物医学模型在大多数下游任务上提高了性能 |
| [^26] | [StructLM: Towards Building Generalist Models for Structured Knowledge Grounding](https://arxiv.org/abs/2402.16671) | StructLM系列模型基于全面指令调整数据集，超越了大多数任务特定模型，在结构化知识连接任务中取得了新的最先进成就。 |
| [^27] | [RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation](https://arxiv.org/abs/2402.16667) | RepoAgent是一个以大型语言模型为动力的开源框架，旨在积极生成、维护和更新代码文档，验证了其在生成高质量存储库级文档方面的有效性。 |
| [^28] | [GigaPevt: Multimodal Medical Assistant](https://arxiv.org/abs/2402.16654) | GigaPevt是第一个结合大型语言模型和专业医疗模型的多模态医疗助手，在对话质量和度量性能方面表现出明显优势，并在问答任务中提高了1.18\%的准确率。 |
| [^29] | [ESG Sentiment Analysis: comparing human and language model performance including GPT](https://arxiv.org/abs/2402.16650) | 本研究比较人类和先进语言模型在衡量ESG相关情绪方面的表现。 |
| [^30] | [Domain Embeddings for Generating Complex Descriptions of Concepts in Italian Language](https://arxiv.org/abs/2402.16632) | 提出了一个新的领域嵌入方法，通过引入从电子词典中提取的信息，解决了连续语义值和离散描述之间的差距 |
| [^31] | [Long-Context Language Modeling with Parallel Context Encoding](https://arxiv.org/abs/2402.16617) | 提出了一种名为CEPE的框架，通过并行编码扩展了现有仅解码器LLMs的上下文窗口，显著降低了计算成本并在语言建模和上下文学习中取得了强大性能表现。 |
| [^32] | [Understanding the Dataset Practitioners Behind Large Language Model Development](https://arxiv.org/abs/2402.16611) | 数据质量是大型语言模型开发中数据集管理者的首要任务，但管理者间对于数据质量定义和评估方法缺乏共识。 |
| [^33] | [PAQA: Toward ProActive Open-Retrieval Question Answering](https://arxiv.org/abs/2402.16608) | 本研究针对对话式搜索系统中存在的数据集不足问题，提出了PAQA方法，通过考虑用户查询和文档中的歧义，生成相关澄清问题，从而改善对话搜索系统的效果。 |
| [^34] | [Rethinking Negative Instances for Generative Named Entity Recognition](https://arxiv.org/abs/2402.16602) | 本研究探索了在生成式命名实体识别中引入负例训练的潜力，结果表明负例的引入通过引入上下文信息和清晰划定标签边界来显著改进系统性能，并提出了一种名为Hierarchical Matching的新颖高效算法，进一步将非结构化预测转化为结构化实体。 |
| [^35] | [Semantic change detection for Slovene language: a novel dataset and an approach based on optimal transport](https://arxiv.org/abs/2402.16596) | 该论文提出了一个新的斯洛文尼亚语数据集，用于评估语义变化检测系统，并提出了一种基于最优输运的全新方法，能够将错误率降低22.8%。 |
| [^36] | [Multi-Bit Distortion-Free Watermarking for Large Language Models](https://arxiv.org/abs/2402.16578) | 该论文扩展了现有的零比特无失真水印方法，通过在水印中嵌入多比特元信息，并开发了一个高效的解码器，实现了从水印中提取信息的过程，具有很低的比特误差率。 |
| [^37] | [Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models](https://arxiv.org/abs/2402.16568) | 该论文提出了一种新颖的生成式时间知识图问答框架GenTKGQA，利用大型语言模型(LLMs)在时间知识图问答任务中的两阶段方法，即子图检索和答案生成。 |
| [^38] | [Aligning Large Language Models to a Domain-specific Graph Database](https://arxiv.org/abs/2402.16567) | 该论文提出了一种将大型语言模型对齐到特定领域的图数据库的方法，通过利用ChatGPT生成NL-GQL数据对并微调LLMs，实现了两者之间的对齐。 |
| [^39] | [Integrating Large Language Models with Graphical Session-Based Recommendation](https://arxiv.org/abs/2402.16539) | 本文提出了一种名为LLMGR的框架，将大型语言模型与图形会话推荐相结合，有效地弥合了推荐系统中大型语言模型在会话推荐领域的适应性差距 |
| [^40] | [LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification](https://arxiv.org/abs/2402.16515) | 提出一种基于LLM和分布导师的知识蒸馏引导下的隐私数据增强方法，用于医学文本分类，将DP-based伪样本生成任务转移到DP-based生成样本鉴别任务，通过教师模型教导学生如何选择带有校准噪声的私有样本以实现差分隐私。 |
| [^41] | [Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision](https://arxiv.org/abs/2402.16508) | 本研究提出了一种基于自监督方法的单个编码-解码模型来解决跨语言问答问题，通过利用维基百科内的跨语言链接结构合成监督信号，取得了优于其他方法的效果。 |
| [^42] | [LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments](https://arxiv.org/abs/2402.16499) | LLMArena是一个新颖且易于扩展的框架，用于评估大型语言模型在多代理动态环境中的多样化能力。 |
| [^43] | [On Languaging a Simulation Engine](https://arxiv.org/abs/2402.16482) | 通过三种功能化类型的语言模型，提出了一种语言到模拟（Lang2Sim）框架，实现了精准将文本描述转化为可执行模拟器输入的方法。 |
| [^44] | [mEdIT: Multilingual Text Editing via Instruction Tuning](https://arxiv.org/abs/2402.16472) | 该论文介绍了mEdIT，这是CoEdIT的多语言扩展，使用指令调整对多语言文本编辑模型进行微调训练，在多语言文本编辑任务中表现强劲。 |
| [^45] | [Unveiling Vulnerability of Self-Attention](https://arxiv.org/abs/2402.16470) | 本文通过提出HackAttend扰动技术，揭示了最先进的PLMs因微小的注意力扰动而产生的高攻击成功率，将文本攻击从词汇扰动扩展到结构扰动。 |
| [^46] | [Defending LLMs against Jailbreaking Attacks via Backtranslation](https://arxiv.org/abs/2402.16459) | 通过反向翻译来防御LLMs免受越狱攻击，将生成的反向翻译提示用于揭示原始提示的实际意图，提高了模型的安全性。 |
| [^47] | [D-XCB: Data-independent Debiasing for Fair and Accurate Transformer-based Cyberbullying Detection](https://arxiv.org/abs/2402.16458) | 该论文提出了ID-XCB，这是首个数据无关去偏技术，能够在缓解模型对引发偏见的词的关注的同时提高网络欺凌检测性能，超越了当前最先进的去偏方法。 |
| [^48] | [RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering](https://arxiv.org/abs/2402.16457) | 本研究提出了一个新的基准RetrievalQA，用于评估自适应检索增强生成技术，发现现有方法存在问题并提出了一种新的方法TA-ARE |
| [^49] | [ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors](https://arxiv.org/abs/2402.16444) | ShieldLM是一个基于LLM的安全检测器，符合一般人类安全标准，支持定制化的检测规则，并提供决策解释。 |
| [^50] | [Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models](https://arxiv.org/abs/2402.16438) | 大型语言模型中的语言特定神经元可以解释其多语能力，通过提出语言激活概率熵（LAPE）的检测方法，研究发现LLMs处理特定语言的能力主要由少量神经元决定。 |
| [^51] | [RoCoIns: Enhancing Robustness of Large Language Models through Code-Style Instructions](https://arxiv.org/abs/2402.16431) | 通过代码风格指令替换自然语言指令，提供更精确的指令并增强大型语言模型的鲁棒性。同时提出一种新方法，在少样本情景下通过组合干净和对抗样本加强模型的鲁棒性。 |
| [^52] | [Predicting Sustainable Development Goals Using Course Descriptions -- from LLMs to Conventional Foundation Models](https://arxiv.org/abs/2402.16420) | 使用LLM生成训练数据，训练语言模型来预测大学课程的可持续发展目标，有助于更好地适应SDGs。 |
| [^53] | [From RAGs to riches: Using large language models to write documents for clinical trials](https://arxiv.org/abs/2402.16406) | 本研究评估了大型语言模型（LLMs）在生成临床试验方案文件部分内容的能力，发现通过检索增强生成（RAG）可显着提高LLM的撰写质量，对LLMs在临床试验相关写作中具有重要意义。 |
| [^54] | [MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property](https://arxiv.org/abs/2402.16389) | 本文提出了一个新的多语言基准MoZIP，用于评估大型语言模型在知识产权领域的表现，并开发了一个新的IP-oriented多语言大型语言模型MoZi，实验证明MoZi在MoZIP基准上的表现优越。 |
| [^55] | [Immunization against harmful fine-tuning attacks](https://arxiv.org/abs/2402.16382) | 本文提出了一种用于防范大型语言模型中有害微调攻击的免疫条件集，以帮助理解如何构建和衡量未来的防御措施。 |
| [^56] | [An Automated End-to-End Open-Source Software for High-Quality Text-to-Speech Dataset Generation](https://arxiv.org/abs/2402.16380) | 该论文介绍了一种端到端工具，用于生成高质量的文本转语音（TTS）模型数据集，实现了语言特定的语音分布整合、自动化录制过程、自动化和人机协作的录音质量保证以及录音格式处理。 |
| [^57] | [Improving LLM-based Machine Translation with Systematic Self-Correction](https://arxiv.org/abs/2402.16379) | 引入了名为TER的系统LLM自校正翻译框架，成功帮助LLMs提高翻译质量，具有更优越的系统性和可解释性。 |
| [^58] | [Unraveling Babel: Exploring Multilingual Activation Patterns within Large Language Models](https://arxiv.org/abs/2402.16367) | 通过将原始的大型语言模型转化为专家混合架构，研究了LLMs的多语言激活模式，发现存在非特定语言的神经元和特定语言激活神经元，以及高频激活神经元可以加速推断并保持性能。 |
| [^59] | [Where Do We Go from Here? Multi-scale Allocentric Relational Inference from Natural Spatial Descriptions](https://arxiv.org/abs/2402.16364) | 论文探讨了基于自然空间描述进行多尺度空间关系推理的方法，发现通过获知地图知识得到的描述能够提供环境的整体结构。 |
| [^60] | [LLM Inference Unveiled: Survey and Roofline Model Insights](https://arxiv.org/abs/2402.16363) | 本文提出了一个基于Roofline模型的框架，用于系统分析LLM推断技术，帮助识别部署中的瓶颈，并为更有效地部署LLM提供策略。 |
| [^61] | [Layer-wise Regularized Dropout for Neural Language Models](https://arxiv.org/abs/2402.16361) | 本文提出了一种专为Transformer-based语言模型设计的新颖的分层正则化Dropout（LR-Drop）方法，通过一致性训练策略逐层对每个Transformer层进行正则化，实现了隐藏状态、多头注意力矩阵和输出分布的一致性。 |
| [^62] | [An Integrated Data Processing Framework for Pretraining Foundation Models](https://arxiv.org/abs/2402.16358) | 提出了一个集成了处理模块和分析模块的数据处理框架，旨在改善数据质量并展示其有效性。 |
| [^63] | [Language-guided Skill Learning with Temporal Variational Inference](https://arxiv.org/abs/2402.16354) | 该论文提出了一种语言引导的技能学习算法，通过整合大型语言模型生成的分割信息来发现可重用的技能，并引入最小描述长度原则来引导这一过程，实现了在不同环境中加速学习并超越基线方法的效果。 |
| [^64] | [MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs](https://arxiv.org/abs/2402.16352) | MathGenie通过问题反向翻译生成合成数据，用于增强LLMs的数学推理能力，并创造了一个家族化的模型系列MathGenieLM。 |
| [^65] | [CodeS: Towards Building Open-source Language Models for Text-to-SQL](https://arxiv.org/abs/2402.16347) | CodeS是一系列用于文本到SQL任务的开源语言模型，通过较小的参数规模实现了更高的准确性，采用了具有挑战性的SQL中心语料库进行渐进预训练来增强其SQL生成能力。 |
| [^66] | [Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation](https://arxiv.org/abs/2402.16333) | 该研究提出了一个混合框架，通过大型语言模型和代理模型对社交媒体用户进行模拟，构建了类似Twitter环境模拟他们对触发事件的反应，为社会运动模拟提供了新途径。 |
| [^67] | [Data-freeWeight Compress and Denoise for Large Language Models](https://arxiv.org/abs/2402.16319) | 无需数据参与，基于大型语言模型结构提出了一种新的权重压缩方法，可有效压缩参数矩阵并保持正交性。 |
| [^68] | [Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models](https://arxiv.org/abs/2402.16315) | Finer工作揭示了大型视觉语言模型在细粒度视觉分类上的短板，尤其是难以生成准确的细致属性解释，尽管具有生成高水平图像解释的能力。 |
| [^69] | [Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering](https://arxiv.org/abs/2402.16313) | 提出了一种Chain-of-Discussion框架，通过多个开源语言模型的协同作用，提高了复杂问题回答的质量 |
| [^70] | [Cross-domain Chinese Sentence Pattern Parsing](https://arxiv.org/abs/2402.16311) | 本文提出了一种利用大型语言模型进行自我训练的创新方法，通过动态生成训练数据将源领域句法规则与目标领域句子相结合，增强句式结构解析器对各种领域的适应能力，实验证明其在教科书和新闻领域的效果优于基于规则的基准模型1.68个百分点。 |
| [^71] | [PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering](https://arxiv.org/abs/2402.16288) | PerLTQA是一个结合了语义和情节记忆的创新QA数据集，旨在探索个性化记忆在QA任务中的应用，提供了一个全面的基准和记忆整合、检索、合成的框架 |
| [^72] | [A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction](https://arxiv.org/abs/2402.16278) | 提出了一种自匹配训练方法，通过两种本体嵌入模型捕获全局和局部信息，提高了概念子类预测的稳健性 |
| [^73] | [UniRetriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval](https://arxiv.org/abs/2402.16261) | 提出了一种UniRetriever框架，利用双编码器架构和两个损失约束实现了多任务候选者选择，适用于不同情境下的对话检索任务。 |
| [^74] | [Topic-to-essay generation with knowledge-based content selection](https://arxiv.org/abs/2402.16248) | 该论文提出了一种基于知识内容选择的复制机制模型，通过整合丰富的语义知识和改进的前缀调整方法，使主题到文章生成任务中的文本生成多样性提高，并贡献了新的中文数据集。 |
| [^75] | [Learning Translations: Emergent Communication Pretraining for Cooperative Language Acquisition](https://arxiv.org/abs/2402.16247) | 提出了一个名为合作语言习得问题（CLAP）的新颖人工智能挑战，通过允许代理在目标社区中从互动数据集中学习，放宽了Zero-Shot Coordination假设。 |
| [^76] | [HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs](https://arxiv.org/abs/2402.16211) | 该论文引入了一个自动可扩展的框架，结合LLMs的幻觉倾向与高效的幻觉检测，创建了用于基准测试的HypoTermQA数据集。 |
| [^77] | [IR2: Information Regularization for Information Retrieval](https://arxiv.org/abs/2402.16200) | 介绍了IR2，一种用于在合成数据生成过程中减少过拟合的信息正则化技术，在复杂查询的信息检索任务中表现出优越性能，同时将成本降低高达50%。 |
| [^78] | [ASEM: Enhancing Empathy in Chatbot through Attention-based Sentiment and Emotion Modeling](https://arxiv.org/abs/2402.16194) | 通过多个编码器的混合专家和专门的注意力策略，ASEM模型提供了共情回复生成所需的情感和关注性能。 |
| [^79] | [Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing](https://arxiv.org/abs/2402.16192) | 提出了一种名为SEMANTICSMOOTH的防御方法，通过聚合多个语义转换副本的预测结果来防御大型语言模型遭遇GCG、PAIR和AutoDAN攻击，同时保持了较强的正常性能。 |
| [^80] | [Attacking LLM Watermarks by Exploiting Their Strengths](https://arxiv.org/abs/2402.16187) | 现有的LLM水印系统虽然具有质量保留、鲁棒性和公开检测API等优点，但也因此容易受到各种攻击，研究者提出了一套实用指南以缓解这些攻击。 |
| [^81] | [Hitting "Probe"rty with Non-Linearity, and More](https://arxiv.org/abs/2402.16168) | 使用非线性结构探针来探究编码信息的结构，并设计了简单有效的新方法，以及可视化框架来评估语言模型中的依存树结构。 |
| [^82] | [DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem](https://arxiv.org/abs/2402.16159) | 提出了一种为开源软件系统量身定制的新颖命名实体识别技术，通过远程监督注释过程、语言启发、查找表、外部知识源和主动学习方法来提高模型性能，有效缓解了成本和专家标注人员稀缺问题，并在关系抽取下游任务中显著优于LLMs。 |
| [^83] | [ChatMusician: Understanding and Generating Music Intrinsically with LLM](https://arxiv.org/abs/2402.16153) | ChatMusician 是一个集成了内在音乐能力的开源LLM，通过对文本兼容的音乐表示法进行持续预训练和微调，能够理解和生成音乐，表现优于GPT-4基准模型。 |
| [^84] | [From Text to Transformation: A Comprehensive Review of Large Language Models' Versatility](https://arxiv.org/abs/2402.16142) | 该研究探讨了大型语言模型在各领域的多功能性，提出了LLMs在健身、城市规划、气候建模和灾难响应等领域中的潜在影响和创新方法。 |
| [^85] | [PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization](https://arxiv.org/abs/2402.16141) | 提出了PeriodicLoRA（PLoRA）来打破LoRA优化中的低秩瓶颈，通过多次累积低秩更新矩阵来实现更高的更新秩，从而提高性能。 |
| [^86] | [What Generative Artificial Intelligence Means for Terminological Definitions](https://arxiv.org/abs/2402.16139) | 生成人工智能工具如ChatGPT在提供定制化的语境特定含义方面表现出色，但在准确性方面存在挑战，可以辅助术语学家进行术语编纂，实现AI效率与人类专业知识的结合。 |
| [^87] | [LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by Long-Short-Term Prompting](https://arxiv.org/abs/2402.16132) | LSTPrompt提出了一种新颖的方法，将时间序列预测任务分解为短期和长期预测子任务，并为每个子任务量身定制提示，旨在提高大型语言模型在零shot时间序列预测中的适应性和性能。 |
| [^88] | [InstructEdit: Instruction-based Knowledge Editing for Large Language Models](https://arxiv.org/abs/2402.16123) | InstructEdit提出了一种基于指令的知识编辑技术，通过简单指令使编辑器适应不同任务的表现，显著提高了多任务编辑中的可靠性。 |
| [^89] | [FuseChat: Knowledge Fusion of Chat Models](https://arxiv.org/abs/2402.16107) | FuseChat通过知识融合将多个对话模型的集体知识转移到目标语言模型中，避免了昂贵的预训练成本。 |
| [^90] | [Interpreting Predictive Probabilities: Model Confidence or Human Label Variation?](https://arxiv.org/abs/2402.16102) | 提出了两种主要观点：一种认为预测概率表明模型置信度，另一种认为预测概率表明人类标签变化。作者建议同时考虑这两种观点以提高自然语言处理系统的可靠性和公平性。 |
| [^91] | [Training a Bilingual Language Model by Mapping Tokens onto a Shared Character Space](https://arxiv.org/abs/2402.16065) | 通过将标记映射到共享字符空间，研究了阿拉伯-希伯来双语语言模型训练。结果表明，使用同时表示两种语言的统一脚本的语言模型在机器翻译上表现出色，相比于保持原有脚本的模型有着更好的性能表现。 |
| [^92] | [Citation-Enhanced Generation for LLM-based Chatbot](https://arxiv.org/abs/2402.16063) | 提出一种基于引文增强的LLM聊天机器人生成方法，采用检索模块搜索支持文档来解决幻觉内容产生的问题。 |
| [^93] | [How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study](https://arxiv.org/abs/2402.16061) | 本文首次通过探究任务研究了大型语言模型逐层编码知识的能力，实验结果显示LLMs更倾向于在上层编码更多的上下文知识。 |
| [^94] | [Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression](https://arxiv.org/abs/2402.16058) | 提出了一种名为Gist-COCO的模型，通过要点压缩来帮助提示解释和工程，可以达到较高的压缩率，并且在实验中表现出优异性能。 |
| [^95] | [LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding](https://arxiv.org/abs/2402.16050) | LSTP提出了语言引导的时空提示学习方法，通过整合时间提示采样器（TPS）和空间提示求解器（SPS）以及一致的训练策略，显著提升了计算效率、时间理解和空间-时间对齐。 |
| [^96] | [LLMs with Chain-of-Thought Are Non-Causal Reasoners](https://arxiv.org/abs/2402.16048) | 本文探讨了大型语言模型在推理过程中思维链条（CoT）的作用，发现LLMs在答案生成过程中与人类推理存在差异，相关因素包括语境学习、有监督微调以及对人类反馈的强化学习。 |
| [^97] | [Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy](https://arxiv.org/abs/2402.16041) | 通过多种群意识优化检测机器生成文本的最大均值离差，解决了机器生成文本与人工编写文本之间微妙的分布差异挑战。 |
| [^98] | [EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings](https://arxiv.org/abs/2402.16040) | 该研究介绍了EHRNoteQA，这是一个新颖的患者特定问题回答基准，旨在评估临床环境中的大型语言模型（LLMs），具有采用多项选择问题回答格式和需要分析多篇临床笔记的特点。 |
| [^99] | [Understanding Public Perceptions of AI Conversational Agents: A Cross-Cultural Analysis](https://arxiv.org/abs/2402.16039) | 该研究通过分析社交媒体讨论，比较了美国和中国人对AI会话代理的看法，发现中国参与者更愉悦地看待CAs，而美国参与者则更看重其功能性，温暖的看法是两国对CAs产生积极情绪的关键因素。 |
| [^100] | [Deep Learning Approaches for Improving Question Answering Systems in Hepatocellular Carcinoma Research](https://arxiv.org/abs/2402.16038) | 深度学习技术在问答系统领域取得的成就，尤其是在肝细胞癌研究中，极大地推动了自然语言处理的发展。 |
| [^101] | [Text Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations](https://arxiv.org/abs/2402.16035) | 本文回顾了Transformer预训练模型在电子商务领域的核心应用，包括文本理解和生成推荐系统等方面，在自动生成产品描述、情感分析、个性化推荐系统构建和客服对话自动处理等方面均取得了积极效果。 |
| [^102] | [Emotion Classification in Short English Texts using Deep Learning Techniques](https://arxiv.org/abs/2402.16034) | 该研究使用深度学习技术在短英文文本中识别情绪，发现基于迁移学习和BERT的文本嵌入方法在分类准确性上表现优异。 |
| [^103] | [Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration](https://arxiv.org/abs/2402.16030) | 本文提出了一种基于价值的校准（VCB）方法，以解决大型语言模型与人类偏好之间的对齐问题，并在实验中表现出比现有方法更好的通用性、稳健性和稳定性。 |
| [^104] | [GraphWiz: An Instruction-Following Language Model for Graph Problems](https://arxiv.org/abs/2402.16029) | GraphWiz是一个开源语言模型，通过引入指令调优数据集和直接偏好优化框架，能够高效解决各种图问题类型，平均准确率达到65%，超过了GPT-4的43.8%。 |
| [^105] | [HiGPT: Heterogeneous Graph Language Model](https://arxiv.org/abs/2402.16024) | 该论文提出了HiGPT模型，致力于解决异质图学习中存在的泛化限制和分布不稳定性问题。 |
| [^106] | [TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages](https://arxiv.org/abs/2402.16021) | 将不同模态解释为不同语言，在语音、图像和文本之间实现了三模翻译，大大减少了计算成本。 |
| [^107] | [PST-Bench: Tracing and Benchmarking the Source of Publications](https://arxiv.org/abs/2402.16009) | 该论文研究了论文来源追踪问题，在计算机科学领域构建了高质量且不断增长的数据集 PST-Bench，揭示了不同主题之间的演化模式差异，并强调了该问题的难度和潜在研究方向。 |
| [^108] | [From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings](https://arxiv.org/abs/2402.16006) | 通过Adversarial Suffixes Embedding Translation Framework，将不可读的敌对后缀翻译为连贯、可读的文本，有助于更容易理解和分析大型语言模型生成有害内容的原因。 |
| [^109] | [A Machine Learning Approach to Detect Customer Satisfaction From Multiple Tweet Parameters](https://arxiv.org/abs/2402.15992) | 使用机器学习方法分析推文以确定客户满意水平，有助于简化研究成千上万条推文并改进航空公司服务的繁琐过程。 |
| [^110] | [$C^3$: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding](https://arxiv.org/abs/2402.15991) | 本研究提出了一种置信度校准模型级联方法，用于增强跨语言自然语言理解任务中小模型的推断效率并解决深度模型过度自信和跨语言置信度分布变化的问题。 |
| [^111] | [Likelihood-based Mitigation of Evaluation Bias in Large Language Models](https://arxiv.org/abs/2402.15987) | 该论文研究了基于大型语言模型（LLM）的评估器中的似然偏差，并提出了一种缓解这种偏差的方法。 |
| [^112] | [Phonetic and Lexical Discovery of a Canine Language using HuBERT](https://arxiv.org/abs/2402.15985) | 使用HuBERT实现了对犬叫声的声素标签分类和词汇识别，发现了具有显著声学一致性的犬词汇，还开发了一个Web系统标记狗叫声中的声素n-gram。 |
| [^113] | [Direct Punjabi to English speech translation using discrete units](https://arxiv.org/abs/2402.15967) | 论文致力于为低资源语言进行研究，提出了一个用于旁遮普语到英语的直接语音翻译模型，以缓解全球语音技术覆盖不足的问题。 |
| [^114] | [GreenLLaMA: A Framework for Detoxification with Explanations](https://arxiv.org/abs/2402.15951) | GreenLLaMA是一种全面的端到端解毒框架，通过跨平台语料库训练出的模型优于当前最先进的模型。 |
| [^115] | [Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models](https://arxiv.org/abs/2402.15938) | 本文提出了一种通过LLMs输出分布进行污染检测的方法CDD，以及一种基于LLMs输出修正的可信评估方法TED，以应对大语言模型在数据污染和可信评估方面面临的挑战。 |
| [^116] | [Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA](https://arxiv.org/abs/2402.15933) | 通过问题条件的2D视图选择过程和双分支Transformer结构，将2D知识整合到3D-VQA系统中，从而弥补了当前方法在3D视觉问答中遇到的挑战。 |
| [^117] | [Frustratingly Simple Prompting-based Text Denoising](https://arxiv.org/abs/2402.15931) | 通过简单的文本去噪技术，本文挑战了传统观点，发现了对于自动作文评分任务的新视角，并强调了如何通过微小改变数据集可以提高最终结果。 |
| [^118] | [Evaluating Prompting Strategies for Grammatical Error Correction Based on Language Proficiency](https://arxiv.org/abs/2402.15930) | 本研究通过研究LLM的表现和第二语言能力之间的互动，针对英语作为外语学习者的不同能力水平，评估了语法错误纠正的提示策略，发现过度纠正主要发生在高级语言学习者的写作中。 |
| [^119] | [QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs](https://arxiv.org/abs/2402.15929) | 本文提出了一种新颖的认证框架QuaCer-C，用于正式认证大型语言模型中知识理解的能力，证书定量化且包含高置信度的概率界限，研究发现，随着参数数量的增加，知识理解能力提高，Mistral模型在这一评估中表现不如其他模型。 |
| [^120] | [MultiContrievers: Analysis of Dense Retrieval Representations](https://arxiv.org/abs/2402.15925) | 该论文对稠密检索器的信息捕获进行了分析，探讨了其与语言模型的比较、信息提取的可行性以及提取性与性能、性别偏见的关系。 |
| [^121] | [PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails](https://arxiv.org/abs/2402.15911) | PRP攻击策略成功地针对多种开源和闭源的守护模型实施了两步前缀攻击，有效跨越多个威胁模型。 |
| [^122] | [SemEval-2024 Task 8: Weighted Layer Averaging RoBERTa for Black-Box Machine-Generated Text Detection](https://arxiv.org/abs/2402.15873) | 本文介绍了SemEval-2024任务8的黑匣子机器生成文本检测中采用的加权层平均RoBERTa技术及结果 |
| [^123] | [SportQA: A Benchmark for Sports Understanding in Large Language Models](https://arxiv.org/abs/2402.15862) | SportQA是一个新的基准测试，旨在评估大型语言模型在体育理解方面的表现，包含超过70,000个问题涵盖不同难度级别的体育知识，并揭示了LLMs在基本体育知识上表现优异但在复杂情境推理方面存在挑战。 |
| [^124] | [MATHWELL: Generating Educational Math Word Problems at Scale](https://arxiv.org/abs/2402.15861) | 使用MATHWELL模型生成了迄今为止最大的英文数学应用题数据集，其中包含20,490个问题，经领域专家评分结果显示，MATHWELL的问题中具有可执行解决方案并符合所有标准的份额比其他选择高出40%，其中74%的可解决问题同时做到了准确和适当。 |
| [^125] | [Prompt Perturbation Consistency Learning for Robust Language Models](https://arxiv.org/abs/2402.15833) | 微调大型语言模型可以产生与判别模型相当的性能，在分析和解决LLMs对输入提示中不同类型扰动的鲁棒性方面取得了重要进展 |
| [^126] | [Linguistic Intelligence in Large Language Models for Telecommunications](https://arxiv.org/abs/2402.15818) | 本研究评估了大型语言模型在电信领域的语言智能知识和理解能力，通过对四个著名模型的零-shot评估，比较了它们在资源受限环境中的表现。 |
| [^127] | [A Theoretical Result on the Inductive Bias of RNN Language Models](https://arxiv.org/abs/2402.15814) | RNN语言模型能够有效表示更大类别的语言模型，具有有界堆栈和广义堆栈更新函数，类似于保留固定数量符号记忆并使用简单更新机制的自动机。 |
| [^128] | [Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method](https://arxiv.org/abs/2402.15813) | 首次将谈判任务形式化描述为不完全信息的不对称游戏，提出了一种用于评估代理表现的方法，并发现扮演买方的难度大且模型大小不能有效提高买方表现，为此提出了一种名为OG-Narrator的新方法。 |
| [^129] | [OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining](https://arxiv.org/abs/2402.15810) | OAG-Bench是一个基于开放学术图的全面、多方面和精细化人工筛选基准，涵盖了多个任务、数据集、基准和实验结果，旨在促进学术图挖掘。 |
| [^130] | [Empowering Large Language Model Agents through Action Learning](https://arxiv.org/abs/2402.15809) | 学习新动作的能力对于大型语言模型代理的学习进步至关重要，本研究提出了开放式行为学习框架，通过迭代学习策略改进动作，增强代理的学习效果。 |
| [^131] | [Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2402.15764) | PEP提出了一个新方法来改善LLMs的数学能力，通过在推理之前细化和阐明问题背景，提升全局上下文建模能力，减少解析困难。 |
| [^132] | [Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens](https://arxiv.org/abs/2402.15758) | 提出了Chimera框架，用于加速大型语言模型推理，通过引入轻量级的草稿模型和两种策略，利用先前生成的令牌来预测后续单词，以解决解码过程中的准确性和效率问题 |
| [^133] | [Dental Severity Assessment through Few-shot Learning and SBERT Fine-tuning](https://arxiv.org/abs/2402.15755) | 通过Few-shot Learning和SBERT Fine-tuning方法，研究发现该方法在口腔健康问题的严重性评估中表现优异，准确率高达94.1%。 |
| [^134] | [HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition](https://arxiv.org/abs/2402.15754) | 提出了一个名为HD-Eval的框架，通过分层标准分解来对齐大型语言模型评估器与人类偏好，从而提升评估效果。 |
| [^135] | [Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning](https://arxiv.org/abs/2402.15751) | 提出了一种稀疏MeZO方法，通过仅对精心选择的参数子集应用零阶优化，实现了在零阶LLM微调中减少参数以获得更好性能的目标 |
| [^136] | [GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation](https://arxiv.org/abs/2402.15745) | GAOKAO-MM 是基于中国高考的多模态基准，为模型的能力设定人类水平要求，评估结果显示目前的LVLMs的准确率普遍不足50%。 |
| [^137] | [ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters](https://arxiv.org/abs/2402.15733) | 该论文介绍了一种用于阿拉伯字符的EEG数据集ArEEG_Chars，通过深度学习实现97%的准确率，在脑机接口中具有重要意义。 |
| [^138] | [How Do Humans Write Code? Large Models Do It the Same Way Too](https://arxiv.org/abs/2402.15729) | 大型语言模型在执行数值计算时经常出错，通过生成可执行代码来解决问题可以减少计算错误，但观察到当大型语言模型使用代码解决数学问题时，会生成更多不正确推理；为解决这一问题，提出了一种受人类编码实践启发的简单而高效方法Human-Think Language（HTL）。 |
| [^139] | [Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models](https://arxiv.org/abs/2402.15721) | 本论文提出了Hal-Eval，一个通用和细粒度的幻觉评估框架，引入了新的幻觉分类法，专注于事件幻觉，通过生成和过滤细粒度幻觉数据来评估大型视觉语言模型对各种幻觉的处理能力。 |
| [^140] | [Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors](https://arxiv.org/abs/2402.15713) | 提出了一种对比提示学习框架，利用预训练语言模型的潜在能力解决灾难性遗忘和过拟合问题，使其成为更好的连续少样本关系提取器 |
| [^141] | [Query Augmentation by Decoding Semantics from Brain Signals](https://arxiv.org/abs/2402.15708) | 提出了一种名为Brain-Aug的方法，通过从脑信号中解码的语义信息增强查询，可以生成更准确的查询，改善文档排序性能，特别适用于模糊查询。 |
| [^142] | [CoRelation: Boosting Automatic ICD Coding Through Contextualized Code Relation Learning](https://arxiv.org/abs/2402.15700) | 通过上下文化的编码关系学习，提出了一种新的框架来增强ICD编码表示的学习，实验结果表明其相比最先进基线方法的有效性。 |
| [^143] | [Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology](https://arxiv.org/abs/2402.15690) | 该研究通过认知一致性理论为大型语言模型的越狱提示提供了心理解释，并提出了一种基于门脚-门技术的自动黑盒越狱方法。 |
| [^144] | [Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical Study](https://arxiv.org/abs/2402.15663) | 该研究调查了在药物监测事件提取中利用ChatGPT的能力，并发现尽管ChatGPT在适当的演示选择策略下表现良好，但仍不及完全微调的小型模型。 |
| [^145] | [Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics](https://arxiv.org/abs/2402.15654) | 本文探讨了在现实环境中进行物理推理问题解决的能力，并尝试解决多模态模型在对象操作和放置任务中无法正确组合知识的问题。 |
| [^146] | [Addressing Order Sensitivity of In-Context Demonstration Examples in Causal Language Models](https://arxiv.org/abs/2402.15637) | 因果语言模型更容易受到上下文示例顺序的影响，为了解决这一挑战，提出了一种信息增强和一致性增强方法。 |
| [^147] | [Fine-Grained Self-Endorsement Improves Factuality and Reasoning](https://arxiv.org/abs/2402.15631) | 提出了利用自认证框架进行细粒度事实级别比较的方法，能够更好地减轻大型语言模型生成过程中的幻觉，尤其适用于长篇生成任务。 |
| [^148] | [Language-Based User Profiles for Recommendation](https://arxiv.org/abs/2402.15623) | 通过使用以人类可读文本表示的用户偏好，提出了Language-based Factorization Model (LFM)，在冷启动环境中与传统方法相比取得了更好的表现 |
| [^149] | [Towards Efficient Active Learning in NLP via Pretrained Representations](https://arxiv.org/abs/2402.15613) | 通过在主动学习循环中使用预训练LLMs的表示，可以显著加快标记数据获取的过程，并通过微调获得最佳性能，同时大大降低计算开销。 |
| [^150] | [Selective "Selective Prediction": Reducing Unnecessary Abstention in Vision-Language Reasoning](https://arxiv.org/abs/2402.15610) | 引入了一种名为ReCoVERR的算法，能够在视觉-语言系统的推理过程中减少过度放弃，通过寻找图像中的相关线索提供额外证据来取代放弃，从而不降低预测准确性。 |
| [^151] | [Alternating Weak Triphone/BPE Alignment Supervision from Hybrid Model Improves End-to-End ASR](https://arxiv.org/abs/2402.15594) | 提出了交替弱三音素/BPE对齐监督来自混合模型改善端到端 ASR，通过弱监督和辅助任务的交替训练，显著提高了ASR性能。 |
| [^152] | [Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts](https://arxiv.org/abs/2402.15589) | 本文研究了使用不同类型/级别的提示来激发三种流行LLM，GPT-3.5、LLaMA2和PaLM2，在学术同行评审过程中自动生成元评论，并进行了详细的定性研究。 |
| [^153] | [CI w/o TN: Context Injection without Task Name for Procedure Planning](https://arxiv.org/abs/2402.15579) | 在程序规划中，本研究提出了一种新的弱监督设置，通过使用视觉起始点和目标观察的标题作为上下文信息，从而实现无需任务名称的上下文注入，从而降低了标注成本 |
| [^154] | [Social Convos: Capturing Agendas and Emotions on Social Media](https://arxiv.org/abs/2402.15571) | 本文提出了一种新颖的方法，用于从讨论特定话题的用户群之间传播的信息中提取影响指标，重点关注影响传播和检测更有意图的影响操作。 |
| [^155] | [Fast Adversarial Attacks on Language Models In One GPU Minute](https://arxiv.org/abs/2402.15570) | 介绍了一种新型的基于束搜索的快速对抗攻击方法BEAST，能够在一分钟内高成功率地越狱对齐的语言模型，同时还能导致语言模型产生幻觉。 |
| [^156] | [Speech Corpus for Korean Children with Autism Spectrum Disorder: Towards Automatic Assessment Systems](https://arxiv.org/abs/2402.15539) | 该论文介绍了专为韩国ASD儿童设计的语音语料库，旨在提升语音技术以促进发音和严重程度评估。 |
| [^157] | [Evaluating the Performance of ChatGPT for Spam Email Detection](https://arxiv.org/abs/2402.15537) | 该研究评估了ChatGPT在英文和中文电子邮件数据集中用于垃圾邮件检测的性能，并探讨了其在这一领域的潜力。 |
| [^158] | [PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain](https://arxiv.org/abs/2402.15527) | PCA-Bench 提出了一个评估多模大型语言模型综合能力的基准，引入复杂场景和错误定位能力，提高部署可靠性，并提出了自动评估协议 PCA-Eval，发现了显著的性能差异。 |
| [^159] | [Detecting misinformation through Framing Theory: the Frame Element-based Model](https://arxiv.org/abs/2402.15525) | 通过框架理论检测不实信息，提出了基于框架元素的模型，并利用大型语言模型和深度神经网络来检测不同框架下准确事实产生的误导信息。 |
| [^160] | [Beware of Words: Evaluating the Lexical Richness of Conversational Large Language Models](https://arxiv.org/abs/2402.15518) | 评估会话式大型语言模型生成文本的语言特征以及这些特征如何取决于模型参数是理解其潜在影响的关键一步。 |
| [^161] | [Large Scale Generative AI Text Applied to Sports and Music](https://arxiv.org/abs/2402.15514) | 这项工作利用生成式人工智能模型将大规模多模数据转化为连贯流畅文本，首次推出了用于体育和音乐领域的AI评论系统，并取得了显著性能提升。 |
| [^162] | [AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning](https://arxiv.org/abs/2402.15506) | AgentOhana提供了一种统一数据和训练流水线的综合解决方案，有助于克服使用大型语言模型（LLMs）进行智能体任务时的挑战。 |
| [^163] | [Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models](https://arxiv.org/abs/2402.15481) | 提出了Prejudice-Caprice Framework（PCF）来全面衡量LLMs中的歧视，考虑了它们在不同上下文中的一贯偏见偏好和偏好变化。 |
| [^164] | [ArabianGPT: Native Arabic GPT-based Large Language](https://arxiv.org/abs/2402.15313) | 提出了ArabianGPT，这是一系列专门为阿拉伯语设计的基于Transformer的模型，包括大小和复杂性不同的ArabianGPT-0.1B和ArabianGPT-0.3B，帮助弥补了本土阿拉伯语大型语言模型的不足。 |
| [^165] | [Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models](https://arxiv.org/abs/2402.15202) | 提出一种通过实例级前缀在注意力空间中进行细粒度比较，从而实现大型语言模型的细粒度脱毒的方法。 |
| [^166] | [Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach](https://arxiv.org/abs/2402.14948) | 提出了基于课程的正无标记学习 CuPUL 方法，能够显著降低嘈杂标签的影响，胜过现有方法 |
| [^167] | [LLMBind: A Unified Modality-Task Integration Framework](https://arxiv.org/abs/2402.14891) | 提出了LLMBind，一种统一的模态任务集成框架，通过将大型语言模型和预训练任务模型绑定在一起，实现了多种模态任务的灵活输入和输出组合。 |
| [^168] | [Vygotsky Distance: Measure for Benchmark Task Similarity](https://arxiv.org/abs/2402.14890) | 论文提出了一种基于相对性能而非任务属性的相似性度量方法，即“维果茨基距离”，可帮助减少评估任务数量并保持高验证质量。 |
| [^169] | [Technical Report on the Checkfor.ai AI-Generated Text Classifier](https://arxiv.org/abs/2402.14873) | Checkfor.ai AI生成文本分类器在区分大型语言模型生成文本和人类编写文本方面表现优异，提出了硬负挖掘与合成镜像训练算法，具有高准确性和泛化能力。 |
| [^170] | [Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment Pre-training for Noisy Slot Filling Task](https://arxiv.org/abs/2402.14494) | 提出了Noise-BERT框架，包含噪声对齐预训练任务，通过对比学习损失和对抗攻击训练策略，以提高在嘈杂环境下的槽填充任务表现。 |
| [^171] | [On the Tip of the Tongue: Analyzing Conceptual Representation in Large Language Models with Reverse-Dictionary Probe](https://arxiv.org/abs/2402.14404) | 该论文通过重新利用反向词典任务的案例研究，探查了大型语言模型对概念推理的能力，发现模型在该任务中表现出高准确性，并且表示空间编码了有关对象类别和细粒度特征的信息，同时还发现该任务探查的概念推理能力能够预测模型在多个基准测试中的一般推理表现。 |
| [^172] | [Towards Building Multilingual Language Model for Medicine](https://arxiv.org/abs/2402.13963) | 本文提出了为医学领域构建多语言语言模型的三个关键贡献:构建了新的多语言医学语料库MMedC，提出了多语言医学多选问答基准MMedBench，并且通过在MMedC上进一步训练获得了性能优越的MMedLM 2模型。 |
| [^173] | [Kuaiji: the First Chinese Accounting Large Language Model](https://arxiv.org/abs/2402.13866) | Kuaiji是第一个中国会计大型语言模型，通过Baichuan框架精心调整，支持的CAtAcctQA数据集，展现出卓越的准确性和响应速度，具有开创性地创建了中国会计数据集，并证实了在真实会计场景中的高效性。 |
| [^174] | [$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens](https://arxiv.org/abs/2402.13718) | 提出了$\infty$Bench，第一个以平均数据长度超过10万个令牌的LLM基准，用于评估处理长上下文的能力 |
| [^175] | [GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick](https://arxiv.org/abs/2402.12948) | 通过开发 GumbelSoft 水印，我们提出了一种能够在高多样性环境中增强生成文本多样性的解决方案，相较于其他方案表现更为优秀。 |
| [^176] | [Archer: A Human-Labeled Text-to-SQL Dataset with Arithmetic, Commonsense and Hypothetical Reasoning](https://arxiv.org/abs/2402.12554) | Archer数据集是一个具有挑战性的双语文本到SQL数据集，包含算术、常识和假设推理的复杂推理，挑战了当前最先进模型的能力。 |
| [^177] | [AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/abs/2402.12226) | AnyGPT是一个统一的多模态语言模型，通过离散表示实现各种模态的统一处理，能够在不改变大型语言模型架构或训练方式的情况下稳定训练，为新模态的无缝整合提供了可能。 |
| [^178] | [Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning](https://arxiv.org/abs/2402.12177) | Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。 |
| [^179] | [Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.12168) | PEFT相对于全参数微调更容易受到权重投毒后门攻击的影响，提出了一个通过置信度识别受污染样本的毒化样本识别模块（PSIM），为权重投毒后门攻击提供稳健防御 |
| [^180] | [Language Model Adaptation to Specialized Domains through Selective Masking based on Genre and Topical Characteristics](https://arxiv.org/abs/2402.12036) | 通过排名关键词并指导屏蔽过程，这项研究提出了一种利用体裁和主题信息定制语言模型适应专业领域的创新方法。 |
| [^181] | [How Interpretable are Reasoning Explanations from Prompting Large Language Models?](https://arxiv.org/abs/2402.11863) | 对大型语言模型推理解释提出了全面、多角度的评估，包括忠实度、强健性和效用，并引入了新的可解释性指标。 |
| [^182] | [Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark](https://arxiv.org/abs/2402.11592) | 本研究提出了一种不使用反向传播的零阶优化方法，用于降低LLM微调中的内存成本，通过全面的基准研究扩展了对不同的ZO优化技术的探索。 |
| [^183] | [Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection](https://arxiv.org/abs/2402.11406) | 本文研究了LLMs在检测隐式仇恨言论和表达信心时的能力，发现LLMs存在两个极端：对可能引起公平问题的群体或话题表现出过于敏感，同时置信度评分过度集中在一个范围内。 |
| [^184] | [Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model](https://arxiv.org/abs/2402.10965) | 大型语言模型在医疗健康领域有着重要作用，然而它们的泛化效果取决于在不同临床环境和人群中的表现，对于泛化能力不足的原因进行了分析，发现在样本较少的医院和特定人群中存在挑战。 |
| [^185] | [Do Llamas Work in English? On the Latent Language of Multilingual Transformers](https://arxiv.org/abs/2402.10588) | 本研究通过对Llama-2系列变压器模型的研究发现，在多语言语言模型中存在英语作为内部枢纽语言的现象，这有助于理解语言模型的功能方式以及语言偏见的起源。 |
| [^186] | [Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment](https://arxiv.org/abs/2402.10207) | 本文介绍了Rewards-in-Context（RiC）方法，该方法通过多个奖励条件控制基础模型的响应，并应用有监督的微调进行对齐。它具有简单性和适应性，并支持在推理时动态调整用户偏好。 |
| [^187] | [SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding](https://arxiv.org/abs/2402.08983) | 本文提出了一种名为SafeDecoding的解决方案，通过安全感知解码策略来防御大型语言模型（LLMs）的越狱攻击。该策略可以生成对用户查询有益且无害的响应，有效缓解了LLMs安全性威胁。 |
| [^188] | [Plausible Extractive Rationalization through Semi-Supervised Entailment Signal](https://arxiv.org/abs/2402.08479) | 本文通过半监督方法，采用蕴涵对齐，以优化可行性，提取有理的方式提供一个可解释的替代模型 |
| [^189] | [Explicit References to Social Values in Fairy Tales: A Comparison between Three European Cultures](https://arxiv.org/abs/2402.08318) | 研究了葡萄牙、意大利和德国童话中明确表达的价值观差异，使用词嵌入技术和罗盘量化分析。初步发现表明这些国家之间存在共享的文化理解和对善良、遵从和普遍价值观的表达。 |
| [^190] | [Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering](https://arxiv.org/abs/2402.08277) | 这项工作探索了如何鲁棒地微调大型语言模型以提高答案的来源质量和答案归因能力，引入了数据生成流水线和四个测试集来评估模型的性能，并展示了在合成数据上微调可以改善内部和外部分布的性能。 |
| [^191] | [Policy Improvement using Language Feedback Models](https://arxiv.org/abs/2402.07876) | 本文介绍了一种使用语言反馈模型（LFMs）改进政策的方法，通过识别期望的行为并进行模仿学习，我们在任务完成率、泛化性能和人类可解释性方面取得了显著改进。 |
| [^192] | [On Provable Length and Compositional Generalization](https://arxiv.org/abs/2402.04875) | 本研究针对包括深度集合、变压器、状态空间模型和简单递归神经网络等多种架构，探索了可证明的长度和组合泛化，认为对于长度和组合泛化，不同架构需要不同程度的表示识别。 |
| [^193] | [RevOrder: A Novel Method for Enhanced Arithmetic in Language Models](https://arxiv.org/abs/2402.03822) | 本文提出了一种名为RevOrder的新方法，通过翻转加法、减法和nD乘以1D的输出数字，显著改善了语言模型中的算术运算。经过全面测试，RevOrder在基本算术运算中达到了完美准确度，并在除法任务中提升了语言模型性能，特别是在处理大数时。在GSM8K数学任务中应用RevOrder进行微调，有效降低了错误率并提高了总体得分。 |
| [^194] | [Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification](https://arxiv.org/abs/2402.03780) | 本文通过分析文体线索比较人类注释和机器分类的方法，揭示了宣传语言的特征，并提出了一个多源、多语言、多模态的数据集PPN。结果表明，人类注释者能够可靠地区分宣传新闻和常规新闻。研究还比较了不同的自然语言处理技术，并提供了一些有关文体线索的发现。 |
| [^195] | [Rendering Graphs for Graph Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2402.02130) | 本文在多模态大型语言模型中为图推理任务引入了视觉信息，并提出了一个新的基准测试数据集GITQA。实验证明，结合文本和视觉信息的结果比单一模态效果更好。 |
| [^196] | [Contextualization Distillation from Large Language Model for Knowledge Graph Completion](https://arxiv.org/abs/2402.01729) | 本论文介绍了一种从大型语言模型中提取上下文信息用于知识图谱补全的策略，该策略能克服现有语料库的限制，显著提高了预训练语言模型在知识图谱补全中的性能。 |
| [^197] | [Does \textsc{DetectGPT} Fully Utilize Perturbation? Selective Perturbation on Model-Based Contrastive Learning Detector would be Better](https://arxiv.org/abs/2402.00263) | 我们提出了一种新颖的检测器，模型名，它使用选择性策略扰动和多对比学习，在减少随机屏蔽引起的信息丢失的同时，进一步提升少样本和个体输入的性能。 |
| [^198] | [Freely Long-Thinking Transformer (FraiLT)](https://arxiv.org/abs/2401.11626) | FraiLT是一个改进的变压器模型，通过递归方法和迭代编码，实现了在紧凑形式下达到较大模型的解释深度，在性能表现上优于较大模型，旨在实现更高效和可访问的语言模型。 |
| [^199] | [MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception](https://arxiv.org/abs/2401.07529) | 本论文提出了一个新的基准MM-SAP，旨在评估多模态大型语言模型在感知中的自我意识能力，填补了先前研究中忽视的领域。 |
| [^200] | [MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts](https://arxiv.org/abs/2401.04081) | 结合混合专家模型的MoE-Mamba在性能上优于Mamba和基准Transformer-MoE，达到了与Mamba相同性能的同时，训练步骤减少了2.35倍。 |
| [^201] | [Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP](https://arxiv.org/abs/2312.12430) | 引入了通过广播查询编码器实现的高效标题重新排序器和为标题重新排序定制的Sigmoid Trick损失函数，相结合在KILT知识基准测试的数据集上取得了最先进的结果。 |
| [^202] | [Mixed Distillation Helps Smaller Language Model Better Reasoning](https://arxiv.org/abs/2312.10730) | 混合蒸馏(MD)框架结合了LLMs中的Program of Thought (PoT)和Chain of Thought (CoT)能力，将多种提示技术蒸馏到较小模型中，显著增强了较小模型在各种任务中的推理能力。 |
| [^203] | [ULMA: Unified Language Model Alignment with Human Demonstration and Point-wise Preference](https://arxiv.org/abs/2312.02554) | 提出了一种逐点直接偏好优化方法，用于统一语言模型对齐，通过将人类演示和逐点偏好相结合，解决了偏好学习中存在的信息丢失和性能次优问题。 |
| [^204] | [A Machine Learning Approach Towards SKILL Code Autocompletion](https://arxiv.org/abs/2312.01921) | 本研究首次将变压器应用于SKILL代码自动补全，提出了一种新型的数据高效生成SKILL代码的方法，旨在提高硬件设计工程师的生产力 |
| [^205] | [Machine Translation for Ge'ez Language](https://arxiv.org/abs/2311.14530) | 该研究探讨了改善盖兹语机器翻译的方法，包括从相关语言进行迁移学习、优化共享词汇和标记分割方法、大型预训练模型的微调，以及在少样本情况下使用大型语言模型进行翻译，其中一种基于语言相关性的多语言神经机器翻译模型相比标准双语模型有4 BLEU的平均性能提升。 |
| [^206] | [Overview of Current Applications of Large Language Models in Various Medical Specialities](https://arxiv.org/abs/2311.12882) | 大型语言模型在医疗领域的应用概述，突出了它们在医疗质量提升中的变革性作用，重点关注了诊断和治疗领域，以及在癌症护理、皮肤科、牙科和心理健康等方面的创新应用。 |
| [^207] | [Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2311.12351) | 本论文对基于Transformer的大型语言模型架构的最新进展进行了全面调查，旨在增强其处理长上下文能力，从预训练到推断过程中进行了分类和分析。 |
| [^208] | [Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification](https://arxiv.org/abs/2311.09114) | 通过实时验证和纠正的策略，文章提出了一种名为Ever的方法，用于减轻大型语言模型生成中的虚构问题。 |
| [^209] | ["We Demand Justice!": Towards Social Context Grounding of Political Texts](https://arxiv.org/abs/2311.09106) | 该论文提出了定义计算环境中理解政治文本中模棱两可陈述所需背景的框架，并提出了挑战性的数据集，以此来分析和预测文本的真实世界背景。 |
| [^210] | [Towards A Unified View of Answer Calibration for Multi-Step Reasoning](https://arxiv.org/abs/2311.09101) | 本文总结了最近答案校准技术的分类法，从统一视角对步级和路径级答案校准进行了彻底评估，结果显示整合两种策略的优势倾向于产生最佳结果。 |
| [^211] | [Reasoning over Description Logic-based Contexts with Transformers](https://arxiv.org/abs/2311.08941) | 本研究构建了一个由描述逻辑知识库生成的合成自然语言问答数据集，以评估基于Transformer模型在丰富语境中的推理能力。 |
| [^212] | [Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?](https://arxiv.org/abs/2311.07564) | 本文研究了作者归属模型在演讲文本中区分发言人的能力，并提出了以会话演讲文本为重点的发言人归属基准。 |
| [^213] | [The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models](https://arxiv.org/abs/2311.05928) | 本研究揭示了Transformer解码器中的各向异性呈钟状曲线，最高各向异性浓度在中间层，与编码器中更均匀分布的各向异性不同，并发现嵌入的内在维度在训练初期增加，随后在训练末期出现压缩，表明更紧凑的表示形式。 |
| [^214] | [Tuning-less Object Naming with a Foundation Model](https://arxiv.org/abs/2311.04924) | 使用transformers的注意力机制，提出了一种无需微调模型即可进行对象命名的方法 |
| [^215] | [Learning to Learn for Few-shot Continual Active Learning](https://arxiv.org/abs/2311.03732) | 提出了一种简单而高效的方法，Meta-Continual Active Learning，通过元学习和经验重播解决少样本持续主动学习中的任务混淆和灾难性遗忘，进一步结合文本增强来确保泛化。 |
| [^216] | [ITEm: Unsupervised Image-Text Embedding Learning for eCommerce](https://arxiv.org/abs/2311.02084) | ITEm是面向电子商务的无监督学习模型，通过学习文本和图片的嵌入来更好地关注不同模态，扩展了BERT，并在两个任务上取得了良好的表现。 |
| [^217] | [On Bilingual Lexicon Induction with Large Language Models](https://arxiv.org/abs/2310.13995) | 本文研究了利用大型语言模型进行双语词汇识别的潜力，通过研究零次提示和少量上下文提示等方法，探讨了这种方法如何与当前BLI方法相比，并如何进行补充。 |
| [^218] | [Quality-Aware Translation Models: Efficient Generation and Quality Estimation in a Single Model](https://arxiv.org/abs/2310.06707) | 提出了一种质量感知翻译模型，通过训练NMT模型来估计其输出质量，可以在解码过程中消除额外的计算成本。 |
| [^219] | [Ask Again, Then Fail: Large Language Models' Vacillations in Judgement](https://arxiv.org/abs/2310.02174) | 目前的语言模型在面对后续问题时常常摇摆不定，研究者提出了一个后续问题机制和两个度量标准来量化这种不一致性，并开发出Unwavering-FQ框架来教导模型保持最初的正确判断，实验证明其有效性。 |
| [^220] | [Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View](https://arxiv.org/abs/2310.02124) | 通过实践实验和理论洞察，探究当代NLP系统之间的协作机制，发现某些协作策略优于先前的方法，并且优化了效率。 |
| [^221] | [Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning](https://arxiv.org/abs/2310.01061) | 提出了一种名为图推理（RoG）的新方法，通过将LLMs与KGs协同工作，实现忠实且可解释的大型语言模型推理。 |
| [^222] | [A blind spot for large language models: Supradiegetic linguistic information](https://arxiv.org/abs/2306.06794) | 大型语言模型的盲点在于其对超叙事语言信息的忽视，研究提出考虑模型如何感知语言信息有助于深入了解其能力。 |
| [^223] | [SmartTrim: Adaptive Tokens and Attention Pruning for Efficient Vision-Language Models](https://arxiv.org/abs/2305.15033) | SmartTrim 提出了一种自适应加速框架，通过识别并修剪每个层中的冗余标记表示和注意力头来提升视觉-语言模型的效率。 |
| [^224] | [Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks](https://arxiv.org/abs/2305.14965) | 该论文提出了正式化和已知越狱分类法以填补对商用大规模语言模型（LLMs）被越狱攻击的缺乏研究，调查现有的越狱方法及其在开源和商用LLMs上的有效性。 |
| [^225] | [CHEAT: A Large-scale Dataset for Detecting ChatGPT-writtEn AbsTracts](https://arxiv.org/abs/2304.12008) | 本研究提出了一个名为CHEAT的大规模数据集，用于检测ChatGPT生成的摘要；研究表明ChatGPT生成的摘要是可以被检测出来的，但随着人类参与的增加，检测的难度也在增加。 |
| [^226] | [Efficient Ensemble for Multimodal Punctuation Restoration using Time-Delay Neural Network](https://arxiv.org/abs/2302.13376) | 使用时间延迟神经网络实现了多模态标点修复的高效集成方法，相比当前最佳模型提高了1.0个F1分数，并且使用了更少的推理网络参数。 |
| [^227] | [Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data](https://arxiv.org/abs/2302.12822) | 提出了Automate-CoT策略，可以通过从小型标记数据集自动增加合理链，并修剪低质量链，构建基于标签的机器生成理由链的候选池，最终选择最佳组合。 |
| [^228] | [Error-Robust Retrieval for Chinese Spelling Check](https://arxiv.org/abs/2211.07843) | 本文提出了一种错误鲁棒的即插即用检索方法RERIC，可直接应用于现有的中文拼写检查模型，通过融合多种表示形式提高了检索的鲁棒性。 |
| [^229] | [OmDet: Large-scale vision-language multi-dataset pre-training with multimodal detection network](https://arxiv.org/abs/2209.05946) | OmDet 提出了一种语言意识的目标检测架构和创新的训练机制，利用多数据集视觉-语言预训练，从不同数据集中积累“视觉词汇”，实现以语言为条件的多模态检测网络，在目标检测、开放式词汇检测和短语定位等场景中表现出优越性能。 |
| [^230] | [The Klarna Product Page Dataset: Web Element Nomination with Graph Neural Networks and Large Language Models](https://arxiv.org/abs/2111.02168) | Klarna产品页面数据集是一个全面多样的网页集合，为解决网络自动化算法设计中数据集稀缺性问题提供了新的资源。 |
| [^231] | [SelectLLM: Can LLMs Select Important Instructions to Annotate?.](http://arxiv.org/abs/2401.16553) | 这项工作提出了一种名为SelectLLM的新方法，利用LLMs选择高质量指令。通过提示LLMs估计每个无标签指令的有用性和影响力，并使用聚类算法将指令分为多个聚类。 |
| [^232] | [HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy.](http://arxiv.org/abs/2401.15207) | HiFT是一种分层全参数微调策略，通过仅在每个训练步骤中更新参数的子集，可以显著减少GPU内存使用，并实现与参数高效微调和标准全参数微调相当的性能。 |
| [^233] | [Airavata: Introducing Hindi Instruction-tuned LLM.](http://arxiv.org/abs/2401.15006) | "Airavata"是一个针对印地语进行指令调整的LLM，通过微调OpenHathi和IndicInstruct数据集，提供更好的协助任务性能，并计划扩展到所有22种计划Indic语言。 |
| [^234] | [Parameter-Efficient Conversational Recommender System as a Language Processing Task.](http://arxiv.org/abs/2401.14194) | 本文将对话推荐系统作为一种语言处理任务进行建模，利用预训练的语言模型来编码项目、理解用户意图，通过语义匹配进行项目推荐，并生成对话。实验证明了该方法的有效性。 |
| [^235] | [Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs.](http://arxiv.org/abs/2401.10065) | 本论文研究了在大型语言模型（LLMs）中触发条件推理能力的方法，通过使用代码提示将自然语言问题转化为代码，从而在多个数据集上实现了显著的性能提升。 |
| [^236] | [Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges.](http://arxiv.org/abs/2401.08664) | 本文回顾了针对教育能力的大型语言模型研究，包括数学、写作、编程、推理和基于知识的问答，旨在探索其在构建下一代智能教育系统中的潜力和挑战。 |
| [^237] | [INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges.](http://arxiv.org/abs/2401.05273) | 本文介绍了INACIA系统，这是一个将大型语言模型整合到巴西审计法院中的系统，可以自动化案件分析的各个阶段，并展示了其在从案件文件中提取信息、评估合法性和生成司法建议方面的潜力。 |
| [^238] | [LAMPAT: Low-Rank Adaption for Multilingual Paraphrasing Using Adversarial Training.](http://arxiv.org/abs/2401.04348) | LAMPAT是第一个无监督的多语言改写模型，通过使用对抗训练，它能够在没有平行语料库的语言环境下生成改写。 |
| [^239] | [PIXAR: Auto-Regressive Language Modeling in Pixel Space.](http://arxiv.org/abs/2401.03321) | 本论文介绍了PIXAR，这是第一个像素自回归的语言模型，可以用于生成自由形式的文本作为图像，而不依赖于预定义的词汇表。同时，论文还提出了一个简单的对抗性预训练方法来解决生成非模糊文本的挑战。 |
| [^240] | [TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training.](http://arxiv.org/abs/2312.08846) | TiMix是一种将文本感知的图像混合技术用于视觉语言预训练的方法，通过集成混合数据增强技术，并从互信息的角度进行理论分析，提高了数据效率并取得了可比较的性能。 |
| [^241] | [People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection.](http://arxiv.org/abs/2311.01270) | 本论文通过自动生成的反事实增强数据（CADs）与手动生成的CADs进行比较，评估它们在提高模型鲁棒性方面的效果。结果显示，手动生成的CADs仍然是最有效的方法。 |
| [^242] | [A Study of Continual Learning Under Language Shift.](http://arxiv.org/abs/2311.01200) | 本文研究了持续学习在语言转换中的应用，发现在更新语言模型时，前向转移效果较好且与语言顺序无关，但后向转移效果可能取决于新语言的顺序和特征。 |
| [^243] | [Function Vectors in Large Language Models.](http://arxiv.org/abs/2310.15213) | 大型语言模型中存在一种简单的神经机制，将输入-输出函数表示为向量。这些函数向量在不同的上下文中具有鲁棒性，并且具有强大的因果效应。同时，它们还具有将语义向量进行组合的能力。 |
| [^244] | [O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models.](http://arxiv.org/abs/2310.14403) | O3D提出了一种基于离线数据的学习框架，利用大规模数据改进了大规模语言模型在顺序决策问题中的性能，通过自动发现可重复使用的技能，提高了模型的表现 |
| [^245] | [Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain.](http://arxiv.org/abs/2310.14053) | 这篇论文提出了一种评估大型代码语言模型自一致性的方法，并指出目前的模型在自一致性方面存在问题。 |
| [^246] | [User Inference Attacks on Large Language Models.](http://arxiv.org/abs/2310.09266) | 本论文研究了在大型语言模型上的用户推理攻击，发现LLMs对于各种微调数据集都很容易受到攻击，尤其是对于离群用户和贡献大量数据的用户。这对保护用户隐私具有重要意义。 |
| [^247] | [Dont Add, dont Miss: Effective Content Preserving Generation from Pre-Selected Text Spans.](http://arxiv.org/abs/2310.09017) | 本论文介绍了一个高质量的受控文本缩减（CTR）模型，解决了内容保留约束不充分强制执行和次优的银标签训练数据的限制，通过在训练和推理中增强内容保留约束，进一步改进了模型性能。 |
| [^248] | [Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation.](http://arxiv.org/abs/2310.07968) | 这项研究引入了零射交互个性化对象导航（ZIPON），通过使用大型语言模型（LLM）和用户反馈，解决了在未知环境中导航到个性化目标对象的问题。 |
| [^249] | [Diversity of Thought Improves Reasoning Abilities of Large Language Models.](http://arxiv.org/abs/2310.07088) | 本文提出了一种方法，通过改变输入提示来提高大规模语言模型的推理能力，从而改善模型在复杂推理场景中的表现。这种方法自动采集模型反馈，生成适合问题的多样化提示，并通过多次推理调用来集成这些多样化的提示。 |
| [^250] | [Let Models Speak Ciphers: Multiagent Debate through Embeddings.](http://arxiv.org/abs/2310.06272) | 本文引入了一种名为CIPHER的通信机制，通过去除LLMs中的标记采样步骤，让模型可以通过期望的原始Transformer输出嵌入来传达其信念，从而解决了在自然语言生成中可能存在的信息丢失风险，并提供了编码更广泛信息的优势。 |
| [^251] | [GeoLLM: Extracting Geospatial Knowledge from Large Language Models.](http://arxiv.org/abs/2310.06213) | GeoLLM是一种能够从大型语言模型中提取地理空间知识的新方法，结合来自OpenStreetMap的辅助地图数据，可以有效应用于测量人口密度和经济生计等任务。 |
| [^252] | [Label-free Node Classification on Graphs with Large Language Models (LLMS).](http://arxiv.org/abs/2310.04668) | 本文介绍了一种使用大型语言模型（LLMs）对图中节点进行无标签分类的方法，即LLM-GNN。它利用LLMs对一小部分节点进行注释，然后通过对LLMs的注释进行训练，使得GNN能够对其余大部分节点进行预测。这种方法充分发挥了GNNs和LLMs的优势，同时解决了它们在处理结构化数据方面的限制。 |
| [^253] | [Automated Evaluation of Classroom Instructional Support with LLMs and BoWs: Connecting Global Predictions to Specific Feedback.](http://arxiv.org/abs/2310.01132) | 本研究旨在利用大型语言模型和词袋模型自动估计课堂教学支持，以提供更具体、频繁和可行动的反馈给教师。实验证明，所提出的方法准确性接近于人工互评可靠性，LLM模型可以更好地捕捉到教学支持特征。 |
| [^254] | [Transformer-VQ: Linear-Time Transformers via Vector Quantization.](http://arxiv.org/abs/2309.16354) | Transformer-VQ是一种基于向量量化实现线性时间的Transformer模型，能够高效计算自注意力，在大规模实验中表现出色。 |
| [^255] | [Art or Artifice? Large Language Models and the False Promise of Creativity.](http://arxiv.org/abs/2309.14556) | 本研究通过提出创造性写作的托兰斯测验(TTCW)来评估大型语言模型(LLMs)的写作创造力。结果表明，LLM生成的故事在创意测试中通过的数量比专业作家写的故事少。此外，我们发现LLMs无法代替专家进行TTCW评估。 |
| [^256] | [Decoding Affect in Dyadic Conversations: Leveraging Semantic Similarity through Sentence Embedding.](http://arxiv.org/abs/2309.12646) | 本研究通过利用句子嵌入和语义相似性，解码了双人对话中的情感，并发现在冲突对话中，妻子的情感与语义相似性呈正相关。 |
| [^257] | [Automatic Answerability Evaluation for Question Generation.](http://arxiv.org/abs/2309.12546) | 这项工作提出了一种新颖的自动评估指标，用于评估问句生成任务中生成的问题是否可以由参考答案回答。实验证明该指标结果可靠，并与人工评价一致。并且这个指标可以补充传统的指标。 |
| [^258] | [Discrete Prompt Compression with Reinforcement Learning.](http://arxiv.org/abs/2308.08758) | 本研究提出了一种使用强化学习的离散提示压缩方法（PCRL），以解决指令调整的语言模型中嵌入训练的挑战。PCRL采用了一种计算效率高的策略网络直接编辑提示，可以灵活应用于各种类型的LM，而不需要梯度访问或标记数据。 |
| [^259] | [LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking.](http://arxiv.org/abs/2308.04945) | LLMeBench是一个灵活的框架，用于加速LLMs基准测试。它可以定制任何NLP任务和模型，无论语言，支持零和少样本学习设置，并允许用户添加新的自定义数据集。已经在31个独特的NLP任务上进行了测试，并计划将框架开源。 |
| [^260] | [Integrating large language models and active inference to understand eye movements in reading and dyslexia.](http://arxiv.org/abs/2308.04941) | 该论文提出了一种集成大型语言模型和主动推理的计算模型，用于模拟阅读过程中的眼动行为。该模型能够准确地预测和推理不同粒度的文本信息，并能够模拟阅读障碍中不适应推理效果的情况。 |
| [^261] | [A Geometric Notion of Causal Probing.](http://arxiv.org/abs/2307.15054) | 本文提出了一种几何观念的因果探测方法，通过在语言模型表示空间的子空间上进行反事实干预，优化了因果概念子空间，以实现概念控制生成。 |
| [^262] | [A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis.](http://arxiv.org/abs/2307.12856) | 这篇论文介绍了一种名为WebAgent的LLM驱动代理，通过自我经验学习，在真实网站上完成任务。该方法通过规划、总结和生成代码来提高在真实网站上的成功率。 |
| [^263] | [Revisit and Outstrip Entity Alignment: A Perspective of Generative Models.](http://arxiv.org/abs/2305.14651) | 本文重新从生成模型的角度研究了基于嵌入的实体对齐（EEA）问题，引入基于生成对抗网络的EEA方法及提出的生成的EEA（GEEA）框架，通过互相变分自动编码器（M-VAE）实现实体从一个KG转换到另一个KG，并且从随机噪声向量生成新的实体，具有较好的效果。 |
| [^264] | [Evaluating the Performance of Large Language Models on GAOKAO Benchmark.](http://arxiv.org/abs/2305.12474) | 本文介绍了一个基于高考考试问题的基准测试GAOKAO-Benchmark，用于评估大型语言模型在客观和主观问题方面的表现。通过对ChatGPT模型的评估，研究发现其在客观问题方面表现出色，同时也揭示了其不足之处和改进的方向。 |
| [^265] | [Smaller Language Models are Better Black-box Machine-Generated Text Detectors.](http://arxiv.org/abs/2305.09859) | 本文研究发现，小型语言模型更适用于作为通用文本检测器，可以更加精确地检测出机器生成的文本，而检测器和生成模型是否具有相同的架构或语料库并不会对检测性能产生显著影响。 |
| [^266] | [Exploiting Pseudo Image Captions for Multimodal Summarization.](http://arxiv.org/abs/2305.05496) | 本文研究了跨模态对比学习中（部分）误负样本的挑战，并提出了一种从更一般下界形式的指导下调节跨模态相似度的对比学习策略，以更精确地优化图像/文本锚定点和其负面文本/图像之间的互信息。 |
| [^267] | [Can Large Language Models Transform Computational Social Science?.](http://arxiv.org/abs/2305.03514) | 本文研究了大型语言模型作为计算社会科学工具的潜力。虽然在分类任务上没有优势，但在自由形式编码任务上表现优异，今后可以作为零-shot检测工具进行使用， |
| [^268] | [New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT.](http://arxiv.org/abs/2305.01181) | 本文提出了使用大型语言模型的机器翻译中的几个新方向，包括风格化MT、交互式MT和基于翻译记忆的MT，并讨论了隐私问题的解决方案。 |

# 详细

[^1]: 基于LLM的自然语言生成评估：现状与挑战

    LLM-based NLG Evaluation: Current Status and Challenges

    [https://rss.arxiv.org/abs/2402.01383](https://rss.arxiv.org/abs/2402.01383)

    这项调研介绍了基于大型语言模型（LLM）的自然语言生成（NLG）评估方法的现状，包括基于LLM导出的指标、引导LLM和使用带有标记评估数据的LLM微调，并讨论了人类与LLM的合作。同时指出了该领域的一些挑战和未来研究方向。

    

    在人工智能领域，评估自然语言生成（NLG）是一个至关重要但挑战重重的问题。传统的评估指标主要通过系统输出和参考文本之间的内容（如n-gram）重叠度来捕捉，远远不够令人满意，而近年来，大型语言模型（LLM）如ChatGPT在NLG评估方面展现出巨大潜力。已经提出了基于LLM的各种自动评估方法，包括基于LLM导出的指标、引导LLM和使用带有标记评估数据的LLM微调。在这项调研中，我们首先给出了基于LLM的NLG评估方法的分类法，并分别讨论了它们的优势和劣势。我们还讨论了人类与LLM的合作用于NLG评估。最后，我们讨论了该领域中的几个待解决的问题，并指出了未来的研究方向。

    Evaluating natural language generation (NLG) is a vital but challenging problem in artificial intelligence. Traditional evaluation metrics mainly capturing content (e.g. n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years. Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled evaluation data. In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively. We also discuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several open problems in this area and point out future research directions.
    
[^2]: Pok\'eLLMon：一个用于使用大型语言模型的Pok\'emon对战的与人类能力相当的代理机器人

    Pok\'eLLMon: A Human-Parity Agent for Pok\'emon Battles with Large Language Models

    [https://rss.arxiv.org/abs/2402.01118](https://rss.arxiv.org/abs/2402.01118)

    Pok\'eLLMon是第一个在战术对战游戏中实现人类能力的语言模型化身代理机器人。它通过上下文强化学习、知识增强生成和一致的行动生成的策略，展现了与人类类似的战斗策略和及时决策，并在Ladder比赛中达到了49%的胜率，在邀请对战中达到了56%的胜率。

    

    我们介绍了\textsc{Pok\'eLLMon}，这是第一个在战术对战游戏中实现人类能力的语言模型化身代理机器人，同时以Pok\'emon对战为例进行了证明。 \textsc{Pok\'eLLMon}的设计采用了三个关键策略：（i）上下文强化学习，即即时使用从对战中获得的基于文本的反馈来逐步完善策略；（ii）知识增强生成，即检索外部知识以对抗产生幻觉现象，并使代理机器人能够及时正确地行动；（iii）一致的行动生成，以减轻代理机器人面对强敌时的“惊慌换手”现象，使其可以逃避战斗。我们展示了与人类进行的在线对战中，\textsc{Pok\'eLLMon}采用与人类类似的战斗策略和及时决策，其在Ladder比赛中达到了49%的胜率，在邀请对战中达到了56%的胜率。我们的实现和可玩的战斗日志可以在以下链接中找到：\url{https://gith

    We introduce \textsc{Pok\'eLLMon}, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pok\'emon battles. The design of \textsc{Pok\'eLLMon} incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the \textit{panic switching} phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates \textsc{Pok\'eLLMon}'s human-like battle strategies and just-in-time decision making, achieving 49\% of win rate in the Ladder competitions and 56\% of win rate in the invited battles. Our implementation and playable battle logs are available at: \url{https://gith
    
[^3]: 评估LLMs中强大遗忘的八种方法

    Eight Methods to Evaluate Robust Unlearning in LLMs

    [https://arxiv.org/abs/2402.16835](https://arxiv.org/abs/2402.16835)

    本文调查了现有遗忘评估方法的技术和局限性，并在"Who's Harry Potter" (WHP)模型上应用了一系列测试，发现WHP的遗忘表现具有泛化能力、与原始模型相当，并在相关领域存在旁路遗忘。

    

    arXiv:2402.16835v1 类型公告：新摘要：机器遗忘可用于从大型语言模型（LLMs）中删除有害能力和记忆文本，但目前尚无标准化方法严格评估它。本文首先调查现有遗忘评估的技术和局限性。其次，我们对Eldan和Russinovich（2023年）的“谁是哈利波特”（WHP）模型的遗忘的稳健性和竞争力应用了广泛的测试集。虽然使用Eldan和Russinovich的“熟悉度”指标评估WHP的遗忘具有很好的泛化能力，但我们发现：i）可以可靠地提取高于基准线的知识量，ii）在哈利波特问答任务上，WHP的表现与原始模型相当，iii）它以与原始模型相当的方式代表潜在知识，iv）在相关领域存在旁路遗忘。总体而言，我们的结果突出了全面评估遗忘的重要性。

    arXiv:2402.16835v1 Announce Type: new  Abstract: Machine unlearning can be useful for removing harmful capabilities and memorized text from large language models (LLMs), but there are not yet standardized methods for rigorously evaluating it. In this paper, we first survey techniques and limitations of existing unlearning evaluations. Second, we apply a comprehensive set of tests for the robustness and competitiveness of unlearning in the "Who's Harry Potter" (WHP) model from Eldan and Russinovich (2023). While WHP's unlearning generalizes well when evaluated with the "Familiarity" metric from Eldan and Russinovich, we find i) higher-than-baseline amounts of knowledge can reliably be extracted, ii) WHP performs on par with the original model on Harry Potter Q&A tasks, iii) it represents latent knowledge comparably to the original model, and iv) there is collateral unlearning in related domains. Overall, our results highlight the importance of comprehensive unlearning evaluation that av
    
[^4]: 神秘的投影：多模态LLMs在没有更丰富的跨模态投影的情况下获得特定领域的视觉能力

    Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections

    [https://arxiv.org/abs/2402.16832](https://arxiv.org/abs/2402.16832)

    MLLMs通过微调获得了特定领域的视觉能力，但投影并未提取相关的领域特定视觉属性。

    

    多模态大型语言模型（MLLMs）如LLaVA和GPT-4(V)使得可以进行关于图像的通用对话。然而，现成的MLLMs可能在诸如皮肤病学和农业等领域的图像上具有有限的能力，因此必须进行微调以解锁特定领域的应用。通过对4个数据集进行实验，在两种微调设置下，我们发现随着MLLM的微调，它确实获得了特定领域的视觉能力，但这些更新并没有导致投影提取相关的领域特定视觉属性。

    arXiv:2402.16832v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable general-purpose conversations about images with the language modality. As off-the-shelf MLLMs may have limited capabilities on images from domains like dermatology and agriculture, they must be fine-tuned to unlock domain-specific applications. The prevalent architecture of current open-source MLLMs comprises two major modules: an image-language (cross-modal) projection network and a large language model. It is desirable to understand the roles of these two modules in modeling domain-specific visual attributes to inform the design of future models and streamline the interpretability efforts on the current models. To this end, via experiments on 4 datasets and under 2 fine-tuning settings, we find that as the MLLM is fine-tuned, it indeed gains domain-specific visual capabilities, but the updates do not lead to the projection extracting relevant domain-specific visual
    
[^5]: SKILL：面向语音自监督学习的相似性感知知识蒸馏

    SKILL: Similarity-aware Knowledge distILLation for Speech Self-Supervised Learning

    [https://arxiv.org/abs/2402.16830](https://arxiv.org/abs/2402.16830)

    SKILL是一种新颖的面向语音自监督学习的知识蒸馏方法，通过在层组之间进行蒸馏，而不是蒸馏教师网络中任意选择的单个层，通过层次聚类程序确定要蒸馏的层，实现了超越DPHuBERT的性能，并在30M参数模型类别中在几个SUPERB任务中取得了最先进的结果。

    

    自监督学习（SSL）在各种语音处理任务中取得了显著的成功。为了提高其效率，先前的工作通常利用压缩技术。最近一个显著的尝试是DPHuBERT，它应用联合知识蒸馏（KD）和结构化修剪来学习一个显著较小的SSL模型。本文通过引入SKILL贡献到这一研究领域，SKILL是一种新颖的方法，它通过对层组进行蒸馏，而不是对教师网络中任意选择的单个层进行蒸馏。确定要蒸馏的层是通过应用于层相似度度量的层次聚类程序实现的。大量实验表明，我们蒸馏后的WavLM Base+不仅胜过DPHuBERT，而且在30M参数模型类别中在几个SUPERB任务中实现了最先进的结果。

    arXiv:2402.16830v1 Announce Type: cross  Abstract: Self-supervised learning (SSL) has achieved remarkable success across various speech-processing tasks. To enhance its efficiency, previous works often leverage the use of compression techniques. A notable recent attempt is DPHuBERT, which applies joint knowledge distillation (KD) and structured pruning to learn a significantly smaller SSL model. In this paper, we contribute to this research domain by introducing SKILL, a novel method that conducts distillation across groups of layers instead of distilling individual arbitrarily selected layers within the teacher network. The identification of the layers to distill is achieved through a hierarchical clustering procedure applied to layer similarity measures. Extensive experiments demonstrate that our distilled version of WavLM Base+ not only outperforms DPHuBERT but also achieves state-of-the-art results in the 30M parameters model class across several SUPERB tasks.
    
[^6]: GISTEmbed：文本嵌入微调中引导样本内训练负例选择

    GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning

    [https://arxiv.org/abs/2402.16829](https://arxiv.org/abs/2402.16829)

    GISTEmbed通过引导模型增强批内负例选择，摆脱随机采样和等效用假设，降低数据质量问题带来的噪声，从而提高模型微调效果。

    

    嵌入模型对于语义搜索、个性化推荐以及生成模型的检索增强等AI应用至关重要，这需要高质量的训练数据。然而，手动数据整理的有限可扩展性促使我们需要自动化方法来确保数据完整性。传统的无监督三元组挖掘自动生成训练数据，对于嵌入模型训练至关重要，但不慎引入偏见和噪声，从而降低模型性能。针对这一问题，我们引入了GISTEmbed，一种通过引导模型在对比训练期间增强批内负例选择的新策略。这种方法摆脱了对于随机抽样和批负例等效用假设的依赖，显著降低了由数据质量问题引起的噪声，提高了模型微调效果。通过与 Massive Text Embedding Benchmark (MTEB) 进行基准测试，GISTEmbed 展示了一致的表现。

    arXiv:2402.16829v1 Announce Type: cross  Abstract: Embedding models are integral to AI applications like semantic search, personalized recommendations, and retrieval augmented generation for LLMs, necessitating high-quality training data. However, the limited scalability of manual data curation prompts the need for automated methods to ensure data integrity. Traditional unsupervised triplet mining automates training data generation, crucial for embedding model training, yet inadvertently injects biases and noise, thereby degrading model performance. Addressing this, we introduce GISTEmbed, a novel strategy that enhances in-batch negative selection during contrastive training through a guide model. This approach departs from reliance on random sampling and equal utility assumption of batch negatives, significantly reducing noise from data quality issues and improving model fine-tuning. Benchmarked against the Massive Text Embedding Benchmark (MTEB), GISTEmbed showcases consistent perfor
    
[^7]: 语言模型数据选择概述

    A Survey on Data Selection for Language Models

    [https://arxiv.org/abs/2402.16827](https://arxiv.org/abs/2402.16827)

    大型语言模型成功的关键在于使用大规模的文本数据集进行无监督预训练，但如何优化选择数据以降低碳足迹和财务成本仍是一个挑战。

    

    最近大型语言模型取得成功的一个主要因素是利用巨大且不断增长的文本数据集进行无监督预训练。然而，简单地在所有可用数据上训练模型可能并不是最佳选择（或不可行），因为可用文本数据的质量可能有所不同。数据过滤也可以通过减少所需的训练量来降低训练模型的碳足迹和财务成本。数据选择方法旨在确定要包括在训练数据集中的哪些候选数据点，以及如何从所选数据点中适当采样。改进的数据选择方法的前景已经导致该领域的研究量迅速扩大。然而，由于深度学习主要受实证证据驱动，对大规模数据进行实验成本昂贵，很少有组织拥有资源进行广泛的数据选择研究。因此，有效数据选择的知识可能大多局限于大型技术公司或研究机构内部。

    arXiv:2402.16827v1 Announce Type: new  Abstract: A major factor in the recent success of large language models is the use of enormous and ever-growing text datasets for unsupervised pre-training. However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary. Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required.   Data selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points. The promise of improved data selection methods has caused the volume of research in the area to rapidly expand. However, because deep learning is mostly driven by empirical evidence and experimentation on large-scale data is expensive, few organizations have the resources for extensive data selection research. Consequently, knowledge of effective data se
    
[^8]: 作为可优化图的语言代理

    Language Agents as Optimizable Graphs

    [https://arxiv.org/abs/2402.16823](https://arxiv.org/abs/2402.16823)

    将基于LLM的代理统一描述为计算图，提出新颖的自动图优化器来改进节点和边，实现了代理之间的自动协作和改进。

    

    多种人类设计的提升技术被提出，用于改进基于大型语言模型（LLMs）的问题求解器，产生了许多不同的代码库。我们通过将LLM代理描述为计算图来统一这些方法。节点实现处理多模态数据或查询LLMs的功能，并且边描述操作之间的信息流动。图形可以递归地组合成代表不同代理之间协作层次的更大组合图（其中边连接不同代理的操作）。我们的新颖自动图优化器（1）优化节点级LLM提示（节点优化）并（2）通过改变图连接性来改善代理协调（边缘优化）。实验证明我们的框架可用于高效开发、集成和自动改进各种LLM代理。代码可在https://github.com/metauto-ai/gptswarm找到。

    arXiv:2402.16823v1 Announce Type: cross  Abstract: Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. The code can be found at https://github.com/metauto-ai/gptswarm.
    
[^9]: 彩虹团队：多样化对抗性提示的开放式生成

    Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts

    [https://arxiv.org/abs/2402.16822](https://arxiv.org/abs/2402.16822)

    Rainbow Teaming提出了一种新方法，通过开放式搜索生成多样化的对抗性提示，可以帮助改善大型语言模型的稳健性，提高安全性，问答和网络安全等领域的模型漏洞。

    

    随着大型语言模型（LLMs）在许多现实世界应用中变得越来越普遍，理解和增强它们对用户输入的稳健性至关重要。现有的用于识别敌对提示的方法往往专注于特定领域，缺乏多样性，或需要大量人工注释。为了解决这些限制，我们提出了彩虹团队，一种用于生成多样化对抗性提示的新方法。彩虹团队将对抗性提示生成视为一个质量 - 多样性问题，并使用开放式搜索来生成既有效又多样的提示。它可以揭示模型在广泛领域内的脆弱性，包括本文中的安全性、问答和网络安全。我们还证明，对由彩虹团队生成的合成数据进行微调可以提高最先进的LLMs的安全性，而不损害它们的一般能力。

    arXiv:2402.16822v1 Announce Type: new  Abstract: As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to user inputs is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem, and uses open-ended search to generate prompts that are both effective and diverse. It can uncover a model's vulnerabilities across a broad range of domains including, in this paper, safety, question answering, and cybersecurity. We also demonstrate that fine-tuning on synthetic data generated by Rainbow Teaming improves the safety of state-of-the-art LLMs without hurting their general capabilities 
    
[^10]: Nemotron-4 15B技术报告

    Nemotron-4 15B Technical Report

    [https://arxiv.org/abs/2402.16819](https://arxiv.org/abs/2402.16819)

    Nemotron-4 15B是一个150亿参数的大型多语言模型，在多语言能力上表现优异，超过其他规模相似的模型。

    

    我们介绍了Nemotron-4 15B，这是一个拥有150亿参数的大型多语言模型，训练过程中使用了8000万亿个文本标记。Nemotron-4 15B在英语、多语言和编码任务上表现出色：在7个下游评估领域中，它在4个领域中表现出色，并在其余领域中取得了竞争性表现，超过了所有现有规模相似的开放模型。具体来说，Nemotron-4 15B展现出了所有规模相似模型中最强的多语言能力，甚至在多语言任务上优于四倍以上的大型模型，以及专门用于多语言任务的模型。

    arXiv:2402.16819v1 Announce Type: new  Abstract: We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.
    
[^11]: 研究通过Gisting的HyperTuning的有效性

    Investigating the Effectiveness of HyperTuning via Gisting

    [https://arxiv.org/abs/2402.16817](https://arxiv.org/abs/2402.16817)

    Gisting方法可用于训练模型将信息压缩为更少的标记表示，构建的HyperLlama模型可以有效地将信息从少量示例压缩成软前缀，并为进一步的前缀微调提供更好的初始化。

    

    Gisting（Mu等，2023）是一种简单的方法，用于训练模型将信息压缩为更少的标记表示，其使用修改后的注意力蒙版，并可作为训练基于Transformer的超网络的经济方法。我们引入了HyperLlama，这是一组基于Llama-2模型构建的Gisting型超网络，它根据少量输入生成特定于任务的软前缀。在P3、Super-NaturalInstructions和Symbol Tuning数据集上的实验中，我们展示了HyperLlama模型可以有效地将信息从少量示例压缩成软前缀。然而，它们在少量上下文示例上的全注意力下的表现仍不如多任务微调语言模型。我们还表明，HyperLlama生成的软前缀可用作进一步前缀微调的更好初始化。总的来说，基于Gisting的超网络是经济且易于实现的，但在实证表现上有一定不稳定性。

    arXiv:2402.16817v1 Announce Type: new  Abstract: Gisting (Mu et al., 2023) is a simple method for training models to compress information into fewer token representations using a modified attention mask, and can serve as an economical approach to training Transformer-based hypernetworks. We introduce HyperLlama, a set of Gisting-based hypernetworks built on Llama-2 models that generates task-specific soft prefixes based on few-shot inputs. In experiments across P3, Super-NaturalInstructions and Symbol Tuning datasets, we show that HyperLlama models can effectively compress information from few-shot examples into soft prefixes. However, they still underperform multi-task fine-tuned language models with full attention over few-shot in-context examples. We also show that HyperLlama-generated soft prefixes can serve as better initializations for further prefix tuning. Overall, Gisting-based hypernetworks are economical and easy to implement, but have mixed empirical performance.
    
[^12]: OncoGPT: 一个针对肿瘤领域专业知识定制的医学对话模型，基于大型语言模型Meta-AI (LLaMA)

    OncoGPT: A Medical Conversational Model Tailored with Oncology Domain Expertise on a Large Language Model Meta-AI (LLaMA)

    [https://arxiv.org/abs/2402.16810](https://arxiv.org/abs/2402.16810)

    本研究开发了一个专门针对肿瘤领域的语言模型，提高了提供肿瘤相关建议的准确性。

    

    过去一年，将大型语言模型(LLMs)应用于医学领域的趋势日益增长，尤其是随着OpenAI开发的ChatGPT等先进语言模型的出现。然而，关于LLMs专门处理肿瘤相关查询的研究有限。本研究的主要目的是开发一个专业化的语言模型，以提高提供与肿瘤相关建议的准确性。我们进行了广泛的在线问题-答案互动数据收集，围绕肿瘤，来源于值得信赖的医生-患者平台。经过数据清理和匿名化处理，建立了一个包含超过180K+肿瘤相关对话的数据集。对话被分类，并由领域专家和临床医生进行了细致审查，以确保准确性。利用LLaMA模型和其他选定的开源数据集，我们进行了迭代的fine-tu

    arXiv:2402.16810v1 Announce Type: new  Abstract: In the past year, there has been a growing trend in applying Large Language Models (LLMs) to the field of medicine, particularly with the advent of advanced language models such as ChatGPT developed by OpenAI. However, there is limited research on LLMs specifically addressing oncology-related queries. The primary aim of this research was to develop a specialized language model that demonstrates improved accuracy in providing advice related to oncology. We performed an extensive data collection of online question-answer interactions centered around oncology, sourced from reputable doctor-patient platforms. Following data cleaning and anonymization, a dataset comprising over 180K+ oncology-related conversations was established. The conversations were categorized and meticulously reviewed by field specialists and clinicians to ensure precision. Employing the LLaMA model and other selected open-source datasets, we conducted iterative fine-tu
    
[^13]: 设定时间：预训练语言模型的时间对齐

    Set the Clock: Temporal Alignment of Pretrained Language Models

    [https://arxiv.org/abs/2402.16797](https://arxiv.org/abs/2402.16797)

    该研究探讨了预训练语言模型的时间混乱问题，并提出了时间对齐的方法，实验证明将LMs对齐到最近时间可以显著提高性能

    

    语言模型（LMs）在来自不同时间点的网络文本上进行训练，通常没有任何明确的时间基础。本研究调查了预训练LMs的时间混乱，并探讨了将它们的内部知识对齐到目标时间的各种方法，我们称之为“时间对齐”。为此，我们首先自动构建了一个包含20K个时态问题及其答案的数据集，涵盖从2000年到2023年的每一年。根据这个数据集，我们在实践中表明，预训练的LMs（例如LLaMa2），尽管有最近的预训练截止日期（例如2022年），大多数使用更早的知识来回答问题（例如在2019年）。然后，我们开发了几种方法，从提示到微调，来对齐LMs在回答问题时使用最新的知识，并探讨了这种对齐中的各种因素。我们的实验证明，将LLaMa2对齐到2022年可以将其性能提高高达62%

    arXiv:2402.16797v1 Announce Type: new  Abstract: Language models (LMs) are trained on web text originating from many points in time and, in general, without any explicit temporal grounding. This work investigates the temporal chaos of pretrained LMs and explores various methods to align their internal knowledge to a target time, which we call "temporal alignment." To do this, we first automatically construct a dataset containing 20K time-sensitive questions and their answers for each year from 2000 to 2023. Based on this dataset, we empirically show that pretrained LMs (e.g., LLaMa2), despite having a recent pretraining cutoff (e.g., 2022), mostly answer questions using earlier knowledge (e.g., in 2019). We then develop several methods, from prompting to finetuning, to align LMs to use their most recent knowledge when answering questions, and investigate various factors in this alignment. Our experiments show that aligning LLaMa2 to the year 2022 can boost its performance by up to 62% 
    
[^14]: 政治罗盘或旋转箭？朝着对大型语言模型中价值观和观点更有意义的评估

    Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models

    [https://arxiv.org/abs/2402.16786](https://arxiv.org/abs/2402.16786)

    挑战传统的受限评估范式，探索更真实的不受限制的对大型语言模型中价值观和观点的评估方法。

    

    最近的许多工作通过多项选择调查和问卷来评估大型语言模型（LLMs）中的价值观和观点。大多数工作的动机是源于对现实世界中LLM应用的担忧。然而，这种对现实世界的关注与当前评估的人为性形成鲜明对比：真实用户通常不会向LLMs提出调查问题。受到这种差异的启发，我们挑战了目前对LLMs中价值观和观点的约束评估范式，并探索了更现实的不受限制的评估。作为一个案例研究，我们专注于广受欢迎的政治罗盘测试（PCT）。在一个系统性评估中，我们发现大多数先前使用PCT的工作都强制模型遵守PCT的多项选择格式。我们展示了当不被强制时，模型给出的答案实质上是不同的；

    arXiv:2402.16786v1 Announce Type: new  Abstract: Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world LLM applications. For example, politically-biased LLMs may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions. Motivated by this discrepancy, we challenge the prevailing constrained evaluation paradigm for values and opinions in LLMs and explore more realistic unconstrained evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT forces models to comply with the PCT's multiple-choice format. We show that models give substantively different answers when not forced; that answers cha
    
[^15]: 大型语言模型量化策略的全面评估

    A Comprehensive Evaluation of Quantization Strategies for Large Language Models

    [https://arxiv.org/abs/2402.16775](https://arxiv.org/abs/2402.16775)

    该研究提出了一个结构化评估框架，针对大型语言模型的量化策略展开全面评估，填补了现有研究中的空白。

    

    增加大型语言模型（LLMs）中的参数数量通常会在下游任务中提高性能，但会增加计算和存储成本，在资源有限的情况下部署变得困难。量化技术通过减少模型权重或激活所需的位数，并最小化性能损失，已经因LLMs的兴起而变得流行。然而，大多数量化研究使用预训练的LLMs，量化对调整过指令的LLMs的影响以及量化LLMs的困惑度与基准性能之间的关系尚不明确。对量化LLMs的评估通常仅限于语言建模和少数分类任务，其在其他基准上的性能尚不清楚。为填补这些空白，我们提出了一个结构化评估框架，包括三个关键维度：（1）知识和容量，（2）对齐性和（3）效率，并进行了广泛的实验。

    arXiv:2402.16775v1 Announce Type: new  Abstract: Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings. Quantization techniques, which reduce the bits needed for model weights or activations with minimal performance loss, have become popular due to the rise of LLMs. However, most quantization studies use pre-trained LLMs, and the impact of quantization on instruction-tuned LLMs and the relationship between perplexity and benchmark performance of quantized LLMs are not well understood. Evaluation of quantized LLMs is often limited to language modeling and a few classification tasks, leaving their performance on other benchmarks unclear. To address these gaps, we propose a structured evaluation framework consisting of three critical dimensions: (1) knowledge \& capacity, (2) alignment, and (3) efficiency, and conduct extensive experi
    
[^16]: CorpusBrain++: 知识密集型语言任务的继续生成预训练框架

    CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks

    [https://arxiv.org/abs/2402.16767](https://arxiv.org/abs/2402.16767)

    本研究介绍了知识密集型语言任务中的持续文档学习任务，并建立了一个新的评估数据集，旨在探索检索模型有效处理动态检索场景的能力。

    

    知识密集型语言任务通常需要从可信的语料库（如维基百科）中检索相关文档以生成特定答案。最近，提出了用于知识密集型语言任务的预训练生成式检索模型CorpusBrain，并取得了新的检索性能最优结果。然而，大多数现有的关于知识密集型语言任务的研究，包括CorpusBrain，在很大程度上集中在静态文档集上，忽视了现实场景的动态性质，其中新文档持续地被纳入源语料库。为了填补这一空白，探索检索模型有效处理知识密集型语言任务中固有的动态检索场景的能力至关重要。在本文中，我们首先介绍了知识密集型语言任务的持续文档学习（CDL）任务，并基于原始KILT数据集构建了一个名为KILT++的新型评估基准数据集。

    arXiv:2402.16767v1 Announce Type: cross  Abstract: Knowledge-intensive language tasks (KILTs) typically require retrieving relevant documents from trustworthy corpora, e.g., Wikipedia, to produce specific answers. Very recently, a pre-trained generative retrieval model for KILTs, named CorpusBrain, was proposed and reached new state-of-the-art retrieval performance. However, most existing research on KILTs, including CorpusBrain, has predominantly focused on a static document collection, overlooking the dynamic nature of real-world scenarios, where new documents are continuously being incorporated into the source corpus. To address this gap, it is crucial to explore the capability of retrieval models to effectively handle the dynamic retrieval scenario inherent in KILTs.   In this work, we first introduce the continual document learning (CDL) task for KILTs and build a novel benchmark dataset named KILT++ based on the original KILT dataset for evaluation. Then, we conduct a comprehensi
    
[^17]: 混合参与式系统中的价值偏好估计和消歧

    Value Preferences Estimation and Disambiguation in Hybrid Participatory Systems

    [https://arxiv.org/abs/2402.16751](https://arxiv.org/abs/2402.16751)

    本研究针对混合参与式系统中的价值偏好估计提出了新方法，通过与参与者互动解决了选择与动机之间的冲突，并重点比较了从动机中估计的价值与仅从选择中估计的价值。

    

    在混合参与式系统中理解公民的价值观对于以公民为中心的政策制定至关重要。我们设想了一个混合参与式系统，在这个系统中，参与者做出选择并提供选择的动机，人工智能代理通过与他们互动来估计他们的价值偏好。我们专注于在参与者的选择和动机之间检测到冲突的情况，并提出了估计价值偏好的方法，同时通过与参与者互动来解决检测到的不一致性。我们将“珍视是经过深思熟虑的有意义行为”这一哲学立场操作化。也就是如果参与者的选择是基于对价值偏好的深思熟虑，那么可以在参与者为选择提供的动机中观察到价值偏好。因此，我们提出并比较了优先考虑从动机中估计的价值而不是仅从选择中估计的价值的价值估计方法。

    arXiv:2402.16751v1 Announce Type: cross  Abstract: Understanding citizens' values in participatory systems is crucial for citizen-centric policy-making. We envision a hybrid participatory system where participants make choices and provide motivations for those choices, and AI agents estimate their value preferences by interacting with them. We focus on situations where a conflict is detected between participants' choices and motivations, and propose methods for estimating value preferences while addressing detected inconsistencies by interacting with the participants. We operationalize the philosophical stance that "valuing is deliberatively consequential." That is, if a participant's choice is based on a deliberation of value preferences, the value preferences can be observed in the motivation the participant provides for the choice. Thus, we propose and compare value estimation methods that prioritize the values estimated from motivations over the values estimated from choices alone.
    
[^18]: DREsS: 英语作为外语写作基于评分标准的数据集

    DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing

    [https://arxiv.org/abs/2402.16733](https://arxiv.org/abs/2402.16733)

    本文发布了一个大型标准数据集DREsS，用于基于评分标准的自动作文评分，在提出了一种基于破坏的作文增强策略CASE后，这个数据集的基线结果提高了45.44％。

    

    自动化作文评分（AES）是英语作为外语写作教育中一种有用的工具，为学生和教师提供实时作文评分。然而，先前的AES模型是在与EFL写作教育实际场景不相关的作文和分数上进行训练的，并且通常由于缺乏适当的数据集而提供单一的整体评分。在本文中，我们发布了DREsS，这是一个用于基于评分标准的自动作文评分的大型标准数据集。DREsS包括三个子数据集：DREsS_New，DREsS_Std.和DREsS_CASE。我们收集了DREsS_New，这是一个由EFL本科生撰写并由英语教育专家评分的真实课堂数据集。我们还将现有的基于评分标准的作文评分数据集标准化为DREsS_Std。我们提出了一个名为CASE的基于破坏的作文增强策略，用于生成20K个DREsS_CASE的合成样本，并将基线结果提高了45.44％。

    arXiv:2402.16733v1 Announce Type: new  Abstract: Automated essay scoring (AES) is a useful tool in English as a Foreign Language (EFL) writing education, offering real-time essay scores for students and instructors. However, previous AES models were trained on essays and scores irrelevant to the practical scenarios of EFL writing education and usually provided a single holistic score due to the lack of appropriate datasets. In this paper, we release DREsS, a large-scale, standard dataset for rubric-based automated essay scoring. DREsS comprises three sub-datasets: DREsS_New, DREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with 1.7K essays authored by EFL undergraduate students and scored by English education experts. We also standardize existing rubric-based essay scoring datasets as DREsS_Std. We suggest CASE, a corruption-based augmentation strategy for essays, which generates 20K synthetic samples of DREsS_CASE and improves the baseline results by 45.44%. 
    
[^19]: CodeChameleon: 针对越狱大型语言模型的个性化加密框架

    CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models

    [https://arxiv.org/abs/2402.16717](https://arxiv.org/abs/2402.16717)

    该论文提出了一种基于个性化加密策略的新型越狱框架CodeChameleon，通过重塑任务格式和嵌入解密功能，成功应对大型语言模型面临的安全挑战。

    

    Adversarial misuse, particularly through `jailbreaking' that circumvents a model's safety and ethical protocols, poses a significant challenge for Large Language Models (LLMs). This paper delves into the mechanisms behind such successful attacks, introducing a hypothesis for the safety mechanism of aligned LLMs: intent security recognition followed by response generation. Grounded in this hypothesis, we propose CodeChameleon, a novel jailbreak framework based on personalized encryption tactics. To elude the intent security recognition phase, we reformulate tasks into a code completion format, enabling users to encrypt queries using personalized encryption functions. To guarantee response generation functionality, we embed a decryption function within the instructions, which allows the LLM to decrypt and execute the encrypted queries successfully. We conduct extensive experiments on 7 LLMs, achieving state-of-the-art average Attack Success

    arXiv:2402.16717v1 Announce Type: new  Abstract: Adversarial misuse, particularly through `jailbreaking' that circumvents a model's safety and ethical protocols, poses a significant challenge for Large Language Models (LLMs). This paper delves into the mechanisms behind such successful attacks, introducing a hypothesis for the safety mechanism of aligned LLMs: intent security recognition followed by response generation. Grounded in this hypothesis, we propose CodeChameleon, a novel jailbreak framework based on personalized encryption tactics. To elude the intent security recognition phase, we reformulate tasks into a code completion format, enabling users to encrypt queries using personalized encryption functions. To guarantee response generation functionality, we embed a decryption function within the instructions, which allows the LLM to decrypt and execute the encrypted queries successfully. We conduct extensive experiments on 7 LLMs, achieving state-of-the-art average Attack Succes
    
[^20]: 量子线性代数是Transformer架构所需的一切

    Quantum linear algebra is all you need for Transformer architectures

    [https://arxiv.org/abs/2402.16714](https://arxiv.org/abs/2402.16714)

    本文研究了在容错性量子计算的视角下Transformer架构，展示了如何利用量子线性代数构建Transformer的关键组件。

    

    生成式机器学习方法如大型语言模型正在彻底改变文本和图像的创作。本文通过容错性量子计算的视角研究了Transformer架构。我们展示了如何准备self-attention矩阵的块编码，并结合量子子程序构建了Transformer中的重要组成部分。

    arXiv:2402.16714v1 Announce Type: cross  Abstract: Generative machine learning methods such as large-language models are revolutionizing the creation of text and images. While these models are powerful they also harness a large amount of computational resources. The transformer is a key component in large language models that aims to generate a suitable completion of a given partial sequence. In this work, we investigate transformer architectures under the lens of fault-tolerant quantum computing. The input model is one where pre-trained weight matrices are given as block encodings to construct the query, key, and value matrices for the transformer. As a first step, we show how to prepare a block encoding of the self-attention matrix, with a row-wise application of the softmax function using the Hadamard product. In addition, we combine quantum subroutines to construct important building blocks in the transformer, the residual connection, layer normalization, and the feed-forward neura
    
[^21]: SelectIT: 通过基于不确定性的自我反思实现大型语言模型的选择性指导调整

    SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection

    [https://arxiv.org/abs/2402.16705](https://arxiv.org/abs/2402.16705)

    SelectIT通过利用大型语言模型本身的能力和基于不确定性的方法，提出了一种无需额外资源的高效选择指导调整数据集的方法，进而提升了模型的能力。

    

    指导调整（IT）对于调整大型语言模型（LLMs）以适应人类中心交互至关重要。最近的进展表明，精心选择一小部分高质量的IT数据可以显着提高LLMs的性能。尽管如此，常见方法通常依赖于额外的模型或数据集，这增加了成本并限制了广泛采用。在这项工作中，我们提出了一种新颖的方法，称为SelectIT，它利用LLM本身的基本能力。具体来说，我们利用LLMs中固有的不确定性，更有效地选择高质量的IT数据，而无需额外资源。此外，我们介绍了一种新颖的IT数据集，名为选择性羊驼（Selective Alpaca），通过将SelectIT应用于Alpaca-GPT4数据集而创建。实证结果表明，使用选择性羊驼进行IT可以极大地提升模型性能。SelectIT的稳健性也得到了验证。

    arXiv:2402.16705v1 Announce Type: new  Abstract: Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs. Despite this, common approaches often rely on additional models or data sets, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed SelectIT, that capitalizes on the foundational capabilities of the LLM itself. Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources. Furthermore, we introduce a novel IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement. The robustness of SelectIT has also b
    
[^22]: 为情感分析生成有效的集成模型

    Generating Effective Ensembles for Sentiment Analysis

    [https://arxiv.org/abs/2402.16700](https://arxiv.org/abs/2402.16700)

    通过Hierarchical Ensemble Construction（HEC）算法将传统NLP模型与转换器模型结合，能够显著优于传统集成模型，提高情感分析准确性。

    

    近年来，转换器模型已经彻底改变了自然语言处理（NLP），在各种任务中取得了出色的结果，包括情感分析（SA）。因此，目前用于SA的最新方法主要依赖于转换器模型， 在基准数据集上取得了令人印象深刻的准确性水平。本文展示了进一步提高这些SA集成模型准确性的关键在于不仅包括转换器模型，还包括传统NLP模型，尽管后者相对于转换器模型处于劣势。然而，正如我们实验证明的，这需要改变集成模型的构建方式，具体依赖于我们提出的分层集成构建（HEC）算法。我们在八个典型SA数据集上的实证研究表明，通过HEC结构化的混合模型类型的集成明显优于传统的集成模型。

    arXiv:2402.16700v1 Announce Type: new  Abstract: In recent years, transformer models have revolutionized Natural Language Processing (NLP), achieving exceptional results across various tasks, including Sentiment Analysis (SA). As such, current state-of-the-art approaches for SA predominantly rely on transformer models alone, achieving impressive accuracy levels on benchmark datasets. In this paper, we show that the key for further improving the accuracy of such ensembles for SA is to include not only transformers, but also traditional NLP models, despite the inferiority of the latter compared to transformer models. However, as we empirically show, this necessitates a change in how the ensemble is constructed, specifically relying on the Hierarchical Ensemble Construction (HEC) algorithm we present. Our empirical studies across eight canonical SA datasets reveal that ensembles incorporating a mix of model types, structured via HEC, significantly outperform traditional ensembles. Finally
    
[^23]: 在大型语言模型中审慎行事：迈向决策感知和可泛化的工具使用

    Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models

    [https://arxiv.org/abs/2402.16696](https://arxiv.org/abs/2402.16696)

    提出了一种决策感知和可泛化的工具使用框架，以帮助大型语言模型在操作工具时提高灵活性和泛化能力

    

    工具增强的大型语言模型（LLM）在获取最新知识和缓解产生幻觉问题方面引起了广泛关注。当前，先进的闭源LLM（如ChatGPT）通过提示和上下文学习技术展示出令人惊讶的工具使用能力。为了增强开源LLM（如LLaMA）在操作工具方面的能力，当前的努力集中于基于模板驱动或基于标记触发的工具使用。然而，前者由于受到限制的工具交互，限制了LLM灵活地解决各种用户查询，而后者在使用新工具时限制了泛化能力，因为工具使用学习基于任务和工具特定的数据集。为了缓解这些问题，本文提出了一种决策感知和可泛化的工具使用框架（DEER）。具体而言，我们首先构建具有多个决策分支的工具使用样本。

    arXiv:2402.16696v1 Announce Type: new  Abstract: Tool-augmented large language models (LLMs) are attracting widespread attention when accessing up-to-date knowledge and alleviating hallucination issues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated surprising tool-usage capabilities through prompting and in-context learning techniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in manipulating tools, current efforts focus on either template-driven or token-triggered tool-usage. However, the former hampers LLMs' flexibility to address diverse user's queries due to constrained tool interactions, while the latter limits the generalizability when engaging with new tools, since tool-usage learning is based on task- and tool-specific datasets. To alleviate these concerns, in this paper, we propose a decision-aware and generalizable tool-usage framework (DEER). Specifically, we first construct the tool-usage samples with multiple decision branches 
    
[^24]: HumanEval-XL：面向跨语言自然语言泛化的多语言代码生成基准

    HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization

    [https://arxiv.org/abs/2402.16694](https://arxiv.org/abs/2402.16694)

    HumanEval-XL 是一个面向跨语言自然语言泛化的多语言代码生成基准，建立23种自然语言和12种编程语言的联系，提供了全面的评估平台，弥补了多语言LLM评估的重要空白。

    

    大型语言模型(LLMs)在从文本提示生成代码方面取得了重大进展。然而，现有的基准主要集中在将英语提示翻译为多语言代码，或者仅限于非常有限的自然语言(NLs)。这些基准忽视了庞大的作为对比的多语言NL到多语言代码的广阔领域，导致了对多语言LLM评估的重大空白。为解决这一问题，我们引入了HumanEval-XL，一个专门设计来解决这一不足的大规模多语言代码生成基准。HumanEval-XL建立了23种NL和12种编程语言(PLs)之间的联系，包括了22,080个提示的集合，平均有8.33个测试用例。通过确保在多个NL和PL之间的并行数据，HumanEval-XL为多语言LLMs提供了全面的评估平台，允许评估对不同NL的理解。

    arXiv:2402.16694v1 Announce Type: new  Abstract: Large language models (LLMs) have made significant progress in generating codes from textual prompts. However, existing benchmarks have mainly concentrated on translating English prompts to multilingual codes or have been constrained to very limited natural languages (NLs). These benchmarks have overlooked the vast landscape of massively multilingual NL to multilingual code, leaving a critical gap in the evaluation of multilingual LLMs. In response, we introduce HumanEval-XL, a massively multilingual code generation benchmark specifically crafted to address this deficiency. HumanEval-XL establishes connections between 23 NLs and 12 programming languages (PLs), and comprises of a collection of 22,080 prompts with an average of 8.33 test cases. By ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a comprehensive evaluation platform for multilingual LLMs, allowing the assessment of the understanding of different NLs. O
    
[^25]: 将生物医学和临床预训练模型调整到法语长文档：一项比较研究

    Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study

    [https://arxiv.org/abs/2402.16689](https://arxiv.org/abs/2402.16689)

    进一步通过法语生物医学文本对英文临床模型进行预训练可以优于其他两种调整策略，结果强调了长序列法语生物医学模型在大多数下游任务上提高了性能

    

    最近，基于BERT的预训练语言模型已被引入法语生物医学领域。尽管这些模型在生物医学和临床NLP任务上取得了最先进的结果，但它们受到512个令牌的有限输入序列长度的限制，在应用于临床记录时会面临挑战。在本文中，我们提出了三种适用于长序列模型的调整策略的比较研究，利用了Longformer架构。我们对这些模型在涵盖生物医学和临床领域的16个下游任务上进行了评估。我们的研究结果表明，进一步通过法语生物医学文本对英文临床模型进行预训练，可以优于将法语生物医学BERT转换为Longformer架构以及从头开始预训练法语生物医学Longformer。结果强调了长序列法语生物医学模型在大多数下游任务上提高了性能

    arXiv:2402.16689v1 Announce Type: new  Abstract: Recently, pretrained language models based on BERT have been introduced for the French biomedical domain. Although these models have achieved state-of-the-art results on biomedical and clinical NLP tasks, they are constrained by a limited input sequence length of 512 tokens, which poses challenges when applied to clinical notes. In this paper, we present a comparative study of three adaptation strategies for long-sequence models, leveraging the Longformer architecture. We conducted evaluations of these models on 16 downstream tasks spanning both biomedical and clinical domains. Our findings reveal that further pre-training an English clinical model with French biomedical texts can outperform both converting a French biomedical BERT to the Longformer architecture and pre-training a French biomedical Longformer from scratch. The results underscore that long-sequence French biomedical models improve performance across most downstream tasks 
    
[^26]: StructLM: 朝向构建结构化知识连接的通用模型

    StructLM: Towards Building Generalist Models for Structured Knowledge Grounding

    [https://arxiv.org/abs/2402.16671](https://arxiv.org/abs/2402.16671)

    StructLM系列模型基于全面指令调整数据集，超越了大多数任务特定模型，在结构化知识连接任务中取得了新的最先进成就。

    

    结构化数据源，如表格、图形和数据库，是普遍存在的知识源。尽管大型语言模型（LLM）在纯文本上表现出色，但它们在解释和利用结构化数据方面的能力仍然有限。我们的研究揭示了LLM在处理结构化数据方面的显着不足，例如，ChatGPT平均落后于最先进模型(SoTA)35%。为增强LLM中的结构化知识连接（SKG）能力，我们开发了一个包含110万个示例的全面指令调整数据集。利用这个数据集，我们训练了一系列基于Code-LLaMA架构的模型，称为StructLM，参数范围从7B到34B。我们的StructLM系列在18个评估数据集中有14个超越了特定任务的模型，并在7个SKG任务上确立了新的SoTA成就。此外，StructLM展现了卓越的泛化能力。

    arXiv:2402.16671v1 Announce Type: new  Abstract: Structured data sources, such as tables, graphs, and databases, are ubiquitous knowledge sources. Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35%. To augment the Structured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a comprehensive instruction tuning dataset comprising 1.1 million examples. Utilizing this dataset, we train a series of models, referred to as StructLM, based on the Code-LLaMA architecture, ranging from 7B to 34B parameters. Our StructLM series surpasses task-specific models on 14 out of 18 evaluated datasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore, StructLM demonstrates exceptional generalizat
    
[^27]: RepoAgent：一种以LLM为动力的开源框架，用于生成存储库级代码文档

    RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation

    [https://arxiv.org/abs/2402.16667](https://arxiv.org/abs/2402.16667)

    RepoAgent是一个以大型语言模型为动力的开源框架，旨在积极生成、维护和更新代码文档，验证了其在生成高质量存储库级文档方面的有效性。

    

    arXiv:2402.16667v1 公告类型：新的 摘要：生成模型在软件工程中展示出了相当大的潜力，特别是在诸如代码生成和调试之类的任务中。然而，在代码文档生成领域中，它们的利用仍未得到充分开发。为此，我们引入了RepoAgent，这是一个以大型语言模型为动力的开源框架，旨在积极生成、维护和更新代码文档。通过定性和定量评估，我们已经验证了我们方法的有效性，表明RepoAgent在生成高质量的存储库级文档方面表现出色。代码和结果可以公开访问：https://github.com/OpenBMB/RepoAgent。

    arXiv:2402.16667v1 Announce Type: new  Abstract: Generative models have demonstrated considerable potential in software engineering, particularly in tasks such as code generation and debugging. However, their utilization in the domain of code documentation generation remains underexplored. To this end, we introduce RepoAgent, a large language model powered open-source framework aimed at proactively generating, maintaining, and updating code documentation. Through both qualitative and quantitative evaluations, we have validated the effectiveness of our approach, showing that RepoAgent excels in generating high-quality repository-level documentation. The code and results are publicly accessible at https://github.com/OpenBMB/RepoAgent.
    
[^28]: GigaPevt：多模态医疗助手

    GigaPevt: Multimodal Medical Assistant

    [https://arxiv.org/abs/2402.16654](https://arxiv.org/abs/2402.16654)

    GigaPevt是第一个结合大型语言模型和专业医疗模型的多模态医疗助手，在对话质量和度量性能方面表现出明显优势，并在问答任务中提高了1.18\%的准确率。

    

    建立一个智能高效的医疗助手仍然是一个具有挑战性的人工智能问题。主要限制来自数据模态的稀缺性，降低了全面的患者感知。本演示论文介绍了GigaPevt，这是第一个结合了大型语言模型的对话功能和专业医疗模型的多模态医疗助手。这种方法在对话质量和度量性能方面具有明显优势，使得在问答任务中准确率提高了1.18\%。

    arXiv:2402.16654v1 Announce Type: cross  Abstract: Building an intelligent and efficient medical assistant is still a challenging AI problem. The major limitation comes from the data modality scarceness, which reduces comprehensive patient perception. This demo paper presents the GigaPevt, the first multimodal medical assistant that combines the dialog capabilities of large language models with specialized medical models. Such an approach shows immediate advantages in dialog quality and metric performance, with a 1.18\% accuracy improvement in the question-answering task.
    
[^29]: ESG情绪分析：比较人类和语言模型的表现，包括GPT

    ESG Sentiment Analysis: comparing human and language model performance including GPT

    [https://arxiv.org/abs/2402.16650](https://arxiv.org/abs/2402.16650)

    本研究比较人类和先进语言模型在衡量ESG相关情绪方面的表现。

    

    本文探讨了在与环保、社会责任和公司治理（ESG）社交媒体相关的情绪测量方面的挑战。近年来，ESG在重要性上增长，金融部门对其表现出了极大兴趣，并且许多企业的绩效在某种程度上取决于其ESG相关声誉。利用情绪分析来衡量ESG相关声誉已经得到发展，随之而来的是对机器进行此类分析的兴趣。数字媒体时代催生了新媒体来源的爆炸式增长，这得益于社交媒体平台的增长。这种不断增长的数据环境已成为许多领域行为洞察研究的极佳来源，涵盖政治、医疗保健和市场研究。我们的研究旨在比较人类表现与语言模型在衡量ESG相关情绪方面的尖端表现。为此，研究人员对150条推文的情绪进行分类。

    arXiv:2402.16650v1 Announce Type: new  Abstract: In this paper we explore the challenges of measuring sentiment in relation to Environmental, Social and Governance (ESG) social media. ESG has grown in importance in recent years with a surge in interest from the financial sector and the performance of many businesses has become based in part on their ESG related reputations. The use of sentiment analysis to measure ESG related reputation has developed and with it interest in the use of machines to do so. The era of digital media has created an explosion of new media sources, driven by the growth of social media platforms. This growing data environment has become an excellent source for behavioural insight studies across many disciplines that includes politics, healthcare and market research. Our study seeks to compare human performance with the cutting edge in machine performance in the measurement of ESG related sentiment. To this end researchers classify the sentiment of 150 tweets an
    
[^30]: 基于领域嵌入生成意大利语概念复杂描述

    Domain Embeddings for Generating Complex Descriptions of Concepts in Italian Language

    [https://arxiv.org/abs/2402.16632](https://arxiv.org/abs/2402.16632)

    提出了一个新的领域嵌入方法，通过引入从电子词典中提取的信息，解决了连续语义值和离散描述之间的差距

    

    在这项研究中，我们提出了一个分布语义资源，其中包含从电子词典中提取的语言和词汇信息，旨在解决连续语义值与一般语义理论提供的离散描述之间的差距。我们的方法引入了一个基于语言数据的替代策略，我们开发了一组特定领域的共现矩阵，从两个来源派生而来：将意大利名词分类为4个语义特征和20

    arXiv:2402.16632v1 Announce Type: new  Abstract: In this work, we propose a Distributional Semantic resource enriched with linguistic and lexical information extracted from electronic dictionaries, designed to address the challenge of bridging the gap between the continuous semantic values represented by distributional vectors and the discrete descriptions offered by general semantics theory. Recently, many researchers have concentrated on the nexus between embeddings and a comprehensive theory of semantics and meaning. This often involves decoding the representation of word meanings in Distributional Models into a set of discrete, manually constructed properties such as semantic primitives or features, using neural decoding techniques. Our approach introduces an alternative strategy grounded in linguistic data. We have developed a collection of domain-specific co-occurrence matrices, derived from two sources: a classification of Italian nouns categorized into 4 semantic traits and 20 
    
[^31]: 具有并行上下文编码的长上下文语言建模

    Long-Context Language Modeling with Parallel Context Encoding

    [https://arxiv.org/abs/2402.16617](https://arxiv.org/abs/2402.16617)

    提出了一种名为CEPE的框架，通过并行编码扩展了现有仅解码器LLMs的上下文窗口，显著降低了计算成本并在语言建模和上下文学习中取得了强大性能表现。

    

    将大型语言模型（LLMs）扩展到处理更长的输入对于许多应用至关重要。然而，transformers的巨大计算成本，以及位置编码的有限泛化能力，限制了它们的上下文窗口的大小。我们引入了一种称为Context Expansion with Parallel Encoding（CEPE）的框架，可以应用于任何现有的仅解码器LLMs，以扩展它们的上下文窗口。CEPE采用一个小型编码器来分块处理长输入，并通过交叉注意力使冻结的解码器能够利用额外的上下文。CEPE高效、通用且多功能：通过使用8K标记文档进行训练，CEPE将LLAMA-2的上下文窗口扩展到128K标记，仅使用1/6的内存即可获得10倍的吞吐量。CEPE在语言建模和上下文学习方面表现出强大性能。CEPE在检索增强应用中也表现出色，而现有的长上下文模型在这方面则退化。

    arXiv:2402.16617v1 Announce Type: new  Abstract: Extending large language models (LLMs) to process longer inputs is crucial for numerous applications. However, the considerable computational cost of transformers, coupled with limited generalization of positional encoding, restricts the size of their context window. We introduce Context Expansion with Parallel Encoding (CEPE), a framework that can be applied to any existing decoder-only LLMs to extend their context window. CEPE adopts a small encoder to process long inputs chunk by chunk and enables the frozen decoder to leverage additional contexts via cross-attention. CEPE is efficient, generalizable, and versatile: trained with 8K-token documents, CEPE extends the context window of LLAMA-2 to 128K tokens, offering 10x the throughput with only 1/6 of the memory. CEPE yields strong performance on language modeling and in-context learning. CEPE also excels in retrieval-augmented applications, while existing long-context models degenerat
    
[^32]: 理解大型语言模型开发背后的数据集管理者

    Understanding the Dataset Practitioners Behind Large Language Model Development

    [https://arxiv.org/abs/2402.16611](https://arxiv.org/abs/2402.16611)

    数据质量是大型语言模型开发中数据集管理者的首要任务，但管理者间对于数据质量定义和评估方法缺乏共识。

    

    随着大型语言模型(LLMs)变得越来越先进和有影响力，审视它们依赖和产生的数据变得越来越重要。本文探讨了数据集管理者的工作内容：首先，我们通过对谷歌贡献LLM开发团队责任的回顾性分析，定义了“数据集管理者”的角色。然后，我们对这些管理者进行了半结构化访谈（N=10）。我们发现数据质量是首要任务。为了评估数据质量，管理者要么凭直觉，要么编写自定义评估逻辑。管理者之间对数据质量的定义和评估方法缺乏共识。我们讨论了这种现象的潜在原因和实现一致性的机会。

    arXiv:2402.16611v1 Announce Type: new  Abstract: As large language models (LLMs) become more advanced and impactful, it is increasingly important to scrutinize the data that they rely upon and produce. What is it to be a dataset practitioner doing this work? We approach this in two parts: first, we define the role of "dataset practitioner" by performing a retrospective analysis on the responsibilities of teams contributing to LLM development at Google. Then, we conduct semi-structured interviews with a cross-section of these practitioners (N=10). We find that data quality is the top priority. To evaluate data quality, practitioners either rely on their own intuition or write custom evaluation logic. There is a lack of consensus across practitioners on what quality is and how to evaluate it. We discuss potential reasons for this phenomenon and opportunities for alignment.
    
[^33]: PAQA：面向积极开放型检索问答的研究

    PAQA: Toward ProActive Open-Retrieval Question Answering

    [https://arxiv.org/abs/2402.16608](https://arxiv.org/abs/2402.16608)

    本研究针对对话式搜索系统中存在的数据集不足问题，提出了PAQA方法，通过考虑用户查询和文档中的歧义，生成相关澄清问题，从而改善对话搜索系统的效果。

    

    会话系统在生成自然语言响应方面取得了显著进展。然而，由于其在信息检索过程中的被动角色，目前作为对话式搜索系统的潜力受到限制。一个主要限制是提供带有支持文档语料库和相关澄清问题的标记模棱两可问题的数据集的稀缺性。本文旨在通过考虑用户查询和文档中存在的固有歧义来解决生成相关澄清问题的挑战。为了实现这一目标，我们提出了PAQA，这是现有AmbiNQ数据集的扩展，其中包含澄清问题。然后，我们评估各种模型，并评估段落检索如何影响模棱两可检测和生成澄清问题。通过解决对话搜索系统中的这一缺陷，我们旨在提供额外的监督以增强其效果。

    arXiv:2402.16608v1 Announce Type: new  Abstract: Conversational systems have made significant progress in generating natural language responses. However, their potential as conversational search systems is currently limited due to their passive role in the information-seeking process. One major limitation is the scarcity of datasets that provide labelled ambiguous questions along with a supporting corpus of documents and relevant clarifying questions. This work aims to tackle the challenge of generating relevant clarifying questions by taking into account the inherent ambiguities present in both user queries and documents. To achieve this, we propose PAQA, an extension to the existing AmbiNQ dataset, incorporating clarifying questions. We then evaluate various models and assess how passage retrieval impacts ambiguity detection and the generation of clarifying questions. By addressing this gap in conversational search systems, we aim to provide additional supervision to enhance their ac
    
[^34]: 重新思考生成式命名实体识别中的负例

    Rethinking Negative Instances for Generative Named Entity Recognition

    [https://arxiv.org/abs/2402.16602](https://arxiv.org/abs/2402.16602)

    本研究探索了在生成式命名实体识别中引入负例训练的潜力，结果表明负例的引入通过引入上下文信息和清晰划定标签边界来显著改进系统性能，并提出了一种名为Hierarchical Matching的新颖高效算法，进一步将非结构化预测转化为结构化实体。

    

    大型语言模型（LLMs）已经展示出在未知任务中擅长泛化的能力。最近在命名实体识别（NER）任务中，通过采用以实体为中心的模式调整，LLMs的显著改进已经在广泛的实体领域中得到展示。在这项工作中，我们探索通过将负例纳入训练来增强现有方法的潜力。我们的实验揭示了负例通过（1）引入上下文信息和（2）清晰划定标签边界而对改进产生显著影响。此外，我们介绍了一种名为Hierarchical Matching的新颖高效算法，旨在将非结构化预测转化为结构化实体。通过整合这些组件，我们提出了GNER，一个生成式NER系统，展示了跨未知实体领域的提升的零-shot性能。我们进行了全面评估。

    arXiv:2402.16602v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities for generalizing in unseen tasks. In the Named Entity Recognition (NER) task, recent advancements have seen the remarkable improvement of LLMs in a broad range of entity domains via instruction tuning, by adopting entity-centric schema. In this work, we explore the potential enhancement of the existing methods by incorporating negative instances into training. Our experiments reveal that negative instances contribute to remarkable improvements by (1) introducing contextual information, and (2) clearly delineating label boundaries. Furthermore, we introduce a novel and efficient algorithm named Hierarchical Matching, which is tailored to transform unstructured predictions into structured entities. By integrating these components, we present GNER, a Generative NER system that shows improved zero-shot performance across unseen entity domains. Our comprehensive evaluation
    
[^35]: Slovene语义变化检测：一个新的数据集和基于最优输运的方法

    Semantic change detection for Slovene language: a novel dataset and an approach based on optimal transport

    [https://arxiv.org/abs/2402.16596](https://arxiv.org/abs/2402.16596)

    该论文提出了一个新的斯洛文尼亚语数据集，用于评估语义变化检测系统，并提出了一种基于最优输运的全新方法，能够将错误率降低22.8%。

    

    在这篇论文中，我们专注于检测斯洛文尼亚语中的语义变化，这是一种拥有两百万使用者的资源较少的斯拉夫语言。检测和跟踪语义变化可以揭示语言的演变，这是由社会和文化变化引起的。最近，已经提出了几种系统来帮助这项研究，但所有这些系统都依赖于手动注释的黄金标准数据集进行评估。本文提出了第一个斯洛文尼亚语数据集，用于评估语义变化检测系统，其中包含了从3000多个手动注释的句对中获得的104个目标词的汇总语义变化评分。我们在这个数据集上评估了几种已有的语义变化检测方法，并提出了一种基于最优输运的新方法，该方法改进了现有的最先进系统，错误率降低了22.8%。

    arXiv:2402.16596v1 Announce Type: new  Abstract: In this paper, we focus on the detection of semantic changes in Slovene, a less resourced Slavic language with two million speakers. Detecting and tracking semantic changes provides insights into the evolution of the language caused by changes in society and culture. Recently, several systems have been proposed to aid in this study, but all depend on manually annotated gold standard datasets for evaluation. In this paper, we present the first Slovene dataset for evaluating semantic change detection systems, which contains aggregated semantic change scores for 104 target words obtained from more than 3000 manually annotated sentence pairs. We evaluate several existing semantic change detection methods on this dataset and also propose a novel approach based on optimal transport that improves on the existing state-of-the-art systems with an error reduction rate of 22.8%.
    
[^36]: 多比特无失真数字水印技术用于大型语言模型

    Multi-Bit Distortion-Free Watermarking for Large Language Models

    [https://arxiv.org/abs/2402.16578](https://arxiv.org/abs/2402.16578)

    该论文扩展了现有的零比特无失真水印方法，通过在水印中嵌入多比特元信息，并开发了一个高效的解码器，实现了从水印中提取信息的过程，具有很低的比特误差率。

    

    曾提出水印大型语言模型的方法，通过稍微改变模型输出分布来区分AI生成的文本和人类生成的文本，但它们也会扭曲文本的质量，从而将水印暴露给对抗性检测。最近提出了无失真水印方法，需要一个秘钥来检测水印。之前的方法通常嵌入零比特水印，除了标记文本为AI生成外并未提供额外信息。我们通过嵌入多比特的元信息作为水印的一部分，扩展了现有的零比特无失真水印方法。我们还开发了一个计算高效的解码器，从水印中提取嵌入的信息，具有很低的比特误码率。

    arXiv:2402.16578v1 Announce Type: new  Abstract: Methods for watermarking large language models have been proposed that distinguish AI-generated text from human-generated text by slightly altering the model output distribution, but they also distort the quality of the text, exposing the watermark to adversarial detection. More recently, distortion-free watermarking methods were proposed that require a secret key to detect the watermark. The prior methods generally embed zero-bit watermarks that do not provide additional information beyond tagging a text as being AI-generated. We extend an existing zero-bit distortion-free watermarking method by embedding multiple bits of meta-information as part of the watermark. We also develop a computationally efficient decoder that extracts the embedded information from the watermark with low bit error rate.
    
[^37]: 使用大型语言模型在时间知识图上进行两阶段生成式问答

    Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models

    [https://arxiv.org/abs/2402.16568](https://arxiv.org/abs/2402.16568)

    该论文提出了一种新颖的生成式时间知识图问答框架GenTKGQA，利用大型语言模型(LLMs)在时间知识图问答任务中的两阶段方法，即子图检索和答案生成。

    

    时间知识图问答(TKGQA)提出了一个重要的挑战任务，因为问题中隐藏着时间约束，并且从动态结构化知识中寻找答案。尽管大型语言模型(LLMs)在推理能力方面取得了相当大的进展，但它们在TKGQA任务中的应用是一个相对未开发的领域。本文首先提出了一种新颖的生成式时间知识图问答框架GenTKGQA，通过两个阶段引导LLMs回答时间性问题：子图检索和答案生成。首先，我们利用LLM的固有知识来挖掘问题中的时间约束和结构链接，无需额外训练，从而缩小了在时间和结构维度上的子图搜索空间。接下来，我们设计了虚拟知识指示器来融合子图和文本表示的图神经网络信号。

    arXiv:2402.16568v1 Announce Type: new  Abstract: Temporal knowledge graph question answering (TKGQA) poses a significant challenge task, due to the temporal constraints hidden in questions and the answers sought from dynamic structured knowledge. Although large language models (LLMs) have made considerable progress in their reasoning ability over structured data, their application to the TKGQA task is a relatively unexplored area. This paper first proposes a novel generative temporal knowledge graph question answering framework, GenTKGQA, which guides LLMs to answer temporal questions through two phases: Subgraph Retrieval and Answer Generation. First, we exploit LLM's intrinsic knowledge to mine temporal constraints and structural links in the questions without extra training, thus narrowing down the subgraph search space in both temporal and structural dimensions. Next, we design virtual knowledge indicators to fuse the graph neural network signals of the subgraph and the text repres
    
[^38]: 将大型语言模型对齐到特定领域的图数据库

    Aligning Large Language Models to a Domain-specific Graph Database

    [https://arxiv.org/abs/2402.16567](https://arxiv.org/abs/2402.16567)

    该论文提出了一种将大型语言模型对齐到特定领域的图数据库的方法，通过利用ChatGPT生成NL-GQL数据对并微调LLMs，实现了两者之间的对齐。

    

    图数据库（Graph DB）被广泛应用于金融、社交网络和医药等各个领域。然而，将自然语言（NL）转换为图查询语言（GQL），通常称为NL2GQL，由于其固有复杂性和专业化特性而变得具有挑战性。一些方法试图利用大型语言模型（LLMs）来解决类似的任务，如文本转SQL。然而，在特定领域的NL2GQL任务中，缺乏特定领域的NL-GQL数据对使得难以建立LLMs和图数据库之间的对齐。为了解决这一挑战，我们提出了一个明确定义的流水线。具体地，我们利用ChatGPT基于给定的图数据库自我生成NL-GQL数据对。然后，我们使用创建的数据来对LLMs进行微调，从而实现LLMs与图数据库之间的对齐。此外，在推断过程中，我们提出了一种提取相关信息的方法。

    arXiv:2402.16567v1 Announce Type: new  Abstract: Graph Databases (Graph DB) are widely applied in various fields, including finance, social networks, and medicine. However, translating Natural Language (NL) into the Graph Query Language (GQL), commonly known as NL2GQL, proves to be challenging due to its inherent complexity and specialized nature. Some approaches have sought to utilize Large Language Models (LLMs) to address analogous tasks like text2SQL. Nevertheless, when it comes to NL2GQL taskson a particular domain, the absence of domain-specific NL-GQL data pairs makes it difficult to establish alignment between LLMs and the graph DB. To address this challenge, we propose a well-defined pipeline. Specifically, we utilize ChatGPT to create NL-GQL data pairs based on the given graph DB with self-instruct. Then, we use the created data to fine-tune LLMs, thereby achieving alignment between LLMs and the graph DB. Additionally, during inference, we propose a method that extracts relev
    
[^39]: 将大型语言模型与图形会话推荐相结合

    Integrating Large Language Models with Graphical Session-Based Recommendation

    [https://arxiv.org/abs/2402.16539](https://arxiv.org/abs/2402.16539)

    本文提出了一种名为LLMGR的框架，将大型语言模型与图形会话推荐相结合，有效地弥合了推荐系统中大型语言模型在会话推荐领域的适应性差距

    

    随着大型语言模型（LLMs）的快速发展，出现了各种探索，利用LLMs在推荐系统上的上下文理解能力。虽然开创性的策略主要是将传统推荐任务转变为自然语言生成挑战，但在会话推荐（SBR）领域的探索相对较少，因为其具体性。SBR主要由图神经网络主导，由于其捕获相邻行为之间的内在和显性关系的能力，取得了许多成功结果。图的结构性质与自然语言的本质形成对比，为LLMs提出了重大的适应性差距。本文介绍了将大型语言模型与图形会话推荐相结合的框架LLMGR，这是一个有效的框架，通过和谐地弥合上述差距

    arXiv:2402.16539v1 Announce Type: cross  Abstract: With the rapid development of Large Language Models (LLMs), various explorations have arisen to utilize LLMs capability of context understanding on recommender systems. While pioneering strategies have primarily transformed traditional recommendation tasks into challenges of natural language generation, there has been a relative scarcity of exploration in the domain of session-based recommendation (SBR) due to its specificity. SBR has been primarily dominated by Graph Neural Networks, which have achieved many successful outcomes due to their ability to capture both the implicit and explicit relationships between adjacent behaviors. The structural nature of graphs contrasts with the essence of natural language, posing a significant adaptation gap for LLMs. In this paper, we introduce large language models with graphical Session-Based recommendation, named LLMGR, an effective framework that bridges the aforementioned gap by harmoniously 
    
[^40]: 基于LLM和分布导师的知识蒸馏引导下的隐私数据增强用于医学文本分类

    LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification

    [https://arxiv.org/abs/2402.16515](https://arxiv.org/abs/2402.16515)

    提出一种基于LLM和分布导师的知识蒸馏引导下的隐私数据增强方法，用于医学文本分类，将DP-based伪样本生成任务转移到DP-based生成样本鉴别任务，通过教师模型教导学生如何选择带有校准噪声的私有样本以实现差分隐私。

    

    随着模型训练的数据并非始终公开可访问，研究人员利用先进的学习算法利用有限的数据或通过数据增强（DA）来扩展数据集。在私有领域进行DA需要隐私保护方法（如匿名化和扰动），但这些方法无法提供保护保证。差分隐私（DP）学习方法在理论上限制保护，但不擅长使用大模型生成伪文本样本。在本文中，我们将基于DP的伪样本生成任务转移为基于DP生成样本鉴别任务，提出了一种带LLM和基于DP鉴别器的DP-based DA方法，用于私有领域的文本分类。我们构建了一个知识蒸馏模型作为基于DP的鉴别器：教师模型访问私有数据，教导学生如何选择具有校准噪声的私有样本以实现DP。

    arXiv:2402.16515v1 Announce Type: new  Abstract: As sufficient data are not always publically accessible for model training, researchers exploit limited data with advanced learning algorithms or expand the dataset via data augmentation (DA). Conducting DA in private domain requires private protection approaches (i.e. anonymization and perturbation), but those methods cannot provide protection guarantees. Differential privacy (DP) learning methods theoretically bound the protection but are not skilled at generating pseudo text samples with large models. In this paper, we transfer DP-based pseudo sample generation task to DP-based generated samples discrimination task, where we propose a DP-based DA method with a LLM and a DP-based discriminator for text classification on private domains. We construct a knowledge distillation model as the DP-based discriminator: teacher models, accessing private data, teaches students how to select private samples with calibrated noise to achieve DP. To 
    
[^41]: 使用大规模合成监督进行跨语言开放域问答的预训练

    Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision

    [https://arxiv.org/abs/2402.16508](https://arxiv.org/abs/2402.16508)

    本研究提出了一种基于自监督方法的单个编码-解码模型来解决跨语言问答问题，通过利用维基百科内的跨语言链接结构合成监督信号，取得了优于其他方法的效果。

    

    跨语言问答（CLQA）是一个复杂的问题，包括从多语言知识库进行跨语言检索，然后在英语或查询语言中生成答案。本文展示了可以使用单个编码-解码模型来解决CLQA。为了有效训练这个模型，我们提出了一种基于利用维基百科内跨语言链接结构的自监督方法。我们展示了如何利用链接的维基百科页面来合成跨语言检索的监督信号，通过一种填空查询形式生成更自然的查询以监督答案生成。最后，我们展示了我们的方法CLASS在监督学习和零-shot情况下均优于可比方法。

    arXiv:2402.16508v1 Announce Type: new  Abstract: Cross-lingual question answering (CLQA) is a complex problem, comprising cross-lingual retrieval from a multilingual knowledge base, followed by answer generation either in English or the query language. Both steps are usually tackled by separate models, requiring substantial annotated datasets, and typically auxiliary resources, like machine translation systems to bridge between languages. In this paper, we show that CLQA can be addressed using a single encoder-decoder model. To effectively train this model, we propose a self-supervised method based on exploiting the cross-lingual link structure within Wikipedia. We demonstrate how linked Wikipedia pages can be used to synthesise supervisory signals for cross-lingual retrieval, through a form of cloze query, and generate more natural queries to supervise answer generation. Together, we show our approach, \texttt{CLASS}, outperforms comparable methods on both supervised and zero-shot lan
    
[^42]: 在动态多代理环境中评估大型语言模型的能力的LLMArena

    LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments

    [https://arxiv.org/abs/2402.16499](https://arxiv.org/abs/2402.16499)

    LLMArena是一个新颖且易于扩展的框架，用于评估大型语言模型在多代理动态环境中的多样化能力。

    

    最近，大型语言模型（LLMs）的进展揭示了它们实现拥有人类水平智能的自主代理的潜力。然而，用于评估LLM代理的现有基准要么使用静态数据集，可能导致数据泄漏，要么仅关注单一代理场景，忽略了多代理互动的复杂性。缺乏一个基准，可以评估LLM代理在多代理动态环境中的多样化能力。为此，我们引入了LLMArena，这是一个新颖且易于扩展的框架，用于评估LLM在多代理动态环境中的多样化能力。LLMArena包含七个不同的游戏环境，采用Trueskill评分来评估LLM代理中关键能力，包括空间推理、战略规划、数字推理、风险评估、沟通、对手建模和团队协作。我们进行了大量的实验。

    arXiv:2402.16499v1 Announce Type: new  Abstract: Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive ex
    
[^43]: 关于对模拟引擎进行语言化

    On Languaging a Simulation Engine

    [https://arxiv.org/abs/2402.16482](https://arxiv.org/abs/2402.16482)

    通过三种功能化类型的语言模型，提出了一种语言到模拟（Lang2Sim）框架，实现了精准将文本描述转化为可执行模拟器输入的方法。

    

    语言模型智能正在彻底改变我们编程材料模拟的方式。然而，模拟场景的多样性使得将人类语言精确转化为定制模拟器变得具有挑战性。在这里，我们使用三种功能化类型的语言模型，提出了一种语言到模拟（Lang2Sim）框架，该框架可以实现交互式导航，通过以多孔矩阵中水吸附的场景实例为例对模拟引擎进行语言化。与逐行编码目标模拟器不同，语言模型解释每个模拟器为具有不变工具功能及其变体输入-输出对的总体。 Lang2Sim通过功能化和序列化语言模型，相应地，对工具分类进行合理化，定制其输入-输出组合，并将模拟器输入精炼为可执行格式。重要的是，

    arXiv:2402.16482v1 Announce Type: new  Abstract: Language model intelligence is revolutionizing the way we program materials simulations. However, the diversity of simulation scenarios renders it challenging to precisely transform human language into a tailored simulator. Here, using three functionalized types of language model, we propose a language-to-simulation (Lang2Sim) framework that enables interactive navigation on languaging a simulation engine, by taking a scenario instance of water sorption in porous matrices. Unlike line-by-line coding of a target simulator, the language models interpret each simulator as an assembly of invariant tool function and its variant input-output pair. Lang2Sim enables the precise transform of textual description by functionalizing and sequentializing the language models of, respectively, rationalizing the tool categorization, customizing its input-output combinations, and distilling the simulator input into executable format. Importantly, dependin
    
[^44]: mEdIT: 通过指令调整实现多语言文本编辑

    mEdIT: Multilingual Text Editing via Instruction Tuning

    [https://arxiv.org/abs/2402.16472](https://arxiv.org/abs/2402.16472)

    该论文介绍了mEdIT，这是CoEdIT的多语言扩展，使用指令调整对多语言文本编辑模型进行微调训练，在多语言文本编辑任务中表现强劲。

    

    我们介绍了mEdIT，这是CoEdIT的一个多语言扩展，CoEdIT是最近最先进的文本编辑模型，用于写作辅助。mEdIT模型通过指令调整对多语言大型预训练语言模型（LLMs）进行微调训练。它们旨在接收用户从自然语言指令中指定所需文本属性的指令，例如Grammatik korrigieren（德语）或Parafrasee la oración（西班牙语）。我们通过从多个公开可用的人工注释文本编辑数据集中策划数据，针对六种不同语系的多语言，为三个文本编辑任务（语法错误校正（GEC）、文本简化和改写）构建了mEdIT。我们详细说明了mEdIT模型的设计和训练，并展示了它们在许多多语言文本编辑基准集上与其他多语言LLMs相比的强大性能。我们还发现mEdIT gen

    arXiv:2402.16472v1 Announce Type: cross  Abstract: We introduce mEdIT, a multi-lingual extension to CoEdIT -- the recent state-of-the-art text editing models for writing assistance. mEdIT models are trained by fine-tuning multi-lingual large, pre-trained language models (LLMs) via instruction tuning. They are designed to take instructions from the user specifying the attributes of the desired text in the form of natural language instructions, such as Grammatik korrigieren (German) or Parafrasee la oraci\'on (Spanish). We build mEdIT by curating data from multiple publicly available human-annotated text editing datasets for three text editing tasks (Grammatical Error Correction (GEC), Text Simplification, and Paraphrasing) across diverse languages belonging to six different language families. We detail the design and training of mEdIT models and demonstrate their strong performance on many multi-lingual text editing benchmarks against other multilingual LLMs. We also find that mEdIT gen
    
[^45]: 揭示自注意力脆弱性

    Unveiling Vulnerability of Self-Attention

    [https://arxiv.org/abs/2402.16470](https://arxiv.org/abs/2402.16470)

    本文通过提出HackAttend扰动技术，揭示了最先进的PLMs因微小的注意力扰动而产生的高攻击成功率，将文本攻击从词汇扰动扩展到结构扰动。

    

    预训练语言模型（PLMs）被发现对微小词语变化具有脆弱性，这对现实世界系统构成了巨大威胁。本文研究了基于Transformer的PLMs的基本结构，即自注意力（SA）机制。我们提出了一种强大的扰动技术HackAttend，通过精心设计的注意力蒙版扰动SA矩阵内的注意力分数。我们展示了最先进的PLMs存在严重脆弱性，微小的注意力扰动（1%）就能产生非常高的攻击成功率（98%）。我们的论文将传统的文本攻击从词汇扰动扩展到更广泛的结构扰动。

    arXiv:2402.16470v1 Announce Type: new  Abstract: Pre-trained language models (PLMs) are shown to be vulnerable to minor word changes, which poses a big threat to real-world systems. While previous studies directly focus on manipulating word inputs, they are limited by their means of generating adversarial samples, lacking generalization to versatile real-world attack. This paper studies the basic structure of transformer-based PLMs, the self-attention (SA) mechanism. (1) We propose a powerful perturbation technique \textit{HackAttend}, which perturbs the attention scores within the SA matrices via meticulously crafted attention masks. We show that state-of-the-art PLMs fall into heavy vulnerability that minor attention perturbations $(1\%)$ can produce a very high attack success rate $(98\%)$. Our paper expands the conventional text attack of word perturbations to more general structural perturbations. (2) We introduce \textit{S-Attend}, a novel smoothing technique that effectively mak
    
[^46]: 通过反向翻译防御LLMs免受越狱攻击

    Defending LLMs against Jailbreaking Attacks via Backtranslation

    [https://arxiv.org/abs/2402.16459](https://arxiv.org/abs/2402.16459)

    通过反向翻译来防御LLMs免受越狱攻击，将生成的反向翻译提示用于揭示原始提示的实际意图，提高了模型的安全性。

    

    尽管许多大型语言模型（LLMs）已经被训练成拒绝有害请求，但它们仍然容易受到越狱攻击的影响，这种攻击会重写原始提示以隐藏其有害意图。在本文中，我们提出了一种新方法，通过“反向翻译”来防御LLMs免受越狱攻击。具体来说，给定目标LLM从输入提示生成的初始响应，我们的反向翻译提示一个语言模型来推断可以导致该响应的输入提示。推断的提示称为反向翻译提示，倾向于揭示原始提示的实际意图，因为它是基于LLM的响应生成的，不是直接由攻击者操纵的。然后，我们再次在反向翻译提示上运行目标LLM，如果模型拒绝了反向翻译提示，则拒绝原始提示。我们解释了所提出的防御措施对其有效性的几个好处。

    arXiv:2402.16459v1 Announce Type: cross  Abstract: Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent. In this paper, we propose a new method for defending LLMs against jailbreaking attacks by ``backtranslation''. Specifically, given an initial response generated by the target LLM from an input prompt, our backtranslation prompts a language model to infer an input prompt that can lead to the response. The inferred prompt is called the backtranslated prompt which tends to reveal the actual intent of the original prompt, since it is generated based on the LLM's response and is not directly manipulated by the attacker. We then run the target LLM again on the backtranslated prompt, and we refuse the original prompt if the model refuses the backtranslated prompt. We explain that the proposed defense provides several benefits on its effectiv
    
[^47]: D-XCB：数据无关去偏方法，用于公平准确基于Transformer的网络欺凌检测

    D-XCB: Data-independent Debiasing for Fair and Accurate Transformer-based Cyberbullying Detection

    [https://arxiv.org/abs/2402.16458](https://arxiv.org/abs/2402.16458)

    该论文提出了ID-XCB，这是首个数据无关去偏技术，能够在缓解模型对引发偏见的词的关注的同时提高网络欺凌检测性能，超越了当前最先进的去偏方法。

    

    骂人话是收集包含网络欺凌事件数据的常用代理。我们的重点是衡量和减轻由骂人话和因此数据收集策略导致事件之间的虚假关联产生的偏见。在演示和量化这些偏见之后，我们介绍了ID-XCB，这是第一个数据无关去偏技术，结合了敌对训练、偏见约束和去偏微调方法，旨在缓解模型对引发偏见的词的关注，同时不影响整体模型性能。我们在两个流行的基于会话的网络欺凌数据集上探讨了ID-XCB，同时进行了全面的消融和泛化研究。我们展示了ID-XCB学习了强大的网络欺凌检测能力，同时减轻了偏见，超过了最先进的去偏方法在性能和偏见缓解方面。我们的定量和定性分析表明其泛化性。

    arXiv:2402.16458v1 Announce Type: new  Abstract: Swear words are a common proxy to collect datasets with cyberbullying incidents. Our focus is on measuring and mitigating biases derived from spurious associations between swear words and incidents occurring as a result of such data collection strategies. After demonstrating and quantifying these biases, we introduce ID-XCB, the first data-independent debiasing technique that combines adversarial training, bias constraints and debias fine-tuning approach aimed at alleviating model attention to bias-inducing words without impacting overall model performance. We explore ID-XCB on two popular session-based cyberbullying datasets along with comprehensive ablation and generalisation studies. We show that ID-XCB learns robust cyberbullying detection capabilities while mitigating biases, outperforming state-of-the-art debiasing methods in both performance and bias mitigation. Our quantitative and qualitative analyses demonstrate its generalisab
    
[^48]: RetrievalQA：评估自适应检索增强生成技术用于短文开放领域问答

    RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering

    [https://arxiv.org/abs/2402.16457](https://arxiv.org/abs/2402.16457)

    本研究提出了一个新的基准RetrievalQA，用于评估自适应检索增强生成技术，发现现有方法存在问题并提出了一种新的方法TA-ARE

    

    自适应检索增强生成（ARAG）旨在动态确定查询是否需要检索，而不是无选择地检索，以增强信息的效率和相关性。然而，先前的研究在评估ARAG方法方面大多被忽视，导致它们的有效性未受到充分研究。本研究提出一种基准，RetrievalQA，包含1,271个涵盖新世界和长尾知识的短文问题。回答这些问题所需的知识不在LLM中；因此，必须检索外部信息来正确回答。这使得RetrievalQA成为评估现有ARAG方法的合适测试平台。我们发现基于校准的方法严重依赖于阈值调整，而普通提示无法有效引导LLMs做出可靠的检索决策。基于我们的发现，提出了一种简单而有效的Time-Aware Adaptive Retrieval（TA-ARE）

    arXiv:2402.16457v1 Announce Type: new  Abstract: Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine the necessity of retrieval for queries instead of retrieving indiscriminately to enhance the efficiency and relevance of the sourced information. However, previous works largely overlook the evaluation of ARAG approaches, leading to their effectiveness being understudied. This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge. The knowledge necessary to answer the questions is absent from LLMs; therefore, external information must be retrieved to answer correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG methods. We observe that calibration-based methods heavily rely on threshold tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable retrieval decisions. Based on our findings, we propose Time-Aware Adaptive Retrieval (TA-ARE), a simple yet e
    
[^49]: ShieldLM: 使LLMs成为对齐、可定制和可解释的安全检测器

    ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors

    [https://arxiv.org/abs/2402.16444](https://arxiv.org/abs/2402.16444)

    ShieldLM是一个基于LLM的安全检测器，符合一般人类安全标准，支持定制化的检测规则，并提供决策解释。

    

    大型语言模型（LLMs）的安全性近年来受到越来越多关注，但在对LLMs的响应中检测安全问题的方法仍然缺乏一个全面的、对齐、可定制和可解释的方法。在本文中，我们提出了ShieldLM，一个基于LLM的安全检测器，它与一般人类安全标准相符，支持定制化的检测规则，并为其决策提供解释。为了训练ShieldLM，我们编制了一个包含14,387个查询-响应对的大型双语数据集，根据各种安全标准对响应的安全性进行了注释。通过大量实验，我们证明了ShieldLM在四个测试集上超越了强基线，展示了出色的定制性和可解释性。除了在标准检测数据集上表现良好外，ShieldLM还被证明在实际情况中作为先进LLMs的安全评估器是有效的。

    arXiv:2402.16444v1 Announce Type: new  Abstract: The safety of Large Language Models (LLMs) has gained increasing attention in recent years, but there still lacks a comprehensive approach for detecting safety issues within LLMs' responses in an aligned, customizable and explainable manner. In this paper, we propose ShieldLM, an LLM-based safety detector, which aligns with general human safety standards, supports customizable detection rules, and provides explanations for its decisions. To train ShieldLM, we compile a large bilingual dataset comprising 14,387 query-response pairs, annotating the safety of responses based on various safety standards. Through extensive experiments, we demonstrate that ShieldLM surpasses strong baselines across four test sets, showcasing remarkable customizability and explainability. Besides performing well on standard detection datasets, ShieldLM has also been shown to be effective in real-world situations as a safety evaluator for advanced LLMs. We relea
    
[^50]: 语言特定神经元：大型语言模型多语能力的关键

    Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models

    [https://arxiv.org/abs/2402.16438](https://arxiv.org/abs/2402.16438)

    大型语言模型中的语言特定神经元可以解释其多语能力，通过提出语言激活概率熵（LAPE）的检测方法，研究发现LLMs处理特定语言的能力主要由少量神经元决定。

    

    大型语言模型(LLMs)展现出显著的多语能力，即使未经过专门策划的多语平行语料库的预训练。解释LLMs处理多语文本的基本机制仍然是一个具有挑战性的问题。在本文中，我们深入研究了LLMs中Transformer架构的构成，以找出语言特定区域。具体而言，我们提出了一种新颖的检测方法，即语言激活概率熵（LAPE），用于识别LLMs内的语言特定神经元。基于LAPE，我们对两个代表性的LLMs，即LLaMA-2和BLOOM进行了全面实验。我们的研究结果表明，LLMs处理特定语言的能力主要是由一小部分神经元决定的，这些神经元主要位于模型的顶部和底部层。此外，我们展示了通过选择性激活或停用神经元来“引导”LLMs的输出语言的可行性。

    arXiv:2402.16438v1 Announce Type: new  Abstract: Large language models (LLMs) demonstrate remarkable multilingual capabilities without being pre-trained on specially curated multilingual parallel corpora. It remains a challenging problem to explain the underlying mechanisms by which LLMs process multilingual texts. In this paper, we delve into the composition of Transformer architectures in LLMs to pinpoint language-specific regions. Specially, we propose a novel detection method, language activation probability entropy (LAPE), to identify language-specific neurons within LLMs. Based on LAPE, we conduct comprehensive experiments on two representative LLMs, namely LLaMA-2 and BLOOM. Our findings indicate that LLMs' proficiency in processing a particular language is predominantly due to a small subset of neurons, primarily situated in the models' top and bottom layers. Furthermore, we showcase the feasibility to "steer" the output language of LLMs by selectively activating or deactivatin
    
[^51]: RoCoIns: 通过代码风格指令增强大型语言模型的鲁棒性

    RoCoIns: Enhancing Robustness of Large Language Models through Code-Style Instructions

    [https://arxiv.org/abs/2402.16431](https://arxiv.org/abs/2402.16431)

    通过代码风格指令替换自然语言指令，提供更精确的指令并增强大型语言模型的鲁棒性。同时提出一种新方法，在少样本情景下通过组合干净和对抗样本加强模型的鲁棒性。

    

    大型语言模型(LLMs)在遵循人类指令方面表现出卓越能力。然而，最近的研究引起了人们对LLMs在接受结合文本对抗样本的指令时鲁棒性的担忧。本文从最近的研究中获得启示，LLMs对指令设计敏感，我们利用代码风格的指令替换了通常的自然语言指令，这些指令更具结构性和 less模糊性。通过这种转换，我们为LLMs提供了更精确的指令，增强了LLMs的鲁棒性。此外，在少样本情景下，我们提出了一种新方法，利用干净和对抗样本来组成上下文演示（\textit{对抗性上下文方法）来进一步增强LLMs的鲁棒性。在八个鲁棒性数据集上的实验表明，我们的方法始终优于使用自然语言提示LLMs的方法。

    arXiv:2402.16431v1 Announce Type: new  Abstract: Large Language Models (LLMs) have showcased remarkable capabilities in following human instructions. However, recent studies have raised concerns about the robustness of LLMs when prompted with instructions combining textual adversarial samples. In this paper, drawing inspiration from recent works that LLMs are sensitive to the design of the instructions, we utilize instructions in code style, which are more structural and less ambiguous, to replace typically natural language instructions. Through this conversion, we provide LLMs with more precise instructions and strengthen the robustness of LLMs. Moreover, under few-shot scenarios, we propose a novel method to compose in-context demonstrations using both clean and adversarial samples (\textit{adversarial context method}) to further boost the robustness of the LLMs. Experiments on eight robustness datasets show that our method consistently outperforms prompting LLMs with natural languag
    
[^52]: 使用课程描述预测可持续发展目标 - 从LLMs到传统基础模型

    Predicting Sustainable Development Goals Using Course Descriptions -- from LLMs to Conventional Foundation Models

    [https://arxiv.org/abs/2402.16420](https://arxiv.org/abs/2402.16420)

    使用LLM生成训练数据，训练语言模型来预测大学课程的可持续发展目标，有助于更好地适应SDGs。

    

    我们提出了一个关于预测大学课程联合国可持续发展目标（SDG）的工作。我们使用一个名为PaLM 2的LLM生成训练数据，将含有嘈杂人工编写的课程描述作为输入。我们利用这些数据训练了几个不同的较小语言模型，以预测大学课程的SDG。这项工作有助于更好地适应SDG的大学层面。在我们的实验中表现最好的模型是具有0.786的F1分数的BART。

    arXiv:2402.16420v1 Announce Type: new  Abstract: We present our work on predicting United Nations sustainable development goals (SDG) for university courses. We use an LLM named PaLM 2 to generate training data given a noisy human-authored course description input as input. We use this data to train several different smaller language models to predict SDGs for university courses. This work contributes to better university level adaptation of SDGs. The best performing model in our experiments was BART with an F1-score of 0.786.
    
[^53]: 从 RAGs 到财富：利用大型语言模型为临床试验撰写文件

    From RAGs to riches: Using large language models to write documents for clinical trials

    [https://arxiv.org/abs/2402.16406](https://arxiv.org/abs/2402.16406)

    本研究评估了大型语言模型（LLMs）在生成临床试验方案文件部分内容的能力，发现通过检索增强生成（RAG）可显着提高LLM的撰写质量，对LLMs在临床试验相关写作中具有重要意义。

    

    临床试验需要撰写大量文件，包括协议、同意书、临床研究报告等。大型语言模型（LLMs）有潜力快速生成这些文件的第一个版本，但人们对其输出质量存在担忧。本文评估了LLMs在生成其中一个文件（临床试验方案）的部分内容。研究发现，现成的LLM在内容相关性和术语使用正确性方面表现合理。然而，存在不足之处：特别是临床思维和逻辑，以及参考文献的适当使用。为提高性能，我们使用了检索增强生成（RAG）来提示LLM使用准确的最新信息。通过使用RAG，LLM的撰写质量显著提高，这对LLMs在临床试验相关写作中的实际可用性具有重要意义。

    arXiv:2402.16406v1 Announce Type: new  Abstract: Clinical trials require numerous documents to be written -- protocols, consent forms, clinical study reports and others. Large language models (LLMs) offer the potential to rapidly generate first versions of these documents, however there are concerns about the quality of their output Here we report an evaluation of LLMs in generating parts of one such document, clinical trial protocols. We find that an offthe-shelf LLM delivers reasonable results, especially when assessing content relevance and the correct use of terminology. However, deficiencies remain: specifically clinical thinking and logic, and appropriate use of references. To improve performance, we used retrieval-augmented generation (RAG) to prompt an LLM with accurate up-to-date information. As a result of using RAG, the writing quality of the LLM improves substantially, which has implications for the practical useability of LLMs in clinical trial-related writing.
    
[^54]: MoZIP：评估知识产权领域大型语言模型的多语言基准

    MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property

    [https://arxiv.org/abs/2402.16389](https://arxiv.org/abs/2402.16389)

    本文提出了一个新的多语言基准MoZIP，用于评估大型语言模型在知识产权领域的表现，并开发了一个新的IP-oriented多语言大型语言模型MoZi，实验证明MoZi在MoZIP基准上的表现优越。

    

    大型语言模型（LLMs）在各种自然语言处理（NLP）任务中表现出色。然而，对LLMs在特定领域（例如知识产权（IP）领域）的表现还了解有限。本文提出了一个新的基准，即第一个面向知识产权领域的多语言智力产权测验(MoZIP)，用于评估LLMs在知识产权领域的表现。MoZIP基准包括三项具有挑战性的任务：知识产权多项选择测验（IPQuiz）、知识产权问答（IPQA）和专利匹配（PatentMatch）。此外，我们还开发了一个新的面向知识产权的多语言大型语言模型（称为MoZi），它是一个基于BLOOMZ的模型，利用多语言IP相关文本数据进行监督微调。我们在MoZIP基准上评估了我们提出的MoZi模型和四种知名LLMs（即BLOOMZ、BELLE、ChatGLM和ChatGPT）。实验结果表明，MoZi胜过其他模型。

    arXiv:2402.16389v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated impressive performance in various natural language processing (NLP) tasks. However, there is limited understanding of how well LLMs perform in specific domains (e.g, the intellectual property (IP) domain). In this paper, we contribute a new benchmark, the first Multilingual-oriented quiZ on Intellectual Property (MoZIP), for the evaluation of LLMs in the IP domain. The MoZIP benchmark includes three challenging tasks: IP multiple-choice quiz (IPQuiz), IP question answering (IPQA), and patent matching (PatentMatch). In addition, we also develop a new IP-oriented multilingual large language model (called MoZi), which is a BLOOMZ-based model that has been supervised fine-tuned with multilingual IP-related text data. We evaluate our proposed MoZi model and four well-known LLMs (i.e., BLOOMZ, BELLE, ChatGLM and ChatGPT) on the MoZIP benchmark. Experimental results demonstrate that MoZi outperfo
    
[^55]: 防范有害微调攻击

    Immunization against harmful fine-tuning attacks

    [https://arxiv.org/abs/2402.16382](https://arxiv.org/abs/2402.16382)

    本文提出了一种用于防范大型语言模型中有害微调攻击的免疫条件集，以帮助理解如何构建和衡量未来的防御措施。

    

    大型语言模型（LLMs）与人类价值观的调整方法主要集中在纠正预训练中出现的不一致。然而，这种关注忽略了另一种不一致的来源：恶意行为者可能有意对LLMs进行微调以实现有害目标。本文提出了一种新兴的威胁模型，该模型源于对齐规避和微调攻击。然而，以前的作品缺乏有效防御条件的清晰呈现。我们提出了对抗LLMs中有害微调的有效防御条件集，称为“免疫条件”，这有助于我们了解如何构建和衡量未来的防御措施。利用这种防御的形式框架，我们提供了不同研究方向的综合，以防止有害微调攻击，并展示了如何在实验中使用这些条件的早期结果。

    arXiv:2402.16382v1 Announce Type: new  Abstract: Approaches to aligning large language models (LLMs) with human values has focused on correcting misalignment that emerges from pretraining. However, this focus overlooks another source of misalignment: bad actors might purposely fine-tune LLMs to achieve harmful goals. In this paper, we present an emerging threat model that has arisen from alignment circumvention and fine-tuning attacks. However, lacking in previous works is a clear presentation of the conditions for effective defence. We propose a set of conditions for effective defence against harmful fine-tuning in LLMs called "Immunization conditions," which help us understand how we would construct and measure future defences. Using this formal framework for defence, we offer a synthesis of different research directions that might be persued to prevent harmful fine-tuning attacks and provide a demonstration of how to use these conditions experimentally showing early results of using
    
[^56]: 一种用于生成高质量文本转语音数据集的自动化端到端开源软件

    An Automated End-to-End Open-Source Software for High-Quality Text-to-Speech Dataset Generation

    [https://arxiv.org/abs/2402.16380](https://arxiv.org/abs/2402.16380)

    该论文介绍了一种端到端工具，用于生成高质量的文本转语音（TTS）模型数据集，实现了语言特定的语音分布整合、自动化录制过程、自动化和人机协作的录音质量保证以及录音格式处理。

    

    数据的可用性对推动人工智能应用至关重要，包括基于语音的技术。随着内容创作的需求增加，尤其是社交媒体上的内容，翻译和文本转语音（TTS）技术已经成为必不可少的工具。本文介绍了一种端到端工具，用于生成高质量的文本转语音（TTS）模型数据集，以解决对高质量数据的急切需求。本作品的贡献多方面，包括：将语言特定的语音分布整合到样本选择中，自动化录制过程，自动化和人机协作的录音质量保证，以及处理录音以满足指定格式。

    arXiv:2402.16380v1 Announce Type: cross  Abstract: Data availability is crucial for advancing artificial intelligence applications, including voice-based technologies. As content creation, particularly in social media, experiences increasing demand, translation and text-to-speech (TTS) technologies have become essential tools. Notably, the performance of these TTS technologies is highly dependent on the quality of the training data, emphasizing the mutual dependence of data availability and technological progress. This paper introduces an end-to-end tool to generate high-quality datasets for text-to-speech (TTS) models to address this critical need for high-quality data. The contributions of this work are manifold and include: the integration of language-specific phoneme distribution into sample selection, automation of the recording process, automated and human-in-the-loop quality assurance of recordings, and processing of recordings to meet specified formats. The proposed application
    
[^57]: 用系统自校正改进基于LLM的机器翻译

    Improving LLM-based Machine Translation with Systematic Self-Correction

    [https://arxiv.org/abs/2402.16379](https://arxiv.org/abs/2402.16379)

    引入了名为TER的系统LLM自校正翻译框架，成功帮助LLMs提高翻译质量，具有更优越的系统性和可解释性。

    

    大型语言模型（LLMs）在机器翻译（MT）领域取得了令人印象深刻的结果。然而，人工仔细评估发现，LLMs生成的翻译仍然包含多个错误。重要的是，将这种错误信息反馈到LLMs中可以实现自校正，并改善翻译性能。受到这些观点的启发，我们引入了一个名为TER的系统LLM自校正翻译框架，代表了在这一方向上的重要进展。我们的研究结果表明：1）我们的自校正框架成功地帮助LLMs提高了多种语言的翻译质量，不管是从高资源语言到低资源语言，还是以英语为中心还是围绕其他语言；2）TER相比先前的方法展示出更优越的系统性和可解释性；3）

    arXiv:2402.16379v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have achieved impressive results in Machine Translation (MT). However, careful evaluations by human reveal that the translations produced by LLMs still contain multiple errors. Importantly, feeding back such error information into the LLMs can lead to self-correction and result in improved translation performance. Motivated by these insights, we introduce a systematic LLM-based self-correcting translation framework, named TER, which stands for Translate, Estimate, and Refine, marking a significant step forward in this direction. Our findings demonstrate that 1) our self-correction framework successfully assists LLMs in improving their translation quality across a wide range of languages, whether it's from high-resource languages to low-resource ones or whether it's English-centric or centered around other languages; 2) TER exhibits superior systematicity and interpretability compared to previous methods; 3)
    
[^58]: 揭示巴别塔：探究大型语言模型内的多语言激活模式

    Unraveling Babel: Exploring Multilingual Activation Patterns within Large Language Models

    [https://arxiv.org/abs/2402.16367](https://arxiv.org/abs/2402.16367)

    通过将原始的大型语言模型转化为专家混合架构，研究了LLMs的多语言激活模式，发现存在非特定语言的神经元和特定语言激活神经元，以及高频激活神经元可以加速推断并保持性能。

    

    最近，大型语言模型（LLMs）在语言处理领域取得了巨大突破，但它们在处理多种语言时的机制仍然是未知的。因此，在这项工作中，我们研究了LLMs的多语言激活模式。通过将原始的大型语言模型（LLMs）转化为专家混合（MoE）架构，我们分析了处理各种语言时专家的激活模式，并展示了这些激活模式在语言家族层面上的联系。我们发现了非特定语言的神经元以及特定语言激活神经元的存在。进一步的探索甚至展示了仅利用高频激活神经元可以加速推断，同时保持可比较的性能。这些发现揭示了LLMs的多语言处理机制，并在指导多语言训练方面具有重要意义。

    arXiv:2402.16367v1 Announce Type: new  Abstract: Recently, large language models (LLMs) have achieved tremendous breakthroughs in the field of language processing, yet their mechanisms in processing multiple languages remain agnostic. Therefore, in this work we study the multilingual activation patterns of LLMs. By transforming the original Large Language Models (LLMs) into a Mixture of Experts (MoE) architecture, we analyze the expert activation patterns when processing various languages and demonstrate the connections of these activation patterns at the level of language families. We discover the existence of non-language-specific neurons as well as language-specific activation neurons. Further exploration even showcases that merely leveraging high-frequency activation neurons can accelerate inference while maintaining comparable performance. These findings shed light on the LLMs' multilingual processing mechanism, and are of significant importance in guiding the multilingual trainin
    
[^59]: 从哪里出发？来自自然空间描述中的多尺度空间关系推理

    Where Do We Go from Here? Multi-scale Allocentric Relational Inference from Natural Spatial Descriptions

    [https://arxiv.org/abs/2402.16364](https://arxiv.org/abs/2402.16364)

    论文探讨了基于自然空间描述进行多尺度空间关系推理的方法，发现通过获知地图知识得到的描述能够提供环境的整体结构。

    

    当用自然语言传达路线时，“获得的空间知识”概念对地理信息检索（GIR）和空间认知研究至关重要。然而，自然语言处理导航研究经常忽视这种获得知识对文本描述的影响。当前导航研究集中在以自我为中心的本地描述（例如，“它将在您的右边”），这些描述需要对代理人的本地知觉进行推理。在地图获得的知识基础上的描述提供了环境的整体视图，并捕捉了其总体结构。

    arXiv:2402.16364v1 Announce Type: new  Abstract: When communicating routes in natural language, the concept of {\em acquired spatial knowledge} is crucial for geographic information retrieval (GIR) and in spatial cognitive research. However, NLP navigation studies often overlook the impact of such acquired knowledge on textual descriptions. Current navigation studies concentrate on egocentric local descriptions (e.g., `it will be on your right') that require reasoning over the agent's local perception. These instructions are typically given as a sequence of steps, with each action-step explicitly mentioning and being followed by a landmark that the agent can use to verify they are on the right path (e.g., `turn right and then you will see...'). In contrast, descriptions based on knowledge acquired through a map provide a complete view of the environment and capture its overall structure. These instructions (e.g., `it is south of Central Park and a block north of a police station') are 
    
[^60]: LLM推断揭示：调查与Roofline模型见解

    LLM Inference Unveiled: Survey and Roofline Model Insights

    [https://arxiv.org/abs/2402.16363](https://arxiv.org/abs/2402.16363)

    本文提出了一个基于Roofline模型的框架，用于系统分析LLM推断技术，帮助识别部署中的瓶颈，并为更有效地部署LLM提供策略。

    

    高效大语言模型（LLM）推断领域正在迅速发展，提供了机遇和挑战的独特结合。虽然该领域已经扩展并充满活力，但至今还没有一个简明的框架来分析LLM推断的各种方法，以便清晰地理解这一领域。我们的调查不仅总结了当前研究现状，还基于Roofline模型引入了一个框架，用于系统分析LLM推断技术。这一框架能够帮助识别LLM部署中的瓶颈，并更深入地了解在实际设备上的实际方面，从而为部署LLM提供更有效的策略。此外，我们还系统地汇总了高效LLM推断的最新进展，涵盖关键领域，比如权重优化（如知识蒸馏和量化）。

    arXiv:2402.16363v1 Announce Type: cross  Abstract: The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework enables identifying the bottlenecks in LLM deployments and provides a deeper understanding of the practical aspects on real devices, thereby informing more effective strategies for deploying LLM. Furthermore, we systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as weight optimization (e.g., Knowledge Distillation and Quantizatio
    
[^61]: 分层正则化Dropout用于神经语言模型

    Layer-wise Regularized Dropout for Neural Language Models

    [https://arxiv.org/abs/2402.16361](https://arxiv.org/abs/2402.16361)

    本文提出了一种专为Transformer-based语言模型设计的新颖的分层正则化Dropout（LR-Drop）方法，通过一致性训练策略逐层对每个Transformer层进行正则化，实现了隐藏状态、多头注意力矩阵和输出分布的一致性。

    

    在当今流行的各种预训练神经语言模型中，dropout已经成为一种不可或缺的正则化技术。为了解决dropout随机性引起的训练和推理不一致性，一些研究采用一致性训练来对输出层的dropout进行正则化。本文提出了一种新颖的分层正则化Dropout（LR-Drop），专为基于Transformer的语言模型设计。具体而言，LR-Drop使用层次一致性训练策略，逐层对每个Transformer层进行正则化。每个训练样本通过dropout采样的两个孪生子模型，然后LR-Drop强制使两个孪生子模型的隐藏状态、多头注意力矩阵和输出分布保持一致。所提出的LR-Drop可以被视为一种“自蒸馏”框架，其中dropout生成的每个子模型都是另一个的“教师”模型。

    arXiv:2402.16361v1 Announce Type: cross  Abstract: Among the various pre-trained neural language models that are popular today, dropout is already an indispensable regularization technique. To solve the inconsistency between training and inference caused by the randomness of dropout, some studies use consistency training to regularize dropout at the output layer. In this paper, we propose a novel Layer-wise Regularized Dropout (LR-Drop), which is specially designed for Transformer-based Language models. Specifically, LR-Drop layer-wise regularizes each Transformer layer using the consistency training strategy. Each training sample passes through the two siamese sub-models sampled by dropout, and then LR-Drop forces the hidden states, multi-head attention matrices, and output distribution of the two siamese sub-models to be consistent. The proposed LR-Drop can be regarded as a "self-distillation" framework, in which each sub-model generated by dropout is the other's "teacher" model and 
    
[^62]: 一个整合的数据处理框架用于预训练基础模型

    An Integrated Data Processing Framework for Pretraining Foundation Models

    [https://arxiv.org/abs/2402.16358](https://arxiv.org/abs/2402.16358)

    提出了一个集成了处理模块和分析模块的数据处理框架，旨在改善数据质量并展示其有效性。

    

    基础模型的能力在很大程度上依赖于大规模、多样化和高质量的预训练数据。为了提高数据质量，研究人员和从业者经常需要手动从不同来源策划数据集，并为每个数据存储库开发专门的数据清洗流程。缺乏统一的数据处理框架，这一过程重复而繁琐。为了缓解这一问题，我们提出了一个集成了处理模块和分析模块的数据处理框架，处理模块包括一系列不同粒度水平的操作符，而分析模块支持对精炼数据进行探查和评估。所提出的框架易于使用且高度灵活。在这篇演示论文中，我们首先介绍如何使用这个框架并展示它在改善数据质量方面的有效性，通过与ChatGPT的自动评估和端到端评估。

    arXiv:2402.16358v1 Announce Type: cross  Abstract: The ability of the foundation models heavily relies on large-scale, diverse, and high-quality pretraining data. In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and develop dedicated data cleansing pipeline for each data repository. Lacking a unified data processing framework, this process is repetitive and cumbersome. To mitigate this issue, we propose a data processing framework that integrates a Processing Module which consists of a series of operators at different granularity levels, and an Analyzing Module which supports probing and evaluation of the refined data. The proposed framework is easy to use and highly flexible. In this demo paper, we first introduce how to use this framework with some example use cases and then demonstrate its effectiveness in improving the data quality with an automated evaluation with ChatGPT and an end-to-end evaluation in 
    
[^63]: 通过时间变分推断进行语言引导的技能学习

    Language-guided Skill Learning with Temporal Variational Inference

    [https://arxiv.org/abs/2402.16354](https://arxiv.org/abs/2402.16354)

    该论文提出了一种语言引导的技能学习算法，通过整合大型语言模型生成的分割信息来发现可重用的技能，并引入最小描述长度原则来引导这一过程，实现了在不同环境中加速学习并超越基线方法的效果。

    

    我们提出了一种从专家演示中发现技能的算法。该算法首先利用大型语言模型（LLMs）来提出轨迹的初始分割。随后，一个分层变分推断框架将LLM生成的分割信息纳入其中，通过合并轨迹段来发现可重用的技能。为了进一步控制压缩和可重用性之间的权衡，我们引入了一个基于最小描述长度原则的新辅助目标，帮助引导这种技能发现过程。我们的结果表明，使用我们方法的Agent能够发现有助于加速学习的技能，在BabyAI（一个网格世界导航环境）以及ALFRED（一个家庭模拟环境）的新长期任务中胜过基线技能学习方法。

    arXiv:2402.16354v1 Announce Type: cross  Abstract: We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.
    
[^64]: MathGenie: 使用问题反向翻译生成合成数据，以增强LLMs的数学推理能力

    MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs

    [https://arxiv.org/abs/2402.16352](https://arxiv.org/abs/2402.16352)

    MathGenie通过问题反向翻译生成合成数据，用于增强LLMs的数学推理能力，并创造了一个家族化的模型系列MathGenieLM。

    

    大型语言模型(LLMs)在数学推理方面展现出巨大潜力。然而，目前开源模型和GPT-4等闭源模型之间在这一领域仍存在性能差距。本文介绍了一种新颖的方法MathGenie，用于从小规模问题-解决方案数据集（称为种子数据）中生成多样且可靠的数学问题。我们扩充了种子数据的真实解决方案，并训练了一个反向翻译模型，将扩充的解决方案翻译回新问题。随后，我们为新问题生成了集成代码解决方案。为确保集成代码解决方案的正确性，我们采用了基于原理的解决方案验证策略。我们在新筛选的数据上对从7B到70B不等的各种预训练模型进行训练，以测试所提出的增强技术的有效性，从而产生了一个称为MathGenieLM的模型系列。

    arXiv:2402.16352v1 Announce Type: cross  Abstract: Large language models (LLMs) have exhibited great potential in mathematical reasoning. However, there remains a performance gap in this area between existing open-source models and closed-source models such as GPT-4. In this paper, we introduce MathGenie, a novel method for generating diverse and reliable math problems from a small-scale problem-solution dataset (denoted as seed data). We augment the ground-truth solutions of our seed data and train a back-translation model to translate the augmented solutions back into new questions. Subsequently, we generate code-integrated solutions for the new questions. To ensure the correctness of the code-integrated solutions, we employ rationale-based strategy for solution verification. Various pretrained models, ranging from 7B to 70B, are trained on the newly curated data to test the effectiveness of the proposed augmentation technique, resulting in a family of models known as MathGenieLM. Th
    
[^65]: CodeS：构建用于文本到SQL的开源语言模型

    CodeS: Towards Building Open-source Language Models for Text-to-SQL

    [https://arxiv.org/abs/2402.16347](https://arxiv.org/abs/2402.16347)

    CodeS是一系列用于文本到SQL任务的开源语言模型，通过较小的参数规模实现了更高的准确性，采用了具有挑战性的SQL中心语料库进行渐进预训练来增强其SQL生成能力。

    

    语言模型在将自然语言问题翻译为SQL查询（Text-to-SQL）任务中表现出有希望的性能。然而，大多数最先进的方法依赖于强大但封闭源的大型语言模型（LLMs），如ChatGPT和GPT-4，可能存在模型架构不明确、数据隐私风险和昂贵的推理开销等局限性。为解决这些问题，我们引入了CodeS，一系列针对文本到SQL任务的预训练语言模型，其参数范围从1B到15B不等。CodeS是一个完全开源的语言模型，能以更小的参数规模实现更高的准确性。本文研究了构建CodeS时面临的研究挑战。为增强CodeS的SQL生成能力，我们采用了一个特别设计的SQL中心语料库进行渐进预训练。基于此，我们解决了架构链接挑战等问题。

    arXiv:2402.16347v1 Announce Type: new  Abstract: Language models have shown promising performance on the task of translating natural language questions into SQL queries (Text-to-SQL). However, most of the state-of-the-art (SOTA) approaches rely on powerful yet closed-source large language models (LLMs), such as ChatGPT and GPT-4, which may have the limitations of unclear model architectures, data privacy risks, and expensive inference overheads. To address the limitations, we introduce CodeS, a series of pre-trained language models with parameters ranging from 1B to 15B, specifically designed for the text-to-SQL task. CodeS is a fully open-source language model, which achieves superior accuracy with much smaller parameter sizes. This paper studies the research challenges in building CodeS. To enhance the SQL generation abilities of CodeS, we adopt an incremental pre-training approach using a specifically curated SQL-centric corpus. Based on this, we address the challenges of schema lin
    
[^66]: 揭示真相促进变革：面向基于代理的大规模社会运动模拟

    Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation

    [https://arxiv.org/abs/2402.16333](https://arxiv.org/abs/2402.16333)

    该研究提出了一个混合框架，通过大型语言模型和代理模型对社交媒体用户进行模拟，构建了类似Twitter环境模拟他们对触发事件的反应，为社会运动模拟提供了新途径。

    

    社交媒体已经成为社会运动的基石，在推动社会变革方面发挥着重要影响力。模拟公众反应并预测潜在影响变得越来越重要。本文介绍了一个社交媒体用户模拟的混合框架，其中用户分为两类。核心用户由大型语言模型驱动，而众多普通用户则由演绎式代理模型建模。我们进一步构建了类似Twitter的环境来复制他们对触发事件的反应动态。随后，我们开发了一个多方面的基准SoMoSiMu-Bench用于评估，并在真实世界数据集上进行全面实验。实验结果表明

    arXiv:2402.16333v1 Announce Type: cross  Abstract: Social media has emerged as a cornerstone of social movements, wielding significant influence in driving societal change. Simulating the response of the public and forecasting the potential impact has become increasingly important. However, existing methods for simulating such phenomena encounter challenges concerning their efficacy and efficiency in capturing the behaviors of social movement participants. In this paper, we introduce a hybrid framework for social media user simulation, wherein users are categorized into two types. Core users are driven by Large Language Models, while numerous ordinary users are modeled by deductive agent-based models. We further construct a Twitter-like environment to replicate their response dynamics following trigger events. Subsequently, we develop a multi-faceted benchmark SoMoSiMu-Bench for evaluation and conduct comprehensive experiments across real-world datasets. Experimental results demonstrat
    
[^67]: 大型语言模型的无数据权重压缩和去噪

    Data-freeWeight Compress and Denoise for Large Language Models

    [https://arxiv.org/abs/2402.16319](https://arxiv.org/abs/2402.16319)

    无需数据参与，基于大型语言模型结构提出了一种新的权重压缩方法，可有效压缩参数矩阵并保持正交性。

    

    大型语言模型(LLMs)正在重塑人工智能研究领域的格局，特别是随着模型参数的显著扩大，跨越各个领域展现出卓越能力。然而，模型参数的可扩展性受限于GPU内存和计算速度的限制。为了解决这些限制，出现了各种权重压缩方法，如剪枝和量化。鉴于语言模型中权重矩阵的低秩特性，通过矩阵分解减少权重在压缩参数方面无疑具有显著潜力和前景。在本文中，借鉴LLMs的内在结构，我们提出了一种称为无数据联合秩-k逼近的新方法，用于压缩参数矩阵。值得注意的是，我们的方法特点在于无需额外涉及任何语料库，同时保持正交性。

    arXiv:2402.16319v1 Announce Type: new  Abstract: Large Language Models (LLMs) are reshaping the research landscape in artificial intelligence, particularly as model parameters scale up significantly, unlocking remarkable capabilities across various domains. Nevertheless, the scalability of model parameters faces constraints due to limitations in GPU memory and computational speed. To address these constraints, various weight compression methods have emerged, such as Pruning and Quantization. Given the low-rank nature of weight matrices in language models, the reduction of weights through matrix decomposition undoubtedly holds significant potential and promise. In this paper, drawing upon the intrinsic structure of LLMs, we propose a novel approach termed Data-free Joint Rank-k Approximation for compressing the parameter matrices. Significantly, our method is characterized by without necessitating additional involvement of any corpus, while simultaneously preserving orthogonality in con
    
[^68]: Finer: 在大型视觉语言模型中研究和增强细粒度视觉概念识别

    Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models

    [https://arxiv.org/abs/2402.16315](https://arxiv.org/abs/2402.16315)

    Finer工作揭示了大型视觉语言模型在细粒度视觉分类上的短板，尤其是难以生成准确的细致属性解释，尽管具有生成高水平图像解释的能力。

    

    最近指导调整的大型视觉语言模型（LVLMs）的进展使模型能够轻松生成高水平的基于图像的解释。尽管这种能力主要归因于大型语言模型（LLMs）中包含的丰富世界知识，但我们的工作揭示了它们在六个不同基准设置下的细粒度视觉分类（FGVC）上的缺陷。最近的LVLMs最先进的模型，如LLaVa-1.5，InstructBLIP和GPT-4V，在分类性能方面严重下降，例如，LLaVA-1.5在斯坦福狗的EM平均下降了65.58，而且还难以根据出现在输入图像中的概念生成具有详细属性的准确解释，尽管它们有生成整体图像级描述的能力。深入分析表明，经过指导调整的LVLMs在给定文本时呈现出模态差距，显示出存在不一致性

    arXiv:2402.16315v1 Announce Type: cross  Abstract: Recent advances in instruction-tuned Large Vision-Language Models (LVLMs) have imbued the models with the ability to generate high-level, image-grounded explanations with ease. While such capability is largely attributed to the rich world knowledge contained within the Large Language Models (LLMs), our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings. Most recent state-of-the-art LVLMs like LLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs for LLaVA-1.5, but also struggle to generate an accurate explanation with detailed attributes based on the concept that appears within an input image despite their capability to generate holistic image-level descriptions. In-depth analyses show that instruction-tuned LVLMs exhibit modality gap, showing discrepancy when given tex
    
[^69]: Chain-of-Discussion：复杂证据问题回答的多模型框架

    Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering

    [https://arxiv.org/abs/2402.16313](https://arxiv.org/abs/2402.16313)

    提出了一种Chain-of-Discussion框架，通过多个开源语言模型的协同作用，提高了复杂问题回答的质量

    

    开放式问题回答需要模型找到适当的证据来形成合理、全面和有帮助的答案。在实际应用中，模型还需要参与对与问题密切相关的潜在场景进行深入讨论。在检索模块的增强下，开源大型语言模型（LLMs）通常能够产生一致的答案，但在可靠证据选择和深入问题分析方面仍不够理想。本文提出了一种新颖的Chain-of-Discussion框架，旨在利用多个开源LLMs之间的协同作用，为开放式QA提供更正确、更全面的答案，尽管它们在个体上还不够强大。我们的实验证明，多个LLMs之间的讨论对提高答案质量起着至关重要的作用。我们在\url{https://github.com/kobaya}上发布了我们的数据和代码。

    arXiv:2402.16313v1 Announce Type: cross  Abstract: Open-ended question answering requires models to find appropriate evidence to form well-reasoned, comprehensive and helpful answers. In practical applications, models also need to engage in extended discussions on potential scenarios closely relevant to the question. With augmentation of retrieval module, open-source Large Language Models (LLMs) can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable evidence selection and in-depth question analysis. In this paper, we propose a novel Chain-of-Discussion framework to leverage the synergy among multiple open-source LLMs aiming to provide \textbf{more correct} and \textbf{more comprehensive} answers for open-ended QA, although they are not strong enough individually. Our experiments show that discussions among multiple LLMs play a vital role in enhancing the quality of answers. We release our data and code at \url{https://github.com/kobaya
    
[^70]: 跨领域的中文句式结构解析

    Cross-domain Chinese Sentence Pattern Parsing

    [https://arxiv.org/abs/2402.16311](https://arxiv.org/abs/2402.16311)

    本文提出了一种利用大型语言模型进行自我训练的创新方法，通过动态生成训练数据将源领域句法规则与目标领域句子相结合，增强句式结构解析器对各种领域的适应能力，实验证明其在教科书和新闻领域的效果优于基于规则的基准模型1.68个百分点。

    

    arXiv:2402.16311v1 公告类型: 跨领域 句式结构（SPS）解析是一种主要用于语言教学的句法分析方法。现有的SPS解析器主要依赖于教科书语料库进行训练，缺乏跨领域能力。为了克服这一限制，本文提出了一种创新方法，利用大型语言模型（LLMs）在自我训练框架内。从源领域中提取部分句法规则，与目标领域句子结合动态生成训练数据，增强了解析器对不同领域的适应能力。在教科书和新闻领域进行的实验表明，所提出的方法效果显著，F1指标比基于规则的基准模型高出1.68个百分点。

    arXiv:2402.16311v1 Announce Type: cross  Abstract: Sentence Pattern Structure (SPS) parsing is a syntactic analysis method primarily employed in language teaching.Existing SPS parsers rely heavily on textbook corpora for training, lacking cross-domain capability.To overcome this constraint, this paper proposes an innovative approach leveraging large language models (LLMs) within a self-training framework. Partial syntactic rules from a source domain are combined with target domain sentences to dynamically generate training data, enhancing the adaptability of the parser to diverse domains.Experiments conducted on textbook and news domains demonstrate the effectiveness of the proposed method, outperforming rule-based baselines by 1.68 points on F1 metrics.
    
[^71]: PerLTQA: 一个用于问题回答中的记忆分类、检索和合成的个人长期记忆数据集

    PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering

    [https://arxiv.org/abs/2402.16288](https://arxiv.org/abs/2402.16288)

    PerLTQA是一个结合了语义和情节记忆的创新QA数据集，旨在探索个性化记忆在QA任务中的应用，提供了一个全面的基准和记忆整合、检索、合成的框架

    

    长期记忆在个人互动中起着至关重要的作用，考虑到长期记忆可以更好地利用世界知识、历史信息和对话中的偏好。我们的研究引入了PerLTQA，一个创新的QA数据集，结合了语义和情节记忆，包括世界知识、用户资料、社会关系、事件和对话。这个数据集被收集用于探讨个性化记忆在QA任务中的应用，重点关注社交互动和事件。PerLTQA具有两种记忆类型和一个包含8,593个问题的30个字符的全面基准，促进了在大型语言模型（LLM）中探索和应用个性化记忆。基于PerLTQA，我们提出了一个记忆整合和生成的新框架，包括三个主要组成部分：记忆分类、记忆检索和记忆合成。我们使用五个LLM和三个评估了这个框架。

    arXiv:2402.16288v1 Announce Type: cross  Abstract: Long-term memory plays a critical role in personal interaction, considering long-term memory can better leverage world knowledge, historical information, and preferences in dialogues. Our research introduces PerLTQA, an innovative QA dataset that combines semantic and episodic memories, including world knowledge, profiles, social relationships, events, and dialogues. This dataset is collected to investigate the use of personalized memories, focusing on social interactions and events in the QA task. PerLTQA features two types of memory and a comprehensive benchmark of 8,593 questions for 30 characters, facilitating the exploration and application of personalized memories in Large Language Models (LLMs). Based on PerLTQA, we propose a novel framework for memory integration and generation, consisting of three main components: Memory Classification, Memory Retrieval, and Memory Synthesis. We evaluate this framework using five LLMs and thre
    
[^72]: 一种使用注释嵌入模型的本体包含关系预测自匹配训练方法

    A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction

    [https://arxiv.org/abs/2402.16278](https://arxiv.org/abs/2402.16278)

    提出了一种自匹配训练方法，通过两种本体嵌入模型捕获全局和局部信息，提高了概念子类预测的稳健性

    

    最近，提出了一种在低维空间中表示实体的本体嵌入，用于本体完成。然而，用于概念子类预测的本体嵌入未解决类似和孤立实体的困难，并且未提取本体中注释公理的全局信息。本文提出了一种针对两种本体嵌入模型的自匹配训练方法：Inverted-index Matrix Embedding (InME) 和 Co-occurrence Matrix Embedding (CoME)。这两种嵌入通过每个单词在一组公理中出现的位置以及每个公理中单词的共现来捕获注释公理中的全局和局部信息。自匹配训练方法提高了概念子类预测的稳健性，当预测的超类与子类相似且孤立于本体中的其他实体时。

    arXiv:2402.16278v1 Announce Type: new  Abstract: Recently, ontology embeddings representing entities in a low-dimensional space have been proposed for ontology completion. However, the ontology embeddings for concept subsumption prediction do not address the difficulties of similar and isolated entities and fail to extract the global information of annotation axioms from an ontology. In this paper, we propose a self-matching training method for the two ontology embedding models: Inverted-index Matrix Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings capture the global and local information in annotation axioms by means of the occurring locations of each word in a set of axioms and the co-occurrences of words in each axiom. The self-matching training method increases the robustness of the concept subsumption prediction when predicted superclasses are similar to subclasses and are isolated to other entities in an ontology. Our evaluation experiments show that
    
[^73]: UniRetriever：各种情境自适应对话检索的多任务候选者选择

    UniRetriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval

    [https://arxiv.org/abs/2402.16261](https://arxiv.org/abs/2402.16261)

    提出了一种UniRetriever框架，利用双编码器架构和两个损失约束实现了多任务候选者选择，适用于不同情境下的对话检索任务。

    

    对话检索是指以迭代和交互方式运行的信息检索系统，需要检索各种外部资源（如人设、知识甚至回应）以有效与用户交互并成功完成对话。为了提高效率和性能，我们提出了一个多任务框架，作为三个主要检索任务的通用检索器：人设选择、知识选择和回应选择。为此，我们设计了一个双编码器架构，包括一个情境自适应对话编码器和一个候选者编码器，旨在通过简单的点积关注长对话中的相关上下文并检索合适的候选者。此外，我们引入了两个损失约束以捕捉...

    arXiv:2402.16261v1 Announce Type: new  Abstract: Conversational retrieval refers to an information retrieval system that operates in an iterative and interactive manner, requiring the retrieval of various external resources, such as persona, knowledge, and even response, to effectively engage with the user and successfully complete the dialogue. However, most previous work trained independent retrievers for each specific resource, resulting in sub-optimal performance and low efficiency. Thus, we propose a multi-task framework function as a universal retriever for three dominant retrieval tasks during the conversation: persona selection, knowledge selection, and response selection. To this end, we design a dual-encoder architecture consisting of a context-adaptive dialogue encoder and a candidate encoder, aiming to attention to the relevant context from the long dialogue and retrieve suitable candidates by simply a dot product. Furthermore, we introduce two loss constraints to capture t
    
[^74]: 基于知识内容选择的主题到文章生成

    Topic-to-essay generation with knowledge-based content selection

    [https://arxiv.org/abs/2402.16248](https://arxiv.org/abs/2402.16248)

    该论文提出了一种基于知识内容选择的复制机制模型，通过整合丰富的语义知识和改进的前缀调整方法，使主题到文章生成任务中的文本生成多样性提高，并贡献了新的中文数据集。

    

    主题到文章生成任务是一项具有挑战性的自然语言生成任务，旨在根据给定的主题词生成具有高语义连贯性的段落级文本。先前的研究主要集中在引入外部知识，而忽略了生成文本多样性不足的问题。为了提高生成多样性，我们提出了一种新颖的带有内容选择模块的复制机制模型，将语言模型的丰富语义知识整合到解码器中。此外，我们引入了改进的前缀调整方法来训练模型，使其能够适应不同的输入复杂性。此外，我们为TEG任务贡献了一个新的中文数据集。实验结果表明，与最先进的方法相比，所提出的模型可以将生成的文本多样性提高35%至59%，同时保持高水平的主题一致性。

    arXiv:2402.16248v1 Announce Type: cross  Abstract: The topic-to-essay generation task is a challenging natural language generation task that aims to generate paragraph-level text with high semantic coherence based on a given set of topic words. Previous work has focused on the introduction of external knowledge, ignoring the insufficient generated text diversity. In order to improve the generation diversity, we propose a novel copy mechanism model with a content selection module that integrates rich semantic knowledge from the language model into the decoder. Furthermore, we introduce the improved prefix tuning method to train the model, enabling it to adapt to varying input complexities. In addition, we have contributed a new Chinese dataset for TEG tasks. Experimental results demonstrate that the proposed model can improve the generated text diversity by 35\% to 59\% compared to the state-of-the-art method, while maintaining a high level of topic consistency.
    
[^75]: 学习翻译：应对合作语言习得的新兴沟通预训练

    Learning Translations: Emergent Communication Pretraining for Cooperative Language Acquisition

    [https://arxiv.org/abs/2402.16247](https://arxiv.org/abs/2402.16247)

    提出了一个名为合作语言习得问题（CLAP）的新颖人工智能挑战，通过允许代理在目标社区中从互动数据集中学习，放宽了Zero-Shot Coordination假设。

    

    在新兴沟通中，代理学习彼此进行沟通，但他们制定的协议是针对他们的训练群体的。这一观察结果导致了对于学习对未在训练中遇到的代理稳健的沟通策略的Zero-Shot Coordination（ZSC）的研究。但是，ZSC通常假设关于在零-shot设置中会遇到的代理的先前数据是无法获得的。在许多情况下，这提出了一个不必要的棘手问题，并排除了通过预先建立的约定进行沟通。我们提出了一个名为合作语言习得问题（CLAP）的新颖人工智能挑战，在其中通过允许“加入者”代理从目标社区内代理之间的互动数据集中学习来放宽了ZSC假设。我们提出并比较了解决CLAPs的两种方法：模仿学习（IL）和新兴沟通的预训练和翻译学习。

    arXiv:2402.16247v1 Announce Type: cross  Abstract: In Emergent Communication (EC) agents learn to communicate with one another, but the protocols that they develop are specialised to their training community. This observation led to research into Zero-Shot Coordination (ZSC) for learning communication strategies that are robust to agents not encountered during training. However, ZSC typically assumes that no prior data is available about the agents that will be encountered in the zero-shot setting. In many cases, this presents an unnecessarily hard problem and rules out communication via preestablished conventions. We propose a novel AI challenge called a Cooperative Language Acquisition Problem (CLAP) in which the ZSC assumptions are relaxed by allowing a 'joiner' agent to learn from a dataset of interactions between agents in a target community. We propose and compare two methods for solving CLAPs: Imitation Learning (IL), and Emergent Communication pretraining and Translation Learni
    
[^76]: HypoTermQA：用于评估LLMs幻觉倾向的假设术语数据集

    HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs

    [https://arxiv.org/abs/2402.16211](https://arxiv.org/abs/2402.16211)

    该论文引入了一个自动可扩展的框架，结合LLMs的幻觉倾向与高效的幻觉检测，创建了用于基准测试的HypoTermQA数据集。

    

    幻觉对于大型语言模型（LLMs）的可靠性和对齐性构成了重大挑战，限制了它们在聊天机器人应用以外的广泛接受程度。尽管不断努力，但幻觉仍然是LLMs中一个普遍存在的挑战。幻觉的检测本身也是一项艰巨的任务，经常需要人工标注或受限制的评估。本文介绍了一个自动可扩展的框架，将LLMs的幻觉倾向与高效的幻觉检测相结合进行基准测试。我们利用LLMs生成与假设现象相关的具有挑战性的任务，随后将它们用作有效幻觉检测的代理。该框架与领域无关，允许在任何领域中使用任何语言模型进行基准测试数据集的创建或评估。我们介绍了公开可用的HypoTermQA基准数据集，其中最先进模型的性能范围如下：

    arXiv:2402.16211v1 Announce Type: new  Abstract: Hallucinations pose a significant challenge to the reliability and alignment of Large Language Models (LLMs), limiting their widespread acceptance beyond chatbot applications. Despite ongoing efforts, hallucinations remain a prevalent challenge in LLMs. The detection of hallucinations itself is also a formidable task, frequently requiring manual labeling or constrained evaluations. This paper introduces an automated scalable framework that combines benchmarking LLMs' hallucination tendencies with efficient hallucination detection. We leverage LLMs to generate challenging tasks related to hypothetical phenomena, subsequently employing them as agents for efficient hallucination detection. The framework is domain-agnostic, allowing the use of any language model for benchmark creation or evaluation in any domain. We introduce the publicly available HypoTermQA Benchmarking Dataset, on which state-of-the-art models' performance ranged between 
    
[^77]: IR2：信息正则化用于信息检索

    IR2: Information Regularization for Information Retrieval

    [https://arxiv.org/abs/2402.16200](https://arxiv.org/abs/2402.16200)

    介绍了IR2，一种用于在合成数据生成过程中减少过拟合的信息正则化技术，在复杂查询的信息检索任务中表现出优越性能，同时将成本降低高达50%。

    

    有效地在训练数据有限的情况下进行信息检索（IR），特别是对于复杂查询，仍然是一项具有挑战性的任务。本文介绍了IR2，即信息检索的信息正则化，一种用于在合成数据生成过程中减少过拟合的技术。该方法在具有复杂查询特征的三个最近的IR任务上进行了测试：DORIS-MAE、ArguAna和WhatsThatBook。实验结果表明，我们的正则化技术不仅在所考虑的任务上优于先前的合成查询生成方法，而且还能将成本降低高达50％。此外，本文将不同阶段的三种正则化方法——输入、提示和输出进行了分类和探索，每种方法相对于没有正则化的模型均提供了不同程度的性能改进。

    arXiv:2402.16200v1 Announce Type: cross  Abstract: Effective information retrieval (IR) in settings with limited training data, particularly for complex queries, remains a challenging task. This paper introduces IR2, Information Regularization for Information Retrieval, a technique for reducing overfitting during synthetic data generation. This approach, representing a novel application of regularization techniques in synthetic data creation for IR, is tested on three recent IR tasks characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook. Experimental results indicate that our regularization techniques not only outperform previous synthetic query generation methods on the tasks considered but also reduce cost by up to 50%. Furthermore, this paper categorizes and explores three regularization methods at different stages of the query synthesis pipeline-input, prompt, and output-each offering varying degrees of performance improvement compared to models where no regulariz
    
[^78]: 通过基于注意力的情感建模增强聊天机器人中的共情能力

    ASEM: Enhancing Empathy in Chatbot through Attention-based Sentiment and Emotion Modeling

    [https://arxiv.org/abs/2402.16194](https://arxiv.org/abs/2402.16194)

    通过多个编码器的混合专家和专门的注意力策略，ASEM模型提供了共情回复生成所需的情感和关注性能。

    

    有效的特征表示在提升依赖深度神经网络的文本生成模型的性能中起着至关重要的作用。然而，当前的方法存在一些缺点，例如无法捕捉语言的深层语义和对输入变化的敏感性，导致生成文本发生显著变化。本文提出了一种新颖的解决方案，通过使用多个编码器的专家混合来提供用户话语情感状态的不同视角，同时提高性能。我们提出了一种名为ASEM的端到端模型架构，对开放领域聊天机器人进行情感分析，实现生成既自然又相关的共情回复。与传统的注意力机制不同，所提出的模型采用了一种独特的专门注意力策略。

    arXiv:2402.16194v1 Announce Type: new  Abstract: Effective feature representations play a critical role in enhancing the performance of text generation models that rely on deep neural networks. However, current approaches suffer from several drawbacks, such as the inability to capture the deep semantics of language and sensitivity to minor input variations, resulting in significant changes in the generated text. In this paper, we present a novel solution to these challenges by employing a mixture of experts, multiple encoders, to offer distinct perspectives on the emotional state of the user's utterance while simultaneously enhancing performance. We propose an end-to-end model architecture called ASEM that performs emotion analysis on top of sentiment analysis for open-domain chatbots, enabling the generation of empathetic responses that are fluent and relevant. In contrast to traditional attention mechanisms, the proposed model employs a specialized attention strategy that uniquely ze
    
[^79]: 通过语义平滑防御大型语言模型遭遇监狱攻击

    Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing

    [https://arxiv.org/abs/2402.16192](https://arxiv.org/abs/2402.16192)

    提出了一种名为SEMANTICSMOOTH的防御方法，通过聚合多个语义转换副本的预测结果来防御大型语言模型遭遇GCG、PAIR和AutoDAN攻击，同时保持了较强的正常性能。

    

    对齐的大型语言模型(LLMs)容易受到监狱攻击的威胁，这些攻击可以绕过目标LLMs的保护措施，并骗过它们生成令人反感的内容。我们提出了SEMANTICSMOOTH，一种基于平滑的防御方法，通过聚合多个经过语义转换的给定输入提示的预测结果，来提高对GCG、PAIR和AutoDAN攻击的抵抗能力。实验结果表明，SEMANTICSMOOTH在保持指导性基准测试（如InstructionFollowing和AlpacaEval）上的强劲性能的同时，实现了对各种攻击的最新技术防御。

    arXiv:2402.16192v1 Announce Type: new  Abstract: Aligned large language models (LLMs) are vulnerable to jailbreaking attacks, which bypass the safeguards of targeted LLMs and fool them into generating objectionable content. While initial defenses show promise against token-based threat models, there do not exist defenses that provide robustness against semantic attacks and avoid unfavorable trade-offs between robustness and nominal performance. To meet this need, we propose SEMANTICSMOOTH, a smoothing-based defense that aggregates the predictions of multiple semantically transformed copies of a given input prompt. Experimental results demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on instruction following benchmarks such as InstructionFollowing and AlpacaEval. The codes will be publicly available at https://github.com/UCSB-NLP-Chang/SemanticSmooth.
    
[^80]: 利用其优势攻击LLM水印

    Attacking LLM Watermarks by Exploiting Their Strengths

    [https://arxiv.org/abs/2402.16187](https://arxiv.org/abs/2402.16187)

    现有的LLM水印系统虽然具有质量保留、鲁棒性和公开检测API等优点，但也因此容易受到各种攻击，研究者提出了一套实用指南以缓解这些攻击。

    

    生成模型的进展使得人工智能生成的文本、代码和图片能够在许多应用中模仿人类生成的内容。水印技术旨在将信息嵌入模型的输出中以验证其来源，对于减少对这些人工智能生成内容的滥用非常有用。然而，现有的水印方案仍然令人意外地容易受到攻击。具体而言，我们展示了现有的LLM水印系统共享的可取特性，例如质量保留、鲁棒性和公开检测API，反过来却使这些系统容易遭受各种攻击。我们在常见水印设计选择方面严格研究潜在攻击，并提出了缓解攻击的最佳实践和防御措施——建立了一套嵌入和检测LLM水印的实用指南。

    arXiv:2402.16187v1 Announce Type: cross  Abstract: Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating misuse of such AI-generated content. However, existing watermarking schemes remain surprisingly susceptible to attack. In particular, we show that desirable properties shared by existing LLM watermarking systems such as quality preservation, robustness, and public detection APIs can in turn make these systems vulnerable to various attacks. We rigorously study potential attacks in terms of common watermark design choices, and propose best practices and defenses for mitigation -- establishing a set of practical guidelines for embedding and detection of LLM watermarks.
    
[^81]: 使用非线性方法和更多探针来探究编码信息的结构

    Hitting "Probe"rty with Non-Linearity, and More

    [https://arxiv.org/abs/2402.16168](https://arxiv.org/abs/2402.16168)

    使用非线性结构探针来探究编码信息的结构，并设计了简单有效的新方法，以及可视化框架来评估语言模型中的依存树结构。

    

    结构探针学习线性变换，以找到依存树如何嵌入语言模型的隐藏状态。我们引入非线性结构探针，重新设计了White等人介绍的非线性结构探针，使其设计更简单但有效。通过设计可视化框架，定性评估句子中两个单词在预测的依存树中的连接强度。我们利用该技术来理解哪种非线性探针变体擅长编码句法信息。此外，还用它定性研究了BERT在每个层中编码的依存树结构。

    arXiv:2402.16168v1 Announce Type: cross  Abstract: Structural probes learn a linear transformation to find how dependency trees are embedded in the hidden states of language models. This simple design may not allow for full exploitation of the structure of the encoded information. Hence, to investigate the structure of the encoded information to its full extent, we incorporate non-linear structural probes. We reformulate the design of non-linear structural probes introduced by White et al. making its design simpler yet effective. We also design a visualization framework that lets us qualitatively assess how strongly two words in a sentence are connected in the predicted dependency tree. We use this technique to understand which non-linear probe variant is good at encoding syntactical information. Additionally, we also use it to qualitatively investigate the structure of dependency trees that BERT encodes in each of its layers. We find that the radial basis function (RBF) is an effectiv
    
[^82]: DistALANER：开源软件生态系统中的远程监督主动学习增强命名实体识别

    DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem

    [https://arxiv.org/abs/2402.16159](https://arxiv.org/abs/2402.16159)

    提出了一种为开源软件系统量身定制的新颖命名实体识别技术，通过远程监督注释过程、语言启发、查找表、外部知识源和主动学习方法来提高模型性能，有效缓解了成本和专家标注人员稀缺问题，并在关系抽取下游任务中显著优于LLMs。

    

    本文提出了一种专为开源软件系统量身定制的新颖命名实体识别（NER）技术。我们的方法旨在通过采用全面的两步远程监督注释过程来解决软件数据标注稀缺的问题。该过程巧妙地利用语言启发、独特的查找表、外部知识源以及主动学习方法。通过利用这些强大的技术，我们不仅提高了模型性能，还有效地缓解了成本和专家标注人员稀缺所带来的限制。值得注意的是，我们的框架在很大程度上明显优于最先进的LLMs。我们还展示了NER在关系抽取的下游任务中的有效性。

    arXiv:2402.16159v1 Announce Type: new  Abstract: This paper proposes a novel named entity recognition (NER) technique specifically tailored for the open-source software systems. Our approach aims to address the scarcity of annotated software data by employing a comprehensive two-step distantly supervised annotation process. This process strategically leverages language heuristics, unique lookup tables, external knowledge sources, and an active learning approach. By harnessing these powerful techniques, we not only enhance model performance but also effectively mitigate the limitations associated with cost and the scarcity of expert annotators. It is noteworthy that our framework significantly outperforms the state-of-the-art LLMs by a substantial margin. We also show the effectiveness of NER in the downstream task of relation extraction.
    
[^83]: ChatMusician：理解和生成具有LLM的音乐内在

    ChatMusician: Understanding and Generating Music Intrinsically with LLM

    [https://arxiv.org/abs/2402.16153](https://arxiv.org/abs/2402.16153)

    ChatMusician 是一个集成了内在音乐能力的开源LLM，通过对文本兼容的音乐表示法进行持续预训练和微调，能够理解和生成音乐，表现优于GPT-4基准模型。

    

    虽然大型语言模型（LLMs）在文本生成方面展现出令人印象深刻的能力，但我们发现它们的能力尚未推广到音乐，也就是人类的创造性语言。我们介绍了ChatMusician，这是一个开源的LLM，集成了内在的音乐能力。它基于对文本兼容的音乐表示法ABC记谱的持续预训练和微调LLaMA2，并且将音乐视为第二语言。ChatMusician可以使用纯文本标记器理解和生成音乐，而无需任何外部多模态神经结构或标记器。有趣的是，赋予音乐能力并不会损害语言能力，甚至可以达到略高的MMLU分数。我们的模型能够根据文本、和弦、旋律、主题、音乐形式等创作结构良好、完整长度的音乐，超越了GPT-4的基线。在我们精心策划的大学级音乐理解基准上，MusicTheory

    arXiv:2402.16153v1 Announce Type: cross  Abstract: While Large Language Models (LLMs) demonstrate impressive capabilities in text generation, we find that their ability has yet to be generalized to music, humanity's creative language. We introduce ChatMusician, an open-source LLM that integrates intrinsic musical abilities. It is based on continual pre-training and finetuning LLaMA2 on a text-compatible music representation, ABC notation, and the music is treated as a second language. ChatMusician can understand and generate music with a pure text tokenizer without any external multi-modal neural structures or tokenizers. Interestingly, endowing musical abilities does not harm language abilities, even achieving a slightly higher MMLU score. Our model is capable of composing well-structured, full-length music, conditioned on texts, chords, melodies, motifs, musical forms, etc, surpassing GPT-4 baseline. On our meticulously curated college-level music understanding benchmark, MusicTheory
    
[^84]: 从文本到转化：大型语言模型多功能性的全面审查

    From Text to Transformation: A Comprehensive Review of Large Language Models' Versatility

    [https://arxiv.org/abs/2402.16142](https://arxiv.org/abs/2402.16142)

    该研究探讨了大型语言模型在各领域的多功能性，提出了LLMs在健身、城市规划、气候建模和灾难响应等领域中的潜在影响和创新方法。

    

    这项开创性的研究探讨了大型语言模型（LLMs）如生成式预训练变换器（GPT）和双向编码器表示来自变换器（BERT）在从技术、金融、医疗保健到教育等各领域的扩展。尽管这些LLMs在自然语言处理（NLP）中已经表现出色，但尚未系统地研究过它们对健身、整体幸福感、城市规划、气候建模以及灾害管理等领域的影响。除了全面分析LLMs在不同领域的广泛利用程度之外，该综述论文还识别了LLMs潜力尚未得到利用的研究空白和领域。该研究揭示了LLMs可以在健身与幸福、城市规划、气候建模和灾害响应等领域留下痕迹的创新方法，这可能会激励他人。

    arXiv:2402.16142v1 Announce Type: cross  Abstract: This groundbreaking study explores the expanse of Large Language Models (LLMs), such as Generative Pre-Trained Transformer (GPT) and Bidirectional Encoder Representations from Transformers (BERT) across varied domains ranging from technology, finance, healthcare to education. Despite their established prowess in Natural Language Processing (NLP), these LLMs have not been systematically examined for their impact on domains such as fitness, and holistic well-being, urban planning, climate modelling as well as disaster management. This review paper, in addition to furnishing a comprehensive analysis of the vast expanse and extent of LLMs' utility in diverse domains, recognizes the research gaps and realms where the potential of LLMs is yet to be harnessed. This study uncovers innovative ways in which LLMs can leave a mark in the fields like fitness and wellbeing, urban planning, climate modelling and disaster response which could inspire 
    
[^85]: PeriodicLoRA: 打破LoRA优化中的低秩瓶颈

    PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization

    [https://arxiv.org/abs/2402.16141](https://arxiv.org/abs/2402.16141)

    提出了PeriodicLoRA（PLoRA）来打破LoRA优化中的低秩瓶颈，通过多次累积低秩更新矩阵来实现更高的更新秩，从而提高性能。

    

    监督微调是使大型语言模型（LLMs）适应下游任务的常见方法，但全微调LLMs需要大量计算资源。最近，由于其成本效益，参数高效微调（PEFT）方法得到了广泛研究。 LoRA是最广泛使用的方法之一，它假设优化过程本质上是低维的。虽然LoRA微调是有效的，但与全微调相比仍存在性能差距，因为其权重更新仅限于低秩矩阵。为了打破LoRA优化中的低秩瓶颈，我们提出了PeriodicLoRA（PLoRA），它多次累积低秩更新矩阵以实现更高的更新秩。PLoRA具有多个训练阶段。在每个阶段，我们仍然仅更新LoRA权重。然而，在每个阶段结束时，我们将LoRA权重卸载到骨干参数中，然后...

    arXiv:2402.16141v1 Announce Type: new  Abstract: Supervised fine-tuning is the most common method to adapt large language models (LLMs) to downstream tasks, but full fine-tuning LLMs requires massive computational resources. Recently, parameter-efficient fine-tuning (PEFT) methods have been widely studied due to its cost-effectiveness. LoRA is one of the most widely used methods, which assumes that the optimization process is essentially low-dimensional. Although LoRA fine-tuning is effective, there is still a performance gap compared to full fine-tuning, since its weight update is limited to low-rank matrices. In order to break the low-rank bottleneck in LoRA Optimization, we propose PeriodicLoRA (PLoRA), which accumulates low-rank update matrices multiple times to achieve a higher update rank. PLoRA has multiple training stages. During each stage, we still update only the LoRA weights. However, at the end of each stage, we unload the LoRA weights into the backbone parameters and then
    
[^86]: 生成人工智能对术语定义的意义

    What Generative Artificial Intelligence Means for Terminological Definitions

    [https://arxiv.org/abs/2402.16139](https://arxiv.org/abs/2402.16139)

    生成人工智能工具如ChatGPT在提供定制化的语境特定含义方面表现出色，但在准确性方面存在挑战，可以辅助术语学家进行术语编纂，实现AI效率与人类专业知识的结合。

    

    本文探讨了生成人工智能（GenAI）对术语定义的创建和消费的影响。像ChatGPT这样的GenAI工具与传统术语资源相比，带来了一系列益处和挑战。ChatGPT在以交互式和定制化的方式提供特定语境含义方面表现出色，但在准确性方面面临挑战。识别资源中的术语定义可能会因其可靠性而继续存在。从术语学家的角度来看，诸如ChatGPT之类的工具使得AI辅助的术语编纂成为可能，包括后期编辑术语编纂，将AI效率与人类专业知识相结合，以实现更快速的定义创建。

    arXiv:2402.16139v1 Announce Type: cross  Abstract: This paper examines the impact of Generative Artificial Intelligence (GenAI) on the creation and consumption of terminological definitions. GenAI tools like ChatGPT present a mix of benefits and drawbacks compared to traditional terminological resources. ChatGPT excels in providing context-specific meanings in an interactive and customized fashion but faces challenges with accuracy. Terminological definitions in recognized resources will likely survive because of their reliability. From the point of view of the terminologist, tools like ChatGPT enable AI-assisted terminography, including post-editing terminography, as an approach blending AI efficiency with human expertise for faster definition creation.
    
[^87]: LSTPrompt: 长短期提示下的大型语言模型作为零-shot时间序列预测器

    LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by Long-Short-Term Prompting

    [https://arxiv.org/abs/2402.16132](https://arxiv.org/abs/2402.16132)

    LSTPrompt提出了一种新颖的方法，将时间序列预测任务分解为短期和长期预测子任务，并为每个子任务量身定制提示，旨在提高大型语言模型在零shot时间序列预测中的适应性和性能。

    

    时间序列预测在现实场景中有着广泛的应用。利用现成的大型语言模型进行提示展现了强大的零shot时间序列预测能力，同时保持计算效率。然而，现有的提示方法过分简化了时间序列预测，将其视为语言下一个标记的预测，忽视了其动态性以及与最先进的提示策略（如Chain-of-Thought）的融合。因此，我们提出了LSTPrompt，一种用于在零shot时间序列预测任务中提示LLMs的新方法。LSTPrompt将时间序列预测分解为短期和长期预测子任务，并为每个子任务量身定制提示。LSTPrompt引导LLMs定期重新评估预测机制，以增强适应性。广泛的评估表明，与现有的提示方法相比，LSTPrompt的性能始终更好，并且与基本时间序列预测模型相比具有竞争力。

    arXiv:2402.16132v1 Announce Type: cross  Abstract: Time-series forecasting (TSF) finds broad applications in real-world scenarios. Prompting off-the-shelf Large Language Models (LLMs) demonstrates strong zero-shot TSF capabilities while preserving computational efficiency. However, existing prompting methods oversimplify TSF as language next-token predictions, overlooking its dynamic nature and lack of integration with state-of-the-art prompt strategies such as Chain-of-Thought. Thus, we propose LSTPrompt, a novel approach for prompting LLMs in zero-shot TSF tasks. LSTPrompt decomposes TSF into short-term and long-term forecasting sub-tasks, tailoring prompts to each. LSTPrompt guides LLMs to regularly reassess forecasting mechanisms to enhance adaptability. Extensive evaluations demonstrate consistently better performance of LSTPrompt than existing prompting methods, and competitive results compared to foundation TSF models.
    
[^88]: InstructEdit：针对大型语言模型的基于指令的知识编辑

    InstructEdit: Instruction-based Knowledge Editing for Large Language Models

    [https://arxiv.org/abs/2402.16123](https://arxiv.org/abs/2402.16123)

    InstructEdit提出了一种基于指令的知识编辑技术，通过简单指令使编辑器适应不同任务的表现，显著提高了多任务编辑中的可靠性。

    

    大型语言模型的知识编辑可以提供一种有效的解决方案，以改变模型的行为而不会对整体性能产生消极影响。然而，当前的方法在跨任务的通用性方面存在问题，需要为每个任务设计一个独特的编辑器，这显著阻碍了更广泛的应用。为了解决这一问题，我们首先分析了知识编辑中的多任务泛化问题。具体地，我们开发了一种基于指令的编辑技术，称为InstructEdit，通过简单的指令促进编辑器同时适应各种任务的表现。通过为每个LLM只使用一个统一的编辑器，我们在实证方面表明，InstructEdit可以提高编辑器的控制能力，从而在多任务编辑设置中平均提高可靠性14.86%。此外，涉及保留未见任务的实验说明，InstructEdi

    arXiv:2402.16123v1 Announce Type: cross  Abstract: Knowledge editing for large language models can offer an efficient solution to alter a model's behavior without negatively impacting the overall performance. However, the current approach encounters issues with limited generalizability across tasks, necessitating one distinct editor for each task, which significantly hinders the broader applications. To address this, we take the first step to analyze the multi-task generalization issue in knowledge editing. Specifically, we develop an instruction-based editing technique, termed InstructEdit, which facilitates the editor's adaptation to various task performances simultaneously using simple instructions. With only one unified editor for each LLM, we empirically demonstrate that InstructEdit can improve the editor's control, leading to an average 14.86% increase in Reliability in multi-task editing setting. Furthermore, experiments involving holdout unseen task illustrate that InstructEdi
    
[^89]: FuseChat：对话模型知识融合

    FuseChat: Knowledge Fusion of Chat Models

    [https://arxiv.org/abs/2402.16107](https://arxiv.org/abs/2402.16107)

    FuseChat通过知识融合将多个对话模型的集体知识转移到目标语言模型中，避免了昂贵的预训练成本。

    

    虽然从头开始训练大型语言模型（LLMs）确实可以导致具有独特能力和优势的模型，但这种方法会产生巨大成本，并可能导致竞争能力的潜在冗余。一种替代策略是将现有的LLMs组合成更强大的LLM，从而减少昂贵的预训练的必要性。但是，由于LLMs的多样化架构，直接参数融合被证明是不可行的。最近，FuseLLM引入了知识融合的概念，通过轻量级的持续训练将多个结构多样的LLM的集体知识转移至目标LLM。在本报告中，我们扩展了FuseLLM框架的可扩展性和灵活性，实现了对话LLM的融合，生成了FuseChat。FuseChat包括两个主要阶段。首先，我们对结构和规模不同的源LLMs进行知识融合

    arXiv:2402.16107v1 Announce Type: new  Abstract: While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies. An alternative strategy is to combine existing LLMs into a more robust LLM, thereby diminishing the necessity for expensive pre-training. However, due to the diverse architectures of LLMs, direct parameter blending proves to be unfeasible. Recently, \textsc{FuseLLM} introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training. In this report, we extend the scalability and flexibility of the \textsc{FuseLLM} framework to realize the fusion of chat LLMs, resulting in \textsc{FuseChat}. \textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs t
    
[^90]: 解释预测概率：模型置信度还是人类标签变化？

    Interpreting Predictive Probabilities: Model Confidence or Human Label Variation?

    [https://arxiv.org/abs/2402.16102](https://arxiv.org/abs/2402.16102)

    提出了两种主要观点：一种认为预测概率表明模型置信度，另一种认为预测概率表明人类标签变化。作者建议同时考虑这两种观点以提高自然语言处理系统的可靠性和公平性。

    

    随着越来越强大且用户友好的自然语言处理系统的崛起，人们越来越关注评估它们是否能够很好地表示不确定性，通过评估其对结果的预测分布质量。我们确定了两种主要观点，它们推动了截然不同的评估协议。第一种将预测概率视为模型置信度的指示；第二种将其视为人类标签变化的指示。我们讨论它们的优点和局限性，并提出认为两者对于值得信赖和公平的自然语言处理系统都至关重要，但仅利用单个预测分布是有限的观点。我们建议工具并突出指向具有关于预测的不确定性和关于人类标签的不确定性脱钩表示模型的激动人心方向。

    arXiv:2402.16102v1 Announce Type: new  Abstract: With the rise of increasingly powerful and user-facing NLP systems, there is growing interest in assessing whether they have a good representation of uncertainty by evaluating the quality of their predictive distribution over outcomes. We identify two main perspectives that drive starkly different evaluation protocols. The first treats predictive probability as an indication of model confidence; the second as an indication of human label variation. We discuss their merits and limitations, and take the position that both are crucial for trustworthy and fair NLP systems, but that exploiting a single predictive distribution is limiting. We recommend tools and highlight exciting directions towards models with disentangled representations of uncertainty about predictions and uncertainty about human labels.
    
[^91]: 通过将标记映射到共享字符空间来训练双语语言模型

    Training a Bilingual Language Model by Mapping Tokens onto a Shared Character Space

    [https://arxiv.org/abs/2402.16065](https://arxiv.org/abs/2402.16065)

    通过将标记映射到共享字符空间，研究了阿拉伯-希伯来双语语言模型训练。结果表明，使用同时表示两种语言的统一脚本的语言模型在机器翻译上表现出色，相比于保持原有脚本的模型有着更好的性能表现。

    

    我们使用阿拉伯文文本的音译版本在希伯来语中训练了一个双语语言模型，以确保两种语言在同一脚本中表示。鉴于阿拉伯语和希伯来语之间的形态学、结构相似性以及大量共同词源词，我们评估了使用统一脚本表示两种语言的语言模型在需要跨语言知识的机器翻译上的表现。结果表明：我们的模型优于保持阿拉伯文本在阿拉伯脚本中的对比模型，展示了音译步骤的有效性。尽管我们的模型在数据集方面训练集大小约为其他现有语言模型的60％，但在机器翻译的两个方向上似乎提供了可比较的性能。

    arXiv:2402.16065v1 Announce Type: new  Abstract: We train a bilingual Arabic-Hebrew language model using a transliterated version of Arabic texts in Hebrew, to ensure both languages are represented in the same script. Given the morphological, structural similarities, and the extensive number of cognates shared among Arabic and Hebrew, we assess the performance of a language model that employs a unified script for both languages, on machine translation which requires cross-lingual knowledge. The results are promising: our model outperforms a contrasting model which keeps the Arabic texts in the Arabic script, demonstrating the efficacy of the transliteration step. Despite being trained on a dataset approximately 60% smaller than that of other existing language models, our model appears to deliver comparable performance in machine translation across both translation directions.
    
[^92]: 基于引文增强的LLM聊天机器人生成

    Citation-Enhanced Generation for LLM-based Chatbot

    [https://arxiv.org/abs/2402.16063](https://arxiv.org/abs/2402.16063)

    提出一种基于引文增强的LLM聊天机器人生成方法，采用检索模块搜索支持文档来解决幻觉内容产生的问题。

    

    大型语言模型（LLMs）在各种情景下展现出强大的通用智能，包括将它们集成到聊天机器人中。然而，基于LLM的聊天机器人面临的一个重要挑战是在回复中可能产生虚构内容，这严重限制了它们的适用性。本文提出了一种新颖的后续引用增强生成（CEG）方法，结合检索论证。与先前侧重于预防生成过程中幻觉的研究不同，我们的方法以后续方式解决了这个问题。它结合了一个检索模块来搜索与生成内容相关的支持文档，并采用基于自然语言推理的方法。

    arXiv:2402.16063v1 Announce Type: cross  Abstract: Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc \textbf{C}itation-\textbf{E}nhanced \textbf{G}eneration (\textbf{CEG}) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-ba
    
[^93]: 大型语言模型如何编码上下文知识？一项逐层探究研究

    How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study

    [https://arxiv.org/abs/2402.16061](https://arxiv.org/abs/2402.16061)

    本文首次通过探究任务研究了大型语言模型逐层编码知识的能力，实验结果显示LLMs更倾向于在上层编码更多的上下文知识。

    

    先前的研究展示了大型语言模型（LLMs）在检索事实和处理上下文知识方面的引人注目能力。然而，关于LLMs逐层编码知识的能力的研究有限，这挑战了我们对它们内部机制的理解。在本文中，我们致力于通过探究任务来首次研究LLMs逐层的能力。我们利用ChatGPT强大的生成能力构建探究数据集，提供与各种事实相对应的多样且连贯的证据。我们采用$\mathcal V$-usable信息作为验证指标，以更好地反映跨不同层编码上下文知识的能力。我们在有冲突和新获得知识方面的实验表明，LLMs：（1）更倾向于在上层编码更多的上下文知识；（2）主要在与知识相关的实体标记内编码上下文知识

    arXiv:2402.16061v1 Announce Type: new  Abstract: Previous work has showcased the intriguing capability of large language models (LLMs) in retrieving facts and processing context knowledge. However, only limited research exists on the layer-wise capability of LLMs to encode knowledge, which challenges our understanding of their internal mechanisms. In this paper, we devote the first attempt to investigate the layer-wise capability of LLMs through probing tasks. We leverage the powerful generative capability of ChatGPT to construct probing datasets, providing diverse and coherent evidence corresponding to various facts. We employ $\mathcal V$-usable information as the validation metric to better reflect the capability in encoding context knowledge across different layers. Our experiments on conflicting and newly acquired knowledge show that LLMs: (1) prefer to encode more context knowledge in the upper layers; (2) primarily encode context knowledge within knowledge-related entity tokens 
    
[^94]: 用更少的文字说更多：通过要点压缩理解提示学习行为

    Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression

    [https://arxiv.org/abs/2402.16058](https://arxiv.org/abs/2402.16058)

    提出了一种名为Gist-COCO的模型，通过要点压缩来帮助提示解释和工程，可以达到较高的压缩率，并且在实验中表现出优异性能。

    

    大型语言模型（LLMs）需要长度较长的提示作为输入上下文，以产生与用户意图一致的输出，这个过程在推理期间会产生额外的成本。本文提出了Gist COnditioned deCOding（Gist-COCO）模型，引入了一种新颖的压缩提示方法，同时还可以协助提示的解释和工程。Gist-COCO采用基于编码器-解码器的语言模型，然后将额外的编码器作为插件模块整合进来，使用要点标记来压缩提示。它微调压缩插件模块，并使用要点标记的表示来模拟原始提示在基本语言模型中的情况。通过将要点标记的表示口头化为要点提示，Gist-COCO的压缩能力可以推广到不同的LLMs，并且具有较高的压缩率。我们的实验表明，Gist-COCO在两个方面均优于以前的提示压缩模型

    arXiv:2402.16058v1 Announce Type: new  Abstract: Large language models (LLMs) require lengthy prompts as the input context to produce output aligned with user intentions, a process that incurs extra costs during inference. In this paper, we propose the Gist COnditioned deCOding (Gist-COCO) model, introducing a novel method for compressing prompts which also can assist the prompt interpretation and engineering. Gist-COCO employs an encoder-decoder based language model and then incorporates an additional encoder as a plugin module to compress prompts with inputs using gist tokens. It finetunes the compression plugin module and uses the representations of gist tokens to emulate the raw prompts in the vanilla language model. By verbalizing the representations of gist tokens into gist prompts, the compression ability of Gist-COCO can be generalized to different LLMs with high compression rates. Our experiments demonstrate that Gist-COCO outperforms previous prompt compression models in both
    
[^95]: LSTP: 语言引导的时空提示学习用于长篇视频文本理解

    LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding

    [https://arxiv.org/abs/2402.16050](https://arxiv.org/abs/2402.16050)

    LSTP提出了语言引导的时空提示学习方法，通过整合时间提示采样器（TPS）和空间提示求解器（SPS）以及一致的训练策略，显著提升了计算效率、时间理解和空间-时间对齐。

    

    尽管视频语言建模取得了进展，但在回应特定任务的语言查询时解释长篇视频的计算挑战仍然存在，这主要是由于高维视频数据的复杂性和语言与空间和时间上视觉线索之间的不一致性。为解决这一问题，我们引入了一种名为语言引导的时空提示学习（LSTP）的新方法。该方法具有两个关键组件：利用光流先验的时间提示采样器（TPS），可利用时间信息有效提取相关视频内容；以及灵巧地捕捉视觉和文本元素之间复杂空间关系的空间提示求解器（SPS）。通过将TPS和SPS与一致的训练策略相协调，我们的框架显著提升了计算效率、时间理解和空间-时间对齐。在两个挑战中的实证评估显示，我们的方法优于现有技术。

    arXiv:2402.16050v1 Announce Type: cross  Abstract: Despite progress in video-language modeling, the computational challenge of interpreting long-form videos in response to task-specific linguistic queries persists, largely due to the complexity of high-dimensional video data and the misalignment between language and visual cues over space and time. To tackle this issue, we introduce a novel approach called Language-guided Spatial-Temporal Prompt Learning (LSTP). This approach features two key components: a Temporal Prompt Sampler (TPS) with optical flow prior that leverages temporal information to efficiently extract relevant video content, and a Spatial Prompt Solver (SPS) that adeptly captures the intricate spatial relationships between visual and textual elements. By harmonizing TPS and SPS with a cohesive training strategy, our framework significantly enhances computational efficiency, temporal understanding, and spatial-temporal alignment. Empirical evaluations across two challeng
    
[^96]: LLMs带有思维链条是非因果推理者

    LLMs with Chain-of-Thought Are Non-Causal Reasoners

    [https://arxiv.org/abs/2402.16048](https://arxiv.org/abs/2402.16048)

    本文探讨了大型语言模型在推理过程中思维链条（CoT）的作用，发现LLMs在答案生成过程中与人类推理存在差异，相关因素包括语境学习、有监督微调以及对人类反馈的强化学习。

    

    本文探讨了大型语言模型（LLMs）推理中思维链条（CoT）的作用。尽管它有改善任务性能的潜力，但我们的分析揭示了在LLMs中正确答案跟随不正确CoTs的频率及反之。我们采用因果分析来评估CoTs/指令与LLMs答案之间的因果关系，揭示LLMs近似的结构因果模型（SCM）。通过比较暗示SCM与人类推理的SCM，我们突显了LLM和人类推理过程之间的差异。我们进一步研究了影响暗示SCM因果结构的因素，揭示了语境学习、有监督微调以及对人类反馈的强化学习显著影响因果关系。我们在https://github.com/StevenZHB/CoT_Causal_Analysis发布了代码和结果。

    arXiv:2402.16048v1 Announce Type: cross  Abstract: This paper explores the role of the Chain of Thought (CoT) in Large Language Models (LLMs) reasoning. Despite its potential to improve task performance, our analysis reveals a surprising frequency of correct answers following incorrect CoTs and vice versa. We employ causal analysis to assess the cause-effect relationship between CoTs/instructions and answers in LLMs, uncovering the Structural Causal Model (SCM) that LLMs approximate. By comparing the implied SCM with that of human reasoning, we highlight discrepancies between LLM and human reasoning processes. We further examine the factors influencing the causal structure of the implied SCM, revealing that in-context learning, supervised fine-tuning, and reinforcement learning on human feedback significantly impact the causal relations. We release the code and results at https://github.com/StevenZHB/CoT_Causal_Analysis.
    
[^97]: 通过多种群意识优化检测机器生成文本的最大均值离差

    Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy

    [https://arxiv.org/abs/2402.16041](https://arxiv.org/abs/2402.16041)

    通过多种群意识优化检测机器生成文本的最大均值离差，解决了机器生成文本与人工编写文本之间微妙的分布差异挑战。

    

    大型语言模型（LLMs）如ChatGPT在生成类人文本方面表现出色。然而，机器生成文本可能存在严重风险，如抄袭问题、误导性信息或幻觉问题。因此，在许多情况下，检测机器生成文本是非常紧迫和重要的。不幸的是，由于LLMs的出色表现，区分机器生成文本和人工编写文本之间的分布差异常常非常微妙，这是具有挑战性的。在这篇论文中，我们试图利用\textit{最大均值离差}（MMD）来解决这个问题，因为MMD可以很好地识别分布差异。然而，直接使用各种机器生成文本对MMD进行训练将导致MMD的方差显著增加，因为不同LLMs的机器生成文本可能包含\textit{多个文本群体}。这将严重损害MMD测量分布差异的能力。

    arXiv:2402.16041v1 Announce Type: new  Abstract: Large language models (LLMs) such as ChatGPT have exhibited remarkable performance in generating human-like texts. However, machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues, misleading information, or hallucination issues. Therefore, it is very urgent and important to detect MGTs in many situations. Unfortunately, it is challenging to distinguish MGTs and human-written texts because the distributional discrepancy between them is often very subtle due to the remarkable performance of LLMs. In this paper, we seek to exploit \textit{maximum mean discrepancy} (MMD) to address this issue in the sense that MMD can well identify distributional discrepancies. However, directly training a detector with MMD using diverse MGTs will incur a significantly increased variance of MMD since MGTs may contain \textit{multiple text populations} due to various LLMs. This will severely impair MMD's ability to measure the diff
    
[^98]: EHRNoteQA：用于在临床环境中评估大型语言模型的患者特定问题回答基准

    EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings

    [https://arxiv.org/abs/2402.16040](https://arxiv.org/abs/2402.16040)

    该研究介绍了EHRNoteQA，这是一个新颖的患者特定问题回答基准，旨在评估临床环境中的大型语言模型（LLMs），具有采用多项选择问题回答格式和需要分析多篇临床笔记的特点。

    

    该研究介绍了EHRNoteQA，这是一个新颖的患者特定问题回答基准，旨在评估临床环境中的大型语言模型（LLMs）。在MIMIC-IV电子健康记录（EHR）的基础上，由三位医疗专家团队精心策划了包含962个独特问题的数据集，每个问题都与特定患者的EHR临床笔记相关联。与现有基于EHR的基准不同的是：首先，它是第一个采用多项选择问题回答格式的数据集，这种设计选择在自动评估的背景下有效评估LLMs的得分性能，与其他格式相比。其次，它需要分析多篇临床笔记才能回答一个问题，反映了实际临床决策制定的复杂性，医生需要审查大量患者病史记录。我们对各种大型语言模型进行了全面评估。

    arXiv:2402.16040v1 Announce Type: new  Abstract: This study introduces EHRNoteQA, a novel patient-specific question answering benchmark tailored for evaluating Large Language Models (LLMs) in clinical environments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three medical professionals has curated the dataset comprising 962 unique questions, each linked to a specific patient's EHR clinical notes. What makes EHRNoteQA distinct from existing EHR-based benchmarks is as follows: Firstly, it is the first dataset to adopt a multi-choice question answering format, a design choice that effectively evaluates LLMs with reliable scores in the context of automatic evaluation, compared to other formats. Secondly, it requires an analysis of multiple clinical notes to answer a single question, reflecting the complex nature of real-world clinical decision-making where clinicians review extensive records of patient histories. Our comprehensive evaluation on various large language models
    
[^99]: 理解公众对AI会话代理的看法：跨文化分析

    Understanding Public Perceptions of AI Conversational Agents: A Cross-Cultural Analysis

    [https://arxiv.org/abs/2402.16039](https://arxiv.org/abs/2402.16039)

    该研究通过分析社交媒体讨论，比较了美国和中国人对AI会话代理的看法，发现中国参与者更愉悦地看待CAs，而美国参与者则更看重其功能性，温暖的看法是两国对CAs产生积极情绪的关键因素。

    

    会话代理（CAs）越来越多地融入日常生活，引发了社交媒体上的重要讨论。尽管先前的研究已经考察了公众对AI的看法，但关于CAs的研究明显不足，对文化差异在CA看法中的研究更少。为填补这一空白，本研究使用计算方法分析了约一百万个围绕CAs的社交媒体讨论，并比较了美国和中国人对CAs的话语和看法。我们发现，中国参与者倾向于愉悦地看待CAs，认为基于语音和具有实体表现的CAs更温暖和更有能力，并一般表达积极情绪。相比之下，美国参与者更看重CAs的功能性，持有矛盾的态度。温暖的看法是两国对CAs产生积极情绪的关键因素。我们讨论了为设计环境适应型CAs的实际影响。

    arXiv:2402.16039v1 Announce Type: cross  Abstract: Conversational Agents (CAs) have increasingly been integrated into everyday life, sparking significant discussions on social media. While previous research has examined public perceptions of AI in general, there is a notable lack in research focused on CAs, with fewer investigations into cultural variations in CA perceptions. To address this gap, this study used computational methods to analyze about one million social media discussions surrounding CAs and compared people's discourses and perceptions of CAs in the US and China. We find Chinese participants tended to view CAs hedonically, perceived voice-based and physically embodied CAs as warmer and more competent, and generally expressed positive emotions. In contrast, US participants saw CAs more functionally, with an ambivalent attitude. Warm perception was a key driver of positive emotions toward CAs in both countries. We discussed practical implications for designing contextually
    
[^100]: 深度学习方法用于改进肝细胞癌研究中的问答系统

    Deep Learning Approaches for Improving Question Answering Systems in Hepatocellular Carcinoma Research

    [https://arxiv.org/abs/2402.16038](https://arxiv.org/abs/2402.16038)

    深度学习技术在问答系统领域取得的成就，尤其是在肝细胞癌研究中，极大地推动了自然语言处理的发展。

    

    近年来，自然语言处理（NLP）领域的进展受益于深度学习技术的发展，特别是通过利用诸如GPU和TPU等强大的计算资源。像BERT和GPT-3这样的模型，在大量数据的训练下，彻底改变了语言理解和生成。这些预训练模型为各种任务提供了坚实的基础，包括语义理解、智能写作和推理，为更通用的人工智能铺平了道路。作为人工智能的一个重要应用，NLP旨在通过自然语言交互来弥合人与计算机之间的差距。本文深入探讨了基于大规模模型的NLP的当前格局和未来展望，重点放在这一领域内的问答系统上。分析了人工智能驱动的问答系统的实际案例和发展，以促进进一步的探索。

    arXiv:2402.16038v1 Announce Type: cross  Abstract: In recent years, advancements in natural language processing (NLP) have been fueled by deep learning techniques, particularly through the utilization of powerful computing resources like GPUs and TPUs. Models such as BERT and GPT-3, trained on vast amounts of data, have revolutionized language understanding and generation. These pre-trained models serve as robust bases for various tasks including semantic understanding, intelligent writing, and reasoning, paving the way for a more generalized form of artificial intelligence. NLP, as a vital application of AI, aims to bridge the gap between humans and computers through natural language interaction. This paper delves into the current landscape and future prospects of large-scale model-based NLP, focusing on the question-answering systems within this domain. Practical cases and developments in artificial intelligence-driven question-answering systems are analyzed to foster further explora
    
[^101]: 使用Transformer模型进行智能电子商务推荐的文本理解和生成

    Text Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations

    [https://arxiv.org/abs/2402.16035](https://arxiv.org/abs/2402.16035)

    本文回顾了Transformer预训练模型在电子商务领域的核心应用，包括文本理解和生成推荐系统等方面，在自动生成产品描述、情感分析、个性化推荐系统构建和客服对话自动处理等方面均取得了积极效果。

    

    随着人工智能技术的快速发展，Transformer结构的预训练模型已成为大型语言模型（LLM）任务的重要工具。在电子商务领域，这些模型特别广泛使用，从文本理解到生成推荐系统，为改善用户体验和优化服务流程提供了强大的技术支持。本文回顾了Transformer预训练模型在电子商务文本理解和推荐生成中的核心应用场景，包括但不限于产品描述的自动生成，用户评论的情感分析，个性化推荐系统的构建以及客户服务对话的自动处理。通过对模型的工作原理、实现过程和特定案例中的应用效果进行详细分析，本文强调了其独特优势。

    arXiv:2402.16035v1 Announce Type: cross  Abstract: With the rapid development of artificial intelligence technology, Transformer structural pre-training model has become an important tool for large language model (LLM) tasks. In the field of e-commerce, these models are especially widely used, from text understanding to generating recommendation systems, which provide powerful technical support for improving user experience and optimizing service processes. This paper reviews the core application scenarios of Transformer pre-training model in e-commerce text understanding and recommendation generation, including but not limited to automatic generation of product descriptions, sentiment analysis of user comments, construction of personalized recommendation system and automated processing of customer service conversations. Through a detailed analysis of the model's working principle, implementation process, and application effects in specific cases, this paper emphasizes the unique advan
    
[^102]: 使用深度学习技术在短英文文本中进行情绪分类

    Emotion Classification in Short English Texts using Deep Learning Techniques

    [https://arxiv.org/abs/2402.16034](https://arxiv.org/abs/2402.16034)

    该研究使用深度学习技术在短英文文本中识别情绪，发现基于迁移学习和BERT的文本嵌入方法在分类准确性上表现优异。

    

    从资源匮乏的语言中的有限文本数据集中检测情绪是一项严峻的挑战，需要专门的框架和计算策略。本研究对使用深度学习技术在短英文文本中识别情绪进行了彻底的研究。深度学习方法采用迁移学习和词嵌入，特别是BERT，以获得更高的准确性。为了评估这些方法，我们引入了“SmallEnglishEmotions”数据集，该数据集包含6372个带有五种主要情绪类别注释的不同短波斯文本。我们的实验表明，迁移学习和基于BERT的文本嵌入在准确分类数据集中的文本方面优于替代方法。

    arXiv:2402.16034v1 Announce Type: cross  Abstract: Detecting emotions in limited text datasets from under-resourced languages presents a formidable obstacle, demanding specialized frameworks and computational strategies. This study conducts a thorough examination of deep learning techniques for discerning emotions in short English texts. Deep learning approaches employ transfer learning and word embedding, notably BERT, to attain superior accuracy. To evaluate these methods, we introduce the "SmallEnglishEmotions" dataset, comprising 6372 varied short Persian texts annotated with five primary emotion categories. Our experiments reveal that transfer learning and BERT-based text embedding outperform alternative methods in accurately categorizing the text in the dataset.
    
[^103]: 不要忘记您的奖励价值: 通过基于价值的校准实现语言模型的对齐

    Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration

    [https://arxiv.org/abs/2402.16030](https://arxiv.org/abs/2402.16030)

    本文提出了一种基于价值的校准（VCB）方法，以解决大型语言模型与人类偏好之间的对齐问题，并在实验中表现出比现有方法更好的通用性、稳健性和稳定性。

    

    从人类反馈中进行强化学习（RLHF）显著提高了大型语言模型（LLM）的生成质量，但最近的研究提出了对近端策略优化（PPO）算法复杂性和不稳定性的担忧，提议一系列基于顺序的校准方法作为可行的替代方法。本文进一步探讨了当前基于顺序的方法，检查它们在利用奖励价值和解决不对齐问题方面的低效性。基于这些发现，我们提出了一种新颖的基于价值的校准（VCB）方法，以更好地使LLMs与人类偏好对齐。实验结果表明，VCB在AI助手和摘要数据集上超越了现有的对齐方法，在各种环境中提供了令人印象深刻的通用性、稳健性和稳定性。

    arXiv:2402.16030v1 Announce Type: cross  Abstract: While Reinforcement Learning from Human Feedback (RLHF) significantly enhances the generation quality of Large Language Models (LLMs), recent studies have raised concerns regarding the complexity and instability associated with the Proximal Policy Optimization (PPO) algorithm, proposing a series of order-based calibration methods as viable alternatives. This paper delves further into current order-based methods, examining their inefficiencies in utilizing reward values and addressing misalignment issues. Building upon these findings, we propose a novel \textbf{V}alue-based \textbf{C}ali\textbf{B}ration (VCB) method to better align LLMs with human preferences. Experimental results demonstrate that VCB surpasses existing alignment methods on AI assistant and summarization datasets, providing impressive generalizability, robustness, and stability in diverse settings.
    
[^104]: GraphWiz：用于图问题的指令跟随语言模型

    GraphWiz: An Instruction-Following Language Model for Graph Problems

    [https://arxiv.org/abs/2402.16029](https://arxiv.org/abs/2402.16029)

    GraphWiz是一个开源语言模型，通过引入指令调优数据集和直接偏好优化框架，能够高效解决各种图问题类型，平均准确率达到65%，超过了GPT-4的43.8%。

    

    大型语言模型（LLMs）在多个领域取得了令人印象深刻的成功，但它们在理解和解决复杂图问题方面的能力尚未得到充分探索。为弥合这一差距，我们引入了GraphInstruct，这是一个新颖而全面的指令调优数据集，旨在为语言模型提供处理各种图问题的能力，利用明确的推理路径。利用GraphInstruct，我们构建了GraphWiz，这是一个能够解决各种图问题类型并生成清晰推理过程的开源语言模型。为增强模型的能力和可靠性，我们将直接偏好优化（DPO）框架纳入图问题求解环境中。增强模型GraphWiz-DPO在九个具有不同复杂性水平的任务中取得了65%的平均准确率，超过了平均准确率为43.8%的GPT-4。此外，我们的研究深入探讨了...

    arXiv:2402.16029v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive success across several fields, but their proficiency in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel and comprehensive instruction-tuning dataset designed to equip language models with the ability to tackle a broad spectrum of graph problems using explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of resolving various graph problem types while generating clear reasoning processes. To enhance the model's capability and reliability, we incorporate the Direct Preference Optimization (DPO) framework into the graph problem-solving context. The enhanced model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%. Moreover, our research delves into the d
    
[^105]: HiGPT：异质图语言模型

    HiGPT: Heterogeneous Graph Language Model

    [https://arxiv.org/abs/2402.16024](https://arxiv.org/abs/2402.16024)

    该论文提出了HiGPT模型，致力于解决异质图学习中存在的泛化限制和分布不稳定性问题。

    

    异构图学习旨在捕捉异构图中实体之间的复杂关系和多样化关系语义，以获得节点和边的有意义表示。最近在异构图神经网络（HGNNs）领域取得了最先进的性能，通过考虑关系的异质性并使用专门的消息函数和聚合规则。然而，现有的异构图学习框架在泛化到不同的异构图数据集方面存在局限。大多数这些框架都遵循同一数据集上的“预训练”和“微调”范式，这限制了它们适应新的和看不见的数据的能力。这引出了一个问题：“我们是否能够将异质图模型泛化为适应具有节点令牌集和关系类型异质性分布变化的不同下游学习任务？”为了解决这些挑战，我们p

    arXiv:2402.16024v1 Announce Type: new  Abstract: Heterogeneous graph learning aims to capture complex relationships and diverse relational semantics among entities in a heterogeneous graph to obtain meaningful representations for nodes and edges. Recent advancements in heterogeneous graph neural networks (HGNNs) have achieved state-of-the-art performance by considering relation heterogeneity and using specialized message functions and aggregation rules. However, existing frameworks for heterogeneous graph learning have limitations in generalizing across diverse heterogeneous graph datasets. Most of these frameworks follow the "pre-train" and "fine-tune" paradigm on the same dataset, which restricts their capacity to adapt to new and unseen data. This raises the question: "Can we generalize heterogeneous graph models to be well-adapted to diverse downstream learning tasks with distribution shifts in both node token sets and relation type heterogeneity?'' To tackle those challenges, we p
    
[^106]: TMT: 通过将不同模态视为不同语言来实现语音、图像和文本之间的三模翻译

    TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages

    [https://arxiv.org/abs/2402.16021](https://arxiv.org/abs/2402.16021)

    将不同模态解释为不同语言，在语音、图像和文本之间实现了三模翻译，大大减少了计算成本。

    

    能够共同处理多模态信息正在成为一项重要任务。然而，有限的配对多模态数据和多模态学习中的大量计算要求阻碍了发展。我们提出了一种新颖的三模翻译（TMT）模型，可以在涵盖语音、图像和文本的任意模态之间进行翻译。我们引入了一个新颖的观点，即将不同模态解释为不同语言，并将多模态翻译视为一个成熟的机器翻译问题。为此，我们将语音和图像数据标记为离散标记，提供了跨模态的统一接口，并大大降低了计算成本。在提出的TMT中，多模态编码器-解码器进行核心翻译，而模态特定处理仅在标记化和去标记化阶段内进行。我们在所有六种模态上评估了提出的TMT。

    arXiv:2402.16021v1 Announce Type: cross  Abstract: The capability to jointly process multi-modal information is becoming an essential task. However, the limited number of paired multi-modal data and the large computational requirements in multi-modal learning hinder the development. We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning speech, image, and text. We introduce a novel viewpoint, where we interpret different modalities as different languages, and treat multi-modal translation as a well-established machine translation problem. To this end, we tokenize speech and image data into discrete tokens, which provide a unified interface across modalities and significantly decrease the computational cost. In the proposed TMT, a multi-modal encoder-decoder conducts the core translation, whereas modality-specific processing is conducted only within the tokenization and detokenization stages. We evaluate the proposed TMT on all six mod
    
[^107]: PST-Bench: 追踪和基准测试论文来源

    PST-Bench: Tracing and Benchmarking the Source of Publications

    [https://arxiv.org/abs/2402.16009](https://arxiv.org/abs/2402.16009)

    该论文研究了论文来源追踪问题，在计算机科学领域构建了高质量且不断增长的数据集 PST-Bench，揭示了不同主题之间的演化模式差异，并强调了该问题的难度和潜在研究方向。

    

    追踪研究论文的来源是研究人员面临的一项基本但具有挑战性的任务。论文之间数十亿级的引用关系阻碍了研究人员有效理解科学的演变过程。迄今为止，仍然缺乏由专业研究人员构建的准确且可扩展的数据集，以识别研究论文的直接来源，基于这一点，可以开发自动算法来扩展科学的演变知识。本文研究了论文来源追踪（PST）问题，并在计算机科学领域构建了一个高质量且不断增长的数据集 PST-Bench。基于 PST-Bench，我们揭示了几个有趣的发现，例如不同主题之间的演化模式差异。对各种方法的探索凸显了 PST-Bench 的难度，指出了该主题的潜在研究方向。数据集和代码已在 https://github.com 上提供。

    arXiv:2402.16009v1 Announce Type: cross  Abstract: Tracing the source of research papers is a fundamental yet challenging task for researchers. The billion-scale citation relations between papers hinder researchers from understanding the evolution of science efficiently. To date, there is still a lack of an accurate and scalable dataset constructed by professional researchers to identify the direct source of their studied papers, based on which automatic algorithms can be developed to expand the evolutionary knowledge of science. In this paper, we study the problem of paper source tracing (PST) and construct a high-quality and ever-increasing dataset PST-Bench in computer science. Based on PST-Bench, we reveal several intriguing discoveries, such as the differing evolution patterns across various topics. An exploration of various methods underscores the hardness of PST-Bench, pinpointing potential directions on this topic. The dataset and codes have been available at https://github.com
    
[^108]: 从噪音到清晰：通过文本嵌入的翻译揭示大型语言模型攻击的敌对后缀

    From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings

    [https://arxiv.org/abs/2402.16006](https://arxiv.org/abs/2402.16006)

    通过Adversarial Suffixes Embedding Translation Framework，将不可读的敌对后缀翻译为连贯、可读的文本，有助于更容易理解和分析大型语言模型生成有害内容的原因。

    

    大型语言模型（LLMs）的安全防御方法仍然有限，因为危险提示被手工策划为仅几种已知的攻击类型，这丧失了与新兴变体同步的能力。最近的研究发现，在有害指令后添加后缀可以突破LLMs的防御，并导致危险输出。虽然这种方法是有效的，但由于不可读性，存在一种孔隙，使得通过常见的防御方法如困惑度过滤器相对容易看穿这种对抗性后缀的内在机制。为了应对这一挑战，本文提出了一种敌对后缀嵌入翻译框架（ASETF），可以将不可读的敌对后缀翻译成连贯的可读文本，从而更容易理解和分析大型语言模型生成有害内容的原因。我们在LLMs上进行了实验，如LLaMa2等。

    arXiv:2402.16006v1 Announce Type: new  Abstract: The safety defense methods of Large language models(LLMs) stays limited because the dangerous prompts are manually curated to just few known attack types, which fails to keep pace with emerging varieties. Recent studies found that attaching suffixes to harmful instructions can hack the defense of LLMs and lead to dangerous outputs. This method, while effective, leaves a gap in understanding the underlying mechanics of such adversarial suffix due to the non-readability and it can be relatively easily seen through by common defense methods such as perplexity filters.To cope with this challenge, in this paper, we propose an Adversarial Suffixes Embedding Translation Framework(ASETF) that are able to translate the unreadable adversarial suffixes into coherent, readable text, which makes it easier to understand and analyze the reasons behind harmful content generation by large language models. We conducted experiments on LLMs such as LLaMa2, 
    
[^109]: 从多个推文参数中检测客户满意度的机器学习方法

    A Machine Learning Approach to Detect Customer Satisfaction From Multiple Tweet Parameters

    [https://arxiv.org/abs/2402.15992](https://arxiv.org/abs/2402.15992)

    使用机器学习方法分析推文以确定客户满意水平，有助于简化研究成千上万条推文并改进航空公司服务的繁琐过程。

    

    自互联网技术得以发展以来，客户满意度已成为企业发展的主要因素之一。在线平台已成为分享评论的主要场所之一。Twitter是其中之一，客户经常在该平台上发布他们的想法。在这些平台上对航班的评论已成为航空公司关注的焦点。积极的评论可以帮助公司增长，而消极的评论则可能迅速破坏其收入和声誉。因此，对航空公司来说，审查客户的反馈和体验、改进其服务以保持竞争力至关重要。然而，研究成千上万条推文并分析它们以确定客户满意度是一项相当困难的任务。利用机器学习方法分析推文以确定客户满意水平可以简化这一费时的过程。已经有一些关于这种策略的工作可自动化该过程使用机器学

    arXiv:2402.15992v1 Announce Type: cross  Abstract: Since internet technologies have advanced, one of the primary factors in company development is customer happiness. Online platforms have become prominent places for sharing reviews. Twitter is one of these platforms where customers frequently post their thoughts. Reviews of flights on these platforms have become a concern for the airline business. A positive review can help the company grow, while a negative one can quickly ruin its revenue and reputation. So it's vital for airline businesses to examine the feedback and experiences of their customers and enhance their services to remain competitive. But studying thousands of tweets and analyzing them to find the satisfaction of the customer is quite a difficult task. This tedious process can be made easier by using a machine learning approach to analyze tweets to determine client satisfaction levels. Some work has already been done on this strategy to automate the procedure using mach
    
[^110]: $C^3$: 用于高效跨语言自然语言理解的置信度校准模型级联

    $C^3$: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding

    [https://arxiv.org/abs/2402.15991](https://arxiv.org/abs/2402.15991)

    本研究提出了一种置信度校准模型级联方法，用于增强跨语言自然语言理解任务中小模型的推断效率并解决深度模型过度自信和跨语言置信度分布变化的问题。

    

    跨语言自然语言理解(NLU)是自然语言处理(NLP)中的一个关键任务。最近的进展已经看到多语言预训练语言模型(mPLMs)显著增强了这些任务的性能。然而，mPLMs在推断期间需要大量资源并产生高计算成本，这给在真实世界和实时系统中部署带来挑战。现有的模型级联方法通过贪婪地基于模型置信度得分从各种模型中选择能够处理当前输入的最轻量模型来增强推断效率。然而，深度模型往往表现出过度自信，而且置信度分布在不同语言之间有所变化。这导致较小模型发出自信但不正确的预测，阻碍它们有效地在测试语言中进行泛化。在本研究中，我们引入了一个置信度校准模型级联。

    arXiv:2402.15991v1 Announce Type: new  Abstract: Cross-lingual natural language understanding (NLU) is a critical task in natural language processing (NLP). Recent advancements have seen multilingual pre-trained language models (mPLMs) significantly enhance the performance of these tasks. However, mPLMs necessitate substantial resources and incur high computational costs during inference, posing challenges for deployment in real-world and real-time systems. Existing model cascade methods seek to enhance inference efficiency by greedily selecting the lightest model capable of processing the current input from a variety of models, based on model confidence scores. Nonetheless, deep models tend to exhibit overconfidence, and confidence distributions vary across languages. This leads to the emission of confident but incorrect predictions by smaller models, hindering their ability to generalize effectively across test languages. In this study, we introduce a confidence calibration model cas
    
[^111]: 基于似然的大型语言模型评估偏差的缓解

    Likelihood-based Mitigation of Evaluation Bias in Large Language Models

    [https://arxiv.org/abs/2402.15987](https://arxiv.org/abs/2402.15987)

    该论文研究了基于大型语言模型（LLM）的评估器中的似然偏差，并提出了一种缓解这种偏差的方法。

    

    大型语言模型(LLMs)被广泛用于评估自然语言生成任务的自动化指标。然而，似然作为衡量LLM对句子可信度的指标，可能会因句子表面差异（如词序和句子结构）而变化。因此，如果将LLMs用于评估，可能存在似然偏差：它们可能会高估具有较高似然性的句子，而低估具有较低似然性的句子。本文对LLM评估器中似然偏差的存在和影响进行了研究。我们还提出了一种缓解似然偏差的方法。我们的方法利用高度偏置的实例作为少样本示例进行上下文学习。我们在评估数据到文本和语法错误纠正任务时的实验结果显示，我们测试的几种LLMs显示出似然偏差。此外，我们提出的方法成功地减轻了这种偏差

    arXiv:2402.15987v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics. However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods. In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators. We also propose a method to mitigate the likelihood bias. Our method utilizes highly biased instances as few-shot examples for in-context learning. Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias. Furthermore, our proposed method successfully mitigates this bias, also impr
    
[^112]: 使用HuBERT进行对犬语言的语音和词汇发现

    Phonetic and Lexical Discovery of a Canine Language using HuBERT

    [https://arxiv.org/abs/2402.15985](https://arxiv.org/abs/2402.15985)

    使用HuBERT实现了对犬叫声的声素标签分类和词汇识别，发现了具有显著声学一致性的犬词汇，还开发了一个Web系统标记狗叫声中的声素n-gram。

    

    本文深入探讨了在犬叫声中潜在沟通模式的开创性探索，并超越了传统的语言分析障碍，大量依赖于人类先验知识和有限数据集来发现狗的叫声中的声音单元。我们提出了一种自监督方法，利用HuBERT实现了声素标签的准确分类，并识别了暗示犬叫声中一种基础词汇的声音模式。我们的研究结果表明，这些确定的犬词汇中存在显著的声学一致性，覆盖了所有观察到的犬叫声序列。我们进一步开发了一个基于Web的犬叫声标记系统。该系统可以在用户上传的狗叫声中突出显示词汇中存在的声素n-gram。

    arXiv:2402.15985v1 Announce Type: cross  Abstract: This paper delves into the pioneering exploration of potential communication patterns within dog vocalizations and transcends traditional linguistic analysis barriers, which heavily relies on human priori knowledge on limited datasets to find sound units in dog vocalization. We present a self-supervised approach with HuBERT, enabling the accurate classification of phoneme labels and the identification of vocal patterns that suggest a rudimentary vocabulary within dog vocalizations. Our findings indicate a significant acoustic consistency in these identified canine vocabulary, covering the entirety of observed dog vocalization sequences. We further develop a web-based dog vocalization labeling system. This system can highlight phoneme n-grams, present in the vocabulary, in the dog audio uploaded by users.
    
[^113]: 使用离散单元进行直接旁遮普语到英语的语音翻译

    Direct Punjabi to English speech translation using discrete units

    [https://arxiv.org/abs/2402.15967](https://arxiv.org/abs/2402.15967)

    论文致力于为低资源语言进行研究，提出了一个用于旁遮普语到英语的直接语音翻译模型，以缓解全球语音技术覆盖不足的问题。

    

    语音到语音翻译尚未达到与文本到文本翻译系统相同的覆盖水平。当前的语音技术在全球超过7000种语言中的覆盖范围上受到严重限制，导致超过一半的人口无法获得这种技术和共享体验。随着语音助手技术（例如社交机器人和语音转文本应用程序）以及听觉内容（例如播客和讲座）的兴起，确保技术对所有人都可用变得比以往任何时候都更加重要。语音翻译可以在缓解技术差距和创造一个更具包容性社会方面发挥至关重要的作用。为了为低资源语言做出贡献，我们的工作提出了一个针对印度语言之一的旁遮普语到英语的直接语音翻译模型。此外，我们探讨了使用离散语音表示的性能。

    arXiv:2402.15967v1 Announce Type: new  Abstract: Speech-to-speech translation is yet to reach the same level of coverage as text-to-text translation systems. The current speech technology is highly limited in its coverage of over 7000 languages spoken worldwide, leaving more than half of the population deprived of such technology and shared experiences. With voice-assisted technology (such as social robots and speech-to-text apps) and auditory content (such as podcasts and lectures) on the rise, ensuring that the technology is available for all is more important than ever. Speech translation can play a vital role in mitigating technological disparity and creating a more inclusive society. With a motive to contribute towards speech translation research for low-resource languages, our work presents a direct speech-to-speech translation model for one of the Indic languages called Punjabi to English. Additionally, we explore the performance of using a discrete representation of speech call
    
[^114]: GreenLLaMA: 一种带有解释的解毒框架

    GreenLLaMA: A Framework for Detoxification with Explanations

    [https://arxiv.org/abs/2402.15951](https://arxiv.org/abs/2402.15951)

    GreenLLaMA是一种全面的端到端解毒框架，通过跨平台语料库训练出的模型优于当前最先进的模型。

    

    先前关于解毒的研究工作分散在某种程度上，因为它们并没有涵盖到真实场景中所需的所有解毒方面。值得注意的是，先前的研究将开发解毒模型的任务局限在仅见过的平台子集上，没有探讨模型在未知平台上的表现如何。此外，这些工作没有解决不可解毒性这一现象，即毒性文本无法在不改变含义的情况下进行解毒。我们提出了GreenLLaMA，这是第一个全面的端到端解毒框架，旨在减轻上述限制。我们首先介绍了一个跨平台伪并行语料库，应用多步数据处理和生成策略利用ChatGPT。然后，我们使用跨平台语料库训练一套解毒模型。我们展示了我们的解毒模型优于使用人工注释的最先进模型的表现。

    arXiv:2402.15951v1 Announce Type: cross  Abstract: Prior works on detoxification are scattered in the sense that they do not cover all aspects of detoxification needed in a real-world scenario. Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored. Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified without altering the meaning. We propose GreenLLaMA, the first comprehensive end-to-end detoxification framework, which attempts to alleviate the aforementioned limitations. We first introduce a cross-platform pseudo-parallel corpus applying multi-step data processing and generation strategies leveraging ChatGPT. We then train a suite of detoxification models with our cross-platform corpus. We show that our detoxification models outperform the SoTA model trained with human-annotated par
    
[^115]: 大语言模型的泛化或记忆：数据污染与可信评估

    Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models

    [https://arxiv.org/abs/2402.15938](https://arxiv.org/abs/2402.15938)

    本文提出了一种通过LLMs输出分布进行污染检测的方法CDD，以及一种基于LLMs输出修正的可信评估方法TED，以应对大语言模型在数据污染和可信评估方面面临的挑战。

    

    最近关于大语言模型（LLMs）令人印象深刻能力的说法通常是通过在开放获取的基准上进行评估来支持的。考虑到LLMs的训练数据的庞大规模和广泛来源，它可能明确或隐含地包含测试数据，导致LLMs更容易受到数据污染的影响。然而，由于训练数据的不透明性、模型的黑盒访问以及合成训练数据的快速增长，对于LLMs来说检测和减轻数据污染面临着重大挑战。在本文中，我们提出了CDD，即通过LLMs输出分布进行污染检测的CDD。CDD仅需要采样文本来检测数据污染，通过识别LLMs输出分布的峰值来进行检测。为了减轻评估中数据污染的影响，我们还提出了TED：基于LLMs输出修正的可信评估。

    arXiv:2402.15938v1 Announce Type: cross  Abstract: Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's outp
    
[^116]: 跨越2D和3D视觉问答之间的鸿沟：一种用于3D VQA的融合方法

    Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA

    [https://arxiv.org/abs/2402.15933](https://arxiv.org/abs/2402.15933)

    通过问题条件的2D视图选择过程和双分支Transformer结构，将2D知识整合到3D-VQA系统中，从而弥补了当前方法在3D视觉问答中遇到的挑战。

    

    在3D视觉问答（3D VQA）中，充分注释数据的稀缺性和有限的视觉内容多样性阻碍了对新颖场景和3D概念的泛化（如ScanQA和SQA数据集仅利用了约800个场景）。目前的方法通过补充2D信息来辅助3D推理。然而，这些方法面临挑战：它们要么使用引入过于复杂且有时与问题无关的视觉线索的自上而下的2D视图，要么依靠来自2D VLM的全局聚合场景/图像级表示，从而丢失了细粒度的视觉语言相关性。为了克服这些局限性，我们的方法利用了问题条件下的2D视图选择过程，准确地指出了关键视觉线索的语义相关2D输入。然后，我们通过双分支Transformer结构将这种2D知识整合到3D-VQA系统中。这种结构采用了双Transformer设计，紧凑地结合

    arXiv:2402.15933v1 Announce Type: cross  Abstract: In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated data and limited visual content diversity hampers the generalization to novel scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and SQA dataset). Current approaches resort supplement 3D reasoning with 2D information. However, these methods face challenges: either they use top-down 2D views that introduce overly complex and sometimes question-irrelevant visual clues, or they rely on globally aggregated scene/image-level representations from 2D VLMs, losing the fine-grained vision-language correlations. To overcome these limitations, our approach utilizes question-conditional 2D view selection procedure, pinpointing semantically relevant 2D inputs for crucial visual clues. We then integrate this 2D knowledge into the 3D-VQA system via a two-branch Transformer structure. This structure, featuring a Twin-Transformer design, compactly combine
    
[^117]: 基于简单提示的文本去噪

    Frustratingly Simple Prompting-based Text Denoising

    [https://arxiv.org/abs/2402.15931](https://arxiv.org/abs/2402.15931)

    通过简单的文本去噪技术，本文挑战了传统观点，发现了对于自动作文评分任务的新视角，并强调了如何通过微小改变数据集可以提高最终结果。

    

    本文介绍了一种新颖的自动作文评分（AES）任务的视角，挑战了将ASAP数据集视为静态实体的传统观点。通过使用提示进行简单文本去噪技术，我们探索了数据集内的动态潜力。尽管我们承认以前重点放在构建回归系统上，但我们的论文强调通过文本去噪对数据集进行微小改变如何可以提高最终结果。

    arXiv:2402.15931v1 Announce Type: new  Abstract: This paper introduces a novel perspective on the automated essay scoring (AES) task, challenging the conventional view of the ASAP dataset as a static entity. Employing simple text denoising techniques using prompting, we explore the dynamic potential within the dataset. While acknowledging the previous emphasis on building regression systems, our paper underscores how making minor changes to a dataset through text denoising can enhance the final results.
    
[^118]: 基于语言能力评估语法错误纠正的提示策略

    Evaluating Prompting Strategies for Grammatical Error Correction Based on Language Proficiency

    [https://arxiv.org/abs/2402.15930](https://arxiv.org/abs/2402.15930)

    本研究通过研究LLM的表现和第二语言能力之间的互动，针对英语作为外语学习者的不同能力水平，评估了语法错误纠正的提示策略，发现过度纠正主要发生在高级语言学习者的写作中。

    

    英语学习者的写作范例可能与母语者不同。鉴于第二语言学习者根据其能力水平存在显著差异的错误类型，本文试图通过研究LLM的表现和第二语言能力之间的互动来减少过度纠正。我们的方法专注于针对不同能力水平的英语作为外语学习者进行零点和少点提示以及微调模型。我们调查了语法错误纠正的结果，发现过度纠正主要发生在高级语言学习者的写作（能力C）中，而并非在能力A（初学者水平）和能力B（中级水平）。经过微调的LLM甚至在少数提示的情况下，写作示例中的英语学习者实际上倾向于减少召回度量。为了使我们的论点具体化，我们对GEC结果进行了全面检查。

    arXiv:2402.15930v1 Announce Type: new  Abstract: The writing examples of English language learners may be different from those of native speakers. Given that there is a significant differences in second language (L2) learners' error types by their proficiency levels, this paper attempts to reduce overcorrection by examining the interaction between LLM's performance and L2 language proficiency. Our method focuses on zero-shot and few-shot prompting and fine-tuning models for GEC for learners of English as a foreign language based on the different proficiency. We investigate GEC results and find that overcorrection happens primarily in advanced language learners' writing (proficiency C) rather than proficiency A (a beginner level) and proficiency B (an intermediate level). Fine-tuned LLMs, and even few-shot prompting with writing examples of English learners, actually tend to exhibit decreased recall measures. To make our claim concrete, we conduct a comprehensive examination of GEC outc
    
[^119]: QuaCer-C：大型语言模型中知识理解的定量认证

    QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs

    [https://arxiv.org/abs/2402.15929](https://arxiv.org/abs/2402.15929)

    本文提出了一种新颖的认证框架QuaCer-C，用于正式认证大型语言模型中知识理解的能力，证书定量化且包含高置信度的概率界限，研究发现，随着参数数量的增加，知识理解能力提高，Mistral模型在这一评估中表现不如其他模型。

    

    大型语言模型（LLMs）在多个基准测试中展现出令人印象深刻的表现。然而，传统研究并未对LLMs的表现提供正式的保证。本文提出了一种新颖的LLM认证框架QuaCer-C，我们在此对知名LLMs的知识理解能力进行正式认证。我们的证书是定量的 - 它们包括对目标LLM在任何相关知识理解提示上给出正确答案的概率的高置信度紧密界限。我们针对Llama、Vicuna和Mistral LLMs的证书表明，知识理解能力随参数数量的增加而提高，并且Mistral模型在这一评估中表现不如其他模型。

    arXiv:2402.15929v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive performance on several benchmarks. However, traditional studies do not provide formal guarantees on the performance of LLMs. In this work, we propose a novel certification framework for LLM, QuaCer-C, wherein we formally certify the knowledge-comprehension capabilities of popular LLMs. Our certificates are quantitative - they consist of high-confidence, tight bounds on the probability that the target LLM gives the correct answer on any relevant knowledge comprehension prompt. Our certificates for the Llama, Vicuna, and Mistral LLMs indicate that the knowledge comprehension capability improves with an increase in the number of parameters and that the Mistral model is less performant than the rest in this evaluation.
    
[^120]: MultiContrievers: 稠密检索表示的分析

    MultiContrievers: Analysis of Dense Retrieval Representations

    [https://arxiv.org/abs/2402.15925](https://arxiv.org/abs/2402.15925)

    该论文对稠密检索器的信息捕获进行了分析，探讨了其与语言模型的比较、信息提取的可行性以及提取性与性能、性别偏见的关系。

    

    稠密检索器将源文档压缩为（可能是有损的）向量表示，然而目前对于失去和保留的信息以及它们如何影响下游任务的分析较少。我们进行了首次对比稠密检索器捕获的信息与它们基于的语言模型（如BERT与Contriever）之间的分析。我们使用25个MultiBert检查点作为随机初始化来训练MultiContrievers，这是一组25个contriever模型。我们测试特定信息（如性别和职业）是否可以从类似维基百科的文档的contriever向量中提取。我们通过信息论探测来衡量这种可提取性。然后我们研究了可提取性与性能、性别偏见之间的关系，以及这些结果对许多随机初始化和数据洗牌的敏感性。我们发现（1）contriever模型有显著增加的可提取性

    arXiv:2402.15925v1 Announce Type: cross  Abstract: Dense retrievers compress source documents into (possibly lossy) vector representations, yet there is little analysis of what information is lost versus preserved, and how it affects downstream tasks. We conduct the first analysis of the information captured by dense retrievers compared to the language models they are based on (e.g., BERT versus Contriever). We use 25 MultiBert checkpoints as randomized initialisations to train MultiContrievers, a set of 25 contriever models. We test whether specific pieces of information -- such as gender and occupation -- can be extracted from contriever vectors of wikipedia-like documents. We measure this extractability via information theoretic probing. We then examine the relationship of extractability to performance and gender bias, as well as the sensitivity of these results to many random initialisations and data shuffles. We find that (1) contriever models have significantly increased extracta
    
[^121]: PRP：传播通用扰动以攻击大型语言模型的守护栏

    PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails

    [https://arxiv.org/abs/2402.15911](https://arxiv.org/abs/2402.15911)

    PRP攻击策略成功地针对多种开源和闭源的守护模型实施了两步前缀攻击，有效跨越多个威胁模型。

    

    大型语言模型（LLMs）通常被设计为对人类无害。不幸的是，最近的研究表明，这些模型容易受到自动越狱攻击的影响，诱使它们生成有害内容。最近的LLMs通常包含了一个额外的防御层，即守护模型，这是第二个LLM，旨在检查和调节主要LLM的输出响应。我们的主要贡献是展示了一种新颖的攻击策略PRP，该策略成功地针对几种开源（如Llama 2）和闭源（如GPT 3.5）守护模型实施。PRP利用了一种两步基于前缀的攻击，通过（a）构建守护模型的通用对抗前缀，并（b）将此前缀传播到响应中。我们发现这一过程在多种威胁模型中都是有效的，包括对手根本无法访问守护模型的模型。我们的工作暗示了...

    arXiv:2402.15911v1 Announce Type: cross  Abstract: Large language models (LLMs) are typically aligned to be harmless to humans. Unfortunately, recent work has shown that such models are susceptible to automated jailbreak attacks that induce them to generate harmful content. More recent LLMs often incorporate an additional layer of defense, a Guard Model, which is a second LLM that is designed to check and moderate the output response of the primary LLM. Our key contribution is to show a novel attack strategy, PRP, that is successful against several open-source (e.g., Llama 2) and closed-source (e.g., GPT 3.5) implementations of Guard Models. PRP leverages a two step prefix-based attack that operates by (a) constructing a universal adversarial prefix for the Guard Model, and (b) propagating this prefix to the response. We find that this procedure is effective across multiple threat models, including ones in which the adversary has no access to the Guard Model at all. Our work suggests t
    
[^122]: SemEval-2024任务8：加权层平均RoBERTa用于黑匣子机器生成文本检测

    SemEval-2024 Task 8: Weighted Layer Averaging RoBERTa for Black-Box Machine-Generated Text Detection

    [https://arxiv.org/abs/2402.15873](https://arxiv.org/abs/2402.15873)

    本文介绍了SemEval-2024任务8的黑匣子机器生成文本检测中采用的加权层平均RoBERTa技术及结果

    

    这份文件包含了作者提交给SemEval 2024任务8会议论文的细节：多生成器、多领域和多语言黑匣子机器生成文本检测子任务A（单语）和B。随着大型语言模型（LLMs）的出现，机器生成文本的检测变得越来越重要。在这份文件中，我们阐述了用于执行相同任务的技术，以及所获得的结果。

    arXiv:2402.15873v1 Announce Type: new  Abstract: This document contains the details of the authors' submission to the proceedings of SemEval 2024's Task 8: Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection Subtask A (monolingual) and B. Detection of machine-generated text is becoming an increasingly important task, with the advent of large language models (LLMs). In this document, we lay out the techniques utilized for performing the same, along with the results obtained.
    
[^123]: SportQA：大型语言模型中体育理解的基准评估

    SportQA: A Benchmark for Sports Understanding in Large Language Models

    [https://arxiv.org/abs/2402.15862](https://arxiv.org/abs/2402.15862)

    SportQA是一个新的基准测试，旨在评估大型语言模型在体育理解方面的表现，包含超过70,000个问题涵盖不同难度级别的体育知识，并揭示了LLMs在基本体育知识上表现优异但在复杂情境推理方面存在挑战。

    

    arXiv:2402.15862v1 报告类型：新的 摘要：对体育领域进行深入理解，这是一项充满战略和动态内容的领域，对于推动自然语言处理（NLP）至关重要。这在评估和推进大型语言模型（LLMs）的背景下尤为重要，鉴于现有专门基准测试之间存在差距。为了弥合这一差距，我们引入了SportQA，这是一个专门设计用于评估LLMs在体育理解方面的新型基准测试。SportQA涵盖了超过70,000个跨三个不同难度级别的多项选择题，每个级别针对体育知识的不同方面，从基本历史事实到复杂的基于情景的推理任务。我们主要利用少样本学习范式辅以“联想链”提示方法对普遍的LLMs进行了彻底评估。我们的结果显示，虽然LLMs在基本的体育知识方面表现出色，但在更复杂的基于情景的推理方面却遇到了困难。

    arXiv:2402.15862v1 Announce Type: new  Abstract: A deep understanding of sports, a field rich in strategic and dynamic content, is crucial for advancing Natural Language Processing (NLP). This holds particular significance in the context of evaluating and advancing Large Language Models (LLMs), given the existing gap in specialized benchmarks. To bridge this gap, we introduce SportQA, a novel benchmark specifically designed for evaluating LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenario-based reasoning tasks. We conducted a thorough evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting. Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-bas
    
[^124]: MATHWELL: 在规模上生成教育数学应用题

    MATHWELL: Generating Educational Math Word Problems at Scale

    [https://arxiv.org/abs/2402.15861](https://arxiv.org/abs/2402.15861)

    使用MATHWELL模型生成了迄今为止最大的英文数学应用题数据集，其中包含20,490个问题，经领域专家评分结果显示，MATHWELL的问题中具有可执行解决方案并符合所有标准的份额比其他选择高出40%，其中74%的可解决问题同时做到了准确和适当。

    

    数学应用题在K-8教育中至关重要，但编写它们耗时且需要领域专业知识。我们认为语言模型可以通过自动生成规模化问题来支持K-8数学教育。为了教育性，生成的问题必须是1）可解决的，2）准确的，3）适当的。现有数据集未标记这些标准，因此不适合训练问题生成器。我们引入了MATHWELL，这是一个经过专家注释数据进行迭代微调的70B Llama-2模型，用于生成K-8数学应用题。借助MATHWELL，我们生成了迄今为止最大的英文应用题数据集，其中包含20,490个问题。经领域专家评分的3,484个问题发现，MATHWELL拥有比其他选择更高的可执行解决方案和满足所有标准的问题份额高出40％，其中74％的问题具有可解的、准确的和适当的解决方案。

    arXiv:2402.15861v1 Announce Type: new  Abstract: Math word problems are critical K-8 educational tools, but writing them is time-consuming and requires domain expertise. We suggest that language models can support K-8 math education by automatically generating problems at scale. To be educational, generated problems must be 1) solvable, 2) accurate, and 3) appropriate. Existing datasets are unlabeled for these criteria, making them ill-suited for training problem generators. We introduce MATHWELL, a Llama-2 (70B) model iteratively finetuned to generate K-8 math word problems using data from expert annotation. Using MATHWELL, we generate the largest English word problem dataset to date, containing 20,490 problems. 3,484 are scored by domain experts who find MATHWELL has a 40% higher share of problems that have executable solutions and meet all criteria than alternatives, with 74% of its problems with executable solutions being solvable, accurate, and appropriate.
    
[^125]: 针对鲁棒性语言模型的提示扰动一致性学习

    Prompt Perturbation Consistency Learning for Robust Language Models

    [https://arxiv.org/abs/2402.15833](https://arxiv.org/abs/2402.15833)

    微调大型语言模型可以产生与判别模型相当的性能，在分析和解决LLMs对输入提示中不同类型扰动的鲁棒性方面取得了重要进展

    

    大型语言模型（LLMs）在诸如问答和文本总结等多个自然语言处理任务中展现出令人印象深刻的性能。然而，在意图分类和槽填充（IC-SF）等序列标注任务上，它们的性能显著落后于判别模型。本文的贡献有三个方面。首先，我们展示了对足够大的LLMs进行微调可以产生与判别模型相媲美的IC-SF性能。接下来，我们系统分析了这些经过微调的模型由于三种不同而相关的输入扰动 - 同音词、同义词和释义 - 导致的性能恶化。最后，我们提出了一种高效的缓解方法，即提示扰动一致性学习。

    arXiv:2402.15833v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated impressive performance on a number of natural language processing tasks, such as question answering and text summarization. However, their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on the robustness of LLMs to various perturbations in the input prompts. The contributions of this paper are three-fold. First, we show that fine-tuning sufficiently large LLMs can produce IC-SF performance comparable to discriminative models. Next, we systematically analyze the performance deterioration of those fine-tuned models due to three distinct yet relevant types of input perturbations - oronyms, synonyms, and paraphrasing. Finally, we propose an efficient mitigation approach, Prompt Perturbatio
    
[^126]: 电信领域大型语言模型中的语言智能

    Linguistic Intelligence in Large Language Models for Telecommunications

    [https://arxiv.org/abs/2402.15818](https://arxiv.org/abs/2402.15818)

    本研究评估了大型语言模型在电信领域的语言智能知识和理解能力，通过对四个著名模型的零-shot评估，比较了它们在资源受限环境中的表现。

    

    大型语言模型(LLMs)已经成为自然语言处理(NLP)领域的重要进展，表现出在语言生成和其他与语言相关任务中的显著能力。本研究旨在评估LLMs在电信领域内的知识和理解能力。为了实现这一目标，我们对四个著名的LLMs-Llama-2、Falcon、Mistral和Zephyr进行了详尽的零-shot评估。这些模型比ChatGPT需要更少的资源，适用于资源受限环境。它们的性能与最先进的微调模型进行了比较。

    arXiv:2402.15818v1 Announce Type: new  Abstract: Large Language Models (LLMs) have emerged as a significant advancement in the field of Natural Language Processing (NLP), demonstrating remarkable capabilities in language generation and other language-centric tasks. Despite their evaluation across a multitude of analytical and reasoning tasks in various scientific domains, a comprehensive exploration of their knowledge and understanding within the realm of natural language tasks in the telecommunications domain is still needed. This study, therefore, seeks to evaluate the knowledge and understanding capabilities of LLMs within this domain. To achieve this, we conduct an exhaustive zero-shot evaluation of four prominent LLMs-Llama-2, Falcon, Mistral, and Zephyr. These models require fewer resources than ChatGPT, making them suitable for resource-constrained environments. Their performance is compared with state-of-the-art, fine-tuned models. To the best of our knowledge, this is the firs
    
[^127]: RNN语言模型归纳偏差的一个理论结果

    A Theoretical Result on the Inductive Bias of RNN Language Models

    [https://arxiv.org/abs/2402.15814](https://arxiv.org/abs/2402.15814)

    RNN语言模型能够有效表示更大类别的语言模型，具有有界堆栈和广义堆栈更新函数，类似于保留固定数量符号记忆并使用简单更新机制的自动机。

    

    最近Hewitt等人（2020）的工作提出了对循环神经网络（RNNs）作为语言模型（LMs）的经验成功可能性的一个解释。 它显示RNNs可以有效地表示在人类语言中普遍存在的有界分层结构。 这表明RNNs的成功可能与它们建模层次结构的能力有关。 然而，对Hewitt等人（2020）构造的更详细检查表明，它不限于分层LMs，这引出了RNNs可以有效表示哪些\emph{其他类型} LMs的问题。 为此，我们概括他们的构造以展示RNNs可以有效表示更大类别的LMs：可以通过带有有界堆栈和广义堆栈更新函数的下推自动机表示的那些。 这类似于一个保留固定数量符号记忆并使用简单更新机制更新记忆的自动机。

    arXiv:2402.15814v1 Announce Type: new  Abstract: Recent work by Hewitt et al. (2020) provides a possible interpretation of the empirical success of recurrent neural networks (RNNs) as language models (LMs).   It shows that RNNs can efficiently represent bounded hierarchical structures that are prevalent in human language.   This suggests that RNNs' success might be linked to their ability to model hierarchy.   However, a closer inspection of Hewitt et al.'s (2020) construction shows that it is not limited to hierarchical LMs, posing the question of what \emph{other classes} of LMs can be efficiently represented by RNNs.   To this end, we generalize their construction to show that RNNs can efficiently represent a larger class of LMs: Those that can be represented by a pushdown automaton with a bounded stack and a generalized stack update function.   This is analogous to an automaton that keeps a memory of a fixed number of symbols and updates the memory with a simple update mechanism.  
    
[^128]: 评估LLMs的谈判能力：一个基准和一个买方增强方法

    Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method

    [https://arxiv.org/abs/2402.15813](https://arxiv.org/abs/2402.15813)

    首次将谈判任务形式化描述为不完全信息的不对称游戏，提出了一种用于评估代理表现的方法，并发现扮演买方的难度大且模型大小不能有效提高买方表现，为此提出了一种名为OG-Narrator的新方法。

    

    谈判是人类之间谈判的一个重要且独特的部分。随着基于LLM的代理学习谈判并表现得像真正的人类一样，如何评估代理的谈判能力仍然是一个悬而未决的问题。我们第一次将谈判任务形式化描述为一种不完全信息的不对称游戏，定义了买方和卖方在多次谈判过程中的收益，使我们能够定量评估一个代理在谈判任务中的表现。我们收集了一个真实产品价格数据集AmazonHistoryPrice，并对各种LLM代理的谈判能力进行了评估。我们发现扮演买方比扮演卖方要困难得多，并且增加模型大小无法有效地提高买方的表现。为了解决这一挑战，我们提出了一种称为OG-Narrator的新方法，该方法集成了一个确定性的报价生成器来控制买方报价的价格范围，并且集成了一个LLM解说者来创建一种自然

    arXiv:2402.15813v1 Announce Type: new  Abstract: Bargaining is an important and unique part of negotiation between humans. As LLM-driven agents learn to negotiate and act like real humans, how to evaluate agents' bargaining abilities remains an open problem. For the first time, we formally described the Bargaining task as an asymmetric incomplete information game, defining the gains of the Buyer and Seller in multiple bargaining processes. It allows us to quantitatively assess an agent's performance in the Bargain task. We collected a real product price dataset, AmazonHistoryPrice, and conducted evaluations of various LLM agents' bargaining abilities. We find that playing a Buyer is much harder than a Seller, and increasing model size can not effectively improve the Buyer's performance. To address the challenge, we propose a novel approach called OG-Narrator that integrates a deterministic Offer Generator to control the price range of Buyer's offers, and an LLM Narrator to create natur
    
[^129]: OAG-Bench：面向学术图挖掘的人工筛选基准

    OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining

    [https://arxiv.org/abs/2402.15810](https://arxiv.org/abs/2402.15810)

    OAG-Bench是一个基于开放学术图的全面、多方面和精细化人工筛选基准，涵盖了多个任务、数据集、基准和实验结果，旨在促进学术图挖掘。

    

    随着科学文献的迅速增长，多功能的学术知识服务越来越依赖全面的学术图挖掘。尽管公开学术图、基准和数据集已经有了，但这些资源通常在多方面和细粒度注释方面存在不足，受限于特定任务类型和领域，或者缺乏真实学术图。本文提出了基于开放学术图（OAG）的全面、多方面和精细化人工筛选基准OAG-Bench。OAG-Bench涵盖了10个任务，20个数据集，70+个基准和120+个截至目前的实验结果。我们针对某些任务提出了新的数据注释策略，并提供一套数据预处理代码、算法实现和标准化评估协议，以促进学术图挖掘。大量实验表明，即使是大型语言模型（LLMs）这样的先进算法也会在某些任务上受限。

    arXiv:2402.15810v1 Announce Type: cross  Abstract: With the rapid proliferation of scientific literature, versatile academic knowledge services increasingly rely on comprehensive academic graph mining. Despite the availability of public academic graphs, benchmarks, and datasets, these resources often fall short in multi-aspect and fine-grained annotations, are constrained to specific task types and domains, or lack underlying real academic graphs. In this paper, we present OAG-Bench, a comprehensive, multi-aspect, and fine-grained human-curated benchmark based on the Open Academic Graph (OAG). OAG-Bench covers 10 tasks, 20 datasets, 70+ baselines, and 120+ experimental results to date. We propose new data annotation strategies for certain tasks and offer a suite of data pre-processing codes, algorithm implementations, and standardized evaluation protocols to facilitate academic graph mining. Extensive experiments reveal that even advanced algorithms like large language models (LLMs) en
    
[^130]: 通过行为学习增强大型语言模型代理

    Empowering Large Language Model Agents through Action Learning

    [https://arxiv.org/abs/2402.15809](https://arxiv.org/abs/2402.15809)

    学习新动作的能力对于大型语言模型代理的学习进步至关重要，本研究提出了开放式行为学习框架，通过迭代学习策略改进动作，增强代理的学习效果。

    

    大型语言模型（LLM）代理近来引起越来越多的关注，然而它们在从试错中学习的能力方面存在限制，这是智能行为的关键因素。本研究认为，从经验中学习新动作的能力对于LLM代理的学习进步至关重要。虽然人类自然地扩展他们的动作空间并通过经验学习发展技能，但LLM代理通常在固定的动作空间内操作，限制了它们的成长潜力。为解决这些挑战，我们的研究探讨了语言代理的开放式行为学习。我们提出了一个名为LearnAct的框架，采用迭代学习策略来创建和改进Python函数形式的动作。在每次迭代中，LLM根据在失败的训练任务中识别出的错误，修订和更新当前可用的动作，从而增强动作的有效性。我们的实验评估是...

    arXiv:2402.15809v1 Announce Type: new  Abstract: Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior. In this work, we argue that the capacity to learn new actions from experience is fundamental to the advancement of learning in LLM agents. While humans naturally expand their action spaces and develop skills through experiential learning, LLM agents typically operate within fixed action spaces, limiting their potential for growth. To address these challenges, our study explores open-action learning for language agents. We introduce a framework LearnAct with an iterative learning strategy to create and improve actions in the form of Python functions. In each iteration, LLM revises and updates the currently available actions based on the errors identified in unsuccessful training tasks, thereby enhancing action effectiveness. Our experimental evaluations acr
    
[^131]: 在跳槽之前三思：问题细化提示改善大型语言模型的数学推理能力

    Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models

    [https://arxiv.org/abs/2402.15764](https://arxiv.org/abs/2402.15764)

    PEP提出了一个新方法来改善LLMs的数学能力，通过在推理之前细化和阐明问题背景，提升全局上下文建模能力，减少解析困难。

    

    大型语言模型（LLMs）在自然语言处理任务中表现出色，但在复杂推理任务中仍面临挑战，并且对输入上下文敏感。本研究提出了一种新的方法，名为问题细化提示（PEP），旨在在推理之前分解和阐明问题背景，从而增强全局上下文建模和减少解析困难。实验结果表明，PEP在复杂推理任务上表现出色，对于问题提出的效果显著。

    arXiv:2402.15764v1 Announce Type: cross  Abstract: Large language models~(LLMs) have exhibited impressive performance across NLP tasks. So far they still face challenges in complex reasoning tasks and can be sensitive to input context. Despite significant efforts have been invested in enhancing reasoning process and improving prefix-prompts robustness, the crucial role of problem context has been overlooked. In this study, we propose a new approach to improve the mathematical capacities of LLMs, named Problem Elaboration Prompting~(PEP). Specifically, PEP decomposes and elucidates the problem context before reasoning, thus enhancing the global context modeling and reducing the parsing difficulties. Experiments on datasets demonstrate promising performances on complex reasoning and indicate the beneficial impact for ill-formed problems. For instance, with the GPT-3.5 model~(\texttt{text-davinci-003}), we observed a 9.93\% improvement with greedy decoding and 8.80\% improvement with self
    
[^132]: Chimera: 融合所有令牌的无损解码方法，加速大型语言模型推理

    Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens

    [https://arxiv.org/abs/2402.15758](https://arxiv.org/abs/2402.15758)

    提出了Chimera框架，用于加速大型语言模型推理，通过引入轻量级的草稿模型和两种策略，利用先前生成的令牌来预测后续单词，以解决解码过程中的准确性和效率问题

    

    大型语言模型（LLMs）在各种任务中展示了显著的能力。然而，它们的广泛应用被资源密集型的解码过程所阻碍。为了解决这一挑战，目前的方法已经合并了额外的解码头，以实现对多个后续令牌的并行预测，从而实现推理加速。然而，这些解码头的准确性远不及自回归解码方法。鉴于这些限制，我们提出了Chimera，这是一个专门为推测采样设计的新框架。在这个框架内，我们引入了一个轻量级的草稿模型，能够有效利用先前生成的令牌来预测后续单词。为了确保准确性和效率，我们在轻量级草稿模型中提出了两种策略。首先，我们专注于在底层捕获短程依赖性。其次，我们利用

    arXiv:2402.15758v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their widespread application is hindered by the resource-intensive decoding process. To address this challenge, current approaches have incorporated additional decoding heads to enable parallel prediction of multiple subsequent tokens, thereby achieving inference acceleration. Nevertheless, the accuracy of these decoding heads falls short of the auto-regressive decoding approach.   In light of these limitations, we propose Chimera, a novel framework specifically designed for speculative sampling. Within this framework, we introduce a lightweight draft model that effectively utilizes previously generated tokens to predict subsequent words. To ensure both accuracy and efficiency, we present two strategies within the lightweight draft model. Firstly, we focus on capturing short-range dependencies at the bottom layer. Secondly, we leverage
    
[^133]: 通过Few-shot Learning和SBERT Fine-tuning进行牙科严重性评估

    Dental Severity Assessment through Few-shot Learning and SBERT Fine-tuning

    [https://arxiv.org/abs/2402.15755](https://arxiv.org/abs/2402.15755)

    通过Few-shot Learning和SBERT Fine-tuning方法，研究发现该方法在口腔健康问题的严重性评估中表现优异，准确率高达94.1%。

    

    牙科疾病严重影响着相当一部分人口，导致各种健康问题，这可能会对个人的整体幸福产生不利影响。将自动化系统整合到口腔保健中变得越来越重要。机器学习方法为解决诊断困难、效率低下和口腔疾病诊断中的错误等挑战提供了可行的解决方案。当医生们难以预测或诊断疾病的早期阶段时，这些方法尤其有用。本研究利用十三种不同的机器学习、深度学习和大型语言模型根据放射科医生的报告来确定口腔健康问题的严重程度。结果显示，Few-shot learning结合SBERT和多层感知器模型在各种实验中优于所有其他模型，达到94.1%的令人印象深刻的准确率。

    arXiv:2402.15755v1 Announce Type: new  Abstract: Dental diseases have a significant impact on a considerable portion of the population, leading to various health issues that can detrimentally affect individuals' overall well-being. The integration of automated systems in oral healthcare has become increasingly crucial. Machine learning approaches offer a viable solution to address challenges such as diagnostic difficulties, inefficiencies, and errors in oral disease diagnosis. These methods prove particularly useful when physicians struggle to predict or diagnose diseases at their early stages. In this study, thirteen different machine learning, deep learning, and large language models were employed to determine the severity level of oral health issues based on radiologists' reports. The results revealed that the Few-shot learning with SBERT and Multi-Layer Perceptron model outperformed all other models across various experiments, achieving an impressive accuracy of 94.1% as the best r
    
[^134]: HD-Eval: 通过分层标准分解对齐大型语言模型评估器

    HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition

    [https://arxiv.org/abs/2402.15754](https://arxiv.org/abs/2402.15754)

    提出了一个名为HD-Eval的框架，通过分层标准分解来对齐大型语言模型评估器与人类偏好，从而提升评估效果。

    

    大型语言模型(LLMs)已经成为昂贵人工评估的一个有前景的替代方案。然而，基于LLMs的评估的对齐和覆盖通常受到评估提示和标准的范围和潜在偏见的限制。为了解决这一挑战，我们提出HD-Eval，这是一个新颖的框架，通过分层标准分解迭代对齐LLM-based评估器与人类喜好。HD-Eval继承了人类专家评估心态的精髓，并通过将给定的评估任务分解为更精细的标准、根据估计的人类偏好聚合它们、通过归因修剪不显著的标准以及进一步分解显著的标准来增强LLM的评估器的对齐。通过在迭代对齐训练过程中集成这些步骤，我们获得了一个全面捕捉自然语言方面的标准的分层分解。

    arXiv:2402.15754v1 Announce Type: new  Abstract: Large language models (LLMs) have emerged as a promising alternative to expensive human evaluations. However, the alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria. To address this challenge, we propose HD-Eval, a novel framework that iteratively aligns LLM-based evaluators with human preference via Hierarchical Criteria Decomposition. HD-Eval inherits the essence from the evaluation mindset of human experts and enhances the alignment of LLM-based evaluators by decomposing a given evaluation task into finer-grained criteria, aggregating them according to estimated human preferences, pruning insignificant criteria with attribution, and further decomposing significant criteria. By integrating these steps within an iterative alignment training process, we obtain a hierarchical decomposition of criteria that comprehensively captures aspects of natural lang
    
[^135]: 稀疏MeZO：在零阶LLM微调中减少参数以获得更好性能

    Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning

    [https://arxiv.org/abs/2402.15751](https://arxiv.org/abs/2402.15751)

    提出了一种稀疏MeZO方法，通过仅对精心选择的参数子集应用零阶优化，实现了在零阶LLM微调中减少参数以获得更好性能的目标

    

    在针对特定任务进行大型语言模型（LLMs）微调通常会产生令人印象深刻的结果，但由于基于梯度的训练中的反向传播而导致内存效率低下。最近提出的高效利用存储器的零阶（MeZO）优化器旨在解决这个问题，在训练过程中只需要前向传递，使其更符合内存友好性。然而，零阶优化中梯度估计的质量往往取决于数据的维数，这可能解释了为什么与各种任务中的标准微调相比，MeZO仍然表现出显著的性能下降。受到参数高效微调（PEFT）成功的启发，本文介绍了稀疏MeZO，这是一种新颖的内存高效的零阶优化方法，仅将ZO应用于精心选择的参数子集。我们提出了一种简单而有效的参数选择方案，获得了显著的性能提升。

    arXiv:2402.15751v1 Announce Type: cross  Abstract: While fine-tuning large language models (LLMs) for specific tasks often yields impressive results, it comes at the cost of memory inefficiency due to back-propagation in gradient-based training. Memory-efficient Zeroth-order (MeZO) optimizers, recently proposed to address this issue, only require forward passes during training, making them more memory-friendly. However, the quality of gradient estimates in zeroth order optimization often depends on the data dimensionality, potentially explaining why MeZO still exhibits significant performance drops compared to standard fine-tuning across various tasks. Inspired by the success of Parameter-Efficient Fine-Tuning (PEFT), this paper introduces Sparse MeZO, a novel memory-efficient zeroth-order optimization approach that applies ZO only to a carefully chosen subset of parameters. We propose a simple yet effective parameter selection scheme that yields significant performance gains with Spar
    
[^136]: GAOKAO-MM: 一个用于多模态模型评估的中国人类水平基准

    GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation

    [https://arxiv.org/abs/2402.15745](https://arxiv.org/abs/2402.15745)

    GAOKAO-MM 是基于中国高考的多模态基准，为模型的能力设定人类水平要求，评估结果显示目前的LVLMs的准确率普遍不足50%。

    

    大型视觉语言模型（LVLMs）已经在图像感知和语言理解方面展示出了极大的能力。然而，现有的多模态基准主要关注基本的感知能力和常识知识，这些无法充分反映出LVLMs的全面能力。我们提出了GAOKAO-MM，一个基于中国高考的多模态基准，包括8个科目和12种类型的图片，如图表、函数图、地图和照片。GAOKAO-MM来源于中国本土背景，并为模型的能力设定了人类水平的要求，包括感知、理解、知识和推理。我们评估了10个LVLMs，发现它们的准确率都低于50%，其中GPT-4-Vision（48.1%）、Qwen-VL-Plus（41.2%）和Gemini-Pro-Vision（35.1%）位列前三名。我们的多维分析结果表明，LVLMs具有适度的

    arXiv:2402.15745v1 Announce Type: cross  Abstract: The Large Vision-Language Models (LVLMs) have demonstrated great abilities in image perception and language understanding. However, existing multimodal benchmarks focus on primary perception abilities and commonsense knowledge which are insufficient to reflect the comprehensive capabilities of LVLMs. We propose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. GAOKAO-MM derives from native Chinese context and sets human-level requirements for the model's abilities, including perception, understanding, knowledge and reasoning. We evaluate 10 LVLMs and find that the accuracies of all of them are lower than 50%, with GPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking in the top three positions. The results of our multi-dimension analysis indicate that LVLMs have moder
    
[^137]: ArEEG_Chars: 用于基于脑电图的设想语音识别的阿拉伯字符数据集

    ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters

    [https://arxiv.org/abs/2402.15733](https://arxiv.org/abs/2402.15733)

    该论文介绍了一种用于阿拉伯字符的EEG数据集ArEEG_Chars，通过深度学习实现97%的准确率，在脑机接口中具有重要意义。

    

    脑机接口（BCI）是近年来热门的研究课题，可以帮助瘫痪患者改善生活。有几项研究自动将脑电图（EEG）信号分类为英文字符和单词。阿拉伯语是世界上使用最广泛的语言之一。然而据我们所知，目前没有针对阿拉伯字符的脑电图信号数据集。在本文中，我们创建了一个用于阿拉伯字符的EEG数据集，并命名为ArEEG_Chars。此外，我们使用深度学习对ArEEG_Chars进行了多项实验。在使用LSTM时获得了最佳结果，准确率达到97%。ArEEG_Chars数据集将对研究人员公开。

    arXiv:2402.15733v1 Announce Type: cross  Abstract: Brain-Computer-Interface (BCI) has been a hot research topic in the last few years that could help paralyzed people in their lives. Several researches were done to classify electroencephalography (EEG) signals automatically into English characters and words. Arabic language is one of the most used languages around the world. However, to the best of our knowledge, there is no dataset for Arabic characters EEG signals. In this paper, we have created an EEG dataset for Arabic characters and named it ArEEG_Chars. Moreover, several experiments were done on ArEEG_Chars using deep learning. Best results were achieved using LSTM and reached an accuracy of 97%. ArEEG_Chars dataset will be public for researchers.
    
[^138]: 人类是如何编写代码的？大型模型也以同样的方式进行

    How Do Humans Write Code? Large Models Do It the Same Way Too

    [https://arxiv.org/abs/2402.15729](https://arxiv.org/abs/2402.15729)

    大型语言模型在执行数值计算时经常出错，通过生成可执行代码来解决问题可以减少计算错误，但观察到当大型语言模型使用代码解决数学问题时，会生成更多不正确推理；为解决这一问题，提出了一种受人类编码实践启发的简单而高效方法Human-Think Language（HTL）。

    

    大型语言模型（LLMs）在执行数值计算时经常出错。与传统的思维链推理相比，程序化思维方法涉及生成可执行代码来解决问题。通过执行这些代码，它可以获得更精确的结果。使用生成的可执行代码而不是自然语言可以减少计算错误。然而，我们观察到当LLMs使用代码解决数学问题时，他们往往生成比使用自然语言更多的不正确推理。为了解决这个问题，我们提出了Human-Think Language（HTL），这是一种受到人类编码实践启发的简单而高效的方法。该方法首先由模型生成用自然语言描述的解决问题方法，然后将其转换为代码，反映出人们在将逻辑以自然语言形式思考后再将其写成代码的过程。此外，它利用了P

    arXiv:2402.15729v1 Announce Type: new  Abstract: Large Language Models (LLMs) often make errors when performing numerical calculations. In contrast to traditional chain-of-thought reasoning, the program-of-thoughts approach involves generating executable code to solve problems. By executing this code, it achieves more precise results. Using generated executable code instead of natural language can reduce computational errors. However, we observe that when LLMs solve mathematical problems using code, they tend to generate more incorrect reasoning than when using natural language. To address this issue, we propose Human-Think Language (HTL), a straightforward yet highly efficient approach inspired by human coding practices. The approach first generates problem-solving methods described in the natural language by the model, then converts them into code, mirroring the process where people think through the logic in natural language before writing it as code. Additionally, it utilizes the P
    
[^139]: Hal-Eval: 一种面向大型视觉语言模型的通用和细粒度幻觉评估框架

    Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models

    [https://arxiv.org/abs/2402.15721](https://arxiv.org/abs/2402.15721)

    本论文提出了Hal-Eval，一个通用和细粒度的幻觉评估框架，引入了新的幻觉分类法，专注于事件幻觉，通过生成和过滤细粒度幻觉数据来评估大型视觉语言模型对各种幻觉的处理能力。

    

    大型视觉语言模型具有非凡的能力，但在图片和其描述之间存在幻觉不一致。以往对LVLMs进行的幻觉评估研究发现了关于对象、属性和关系的幻觉，但忽略了围绕虚构实体创建整个叙事的复杂幻觉。本文引入了一种精细的幻觉分类法，其中包括一个新的类别：事件幻觉。然后，我们利用先进的LLMs生成和过滤由各种类型的幻觉组成的细粒度幻觉数据，特别关注事件幻觉，为在我们的通用评估框架内集成辨别和生成评估方法奠定基础。所提出的基准可以独特地评估LVLMs处理广泛幻觉的能力，使其成为一个可靠和全面的工具。

    arXiv:2402.15721v1 Announce Type: new  Abstract: Large Vision Language Models exhibit remarkable capabilities but struggle with hallucinations inconsistencies between images and their descriptions. Previous hallucination evaluation studies on LVLMs have identified hallucinations in terms of objects, attributes, and relations but overlooked complex hallucinations that create an entire narrative around a fictional entity. In this paper, we introduce a refined taxonomy of hallucinations, featuring a new category: Event Hallucination. We then utilize advanced LLMs to generate and filter fine grained hallucinatory data consisting of various types of hallucinations, with a particular focus on event hallucinations, laying the groundwork for integrating discriminative and generative evaluation methods within our universal evaluation framework. The proposed benchmark distinctively assesses LVLMs ability to tackle a broad spectrum of hallucinations, making it a reliable and comprehensive tool fo
    
[^140]: 提高预训练语言模型的连续少样本关系提取器能力

    Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors

    [https://arxiv.org/abs/2402.15713](https://arxiv.org/abs/2402.15713)

    提出了一种对比提示学习框架，利用预训练语言模型的潜在能力解决灾难性遗忘和过拟合问题，使其成为更好的连续少样本关系提取器

    

    持续少样本关系提取（CFRE）是一个实际问题，需要模型在避免忘记旧关系的同时连续学习新关系，只有极少量标记训练数据。主要挑战是灾难性遗忘和过拟合。本文利用提示学习来探索预训练语言模型的隐式能力，以解决上述两个挑战，从而使语言模型成为更好的连续少样本关系提取器。具体来说，我们提出了一种对比提示学习框架，设计提示表示以获得更广义的知识，可以轻松适应旧的和新的类别，并基于边界的对比学习，更多地关注困难样本，从而缓解灾难性遗忘和过拟合问题。为了进一步解决低资源场景中的过拟合问题，我们引入了一种有效的记忆增强策略，利用了...

    arXiv:2402.15713v1 Announce Type: cross  Abstract: Continual Few-shot Relation Extraction (CFRE) is a practical problem that requires the model to continuously learn novel relations while avoiding forgetting old ones with few labeled training data. The primary challenges are catastrophic forgetting and overfitting. This paper harnesses prompt learning to explore the implicit capabilities of pre-trained language models to address the above two challenges, thereby making language models better continual few-shot relation extractors. Specifically, we propose a Contrastive Prompt Learning framework, which designs prompt representation to acquire more generalized knowledge that can be easily adapted to old and new categories, and margin-based contrastive learning to focus more on hard samples, therefore alleviating catastrophic forgetting and overfitting issues. To further remedy overfitting in low-resource scenarios, we introduce an effective memory augmentation strategy that employs well-
    
[^141]: 从脑信号解码查询语义的查询扩展

    Query Augmentation by Decoding Semantics from Brain Signals

    [https://arxiv.org/abs/2402.15708](https://arxiv.org/abs/2402.15708)

    提出了一种名为Brain-Aug的方法，通过从脑信号中解码的语义信息增强查询，可以生成更准确的查询，改善文档排序性能，特别适用于模糊查询。

    

    查询扩展是用于细化语义不准确查询的关键技术。传统上，查询扩展依赖于从最初检索到的、潜在相关的文档中提取信息。如果最初检索到的文档质量较低，则查询扩展的有效性也会受到限制。我们提出了Brain-Aug，通过将从脑信号解码的语义信息结合到查询中来增强查询。Brain-Aug使用了在脑信号信息构建的提示和面向排名的推理方法生成原始查询的延续部分。对fMRI数据集的实验结果显示，Brain-Aug生成的查询在语义上更准确，导致改进的文档排序性能。脑信号带来的这种改进对于模糊查询特别显著。

    arXiv:2402.15708v1 Announce Type: cross  Abstract: Query augmentation is a crucial technique for refining semantically imprecise queries. Traditionally, query augmentation relies on extracting information from initially retrieved, potentially relevant documents. If the quality of the initially retrieved documents is low, then the effectiveness of query augmentation would be limited as well. We propose Brain-Aug, which enhances a query by incorporating semantic information decoded from brain signals. BrainAug generates the continuation of the original query with a prompt constructed with brain signal information and a ranking-oriented inference approach. Experimental results on fMRI (functional magnetic resonance imaging) datasets show that Brain-Aug produces semantically more accurate queries, leading to improved document ranking performance. Such improvement brought by brain signals is particularly notable for ambiguous queries.
    
[^142]: 《CoRelation: 通过上下文化的编码关系学习提升自动ICD编码》

    CoRelation: Boosting Automatic ICD Coding Through Contextualized Code Relation Learning

    [https://arxiv.org/abs/2402.15700](https://arxiv.org/abs/2402.15700)

    通过上下文化的编码关系学习，提出了一种新的框架来增强ICD编码表示的学习，实验结果表明其相比最先进基线方法的有效性。

    

    自动国际疾病分类（ICD）编码在从临床记录中提取相关信息以便正确记录和计费方面起着至关重要的作用。提升自动ICD编码性能的一个重要方向是对ICD编码关系进行建模。然而，当前方法对ICD编码之间错综复杂的关系建模不足，通常忽视了临床记录中上下文的重要性。本文提出了一种新颖方法，即一种上下文化和灵活的框架，以增强ICD编码表示的学习。与现有方法不同，我们的方法采用了考虑临床记录上下文的依赖学习范式，对建模所有可能的编码关系。我们在六个公共ICD编码数据集上评估了我们的方法，实验结果表明与最先进基线方法相比，我们的方法的有效性。

    arXiv:2402.15700v1 Announce Type: cross  Abstract: Automatic International Classification of Diseases (ICD) coding plays a crucial role in the extraction of relevant information from clinical notes for proper recording and billing. One of the most important directions for boosting the performance of automatic ICD coding is modeling ICD code relations. However, current methods insufficiently model the intricate relationships among ICD codes and often overlook the importance of context in clinical notes. In this paper, we propose a novel approach, a contextualized and flexible framework, to enhance the learning of ICD code representations. Our approach, unlike existing methods, employs a dependent learning paradigm that considers the context of clinical notes in modeling all possible code relations. We evaluate our approach on six public ICD coding datasets and the experimental results demonstrate the effectiveness of our approach compared to state-of-the-art baselines.
    
[^143]: 踏进大型语言模型“越狱”的认知心理学

    Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology

    [https://arxiv.org/abs/2402.15690](https://arxiv.org/abs/2402.15690)

    该研究通过认知一致性理论为大型语言模型的越狱提示提供了心理解释，并提出了一种基于门脚-门技术的自动黑盒越狱方法。

    

    大型语言模型（LLMs）渐渐成为人们获取新知识的入口。然而，攻击者可以打破模型的安全保护（“监狱”）以访问受限信息，这称为“越狱”。先前的研究显示了当前LLMs在面对此类越狱攻击时的薄弱性。然而，对LLMs在接收越狱提示时内在决策机制的理解明显欠缺。我们的研究提供了越狱提示的心理解释。借鉴认知一致性理论，我们认为越狱的关键是引导LLMs在错误方向上实现认知协调。此外，我们提出了一种基于门脚-门的自动黑盒越狱方法。这种方法逐步诱导模型通过多步增量提示回答有害问题。

    arXiv:2402.15690v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have gradually become the gateway for people to acquire new knowledge. However, attackers can break the model's security protection ("jail") to access restricted information, which is called "jailbreaking." Previous studies have shown the weakness of current LLMs when confronted with such jailbreaking attacks. Nevertheless, comprehension of the intrinsic decision-making mechanism within the LLMs upon receipt of jailbreak prompts is noticeably lacking. Our research provides a psychological explanation of the jailbreak prompts. Drawing on cognitive consistency theory, we argue that the key to jailbreak is guiding the LLM to achieve cognitive coordination in an erroneous direction. Further, we propose an automatic black-box jailbreaking method based on the Foot-in-the-Door (FITD) technique. This method progressively induces the model to answer harmful questions via multi-step incremental prompts. We instantiat
    
[^144]: 在药物监测事件提取中利用ChatGPT：一项实证研究

    Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical Study

    [https://arxiv.org/abs/2402.15663](https://arxiv.org/abs/2402.15663)

    该研究调查了在药物监测事件提取中利用ChatGPT的能力，并发现尽管ChatGPT在适当的演示选择策略下表现良好，但仍不及完全微调的小型模型。

    

    随着大型语言模型（LLMs）的出现，人们越来越感兴趣探索其在医疗应用中的潜力。本研究旨在调查LLMs，特别是ChatGPT，在药物监测事件提取中的能力，其主要目标是从文本医疗来源中识别和提取不良事件或潜在治疗事件。我们进行了广泛实验，评估ChatGPT在药物监测事件提取任务中的性能，采用各种提示和演示选择策略。研究结果表明，尽管ChatGPT在适当的演示选择策略下表现出合理的性能，但与完全微调的小型模型相比仍存在不足。此外，我们探讨了利用ChatGPT进行数据增强的潜力。然而，我们的调查发现，将合成数据纳入微调中

    arXiv:2402.15663v1 Announce Type: new  Abstract: With the advent of large language models (LLMs), there has been growing interest in exploring their potential for medical applications. This research aims to investigate the ability of LLMs, specifically ChatGPT, in the context of pharmacovigilance event extraction, of which the main goal is to identify and extract adverse events or potential therapeutic events from textual medical sources. We conduct extensive experiments to assess the performance of ChatGPT in the pharmacovigilance event extraction task, employing various prompts and demonstration selection strategies. The findings demonstrate that while ChatGPT demonstrates reasonable performance with appropriate demonstration selection strategies, it still falls short compared to fully fine-tuned small models. Additionally, we explore the potential of leveraging ChatGPT for data augmentation. However, our investigation reveals that the inclusion of synthesized data into fine-tuning m
    
[^145]: 探索多模态推理物理动力学中的失败案例

    Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics

    [https://arxiv.org/abs/2402.15654](https://arxiv.org/abs/2402.15654)

    本文探讨了在现实环境中进行物理推理问题解决的能力，并尝试解决多模态模型在对象操作和放置任务中无法正确组合知识的问题。

    

    在这篇论文中，我们探讨了LLMs在现实环境中进行物理推理问题解决的能力。我们构建了一个简单的模拟环境，并展示了在零-shot设置中，文本和多模态LLMs展示了关于各种物体的原子世界知识，但在物体操作和放置任务的正确解决方案中未能将这些知识组合在一起。我们还使用了BLIP，这是一个训练有交叉模态注意力的视觉语言模型，识别了与对象物理属性相关的导致该模型无法基于的案例。最后，我们提出了一种发现环境中对象相关属性的程序，并提出了一种方法将这些知识提炼回LLM中。

    arXiv:2402.15654v1 Announce Type: new  Abstract: In this paper, we present an exploration of LLMs' abilities to problem solve with physical reasoning in situated environments. We construct a simple simulated environment and demonstrate examples of where, in a zero-shot setting, both text and multimodal LLMs display atomic world knowledge about various objects but fail to compose this knowledge in correct solutions for an object manipulation and placement task. We also use BLIP, a vision-language model trained with more sophisticated cross-modal attention, to identify cases relevant to object physical properties that that model fails to ground. Finally, we present a procedure for discovering the relevant properties of objects in the environment and propose a method to distill this knowledge back into the LLM.
    
[^146]: 处理因果语言模型中上下文示例对顺序敏感性的问题

    Addressing Order Sensitivity of In-Context Demonstration Examples in Causal Language Models

    [https://arxiv.org/abs/2402.15637](https://arxiv.org/abs/2402.15637)

    因果语言模型更容易受到上下文示例顺序的影响，为了解决这一挑战，提出了一种信息增强和一致性增强方法。

    

    在自然语言处理中，上下文学习已经成为一种流行的范式。然而，其性能可能会受到上下文示例顺序的显著影响。在本文中，我们发现因果语言模型（CausalLMs）对此顺序比前缀语言模型（PrefixLMs）更敏感。我们将这一现象归因于CausalLMs中的自回归注意力掩模，这些掩模限制每个标记不能访问随后的标记的信息。这导致不同位置的样本具有不同的感受野，从而导致不同位置的表征差异。为了应对这一挑战，我们引入了一种无监督微调方法，称为信息增强和一致性增强方法。该方法利用对比学习来对齐不同位置上下文示例的表征，并引入一致性损失以确保相似性。

    arXiv:2402.15637v1 Announce Type: new  Abstract: In-context learning has become a popular paradigm in natural language processing. However, its performance can be significantly influenced by the order of in-context demonstration examples. In this paper, we found that causal language models (CausalLMs) are more sensitive to this order compared to prefix language models (PrefixLMs). We attribute this phenomenon to the auto-regressive attention masks within CausalLMs, which restrict each token from accessing information from subsequent tokens. This results in different receptive fields for samples at different positions, thereby leading to representation disparities across positions. To tackle this challenge, we introduce an unsupervised fine-tuning method, termed the Information-Augmented and Consistency-Enhanced approach. This approach utilizes contrastive learning to align representations of in-context examples across different positions and introduces a consistency loss to ensure simi
    
[^147]: 细粒度的自认证可以提升事实性和推理能力

    Fine-Grained Self-Endorsement Improves Factuality and Reasoning

    [https://arxiv.org/abs/2402.15631](https://arxiv.org/abs/2402.15631)

    提出了利用自认证框架进行细粒度事实级别比较的方法，能够更好地减轻大型语言模型生成过程中的幻觉，尤其适用于长篇生成任务。

    

    这项工作研究了如何在推理时通过缓解存在事实冲突的幻觉来改善大型语言模型（LLM）生成。特别是，我们提出了一个自认证框架，利用跨多个抽样响应进行细粒度的事实级别比较。与先前的组合方法（王等，2022年；陈等，2023年）进行响应级别选择相比，我们的方法能够更好地减轻幻觉，特别是对于长篇生成任务。我们的方法可以广泛有益于较小和开源的LLM，因为它主要进行简单的基于内容的比较。在传记上的实验证明，我们的方法可以通过简单直观的提示有效改善不同规模的LLM生成的事实性。此外，对TriviaQA和GSM8K的全面分析展示了自认证在更广泛应用中的潜力。

    arXiv:2402.15631v1 Announce Type: cross  Abstract: This work studies improving large language model (LLM) generations at inference time by mitigating fact-conflicting hallucinations. Particularly, we propose a self-endorsement framework that leverages the fine-grained fact-level comparisons across multiple sampled responses. Compared with prior ensemble methods (Wang et al., 2022;Chen et al., 2023)) that perform response-level selection, our approach can better alleviate hallucinations, especially for longform generation tasks. Our approach can broadly benefit smaller and open-source LLMs as it mainly conducts simple content-based comparisons. Experiments on Biographies show that our method can effectively improve the factuality of generations with simple and intuitive prompts across different scales of LLMs. Besides, comprehensive analyses on TriviaQA and GSM8K demonstrate the potential of self-endorsement for broader application.
    
[^148]: 基于语言的用户偏好推荐方法

    Language-Based User Profiles for Recommendation

    [https://arxiv.org/abs/2402.15623](https://arxiv.org/abs/2402.15623)

    通过使用以人类可读文本表示的用户偏好，提出了Language-based Factorization Model (LFM)，在冷启动环境中与传统方法相比取得了更好的表现

    

    大多数传统的推荐方法（如矩阵分解）将用户偏好表示为高维向量。不幸的是，这些向量缺乏可解释性和可控性，在冷启动环境下往往表现不佳。为了解决这些缺点，我们探索了使用以人类可读文本表示的用户偏好。我们提出了基于语言的因子分解模型（LFM），它本质上是一个编码器/解码器模型，其中编码器和解码器均为大型语言模型（LLM）。编码器LLM从用户的评分历史生成用户兴趣的简洁自然语言描述。解码器LLM使用这个简要描述来完成预测性的下游任务。我们在MovieLens数据集上评估了LFM方法，将其与矩阵分解和直接从用户评分历史预测的LLM模型进行了比较。在冷启动环境下，我们发现我们的方法能够...

    arXiv:2402.15623v1 Announce Type: new  Abstract: Most conventional recommendation methods (e.g., matrix factorization) represent user profiles as high-dimensional vectors. Unfortunately, these vectors lack interpretability and steerability, and often perform poorly in cold-start settings. To address these shortcomings, we explore the use of user profiles that are represented as human-readable text. We propose the Language-based Factorization Model (LFM), which is essentially an encoder/decoder model where both the encoder and the decoder are large language models (LLMs). The encoder LLM generates a compact natural-language profile of the user's interests from the user's rating history. The decoder LLM uses this summary profile to complete predictive downstream tasks. We evaluate our LFM approach on the MovieLens dataset, comparing it against matrix factorization and an LLM model that directly predicts from the user's rating history. In cold-start settings, we find that our method can h
    
[^149]: 通过预训练表示实现在NLP中的高效主动学习

    Towards Efficient Active Learning in NLP via Pretrained Representations

    [https://arxiv.org/abs/2402.15613](https://arxiv.org/abs/2402.15613)

    通过在主动学习循环中使用预训练LLMs的表示，可以显著加快标记数据获取的过程，并通过微调获得最佳性能，同时大大降低计算开销。

    

    大型语言模型（LLMs）的微调现在是文本分类中常用的方法，在许多应用中都能看到。当标记文档稀缺时，主动学习有助于节省注释工作，但需要在每次获取迭代时重新训练大规模模型。我们通过在主动学习循环中使用LLMs的预训练表示，显著加快了这一过程，一旦获得所需数量的标记数据，就可以对LLMs进行微调，以获得最佳性能。通过在常见的文本分类基准上验证，以预训练的BERT和RoBERTa作为基础，我们的策略产生了与在整个主动学习循环中微调相似的性能，但计算开销降低了数个数量级。使用我们的程序获取的数据可以跨预训练网络进行泛化，从而可以灵活选择最终模型或更新。

    arXiv:2402.15613v1 Announce Type: cross  Abstract: Fine-tuning Large Language Models (LLMs) is now a common approach for text classification in a wide range of applications. When labeled documents are scarce, active learning helps save annotation efforts but requires retraining of massive models on each acquisition iteration. We drastically expedite this process by using pretrained representations of LLMs within the active learning loop and, once the desired amount of labeled data is acquired, fine-tuning that or even a different pretrained LLM on this labeled data to achieve the best performance. As verified on common text classification benchmarks with pretrained BERT and RoBERTa as the backbone, our strategy yields similar performance to fine-tuning all the way through the active learning loop but is orders of magnitude less computationally expensive. The data acquired with our procedure generalizes across pretrained networks, allowing flexibility in choosing the final model or upda
    
[^150]: 选择“选择性预测”：减少视觉语言推理中不必要的弃权

    Selective "Selective Prediction": Reducing Unnecessary Abstention in Vision-Language Reasoning

    [https://arxiv.org/abs/2402.15610](https://arxiv.org/abs/2402.15610)

    引入了一种名为ReCoVERR的算法，能够在视觉-语言系统的推理过程中减少过度放弃，通过寻找图像中的相关线索提供额外证据来取代放弃，从而不降低预测准确性。

    

    先前关于选择性预测的工作旨在通过允许视觉-语言模型（VLM）在不确定时放弃回答，以最小化错误预测。然而，当部署一个对不准确预测容忍度低的视觉-语言系统时，选择性预测可能过于谨慎，并且在许多正确预测上过于放弃。我们引入ReCoVERR，一种用于减少选择性视觉-语言系统过度放弃的推理时间算法，而不降低预测准确性。当VLM做出低置信度预测时，ReCoVERR尝试在图像中找到提供额外证据的相关线索，而不是放弃。ReCoVERR使用LLM向VLM提出相关问题，收集高置信度证据，如果足够的证据确认预测，则系统做出预测而不是放弃。ReCoVERR使两个VLM，BLIP2和InstructBLIP，能够回答。

    arXiv:2402.15610v1 Announce Type: new  Abstract: Prior work on selective prediction minimizes incorrect predictions from vision-language models (VLMs) by allowing them to abstain from answering when uncertain. However, when deploying a vision-language system with low tolerance for inaccurate predictions, selective prediction may be over-cautious and abstain too frequently, even on many correct predictions. We introduce ReCoVERR, an inference-time algorithm to reduce the over-abstention of a selective vision-language system without decreasing prediction accuracy. When the VLM makes a low-confidence prediction, instead of abstaining ReCoVERR tries to find relevant clues in the image that provide additional evidence for the prediction. ReCoVERR uses an LLM to pose related questions to the VLM, collects high-confidence evidences, and if enough evidence confirms the prediction the system makes a prediction instead of abstaining. ReCoVERR enables two VLMs, BLIP2 and InstructBLIP, to answer u
    
[^151]: 交替弱三音素/BPE对齐监督来自混合模型改善端到端 ASR

    Alternating Weak Triphone/BPE Alignment Supervision from Hybrid Model Improves End-to-End ASR

    [https://arxiv.org/abs/2402.15594](https://arxiv.org/abs/2402.15594)

    提出了交替弱三音素/BPE对齐监督来自混合模型改善端到端 ASR，通过弱监督和辅助任务的交替训练，显著提高了ASR性能。

    

    本文提出了交替弱三音素/BPE对齐监督方法，以改善端到端模型训练。为此，使用现有的混合ASR系统提取三音素和BPE对齐。然后，在编码器的中间层表示上分别针对三音素对齐和BPE对齐计算基于交叉熵的中间辅助损失，获得正则化效应。通过采用参数为0.5的强标签平滑实现弱监督。在TED-LIUM 2数据集上的实验结果表明，基于三音素或BPE对齐的弱监督均改善了ASR性能，超越了标准CTC辅助损失。此外，它们的组合进一步降低了词错误率。我们还研究了模型训练过程中两个辅助任务的交替操作，并观察到额外的性能增益。总体而言，提出的技术导致了超过10%相对词错误率的性能改进。

    arXiv:2402.15594v1 Announce Type: new  Abstract: In this paper, alternating weak triphone/BPE alignment supervision is proposed to improve end-to-end model training. Towards this end, triphone and BPE alignments are extracted using a pre-existing hybrid ASR system. Then, regularization effect is obtained by cross-entropy based intermediate auxiliary losses computed on such alignments at a mid-layer representation of the encoder for triphone alignments and at the encoder for BPE alignments. Weak supervision is achieved through strong label smoothing with parameter of 0.5. Experimental results on TED-LIUM 2 indicate that either triphone or BPE alignment based weak supervision improves ASR performance over standard CTC auxiliary loss. Moreover, their combination lowers the word error rate further. We also investigate the alternation of the two auxiliary tasks during model training, and additional performance gain is observed. Overall, the proposed techniques result in over 10% relative er
    
[^152]: 从学术手稿的同行评审叙事中要求LLMs撰写元评论草案

    Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts

    [https://arxiv.org/abs/2402.15589](https://arxiv.org/abs/2402.15589)

    本文研究了使用不同类型/级别的提示来激发三种流行LLM，GPT-3.5、LLaMA2和PaLM2，在学术同行评审过程中自动生成元评论，并进行了详细的定性研究。

    

    学术同行评审过程中最重要但也最繁重的任务之一是撰写元评论，这涉及根据多位专家的同行评审叙事理解学术手稿的核心贡献、优点和缺点，然后将这些专家多视角的看法总结为简洁的整体概述。鉴于生成型AI，尤其是大型语言模型（LLMs）的最新重大发展，我们有充分的理由深入研究LLMs在学术同行评审环境中生成这种元评论的实用性。本文通过使用三种流行的LLM，即GPT-3.5、LLaMA2和PaLM2，执行案例研究，通过基于最近提出的TELeR分类法以不同类型/级别的提示促使它们自动生成元评论。最后，我们对LLM生成的元评论进行了详细的定性研究，并总结了我们的发现。

    arXiv:2402.15589v1 Announce Type: cross  Abstract: One of the most important yet onerous tasks in the academic peer-reviewing process is composing meta-reviews, which involves understanding the core contributions, strengths, and weaknesses of a scholarly manuscript based on peer-review narratives from multiple experts and then summarizing those multiple experts' perspectives into a concise holistic overview. Given the latest major developments in generative AI, especially Large Language Models (LLMs), it is very compelling to rigorously study the utility of LLMs in generating such meta-reviews in an academic peer-review setting. In this paper, we perform a case study with three popular LLMs, i.e., GPT-3.5, LLaMA2, and PaLM2, to automatically generate meta-reviews by prompting them with different types/levels of prompts based on the recently proposed TELeR taxonomy. Finally, we perform a detailed qualitative study of the meta-reviews generated by the LLMs and summarize our findings and 
    
[^153]: 不需要任务名称的CI: 无需任务名称的上下文注入用于程序规划

    CI w/o TN: Context Injection without Task Name for Procedure Planning

    [https://arxiv.org/abs/2402.15579](https://arxiv.org/abs/2402.15579)

    在程序规划中，本研究提出了一种新的弱监督设置，通过使用视觉起始点和目标观察的标题作为上下文信息，从而实现无需任务名称的上下文注入，从而降低了标注成本

    

    该论文探讨了指导视频中程序规划的挑战，其中涉及根据视频中的视觉起始点和目标观察创建目标导向计划。先前的研究通过逐渐减弱训练监督来解决这一问题，从重型中间视觉观察或语言说明到任务类别监督。然而，随着大型语言模型的出现，即使只给出任务名称，这些模型也能生成详细的计划。在这项研究中，我们提出了一种没有任务名称作为监督的更弱设置，这是当前大型语言模型无法解决的，因为它们需要具有足够信息的良好提示。具体来说，我们假设先前的中间监督可以作为上下文信息，并且我们使用视觉起始点和目标观察的标题作为一个更廉价的监督形式。这种方法大大降低了标注成本

    arXiv:2402.15579v1 Announce Type: cross  Abstract: This paper explores the challenge of procedure planning in instructional videos, which involves creating goal-directed plans based on visual start and goal observations from videos. Previous research has tackled this problem with gradually weaker training supervision, from heavy intermediate visual observations or language instructions to task class supervision. However, with the advent of large language models, even given only the task name, these models can produce a detailed plan. In this study, we propose a much weaker setting without task name as supervision, which is not currently solvable by existing large language models since they require good prompts with sufficient information. Specifically, we hypothesize that previous intermediate supervisions can serve as context information, and we use captions of visual start and goal observations as a much cheaper form of supervision. This approach greatly reduces the labeling cost sin
    
[^154]: Social Convos: 捕捉社交媒体上的议程和情绪

    Social Convos: Capturing Agendas and Emotions on Social Media

    [https://arxiv.org/abs/2402.15571](https://arxiv.org/abs/2402.15571)

    本文提出了一种新颖的方法，用于从讨论特定话题的用户群之间传播的信息中提取影响指标，重点关注影响传播和检测更有意图的影响操作。

    

    社交媒体平台是在类似选举或流行病等重大公共事件期间传播有针对性信息的热门工具。对信息流量的系统分析可以为不同人群之间的主流观点和社会动态提供宝贵见解。我们特别关注影响传播，特别是是否能够检测到更有意图的影响操作。然而，过滤出具有显著影响指标的基本信息，以及通常混乱的社交媒体流量是一个重大挑战。在本文中，我们提出了一种新方法，从围绕特定话题讨论的用户群之间传播的信息中提取影响指标。我们建立在"对话"的概念上，以识别积极推动该话题周围某种特定议程的有影响力的作者。我们专注于两个影响指标：（控制）

    arXiv:2402.15571v1 Announce Type: cross  Abstract: Social media platforms are popular tools for disseminating targeted information during major public events like elections or pandemics. Systematic analysis of the message traffic can provide valuable insights into prevailing opinions and social dynamics among different segments of the population. We are specifically interested in influence spread, and in particular whether more deliberate influence operations can be detected. However, filtering out the essential messages with telltale influence indicators from the extensive and often chaotic social media traffic is a major challenge. In this paper we present a novel approach to extract influence indicators from messages circulating among groups of users discussing particular topics. We build upon the concept of a convo to identify influential authors who are actively promoting some particular agenda around that topic within the group. We focus on two influence indicators: the (control 
    
[^155]: 一分钟内在语言模型上的快速对抗攻击

    Fast Adversarial Attacks on Language Models In One GPU Minute

    [https://arxiv.org/abs/2402.15570](https://arxiv.org/abs/2402.15570)

    介绍了一种新型的基于束搜索的快速对抗攻击方法BEAST，能够在一分钟内高成功率地越狱对齐的语言模型，同时还能导致语言模型产生幻觉。

    

    在这篇论文中，我们介绍了一种新型的快速基于束搜索的语言模型对抗攻击（BEAST）。BEAST采用可解释的参数，使攻击者能够在攻击速度、成功率和对抗性提示的可读性之间取得平衡。BEAST的计算效率使我们能够研究其在语言模型中用于越狱、引发幻觉和隐私攻击的应用。我们的基于梯度的有针对性攻击可以在一分钟内越狱对齐的语言模型，攻击成功率很高。例如，与基于梯度的基准相比，BEAST可以在一分钟内越狱 Vicuna-7B-v1.5，成功率达到89%，而基准方法需要一个小时以上才能使用单个 Nvidia RTX A6000 48GB GPU 实现70%的成功率。此外，我们发现一个独特的结果，即我们的非有针对性攻击会导致语言模型聊天机器人产生幻觉。通过人类评估，我们发现我们的非有针对性攻击导致了

    arXiv:2402.15570v1 Announce Type: cross  Abstract: In this paper, we introduce a novel class of fast, beam search-based adversarial attack (BEAST) for Language Models (LMs). BEAST employs interpretable parameters, enabling attackers to balance between attack speed, success rate, and the readability of adversarial prompts. The computational efficiency of BEAST facilitates us to investigate its applications on LMs for jailbreaking, eliciting hallucinations, and privacy attacks. Our gradient-free targeted attack can jailbreak aligned LMs with high attack success rates within one minute. For instance, BEAST can jailbreak Vicuna-7B-v1.5 under one minute with a success rate of 89% when compared to a gradient-based baseline that takes over an hour to achieve 70% success rate using a single Nvidia RTX A6000 48GB GPU. Additionally, we discover a unique outcome wherein our untargeted attack induces hallucinations in LM chatbots. Through human evaluations, we find that our untargeted attack cause
    
[^156]: 面向自闭症谱系障碍韩国儿童的语音语料库：走向自动评估系统

    Speech Corpus for Korean Children with Autism Spectrum Disorder: Towards Automatic Assessment Systems

    [https://arxiv.org/abs/2402.15539](https://arxiv.org/abs/2402.15539)

    该论文介绍了专为韩国ASD儿童设计的语音语料库，旨在提升语音技术以促进发音和严重程度评估。

    

    尽管对自闭症谱系障碍（ASD）儿童数字化治疗的需求不断增长，但目前尚无针对韩国ASD儿童的语音语料库。本文介绍了一个专为韩国ASD儿童设计的语音语料库，旨在推进语音技术，如发音和严重程度评估。语音与语言评估会话中的语音录音被转录，并针对发音和语言特征进行了注释。三名语音与语言病理师使用3点力克特量表对这些录音进行了社交沟通严重程度（SCS）和发音熟练度（PP）评分。参与者总数将有300名ASD儿童和50名正常发育（TD）儿童。本文还分析了从收集的语音数据中提取并完成注释的73名ASD儿童和9名TD儿童的声学和语言特征。

    arXiv:2402.15539v1 Announce Type: cross  Abstract: Despite the growing demand for digital therapeutics for children with Autism Spectrum Disorder (ASD), there is currently no speech corpus available for Korean children with ASD. This paper introduces a speech corpus specifically designed for Korean children with ASD, aiming to advance speech technologies such as pronunciation and severity evaluation. Speech recordings from speech and language evaluation sessions were transcribed, and annotated for articulatory and linguistic characteristics. Three speech and language pathologists rated these recordings for social communication severity (SCS) and pronunciation proficiency (PP) using a 3-point Likert scale. The total number of participants will be 300 for children with ASD and 50 for typically developing (TD) children. The paper also analyzes acoustic and linguistic features extracted from speech data collected and completed for annotation from 73 children with ASD and 9 TD children to i
    
[^157]: 评估ChatGPT用于垃圾邮件检测的性能

    Evaluating the Performance of ChatGPT for Spam Email Detection

    [https://arxiv.org/abs/2402.15537](https://arxiv.org/abs/2402.15537)

    该研究评估了ChatGPT在英文和中文电子邮件数据集中用于垃圾邮件检测的性能，并探讨了其在这一领域的潜力。

    

    电子邮件继续是专业和商业领域中至关重要且广泛使用的通信媒介。然而，垃圾邮件的普及给用户带来了重大挑战，扰乱了他们的日常工作并降低了生产率。因此，基于内容准确地识别和过滤垃圾邮件对网络安全至关重要。最近自然语言处理领域的发展，特别是大型语言模型如ChatGPT，在诸如问答和文本生成等任务中表现出色。然而，其在垃圾邮件识别方面的潜力尚未得到充分探索。为了填补这一空白，本研究尝试评估ChatGPT在英文和中文电子邮件数据集中用于垃圾邮件识别的能力。我们利用ChatGPT进行垃圾邮件检测，采用上下文学习，需要提示说明和少量示范。

    arXiv:2402.15537v1 Announce Type: cross  Abstract: Email continues to be a pivotal and extensively utilized communication medium within professional and commercial domains. Nonetheless, the prevalence of spam emails poses a significant challenge for users, disrupting their daily routines and diminishing productivity. Consequently, accurately identifying and filtering spam based on content has become crucial for cybersecurity. Recent advancements in natural language processing, particularly with large language models like ChatGPT, have shown remarkable performance in tasks such as question answering and text generation. However, its potential in spam identification remains underexplored. To fill in the gap, this study attempts to evaluate ChatGPT's capabilities for spam identification in both English and Chinese email datasets. We employ ChatGPT for spam email detection using in-context learning, which requires a prompt instruction and a few demonstrations. We also investigate how the t
    
[^158]: PCA-Bench: 评估多模大型语言模型在感知-认知-行动链中的表现

    PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain

    [https://arxiv.org/abs/2402.15527](https://arxiv.org/abs/2402.15527)

    PCA-Bench 提出了一个评估多模大型语言模型综合能力的基准，引入复杂场景和错误定位能力，提高部署可靠性，并提出了自动评估协议 PCA-Eval，发现了显著的性能差异。

    

    我们提出了PCA-Bench，这是一个用于评估多模大型语言模型（MLLMs）综合能力的多模决策基准。与之前专注于简单任务和单个模型能力的基准不同，PCA-Bench引入了三个复杂场景：自动驾驶、家庭机器人和开放世界游戏。在给定任务指令和多样化上下文的情况下，模型需要无缝整合感知、认知和行动的多重能力，以进行推理链以做出准确决定。此外，PCA-Bench具有错误定位能力，审查模型在感知、知识或推理等领域的不准确性。这提高了部署MLLMs的可靠性。为了在评估中平衡准确性和效率，我们提出了PCA-Eval，一种自动评估协议，并评估了10种流行的MLLMs。结果显示了显著的性能差异。

    arXiv:2402.15527v1 Announce Type: cross  Abstract: We present PCA-Bench, a multimodal decision-making benchmark for evaluating the integrated capabilities of Multimodal Large Language Models (MLLMs). Departing from previous benchmarks focusing on simplistic tasks and individual model capability, PCA-Bench introduces three complex scenarios: autonomous driving, domestic robotics, and open-world games. Given task instructions and diverse contexts, the model is required to seamlessly integrate multiple capabilities of Perception, Cognition, and Action in a reasoning chain to make accurate decisions. Moreover, PCA-Bench features error localization capabilities, scrutinizing model inaccuracies in areas such as perception, knowledge, or reasoning. This enhances the reliability of deploying MLLMs. To balance accuracy and efficiency in evaluation, we propose PCA-Eval, an automatic evaluation protocol, and assess 10 prevalent MLLMs. The results reveal significant performance disparities between
    
[^159]: 通过框架理论检测不实信息：基于框架元素的模型

    Detecting misinformation through Framing Theory: the Frame Element-based Model

    [https://arxiv.org/abs/2402.15525](https://arxiv.org/abs/2402.15525)

    通过框架理论检测不实信息，提出了基于框架元素的模型，并利用大型语言模型和深度神经网络来检测不同框架下准确事实产生的误导信息。

    

    在本文中，我们深入探讨了误导信息检测这一快速发展的挑战，特别关注叙事框架的微妙操纵——这是人工智能领域中尚未充分探讨的一个领域。生成式人工智能模型生成误导性叙事的潜力凸显了这一问题的紧迫性。基于传播和框架理论，我们认为准确信息的展示或“框架化”可以显著改变其解释，可能导致误导信息。我们通过真实案例突出了这一问题，展示了叙事框架的转变如何将基于事实的信息转化为误导信息。为解决这一挑战，我们提出了一种创新方法，利用预训练的大型语言模型和深度神经网络的能力来检测源自不同框架下表现的准确事实的误导信息。

    arXiv:2402.15525v1 Announce Type: new  Abstract: In this paper, we delve into the rapidly evolving challenge of misinformation detection, with a specific focus on the nuanced manipulation of narrative frames - an under-explored area within the AI community. The potential for Generative AI models to generate misleading narratives underscores the urgency of this problem. Drawing from communication and framing theories, we posit that the presentation or 'framing' of accurate information can dramatically alter its interpretation, potentially leading to misinformation. We highlight this issue through real-world examples, demonstrating how shifts in narrative frames can transmute fact-based information into misinformation. To tackle this challenge, we propose an innovative approach leveraging the power of pre-trained Large Language Models and deep neural networks to detect misinformation originating from accurate facts portrayed under different frames. These advanced AI techniques offer unpr
    
[^160]: 当心言辞：评估会话式大型语言模型的词汇丰富度

    Beware of Words: Evaluating the Lexical Richness of Conversational Large Language Models

    [https://arxiv.org/abs/2402.15518](https://arxiv.org/abs/2402.15518)

    评估会话式大型语言模型生成文本的语言特征以及这些特征如何取决于模型参数是理解其潜在影响的关键一步。

    

    在很多不同任务中正在评估会话式大型语言模型（LLMs）的性能，特别是ChatGPT，从逻辑推理或数学到回答各种主题的问题。然而，对这些LLMs生成的文本的语言特征的研究却少之又少。这是令人惊讶的，因为LLMs是语言模型，了解它们如何使用语言是重要的。事实上，会话式LLMs可能对语言的演变产生重要影响，因为它们最终可能主导新文本的创作。因此，评估它们生成的文本的语言特征以及这些特征如何取决于模型参数是了解潜在影响的第一步。

    arXiv:2402.15518v1 Announce Type: new  Abstract: The performance of conversational Large Language Models (LLMs) in general, and of ChatGPT in particular, is currently being evaluated on many different tasks, from logical reasoning or maths to answering questions on a myriad of topics. Instead, much less attention is being devoted to the study of the linguistic features of the texts generated by these LLMs. This is surprising since LLMs are models for language, and understanding how they use the language is important. Indeed, conversational LLMs are poised to have a significant impact on the evolution of languages as they may eventually dominate the creation of new text. This means that for example, if conversational LLMs do not use a word it may become less and less frequent and eventually stop being used altogether. Therefore, evaluating the linguistic features of the text they produce and how those depend on the model parameters is the first step toward understanding the potential im
    
[^161]: 大规模生成式人工智能文本在体育和音乐领域的应用

    Large Scale Generative AI Text Applied to Sports and Music

    [https://arxiv.org/abs/2402.15514](https://arxiv.org/abs/2402.15514)

    这项工作利用生成式人工智能模型将大规模多模数据转化为连贯流畅文本，首次推出了用于体育和音乐领域的AI评论系统，并取得了显著性能提升。

    

    我们解决了将媒体内容（包括评论和个性化新闻报道）扩展到全球大型体育和音乐活动的生产问题。我们的方法依赖生成式人工智能模型，将大量多模数据（例如视频、文章、实时比分、统计数据和资料）转换为连贯流畅的文本。基于这一方法，我们首次推出了一款人工智能评论系统，该系统被部署用于为2023年美国公开赛、温布尔登公开赛和大师赛的精彩片段制作自动化叙述。我们的解决方案还被扩展用于为ESPN梦幻橄榄球和格莱美奖音乐艺术家故事创造个性化内容。这些应用程序采用了相同的软件架构，实现了15倍的速度提升，平均Rouge-L为82.00，困惑度为6.6。

    arXiv:2402.15514v1 Announce Type: cross  Abstract: We address the problem of scaling up the production of media content, including commentary and personalized news stories, for large-scale sports and music events worldwide. Our approach relies on generative AI models to transform a large volume of multimodal data (e.g., videos, articles, real-time scoring feeds, statistics, and fact sheets) into coherent and fluent text. Based on this approach, we introduce, for the first time, an AI commentary system, which was deployed to produce automated narrations for highlight packages at the 2023 US Open, Wimbledon, and Masters tournaments. In the same vein, our solution was extended to create personalized content for ESPN Fantasy Football and stories about music artists for the Grammy awards. These applications were built using a common software architecture achieved a 15x speed improvement with an average Rouge-L of 82.00 and perplexity of 6.6. Our work was successfully deployed at the aforeme
    
[^162]: AgentOhana：为有效智能体学习设计统一数据和训练流水线

    AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning

    [https://arxiv.org/abs/2402.15506](https://arxiv.org/abs/2402.15506)

    AgentOhana提供了一种统一数据和训练流水线的综合解决方案，有助于克服使用大型语言模型（LLMs）进行智能体任务时的挑战。

    

    由大型语言模型（LLMs）提供支持的自主智能体引起了重大研究关注。然而，充分利用LLMs的潜力进行基于智能体的任务面临困难，这是由于具有多轮轨迹的多样化数据源的异构性。在本文中，我们介绍AgentOhana作为解决这些挑战的综合解决方案。AgentOhana从不同环境中聚合智能体轨迹，涵盖了各种情景。它精心地将这些轨迹标准化和统一到一致的格式中，简化了为智能体训练优化的通用数据加载器的创建。通过数据统一，我们的训练流水线在不同数据源之间保持平衡，并在数据集划分和模型训练过程中保持设备之间的独立随机性。此外，我们还介绍了xLAM-v0.1，一个大动作模式

    arXiv:2402.15506v1 Announce Type: new  Abstract: Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges. \textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \textbf{xLAM-v0.1}, a large action mode
    
[^163]: 偏见和反复无常：衡量大型语言模型社会歧视的统计框架

    Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models

    [https://arxiv.org/abs/2402.15481](https://arxiv.org/abs/2402.15481)

    提出了Prejudice-Caprice Framework（PCF）来全面衡量LLMs中的歧视，考虑了它们在不同上下文中的一贯偏见偏好和偏好变化。

    

    arXiv:2402.15481v1 公告类型: 新的 摘要: 大型语言模型（LLMs）在社会运营中的日益融合加剧了它们对经济、法律、教育和医疗等重要领域决策的影响，引发了公众对这些模型涉及歧视安全和可靠性的担忧。然而，先前的歧视测量框架仅评估LLMs的平均歧视行为，往往由于忽视了一个额外的导致歧视的因素，即LLMs在不同上下文中的预测变化而变得不足。在这项工作中，我们提出了Prejudice-Caprice Framework（PCF），通过考虑LLMs的一贯偏见偏好和在多样上

    arXiv:2402.15481v1 Announce Type: new  Abstract: The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability. However, prior discrimination measuring frameworks solely assess the average discriminatory behavior of LLMs, often proving inadequate due to the overlook of an additional discrimination-leading factor, i.e., the LLMs' prediction variation across diverse contexts. In this work, we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently biased preference and preference variation across diverse contexts. Specifically, we mathematically dissect the aggregated contextualized discrimination risk of LLMs into prejudice risk, originating from LLMs' persistent prejudice, and caprice risk, stemmin
    
[^164]: ArabianGPT：基于原生阿拉伯语的大型语言模型

    ArabianGPT: Native Arabic GPT-based Large Language

    [https://arxiv.org/abs/2402.15313](https://arxiv.org/abs/2402.15313)

    提出了ArabianGPT，这是一系列专门为阿拉伯语设计的基于Transformer的模型，包括大小和复杂性不同的ArabianGPT-0.1B和ArabianGPT-0.3B，帮助弥补了本土阿拉伯语大型语言模型的不足。

    

    英语和拉丁语为主导的大型语言模型（LLMs）的主导地位导致了本土阿拉伯语LLMs的显著不足。本文提出ArabianGPT，这是一系列基于Transformer的模型，专门为阿拉伯语设计而成。这些模型包括ArabianGPT-0.1B和ArabianGPT-0.3B，大小和复杂性不同，与阿拉伯语的微妙语言特征相契合。

    arXiv:2402.15313v1 Announce Type: cross  Abstract: The predominance of English and Latin-based large language models (LLMs) has led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic's intricate morphology and syntax. Consequently, there is a theoretical and practical imperative for developing LLMs predominantly focused on Arabic linguistic elements. To address this gap, this paper proposes ArabianGPT, a series of transformer-based models within the ArabianLLM suite designed explicitly for Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in size and complexity, aligning with the nuanced linguistic characteristics of Arabic. The AraNizer tokenizer, integral to these models, addresses the unique morphological aspects of Arabic script, ensuring more accurate text processing. Empirical results from fine-tuning t
    
[^165]: 通过实例级前缀实现大型语言模型的细粒度脱毒

    Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models

    [https://arxiv.org/abs/2402.15202](https://arxiv.org/abs/2402.15202)

    提出一种通过实例级前缀在注意力空间中进行细粒度比较，从而实现大型语言模型的细粒度脱毒的方法。

    

    通过训练大型语言模型（LLMs），在自然语言处理（NLP）任务中取得了令人印象深刻的结果。然而，这些模型偶尔会对某些提示生成毒性内容，如侮辱、威胁和粗话，从而限制了它们的实际效用。为了解决这一问题，利用各种基于微调和基于解码的方法来减轻毒性。然而，这些方法通常需要额外的成本，如高质量的训练数据或辅助模型。在本文中，我们提出了通过实例级前缀进行细粒度脱毒（FGDILP），以减轻毒性文本而无需额外费用。具体来说，FGDILP通过在实例级别使用一个带有正前缀的提示来对比注意力空间中的上下文表示，而多个带有负前缀的提示。这允许构建细粒度的次毒性向量，使文本被识别为次毒性变得更加精细。

    arXiv:2402.15202v1 Announce Type: new  Abstract: Impressive results have been achieved in natural language processing (NLP) tasks through the training of large language models (LLMs). However, these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility. To tackle this issue, various finetuning-based and decoding-based approaches have been utilized to mitigate toxicity. However, these methods typically necessitate additional costs such as high-quality training data or auxiliary models. In this paper, we propose fine-grained detoxification via instance-level prefixes (FGDILP) to mitigate toxic text without additional cost. Specifically, FGDILP contrasts the contextualized representation in attention space using a positive prefix-prepended prompt against multiple negative prefix-prepended prompts at the instance level. This allows for constructing fine-grained subtoxicity vectors, whic
    
[^166]: 重新审视远程监督命名实体识别：一个新的基准和简单方法

    Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach

    [https://arxiv.org/abs/2402.14948](https://arxiv.org/abs/2402.14948)

    提出了基于课程的正无标记学习 CuPUL 方法，能够显著降低嘈杂标签的影响，胜过现有方法

    

    本文深入探讨了在远程监督（DS-NER）框架下的命名实体识别（NER），主要挑战在于标签质量受到误差的影响，如假阳性、假阴性和正向类型错误。我们批判性地评估了当前DS-NER方法的有效性，使用了一个名为QTL的真实世界基准数据集，揭示它们的性能往往不符合预期。为了解决标签噪声普遍问题，我们引入了一种简单而有效的方法，基于课程的正无标记学习（CuPUL），在训练过程中策略性地从“易”和更清洁的样本开始，以增强模型对嘈杂样本的韧性。我们的实证结果突出了CuPUL减少嘈杂标签影响并胜过现有方法的能力。

    arXiv:2402.14948v1 Announce Type: new  Abstract: This paper delves into Named Entity Recognition (NER) under the framework of Distant Supervision (DS-NER), where the main challenge lies in the compromised quality of labels due to inherent errors such as false positives, false negatives, and positive type errors. We critically assess the efficacy of current DS-NER methodologies using a real-world benchmark dataset named QTL, revealing that their performance often does not meet expectations. To tackle the prevalent issue of label noise, we introduce a simple yet effective approach, Curriculum-based Positive-Unlabeled Learning CuPUL, which strategically starts on "easy" and cleaner samples during the training process to enhance model resilience to noisy samples. Our empirical results highlight the capability of CuPUL to significantly reduce the impact of noisy labels and outperform existing methods.
    
[^167]: LLMBind: 一种统一的模态任务集成框架

    LLMBind: A Unified Modality-Task Integration Framework

    [https://arxiv.org/abs/2402.14891](https://arxiv.org/abs/2402.14891)

    提出了LLMBind，一种统一的模态任务集成框架，通过将大型语言模型和预训练任务模型绑定在一起，实现了多种模态任务的灵活输入和输出组合。

    

    最近对于多模态大型语言模型在处理各种模态任务方面取得了进展，但它们对于复杂的多模态任务的集成能力有限，从而限制了该领域的发展。在这项工作中，我们带头探索并提出了LLMBind，一种用于模态任务集成的统一框架，该框架将大型语言模型和相应的预训练任务模型与任务特定的标记绑定在一起。因此，LLMBind可以以多种图像、文本、视频和音频的组合解释输入并生成输出。具体来说，我们引入了一种专家混合技术，通过不同专家之间的协作实现不同多模态任务的有效学习。此外，我们创建了一个包含40万条指令数据的多任务数据集，解锁了交互式视觉生成和编辑任务的能力。大量实验证明了我们的方法的有效性。

    arXiv:2402.14891v1 Announce Type: cross  Abstract: While recent progress in multimodal large language models tackles various modality tasks, they posses limited integration capabilities for complex multi-modality tasks, consequently constraining the development of the field. In this work, we take the initiative to explore and propose the LLMBind, a unified framework for modality task integration, which binds Large Language Models and corresponding pre-trained task models with task-specific tokens. Consequently, LLMBind can interpret inputs and produce outputs in versatile combinations of image, text, video, and audio. Specifically, we introduce a Mixture-of-Experts technique to enable effective learning for different multimodal tasks through collaboration among diverse experts. Furthermore, we create a multi-task dataset comprising 400k instruction data, which unlocks the ability for interactive visual generation and editing tasks. Extensive experiments show the effectiveness of our fr
    
[^168]: Vygotsky Distance: 用于基准任务相似性的度量方法

    Vygotsky Distance: Measure for Benchmark Task Similarity

    [https://arxiv.org/abs/2402.14890](https://arxiv.org/abs/2402.14890)

    论文提出了一种基于相对性能而非任务属性的相似性度量方法，即“维果茨基距离”，可帮助减少评估任务数量并保持高验证质量。

    

    论文介绍了一种理论工具和实践算法来计算基准任务之间的相似性，称之为"维果茨基距离"。这种相似性度量的核心思想是基于“学生”在给定任务上的相对表现，而不是基于任务本身的属性。如果两个任务在维果茨基距离上彼此接近，模型在这些任务上 tend to have similar relative performance。因此，通过了解任务之间的维果茨基距离，可以显著减少评估任务数量，同时保持高验证质量。

    arXiv:2402.14890v1 Announce Type: cross  Abstract: Evaluation plays a significant role in modern natural language processing. Most modern NLP benchmarks consist of arbitrary sets of tasks that neither guarantee any generalization potential for the model once applied outside the test set nor try to minimize the resource consumption needed for model evaluation. This paper presents a theoretical instrument and a practical algorithm to calculate similarity between benchmark tasks, we call this similarity measure "Vygotsky distance". The core idea of this similarity measure is that it is based on relative performance of the "students" on a given task, rather that on the properties of the task itself. If two tasks are close to each other in terms of Vygotsky distance the models tend to have similar relative performance on them. Thus knowing Vygotsky distance between tasks one can significantly reduce the number of evaluation tasks while maintaining a high validation quality. Experiments on v
    
[^169]: Checkfor.ai AI生成文本分类器技术报告

    Technical Report on the Checkfor.ai AI-Generated Text Classifier

    [https://arxiv.org/abs/2402.14873](https://arxiv.org/abs/2402.14873)

    Checkfor.ai AI生成文本分类器在区分大型语言模型生成文本和人类编写文本方面表现优异，提出了硬负挖掘与合成镜像训练算法，具有高准确性和泛化能力。

    

    我们提出了Checkfor.ai文本分类器，这是一个基于Transformer的神经网络，经过训练可以区分由大型语言模型编写的文本和由人类编写的文本。Checkfor.ai在由十种文本领域（学生写作、创意写作、科学写作、书籍、百科全书、新闻、电子邮件、科学论文、简答问答）和8个开源闭源大型语言模型组成的综合基准测试中，表现优于零冲击方法如DetectGPT以及主流商业AI检测工具，误差率降低了9倍以上。我们提出了一种训练算法，即硬负挖掘与合成镜像，使我们的分类器能够在评论等高数据领域实现几个数量级的更低误报率。最后，我们展示了Checkfor.ai不对非母语英语人士产生偏见，并推广到训练过程中未见的领域和模型。

    arXiv:2402.14873v1 Announce Type: cross  Abstract: We present the Checkfor.ai text classifier, a transformer-based neural network trained to distinguish text written by large language models from text written by humans. Checkfor.ai outperforms zero-shot methods such as DetectGPT as well as leading commercial AI detection tools with over 9 times lower error rates on a comprehensive benchmark comprised of ten text domains (student writing, creative writing, scientific writing, books, encyclopedias, news, email, scientific papers, short-form Q\&A) and 8 open- and closed-source large language models. We propose a training algorithm, hard negative mining with synthetic mirrors, that enables our classifier to achieve orders of magnitude lower false positive rates on high-data domains such as reviews. Finally, we show that Checkfor.ai is not biased against nonnative English speakers and generalizes to domains and models unseen during training.
    
[^170]: Noise-BERT: 一种具有噪声对齐预训练的统一扰动鲁棒性框架，用于嘈杂的槽填充任务

    Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment Pre-training for Noisy Slot Filling Task

    [https://arxiv.org/abs/2402.14494](https://arxiv.org/abs/2402.14494)

    提出了Noise-BERT框架，包含噪声对齐预训练任务，通过对比学习损失和对抗攻击训练策略，以提高在嘈杂环境下的槽填充任务表现。

    

    在现实对话系统中，用户输入信息经常遭受各种类型的输入扰动，这影响了槽填充任务。尽管基于规则的数据增强方法已经取得了令人满意的结果，但当面对未知噪声干扰时，它们无法展现出期望的泛化能力。在本研究中，我们通过提出Noise-BERT来解决槽填充中输入扰动带来的挑战，这是一个具有噪声对齐预训练的统一扰动鲁棒性框架。我们的框架包含两个Noise Alignment预训练任务：槽屏蔽预测和句子嘈杂度判别，旨在引导预训练语言模型捕捉准确的槽信息和噪声分布。在微调过程中，我们采用对比学习损失来增强实体和标签的语义表示。此外，我们引入了对抗攻击训练策略以提高语义表示的鲁棒性。

    arXiv:2402.14494v1 Announce Type: new  Abstract: In a realistic dialogue system, the input information from users is often subject to various types of input perturbations, which affects the slot-filling task. Although rule-based data augmentation methods have achieved satisfactory results, they fail to exhibit the desired generalization when faced with unknown noise disturbances. In this study, we address the challenges posed by input perturbations in slot filling by proposing Noise-BERT, a unified Perturbation-Robust Framework with Noise Alignment Pre-training. Our framework incorporates two Noise Alignment Pre-training tasks: Slot Masked Prediction and Sentence Noisiness Discrimination, aiming to guide the pre-trained language model in capturing accurate slot information and noise distribution. During fine-tuning, we employ a contrastive learning loss to enhance the semantic representation of entities and labels. Additionally, we introduce an adversarial attack training strategy to i
    
[^171]: 在巨大语言模型中分析概念表达：借助反向词典探查

    On the Tip of the Tongue: Analyzing Conceptual Representation in Large Language Models with Reverse-Dictionary Probe

    [https://arxiv.org/abs/2402.14404](https://arxiv.org/abs/2402.14404)

    该论文通过重新利用反向词典任务的案例研究，探查了大型语言模型对概念推理的能力，发现模型在该任务中表现出高准确性，并且表示空间编码了有关对象类别和细粒度特征的信息，同时还发现该任务探查的概念推理能力能够预测模型在多个基准测试中的一般推理表现。

    

    探查和增强大型语言模型的推理能力仍然是一个关键的未解问题。在这里，我们重新利用反向词典任务作为一个案例研究，来探查LLMs对概念推理的能力。我们使用上下文学习来引导模型生成一个语言描述中暗示的对象概念的术语。模型在这个任务中稳健地实现了高准确性，并且它们的表示空间编码了关于对象类别和细粒度特征的信息。进一步的实验表明，通过反向词典任务探查的概念推理能力能够预测模型在多个基准测试中的一般推理表现，尽管模型在句法泛化行为上表现相似。探索性分析表明，通过提示LLMs使用描述$\Rightarrow$单词示例可能会诱导出超越任务构型表面差异的泛化，并促进模型对更广泛的共同性的研究

    arXiv:2402.14404v1 Announce Type: cross  Abstract: Probing and enhancing large language models' reasoning capacity remains a crucial open question. Here we re-purpose the reverse dictionary task as a case study to probe LLMs' capacity for conceptual inference. We use in-context learning to guide the models to generate the term for an object concept implied in a linguistic description. Models robustly achieve high accuracy in this task, and their representation space encodes information about object categories and fine-grained features. Further experiments suggest that the conceptual inference ability as probed by the reverse-dictionary task predicts model's general reasoning performance across multiple benchmarks, despite similar syntactic generalization behaviors across models. Explorative analyses suggest that prompting LLMs with description$\Rightarrow$word examples may induce generalization beyond surface-level differences in task construals and facilitate models on broader commons
    
[^172]: 为医学构建多语言语言模型

    Towards Building Multilingual Language Model for Medicine

    [https://arxiv.org/abs/2402.13963](https://arxiv.org/abs/2402.13963)

    本文提出了为医学领域构建多语言语言模型的三个关键贡献:构建了新的多语言医学语料库MMedC，提出了多语言医学多选问答基准MMedBench，并且通过在MMedC上进一步训练获得了性能优越的MMedLM 2模型。

    

    本文旨在开发一种面向医学的开源多语言语言模型，使得更广泛的语言多样性受众受益。我们的工作主要贡献体现在以下几个方面:首先，针对多语言医学特定适应性，我们构建了一个新的多语言医学语料库，包含大约25.5B个tokens，覆盖了6种主要语言，被称为MMedC，这使得现有通用LLM能够进行自回归训练。其次，为了监测医学领域多语言LLM的发展，我们提出了一个新的带有解释的多语言医学多选问答基准，称为MMedBench；第三，我们评估了一些流行的开源大型语言模型(LLMs)在我们的基准上的表现，以及那些在MMedC上进一步进行自回归训练的模型，最终，我们的最终模型，命名为MMedLM 2，仅有7B参数，取得了卓越的性能。

    arXiv:2402.13963v1 Announce Type: new  Abstract: In this paper, we aim to develop an open-source, multilingual language model for medicine, that the benefits a wider, linguistically diverse audience from different regions. In general, we present the contribution from the following aspects: first, for multilingual medical-specific adaptation, we construct a new multilingual medical corpus, that contains approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, that enables auto-regressive training for existing general LLMs. second, to monitor the development of multilingual LLMs in medicine, we propose a new multilingual medical multi-choice question-answering benchmark with rationale, termed as MMedBench; third, we have assessed a number of popular, opensource large language models (LLMs) on our benchmark, along with those further auto-regressive trained on MMedC, as a result, our final model, termed as MMedLM 2, with only 7B parameters, achieves superior performance c
    
[^173]: Kuaiji：第一个中国会计大型语言模型

    Kuaiji: the First Chinese Accounting Large Language Model

    [https://arxiv.org/abs/2402.13866](https://arxiv.org/abs/2402.13866)

    Kuaiji是第一个中国会计大型语言模型，通过Baichuan框架精心调整，支持的CAtAcctQA数据集，展现出卓越的准确性和响应速度，具有开创性地创建了中国会计数据集，并证实了在真实会计场景中的高效性。

    

    大语言模型（LLMs）如ChatGPT和GPT-4已经展示出在理解和生成自然语言方面的出色能力。然而，当面临任务要求适应会计等专业领域时，它们会遇到困难。为了解决这一挑战，我们引入了Kuaiji，一个专门定制的会计大型语言模型。Kuaiji经过精心调整，使用包含连续预训练和监督微调过程的Baichuan框架。在CAtAcctQA的支持下，这是一个包含大量真实会计师与客户对话的数据集，Kuaiji表现出卓越的准确性和响应速度。我们的贡献包括创建了第一个中国会计数据集，将Kuaiji建立为一种领先的开源中国会计LLM，并通过真实会计场景对其有效性进行了验证。

    arXiv:2402.13866v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated impressive proficiency in comprehending and generating natural language. However, they encounter difficulties when tasked with adapting to specialized domains such as accounting. To address this challenge, we introduce Kuaiji, a tailored Accounting Large Language Model. Kuaiji is meticulously fine-tuned using the Baichuan framework, which encompasses continuous pre-training and supervised fine-tuning processes. Supported by CAtAcctQA, a dataset containing large genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy and response speed. Our contributions encompass the creation of the first Chinese accounting dataset, the establishment of Kuaiji as a leading open-source Chinese accounting LLM, and the validation of its efficacy through real-world accounting scenarios.
    
[^174]: $\infty$Bench: 将长上下文评估扩展至超过10万令牌

    $\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens

    [https://arxiv.org/abs/2402.13718](https://arxiv.org/abs/2402.13718)

    提出了$\infty$Bench，第一个以平均数据长度超过10万个令牌的LLM基准，用于评估处理长上下文的能力

    

    处理和推理长上下文对于大型语言模型（LLMs）的许多实际应用至关重要，如文档理解和代理构建。本文提出$\infty$Bench，第一个LLM基准，平均数据长度超过10万个令牌。$\infty$Bench包含涵盖不同领域的合成和现实任务，以英文和中文呈现。$\infty$Bench中的任务旨在需要深刻理解上下文中的长依赖性，并且简单地从上下文中检索有限数量的段落对于这些任务来说是不够的。

    arXiv:2402.13718v1 Announce Type: new  Abstract: Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose $\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens. $\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks
    
[^175]: GumbelSoft: 通过GumbelMax技巧实现多样化的语言模型数字水印

    GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick

    [https://arxiv.org/abs/2402.12948](https://arxiv.org/abs/2402.12948)

    通过开发 GumbelSoft 水印，我们提出了一种能够在高多样性环境中增强生成文本多样性的解决方案，相较于其他方案表现更为优秀。

    

    大型语言模型(Large language models, LLMs)能够生成类似人类的文本，但也引发了人们对其在虚假新闻和学术不诚实方面的担忧。解码为基础的水印，尤其是基于GumbelMax技巧的水印(GM水印)，是防范机器生成文本滥用的杰出解决方案，因其显著的可检测性而脱颖而出。然而，GM水印在生成多样性方面面临一个主要挑战，对于相同提示始终产生相同输出，从而负面影响生成多样性和用户体验。为了克服这一局限，我们提出了一种新型GM水印，即Logits-Addition水印，及其三个变体，专门设计用于增强多样性。在这些变体中，GumbelSoft水印(作为Logits-Addition水印的一个softmax变体)在高多样性环境中表现出优越性能，其AUROC分数超过其他两个替代变体0.1至0.3，并超越其他方案。

    arXiv:2402.12948v1 Announce Type: new  Abstract: Large language models (LLMs) excellently generate human-like text, but also raise concerns about misuse in fake news and academic dishonesty. Decoding-based watermark, particularly the GumbelMax-trick-based watermark(GM watermark), is a standout solution for safeguarding machine-generated texts due to its notable detectability. However, GM watermark encounters a major challenge with generation diversity, always yielding identical outputs for the same prompt, negatively impacting generation diversity and user experience. To overcome this limitation, we propose a new type of GM watermark, the Logits-Addition watermark, and its three variants, specifically designed to enhance diversity. Among these, the GumbelSoft watermark (a softmax variant of the Logits-Addition watermark) demonstrates superior performance in high diversity settings, with its AUROC score outperforming those of the two alternative variants by 0.1 to 0.3 and surpassing oth
    
[^176]: Archer: 一个具有算术、常识和假设推理的人工标记文本到SQL数据集

    Archer: A Human-Labeled Text-to-SQL Dataset with Arithmetic, Commonsense and Hypothetical Reasoning

    [https://arxiv.org/abs/2402.12554](https://arxiv.org/abs/2402.12554)

    Archer数据集是一个具有挑战性的双语文本到SQL数据集，包含算术、常识和假设推理的复杂推理，挑战了当前最先进模型的能力。

    

    我们提出了Archer，一个具有挑战性的双语文本到SQL数据集，专注于包括算术、常识和假设推理在内的复杂推理。它包含1,042个英文问题和1,042个中文问题，以及521个唯一的SQL查询，涵盖了20个领域中的20个英语数据库。值得注意的是，与现有公开数据集相比，这个数据集展示了显著更高的复杂性水平。我们的评估表明，Archer挑战了当前最先进模型的能力，Spider排行榜上的排名靠前的模型在Archer测试集上仅达到6.73%的执行准确率。因此，Archer为这一领域的未来研究提出了重大挑战。

    arXiv:2402.12554v1 Announce Type: new  Abstract: We present Archer, a challenging bilingual text-to-SQL dataset specific to complex reasoning, including arithmetic, commonsense and hypothetical reasoning. It contains 1,042 English questions and 1,042 Chinese questions, along with 521 unique SQL queries, covering 20 English databases across 20 domains. Notably, this dataset demonstrates a significantly higher level of complexity compared to existing publicly available datasets. Our evaluation shows that Archer challenges the capabilities of current state-of-the-art models, with a high-ranked model on the Spider leaderboard achieving only 6.73% execution accuracy on Archer test set. Thus, Archer presents a significant challenge for future research in this field.
    
[^177]: AnyGPT：统一的多模式离散序列建模语言模型

    AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling

    [https://arxiv.org/abs/2402.12226](https://arxiv.org/abs/2402.12226)

    AnyGPT是一个统一的多模态语言模型，通过离散表示实现各种模态的统一处理，能够在不改变大型语言模型架构或训练方式的情况下稳定训练，为新模态的无缝整合提供了可能。

    

    我们介绍了 AnyGPT，这是一个任意多模式语言模型，利用离散表示统一处理各种模态，包括语音、文本、图像和音乐。AnyGPT 可以稳定训练，无需对当前大型语言模型（LLM）架构或训练范式进行任何改动。相反，它仅依赖于数据级预处理，促进了新模态的无缝集成到LLM中，类似于新语言的整合。我们构建了一个多模式文本中心的数据集，用于多模式对齐预训练。利用生成模型，我们合成了第一个大规模任意多模式指令数据集。它包括108k个多轮对话示例，精细地交织各种模态，从而使模型能够处理多模态输入和输出的任意组合。实验结果表明，AnyGPT能够促进...

    arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
    
[^178]: Mafin: 用模型增强微调来增强黑盒嵌入

    Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning

    [https://arxiv.org/abs/2402.12177](https://arxiv.org/abs/2402.12177)

    Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。

    

    检索增强生成（RAG）已经成为缓解大型语言模型（LLMs）中幻觉的有效解决方案。RAG中的检索阶段通常涉及预训练的嵌入模型，将查询和段落转换为向量以捕获它们的语义。然而，当应用于特定领域知识时，标准的预训练嵌入模型可能表现出次优性能，需要进行微调。本文解决了仅能从黑盒模型获取嵌入的情况。我们引入了模型增强微调（Mafin）--一种通过用可训练的嵌入模型增强黑盒嵌入模型来进行微调的新方法。我们的结果表明，Mafin仅需要训练一个小的增强模型就可以显著提高黑盒嵌入的性能。我们验证了我们的方法在有标签和无标签数据集上的有效性。

    arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
    
[^179]: 针对参数高效微调的权重投毒后门攻击的防御

    Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning

    [https://arxiv.org/abs/2402.12168](https://arxiv.org/abs/2402.12168)

    PEFT相对于全参数微调更容易受到权重投毒后门攻击的影响，提出了一个通过置信度识别受污染样本的毒化样本识别模块（PSIM），为权重投毒后门攻击提供稳健防御

    

    最近，针对语言模型应用提出并成功实施了各种参数高效微调（PEFT）策略。然而，这引发了一个问题，即当面对权重投毒后门攻击时，仅更新有限模型参数的PEFT是否构成安全漏洞。我们展示了PEFT相对于全参数微调方法更容易受到权重投毒后门攻击的影响，预定义的触发器仍然易受利用，预定义的目标在微调后依然保持高置信度。受到这一见解的启发，我们开发了一个利用PEFT的毒化样本识别模块（PSIM），通过置信度识别受污染样本，提供针对权重投毒后门攻击的稳健防御。具体而言，我们利用PEFT训练PSIM，带有随机重置样本标签。在推断过程中，

    arXiv:2402.12168v1 Announce Type: cross  Abstract: Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented. However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks. In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning. Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels. During the inference pr
    
[^180]: 通过基于体裁和主题特征的选择性屏蔽，将语言模型适应专业领域

    Language Model Adaptation to Specialized Domains through Selective Masking based on Genre and Topical Characteristics

    [https://arxiv.org/abs/2402.12036](https://arxiv.org/abs/2402.12036)

    通过排名关键词并指导屏蔽过程，这项研究提出了一种利用体裁和主题信息定制语言模型适应专业领域的创新方法。

    

    最近，预训练语言建模的进展在各种自然语言处理（NLP）任务中取得了显著进展。模型训练期间的词屏蔽构成了像BERT这样的架构中语言建模的关键组成部分。然而，目前的词屏蔽方法依赖于随机选择，可能忽视领域特定的语言属性。在本文中，我们介绍了一种创新的屏蔽方法，利用体裁和主题信息来定制语言模型以适应专业领域。我们的方法包括一个排名过程，根据单词的重要性对其进行优先级排序，随后引导屏蔽过程。我们进行的实验使用法律领域内的持续预训练，强调了我们的方法在英语LegalGLUE基准上的有效性。预训练语言模型和代码可免费使用。

    arXiv:2402.12036v1 Announce Type: new  Abstract: Recent advances in pre-trained language modeling have facilitated significant progress across various natural language processing (NLP) tasks. Word masking during model training constitutes a pivotal component of language modeling in architectures like BERT. However, the prevalent method of word masking relies on random selection, potentially disregarding domain-specific linguistic attributes. In this article, we introduce an innovative masking approach leveraging genre and topicality information to tailor language models to specialized domains. Our method incorporates a ranking process that prioritizes words based on their significance, subsequently guiding the masking procedure. Experiments conducted using continual pre-training within the legal domain have underscored the efficacy of our approach on the LegalGLUE benchmark in the English language. Pre-trained language models and code are freely available for use.
    
[^181]: 大型语言模型推理解释的可解释性有多高？

    How Interpretable are Reasoning Explanations from Prompting Large Language Models?

    [https://arxiv.org/abs/2402.11863](https://arxiv.org/abs/2402.11863)

    对大型语言模型推理解释提出了全面、多角度的评估，包括忠实度、强健性和效用，并引入了新的可解释性指标。

    

    Prompt Engineering已经引起了人们的极大关注，可以增强大型语言模型在多项任务中的性能。Chain-of-Thought等技术不仅增强了任务性能，还描绘了清晰的推理步骤轨迹，为观众提供了一种有形的解释形式。我们对可解释性进行了全面多角度的评估，不仅考虑了忠实度，还考虑了在多个常识推理基准测试中的强健性和效用。此外，我们引入了一个简单的可解释性指标。

    arXiv:2402.11863v1 Announce Type: new  Abstract: Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability ali
    
[^182]: 重新探讨零阶优化在内存高效LLM微调中的应用：一个基准研究

    Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark

    [https://arxiv.org/abs/2402.11592](https://arxiv.org/abs/2402.11592)

    本研究提出了一种不使用反向传播的零阶优化方法，用于降低LLM微调中的内存成本，通过全面的基准研究扩展了对不同的ZO优化技术的探索。

    

    在自然语言处理（NLP）领域的不断发展中，使用SGD和Adam等一阶（FO）优化器微调预训练的大型语言模型（LLMs）已成为标准。然而，随着LLMs体积的增长，由于FO梯度计算的反向传播（BP）带来的巨大内存开销构成了一个重大挑战。解决这个问题至关重要，尤其对于内存效率至关重要的设备端训练等应用。本文提出了一种转向不使用BP的零阶（ZO）优化的方法，用于在LLM微调过程中降低内存成本，构建在MeZO提出的概念基础上。与传统的ZO梯度下降方法不同，我们的工作将探索扩展到更广泛的ZO优化技术，通过全面的、首次推出的基准研究跨越五个LLM系列（Roberta，OPT，LLaMA，Vicuna，Mistral），三种任务复杂性和五种微调方案。

    arXiv:2402.11592v1 Announce Type: new  Abstract: In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes
    
[^183]: 不要走向极端：揭示LLMs在隐式仇恨言论检测中的过度敏感性和校准限制

    Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection

    [https://arxiv.org/abs/2402.11406](https://arxiv.org/abs/2402.11406)

    本文研究了LLMs在检测隐式仇恨言论和表达信心时的能力，发现LLMs存在两个极端：对可能引起公平问题的群体或话题表现出过于敏感，同时置信度评分过度集中在一个范围内。

    

    大型语言模型（LLMs）的公平性和可信度越来越受到关注。隐式仇恨言论，利用间接语言传达仇恨意图，占据实践中的重要部分。然而，LLMs有效解决这一问题的程度尚未得到充分审查。本文探讨了LLMs检测隐式仇恨言论（分类任务）以及对其响应的信心进行表达（校准任务）的能力。我们的评估细致考虑了各种提示模式和主流的不确定性估计方法。我们的研究结果突出了LLMs展示了两个极端：（1）LLMs对可能导致公平性问题的群体或话题显示出过度的敏感性，导致将良性陈述错误分类为仇恨言论。 （2）LLMs对每种方法的置信度得分过度集中在一个固定范围上，无论数据集的复杂性如何也保持不变。

    arXiv:2402.11406v1 Announce Type: new  Abstract: The fairness and trustworthiness of Large Language Models (LLMs) are receiving increasing attention. Implicit hate speech, which employs indirect language to convey hateful intentions, occupies a significant portion of practice. However, the extent to which LLMs effectively address this issue remains insufficiently examined. This paper delves into the capability of LLMs to detect implicit hate speech (Classification Task) and express confidence in their responses (Calibration Task). Our evaluation meticulously considers various prompt patterns and mainstream uncertainty estimation methods. Our findings highlight that LLMs exhibit two extremes: (1) LLMs display excessive sensitivity towards groups or topics that may cause fairness issues, resulting in misclassifying benign statements as hate speech. (2) LLMs' confidence scores for each method excessively concentrate on a fixed range, remaining unchanged regardless of the dataset's complex
    
[^184]: 医疗AI中的泛化性能：临床大型语言模型的评估

    Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model

    [https://arxiv.org/abs/2402.10965](https://arxiv.org/abs/2402.10965)

    大型语言模型在医疗健康领域有着重要作用，然而它们的泛化效果取决于在不同临床环境和人群中的表现，对于泛化能力不足的原因进行了分析，发现在样本较少的医院和特定人群中存在挑战。

    

    大型语言模型（LLMs）的进展为医疗健康领域提供了新机遇，可以改善患者护理、临床决策以及提升医师和管理人员的工作流程。然而，这些模型的潜力重要取决于它们在临床环境和人群中有效泛化的能力，这是在早期开发中经常被低估的挑战。为了更好地理解这些挑战的原因并制定缓解方法，我们评估了ClinicLLM，这是一个在 [HOSPITAL] 的临床笔记上训练的LLM模型，对其在30天全因素再入院预测中的表现进行分析，关注跨医院和患者特征的变异性。我们发现在样本较少的医院、政府和未指定保险的患者、老年人以及高共病性患者中，泛化效果较差。为了了解泛化不彰的原因，我们调查了样本量

    arXiv:2402.10965v1 Announce Type: new  Abstract: Advances in large language models (LLMs) provide new opportunities in healthcare for improved patient care, clinical decision-making, and enhancement of physician and administrator workflows. However, the potential of these models importantly depends on their ability to generalize effectively across clinical environments and populations, a challenge often underestimated in early development. To better understand reasons for these challenges and inform mitigation approaches, we evaluated ClinicLLM, an LLM trained on [HOSPITAL]'s clinical notes, analyzing its performance on 30-day all-cause readmission prediction focusing on variability across hospitals and patient characteristics. We found poorer generalization particularly in hospitals with fewer samples, among patients with government and unspecified insurance, the elderly, and those with high comorbidities. To understand reasons for lack of generalization, we investigated sample sizes 
    
[^185]: 拉马在英语中有效吗？关于多语言变压器的潜在语言

    Do Llamas Work in English? On the Latent Language of Multilingual Transformers

    [https://arxiv.org/abs/2402.10588](https://arxiv.org/abs/2402.10588)

    本研究通过对Llama-2系列变压器模型的研究发现，在多语言语言模型中存在英语作为内部枢纽语言的现象，这有助于理解语言模型的功能方式以及语言偏见的起源。

    

    我们探讨了是否在不平衡、英语主导的语料库上训练的多语言语言模型使用英语作为内部枢纽语言的问题——这对于理解语言模型的功能方式以及语言偏见的起源至关重要。 我们关注Llama-2系列变压器模型，通过使用精心构建的非英语提示和唯一正确的单词延续来进行研究。 从一层到另一层，变压器逐渐将最终提示令牌的输入嵌入映射到输出嵌入，从中计算下一个令牌的概率。 通过跟踪其在高维空间中的中间嵌入，揭示了三个不同的阶段，即中间嵌入（1）开始远离输出令牌嵌入；（2）在中间层已经允许解码一个语义正确的下一个令牌，但更倾向于英语版本而不是输入语言的版本；（3）最终移动到

    arXiv:2402.10588v1 Announce Type: new  Abstract: We ask whether multilingual language models trained on unbalanced, English-dominated corpora use English as an internal pivot language -- a question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the Llama-2 family of transformer models, our study uses carefully constructed non-English prompts with a unique correct single-token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next-token probabilities are computed. Tracking intermediate embeddings through their high-dimensional space reveals three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already allow for decoding a semantically correct next token in the middle layers, but give higher probability to its version in English than in the input language; (3) finally move into an
    
[^186]: 基于上下文的奖励：基于动态偏好调整的多目标基础模型对齐

    Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment

    [https://arxiv.org/abs/2402.10207](https://arxiv.org/abs/2402.10207)

    本文介绍了Rewards-in-Context（RiC）方法，该方法通过多个奖励条件控制基础模型的响应，并应用有监督的微调进行对齐。它具有简单性和适应性，并支持在推理时动态调整用户偏好。

    

    我们考虑了基于人类偏好的基础模型多目标对齐问题，这是实现有益和无害的人工智能系统的关键步骤。然而，使用强化学习（RL）对大型基础模型进行微调通常是昂贵且不稳定的，并且人类偏好的多维度、异质性和冲突性进一步复杂化了对齐过程。在本文中，我们引入了Rewards-in-Context（RiC）方法，它使得基础模型的响应取决于其提示上下文中的多个奖励，并应用有监督的微调来进行对齐。RiC的显著特点是简单性和适应性，因为它只需要对单个基础模型进行有监督的微调，并支持在推理时动态调整用户偏好。受到抽象的凸优化问题的解析解的启发，我们提出了一种动态推理时调整方法。

    arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
    
[^187]: SafeDecoding: 通过安全感知解码防御越狱攻击

    SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding

    [https://arxiv.org/abs/2402.08983](https://arxiv.org/abs/2402.08983)

    本文提出了一种名为SafeDecoding的解决方案，通过安全感知解码策略来防御大型语言模型（LLMs）的越狱攻击。该策略可以生成对用户查询有益且无害的响应，有效缓解了LLMs安全性威胁。

    

    随着大型语言模型（LLMs）越来越多地应用于代码生成和聊天机器人辅助等现实应用中，人们为了使LLM的行为与人类价值观保持一致，包括安全性在内做出了大量努力。越狱攻击旨在引发LLM的非预期和不安全行为，仍然是LLM安全性的重要威胁。本文旨在通过引入SafeDecoding来防御LLM的越狱攻击，这是一种安全感知的解码策略，用于生成对用户查询有益且无害的响应。我们在开发SafeDecoding时的洞察力基于观察到，即使代表有害内容的标记的概率超过代表无害响应的标记的概率，安全免责声明仍然出现在按概率降序排序的标记中的前几个。这使我们能够通过识别安全免责声明并增强其良性影响力来减轻越狱攻击。

    arXiv:2402.08983v1 Announce Type: cross Abstract: As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat. In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their
    
[^188]: 可信的取样合理化通过半监督的蕴涵信号

    Plausible Extractive Rationalization through Semi-Supervised Entailment Signal

    [https://arxiv.org/abs/2402.08479](https://arxiv.org/abs/2402.08479)

    本文通过半监督方法，采用蕴涵对齐，以优化可行性，提取有理的方式提供一个可解释的替代模型

    

    复杂和不透明的黑盒子模型的增加需要采用可解释的措施，其中一种选择是提取有理的模型，它们作为更可解释的替代方案。这些模型，也称为先解释然后预测模型，使用解释模型来提取有理，然后使用提取的信息来调整预测模型。它们的主要目标是提供精确和忠实的解释，由提取的有理表示。在本文中，我们采用半监督方法来优化提取有理的可行性。我们采用一个预训练的自然语言推理（NLI）模型，并在一个小型的有监督有理集（10%）上进一步微调它。通过蕴涵对齐，NLI预测模型被利用作为解释模型的一种监督信号源。通过在问答任务中强制解释和答案之间的对齐一致，我们证明了性能得到了提升。

    The increasing use of complex and opaque black box models requires the adoption of interpretable measures, one such option is extractive rationalizing models, which serve as a more interpretable alternative. These models, also known as Explain-Then-Predict models, employ an explainer model to extract rationales and subsequently condition the predictor with the extracted information. Their primary objective is to provide precise and faithful explanations, represented by the extracted rationales. In this paper, we take a semi-supervised approach to optimize for the plausibility of extracted rationales. We adopt a pre-trained natural language inference (NLI) model and further fine-tune it on a small set of supervised rationales ($10\%$). The NLI predictor is leveraged as a source of supervisory signals to the explainer via entailment alignment. We show that, by enforcing the alignment agreement between the explanation and answer in a question-answering task, the performance can be improve
    
[^189]: 《童话中明确表达的社会价值观：三种欧洲文化的比较》

    Explicit References to Social Values in Fairy Tales: A Comparison between Three European Cultures

    [https://arxiv.org/abs/2402.08318](https://arxiv.org/abs/2402.08318)

    研究了葡萄牙、意大利和德国童话中明确表达的价值观差异，使用词嵌入技术和罗盘量化分析。初步发现表明这些国家之间存在共享的文化理解和对善良、遵从和普遍价值观的表达。

    

    研究童话中的社会价值观可以了解价值观在时空中的传递。我们提出使用词嵌入技术和罗盘来量化葡萄牙、意大利和德国童话中的价值观传递。我们研究这三种国家的童话在明确表达价值观方面的差异。为此，我们指定了一个充满价值观的词汇列表，考虑它们的词干，并分析在专门预训练的Word2Vec模型中它们之间的距离。我们通过多角度验证和批判性讨论量化模型所提出的假设的有效性。我们认为，这是一个可复用和可重现的方法来研究历史语料库中明确引用的价值观。最后，我们的初步发现暗示有着共享文化理解和对善良、遵从和普遍价值观的表达。

    The study of social values in fairy tales opens the possibility to learn about the communication of values across space and time. We propose to study the communication of values in fairy tales from Portugal, Italy and Germany using a technique called word embedding with a compass to quantify vocabulary differences and commonalities. We study how these three national traditions of fairy tales differ in their explicit references to values. To do this, we specify a list of value-charged tokens, consider their word stems and analyse the distance between these in a bespoke pre-trained Word2Vec model. We triangulate and critically discuss the validity of the resulting hypotheses emerging from this quantitative model. Our claim is that this is a reusable and reproducible method for the study of the values explicitly referenced in historical corpora. Finally, our preliminary findings hint at a shared cultural understanding and the expression of values such as Benevolence, Conformity, and Unive
    
[^190]: 朝着忠实和强大的基于证据的问答专家的方向前进

    Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering

    [https://arxiv.org/abs/2402.08277](https://arxiv.org/abs/2402.08277)

    这项工作探索了如何鲁棒地微调大型语言模型以提高答案的来源质量和答案归因能力，引入了数据生成流水线和四个测试集来评估模型的性能，并展示了在合成数据上微调可以改善内部和外部分布的性能。

    

    对大型语言模型（LLM）更忠实和可追踪的答案的进步对于各种研究和实践活动至关重要。其中一种达到这个目标的方法是基于可靠的来源提供答案。然而，这种基于证据的问答在使用LLM时已经证明在引用正确的来源（来源质量）和准确地表示来源中的信息（答案归因能力）方面工作不足。在这项工作中，我们系统地研究了如何鲁棒地微调LLM，以提高来源质量和答案归因能力。具体而言，我们引入了一个数据生成流水线，其中包括自动数据质量过滤器，可以大规模合成多样化的高质量训练和测试数据。我们还引入了四个测试集，以对微调后的专家模型的鲁棒性进行基准测试。广泛的评估结果表明，在合成数据上进行微调可以提高在内部和外部分布的性能。%基于证据的问答案例。此外，我们展示了用于评估的四个测试集，以评估微调后的专家模型的鲁棒性。

    Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we sho
    
[^191]: 使用语言反馈模型来改进政策

    Policy Improvement using Language Feedback Models

    [https://arxiv.org/abs/2402.07876](https://arxiv.org/abs/2402.07876)

    本文介绍了一种使用语言反馈模型（LFMs）改进政策的方法，通过识别期望的行为并进行模仿学习，我们在任务完成率、泛化性能和人类可解释性方面取得了显著改进。

    

    我们引入了语言反馈模型（LFMs），用于在指令遵循中识别期望的行为-有助于实现指令中指定任务的行动-以进行模仿学习。为了训练LFMs，我们从大型语言模型（LLMs）获取对视觉轨迹进行语言描述的反馈。首先，通过使用LFMs识别期望模仿的行为，我们在三种不同的语言基础环境（Touchdown，ScienceWorld和ALFWorld）上，在任务完成率上改善了强行为克隆的基线方法。其次，与LLMs直接预测行动相比，使用LFMs在LLM输出标记的数量相同的情况下表现更好。第三，LFMs适应未见环境，通过一轮适应使任务完成率提高了3.5-12.0％。最后，可以修改LFM以提供人类可解释的反馈，无需性能损失，从而允许人类验证模仿学习的期望行为。

    We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
    
[^192]: 关于可证明的长度和组合泛化

    On Provable Length and Compositional Generalization

    [https://arxiv.org/abs/2402.04875](https://arxiv.org/abs/2402.04875)

    本研究针对包括深度集合、变压器、状态空间模型和简单递归神经网络等多种架构，探索了可证明的长度和组合泛化，认为对于长度和组合泛化，不同架构需要不同程度的表示识别。

    

    长度泛化——对训练时未见到的更长序列的泛化能力，以及组合泛化——对训练时未见到的令牌组合的泛化能力，在序列到序列模型中是重要的非分布化泛化形式。在这项工作中，我们在包括深度集合、变压器、状态空间模型和简单递归神经网络在内的一系列架构中，朝着可证明的长度和组合泛化迈出了第一步。根据架构的不同，我们证明了不同程度的表示识别的必要性，例如与真实表示具有线性或排列关系。

    Length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models. In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets. Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.
    
[^193]: RevOrder：一种增强语言模型中算术运算的新方法

    RevOrder: A Novel Method for Enhanced Arithmetic in Language Models

    [https://arxiv.org/abs/2402.03822](https://arxiv.org/abs/2402.03822)

    本文提出了一种名为RevOrder的新方法，通过翻转加法、减法和nD乘以1D的输出数字，显著改善了语言模型中的算术运算。经过全面测试，RevOrder在基本算术运算中达到了完美准确度，并在除法任务中提升了语言模型性能，特别是在处理大数时。在GSM8K数学任务中应用RevOrder进行微调，有效降低了错误率并提高了总体得分。

    

    本文提出了RevOrder，一种旨在改善大型语言模型中算术运算的新技术。该方法通过翻转加法、减法和n位数乘以1位数（nD乘以1D）的输出数字，显著降低了顺序中间数字的数量 (CSID)，这是我们引入的一种评估方程复杂性的新度量。通过全面的测试，RevOrder不仅在基本的算术运算中达到了完美的准确度，而且在除法任务中显著提升了语言模型的性能，特别是在传统模型难以处理的大数情况下。RevOrder的实现对于训练和推理阶段都具有成本效益。此外，将RevOrder应用于对GSM8K数学任务进行微调的LLaMA2-7B模型中，取得了显著的改善，将方程计算错误率降低了46%，将总体得分从41.6提升到44.4。

    This paper presents RevOrder, a novel technique aimed at improving arithmetic operations in large language models (LLMs) by reversing the output digits in addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks. Our method significantly reduces the Count of Sequential Intermediate Digits (CSID) to $\mathcal{O}(1)$, a new metric we introduce to assess equation complexity. Through comprehensive testing, RevOrder not only achieves perfect accuracy in basic arithmetic operations but also substantially boosts LLM performance in division tasks, particularly with large numbers where traditional models struggle. Implementation of RevOrder is cost-effective for both training and inference phases. Moreover, applying RevOrder to fine-tune the LLaMA2-7B model on the GSM8K math task results in a considerable improvement, reducing equation calculation errors by 46% and increasing overall scores from 41.6 to 44.4.
    
[^194]: 揭示宣传：基于人类注释和机器分类的文体线索比较分析

    Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification

    [https://arxiv.org/abs/2402.03780](https://arxiv.org/abs/2402.03780)

    本文通过分析文体线索比较人类注释和机器分类的方法，揭示了宣传语言的特征，并提出了一个多源、多语言、多模态的数据集PPN。结果表明，人类注释者能够可靠地区分宣传新闻和常规新闻。研究还比较了不同的自然语言处理技术，并提供了一些有关文体线索的发现。

    

    本文研究了宣传语言及其文体特征。提出了PPN数据集，即宣传性伪新闻数据集，它是一种多源、多语言、多模态的数据集，由专家机构确定的宣传来源网站上的新闻文章组成。从该数据集中随机选择了一部分样本与来自常规法国新闻的文章混合，并对它们的URL进行了掩盖，以进行人类注释实验，使用11个不同的标签。结果显示，人类注释者能够可靠地区分两种类型的新闻纸对每个标签。我们提出了不同的自然语言处理技术来识别注释者使用的线索，并与机器分类进行比较。其中包括使用VAGO分析器进行辞述模糊和主观性的测量，使用TF-IDF作为基准，以及四种不同的分类器：两个基于RoBERTa模型的模型，使用句法的CATS，以及结合句法和语义特征的一个XGBoost模型。

    This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   K
    
[^195]: 在多模态大型语言模型中为图推理渲染图形

    Rendering Graphs for Graph Reasoning in Multimodal Large Language Models

    [https://arxiv.org/abs/2402.02130](https://arxiv.org/abs/2402.02130)

    本文在多模态大型语言模型中为图推理任务引入了视觉信息，并提出了一个新的基准测试数据集GITQA。实验证明，结合文本和视觉信息的结果比单一模态效果更好。

    

    大型语言模型(LLMs)在机器人规划、知识图谱补全和常识推理等任务中越来越多地使用图结构，LLMs能够理解文本格式的图信息，但忽视了丰富的视觉模态，而视觉是人类理解结构信息和进行图推理的直观方式。将图结构表示为视觉图像(即视觉图)的潜在益处和能力仍未被探索。本文在图推理任务中首次引入视觉信息，并提出一个新的基准测试数据集GITQA，其中每个样本是一个元组(图、图像、文本描述)。我们利用最先进的多模态LLMs在GITQA基准测试数据集上进行了大量实验证明，结合文本和视觉信息的结果比单一模态效果更好。此外，在LLaVA-7B/13B模型的微调上表现出色。

    Large Language Models (LLMs) are increasingly used for various tasks with graph structures, such as robotic planning, knowledge graph completion, and common-sense reasoning. Though LLMs can comprehend graph information in a textual format, they overlook the rich visual modality, which is an intuitive way for humans to comprehend structural information and conduct graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., visual graph) is still unexplored. In this paper, we take the first step in incorporating visual information into graph reasoning tasks and propose a new benchmark GITQA, where each sample is a tuple (graph, image, textual description). We conduct extensive experiments on the GITQA benchmark using state-of-the-art multimodal LLMs. Results on graph reasoning tasks show that combining textual and visual information together performs better than using one modality alone. Moreover, the LLaVA-7B/13B models finetuned on 
    
[^196]: 从大型语言模型中提取上下文信息用于知识图谱补全

    Contextualization Distillation from Large Language Model for Knowledge Graph Completion

    [https://arxiv.org/abs/2402.01729](https://arxiv.org/abs/2402.01729)

    本论文介绍了一种从大型语言模型中提取上下文信息用于知识图谱补全的策略，该策略能克服现有语料库的限制，显著提高了预训练语言模型在知识图谱补全中的性能。

    

    虽然文本信息显著提高了预训练语言模型（PLMs）在知识图谱补全（KGC）中的性能，但现有语料库从维基百科文章或同义词定义中收集的静态和噪声性质常常限制了基于PLM的KGC模型的潜力。为了克服这些挑战，我们提出了上下文化蒸馏策略，这是一种通用的可插入和可播放的方法，与判别和生成的KGC框架兼容。我们的方法首先指导大型语言模型（LLMs）将紧凑的结构化三元组转换为上下文丰富的段落。随后，我们引入了两个定制的辅助任务，重建和上下文化，使较小的KGC模型能够吸收这些丰富的三元组中的见解。对多种数据集和KGC技术的全面评估突出了我们方法的功效和适应性，揭示了无论基础管道如何，始终能提高性能。

    While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pip
    
[^197]: DetectGPT是否充分利用了扰动？基于模型对比学习的选择性扰动会更好

    Does \textsc{DetectGPT} Fully Utilize Perturbation? Selective Perturbation on Model-Based Contrastive Learning Detector would be Better

    [https://arxiv.org/abs/2402.00263](https://arxiv.org/abs/2402.00263)

    我们提出了一种新颖的检测器，模型名，它使用选择性策略扰动和多对比学习，在减少随机屏蔽引起的信息丢失的同时，进一步提升少样本和个体输入的性能。

    

    大型语言模型的不断发展引发了对其滥用的增长关注。DetectGPT是一种零-shot基于度量的无监督机器生成文本检测器，首次引入了扰动并展现了巨大的性能提升。然而，DetectGPT的随机扰动策略可能会引入噪声，限制了可区分性和进一步的性能提升。此外，它的逻辑回归模块依赖于设置阈值，这会影响个体或小批量输入的泛化性和适用性。因此，我们提出了一种新颖的检测器，模型名，它使用选择性策略扰动来缓解随机屏蔽所引起的重要信息丢失，并利用多对比学习捕捉扰动期间的隐含模式信息，便于少量样本的性能提升。实验结果表明，模型名在四个公共数据集上的平均准确率比SOTA方法高出1.20\%。我们进一步分析了...

    The burgeoning capabilities of large language models (LLMs) have raised growing concerns about abuse. DetectGPT, a zero-shot metric-based unsupervised machine-generated text detector, first introduces perturbation and shows great performance improvement. However, DetectGPT's random perturbation strategy might introduce noise, limiting the distinguishability and further performance improvements. Moreover, its logit regression module relies on setting the threshold, which harms the generalizability and applicability of individual or small-batch inputs. Hence, we propose a novel detector, \modelname{}, which uses selective strategy perturbation to relieve the important information loss caused by random masking, and multi-pair contrastive learning to capture the implicit pattern information during perturbation, facilitating few-shot performance. The experiments show that \modelname{} outperforms the SOTA method by 1.20\% in accuracy on average on four public datasets. We further analyze th
    
[^198]: 自由长思变压器（FraiLT）

    Freely Long-Thinking Transformer (FraiLT)

    [https://arxiv.org/abs/2401.11626](https://arxiv.org/abs/2401.11626)

    FraiLT是一个改进的变压器模型，通过递归方法和迭代编码，实现了在紧凑形式下达到较大模型的解释深度，在性能表现上优于较大模型，旨在实现更高效和可访问的语言模型。

    

    自由长思变压器（FraiLT）是一个改进的变压器模型，旨在增强处理能力而不增加规模。它采用递归方法，多次迭代子层，并引入迭代编码以在这些周期中保持意识。迭代编码使FraiLT能够以紧凑的形式实现较大模型的解释深度。在合成故事数据集上进行评估时，FraiLT优于较大的模型，展示了其在减少内存需求的同时提供高质量性能的能力。该模型代表了更高效和可访问的语言模型的一步。

    arXiv:2401.11626v2 Announce Type: replace-cross  Abstract: Freely Long-Thinking Transformer (FraiLT) is an improved transformer model designed to enhance processing capabilities without scaling up size. It utilizes a recursive approach, iterating over a subset of layers multiple times, and introduces iteration encodings to maintain awareness across these cycles. Iteration encoding allows FraiLT to achieve the interpretive depth of larger models in a compact form. When evaluated on a synthetic story dataset, FraiLT outperformed larger models, showcasing its ability to deliver high-quality performance while reducing memory demands. This model represents a step forward towards more efficient and accessible language models.
    
[^199]: MM-SAP：用于评估多模态大型语言模型自我意识在感知中的全面基准

    MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception

    [https://arxiv.org/abs/2401.07529](https://arxiv.org/abs/2401.07529)

    本论文提出了一个新的基准MM-SAP，旨在评估多模态大型语言模型在感知中的自我意识能力，填补了先前研究中忽视的领域。

    

    多模态大型语言模型（MLLMs）近期的进展展示了其在视觉感知和理解方面出色的能力。然而，这些模型也存在幻觉问题，这限制了它们作为人工智能系统的可靠性。我们认为这些幻觉部分原因在于模型在理解从图像中能够和不能够感知的内容方面存在困难，这种能力我们称之为感知中的自我意识。尽管重要性重大，在先前的研究中却忽视了MLLMs的这个方面。本文旨在定义和评估MLLMs在感知中的自我意识。为此，我们首先引入了感知中的知识象限，这有助于定义MLLMs对图像了解和不了解的内容。利用这一框架，我们提出了一项新颖的基准，即专门设计用于评估这种能力的MLLMs感知中的自我意识基准（MM-SAP）。我们将MM-SAP应用于各种情况

    arXiv:2401.07529v2 Announce Type: replace-cross  Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in visual perception and understanding. However, these models also suffer from hallucinations, which limit their reliability as AI systems. We believe that these hallucinations are partially due to the models' struggle with understanding what they can and cannot perceive from images, a capability we refer to as self-awareness in perception. Despite its importance, this aspect of MLLMs has been overlooked in prior studies. In this paper, we aim to define and evaluate the self-awareness of MLLMs in perception. To do this, we first introduce the knowledge quadrant in perception, which helps define what MLLMs know and do not know about images. Using this framework, we propose a novel benchmark, the Self-Awareness in Perception for MLLMs (MM-SAP), specifically designed to assess this capability. We apply MM-SAP to a variety of 
    
[^200]: MoE-Mamba: 混合专家模型的高效选择性状态空间模型

    MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts

    [https://arxiv.org/abs/2401.04081](https://arxiv.org/abs/2401.04081)

    结合混合专家模型的MoE-Mamba在性能上优于Mamba和基准Transformer-MoE，达到了与Mamba相同性能的同时，训练步骤减少了2.35倍。

    

    状态空间模型（SSMs）已经成为顺序建模领域的严肃竞争者，挑战了Transformer的主导地位。与此同时，混合专家（MoE）显著改进了基于Transformer的大型语言模型，包括最近的最先进开放模型。我们提出要发掘SSMs在扩展方面的潜力，它们应该与MoE相结合。我们在Mamba上展示了这一点，这是一个最近基于SSM的模型，取得了显著的性能。我们的模型MoE-Mamba在性能方面表现优异，优于Mamba和基准Transformer-MoE。特别地，MoE-Mamba在更少的训练步骤中达到与Mamba相同的性能，同时保持Mamba相对于Transformer的推理性能增益。

    arXiv:2401.04081v2 Announce Type: replace-cross  Abstract: State Space Models (SSMs) have become serious contenders in the field of sequential modeling, challenging the dominance of Transformers. At the same time, Mixture of Experts (MoE) has significantly improved Transformer-based Large Language Models, including recent state-of-the-art open models. We propose that to unlock the potential of SSMs for scaling, they should be combined with MoE. We showcase this on Mamba, a recent SSM-based model that achieves remarkable performance. Our model, MoE-Mamba, outperforms both Mamba and baseline Transformer-MoE. In particular, MoE-Mamba reaches the same performance as Mamba in $2.35\times$ fewer training steps while preserving the inference performance gains of Mamba against Transformer.
    
[^201]: 高效的标题重新排序器，用于快速和改进的知识密集型自然语言处理

    Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP

    [https://arxiv.org/abs/2312.12430](https://arxiv.org/abs/2312.12430)

    引入了通过广播查询编码器实现的高效标题重新排序器和为标题重新排序定制的Sigmoid Trick损失函数，相结合在KILT知识基准测试的数据集上取得了最先进的结果。

    

    在最近的RAG方法中，重新排序器在提升检索准确性方面发挥着关键作用，能够揭示每对查询和文本之间的逻辑关系。然而，现有的重新排序器需要反复对查询和大量长文本进行编码。这导致了较高的计算成本，并限制了检索文本的数量，从而影响了准确性。作为问题的解决方案，我们引入了通过广播查询编码器实现的高效标题重新排序器，这是一种用于标题重新排序的新技术，可以使速度提高20倍至40倍，超过基准通道重新排序器。此外，我们还引入了Sigmoid Trick，一种为标题重新排序定制的新损失函数。将这两种技术结合起来，我们在从KILT知识基准测试中实验的四个数据集上都经验验证了它们的有效性，实现了最先进的结果。

    arXiv:2312.12430v3 Announce Type: replace-cross  Abstract: In recent RAG approaches, rerankers play a pivotal role in refining retrieval accuracy with the ability of revealing logical relations for each pair of query and text. However, existing rerankers are required to repeatedly encode the query and a large number of long retrieved text. This results in high computational costs and limits the number of retrieved text, hindering accuracy. As a remedy of the problem, we introduce the Efficient Title Reranker via Broadcasting Query Encoder, a novel technique for title reranking that achieves a 20x-40x speedup over the vanilla passage reranker. Furthermore, we introduce Sigmoid Trick, a novel loss function customized for title reranking. Combining both techniques, we empirically validated their effectiveness, achieving state-of-the-art results on all four datasets we experimented with from the KILT knowledge benchmark.
    
[^202]: 混合蒸馏有助于较小的语言模型更好地推理

    Mixed Distillation Helps Smaller Language Model Better Reasoning

    [https://arxiv.org/abs/2312.10730](https://arxiv.org/abs/2312.10730)

    混合蒸馏(MD)框架结合了LLMs中的Program of Thought (PoT)和Chain of Thought (CoT)能力，将多种提示技术蒸馏到较小模型中，显著增强了较小模型在各种任务中的推理能力。

    

    虽然大型语言模型(LLMs)在最近的自然语言处理(NLP)任务中表现出了异常的性能，但由于在真实应用中的高计算和内存需求，它们的部署面临着重大挑战。最近的研究集中于通过从LLMs蒸馏知识来增强较小模型，在特定任务中取得了令人满意的结果。然而，这些模型在特别需要推理的任务中往往难以与LLMs的性能匹敌。在这项工作中，我们介绍了混合蒸馏(MD)框架，该框架利用了LLMs中的Program of Thought (PoT)和Chain of Thought (CoT)能力的优势，结合多种提示技术，并将这些能力蒸馏到较小的模型中。我们的实验结果表明，MD显著增强了较小模型在各种任务中的单路径和多路径推理能力。

    arXiv:2312.10730v2 Announce Type: replace-cross  Abstract: While large language models (LLMs) have demonstrated exceptional performance in recent natural language processing (NLP) tasks, their deployment poses substantial challenges due to high computational and memory demands in real-world applications. Recent studies have focused on enhancing smaller models through knowledge distillation from LLMs, yielding promising results. However, these models often struggle to match the performance of LLMs, especially in tasks that require reasoning. In this work, we introduce Mixed Distillation (MD) framework, which capitalizes on the strengths of Program of Thought (PoT) and Chain of Thought (CoT) capabilities within LLMs, combining multiple prompting techniques and distilling these capabilities into smaller models. Our experimental results show that MD significantly enhances the single-path and multi-path reasoning ability of smaller models in various tasks. In terms of accuracy and generalit
    
[^203]: ULMA：人类演示和逐点偏好统一语言模型对齐

    ULMA: Unified Language Model Alignment with Human Demonstration and Point-wise Preference

    [https://arxiv.org/abs/2312.02554](https://arxiv.org/abs/2312.02554)

    提出了一种逐点直接偏好优化方法，用于统一语言模型对齐，通过将人类演示和逐点偏好相结合，解决了偏好学习中存在的信息丢失和性能次优问题。

    

    将语言模型与人类期望（例如，有益和无害）对齐已成为大型语言模型的迫切挑战。典型的对齐过程包括监督微调和偏好学习。大多数偏好学习方法（如RLHF和DPO）依赖于成对偏好数据，这并不充分地解决人类反馈是逐点的情况，导致潜在信息丢失和性能次优。为了解决这一问题，我们引入了逐点直接偏好优化，一种旨在有效利用逐点反馈的新颖偏好学习方法。我们的工作还揭示了监督微调和逐点偏好学习之间的新颖联系，最终形成了统一语言模型对齐，这是一种将对齐与人类演示和逐点偏好统一的单步方法。在逐点偏好数据集上进行了大量实验。

    arXiv:2312.02554v2 Announce Type: replace-cross  Abstract: Aligning language models to human expectations, e.g., being helpful and harmless, has become a pressing challenge for large language models. A typical alignment procedure consists of supervised fine-tuning and preference learning. Most preference learning methods, such as RLHF and DPO, depend on pairwise preference data, which inadequately address scenarios where human feedback is point-wise, leading to potential information loss and suboptimal performance. Addressing this gap, we introduce Point-wise Direct Preference Optimization, a novel preference learning method designed to harness point-wise feedback effectively. Our work also uncovers a novel connection between supervised fine-tuning and point-wise preference learning, culminating in Unified Language Model Alignment, a single-step method that unifies the alignment with human demonstrations and point-wise preferences. Extensive experiments on point-wise preference dataset
    
[^204]: 一种基于机器学习的SKILL代码自动补全方法

    A Machine Learning Approach Towards SKILL Code Autocompletion

    [https://arxiv.org/abs/2312.01921](https://arxiv.org/abs/2312.01921)

    本研究首次将变压器应用于SKILL代码自动补全，提出了一种新型的数据高效生成SKILL代码的方法，旨在提高硬件设计工程师的生产力

    

    随着摩尔定律继续增加电子系统的复杂性，电子设计自动化（EDA）必须不断提升以满足全球需求。EDA技术中一个重要的例子是SKILL，一种用于定制和扩展EDA软件的脚本语言。最近，在学术界使用变压器架构的代码生成模型取得了令人印象深刻的成果，甚至已经在商业开发工具中使用以提高开发者的生产力。据我们所知，本研究是首次将变压器应用于SKILL代码自动补全，旨在提高硬件设计工程师的生产力。本研究提出并实验验证了一种新型的数据高效生成SKILL代码的方法。更具体地说，我们提出了一种新颖的方法（i）创建具有未标记和标记数据的高质量SKILL数据集，（ii）一个训练策略，可以在T5模型之前进行训练

    arXiv:2312.01921v2 Announce Type: replace-cross  Abstract: As Moore's Law continues to increase the complexity of electronic systems, Electronic Design Automation (EDA) must advance to meet global demand. An important example of an EDA technology is SKILL, a scripting language used to customize and extend EDA software. Recently, code generation models using the transformer architecture have achieved impressive results in academic settings and have even been used in commercial developer tools to improve developer productivity. To the best of our knowledge, this study is the first to apply transformers to SKILL code autocompletion towards improving the productivity of hardware design engineers. In this study, a novel, data-efficient methodology for generating SKILL code is proposed and experimentally validated. More specifically, we propose a novel methodology for (i) creating a high-quality SKILL dataset with both unlabeled and labeled data, (ii) a training strategy where T5 models pre-
    
[^205]: 用于盖兹语的机器翻译

    Machine Translation for Ge'ez Language

    [https://arxiv.org/abs/2311.14530](https://arxiv.org/abs/2311.14530)

    该研究探讨了改善盖兹语机器翻译的方法，包括从相关语言进行迁移学习、优化共享词汇和标记分割方法、大型预训练模型的微调，以及在少样本情况下使用大型语言模型进行翻译，其中一种基于语言相关性的多语言神经机器翻译模型相比标准双语模型有4 BLEU的平均性能提升。

    

    arXiv:2311.14530v2 公告类型: 替换 摘要: 机器翻译(MT)用于低资源语言，如盖兹语，一种古老的语言，不再是任何社区的母语，面临诸如词汇外生、领域不匹配和缺乏足够标记的训练数据等挑战。在这项工作中，我们探讨了改善盖兹语MT的各种方法，包括从相关语言进行迁移学习、优化共享词汇和标记分割方法、微调大型预训练模型，以及使用大型语言模型(LLMs)进行少样本翻译配合模糊匹配。我们基于语言相关性开发了一种多语言神经机器翻译(MNMT)模型，使标准双语模型的平均性能提高约4 BLEU。我们还尝试微调NLLB-200模型，这是当今可用的最先进的翻译模型之一，但发现只有4k训练样本的盖兹语表现不佳。

    arXiv:2311.14530v2 Announce Type: replace  Abstract: Machine translation (MT) for low-resource languages such as Ge'ez, an ancient language that is no longer the native language of any community, faces challenges such as out-of-vocabulary words, domain mismatches, and lack of sufficient labeled training data. In this work, we explore various methods to improve Ge'ez MT, including transfer-learning from related languages, optimizing shared vocabulary and token segmentation approaches, finetuning large pre-trained models, and using large language models (LLMs) for few-shot translation with fuzzy matches. We develop a multilingual neural machine translation (MNMT) model based on languages relatedness, which brings an average performance improvement of about 4 BLEU compared to standard bilingual models. We also attempt to finetune the NLLB-200 model, one of the most advanced translation models available today, but find that it performs poorly with only 4k training samples for Ge'ez. Furthe
    
[^206]: 当前大型语言模型在各种医学专业中的应用概览

    Overview of Current Applications of Large Language Models in Various Medical Specialities

    [https://arxiv.org/abs/2311.12882](https://arxiv.org/abs/2311.12882)

    大型语言模型在医疗领域的应用概述，突出了它们在医疗质量提升中的变革性作用，重点关注了诊断和治疗领域，以及在癌症护理、皮肤科、牙科和心理健康等方面的创新应用。

    

    我们旨在概述大型语言模型（LLMs）在医疗领域最新应用，突出它们在提升医疗质量方面的变革性作用。通过处理来自不同医学领域的大量数据，LLMs已成为协助医生、医疗提供者和患者的关键工具。我们重点审视了LLMs在医疗领域的应用，主要集中在诊断和治疗相关应用上。我们强调了LLMs在癌症护理、皮肤科、牙科和心理健康领域的应用，强调了它们为医学诊断和患者护理带来的创新。分析涵盖了将LLMs整合到医疗领域中所面临的挑战和机遇，指出了尽管存在当前局限，但它们在各种医学专业中的潜力。此外，我们还概述了在医学领域处理各种数据类型的情况。

    arXiv:2311.12882v2 Announce Type: replace  Abstract: We aim to provide an overview of the latest applications of Large Language Models (LLMs) in the healthcare sector, highlighting their transformative role in enhancing medical care quality. By processing vast amounts of data from diverse medical domains, LLMs have become pivotal in assisting doctors, healthcare providers, and patients. We review the application of Large Language Models (LLMs) in healthcare, focusing on diagnostics and treatment related applications. We highlight the use of LLMs in cancer care, dermatology, dental, and mental health, emphasizing the innovation they bring to medical diagnostics and patient care. The analysis addresses the challenges and opportunities of integrating LLMs in healthcare, noting their potential in various medical specialties despite current limitations. Further, we provide an overview of handling various data types in the medical field.
    
[^207]: 在长上下文大语言模型中推进Transformer架构：一项全面调查

    Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey

    [https://arxiv.org/abs/2311.12351](https://arxiv.org/abs/2311.12351)

    本论文对基于Transformer的大型语言模型架构的最新进展进行了全面调查，旨在增强其处理长上下文能力，从预训练到推断过程中进行了分类和分析。

    

    基于Transformer的大型语言模型（LLMs）已应用于知识库、人机界面和动态代理等多个领域，标志着迈向达到人工通用智能(AGI)的一大步。然而，当前的LLMs主要是在短文本片段上进行预训练，这危及了它们在处理在实际场景中频繁遇到的长上下文提示时的有效性。本文对最近在旨在增强LLMs长上下文能力的基于Transformer的LLM架构的进展进行了全面调查，涵盖了整个模型生命周期，从预训练到推断。首先，我们阐述并分析了当前基于Transformer模型处理长上下文输入和输出的问题。然后，我们提供了一个解决这些问题的Transformer架构升级的分类和景观。随后，我们进行了一项调查

    arXiv:2311.12351v2 Announce Type: replace  Abstract: Transformer-based Large Language Models (LLMs) have been applied in diverse areas such as knowledge bases, human interfaces, and dynamic agents, and marking a stride towards achieving Artificial General Intelligence (AGI). However, current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios. This article offers a comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference. We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. We then provide a taxonomy and the landscape of upgrades on Transformer architecture to solve these problems. Afterwards, we provide an investi
    
[^208]: 通过实时验证和纠正减轻大型语言模型中的虚构问题

    Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification

    [https://arxiv.org/abs/2311.09114](https://arxiv.org/abs/2311.09114)

    通过实时验证和纠正的策略，文章提出了一种名为Ever的方法，用于减轻大型语言模型生成中的虚构问题。

    

    大型语言模型(LLMs)在生成流畅文本方面表现出色。然而，它们经常遇到生成不准确或虚构内容的挑战。这个问题普遍存在于非基于检索的生成和检索增强生成方法中，现有的事后纠正方法可能无法解决“滚雪球”问题导致的累积虚构错误，特别是在推理任务中。为了解决这些挑战，我们提出了一种名为“Ever”的新方法。Ever采用实时、逐步的生成和虚构纠正策略，而不是等到生成过程结束才纠正虚构。其主要目标是在文本生成过程中检测和纠正虚构。与基于检索和非基于检索的基线模型相比，

    arXiv:2311.09114v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content. This issue is common in both non-retrieval-based generation and retrieval-augmented generation approaches, and existing post-hoc rectification methods may not address the accumulated hallucination errors that may be caused by the "snowballing" issue, especially in reasoning tasks. To tackle these challenges, we introduce a novel approach called Real-time Verification and Rectification (Ever). Instead of waiting until the end of the generation process to rectify hallucinations, Ever employs a real-time, step-wise generation and hallucination rectification strategy. The primary objective is to detect and rectify hallucinations as they occur during the text generation process. When compared to both retrieval-based and non-retrieval-based basel
    
[^209]: “我们要求公正！”：政治文本的社会背景奠基

    "We Demand Justice!": Towards Social Context Grounding of Political Texts

    [https://arxiv.org/abs/2311.09106](https://arxiv.org/abs/2311.09106)

    该论文提出了定义计算环境中理解政治文本中模棱两可陈述所需背景的框架，并提出了挑战性的数据集，以此来分析和预测文本的真实世界背景。

    

    社交媒体话语经常包括“看似相似的语言被政治光谱两端使用”，往往转化为截然不同的观点。例如，“思念与祈祷”可能表达对大规模枪击受害者的同情，也可能批评对该问题缺乏立法行动。本文在计算环境中定义了完全理解此类模棱两可陈述所需的背景，并使其基于现实世界的实体、行动和态度。我们提出了两个需要理解文本实际背景的具有挑战性的数据集。我们将这些数据集与基于大型预训练模型（如RoBERTa和GPT-3）构建的模型进行了基准测试。此外，我们开发并对现有的“议语情境化框架”和“政治角色表征”模型进行了基准测试。我们分析了数据集和预测以获得更多洞见。

    arXiv:2311.09106v2 Announce Type: replace  Abstract: Social media discourse frequently consists of 'seemingly similar language used by opposing sides of the political spectrum', often translating to starkly contrasting perspectives. E.g., 'thoughts and prayers', could express sympathy for mass-shooting victims, or criticize the lack of legislative action on the issue. This paper defines the context required to fully understand such ambiguous statements in a computational setting and ground them in real-world entities, actions, and attitudes. We propose two challenging datasets that require an understanding of the real-world context of the text. We benchmark these datasets against models built upon large pre-trained models, such as RoBERTa and GPT-3. Additionally, we develop and benchmark more structured models building upon existing Discourse Contextualization Framework and Political Actor Representation models. We analyze the datasets and the predictions to obtain further insights int
    
[^210]: 朝向多步推理的答案校准统一视图

    Towards A Unified View of Answer Calibration for Multi-Step Reasoning

    [https://arxiv.org/abs/2311.09101](https://arxiv.org/abs/2311.09101)

    本文总结了最近答案校准技术的分类法，从统一视角对步级和路径级答案校准进行了彻底评估，结果显示整合两种策略的优势倾向于产生最佳结果。

    

    大型语言模型（LLMs）使用“思维链”提示扩展了改进多步推理能力的范围。我们通常将多步推理分为两个阶段：路径生成以生成推理路径；和答案校准后处理推理路径以获得最终答案。然而，现有文献缺乏对不同答案校准方法的系统分析。本文总结了最近答案校准技术的分类法，并将其分解为步级和路径级策略。然后，我们从统一视角对这些策略进行了彻底评估，系统地审查了多路径上的步级和路径级答案校准。实验结果表明，整合两种策略的优势倾向于产生最佳结果。我们的研究有可能启示优化多步推理系统的关键见解。

    arXiv:2311.09101v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have broadened the scope for improving multi-step reasoning capabilities. We generally divide multi-step reasoning into two phases: path generation to generate the reasoning path(s); and answer calibration post-processing the reasoning path(s) to obtain a final answer. However, the existing literature lacks systematic analysis on different answer calibration approaches. In this paper, we summarize the taxonomy of recent answer calibration techniques and break them down into step-level and path-level strategies. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Experimental results reveal that integrating the dominance of both strategies tends to derive optimal outcomes. Our study holds the potential to illuminate key insights for opti
    
[^211]: 基于Transformer的描述逻辑语境推理

    Reasoning over Description Logic-based Contexts with Transformers

    [https://arxiv.org/abs/2311.08941](https://arxiv.org/abs/2311.08941)

    本研究构建了一个由描述逻辑知识库生成的合成自然语言问答数据集，以评估基于Transformer模型在丰富语境中的推理能力。

    

    目前，衡量基于Transformer模型的推理能力的一种方式是通过评估在自然语言表达的合成语境中对逻辑问题回答或证明生成等下游任务的准确性。然而，大多数实际使用的语境非常简单；在大多数情况下，它们是由仅含有少量逻辑运算符和量词的短一阶逻辑句子生成的。本文旨在回答基于Transformer模型能够在表达丰富语境中执行推理的问题。为此，我们构建了一个由描述逻辑知识库生成的合成自然语言问答数据集。为生成知识库，我们使用了表达式语言$\mathcal{ALCQ$。生成的数据集包含384K个示例，并且在两个维度上增加：i) 推理深度，和ii) 句子长度。

    arXiv:2311.08941v2 Announce Type: replace-cross  Abstract: One way that the current state of the art measures the reasoning ability of transformer-based models is by evaluating accuracy in downstream tasks like logical question answering or proof generation over synthetic contexts expressed in natural language. However, most of the contexts used are in practice very simple; in most cases, they are generated from short first-order logic sentences with only a few logical operators and quantifiers. In this work, we seek to answer the question how well a transformer-based model will perform reasoning over expressive contexts. For this purpose, we construct a synthetic natural language question-answering dataset, generated by description logic knowledge bases. For the generation of the knowledge bases, we use the expressive language $\mathcal{ALCQ}$. The resulting dataset contains 384K examples, and increases in two dimensions: i) reasoning depth, and ii) length of sentences. We show that t
    
[^212]: 作者归属模型能否区分演讲文本中的发言人？

    Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?

    [https://arxiv.org/abs/2311.07564](https://arxiv.org/abs/2311.07564)

    本文研究了作者归属模型在演讲文本中区分发言人的能力，并提出了以会话演讲文本为重点的发言人归属基准。

    

    作者归属验证是确定两个不同书面样本是否同属一作者的任务，通常涉及对书面文本的归因。本文探讨了转录演讲的归属问题，这带来了新的挑战。其中一个主要挑战是，许多文体特征，如标点和大写，在这种情境下并不具备信息量。另一方面，转录的演讲呈现其他模式，如填充词和回应性声音（例如“嗯”，“嗯，嗯”），这些可能是不同发言人的特征性表现。我们提出了一个新的以会话演讲文本为重点的发言人归属基准。为了限制发言人与话题之间的虚假关联，我们使用会话提示和参与同一对话的发言人构建不同难度的验证试验。通过比较一系列方法，在这一新基准上建立了最新技术水平。

    arXiv:2311.07564v2 Announce Type: replace  Abstract: Authorship verification is the task of determining if two distinct writing samples share the same author and is typically concerned with the attribution of written text. In this paper, we explore the attribution of transcribed speech, which poses novel challenges. The main challenge is that many stylistic features, such as punctuation and capitalization, are not informative in this setting. On the other hand, transcribed speech exhibits other patterns, such as filler words and backchannels (e.g., 'um', 'uh-huh'), which may be characteristic of different speakers. We propose a new benchmark for speaker attribution focused on conversational speech transcripts. To limit spurious associations of speakers with topic, we employ both conversation prompts and speakers participating in the same conversation to construct verification trials of varying difficulties. We establish the state of the art on this new benchmark by comparing a suite of
    
[^213]: 学习的形状：基于Transformer模型的各向异性和内在维度研究

    The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models

    [https://arxiv.org/abs/2311.05928](https://arxiv.org/abs/2311.05928)

    本研究揭示了Transformer解码器中的各向异性呈钟状曲线，最高各向异性浓度在中间层，与编码器中更均匀分布的各向异性不同，并发现嵌入的内在维度在训练初期增加，随后在训练末期出现压缩，表明更紧凑的表示形式。

    

    在这项研究中，我们针对Transformer架构中嵌入的各向异性动态和内在维度展开调查，重点关注编码器和解码器之间的二分法。我们的研究结果显示，Transformer解码器中的各向异性配置呈现出明显的钟状曲线，具有最高的各向异性浓度在中间层。这种模式与编码器中观察到的更均匀分布的各向异性有所不同。此外，我们发现嵌入的内在维度在训练的初始阶段增加，表明向更高维空间的扩展。然后在训练末尾出现向更低维度的压缩阶段，暗示着对更紧凑表示的改进。我们的结果为理解编码器和解码器嵌入属性提供了新的见解。

    arXiv:2311.05928v2 Announce Type: replace-cross  Abstract: In this study, we present an investigation into the anisotropy dynamics and intrinsic dimension of embeddings in transformer architectures, focusing on the dichotomy between encoders and decoders. Our findings reveal that the anisotropy profile in transformer decoders exhibits a distinct bell-shaped curve, with the highest anisotropy concentrations in the middle layers. This pattern diverges from the more uniformly distributed anisotropy observed in encoders. In addition, we found that the intrinsic dimension of embeddings increases in the initial phases of training, indicating an expansion into higher-dimensional space. Which is then followed by a compression phase towards the end of training with dimensionality decrease, suggesting a refinement into more compact representations. Our results provide fresh insights to the understanding of encoders and decoders embedding properties.
    
[^214]: 无调谐的基础模型对象命名

    Tuning-less Object Naming with a Foundation Model

    [https://arxiv.org/abs/2311.04924](https://arxiv.org/abs/2311.04924)

    使用transformers的注意力机制，提出了一种无需微调模型即可进行对象命名的方法

    

    我们实现了一个实时对象命名系统，可以学习一组从未见过的命名实体。我们的方法采用了一个现有的基础模型，在开始之前我们认为它准备好接受任何内容。它将观察到的图像转换为相对较小的特征向量，我们将这些特征向量与逐渐构建的词汇表中的索引相关联，且无需对模型进行任何微调。我们的贡献在于使用了来自transformers注意力机制的关联机制。它具有支持从不相关信息中泛化以区分实体并潜在地能够与远超出词汇表索引的实体相关联的特性。因此，该系统可以以一次性方式工作，并正确地为不同上下文中命名的对象命名。我们还概述了通过黑板架构集成的系统模块的实现细节。最后，我们调查了系统的质量，主要着眼于它能够识别多少对象

    arXiv:2311.04924v2 Announce Type: replace-cross  Abstract: We implement a real-time object naming system that enables learning a set of named entities never seen. Our approach employs an existing foundation model that we consider ready to see anything before starting. It turns seen images into relatively small feature vectors that we associate with index to a gradually built vocabulary without any training of fine-tuning of the model. Our contribution is using the association mechanism known from transformers as attention. It has features that support generalization from irrelevant information for distinguishing the entities and potentially enable associating with much more than indices to vocabulary. As a result, the system can work in a one-shot manner and correctly name objects named in different contents. We also outline implementation details of the system modules integrated by a blackboard architecture. Finally, we investigate the system's quality, mainly how many objects it can 
    
[^215]: 学习学习以进行少样本持久主动学习

    Learning to Learn for Few-shot Continual Active Learning

    [https://arxiv.org/abs/2311.03732](https://arxiv.org/abs/2311.03732)

    提出了一种简单而高效的方法，Meta-Continual Active Learning，通过元学习和经验重播解决少样本持续主动学习中的任务混淆和灾难性遗忘，进一步结合文本增强来确保泛化。

    

    持续学习旨在确保解决先前见过的任务的稳定性，同时展示对新领域的可塑性。最近在持续学习方面的进展主要局限于监督学习设置，尤其是在自然语言处理领域。在这项工作中，我们考虑了一种少样本持续主动学习（CAL）设置，其中标记数据不足，但未标记数据充足，但有限的注释预算。我们提出了一种简单而高效的方法，称为元持续主动学习。具体地，我们采用元学习和经验重播来解决任务之间的混淆和灾难性遗忘。我们进一步结合文本增强来确保泛化性能。我们在基准文本分类数据集上进行了大量实验，以验证所提方法的有效性，并分析了在少样本CAL设置中不同主动学习策略的影响。我们的实验结果表明

    arXiv:2311.03732v2 Announce Type: replace-cross  Abstract: Continual learning strives to ensure stability in solving previously seen tasks while demonstrating plasticity in a novel domain. Recent advances in CL are mostly confined to a supervised learning setting, especially in NLP domain. In this work, we consider a few-shot continual active learning (CAL) setting where labeled data are inadequate, and unlabeled data are abundant but with a limited annotation budget. We propose a simple but efficient method, called Meta-Continual Active Learning. Specifically, we employ meta-learning and experience replay to address inter-task confusion and catastrophic forgetting. We further incorporate textual augmentations to ensure generalization. We conduct extensive experiments on benchmark text classification datasets to validate the effectiveness of the proposed method and analyze the effect of different active learning strategies in few-shot CAL setting. Our experimental results demonstrate t
    
[^216]: ITEm：面向电子商务的无监督图片文本嵌入学习

    ITEm: Unsupervised Image-Text Embedding Learning for eCommerce

    [https://arxiv.org/abs/2311.02084](https://arxiv.org/abs/2311.02084)

    ITEm是面向电子商务的无监督学习模型，通过学习文本和图片的嵌入来更好地关注不同模态，扩展了BERT，并在两个任务上取得了良好的表现。

    

    产品嵌入是电子商务中广泛应用的基石。通过从多种模态学习的产品嵌入显示出明显的改进，因为不同的模态提供了互补信息。然而，一些模态比其他模态更具信息优势。如何教导模型从不同模态学习嵌入而不忽视较不显著模态的信息是具有挑战性的。我们提出了一个图片文本嵌入模型（ITEm），这是一种设计用来更好地关注图片和文本模态的无监督学习方法。我们通过（1）学习文本和图片的嵌入而不知道感兴趣的区域；（2）训练一个全局表示来预测掩码单词并构建掩码图像补丁而不对它们的单独表示进行过程来扩展BERT。我们在两个任务上评估了预训练的ITEm：寻找扩展。

    arXiv:2311.02084v2 Announce Type: replace-cross  Abstract: Product embedding serves as a cornerstone for a wide range of applications in eCommerce. The product embedding learned from multiple modalities shows significant improvement over that from a single modality, since different modalities provide complementary information. However, some modalities are more informatively dominant than others. How to teach a model to learn embedding from different modalities without neglecting information from the less dominant modality is challenging. We present an image-text embedding model (ITEm), an unsupervised learning method that is designed to better attend to image and text modalities. We extend BERT by (1) learning an embedding from text and image without knowing the regions of interest; (2) training a global representation to predict masked words and to construct masked image patches without their individual representations. We evaluate the pre-trained ITEm on two tasks: the search for ext
    
[^217]: 关于利用大型语言模型进行双语词汇识别

    On Bilingual Lexicon Induction with Large Language Models

    [https://arxiv.org/abs/2310.13995](https://arxiv.org/abs/2310.13995)

    本文研究了利用大型语言模型进行双语词汇识别的潜力，通过研究零次提示和少量上下文提示等方法，探讨了这种方法如何与当前BLI方法相比，并如何进行补充。

    

    双语词汇识别（BLI）是多语言自然语言处理中的核心任务，目前在很大程度上仍然依赖于计算跨语言单词表示。受自然语言处理领域向大型语言模型（LLMs）的全球范式转变的启发，我们探讨了最新一代LLMs在双语词汇开发中的潜力。我们提出了以下研究问题：是否可能促使和微调多语言LLMs（mLLMs）以进行BLI，并且这种方法与当前BLI方法相比如何以及如何补充？为此，我们系统地研究了1）用于无监督BLI的零次提示和2）使用一组种子翻译对进行少量上下文提示，均无需进行任何LLM微调，以及3）对较小LLMs进行标准BLI导向微调。我们在涵盖不同大小（从0.3B到13B参数）的18个开源文本对文本mLLMs上进行实验，涵盖两个标准BLI基准测试。

    arXiv:2310.13995v2 Announce Type: replace-cross  Abstract: Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against and complement current BLI approaches? To this end, we systematically study 1) zero-shot prompting for unsupervised BLI and 2) few-shot in-context prompting with a set of seed translation pairs, both without any LLM fine-tuning, as well as 3) standard BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two standard BLI benchmarks covering a rang
    
[^218]: 质量感知翻译模型：单一模型中的高效生成和质量评估

    Quality-Aware Translation Models: Efficient Generation and Quality Estimation in a Single Model

    [https://arxiv.org/abs/2310.06707](https://arxiv.org/abs/2310.06707)

    提出了一种质量感知翻译模型，通过训练NMT模型来估计其输出质量，可以在解码过程中消除额外的计算成本。

    

    最大后验（MAP）解码是神经机器翻译（NMT）模型中最广泛使用的解码策略。 研究表明，模型概率与人类判断相关，但不能总是成立，生成质量可以通过解码来优化一个以度量或质量评估信号支持的效用函数来提高，即最小贝叶斯风险（MBR）或质量感知解码。 这些方法的主要缺点在于它们需要一个额外的模型在解码过程中计算效用函数，会显著增加计算成本。 本文提出通过训练NMT模型自己来估计其输出质量，从而使NMT模型本身具备质量感知能力。 使用这种方法进行MBR解码可以显著减小尺寸。

    arXiv:2310.06707v2 Announce Type: replace-cross  Abstract: Maximum-a-posteriori (MAP) decoding is the most widely used decoding strategy for neural machine translation (NMT) models. The underlying assumption is that model probability correlates well with human judgment, with better translations getting assigned a higher score by the model. However, research has shown that this assumption does not always hold, and generation quality can be improved by decoding to optimize a utility function backed by a metric or quality-estimation signal, as is done by Minimum Bayes Risk (MBR) or Quality-Aware decoding. The main disadvantage of these approaches is that they require an additional model to calculate the utility function during decoding, significantly increasing the computational cost. In this paper, we propose to make the NMT models themselves quality-aware by training them to estimate the quality of their own output. Using this approach for MBR decoding we can drastically reduce the size
    
[^219]: 让循环的询问: 大型语言模型在判断中的摇摆

    Ask Again, Then Fail: Large Language Models' Vacillations in Judgement

    [https://arxiv.org/abs/2310.02174](https://arxiv.org/abs/2310.02174)

    目前的语言模型在面对后续问题时常常摇摆不定，研究者提出了一个后续问题机制和两个度量标准来量化这种不一致性，并开发出Unwavering-FQ框架来教导模型保持最初的正确判断，实验证明其有效性。

    

    我们观察到目前的会话式语言模型在面对后续问题时往往在其判断上摇摆不定，即使原始判断是正确的。这种摇摆对于生成可靠回复和建立用户信任构成了重要挑战。为了全面评估这一问题，我们引入了一个后续问题机制以及两个度量标准来量化这种不一致性，确认了当前语言模型普遍存在这种情况。为了缓解这一问题，我们探讨了各种提示策略用于闭源模型；此外，我们开发了一个基于训练的框架Unwavering-FQ，通过合成高质量的偏好数据来教导语言模型保持其最初的正确判断。我们的实验结果验证了我们框架的有效性以及其增强模型通用能力的能力。

    arXiv:2310.02174v2 Announce Type: replace-cross  Abstract: We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework Unwavering-FQ that teaches language models to maintain their originally correct judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models (https://github.com/NUSTM/LLMs-Waver-In-Judgements).
    
[^220]: 探索LLM代理的协作机制：社会心理学视角

    Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View

    [https://arxiv.org/abs/2310.02124](https://arxiv.org/abs/2310.02124)

    通过实践实验和理论洞察，探究当代NLP系统之间的协作机制，发现某些协作策略优于先前的方法，并且优化了效率。

    

    随着自然语言处理（NLP）系统越来越多地应用于复杂的社会环境中，一个迫切的问题出现了：这些NLP系统能否模仿类人类的协作智能，在由多个大型语言模型（LLMs）组成的多代理社会中？本文通过将实践实验与理论观点相结合，探究当代NLP系统之间的协作机制。我们构建了四个由LLM代理组成的独特“社会”，每个代理以特定的“特质”（随和或过于自信）为特征，并与不同的“思维模式”（辩论或反思）展开协作。通过在三个基准数据集上评估这些多代理社会，我们发现某些协作策略不仅胜过先前顶尖方法，而且优化了效率（使用更少的API令牌）。此外，我们的结果进一步说明LLM代理可以

    arXiv:2310.02124v2 Announce Type: replace-cross  Abstract: As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents mani
    
[^221]: 在图上推理：忠实且可解释的大型语言模型推理

    Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning

    [https://arxiv.org/abs/2310.01061](https://arxiv.org/abs/2310.01061)

    提出了一种名为图推理（RoG）的新方法，通过将LLMs与KGs协同工作，实现忠实且可解释的大型语言模型推理。

    

    大型语言模型（LLMs）在复杂任务中展示了令人印象深刻的推理能力。然而，在推理过程中它们缺乏最新知识，经历幻觉，这可能导致不正确的推理过程，并降低其性能和可信度。知识图（KGs）以结构化格式捕获了大量事实，为推理提供了可靠的知识来源。然而，现有基于KG的LLM推理方法只将KGs视为事实知识库，忽视其结构信息对推理的重要性。本文提出了一种称为图推理（RoG）的新方法，通过使LLMs与KGs协同工作，实现忠实且可解释的推理。具体而言，我们提出了一个规划-检索-推理的框架，其中RoG首先生成由KGs作为忠实计划的关系路径。这些计划然后用于检索有效的推理过程。

    arXiv:2310.01061v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning p
    
[^222]: 大型语言模型的盲点：超叙事语言信息

    A blind spot for large language models: Supradiegetic linguistic information

    [https://arxiv.org/abs/2306.06794](https://arxiv.org/abs/2306.06794)

    大型语言模型的盲点在于其对超叙事语言信息的忽视，研究提出考虑模型如何感知语言信息有助于深入了解其能力。

    

    像ChatGPT这样的大型语言模型(LLMs)反映了人工智能领域的深刻变革，实现了令人印象深刻甚至令人震惊的类人语言流利度。它们目前和潜在的能力范围是一个积极探讨的领域，绝非仅限于科研人员。人们通常将LLMs的训练数据框定为“文本”甚至“语言”。我们使用来自语言学、体现认知、认知科学、数学和历史等领域的思想，仔细审视这一框架的细节。我们提出，考虑像ChatGPT这样的LLM是什么感觉，正如纳格尔可能会说的那样，可以帮助我们深入了解其整体能力，特别是，其接受的语言训练数据可以被有益地重新构思为对语言中编码的叙事信息的接触，其缺陷可以被重新构思为对这些信息的无知。

    arXiv:2306.06794v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) like ChatGPT reflect profound changes in the field of Artificial Intelligence, achieving a linguistic fluency that is impressively, even shockingly, human-like. The extent of their current and potential capabilities is an active area of investigation by no means limited to scientific researchers. It is common for people to frame the training data for LLMs as "text" or even "language". We examine the details of this framing using ideas from several areas, including linguistics, embodied cognition, cognitive science, mathematics, and history. We propose that considering what it is like to be an LLM like ChatGPT, as Nagel might have put it, can help us gain insight into its capabilities in general, and in particular, that its exposure to linguistic training data can be productively reframed as exposure to the diegetic information encoded in language, and its deficits can be reframed as ignorance of ext
    
[^223]: SmartTrim：用于高效的视觉-语言模型的自适应标记和注意力修剪

    SmartTrim: Adaptive Tokens and Attention Pruning for Efficient Vision-Language Models

    [https://arxiv.org/abs/2305.15033](https://arxiv.org/abs/2305.15033)

    SmartTrim 提出了一种自适应加速框架，通过识别并修剪每个层中的冗余标记表示和注意力头来提升视觉-语言模型的效率。

    

    尽管基于Transformer的视觉-语言模型（VLMs）在各种视觉-语言任务上取得了显著的性能，但输入和参数中存在冗余，严重影响了它们在实际应用中的效率。鉴于挑战，我们提出了SmartTrim，这是一种自适应加速框架，用于调整每个实例的计算开销。具体而言，我们将轻量级模块集成到原始主干中，以识别并修剪每个层中的冗余标记表示和注意力头。此外，我们设计了自我蒸馏策略，以增强修剪模型和其完全容量对应物之间预测的一致性。在各种视觉-语言任务中的实验结果不断展示

    arXiv:2305.15033v2 Announce Type: replace  Abstract: Despite achieving remarkable performance on various vision-language tasks, Transformer-based Vision-Language Models (VLMs) suffer from redundancy in inputs and parameters, significantly hampering their efficiency in real-world applications. Moreover, the degree of redundancy in token representations and model parameters, such as attention heads, varies significantly for different inputs. In light of the challenges, we propose SmartTrim, an adaptive acceleration framework for VLMs, which adjusts the computational overhead per instance. Specifically, we integrate lightweight modules into the original backbone to identify and prune redundant token representations and attention heads within each layer. Furthermore, we devise a self-distillation strategy to enhance the consistency between the predictions of the pruned model and its fully-capacity counterpart. Experimental results across various vision-language tasks consistently demonstra
    
[^224]: 欺骗LLMs让其不遵从：正式化、分析和检测越狱行为

    Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks

    [https://arxiv.org/abs/2305.14965](https://arxiv.org/abs/2305.14965)

    该论文提出了正式化和已知越狱分类法以填补对商用大规模语言模型（LLMs）被越狱攻击的缺乏研究，调查现有的越狱方法及其在开源和商用LLMs上的有效性。

    

    最近对商用大规模语言模型（LLMs）的探索表明，非专家用户可以通过简单操纵他们的提示来越狱LLMs；导致退化的输出行为、隐私与安全漏洞、冒犯性输出以及违反内容监管政策。有限的研究已进行了对这些攻击及其缓解措施的正式化和分析。我们通过提出一个形式化描述和已知（及可能的）越狱分类法来填补这一差距。我们调查现有的越狱方法及其在开源和商用LLMs（如基于GPT的模型、OPT、BLOOM和FLAN-T5-XXL）上的有效性。我们进一步讨论了越狱检测在针对已知攻击方面的挑战。为了我们的分析，我们收集了4项任务的3700个越狱提示的数据集。我们将随着模型输出一起公开这一数据集。

    arXiv:2305.14965v2 Announce Type: replace  Abstract: Recent explorations with commercial Large Language Models (LLMs) have shown that non-expert users can jailbreak LLMs by simply manipulating their prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies. Limited studies have been conducted to formalize and analyze these attacks and their mitigations. We bridge this gap by proposing a formalism and a taxonomy of known (and possible) jailbreaks. We survey existing jailbreak methods and their effectiveness on open-source and commercial LLMs (such as GPT-based models, OPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak detection in terms of their effectiveness against known attacks. For our analysis, we collect a dataset of 3700 jailbreak prompts across 4 tasks. We will make the dataset public along with the model outputs.
    
[^225]: CHEAT: 用于检测ChatGPT生成摘要的大规模数据集

    CHEAT: A Large-scale Dataset for Detecting ChatGPT-writtEn AbsTracts

    [https://arxiv.org/abs/2304.12008](https://arxiv.org/abs/2304.12008)

    本研究提出了一个名为CHEAT的大规模数据集，用于检测ChatGPT生成的摘要；研究表明ChatGPT生成的摘要是可以被检测出来的，但随着人类参与的增加，检测的难度也在增加。

    

    ChatGPT的强大能力引起了学术界的广泛关注。恶意用户可以通过ChatGPT合成虚假的学术内容，这对学术的严谨性和原创性造成了极大的伤害。有必要开发ChatGPT生成内容检测算法，需要大规模数据集的支持。本文首次研究了ChatGPT对学术界可能造成的负面影响，并提出了一个大规模的ChatGPT生成摘要数据集（CHEAT）来支持检测算法的开发。其中，ChatGPT生成的摘要数据集包含35,304个合成摘要，其代表有Generation, Polish, 和 Mix。基于这些数据，我们对现有的文本合成检测算法进行了彻底分析。我们展示了ChatGPT生成的摘要是可以被检测出来的，而检测难度随着人类参与的增加而增加。我们的数据集可以在https://gith上找到。

    arXiv:2304.12008v2 Announce Type: replace  Abstract: The powerful ability of ChatGPT has caused widespread concern in the academic community. Malicious users could synthesize dummy academic content through ChatGPT, which is extremely harmful to academic rigor and originality. The need to develop ChatGPT-written content detection algorithms call for large-scale datasets. In this paper, we initially investigate the possible negative impact of ChatGPT on academia,and present a large-scale CHatGPT-writtEn AbsTract dataset (CHEAT) to support the development of detection algorithms. In particular, the ChatGPT-written abstract dataset contains 35,304 synthetic abstracts, with Generation, Polish, and Mix as prominent representatives. Based on these data, we perform a thorough analysis of the existing text synthesis detection algorithms. We show that ChatGPT-written abstracts are detectable, while the detection difficulty increases with human involvement.Our dataset is available in https://gith
    
[^226]: 使用时间延迟神经网络实现多模态标点修复的高效集成

    Efficient Ensemble for Multimodal Punctuation Restoration using Time-Delay Neural Network

    [https://arxiv.org/abs/2302.13376](https://arxiv.org/abs/2302.13376)

    使用时间延迟神经网络实现了多模态标点修复的高效集成方法，相比当前最佳模型提高了1.0个F1分数，并且使用了更少的推理网络参数。

    

    标点修复在自动语音识别的后处理过程中起着至关重要的作用，但对模型效率的要求是关键。为此，我们提出了EfficientPunct，这是一种使用多模态时间延迟神经网络的集成方法，通过使用不到十分之一的推理网络参数，优于当前最佳模型1.0个F1分数。我们简化了语音识别器，以有效地输出用于标点修复的隐藏层声学嵌入，同时使用BERT提取有意义的文本嵌入。通过使用强制对齐和时间卷积，我们消除了注意力融合的必要性，极大提高了计算效率并提高了性能。EfficientPunct通过一个集成方法，对BERT纯粹基于语言的预测的权重略高于多模态网络的预测，树立了一个新的技术水平。我们的代码可在 https://githu

    arXiv:2302.13376v2 Announce Type: replace  Abstract: Punctuation restoration plays an essential role in the post-processing procedure of automatic speech recognition, but model efficiency is a key requirement for this task. To that end, we present EfficientPunct, an ensemble method with a multimodal time-delay neural network that outperforms the current best model by 1.0 F1 points, using less than a tenth of its inference network parameters. We streamline a speech recognizer to efficiently output hidden layer acoustic embeddings for punctuation restoration, as well as BERT to extract meaningful text embeddings. By using forced alignment and temporal convolutions, we eliminate the need for attention-based fusion, greatly increasing computational efficiency and raising performance. EfficientPunct sets a new state of the art with an ensemble that weights BERT's purely language-based predictions slightly more than the multimodal network's predictions. Our code is available at https://githu
    
[^227]: 用标注数据进行链式思维的自动提示增强与选择

    Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data

    [https://arxiv.org/abs/2302.12822](https://arxiv.org/abs/2302.12822)

    提出了Automate-CoT策略，可以通过从小型标记数据集自动增加合理链，并修剪低质量链，构建基于标签的机器生成理由链的候选池，最终选择最佳组合。

    

    Chain-of-thought prompting（CoT）推进了大型语言模型（LLMs）的推理能力，在算术、常识和符号推理任务中取得了优越性能。然而，大多数CoT研究依赖于精心设计的人工注释的合理链，以提示语言模型，这对于现实世界中存在标记的训练数据但没有人工注释的合理链的应用构成了挑战。这为CoT提示应用于这些通用任务带来了障碍。本文提出了一种新策略，Automate-CoT（自动提示增强与选择与链思），可以通过从小型标记数据集中自动增强合理链，然后修剪低质量链，基于标签构建基于机器生成的理由链的候选池。最后，它选择了几个理由链的最佳组合。

    arXiv:2302.12822v2 Announce Type: replace  Abstract: Chain-of-thought prompting (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in arithmetic, commonsense, and symbolic reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt the language model, which poses challenges for real-world applications where labeled training data is available without human-annotated rational chains. This creates barriers to applications of CoT prompting to these general tasks. This paper proposes a new strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoTs by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machine-generated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains
    
[^228]: 面向中文拼写检查的错误鲁棒检索

    Error-Robust Retrieval for Chinese Spelling Check

    [https://arxiv.org/abs/2211.07843](https://arxiv.org/abs/2211.07843)

    本文提出了一种错误鲁棒的即插即用检索方法RERIC，可直接应用于现有的中文拼写检查模型，通过融合多种表示形式提高了检索的鲁棒性。

    

    中文拼写检查（CSC）旨在检测和纠正中文语境中的错误标记，具有广泛的应用。然而，它面临着标注数据不足和以前的方法可能实际上并未充分利用现有数据集的挑战。在本文中，我们引入了一种面向中文拼写检查的错误鲁棒信息的即插即用检索方法（RERIC），可直接应用于现有的CSC模型。检索的数据存储完全基于训练数据构建，并根据CSC的特性进行精心设计。具体来说，我们在检索过程中在查询和关键字的计算中使用融合音系、形态和上下文信息的多模态表示，以增强对潜在错误的鲁棒性。此外，为了更好地判断检索到的候选项，会考虑要检查的标记周围的n-gram。

    arXiv:2211.07843v2 Announce Type: replace  Abstract: Chinese Spelling Check (CSC) aims to detect and correct error tokens in Chinese contexts, which has a wide range of applications. However, it is confronted with the challenges of insufficient annotated data and the issue that previous methods may actually not fully leverage the existing datasets. In this paper, we introduce our plug-and-play retrieval method with error-robust information for Chinese Spelling Check (RERIC), which can be directly applied to existing CSC models. The datastore for retrieval is built completely based on the training data, with elaborate designs according to the characteristics of CSC. Specifically, we employ multimodal representations that fuse phonetic, morphologic, and contextual information in the calculation of query and key during retrieval to enhance robustness against potential errors. Furthermore, in order to better judge the retrieved candidates, the n-gram surrounding the token to be checked is 
    
[^229]: OmDet: 大规模视觉-语言多数据集预训练与多模式检测网络

    OmDet: Large-scale vision-language multi-dataset pre-training with multimodal detection network

    [https://arxiv.org/abs/2209.05946](https://arxiv.org/abs/2209.05946)

    OmDet 提出了一种语言意识的目标检测架构和创新的训练机制，利用多数据集视觉-语言预训练，从不同数据集中积累“视觉词汇”，实现以语言为条件的多模态检测网络，在目标检测、开放式词汇检测和短语定位等场景中表现出优越性能。

    

    目标检测（OD）在开放式词汇和开放式场景中的进展是计算机视觉中的一项重要挑战。本文介绍了OmDet，一种新颖的具有语言意识的目标检测架构，以及一种创新的训练机制，利用持续学习和多数据集视觉-语言预训练。OmDet利用自然语言作为通用知识表示，从各种数据集中积累“视觉词汇”，将任务统一为一个以语言为条件的检测框架。我们的多模态检测网络（MDN）克服了多数据集联合训练的挑战，并在不需要手动标签分类合并的情况下泛化到众多训练数据集中。我们展示了OmDet在野外目标检测、开放式词汇检测和短语定位方面优于强基线方法的性能表现，达到了最先进的结果。消融研究揭示了扩展的影响。

    arXiv:2209.05946v2 Announce Type: replace-cross  Abstract: The advancement of object detection (OD) in open-vocabulary and open-world scenarios is a critical challenge in computer vision. This work introduces OmDet, a novel language-aware object detection architecture, and an innovative training mechanism that harnesses continual learning and multi-dataset vision-language pre-training. Leveraging natural language as a universal knowledge representation, OmDet accumulates a "visual vocabulary" from diverse datasets, unifying the task as a language-conditioned detection framework. Our multimodal detection network (MDN) overcomes the challenges of multi-dataset joint training and generalizes to numerous training datasets without manual label taxonomy merging. We demonstrate superior performance of OmDet over strong baselines in object detection in the wild, open-vocabulary detection, and phrase grounding, achieving state-of-the-art results. Ablation studies reveal the impact of scaling th
    
[^230]: Klarna产品页面数据集：利用图神经网络和大型语言模型进行网络元素提名

    The Klarna Product Page Dataset: Web Element Nomination with Graph Neural Networks and Large Language Models

    [https://arxiv.org/abs/2111.02168](https://arxiv.org/abs/2111.02168)

    Klarna产品页面数据集是一个全面多样的网页集合，为解决网络自动化算法设计中数据集稀缺性问题提供了新的资源。

    

    Web自动化有可能彻底改变用户与数字世界的互动方式，通过复杂的计算方法提供无与伦比的帮助，简化任务。在这一演进过程中，网络元素提名任务至关重要，它涉及识别网页上的独特元素。不幸的是，网络自动化算法设计的发展受到全面和真实反映网络应用程序复杂性的数据集的稀缺性的阻碍。为了解决这一问题，我们推出了Klarna产品页面数据集，这是一个全面多样的网页集合，超越了现有数据集的丰富性和多样性。该数据集包含来自8,175个电子商务网站的51,701个手动标记的产品页面，覆盖了八个地理区域，并附带了一组渲染页面截图数据集。为了开始研究Klarna产品页面数据集，我们进行了实证研究

    arXiv:2111.02168v4 Announce Type: replace-cross  Abstract: Web automation holds the potential to revolutionize how users interact with the digital world, offering unparalleled assistance and simplifying tasks via sophisticated computational methods. Central to this evolution is the web element nomination task, which entails identifying unique elements on webpages. Unfortunately, the development of algorithmic designs for web automation is hampered by the scarcity of comprehensive and realistic datasets that reflect the complexity faced by real-world applications on the Web. To address this, we introduce the Klarna Product Page Dataset, a comprehensive and diverse collection of webpages that surpasses existing datasets in richness and variety. The dataset features 51,701 manually labeled product pages from 8,175 e-commerce websites across eight geographic regions, accompanied by a dataset of rendered page screenshots. To initiate research on the Klarna Product Page Dataset, we empirical
    
[^231]: SelectLLM：LLMs能否选择重要的指令进行注释？

    SelectLLM: Can LLMs Select Important Instructions to Annotate?. (arXiv:2401.16553v1 [cs.CL])

    [http://arxiv.org/abs/2401.16553](http://arxiv.org/abs/2401.16553)

    这项工作提出了一种名为SelectLLM的新方法，利用LLMs选择高质量指令。通过提示LLMs估计每个无标签指令的有用性和影响力，并使用聚类算法将指令分为多个聚类。

    

    使用大量且多样化的指令数据集训练大型语言模型(LLMs)可以使模型理解和遵循人类指令。最近的研究表明，使用一小组高质量的指令可以超过使用大量更嘈杂的指令。由于指令是无标签的，且响应是自然文本，传统的主动学习方案无法直接应用于选择无标签指令。在这项工作中，我们提出了一种新的指令选择方法，称为SelectLLM，它利用LLMs选择高质量指令。我们的高级思想是利用LLMs通过提示来估计每个指令在没有相应标签（即响应）的情况下的有用性和影响力。SelectLLM包括两个步骤：使用聚类算法（例如CoreSet）将无标签指令划分为多个聚类，然后提示LLMs在其中选择高质量指令。

    Training large language models (LLMs) with a large and diverse instruction dataset aligns the models to comprehend and follow human instructions. Recent works have shown that using a small set of high-quality instructions can outperform using large yet more noisy ones. Because instructions are unlabeled and their responses are natural text, traditional active learning schemes with the model's confidence cannot be directly applied to the selection of unlabeled instructions. In this work, we propose a novel method for instruction selection, called SelectLLM, that leverages LLMs for the selection of high-quality instructions. Our high-level idea is to use LLMs to estimate the usefulness and impactfulness of each instruction without the corresponding labels (i.e., responses), via prompting. SelectLLM involves two steps: dividing the unlabelled instructions using a clustering algorithm (e.g., CoreSet) to multiple clusters, and then prompting LLMs to choose high-quality instructions within e
    
[^232]: HiFT:一种分层全参数微调策略

    HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy. (arXiv:2401.15207v1 [cs.LG])

    [http://arxiv.org/abs/2401.15207](http://arxiv.org/abs/2401.15207)

    HiFT是一种分层全参数微调策略，通过仅在每个训练步骤中更新参数的子集，可以显著减少GPU内存使用，并实现与参数高效微调和标准全参数微调相当的性能。

    

    随着语言模型的增长，在下游任务中微调语言模型的全参数需要占用大量GPU内存。现有方法利用零阶优化器以节省GPU内存，但这可能会影响语言模型的性能，因为非零阶优化器在大多数下游任务上更容易收敛。本文提出了一种新颖的独立于优化器的端到端分层微调策略HiFT，它仅在每个训练步骤中更新参数的子集。 HiFT可以显著减少存储在GPU内存中的梯度和优化器状态参数的量，从而减少GPU内存的使用。我们的结果表明：（1）HiFT实现了与参数高效微调和标准全参数微调相当的性能。（2）HiFT支持包括在内的各种优化器

    Full-parameter fine-tuning has become the go-to choice for adapting language models (LMs) to downstream tasks due to its excellent performance. As LMs grow in size, fine-tuning the full parameters of LMs requires a prohibitively large amount of GPU memory. Existing approaches utilize zeroth-order optimizer to conserve GPU memory, which can potentially compromise the performance of LMs as non-zero order optimizers tend to converge more readily on most downstream tasks. In this paper, we propose a novel optimizer-independent end-to-end hierarchical fine-tuning strategy, HiFT, which only updates a subset of parameters at each training step. HiFT can significantly reduce the amount of gradients and optimizer state parameters residing in GPU memory at the same time, thereby reducing GPU memory usage. Our results demonstrate that: (1) HiFT achieves comparable performance to parameter-efficient fine-tuning and standard full parameter fine-tuning. (2) HiFT supports various optimizers including
    
[^233]: Airavata: 引入针对印地语指令调整的LLM

    Airavata: Introducing Hindi Instruction-tuned LLM. (arXiv:2401.15006v1 [cs.CL])

    [http://arxiv.org/abs/2401.15006](http://arxiv.org/abs/2401.15006)

    "Airavata"是一个针对印地语进行指令调整的LLM，通过微调OpenHathi和IndicInstruct数据集，提供更好的协助任务性能，并计划扩展到所有22种计划Indic语言。

    

    我们宣布首次发布了"Airavata"，这是一个针对印地语进行指令调整的LLM。通过将OpenHathi与各种指令调整的印地语数据集进行微调，Airavata更适合辅助任务。除了模型外，我们还分享了IndicInstruct数据集，这是一组用于进一步研究Indic LLM的多样化指令调整数据集。此外，我们还提供了评估基准和评估框架，以评估LLM在印地语任务中的性能。目前，Airavata支持印地语，但我们计划将其扩展到所有22种计划Indic语言。您可以在https://ai4bharat.github.io/airavata上访问所有工件。

    We announce the initial release of "Airavata," an instruction-tuned LLM for Hindi. Airavata was created by fine-tuning OpenHathi with diverse, instruction-tuning Hindi datasets to make it better suited for assistive tasks. Along with the model, we also share the IndicInstruct dataset, which is a collection of diverse instruction-tuning datasets to enable further research for Indic LLMs. Additionally, we present evaluation benchmarks and a framework for assessing LLM performance across tasks in Hindi. Currently, Airavata supports Hindi, but we plan to expand this to all 22 scheduled Indic languages. You can access all artifacts at https://ai4bharat.github.io/airavata.
    
[^234]: 作为一种语言处理任务的参数高效的对话推荐系统

    Parameter-Efficient Conversational Recommender System as a Language Processing Task. (arXiv:2401.14194v1 [cs.CL])

    [http://arxiv.org/abs/2401.14194](http://arxiv.org/abs/2401.14194)

    本文将对话推荐系统作为一种语言处理任务进行建模，利用预训练的语言模型来编码项目、理解用户意图，通过语义匹配进行项目推荐，并生成对话。实验证明了该方法的有效性。

    

    对话式推荐系统旨在通过自然语言对话来向用户推荐相关的项目。之前的工作通常利用外部知识图谱来提供项目的语义信息，利用语言模型进行对话生成，以及利用推荐模块进行相关项目的排序。这种多组件的组合导致训练过程繁琐，并且导致对话生成和项目推荐之间的语义不配对问题。在本文中，我们使用自然语言表示项目，并将对话式推荐系统作为一种自然语言处理任务进行建模。因此，我们利用预训练的语言模型来编码项目，在对话中理解用户意图，通过语义匹配进行项目推荐，并生成对话。作为一个统一的模型，我们的PECRS（参数高效的对话推荐系统）可以在单个阶段进行优化，而不依赖非文本元数据，如知识图谱。实验证明了我们方法的有效性。

    Conversational recommender systems (CRS) aim to recommend relevant items to users by eliciting user preference through natural language conversation. Prior work often utilizes external knowledge graphs for items' semantic information, a language model for dialogue generation, and a recommendation module for ranking relevant items. This combination of multiple components suffers from a cumbersome training process, and leads to semantic misalignment issues between dialogue generation and item recommendation. In this paper, we represent items in natural language and formulate CRS as a natural language processing task. Accordingly, we leverage the power of pre-trained language models to encode items, understand user intent via conversation, perform item recommendation through semantic matching, and generate dialogues. As a unified model, our PECRS (Parameter-Efficient CRS), can be optimized in a single stage, without relying on non-textual metadata such as a knowledge graph. Experiments on
    
[^235]: 代码提示在文本+代码LLMs中引发了条件推理能力

    Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs. (arXiv:2401.10065v1 [cs.CL])

    [http://arxiv.org/abs/2401.10065](http://arxiv.org/abs/2401.10065)

    本论文研究了在大型语言模型（LLMs）中触发条件推理能力的方法，通过使用代码提示将自然语言问题转化为代码，从而在多个数据集上实现了显著的性能提升。

    

    推理是实现语言理解的基本组成部分。在多种推理类型中，条件推理是一种在某些条件下得出不同结论的能力，在大型语言模型（LLMs）中一直没有得到充分研究。最近的提示方法，如思维链，显著改进了在推理任务上的LLMs性能。然而，我们对于什么触发了LLMs中的推理能力仍然知之甚少。我们假设代码提示能够触发在文本和代码上训练的LLMs中的条件推理。我们提出了一系列的提示，将自然语言问题转化为代码，并用生成的代码提示LLMs。我们的实验发现，在需要条件推理的多个数据集上，代码提示使得GPT 3.5的性能提升了2.6到7.7个百分点。接着，我们进行了实验，探索了代码提示如何引发条件推理能力以及通过哪些特征进行。我们观察到，提示的形式和内容对于引发条件推理能力起到了重要作用。

    Reasoning is a fundamental component for achieving language understanding. Among the multiple types of reasoning, conditional reasoning, the ability to draw different conclusions depending on some condition, has been understudied in large language models (LLMs). Recent prompting methods, such as chain of thought, have significantly improved LLMs on reasoning tasks. Nevertheless, there is still little understanding of what triggers reasoning abilities in LLMs. We hypothesize that code prompts can trigger conditional reasoning in LLMs trained on text and code. We propose a chain of prompts that transforms a natural language problem into code and prompts the LLM with the generated code. Our experiments find that code prompts exhibit a performance boost between 2.6 and 7.7 points on GPT 3.5 across multiple datasets requiring conditional reasoning. We then conduct experiments to discover how code prompts elicit conditional reasoning abilities and through which features. We observe that prom
    
[^236]: 将大型语言模型应用于教育：基本能力、潜力和挑战

    Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges. (arXiv:2401.08664v1 [cs.AI])

    [http://arxiv.org/abs/2401.08664](http://arxiv.org/abs/2401.08664)

    本文回顾了针对教育能力的大型语言模型研究，包括数学、写作、编程、推理和基于知识的问答，旨在探索其在构建下一代智能教育系统中的潜力和挑战。

    

    在线教育平台利用互联网分发教育资源，旨在提供便捷的教育，但往往在与学生的实时交流方面存在不足。由于需要解决学生在学习过程中遇到的多样化障碍的挑战，它们经常难以提供个性化的教育资源。最近出现的大型语言模型（LLMs），如ChatGPT，提供了通过理解个体请求解决这一问题的可能性。虽然LLMs在各个领域都取得了成功，但基于LLM的教育系统的构建仍然面临着广泛的教育技能要求。本文回顾了与教育能力相关的近期出现的LLM研究，包括数学、写作、编程、推理和基于知识的问答，旨在探索它们在构建下一代智能教育系统方面的潜力。

    Online education platforms, leveraging the internet to distribute education resources, seek to provide convenient education but often fall short in real-time communication with students. They often struggle to offer personalized education resources due to the challenge of addressing the diverse obstacles students encounter throughout their learning journey. Recently, the emergence of large language models (LLMs), such as ChatGPT, offers the possibility for resolving this issue by comprehending individual requests. Although LLMs have been successful in various fields, creating an LLM-based education system is still challenging for the wide range of educational skills required. This paper reviews the recently emerged LLM researches related to educational capabilities, including mathematics, writing, programming, reasoning, and knowledge-based question answering, with the aim to explore their potential in constructing the next-generation intelligent education system. Based on the current 
    
[^237]: INACIA：将大型语言模型整合到巴西审计法院中的机会和挑战

    INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges. (arXiv:2401.05273v1 [cs.CL])

    [http://arxiv.org/abs/2401.05273](http://arxiv.org/abs/2401.05273)

    本文介绍了INACIA系统，这是一个将大型语言模型整合到巴西审计法院中的系统，可以自动化案件分析的各个阶段，并展示了其在从案件文件中提取信息、评估合法性和生成司法建议方面的潜力。

    

    本文介绍了INACIA（基于人工智能的辅助指令系统），这是一个开创性的系统，旨在将大型语言模型（LLMs）整合到巴西联邦审计法院（TCU）的运营框架中。该系统自动化了案件分析的各个阶段，包括基本信息提取、可受理性审查、Periculum in mora和Fumus boni iuris分析以及建议生成。通过一系列实验，我们展示了INACIA从案件文件中提取相关信息、评估其合法性并生成司法建议的潜力。利用验证数据集和LLMs，我们的评估方法提供了一种创新的方法来评估系统性能，与人类判断高度相关。结果突显了INACIA处理复杂法律任务的能力，表明其适用于增加法律系统的效率和司法公正性。

    This paper introduces INACIA (Instru\c{c}\~ao Assistida com Intelig\^encia Artificial), a groundbreaking system designed to integrate Large Language Models (LLMs) into the operational framework of Brazilian Federal Court of Accounts (TCU). The system automates various stages of case analysis, including basic information extraction, admissibility examination, Periculum in mora and Fumus boni iuris analyses, and recommendations generation. Through a series of experiments, we demonstrate INACIA's potential in extracting relevant information from case documents, evaluating its legal plausibility, and generating judicial recommendations. Utilizing a validation dataset alongside LLMs, our evaluation methodology presents an innovative approach to assessing system performance, correlating highly with human judgment. The results highlight INACIA's proficiency in handling complex legal tasks, indicating its suitability for augmenting efficiency and judicial fairness within legal systems. The pap
    
[^238]: LAMPAT：使用对抗训练进行低秩多语言改写的方法

    LAMPAT: Low-Rank Adaption for Multilingual Paraphrasing Using Adversarial Training. (arXiv:2401.04348v1 [cs.CL])

    [http://arxiv.org/abs/2401.04348](http://arxiv.org/abs/2401.04348)

    LAMPAT是第一个无监督的多语言改写模型，通过使用对抗训练，它能够在没有平行语料库的语言环境下生成改写。

    

    改写是指使用不同的词语或句子结构来传达相同含义的文本。它可以用作自动数据增强工具，特别是在处理数据不足的低资源语言时。为了在多语言环境下生成改写，先前的研究利用了机器翻译领域的知识，通过在相同语言中进行零样本机器翻译来形成改写。尽管在人工评估中表现良好，但这些方法仍然需要平行翻译数据集，因此无法应用于没有平行语料库的语言。为了解决这个问题，我们提出了第一个无监督的多语言改写模型，LAMPAT（低秩多语言改写的适应性低秩多语言改写模型），其中单语数据集已经足够。

    Paraphrases are texts that convey the same meaning while using different words or sentence structures. It can be used as an automatic data augmentation tool for many Natural Language Processing tasks, especially when dealing with low-resource languages, where data shortage is a significant problem. To generate a paraphrase in multilingual settings, previous studies have leveraged the knowledge from the machine translation field, i.e., forming a paraphrase through zero-shot machine translation in the same language. Despite good performance on human evaluation, those methods still require parallel translation datasets, thus making them inapplicable to languages that do not have parallel corpora. To mitigate that problem, we proposed the first unsupervised multilingual paraphrasing model, LAMPAT ($\textbf{L}$ow-rank $\textbf{A}$daptation for $\textbf{M}$ultilingual $\textbf{P}$araphrasing using $\textbf{A}$dversarial $\textbf{T}$raining), by which monolingual dataset is sufficient enough 
    
[^239]: PIXAR：像素空间中的自回归语言建模

    PIXAR: Auto-Regressive Language Modeling in Pixel Space. (arXiv:2401.03321v1 [cs.CL])

    [http://arxiv.org/abs/2401.03321](http://arxiv.org/abs/2401.03321)

    本论文介绍了PIXAR，这是第一个像素自回归的语言模型，可以用于生成自由形式的文本作为图像，而不依赖于预定义的词汇表。同时，论文还提出了一个简单的对抗性预训练方法来解决生成非模糊文本的挑战。

    

    最近的研究显示可以构建基于像素表示的开放词汇量大语言模型（LLMs），这些模型以编码器-解码器模型的形式，重构遮蔽的图像文本补丁。然而，这些基于像素的LLMs仅限于自编码任务，无法生成新的文本作为图像。因此，它们不能用于开放式回答或生成式语言任务。在这项工作中，我们克服了这一限制，并引入了PIXAR，这是第一个不需要预定义词汇表的像素自回归LLM，用于输入和输出文本。PIXAR只有一个解码器，可以回答自由形式的生成任务，并且在文本表示学习性能上与以前的编码器-解码器模型持平。此外，我们强调了以自回归方式生成非模糊文本作为图像的挑战，并将其与通常的最大似然目标联系起来。我们提出了一个简单的对抗性预训练方法，该方法在很大程度上解决了这个问题。

    Recent works showed the possibility of building open-vocabulary large language models (LLMs) that directly operate on pixel representations and are implemented as encoder-decoder models that reconstruct masked image patches of rendered text. However, these pixel-based LLMs are limited to autoencoding tasks and cannot generate new text as images. As such, they cannot be used for open-answer or generative language tasks. In this work, we overcome this limitation and introduce PIXAR, the first pixel-based autoregressive LLM that does not rely on a pre-defined vocabulary for both input and output text. Consisting of only a decoder, PIXAR can answer free-form generative tasks while keeping the text representation learning performance on par with previous encoder-decoder models. Furthermore, we highlight the challenges to autoregressively generate non-blurred text as images and link this to the usual maximum likelihood objective. We propose a simple adversarial pretraining that significantly
    
[^240]: TiMix: 文本感知图像混合用于有效的视觉语言预训练

    TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training. (arXiv:2312.08846v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.08846](http://arxiv.org/abs/2312.08846)

    TiMix是一种将文本感知的图像混合技术用于视觉语言预训练的方法，通过集成混合数据增强技术，并从互信息的角度进行理论分析，提高了数据效率并取得了可比较的性能。

    

    自监督的多模态对比学习（SMCL）通过对齐视觉和语言模态，显著推进了现代视觉语言预训练（VLP）模型的发展。然而，由于网络收集的文本-图像对中存在噪声，扩大SMCL的训练数据量在计算成本和数据效率方面面临着相当大的障碍。为了提高VLP的数据效率，我们提出了文本感知图像混合（TiMix），将基于混合的数据增强技术集成到SMCL中，显著提升了性能，而不会显著增加计算开销。我们从互信息（MI）的角度对TiMix进行了理论分析，表明跨模态对比学习的混合数据样本隐式地作为对比损失的正则化器。实验结果表明，即使使用较少的训练数据和较短的训练时间，TiMix在下游任务上表现出可比较的性能。

    Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances modern Vision-Language Pre-training (VLP) models by aligning visual and linguistic modalities. Due to noises in web-harvested text-image pairs, however, scaling up training data volume in SMCL presents considerable obstacles in terms of computational cost and data inefficiency. To improve data efficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates mix-based data augmentation techniques into SMCL, yielding significant performance improvements without significantly increasing computational overhead. We provide a theoretical analysis of TiMixfrom a mutual information (MI) perspective, showing that mixed data samples for cross-modal contrastive learning implicitly serve as a regularizer for the contrastive loss. The experimental results demonstrate that TiMix exhibits a comparable performance on downstream tasks, even with a reduced amount of training data and shorter training time, when be
    
[^241]: 人类进行更好的编辑：衡量使用LLM生成的反事实增强数据在有害语言检测中的效果

    People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection. (arXiv:2311.01270v1 [cs.CL])

    [http://arxiv.org/abs/2311.01270](http://arxiv.org/abs/2311.01270)

    本论文通过自动生成的反事实增强数据（CADs）与手动生成的CADs进行比较，评估它们在提高模型鲁棒性方面的效果。结果显示，手动生成的CADs仍然是最有效的方法。

    

    自然语言处理模型在许多重要的社会计算任务中被使用，如检测性别歧视、种族歧视或其他仇恨内容。因此，这些模型对虚假特征的鲁棒性至关重要。过去的工作尝试解决这些虚假特征问题，其中包括反事实增强数据（CADs）的训练数据增强方法。CADs对现有的训练数据进行最小改动并翻转标签；在其上进行训练可能减少模型对虚假特征的依赖。然而，手动生成CADs可能耗时且昂贵。因此，在这项工作中，我们评估了是否可以使用生成型自然语言处理模型自动化这个任务。我们使用Polyjuice、ChatGPT和Flan-T5自动生成CADs，并与手动生成的CADs进行比较，评估它们在提高模型鲁棒性方面的实用性。通过在多个领域外测试集上测试模型的性能以及每个数据点的有效性，我们的结果表明，尽管手动CADs仍然是最有效的方法。

    NLP models are used in a variety of critical social computing tasks, such as detecting sexist, racist, or otherwise hateful content. Therefore, it is imperative that these models are robust to spurious features. Past work has attempted to tackle such spurious features using training data augmentation, including Counterfactually Augmented Data (CADs). CADs introduce minimal changes to existing training data points and flip their labels; training on them may reduce model dependency on spurious features. However, manually generating CADs can be time-consuming and expensive. Hence in this work, we assess if this task can be automated using generative NLP models. We automatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluate their usefulness in improving model robustness compared to manually-generated CADs. By testing both model performance on multiple out-of-domain test sets and individual data point efficacy, our results show that while manual CADs are still the most e
    
[^242]: 持续学习在语言转换中的研究

    A Study of Continual Learning Under Language Shift. (arXiv:2311.01200v1 [cs.CL])

    [http://arxiv.org/abs/2311.01200](http://arxiv.org/abs/2311.01200)

    本文研究了持续学习在语言转换中的应用，发现在更新语言模型时，前向转移效果较好且与语言顺序无关，但后向转移效果可能取决于新语言的顺序和特征。

    

    最近语言模型预训练的数据和模型规模的增加导致了巨大的训练成本。在随时间推移而出现新数据的情况下，更新模型而不是完全重新训练可以带来显著的收益。在本文中，我们研究了在新语言出现时更新语言模型时的好处和弊端，即在语言转换中持续学习的情况。从单语英语语言模型出发，我们逐步添加了来自挪威语和冰岛语的数据，以研究前向和后向转移效果如何取决于预训练顺序和语言特征，对于不同的模型大小和学习率调度器。我们的结果表明，尽管前向转移主要是正向的，不受语言顺序的影响，但后向转移则可能是正向的或负向的，具体取决于新语言的顺序和特征。为了解释这些模式，我们探索了几种语言相似度度量方法。

    The recent increase in data and model scale for language model pre-training has led to huge training costs. In scenarios where new data become available over time, updating a model instead of fully retraining it would therefore provide significant gains. In this paper, we study the benefits and downsides of updating a language model when new data comes from new languages - the case of continual learning under language shift. Starting from a monolingual English language model, we incrementally add data from Norwegian and Icelandic to investigate how forward and backward transfer effects depend on the pre-training order and characteristics of languages, for different model sizes and learning rate schedulers. Our results show that, while forward transfer is largely positive and independent of language order, backward transfer can be either positive or negative depending on the order and characteristics of new languages. To explain these patterns we explore several language similarity metr
    
[^243]: 大型语言模型中的函数向量

    Function Vectors in Large Language Models. (arXiv:2310.15213v1 [cs.CL])

    [http://arxiv.org/abs/2310.15213](http://arxiv.org/abs/2310.15213)

    大型语言模型中存在一种简单的神经机制，将输入-输出函数表示为向量。这些函数向量在不同的上下文中具有鲁棒性，并且具有强大的因果效应。同时，它们还具有将语义向量进行组合的能力。

    

    我们报告了一个简单的神经机制，将输入-输出函数表示为自回归变换语言模型（LMs）中的向量。通过在各种上下文学习（ICL）任务上使用因果中介分析，我们发现少数注意力头传输了展示任务的紧凑表示，我们称之为函数向量（FV）。FV对上下文的变化具有鲁棒性，即它们在不类似于其收集时的ICL上下文的情况下触发对输入的任务执行，例如零样本和自然文本设置。我们在各种任务、模型和层上测试了FV，并在中层发现强大的因果效应。我们研究了FV的内部结构，并发现虽然它们通常包含编码函数的输出空间的信息，但仅此信息无法重构FV。最后，我们测试了FV中的语义向量组合，并发现在某种程度上存在组合的能力。

    We report the presence of a simple neural mechanism that represents an input-output function as a vector within autoregressive transformer language models (LMs). Using causal mediation analysis on a diverse range of in-context-learning (ICL) tasks, we find that a small number attention heads transport a compact representation of the demonstrated task, which we call a function vector (FV). FVs are robust to changes in context, i.e., they trigger execution of the task on inputs such as zero-shot and natural text settings that do not resemble the ICL contexts from which they are collected. We test FVs across a range of tasks, models, and layers and find strong causal effects across settings in middle layers. We investigate the internal structure of FVs and find while that they often contain information that encodes the output space of the function, this information alone is not sufficient to reconstruct an FV. Finally, we test semantic vector composition in FVs, and find that to some exte
    
[^244]: O3D: 基于离线数据的发现与蒸馏方法，用于大规模语言模型在顺序决策中的应用

    O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models. (arXiv:2310.14403v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.14403](http://arxiv.org/abs/2310.14403)

    O3D提出了一种基于离线数据的学习框架，利用大规模数据改进了大规模语言模型在顺序决策问题中的性能，通过自动发现可重复使用的技能，提高了模型的表现

    

    最近对大规模语言模型 (LLMs)的研究取得了令人期待的进展，在解决顺序决策问题方面显示出了良好的性能。通过模仿提示中提供的少量示例（即上下文学习），LLM代理可以与外部环境交互并完成给定任务，而无需额外的训练。然而，这种少量示例往往不足以生成复杂且长期目标任务的高质量解决方案，而有限的上下文长度无法处理更大规模的演示。为此，我们提出了一种利用离线数据的学习框架，以大规模的离线数据（例如人类交互的日志）来改进LLM代理的上下文学习性能。我们通过文本和代码两种方法正式定义了LLM强化策略。然后，我们引入了一种名为O3D的离线数据驱动发现和蒸馏框架，以改善LLM强化策略而无需微调。O3D自动地发现可重复使用的技能

    Recent advancements in large language models (LLMs) have exhibited promising performance in solving sequential decision-making problems. By imitating few-shot examples provided in the prompts (i.e., in-context learning), an LLM agent can interact with an external environment and complete given tasks without additional training. However, such few-shot examples are often insufficient to generate high-quality solutions for complex and long-horizon tasks, while the limited context length cannot consume larger-scale demonstrations. To this end, we propose an offline learning framework that utilizes offline data at scale (e.g, logs of human interactions) to facilitate the in-context learning performance of LLM agents. We formally define LLM-powered policies with both text-based approaches and code-based approaches. We then introduce an Offline Data-driven Discovery and Distillation (O3D) framework to improve LLM-powered policies without finetuning. O3D automatically discovers reusable skills
    
[^245]: 超越准确性：用IdentityChain评估大型代码语言模型的自一致性

    Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain. (arXiv:2310.14053v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14053](http://arxiv.org/abs/2310.14053)

    这篇论文提出了一种评估大型代码语言模型自一致性的方法，并指出目前的模型在自一致性方面存在问题。

    

    代码语言模型(Code LLMs)在实际应用中的使用越来越多，因此对它们进行评估至关重要。传统的准确性评估方法评估Code LLMs在一系列独立任务上的性能，但忽视了其在不同任务上的自一致性。直观来讲，一个可信赖的模型在为其自身的代码生成自然语言规范以及为其自身的规范生成代码时应该是自一致的。未能保持自一致性揭示了对自然语言和编程语言共享语义的理解的不足，从而削弱了模型的可信度。本文首先正式定义了Code LLMs的自一致性，然后设计了一个名为IdentityChain的框架，可以同时有效且高效地评估模型的自一致性和传统准确性。我们研究了11个Code LLMs，并表明它们未能保持自一致性。

    Code Large Language Models (Code LLMs) are being increasingly employed in real-life applications, so evaluating them is critical. While the conventional accuracy evaluates the performance of Code LLMs on a set of individual tasks, their self-consistency across different tasks is overlooked. Intuitively, a trustworthy model should be self-consistent when generating natural language specifications for its own code and generating code for its own specifications. Failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model. In this paper, we first formally define the self-consistency of Code LLMs and then design a framework, IdentityChain, which effectively and efficiently evaluates the self-consistency and conventional accuracy of a model at the same time. We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indee
    
[^246]: 大型语言模型上的用户推理攻击

    User Inference Attacks on Large Language Models. (arXiv:2310.09266v1 [cs.CR])

    [http://arxiv.org/abs/2310.09266](http://arxiv.org/abs/2310.09266)

    本论文研究了在大型语言模型上的用户推理攻击，发现LLMs对于各种微调数据集都很容易受到攻击，尤其是对于离群用户和贡献大量数据的用户。这对保护用户隐私具有重要意义。

    

    微调是将大型语言模型（LLMs）定制为专业任务和应用的常见有效方法。本文研究了在用户数据上微调LLMs的隐私问题。为此，我们定义了一个称为用户推理的现实威胁模型，其中攻击者推断出用户的数据是否被用于微调。我们实现了这种威胁模型的攻击，只需要从用户那里获取一小组样本（可能与用于训练的样本不同）和对微调LLM的黑盒访问权限。我们发现，LLMs在各种微调数据集上易受用户推理攻击的影响，有时攻击成功率接近完美。此外，我们调查了哪些特性使用户容易受到用户推理的攻击，发现离群用户（即数据分布与其他用户明显不同）和贡献大量数据的用户更容易受到攻击。最后，我们探索了解决这种攻击的方案。

    Fine-tuning is a common and effective method for tailoring large language models (LLMs) to specialized tasks and applications. In this paper, we study the privacy implications of fine-tuning LLMs on user data. To this end, we define a realistic threat model, called user inference, wherein an attacker infers whether or not a user's data was used for fine-tuning. We implement attacks for this threat model that require only a small set of samples from a user (possibly different from the samples used for training) and black-box access to the fine-tuned LLM. We find that LLMs are susceptible to user inference attacks across a variety of fine-tuning datasets, at times with near perfect attack success rates. Further, we investigate which properties make users vulnerable to user inference, finding that outlier users (i.e. those with data distributions sufficiently different from other users) and users who contribute large quantities of data are most susceptible to attack. Finally, we explore s
    
[^247]: 不添加，不错过：从预选文本段生成有效的内容保留生成模型

    Dont Add, dont Miss: Effective Content Preserving Generation from Pre-Selected Text Spans. (arXiv:2310.09017v1 [cs.CL])

    [http://arxiv.org/abs/2310.09017](http://arxiv.org/abs/2310.09017)

    本论文介绍了一个高质量的受控文本缩减（CTR）模型，解决了内容保留约束不充分强制执行和次优的银标签训练数据的限制，通过在训练和推理中增强内容保留约束，进一步改进了模型性能。

    

    最近引入的受控文本缩减（CTR）任务在典型的摘要任务中将文本生成步骤隔离出来。它通过挑战模型在输入文本的预选内容（"高亮"）中生成连贯的文本来实现。这种框架在类似摘要的任务中增加了模块化能力，允许将单个CTR模型与各种内容选择设置和模块配对使用。然而，目前还没有可靠的CTR模型，而且现有任务基线的性能中等，无法实际使用。为了填补这个空白，我们引入了一个高质量的开源CTR模型，解决了两个先前的关键限制：不充分强制执行内容保留约束和次优的银标签训练数据。通过在训练中通过强化学习和推理中通过受控解码策略来增强内容保留约束。此外，我们还大幅改进了银标签训练数据。

    The recently introduced Controlled Text Reduction (CTR) task isolates the text generation step within typical summarization-style tasks. It does so by challenging models to generate coherent text conforming to pre-selected content within the input text ("highlights").  This framing enables increased modularity in summarization-like tasks, allowing to couple a single CTR model with various content-selection setups and modules.  However, there are currently no reliable CTR models, while the performance of the existing baseline for the task is mediocre, falling short of practical utility.  Here, we address this gap by introducing a high-quality, open-source CTR model that tackles two prior key limitations: inadequate enforcement of the content-preservation constraint, and suboptimal silver training data.  Addressing these, we amplify the content-preservation constraint in both training, via RL, and inference, via a controlled decoding strategy.  Further, we substantially improve the silve
    
[^248]: 思考、行动和问：开放世界互动个性化机器人导航

    Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation. (arXiv:2310.07968v1 [cs.RO])

    [http://arxiv.org/abs/2310.07968](http://arxiv.org/abs/2310.07968)

    这项研究引入了零射交互个性化对象导航（ZIPON），通过使用大型语言模型（LLM）和用户反馈，解决了在未知环境中导航到个性化目标对象的问题。

    

    零射命令对象导航（ZSON）使代理能够在未知环境中导航到开放词汇对象。现有的ZSON工作主要集中在遵循个别指令以寻找通用对象类，忽略了自然语言交互的利用和识别用户特定对象的复杂性。为了解决这些局限性，我们引入了零射交互个性化对象导航（ZIPON），其中机器人需要在与用户对话的同时导航到个性化目标对象。为了解决ZIPON问题，我们提出了一种新的框架称为开放世界互动个性化导航（ORION），该框架使用大型语言模型（LLM）进行序列决策，以操作不同的感知、导航和通信模块。实验结果表明，能够利用用户反馈的互动代理的性能有了显著改进。然而，在任务完成和用户满意度之间取得良好的平衡仍然具有挑战性。

    Zero-Shot Object Navigation (ZSON) enables agents to navigate towards open-vocabulary objects in unknown environments. The existing works of ZSON mainly focus on following individual instructions to find generic object classes, neglecting the utilization of natural language interaction and the complexities of identifying user-specific objects. To address these limitations, we introduce Zero-shot Interactive Personalized Object Navigation (ZIPON), where robots need to navigate to personalized goal objects while engaging in conversations with users. To solve ZIPON, we propose a new framework termed Open-woRld Interactive persOnalized Navigation (ORION), which uses Large Language Models (LLMs) to make sequential decisions to manipulate different modules for perception, navigation and communication. Experimental results show that the performance of interactive agents that can leverage user feedback exhibits significant improvement. However, obtaining a good balance between task completion 
    
[^249]: 思维多样性提升大规模语言模型的推理能力

    Diversity of Thought Improves Reasoning Abilities of Large Language Models. (arXiv:2310.07088v1 [cs.CL])

    [http://arxiv.org/abs/2310.07088](http://arxiv.org/abs/2310.07088)

    本文提出了一种方法，通过改变输入提示来提高大规模语言模型的推理能力，从而改善模型在复杂推理场景中的表现。这种方法自动采集模型反馈，生成适合问题的多样化提示，并通过多次推理调用来集成这些多样化的提示。

    

    大规模语言模型在需要复杂推理的环境中表现不佳。然而，将模型指导分解问题为更小的推理步骤或通过修改解码步骤使各种生成结果合并可以提升性能。目前的方法都假设输入提示是固定的，并期望解码策略引入所需的多样性。本文放松了这个假设，并讨论了如何通过创建和利用输入提示的变化来提升思维多样性以改善模型性能。我们提出了一种方法，通过向语言模型征求反馈来构思适合问题的方法，从而自动提高提示的多样性。我们在我们的方法DIV-SE (DIVerse reasoning path Self-Ensemble)中对多样的提示进行合成，通过多次推理调用实现。我们还提出了一种经济高效的替代方案，即在一个推理中使用多样的提示。

    Large language models (LLMs) are documented to struggle in settings that require complex reasoning. Nevertheless, instructing the model to break down the problem into smaller reasoning steps (Wei et al., 2022), or ensembling various generations through modifying decoding steps (Wang et al., 2023) boosts performance. Current methods assume that the input prompt is fixed and expect the decoding strategies to introduce the diversity needed for ensembling. In this work, we relax this assumption and discuss how one can create and leverage variations of the input prompt as a means to diversity of thought to improve model performance. We propose a method that automatically improves prompt diversity by soliciting feedback from the LLM to ideate approaches that fit for the problem. We then ensemble the diverse prompts in our method DIV-SE (DIVerse reasoning path Self-Ensemble) across multiple inference calls. We also propose a cost-effective alternative where diverse prompts are used within a s
    
[^250]: 让模型说密文: 通过嵌入进行多智能体辩论

    Let Models Speak Ciphers: Multiagent Debate through Embeddings. (arXiv:2310.06272v1 [cs.CL])

    [http://arxiv.org/abs/2310.06272](http://arxiv.org/abs/2310.06272)

    本文引入了一种名为CIPHER的通信机制，通过去除LLMs中的标记采样步骤，让模型可以通过期望的原始Transformer输出嵌入来传达其信念，从而解决了在自然语言生成中可能存在的信息丢失风险，并提供了编码更广泛信息的优势。

    

    最近，对大型语言模型（LLMs）之间的讨论和辩论引起了广泛关注，因为它们有潜力增强LLMs的推理能力。尽管自然语言由于LLMs的语言理解能力而成为明显的交流选择，但生成自然语言时需要进行的标记采样步骤可能存在信息丢失的潜在风险，因为它仅使用一个标记来代表模型在整个词汇表中的信念。在本文中，我们介绍了一种名为CIPHER（通过嵌入表示进行交流的网络模型协议）的通信机制来解决这个问题。具体来说，我们从LLMs中去除了标记采样步骤，让它们通过原始Transformer输出嵌入的期望来传达它们的信念。值得注意的是，通过偏离自然语言，CIPHER在不对模型权重进行任何修改的情况下，提供了编码更广泛信息的优势。

    Discussion and debate among Large Language Models (LLMs) have gained considerable attention due to their potential to enhance the reasoning ability of LLMs. Although natural language is an obvious choice for communication due to LLM's language understanding capability, the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary. In this paper, we introduce a communication regime named CIPHER (Communicative Inter-Model Protocol Through Embedding Representation) to address this issue. Specifically, we remove the token sampling step from LLMs and let them communicate their beliefs across the vocabulary through the expectation of the raw transformer output embeddings. Remarkably, by deviating from natural language, CIPHER offers an advantage of encoding a broader spectrum of information without any modification to the model weights. While the state-of-the-a
    
[^251]: GeoLLM: 从大型语言模型中提取地理空间知识

    GeoLLM: Extracting Geospatial Knowledge from Large Language Models. (arXiv:2310.06213v1 [cs.CL])

    [http://arxiv.org/abs/2310.06213](http://arxiv.org/abs/2310.06213)

    GeoLLM是一种能够从大型语言模型中提取地理空间知识的新方法，结合来自OpenStreetMap的辅助地图数据，可以有效应用于测量人口密度和经济生计等任务。

    

    机器学习在各种地理空间任务中的应用越来越普遍，但常常依赖于全球范围可用的卫星图像等预测变量，这可能要么很昂贵，要么缺乏预测能力。本文探讨了一个问题，即互联网语言语料库中包含的大量知识是否可以利用大型语言模型（LLM）进行地理空间预测任务。我们首先证明了LLM中嵌入了有关位置的显著空间信息，但仅使用地理坐标来查询LLM对于预测人口密度等关键指标是无效的。然后，我们提出了一种名为GeoLLM的新方法，可以有效地从LLM中提取地理空间知识，并结合来自OpenStreetMap的辅助地图数据。我们展示了我们的方法在多个国际社区关心的任务中的实用性，包括人口密度和经济生计的测量。在这些任务中

    The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power. Here we explore the question of whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models (LLMs), can be leveraged for geospatial prediction tasks. We first demonstrate that LLMs embed remarkable spatial information about locations, but naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density. We then present GeoLLM, a novel method that can effectively extract geospatial knowledge from LLMs with auxiliary map data from OpenStreetMap. We demonstrate the utility of our approach across multiple tasks of central interest to the international community, including the measurement of population density and economic livelihoods. Across these task
    
[^252]: 使用大型语言模型（LLMS）对图中的节点进行无标签分类

    Label-free Node Classification on Graphs with Large Language Models (LLMS). (arXiv:2310.04668v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04668](http://arxiv.org/abs/2310.04668)

    本文介绍了一种使用大型语言模型（LLMs）对图中节点进行无标签分类的方法，即LLM-GNN。它利用LLMs对一小部分节点进行注释，然后通过对LLMs的注释进行训练，使得GNN能够对其余大部分节点进行预测。这种方法充分发挥了GNNs和LLMs的优势，同时解决了它们在处理结构化数据方面的限制。

    

    近年来，图神经网络（Graph Neural Networks，GNNs）在节点分类方面取得了显著的进展。然而，为了确保良好的性能，它们需要大量高质量的标签。相比之下，大型语言模型（Large Language Models，LLMs）在文本属性图上展现出了令人印象深刻的零样学习能力。然而，它们在高效处理结构化数据方面面临挑战，并且推理成本较高。鉴于这些观察结果，本文引入了一种基于LLMs的无标签图节点分类方法，命名为LLM-GNN。它集成了GNNs和LLMs的优势，同时减轻了它们的限制。具体而言，LLMs被用来注释一小部分节点，然后通过对LLMs的注释进行训练，使GNNs能够预测其余大部分节点。LLM-GNN的实现面临一个独特的挑战：我们如何主动选择要由LLMs注释的节点，从而增强GNN的训练？我们如何利用LLMs来优化结构化数据的处理？

    In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to ob
    
[^253]: 使用LLM和BoWs自动评估课堂教学支持：将全局预测与具体反馈相连接

    Automated Evaluation of Classroom Instructional Support with LLMs and BoWs: Connecting Global Predictions to Specific Feedback. (arXiv:2310.01132v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.01132](http://arxiv.org/abs/2310.01132)

    本研究旨在利用大型语言模型和词袋模型自动估计课堂教学支持，以提供更具体、频繁和可行动的反馈给教师。实验证明，所提出的方法准确性接近于人工互评可靠性，LLM模型可以更好地捕捉到教学支持特征。

    

    为了向教师提供更具体、更频繁和可行动的反馈，我们探讨了如何利用大型语言模型（LLMs）来估计“教学支持”领域的CLASS课堂评估得分，该评估方法是广泛使用的观测协议。我们设计了一个机器学习架构，使用Meta的Llama2的零-shot提示，和/或经典的词袋（BoW）模型，用于对教师言语的个别话语（使用OpenAI的Whisper进行自动转录）进行分类，以确定是否存在教学支持。然后，这些话语级的判断结果在整个15分钟的观察会话中进行聚合，以估计全局CLASS得分。在幼儿园和学前班教室的两个经过CLASS编码的数据集上进行的实验证明：（1）所提出的方法自动估计CLASS教学支持的准确性（Pearson R高达0.47）接近人工互评可靠性（最高R=0.55）；（2）LLM模型可以更好地捕捉到小班教室中的教学支持特征。

    With the aim to provide teachers with more specific, frequent, and actionable feedback about their teaching, we explore how Large Language Models (LLMs) can be used to estimate ``Instructional Support'' domain scores of the CLassroom Assessment Scoring System (CLASS), a widely used observation protocol. We design a machine learning architecture that uses either zero-shot prompting of Meta's Llama2, and/or a classic Bag of Words (BoW) model, to classify individual utterances of teachers' speech (transcribed automatically using OpenAI's Whisper) for the presence of Instructional Support. Then, these utterance-level judgments are aggregated over an entire 15-min observation session to estimate a global CLASS score. Experiments on two CLASS-coded datasets of toddler and pre-kindergarten classrooms indicate that (1) automatic CLASS Instructional Support estimation accuracy using the proposed method (Pearson $R$ up to $0.47$) approaches human inter-rater reliability (up to $R=0.55$); (2) LLM
    
[^254]: Transformer-VQ: 基于向量量化实现线性时间的Transformer

    Transformer-VQ: Linear-Time Transformers via Vector Quantization. (arXiv:2309.16354v1 [cs.LG])

    [http://arxiv.org/abs/2309.16354](http://arxiv.org/abs/2309.16354)

    Transformer-VQ是一种基于向量量化实现线性时间的Transformer模型，能够高效计算自注意力，在大规模实验中表现出色。

    

    我们引入了Transformer-VQ，一种仅编码器的Transformer，能够在线性时间内计算基于softmax的密集自注意力。Transformer-VQ的高效注意力是通过向量量化键和一种新颖的缓存机制实现的。在大规模实验中，Transformer-VQ在质量上表现出色，Enwik8(0.99 bpb)，PG-19(26.6 ppl)和ImageNet64(3.16 bpb)都取得了很好的结果。

    We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In large-scale experiments, Transformer-VQ is shown highly competitive in quality, with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64 (3.16 bpb). Code: https://github.com/transformer-vq/transformer_vq
    
[^255]: 艺术还是技巧？大型语言模型与创造力的虚假承诺

    Art or Artifice? Large Language Models and the False Promise of Creativity. (arXiv:2309.14556v1 [cs.CL])

    [http://arxiv.org/abs/2309.14556](http://arxiv.org/abs/2309.14556)

    本研究通过提出创造性写作的托兰斯测验(TTCW)来评估大型语言模型(LLMs)的写作创造力。结果表明，LLM生成的故事在创意测试中通过的数量比专业作家写的故事少。此外，我们发现LLMs无法代替专家进行TTCW评估。

    

    研究人员认为，大型语言模型(LLMs)具有从博客到故事的高质量写作能力。然而，客观评估一段文字的创造力是具有挑战性的。受创造性思维的托兰斯测验(TTC)的启发，我们使用共识评估技术[3]，提出了创造性写作的托兰斯测验(TTCW)来评估创造力作为一个产品。TTCW由包含在流畅度、灵活性、独创性和细致度原始维度中的14个二元测试组成。我们招募了10位创意作家，并使用TTCW对48个由专业作家或LLMs撰写的故事进行人工评估。我们的分析表明，LLM生成的故事通过的TTCW测试比专业作家写的故事少了3-10倍。此外，我们探索了使用LLMs作为评价者，以自动化TTCW评估，结果显示没有一个LLM与专家评估呈正相关。

    Researchers have argued that large language models (LLMs) exhibit high-quality writing capabilities from blogs to stories. However, evaluating objectively the creativity of a piece of writing is challenging. Inspired by the Torrance Test of Creative Thinking (TTCT), which measures creativity as a process, we use the Consensual Assessment Technique [3] and propose the Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product. TTCW consists of 14 binary tests organized into the original dimensions of Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative writers and implement a human assessment of 48 stories written either by professional authors or LLMs using TTCW. Our analysis shows that LLM-generated stories pass 3-10X less TTCW tests than stories written by professionals. In addition, we explore the use of LLMs as assessors to automate the TTCW evaluation, revealing that none of the LLMs positively correlate with the expert assessments.
    
[^256]: 在双人对话中解码情感：通过句子嵌入利用语义相似性

    Decoding Affect in Dyadic Conversations: Leveraging Semantic Similarity through Sentence Embedding. (arXiv:2309.12646v1 [cs.CL])

    [http://arxiv.org/abs/2309.12646](http://arxiv.org/abs/2309.12646)

    本研究通过利用句子嵌入和语义相似性，解码了双人对话中的情感，并发现在冲突对话中，妻子的情感与语义相似性呈正相关。

    

    自然语言处理的最新进展突显了句子嵌入在测量语义相似性方面的潜力。然而，其在分析现实中的双人互动并预测对话参与者情感方面的应用仍然很少。为了弥补这一空白，本研究利用50对夫妻之间关于冲突和愉快活动的口头对话。采用基于Transformer的模型all-MiniLM-L6-v2来获得每个发言者话语的嵌入。然后，通过嵌入相邻话语之间的余弦相似性的平均值对对话的整体相似性进行量化。结果显示，语义相似性与妻子在冲突对话中的情感呈正相关（但在愉快对话中不相关）。此外，无论对话类型如何，都未观察到这种相关性与丈夫的情感之间。两个验证检验进一步支持了t

    Recent advancements in Natural Language Processing (NLP) have highlighted the potential of sentence embeddings in measuring semantic similarity. Yet, its application in analyzing real-world dyadic interactions and predicting the affect of conversational participants remains largely uncharted. To bridge this gap, the present study utilizes verbal conversations within 50 married couples talking about conflicts and pleasant activities. Transformer-based model all-MiniLM-L6-v2 was employed to obtain the embeddings of the utterances from each speaker. The overall similarity of the conversation was then quantified by the average cosine similarity between the embeddings of adjacent utterances. Results showed that semantic similarity had a positive association with wives' affect during conflict (but not pleasant) conversations. Moreover, this association was not observed with husbands' affect regardless of conversation types. Two validation checks further provided support for the validity of t
    
[^257]: 问句生成的自动回答可行性评估

    Automatic Answerability Evaluation for Question Generation. (arXiv:2309.12546v1 [cs.CL])

    [http://arxiv.org/abs/2309.12546](http://arxiv.org/abs/2309.12546)

    这项工作提出了一种新颖的自动评估指标，用于评估问句生成任务中生成的问题是否可以由参考答案回答。实验证明该指标结果可靠，并与人工评价一致。并且这个指标可以补充传统的指标。

    

    传统的自动评估指标，如BLEU和ROUGE，是为自然语言生成（NLG）任务开发的，它们基于生成文本与参考文本之间的n-gram重叠度来衡量。这些简单的评估指标可能对于更复杂的任务，比如问句生成（QG），是不足够的，因为QG需要生成可以由参考答案回答的问题。因此，开发更复杂的自动评估指标仍然是QG研究中的一个紧迫问题。本文提出了一种基于提示的可回答性度量（PMAN），一种新颖的自动评估指标，用于评估问句生成任务中生成的问题是否可以由参考答案回答。大量实验证明，该评估结果可靠，并与人工评价一致。我们进一步应用我们的指标来评估QG模型的性能，结果显示我们的指标补充了传统的指标。我们基于ChatGPT的QG模型的实现取得了令人满意的成绩。

    Conventional automatic evaluation metrics, such as BLEU and ROUGE, developed for natural language generation (NLG) tasks, are based on measuring the n-gram overlap between the generated and reference text. These simple metrics may be insufficient for more complex tasks, such as question generation (QG), which requires generating questions that are answerable by the reference answers. Developing a more sophisticated automatic evaluation metric, thus, remains as an urgent problem in QG research. This work proposes a Prompting-based Metric on ANswerability (PMAN), a novel automatic evaluation metric to assess whether the generated questions are answerable by the reference answers for the QG tasks. Extensive experiments demonstrate that its evaluation results are reliable and align with human evaluations. We further apply our metric to evaluate the performance of QG models, which shows our metric complements conventional metrics. Our implementation of a ChatGPT-based QG model achieves stat
    
[^258]: 使用强化学习的离散提示压缩

    Discrete Prompt Compression with Reinforcement Learning. (arXiv:2308.08758v1 [cs.CL])

    [http://arxiv.org/abs/2308.08758](http://arxiv.org/abs/2308.08758)

    本研究提出了一种使用强化学习的离散提示压缩方法（PCRL），以解决指令调整的语言模型中嵌入训练的挑战。PCRL采用了一种计算效率高的策略网络直接编辑提示，可以灵活应用于各种类型的LM，而不需要梯度访问或标记数据。

    

    指令调整的语言模型（LM）被用户广泛使用来解决与任务特定提示相关的各种问题。由于上下文窗口长度和计算成本的限制，鼓励开发压缩提示的方法。现有方法严重依赖于训练嵌入，这些嵌入被设计为容纳多个记号含义。这在解释性、固定数量的嵌入记号、在不同LM之间的可重用性以及与黑盒API交互时的不适用性方面带来了挑战。本研究提出了一种使用强化学习的提示压缩方法（PCRL），它解决了这些问题。PCRL采用了一种计算效率高的策略网络，直接编辑提示。PCRL的训练方法可以灵活地应用于各种类型的LM，以及只有解码器和编码器-解码器架构，而不需要使用梯度访问LM或标记数据进行训练。

    Instruction-tuned Language Models (LMs) are widely used by users to address various problems with task-specific prompts. Constraints associated with the context window length and computational costs encourage the development of compressed prompts. Existing methods rely heavily on training embeddings, which are designed to accommodate multiple token meanings. This presents challenges in terms of interpretability, a fixed number of embedding tokens, reusability across different LMs, and inapplicability when interacting with black-box APIs. This study proposes prompt compression with reinforcement learning (PCRL), a novel discrete prompt compression method that addresses these issues. PCRL employs a computationally efficient policy network that directly edits prompts. The PCRL training approach can be flexibly applied to various types of LMs, as well as decoder-only and encoder-decoder architecture, and can be trained without gradient access to LMs or labeled data. PCRL achieves an averag
    
[^259]: LLMeBench：用于加速LLMs基准测试的灵活框架

    LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking. (arXiv:2308.04945v1 [cs.CL])

    [http://arxiv.org/abs/2308.04945](http://arxiv.org/abs/2308.04945)

    LLMeBench是一个灵活的框架，用于加速LLMs基准测试。它可以定制任何NLP任务和模型，无论语言，支持零和少样本学习设置，并允许用户添加新的自定义数据集。已经在31个独特的NLP任务上进行了测试，并计划将框架开源。

    

    最近大型语言模型（LLMs）的发展和成功使得需要评估它们在不同语言的各种NLP任务中的性能。尽管已经开发并公开了几个框架，但对于不同用户来说，它们对特定任务和数据集的定制能力通常很复杂。在这项研究中，我们引入了LLMeBench框架。最初是为了使用OpenAI的GPT和BLOOM模型评估阿拉伯语NLP任务而开发的；它可以无缝定制任何NLP任务和模型，无论语言如何。该框架还具有零和少样本学习设置。可以在不到10分钟内添加新的自定义数据集，并且用户可以使用自己的模型API密钥来评估当前任务。该框架已经在90个实验设置中使用53个公开可用数据集对31个独特的NLP任务进行了测试，涉及大约296K个数据点。我们计划将该框架开源供社区使用。

    The recent development and success of Large Language Models (LLMs) necessitate an evaluation of their performance across diverse NLP tasks in different languages. Although several frameworks have been developed and made publicly available, their customization capabilities for specific tasks and datasets are often complex for different users. In this study, we introduce the LLMeBench framework. Initially developed to evaluate Arabic NLP tasks using OpenAI's GPT and BLOOM models; it can be seamlessly customized for any NLP task and model, regardless of language. The framework also features zero- and few-shot learning settings. A new custom dataset can be added in less than 10 minutes, and users can use their own model API keys to evaluate the task at hand. The developed framework has been already tested on 31 unique NLP tasks using 53 publicly available datasets within 90 experimental setups, involving approximately 296K data points. We plan to open-source the framework for the community
    
[^260]: 集成大型语言模型和主动推理以理解阅读和阅读障碍中的眼动行为

    Integrating large language models and active inference to understand eye movements in reading and dyslexia. (arXiv:2308.04941v1 [q-bio.NC])

    [http://arxiv.org/abs/2308.04941](http://arxiv.org/abs/2308.04941)

    该论文提出了一种集成大型语言模型和主动推理的计算模型，用于模拟阅读过程中的眼动行为。该模型能够准确地预测和推理不同粒度的文本信息，并能够模拟阅读障碍中不适应推理效果的情况。

    

    我们提出了一种新颖的计算模型，采用层次化主动推理来模拟阅读和眼动行为。该模型将语言处理描述为对层次生成模型的推理，从音节到句子的不同粒度实现预测和推理。我们的方法结合了大型语言模型的优势，用于实现逼真的文本预测，以及主动推理用于引导眼动到信息丰富的文本信息，从而使得对预测进行测试成为可能。该模型能够熟练阅读已知和未知的单词和句子，并遵循阅读双路理论中的词汇和非词汇路径的区分。值得注意的是，我们的模型允许模拟阅读过程中对眼动行为产生不适应推理效果的情况，例如阅读障碍。为了模拟这种情况，我们在阅读过程中减弱了先验的贡献，导致不正确的推理和更加断片化的阅读。

    We present a novel computational model employing hierarchical active inference to simulate reading and eye movements. The model characterizes linguistic processing as inference over a hierarchical generative model, facilitating predictions and inferences at various levels of granularity, from syllables to sentences.  Our approach combines the strengths of large language models for realistic textual predictions and active inference for guiding eye movements to informative textual information, enabling the testing of predictions. The model exhibits proficiency in reading both known and unknown words and sentences, adhering to the distinction between lexical and nonlexical routes in dual-route theories of reading. Notably, our model permits the exploration of maladaptive inference effects on eye movements during reading, such as in dyslexia. To simulate this condition, we attenuate the contribution of priors during the reading process, leading to incorrect inferences and a more fragmented
    
[^261]: 一种几何观念的因果探测

    A Geometric Notion of Causal Probing. (arXiv:2307.15054v1 [cs.CL])

    [http://arxiv.org/abs/2307.15054](http://arxiv.org/abs/2307.15054)

    本文提出了一种几何观念的因果探测方法，通过在语言模型表示空间的子空间上进行反事实干预，优化了因果概念子空间，以实现概念控制生成。

    

    大型语言模型依赖于文本的实值表示来进行预测。这些表示包含了模型在训练数据上学到的信息，包括语言属性和基于性别的人口偏见等。越来越多的研究关注通过在表示空间的子空间上进行正交投影来获得关于这些概念的信息。我们通过提出语言模型表示空间子空间的内在信息的形式定义，为这项研究贡献了新的内容。我们提出了一种反事实方法来避免虚假相关的失效模式，通过独立处理子空间中的分量和其正交补空间中的分量。我们展示了在子空间中的反事实信息概念是由一个因果概念子空间进行优化的。此外，这种干预使我们能够通过操作来尝试概念控制生成。

    Large language models rely on real-valued representations of text to make their predictions. These representations contain information learned from the data that the model has trained on, including knowledge of linguistic properties and forms of demographic bias, e.g., based on gender. A growing body of work has considered information about concepts such as these using orthogonal projections onto subspaces of the representation space. We contribute to this body of work by proposing a formal definition of intrinsic information in a subspace of a language model's representation space. We propose a counterfactual approach that avoids the failure mode of spurious correlations (Kumar et al., 2022) by treating components in the subspace and its orthogonal complement independently. We show that our counterfactual notion of information in a subspace is optimizing by an causal concept subspace. Furthermore, this intervention allows us to attempt concept controlled generation by manipulating the
    
[^262]: 一种具有规划、长期上下文理解和程序合成能力的现实世界WebAgent

    A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. (arXiv:2307.12856v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.12856](http://arxiv.org/abs/2307.12856)

    这篇论文介绍了一种名为WebAgent的LLM驱动代理，通过自我经验学习，在真实网站上完成任务。该方法通过规划、总结和生成代码来提高在真实网站上的成功率。

    

    最近，预训练的大型语言模型（LLMs）在自主Web自动化方面取得了更好的泛化性能和样本效率。然而，在真实世界的网站上，性能仍然受到三个方面的限制：开放领域性、有限的上下文长度和对HTML的归纳偏差的缺乏。我们介绍了一种名为WebAgent的LLM驱动代理，它通过自我经验学习，在遵循自然语言指令的前提下，在真实网站上完成任务。WebAgent通过将指令分解为规范的子指令，将长HTML文档总结为与任务相关的片段，并通过从中生成的Python程序对网站进行操作来提前进行规划。我们使用Flan-U-PaLM设计了WebAgent，用于生成有根代码，并使用HTML-T5进行预训练LLMs，利用局部和全局注意机制以及混合长跨度去噪目标来进行规划和总结。我们通过实验证明，我们的模块化方法提高了在真实网站上的成功率。

    Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by ov
    
[^263]: 从生成模型的角度重新审视实体对齐及超越：一个视角

    Revisit and Outstrip Entity Alignment: A Perspective of Generative Models. (arXiv:2305.14651v1 [cs.CL])

    [http://arxiv.org/abs/2305.14651](http://arxiv.org/abs/2305.14651)

    本文重新从生成模型的角度研究了基于嵌入的实体对齐（EEA）问题，引入基于生成对抗网络的EEA方法及提出的生成的EEA（GEEA）框架，通过互相变分自动编码器（M-VAE）实现实体从一个KG转换到另一个KG，并且从随机噪声向量生成新的实体，具有较好的效果。

    

    最近，基于嵌入的方法在利用多模态知识图谱（KG）嵌入的实体对齐方面取得了巨大成功。在本文中，我们从生成模型的角度研究了基于嵌入的实体对齐（EEA）。我们表明EEA是一个特殊的问题，其主要目标类似于典型生成模型中的目标，基于这个目标，我们从理论上证明了最近发展的基于生成对抗网络（GAN）的EEA方法的有效性。然后，我们揭示了他们不完整的目标限制了实体对齐和实体合成（即生成新实体）的能力。我们通过引入生成的EEA（abbr.，GEEA）框架和提出的互相变分自动编码器（M-VAE）作为生成模型来缓解这个问题。M-VAE可以将一个实体从一个KG转换到另一个KG，并从随机噪声向量生成新实体。我们通过理论分析和实证实验展示了GEEA的优势。

    Recent embedding-based methods have achieved great successes on exploiting entity alignment from knowledge graph (KG) embeddings of multiple modals. In this paper, we study embedding-based entity alignment (EEA) from a perspective of generative models. We show that EEA is a special problem where the main objective is analogous to that in a typical generative model, based on which we theoretically prove the effectiveness of the recently developed generative adversarial network (GAN)-based EEA methods. We then reveal that their incomplete objective limits the capacity on both entity alignment and entity synthesis (i.e., generating new entities). We mitigate this problem by introducing a generative EEA (abbr., GEEA) framework with the proposed mutual variational autoencoder (M-VAE) as the generative model. M-VAE can convert an entity from one KG to another and generate new entities from random noise vectors. We demonstrate the power of GEEA with theoretical analysis and empirical experime
    
[^264]: 在高考基准测试上评估大型语言模型的性能

    Evaluating the Performance of Large Language Models on GAOKAO Benchmark. (arXiv:2305.12474v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12474](http://arxiv.org/abs/2305.12474)

    本文介绍了一个基于高考考试问题的基准测试GAOKAO-Benchmark，用于评估大型语言模型在客观和主观问题方面的表现。通过对ChatGPT模型的评估，研究发现其在客观问题方面表现出色，同时也揭示了其不足之处和改进的方向。

    

    大型语言模型已经在各种自然语言处理任务中展示了出色的性能；然而它们在更具挑战性和领域特定的任务中的功效仍然不太清楚。本文介绍了GAOKAO-Benchmark（GAOKAO-Bench），这是一个直观的基准测试，它使用中国高考考试的题目作为测试样本，评估大型语言模型。为了尽可能地使评估结果与人类一致，我们设计了一种基于零-shot提示的方法，通过将问题分为主观和客观类型来分析模型的准确性和评分率。我们评估了ChatGPT模型在GAOKAO-Benchmark性能上的表现。我们的研究发现，ChatGPT模型在解决客观问题方面表现出色，同时也揭示了其不足之处和改进的方向。为了进一步审查模型的响应，我们加入了人类评估。总之，本研究为创建一个稳健的评估GAOKAO基准测试提供了贡献。

    Large language models have demonstrated remarkable performance across various natural language processing tasks; however, their efficacy in more challenging and domain-specific tasks remains less explored. This paper introduces the GAOKAO-Benchmark (GAOKAO-Bench), an intuitive benchmark that employs questions from the Chinese Gaokao examination as test samples for evaluating large language models.In order to align the evaluation results with humans as much as possible, we designed a method based on zero-shot prompts to analyze the accuracy and scoring rate of the model by dividing the questions into subjective and objective types. We evaluated the ChatGPT model on GAOKAO-Benchmark performance.Our findings reveal that the ChatGPT model excels in tackling objective questions, while also shedding light on its shortcomings and areas for improvement. To further scrutinize the model's responses, we incorporate human evaluations.In conclusion, this research contributes a robust evaluation ben
    
[^265]: 小型语言模型更适合作为黑匣子机器生成文本检测器

    Smaller Language Models are Better Black-box Machine-Generated Text Detectors. (arXiv:2305.09859v1 [cs.CL])

    [http://arxiv.org/abs/2305.09859](http://arxiv.org/abs/2305.09859)

    本文研究发现，小型语言模型更适用于作为通用文本检测器，可以更加精确地检测出机器生成的文本，而检测器和生成模型是否具有相同的架构或语料库并不会对检测性能产生显著影响。

    

    随着流畅的生成语言模型的出现，它们可以生成与人类写作的非常相似的令人信服的话语，因此区分一段文本是由机器生成的还是人类写作的变得更加具有挑战性和重要性，因为这样的模型可以用于传播错误信息、虚假新闻、虚假评论并模仿某些作者和人物。为此，已经提出了许多检测机器生成文本的方法。其中大部分方法需要访问目标模型的 logits，或需要可以从目标模型中进行采样的能力。其中一种黑匣子检测方法依赖于观察到生成文本在生成器的似然函数下是局部最优的，而人类写作的文本则不是。我们发现，总体而言，较小且部分训练的模型更适合作为通用文本检测器：它们可以更精确地检测来自小型和大型模型的生成文本。有趣的是，我们发现检测器和生成模型是否具有相同的架构或相同的语料库对检测性能没有显著影响。

    With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generat
    
[^266]: 利用伪图像说明进行多模态摘要

    Exploiting Pseudo Image Captions for Multimodal Summarization. (arXiv:2305.05496v1 [cs.CL])

    [http://arxiv.org/abs/2305.05496](http://arxiv.org/abs/2305.05496)

    本文研究了跨模态对比学习中（部分）误负样本的挑战，并提出了一种从更一般下界形式的指导下调节跨模态相似度的对比学习策略，以更精确地优化图像/文本锚定点和其负面文本/图像之间的互信息。

    

    视觉语言预训练中的跨模态对比学习面临（部分）误负样本的挑战。本文从互信息（MI）优化的角度研究了该问题。我们理论上证明了当存在噪声时，包括负样本的MI也很重要。在更一般的优化下界形式的指导下，我们提出了一种由逐步细化的跨模态相似度调节的对比学习策略，以更精确地优化图像/文本锚定点和其负面文本/图像之间的MI，而不是错误地将其最小化。在四个下游跨模态任务上，我们的方法表现竞争力，并在理论指导下系统地平衡了（部分）误负样本的有利和有害效果。

    Cross-modal contrastive learning in vision language pretraining (VLP) faces the challenge of (partial) false negatives. In this paper, we study this problem from the perspective of Mutual Information (MI) optimization. It is common sense that InfoNCE loss used in contrastive learning will maximize the lower bound of MI between anchors and their positives, while we theoretically prove that MI involving negatives also matters when noises commonly exist. Guided by a more general lower bound form for optimization, we propose a contrastive learning strategy regulated by progressively refined cross-modal similarity, to more accurately optimize MI between an image/text anchor and its negative texts/images instead of improperly minimizing it. Our method performs competitively on four downstream cross-modal tasks and systematically balances the beneficial and harmful effects of (partial) false negative samples under theoretical guidance.
    
[^267]: 大型语言模型能否改变计算社会科学？

    Can Large Language Models Transform Computational Social Science?. (arXiv:2305.03514v1 [cs.CL])

    [http://arxiv.org/abs/2305.03514](http://arxiv.org/abs/2305.03514)

    本文研究了大型语言模型作为计算社会科学工具的潜力。虽然在分类任务上没有优势，但在自由形式编码任务上表现优异，今后可以作为零-shot检测工具进行使用，

    

    ChatGPT等大型语言模型(LLMs)能够成功地在许多语言处理任务中进行零-shot操作（无需训练数据）。如果这种能力也适用于对说服力和政治意识形态等社会现象的编码，那么LLMs就可以有效地改变计算社会科学(CSS)。本研究提供了使用LLMs作为CSS工具的路线图。为此，我们提供了一组优秀的提示实践以及一个广泛的评估流程，以测量13种语言模型在24个代表性的CSS基准测试上的零-shot性能。在分类任务上，LLMs无法超越最佳微调模型，但仍然与人类达成了公平的协议水平。在自由形式的编码任务（生成）上，LLMs生成的解释常常超过了工作者的黄金参考的质量。我们得出结论，今天的LLMs可以通过两种方式从根本上增强CSS研究流程：(1)作为零-shot检测工具进行无缝工作。

    Large Language Models (LLMs) like ChatGPT are capable of successfully performing many language processing tasks zero-shot (without the need for training data). If this capacity also applies to the coding of social phenomena like persuasiveness and political ideology, then LLMs could effectively transform Computational Social Science (CSS). This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 24 representative CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that today's LLMs can radically augment the CSS research pipeline in two ways: (1) serving as zero-shot d
    
[^268]: 使用大语言模型的机器翻译新趋势：以ChatGPT为例

    New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT. (arXiv:2305.01181v1 [cs.CL])

    [http://arxiv.org/abs/2305.01181](http://arxiv.org/abs/2305.01181)

    本文提出了使用大型语言模型的机器翻译中的几个新方向，包括风格化MT、交互式MT和基于翻译记忆的MT，并讨论了隐私问题的解决方案。

    

    近年来，机器翻译（MT）在深度学习的推动下取得了显著进展，特别是在GPT-3和ChatGPT等大型语言模型（LLMs）的出现后。这为使用LLMs的MT带来了新的挑战和机遇。本文提出了一些有趣的使用LLMs的MT方向，包括风格化MT、交互式MT和基于翻译记忆的MT，以及一种使用LLMs的新评估范例。同时，我们还讨论了使用LLMs的MT中的隐私问题，并提出了一种基本的隐私保护方法以减轻此类风险。为了说明我们提出的方法的潜力，我们给出了几个以上提到的新方向的示例，展示了所提出方向的可行性，并突出了使用LLMs的MT未来研究的机会和挑战。

    Machine Translation (MT) has made significant progress in recent years using deep learning, especially after the emergence of large language models (LLMs) such as GPT-3 and ChatGPT. This brings new challenges and opportunities for MT using LLMs. In this paper, we brainstorm some interesting directions for MT using LLMs, including stylized MT, interactive MT, and Translation Memory-based MT, as well as a new evaluation paradigm using LLMs. We also discuss the privacy concerns in MT using LLMs and a basic privacy-preserving method to mitigate such risks. To illustrate the potential of our proposed directions, we present several examples for the new directions mentioned above, demonstrating the feasibility of the proposed directions and highlight the opportunities and challenges for future research in MT using LLMs.
    

