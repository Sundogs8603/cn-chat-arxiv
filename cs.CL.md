# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Interpretation of Intracardiac Electrograms Through Textual Representations](https://rss.arxiv.org/abs/2402.01115) | 本研究首次利用预训练的语言模型，通过文本表示的方式对心内电图进行插值和房颤分类。相比其他表示方法，我们的方法在房颤分类上表现出竞争性的性能。 |
| [^2] | [KoCoNovel: Annotated Dataset of Character Coreference in Korean Novels](https://arxiv.org/abs/2404.01140) | KoCoNovel是一个从韩国文学文本中衍生出的小说人物共指数据集，提供了详细的注释指南，是韩语公共共指解决语料库中第二大规模的数据集，也是第一个基于文学文本的数据集 |
| [^3] | [Source-Aware Training Enables Knowledge Attribution in Language Models](https://arxiv.org/abs/2404.01019) | 源感知训练使语言模型具备知识归因能力，进而增强了其透明度、可解释性和可验证性。 |
| [^4] | [MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in Conversations with Multimodal Language Models](https://arxiv.org/abs/2404.00511) | 本文提出了一种集成文本、音频和视觉模态的多模态情绪识别和多模态情绪原因提取框架，通过利用专门的情绪编码器和模态特定特征，实现了提升情绪理解和因果推理的竞争性成果。 |
| [^5] | [Concept-based Analysis of Neural Networks via Vision-Language Models](https://arxiv.org/abs/2403.19837) | 本文提出利用视觉语言模型作为透镜，通过其隐含的高层次概念来进行对视觉模型的分析。 |
| [^6] | [DANCER: Entity Description Augmented Named Entity Corrector for Automatic Speech Recognition](https://arxiv.org/abs/2403.17645) | DANCER提出了一种新颖的Description Augmented Named entity CorrEctoR（DANCER）模型，通过利用实体描述为自动语音识别中的NEC提供额外信息，帮助缓解NE列表中的音素混淆问题。 |
| [^7] | [A Multi-Label Dataset of French Fake News: Human and Machine Insights](https://arxiv.org/abs/2403.16099) | 通过建立一份包括 100 篇文档的多标签数据集 OBSINFOX，研究了人类与机器在认定假新闻特征上的差异，并发现了语料库中讽刺文本的普遍存在。 |
| [^8] | [StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows](https://arxiv.org/abs/2403.11322) | 提出了一种使用StateFlow的新颖LLM任务解决范式，将复杂任务解决过程概念化为状态机，通过状态转换确保LLM响应的清晰跟踪和管理。 |
| [^9] | [Gemma: Open Models Based on Gemini Research and Technology](https://arxiv.org/abs/2403.08295) | Gemma是基于Gemini研究和技术所构建的开放模型系列，在语言理解、推理和安全性等方面表现出色，负责任地发布这些大型语言模型对于提高前沿模型的安全性至关重要。 |
| [^10] | [Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks](https://arxiv.org/abs/2403.04814) | 该研究引入了一个新的基准SAFIM用于评估LLMs在代码填空任务上的句法感知完成表现，发现FIM预训练不仅提高了FIM的熟练度，还改善了LLMs的左到右推理，挑战了传统观念并表明预训练方法和数据品质对模型性能的影响更甚于模型大小。 |
| [^11] | [Me LLaMA: Foundation Large Language Models for Medical Applications](https://arxiv.org/abs/2402.12749) | Me LLaMA是一个医学领域的大型语言模型系列，通过持续预训练和指导调整在大型医学数据集上训练而成，其在零-shot和少-shot学习方面表现优于现有的医学语言模型和商业巨头ChatGPT。 |
| [^12] | [Driving Everywhere with Large Language Model Policy Adaptation](https://arxiv.org/abs/2402.05932) | 本文介绍了LLaDA，一种使用大型语言模型的工具，使得驾驶员和自动驾驶车辆能够在各地驾驶，通过将任务和运动计划调整到新位置的交通规则。通过广泛的用户研究，证明了LLaDA指导在解决意外情况和适应AV运动计划策略方面的有效性。 |
| [^13] | [Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective](https://arxiv.org/abs/2312.01957) | 本文提出了一种将RLAIF解释为贝叶斯推断的方法，通过经过精炼的自我批评对LLM的输出进行精炼，为获得微调模型提供了一种可行且廉价的替代方案。 |
| [^14] | [A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models.](http://arxiv.org/abs/2311.00445) | 这项研究通过对比人类和语言模型在三段论推理中的表现，发现较大的语言模型更合逻辑，甚至比人类更合逻辑，但即使最大的语言模型也会出现与人类推理类似的错误，总体上认为语言模型在某些情况下能够克服人类偏见。 |
| [^15] | [The Impact of Depth and Width on Transformer Language Model Generalization.](http://arxiv.org/abs/2310.19956) | 深层的transformer语言模型在组合式泛化能力上比浅层模型更好，但额外层数的相对收益会迅速减小。 |
| [^16] | [Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring.](http://arxiv.org/abs/2310.15461) | 本文研究了人机语言模型交互如何支持自主引导式心理健康干预，通过以认知重建作为案例研究。研究结果显示，我们的系统对参与者情绪强度产生积极影响，并帮助他们克服了负面思维。 |
| [^17] | [CrisisTransformers: Pre-trained language models and sentence encoders for crisis-related social media texts.](http://arxiv.org/abs/2309.05494) | CrisisTransformers是一组针对危机相关文本优化的预训练语言模型和句子编码器，旨在有效处理危机相关社交媒体文本，并为紧急响应者提供综合视图。 |
| [^18] | [ChatGPT and Simple Linguistic Inferences: Blind Spots and Blinds.](http://arxiv.org/abs/2305.14785) | 本文研究了ChatGPT语言模型的局限性，指出其在简单语言推断任务中存在困难，并探讨了如何改善其对基本语言概念的理解。 |
| [^19] | [Impact of Position Bias on Language Models in Token Classification.](http://arxiv.org/abs/2304.13567) | 研究了语言模型在token分类任务中的位置偏差问题，通过实验表明在具体任务中，BERT、ERNIE、ELECTRA等编码器以及GPT2和BLOOM等解码器的平均性能下降了3%和9%。 |

# 详细

[^1]: 通过文本表示解读心内电图

    Interpretation of Intracardiac Electrograms Through Textual Representations

    [https://rss.arxiv.org/abs/2402.01115](https://rss.arxiv.org/abs/2402.01115)

    本研究首次利用预训练的语言模型，通过文本表示的方式对心内电图进行插值和房颤分类。相比其他表示方法，我们的方法在房颤分类上表现出竞争性的性能。

    

    理解房颤(AFib)的不规则电活动一直是心电图学中的一个重要挑战。对于严重的房颤病例，进行导管消融以获取心内电图(EGMs)。EGMs提供了心脏电活动的复杂细节和局部化信息，是可解释的心脏研究的理想模式。近年来，人工智能(AI)的进展使得一些研究可以利用深度学习框架来解释房颤中的EGMs。此外，语言模型(LMs)在能够推广到未见过的领域方面表现出了出色的性能，尤其在医疗领域。在本研究中，我们首次利用预训练的LMs来通过掩码语言建模对EGM插值和房颤分类进行微调。我们将EGM形式化为文本序列，并与其他表示方法相比，在房颤分类方面展示了竞争性的性能。最后，我们提供了全面的解释性分析。

    Understanding the irregular electrical activity of atrial fibrillation (AFib) has been a key challenge in electrocardiography. For serious cases of AFib, catheter ablations are performed to collect intracardiac electrograms (EGMs). EGMs offer intricately detailed and localized electrical activity of the heart and are an ideal modality for interpretable cardiac studies. Recent advancements in artificial intelligence (AI) has allowed some works to utilize deep learning frameworks to interpret EGMs during AFib. Additionally, language models (LMs) have shown exceptional performance in being able to generalize to unseen domains, especially in healthcare. In this study, we are the first to leverage pretrained LMs for finetuning of EGM interpolation and AFib classification via masked language modeling. We formulate the EGM as a textual sequence and present competitive performances on AFib classification compared against other representations. Lastly, we provide a comprehensive interpretabilit
    
[^2]: KoCoNovel：韩国小说中人物共指的注释数据集

    KoCoNovel: Annotated Dataset of Character Coreference in Korean Novels

    [https://arxiv.org/abs/2404.01140](https://arxiv.org/abs/2404.01140)

    KoCoNovel是一个从韩国文学文本中衍生出的小说人物共指数据集，提供了详细的注释指南，是韩语公共共指解决语料库中第二大规模的数据集，也是第一个基于文学文本的数据集

    

    我们提出了KoCoNovel，这是一个从韩国文学文本中衍生出的小说人物共指数据集，配备了详细的注释指南。KoCoNovel由50部现代和当代韩国小说中的178,000个标记组成，是继NIKL语料库之后韩语公共共指解决语料库中第二大规模的数据集，也是第一个基于文学文本的数据集。为了扩大其实用性，我们提供了四个不同版本的KoCoNovel，为全知作者和读者的观点以及处理多实体的分离或重叠提供选择。这种方法整合了围绕文学文本共指解析的现有话语，为研究提供了一个全面的数据集。KoCoNovel的一个显著特点是24%的人物提及是单一普通名词，缺少所有格标记或冠词。这一特征特别受到韩语称谓的细微差别的影响。

    arXiv:2404.01140v1 Announce Type: new  Abstract: We present KoCoNovel, an novel character coreference dataset derived from Korean literary texts, complete with detailed annotation guidelines. Comprising 178K tokens from 50 modern and contemporary Korean novels, KoCoNovel stands as the second-largest public coreference resolution corpus in Korean, after the NIKL corpus, and the first to be based on literary texts. To broaden its utility, we provide four distinct versions of KoCoNovel, offering options for the perspectives of the omniscient author and readers, and for handling multiple entities as either separate or overlapping. This approach integrates existing discourse surrounding coreference resolution in literary texts, providing a comprehensive dataset for exploration. One of KoCoNovel's distinctive features is that 24% of all character mentions are single common nouns, lacking possessive markers or articles. This feature is particularly influenced by the nuances of Korean address 
    
[^3]: 源感知训练使语言模型具备知识归因能力

    Source-Aware Training Enables Knowledge Attribution in Language Models

    [https://arxiv.org/abs/2404.01019](https://arxiv.org/abs/2404.01019)

    源感知训练使语言模型具备知识归因能力，进而增强了其透明度、可解释性和可验证性。

    

    大型语言模型（LLMs）在预训练期间学到了大量知识，但往往对此类知识的来源毫不在意。本文研究了内在源引用问题，要求LLMs引用支持生成响应的预训练来源。内在源引用可以增强LLMs的透明度、可解释性和可验证性。为赋予LLMs这种能力，我们探索了源感知训练——一个后预训练配方，包括（i）训练LLMs将唯一源文档标识符与每个文档中的知识关联起来，然后（ii）进行指示调整，教导LLMs在被提示时引用支持的预训练来源。源感知训练可以轻松应用于即插即用的预训练LLMs，并与现有的预训练/微调框架的差异最小。通过对精心策划的数据进行实验，我们展示了我们的训练配方可以实现

    arXiv:2404.01019v1 Announce Type: cross  Abstract: Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source(s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining source supporting a generated response. Intrinsic source citation can enhance LLM transparency, interpretability, and verifiability. To give LLMs such ability, we explore source-aware training -- a post pretraining recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning to teach the LLM to cite a supporting pretraining source when prompted. Source-aware training can easily be applied to pretrained LLMs off the shelf, and diverges minimally from existing pretraining/fine-tuning frameworks. Through experiments on carefully curated data, we demonstrate that our training recipe can en
    
[^4]: MIPS在SemEval-2024任务3中的表现：使用多模态语言模型在对话中进行多模态情绪-原因对提取

    MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in Conversations with Multimodal Language Models

    [https://arxiv.org/abs/2404.00511](https://arxiv.org/abs/2404.00511)

    本文提出了一种集成文本、音频和视觉模态的多模态情绪识别和多模态情绪原因提取框架，通过利用专门的情绪编码器和模态特定特征，实现了提升情绪理解和因果推理的竞争性成果。

    

    本文介绍了我们在SemEval 2024任务3的子任务2中关于对话中多模态情绪原因分析的获奖提交。我们提出了一种新颖的多模态情绪识别和多模态情绪原因提取（MER-MCE）框架，该框架利用专门的情绪编码器整合文本、音频和视觉三种模态。我们的方法通过利用模态特定特征提升情绪理解和因果推理，使自己脱颖而出。实验评估表明了我们多模态方法的优势，我们的提交取得了竞争性的加权F1分数为0.3435，在0.0339之后排名第一的团队，仅在0.0025之后排名第二。项目链接：https://github.com/MIPS-COLT/MER-MCE.git

    arXiv:2404.00511v1 Announce Type: new  Abstract: This paper presents our winning submission to Subtask 2 of SemEval 2024 Task 3 on multimodal emotion cause analysis in conversations. We propose a novel Multimodal Emotion Recognition and Multimodal Emotion Cause Extraction (MER-MCE) framework that integrates text, audio, and visual modalities using specialized emotion encoders. Our approach sets itself apart from top-performing teams by leveraging modality-specific features for enhanced emotion understanding and causality inference. Experimental evaluation demonstrates the advantages of our multimodal approach, with our submission achieving a competitive weighted F1 score of 0.3435, ranking third with a margin of only 0.0339 behind the 1st team and 0.0025 behind the 2nd team. Project: https://github.com/MIPS-COLT/MER-MCE.git
    
[^5]: 通过视觉语言模型对神经网络进行基于概念的分析

    Concept-based Analysis of Neural Networks via Vision-Language Models

    [https://arxiv.org/abs/2403.19837](https://arxiv.org/abs/2403.19837)

    本文提出利用视觉语言模型作为透镜，通过其隐含的高层次概念来进行对视觉模型的分析。

    

    视觉深度神经网络（DNNs）的形式化分析非常可取，但由于难以表达视觉任务的形式化规范以及缺乏高效的验证程序，这是非常具有挑战性的。在本文中，我们提出利用新兴的多模态、视觉语言、基础模型（VLMs）作为一种通过其可以推理视觉模型的透镜。VLMs已经在大量图像及其文本描述上进行了训练，因此隐式地了解描述这些图像的高层次、人类可理解的概念。我们描述了一种名为$\texttt{Con}_{\texttt{spec}}$的逻辑规范语言，旨在便于按照这些概念编写规范。为了定义和形式化检查$\texttt{Con}_{\texttt{spec}}$规范，我们利用了一个VLM，它提供了一种编码和高效检查视觉模型的自然语言属性的方法。我们展示了我们的te

    arXiv:2403.19837v1 Announce Type: cross  Abstract: Formal analysis of vision-based deep neural networks (DNNs) is highly desirable but it is very challenging due to the difficulty of expressing formal specifications for vision tasks and the lack of efficient verification procedures. In this paper, we propose to leverage emerging multimodal, vision-language, foundation models (VLMs) as a lens through which we can reason about vision models. VLMs have been trained on a large body of images accompanied by their textual description, and are thus implicitly aware of high-level, human-understandable concepts describing the images. We describe a logical specification language $\texttt{Con}_{\texttt{spec}}$ designed to facilitate writing specifications in terms of these concepts. To define and formally check $\texttt{Con}_{\texttt{spec}}$ specifications, we leverage a VLM, which provides a means to encode and efficiently check natural-language properties of vision models. We demonstrate our te
    
[^6]: DANCER：针对自动语音识别的实体描述增强命名实体校正器

    DANCER: Entity Description Augmented Named Entity Corrector for Automatic Speech Recognition

    [https://arxiv.org/abs/2403.17645](https://arxiv.org/abs/2403.17645)

    DANCER提出了一种新颖的Description Augmented Named entity CorrEctoR（DANCER）模型，通过利用实体描述为自动语音识别中的NEC提供额外信息，帮助缓解NE列表中的音素混淆问题。

    

    最近提出了一系列用于ASR的快速轻量级命名实体校正（NEC）模型，通常构建在音素级编辑距离算法基础上，并展现出令人印象深刻的NEC性能。然而，随着命名实体（NE）列表的增加，NE列表中的音素混淆问题变得更加严重；例如，同音异义词的问题大大增加。鉴此，我们提出了一种新颖的描述增强型命名实体校正器（称为DANCER），利用实体描述提供额外信息，以便在ASR转录中为NEC提供辅助减轻音素混淆。

    arXiv:2403.17645v1 Announce Type: new  Abstract: End-to-end automatic speech recognition (E2E ASR) systems often suffer from mistranscription of domain-specific phrases, such as named entities, sometimes leading to catastrophic failures in downstream tasks. A family of fast and lightweight named entity correction (NEC) models for ASR have recently been proposed, which normally build on phonetic-level edit distance algorithms and have shown impressive NEC performance. However, as the named entity (NE) list grows, the problems of phonetic confusion in the NE list are exacerbated; for example, homophone ambiguities increase substantially. In view of this, we proposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER), which leverages entity descriptions to provide additional information to facilitate mitigation of phonetic confusion for NEC on ASR transcription. To this end, an efficient entity description augmented masked language model (EDA-MLM) comprised of a dense re
    
[^7]: 一份法国假新闻的多标签数据集：人类与机器视角

    A Multi-Label Dataset of French Fake News: Human and Machine Insights

    [https://arxiv.org/abs/2403.16099](https://arxiv.org/abs/2403.16099)

    通过建立一份包括 100 篇文档的多标签数据集 OBSINFOX，研究了人类与机器在认定假新闻特征上的差异，并发现了语料库中讽刺文本的普遍存在。

    

    我们提出了一个由 8 位注释者使用 11 个标签注释的来自 17 个法国被专家机构认为不可靠的新闻来源选取的 100 篇文档的语料库 OBSINFOX。通过收集比通常更多的标签和注释者，我们可以识别人类认为具有代表性的假新闻的特征，并将其与自动分类器的预测进行比较。我们使用 Gate Cloud 进行主题和体裁分析，这表明语料库中类似讽刺的文本普遍存在。然后我们使用 VAGO 主观性分析器及其神经版本，以澄清标签“主观”与标签“假新闻”之间的联系。该带有注释的数据集可通过以下网址在线获取：https://github.com/obs-info/obsinfox

    arXiv:2403.16099v1 Announce Type: new  Abstract: We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of French press considered unreliable by expert agencies, annotated using 11 labels by 8 annotators. By collecting more labels than usual, by more annotators than is typically done, we can identify features that humans consider as characteristic of fake news, and compare them to the predictions of automated classifiers. We present a topic and genre analysis using Gate Cloud, indicative of the prevalence of satire-like text in the corpus. We then use the subjectivity analyzer VAGO, and a neural version of it, to clarify the link between ascriptions of the label Subjective and ascriptions of the label Fake News. The annotated dataset is available online at the following url: https://github.com/obs-info/obsinfox   Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion, Exaggeration, French Press
    
[^8]: 使用StateFlow增强LLM任务解决能力通过状态驱动工作流

    StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows

    [https://arxiv.org/abs/2403.11322](https://arxiv.org/abs/2403.11322)

    提出了一种使用StateFlow的新颖LLM任务解决范式，将复杂任务解决过程概念化为状态机，通过状态转换确保LLM响应的清晰跟踪和管理。

    

    使用大型语言模型（LLM）来解决复杂任务的趋势日益明显，例如需要一系列操作和与工具环境动态交互的任务。本文提出了StateFlow，一种新颖的基于LLM的任务求解范式，将由LLM支持的复杂任务解决过程概念化为状态机。通过正确构建状态和定义状态转换，StateFlow确定了任务求解的进展，确保清晰跟踪和管理LLM在整个任务求解过程中的响应。在每个状态中，StateFlow允许执行一系列动作，不仅包括根据特定提示指导生成LLM响应，还包括根据需要利用外部工具。状态转换由LLM做出的特定规则或决策控制，允许通过任务的预定义StateFlow模型动态自适应地进行进展。

    arXiv:2403.11322v1 Announce Type: cross  Abstract: It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and environments. In this paper, we propose StateFlow, a novel LLM-based task-solving paradigm that conceptualizes complex task-solving processes backed by LLMs as state machines. With proper construction of states and definition of state transitions, StateFlow grounds the progress of task-solving, ensuring clear tracking and management of LLMs' responses throughout the task-solving process. Within each state, StateFlow allows execution of a series of actions, involving not only the generation of LLM's responses guided by a specific prompt, but also the utilization of external tools as needed. State transitions are controlled by specific rules or decisions made by the LLM, allowing for a dynamic and adaptive progression through the task's pre-defined StateFlow model. Evalua
    
[^9]: Gemma：基于Gemini研究和技术的开放模型

    Gemma: Open Models Based on Gemini Research and Technology

    [https://arxiv.org/abs/2403.08295](https://arxiv.org/abs/2403.08295)

    Gemma是基于Gemini研究和技术所构建的开放模型系列，在语言理解、推理和安全性等方面表现出色，负责任地发布这些大型语言模型对于提高前沿模型的安全性至关重要。

    

    本文介绍了Gemma，这是一个基于Gemini模型研究和技术构建的轻量级、最先进的开放模型系列。Gemma模型在语言理解、推理和安全性等学术基准上表现出色。我们发布了两个规模的模型（20亿和70亿参数），并提供了预训练和微调的检查点。Gemma在18个基于文本的任务中，有11个任务优于类似规模的开放模型，并对模型的安全性和责任方面进行了全面评估，同时详细描述了模型开发过程。我们相信负责任地发布大型语言模型对于提高前沿模型的安全性，并实现下一波大型语言模型创新至关重要。

    arXiv:2403.08295v1 Announce Type: cross  Abstract: This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.
    
[^10]: 在句法感知代码填空任务上评估LLMs

    Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks

    [https://arxiv.org/abs/2403.04814](https://arxiv.org/abs/2403.04814)

    该研究引入了一个新的基准SAFIM用于评估LLMs在代码填空任务上的句法感知完成表现，发现FIM预训练不仅提高了FIM的熟练度，还改善了LLMs的左到右推理，挑战了传统观念并表明预训练方法和数据品质对模型性能的影响更甚于模型大小。

    

    我们介绍了一种名为Syntax-Aware Fill-In-the-Middle（SAFIM）的新基准，用于评估大型语言模型（LLMs）在代码填空（FIM）任务上的表现。该基准侧重于程序结构的句法感知完成，如代码块和条件表达式，并包括来自多种编程语言的17,720个示例，来源于2022年4月之后的最新代码提交，以最小化数据污染。 SAFIM提供了一个强大的框架，具有各种提示设计和新颖的句法感知后处理技术，有助于在LLMs之间进行准确和公平的比较。我们对15个LLMs进行了全面评估，结果表明FIM预训练不仅提升了FIM的熟练程度，还改进了LLMs的左到右（L2R）推理。我们的发现挑战了传统观念，并表明预训练方法和数据质量对模型性能的影响大于模型大小。因此，SAFIM为未来构建

    arXiv:2403.04814v1 Announce Type: cross  Abstract: We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future 
    
[^11]: Me LLaMA: 为医疗应用构建大型语言模型的基础

    Me LLaMA: Foundation Large Language Models for Medical Applications

    [https://arxiv.org/abs/2402.12749](https://arxiv.org/abs/2402.12749)

    Me LLaMA是一个医学领域的大型语言模型系列，通过持续预训练和指导调整在大型医学数据集上训练而成，其在零-shot和少-shot学习方面表现优于现有的医学语言模型和商业巨头ChatGPT。

    

    最近，诸如ChatGPT和LLaMA等大型语言模型(LLMs)在许多人工智能应用中展现出巨大的潜力。然而，它们在医学任务上的表现不够理想，并且可以通过在大型领域特定数据集上进行训练来进一步改进。本研究引入了Me LLaMA，一个医学LLM系列，包括基础模型- Me LLaMA 13/70B及其 chat-enhanced 版本- Me LLaMA 13/70B-chat，通过持续对LLaMA2进行预训练和指导调整，使用大规模医学数据开发而成。我们用于训练和评估的领域特定数据套件包括一个具有129B tokens的大规模持续预训练数据集，一个包含214k个样本的指导调整数据集，以及跨越14个数据集的六项任务的医学评估基准(MIBE)。我们使用MIBE进行的广泛评估显示，Me LLaMA模型在零-shot和少-shot学习方面超越了现有的开源医学LLMs，并且在商业巨头如ChatGPT上表现出色。

    arXiv:2402.12749v1 Announce Type: cross  Abstract: Recent large language models (LLMs) like ChatGPT and LLaMA have shown great promise in many AI applications. However, their performance on medical tasks is suboptimal and can be further improved by training on large domain-specific datasets. This study introduces Me LLaMA, a medical LLM family including foundation models - Me LLaMA 13/70B and their chat-enhanced versions - Me LLaMA 13/70B-chat, developed through the continual pre-training and instruction tuning of LLaMA2 using large medical data. Our domain-specific data suite for training and evaluation, includes a large-scale continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a medical evaluation benchmark (MIBE) across six tasks with 14 datasets. Our extensive evaluation using MIBE shows that Me LLaMA models surpass existing open-source medical LLMs in zero-shot and few-shot learning and outperform commercial giants like ChatGPT on 
    
[^12]: 通过大型语言模型政策适应在各处驾驶

    Driving Everywhere with Large Language Model Policy Adaptation

    [https://arxiv.org/abs/2402.05932](https://arxiv.org/abs/2402.05932)

    本文介绍了LLaDA，一种使用大型语言模型的工具，使得驾驶员和自动驾驶车辆能够在各地驾驶，通过将任务和运动计划调整到新位置的交通规则。通过广泛的用户研究，证明了LLaDA指导在解决意外情况和适应AV运动计划策略方面的有效性。

    

    自动驾驶中，将驾驶行为适应新环境、习俗和法律是一个长期存在的问题，这限制了自动驾驶车辆(AVs)的广泛部署。本文介绍了LLaDA，一种简单而强大的工具，可以使人类驾驶员和自动驾驶车辆都能在各处行驶，通过将任务和运动计划调整到新位置的交通规则。LLaDA利用大型语言模型(LLMs)在解释本地驾驶手册中的交通规则时具有令人印象深刻的零-shot泛化能力。通过广泛的用户研究，我们展示了LLaDA的指导在解决现实世界中的意外情况时的有用性。我们还展示了LLaDA在真实世界数据集中调整AV运动计划策略的能力；LLaDA在所有指标上优于基线规划方法。请访问我们的网站了解更多详细信息：https://boyiliee.github.io/llada.

    Adapting driving behavior to new environments, customs, and laws is a long-standing problem in autonomous driving, precluding the widespread deployment of autonomous vehicles (AVs). In this paper, we present LLaDA, a simple yet powerful tool that enables human drivers and autonomous vehicles alike to drive everywhere by adapting their tasks and motion plans to traffic rules in new locations. LLaDA achieves this by leveraging the impressive zero-shot generalizability of large language models (LLMs) in interpreting the traffic rules in the local driver handbook. Through an extensive user study, we show that LLaDA's instructions are useful in disambiguating in-the-wild unexpected situations. We also demonstrate LLaDA's ability to adapt AV motion planning policies in real-world datasets; LLaDA outperforms baseline planning approaches on all our metrics. Please check our website for more details: https://boyiliee.github.io/llada.
    
[^13]: 使用合成数据的经过精炼的LLMs自我批评：一种贝叶斯视角

    Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective

    [https://arxiv.org/abs/2312.01957](https://arxiv.org/abs/2312.01957)

    本文提出了一种将RLAIF解释为贝叶斯推断的方法，通过经过精炼的自我批评对LLM的输出进行精炼，为获得微调模型提供了一种可行且廉价的替代方案。

    

    本文提出了将RLAIF解释为贝叶斯推断的方法，通过引入经过精炼的自我批评(dSC)，该方法通过Gibbs采样器对LLM的输出进行精炼，然后将其蒸馏成一个微调模型。只需要合成数据，dSC在涉及安全性、情感和隐私控制的实验中得到了应用，表明它可以作为对齐LLMs的一种可行且廉价的替代方案。代码在\url{https://github.com/vicgalle/distilled-self-critique}上发布。

    arXiv:2312.01957v2 Announce Type: replace  Abstract: This paper proposes an interpretation of RLAIF as Bayesian inference by introducing distilled Self-Critique (dSC), which refines the outputs of a LLM through a Gibbs sampler that is later distilled into a fine-tuned model. Only requiring synthetic data, dSC is exercised in experiments regarding safety, sentiment, and privacy control, showing it can be a viable and cheap alternative to align LLMs. Code released at \url{https://github.com/vicgalle/distilled-self-critique}.
    
[^14]: 人类和语言模型中的三段论推理的系统比较

    A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models. (arXiv:2311.00445v1 [cs.CL])

    [http://arxiv.org/abs/2311.00445](http://arxiv.org/abs/2311.00445)

    这项研究通过对比人类和语言模型在三段论推理中的表现，发现较大的语言模型更合逻辑，甚至比人类更合逻辑，但即使最大的语言模型也会出现与人类推理类似的错误，总体上认为语言模型在某些情况下能够克服人类偏见。

    

    理性行为的一个核心组成部分是逻辑推理：确定哪些结论可以从一组前提中得出。心理学家已经记录下人类推理与逻辑规则不符的几种方式。语言模型是否能够复制这些偏差，或者它们能够克服这些偏差？我们关注三段论的情况 - 从两个简单前提中推导出的推理，这在心理学中已经广泛研究 - 我们发现较大的模型比较合逻辑，而且比人类更合逻辑。与此同时，即使是最大的模型也会出现系统性错误，其中一些错误与人类推理的偏见相似，例如排序效应和逻辑谬误。总体上，我们发现语言模型模仿了训练数据中包含的人类偏见，但在某些情况下能够克服这些偏见。

    A central component of rational behavior is logical inference: the process of determining which conclusions follow from a set of premises. Psychologists have documented several ways in which humans' inferences deviate from the rules of logic. Do language models, which are trained on text generated by humans, replicate these biases, or are they able to overcome them? Focusing on the case of syllogisms -- inferences from two simple premises, which have been studied extensively in psychology -- we show that larger models are more logical than smaller ones, and also more logical than humans. At the same time, even the largest models make systematic errors, some of which mirror human reasoning biases such as ordering effects and logical fallacies. Overall, we find that language models mimic the human biases included in their training data, but are able to overcome them in some cases.
    
[^15]: 深度和宽度对Transformer语言模型泛化能力的影响

    The Impact of Depth and Width on Transformer Language Model Generalization. (arXiv:2310.19956v1 [cs.CL])

    [http://arxiv.org/abs/2310.19956](http://arxiv.org/abs/2310.19956)

    深层的transformer语言模型在组合式泛化能力上比浅层模型更好，但额外层数的相对收益会迅速减小。

    

    为了处理新的句子，语言模型（LMs）必须以组合的方式进行泛化 - 将熟悉的元素以新的方式结合起来。模型结构的哪些方面促进了组合式泛化？针对transformers，我们测试假设，即当transformers更深（具有更多层次）时，它们更容易进行组合式泛化，这个假设基于最近的理论和实证研究。由于简单地增加层数会增加总参数数量，混淆了深度和大小，我们构建了三类模型，通过以保持总参数数量恒定的方式来权衡深度和宽度（分别为41M、134M和374M个参数）。我们将所有模型预训练为LMS，然后在测试组合式泛化的任务上进行微调。我们得出了三个主要结论：（1）在微调后，深层模型在超出分布范围的情况下比浅层模型更好地进行泛化，但额外层次的相对收益迅速减小；（2）在每个模型组中，深层模型表现出更好的组合式泛化能力...（文本已截断）

    To process novel sentences, language models (LMs) must generalize compositionally -- combine familiar elements in new ways. What aspects of a model's structure promote compositional generalization? Focusing on transformers, we test the hypothesis, motivated by recent theoretical and empirical work, that transformers generalize more compositionally when they are deeper (have more layers). Because simply adding layers increases the total number of parameters, confounding depth and size, we construct three classes of models which trade off depth for width such that the total number of parameters is kept constant (41M, 134M and 374M parameters). We pretrain all models as LMs and fine-tune them on tasks that test for compositional generalization. We report three main conclusions: (1) after fine-tuning, deeper models generalize better out-of-distribution than shallower models do, but the relative benefit of additional layers diminishes rapidly; (2) within each family, deeper models show bett
    
[^16]: 通过人机语言模型交互促进自主引导式心理健康干预：认知重建的案例研究

    Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring. (arXiv:2310.15461v1 [cs.HC])

    [http://arxiv.org/abs/2310.15461](http://arxiv.org/abs/2310.15461)

    本文研究了人机语言模型交互如何支持自主引导式心理健康干预，通过以认知重建作为案例研究。研究结果显示，我们的系统对参与者情绪强度产生积极影响，并帮助他们克服了负面思维。

    

    自主引导式心理健康干预，如学习和实践应对策略的“自助工具”，在改善心理健康护理的可及性方面显示出巨大潜力。然而，这些干预常常需要认知负担和情绪触发，从而造成限制其大规模实施和普及的可及性障碍。本文研究人机语言模型交互如何支持自主引导式心理健康干预。我们以认知重建作为一个以证据为基础的治疗技术的案例研究。在一项经过IRB批准的大型心理健康网站上进行的随机现场研究中，涉及了15,531名参与者，我们设计并评估了一个使用语言模型来支持人们在认知重建的各个步骤中的系统。我们的研究结果表明，我们的系统对67%的参与者的情绪强度产生积极影响，并帮助65%的人克服消极思维。尽管青少年报道较差。

    Self-guided mental health interventions, such as "do-it-yourself" tools to learn and practice coping strategies, show great promise to improve access to mental health care. However, these interventions are often cognitively demanding and emotionally triggering, creating accessibility barriers that limit their wide-scale implementation and adoption. In this paper, we study how human-language model interaction can support self-guided mental health interventions. We take cognitive restructuring, an evidence-based therapeutic technique to overcome negative thinking, as a case study. In an IRB-approved randomized field study on a large mental health website with 15,531 participants, we design and evaluate a system that uses language models to support people through various steps of cognitive restructuring. Our findings reveal that our system positively impacts emotional intensity for 67% of participants and helps 65% overcome negative thoughts. Although adolescents report relatively worse o
    
[^17]: CrisisTransformers：用于危机相关社交媒体文本的预训练语言模型和句子编码器

    CrisisTransformers: Pre-trained language models and sentence encoders for crisis-related social media texts. (arXiv:2309.05494v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05494](http://arxiv.org/abs/2309.05494)

    CrisisTransformers是一组针对危机相关文本优化的预训练语言模型和句子编码器，旨在有效处理危机相关社交媒体文本，并为紧急响应者提供综合视图。

    

    社交媒体平台在危机沟通中起着重要作用，但由于其非正式性质，分析危机相关社交媒体文本具有挑战性。基于Transformer的预训练模型如BERT和RoBERTa在各种自然语言处理任务中表现出成功，但它们并不针对危机相关文本进行优化。此外，通用的句子编码器用于生成句子嵌入，而不考虑危机相关文本中的文本复杂性。文本分类、语义搜索和聚类等应用的进展有助于有效处理危机相关文本，这对于紧急响应者获得危机事件的综合视图至关重要，无论该事件是历史事件还是实时事件。为填补危机信息学文献中的这些空白，本研究引入了CrisisTransformers，这是一组在超过150亿个词元的推文语料库上训练的预训练语言模型和句子编码器的集成。

    Social media platforms play an essential role in crisis communication, but analyzing crisis-related social media texts is challenging due to their informal nature. Transformer-based pre-trained models like BERT and RoBERTa have shown success in various NLP tasks, but they are not tailored for crisis-related texts. Furthermore, general-purpose sentence encoders are used to generate sentence embeddings, regardless of the textual complexities in crisis-related texts. Advances in applications like text classification, semantic search, and clustering contribute to effective processing of crisis-related texts, which is essential for emergency responders to gain a comprehensive view of a crisis event, whether historical or real-time. To address these gaps in crisis informatics literature, this study introduces CrisisTransformers, an ensemble of pre-trained language models and sentence encoders trained on an extensive corpus of over 15 billion word tokens from tweets associated with more than 
    
[^18]: ChatGPT和简单的语言推断：盲点和缺陷

    ChatGPT and Simple Linguistic Inferences: Blind Spots and Blinds. (arXiv:2305.14785v1 [cs.CL])

    [http://arxiv.org/abs/2305.14785](http://arxiv.org/abs/2305.14785)

    本文研究了ChatGPT语言模型的局限性，指出其在简单语言推断任务中存在困难，并探讨了如何改善其对基本语言概念的理解。

    

    本文研究了ChatGPT的理解能力限制，针对对于人类来说通常简单的推断任务，但这些似乎对该模型具有挑战性。具体来说，我们针对(i)语法规定的蕴含，(ii)带有不确定性证据副词的前提，以及(iii)单调蕴含性进行了研究。我们为这些推理类型提供了专家设计的评估集，并在零样本设置下进行实验。我们的结果表明，该模型在这些推理类型方面存在困难，展示中等到低精度。此外，尽管ChatGPT在直接提示下表现出对底层语言概念的了解，但它经常不能利用这些知识作出正确的推断。更有趣的是，进一步的实验表明，将前提嵌入前提条件触发或非实际性动词会导致模型更频繁地预测蕴含，而不考虑正确的语义标签。总的来说，这些结果揭示了当前语言模型的局限性，以及继续改善它们对基本语言概念的理解的必要性。

    This paper sheds light on the limitations of ChatGPT's understanding capabilities, focusing on simple inference tasks that are typically easy for humans but appear to be challenging for the model. Specifically, we target (i) grammatically-specified entailments, (ii) premises with evidential adverbs of uncertainty, and (iii) monotonicity entailments. We present expert-designed evaluation sets for these inference types and conduct experiments in a zero-shot setup. Our results show that the model struggles with these types of inferences, exhibiting moderate to low accuracy. Moreover, while ChatGPT demonstrates knowledge of the underlying linguistic concepts when prompted directly, it often fails to incorporate this knowledge to make correct inferences. Even more strikingly, further experiments show that embedding the premise under presupposition triggers or non-factive verbs causes the model to predict entailment more frequently {regardless} of the correct semantic label. Overall these re
    
[^19]: 位置偏差对token分类中的语言模型的影响

    Impact of Position Bias on Language Models in Token Classification. (arXiv:2304.13567v1 [cs.CL])

    [http://arxiv.org/abs/2304.13567](http://arxiv.org/abs/2304.13567)

    研究了语言模型在token分类任务中的位置偏差问题，通过实验表明在具体任务中，BERT、ERNIE、ELECTRA等编码器以及GPT2和BLOOM等解码器的平均性能下降了3%和9%。

    

    语言模型在自然语言处理任务中表现出了最先进的性能。命名实体识别(NER)或词性标注等下游任务已知存在数据不平衡问题，特别是在正负示例的比例和类不平衡方面。本文研究了语言模型的另一个特定问题，即token分类任务中正示例的位置偏差。因此，我们对基于Token分类基准测试的语言模型的性能进行了深入的位置偏差评估。我们的研究包括CoNLL03和OntoNote5.0用于NER，English Tree Bank UD_en和TweeBank用于POS标记。我们提出了一种评估方法，以研究Transformer模型中的位置偏差。我们发现像BERT、ERNIE、ELECTRA这样的编码器和像GPT2 和BLOOM这样的解码器平均性能下降了3%和9%。

    Language Models (LMs) have shown state-of-the-art performance in Natural Language Processing (NLP) tasks. Downstream tasks such as Named Entity Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data imbalance issues, specifically in terms of the ratio of positive to negative examples, and class imbalance. In this paper, we investigate an additional specific issue for language models, namely the position bias of positive examples in token classification tasks. Therefore, we conduct an in-depth evaluation of the impact of position bias on the performance of LMs when fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We propose an evaluation approach to investigate position bias in Transformer models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as GPT2 and BLOOM can suffer from this bias with an average drop of 3\% and 9\% in their performan
    

