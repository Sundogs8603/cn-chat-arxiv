# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions.](http://arxiv.org/abs/2305.14795) | 本文提出了一种基准测试MQuAKE，通过多跳问题评估编辑模型是否能够正确回答因编辑事实而答案应该改变的问题。研究发现当前的知识编辑方法可以准确召回已编辑的事实，但在多跳问题上表现灾难性失败。 |
| [^2] | [Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification.](http://arxiv.org/abs/2305.14794) | 本文重新审视了基于种子匹配的伪标签生成方法，并通过简单的单词删除来缓解因规则注入的标签偏见而带来的影响，提高该方法的性能，其性能达到甚至超过最先进技术。 |
| [^3] | [Faithful Low-Resource Data-to-Text Generation through Cycle Training.](http://arxiv.org/abs/2305.14793) | 本文通过基于循环训练的方法，在少量监督数据的情况下，实现了生成文本任务与全监督方法相近的性能，同时极大地提高了非域数据生成的文本的准确性。 |
| [^4] | [Large Language Models as Counterfactual Generator: Strengths and Weaknesses.](http://arxiv.org/abs/2305.14791) | 本文研究了大型语言模型（LLMs）作为反事实生成器的能力，通过数据增强实验发现它们在各个任务中表现优异，但仍存在自我限制和缺乏逻辑指导等问题。 |
| [^5] | [Advancing Topic Segmentation and Outline Generation in Chinese Texts: The Paragraph-level Topic Representation, Corpus, and Benchmark.](http://arxiv.org/abs/2305.14790) | 本文提出了一种分层的段落级中文主题结构表示，使用句子而不是关键词来表示子主题，构建了大规模、高质量的中文段落级主题结构语料库。 |
| [^6] | [Adapting Language Models to Compress Contexts.](http://arxiv.org/abs/2305.14788) | 本论文提出了一种将预训练的语言模型改进为自动压缩器的方法，能够将长篇文本压缩成紧凑的摘要向量，提高上下文的利用效率和降低计算成本，同时通过在上下文学习中的应用，证明了该方法能够提高精度并降低推断成本。 |
| [^7] | [ChatGPT and Simple Linguistic Inferences: Blind Spots and Blinds.](http://arxiv.org/abs/2305.14785) | 本文研究了ChatGPT语言模型的局限性，指出其在简单语言推断任务中存在困难，并探讨了如何改善其对基本语言概念的理解。 |
| [^8] | [Disentangled Phonetic Representation for Chinese Spelling Correction.](http://arxiv.org/abs/2305.14783) | 本论文提出了一种新的方法，通过解开中文文本中的语音学信息和字符信息，实现两者之间的直接交互，有效利用语音学信息提高中文拼写纠错的准确性。 |
| [^9] | [Text Conditional Alt-Text Generation for Twitter Images.](http://arxiv.org/abs/2305.14779) | 本文针对Twitter上分享的图像提出了一种文本条件下的替代文本生成方法。通过CLIP前缀模型，该模型结合图像和推文中的文本信息，生成关于图像的上下文相关的替代文本。 |
| [^10] | [Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models.](http://arxiv.org/abs/2305.14775) | 本文提出了一个系统性的框架来衡量PLMs中参数化知识的利用，研究发现PLMs存在已获取的知识和利用的知识之间的差距，在分布变化下有限的鲁棒性，较大的模型可以弥补已获取知识的差距，但利用知识的差距仍然存在。 |
| [^11] | [A Controllable QA-based Framework for Decontextualization.](http://arxiv.org/abs/2305.14772) | 本文提出了一个基于问答的去文本化框架，可以更好地展示提取的文本摘录。在问答和引证上的表现类似于端到端方法，并且支持用户信息需求及偏好的可控性。 |
| [^12] | [SSD-2: Scaling and Inference-time Fusion of Diffusion Language Models.](http://arxiv.org/abs/2305.14771) | 本文介绍了一种扩散语言模型SSD-2，它可以从0.4B扩展到13B参数，并经过微调来遵循指令。与自回归模型相比，使用SSD-2可以形成更有效的模型合作，产生更好的结果。 |
| [^13] | [Using Natural Language Explanations to Rescale Human Judgments.](http://arxiv.org/abs/2305.14770) | 本文提出使用自然语言解释来调整标注者之间存在的尺度不一致，解决了主观NLP任务中标注者之间分歧的问题。 |
| [^14] | [BeamSearchQA: Large Language Models are Strong Zero-Shot QA Solver.](http://arxiv.org/abs/2305.14766) | BeamSearchQA利用大型语言模型进行迭代式生成问题，以捕捉隐含知识并优化问答过程，在NQ和WebQ测试集上分别达到了71.7％和46.7％的F1分数，显着优于现有的最先进方法。 |
| [^15] | [Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models.](http://arxiv.org/abs/2305.14763) | 本文研究了大型语言模型的神经心智理论能力，并发现虽然模型表现出一定程度的N-ToM能力，但远非稳健，存在对抗性例子困难等问题。 |
| [^16] | [UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning.](http://arxiv.org/abs/2305.14761) | 本文提出了UniChart，这是一个用于图表理解和推理的通用视觉语言预训练模型。UniChart首先建立了一个包括各种不同主题和视觉风格的大量图表语料库，然后使用图表特定的预训练任务来编码图表，并在几个图表理解和推理任务上表现出色。 |
| [^17] | [Bi-Drop: Generalizable Fine-tuning for Pre-trained Language Models via Adaptive Subnetwork Optimization.](http://arxiv.org/abs/2305.14760) | 本文提出了一种动态微调策略Bi-Drop来针对预训练语言模型在大规模微调时可能出现的过拟合现象。经过GLUE基准测试，Bi-Drop的性能优于其他微调方法，并对于多任务、多领域转移、数据不均衡和低资源情况下也表现出强大的鲁棒性。 |
| [^18] | [Human-Centered Metrics for Dialog System Evaluation.](http://arxiv.org/abs/2305.14757) | 本文提出了从心理学构造中提取的可解释度量标准，用于评估对话系统，通过情绪熵、语言风格和情感匹配、宜人性和共情等五个度量，这些人类度量标准与现有的自动度量标准不相关且具有更高的预测准确度。 |
| [^19] | [Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting.](http://arxiv.org/abs/2305.14755) | 本文研究提出了在文体改写的重写和评估阶段整合文本上下文的必要性，并通过few-shot prompting比较非上下文改写和上下文改写的效果。研究发现，自动度量指标不一定能反映出人类的偏好。 |
| [^20] | [DialogVCS: Robust Natural Language Understanding in Dialogue System Upgrade.](http://arxiv.org/abs/2305.14751) | 本论文提出了DialogVCS，建立了4个数据集用于测试鲁棒NLU模型的可靠性。在这些数据集中，新意图的出现可能会与现有意图在语义上存在关联，作者提出了基于多标签分类任务的方法来解决这个问题。 |
| [^21] | [Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation.](http://arxiv.org/abs/2305.14750) | 大型语言模型（LLMs）常常不能完全回答复杂问题。我们提出基于答案断言分解的细粒度自我评估方法，以验证答案满足哪些问题标准。初步实验表明，该方法可帮助发现模型错误和知识差距。 |
| [^22] | [ECHo: Event Causality Inference via Human-centric Reasoning.](http://arxiv.org/abs/2305.14740) | ECHo是一个基于人类中心推理的事件因果推断数据集，并提出了一个与CoT范式对齐的统一框架来评估当前AI系统的推理能力。 |
| [^23] | [Trusting Your Evidence: Hallucinate Less with Context-aware Decoding.](http://arxiv.org/abs/2305.14739) | 提出了上下文感知解码 (CAD)，在不需要额外训练的情况下，显著提高了不同LM系列的忠实度，特别是在解决知识冲突至关重要的任务中取得了显着的改进。 |
| [^24] | [Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection.](http://arxiv.org/abs/2305.14735) | 本文提出了一种基于异常值的方法，用于识别在毒性检测中受到伤害的人群，发现对于这些异常值，模型性能较差，他们面临的毒性更高。 |
| [^25] | [Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation.](http://arxiv.org/abs/2305.14734) | 本文使用了两个预训练的序列到序列模型在阿拉伯语GEC方面取得了最新的结果，并提出下文第一阿拉伯语多类语法错误检测结果。文章表明，使用GED作为GEC模型的辅助输入有助于提高性能，而上下文形态预处理有助于接着GEC系统，文章在两个共享任务数据集上取得了最新的GEC结果以及一个强有力的基准。 |
| [^26] | [SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations.](http://arxiv.org/abs/2305.14728) | SenteCon是一种能够提供高级可解释性的方法，通过把文本编码为可解释类别的层，同时不会对下游任务的预测性能造成影响。 |
| [^27] | [In-Context Demonstration Selection with Cross Entropy Difference.](http://arxiv.org/abs/2305.14726) | 本论文提出了一种基于交叉熵差异的方法，用于在大型语言模型中选择最佳的上下文演示来提高零-shot任务性能。 |
| [^28] | [AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes.](http://arxiv.org/abs/2305.14725) | 提出了一种属性感知的多模态实体链接方法，并构建了一个大型数据集AMELI，实验证明了将属性信息纳入实体链接过程的重要性。 |
| [^29] | [I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors.](http://arxiv.org/abs/2305.14724) | 本论文提出一个新的任务——从语言隐喻生成视觉隐喻，并且基于大语言模型和扩散模型之间的协作，成功地实现了共创出具有视觉冲击力和语义含义的隐喻。 |
| [^30] | [CuRIAM: Corpus re Interpretation and Metalanguage in U.S. Supreme Court Opinions.](http://arxiv.org/abs/2305.14719) | 本研究探讨了美国最高法院观点中的语料库再解释和元语言，并发现了一些法官使用的元语言类型的模式。 |
| [^31] | [Improving Language Models with Advantage-based Offline Policy Gradients.](http://arxiv.org/abs/2305.14718) | 本文介绍了一种简单的训练算法Left-over Lunch RL （LoL-RL），使用离线策略梯度学习任何序列到序列数据，从而实现优化LM效用的方法。 |
| [^32] | [Exploiting Correlations Between Contexts and Definitions with Multiple Definition Modeling.](http://arxiv.org/abs/2305.14717) | 本文提出了一种新的多重定义建模（MDM）任务，将目标词的所有上下文和定义汇集在一起，相比单个定义建模（SDM）有更好的表现，在预训练任务中可以提高SDM的性能，在零-shot情况下有可比性。 |
| [^33] | [GlobalBench: A Benchmark for Global Progress in Natural Language Processing.](http://arxiv.org/abs/2305.14716) | GlobalBench是一个动态追踪所有语言NLP数据集进展的基准，旨在提高公平语言技术的全球发展，并确定最需要服务的语言，目前这些语言主要是非洲和美洲原住民语言。 |
| [^34] | [Gender Biases in Automatic Evaluation Metrics: A Case Study on Image Captioning.](http://arxiv.org/abs/2305.14711) | 本文研究了模型评估度量中的性别偏见对图像字幕任务的影响，并提出了替代方案以解决这一问题。 |
| [^35] | [Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models.](http://arxiv.org/abs/2305.14710) | 使用指令调整方法在众包数据集上训练的大规模语言模型，存在后门漏洞，攻击者只需注入极少量的恶意指令便可永久控制模型行为，且难以被修复，需要更加健全的防御机制。 |
| [^36] | [The student becomes the master: Matching GPT3 on Scientific Factual Error Correction.](http://arxiv.org/abs/2305.14707) | 本文提出了一种不需要验证者且不做领域假设的主张校正系统，能够显著提高科学事实错误校正任务的性能，并通过使用LLM的提示方法和主张感知的解码过程来提高校正质量。 |
| [^37] | [Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts.](http://arxiv.org/abs/2305.14705) | Flan-MoE是一种指令调优的稀疏Mixture of Experts（MoE）语言模型，相对于密集模型，在指令微调和任务特定微调后均表现更好。最大模型Flan-MoE-32B的性能在四个基准测试中超越Flan-PaLM-62B，同时只利用了1/3的FLOPs。 |
| [^38] | [Analyzing Influential Factors in Human Preference Judgments via GPT-4.](http://arxiv.org/abs/2305.14702) | 本文利用Bradley-Terry-Luce模型对OpenAI公布的人类偏好判断数据集进行了深入研究，揭示了人类偏好判断中所蕴含的固有偏好，提出了提高样本效率的策略，并为构建平衡的人类偏好判断数据集提供了洞见。 |
| [^39] | [Modeling rapid language learning by distilling Bayesian priors into artificial neural networks.](http://arxiv.org/abs/2305.14701) | 本文介绍了一种将贝叶斯模型的归纳偏见注入神经网络中的方法，可以从有限的自然数据中进行学习，同时具有灵活性以推广到新的示例。 |
| [^40] | [SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to Rank.](http://arxiv.org/abs/2305.14696) | 提出了一种自监督的外部样本检测方法SELFOOD，通过将外部样本检测视为文档标签内部排序问题，使用比较排序损失对分类器进行训练，实现无监督的外部样本检测。 |
| [^41] | [A Causal View of Entity Bias in (Large) Language Models.](http://arxiv.org/abs/2305.14695) | 研究提出了一种结构因果模型（SCM）并提供因果干预技术，以缓解大型语言模型中的实体偏见，从而减少偏见信息，同时保留相似实体的共同预测信息。 |
| [^42] | [Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs.](http://arxiv.org/abs/2305.14693) | 本文探讨了大型语言模型中个性是否存在的问题，并发现目前自我评估测试无法准确测量语言模型中的个性。为了未来的研究，需要专门为LLM设计新的自我评估测试。 |
| [^43] | [ExpertPrompting: Instructing Large Language Models to be Distinguished Experts.](http://arxiv.org/abs/2305.14688) | 本论文提出了“专家提示”技术，用于训练大型语言模型成为杰出的专家。该方法使用上下文学习自动生成每个指令的详细和定制的专家身份描述，并要求模型根据这些提示提供答案。基于这种技术，本文提出了一个新的开源聊天助手ExpertLLaMA，该助手在评估中表现出高质量的数据和96％的ChatGPT能力。 |
| [^44] | [TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering.](http://arxiv.org/abs/2305.14682) | TACR是一个用于混合文本和表格QA场景的新模型，通过对表格-问题间的对齐增强单元选择来检索细粒度证据，并将包含选定单元的行作为上下文进行回答推理，在HybridQA和WikiTableQuestions（WTQ）数据集上取得了最先进的结果。 |
| [^45] | [Emergent inabilities? Inverse scaling over the course of pretraining.](http://arxiv.org/abs/2305.14681) | 该研究探讨了模型在训练中针对特定任务的表现是否会下降，发现Pythia模型在更多的参数下，尽管整体表现良好，但在引述重复和重新定义数学两个任务的训练中性能下降。需要在任何时候测试模型在相应基准上的表现。 |
| [^46] | [GRILL: Grounded Vision-language Pre-training via Aligning Text and Image Regions.](http://arxiv.org/abs/2305.14676) | GRILL是一种基于文本和图像区域对齐的基于场景的视觉语言预训练模型，它可以在训练样本很少或没有样本的情况下用于各种视觉语言任务，并且在各种零/少样本VL任务上表现更好。 |
| [^47] | [Quantifying Character Similarity with Vision Transformers.](http://arxiv.org/abs/2305.14672) | 本研究使用ViT对OCR文档进行字符相似性度量，提供了一种可扩展的计算字符替换成本的方法，避免了使用常用字符串替换方法导致的数据偏差。 |
| [^48] | [Diffusion Models in NLP: A Survey.](http://arxiv.org/abs/2305.14671) | 本文全面综述了扩散模型在NLP中的各种应用，比较了其与自回归模型的优劣，指出扩散模型在并行生成、文本插值、词级别控制等方面具有明显优势。 |
| [^49] | [You Are What You Annotate: Towards Better Models through Annotator Representations.](http://arxiv.org/abs/2305.14663) | 提出了一种方法，通过创建注释者和注释的矩阵表示以捕捉其特点，并在建模过程中使用它们来显著提高自然语言处理模型的性能，同时帮助民主化人工智能。 |
| [^50] | [Complex Mathematical Symbol Definition Structures: A Dataset and Model for Coordination Resolution in Definition Extraction.](http://arxiv.org/abs/2305.14660) | 该论文介绍了SymDef，一个英文数据集，用于协调解决定义提取中的复杂数学符号及其相应定义之间的链接问题。并且引入了一种新的方法，通过遮盖数学符号，为每个符号创建一个副本，并使用槽填充来预测其相应的定义跨度。 |
| [^51] | [InteractiveIE: Towards Assessing the Strength of Human-AI Collaboration in Improving the Performance of Information Extraction.](http://arxiv.org/abs/2305.14659) | 本文提出InteractiveIE方法，使用自动问答生成法来引出信息提取中的模板插槽，通过微小量代理人类监督，提高在生物医学和法律文档等领域中信息提取任务性能。 |
| [^52] | [Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality.](http://arxiv.org/abs/2305.14658) | 本文重点研究使用大型语言模型（LLMs）为基础的无参考评估器的局限性，并构建了两个对抗元评估对话生成数据集，以全面评估这些评估器的可靠性。 |
| [^53] | [Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion.](http://arxiv.org/abs/2305.14652) | 本文提出了一种基于去噪瓶颈和最大化互信息的视频多模态融合模型（DBF），该模型可以细粒度地过滤掉冗余和噪声信息，同时保留不同模态中的关键信息，并在多语言视频分类任务中表现出显著优越性。 |
| [^54] | [Revisit and Outstrip Entity Alignment: A Perspective of Generative Models.](http://arxiv.org/abs/2305.14651) | 本文重新从生成模型的角度研究了基于嵌入的实体对齐（EEA）问题，引入基于生成对抗网络的EEA方法及提出的生成的EEA（GEEA）框架，通过互相变分自动编码器（M-VAE）实现实体从一个KG转换到另一个KG，并且从随机噪声向量生成新的实体，具有较好的效果。 |
| [^55] | [Meta-review Generation with Checklist-guided Iterative Introspection.](http://arxiv.org/abs/2305.14647) | 本文提出了科学观点总结的任务，以合成研究论文评审的元评审，引入了一个新的ORSUM数据集，并提出了基于检查表引导迭代自查的方法，该方法表现出良好的效果。 |
| [^56] | [Iteratively Improving Biomedical Entity Linking and Event Extraction via Hard Expectation-Maximization.](http://arxiv.org/abs/2305.14645) | 本研究提出了一种通过困难期望最大化来交替进行生物医学实体链接和事件提取的方法，具有全局结构和局部相关性约束。实验结果表明，在基准数据集上该方法有效地提高了生物医学实体链接和事件提取的准确性。 |
| [^57] | [CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation.](http://arxiv.org/abs/2305.14635) | CMOT是一种用于跨模态语音翻译的方法，通过最优传输找到语音和文本序列之间的对齐，并在标记级别上混合不同模态的序列，实现了在有限数据下更好的性能表现。 |
| [^58] | [Testing Causal Models of Word Meaning in GPT-3 and -4.](http://arxiv.org/abs/2305.14630) | 本文使用HIPE理论中描述的词义因果关系模型，对GPT-3和GPT-4的词汇表示进行了评估，发现GPT-3没有编码因果结构的证据，而GPT-4则有。 |
| [^59] | [Enabling Large Language Models to Generate Text with Citations.](http://arxiv.org/abs/2305.14627) | 本文提出ALCE，是首个自动LLMs引文评估基准，实现大型语言模型生成带引文的文本，提高其事实正确性和可验证性；提示LLMs特定的关键词或利用外部知识源可以显著提高其引文准确性。 |
| [^60] | [KNN-LM Does Not Improve Open-ended Text Generation.](http://arxiv.org/abs/2305.14625) | 本文研究了基于插值的检索增强语言模型（LM）的生成质量，发现KNN-LM并不能提高开放式文本生成的整体质量。 |
| [^61] | [Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models.](http://arxiv.org/abs/2305.14623) | 本文介绍了Self-Checker框架，它由即插即用的模块组成，能够在几乎零次启动的情况下利用大型语言模型进行快速高效的事实检查，这对于在资源有限的环境下构建事实检查系统非常有用。 |
| [^62] | [EXnet: Efficient In-context Learning for Data-less Text classification.](http://arxiv.org/abs/2305.14622) | EXnet是一个模型，旨在进行上下文学习，可以在没有示例数量限制的情况下进行。上下文学习是提高任务准确性和跨任务普适性的有效方法，特别是在文本分类方面。 |
| [^63] | [Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations.](http://arxiv.org/abs/2305.14618) | 本论文提出一种基于互斥解释的诱导式常识推理方法，结果表明在各类诱导式推理数据集上优于或与预训练语言模型和其他知识增强方法相当。 |
| [^64] | [COMET-M: Reasoning about Multiple Events in Complex Sentences.](http://arxiv.org/abs/2305.14617) | 提出了COMET-M，该模型可以推理复杂句子中多个事件之间的关系以及生成常识推断，并在35K个人类编写的推断上进行训练，相对于之前的COMET模型在生成多事件推断方面有显着的性能改进。 |
| [^65] | [Exploring the Grounding Issues in Image Caption.](http://arxiv.org/abs/2305.14616) | 该论文为我们从计算认知语言的角度出发，探讨了图像描述中的多模态语义表征中的基础问题。研究结果表明，情境含义和功能性是生成适当的图像描述的关键。 |
| [^66] | [Selectively Answering Ambiguous Questions.](http://arxiv.org/abs/2305.14613) | 本研究调查了解决模糊问题的方法，通过定量测量模型输出中的重复性，找出了在含糊问题集中回答高精度子集问题的最可靠方法。这种基于采样的方法可以有效解决问题回答中的歧义，并提高语言模型的可靠性。 |
| [^67] | [This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models.](http://arxiv.org/abs/2305.14610) | 本文提出了地缘政治偏见的概念，并以领土争端为例，利用多语言、多选题的数据集BorderLines和几个定量指标分析语言模型响应中的地缘政治偏见现象。 |
| [^68] | [OpenPI2.0: An Improved Dataset for Entity Tracking in Texts.](http://arxiv.org/abs/2305.14603) | OpenPI2.0是一个用于实体追踪的改进数据集，它包括规范化实体、显著性注释和下游应用调查等特点。 |
| [^69] | [Learning Semantic Role Labeling from Compatible Label Sequences.](http://arxiv.org/abs/2305.14600) | 该论文探讨了如何从不相交的兼容标签序列中高效地学习，将此运用于语义角色标注任务，提出了联合处理VerbNet和PropBank标签的方法，并验证了其有效性。 |
| [^70] | [Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations.](http://arxiv.org/abs/2305.14599) | 本文提出了InterSent框架，探索将不同的组合属性纳入句子嵌入空间中使嵌入变换变为组合句子操作，实现了可解释的句子表示学习。 |
| [^71] | [Voices of Her: Analyzing Gender Differences in the AI Publication World.](http://arxiv.org/abs/2305.14597) | 通过对AI学术界的78K研究人员的分析，研究发现女性第一作者的论文具有不同的语言风格，例如更长的文本、更多的正面情感词汇和更引人注目的标题；在AI论文的合著中存在很大的性别同质性。我们鼓励未来实现更多的性别平等和多样性。 |
| [^72] | [Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy.](http://arxiv.org/abs/2305.14596) | 当大型语言模型应用于多项选择题时，其注意力往往会分散到许多无效的词汇符号上，这会导致模型真实性能的低估。本文提出了一种数学形式化方法来研究这种现象，并发现通过使用只含一个示例的上下文学习方法可以提高对有效选择的注意力。 |
| [^73] | [Instruction Tuning with Lexicons for Zero-Shot Style Classification.](http://arxiv.org/abs/2305.14592) | 通过使用词典指导语言模型识别训练期间未见过的新风格，可以有效地提高零样本性能的转移。 |
| [^74] | [ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers.](http://arxiv.org/abs/2305.14591) | ALGO框架使用由LLM生成的神谕指导创造和验证算法程序，以提高现有代码生成模型的算法问题解决能力。 |
| [^75] | [RE$^2$: Region-Aware Relation Extraction from Visually Rich Documents.](http://arxiv.org/abs/2305.14590) | RE$^2$方法利用视觉丰富文档中实体块之间的区域级空间结构来提高它们的关系预测能力，表现出较好的性能。 |
| [^76] | [Evaluating end-to-end entity linking on domain-specific knowledge bases: Learning about ancient technologies from museum collections.](http://arxiv.org/abs/2305.14588) | 本文评估并改进了实体链接方法在博物馆藏品数据中的应用，展示微调的最新端到端 EL 模型明显优于现有方法，同时提供了数据集和最佳模型。 |
| [^77] | [Contextualized Topic Coherence Metrics.](http://arxiv.org/abs/2305.14587) | 本研究提出了一种上下文化主题相干性评估指标（CTC），该指标不仅在短文档上运作良好，而且在相对于其他五个指标评估上具有更高的性能表现。 |
| [^78] | [Making the Implicit Explicit: Implicit Content as a First Class Citizen in NLP.](http://arxiv.org/abs/2305.14583) | 该研究通过将自然语言处理的重点放在隐式内容上，提出了一种通过推理和分解方法降低自然语言处理复杂度的新方法，并在嵌入，计算政治学和构建发现方面实现了显著的改进和应用。 |
| [^79] | [Evaluating OpenAI's Whisper ASR for Punctuation Prediction and Topic Modeling of life histories of the Museum of the Person.](http://arxiv.org/abs/2305.14580) | 本文首次对葡萄牙语中的Whisper ASR进行了标点符号预测方面的研究，并为标点符号预测在主题建模中的应用提供了有益的实验评估。 |
| [^80] | [Connecting the Dots: What Graph-Based Text Representations Work Best for Text Classification using Graph Neural Networks?.](http://arxiv.org/abs/2305.14578) | 本文研究了基于图的文本表示方法在文本分类中的应用，发现文本输入特征和领域要素对图的性能具有重要影响，BERT在处理短文本时难以收敛，图方法对于较长的文档特别有益。 |
| [^81] | [Difference-Masking: Choosing What to Mask in Continued Pretraining.](http://arxiv.org/abs/2305.14577) | 本文提出了一种自动选取遮蔽数据的方法（Difference-Masking），以提高在继续预训练中的自监督学习模型的性能，方法是通过考虑未标记的目标域与预训练域的不同之处来进行。实验证明，该方法可以有效地提升继续预训练任务的性能，且具有跨任务的适用性。 |
| [^82] | [Detecting and Mitigating Indirect Stereotypes in Word Embeddings.](http://arxiv.org/abs/2305.14574) | 本文提出了一种新方法，称为有偏间接关系修改（BIRM），以减轻分布式词嵌入中的间接偏见，通过考虑标记偏见属性的单词在存在情况下给定一对单词的共现概率如何变化，并利用这一点平均偏见属性的影响。 |
| [^83] | [From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding.](http://arxiv.org/abs/2305.14571) | 本文提出了一种新颖的开放词汇语言模型，它采用了分层两级方法和浅层Transformer体系结构从字符中学习单词，能够提高模型的容忍度和适应性。 |
| [^84] | [Few-shot Unified Question Answering: Tuning Models or Prompts?.](http://arxiv.org/abs/2305.14569) | 本文探讨了调整模型和提示两种范式在低资源情况下用于统一QA的潜力，研究发现在良好的初始化条件下，提示调整可以在少样本情况下的表现与模型调整相当，通过参数共享和简单的提示初始化知识转移技术，提示调整在低资源情况下可以实现显著的性能提升。 |
| [^85] | [PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents.](http://arxiv.org/abs/2305.14564) | PEARL是一种提示框架，用于改善大型语言模型对长文档的推理。 它包括三个阶段：行动挖掘，计划制定和计划执行。 PEARL将问题分解成一系列动作并在文档上执行以获得答案。 |
| [^86] | [Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations.](http://arxiv.org/abs/2305.14556) | 本文基于ChatGPT模型，研究了AI生成面向目标的对话和注释的潜力，实验结果表明生成的对话和注释质量与人类生成的相当。 |
| [^87] | [All Roads Lead to Rome? Exploring the Invariance of Transformers' Representations.](http://arxiv.org/abs/2305.14555) | 本文探究了Transformer模型表示空间的可靠性问题，提出了双射假设，并提出了一种基于可逆神经网络的模型BERT-INN，来更有效地学习双射，实验结果显示其优势。 |
| [^88] | [Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization.](http://arxiv.org/abs/2305.14548) | 本文提出了一种易于解释的细粒度不一致性检测模型FineGrainFact，通过语义角色标注显式地表示文档和摘要中的事实，并强调相关的语义框架来预测不一致性，实验表明该模型优于强基线。 |
| [^89] | [On the Transferability of Whisper-based Representations for "In-the-Wild" Cross-Task Downstream Speech Applications.](http://arxiv.org/abs/2305.14546) | Whisper模型是一个基于变压器的模型，可用于各种语音任务。本篇文章探讨了Whisper表示在其他四种语音任务和“野外”任务中的可迁移性和鲁棒性。实验结果表明，Whisper具有跨任务的真实环境部署潜力。 |
| [^90] | [LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond.](http://arxiv.org/abs/2305.14540) | LLMs能够在基本的实际不一致性检测基准测试中表现出竞争力，但大多数LLMs不能成功完成更复杂任务方案的测试。作者提出了一个新的非一致性检测基准测试SummEdits，并表明大多数LLMs并不擅长完成此项测试。 |
| [^91] | [Cascaded Beam Search: Plug-and-Play Terminology-Forcing For Neural Machine Translation.](http://arxiv.org/abs/2305.14538) | 本文提出了一种级联束搜索算法，用于神经机器翻译中的术语约束，可以即插即用，无需重新训练。该方法可将目标术语的概率增加，并使用基于网格束搜索的级联束设置，性能良好。 |
| [^92] | [MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems.](http://arxiv.org/abs/2305.14536) | MathDial是一个由实际教师和大型语言模型半合成生成的对话数据集，旨在解决自动对话辅导工具缺少高质量数据的问题。这个数据集关注通过引导学生使用问题来探索数学问题。 |
| [^93] | [Detecting Propaganda Techniques in Code-Switched Social Media Text.](http://arxiv.org/abs/2305.14534) | 本文提出了一个新任务，即在混合语言的社交媒体文本中检测宣传技术。为了支持这一任务，作者创建了一个包含1030个文本的英语和罗马乌尔混合语言的语料库，并进行了一系列实验。 |
| [^94] | [How to Choose How to Choose Your Chatbot: A Massively Multi-System MultiReference Data Set for Dialog Metric Evaluation.](http://arxiv.org/abs/2305.14533) | 该研究发布了MMSMR数据集，该数据集包含8个参考对话，旨在促进对话度量和评估的未来工作。该研究使用1750个系统对其进行了评估，以了解稳健相关性并了解测试集中所需的内容。 |
| [^95] | [Eliminating Spurious Correlations from Pre-trained Models via Data Mixing.](http://arxiv.org/abs/2305.14521) | 本文提出了一种通过数据混合来消除预训练模型中虚假相关性的方法，来提高模型对于新样本的预测能力。这种方法经过理论证明和多种任务实验验证，可以取得良好的效果。 |
| [^96] | [Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models.](http://arxiv.org/abs/2305.14507) | 本文探究了大型语言模型（LLM）是否能够在扰动证据下做出逻辑推理的结论，结果发现即使是最先进的GPT模型在处理操纵证据下的推理时也存在困难。 |
| [^97] | [RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning.](http://arxiv.org/abs/2305.14502) | 本文提出了一种 RetICL 的方法来优化地选择用于上下文学习模型的示例。此方法利用强化学习框架将序列示例选择问题作为马尔科夫决策过程，并且优化选择为使任务表现最佳的组合。 |
| [^98] | [NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders.](http://arxiv.org/abs/2305.14499) | NAIL是一种带有高效非自回归解码器的词汇检索指数模型，可与现有的预训练模型兼容，并且使用商品CPU提供服务。它可以捕捉Transformer交叉关注模型收益高达86％的方法，与BM25检索器结合使用匹配当前最先进的双编码器检索器的质量。 |
| [^99] | [Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement.](http://arxiv.org/abs/2305.14497) | Self-Polish是一种方法，通过促使模型逐步完善给定的问题以使其更易理解和可解决，以增强大型语言模型的推理能力。 |
| [^100] | [Prompt position really matters in few-shot and zero-shot NLU tasks.](http://arxiv.org/abs/2305.14493) | 该论文通过实证研究发现，提示位置对于少样本和零样本任务的模型性能具有实质性影响，先前研究中使用的提示位置通常是次优的，提示位置优化应成为重要的研究方向。 |
| [^101] | [Sociocultural Norm Similarities and Differences via Situational Alignment and Explainable Textual Entailment.](http://arxiv.org/abs/2305.14492) | 本论文提出了一种新方法来比较中美文化的社会规范，通过情境对齐和上下文学习提取社会规范。作者构建了一个高质量的数据集，通过测试发现该方法具有研究社会文化异同的潜力。 |
| [^102] | [Are Large Language Models Robust Zero-shot Coreference Resolvers?.](http://arxiv.org/abs/2305.14489) | 本文研究了零-shot共指解析技术在复杂语言环境下的应用，并证明指令调整的语言模型具有鲁棒性零 shot 普适性。 |
| [^103] | [Language Model Self-improvement by Reinforcement Learning Contemplation.](http://arxiv.org/abs/2305.14483) | 本文提出了一种无监督的方法 SIRLC，可以使用强化学习有效地提高语言模型的性能，而不需要使用外部标签，并且可以应用于各种NLP任务。 |
| [^104] | [Is a Prestigious Job the same as a Prestigious Country? A Case Study on Multilingual Sentence Embeddings and European Countries.](http://arxiv.org/abs/2305.14482) | 本文研究了多语句子嵌入如何捕捉欧洲国家和职业，并发现嵌入中最突出的国家特征是其GPD经济实力。本研究中的大部分国家维度与职业维度不相关，但一种模型表现出职业声望和原籍国之间的联系，这是一种潜在的基于国籍的歧视。 |
| [^105] | [FOCUS: Effective Embedding Initialization for Specializing Pretrained Multilingual Models on a Single Language.](http://arxiv.org/abs/2305.14481) | 本文提出了FOCUS，在多语言源模型设置下，该方法使用重叠标记组合有效地初始化预训练的模型权重，提高了这种方法在适应新语言时的性能表现。 |
| [^106] | [BAND: Biomedical Alert News Dataset.](http://arxiv.org/abs/2305.14480) | 该论文介绍了生物医学警报新闻数据集（BAND），其包括了1,508个样本和30个与流行病学相关的问题，为自然语言处理领域提供了新的挑战，需要更好的内容伪装能力和重要信息推断能力。该数据集可以为疾病监测和流行病学分析提供有价值的洞察力。 |
| [^107] | [CGCE: A Chinese Generative Chat Evaluation Benchmark for General and Financial Domains.](http://arxiv.org/abs/2305.14471) | 中国的CGCE基准为通用和金融领域建立了标准化的评测框架，包含200个通用领域问题和150个金融领域特定的职业问题，旨在提升自然语言生成研究的发展。 |
| [^108] | [Run Like a Girl! Sports-Related Gender Bias in Language and Vision.](http://arxiv.org/abs/2305.14468) | 本文分析了两个语言和视觉数据集中的性别偏见，不仅发现这两个数据集都存在女性的欠代表问题，而且还发现了一种命名偏见：在运动中男性更有可能被命名与他们运动相关的称呼，这种偏见对女性表征造成了损害。 |
| [^109] | [Enhancing Generation through Summarization Duality and Explicit Outline Control.](http://arxiv.org/abs/2305.14459) | 本文提出了一个两阶段的摘要增强的大纲监督生成框架，能够更好地生成明确和合理的大纲，并引入了一个新颖的显式大纲控制方法以更有效地利用生成的大纲。 |
| [^110] | [Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA.](http://arxiv.org/abs/2305.14458) | 本研究引入了SALSA框架，对大型语言模型进行细粒度的文本简化评估，通过21种不同编辑类型，揭示了不同模型和人类文本简化的偏好和表现，并开发了LENS-SALSA指标用于自动简化度量。 |
| [^111] | [Pre-training Language Models for Comparative Reasoning.](http://arxiv.org/abs/2305.14457) | 本文提出一种预训练语言模型的新框架，旨在增强其在比较推理方面的能力。通过使用可扩展的基于文本实体比较数据的方法和新的预训练任务，该框架得到了显著的结果。 |
| [^112] | [Having Beer after Prayer? Measuring Cultural Bias in Large Language Models.](http://arxiv.org/abs/2305.14456) | 这篇论文研究了大型语言模型在处理和生成阿拉伯文本时出现的文化偏向西方文化的现象，表明语言模型在人名、食品、服装、地点、文学、饮料、宗教和体育等八个文化方面存在偏见。这些发现引发对于当前语言模型文化相关性的担忧。 |
| [^113] | [On Robustness of Finetuned Transformer-based NLP Models.](http://arxiv.org/abs/2305.14453) | 本文研究了BERT、GPT-2和T5这三种语言模型对文本扰动的鲁棒性，通过量化fine-tuning后的语言模型表示与预训练表示的差异来探究模型的变化程度。 |
| [^114] | [Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors.](http://arxiv.org/abs/2305.14450) | 本研究从性能、度量标准、鲁棒性和错误类型四个方面评估ChatGPT的信息抽取能力，发现了ChatGPT与SOTA结果之间存在巨大的性能差距，同时提出了一种软匹配策略以更准确地反映ChatGPT的性能。 |
| [^115] | [Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions.](http://arxiv.org/abs/2305.14441) | 本文针对开放领域问答系统中存在的对比一致性问题，通过收集最小编辑问题构建了具有挑战性的对比集合，发现广泛使用的DPR在对比集合上表现不佳，引入了查询端对比损失和数据增强来改善DPR的训练，并进行实验验证。 |
| [^116] | [Domain-Expanded ASTE: Rethinking Generalization in Aspect Sentiment Triplet Extraction.](http://arxiv.org/abs/2305.14434) | 该论文提出了一个领域扩展的ASTE基准数据集，通过生成方法来解决领域泛化的问题。 |
| [^117] | [Image Manipulation via Multi-Hop Instructions -- A New Dataset and Weakly-Supervised Neuro-Symbolic Approach.](http://arxiv.org/abs/2305.14410) | 该论文提出了一个基于神经符号概念学习的图像操作系统NeuroSIM，它可以通过多跳指令在多物体场景中执行复杂的推理，只需要弱监督的数据集，并创建了一个新的数据集。该系统具有很高的竞争力或超过SOTA基线。 |
| [^118] | [AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback.](http://arxiv.org/abs/2305.14387) | 该论文提出了一种名为AlpacaFarm的低成本模拟器，该模拟器为从人类反馈中学习的研究和开发提供了一种解决方案，通过设计LLM提示来模拟人类反馈，提出自动评估并提供参考实现，克服了数据收集的高昂成本、缺乏可信的评估和缺乏参考方法实现的挑战。 |
| [^119] | [Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation.](http://arxiv.org/abs/2305.14386) | 本文提出了一种使用GPT-3生成定制化练习，教授数学应用题解决方法的新方法，该方法考虑学生模型的弱点并以教育科学原理为基础进行定制化的学习体验，并取得了比其他大型语言模型更好的表现。 |
| [^120] | [Finding the Pillars of Strength for Multi-Head Attention.](http://arxiv.org/abs/2305.14380) | 本研究提出聚合头注意力(Grouped Head Attention)，使用自监督组约束进行训练，为注意力头进行分组，其中每个组专注于一个重要而独特的特征子集。此方法可以缓解MHA的冗余性和过度参数化问题，并导致更有效和高效的MHA，进而在基准测试中取得了性能提升。 |
| [^121] | [Anchor Prediction: Automatic Refinement of Internet Links.](http://arxiv.org/abs/2305.14337) | 本研究介绍了锚点预测的任务，通过对链接目标网页的特定部分进行识别，帮助读者更有效地在链接网页中找到相关信息。 |
| [^122] | [ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models.](http://arxiv.org/abs/2305.14323) | 这项研究提出了一个基于工具增强的链式思维推理框架 ChatCoT，用于基于聊天的大型语言模型，通过聊天的方式实现多轮推理，巧妙地结合了思维链跟踪和工具操作的方法，提高了大型语言模型的推理能力。 |
| [^123] | [How Old is GPT?: The HumBEL Framework for Evaluating Language Models using Human Demographic Data.](http://arxiv.org/abs/2305.14195) | 本论文提出了一种使用人类人口数据对语言模型进行评估的框架，并发现GPT-3.5在不同任务中表现不同，在单词意义推断方面模拟了典型6-9岁儿童的能力，在记忆方面则表现优于典型21岁年轻人。 |
| [^124] | [Revisiting Acceptability Judgements.](http://arxiv.org/abs/2305.14091) | 本文介绍了第一个大规模的非英语语言可接受性数据集CoLAC，发现语言模型性能低于人类的表现，但跨语言转移可行，并可以追溯到预训练阶段。 |
| [^125] | [One-stop Training of Multiple Capacity Models.](http://arxiv.org/abs/2305.14066) | 本文提出一种新颖的一站式训练框架，可同时训练高容量和低容量模型，相较于知识蒸馏更快速、更高效，实验结果表明该方法在多语言翻译任务上优于低容量基准模型，并在高容量模型上实现了可比或更好的性能，同时提高了模型稳定性和泛化能力。 |
| [^126] | [Flexible Grammar-Based Constrained Decoding for Language Models.](http://arxiv.org/abs/2305.13971) | 本文提出了一种使用形式语法约束丰富解码步骤的方法，有效生成符合特定语法的复杂输出结构，同时允许任何上下文无关语法集成。实验证明该方法在四个信息提取任务上实现了最先进的性能表现。 |
| [^127] | [LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models.](http://arxiv.org/abs/2305.13718) | 本文介绍了 LogicLLM，一种通过自监督后训练来提高大语言模型的逻辑推理能力的方法，该方法有效地在常见逻辑推理任务上进行表现，超过了目前最先进的无监督基线方法。 |
| [^128] | [IdEALS: Idiomatic Expressions for Advancement of Language Skills.](http://arxiv.org/abs/2305.13637) | 这篇论文探索了使用惯用表达式来提高学生写作能力的方法，并评估了不同方法的性能表现。 |
| [^129] | [Better Low-Resource Entity Recognition Through Translation and Annotation Fusion.](http://arxiv.org/abs/2305.13582) | 本研究提出了一种通过翻译和注释融合的框架，可以改进低资源语言文本的命名实体识别。通过TransFusion模型，可以在不同语言之间进行强大的预测，且在两个低资源命名实体识别数据集上表现一致优秀。 |
| [^130] | [Multimodal Automated Fact-Checking: A Survey.](http://arxiv.org/abs/2305.13507) | 本调查提出了一个多模态自动事实核查的框架，并包括了独特的子任务，重点关注了文本，图像，音频和视频这四种模态的现实应用。纪录了相关的基准模型，讨论了未来研究的局限性和前景。 |
| [^131] | [Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference.](http://arxiv.org/abs/2305.13484) | Flover是一种用于自回归模型并行推断的时间融合框架，解决了并行性不足和灵活性差的问题，可以实现更加高效的推断性能。 |
| [^132] | [Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes.](http://arxiv.org/abs/2305.13300) | 本文研究了大型语言模型（LLMs）在遭遇知识冲突时的行为。结果发现，LLMs可以高度接受外部连贯且有说服力的证据，即使与其参数化内存存在冲突，但也可能有局限性。 |
| [^133] | [AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the Web.](http://arxiv.org/abs/2305.13117) | 本文介绍了AVeriTeC数据集，该数据集包括4,568个真实世界的主张，每个主张都有在线证据支持的问题-答案对和文本证明解释。该数据集可以避免一些常见的问题，并提供了一个评估方案来对开放网络上的主张进行验证。 |
| [^134] | [Federated Learning of Medical Concepts Embedding using BEHRT.](http://arxiv.org/abs/2305.13052) | 该论文提出了一种基于BEHRT的联邦学习方法来学习医疗概念嵌入，在不传输任何隐私数据的情况下，实现了多个医疗中心的EHR数据的共同学习。实验证明该方法在预测病人下一次诊断方面有效。 |
| [^135] | [Can Large Language Models emulate an inductive Thematic Analysis of semi-structured interviews? An exploration and provocation on the limits of the approach and the model.](http://arxiv.org/abs/2305.13014) | 本文介绍了一项实验的结果和反思，该实验使用模型GPT 3.5-Turbo来模拟归纳式主题分析的某些方面。尝试使用LLM进行基于人类解释的分析显然是一种挑战，但也是了解这些系统在定性研究中能否使用的一种方式。 |
| [^136] | [MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering.](http://arxiv.org/abs/2305.12820) | 本文提出一项新任务，即回答多表问题。 我们的新模型MultiTabQA不仅可以回答多表问题，还可以泛化地生成表格答案，并在实验中表现出比现有基线更好的性能。 |
| [^137] | [TheoremQA: A Theorem-driven Question Answering dataset.](http://arxiv.org/abs/2305.12524) | 该论文介绍了一个定理驱动的问题回答数据集TheoremQA，可用于评估AI模型在应用定理解决科学问题时的能力，经过测试，GPT-4在解决这些问题上的准确率远高于其他模型。 |
| [^138] | [XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages.](http://arxiv.org/abs/2305.11938) | 该论文提出了 XTREME-UP，一个以少量数据评估代表性不足语言的 NLP 系统性能的用户中心稀缺数据基准测试。它专注于用户中心任务，聚焦于代表性不足的语言，覆盖 88 种语言，并介绍了新的 OCR、自动完成、语义分析和音译数据集，旨在帮助推进代表性不足语言的高度多语言 NLP 系统的发展。 |
| [^139] | [LLM Itself Can Read and Generate CXR Images.](http://arxiv.org/abs/2305.11490) | 该论文提出了一种新方法，可以在不需要进行结构更改、额外训练、或训练专门网络的情况下，通过微调预先训练的LLM来读取和生成像文本一样的图像，并应用于胸部X线（CXR）图像的生成任务中。 |
| [^140] | [Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data.](http://arxiv.org/abs/2305.11326) | 本文提出了使用聊天机器人作为自动化创造的会话接口来方便大众探索表格数据的方法。 |
| [^141] | [Evidence of Meaning in Language Models Trained on Programs.](http://arxiv.org/abs/2305.11169) | 该论文证明了，通过在程序语料库上训练语言模型，即使没有针对学习语言语义提供归纳偏差，语言模型仍然能够学习含义。线性探测器能够从模型状态中提取程序状态的抽象，准确性与模型泛化到新程序的能力显著相关。 |
| [^142] | [The Web Can Be Your Oyster for Improving Large Language Models.](http://arxiv.org/abs/2305.10998) | 本文提出了一种新的大型语言模型增强方法，即使用自适应搜索引擎辅助学习方法，将大规模网络数据与LLMs融合，从而避免无用或嘈杂的增强。 |
| [^143] | [Language Models Meet World Models: Embodied Experiences Enhance Language Models.](http://arxiv.org/abs/2305.10626) | 本文提出一种新的增强语言模型的方法——将其与世界模型相结合，通过有目的的规划和随机探索获得丰富的实体经验进行微调, 以提高其在物理环境下的推理和行为能力，并在语言基准上保持或提高性能。 |
| [^144] | [DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining.](http://arxiv.org/abs/2305.10429) | DoReMi方法使用分组分布式鲁棒优化训练小型代理模型以产生域权重，再使用这些权重重新采样数据集训练大型模型，相比使用默认权重的基线模型，在The Pile和GLaM数据集上平均提高了6.5%和4.7%的few-shot下游准确度，分别使用2.6倍和相同的训练步骤达到基线准确度。 |
| [^145] | [Better speech synthesis through scaling.](http://arxiv.org/abs/2305.07243) | 该论文介绍了一种将图像生成方法应用于语音合成领域的方法，提出了一个表达性强、多语音的文本转语音系统TorToise。 |
| [^146] | [When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks.](http://arxiv.org/abs/2305.06626) | 本文通过预测单个标注者的打分，并结合文本目标群体的预测，模拟了目标群体成员的意见，通过使用他们的人口统计学数据和在线意见预测标注者的打分，在仇恨言论检测等主观任务中提高了模型性能。 |
| [^147] | [Chain-of-Dictionary Prompting Elicits Translation in Large Language Models.](http://arxiv.org/abs/2305.06575) | 研究通过在大型语言模型中添加字典链提示的方法来改进低资源语言的翻译能力，实验结果表明能显著提高翻译质量。 |
| [^148] | [AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment.](http://arxiv.org/abs/2305.04476) | 本文提出了基于跨模态对齐的STS模型AlignSTS，通过一种新颖的节奏适配器来预测目标节奏表示以弥合内容和音高之间的模态差距，并使用交叉注意力重新对齐内容进行跨模态融合重新合成。该模型表现优异。 |
| [^149] | [LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions.](http://arxiv.org/abs/2304.14402) | 本文提出的LaMini-LM是一种基于大规模指令的多样性压缩模型群集，从指令微调过的LLMs中提取知识到更小的模型中，其在15个不同的NLP基准测试中与其他竞争基线的表现相当，但体积约小了10倍。 |
| [^150] | [Why Does ChatGPT Fall Short in Answering Questions Faithfully?.](http://arxiv.org/abs/2304.10513) | 该论文分析了ChatGPT在问答系统中的失误，归纳并确定其失败的原因类型和关键能力，进一步提出了潜在方法来提高其准确性。 |
| [^151] | [Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data.](http://arxiv.org/abs/2304.01196) | 提出了一种流程，通过利用ChatGPT与自身对话，能够自动生成高质量的多轮聊天语料库，并采用参数高效调整来增强开源的大型语言模型LLaMA，得到的模型被命名为Baize，在最小化潜在风险的护栏下，在多轮对话中展现出良好的性能。 |
| [^152] | [GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment.](http://arxiv.org/abs/2303.16634) | 本文介绍了GPTEval，一个利用链式思考和形式填充评价NLG生成的质量。实验表明，在文本摘要任务中，GPTEval结合GPT-4取得了0.514的斯皮尔曼相关系数，胜过其他方法。 |
| [^153] | [InterviewBot: Real-Time End-to-End Dialogue System to Interview Students for College Admission.](http://arxiv.org/abs/2303.15049) | 论文介绍了一种基于神经元的端到端对话模型- InterviewBot，该模型利用与输入的历史对话和定制主题集成到同一嵌入空间中来评估外国学生申请美国大学的学术和文化准备情况。同时，为克服基于变形金刚编码器-解码器模型的输入/输出大小限制，提出了上下文关注和主题存储两种新方法。该模型在统计和动态测试中表现出了流畅性和上下文感知的高度满意。 |
| [^154] | [Translate your gibberish: black-box adversarial attack on machine translation systems.](http://arxiv.org/abs/2303.10974) | 本文研究了机器翻译系统的黑盒对抗攻击方法，通过使用一种新颖的基于梯度自由张量的优化器，可以欺骗在线翻译工具产生错误的翻译，这影响了用户学习语言的体验。 |
| [^155] | [Technical report: Graph Neural Networks go Grammatical.](http://arxiv.org/abs/2303.01590) | 本文介绍了一种将代数语言片段与图神经网络形式上联系的框架，并从MATLANG定义了一个符合3-WL测试的语法，进而得出一个符合3-WL GNN模型的G$^2$N$^2$。此外，语法方法还提供了计算长度为六及以下的环和弦环的代数公式，并在多个下游任务中取得优秀的表现。 |
| [^156] | [Weighted Sampling for Masked Language Modeling.](http://arxiv.org/abs/2302.14225) | 本文提出了两种有效的加权采样策略用于遮蔽语言建模，可以有效解决高频率词汇导致的偏差问题，提高了语言模型的性能和迁移学习能力。 |
| [^157] | [STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training.](http://arxiv.org/abs/2302.09736) | STOA-VLP是一个视频-语言预训练框架，可以同时跨越空间和时间维度建模对象和动作信息，并且在HACS数据集上的实验结果比现有方法要好。 |
| [^158] | [Measuring The Impact Of Programming Language Distribution.](http://arxiv.org/abs/2302.01973) | 该研究提出了BabelCode框架和Translating Python Programming Puzzles（TP3）基准测试，探讨了平衡训练数据集中14种编程语言分布的影响。结果显示平衡分布有助于大型语言模型在低资源语言上的性能提升。 |
| [^159] | [Replug: Retrieval-augmented black-box language models.](http://arxiv.org/abs/2301.12652) | REPLUG是一种检索增强的语言建模框架，它通过在黑盒语言模型输入前添加检索到的文档来增强模型，同时可以通过LM监督检索模型来提高预测性能。 |
| [^160] | [MGeo: Multi-Modal Geographic Pre-Training Method.](http://arxiv.org/abs/2301.04283) | 本文提出了一种新的查询-POI匹配方法MGeo，通过结合多模态数据，有效利用多样化的地理信息上下文提高了性能。 |
| [^161] | [Towards multi-document summarization in the open-domain.](http://arxiv.org/abs/2212.10526) | 本论文针对"开放领域"的多文档摘要任务进行研究，发现最先进的摘要器在此情境下性能下降严重，但进行额外的开放领域的训练可以降低对不完美检索的敏感性。 |
| [^162] | [DePlot: One-shot visual language reasoning by plot-to-table translation.](http://arxiv.org/abs/2212.10505) | 本论文提出了一种名为DePlot的方法，它是一个模态转换模块，能够将图表的图像转换为线性化的表格。利用这个模块和预训练的大型语言模型的少量推理能力，我们开发了一种新的一次性视觉语言推理方法。 |
| [^163] | [Can Current Task-oriented Dialogue Models Automate Real-world Scenarios in the Wild?.](http://arxiv.org/abs/2212.10504) | 任务导向对话系统基于槽填充框架取得了显著的成功，但当前的 TOD 基准测试局限于代理真实世界情景。WebTOD 是建立可伸缩 TOD 系统的另一种方向，可以实现对 Web/移动接口的理解。 |
| [^164] | [SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization.](http://arxiv.org/abs/2212.10465) | SODA是第一个公开发布的百万级别高质量社交对话数据集，通过从知识图谱中上下文化社交常识知识进行蒸馏，我们训练出了COSMO，其比目前最佳表现的对话模型更为自然和一致，这有助于了解知识丰富型对话和自然社交闲聊之间的差异。 |
| [^165] | [What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary.](http://arxiv.org/abs/2212.10380) | 本文探讨了双编码器用于稠密检索的机制，通过将向量表示投影到模型的词汇表空间来解释它们，进一步解释了一些失败案例，提出了一种简单的方法在推理时丰富查询和段落表示与词汇信息，显著提高了性能。 |
| [^166] | [Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis.](http://arxiv.org/abs/2212.10356) | 该论文通过感受野分析透视了Transformer长度外推中的相对位置嵌入设计，提出了第一个真正使用比训练序列长的长度信息的无参数相对位置嵌入设计Sandwich。 |
| [^167] | [CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context.](http://arxiv.org/abs/2212.10007) | CoCoMIC是一个能联合建模文件内外上下文的代码补全框架，其中包括一个跨文件上下文查找工具CCFINDER。使用该框架相对提高了33.94％的精确匹配率和28.6％的前5个候选项准确率。 |
| [^168] | [MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering.](http://arxiv.org/abs/2212.09662) | 本文提出了一种新的预训练方法 MatCha ，提高了视觉语言模型在图表和语言数据联合建模方面的能力，取得了近20%的性能提升。 |
| [^169] | [Large Language Models are reasoners with Self-Verification.](http://arxiv.org/abs/2212.09561) | 本文提出了一种新的自我验证方法，使用CoT的结论来构建新样本并要求LLM重新预测原始条件，以提高推理准确性。实验证明，LLMs可以对其自己的结论进行自我验证并实现竞争性的推理性能。 |
| [^170] | [Decoder Tuning: Efficient Language Understanding as Decoding.](http://arxiv.org/abs/2212.08408) | 本文提出了解码器调整（DecT）方法，通过优化任务特定的解码器网络来适应预训练模型的输出端，避免了传统方法中输入端的高计算和时间成本。 |
| [^171] | [Controllable Text Generation via Probability Density Estimation in the Latent Space.](http://arxiv.org/abs/2212.08307) | 这篇论文提出了一种利用概率密度估计在潜空间中进行可控文本生成的新框架，并采用可逆变换函数将复杂分布映射到先验空间中的简单高斯分布，实现了一种灵活、高效、高质量的文本生成方法。 |
| [^172] | [A fine-grained comparison of pragmatic language understanding in humans and language models.](http://arxiv.org/abs/2212.06801) | 本文对于人类和语言模型在实用语言理解方面进行了实验比较，研究发现较大的语言模型在实现字面解释方面表现较好，但在依赖社交方面的现象方面仍存在困难。 |
| [^173] | [Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages.](http://arxiv.org/abs/2212.05409) | 本文建立了适用于超过十亿印度语言使用者的自然语言理解能力的三个重要贡献，包括最大的单语语料库，包括20.9B的记号，涵盖24种语言，以及人工监督的基准数据集IndicXTREME，包括9种不同类型的NLU任务，跨越20种语言。这是首个旨在测试印度语言的多语言文本理解标准基准。 |
| [^174] | [Towards human-compatible autonomous car: A study of non-verbal Turing test in automated driving with affective transition modelling.](http://arxiv.org/abs/2212.02908) | 本文研究了自动驾驶汽车的人性化问题，通过非语言图灵测试，发现当前AI驾驶员不能创造类似人类的驾乘体验，需要在情感过渡模型中进行改进。 |
| [^175] | [SciRepEval: A Multi-Format Benchmark for Scientific Document Representations.](http://arxiv.org/abs/2211.13308) | SciRepEval是第一个综合评估科学文献表示的全面基准，其中包括四种格式的 25 个任务。通过使用格式特定的控制代码和适配器，可以改进科学文献表示模型的泛化能力。 |
| [^176] | [Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text.](http://arxiv.org/abs/2211.11300) | 本文提出了一种多层知识蒸馏方法，融合了语言模型的训练和微调方法来进行文本中的离群检测，实验结果表明其有效性。 |
| [^177] | [Conceptor-Aided Debiasing of Large Language Models.](http://arxiv.org/abs/2211.11087) | 本论文提出一种基于概念器的大型语言模型去偏见方法。我们通过后处理和一种新架构CI-BERT将概念器投影纳入所有层中。概念器后处理方法取得了最先进的去偏见结果，同时保持或改善了模型的性能。 |
| [^178] | [Efficient Transformers with Dynamic Token Pooling.](http://arxiv.org/abs/2211.09761) | 本论文提出了一种动态记号池化机制，可以在自然意义单位具有不同长度的情况下，缩短中间层的序列长度。实验证明，相比具有固定池化的普通Transformer，动态池化机制既更快，更准确地进行分段和语言建模。 |
| [^179] | [Style Classification of Rabbinic Literature for Detection of Lost Midrash Tanhuma Material.](http://arxiv.org/abs/2211.09710) | 本文提出了一种拉比文学的分类系统，可以通过其风格来检测Midrash Tanhuma中的失落材料。 |
| [^180] | [Generating Multilingual Gender-Ambiguous Text-to-Speech Voices.](http://arxiv.org/abs/2211.00375) | 该论文介绍了一种生成多语言的性别不明确的TTS声音的方法，通过提出的性别感知方法从潜在说话人中有效地进行采样，成功生成了一系列新的、多样化的、一致性和性别不明确性更强的声音，具有很强的实验表现。 |
| [^181] | [Towards standardizing Korean Grammatical Error Correction: Datasets and Annotation.](http://arxiv.org/abs/2210.14389) | 本文提出了三个涵盖广泛韩语语法错误的数据集和一个KAGAS注释系统。我们使用这些数据集训练的基线模型，显示其在更广泛的错误类型上明显优于当前使用的统计韩语GEC系统(Hanspell)。 |
| [^182] | [Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations.](http://arxiv.org/abs/2210.07586) | 本篇论文提出了一种名为HighGEN的新框架，通过使用短语嵌入搜索方法生成实体丰富的伪字典，在使用嵌入距离验证过程减少误报的基础上生成高覆盖率的NER数据集。 |
| [^183] | [News Summarization and Evaluation in the Era of GPT-3.](http://arxiv.org/abs/2209.12356) | GPT-3在新闻摘要中具有出色表现，但自动评价指标无法可靠评估其清晰度和准确性，从而提出了一个新的评估挑战。 |
| [^184] | [Evaluating and Inducing Personality in Pre-trained Language Models.](http://arxiv.org/abs/2206.07550) | 本文提出了Machine Personality Inventory (MPI)数据集用于评估机器个性，通过评估，证明了预训练语言模型(LLM)具有个性。进一步提出了Personality Prompting (P^2)方法，用于以可控的方式诱导LLM具有特定个性。 |
| [^185] | [Making Large Language Models Better Reasoners with Step-Aware Verifier.](http://arxiv.org/abs/2206.02336) | 本论文提出了一种名为DIVERSE的新方法，它使用多样的提示、验证器过滤不正确的答案并逐个验证每个推理步骤，以进一步增强语言模型的推理能力。 |
| [^186] | [GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles.](http://arxiv.org/abs/2205.12505) | 本文提出了一个大而全的EAE本体论，105个事件和220个论元角色的包含在内，利用这个本体论创建了一种多样化的通用性基准测试数据集GENEVA，共包含四个测试套件，旨在评估模型处理有限数据的能力。 |
| [^187] | [Enhancing Cross-lingual Prompting with Dual Prompt Augmentation.](http://arxiv.org/abs/2202.07255) | 该论文提出了双重提示增强跨语言提示的方法DPA，利用语言无关的通用提示方法，大大缓解了数据稀缺问题。在XNLI任务上，该方法可以在仅有16个英语训练样本的情况下，将性能从34.99%提高到46.54%。 |
| [^188] | [Different Affordances on Facebook and SMS Text Messaging Do Not Impede Generalization of Language-Based Predictive Models.](http://arxiv.org/abs/2202.01802) | 本研究研究了Facebook和短信文本之间的差异对基于语言的预测模型的泛化的影响，并发现这些差异并不会显著影响这些模型在这两个平台之间的相关性。 |
| [^189] | [A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models.](http://arxiv.org/abs/2201.05337) | 这篇论文综述了基于Transformer预训练语言模型的可控文本生成方法。这些方法利用大规模预训练语言模型生成多样化、流畅的文本，但由于深度神经网络的可解释性较低，需要保证其可控性。 |
| [^190] | [DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models.](http://arxiv.org/abs/2111.00160) | 本研究提出了一种名为DSEE的框架，通过利用权重更新和最终模型权重的稀疏先验，以实现资源和参数效率高的微调。实验证明，DSEE可显著减少参数和推理时间，同时达到与最先进微调方法相当的性能水平。 |

# 详细

[^1]: MQuAKE：通过多跳问题评估语言模型中的知识编辑

    MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions. (arXiv:2305.14795v1 [cs.CL])

    [http://arxiv.org/abs/2305.14795](http://arxiv.org/abs/2305.14795)

    本文提出了一种基准测试MQuAKE，通过多跳问题评估编辑模型是否能够正确回答因编辑事实而答案应该改变的问题。研究发现当前的知识编辑方法可以准确召回已编辑的事实，但在多跳问题上表现灾难性失败。

    

    大型语言模型（LLM）中存储的信息很快就会过时，重新训练并非总是可行的选择。这促使人们开发了通过更新模型权重注入新事实的一系列技术。当前的评估方法非常有限，主要验证编辑事实的召回率，但更改一个事实应该会对模型的相关信念产生连锁反应。如果我们编辑英国首相为Rishi Sunak，那么对于“谁是英国首相的配偶”这个问题，我们应该得到一个不同的答案。在这项工作中，我们提出了一个基准MQuAKE（用于知识编辑的多跳问答），包括多跳问题，评估编辑后的模型是否正确回答那些因编辑事实而答案应该改变的问题。虽然我们发现当前的知识编辑方法可以准确召回已编辑的事实，但它们在构建的多跳问题上遭遇了灾难性失败。因此，我们建议对LLMs的评估必须超越简单的事实召回，并纳入更微妙的知识编辑质量评估。

    The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model's related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark MQuAKE (Multi-hop Question Answering for Knowledge Editing) comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus 
    
[^2]: 省心学习变得领先：重新审视基于简单种子弱监督文本分类

    Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification. (arXiv:2305.14794v1 [cs.CL])

    [http://arxiv.org/abs/2305.14794](http://arxiv.org/abs/2305.14794)

    本文重新审视了基于种子匹配的伪标签生成方法，并通过简单的单词删除来缓解因规则注入的标签偏见而带来的影响，提高该方法的性能，其性能达到甚至超过最先进技术。

    

    近来，弱监督文本分类的研究主要集中在设计复杂的方法，将高层次的人类启发式方法转化为高质量的伪标签。在本文中，我们重新审视了基于种子匹配的方法，它是生成伪标签的最简单方法，我们展示了它的强大性能。我们表明种子匹配的有限性能很大程度上归因于种子匹配规则注入的标签偏差，这会阻止分类器学习可靠的置信度来选择高质量伪标签。有趣的是，简单地删除匹配输入文本中的种子词可以缓解标签偏差并帮助学习更好的置信度。随后，种子匹配的性能可以显著提高，使它达到或甚至超过最先进技术。此外，为了处理种子词不为人知的情况，我们建议简单地删除输入文本中的单词标记。

    Recent advances in weakly supervised text classification mostly focus on designing sophisticated methods to turn high-level human heuristics into quality pseudo-labels. In this paper, we revisit the seed matching-based method, which is arguably the simplest way to generate pseudo-labels, and show that its power was greatly underestimated. We show that the limited performance of seed matching is largely due to the label bias injected by the simple seed-match rule, which prevents the classifier from learning reliable confidence for selecting high-quality pseudo-labels. Interestingly, simply deleting the seed words present in the matched input texts can mitigate the label bias and help learn better confidence. Subsequently, the performance achieved by seed matching can be improved significantly, making it on par with or even better than the state-of-the-art. Furthermore, to handle the case when the seed words are not made known, we propose to simply delete the word tokens in the input tex
    
[^3]: 基于循环训练的低资源数据生成文本方法来自于准确性

    Faithful Low-Resource Data-to-Text Generation through Cycle Training. (arXiv:2305.14793v1 [cs.CL])

    [http://arxiv.org/abs/2305.14793](http://arxiv.org/abs/2305.14793)

    本文通过基于循环训练的方法，在少量监督数据的情况下，实现了生成文本任务与全监督方法相近的性能，同时极大地提高了非域数据生成的文本的准确性。

    

    近年来，从结构化数据生成文本的方法取得了显著进展，主要是通过在大型数据集上微调预训练的语言模型。然而，这些模型在特定领域的数据上可能无法产生与输入数据相符的输出文本，尤其是在域外数据上。由于缺少特定领域的足够注释数据，因此我们寻求一种无监督的方法来改善输出文本的准确性。我们在本文中通过循环训练来解决这个问题，因为这个问题本质上是结构化数据和文本之间表示的一致性问题。循环训练使用两个互为反函数的模型：一个从结构化数据生成文本，另一个从自然语言文本生成结构化数据。我们表明，在少量监督数据（我们的情况下100个样本）的情况下初始化的循环训练方法，可以实现数据生成文本任务与全监督方法相近的性能，同时极大地提高了非域数据生成的文本的准确性。

    Methods to generate text from structured data have advanced significantly in recent years, primarily due to fine-tuning of pre-trained language models on large datasets. However, such models can fail to produce output faithful to the input data, particularly on out-of-domain data. Sufficient annotated data is often not available for specific domains, leading us to seek an unsupervised approach to improve the faithfulness of output text. Since the problem is fundamentally one of consistency between the representations of the structured data and text, we evaluate the effectiveness of cycle training in this work. Cycle training uses two models which are inverses of each other: one that generates text from structured data, and one which generates the structured data from natural language text. We show that cycle training, when initialized with a small amount of supervised data (100 samples in our case), achieves nearly the same performance as fully supervised approaches for the data-to-tex
    
[^4]: 大型语言模型作为反事实生成器: 优势和劣势

    Large Language Models as Counterfactual Generator: Strengths and Weaknesses. (arXiv:2305.14791v1 [cs.CL])

    [http://arxiv.org/abs/2305.14791](http://arxiv.org/abs/2305.14791)

    本文研究了大型语言模型（LLMs）作为反事实生成器的能力，通过数据增强实验发现它们在各个任务中表现优异，但仍存在自我限制和缺乏逻辑指导等问题。

    

    大型语言模型（LLMs）在自然语言理解和生成任务中表现出卓越的性能。然而，它们生成反事实的能力，如用于数据增强的反事实，仍未得到充分的研究。本研究旨在调查LLMs的反事实生成能力，并分析影响这种能力的因素。首先，我们通过在情感分析、自然语言推理、命名实体识别和关系抽取等四个任务中进行数据增强实验来评估LLMs在反事实生成方面的效果。虽然LLMs在各种设置中都表现出有希望的改进，但由于其自我限制和缺乏与常识相符的逻辑指导来生成反事实，它们在复杂任务中仍然面临困难。其次，我们的分析揭示了为LLMs提供准确的任务定义和详细的逐步指导在生成有效的反事实中发挥了关键作用。

    Large language models (LLMs) have demonstrated remarkable performance in a range of natural language understanding and generation tasks. Yet, their ability to generate counterfactuals, which can be used for areas like data augmentation, remains under-explored. This study aims to investigate the counterfactual generation capabilities of LLMs and analysis factors that influence this ability. First, we evaluate how effective are LLMs in counterfactual generation through data augmentation experiments for small language models (SLMs) across four tasks: sentiment analysis, natural language inference, named entity recognition, and relation extraction. While LLMs show promising enhancements in various settings, they struggle in complex tasks due to their self-limitations and the lack of logical guidance to produce counterfactuals that align with commonsense. Second, our analysis reveals the pivotal role of providing accurate task definitions and detailed step-by-step instructions to LLMs in ge
    
[^5]: 提升中文文本主题划分和纲要生成：段落级主题表示，语料库和基准

    Advancing Topic Segmentation and Outline Generation in Chinese Texts: The Paragraph-level Topic Representation, Corpus, and Benchmark. (arXiv:2305.14790v1 [cs.CL])

    [http://arxiv.org/abs/2305.14790](http://arxiv.org/abs/2305.14790)

    本文提出了一种分层的段落级中文主题结构表示，使用句子而不是关键词来表示子主题，构建了大规模、高质量的中文段落级主题结构语料库。

    

    主题划分和纲要生成旨在将一个文档分成连贯的主题段落并生成相应的子标题。这个过程揭示了一个文档的话题结构，有助于从更高的层次快速把握和理解文档的整体情境。然而，与英语领域取得的成功相比，由于缺乏适当的段落级主题表示和大规模、高质量的中文语料库，这一领域的研究和应用受到了限制。为了解决这些问题，我们引入了一种分层的段落级主题结构表示，包括标题、子标题和段落，综合地模拟了文档的话题结构。此外，我们通过使用句子而不是关键词来表示子主题，确保更全面地表示文档内的主题分布。根据这种表示，我们构建了最大的中文段落级主题结构语料库之一。

    Topic segmentation and outline generation strive to divide a document into coherent topic sections and generate corresponding subheadings. Such a process unveils the discourse topic structure of a document that benefits quickly grasping and understanding the overall context of the document from a higher level. However, research and applications in this field have been restrained due to the lack of proper paragraph-level topic representations and large-scale, high-quality corpora in Chinese compared to the success achieved in English. Addressing these issues, we introduce a hierarchical paragraph-level topic structure representation with title, subheading, and paragraph that comprehensively models the document discourse topic structure. In addition, we ensure a more holistic representation of topic distribution within the document by using sentences instead of keywords to represent sub-topics. Following this representation, we construct the largest Chinese Paragraph-level Topic Structur
    
[^6]: 将语言模型改进为自动压缩器以提高模型上下文的利用效率

    Adapting Language Models to Compress Contexts. (arXiv:2305.14788v1 [cs.CL])

    [http://arxiv.org/abs/2305.14788](http://arxiv.org/abs/2305.14788)

    本论文提出了一种将预训练的语言模型改进为自动压缩器的方法，能够将长篇文本压缩成紧凑的摘要向量，提高上下文的利用效率和降低计算成本，同时通过在上下文学习中的应用，证明了该方法能够提高精度并降低推断成本。

    

    基于Transformer的语言模型是功能强大且广泛应用的工具，但其有限的上下文窗口和高计算成本约束了其实用性。本文提出了将预训练的语言模型改进为自动压缩器，能够将长篇文本压缩成紧凑的摘要向量，从而提高上下文的利用效率和降低计算成本。同时，摘要向量通过无监督学习的方式进行训练，并作为软提示被模型使用。

    Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments and summary vectors from all previous segments are used in language modeling. We fine-tune OPT models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations. We find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference cost. Finally, we explore the benefits of pre-
    
[^7]: ChatGPT和简单的语言推断：盲点和缺陷

    ChatGPT and Simple Linguistic Inferences: Blind Spots and Blinds. (arXiv:2305.14785v1 [cs.CL])

    [http://arxiv.org/abs/2305.14785](http://arxiv.org/abs/2305.14785)

    本文研究了ChatGPT语言模型的局限性，指出其在简单语言推断任务中存在困难，并探讨了如何改善其对基本语言概念的理解。

    

    本文研究了ChatGPT的理解能力限制，针对对于人类来说通常简单的推断任务，但这些似乎对该模型具有挑战性。具体来说，我们针对(i)语法规定的蕴含，(ii)带有不确定性证据副词的前提，以及(iii)单调蕴含性进行了研究。我们为这些推理类型提供了专家设计的评估集，并在零样本设置下进行实验。我们的结果表明，该模型在这些推理类型方面存在困难，展示中等到低精度。此外，尽管ChatGPT在直接提示下表现出对底层语言概念的了解，但它经常不能利用这些知识作出正确的推断。更有趣的是，进一步的实验表明，将前提嵌入前提条件触发或非实际性动词会导致模型更频繁地预测蕴含，而不考虑正确的语义标签。总的来说，这些结果揭示了当前语言模型的局限性，以及继续改善它们对基本语言概念的理解的必要性。

    This paper sheds light on the limitations of ChatGPT's understanding capabilities, focusing on simple inference tasks that are typically easy for humans but appear to be challenging for the model. Specifically, we target (i) grammatically-specified entailments, (ii) premises with evidential adverbs of uncertainty, and (iii) monotonicity entailments. We present expert-designed evaluation sets for these inference types and conduct experiments in a zero-shot setup. Our results show that the model struggles with these types of inferences, exhibiting moderate to low accuracy. Moreover, while ChatGPT demonstrates knowledge of the underlying linguistic concepts when prompted directly, it often fails to incorporate this knowledge to make correct inferences. Even more strikingly, further experiments show that embedding the premise under presupposition triggers or non-factive verbs causes the model to predict entailment more frequently {regardless} of the correct semantic label. Overall these re
    
[^8]: 中文拼写纠错中的语音学表示分解

    Disentangled Phonetic Representation for Chinese Spelling Correction. (arXiv:2305.14783v1 [cs.CL])

    [http://arxiv.org/abs/2305.14783](http://arxiv.org/abs/2305.14783)

    本论文提出了一种新的方法，通过解开中文文本中的语音学信息和字符信息，实现两者之间的直接交互，有效利用语音学信息提高中文拼写纠错的准确性。

    

    中文拼写纠错旨在检测和纠正中文文本中的错误字符。虽然在这项任务中引入了语音学信息（汉语拼音），但通常会将语音学表示与字符表示合并，这往往会削弱正常文本的表示效果。在这项工作中，我们提出将两种类型的特征解开以允许文本和语音信息之间的直接交互。为了学习有用的语音学表示，我们引入了一个拼音到字符的目标，要求模型仅基于语音信息预测正确的字符，其中施加了一个分离掩码以禁用从语音输入到文本的注意力。为了避免过度拟合语音，我们进一步设计了一个自我蒸馏模块，以确保语义信息在预测中起主导作用。在三个中文拼写纠错基准测试上的大量实验证明了我们的方法在使用语音学信息进行中文拼写纠错方面的优越性。

    Chinese Spelling Correction (CSC) aims to detect and correct erroneous characters in Chinese texts. Although efforts have been made to introduce phonetic information (Hanyu Pinyin) in this task, they typically merge phonetic representations with character representations, which tends to weaken the representation effect of normal texts. In this work, we propose to disentangle the two types of features to allow for direct interaction between textual and phonetic information. To learn useful phonetic representations, we introduce a pinyin-to-character objective to ask the model to predict the correct characters based solely on phonetic information, where a separation mask is imposed to disable attention from phonetic input to text. To avoid overfitting the phonetics, we further design a self-distillation module to ensure that semantic information plays a major role in the prediction. Extensive experiments on three CSC benchmarks demonstrate the superiority of our method in using phonetic 
    
[^9]: Twitter图像的文本条件下的替代文本生成

    Text Conditional Alt-Text Generation for Twitter Images. (arXiv:2305.14779v1 [cs.CV])

    [http://arxiv.org/abs/2305.14779](http://arxiv.org/abs/2305.14779)

    本文针对Twitter上分享的图像提出了一种文本条件下的替代文本生成方法。通过CLIP前缀模型，该模型结合图像和推文中的文本信息，生成关于图像的上下文相关的替代文本。

    

    本文提出了一种针对社交媒体特别是Twitter上分享的图像生成替代文本（或alt-text）描述的方法。与图像的字幕不同，文本替换文本更加直白描述和上下文特定。此外，关键是，发布到Twitter上的图像通常是由用户编写的文本附加的，尽管这些文本不一定描述图像，但可能提供有用的上下文信息，如果正确利用可以提供信息，例如推文可能会命名图片中模型之前没有见过的不常见的对象。我们通过一个CLIP前缀模型来解决这个问题，该模型提取图像的嵌入并将其传递给映射网络，该网络输出单词嵌入空间中的短序列，或称为“前缀”，我们将推文本身的文本也连接到其中。这样，模型就可以在文章中条件化视觉和文本信息。然后将合并的多模式前缀作为预训练的语言模型的提示输入。

    In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter. This task is more than just a special case of image captioning, as alt-text is both more literally descriptive and context-specific. Also critically, images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly leveraged can be informative -- e.g. the tweet may name an uncommon object in the image that the model has not previously seen. We address this with a CLIP prefix model that extracts an embedding of the image and passes it to a mapping network that outputs a short sequence in word embedding space, or a ``prefix'', to which we also concatenate the text from the tweet itself. This lets the model condition on both visual and textual information from the post. The combined multimodal prefix is then fed as a prompt to a pretrained lang
    
[^10]: 在预训练语言模型中测量知识获取和利用之间的差距

    Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models. (arXiv:2305.14775v1 [cs.CL])

    [http://arxiv.org/abs/2305.14775](http://arxiv.org/abs/2305.14775)

    本文提出了一个系统性的框架来衡量PLMs中参数化知识的利用，研究发现PLMs存在已获取的知识和利用的知识之间的差距，在分布变化下有限的鲁棒性，较大的模型可以弥补已获取知识的差距，但利用知识的差距仍然存在。

    

    尽管预训练语言模型（PLMs）已经显示出获取了大量的知识，但目前仍不清楚这些参数化知识中有多少实际可用于下游任务的执行。本文提出了一个系统性的框架来衡量PLMs中参数化知识的利用。我们的框架首先从PLM参数中提取知识，随后围绕这些提取的知识构建下游任务。因此，模型的表现完全依赖于利用模型所具备的知识，避免了不充分的信号等混淆因素。作为一个示例，我们研究了PLMs中的事实知识，并测量了125M到13B参数PLMs的利用。我们观察到：（1）PLMs在已获取的知识和利用的知识之间存在两个差距，（2）在分布变化下，它们在利用知识方面表现出有限的鲁棒性，（3）较大的模型可以弥补已获取知识的差距，但利用知识的差距仍然存在。总的来说，我们的研究为当前PLMs在利用已获取知识方面的局限性提供了见解。

    While pre-trained language models (PLMs) have shown evidence of acquiring vast amounts of knowledge, it remains unclear how much of this parametric knowledge is actually usable in performing downstream tasks. We propose a systematic framework to measure parametric knowledge utilization in PLMs. Our framework first extracts knowledge from a PLM's parameters and subsequently constructs a downstream task around this extracted knowledge. Performance on this task thus depends exclusively on utilizing the model's possessed knowledge, avoiding confounding factors like insufficient signal. As an instantiation, we study factual knowledge of PLMs and measure utilization across 125M to 13B parameter PLMs. We observe that: (1) PLMs exhibit two gaps in acquired vs. utilized knowledge, (2) they show limited robustness in utilizing knowledge under distribution shifts, and (3) larger models close the acquired knowledge gap but the utilized knowledge gap remains. Overall, our study provides insights 
    
[^11]: 一种可控的基于问答的去文本化框架

    A Controllable QA-based Framework for Decontextualization. (arXiv:2305.14772v1 [cs.CL])

    [http://arxiv.org/abs/2305.14772](http://arxiv.org/abs/2305.14772)

    本文提出了一个基于问答的去文本化框架，可以更好地展示提取的文本摘录。在问答和引证上的表现类似于端到端方法，并且支持用户信息需求及偏好的可控性。

    

    许多真实场景下的应用需要将提取的摘录展示给用户，这些摘录往往需要解耦原来的文本才能更好地呈现给用户。本文研究了LLMs在问答和引证上的去文本化能力，并提出了一个基于问答的去文本化框架，该框架可以更好地满足用户信息需求及偏好，并且在结果上表现出类似于端到端方法的竞争力。我们同时探讨了如何通过该框架将用户偏好融入到系统中，从而实现了可控性。

    Many real-world applications require surfacing extracted snippets to users, whether motivated by assistive tools for literature surveys or document cross-referencing, or needs to mitigate and recover from model generated inaccuracies., Yet, these passages can be difficult to consume when divorced from their original document context. In this work, we explore the limits of LLMs to perform decontextualization of document snippets in user-facing scenarios, focusing on two real-world settings - question answering and citation context previews for scientific documents. We propose a question-answering framework for decontextualization that allows for better handling of user information needs and preferences when determining the scope of rewriting. We present results showing state-of-the-art LLMs under our framework remain competitive with end-to-end approaches. We also explore incorporating user preferences into the system, finding our framework allows for controllability.
    
[^12]: SSD-2：扩展和推理时间融合的扩散语言模型

    SSD-2: Scaling and Inference-time Fusion of Diffusion Language Models. (arXiv:2305.14771v1 [cs.CL])

    [http://arxiv.org/abs/2305.14771](http://arxiv.org/abs/2305.14771)

    本文介绍了一种扩散语言模型SSD-2，它可以从0.4B扩展到13B参数，并经过微调来遵循指令。与自回归模型相比，使用SSD-2可以形成更有效的模型合作，产生更好的结果。

    

    基于扩散的语言模型被证明是具有竞争力的生成模型，易于在推理时进行控制，并且是自回归语言模型的有希望的替代方案。然而，现有的扩散模型仅在相对较小的规模下进行了研究。本文通过对最近提出的扩散模型SSD-LM的研究，探索了将其从0.4B扩展到13B参数的方法，并提出了几种改进其训练和推理效率的技术。我们命名这个新模型为SSD-2。我们还展示了这个模型可以很容易地通过微调来遵循指令。最后，利用扩散模型在推理时的控制能力，我们展示了SSD-2可以与100倍更小的模型合作形成新的集成模型，这些模型可以由个人用户进行定制和部署。与自回归模型相比，我们发现扩散模型之间的协作更加有效，从而产生更好的结果。

    Diffusion-based language models (LMs) have been shown to be competent generative models that are easy to control at inference and are a promising alternative to autoregressive LMs. While autoregressive LMs have benefited immensely from scaling and instruction-based learning, existing studies on diffusion LMs have been conducted on a relatively smaller scale. Starting with a recently proposed diffusion model SSD-LM, in this work we explore methods to scale it from 0.4B to 13B parameters, proposing several techniques to improve its training and inference efficiency. We call the new model SSD-2. We further show that this model can be easily finetuned to follow instructions. Finally, leveraging diffusion models' capability at inference-time control, we show that SSD-2 facilitates novel ensembles with 100x smaller models that can be customized and deployed by individual users. We find that compared to autoregressive models, the collaboration between diffusion models is more effective, leadi
    
[^13]: 利用自然语言解释重新调整人类评价

    Using Natural Language Explanations to Rescale Human Judgments. (arXiv:2305.14770v1 [cs.CL])

    [http://arxiv.org/abs/2305.14770](http://arxiv.org/abs/2305.14770)

    本文提出使用自然语言解释来调整标注者之间存在的尺度不一致，解决了主观NLP任务中标注者之间分歧的问题。

    

    大型语言模型（LLM）的出现带来了需要高质量人标记数据的紧迫需求，特别是对于人的反馈和评估等过程。一种常见的做法是通过多个众包工作者的共识来标注数据。然而，不同的标注者可能对标注方案有不同的解释，除非接受了广泛的培训，否则对于主观的NLP任务，甚至受过训练的专家标注者也可能会出现巨大的分歧。我们展示了这些细微差别可以通过高质量的自然语言解释进行捕捉，提出了一种使用LLM在存在分歧时重新调整大小排序注释的方法。具体而言，我们将Likert评分和相应的自然语言解释输入LLM，并提示它产生一个数字得分。这个得分应该反映注释者对示例的基本评估。解释的存在使LLM能够在尺度使用差异存在的情况下使评级在标注者之间同质化。

    The rise of large language models (LLMs) has brought a critical need for high-quality human-labeled data, particularly for processes like human feedback and evaluation. A common practice is to label data via consensus annotation over the judgments of multiple crowdworkers. However, different annotators may have different interpretations of labeling schemes unless given extensive training, and for subjective NLP tasks, even trained expert annotators can diverge heavily. We show that these nuances can be captured by high quality natural language explanations, and propose a method to rescale ordinal annotation in the presence of disagreement using LLMs. Specifically, we feed Likert ratings and corresponding natural language explanations into an LLM and prompt it to produce a numeric score. This score should reflect the underlying assessment of the example by the annotator. The presence of explanations allows the LLM to homogenize ratings across annotators in spite of scale usage differenc
    
[^14]: BeamSearchQA: 大型语言模型是强大的零-shot QA求解器

    BeamSearchQA: Large Language Models are Strong Zero-Shot QA Solver. (arXiv:2305.14766v1 [cs.CL])

    [http://arxiv.org/abs/2305.14766](http://arxiv.org/abs/2305.14766)

    BeamSearchQA利用大型语言模型进行迭代式生成问题，以捕捉隐含知识并优化问答过程，在NQ和WebQ测试集上分别达到了71.7％和46.7％的F1分数，显着优于现有的最先进方法。

    

    开放领域的问答是一个关键任务，通常需要访问外部信息。现有方法通常采用单轮检索-阅读方法，首先检索相关文档，然后基于检索的信息回答问题。然而，在某些情况下，回答问题需要隐含的知识，这些知识不直接从问题本身中获得。在这项工作中，我们提出了一种新的问答流程，称为BeamSearchQA。我们的方法利用大规模语言模型（LLMs）迭代生成关于原始问题的新问题，实现迭代推理过程。通过迭代细化和扩展问题的范围，我们的方法旨在捕捉并利用可能无法通过检索直接获取的隐藏知识。我们在广泛使用的开放领域NQ和WebQ数据集上评估了我们的方法。实验结果表明，BeamSearchQA明显优于现有的最先进方法，在NQ和WebQ测试集上分别达到了71.7％和46.7％的F1分数。

    Open-domain question answering is a crucial task that often requires accessing external information. Existing methods typically adopt a single-turn retrieve-then-read approach, where relevant documents are first retrieved, and questions are then answered based on the retrieved information. However, there are cases where answering a question requires implicit knowledge that is not directly retrievable from the question itself. In this work, we propose a novel question-answering pipeline called eamSearchQA. Our approach leverages large language models(LLMs) to iteratively generate new questions about the original question, enabling an iterative reasoning process. By iteratively refining and expanding the scope of the question, our method aims to capture and utilize hidden knowledge that may not be directly obtainable through retrieval. We evaluate our approach on the widely-used open-domain NQ and WebQ datasets. The experimental results demonstrate that BeamSearchQA significantly outperf
    
[^15]: Clever Hans还是神经心智理论？在大语言模型中对社交推理进行压力测试。

    Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models. (arXiv:2305.14763v1 [cs.CL])

    [http://arxiv.org/abs/2305.14763](http://arxiv.org/abs/2305.14763)

    本文研究了大型语言模型的神经心智理论能力，并发现虽然模型表现出一定程度的N-ToM能力，但远非稳健，存在对抗性例子困难等问题。

    

    关于人工智能的能力的争论日益升级，需要开发可靠的度量标准来评估机器的“智能”。最近，许多轶事性的例子被用来暗示像ChatGPT和GPT-4这样的新型大型语言模型（LLMs）展示了神经心智理论（N-ToM）;然而，先前的研究对这些能力得出了相互矛盾的结论。我们通过对6项任务进行广泛评估来调查LLMs的N-ToM程度，并发现LLMs表现出某些N-ToM能力，但这种行为还远非稳健。我们进一步检查影响N-ToM任务表现的因素，并发现LLMs在对抗性例子上存在困难，表明依赖于浅层启发式而不是稳健的ToM能力。我们警告不要从轶事性的例子、有限的基准测试和使用人类设计的心理测试来评估模型中得出结论。

    The escalating debate on AI's capabilities warrants developing reliable metrics to assess machine "intelligence". Recently, many anecdotal examples were used to suggest that newer large language models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.
    
[^16]: UniChart：面向图表理解和推理的通用视觉语言预训练模型

    UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning. (arXiv:2305.14761v1 [cs.CL])

    [http://arxiv.org/abs/2305.14761](http://arxiv.org/abs/2305.14761)

    本文提出了UniChart，这是一个用于图表理解和推理的通用视觉语言预训练模型。UniChart首先建立了一个包括各种不同主题和视觉风格的大量图表语料库，然后使用图表特定的预训练任务来编码图表，并在几个图表理解和推理任务上表现出色。

    

    图表在数据分析、可视化重要见解和回答数据的复杂推理问题方面非常流行。为了方便使用自然语言进行基于图表的数据分析，最近引入了几个下游任务，例如图表问答和图表总结。然而，大多数解决这些任务的方法都使用语言或视觉-语言任务的预训练，而不试图明确建模图表的结构（例如，如何视觉编码数据以及如何将图表元素相互关联）。为了解决这个问题，我们首先建立了一个包括各种不同主题和视觉风格的大量图表语料库。然后，我们提出了UniChart，这是一个用于图表理解和推理的预训练模型。UniChart对图表的相关文本、数据和视觉元素进行编码，然后使用基于图表的文本解码器以自然语言生成预期的输出。我们提出了几个面向图表的预训练任务，包括：（i）低层次视觉编码预测，（ii）图表元素关系预测和（iii）图表问题回答预测。我们的评估显示，UniChart在几个图表理解和推理任务上表现优于强基线。

    Charts are very popular for analyzing data, visualizing key insights and answering complex reasoning questions about data. To facilitate chart-based data analysis using natural language, several downstream tasks have been introduced recently such as chart question answering and chart summarization. However, most of the methods that solve these tasks use pretraining on language or vision-language tasks that do not attempt to explicitly model the structure of the charts (e.g., how data is visually encoded and how chart elements are related to each other). To address this, we first build a large corpus of charts covering a wide variety of topics and visual styles. We then present UniChart, a pretrained model for chart comprehension and reasoning. UniChart encodes the relevant text, data, and visual elements of charts and then uses a chart-grounded text decoder to generate the expected output in natural language. We propose several chart-specific pretraining tasks that include: (i) low-lev
    
[^17]: Bi-Drop: 自适应子网络优化的预训练语言模型通用微调

    Bi-Drop: Generalizable Fine-tuning for Pre-trained Language Models via Adaptive Subnetwork Optimization. (arXiv:2305.14760v1 [cs.CL])

    [http://arxiv.org/abs/2305.14760](http://arxiv.org/abs/2305.14760)

    本文提出了一种动态微调策略Bi-Drop来针对预训练语言模型在大规模微调时可能出现的过拟合现象。经过GLUE基准测试，Bi-Drop的性能优于其他微调方法，并对于多任务、多领域转移、数据不均衡和低资源情况下也表现出强大的鲁棒性。

    

    预训练语言模型在各种自然语言理解任务中已取得了显著成功。然而，如果训练集有限，对预训练模型的大规模微调容易出现过拟合，从而导致性能下降。本文提出了一种动态微调策略Bi-Drop来针对预训练语言模型的缺点。它利用dropout生成的各种子模型的梯度信息有选择性地更新模型参数。在GLUE基准测试中的实验表明，Bi-Drop的性能优于之前的微调方法，并在各种预训练模型中表现出对比纯微调更强的稳健性与通用性。此外，实证结果表明，在多任务、多领域转移、数据不均衡和低资源情况下，Bi-Drop的表现差异显著，具有强大的泛化能力和鲁棒性。

    Pretrained language models have achieved remarkable success in a variety of natural language understanding tasks. Nevertheless, finetuning large pretrained models on downstream tasks is susceptible to overfitting if the training set is limited, which will lead to diminished performance. In this work, we propose a dynamic fine-tuning strategy for pretrained language models called Bi-Drop. It utilizes the gradient information of various sub-models generated by dropout to update the model parameters selectively. Experiments on the GLUE benchmark show that Bi-Drop outperforms previous fine-tuning methods by a considerable margin, and exhibits consistent superiority over vanilla fine-tuning across various pretrained models. Furthermore, empirical results indicate that Bi-Drop yields substantial improvements in the multiple task or domain transfer, data imbalance, and low-resource scenarios, demonstrating superb generalization ability and robustness.
    
[^18]: 面向人类中心的度量评估对话系统

    Human-Centered Metrics for Dialog System Evaluation. (arXiv:2305.14757v1 [cs.CL])

    [http://arxiv.org/abs/2305.14757](http://arxiv.org/abs/2305.14757)

    本文提出了从心理学构造中提取的可解释度量标准，用于评估对话系统，通过情绪熵、语言风格和情感匹配、宜人性和共情等五个度量，这些人类度量标准与现有的自动度量标准不相关且具有更高的预测准确度。

    

    我们提出了一种通过心理学角度来评估对话系统的度量方法：对话代理人像人类一样表达了多种状态（短期因素，如情绪）和特质（更长期因素，如个性）。这些可解释的度量标准由来自已建立的心理学构造的五种度量组成，可以应用于对话和对话中的每个回合：情绪熵，语言风格和情感匹配，以及宜人性和共情。我们将这些人类度量标准与6种最先进的自动度量标准（例如BARTScore和BLEURT）在7个标准对话系统数据集上进行了比较。我们还介绍了一个新颖的数据集，即Three Bot Dialog Evaluation Corpus，其中包含来自ChatGPT、GPT-3和BlenderBot的已注释对话。我们证明了所提出的人类度量标准提供了新颖的信息，与自动度量标准不相关，并可在预测对话系统质量时超越现有的自动度量标准。

    We present metrics for evaluating dialog systems through a psychologically-grounded "human" lens: conversational agents express a diversity of both states (short-term factors like emotions) and traits (longer-term factors like personality) just as people do. These interpretable metrics consist of five measures from established psychology constructs that can be applied both across dialogs and on turns within dialogs: emotional entropy, linguistic style and emotion matching, as well as agreeableness and empathy. We compare these human metrics against 6 state-of-the-art automatic metrics (e.g. BARTScore and BLEURT) on 7 standard dialog system data sets. We also introduce a novel data set, the Three Bot Dialog Evaluation Corpus, which consists of annotated conversations from ChatGPT, GPT-3, and BlenderBot. We demonstrate the proposed human metrics offer novel information, are uncorrelated with automatic metrics, and lead to increased accuracy beyond existing automatic metrics for predictin
    
[^19]: 上下文模型及评估在文体改写中的必要性研究

    Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting. (arXiv:2305.14755v1 [cs.CL])

    [http://arxiv.org/abs/2305.14755](http://arxiv.org/abs/2305.14755)

    本文研究提出了在文体改写的重写和评估阶段整合文本上下文的必要性，并通过few-shot prompting比较非上下文改写和上下文改写的效果。研究发现，自动度量指标不一定能反映出人类的偏好。

    

    大多数现有的文体改写方法在句子级别操作，但是忽视文本更广泛的上下文可以导致改写结果是一般化、歧义和不连贯的。本文提出整合文本上下文到文体改写的重写和评估阶段，重点关注形式、毒性和情感转移任务。我们通过对 GPT-3.5 和 GPT NeoX 的 few-shot 提问比较重写的方法，并比较非上下文改写和上下文改写。我们的实验表明，人们通常更喜欢上下文改写，但自动度量指标（如 BLEU，sBERT）不是这样的。为弥合这种差距，我们提出通用自动度量指标的上下文融合版本，并证明这些更能反映人类偏好。总的来说，本文强调在文体改写的重写和评估阶段整合文本上下文的重要性。

    Most existing stylistic text rewriting methods operate on a sentence level, but ignoring the broader context of the text can lead to generic, ambiguous, and incoherent rewrites. In this paper, we propose the integration of preceding textual context into both the rewriting and evaluation stages of stylistic text rewriting, focusing on formality, toxicity, and sentiment transfer tasks. We conduct a comparative evaluation of rewriting through few-shot prompting of GPT-3.5 and GPT NeoX, comparing non-contextual rewrites to contextual rewrites. Our experiments show that humans often prefer contextual rewrites over non-contextual ones, but automatic metrics (e.g., BLEU, sBERT) do not. To bridge this gap, we propose context-infused versions of common automatic metrics, and show that these better reflect human preferences. Overall, our paper highlights the importance of integrating preceding textual context into both the rewriting and evaluation stages of stylistic text rewriting.
    
[^20]: DialogVCS：对话系统升级中的鲁棒自然语言理解

    DialogVCS: Robust Natural Language Understanding in Dialogue System Upgrade. (arXiv:2305.14751v1 [cs.CL])

    [http://arxiv.org/abs/2305.14751](http://arxiv.org/abs/2305.14751)

    本论文提出了DialogVCS，建立了4个数据集用于测试鲁棒NLU模型的可靠性。在这些数据集中，新意图的出现可能会与现有意图在语义上存在关联，作者提出了基于多标签分类任务的方法来解决这个问题。

    

    在产品对话系统的不断更新中，我们需要重新训练自然语言理解（NLU）模型，因为实际用户数据会与上一次更新累积的数据合并。新数据中会出现新的意图，并可能与现有意图在语义上存在纠缠，例如，语义过于特定或通用的新意图实际上是语义空间中某些现有意图的子集或超集，从而影响了NLU模型的鲁棒性。作为解决此问题的第一次尝试，我们设立了一个新的基准，包括4个对话版本控制数据集（DialogVCS）。我们将具有不完美数据的系统更新中的意图检测定义为具有正但未标记意图的多标签分类任务，要求模型在推断过程中识别所有适当的意图，包括具有语义纠缠的意图。我们还提出了全面的基准模型并进行了实验评估。

    In the constant updates of the product dialogue systems, we need to retrain the natural language understanding (NLU) model as new data from the real users would be merged into the existent data accumulated in the last updates. Within the newly added data, new intents would emerge and might have semantic entanglement with the existing intents, e.g. new intents that are semantically too specific or generic are actually subset or superset of some existing intents in the semantic space, thus impairing the robustness of the NLU model. As the first attempt to solve this problem, we setup a new benchmark consisting of 4 Dialogue Version Control dataSets (DialogVCS). We formulate the intent detection with imperfect data in the system update as a multi-label classification task with positive but unlabeled intents, which asks the models to recognize all the proper intents, including the ones with semantic entanglement, in the inference. We also propose comprehensive baseline models and conduct i
    
[^21]: 解决大语言模型在回答复杂问题时的评估问题：基于答案断言分解的细粒度自我评估方法

    Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation. (arXiv:2305.14750v1 [cs.CL])

    [http://arxiv.org/abs/2305.14750](http://arxiv.org/abs/2305.14750)

    大型语言模型（LLMs）常常不能完全回答复杂问题。我们提出基于答案断言分解的细粒度自我评估方法，以验证答案满足哪些问题标准。初步实验表明，该方法可帮助发现模型错误和知识差距。

    

    在回答复杂问题时，大型语言模型（LLMs）可能生成的答案不能满足问题的所有标准。虽然现有的自我评估技术旨在检测这些答案是否正确，但这些技术无法确定生成的答案满足问题的哪些标准。为了解决这个问题，我们提出了答案断言分解（ABCD），这是一种提示策略，可将问题分解为一系列可以用来验证答案满足哪些问题标准的真/假断言。使用分解的ABCD断言，我们执行了细粒度的自我评估。通过对三个数据集（包括新收集的挑战数据集ObscureQA）的初步实验，我们发现GPT-3.5有一定能力确定其答案在多大程度上满足输入问题的标准，并可以提供关于模型错误和知识差距的见解。

    When answering complex questions, large language models (LLMs) may produce answers that do not satisfy all criteria of the question. While existing self-evaluation techniques aim to detect if such answers are correct, these techniques are unable to determine which criteria of the question are satisfied by the generated answers. To address this issue, we propose answer-based claim decomposition (ABCD), a prompting strategy that decomposes questions into a series of true/false claims that can be used to verify which criteria of the input question an answer satisfies. Using the decomposed ABCD claims, we perform fine-grained self-evaluation. Through preliminary experiments on three datasets, including a newly-collected challenge dataset ObscureQA, we find that GPT-3.5 has some ability to determine to what extent its answer satisfies the criteria of the input question, and can give insights into the errors and knowledge gaps of the model.
    
[^22]: ECHo: 基于人类中心推理的事件因果推断

    ECHo: Event Causality Inference via Human-centric Reasoning. (arXiv:2305.14740v1 [cs.AI])

    [http://arxiv.org/abs/2305.14740](http://arxiv.org/abs/2305.14740)

    ECHo是一个基于人类中心推理的事件因果推断数据集，并提出了一个与CoT范式对齐的统一框架来评估当前AI系统的推理能力。

    

    我们介绍了 ECHo，这是一个基于视觉和语言社交情境的事件因果推断诊断数据集。 ECHo利用从犯罪剧中收集的真实人类中心演绎信息，通过激发中间心灵理论（ToM）来弥合多模态推理的鸿沟，从而提高社交智能。我们提出了一个与Chain-of-Thought（CoT）范式对齐的统一框架，以评估当前AI系统的推理能力。这个ToM增强的CoT管道可以在 零-shot视觉和语言理解中包容和整合各种大型基础模型。利用这个框架，我们通过三个互补的基于人类中心的ECHo任务来审查先进的大型语言和多模态模型。进一步的分析表明，ECHo是一个具有挑战性的数据集，可以暴露推理中的不完善和不一致性。

    We introduce ECHo, a diagnostic dataset of event causality inference grounded in visual-and-linguistic social scenarios. ECHo employs real-world human-centric deductive information collected from crime drama, bridging the gap in multimodal reasoning towards higher social intelligence through the elicitation of intermediate Theory-of-Mind (ToM). We propose a unified framework aligned with the Chain-of-Thought (CoT) paradigm to assess the reasoning capability of current AI systems. This ToM-enhanced CoT pipeline can accommodate and integrate various large foundation models in zero-shot visual-and-linguistic understanding. With this framework, we scrutinize the advanced large language and multimodal models via three complementary human-centric ECHo tasks. Further analysis demonstrates ECHo as a challenging dataset to expose imperfections and inconsistencies in reasoning.
    
[^23]: 在上下文感知解码的帮助下信任您的证据：更少的幻觉

    Trusting Your Evidence: Hallucinate Less with Context-aware Decoding. (arXiv:2305.14739v1 [cs.CL])

    [http://arxiv.org/abs/2305.14739](http://arxiv.org/abs/2305.14739)

    提出了上下文感知解码 (CAD)，在不需要额外训练的情况下，显著提高了不同LM系列的忠实度，特别是在解决知识冲突至关重要的任务中取得了显着的改进。

    

    语言模型经常难以充分关注输入上下文，并生成不忠实或包含幻觉的文本。为了缓解这个问题，我们提出了上下文感知解码 (CAD)，它遵循对比输出分布，可以放大使用模型时有无上下文时输出概率之间的差异。我们的实验表明，CAD在不需要额外训练的情况下，显著提高了不同LM系列的忠实度，包括OPT、GPT、LLaMA和FLAN-T5用于总结任务 (例如，LLaMA在事实度度量方面获得了14.3%的增益)。此外，当模型的先验知识与所提供的上下文冲突时，CAD特别有效，从而在解决知识冲突至关重要的任务中取得了显着的改进。

    Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
    
[^24]: 边缘聚焦：基于异常值的毒性检测中受损人群的识别

    Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection. (arXiv:2305.14735v1 [cs.CL])

    [http://arxiv.org/abs/2305.14735](http://arxiv.org/abs/2305.14735)

    本文提出了一种基于异常值的方法，用于识别在毒性检测中受到伤害的人群，发现对于这些异常值，模型性能较差，他们面临的毒性更高。

    

    衡量人工智能对边缘社区影响的标准方法是确定特定人口群体之间的性能差异。这些方法旨在解决针对弱势群体的伤害问题，但它们会掩盖由交叉子群或跨人口群体共享的伤害模式。相反，我们将“边缘”定义为具有远离“常态” 的人口属性的数据点，并度量针对这些异常值的伤害。我们提出了一种基于群体的性能差异指数（GPDI），以衡量数据集细分为子组对面临增加的伤害的识别程度。我们将我们的方法应用于检测毒性检测中的差异，并发现针对异常值的文本在所有类型的毒性检验中毒性更高，高达28％至86％。我们还发现，对于人口学异常值，模型性能始终较差，异常值和非异常值之间的错误差距高达10％。

    A standard method for measuring the impacts of AI on marginalized communities is to determine performance discrepancies between specified demographic groups. These approaches aim to address harms toward vulnerable groups, but they obscure harm patterns faced by intersectional subgroups or shared across demographic groups. We instead operationalize "the margins" as data points that are statistical outliers due to having demographic attributes distant from the "norm" and measure harms toward these outliers. We propose a Group-Based Performance Disparity Index (GPDI) that measures the extent to which a subdivision of a dataset into subgroups identifies those facing increased harms. We apply our approach to detecting disparities in toxicity detection and find that text targeting outliers is 28% to 86% more toxic for all types of toxicity examined. We also discover that model performance is consistently worse for demographic outliers, with disparities in error between outliers and non-outli
    
[^25]: 阿拉伯语语法错误检测和修正的进展：一项实证调查

    Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation. (arXiv:2305.14734v1 [cs.CL])

    [http://arxiv.org/abs/2305.14734](http://arxiv.org/abs/2305.14734)

    本文使用了两个预训练的序列到序列模型在阿拉伯语GEC方面取得了最新的结果，并提出下文第一阿拉伯语多类语法错误检测结果。文章表明，使用GED作为GEC模型的辅助输入有助于提高性能，而上下文形态预处理有助于接着GEC系统，文章在两个共享任务数据集上取得了最新的GEC结果以及一个强有力的基准。

    

    语法错误纠正(GEC)是一个经过深入研究的英语问题，在许多已有的模型和数据集的基础上进行了研究。然而，由于数据稀缺和语言复杂性等挑战，对形态丰富的语言进行GEC研究的限制较多。本文通过使用两个新开发的基于Transformer预训练序列到序列模型，首次在阿拉伯语GEC中获得了结果。我们解决了多类阿拉伯语语法错误检测(GED)任务，并首次在多类阿拉伯语GED上获得了结果。我们表明，在跨越不同类型的三个数据集上使用GED信息作为辅助输入的GEC模型可以提高GEC性能。此外，我们还研究了上下文形态预处理在帮助GEC系统方面的应用。我们的模型在两个阿拉伯语GEC共享任务数据集上取得了最新结果，并在新创建的数据集上建立了一个强有力的基准。

    Grammatical error correction (GEC) is a well-explored problem in English with many existing models and datasets. However, research on GEC in morphologically rich languages has been limited due to challenges such as data scarcity and language complexity. In this paper, we present the first results on Arabic GEC by using two newly developed Transformer-based pretrained sequence-to-sequence models. We address the task of multi-class Arabic grammatical error detection (GED) and present the first results on multi-class Arabic GED. We show that using GED information as auxiliary input in GEC models improves GEC performance across three datasets spanning different genres. Moreover, we also investigate the use of contextual morphological preprocessing in aiding GEC systems. Our models achieve state-of-the-art results on two Arabic GEC shared tasks datasets and establish a strong benchmark on a newly created dataset.
    
[^26]: SenteCon: 利用词汇表学习人类可解释的语言表示

    SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations. (arXiv:2305.14728v1 [cs.CL])

    [http://arxiv.org/abs/2305.14728](http://arxiv.org/abs/2305.14728)

    SenteCon是一种能够提供高级可解释性的方法，通过把文本编码为可解释类别的层，同时不会对下游任务的预测性能造成影响。

    

    近年来，深度语言表示已成为语言特征化的主要形式，但在许多情况下，了解模型的决策过程是很重要的。这不仅需要一个可解释的模型，还需要可解释的特征。特别是，语言必须以可解释的方式特征化，同时仍然很好地描述原始文本。我们提出了SenteCon，一种在深度语言表示中引入人类可解释性的方法。给定一个文本段落，SenteCon将文本编码为可解释类别的层，其中每个维度对应于特定类别的相关性。我们的实证评估表明，使用SenteCon对语言进行编码可以在对下游任务的预测性能几乎没有成本的情况下提供高级可解释性。此外，我们发现，在下游性能和协议方面，SenteCon优于现有的可解释语言表示。

    Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model's decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language must be featurized in a way that is interpretable while still characterizing the original text well. We present SenteCon, a method for introducing human interpretability in deep language representations. Given a passage of text, SenteCon encodes the text as a layer of interpretable categories in which each dimension corresponds to the relevance of a specific category. Our empirical evaluations indicate that encoding language with SenteCon provides high-level interpretability at little to no cost to predictive performance on downstream tasks. Moreover, we find that SenteCon outperforms existing interpretable language representations with respect to both its downstream performance and its agr
    
[^27]: 基于交叉熵差异的上下文演示选择方法

    In-Context Demonstration Selection with Cross Entropy Difference. (arXiv:2305.14726v1 [cs.CL])

    [http://arxiv.org/abs/2305.14726](http://arxiv.org/abs/2305.14726)

    本论文提出了一种基于交叉熵差异的方法，用于在大型语言模型中选择最佳的上下文演示来提高零-shot任务性能。

    

    大型语言模型（LLMs）可以利用上下文演示来提高零-shot任务的性能。然而，选择最佳的上下文示例很具挑战性，因为模型性能可能因所选示例而异。我们提出了一种基于交叉熵差异（CED）的方法，用于选择上下文演示。我们的方法基于这样一个观察结果：在有限调整了这些演示的语言模型上，上下文演示的有效性与测试示例的困惑度呈负相关。我们利用参数有效的微调在训练数据上训练小型模型，以计算测试示例和每个候选上下文演示之间的交叉熵差异。该指标用于为每个测试输入独立地排名和选择上下文演示。我们在一个混合域数据集上评估了我们的方法，该数据集结合了8个基准测试，代表4个文本生成任务，结果表明CED在上下文演示选择方面提高了大型语言模型的零-shot性能。

    Large language models (LLMs) can use in-context demonstrations to improve performance on zero-shot tasks. However, selecting the best in-context examples is challenging because model performance can vary widely depending on the selected examples. We present a cross-entropy difference (CED) method for selecting in-context demonstrations. Our method is based on the observation that the effectiveness of in-context demonstrations negatively correlates with the perplexity of the test example by a language model that was finetuned on that demonstration. We utilize parameter efficient finetuning to train small models on training data that are used for computing the cross-entropy difference between a test example and every candidate in-context demonstration. This metric is used to rank and select in-context demonstrations independently for each test input. We evaluate our method on a mix-domain dataset that combines 8 benchmarks, representing 4 text generation tasks, showing that CED for in-co
    
[^28]: AMELI:细粒度属性增强多模态实体链接

    AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes. (arXiv:2305.14725v1 [cs.CL])

    [http://arxiv.org/abs/2305.14725](http://arxiv.org/abs/2305.14725)

    提出了一种属性感知的多模态实体链接方法，并构建了一个大型数据集AMELI，实验证明了将属性信息纳入实体链接过程的重要性。

    

    我们提出了一种属性感知的多模态实体链接方法，其中输入是一个由文本和图像描述的提及，目标是从一个多模态知识库中预测相应的目标实体，其中每个实体都是用文本描述、视觉图像和一组属性值描述的。为了支持这项研究，我们构建了一个大型数据集AMELI，其中包含18,472个评论和35,598个产品。我们在AMELI上进行了实验，使用当前最先进的多模态实体链接方法和我们增强的属性感知模型来建立基准性能，并展示了将属性信息纳入实体链接过程中的重要性。据我们所知，我们是第一个为属性感知多模态实体链接任务建立基准数据集和解决方案的团队。数据集和代码将公开提供。

    We propose attribute-aware multimodal entity linking, where the input is a mention described with a text and image, and the goal is to predict the corresponding target entity from a multimodal knowledge base (KB) where each entity is also described with a text description, a visual image and a set of attributes and values. To support this research, we construct AMELI, a large-scale dataset consisting of 18,472 reviews and 35,598 products. To establish baseline performance on AMELI, we experiment with the current state-of-the-art multimodal entity linking approaches and our enhanced attribute-aware model and demonstrate the importance of incorporating the attribute information into the entity linking process. To be best of our knowledge, we are the first to build benchmark dataset and solutions for the attribute-aware multimodal entity linking task. Datasets and codes will be made publicly available.
    
[^29]: 我寻觅一个隐喻：大语言模型和扩散模型共创视觉隐喻

    I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. (arXiv:2305.14724v1 [cs.CL])

    [http://arxiv.org/abs/2305.14724](http://arxiv.org/abs/2305.14724)

    本论文提出一个新的任务——从语言隐喻生成视觉隐喻，并且基于大语言模型和扩散模型之间的协作，成功地实现了共创出具有视觉冲击力和语义含义的隐喻。

    

    视觉隐喻是通过图像来说服或传达创意想法的强大修辞手法。与语言隐喻类似，它们通过符号主义和符号的并置隐含地传达含义。我们提出了一个从语言隐喻生成视觉隐喻的新任务。这对于基于扩散的文本到图像模型（如DALL $\cdot$ E 2）来说是一项具有挑战性的任务，因为它需要模拟隐含含义和组合性。我们提出了通过大型语言模型（LLMs）和扩散模型之间的协作来解决这个问题：采用以“串联思维”为提示的Instruct GPT-3（davinci-002）生成代表语言隐喻的视觉阐述的文本，其中包含隐含含义和相关对象，然后将其用作扩散的文本到图像模型的输入。通过人机协作框架，人们与LLM和表现最佳的扩散模型进行交互，创建一个高质量的隐喻和它们的视觉对应的数据集。我们的实验表明，LLMs和扩散模型之间的协作可以共同创造出具有视觉冲击力和语义含义的隐喻。

    Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic metaphors. This is a challenging task for diffusion-based text-to-image models, such as DALL$\cdot$E 2, since it requires the ability to model implicit meaning and compositionality. We propose to solve the task through the collaboration between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3 (davinci-002) with Chain-of-Thought prompting generates text that represents a visual elaboration of the linguistic metaphor containing the implicit meaning and relevant objects, which is then used as input to the diffusion-based text-to-image models.Using a human-AI collaboration framework, where humans interact both with the LLM and the top-performing diffusion model, we create a high-qu
    
[^30]: CuRIAM: 美国最高法院观点中的语料库再解释和元语言

    CuRIAM: Corpus re Interpretation and Metalanguage in U.S. Supreme Court Opinions. (arXiv:2305.14719v1 [cs.CL])

    [http://arxiv.org/abs/2305.14719](http://arxiv.org/abs/2305.14719)

    本研究探讨了美国最高法院观点中的语料库再解释和元语言，并发现了一些法官使用的元语言类型的模式。

    

    大多数司法决定涉及对法律文件的解释；因此，司法观点要使用语言作为一种介质来评论或引起对其他语言的关注。这样使用的语言称为元语言。我们开发了一个注释模式来分类法律元语言的类型，并将我们的模式应用于一组美国最高法院的观点，得到了一个总共59k令牌的语料库。我们对法官使用的元语言类型观察到了几种模式。

    Most judicial decisions involve the interpretation of legal texts; as such, judicial opinion requires the use of language as a medium to comment on or draw attention to other language. Language used this way is called metalanguage. We develop an annotation schema for categorizing types of legal metalanguage and apply our schema to a set of U.S. Supreme Court opinions, yielding a corpus totaling 59k tokens. We remark on several patterns observed in the kinds of metalanguage used by the justices.
    
[^31]: 利用基于优势的离线策略梯度改进语言模型

    Improving Language Models with Advantage-based Offline Policy Gradients. (arXiv:2305.14718v1 [cs.CL])

    [http://arxiv.org/abs/2305.14718](http://arxiv.org/abs/2305.14718)

    本文介绍了一种简单的训练算法Left-over Lunch RL （LoL-RL），使用离线策略梯度学习任何序列到序列数据，从而实现优化LM效用的方法。

    

    根据用户定义的质量或风格限制提高语言模型生成是具有挑战性的。典型的方法包括学习额外的人工编写数据，使用启发式方法过滤“低质量”数据和/或使用强化学习与人体反馈（RLHF）。然而，过滤会删除有价值的训练信号，而数据收集和RLHF不断需要额外的人工编写或LM探索数据，这可能成本高。一个自然的问题是“我们可以利用RL来优化现有的众包和互联网数据上的LM效用吗？”为此，我们提出了剩余午餐强化学习（LoL-RL），这是一种简单的训练算法，使用离线策略梯度来学习语言生成任务作为1步RL游戏。 LoL-RL可以微调LM，以优化任意基于分类器或人定义的效用函数的任何序列到序列数据。使用不同大小模型的五个不同语言生成任务的实验

    Improving language model generations according to some user-defined quality or style constraints is challenging. Typical approaches include learning on additional human-written data, filtering ``low-quality'' data using heuristics and/or using reinforcement learning with human feedback (RLHF). However, filtering can remove valuable training signals, whereas data collection and RLHF constantly require additional human-written or LM exploration data which can be costly to obtain. A natural question to ask is ``Can we leverage RL to optimize LM utility on existing crowd-sourced and internet data?''  To this end, we present Left-over Lunch RL (LoL-RL), a simple training algorithm that uses offline policy gradients for learning language generation tasks as a 1-step RL game. LoL-RL can finetune LMs to optimize arbitrary classifier-based or human-defined utility functions on any sequence-to-sequence data. Experiments with five different language generation tasks using models of varying sizes 
    
[^32]: 利用多重定义建模来挖掘上下文和定义之间的关联

    Exploiting Correlations Between Contexts and Definitions with Multiple Definition Modeling. (arXiv:2305.14717v1 [cs.CL])

    [http://arxiv.org/abs/2305.14717](http://arxiv.org/abs/2305.14717)

    本文提出了一种新的多重定义建模（MDM）任务，将目标词的所有上下文和定义汇集在一起，相比单个定义建模（SDM）有更好的表现，在预训练任务中可以提高SDM的性能，在零-shot情况下有可比性。

    

    定义建模是高级自然语言应用程序（例如理解和对话）中的重要任务。目前，这种模型侧重于在给定上下文中为目标词或短语生成一个定义，我们将其称为单个定义建模（SDM）。然而，这种方法不能充分地模拟单词的不同上下文和定义之间的相关性和模式。此外，为SDM创建训练数据集需要大量的人力专业知识和努力。在本文中，我们精心设计了一个称为多重定义建模（MDM）的新任务，将目标词的所有上下文和定义汇集在一起。我们演示了创建模型以及多个训练集的自动化过程的便利性。在实验中，我们证明并分析了MDM的好处，包括使用MDM作为预训练任务以提高SDM的性能及其在零-shot情况下的性能可比性。

    Definition modeling is an important task in advanced natural language applications such as understanding and conversation. Since its introduction, it focus on generating one definition for a target word or phrase in a given context, which we refer to as Single Definition Modeling (SDM). However, this approach does not adequately model the correlations and patterns among different contexts and definitions of words. In addition, the creation of a training dataset for SDM requires significant human expertise and effort. In this paper, we carefully design a new task called Multiple Definition Modeling (MDM) that pool together all contexts and definition of target words. We demonstrate the ease of creating a model as well as multiple training sets automatically. % In the experiments, we demonstrate and analyze the benefits of MDM, including improving SDM's performance by using MDM as the pretraining task and its comparable performance in the zero-shot setting.
    
[^33]: GlobalBench：自然语言处理全球进展的基准

    GlobalBench: A Benchmark for Global Progress in Natural Language Processing. (arXiv:2305.14716v1 [cs.CL])

    [http://arxiv.org/abs/2305.14716](http://arxiv.org/abs/2305.14716)

    GlobalBench是一个动态追踪所有语言NLP数据集进展的基准，旨在提高公平语言技术的全球发展，并确定最需要服务的语言，目前这些语言主要是非洲和美洲原住民语言。

    

    尽管NLP有了重大进展，但在不同语言的NLP系统表现上仍存在显着差异。这些差异可能是由于资源分配不均和对不足资源语言的工作激励不够优化所导致的。为了跟踪并进一步激励公平语言技术的全球发展，我们介绍GlobalBench。先前的多语言基准是静态的，并且关注的是少数任务和少数语言。相比之下，GlobalBench是一个不断扩大的集合，旨在动态跟踪所有语言的所有NLP数据集的进展。除了仅仅测量准确度外，GlobalBench还跟踪了所有语言的预计每人效用和技术公平性，提供了一个多方面的视角，了解语言技术如何为世界各地的人服务。此外，GlobalBench旨在识别最需要服务的语言，并奖励针对这些语言的研究工作。目前，GlobalBench中最需要服务的语言主要是非洲和美洲原住民语言。

    Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served l
    
[^34]: 自动评估度量中的性别偏见：基于图像字幕的案例研究

    Gender Biases in Automatic Evaluation Metrics: A Case Study on Image Captioning. (arXiv:2305.14711v1 [cs.CL])

    [http://arxiv.org/abs/2305.14711](http://arxiv.org/abs/2305.14711)

    本文研究了模型评估度量中的性别偏见对图像字幕任务的影响，并提出了替代方案以解决这一问题。

    

    预训练模型评估度量在图像字幕等各种自然语言生成任务中已经证明具有强大的性能，并与人类判断高度相关。然而，它们对公平性的影响尚未被深入探讨——人们普遍认为预训练模型可能会编码社会偏见，并将其用于评估目的可能会无意中表现并潜在地放大偏见。本文针对图像字幕任务，对模型评估度量中的性别偏见进行了系统研究。具体而言，我们首先确定和量化了不同评估度量中关于职业、活动和物体概念的性别偏见。然后，我们展示了使用这些有偏见的度量带来的负面后果，比如在部署过程中偏向有偏见的生成模型，并通过强化学习向生成模型传播偏见。我们还提出了一个简单但有效的替代方案。

    Pretrained model-based evaluation metrics have demonstrated strong performance with high correlations with human judgments in various natural language generation tasks such as image captioning. Despite the impressive results, their impact on fairness is under-explored -- it is widely acknowledged that pretrained models can encode societal biases, and utilizing them for evaluation purposes may inadvertently manifest and potentially amplify biases. In this paper, we conduct a systematic study in gender biases of model-based evaluation metrics with a focus on image captioning tasks. Specifically, we first identify and quantify gender biases in different evaluation metrics regarding profession, activity, and object concepts. Then, we demonstrate the negative consequences of using these biased metrics, such as favoring biased generation models in deployment and propagating the biases to generation models through reinforcement learning. We also present a simple but effective alternative to r
    
[^35]: 训练指令作为后门: 大规模语言模型指令调整的后门漏洞

    Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models. (arXiv:2305.14710v1 [cs.CL])

    [http://arxiv.org/abs/2305.14710](http://arxiv.org/abs/2305.14710)

    使用指令调整方法在众包数据集上训练的大规模语言模型，存在后门漏洞，攻击者只需注入极少量的恶意指令便可永久控制模型行为，且难以被修复，需要更加健全的防御机制。

    

    本文研究使用指令调整方法在众包数据集上训练的模型，其目的是达到更好的性能表现。然而，我们提出了一个与该培训范例相关的安全问题。研究表明，攻击者只需在成千上万的数据中注入极少量的恶意指令，便可以通过数据毒化来控制模型行为，甚至无需修改数据实例或标签本身。通过这种指令攻击，攻击者可以在四个常用的 NLP 数据集上实现超过90% 的攻击成功率，并引起易于转移到 15 种不同数据集的持久后门。这种攻击还可以直接应用于多个数据集的有毒指令。最后，该攻击显示出对现有推理时防御的抵抗力。这些发现凸显了在语言模型训练中需要更为健全的防御机制。

    Instruction-tuned models are trained on crowdsourcing datasets with task instructions to achieve superior performance. However, in this work we raise security concerns about this training paradigm. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions among thousands of gathered data and control model behavior through data poisoning, without even the need of modifying data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets, and cause persistent backdoors that are easily transferred to 15 diverse datasets zero-shot. In this way, the attacker can directly apply poisoned instructions designed for one dataset on many other datasets. Moreover, the poisoned model cannot be cured by continual learning. Lastly, instruction attacks show resistance to existing inference-time defense. These findings highlight the need for more robust defens
    
[^36]: 学生超越了大师：基于GPT-3的科学事实错误校正方法的匹配

    The student becomes the master: Matching GPT3 on Scientific Factual Error Correction. (arXiv:2305.14707v1 [cs.CL])

    [http://arxiv.org/abs/2305.14707](http://arxiv.org/abs/2305.14707)

    本文提出了一种不需要验证者且不做领域假设的主张校正系统，能够显著提高科学事实错误校正任务的性能，并通过使用LLM的提示方法和主张感知的解码过程来提高校正质量。

    

    由于创建错误校正数据集的成本极高，大多数事实主张校正方法依赖于强大的验证模型来指导校正过程。这导致在科学事实校正等领域性能显著下降，因为好的验证模型并不总是存在。在本研究中，我们介绍了一种不做领域假设且不需要验证者的主张校正系统，但能够比现有方法提高一个数量级的性能 - 在SciFact数据集上实现94％的修正准确性，在SciFact-Open数据集上实现62.5％的修正准确性，分别比下一个最好的方法高出0.5％和1.50％。我们的方法利用LLMs中的提示功能，在训练期间创建一个丰富注释的数据集，可用于完全监督的训练和正则化。我们还使用主张感知的解码过程来提高纠正主张的质量。我们的方法与用于创建数据集的LLM相竞争，证明了利用基于LLM的训练提高科学主张校正任务性能的可能性。

    Due to the prohibitively high cost of creating error correction datasets, most Factual Claim Correction methods rely on a powerful verification model to guide the correction process. This leads to a significant drop in performance in domains like Scientific Claim Correction, where good verification models do not always exist. In this work, we introduce a claim correction system that makes no domain assumptions and does not require a verifier but is able to outperform existing methods by an order of magnitude -- achieving 94% correction accuracy on the SciFact dataset, and 62.5% on the SciFact-Open dataset, compared to the next best methods 0.5% and 1.50% respectively. Our method leverages the power of prompting with LLMs during training to create a richly annotated dataset that can be used for fully supervised training and regularization. We additionally use a claim-aware decoding procedure to improve the quality of corrected claims. Our method is competitive with the very LLM that was
    
[^37]: Flan-MoE: 通过稀疏Mixture of Experts扩展指令调优的语言模型

    Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts. (arXiv:2305.14705v1 [cs.CL])

    [http://arxiv.org/abs/2305.14705](http://arxiv.org/abs/2305.14705)

    Flan-MoE是一种指令调优的稀疏Mixture of Experts（MoE）语言模型，相对于密集模型，在指令微调和任务特定微调后均表现更好。最大模型Flan-MoE-32B的性能在四个基准测试中超越Flan-PaLM-62B，同时只利用了1/3的FLOPs。

    

    语言模型的爆炸性增长和应用需求导致有效和可扩展方法的需求增加。本文介绍了一套Instruction-Finetuned Sparse Mixture-of-Expert (MoE)语言模型，即Flan-MoE。我们发现，仅针对任务特定数据集进行MoE模型的微调会导致性能不如相同计算复杂度的密集模型。然而，我们的Flan-MoE在多个实验设置下都优于密集模型：仅指令微调和指令微调后进行任务特定微调。这表明，指令微调是MoE模型的必要阶段。具体来说，我们的最大模型Flan-MoE-32B在四个基准测试中超越了Flan-PaLM-62B的性能，同时只利用了1/3的FLOPs。Flan-MoE的成功鼓舞我们重新思考大规模、高性能语言模型的设计，尤其是在任务无关学习的情况下。

    The explosive growth of language models and their applications have led to an increased demand for efficient and scalable methods. In this paper, we introduce Flan-MoE, a set of Instruction-Finetuned Sparse Mixture-of-Expert (MoE) models. We show that naively finetuning MoE models on a task-specific dataset (in other words, no instruction-finetuning) often yield worse performance compared to dense models of the same computational complexity. However, our Flan-MoE outperforms dense models under multiple experiment settings: instruction-finetuning only and instruction-finetuning followed by task-specific finetuning. This shows that instruction-finetuning is an essential stage for MoE models. Specifically, our largest model, Flan-MoE-32B, surpasses the performance of Flan-PaLM-62B on four benchmarks, while utilizing only one-third of the FLOPs. The success of Flan-MoE encourages rethinking the design of large-scale, high-performance language models, under the setting of task-agnostic lear
    
[^38]: 通过GPT-4分析影响人类偏好判断的因素

    Analyzing Influential Factors in Human Preference Judgments via GPT-4. (arXiv:2305.14702v1 [cs.CL])

    [http://arxiv.org/abs/2305.14702](http://arxiv.org/abs/2305.14702)

    本文利用Bradley-Terry-Luce模型对OpenAI公布的人类偏好判断数据集进行了深入研究，揭示了人类偏好判断中所蕴含的固有偏好，提出了提高样本效率的策略，并为构建平衡的人类偏好判断数据集提供了洞见。

    

    人类偏好判断在引导大型语言模型生成符合人类偏好的输出和评估自动摘要度量方面具有至关重要的作用。然而，对于这些偏好判断的共同影响和因素的相对重要性等问题，目前的研究仍较为有限。本文利用Bradley-Terry-Luce模型对OpenAI公布的人类偏好判断数据集进行了深入研究，识别了可能影响人类偏好判断的关键因素。研究结果揭示了人类偏好判断中所蕴含的固有偏好，并提出了提高样本效率的策略，最后对于如何构建平衡的人类偏好判断数据集提供了洞见。

    Pairwise human judgments are pivotal in guiding large language models (LLMs) to generate outputs that align with human preferences. They are also often used in summarization evaluation, complementing existing automatic metrics. Despite their significance, however, there has been limited research probing these pairwise human judgments. The collective impact and respective weights of factors such as informativeness, coherence, fluency, and factual consistency remain elusive. The impact of hidden factors on the final judgment is also unclear. In this paper, we conduct an in-depth examination of a dataset of pairwise human judgments released by OpenAI. Utilizing the Bradley-Terry-Luce model, we identify key factors that could potentially influence human judgments. Our research uncovers the inherent preferences embedded in human judgments and suggests strategies to boost sample efficiency. Finally, we provide insights on the construction of balanced datasets for human judgment evaluations, 
    
[^39]: 论文标题：将贝叶斯先验注入神经网络中进行快速语言学习建模

    Modeling rapid language learning by distilling Bayesian priors into artificial neural networks. (arXiv:2305.14701v1 [cs.CL])

    [http://arxiv.org/abs/2305.14701](http://arxiv.org/abs/2305.14701)

    本文介绍了一种将贝叶斯模型的归纳偏见注入神经网络中的方法，可以从有限的自然数据中进行学习，同时具有灵活性以推广到新的示例。

    

    人类可以从极少的经验中学习语言。开发能够解释这种能力的计算模型一直是认知科学的一项重大挑战。贝叶斯模型将指导概括的强烈归纳偏见因素结合起来，成功地解释了人类如何从少数控制环境中的例子中进行归纳，但通常过于严格而无法应用于更自然的数据。相比之下，神经网络具有灵活的表示形式，使它们能够很好地从自然数据中进行学习，但需要比人类接收到的更多的示例。我们展示了一种方法，通过将贝叶斯模型的偏见注入神经网络中，从有限的自然数据中进行学习是可能的。与贝叶斯模型一样，结果系统可以从少量的示例中学习形式语言模式，同时仍保持灵活性，以推广到新的示例。

    Humans can learn languages from remarkably little experience. Developing computational models that explain this ability has been a major challenge in cognitive science. Bayesian models that build in strong inductive biases factors that guide generalization - have been successful at explaining how humans might generalize from few examples in controlled settings but are usually too restrictive to be tractably applied to more naturalistic data. By contrast, neural networks have flexible representations that allow them to learn well from naturalistic data but require many more examples than humans receive. We show that learning from limited naturalistic data is possible with an approach that combines the strong inductive biases of a Bayesian model with the flexible representations of a neural network. This approach works by distilling a Bayesian model's biases into a neural network. Like a Bayesian model, the resulting system can learn formal linguistic patterns from a small number of ex
    
[^40]: SELFOOD: 基于自学习排序的无监督外部样本检测

    SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to Rank. (arXiv:2305.14696v1 [cs.CL])

    [http://arxiv.org/abs/2305.14696](http://arxiv.org/abs/2305.14696)

    提出了一种自监督的外部样本检测方法SELFOOD，通过将外部样本检测视为文档标签内部排序问题，使用比较排序损失对分类器进行训练，实现无监督的外部样本检测。

    

    采用交叉熵损失训练的深度神经分类器常常存在校准不良的问题，需要进行外部样本检测。传统的有监督外部样本检测方法需要昂贵的手动注释内部和外部样本。为了解决注释瓶颈问题，我们介绍了SELFOOD，这是一种仅需要内部样本作为监督的自监督外部样本检测方法。我们将外部样本检测看作一个文档之间标签内部排序问题，并使用比较排序损失（IDIL loss）训练分类器。具体来说，对于内部样本文档及其标签，我们训练分类器将属于该标签的文档的softmax分数排名排在其他标签文档的分数之上。与交叉熵损失不同，当达到期望的置信度排名时，我们的IDIL损失函数将达到零并进行反向传播来降低概率。

    Deep neural classifiers trained with cross-entropy loss (CE loss) often suffer from poor calibration, necessitating the task of out-of-distribution (OOD) detection. Traditional supervised OOD detection methods require expensive manual annotation of in-distribution and OOD samples. To address the annotation bottleneck, we introduce SELFOOD, a self-supervised OOD detection method that requires only in-distribution samples as supervision. We cast OOD detection as an inter-document intra-label (IDIL) ranking problem and train the classifier with our pairwise ranking loss, referred to as IDIL loss. Specifically, given a set of in-distribution documents and their labels, for each label, we train the classifier to rank the softmax scores of documents belonging to that label to be higher than the scores of documents that belong to other labels. Unlike CE loss, our IDIL loss function reaches zero when the desired confidence ranking is achieved and gradients are backpropagated to decrease probab
    
[^41]: 大型语言模型中的实体偏见：一种因果视角

    A Causal View of Entity Bias in (Large) Language Models. (arXiv:2305.14695v1 [cs.CL])

    [http://arxiv.org/abs/2305.14695](http://arxiv.org/abs/2305.14695)

    研究提出了一种结构因果模型（SCM）并提供因果干预技术，以缓解大型语言模型中的实体偏见，从而减少偏见信息，同时保留相似实体的共同预测信息。

    

    实体偏见广泛影响预训练的大型语言模型，导致它们过度依赖（有偏见的）参数化知识来进行不准确的预测。尽管因果相关的方法已经显示出缓解实体偏见的巨大潜力，但在实践中精确估计潜在因果模型的参数仍然很困难，黑盒子的语言模型更无法调整。为了解决这些问题，我们提出了一种特定的结构因果模型（SCM），其参数比较容易估计。在此基础上，我们提出了因果干预技术，以缓解白盒和黑盒设置中的实体偏见。这种因果干预将原始实体与相邻实体一起进行扰动。这种干预减少了与原始实体相关的特定偏向信息，同时仍保留了来自类似实体的足够共同预测信息。

    Entity bias widely affects pretrained (large) language models, causing them to excessively rely on (biased) parametric knowledge to make unfaithful predictions. Although causality-inspired methods have shown great potential to mitigate entity bias, it is hard to precisely estimate the parameters of underlying causal models in practice. The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits. To address these problems, we propose a specific structured causal model (SCM) whose parameters are comparatively easier to estimate. Building upon this SCM, we propose causal intervention techniques to mitigate entity bias for both white-box and black-box settings. The proposed causal intervention perturbs the original entity with neighboring entities. This intervention reduces specific biasing information pertaining to the original entity while still preserving sufficient common predictive information from similar entities. 
    
[^42]: 大型语言模型是否拥有人格?——在测量LLM中的个性时，自我评估测试的适用性研究

    Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs. (arXiv:2305.14693v1 [cs.CL])

    [http://arxiv.org/abs/2305.14693](http://arxiv.org/abs/2305.14693)

    本文探讨了大型语言模型中个性是否存在的问题，并发现目前自我评估测试无法准确测量语言模型中的个性。为了未来的研究，需要专门为LLM设计新的自我评估测试。

    

    大型语言模型（LLM）是否拥有人格？简短的回答是“我们不知道！”。本文表明我们还没有合适的工具能够测量语言模型的人格。个性是影响行为的重要特征。随着LLM在各种任务中模拟人类智能和表现，自然而然地问出的一个问题是，这些模型是否拥有了人格。以前的研究通过自我评估人格测试来评估机器人格，这是一组用于评估人类个性的多项选择题。一个基本的假设是，人类个性测试可以准确地测量机器人格。在本文中，我们研究了五个不同尺寸范围（从1.5B到30B）的LLM中的个性浮现。我们提出了选项顺序对称性原则作为这些自我评估测试可靠性的必要条件。在这个条件下，答题和他们的排序不应影响分数。然而，我们发现现有的自我评估个性测试对LLM来说是不可靠的。我们的分析表明，常用的自我评估测试对人类具有偏见，没有考虑到LLM的独特特征。为了促进LLM个性测量的未来研究，我们强调了开发专门为LLM设计的新的自我评估测试的必要性。

    Have Large Language Models (LLMs) developed a personality? The short answer is a resounding "We Don't Know!". In this paper, we show that we do not yet have the right tools to measure personality in language models. Personality is an important characteristic that influences behavior. As LLMs emulate human-like intelligence and performance in various tasks, a natural question to ask is whether these models have developed a personality. Previous works have evaluated machine personality through self-assessment personality tests, which are a set of multiple-choice questions created to evaluate personality in humans. A fundamental assumption here is that human personality tests can accurately measure personality in machines. In this paper, we investigate the emergence of personality in five LLMs of different sizes ranging from 1.5B to 30B. We propose the Option-Order Symmetry property as a necessary condition for the reliability of these self-assessment tests. Under this condition, the answ
    
[^43]: 专家提示：指导大型语言模型成为杰出的专家

    ExpertPrompting: Instructing Large Language Models to be Distinguished Experts. (arXiv:2305.14688v1 [cs.CL])

    [http://arxiv.org/abs/2305.14688](http://arxiv.org/abs/2305.14688)

    本论文提出了“专家提示”技术，用于训练大型语言模型成为杰出的专家。该方法使用上下文学习自动生成每个指令的详细和定制的专家身份描述，并要求模型根据这些提示提供答案。基于这种技术，本文提出了一个新的开源聊天助手ExpertLLaMA，该助手在评估中表现出高质量的数据和96％的ChatGPT能力。

    

    如果以适当的提示方式进行处理，对齐的大型语言模型（LLM）的回答质量可以大大提高。在本文中，我们提出了专家提示，以引发LLMs作为杰出专家回答的潜力。我们首先利用上下文学习自动生成每个特定指令的详细和定制的专家身份描述，然后要求LLMs根据这种代理人背景提供答案。基于这种增强的提示策略，我们使用GPT-3.5生成了一组新的指令遵循数据，并训练了一个竞争性的开源聊天助手ExpertLLaMA。我们采用基于GPT4的评估显示：1）专家数据的质量显著高于普通答案，2）ExpertLLaMA胜过现有的开源对手，实现了ChatGPT能力的96％。所有数据和ExpertLLaMA模型将在\url{https://github.com/OFA-Sys/Exp}上公开。

    The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts. In this paper, we propose ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts. We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background. Based on this augmented prompting strategy, we produce a new set of instruction-following data using GPT-3.5, and train a competitive open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves 96\% of the original ChatGPT's capability. All data and the ExpertLLaMA model will be made publicly available at \url{https://github.com/OFA-Sys/Exp
    
[^44]: TACR：基于表格对齐的混合问答中的单元选择和推理模型

    TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering. (arXiv:2305.14682v1 [cs.CL])

    [http://arxiv.org/abs/2305.14682](http://arxiv.org/abs/2305.14682)

    TACR是一个用于混合文本和表格QA场景的新模型，通过对表格-问题间的对齐增强单元选择来检索细粒度证据，并将包含选定单元的行作为上下文进行回答推理，在HybridQA和WikiTableQuestions（WTQ）数据集上取得了最先进的结果。

    

    近年来，针对表格和与表格单元链接的段落进行推理的混合问答（HQA）已经得到了显着的研究。在HQA和其他段落-表格QA数据集中，一个常见的挑战是通常不可能遍历所有表格行、列和链接的段落来检索证据。这使得以前的研究很难展示其在检索答案时的推理能力。为了弥合这一差距，我们提出了一种新的基于表格对齐的单元选择和推理模型（TACR）用于混合文本和表格QA，并在HybridQA和WikiTableQuestions数据集上进行了评估。在证据检索中，我们设计了一种表格-问题对齐增强的单元选择方法来检索细粒度证据。在回答推理中，我们将包含选定单元的行作为上下文，加入一个QA模块。在HybridQA和WikiTableQuestions（WTQ）数据集上的实验结果表明，TACR取得了最先进的结果。

    Hybrid Question-Answering (HQA), which targets reasoning over tables and passages linked from table cells, has witnessed significant research in recent years. A common challenge in HQA and other passage-table QA datasets is that it is generally unrealistic to iterate over all table rows, columns, and linked passages to retrieve evidence. Such a challenge made it difficult for previous studies to show their reasoning ability in retrieving answers. To bridge this gap, we propose a novel Table-alignment-based Cell-selection and Reasoning model (TACR) for hybrid text and table QA, evaluated on the HybridQA and WikiTableQuestions datasets. In evidence retrieval, we design a table-question-alignment enhanced cell-selection method to retrieve fine-grained evidence. In answer reasoning, we incorporate a QA module that treats the row containing selected cells as context. Experimental results over the HybridQA and WikiTableQuestions (WTQ) datasets show that TACR achieves state-of-the-art results
    
[^45]: 训练过程中的逆比例缩放现象：累赘还是贡献？

    Emergent inabilities? Inverse scaling over the course of pretraining. (arXiv:2305.14681v1 [cs.CL])

    [http://arxiv.org/abs/2305.14681](http://arxiv.org/abs/2305.14681)

    该研究探讨了模型在训练中针对特定任务的表现是否会下降，发现Pythia模型在更多的参数下，尽管整体表现良好，但在引述重复和重新定义数学两个任务的训练中性能下降。需要在任何时候测试模型在相应基准上的表现。

    

    模型参数大小是否会导致逆比例缩放现象呢？这个研究探讨了语言模型在训练中针对特定任务的表现是否会下降，尽管整体表现仍然不错。研究发现在逆比例缩放挑战中的两个任务 - 引述重复和重新定义数学，Pythia模型运用更多的参数时，这两个任务在训练过程中的性能确实会下降，尽管这些模型在整体上表现良好。这突显了在任何时候如果模型被训练了额外的数据，那么需要测试模型在相应基准上的表现，即使它们的整体表现有所提高。

    Does inverse scaling only occur as a function of model parameter size, or can it also occur over the course of training? We carry out an exploratory study investigating whether, over the course of training on the language modeling task, the performance of language models at specific tasks can decrease while general performance remains high. We find that for two tasks from the Inverse Scaling Challenge - quote-repetition and redefine-math - this is indeed the case. Specifically, we find that for Pythia (Biderman et al., 2023) models with a higher number of parameters, performance decreases over the course of training at these two tasks, despite these models showing standard (positive) scaling overall. This highlights the importance of testing model performance at all relevant benchmarks any time they are trained on additional data, even if their overall performance improves.
    
[^46]: GRILL：基于文本和图像区域对齐的基于场景的视觉语言预训练模型

    GRILL: Grounded Vision-language Pre-training via Aligning Text and Image Regions. (arXiv:2305.14676v1 [cs.CL])

    [http://arxiv.org/abs/2305.14676](http://arxiv.org/abs/2305.14676)

    GRILL是一种基于文本和图像区域对齐的基于场景的视觉语言预训练模型，它可以在训练样本很少或没有样本的情况下用于各种视觉语言任务，并且在各种零/少样本VL任务上表现更好。

    

    少样本学习器的泛化能力是实现更好的零/少样本性能的重要能力，但这种泛化适用于包括基础和生产任务在内的视觉语言任务一直未得到充分探索。本文介绍了GRILL，一种新型视觉语言模型，可以推广到包括视觉问答、标题和基础任务在内的各种任务，其训练样本很少或没有样本。具体而言，GRILL通过利用对象-文本对齐来学习对象的定位，这使其能够以零/少样本的方式转移到基础任务。我们在各种零/少样本VL任务上评估了我们的模型，并展示了它始终优于现有的少样本方法。

    Generalization to unseen tasks is an important ability for few-shot learners to achieve better zero-/few-shot performance on diverse tasks. However, such generalization to vision-language tasks including grounding and generation tasks has been under-explored; existing few-shot VL models struggle to handle tasks that involve object grounding and multiple images such as visual commonsense reasoning or NLVR2. In this paper, we introduce GRILL, GRounded vIsion Language aLigning, a novel VL model that can be generalized to diverse tasks including visual question answering, captioning, and grounding tasks with no or very few training instances. Specifically, GRILL learns object grounding and localization by exploiting object-text alignments, which enables it to transfer to grounding tasks in a zero-/few-shot fashion. We evaluate our model on various zero-/few-shot VL tasks and show that it consistently surpasses the state-of-the-art few-shot methods.
    
[^47]: 基于Vision Transformers的字符相似度量化

    Quantifying Character Similarity with Vision Transformers. (arXiv:2305.14672v1 [cs.CL])

    [http://arxiv.org/abs/2305.14672](http://arxiv.org/abs/2305.14672)

    本研究使用ViT对OCR文档进行字符相似性度量，提供了一种可扩展的计算字符替换成本的方法，避免了使用常用字符串替换方法导致的数据偏差。

    

    记录链接是定量社会科学的基石，因为分析通常需要链接来自多个嘈杂的来源的数据。现成的字符串匹配方法被广泛使用，因为它们易于实现和扩展。然而，并非所有字符替换的概率都相等，在某些情况下，有一些常用的手工制作的列表，用来表示哪些字符串替换更有可能，从而提高字符串匹配的准确性。但是，对于许多情况，这样的清单并不存在，从而使得具有链接数据集的研究向少数高资源的背景倾斜，这些背景并不代表人类社会的多样性。本研究开发了一种可扩展的方法，通过使用经过增强的数字字体进行大规模自监督训练的Vision Transformers（ViT），来测量OCR文档的字符替换成本。对于每一种使用CJK脚本编写的语言，我们对相同字符的不同增强进行对比学习度量空间，从而量化字符的相似性。

    Record linkage is a bedrock of quantitative social science, as analyses often require linking data from multiple, noisy sources. Off-the-shelf string matching methods are widely used, as they are straightforward and cheap to implement and scale. Not all character substitutions are equally probable, and for some settings there are widely used handcrafted lists denoting which string substitutions are more likely, that improve the accuracy of string matching. However, such lists do not exist for many settings, skewing research with linked datasets towards a few high-resource contexts that are not representative of the diversity of human societies. This study develops an extensible way to measure character substitution costs for OCR'ed documents, by employing large-scale self-supervised training of vision transformers (ViT) with augmented digital fonts. For each language written with the CJK script, we contrastively learn a metric space where different augmentations of the same character a
    
[^48]: 自然语言处理中的扩散模型综述

    Diffusion Models in NLP: A Survey. (arXiv:2305.14671v1 [cs.CL])

    [http://arxiv.org/abs/2305.14671](http://arxiv.org/abs/2305.14671)

    本文全面综述了扩散模型在NLP中的各种应用，比较了其与自回归模型的优劣，指出扩散模型在并行生成、文本插值、词级别控制等方面具有明显优势。

    

    本综述论文全面回顾了扩散模型在自然语言处理(NLP)中的应用。扩散模型是一类数学模型，旨在捕捉信息或信号在网络或流形上的扩散。在NLP中，扩散模型已被用于各种应用，如自然语言生成、情感分析、主题建模和机器翻译。本文讨论了NLP中使用的不同扩散模型的公式、优劣点和应用。同时，我们还对扩散模型与其他生成模型进行了全面比较，特别是自回归(AR)模型，还研究了如何将Transformer与扩散模型结合以实现多样化的架构。相比AR模型，扩散模型在并行生成、文本插值、词级别控制(如句法结构)等方面具有明显的优势。

    This survey paper provides a comprehensive review of the use of diffusion models in natural language processing (NLP). Diffusion models are a class of mathematical models that aim to capture the diffusion of information or signals across a network or manifold. In NLP, diffusion models have been used in a variety of applications, such as natural language generation, sentiment analysis, topic modeling, and machine translation. This paper discusses the different formulations of diffusion models used in NLP, their strengths and limitations, and their applications. We also perform a thorough comparison between diffusion models and alternative generative models, specifically highlighting the autoregressive (AR) models, while also examining how diverse architectures incorporate the Transformer in conjunction with diffusion models. Compared to AR models, diffusion models have significant advantages for parallel generation, text interpolation, token-level controls such as syntactic structures a
    
[^49]: 你就是你的注释：通过注释者表示实现更好的模型

    You Are What You Annotate: Towards Better Models through Annotator Representations. (arXiv:2305.14663v1 [cs.CL])

    [http://arxiv.org/abs/2305.14663](http://arxiv.org/abs/2305.14663)

    提出了一种方法，通过创建注释者和注释的矩阵表示以捕捉其特点，并在建模过程中使用它们来显著提高自然语言处理模型的性能，同时帮助民主化人工智能。

    

    在自然语言处理（NLP）任务中，注释者的不一致是普遍存在的。存在多种原因导致注释者的不同意见，包括任务的主观性、困难案例、不明确的指南等等。我们的方法不是仅仅汇总注释的标签来获得数据注释，而是提议要明确考虑注释者特征，并在建模过程中利用它们。我们利用可学习的矩阵分别为注释者（注释者嵌入）和其注释（注释嵌入）创建表示。我们的方法通过使用少于1％的模型参数，在各种NLP基准测试中显著提高了模型性能。通过捕捉单个注释者的独特倾向和主观性，我们的嵌入有助于民主化人工智能，并确保人工智能模型包含多元化的观点。

    Annotator disagreement is ubiquitous in natural language processing (NLP) tasks. There are multiple reasons for such disagreements, including the subjectivity of the task, difficult cases, unclear guidelines, and so on. Rather than simply aggregating labels to obtain data annotations, we instead propose to explicitly account for the annotator idiosyncrasies and leverage them in the modeling process. We create representations for the annotators (annotator embeddings) and their annotations (annotation embeddings) with learnable matrices associated with each. Our approach significantly improves model performance on various NLP benchmarks by adding fewer than 1% model parameters. By capturing the unique tendencies and subjectivity of individual annotators, our embeddings help democratize AI and ensure that AI models are inclusive of diverse viewpoints.
    
[^50]: 复杂数学符号定义结构:用于定义提取中协调解决的数据集与模型

    Complex Mathematical Symbol Definition Structures: A Dataset and Model for Coordination Resolution in Definition Extraction. (arXiv:2305.14660v1 [cs.CL])

    [http://arxiv.org/abs/2305.14660](http://arxiv.org/abs/2305.14660)

    该论文介绍了SymDef，一个英文数据集，用于协调解决定义提取中的复杂数学符号及其相应定义之间的链接问题。并且引入了一种新的方法，通过遮盖数学符号，为每个符号创建一个副本，并使用槽填充来预测其相应的定义跨度。

    

    数学符号定义提取对于提高学术阅读接口和学术信息提取非常重要。 然而，该任务面临几个挑战：数学符号难以处理，因为它们不由自然语言形态素组成; 学术论文经常包含需要解决复杂坐标结构的句子。我们介绍了SymDef，这是一个包含5927个来自全文科学论文的英文句子的数据集，每个句子都用其相应的定义链接了所有数学符号进行了注释。该数据集专注于复杂的协调结构，如“分别”结构，这些结构经常包含重叠的定义跨度。我们还引入了一种新的定义提取方法，该方法遮盖数学符号，为每个符号创建一个副本，指定一个目标符号，并使用槽填充预测其相应的定义跨度。我们的实验表明：

    Mathematical symbol definition extraction is important for improving scholarly reading interfaces and scholarly information extraction (IE). However, the task poses several challenges: math symbols are difficult to process as they are not composed of natural language morphemes; and scholarly papers often contain sentences that require resolving complex coordinate structures. We present SymDef, an English language dataset of 5,927 sentences from full-text scientific papers where each sentence is annotated with all mathematical symbols linked with their corresponding definitions. This dataset focuses specifically on complex coordination structures such as "respectively" constructions, which often contain overlapping definition spans. We also introduce a new definition extraction method that masks mathematical symbols, creates a copy of each sentence for each symbol, specifies a target symbol, and predicts its corresponding definition spans using slot filling. Our experiments show that ou
    
[^51]: InteractiveIE：评估人工智能与人类协作的强度，提高信息提取的性能

    InteractiveIE: Towards Assessing the Strength of Human-AI Collaboration in Improving the Performance of Information Extraction. (arXiv:2305.14659v1 [cs.CL])

    [http://arxiv.org/abs/2305.14659](http://arxiv.org/abs/2305.14659)

    本文提出InteractiveIE方法，使用自动问答生成法来引出信息提取中的模板插槽，通过微小量代理人类监督，提高在生物医学和法律文档等领域中信息提取任务性能。

    

    学习基于模板的文档信息提取是一项关键且困难的任务。先前的基于模板的信息提取方法假定具有领域模板的先验知识；然而，实际的信息提取没有预定义的模式，并且是一个即兴创造的现象。为了在实际情况下快速引导模板，我们需要从文档中在零或最小监督的情况下引出模板插槽。由于问答的目的与信息提取的目标交汇，我们使用自动问答生成从文档中引出模板插槽，并研究微小量的代理人类监督（称为InteractiveIE）如何进一步提高性能。在获取训练数据昂贵的生物医学和法律文档上的大量实验揭示了使用InteractiveIE相对于仅使用人工智能的基准测试的鼓舞人心的性能提升趋势。

    Learning template based information extraction from documents is a crucial yet difficult task. Prior template-based IE approaches assume foreknowledge of the domain templates; however, real-world IE do not have pre-defined schemas and it is a figure-out-as you go phenomena. To quickly bootstrap templates in a real-world setting, we need to induce template slots from documents with zero or minimal supervision. Since the purpose of question answering intersect with the goal of information extraction, we use automatic question generation to induce template slots from the documents and investigate how a tiny amount of a proxy human-supervision on-the-fly (termed as InteractiveIE) can further boost the performance. Extensive experiments on biomedical and legal documents, where obtaining training data is expensive, reveal encouraging trends of performance improvement using InteractiveIE over AI-only baseline.
    
[^52]: 无法评估的生成响应质量的评估: Evaluate What You Can't Evaluate

    Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality. (arXiv:2305.14658v1 [cs.CL])

    [http://arxiv.org/abs/2305.14658](http://arxiv.org/abs/2305.14658)

    本文重点研究使用大型语言模型（LLMs）为基础的无参考评估器的局限性，并构建了两个对抗元评估对话生成数据集，以全面评估这些评估器的可靠性。

    

    大型语言模型（LLMs）如ChatGPT已经展现出惊人的语言理解和生成能力。虽然以LLMs为基础的无参考评估器比传统基于参考文献的评估器显示出更好的人类语义对齐度，但是在使用以LLMs为基础的无参考评估器时仍然存在很多挑战。无参考评估器更适用于具有不同语义响应的开放式例子。但并不是所有的例子都是开放式的，对于具有唯一正确语义响应的闭合式例子，如果给出与事实和参考的语义不一致的响应，无参考评估器仍然会认为其具有高质量。为了全面评估以LLMs为基础的评估器的可靠性，我们构建了两个对抗元评估对话生成数据集KdConv-ADV和DSTC7-ADV基于KdConv和DSTC7-AVSD。与以前的元评估基准相比，KdConv-ADV和DSTC7-ADV更具挑战性。

    LLMs (large language models) such as ChatGPT have shown remarkable language understanding and generation capabilities. Although reference-free evaluators based on LLMs show better human alignment than traditional reference-based evaluators, there are many challenges in using reference-free evaluators based on LLMs. Reference-free evaluators are more suitable for open-ended examples with different semantics responses. But not all examples are open-ended. For closed-ended examples with unique correct semantic response, reference-free evaluators will still consider it high quality when giving a response that is inconsistent with the facts and the semantic of reference. In order to comprehensively evaluate the reliability of evaluators based on LLMs, we construct two adversarial meta-evaluation dialogue generation datasets KdConv-ADV and DSTC7-ADV based on KdConv and DSTC7-AVSD, respectively. Compared to previous meta-evaluation benchmarks, KdConv-ADV and DSTC7-ADV are much more challengin
    
[^53]: 基于最大化互信息的视频多模态融合去噪瓶颈模型

    Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion. (arXiv:2305.14652v1 [cs.CL])

    [http://arxiv.org/abs/2305.14652](http://arxiv.org/abs/2305.14652)

    本文提出了一种基于去噪瓶颈和最大化互信息的视频多模态融合模型（DBF），该模型可以细粒度地过滤掉冗余和噪声信息，同时保留不同模态中的关键信息，并在多语言视频分类任务中表现出显著优越性。

    

    视频多模态融合旨在将视频中的多模态信号（如视觉、音频和文本）整合，以使用多模态内容进行补充预测。然而，与其他图像-文本多模态任务不同，视频具有更长的多模态序列，在视觉和音频模态中存在更多的冗余和噪声。因此，我们提出了一种用于细粒度视频多模态融合的去噪瓶颈融合（DBF）模型。我们一方面采用瓶颈机制，以限制的感受野过滤噪声和冗余信息。另一方面，我们使用最大化互信息模块来调节过滤模块，以保留不同模态中的关键信息。我们的DBF模型在多语言视频分类任务中显著优于当前最先进的基准模型。

    Video multimodal fusion aims to integrate multimodal signals in videos, such as visual, audio and text, to make a complementary prediction with multiple modalities contents. However, unlike other image-text multimodal tasks, video has longer multimodal sequences with more redundancy and noise in both visual and audio modalities. Prior denoising methods like forget gate are coarse in the granularity of noise filtering. They often suppress the redundant and noisy information at the risk of losing critical information. Therefore, we propose a denoising bottleneck fusion (DBF) model for fine-grained video multimodal fusion. On the one hand, we employ a bottleneck mechanism to filter out noise and redundancy with a restrained receptive field. On the other hand, we use a mutual information maximization module to regulate the filter-out module to preserve key information within different modalities. Our DBF model achieves significant improvement over current state-of-the-art baselines on mult
    
[^54]: 从生成模型的角度重新审视实体对齐及超越：一个视角

    Revisit and Outstrip Entity Alignment: A Perspective of Generative Models. (arXiv:2305.14651v1 [cs.CL])

    [http://arxiv.org/abs/2305.14651](http://arxiv.org/abs/2305.14651)

    本文重新从生成模型的角度研究了基于嵌入的实体对齐（EEA）问题，引入基于生成对抗网络的EEA方法及提出的生成的EEA（GEEA）框架，通过互相变分自动编码器（M-VAE）实现实体从一个KG转换到另一个KG，并且从随机噪声向量生成新的实体，具有较好的效果。

    

    最近，基于嵌入的方法在利用多模态知识图谱（KG）嵌入的实体对齐方面取得了巨大成功。在本文中，我们从生成模型的角度研究了基于嵌入的实体对齐（EEA）。我们表明EEA是一个特殊的问题，其主要目标类似于典型生成模型中的目标，基于这个目标，我们从理论上证明了最近发展的基于生成对抗网络（GAN）的EEA方法的有效性。然后，我们揭示了他们不完整的目标限制了实体对齐和实体合成（即生成新实体）的能力。我们通过引入生成的EEA（abbr.，GEEA）框架和提出的互相变分自动编码器（M-VAE）作为生成模型来缓解这个问题。M-VAE可以将一个实体从一个KG转换到另一个KG，并从随机噪声向量生成新实体。我们通过理论分析和实证实验展示了GEEA的优势。

    Recent embedding-based methods have achieved great successes on exploiting entity alignment from knowledge graph (KG) embeddings of multiple modals. In this paper, we study embedding-based entity alignment (EEA) from a perspective of generative models. We show that EEA is a special problem where the main objective is analogous to that in a typical generative model, based on which we theoretically prove the effectiveness of the recently developed generative adversarial network (GAN)-based EEA methods. We then reveal that their incomplete objective limits the capacity on both entity alignment and entity synthesis (i.e., generating new entities). We mitigate this problem by introducing a generative EEA (abbr., GEEA) framework with the proposed mutual variational autoencoder (M-VAE) as the generative model. M-VAE can convert an entity from one KG to another and generate new entities from random noise vectors. We demonstrate the power of GEEA with theoretical analysis and empirical experime
    
[^55]: 基于检查表引导迭代自查的元评审生成

    Meta-review Generation with Checklist-guided Iterative Introspection. (arXiv:2305.14647v1 [cs.CL])

    [http://arxiv.org/abs/2305.14647](http://arxiv.org/abs/2305.14647)

    本文提出了科学观点总结的任务，以合成研究论文评审的元评审，引入了一个新的ORSUM数据集，并提出了基于检查表引导迭代自查的方法，该方法表现出良好的效果。

    

    在科学领域中，不同的观点可能会导致对评审意见的争议或共识。然而，当前的观点总结数据集主要集中在产品评论领域，没有考虑到这种可变性，假设输入的意见是没有争议的。为了填补这一空白，我们提出了科学观点总结的任务，将研究论文评审合成为元评审。为了促进这个任务，我们引入了一个新的ORSUM数据集，涵盖了来自39个会议的10,989篇论文元审查和40,903篇论文审查。此外，我们提出了基于检查表引导迭代自查的方法，将任务分解为几个阶段，并在检查表的指导下迭代地完善摘要。我们得出结论：（1）人工撰写的摘要并不总是可靠的，因为许多人并没有遵循指南，（2）任务分解和迭代自我完善的组合表现出了良好的效果。

    Opinions in the scientific domain can be divergent, leading to controversy or consensus among reviewers. However, current opinion summarization datasets mostly focus on product review domains, which do not account for this variability under the assumption that the input opinions are non-controversial. To address this gap, we propose the task of scientific opinion summarization, where research paper reviews are synthesized into meta-reviews. To facilitate this task, we introduce a new ORSUM dataset covering 10,989 paper meta-reviews and 40,903 paper reviews from 39 conferences. Furthermore, we propose the Checklist-guided Iterative Introspection (CGI$^2$) approach, which breaks down the task into several stages and iteratively refines the summary under the guidance of questions from a checklist. We conclude that (1) human-written summaries are not always reliable since many do not follow the guideline, and (2) the combination of task decomposition and iterative self-refinement shows pro
    
[^56]: 通过困难期望最大化迭代改善生物医学实体链接和事件提取

    Iteratively Improving Biomedical Entity Linking and Event Extraction via Hard Expectation-Maximization. (arXiv:2305.14645v1 [cs.CL])

    [http://arxiv.org/abs/2305.14645](http://arxiv.org/abs/2305.14645)

    本研究提出了一种通过困难期望最大化来交替进行生物医学实体链接和事件提取的方法，具有全局结构和局部相关性约束。实验结果表明，在基准数据集上该方法有效地提高了生物医学实体链接和事件提取的准确性。

    

    生物医学实体链接和事件提取是支持生物医学领域文本理解和检索的两个关键任务。先前的研究通常将这两个任务分别解决或以流水线方式解决，从而导致错误传播。为了解决这些挑战，我们提出了通过将事件结构和实体引用作为两组潜在变量来联合生物医学实体链接和事件提取，并通过困难的期望最大化交错两个歧义消除步骤的方法。具体来说，我们在全局结构约束和局部相关性约束的指导下，迭代优化实体和事件分配。在基准数据集上的实验结果表明，我们提出的方法在联合改进生物医学实体链接和事件提取方面非常有效。

    Biomedical entity linking and event extraction are two crucial tasks to support text understanding and retrieval in the biomedical domain. These two tasks intrinsically benefit each other: entity linking disambiguates the biomedical concepts by referring to external knowledge bases and the domain knowledge further provides additional clues to understand and extract the biological processes, while event extraction identifies a key trigger and entities involved to describe each biological process which also captures the structural context to better disambiguate the biomedical entities. However, previous research typically solves these two tasks separately or in a pipeline, leading to error propagation. What's more, it's even more challenging to solve these two tasks together as there is no existing dataset that contains annotations for both tasks. To solve these challenges, we propose joint biomedical entity linking and event extraction by regarding the event structures and entity refere
    
[^57]: CMOT: 通过最优传输进行跨模态Mixup，用于语音翻译

    CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation. (arXiv:2305.14635v1 [cs.CL])

    [http://arxiv.org/abs/2305.14635](http://arxiv.org/abs/2305.14635)

    CMOT是一种用于跨模态语音翻译的方法，通过最优传输找到语音和文本序列之间的对齐，并在标记级别上混合不同模态的序列，实现了在有限数据下更好的性能表现。

    

    端到端语音翻译（ST）是将源语言中的语音信号翻译成目标语言文本的任务。作为一项跨模态任务，端到端ST在有限数据下进行训练非常困难。现有的方法通常尝试从机器翻译（MT）中转移知识，但其性能由于语音和文本之间的模态差距受到限制。本文提出了Cross-modal Mixup via Optimal Transport（CMOT）来克服模态差距。我们通过最优传输找到语音和文本序列之间的对齐，然后使用对齐在标记级别上混合来自不同模态的序列。在MuST-C ST基准测试上的实验表明，CMOT在8个翻译方向上实现了平均BLEU值为30.0，超过了以前的方法。进一步的分析表明，CMOT可以自适应地找到模态之间的对齐，有助于缓解语音和文本之间的模态差距。代码公开可用于 https://github.com/ic

    End-to-end speech translation (ST) is the task of translating speech signals in the source language into text in the target language. As a cross-modal task, end-to-end ST is difficult to train with limited data. Existing methods often try to transfer knowledge from machine translation (MT), but their performances are restricted by the modality gap between speech and text. In this paper, we propose Cross-modal Mixup via Optimal Transport CMOT to overcome the modality gap. We find the alignment between speech and text sequences via optimal transport and then mix up the sequences from different modalities at a token level using the alignment. Experiments on the MuST-C ST benchmark demonstrate that CMOT achieves an average BLEU of 30.0 in 8 translation directions, outperforming previous methods. Further analysis shows CMOT can adaptively find the alignment between modalities, which helps alleviate the modality gap between speech and text. Code is publicly available at https://github.com/ic
    
[^58]: 在GPT-3和GPT-4中测试词义因果模型

    Testing Causal Models of Word Meaning in GPT-3 and -4. (arXiv:2305.14630v1 [cs.CL])

    [http://arxiv.org/abs/2305.14630](http://arxiv.org/abs/2305.14630)

    本文使用HIPE理论中描述的词义因果关系模型，对GPT-3和GPT-4的词汇表示进行了评估，发现GPT-3没有编码因果结构的证据，而GPT-4则有。

    

    大型语言模型（LLM）推动了自然语言处理的巨大改进。然而，这些模型如何表示词汇概念，即它们使用的单词的含义，尚不清楚。本文通过HIPE理论（一种概念表示理论，侧重于描述人造物品的词语的表示）评估了GPT-3和GPT-4的词汇表示。该理论提出了一个将这些词汇的含义与它们所描述的对象的形式、用途和历史联系起来的因果关系图。我们使用Chaigneau等人（2004）最初用于人类评估该理论的相同刺激来测试LLM，并考虑了多种提示设计。我们的实验涉及有关因果结果、对象功能和对象命名的判断。我们没有发现GPT-3编码HIPE假设的因果结构的证据，但是发现GPT-4编码了这样的结构。研究结果有助于更好地理解大型语言模型如何表示词义。

    Large Language Models (LLMs) have driven extraordinary improvements in NLP. However, it is unclear how such models represent lexical concepts-i.e., the meanings of the words they use. This paper evaluates the lexical representations of GPT-3 and GPT-4 through the lens of HIPE theory, a theory of concept representations which focuses on representations of words describing artifacts (such as "mop", "pencil", and "whistle"). The theory posits a causal graph that relates the meanings of such words to the form, use, and history of the objects to which they refer. We test LLMs using the same stimuli originally used by Chaigneau et al. (2004) to evaluate the theory in humans, and consider a variety of prompt designs. Our experiments concern judgements about causal outcomes, object function, and object naming. We find no evidence that GPT-3 encodes the causal structure hypothesized by HIPE, but do find evidence that GPT-4 encodes such structure. The results contribute to a growing body of rese
    
[^59]: 实现大型语言模型生成带引文的文本

    Enabling Large Language Models to Generate Text with Citations. (arXiv:2305.14627v1 [cs.CL])

    [http://arxiv.org/abs/2305.14627](http://arxiv.org/abs/2305.14627)

    本文提出ALCE，是首个自动LLMs引文评估基准，实现大型语言模型生成带引文的文本，提高其事实正确性和可验证性；提示LLMs特定的关键词或利用外部知识源可以显著提高其引文准确性。

    

    大型语言模型（LLMs）已成为广泛使用的信息寻找工具，但生成的输出容易出现幻觉。本文旨在实现LLMs生成带引文的文本，提高其事实正确性和可验证性。我们提出了ALCE，这是首个自动LLMs引文评估基准。ALCE收集了各种问题和检索语料库，并要求建立端到端系统以检索支持证据并生成带有引文的答案。我们沿着流畅性、正确性和引文质量三个维度构建自动指标，并展示了它们与人类判断的强相关性。我们使用最先进的LLMs和新的提示策略进行实验，结果表明当前系统仍有相当大的提升空间--例如，提示LLMs特定的关键词或利用外部知识源可以显著提高其引文准确性。我们的工作为未来研究发展能够生成可验证和可信赖输出的LLMs提供了坚实基础。

    Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, we aim to enable LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare with different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We build automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvements -for example,
    
[^60]: KNN-LM并不能提高开放式文本生成的质量

    KNN-LM Does Not Improve Open-ended Text Generation. (arXiv:2305.14625v1 [cs.CL])

    [http://arxiv.org/abs/2305.14625](http://arxiv.org/abs/2305.14625)

    本文研究了基于插值的检索增强语言模型（LM）的生成质量，发现KNN-LM并不能提高开放式文本生成的整体质量。

    

    本文研究了基于插值的检索增强语言模型（LM）的生成质量。这些方法以KNN-LM为代表，将LM预测的下一个单词分布与给定前缀最相关的检索形成的分布插值。虽然KNN-LM和相关方法在困惑度方面表现出色，但我们发现它们在开放式生成质量方面没有相应的改进，这是通过自动评估指标（例如MAUVE）和人工评估来衡量的。进一步探究，我们发现与检索分布插值相比，对于WikiText-103测试集中的大多数标记，插值会增加困惑度，尽管由于困惑度显著降低的标记数量更少，因此总体上困惑度更低。然而，当在推理时解码长序列时，对这个小标记集的显著改进并不能转化为整个序列的持续性改进，这表明KNN-LM和相关方法的改进是局部现象，而不是LM的普遍改进。

    In this paper, we study the generation quality of interpolation-based retrieval-augmented language models (LMs). These methods, best exemplified by the KNN-LM, interpolate the LM's predicted distribution of the next word with a distribution formed from the most relevant retrievals for a given prefix. While the KNN-LM and related methods yield impressive decreases in perplexity, we discover that they do not exhibit corresponding improvements in open-ended generation quality, as measured by both automatic evaluation metrics (e.g., MAUVE) and human evaluations. Digging deeper, we find that interpolating with a retrieval distribution actually increases perplexity compared to a baseline Transformer LM for the majority of tokens in the WikiText-103 test set, even though the overall perplexity is lower due to a smaller number of tokens for which perplexity dramatically decreases after interpolation. However, when decoding a long sequence at inference time, significant improvements on this sma
    
[^61]: Self-Checker：用于基于大语言模型事实检查的即插即用模块

    Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models. (arXiv:2305.14623v1 [cs.CL])

    [http://arxiv.org/abs/2305.14623](http://arxiv.org/abs/2305.14623)

    本文介绍了Self-Checker框架，它由即插即用的模块组成，能够在几乎零次启动的情况下利用大型语言模型进行快速高效的事实检查，这对于在资源有限的环境下构建事实检查系统非常有用。

    

    事实检查是NLP中的一个重要任务，通常用于验证主张的事实准确性。以前的工作主要集中在对特定数据集进行预先训练的语言模型微调上，这可能需要大量的计算资源和时间。随着像ChatGPT和GPT-3这样的大型语言模型的快速发展，研究人员现在正在探索它们的上下文学习能力以执行各种任务。本文介绍了Self-Checker，这是一个框架，包括一组即插即用的模块，通过在几乎零次启动的情况下仅提示LLMs，从而便于对事实进行检查。该框架提供了在资源有限的环境中构建事实检查系统的快速高效方法。实证结果表明Self-Checker在利用LLMs进行事实检查方面具有潜力。然而，与SOTA微调模型相比仍有很大的改进空间，这表明需要进一步的研究和开发。

    Fact-checking is an essential task in NLP that is commonly utilized for validating the factual accuracy of claims. Prior work has mainly focused on fine-tuning pre-trained languages models on specific datasets, which can be computationally intensive and time-consuming. With the rapid development of large language models (LLMs), such as ChatGPT and GPT-3, researchers are now exploring their in-context learning capabilities for a wide range of tasks. In this paper, we aim to assess the capacity of LLMs for fact-checking by introducing Self-Checker, a framework comprising a set of plug-and-play modules that facilitate fact-checking by purely prompting LLMs in an almost zero-shot setting. This framework provides a fast and efficient way to construct fact-checking systems in low-resource environments. Empirical results demonstrate the potential of Self-Checker in utilizing LLMs for fact-checking. However, there is still significant room for improvement compared to SOTA fine-tuned models, wh
    
[^62]: EXnet: 无数据文本分类的高效上下文学习

    EXnet: Efficient In-context Learning for Data-less Text classification. (arXiv:2305.14622v1 [cs.CL])

    [http://arxiv.org/abs/2305.14622](http://arxiv.org/abs/2305.14622)

    EXnet是一个模型，旨在进行上下文学习，可以在没有示例数量限制的情况下进行。上下文学习是提高任务准确性和跨任务普适性的有效方法，特别是在文本分类方面。

    

    大型预训练语言模型（PLMs）在编码世界知识方面取得了显着进展，并产生了一系列新的学习范例，包括零-shot、少-shot和上下文学习。许多语言任务可以被建模为一组提示（例如，这段文本是否与地理有关？）和语言模型可以提供二进制答案，即“是”或“否”。有证据表明，许多PLM使用的下一个单词预测与零-shot范例不相符合。因此，PLMs会被微调为问答系统。上下文学习通过将提示和示例结合起来，扩展了零-shot学习，从而提高了任务的准确性。我们的论文介绍了EXnet，这是一个专门设计用于在上下文中进行学习的模型，没有任何例外的限制。我们认为，在上下文学习是提高任务准确性的有效方法，并提供示例有助于跨任务的普适性，特别是在文本分类方面。

    Large pre-trained language models (PLMs) have made significant progress in encoding world knowledge and spawned a new set of learning paradigms including zero-shot, few-shot, and in-context learning. Many language tasks can be modeled as a set of prompts (for example, is this text about geography?) and language models can provide binary answers, i.e., Yes or No. There is evidence to suggest that the next-word prediction used by many PLMs does not align well with zero-shot paradigms. Therefore, PLMs are fine-tuned as a question-answering system. In-context learning extends zero-shot learning by incorporating prompts and examples, resulting in increased task accuracy. Our paper presents EXnet, a model specifically designed to perform in-context learning without any limitations on the number of examples. We argue that in-context learning is an effective method to increase task accuracy, and providing examples facilitates cross-task generalization, especially when it comes to text classifi
    
[^63]: 通过互斥解释开展诱导式常识推理

    Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations. (arXiv:2305.14618v1 [cs.CL])

    [http://arxiv.org/abs/2305.14618](http://arxiv.org/abs/2305.14618)

    本论文提出一种基于互斥解释的诱导式常识推理方法，结果表明在各类诱导式推理数据集上优于或与预训练语言模型和其他知识增强方法相当。

    

    诱导式推理旨在寻找一个事件的可能解释。在常识任务中，存在着多种合理的解释形式，因此诱导式推理对于这类任务至关重要。现有的自然语言处理中，关于诱导式推理的方法通常依赖于手工标注的数据，但是这种标注往往带有主观性和偏见。本文提出了一种诱导式常识推理的方法，这种方法利用了一个事实，即对于给定的上下文，只有解释的一个子集是正确的。本方法利用后验规范化来施加互斥约束，鼓励模型学习流畅解释和合理的解释之间的区别。我们在各类诱导式推理数据集上对该方法进行了评估；实验结果表明，我们的方法在零-shot情况下优于或与直接应用预训练语言模型和其他知识增强方法相当。

    Abductive reasoning aims to find plausible explanations for an event. This style of reasoning is critical for commonsense tasks where there are often multiple plausible explanations. Existing approaches for abductive reasoning in natural language processing (NLP) often rely on manually generated annotations for supervision; however, such annotations can be subjective and biased. Instead of using direct supervision, this work proposes an approach for abductive commonsense reasoning that exploits the fact that only a subset of explanations is correct for a given context. The method uses posterior regularization to enforce a mutual exclusion constraint, encouraging the model to learn the distinction between fluent explanations and plausible ones. We evaluate our approach on a diverse set of abductive reasoning datasets; experimental results show that our approach outperforms or is comparable to directly applying pretrained language models in a zero-shot manner and other knowledge-augmente
    
[^64]: COMET-M: 在复杂句子中推理多个事件

    COMET-M: Reasoning about Multiple Events in Complex Sentences. (arXiv:2305.14617v1 [cs.CL])

    [http://arxiv.org/abs/2305.14617](http://arxiv.org/abs/2305.14617)

    提出了COMET-M，该模型可以推理复杂句子中多个事件之间的关系以及生成常识推断，并在35K个人类编写的推断上进行训练，相对于之前的COMET模型在生成多事件推断方面有显着的性能改进。

    

    理解说话者的意图通常涉及绘制常识推断，以推理未明确陈述的内容。在多事件句子中，需要基于上下文知识理解事件之间的关系。我们提出了COMET-M（Multi-Event），这是一个以事件为中心的常识模型，能够针对复杂句子内的目标事件生成常识推断。COMET-M是基于COMET（Bosselut et al.，2019）发展而来的，后者擅长为简单句子生成以事件为中心的推断，但在自然文本中普遍存在的多事件句子的复杂性方面表现不佳。为了克服这个限制，我们整理了一个包含35K个人类编写推断的多事件推断数据集。 我们在人类编写的推断上训练了COMET-M，并创建了使用自动标记示例的基线。实验结果表明，COMET-M在生成多事件推断方面相对于COMET具有显着的性能改进。此外，COMET-M成功预测了测试集中60％的复杂句子目标事件的常识推断。

    Understanding the speaker's intended meaning often involves drawing commonsense inferences to reason about what is not stated explicitly. In multi-event sentences, it requires understanding the relationships between events based on contextual knowledge. We propose COMET-M (Multi-Event), an event-centric commonsense model capable of generating commonsense inferences for a target event within a complex sentence. COMET-M builds upon COMET (Bosselut et al., 2019), which excels at generating event-centric inferences for simple sentences, but struggles with the complexity of multi-event sentences prevalent in natural text. To overcome this limitation, we curate a multi-event inference dataset of 35K human-written inferences. We trained COMET-M on the human-written inferences and also created baselines using automatically labeled examples. Experimental results demonstrate the significant performance improvement of COMET-M over COMET in generating multi-event inferences. Moreover, COMET-M succ
    
[^65]: 在图像描述中探讨对基础问题

    Exploring the Grounding Issues in Image Caption. (arXiv:2305.14616v1 [cs.CL])

    [http://arxiv.org/abs/2305.14616](http://arxiv.org/abs/2305.14616)

    该论文为我们从计算认知语言的角度出发，探讨了图像描述中的多模态语义表征中的基础问题。研究结果表明，情境含义和功能性是生成适当的图像描述的关键。

    

    本篇论文从计算认知语言的角度探讨了多模态语义表征中的基础问题。采用了感知性、功能性、显著性、注意力和生态学多样性关联等五个基础属性进行注释和分析。通过对Flickr30k数据集中所选的图像进行探索性分析和统计建模来进行研究。研究结果表明，对一个对象或事件的全面理解需要认知注意力、语义表达的语义区分和多模态构建。在构建过程中，观察者将情境含义和功能性融入到多模态语义当中，将其巩固到包含视觉和文本元素的图像-文本数据集中的图像描述中。我们的研究结果表明，情境含义和功能性对于基础自然语言理解系统生成适当的图像描述至关重要。

    This paper explores the grounding issue concerning multimodal semantic representation from a computational cognitive-linguistic view. Five perceptual properties of groundedness are annotated and analyzed: Affordance, Perceptual salience, Object number, Gaze cueing, and Ecological Niche Association (ENA). We annotated selected images from the Flickr30k dataset with exploratory analyses and statistical modeling of their captions. Our findings suggest that a comprehensive understanding of an object or event requires cognitive attention, semantic distinctions in linguistic expression, and multimodal construction. During this construction process, viewers integrate situated meaning and affordance into multimodal semantics, which is consolidated into image captions used in the image-text dataset incorporating visual and textual elements. Our findings suggest that situated meaning and affordance grounding are critical for grounded natural language understanding systems to generate appropriate
    
[^66]: 对模糊问题的有选择性回答

    Selectively Answering Ambiguous Questions. (arXiv:2305.14613v1 [cs.CL])

    [http://arxiv.org/abs/2305.14613](http://arxiv.org/abs/2305.14613)

    本研究调查了解决模糊问题的方法，通过定量测量模型输出中的重复性，找出了在含糊问题集中回答高精度子集问题的最可靠方法。这种基于采样的方法可以有效解决问题回答中的歧义，并提高语言模型的可靠性。

    

    可靠的语言模型应该在不知道答案的情况下放弃回答问题。然而，由于问问者意图或上下文的不确定性，问题的答案也可能不清楚。本研究从这个角度调查了问题回答，专注于在众多本质上含糊的问题集中回答高精度子集的问题。在此设置中，我们发现定量测量一组采样模型输出中的重复性是最可靠的校准方法，而非先前工作中使用的模型的概率或自我验证。我们发现，这种方法适用于不同类型的不确定性，不同的模型规模，以及带或不带指导调整。我们的结果表明，基于采样的方法可以有效解决问题回答中的歧义，并提高语言模型的可靠性。

    Trustworthy language models should abstain from answering questions when they do not know the answer. However, the answer to a question can be unknown for a variety of reasons. Prior research has focused on the case in which the question is clear and the answer is unambiguous but possibly unknown. However, the answer to a question can also be unclear due to uncertainty of the questioner's intent or context. We investigate question answering from this perspective, focusing on answering a subset of questions with a high degree of accuracy, from a set of questions in which many are inherently ambiguous. In this setting, we find that the most reliable approach to calibration involves quantifying repetition within a set of sampled model outputs, rather than the model's likelihood or self-verification as used in prior work. % We find this to be the case across different types of uncertainty, varying model scales and both with or without instruction tuning. Our results suggest that sampling-b
    
[^67]: 这片土地是你我的土地：评估语言模型中的地缘政治偏见

    This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models. (arXiv:2305.14610v1 [cs.CL])

    [http://arxiv.org/abs/2305.14610](http://arxiv.org/abs/2305.14610)

    本文提出了地缘政治偏见的概念，并以领土争端为例，利用多语言、多选题的数据集BorderLines和几个定量指标分析语言模型响应中的地缘政治偏见现象。

    

    我们引入了地缘政治偏见的概念——即根据语言环境报道不同的地缘政治知识的倾向。我们以领土争端为案例进行了研究。例如，对于被广泛争议的南沙群岛，如果用中文问，LM是否更有可能说它们属于中国，而如果用塔加洛语问，则更有可能说它们属于菲律宾？为了评估是否存在这种偏见，我们首先从维基百科上收集了一组领土争端数据，然后将每个领土与一组多语言、多选题联系起来。这个数据集被称为BorderLines，它包括250个领土和45种语言的问题。我们将这些问题集提交给语言模型，并通过几个提出的定量指标分析它们的响应中地缘政治偏见。这些指标比较不同语言的回答以及实际的地缘政治情况。地缘政治偏见现象是一种独特的跨语言评估。

    We introduce the notion of geopolitical bias -- a tendency to report different geopolitical knowledge depending on the linguistic context. As a case study, we consider territorial disputes between countries. For example, for the widely contested Spratly Islands, would an LM be more likely to say they belong to China if asked in Chinese, vs. to the Philippines if asked in Tagalog? To evaluate if such biases exist, we first collect a dataset of territorial disputes from Wikipedia, then associate each territory with a set of multilingual, multiple-choice questions. This dataset, termed BorderLines, consists of 250 territories with questions in 45 languages. We pose these question sets to language models, and analyze geopolitical bias in their responses through several proposed quantitative metrics. The metrics compare between responses in different question languages as well as to the actual geopolitical situation. The phenomenon of geopolitical bias is a uniquely cross-lingual evaluation
    
[^68]: OpenPI2.0: 一种用于实体追踪的改进数据集

    OpenPI2.0: An Improved Dataset for Entity Tracking in Texts. (arXiv:2305.14603v1 [cs.CL])

    [http://arxiv.org/abs/2305.14603](http://arxiv.org/abs/2305.14603)

    OpenPI2.0是一个用于实体追踪的改进数据集，它包括规范化实体、显著性注释和下游应用调查等特点。

    

    将文本表示为实体信息一直被认为在事件推理中非常有效。我们提出了OpenPI2.0，这是一个用于追踪程序性文本中实体状态的改进数据集。OpenPI2.0不仅具有规范化实体以促进评估，还包括涵盖人工标签和自动预测的显著性注释。关于实体显著性，我们提供了有关注释主观性、建模可行性以及在问题回答和经典计划等任务中的下游应用的调查。

    Representing texts as information about entities has long been deemed effective in event reasoning. We propose OpenPI2.0, an improved dataset for tracking entity states in procedural texts. OpenPI2.0 features not only canonicalized entities that facilitate evaluation, but also salience annotations including both manual labels and automatic predictions. Regarding entity salience, we provide a survey on annotation subjectivity, modeling feasibility, and downstream applications in tasks such as question answering and classical planning.
    
[^69]: 学习从兼容标签序列中的语义角色标注

    Learning Semantic Role Labeling from Compatible Label Sequences. (arXiv:2305.14600v1 [cs.CL])

    [http://arxiv.org/abs/2305.14600](http://arxiv.org/abs/2305.14600)

    该论文探讨了如何从不相交的兼容标签序列中高效地学习，将此运用于语义角色标注任务，提出了联合处理VerbNet和PropBank标签的方法，并验证了其有效性。

    

    本文探讨了如何高效地学习从不相交的兼容标签序列中标注的问题。我们认为，不相交标签集之间的兼容结构有助于模型的学习和推理。我们在语义角色标注（SRL）任务中验证了这一假设，具体地，标记具有两个角色序列的句子：VerbNet参数和PropBank参数。先前的研究已经表明跨任务交互可以提高性能。但是，这两个任务仍然是分别解码的，存在生成结构不一致的标签序列 (在像SEMLINK的词典中)的风险。为了消除这个问题，我们首先提出了一个简单而有效的设置，联合处理VerbNet和PropBank标签作为一个序列。通过这个设置，我们证明了在解码过程中强制执行SEMLINK约束不断提高总F1值。通过特殊的输入构造，我们的联合模型可以以超过99%的准确性从PropBank参数中推断出VerbNet参数。我们还提出了一种co

    This paper addresses the question of how to efficiently learn from disjoint, compatible label sequences. We argue that the compatible structures between disjoint label sets help model learning and inference. We verify this hypothesis on the task of semantic role labeling (SRL), specifically, tagging a sentence with two role sequences: VerbNet arguments and PropBank arguments. Prior work has shown that cross-task interaction improves performance. However, the two tasks are still separately decoded, running the risk of generating structurally inconsistent label sequences (as per lexicons like SEMLINK). To eliminate this issue, we first propose a simple and effective setup that jointly handles VerbNet and PropBank labels as one sequence. With this setup, we show that enforcing SEMLINK constraints during decoding constantly improves the overall F1. With special input constructions, our joint model infers VerbNet arguments from PropBank arguments with over 99% accuracy. We also propose a co
    
[^70]: 连续和离散空间的桥梁: 通过组合操作进行可解释的句子表示学习

    Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations. (arXiv:2305.14599v1 [cs.CL])

    [http://arxiv.org/abs/2305.14599](http://arxiv.org/abs/2305.14599)

    本文提出了InterSent框架，探索将不同的组合属性纳入句子嵌入空间中使嵌入变换变为组合句子操作，实现了可解释的句子表示学习。

    

    传统的句子嵌入模型将句子编码为向量表示，以捕捉诸如句子语义相似性之类的有用属性。然而，除了相似性外，句子语义还可以通过组合操作（如句子融合或差异）进行解释。不清楚是否可以直接将句子的组合语义反映为嵌入空间中的组合操作。为了更有效地连接连续嵌入和离散文本空间，我们探索了将各种组合性质合并到句子嵌入空间中的可行性，从而允许我们将嵌入变换解释为组合句子操作。我们提出了InterSent，一种学习可解释的句子嵌入的端到端框架，它支持嵌入空间中的组合句子操作。我们的方法优化操作器网络和瓶颈编码器-解码器模型，以产生有意义的生成句子。

    Traditional sentence embedding models encode sentences into vector representations to capture useful properties such as the semantic similarity between sentences. However, in addition to similarity, sentence semantics can also be interpreted via compositional operations such as sentence fusion or difference. It is unclear whether the compositional semantics of sentences can be directly reflected as compositional operations in the embedding space. To more effectively bridge the continuous embedding and discrete text spaces, we explore the plausibility of incorporating various compositional properties into the sentence embedding space that allows us to interpret embedding transformations as compositional sentence operations. We propose InterSent, an end-to-end framework for learning interpretable sentence embeddings that supports compositional sentence operations in the embedding space. Our method optimizes operator networks and a bottleneck encoder-decoder model to produce meaningful an
    
[^71]: 她们的声音：分析人工智能出版领域的性别差异

    Voices of Her: Analyzing Gender Differences in the AI Publication World. (arXiv:2305.14597v1 [cs.CL])

    [http://arxiv.org/abs/2305.14597](http://arxiv.org/abs/2305.14597)

    通过对AI学术界的78K研究人员的分析，研究发现女性第一作者的论文具有不同的语言风格，例如更长的文本、更多的正面情感词汇和更引人注目的标题；在AI论文的合著中存在很大的性别同质性。我们鼓励未来实现更多的性别平等和多样性。

    

    虽然先前的研究已经分析了学术界中的性别偏见，但是我们仍然缺乏一个全面的人工智能社区性别差异的分析，涵盖各种主题和不同的发展趋势。我们使用AI Scholar数据集中的78K位AI领域的研究人员，发现了一些性别差异：（1）虽然女性研究人员的总引用次数比男性少，但这种引用差异并不适用于所有学术年龄组；（2）在AI论文的合著中存在很大的性别同质性；（3）女性第一作者的论文显示出不同的语言风格，例如更长的文本、更多的正面情感词汇和更引人注目的标题。我们的分析为我们的AI社区现有的人口统计趋势提供了一个窗口，并鼓励在未来实现更多的性别平等和多样性。我们的代码和数据可在https://github.com/causalNLP/ai-scholar-gender找到。

    While several previous studies have analyzed gender bias in research, we are still missing a comprehensive analysis of gender differences in the AI community, covering diverse topics and different development trends. Using the AI Scholar dataset of 78K researchers in the field of AI, we identify several gender differences: (1) Although female researchers tend to have fewer overall citations than males, this citation difference does not hold for all academic-age groups; (2) There exist large gender homophily in co-authorship on AI papers; (3) Female first-authored papers show distinct linguistic styles, such as longer text, more positive emotion words, and more catchy titles than male first-authored papers. Our analysis provides a window into the current demographic trends in our AI community, and encourages more gender equality and diversity in the future. Our code and data are at https://github.com/causalNLP/ai-scholar-gender.
    
[^72]: 注意力不一定意味着在解答中选择正确率很高

    Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy. (arXiv:2305.14596v1 [cs.CL])

    [http://arxiv.org/abs/2305.14596](http://arxiv.org/abs/2305.14596)

    当大型语言模型应用于多项选择题时，其注意力往往会分散到许多无效的词汇符号上，这会导致模型真实性能的低估。本文提出了一种数学形式化方法来研究这种现象，并发现通过使用只含一个示例的上下文学习方法可以提高对有效选择的注意力。

    

    当大型语言模型被应用于零或少样本的鉴别性任务，例如多项选择题时，它们的注意力（即概率质量）会分散在许多无效的词汇符号上。这种在具有相同含义的多个表面形式之间分散导致了模型真实性能的低估，称为“表面形式竞争”（SFC）假说。这促使引入各种概率规范化方法，然而仍存在许多核心问题未解答。我们如何测量SFC或注意力？是否有直接的方法可以增加对有效选择的注意力？增加注意力总是能提高任务准确性吗？我们提出了一种数学形式化方法来研究这种现象，提供了一种量化注意力的度量方法，并确定了一种简单的增加注意力的方法，即通过仅包含答案选项的一个示例进行上下文学习。

    When large language models (LMs) are applied in zero- or few-shot settings to discriminative tasks such as multiple-choice questions, their attentiveness (i.e., probability mass) is spread across many vocabulary tokens that are not valid choices. Such a spread across multiple surface forms with identical meaning is thought to cause an underestimation of a model's true performance, referred to as the "surface form competition" (SFC) hypothesis. This has motivated the introduction of various probability normalization methods. However, many core questions remain unanswered. How do we measure SFC or attentiveness? Are there direct ways of increasing attentiveness on valid choices? Does increasing attentiveness always improve task accuracy? We propose a mathematical formalism for studying this phenomenon, provide a metric for quantifying attentiveness, and identify a simple method for increasing it -namely, in-context learning with even just one example containing answer choices. The form
    
[^73]: 带词典的指令优化用于零样式分类

    Instruction Tuning with Lexicons for Zero-Shot Style Classification. (arXiv:2305.14592v1 [cs.CL])

    [http://arxiv.org/abs/2305.14592](http://arxiv.org/abs/2305.14592)

    通过使用词典指导语言模型识别训练期间未见过的新风格，可以有效地提高零样本性能的转移。

    

    风格用于传达作者的意图和态度。尽管大型预训练语言模型在风格分类上取得了成功，但先前的研究依赖于带标签的样本进行微调。启发大型语言模型在没有微调的情况下对风格进行分类具有挑战性，因为语言风格可能很难定义。在这项研究中，我们调查了风格词典作为指导语言模型如何识别在训练期间未见过的新风格的有效性。我们的实验表明，基于词典的指令显著提高了零样本性能的转移。我们将发布我们的代码和数据。

    Style is used to convey authors' intentions and attitudes. Despite the success of large pre-trained language models on style classification, prior work relies on fine-tuning with labeled examples. Prompting large language models to classify style without fine-tuning is challenging because language styles can be difficult to define. In this study, we investigate the effectiveness of style lexicons as a means for instructing language models how to identify new styles that are unseen during training. Our experiments show that lexicon-based instructions improve transfer zero-shot performance significantly. We will release our code and data.
    
[^74]: ALGO：使用生成的神谕验证程序的合成算法程序

    ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers. (arXiv:2305.14591v1 [cs.CL])

    [http://arxiv.org/abs/2305.14591](http://arxiv.org/abs/2305.14591)

    ALGO框架使用由LLM生成的神谕指导创造和验证算法程序，以提高现有代码生成模型的算法问题解决能力。

    

    大型语言模型(Large language models, LLMs)在实现代码的功能描述方面表现出色，但在需要确定适当算法的算法问题上亟需提升。此外，LLM生成的程序缺乏保证正确性并需要人工验证。为了解决这些挑战，我们提出了ALGO框架，该框架使用由LLM生成的神谕指导创造和验证算法程序。ALGO首先通过促使LLM枚举相关变量的所有组合来生成具有可能的正确性但可能较慢的参考神谕。然后，利用该神谕指导任意搜索策略来探索算法空间并验证合成的算法。我们的研究表明，LLM生成的神谕在88%的情况下是正确的。使用这些神谕作为验证程序，ALGO可以以模型无关的方式与任何现有的代码生成模型集成，以提高其算法问题解决能力。

    Large language models (LLMs) excel at implementing code from functionality descriptions, but struggle with algorithmic problems that require not only implementation but also identification of the suitable algorithm. Moreover, LLM-generated programs lack guaranteed correctness and require human verification. To address these challenges, we propose ALGO, a framework that synthesizes Algorithmic programs with LLM-Generated Oracles to guide the creation and verify their correctness. ALGO first generates a probably correct but possibly slow reference oracle by prompting an LLM to exhaustively enumerate all the combinations of relevant variables. This oracle is then utilized to guide an arbitrary search strategy in exploring the algorithm space and to verify the algorithms synthesized. Our study shows that the LLM-generated oracles are correct for 88% of the cases. With the oracles as verifiers, ALGO can be integrated with any existing code generation model in a model-agnostic manner to enha
    
[^75]: RE$^2$: 面向视觉丰富文档的区域感知关系抽取

    RE$^2$: Region-Aware Relation Extraction from Visually Rich Documents. (arXiv:2305.14590v1 [cs.CL])

    [http://arxiv.org/abs/2305.14590](http://arxiv.org/abs/2305.14590)

    RE$^2$方法利用视觉丰富文档中实体块之间的区域级空间结构来提高它们的关系预测能力，表现出较好的性能。

    

    当前的表单理解研究主要依赖于大型预训练语言模型，需要广泛的预训练数据。然而，布局结构（即视觉丰富文档中实体块之间的空间关系）对于关系抽取的重要性却被忽视了。本文提出了一种名为 RE$^2$ 的区域感知关系抽取方法，利用实体块之间的区域级空间结构来提高它们的关系预测能力。我们设计了一种边缘感知图注意力网络，来学习实体之间的交互作用，同时考虑它们的区域级表示所定义的空间关系。我们还引入了一个约束目标，来规范模型以符合关系抽取任务的固有约束条件。在各种数据集、语言和领域的广泛实验中，我们提出的方法表现出了优越性。

    Current research in form understanding predominantly relies on large pre-trained language models, necessitating extensive data for pre-training. However, the importance of layout structure (i.e., the spatial relationship between the entity blocks in the visually rich document) to relation extraction has been overlooked. In this paper, we propose REgion-Aware Relation Extraction (RE$^2$) that leverages region-level spatial structure among the entity blocks to improve their relation prediction. We design an edge-aware graph attention network to learn the interaction between entities while considering their spatial relationship defined by their region-level representations. We also introduce a constraint objective to regularize the model towards consistency with the inherent constraints of the relation extraction task. Extensive experiments across various datasets, languages and domains demonstrate the superiority of our proposed approach.
    
[^76]: 评估领域特定知识库的端到端实体链接：从博物馆收藏中学习古代技术。

    Evaluating end-to-end entity linking on domain-specific knowledge bases: Learning about ancient technologies from museum collections. (arXiv:2305.14588v1 [cs.CL])

    [http://arxiv.org/abs/2305.14588](http://arxiv.org/abs/2305.14588)

    本文评估并改进了实体链接方法在博物馆藏品数据中的应用，展示微调的最新端到端 EL 模型明显优于现有方法，同时提供了数据集和最佳模型。

    

    为研究社会、经济和历史问题，社会科学和人文学科研究人员开始使用越来越大的非结构化文本数据集。虽然自然语言处理的最近进展提供了许多有效处理这些数据的工具，但大多数现有方法依赖于不适用于领域特定任务的通用解决方案，其性能和适合性都不是很好评估。本文提出了一种尝试去填补领域差距的方法，通过探索使用现代实体链接方法来丰富博物馆藏品数据。我们收集了一个数据集，其中包含7,510对实体提及，共1700多个文本进行了注释，详细评估了一些现成的解决方案，并使用此数据对最近的端到端 EL 模型进行微调。我们展示了我们微调的模型明显优于目前此领域中其他可用的方法，并展示了该模型的概念验证用例，同时开放了我们的数据集和我们的最佳模型。

    To study social, economic, and historical questions, researchers in the social sciences and humanities have started to use increasingly large unstructured textual datasets. While recent advances in NLP provide many tools to efficiently process such data, most existing approaches rely on generic solutions whose performance and suitability for domain-specific tasks is not well understood. This work presents an attempt to bridge this domain gap by exploring the use of modern Entity Linking approaches for the enrichment of museum collection data. We collect a dataset comprising of more than 1700 texts annotated with 7,510 mention-entity pairs, evaluate some off-the-shelf solutions in detail using this dataset and finally fine-tune a recent end-to-end EL model on this data. We show that our fine-tuned model significantly outperforms other approaches currently available in this domain and present a proof-of-concept use case of this model. We release our dataset and our best model.
    
[^77]: 上下文化主题相干性评估指标

    Contextualized Topic Coherence Metrics. (arXiv:2305.14587v1 [cs.CL])

    [http://arxiv.org/abs/2305.14587](http://arxiv.org/abs/2305.14587)

    本研究提出了一种上下文化主题相干性评估指标（CTC），该指标不仅在短文档上运作良好，而且在相对于其他五个指标评估上具有更高的性能表现。

    

    近年来，神经主题建模的大量研究被指责在优化自动化主题评估指标的同时牺牲了实质性的主题识别。但是，人工标注的成本高昂且耗时。本文提出了一种基于LLM的方法，受到人类主题评估的启发，提出了一种度量系列，称为上下文化主题相干性（CTC）。我们评估了一个全自动的版本以及一个半自动化的CTC，该版本针对人工中心的相干性评估，同时保持了自动化方法的效率。我们在六个主题模型上相对于五个其他指标评估CTC，并发现它优于自动主题一致性方法，在短文档上运作良好，并且不容易受到评分高但毫无意义的主题的影响。

    The recent explosion in work on neural topic modeling has been criticized for optimizing automated topic evaluation metrics at the expense of actual meaningful topic identification. But human annotation remains expensive and time-consuming. We propose LLM-based methods inspired by standard human topic evaluations, in a family of metrics called Contextualized Topic Coherence (CTC). We evaluate both a fully automated version as well as a semi-automated CTC that allows human-centered evaluation of coherence while maintaining the efficiency of automated methods. We evaluate CTC relative to five other metrics on six topic models and find that it outperforms automated topic coherence methods, works well on short documents, and is not susceptible to meaningless but high-scoring topics.
    
[^78]: 让隐含的显性化：以NLP中的隐式内容为第一公民

    Making the Implicit Explicit: Implicit Content as a First Class Citizen in NLP. (arXiv:2305.14583v1 [cs.CL])

    [http://arxiv.org/abs/2305.14583](http://arxiv.org/abs/2305.14583)

    该研究通过将自然语言处理的重点放在隐式内容上，提出了一种通过推理和分解方法降低自然语言处理复杂度的新方法，并在嵌入，计算政治学和构建发现方面实现了显著的改进和应用。

    

    语言是多元化的，一个表述可以用等价的形式重申，而其中的隐含和显性内容支持各种逻辑和语用推理。在处理表述时，我们考虑这些不同的方面，因为我们需要理解“这里很黑”可能是一个暗示需要打开灯。然而，NLP方法通常仅仅基于表面形式操作，省略了这种细微差别。在这项工作中，我们用语言来表示语言，并引导LLM将表述分解为逻辑和可信的推理。分解的降低复杂性，使它们更容易嵌入，开启了新的应用。我们的技术变化在句子嵌入基准测试中实现了最先进的改进，在计算政治学中有实质性应用，并引出一种新的构建发现过程，我们用人工注释验证了这种过程。

    Language is multifaceted. A given utterance can be re-expressed in equivalent forms, and its implicit and explicit content support various logical and pragmatic inferences. When processing an utterance, we consider these different aspects, as mediated by our interpretive goals -- understanding that "it's dark in here" may be a veiled direction to turn on a light. Nonetheless, NLP methods typically operate over the surface form alone, eliding this nuance.  In this work, we represent language with language, and direct an LLM to decompose utterances into logical and plausible inferences. The reduced complexity of the decompositions makes them easier to embed, opening up novel applications. Variations on our technique lead to state-of-the-art improvements on sentence embedding benchmarks, a substantive application in computational political science, and to a novel construct-discovery process, which we validate with human annotations.
    
[^79]: 评估OpenAI提供的Whisper ASR在Museum of the Person的生活史中进行标点符号预测和主题建模的效果

    Evaluating OpenAI's Whisper ASR for Punctuation Prediction and Topic Modeling of life histories of the Museum of the Person. (arXiv:2305.14580v1 [cs.CL])

    [http://arxiv.org/abs/2305.14580](http://arxiv.org/abs/2305.14580)

    本文首次对葡萄牙语中的Whisper ASR进行了标点符号预测方面的研究，并为标点符号预测在主题建模中的应用提供了有益的实验评估。

    

    自动语音识别（ASR）系统在人机交互应用中扮演着重要角色。然而，过去十年中提出的葡萄牙语ASR模型在正确识别自动转录中的标点符号方面存在局限性，这使得这些转录不能被其他系统、模型和甚至是人类使用。最近，OpenAI提出了Whisper ASR，这是一个通用的语音识别模型，有望处理这些限制。本研究是第一次针对葡萄牙语中Whisper的标点符号预测性能进行的研究。我们使用实验评估来考虑关于停顿点（逗号）和完整思想（感叹、疑问和句号）的理论方面，以及与基于转录的主题建模相关的实际方面，使用标点符号来提高性能的应用。

    Automatic speech recognition (ASR) systems play a key role in applications involving human-machine interactions. Despite their importance, ASR models for the Portuguese language proposed in the last decade have limitations in relation to the correct identification of punctuation marks in automatic transcriptions, which hinder the use of transcriptions by other systems, models, and even by humans. However, recently Whisper ASR was proposed by OpenAI, a general-purpose speech recognition model that has generated great expectations in dealing with such limitations. This chapter presents the first study on the performance of Whisper for punctuation prediction in the Portuguese language. We present an experimental evaluation considering both theoretical aspects involving pausing points (comma) and complete ideas (exclamation, question, and fullstop), as well as practical aspects involving transcript-based topic modeling - an application dependent on punctuation marks for promising performan
    
[^80]: 连接点：基于图网络的文本表示在文本分类中的最佳表现研究

    Connecting the Dots: What Graph-Based Text Representations Work Best for Text Classification using Graph Neural Networks?. (arXiv:2305.14578v1 [cs.CL])

    [http://arxiv.org/abs/2305.14578](http://arxiv.org/abs/2305.14578)

    本文研究了基于图的文本表示方法在文本分类中的应用，发现文本输入特征和领域要素对图的性能具有重要影响，BERT在处理短文本时难以收敛，图方法对于较长的文档特别有益。

    

    鉴于图神经网络在结构感知机器学习中的成功，许多研究已经探索了它们作为传统特征表示模型的替代方法，用于文本分类。然而，大多数研究仅考虑了特定领域，并验证了具有特定特征的数据。本文对提出用于文本分类的基于图的文本表示方法进行了广泛的实证研究，确定了实际实施的含义和领域中的挑战。我们比较了五个数据集中的几种GNN架构以及BERT，涵盖了长短文档。结果表明：i）图的性能与文本输入特征和领域密切相关，ii）尽管其表现出色，但BERT在处理短文本时难以收敛， iii）图方法对于较长的文档特别有益。

    Given the success of Graph Neural Networks (GNNs) for structure-aware machine learning, numerous studies have explored their application to text classification, as an alternative to traditional feature representation models. However, most studies considered just a specific domain and validated on data with particular characteristics. This work presents an extensive empirical investigation of graph-based text representation methods proposed for text classification, identifying practical implications and open challenges in the field. We compare several GNN architectures as well as BERT across five datasets, encompassing short and also long documents. The results show that: i) graph performance is highly related to the textual input features and domain, ii) despite its outstanding performance, BERT has difficulties converging when dealing with short texts, iii) graph methods are particularly beneficial for longer documents.
    
[^81]: 差异性遮挡：选择在继续预训练中遮挡什么

    Difference-Masking: Choosing What to Mask in Continued Pretraining. (arXiv:2305.14577v1 [cs.LG])

    [http://arxiv.org/abs/2305.14577](http://arxiv.org/abs/2305.14577)

    本文提出了一种自动选取遮蔽数据的方法（Difference-Masking），以提高在继续预训练中的自监督学习模型的性能，方法是通过考虑未标记的目标域与预训练域的不同之处来进行。实验证明，该方法可以有效地提升继续预训练任务的性能，且具有跨任务的适用性。

    

    自监督学习(SSL)，特别是遮挡预测目标的目标，已经在各种下游任务中证明了很好的性能，然而，大多数方法都是随机地进行标记和遮挡，而在教育领域有强烈的直觉认为，决定什么需要遮挡可以实质性地改善学习结果。我们引入了差异遮挡(Difference-Masking)，一种自动选择遮挡什么的方法，在继续预训练中通过考虑未标记的目标域与预训练域的不同之处来实现。实证上，我们发现差异遮挡在四个不同的语言和多模态视频任务的继续预训练设置中优于基线。差异性遮挡的跨任务适用性支持我们的框架在语言、视觉和其他领域的SSL预训练中的有效性。

    Self-supervised learning (SSL) and the objective of masking-and-predicting in particular have led to promising SSL performance on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition from the field of education that deciding what to mask can substantially improve learning outcomes. We introduce Difference-Masking, an approach that automatically chooses what to mask during continued pretraining by considering what makes an unlabelled target domain different from the pretraining domain. Empirically, we find that Difference-Masking outperforms baselines on continued pretraining settings across four diverse language and multimodal video tasks. The cross-task applicability of Difference-Masking supports the effectiveness of our framework for SSL pretraining in language, vision, and other domains.
    
[^82]: 探测和减少词嵌入中的间接刻板印象

    Detecting and Mitigating Indirect Stereotypes in Word Embeddings. (arXiv:2305.14574v1 [cs.CL])

    [http://arxiv.org/abs/2305.14574](http://arxiv.org/abs/2305.14574)

    本文提出了一种新方法，称为有偏间接关系修改（BIRM），以减轻分布式词嵌入中的间接偏见，通过考虑标记偏见属性的单词在存在情况下给定一对单词的共现概率如何变化，并利用这一点平均偏见属性的影响。

    

    常见的词嵌入方法会学习到在使用单词时存在的社会偏见和有害刻板印象。这些偏见不仅存在于词本身和其明确的刻板印象标记之间，而且还存在于共享相关刻板印象的词之间。这种称为“间接偏见”的现象已经阻碍了之前的试图消除这些偏见的尝试。本文提出了一种称为“有偏间接关系修改（BIRM）”的新方法，在嵌入词之前通过修改单词之间的有偏关系来减轻分布式词嵌入中的间接偏见。方法是通过考虑在标记偏见属性的单词存在的情况下给定一对单词的共现概率如何变化，并利用这一点平均偏见属性的影响。为了评估这种方法，我们进行了一系列常见的测试，并证明了在稍微减少语义方面的同时，减少了词嵌入中的偏见测量。

    Societal biases in the usage of words, including harmful stereotypes, are frequently learned by common word embedding methods. These biases manifest not only between a word and an explicit marker of its stereotype, but also between words that share related stereotypes. This latter phenomenon, sometimes called "indirect bias,'' has resisted prior attempts at debiasing. In this paper, we propose a novel method called Biased Indirect Relationship Modification (BIRM) to mitigate indirect bias in distributional word embeddings by modifying biased relationships between words before embeddings are learned. This is done by considering how the co-occurrence probability of a given pair of words changes in the presence of words marking an attribute of bias, and using this to average out the effect of a bias attribute. To evaluate this method, we perform a series of common tests and demonstrate that measures of bias in the word embeddings are reduced in exchange for minor reduction in the semantic
    
[^83]: 从字符到词：用于开放词汇语言理解的分层预训练语言模型

    From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding. (arXiv:2305.14571v1 [cs.CL])

    [http://arxiv.org/abs/2305.14571](http://arxiv.org/abs/2305.14571)

    本文提出了一种新颖的开放词汇语言模型，它采用了分层两级方法和浅层Transformer体系结构从字符中学习单词，能够提高模型的容忍度和适应性。

    

    当前自然语言理解的最新模型需要对原始文本进行预处理，将其转换为离散标记。这个过程称为分词，依赖于预先构建的单词或子词。但这种固定词汇表限制了模型对拼写错误的容忍度和适应新领域的能力。本文提出了一种新颖的开放词汇语言模型，采用了分层的两级方法：一个在词级别，另一个在序列级别。具体来说，我们设计了一个内部单词模块，使用浅层Transformer体系结构从其字符中学习单词表示，并另一个深层Transformer模块，将每个单词表示置于整个单词序列中。因此，我们的模型直接在具有显式单词边界意识的字符序列上运行，但没有偏见的子单词或单词级词汇表。各种下游任务的实验表明我们的模型比其他模型有更好的性能。

    Current state-of-the-art models for natural language understanding require a preprocessing step to convert raw text into discrete tokens. This process known as tokenization relies on a pre-built vocabulary of words or sub-word morphemes. This fixed vocabulary limits the model's robustness to spelling errors and its capacity to adapt to new domains. In this work, we introduce a novel open-vocabulary language model that adopts a hierarchical two-level approach: one at the word level and another at the sequence level. Concretely, we design an intra-word module that uses a shallow Transformer architecture to learn word representations from their characters, and a deep inter-word Transformer module that contextualizes each word representation by attending to the entire word sequence. Our model thus directly operates on character sequences with explicit awareness of word boundaries, but without biased sub-word or word-level vocabulary. Experiments on various downstream tasks show that our me
    
[^84]: 少样本统一问答：调整模型还是提示？

    Few-shot Unified Question Answering: Tuning Models or Prompts?. (arXiv:2305.14569v1 [cs.CL])

    [http://arxiv.org/abs/2305.14569](http://arxiv.org/abs/2305.14569)

    本文探讨了调整模型和提示两种范式在低资源情况下用于统一QA的潜力，研究发现在良好的初始化条件下，提示调整可以在少样本情况下的表现与模型调整相当，通过参数共享和简单的提示初始化知识转移技术，提示调整在低资源情况下可以实现显著的性能提升。

    

    问答（QA）任务通常研究特定的问题类型、知识领域或推理技能，导致专门针对特定类别的QA任务的模型。虽然最近的研究探讨了统一QA模型的想法，但这些模型通常只在高资源情况下进行探索，并需要重新训练以扩展其能力。本文探讨了调整模型和提示两种范式在低资源情况下用于统一QA的潜力，采用16个QA数据集进行了详尽的应用分析，发现在良好的初始化条件下，提示调整可以在少样本情况下的表现与模型调整相当。研究还显示参数共享导致优秀的少样本性能，简单的提示初始化知识转移技术可以有效，提示调整在低资源范围内通过预训练实现了显著的性能提升。

    Question-answering (QA) tasks often investigate specific question types, knowledge domains, or reasoning skills, leading to specialized models catering to specific categories of QA tasks. While recent research has explored the idea of unified QA models, such models are usually explored for high-resource scenarios and require re-training to extend their capabilities. To overcome these drawbacks, the paper explores the potential of two paradigms of tuning, model, and prompts, for unified QA under a low-resource setting. The paper provides an exhaustive analysis of their applicability using 16 QA datasets, revealing that prompt tuning can perform as well as model tuning in a few-shot setting with a good initialization. The study also shows that parameter-sharing results in superior few-shot performance, simple knowledge transfer techniques for prompt initialization can be effective, and prompt tuning achieves a significant performance boost from pre-training in a low-resource regime. The 
    
[^85]: PEARL：通过对长文档的规划和执行行动来指导大型语言模型进行推理

    PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents. (arXiv:2305.14564v1 [cs.CL])

    [http://arxiv.org/abs/2305.14564](http://arxiv.org/abs/2305.14564)

    PEARL是一种提示框架，用于改善大型语言模型对长文档的推理。 它包括三个阶段：行动挖掘，计划制定和计划执行。 PEARL将问题分解成一系列动作并在文档上执行以获得答案。

    

    在本文中，我们提出了一个叫做PEARL的提示框架，以改善长文档推理，它由三个阶段组成：行动挖掘、计划制定和计划执行。具体来说，给定一个有关长文档的问题，PEARL将问题分解成一系列动作（例如，SUMMARIZE、FIND_EVENT、FIND_RELATION），然后在文档上执行它们以获得答案。PEARL的每个阶段都是通过最少人工输入的零射或少射提示LLM（在我们的工作中是GPT-4）来实现的。

    Strategies such as chain-of-thought prompting improve the performance of large language models (LLMs) on complex reasoning tasks by decomposing input examples into intermediate steps. However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain. In this work, we propose PEARL, a prompting framework to improve reasoning over long documents, which consists of three stages: action mining, plan formulation, and plan execution. More specifically, given a question about a long document, PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE, FIND_EVENT, FIND_RELATION) and then executes them over the document to obtain the answer. Each stage of PEARL is implemented via zero-shot or few-shot prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate PEARL on a challenging subset of the QuALITY dataset, which contains questions tha
    
[^86]: 揭示ChatGPT: AI-生成面向目标的对话和注释的关键分析

    Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations. (arXiv:2305.14556v1 [cs.CL])

    [http://arxiv.org/abs/2305.14556](http://arxiv.org/abs/2305.14556)

    本文基于ChatGPT模型，研究了AI生成面向目标的对话和注释的潜力，实验结果表明生成的对话和注释质量与人类生成的相当。

    

    大型预训练语言模型以提示技术为手段产生高质量文本的能力前所未有。这一事实为数据收集和注释带来了新的可能性，特别是在这种数据稀缺、难以收集、昂贵甚至敏感的情况下。在本文中，我们探索了这些模型生成和注释面向目标的对话的潜力，并进行了深入的分析以评估它们的质量。我们的实验采用ChatGPT，并包括三类面向目标的对话（任务导向、协作和说明性）、两种生成模式（交互式和一次性）和两种语言（英语和意大利语）。基于广泛的人工评估，我们证明了生成的对话和注释的质量与人类生成的相当。

    Large pre-trained language models have exhibited unprecedented capabilities in producing high-quality text via prompting techniques. This fact introduces new possibilities for data collection and annotation, particularly in situations where such data is scarce, complex to gather, expensive, or even sensitive. In this paper, we explore the potential of these models to generate and annotate goal-oriented dialogues, and conduct an in-depth analysis to evaluate their quality. Our experiments employ ChatGPT, and encompass three categories of goal-oriented dialogues (task-oriented, collaborative, and explanatory), two generation modes (interactive and one-shot), and two languages (English and Italian). Based on extensive human-based evaluations, we demonstrate that the quality of generated dialogues and annotations is on par with those generated by humans.
    
[^87]: 所有道路通往罗马？探究Transformer表示的不变性。

    All Roads Lead to Rome? Exploring the Invariance of Transformers' Representations. (arXiv:2305.14555v1 [cs.CL])

    [http://arxiv.org/abs/2305.14555](http://arxiv.org/abs/2305.14555)

    本文探究了Transformer模型表示空间的可靠性问题，提出了双射假设，并提出了一种基于可逆神经网络的模型BERT-INN，来更有效地学习双射，实验结果显示其优势。

    

    Transformer模型为各种自然语言处理任务带来了极大的进展，因此引发了对模型学习的表示的可解释性研究。本文提出了一个基本问题，即模型表示的可靠性问题。我们探究Transformer是否学习到了本质上同构的表示空间，或者这些表示空间是否对其预训练过程中的随机种子敏感。我们提出了双射假设，并提出了一种基于可逆神经网络的模型BERT-INN，来比其他现有双射方法（如规范相关分析（CCA））更有效地学习双射。我们通过理论和大量实验展示了BERT-INN的优势，并将其应用于对齐重现的BERT嵌入，以获得对可解释性研究有意义的见解。我们的代码链接在文章中。

    Transformer models bring propelling advances in various NLP tasks, thus inducing lots of interpretability research on the learned representations of the models. However, we raise a fundamental question regarding the reliability of the representations. Specifically, we investigate whether transformers learn essentially isomorphic representation spaces, or those that are sensitive to the random seeds in their pretraining process. In this work, we formulate the Bijection Hypothesis, which suggests the use of bijective methods to align different models' representation spaces. We propose a model based on invertible neural networks, BERT-INN, to learn the bijection more effectively than other existing bijective methods such as the canonical correlation analysis (CCA). We show the advantage of BERT-INN both theoretically and through extensive experiments, and apply it to align the reproduced BERT embeddings to draw insights that are meaningful to the interpretability research. Our code is at 
    
[^88]: 文本摘要中易于解释的细粒度不一致性检测

    Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization. (arXiv:2305.14548v1 [cs.CL])

    [http://arxiv.org/abs/2305.14548](http://arxiv.org/abs/2305.14548)

    本文提出了一种易于解释的细粒度不一致性检测模型FineGrainFact，通过语义角色标注显式地表示文档和摘要中的事实，并强调相关的语义框架来预测不一致性，实验表明该模型优于强基线。

    

    现有的文本摘要中事实一致性评估方法提供了二元预测和有限的摘要系统弱点洞见。因此，我们提出了细粒度不一致性检测任务，其目标是预测摘要中细粒度的事实错误类型。受人类检查摘要中事实不一致性方法的启示，我们提出了FineGrainFact，一种易于解释的细粒度不一致性检测模型，该模型通过语义角色标注显式地表示文档和摘要中的事实，并强调相关的语义框架来预测不一致性。这些语义框架有助于验证预测的错误类型并修正不一致的摘要。实验结果表明，我们的模型优于强基线，并提供了支持或反驳摘要的证据。

    Existing factual consistency evaluation approaches for text summarization provide binary predictions and limited insights into the weakness of summarization systems. Therefore, we propose the task of fine-grained inconsistency detection, the goal of which is to predict the fine-grained types of factual errors in a summary. Motivated by how humans inspect factual inconsistency in summaries, we propose an interpretable fine-grained inconsistency detection model, FineGrainFact, which explicitly represents the facts in the documents and summaries with semantic frames extracted by semantic role labeling, and highlights the related semantic frames to predict inconsistency. The highlighted semantic frames help verify predicted error types and correct inconsistent summaries. Experiment results demonstrate that our model outperforms strong baselines and provides evidence to support or refute the summary.
    
[^89]: 基于Whisper的表示在各种交叉任务下的可迁移性研究

    On the Transferability of Whisper-based Representations for "In-the-Wild" Cross-Task Downstream Speech Applications. (arXiv:2305.14546v1 [eess.AS])

    [http://arxiv.org/abs/2305.14546](http://arxiv.org/abs/2305.14546)

    Whisper模型是一个基于变压器的模型，可用于各种语音任务。本篇文章探讨了Whisper表示在其他四种语音任务和“野外”任务中的可迁移性和鲁棒性。实验结果表明，Whisper具有跨任务的真实环境部署潜力。

    

    大型自监督预训练的语音模型在各种语音处理任务中取得了显着的成功。这些模型的自监督训练导致了可用于不同下游任务的通用语音表示，从自动语音识别(ASR)到说话人识别等任务。最近，提出了一个基于变压器的模型Whisper，通过大量的弱监督数据进行了ASR的训练； 它优于几种现有的最先进的自监督模型。鉴于Whisper在ASR方面的优越性，在这篇论文中，我们探讨了该表示在SUPERB基准测试中其他四个语音任务的可迁移性。此外，我们还探讨了Whisper表示在被环境噪声和房间混响破坏的“野外”任务中的鲁棒性。实验结果表明，Whisper在各种任务和环境条件下都取得了有希望的结果，因此展示了在跨任务的真实环境部署方面的潜力。

    Large self-supervised pre-trained speech models have achieved remarkable success across various speech-processing tasks. The self-supervised training of these models leads to universal speech representations that can be used for different downstream tasks, ranging from automatic speech recognition (ASR) to speaker identification. Recently, Whisper, a transformer-based model was proposed and trained on large amount of weakly supervised data for ASR; it outperformed several state-of-the-art self-supervised models. Given the superiority of Whisper for ASR, in this paper we explore the transferability of the representation for four other speech tasks in SUPERB benchmark. Moreover, we explore the robustness of Whisper representation for ``in the wild'' tasks where speech is corrupted by environment noise and room reverberation. Experimental results show Whisper achieves promising results across tasks and environmental conditions, thus showing potential for cross-task real-world deployment.
    
[^90]: 基于事实的推理：LLMs的洞见与超越

    LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond. (arXiv:2305.14540v1 [cs.CL])

    [http://arxiv.org/abs/2305.14540](http://arxiv.org/abs/2305.14540)

    LLMs能够在基本的实际不一致性检测基准测试中表现出竞争力，但大多数LLMs不能成功完成更复杂任务方案的测试。作者提出了一个新的非一致性检测基准测试SummEdits，并表明大多数LLMs并不擅长完成此项测试。

    

    随着LLMs在实际场景中的出现，拥有能够有效检测实际不一致性的方法对于减少错误信息传播并提高模型输出的信任度至关重要。通过对现有的实际一致性基准进行测试，我们发现与传统的非LLM方法相比，少数大型语言模型在事实不一致性检测分类基准方面表现竞争力。然而，更详细的分析揭示了大多数LLMs在更复杂的任务方案上失败，并揭示了现有评估基准存在的问题，影响了评估精度。为了解决这个问题，我们提出了一个新的不一致性检测基准协议，并在一个10个领域的基准测试SummEdits中实施。这个新基准测试每个样本比之前的基准测试更具有20倍的成本效益，并且高度可重复，因为我们估计人与人之间的一致性约为0.9。大多数LLMs在SummEdits上都表现不佳，其性能……（原文未完，略）

    With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals that most LLMs fail on more complex formulations of the task and exposes issues with existing evaluation benchmarks, affecting evaluation precision. To address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called SummEdits. This new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with performance 
    
[^91]: 级联束搜索：用于神经机器翻译的即插即用术语约束方法

    Cascaded Beam Search: Plug-and-Play Terminology-Forcing For Neural Machine Translation. (arXiv:2305.14538v1 [cs.CL])

    [http://arxiv.org/abs/2305.14538](http://arxiv.org/abs/2305.14538)

    本文提出了一种级联束搜索算法，用于神经机器翻译中的术语约束，可以即插即用，无需重新训练。该方法可将目标术语的概率增加，并使用基于网格束搜索的级联束设置，性能良好。

    

    本文提出了一种适用于具有术语约束的翻译的即插即用方法。术语约束是许多现代翻译流程的重要方面，在专业领域和新兴领域（如COVID-19疫情）中，准确翻译专业术语至关重要。最近的方法通常训练模型从输入中复制术语并将其馈送到目标语句中。但这需要昂贵的培训，每当基础语言模型发生变化或系统需要专门化到新领域时都需要重新训练。我们提出级联束搜索，一种即插即用的术语强制方法，不需要训练。这种级联束搜索有两个部分：1）逻辑操作，增加目标术语的概率；2）基于网格束搜索的级联束设置，其中根据它们包含的术语数将束组合。我们通过竞争方式评估了我们的方法的性能。

    This paper presents a plug-and-play approach for translation with terminology constraints. Terminology constraints are an important aspect of many modern translation pipelines. In both specialized domains and newly emerging domains (such as the COVID-19 pandemic), accurate translation of technical terms is crucial. Recent approaches often train models to copy terminologies from the input into the output sentence by feeding the target terminology along with the input. But this requires expensive training whenever the underlying language model is changed or the system should specialize to a new domain. We propose Cascade Beam Search, a plug-and-play terminology-forcing approach that requires no training. Cascade Beam Search has two parts: 1) logit manipulation to increase the probability of target terminologies and 2) a cascading beam setup based on grid beam search, where beams are grouped by the number of terminologies they contain. We evaluate the performance of our approach by compet
    
[^92]: MathDial: 一个以数学推理问题为基础的富含教育性属性的对话辅导数据集。

    MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems. (arXiv:2305.14536v1 [cs.CL])

    [http://arxiv.org/abs/2305.14536](http://arxiv.org/abs/2305.14536)

    MathDial是一个由实际教师和大型语言模型半合成生成的对话数据集，旨在解决自动对话辅导工具缺少高质量数据的问题。这个数据集关注通过引导学生使用问题来探索数学问题。

    

    尽管自动对话辅导工具在个性化和提高教育可及性方面具有巨大潜力，但由于缺乏足够大规模且高质量的数据集，这类系统的研究一直受到阻碍。然而，收集此类数据集仍然具有挑战性，因为录制辅导会引发隐私问题，而众包则会导致数据质量不足。为了解决这个问题，我们提出了一个框架，通过将真正的老师与大型语言模型(LLM)配对，构建常见学生错误的支架，来半合成地生成这些对话。在本文中，我们描述了使用这个框架来收集MathDial的最新工作，这是一个当前拥有约1.5k个基于多步数学词问题的辅导对话数据集。我们展示了我们的数据集展现出丰富的教育性属性，主要关注通过引导学生使用问题来探索问题。此外，我们概述了MathDial及其地面注释可用于微调...

    Although automatic dialogue tutors hold great potential in making education personalized and more accessible, research on such systems has been hampered by a lack of sufficiently large and high-quality datasets. However, collecting such datasets remains challenging, as recording tutoring sessions raises privacy concerns and crowdsourcing leads to insufficient data quality. To address this problem, we propose a framework to semi-synthetically generate such dialogues by pairing real teachers with a large language model (LLM) scaffolded to represent common student errors. In this paper, we describe our ongoing efforts to use this framework to collect MathDial, a dataset of currently ca. 1.5k tutoring dialogues grounded in multi-step math word problems. We show that our dataset exhibits rich pedagogical properties, focusing on guiding students using sense-making questions to let them explore problems. Moreover, we outline that MathDial and its grounding annotations can be used to finetune 
    
[^93]: 在混合语言的社交媒体文本中检测宣传技术

    Detecting Propaganda Techniques in Code-Switched Social Media Text. (arXiv:2305.14534v1 [cs.CL])

    [http://arxiv.org/abs/2305.14534](http://arxiv.org/abs/2305.14534)

    本文提出了一个新任务，即在混合语言的社交媒体文本中检测宣传技术。为了支持这一任务，作者创建了一个包含1030个文本的英语和罗马乌尔混合语言的语料库，并进行了一系列实验。

    

    宣传是一种旨在影响公众舆论和心态以推广特定议程的沟通形式。随着社交媒体的崛起，宣传已经迅速传播，引发了对自动宣传检测系统的需求。大多数宣传检测工作都集中在高资源语言（如英语）上，几乎没有为低资源语言检测宣传做出努力。然而，在社交媒体交流中发现多种语言的混合现象是很常见的，这被称为码混。码混在同一文本中结合了不同的语言，这对于自动系统构成了挑战。考虑到这一点，我们在此提出了检测混合文本中宣传技术的新任务。为了支持这个任务，我们创建了一个包含1030个文本的语料库，这些文本在英语和罗马乌尔都进行了混合，并用20种宣传技巧进行了注释，我们已经公开了这个语料库。我们进行了一系列实验来对比不同的模型和特征集合在此任务上的表现。

    Propaganda is a form of communication intended to influence the opinions and the mindset of the public to promote a particular agenda. With the rise of social media, propaganda has spread rapidly, leading to the need for automatic propaganda detection systems. Most work on propaganda detection has focused on high-resource languages, such as English, and little effort has been made to detect propaganda for low-resource languages. Yet, it is common to find a mix of multiple languages in social media communication, a phenomenon known as code-switching. Code-switching combines different languages within the same text, which poses a challenge for automatic systems. With this in mind, here we propose the novel task of detecting propaganda techniques in code-switched text. To support this task, we create a corpus of 1,030 texts code-switching between English and Roman Urdu, annotated with 20 propaganda techniques, which we make publicly available. We perform a number of experiments contrastin
    
[^94]: 如何选择您的聊天机器人：用于对话指标评估的大规模多系统多参考数据集

    How to Choose How to Choose Your Chatbot: A Massively Multi-System MultiReference Data Set for Dialog Metric Evaluation. (arXiv:2305.14533v1 [cs.CL])

    [http://arxiv.org/abs/2305.14533](http://arxiv.org/abs/2305.14533)

    该研究发布了MMSMR数据集，该数据集包含8个参考对话，旨在促进对话度量和评估的未来工作。该研究使用1750个系统对其进行了评估，以了解稳健相关性并了解测试集中所需的内容。

    

    我们发布了MMSMR，这是一个大规模多系统多参考数据集，旨在促进对话的度量和评估的未来工作。用于对话评估的自动指标应该是人类判断的可靠代理；然而，目前对其稳健性的验证还远远不够令人满意。为了量化稳健性相关性并了解测试集中所需的内容，我们扩展了单参考评估集，推出了一个包含8个参考对话的数据集，并介绍了这个新的语言学习对话数据集。然后我们训练了1750个系统，并在我们的新测试集和DailyDialog数据集上对它们进行了评估。我们发布了这个新的测试集，以及每个系统在各种数据集上的模型超参数、推理输出和指标分数。

    We release MMSMR, a Massively Multi-System MultiReference dataset to enable future work on metrics and evaluation for dialog. Automatic metrics for dialogue evaluation should be robust proxies for human judgments; however, the verification of robustness is currently far from satisfactory. To quantify the robustness correlation and understand what is necessary in a test set, we create and release an 8-reference dialog dataset by extending single-reference evaluation sets and introduce this new language learning conversation dataset. We then train 1750 systems and evaluate them on our novel test set and the DailyDialog dataset. We release the novel test set, and model hyper parameters, inference outputs, and metric scores for each system on a variety of datasets.
    
[^95]: 通过数据混合消除预训练模型中的虚假相关性

    Eliminating Spurious Correlations from Pre-trained Models via Data Mixing. (arXiv:2305.14521v1 [cs.LG])

    [http://arxiv.org/abs/2305.14521](http://arxiv.org/abs/2305.14521)

    本文提出了一种通过数据混合来消除预训练模型中虚假相关性的方法，来提高模型对于新样本的预测能力。这种方法经过理论证明和多种任务实验验证，可以取得良好的效果。

    

    在大数据集上预训练的机器学习模型取得了显著的收敛性和鲁棒性。然而，这些模型往往利用了某些属性和标签之间的虚假相关性，在特定类别的大多数示例中普遍存在，但并不足以预测这些类别。学到的虚假相关性可能会在对新数据进行微调后仍然存在，这会降低模型对不展现虚假相关性的示例的性能。本文提出了一种简单而高效的方法，以消除预训练模型中的虚假相关性。我们方法的关键思想是利用一小组带有虚假属性的示例，并通过数据混合来平衡所有类别中的虚假属性。我们在理论上证实了我们的方法的有效性，并在各种视觉和NLP任务上进行了实证，包括消除虚假相关性。

    Machine learning models pre-trained on large datasets have achieved remarkable convergence and robustness properties. However, these models often exploit spurious correlations between certain attributes and labels, which are prevalent in the majority of examples within specific categories but are not predictive of these categories in general. The learned spurious correlations may persist even after fine-tuning on new data, which degrades models' performance on examples that do not exhibit the spurious correlation. In this work, we propose a simple and highly effective method to eliminate spurious correlations from pre-trained models. The key idea of our method is to leverage a small set of examples with spurious attributes, and balance the spurious attributes across all classes via data mixing. We theoretically confirm the effectiveness of our method, and empirically demonstrate its state-of-the-art performance on various vision and NLP tasks, including eliminating spurious correlation
    
[^96]: 扰动证据下的推理：探究大型语言模型的学生模拟能力

    Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models. (arXiv:2305.14507v1 [cs.CL])

    [http://arxiv.org/abs/2305.14507](http://arxiv.org/abs/2305.14507)

    本文探究了大型语言模型（LLM）是否能够在扰动证据下做出逻辑推理的结论，结果发现即使是最先进的GPT模型在处理操纵证据下的推理时也存在困难。

    

    本文探究了大型语言模型（LLM）是否能够处理伴有扭曲事实的逻辑推理，即扰动证据下的推理（DUPE）。由于LLM通常依赖于编码了大部分准确信息的参数进行推理和推断，DUPE对LLM提出了独特的挑战，因为在DUPE中，LLM必须在提示中存在的被操纵或伪造的证据上进行推理，这可能导致仅在被操纵证据下才有效的错误结论。我们的目标是通过DUPE确定LLM是否能够得出这些错误的结论，并确定影响推理过程的主要因素是参数中编码的数据还是提示中的被操纵证据。为了评估LLM的DUPE能力，我们创建了一个DUPE版本的StrategyQA数据集，其中事实被操纵以扭转问题的答案。我们的研究发现，即使是最先进的GPT模型在处理操纵证据下的推理时也存在困难。

    We explore whether Large Language Models (LLMs) are capable of logical reasoning with distorted facts, which we call Deduction under Perturbed Evidence (DUPE). DUPE presents a unique challenge to LLMs since they typically rely on their parameters, which encode mostly accurate information, to reason and make inferences. However, in DUPE, LLMs must reason over manipulated or falsified evidence present in their prompts, which can result in false conclusions that are valid only under the manipulated evidence. Our goal with DUPE is to determine whether LLMs can arrive at these false conclusions and identify whether the dominant factor influencing the deduction process is the encoded data in the parameters or the manipulated evidence in the prompts. To evaluate the DUPE capabilities of LLMs, we create a DUPEd version of the StrategyQA dataset, where facts are manipulated to reverse the answer to the question. Our findings show that even the most advanced GPT models struggle to reason on mani
    
[^97]: 用强化学习实现的顺序检索上下文示例的RetICL

    RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning. (arXiv:2305.14502v1 [cs.CL])

    [http://arxiv.org/abs/2305.14502](http://arxiv.org/abs/2305.14502)

    本文提出了一种 RetICL 的方法来优化地选择用于上下文学习模型的示例。此方法利用强化学习框架将序列示例选择问题作为马尔科夫决策过程，并且优化选择为使任务表现最佳的组合。

    

    最近在大语言模型领域中的许多发展都集中在促使它们执行特定任务。一种有效的提示方法是上下文学习，其中模型在给定一个（或多个）示例的情况下执行（可能是新的）生成/预测任务。先前的工作表明，示例的选择可能对任务的表现产生很大的影响。然而，找到好的示例并不是简单的，因为代表性示例组的定义可以根据任务的不同而大不相同。虽然存在许多选择上下文示例的现有方法，但它们通常独立地对示例进行评分，忽略它们之间的依赖关系以及向大型语言模型提供示例的顺序。在这项工作中，我们提出了一种可学习的方法——In-Context Learning的检索RetICL，用于建模和逐步选择上下文示例。我们把顺序示例选择的问题作为马尔科夫决策过程，设计了一个示例。

    Many recent developments in large language models focus on prompting them to perform specific tasks. One effective prompting method is in-context learning, where the model performs a (possibly new) generation/prediction task given one (or more) examples. Past work has shown that the choice of examples can make a large impact on task performance. However, finding good examples is not straightforward since the definition of a representative group of examples can vary greatly depending on the task. While there are many existing methods for selecting in-context examples, they generally score examples independently, ignoring the dependency between them and the order in which they are provided to the large language model. In this work, we propose Retrieval for In-Context Learning (RetICL), a learnable method for modeling and optimally selecting examples sequentially for in-context learning. We frame the problem of sequential example selection as a Markov decision process, design an example r
    
[^98]: NAIL: 带高效非自回归解码器的词汇检索指数

    NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders. (arXiv:2305.14499v1 [cs.CL])

    [http://arxiv.org/abs/2305.14499](http://arxiv.org/abs/2305.14499)

    NAIL是一种带有高效非自回归解码器的词汇检索指数模型，可与现有的预训练模型兼容，并且使用商品CPU提供服务。它可以捕捉Transformer交叉关注模型收益高达86％的方法，与BM25检索器结合使用匹配当前最先进的双编码器检索器的质量。

    

    神经文档重新排名器在精度方面非常有效。然而，最好的模型需要专用硬件进行服务，这是昂贵并且通常是不可行的。为了避免这种服务时间要求，我们提出一种捕捉Transformer交叉关注模型收益高达86％的方法，该方法使用只需要每个文档转换器FLOP的10-6％的词汇得分功能，并且可以使用商品CPU提供服务。当与BM25检索器结合使用时，此方法可以匹配现有的最先进的双编码器检索器的质量，该检索器仍需要加速器进行查询编码。我们将NAIL（带有语言模型的非自回归索引）引入为与最近的编码器-解码器和仅解码器大型语言模型（例如T5、GPT-3和PaLM）兼容的模型体系结构。该模型体系结构可以利用现有的预训练检查点，并可以微调以有效地构建不需要n

    Neural document rerankers are extremely effective in terms of accuracy. However, the best models require dedicated hardware for serving, which is costly and often not feasible. To avoid this serving-time requirement, we present a method of capturing up to 86% of the gains of a Transformer cross-attention model with a lexicalized scoring function that only requires 10-6% of the Transformer's FLOPs per document and can be served using commodity CPUs. When combined with a BM25 retriever, this approach matches the quality of a state-of-the art dual encoder retriever, that still requires an accelerator for query encoding. We introduce NAIL (Non-Autoregressive Indexing with Language models) as a model architecture that is compatible with recent encoder-decoder and decoder-only large language models, such as T5, GPT-3 and PaLM. This model architecture can leverage existing pre-trained checkpoints and can be fine-tuned for efficiently constructing document representations that do not require n
    
[^99]: 自我精磨：通过问题精化增强大型语言模型的推理能力

    Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement. (arXiv:2305.14497v1 [cs.CL])

    [http://arxiv.org/abs/2305.14497](http://arxiv.org/abs/2305.14497)

    Self-Polish是一种方法，通过促使模型逐步完善给定的问题以使其更易理解和可解决，以增强大型语言模型的推理能力。

    

    利用 Chain-of-Thought（CoT）等提示方法可以增强大型语言模型的推理能力，研究人员广泛探索了合理化和答案生成过程。然而，他们忽略了低质量推理问题可能会显着影响推理性能的潜在挑战。本文提出了Self-Polish（SP）方法，通过促使模型逐步完善给定的问题以使其更易理解和可解决，以促进模型的问题解决过程。具体而言，该方法教授模型消除无关信息，重新排列逻辑结构，并将局部条件并行组织成新的条件。 SP与所有其他提示方法正交，方便将其与最先进的技术集成以进一步提高性能。我们在五个基准测试中进行了彻底的实验以证明其有效性。

    Prompting methods such as Chain-of-Thought (CoT) have shed new light on enhancing the reasoning capabilities of large language models, and researchers have extensively explored the generation process of rationales and answers. However, they have overlooked the potential challenges posed by the poor quality of reasoning problems, which may influence the reasoning performance significantly. In this work, we propose Self-Polish (SP), a novel method that facilitates the model's problem-solving process by prompting them to progressively refine the given problems to be more comprehensible and solvable. Specifically, the method teaches models to eliminate irrelevant information, rearrange the logic structure and organize local conditions into new ones parallelly. SP is orthogonal to all other prompting methods, making it convenient to integrate with state-of-the-art techniques for further improvement. We conduct thorough experiments on five benchmarks to illustrate the effectiveness of the pr
    
[^100]: 少样本和零样本NLU任务中提示位置确实很重要

    Prompt position really matters in few-shot and zero-shot NLU tasks. (arXiv:2305.14493v1 [cs.CL])

    [http://arxiv.org/abs/2305.14493](http://arxiv.org/abs/2305.14493)

    该论文通过实证研究发现，提示位置对于少样本和零样本任务的模型性能具有实质性影响，先前研究中使用的提示位置通常是次优的，提示位置优化应成为重要的研究方向。

    

    基于提示的模型在零样本和少样本学习领域取得了显著进展，吸引了众多研究者的关注。但是，有效提示模板的开发起着至关重要的作用。然而，先前的研究主要集中在提示词汇选择或保留提示位置的嵌入初始化方面。在这项实证研究中，我们对自然语言理解任务的提示位置选项进行了迄今为止最全面的分析。我们的发现量化了提示位置对模型性能的实质性影响。我们观察到，先前研究中使用的提示位置对于零样本和少样本设置通常是次优的。这些发现表明，提示位置优化是一个有趣的研究方向，与现有的提示工程重心并列。

    Prompt-based models have made remarkable advancements in the fields of zero-shot and few-shot learning, attracting a lot of attention from researchers. Developing an effective prompt template plays a critical role. However, prior studies have mainly focused on prompt vocabulary selection or embedding initialization with the reserved prompt position fixed. In this empirical study, we conduct the most comprehensive analysis to date of prompt position option for natural language understanding tasks. Our findings quantify the substantial impact prompt position has on model performance. We observe that the prompt position used in prior studies is often sub-optimal for both zero-shot and few-shot settings. These findings suggest prompt position optimisation as an interesting research direction alongside the existing focus on prompt engineering.
    
[^101]: 通过情境对齐和可解释文本蕴含来比较社会文化规范的异同：以中国和美国文化为例

    Sociocultural Norm Similarities and Differences via Situational Alignment and Explainable Textual Entailment. (arXiv:2305.14492v1 [cs.CL])

    [http://arxiv.org/abs/2305.14492](http://arxiv.org/abs/2305.14492)

    本论文提出了一种新方法来比较中美文化的社会规范，通过情境对齐和上下文学习提取社会规范。作者构建了一个高质量的数据集，通过测试发现该方法具有研究社会文化异同的潜力。

    

    跨文化推理需要建立在相应文化背景下的准确规范基础之上。 然而，当前的社会规范计算模型主要关注美国社会。 本文提出了一种新的方法来发现和比较中国和美国文化的社会规范。 我们通过利用一个中文问答平台 - 知乎 - 和现有的SocialChemistry数据集作为不同文化维度的代理，通过情境对齐跨文化比较社会情境，使用上下文学习从文本中提取社会规范。我们将Chain-of-Thought提示嵌入到人工智能协作框架中，构建了一个高质量的数据集，其中包含3,069个跨中国和美国文化对齐的社会规范以及相应的自由文本解释。为了测试模型在跨文化社会规范方面的推理能力，我们提出了社会规范的可解释文本蕴含任务，其目标是预测一个文化中的社会规范的解释是否暗示另一个文化中的类似规范。我们使用这个任务来评估我们的方法，并发现我们的数据集提供了研究社会文化的异同的丰富机会。

    Designing systems that can reason across cultures requires that they are grounded in the norms of the contexts in which they operate. However, current research on developing computational models of social norms has primarily focused on American society. Here, we propose a novel approach to discover and compare descriptive social norms across Chinese and American cultures. We demonstrate our approach by leveraging discussions on a Chinese Q&A platform-Zhihu-and the existing SocialChemistry dataset as proxies for contrasting cultural axes, align social situations cross-culturally, and extract social norms from texts using in-context learning. Embedding Chain-of-Thought prompting in a human-AI collaborative framework, we build a high-quality dataset of 3,069 social norms aligned with social situations across Chinese and American cultures alongside corresponding free-text explanations. To test the ability of models to reason about social norms across cultures, we introduce the task of expl
    
[^102]: 大型语言模型是否具有鲁棒的零-shot共指解析能力？

    Are Large Language Models Robust Zero-shot Coreference Resolvers?. (arXiv:2305.14489v1 [cs.CL])

    [http://arxiv.org/abs/2305.14489](http://arxiv.org/abs/2305.14489)

    本文研究了零-shot共指解析技术在复杂语言环境下的应用，并证明指令调整的语言模型具有鲁棒性零 shot 普适性。

    

    最近，领域自适应的共指消解取得了进展，依靠使用目标领域的注释数据进行持续训练。同时，预训练的大型语言模型 (LMs) 在广泛的 NLP 任务中展示了强大的零和少量样本学习能力，包括代词消解。虽然这表明了共指能力的证据，但以往的研究大都使用简单的句子级别数据集 (如 Winograd Schema 挑战赛) 研究这种能力。在这项工作中，我们通过评估指令调整的语言模型在更加困难的、语言上复杂的共指基准测试 (如 CoNLL-2012) 上的可行性来评估零-shot学习进行共指消解的可行性。我们证明零-shot提示优于当前的无监督共指系统。进一步的研究揭示了指令调整 LMs 在广泛的领域、语言和时间段上具有强大的鲁棒性零 shot 普适性，以及对上下文和指示词的强烈依赖。

    Recent progress in domain adaptation for coreference resolution relies on continued training using annotated data from target domains. At the same time, pre-trained large language models (LMs) have exhibited strong zero- and few-shot learning abilities across a wide range of NLP tasks including pronoun resolution. While this demonstrates evidence of coreference ability, previous work has mostly studied this ability using simple sentence-level datasets such as the Winograd Schema Challenge. In this work, we assess the feasibility of zero-shot learning for coreference resolution by evaluating instruction-tuned language models on more difficult, linguistically-complex coreference benchmarks (e.g., CoNLL-2012). We demonstrate that zero-shot prompting outperforms current unsupervised coreference systems. Further investigations reveal the robust zero-shot generalization ability of instruction-tuned LMs across a wide range of domains, languages, and time periods, as well as a strong reliance 
    
[^103]: 基于强化学习反思的语言模型自我提升方法

    Language Model Self-improvement by Reinforcement Learning Contemplation. (arXiv:2305.14483v1 [cs.CL])

    [http://arxiv.org/abs/2305.14483](http://arxiv.org/abs/2305.14483)

    本文提出了一种无监督的方法 SIRLC，可以使用强化学习有效地提高语言模型的性能，而不需要使用外部标签，并且可以应用于各种NLP任务。

    

    大型语言模型在自然语言处理任务中表现出了卓越的性能。然而，对这些模型进行微调常常需要大量的监督来获取，这样很显然是耗时且昂贵的。该论文提出了一种名为语言模型强化学习反思自我提升（SIRLC）的新型无监督方法，可以在不依赖外部标签的情况下改善LLMs。该方法基于这样一个观察结果：语言模型比生成文本更容易评估文本质量。在此基础上，SIRLC给LLMs分配了双重角色，一方面作为学生生成无标签问题的答案，另一方面作为教师评估生成的文本并据此给出分数。采用强化学习来更新模型参数以最大化评估分数。我们证明了SIRLC可以应用于各种NLP任务，例如推理问题、文本分类和语言模型训练。

    Large Language Models (LLMs) have exhibited remarkable performance across various natural language processing (NLP) tasks. However, fine-tuning these models often necessitates substantial supervision, which can be expensive and time-consuming to obtain. This paper introduces a novel unsupervised method called LanguageModel Self-Improvement by Reinforcement Learning Contemplation (SIRLC) that improves LLMs without reliance on external labels. Our approach is grounded in the observation that it is simpler for language models to assess text quality than to generate text. Building on this insight, SIRLC assigns LLMs dual roles as both student and teacher. As a student, the LLM generates answers to unlabeled questions, while as a teacher, it evaluates the generated text and assigns scores accordingly. The model parameters are updated using reinforcement learning to maximize the evaluation score. We demonstrate that SIRLC can be applied to various NLP tasks, such as reasoning problems, text 
    
[^104]: 是否具有声望的工作与声望高的国家相同？多语句子嵌入和欧洲国家的案例研究

    Is a Prestigious Job the same as a Prestigious Country? A Case Study on Multilingual Sentence Embeddings and European Countries. (arXiv:2305.14482v1 [cs.CL])

    [http://arxiv.org/abs/2305.14482](http://arxiv.org/abs/2305.14482)

    本文研究了多语句子嵌入如何捕捉欧洲国家和职业，并发现嵌入中最突出的国家特征是其GPD经济实力。本研究中的大部分国家维度与职业维度不相关，但一种模型表现出职业声望和原籍国之间的联系，这是一种潜在的基于国籍的歧视。

    

    我们研究了多语句子表示如何捕捉欧洲国家，以及这种差异如何在欧洲语言之间不同。我们用模板句子提示模型，将其机器翻译成12种欧洲语言，并分析嵌入中最突出的维度。我们的分析表明，嵌入中最突出的国家特征是其GPD经济实力。当特别询问声望高低时，嵌入空间清楚地区分了声望高低的工作。三个受研究的模型中的大部分国家维度与职业维度不相关，但Distilled Multilingual Universal Sentence Encoder模型表现出职业声望和原籍国之间的联系，这是一种潜在的基于国籍的歧视。我们的发现在不同的语言中是一致的，并且在一定程度上与上述例外情况下的受研究的表示模型一致。

    We study how multilingual sentence representations capture European countries and how this differs across European languages. We prompt the models with templated sentences that we machine-translate into 12 European languages and analyze the most prominent dimensions in the embeddings. Our analysis reveals that the most prominent country feature in the embedding is its economic strength in terms of GPD. When prompted specifically for job prestige, the embedding space clearly distinguishes high and low-prestige jobs. The occupational dimension is uncorrelated with the most dominant country dimensions for three out of four studied models. One model: Distilled Multilingual Universal Sentence Encoder, however, exhibited a connection between occupational prestige and country of origin, which is a potential source of nationality-based discrimination. Our findings are consistent across languages and, to some extent, with the exception mentioned above, across studied representation models.
    
[^105]: FOCUS：基于单语言的预训练多语言模型的有效嵌入初始化方法

    FOCUS: Effective Embedding Initialization for Specializing Pretrained Multilingual Models on a Single Language. (arXiv:2305.14481v1 [cs.CL])

    [http://arxiv.org/abs/2305.14481](http://arxiv.org/abs/2305.14481)

    本文提出了FOCUS，在多语言源模型设置下，该方法使用重叠标记组合有效地初始化预训练的模型权重，提高了这种方法在适应新语言时的性能表现。

    

    在低资源语言中获得高质量的语言模型需要大量的数据和计算。使用在高资源语言上预训练的模型权重作为温启动，可以减少此需求。为了适应新语言，需要对预训练的词汇表和嵌入进行调整。在以前的工作中，针对适应后的词汇表的嵌入初始化大多聚焦于单语言源模型。本文研究了多语言源模型设置，并提出了FOCUS-快速重叠标记组合使用Sparsemax的新型嵌入初始化方法，当适应XLM-R时，它的表现优于以前的工作。FOCUS将新增的标记表示为预训练和新词汇表之间的重叠标记组合。这些重叠标记是基于辅助标记嵌入空间中的语义相似性进行选择的。我们实现的FOCUS公开在GitHub上。

    Using model weights pretrained on a high-resource language as a warm start can reduce the need for data and compute to obtain high-quality language models in low-resource languages. To accommodate the new language, the pretrained vocabulary and embeddings need to be adapted. Previous work on embedding initialization for such adapted vocabularies has mostly focused on monolingual source models. In this paper, we investigate the multilingual source model setting and propose FOCUS - Fast Overlapping Token Combinations Using Sparsemax, a novel embedding initialization method that outperforms previous work when adapting XLM-R. FOCUS represents newly added tokens as combinations of tokens in the overlap of the pretrained and new vocabularies. The overlapping tokens are selected based on semantic similarity in an auxiliary token embedding space. Our implementation of FOCUS is publicly available on GitHub.
    
[^106]: BAND: 生物医学警报新闻数据集

    BAND: Biomedical Alert News Dataset. (arXiv:2305.14480v1 [cs.CL])

    [http://arxiv.org/abs/2305.14480](http://arxiv.org/abs/2305.14480)

    该论文介绍了生物医学警报新闻数据集（BAND），其包括了1,508个样本和30个与流行病学相关的问题，为自然语言处理领域提供了新的挑战，需要更好的内容伪装能力和重要信息推断能力。该数据集可以为疾病监测和流行病学分析提供有价值的洞察力。

    

    传染性疾病的爆发对人类健康和福利构成重大威胁。为了改善疾病监测和了解疾病传播情况，已经开发出了几个监测系统来监视每日新闻警报和社交媒体。然而，由于缺乏经过良好注释的报告数据，现有系统在与相应提醒或新闻的流行病学分析方面缺乏严谨性。为了解决这一差距，我们介绍了生物医学警报新闻数据集（BAND），包括来自现有报告新闻文章、公开电子邮件和提醒的1,508个样本以及30个与流行病学相关的问题。这些问题需要模型专家的推理能力，从而为疾病爆发提供有价值的洞察力。BAND数据集为自然语言处理领域提供了新的挑战，需要更好的内容伪装能力和重要信息推断能力。我们提供了几个基准任务，包括命名实体。

    Infectious disease outbreaks continue to pose a significant threat to human health and well-being. To improve disease surveillance and understanding of disease spread, several surveillance systems have been developed to monitor daily news alerts and social media. However, existing systems lack thorough epidemiological analysis in relation to corresponding alerts or news, largely due to the scarcity of well-annotated reports data. To address this gap, we introduce the Biomedical Alert News Dataset (BAND), which includes 1,508 samples from existing reported news articles, open emails, and alerts, as well as 30 epidemiology-related questions. These questions necessitate the model's expert reasoning abilities, thereby offering valuable insights into the outbreak of the disease. The BAND dataset brings new challenges to the NLP world, requiring better disguise capability of the content and the ability to infer important information. We provide several benchmark tasks, including Named Entity
    
[^107]: CGCE：一个适用于通用和金融领域的中文生成式聊天评测基准

    CGCE: A Chinese Generative Chat Evaluation Benchmark for General and Financial Domains. (arXiv:2305.14471v1 [cs.CL])

    [http://arxiv.org/abs/2305.14471](http://arxiv.org/abs/2305.14471)

    中国的CGCE基准为通用和金融领域建立了标准化的评测框架，包含200个通用领域问题和150个金融领域特定的职业问题，旨在提升自然语言生成研究的发展。

    

    生成式聊天模型，如ChatGPT和GPT-4，通过整合指令和人类反馈来实现显著的性能改进，革新了自然语言生成技术。然而，缺乏标准化的聊天模型评测基准，特别是适用于中文和特定领域模型的评测基准，这阻碍了它们的评估和进步。为了解决这个问题，我们介绍了中国生成式聊天（CGCE）基准，专注于通用和金融领域。CGCE基准涵盖了多个任务，包括200个通用领域问题和150个金融领域的特定职业问题。手动评分考虑了准确性、连贯性、表达清晰度和完整度等因素。CGCE基准提供了一个标准化的框架，以评估和比较中文生成式聊天模型，促进了自然语言生成研究的进展。

    Generative chat models, such as ChatGPT and GPT-4, have revolutionized natural language generation (NLG) by incorporating instructions and human feedback to achieve significant performance improvements. However, the lack of standardized evaluation benchmarks for chat models, particularly for Chinese and domain-specific models, hinders their assessment and progress. To address this gap, we introduce the Chinese Generative Chat Evaluation (CGCE) benchmark, focusing on general and financial domains. The CGCE benchmark encompasses diverse tasks, including 200 questions in the general domain and 150 specific professional questions in the financial domain. Manual scoring evaluates factors such as accuracy, coherence, expression clarity, and completeness. The CGCE benchmark provides researchers with a standardized framework to assess and compare Chinese generative chat models, fostering advancements in NLG research.
    
[^108]: 像女孩一样奔跑！体育相关的语言和视觉性别偏见

    Run Like a Girl! Sports-Related Gender Bias in Language and Vision. (arXiv:2305.14468v1 [cs.CV])

    [http://arxiv.org/abs/2305.14468](http://arxiv.org/abs/2305.14468)

    本文分析了两个语言和视觉数据集中的性别偏见，不仅发现这两个数据集都存在女性的欠代表问题，而且还发现了一种命名偏见：在运动中男性更有可能被命名与他们运动相关的称呼，这种偏见对女性表征造成了损害。

    

    在语言和视觉数据集以及模型中存在的性别偏见有可能使有害的刻板印象和歧视得以持续存在。本文分析了两个语言和视觉数据集中的性别偏见。与之前的研究相符，我们发现这两个数据集都存在女性的欠代表问题，这会导致女性不可见化。此外，我们还假设并发现了人们对参与运动的人的命名选择存在一种偏见：当参与运动的是男性或男孩时，说话者更经常使用指示该运动的名字（例如“ 网球选手”或“ 冲浪者”），而当参与运动的是女性或女孩时，平均每个性别的与运动相关的名称仅有 35％ 左右，而男性则约为 46％。一个针对这些命名数据进行训练的计算模型会重现这种偏见。我们认为，数据和模型都会对女性造成表征损害。

    Gender bias in Language and Vision datasets and models has the potential to perpetuate harmful stereotypes and discrimination. We analyze gender bias in two Language and Vision datasets. Consistent with prior work, we find that both datasets underrepresent women, which promotes their invisibilization. Moreover, we hypothesize and find that a bias affects human naming choices for people playing sports: speakers produce names indicating the sport (e.g. 'tennis player' or 'surfer') more often when it is a man or a boy participating in the sport than when it is a woman or a girl, with an average of 46% vs. 35% of sports-related names for each gender. A computational model trained on these naming data reproduces the bias. We argue that both the data and the model result in representational harm against women.
    
[^109]: 通过摘要二元性和显式大纲控制增强生成

    Enhancing Generation through Summarization Duality and Explicit Outline Control. (arXiv:2305.14459v1 [cs.CL])

    [http://arxiv.org/abs/2305.14459](http://arxiv.org/abs/2305.14459)

    本文提出了一个两阶段的摘要增强的大纲监督生成框架，能够更好地生成明确和合理的大纲，并引入了一个新颖的显式大纲控制方法以更有效地利用生成的大纲。

    

    自动开放式长文本生成面临语义不连贯和情节不可信的重大挑战。先前的工作通常通过设计无监督任务中的短语或抽象信号的大纲来缓解此问题，但这往往是不稳定且难以解释的。在假设摘要作为已成熟的大纲的情况下，我们介绍了一个两阶段、摘要增强的大纲监督生成框架。该框架利用摘要任务的双重特征来改进大纲预测，从而产生更明确和合理的大纲。此外，我们发现基于大纲的生成具有未充分利用的问题，无论是标准的预训练语言模型（例如GPT-2、BART）还是大型语言模型（例如Vicuna、ChatGPT）。为了解决这个问题，我们提出了一种新颖的显式大纲控制方法，以更有效地利用生成的大纲。

    Automatically open-ended long text generation poses significant challenges due to semantic incoherence and plot implausibility. Previous works usually alleviate this problem through outlines in the form of short phrases or abstractive signals by designing unsupervised tasks, which tend to be unstable and weakly interpretable.  Assuming that a summary serves as a mature outline, we introduce a two-stage, summary-enhanced outline supervised generation framework. This framework leverages the dual characteristics of the summarization task to improve outline prediction, resulting in more explicit and plausible outlines. Furthermore, we identify an underutilization issue in outline-based generation with both standard pretrained language models (e.g., GPT-2, BART) and large language models (e.g., Vicuna, ChatGPT). To address this, we propose a novel explicit outline control method for more effective utilization of generated outlines.
    
[^110]: 在成功和失败之间跳舞：使用SALSA进行编辑级别的简化评估

    Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA. (arXiv:2305.14458v1 [cs.CL])

    [http://arxiv.org/abs/2305.14458](http://arxiv.org/abs/2305.14458)

    本研究引入了SALSA框架，对大型语言模型进行细粒度的文本简化评估，通过21种不同编辑类型，揭示了不同模型和人类文本简化的偏好和表现，并开发了LENS-SALSA指标用于自动简化度量。

    

    大型语言模型（例如GPT-3.5）可以产生高度评级的简化文本，但当前的人类评估方法未能提供对系统特定优势和劣势的清晰了解。为了解决这个限制，我们引入了SALSA，这是一个基于编辑的人类注释框架，可以实现整体和精细的文本简化评估。我们开发了21种基于语言学的编辑类型，涵盖了概念、句法和词汇简单性的所有成功和失败维度。使用SALSA，我们在700个简化案例上收集了12K个编辑注释，揭示了微调模型、少样本LLM和人类之间转化方法分布的差异，并发现GPT-3.5执行的高质量编辑比人类更多，但仍然存在频繁的错误。使用我们的精细注释，我们开发了LENS-SALSA，一种无参考自动简化度量，训练以直接从输入文本和提议的简化中预测句子和编辑级别质量分数。

    Large language models (e.g., GPT-3.5) are uniquely capable of producing highly rated text simplification, yet current human evaluation methods fail to provide a clear understanding of systems' specific strengths and weaknesses. To address this limitation, we introduce SALSA, an edit-based human annotation framework that enables holistic and fine-grained text simplification evaluation. We develop twenty one linguistically grounded edit types, covering the full spectrum of success and failure across dimensions of conceptual, syntactic and lexical simplicity. Using SALSA, we collect 12K edit annotations on 700 simplifications, revealing discrepancies in the distribution of transformation approaches performed by fine-tuned models, few-shot LLMs and humans, and finding GPT-3.5 performs more quality edits than humans, but still exhibits frequent errors. Using our fine-grained annotations, we develop LENS-SALSA, a reference-free automatic simplification metric, trained to predict sentence- an
    
[^111]: 为比较推理预训练语言模型

    Pre-training Language Models for Comparative Reasoning. (arXiv:2305.14457v1 [cs.CL])

    [http://arxiv.org/abs/2305.14457](http://arxiv.org/abs/2305.14457)

    本文提出一种预训练语言模型的新框架，旨在增强其在比较推理方面的能力。通过使用可扩展的基于文本实体比较数据的方法和新的预训练任务，该框架得到了显著的结果。

    

    本文提出了一种新框架，用于预训练语言模型以增强其在文本比较推理方面的能力。我们的方法涉及可扩展的用于收集基于文本实体比较数据的方法，并设计了三个新的预训练任务。在多个下游任务，包括比较问答、问句生成和摘要生成方面的评估表明，我们的预训练框架大大提高了语言模型的比较推理能力，尤其是在资源匮乏的情况下。此外，本工作还发布了第一个比较推理综合基准。

    In this paper, we propose a novel framework to pre-train language models for enhancing their abilities of comparative reasoning over texts. While recent research has developed models for NLP tasks that require comparative reasoning, they suffer from costly manual data labeling and limited generalizability to different tasks. Our approach involves a scalable method for collecting data for text-based entity comparison, which leverages both structured and unstructured data, and the design of three novel pre-training tasks. Evaluation on a range of downstream tasks including comparative question answering, question generation, and summarization shows that our pre-training framework significantly improves the comparative reasoning abilities of language models, especially under low-resource conditions. This work also releases the first integrated benchmark for comparative reasoning over texts.
    
[^112]: 在祈祷之后喝啤酒？测量大型语言模型中的文化偏见。

    Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. (arXiv:2305.14456v1 [cs.CL])

    [http://arxiv.org/abs/2305.14456](http://arxiv.org/abs/2305.14456)

    这篇论文研究了大型语言模型在处理和生成阿拉伯文本时出现的文化偏向西方文化的现象，表明语言模型在人名、食品、服装、地点、文学、饮料、宗教和体育等八个文化方面存在偏见。这些发现引发对于当前语言模型文化相关性的担忧。

    

    语言模型是否存在文化偏见？语言模型符合所服务社区的文化因素很重要。然而，本文表明在处理和生成阿拉伯文本时，语言模型存在显著的偏向西方文化的偏见，倾向于产生西方文化相关内容而非阿拉伯文化相关内容。我们通过使用从在线社交媒体上收集的自然出现的上下文和基于可能性评分的指标来量化这种偏见。我们的实验显示，阿拉伯语单语和多语模型在八个不同的文化方面存在西方文化偏见，包括人名、食品、服装、地点、文学、饮料、宗教和体育。当输入的阿拉伯语句子越接近英语时，模型也更容易表现出偏见。这些发现引发人们对当前语言模型文化相关性的担忧。我们的分析表明，在模型设计中应更多考虑文化因素和多样性。

    Are language models culturally biased? It is important that language models conform to the cultural aspects of the communities they serve. However, we show in this paper that language models suffer from a significant bias towards Western culture when handling and generating text in Arabic, often preferring, and producing Western-fitting content as opposed to the relevant Arab content. We quantify this bias through a likelihood scoring-based metric using naturally occurring contexts that we collect from online social media. Our experiments reveal that both Arabic monolingual and multilingual models exhibit bias towards Western culture in eight different cultural aspects: person names, food, clothing, location, literature, beverage, religion, and sports. Models also tend to exhibit more bias when prompted with Arabic sentences that are more linguistically aligned with English. These findings raise concerns about the cultural relevance of current language models. Our analyses show that pr
    
[^113]: 关于Transformer-based NLP模型的鲁棒性的研究

    On Robustness of Finetuned Transformer-based NLP Models. (arXiv:2305.14453v1 [cs.CL])

    [http://arxiv.org/abs/2305.14453](http://arxiv.org/abs/2305.14453)

    本文研究了BERT、GPT-2和T5这三种语言模型对文本扰动的鲁棒性，通过量化fine-tuning后的语言模型表示与预训练表示的差异来探究模型的变化程度。

    

    Transformer-based的预训练模型如BERT、GPT-2和T5已经被用于大量自然语言处理任务的fine-tuning，被证明非常有效。然而，在fine-tuning过程中，这些模型各层与预训练检查点相比的变化尚未得到深入研究。此外，这些模型对文本输入的扰动的鲁棒性如何？这种鲁棒性是否因模型fine-tuning的NLP任务而异？本文研究了三种语言模型（BERT、GPT-2和T5）在General Language Understanding Evaluation（GLUE）基准测试上对八种不同文本扰动的鲁棒性。此外，我们使用两个指标（CKA和STIR）来量化fine-tuning后的语言模型表示与预训练表示之间的变化。

    Transformer-based pretrained models like BERT, GPT-2 and T5 have been finetuned for a large number of natural language processing (NLP) tasks, and have been shown to be very effective. However, while finetuning, what changes across layers in these models with respect to pretrained checkpoints is under-studied. Further, how robust are these models to perturbations in input text? Does the robustness vary depending on the NLP task for which the models have been finetuned? While there exists some work on studying robustness of BERT finetuned for a few NLP tasks, there is no rigorous study which compares this robustness across encoder only, decoder only and encoder-decoder models.  In this paper, we study the robustness of three language models (BERT, GPT-2 and T5) with eight different text perturbations on the General Language Understanding Evaluation (GLUE) benchmark. Also, we use two metrics (CKA and STIR) to quantify changes between pretrained and finetuned language model representation
    
[^114]: ChatGPT是否解决了信息抽取问题？性能、度量标准、鲁棒性和错误分析

    Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors. (arXiv:2305.14450v1 [cs.CL])

    [http://arxiv.org/abs/2305.14450](http://arxiv.org/abs/2305.14450)

    本研究从性能、度量标准、鲁棒性和错误类型四个方面评估ChatGPT的信息抽取能力，发现了ChatGPT与SOTA结果之间存在巨大的性能差距，同时提出了一种软匹配策略以更准确地反映ChatGPT的性能。

    

    ChatGPT激发了大型语言模型领域的研究热潮。本文从性能、度量标准、鲁棒性和错误类型四个方面评估了ChatGPT的能力。具体而言，我们在零样本、小样本和思考串联等场景下对17个数据集的14个IE子任务评估了ChatGPT的性能，并发现ChatGPT与SOTA结果之间存在巨大的性能差距。接下来，我们重新思考这种差距，并提出一种软匹配策略以更准确地反映ChatGPT的性能。然后，我们分析了ChatGPT在14个IE子任务上的鲁棒性，并发现：1）ChatGPT很少输出无效的响应；2）不相关的上下文和长尾目标类型极大地影响了ChatGPT的性能；3）ChatGPT无法很好地理解RE任务中的主客体关系。最后，我们分析了ChatGPT的错误，并发现“未注释的跨度”是最主要的错误类型。这引起了有关现实场景下信息提取性能的担忧。

    ChatGPT has stimulated the research boom in the field of large language models. In this paper, we assess the capabilities of ChatGPT from four perspectives including Performance, Evaluation Criteria, Robustness and Error Types. Specifically, we first evaluate ChatGPT's performance on 17 datasets with 14 IE sub-tasks under the zero-shot, few-shot and chain-of-thought scenarios, and find a huge performance gap between ChatGPT and SOTA results. Next, we rethink this gap and propose a soft-matching strategy for evaluation to more accurately reflect ChatGPT's performance. Then, we analyze the robustness of ChatGPT on 14 IE sub-tasks, and find that: 1) ChatGPT rarely outputs invalid responses; 2) Irrelevant context and long-tail target types greatly affect ChatGPT's performance; 3) ChatGPT cannot understand well the subject-object relationships in RE task. Finally, we analyze the errors of ChatGPT, and find that "unannotated spans" is the most dominant error type. This raises concerns about 
    
[^115]: 开放领域问答系统在最小编辑问题上的对比一致性探究

    Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions. (arXiv:2305.14441v1 [cs.CL])

    [http://arxiv.org/abs/2305.14441](http://arxiv.org/abs/2305.14441)

    本文针对开放领域问答系统中存在的对比一致性问题，通过收集最小编辑问题构建了具有挑战性的对比集合，发现广泛使用的DPR在对比集合上表现不佳，引入了查询端对比损失和数据增强来改善DPR的训练，并进行实验验证。

    

    对比一致性是NLP中的一个关键方面，是模型在存在扰动的情况下进行一致正确预测的能力。尽管在情感分析和阅读理解等任务中已经有所研究，但在开放式领域问答（OpenQA）中仍未被探讨，主要是由于难以收集到符合事实要求的扰动问题。针对此问题，我们采用了人工标注和大型语言模型生成相结合的方式收集了最小编辑问题，作为具有挑战性的对比集合，用于评估OpenQA模型。实验结果表明，尽管对训练集拟合良好且在标准测试集上表现竞争力，但广泛使用的密集式段落取回器（DPR）在我们的对比集合上表现不佳。为了解决这个问题，我们引入了一种简单有效的查询端对比损失，结合数据增强来改进DPR的训练。我们对对比集合进行的实验表明，DPR的对比一致性得到了改善。

    Contrast consistency, the ability of a model to make consistently correct predictions in the presence of perturbations, is an essential aspect in NLP. While studied in tasks such as sentiment analysis and reading comprehension, it remains unexplored in open-domain question answering (OpenQA) due to the difficulty of collecting perturbed questions that satisfy factuality requirements. In this work, we collect minimally edited questions as challenging contrast sets to evaluate OpenQA models. Our collection approach combines both human annotation and large language model generation. We find that the widely used dense passage retriever (DPR) performs poorly on our contrast sets, despite fitting the training set well and performing competitively on standard test sets. To address this issue, we introduce a simple and effective query-side contrastive loss with the aid of data augmentation to improve DPR training. Our experiments on the contrast sets demonstrate that DPR's contrast consistency
    
[^116]: 面向领域扩展的ASTE：重新审视情感三元组提取中的泛化问题

    Domain-Expanded ASTE: Rethinking Generalization in Aspect Sentiment Triplet Extraction. (arXiv:2305.14434v1 [cs.CL])

    [http://arxiv.org/abs/2305.14434](http://arxiv.org/abs/2305.14434)

    该论文提出了一个领域扩展的ASTE基准数据集，通过生成方法来解决领域泛化的问题。

    

    面向领域扩展的Aspect Sentiment Triplet Extraction (ASTE) 是Aspect-Based Sentiment Analysis (ABSA) 中的一个子任务，考虑每个观点术语、它们表达的情感及相应的方面目标。然而，现有方法仅限于两个领域内的场景。因此，我们提出了一个领域扩展的基准数据集，以应对领域内、领域间和跨领域的情况。我们基于酒店和化妆品评论，标注了超过4000个数据样本来支持新的基准数据集。我们对五种现有的方法进行了分析，结果表明，尽管领域内和领域外的性能存在显著差距，但生成方法在领域泛化方面具有很强的潜力。我们的数据集、代码实现和模型均可在https://github.com/DAMO-NLP-SG/domain-expanded-aste 上获得。

    Aspect Sentiment Triplet Extraction (ASTE) is a subtask of Aspect-Based Sentiment Analysis (ABSA) that considers each opinion term, their expressed sentiment, and the corresponding aspect targets. However, existing methods are limited to the in-domain setting with two domains. Hence, we propose a domain-expanded benchmark to address the in-domain, out-of-domain and cross-domain settings. We support the new benchmark by annotating more than 4000 data samples for two new domains based on hotel and cosmetics reviews. Our analysis of five existing methods shows that while there is a significant gap between in-domain and out-of-domain performance, generative methods have a strong potential for domain generalization. Our datasets, code implementation and models are available at https://github.com/DAMO-NLP-SG/domain-expanded-aste .
    
[^117]: 通过多跳指令进行图像操作——一个新的数据集和基于弱监督的神经符号方法

    Image Manipulation via Multi-Hop Instructions -- A New Dataset and Weakly-Supervised Neuro-Symbolic Approach. (arXiv:2305.14410v1 [cs.CV])

    [http://arxiv.org/abs/2305.14410](http://arxiv.org/abs/2305.14410)

    该论文提出了一个基于神经符号概念学习的图像操作系统NeuroSIM，它可以通过多跳指令在多物体场景中执行复杂的推理，只需要弱监督的数据集，并创建了一个新的数据集。该系统具有很高的竞争力或超过SOTA基线。

    

    我们对通过自然语言文本进行图像操作感兴趣，这是多个人工智能应用程序中有用的任务，但需要对多模态空间进行复杂的推理。我们扩展了最近提出的神经符号概念学习(NSCL)，该方法在视觉问答(VQA)任务上非常有效，扩展其用于图像操作的任务。我们的系统称为NeuroSIM，可以在多物体场景上执行复杂的多跳推理，只需要以VQA的注释数据形式提供弱监督。NeuroSIM将指令解析成符号程序，基于由对象属性和操作组成的专业领域语言(DSL)，指导其执行。我们为这个任务创建了一个新的数据集，广泛的实验表明，NeuroSIM与使用监督数据进行操作的SOTA基线相比具有很高的竞争力或超过SOTA基线。

    We are interested in image manipulation via natural language text -- a task that is useful for multiple AI applications but requires complex reasoning over multi-modal spaces. We extend recently proposed Neuro Symbolic Concept Learning (NSCL), which has been quite effective for the task of Visual Question Answering (VQA), for the task of image manipulation. Our system referred to as NeuroSIM can perform complex multi-hop reasoning over multi-object scenes and only requires weak supervision in the form of annotated data for VQA. NeuroSIM parses an instruction into a symbolic program, based on a Domain Specific Language (DSL) comprising of object attributes and manipulation operations, that guides its execution. We create a new dataset for the task, and extensive experiments demonstrate that NeuroSIM is highly competitive with or beats SOTA baselines that make use of supervised data for manipulation.
    
[^118]: AlpacaFarm: 一种从人类反馈中学习的方法模拟框架

    AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])

    [http://arxiv.org/abs/2305.14387](http://arxiv.org/abs/2305.14387)

    该论文提出了一种名为AlpacaFarm的低成本模拟器，该模拟器为从人类反馈中学习的研究和开发提供了一种解决方案，通过设计LLM提示来模拟人类反馈，提出自动评估并提供参考实现，克服了数据收集的高昂成本、缺乏可信的评估和缺乏参考方法实现的挑战。

    

    大型语言模型（LLMs）如ChatGPT因其良好的指令跟随能力而得到了广泛应用。开发这些LLMs需要使用人类反馈进行训练的复杂且尚不明确的工作流程。将此指令跟随过程复制和理解面临三大挑战： 数据收集的高昂成本，缺乏可信的评估和缺乏参考方法实现。我们通过AlpacaFarm解决了这些挑战，这是一个低成本的模拟器，可用于从反馈中学习的研究和开发。第一，我们设计了LLM提示来模拟人类反馈，其成本比众包工作者便宜45倍，并且与人类反馈具有高度一致性。第二，我们提出了一种自动评估方法，并将其与真实世界交互中获得的人类指令进行验证。第三，我们为几种从配对反馈中学习的方法（PPO，best-of-n，expert iteration等）提供了参考实现。

    Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback
    
[^119]: 让GPT成为数学教师：使用定制化练习生成教授数学应用题解决方法

    Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation. (arXiv:2305.14386v1 [cs.LG])

    [http://arxiv.org/abs/2305.14386](http://arxiv.org/abs/2305.14386)

    本文提出了一种使用GPT-3生成定制化练习，教授数学应用题解决方法的新方法，该方法考虑学生模型的弱点并以教育科学原理为基础进行定制化的学习体验，并取得了比其他大型语言模型更好的表现。

    

    本文提出了一种新的方法，将大型语言模型（LLMs）中的数学应用题解决能力提炼为更小、更高效的学生模型。我们的方法旨在考虑学生模型的弱点，通过生成与教育科学原理（如知识跟踪和个性化学习）对齐的有针对性的练习来促进定制化的学习体验。我们让GPT-3成为数学教师，迭代执行两个步骤：1）在由GPT生成的练习册上评估学生模型的当前学习状况；2）使用GPT-3生成的定制化练习样本训练学生模型，以提高其性能。实验结果表明，我们的方法在三个不同基准测试中比LLMs（例如，GPT-3和PaLM）具有更高的准确性，同时使用的参数数量明显较少。此外，我们对方法中各个组件进行了综合分析，以证明其有效性。

    In this paper, we present a novel approach for distilling math word problem solving capabilities from large language models (LLMs) into smaller, more efficient student models. Our approach is designed to consider the student model's weaknesses and foster a tailored learning experience by generating targeted exercises aligned with educational science principles, such as knowledge tracing and personalized learning. Concretely, we let GPT-3 be a math tutor and run two steps iteratively: 1) assessing the student model's current learning status on a GPT-generated exercise book, and 2) improving the student model by training it with tailored exercise samples generated by GPT-3. Experimental results reveal that our approach outperforms LLMs (e.g., GPT-3 and PaLM) in accuracy across three distinct benchmarks while employing significantly fewer parameters. Furthermore, we provide a comprehensive analysis of the various components within our methodology to substantiate their efficacy.
    
[^120]: 寻找多头注意力的支柱

    Finding the Pillars of Strength for Multi-Head Attention. (arXiv:2305.14380v1 [cs.LG])

    [http://arxiv.org/abs/2305.14380](http://arxiv.org/abs/2305.14380)

    本研究提出聚合头注意力(Grouped Head Attention)，使用自监督组约束进行训练，为注意力头进行分组，其中每个组专注于一个重要而独特的特征子集。此方法可以缓解MHA的冗余性和过度参数化问题，并导致更有效和高效的MHA，进而在基准测试中取得了性能提升。

    

    最近的研究揭示了多头注意力(Multi-Head Attention, MHA)的一些问题，例如冗余性和过度参数化。具体而言，MHA的头最初设计为从不同的表征子空间中关注信息，然而，先前的研究发现一些注意力头可能学习类似的特征，并且可以通过修剪来提高效率而不会损害性能。受最小冗余特征选择的启发，我们假设聚焦于最具代表性和独特性的特征，并最小化资源消耗，可以缓解上述问题，并导致更有效和高效的MHA。具体地，我们提出了聚合头注意力(Grouped Head Attention)，使用自监督组约束进行训练，为注意力头进行分组，其中每个组专注于一个重要而独特的特征子集。此外，我们还提出了一种投票保留程序(Voting-to-Stay)，以删除冗余头，从而实现具有更轻量级权重的转换器。此外，我们的方法在三个知名基准测试中取得了显著的性能提升，并且我们的消融研究提供了支持性的分析。

    Recent studies have revealed some issues of Multi-Head Attention (MHA), e.g., redundancy and over-parameterization. Specifically, the heads of MHA were originally designed to attend to information from different representation subspaces, whereas prior studies found that some attention heads likely learn similar features and can be pruned without harming performance. Inspired by the minimum-redundancy feature selection, we assume that focusing on the most representative and distinctive features with minimum resources can mitigate the above issues and lead to more effective and efficient MHAs. In particular, we propose Grouped Head Attention, trained with a self-supervised group constraint that group attention heads, where each group focuses on an essential but distinctive feature subset. We additionally propose a Voting-to-Stay procedure to remove redundant heads, thus achieving a transformer with lighter weights. Moreover, our method achieves significant performance gains on three well
    
[^121]: Anchor Prediction: 自动完善互联网链接

    Anchor Prediction: Automatic Refinement of Internet Links. (arXiv:2305.14337v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14337](http://arxiv.org/abs/2305.14337)

    本研究介绍了锚点预测的任务，通过对链接目标网页的特定部分进行识别，帮助读者更有效地在链接网页中找到相关信息。

    

    互联网链接是通过提供便捷的访问相关信息，帮助用户深入了解一个主题。然而，大部分链接都是无锚点的 - 它们将目标网页作为一个整体链接，读者可能会花费大量的精力定位目标网页中丰富他们理解链接源上下文的特定部分。为了帮助读者有效地在链接的网页中找到信息，我们引入了锚点预测的任务，目标是确定与源链接上下文最相关的目标网页的特定部分。我们发布了作者锚点数据集，其中包括 34K 个自然出现的带锚点链接，反映了源文章作者的相关判断。为了模拟读者相关判断，我们注释并发布了读者锚点数据集，这是读者发现有用的锚点的评估集。我们的分析表明，有效的锚点预测通常需要联合推理。

    Internet links enable users to deepen their understanding of a topic by providing convenient access to related information. However, the majority of links are unanchored -- they link to a target webpage as a whole, and readers may expend considerable effort localizing the specific parts of the target webpage that enrich their understanding of the link's source context. To help readers effectively find information in linked webpages, we introduce the task of anchor prediction, where the goal is to identify the specific part of the linked target webpage that is most related to the source linking context. We release the AuthorAnchors dataset, a collection of 34K naturally-occurring anchored links, which reflect relevance judgments by the authors of the source article. To model reader relevance judgments, we annotate and release ReaderAnchors, an evaluation set of anchors that readers find useful. Our analysis shows that effective anchor prediction often requires jointly reasoning over len
    
[^122]: ChatCoT: 基于工具增强的链式思维推理在基于聊天大语言模型上的应用

    ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models. (arXiv:2305.14323v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14323](http://arxiv.org/abs/2305.14323)

    这项研究提出了一个基于工具增强的链式思维推理框架 ChatCoT，用于基于聊天的大型语言模型，通过聊天的方式实现多轮推理，巧妙地结合了思维链跟踪和工具操作的方法，提高了大型语言模型的推理能力。

    

    虽然大型语言模型在各种评估基准中取得了出色的表现，但它们在需要特定知识和多次推理的复杂推理任务中仍然存在困难。为了提高推理能力，我们提出了一种基于工具增强的链式思维推理框架 ChatCoT，用于基于聊天的大型语言模型。在 ChatCoT 中，我们将链式思维推理建模为多轮对话，通过聊天的方式更自然地利用工具。在每个轮次中，LLM 能够交互工具或进行推理。我们的方法可以有效利用基于聊天的 LLM 的多轮对话能力，并以统一的方式集成思维链跟踪和工具操作。特别地，我们通过工具、任务和推理格式初始化对话的早期轮次，并提出了一个迭代的工具增强推理步骤来逐步进行工具增强推理。实验结果在两个任务上展示了 ChatCoT 的有效性。

    Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning. To improve the reasoning abilities, we propose \textbf{ChatCoT}, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs. In ChatCoT, we model the chain-of-thought~(CoT) reasoning as multi-turn conversations, to utilize tools in a more natural way through chatting. At each turn, LLMs can either interact with tools or perform the reasoning. Our approach can effectively leverage the multi-turn conversation ability of chat-based LLMs, and integrate the thought chain following and tools manipulation in a unified way. Specially, we initialize the early turns of the conversation by the tools, tasks and reasoning format, and propose an iterative \emph{tool-augmented reasoning} step to perform step-by-step tool-augmented reasoning. The experiment results on two 
    
[^123]: GPT究竟有多老？HumBEL框架通过人群数据评估语言模型

    How Old is GPT?: The HumBEL Framework for Evaluating Language Models using Human Demographic Data. (arXiv:2305.14195v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14195](http://arxiv.org/abs/2305.14195)

    本论文提出了一种使用人类人口数据对语言模型进行评估的框架，并发现GPT-3.5在不同任务中表现不同，在单词意义推断方面模拟了典型6-9岁儿童的能力，在记忆方面则表现优于典型21岁年轻人。

    

    近年来，大型预训练语言模型在自然语言处理中得到广泛应用，然而目前的评估方法并未考虑模型的语言使用与特定人群之间的一致性，尤其是在对话式人工智能应用中这一点非常重要。为了填补这一空白，我们探讨了如何测量和比较语言模型的语言能力与人类子群之间的差异。我们借助语言病理学的临床技术，该学科已经建立了不同（人类）年龄阶段的语言能力发展规范，对技能进行评估，我们与领域专家（即持有临床许可证的语言病理学家）进行了评估，并提出了自动化的评估技术以实现规模化评估。我们发现，GPT-3.5的能力因任务而异，在单词意义推断方面模拟了典型6-9岁儿童的能力，在记忆方面则表现优于典型21岁年轻人。GPT-3.5（InstructGPT）在社交交互任务中也存在一定的困难。

    While large pre-trained language models (LMs) find greater use across NLP, existing evaluation protocols do not consider how LM language use aligns with particular human demographic groups, which can be an important consideration in conversational AI applications. To remedy this gap, we consider how LM language skills can be measured and compared to human sub-populations. We suggest clinical techniques from Speech Language Pathology, which has well-established norms for acquisition of language skills, organized by (human) age. We conduct evaluation with a domain expert (i.e., a clinically licensed speech language pathologist), and also propose automated techniques to substitute clinical evaluation at scale. We find LM capability varies widely depending on task with GPT-3.5 mimicking the ability of a typical 6-9 year old at tasks requiring inference about word meanings and simultaneously outperforming a typical 21 year old at memorization. GPT-3.5 (InstructGPT) also has trouble with soc
    
[^124]: 重温接受性判断

    Revisiting Acceptability Judgements. (arXiv:2305.14091v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14091](http://arxiv.org/abs/2305.14091)

    本文介绍了第一个大规模的非英语语言可接受性数据集CoLAC，发现语言模型性能低于人类的表现，但跨语言转移可行，并可以追溯到预训练阶段。

    

    至NLP社区最后一次关注语言可接受性已经过去多年。在本文中，我们在大型语言模型的背景下重温这个话题。我们引入了 CoLAC-中文语言可接受性语料库，这是第一个由母语讲者验证并带有两组标签的大规模非英语可接受性数据集。我们的实验表明，即使最大的InstructGPT模型在CoLAC上也只能表现随机水平，而ChatGPT的性能（48.30 MCC）也远低于监督模型（59.03 MCC）和人类（65.11 MCC）。通过跨语言转移实验和细粒度的语言分析，我们首次证明了语言可接受性知识可以在不同类型的语言之间转移，而且可以追溯到预训练阶段。

    Years have passed since the NLP community has last focused on linguistic acceptability. In this work, we revisit this topic in the context of large language models. We introduce CoLAC - Corpus of Linguistic Acceptability in Chinese, the first large-scale non-English acceptability dataset that is verified by native speakers and comes with two sets of labels. Our experiments show that even the largest InstructGPT model performs only at chance level on CoLAC, while ChatGPT's performance (48.30 MCC) is also way below supervised models (59.03 MCC) and human (65.11 MCC). Through cross-lingual transfer experiments and fine-grained linguistic analysis, we demonstrate for the first time that knowledge of linguistic acceptability can be transferred across typologically distinct languages, as well as be traced back to pre-training.
    
[^125]: 多容量模型的一站式训练

    One-stop Training of Multiple Capacity Models. (arXiv:2305.14066v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14066](http://arxiv.org/abs/2305.14066)

    本文提出一种新颖的一站式训练框架，可同时训练高容量和低容量模型，相较于知识蒸馏更快速、更高效，实验结果表明该方法在多语言翻译任务上优于低容量基准模型，并在高容量模型上实现了可比或更好的性能，同时提高了模型稳定性和泛化能力。

    

    训练容量不同的模型可用于在不同场景下部署模型，高容量模型具有更好的性能，低容量模型在训练和推断时需要更少的计算资源。本文提出了一种新颖的一站式训练框架，可同时训练高容量和低容量模型。该框架包括两个组合模型体系结构和一个名为Two-Stage Joint-Training (TSJT)的联合训练算法。与知识蒸馏不同的是，我们的方法同时整合了不同容量模型的监督，导致更快速、更高效的收敛。在多语言机器翻译基准测试WMT10上的大量实验表明，我们的方法优于低容量基准模型，并在高容量模型上实现了可比或更好的性能。值得注意的是，分析表明我们的方法明显提高了模型稳定性和泛化能力。

    Training models with varying capacities can be advantageous for deploying them in different scenarios. While high-capacity models offer better performance, low-capacity models require fewer computing resources for training and inference. In this work, we propose a novel one-stop training framework to jointly train high-capacity and low-capactiy models. This framework consists of two composite model architectures and a joint training algorithm called Two-Stage Joint-Training (TSJT). Unlike knowledge distillation, where multiple capacity models are trained from scratch separately, our approach integrates supervisions from different capacity models simultaneously, leading to faster and more efficient convergence. Extensive experiments on the multilingual machine translation benchmark WMT10 show that our method outperforms low-capacity baseline models and achieves comparable or better performance on high-capacity models. Notably, the analysis demonstrates that our method significantly infl
    
[^126]: 基于语法约束的语言模型灵活解码技术

    Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])

    [http://arxiv.org/abs/2305.13971](http://arxiv.org/abs/2305.13971)

    本文提出了一种使用形式语法约束丰富解码步骤的方法，有效生成符合特定语法的复杂输出结构，同时允许任何上下文无关语法集成。实验证明该方法在四个信息提取任务上实现了最先进的性能表现。

    

    LLM在许多任务中展现出了惊人的少量样本表现，但在生成信息提取所需的复杂输出结构时仍存在困难。这个限制源于LLM在没有微调的情况下倾向于生成自由文本而不是遵循特定语法的精确结构。在本文中，我们提出在解码步骤中使用形式语法约束来丰富模型。在搜索过程中，只有符合语法产生规则的有效令牌能被考虑到。这样就强制只产生有效的序列。我们的框架非常通用和灵活，允许任何上下文无关语法(CFG)集成到我们的自定义约束beam搜索实现中。我们展示了许多NLP任务的输出可以被表示为形式语言，使它们适合在我们的框架中直接使用。对于输出空间取决于输入的任务，我们提出了基于输入的CFG，根据特定于输入的特征更新产生规则。实验证明了我们的方法在生成复杂输出结构方面的有效性，并在四个信息提取任务上实现了最先进的性能。

    LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
    
[^127]: LogicLLM：探索自监督逻辑增强训练的大语言模型

    LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models. (arXiv:2305.13718v1 [cs.CL])

    [http://arxiv.org/abs/2305.13718](http://arxiv.org/abs/2305.13718)

    本文介绍了 LogicLLM，一种通过自监督后训练来提高大语言模型的逻辑推理能力的方法，该方法有效地在常见逻辑推理任务上进行表现，超过了目前最先进的无监督基线方法。

    

    改善语言模型的逻辑推理能力的现有努力主要依赖于有监督微调，这阻碍了将模型泛化到新的领域和/或任务。然而，通过发展大语言模型（LLM）已经证明了将丰富的知识压缩为单个代理的能力，使它们能够有效地处理多个任务。然而，我们的初步实验表明，LLMs 在逻辑推理方面并没有表现出能力。LLMs 在逻辑推理基准测试中的表现远远落后于现有的最先进基线。在本文中，我们首次尝试通过自监督后训练来探索融合逻辑知识的可行性，并通过上下文学习来激活它，我们将其称为LogicLLM。具体来说，我们设计了一个MERIt 的自回归目标变体，并将其与两个LLM系列FLAN-T5和LLaMA集成在一起，参数大小范围从30亿到130亿。实验结果表明，我们的方法在常用推理策略上与目前最先进的有监督方法相当，并且远远超过了目前最先进的无监督基线方法。

    Existing efforts to improve logical reasoning ability of language models have predominantly relied on supervised fine-tuning, hindering generalization to new domains and/or tasks. The development of Large Langauge Models (LLMs) has demonstrated the capacity of compressing abundant knowledge into a single proxy, enabling them to tackle multiple tasks effectively. Our preliminary experiments, nevertheless, show that LLMs do not show capability on logical reasoning. The performance of LLMs on logical reasoning benchmarks is far behind the existing state-of-the-art baselines. In this paper, we make the first attempt to investigate the feasibility of incorporating logical knowledge through self-supervised post-training, and activating it via in-context learning, which we termed as LogicLLM. Specifically, we devise an auto-regressive objective variant of MERIt and integrate it with two LLM series, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to 13 billion. The results 
    
[^128]: IdEALS: 提高语言技能的惯用表达

    IdEALS: Idiomatic Expressions for Advancement of Language Skills. (arXiv:2305.13637v1 [cs.CL])

    [http://arxiv.org/abs/2305.13637](http://arxiv.org/abs/2305.13637)

    这篇论文探索了使用惯用表达式来提高学生写作能力的方法，并评估了不同方法的性能表现。

    

    虽然在发展语法错误纠正（GEC）方法方面有了重大进展，但关注词语选择方面的改进显著缺乏，并通过用高级表达式替换短语来增强语句表达能力是一种鲜为人知的方面。在本文中，我们关注这个领域，介绍了我们对将惯用表达式用于学生写作中的研究。为了方便我们的研究，我们使用真实数据策划了广泛的训练集和专家注释的测试集，并评估了各种方法，并将它们与人类专家的表现进行比较。

    Although significant progress has been made in developing methods for Grammatical Error Correction (GEC), addressing word choice improvements has been notably lacking and enhancing sentence expressivity by replacing phrases with advanced expressions is an understudied aspect. In this paper, we focus on this area and present our investigation into the task of incorporating the usage of idiomatic expressions in student writing. To facilitate our study, we curate extensive training sets and expert-annotated testing sets using real-world data and evaluate various approaches and compare their performance against human experts.
    
[^129]: 通过翻译和注释融合改进低资源实体识别

    Better Low-Resource Entity Recognition Through Translation and Annotation Fusion. (arXiv:2305.13582v1 [cs.CL])

    [http://arxiv.org/abs/2305.13582](http://arxiv.org/abs/2305.13582)

    本研究提出了一种通过翻译和注释融合的框架，可以改进低资源语言文本的命名实体识别。通过TransFusion模型，可以在不同语言之间进行强大的预测，且在两个低资源命名实体识别数据集上表现一致优秀。

    

    预训练的多语言语言模型已经在跨语言转移方面实现了重大进展。然而，这些模型在从高资源语言转移至低资源语言时，通常表现出性能差异，特别是对于未被充分训练或未包含在预训练数据中的语言。受这些模型在高资源语言上表现优秀的启发，我们介绍了一个翻译和融合框架，该框架将低资源语言文本翻译成高资源语言进行注释，然后将注释融合回低资源语言。基于该框架，我们提出了TransFusion模型，该模型训练用于融合来自高资源语言的预测结果，以在低资源语言上进行强大的预测。我们在两个低资源命名实体识别（NER）数据集MasakhaNER2.0和LORELEI NER上评估了我们的方法，并展示了与最先进的跨语言NER基线的一致优秀性能。

    Pre-trained multilingual language models have enabled significant advancements in cross-lingual transfer. However, these models often exhibit a performance disparity when transferring from high-resource languages to low-resource languages, especially for languages that are underrepresented or not in the pre-training data. Motivated by the superior performance of these models on high-resource languages compared to low-resource languages, we introduce a Translation-and-fusion framework, which translates low-resource language text into a high-resource language for annotation using fully supervised models before fusing the annotations back into the low-resource language. Based on this framework, we present TransFusion, a model trained to fuse predictions from a high-resource language to make robust predictions on low-resource languages. We evaluate our methods on two low-resource named entity recognition (NER) datasets, MasakhaNER2.0 and LORELEI NER, covering 25 languages, and show consist
    
[^130]: 多模态自动事实核查：一份调查

    Multimodal Automated Fact-Checking: A Survey. (arXiv:2305.13507v1 [cs.CL])

    [http://arxiv.org/abs/2305.13507](http://arxiv.org/abs/2305.13507)

    本调查提出了一个多模态自动事实核查的框架，并包括了独特的子任务，重点关注了文本，图像，音频和视频这四种模态的现实应用。纪录了相关的基准模型，讨论了未来研究的局限性和前景。

    

    错误信息，即事实上不正确的信息，通常以多种形式传达，例如带有标题的图像。 它被人们视为更可信，比其仅限于文本的对应物扩散速度更快，范围更广。 尽管越来越多的研究涉及自动事实核查（AFC），但以往的调查主要集中在文本误导方面。 在本调查中，我们构建了一个包括多模态误导独特子任务在内的AFC框架。此外，我们在我们的框架上讨论了不同社区所发展的相关术语。 我们重点关注现实世界事实核查中存在的四种模态：文本，图像，音频和视频。 我们调查了基准和模型，并讨论了未来研究的局限性和有前途的方向。

    Misinformation, i.e. factually incorrect information, is often conveyed in multiple modalities, e.g. an image accompanied by a caption. It is perceived as more credible by humans, and spreads faster and wider than its text-only counterparts. While an increasing body of research investigates automated fact-checking (AFC), previous surveys mostly focus on textual misinformation. In this survey, we conceptualise a framework for AFC including subtasks unique to multimodal misinformation. Furthermore, we discuss related terminological developed in different communities in the context of our framework. We focus on four modalities prevalent in real-world fact-checking: text, image, audio, and video. We survey benchmarks and models, and discuss limitations and promising directions for future research.
    
[^131]: Flover：一种用于高效自回归模型并行推断的时间融合框架

    Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v1 [cs.DC])

    [http://arxiv.org/abs/2305.13484](http://arxiv.org/abs/2305.13484)

    Flover是一种用于自回归模型并行推断的时间融合框架，解决了并行性不足和灵活性差的问题，可以实现更加高效的推断性能。

    

    在深度学习领域快速发展的背景下，模型推断性能成为一个关键因素，尤其是在模型变得更加复杂并被部署在多个应用场景中的情况下。自回归模型由于在众多生成任务中表现优异，因此备受关注。这些模型设计上采用了一种时间依赖结构，其中当前token的概率分布受到前面token的影响。然而，这种本质上的序列特性遵循马尔可夫链假设，缺乏时间并行性，因此存在独特的挑战。特别是在工业背景下，推断请求遵循泊松时间分布，需要不同的响应长度，这种并行性的缺失更加明显。现有的解决方案如动态批处理和并发模型实例，然而，这些粗粒度的方法存在严重的开销和缺乏灵活性，无法实现最优化。

    In the rapidly evolving field of deep learning, the performance of model inference has become a pivotal aspect as models become more complex and are deployed in diverse applications. Among these, autoregressive models stand out due to their state-of-the-art performance in numerous generative tasks. These models, by design, harness a temporal dependency structure, where the current token's probability distribution is conditioned on preceding tokens. This inherently sequential characteristic, however, adheres to the Markov Chain assumption and lacks temporal parallelism, which poses unique challenges. Particularly in industrial contexts where inference requests, following a Poisson time distribution, necessitate diverse response lengths, this absence of parallelism is more profound. Existing solutions, such as dynamic batching and concurrent model instances, nevertheless, come with severe overheads and a lack of flexibility, these coarse-grained methods fall short of achieving optimal la
    
[^132]: 大型语言模型在知识冲突中的行为揭秘：自适应变色龙还是固执的树獭

    Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes. (arXiv:2305.13300v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13300](http://arxiv.org/abs/2305.13300)

    本文研究了大型语言模型（LLMs）在遭遇知识冲突时的行为。结果发现，LLMs可以高度接受外部连贯且有说服力的证据，即使与其参数化内存存在冲突，但也可能有局限性。

    

    通过向大型语言模型（LLMs）提供外部信息，工具增强（包括检索增强）已成为解决LLMs静态参数化内存限制的有希望的解决方案。然而，当这些证据与它们的参数化内存发生冲突时，LLMs对这些外部证据有多少接受能力？我们提出了一个系统性的框架来从LLMs中获取高质量的参数化内存，并构建相应的对立内存，从而使我们能够进行一系列受控实验。我们的调查揭示了LLMs表现出看似矛盾的行为。一方面，与以往的观念不同，我们发现，只要外部证据是连贯且有说服力的，LLMs即使与其参数化内存存在冲突也可以高度接受外部证据。另一方面，LLMs也可能会表现出局限性，尤其是当其参数化内存受到威胁时。

    By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory. However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory? We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts. We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. Our investigation reveals seemingly contradicting behaviors of LLMs. On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. On the other hand, LLMs also d
    
[^133]: AVeriTeC: 一个包含网络证据的真实世界主张验证数据集

    AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the Web. (arXiv:2305.13117v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13117](http://arxiv.org/abs/2305.13117)

    本文介绍了AVeriTeC数据集，该数据集包括4,568个真实世界的主张，每个主张都有在线证据支持的问题-答案对和文本证明解释。该数据集可以避免一些常见的问题，并提供了一个评估方案来对开放网络上的主张进行验证。

    

    现有的自动事实检查数据集存在重大局限性，例如依赖人工主张、缺乏证据和中间推理注释，或包括发布主张后的证据。在本文中，我们介绍了 AVeriTeC，这是一个新的数据集，包括 4,568 个真实世界的主张，涵盖了50个不同组织的事实检查。每个主张都用在线可用证据支持的问题-答案对进行注释，以及文本的证明解释证据如何相互结合产生结论。通过多轮注释过程，我们避免了常见的问题，包括上下文依赖性、证据不足和时间泄漏，并在结论上达成了相当大的注释员协议 $\kappa=0.619$。我们开发了一个基线以及一个方法来验证通过对开放网络进行几个问题回答步骤来检查主张。

    Existing datasets for automated fact-checking have substantial limitations, such as relying on artificial claims, lacking annotations for evidence and intermediate reasoning, or including evidence published after the claim. In this paper we introduce AVeriTeC, a new dataset of 4,568 real-world claims covering fact-checks by 50 different organizations. Each claim is annotated with question-answer pairs supported by evidence available online, as well as textual justifications explaining how the evidence combines to produce a verdict. Through a multi-round annotation process, we avoid common pitfalls including context dependence, evidence insufficiency, and temporal leakage, and reach a substantial inter-annotator agreement of $\kappa=0.619$ on verdicts. We develop a baseline as well as an evaluation scheme for verifying claims through several question-answering steps against the open web.
    
[^134]: 基于BEHRT的医疗概念嵌入联邦学习

    Federated Learning of Medical Concepts Embedding using BEHRT. (arXiv:2305.13052v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2305.13052](http://arxiv.org/abs/2305.13052)

    该论文提出了一种基于BEHRT的联邦学习方法来学习医疗概念嵌入，在不传输任何隐私数据的情况下，实现了多个医疗中心的EHR数据的共同学习。实验证明该方法在预测病人下一次诊断方面有效。

    

    电子健康记录（EHR）数据包含病人的诊断、药物、治疗等医疗记录。这些数据通常被视为敏感的医疗信息。因此，医疗中心的EHR数据通常不能被共享，这使得使用多个中心的EHR数据创建预测模型变得困难，而这对于模型的强健性和普适性至关重要。联邦学习是一种算法方法，可以在多个位置的数据上学习共享模型，而无需将所有数据存储在中心位置。我们提出了一种联邦学习方法来学习医疗概念嵌入。这个预训练模型叫做BEHRT，是在一个大型EHR数据语料库上训练的，它将医疗概念编码成高维向量。我们的方法可以在多个医疗中心的EHR数据中获取医疗概念的嵌入，而不需要在它们之间交换记录。我们将模型应用于一个实际的预测任务，使用多个中心的EHR数据来预测下一次诊断，实验结果证明了我们方法的有效性。

    Electronic Health Records (EHR) data contains medical records such as diagnoses, medications, procedures, and treatments of patients. This data is often considered sensitive medical information. Therefore, the EHR data from the medical centers often cannot be shared, making it difficult to create prediction models using multi-center EHR data, which is essential for such models' robustness and generalizability. Federated Learning (FL) is an algorithmic approach that allows learning a shared model using data in multiple locations without the need to store all data in a central place. An example of a prediction model's task is to predict future diseases. More specifically, the model needs to predict patient's next visit diagnoses, based on current and previous clinical data. Such a prediction model can support care providers in making clinical decisions and even provide preventive treatment. We propose a federated learning approach for learning medical concepts embedding. This pre-trained
    
[^135]: 大型语言模型能够模拟语境不明结构化访谈的归纳式主题分析吗？对方法和模型的局限性进行探讨和挑战

    Can Large Language Models emulate an inductive Thematic Analysis of semi-structured interviews? An exploration and provocation on the limits of the approach and the model. (arXiv:2305.13014v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13014](http://arxiv.org/abs/2305.13014)

    本文介绍了一项实验的结果和反思，该实验使用模型GPT 3.5-Turbo来模拟归纳式主题分析的某些方面。尝试使用LLM进行基于人类解释的分析显然是一种挑战，但也是了解这些系统在定性研究中能否使用的一种方式。

    

    大型语言模型（LLMs）已成为强大的生成人工智能解决方案，可应用于多个领域和工作领域。本文介绍了一项实验的结果和反思，该实验使用模型GPT 3.5-Turbo来模拟归纳式主题分析的某些方面。先前的研究在该主题上主要进行演绎分析。主题分析是一种在社会科学中常用的定性分析方法，它基于人类分析师的解释以及定性数据中的显式和潜在含义的识别。尝试使用LLM进行基于人类解释的分析显然是一种挑战，但也是了解这些系统在定性研究中能否使用的一种方式。本文介绍了尝试进行此模拟的动机，并反思了Braun和Clarke提出的六个步骤至少部分地如何进行主题分析的方法。

    Large Language Models (LLMs) have emerged as powerful generative Artificial Intelligence solutions which can be applied to several fields and areas of work. This paper presents results and reflection of an experiment done to use the model GPT 3.5-Turbo to emulate some aspects of an inductive Thematic Analysis. Previous research on this subject has largely worked on conducting deductive analysis. Thematic Analysis is a qualitative method for analysis commonly used in social sciences and it is based on interpretations made by the human analyst(s) and the identification of explicit and latent meanings in qualitative data. Attempting an analysis based on human interpretation with an LLM clearly is a provocation but also a way to learn something about how these systems can or cannot be used in qualitative research. The paper presents the motivations for attempting this emulation, it reflects on how the six steps to a Thematic Analysis proposed by Braun and Clarke can at least partially be r
    
[^136]: MultiTabQA：针对多表问题回答生成表格答案

    MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering. (arXiv:2305.12820v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12820](http://arxiv.org/abs/2305.12820)

    本文提出一项新任务，即回答多表问题。 我们的新模型MultiTabQA不仅可以回答多表问题，还可以泛化地生成表格答案，并在实验中表现出比现有基线更好的性能。

    

    近期，使用大型语言模型进行表格问答的研究取得了一定进展，但其覆盖面还受限于答复单表问题。然而，现实中查询具有复杂性，常跨越多个关系数据库或网页表格。对于单表问题，不涉及常见的表格操作，如集合运算、笛卡尔积（连接）或嵌套查询。此外，多表操作通常会产生表格输出，这就需要表格QA模型有生成表格的能力。为了填补这一空白，我们提出了一个新任务，即回答多表问题。我们的模型MultiTabQA不仅回答多表问题，还可以泛化地生成表格答案。为了使训练更有效，我们建立了一个包含132,645个SQL查询和表格答案的预训练数据集。此外，我们通过引入不同严格程度的特定于表格的度量标准对生成的表格进行了评估，评估实验表明，MultiTabQA在新任务上的表现大大优于现有基线。

    Recent advances in tabular question answering (QA) with large language models are constrained in their coverage and only answer questions over a single table. However, real-world queries are complex in nature, often over multiple tables in a relational database or web page. Single table questions do not involve common table operations such as set operations, Cartesian products (joins), or nested queries. Furthermore, multi-table operations often result in a tabular output, which necessitates table generation capabilities of tabular QA models. To fill this gap, we propose a new task of answering questions over multiple tables. Our model, MultiTabQA, not only answers questions over multiple tables, but also generalizes to generate tabular answers. To enable effective training, we build a pre-training dataset comprising of 132,645 SQL queries and tabular answers. Further, we evaluate the generated tables by introducing table-specific metrics of varying strictness assessing various levels 
    
[^137]: TheoremQA：一种定理驱动的问题回答数据集

    TheoremQA: A Theorem-driven Question Answering dataset. (arXiv:2305.12524v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12524](http://arxiv.org/abs/2305.12524)

    该论文介绍了一个定理驱动的问题回答数据集TheoremQA，可用于评估AI模型在应用定理解决科学问题时的能力，经过测试，GPT-4在解决这些问题上的准确率远高于其他模型。

    

    最近的LLMs如GPT-4和PaLM-2在解决像GSM8K这样的基本数学问题方面取得了巨大进展，准确率超过90%。然而，它们解决需要领域特定知识（即定理）的更具挑战性的数学问题的能力尚未得到深入研究。本文介绍了TheoremQA，这是第一个定理驱动的问题回答数据集，旨在评估AI模型应用定理解决具有挑战性的科学问题的能力。TheoremQA由领域专家策划，包含来自数学、物理、电气与计算机科学以及金融学的800个高质量问题，涵盖350个定理（例如泰勒定理、拉格朗日定理、哈夫曼编码、量子定理、弹性定理等等）。我们评估了16个大型语言和代码模型以及不同的提示策略，例如Chain-of-Thoughts和Program-of-Thoughts。我们发现，GPT-4解决这些问题的能力是无与伦比的，使用Program-of-Thoughts提示策略时准确率达51%，而其他模型远远落后。

    The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in solving fundamental math problems like GSM8K by achieving over 90% accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. In this paper, we introduce TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models' capabilities to apply theorems to solve challenging science problems. TheoremQA is curated by domain experts containing 800 high-quality questions covering 350 theorems (e.g. Taylor's theorem, Lagrange's theorem, Huffman coding, Quantum Theorem, Elasticity Theorem, etc) from Math, Physics, EE&CS, and Finance. We evaluate a wide spectrum of 16 large language and code models with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that GPT-4's capabilities to solve these problems are unparalleled, achieving an accuracy of 51% with Progra
    
[^138]: XTREME-UP：针对少样本数据的用户中心稀缺数据基准测试，在代表性不足的语言上进行评估

    XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages. (arXiv:2305.11938v1 [cs.CL])

    [http://arxiv.org/abs/2305.11938](http://arxiv.org/abs/2305.11938)

    该论文提出了 XTREME-UP，一个以少量数据评估代表性不足语言的 NLP 系统性能的用户中心稀缺数据基准测试。它专注于用户中心任务，聚焦于代表性不足的语言，覆盖 88 种语言，并介绍了新的 OCR、自动完成、语义分析和音译数据集，旨在帮助推进代表性不足语言的高度多语言 NLP 系统的发展。

    

    数据稀缺是高度多语言 NLP 系统发展的关键问题。然而对于许多代表性不足的语言（UL）—— NLP 研究在满足用户需求方面特别滞后的语言——注释少量数据是可行的。基于此，我们提出了 XTREME-UP，这是一个基准测试，其重点是稀缺数据方案而不是零样本；其聚焦于用户中心任务（即许多高资源语言使用者广泛采用的任务）；以及其聚焦于代表性不足的语言，在这些语言中，稀缺数据的情况往往最为现实。XTREME-UP 评估语言模型在 88 种代表性不足的语言上，跨越了 9 项主要的用户中心技术，包括 ASR、OCR、MT 和信息访问任务。我们为 OCR、自动完成、语义分析和音译创建了新的数据集，并在其他任务上构建并完善了现有的数据集。XTREME-UP 提供了一种评估 NLP 系统在代表性不足的语言上稀缺数据方案的性能的方法。它专注于实际价值对于高资源语言使用者的用户中心任务，并涵盖了 88 种语言。其目标是帮助推进多语言 NLP 系统对于稀缺数据资源的代表性不足的语言的发展。

    Data scarcity is a crucial issue for the development of highly multilingual NLP systems. Yet for many under-represented languages (ULs) -- languages for which NLP re-search is particularly far behind in meeting user needs -- it is feasible to annotate small amounts of data. Motivated by this, we propose XTREME-UP, a benchmark defined by: its focus on the scarce-data scenario rather than zero-shot; its focus on user-centric tasks -- tasks with broad adoption by speakers of high-resource languages; and its focus on under-represented languages where this scarce-data scenario tends to be most realistic. XTREME-UP evaluates the capabilities of language models across 88 under-represented languages over 9 key user-centric technologies including ASR, OCR, MT, and information access tasks that are of general utility. We create new datasets for OCR, autocomplete, semantic parsing, and transliteration, and build on and refine existing datasets for other tasks. XTREME-UP provides methodology for e
    
[^139]: LLM自身可读取和生成CXR图像

    LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])

    [http://arxiv.org/abs/2305.11490](http://arxiv.org/abs/2305.11490)

    该论文提出了一种新方法，可以在不需要进行结构更改、额外训练、或训练专门网络的情况下，通过微调预先训练的LLM来读取和生成像文本一样的图像，并应用于胸部X线（CXR）图像的生成任务中。

    

    借助于近期大语言模型（LLMs）的显著发展，人们正积极尝试将LLMs的实用性扩展到多模态任务。已经有人尝试连接语言和视觉信息，并且也在不断尝试为LLMs添加视觉能力。然而，现有的尝试只使用LLMs作为图像解码器，没有尝试通过自然语言来生成图像。通过采用VQ-GAN框架，将图像的潜在表示视为一种文本标记，我们提出了一种新方法，可以微调预先训练的LLM，以像文本一样读取和生成图像，而无需进行结构更改、额外的训练目标或训练专门的网络，同时仍保留LLM的指令跟随能力。我们将此框架应用于胸部X线（CXR）图像的生成任务中，因为这是一个复杂信息在视觉和语言之间翻译的领域。

    Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
    
[^140]: 自动生成会话接口以便于探索表格数据

    Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data. (arXiv:2305.11326v1 [cs.CL])

    [http://arxiv.org/abs/2305.11326](http://arxiv.org/abs/2305.11326)

    本文提出了使用聊天机器人作为自动化创造的会话接口来方便大众探索表格数据的方法。

    

    表格数据是在线发布和交换结构化数据的最常见格式。一个明确的例子是各种类型的公共行政机构发布的开放数据门户数量的增长。但是，这些数据源的利用目前仅限于能够以程序方式处理和消化此类数据的技术人员。作为替代方案，我们建议使用聊天机器人提供会话接口，以便于探索表格数据源。通过我们的方法，任何普通公民都可以从中受益并利用它们。此外，我们的聊天机器人不是手动创建的：相反，它们是通过实例化可配置的对话模式集从数据源本身自动生成的。

    Tabular data is the most common format to publish and exchange structured data online. A clear example is the growing number of open data portals published by all types of public administrations. However, exploitation of these data sources is currently limited to technical people able to programmatically manipulate and digest such data. As an alternative, we propose the use of chatbots to offer a conversational interface to facilitate the exploration of tabular data sources. With our approach, any regular citizen can benefit and leverage them. Moreover, our chatbots are not manually created: instead, they are automatically generated from the data source itself thanks to the instantiation of a configurable collection of conversation patterns.
    
[^141]: 在编程语言模型中发现语义的证据

    Evidence of Meaning in Language Models Trained on Programs. (arXiv:2305.11169v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.11169](http://arxiv.org/abs/2305.11169)

    该论文证明了，通过在程序语料库上训练语言模型，即使没有针对学习语言语义提供归纳偏差，语言模型仍然能够学习含义。线性探测器能够从模型状态中提取程序状态的抽象，准确性与模型泛化到新程序的能力显著相关。

    

    我们提供证据表明，尽管被训练只是执行文本上的下一个标记预测，特别是一个程序语料库，语言模型仍然能够学习含义。每个程序都以（文本）输入输出示例的形式作为规范。与程序一起工作使我们能够精确地定义与语言中有关含义的概念（例如，正确性和语义），使得程序综合成为一个中间测试平台，用于表征语言模型中是否存在含义的存在（或不存在）。我们首先在程序语料库上训练了一个Transformer模型，然后探查了已经完成规范的程序时，经过训练的模型的隐藏状态。尽管没有针对学习语言语义提供归纳偏差，但我们发现，线性探测器能够从模型状态中提取当前和未来程序状态的抽象。此外，线性探测器的准确性与模型泛化到新程序的能力强有力、统计学显著地相关。

    We present evidence that language models can learn meaning despite being trained only to perform next token prediction on text, specifically a corpus of programs. Each program is preceded by a specification in the form of (textual) input-output examples. Working with programs enables us to precisely define concepts relevant to meaning in language (e.g., correctness and semantics), making program synthesis well-suited as an intermediate testbed for characterizing the presence (or absence) of meaning in language models.  We first train a Transformer model on the corpus of programs, then probe the trained model's hidden states as it completes a program given a specification. Despite providing no inductive bias toward learning the semantics of the language, we find that a linear probe is able to extract abstractions of both current and future program states from the model states. Moreover, there is a strong, statistically significant correlation between the accuracy of the probe and the mo
    
[^142]: 网络可以为改进大语言模型提供更多的可选资源

    The Web Can Be Your Oyster for Improving Large Language Models. (arXiv:2305.10998v1 [cs.CL])

    [http://arxiv.org/abs/2305.10998](http://arxiv.org/abs/2305.10998)

    本文提出了一种新的大型语言模型增强方法，即使用自适应搜索引擎辅助学习方法，将大规模网络数据与LLMs融合，从而避免无用或嘈杂的增强。

    

    大型语言模型(LLMs)可以编码大量的世界知识。然而，由于这些知识在模型训练时已经固化，这些模型变得静态，并且受到了当时训练数据的限制。为了进一步提高LLMs在知识密集型任务中的能力，我们考虑使用搜索引擎将LLMs与海量网络进行融合增强。与以往的增强来源（如维基百科数据转储）不同，网络提供了更广泛、更全面且不断更新的信息。在本文中，我们提出了一个名为UNIWEB的网络增强LLM，它以统一的文本到文本格式在16个知识密集型任务上进行训练。我们的方法并不是简单地使用从网络检索到的内容，而是提出了一种自适应的搜索引擎辅助学习方法，该方法可以自我评估LLM的预测置信度，并自适应地确定何时参考网络获取更多数据，从而避免无用或嘈杂的增强。

    Large language models (LLMs) encode a large amount of world knowledge. However, as such knowledge is frozen at the time of model training, the models become static and limited by the training data at that time. In order to further improve the capacity of LLMs for knowledge-intensive tasks, we consider augmenting LLMs with the large-scale web using search engine. Unlike previous augmentation sources (e.g., Wikipedia data dump), the web provides broader, more comprehensive and constantly updated information. In this paper, we present a web-augmented LLM UNIWEB, which is trained over 16 knowledge-intensive tasks in a unified text-to-text format. Instead of simply using the retrieved contents from web, our approach has made two major improvements. Firstly, we propose an adaptive search engine assisted learning method that can self-evaluate the confidence level of LLM's predictions, and adaptively determine when to refer to the web for more data, which can avoid useless or noisy augmentatio
    
[^143]: 语言模型遇见世界模型：实体经验增强语言模型

    Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v1 [cs.CL])

    [http://arxiv.org/abs/2305.10626](http://arxiv.org/abs/2305.10626)

    本文提出一种新的增强语言模型的方法——将其与世界模型相结合，通过有目的的规划和随机探索获得丰富的实体经验进行微调, 以提高其在物理环境下的推理和行为能力，并在语言基准上保持或提高性能。

    

    虽然大型语言模型 (LMs) 在许多任务上表现出了非凡的能力，但它们常常在处理物理环境下的简单推理和规划问题时遇到困难，例如理解物体永恒或规划家庭活动。这种限制源于 LM 仅受书面语言训练，缺少必要的实体知识和技能。本文提出了一种新的增强 LM 的方法，即将其与世界模型相结合进行微调，以获得多样化的实体知识，同时保持其一般语言能力。本方法在世界模型中部署一个融入实体经验的代理，特别是一个模拟物理世界的仿真器(VirtualHome)，通过有目的的规划和随机探索获得多样化的实体经验。然后，利用这些经验微调 LM ，以教授在物理世界中的各种推理和行为能力，例如规划和完成目标、物体永恒和跟踪等。此外，我们相信我们的方法可以轻松扩展以利用其他模拟器，包括机器人或自动驾驶汽车。我们证明了我们的方法显着提高了 LM 在一系列物理推理任务中的性能，同时保留并经常提高了它们在语言基准上的表现。

    While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is
    
[^144]: DoReMi: 优化数据混合加速语言模型预训练

    DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. (arXiv:2305.10429v1 [cs.CL])

    [http://arxiv.org/abs/2305.10429](http://arxiv.org/abs/2305.10429)

    DoReMi方法使用分组分布式鲁棒优化训练小型代理模型以产生域权重，再使用这些权重重新采样数据集训练大型模型，相比使用默认权重的基线模型，在The Pile和GLaM数据集上平均提高了6.5%和4.7%的few-shot下游准确度，分别使用2.6倍和相同的训练步骤达到基线准确度。

    

    预训练数据域的混合比例（例如，维基百科、图书、网页文本）极大地影响语言模型（LM）性能。在本文中，我们提出了一种称为DoReMi的Domain Reweighting with Minimax Optimization方法，它首先使用分组分布式鲁棒优化（Group DRO）训练一个小代理模型，以产生域权重（混合比例），而不需要知道下游任务的知识。然后我们使用这些域权重重新采样一个数据集，并训练一个更大的，全尺寸的模型。在我们的实验中，我们使用DoReMi在一个280M参数的代理模型上，更有效地找到训练一个8B参数模型（30倍大）的域权重。在The Pile上，即使在减小一些域的比重时，DoReMi也能提高所有域的perplexity。相比使用The Pile的默认域权重训练的基线模型，DoReMi将平均few-shot下游准确度提高了6.5%，并使用2.6倍的训练步骤达到基线准确度。在GLaM数据集上，DoReMi没有任何关于下游任务的知识，提高了4.7%（次于现有最先进的模型）的few-shot准确度，在相同的训练步骤下提高了9.0%的准确度。

    The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no know
    
[^145]: 通过缩放实现更好的语音合成

    Better speech synthesis through scaling. (arXiv:2305.07243v1 [cs.SD])

    [http://arxiv.org/abs/2305.07243](http://arxiv.org/abs/2305.07243)

    该论文介绍了一种将图像生成方法应用于语音合成领域的方法，提出了一个表达性强、多语音的文本转语音系统TorToise。

    

    近年来，图像生成领域在自回归变换器和DDPM的应用下得到了革命性突破。这些方法将图像生成过程建模为逐步概率过程，并利用大量的计算和数据来学习图像分布。本文将这种性能提升方法运用到语音合成中，提出了一种名为TorToise的表达性、多语音的文本转语音系统。所有模型代码和训练权重均已开源，存放于https://github.com/neonbjb/tortoise-tts。

    In recent years, the field of image generation has been revolutionized by the application of autoregressive transformers and DDPMs. These approaches model the process of image generation as a step-wise probabilistic processes and leverage large amounts of compute and data to learn the image distribution. This methodology of improving performance need not be confined to images. This paper describes a way to apply advances in the image generative domain to speech synthesis. The result is TorToise -- an expressive, multi-voice text-to-speech system.  All model code and trained weights have been open-sourced at https://github.com/neonbjb/tortoise-tts.
    
[^146]: 当多数人是错误的：利用标注者不一致性进行主观任务

    When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v1 [cs.CL])

    [http://arxiv.org/abs/2305.06626](http://arxiv.org/abs/2305.06626)

    本文通过预测单个标注者的打分，并结合文本目标群体的预测，模拟了目标群体成员的意见，通过使用他们的人口统计学数据和在线意见预测标注者的打分，在仇恨言论检测等主观任务中提高了模型性能。

    

    在自然语言处理中，虽然通常使用标注者的多数投票来确定标签，但在仇恨言论检测等主观任务中，标注者之间存在不一致性可能反映出群体观点的差异，而不是噪声。因此，仇恨言论检测的一个关键问题是一个语句是否冒犯了它所针对的人群，而这可能只占标注者池的一小部分。本文构建了一个模型，预测可能具有冒犯性文本上每个标注者的打分，并结合文本的预测目标群体来模拟目标群体成员的意见。我们展示了一系列的评估指标，包括提高了22％在预测每个标注者的打分上的性能，提高了33％在预测标注者之间方差上的性能，这提供了下游用来衡量模型不确定性的方法。我们发现可以使用标注者的人口统计信息和其在线意见来预测标注者的打分。

    Though majority vote among annotators is typically used for ground truth labels in natural language processing, annotator disagreement in tasks such as hate speech detection may reflect differences among group opinions, not noise. Thus, a crucial problem in hate speech detection is whether a statement is offensive to the demographic group that it targets, which may constitute a small fraction of the annotator pool. We construct a model that predicts individual annotator ratings on potentially offensive text and combines this information with the predicted target group of the text to model the opinions of target group members. We show gains across a range of metrics, including raising performance over the baseline by 22% at predicting individual annotators' ratings and 33% at predicting variance among annotators, which provides a method of measuring model uncertainty downstream. We find that annotators' ratings can be predicted using their demographic information and opinions on online 
    
[^147]: 大型语言模型中的字典链提示在翻译中的应用

    Chain-of-Dictionary Prompting Elicits Translation in Large Language Models. (arXiv:2305.06575v1 [cs.CL])

    [http://arxiv.org/abs/2305.06575](http://arxiv.org/abs/2305.06575)

    研究通过在大型语言模型中添加字典链提示的方法来改进低资源语言的翻译能力，实验结果表明能显著提高翻译质量。

    

    大型语言模型(LLMs)在多语言神经机器翻译(MNMT)中表现出惊人的性能，即使没有平行数据也能训练。然而，尽管训练数据量巨大，它们仍然难以翻译稀有词汇，特别是对于低资源语言。更糟糕的是，通常情况下，在低资源语言上，很难检索到相关示范来进行上下文学习，这限制了LLMs在翻译方面的实际应用——我们该如何缓解这个问题？为此，我们提出了一种新的方法，CoD，通过使用多语言字典链为一部分输入单词增加LLMs的先前知识，从而促进LLMs的翻译能力。广泛的实验表明，在FLORES-200全开发测试集上，通过将CoD和ChatGPT相结合，可以获得高达13倍的MNMT ChrF++分数的收益（英语到塞尔维亚语，西里尔字母书写，ChrF ++分数从3.08增加到42.63）。我们进一步展示了该方法在其他数据集上的重要作用。

    Large language models (LLMs) have shown surprisingly good performance in multilingual neural machine translation (MNMT) even when trained without parallel data. Yet, despite the fact that the amount of training data is gigantic, they still struggle with translating rare words, particularly for low-resource languages. Even worse, it is usually unrealistic to retrieve relevant demonstrations for in-context learning with low-resource languages on LLMs, which restricts the practical use of LLMs for translation -- how should we mitigate this problem? To this end, we present a novel method, CoD, which augments LLMs with prior knowledge with the chains of multilingual dictionaries for a subset of input words to elicit translation abilities for LLMs. Extensive experiments indicate that augmenting ChatGPT with CoD elicits large gains by up to 13x ChrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in Cyrillic script) on FLORES-200 full devtest set. We further demonstrate the im
    
[^148]: AlignSTS：通过跨模态对齐实现语音转唱

    AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment. (arXiv:2305.04476v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2305.04476](http://arxiv.org/abs/2305.04476)

    本文提出了基于跨模态对齐的STS模型AlignSTS，通过一种新颖的节奏适配器来预测目标节奏表示以弥合内容和音高之间的模态差距，并使用交叉注意力重新对齐内容进行跨模态融合重新合成。该模型表现优异。

    

    语音转唱 (STS) 任务旨在在面对一个主要挑战时，生成与语音录音相对应的唱歌样本：在没有文本的情况下，目标（唱歌）音高轮廓和源（语音）内容之间的对齐难以学习。本文提出了基于显式跨模态对齐的STS模型AlignSTS，将语音变化（如音高和内容）视为不同的模态。受人类如何唱出旋律的歌词机制的启发，AlignSTS: 1）采用一种新颖的节奏适配器来预测目标节奏表示，以弥合内容和音高之间的模态差距，其中节奏表示以简单而有效的方式计算，并量化为离散空间；2）使用预测的节奏表示基于交叉注意力重新对齐内容，并进行跨模态融合重新合成。广泛的实验表明，AlignSTS取得了优越的性能。

    The speech-to-singing (STS) voice conversion task aims to generate singing samples corresponding to speech recordings while facing a major challenge: the alignment between the target (singing) pitch contour and the source (speech) content is difficult to learn in a text-free situation. This paper proposes AlignSTS, an STS model based on explicit cross-modal alignment, which views speech variance such as pitch and content as different modalities. Inspired by the mechanism of how humans will sing the lyrics to the melody, AlignSTS: 1) adopts a novel rhythm adaptor to predict the target rhythm representation to bridge the modality gap between content and pitch, where the rhythm representation is computed in a simple yet effective way and is quantized into a discrete space; and 2) uses the predicted rhythm representation to re-align the content based on cross-attention and conducts a cross-modal fusion for re-synthesize. Extensive experiments show that AlignSTS achieves superior performanc
    
[^149]: LaMini-LM: 基于大规模指令的多样性压缩模型群集

    LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions. (arXiv:2304.14402v1 [cs.CL])

    [http://arxiv.org/abs/2304.14402](http://arxiv.org/abs/2304.14402)

    本文提出的LaMini-LM是一种基于大规模指令的多样性压缩模型群集，从指令微调过的LLMs中提取知识到更小的模型中，其在15个不同的NLP基准测试中与其他竞争基线的表现相当，但体积约小了10倍。

    

    指令微调的大型语言模型表现出优秀的生成能力，但是这些模型需要大量的资源。为了减轻这个问题，我们探索从微调过的LLMs中提取知识到更小的模型中。为此，我们仔细开发了一组258万份基于现有和新生成的指令。除了规模大之外，我们还设计了广泛的话题，以确保指令的多样性，并使用gpt-3.5-turbo为这些指令生成响应。我们使用这些指令来微调多个模型，即LaMini-LM，包括编码器-解码器和仅解码器系列。我们对这些模型进行自动（在15个不同的NLP基准测试中）和手动评估。结果表明，我们提出的LaMini-LM与其他竞争基线的表现相当，而且体积约小了10倍。

    Large language models (LLMs) with instruction finetuning demonstrate superior generative capabilities. However, these models are resource intensive. To alleviate this issue, we explore distilling knowledge from instruction-tuned LLMs to much smaller ones. To this end, we carefully develop a large set of 2.58M instructions based on both existing and newly-generated instructions. In addition to being sizeable, we design our instructions to cover a broad set of topics to ensure. A thorough investigation of our instruction data demonstrate their diversity, and we generate responses for these instructions using gpt-3.5-turbo. We then exploit the instructions to tune a host of models, dubbed LaMini-LM, of varying sizes, both from the encoder-decoder as well as the decoder-only families. We evaluate our models both automatically (on 15 different NLP benchmarks) and manually. Results show that our proposed LaMini-LM are on par with competitive baselines while being nearly 10 times smaller in s
    
[^150]: 为什么ChatGPT无法准确回答问题？

    Why Does ChatGPT Fall Short in Answering Questions Faithfully?. (arXiv:2304.10513v1 [cs.CL])

    [http://arxiv.org/abs/2304.10513](http://arxiv.org/abs/2304.10513)

    该论文分析了ChatGPT在问答系统中的失误，归纳并确定其失败的原因类型和关键能力，进一步提出了潜在方法来提高其准确性。

    

    最近，大型语言模型，如ChatGPT，展示出对人类生活各方面产生重大影响的巨大潜力。然而，ChatGPT在诚实性等方面仍然面临挑战。以问答系统为代表应用程序，我们试图了解为什么ChatGPT在准确回答问题方面有所不足。为了回答这个问题，我们试图分析ChatGPT在复杂的开放领域问答中失败的原因，并确定与这些失败有关的能力。具体来说，我们将ChatGPT的失败归为四种类型：理解、事实性、具体性和推理。我们进一步确定了与QA失败有关的三个关键能力：知识记忆、知识关联和知识推理。此外，我们还进行了围绕这些能力的实验，并提出了提高准确性的潜在方法。结果表明，向模型提供细粒度的外部知识、给予提示来帮助它聚焦并加强关键能力，这都有助于提高其准确性。

    Recent advancements in Large Language Models, such as ChatGPT, have demonstrated significant potential to impact various aspects of human life. However, ChatGPT still faces challenges in aspects like faithfulness. Taking question answering as a representative application, we seek to understand why ChatGPT falls short in answering questions faithfully. To address this question, we attempt to analyze the failures of ChatGPT in complex open-domain question answering and identifies the abilities under the failures. Specifically, we categorize ChatGPT's failures into four types: comprehension, factualness, specificity, and inference. We further pinpoint three critical abilities associated with QA failures: knowledge memorization, knowledge association, and knowledge reasoning. Additionally, we conduct experiments centered on these abilities and propose potential approaches to enhance faithfulness. The results indicate that furnishing the model with fine-grained external knowledge, hints for
    
[^151]: Baize:一种基于自我对话数据参数高效调整的开源聊天模型

    Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data. (arXiv:2304.01196v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.01196](http://arxiv.org/abs/2304.01196)

    提出了一种流程，通过利用ChatGPT与自身对话，能够自动生成高质量的多轮聊天语料库，并采用参数高效调整来增强开源的大型语言模型LLaMA，得到的模型被命名为Baize，在最小化潜在风险的护栏下，在多轮对话中展现出良好的性能。

    

    聊天模型，如ChatGPT，展现出惊人的能力，并在众多领域得到迅速应用。但是，这些模型只能通过受限制的API进行访问，从而制造了新的研究和领域进展的障碍。我们提出了一种流程，通过利用ChatGPT与自身对话，能够自动生成高质量的多轮聊天语料库。随后，我们采用参数高效调整来增强开源的大型语言模型LLaMA。所得到的模型被命名为Baize，在最小化潜在风险的护栏下，在多轮对话中展现出良好的性能。Baize的模型和数据仅用于研究目的，可在https://github.com/project-baize/baize进行下载。在线演示也可在https://huggingface.co/spaces/project-baize/baize-lora-7B进行访问。

    Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. The Baize models and data are released for research purposes only at https://github.com/project-baize/baize. An online demo is also available at https://huggingface.co/spaces/project-baize/baize-lora-7B.
    
[^152]: GPTEval：使用GPT-4和更好的人类对齐来评估NLG

    GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment. (arXiv:2303.16634v1 [cs.CL])

    [http://arxiv.org/abs/2303.16634](http://arxiv.org/abs/2303.16634)

    本文介绍了GPTEval，一个利用链式思考和形式填充评价NLG生成的质量。实验表明，在文本摘要任务中，GPTEval结合GPT-4取得了0.514的斯皮尔曼相关系数，胜过其他方法。

    

    自然语言生成（NLG）系统生成的文本质量很难进行自动测量。传统的基于参考的度量标准，如BLEU和ROUGE已被证明在需要创造性和多样性的任务中与人类判断的相关性相对较低。最近的研究建议使用大型语言模型（LLMs）作为无参考的NLG评估度量标准，这些模型适用于缺乏人类参考的新任务。然而，这些基于LLM的评估器仍然比中等规模的神经评估器的人类对应度低。在这项工作中，我们提出了GPTEval，一个使用链式思考（CoT）和形式填充范式来评估NLG输出质量的框架。我们针对两个生成任务，文本摘要和对话生成进行了实验。我们展示出，GPTEval结合GPT-4作为骨干模型，在摘要任务上实现了0.514的斯皮尔曼相关系数，胜过其他方法。

    The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present GPTEval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that GPTEval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperform
    
[^153]: InterviewBot：面向大学招生考试的实时端到端对话系统

    InterviewBot: Real-Time End-to-End Dialogue System to Interview Students for College Admission. (arXiv:2303.15049v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.15049](http://arxiv.org/abs/2303.15049)

    论文介绍了一种基于神经元的端到端对话模型- InterviewBot，该模型利用与输入的历史对话和定制主题集成到同一嵌入空间中来评估外国学生申请美国大学的学术和文化准备情况。同时，为克服基于变形金刚编码器-解码器模型的输入/输出大小限制，提出了上下文关注和主题存储两种新方法。该模型在统计和动态测试中表现出了流畅性和上下文感知的高度满意。

    

    我们提出了InterviewBot，它动态将会话历史和定制主题集成到一致的嵌入空间中，以进行10分钟的混合领域（开放和封闭）对话，以评估外国学生申请美国大学的学术和文化准备情况。为构建基于神经元的端到端对话模型，我们自动转录了7,361个人对人的面试音频记录，其中440个进行了手动纠正以进行微调和评估。为了克服基于变形金刚编码器-解码器模型的输入/输出大小限制，我们提出了两种新方法，上下文关注和主题存储，使模型能够进行相关和一致的交互。我们的最终模型在统计上被测试，通过将其响应与面试数据进行比较，并动态地邀请专业面试官和各种学生与其实时交互，在流畅性和上下文感知方面发现其非常令人满意。

    We present the InterviewBot that dynamically integrates conversation history and customized topics into a coherent embedding space to conduct 10 mins hybrid-domain (open and closed) conversations with foreign students applying to U.S. colleges for assessing their academic and cultural readiness. To build a neural-based end-to-end dialogue model, 7,361 audio recordings of human-to-human interviews are automatically transcribed, where 440 are manually corrected for finetuning and evaluation. To overcome the input/output size limit of a transformer-based encoder-decoder model, two new methods are proposed, context attention and topic storing, allowing the model to make relevant and consistent interactions. Our final model is tested both statistically by comparing its responses to the interview data and dynamically by inviting professional interviewers and various students to interact with it in real-time, finding it highly satisfactory in fluency and context awareness.
    
[^154]: 将您的无意义翻译：黑盒对抗攻击机器翻译系统

    Translate your gibberish: black-box adversarial attack on machine translation systems. (arXiv:2303.10974v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.10974](http://arxiv.org/abs/2303.10974)

    本文研究了机器翻译系统的黑盒对抗攻击方法，通过使用一种新颖的基于梯度自由张量的优化器，可以欺骗在线翻译工具产生错误的翻译，这影响了用户学习语言的体验。

    

    神经网络广泛部署于工业尺度的自然语言处理任务中，最常见的用途是自动机器翻译系统的组成部分。在本文中，我们提出了一种简单的方法来欺骗最先进的机器翻译工具在俄语和英语之间的翻译任务中。使用一种新颖的基于梯度自由张量的优化器，我们展示了许多在线翻译工具，如 Google、DeepL 和 Yandex，都可能为无意义的对抗性输入查询产生错误或冒犯性翻译，并拒绝翻译看似良性的输入短语。这种漏洞可能会干扰对一种新语言的理解，且在使用机器翻译系统时简单地恶化用户的体验，因此需要对这些工具进行额外的改进以建立更好的翻译。

    Neural networks are deployed widely in natural language processing tasks on the industrial scale, and perhaps the most often they are used as compounds of automatic machine translation systems. In this work, we present a simple approach to fool state-of-the-art machine translation tools in the task of translation from Russian to English and vice versa. Using a novel black-box gradient-free tensor-based optimizer, we show that many online translation tools, such as Google, DeepL, and Yandex, may both produce wrong or offensive translations for nonsensical adversarial input queries and refuse to translate seemingly benign input phrases. This vulnerability may interfere with understanding a new language and simply worsen the user's experience while using machine translation systems, and, hence, additional improvements of these tools are required to establish better translation.
    
[^155]: 技术报告：图神经网络也可以变得语法化

    Technical report: Graph Neural Networks go Grammatical. (arXiv:2303.01590v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01590](http://arxiv.org/abs/2303.01590)

    本文介绍了一种将代数语言片段与图神经网络形式上联系的框架，并从MATLANG定义了一个符合3-WL测试的语法，进而得出一个符合3-WL GNN模型的G$^2$N$^2$。此外，语法方法还提供了计算长度为六及以下的环和弦环的代数公式，并在多个下游任务中取得优秀的表现。

    

    本文提出了一个框架，将一个代数语言的一个片段与图神经网络（GNN）形式上联系起来。它依赖于上下文无关语法（CFG），将代数操作组织成可以翻译为GNN层模型的生成规则。由于直接从语言派生出的CFG的规则和变量包含冗余，因此介绍了一种语法简化方案，使得将其翻译为GNN层成为可能。应用这种策略，从MATLANG定义了一个符合第三阶Weisfeiler-Lehman（3-WL）测试要求的语法。从这个3-WL CFG中，我们得出了一个经过证明符合3-WL GNN模型的G$^2$N$^2$。此外，这种语法方法使我们能够提供计算长度为六及以下的环和弦环的代数公式，从而阐明了3-WL的计数能力。多个实验证明，G$^2$N$^2$在许多下游任务中的表现要比其他3-WL GNN更为高效。

    This paper proposes a framework to formally link a fragment of an algebraic language to a Graph Neural Network (GNN). It relies on Context Free Grammars (CFG) to organise algebraic operations into generative rules that can be translated into a GNN layer model. Since the rules and variables of a CFG directly derived from a language contain redundancies, a grammar reduction scheme is presented making tractable the translation into a GNN layer. Applying this strategy, a grammar compliant with the third-order Weisfeiler-Lehman (3-WL) test is defined from MATLANG. From this 3-WL CFG, we derive a provably 3-WL GNN model called G$^2$N$^2$. Moreover, this grammatical approach allows us to provide algebraic formulas to count the cycles of length up to six and chordal cycles at the edge level, which enlightens the counting power of 3-WL. Several experiments illustrate that G$^2$N$^2$ efficiently outperforms other 3-WL GNNs on many downstream tasks.
    
[^156]: 基于加权采样的遮蔽语言模型。

    Weighted Sampling for Masked Language Modeling. (arXiv:2302.14225v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.14225](http://arxiv.org/abs/2302.14225)

    本文提出了两种有效的加权采样策略用于遮蔽语言建模，可以有效解决高频率词汇导致的偏差问题，提高了语言模型的性能和迁移学习能力。

    

    遮蔽语言建模（MLM）被广泛用于预训练语言模型。MLM中标准的随机遮蔽策略会导致预训练的语言模型（PLMs）偏向于高频率的词汇。罕见词汇的表示学习效果较差，PLMs在下游任务中的性能有限。为了解决这个频率偏差问题，我们提出了两种简单而有效的基于词频和训练损失的加权遮蔽策略。我们将这两种策略应用于BERT，并获得了加权采样BERT（WSBERT）。在语义文本相似性基准（STS）上的实验表明，WSBERT显着改进了BERT的句子嵌入。将WSBERT与校准方法和提示学习相结合，进一步改善了句子嵌入。我们还调查了在GLUE基准上微调WSBERT，并表明加权采样也改善了骨干PLM的迁移学习能力。我们进一步分析并提供了见解。

    Masked Language Modeling (MLM) is widely used to pretrain language models. The standard random masking strategy in MLM causes the pre-trained language models (PLMs) to be biased toward high-frequency tokens. Representation learning of rare tokens is poor and PLMs have limited performance on downstream tasks. To alleviate this frequency bias issue, we propose two simple and effective Weighted Sampling strategies for masking tokens based on the token frequency and training loss. We apply these two strategies to BERT and obtain Weighted-Sampled BERT (WSBERT). Experiments on the Semantic Textual Similarity benchmark (STS) show that WSBERT significantly improves sentence embeddings over BERT. Combining WSBERT with calibration methods and prompt learning further improves sentence embeddings. We also investigate fine-tuning WSBERT on the GLUE benchmark and show that Weighted Sampling also improves the transfer learning capability of the backbone PLM. We further analyze and provide insights in
    
[^157]: STOA-VLP：用于视频-语言预训练的对象和动作空间-时间建模

    STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training. (arXiv:2302.09736v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.09736](http://arxiv.org/abs/2302.09736)

    STOA-VLP是一个视频-语言预训练框架，可以同时跨越空间和时间维度建模对象和动作信息，并且在HACS数据集上的实验结果比现有方法要好。

    

    尽管大规模的视频-语言预训练模型通常在各种下游任务上取得了显着进展，但在预训练阶段采用细粒度信息的想法并没有得到很好的探索。我们提出了STOA-VLP，这是一个预训练框架，可以跨越空间和时间维度同时建模对象和动作信息。实验结果表明，我们的STOA-VLP方法在HACS数据集上的性能显着优于现有方法，证明了我们提出的细粒度信息建模策略的有效性。

    Although large-scale video-language pre-training models, which usually build a global alignment between the video and the text, have achieved remarkable progress on various downstream tasks, the idea of adopting fine-grained information during the pre-training stage is not well explored. In this work, we propose STOA-VLP, a pre-training framework that jointly models object and action information across spatial and temporal dimensions. More specifically, the model regards object trajectories across frames and multiple action features from the video as fine-grained features. Besides, We design two auxiliary tasks to better incorporate both kinds of information into the pre-training process of the video-language model. The first is the dynamic object-text alignment task, which builds a better connection between object trajectories and the relevant noun tokens. The second is the spatial-temporal action set prediction, which guides the model to generate consistent action features by predict
    
[^158]: 测量编程语言分布的影响

    Measuring The Impact Of Programming Language Distribution. (arXiv:2302.01973v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01973](http://arxiv.org/abs/2302.01973)

    该研究提出了BabelCode框架和Translating Python Programming Puzzles（TP3）基准测试，探讨了平衡训练数据集中14种编程语言分布的影响。结果显示平衡分布有助于大型语言模型在低资源语言上的性能提升。

    

    目前用于评估神经代码模型的基准测试只集中在很少的一部分编程语言上，不包括许多流行的语言，例如Go或Rust。为了解决这个问题，我们提出了BabelCode框架，用于基于执行的评估任何语言中的任何基准测试。BabelCode使得可以对模型的内存、运行时间和单个测试案例结果进行新的定性性能调查。此外，我们还提供了一个名为Translating Python Programming Puzzles（TP3）的新代码翻译数据集，该数据集来自Python Programming Puzzles（Schuster等人，2021）基准测试，涉及将专家级Python函数翻译成任何语言。通过对BabelCode和TP3基准测试的研究，我们探讨了在训练数据集中平衡14种语言的分布是否可以提高大型语言模型在低资源语言上的性能。在平衡语料库上训练模型，平均而言，相对于不平衡分布的情况，所有任务和语言的$pass@k$结果平均提高了12.34%。

    Current benchmarks for evaluating neural code models focus on only a small subset of programming languages, excluding many popular languages such as Go or Rust. To ameliorate this issue, we present the BabelCode framework for execution-based evaluation of any benchmark in any language. BabelCode enables new investigations into the qualitative performance of models' memory, runtime, and individual test case results. Additionally, we present a new code translation dataset called Translating Python Programming Puzzles (TP3) from the Python Programming Puzzles (Schuster et al. 2021) benchmark that involves translating expert-level python functions to any language. With both BabelCode and the TP3 benchmark, we investigate if balancing the distributions of 14 languages in a training dataset improves a large language model's performance on low-resource languages. Training a model on a balanced corpus results in, on average, 12.34% higher $pass@k$ across all tasks and languages compared to the
    
[^159]: Replug：检索增强的黑盒语言模型

    Replug: Retrieval-augmented black-box language models. (arXiv:2301.12652v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.12652](http://arxiv.org/abs/2301.12652)

    REPLUG是一种检索增强的语言建模框架，它通过在黑盒语言模型输入前添加检索到的文档来增强模型，同时可以通过LM监督检索模型来提高预测性能。

    

    我们介绍了 REPLUG，一种检索增强的语言建模框架，将语言模型（LM）视为黑盒，并用可调整的检索模型增强它。与以前通过特殊的交叉关注机制对语言模型进行训练以对检索文本进行编码的检索增强型LM不同，REPLUG仅将检索到的文档前置到冻结的黑盒LM输入中。这种简单的设计可以轻松地应用于任何现有的检索和语言模型。此外，我们显示LM可以用于监督检索模型，该检索模型可以找到帮助LM进行更好预测的文档。我们的实验证明，在调整了检索器的情况下，REPLUG可以使GPT-3（175B）的语言建模性能提高6.3％，同时使Codex在五次测试MMLU时性能提高5.1％。

    We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%.
    
[^160]: MGeo：多模态地理预训练方法

    MGeo: Multi-Modal Geographic Pre-Training Method. (arXiv:2301.04283v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.04283](http://arxiv.org/abs/2301.04283)

    本文提出了一种新的查询-POI匹配方法MGeo，通过结合多模态数据，有效利用多样化的地理信息上下文提高了性能。

    

    查询与兴趣点 (POI) 匹配是基于位置的服务 (LBS) 中的核心任务（如导航地图），它将用户的意图与真实世界的地理信息相连接。最近，预训练模型 (PTM) 在许多自然语言处理 (NLP) 任务中都取得了进展。通用的基于文本的 PTM 并没有足够的地理知识用于查询-POI 匹配。为了克服这个局限性，相关文献尝试根据地理相关语料库采用领域自适应预训练。但是，查询通常包含多个地理对象的提及，比如附近的道路和利益区域（ROIs）。这些多样化的地理对象及其关系被称为地理上下文（GC）,对于检索最相关的 POI，是至关重要的。单模态 PTM 很难利用重要的地理上下文，因此性能受限。在这项工作中，我们提出了一种新的查询-POI 匹配方法“多模态地理语言模型”(MGeo)，它包括......

    As a core task in location-based services (LBS) (e.g., navigation maps), query and point of interest (POI) matching connects users' intent with real-world geographic information. Recently, pre-trained models (PTMs) have made advancements in many natural language processing (NLP) tasks. Generic text-based PTMs do not have enough geographic knowledge for query-POI matching. To overcome this limitation, related literature attempts to employ domain-adaptive pre-training based on geo-related corpus. However, a query generally contains mentions of multiple geographic objects, such as nearby roads and regions of interest (ROIs). The geographic context (GC), i.e., these diverse geographic objects and their relationships, is therefore pivotal to retrieving the most relevant POI. Single-modal PTMs can barely make use of the important GC and therefore have limited performance. In this work, we propose a novel query-POI matching method Multi-modal Geographic language model (MGeo), which comprises 
    
[^161]: 面向开放领域的多文档摘要

    Towards multi-document summarization in the open-domain. (arXiv:2212.10526v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10526](http://arxiv.org/abs/2212.10526)

    本论文针对"开放领域"的多文档摘要任务进行研究，发现最先进的摘要器在此情境下性能下降严重，但进行额外的开放领域的训练可以降低对不完美检索的敏感性。

    

    多文档摘要(MDS)通常假设提供一组主题相关的文档。但是，这个文档集通常是数据集策划过程的产物；在实践中，它不一定可用，需要根据信息需求，即问题或主题陈述进行检索。我们通过形式化任务并使用现有数据集，检索器和摘要器来引导这个更具挑战性的“开放领域”设置的研究。通过广泛的实验，我们确定：(1)即使检索性能较高，最先进的摘要器在应用于开放领域时也会大幅降低性能;(2)在开放领域的设置中进行额外的训练可以降低对不完美检索的敏感性，(3)摘要器对检索重复文档和检索文档的顺序不敏感，但对其他错误，如检索无关文档的敏感性较高。根据我们的研究结果，我们提供了

    Multi-document summarization (MDS) traditionally assumes a set of topic-related documents are provided. However, this document set is often an artifact of the dataset curation process; in practice, it is not necessarily available and would need to be retrieved given an information need, i.e. a question or topic statement. We study this more challenging "open-domain" setting by formalizing the task and bootstrapping it using existing datasets, retrievers and summarizers. Via extensive experimentation, we determine that: (1) state-of-the-art summarizers suffer large reductions in performance when applied to the open-domain, even when retrieval performance is high, (2) additional training in the open-domain setting can reduce this sensitivity to imperfect retrieval, and (3) summarizers are insensitive to the retrieval of duplicate documents and the order of retrieved documents, but highly sensitive to other errors, like the retrieval of irrelevant documents. Based on our results, we provi
    
[^162]: DePlot：利用图表转表格翻译的一次性视觉语言推理

    DePlot: One-shot visual language reasoning by plot-to-table translation. (arXiv:2212.10505v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10505](http://arxiv.org/abs/2212.10505)

    本论文提出了一种名为DePlot的方法，它是一个模态转换模块，能够将图表的图像转换为线性化的表格。利用这个模块和预训练的大型语言模型的少量推理能力，我们开发了一种新的一次性视觉语言推理方法。

    

    视觉语言如图表和图形在人类世界中无处不在，理解图表需要强大的推理能力。本论文提出了一种新的一次性视觉语言推理方法，将视觉语言推理的挑战分解为两个步骤：（1）图形到文本的翻译，（2）对翻译后的文本进行推理。该方法的关键是一个称为DePlot的模态转换模块，它将图表的图像转换为线性表格。DePlot的输出可以直接用于启动预训练的大型语言模型，利用大型语言模型的少量推理能力。为了获得DePlot，我们通过建立统一的任务格式和指标，对图形到表格的任务进行标准化，并在此任务上对DePlot进行端到端训练。

    Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this tas
    
[^163]: 当前的任务导向对话模型能否处理真实世界中的场景？

    Can Current Task-oriented Dialogue Models Automate Real-world Scenarios in the Wild?. (arXiv:2212.10504v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10504](http://arxiv.org/abs/2212.10504)

    任务导向对话系统基于槽填充框架取得了显著的成功，但当前的 TOD 基准测试局限于代理真实世界情景。WebTOD 是建立可伸缩 TOD 系统的另一种方向，可以实现对 Web/移动接口的理解。

    

    任务导向对话系统主要基于基于槽填充框架的任务导向对话（SF-TOD）框架，在该框架下对话被分解为更小的、可控制的单元（即槽），以完成特定的任务。一系列基于该框架的方法在各种 TOD 基准测试中取得了显著的成功。然而，我们认为当前的 TOD 基准测试局限于代理真实世界情景，当前的 TOD 模型仍有很长的路要走。在这篇观点论文中，我们首先确定了 SF-TOD 系统的当前状态和局限性。之后，我们探讨了 WebTOD 框架，这是建立可伸缩 TOD 系统的另一种方向，当 Web/移动接口可用时可以实现。在 WebTOD 中，对话系统通过大规模语言模型的支持，学习如何理解人类代理与之交互的 Web/移动接口。

    Task-oriented dialogue (TOD) systems are mainly based on the slot-filling-based TOD (SF-TOD) framework, in which dialogues are broken down into smaller, controllable units (i.e., slots) to fulfill a specific task. A series of approaches based on this framework achieved remarkable success on various TOD benchmarks. However, we argue that the current TOD benchmarks are limited to surrogate real-world scenarios and that the current TOD models are still a long way to cover the scenarios. In this position paper, we first identify current status and limitations of SF-TOD systems. After that, we explore the WebTOD framework, the alternative direction for building a scalable TOD system when a web/mobile interface is available. In WebTOD, the dialogue system learns how to understand the web/mobile interface that the human agent interacts with, powered by a large-scale language model.
    
[^164]: SODA: 具有社交常识语境化的百万规模对话蒸馏

    SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization. (arXiv:2212.10465v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10465](http://arxiv.org/abs/2212.10465)

    SODA是第一个公开发布的百万级别高质量社交对话数据集，通过从知识图谱中上下文化社交常识知识进行蒸馏，我们训练出了COSMO，其比目前最佳表现的对话模型更为自然和一致，这有助于了解知识丰富型对话和自然社交闲聊之间的差异。

    

    我们提出了SODA：第一个公开可用的百万规模高质量社交对话数据集。与大多数现有的众包小规模对话语料库不同，我们通过从知识图谱（Atomic10x; West等人，2022）中的社交常识知识进行上下文化，提炼了150万个社交对话。人类评估表明，SODA中的对话比以前的由人类撰写的数据集更一致、更具体且（令人惊讶地）更自然。我们使用SODA训练了COSMO：一个通用的对话模型，在未知的数据集上比最佳表现的对话模型（例如GODEL、BlenderBot-1、Koala、Vicuna）更自然和一致。实验结果表明，COSMO有时甚至被认为优于原始的人工编写的标准回答。此外，我们的结果揭示了知识丰富型对话和自然社交闲聊之间的区别。

    We present SODA: the first publicly available, million-scale high-quality social dialogue dataset. In contrast to most existing crowdsourced, small-scale dialogue corpora, we distill 1.5M socially-grounded dialogues from a large language model (InstructGPT; Ouyang et al., 2022). Dialogues are distilled by contextualizing social commonsense knowledge from a knowledge graph (Atomic10x; West et al., 2022). Human evaluation shows that dialogues in SODA are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets.  Using SODA, we train COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. 
    
[^165]: 你所谓的令牌是关于什么的？稠密检索作为词汇表上的分布。

    What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary. (arXiv:2212.10380v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10380](http://arxiv.org/abs/2212.10380)

    本文探讨了双编码器用于稠密检索的机制，通过将向量表示投影到模型的词汇表空间来解释它们，进一步解释了一些失败案例，提出了一种简单的方法在推理时丰富查询和段落表示与词汇信息，显著提高了性能。

    

    双编码器现在是稠密检索的主要架构。然而，我们对它们如何表示文本以及为什么会导致良好性能知之甚少。在本文中，我们通过词汇表上的分布来阐明这个问题。我们建议通过将双编码器产生的向量表示投影到模型的词汇表空间中来解释它们，我们展示了产生的投影包含丰富的语义信息，并将它们与稀疏检索之间进行联系。我们发现，这种观点可以解释稠密检索器的一些失败案例。例如，我们观察到模型无法处理尾部实体与令牌分布倾向于忘记这些实体的某些令牌之间存在相关性。我们利用了这一洞察，并提出了一种在推理时丰富查询和段落表示与词汇信息的简单方法，并展示了这相比于常规的双编码器有显著的性能提升。

    Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model's vocabulary space. We show that the resulting projections contain rich semantic information, and draw connection between them and sparse retrieval. We find that this view can offer an explanation for some of the failure cases of dense retrievers. For example, we observe that the inability of models to handle tail entities is correlated with a tendency of the token distributions to forget some of the tokens of those entities. We leverage this insight and propose a simple way to enrich query and passage representations with lexical information at inference time, and show that this significantly improves performance compared to 
    
[^166]: 通过感受野分析透视Transformer长度外推

    Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis. (arXiv:2212.10356v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10356](http://arxiv.org/abs/2212.10356)

    该论文通过感受野分析透视了Transformer长度外推中的相对位置嵌入设计，提出了第一个真正使用比训练序列长的长度信息的无参数相对位置嵌入设计Sandwich。

    

    长度外推允许在短序列上训练Transformer语言模型，当测试较长序列时仍保持困惑度。相对位置嵌入设计ALiBi到目前为止已被广泛使用。我们通过一个新颖的累积标准化梯度工具，通过感受野分析解剖了ALiBi。感受野的概念进一步允许我们修改香草正弦位置嵌入，创建了~\textbf{Sandwich}，这是第一个真正使用比训练序列长的长度信息的无参数相对位置嵌入设计。Sandwich与KERPLE和T5共享具有可学习相对位置嵌入的对数衰减时间偏差模式；这些揭示了未来可外推的位置嵌入设计。

    Length extrapolation permits training a transformer language model on short sequences that preserves perplexities when tested on substantially longer sequences. A relative positional embedding design, ALiBi, has had the widest usage to date. We dissect ALiBi via the lens of receptive field analysis empowered by a novel cumulative normalized gradient tool. The concept of receptive field further allows us to modify the vanilla Sinusoidal positional embedding to create ~\textbf{Sandwich}, the first parameter-free relative positional embedding design that truly length information uses longer than the training sequence. Sandwich shares with KERPLE and T5 the same logarithmic decaying temporal bias pattern with learnable relative positional embeddings; these elucidate future extrapolatable positional embedding design.
    
[^167]: CoCoMIC: 基于联合建模文件内外上下文的代码补全

    CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context. (arXiv:2212.10007v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10007](http://arxiv.org/abs/2212.10007)

    CoCoMIC是一个能联合建模文件内外上下文的代码补全框架，其中包括一个跨文件上下文查找工具CCFINDER。使用该框架相对提高了33.94％的精确匹配率和28.6％的前5个候选项准确率。

    

    预训练的代码语言模型在代码补全方面取得了巨大成功，但它们只根据文件内的内容进行代码补全，即考虑文件内上下文，而忽略了同一项目中其他文件中丰富的语义信息，即跨文件上下文，后者是现代模块化软件开发中特别有用的关键信息来源。这种忽视限制了代码语言模型在代码补全中的能力，导致出现意外行为，例如生成幻想的类成员函数或带有意外参数的函数调用。在这项工作中，我们开发了一个跨文件上下文查找工具CCFINDER，该工具能够有效地定位并检索最相关的跨文件上下文。我们提出了CoCoMIC框架，它在预先训练的代码语言模型之上，将文件内和跨文件上下文联合进行学习。CoCoMIC成功地提高了现有的代码语言模型，在精确匹配率方面相对提高了33.94％，在前5个候选项准确率方面相对提高了28.6％，这证明了对于代码补全，建模跨文件上下文是非常有效的。

    While pre-trained language models (LM) for code have achieved great success in code completion, they generate code conditioned only on the contents within the file, i.e., in-file context, but ignore the rich semantics in other files within the same project, i.e., cross-file context, a critical source of information that is especially useful in modern modular software development. Such overlooking constrains code language models' capacity in code completion, leading to unexpected behaviors such as generating hallucinated class member functions or function calls with unexpected arguments. In this work, we develop a cross-file context finder tool, CCFINDER, that effectively locates and retrieves the most relevant cross-file context. We propose CoCoMIC, a framework that incorporates cross-file context to learn the in-file and cross-file context jointly on top of pretrained code LMs. CoCoMIC successfully improves the existing code LM with a 33.94% relative increase in exact match and a 28.6
    
[^168]: MatCha：数学推理和图表解析增强视觉语言预训练

    MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering. (arXiv:2212.09662v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09662](http://arxiv.org/abs/2212.09662)

    本文提出了一种新的预训练方法 MatCha ，提高了视觉语言模型在图表和语言数据联合建模方面的能力，取得了近20%的性能提升。

    

    视觉语言数据在人类世界中非常普遍，但是最先进的视觉语言模型在这些数据上的表现并不理想。本论文提出 MatCha（数学推理和图表解析预训练）来增强视觉语言模型在联合建模图表/绘图和语言数据方面的能力。我们提出了几个预训练任务，这些任务涵盖了绘图拆解和数字推理这些视觉语言模型的关键能力。在标准基准测试中，MatCha 模型在 PlotQA 和 ChartQA 等方面的表现均超过了最先进的方法近 20%。我们还检查了 MatCha 预训练在截图、教科书图示和文档图形等领域的迁移能力，并观察到总体改善，这验证了 MatCha 预训练在更广泛的视觉语言任务上的有用性。

    Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art vision-language models do not perform well on these data. We propose MatCha (Math reasoning and Chart derendering pretraining) to enhance visual language models' capabilities in jointly modeling charts/plots and language data. Specifically, we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling.  We perform the MatCha pretraining starting from Pix2Struct, a recently proposed image-to-text visual language model. On standard benchmarks such as PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as much as nearly 20%. We also examine how well MatCha pretraining transfers to domains such as screenshots, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MatCha pretraining on broader visual language tas
    
[^169]: 大型语言模型是带有自我验证的推理器

    Large Language Models are reasoners with Self-Verification. (arXiv:2212.09561v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.09561](http://arxiv.org/abs/2212.09561)

    本文提出了一种新的自我验证方法，使用CoT的结论来构建新样本并要求LLM重新预测原始条件，以提高推理准确性。实验证明，LLMs可以对其自己的结论进行自我验证并实现竞争性的推理性能。

    

    当大型语言模型（LLM）通过思维链（CoT）进行复杂推理时，它非常敏感于个别错误。为了解决这个问题，我们必须训练验证器。我们提出一种称为自我验证的新方法，该方法使用CoT的结论作为条件来构建一个新样本，并要求LLM重新预测被掩盖的原始条件。我们基于准确性计算可解释的验证分数。该方法可以在使用少量样本学习时提高多个算术和逻辑推理数据集的准确性。我们已经证明LLM可以对其自己的结论进行可解释的自我验证并实现竞争性的推理性能。全面的实验表明，我们的方法可以帮助多种带有自我验证功能的大型语言模型避免混淆。

    When a large language model (LLM) performs complex reasoning by chain of thought (CoT), it can be highly sensitive to individual mistakes. We have had to train verifiers to address this issue. As we all know, after human inferring a conclusion, they often check it by re-verifying it, which can avoid some mistakes. We propose a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked. We calculate an explainable verification score based on the accuracy. This method can improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. we have demonstrated that LLMs can conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Extensive experimentals have demonstrated that our method can help multiple large language models with self-verification can avoid interference from inco
    
[^170]: 解码器调整：将高效语言理解作为解码

    Decoder Tuning: Efficient Language Understanding as Decoding. (arXiv:2212.08408v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.08408](http://arxiv.org/abs/2212.08408)

    本文提出了解码器调整（DecT）方法，通过优化任务特定的解码器网络来适应预训练模型的输出端，避免了传统方法中输入端的高计算和时间成本。

    

    随着预训练模型（PTMs）的规模不断增加，只向用户提供推理API（即模型为服务（MaaS）设置）已成为一种新兴的做法。为了适应参数冻结的PTMs，大多数现有方法集中在输入端，寻找强有力的提示来刺激模型产生正确的答案。然而，我们认为输入端的适应可能很困难，因为缺少梯度信号，并且通常需要数千个API查询，导致高计算和时间成本。基于此，我们提出了解码器调整（DecT），它与当前方法相反，通过优化特定于任务的解码器网络来适应PTMs的输出端。具体来说，DecT首先提取被提示刺激的输出分数作为初始预测。在此基础上，我们还在输出表示上训练了另一个解码器网络，以结合后验数据知识。通过基于梯度的优化，DecT可以在几秒钟内训练，并且每个样本只需要一个PTM查询。

    With the evergrowing sizes of pre-trained models (PTMs), it has been an emerging practice to only provide the inference APIs for users, namely model-as-a-service (MaaS) setting. To adapt PTMs with model parameters frozen, most current approaches focus on the input side, seeking for powerful prompts to stimulate models for correct answers. However, we argue that input-side adaptation could be arduous due to the lack of gradient signals and they usually require thousands of API queries, resulting in high computation and time costs. In light of this, we present Decoder Tuning (DecT), which in contrast optimizes task-specific decoder networks on the output side. Specifically, DecT first extracts prompt-stimulated output scores for initial predictions. On top of that, we train an additional decoder network on the output representations to incorporate posterior data knowledge. By gradient-based optimization, DecT can be trained within several seconds and requires only one PTM query per sampl
    
[^171]: 通过概率密度估计在潜空间中进行可控文本生成

    Controllable Text Generation via Probability Density Estimation in the Latent Space. (arXiv:2212.08307v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.08307](http://arxiv.org/abs/2212.08307)

    这篇论文提出了一种利用概率密度估计在潜空间中进行可控文本生成的新框架，并采用可逆变换函数将复杂分布映射到先验空间中的简单高斯分布，实现了一种灵活、高效、高质量的文本生成方法。

    

    先前关于可控文本生成的研究探索了来自潜空间的控制思想，例如通过属性相关分类器优化表示或从相关离散样本中采样表示。然而，它们在建模潜空间和控制方面均不够有效，导致受控制的文本质量和多样性较低。在本研究中，我们提出了一种新颖的控制框架，利用潜空间中的概率密度估计。我们的方法利用可逆变换函数（正则流），将潜空间中的复杂分布映射到先验空间中简单的高斯分布。因此，我们可以在先验空间中执行复杂而灵活的控制，并由于可逆变换的单一映射属性将控制效果反馈到潜空间中。单属性控制和多属性控制实验表明，我们的方法优于几种强大的基准方法。

    Previous work on controllable text generation has explored the idea of control from the latent space, such as optimizing a representation with attribute-related classifiers or sampling a representation from relevant discrete samples. However, they are not effective enough in modeling both the latent space and the control, leaving controlled text with low quality and diversity. In this work, we propose a novel control framework using probability density estimation in the latent space. Our method utilizes an invertible transformation function, the Normalizing Flow, that maps the complex distributions in the latent space to simple Gaussian distributions in the prior space. Thus, we can perform sophisticated and flexible control in the prior space and feed the control effects back into the latent space owing to the one-one-mapping property of invertible transformations. Experiments on single-attribute controls and multi-attribute control reveal that our method outperforms several strong ba
    
[^172]: 人类和语言模型之间实用语言理解的精细比较

    A fine-grained comparison of pragmatic language understanding in humans and language models. (arXiv:2212.06801v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.06801](http://arxiv.org/abs/2212.06801)

    本文对于人类和语言模型在实用语言理解方面进行了实验比较，研究发现较大的语言模型在实现字面解释方面表现较好，但在依赖社交方面的现象方面仍存在困难。

    

    实用语言理解是人类交流中至关重要且不易理解的方面，对于人造语言模型来说这一点仍然存在很大的挑战。本文通过零基础提示下对英语材料进行了7种实用现象的语言模型和人类的精细比较，探讨模型在解释发言者表达时是否具有语用理解、是否和人类存在相似的错误类型和解决这些任务时使用相似的语言线索。研究发现较大的语言模型能够获得高精度并且与人类的错误模式相匹配：在不正确的答案中，模型更倾向于字面上的解释。我们还发现初步证据表明，模型和人类对相似的语言线索也很敏感。研究结果表明，即使没有明确构建的心理状态表示，模型也可以表现出实用行为。但是，模型在依赖社交方面的现象方面仍存在一定困难。

    Pragmatics and non-literal language understanding are essential to human communication, and present a long-standing challenge for artificial language models. We perform a fine-grained comparison of language models and humans on seven pragmatic phenomena, using zero-shot prompting on an expert-curated set of English materials. We ask whether models (1) select pragmatic interpretations of speaker utterances, (2) make similar error patterns as humans, and (3) use similar linguistic cues as humans to solve the tasks. We find that the largest models achieve high accuracy and match human error patterns: within incorrect responses, models favor literal interpretations over heuristic-based distractors. We also find preliminary evidence that models and humans are sensitive to similar linguistic cues. Our results suggest that pragmatic behaviors can emerge in models without explicitly constructed representations of mental states. However, models tend to struggle with phenomena relying on social 
    
[^173]: 为确保印度语不被落下：建立单语语料库、基准和印度语模型

    Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages. (arXiv:2212.05409v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.05409](http://arxiv.org/abs/2212.05409)

    本文建立了适用于超过十亿印度语言使用者的自然语言理解能力的三个重要贡献，包括最大的单语语料库，包括20.9B的记号，涵盖24种语言，以及人工监督的基准数据集IndicXTREME，包括9种不同类型的NLU任务，跨越20种语言。这是首个旨在测试印度语言的多语言文本理解标准基准。

    

    建立适用于超过十亿印度语言使用者的自然语言理解（NLU）能力至关重要。本文旨在通过三个重要方面的贡献来改善印度语言的NLU能力，包括(i) 单语语料库 (ii) NLU测试集 (iii) 针对印度语言的多语言LLM。具体而言，我们创建了最大的单语语料库IndicCorp，在4个语言家族的24种语言中涵盖20.9B个标记，2.3倍于以前的工作，在支持12种其他语言的同时。接下来，我们创建一个人工监督的基准数据集IndicXTREME，涵盖20种语言的九种不同类型的NLU任务。跨语言和任务，IndicXTREME共包含105个评估集，其中52个是对文献的新贡献。据我们所知，这是首个旨在测试印度语言的多语言文本理解标准基准。

    Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work, while supporting 12 additional languages. Next, we create a human-supervised benchmark, IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions to the literature. To the best of our knowledge, this is the first effort towards creating a standard benchmark for Indic languages that aims to test the multilingual z
    
[^174]: 迈向人类兼容自动驾驶汽车：情感过渡模型中的非语言图灵测试研究

    Towards human-compatible autonomous car: A study of non-verbal Turing test in automated driving with affective transition modelling. (arXiv:2212.02908v5 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2212.02908](http://arxiv.org/abs/2212.02908)

    本文研究了自动驾驶汽车的人性化问题，通过非语言图灵测试，发现当前AI驾驶员不能创造类似人类的驾乘体验，需要在情感过渡模型中进行改进。

    

    当人类走向无需双手的生活方式时，自动驾驶汽车是不可或缺的。现有文献强调，如果自动驾驶汽车能够以类似人类的方式驾驶，人们会更容易接受它。然而，仅有少量研究从乘客角度的自然体验来检验目前的自动驾驶汽车是否具有类似人类的特征。本研究通过一项真实道路环境下的试验，测试了69位参与者的反馈，尝试了解AI驾驶人员能否为乘客创造类似人类的驾乘体验。我们设计了一种基于驾乘体验的非语言图灵测试，要求参与者作为乘客乘坐AI驾驶人员或人类驾驶人员驾驶的自动驾驶汽车，并判断司机是人类还是AI。结果显示，充当AI驾驶员的汽车未能通过我们的测试，因为乘客能够超过随机猜测地识别出AI驾驶员。相比之下，当人类驾驶员驾驶车辆时，乘客的判断结果约在随机猜测水平附近。我们进一步探讨了人类乘客在驾驶过程中给予了哪些人性化特征的归因。

    Autonomous cars are indispensable when humans go further down the hands-free route. Although existing literature highlights that the acceptance of the autonomous car will increase if it drives in a human-like manner, sparse research offers the naturalistic experience from a passenger's seat perspective to examine the human likeness of current autonomous cars. The present study tested whether the AI driver could create a human-like ride experience for passengers based on 69 participants' feedback in a real-road scenario. We designed a ride experience-based version of the non-verbal Turing test for automated driving. Participants rode in autonomous cars (driven by either human or AI drivers) as a passenger and judged whether the driver was human or AI. The AI driver failed to pass our test because passengers detected the AI driver above chance. In contrast, when the human driver drove the car, the passengers' judgement was around chance. We further investigated how human passengers ascri
    
[^175]: SciRepEval：一个用于科学文献表示的多格式基准

    SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.13308](http://arxiv.org/abs/2211.13308)

    SciRepEval是第一个综合评估科学文献表示的全面基准，其中包括四种格式的 25 个任务。通过使用格式特定的控制代码和适配器，可以改进科学文献表示模型的泛化能力。

    

    学习的科学文献表示可以作为下游任务的有价值输入特征，无需进一步微调。然而，用于评估这些表示的现有基准未能捕捉到相关任务的多样性。为此，我们介绍了 SciRepEval，第一个用于训练和评估科学文献表示的全面基准。它包括四种格式的 25 个具有挑战性和现实性的任务，其中 11 个是新任务：分类、回归、排名和搜索。我们使用该基准来研究和改进科学文档表示模型的泛化能力。我们展示了最先进的模型如何在任务格式方面缺乏泛化性能，简单的多任务训练也不能改进它们。然而，一种新的方法，学习每个文档的多个嵌入，每个嵌入专门针对不同的格式，可以提高性能。我们尝试使用任务格式特定的控制代码和适配器。

    Learned representations of scientific documents can serve as valuable input features for downstream tasks, without the need for further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks, 11 of which are new, across four formats: classification, regression, ranking and search. We then use the benchmark to study and improve the generalization ability of scientific document representation models. We show how state-of-the-art models struggle to generalize across task formats, and that simple multi-task training fails to improve them. However, a new approach that learns multiple embeddings per document, each tailored to a different format, can improve performance. We experiment with task-format-specific control codes and adapters in 
    
[^176]: 文本中的离群检测的多层知识蒸馏

    Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text. (arXiv:2211.11300v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.11300](http://arxiv.org/abs/2211.11300)

    本文提出了一种多层知识蒸馏方法，融合了语言模型的训练和微调方法来进行文本中的离群检测，实验结果表明其有效性。

    

    自监督表示学习已经证明是只使用内分布(ID)样例文本进行离群检测的宝贵组成部分。这些方法要么从头开始训练语言模型，要么通过使用ID样例微调预训练语言模型，然后将语言模型输出的困惑度作为离群得分。本文分析了两种离群检测方法的互补特性，并提出了一种融合它们优势并减轻它们的局限性的多层知识蒸馏方法。具体而言，我们使用微调模型作为老师，在ID示例上教授一个随机初始化的学生模型。除了预测层蒸馏外，我们还提出了一种基于相似性的中间层蒸馏方法，以全面探索老师模型的表示空间。通过这种方式，学习的学生可以更好地表示ID数据流形，同时获得更强的将OoD示例映射到流形之外的能力。基准数据集上的实验结果证明了我们所提出的方法与竞争基线相比的有效性。

    Self-supervised representation learning has proved to be a valuable component for out-of-distribution (OoD) detection with only the texts of in-distribution (ID) examples. These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and then take the perplexity output by the language model as OoD scores. In this paper, we analyze the complementary characteristics of both OoD detection methods and propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations. Specifically, we use a fine-tuned model as the teacher to teach a randomly initialized student model on the ID examples. Besides the prediction layer distillation, we present a similarity-based intermediate layer distillation method to thoroughly explore the representation space of the teacher model. In this way, the learned student can better represent the ID data manifold while gaining a stronger ability to map O
    
[^177]: 基于概念器辅助的大型语言模型去偏见

    Conceptor-Aided Debiasing of Large Language Models. (arXiv:2211.11087v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.11087](http://arxiv.org/abs/2211.11087)

    本论文提出一种基于概念器的大型语言模型去偏见方法。我们通过后处理和一种新架构CI-BERT将概念器投影纳入所有层中。概念器后处理方法取得了最先进的去偏见结果，同时保持或改善了模型的性能。

    

    预训练的大型语言模型(LLMs)反映了它们训练语料库中固有的社会偏见。许多方法已被提出来减轻这个问题，但它们通常未能去偏见或者会牺牲模型的准确性。我们使用概念器——一种软投影方法——来识别和去除如BERT和GPT等LLMs中的偏见子空间。我们提出了两种应用概念器的方法：（1）通过后处理进行偏见子空间投影；（2）一种新的架构——概念器介入BERT(CI-BERT)，它在训练期间明确地将概念器投影纳入所有层中。我们发现，概念器后处理在保持或提高LLMs在GLUE基准测试中的性能的同时，实现了最先进的去偏见结果。此外，它在各种情况下都很稳健，并且可以通过对现有偏见子空间的逻辑操作来有效地减轻交集偏见。虽然CI-BERT的训练考虑了所有层的偏见，并且在某些任务上表现更好，但它的训练成本更高。

    Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. We use conceptors--a soft projection method--to identify and remove the bias subspace in LLMs such as BERT and GPT. We propose two methods of applying conceptors (1) bias subspace projection by post-processing; and (2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly incorporates the conceptor projection into all layers during training. We find that conceptor post-processing achieves state-of-the-art (SoTA) debiasing results while maintaining or improving LLMs' performance on the GLUE benchmark. Also, it is robust in various scenarios and can mitigate intersectional bias efficiently by its logical operation on the existing bias subspaces. Although CI-BERT's training takes all layers' bias into account and can beat its post-processing counterpa
    
[^178]: 动态记号池化的高效Transformer

    Efficient Transformers with Dynamic Token Pooling. (arXiv:2211.09761v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.09761](http://arxiv.org/abs/2211.09761)

    本论文提出了一种动态记号池化机制，可以在自然意义单位具有不同长度的情况下，缩短中间层的序列长度。实验证明，相比具有固定池化的普通Transformer，动态池化机制既更快，更准确地进行分段和语言建模。

    

    Transformer模型在自然语言处理方面的性能表现卓越，但在内存和时间复杂度方面效率低下。一种可能的解决方法是通过固定长度的片段池化记号来缩短中间层的序列长度。然而，诸如单词或短语之类的自然意义单位具有不同的大小。为解决这种不匹配，我们为语言模型配备了一种动态池化机制，可以以自回归的方式预测段边界。我们比较了多种推断边界的方法，包括通过随机重新参数化进行端对端学习、基于子词分词器或条件熵峰值的监督学习，以及基于语言学原理的边界。我们对来自多个数据集和形态不同的语言的文本进行字符级别的评估。结果表明，动态池化机制既比具有固定池化的普通Transformer更快，更准确地进行分段和语言建模。

    Transformers achieve unrivalled performance in modelling language, but remain inefficient in terms of memory and time complexity. A possible remedy is to reduce the sequence length in the intermediate layers by pooling fixed-length segments of tokens. Nevertheless, natural units of meaning, such as words or phrases, display varying sizes. To address this mismatch, we equip language models with a dynamic-pooling mechanism, which predicts segment boundaries in an autoregressive fashion. We compare several methods to infer boundaries, including end-to-end learning through stochastic re-parameterisation, supervised learning (based on segmentations from subword tokenizers or spikes in conditional entropy), as well as linguistically motivated boundaries. We perform character-level evaluation on texts from multiple datasets and morphologically diverse languages. The results demonstrate that dynamic pooling, which jointly segments and models language, is both faster and more accurate than vani
    
[^179]: 翻译：利用风格分类来检测失落的《Midrash Tanhuma》材料

    Style Classification of Rabbinic Literature for Detection of Lost Midrash Tanhuma Material. (arXiv:2211.09710v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.09710](http://arxiv.org/abs/2211.09710)

    本文提出了一种拉比文学的分类系统，可以通过其风格来检测Midrash Tanhuma中的失落材料。

    

    Midrash集合是复杂的拉比文献作品，由多种语言的文本组成，经过不稳定的口头和书面传递过程演变而来。确定这种合集中的一个给定段落的起源并不总是直观的，常常是学者之间的争议，然而对于学者们理解段落及其与拉比文集中其他文本的关系至关重要。为了解决这个问题，我们提出了一个基于风格的拉比文学分类系统，利用最近发布的针对希伯来语的预训练Transformer模型。此外，我们展示了如何利用我们的方法来发现失落的Midrash Tanhuma材料。

    Midrash collections are complex rabbinic works that consist of text in multiple languages, which evolved through long processes of unstable oral and written transmission. Determining the origin of a given passage in such a compilation is not always straightforward and is often a matter of dispute among scholars, yet it is essential for scholars' understanding of the passage and its relationship to other texts in the rabbinic corpus.  To help solve this problem, we propose a system for classification of rabbinic literature based on its style, leveraging recently released pretrained Transformer models for Hebrew. Additionally, we demonstrate how our method can be applied to uncover lost material from Midrash Tanhuma.
    
[^180]: 生成多语言的性别不明确的文本转语音声音

    Generating Multilingual Gender-Ambiguous Text-to-Speech Voices. (arXiv:2211.00375v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.00375](http://arxiv.org/abs/2211.00375)

    该论文介绍了一种生成多语言的性别不明确的TTS声音的方法，通过提出的性别感知方法从潜在说话人中有效地进行采样，成功生成了一系列新的、多样化的、一致性和性别不明确性更强的声音，具有很强的实验表现。

    

    语音用户界面的性别是其被感知身份的关键元素。最近，越来越多的界面开始采用不明确的性别，而不是明确界定为男性或女性。这项工作解决了在多说话人，多语言环境中生成新的性别不明确的TTS语音的任务。这是通过使用提出的性别感知方法有效地从潜在的说话人嵌入空间中进行采样来实现的。广泛的客观和主观评估清楚地表明，该方法能够有效地生成一系列新的、多样化的声音，这些声音在所有考察的语言中都被认为比基线声音更具性别不明确性。有趣的是，性别认知被发现在听众的两个人口统计因素方面具有鲁棒性：母语和性别。据我们所知，这是第一个可以可靠地生成多种性别不明确声音的系统性和经过验证的方法。

    The gender of any voice user interface is a key element of its perceived identity. Recently, there has been increasing interest in interfaces where the gender is ambiguous rather than clearly identifying as female or male. This work addresses the task of generating novel gender-ambiguous TTS voices in a multi-speaker, multilingual setting. This is accomplished by efficiently sampling from a latent speaker embedding space using a proposed gender-aware method. Extensive objective and subjective evaluations clearly indicate that this method is able to efficiently generate a range of novel, diverse voices that are consistent and perceived as more gender-ambiguous than a baseline voice across all the languages examined. Interestingly, the gender perception is found to be robust across two demographic factors of the listeners: native language and gender. To our knowledge, this is the first systematic and validated approach that can reliably generate a variety of gender-ambiguous voices.
    
[^181]: 走向标准化的韩语语法错误修正：数据集与注释

    Towards standardizing Korean Grammatical Error Correction: Datasets and Annotation. (arXiv:2210.14389v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.14389](http://arxiv.org/abs/2210.14389)

    本文提出了三个涵盖广泛韩语语法错误的数据集和一个KAGAS注释系统。我们使用这些数据集训练的基线模型，显示其在更广泛的错误类型上明显优于当前使用的统计韩语GEC系统(Hanspell)。

    

    相比英文等其他主要语言，研究韩语语法错误修正(GEC)的研究相对较少。我们将这种问题归因于缺乏一个精心设计的用于韩语GEC的评估基准。本文从不同来源(Kor-Lang8, Kor-Native, 和 Kor-Learner)收集了三个数据集，涵盖了广泛的韩语语法错误。考虑到韩语语法的性质，我们为韩语定义了14种错误类型，并提供了KAGAS(韩国自动语法错误注释系统)，可以从平行语料库自动注释错误类型。我们使用KAGAS在我们的数据集上进行评估基准，并提供了从我们的数据集训练的基线模型。我们显示，使用我们的数据集训练的模型在更广泛的错误类型上明显优于当前使用的统计韩语GEC系统(Hanspell)，展示了数据集的多样性和有用性。本文中使用的实现和数据集公开可用。

    Research on Korean grammatical error correction (GEC) is limited, compared to other major languages such as English. We attribute this problematic circumstance to the lack of a carefully designed evaluation benchmark for Korean GEC. In this work, we collect three datasets from different sources (Kor-Lang8, Kor-Native, and Kor-Learner) that covers a wide range of Korean grammatical errors. Considering the nature of Korean grammar, We then define 14 error types for Korean and provide KAGAS (Korean Automatic Grammatical error Annotation System), which can automatically annotate error types from parallel corpora. We use KAGAS on our datasets to make an evaluation benchmark for Korean, and present baseline models trained from our datasets. We show that the model trained with our datasets significantly outperforms the currently used statistical Korean GEC system (Hanspell) on a wider range of error types, demonstrating the diversity and usefulness of the datasets. The implementations and dat
    
[^182]: 使用短语表示查询自动生成命名实体识别数据集

    Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations. (arXiv:2210.07586v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.07586](http://arxiv.org/abs/2210.07586)

    本篇论文提出了一种名为HighGEN的新框架，通过使用短语嵌入搜索方法生成实体丰富的伪字典，在使用嵌入距离验证过程减少误报的基础上生成高覆盖率的NER数据集。

    

    大多数弱监督的命名实体识别（NER）模型依赖于由专家提供的领域特定词典。然而，在许多没有字典的领域中，这种方法不可行。在最近的一项研究中，使用短语检索模型自动从维基百科中提取实体构建了伪字典，但这些字典的覆盖面往往有限，因为检索器很可能会检索到流行的实体而不是罕见的实体。在本研究中，我们提出了一个新的框架——HighGEN，它使用具有高覆盖率的伪字典生成NER数据集。具体来说，我们使用一种新的搜索方法——短语嵌入搜索来创建富实体字典，该方法鼓励检索器在一个密集的各种实体的空间中搜索。此外，我们使用一种基于实体提及和实体类型之间嵌入距离的新的验证过程，以减少高覆盖率伪标签中的误报噪声。

    Most weakly supervised named entity recognition (NER) models rely on domain-specific dictionaries provided by experts. This approach is infeasible in many domains where dictionaries do not exist. While a phrase retrieval model was used to construct pseudo-dictionaries with entities retrieved from Wikipedia automatically in a recent study, these dictionaries often have limited coverage because the retriever is likely to retrieve popular entities rather than rare ones. In this study, we present a novel framework, HighGEN, that generates NER datasets with high-coverage pseudo-dictionaries. Specifically, we create entity-rich dictionaries with a novel search method, called phrase embedding search, which encourages the retriever to search a space densely populated with various entities. In addition, we use a new verification process based on the embedding distance between candidate entity mentions and entity types to reduce the false-positive noise in weak labels generated by high-coverage 
    
[^183]: GPT-3时代的新闻摘要与评估

    News Summarization and Evaluation in the Era of GPT-3. (arXiv:2209.12356v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.12356](http://arxiv.org/abs/2209.12356)

    GPT-3在新闻摘要中具有出色表现，但自动评价指标无法可靠评估其清晰度和准确性，从而提出了一个新的评估挑战。

    

    最近大型语言模型，如GPT-3在NLP研究中得到了极大的成功，我们着眼于经典基准领域新闻摘要，研究了其对文本摘要的影响。我们展示了GPT-3摘要与基于大型摘要数据集训练的微调模型相比较，不仅人类更喜欢使用仅一个任务描述来促发的GPT-3摘要，而且这些摘要也不会像一些常规数据集具有的信息事实不准确等问题。我们进一步研究了这对于评估的意义，尤其是金标准测试集的作用。我们的实验表明，无论是基于参考文本还是无参考文本的自动评价指标都不能可靠地评估GPT-3摘要。最后，我们评估了模型在通用摘要之外的一种场景，即基于关键词的摘要，并展示了微调方法与促发方法之间的比较。为了支持进一步的研究，我们发布了一个语料库。

    The recent success of prompting large language models like GPT-3 has led to a paradigm shift in NLP research. In this paper, we study its impact on text summarization, focusing on the classic benchmark domain of news summarization. First, we investigate how GPT-3 compares against fine-tuned models trained on large summarization datasets. We show that not only do humans overwhelmingly prefer GPT-3 summaries, prompted using only a task description, but these also do not suffer from common dataset-specific issues such as poor factuality. Next, we study what this means for evaluation, particularly the role of gold standard test sets. Our experiments show that both reference-based and reference-free automatic metrics cannot reliably evaluate GPT-3 summaries. Finally, we evaluate models on a setting beyond generic summarization, specifically keyword-based summarization, and show how dominant fine-tuning approaches compare to prompting.  To support further research, we release: (a) a corpus o
    
[^184]: 评估和诱导预训练语言模型的个性

    Evaluating and Inducing Personality in Pre-trained Language Models. (arXiv:2206.07550v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.07550](http://arxiv.org/abs/2206.07550)

    本文提出了Machine Personality Inventory (MPI)数据集用于评估机器个性，通过评估，证明了预训练语言模型(LLM)具有个性。进一步提出了Personality Prompting (P^2)方法，用于以可控的方式诱导LLM具有特定个性。

    

    个性起源于哲学探索，关注个体在思考、情感和行为方面的差异。为了构建能够与人类日常合作的社交机器，我们想知道：现有的大型语言模型(LLMs)是否拥有与人类类似的个性？如果是，我们如何评估它们？进一步地，在此评估框架的基础上，如何以可控的方式诱导具有特定个性的语言模型？为回答这些问题，我们提出了机器个性库(Machine Personality Inventory, MPI)数据集，用于评估机器的个性。MPI遵循标准化的个性测试，基于五因素人格理论和人格评估库建立。通过用MPI系统地评估LLM，我们提供了第一个证据，证明了LLM的个性。我们进一步设计了一种个性提示(Personality Prompting, P^2)方法，以可控的方式诱导LLMs具有特定的个性。

    Originating as a philosophical quest, the study of personality concerns how individuals differ in thinking, feeling, and behaving. Towards building social machines that work with humans on a daily basis, we are motivated to ask: Do existing Large Language Models (LLMs) possess personalities akin to their human counterparts? If so, how can we evaluate them? Further, given this evaluation framework, how can we induce a particular personality in a controllable fashion? To answer these three questions, we propose the Machine Personality Inventory (MPI) dataset for evaluating the machine personality; MPI follows standardized personality tests, built upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By systematically evaluating LLMs with MPI, we provide the first piece of evidence showing the existence of personality in LLMs. We further devise a Personality Prompting (P^2) method to induce LLMs with a specific personality in a controllable manner
    
[^185]: 使用步骤感知验证器使大型语言模型成为更好的推理者

    Making Large Language Models Better Reasoners with Step-Aware Verifier. (arXiv:2206.02336v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.02336](http://arxiv.org/abs/2206.02336)

    本论文提出了一种名为DIVERSE的新方法，它使用多样的提示、验证器过滤不正确的答案并逐个验证每个推理步骤，以进一步增强语言模型的推理能力。

    

    小样本学习是一个具有挑战性的任务，需要语言模型从有限的示例中进行泛化。像GPT-3和PaLM这样的大型语言模型在这个领域取得了令人瞩目的进展，但它们在推理任务中仍然面临困难，比如GSM8K，这是一个算术问题的基准。为了提高它们的推理能力，之前的工作提出了使用提示来引导语言模型在给出最终答案之前进行一系列推理步骤，从而在GSM8K上实现了显着的提高，从17.9%提高到了58.1%的解题率。在本文中，我们提出了一种名为DIVERSE（多样的推理步骤验证器）的新方法，进一步增强了语言模型的推理能力。DIVERSE有三个主要组成部分：第一，它生成多样的提示，探索相同问题的不同推理路径；第二，它使用验证器根据加权投票方案过滤掉不正确的答案；第三，它逐个验证每个推理步骤。

    Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DIVERSE (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DIVERSE has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually i
    
[^186]: GENEVA：“通用性基准测试”事件论元提取，涵盖数百种事件类型和论元角色

    GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles. (arXiv:2205.12505v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.12505](http://arxiv.org/abs/2205.12505)

    本文提出了一个大而全的EAE本体论，105个事件和220个论元角色的包含在内，利用这个本体论创建了一种多样化的通用性基准测试数据集GENEVA，共包含四个测试套件，旨在评估模型处理有限数据的能力。

    

    最近事件论元提取（EAE）的研究关注于提高模型的通用性以适应新的事件类型和领域。然而，标准的评估数据集如ACE和ERE只涵盖不到40种事件类型和25种面向实体的论元角色。数据集的有限多样性和覆盖范围影响了这些数据集对EAE模型通用性的充分评估。本文提出了一个大而全的EAE本体论，在FrameNet的基础上创建了包含115个事件和220个论元角色的本体论，其中许多角色不是实体。我们利用这个本体论进一步引入了GENEVA，一种多样化的通用性基准测试数据集，包括四个测试套件，旨在评估模型处理有限数据的能力。

    Recent works in Event Argument Extraction (EAE) have focused on improving model generalizability to cater to new events and domains. However, standard benchmarking datasets like ACE and ERE cover less than 40 event types and 25 entity-centric argument roles. Limited diversity and coverage hinder these datasets from adequately evaluating the generalizability of EAE models. In this paper, we first contribute by creating a large and diverse EAE ontology. This ontology is created by transforming FrameNet, a comprehensive semantic role labeling (SRL) dataset for EAE, by exploiting the similarity between these two tasks. Then, exhaustive human expert annotations are collected to build the ontology, concluding with 115 events and 220 argument roles, with a significant portion of roles not being entities. We utilize this ontology to further introduce GENEVA, a diverse generalizability benchmarking dataset comprising four test suites, aimed at evaluating models' ability to handle limited data a
    
[^187]: 用双重提示增强跨语言提示

    Enhancing Cross-lingual Prompting with Dual Prompt Augmentation. (arXiv:2202.07255v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.07255](http://arxiv.org/abs/2202.07255)

    该论文提出了双重提示增强跨语言提示的方法DPA，利用语言无关的通用提示方法，大大缓解了数据稀缺问题。在XNLI任务上，该方法可以在仅有16个英语训练样本的情况下，将性能从34.99%提高到46.54%。

    

    开发出一种新的双重提示方法(DPA)，用于在跨语言模型训练中缓解数据稀缺问题。该方法采用了语言无关的通用提示方法，可以在仅有每类16个英语训练样本的情况下，将在XNLI任务上的性能从34.99%提高到46.54%。

    Prompting shows promising results in few-shot scenarios. However, its strength for multilingual/cross-lingual problems has not been fully exploited. Zhao and Sch\"utze (2021) made initial explorations in this direction by presenting that cross-lingual prompting outperforms cross-lingual finetuning. In this paper, we conduct an empirical exploration on the effect of each component in cross-lingual prompting and derive language-agnostic Universal Prompting, which helps alleviate the discrepancies between source-language training and target-language inference. Based on this, we propose DPA, a dual prompt augmentation framework, aiming at relieving the data scarcity issue in few-shot cross-lingual prompting. Notably, for XNLI, our method achieves 46.54% with only 16 English training examples per class, significantly better than 34.99% of finetuning. Our code is available at https://github.com/DAMO-NLP-SG/DPA.
    
[^188]: Facebook和短信文本之间的不同机会不会妨碍基于语言的预测模型的泛化

    Different Affordances on Facebook and SMS Text Messaging Do Not Impede Generalization of Language-Based Predictive Models. (arXiv:2202.01802v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.01802](http://arxiv.org/abs/2202.01802)

    本研究研究了Facebook和短信文本之间的差异对基于语言的预测模型的泛化的影响，并发现这些差异并不会显著影响这些模型在这两个平台之间的相关性。

    

    自适应的移动设备健康干预通常使用在非移动设备数据上训练的机器学习模型，如社交媒体文本，由于收集大量短信数据的困难和高昂的费用。因此，了解这些平台之间的模型差异和泛化对于正确部署至关重要。我们使用120个共享这两种平台的用户样本，研究了Facebook和短信之间的心理语言差异以及其对领域外模型性能的影响。我们发现用户使用Facebook分享经验（如休闲），而使用短信进行任务导向和会话目的（例如计划确认），反映了差异的机会。为了检验这些差异的下游效应，我们使用预先训练的基于Facebook的语言模型在Facebook和短信上估计年龄，性别，抑郁，生活满意度和压力。我们发现在Facebook和短信之间的相关性中没有显着差异。

    Adaptive mobile device-based health interventions often use machine learning models trained on non-mobile device data, such as social media text, due to the difficulty and high expense of collecting large text message (SMS) data. Therefore, understanding the differences and generalization of models between these platforms is crucial for proper deployment. We examined the psycho-linguistic differences between Facebook and text messages, and their impact on out-of-domain model performance, using a sample of 120 users who shared both. We found that users use Facebook for sharing experiences (e.g., leisure) and SMS for task-oriented and conversational purposes (e.g., plan confirmations), reflecting the differences in the affordances. To examine the downstream effects of these differences, we used pre-trained Facebook-based language models to estimate age, gender, depression, life satisfaction, and stress on both Facebook and SMS. We found no significant differences in correlations between 
    
[^189]: 基于Transformer预训练语言模型的可控文本生成综述

    A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models. (arXiv:2201.05337v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2201.05337](http://arxiv.org/abs/2201.05337)

    这篇论文综述了基于Transformer预训练语言模型的可控文本生成方法。这些方法利用大规模预训练语言模型生成多样化、流畅的文本，但由于深度神经网络的可解释性较低，需要保证其可控性。

    

    可控文本生成是自然语言生成领域中新兴的方向，被认为对于开发更自然、更符合特定应用场景的先进文本生成技术至关重要。近年来，利用大规模预训练语言模型（PLMs），尤其是广泛使用的基于Transformer的PLMs，已成为自然语言生成新范式，可以生成更多样化、更流畅的文本。然而，由于深度神经网络的可解释性较低，这些方法的可控性需要得到保证。为此，基于Transformer的PLMs的可控文本生成已成为一个快速增长但具有挑战性的新研究热点。最近3-4年出现了各种方法，针对可能需要不同类型控制约束的不同CTG任务。在本文中，我们对当前基于Transformer预训练语言模型的可控文本生成方法进行了系统的综述。

    Controllable Text Generation (CTG) is emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that are more natural and better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the lower level of interpretability of deep neural networks, the controllability of these methods need to be guaranteed. To this end, controllable text generation using transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the recent 3-4 years, targeting different CTG tasks which may require different types of controlled constraints. In this paper, we present a systematic critical review on the com
    
[^190]: DSEE：双稀疏嵌入预训练语言模型的高效调优

    DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models. (arXiv:2111.00160v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.00160](http://arxiv.org/abs/2111.00160)

    本研究提出了一种名为DSEE的框架，通过利用权重更新和最终模型权重的稀疏先验，以实现资源和参数效率高的微调。实验证明，DSEE可显著减少参数和推理时间，同时达到与最先进微调方法相当的性能水平。

    

    巨型预训练模型已成为自然语言处理的核心，是微调各种下游任务的起点。但该范例仍存在两个痛点：（a）随着预训练模型变得越来越大（例如，GPT-3有175B个参数），即使是微调的过程也可能耗时和计算资源密集；（b）默认情况下，微调后的模型与其起点一样大，这既不明智，因为它具有更专业的功能，也不实用，因为许多微调模型将在资源受限的环境中部署。为了解决这些痛点，我们提出了一种资源和参数有效的微调框架，利用权重更新和最终模型权重中的稀疏先验。我们的框架名为双稀疏嵌入高效调优（DSEE），旨在实现两个关键目标：（i）通过强制稀疏感知低秩逼近预训练模型实现参数有效的微调，（ii）通过在微调期间利用稀疏感知检查点和在最终模型权重中使用动态稀疏来实现资源有效的微调。实验证明，DSEE可在显著减少参数和推理时间的同时，实现与最先进的微调方法相当的性能。

    Gigantic pre-trained models have become central to natural language processing (NLP), serving as the starting point for fine-tuning towards a range of downstream tasks. However, two pain points persist for this paradigm: (a) as the pre-trained models grow bigger (e.g., 175B parameters for GPT-3), even the fine-tuning process can be time-consuming and computationally expensive; (b) the fine-tuned model has the same size as its starting point by default, which is neither sensible due to its more specialized functionality, nor practical since many fine-tuned models will be deployed in resource-constrained environments. To address these pain points, we propose a framework for resource- and parameter-efficient fine-tuning by leveraging the sparsity prior in both weight updates and the final model weights. Our proposed framework, dubbed Dually Sparsity-Embedded Efficient Tuning (DSEE), aims to achieve two key objectives: (i) parameter efficient fine-tuning - by enforcing sparsity-aware low-r
    

