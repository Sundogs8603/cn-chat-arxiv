# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Vaccine: Perturbation-aware Alignment for Large Language Model](https://rss.arxiv.org/abs/2402.01109) | 疫苗是一种针对大规模语言模型的干扰感知对齐技术，通过逐渐添加扰动产生不变的隐藏嵌入，提高对抗有害提示引起的嵌入漂移的对齐鲁棒性，同时保留对良性提示的推理能力。 |
| [^2] | [Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling](https://arxiv.org/abs/2402.19471) | 研究利用大型语言模型提出信息量丰富的问题，在Battleship游戏中展示出与人类表现相匹配的效果，并揭示了贝叶斯模型如何指导问问题行为。 |
| [^3] | [TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning](https://arxiv.org/abs/2402.19467) | TV-TREES是第一个多模态蕴涵树生成器，通过生成视频直接蕴涵的简单前提与高级结论之间的蕴涵关系树，实现了可解释联合模态推理，并在挑战性的TVQA数据集上展示了最先进的零-shot性能。 |
| [^4] | [Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models](https://arxiv.org/abs/2402.19465) | 本文研究探索了大型语言模型在预训练期间的可信度，揭示了早期预训练LLMs已经能够区分各个可信度维度中的概念，提出了从预训练检查点中提取转向向量以增强LLM可信度的方法。 |
| [^5] | [Curiosity-driven Red-teaming for Large Language Models](https://arxiv.org/abs/2402.19464) | 研究提出了一种新方法，能够通过训练红队LLM，自动化生成测试案例，以最大化引出目标LLM不良响应，以解决当前RL方法生成测试案例覆盖范围较低的问题。 |
| [^6] | [Accelerating materials discovery for polymer solar cells: Data-driven insights enabled by natural language processing](https://arxiv.org/abs/2402.19462) | 通过自然语言处理和数据驱动方法，我们展示了一个加速聚合物太阳能电池材料发现的工作流程，可以显著减少发现时间并预测未被报道的有潜力的受体-给体组合。 |
| [^7] | [$\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation](https://arxiv.org/abs/2402.19457) | $\texttt{COSMIC}$是一种以相互信息为基础的新的摘要评估方法，有效预测下游任务表现，并与人类判断相关性强。竞争性能优于$\texttt{BERTScore}$和$\texttt{ROUGE}$。 |
| [^8] | [Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap](https://arxiv.org/abs/2402.19450) | 提出了一个用于评估语言模型推理能力的框架，通过功能变体的基准进行鲁棒性评估，发现静态基准和功能基准的准确性之间存在推理差距 |
| [^9] | [Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models](https://arxiv.org/abs/2402.19449) | 研究发现语言模型中的重尾类别不平衡问题导致了优化动态上的困难，Adam和基于符号的方法在这种情况下优于梯度下降。 |
| [^10] | [ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL](https://arxiv.org/abs/2402.19446) | 本文提出了一个用于构建LLMs的多轮强化学习算法的框架 |
| [^11] | [Compositional API Recommendation for Library-Oriented Code Generation](https://arxiv.org/abs/2402.19431) | 提出了CAPIR（Compositional API Recommendation）来为粗粒度需求推荐API，并采用“分而治之”的策略将任务描述分解为详细的子任务。 |
| [^12] | [Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models](https://arxiv.org/abs/2402.19427) | 提出了 Griffin 模型，将门控线性循环与局部注意力相结合，实现高效的语言模型，该模型在推理过程中具有低延迟和高吞吐量。 |
| [^13] | [PaECTER: Patent-level Representation Learning using Citation-informed Transformers](https://arxiv.org/abs/2402.19411) | PaECTER是一个专为专利设计的开放源码文档级编码器，利用引文信息对BERT进行微调，生成专利文档的数值表示，并在专利领域的相似性任务中表现优异。 |
| [^14] | [On the Scaling Laws of Geographical Representation in Language Models](https://arxiv.org/abs/2402.19406) | 地理知识可以在大型语言模型中观察到，随着模型规模增加而一致扩展，但更大的模型无法消除训练数据中的地理偏见。 |
| [^15] | [Entity-Aware Multimodal Alignment Framework for News Image Captioning](https://arxiv.org/abs/2402.19404) | 设计了面向实体的多模态对齐任务和对齐框架，提高了新闻图像字幕生成任务的性能表现。 |
| [^16] | [Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy](https://arxiv.org/abs/2402.19379) | 该研究通过将十二个LLMs组成的LLM集成方法与925名人类预测者的群体预测进行比较，发现LLM群体优于简单的无信息基准，并在统计上等效于人类群体。 |
| [^17] | [OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models](https://arxiv.org/abs/2402.19371) | OpenMedLM 提出了一个提示平台，利用提示工程在医学问答中能够超越对开源大型语言模型进行微调，实现了在医学基准上的 SOTA 性能。 |
| [^18] | [Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process](https://arxiv.org/abs/2402.19350) | 该研究引入了一个促进显式和隐式知识的框架，用于多跳问题回答，从人类阅读过程的角度连接输入文段和预训练知识。 |
| [^19] | [Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge](https://arxiv.org/abs/2402.19334) | 将带后门的模型与其他同类模型合并可以有效治疗后门漏洞，为后门攻击提供推理阶段的有效和高效防御 |
| [^20] | [Compact Speech Translation Models via Discrete Speech Units Pretraining](https://arxiv.org/abs/2402.19333) | 通过在离散语音单元上预训练较小模型，以蒸馏SSL模型的知识，实现了紧凑的语音翻译模型，具有短推理管道和适用于低资源环境等优点 |
| [^21] | [WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset](https://arxiv.org/abs/2402.19282) | WanJuan-CC是一个安全高质量的开源英文网络文本数据集，通过处理大规模的Common Crawl数据并经过多项筛选和过滤步骤得到，为语言模型的预训练提供了重要资源。 |
| [^22] | [PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval](https://arxiv.org/abs/2402.19273) | PlanGPT是第一个专为城市和空间规划量身定制的大型语言模型，通过定制数据库检索框架、领域特定微调和先进工具功能，实现了高效的性能表现和提供高质量规划响应。 |
| [^23] | [Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation](https://arxiv.org/abs/2402.19267) | 提出了一种新颖的无监督数据选择方法，通过捕获领域特定机器翻译中令人困扰的命名实体，实现了高质量翻译效果。 |
| [^24] | [GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers](https://arxiv.org/abs/2402.19255) | 通过引入对抗式小学数学数据集（GSM-Plus），评估了25个LLMs和4种提示技术，在广泛的问题变化中展示LLMs的数学推理能力，并发现它们的表现远非稳健。 |
| [^25] | [Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark](https://arxiv.org/abs/2402.19248) | 本论文提出了CDQA，一个中文动态问答基准测试，致力于提高中文大型语言模型（LLMs）回答动态问题的能力，并通过高质量数据和精细样本分类实现了对LLMs能力更细致的观察。实验结果表明，CDQA具有挑战性且值得进一步研究。 |
| [^26] | [Memory-Augmented Generative Adversarial Transformers](https://arxiv.org/abs/2402.19218) | 通过在标准变压器架构中增加额外的记忆库和注意力层，该研究提出了一种可以提高变压器生成语言准确性的方法。 |
| [^27] | [PeLLE: Encoder-based language models for Brazilian Portuguese based on open data](https://arxiv.org/abs/2402.19204) | PeLLE是基于RoBERTa架构的巴西葡萄牙语编码器语言模型系列，在其预训练中使用了Carolina语料库的开放数据，研究发现在多个下游任务中，使用较大模型的性能更好，但有些任务会因为使用较小但精选的数据在预训练中而有所益处。 |
| [^28] | [PRSA: Prompt Reverse Stealing Attacks against Large Language Models](https://arxiv.org/abs/2402.19200) | 本文提出了针对商业LLMs的提示反窃取攻击框架PRSA，通过分析输入-输出对的关键特征实现攻击。 |
| [^29] | [Improving Legal Judgement Prediction in Romanian with Long Text Encoders](https://arxiv.org/abs/2402.19170) | 本研究关注通过扩展Transformer模型的序列长度来更好理解法律语料库中的长文档，并在罗马尼亚的4个LJP数据集上进行了广泛实验。 |
| [^30] | [Teaching Large Language Models an Unseen Language on the Fly](https://arxiv.org/abs/2402.19167) | 通过提示，在飞行中教授大型语言模型一种未知语言，提出DiPMT ++框架，通过上下文学习使LLMs适应看不见的语言，并实现了壮语和汉语之间的翻译性能显著提升，并展示了该框架在帮助人类翻译完全未知语言方面的实用性。 |
| [^31] | [Evaluating Webcam-based Gaze Data as an Alternative for Human Rationale Annotations](https://arxiv.org/abs/2402.19133) | 论文讨论了使用基于网络摄像头的眼动数据作为评估重要性评分的替代方案，通过比较不同语言模型对WebQAmGaze数据集的表现，结果表明凝视数据提供了宝贵的语言学信息。 |
| [^32] | [VIXEN: Visual Text Comparison Network for Image Difference Captioning](https://arxiv.org/abs/2402.19119) | 提出了一种名为VIXEN的技术，能够用文本简洁地总结一对图像之间的视觉差异，为突出内容操作提供潜在的缓解方法 |
| [^33] | [How to Understand "Support"? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding](https://arxiv.org/abs/2402.19116) | 提出了一种隐式增强因果推断方法（IECI），用于解决弱监督短语定位任务中的挑战，通过标注高质量数据集进行评估，并相比基线方法展现出明显优势。 |
| [^34] | [Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models](https://arxiv.org/abs/2402.19103) | 该论文对大型语言模型中的虚假前提幻觉进行了全面分析，提出了一种名为“FAITH”的方法，用于减轻虚假前提幻觉。 |
| [^35] | [TEncDM: Understanding the Properties of Diffusion Model in the Space of Language Model Encodings](https://arxiv.org/abs/2402.19097) | 通过在语言模型编码空间中训练模型，并使用基于Transformer的解码器以及自我调节，本文提出了名为TEncDM的文本编码扩散模型，在两个文本生成任务上展示了其优越性 |
| [^36] | [Survey in Characterization of Semantic Change](https://arxiv.org/abs/2402.19088) | 语义变化对计算语言学算法的结果质量可能会产生影响，因此重要性日益凸显。 |
| [^37] | [Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment](https://arxiv.org/abs/2402.19085) | 引入了可控偏好优化（CPO）方法，明确为不同目标指定偏好分数，从而引导模型生成符合需求的响应。 |
| [^38] | [Pointing out the Shortcomings of Relation Extraction Models with Semantically Motivated Adversarials](https://arxiv.org/abs/2402.19076) | 本文通过替换实体提及来生成对抗性示例，揭示了关系抽取模型的捷径特性缺陷 |
| [^39] | [Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: A Benchmark Study](https://arxiv.org/abs/2402.19052) | 本研究评估大型语言模型在选择性总结心理健康咨询会话中的功效，并引入了MentalCLOUDS数据集作为基准，以探索其在辅导组件指导的总结任务中的性能。 |
| [^40] | [PopALM: Popularity-Aligned Language Models for Social Media Trendy Response Prediction](https://arxiv.org/abs/2402.18950) | 提出了Popularity-Aligned Language Models (PopALM)来区分受大众喜欢的回复，通过强化学习和课程学习来提高高级语言模型在社交媒体热门事件回应预测中的性能 |
| [^41] | [Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models](https://arxiv.org/abs/2402.18945) | 论文提出了一种名为Syntactic Ghost的新方法，实现了对预训练语言模型进行无感知和通用的后门植入。 |
| [^42] | [SemEval 2024 -- Task 10: Emotion Discovery and Reasoning its Flip in Conversation (EDiReF)](https://arxiv.org/abs/2402.18944) | SemEval-2024的任务10旨在识别对话中的情绪并找出背后的原因，参与者需自动执行情绪识别和情绪转变推理的子任务，取得了不错的结果。 |
| [^43] | [Inappropriate Pause Detection In Dysarthric Speech Using Large-Scale Speech Recognition](https://arxiv.org/abs/2402.18923) | 该研究通过扩展大规模语音识别模型，提出了一种在发音障碍语音中检测不当停顿的方法，包括任务设计、标注策略和不当停顿预测层，为评估病情严重程度和语言治疗提供了新思路。 |
| [^44] | [AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging](https://arxiv.org/abs/2402.18913) | 提出一种新的跨语言转移方法 $\texttt{AdaMergeX}$，利用自适应适配器融合来解决任务能力和语言能力之间的关系。 |
| [^45] | [Updating Language Models with Unstructured Facts: Towards Practical Knowledge Editing](https://arxiv.org/abs/2402.18909) | 本文提出了一个新的基准，非结构化知识编辑（UKE），旨在使用非结构化文本作为知识更新，避免了繁琐的结构化事实构建，具有更高效和响应性的知识编辑能力。 |
| [^46] | [Principal Component Analysis as a Sanity Check for Bayesian Phylolinguistic Reconstruction](https://arxiv.org/abs/2402.18877) | 提出了一种简单的合理性检查方法：通过主成分分析将重建树投影到空间中，有效地可视化了异常情况，特别是在形式上的波动。 |
| [^47] | [Reducing Hallucinations in Entity Abstract Summarization with Facts-Template Decomposition](https://arxiv.org/abs/2402.18873) | 通过事实-模板分解，提出了一个可解释的框架SlotSum，用于减少预训练语言模型在实体摘要中生成幻觉的问题。 |
| [^48] | [Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning](https://arxiv.org/abs/2402.18865) | 通过模式连接调查了连续微调中不同极小值之间的几何连接，揭示了大型语言模型中的灾难性遗忘问题。 |
| [^49] | [Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP Models on Accuracy and Semantic Coherence](https://arxiv.org/abs/2402.18849) | 通过整合NLP大型模型，本研究提出一种LSB-NLP混合框架，显著提高了隐写文本提取的准确性和鲁棒性，尤其在处理中文字符时表现优异。 |
| [^50] | [When does word order matter and when doesn't it?](https://arxiv.org/abs/2402.18838) | 本文研究了语言模型对词序的敏感度问题，通过量化词序信息量，发现在语言提示提供冗余信息时，模型对词序变化不敏感，且不同任务间的不敏感程度有所差异。 |
| [^51] | [Utilizing Local Hierarchy with Adversarial Training for Hierarchical Text Classification](https://arxiv.org/abs/2402.18825) | 通过引入对抗性框架和本地层次结构，我们提出了一个适用于几乎所有HTC模型的HiAdv框架，优化了分层文本分类，并证明了本地层次结构的有效性，特别对于训练数据不足的罕见类别。 |
| [^52] | [How do Large Language Models Handle Multilingualism?](https://arxiv.org/abs/2402.18815) | 大型语言模型展示了处理多语言任务的出色性能，研究发现在不同层次中处理多语言输入的策略，以及处理特定语言时的语言特定神经元存在。 |
| [^53] | [On the Decision-Making Abilities in Role-Playing using Large Language Models](https://arxiv.org/abs/2402.18807) | 本文评估了大型语言模型在角色扮演中的决策能力，并提供了指标和指导以增强其在此任务中的表现。 |
| [^54] | [ARTiST: Automated Text Simplification for Task Guidance in Augmented Reality](https://arxiv.org/abs/2402.18797) | 提出了ARTiST，这是一个自动文本简化系统，结合少量示例提示和GPT-3模型，通过整合简化技术生成了适用于头戴式显示器的简化AR文本。实验结果表明，ARTiST显著降低了认知负担，并提高了性能。 |
| [^55] | [MPAT: Building Robust Deep Neural Networks against Textual Adversarial Attacks](https://arxiv.org/abs/2402.18792) | 提出了一种基于恶意扰动的对抗训练方法（MPAT）来构建鲁棒的深度神经网络，用于抵御文本对抗攻击。 |
| [^56] | [FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning](https://arxiv.org/abs/2402.18789) | FlexLLM是第一个可以在同一迭代中共同提供推理和参数高效微调请求的系统，通过引入标记级微调机制实现共享GPU资源的高效利用 |
| [^57] | [Advancing Generative AI for Portuguese with Open Decoder Gerv\'asio PT*](https://arxiv.org/abs/2402.18766) | 提出了一种葡萄牙语生成人工智能的开放解码器模型Gerv\'asio PT*，创造了新的技术水平，促进葡萄牙语言技术研究和创新。 |
| [^58] | [How Much Annotation is Needed to Compare Summarization Models?](https://arxiv.org/abs/2402.18756) | 本研究通过实证研究发现，仅需要不到100个例子就能够得出对摘要系统的明确偏好，并且只有一些自动评估指标能够适度预测模型的胜率。 |
| [^59] | [Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains](https://arxiv.org/abs/2402.18747) | 细调的机器翻译度量在未知领域中表现出明显的性能下降，相对于依赖表面形式的度量和未经MT质量判断细调的预训练度量。 |
| [^60] | [Priority Sampling of Large Language Models for Compilers](https://arxiv.org/abs/2402.18734) | 提出了一种优先采样技术，能够按照模型信心度产生唯一样本，在生成和优化代码时表现优于核采样方法。 |
| [^61] | [Learning to Compress Prompt in Natural Language Formats](https://arxiv.org/abs/2402.18700) | 该研究旨在通过提出自然语言提示封装（Nano-Capsulator）框架，解决了在自然语言格式中压缩提示的挑战，以提高大型语言模型的可转移性和性能。 |
| [^62] | [Grounding Language Models for Visual Entity Recognition](https://arxiv.org/abs/2402.18695) | 通过AutoVER模型，我们提出了一种在视觉实体识别中应用自回归模型的方法，通过检索增强的约束生成，成功区分巨大标签空间中相似的实体，并在Oven-Wiki基准测试上取得显著进展。 |
| [^63] | [RORA: Robust Free-Text Rationale Evaluation](https://arxiv.org/abs/2402.18678) | RORA 提出了一种新的评估方法，用于衡量自由文本理由对标签的新信息贡献，并在评估中优于现有方法。 |
| [^64] | [Simple linear attention language models balance the recall-throughput tradeoff](https://arxiv.org/abs/2402.18668) | 提出了一种简单的线性注意力语言模型架构，可以平衡召回和内存消耗之间的权衡。 |
| [^65] | [FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability](https://arxiv.org/abs/2402.18667) | FOFO是一个基准测试，用于评估大型语言模型追随复杂、领域特定格式的能力，揭示了LLMs在格式追随能力方面的表现和不同领域之间的差异 |
| [^66] | [Large Language Models and Games: A Survey and Roadmap](https://arxiv.org/abs/2402.18659) | 这项研究调查了大型语言模型在游戏领域中的多种应用及其角色，指出了未开发领域和未来发展方向，同时探讨了在游戏领域中大型语言模型的潜力和限制。 |
| [^67] | [MMSR: Symbolic Regression is a Multimodal Task](https://arxiv.org/abs/2402.18603) | 符号回归被视为一个多模态任务，研究人员将数据到表达式的映射视为翻译问题，引入大规模预训练模型。 |
| [^68] | [Verif.ai: Towards an Open-Source Scientific Generative Question-Answering System with Referenced and Verifiable Answers](https://arxiv.org/abs/2402.18589) | Verif.ai是一个具有引用和可验证答案的开源科学生成式问答系统，通过信息检索、生成模型和验证引擎的结合实现对主张的生成和验证。 |
| [^69] | [Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards](https://arxiv.org/abs/2402.18571) | 提出了方向偏好对齐（DPA）框架，通过多目标奖励模拟不同偏好配置，以实现用户相关的偏好控制。 |
| [^70] | [RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval](https://arxiv.org/abs/2402.18510) | 本文研究了RNNs和Transformer在处理算法问题时的表现能力差距，发现RNNs存在关键瓶颈，即无法完美地从上下文中检索信息，导致无法像Transformer那样轻松解决需要这种能力的任务。 |
| [^71] | [Language Models Represent Beliefs of Self and Others](https://arxiv.org/abs/2402.18496) | 通过神经激活线性解析语言模型中代理人观点下的信念状态，揭示了大型语言模型内部表述自我和他人信念，这对社会推理过程至关重要，并在多样社会推理任务中具有潜在的泛化能力。 |
| [^72] | [A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models](https://arxiv.org/abs/2402.18409) | 提出了一个新颖的评估基准，用于评估大型视觉语言模型的认知能力，发现LVLMs与人类之间存在较大的认知能力差距。 |
| [^73] | [Exploration of Adapter for Noise Robust Automatic Speech Recognition](https://arxiv.org/abs/2402.18275) | 本文研究了适配器用于噪声鲁棒自动语音识别的探索，发现将适配器插入浅层可以获得更显著的效果，真实数据比模拟数据更有效，并且将适配器集成到基于语音增强的ASR系统中可以带来实质性改进。 |
| [^74] | [MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery](https://arxiv.org/abs/2402.18169) | 提出了一种基于大型语言模型的MIKO框架，利用多模态和文本的协同作用揭示社交媒体用户的意图。 |
| [^75] | [Assessing the Efficacy of Grammar Error Correction: A Human Evaluation Approach in the Japanese Context](https://arxiv.org/abs/2402.18101) | 评估了一种日本背景下人类评估方法中最先进语法错误检测和校正模型的性能，在错误校正方面表现出较高的准确性和保守性，在错误检测方面表现出高精确度和调整召回率。 |
| [^76] | [Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions](https://arxiv.org/abs/2402.18060) | 在回答医学问题方面，大型语言模型在处理具有挑战性的实际临床案例上的表现是关键，因此构建了两个结构化数据集进行评估。 |
| [^77] | [Retrieval is Accurate Generation](https://arxiv.org/abs/2402.17532) | 提出了一种从支持文档中选择上下文感知短语的新颖生成方法，通过迭代式自我强化提高训练数据准确性，在各种任务中表现优越，提高了开放式文本生成的质量。 |
| [^78] | [Defending LLMs against Jailbreaking Attacks via Backtranslation](https://arxiv.org/abs/2402.16459) | 通过反向翻译来防御LLMs免受越狱攻击，将生成的反向翻译提示用于揭示原始提示的实际意图，提高了模型的安全性。 |
| [^79] | [LLM Inference Unveiled: Survey and Roofline Model Insights](https://arxiv.org/abs/2402.16363) | 本文提出了一个基于Roofline模型的框架，用于系统分析LLM推断技术，帮助识别部署中的瓶颈，并为更有效地部署LLM提供策略。 |
| [^80] | [Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing](https://arxiv.org/abs/2402.16192) | 提出了一种名为SEMANTICSMOOTH的防御方法，通过聚合多个语义转换副本的预测结果来防御大型语言模型遭遇GCG、PAIR和AutoDAN攻击，同时保持了较强的正常性能。 |
| [^81] | [Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy](https://arxiv.org/abs/2402.16041) | 通过多种群意识优化检测机器生成文本的最大均值离差，解决了机器生成文本与人工编写文本之间微妙的分布差异挑战。 |
| [^82] | [Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method](https://arxiv.org/abs/2402.15813) | 首次将谈判任务形式化描述为不完全信息的不对称游戏，提出了一种用于评估代理表现的方法，并发现扮演买方的难度大且模型大小不能有效提高买方表现，为此提出了一种名为OG-Narrator的新方法。 |
| [^83] | [RelayAttention for Efficient Large Language Model Serving with Long System Prompts](https://arxiv.org/abs/2402.14808) | 本论文提出的RelayAttention算法旨在改善涉及长系统提示的大型语言模型服务的效率，通过一次性从DRAM读取隐藏状态来消除现有因果注意力算法中的内存访问冗余。 |
| [^84] | [Two Counterexamples to \textit{Tokenization and the Noiseless Channel}](https://arxiv.org/abs/2402.14614) | 该论文讨论了在《Tokenization and the Noiseless Channel》提出的使用Rényi效率作为分词器评估机制的局限性，并描述了两个BPE分词的反例，展示了Rényi效率无法捕捉到所有优秀分词方案的情况。 |
| [^85] | [Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding](https://arxiv.org/abs/2402.12374) | Sequoia是一种可扩展、稳健且硬件感知的推测解码算法，通过引入动态规划算法优化标记树结构、采用新颖的采样和验证方法实现稳健性能以及硬件感知的树优化器最大化推测性能。 |
| [^86] | [Assessing LLMs' Mathematical Reasoning in Financial Document Question Answering](https://arxiv.org/abs/2402.11194) | 通过实验评估了LLMs在金融表格问答中的数学推理能力，发现引入了一种新型提示技术，能够在性能上胜过其他基线模型 |
| [^87] | [The optimal placement of the head in the noun phrase. The case of demonstrative, numeral, adjective and noun](https://arxiv.org/abs/2402.10311) | 本研究旨在探讨句法依赖距离最小化与意外减少最小化原则在名词短语中的冲突，结论显示当涉及的单词较少且单词较短时，意外减少可能会超越句法依赖距离优化。 |
| [^88] | [Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients](https://arxiv.org/abs/2402.10153) | 本文提出了一种知识注入的基于LLM的对话式健康代理（CHA）用于糖尿病患者，通过整合领域特定知识和分析能力，提高了糖尿病管理的准确性和效果。 |
| [^89] | [Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation](https://arxiv.org/abs/2402.05699) | 本文提出了一个通过社交场景模拟来自对齐大型语言模型的方法，以减轻其被滥用造成的潜在不良影响。通过一个名为MATRIX的虚拟排练空间，LLM可以在回答查询前考虑社交后果，并通过MATRIX-simulated数据的微调，保持对人类价值的遵从和推理速度的平衡。实验证明，在温和假设下，带有MATRIX的LLM胜过了宪法AI。 |
| [^90] | [Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models](https://arxiv.org/abs/2402.03659) | 这个论文介绍了使用大型语言模型生成可解释的股票预测的方法，并提出了Summarize-Explain-Predict（SEP）模型来解决股票预测中的解释问题和数据标注成本的挑战。 |
| [^91] | [BPDec: Unveiling the Potential of Masked Language Modeling Decoder in BERT pretraining](https://arxiv.org/abs/2401.15861) | 本文揭示了BPDec（BERT预训练解码器）的潜力，强调增强的掩码语言建模解码器设计及研究在BERT预训练中的重要性。 |
| [^92] | [Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking](https://arxiv.org/abs/2311.09827) | 本文研究了一种新颖的越狱攻击，针对大型语言模型的认知结构和过程进行设计，通过认知过载攻击，即使在安全对齐之后，也可以激发LLMs产生有害或不道德的响应。 |
| [^93] | [OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking](https://arxiv.org/abs/2311.09758) | 本研究提出了一种新颖的SLM/LLM路由框架，以提高计算效率和增强任务性能，通过利用结构化知识提取任务中SLMs和LLMs的互补优势，从而降低成本而不牺牲性能。 |
| [^94] | [HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models](https://arxiv.org/abs/2310.14566) | HallusionBench是一个专为评估大型视觉语言模型在图像背景推理中面临挑战的基准，通过引入新颖结构和量化分析，显示出GPT-4V取得了31.42%的准确率，远高于其他模型。 |
| [^95] | [Revisiting the Hypothesis: Do pretrained Transformers Learn In-Context by Gradient Descent?](https://arxiv.org/abs/2310.08540) | 本研究重新审视了预训练的Transformer是否通过梯度下降在上下文中学习的假设，并发现现有研究中的假设存在限制性假设，使其与实际语言模型训练时的语境存在显著差异。同时，通过对真实模型的观察和比较，揭示了ICL和GD在观察演示顺序上的不同敏感性。 |
| [^96] | [Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic](https://arxiv.org/abs/2309.13339) | 提出了LoT（Logical Thoughts）提示，一个自我改进框架，利用根植于符号逻辑的原则，特别是归谬法，逐步验证和纠正大型语言模型的零射链推理过程。 |
| [^97] | [WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models.](http://arxiv.org/abs/2401.13919) | WebVoyager是一种创新的基于大型多模态模型的Web代理，能够通过与真实网站交互来端到端地完成用户指令。它提出了一个新的Web代理评估协议，并在实际任务中取得了显著的成功率。 |
| [^98] | [BIBench: Benchmarking Data Analysis Knowledge of Large Language Models.](http://arxiv.org/abs/2401.02982) | BIBench是一个旨在评估大型语言模型（LLMs）在商业智能（BI）数据分析领域中能力的综合基准测试，其通过测试模型在BI基础知识、应用知识和技术技能三个维度上的表现来进行评估。 |
| [^99] | [Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models.](http://arxiv.org/abs/2310.18127) | 本文提出了一种利用大型语言模型的强化学习框架，能够学习提问相关问题并进行推理来指导在实际环境中执行的行为的学习。 |
| [^100] | [A Semantic Invariant Robust Watermark for Large Language Models.](http://arxiv.org/abs/2310.06356) | 本论文提出了一种对大型语言模型进行语义不变水印的方法，该方法通过利用另一个嵌入式语言模型生成所有前面token的语义嵌入，然后利用训练得到的水印模型将这些语义嵌入转换为水印logits。该方法既具有攻击鲁棒性又具有安全鲁棒性。 |
| [^101] | [Redefining Digital Health Interfaces with Large Language Models.](http://arxiv.org/abs/2310.03560) | 本论文提出了利用大型语言模型（LLMs）重新定义数字健康界面的方法，将LLMs与外部工具结合使用，从而提高了与临床技术的互动效果，改善了数字医疗工具和AI模型的实用性，并解决了在临床环境中使用LLMs的问题。 |
| [^102] | [Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI.](http://arxiv.org/abs/2309.12444) | 这篇论文提出了基于生成AI的医疗对话模型的评估指标问题，并强调了现有指标对医学和健康概念的理解不足以及忽略了用户体验因素。 |
| [^103] | [Rehearsal: Simulating Conflict to Teach Conflict Resolution.](http://arxiv.org/abs/2309.12309) | 演练是一个系统，通过模拟冲突和提供反馈，教授用户冲突解决的技能。利用演练，用户可以练习处理各种冲突场景，并学习如何运用冲突策略。 |
| [^104] | [CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought.](http://arxiv.org/abs/2309.11143) | CoT-BERT提出了一种通过思维链条增强无监督句子表示的方法，通过两个阶段的处理，引入思维链条的概念进行向量化，以提高模型性能。 |
| [^105] | [Re-Reading Improves Reasoning in Language Models.](http://arxiv.org/abs/2309.06275) | 许多研究关注于如何引导和结构化大型语言模型的推理过程，但很少有研究关注于输入问题本身。本研究引入了一种称为“重新阅读”的提示策略，通过深入阅读输入提示中的问题信息，提供了更深入的洞察、更准确的模式识别和更有效的推理能力。 |
| [^106] | [Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models.](http://arxiv.org/abs/2309.05605) | 本文提出了一种通过向Transformer-Based语言模型的LLM注意力头部定向注入内存来纠正多跳推理错误的方法，从而提高了模型在处理多跳推理问题时的表现。 |
| [^107] | [A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment.](http://arxiv.org/abs/2308.05696) | 本研究初步探讨复杂性与对齐之间的固有关系。通过使用"tree-instruct"方法来增强指令数据的复杂性，可以在对齐到任务和用户偏好方面提供更好的性能。 |
| [^108] | [A Private Watermark for Large Language Models.](http://arxiv.org/abs/2307.16230) | 这项工作提出了一种私密水印算法，通过使用两个不同的神经网络进行水印生成和检测，并共享部分参数，实现了高效且高准确性的检测，同时对生成和检测速度影响最小。 |
| [^109] | ["It Felt Like Having a Second Mind": Investigating Human-AI Co-creativity in Prewriting with Large Language Models.](http://arxiv.org/abs/2307.10811) | 通过三节次的定性研究，探究了人类与大型语言模型在预写过程中的合作模式，并发现了一个三阶段的人机共创过程：构思、启发和实施。在这个合作过程中，人类扮演着主导角色。 |
| [^110] | [Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages.](http://arxiv.org/abs/2305.02215) | 本文研究使用语言分类方法探究单语BERT的语言属性，核心发现为BERT正在复制传统的语言模型。 |
| [^111] | [Does GPT-3 Demonstrate Psychopathy? Evaluating Large Language Models from a Psychological Perspective.](http://arxiv.org/abs/2212.10529) | 本文从心理学角度评估大型语言模型的安全性，发现所有模型在短暗三合一测验上的得分都高于人类平均水平，存在相对较暗的人格模式。尽管经过指标微调，两种模型仍呈现隐含的黑暗人格模式。同时，本文观察到GPT-3和InstructGPT的幸福感得分持续增加。 |

# 详细

[^1]: 疫苗：针对大规模语言模型的干扰感知对齐技术

    Vaccine: Perturbation-aware Alignment for Large Language Model

    [https://rss.arxiv.org/abs/2402.01109](https://rss.arxiv.org/abs/2402.01109)

    疫苗是一种针对大规模语言模型的干扰感知对齐技术，通过逐渐添加扰动产生不变的隐藏嵌入，提高对抗有害提示引起的嵌入漂移的对齐鲁棒性，同时保留对良性提示的推理能力。

    

    作为一种新的微调即服务范 paradigm，大型语言模型 (LLM) 为用户上传的一小部分有害数据提供了新的攻击面，这些数据很容易欺骗微调过程从而产生对齐失效的模型。我们进行了实证分析，揭示了一种可能导致对齐失效的有害嵌入漂移现象。受到我们的发现启发，我们提出了疫苗 (Vaccine) ，一种针对干扰感知的对齐技术，以减轻用户微调的安全风险。疫苗的核心思想是通过在对齐阶段逐渐添加精心设计的扰动，产生不变的隐藏嵌入，从而使嵌入能够抵御来自未经消毒的用户数据的有害扰动。我们在开源主流LLM（如Llama2，Opt，Vicuna）上的实验结果表明，疫苗能够提高对抗有害提示引起的嵌入漂移的对齐鲁棒性，同时保留对良性提示的推理能力。

    The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompt
    
[^2]: 严格的LIPS沉没舰船：在Battleship中使用语言信息程序抽样提出问题

    Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling

    [https://arxiv.org/abs/2402.19471](https://arxiv.org/abs/2402.19471)

    研究利用大型语言模型提出信息量丰富的问题，在Battleship游戏中展示出与人类表现相匹配的效果，并揭示了贝叶斯模型如何指导问问题行为。

    

    问题结合了我们对语言的掌握和我们对于在有限认知资源情况下推断不确定性的出色能力。人们如何在巨大假设空间中提出信息量丰富的问题？我们研究了这些在基于战舰游戏Battleship的经典提问任务中的权衡。我们的语言信息程序抽样（LIPS）模型利用大型语言模型（LLMs）生成自然语言问题，将其转化为符号程序，并评估其预期信息增益。我们发现，即使在一个令人惊讶的资源预算下，这种简单的蒙特卡罗优化策略也能产生反映人类在各种Battleship棋盘场景中表现的丰富问题。相比之下，仅使用LLM的基线在将问题与棋盘状态联系起来方面存在困难；值得注意的是，GPT-4V并没有比无视觉基线提供改进。我们的结果展示了贝叶斯提问模型如何可能模拟和指导人类的问问题行为。

    arXiv:2402.19471v1 Announce Type: cross  Abstract: Questions combine our mastery of language with our remarkable facility for reasoning about uncertainty. How do people navigate vast hypothesis spaces to pose informative questions given limited cognitive resources? We study these tradeoffs in a classic grounded question-asking task based on the board game Battleship. Our language-informed program sampling (LIPS) model uses large language models (LLMs) to generate natural language questions, translate them into symbolic programs, and evaluate their expected information gain. We find that with a surprisingly modest resource budget, this simple Monte Carlo optimization strategy yields informative questions that mirror human performance across varied Battleship board scenarios. In contrast, LLM-only baselines struggle to ground questions in the board state; notably, GPT-4V provides no improvement over non-visual baselines. Our results illustrate how Bayesian models of question-asking can l
    
[^3]: TV-TREES：用于神经符号视频推理的多模态蕴涵树

    TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning

    [https://arxiv.org/abs/2402.19467](https://arxiv.org/abs/2402.19467)

    TV-TREES是第一个多模态蕴涵树生成器，通过生成视频直接蕴涵的简单前提与高级结论之间的蕴涵关系树，实现了可解释联合模态推理，并在挑战性的TVQA数据集上展示了最先进的零-shot性能。

    

    在处理电视剪辑等复杂的多模态内容进行问答是一项具有挑战性的任务。这部分是因为当前的视频-语言模型依赖于单模态推理，在处理长输入时性能下降，并且缺乏可解释性。我们提出了TV-TREES，这是第一个多模态蕴涵树生成器。TV-TREES作为一种促进可解释联合模态推理的视频理解方法，通过生成视频直接蕴涵的简单前提与高级结论之间的蕴涵关系树。随后，我们引入了多模态蕴涵树生成任务来评估此类方法的推理质量。我们的方法在具有挑战性的TVQA数据集上的实验结果展示了可解释的、具有最先进零-shot性能的完整视频剪辑，展示了与黑盒方法相比的最佳实践。

    arXiv:2402.19467v1 Announce Type: cross  Abstract: It is challenging to perform question-answering over complex, multimodal content such as television clips. This is in part because current video-language models rely on single-modality reasoning, have lowered performance on long inputs, and lack interpetability. We propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions. We then introduce the task of multimodal entailment tree generation to evaluate the reasoning quality of such methods. Our method's experimental results on the challenging TVQA dataset demonstrate intepretable, state-of-the-art zero-shot performance on full video clips, illustrating a best of both worlds contrast to black-box methods.
    
[^4]: 追踪可信度动态：重访大型语言模型的预训练期

    Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models

    [https://arxiv.org/abs/2402.19465](https://arxiv.org/abs/2402.19465)

    本文研究探索了大型语言模型在预训练期间的可信度，揭示了早期预训练LLMs已经能够区分各个可信度维度中的概念，提出了从预训练检查点中提取转向向量以增强LLM可信度的方法。

    

    确保大型语言模型（LLMs）的可信度至关重要。大多数研究集中在充分预训练的LLMs上，以更好地理解和提高LLMs的可信度。本文旨在揭示预训练的潜力，首次探索了LLMs在此期间的可信度，专注于五个关键维度：可靠性、隐私、有害度、公平性和稳健性。我们首先对LLMs应用线性探测。高探测准确度表明，\textit{早期预训练的LLMs已经能够区分每个可信度维度中的概念}。因此，为了进一步揭示预训练的潜在可能性，我们从LLM的预训练检查点中提取转向向量，以增强LLM的可信度。最后，受到~\citet{choi2023understanding} 的启发，相互信息估计受线性探测准确度的限制，我们还用相互信息探测LLMs来探究

    arXiv:2402.19465v1 Announce Type: cross  Abstract: Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that \textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM's pre-training checkpoints to enhance the LLM's trustworthiness. Finally, inspired by~\citet{choi2023understanding} that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to in
    
[^5]: 大语言模型的好奇驱动的红队对抗

    Curiosity-driven Red-teaming for Large Language Models

    [https://arxiv.org/abs/2402.19464](https://arxiv.org/abs/2402.19464)

    研究提出了一种新方法，能够通过训练红队LLM，自动化生成测试案例，以最大化引出目标LLM不良响应，以解决当前RL方法生成测试案例覆盖范围较低的问题。

    

    大型语言模型（LLMs）在许多自然语言应用中具有巨大潜力，但存在生成不正确或有毒内容的风险。为了探究LLM何时生成不需要的内容，当前的范例是招募一个人类测试者\textit{红队}来设计输入提示（即测试案例），这些提示可以引出LLMs的不良反应。然而，仅依赖人类测试者是昂贵且耗时的。近期的研究通过训练一个单独的采用强化学习（RL）的红队LLM自动化红队对抗，生成最大化引出目标LLMs不良响应的测试案例。然而，当前的RL方法只能生成少量有效的测试案例，导致对引出目标LLMs不良响应提示范围的覆盖率较低。为了克服这一限制，我们将增加生成测试案例覆盖范围的问题与.

    arXiv:2402.19464v1 Announce Type: cross  Abstract: Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a \textit{red team} of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from LLMs. However, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM. To overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases an
    
[^6]: 加速聚合物太阳能电池材料发现：自然语言处理实现的数据驱动洞见

    Accelerating materials discovery for polymer solar cells: Data-driven insights enabled by natural language processing

    [https://arxiv.org/abs/2402.19462](https://arxiv.org/abs/2402.19462)

    通过自然语言处理和数据驱动方法，我们展示了一个加速聚合物太阳能电池材料发现的工作流程，可以显著减少发现时间并预测未被报道的有潜力的受体-给体组合。

    

    我们提出了一个自然语言处理流程，用于从文献中提取聚合物太阳能电池属性数据，并模拟各种主动学习策略。虽然数据驱动方法已经被广泛建立起来，可以比爱迪生试错法更快地发现新材料，但它们的益处尚未得到量化。我们的方法展示了发现时间潜在减少约75％，相当于材料创新加速15年。我们的流程使我们能够从超过3300篇论文中提取数据，这比其他人报告的类似数据集大约多5倍。我们还训练了机器学习模型来预测功率转换效率，并使用我们的模型识别了尚未报道的有前途的受体-给体组合。因此，我们展示了一个工作流程，从已发表的文献到提取的材料属性数据，进而用于获得

    arXiv:2402.19462v1 Announce Type: cross  Abstract: We present a natural language processing pipeline that was used to extract polymer solar cell property data from the literature and simulate various active learning strategies. While data-driven methods have been well established to discover novel materials faster than Edisonian trial-and-error approaches, their benefits have not been quantified. Our approach demonstrates a potential reduction in discovery time by approximately 75 %, equivalent to a 15 year acceleration in material innovation. Our pipeline enables us to extract data from more than 3300 papers which is ~5 times larger than similar data sets reported by others. We also trained machine learning models to predict the power conversion efficiency and used our model to identify promising donor-acceptor combinations that are as yet unreported. We thus demonstrate a workflow that goes from published literature to extracted material property data which in turn is used to obtain 
    
[^7]: $\texttt{COSMIC}$: 相互信息用于任务无关摘要评估

    $\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation

    [https://arxiv.org/abs/2402.19457](https://arxiv.org/abs/2402.19457)

    $\texttt{COSMIC}$是一种以相互信息为基础的新的摘要评估方法，有效预测下游任务表现，并与人类判断相关性强。竞争性能优于$\texttt{BERTScore}$和$\texttt{ROUGE}$。

    

    评估总结质量存在显著挑战。为此，我们提出了一种新颖的面向任务的评估方法，根据总结器生成对下游任务有用且保留任务结果的摘要能力。我们在理论上建立了这些任务的结果错误概率与源文本和生成摘要之间的相互信息之间的直接关系。我们引入了$\texttt{COSMIC}$作为这一度量的实际实现，展示了它与基于人类判断的度量之间的强相关性，以及它在预测下游任务性能方面的有效性。对已建立的度量如$\texttt{BERTScore}$和$\texttt{ROUGE}$的比较分析凸显了$\texttt{COSMIC}$的竞争性能。

    arXiv:2402.19457v1 Announce Type: cross  Abstract: Assessing the quality of summarizers poses significant challenges. In response, we propose a novel task-oriented evaluation approach that assesses summarizers based on their capacity to produce summaries that are useful for downstream tasks, while preserving task outcomes. We theoretically establish a direct relationship between the resulting error probability of these tasks and the mutual information between source texts and generated summaries. We introduce $\texttt{COSMIC}$ as a practical implementation of this metric, demonstrating its strong correlation with human judgment-based metrics and its effectiveness in predicting downstream task performance. Comparative analyses against established metrics like $\texttt{BERTScore}$ and $\texttt{ROUGE}$ highlight the competitive performance of $\texttt{COSMIC}$.
    
[^8]: 用于鲁棒推理性能评估的功能基准及推理差距

    Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap

    [https://arxiv.org/abs/2402.19450](https://arxiv.org/abs/2402.19450)

    提出了一个用于评估语言模型推理能力的框架，通过功能变体的基准进行鲁棒性评估，发现静态基准和功能基准的准确性之间存在推理差距

    

    我们提出了一个框架，利用基准的功能变体对语言模型的推理能力进行鲁棒评估。解决推理测试的模型在静态问题的表现与功能变体快照相比应该没有差异。我们将MATH基准的相关片段重写为其功能变体MATH()，并将其他基准的功能化随之而来。在对当前最先进模型在MATH()快照上进行评估时，我们发现了推理差距--静态准确性与功能准确性之间的百分比差异。我们发现了在表现良好的静态基准上的最先进封闭和开放权重模型之间的推理差距，从58.35%到80.31%，但要注意的是，这些差距可能在使用更复杂提示策略时更小。在这里，我们展示了这样的模型，在真实情况下具有良好的推理性能

    arXiv:2402.19450v1 Announce Type: new  Abstract: We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35% to 80.31% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real
    
[^9]: Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models

    Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models

    [https://arxiv.org/abs/2402.19449](https://arxiv.org/abs/2402.19449)

    研究发现语言模型中的重尾类别不平衡问题导致了优化动态上的困难，Adam和基于符号的方法在这种情况下优于梯度下降。

    

    本文研究了在语言建模任务中存在的重尾类别不平衡问题，以及为什么Adam在优化大型语言模型时的表现优于梯度下降方法。我们发现，由于语言建模任务中存在的重尾类别不平衡，使用梯度下降时，与不常见单词相关的损失下降速度比与常见单词相关的损失下降速度慢。由于大多数样本来自相对不常见的单词，平均损失值在梯度下降时下降速度较慢。相比之下，Adam和基于符号的方法却不受此问题影响，并改善了所有类别的预测性能。我们在不同架构和数据类型上进行了实证研究，证明了这种行为确实是由类别不平衡引起的。

    arXiv:2402.19449v1 Announce Type: cross  Abstract: Adam has been shown to outperform gradient descent in optimizing large language transformers empirically, and by a larger margin than on other tasks, but it is unclear why this happens. We show that the heavy-tailed class imbalance found in language modeling tasks leads to difficulties in the optimization dynamics. When training with gradient descent, the loss associated with infrequent words decreases slower than the loss associated with frequent ones. As most samples come from relatively infrequent words, the average loss decreases slowly with gradient descent. On the other hand, Adam and sign-based methods do not suffer from this problem and improve predictions on all classes. To establish that this behavior is indeed caused by class imbalance, we show empirically that it persist through different architectures and data types, on language transformers, vision CNNs, and linear models. We further study this phenomenon on a linear clas
    
[^10]: ArCHer: 通过分层多轮强化学习训练语言模型代理

    ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL

    [https://arxiv.org/abs/2402.19446](https://arxiv.org/abs/2402.19446)

    本文提出了一个用于构建LLMs的多轮强化学习算法的框架

    

    大型语言模型（LLMs）的一个广泛应用案例是目标导向的决策任务（或“代理”任务），在这些任务中，LLM不仅需要为给定提示生成完成，而且需要在多轮交互中做出智能决策以完成任务（例如，与网络交互，使用工具或提供客户支持）。本文提出了一个用于构建LLMs的多轮强化学习算法的框架。

    arXiv:2402.19446v1 Announce Type: cross  Abstract: A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or "agent" tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we develop a framework for building multi-turn RL algorithms for fin
    
[^11]: 面向库导向代码生成的组合式API推荐

    Compositional API Recommendation for Library-Oriented Code Generation

    [https://arxiv.org/abs/2402.19431](https://arxiv.org/abs/2402.19431)

    提出了CAPIR（Compositional API Recommendation）来为粗粒度需求推荐API，并采用“分而治之”的策略将任务描述分解为详细的子任务。

    

    大型语言模型在代码生成方面取得了出色的表现，但在生成面向库的代码方面表现仍不尽如人意，尤其是针对LLM训练数据中不存在的库。先前的工作利用API推荐技术帮助LLMs使用库：它检索与用户需求相关的API，然后将它们作为上下文来提示LLMs。然而，开发需求可能是粗粒度的，需要结合多个细粒度API。这种粒度不一致使API推荐成为一项具有挑战性的任务。为了解决这个问题，我们提出了CAPIR（组合式API推荐），它采用“分而治之”的策略为粗粒度要求推荐API。具体而言，CAPIR采用基于LLM的分解器将粗粒度任务描述分解为几个详细的子任务。然后，CAPIR应用基于嵌入的

    arXiv:2402.19431v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved exceptional performance in code generation. However, the performance remains unsatisfactory in generating library-oriented code, especially for the libraries not present in the training data of LLMs. Previous work utilizes API recommendation technology to help LLMs use libraries: it retrieves APIs related to the user requirements, then leverages them as context to prompt LLMs. However, developmental requirements can be coarse-grained, requiring a combination of multiple fine-grained APIs. This granularity inconsistency makes API recommendation a challenging task. To address this, we propose CAPIR (Compositional API Recommendation), which adopts a "divide-and-conquer" strategy to recommend APIs for coarse-grained requirements. Specifically, CAPIR employs an LLM-based Decomposer to break down a coarse-grained task description into several detailed subtasks. Then, CAPIR applies an embedding-based
    
[^12]: Griffin: 将门控线性循环与局部注意力相结合，实现高效语言模型

    Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models

    [https://arxiv.org/abs/2402.19427](https://arxiv.org/abs/2402.19427)

    提出了 Griffin 模型，将门控线性循环与局部注意力相结合，实现高效的语言模型，该模型在推理过程中具有低延迟和高吞吐量。

    

    循环神经网络（RNNs）在长序列上具有快速推理和高效扩展的优势，但训练困难且难以扩展。本文提出了Hawk，一种具有门控线性循环的RNN，以及Griffin，一种混合模型，将门控线性循环与局部注意力相结合。Hawk在下游任务的表现超过了Mamba，而Griffin在训练时仅使用了6倍少的令牌数量却与Llama-2的表现相匹配。我们还展示了Griffin在训练期间可以对比训练时长得多的序列进行推断。我们的模型在训练时具有与Transformer相匹配的硬件效率，而在推理过程中具有更低的延迟和更高的吞吐量。我们将Griffin扩展到了14B参数，并解释了如何对我们的模型进行分片以实现高效的分布式训练。

    arXiv:2402.19427v1 Announce Type: cross  Abstract: Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.
    
[^13]: PaECTER：使用引文信息的专利级表示学习

    PaECTER: Patent-level Representation Learning using Citation-informed Transformers

    [https://arxiv.org/abs/2402.19411](https://arxiv.org/abs/2402.19411)

    PaECTER是一个专为专利设计的开放源码文档级编码器，利用引文信息对BERT进行微调，生成专利文档的数值表示，并在专利领域的相似性任务中表现优异。

    

    PaECTER是一个公开可用的、面向专利的文档级编码器，我们利用审核员添加的引文信息对BERT进行微调，为专利文档生成数值表示。与专利领域中当前最先进的模型相比，PaECTER在相似性任务中表现更好。具体来说，我们的模型在专利引文预测测试数据集上两种不同的排名评估指标上均优于下一个最佳专利特定的预训练语言模型（专利BERT）。与25个不相关的专利相比，PaECTER在平均排名1.32处预测到至少一个最相似的专利。PaECTER从专利文本生成的数值表示可用于分类、追踪知识流动或语义相似性搜索等下游任务。语义相似性搜索在发明人和专利的先前技术搜索背景中尤为重要。

    arXiv:2402.19411v1 Announce Type: cross  Abstract: PaECTER is a publicly available, open-source document-level encoder specific for patents. We fine-tune BERT for Patents with examiner-added citation information to generate numerical representations for patent documents. PaECTER performs better in similarity tasks than current state-of-the-art models used in the patent domain. More specifically, our model outperforms the next-best patent specific pre-trained language model (BERT for Patents) on our patent citation prediction test dataset on two different rank evaluation metrics. PaECTER predicts at least one most similar patent at a rank of 1.32 on average when compared against 25 irrelevant patents. Numerical representations generated by PaECTER from patent text can be used for downstream tasks such as classification, tracing knowledge flows, or semantic similarity search. Semantic similarity search is especially relevant in the context of prior art search for both inventors and paten
    
[^14]: 关于语言模型中地理表示的规模定律研究

    On the Scaling Laws of Geographical Representation in Language Models

    [https://arxiv.org/abs/2402.19406](https://arxiv.org/abs/2402.19406)

    地理知识可以在大型语言模型中观察到，随着模型规模增加而一致扩展，但更大的模型无法消除训练数据中的地理偏见。

    

    语言模型长期以来被证明在其隐藏表示中嵌入了地理信息。最近的一项研究将这一结果扩展到了大型语言模型(LLMs)。本文通过观察语言模型规模扩大时地理知识的演化，提出填补现有和最近文献之间的空白。我们展示了即使对于微小模型，地理知识也是可观测的，并且随着模型大小的增加而一致扩展。值得注意的是，我们发现更大的语言模型无法消除训练数据中固有的地理偏见。

    arXiv:2402.19406v1 Announce Type: cross  Abstract: Language models have long been shown to embed geographical information in their hidden representations. This line of work has recently been revisited by extending this result to Large Language Models (LLMs). In this paper, we propose to fill the gap between well-established and recent literature by observing how geographical knowledge evolves when scaling language models. We show that geographical knowledge is observable even for tiny models, and that it scales consistently as we increase the model size. Notably, we observe that larger language models cannot mitigate the geographical bias that is inherent to the training data.
    
[^15]: 面向实体的多模态对齐框架用于新闻图像字幕生成

    Entity-Aware Multimodal Alignment Framework for News Image Captioning

    [https://arxiv.org/abs/2402.19404](https://arxiv.org/abs/2402.19404)

    设计了面向实体的多模态对齐任务和对齐框架，提高了新闻图像字幕生成任务的性能表现。

    

    新闻图像字幕生成任务是图像字幕生成任务的一个变体，要求模型生成一个更具信息性的字幕，其中包含新闻图像和相关新闻文章。近年来，多模态大型语言模型发展迅速，并在新闻图像字幕生成任务中表现出前景。然而，根据我们的实验，常见的多模态大型语言模型在零样本设定下生成实体方面表现不佳。即使在新闻图像字幕生成数据集上进行简单微调，它们处理实体信息的能力仍然有限。为了获得一个更强大的模型来处理多模态实体信息，我们设计了两个多模态实体感知对齐任务和一个对齐框架，以对齐模型并生成新闻图像字幕。我们的方法在GoodNews数据集上将CIDEr分数提高到86.29（从72.33），在NYTimes800k数据集上将其提高到85.61（从70.83），优于先前的最先进模型。

    arXiv:2402.19404v1 Announce Type: cross  Abstract: News image captioning task is a variant of image captioning task which requires model to generate a more informative caption with news image and the associated news article. Multimodal Large Language models have developed rapidly in recent years and is promising in news image captioning task. However, according to our experiments, common MLLMs are not good at generating the entities in zero-shot setting. Their abilities to deal with the entities information are still limited after simply fine-tuned on news image captioning dataset. To obtain a more powerful model to handle the multimodal entity information, we design two multimodal entity-aware alignment tasks and an alignment framework to align the model and generate the news image captions. Our method achieves better results than previous state-of-the-art models in CIDEr score (72.33 -> 86.29) on GoodNews dataset and (70.83 -> 85.61) on NYTimes800k dataset.
    
[^16]: 硅谷人群的智慧：LLM集成预测能力达到人群准确率水平

    Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy

    [https://arxiv.org/abs/2402.19379](https://arxiv.org/abs/2402.19379)

    该研究通过将十二个LLMs组成的LLM集成方法与925名人类预测者的群体预测进行比较，发现LLM群体优于简单的无信息基准，并在统计上等效于人类群体。

    

    实践中人类预测准确性依赖于“群体智慧”效应，即通过聚合一群个体预测者的预测可以显著提高对未来事件的预测。过去关于大型语言模型（LLMs）预测能力的研究表明，作为个体预测者的前沿LLMs表现不佳，与人类群体预测比赛的黄金标准相比。我们通过使用一个由十二个LLMs组成的LLM集成方法，扩展了研究。我们将31个二元问题的聚合LLM预测与一个来自三个月预测比赛的925名人类预测者的群体预测进行比较。我们的主要分析表明，LLM群体的表现优于简单的无信息基准，并在统计上等效于人类群体。我们还观察到一种顺从效应，平均模型预测明显高于50%，尽管几乎是平等的。

    arXiv:2402.19379v1 Announce Type: cross  Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even
    
[^17]: OpenMedLM：在医学问答中，提示工程可以胜过对开源大型语言模型进行微调

    OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models

    [https://arxiv.org/abs/2402.19371](https://arxiv.org/abs/2402.19371)

    OpenMedLM 提出了一个提示平台，利用提示工程在医学问答中能够超越对开源大型语言模型进行微调，实现了在医学基准上的 SOTA 性能。

    

    LLMs 在完成一系列专门任务方面变得越来越有能力，并且可以用来扩大对医学知识的公平访问。大多数医学 LLMs 都涉及大量微调，利用专门的医学数据和大量的计算资源，因此成本高昂。许多表现前列的 LLMs 是专有的，他们的访问仅限于少数研究团体。然而，开源（OS）模型代表了医学 LLMs 的一个重要增长领域，由于性能显著提升以及提供卫生保健所需的透明度和合规性的内在能力。我们提出了 OpenMedLM，这是一个提示平台，为医学基准上的 OS LLMs 提供了最先进的性能。我们在四个医学基准（MedQA、MedMCQA、PubMedQA、MMLU 医学子集）上评估了一系列 OS 基础 LLMs（7B-70B）。我们采用了一系列提示策略，包括零s

    arXiv:2402.19371v1 Announce Type: cross  Abstract: LLMs have become increasingly capable at accomplishing a range of specialized-tasks and can be utilized to expand equitable access to medical knowledge. Most medical LLMs have involved extensive fine-tuning, leveraging specialized medical data and significant, thus costly, amounts of computational power. Many of the top performing LLMs are proprietary and their access is limited to very few research groups. However, open-source (OS) models represent a key area of growth for medical LLMs due to significant improvements in performance and an inherent ability to provide the transparency and compliance required in healthcare. We present OpenMedLM, a prompting platform which delivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks. We evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks (MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series of prompting strategies, including zero-s
    
[^18]: 基于人类阅读过程的多跳问题回答中促进显式和隐式知识

    Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process

    [https://arxiv.org/abs/2402.19350](https://arxiv.org/abs/2402.19350)

    该研究引入了一个促进显式和隐式知识的框架，用于多跳问题回答，从人类阅读过程的角度连接输入文段和预训练知识。

    

    预训练语言模型（PLMs）利用思维链（CoT）模拟人类推理和推断过程，实现了在多跳QA方面高效的性能。然而，当处理复杂问题时，PLMs的推理能力和人类之间仍存在差距。心理学研究表明，在阅读过程中，输入文段中的显式信息与人类先验知识之间存在重要联系。然而，当前的研究未能充分关注从人类认知研究的角度链接输入文段和基于PLMs预训练知识。在本研究中，我们引入了一个促进显式和隐式知识（PEI）框架，使用提示连接显式和隐式知识，与人类阅读过程对齐，用于多跳QA。我们将输入文段视为显式知识，利用它们通过统一提示推导隐式知识。

    arXiv:2402.19350v1 Announce Type: new  Abstract: Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to simulate human reasoning and inference processes, achieving proficient performance in multi-hop QA. However, a gap persists between PLMs' reasoning abilities and those of humans when tackling complex problems. Psychological studies suggest a vital connection between explicit information in passages and human prior knowledge during reading. Nevertheless, current research has given insufficient attention to linking input passages and PLMs' pre-training-based knowledge from the perspective of human cognition studies. In this study, we introduce a \textbf{P}rompting \textbf{E}xplicit and \textbf{I}mplicit knowledge (PEI) framework, which uses prompts to connect explicit and implicit knowledge, aligning with human reading process for multi-hop QA. We consider the input passages as explicit knowledge, employing them to elicit implicit knowledge through unified prompt reason
    
[^19]: 这里有一个免费午餐：使用模型合并消毒带后门的模型

    Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge

    [https://arxiv.org/abs/2402.19334](https://arxiv.org/abs/2402.19334)

    将带后门的模型与其他同类模型合并可以有效治疗后门漏洞，为后门攻击提供推理阶段的有效和高效防御

    

    通过开源倡议使预训练语言模型民主化快速推动了创新，并扩大了对尖端技术的访问。然而，这种开放性也带来了重大安全风险，包括后门攻击，其中隐藏的恶意行为由特定输入触发，损害自然语言处理（NLP）系统的完整性和可靠性。本文建议通过将带后门的模型与其他同类模型合并，可以治疗后门漏洞，即使这些模型并非全部安全。在我们的实验中，我们探索了各种模型（BERT-Base、RoBERTa-Large、Llama2-7B和Mistral-7B）和数据集（SST-2、OLID、AG News和QNLI）。与多种先进的防御方法相比，我们的方法提供了一种有效且高效的推理阶段对抗后门攻击的防御，而无需额外资源或特定知识。我们的方法始终表现优秀

    arXiv:2402.19334v1 Announce Type: new  Abstract: The democratization of pre-trained language models through open-source initiatives has rapidly advanced innovation and expanded access to cutting-edge technologies. However, this openness also brings significant security risks, including backdoor attacks, where hidden malicious behaviors are triggered by specific inputs, compromising natural language processing (NLP) system integrity and reliability. This paper suggests that merging a backdoored model with other homogeneous models can remediate backdoor vulnerabilities even if such models are not entirely secure. In our experiments, we explore various models (BERT-Base, RoBERTa-Large, Llama2-7B, and Mistral-7B) and datasets (SST-2, OLID, AG News, and QNLI). Compared to multiple advanced defensive approaches, our method offers an effective and efficient inference-stage defense against backdoor attacks without additional resources or specific knowledge. Our approach consistently outperform
    
[^20]: 通过离散语音单元预训练实现紧凑的语音翻译模型

    Compact Speech Translation Models via Discrete Speech Units Pretraining

    [https://arxiv.org/abs/2402.19333](https://arxiv.org/abs/2402.19333)

    通过在离散语音单元上预训练较小模型，以蒸馏SSL模型的知识，实现了紧凑的语音翻译模型，具有短推理管道和适用于低资源环境等优点

    

    使用自监督学习（SSL）作为模型初始化如今在语音翻译（ST）中获得强大结果是常见的。然而，它们也会占用大量内存，阻碍了设备部署。本文利用SSL模型通过在其离散语音单元（DSU）上预训练较小模型。我们在1）Filterbank-to-DSU和2）DSU-to-Translation数据上预训练编码器-解码器模型，然后取自1）的编码器和来自2）的解码器来初始化一个新模型，在有限的语音翻译数据上微调。通过使用DSU预训练来提炼SSL模型的知识，最终模型变得紧凑。我们的方法相比于使用DSU作为模型输入有几个优点，比如推理管道更短和对（DSU）标记化的鲁棒性。与ASR预训练相比，它不需要转录，使其适用于资源匮乏的环境。在CoVoST-2 X-En上的评估显示我们的方法是

    arXiv:2402.19333v1 Announce Type: new  Abstract: Using Self-Supervised Learning (SSL) as model initialization is now common to obtain strong results in Speech Translation (ST). However, they also impose a large memory footprint, hindering on-device deployment. In this paper, we leverage the SSL models by pretraining smaller models on their Discrete Speech Units (DSU). We pretrain encoder-decoder models on 1) Filterbank-to-DSU and 2) DSU-to-Translation data, and take the encoder from 1) and the decoder from 2) to initialise a new model, finetuning this on limited speech-translation data. The final model becomes compact by using the DSU pretraining to distil the knowledge of the SSL model. Our method has several benefits over using DSU as model inputs, such as shorter inference pipeline and robustness over (DSU) tokenization. In contrast to ASR pretraining, it does not require transcripts, making it applicable to low-resource settings. Evaluation on CoVoST-2 X-En shows that our method is
    
[^21]: WanJuan-CC：一个安全且高质量的开源英文网络文本数据集

    WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset

    [https://arxiv.org/abs/2402.19282](https://arxiv.org/abs/2402.19282)

    WanJuan-CC是一个安全高质量的开源英文网络文本数据集，通过处理大规模的Common Crawl数据并经过多项筛选和过滤步骤得到，为语言模型的预训练提供了重要资源。

    

    本文介绍了 WanJuan-CC，这是一个安全且高质量的开源英文网络文本数据集，来源于Common Crawl数据。研究解决了为语言模型构建大规模预训练数据集所面临的挑战，这需要大量高质量数据。设计了一个全面的流程来处理Common Crawl数据，包括提取、启发式规则过滤、模糊去重、内容安全过滤和数据质量过滤。从大约680亿个原始英文文档中，我们获得了22万亿标记的安全数据，并从中选出了10万亿标记的高质量数据作为WanJuan-CC的一部分。我们已经开源了这个数据集中的3000亿标记。该论文还提供了与数据质量相关的统计信息，使用户可以根据自己的需求选择适当的数据。为评估数据集的质量和实用性，我们使用WanJuan-CC训练了10亿参数和30亿参数的模型。

    arXiv:2402.19282v1 Announce Type: new  Abstract: This paper presents WanJuan-CC, a safe and high-quality open-sourced English webtext dataset derived from Common Crawl data. The study addresses the challenges of constructing large-scale pre-training datasets for language models, which require vast amounts of high-quality data. A comprehensive process was designed to handle Common Crawl data, including extraction, heuristic rule filtering, fuzzy deduplication, content safety filtering, and data quality filtering. From approximately 68 billion original English documents, we obtained 2.22T Tokens of safe data and selected 1.0T Tokens of high-quality data as part of WanJuan-CC. We have open-sourced 300B Tokens from this dataset. The paper also provides statistical information related to data quality, enabling users to select appropriate data according to their needs. To evaluate the quality and utility of the dataset, we trained 1B-parameter and 3B-parameter models using WanJuan-CC and ano
    
[^22]: PlanGPT: 用定制语言模型和高效检索增强城市规划

    PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval

    [https://arxiv.org/abs/2402.19273](https://arxiv.org/abs/2402.19273)

    PlanGPT是第一个专为城市和空间规划量身定制的大型语言模型，通过定制数据库检索框架、领域特定微调和先进工具功能，实现了高效的性能表现和提供高质量规划响应。

    

    在城市规划领域，通用大型语言模型往往难以满足规划者的特定需求。生成城市规划文本、检索相关信息和评估规划文件等任务都存在独特挑战。为提高城市专业人员的效率并克服这些障碍，我们引入了PlanGPT，第一个专为城市和空间规划量身定制的大型语言模型。通过与中国城市规划研究院等机构的合作努力开发，PlanGPT利用定制的本地数据库检索框架、基础模型的领域特定微调和先进的工具功能。实证测试表明，PlanGPT取得了先进性能，提供了精确适应城市规划复杂性的高质量响应。

    arXiv:2402.19273v1 Announce Type: new  Abstract: In the field of urban planning, general-purpose large language models often struggle to meet the specific needs of planners. Tasks like generating urban planning texts, retrieving related information, and evaluating planning documents pose unique challenges. To enhance the efficiency of urban professionals and overcome these obstacles, we introduce PlanGPT, the first specialized Large Language Model tailored for urban and spatial planning. Developed through collaborative efforts with institutions like the Chinese Academy of Urban Planning, PlanGPT leverages a customized local database retrieval framework, domain-specific fine-tuning of base models, and advanced tooling capabilities. Empirical tests demonstrate that PlanGPT has achieved advanced performance, delivering responses of superior quality precisely tailored to the intricacies of urban planning.
    
[^23]: 强大的无监督数据选择指导：捕获领域特定机器翻译中令人困扰的命名实体

    Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation

    [https://arxiv.org/abs/2402.19267](https://arxiv.org/abs/2402.19267)

    提出了一种新颖的无监督数据选择方法，通过捕获领域特定机器翻译中令人困扰的命名实体，实现了高质量翻译效果。

    

    使用大量数据集可以训练多语言机器翻译模型；然而，这些模型通常无法准确翻译专业领域中的句子。获得和翻译领域特定数据虽然成本高昂，但对于高质量翻译是不可避免的。因此，在无监督设置中找到最“有效”的数据成为减少标注成本的实用策略。最近的研究表明，可以通过选择“适当困难的数据”来找到这些有效数据，这意味着数据不应过于困难或过于简单，尤其是在数据量有限的情况下。然而，我们发现建立无监督数据选择标准仍具挑战性，因为“适当困难度”可能因所训练的数据领域而异。我们引入了一种新颖的无监督数据选择方法，‘Capturing Perplexing Named Entities’。

    arXiv:2402.19267v1 Announce Type: cross  Abstract: Employing extensive datasets enables the training of multilingual machine translation models; however, these models often fail to accurately translate sentences within specialized domains. Although obtaining and translating domain-specific data incurs high costs, it is inevitable for high-quality translations. Hence, finding the most 'effective' data with an unsupervised setting becomes a practical strategy for reducing labeling costs. Recent research indicates that this effective data could be found by selecting 'properly difficult data' based on its volume. This means the data should not be excessively challenging or overly simplistic, especially if the amount of data is limited. However, we found that establishing a criterion for unsupervised data selection remains challenging, as the 'proper difficulty' might vary based on the data domain being trained on. We introduce a novel unsupervised data selection method, 'Capturing Perplexi
    
[^24]: GSM-Plus：评估LLMs作为数学问题解决者的稳健性的全面基准

    GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers

    [https://arxiv.org/abs/2402.19255](https://arxiv.org/abs/2402.19255)

    通过引入对抗式小学数学数据集（GSM-Plus），评估了25个LLMs和4种提示技术，在广泛的问题变化中展示LLMs的数学推理能力，并发现它们的表现远非稳健。

    

    大型语言模型（LLMs）在各种数学推理基准上取得了令人印象深刻的表现。然而，关于这些模型是否真正理解并应用数学知识，还是仅仅依赖于数学推理的捷径，存在越来越多的争论。一个基本且经常发生的证据是，当数学问题稍作更改时，LLMs可能会出现错误行为。这促使我们通过测试各种问题变化来评估LLMs的数学推理能力的稳健性。我们引入了对抗式小学数学（\datasetname）数据集，这是对GSM8K的扩展，并添加了各种数学扰动。我们对25个LLMs和4种提示技术进行的实验表明，虽然LLMs展现出不同水平的数学推理能力，但它们的表现远非稳健。特别是，即使是在GSM8K中已解决的问题，LLMs也可能出错。

    arXiv:2402.19255v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (\datasetname) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes 
    
[^25]: 让大型语言模型应对最新挑战！一个中文动态问答基准测试

    Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark

    [https://arxiv.org/abs/2402.19248](https://arxiv.org/abs/2402.19248)

    本论文提出了CDQA，一个中文动态问答基准测试，致力于提高中文大型语言模型（LLMs）回答动态问题的能力，并通过高质量数据和精细样本分类实现了对LLMs能力更细致的观察。实验结果表明，CDQA具有挑战性且值得进一步研究。

    

    arXiv:2402.19248v1 公告类型：新  摘要：如何更好地评估大型语言模型（LLMs）的能力是当前LLMs研究的焦点和热点。先前的研究指出，由于大规模迭代更新LLMs的成本极高，它们经常无法很好地回答最新的动态问题。为了促进中文LLMs回答动态问题的能力提升，在本文中，我们引入了 CDQA，一个包含与中国互联网上最新新闻相关的问答对的中文动态问答基准测试。我们通过将人类和模型结合的流程获得高质量数据，并根据答案变化频率精细分类样本，以便更细致地观察LLMs的能力。我们还在CDQA上评估和分析了主流和先进的中文LLMs。广泛的实验和宝贵的见解表明，我们提出的CDQA是具有挑战性且值得进一步研究的。

    arXiv:2402.19248v1 Announce Type: new  Abstract: How to better evaluate the capabilities of Large Language Models (LLMs) is the focal point and hot topic in current LLMs research. Previous work has noted that due to the extremely high cost of iterative updates of LLMs, they are often unable to answer the latest dynamic questions well. To promote the improvement of Chinese LLMs' ability to answer dynamic questions, in this paper, we introduce CDQA, a Chinese Dynamic QA benchmark containing question-answer pairs related to the latest news on the Chinese Internet. We obtain high-quality data through a pipeline that combines humans and models, and carefully classify the samples according to the frequency of answer changes to facilitate a more fine-grained observation of LLMs' capabilities. We have also evaluated and analyzed mainstream and advanced Chinese LLMs on CDQA. Extensive experiments and valuable insights suggest that our proposed CDQA is challenging and worthy of more further stud
    
[^26]: 基于记忆增强的生成对抗变压器

    Memory-Augmented Generative Adversarial Transformers

    [https://arxiv.org/abs/2402.19218](https://arxiv.org/abs/2402.19218)

    通过在标准变压器架构中增加额外的记忆库和注意力层，该研究提出了一种可以提高变压器生成语言准确性的方法。

    

    依赖大型语言模型（如变压器）的会话AI系统在将外部数据（如事实）与其生成的语言相互交织时存在困难。普通的变压器架构并未设计用于准确回答事实问题。本文探讨了解决这一问题的可能途径。我们建议通过在标准变压器架构上扩展额外信息的记忆库（如来自知识库的事实）和用于处理这一记忆的额外注意力层。我们将这个增强记忆添加到启发式生成对抗网络的变压器架构中。这种设置允许在变压器生成的语言上实施任意的快乐条件。首先，我们展示了这种机制如何被用于处理目标导向对话中的事实问题。其次，我们展示了我们的方法如何对应用程序可能是有用的。

    arXiv:2402.19218v1 Announce Type: new  Abstract: Conversational AI systems that rely on Large Language Models, like Transformers, have difficulty interweaving external data (like facts) with the language they generate. Vanilla Transformer architectures are not designed for answering factual questions with high accuracy. This paper investigates a possible route for addressing this problem. We propose to extend the standard Transformer architecture with an additional memory bank holding extra information (such as facts drawn from a knowledge base), and an extra attention layer for addressing this memory. We add this augmented memory to a Generative Adversarial Network-inspired Transformer architecture. This setup allows for implementing arbitrary felicity conditions on the generated language of the Transformer. We first demonstrate how this machinery can be deployed for handling factual questions in goal-oriented dialogues. Secondly, we demonstrate that our approach can be useful for app
    
[^27]: 基于开放数据的巴西葡萄牙语编码器语言模型PeLLE

    PeLLE: Encoder-based language models for Brazilian Portuguese based on open data

    [https://arxiv.org/abs/2402.19204](https://arxiv.org/abs/2402.19204)

    PeLLE是基于RoBERTa架构的巴西葡萄牙语编码器语言模型系列，在其预训练中使用了Carolina语料库的开放数据，研究发现在多个下游任务中，使用较大模型的性能更好，但有些任务会因为使用较小但精选的数据在预训练中而有所益处。

    

    在本文中，我们介绍了PeLLE，这是一个基于RoBERTa架构的大型语言模型系列，用于巴西葡萄牙语，训练数据来自Carolina语料库。为了可重复的结果，我们描述了模型的预训练细节。我们还评估了PeLLE模型与一组现有的多语言和PT-BR精调预训练的Transformer LLM编码器，在多个下游任务中对比了大型模型与较小但经过筛选的预训练模型的性能。我们得出结论，一些任务使用更大的模型效果更好，但在预训练中，一些任务受益于较小但精选的数据。

    arXiv:2402.19204v1 Announce Type: new  Abstract: In this paper we present PeLLE, a family of large language models based on the RoBERTa architecture, for Brazilian Portuguese, trained on curated, open data from the Carolina corpus. Aiming at reproducible results, we describe details of the pretraining of the models. We also evaluate PeLLE models against a set of existing multilingual and PT-BR refined pretrained Transformer-based LLM encoders, contrasting performance of large versus smaller-but-curated pretrained models in several downstream tasks. We conclude that several tasks perform better with larger models, but some tasks benefit from smaller-but-curated data in its pretraining.
    
[^28]: PRSA：大型语言模型的提示反盗窃攻击

    PRSA: Prompt Reverse Stealing Attacks against Large Language Models

    [https://arxiv.org/abs/2402.19200](https://arxiv.org/abs/2402.19200)

    本文提出了针对商业LLMs的提示反窃取攻击框架PRSA，通过分析输入-输出对的关键特征实现攻击。

    

    提示作为重要的知识产权，使得大型语言模型（LLMs）能够执行特定任务而无需微调，突显了它们不断增长的重要性。随着基于提示的服务的崛起，如提示市场和LLM应用程序，提供者经常通过输入-输出示例展示提示的能力，以吸引用户。然而，这种范式提出了一个关键的安全问题：暴露输入-输出对是否会对潜在提示泄漏构成风险，侵犯开发者的知识产权？就我们所知，这个问题还没有得到全面探讨。为了弥补这一空白，在本文中，我们进行了首次深入探讨，并提出了一个针对商业LLMs的提示反窃取攻击框架，即PRSA。PRSA的主要思想是通过分析输入-输出对的关键特征，我们模仿并g

    arXiv:2402.19200v1 Announce Type: cross  Abstract: Prompt, recognized as crucial intellectual property, enables large language models (LLMs) to perform specific tasks without the need of fine-tuning, underscoring their escalating importance. With the rise of prompt-based services, such as prompt marketplaces and LLM applications, providers often display prompts' capabilities through input-output examples to attract users. However, this paradigm raises a pivotal security concern: does the exposure of input-output pairs pose the risk of potential prompt leakage, infringing on the intellectual property rights of the developers? To our knowledge, this problem still has not been comprehensively explored yet. To remedy this gap, in this paper, we perform the first in depth exploration and propose a novel attack framework for reverse-stealing prompts against commercial LLMs, namely PRSA. The main idea of PRSA is that by analyzing the critical features of the input-output pairs, we mimic and g
    
[^29]: 通过长文本编码器提升罗马尼亚法律判决预测的能力

    Improving Legal Judgement Prediction in Romanian with Long Text Encoders

    [https://arxiv.org/abs/2402.19170](https://arxiv.org/abs/2402.19170)

    本研究关注通过扩展Transformer模型的序列长度来更好理解法律语料库中的长文档，并在罗马尼亚的4个LJP数据集上进行了广泛实验。

    

    最近几年，自然语言处理（NLP）领域取得了惊人的新成果，在各种任务上实现了接近人类水平的性能。法律NLP领域也随之发展迅猛。然而，通用模型并不直接适用于法律领域。由于其专业词汇、长文档等特点，法律NLP通常需要特定模型和方法。本文研究了专业和通用模型用于预测法律案例的最终裁决的方法，即法律判决预测（LJP）任务。我们特别关注如何扩展基于Transformer模型的序列长度，以更好地理解法律语料库中的长文档。在来自两个来源、规模和文档长度显著不同时的4个罗马尼亚LJP数据集上进行了大量实验，结果显示专门模型...

    arXiv:2402.19170v1 Announce Type: cross  Abstract: In recent years,the entire field of Natural Language Processing (NLP) has enjoyed amazing novel results achieving almost human-like performance on a variety of tasks. Legal NLP domain has also been part of this process, as it has seen an impressive growth. However, general-purpose models are not readily applicable for legal domain. Due to the nature of the domain (e.g. specialized vocabulary, long documents) specific models and methods are often needed for Legal NLP. In this work we investigate both specialized and general models for predicting the final ruling of a legal case, task known as Legal Judgment Prediction (LJP). We particularly focus on methods to extend to sequence length of Transformer-based models to better understand the long documents present in legal corpora. Extensive experiments on 4 LJP datasets in Romanian, originating from 2 sources with significantly different sizes and document lengths, show that specialized mo
    
[^30]: 在需要时教授大型语言模型一种未知语言

    Teaching Large Language Models an Unseen Language on the Fly

    [https://arxiv.org/abs/2402.19167](https://arxiv.org/abs/2402.19167)

    通过提示，在飞行中教授大型语言模型一种未知语言，提出DiPMT ++框架，通过上下文学习使LLMs适应看不见的语言，并实现了壮语和汉语之间的翻译性能显著提升，并展示了该框架在帮助人类翻译完全未知语言方面的实用性。

    

    现有的大型语言模型在支持许多低资源语言方面存在困难，特别是在极低资源语言方面，在这些语言中，有效参数更新所需的训练数据极少。因此，我们研究了LLM是否可以仅通过提示在飞行中学习一种新语言。为了研究这个问题，我们为壮语收集了一个研究套件，这是当前没有LLMs支持的一种语言。我们介绍了一种名为DiPMT++的框架，用于通过上下文学习将LLMs适应看不见的语言。使用一本词典和仅有5K对平行句子，DiPMT++将GPT-4的性能从0提升到16 BLEU，用于汉语到壮语的翻译，并实现了壮语到汉语的32 BLEU。此外，我们展示了这一框架在帮助人类翻译完全未知语言方面的实际用途，这有助于维护语言多样性。

    arXiv:2402.19167v1 Announce Type: new  Abstract: Existing large language models struggle to support numerous low-resource languages, particularly the extremely low-resource ones where there is minimal training data available for effective parameter updating. We thus investigate whether LLMs can learn a new language on the fly solely through prompting. To study this question, we collect a research suite for Zhuang, a language supported by no LLMs currently. We introduce \textsc{DiPMT++}, a framework for adapting LLMs to unseen languages by in-context learning. Using a dictionary and only 5K parallel sentences, \textsc{DiPMT++} significantly enhances the performance of GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and achieves 32 BLEU for Zhuang-to-Chinese translation. Furthermore, we demonstrate the practical utility of this framework in aiding humans to translate completely unseen languages, which could contribute to the preservation of linguistic diversity.
    
[^31]: 用基于网络摄像头的凝视数据作为人类原因标注的替代方案的评估

    Evaluating Webcam-based Gaze Data as an Alternative for Human Rationale Annotations

    [https://arxiv.org/abs/2402.19133](https://arxiv.org/abs/2402.19133)

    论文讨论了使用基于网络摄像头的眼动数据作为评估重要性评分的替代方案，通过比较不同语言模型对WebQAmGaze数据集的表现，结果表明凝视数据提供了宝贵的语言学信息。

    

    在评估自然语言处理中的解释性方法时，以手动注释的输入跨度形式呈现的原因通常作为基准真相。然而，它们耗时且往往受注释过程的影响。在本文中，我们讨论了当评估重要性评分时，人类凝视，即基于网络摄像头的眼动跟踪记录，是否构成一个有效的替代方案。我们评估了凝视数据提供的附加信息，比如总阅读时间、凝视熵以及解码准确性，与人类原因标注相关。我们将WebQAmGaze，一个用于信息检索QA的多语言数据集，与4种不同的多语言Transformer语言模型（mBERT，distil-mBERT，XLMR和XLMR-L）以及3种语言（英语，西班牙语和德语）的注意力和可解释性重要性分数进行比较。我们的流程可以轻松应用于其他任务和语言。我们的研究结果表明，凝视数据提供了宝贵的语言学信息。

    arXiv:2402.19133v1 Announce Type: new  Abstract: Rationales in the form of manually annotated input spans usually serve as ground truth when evaluating explainability methods in NLP. They are, however, time-consuming and often biased by the annotation process. In this paper, we debate whether human gaze, in the form of webcam-based eye-tracking recordings, poses a valid alternative when evaluating importance scores. We evaluate the additional information provided by gaze data, such as total reading times, gaze entropy, and decoding accuracy with respect to human rationale annotations. We compare WebQAmGaze, a multilingual dataset for information-seeking QA, with attention and explainability-based importance scores for 4 different multilingual Transformer-based language models (mBERT, distil-mBERT, XLMR, and XLMR-L) and 3 languages (English, Spanish, and German). Our pipeline can easily be applied to other tasks and languages. Our findings suggest that gaze data offers valuable linguist
    
[^32]: VIXEN: 图像差异字幕的视觉文本比较网络

    VIXEN: Visual Text Comparison Network for Image Difference Captioning

    [https://arxiv.org/abs/2402.19119](https://arxiv.org/abs/2402.19119)

    提出了一种名为VIXEN的技术，能够用文本简洁地总结一对图像之间的视觉差异，为突出内容操作提供潜在的缓解方法

    

    我们提出了VIXEN - 一种能够用文本简洁地总结一对图像之间的视觉差异，以突出其中的任何内容操作的技术。我们的网络以成对的方式线性映射图像特征，构建出一个软提示，用于预训练大型语言模型。我们通过在最近的InstructPix2Pix数据集中利用提示到提示编辑框架生成的合成操作图像来训练，解决了现有图像差异字幕（IDC）数据集中训练数据量少，操作类型多样性不足的挑战。我们通过GPT-3生成的变化摘要来扩充这个数据集。我们展示了VIXEN能为不同图像内容和编辑类型生成最新的易懂的差异字幕，为防止通过操纵图像内容传播的信息错误提供潜在的缓解方法。代码和数据可在http://github.com/alexblck/vixen获取。

    arXiv:2402.19119v1 Announce Type: cross  Abstract: We present VIXEN - a technique that succinctly summarizes in text the visual differences between a pair of images in order to highlight any content manipulation present. Our proposed network linearly maps image features in a pairwise manner, constructing a soft prompt for a pretrained large language model. We address the challenge of low volume of training data and lack of manipulation variety in existing image difference captioning (IDC) datasets by training on synthetically manipulated images from the recent InstructPix2Pix dataset generated via prompt-to-prompt editing framework. We augment this dataset with change summaries produced via GPT-3. We show that VIXEN produces state-of-the-art, comprehensible difference captions for diverse image contents and edit types, offering a potential mitigation against misinformation disseminated via manipulated image content. Code and data are available at http://github.com/alexblck/vixen
    
[^33]: 如何理解“支持”？一种隐式增强因果推断方法用于弱监督短语定位

    How to Understand "Support"? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding

    [https://arxiv.org/abs/2402.19116](https://arxiv.org/abs/2402.19116)

    提出了一种隐式增强因果推断方法（IECI），用于解决弱监督短语定位任务中的挑战，通过标注高质量数据集进行评估，并相比基线方法展现出明显优势。

    

    弱监督短语定位（WPG）是一个新兴的任务，用于推断细粒度短语-区域匹配，仅利用粗粒度的句子-图像对进行训练。然而，现有关于WPG的研究很大程度上忽略了隐式短语-区域匹配关系，这对于评估模型理解深层多模态语义的能力至关重要。为此，本文提出了一种隐式增强因果推断（IECI）方法来解决对建模隐式关系和突出显性关系的挑战。具体而言，该方法分别利用干预和反事实技术来应对上述两个挑战。此外，还标注了一个高质量的隐式增强数据集来评估IECI，详细评估显示IECI相比最先进基线方法有很大优势。特别地，我们观察到了一个有趣的发现。

    arXiv:2402.19116v1 Announce Type: cross  Abstract: Weakly-supervised Phrase Grounding (WPG) is an emerging task of inferring the fine-grained phrase-region matching, while merely leveraging the coarse-grained sentence-image pairs for training. However, existing studies on WPG largely ignore the implicit phrase-region matching relations, which are crucial for evaluating the capability of models in understanding the deep multimodal semantics. To this end, this paper proposes an Implicit-Enhanced Causal Inference (IECI) approach to address the challenges of modeling the implicit relations and highlighting them beyond the explicit. Specifically, this approach leverages both the intervention and counterfactual techniques to tackle the above two challenges respectively. Furthermore, a high-quality implicit-enhanced dataset is annotated to evaluate IECI and detailed evaluations show the great advantages of IECI over the state-of-the-art baselines. Particularly, we observe an interesting findi
    
[^34]: 震撼基础的细语：分析和减轻大型语言模型中的虚假前提幻觉

    Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models

    [https://arxiv.org/abs/2402.19103](https://arxiv.org/abs/2402.19103)

    该论文对大型语言模型中的虚假前提幻觉进行了全面分析，提出了一种名为“FAITH”的方法，用于减轻虚假前提幻觉。

    

    大型语言模型(LLMs)展现出令人印象深刻的能力，但仍然受到幻觉问题的困扰。这个问题的一个重要类型是虚假前提幻觉，我们定义为当LLMs面对虚假前提问题时生成幻觉文本的现象。本文对虚假前提幻觉进行了全面分析，并阐明了其内部工作机制：一小部分注意力头(我们将其指定为虚假前提头)扰乱了知识提取过程，导致虚假前提幻觉的发生。基于我们的分析，我们提出了“FAITH”(虚假前提注意力头约束以减轻幻觉)这一新颖有效的方法来减轻虚假前提幻觉。它在模型推理过程中约束虚假前提注意力头。令人印象深刻的是，

    arXiv:2402.19103v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose \textbf{FAITH} (\textbf{F}alse premise \textbf{A}ttention head constra\textbf{I}ining for mi\textbf{T}igating \textbf{H}allucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively,
    
[^35]: TEncDM: 在语言模型编码空间中理解扩散模型的属性

    TEncDM: Understanding the Properties of Diffusion Model in the Space of Language Model Encodings

    [https://arxiv.org/abs/2402.19097](https://arxiv.org/abs/2402.19097)

    通过在语言模型编码空间中训练模型，并使用基于Transformer的解码器以及自我调节，本文提出了名为TEncDM的文本编码扩散模型，在两个文本生成任务上展示了其优越性

    

    受到扩散模型在各个领域取得成功的启发，许多研究论文提出了将其应用于文本数据的方法。尽管有这些努力，但没有一种方法能够达到大型语言模型的质量。本文对文本扩散模型的关键组件进行了全面分析，并介绍了一种名为Text Encoding Diffusion Model (TEncDM)的新方法。我们在语言模型编码空间中训练我们的模型，而不是通常使用的标记嵌入空间。此外，我们提出使用基于Transformer的解码器，利用上下文信息进行文本重构。我们还分析了自我调节，并发现这会增加模型输出的数量级，从而减少推理阶段的去噪步骤数量。在两个下游文本生成任务QQP和XSum上对TEncDM的评估表明其优越性。

    arXiv:2402.19097v1 Announce Type: new  Abstract: Drawing inspiration from the success of diffusion models in various domains, numerous research papers proposed methods for adapting them to text data. Despite these efforts, none of them has managed to achieve the quality of the large language models. In this paper, we conduct a comprehensive analysis of key components of the text diffusion models and introduce a novel approach named Text Encoding Diffusion Model (TEncDM). Instead of the commonly used token embedding space, we train our model in the space of the language model encodings. Additionally, we propose to use a Transformer-based decoder that utilizes contextual information for text reconstruction. We also analyse self-conditioning and find that it increases the magnitude of the model outputs, allowing the reduction of the number of denoising steps at the inference stage. Evaluation of TEncDM on two downstream text generation tasks, QQP and XSum, demonstrates its superiority ove
    
[^36]: 对语义变化特征的调查

    Survey in Characterization of Semantic Change

    [https://arxiv.org/abs/2402.19088](https://arxiv.org/abs/2402.19088)

    语义变化对计算语言学算法的结果质量可能会产生影响，因此重要性日益凸显。

    

    活语言不断发展，以吸纳人类社会的文化变化。这种演变通过新词语（新单词）或单词的语义变化（赋予已有单词新的含义）来体现。理解单词的含义对解释来自不同文化（地方用语或俚语）、领域（例如技术术语）或时代的文本至关重要。在计算机科学中，这些单词与计算语言学算法相关，例如翻译、信息检索、问答等。语义变化可能会影响这些算法的结果质量。因此，了解和形式化表征这些变化是很重要的。研究这种影响是计算语言学界近期引起关注的问题。几种方法提出了检测语义变化的方法，具有较高的精度，但需要更多努力来对其进行表征。

    arXiv:2402.19088v1 Announce Type: cross  Abstract: Live languages continuously evolve to integrate the cultural change of human societies. This evolution manifests through neologisms (new words) or \textbf{semantic changes} of words (new meaning to existing words). Understanding the meaning of words is vital for interpreting texts coming from different cultures (regionalism or slang), domains (e.g., technical terms), or periods. In computer science, these words are relevant to computational linguistics algorithms such as translation, information retrieval, question answering, etc. Semantic changes can potentially impact the quality of the outcomes of these algorithms. Therefore, it is important to understand and characterize these changes formally. The study of this impact is a recent problem that has attracted the attention of the computational linguistics community. Several approaches propose methods to detect semantic changes with good precision, but more effort is needed to charact
    
[^37]: 可控偏好优化：朝着可控多目标对齐方向发展

    Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment

    [https://arxiv.org/abs/2402.19085](https://arxiv.org/abs/2402.19085)

    引入了可控偏好优化（CPO）方法，明确为不同目标指定偏好分数，从而引导模型生成符合需求的响应。

    

    人工智能中的对齐工作旨在追求模型响应与人类偏好和价值的一致性。本文引入了可控偏好优化（CPO）方法，明确为不同目标指定偏好分数，从而引导模型生成符合需求的响应。实验分析表明，经过对齐的模型可以提供符合各种偏好的响应。

    arXiv:2402.19085v1 Announce Type: new  Abstract: Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the "alignment tax" -a compromise where enhancements in alignment within one objective (e.g.,harmlessness) can diminish performance in others (e.g.,helpfulness). However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among t
    
[^38]: 用语义驱动的对抗生成模型指出关系抽取模型的缺陷

    Pointing out the Shortcomings of Relation Extraction Models with Semantically Motivated Adversarials

    [https://arxiv.org/abs/2402.19076](https://arxiv.org/abs/2402.19076)

    本文通过替换实体提及来生成对抗性示例，揭示了关系抽取模型的捷径特性缺陷

    

    近年来，大型语言模型在各种自然语言处理任务中取得了最先进的性能。然而，调查显示这些模型往往依赖于捷径特性，导致不准确的预测，使模型在泛化到分布之外（OOD）样本时不可靠。本文介绍了几种基于语义动机的策略，通过替换实体提及来生成对抗性示例，以此来探究关系抽取模型的缺陷。

    arXiv:2402.19076v1 Announce Type: new  Abstract: In recent years, large language models have achieved state-of-the-art performance across various NLP tasks. However, investigations have shown that these models tend to rely on shortcut features, leading to inaccurate predictions and causing the models to be unreliable at generalization to out-of-distribution (OOD) samples. For instance, in the context of relation extraction (RE), we would expect a model to identify the same relation independently of the entities involved in it. For example, consider the sentence "Leonardo da Vinci painted the Mona Lisa" expressing the created(Leonardo_da_Vinci, Mona_Lisa) relation. If we substiute "Leonardo da Vinci" with "Barack Obama", then the sentence still expresses the created relation. A robust model is supposed to detect the same relation in both cases. In this work, we describe several semantically-motivated strategies to generate adversarial examples by replacing entity mentions and investigat
    
[^39]: 探索大型语言模型在总结心理健康咨询会话中的功效：基准研究

    Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: A Benchmark Study

    [https://arxiv.org/abs/2402.19052](https://arxiv.org/abs/2402.19052)

    本研究评估大型语言模型在选择性总结心理健康咨询会话中的功效，并引入了MentalCLOUDS数据集作为基准，以探索其在辅导组件指导的总结任务中的性能。

    

    全面总结会话有助于在心理健康咨询中有效地保持连续性，促进知情疗法规划。然而，手动总结存在重大挑战，分散专家注意力，偏离核心咨询流程。本研究评估了最先进的大型语言模型（LLMs）在通过基于方面的总结有选择性地总结各种疗法会话组件方面的效果，旨在对其性能进行基准测试。我们引入了MentalCLOUDS，一个由导师组件指导的总结数据集，包括191个重点关注三个不同辅导组件（也称为辅导方面）的辅导会话。此外，我们评估了11种最先进的LLM在解决辅导中基于组件的总结任务方面的能力。利用标准总结度量定量评估生成的总结，并在质量上验证

    arXiv:2402.19052v1 Announce Type: new  Abstract: Comprehensive summaries of sessions enable an effective continuity in mental health counseling, facilitating informed therapy planning. Yet, manual summarization presents a significant challenge, diverting experts' attention from the core counseling process. This study evaluates the effectiveness of state-of-the-art Large Language Models (LLMs) in selectively summarizing various components of therapy sessions through aspect-based summarization, aiming to benchmark their performance. We introduce MentalCLOUDS, a counseling-component guided summarization dataset consisting of 191 counseling sessions with summaries focused on three distinct counseling components (aka counseling aspects). Additionally, we assess the capabilities of 11 state-of-the-art LLMs in addressing the task of component-guided summarization in counseling. The generated summaries are evaluated quantitatively using standard summarization metrics and verified qualitatively
    
[^40]: PopALM: 面向社交媒体热门事件回应预测的受欢迎度对齐语言模型

    PopALM: Popularity-Aligned Language Models for Social Media Trendy Response Prediction

    [https://arxiv.org/abs/2402.18950](https://arxiv.org/abs/2402.18950)

    提出了Popularity-Aligned Language Models (PopALM)来区分受大众喜欢的回复，通过强化学习和课程学习来提高高级语言模型在社交媒体热门事件回应预测中的性能

    

    arXiv:2402.18950v1 公告类型：新 抽象：社交媒体平台每天都在展示数百万事件。为了初步预测对这些事件的主流公众反应，我们研究时髦的响应预测，以自动生成对社交媒体事件的热门用户回复。虽然先前的工作侧重于生成响应而不考虑受欢迎程度，我们提出了受欢迎度对齐语言模型（PopALM），通过强化学习区分受大众喜欢的回复。鉴别用户“喜欢”的嘈杂标签，我们定制了课程学习在近端策略优化（PPO）中，以帮助模型捕捉基于易到难的训练的关键样本。在实验中，我们构建了一个大规模的微博数据集用于时髦的响应预测，其结果表明PopALM可以帮助提升高级语言模型的性能。

    arXiv:2402.18950v1 Announce Type: new  Abstract: Social media platforms are daily exhibiting millions of events. To preliminarily predict the mainstream public reaction to these events, we study trendy response prediction to automatically generate top-liked user replies to social media events. While previous works focus on generating responses without factoring in popularity, we propose Popularity-Aligned Language Models (PopALM) to distinguish responses liked by a larger audience through reinforcement learning. Recognizing the noisy labels from user "likes", we tailor-make curriculum learning in proximal policy optimization (PPO) to help models capture the essential samples for easy-to-hard training. In experiments, we build a large-scale Weibo dataset for trendy response prediction, and its results show that PopALM can help boost the performance of advanced language models.
    
[^41]: Syntactic Ghost：一种对预训练语言模型进行的无感知通用后门攻击

    Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models

    [https://arxiv.org/abs/2402.18945](https://arxiv.org/abs/2402.18945)

    论文提出了一种名为Syntactic Ghost的新方法，实现了对预训练语言模型进行无感知和通用的后门植入。

    

    预训练语言模型（PLMs）被发现容易受到后门攻击，可以将漏洞转移到各种下游任务中。然而，现有的PLM后门攻击采用明显的触发器，在手动对准的情况下进行，因此在效果、隐匿性和通用性方面无法同时满足期望目标。本文提出了一种新方法，实现了不可见和通用的后门植入，称为Syntactic Ghost（简称为synGhost）。具体来说，该方法敌意地使用具有不同预定义句法结构的毒害样本作为隐蔽触发器，然后将后门植入到预训练表示空间，而不会破坏原始知识。毒害样本的输出表示在特征空间中尽可能均匀地分布，通过对比学习形成广泛的后门。此外，在亮

    arXiv:2402.18945v1 Announce Type: cross  Abstract: Pre-trained language models (PLMs) have been found susceptible to backdoor attacks, which can transfer vulnerabilities to various downstream tasks. However, existing PLM backdoors are conducted with explicit triggers under the manually aligned, thus failing to satisfy expectation goals simultaneously in terms of effectiveness, stealthiness, and universality. In this paper, we propose a novel approach to achieve invisible and general backdoor implantation, called \textbf{Syntactic Ghost} (synGhost for short). Specifically, the method hostilely manipulates poisoned samples with different predefined syntactic structures as stealth triggers and then implants the backdoor to pre-trained representation space without disturbing the primitive knowledge. The output representations of poisoned samples are distributed as uniformly as possible in the feature space via contrastive learning, forming a wide range of backdoors. Additionally, in light 
    
[^42]: SemEval 2024 -- 任务10：情绪发现及对话中情绪转变的推理（EDiReF）

    SemEval 2024 -- Task 10: Emotion Discovery and Reasoning its Flip in Conversation (EDiReF)

    [https://arxiv.org/abs/2402.18944](https://arxiv.org/abs/2402.18944)

    SemEval-2024的任务10旨在识别对话中的情绪并找出背后的原因，参与者需自动执行情绪识别和情绪转变推理的子任务，取得了不错的结果。

    

    我们提出了SemEval-2024任务10，这是一个关于在单语种英语和印地语-英语混合对话中识别情绪并找出情绪转变背后原因的共享任务。该任务包括三个不同的子任务 - 用于混合对话中情绪识别、混合对话中情绪转变推理、以及英文对话中情绪转变推理。参与系统被要求自动执行一个或多个这些子任务。这些任务的数据集包括手动注释的对话，重点放在情绪和触发情绪转变的原因上（任务数据可在https://github.com/LCS2-IIITD/EDiReF-SemEval2024.git获取）。总共有84个参与者参与了这个任务，其中最擅长的系统在各个子任务上获得了0.70、0.79和0.76的F1分数。本文总结了来自24个团队的结果和发现以及他们系统的描述。

    arXiv:2402.18944v1 Announce Type: cross  Abstract: We present SemEval-2024 Task 10, a shared task centred on identifying emotions and finding the rationale behind their flips within monolingual English and Hindi-English code-mixed dialogues. This task comprises three distinct subtasks - emotion recognition in conversation for code-mixed dialogues, emotion flip reasoning for code-mixed dialogues, and emotion flip reasoning for English dialogues. Participating systems were tasked to automatically execute one or more of these subtasks. The datasets for these tasks comprise manually annotated conversations focusing on emotions and triggers for emotion shifts (The task data is available at https://github.com/LCS2-IIITD/EDiReF-SemEval2024.git). A total of 84 participants engaged in this task, with the most adept systems attaining F1-scores of 0.70, 0.79, and 0.76 for the respective subtasks. This paper summarises the results and findings from 24 teams alongside their system descriptions.
    
[^43]: 利用大规模语音识别技术检测发音障碍语音中的不当停顿

    Inappropriate Pause Detection In Dysarthric Speech Using Large-Scale Speech Recognition

    [https://arxiv.org/abs/2402.18923](https://arxiv.org/abs/2402.18923)

    该研究通过扩展大规模语音识别模型，提出了一种在发音障碍语音中检测不当停顿的方法，包括任务设计、标注策略和不当停顿预测层，为评估病情严重程度和语言治疗提供了新思路。

    

    发音障碍是中风患者常见问题，严重影响语音可懂性。不当停顿在病情评估和语言治疗中是关键指标。我们提出使用大规模语音识别模型来检测发音障碍语音中的不当停顿。为此，我们提出了任务设计、标注策略和一个带有不当停顿预测层的语音识别模型。首先，我们将停顿检测视为语音识别，使用自动语音识别（ASR）模型将语音转换为带有停顿标记的文本。根据设计的新任务，在文本级别标出停顿位置及其是否适当。我们与言语病理学家合作制定标注标准，确保高质量的带标注数据。最后，我们通过添加一个不当停顿预测层扩展了ASR模型，实现端到端的不当停顿检测。

    arXiv:2402.18923v1 Announce Type: new  Abstract: Dysarthria, a common issue among stroke patients, severely impacts speech intelligibility. Inappropriate pauses are crucial indicators in severity assessment and speech-language therapy. We propose to extend a large-scale speech recognition model for inappropriate pause detection in dysarthric speech. To this end, we propose task design, labeling strategy, and a speech recognition model with an inappropriate pause prediction layer. First, we treat pause detection as speech recognition, using an automatic speech recognition (ASR) model to convert speech into text with pause tags. According to the newly designed task, we label pause locations at the text level and their appropriateness. We collaborate with speech-language pathologists to establish labeling criteria, ensuring high-quality annotated data. Finally, we extend the ASR model with an inappropriate pause prediction layer for end-to-end inappropriate pause detection. Moreover, we p
    
[^44]: AdaMergeX: 跨语言大语言模型的自适应适配器融合

    AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging

    [https://arxiv.org/abs/2402.18913](https://arxiv.org/abs/2402.18913)

    提出一种新的跨语言转移方法 $\texttt{AdaMergeX}$，利用自适应适配器融合来解决任务能力和语言能力之间的关系。

    

    作为在特定语言的目标任务上进行直接微调的有效替代方案，跨语言转移通过在源语言上微调目标任务并在目标语言中选择另一个任务来解耦了有限训练数据的挑战，从而分离了“任务能力”和“语言能力”。然而，它们未能充分将任务能力与源语言或者语言能力与选择的任务完全分开。本文承认任务能力和语言能力之间的相互依赖，并将我们的注意力集中在目标语言和源语言之间的任务差距上。由于该差距消除了任务的影响，我们假定它在各任务间保持一致。基于这一假设，我们提出了一种名为 $\texttt{AdaMergeX}$ 的新的跨语言转移方法，利用自适应适配器融合。

    arXiv:2402.18913v1 Announce Type: cross  Abstract: As an effective alternative to the direct fine-tuning on target tasks in specific languages, cross-lingual transfer addresses the challenges of limited training data by decoupling ''task ability'' and ''language ability'' by fine-tuning on the target task in the source language and another selected task in the target language, respectively. However, they fail to fully separate the task ability from the source language or the language ability from the chosen task. In this paper, we acknowledge the mutual reliance between task ability and language ability and direct our attention toward the gap between the target language and the source language on tasks. As the gap removes the impact of tasks, we assume that it remains consistent across tasks. Based on this assumption, we propose a new cross-lingual transfer method called $\texttt{AdaMergeX}$ that utilizes adaptive adapter merging. By introducing a reference task, we can determine that 
    
[^45]: 使用非结构化事实更新语言模型：迈向实用知识编辑

    Updating Language Models with Unstructured Facts: Towards Practical Knowledge Editing

    [https://arxiv.org/abs/2402.18909](https://arxiv.org/abs/2402.18909)

    本文提出了一个新的基准，非结构化知识编辑（UKE），旨在使用非结构化文本作为知识更新，避免了繁琐的结构化事实构建，具有更高效和响应性的知识编辑能力。

    

    知识编辑旨在将知识更新注入语言模型中，使其保持正确性和最新性。然而，当前的评估策略明显不切实际：它们仅使用精心策划的结构化事实（主题、关系和对象的三元组）进行更新，而现实世界的知识更新通常出现在新闻文章等非结构化文本中。本文提出了一个新的基准，非结构化知识编辑（UKE）。它使用非结构化文本直接评估编辑性能，称为非结构化事实。因此，UKE避免了繁琐的结构化事实构建，实现了高效和响应迅速的知识编辑，成为一个更实用的基准。我们在新构建的数据集上进行了大量实验，并展示了UKE对最先进的知识编辑方法构成了重大挑战，导致它们的关键性能下降。

    arXiv:2402.18909v1 Announce Type: cross  Abstract: Knowledge editing aims to inject knowledge updates into language models to keep them correct and up-to-date. However, its current evaluation strategies are notably impractical: they solely update with well-curated structured facts (triplets with subjects, relations, and objects), whereas real-world knowledge updates commonly emerge in unstructured texts like news articles. In this paper, we propose a new benchmark, Unstructured Knowledge Editing (UKE). It evaluates editing performance directly using unstructured texts as knowledge updates, termed unstructured facts. Hence UKE avoids the laborious construction of structured facts and enables efficient and responsive knowledge editing, becoming a more practical benchmark. We conduct extensive experiments on newly built datasets and demonstrate that UKE poses a significant challenge to state-of-the-art knowledge editing methods, resulting in their critical performance declines. We further
    
[^46]: 主成分分析作为贝叶斯语言演化重建的合理性检查

    Principal Component Analysis as a Sanity Check for Bayesian Phylolinguistic Reconstruction

    [https://arxiv.org/abs/2402.18877](https://arxiv.org/abs/2402.18877)

    提出了一种简单的合理性检查方法：通过主成分分析将重建树投影到空间中，有效地可视化了异常情况，特别是在形式上的波动。

    

    抽象: 通过主成分分析将重建树投影到一个空间上，是一个简单的合理性检查方法。通过使用合成和真实数据，我们展示了我们的方法可以有效地可视化异常，尤其是在形式上的波动。

    arXiv:2402.18877v1 Announce Type: new  Abstract: Bayesian approaches to reconstructing the evolutionary history of languages rely on the tree model, which assumes that these languages descended from a common ancestor and underwent modifications over time. However, this assumption can be violated to different extents due to contact and other factors. Understanding the degree to which this assumption is violated is crucial for validating the accuracy of phylolinguistic inference. In this paper, we propose a simple sanity check: projecting a reconstructed tree onto a space generated by principal component analysis. By using both synthetic and real data, we demonstrate that our method effectively visualizes anomalies, particularly in the form of jogging.
    
[^47]: 通过事实-模板分解减少实体摘要中的幻觉

    Reducing Hallucinations in Entity Abstract Summarization with Facts-Template Decomposition

    [https://arxiv.org/abs/2402.18873](https://arxiv.org/abs/2402.18873)

    通过事实-模板分解，提出了一个可解释的框架SlotSum，用于减少预训练语言模型在实体摘要中生成幻觉的问题。

    

    实体摘要总结旨在基于一组相关的互联网文档生成一个给定实体的连贯描述。预训练语言模型（PLMs）在这一任务中取得了显著的成功，但可能会出现幻觉，即生成关于实体的非事实信息。为了解决这个问题，我们将摘要分解成两部分：表示给定实体的事实信息的事实，PLMs容易捏造；以及包含通用内容且为事实指定槽的模板，PLMs可以有效地生成。基于事实-模板分解，我们提出了SlotSum，一个可解释的实体摘要总结框架。SlotSum首先创建模板，然后根据输入文档预测每个模板槽的事实。受益于我们的事实-模板分解，SlotSum可以轻松定位错误，并进一步纠正幻觉。

    arXiv:2402.18873v1 Announce Type: new  Abstract: Entity abstract summarization aims to generate a coherent description of a given entity based on a set of relevant Internet documents. Pretrained language models (PLMs) have achieved significant success in this task, but they may suffer from hallucinations, i.e. generating non-factual information about the entity. To address this issue, we decompose the summary into two components: Facts that represent the factual information about the given entity, which PLMs are prone to fabricate; and Template that comprises generic content with designated slots for facts, which PLMs can generate competently. Based on the facts-template decomposition, we propose SlotSum, an explainable framework for entity abstract summarization. SlotSum first creates the template and then predicts the fact for each template slot based on the input documents. Benefiting from our facts-template decomposition, SlotSum can easily locate errors and further rectify halluci
    
[^48]: 分析和减少参数高效调整中的灾难性遗忘

    Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning

    [https://arxiv.org/abs/2402.18865](https://arxiv.org/abs/2402.18865)

    通过模式连接调查了连续微调中不同极小值之间的几何连接，揭示了大型语言模型中的灾难性遗忘问题。

    

    已有研究显示，大型语言模型（LLMs）在语言理解和生成方面表现出色。然而，当LLMs不断在复杂和多样化的特定领域下游任务上进行微调时，对历史任务的推理性能会急剧下降，这被称为灾难性遗忘问题。需要在学习可塑性和记忆稳定性之间保持权衡。已有很多研究探讨了诸如记忆重放、正则化和参数隔离等策略，但在连续的LLMs微调场景中，对各个相邻极小值之间的几何连接知之甚少。在这项工作中，我们通过模式连接的视角调查了不同极小值之间的几何连接，这意味着不同极小值可以通过一个低损失的山谷相连接。通过大量实验，我们揭示了LLMs微调中的模式连接现象。

    arXiv:2402.18865v1 Announce Type: cross  Abstract: Existing research has shown that large language models (LLMs) exhibit remarkable performance in language understanding and generation. However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem. A trade-off needs to be kept between learning plasticity and memory stability. Plenty of existing works have explored strategies like memory replay, regularization and parameter isolation, but little is known about the geometric connection of various adjacent minima in the continual LLMs fine-tuning scenarios. In this work, we investigate the geometric connections of different minima through the lens of mode connectivity, which means different minima can be connected by a low-loss valley. Through extensive experiments, we uncover the mode connectivity phenomenon in the LLMs contin
    
[^49]: 提升隐写文本提取：评估自然语言处理模型对准确性和语义连贯性的影响

    Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP Models on Accuracy and Semantic Coherence

    [https://arxiv.org/abs/2402.18849](https://arxiv.org/abs/2402.18849)

    通过整合NLP大型模型，本研究提出一种LSB-NLP混合框架，显著提高了隐写文本提取的准确性和鲁棒性，尤其在处理中文字符时表现优异。

    

    这项研究讨论了一种新方法，将图像隐写术技术与自然语言处理（NLP）大型模型相结合，旨在提高提取隐写文本的准确性和鲁棒性。传统的最低有效位（LSB）隐写术技术在处理复杂字符编码（如中文字符）时在信息提取的准确性和鲁棒性方面面临挑战。为了解决这一问题，本研究提出了一种创新的LSB-NLP混合框架。该框架集成了NLP大型模型的先进能力，如错误检测、纠正和语义一致性分析，以及信息重建技术，从而显著提高了隐写文本提取的鲁棒性。实验结果显示，LSB-NLP混合框架在提高隐写文本提取准确性方面表现出色，特别是在处理中文字符方面。

    arXiv:2402.18849v1 Announce Type: cross  Abstract: This study discusses a new method combining image steganography technology with Natural Language Processing (NLP) large models, aimed at improving the accuracy and robustness of extracting steganographic text. Traditional Least Significant Bit (LSB) steganography techniques face challenges in accuracy and robustness of information extraction when dealing with complex character encoding, such as Chinese characters. To address this issue, this study proposes an innovative LSB-NLP hybrid framework. This framework integrates the advanced capabilities of NLP large models, such as error detection, correction, and semantic consistency analysis, as well as information reconstruction techniques, thereby significantly enhancing the robustness of steganographic text extraction. Experimental results show that the LSB-NLP hybrid framework excels in improving the extraction accuracy of steganographic text, especially in handling Chinese characters. 
    
[^50]: 什么时候词序重要，什么时候不重要？

    When does word order matter and when doesn't it?

    [https://arxiv.org/abs/2402.18838](https://arxiv.org/abs/2402.18838)

    本文研究了语言模型对词序的敏感度问题，通过量化词序信息量，发现在语言提示提供冗余信息时，模型对词序变化不敏感，且不同任务间的不敏感程度有所差异。

    

    在自然语言理解（NLU）任务中，语言模型（LMs）可能对词序变化不敏感。本文提出语言冗余性可以解释这一现象，即词序和其他语言提示（如格标）提供重叠且冗余信息。我们的假设是，当顺序提供冗余信息时，模型对词序的不敏感表现，而不同任务之间的不敏感程度各不相同。我们使用无序和打乱顺序的句子之间的互信息（MI）来量化词序的信息量。我们的结果显示，词序信息越不具信息量，模型在无序和打乱顺序的句子之间的预测越一致。我们还发现这种影响在不同任务之间存在差异：对于一些任务，如SST-2，LM的预测几乎总是与原始结果一致，即使点间互信息（PMI）发生变化，也是如此。

    arXiv:2402.18838v1 Announce Type: new  Abstract: Language models (LMs) may appear insensitive to word order changes in natural language understanding (NLU) tasks. In this paper, we propose that linguistic redundancy can explain this phenomenon, whereby word order and other linguistic cues such as case markers provide overlapping and thus redundant information. Our hypothesis is that models exhibit insensitivity to word order when the order provides redundant information, and the degree of insensitivity varies across tasks. We quantify how informative word order is using mutual information (MI) between unscrambled and scrambled sentences. Our results show the effect that the less informative word order is, the more consistent the model's predictions are between unscrambled and scrambled sentences. We also find that the effect varies across tasks: for some tasks, like SST-2, LMs' prediction is almost always consistent with the original one even if the Pointwise-MI (PMI) changes, while fo
    
[^51]: 利用对抗训练的本地层次结构进行分层文本分类

    Utilizing Local Hierarchy with Adversarial Training for Hierarchical Text Classification

    [https://arxiv.org/abs/2402.18825](https://arxiv.org/abs/2402.18825)

    通过引入对抗性框架和本地层次结构，我们提出了一个适用于几乎所有HTC模型的HiAdv框架，优化了分层文本分类，并证明了本地层次结构的有效性，特别对于训练数据不足的罕见类别。

    

    分层文本分类（HTC）是多标签分类的一个具有挑战性的子任务，因为其复杂的分类结构。几乎所有最近的HTC作品都关注标签如何结构化，但忽略了根据每个输入文本的地面真实标签的子结构，其中包含丰富的标签共现信息。在这项工作中，我们引入了这种本地层次结构和一个对抗性框架。我们提出了一个名为HiAdv的框架，可以适应几乎所有HTC模型，并将本地层次结构作为辅助信息进行优化。我们在两个典型的HTC模型上进行测试，并发现HiAdv在所有情况下都是有效的，能够处理复杂的分类层次结构。进一步的实验证明，我们框架的提升确实来自本地层次结构，本地层次结构有利于那些训练数据不足的罕见类别。

    arXiv:2402.18825v1 Announce Type: new  Abstract: Hierarchical text classification (HTC) is a challenging subtask of multi-label classification due to its complex taxonomic structure. Nearly all recent HTC works focus on how the labels are structured but ignore the sub-structure of ground-truth labels according to each input text which contains fruitful label co-occurrence information. In this work, we introduce this local hierarchy with an adversarial framework. We propose a HiAdv framework that can fit in nearly all HTC models and optimize them with the local hierarchy as auxiliary information. We test on two typical HTC models and find that HiAdv is effective in all scenarios and is adept at dealing with complex taxonomic hierarchies. Further experiments demonstrate that the promotion of our framework indeed comes from the local hierarchy and the local hierarchy is beneficial for rare classes which have insufficient training data.
    
[^52]: 大型语言模型如何处理多语言？

    How do Large Language Models Handle Multilingualism?

    [https://arxiv.org/abs/2402.18815](https://arxiv.org/abs/2402.18815)

    大型语言模型展示了处理多语言任务的出色性能，研究发现在不同层次中处理多语言输入的策略，以及处理特定语言时的语言特定神经元存在。

    

    大型语言模型（LLMs）展现出在各种语言上出色的性能。本文探讨了一个问题：大型语言模型如何处理多语言？我们引入了一个框架，描述了LLMs处理多语言输入的过程：在前几层中，LLMs理解问题，将多语言输入转换为英语以便促进任务解决阶段。在中间层中，LLMs通过以英语思考并整合多语言知识来进行解决问题，利用自注意力和前馈结构，分别获取事实内容。在最后几层中，LLMs生成与查询的原始语言一致的响应。此外，我们研究了处理特定语言时特定语言神经元的存在。为了检测由输入语言激活的神经元，即使没有标签，我们创新性地设计了一个并行语言特定的

    arXiv:2402.18815v1 Announce Type: cross  Abstract: Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages. In this work, we delve into the question: How do LLMs handle multilingualism? We introduce a framework that depicts LLMs' processing of multilingual inputs: In the first several layers, LLMs understand the question, converting multilingual inputs into English to facilitate the task-solving phase. In the intermediate layers, LLMs engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the self-attention and feed-forward structures, respectively. In the last several layers, LLMs generate responses that align with the original language of the query. In addition, we investigate the existence of language-specific neurons when processing a certain language. To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specif
    
[^53]: 论使用大型语言模型进行角色扮演中的决策能力

    On the Decision-Making Abilities in Role-Playing using Large Language Models

    [https://arxiv.org/abs/2402.18807](https://arxiv.org/abs/2402.18807)

    本文评估了大型语言模型在角色扮演中的决策能力，并提供了指标和指导以增强其在此任务中的表现。

    

    大型语言模型（LLMs）现在越来越多地用于角色扮演任务，特别是在模仿特定领域专家时，主要通过角色扮演提示。在实际场景中互动时，角色的决策能力显著地塑造其行为模式。本文集中评估LLMs在角色扮演后的决策能力，从而验证角色扮演的有效性。我们的目标是为增强LLMs在角色扮演任务中的决策能力提供指标和指导。具体来说，我们首先使用LLMs生成对应于迈尔斯-布里格斯类型指标（MBTI）的16种人格类型的虚拟角色描述，代表人口的细分。然后，我们设计了具体的定量操作，从适应性、探索性等四个方面评估LLMs在角色扮演后的决策能力。

    arXiv:2402.18807v1 Announce Type: cross  Abstract: Large language models (LLMs) are now increasingly utilized for role-playing tasks, especially in impersonating domain-specific experts, primarily through role-playing prompts. When interacting in real-world scenarios, the decision-making abilities of a role significantly shape its behavioral patterns. In this paper, we concentrate on evaluating the decision-making abilities of LLMs post role-playing thereby validating the efficacy of role-playing. Our goal is to provide metrics and guidance for enhancing the decision-making abilities of LLMs in role-playing tasks. Specifically, we first use LLMs to generate virtual role descriptions corresponding to the 16 personality types of Myers-Briggs Type Indicator (abbreviated as MBTI) representing a segmentation of the population. Then we design specific quantitative operations to evaluate the decision-making abilities of LLMs post role-playing from four aspects: adaptability, exploration$\&$ex
    
[^54]: ARTiST：用于增强现实任务指导的自动文本简化

    ARTiST: Automated Text Simplification for Task Guidance in Augmented Reality

    [https://arxiv.org/abs/2402.18797](https://arxiv.org/abs/2402.18797)

    提出了ARTiST，这是一个自动文本简化系统，结合少量示例提示和GPT-3模型，通过整合简化技术生成了适用于头戴式显示器的简化AR文本。实验结果表明，ARTiST显著降低了认知负担，并提高了性能。

    

    在增强现实中呈现的文本为用户提供了就地、实时信息。然而，在从事认知需求高的AR任务时，尤其是在头戴式显示器上呈现时，这些内容很难快速理解。我们提出了ARTiST，这是一个自动文本简化系统，它使用少量示例提示和GPT-3模型，专门为增强现实优化文本长度和语义内容。我们的系统结合了自定义的误差校准模型和少量示例提示，以整合句法、词汇、阐述和内容简化技术，并为头戴式显示器生成简化的AR文本。一项包括七名用户和三名专家的形成性研究开发了这一系统。一项包括16名用户的实证研究结果表明，ARTiST减轻了认知负荷，并显着提高了性能，相比较于未修改的文本和传统方法修改的文本。

    arXiv:2402.18797v1 Announce Type: cross  Abstract: Text presented in augmented reality provides in-situ, real-time information for users. However, this content can be challenging to apprehend quickly when engaging in cognitively demanding AR tasks, especially when it is presented on a head-mounted display. We propose ARTiST, an automatic text simplification system that uses a few-shot prompt and GPT-3 models to specifically optimize the text length and semantic content for augmented reality. Developed out of a formative study that included seven users and three experts, our system combines a customized error calibration model with a few-shot prompt to integrate the syntactic, lexical, elaborative, and content simplification techniques, and generate simplified AR text for head-worn displays. Results from a 16-user empirical study showed that ARTiST lightens the cognitive load and improves performance significantly over both unmodified text and text modified via traditional methods. Our 
    
[^55]: MPAT: 抗击文本对抗攻击的鲁棒深度神经网络构建

    MPAT: Building Robust Deep Neural Networks against Textual Adversarial Attacks

    [https://arxiv.org/abs/2402.18792](https://arxiv.org/abs/2402.18792)

    提出了一种基于恶意扰动的对抗训练方法（MPAT）来构建鲁棒的深度神经网络，用于抵御文本对抗攻击。

    

    深度神经网络已被证明对于对抗性示例是脆弱的，并且已经提出了各种方法来防御自然语言处理任务的对抗攻击。然而，先前的防御方法在保持有效的防御同时确保原始任务性能方面存在局限性。本文提出了一种基于恶意扰动的对抗训练方法（MPAT），用于构建抵御文本对抗攻击的鲁棒深度神经网络。具体而言，我们构建了一个多级恶意示例生成策略，以生成带有恶意扰动的对抗性示例，这些示例被用来代替模型训练中的原始输入。此外，我们采用了一个新颖的训练目标函数，以确保达到防御目标而不损害原始任务上的性能。我们进行了全面的实验来评估我们的防御方法，通过攻击五个v

    arXiv:2402.18792v1 Announce Type: cross  Abstract: Deep neural networks have been proven to be vulnerable to adversarial examples and various methods have been proposed to defend against adversarial attacks for natural language processing tasks. However, previous defense methods have limitations in maintaining effective defense while ensuring the performance of the original task. In this paper, we propose a malicious perturbation based adversarial training method (MPAT) for building robust deep neural networks against textual adversarial attacks. Specifically, we construct a multi-level malicious example generation strategy to generate adversarial examples with malicious perturbations, which are used instead of original inputs for model training. Additionally, we employ a novel training objective function to ensure achieving the defense goal without compromising the performance on the original task. We conduct comprehensive experiments to evaluate our defense method by attacking five v
    
[^56]: FlexLLM：一种用于共同提供大型语言模型推理和参数高效微调的系统

    FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning

    [https://arxiv.org/abs/2402.18789](https://arxiv.org/abs/2402.18789)

    FlexLLM是第一个可以在同一迭代中共同提供推理和参数高效微调请求的系统，通过引入标记级微调机制实现共享GPU资源的高效利用

    

    Parameter-efficient finetuning（PEFT）是一种广泛使用的技术，用于为不同任务调整大型语言模型。通常，服务提供商会为用户创建单独的系统，以执行PEFT模型微调和推理任务。这是因为现有系统无法处理包含推理和PEFT微调请求混合的工作负载。因此，共享的GPU资源利用不足，导致效率低下。为解决这一问题，我们提出了FlexLLM，这是第一个可以在同一迭代中为推理和参数高效微调请求提供服务的系统。我们的系统利用这两个任务的互补性质，并利用共享的GPU资源来共同运行它们，使用一种称为共同提供的方法。为实现这一目标，FlexLLM引入了一种新颖的标记级微调机制，将序列的微调计算分解为更小的标记级计算，并使用依赖并行化。

    arXiv:2402.18789v1 Announce Type: cross  Abstract: Parameter-efficient finetuning (PEFT) is a widely used technique to adapt large language models for different tasks. Service providers typically create separate systems for users to perform PEFT model finetuning and inference tasks. This is because existing systems cannot handle workloads that include a mix of inference and PEFT finetuning requests. As a result, shared GPU resources are underutilized, leading to inefficiencies. To address this problem, we present FlexLLM, the first system that can serve inference and parameter-efficient finetuning requests in the same iteration. Our system leverages the complementary nature of these two tasks and utilizes shared GPU resources to run them jointly, using a method called co-serving. To achieve this, FlexLLM introduces a novel token-level finetuning mechanism, which breaks down the finetuning computation of a sequence into smaller token-level computations and uses dependent parallelization
    
[^57]: 推动葡萄牙语生成人工智能与开放解码器Gerv\'asio PT*

    Advancing Generative AI for Portuguese with Open Decoder Gerv\'asio PT*

    [https://arxiv.org/abs/2402.18766](https://arxiv.org/abs/2402.18766)

    提出了一种葡萄牙语生成人工智能的开放解码器模型Gerv\'asio PT*，创造了新的技术水平，促进葡萄牙语言技术研究和创新。

    

    为了推进葡萄牙语的神经解码，本文提出了一种全新的基于Transformer的、经过指令调整的开放解码器模型，从这方面创造了新的技术水平。为了开发这个解码器，我们使用了一个强大的LLaMA~27B模型作为起点，并通过进一步训练对包括为此目的准备的新葡萄牙语指令数据集在内的语言资源进行改进，这些数据集也在本文中提供。所有版本的Gerv\'asio都是开源的，可以免费使用，并可以在消费级硬件上运行，旨在促进葡萄牙语言技术研究和创新的发展。

    arXiv:2402.18766v1 Announce Type: new  Abstract: To advance the neural decoding of Portuguese, in this paper we present a fully open Transformer-based, instruction-tuned decoder model that sets a new state of the art in this respect. To develop this decoder, which we named Gerv\'asio PT*, a strong LLaMA~2 7B model was used as a starting point, and its further improvement through additional training was done over language resources that include new instruction data sets of Portuguese prepared for this purpose, which are also contributed in this paper. All versions of Gerv\'asio are open source and distributed for free under an open license, including for either research or commercial usage, and can be run on consumer-grade hardware, thus seeking to contribute to the advancement of research and innovation in language technology for Portuguese.
    
[^58]: 需要多少注释才能比较摘要模型？

    How Much Annotation is Needed to Compare Summarization Models?

    [https://arxiv.org/abs/2402.18756](https://arxiv.org/abs/2402.18756)

    本研究通过实证研究发现，仅需要不到100个例子就能够得出对摘要系统的明确偏好，并且只有一些自动评估指标能够适度预测模型的胜率。

    

    近代的指导调整模型在文本生成任务（如摘要）中变得非常有能力，并且预计会以稳定的速度发布。实际上，人们现在可能希望在应用于新领域或目的时，自信地选择但又付出最少努力的最佳摘要模型。在这项工作中，我们通过实证研究调查了在新闻摘要环境中选择首选模型所需的测试样本大小。实证结果表明，无论是自动评估还是人工评估，比较评估都会迅速收敛，从不到100个示例中会出现对系统的明确偏好。人类偏好数据使我们能够量化自动分数能够如何复制在各种下游摘要任务中的偏好排名。我们发现，虽然自动指标在较小样本大小时稳定，但只有一些自动指标能够适度预测模型的获胜率。

    arXiv:2402.18756v1 Announce Type: new  Abstract: Modern instruction-tuned models have become highly capable in text generation tasks such as summarization, and are expected to be released at a steady pace. In practice one may now wish to choose confidently, but with minimal effort, the best performing summarization model when applied to a new domain or purpose. In this work, we empirically investigate the test sample size necessary to select a preferred model in the context of news summarization. Empirical results reveal that comparative evaluation converges quickly for both automatic and human evaluation, with clear preferences for a system emerging from under 100 examples. The human preference data allows us to quantify how well automatic scores can reproduce preference rankings across a variety of downstream summarization tasks. We find that, while automatic metrics are stable at smaller sample sizes, only some automatic metrics are able to moderately predict model win rates accordi
    
[^59]: 细调的机器翻译度量在未知领域中存在困难

    Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains

    [https://arxiv.org/abs/2402.18747](https://arxiv.org/abs/2402.18747)

    细调的机器翻译度量在未知领域中表现出明显的性能下降，相对于依赖表面形式的度量和未经MT质量判断细调的预训练度量。

    

    我们引入了一个新的、涵盖生物医学领域中11种语言对的广泛的多维质量度量(MQM)注释数据集。我们利用这个数据集来探究在训练和推断之间的领域转移时，是否那些根据人工生成的机器翻译质量判断进行细调的MT度量是稳健的。我们发现，在未知领域的情况下，细调的度量相对于依赖表面形式的度量以及未经MT质量判断细调的预训练度量表现出显著的性能下降。

    arXiv:2402.18747v1 Announce Type: cross  Abstract: We introduce a new, extensive multidimensional quality metrics (MQM) annotated dataset covering 11 language pairs in the biomedical domain. We use this dataset to investigate whether machine translation (MT) metrics which are fine-tuned on human-generated MT quality judgements are robust to domain shifts between training and inference. We find that fine-tuned metrics exhibit a substantial performance drop in the unseen domain scenario relative to metrics that rely on the surface form, as well as pre-trained metrics which are not fine-tuned on MT quality judgments.
    
[^60]: 编译器大型语言模型的优先采样

    Priority Sampling of Large Language Models for Compilers

    [https://arxiv.org/abs/2402.18734](https://arxiv.org/abs/2402.18734)

    提出了一种优先采样技术，能够按照模型信心度产生唯一样本，在生成和优化代码时表现优于核采样方法。

    

    大型语言模型在生成和优化代码方面表现出巨大潜力。广泛使用的采样方法，比如核采样（Nucleus Sampling），增加了生成的多样性，但在低温度下经常产生重复的样本，在高温度下产生不连贯的样本。此外，温度系数必须针对每个任务进行调整，限制了其可用性。我们提出了优先采样（Priority Sampling），一种简单且确定性的采样技术，它产生按模型置信度排序的唯一样本。每个新样本都会扩展扩展搜索树中概率最高的未扩展令牌。此外，优先采样支持基于正则表达式的生成，提供可控和结构化的探索过程。优先采样在任意数量的样本情况下均优于核采样，将原始模型的性能从2.87%提升至5%超过-Oz。此外，它超过了自

    arXiv:2402.18734v1 Announce Type: cross  Abstract: Large language models show great potential in generating and optimizing code. Widely used sampling methods such as Nucleus Sampling increase the diversity of generation but often produce repeated samples for low temperatures and incoherent samples for high temperatures. Furthermore, the temperature coefficient has to be tuned for each task, limiting its usability. We present Priority Sampling, a simple and deterministic sampling technique that produces unique samples ordered by the model's confidence. Each new sample expands the unexpanded token with the highest probability in the augmented search tree. Additionally, Priority Sampling supports generation based on regular expression that provides a controllable and structured exploration process. Priority Sampling outperforms Nucleus Sampling for any number of samples, boosting the performance of the original model from 2.87% to 5% improvement over -Oz. Moreover, it outperforms the auto
    
[^61]: 学习在自然语言格式中压缩提示

    Learning to Compress Prompt in Natural Language Formats

    [https://arxiv.org/abs/2402.18700](https://arxiv.org/abs/2402.18700)

    该研究旨在通过提出自然语言提示封装（Nano-Capsulator）框架，解决了在自然语言格式中压缩提示的挑战，以提高大型语言模型的可转移性和性能。

    

    大型语言模型（LLMs）擅长处理多个自然语言处理任务，但它们的能力受到长上下文、推理速度慢以及计算结果成本高的限制。部署具有精确和信息丰富上下文的LLMs有助于用户更有效和更具成本效益地处理大规模数据集。现有作品依赖将长提示上下文压缩为软提示。然而，软提示压缩在不同LLM之间的可转移性受到限制，尤其是基于API的LLMs。因此，本研究旨在以LLM可转移性的形式压缩长提示的自然语言形式。这带来两个挑战：(i) 自然语言（NL）提示不兼容反向传播，(ii) NL提示在施加长度约束方面缺乏灵活性。在本研究中，我们提出了一种自然语言提示封装（Nano-Capsulator）框架

    arXiv:2402.18700v1 Announce Type: cross  Abstract: Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results. Deploying LLMs with precise and informative context helps users process large-scale datasets more effectively and cost-efficiently. Existing works rely on compressing long prompt contexts into soft prompts. However, soft prompt compression encounters limitations in transferability across different LLMs, especially API-based LLMs. To this end, this work aims to compress lengthy prompts in the form of natural language with LLM transferability. This poses two challenges: (i) Natural Language (NL) prompts are incompatible with back-propagation, and (ii) NL prompts lack flexibility in imposing length constraints. In this work, we propose a Natural Language Prompt Encapsulation (Nano-Capsulator) framewor
    
[^62]: 将语言模型应用在视觉实体识别上

    Grounding Language Models for Visual Entity Recognition

    [https://arxiv.org/abs/2402.18695](https://arxiv.org/abs/2402.18695)

    通过AutoVER模型，我们提出了一种在视觉实体识别中应用自回归模型的方法，通过检索增强的约束生成，成功区分巨大标签空间中相似的实体，并在Oven-Wiki基准测试上取得显著进展。

    

    我们引入AutoVER，一种用于视觉实体识别的自回归模型。我们的模型通过使用检索增强的约束生成，扩展了自回归多模式大型语言模型。它在处理跨领域实体时减轻了低性能，在需要视觉推理的查询中表现出色。我们的方法通过在硬负对上进行对比训练，并在序列-序列目标中并行进行训练，学习在巨大的标签空间中区分相似的实体。在推断过程中，一系列检索的候选答案明确指导语言生成，通过消除无效的解码路径。所提出的方法在最近提出的Oven-Wiki基准测试的不同数据集拆分中实现了显著的改进。在已知实体拆分上的准确率从32.7%提高到61.5%。该方法还通过大幅度提升在未知和查询拆分上的性能，表现出卓越的表现。

    arXiv:2402.18695v1 Announce Type: cross  Abstract: We introduce AutoVER, an Autoregressive model for Visual Entity Recognition. Our model extends an autoregressive Multi-modal Large Language Model by employing retrieval augmented constrained generation. It mitigates low performance on out-of-domain entities while excelling in queries that require visually-situated reasoning. Our method learns to distinguish similar entities within a vast label space by contrastively training on hard negative pairs in parallel with a sequence-to-sequence objective without an external retriever. During inference, a list of retrieved candidate answers explicitly guides language generation by removing invalid decoding paths. The proposed method achieves significant improvements across different dataset splits in the recently proposed Oven-Wiki benchmark. Accuracy on the Entity seen split rises from 32.7% to 61.5%. It also demonstrates superior performance on the unseen and query splits by a substantial dou
    
[^63]: RORA：强大的自由文本理由评估

    RORA: Robust Free-Text Rationale Evaluation

    [https://arxiv.org/abs/2402.18678](https://arxiv.org/abs/2402.18678)

    RORA 提出了一种新的评估方法，用于衡量自由文本理由对标签的新信息贡献，并在评估中优于现有方法。

    

    自由文本理由在可解释的自然语言处理中发挥着重要作用，弥合了模型决策背后的知识和推理差距。然而，由于潜在推理路径的多样性及相应缺乏明确的真相基础，其评估仍然是一个挑战。现有的评估指标依赖于理由支持目标标签的程度，但我们发现这些指标在评估意外泄漏标签的理由时存在不足。为解决这个问题，我们提出了RORA，一种针对标签泄漏的强大的自由文本理由评估方法。RORA量化了理由为证明标签提供的新信息。这是通过评估与泄漏特征抗干扰的预测性家族条件V信息实现的。RORA在评估人工撰写、合成或模型生成的理由方面始终优于现有方法。

    arXiv:2402.18678v1 Announce Type: new  Abstract: Free-text rationales play a pivotal role in explainable NLP, bridging the knowledge and reasoning gaps behind a model's decision-making. However, due to the diversity of potential reasoning paths and a corresponding lack of definitive ground truth, their evaluation remains a challenge. Existing evaluation metrics rely on the degree to which a rationale supports a target label, but we find these fall short in evaluating rationales that inadvertently leak the labels. To address this problem, we propose RORA, a Robust free-text Rationale evaluation against label leakage. RORA quantifies the new information supplied by a rationale to justify the label. This is achieved by assessing the conditional V-information \citep{hewitt-etal-2021-conditional} with a predictive family robust against leaky features that can be exploited by a small model. RORA consistently outperforms existing approaches in evaluating human-written, synthetic, or model-gen
    
[^64]: 简单的线性注意力语言模型平衡了召回-吞吐量的折衷

    Simple linear attention language models balance the recall-throughput tradeoff

    [https://arxiv.org/abs/2402.18668](https://arxiv.org/abs/2402.18668)

    提出了一种简单的线性注意力语言模型架构，可以平衡召回和内存消耗之间的权衡。

    

    最近的研究表明，基于注意力的语言模型擅长召回，即在上下文中已经看到的标记。然而，在推断过程中，基于注意力的模型的效率受到KV-cache的内存消耗的瓶颈限制。在这项工作中，我们探讨了是否可以提高语言模型的效率（例如通过减少内存消耗）而不影响召回。通过将实验和理论应用于广泛的架构，我们确定了模型状态大小和召回能力之间的一个关键权衡。我们发现，与注意力的高效替代方法（例如H3、Mamba、RWKV）保持固定大小的循环状态，但在召回方面表现不佳。我们提出了BASED，这是一种结合了线性和滑动窗口注意力的简单架构。通过改变BASED窗口大小和线性注意力特征维度，我们可以调整状态大小，并遍历召回-内存折衷的帕累托前沿。

    arXiv:2402.18668v1 Announce Type: new  Abstract: Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff cu
    
[^65]: FOFO：用于评估LLMs格式追随能力的基准测试

    FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability

    [https://arxiv.org/abs/2402.18667](https://arxiv.org/abs/2402.18667)

    FOFO是一个基准测试，用于评估大型语言模型追随复杂、领域特定格式的能力，揭示了LLMs在格式追随能力方面的表现和不同领域之间的差异

    

    本文介绍了FoFo，这是一个开创性的基准测试，用于评估大型语言模型（LLMs）追随复杂领域特定格式的能力，这是它们作为AI代理的应用中至关重要但未经充分考虑的能力。尽管LLMs有了进展，现有的基准测试未能充分评估它们的格式追随能力。FoFo通过AI-人类协作方法开发了多样化的真实世界格式和指令，填补了这一空白。我们的评估跨越开源模型（例如Llama 2，WizardLM）和闭源模型（例如GPT-4，PALM2，Gemini），突出了三个关键发现：开源模型在格式遵循方面明显落后于闭源模型；LLMs的格式追随表现独立于它们的内容生成质量；LLMs的格式熟练度在不同领域之间变化。这些见解表明需要专门调整格式追随技能，并突出了FoFo的重要性

    arXiv:2402.18667v1 Announce Type: new  Abstract: This paper presents FoFo, a pioneering benchmark for evaluating large language models' (LLMs) ability to follow complex, domain-specific formats, a crucial yet underexamined capability for their application as AI agents. Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately. FoFo fills this gap with a diverse range of real-world formats and instructions, developed through an AI-Human collaborative method. Our evaluation across both open-source (e.g., Llama 2, WizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three key findings: open-source models significantly lag behind closed-source ones in format adherence; LLMs' format-following performance is independent of their content generation quality; and LLMs' format proficiency varies across different domains. These insights suggest the need for specialized tuning for format-following skills and highlight FoFo's ro
    
[^66]: 大型语言模型与游戏：调研与路线图

    Large Language Models and Games: A Survey and Roadmap

    [https://arxiv.org/abs/2402.18659](https://arxiv.org/abs/2402.18659)

    这项研究调查了大型语言模型在游戏领域中的多种应用及其角色，指出了未开发领域和未来发展方向，同时探讨了在游戏领域中大型语言模型的潜力和限制。

    

    近年来，大型语言模型（LLMs）的研究急剧增加，并伴随着公众对该主题的参与。尽管起初是自然语言处理中的一小部分，LLMs在广泛的应用和领域中展现出显著潜力，包括游戏。本文调查了LLMs在游戏中及为游戏提供支持的各种应用的最新技术水平，并明确了LLMs在游戏中可以扮演的不同角色。重要的是，我们讨论了尚未开发的领域和LLMs在游戏中未来应用的有前途的方向，以及在游戏领域中LLMs的潜力和限制。作为LLMs和游戏交叉领域的第一份综合调查和路线图，我们希望本文能够成为这一激动人心的新领域的开创性研究和创新的基础。

    arXiv:2402.18659v1 Announce Type: cross  Abstract: Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.
    
[^67]: MMSR：符号回归是一个多模态任务

    MMSR: Symbolic Regression is a Multimodal Task

    [https://arxiv.org/abs/2402.18603](https://arxiv.org/abs/2402.18603)

    符号回归被视为一个多模态任务，研究人员将数据到表达式的映射视为翻译问题，引入大规模预训练模型。

    

    数学公式是探索自然规律几千年来人类智慧的结晶。用简洁的数学公式描述复杂的自然规律是科学家不断追求的目标，也是人工智能面临的重大挑战。这一领域被称为符号回归。在本文中，研究人员将从数据到表达式的映射视为翻译问题，并引入了相应的大规模预训练模型。

    arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
    
[^68]: Verif.ai: 一种具有引用和可验证答案的开源科学生成式问答系统

    Verif.ai: Towards an Open-Source Scientific Generative Question-Answering System with Referenced and Verifiable Answers

    [https://arxiv.org/abs/2402.18589](https://arxiv.org/abs/2402.18589)

    Verif.ai是一个具有引用和可验证答案的开源科学生成式问答系统，通过信息检索、生成模型和验证引擎的结合实现对主张的生成和验证。

    

    在本文中，我们介绍了项目Verif.ai的当前进展，这是一个具有引用和可验证答案的开源科学生成式问答系统。该系统的组成部分包括（1）一个信息检索系统，结合语义和词汇搜索技术对科学论文（PubMed）进行检索，（2）一个经过微调的生成模型（Mistral 7B），获取前几个答案并生成附有从中得出主张的论文引用的答案，以及（3）一个验证引擎，用于交叉检查生成的主张和从中得出主张的摘要或论文，验证生成主张时是否存在任何错觉。我们通过提供上下文中的摘要加强了生成模型，但此外，一个独立的方法和模型集正在验证答案并检查是否存在错觉。因此，我们相信通过使用我们的方法，我们可以使科学家们

    arXiv:2402.18589v1 Announce Type: cross  Abstract: In this paper, we present the current progress of the project Verif.ai, an open-source scientific generative question-answering system with referenced and verified answers. The components of the system are (1) an information retrieval system combining semantic and lexical search techniques over scientific papers (PubMed), (2) a fine-tuned generative model (Mistral 7B) taking top answers and generating answers with references to the papers from which the claim was derived, and (3) a verification engine that cross-checks the generated claim and the abstract or paper from which the claim was derived, verifying whether there may have been any hallucinations in generating the claim. We are reinforcing the generative model by providing the abstract in context, but in addition, an independent set of methods and models are verifying the answer and checking for hallucinations. Therefore, we believe that by using our method, we can make scientis
    
[^69]: 用于满足多样用户偏好的算术控制LLMs：具有多目标奖励的方向偏好对齐

    Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards

    [https://arxiv.org/abs/2402.18571](https://arxiv.org/abs/2402.18571)

    提出了方向偏好对齐（DPA）框架，通过多目标奖励模拟不同偏好配置，以实现用户相关的偏好控制。

    

    针对大型语言模型（LLMs）的精细控制仍然是一个重要挑战，阻碍了它们适应各种用户需求。本文提出了方向偏好对齐（DPA）框架，通过多目标奖励建模来表示多样化的偏好配置，将用户偏好建模为奖励空间中的方向（即单位向量）以实现用户相关的偏好控制。

    arXiv:2402.18571v1 Announce Type: cross  Abstract: Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance
    
[^70]: RNNs还不是Transformer：在上下文检索中的关键瓶颈

    RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval

    [https://arxiv.org/abs/2402.18510](https://arxiv.org/abs/2402.18510)

    本文研究了RNNs和Transformer在处理算法问题时的表现能力差距，发现RNNs存在关键瓶颈，即无法完美地从上下文中检索信息，导致无法像Transformer那样轻松解决需要这种能力的任务。

    

    本文探讨循环神经网络（RNNs）和Transformer在解决算法问题时的表示能力差距。我们重点关注RNNs是否能在处理长序列时，通过Chain-of-Thought (CoT)提示，与Transformer的性能相匹配。我们的理论分析显示CoT可以改进RNNs，但无法弥补与Transformer之间的差距。关键瓶颈在于RNNs无法完全从上下文中检索信息，即使经过CoT的增强：对于几个明确或隐式需要这种能力的任务，如联想召回和确定图是否为树，我们证明RNNs表达能力不足以解决这些任务，而Transformer可以轻松解决。相反，我们证明采用增强RNNs上下文检索能力的技术，包括

    arXiv:2402.18510v1 Announce Type: cross  Abstract: This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems. We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease. Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, inclu
    
[^71]: 语言模型表达自我和他人信念

    Language Models Represent Beliefs of Self and Others

    [https://arxiv.org/abs/2402.18496](https://arxiv.org/abs/2402.18496)

    通过神经激活线性解析语言模型中代理人观点下的信念状态，揭示了大型语言模型内部表述自我和他人信念，这对社会推理过程至关重要，并在多样社会推理任务中具有潜在的泛化能力。

    

    理解和归因心理状态，即心灵理论（ToM），被视为人类社会推理的基本能力。虽然大型语言模型（LLMs）似乎具有某些ToM能力，但这些能力背后的机制仍然令人费解。在本研究中，我们发现通过语言模型的神经激活线性解码各个代理人观点下的信念状态是可能的，这表明存在自我的内部表述和他人信念的表示。通过操纵这些表征，我们观察到模型的ToM性能发生显著变化，突显了其在社会推理过程中的关键作用。此外，我们的发现还延伸到涉及不同因果推理模式的多样社会推理任务，暗示了这些表征的潜在泛化能力。

    arXiv:2402.18496v1 Announce Type: new  Abstract: Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.
    
[^72]: 一个针对大型视觉语言模型图像推理和描述的认知评估基准

    A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models

    [https://arxiv.org/abs/2402.18409](https://arxiv.org/abs/2402.18409)

    提出了一个新颖的评估基准，用于评估大型视觉语言模型的认知能力，发现LVLMs与人类之间存在较大的认知能力差距。

    

    尽管大型视觉语言模型(LVLMs)近年来取得了成功，但它们很少受到全面的认知能力测试。受到人类认知测试中广泛使用的“偷饼干”任务的启发，我们提出了一个新颖的评估基准，利用具有丰富语义的图像评估LVLMs的高级认知能力。它定义了八种推理能力，并包括图像描述任务和视觉问答任务。我们对知名LVLMs进行的评估表明，在LVLMs和人类之间仍存在较大的认知能力差距。

    arXiv:2402.18409v1 Announce Type: new  Abstract: Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the "Cookie Theft" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics. It defines eight reasoning capabilities and consists of an image description task and a visual question answering task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.
    
[^73]: 探索适配器用于噪声鲁棒自动语音识别

    Exploration of Adapter for Noise Robust Automatic Speech Recognition

    [https://arxiv.org/abs/2402.18275](https://arxiv.org/abs/2402.18275)

    本文研究了适配器用于噪声鲁棒自动语音识别的探索，发现将适配器插入浅层可以获得更显著的效果，真实数据比模拟数据更有效，并且将适配器集成到基于语音增强的ASR系统中可以带来实质性改进。

    

    适应鲁棒的自动语音识别（ASR）系统以解决未知噪声场景至关重要。将适配器集成到神经网络中已经成为一种强大的迁移学习技术。本文深入研究了基于适配器的噪声鲁棒ASR适应。我们使用了CHiME--4数据集进行实验。结果显示，在浅层插入适配器能够产生更显著的效果，在仅在浅层内部进行适应和在所有层之间进行适应之间没有显著差异。此外，模拟数据有助于系统改善其在实际噪声条件下的表现。然而，在数据量相同时，真实数据比模拟数据更有效。在适配器训练中，多条件训练仍然有效。此外，将适配器集成到基于语音增强的ASR系统中会带来显著的改进。

    arXiv:2402.18275v1 Announce Type: cross  Abstract: Adapting a robust automatic speech recognition (ASR) system to tackle unseen noise scenarios is crucial. Integrating adapters into neural networks has emerged as a potent technique for transfer learning. This paper thoroughly investigates adapter-based noise-robust ASR adaptation. We conducted the experiments using the CHiME--4 dataset. The results show that inserting the adapter in the shallow layer yields superior effectiveness, and there is no significant difference between adapting solely within the shallow layer and adapting across all layers. Besides, the simulated data helps the system to improve its performance under real noise conditions. Nonetheless, when the amount of data is the same, the real data is more effective than the simulated data. Multi-condition training remains valid for adapter training. Furthermore, integrating adapters into speech enhancement-based ASR systems yields substantial improvements.
    
[^74]: MIKO：基于大型语言模型的社交媒体常识发现的多模态意图知识蒸馏

    MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery

    [https://arxiv.org/abs/2402.18169](https://arxiv.org/abs/2402.18169)

    提出了一种基于大型语言模型的MIKO框架，利用多模态和文本的协同作用揭示社交媒体用户的意图。

    

    社交媒体已经成为与他人联系、了解新闻、表达观点以及找到娱乐的无处不在的工具。然而，由于社交媒体帖子中意图的隐含性、需要跨模态理解文本和图像、以及存在标签、拼写错误和复杂缩写等嘈杂信息，理解社交媒体帖子背后的意图仍然具有挑战性。为了解决这些挑战，我们提出了MIKO，一个多模态意图知识蒸馏框架，它共同利用大型语言模型（LLM）和多模态大型语言模型（MLLM）来揭示用户的意图。具体来说，我们使用MLLM来解释图像，并使用LLM从文本中提取关键信息，最后再次指导LLM生成意图。通过将MIKO应用于公开可用的社交媒体数据集，我们构建了一个意图知识

    arXiv:2402.18169v1 Announce Type: new  Abstract: Social media has become a ubiquitous tool for connecting with others, staying updated with news, expressing opinions, and finding entertainment. However, understanding the intention behind social media posts remains challenging due to the implicitness of intentions in social media posts, the need for cross-modality understanding of both text and images, and the presence of noisy information such as hashtags, misspelled words, and complicated abbreviations. To address these challenges, we present MIKO, a Multimodal Intention Kowledge DistillatiOn framework that collaboratively leverages a Large Language Model (LLM) and a Multimodal Large Language Model (MLLM) to uncover users' intentions. Specifically, we use an MLLM to interpret the image and an LLM to extract key information from the text and finally instruct the LLM again to generate intentions. By applying MIKO to publicly available social media datasets, we construct an intention kno
    
[^75]: 评估语法错误校正的有效性：在日本背景下的人类评估方法

    Assessing the Efficacy of Grammar Error Correction: A Human Evaluation Approach in the Japanese Context

    [https://arxiv.org/abs/2402.18101](https://arxiv.org/abs/2402.18101)

    评估了一种日本背景下人类评估方法中最先进语法错误检测和校正模型的性能，在错误校正方面表现出较高的准确性和保守性，在错误检测方面表现出高精确度和调整召回率。

    

    在这项研究中，我们使用自动注释工具包ERRANT，评估了最先进的序列标记语法错误检测和校正模型（SeqTagger）在日本大学生写作样本上的表现。首先，我们将SeqTagger的性能与人类专家校正作为基准来评估错误校正的性能。然后采用人工标注方法来评估Seqtagger在错误检测方面的表现，使用写作数据集的一个子集。结果显示，在整个数据集上，错误校正的精确度为63.66%，召回率为20.19%。对于子集，在手动排除了语义和机械错误等不相关错误后，模型在错误检测方面显示出97.98%的调整精确度和42.98%的调整召回率，表明模型具有很高的准确性，但也表现出保守性。对模型未检测到的错误进行的主题分析揭示了限定词和冠词等问题。

    arXiv:2402.18101v1 Announce Type: new  Abstract: In this study, we evaluated the performance of the state-of-the-art sequence tagging grammar error detection and correction model (SeqTagger) using Japanese university students' writing samples. With an automatic annotation toolkit, ERRANT, we first evaluated SeqTagger's performance on error correction with human expert correction as the benchmark. Then a human-annotated approach was adopted to evaluate Seqtagger's performance in error detection using a subset of the writing dataset. Results indicated a precision of 63.66% and a recall of 20.19% for error correction in the full dataset. For the subset, after manual exclusion of irrelevant errors such as semantic and mechanical ones, the model shows an adjusted precision of 97.98% and an adjusted recall of 42.98% for error detection, indicating the model's high accuracy but also its conservativeness. Thematic analysis on errors undetected by the model revealed that determiners and article
    
[^76]: 在回答和解释具有挑战性的医学问题上对大型语言模型的基准测试

    Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions

    [https://arxiv.org/abs/2402.18060](https://arxiv.org/abs/2402.18060)

    在回答医学问题方面，大型语言模型在处理具有挑战性的实际临床案例上的表现是关键，因此构建了两个结构化数据集进行评估。

    

    LLMs在回答医学问题方面表现出色，例如通过医学执照考试。然而，大多数现有的基准测试依赖于委员会考试问题或一般医学问题，无法捕捉真实临床案例的复杂性。此外，缺乏答案的参考解释阻碍了对模型解释的评估，这对支持医生做出复杂的医疗决策至关重要。为解决这些挑战，我们构建了两个新数据集：JAMA临床挑战和Medbullets。JAMA临床挑战包含基于具有挑战性的临床案例的问题，而Medbullets包含类似USMLE Step 2&3风格的临床问题。两个数据集均以多项选择问题-回答任务的结构化形式呈现，每个问题都附有专家撰写的解释。我们使用各种提示在这两个数据集上评估了四个LLMs。实验表明

    arXiv:2402.18060v1 Announce Type: new  Abstract: LLMs have demonstrated impressive performance in answering medical questions, such as passing medical licensing examinations. However, most existing benchmarks rely on board exam questions or general medical questions, falling short in capturing the complexity of realistic clinical cases. Moreover, the lack of reference explanations for answers hampers the evaluation of model explanations, which are crucial to supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets. JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises USMLE Step 2&3 style clinical questions. Both datasets are structured as multiple-choice question-answering tasks, where each question is accompanied by an expert-written explanation. We evaluate four LLMs on the two datasets using various prompts. Experiments demonstrat
    
[^77]: 检索即精准生成

    Retrieval is Accurate Generation

    [https://arxiv.org/abs/2402.17532](https://arxiv.org/abs/2402.17532)

    提出了一种从支持文档中选择上下文感知短语的新颖生成方法，通过迭代式自我强化提高训练数据准确性，在各种任务中表现优越，提高了开放式文本生成的质量。

    

    标准语言模型通过从固定的、有限的和独立的词汇中选择标记来生成文本。我们介绍了一种新颖的方法，从一组支持文档中选择上下文感知的短语。这种范式转变中最重要的挑战之一是确定训练数据，因为文本可以以多种方式分割，并且每个片段都可以从多个可能的文档中检索到。为了解决这个问题，我们提出使用语言启发式初始化训练数据，更重要的是通过迭代式自我强化来引导训练数据。大量实验表明，我们的模型不仅在各种知识密集型任务上优于标准语言模型，而且在开放式文本生成中展现出更好的生成质量。例如，与标准语言模型对应的模型，在开放性任务上将准确率从23.47%提高到36.27%。

    arXiv:2402.17532v1 Announce Type: new  Abstract: Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation. For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on Open
    
[^78]: 通过反向翻译防御LLMs免受越狱攻击

    Defending LLMs against Jailbreaking Attacks via Backtranslation

    [https://arxiv.org/abs/2402.16459](https://arxiv.org/abs/2402.16459)

    通过反向翻译来防御LLMs免受越狱攻击，将生成的反向翻译提示用于揭示原始提示的实际意图，提高了模型的安全性。

    

    尽管许多大型语言模型（LLMs）已经被训练成拒绝有害请求，但它们仍然容易受到越狱攻击的影响，这种攻击会重写原始提示以隐藏其有害意图。在本文中，我们提出了一种新方法，通过“反向翻译”来防御LLMs免受越狱攻击。具体来说，给定目标LLM从输入提示生成的初始响应，我们的反向翻译提示一个语言模型来推断可以导致该响应的输入提示。推断的提示称为反向翻译提示，倾向于揭示原始提示的实际意图，因为它是基于LLM的响应生成的，不是直接由攻击者操纵的。然后，我们再次在反向翻译提示上运行目标LLM，如果模型拒绝了反向翻译提示，则拒绝原始提示。我们解释了所提出的防御措施对其有效性的几个好处。

    arXiv:2402.16459v1 Announce Type: cross  Abstract: Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent. In this paper, we propose a new method for defending LLMs against jailbreaking attacks by ``backtranslation''. Specifically, given an initial response generated by the target LLM from an input prompt, our backtranslation prompts a language model to infer an input prompt that can lead to the response. The inferred prompt is called the backtranslated prompt which tends to reveal the actual intent of the original prompt, since it is generated based on the LLM's response and is not directly manipulated by the attacker. We then run the target LLM again on the backtranslated prompt, and we refuse the original prompt if the model refuses the backtranslated prompt. We explain that the proposed defense provides several benefits on its effectiv
    
[^79]: LLM推断揭示：调查与Roofline模型见解

    LLM Inference Unveiled: Survey and Roofline Model Insights

    [https://arxiv.org/abs/2402.16363](https://arxiv.org/abs/2402.16363)

    本文提出了一个基于Roofline模型的框架，用于系统分析LLM推断技术，帮助识别部署中的瓶颈，并为更有效地部署LLM提供策略。

    

    高效大语言模型（LLM）推断领域正在迅速发展，提供了机遇和挑战的独特结合。虽然该领域已经扩展并充满活力，但至今还没有一个简明的框架来分析LLM推断的各种方法，以便清晰地理解这一领域。我们的调查不仅总结了当前研究现状，还基于Roofline模型引入了一个框架，用于系统分析LLM推断技术。这一框架能够帮助识别LLM部署中的瓶颈，并更深入地了解在实际设备上的实际方面，从而为部署LLM提供更有效的策略。此外，我们还系统地汇总了高效LLM推断的最新进展，涵盖关键领域，比如权重优化（如知识蒸馏和量化）。

    arXiv:2402.16363v1 Announce Type: cross  Abstract: The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework enables identifying the bottlenecks in LLM deployments and provides a deeper understanding of the practical aspects on real devices, thereby informing more effective strategies for deploying LLM. Furthermore, we systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as weight optimization (e.g., Knowledge Distillation and Quantizatio
    
[^80]: 通过语义平滑防御大型语言模型遭遇监狱攻击

    Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing

    [https://arxiv.org/abs/2402.16192](https://arxiv.org/abs/2402.16192)

    提出了一种名为SEMANTICSMOOTH的防御方法，通过聚合多个语义转换副本的预测结果来防御大型语言模型遭遇GCG、PAIR和AutoDAN攻击，同时保持了较强的正常性能。

    

    对齐的大型语言模型(LLMs)容易受到监狱攻击的威胁，这些攻击可以绕过目标LLMs的保护措施，并骗过它们生成令人反感的内容。我们提出了SEMANTICSMOOTH，一种基于平滑的防御方法，通过聚合多个经过语义转换的给定输入提示的预测结果，来提高对GCG、PAIR和AutoDAN攻击的抵抗能力。实验结果表明，SEMANTICSMOOTH在保持指导性基准测试（如InstructionFollowing和AlpacaEval）上的强劲性能的同时，实现了对各种攻击的最新技术防御。

    arXiv:2402.16192v1 Announce Type: new  Abstract: Aligned large language models (LLMs) are vulnerable to jailbreaking attacks, which bypass the safeguards of targeted LLMs and fool them into generating objectionable content. While initial defenses show promise against token-based threat models, there do not exist defenses that provide robustness against semantic attacks and avoid unfavorable trade-offs between robustness and nominal performance. To meet this need, we propose SEMANTICSMOOTH, a smoothing-based defense that aggregates the predictions of multiple semantically transformed copies of a given input prompt. Experimental results demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on instruction following benchmarks such as InstructionFollowing and AlpacaEval. The codes will be publicly available at https://github.com/UCSB-NLP-Chang/SemanticSmooth.
    
[^81]: 通过多种群意识优化检测机器生成文本的最大均值离差

    Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy

    [https://arxiv.org/abs/2402.16041](https://arxiv.org/abs/2402.16041)

    通过多种群意识优化检测机器生成文本的最大均值离差，解决了机器生成文本与人工编写文本之间微妙的分布差异挑战。

    

    大型语言模型（LLMs）如ChatGPT在生成类人文本方面表现出色。然而，机器生成文本可能存在严重风险，如抄袭问题、误导性信息或幻觉问题。因此，在许多情况下，检测机器生成文本是非常紧迫和重要的。不幸的是，由于LLMs的出色表现，区分机器生成文本和人工编写文本之间的分布差异常常非常微妙，这是具有挑战性的。在这篇论文中，我们试图利用\textit{最大均值离差}（MMD）来解决这个问题，因为MMD可以很好地识别分布差异。然而，直接使用各种机器生成文本对MMD进行训练将导致MMD的方差显著增加，因为不同LLMs的机器生成文本可能包含\textit{多个文本群体}。这将严重损害MMD测量分布差异的能力。

    arXiv:2402.16041v1 Announce Type: new  Abstract: Large language models (LLMs) such as ChatGPT have exhibited remarkable performance in generating human-like texts. However, machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues, misleading information, or hallucination issues. Therefore, it is very urgent and important to detect MGTs in many situations. Unfortunately, it is challenging to distinguish MGTs and human-written texts because the distributional discrepancy between them is often very subtle due to the remarkable performance of LLMs. In this paper, we seek to exploit \textit{maximum mean discrepancy} (MMD) to address this issue in the sense that MMD can well identify distributional discrepancies. However, directly training a detector with MMD using diverse MGTs will incur a significantly increased variance of MMD since MGTs may contain \textit{multiple text populations} due to various LLMs. This will severely impair MMD's ability to measure the diff
    
[^82]: 评估LLMs的谈判能力：一个基准和一个买方增强方法

    Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method

    [https://arxiv.org/abs/2402.15813](https://arxiv.org/abs/2402.15813)

    首次将谈判任务形式化描述为不完全信息的不对称游戏，提出了一种用于评估代理表现的方法，并发现扮演买方的难度大且模型大小不能有效提高买方表现，为此提出了一种名为OG-Narrator的新方法。

    

    谈判是人类之间谈判的一个重要且独特的部分。随着基于LLM的代理学习谈判并表现得像真正的人类一样，如何评估代理的谈判能力仍然是一个悬而未决的问题。我们第一次将谈判任务形式化描述为一种不完全信息的不对称游戏，定义了买方和卖方在多次谈判过程中的收益，使我们能够定量评估一个代理在谈判任务中的表现。我们收集了一个真实产品价格数据集AmazonHistoryPrice，并对各种LLM代理的谈判能力进行了评估。我们发现扮演买方比扮演卖方要困难得多，并且增加模型大小无法有效地提高买方的表现。为了解决这一挑战，我们提出了一种称为OG-Narrator的新方法，该方法集成了一个确定性的报价生成器来控制买方报价的价格范围，并且集成了一个LLM解说者来创建一种自然

    arXiv:2402.15813v1 Announce Type: new  Abstract: Bargaining is an important and unique part of negotiation between humans. As LLM-driven agents learn to negotiate and act like real humans, how to evaluate agents' bargaining abilities remains an open problem. For the first time, we formally described the Bargaining task as an asymmetric incomplete information game, defining the gains of the Buyer and Seller in multiple bargaining processes. It allows us to quantitatively assess an agent's performance in the Bargain task. We collected a real product price dataset, AmazonHistoryPrice, and conducted evaluations of various LLM agents' bargaining abilities. We find that playing a Buyer is much harder than a Seller, and increasing model size can not effectively improve the Buyer's performance. To address the challenge, we propose a novel approach called OG-Narrator that integrates a deterministic Offer Generator to control the price range of Buyer's offers, and an LLM Narrator to create natur
    
[^83]: RelayAttention：用于高效实现大型语言模型服务与长系统提示的论文

    RelayAttention for Efficient Large Language Model Serving with Long System Prompts

    [https://arxiv.org/abs/2402.14808](https://arxiv.org/abs/2402.14808)

    本论文提出的RelayAttention算法旨在改善涉及长系统提示的大型语言模型服务的效率，通过一次性从DRAM读取隐藏状态来消除现有因果注意力算法中的内存访问冗余。

    

    实际的大型语言模型（LLM）服务可能涉及一个长的系统提示，其中包含任务的指示、示例和知识文档，并在许多请求中复用。然而，长系统提示会导致吞吐量/延迟瓶颈，因为生成下一个标记的成本随着序列长度的增长而增加。本文旨在提高涉及长系统提示的LLM服务的效率。我们的关键观察是，处理这些系统提示在现有因果注意力计算算法中需要大量冗余的内存访问。具体来说，对于批量请求，系统提示的缓存隐藏状态（即键-值对）被多次从芯片外的DRAM传输到芯片上的SRAM，每次对应一个单独的请求。为了消除这种冗余，我们提出了RelayAttention，一种注意力算法，允许仅从DRAM读取这些隐藏状态一次。

    arXiv:2402.14808v1 Announce Type: new  Abstract: Practical large language model (LLM) services may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across numerous requests. However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length. This paper aims to improve the efficiency of LLM services that involve long system prompts. Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (i.e., key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once 
    
[^84]: 《Tokenization and the Noiseless Channel》的两个反例

    Two Counterexamples to \textit{Tokenization and the Noiseless Channel}

    [https://arxiv.org/abs/2402.14614](https://arxiv.org/abs/2402.14614)

    该论文讨论了在《Tokenization and the Noiseless Channel》提出的使用Rényi效率作为分词器评估机制的局限性，并描述了两个BPE分词的反例，展示了Rényi效率无法捕捉到所有优秀分词方案的情况。

    

    在《Tokenization and the Noiseless Channel》中，建议使用Rényi效率作为评估分词器的固有机制: 对于NLP任务，应选择导致unigram分布Rényi效率最高的分词器。因此，Rényi效率被视为下游性能的预测器（例如，用于预测机器翻译任务的BLEU），而无需通过训练不同分词器的多个模型这一昂贵的步骤。尽管有用，但这一度量标准的预测能力并不完美，作者指出有其他优秀分词方案的附加特质Rényi效率本身无法捕捉。我们描述了两种BPE分词的变体，可以在增加Rényi效率的同时降低下游模型性能。这些反例揭示了Rényi效率作为固有分词的情况下存在失败的情况。

    arXiv:2402.14614v1 Announce Type: new  Abstract: In \textit{Tokenization and the Noiseless Channel} \cite{zouhar-etal-2023-tokenization}, R\'enyi efficiency is suggested as an intrinsic mechanism for evaluating a tokenizer: for NLP tasks, the tokenizer which leads to the highest R\'enyi efficiency of the unigram distribution should be chosen. The R\'enyi efficiency is thus treated as a predictor of downstream performance (e.g., predicting BLEU for a machine translation task), without the expensive step of training multiple models with different tokenizers. Although useful, the predictive power of this metric is not perfect, and the authors note there are additional qualities of a good tokenization scheme that R\'enyi efficiency alone cannot capture.   We describe two variants of BPE tokenization which can arbitrarily increase R\'enyi efficiency while decreasing the downstream model performance. These counterexamples expose cases where R\'enyi efficiency fails as an intrinsic tokenizati
    
[^85]: Sequoia: 可扩展、稳健且硬件感知的推测解码

    Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding

    [https://arxiv.org/abs/2402.12374](https://arxiv.org/abs/2402.12374)

    Sequoia是一种可扩展、稳健且硬件感知的推测解码算法，通过引入动态规划算法优化标记树结构、采用新颖的采样和验证方法实现稳健性能以及硬件感知的树优化器最大化推测性能。

    

    随着大型语言模型（LLMs）的使用增多，使用这些模型进行高效推理变得日益重要。虽然最近推测解码已经成为加速推理的一个有前途的方向，但现有方法在扩展到较大的推测预算、适应不同超参数和硬件方面存在局限性。本文介绍了Sequoia，一个可扩展、稳健且硬件感知的用于推测解码的算法。为了实现更好的可扩展性，Sequoia引入了一个动态规划算法来找到用于被推测标记的最佳树结构。为了实现稳健的推测性能，Sequoia使用了一种新颖的采样和验证方法，该方法在不同解码温度下优于先前的方法。最后，Sequoia引入了一种硬件感知的树优化器，通过自动选择给定情况下的标记树大小和深度来最大化推测性能。

    arXiv:2402.12374v1 Announce Type: new  Abstract: As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important. While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware. This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding. To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens. To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a giv
    
[^86]: 在金融文档问答中评估LLMs的数学推理能力

    Assessing LLMs' Mathematical Reasoning in Financial Document Question Answering

    [https://arxiv.org/abs/2402.11194](https://arxiv.org/abs/2402.11194)

    通过实验评估了LLMs在金融表格问答中的数学推理能力，发现引入了一种新型提示技术，能够在性能上胜过其他基线模型

    

    大型语言模型（LLMs）在自然语言理解方面表现出色，但它们在具有结构化表格和非结构化文本混合的复杂数学推理方面的能力尚不确定。本研究探讨了LLMs在四个金融表格问答数据集上的数学推理能力：TATQA、FinQA、ConvFinQA和Multihiertt。通过对各种模型和提示技术进行广泛实验，我们评估了LLMs如何适应复杂表格和数学任务。我们关注对表格复杂性的敏感性以及在增加算术推理步骤数量时性能变化。结果揭示了LLMs处理半结构化表格中复杂数学场景的能力和局限性。最终，我们引入了一种针对半结构化文档的新型提示技术，在性能方面与其他基线相匹配或胜过，并提供了对LLMs能力的微妙理解。

    arXiv:2402.11194v1 Announce Type: new  Abstract: Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with an amalgamation of structured tables and unstructured text is uncertain. This study explores LLMs' mathematical reasoning on four financial tabular question-answering datasets: TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with various models and prompting techniques, we assess how LLMs adapt to complex tables and mathematical tasks. We focus on sensitivity to table complexity and performance variations with an increasing number of arithmetic reasoning steps. The results provide insights into LLMs' capabilities and limitations in handling complex mathematical scenarios for semi-structured tables. Ultimately, we introduce a novel prompting technique tailored to semi-structured documents, matching or outperforming other baselines in performance while providing a nuanced understanding 
    
[^87]: 名词短语中头部的最佳位置。指示语、数词、形容词和名词的案例。

    The optimal placement of the head in the noun phrase. The case of demonstrative, numeral, adjective and noun

    [https://arxiv.org/abs/2402.10311](https://arxiv.org/abs/2402.10311)

    本研究旨在探讨句法依赖距离最小化与意外减少最小化原则在名词短语中的冲突，结论显示当涉及的单词较少且单词较短时，意外减少可能会超越句法依赖距离优化。

    

    一句话的词序由多种原则塑造。句法依赖距离最小化原则与意外减少最小化原则（或可预测性最大化）在单一头部的句法依赖结构中存在冲突：前者预测头部应该放置在线性排列的中心，后者预测头部应该放置在两端之一（要么在首位，要么在末位）。一个关键问题是何时意外减少（或可预测性最大化）应该超越句法依赖距离最小化。在单一头部结构的背景下，预测在满足两个条件时更有可能发生，即（a）涉及的单词较少，并且（b）单词较短。在这里，我们在由指示语、数词、形容词和名词组成的名词短语上测试了这一预测。我们发现，在首选顺序中...（缺失部分无法提供完整翻译）

    arXiv:2402.10311v1 Announce Type: new  Abstract: The word order of a sentence is shaped by multiple principles. The principle of syntactic dependency distance minimization is in conflict with the principle of surprisal minimization (or predictability maximization) in single head syntactic dependency structures: while the former predicts that the head should be placed at the center of the linear arrangement, the latter predicts that the head should be placed at one of the ends (either first or last). A critical question is when surprisal minimization (or predictability maximization) should surpass syntactic dependency distance minimization. In the context of single head structures, it has been predicted that this is more likely to happen when two conditions are met, i.e. (a) fewer words are involved and (b) words are shorter. Here we test the prediction on the noun phrase when its composed of a demonstrative, a numeral, an adjective and a noun. We find that, across preferred orders in l
    
[^88]: 知识注入的基于LLM的对话式健康代理：糖尿病患者的案例研究

    Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients

    [https://arxiv.org/abs/2402.10153](https://arxiv.org/abs/2402.10153)

    本文提出了一种知识注入的基于LLM的对话式健康代理（CHA）用于糖尿病患者，通过整合领域特定知识和分析能力，提高了糖尿病管理的准确性和效果。

    

    有效的糖尿病管理对于糖尿病患者的健康至关重要。大型语言模型（LLM）为糖尿病管理开辟了新的途径，提高了其效果。然而，目前基于LLM的方法受限于对一般来源的依赖，缺乏与领域特定知识的整合，导致回复不准确。本文提出了一种知识注入的基于LLM的对话式健康代理（CHA）用于糖尿病患者。我们根据开源的openCHA框架进行定制，并增强了我们的CHA的外部知识和分析能力。这种整合包括两个关键组成部分：1）整合美国糖尿病协会的膳食指南和Nutritionix的信息；2）部署分析工具，实现营养摄入计算并与指南进行比较。我们将提出的CHA与GPT4进行比较。我们的评估包括100个糖尿病患者的使用情况。

    arXiv:2402.10153v1 Announce Type: new  Abstract: Effective diabetes management is crucial for maintaining health in diabetic patients. Large Language Models (LLMs) have opened new avenues for diabetes management, facilitating their efficacy. However, current LLM-based approaches are limited by their dependence on general sources and lack of integration with domain-specific knowledge, leading to inaccurate responses. In this paper, we propose a knowledge-infused LLM-powered conversational health agent (CHA) for diabetic patients. We customize and leverage the open-source openCHA framework, enhancing our CHA with external knowledge and analytical capabilities. This integration involves two key components: 1) incorporating the American Diabetes Association dietary guidelines and the Nutritionix information and 2) deploying analytical tools that enable nutritional intake calculation and comparison with the guidelines. We compare the proposed CHA with GPT4. Our evaluation includes 100 diabe
    
[^89]: 通过基于垄断对话的社交场景模拟实现大型语言模型的自对齐

    Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation

    [https://arxiv.org/abs/2402.05699](https://arxiv.org/abs/2402.05699)

    本文提出了一个通过社交场景模拟来自对齐大型语言模型的方法，以减轻其被滥用造成的潜在不良影响。通过一个名为MATRIX的虚拟排练空间，LLM可以在回答查询前考虑社交后果，并通过MATRIX-simulated数据的微调，保持对人类价值的遵从和推理速度的平衡。实验证明，在温和假设下，带有MATRIX的LLM胜过了宪法AI。

    

    将大型语言模型(LLMs)与人类价值对齐，以减轻其被滥用造成的潜在不良影响，具有重要意义。本文借鉴社会学的见解，即认识到所有各方的关切是塑造人类价值观的关键因素，提出了一种自对齐LLMs的新方向：社交场景模拟。为此，我们提出了一个名为MATRIX的创新社交场景模拟器，它可以模拟用户输入查询周围的现实场景，使LLM在回答前能够考虑社交后果。MATRIX类似于一个“垄断对话”下的虚拟排练空间，LLM在其中扮演与查询相关的多个角色并进行自我实践。为了引入这种对齐能力，我们使用MATRIX模拟数据对LLM进行微调，确保其在不影响推理速度的情况下符合人类价值观。理论上，我们证明了在温和假设下，带有MATRIX的LLM胜过了宪法AI。最后，大量实验证实了我们的方法在多个任务上都取得了最佳性能。

    Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms Constitutional AI under mild assumptions. Finally, extensive experiments validate that ou
    
[^90]: 使用自反大型语言模型学习生成可解释的股票预测

    Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models

    [https://arxiv.org/abs/2402.03659](https://arxiv.org/abs/2402.03659)

    这个论文介绍了使用大型语言模型生成可解释的股票预测的方法，并提出了Summarize-Explain-Predict（SEP）模型来解决股票预测中的解释问题和数据标注成本的挑战。

    

    对于传统的非生成式深度学习模型来说，解释股票预测通常是一项困难的任务，其中解释仅限于可视化重要文本上的注意力权重。目前，大型语言模型（LLM）为解决这个问题提供了一个解决方案，因为它们具有生成人类可读解释其决策过程的能力。然而，股票预测对LLM来说仍然具有挑战性，因为它需要能够权衡混乱社会文本对股票价格的不同影响。随着引入解释组件，问题变得越来越困难，需要LLM能够用口头方式解释为什么某些因素比其他因素更重要。另一方面，要为这样的任务对LLM进行微调，需要专家标注的样本来解释训练集中的每次股票波动，这在成本和实际可扩展性上是昂贵且不可行的。为了解决这些问题，我们提出了我们的Summarize-Explain-Predict（SEP）模型。

    Explaining stock predictions is generally a difficult task for traditional non-generative deep learning models, where explanations are limited to visualizing the attention weights on important texts. Today, Large Language Models (LLMs) present a solution to this problem, given their known capabilities to generate human-readable explanations for their decision-making process. However, the task of stock prediction remains challenging for LLMs, as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices. The problem gets progressively harder with the introduction of the explanation component, which requires LLMs to explain verbally why certain factors are more important than the others. On the other hand, to fine-tune LLMs for such a task, one would need expert-annotated samples of explanation for every stock movement in the training set, which is expensive and impractical to scale. To tackle these issues, we propose our Summarize-Explain-Predict (SEP) 
    
[^91]: BPDec: 揭示BERT预训练中掩码语言建模解码器的潜力

    BPDec: Unveiling the Potential of Masked Language Modeling Decoder in BERT pretraining

    [https://arxiv.org/abs/2401.15861](https://arxiv.org/abs/2401.15861)

    本文揭示了BPDec（BERT预训练解码器）的潜力，强调增强的掩码语言建模解码器设计及研究在BERT预训练中的重要性。

    

    BERT（来自Transformer的双向编码表示）通过其在许多任务上出色的性能彻底改变了自然语言处理领域。然而，大多数研究人员主要集中在与模型结构相关的增强，例如相对位置嵌入和更有效的注意机制。还有一些人深入研究了与掩码语言建模相关的预训练技巧，包括整词掩码。DeBERTa引入了一种针对BERT编码器模型进行预训练的增强解码器，证明效果非常显著。我们认为围绕增强掩码语言建模解码器的设计和研究并未得到应有的重视。在本文中，我们提出了几种增强解码器的设计，并介绍了BPDec（BERT预训练解码器），这是一种用于建模训练的新方法。通常，预训练的BERT模型会针对特定的自然语

    arXiv:2401.15861v2 Announce Type: replace-cross  Abstract: BERT (Bidirectional Encoder Representations from Transformers) has revolutionized the field of natural language processing through its exceptional performance on numerous tasks. Yet, the majority of researchers have mainly concentrated on enhancements related to the model structure, such as relative position embedding and more efficient attention mechanisms. Others have delved into pretraining tricks associated with Masked Language Modeling, including whole word masking. DeBERTa introduced an enhanced decoder adapted for BERT's encoder model for pretraining, proving to be highly effective. We argue that the design and research around enhanced masked language modeling decoders have been underappreciated. In this paper, we propose several designs of enhanced decoders and introduce BPDec (BERT Pretraining Decoder), a novel method for modeling training. Typically, a pretrained BERT model is fine-tuned for specific Natural Language 
    
[^92]: 认知负荷: 通过超载逻辑思维越狱大型语言模型

    Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking

    [https://arxiv.org/abs/2311.09827](https://arxiv.org/abs/2311.09827)

    本文研究了一种新颖的越狱攻击，针对大型语言模型的认知结构和过程进行设计，通过认知过载攻击，即使在安全对齐之后，也可以激发LLMs产生有害或不道德的响应。

    

    虽然大型语言模型（LLMs）展示了越来越强大的能力，但也引发了各种有害行为。作为代表，越狱攻击可能引发LLMs产生有害或不道德的响应，即使经过了安全对齐。本文研究了一类新颖的越狱攻击，专门针对LLMs的认知结构和过程进行设计。具体来说，我们分析了LLMs在面对（1）多语言认知负荷，（2）隐晦表达和（3）效果推导推理时的安全性脆弱性。与先前的越狱攻击不同，我们提出的认知负载是一种无需了解模型架构或访问模型权重的黑盒攻击。在AdvBench和MasterKey上进行的实验表明，包括流行的开源模型Llama 2和专有模型ChatGPT在内的各种LLMs可以通过认知过载受到影响。

    arXiv:2311.09827v2 Announce Type: replace  Abstract: While large language models (LLMs) have demonstrated increasing power, they have also given rise to a wide range of harmful behaviors. As representatives, jailbreak attacks can provoke harmful or unethical responses from LLMs, even after safety alignment. In this paper, we investigate a novel category of jailbreak attacks specifically designed to target the cognitive structure and processes of LLMs. Specifically, we analyze the safety vulnerability of LLMs in the face of (1) multilingual cognitive overload, (2) veiled expression, and (3) effect-to-cause reasoning. Different from previous jailbreak attacks, our proposed cognitive overload is a black-box attack with no need for knowledge of model architecture or access to model weights. Experiments conducted on AdvBench and MasterKey reveal that various LLMs, including both popular open-source model Llama 2 and the proprietary model ChatGPT, can be compromised through cognitive overloa
    
[^93]: OrchestraLLM：用于对话状态跟踪的语言模型高效编排

    OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking

    [https://arxiv.org/abs/2311.09758](https://arxiv.org/abs/2311.09758)

    本研究提出了一种新颖的SLM/LLM路由框架，以提高计算效率和增强任务性能，通过利用结构化知识提取任务中SLMs和LLMs的互补优势，从而降低成本而不牺牲性能。

    

    大型语言模型（LLMs）已经彻底改变了自然语言处理系统的格局，但计算成本昂贵。为了降低成本而不损害性能，先前的研究探索了各种方法来利用小型语言模型（SLMs）作为其更大型对应物的经济有效替代品。受到SLMs和LLMs在结构化知识提取任务中显示出互补优势的发现驱动，本文提出了一种新颖的SLM/LLM路由框架，旨在提高计算效率并增强任务性能。首先，创建示范池以表示每个LM提供更可靠答案的上下文类型，利用句子嵌入进行微调，使上下文相似性接近对话状态相似性。然后，在推理过程中，检索到测试实例的k个最近示范，并根据情况路由实例。

    arXiv:2311.09758v2 Announce Type: replace  Abstract: Large language models (LLMs) have revolutionized the landscape of Natural Language Processing systems, but are computationally expensive. To reduce the cost without sacrificing performance, previous studies have explored various approaches to harness the potential of Small Language Models (SLMs) as cost-effective alternatives to their larger counterparts. Driven by findings that SLMs and LLMs exhibit complementary strengths in a structured knowledge extraction task, this work presents a novel SLM/LLM routing framework designed to improve computational efficiency and enhance task performance. First, exemplar pools are created to represent the types of contexts where each LM provides a more reliable answer, leveraging a sentence embedding fine-tuned so that context similarity is close to dialogue state similarity. Then, during inference, the k-nearest exemplars to the testing instance are retrieved, and the instance is routed according
    
[^94]: HallusionBench：一种用于评估大型视觉语言模型中纠缠的语言幻觉和视幻觉的高级诊断套件

    HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models

    [https://arxiv.org/abs/2310.14566](https://arxiv.org/abs/2310.14566)

    HallusionBench是一个专为评估大型视觉语言模型在图像背景推理中面临挑战的基准，通过引入新颖结构和量化分析，显示出GPT-4V取得了31.42%的准确率，远高于其他模型。

    

    我们介绍了HallusionBench，这是一个专为评估图像背景推理而设计的全面基准。这个基准对于高级大型视觉语言模型（LVLMs）（如GPT-4V（Vision）、Gemini Pro Vision和LLaVA-1.5）提出了重大挑战，强调对视觉数据的微妙理解和解释。该基准包含346张图像和1129个问题，全部由人类专家精心设计。我们为这些视觉问题引入了一种新颖的结构，旨在建立对照组。这种结构使我们能够对模型的响应倾向、逻辑一致性和各种故障模式进行定量分析。在我们对HallusionBench的评估中，我们对14种不同模型进行了基准测试，突出了目前最先进的GPT-4V取得的31.42％的问题对准确率。值得注意的是，所有其他评估模型的准确率均低于16％。

    arXiv:2310.14566v3 Announce Type: replace-cross  Abstract: We introduce HallusionBench, a comprehensive benchmark designed for the evaluation of image-context reasoning. This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(Vision), Gemini Pro Vision, and LLaVA-1.5, by emphasizing nuanced understanding and interpretation of visual data. The benchmark comprises 346 images paired with 1129 questions, all meticulously crafted by human experts. We introduce a novel structure for these visual questions designed to establish control groups. This structure enables us to conduct a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes. In our evaluation on HallusionBench, we benchmarked 14 different models, highlighting a 31.42% question-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all other evaluated models achieve accuracy below 16%. Moreover, our analysis not only h
    
[^95]: 重新审视假设：预训练的Transformer是否通过梯度下降在上下文中学习？

    Revisiting the Hypothesis: Do pretrained Transformers Learn In-Context by Gradient Descent?

    [https://arxiv.org/abs/2310.08540](https://arxiv.org/abs/2310.08540)

    本研究重新审视了预训练的Transformer是否通过梯度下降在上下文中学习的假设，并发现现有研究中的假设存在限制性假设，使其与实际语言模型训练时的语境存在显著差异。同时，通过对真实模型的观察和比较，揭示了ICL和GD在观察演示顺序上的不同敏感性。

    

    LLM中的In-Context Learning（ICL）的出现仍然是一个重要现象，但我们对其了解甚少。为了解释ICL，最近的研究尝试在理论上将其与梯度下降（GD）联系起来。我们问，这种联系在实际预训练模型中是否成立？我们强调先前作品中的限制性假设使得它们的语境与语言模型实际训练时的实际语境差别很大。例如，这些研究中使用的理论手工构造的权重具有与真实LLM不匹配的属性。此外，他们的实验验证使用ICL目标（明确为ICL训练模型），这与野外出现的ICL有所不同。我们还寻找了真实模型中的证据。我们观察到ICL和GD对于观察演示的顺序有不同的敏感性。最后，我们在自然环境中探讨并比较ICL与GD假设。

    arXiv:2310.08540v4 Announce Type: replace-cross  Abstract: The emergence of In-Context Learning (ICL) in LLMs remains a significant phenomenon with little understanding. To explain ICL, recent studies try to theoretically connect it to Gradient Descent (GD). We ask, does this connection hold up in actual pre-trained models?   We highlight the limiting assumptions in prior works that make their context considerably different from the practical context in which language models are trained. For example, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. Furthermore, their experimental verification uses ICL objective (training models explicitly for ICL), which differs from the emergent ICL in the wild.   We also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. 
    
[^96]: 通过逻辑增强大型语言模型中的零射链推理能力

    Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic

    [https://arxiv.org/abs/2309.13339](https://arxiv.org/abs/2309.13339)

    提出了LoT（Logical Thoughts）提示，一个自我改进框架，利用根植于符号逻辑的原则，特别是归谬法，逐步验证和纠正大型语言模型的零射链推理过程。

    

    大型语言模型的最新进展展示了它们在各个领域的 remarkable generalizability。然而，它们的推理能力仍有很大的提升空间，特别是在需要多步推理的情况下。尽管大型语言模型具有广泛的知识，但它们的推理经常未能有效利用这些知识来建立连贯的思维范式。这些模型有时会出现幻觉，因为它们的推理过程未受逻辑原则的限制。为了改进大型语言模型的零射链推理能力，我们提出了 LoT（Logical Thoughts）提示，这是一个自我改进的框架，利用根植于符号逻辑的原则，特别是归谬法，逐步系统地验证和纠正推理过程。在语言任务上进行的实验评估

    arXiv:2309.13339v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts) prompting, a self-improvement framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in
    
[^97]: WebVoyager：使用大型多模态模型构建端到端的Web Agent

    WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. (arXiv:2401.13919v1 [cs.CL])

    [http://arxiv.org/abs/2401.13919](http://arxiv.org/abs/2401.13919)

    WebVoyager是一种创新的基于大型多模态模型的Web代理，能够通过与真实网站交互来端到端地完成用户指令。它提出了一个新的Web代理评估协议，并在实际任务中取得了显著的成功率。

    

    大型语言模型（LLMs）的进步引领了一个由真实世界中自主应用程序的发展所标志的新时代，推动了基于网络的高级代理的创新。现有的网络代理通常只处理一个输入模态，并且仅在简化的网络模拟器或静态的网络快照中进行评估，极大地限制了它们在真实场景中的适用性。为了填补这一差距，我们引入了WebVoyager，一种创新的基于大型多模态模型（LMM）的Web代理，通过与真实网站进行交互，能够端到端地完成用户指令。此外，我们提出了一个新的Web代理评估协议，以解决开放式Web代理任务的自动评估挑战，利用了GPT-4V的强大多模态理解能力。我们通过收集来自15个广泛使用的网站的真实世界任务来创建一个新的基准来评估我们的代理。我们展示了WebVoyager实现了55.7％的任务成功率，显著地.....

    The advancement of large language models (LLMs) leads to a new era marked by the development of autonomous applications in the real world, which drives innovation in the creation of advanced web-based agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we propose a new evaluation protocol for web agents to address the challenges of automatic evaluation of open-ended web agent tasks, leveraging the robust multimodal comprehension capabilities of GPT-4V. We create a new benchmark by gathering real-world tasks from 15 widely used websites to evaluate our agents. We show that WebVoyager achieves a 55.7% task success rate, significantly 
    
[^98]: BIBench: 大型语言模型数据分析知识基准测试

    BIBench: Benchmarking Data Analysis Knowledge of Large Language Models. (arXiv:2401.02982v1 [cs.CL])

    [http://arxiv.org/abs/2401.02982](http://arxiv.org/abs/2401.02982)

    BIBench是一个旨在评估大型语言模型（LLMs）在商业智能（BI）数据分析领域中能力的综合基准测试，其通过测试模型在BI基础知识、应用知识和技术技能三个维度上的表现来进行评估。

    

    大型语言模型（LLMs）在各种任务中展示了令人印象深刻的能力。然而，它们在数据分析的专业领域中的熟练度和可靠性，特别是在以数据驱动思维为重点的领域中，仍然存在不确定性。为了填补这一差距，我们介绍了BIBench，这是一个全面的基准测试，旨在评估LLMs在商业智能（BI）的背景下的数据分析能力。BIBench通过三个维度评估LLMs：1）BI基础知识，评估模型的数值推理能力和对金融概念的熟悉程度；2）BI知识应用，确定模型快速理解文本信息并从多个视角生成分析问题的能力；3）BI技术技能，检查模型使用技术知识解决现实数据分析挑战的能力。BIBench包括11个子任务，涵盖分类、提取和生成三种任务类型。

    Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. However, their proficiency and reliability in the specialized domain of Data Analysis, particularly with a focus on data-driven thinking, remain uncertain. To bridge this gap, we introduce BIBench, a comprehensive benchmark designed to evaluate the data analysis capabilities of LLMs within the context of Business Intelligence (BI). BIBench assesses LLMs across three dimensions: 1) BI foundational knowledge, evaluating the models' numerical reasoning and familiarity with financial concepts; 2) BI knowledge application, determining the models' ability to quickly comprehend textual information and generate analysis questions from multiple views; and 3) BI technical skills, examining the models' use of technical knowledge to address real-world data analysis challenges. BIBench comprises 11 sub-tasks, spanning three categories of task types: classification, extraction, and generation. Additi
    
[^99]: 提问更多，了解更多：利用大型语言模型强化学习的决策问题与思维链

    Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models. (arXiv:2310.18127v1 [cs.LG])

    [http://arxiv.org/abs/2310.18127](http://arxiv.org/abs/2310.18127)

    本文提出了一种利用大型语言模型的强化学习框架，能够学习提问相关问题并进行推理来指导在实际环境中执行的行为的学习。

    

    大型语言模型通过将基于行动的策略与思维链（CoT）推理相结合，展示了解决复杂实际挑战的潜力。然而，对于该框架的有效性来说，具有高质量的提示非常重要。目前，这些提示是通过广泛使用人力手工制作的，导致CoT策略经常无法推广。为了确保低层控制器适当地处理CoT推理，还需要人为介入来开发接地函数。在本文中，我们迈出了迈向在复杂推理中应用实际环境中的任务解决的完全集成的端到端框架的第一步。为此，我们提供了一个新的领导者-追随者双层框架，能够学习提问相关问题（提示），并随后进行推理，指导在环境中执行的行为的学习。一个好的提示应该基于历史的自省性修订来进行修改。

    Large language models (LLMs) demonstrate their promise in tackling complicated practical challenges by combining action-based policies with chain of thought (CoT) reasoning. Having high-quality prompts on hand, however, is vital to the framework's effectiveness. Currently, these prompts are handcrafted utilizing extensive human labor, resulting in CoT policies that frequently fail to generalize. Human intervention is also required in order to develop grounding functions that ensure low-level controllers appropriately process CoT reasoning. In this paper, we take the first step towards a fully integrated end-to-end framework for task-solving in real settings employing complicated reasoning. To that purpose, we offer a new leader-follower bilevel framework capable of learning to ask relevant questions (prompts) and subsequently undertaking reasoning to guide the learning of actions to be performed in an environment. A good prompt should make introspective revisions based on historical fi
    
[^100]: 大型语言模型的语义不变鲁棒水印

    A Semantic Invariant Robust Watermark for Large Language Models. (arXiv:2310.06356v1 [cs.CR])

    [http://arxiv.org/abs/2310.06356](http://arxiv.org/abs/2310.06356)

    本论文提出了一种对大型语言模型进行语义不变水印的方法，该方法通过利用另一个嵌入式语言模型生成所有前面token的语义嵌入，然后利用训练得到的水印模型将这些语义嵌入转换为水印logits。该方法既具有攻击鲁棒性又具有安全鲁棒性。

    

    大型语言模型的水印算法在检测LLM生成的文本方面取得了极高的准确性。然而，先前的算法在攻击鲁棒性和安全鲁棒性之间存在一个权衡。本文提出了一种对LLM进行语义不变水印的方法，该方法既具有攻击鲁棒性又具有安全鲁棒性。我们的方法通过另一个嵌入式LLM生成所有前面token的语义嵌入，然后通过训练得到的水印模型将这些语义嵌入转换为水印logits。

    Watermark algorithms for large language models (LLMs) have achieved extremely high accuracy in detecting text generated by LLMs. Such algorithms typically involve adding extra watermark logits to the LLM's logits at each generation step. However, prior algorithms face a trade-off between attack robustness and security robustness. This is because the watermark logits for a token are determined by a certain number of preceding tokens; a small number leads to low security robustness, while a large number results in insufficient attack robustness. In this work, we propose a semantic invariant watermarking method for LLMs that provides both attack robustness and security robustness. The watermark logits in our work are determined by the semantics of all preceding tokens. Specifically, we utilize another embedding LLM to generate semantic embeddings for all preceding tokens, and then these semantic embeddings are transformed into the watermark logits through our trained watermark model. Subs
    
[^101]: 用大型语言模型重新定义数字健康界面

    Redefining Digital Health Interfaces with Large Language Models. (arXiv:2310.03560v1 [cs.CL])

    [http://arxiv.org/abs/2310.03560](http://arxiv.org/abs/2310.03560)

    本论文提出了利用大型语言模型（LLMs）重新定义数字健康界面的方法，将LLMs与外部工具结合使用，从而提高了与临床技术的互动效果，改善了数字医疗工具和AI模型的实用性，并解决了在临床环境中使用LLMs的问题。

    

    数字健康工具具有显著改善医疗服务传递的潜力。然而，由于可用性和信任方面的挑战，它们的使用仍然相对有限。最近，大型语言模型（LLMs）作为具有处理复杂信息和生成人类质量文本能力的通用模型出现，为医疗保健领域提供了丰富的潜在应用。直接在临床环境中应用LLMs并不简单，因为LLMs容易提供不一致或无意义的答案。我们展示了如何利用外部工具使LLMs在临床医疗技术互动中提供全新界面。这增强了数字医疗工具和AI模型的实用性和实际影响，同时解决了在临床环境中使用LLMs的当前问题，如幻觉。我们通过心血管疾病和糖尿病风险预测的例子阐述了我们的方法，并突出了其中的好处。

    Digital health tools have the potential to significantly improve the delivery of healthcare services. However, their use remains comparatively limited due, in part, to challenges surrounding usability and trust. Recently, Large Language Models (LLMs) have emerged as general-purpose models with the ability to process complex information and produce human-quality text, presenting a wealth of potential applications in healthcare. Directly applying LLMs in clinical settings is not straightforward, with LLMs susceptible to providing inconsistent or nonsensical answers. We demonstrate how LLMs can utilize external tools to provide a novel interface between clinicians and digital technologies. This enhances the utility and practical impact of digital healthcare tools and AI models while addressing current issues with using LLM in clinical settings such as hallucinations. We illustrate our approach with examples from cardiovascular disease and diabetes risk prediction, highlighting the benefit
    
[^102]: 基于生成AI的医疗对话效果的量化度量

    Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI. (arXiv:2309.12444v1 [cs.CL])

    [http://arxiv.org/abs/2309.12444](http://arxiv.org/abs/2309.12444)

    这篇论文提出了基于生成AI的医疗对话模型的评估指标问题，并强调了现有指标对医学和健康概念的理解不足以及忽略了用户体验因素。

    

    生成人工智能将通过将传统的患者护理转变为更个性化、高效和积极的过程，彻底改变医疗保健交付方式。聊天机器人作为互动对话模型，很可能推动医疗保健的以患者为中心的转型。通过提供诊断、个性化生活方式建议和心理健康支持等各种服务，目标是大幅度提高患者的健康结果，同时减轻医疗保健提供者的工作负担。医疗应用的生命关键性要求建立统一全面的对话模型评估指标。已有的针对各种通用大型语言模型(LLMs)提出的评估指标在理解医学和健康概念及其在促进患者福祉方面的重要性方面存在不足。此外，这些指标忽略了关键的用户体验因素。

    Generative Artificial Intelligence is set to revolutionize healthcare delivery by transforming traditional patient care into a more personalized, efficient, and proactive process. Chatbots, serving as interactive conversational models, will probably drive this patient-centered transformation in healthcare. Through the provision of various services, including diagnosis, personalized lifestyle recommendations, and mental health support, the objective is to substantially augment patient health outcomes, all the while mitigating the workload burden on healthcare providers. The life-critical nature of healthcare applications necessitates establishing a unified and comprehensive set of evaluation metrics for conversational models. Existing evaluation metrics proposed for various generic large language models (LLMs) demonstrate a lack of comprehension regarding medical and health concepts and their significance in promoting patients' well-being. Moreover, these metrics neglect pivotal user-ce
    
[^103]: 演练：通过模拟冲突来教授冲突解决方法

    Rehearsal: Simulating Conflict to Teach Conflict Resolution. (arXiv:2309.12309v1 [cs.HC])

    [http://arxiv.org/abs/2309.12309](http://arxiv.org/abs/2309.12309)

    演练是一个系统，通过模拟冲突和提供反馈，教授用户冲突解决的技能。利用演练，用户可以练习处理各种冲突场景，并学习如何运用冲突策略。

    

    人际冲突是一种令人不舒服但不可避免的生活事实。成功地处理冲突是一种技能，可以通过刻意练习来学习，但是很少有人能够获得有效的培训或反馈。为了扩大这种机会，我们介绍了演练（Rehearsal）系统，该系统允许用户与可信的模拟对话者一起排练冲突，探索如果情况如何的“假设”场景以识别替代的对话路径，并通过反馈学习何时以及如何应用特定的冲突策略。用户可以使用演练来练习处理各种已定义的冲突场景，从办公室争议到情感问题，或者他们也可以选择创建自己的冲突场景。为了实现演练，我们开发了IRP提示方法，该方法通过冲突解决中具有影响力的利益-权力-能力（IRP）理论来调节大型语言模型的输出。演练使用IRP生成基于冲突解决理论的话语，引导用户实践应用冲突解决策略。

    Interpersonal conflict is an uncomfortable but unavoidable fact of life. Navigating conflict successfully is a skill -- one that can be learned through deliberate practice -- but few have access to effective training or feedback. To expand this access, we introduce Rehearsal, a system that allows users to rehearse conflicts with a believable simulated interlocutor, explore counterfactual "what if?" scenarios to identify alternative conversational paths, and learn through feedback on how and when to apply specific conflict strategies. Users can utilize Rehearsal to practice handling a variety of predefined conflict scenarios, from office disputes to relationship issues, or they can choose to create their own. To enable Rehearsal, we develop IRP prompting, a method of conditioning output of a large language model on the influential Interest-Rights-Power (IRP) theory from conflict resolution. Rehearsal uses IRP to generate utterances grounded in conflict resolution theory, guiding users t
    
[^104]: CoT-BERT: 通过思维链条增强无监督句子表示

    CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought. (arXiv:2309.11143v1 [cs.CL])

    [http://arxiv.org/abs/2309.11143](http://arxiv.org/abs/2309.11143)

    CoT-BERT提出了一种通过思维链条增强无监督句子表示的方法，通过两个阶段的处理，引入思维链条的概念进行向量化，以提高模型性能。

    

    无监督句子表示学习旨在将输入句子转化为富含复杂语义信息的固定长度向量，同时消除对标注数据的依赖。近年来，在对比学习和提示工程的推动下，该领域取得了显著进展，极大地缩小了无监督和有监督策略之间的差距。然而，在这个轨迹中，仍然没有充分利用思维链条的潜在能力。为了释放预训练模型（如BERT）中的潜能，我们提出了一个句子表示的两阶段方法：理解和摘要。随后，后一阶段的输出被利用为输入句子的向量化表示。为了进一步提高性能，我们对对比学习损失函数和模板去噪技术进行了精细调整。严格的实验验证了我们的方法CoT-BERT的优越性。

    Unsupervised sentence representation learning aims to transform input sentences into fixed-length vectors enriched with intricate semantic information while obviating the reliance on labeled data. Recent progress within this field, propelled by contrastive learning and prompt engineering, has significantly bridged the gap between unsupervised and supervised strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains largely untapped within this trajectory. To unlock latent capabilities within pre-trained models, such as BERT, we propose a two-stage approach for sentence representation: comprehension and summarization. Subsequently, the output of the latter phase is harnessed as the vectorized representation of the input sentence. For further performance enhancement, we meticulously refine both the contrastive learning loss function and the template denoising technique for prompt engineering. Rigorous experimentation substantiates our method, CoT-BERT, transcending a
    
[^105]: 重新阅读改善语言模型的推理能力

    Re-Reading Improves Reasoning in Language Models. (arXiv:2309.06275v1 [cs.CL])

    [http://arxiv.org/abs/2309.06275](http://arxiv.org/abs/2309.06275)

    许多研究关注于如何引导和结构化大型语言模型的推理过程，但很少有研究关注于输入问题本身。本研究引入了一种称为“重新阅读”的提示策略，通过深入阅读输入提示中的问题信息，提供了更深入的洞察、更准确的模式识别和更有效的推理能力。

    

    推理对于大型语言模型（LLM）是一个重要而具有挑战性的问题。目前的研究主要集中在开发多样化的提示策略，以引导和结构化LLM的推理过程。然而，这些基于仅解码的因果语言模型的方法通常在单个前向传递中操作输入问题，可能会忽略人类推理中丰富的前后交互。对于嵌入在提示中的输入问题这一关键维度，目前关注较少。为此，我们引入了一种简单但高效的提示策略，称为“重新阅读”。从人类学习和问题解决中汲取灵感，重新阅读意味着重访嵌在输入提示中的问题信息。这种方法与认知增强的原则完美契合，使LLM能够深入洞察、识别复杂的模式、建立 mor

    Reasoning presents a significant and challenging issue for Large Language Models (LLMs). The predominant focus of research has revolved around developing diverse prompting strategies to guide and structure the reasoning processes of LLMs. However, these approaches based on decoder-only causal language models often operate the input question in a single forward pass, potentially missing the rich, back-and-forth interactions inherent in human reasoning. Scant attention has been paid to a critical dimension, i.e., the input question itself embedded within the prompts. In response, we introduce a deceptively simple yet highly effective prompting strategy, termed question "re-reading". Drawing inspiration from human learning and problem-solving, re-reading entails revisiting the question information embedded within input prompts. This approach aligns seamlessly with the cognitive principle of reinforcement, enabling LLMs to extract deeper insights, identify intricate patterns, establish mor
    
[^106]: 内存注入：在Transformer-Based语言模型中纠正多跳推理错误

    Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models. (arXiv:2309.05605v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05605](http://arxiv.org/abs/2309.05605)

    本文提出了一种通过向Transformer-Based语言模型的LLM注意力头部定向注入内存来纠正多跳推理错误的方法，从而提高了模型在处理多跳推理问题时的表现。

    

    回答多跳推理问题需要从多个信息源中检索和综合信息。大语言模型(LLMs)往往难以保持一致的推理能力。本文提出了一种通过在LLM注意力头部进行定向内存注入来确定和纠正多跳推理错误的方法。首先，我们分析了GPT-2模型在单跳和多跳提示下各层的激活情况。然后，我们提出了一种机制，允许用户在推理过程中向关键LLM位置注入相关的提示特定信息，我们将其称为“记忆”。通过在推理过程中使LLM能够整合额外的相关信息，我们提高了多跳提示生成的质量。我们实证表明，将简单、高效且定向的记忆注入到关键注意力层中往往能够提高多跳任务中所需下一个标记的概率，提高了达到424%。

    Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as "memories," at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.
    
[^107]: 复杂性与对齐之间固有关系的初步研究

    A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment. (arXiv:2308.05696v1 [cs.CL])

    [http://arxiv.org/abs/2308.05696](http://arxiv.org/abs/2308.05696)

    本研究初步探讨复杂性与对齐之间的固有关系。通过使用"tree-instruct"方法来增强指令数据的复杂性，可以在对齐到任务和用户偏好方面提供更好的性能。

    

    使用开放域指令数据培训大型语言模型(LLMs)在对齐到最终任务和用户偏好方面取得了显著成功。大量研究表明，提高指令数据的质量和多样性始终能够改善性能。然而，作为一个关键指标，数据复杂性的影响尚未被充分探索，主要包括三个方面：(1)扩展规律，性能改进在复杂性增加时的可持续性尚不确定，(2)额外的标记，复杂性带来的改进是否来自引入更多训练标记，以及(3)课程设置，将从简单到困难的指令纳入是否具有潜在优势也尚未完全理解。在本文中，我们提出了"tree-instruct"，以可控的方式系统地增强指令数据的复杂性。这种方法将指定数量的节点添加到指令语义树中，从而产生新的指令数据。

    Training large language models (LLMs) with open-domain instruction data has yielded remarkable success in aligning to end tasks and user preferences. Extensive research has highlighted that enhancing the quality and diversity of instruction data consistently improves performance. However, the impact of data complexity, as a crucial metric, remains relatively unexplored in three aspects: (1) scaling law, where the sustainability of performance improvements with increasing complexity is uncertain, (2) additional tokens, whether the improvement brought by complexity comes from introducing more training tokens, and (3) curriculum tuning, where the potential advantages of incorporating instructions ranging from easy to difficult are not yet fully understood. In this paper, we propose \textit{tree-instruct} to systematically enhance the complexity of instruction data in a controllable manner. This approach adds a specified number of nodes into the instruction semantic tree, yielding new inst
    
[^108]: 大型语言模型的私密水印

    A Private Watermark for Large Language Models. (arXiv:2307.16230v1 [cs.CL])

    [http://arxiv.org/abs/2307.16230](http://arxiv.org/abs/2307.16230)

    这项工作提出了一种私密水印算法，通过使用两个不同的神经网络进行水印生成和检测，并共享部分参数，实现了高效且高准确性的检测，同时对生成和检测速度影响最小。

    

    最近，针对大型语言模型（LLMs）的文本水印算法已经减轻了LLMs生成的文本可能带来的伪新闻和版权问题。然而，当前文本水印算法的水印检测需要生成过程的密钥，使其容易受到违规和伪造的影响。在这项工作中，我们提出了第一个私密水印算法，通过在水印生成和检测阶段使用两个不同的神经网络而不是使用相同的密钥来扩展当前的文本水印算法。同时，水印生成和检测网络的部分参数是共享的，这使得检测网络能够以高效的方式实现高准确性。实验证明，由于两个网络的参数规模较小，我们的算法确保了高的检测准确性，并对生成和检测速度的影响最小。

    Recently, text watermarking algorithms for large language models (LLMs) have been mitigating the potential harms of text generated by the LLMs, including fake news and copyright issues. However, the watermark detection of current text algorithms requires the key from the generation process, making them susceptible to breaches and counterfeiting. In this work, we propose the first private watermarking algorithm, which extends the current text watermarking algorithms by using two different neural networks respectively for watermark generation and detection, rather than using the same key at both stages. Meanwhile, part of the parameters of the watermark generation and detection networks are shared, which makes the detection network achieve a high accuracy very efficiently. Experiments show that our algorithm ensures high detection accuracy with minimal impact on generation and detection speed, due to the small parameter size of both networks. Additionally, our subsequent analysis demonst
    
[^109]: "感觉像有第二个思维": 探究在大型语言模型中进行创意可写性预写的人机共创

    "It Felt Like Having a Second Mind": Investigating Human-AI Co-creativity in Prewriting with Large Language Models. (arXiv:2307.10811v1 [cs.HC])

    [http://arxiv.org/abs/2307.10811](http://arxiv.org/abs/2307.10811)

    通过三节次的定性研究，探究了人类与大型语言模型在预写过程中的合作模式，并发现了一个三阶段的人机共创过程：构思、启发和实施。在这个合作过程中，人类扮演着主导角色。

    

    预写是在第一稿之前发现和发展思想的过程，它需要发散性思维，通常涉及到无结构的策略，如图表、概述和自由写作等。虽然已经证明大型语言模型（LLMs）在各种任务中都是有用的，包括创意写作，但对用户如何与LLMs合作来支持预写的方式知之甚少。在这种创造性过程中，LLMs的首选合作角色和主动性也不明确。为了研究人类与LLMs在预写过程中的合作模式和动力学，我们进行了一项三节次的定性研究，与15位参与者进行了两个创造性任务：写故事和写口号。研究结果表明，在合作的预写过程中，似乎存在着一个三阶段迭代的人机共创过程，包括构思、启发和实施阶段。这个合作过程以人类在主导角色中取得了成功。

    Prewriting is the process of discovering and developing ideas before a first draft, which requires divergent thinking and often implies unstructured strategies such as diagramming, outlining, free-writing, etc. Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting. The preferred collaborative role and initiative of LLMs during such a creativity process is also unclear. To investigate human-LLM collaboration patterns and dynamics during prewriting, we conducted a three-session qualitative study with 15 participants in two creative tasks: story writing and slogan writing. The findings indicated that during collaborative prewriting, there appears to be a three-stage iterative Human-AI Co-creativity process that includes Ideation, Illumination, and Implementation stages. This collaborative process champions the human in a dominant role, in
    
[^110]: 用语言分类探究单语BERT的语言属性

    Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages. (arXiv:2305.02215v1 [cs.CL])

    [http://arxiv.org/abs/2305.02215](http://arxiv.org/abs/2305.02215)

    本文研究使用语言分类方法探究单语BERT的语言属性，核心发现为BERT正在复制传统的语言模型。

    

    Transformer模型的巨大成功使人们产生了一个问题：这些机器是在复制某些传统的语言模型，还是发现了根本性的新理论？在本文中，我们提出了一种新的研究观点，使用语言之间的类型相似性来对比不同语言的transformer模型，观察这些相似性是否出现在特定的层次。为了进行这项研究，我们提出了基于中心核对齐的权重矩阵相似度测量方法。我们发现，句法类型学相似性与中间层权重之间的相似性是一致的。这一发现确认了通过句法探针方法获得的BERT的结果，并因此重要地证明了BERT正在复制传统的语言模型。

    The overwhelming success of transformers is a real conundrum stimulating a compelling question: are these machines replicating some traditional linguistic models or discovering radically new theories? In this paper, we propose a novel standpoint to investigate this important question. Using typological similarities among languages, we aim to layer-wise compare transformers for different languages to observe whether these similarities emerge for particular layers. For this investigation, we propose to use Centered kernel alignment to measure similarity among weight matrices. We discovered that syntactic typological similarity is consistent with the similarity among weights in the middle layers. This finding confirms results obtained by syntactically probing BERT and, thus, gives an important confirmation that BERT is replicating traditional linguistic models.
    
[^111]: GPT-3是否展示出精神病态？从心理学角度评估大型语言模型

    Does GPT-3 Demonstrate Psychopathy? Evaluating Large Language Models from a Psychological Perspective. (arXiv:2212.10529v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10529](http://arxiv.org/abs/2212.10529)

    本文从心理学角度评估大型语言模型的安全性，发现所有模型在短暗三合一测验上的得分都高于人类平均水平，存在相对较暗的人格模式。尽管经过指标微调，两种模型仍呈现隐含的黑暗人格模式。同时，本文观察到GPT-3和InstructGPT的幸福感得分持续增加。

    

    本文旨在从心理学角度确定大型语言模型（LLMs）的安全性。我们设计了无偏的提示来系统性地评估LLMs。首先，我们使用了两个人格测试——短暗三合一测验（SD-3）和大五人格问卷（BFI）测试了三个不同的LLMs。所有模型在SD-3上的得分都高于人类平均水平，表明存在相对较暗的人格模式。尽管经过指标微调以减少毒性，InstructGPT和FLAN-T5仍然呈现出隐含的黑暗人格模式；在SD-3的玛基雅维利主义和自恋狂特征上，这两种模型的得分都高于自监督GPT-3。然后，我们使用幸福感测试评估了GPT-3系列中的LLMs，以研究更多训练数据的微调对其影响。我们观察到GPT-3和InstructGPT的幸福感得分持续增加。鉴于这些观察结果，我们展示了使用正面回答从而指标微调FLAN-T5的方法。

    In this work, we determined whether large language models (LLMs) are psychologically safe. We designed unbiased prompts to systematically evaluate LLMs from a psychological perspective. First, we tested three different LLMs by using two personality tests: Short Dark Triad (SD-3) and Big Five Inventory (BFI). All models scored higher than the human average on SD-3, suggesting a relatively darker personality pattern. Despite being instruction fine-tuned with safety metrics to reduce toxicity, InstructGPT and FLAN-T5 still showed implicit dark personality patterns; both models scored higher than self-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3. Then, we evaluated the LLMs in the GPT-3 series by using well-being tests to study the impact of fine-tuning with more training data. We observed a continuous increase in the well-being scores of GPT-3 and InstructGPT. Following these observations, we showed that instruction fine-tuning FLAN-T5 with positive answers from 
    

