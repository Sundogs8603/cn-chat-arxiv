# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Covidia: COVID-19 Interdisciplinary Academic Knowledge Graph.](http://arxiv.org/abs/2304.07242) | Covidia是一种COVID-19跨学科学术知识图谱，通过有效的论文分类和高效的跨领域知识提取和整合，弥合了不同领域对COVID-19知识的差距。 |
| [^2] | [Optimal inference of a generalised Potts model by single-layer transformers with factored attention.](http://arxiv.org/abs/2304.07235) | 我们将分析和数值推导结合，在基于广义 Potts 模型的数据上，对经过改进适应这种模型的self-attention机制进行训练，发现经过修改的self-attention机制可以在极限采样下准确学习Potts模型。这个“分解”注意力机制通过从数据中学习相关属性，可以提高Transformer的性能和可解释性。 |
| [^3] | [Just Tell Me: Prompt Engineering in Business Process Management.](http://arxiv.org/abs/2304.07183) | 该论文探讨了提示工程方法在业务流程管理中的应用。这种方法可以利用预训练的语言模型解决微调需要大量数据的问题，并为BPM研究带来诸多潜力。 |
| [^4] | [OPI at SemEval 2023 Task 9: A Simple But Effective Approach to Multilingual Tweet Intimacy Analysis.](http://arxiv.org/abs/2304.07130) | 本文提出了针对SemEval 2023任务9的高效方法，用于多语言推文亲密度分析。该方法利用领域内预训练和伪标记示例扩展训练集，以训练回归模型集合，最终在十种语言中排名前五。 |
| [^5] | [OPI at SemEval 2023 Task 1: Image-Text Embeddings and Multimodal Information Retrieval for Visual Word Sense Disambiguation.](http://arxiv.org/abs/2304.07127) | 本文介绍了我们用于SemEval 2023视觉词义消岐共享任务的系统，其中集成了多模态嵌入、学习排序方法和基于知识的方法。我们的解决方案在多语言任务中排名第三，在波斯语跟踪中获胜。 |
| [^6] | [Keeping the Questions Conversational: Using Structured Representations to Resolve Dependency in Conversational Question Answering.](http://arxiv.org/abs/2304.07125) | 该论文提出了一个名为CONVSR的框架，可以在不影响对话性质的同时，使用结构化表示解决对话式问答中的依赖关系问题。 |
| [^7] | [Task-oriented Document-Grounded Dialog Systems by HLTPR@RWTH for DSTC9 and DSTC10.](http://arxiv.org/abs/2304.07101) | 本论文总结了HLTPR@RWTH团队在DSTC9和DSTC10中为任务导向型文档对话系统所做的贡献，包括提出了不同的方法来使选择任务更有效率，在DSTC10中提出了数据增强技术来提高模型的鲁棒性并适应生成回答的风格，以及提出了一个嘈杂的通道模型来直接建模语音识别错误。实验结果表明，该团队的方法显著优于基线模型。 |
| [^8] | [SEA: A Scalable Entity Alignment System.](http://arxiv.org/abs/2304.07065) | 提出了一个可扩展的实体对齐系统SEA，它包括了六个最先进的EA模型并能够使用户轻松建立、评估自己的模型，提高了基于GNN的EA模型在实际应用中的可用性和效率。 |
| [^9] | [Label Dependencies-aware Set Prediction Networks for Multi-label Text Classification.](http://arxiv.org/abs/2304.07022) | 本论文提出了一种标签相依感知集合预测网络用于解决多标签文本分类问题。该方法将多标签分类视为直接集合预测问题，通过标签之间的统计关系构建关联矩阵并结合GCN学习标签信息，同时利用句子信息和标签信息，最终结果表明其性能优于以前的方法。 |
| [^10] | [Dialogue Games for Benchmarking Language Understanding: Motivation, Taxonomy, Strategy.](http://arxiv.org/abs/2304.07007) | 本论文提出通过构建不同类型的对话游戏进行实践性语言使用的测试，以评价计算机程序的“人工语言理解”能力，从而补充形式化语言理解测试的不足之处。 |
| [^11] | [SimpLex: a lexical text simplification architecture.](http://arxiv.org/abs/2304.07002) | SimpLex是一种用于生成简化英文句子的新型简化架构，它使用词嵌入或句子转换器来生成简化句子并集成到易用的软件中。实验结果中发现，变压器模型在SARI得分方面表现优异，而基于词嵌入的模型则在困惑度方面表现更好。 |
| [^12] | [HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge.](http://arxiv.org/abs/2304.06975) | HuaTuo 是一种融合中医知识进行监督微调的 LLaMA 模型，可以更可靠地回答生物医学领域的问题。 |
| [^13] | [Prompt Engineering and Calibration for Zero-Shot Commonsense Reasoning.](http://arxiv.org/abs/2304.06962) | 本文研究并评估提示工程和校准策略对于小型语言模型在五个常识推理基准上的表现，发现每种策略倾向于某些模型，但联合效果为负。 |
| [^14] | [Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text.](http://arxiv.org/abs/2304.06939) | Multimodal C4是一个开放的、以图像与文本交替形式存在的数据库，其使用线性分配算法将图像放到长文本段落中，可用于通过少量样本学习和复杂相关度提示的建模。 |
| [^15] | [HCAM -- Hierarchical Cross Attention Model for Multi-modal Emotion Recognition.](http://arxiv.org/abs/2304.06910) | 该论文提出了一种分层交叉注意模型（HCAM）用于多模态情感识别，使用递归和共同注意神经网络模型进行音频和文本表示，将这两种模态信息以共同注意方式结合，取得了比现有方法更好的情感识别效果。 |
| [^16] | [Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales.](http://arxiv.org/abs/2304.06875) | 提出了一种新的方法muP，可以提高超参数的缩放律的拟合精度，减少对大模型超参数的搜索，从而实现在大规模模型上进行损失预测。 |
| [^17] | [Evaluation of Social Biases in Recent Large Pre-Trained Models.](http://arxiv.org/abs/2304.06861) | 本文研究了最近发布的三个预训练模型的偏见问题，并评估了它们在两个偏见基准上的表现，探讨了是否随着技术进步，最新的、更快、更轻的模型在开发时负责任地降低了与旧模型相比的社会偏见。 |
| [^18] | [Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter.](http://arxiv.org/abs/2304.06858) | 本文介绍了一个推特疫苗数据集Vax-Culture，它旨在找出推广疫苗错误信息的文化和政治信念的重叠部分，帮助开发机器学习模型以自动检测疫苗错误信息帖子并应对其负面影响。 |
| [^19] | [SemEval-2023 Task 12: Sentiment Analysis for African Languages (AfriSenti-SemEval).](http://arxiv.org/abs/2304.06845) | SemEval-2023举办了非洲语言情感分析挑战赛（AfriSenti-SemEval），旨在提供非洲语言的情感分类数据集。该挑战赛包括单语、多语言和零样本分类三个子任务，并吸引了众多研究者的参与。 |
| [^20] | [On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence.](http://arxiv.org/abs/2304.06798) | 本文研究了在地理空间AI中开发基础模型的机遇和挑战，测试了多种FMs在地理子领域中的表现，发现在文本任务上的表现优于任务特定的定制模型，但在发展中也面临着缺少数据集和需要专业技术微调的挑战。 |
| [^21] | [Efficient Sequence Transduction by Jointly Predicting Tokens and Durations.](http://arxiv.org/abs/2304.06795) | 本文提出了一种新型的序列转导架构TDT，它可以联合预测标记和持续时间，从而实现比传统Transducers更高的准确性和显着更快的推理速度。 |
| [^22] | [RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment.](http://arxiv.org/abs/2304.06767) | RAFT框架引入了奖励排名微调方法，用于对齐生成型基础模型，以解决强化学习带来的低效和不稳定性问题。 |
| [^23] | [Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation.](http://arxiv.org/abs/2304.06671) | 本文提出了布局引导下图像生成的诊断基准LayoutBench，对数量、位置、大小和形状四种空间控制技能进行了研究，发现好的ID布局控制在任意布局的野外环境下可能不具有良好的推广性。接着，我们提出了一种新的基准方法IterInpaint通过修复逐步生成前景和背景区域，显现出在OOD布局方面更强的通用性。 |
| [^24] | [G2T: A simple but versatile framework for topic modeling based on pretrained language model and community detection.](http://arxiv.org/abs/2304.06653) | G2T是一种基于预训练语言模型和社区检测的主题建模框架，自动评估表明，G2T在多个数据集上均与当前最先进的方法相比表现更好。 |
| [^25] | [Exploring the State of the Art in Legal QA Systems.](http://arxiv.org/abs/2304.06623) | 法律问题回答系统的研究面临着复杂性和多样性等挑战，但其在客户服务、教育、研究和跨语言交流等方面具有广泛应用。 |
| [^26] | [PDF-VQA: A New Dataset for Real-World VQA on PDF Documents.](http://arxiv.org/abs/2304.06447) | 该研究提出了一个新的文档VQA数据集PDF-VQA，以多个页面的完整文档作为研究对象，通过机器学习模型识别与处理文档元素、结构和内容等方面，为解决真实世界中的文档理解问题提供新的资源。 |
| [^27] | [Sign Language Translation from Instructional Videos.](http://arxiv.org/abs/2304.06371) | 本论文描述了如何使用I3D视频特征培训Transformer模型，据此对How2Sign数据集进行手语翻译。作者提供了公共代码和首个开源实现。 |
| [^28] | [Reinforcement Learning Tutor Better Supported Lower Performers in a Math Task.](http://arxiv.org/abs/2304.04933) | 本文证明了深度强化学习可用于提供自适应的教育支持，尤其对于最初成绩较低的学生具有最大的益处。 |
| [^29] | [Geotechnical Parrot Tales (GPT): Overcoming GPT hallucinations with prompt engineering for geotechnical applications.](http://arxiv.org/abs/2304.02138) | 本文探讨了如何利用GPT在岩土工程应用中的全部潜力，着重讨论及时工程的重要性，并开发了一个统一的自然语言接口，用于处理复杂的岩土工程任务和数据分析。 |
| [^30] | [WebQAmGaze: A Multilingual Webcam Eye-Tracking-While-Reading Dataset.](http://arxiv.org/abs/2303.17876) | WebQAmGaze是一个多语言低成本的阅读时眼动追踪数据集，包括332位参与者的数据，对相关段落的注视似乎能够反映回答理解问题的准确性。这份数据可以推动基于网络摄像头的阅读研究并开辟更便宜、更易获得的数据收集方式。 |
| [^31] | [Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning.](http://arxiv.org/abs/2303.10475) | 传统的自然语言处理机器学习需要大规模的任务特定示例，但这不适用于任务可能过于复杂或成本过高以进行注释的场景。因此，社区对于自然语言处理中新的监督寻求范式--从任务指令学习--越来越感兴趣。 |
| [^32] | [CoLT5: Faster Long-Range Transformers with Conditional Computation.](http://arxiv.org/abs/2303.09752) | CoLT5是一种基于条件计算的Transformer模型，通过优先处理重要标记来加速长距离输入的处理。CoLT5在SCROLLS基准测试上表现最好，并能够有效地处理长达64k输入长度。 |
| [^33] | [Inseq: An Interpretability Toolkit for Sequence Generation Models.](http://arxiv.org/abs/2302.13942) | 本文介绍了Inseq，这是一个Python工具包，旨在推广可解释性序列生成模型的分析。它为常见的解码器和编码器-解码器Transformers架构提供了提取模型内部信息和特征重要性得分的直观优化方法。作者还在机器翻译模型和GPT-2中展示了Inseq的潜力，证明其有助于推动可解释性自然语言生成的未来发展。 |
| [^34] | [Alloprof: a new French question-answer education dataset and its use in an information retrieval case study.](http://arxiv.org/abs/2302.07738) | 这个论文介绍了一个新的阿洛普夫法语问答数据集，收集了来自10,368名学生的29,349个问题和解释，并展示了在信息检索任务中使用该数据集的案例研究。 |
| [^35] | [EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata.](http://arxiv.org/abs/2301.04647) | 本文通过学习图像和相机元数据之间的交叉模态关联提取相机信息，并使用得到的特征成功实现拼接图像区域的"零样本"定位。 |
| [^36] | [Using Active Learning Methods to Strategically Select Essays for Automated Scoring.](http://arxiv.org/abs/2301.00628) | 本研究提出了三种主动学习方法，可最小化必须由人工评分者进行评分的文章数量，同时提供训练现代自动化论文打分系统所需的数据。 |
| [^37] | [Leveraging Natural Language Processing to Augment Structured Social Determinants of Health Data in the Electronic Health Record.](http://arxiv.org/abs/2212.07538) | 本文开发了一个SDOH信息提取器，通过将其应用于EHR中的临床叙述，捕获详细的SDOH信息，并将提取的表示与现有的结构化数据进行组合以获得信息增益。 |
| [^38] | [AUC Maximization for Low-Resource Named Entity Recognition.](http://arxiv.org/abs/2212.04800) | 本文提出了在低资源命名实体识别中使用AUC最大化的方法，通过结合两个最大化AUC分数的二进制分类器，在低资源NER设置下实现了显着的性能提高，优于传统的损失函数。 |
| [^39] | [Make More of Your Data: Minimal Effort Data Augmentation for Automatic Speech Recognition and Translation.](http://arxiv.org/abs/2210.15398) | 本文研究了一种简单、经济的数据扩充方法，即将原始数据样本串联以构建新的训练实例。使用这种方法继续训练能够改进Transformer和Conformer模型，并在多种任务中实现了长达0.9 WER的改进。 |
| [^40] | [End-to-End Entity Detection with Proposer and Regressor.](http://arxiv.org/abs/2210.10260) | 该论文提出了一种基于提议器和回归器的端到端实体检测方法，通过利用特征金字塔网络生成高质量的实体提议，并对提议进行精细调整以生成最终的预测结果。该模型具有查询语义丰富、实体定位精度高、模型训练容易等优点，还引入了空间调制变压器来增强内部关系的建模能力。实验结果表明，该方法显著优于现有的最先进方法。 |
| [^41] | [Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion.](http://arxiv.org/abs/2210.08471) | 本文提出了一种依赖增强的自适应融合注意力模型，它将依赖信息与原始语义信号自适应融合，以更好地模拟复杂的语义匹配关系。 |
| [^42] | [StyLEx: Explaining Style Using Human Lexical Annotations.](http://arxiv.org/abs/2210.07469) | StyLEx是一种可以从人工注释的语言风格特征解释中学习，提供句子级风格预测以及人类-like的风格词汇解释的模型。 |
| [^43] | [DABERT: Dual Attention Enhanced BERT for Semantic Matching.](http://arxiv.org/abs/2210.03454) | DABERT 通过双重注意力机制和自适应融合模块增强了 BERT 在捕捉句子对之间细微差异的能力，并在实验中取得了良好的效果。 |
| [^44] | [UniCausal: Unified Benchmark and Repository for Causal Text Mining.](http://arxiv.org/abs/2208.09163) | UniCausal是一个跨三个任务的因果关系文本挖掘统一基准，整合了六个高质量的语料库的注释。UniCausal可用于评估现有模型能力，并鼓励开发新的因果关系文本挖掘方法和框架。 |

# 详细

[^1]: Covidia: COVID-19 跨学科学术知识图谱

    Covidia: COVID-19 Interdisciplinary Academic Knowledge Graph. (arXiv:2304.07242v1 [cs.IR])

    [http://arxiv.org/abs/2304.07242](http://arxiv.org/abs/2304.07242)

    Covidia是一种COVID-19跨学科学术知识图谱，通过有效的论文分类和高效的跨领域知识提取和整合，弥合了不同领域对COVID-19知识的差距。

    

    COVID-19 疫情激发了不同领域广泛的研究工作。现有的COVID-19文献和知识平台只关注生物学和医学领域的论文收集，忽略了跨学科的努力，这妨碍了领域间的知识共享和研究合作以解决问题。研究跨学科研究需要有效的论文分类和高效的跨领域知识提取和整合。在这项工作中，我们提出Covidia，COVID-19跨学科学术知识图谱，以弥合不同领域对COVID-19知识的差距。我们基于对比学习设计框架进行学科分类，并提出了一种新的学术知识图谱方案，用于交叉领域的实体提取、关系分类和本体论管理。基于Covidia，我们还建立了发现COVID-19研究的知识发现基准。

    The pandemic of COVID-19 has inspired extensive works across different research fields. Existing literature and knowledge platforms on COVID-19 only focus on collecting papers on biology and medicine, neglecting the interdisciplinary efforts, which hurdles knowledge sharing and research collaborations between fields to address the problem. Studying interdisciplinary researches requires effective paper category classification and efficient cross-domain knowledge extraction and integration. In this work, we propose Covidia, COVID-19 interdisciplinary academic knowledge graph to bridge the gap between knowledge of COVID-19 on different domains. We design frameworks based on contrastive learning for disciplinary classification, and propose a new academic knowledge graph scheme for entity extraction, relation classification and ontology management in accordance with interdisciplinary researches. Based on Covidia, we also establish knowledge discovery benchmarks for finding COVID-19 research
    
[^2]: 利用分解注意力机制的单层Transformer对广义Potts模型进行最优推断

    Optimal inference of a generalised Potts model by single-layer transformers with factored attention. (arXiv:2304.07235v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2304.07235](http://arxiv.org/abs/2304.07235)

    我们将分析和数值推导结合，在基于广义 Potts 模型的数据上，对经过改进适应这种模型的self-attention机制进行训练，发现经过修改的self-attention机制可以在极限采样下准确学习Potts模型。这个“分解”注意力机制通过从数据中学习相关属性，可以提高Transformer的性能和可解释性。

    

    Transformer 是一种革命性的神经网络，在自然语言处理和蛋白质科学方面取得了实践上的成功。它们的关键构建块是一个叫做自注意力机制的机制，它被训练用于预测句子中缺失的词。尽管Transformer在应用中取得了实践上的成功，但是自注意力机制究竟从数据中学到了什么以及它是怎么做到的还不是很清楚。本文针对从具有相互作用的位置和 Potts 颜色中提取的数据在训练的Transformer上给出了精确的分析和数值刻画。我们证明，虽然一般的transformer需要多层学习才能准确学习这个分布，但是经过小改进的自注意力机制在无限采样的极限下可以完美地学习Potts模型。我们还计算了这个修改后的自注意力机制所谓“分解”的泛化误差，并在合成数据上数值演示了我们的发现。我们的结果为解释Transformer的内在工作原理以及提高其性能和可解释性提供了新的思路。

    Transformers are the type of neural networks that has revolutionised natural language processing and protein science. Their key building block is a mechanism called self-attention which is trained to predict missing words in sentences. Despite the practical success of transformers in applications it remains unclear what self-attention learns from data, and how. Here, we give a precise analytical and numerical characterisation of transformers trained on data drawn from a generalised Potts model with interactions between sites and Potts colours. While an off-the-shelf transformer requires several layers to learn this distribution, we show analytically that a single layer of self-attention with a small modification can learn the Potts model exactly in the limit of infinite sampling. We show that this modified self-attention, that we call ``factored'', has the same functional form as the conditional probability of a Potts spin given the other spins, compute its generalisation error using t
    
[^3]: Just Tell Me: 业务流程管理中的提示工程

    Just Tell Me: Prompt Engineering in Business Process Management. (arXiv:2304.07183v1 [cs.AI])

    [http://arxiv.org/abs/2304.07183](http://arxiv.org/abs/2304.07183)

    该论文探讨了提示工程方法在业务流程管理中的应用。这种方法可以利用预训练的语言模型解决微调需要大量数据的问题，并为BPM研究带来诸多潜力。

    

    GPT-3和其他几个语言模型可以有效地处理各种自然语言处理任务，包括机器翻译和文本摘要。最近，它们也在业务流程管理（BPM）领域成功应用，例如用于预测过程监控和从文本中提取过程。然而，这通常需要对所用语言模型进行微调，其中包括大量合适的训练数据。解决这个问题的一种可能解决方案是使用提示工程，它利用预训练的语言模型，而无需进行微调。认识到这一点，我们认为提示工程可以帮助将语言模型的能力引入BPM研究。本篇文章利用这一观点，通过确定相关的潜力和挑战，为BPM研究的提示工程使用制定研究议程。

    GPT-3 and several other language models (LMs) can effectively address various natural language processing (NLP) tasks, including machine translation and text summarization. Recently, they have also been successfully employed in the business process management (BPM) domain, e.g., for predictive process monitoring and process extraction from text. This, however, typically requires fine-tuning the employed LM, which, among others, necessitates large amounts of suitable training data. A possible solution to this problem is the use of prompt engineering, which leverages pre-trained LMs without fine-tuning them. Recognizing this, we argue that prompt engineering can help bring the capabilities of LMs to BPM research. We use this position paper to develop a research agenda for the use of prompt engineering for BPM research by identifying the associated potentials and challenges.
    
[^4]: SemEval 2023任务9中针对多语言推文亲密度分析的简洁高效方法

    OPI at SemEval 2023 Task 9: A Simple But Effective Approach to Multilingual Tweet Intimacy Analysis. (arXiv:2304.07130v1 [cs.CL])

    [http://arxiv.org/abs/2304.07130](http://arxiv.org/abs/2304.07130)

    本文提出了针对SemEval 2023任务9的高效方法，用于多语言推文亲密度分析。该方法利用领域内预训练和伪标记示例扩展训练集，以训练回归模型集合，最终在十种语言中排名前五。

    

    本文讨论了我们针对SemEval 2023多语言推文亲密度分析任务的提交。任务旨在评估10种语言中Twitter帖子的亲密程度。我们提出的方法包括几个步骤。首先，我们进行领域内预训练，创建适应Twitter数据的语言模型。接下来，我们使用伪标记的示例扩展训练集，训练回归模型集合。扩展的数据集用于训练最终的解决方案。我们的方法在10种语言子任务中排名前五，并获得所有语言中最高的平均得分。

    This paper describes our submission to the SemEval 2023 multilingual tweet intimacy analysis shared task. The goal of the task was to assess the level of intimacy of Twitter posts in ten languages. The proposed approach consists of several steps. First, we perform in-domain pre-training to create a language model adapted to Twitter data. In the next step, we train an ensemble of regression models to expand the training set with pseudo-labeled examples. The extended dataset is used to train the final solution. Our method was ranked first in five out of ten language subtasks, obtaining the highest average score across all languages.
    
[^5]: SemEval 2023任务1中的OPI：基于图像-文本嵌入和多模态信息检索的视觉词义消岐

    OPI at SemEval 2023 Task 1: Image-Text Embeddings and Multimodal Information Retrieval for Visual Word Sense Disambiguation. (arXiv:2304.07127v1 [cs.CL])

    [http://arxiv.org/abs/2304.07127](http://arxiv.org/abs/2304.07127)

    本文介绍了我们用于SemEval 2023视觉词义消岐共享任务的系统，其中集成了多模态嵌入、学习排序方法和基于知识的方法。我们的解决方案在多语言任务中排名第三，在波斯语跟踪中获胜。

    

    视觉词义消岐的目标是找到最符合提供的词义描述的图像，这是一个具有挑战性的问题，需要结合语言和图像理解的方法。本文介绍了我们针对SemEval 2023视觉词义消岐共享任务的提交。所提出的系统集成了多模态嵌入、学习排序方法和基于知识的方法。我们基于CLIP模型构建了一个分类器，其结果使用从维基百科和词汇数据库检索到的附加信息进行增强。我们的解决方案在多语言任务中排名第三，在波斯语跟踪中获胜，是三个语言子任务中的一个。

    The goal of visual word sense disambiguation is to find the image that best matches the provided description of the word's meaning. It is a challenging problem, requiring approaches that combine language and image understanding. In this paper, we present our submission to SemEval 2023 visual word sense disambiguation shared task. The proposed system integrates multimodal embeddings, learning to rank methods, and knowledge-based approaches. We build a classifier based on the CLIP model, whose results are enriched with additional information retrieved from Wikipedia and lexical databases. Our solution was ranked third in the multilingual task and won in the Persian track, one of the three language subtasks.
    
[^6]: 让问题更具对话性：使用结构化表示解决问答中的依赖关系

    Keeping the Questions Conversational: Using Structured Representations to Resolve Dependency in Conversational Question Answering. (arXiv:2304.07125v1 [cs.CL])

    [http://arxiv.org/abs/2304.07125](http://arxiv.org/abs/2304.07125)

    该论文提出了一个名为CONVSR的框架，可以在不影响对话性质的同时，使用结构化表示解决对话式问答中的依赖关系问题。

    

    拥有一个能够参与对话式问答（ConvQA）的智能对话代理现在已不仅仅局限于科幻电影，并且已经成为现实。这些智能代理需要理解和正确解释作为给定问题背景的顺序转换。然而，这些顺序的问题有时会被留下隐含的，因此需要解决一些自然语言现象，例如指代和省略。对问题重写的任务有潜力通过将它们转换成明确意图的问题来解决解决上下文转换之间依赖关系的挑战。然而，重写隐式问题的解决方案存在一些潜在的挑战，例如生成冗长的问题并通过生成自包含问题的方式使对话中的情境削弱了。在本文中，我们提出了一个新的框架CONVSR（使用结构化表示的CONVQA）。

    Having an intelligent dialogue agent that can engage in conversational question answering (ConvQA) is now no longer limited to Sci-Fi movies only and has, in fact, turned into a reality. These intelligent agents are required to understand and correctly interpret the sequential turns provided as the context of the given question. However, these sequential questions are sometimes left implicit and thus require the resolution of some natural language phenomena such as anaphora and ellipsis. The task of question rewriting has the potential to address the challenges of resolving dependencies amongst the contextual turns by transforming them into intent-explicit questions. Nonetheless, the solution of rewriting the implicit questions comes with some potential challenges such as resulting in verbose questions and taking conversational aspect out of the scenario by generating self-contained questions. In this paper, we propose a novel framework, CONVSR (CONVQA using Structured Representations)
    
[^7]: HLTPR@RWTH在DSTC9和DSTC10中的任务导向型文档对话系统

    Task-oriented Document-Grounded Dialog Systems by HLTPR@RWTH for DSTC9 and DSTC10. (arXiv:2304.07101v1 [cs.CL])

    [http://arxiv.org/abs/2304.07101](http://arxiv.org/abs/2304.07101)

    本论文总结了HLTPR@RWTH团队在DSTC9和DSTC10中为任务导向型文档对话系统所做的贡献，包括提出了不同的方法来使选择任务更有效率，在DSTC10中提出了数据增强技术来提高模型的鲁棒性并适应生成回答的风格，以及提出了一个嘈杂的通道模型来直接建模语音识别错误。实验结果表明，该团队的方法显著优于基线模型。

    

    本文总结了我们在第9和第10次Dialog System Technology Challenges（DSTC9和DSTC10）中为对话基于文档的任务作出的贡献。在两次迭代中，任务由三个子任务组成：首先检测当前回合是否需要知识，其次选择相关的知识文档，第三生成基于所选文档的回答。对于DSTC9，我们提出了不同的方法来使选择任务更有效率。其中最好的方法——分层选择，实际上比原始基线改进了结果，并提高了24倍速度。在DSTC10迭代中，挑战是要使经过书面对话训练的系统能够在嘈杂的自动语音识别转录中表现良好。因此，我们提出了数据增强技术来提高模型的鲁棒性，以及适应生成回答的风格，使其与前期对话相匹配的方法。此外，我们提出了一个嘈杂的通道模型来直接建模语音识别错误。实验结果表明，我们的方法在所有子任务上显著优于基线模型。

    This paper summarizes our contributions to the document-grounded dialog tasks at the 9th and 10th Dialog System Technology Challenges (DSTC9 and DSTC10). In both iterations the task consists of three subtasks: first detect whether the current turn is knowledge seeking, second select a relevant knowledge document, and third generate a response grounded on the selected document. For DSTC9 we proposed different approaches to make the selection task more efficient. The best method, Hierarchical Selection, actually improves the results compared to the original baseline and gives a speedup of 24x. In the DSTC10 iteration of the task, the challenge was to adapt systems trained on written dialogs to perform well on noisy automatic speech recognition transcripts. Therefore, we proposed data augmentation techniques to increase the robustness of the models as well as methods to adapt the style of generated responses to fit well into the proceeding dialog. Additionally, we proposed a noisy channel
    
[^8]: SEA: 一个可扩展实体对齐系统

    SEA: A Scalable Entity Alignment System. (arXiv:2304.07065v1 [cs.CL])

    [http://arxiv.org/abs/2304.07065](http://arxiv.org/abs/2304.07065)

    提出了一个可扩展的实体对齐系统SEA，它包括了六个最先进的EA模型并能够使用户轻松建立、评估自己的模型，提高了基于GNN的EA模型在实际应用中的可用性和效率。

    

    实体对齐旨在在不同知识图谱中找到相应的实体。现有的实体对齐方法通常使用图神经网络来编码实体。然而，大多数方法都是在全批量模式下训练模型和评估结果，这使得实体对齐在大规模数据集上无法扩展。为了增强基于图神经网络的实体对齐模型在实际应用中的可用性，我们提出了一个可扩展的实体对齐系统SEA。它能够(i)训练大规模的图神经网络用于实体对齐，(ii)加速归一化和评估过程，(iii)为用户提供清晰的结果以估计不同的模型和参数设置。SEA只需要一个图形卡就可以运行。此外，SEA包括六个最先进的实体对齐模型，并为用户提供快速建立和评估自己模型的方法。因此，SEA允许用户在不涉及复杂实现的情况下执行实体对齐，如负抽样和GPU加速。

    Entity alignment (EA) aims to find equivalent entities in different knowledge graphs (KGs). State-of-the-art EA approaches generally use Graph Neural Networks (GNNs) to encode entities. However, most of them train the models and evaluate the results in a fullbatch fashion, which prohibits EA from being scalable on largescale datasets. To enhance the usability of GNN-based EA models in real-world applications, we present SEA, a scalable entity alignment system that enables to (i) train large-scale GNNs for EA, (ii) speed up the normalization and the evaluation process, and (iii) report clear results for users to estimate different models and parameter settings. SEA can be run on a computer with merely one graphic card. Moreover, SEA encompasses six state-of-the-art EA models and provides access for users to quickly establish and evaluate their own models. Thus, SEA allows users to perform EA without being involved in tedious implementations, such as negative sampling and GPU-accelerated
    
[^9]: 多标签文本分类的标签相依感知集合预测网络

    Label Dependencies-aware Set Prediction Networks for Multi-label Text Classification. (arXiv:2304.07022v1 [cs.CL])

    [http://arxiv.org/abs/2304.07022](http://arxiv.org/abs/2304.07022)

    本论文提出了一种标签相依感知集合预测网络用于解决多标签文本分类问题。该方法将多标签分类视为直接集合预测问题，通过标签之间的统计关系构建关联矩阵并结合GCN学习标签信息，同时利用句子信息和标签信息，最终结果表明其性能优于以前的方法。

    

    多标签文本分类旨在从句子中提取所有相关标签，可视为序列生成问题。然而，训练集中的标签是无序的。我们建议将其视为直接集合预测问题，而不需要考虑标签的顺序。此外，为了建模标签之间的关联，通过标签之间的统计关系构建关联矩阵，并使用GCN来学习标签信息。基于所学的标签信息，集合预测网络可以同时利用句子信息和标签信息进行多标签文本分类。此外，还对集合预测网络的输出概率分布施加广义巴氏距离，以提高其召回率。在四个多标签数据集上的实验结果表明了所提出方法的有效性，并且其性能大大优于以前的方法。

    Multi-label text classification aims to extract all the related labels from a sentence, which can be viewed as a sequence generation problem. However, the labels in training dataset are unordered. We propose to treat it as a direct set prediction problem and don't need to consider the order of labels. Besides, in order to model the correlation between labels, the adjacency matrix is constructed through the statistical relations between labels and GCN is employed to learn the label information. Based on the learned label information, the set prediction networks can both utilize the sentence information and label information for multi-label text classification simultaneously. Furthermore, the Bhattacharyya distance is imposed on the output probability distributions of the set prediction networks to increase the recall ability. Experimental results on four multi-label datasets show the effectiveness of the proposed method and it outperforms previous method a substantial margin.
    
[^10]: 基于对话游戏的语言理解基准测量：动机、分类和策略

    Dialogue Games for Benchmarking Language Understanding: Motivation, Taxonomy, Strategy. (arXiv:2304.07007v1 [cs.CL])

    [http://arxiv.org/abs/2304.07007](http://arxiv.org/abs/2304.07007)

    本论文提出通过构建不同类型的对话游戏进行实践性语言使用的测试，以评价计算机程序的“人工语言理解”能力，从而补充形式化语言理解测试的不足之处。

    

    如何测量“理解语言的能力”？如果是衡量一个人的能力，这几乎从未以不合格的方式提出：无论应用何种正式测试，都是在人们日常社交实践的语言使用背景下进行的，所测量的是一种专业的语言理解（例如第二语言或书面技术语言）。计算机程序没有这样的背景。这对于正式的语言理解测试的适用性意味着什么？我认为这些测试需要补充嵌入在实践中的语言使用测试，以得出更全面的“人工语言理解”评估。为了系统地进行这样的测试，我提议使用“对话游戏”，构建提供情境嵌入的活动。我描述了一种与潜在能力模型相关联的对话游戏类型分类。

    How does one measure "ability to understand language"? If it is a person's ability that is being measured, this is a question that almost never poses itself in an unqualified manner: Whatever formal test is applied, it takes place on the background of the person's language use in daily social practice, and what is measured is a specialised variety of language understanding (e.g., of a second language; or of written, technical language). Computer programs do not have this background. What does that mean for the applicability of formal tests of language understanding? I argue that such tests need to be complemented with tests of language use embedded in a practice, to arrive at a more comprehensive evaluation of "artificial language understanding". To do such tests systematically, I propose to use "Dialogue Games" -- constructed activities that provide a situational embedding for language use. I describe a taxonomy of Dialogue Game types, linked to a model of underlying capabilites that 
    
[^11]: SimpLex：一种词汇文本简化架构

    SimpLex: a lexical text simplification architecture. (arXiv:2304.07002v1 [cs.CL])

    [http://arxiv.org/abs/2304.07002](http://arxiv.org/abs/2304.07002)

    SimpLex是一种用于生成简化英文句子的新型简化架构，它使用词嵌入或句子转换器来生成简化句子并集成到易用的软件中。实验结果中发现，变压器模型在SARI得分方面表现优异，而基于词嵌入的模型则在困惑度方面表现更好。

    

    文本简化是将给定句子或文本生成易于理解的句子的过程。简化的目的是在不损失含义或细微差别的情况下减少给定文本或句子的词汇和语法复杂性。在本文中，我们介绍了SimpLex，一种用于生成简化英文句子的新型简化架构。为了生成简化句子，所提出的架构使用词嵌入（即Word2Vec）和困惑度或句子转换器（即BERT、RoBERTa和GPT2）和余弦相似度之一。该解决方案集成到一个用户友好的、易于使用的软件中。我们使用两个指标（即SARI和困惑度降低）评估了我们的系统。从实验角度来看，我们观察到变压器模型在SARI得分方面优于其他模型。然而，从困惑度方面来看，基于词嵌入的模型表现更好。

    Text simplification (TS) is the process of generating easy-to-understand sentences from a given sentence or piece of text. The aim of TS is to reduce both the lexical (which refers to vocabulary complexity and meaning) and syntactic (which refers to the sentence structure) complexity of a given text or sentence without the loss of meaning or nuance. In this paper, we present \textsc{SimpLex}, a novel simplification architecture for generating simplified English sentences. To generate a simplified sentence, the proposed architecture uses either word embeddings (i.e., Word2Vec) and perplexity, or sentence transformers (i.e., BERT, RoBERTa, and GPT2) and cosine similarity. The solution is incorporated into a user-friendly and simple-to-use software. We evaluate our system using two metrics, i.e., SARI, and Perplexity Decrease. Experimentally, we observe that the transformer models outperform the other models in terms of the SARI score. However, in terms of Perplexity, the Word-Embeddings-
    
[^12]: HuaTuo: 融合中医知识优化 LLaMA 模型

    HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge. (arXiv:2304.06975v1 [cs.CL])

    [http://arxiv.org/abs/2304.06975](http://arxiv.org/abs/2304.06975)

    HuaTuo 是一种融合中医知识进行监督微调的 LLaMA 模型，可以更可靠地回答生物医学领域的问题。

    

    大规模语言模型（LLMs），例如 LLaMA 模型，在各种通用自然语言处理（NLP）任务中已经证明了它们的有效性。然而，由于需要医学专业知识来回答问题，LLMs 在生物医学领域的任务中表现仍不理想。应对这一挑战，我们提出了 HuaTuo，这是一种基于 LLaMA 的模型，它通过生成的 QA（问题-回答）实例进行了监督微调。实验结果表明，HuaTuo 生成的响应具有更可靠的医学知识。我们提出的 HuaTuo 模型可在 https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese 上获得。

    Large Language Models (LLMs), such as the LLaMA model, have demonstrated their effectiveness in various general-domain natural language processing (NLP) tasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain tasks due to the need for medical expertise in the responses. In response to this challenge, we propose HuaTuo, a LLaMA-based model that has been supervised-fine-tuned with generated QA (Question-Answer) instances. The experimental results demonstrate that HuaTuo generates responses that possess more reliable medical knowledge. Our proposed HuaTuo model is accessible at https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese.
    
[^13]: 用于零样本常识推理的提示工程和校准

    Prompt Engineering and Calibration for Zero-Shot Commonsense Reasoning. (arXiv:2304.06962v1 [cs.CL])

    [http://arxiv.org/abs/2304.06962](http://arxiv.org/abs/2304.06962)

    本文研究并评估提示工程和校准策略对于小型语言模型在五个常识推理基准上的表现，发现每种策略倾向于某些模型，但联合效果为负。

    

    提示工程和校准使得大型语言模型在推理任务，包括多项选择常识推理中表现出色。从实际角度出发，我们在较小的语言模型上研究并评估了这些策略。通过对五个常识推理基准的实验，我们发现每种策略都倾向于某些模型，但它们的联合效果大多为负。

    Prompt engineering and calibration make large language models excel at reasoning tasks, including multiple choice commonsense reasoning. From a practical perspective, we investigate and evaluate these strategies on smaller language models. Through experiments on five commonsense reasoning benchmarks, we find that each strategy favors certain models, but their joint effects are mostly negative.
    
[^14]: 多模态C4：一种包含大量图像和文本的开放式数据库

    Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text. (arXiv:2304.06939v1 [cs.CV])

    [http://arxiv.org/abs/2304.06939](http://arxiv.org/abs/2304.06939)

    Multimodal C4是一个开放的、以图像与文本交替形式存在的数据库，其使用线性分配算法将图像放到长文本段落中，可用于通过少量样本学习和复杂相关度提示的建模。

    

    上下文视觉和语言模型需要支持任意交替的图像和文本序列作为输入, 这种格式不仅可以通过交替独立监督的(图像,文本)示例来进行低次学习,而且可以应对更复杂的提示, 涉及图像间互动,例如“图像A和图像B有什么共同之处?”现有的预训练模型使用类似于交替图像+文本的web语料库。但是，迄今为止，这种形式的大规模数据还没有公开提供。我们发布了Multimodal C4 (mmc4)，这是一个加强版的c4文本库，其中插入了图像。我们使用一个线性分配算法，使用CLIP特征将图像放到更长的文本体中，此过程优于其他替代方案。mmc4涵盖了诸如烹饪，旅游，技术等日常主题。对随机样本的手动检查表明，绝大多数(90%)的图像与主题相关。

    In-context vision and language models like Flamingo support arbitrarily interleaved sequences of images and text as input. This format not only enables few-shot learning via interleaving independent supervised (image, text) examples, but also, more complex prompts involving interaction between images, e.g., "What do image A and image B have in common?" To support this interface, pretraining occurs over web corpora that similarly contain interleaved images+text. To date, however, large-scale data of this form have not been publicly available.  We release Multimodal C4 (mmc4), an augmentation of the popular text-only c4 corpus with images interleaved. We use a linear assignment algorithm to place images into longer bodies of text using CLIP features, a process that we show outperforms alternatives. mmc4 spans everyday topics like cooking, travel, technology, etc. A manual inspection of a random sample of documents shows that a vast majority (90%) of images are topically relevant, and tha
    
[^15]: HCAM--多模态情感识别的分层交叉注意模型

    HCAM -- Hierarchical Cross Attention Model for Multi-modal Emotion Recognition. (arXiv:2304.06910v1 [eess.AS])

    [http://arxiv.org/abs/2304.06910](http://arxiv.org/abs/2304.06910)

    该论文提出了一种分层交叉注意模型（HCAM）用于多模态情感识别，使用递归和共同注意神经网络模型进行音频和文本表示，将这两种模态信息以共同注意方式结合，取得了比现有方法更好的情感识别效果。

    

    对话情感识别由于情感表达的多模态性而具有挑战性。我们提出了一种采用递归和共同注意神经网络模型的分层交叉注意模型（HCAM）方法用于多模态情感识别。模型的输入包括两种模态，即通过可学习wav2vec方法处理的音频数据和使用双向编码器来自变压器（BERT）模型表示的文本数据。音频和文本表示使用一组双向递归神经网络层进行处理，使用自注意将给定对话中的每个话语转换为固定维度的嵌入。为了整合上下文知识和两种模态的信息，使用共同注意层将音频和文本嵌入进行组合，试图衡量与情感识别任务相关的话语级嵌入。在CMU-MOSI数据集上训练和评估神经网络模型，这是一个大规模的多模态会话数据集。实验结果表明，所提出的HCAM方法优于现有的情感识别最先进方法。

    Emotion recognition in conversations is challenging due to the multi-modal nature of the emotion expression. We propose a hierarchical cross-attention model (HCAM) approach to multi-modal emotion recognition using a combination of recurrent and co-attention neural network models. The input to the model consists of two modalities, i) audio data, processed through a learnable wav2vec approach and, ii) text data represented using a bidirectional encoder representations from transformers (BERT) model. The audio and text representations are processed using a set of bi-directional recurrent neural network layers with self-attention that converts each utterance in a given conversation to a fixed dimensional embedding. In order to incorporate contextual knowledge and the information across the two modalities, the audio and text embeddings are combined using a co-attention layer that attempts to weigh the utterance level embeddings relevant to the task of emotion recognition. The neural network
    
[^16]: 不需重新搜索的研究：最大更新参数化可精确预测跨尺度的损失

    Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v1 [cs.CL])

    [http://arxiv.org/abs/2304.06875](http://arxiv.org/abs/2304.06875)

    提出了一种新的方法muP，可以提高超参数的缩放律的拟合精度，减少对大模型超参数的搜索，从而实现在大规模模型上进行损失预测。

    

    随着语言模型的扩大，验证研究想法变得越来越昂贵，因为小模型的结论不能简单地转移到大模型。解决方案是建立一个通用系统，仅基于小模型的结果和超参数直接预测大模型的一些指标。现有的基于缩放律的方法需要在最大的模型上进行超参数搜索，但由于资源有限，这是不切实际的。我们通过展示我们的发现表明，最大更新参数化（muP）使得可以在靠近常见损失流域的超参数的情况下准确拟合超参数的缩放律，而不需要任何搜索。因此，不同的模型可以在大尺度上进行损失预测，在训练开始之前就可以进行直接比较。我们提出了一种新的范式，作为可靠的学术研究的第一步，适用于任何模型规模，而不需大量的计算。代码将很快公开可用。

    As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones. A possible solution is to establish a generic system that directly predicts some metrics for large models solely based on the results and hyperparameters from small models. Existing methods based on scaling laws require hyperparameter search on the largest models, which is impractical with limited resources. We address this issue by presenting our discoveries indicating that Maximal Update parametrization (muP) enables accurate fitting of scaling laws for hyperparameters close to common loss basins, without any search. Thus, different models can be directly compared on large scales with loss prediction even before the training starts. We propose a new paradigm as a first step towards reliable academic research for any model scale without heavy computation. Code will be publicly available shortly.
    
[^17]: 对最新大规模预训练模型中的社会偏见进行评估

    Evaluation of Social Biases in Recent Large Pre-Trained Models. (arXiv:2304.06861v1 [cs.CL])

    [http://arxiv.org/abs/2304.06861](http://arxiv.org/abs/2304.06861)

    本文研究了最近发布的三个预训练模型的偏见问题，并评估了它们在两个偏见基准上的表现，探讨了是否随着技术进步，最新的、更快、更轻的模型在开发时负责任地降低了与旧模型相比的社会偏见。

    

    大规模预训练语言模型广泛应用在社区中，通常使用来自于互联网等开放来源的未审核或未筛选的数据进行训练。由于这一点，我们在在线平台上看到的偏见反映了社会上的偏见，并被这些模型所捕捉和学习。这些模型被应用于影响数百万人的应用程序中，它们内在的偏见对于定向的社会群体是有害的。在本文中，我们研究新预训练模型发布后的偏见缩减趋势。选择了三个最新模型(ELECTRA、DeBERTa和DistilBERT)，并对两个偏见基准（StereoSet和CrowS-Pairs）进行评估。它们使用相关度量标准与BERT进行比较。我们探索是否随着技术的进步和新的、更快、更轻的模型发布，它们是否负责任地发展，使其内在的社会偏见与旧模型相比有所降低？

    Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups. In this work, we study the general trend in bias reduction as newer pre-trained models are released. Three recent models ( ELECTRA, DeBERTa, and DistilBERT) are chosen and evaluated against two bias benchmarks, StereoSet and CrowS-Pairs. They are compared to the baseline of BERT using the associated metrics. We explore whether as advancements are made and newer, faster, lighter models are released: are they being developed responsibly such that their inherent social biases have been reduced compared to their older counterparts? The re
    
[^18]: Vax-Culture: 用于研究推特上疫苗讨论的数据集

    Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter. (arXiv:2304.06858v1 [cs.SI])

    [http://arxiv.org/abs/2304.06858](http://arxiv.org/abs/2304.06858)

    本文介绍了一个推特疫苗数据集Vax-Culture，它旨在找出推广疫苗错误信息的文化和政治信念的重叠部分，帮助开发机器学习模型以自动检测疫苗错误信息帖子并应对其负面影响。

    

    COVID-19疫情期间，疫苗犹豫继续是公共卫生官员面临的主要挑战。由于该犹豫破坏了疫苗运动，许多研究人员试图确定其根本原因，并发现社交媒体平台上反疫苗错误信息的不断增长是该问题的关键因素。我们将推特作为误导内容的来源，并旨在提取推广疫苗错误信息的文化和政治信念的重叠部分。为此，我们收集了一个与疫苗有关的推文数据集，并借助专业沟通和新闻背景的注释人员进行注释。我们最终希望这可以带来有效和有针对性的公共卫生通信策略，以接触那些持反疫苗信仰者。此外，这些信息有助于开发机器学习模型以自动检测疫苗错误信息帖子并应对其负面影响。

    Vaccine hesitancy continues to be a main challenge for public health officials during the COVID-19 pandemic. As this hesitancy undermines vaccine campaigns, many researchers have sought to identify its root causes, finding that the increasing volume of anti-vaccine misinformation on social media platforms is a key element of this problem. We explored Twitter as a source of misleading content with the goal of extracting overlapping cultural and political beliefs that motivate the spread of vaccine misinformation. To do this, we have collected a data set of vaccine-related Tweets and annotated them with the help of a team of annotators with a background in communications and journalism. Ultimately we hope this can lead to effective and targeted public health communication strategies for reaching individuals with anti-vaccine beliefs. Moreover, this information helps with developing Machine Learning models to automatically detect vaccine misinformation posts and combat their negative impa
    
[^19]: SemEval-2023任务12: 非洲语言情感分析（AfriSenti-SemEval）

    SemEval-2023 Task 12: Sentiment Analysis for African Languages (AfriSenti-SemEval). (arXiv:2304.06845v1 [cs.CL])

    [http://arxiv.org/abs/2304.06845](http://arxiv.org/abs/2304.06845)

    SemEval-2023举办了非洲语言情感分析挑战赛（AfriSenti-SemEval），旨在提供非洲语言的情感分类数据集。该挑战赛包括单语、多语言和零样本分类三个子任务，并吸引了众多研究者的参与。

    

    本文介绍了第一个非洲语材料的SemEval挑战赛——非洲语言情感分析（AfriSenti-SemEval），其中包含14种非洲语言（阿姆哈拉语、阿尔及利亚阿拉伯语、豪萨语、伊博语、卢旺达语、摩洛哥阿拉伯语、莫桑比克葡萄牙语、尼日利亚皮钦语、奥罗莫语、斯瓦希里语、提格里尼亚语、特威语、克森语和约鲁巴语）。我们提供了三个子任务：（1）单语分类，共收到44个提交结果；（2）多语言分类，共收到32个提交结果；（3）零样本分类，共收到34个提交结果。其中，NLNDE团队在任务A和B中分别获得了71.31和75.06加权F1分数的最佳系统。UCAS-IIE-NLP在任务C上平均获得了58.15加权F1分数的最佳系统。

    We present the first Africentric SemEval Shared task, Sentiment Analysis for African Languages (AfriSenti-SemEval) - the dataset is available at https://github.com/afrisenti-semeval/afrisent-semeval-2023. AfriSenti-SemEval is a sentiment classification challenge in 14 African languages - Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda, Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili, Tigrinya, Twi, Xitsonga, and Yor\`ub\'a (Muhammad et al., 2023), using a 3-class labeled data: positive, negative, and neutral. We present three subtasks: (1) Task A: monolingual classification, which received 44 submissions; (2) Task B: multilingual classification, which received 32 submissions; and (3) Task C: zero-shot classification, which received 34 submissions. The best system for tasks A and B was achieved by NLNDE team with 71.31 and 75.06 weighted F1, respectively. UCAS-IIE-NLP achieved the best system on average for task C with 58.15 weighted F1. We describe the variou
    
[^20]: 论基础模型在地理空间AI中的机遇与挑战

    On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence. (arXiv:2304.06798v1 [cs.AI])

    [http://arxiv.org/abs/2304.06798](http://arxiv.org/abs/2304.06798)

    本文研究了在地理空间AI中开发基础模型的机遇和挑战，测试了多种FMs在地理子领域中的表现，发现在文本任务上的表现优于任务特定的定制模型，但在发展中也面临着缺少数据集和需要专业技术微调的挑战。

    

    基础模型（FMs）是指在大规模数据上以任务无关的方式进行训练，并通过微调、少样本甚至零样本学习适用于广泛下游任务的大型预训练模型。虽然在语言和视觉任务中大获成功，但我们尚未见到为地理空间人工智能（GeoAI）开发基础模型的尝试。本文探讨开发多模态基础模型以应对GeoAI的潜力和挑战。我们首先通过在多个地理空间子域中进行七项任务的测试，包括地理语义、健康地理学、城市地理学和遥感等，研究了现有许多FMs的潜力。结果表明，在仅涉及文本模态的一些地理空间任务（例如地名识别、位置描述识别以及美国州级/县级痴呆症时间序列预测）中，这些任务无关的LLM也可以胜任任务特定的完全定制模型。我们进一步讨论了为地理空间AI开发FMs的挑战，包括缺乏大规模地理空间数据集和需要专门的地理空间微调技术。最后，我们确定了多模态FMs的潜在研究方向和应用，以惠及地理空间AI社区。

    Large pre-trained models, also known as foundation models (FMs), are trained in a task-agnostic manner on large-scale data and can be adapted to a wide range of downstream tasks by fine-tuning, few-shot, or even zero-shot learning. Despite their successes in language and vision tasks, we have yet seen an attempt to develop foundation models for geospatial artificial intelligence (GeoAI). In this work, we explore the promises and challenges of developing multimodal foundation models for GeoAI. We first investigate the potential of many existing FMs by testing their performances on seven tasks across multiple geospatial subdomains including Geospatial Semantics, Health Geography, Urban Geography, and Remote Sensing. Our results indicate that on several geospatial tasks that only involve text modality such as toponym recognition, location description recognition, and US state-level/county-level dementia time series forecasting, these task-agnostic LLMs can outperform task-specific fully-s
    
[^21]: Token-and-Duration Transducer架构：联合预测标记与时长的高效序列转导

    Efficient Sequence Transduction by Jointly Predicting Tokens and Durations. (arXiv:2304.06795v1 [eess.AS])

    [http://arxiv.org/abs/2304.06795](http://arxiv.org/abs/2304.06795)

    本文提出了一种新型的序列转导架构TDT，它可以联合预测标记和持续时间，从而实现比传统Transducers更高的准确性和显着更快的推理速度。

    

    本文提出了一种用于序列到序列任务的新型Token-and-Duration Transducer(TDT)架构。TDT通过联合预测标记和持续时间，即发射的标记覆盖的输入帧的数量，来扩展传统的RNN-Transducer架构。它使用具有两个独立标准化输出的联合网络来生成标记和持续时间的分布。在推理期间，TDT模型可以通过预测的持续时间输出跳过输入帧，使其比逐帧处理编码器输出的传统Transducers显着更快。在不同的序列转导任务上，TDT模型均实现了更高的准确性和显着更快的推理速度。语音识别的TDT模型比RNN-Transducers获得更好的准确性，并且推理速度高达2.82倍。语音翻译的TDT模型与MUST-C测试相比提高了1个BLEU分数。

    This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than RNN-Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared wi
    
[^22]: RAFT: 奖励排名微调用于生成型基础模型对齐

    RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])

    [http://arxiv.org/abs/2304.06767](http://arxiv.org/abs/2304.06767)

    RAFT框架引入了奖励排名微调方法，用于对齐生成型基础模型，以解决强化学习带来的低效和不稳定性问题。

    

    生成型基础模型容易受到广泛的无监督训练数据带来的隐式偏见的影响。这些偏见可能导致子优样本、扭曲的结果和不公平，可能产生重大影响。因此，将这些模型与人的伦理和偏好对齐是确保它们在真实应用中负责任和有效的部署的关键步骤。以往的研究主要采用人类反馈的强化学习（ RLHF）作为解决这个问题的手段。在 RL 算法的指导下，用人类反馈指导的奖励模型对生成模型进行微调。然而， RL 算法的低效性和不稳定性常常会对生成模型的成功对齐产生重大障碍，因此需要开发一种更为强大和简化的方法。为此，我们引入了一个新的框架，即奖励排名微调（ RAFT ），旨在对齐生成基础模型。

    Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
    
[^23]: 布局引导下的图像生成的诊断基准和迭代修复

    Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation. (arXiv:2304.06671v1 [cs.CV])

    [http://arxiv.org/abs/2304.06671](http://arxiv.org/abs/2304.06671)

    本文提出了布局引导下图像生成的诊断基准LayoutBench，对数量、位置、大小和形状四种空间控制技能进行了研究，发现好的ID布局控制在任意布局的野外环境下可能不具有良好的推广性。接着，我们提出了一种新的基准方法IterInpaint通过修复逐步生成前景和背景区域，显现出在OOD布局方面更强的通用性。

    

    空间控制是可控图像生成的核心能力。在布局引导下的图像生成方面的进展已经显示出在具有类似空间配置的内分布（ID）数据集上有良好的结果。然而，当面对任意不确定的布局的离线分布样本时，这些模型的表现还不清楚。在本文中，我们提出了LayoutBench，这是一种对布局引导下的图像生成进行诊断的基准，它检查了四种空间控制技能：数量，位置，大小和形状。我们对两种最近代表性的布局引导下的图像生成方法进行了基准测试，并观察到良好的ID布局控制可能无法很好地推广到任意布局的野外环境（例如，边界上的对象）。接下来，我们提出了一个新的基准方法IterInpaint，它通过修复逐步生成前景和背景区域，展示出在LayoutBench的OOD布局上更强的通用性。我们进行了数量和定性评估，表明IterInpaint相对于现有方法具有更好的生成多样和视觉上令人愉悦的图像和可控的空间布局。

    Spatial control is a core capability in controllable image generation. Advancements in layout-guided image generation have shown promising results on in-distribution (ID) datasets with similar spatial configurations. However, it is unclear how these models perform when facing out-of-distribution (OOD) samples with arbitrary, unseen layouts. In this paper, we propose LayoutBench, a diagnostic benchmark for layout-guided image generation that examines four categories of spatial control skills: number, position, size, and shape. We benchmark two recent representative layout-guided image generation methods and observe that the good ID layout control may not generalize well to arbitrary layouts in the wild (e.g., objects at the boundary). Next, we propose IterInpaint, a new baseline that generates foreground and background regions in a step-by-step manner via inpainting, demonstrating stronger generalizability than existing models on OOD layouts in LayoutBench. We perform quantitative and q
    
[^24]: G2T: 基于预训练语言模型和社区检测的主题建模框架

    G2T: A simple but versatile framework for topic modeling based on pretrained language model and community detection. (arXiv:2304.06653v1 [cs.CL])

    [http://arxiv.org/abs/2304.06653](http://arxiv.org/abs/2304.06653)

    G2T是一种基于预训练语言模型和社区检测的主题建模框架，自动评估表明，G2T在多个数据集上均与当前最先进的方法相比表现更好。

    

    先前的研究表明，基于聚类的主题模型能够通过适当的词语筛选方法聚类高质量的句子嵌入，生成比生成式概率主题模型更好的主题。然而，这些方法存在选择合适参数的困难以及不完整的模型忽略单词与主题及主题与文本之间的定量关系的问题。为了解决这些问题，我们提出了一种简洁但有效的主题建模框架，即图主题（G2T）。

    It has been reported that clustering-based topic models, which cluster high-quality sentence embeddings with an appropriate word selection method, can generate better topics than generative probabilistic topic models. However, these approaches suffer from the inability to select appropriate parameters and incomplete models that overlook the quantitative relation between words with topics and topics with text. To solve these issues, we propose graph to topic (G2T), a simple but effective framework for topic modelling. The framework is composed of four modules. First, document representation is acquired using pretrained language models. Second, a semantic graph is constructed according to the similarity between document representations. Third, communities in document semantic graphs are identified, and the relationship between topics and documents is quantified accordingly. Fourth, the word--topic distribution is computed based on a variant of TFIDF. Automatic evaluation suggests that G2
    
[^25]: 探索法律问题回答系统的现状

    Exploring the State of the Art in Legal QA Systems. (arXiv:2304.06623v1 [cs.CL])

    [http://arxiv.org/abs/2304.06623](http://arxiv.org/abs/2304.06623)

    法律问题回答系统的研究面临着复杂性和多样性等挑战，但其在客户服务、教育、研究和跨语言交流等方面具有广泛应用。

    

    回答与法律领域相关的问题是一项复杂的任务，主要是由于复杂的法律文档系统的复杂性和多样性。为法律问题提供准确的答案通常需要相关领域的专业知识，这使得即使对于人类专家来说，这项任务也更具挑战性。问答系统（QA）旨在生成对以人类语言提出的问题的答案。它们使用自然语言处理来理解问题并搜索信息以找到相关答案。QA具有各种实际应用，包括客户服务、教育、研究和跨语言交流。然而，它们面临着诸如改进自然语言理解和处理复杂和模糊问题等挑战。

    Answering questions related to the legal domain is a complex task, primarily due to the intricate nature and diverse range of legal document systems. Providing an accurate answer to a legal query typically necessitates specialized knowledge in the relevant domain, which makes this task all the more challenging, even for human experts. QA (Question answering systems) are designed to generate answers to questions asked in human languages. They use natural language processing to understand questions and search through information to find relevant answers. QA has various practical applications, including customer service, education, research, and cross-lingual communication. However, they face challenges such as improving natural language understanding and handling complex and ambiguous questions. Answering questions related to the legal domain is a complex task, primarily due to the intricate nature and diverse range of legal document systems. Providing an accurate answer to a legal query
    
[^26]: PDF-VQA: 一个新的用于PDF文件真实世界VQA的数据集

    PDF-VQA: A New Dataset for Real-World VQA on PDF Documents. (arXiv:2304.06447v1 [cs.CV])

    [http://arxiv.org/abs/2304.06447](http://arxiv.org/abs/2304.06447)

    该研究提出了一个新的文档VQA数据集PDF-VQA，以多个页面的完整文档作为研究对象，通过机器学习模型识别与处理文档元素、结构和内容等方面，为解决真实世界中的文档理解问题提供新的资源。

    

    基于文档的视觉问答（VQA）研究文档图像的文档理解问题。我们提出了一个新的基于文档的VQA数据集PDF-VQA，从文档元素识别、文档布局结构理解以及上下文理解和关键信息提取等各个方面全面探讨文档理解问题。我们的PDF-VQA数据集将文档理解的规模从单个文档页面扩展到询问多个页面的完整文档。我们还提出了一个新的基于图形的VQA模型，明确地集成了不同文档元素之间的空间和层次结构关系，以提高文档结构的理解能力。该性能与多个基线模型相比较，可以适用于不同的问题类型和任务。

    Document-based Visual Question Answering examines the document understanding of document images in conditions of natural language questions. We proposed a new document-based VQA dataset, PDF-VQA, to comprehensively examine the document understanding from various aspects, including document element recognition, document layout structural understanding as well as contextual understanding and key information extraction. Our PDF-VQA dataset extends the current scale of document understanding that limits on the single document page to the new scale that asks questions over the full document of multiple pages. We also propose a new graph-based VQA model that explicitly integrates the spatial and hierarchically structural relationships between different document elements to boost the document structural understanding. The performances are compared with several baselines over different question types and tasks\footnote{The full dataset will be released after paper acceptance.
    
[^27]: 从指导视频中进行手语翻译

    Sign Language Translation from Instructional Videos. (arXiv:2304.06371v1 [cs.CL])

    [http://arxiv.org/abs/2304.06371](http://arxiv.org/abs/2304.06371)

    本论文描述了如何使用I3D视频特征培训Transformer模型，据此对How2Sign数据集进行手语翻译。作者提供了公共代码和首个开源实现。

    

    自动手语翻译（SLT）到口语语言的进展大部分都是基于规模有限、领域受限的数据集进行评估的。本论文通过提供首个基于大规模数据集"How2Sign"的基准结果，推动了该领域的最新研究进展。我们使用I3D视频特征对Transformer进行训练，使用降低的BLEU分数作为验证的参考指标代替广泛使用的BLEU分数。我们报告了8.03的BLEU分数，并发布了首个开源实现，以推动进一步的进展。

    The advances in automatic sign language translation (SLT) to spoken languages have been mostly benchmarked with datasets of limited size and restricted domains. Our work advances the state of the art by providing the first baseline results on How2Sign, a large and broad dataset.  We train a Transformer over I3D video features, using the reduced BLEU as a reference metric for validation, instead of the widely used BLEU score. We report a result of 8.03 on the BLEU score, and publish the first open-source implementation of its kind to promote further advances.
    
[^28]: 强化学习辅导员在数学任务中更好地支持了低成绩学生

    Reinforcement Learning Tutor Better Supported Lower Performers in a Math Task. (arXiv:2304.04933v1 [cs.AI])

    [http://arxiv.org/abs/2304.04933](http://arxiv.org/abs/2304.04933)

    本文证明了深度强化学习可用于提供自适应的教育支持，尤其对于最初成绩较低的学生具有最大的益处。

    

    资源限制使得为所有学生提供个性化教学变得困难。强化学习可以成为减少发展成本、提高智能辅导软件效果的关键工具，旨在为学生提供正确的支持。在这里，我们展示了深度强化学习如何在叙述故事线软件中为学习“容积”概念的学生提供自适应教育支持。通过解释性人工智能工具，我们也提取了有关学习的可解释洞见，证明了所得政策在不同的学生群体中具有类似的表现。最重要的是，在这两项研究中，强化学习故事系统对最初的预测分数最低的学生有最大的益处，这表明了AI适应并为低成绩学生提供支持的机会。

    Resource limitations make it hard to provide all students with one of the most effective educational interventions: personalized instruction. Reinforcement learning could be a key tool to reduce the development cost and improve the effectiveness of, intelligent tutoring software that aims to provide the right support, at the right time, to a student. Here we illustrate that deep reinforcement learning can be used to provide adaptive pedagogical support to students learning about the concept of volume in a narrative storyline software. Using explainable artificial intelligence tools, we also extracted interpretable insights about the pedagogical policy learned, and we demonstrate that the resulting policy had similar performance in a different student population. Most importantly, in both studies the reinforcement-learning narrative system had the largest benefit for those students with the lowest initial pretest scores, suggesting the opportunity for AI to adapt and provide support for
    
[^29]: 土木鹦鹉传奇(GPT)：利用及时工程克服GPT幻觉以在岩土工程中应用

    Geotechnical Parrot Tales (GPT): Overcoming GPT hallucinations with prompt engineering for geotechnical applications. (arXiv:2304.02138v1 [cs.CL])

    [http://arxiv.org/abs/2304.02138](http://arxiv.org/abs/2304.02138)

    本文探讨了如何利用GPT在岩土工程应用中的全部潜力，着重讨论及时工程的重要性，并开发了一个统一的自然语言接口，用于处理复杂的岩土工程任务和数据分析。

    

    大型语言模型（LLM）的普及，如OpenAI的ChatGPT，可能会彻底改变包括岩土工程在内的各个行业。 但是，GPT模型有时会生成听起来很有道理但错误的输出，导致幻觉产生。 本文讨论了在缓解这些风险和利用GPT在岩土工程应用中的全部潜力方面，及时工程的重要性。 我们探讨了与LLM相关的挑战和陷阱，并强调了上下文在确保准确和有价值的响应方面的作用。 此外，我们还研究了特定于上下文的搜索引擎的开发以及LLM成为复杂任务（例如数据分析和设计）的自然界面的潜力。 我们还开发了一个统一的自然语言接口，用于处理复杂的岩土工程任务和数据分析。 通过将GPT集成到岩土工程工作流中，专业人员可以简化他们的工作并发展可持续性。

    The widespread adoption of large language models (LLMs), such as OpenAI's ChatGPT, could revolutionized various industries, including geotechnical engineering. However, GPT models can sometimes generate plausible-sounding but false outputs, leading to hallucinations. In this article, we discuss the importance of prompt engineering in mitigating these risks and harnessing the full potential of GPT for geotechnical applications. We explore the challenges and pitfalls associated with LLMs and highlight the role of context in ensuring accurate and valuable responses. Furthermore, we examine the development of context-specific search engines and the potential of LLMs to become a natural interface for complex tasks, such as data analysis and design. We also develop a unified interface using natural language to handle complex geotechnical engineering tasks and data analysis. By integrating GPT into geotechnical engineering workflows, professionals can streamline their work and develop sustain
    
[^30]: WebQAmGaze: 一份多语言Webcam阅读时眼动追踪数据集

    WebQAmGaze: A Multilingual Webcam Eye-Tracking-While-Reading Dataset. (arXiv:2303.17876v1 [cs.CL])

    [http://arxiv.org/abs/2303.17876](http://arxiv.org/abs/2303.17876)

    WebQAmGaze是一个多语言低成本的阅读时眼动追踪数据集，包括332位参与者的数据，对相关段落的注视似乎能够反映回答理解问题的准确性。这份数据可以推动基于网络摄像头的阅读研究并开辟更便宜、更易获得的数据收集方式。

    

    我们创建了WebQAmGaze，这是一个多语种低成本的阅读时眼动追踪数据集，旨在支持公平透明的自然语言处理模型的开发。WebQAmGaze包括了来自332位参与者阅读英语、西班牙语和德语文本时的网络摄像头眼动数据。每个参与者都会完成两个阅读任务，包括五篇文章的正常阅读和信息寻找任务。经过数据预处理，我们发现对相关段落的注视似乎意味着回答理解问题的正确性。此外，我们与高质量的眼动追踪数据进行了比较分析，结果显示Webcam-ET获得的特征与商业ET设备的特征之间存在中等的相关性。我们相信这份数据可以推动基于网络摄像头的阅读研究并开辟更便宜、更易获得的数据收集方式。WebQAmGaze对于了解问题回答的认知过程以及自然语言处理模型的公平透明具有实用价值。

    We create WebQAmGaze, a multilingual low-cost eye-tracking-while-reading dataset, designed to support the development of fair and transparent NLP models. WebQAmGaze includes webcam eye-tracking data from 332 participants naturally reading English, Spanish, and German texts. Each participant performs two reading tasks composed of five texts, a normal reading and an information-seeking task. After preprocessing the data, we find that fixations on relevant spans seem to indicate correctness when answering the comprehension questions. Additionally, we perform a comparative analysis of the data collected to high-quality eye-tracking data. The results show a moderate correlation between the features obtained with the webcam-ET compared to those of a commercial ET device. We believe this data can advance webcam-based reading studies and open a way to cheaper and more accessible data collection. WebQAmGaze is useful to learn about the cognitive processes behind question answering (QA) and to a
    
[^31]: 仅仅提示足够了吗？不是的。指导学习的全面和更广阔视角（arXiv：2303.10475v1 [cs.CL]）

    Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning. (arXiv:2303.10475v1 [cs.CL])

    [http://arxiv.org/abs/2303.10475](http://arxiv.org/abs/2303.10475)

    传统的自然语言处理机器学习需要大规模的任务特定示例，但这不适用于任务可能过于复杂或成本过高以进行注释的场景。因此，社区对于自然语言处理中新的监督寻求范式--从任务指令学习--越来越感兴趣。

    

    任务语义可以通过一组输入输出示例或一条文本指令来表达。传统的自然语言处理（NLP）机器学习方法主要依赖于大规模的任务特定示例的可用性。这引起了两个问题：首先，收集任务特定标记示例不适用于任务可能过于复杂或成本过高以进行注释的场景，或者系统需要立即处理新任务。其次，这不是用户友好的，因为最终用户可能更愿意在使用系统之前提供任务描述而不是一组示例。因此，社区对于自然语言处理中新的监督寻求范式--从任务指令学习--越来越感兴趣。尽管取得了令人印象深刻的进展，但社区仍然面临着一些共同的问题。本次调查旨在总结指导学习的当前研究，特别是回答以下问题：

    Task semantics can be expressed by a set of input-to-output examples or a piece of textual instruction. Conventional machine learning approaches for natural language processing (NLP) mainly rely on the availability of large-scale sets of task-specific examples. Two issues arise: first, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system. Therefore, the community is paying increasing interest in a new supervision-seeking paradigm for NLP: learning from task instructions. Despite its impressive progress, there are some common issues that the community struggles with. This survey paper tries to summarize the current research on instruction learning, particularly, by answering the following questions:
    
[^32]: CoLT5: 基于条件计算的快速长距离Transformer模型

    CoLT5: Faster Long-Range Transformers with Conditional Computation. (arXiv:2303.09752v1 [cs.CL])

    [http://arxiv.org/abs/2303.09752](http://arxiv.org/abs/2303.09752)

    CoLT5是一种基于条件计算的Transformer模型，通过优先处理重要标记来加速长距离输入的处理。CoLT5在SCROLLS基准测试上表现最好，并能够有效地处理长达64k输入长度。

    

    许多自然语言处理任务需要处理长输入，但使用Transformer处理长文档很昂贵——这不仅是因为二次注意复杂性，还因为对每个标记应用前馈和投影层。然而，不是所有标记都同样重要，特别是对于较长的文档。我们提出了CoLT5，一种长输入Transformer模型，通过使用条件计算来利用此直觉，在前馈和注意层中为重要标记提供更多资源。我们展示了CoLT5比LongT5表现更强，训练和推理速度更快，在长输入SCROLLS基准测试上达到了SOTA。此外，CoLT5能够有效且可控地利用极长的输入，展示了高达64k输入长度的强大增益。

    Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.
    
[^33]: Inseq：一个用于序列生成模型的可解释性工具包

    Inseq: An Interpretability Toolkit for Sequence Generation Models. (arXiv:2302.13942v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.13942](http://arxiv.org/abs/2302.13942)

    本文介绍了Inseq，这是一个Python工具包，旨在推广可解释性序列生成模型的分析。它为常见的解码器和编码器-解码器Transformers架构提供了提取模型内部信息和特征重要性得分的直观优化方法。作者还在机器翻译模型和GPT-2中展示了Inseq的潜力，证明其有助于推动可解释性自然语言生成的未来发展。

    

    自然语言处理领域的过去的可解释性研究主要集中在流行的分类任务上，而在生成任务中往往被忽视，部分原因是缺乏专门的工具。在本文中，我们介绍了Inseq，一个Python库，用于使序列生成模型的可解释性分析普及化。Inseq能够直观且优化地提取流行的仅解码器和编码器解码器Transformers架构的模型内部信息和特征重要性分数。我们还展示了它的潜力，通过使用它来突出机器翻译模型中的性别偏见并在GPT-2中定位事实知识。由于其支持对比特征归因等前沿技术的可扩展接口，因此Inseq可以推动可解释性自然语言生成的未来发展，集中优良实践，并实现公正和可重复的模型评估。

    Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models' internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations.
    
[^34]: Alloprof：一个新的法语问答教育数据集及其在信息检索案例研究中的应用

    Alloprof: a new French question-answer education dataset and its use in an information retrieval case study. (arXiv:2302.07738v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.07738](http://arxiv.org/abs/2302.07738)

    这个论文介绍了一个新的阿洛普夫法语问答数据集，收集了来自10,368名学生的29,349个问题和解释，并展示了在信息检索任务中使用该数据集的案例研究。

    

    教师和学生越来越依赖在线学习资源来补充学校提供的资源。可用资源的广度和深度的增加对学生来说是一件好事，但前提是他们能够找到答案。问答和信息检索系统已受益于公共数据集，以训练和评估其算法，但大多数这些数据集都是英文文本，由成年人编写和阅读。我们介绍了一个新的公共法语问答数据集，从总部位于魁北克的小学和中学帮助网站Alloprof收集，包含29,349个问题及其解释，涵盖各种学科的10,368名学生，超过一半的解释包含链接到其他问题或网站上的2,596个参考页面之一。我们还向您展示了在信息检索任务中使用本数据集的案例研究。

    Teachers and students are increasingly relying on online learning resources to supplement the ones provided in school. This increase in the breadth and depth of available resources is a great thing for students, but only provided they are able to find answers to their queries. Question-answering and information retrieval systems have benefited from public datasets to train and evaluate their algorithms, but most of these datasets have been in English text written by and for adults. We introduce a new public French question-answering dataset collected from Alloprof, a Quebec-based primary and high-school help website, containing 29 349 questions and their explanations in a variety of school subjects from 10 368 students, with more than half of the explanations containing links to other questions or some of the 2 596 reference pages on the website. We also present a case study of this dataset in an information retrieval task. This dataset was collected on the Alloprof public forum, with 
    
[^35]: EXIF作为一种语言：学习图像与相机元数据之间的交叉模态关联

    EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata. (arXiv:2301.04647v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.04647](http://arxiv.org/abs/2301.04647)

    本文通过学习图像和相机元数据之间的交叉模态关联提取相机信息，并使用得到的特征成功实现拼接图像区域的"零样本"定位。

    

    本文旨在学习一个视觉表示，从而提取与所记录的照片相关的相机信息。为此，我们在图像块和自动插入到图像文件中的EXIF元数据之间训练了一个多模态嵌入。我们的模型通过将元数据转换为文本，然后使用transformer进行处理来表示此元数据。我们学习的特征在下游图像取证和校准任务上明显优于其他自监督和有监督特征。特别地，我们成功地通过对图像内所有块的视觉嵌入进行聚类来实现"零样本"的拼接图像区域定位。

    We learn a visual representation that captures information about the camera that recorded a given photo. To do this, we train a multimodal embedding between image patches and the EXIF metadata that cameras automatically insert into image files. Our model represents this metadata by simply converting it to text and then processing it with a transformer. The features that we learn significantly outperform other self-supervised and supervised features on downstream image forensics and calibration tasks. In particular, we successfully localize spliced image regions "zero shot" by clustering the visual embeddings for all of the patches within an image.
    
[^36]: 使用主动学习方法来有策略地选择自动评分的作文

    Using Active Learning Methods to Strategically Select Essays for Automated Scoring. (arXiv:2301.00628v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.00628](http://arxiv.org/abs/2301.00628)

    本研究提出了三种主动学习方法，可最小化必须由人工评分者进行评分的文章数量，同时提供训练现代自动化论文打分系统所需的数据。

    

    自动化评分的研究变得越来越重要，因为它作为一种评估学生大规模书面作答能力的方法。随着学生转向在线学习环境，需要评估大量书面作答评估。本研究的目的是描述和评估三种可以用于最小化必须由人工评分者进行评分的作文数量的主动学习方法，同时仍然提供训练现代自动化论文打分系统所需的数据。这三种主动学习方法是基于不确定性、基于拓扑、混合方法。这三种方法被用于选择作为自动化学生评估奖竞赛的一部分的文章，之后使用双向编码器表示法从变换语言模型中进行训练的评分模型进行分类。

    Research on automated essay scoring has become increasing important because it serves as a method for evaluating students' written-responses at scale. Scalable methods for scoring written responses are needed as students migrate to online learning environments resulting in the need to evaluate large numbers of written-response assessments. The purpose of this study is to describe and evaluate three active learning methods than can be used to minimize the number of essays that must be scored by human raters while still providing the data needed to train a modern automated essay scoring system. The three active learning methods are the uncertainty-based, the topological-based, and the hybrid method. These three methods were used to select essays included as part of the Automated Student Assessment Prize competition that were then classified using a scoring model that was training with the bidirectional encoder representations from transformer language model. All three active learning met
    
[^37]: 利用自然语言处理增强电子病历中的结构化社会卫生因素数据

    Leveraging Natural Language Processing to Augment Structured Social Determinants of Health Data in the Electronic Health Record. (arXiv:2212.07538v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.07538](http://arxiv.org/abs/2212.07538)

    本文开发了一个SDOH信息提取器，通过将其应用于EHR中的临床叙述，捕获详细的SDOH信息，并将提取的表示与现有的结构化数据进行组合以获得信息增益。

    

    目的：社会卫生因素（SDOH）影响健康结果，并通过结构化数据和非结构化临床记录在电子病历（EHR）中进行记录。然而，临床记录通常包含更全面的SDOH信息，详细说明方面，例如状态、严重程度和时间性。本文有两个主要目标：i）开发自然语言处理（NLP）信息提取模型，以捕获详细的SDOH信息；ii）通过将SDOH抽取器应用于临床叙述并将提取的表示与现有的结构化数据组合来评估所获得的信息增益。

    Objective: Social determinants of health (SDOH) impact health outcomes and are documented in the electronic health record (EHR) through structured data and unstructured clinical notes. However, clinical notes often contain more comprehensive SDOH information, detailing aspects such as status, severity, and temporality. This work has two primary objectives: i) develop a natural language processing (NLP) information extraction model to capture detailed SDOH information and ii) evaluate the information gain achieved by applying the SDOH extractor to clinical narratives and combining the extracted representations with existing structured data.  Materials and Methods: We developed a novel SDOH extractor using a deep learning entity and relation extraction architecture to characterize SDOH across various dimensions. In an EHR case study, we applied the SDOH extractor to a large clinical data set with 225,089 patients and 430,406 notes with social history sections and compared the extracted S
    
[^38]: 低资源命名实体识别中的AUC最大化

    AUC Maximization for Low-Resource Named Entity Recognition. (arXiv:2212.04800v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.04800](http://arxiv.org/abs/2212.04800)

    本文提出了在低资源命名实体识别中使用AUC最大化的方法，通过结合两个最大化AUC分数的二进制分类器，在低资源NER设置下实现了显着的性能提高，优于传统的损失函数。

    

    目前命名实体识别领域的工作使用交叉熵（CE）或条件随机场（CRF）作为优化NER模型的目标/损失函数。然而，这两种传统的NER问题的目标函数通常在数据分布平衡，并且有足够的注释训练样例时可以产生足够的性能。但由于NER本质上是一个不平衡的标记问题，在低资源情况下使用这些标准目标函数时，模型性能可能会受到影响。基于最大化ROC曲线下面积（AUC）的最新进展，我们提出通过最大化AUC分数来优化NER模型。我们提供证据表明，通过简单地结合两个最大化AUC分数的二进制分类器，在低资源NER设置下实现了显着的性能提高，优于传统的损失函数。我们还进行了广泛的实验，以展示我们的方法在低资源情况下的优势。

    Current work in named entity recognition (NER) uses either cross entropy (CE) or conditional random fields (CRF) as the objective/loss functions to optimize the underlying NER model. Both of these traditional objective functions for the NER problem generally produce adequate performance when the data distribution is balanced and there are sufficient annotated training examples. But since NER is inherently an imbalanced tagging problem, the model performance under the low-resource settings could suffer using these standard objective functions. Based on recent advances in area under the ROC curve (AUC) maximization, we propose to optimize the NER model by maximizing the AUC score. We give evidence that by simply combining two binary-classifiers that maximize the AUC score, significant performance improvement over traditional loss functions is achieved under low-resource NER settings. We also conduct extensive experiments to demonstrate the advantages of our method under the low-resource 
    
[^39]: 最小化增广，最大化数据：语音识别与翻译中的数据扩充方法研究

    Make More of Your Data: Minimal Effort Data Augmentation for Automatic Speech Recognition and Translation. (arXiv:2210.15398v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.15398](http://arxiv.org/abs/2210.15398)

    本文研究了一种简单、经济的数据扩充方法，即将原始数据样本串联以构建新的训练实例。使用这种方法继续训练能够改进Transformer和Conformer模型，并在多种任务中实现了长达0.9 WER的改进。

    

    数据增广是一种根据已有数据生成新的训练数据的技术。本文评估了将原始数据样本串联以构建新的训练实例的简单且经济的方法。继续使用这样的增广数据进行训练能够改进原始数据优化的Transformer和Conformer模型。我们在LibriSpeech-960h测试集上展示了显著的改进（test-clean和test-other的WER分别为2.83和6.87），这些改进也在与浅层融合相结合的模型中得以体现（WER为2.55和6.27）。我们的继续训练方法还在CoVoST-2的四种非英语语言的ASR部分中实现了长达0.9 WER的改进，并且我们观察到这些收益与原始训练数据的大小高度相关。我们比较了不同的串联策略，并发现我们的方法不需要说话人信息即可实现其改进。最后，我们在两个数据集上展示了我们的方法也可适用于翻译任务。

    Data augmentation is a technique to generate new training data based on existing data. We evaluate the simple and cost-effective method of concatenating the original data examples to build new training instances. Continued training with such augmented data is able to improve off-the-shelf Transformer and Conformer models that were optimized on the original data only. We demonstrate considerable improvements on the LibriSpeech-960h test sets (WER 2.83 and 6.87 for test-clean and test-other), which carry over to models combined with shallow fusion (WER 2.55 and 6.27). Our method of continued training also leads to improvements of up to 0.9 WER on the ASR part of CoVoST-2 for four non English languages, and we observe that the gains are highly dependent on the size of the original training data. We compare different concatenation strategies and found that our method does not need speaker information to achieve its improvements. Finally, we demonstrate on two datasets that our methods also
    
[^40]: 基于提议器和回归器的端到端实体检测方法

    End-to-End Entity Detection with Proposer and Regressor. (arXiv:2210.10260v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.10260](http://arxiv.org/abs/2210.10260)

    该论文提出了一种基于提议器和回归器的端到端实体检测方法，通过利用特征金字塔网络生成高质量的实体提议，并对提议进行精细调整以生成最终的预测结果。该模型具有查询语义丰富、实体定位精度高、模型训练容易等优点，还引入了空间调制变压器来增强内部关系的建模能力。实验结果表明，该方法显著优于现有的最先进方法。

    

    命名实体识别是自然语言处理中的传统任务。特别是，由于嵌套场景的普遍存在，嵌套实体识别受到广泛关注。最近的研究将目标检测中的集合预测被转移应用于应对实体嵌套，但是这些方法的问题在于需要手动创建查询向量，无法适应上下文中丰富的语义信息。本文提出了一种基于提议器和回归器的端到端实体检测方法来解决这些问题。首先，提议器利用特征金字塔网络生成高质量的实体提议。然后，回归器对提议进行精细调整以生成最终的预测结果。该模型采用了仅编码器架构，因此具有查询语义丰富、实体定位精度高、模型训练容易等优点。此外，我们引入了空间调制变压器来增强模型对不同实体之间内部关系的建模能力。在两个基准数据集上的实验结果表明，我们的模型显著优于现有的最先进方法。

    Named entity recognition is a traditional task in natural language processing. In particular, nested entity recognition receives extensive attention for the widespread existence of the nesting scenario. The latest research migrates the well-established paradigm of set prediction in object detection to cope with entity nesting. However, the manual creation of query vectors, which fail to adapt to the rich semantic information in the context, limits these approaches. An end-to-end entity detection approach with proposer and regressor is presented in this paper to tackle the issues. First, the proposer utilizes the feature pyramid network to generate high-quality entity proposals. Then, the regressor refines the proposals for generating the final prediction. The model adopts encoder-only architecture and thus obtains the advantages of the richness of query semantics, high precision of entity localization, and easiness of model training. Moreover, we introduce the novel spatially modulated
    
[^41]: 通过结构增强的预训练模型和自适应融合提高语义匹配

    Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion. (arXiv:2210.08471v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.08471](http://arxiv.org/abs/2210.08471)

    本文提出了一种依赖增强的自适应融合注意力模型，它将依赖信息与原始语义信号自适应融合，以更好地模拟复杂的语义匹配关系。

    

    基于Transformer的预训练模型，如BERT，在语义句子匹配方面取得了很大的进展。同时，依赖性先验知识在多个NLP任务中也显示出普遍的益处。然而，如何将依赖性先验结构有效地集成到预训练模型中，以更好地模拟复杂的语义匹配关系，仍未确定。在本文中，我们提出了一种名为DAFA的依赖增强自适应融合注意力模型，这将依赖结构明确地引入预训练模型，并将其自适应地融合到语义信息中。具体地，DAFA首先提出了一个结构敏感范式来构建一个依赖矩阵，以校准注意力权重。它采用自适应融合模块来集成获取的依赖信息和原始语义信号。此外，DAFA重构了注意力计算流程，并提供了更好的可解释性。

    Transformer-based pre-trained models like BERT have achieved great progress on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also shown general benefits in multiple NLP tasks. However, how to efficiently integrate dependency prior structure into pre-trained models to better model complex semantic matching relations is still unsettled. In this paper, we propose the \textbf{D}ependency-Enhanced \textbf{A}daptive \textbf{F}usion \textbf{A}ttention (\textbf{DAFA}), which explicitly introduces dependency structure into pre-trained models and adaptively fuses it with semantic information. Specifically, \textbf{\emph{(i)}} DAFA first proposes a structure-sensitive paradigm to construct a dependency matrix for calibrating attention weights. It adopts an adaptive fusion module to integrate the obtained dependency information and the original semantic signals. Moreover, DAFA reconstructs the attention calculation flow and provides better interpretability. By applying it o
    
[^42]: StyLEx：使用人类词汇注释解释语言风格

    StyLEx: Explaining Style Using Human Lexical Annotations. (arXiv:2210.07469v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.07469](http://arxiv.org/abs/2210.07469)

    StyLEx是一种可以从人工注释的语言风格特征解释中学习，提供句子级风格预测以及人类-like的风格词汇解释的模型。

    

    大型预训练语言模型在各种风格分类任务中取得了令人印象深刻的成果，但它们经常学习到虚假的特定领域词汇以进行预测。为了解决这个问题，我们介绍了StyLEx，这是一种模型，它从人工注释的语言风格特征解释中学习，并联合学习执行任务并预测这些特征作为模型解释。我们的实验证明，StyLEx可以在不牺牲任务性能的情况下，提供类似于人类的风格词汇解释的句子级风格预测，包括域内和域外数据集。与广泛使用的显著性图形可视化方法相比，StyLEx的解释在解释指标（充分性，可信度）和人类评注评估时均有显着提升，并且更容易被人类评判者理解。

    Large pre-trained language models have achieved impressive results on various style classification tasks, but they often learn spurious domain-specific words to make predictions (Hayati et al., 2021). While human explanation highlights stylistic tokens as important features for this task, we observe that model explanations often do not align with them. To tackle this issue, we introduce StyLEx, a model that learns from human-annotated explanations of stylistic features and jointly learns to perform the task and predict these features as model explanations. Our experiments show that StyLEx can provide human-like stylistic lexical explanations without sacrificing the performance of sentence-level style prediction on both in-domain and out-of-domain datasets. Explanations from StyLEx show significant improvements in explanation metrics (sufficiency, plausibility) and when evaluated with human annotations. They are also more understandable by human judges compared to the widely-used salien
    
[^43]: DABERT：双重注意力增强的BERT语义匹配模型

    DABERT: Dual Attention Enhanced BERT for Semantic Matching. (arXiv:2210.03454v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.03454](http://arxiv.org/abs/2210.03454)

    DABERT 通过双重注意力机制和自适应融合模块增强了 BERT 在捕捉句子对之间细微差异的能力，并在实验中取得了良好的效果。

    

    基于Transformer的预训练语言模型（如BERT）在语义句子匹配方面取得了杰出的成果。然而，现有模型仍然在捕捉微小差异的能力上存在不足。如加入、删除或修改句子中的一个单词等噪声可能导致模型预测出错。为了缓解这个问题，我们提出了一种新颖的双重注意力增强的BERT模型（DABERT），以增强BERT在捕捉句子对之间细微差异方面的能力。DABERT由（1）双重注意力模块和（2）自适应融合模块构成。我们在经典的语义匹配和鲁棒性测试数据集上进行了广泛的实验，实验结果表明了DABERT的有效性。

    Transformer-based pre-trained language models such as BERT have achieved remarkable results in Semantic Sentence Matching. However, existing models still suffer from insufficient ability to capture subtle differences. Minor noise like word addition, deletion, and modification of sentences may cause flipped predictions. To alleviate this problem, we propose a novel Dual Attention Enhanced BERT (DABERT) to enhance the ability of BERT to capture fine-grained differences in sentence pairs. DABERT comprises (1) Dual Attention module, which measures soft word matches by introducing a new dual channel alignment mechanism to model affinity and difference attention. (2) Adaptive Fusion module, this module uses attention to learn the aggregation of difference and affinity features, and generates a vector describing the matching details of sentence pairs. We conduct extensive experiments on well-studied semantic matching and robustness test datasets, and the experimental results show the effectiv
    
[^44]: UniCausal：因果关系文本挖掘的统一基准与仓库

    UniCausal: Unified Benchmark and Repository for Causal Text Mining. (arXiv:2208.09163v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.09163](http://arxiv.org/abs/2208.09163)

    UniCausal是一个跨三个任务的因果关系文本挖掘统一基准，整合了六个高质量的语料库的注释。UniCausal可用于评估现有模型能力，并鼓励开发新的因果关系文本挖掘方法和框架。

    

    当前的因果关系文本挖掘数据集在目标、数据覆盖和注释方案等方面存在差异。这些不一致的努力妨碍了建模能力和模型性能的公平比较。此外，很少有数据集包括因果关系跨度注释，这是进行端到端因果关系提取所必需的。为了解决这些问题，我们提出了UniCausal，这是一个跨三个任务的统一因果关系文本挖掘基准：（I）因果序列分类，（II）因果跨度检测和（III）因果对分类。我们整合和对齐了六个高质量的，主要是人工注释的语料库的注释，分别为每个任务提供了总数为58,720、12,144和69,165个示例。由于因果关系的定义可能是主观的，我们的框架旨在允许研究人员在某些或所有数据集和任务上工作。为了创建一个初始基准，我们对BERT预训练语言模型进行了微调，分别针对每个任务实现了70.10％的二进制F1、52.42％的宏F1和67.18％的宏F1。UniCausal可以作为评估现有模型能力的基准，并鼓励开发因果关系文本挖掘的新方法和框架。

    Current causal text mining datasets vary in objectives, data coverage, and annotation schemes. These inconsistent efforts prevent modeling capabilities and fair comparisons of model performance. Furthermore, few datasets include cause-effect span annotations, which are needed for end-to-end causal relation extraction. To address these issues, we propose UniCausal, a unified benchmark for causal text mining across three tasks: (I) Causal Sequence Classification, (II) Cause-Effect Span Detection and (III) Causal Pair Classification. We consolidated and aligned annotations of six high quality, mainly human-annotated, corpora, resulting in a total of 58,720, 12,144 and 69,165 examples for each task respectively. Since the definition of causality can be subjective, our framework was designed to allow researchers to work on some or all datasets and tasks. To create an initial benchmark, we fine-tuned BERT pre-trained language models to each task, achieving 70.10% Binary F1, 52.42% Macro F1, 
    

