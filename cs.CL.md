# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias](https://arxiv.org/abs/2404.00929) | 该论文对多语言大型语言模型进行了全面分析，深入讨论了关键问题，包括多语言语料库、对齐和偏见。 |
| [^2] | [NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using Representative Data](https://arxiv.org/abs/2403.19260) | 首次引入 NaijaHate 数据集，在尼日利亚推特上评估 HSD，发现在代表性数据上评估的 HSD 性能高估了真实世界的表现，提出 NaijaXLM-T 模型，突出了域自适应预训练和微调在最大化 HSD 性能中的关键作用 |
| [^3] | [NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method](https://arxiv.org/abs/2403.18680) | NL-ITI通过引入非线性探测和多令牌干预，成功改进了ITI方法，在多个多选基准上取得了显著的性能改进。 |
| [^4] | [Argument-Aware Approach To Event Linking](https://arxiv.org/abs/2403.15097) | 引入论据感知方法改进事件链接模型，能更好地识别和分类不在知识库中的事件提及，弥补了这一领域的研究空白。 |
| [^5] | [Wav2Gloss: Generating Interlinear Glossed Text from Speech](https://arxiv.org/abs/2403.13169) | Wav2Gloss提出了从语音中自动提取语言注释的任务，并引入了第一个数据集Fieldwork，分析表明预先训练的解码器有助于翻译和注释，并且端到端的系统效果较好。 |
| [^6] | [DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models](https://arxiv.org/abs/2403.10081) | 提出了一种新框架DRAGIN，旨在解决大型语言模型在文本生成过程中动态检索和生成中存在的问题。 |
| [^7] | [LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code](https://arxiv.org/abs/2403.07974) | LiveCodeBench提出了一个全面的、无污染的LLMs评估工具，聚焦于从LeetCode、AtCoder和CodeForces等平台连续收集的新问题，覆盖自修复、代码执行、测试输出预测等更广泛的代码相关能力。 |
| [^8] | [ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis](https://arxiv.org/abs/2403.06932) | ERA-CoT 提出了一种新颖的方法，通过捕获实体之间的关系和支持思维链，帮助大型语言模型(LLMs)理解上下文，提高了多样任务的推理准确性。 |
| [^9] | [RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback](https://arxiv.org/abs/2403.06840) | 通过迭代自反馈的检索增强方法在指定任务的特定场景中提高模型性能，优于现有基准模型，显著增强了事实推理能力。 |
| [^10] | [Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos](https://arxiv.org/abs/2403.05535) | 该论文提出了LaGTran框架，利用文本描述来引导知识转移，在处理具有挑战性的数据集上表现出显著优势。 |
| [^11] | [PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset](https://arxiv.org/abs/2403.03167) | 论文提出了一个名为PARADISE的任务，通过实用程序文本进行演绎推理，评估语言模型仅从给定目标推断计划的能力 |
| [^12] | [CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following](https://arxiv.org/abs/2403.03129) | 提出了一个协作生成框架CoGenesis，整合大型和小型模型，以逻辑方式解决隐私问题。 |
| [^13] | [Views Are My Own, But Also Yours: Benchmarking Theory of Mind using Common Ground](https://arxiv.org/abs/2403.02451) | 介绍了第一个基于自然口语对话的ToM数据集，展示了LM在ToM方面的困难，并表明在Common-ToM上整合简单明确的信念表示可以提高LM的性能。 |
| [^14] | [RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models](https://arxiv.org/abs/2403.02271) | 通过训练少样本释义模型并在训练和测试时用释义丰富数据，可以提高语言模型的性能，超出仅通过参数高效微调的效果。 |
| [^15] | [VariErr NLI: Separating Annotation Error from Human Label Variation](https://arxiv.org/abs/2403.01931) | 该研究提出了一个新的方法和数据集VariErr，专注于NLI任务中的注释错误和人类标签变化的区分。研究填补了在处理信号非黑白情况下的先前空白。 |
| [^16] | [DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable Causal Inference](https://arxiv.org/abs/2403.01166) | 本论文提出了一种基于多变量因果推断的新框架，用于去偏方面级情感分析，从而解决神经网络模型学习虚假相关性的问题。 |
| [^17] | [STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2403.01165) | 本论文提出了一种新颖的方法，有效地将基于不确定性的主动学习和LoRA相结合，以解决大型语言模型数据高效微调中遇到的问题。 |
| [^18] | [Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation](https://arxiv.org/abs/2402.18334) | Bonito是一种用于生成指令调优训练数据集的模型，通过将未注释的文本转换为特定任务训练数据，实现大型语言模型对用户专属数据的零shot任务适应，并显著提高了预训练和指令调整模型的平均性能。 |
| [^19] | [Evaluating Quantized Large Language Models](https://arxiv.org/abs/2402.18158) | 该论文通过全面评估后训练量化对权重、激活和KV缓存的影响，以指导选择量化方法，并对11种模型系列进行了评估，覆盖了多种任务类型。 |
| [^20] | [Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models](https://arxiv.org/abs/2402.18059) | 提出一种利用多目标优化方法的水印技术，通过轻量级网络生成特定令牌水印logits和分割比率，在保证检测性的同时提升了文本的语义完整性。 |
| [^21] | [Variational Learning is Effective for Large Deep Networks](https://arxiv.org/abs/2402.17641) | 变分学习在大型深度网络中展现出非常好的效果，IVON优化器在训练大型网络时几乎能与Adam相媲美甚至胜过它，且预测不确定性更准确，对模型微调、泛化误差预测和数据敏感性估计均有显著改进。 |
| [^22] | [Deep Learning Based Named Entity Recognition Models for Recipes](https://arxiv.org/abs/2402.17447) | 该研究基于深度学习，针对食谱文本开发了命名实体识别模型，通过系统的数据处理和分析，构建了用于自动生成新食谱的数据集。 |
| [^23] | [A Comprehensive Evaluation of Quantization Strategies for Large Language Models](https://arxiv.org/abs/2402.16775) | 该研究提出了一个结构化评估框架，针对大型语言模型的量化策略展开全面评估，填补了现有研究中的空白。 |
| [^24] | [Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models](https://arxiv.org/abs/2402.16438) | 大型语言模型中的语言特定神经元可以解释其多语能力，通过提出语言激活概率熵（LAPE）的检测方法，研究发现LLMs处理特定语言的能力主要由少量神经元决定。 |
| [^25] | [Addressing Order Sensitivity of In-Context Demonstration Examples in Causal Language Models](https://arxiv.org/abs/2402.15637) | 因果语言模型更容易受到上下文示例顺序的影响，为了解决这一挑战，提出了一种信息增强和一致性增强方法。 |
| [^26] | [PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2402.15082) | PEMT 是一种基于多任务迁移学习的参数高效微调框架，通过扩展专家混合框架，以权重组合捕获源任务上训练的适配器，从而有效利用任务特定知识和源目标任务之间的相关性。 |
| [^27] | [Optimizing Language Models for Human Preferences is a Causal Inference Problem](https://arxiv.org/abs/2402.14979) | 本文首次提出将语言模型优化视为一个因果问题，提出了因果偏好优化方法并通过双重稳健CPO(DR-CPO)降低了替代目标的方差。 |
| [^28] | [Understanding and Patching Compositional Reasoning in LLMs](https://arxiv.org/abs/2402.14328) | 本研究通过 Logit Lens 和干预实验揭示了LLMs中隐性推理结果的重要性，开发了一种修补组合推理错误的轻量级方法。 |
| [^29] | [Multi-modal Stance Detection: New Datasets and Model](https://arxiv.org/abs/2402.14298) | 本文研究了多模式立场检测，提出了新的数据集和模型，并展示了所提出的TMPT框架在多模式立场检测中取得了最先进的性能 |
| [^30] | [FanOutQA: Multi-Hop, Multi-Document Question Answering for Large Language Models](https://arxiv.org/abs/2402.14116) | FanOutQA 提出了一个高质量的多跳、多文档问答数据集，用于评估大型语言模型在复杂推理能力上的表现，并发现当代模型在长篇上下文中仍有改进交叉文档依赖推理的空间。 |
| [^31] | [OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems](https://arxiv.org/abs/2402.14008) | 提出了OlympiadBench，一个奥林匹亚级别的双语多模态科学基准，包括8952个问题，旨在评估大型语言模型和多模态模型在复杂问题上的能力。 |
| [^32] | [$\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for In-Context Learning](https://arxiv.org/abs/2402.13874) | 本文提出了$\texttt{Se}^2$，一种顺序感知方法，利用大型语言模型的反馈帮助捕捉示例之间的相互关系和序列信息，显著丰富了上下文学习提示的相关性和相关性。 |
| [^33] | [Soft Self-Consistency Improves Language Model Agents](https://arxiv.org/abs/2402.13212) | Soft Self-Consistency (Soft-SC)通过软化评分标准替代自一致性的多数投票方法，提高了在涉及生成多个动作的长期互动任务中的性能和效率 |
| [^34] | [TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification](https://arxiv.org/abs/2402.12991) | TRAP提出了一种名为Targeted Random Adversarial Prompt (TRAP)的方法，用于识别特定LLM的使用，并在单次交互后以超过95%的真阳性率和低于0.2%的误报率检测目标LLMs。 |
| [^35] | [Tree-Planted Transformers: Large Language Models with Implicit Syntactic Supervision](https://arxiv.org/abs/2402.12691) | 提出了一种新方法 Tree-Planted Transformers (TPT)，通过在 Transformer LMs 的注意权重中隐式地“种植”树木来反映自然语言的句法结构，实现了句法大型语言模型（SLLM）的结合。 |
| [^36] | [Reflect-RL: Two-Player Online RL Fine-Tuning for LMs](https://arxiv.org/abs/2402.12621) | 提出Reflect-RL，使用在线强化学习来微调语言模型，在这过程中引入了反射模型协助，并采用了负例生成、单提示动作枚举和课程学习来提高性能。 |
| [^37] | [The (R)Evolution of Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2402.12451) | 多模态大型语言模型能够无缝整合视觉和文本模态，为生成智能提供了新的可能性。 |
| [^38] | [Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data](https://arxiv.org/abs/2402.12424) | 本研究探讨了LLM在解释表格数据方面的有效性，比较了文本和图像表格表示对LLM性能的影响，为在表格相关任务上有效使用LLM提供了见解。 |
| [^39] | [Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!](https://arxiv.org/abs/2402.12343) | 安全对齐的大型语言模型可能会通过模拟失调框架，在对抗性操纵下产生危险结果，对训练的语言模型具有双倍有害性，高于强基线，强调了即使在安全对齐后也需要重新评估开源语言模型的重要性。 |
| [^40] | [Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation](https://arxiv.org/abs/2402.11894) | 本文提出自动化数据集更新策略，利用两种系统验证过的策略生成看不见和高质量的测试样本，以缓解数据泄漏问题并实现评估稳定性。 |
| [^41] | [Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?](https://arxiv.org/abs/2402.11597) | 大型语言模型在多任务推断时表现出更高的性能，相比单任务推断平均推断时间减少1.46倍，并且在MTI Bench上显示出最多高达12.4%的性能改善。 |
| [^42] | [KMMLU: Measuring Massive Multitask Language Understanding in Korean](https://arxiv.org/abs/2402.11548) | KMMLU是一个新的韩语基准，包含35,030道专家级多选题，从原始韩语考试中收集而来，测试了26个LLM模型，发现这些模型在KMMLU上的表现有很大提升空间。 |
| [^43] | [Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM](https://arxiv.org/abs/2402.11517) | Knowledge-to-SQL框架利用数据专家LLM提供有用知识，增强文本到SQL模型的鲁棒性。 |
| [^44] | [LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation](https://arxiv.org/abs/2402.11485) | LEIA是一种语言适应调整方法，利用维基百科实体名称跨语言增强目标语言语料库，通过左到右的语言建模训练，显著提高了各种非英语语言的表现。 |
| [^45] | [Tasks That Language Models Don't Learn](https://arxiv.org/abs/2402.11349) | 大型语言模型没有学习到语言的视听特性，在新的任务基准测试中表现较差，暴露了人类语言理解与语言模型感官处理能力之间的根本差距。 |
| [^46] | [Contrastive Instruction Tuning](https://arxiv.org/abs/2402.11138) | 提出了对比指令调整方法，通过最大化相似性来提高大型语言模型对未知任务指令的稳健性 |
| [^47] | [When is Tree Search Useful for LLM Planning? It Depends on the Discriminator](https://arxiv.org/abs/2402.10890) | 当前研究通过实验分析了大型语言模型在多步问题求解中使用树搜索的可行性，指出高级规划方法需要鉴别器至少90%准确性才能显著提高性能。 |
| [^48] | [Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning](https://arxiv.org/abs/2402.10639) | 本研究对领域特定适配器混合在领域内评估中的泛化性进行了全面分析，并探讨了混合适配器的内部运作，为适应新领域的性能优化提供了关键洞见 |
| [^49] | [Do Llamas Work in English? On the Latent Language of Multilingual Transformers](https://arxiv.org/abs/2402.10588) | 本研究通过对Llama-2系列变压器模型的研究发现，在多语言语言模型中存在英语作为内部枢纽语言的现象，这有助于理解语言模型的功能方式以及语言偏见的起源。 |
| [^50] | [Direct Preference Optimization with an Offset](https://arxiv.org/abs/2402.10571) | 提出了一种新颖的直接偏好优化方法，即具有偏置的DPO（ODPO），在微调过程中不同对待每个偏好对。 |
| [^51] | [Pushing the Limits of Zero-shot End-to-End Speech Translation](https://arxiv.org/abs/2402.10422) | 引入了ZeroSwot方法，实现了零-shot ST，通过CTC压缩和最优传输，仅利用ASR数据训练语音编码器，并与多语言MT模型在推断时无缝集成，实现直接从语音到文本的翻译。 |
| [^52] | [Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence](https://arxiv.org/abs/2402.10073) | 本论文提出了一个称为\textbf{MoEI}的方法，通过引入一个大规模的EI相关任务集合\textsc{EiBench}并采用模块化的训练过程和情感智力增强器的集成，综合提高了LLM的情感智力（EI）而不损害其普适智能（GI）。 |
| [^53] | [Bridging the Empirical-Theoretical Gap in Neural Network Formal Language Learning Using Minimum Description Length](https://arxiv.org/abs/2402.10013) | 通过使用最小描述长度目标（MDL），解决了神经网络在形式语言学习中的经验-理论差距问题。 |
| [^54] | [Label-Efficient Model Selection for Text Generation](https://arxiv.org/abs/2402.07891) | DiffUse是一种标注效率高的文本生成模型选择方法，它通过聚类文本语义差异的嵌入来选择更具信息量的实例，并能显著减少所需的注释数量。 |
| [^55] | [Mercury: An Efficiency Benchmark for LLM Code Synthesis](https://arxiv.org/abs/2402.07844) | Mercury提出了一个针对LLM代码综合任务的效率评估基准，通过引入新的度量标准Beyond@K来衡量归一化的代码效率，从而鼓励生成功能正确且计算效率高的代码。 |
| [^56] | [T-RAG: Lessons from the LLM Trenches](https://arxiv.org/abs/2402.07483) | T-RAG是一个基于LLM的应用程序，用于私人企业文件问答，它结合了RAG框架和经过微调的开源LLM，并分享了构建和部署过程中的经验教训。 |
| [^57] | [Through the Lens of Split Vote: Exploring Disagreement, Difficulty and Calibration in Legal Case Outcome Classification](https://arxiv.org/abs/2402.07214) | 通过研究分割投票，探索律师在处理法律案件结果分类时面临的意见分歧和困难，并在欧洲人权法院收集了法官的投票数据集进行研究。这项研究还评估了模型和人类之间感知困难的一致性以及模型的置信度和人类校准。 |
| [^58] | [NICE: To Optimize In-Context Examples or Not?](https://arxiv.org/abs/2402.06733) | 通过研究在提供任务特定指令的情况下是否需要优化上下文示例，我们挑战了对于指导性LLMs的共识，并发现在某些任务中，不同的优化上下文示例方法会产生递减的回报。我们引入了"度量标准"，用于衡量从给定指令中学习任务的能力，并提供了一个启发式方法，帮助决定是否优化指令还是ICE用于任何新任务。 |
| [^59] | [MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark](https://arxiv.org/abs/2402.04788) | 本文引入了MLLM作为法官的新基准测试，用于评估MLLM在辅助法官方面的能力。研究结果显示，MLLM在对比评估任务中展示出了非常类人的辨别能力，但在评分评估和批量排序任务中与人类偏好存在显著的差异。此外，即使对于先进模型如GPT-4V，MLLM仍然面临着偏见、幻觉式回答和不一致性等判断方面的挑战。 |
| [^60] | [Multipath parsing in the brain](https://arxiv.org/abs/2401.18046) | 本研究通过将增量生成依存关系解析器的预测与人们进行功能神经成像的时间数据相关，发现了人类在逐词理解句子时存在多路径解析的证据。 |
| [^61] | [Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2401.17263) | 该论文提出了一种鲁棒的提示优化算法（RPO）用于对抗语言模型的破解攻击，通过梯度优化来确保输出的无害性，并成功降低了攻击成功率。 |
| [^62] | [SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models](https://arxiv.org/abs/2401.08295) | 提出了一种共享注意力框架（SAPT），通过共享注意力学习与选择模块对齐PET学习和选择，以同时解决大型语言模型中的灾难性遗忘和知识转移挑战。 |
| [^63] | [WatME: Towards Lossless Watermarking Through Lexical Redundancy](https://arxiv.org/abs/2311.09832) | WatME通过利用词汇冗余的语言先验知识，动态优化语言模型解码过程中的词汇使用，避免适当词汇不可用的情况，维持语言模型的表现力。 |
| [^64] | [TextEE: Benchmark, Reevaluation, Reflections, and Future Challenges in Event Extraction](https://arxiv.org/abs/2311.09562) | 本研究提出了TextEE，这是一个用于事件提取的标准化、公平和可重复的基准，解决了评估中存在的挑战。 |
| [^65] | [GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models](https://arxiv.org/abs/2311.09048) | GRASP是一个新的基准测试，用于评估视频多模态大型语言模型的语言基础和物理理解能力。通过Unity模拟进行两层评估，揭示这些模型在语言基础和直觉物理学能力方面的显著缺陷。 |
| [^66] | [MELA: Multilingual Evaluation of Linguistic Acceptability](https://arxiv.org/abs/2311.09033) | MELA是第一个覆盖10种语言的多语言语言可接受性基准，通过分析XLM-R的微调权重，探讨了跨语言迁移困难性，结果表明在上下文示例方面ChatGPT表现良好但仍落后于经过微调的XLM-R。 |
| [^67] | [ReGAL: Refactoring Programs to Discover Generalizable Abstractions.](http://arxiv.org/abs/2401.16467) | ReGAL提出了一种用于发现通用抽象的程序重构方法，可以通过重构代码学习可重用的函数库，利用这些共享函数库可以更准确地预测程序。 |
| [^68] | [Do Not (Always) Look Right: Investigating the Capabilities of Decoder-Based Large Language Models for Sequence Labeling.](http://arxiv.org/abs/2401.14556) | 本文研究了基于解码器的大型语言模型在序列标注方面的能力，并探索了提高它们性能的策略。 |
| [^69] | [VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks.](http://arxiv.org/abs/2401.13649) | VisualWebArena是一个评估多模态Web代理性能的基准，在真实的“视觉基础任务”上对代理进行了测试。它要求代理准确处理图像-文本输入，解释自然语言指令，并在网站上执行动作来完成用户定义的目标。 |
| [^70] | [Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads.](http://arxiv.org/abs/2401.10774) | Medusa是一个能够提升LLM推理性能的简洁框架，通过增加多个解码头以实现并行预测多个后续标记，并通过树状注意力机制和并行处理来减少解码步骤。 |
| [^71] | [Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on Data-to-Text Generation.](http://arxiv.org/abs/2401.10186) | 开放式大型语言模型在零-shot设置下能够从各种标准数据格式中生成流畅和连贯的文本，但是输出的语义准确性仍然是一个重要问题。 |
| [^72] | [Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation.](http://arxiv.org/abs/2401.06688) | 本文介绍了一种利用质量估计指标合并机器翻译假设的方法，该方法在大型语言模型和多语言翻译模型上提升了翻译质量。通过使用候选池和QE指标，我们的方法能够生成多样且准确的翻译结果。 |
| [^73] | [Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation.](http://arxiv.org/abs/2401.06568) | 本论文研究了大型语言模型（LLMs）如何利用源语言和参考信息评估机器翻译的质量，并发现参考信息显著提高了评估准确性，而源语言信息有时会适得其反，表明在使用LLMs评估翻译时存在跨语言能力不足的问题。 |
| [^74] | [A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way Parallelism.](http://arxiv.org/abs/2401.05749) | 互联网上的大量内容是通过机器翻译多向翻译的，其低质量可能会对使用多语言大型语言模型进行训练产生严重影响。 |
| [^75] | [DebugBench: Evaluating Debugging Capability of Large Language Models.](http://arxiv.org/abs/2401.04621) | 该论文介绍了一个名为DebugBench的LLM调试基准，用于评估大型语言模型的调试能力。研究发现闭源模型与人类相比具有较低的调试性能，而开源模型未能达到合格率。 |
| [^76] | [SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models.](http://arxiv.org/abs/2401.00793) | SecFormer是一个优化框架，旨在实现Transformer模型的快速准确隐私保护推理。通过消除高成本的指数和线性操作，SecFormer能够有效解决在大型语言模型中应用SMPC时的性能问题。 |
| [^77] | [Why Can Large Language Models Generate Correct Chain-of-Thoughts?.](http://arxiv.org/abs/2310.13571) | 本文研究了大型语言模型如何生成连贯的思维链条，并通过建立几何收敛速率的框架来解释它与真实语言来源之间的相似性。这一研究结果为大型语言模型在推理任务中的性能提升提供了理论支持。 |
| [^78] | [AdaLomo: Low-memory Optimization with Adaptive Learning Rate.](http://arxiv.org/abs/2310.10195) | AdaLomo是一种低内存优化方法，通过引入自适应学习率来改善大型语言模型优化器的性能，同时保持内存效率。 |
| [^79] | [Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models.](http://arxiv.org/abs/2310.04406) | 语言代理树搜索（LATS）是一个通用框架，利用大型语言模型（LLMs）的能力在规划、行动和推理方面相互协同，通过使用具有外部反馈的环境，实现更加深思熟虑和适应性的问题解决机制。实验评估表明，LATS在多个领域具有广泛的应用性，特别在编程方面表现出了94.4%的准确率。 |
| [^80] | [Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning.](http://arxiv.org/abs/2310.03309) | 利用大型语言模型进行演绎推理是一个具有挑战性的问题。这篇论文提出了一个简明有序的方法，将任务分解为子任务并且人类化地组织思维，以提高演绎推理的效果。 |
| [^81] | [Self-Specialization: Uncovering Latent Expertise within Large Language Models.](http://arxiv.org/abs/2310.00160) | 该论文研究了大型语言模型的自我特化，通过使用专业领域的数据和少量标记种子进行自我对齐，提高了在目标领域的零样本和少样本性能。 |
| [^82] | [A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future.](http://arxiv.org/abs/2309.15402) | 本文首次全面调查了思维链推理领域的研究，涵盖了构建、结构变体和增强技术等方法分类，以及规划、工具使用和提炼等前沿应用。同时讨论了挑战和未来发展方向。这份调查报告对于在思维链推理领域寻求创新的研究人员来说是一个有价值的资源。 |
| [^83] | [CB-Whisper: Contextual Biasing Whisper using Open-Vocabulary Keyword-Spotting.](http://arxiv.org/abs/2309.09552) | CB-Whisper是一种基于OpenAI的Whisper模型的自动语音识别系统，通过使用开放词汇关键词检测（OV-KWS）识别罕见的命名实体，并使用这些实体作为提示来改进识别效果。实验证明，该方法在提高实体召回率的同时会略微增加混淆错误率（MER）。 |
| [^84] | [Investigating Gender Bias in News Summarization.](http://arxiv.org/abs/2309.08047) | 本研究调查了新闻概述中的性别偏见，发现大型语言模型（LLMs）会重复和强化有害的社会偏见。研究提出了一些方法来量化模型中的有偏行为，并提出了一种生成具有控制人口属性的输入文档的方法。 |
| [^85] | [How does representation impact in-context learning: A exploration on a synthetic task.](http://arxiv.org/abs/2309.06054) | 本研究通过探索表示学习的角度，研究了表示对上下文学习的影响。实验结果表明，在上下文学习中，上下文内部成分对学习性能起到重要作用。 |
| [^86] | [A Small and Fast BERT for Chinese Medical Punctuation Restoration.](http://arxiv.org/abs/2308.12568) | 该论文提出了一种用于中文医学标点修复的快速小型BERT模型。通过结合监督对比学习和辅助预训练任务，该模型在具有较小模型大小的情况下，能够实现与最先进的中文RoBERTa模型相当的95%性能。 |
| [^87] | [Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT.](http://arxiv.org/abs/2308.07876) | 该论文通过利用已建立的注释编码本的知识，探索零样本方法用于政治事件本体关系分类，并介绍一种基于自然语言推理的方法，名为ZSP。ZSP采用了一种树查询框架，提高了解释性、效率和对模式更改的适应性。在细粒度根代码分类上，ZSP的性能明显优于ChatGPT，F1得分提高了40%。 |
| [^88] | [Robust Distortion-free Watermarks for Language Models.](http://arxiv.org/abs/2307.15593) | 该论文提出了一种在语言模型中添加鲁棒无畸变水印的方法，通过映射随机数序列到语言模型的样本，可以实现在不改变文本分布的前提下对水印文本进行检测，并且在多种改写攻击下依然保持较高的鲁棒性，实验证明在40-50%的随机扰动下仍可可靠地检测到水印文本。 |
| [^89] | [Full Parameter Fine-tuning for Large Language Models with Limited Resources.](http://arxiv.org/abs/2306.09782) | 本文提出了一种低内存使用的优化器LOMO，可以实现在有限资源下对大型语言模型进行全参数微调，从而降低研究门槛。 |
| [^90] | [Instruction Tuning with Lexicons for Zero-Shot Style Classification.](http://arxiv.org/abs/2305.14592) | 通过使用词典指导语言模型识别训练期间未见过的新风格，可以有效地提高零样本性能的转移。 |
| [^91] | [What's happening in your neighborhood? A Weakly Supervised Approach to Detect Local News.](http://arxiv.org/abs/2301.08146) | 该论文介绍了一种自动化的本地新闻检测和基于内容的本地新闻推荐方法，通过弱监督框架和自动化数据处理，与传统方法相比具有更高的准确性和覆盖率。 |

# 详细

[^1]: 多语言大型语言模型：语料库、对齐和偏见综述

    A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias

    [https://arxiv.org/abs/2404.00929](https://arxiv.org/abs/2404.00929)

    该论文对多语言大型语言模型进行了全面分析，深入讨论了关键问题，包括多语言语料库、对齐和偏见。

    

    基于大型语言模型（LLMs）的基础上，发展了多语言大型语言模型（MLLMs）来解决多语言自然语言处理任务的挑战，希望实现从高资源到低资源语言的知识转移。然而，仍然存在重要限制和挑战，比如语言不平衡、多语言对齐和固有偏见。本文旨在对MLLMs进行全面分析，深入讨论围绕这些关键问题的议题。

    arXiv:2404.00929v1 Announce Type: cross  Abstract: Based on the foundation of Large Language Models (LLMs), Multilingual Large Language Models (MLLMs) have been developed to address the challenges of multilingual natural language processing tasks, hoping to achieve knowledge transfer from high-resource to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolution, key techniques, and multilingual capacities. Secondly, we explore widely utilized multilingual corpora for MLLMs' training and multilingual datasets oriented for downstream tasks that are crucial for enhancing the cross-lingual capability of MLLMs. Thirdly, we survey the existing studies on multilingual representati
    
[^2]: NaijaHate: 使用代表性数据评估尼日利亚 Twitter 上的仇恨言论检测

    NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using Representative Data

    [https://arxiv.org/abs/2403.19260](https://arxiv.org/abs/2403.19260)

    首次引入 NaijaHate 数据集，在尼日利亚推特上评估 HSD，发现在代表性数据上评估的 HSD 性能高估了真实世界的表现，提出 NaijaXLM-T 模型，突出了域自适应预训练和微调在最大化 HSD 性能中的关键作用

    

    为了解决在线平台上恶意内容蔓延的全球问题，通常会在美国收集的数据集上开发仇恨言论检测（HSD）模型，从而无法推广到来自大多数世界的英语方言。此外，HSD模型通常在策划样本上进行评估，这引发了对在真实环境中高估模型性能的担忧。在这项工作中，我们引入了第一个用于HSD标注的 NaijaHate 数据集，其中包含尼日利亚推文的代表性样本。我们证明，HSD在传统文献中传统使用的有偏见数据集上评估，在代表性数据上很大程度上高估了真实世界的性能。我们还提出了 NaijaXLM-T，一个针对尼日利亚 Twitter 上下文量身定制的预训练模型，并建立了域自适应预训练和微调在最大化HSD性能方面的关键作用。最后，我们表明，在这种情况下，人-机混合方法发挥了关键作用。

    arXiv:2403.19260v1 Announce Type: new  Abstract: To address the global issue of hateful content proliferating in online platforms, hate speech detection (HSD) models are typically developed on datasets collected in the United States, thereby failing to generalize to English dialects from the Majority World. Furthermore, HSD models are often evaluated on curated samples, raising concerns about overestimating model performance in real-world settings. In this work, we introduce NaijaHate, the first dataset annotated for HSD which contains a representative sample of Nigerian tweets. We demonstrate that HSD evaluated on biased datasets traditionally used in the literature largely overestimates real-world performance on representative data. We also propose NaijaXLM-T, a pretrained model tailored to the Nigerian Twitter context, and establish the key role played by domain-adaptive pretraining and finetuning in maximizing HSD performance. Finally, we show that in this context, a human-in-the-l
    
[^3]: NL-ITI：优化探测和干预以改进ITI方法

    NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method

    [https://arxiv.org/abs/2403.18680](https://arxiv.org/abs/2403.18680)

    NL-ITI通过引入非线性探测和多令牌干预，成功改进了ITI方法，在多个多选基准上取得了显著的性能改进。

    

    大语言模型(LLM)容易返回虚假信息，这是人工智能领域的一个主要挑战。本文探讨了推理时干预(Inference-Time-Intervention, ITI)方法引入的范式。首先，ITI方法识别包含最多所需知识类型(例如真实信息)的注意力头。随后，在推理过程中，LLM激活被移动到所选注意力头的子集。我们通过引入非线性探测和多令牌干预-非线性ITI(NL-ITI)进一步改进了ITI框架。NL-ITI在多个多选基准上进行了测试，包括TruthfulQA，我们在这项基准上相对于基线ITI结果报告了约14%的MC1指标改进。NL-ITI还在其他测试集上取得了令人鼓舞的成绩-在MMLU的商业伦理子领域上，比基线LLaMA2-7B有约18%的MC1改进。此外，NL-ITI在效果更好的同时也更少侵入。

    arXiv:2403.18680v1 Announce Type: new  Abstract: Large Language Models (LLM) are prone to returning false information. It constitutes one of major challenges in the AI field. In our work, we explore paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it identifies attention heads, which contain the highest amount of desired type of knowledge (e.g., truthful). Afterwards, during inference, LLM activations are shifted for chosen subset of attention heads. We further improved the ITI framework by introducing a nonlinear probing and multi-token intervention - Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice benchmarks, including TruthfulQA, on which we report around 14% MC1 metric improvement with respect to the baseline ITI results. NL-ITI achieves also encouraging results on other testsets - on Business Ethics subdomain of MMLU, around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI performs better while being less invasive in t
    
[^4]: 论据感知事件链接方法

    Argument-Aware Approach To Event Linking

    [https://arxiv.org/abs/2403.15097](https://arxiv.org/abs/2403.15097)

    引入论据感知方法改进事件链接模型，能更好地识别和分类不在知识库中的事件提及，弥补了这一领域的研究空白。

    

    arXiv:2403.15097v1 公告类型: 跨领域 摘要: 事件链接将文本中的事件提及与知识库（KB）中相关节点连接起来。先前在事件链接方面的研究主要借鉴了实体链接的方法，忽略了事件的独特特征。与广泛探讨的实体链接任务相比，事件具有更加复杂的结构，可以通过检查其关联的论据更有效地加以区分。此外，事件的信息丰富性导致事件知识库的稀缺性。这强调了事件链接模型需要识别和分类不在知识库中的事件提及作为“超出知识库”的重要性，而这一领域受到了有限关注。在这项工作中，我们通过引入一个论据感知方法来应对这些挑战。首先，我们通过标记事件论据信息来改进事件链接模型，有助于识别有关事件提及的关键信息。随后，为了帮助模型处理“超出知识库”

    arXiv:2403.15097v1 Announce Type: cross  Abstract: Event linking connects event mentions in text with relevant nodes in a knowledge base (KB). Prior research in event linking has mainly borrowed methods from entity linking, overlooking the distinct features of events. Compared to the extensively explored entity linking task, events have more complex structures and can be more effectively distinguished by examining their associated arguments. Moreover, the information-rich nature of events leads to the scarcity of event KBs. This emphasizes the need for event linking models to identify and classify event mentions not in the KB as ``out-of-KB,'' an area that has received limited attention. In this work, we tackle these challenges by introducing an argument-aware approach. First, we improve event linking models by augmenting input text with tagged event argument information, facilitating the recognition of key information about event mentions. Subsequently, to help the model handle ``out-
    
[^5]: Wav2Gloss：从语音生成分词后的文字注释

    Wav2Gloss: Generating Interlinear Glossed Text from Speech

    [https://arxiv.org/abs/2403.13169](https://arxiv.org/abs/2403.13169)

    Wav2Gloss提出了从语音中自动提取语言注释的任务，并引入了第一个数据集Fieldwork，分析表明预先训练的解码器有助于翻译和注释，并且端到端的系统效果较好。

    

    世界上成千上万种语言面临灭绝的危险，这对文化身份和人类语言多样性构成了巨大威胁。分词后的文字注释（IGT）是一种语言注释形式，可以支持对这些语言社区进行文档编制和资源创建。IGT通常包括（1）转录，（2）形态分割，（3）文本注释 和（4）到主流语言的自由翻译。我们提出了Wav2Gloss：一个任务，从语音中自动提取这四个注释组件，并引入了第一个数据集Fieldwork，这是一个包含37种语言的语音语料库，所有这些都有标准格式和训练/评估集划分。

    arXiv:2403.13169v1 Announce Type: new  Abstract: Thousands of the world's languages are in danger of extinction--a tremendous threat to cultural identities and human language diversity. Interlinear Glossed Text (IGT) is a form of linguistic annotation that can support documentation and resource creation for these languages' communities. IGT typically consists of (1) transcriptions, (2) morphological segmentation, (3) glosses, and (4) free translations to a majority language. We propose Wav2Gloss: a task to extract these four annotation components automatically from speech, and introduce the first dataset to this end, Fieldwork: a corpus of speech with all these annotations covering 37 languages with standard formatting and train/dev/test splits. We compare end-to-end and cascaded Wav2Gloss methods, with analysis suggesting that pre-trained decoders assist with translation and glossing, that multi-task and multilingual approaches are underperformant, and that end-to-end systems perform 
    
[^6]: DRAGIN：基于大型语言模型实时信息需求的动态检索增强生成

    DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models

    [https://arxiv.org/abs/2403.10081](https://arxiv.org/abs/2403.10081)

    提出了一种新框架DRAGIN，旨在解决大型语言模型在文本生成过程中动态检索和生成中存在的问题。

    

    动态检索增强生成（RAG）范式在大型语言模型（LLMs）的文本生成过程中主动决定何时以及何时检索。该范式的两个关键元素是确定激活检索模块的最佳时机（决定何时检索）以及一旦触发检索，制定适当的查询（确定要检索什么）。然而，当前动态RAG方法在两个方面都存在不足。首先，决定何时进行检索的策略通常依赖于静态规则。此外，决定要检索什么的策略通常局限于LLM的最近一句或最后几个标记，而LLM的实时信息需求可能跨越整个上下文。为克服这些局限性，我们引入了一个新框架DRAGIN， 即基于LLMs实时信息需求的动态检索增强生成。

    arXiv:2403.10081v1 Announce Type: new  Abstract: Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs). There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve). However, current dynamic RAG methods fall short in both aspects. Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's real-time information needs may span across the entire context. To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the real-time Information Needs of LLMs. Our framework is specif
    
[^7]: LiveCodeBench：用于代码的大型语言模型的全面和无污染评估

    LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code

    [https://arxiv.org/abs/2403.07974](https://arxiv.org/abs/2403.07974)

    LiveCodeBench提出了一个全面的、无污染的LLMs评估工具，聚焦于从LeetCode、AtCoder和CodeForces等平台连续收集的新问题，覆盖自修复、代码执行、测试输出预测等更广泛的代码相关能力。

    

    大型语言模型（LLMs）应用于与代码相关的应用程序已经成为一个突出的领域，吸引了学术界和工业界的极大兴趣。然而，随着新的和改进的LLMs的开发，现有的评估基准（例如HumanEval，MBPP）不再足以评估它们的能力。在这项工作中，我们提出LiveCodeBench，这是一个全面的、无污染的LLMs评估工具，用于代码，它会从三个竞赛平台（LeetCode、AtCoder和CodeForces）上连续地收集新问题。值得注意的是，我们的基准还着重关注更广泛的与代码相关的能力，如自修复、代码执行和测试输出预测，而不仅仅是代码生成。目前，LiveCodeBench托管了在2023年5月至2024年2月之间发布的400个高质量编码问题。我们已经评估了9个基本LLMs和20个指令调整的LLMs。

    arXiv:2403.07974v1 Announce Type: cross  Abstract: Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and February 2024. We have evaluated 9 base LLMs and 20 instruction-tuned LLMs o
    
[^8]: ERA-CoT: 通过实体关系分析改进思维链

    ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis

    [https://arxiv.org/abs/2403.06932](https://arxiv.org/abs/2403.06932)

    ERA-CoT 提出了一种新颖的方法，通过捕获实体之间的关系和支持思维链，帮助大型语言模型(LLMs)理解上下文，提高了多样任务的推理准确性。

    

    大型语言模型在各种自然语言处理任务中取得了可观的成就。然而，当处理涉及多个实体的复杂场景时，LLMs 仍然面临重大挑战。这些挑战源于存在需要多步推理的隐式关系。在本文中，我们提出了一种新颖的方法 ERA-CoT，通过捕获实体之间的关系来帮助 LLMs 理解上下文，并通过思维链（CoT）支持不同任务的推理。实验结果表明，与当前的 CoT 提示方法相比，ERA-CoT 表现出我们提出的方法在 GPT3.5 上平均比以前的 SOTA 基线实现了显著的 5.1% 改进的卓越性能。我们的分析表明，ERA-CoT 提高了LLM对实体关系的理解，显著提高了问题回答的准确性。

    arXiv:2403.06932v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks. However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step reasoning. In this paper, we propose a novel approach ERA-CoT, which aids LLMs in understanding context by capturing relationships between entities and supports the reasoning of diverse tasks through Chain-of-Thoughts (CoT). Experimental results show that ERA-CoT demonstrates the superior performance of our proposed method compared to current CoT prompting methods, achieving a significant improvement of an average of 5.1\% on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that ERA-CoT increases the LLM's understanding of entity relationships, significantly improves the accuracy of question answering
    
[^9]: RA-ISF: 通过迭代自反馈学习检索增强以回答和理解

    RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback

    [https://arxiv.org/abs/2403.06840](https://arxiv.org/abs/2403.06840)

    通过迭代自反馈的检索增强方法在指定任务的特定场景中提高模型性能，优于现有基准模型，显著增强了事实推理能力。

    

    大型语言模型(LLMs)在许多任务中表现出色，但仍然严重依赖存储在其参数中的知识。检索增强生成(RAG)方法通过整合外部知识来解决这一问题。模型可以通过检索与查询相关的知识来回答以前无法回答的问题。本文提出了检索增强迭代自反馈(RA-ISF)框架，通过三个子模块迭代分解任务并处理它们，以增强模型的问题解决能力。实验证明，我们的方法优于现有基准，在诸如GPT3.5、Llama2之类的模型上表现良好，显著增强了事实推理能力。

    arXiv:2403.06840v1 Announce Type: cross  Abstract: Large language models (LLMs) demonstrate exceptional performance in numerous tasks but still heavily rely on knowledge stored in their parameters. Moreover, updating this knowledge incurs high training costs. Retrieval-augmented generation (RAG) methods address this issue by integrating external knowledge. The model can answer questions it couldn't previously by retrieving knowledge relevant to the query. This approach improves performance in certain scenarios for specific tasks. However, if irrelevant texts are retrieved, it may impair model performance. In this paper, we propose Retrieval Augmented Iterative Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and processes them in three submodules to enhance the model's problem-solving capabilities. Experiments show that our method outperforms existing benchmarks, performing well on models like GPT3.5, Llama2, significantly enhancing factual reasoning capabilities a
    
[^10]: 讲述，而不是展示！：语言指导有助于在图像和视频领域之间进行转移

    Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos

    [https://arxiv.org/abs/2403.05535](https://arxiv.org/abs/2403.05535)

    该论文提出了LaGTran框架，利用文本描述来引导知识转移，在处理具有挑战性的数据集上表现出显著优势。

    

    我们介绍了LaGTran，这是一个新颖的框架，利用即可获得或易于获取的文本描述，引导从带标签的源数据到具有域偏移的无标签目标数据的鲁棒性知识转移。受到我们观察到更富语义的文本模态具有更有利的转移特性的启发，我们设计了一个转移机制，使用源训练的文本分类器在目标文本描述上生成预测，并利用这些预测作为相应图像的监督。我们的方法以语言指导为驱动，出奇地简单易行，却在具有挑战性的数据集如GeoNet和DomainNet上显著优于以往所有方法，验证了其极其有效性。

    arXiv:2403.05535v1 Announce Type: cross  Abstract: We introduce LaGTran, a novel framework that utilizes readily available or easily acquired text descriptions to guide robust transfer of discriminative knowledge from labeled source to unlabeled target data with domain shifts. While unsupervised adaptation methods have been established to address this problem, they show limitations in handling challenging domain shifts due to their exclusive operation within the pixel-space. Motivated by our observation that semantically richer text modality has more favorable transfer properties, we devise a transfer mechanism to use a source-trained text-classifier to generate predictions on the target text descriptions, and utilize these predictions as supervision for the corresponding images. Our approach driven by language guidance is surprisingly easy and simple, yet significantly outperforms all prior approaches on challenging datasets like GeoNet and DomainNet, validating its extreme effectiven
    
[^11]: PARADISE：通过过程警告和提示数据集评估语言模型的隐式规划能力

    PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset

    [https://arxiv.org/abs/2403.03167](https://arxiv.org/abs/2403.03167)

    论文提出了一个名为PARADISE的任务，通过实用程序文本进行演绎推理，评估语言模型仅从给定目标推断计划的能力

    

    最近，社区对于大型语言模型是否具备规划或执行计划的能力越发感兴趣。然而，大多数先前研究使用LLMs为简化场景生成高级计划，适应度量缺乏语言复杂性和领域多样性，限制其规划能力的分析。为了解决这一问题，我们提出了PARADISE，这是一个使用Q＆A格式的演绎推理任务，采用来自wikiHow的实用程序文本。它涉及与目标直接相关的警告和提示推断任务，排除中间步骤，旨在测试模型仅从给定目标推断计划的隐含知识的能力。

    arXiv:2403.03167v1 Announce Type: new  Abstract: Recently, there has been growing interest within the community regarding whether large language models are capable of planning or executing plans. However, most prior studies use LLMs to generate high-level plans for simplified scenarios lacking linguistic complexity and domain diversity, limiting analysis of their planning abilities. These setups constrain evaluation methods (e.g., predefined action space), architectural choices (e.g., only generative models), and overlook the linguistic nuances essential for realistic analysis. To tackle this, we present PARADISE, an abductive reasoning task using Q\&A format on practical procedural text sourced from wikiHow. It involves warning and tip inference tasks directly associated with goals, excluding intermediary steps, with the aim of testing the ability of the models to infer implicit knowledge of the plan solely from the given goal. Our experiments, utilizing fine-tuned language models and
    
[^12]: CoGenesis：一个协作大型和小型语言模型的框架，用于安全的上下文感知指令跟随

    CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following

    [https://arxiv.org/abs/2403.03129](https://arxiv.org/abs/2403.03129)

    提出了一个协作生成框架CoGenesis，整合大型和小型模型，以逻辑方式解决隐私问题。

    

    随着语言模型（LMs）的发展，它们接触私人数据的可能性越来越不可避免，它们的部署（尤其是较小的模型）在个人设备上，如PC和智能手机上，已成为一种盛行趋势。在充满用户信息的环境中，使模型既能保护用户隐私又能高效执行命令成为一项重要的研究课题。本文提出了CoGenesis，一个协作生成框架，集成了大型模型（托管在云基础设施上）和小型模型（部署在本地设备上），以逻辑方式解决隐私问题。最初，我们设计了一个管道来创建个性化的写作指导数据集，其中包含了丰富的上下文细节，作为这一研究问题的测试基础。随后，我们介绍了基于草图和对数的两个CoGenesis变体。我们的实验结果基于我们合成的数据集和另外两个op

    arXiv:2403.03129v1 Announce Type: new  Abstract: With the advancement of language models (LMs), their exposure to private data is increasingly inevitable, and their deployment (especially for smaller ones) on personal devices, such as PCs and smartphones, has become a prevailing trend. In contexts laden with user information, enabling models to both safeguard user privacy and execute commands efficiently emerges as an essential research imperative. In this paper, we propose CoGenesis, a collaborative generation framework integrating large (hosted on cloud infrastructure) and small models (deployed on local devices) to address privacy concerns logically. Initially, we design a pipeline to create personalized writing instruction datasets enriched with extensive context details as the testbed of this research issue. Subsequently, we introduce two variants of CoGenesis based on sketch and logits respectively. Our experimental findings, based on our synthesized dataset and two additional op
    
[^13]: 观点是我的，也是你的：使用共同基础评估心灵理论

    Views Are My Own, But Also Yours: Benchmarking Theory of Mind using Common Ground

    [https://arxiv.org/abs/2403.02451](https://arxiv.org/abs/2403.02451)

    介绍了第一个基于自然口语对话的ToM数据集，展示了LM在ToM方面的困难，并表明在Common-ToM上整合简单明确的信念表示可以提高LM的性能。

    

    最近，评估语言模型（LMs）的心灵理论（ToM）能力引起了很多关注。然而，许多现有的基准测试依赖于合成数据，这可能导致实验结果与人类行为不一致。我们引入了第一个基于自然发生的口头对话的ToM数据集，Common-ToM，并展示LMs在展示ToM方面存在困难。然后，我们展示了在Common-ToM上整合一个简单明确的信念表示可以提高LM的性能。

    arXiv:2403.02451v1 Announce Type: new  Abstract: Evaluating the theory of mind (ToM) capabilities of language models (LMs) has recently received much attention. However, many existing benchmarks rely on synthetic data which risks misaligning the resulting experiments with human behavior. We introduce the first ToM dataset based on naturally occurring spoken dialogs, Common-ToM, and show that LMs struggle to demonstrate ToM. We then show that integrating a simple, explicit representation of beliefs improves LM performance on Common-ToM.
    
[^14]: RIFF: 学习为语言模型的少样本微调改写输入

    RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models

    [https://arxiv.org/abs/2403.02271](https://arxiv.org/abs/2403.02271)

    通过训练少样本释义模型并在训练和测试时用释义丰富数据，可以提高语言模型的性能，超出仅通过参数高效微调的效果。

    

    预训练语言模型（PLMs）可以精确地为下游文本处理任务进行微调。最近，研究人员引入了几种参数高效的微调方法，优化输入提示或调整少量模型参数（例如 LoRA）。在本研究中，我们探讨了改变原始任务的输入文本与参数高效微调方法相结合的影响。为了最有效地重写输入文本，我们使用最大边际似然目标训练了一个少样本释义模型。使用六个少样本文本分类数据集，我们展示了在训练和测试时用释义丰富数据可以提高性能，超出了仅通过参数高效微调可以实现的性能。

    arXiv:2403.02271v1 Announce Type: new  Abstract: Pre-trained Language Models (PLMs) can be accurately fine-tuned for downstream text processing tasks. Recently, researchers have introduced several parameter-efficient fine-tuning methods that optimize input prompts or adjust a small number of model parameters (e.g LoRA). In this study, we explore the impact of altering the input text of the original task in conjunction with parameter-efficient fine-tuning methods. To most effectively rewrite the input text, we train a few-shot paraphrase model with a Maximum-Marginal Likelihood objective. Using six few-shot text classification datasets, we show that enriching data with paraphrases at train and test time enhances the performance beyond what can be achieved with parameter-efficient fine-tuning alone.
    
[^15]: VariErr NLI: 将注释错误与人类标签变化区分开来

    VariErr NLI: Separating Annotation Error from Human Label Variation

    [https://arxiv.org/abs/2403.01931](https://arxiv.org/abs/2403.01931)

    该研究提出了一个新的方法和数据集VariErr，专注于NLI任务中的注释错误和人类标签变化的区分。研究填补了在处理信号非黑白情况下的先前空白。

    

    人类标签变化是由于注释者出于有效原因将不同标签分配给同一项而产生的，而注释错误是指由于无效原因分配标签。这两个问题在自然语言处理基准中普遍存在，但现有研究通常是孤立研究它们。据我们所知，以前没有专注于区分错误与信号的先前工作，特别是在信号超越黑白之处。为了填补这一空白，我们介绍了一种系统方法和一个新数据集VariErr（变异与错误），重点关注英语NLI任务。我们提出了一个包含两轮注释方案的方法，注释者解释每个标签，然后判断标签解释对的有效性。VariErr包含对500个重新注释的NLI项目上的1,933个解释进行的7,574个有效性判断。我们评估了各种自动错误检测（AED）方法和GPT在揭示错误与信号之间的有效性。

    arXiv:2403.01931v1 Announce Type: new  Abstract: Human label variation arises when annotators assign different labels to the same item for valid reasons, while annotation errors occur when labels are assigned for invalid reasons. These two issues are prevalent in NLP benchmarks, yet existing research has studied them in isolation. To the best of our knowledge, there exists no prior work that focuses on teasing apart error from signal, especially in cases where signal is beyond black-and-white. To fill this gap, we introduce a systematic methodology and a new dataset, VariErr (variation versus error), focusing on the NLI task in English. We propose a 2-round annotation scheme with annotators explaining each label and subsequently judging the validity of label-explanation pairs. \name{} contains 7,574 validity judgments on 1,933 explanations for 500 re-annotated NLI items. We assess the effectiveness of various automatic error detection (AED) methods and GPTs in uncovering errors versus 
    
[^16]: DINER：使用多变量因果推断来去偏方面级情感分析

    DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable Causal Inference

    [https://arxiv.org/abs/2403.01166](https://arxiv.org/abs/2403.01166)

    本论文提出了一种基于多变量因果推断的新框架，用于去偏方面级情感分析，从而解决神经网络模型学习虚假相关性的问题。

    

    尽管取得了显著进展，基于神经网络的方面级情感分析（ABSA）模型容易从注释偏见中学习到虚假相关性，导致在对抗性数据转换上鲁棒性较差。在去偏解决方案中，基于因果推断的方法引起了许多研究关注，主要可分为因果干预方法和反事实推理方法。然而，目前大多数去偏方法都集中在单变量因果推断上，这对于具有两个输入变量（目标方面和评论）的ABSA并不适用。在本文中，我们提出了一个基于多变量因果推断的新框架用于去偏ABSA。在这个框架中，不同类型的偏见基于不同的因果干预方法得到处理。对于评论分支，偏见被建模为来自上下文的间接混杂，其中实施反向调整干预。

    arXiv:2403.01166v1 Announce Type: cross  Abstract: Though notable progress has been made, neural-based aspect-based sentiment analysis (ABSA) models are prone to learn spurious correlations from annotation biases, resulting in poor robustness on adversarial data transformations. Among the debiasing solutions, causal inference-based methods have attracted much research attention, which can be mainly categorized into causal intervention methods and counterfactual reasoning methods. However, most of the present debiasing methods focus on single-variable causal inference, which is not suitable for ABSA with two input variables (the target aspect and the review). In this paper, we propose a novel framework based on multi-variable causal inference for debiasing ABSA. In this framework, different types of biases are tackled based on different causal intervention methods. For the review branch, the bias is modeled as indirect confounding from context, where backdoor adjustment intervention is 
    
[^17]: STAR: 使用动态主动学习约束LoRA，实现大型语言模型数据高效微调

    STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models

    [https://arxiv.org/abs/2403.01165](https://arxiv.org/abs/2403.01165)

    本论文提出了一种新颖的方法，有效地将基于不确定性的主动学习和LoRA相结合，以解决大型语言模型数据高效微调中遇到的问题。

    

    大型语言模型(LLMs)通过提示方法展示了少样本学习的强大能力，但对于复杂推理任务仍需监督训练。针对LLMs的参数众多和内存消耗大问题，分别提出了参数高效微调(PEFT)方法和内存高效微调方法。然而，数据高效微调旨在解决大量注释数据消耗的问题，却鲜有研究。一种明显的方式是将PEFT方法与主动学习相结合。然而，实验结果表明这种组合并非简单，并产生较差的结果。通过探针实验，这一观察结果可能由两个主要原因解释：不确定性差距和模型校准不佳。因此，在本文中，我们提出了一种新颖的方法，有效地将基于不确定性的主动学习和LoRA进行整合。

    arXiv:2403.01165v1 Announce Type: cross  Abstract: Though Large Language Models (LLMs) have demonstrated the powerful capabilities of few-shot learning through prompting methods, supervised training is still necessary for complex reasoning tasks. Because of their extensive parameters and memory consumption, both Parameter-Efficient Fine-Tuning (PEFT) methods and Memory-Efficient Fine-Tuning methods have been proposed for LLMs. Nevertheless, the issue of large annotated data consumption, the aim of Data-Efficient Fine-Tuning, remains unexplored. One obvious way is to combine the PEFT method with active learning. However, the experimental results show that such a combination is not trivial and yields inferior results. Through probe experiments, such observation might be explained by two main reasons: uncertainty gap and poor model calibration. Therefore, in this paper, we propose a novel approach to effectively integrate uncertainty-based active learning and LoRA. Specifically, for the u
    
[^18]: 学习生成用于零shot任务适应的指令调优数据集

    Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation

    [https://arxiv.org/abs/2402.18334](https://arxiv.org/abs/2402.18334)

    Bonito是一种用于生成指令调优训练数据集的模型，通过将未注释的文本转换为特定任务训练数据，实现大型语言模型对用户专属数据的零shot任务适应，并显著提高了预训练和指令调整模型的平均性能。

    

    我们介绍了Bonito，这是一个开源模型，用于条件任务生成：将未注释的文本转换为用于指令调优的特定任务训练数据集。我们的目标是在用户专门的私人数据上实现大型语言模型的零shot任务适应。我们使用1.65M个示例的新大规模数据集训练Bonito，该数据集是通过将现有的指令调优数据集重新混合成元模板而创建的。数据集的元模板产生训练示例，其中输入是未注释的文本和任务属性，输出包括指令和响应。我们使用Bonito为七个专业领域的数据集生成合成任务，跨三种任务类型 -- 是非问答、抽取式问答和自然语言推理 -- 并调整语言模型。我们展示了Bonito显著改善了预训练和指令调整模型的平均性能。

    arXiv:2402.18334v1 Announce Type: new  Abstract: We introduce Bonito, an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for instruction tuning. Our goal is to enable zero-shot task adaptation of large language models on users' specialized, private data. We train Bonito on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains across three task types -- yes-no question answering, extractive question answering, and natural language inference -- and adapt language models. We show that Bonito significantly improves the average performance of pretrained and instruction tuned mode
    
[^19]: 评估量化大型语言模型

    Evaluating Quantized Large Language Models

    [https://arxiv.org/abs/2402.18158](https://arxiv.org/abs/2402.18158)

    该论文通过全面评估后训练量化对权重、激活和KV缓存的影响，以指导选择量化方法，并对11种模型系列进行了评估，覆盖了多种任务类型。

    

    后训练量化（PTQ）已经成为减少大型语言模型（LLMs）成本的一种有前景的技术，具体地，PTQ可以有效地减轻LLMs中的内存消耗并降低计算开销。为了满足各种场景下高效率和性能的要求，对量化LLMs进行全面评估是必要的，以指导量化方法的选择。本文通过评估PTQ对11个模型系列（包括OPT、LLaMA2、Falcon、Bloomz、Mistral、ChatGLM、Vicuna、LongChat、StableLM、Gemma和Mamba）的权重、激活和KV缓存的影响，范围从125M到180B，全面评估了这些因素。评估涵盖了五种类型的任务：基础NLP、突然出现的能力、可靠性、对话和长上下文任务。此外，我们还评估了最先进的量化方法，以展示它们的应用。

    arXiv:2402.18158v1 Announce Type: cross  Abstract: Post-training quantization (PTQ) has emerged as a promising technique to reduce the cost of large language models (LLMs). Specifically, PTQ can effectively mitigate memory consumption and reduce computational overhead in LLMs. To meet the requirements of both high efficiency and performance across diverse scenarios, a comprehensive evaluation of quantized LLMs is essential to guide the selection of quantization methods. This paper presents a thorough evaluation of these factors by evaluating the effect of PTQ on Weight, Activation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with parameters ranging from 125M to 180B. The evaluation encompasses five types of tasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context tasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization methods to demonstrate their appli
    
[^20]: 具有增强可检测性和语义连贯性的大型语言模型的特定令牌水印技术

    Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models

    [https://arxiv.org/abs/2402.18059](https://arxiv.org/abs/2402.18059)

    提出一种利用多目标优化方法的水印技术，通过轻量级网络生成特定令牌水印logits和分割比率，在保证检测性的同时提升了文本的语义完整性。

    

    大型语言模型生成高质量的响应，潜在地存在误导信息的问题，强调了通过区分人工智能生成和人类撰写的文本来加以规范的必要性。水印技术在这种情况下至关重要，它涉及在LLM推理阶段向文本中嵌入隐藏标记，而这对人类来说是不可感知的。然而，当前的水印算法面临着实现插入水印的可检测性和生成文本的语义完整性两方面的挑战，增强其中一个方面常常会损害另一个方面。为了克服这一问题，我们引入了一种新颖的多目标优化（MOO）方法，用于水印技术，利用轻量级网络生成特定令牌水印logits和分割比率。通过利用MOO来优化检测和语义目标函数，我们的方法同时实现了可检测性和语义完整性。实验结果表明，我们的方法在...

    arXiv:2402.18059v1 Announce Type: cross  Abstract: Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Current watermarking algorithms, however, face the challenge of achieving both the detectability of inserted watermarks and the semantic integrity of generated texts, where enhancing one aspect often undermines the other. To overcome this, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that ou
    
[^21]: 变分学习对大型深度网络有效

    Variational Learning is Effective for Large Deep Networks

    [https://arxiv.org/abs/2402.17641](https://arxiv.org/abs/2402.17641)

    变分学习在大型深度网络中展现出非常好的效果，IVON优化器在训练大型网络时几乎能与Adam相媲美甚至胜过它，且预测不确定性更准确，对模型微调、泛化误差预测和数据敏感性估计均有显著改进。

    

    我们提供了大量实证证据，反驳了变分学习对大型神经网络无效的普遍看法。我们展示了一种名为Improved Variational Online Newton (IVON)的优化器，在训练大型网络（如GPT-2和ResNets）时始终能够与Adam相匹配或胜过它。IVON的计算成本几乎与Adam相同，但其预测不确定性更好。我们展示了IVON的几种新用例，其中我们改进了大型语言模型的微调和模型合并，在准确预测泛化误差和忠实估计对数据的敏感性方面。我们找到了大量支持变分学习有效性的证据。

    arXiv:2402.17641v1 Announce Type: cross  Abstract: We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks. We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch. IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better. We show several new use cases of IVON where we improve fine-tuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data. We find overwhelming evidence in support of effectiveness of variational learning.
    
[^22]: 食谱的深度学习命名实体识别模型

    Deep Learning Based Named Entity Recognition Models for Recipes

    [https://arxiv.org/abs/2402.17447](https://arxiv.org/abs/2402.17447)

    该研究基于深度学习，针对食谱文本开发了命名实体识别模型，通过系统的数据处理和分析，构建了用于自动生成新食谱的数据集。

    

    食物通过各种努力方式影响着我们的生活，包括口味、营养、健康和可持续性。食谱是通过非结构化文本代代相传的文化胶囊。自动识别命名实体的协议，即食谱文本的基本组成部分，对于各种应用来说都具有巨大价值，从信息提取到新颖食谱生成。命名实体识别是一种从已知标签的非结构化或半结构化数据中提取信息的技术。我们从手动注释的6,611个成分短语的数据开始，累积创建了26,445个短语的增强数据集。同时，我们系统地清理和分析了来自RecipeDB的成分短语，这是黄金标准的食谱数据存储库，并使用Stanford NER进行了标注。基于分析，我们使用基于聚类的方法对88,526个短语的子集进行了取样，同时保留了多样性。

    arXiv:2402.17447v1 Announce Type: cross  Abstract: Food touches our lives through various endeavors, including flavor, nourishment, health, and sustainability. Recipes are cultural capsules transmitted across generations via unstructured text. Automated protocols for recognizing named entities, the building blocks of recipe text, are of immense value for various applications ranging from information extraction to novel recipe generation. Named entity recognition is a technique for extracting information from unstructured or semi-structured data with known labels. Starting with manually-annotated data of 6,611 ingredient phrases, we created an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we systematically cleaned and analyzed ingredient phrases from RecipeDB, the gold-standard recipe data repository, and annotated them using the Stanford NER. Based on the analysis, we sampled a subset of 88,526 phrases using a clustering-based approach while preserving the diversity
    
[^23]: 大型语言模型量化策略的全面评估

    A Comprehensive Evaluation of Quantization Strategies for Large Language Models

    [https://arxiv.org/abs/2402.16775](https://arxiv.org/abs/2402.16775)

    该研究提出了一个结构化评估框架，针对大型语言模型的量化策略展开全面评估，填补了现有研究中的空白。

    

    增加大型语言模型（LLMs）中的参数数量通常会在下游任务中提高性能，但会增加计算和存储成本，在资源有限的情况下部署变得困难。量化技术通过减少模型权重或激活所需的位数，并最小化性能损失，已经因LLMs的兴起而变得流行。然而，大多数量化研究使用预训练的LLMs，量化对调整过指令的LLMs的影响以及量化LLMs的困惑度与基准性能之间的关系尚不明确。对量化LLMs的评估通常仅限于语言建模和少数分类任务，其在其他基准上的性能尚不清楚。为填补这些空白，我们提出了一个结构化评估框架，包括三个关键维度：（1）知识和容量，（2）对齐性和（3）效率，并进行了广泛的实验。

    arXiv:2402.16775v1 Announce Type: new  Abstract: Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings. Quantization techniques, which reduce the bits needed for model weights or activations with minimal performance loss, have become popular due to the rise of LLMs. However, most quantization studies use pre-trained LLMs, and the impact of quantization on instruction-tuned LLMs and the relationship between perplexity and benchmark performance of quantized LLMs are not well understood. Evaluation of quantized LLMs is often limited to language modeling and a few classification tasks, leaving their performance on other benchmarks unclear. To address these gaps, we propose a structured evaluation framework consisting of three critical dimensions: (1) knowledge \& capacity, (2) alignment, and (3) efficiency, and conduct extensive experi
    
[^24]: 语言特定神经元：大型语言模型多语能力的关键

    Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models

    [https://arxiv.org/abs/2402.16438](https://arxiv.org/abs/2402.16438)

    大型语言模型中的语言特定神经元可以解释其多语能力，通过提出语言激活概率熵（LAPE）的检测方法，研究发现LLMs处理特定语言的能力主要由少量神经元决定。

    

    大型语言模型(LLMs)展现出显著的多语能力，即使未经过专门策划的多语平行语料库的预训练。解释LLMs处理多语文本的基本机制仍然是一个具有挑战性的问题。在本文中，我们深入研究了LLMs中Transformer架构的构成，以找出语言特定区域。具体而言，我们提出了一种新颖的检测方法，即语言激活概率熵（LAPE），用于识别LLMs内的语言特定神经元。基于LAPE，我们对两个代表性的LLMs，即LLaMA-2和BLOOM进行了全面实验。我们的研究结果表明，LLMs处理特定语言的能力主要是由一小部分神经元决定的，这些神经元主要位于模型的顶部和底部层。此外，我们展示了通过选择性激活或停用神经元来“引导”LLMs的输出语言的可行性。

    arXiv:2402.16438v1 Announce Type: new  Abstract: Large language models (LLMs) demonstrate remarkable multilingual capabilities without being pre-trained on specially curated multilingual parallel corpora. It remains a challenging problem to explain the underlying mechanisms by which LLMs process multilingual texts. In this paper, we delve into the composition of Transformer architectures in LLMs to pinpoint language-specific regions. Specially, we propose a novel detection method, language activation probability entropy (LAPE), to identify language-specific neurons within LLMs. Based on LAPE, we conduct comprehensive experiments on two representative LLMs, namely LLaMA-2 and BLOOM. Our findings indicate that LLMs' proficiency in processing a particular language is predominantly due to a small subset of neurons, primarily situated in the models' top and bottom layers. Furthermore, we showcase the feasibility to "steer" the output language of LLMs by selectively activating or deactivatin
    
[^25]: 处理因果语言模型中上下文示例对顺序敏感性的问题

    Addressing Order Sensitivity of In-Context Demonstration Examples in Causal Language Models

    [https://arxiv.org/abs/2402.15637](https://arxiv.org/abs/2402.15637)

    因果语言模型更容易受到上下文示例顺序的影响，为了解决这一挑战，提出了一种信息增强和一致性增强方法。

    

    在自然语言处理中，上下文学习已经成为一种流行的范式。然而，其性能可能会受到上下文示例顺序的显著影响。在本文中，我们发现因果语言模型（CausalLMs）对此顺序比前缀语言模型（PrefixLMs）更敏感。我们将这一现象归因于CausalLMs中的自回归注意力掩模，这些掩模限制每个标记不能访问随后的标记的信息。这导致不同位置的样本具有不同的感受野，从而导致不同位置的表征差异。为了应对这一挑战，我们引入了一种无监督微调方法，称为信息增强和一致性增强方法。该方法利用对比学习来对齐不同位置上下文示例的表征，并引入一致性损失以确保相似性。

    arXiv:2402.15637v1 Announce Type: new  Abstract: In-context learning has become a popular paradigm in natural language processing. However, its performance can be significantly influenced by the order of in-context demonstration examples. In this paper, we found that causal language models (CausalLMs) are more sensitive to this order compared to prefix language models (PrefixLMs). We attribute this phenomenon to the auto-regressive attention masks within CausalLMs, which restrict each token from accessing information from subsequent tokens. This results in different receptive fields for samples at different positions, thereby leading to representation disparities across positions. To tackle this challenge, we introduce an unsupervised fine-tuning method, termed the Information-Augmented and Consistency-Enhanced approach. This approach utilizes contrastive learning to align representations of in-context examples across different positions and introduces a consistency loss to ensure simi
    
[^26]: PEMT: 多任务相关性引导的专家混合模型实现了参数高效的迁移学习

    PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning

    [https://arxiv.org/abs/2402.15082](https://arxiv.org/abs/2402.15082)

    PEMT 是一种基于多任务迁移学习的参数高效微调框架，通过扩展专家混合框架，以权重组合捕获源任务上训练的适配器，从而有效利用任务特定知识和源目标任务之间的相关性。

    

    针对参数高效微调（PEFT）作为将预训练语言模型有效地适应各种任务的方法已经崛起。最近，人们对从一个或多个任务转移知识到下游目标任务以实现性能提升产生了越来越浓厚的兴趣。然而，当前方法通常要么在各个任务上训练适配器，要么从源任务中提取共享知识，未能充分利用任务特定知识和源任务与目标任务之间的相关性。为了克服这些限制，我们提出了PEMT，这是一种基于多任务迁移学习的创新参数高效微调框架。PEMT将专家混合（MoE）框架扩展为源任务上训练的适配器的加权组合以捕获可转移知识。这些权重由一个门控单元确定，利用任务之间的相关性来测量目标任务和每个源任务之间的相关性。

    arXiv:2402.15082v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as an effective method for adapting pre-trained language models to various tasks efficiently. Recently, there has been a growing interest in transferring knowledge from one or multiple tasks to the downstream target task to achieve performance improvements. However, current approaches typically either train adapters on individual tasks or distill shared knowledge from source tasks, failing to fully exploit task-specific knowledge and the correlation between source and target tasks. To overcome these limitations, we propose PEMT, a novel parameter-efficient fine-tuning framework based on multi-task transfer learning. PEMT extends the mixture-of-experts (MoE) framework to capture the transferable knowledge as a weighted combination of adapters trained on source tasks. These weights are determined by a gated unit, measuring the correlation between the target and each source task using task 
    
[^27]: 优化语言模型以符合人类偏好是一个因果推断问题

    Optimizing Language Models for Human Preferences is a Causal Inference Problem

    [https://arxiv.org/abs/2402.14979](https://arxiv.org/abs/2402.14979)

    本文首次提出将语言模型优化视为一个因果问题，提出了因果偏好优化方法并通过双重稳健CPO(DR-CPO)降低了替代目标的方差。

    

    随着大型语言模型(LLMs)在学术和商业领域的广泛应用，越来越多的人对允许语言模型生成符合人类偏好文本的方法产生了兴趣。本文首次探索了从直接结果数据集中针对人类偏好进行语言模型优化，其中每个样本由一段文本和一个衡量读者响应的相关数值结果组成。我们首次提出应将语言模型优化视为一个因果问题，以确保模型正确学习文本与结果之间的关系。我们正式化了这个因果语言优化问题，并开发了一种方法--因果偏好优化(CPO)--来解决该问题的无偏替代目标。我们进一步使用双重稳健的CPO(DR-CPO)扩展CPO，降低了替代目标的方差，同时保留了明显强有力的保证。

    arXiv:2402.14979v1 Announce Type: cross  Abstract: As large language models (LLMs) see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences. In this paper, we present an initial exploration of language model optimization for human preferences from direct outcome datasets, where each sample consists of a text and an associated numerical outcome measuring the reader's response. We first propose that language model optimization should be viewed as a causal problem to ensure that the model correctly learns the relationship between the text and the outcome. We formalize this causal language optimization problem, and we develop a method--causal preference optimization (CPO)--that solves an unbiased surrogate objective for the problem. We further extend CPO with doubly robust CPO (DR-CPO), which reduces the variance of the surrogate objective while retaining provably strong guarante
    
[^28]: 理解和修补LLMs中的组成推理

    Understanding and Patching Compositional Reasoning in LLMs

    [https://arxiv.org/abs/2402.14328](https://arxiv.org/abs/2402.14328)

    本研究通过 Logit Lens 和干预实验揭示了LLMs中隐性推理结果的重要性，开发了一种修补组合推理错误的轻量级方法。

    

    LLMs 在推理任务中遇到困难，本研究通过 Logit Lens 和干预实验深入研究了LLMs的内部隐藏状态，发现多数推理失败源自于不正确生成或利用的隐性推理结果。我们的研究揭示了隐性推理结果在中间层中的出现，并对最终显式推理结果的形成起到因果作用。我们的探索进一步发现了 MHSA 模块在这些层中的存在，成为准确生成和利用隐性推理结果的关键。基于以上发现，我们开发了 CREME，一种轻量级方法，通过编辑位于的 MHSA 模块来修补组合推理中的错误。

    arXiv:2402.14328v1 Announce Type: new  Abstract: LLMs have marked a revolutonary shift, yet they falter when faced with compositional reasoning tasks. Our research embarks on a quest to uncover the root causes of compositional reasoning failures of LLMs, uncovering that most of them stem from the improperly generated or leveraged implicit reasoning results. Inspired by our empirical findings, we resort to Logit Lens and an intervention experiment to dissect the inner hidden states of LLMs. This deep dive reveals that implicit reasoning results indeed surface within middle layers and play a causative role in shaping the final explicit reasoning results. Our exploration further locates multi-head self-attention (MHSA) modules within these layers, which emerge as the linchpins in accurate generation and leveraing of implicit reasoning results. Grounded on the above findings, we develop CREME, a lightweight method to patch errors in compositional reasoning via editing the located MHSA modu
    
[^29]: 多模式立场检测：新数据集和模型

    Multi-modal Stance Detection: New Datasets and Model

    [https://arxiv.org/abs/2402.14298](https://arxiv.org/abs/2402.14298)

    本文研究了多模式立场检测，提出了新的数据集和模型，并展示了所提出的TMPT框架在多模式立场检测中取得了最先进的性能

    

    立场检测是一项具有挑战性的任务，旨在从社交媒体平台中识别针对特定目标的公众意见。以往的立场检测工作主要集中在纯文本上。本文研究了包含文本和图像的推文的多模式立场检测，这在当今快速增长的社交媒体平台上是普遍存在的，人们经常发布多模式消息。为此，我们基于Twitter创建了五个新的不同领域的多模式立场检测数据集，其中每个示例包含文本和图像。此外，我们提出了一个简单而有效的目标多模式提示调整（TMPT）框架，其中利用目标信息从文本和视觉模态中学习多模式立场特征。对我们的三个基准数据集的实验证结果表明，所提出的TMPT在多模式立场检测中实现了最先进的性能。

    arXiv:2402.14298v1 Announce Type: new  Abstract: Stance detection is a challenging task that aims to identify public opinion from social media platforms with respect to specific targets. Previous work on stance detection largely focused on pure texts. In this paper, we study multi-modal stance detection for tweets consisting of texts and images, which are prevalent in today's fast-growing social media platforms where people often post multi-modal messages. To this end, we create five new multi-modal stance detection datasets of different domains based on Twitter, in which each example consists of a text and an image. In addition, we propose a simple yet effective Targeted Multi-modal Prompt Tuning framework (TMPT), where target information is leveraged to learn multi-modal stance features from textual and visual modalities. Experimental results on our three benchmark datasets show that the proposed TMPT achieves state-of-the-art performance in multi-modal stance detection.
    
[^30]: FanOutQA：用于大型语言模型的多跳、多文档问答

    FanOutQA: Multi-Hop, Multi-Document Question Answering for Large Language Models

    [https://arxiv.org/abs/2402.14116](https://arxiv.org/abs/2402.14116)

    FanOutQA 提出了一个高质量的多跳、多文档问答数据集，用于评估大型语言模型在复杂推理能力上的表现，并发现当代模型在长篇上下文中仍有改进交叉文档依赖推理的空间。

    

    一种常见于日常场景中的问题类型是“fan-out”问题，即复杂的多跳、多文档推理问题，需要找到大量实体的信息。然而，目前很少有资源可以评估大型语言模型在这种问题回答能力上的表现。为了更全面地评估LLMs中的复杂推理能力，我们提出了FanOutQA，这是一个高质量的fan-out问题-答案对数据集，包括英文维基百科作为知识库的人工注释分解。我们在数据集上制定了三种基准设置，并对7个LLMs进行了基准测试，包括GPT-4、LLaMA 2、Claude-2.1和Mixtral-8x7B，发现当代模型在长篇上下文中仍有改进推理跨文档依赖的空间。我们提供我们的数据集和开源工具来运行模型，以鼓励在https://fanoutqa.com上进行评估。

    arXiv:2402.14116v1 Announce Type: cross  Abstract: One type of question that is commonly found in day-to-day scenarios is ``fan-out'' questions, complex multi-hop, multi-document reasoning questions that require finding information about a large number of entities. However, there exist few resources to evaluate this type of question-answering capability among large language models. To evaluate complex reasoning in LLMs more fully, we present FanOutQA, a high-quality dataset of fan-out question-answer pairs and human-annotated decompositions with English Wikipedia as the knowledge base. We formulate three benchmark settings across our dataset and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B, finding that contemporary models still have room to improve reasoning over inter-document dependencies in a long context. We provide our dataset and open-source tools to run models to encourage evaluation at https://fanoutqa.com
    
[^31]: OlympiadBench：一个具有奥林匹亚级别双语多模态科学问题的挑战性基准

    OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems

    [https://arxiv.org/abs/2402.14008](https://arxiv.org/abs/2402.14008)

    提出了OlympiadBench，一个奥林匹亚级别的双语多模态科学基准，包括8952个问题，旨在评估大型语言模型和多模态模型在复杂问题上的能力。

    

    最近的进展使得大型语言模型（LLMs）和大型多模态模型（LMMs）在各种任务中超越了一般人类的能力，接近了多个领域人类专家的熟练水平。本文提出了OlympiadBench，一个奥林匹亚级别的双语多模态科学基准，包括来自奥林匹亚级别数学和物理竞赛以及中国高考的8952个问题。每个问题都配有专家级注释，以进行逐步的推理。在OlympiadBench上评估顶尖模型，我们实施了全面的评估方法，以准确评估模型的响应。值得注意的是，表现最佳的模型GPT-4V在OlympiadBench上获得了17.23%的平均分，其中在物理学中仅为11.28%。

    arXiv:2402.14008v1 Announce Type: new  Abstract: Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.23% on OlympiadBench, with a mere 11.28% in physics, hig
    
[^32]: $\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for In-Context Learning

    $\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for In-Context Learning

    [https://arxiv.org/abs/2402.13874](https://arxiv.org/abs/2402.13874)

    本文提出了$\texttt{Se}^2$，一种顺序感知方法，利用大型语言模型的反馈帮助捕捉示例之间的相互关系和序列信息，显著丰富了上下文学习提示的相关性和相关性。

    

    众所周知，大型语言模型（LLMs）在上下文学习（ICL）中具有出色的能力，但需要通过示例示范来激活。以往的研究广泛探讨了用于ICL的示例选择，主要遵循“先选择再组织”的范式，这些方法往往忽视示例之间的内在关系，存在训练和推理之间的不一致性。本文将问题表述为一个序贯选择问题，并引入$\texttt{Se}^2$，这是一种顺序感知方法，利用LLM对不同上下文的反馈，有助于捕捉示例之间的相互关系和序列信息，显著丰富ICL提示的上下文相关性和相关性。同时，我们利用束搜索来寻找和构建示例序列，增强了质量和多样性。我们在8个不同类别中的23个NLP任务上进行了大量实验

    arXiv:2402.13874v1 Announce Type: new  Abstract: The remarkable capability of large language models (LLMs) for in-context learning (ICL) needs to be activated by demonstration examples. Prior work has extensively explored the selection of examples for ICL, predominantly following the "select then organize" paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference. In this paper, we formulate the problem as a $\textit{se}$quential $\textit{se}$lection problem and introduce $\texttt{Se}^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts. Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity. Extensive experiments across 23 NLP tasks from 8 distinct categories i
    
[^33]: 软自一致性改善语言模型代理

    Soft Self-Consistency Improves Language Model Agents

    [https://arxiv.org/abs/2402.13212](https://arxiv.org/abs/2402.13212)

    Soft Self-Consistency (Soft-SC)通过软化评分标准替代自一致性的多数投票方法，提高了在涉及生成多个动作的长期互动任务中的性能和效率

    

    大型语言模型（LLMs）生成可以通过对多个解决方案进行抽样和评分来改进，以选择最终答案。当前的“抽样和选择”方法如自一致性（SC）依赖于多数投票来评分答案。然而，当任务有许多不同且有效的答案时，通过投票进行选择需要大量样本。这使得SC在涉及顺序生成多个动作（答案）的互动任务时成本过高。在确定大多数投票未能为此类任务提供一致的收益之后，我们展示了如何通过软化评分标准来提高成功率。我们引入了软自一致性（Soft-SC），它用模型可能性计算连续分数来取代SC的不连续评分，即使动作分布稀疏，也允许选择。软自一致性在长期互动任务上提高了性能和效率，需要较少的样本和投票。

    arXiv:2402.13212v1 Announce Type: cross  Abstract: Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current "sample and select" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (Soft-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requi
    
[^34]: TRAP: 面向黑盒身份验证的有针对性随机对抗提示诱饵

    TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification

    [https://arxiv.org/abs/2402.12991](https://arxiv.org/abs/2402.12991)

    TRAP提出了一种名为Targeted Random Adversarial Prompt (TRAP)的方法，用于识别特定LLM的使用，并在单次交互后以超过95%的真阳性率和低于0.2%的误报率检测目标LLMs。

    

    大型语言模型（LLM）服务和模型通常伴随着关于谁可以使用它们以及他们必须如何使用它们的法律规定。评估发布的LLMs的合规性是至关重要的，因为这些规定保护了LLM贡献者的利益并防止了滥用。在这种背景下，我们描述了黑盒身份验证（BBIV）的新问题。其目标是确定第三方应用是否通过其聊天功能使用某个特定的LLM。我们提出了一种名为目标随机对抗提示（TRAP）的方法，用于识别正在使用的具体LLM。我们重新利用了最初用于越狱的对抗性后缀，以从目标LLM获得预定义的答案，而其他模型则给出随机答案。TRAP可以在单次交互后以超过95%的真阳性率和低于0.2%的误报率检测目标LLMs。即使LLM有不会显著改变的细微变化，TRAP仍然有效。

    arXiv:2402.12991v1 Announce Type: cross  Abstract: Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them. Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse. In this context, we describe the novel problem of Black-box Identity Verification (BBIV). The goal is to determine whether a third-party application uses a certain LLM through its chat function. We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers. TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the LLM has minor changes that do not significantly alter the
    
[^35]: 树种植变压器：具有隐式句法监督的大型语言模型

    Tree-Planted Transformers: Large Language Models with Implicit Syntactic Supervision

    [https://arxiv.org/abs/2402.12691](https://arxiv.org/abs/2402.12691)

    提出了一种新方法 Tree-Planted Transformers (TPT)，通过在 Transformer LMs 的注意权重中隐式地“种植”树木来反映自然语言的句法结构，实现了句法大型语言模型（SLLM）的结合。

    

    大型语言模型（LLMs）在大规模文本语料上的可扩展性取得了显著成功，但在训练效率方面存在一些缺点。相比之下，句法语言模型（SLMs）可以通过句法监督高效训练，达到相对较高的性能，但在可扩展性方面存在困难。因此，鉴于LLMs和SLMs的互补优势，有必要开发一种将LLMs的可扩展性与SLMs的训练效率结合起来的体系结构，即句法大型语言模型（SLLM）。在本文中，我们提出一种名为“树种植”的新方法：在Transformer LMs的注意权重中暗示地“种植”树木，以反映自然语言的句法结构。具体而言，通过树种植训练的Transformer LMs将被称为树种植变压器（TPT），它们通过树种植在小型树库上学习语法，然后通过继续在大规模文本语料上进行缩放。

    arXiv:2402.12691v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved remarkable success thanks to scalability on large text corpora, but have some drawback in training efficiency. In contrast, Syntactic Language Models (SLMs) can be trained efficiently to reach relatively high performance thanks to syntactic supervision, but have trouble with scalability. Thus, given these complementary advantages of LLMs and SLMs, it is necessary to develop an architecture that integrates the scalability of LLMs with the training efficiency of SLMs, namely Syntactic Large Language Models (SLLM). In this paper, we propose a novel method dubbed tree-planting: implicitly "plant" trees into attention weights of Transformer LMs to reflect syntactic structures of natural language. Specifically, Transformer LMs trained with tree-planting will be called Tree-Planted Transformers (TPT), which learn syntax on small treebanks via tree-planting and then scale on large text corpora via conti
    
[^36]: Reflect-RL：两个玩家在线RL微调语言模型

    Reflect-RL: Two-Player Online RL Fine-Tuning for LMs

    [https://arxiv.org/abs/2402.12621](https://arxiv.org/abs/2402.12621)

    提出Reflect-RL，使用在线强化学习来微调语言模型，在这过程中引入了反射模型协助，并采用了负例生成、单提示动作枚举和课程学习来提高性能。

    

    随着语言模型在各个领域展示其能力，将它们应用于需要多轮交互的任务变得越来越受欢迎。这些任务通常具有复杂的动态性，因此仅在有限的离线数据集上进行监督微调（SFT）无法取得良好性能。然而，只有少数研究尝试在交互式决策制定环境内直接对LM进行训练。我们旨在创建一个在这些环境中使用在线强化学习（RL）对LM进行微调的有效机制。我们提出了Reflect-RL，一个两个玩家的系统，使用在线RL对LM进行微调，在此过程中，一个冻结的反射模型辅助策略模型。为了为热身SFT阶段生成数据，我们使用负例生成来增强反射模型的纠错能力。此外，我们设计了单提示动作枚举，并应用了课程学习让策略模型学习更多。

    arXiv:2402.12621v1 Announce Type: cross  Abstract: As language models (LMs) demonstrate their capabilities in various fields, their application to tasks requiring multi-round interactions has become increasingly popular. These tasks usually have complex dynamics, so supervised fine-tuning (SFT) on a limited offline dataset does not yield good performance. However, only a few works attempted to directly train the LMs within interactive decision-making environments. We aim to create an effective mechanism to fine-tune LMs with online reinforcement learning (RL) in these environments. We propose Reflect-RL, a two-player system to fine-tune an LM using online RL, where a frozen reflection model assists the policy model. To generate data for the warm-up SFT stage, we use negative example generation to enhance the error-correction ability of the reflection model. Furthermore, we designed single-prompt action enumeration and applied curriculum learning to allow the policy model to learn more 
    
[^37]: 多模态大型语言模型的革命：一项调查

    The (R)Evolution of Multimodal Large Language Models: A Survey

    [https://arxiv.org/abs/2402.12451](https://arxiv.org/abs/2402.12451)

    多模态大型语言模型能够无缝整合视觉和文本模态，为生成智能提供了新的可能性。

    

    连接文本和视觉模态在生成智能中扮演着重要角色。受大型语言模型成功的启发，目前已有大量研究工作致力于开发多模态大型语言模型（MLLMs）。这些模型可以无缝地整合视觉和文本模态，同时作为输入和输出，提供基于对话的接口和遵循指令的能力。本文全面审查了最近基于视觉的MLLMs，分析了它们的架构选择、多模态对齐策略和训练技术。我们还对这些模型在各种任务上进行了详细分析，包括视觉定位、图像生成和编辑、视觉理解以及特定领域的应用。此外，我们编制并描述了训练数据集和评估基准，对现有模型进行了比较。

    arXiv:2402.12451v1 Announce Type: cross  Abstract: Connecting text and visual modalities plays an essential role in generative intelligence. For this reason, inspired by the success of large language models, significant research efforts are being devoted to the development of Multimodal Large Language Models (MLLMs). These models can seamlessly integrate visual and textual modalities, both as input and output, while providing a dialogue-based interface and instruction-following capabilities. In this paper, we provide a comprehensive review of recent visual-based MLLMs, analyzing their architectural choices, multimodal alignment strategies, and training techniques. We also conduct a detailed analysis of these models across a wide range of tasks, including visual grounding, image generation and editing, visual understanding, and domain-specific applications. Additionally, we compile and describe training datasets and evaluation benchmarks, conducting comparisons among existing models in 
    
[^38]: 表格作为图片？探讨LLM在多模态表格数据表示上的优势和局限性

    Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data

    [https://arxiv.org/abs/2402.12424](https://arxiv.org/abs/2402.12424)

    本研究探讨了LLM在解释表格数据方面的有效性，比较了文本和图像表格表示对LLM性能的影响，为在表格相关任务上有效使用LLM提供了见解。

    

    在本文中，我们通过不同的提示策略和数据格式研究了各种LLM在解释表格数据方面的有效性。我们的分析涵盖了六个针对与表格相关任务的基准，如问答和事实核查。我们首次介绍了LLM在基于图像的表格表示上的表现评估。具体地，我们比较了五种基于文本和三种基于图像的表格表示，展示了表示和提示对LLM性能的影响。我们的研究为在表格相关任务上有效使用LLM提供了见解。

    arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
    
[^39]: 模拟失调: 大型语言模型的安全对齐可能会适得其反！

    Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!

    [https://arxiv.org/abs/2402.12343](https://arxiv.org/abs/2402.12343)

    安全对齐的大型语言模型可能会通过模拟失调框架，在对抗性操纵下产生危险结果，对训练的语言模型具有双倍有害性，高于强基线，强调了即使在安全对齐后也需要重新评估开源语言模型的重要性。

    

    大型语言模型（LLMs）需要进行安全对齐，以确保与人类进行安全的对话。然而，在这项工作中，我们引入了一种推理时攻击框架，表明安全对齐也可能在对抗性操纵下无意中促成有害结果。这个框架被命名为模拟失调（ED），在输出空间中不良地组合了一对开源预训练和安全对齐的语言模型，产生了一个有害的语言模型而无需任何训练。我们对ED在三个数据集和四个模型系列（Llama-1、Llama-2、Mistral和Alpaca）上的实验表明，ED使预训练模型的有害性增加了一倍，并胜过强基线，以较大优势在48个评估子集中的43个中实现了最高的有害率。至关重要的是，我们的研究结果凸显了即使在安全对齐后，重新评估开源语言模型实践的重要性。

    arXiv:2402.12343v1 Announce Type: new  Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.
    
[^40]: 你见过我吗？自动化数据集更新以实现可靠及及时的评估

    Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation

    [https://arxiv.org/abs/2402.11894](https://arxiv.org/abs/2402.11894)

    本文提出自动化数据集更新策略，利用两种系统验证过的策略生成看不见和高质量的测试样本，以缓解数据泄漏问题并实现评估稳定性。

    

    由于大型语言模型（LLMs）的能力不断扩大和预训练数据的增加，LLMs面临着日益严重的评估挑战。一方面，数据泄漏问题导致对现有基准的过度估计。另一方面，定期手动整理数据集成本高昂。本文提出自动化数据集更新以实现可靠及及时的评估。基本思想是基于现有样本生成看不见的高质量测试样本以减轻泄漏问题。具体而言，我们提出了两种策略并进行了系统验证。第一种是模仿策略，利用LLMs创建类似现有样本的新样本，最大程度地保留原始数据集的风格。我们的实验表明它在多次实例中的评估稳定性以及在大多数情况下处理数据泄漏问题的有效性。

    arXiv:2402.11894v1 Announce Type: new  Abstract: Due to the expanding capabilities and pre-training data, Large Language Models (LLMs) are facing increasingly serious evaluation challenges. On one hand, the data leakage issue cause over-estimation on existing benchmarks. On the other hand, periodically curating datasets manually is costly. In this paper, we propose to automate dataset updates for reliable and timely evaluation. The basic idea is to generate unseen and high-quality testing samples based on existing ones to mitigate leakage issues. In specific, we propose two strategies with systematically verification. First, the mimicking strategy employs LLMs to create new samples resembling existing ones, to the maximum extent preserving the stylistic of the original dataset. Our experiments demonstrate its evaluation stability across multiple instantiations and its effectiveness in dealing with data leakage issues in most cases. Second, for the cases that mimicking dataset works poo
    
[^41]: 多任务推断: 大型语言模型能够同时遵循多个指令吗？

    Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?

    [https://arxiv.org/abs/2402.11597](https://arxiv.org/abs/2402.11597)

    大型语言模型在多任务推断时表现出更高的性能，相比单任务推断平均推断时间减少1.46倍，并且在MTI Bench上显示出最多高达12.4%的性能改善。

    

    大型语言模型（LLMs）通常被要求在每次推断调用中遵循单个指令。在这项工作中，我们分析了LLMs是否也具有处理多个指令的能力，称为多任务推断。为此，我们引入了MTI Bench（多任务推断基准），一个包括25个任务的5000个实例的综合评估基准。MTI Bench中的每个任务都涉及2到3个子任务。正如预期的那样，我们首先证明了多任务推断平均降低了1.46倍的总推断时间，因为它不需要多次推断调用。有趣的是，与预期LLMs在任务被划分时表现更好相反，我们发现最先进的LLMs，例如Llama-2-Chat-70B和GPT-4，在MTI Bench上通过多任务推断与单任务推断相比可以获得高达7.3％和12.4％的性能改善。我们发布了MTI Bench。

    arXiv:2402.11597v1 Announce Type: new  Abstract: Large language models (LLMs) are typically prompted to follow a single instruction per inference call. In this work, we analyze whether LLMs also hold the capability to handle multiple instructions simultaneously, denoted as Multi-Task Inference. For this purpose, we introduce the MTI Bench(Multi-Task Inference Benchmark), a comprehensive evaluation benchmark encompassing 5,000 instances across 25 tasks. Each task in the MTI Bench involves 2 to 3 sub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces the total inference time by 1.46 times in average since it does not require multiple inference calls. Interestingly, contrary to the expectation that LLMs would perform better when tasks are divided, we find that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3% and 12.4% improved performance with Multi-Task Inference compared to Single-Task Inference on the MTI Bench. We release the MTI Bench
    
[^42]: KMMLU：在韩语中测量大规模多任务语言理解

    KMMLU: Measuring Massive Multitask Language Understanding in Korean

    [https://arxiv.org/abs/2402.11548](https://arxiv.org/abs/2402.11548)

    KMMLU是一个新的韩语基准，包含35,030道专家级多选题，从原始韩语考试中收集而来，测试了26个LLM模型，发现这些模型在KMMLU上的表现有很大提升空间。

    

    我们提出了KMMLU，这是一个新的韩语基准，涵盖了来自人文科学到STEM的45个学科的35,030道专家级多项选择题。与之前从现有英语基准翻译而来的韩语基准不同，KMMLU是从原始韩语考试中收集的，捕捉了韩语的语言和文化方面。我们测试了26个公开可用的和专有的LLM，发现有很大的改进空间。最好的公开模型在KMMLU上的准确率为50.54%，远低于平均人类表现的62.6%。这个模型主要是针对英语和中文进行训练的，而不是韩语。目前针对韩语的LLM，如Polyglot-Ko，表现得更糟。令人惊讶的是，即使是最有能力的专有LLM，例如GPT-4和HyperCLOVA X，也只分别达到了59.95%和53.40%。这表明需要进一步的工作来改进韩语LLM，而KMMLU提供了追踪这一进展的正确工具。

    arXiv:2402.11548v1 Announce Type: new  Abstract: We propose KMMLU, a new Korean benchmark with 35,030 expert-level multiple-choice questions across 45 subjects ranging from humanities to STEM. Unlike previous Korean benchmarks that are translated from existing English benchmarks, KMMLU is collected from original Korean exams, capturing linguistic and cultural aspects of the Korean language. We test 26 publically available and proprietary LLMs, identifying significant room for improvement. The best publicly available model achieves 50.54% on KMMLU, far below the average human performance of 62.6%. This model was primarily trained for English and Chinese, not Korean. Current LLMs tailored to Korean, such as Polyglot-Ko, perform far worse. Surprisingly, even the most capable proprietary LLMs, e.g., GPT-4 and HyperCLOVA X, achieve 59.95% and 53.40%, respectively. This suggests that further work is needed to improve Korean LLMs, and KMMLU offers the right tool to track this progress. We mak
    
[^43]: 知识到SQL：用数据专家LLM增强SQL生成

    Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM

    [https://arxiv.org/abs/2402.11517](https://arxiv.org/abs/2402.11517)

    Knowledge-to-SQL框架利用数据专家LLM提供有用知识，增强文本到SQL模型的鲁棒性。

    

    为用户查询生成准确的SQL（文本到SQL）是一个长期存在的问题，因为生成SQL需要理解查询和数据库，然后根据数据库检索准确的数据。现有模型依赖于大型语言模型（LLMs）的综合能力，根据数据库模式生成SQL。然而，有些必要的知识没有明确包含在数据库模式中，或者被LLMs学习了。因此，知识不足的查询生成的SQL可能是不准确的，这会对文本到SQL模型的鲁棒性产生负面影响。为了应对这种情况，我们提出了Knowledge-to-SQL框架，该框架采用定制的数据专家LLM（DELLM）为所有类型的文本到SQL模型提供有用的知识。具体地，我们详细介绍了DELLM的设计，包括表格读取和基本微调过程。

    arXiv:2402.11517v1 Announce Type: new  Abstract: Generating accurate SQL for user queries (text-to-SQL) is a long-standing problem since the generation of the SQL requires comprehending the query and database and retrivale the accurate data from the database accordingly. Existing models rely on the comprehensive ability of Large Language Models (LLMs) to generate the SQL according to the database schema. However, there is some necessary knowledge that is not explicitly included in the database schema or has been learned by LLMs. Thus, the generated SQL of the knowledge-insufficient queries may be inaccurate, which negatively impacts the robustness of the text-to-SQL models. To deal with this situation, we propose the Knowledge-to-SQL framework, which employs tailored Data Expert LLM (DELLM) to provide helpful knowledge for all types of text-to-SQL models. Specifically, we provide the detailed design of DELLM, in terms of table reading, and the basic fine-tuning process. We further prov
    
[^44]: LEIA: 利用基于实体的数据增强在语言模型中促进跨语言知识转移

    LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation

    [https://arxiv.org/abs/2402.11485](https://arxiv.org/abs/2402.11485)

    LEIA是一种语言适应调整方法，利用维基百科实体名称跨语言增强目标语言语料库，通过左到右的语言建模训练，显著提高了各种非英语语言的表现。

    

    将基于英语的大型语言模型（LLMs）适应其他语言的操作由于跨语言转移的效率和潜力而变得越来越受欢迎。然而，现有的语言适应方法常常忽视跨语言监督的好处。在本研究中，我们介绍LEIA，一种利用跨语言对齐的维基百科实体名称的语言适应调整方法。该方法涉及使用英语实体名称增强目标语言语料库，并使用从左到右的语言建模训练模型。我们在多样的问答数据集上评估LEIA，使用7B参数的LLMs，展示了在各种非英语语言上的显著性能增益。源代码可在https://github.com/studio-ousia/leia上获得。

    arXiv:2402.11485v1 Announce Type: cross  Abstract: Adapting English-based large language models (LLMs) to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer. However, existing language adaptation methods often overlook the benefits of cross-lingual supervision. In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages. This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling. We assess LEIA on diverse question answering datasets using 7B-parameter LLMs, demonstrating significant performance gains across various non-English languages. The source code is available at https://github.com/studio-ousia/leia.
    
[^45]: 语言模型未学习的任务

    Tasks That Language Models Don't Learn

    [https://arxiv.org/abs/2402.11349](https://arxiv.org/abs/2402.11349)

    大型语言模型没有学习到语言的视听特性，在新的任务基准测试中表现较差，暴露了人类语言理解与语言模型感官处理能力之间的根本差距。

    

    我们认为，我们目前的大型语言模型（LLMs）没有学习到语言的某些特性。我们通过一系列任务（称为H-TEST）对语言的视听特性进行了实证研究。这一基准测试突显了人类语言理解与LLMs的感官受限处理能力之间的根本差距。支持我们的假设，1. 故意推理（思维链），2. 少量案例，或3. 同一模型系列的更强大LLM（LLaMA 2 13B->LLaMA 2 70B）并不能简单地带来H-TEST性能的改善。因此，我们特别将其与玛丽的哲学案例联系起来，她在感官受限环境中了解世界（Jackson，1986）。我们的实验表明，一些最强大的专有LLMs的表现接近于随机基准准确率50％，突显了极限。

    arXiv:2402.11349v1 Announce Type: cross  Abstract: We argue that there are certain properties of language that our current large language models (LLMs) don't learn. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-TEST. This benchmark highlights a fundamental gap between human linguistic comprehension, which naturally integrates sensory experiences, and the sensory-deprived processing capabilities of LLMs. In support of our hypothesis, 1. deliberate reasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the same model family (LLaMA 2 13B -> LLaMA 2 70B) do not trivially bring improvements in H-TEST performance. Therefore, we make a particular connection to the philosophical case of Mary, who learns about the world in a sensory-deprived environment (Jackson, 1986). Our experiments show that some of the strongest proprietary LLMs stay near random chance baseline accuracy of 50%, highlighting the limit
    
[^46]: 对比指令调整

    Contrastive Instruction Tuning

    [https://arxiv.org/abs/2402.11138](https://arxiv.org/abs/2402.11138)

    提出了对比指令调整方法，通过最大化相似性来提高大型语言模型对未知任务指令的稳健性

    

    指令调整一直被用作改善大型语言模型（LLMs）在未知任务上的性能的一种有前途的方法。然而，当前的LLMs在面临未知指令时表现出有限的稳健性，当相同的指令以稍微变化的形式或语言风格提出时会产生不一致的输出。这种行为表明LLMs对文本变化的稳健性和对未知指令的泛化能力不足，可能会导致可信度问题。因此，我们提出了对比指令调整，该方法在最大化语义上等价的指令-实例对的隐藏表示之间的相似性的同时，最小化语义上不同的对之间的相似性。为了促进这种方法，我们通过释义任务指令，扩充现有的FLAN集合。在PromptBench基准测试上的实验表明，对比指令调整（CoIN）一直提高了LLMs对未知指令的稳健性

    arXiv:2402.11138v1 Announce Type: cross  Abstract: Instruction tuning has been used as a promising approach to improve the performance of large language models (LLMs) on unseen tasks. However, current LLMs exhibit limited robustness to unseen instructions, generating inconsistent outputs when the same instruction is phrased with slightly varied forms or language styles. This behavior indicates LLMs' lack of robustness to textual variations and generalizability to unseen instructions, potentially leading to trustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning, which maximizes the similarity between the hidden representations of semantically equivalent instruction-instance pairs while minimizing the similarity between semantically different ones. To facilitate this approach, we augment the existing FLAN collection by paraphrasing task instructions. Experiments on the PromptBench benchmark show that CoIN consistently improves LLMs' robustness to unseen instructio
    
[^47]: LLM规划中树搜索何时有用？取决于鉴别器

    When is Tree Search Useful for LLM Planning? It Depends on the Discriminator

    [https://arxiv.org/abs/2402.10890](https://arxiv.org/abs/2402.10890)

    当前研究通过实验分析了大型语言模型在多步问题求解中使用树搜索的可行性，指出高级规划方法需要鉴别器至少90%准确性才能显著提高性能。

    

    在本文中，我们通过一个语言代理框架研究了大型语言模型（LLMs）如何在多步问题下解决问题，该框架包括生成器、鉴别器和规划方法三个部分。我们研究了两种先进规划方法，迭代校正和树搜索的实际效用。我们全面分析了鉴别准确性如何影响代理在使用这两种方法或更简单的重新排序方法时的整体性能。在两项任务，文本到SQL解析和数学推理上的实验表明：（1）高级规划方法需要至少90%准确性的鉴别器才能实现显著改进；（2）当前LLMs的鉴别能力尚未满足高级规划方法实现这种改进的需求；（3）采用基于LLM的鉴别器时，高级规划方法可能无法充分平衡准确性和效率。

    arXiv:2402.10890v1 Announce Type: cross  Abstract: In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compare
    
[^48]: 基于符号权重方向的领域特定适配器混合的泛化与其在有效模型剪枝中的应用

    Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning

    [https://arxiv.org/abs/2402.10639](https://arxiv.org/abs/2402.10639)

    本研究对领域特定适配器混合在领域内评估中的泛化性进行了全面分析，并探讨了混合适配器的内部运作，为适应新领域的性能优化提供了关键洞见

    

    基于适配器的几种参数高效的微调方法被提出作为一种简化的方法，不仅能将单一专业知识整合到现有的预训练语言模型（PLM）中，还能一次性整合多个专业知识。最近的作品如AdapterSoup提出了通过模型权重平均化在推理过程中混合领域特定适配器的方法，以优化在新领域中的性能，并具有出色的计算效率。然而，当前这种新兴的权重空间适配器混合机制在未知的领域内例子上的基本泛化性仍未被探讨。因此，在本研究中，我们进行了全面分析，阐明了领域特定适配器混合在领域内评估中的泛化性。我们还通过分析它们的权重符号来深入研究领域特定适配器混合的内部运作，得出了关键的分析。

    arXiv:2402.10639v1 Announce Type: new  Abstract: Several parameter-efficient fine-tuning methods based on adapters have been proposed as a streamlined approach to incorporate not only a single specialized knowledge into existing Pre-Trained Language Models (PLMs) but also multiple of them at once. Recent works such as AdapterSoup propose to mix not all but only a selective sub-set of domain-specific adapters during inference via model weight averaging to optimize performance on novel, unseen domains with excellent computational efficiency. However, the essential generalizability of this emerging weight-space adapter mixing mechanism on unseen, in-domain examples remains unexplored. Thus, in this study, we conduct a comprehensive analysis to elucidate the generalizability of domain-specific adapter mixtures in in-domain evaluation. We also provide investigations into the inner workings of the mixture of domain-specific adapters by analyzing their weight signs, yielding critical analysis
    
[^49]: 拉马在英语中有效吗？关于多语言变压器的潜在语言

    Do Llamas Work in English? On the Latent Language of Multilingual Transformers

    [https://arxiv.org/abs/2402.10588](https://arxiv.org/abs/2402.10588)

    本研究通过对Llama-2系列变压器模型的研究发现，在多语言语言模型中存在英语作为内部枢纽语言的现象，这有助于理解语言模型的功能方式以及语言偏见的起源。

    

    我们探讨了是否在不平衡、英语主导的语料库上训练的多语言语言模型使用英语作为内部枢纽语言的问题——这对于理解语言模型的功能方式以及语言偏见的起源至关重要。 我们关注Llama-2系列变压器模型，通过使用精心构建的非英语提示和唯一正确的单词延续来进行研究。 从一层到另一层，变压器逐渐将最终提示令牌的输入嵌入映射到输出嵌入，从中计算下一个令牌的概率。 通过跟踪其在高维空间中的中间嵌入，揭示了三个不同的阶段，即中间嵌入（1）开始远离输出令牌嵌入；（2）在中间层已经允许解码一个语义正确的下一个令牌，但更倾向于英语版本而不是输入语言的版本；（3）最终移动到

    arXiv:2402.10588v1 Announce Type: new  Abstract: We ask whether multilingual language models trained on unbalanced, English-dominated corpora use English as an internal pivot language -- a question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the Llama-2 family of transformer models, our study uses carefully constructed non-English prompts with a unique correct single-token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next-token probabilities are computed. Tracking intermediate embeddings through their high-dimensional space reveals three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already allow for decoding a semantically correct next token in the middle layers, but give higher probability to its version in English than in the input language; (3) finally move into an
    
[^50]: 具有偏置的直接偏好优化

    Direct Preference Optimization with an Offset

    [https://arxiv.org/abs/2402.10571](https://arxiv.org/abs/2402.10571)

    提出了一种新颖的直接偏好优化方法，即具有偏置的DPO（ODPO），在微调过程中不同对待每个偏好对。

    

    直接偏好优化（DPO）是一种成功的微调策略，用于使大型语言模型与人类偏好保持一致，而无需训练奖励模型或使用强化学习。本文提出了一种DPO的泛化形式，称为具有偏置的DPO（ODPO），在微调过程中不将每个偏好对视为相等。

    arXiv:2402.10571v1 Announce Type: cross  Abstract: Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal: while in some cases the preferred response is only slightly better than the dispreferred response, there can be a stronger preference for one response when, for example, the other response includes harmful or toxic content. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset valu
    
[^51]: 推动零-shot端到端语音翻译的极限

    Pushing the Limits of Zero-shot End-to-End Speech Translation

    [https://arxiv.org/abs/2402.10422](https://arxiv.org/abs/2402.10422)

    引入了ZeroSwot方法，实现了零-shot ST，通过CTC压缩和最优传输，仅利用ASR数据训练语音编码器，并与多语言MT模型在推断时无缝集成，实现直接从语音到文本的翻译。

    

    数据稀缺和语音与文本模态之间的模态差距是端到端语音翻译（ST）系统的两个主要障碍，从而阻碍了其性能。 以往的工作尝试通过利用外部MT数据和优化距离度量来减轻这些挑战，从而使语音-文本表示更加接近。 然而，通常需要一些ST数据才能获得竞争性结果。 出于这个原因，我们介绍了ZeroSwot，这是一种零-shot ST方法，可以在没有任何配对的ST数据的情况下弥合模态差距。 利用一种新颖的CTC压缩和最优传输技术，我们只使用ASR数据训练语音编码器，以与一个大规模多语言MT模型的表示空间进行对齐。 语音编码器在推断时与MT模型无缝集成，使得可以直接在所有MT模型支持的语言中从语音翻译为文本。 我们的实验表明，我们可以有效地平滑地关闭m模态间的空间.

    arXiv:2402.10422v1 Announce Type: new  Abstract: Data scarcity and the modality gap between the speech and text modalities are two major obstacles of end-to-end Speech Translation (ST) systems, thus hindering their performance. Prior work has attempted to mitigate these challenges by leveraging external MT data and optimizing distance metrics that bring closer the speech-text representations. However, achieving competitive results typically requires some ST data. For this reason, we introduce ZeroSwot, a method for zero-shot ST that bridges the modality gap without any paired ST data. Leveraging a novel CTC compression and Optimal Transport, we train a speech encoder using only ASR data, to align with the representation space of a massively multilingual MT model. The speech encoder seamlessly integrates with the MT model at inference, enabling direct translation from speech to text, across all languages supported by the MT model. Our experiments show that we can effectively close the m
    
[^52]: 同时重视：在不损害普适智能的情感智能大语言模型中增强情绪智力

    Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence

    [https://arxiv.org/abs/2402.10073](https://arxiv.org/abs/2402.10073)

    本论文提出了一个称为\textbf{MoEI}的方法，通过引入一个大规模的EI相关任务集合\textsc{EiBench}并采用模块化的训练过程和情感智力增强器的集成，综合提高了LLM的情感智力（EI）而不损害其普适智能（GI）。

    

    情感智力（EI）包括情感感知、情感认知和情感表达，在提高当前基于大语言模型（LLM）的对话式普适人工智能助手与用户交互体验方面起着关键作用。先前的工作主要集中在通过对EI相关的分类或回归任务的天真微调提高其情感感知能力。然而，这导致了EI的不完全增强和对普适智能的灾难性遗忘。为此，我们首先引入了一个大规模的以文本为基础的EI相关任务集合\textsc{EiBench}，其中包括了覆盖了EI的三个方面的任务指示，为综合增强LLM的EI奠定了坚实的基础。然后，提出了一种新颖的模块化情绪智力增强方法（\textbf{MoEI}），包含了模块化的训练过程和情感智力增强器的集成，以同时提高EI和保持普适智能。

    arXiv:2402.10073v1 Announce Type: new  Abstract: Emotional Intelligence (EI), consisting of emotion perception, emotion cognition and emotion expression, plays the critical roles in improving user interaction experience for the current large language model (LLM) based conversational general AI assistants. Previous works mainly focus on raising the emotion perception ability of them via naive fine-tuning on EI-related classification or regression tasks. However, this leads to the incomplete enhancement of EI and catastrophic forgetting of the general intelligence (GI). To this end, we first introduce \textsc{EiBench}, a large-scale collection of EI-related tasks in the text-to-text formation with task instructions that covers all three aspects of EI, which lays a solid foundation for the comprehensive EI enhancement of LLMs. Then a novel \underline{\textbf{Mo}}dular \underline{\textbf{E}}motional \underline{\textbf{I}}ntelligence enhancement method (\textbf{MoEI}), consisting of Modular
    
[^53]: 利用最小描述长度缩小神经网络形式语言学习中的经验-理论差距

    Bridging the Empirical-Theoretical Gap in Neural Network Formal Language Learning Using Minimum Description Length

    [https://arxiv.org/abs/2402.10013](https://arxiv.org/abs/2402.10013)

    通过使用最小描述长度目标（MDL），解决了神经网络在形式语言学习中的经验-理论差距问题。

    

    神经网络在许多任务中提供了良好的近似，但是即使理论工作表明这些完美的解可以由特定的架构来表示，它们仍然无法达到完美的泛化。通过使用形式语言学习的任务，我们专注于一个简单的形式语言，并展示了理论上正确的解决方案实际上不是常用目标函数的最优解，即使使用了正则化技术（如L1，L2）或其他元启发式方法（早停，dropout）。然而，将标准目标替换为最小描述长度目标（MDL）可以使正确的解成为最优解。

    arXiv:2402.10013v1 Announce Type: new  Abstract: Neural networks offer good approximation to many tasks but consistently fail to reach perfect generalization, even when theoretical work shows that such perfect solutions can be expressed by certain architectures. Using the task of formal language learning, we focus on one simple formal language and show that the theoretically correct solution is in fact not an optimum of commonly used objectives -- even with regularization techniques that according to common wisdom should lead to simple weights and good generalization (L1, L2) or other meta-heuristics (early-stopping, dropout). However, replacing standard targets with the Minimum Description Length objective (MDL) results in the correct solution being an optimum.
    
[^54]: 标注效率高的文本生成模型选择

    Label-Efficient Model Selection for Text Generation

    [https://arxiv.org/abs/2402.07891](https://arxiv.org/abs/2402.07891)

    DiffUse是一种标注效率高的文本生成模型选择方法，它通过聚类文本语义差异的嵌入来选择更具信息量的实例，并能显著减少所需的注释数量。

    

    针对给定目标任务的模型选择可能成本高昂，因为它可能需要对不同模型输出的质量进行广泛的注释。我们引入了DiffUse，一种有效的方法来在候选文本生成模型之间做出明智的决策。DiffUse减少了所需的偏好注释数量，从而节省了在评估中宝贵的时间和资源。DiffUse通过聚类表示模型输出之间的语义差异的嵌入来智能选择实例。因此，它能够识别出一些更有信息量的例子来进行偏好决策。我们的方法与模型无关，可以应用于任何文本生成模型。此外，我们提出了一种实用的迭代方法来动态确定要注释的实例数量。通过对数百个模型对进行一系列实验，我们证明了DiffUse可以显著减少所需的注释数量，最多可减少75%，同时保持高评估水平。

    Model selection for a given target task can be costly, as it may entail extensive annotation of the quality of outputs of different models. We introduce DiffUse, an efficient method to make an informed decision between candidate text generation models. DiffUse reduces the required amount of preference annotations, thus saving valuable time and resources in performing evaluation. DiffUse intelligently selects instances by clustering embeddings that represent the semantic differences between model outputs. Thus, it is able to identify a subset of examples that are more informative for preference decisions. Our method is model-agnostic, and can be applied to any text generation model. Moreover, we propose a practical iterative approach for dynamically determining how many instances to annotate. In a series of experiments over hundreds of model pairs, we demonstrate that DiffUse can dramatically reduce the required number of annotations -- by up to 75% -- while maintaining high evaluation 
    
[^55]: Mercury: 一种用于LLM代码综合效率评估的基准

    Mercury: An Efficiency Benchmark for LLM Code Synthesis

    [https://arxiv.org/abs/2402.07844](https://arxiv.org/abs/2402.07844)

    Mercury提出了一个针对LLM代码综合任务的效率评估基准，通过引入新的度量标准Beyond@K来衡量归一化的代码效率，从而鼓励生成功能正确且计算效率高的代码。

    

    尽管在评估大型语言模型（LLM）进行代码综合方面取得了进展，但基准主要集中在功能正确性上，忽视了代码效率的重要性。我们提出了Mercury，这是第一个专用于评估LLM代码综合任务的代码效率的基准。Mercury由1,889个涵盖不同难度级别的编程任务组成，还包括生成无限案例的测试用例生成器，以进行全面评估。与现有的基准不同，Mercury集成了一种新的度量标准Beyond@K，以基于历史提交来衡量归一化的代码效率，从而为代码综合提供了新的评估指标，鼓励生成功能正确且计算效率高的代码，体现了现实世界软件开发的标准。我们的研究结果表明，虽然LLM表现出生成功能正确代码的显著能力，但它们在效率输出方面仍存在很大的差距。

    Despite advancements in evaluating Large Language Models (LLMs) for code synthesis, benchmarks have predominantly focused on functional correctness, overlooking the importance of code efficiency. We present Mercury, the first benchmark designated for assessing the code efficiency of LLM code synthesis tasks. Mercury consists of 1,889 programming tasks covering diverse difficulty levels alongside test case generators generating unlimited cases for comprehensive evaluation. Unlike existing benchmarks, Mercury integrates a novel metric Beyond@K to measure normalized code efficiency based on historical submissions, leading to a new evaluation indicator for code synthesis, which encourages generating functionally correct and computationally efficient code, mirroring the real-world software development standard. Our findings reveal that while LLMs demonstrate the remarkable capability to generate functionally correct code, there still exists a substantial gap in their efficiency output, unde
    
[^56]: T-RAG: 来自LLM战场的经验教训

    T-RAG: Lessons from the LLM Trenches

    [https://arxiv.org/abs/2402.07483](https://arxiv.org/abs/2402.07483)

    T-RAG是一个基于LLM的应用程序，用于私人企业文件问答，它结合了RAG框架和经过微调的开源LLM，并分享了构建和部署过程中的经验教训。

    

    大型语言模型（LLM）展示了惊人的语言能力，推动了将它们整合到各个领域的应用的尝试。一个重要的应用领域是对私人企业文件进行问答，其中主要考虑因素是数据安全，需要能够在本地部署的应用程序，有限的计算资源和对查询正确响应的健壮应用的需求。检索增强生成（RAG）已成为构建基于LLM的应用程序的最重要的框架。虽然构建RAG相对简单，但要使其健壮和可靠的应用程序需要广泛的定制化和相对深入的应用领域知识。我们分享了构建和部署一个基于LLM的私人组织文件问答应用的经验。我们的应用结合了RAG的使用和经过微调的开源LLM。此外，我们的系统还具有 ...

    Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications. While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an LLM application for question answering over private organizational documents. Our application combines the use of RAG with a finetuned open-source LLM. Additionally, our system, which w
    
[^57]: 透过分割投票的视角: 探索在法律案件结果分类中的意见分歧、困难和校准

    Through the Lens of Split Vote: Exploring Disagreement, Difficulty and Calibration in Legal Case Outcome Classification

    [https://arxiv.org/abs/2402.07214](https://arxiv.org/abs/2402.07214)

    通过研究分割投票，探索律师在处理法律案件结果分类时面临的意见分歧和困难，并在欧洲人权法院收集了法官的投票数据集进行研究。这项研究还评估了模型和人类之间感知困难的一致性以及模型的置信度和人类校准。

    

    在法律决策中，当法官无法达成一致决定时，就会出现分割投票(SV)，给必须处理各种法律论点和意见的律师带来了困难。在高风险领域，理解人类和AI系统之间感知困难的一致性对于建立信任至关重要。然而，现有的自然语言处理校准方法主要关注分类器对预测性能的认知，通常是与人类的多数类进行比较，而忽视了人类标签变化的固有差异（HLV）。本文将分割投票视为自然可观察的人类意见分歧和价值多元主义，并从欧洲人权法院（ECHR）收集法官的投票分布，提出了带有SV信息的案件结果分类（COC）数据集SV-ECHR。我们建立了包含SV特定子类别的不同意见的分类法。我们进一步评估模型和人类之间感知困难的一致性，以及COC模型的置信度和人类校准。我们观察到了限制性的...

    In legal decisions, split votes (SV) occur when judges cannot reach a unanimous decision, posing a difficulty for lawyers who must navigate diverse legal arguments and opinions. In high-stakes domains, understanding the alignment of perceived difficulty between humans and AI systems is crucial to build trust. However, existing NLP calibration methods focus on a classifier's awareness of predictive performance, measured against the human majority class, overlooking inherent human label variation (HLV). This paper explores split votes as naturally observable human disagreement and value pluralism. We collect judges' vote distributions from the European Court of Human Rights (ECHR), and present SV-ECHR, a case outcome classification (COC) dataset with SV information. We build a taxonomy of disagreement with SV-specific subcategories. We further assess the alignment of perceived difficulty between models and humans, as well as confidence- and human-calibration of COC models. We observe lim
    
[^58]: NICE: 优化上下文示例还是不优化？

    NICE: To Optimize In-Context Examples or Not?

    [https://arxiv.org/abs/2402.06733](https://arxiv.org/abs/2402.06733)

    通过研究在提供任务特定指令的情况下是否需要优化上下文示例，我们挑战了对于指导性LLMs的共识，并发现在某些任务中，不同的优化上下文示例方法会产生递减的回报。我们引入了"度量标准"，用于衡量从给定指令中学习任务的能力，并提供了一个启发式方法，帮助决定是否优化指令还是ICE用于任何新任务。

    

    最近的研究表明，大型语言模型（LLMs）通过上下文学习和优化上下文示例（ICE），在各种任务上表现出色。然而，大多数研究假设在提示信息中要么是固定的，要么没有提供指令，导致了一个表面上的共识：优化上下文示例对于提高性能至关重要。我们针对经过指导的LLMs挑战这一共识，研究在提供了任务特定指令的情况下优化上下文示例是否必要，并发现有一些任务对于不同的优化上下文示例方法产生递减的回报。我们引入了一种任务特定的度量标准，称为"度量标准"（Metric），用于量化从给定指令中学习任务的能力，并提供了一个启发式方法，帮助决定是否优化指令还是ICE用于任何新任务。通过对各种任务和逐步增加的指令集的系统性研究，我们验证了该启发式方法的有效性。

    Recent works have shown that large language models (LLMs) work remarkably well on a wide range of tasks through in-context learning and optimization of in-context examples (ICE). However, most of these studies assume either a fixed or no instruction provided in the prompt, leading to the apparent consensus that the optimization of in-context examples is critical for better performance. We challenge this consensus for instruction-tuned LLMs by investigating the necessity of optimizing in-context examples when task-specific instructions are provided, and find that there are tasks for which various ways of optimizing in-context examples yield diminishing returns. We introduce a task-specific metric called \metriclong{} (\metric) that quantifies the learnability of tasks from a given instruction, and provides a heuristic that helps decide whether to optimize for instructions or ICE for any new task. On a wide range of tasks and a systematically created instruction set with gradually added 
    
[^59]: MLLM作为法官：使用视觉语言基准评估多模态MLLM作为法官的能力

    MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark

    [https://arxiv.org/abs/2402.04788](https://arxiv.org/abs/2402.04788)

    本文引入了MLLM作为法官的新基准测试，用于评估MLLM在辅助法官方面的能力。研究结果显示，MLLM在对比评估任务中展示出了非常类人的辨别能力，但在评分评估和批量排序任务中与人类偏好存在显著的差异。此外，即使对于先进模型如GPT-4V，MLLM仍然面临着偏见、幻觉式回答和不一致性等判断方面的挑战。

    

    多模态大型语言模型（MLLMs）近来引起了广泛关注，展现出人工智能方面的巨大潜力。然而，评估MLLM的实用性存在着相当大的挑战，主要是由于缺乏与人类偏好相符的多模态基准测试。本文受到LLM模型中LLM作为法官的启发，引入了一个新的基准测试，被称为MLLM作为法官，用于评估MLLM在协助法官方面的能力，包括三个不同的任务：评分评估、对比评估和批量排序。我们的研究发现，虽然MLLM在对比评估方面展示出了令人瞩目的类人辨别能力，但在评分评估和批量排序任务中与人类偏好存在显著的差异。此外，即使对于像GPT-4V这样的先进模型，MLLM仍然面临着判断方面的挑战，包括多样的偏见、幻觉式的回答和不一致性。这些发现强调了对MLLM的改进和进一步研究的迫切需要。

    Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Inspired by LLM-as-a-Judge in LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges including three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further rese
    
[^60]: 多路径解析在大脑中的研究

    Multipath parsing in the brain

    [https://arxiv.org/abs/2401.18046](https://arxiv.org/abs/2401.18046)

    本研究通过将增量生成依存关系解析器的预测与人们进行功能神经成像的时间数据相关，发现了人类在逐词理解句子时存在多路径解析的证据。

    

    人类逐词理解句子时，以所听到的顺序进行。这种增量方式需要解决临时的语法关系歧义。我们通过将增量生成依存关系解析器的预测与在听播音书时进行功能神经成像的人们的时间数据进行相关，来研究人类是如何处理这些语法歧义的。特别是，我们比较了关于逐词理解过程中正在进行的语法分析数量的竞争性假设：一个与多个。这个比较涉及将最先进的依存关系解析器使用经过LLM调整的编码来评估语法惊讶度，与现有的fMRI数据集相对照。在英文和中文的数据中，我们发现了多路径解析的证据。与该多路径效应相关的脑区包括双侧颞叶上沟。

    Humans understand sentences word-by-word, in the order that they hear them. This incrementality entails resolving temporary ambiguities about syntactic relationships. We investigate how humans process these syntactic ambiguities by correlating predictions from incremental generative dependency parsers with timecourse data from people undergoing functional neuroimaging while listening to an audiobook. In particular, we compare competing hypotheses regarding the number of developing syntactic analyses in play during word-by-word comprehension: one vs more than one. This comparison involves evaluating syntactic surprisal from a state-of-the-art dependency parser with LLM-adapted encodings against an existing fMRI dataset. In both English and Chinese data, we find evidence for multipath parsing. Brain regions associated with this multipath effect include bilateral superior temporal gyrus.
    
[^61]: 鲁棒的提示优化用于对抗语言模型的破解攻击

    Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks

    [https://arxiv.org/abs/2401.17263](https://arxiv.org/abs/2401.17263)

    该论文提出了一种鲁棒的提示优化算法（RPO）用于对抗语言模型的破解攻击，通过梯度优化来确保输出的无害性，并成功降低了攻击成功率。

    

    尽管在人工智能对齐方面取得了一些进展，但语言模型（LM）仍然容易受到对抗性攻击或破解攻击的影响，其中对手修改输入提示以诱导有害行为。虽然已经提出了一些防御方法，但它们仅关注狭窄的威胁模型，并不能提供强大的防御。为了实现强大的防御，我们首次提出了用于对抗破解攻击的对抗目标，并提出了一种名为鲁棒提示优化（RPO）的算法，该算法利用基于梯度的令牌优化来确保输出的无害性。通过这种方法，我们得到了一个易于访问的后缀，显著改善了对破解攻击的强韧性，包括优化过程中出现的破解攻击以及未知的破解攻击，将攻击成功率从84%降低到8.66%，在20个破解攻击中。此外，我们还发现RPO对正常LM使用的影响较小，在适应性攻击下仍然有效，并且可以迁移到黑盒模型中，降低攻击成功率。

    Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
    
[^62]: SAPT：一种共享注意力框架，用于大型语言模型的参数高效持续学习

    SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models

    [https://arxiv.org/abs/2401.08295](https://arxiv.org/abs/2401.08295)

    提出了一种共享注意力框架（SAPT），通过共享注意力学习与选择模块对齐PET学习和选择，以同时解决大型语言模型中的灾难性遗忘和知识转移挑战。

    

    持续学习（CL）能力对于在动态世界部署大型语言模型（LLM）至关重要。现有方法设计学习模块，通过参数高效调整（PET）块获取特定任务的知识，并通过选择模块选择出相应的输入，旨在应对CL中的灾难式遗忘和知识转移挑战。然而，这些方法往往只解决其中一个挑战，忽视了通过将两个模块对齐来有效同时解决灾难式遗忘和知识转移的潜力。为此，我们提出了一种新颖的共享注意力框架（SAPT），通过共享注意力学习与选择模块来对齐PET学习和选择。在两个CL基准测试上进行的广泛实验表明SAPT的优越性。此外，当我们将其扩展到不同模型大小时，SAPT一直展现出其优越性。

    arXiv:2401.08295v2 Announce Type: replace  Abstract: The continual learning (CL) ability is vital for deploying large language models (LLMs) in the dynamic world. Existing methods devise the learning module to acquire task-specific knowledge with parameter-efficient tuning (PET) block and the selection module to pick out the corresponding one for the testing input, aiming at handling the challenges of catastrophic forgetting and knowledge transfer in CL. However, these methods tend to address only one of the challenges, ignoring the potential of aligning the two modules to effectively address catastrophic forgetting and knowledge transfer simultaneously. To this end, we propose a novel Shared Attention Framework (SAPT), to align the PET learning and selection via the Shared Attentive Learning \& Selection module. Extensive Experiments on two CL benchmarks demonstrate the superiority of SAPT. Moreover, SAPT consistently demonstrates its superiority when we scale it to different model si
    
[^63]: WatME：通过词汇冗余实现无损水印

    WatME: Towards Lossless Watermarking Through Lexical Redundancy

    [https://arxiv.org/abs/2311.09832](https://arxiv.org/abs/2311.09832)

    WatME通过利用词汇冗余的语言先验知识，动态优化语言模型解码过程中的词汇使用，避免适当词汇不可用的情况，维持语言模型的表现力。

    

    arXiv:2311.09832v2 公告类型：替换。文本水印技术已经成为一种重要的检测机器生成文本的技术。然而，现有方法通常在解码过程中使用任意的词汇分割，导致在响应生成过程中缺乏适当的词汇，并破坏了语言模型的表现力，严重降低了文本响应的质量。为了解决这些问题，我们引入了一种新颖的方法，即互斥式水印（WatME）。具体来说，通过利用固有词汇冗余的语言先验知识，WatME 可以在语言模型的解码过程中动态优化可用词汇的使用。它采用互斥规则来管理这种冗余，避免了适当的词汇不可用的情况，同时保持了大型语言模型（LLMs）的表现力。我们提出理论分析和实证证据，证明了WatME的有效性。

    arXiv:2311.09832v2 Announce Type: replace  Abstract: Text watermarking has emerged as an important technique for detecting machine-generated text. However, existing methods generally use arbitrary vocabulary partitioning during decoding, which results in the absence of appropriate words during the response generation and disrupts the language model's expressiveness, thus severely degrading the quality of text response. To address these issues, we introduce a novel approach, Watermarking with Mutual Exclusion (WatME). Specifically, by leveraging linguistic prior knowledge of inherent lexical redundancy, WatME can dynamically optimize the use of available vocabulary during the decoding process of language models. It employs a mutually exclusive rule to manage this redundancy, avoiding situations where appropriate words are unavailable and maintaining the expressive power of large language models (LLMs). We present theoretical analysis and empirical evidence demonstrating that WatME subst
    
[^64]: TextEE：事件提取中的基准、重新评估、反思和未来挑战

    TextEE: Benchmark, Reevaluation, Reflections, and Future Challenges in Event Extraction

    [https://arxiv.org/abs/2311.09562](https://arxiv.org/abs/2311.09562)

    本研究提出了TextEE，这是一个用于事件提取的标准化、公平和可重复的基准，解决了评估中存在的挑战。

    

    事件提取由于其广泛的应用而引起了广泛关注。然而，最近的研究引起了对评估问题的关注，表明报告的分数可能无法准确反映真实性能。在这项工作中，我们确定并解决了评估挑战，包括由于不同的数据假设或预处理步骤而引起的不一致性，目前评估框架的不足可能引入数据集或数据分割偏见，以及一些先前方法的低可重复性。为了解决这些挑战，我们提出了TextEE，这是一个用于事件提取的标准化、公平和可重复的基准。TextEE包括标准化的数据预处理脚本和用于跨七个不同领域的14个数据集的数据分割，并包括14种最近的方法，进行全面的基准重新评估。我们还对我们的TextEE基准上的五种不同的大型语言模型进行了评估，并演示了如何

    arXiv:2311.09562v2 Announce Type: replace  Abstract: Event extraction has gained considerable interest due to its wide-ranging applications. However, recent studies draw attention to evaluation issues, suggesting that reported scores may not accurately reflect the true performance. In this work, we identify and address evaluation challenges, including inconsistency due to varying data assumptions or preprocessing steps, the insufficiency of current evaluation frameworks that may introduce dataset or data split bias, and the low reproducibility of some previous approaches. To address these challenges, we present TextEE, a standardized, fair, and reproducible benchmark for event extraction. TextEE comprises standardized data preprocessing scripts and splits for 14 datasets spanning seven diverse domains and includes 14 recent methodologies, conducting a comprehensive benchmark reevaluation. We also evaluate five varied large language models on our TextEE benchmark and demonstrate how the
    
[^65]: GRASP: 一种评估多模态语言模型中语言基础和情境物理理解的新型基准测试

    GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models

    [https://arxiv.org/abs/2311.09048](https://arxiv.org/abs/2311.09048)

    GRASP是一个新的基准测试，用于评估视频多模态大型语言模型的语言基础和物理理解能力。通过Unity模拟进行两层评估，揭示这些模型在语言基础和直觉物理学能力方面的显著缺陷。

    

    本文介绍了GRASP，一种评估基于视频的多模态大型语言模型(LLMs)的语言基础和物理理解能力的新型基准测试。通过利用Unity模拟的两层方法进行评估。第一层测试语言基础，通过评估模型将简单的文本描述与视觉信息相关联的能力。第二层评估模型对"直觉物理学"原理的理解能力，如物体永恒性和连续性。除了发布基准测试，我们还使用它评估了几种最先进的多模态LLMs。我们的评估揭示了这些模型在语言基础和直觉物理学能力方面存在显著的缺陷。尽管它们表现出了一定的基础能力，尤其是对于颜色和形状，但这些能力很大程度上依赖于启发策略。同时，所有模型在In部分的表现都低于或等于50%的随机水平。

    This paper presents GRASP, a novel benchmark to evaluate the language grounding and physical understanding capabilities of video-based multimodal large language models (LLMs). This evaluation is accomplished via a two-tier approach leveraging Unity simulations. The first level tests for language grounding by assessing a model's ability to relate simple textual descriptions with visual information. The second level evaluates the model's understanding of "Intuitive Physics" principles, such as object permanence and continuity. In addition to releasing the benchmark, we use it to evaluate several state-of-the-art multimodal LLMs. Our evaluation reveals significant shortcomings in the language grounding and intuitive physics capabilities of these models. Although they exhibit at least some grounding capabilities, particularly for colors and shapes, these capabilities depend heavily on the prompting strategy. At the same time, all models perform below or at the chance level of 50% in the In
    
[^66]: MELA：多语言语言可接受性评估

    MELA: Multilingual Evaluation of Linguistic Acceptability

    [https://arxiv.org/abs/2311.09033](https://arxiv.org/abs/2311.09033)

    MELA是第一个覆盖10种语言的多语言语言可接受性基准，通过分析XLM-R的微调权重，探讨了跨语言迁移困难性，结果表明在上下文示例方面ChatGPT表现良好但仍落后于经过微调的XLM-R。

    

    最近，针对大型语言模型（LLMs）的基准主要集中在应用驱动的任务，如复杂推理和代码生成上，导致LLMs的纯语言评估严重不足。针对这一背景，我们引入了Multilingual Evaluation of Linguistic Acceptability（MELA），这是第一个涵盖来自多个语言家族的10种语言、共48K个样本的语言可接受性多语言基准。我们建立了常用LLMs和监督模型的基线，使用XLM-R进行跨语言迁移和多任务学习实验。为了实现多语言可解释性，我们分析了微调后的XLM-R的权重，探讨了识别不同语言之间迁移困难性的可能性。我们的结果显示，ChatGPT从上下文示例中受益良多，但仍落后于经过微调的XLM-R，而GPT-4的性能与之相当。

    arXiv:2311.09033v2 Announce Type: replace-cross  Abstract: Recent benchmarks for Large Language Models (LLMs) have mostly focused on application-driven tasks such as complex reasoning and code generation, and this has led to a scarcity in purely linguistic evaluation of LLMs. Against this background, we introduce Multilingual Evaluation of Linguistic Acceptability -- MELA, the first multilingual benchmark on linguistic acceptability with 48K samples covering 10 languages from a diverse set of language families. We establish baselines of commonly used LLMs along with supervised models, and conduct cross-lingual transfer and multi-task learning experiments with XLM-R. In pursuit of multilingual interpretability, we analyze the weights of fine-tuned XLM-R to explore the possibility of identifying transfer difficulty between languages. Our results show that ChatGPT benefits much from in-context examples but still lags behind fine-tuned XLM-R, while the performance of GPT-4 is on par with f
    
[^67]: ReGAL: 用于发现通用抽象的程序重构方法

    ReGAL: Refactoring Programs to Discover Generalizable Abstractions. (arXiv:2401.16467v1 [cs.SE])

    [http://arxiv.org/abs/2401.16467](http://arxiv.org/abs/2401.16467)

    ReGAL提出了一种用于发现通用抽象的程序重构方法，可以通过重构代码学习可重用的函数库，利用这些共享函数库可以更准确地预测程序。

    

    虽然大型语言模型（LLMs）越来越多地被用于程序合成，但它们缺乏开发有用抽象所需的全局视角；它们通常一次预测一个程序，经常重复相同的功能。从头开始生成冗余代码既低效又容易出错。为了解决这个问题，我们提出了用于通用抽象学习的重构方法（ReGAL），通过代码重构来学习可重用函数库，即在不改变代码执行输出的情况下重组代码。ReGAL从一小组现有程序中学习，通过执行验证和细化抽象。我们发现，ReGAL发现的共享函数库使得在不同领域预测程序变得更加容易。在三个数据集（LOGO图形生成、日期推理和基于Minecraft的文字游戏TextCraft）上，开源和专有的LLMs在使用ReGAL函数库预测程序时准确性得到提高。

    While large language models (LLMs) are increasingly being used for program synthesis, they lack the global view needed to develop useful abstractions; they generally predict programs one at a time, often repeating the same functionality. Generating redundant code from scratch is both inefficient and error-prone. To address this, we propose Refactoring for Generalizable Abstraction Learning (ReGAL), a gradient-free method for learning a library of reusable functions via code refactorization, i.e. restructuring code without changing its execution output. ReGAL learns from a small set of existing programs, iteratively verifying and refining its abstractions via execution. We find that the shared function libraries discovered by ReGAL make programs easier to predict across diverse domains. On three datasets (LOGO graphics generation, Date reasoning, and TextCraft, a Minecraft-based text game), both open-source and proprietary LLMs improve in accuracy when predicting programs with ReGAL fun
    
[^68]: 不一定总是向右看：研究基于解码器的大型语言模型在序列标注中的能力

    Do Not (Always) Look Right: Investigating the Capabilities of Decoder-Based Large Language Models for Sequence Labeling. (arXiv:2401.14556v1 [cs.CL])

    [http://arxiv.org/abs/2401.14556](http://arxiv.org/abs/2401.14556)

    本文研究了基于解码器的大型语言模型在序列标注方面的能力，并探索了提高它们性能的策略。

    

    基于掩码语言建模（MLM）目标的预训练语言模型在自然语言理解（NLU）任务中表现出色。虽然与相似规模的因果语言建模解码器相比，经过微调的MLM-based编码器始终表现优异，但最近将解码器模型扩展至数十亿参数的趋势使得大型语言模型（LLMs）能够与MLM-based编码器相抗衡。尽管规模扩大了它们在NLU任务中的能力，但LLMs在信息提取（IE）任务中，尤其是序列标注（SL）任务方面仍然落后于SOTA结果。然而，LLMs的SL性能是由其固有的限制决定的还是可以改进的，仍然不清楚。为了解决这个问题，我们探索了改进"开放式"LLMs（Llama2和Mistral）在IE任务中的SL性能的策略。我们研究了解码器块组内的双向信息流，应用了层次逐层移除或启用因果掩码（CM）进来LLM微调。这种方法产生了...

    Pre-trained language models based on masked language modeling (MLM) objective excel in natural language understanding (NLU) tasks. While fine-tuned MLM-based encoders consistently outperform causal language modeling decoders of comparable size, a recent trend of scaling decoder models to multiple billion parameters resulted in large language models (LLMs), making them competitive with MLM-based encoders. Although scale amplifies their prowess in NLU tasks, LLMs fall short of SOTA results in information extraction (IE) tasks, many framed as sequence labeling (SL). However, whether this is an intrinsic limitation of LLMs or whether their SL performance can be improved remains unclear. To address this, we explore strategies to enhance the SL performance of "open" LLMs (Llama2 and Mistral) on IE tasks. We investigate bidirectional information flow within groups of decoder blocks, applying layer-wise removal or enforcement of the causal mask (CM) during LLM fine-tuning. This approach yields
    
[^69]: VisualWebArena: 在真实视觉Web任务上评估多模态代理

    VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks. (arXiv:2401.13649v1 [cs.LG])

    [http://arxiv.org/abs/2401.13649](http://arxiv.org/abs/2401.13649)

    VisualWebArena是一个评估多模态Web代理性能的基准，在真实的“视觉基础任务”上对代理进行了测试。它要求代理准确处理图像-文本输入，解释自然语言指令，并在网站上执行动作来完成用户定义的目标。

    

    能够在网络上进行计划、推理和执行动作的自主代理为自动化计算机任务提供了一个有前途的途径。然而，现有的大多数基准主要关注基于文本的代理，在效果上忽视了许多需要视觉信息才能有效解决的自然任务。鉴于大多数计算机界面是为人类感知而设计的，视觉信息往往以文本数据无法有效利用的方式增强文本数据。为了弥补这一差距，我们引入了VisualWebArena，这是一个设计用于评估多模态Web代理在真实的“视觉基础任务”上性能的基准。VisualWebArena包括一组多样且复杂的基于Web的任务，评估自主多模态代理的各种能力。为了在这个基准上执行，代理需要准确处理图像-文本输入，解释自然语言指令，并在网站上执行动作以完成用户定义的目标。

    Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \textit{visually grounded tasks}. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an ext
    
[^70]: Medusa: 多解码头的简洁LLM推理加速框架

    Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads. (arXiv:2401.10774v1 [cs.LG])

    [http://arxiv.org/abs/2401.10774](http://arxiv.org/abs/2401.10774)

    Medusa是一个能够提升LLM推理性能的简洁框架，通过增加多个解码头以实现并行预测多个后续标记，并通过树状注意力机制和并行处理来减少解码步骤。

    

    大型语言模型（LLMs）中的推理过程通常受限于自回归解码过程中的并行性缺失，使得大多数操作受限于加速器的内存带宽。虽然已经提出了类似于推测解码的方法来解决这个问题，但由于获得和维护独立的草稿模型所涉及的挑战，它们的实施受到了阻碍。在本文中，我们提出了一种高效的方法，通过添加额外的解码头来增强LLM推理，以并行预测多个后续标记。Medusa利用基于树的注意力机制，在每个解码步骤中同时构造多个候选延续并进行验证。通过利用并行处理，Medusa在单步延迟方面仅引入了最小的开销，同时大大降低了所需的解码步骤数。

    The inference process in Large Language Models (LLMs) is often limited due to the absence of parallelism in the auto-regressive decoding process, resulting in most operations being restricted by the memory bandwidth of accelerators. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa introduces only minimal overhead in terms of single-step latency while substantially reducing the number of decoding steps required.  We present two levels of fine-tuning procedures for Medusa to meet the needs o
    
[^71]: 超越基于参考指标：分析开放式LLM在数据到文本生成上的行为

    Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on Data-to-Text Generation. (arXiv:2401.10186v1 [cs.CL])

    [http://arxiv.org/abs/2401.10186](http://arxiv.org/abs/2401.10186)

    开放式大型语言模型在零-shot设置下能够从各种标准数据格式中生成流畅和连贯的文本，但是输出的语义准确性仍然是一个重要问题。

    

    我们调查开放式大型语言模型(LLMs)在从结构化数据生成连贯和相关文本方面的程度。为了防止基准泄露到LLM训练数据中的偏差，我们收集了Quintd-1:一个为5个数据到文本(D2T)生成任务设计的专门基准，该任务包括从公共API中收集的标准格式的结构化数据记录。我们利用无参考评估指标和LLMs的上下文学习能力，使我们能够在没有人工写作参考资料的情况下测试模型。我们的评估重点是在token级别上对语义准确性错误进行注释，结合人类标注者和基于GPT-4的指标。我们系统地研究了模型在不同领域和任务中的行为，发现7B参数的最先进开放式LLMs可以在零-shot设置中从各种标准数据格式中生成流畅和连贯的文本。然而，我们也表明输出的语义准确性仍然是一个重大问题：在我们的基准上，80%的输出存在语义准确性错误。

    We investigate to which extent open large language models (LLMs) can generate coherent and relevant text from structured data. To prevent bias from benchmarks leaked into LLM training data, we collect Quintd-1: an ad-hoc benchmark for five data-to-text (D2T) generation tasks, consisting of structured data records in standard formats gathered from public APIs. We leverage reference-free evaluation metrics and LLMs' in-context learning capabilities, allowing us to test the models with no human-written references. Our evaluation focuses on annotating semantic accuracy errors on token-level, combining human annotators and a metric based on GPT-4. Our systematic examination of the models' behavior across domains and tasks suggests that state-of-the-art open LLMs with 7B parameters can generate fluent and coherent text from various standard data formats in zero-shot settings. However, we also show that semantic accuracy of the outputs remains a major issue: on our benchmark, 80% of outputs o
    
[^72]: 不要排名，要合并！使用质量估计来合并机器翻译假设

    Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation. (arXiv:2401.06688v1 [cs.CL])

    [http://arxiv.org/abs/2401.06688](http://arxiv.org/abs/2401.06688)

    本文介绍了一种利用质量估计指标合并机器翻译假设的方法，该方法在大型语言模型和多语言翻译模型上提升了翻译质量。通过使用候选池和QE指标，我们的方法能够生成多样且准确的翻译结果。

    

    神经机器翻译系统通过给定源句子估计目标句子的概率，但这些估计可能与人类喜好不一致。本研究引入了QE-fusion方法，该方法利用更能与人类判断相关的质量估计指标（QE）来综合改进翻译结果。QE-fusion利用从模型中抽取的候选池，使用像CometKiwi这样的QE指标组合不同候选的片段。我们将QE-fusion与波束搜索和最近的重新排序技术（如最小贝叶斯风险解码或QE-重新排序）进行比较。当应用于用于翻译的大型语言模型（PolyLM、XGLM、Llama2和Mistral）和多语言翻译模型（NLLB）时，我们的方法在COMET和BLEURT评分方面始终提高翻译质量，涵盖五种语言对。值得注意的是，由于能够生成多样化的输出，我们的方法对于大型语言模型的改进更大。我们证明了我们的方法能够产生多样且准确的翻译结果。

    Neural machine translation systems estimate probabilities of target sentences given source sentences, yet these estimates may not align with human preferences. This work introduces QE-fusion, a method utilizing a quality estimation metric (QE) that better correlates with human judgments to synthesize improved translations. QE-fusion leverages a candidate pool sampled from a model, combining spans from different candidates using QE metrics such as CometKiwi. We compare QE-fusion against beam search and recent reranking techniques, such as Minimum Bayes Risk decoding or QE-reranking. Our method consistently improves translation quality in terms of COMET and BLEURT scores when applied to large language models (LLMs) used for translation (PolyLM, XGLM, Llama2, and Mistral) and to multilingual translation models (NLLB), over five language pairs. Notably, QE-fusion exhibits larger improvements for LLMs due to their ability to generate diverse outputs. We demonstrate that our approach generat
    
[^73]: 在源语言中迷失：大型语言模型如何评估机器翻译的质量

    Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation. (arXiv:2401.06568v1 [cs.CL])

    [http://arxiv.org/abs/2401.06568](http://arxiv.org/abs/2401.06568)

    本论文研究了大型语言模型（LLMs）如何利用源语言和参考信息评估机器翻译的质量，并发现参考信息显著提高了评估准确性，而源语言信息有时会适得其反，表明在使用LLMs评估翻译时存在跨语言能力不足的问题。

    

    大型语言模型（LLMs）在机器翻译评估任务中取得了显著的成果，但对它们如何利用提供的数据进行评估仍存在知识差距。本研究旨在探索LLMs如何利用源语言和参考信息评估翻译，以更好地理解LLMs的工作机制。为此，我们设计了涵盖各种输入模式和模型类型的受控实验，并采用粗粒度和细粒度的提示来区分源语言和参考信息的实用性。令人惊讶的是，我们发现参考信息显著提高了评估准确性，而源语言信息有时会适得其反，表明在使用LLMs评估翻译时存在跨语言能力不足的问题。我们还对LLMs进行了翻译错误检测的元评估，观察到了类似的现象。这些发现还暗示了一种潜在的方法，即利用参考信息来改善机器翻译的质量评估任务。

    Large Language Models (LLMs) have achieved remarkable results in the machine translation evaluation task, yet there remains a gap in knowledge regarding how they utilize the provided data to conduct evaluations. This study aims to explore how LLMs leverage source and reference information in evaluating translations, with the ultimate goal of better understanding the working mechanism of LLMs. To this end, we design the controlled experiments across various input modes and model types, and employ both coarse-grained and fine-grained prompts to discern the utility of source versus reference information. Surprisingly, we find that reference information significantly enhances the evaluation accuracy, while source information sometimes is counterproductive, indicating a lack of cross-lingual capability when using LLMs to evaluate translations. We further conduct a meta-evaluation for translation error detection of LLMs, observing a similar phenomenon. These findings also suggest a potential
    
[^74]: 互联网上的大量内容都是机器翻译的：来自多向并行性的洞察

    A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way Parallelism. (arXiv:2401.05749v1 [cs.CL])

    [http://arxiv.org/abs/2401.05749](http://arxiv.org/abs/2401.05749)

    互联网上的大量内容是通过机器翻译多向翻译的，其低质量可能会对使用多语言大型语言模型进行训练产生严重影响。

    

    我们展示了互联网上的内容经常被翻译成多种语言，并且这些多向翻译的低质量表明它们很可能是使用机器翻译（MT）创建的。多向并行的机器生成内容不仅在资源较少的语言中占主导地位，而且构成该语言中总体网页内容的很大一部分。我们还发现证据表明，被翻译成多种语言的内容存在选择性偏差，与将低质量英文内容通过机器翻译大规模翻译成许多资源较少的语言一致。我们的工作对于在网络上从单语和双语数据训练多语言大型语言模型等模型提出了严重的担忧。

    We show that content on the web is often translated into many languages, and the low quality of these multi-way translations indicates they were likely created using Machine Translation (MT). Multi-way parallel, machine generated content not only dominates the translations in lower resource languages; it also constitutes a large fraction of the total web content in those languages. We also find evidence of a selection bias in the type of content which is translated into many languages, consistent with low quality English content being translated en masse into many lower resource languages, via MT. Our work raises serious concerns about training models such as multilingual large language models on both monolingual and bilingual data scraped from the web.
    
[^75]: DebugBench: 评估大型语言模型的调试能力

    DebugBench: Evaluating Debugging Capability of Large Language Models. (arXiv:2401.04621v1 [cs.SE])

    [http://arxiv.org/abs/2401.04621](http://arxiv.org/abs/2401.04621)

    该论文介绍了一个名为DebugBench的LLM调试基准，用于评估大型语言模型的调试能力。研究发现闭源模型与人类相比具有较低的调试性能，而开源模型未能达到合格率。

    

    大型语言模型（LLMs）展示出了出色的编码能力。然而，作为编程能力的另一个关键组成部分，LLMs的调试能力仍然相对未被探索。之前对LLMs的调试能力评估受到数据泄露风险、数据集规模和测试漏洞种类的限制。为了克服这些不足，我们引入了一个名为“DebugBench”的LLM调试基准，包含4253个实例。它涵盖了C ++，Java和Python中四个主要的错误类别和18个次要类型。为了构建DebugBench，我们从LeetCode社区收集了代码片段，使用GPT-4向源数据中注入错误，并进行严格的质量检查。我们在零样例情况下评估了两个商业模型和三个开源模型。我们发现，（1）与人类相比，闭源模型如GPT-4表现出较低的调试性能，而开源模型如Code Llama无法达到任何合格率；（2）t

    Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and three open-source models in a zero-shot scenario. We find that (1) while closed-source models like GPT-4 exhibit inferior debugging performance compared to humans, open-source models such as Code Llama fail to attain any pass rate scores; (2) t
    
[^76]: SecFormer：面向大型语言模型的快速准确隐私保护推理

    SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models. (arXiv:2401.00793v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.00793](http://arxiv.org/abs/2401.00793)

    SecFormer是一个优化框架，旨在实现Transformer模型的快速准确隐私保护推理。通过消除高成本的指数和线性操作，SecFormer能够有效解决在大型语言模型中应用SMPC时的性能问题。

    

    随着在云平台上部署大型语言模型以提供推理服务的使用增加，隐私问题日益加剧，尤其是涉及投资计划和银行账户等敏感数据。安全多方计算（SMPC）被视为保护推理数据和模型参数隐私的一种有前途的解决方案。然而，SMPC在大型语言模型（特别是基于Transformer架构的模型）的隐私保护推理中的应用往往会导致显著的减速或性能下降。这主要是由于Transformer架构中的众多非线性操作不适合SMPC，并且难以有效规避或优化。为了解决这个问题，我们引入了一个先进的优化框架，称为SecFormer，以实现Transformer模型的快速准确隐私保护推理。通过实施模型设计优化，我们成功消除了高成本的指数和线性操作，并取得了良好的性能。

    With the growing use of large language models hosted on cloud platforms to offer inference services, privacy concerns are escalating, especially concerning sensitive data like investment plans and bank account details. Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect the privacy of inference data and model parameters. However, the application of SMPC in Privacy-Preserving Inference (PPI) for large language models, particularly those based on the Transformer architecture, often leads to considerable slowdowns or declines in performance. This is largely due to the multitude of nonlinear operations in the Transformer architecture, which are not well-suited to SMPC and difficult to circumvent or optimize effectively. To address this concern, we introduce an advanced optimization framework called SecFormer, to achieve fast and accurate PPI for Transformer models. By implementing model design optimization, we successfully eliminate the high-cost exponential and 
    
[^77]: 大型语言模型为何能生成正确的思维链条？

    Why Can Large Language Models Generate Correct Chain-of-Thoughts?. (arXiv:2310.13571v1 [cs.CL])

    [http://arxiv.org/abs/2310.13571](http://arxiv.org/abs/2310.13571)

    本文研究了大型语言模型如何生成连贯的思维链条，并通过建立几何收敛速率的框架来解释它与真实语言来源之间的相似性。这一研究结果为大型语言模型在推理任务中的性能提升提供了理论支持。

    

    本文深入研究了大型语言模型（LLM）的能力，特别关注推动对思维链条引发能力的理论理解。我们研究了如何有效地诱导LLM生成连贯的思维链条。为了实现这一目标，我们引入了一个针对自然语言生成的两级分层图模型。在这个框架下，我们建立了一个有说服力的几何收敛速率，用于衡量LLM生成的思维链条与真实语言来源的思维链条之间的相似性。我们的研究结果为LLM能够产生正确的思维序列（可能）解释了在需要推理能力的任务中性能提升的能力提供了理论上的解释。

    This paper delves into the capabilities of large language models (LLMs), specifically focusing on advancing the theoretical comprehension of chain-of-thought prompting. We investigate how LLMs can be effectively induced to generate a coherent chain of thoughts. To achieve this, we introduce a two-level hierarchical graphical model tailored for natural language generation. Within this framework, we establish a compelling geometrical convergence rate that gauges the likelihood of an LLM-generated chain of thoughts compared to those originating from the true language. Our findings provide a theoretical justification for the ability of LLMs to produce the correct sequence of thoughts (potentially) explaining performance gains in tasks demanding reasoning skills.
    
[^78]: AdaLomo: 低内存优化与自适应学习率

    AdaLomo: Low-memory Optimization with Adaptive Learning Rate. (arXiv:2310.10195v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.10195](http://arxiv.org/abs/2310.10195)

    AdaLomo是一种低内存优化方法，通过引入自适应学习率来改善大型语言模型优化器的性能，同时保持内存效率。

    

    大型语言模型取得了显著的成功，但其庞大的参数规模需要大量的训练内存，从而设置了很高的门槛。尽管最近提出的低内存优化（LOMO）减少了内存占用，但其优化技术类似于随机梯度下降，对超参数敏感并展现出次优的收敛性，无法与当前大型语言模型优化器AdamW的性能相媲美。通过对Adam优化器进行经验分析，我们发现相对于动量来说，自适应学习率对于弥合差距更为关键。基于此，我们引入了带有自适应学习率的低内存优化（AdaLomo），为每个参数提供自适应学习率。为了保持内存效率，我们在优化器状态中采用非负矩阵分解来估计二阶矩。此外，我们建议使用分组更新规范。

    Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update norma
    
[^79]: 语言代理树搜索统一了语言模型中的推理、行动和规划

    Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models. (arXiv:2310.04406v1 [cs.AI])

    [http://arxiv.org/abs/2310.04406](http://arxiv.org/abs/2310.04406)

    语言代理树搜索（LATS）是一个通用框架，利用大型语言模型（LLMs）的能力在规划、行动和推理方面相互协同，通过使用具有外部反馈的环境，实现更加深思熟虑和适应性的问题解决机制。实验评估表明，LATS在多个领域具有广泛的应用性，特别在编程方面表现出了94.4%的准确率。

    

    虽然大型语言模型（LLMs）在一系列决策任务上表现出了令人印象深刻的性能，但它们依赖于简单的行动过程，并未能广泛部署作为自主代理。我们引入了LATS（语言代理树搜索），这是一个通用框架，将LLMs在规划、行动和推理方面的能力相互协同。LATS借鉴了模型导向的强化学习中的蒙特卡洛树搜索的思想，将LLMs用作代理、价值函数和优化器，重新利用其潜在的优势以提升决策能力。关键的一点是LATS使用一个具有外部反馈的环境，这提供了一种更加深思熟虑和适应性的问题解决机制，超越了现有技术的局限性。我们在编程、HotPotQA和WebShop等多个领域进行了实验评估，证明了LATS在推理和行动方面的适用性。特别是，在编程方面，LATS实现了94.4%的准确率。

    While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4\% for programming on Hu
    
[^80]: 简明有序的感知有助于大型语言模型进行演绎推理

    Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning. (arXiv:2310.03309v1 [cs.CL])

    [http://arxiv.org/abs/2310.03309](http://arxiv.org/abs/2310.03309)

    利用大型语言模型进行演绎推理是一个具有挑战性的问题。这篇论文提出了一个简明有序的方法，将任务分解为子任务并且人类化地组织思维，以提高演绎推理的效果。

    

    利用大型语言模型（LLMs）解决演绎推理问题已经引起了越来越多的关注。在复杂的演绎问题中仍然很难取得令人满意的结果，这类问题具有大量前提（即事实或规则），其中涉及实体之间错综复杂的关系，需要进行多跳推理。一种直观的解决方案是将原始任务分解为较小的子任务，然后以前向（例如选择-推理）或反向（例如LAMBADA）方式将多个因果推理步骤连接在一起。然而，这些技术不可避免地需要大量的总体阶段，导致计算开销大，并且有更高的可能性产生误导性的步骤。除了逐阶段分解之外，我们还从人类问题解决的另一个方面获得了启发。人类倾向于提炼出最相关的信息并有序地组织思维（例如创建思维导图），这有助于他们对问题进行有效的推理。

    Exploiting large language models (LLMs) to tackle deductive reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex deductive problems, characterized by plenty of premises (i.e., facts or rules) entailing intricate relationships among entities and requiring multi-hop reasoning. One intuitive solution is to decompose the original task into smaller sub-tasks, and then chain the multiple casual reasoning steps together in a forward (e.g., Selection-Inference) or backward (e.g., LAMBADA) direction. However, these techniques inevitably necessitate a large number of overall stages, leading to computationally expensive operations and a higher possibility of making misleading steps. In addition to stage-by-stage decomposition, we draw inspiration from another aspect of human problem-solving. Humans tend to distill the most relevant information and organize their thoughts systematically (e.g., creating mind maps), which assists th
    
[^81]: 自我特化：揭示大型语言模型中的潜在专业知识

    Self-Specialization: Uncovering Latent Expertise within Large Language Models. (arXiv:2310.00160v1 [cs.CL])

    [http://arxiv.org/abs/2310.00160](http://arxiv.org/abs/2310.00160)

    该论文研究了大型语言模型的自我特化，通过使用专业领域的数据和少量标记种子进行自我对齐，提高了在目标领域的零样本和少样本性能。

    

    最近的研究表明，自我调整的有效性，即通过使用少量人类编写的种子数据自动生成教学数据，使大型语言模型自动对齐以遵循一般指示。在这项工作中，我们不再关注一般对齐，而是专注于专家领域特化的自我对齐（例如，生物医学），发现它对于提高目标领域的零样本和少样本性能非常有效。首先，我们介绍了现有对齐模型在专业领域内的基准结果，揭示了“通用”指示跟随训练对下游专家领域性能的边际效应。为了解决这个问题，我们探索了自我特化，利用领域特定的未标记数据和少量标记种子进行自我对齐过程。当通过检索来减少产生幻觉并提高对齐的并发性后，自我特化提供了一种解决方案。

    Recent works have demonstrated the effectiveness of self-alignment in which a large language model is, by itself, aligned to follow general instructions through the automatic generation of instructional data using a handful of human-written seeds. Instead of general alignment, in this work, we focus on self-alignment for expert domain specialization (e.g., biomedicine), discovering it to be very effective for improving zero-shot and few-shot performance in target domains of interest. As a preliminary, we first present the benchmark results of existing aligned models within a specialized domain, which reveals the marginal effect that "generic" instruction-following training has on downstream expert domains' performance. To remedy this, we explore self-specialization that leverages domain-specific unlabelled data and a few labeled seeds for the self-alignment process. When augmented with retrieval to reduce hallucination and enhance concurrency of the alignment, self-specialization offer
    
[^82]: 关于思维链推理：进展、前沿和未来的调查

    A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. (arXiv:2309.15402v1 [cs.CL])

    [http://arxiv.org/abs/2309.15402](http://arxiv.org/abs/2309.15402)

    本文首次全面调查了思维链推理领域的研究，涵盖了构建、结构变体和增强技术等方法分类，以及规划、工具使用和提炼等前沿应用。同时讨论了挑战和未来发展方向。这份调查报告对于在思维链推理领域寻求创新的研究人员来说是一个有价值的资源。

    

    思维链推理是人类智能的基本认知过程，在人工智能和自然语言处理领域引起了广泛关注。然而，目前仍缺乏一份全面的调查报告。为此，我们迈出了第一步，仔细广泛地概述了这个研究领域。我们用“X-of-Thought”来指代广义上的思维链推理。具体而言，我们根据方法的分类体系对当前的研究进行了系统组织，包括思维链的构建、结构变体和增强技术。此外，我们描述了思维链在规划、工具使用和提炼等领域的前沿应用。此外，我们还讨论了一些挑战和未来的方向，包括忠实度、多模态和理论等。我们希望这份调查报告能成为寻求在思维链推理领域创新的研究人员的宝贵资源。

    Chain-of-thought reasoning, a cognitive process fundamental to human intelligence, has garnered significant attention in the realm of artificial intelligence and natural language processing. However, there still remains a lack of a comprehensive survey for this arena. To this end, we take the first step and present a thorough survey of this research field carefully and widely. We use X-of-Thought to refer to Chain-of-Thought in a broad sense. In detail, we systematically organize the current research according to the taxonomies of methods, including XoT construction, XoT structure variants, and enhanced XoT. Additionally, we describe XoT with frontier applications, covering planning, tool use, and distillation. Furthermore, we address challenges and discuss some future directions, including faithfulness, multi-modal, and theory. We hope this survey serves as a valuable resource for researchers seeking to innovate within the domain of chain-of-thought reasoning.
    
[^83]: CB-Whisper: 使用开放词汇关键词检测进行上下文偏置的Whisper

    CB-Whisper: Contextual Biasing Whisper using Open-Vocabulary Keyword-Spotting. (arXiv:2309.09552v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.09552](http://arxiv.org/abs/2309.09552)

    CB-Whisper是一种基于OpenAI的Whisper模型的自动语音识别系统，通过使用开放词汇关键词检测（OV-KWS）识别罕见的命名实体，并使用这些实体作为提示来改进识别效果。实验证明，该方法在提高实体召回率的同时会略微增加混淆错误率（MER）。

    

    自动语音识别系统往往难以识别罕见的命名实体，如个人姓名、组织机构和在训练数据中不经常遇到的专业术语。本文提出了一种基于OpenAI的Whisper模型的Contextual Biasing Whisper（CB-Whisper）自动语音识别系统，通过使用Whisper编码器的隐藏状态执行开放词汇关键词检测（OV-KWS）来识别用户定义的命名实体。识别出的实体被用作Whisper解码器的提示。我们首先提出了一种使用OV-KWS和ASR任务进行多任务训练的方法来优化模型。实验证明，与原始Whisper模型相比，这种方法在中国Aishell热词子集和两个内部代码切换测试集上显著提高了实体召回率。然而，由于灾难性遗忘，我们观察到在内部测试集上混淆错误率（MER）略微增加。为了解决这个问题并使用不同大小的Whisper模型，我们进一步提出了一种解决方案。

    End-to-end automatic speech recognition (ASR) systems often struggle to recognize rare name entities, such as personal names, organizations, and terminologies not frequently encountered in the training data. This paper presents Contextual Biasing Whisper (CB-Whisper), a novel ASR system based on OpenAI's Whisper model that can recognize user-defined name entities by performing open-vocabulary keyword-spotting (OV-KWS) using the hidden states of Whisper encoder. The recognized entities are used as prompts for the Whisper decoder. We first propose a multitask training approach with OV-KWS and ASR tasks to optimize the model. Experiments show that this approach substantially improves the entity recalls compared to the original Whisper model on Chinese Aishell hot word subsets and two internal code-switch test sets. However, we observed a slight increase in mixed-error-rate (MER) on internal test sets due to catastrophic forgetting. To address this problem and use different sizes of the Wh
    
[^84]: 调查新闻概述中的性别偏见

    Investigating Gender Bias in News Summarization. (arXiv:2309.08047v1 [cs.CL])

    [http://arxiv.org/abs/2309.08047](http://arxiv.org/abs/2309.08047)

    本研究调查了新闻概述中的性别偏见，发现大型语言模型（LLMs）会重复和强化有害的社会偏见。研究提出了一些方法来量化模型中的有偏行为，并提出了一种生成具有控制人口属性的输入文档的方法。

    

    概述是大型语言模型（LLMs）的一个重要应用。以往对概述模型的评估主要关注它们在内容选择、语法正确性和连贯性方面的性能。然而，众所周知，LLMs会重复和强化有害的社会偏见。这引发了一个问题：在一个相对受限制的环境，比如概述，这些偏见会对模型的输出产生影响吗？为了解答这个问题，我们首先提出了一些关于概述模型中的有偏行为的定义，并引入了一些实际方法来量化它们。由于我们发现输入文档中存在的偏见可能干扰我们的分析，我们还提出了一种方法来生成具有仔细控制人口属性的输入文档。这使我们能够规避这个问题，同时仍然使用一些现实的输入文档进行工作。最后，我们将我们的方法应用于专门构建的概述模型和通用用途的模型生成的概述。

    Summarization is an important application of large language models (LLMs). Most previous evaluation of summarization models has focused on their performance in content selection, grammaticality and coherence. However, it is well known that LLMs reproduce and reinforce harmful social biases. This raises the question: Do these biases affect model outputs in a relatively constrained setting like summarization?  To help answer this question, we first motivate and introduce a number of definitions for biased behaviours in summarization models, along with practical measures to quantify them. Since we find biases inherent to the input document can confound our analysis, we additionally propose a method to generate input documents with carefully controlled demographic attributes. This allows us to sidestep this issue, while still working with somewhat realistic input documents.  Finally, we apply our measures to summaries generated by both purpose-built summarization models and general purpose
    
[^85]: 表示对上下文学习的影响：对合成任务的探索

    How does representation impact in-context learning: A exploration on a synthetic task. (arXiv:2309.06054v1 [cs.LG])

    [http://arxiv.org/abs/2309.06054](http://arxiv.org/abs/2309.06054)

    本研究通过探索表示学习的角度，研究了表示对上下文学习的影响。实验结果表明，在上下文学习中，上下文内部成分对学习性能起到重要作用。

    

    上下文学习，即从上下文样本中学习，是Transformer的一项引人注目的能力。然而，驱动上下文学习的机制尚未被充分理解。本研究旨在从一个未被充分探索的表示学习角度进行调查。在上下文学习场景中，表示更加复杂，表示可以受到模型权重和上下文样本的影响。我们将上述两个概念方面的表示分别称为权重内部成分和上下文内部成分。为了研究这两个成分如何影响上下文学习能力，我们构建了一个新颖的合成任务，从而可以设计两个探针，即权重内部探针和上下文探针，分别评估这两个成分。我们证明上下文内部成分的好坏与上下文学习性能高度相关，这表明上下文学习与表示学习之间的纠缠关系。

    In-context learning, i.e., learning from in-context samples, is an impressive ability of Transformer. However, the mechanism driving the in-context learning is not yet fully understood. In this study, we aim to investigate from an underexplored perspective of representation learning. The representation is more complex for in-context learning senario, where the representation can be impacted by both model weights and in-context samples. We refer the above two conceptually aspects of representation as in-weight component and in-context component, respectively. To study how the two components affect in-context learning capabilities, we construct a novel synthetic task, making it possible to device two probes, in-weights probe and in-context probe, to evaluate the two components, respectively. We demonstrate that the goodness of in-context component is highly related to the in-context learning performance, which indicates the entanglement between in-context learning and representation lear
    
[^86]: 用于中文医学标点修复的小型快速BERT模型

    A Small and Fast BERT for Chinese Medical Punctuation Restoration. (arXiv:2308.12568v1 [cs.CL])

    [http://arxiv.org/abs/2308.12568](http://arxiv.org/abs/2308.12568)

    该论文提出了一种用于中文医学标点修复的快速小型BERT模型。通过结合监督对比学习和辅助预训练任务，该模型在具有较小模型大小的情况下，能够实现与最先进的中文RoBERTa模型相当的95%性能。

    

    在临床听写中，没有明确标点符号的自动语音识别（ASR）导致了对听写报告的误解。为了使用ASR提供精确和易懂的临床报告，需要进行自动标点修复。考虑到实际情况，我们提出了一种基于“预训练和微调”范式的快速轻量级预训练模型，用于中文医学标点修复。在这项工作中，我们通过结合监督对比学习和一种新颖的辅助预训练任务（标点符号预测）来提炼预训练模型，使其适用于标点修复。我们在各种提炼模型上的实验表明，相对于最先进的中文RoBERTa模型，我们的模型可以在10%的模型大小的情况下实现95%的性能。

    In clinical dictation, utterances after automatic speech recognition (ASR) without explicit punctuation marks may lead to the misunderstanding of dictated reports. To give a precise and understandable clinical report with ASR, automatic punctuation restoration is required. Considering a practical scenario, we propose a fast and light pre-trained model for Chinese medical punctuation restoration based on 'pretraining and fine-tuning' paradigm. In this work, we distill pre-trained models by incorporating supervised contrastive learning and a novel auxiliary pre-training task (Punctuation Mark Prediction) to make it well-suited for punctuation restoration. Our experiments on various distilled models reveal that our model can achieve 95% performance while 10% model size relative to state-of-the-art Chinese RoBERTa.
    
[^87]: 通过编码本知识、自然语言推理和ChatGPT来合成政治零样本关系分类

    Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT. (arXiv:2308.07876v1 [cs.CL])

    [http://arxiv.org/abs/2308.07876](http://arxiv.org/abs/2308.07876)

    该论文通过利用已建立的注释编码本的知识，探索零样本方法用于政治事件本体关系分类，并介绍一种基于自然语言推理的方法，名为ZSP。ZSP采用了一种树查询框架，提高了解释性、效率和对模式更改的适应性。在细粒度根代码分类上，ZSP的性能明显优于ChatGPT，F1得分提高了40%。

    

    最近的事件编码的监督模型在性能方面远远超过模式匹配方法。然而，它们仅仅依赖于新的注释，忽视了专家数据库中的大量知识，限制了它们在细粒度分类中的适用性。为了解决这些限制，我们通过利用已建立的注释编码本的知识，探索零样本方法用于政治事件本体关系分类。我们的研究涵盖了ChatGPT和一种新颖的基于自然语言推理的方法，名为ZSP。ZSP采用了一种树查询框架，将任务分解为上下文、语态和类别消歧的不同层次。该框架提高了解释性、效率和对模式更改的适应性。通过在我们新策划的数据集上进行大量实验，我们指出了ChatGPT中的不稳定性问题，并突出了ZSP的卓越性能。ZSP在细粒度根代码分类的F1得分上取得了令人印象深刻的提高40%。

    Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classific
    
[^88]: 语言模型的鲁棒无畸变水印方法

    Robust Distortion-free Watermarks for Language Models. (arXiv:2307.15593v1 [cs.LG])

    [http://arxiv.org/abs/2307.15593](http://arxiv.org/abs/2307.15593)

    该论文提出了一种在语言模型中添加鲁棒无畸变水印的方法，通过映射随机数序列到语言模型的样本，可以实现在不改变文本分布的前提下对水印文本进行检测，并且在多种改写攻击下依然保持较高的鲁棒性，实验证明在40-50%的随机扰动下仍可可靠地检测到水印文本。

    

    我们提出了一种在自回归语言模型中添加水印的方法，并且这些水印对扰动具有鲁棒性，而不会改变文本的分布，同时保证生成预算在一定范围内。我们用随机水印密钥计算的随机数序列映射到语言模型的样本来生成带水印的文本。要检测水印文本，只要知道密钥的任何一方都可以将文本与随机数序列对齐。我们使用两种采样方案来实例化水印方法：反变换采样和指数最小采样。我们将这些水印应用于三个语言模型——OPT-1.3B、LLaMA-7B和Alpaca-7B，以实验证明它们的统计功效和对各种改写攻击的鲁棒性。值得注意的是，对于OPT-1.3B和LLaMA-7B模型，即使在随机扰动了40-50%的词元后，我们仍然可以可靠地检测到带水印的文本（$p \leq 0.01$），只需要35个词元。

    We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq 0.01$) from $35$ tokens even after corrupting between $40$-$50$\% of the tokens via random
    
[^89]: 低资源情况下大型语言模型全参数微调方法研究

    Full Parameter Fine-tuning for Large Language Models with Limited Resources. (arXiv:2306.09782v1 [cs.CL])

    [http://arxiv.org/abs/2306.09782](http://arxiv.org/abs/2306.09782)

    本文提出了一种低内存使用的优化器LOMO，可以实现在有限资源下对大型语言模型进行全参数微调，从而降低研究门槛。

    

    大型语言模型（LLMs）在自然语言处理（NLP）领域具有革命性的影响，但需要大量GPU资源进行训练，从而造成研究门槛高。现有方法主要关注参数有效微调，即微调或添加少量参数，但很少有关注在有限资源情况下全参数微调的挑战。本文提出了一种新的优化器LOw-Memory Optimization（LOMO）, 通过将梯度计算和参数更新一步融合以减少内存使用。通过将LOMO与现有的内存节省技术相结合，我们将内存使用量降低到DeepSpeed方案的10.8％。因此，我们的方法使65B模型的全参数微调在只需单台机器上执行，该机器搭载8个RTX 3090，每个显存为24GB。

    Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 RTX 3090, each with 24GB memory.
    
[^90]: 带词典的指令优化用于零样式分类

    Instruction Tuning with Lexicons for Zero-Shot Style Classification. (arXiv:2305.14592v1 [cs.CL])

    [http://arxiv.org/abs/2305.14592](http://arxiv.org/abs/2305.14592)

    通过使用词典指导语言模型识别训练期间未见过的新风格，可以有效地提高零样本性能的转移。

    

    风格用于传达作者的意图和态度。尽管大型预训练语言模型在风格分类上取得了成功，但先前的研究依赖于带标签的样本进行微调。启发大型语言模型在没有微调的情况下对风格进行分类具有挑战性，因为语言风格可能很难定义。在这项研究中，我们调查了风格词典作为指导语言模型如何识别在训练期间未见过的新风格的有效性。我们的实验表明，基于词典的指令显著提高了零样本性能的转移。我们将发布我们的代码和数据。

    Style is used to convey authors' intentions and attitudes. Despite the success of large pre-trained language models on style classification, prior work relies on fine-tuning with labeled examples. Prompting large language models to classify style without fine-tuning is challenging because language styles can be difficult to define. In this study, we investigate the effectiveness of style lexicons as a means for instructing language models how to identify new styles that are unseen during training. Our experiments show that lexicon-based instructions improve transfer zero-shot performance significantly. We will release our code and data.
    
[^91]: 你所在社区发生了什么？一种弱监督方法用于发现本地新闻。

    What's happening in your neighborhood? A Weakly Supervised Approach to Detect Local News. (arXiv:2301.08146v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2301.08146](http://arxiv.org/abs/2301.08146)

    该论文介绍了一种自动化的本地新闻检测和基于内容的本地新闻推荐方法，通过弱监督框架和自动化数据处理，与传统方法相比具有更高的准确性和覆盖率。

    

    本地新闻是影响特定地理区域（如城市、县和州）用户的新闻子集。检测本地新闻是准确地推荐本地新闻的关键步骤。基于最新的自然语言处理技术，我们开发了一种集成化的流程，实现了自动化本地新闻检测和基于内容的本地新闻推荐。本文着重介绍了管道的第一步骤：（1）结合领域知识和自动数据处理的弱监督框架，（2）可扩展到多语言设置。与斯坦福CoreNLP NER模型相比，我们的流程在经过真实世界和人工标记数据的评估时具有更高的精度和召回率。

    Local news articles are a subset of news that impact users in a geographical area, such as a city, county, or state. Detecting local news (Step 1) and subsequently deciding its geographical location as well as radius of impact (Step 2) are two important steps towards accurate local news recommendation. Naive rule-based methods, such as detecting city names from the news title, tend to give erroneous results due to lack of understanding of the news content. Empowered by the latest development in natural language processing, we develop an integrated pipeline that enables automatic local news detection and content-based local news recommendations. In this paper, we focus on Step 1 of the pipeline, which highlights: (1) a weakly supervised framework incorporated with domain knowledge and auto data processing, and (2) scalability to multi-lingual settings. Compared with Stanford CoreNLP NER model, our pipeline has higher precision and recall evaluated on a real-world and human-labeled datas
    

