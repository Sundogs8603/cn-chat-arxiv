# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding.](http://arxiv.org/abs/2305.00633) | 本论文提出了一种通过自我评估引导解码提高推理的方法，使用经过校准的自动标准探索推理搜索空间，使搜索能够产生更高质量的最终预测结果；使用自我评估引导的随机束搜索在产生推理链的质量和多样性之间平衡权衡，适应多数投票，并且可以准确判断逻辑错误，提高一致性和鲁棒性。 |
| [^2] | [Low-Resourced Machine Translation for Senegalese Wolof Language.](http://arxiv.org/abs/2305.00606) | 本文介绍了一种低资源机器翻译方法，面向沃洛夫语，使用递归神经网络模型，并提供了一个沃洛夫语/法语的平行语料库，在子单词的训练数据和法语-英语语言对上，该方法表现出更好的性能。 |
| [^3] | [Reliable Gradient-free and Likelihood-free Prompt Tuning.](http://arxiv.org/abs/2305.00593) | 本文提供一种能够应对挑战性情景，即仅具备API访问权限的情况下，建模API进行优化的方法，并能够对推理不确定性进行量化。 |
| [^4] | [How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model.](http://arxiv.org/abs/2305.00586) | 本研究运用机械式可解释性技术探究了GPT-2 Small的数学能力，并确定了它的计算图中的一个小电路用于计算大于符号，该电路的多层感知器提高了结束年份大于开始年份的概率，并且该电路具有广泛的适用性。 |
| [^5] | [Multimodal Graph Transformer for Multimodal Question Answering.](http://arxiv.org/abs/2305.00581) | 本文提出了一种新的多模态图转换器，它使用图神经网络将文本和视觉数据的多模态图形信息与vanilla self-attention机制相结合，以增强模型理解结构化输入数据的能力，并在两种具有挑战性的多模态问答基准上取得了性能提升。 |
| [^6] | [How to enumerate trees from a context-free grammar.](http://arxiv.org/abs/2305.00522) | 本文介绍了一种简单的算法，用于枚举上下文无关文法（CFG）生成的树，提供了通用的方法来对自然逻辑语言中的表达式进行编号，并可能扩展到其他组合问题。 |
| [^7] | [SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support.](http://arxiv.org/abs/2305.00450) | 本研究提出了SMILE方法，使用ChatGPT将公共单轮对话扩展为多轮对话，生成了大规模、多样化、接近真实生活的多轮心理健康支持对话语料库，可用于训练和评估专门的对话系统。 |
| [^8] | [Building a Non-native Speech Corpus Featuring Chinese-English Bilingual Children: Compilation and Rationale.](http://arxiv.org/abs/2305.00446) | 本文介绍了一份非母语语音语料库，由50名中英双语儿童提供英语（L2）叙述，为第二语言教学提供了有价值的资源并有可能提高自动语音识别（ASR）的整体性能。 |
| [^9] | [Constructing a Knowledge Graph from Textual Descriptions of Software Vulnerabilities in the National Vulnerability Database.](http://arxiv.org/abs/2305.00382) | 本文提出了一种从国家漏洞数据库信息中构建漏洞知识图谱的新方法，结合了命名实体识别、关系提取和实体预测。该方法有助于解决网络安全知识图谱中缺失实体的问题。 |
| [^10] | [S2abEL: A Dataset for Entity Linking from Scientific Tables.](http://arxiv.org/abs/2305.00366) | 该论文提供了第一个专注于科学表格的 EL 数据集 S2abEL，用于实体链接任务。由于科学知识库的不完整性和语境影响，科学表格上的 EL 具有挑战性，该数据集专注于机器学习结果表中的 EL，包含手工标记的单元格类型、属性和实体链接，并引入了一种优于其他方法的神经基线方法。 |
| [^11] | [POUF: Prompt-oriented unsupervised fine-tuning for large pre-trained models.](http://arxiv.org/abs/2305.00350) | 本文提出了一种基于提示的无监督微调框架，可以在未标记的目标数据上微调大型预训练模型以适应下游任务，实验结果表明该方法在图像分类、情感分析和自然语言推理等任务中表现更好。 |
| [^12] | [A Cognitive Account of the Puzzle of Ideography.](http://arxiv.org/abs/2305.00296) | 符号表意的认知机制解释为什么语言在人类交流中占主导地位 |
| [^13] | [Hierarchical Dialogue Understanding with Special Tokens and Turn-level Attention.](http://arxiv.org/abs/2305.00262) | HiDialog是一种有效的用于对话理解的分层模型，其通过插入特殊标记和提出轮次级别的注意力来建模不同轮次的语义变化，并利用异构图模块来优化所学的嵌入。在对话关系提取，对话情感识别和对话行为分类任务中，HiDialog取得了最先进的性能。 |
| [^14] | [A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions.](http://arxiv.org/abs/2305.00237) | 本文综述了ChatGPT在教育、营销、软件工程和医疗保健领域的潜在应用、局限性和未来方向，探讨ChatGPT作为一种高级语言交互机器人的研究现状与实践意义。 |
| [^15] | [Still no evidence for an effect of the proportion of non-native speakers on language complexity -- A response to Kauhanen, Einhaus & Walkden (2023).](http://arxiv.org/abs/2305.00217) | 本研究为对Kauhanen、Einhaus和Walkden（2023）的回应，仍然没有证据表明大量的L2用户影响语言复杂性。 |
| [^16] | [Examining European Press Coverage of the Covid-19 No-Vax Movement: An NLP Framework.](http://arxiv.org/abs/2305.00182) | 本文旨在研究欧洲传统新闻媒体在反对Covid-19抗疫拒绝疫苗运动和相关虚假信息方面的作用。结果表明，欧洲高质量媒体积极反对社交媒体上传播的虚假信息，并对拒绝疫苗的趋势持批评态度。 |
| [^17] | [Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.](http://arxiv.org/abs/2305.00118) | 通过名字填空成员推断查询，该研究考古了ChatGPT和GPT-4已知的图书，发现这些模型已经记忆了大量受版权保护的材料，这支持了一个使用已知训练数据的开放模型案例。 |
| [^18] | [NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis.](http://arxiv.org/abs/2305.00090) | 本文研究了低资源多语言情感分析的问题，提出了一种利用适应性预训练和源语言选择的方法，能够显著提高性能，并且在SemEval-2023 Task 12中获得了最先进的表现。 |
| [^19] | [HausaNLP at SemEval-2023 Task 10: Transfer Learning, Synthetic Data and Side-Information for Multi-Level Sexism Classification.](http://arxiv.org/abs/2305.00076) | 本研究探讨了使用迁移学习、合成数据和辅助信息来进行多层次性别歧视分类，并在SemEval-2023 Task 10中取得了具有竞争力的结果。 |
| [^20] | [Explainable Verbal Reasoner Plus (EVR+): A Natural Language Reasoning Framework that Supports Diverse Compositional Reasoning.](http://arxiv.org/abs/2305.00061) | EVR+是一种语言推理框架，通过允许生成和执行符号运算符以及将复杂任务分解为多个简单任务等方式增强了语言模型的组合推理能力。它支持更多种类的推理，例如嵌套循环和不同类型的递归。 |
| [^21] | [Causal Reasoning and Large Language Models: Opening a New Frontier for Causality.](http://arxiv.org/abs/2305.00050) | 大型语言模型在因果推理任务中取得了新的最高准确率，但是其鲁棒性仍然存在难以预测的失败模式。 |
| [^22] | [Text-Blueprint: An Interactive Platform for Plan-based Conditional Generation.](http://arxiv.org/abs/2305.00034) | 这篇论文介绍了一个基于web浏览器的交互式平台，使用一系列问题 - 回答对作为蓝图计划，以引导文本生成，并说明用户如何通过编辑和修改蓝图来改善或控制生成的输出。 |
| [^23] | [HQP: A Human-Annotated Dataset for Detecting Online Propaganda.](http://arxiv.org/abs/2304.14931) | HQP是一个人工标注的网络宣传检测数据集，与现有的弱标签数据集相比，使用HQP进行训练可以提高44%的准确率。 |
| [^24] | [Towards autonomous system: flexible modular production system enhanced with large language model agents.](http://arxiv.org/abs/2304.14721) | 本论文介绍了一种将大语言模型、数字孪生和工业自动化系统相结合的框架，实现生产过程的智能化规划和控制。通过LLM代理的协调控制，实现了灵活生产的自主规划和控制，能够处理未预定义的任务并规划生产过程。 |
| [^25] | [Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery.](http://arxiv.org/abs/2304.13714) | 本研究评估了在临床环境中使用GPT-3.5和GPT-4解决医学问题的安全性以及与信息技术咨询服务报告的一致性。研究结果表明，两个LLMs都可以以安全和一致的方式满足医生的信息需求。 |
| [^26] | [ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries.](http://arxiv.org/abs/2304.13620) | 本文提出了ChartSumm数据集，用于长短摘要自动生成任务，包括84000多个图表及其元数据和描述。研究发现，现有的自动摘要模型虽然得分不错，但经常面临错觉、漏掉重要数据点以及不正确解释复杂趋势等问题。 |
| [^27] | [SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts.](http://arxiv.org/abs/2304.09548) | SemEval 2023举办了LegalEval共享任务，即理解法律文本，包括 自动结构化和语义连贯化的法律文件（Task-A），法律命名实体识别（Task-B）以及自动预测法律案件结果和提供预测解释（Task-C）。26个团队提交了系统论文并在所有子任务中优于基准线，但仍有改进空间。 |
| [^28] | [A Comprehensive Evaluation of the Copy Mechanism for Natural Language to SPARQL Query Generation.](http://arxiv.org/abs/2304.07772) | 本研究综合评估自然语言到SPARQL查询生成中的复制机制，通过大量实验研究预训练和非预训练模型、问题注释格式以及使用复制机制的影响，并证明了在这些方面的优化可以提高性能。 |
| [^29] | [Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales.](http://arxiv.org/abs/2304.06875) | 提出了一种新的方法muP，可以提高超参数的缩放律的拟合精度，减少对大模型超参数的搜索，从而实现在大规模模型上进行损失预测。 |
| [^30] | [SemEval-2023 Task 12: Sentiment Analysis for African Languages (AfriSenti-SemEval).](http://arxiv.org/abs/2304.06845) | SemEval-2023举办了非洲语言情感分析挑战赛（AfriSenti-SemEval），旨在提供非洲语言的情感分类数据集。该挑战赛包括单语、多语言和零样本分类三个子任务，并吸引了众多研究者的参与。 |
| [^31] | [Could a Large Language Model be Conscious?.](http://arxiv.org/abs/2303.07103) | 本文分析了大型语言模型是否具有意识的可能性，目前的模型存在着意识的显著障碍，但未来十年随着障碍被克服，后继的大型语言模型可能会具有意识。 |
| [^32] | [Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings.](http://arxiv.org/abs/2303.05737) | 本文提出了一种临床BERTScore（CBERTScore）度量，它比其他度量更严厉地惩罚临床相关的错误，更接近于临床医生对医学句子的偏好。作者还收集了13个临床医生对149个现实医学句子的偏好基准，称为临床转录偏好基准（CTP），证明CBERTScore更接近于临床医生的偏好，并将基准发布给社区以进一步开发具有临床意识的ASR度量。 |
| [^33] | [Extrapolative Controlled Sequence Generation via Iterative Refinement.](http://arxiv.org/abs/2303.04562) | 本文提出了一种名为ICE的新方法来解决外推控制序列生成问题，该方法使用迭代控制编辑技术，能够在自动设计领域，特别是药物研究领域取得较优的性能表现。 |
| [^34] | [A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT.](http://arxiv.org/abs/2302.09419) | 本文全面回顾了预训练基础模型的最新研究进展和发展历程，包括它们的架构、培训目标、预培训任务、微调策略和评估。同时，讨论了其局限性和未来研究方向。 |
| [^35] | [Qualitative Analysis of a Graph Transformer Approach to Addressing Hate Speech: Adapting to Dynamically Changing Content.](http://arxiv.org/abs/2301.10871) | 本研究探讨了一种利用图形转换器网络和基于BERT的自然语言处理模型，结合注意力机制，预测社交媒体中仇恨言论的方法。该方法通过考虑帖子后续的讨论来成功检测出可能出现仇恨言论的情况。研究还探讨了所提出模型的有效性和不足，以及未来可以将其扩展到更全面的方向。 |
| [^36] | [Visual Semantic Relatedness Dataset for Image Captioning.](http://arxiv.org/abs/2301.08784) | 本文提出了一个新的数据集，扩展了COCO字幕数据集，向其中添加了包括场景信息等更多文本内容，可以帮助图像字幕系统更好地理解场景和物体，提高其性能。 |
| [^37] | [Multi-Aspect Explainable Inductive Relation Prediction by Sentence Transformer.](http://arxiv.org/abs/2301.01664) | 本文提出了关系路径覆盖率和关系路径置信度的概念，以在模型训练之前过滤出不可靠的路径，提高基于句子Transformer的知识推理句子Transformer的性能，从而在KG中实现了多方面的可解释感知关系预测。 |
| [^38] | [Hungry Hungry Hippos: Towards Language Modeling with State Space Models.](http://arxiv.org/abs/2212.14052) | 本文针对SSMs在语言建模上表现不足以及硬件利用率低下的问题，提出了一种新的SSM层H3，并将其与建模关注机制相结合，通过硬件优化实现了语言建模基准的最新性能，突出SSMs在语言建模中的潜力。 |
| [^39] | [Dataless Knowledge Fusion by Merging Weights of Language Models.](http://arxiv.org/abs/2212.09849) | 本文提出了一种无数据知识融合方法，可以合并在不同训练数据集上建立的单个模型，以得到一个在所有数据集领域上表现良好且可以推广到域外数据的单一模型。 |
| [^40] | [SceneGATE: Scene-Graph based co-Attention networks for TExt visual question answering.](http://arxiv.org/abs/2212.08283) | 本文提出了一种基于场景图的共同关注网络（SceneGATE）用于TextVQA，通过发现场景图的潜在语义，捕捉了图像中物体、OCR标记和问题词之间的语义关系，并在Text-VQA和ST-VQA两个基准数据集上表现显著优于现有方法。 |
| [^41] | [Exploring Segmentation Approaches for Neural Machine Translation of Code-Switched Egyptian Arabic-English Text.](http://arxiv.org/abs/2210.06990) | 本文研究了不同分割方法对Egyptian Arabic-English代码转换神经机器翻译的影响，实验结果表明形态学感知分段器在分段任务上表现最佳，但在MT任务中表现欠佳。但对于极低资源情形，形态学感知分段器和频率分割结合使用可以获得最佳结果。 |
| [^42] | [Iterative Document-level Information Extraction via Imitation Learning.](http://arxiv.org/abs/2210.06600) | 本论文提出了一种迭代式信息抽取模型IterX，通过模仿学习的方式解决了模板顺序问题，并取得了在多个基准测试中的最先进结果。 |
| [^43] | [MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers.](http://arxiv.org/abs/2210.06425) | MiniALBERT 是一种模型蒸馏技术，结合了跨层参数共享等策略，将完全参数化的语言模型知识转换成为紧凑递归学生模型。MiniALBERT 在基准 NLP 任务上的实验表明，它在性能上优于多个最先进的紧凑型语言模型，并且具有更少的参数数量。 |
| [^44] | [Transformer-based Text Classification on Unified Bangla Multi-class Emotion Corpus.](http://arxiv.org/abs/2210.06405) | 该论文针对孟加拉语这一低资源语言，提出了基于Transformer的模型进行情感分类的方法。该模型在统一孟加拉多分类情感语料库（UBMEC）上的表现优于多个最先进的模型。 |
| [^45] | [Multilingual Representation Distillation with Contrastive Learning.](http://arxiv.org/abs/2210.05033) | 本文将对比学习融入到多语言表示蒸馏中，用于平行语句的质量估计，实验证明在不同低资源语言中方法优于之前的句子编码器，例如 LASER，LASER3 和 LaBSE。 |
| [^46] | [Conditional Generation with a Question-Answering Blueprint.](http://arxiv.org/abs/2207.00397) | 本文提出了一种新的文本规划方法，将其作为一系列问答对的序列来带有问答蓝图的条件生成，通过该方法可以提高生成结果的忠实度、覆盖率和信息量。 |
| [^47] | [Offline RL for Natural Language Generation with Implicit Language Q Learning.](http://arxiv.org/abs/2206.11871) | 本研究提出了一种新颖的自然语言生成离线强化学习方法ILQL，通过在学习价值函数时采用价值保守性和隐式数据集支持约束的组合，引导语言模型生成最大化用户指定的效用函数的语言输出，从而解决巨型语言模型完成用户指定任务时存在的不一致性问题。 |
| [^48] | [Meta Self-Refinement for Robust Learning with Weak Supervision.](http://arxiv.org/abs/2205.07290) | 提出了一个名为元自我完善（MSR）的抗噪声学习框架，采用教师模型来提供高度自信的标签，并通过聚合不同轮次学生模型的输出来更新教师模型，较现有最先进方法有明显提高。 |
| [^49] | [Hierarchical Softmax for End-to-End Low-resource Multilingual Speech Recognition.](http://arxiv.org/abs/2204.03855) | 本文提出了一种利用邻近语言提高低资源场景下多语言语音识别效果的方法，通过构建哈夫曼树实现多语言的分层Softmax解码，从而实现类似标记之间的跨语言知识共享，为低资源语音识别的准确性和效率提升提供了有效途径。 |
| [^50] | [Machine Explanations and Human Understanding.](http://arxiv.org/abs/2202.04092) | 研究讨论了机器解释和人类理解之间的相互作用，并确定了三个核心概念。结果显示，在没有关于特定任务直觉的假设下，解释可提高人类对模型决策边界的理解，但对任务决策边界和模型错误则没有充分证据支持。 |
| [^51] | [YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone.](http://arxiv.org/abs/2112.02418) | YourTTS提出了一种新方法，实现了面向零数据多说话人的语音合成与零数据语音转换任务，并在相关数据集上实现了最新的最佳效果，并为低资源语言的零数据多说话人语音合成提供了可能性。 |
| [^52] | [Toward Subgraph-Guided Knowledge Graph Question Generation with Graph Neural Networks.](http://arxiv.org/abs/2004.06015) | 该研究提出了一种基于图神经网络的新模型，可以从知识图谱子图和目标答案中生成问题，并且在两个基准测试中表现出优于现有方法的新的最先进的得分。 |
| [^53] | [Error correction and extraction in request dialogs.](http://arxiv.org/abs/2004.04243) | 该论文提出了一种对话系统实用组件，可自动检测和修正用户发出的请求信息中的错误，并将修正信息进行提取对，以实现学习和避免重复开发的优势。该方法适用于多种语序列标签、序列到序列和序列分类等情形。 |

# 详细

[^1]: 分解增强推理的自我评估引导解码

    Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding. (arXiv:2305.00633v1 [cs.CL])

    [http://arxiv.org/abs/2305.00633](http://arxiv.org/abs/2305.00633)

    本论文提出了一种通过自我评估引导解码提高推理的方法，使用经过校准的自动标准探索推理搜索空间，使搜索能够产生更高质量的最终预测结果；使用自我评估引导的随机束搜索在产生推理链的质量和多样性之间平衡权衡，适应多数投票，并且可以准确判断逻辑错误，提高一致性和鲁棒性。

    

    我们提出了一种有效的提示方法，通过随机束搜索结合自我评估引导。我们的方法使用经过校准的自动标准探索推理搜索空间。这使得有效搜索能够产生更高质量的最终预测结果。使用自我评估引导的随机束搜索，我们在产生推理链的质量和多样性之间平衡权衡，从而能够适应多数投票，并在GSM8K、AQUA和StrategyQA基准测试中以少量示例准确性分别超越对应的Codex-backboned基线$6.34\%$、$9.56\%$和$5.46\%$。对我们的分解式推理分析发现，它可以指出逻辑错误并导致更高的一致性和鲁棒性。

    We propose an effective prompting approach that integrates self-evaluation guidance through stochastic beam search. Our approach explores the reasoning search space using a well-calibrated automatic criterion. This enables an efficient search to produce higher-quality final predictions. With the self-evaluation guided stochastic beam search, we also balance the quality--diversity trade-off in the generation of reasoning chains. This allows our approach to adapt well with majority voting and surpass the corresponding Codex-backboned baselines by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQUA, and StrategyQA benchmarks, respectively, in few-shot accuracy. Analysis of our decompositional reasoning finds it pinpoints logic failures and leads to higher consistency and robustness.
    
[^2]: 面向塞内加尔沃洛夫语的低资源机器翻译

    Low-Resourced Machine Translation for Senegalese Wolof Language. (arXiv:2305.00606v1 [cs.CL])

    [http://arxiv.org/abs/2305.00606](http://arxiv.org/abs/2305.00606)

    本文介绍了一种低资源机器翻译方法，面向沃洛夫语，使用递归神经网络模型，并提供了一个沃洛夫语/法语的平行语料库，在子单词的训练数据和法语-英语语言对上，该方法表现出更好的性能。

    

    自然语言处理(NLP)研究在近年来取得了重大进展，建立了新的基准，但这些进展主要惠及资源丰富的语言，如英语和法语。大多数其他语言仍然缺乏足够的资源，如撒哈拉以南的非洲语言之一的沃洛夫语。本文介绍了一种包含12.3万个句子的沃洛夫语/法语平行语料库，我们在其中进行了基于递归神经网络(RNN)的机器翻译模型的实验，并对不同数据配置进行了比较。我们注意到，在子单词训练的数据和法语-英语语言对上训练的模型上表现出了性能提升，相比在相同实验条件下使用法语-沃洛夫语言对进行训练的模型。

    Natural Language Processing (NLP) research has made great advancements in recent years with major breakthroughs that have established new benchmarks. However, these advances have mainly benefited a certain group of languages commonly referred to as resource-rich such as English and French. Majority of other languages with weaker resources are then left behind which is the case for most African languages including Wolof. In this work, we present a parallel Wolof/French corpus of 123,000 sentences on which we conducted experiments on machine translation models based on Recurrent Neural Networks (RNN) in different data configurations. We noted performance gains with the models trained on subworded data as well as those trained on the French-English language pair compared to those trained on the French-Wolof pair under the same experimental conditions.
    
[^3]: 可靠的无梯度和无似然对话式建模API优化方法

    Reliable Gradient-free and Likelihood-free Prompt Tuning. (arXiv:2305.00593v1 [cs.LG])

    [http://arxiv.org/abs/2305.00593](http://arxiv.org/abs/2305.00593)

    本文提供一种能够应对挑战性情景，即仅具备API访问权限的情况下，建模API进行优化的方法，并能够对推理不确定性进行量化。

    

    由于隐私或商业限制，大型预训练语言模型（PLMs）通常作为黑盒API提供。对这些模型进行下游任务的微调是具有挑战性的，因为既无法访问模型的内部表示，也无法通过它传播梯度。本文通过开发只有API访问权限的PLM的自适应技术来应对这些挑战。在最近的软提示调整工作的基础上，我们开发了在不需要计算梯度的情况下调整软提示的方法。此外，我们扩展了这些技术，不需要访问PLM除了输入嵌入之外的任何内部表示。我们的方法学习了提示的分布，而不是单一的提示，可以对推理不确定性进行量化，这是在仅具有API访问权限的情况下考虑提示不确定性的首次尝试。最后，通过广泛的实验，我们仔细检查了所提出的方法，并表明它们的性能与基于梯度和基于似然的方法相当，甚至更好。

    Due to privacy or commercial constraints, large pre-trained language models (PLMs) are often offered as black-box APIs. Fine-tuning such models to downstream tasks is challenging because one can neither access the model's internal representations nor propagate gradients through it. This paper addresses these challenges by developing techniques for adapting PLMs with only API access. Building on recent work on soft prompt tuning, we develop methods to tune the soft prompts without requiring gradient computation. Further, we develop extensions that in addition to not requiring gradients also do not need to access any internal representation of the PLM beyond the input embeddings. Moreover, instead of learning a single prompt, our methods learn a distribution over prompts allowing us to quantify predictive uncertainty. Ours is the first work to consider uncertainty in prompts when only having API access to the PLM. Finally, through extensive experiments, we carefully vet the proposed meth
    
[^4]: GPT-2是如何计算大于符号的？解释预训练语言模型中的数学能力

    How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])

    [http://arxiv.org/abs/2305.00586](http://arxiv.org/abs/2305.00586)

    本研究运用机械式可解释性技术探究了GPT-2 Small的数学能力，并确定了它的计算图中的一个小电路用于计算大于符号，该电路的多层感知器提高了结束年份大于开始年份的概率，并且该电路具有广泛的适用性。

    

    预训练语言模型在未被明确训练的任务上表现出惊人的能力，但它们如何实现这些功能却不为人所知。本文通过机械式可解释性技术探究预训练语言模型通常具有的基本数学能力。具体来说，我们以GPT-2 Small为例，研究其能否通过输入"战争持续时间是从1732年到17年"，预测出有效的两位数字的截止年份 (大于32年)。我们首先确定了一个电路，即GPT-2 Small计算图的一个小子集，用于计算这个任务的输出，然后我们解释了每个电路组件的作用，显示出GPT-2 Small的最终多层感知器提高了结束年份大于开始年份的概率。最后，我们证明了我们的电路适用于其他任务，在其他大于场景中发挥作用。

    Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years > 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
    
[^5]: 多模态图转换器用于多模态问答

    Multimodal Graph Transformer for Multimodal Question Answering. (arXiv:2305.00581v1 [cs.CV])

    [http://arxiv.org/abs/2305.00581](http://arxiv.org/abs/2305.00581)

    本文提出了一种新的多模态图转换器，它使用图神经网络将文本和视觉数据的多模态图形信息与vanilla self-attention机制相结合，以增强模型理解结构化输入数据的能力，并在两种具有挑战性的多模态问答基准上取得了性能提升。

    

    尽管Transformer模型在视觉和语言任务中取得了成功，但它们通常只是暗示性地学习庞大的数据，并且不能直接利用结构化输入数据。另一方面，结构化学习方法如图神经网络（GNN）可以整合先前的信息，但与Transformer模型几乎无法竞争。在这项工作中，我们旨在从两个世界中受益，并提出了一种新的多模态图转换器，用于需要在多个模态之间进行推理的问答任务。我们引入了一个涉及图形的即插即用准注意力机制，以将从文本和视觉数据中获取的多模态图形信息并入 vanilla self-attention 中作为有效先验知识。特别地，我们构建文本图、密集区域图和语义图生成邻接矩阵，然后将它们与输入的视觉和语言特征组合以执行下游推理。使用图形信息来规范self-attention机制，增强了模型理解结构化输入数据的能力，并且在两种具有挑战性的多模态问答基准(VQA和GQA)上取得了明显的性能提升。

    Despite the success of Transformer models in vision and language tasks, they often learn knowledge from enormous data implicitly and cannot utilize structured input data directly. On the other hand, structured learning approaches such as graph neural networks (GNNs) that integrate prior information can barely compete with Transformer models. In this work, we aim to benefit from both worlds and propose a novel Multimodal Graph Transformer for question answering tasks that requires performing reasoning across multiple modalities. We introduce a graph-involved plug-and-play quasi-attention mechanism to incorporate multimodal graph information, acquired from text and visual data, to the vanilla self-attention as effective prior. In particular, we construct the text graph, dense region graph, and semantic graph to generate adjacency matrices, and then compose them with input vision and language features to perform downstream reasoning. Such a way of regularizing self-attention with graph in
    
[^6]: 如何从上下文无关文法中枚举树

    How to enumerate trees from a context-free grammar. (arXiv:2305.00522v1 [cs.CL])

    [http://arxiv.org/abs/2305.00522](http://arxiv.org/abs/2305.00522)

    本文介绍了一种简单的算法，用于枚举上下文无关文法（CFG）生成的树，提供了通用的方法来对自然逻辑语言中的表达式进行编号，并可能扩展到其他组合问题。

    

    本文介绍了一种简单的算法，用于枚举上下文无关文法（CFG）生成的树。该算法使用配对函数将CFG推导和自然数形成一一映射，从而可以通过计数唯一解码树。这提供了一种通用的方法来对自然逻辑语言中的表达式进行编号，并且可能可以扩展到其他组合问题。此外，我还展示了如何将此算法推广到更一般的派生形式，包括树上Lempel-Ziv编码的类比。

    I present a simple algorithm for enumerating the trees generated by a Context Free Grammar (CFG). The algorithm uses a pairing function to form a bijection between CFG derivations and natural numbers, so that trees can be uniquely decoded from counting. This provides a general way to number expressions in natural logical languages, and potentially can be extended to other combinatorial problems. I also show how this algorithm may be generalized to more general forms of derivation, including analogs of Lempel-Ziv coding on trees.
    
[^7]: SMILE：利用ChatGPT实现单轮到多轮包容性语言扩展的心理健康支持

    SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support. (arXiv:2305.00450v1 [cs.CL])

    [http://arxiv.org/abs/2305.00450](http://arxiv.org/abs/2305.00450)

    本研究提出了SMILE方法，使用ChatGPT将公共单轮对话扩展为多轮对话，生成了大规模、多样化、接近真实生活的多轮心理健康支持对话语料库，可用于训练和评估专门的对话系统。

    

    开发专门的对话系统以提供心理健康支持已成为越来越多的研究关注点。然而，由于个人信息的敏感性以及所需的时间和成本，获取大规模的真实多轮心理健康支持对话存在困难。为了解决这些问题，我们引入了SMILE方法，一种使用ChatGPT将公共单轮对话扩展为多轮对话的包容性语言扩展技术。我们首先进行了初步的探索性研究，验证了SMILE方法的有效性。此外，我们对使用和未使用SMILE方法生成的数据集进行了全面系统的对比分析，证明SMILE方法可以产生大规模、多样化、接近真实生活的多轮心理健康支持对话语料库，包括对话主题、词汇和语义特征。最后，我们使用收集的语料库来训练和评估专门的心理健康支持对话系统。

    There has been an increasing research interest in developing specialized dialogue systems that can offer mental health support. However, gathering large-scale and real-life multi-turn conversations for mental health support poses challenges due to the sensitivity of personal information, as well as the time and cost involved. To address these issues, we introduce the SMILE approach, an inclusive language expansion technique that employs ChatGPT to extend public single-turn dialogues into multi-turn ones. Our research first presents a preliminary exploratory study that validates the effectiveness of the SMILE approach. Furthermore, we conduct a comprehensive and systematic contrastive analysis of datasets generated with and without the SMILE approach, demonstrating that the SMILE method results in a large-scale, diverse, and close-to-real-life multi-turn mental health support conversation corpus, including dialog topics, lexical and semantic features. Finally, we use the collected corpu
    
[^8]: 构建一份中英双语儿童非母语语音语料库：编制和原理

    Building a Non-native Speech Corpus Featuring Chinese-English Bilingual Children: Compilation and Rationale. (arXiv:2305.00446v1 [cs.CL])

    [http://arxiv.org/abs/2305.00446](http://arxiv.org/abs/2305.00446)

    本文介绍了一份非母语语音语料库，由50名中英双语儿童提供英语（L2）叙述，为第二语言教学提供了有价值的资源并有可能提高自动语音识别（ASR）的整体性能。

    

    本文介绍了一份非母语语音语料库，包含了来自50位5-6岁的中英双语儿童的叙述。提供了总计6.5小时的英语（L2）叙述理解测试的转录文本，以及评分和语法及发音错误的注释。这些孩子还完成了中文（L1）的平行MAIN测试以便进行参考。对于所有测试，我们记录了音频和视频，并采用了自主开发的远程收集方法。视频记录有助于减轻在转录过程中由年幼儿童的L2叙述低可懂性所带来的难度。该语料库为第二语言教学提供了有价值的资源，并有可能提高自动语音识别（ASR）的整体性能。

    This paper introduces a non-native speech corpus consisting of narratives from fifty 5- to 6-year-old Chinese-English children. Transcripts totaling 6.5 hours of children taking a narrative comprehension test in English (L2) are presented, along with human-rated scores and annotations of grammatical and pronunciation errors. The children also completed the parallel MAIN tests in Chinese (L1) for reference purposes. For all tests we recorded audio and video with our innovative self-developed remote collection methods. The video recordings serve to mitigate the challenge of low intelligibility in L2 narratives produced by young children during the transcription process. This corpus offers valuable resources for second language teaching and has the potential to enhance the overall performance of automatic speech recognition (ASR).
    
[^9]: 从国家漏洞数据库的文本描述中构建知识图谱

    Constructing a Knowledge Graph from Textual Descriptions of Software Vulnerabilities in the National Vulnerability Database. (arXiv:2305.00382v1 [cs.CR])

    [http://arxiv.org/abs/2305.00382](http://arxiv.org/abs/2305.00382)

    本文提出了一种从国家漏洞数据库信息中构建漏洞知识图谱的新方法，结合了命名实体识别、关系提取和实体预测。该方法有助于解决网络安全知识图谱中缺失实体的问题。

    

    知识图谱已经显示出了在多个网络安全领域，例如漏洞评估和威胁分析方面的潜力。在本文中，我们提出了一种从国家漏洞数据库的信息中构建漏洞知识图谱的新方法。我们的方法结合了命名实体识别（NER）、关系提取（RE）、以及使用神经模型、启发式规则和知识图谱嵌入的实体预测。我们演示了我们的方法如何有助于解决网络安全知识图谱中缺失实体的问题，并对其性能进行了评估。

    Knowledge graphs have shown promise for several cybersecurity tasks, such as vulnerability assessment and threat analysis. In this work, we present a new method for constructing a vulnerability knowledge graph from information in the National Vulnerability Database (NVD). Our approach combines named entity recognition (NER), relation extraction (RE), and entity prediction using a combination of neural models, heuristic rules, and knowledge graph embeddings. We demonstrate how our method helps to fix missing entities in knowledge graphs used for cybersecurity and evaluate the performance.
    
[^10]: S2abEL：一份用于科学表格实体链接的数据集

    S2abEL: A Dataset for Entity Linking from Scientific Tables. (arXiv:2305.00366v1 [cs.CL])

    [http://arxiv.org/abs/2305.00366](http://arxiv.org/abs/2305.00366)

    该论文提供了第一个专注于科学表格的 EL 数据集 S2abEL，用于实体链接任务。由于科学知识库的不完整性和语境影响，科学表格上的 EL 具有挑战性，该数据集专注于机器学习结果表中的 EL，包含手工标记的单元格类型、属性和实体链接，并引入了一种优于其他方法的神经基线方法。

    

    实体链接（EL）是将文本提及链接到知识库中相应条目的任务，这对于许多知识密集型的自然语言处理应用来说是至关重要的。当应用于科学论文中的表格时，EL是实现大规模科学知识库的一步，这可以实现先进的科学问答和分析。我们提供了第一个针对科学表格中的EL的数据集。科学表格的EL尤其具有挑战性，因为科学知识库可能非常不完整，并且通常需要理解论文中的文本以及表格的上下文来消除歧义。我们的数据集S2abEL专注于机器学习结果表中的EL，并包括来自PaperswithCode分类法的8,429个单元格的手工标记的单元格类型、来源属性和实体链接。我们引入了一种针对科学表格的神经基线方法，该方法包含许多知识库之外提及的实体，并显示它明显优于其他方法。

    Entity linking (EL) is the task of linking a textual mention to its corresponding entry in a knowledge base, and is critical for many knowledge-intensive NLP applications. When applied to tables in scientific papers, EL is a step toward large-scale scientific knowledge bases that could enable advanced scientific question answering and analytics. We present the first dataset for EL in scientific tables. EL for scientific tables is especially challenging because scientific knowledge bases can be very incomplete, and disambiguating table mentions typically requires understanding the papers's tet in addition to the table. Our dataset, S2abEL, focuses on EL in machine learning results tables and includes hand-labeled cell types, attributed sources, and entity links from the PaperswithCode taxonomy for 8,429 cells from 732 tables. We introduce a neural baseline method designed for EL on scientific tables containing many out-of-knowledge-base mentions, and show that it significantly outperfor
    
[^11]: POUF: 面向提示的无监督大型预训练模型微调

    POUF: Prompt-oriented unsupervised fine-tuning for large pre-trained models. (arXiv:2305.00350v1 [cs.LG])

    [http://arxiv.org/abs/2305.00350](http://arxiv.org/abs/2305.00350)

    本文提出了一种基于提示的无监督微调框架，可以在未标记的目标数据上微调大型预训练模型以适应下游任务，实验结果表明该方法在图像分类、情感分析和自然语言推理等任务中表现更好。

    

    通过提示，大规模预训练模型在近年来变得更加表现出色和强大。虽然这些大型模型具有零-shot 能力，但通常仍需要有标签的数据来适应下游任务。为了克服这个关键限制，我们提出了一种无监督微调框架，直接在未标记的目标数据上微调模型或提示。我们演示如何将该方法应用于语言增强的视觉和掩蔽语言模型，通过对齐从提示和目标数据中提取的离散分布来实现。为了验证我们方法的适用性，我们对图像分类、情感分析和自然语言推理任务进行了广泛的实验。在 13 个与图像相关的任务和 15 个与语言相关的任务中，该方法均比基线表现更好。

    Through prompting, large-scale pre-trained models have become more expressive and powerful, gaining significant attention in recent years. Though these big models have zero-shot capabilities, in general, labeled data are still required to adapt them to downstream tasks. To overcome this critical limitation, we propose an unsupervised fine-tuning framework to directly fine-tune the model or prompt on the unlabeled target data. We demonstrate how to apply our method to both language-augmented vision and masked-language models by aligning the discrete distributions extracted from the prompts and target data. To verify our approach's applicability, we conduct extensive experiments on image classification, sentiment analysis, and natural language inference tasks. Across 13 image-related tasks and 15 language-related ones, the proposed approach achieves consistent improvements over the baselines.
    
[^12]: 符号表意的认知解释之谜

    A Cognitive Account of the Puzzle of Ideography. (arXiv:2305.00296v1 [q-bio.NC])

    [http://arxiv.org/abs/2305.00296](http://arxiv.org/abs/2305.00296)

    符号表意的认知机制解释为什么语言在人类交流中占主导地位

    

    在这篇对Morin的《符号表意的谜题》的评论文章中，我们提出了一个关于表意符号谜题的认知新解释，它补充了Morin的规范化解释。有效的口语语言规范化被现象学地归因为模态效应与认知表征的块状化，进一步受多感官整合和注意力串行本质的帮助。这些认知机制对于解释为什么语言在通用人类交流中占主导地位至关重要。

    In this commentary article to 'The Puzzle of Ideography' by Morin, we put forth a new cognitive account of the puzzle of ideography, that complements the standardization account of Morin. Efficient standardization of spoken language is phenomenologically attributed to a modality effect coupled with chunking of cognitive representations, further aided by multi-sensory integration and the serialized nature of attention. These cognitive mechanisms are crucial for explaining why languages dominate graphic codes for general-purpose human communication.
    
[^13]: 带有特殊标记和轮次级别注意力的分层对话理解

    Hierarchical Dialogue Understanding with Special Tokens and Turn-level Attention. (arXiv:2305.00262v1 [cs.CL])

    [http://arxiv.org/abs/2305.00262](http://arxiv.org/abs/2305.00262)

    HiDialog是一种有效的用于对话理解的分层模型，其通过插入特殊标记和提出轮次级别的注意力来建模不同轮次的语义变化，并利用异构图模块来优化所学的嵌入。在对话关系提取，对话情感识别和对话行为分类任务中，HiDialog取得了最先进的性能。

    

    相对于标准文本，机器理解对话更具挑战性，因为每个轮次中语义的动态和意外的变化。为了模拟这种不一致的语义，我们提出了一个简单但有效的分层对话理解模型HiDialog。具体而言，我们首先在对话中插入多个特殊标记，并提出轮次级别的注意力来层次学习轮次嵌入。然后，利用异构图模块来优化所学的嵌入。我们对各种对话理解任务进行评估，包括对话关系提取，对话情感识别和对话行为分类。结果表明，我们简单的方法在以上所有三个任务上都达到了最先进的性能。我们的所有源代码都公开在https://github.com/ShawX825/HiDialog上。

    Compared with standard text, understanding dialogue is more challenging for machines as the dynamic and unexpected semantic changes in each turn. To model such inconsistent semantics, we propose a simple but effective Hierarchical Dialogue Understanding model, HiDialog. Specifically, we first insert multiple special tokens into a dialogue and propose the turn-level attention to learn turn embeddings hierarchically. Then, a heterogeneous graph module is leveraged to polish the learned embeddings. We evaluate our model on various dialogue understanding tasks including dialogue relation extraction, dialogue emotion recognition, and dialogue act classification. Results show that our simple approach achieves state-of-the-art performance on all three tasks above. All our source code is publicly available at https://github.com/ShawX825/HiDialog.
    
[^14]: ChatGPT在教育、营销、软件工程和医疗保健方面的应用综述：优势、缺陷和研究方向

    A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions. (arXiv:2305.00237v1 [cs.CL])

    [http://arxiv.org/abs/2305.00237](http://arxiv.org/abs/2305.00237)

    本文综述了ChatGPT在教育、营销、软件工程和医疗保健领域的潜在应用、局限性和未来方向，探讨ChatGPT作为一种高级语言交互机器人的研究现状与实践意义。

    

    ChatGPT是一种人工智能语言模型，使用深度学习算法生成类似于人类对文本提示的回复。最新的ChatGPT版本于2022年11月推出，其强大的功能、大量的可能应用以及滥用的可能性在产业和学术界引起了轰动。在撰写本文时，其他几个语言模型（例如Google Bard和Meta LLaMA）也刚刚推出，试图在庞大的潜在市场中占据一席之地。这些模型具有革命性的能力，可以应用于很多领域，包括教育、软件工程、医疗保健和营销。本文将讨论在每个领域使用先进语言交互机器人（例如ChatGPT）的可能应用、缺陷和研究方向。我们首先简要介绍了ChatGPT的发展时间线。

    ChatGPT is a type of artificial intelligence language model that uses deep learning algorithms to generate human-like responses to text-based prompts. The introduction of the latest ChatGPT version in November of 2022 has caused shockwaves in the industrial and academic communities for its powerful capabilities, plethora of possible applications, and the great possibility for abuse. At the time of writing this work, several other language models (e.g., Google Bard and Meta LLaMA) just came out in an attempt to get a foothold in the vast possible market. These models have the ability to revolutionize the way we interact with computers and have potential applications in many fields, including education, software engineering, healthcare, and marketing. In this paper, we will discuss the possible applications, drawbacks, and research directions using advanced language Chatbots (e.g., ChatGPT) in each of these fields. We first start with a brief introduction and the development timeline of 
    
[^15]: 对Kauhanen、Einhaus和Walkden（2023年）的回应：仍然没有证据证明非母语用户比例对语言复杂度有影响（arXiv:2305.00217v1 [cs.CL]）

    Still no evidence for an effect of the proportion of non-native speakers on language complexity -- A response to Kauhanen, Einhaus & Walkden (2023). (arXiv:2305.00217v1 [cs.CL])

    [http://arxiv.org/abs/2305.00217](http://arxiv.org/abs/2305.00217)

    本研究为对Kauhanen、Einhaus和Walkden（2023）的回应，仍然没有证据表明大量的L2用户影响语言复杂性。

    

    近期在《语言进化杂志》发表的一篇论文中，Kauhanen、Einhaus和Walkden（https://doi.org/10.1093/jole/lzad005，KEW）挑战了我在一篇论文中（Koplenig，Royal Society Open Science，6，181274（2019），https://doi.org/10.1098/rsos.181274）所呈现的结果。在该论文中，我试图通过一系列的统计分析来表明大量L2（第二语言）用户似乎不会影响语言的（语法或统计）复杂性。为此，我专注于Ethnologue评估语言地位的方式：如果一种语言除了被L1（第一语言）使用者之外，还应该有大量的L2使用者，那么该语言就被描述为传播性的。KEW批评了将传播性作为语言是否拥有大量L2使用者（二元）指标的使用，以及在直接估计L2比例的情况下，将L2用户比例归为非传播性语言的想法。

    In a recent paper published in the Journal of Language Evolution, Kauhanen, Einhaus & Walkden (https://doi.org/10.1093/jole/lzad005, KEW) challenge the results presented in one of my papers (Koplenig, Royal Society Open Science, 6, 181274 (2019), https://doi.org/10.1098/rsos.181274), in which I tried to show through a series of statistical analyses that large numbers of L2 (second language) speakers do not seem to affect the (grammatical or statistical) complexity of a language. To this end, I focus on the way in which the Ethnologue assesses language status: a language is characterised as vehicular if, in addition to being used by L1 (first language) speakers, it should also have a significant number of L2 users. KEW criticise both the use of vehicularity as a (binary) indicator of whether a language has a significant number of L2 users and the idea of imputing a zero proportion of L2 speakers to non-vehicular languages whenever a direct estimate of that proportion is unavailable. Whi
    
[^16]: 研究欧洲新闻如何报道 Covid-19 抗疫拒绝疫苗运动：基于 NLP 的框架分析

    Examining European Press Coverage of the Covid-19 No-Vax Movement: An NLP Framework. (arXiv:2305.00182v1 [cs.CL])

    [http://arxiv.org/abs/2305.00182](http://arxiv.org/abs/2305.00182)

    本文旨在研究欧洲传统新闻媒体在反对Covid-19抗疫拒绝疫苗运动和相关虚假信息方面的作用。结果表明，欧洲高质量媒体积极反对社交媒体上传播的虚假信息，并对拒绝疫苗的趋势持批评态度。

    

    本文分析了欧洲新闻媒体如何处理抗疫拒绝疫苗运动，以及与该运动相关的虚假信息和错误信息。使用22个月期间（2020-2021年）来自19家欧洲报纸的1786篇文章的策划数据集，运用自然语言处理技术，包括主题建模、情感分析、语义关系和命名实体识别，以及语义网络等，以了解欧洲传统新闻媒体在虚假信息生态系统中的特定作用。多角度分析的结果表明，欧洲知名媒体积极反对社交媒体上流传的各种虚假信息，对拒绝疫苗的趋势持批评态度，无论报纸的政治方向如何。这证实了研究高质量新闻媒体在虚假信息生态系统中的作用的相关性。

    This paper examines how the European press dealt with the no-vax reactions against the Covid-19 vaccine and the dis- and misinformation associated with this movement. Using a curated dataset of 1786 articles from 19 European newspapers on the anti-vaccine movement over a period of 22 months in 2020-2021, we used Natural Language Processing techniques including topic modeling, sentiment analysis, semantic relationship with word embeddings, political analysis, named entity recognition, and semantic networks, to understand the specific role of the European traditional press in the disinformation ecosystem. The results of this multi-angle analysis demonstrate that the European well-established press actively opposed a variety of hoaxes mainly spread on social media, and was critical of the anti-vax trend, regardless of the political orientation of the newspaper. This confirms the relevance of studying the role of high-quality press in the disinformation ecosystem.
    
[^17]: 聊天GPT/GPT-4已知的图书的考古学研究

    Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4. (arXiv:2305.00118v1 [cs.CL])

    [http://arxiv.org/abs/2305.00118](http://arxiv.org/abs/2305.00118)

    通过名字填空成员推断查询，该研究考古了ChatGPT和GPT-4已知的图书，发现这些模型已经记忆了大量受版权保护的材料，这支持了一个使用已知训练数据的开放模型案例。

    

    本研究使用名字填空成员推断查询，对ChatGPT和GPT-4已知的图书进行数据考古学推断，发现OpenAI模型已经记忆了大量受版权保护的材料，并且记忆的程度与这些书籍在网上出现的频率有关。这些模型记忆未知的书籍的能力使得文化分析的测量有效性评估变得复杂，因为它会污染测试数据。我们展示了这些模型在记忆的书籍上的表现远远优于未记忆的书籍在下游任务中的表现，这支持了一个使用已知训练数据的开放模型案例。

    In this work, we carry out a data archaeology to infer books that are known to ChatGPT and GPT-4 using a name cloze membership inference query. We find that OpenAI models have memorized a wide collection of copyrighted materials, and that the degree of memorization is tied to the frequency with which passages of those books appear on the web. The ability of these models to memorize an unknown set of books complicates assessments of measurement validity for cultural analytics by contaminating test data; we show that models perform much better on memorized books than on non-memorized books for downstream tasks. We argue that this supports a case for open models whose training data is known.
    
[^18]: NLNDE在SemEval-2023第12任务中：适应性预训练和来源语言选择用于低资源多语言情感分析

    NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis. (arXiv:2305.00090v1 [cs.CL])

    [http://arxiv.org/abs/2305.00090](http://arxiv.org/abs/2305.00090)

    本文研究了低资源多语言情感分析的问题，提出了一种利用适应性预训练和源语言选择的方法，能够显著提高性能，并且在SemEval-2023 Task 12中获得了最先进的表现。

    

    本文描述了我们为SemEval-2023第12项任务“使用Twitter数据集进行低资源非洲语言情感分析”开发的系统。情感分析是自然语言处理中研究广泛的应用之一。然而，大多数先前的研究仍然集中在少数高资源语言上。由于此任务中训练数据有限，为低资源语言建立可靠的情感分析系统仍然具有挑战性。在这项工作中，我们提出利用语言自适应和任务自适应预训练非洲文本，并在非洲语言为中心的预训练语言模型的基础上研究源语言选择的迁移学习。我们的关键发现是：（1）使用较小但相关的语料库将预训练模型适应目标语言和任务可以使F1分数提高10个百分点以上。（2）在训练期间选择具有正迁移增益的源语言可以避免来自不同源语言的有害干扰并实现更好的性能。（3）所提出的方法在Afrikaans和Zulu语言的SemEval-2023 Task 12中均取得了最先进的表现。

    This paper describes our system developed for the SemEval-2023 Task 12 "Sentiment Analysis for Low-resource African Languages using Twitter Dataset". Sentiment analysis is one of the most widely studied applications in natural language processing. However, most prior work still focuses on a small number of high-resource languages. Building reliable sentiment analysis systems for low-resource languages remains challenging, due to the limited training data in this task. In this work, we propose to leverage language-adaptive and task-adaptive pretraining on African texts and study transfer learning with source language selection on top of an African language-centric pretrained language model. Our key findings are: (1) Adapting the pretrained model to the target language and task using a small yet relevant corpus improves performance remarkably by more than 10 F1 score points. (2) Selecting source languages with positive transfer gains during training can avoid harmful interference from di
    
[^19]: HausaNLP在SemEval-2023 Task 10中的应用：基于迁移学习、合成数据和辅助信息的多层次性别歧视分类

    HausaNLP at SemEval-2023 Task 10: Transfer Learning, Synthetic Data and Side-Information for Multi-Level Sexism Classification. (arXiv:2305.00076v1 [cs.CL])

    [http://arxiv.org/abs/2305.00076](http://arxiv.org/abs/2305.00076)

    本研究探讨了使用迁移学习、合成数据和辅助信息来进行多层次性别歧视分类，并在SemEval-2023 Task 10中取得了具有竞争力的结果。

    

    本文介绍我们参与 SemEval-2023 Task 10 的结果，这是一项针对英文Gab和Reddit数据集进行恶意语言（性别歧视）分类的任务。我们研究了使用两个语言模型进行迁移学习的效果：XLM-T（情感分类）和HateBERT（相同领域--Reddit）。我们还利用未标记数据的合成分类和中间类信息来最大化模型的性能。我们在Task A中提交了一个系统，排名第49名，F1分数为0.82。这个结果表明它很有竞争力，因为它只比最佳系统低0.052％的F1得分。

    We present the findings of our participation in the SemEval-2023 Task 10: Explainable Detection of Online Sexism (EDOS) task, a shared task on offensive language (sexism) detection on English Gab and Reddit dataset. We investigated the effects of transferring two language models: XLM-T (sentiment classification) and HateBERT (same domain -- Reddit) for multi-level classification into Sexist or not Sexist, and other subsequent sub-classifications of the sexist data. We also use synthetic classification of unlabelled dataset and intermediary class information to maximize the performance of our models. We submitted a system in Task A, and it ranked 49th with F1-score of 0.82. This result showed to be competitive as it only under-performed the best system by 0.052% F1-score.
    
[^20]: 可解释的语言推理增强器：支持各种组合推理的自然语言推理框架

    Explainable Verbal Reasoner Plus (EVR+): A Natural Language Reasoning Framework that Supports Diverse Compositional Reasoning. (arXiv:2305.00061v1 [cs.CL])

    [http://arxiv.org/abs/2305.00061](http://arxiv.org/abs/2305.00061)

    EVR+是一种语言推理框架，通过允许生成和执行符号运算符以及将复杂任务分解为多个简单任务等方式增强了语言模型的组合推理能力。它支持更多种类的推理，例如嵌套循环和不同类型的递归。

    

    自然语言模型已成功应用于各种自然语言处理推理任务，但仍然面临组合推理泛化问题。本文提出了一种名为“可解释的语言推理增强器（EVR+）”的推理框架，通过以下方式增强语言模型的组合推理能力：（1）允许模型明确生成和执行符号运算符，（2）以灵活的方式将复杂任务分解为多个简单任务。与其前身Explainable Verbal Reasoner (EVR)和采用类似思路的其他方法相比，我们的框架支持更多种类的推理，例如嵌套循环和不同类型的递归。为了评估我们的推理框架，我们构建了一个合成数据集，其中包括需要组合推理的5个任务。结果表明，我们的推理框架可以提高语言模型在这5个任务中的组合推理性能。

    Languages models have been successfully applied to a variety of reasoning tasks in NLP, yet the language models still suffer from compositional generalization. In this paper we present Explainable Verbal Reasoner Plus (EVR+), a reasoning framework that enhances language models' compositional reasoning ability by (1) allowing the model to explicitly generate and execute symbolic operators, and (2) allowing the model to decompose a complex task into several simpler ones in a flexible manner. Compared with its predecessor Explainable Verbal Reasoner (EVR) and other previous approaches adopting similar ideas, our framework supports more diverse types of reasoning such as nested loops and different types of recursion. To evaluate our reasoning framework, we build a synthetic dataset with five tasks that require compositional reasoning. Results show that our reasoning framework can enhance the language model's compositional generalization performance on the five tasks, using a fine-tuned lan
    
[^21]: 因果推理与大型语言模型：开启因果研究的新篇章

    Causal Reasoning and Large Language Models: Opening a New Frontier for Causality. (arXiv:2305.00050v1 [cs.AI])

    [http://arxiv.org/abs/2305.00050](http://arxiv.org/abs/2305.00050)

    大型语言模型在因果推理任务中取得了新的最高准确率，但是其鲁棒性仍然存在难以预测的失败模式。

    

    大型语言模型的因果能力备受争议，并且对将其应用于医学、科学、法律和政策等具有社会影响力的领域具有重要意义。我们进一步探讨了LLMs及其因果推理的区别，以及潜在的建构和测量效度威胁。基于GPT-3.5和4的算法在多个因果基准测试上取得了新的最高准确率。与此同时，LLMs展示了难以预测的失败模式，我们提供了一些技术来解释它们的鲁棒性。

    The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness.  Crucially, LLMs perform these causal tasks while relying on sources of knowledg
    
[^22]: 文本蓝图：基于计划的条件生成的交互平台

    Text-Blueprint: An Interactive Platform for Plan-based Conditional Generation. (arXiv:2305.00034v1 [cs.CL])

    [http://arxiv.org/abs/2305.00034](http://arxiv.org/abs/2305.00034)

    这篇论文介绍了一个基于web浏览器的交互式平台，使用一系列问题 - 回答对作为蓝图计划，以引导文本生成，并说明用户如何通过编辑和修改蓝图来改善或控制生成的输出。

    

    虽然条件生成模型现在已经能够生成自然语言以创建流畅的文本，但控制生成过程仍然困难，导致生成无关，重复和虚假的内容。最近的研究表明，规划可以是一个有用的中间步骤，使条件生成 less opaque and more grounded. 我们提出了一个基于web浏览器的演示，用于基于查询的摘要，该摘要使用一系列问题 - 回答对作为蓝图计划，以引导文本生成（即，如何说什么以及以什么顺序）。我们演示了用户如何与生成的文本和相关的计划可视化进行交互，例如通过编辑和修改蓝图以改善或控制生成的输出。我们提供了一个短视频来演示我们的系统，可在https://goo.gle/text-blueprint-demo获得。

    While conditional generation models can now generate natural language well enough to create fluent text, it is still difficult to control the generation process, leading to irrelevant, repetitive, and hallucinated content. Recent work shows that planning can be a useful intermediate step to render conditional generation less opaque and more grounded. We present a web browser-based demonstration for query-focused summarization that uses a sequence of question-answer pairs, as a blueprint plan for guiding text generation (i.e., what to say and in what order). We illustrate how users may interact with the generated text and associated plan visualizations, e.g., by editing and modifying the blueprint in order to improve or control the generated output.  A short video demonstrating our system is available at https://goo.gle/text-blueprint-demo.
    
[^23]: HQP：一份人工标注的用于检测网络宣传的数据集

    HQP: A Human-Annotated Dataset for Detecting Online Propaganda. (arXiv:2304.14931v1 [cs.CL])

    [http://arxiv.org/abs/2304.14931](http://arxiv.org/abs/2304.14931)

    HQP是一个人工标注的网络宣传检测数据集，与现有的弱标签数据集相比，使用HQP进行训练可以提高44%的准确率。

    

    网络宣传对社会的完整性构成了严重威胁。然而，现有的检测网络宣传的数据集存在一个关键限制：它们是使用弱标签进行注释的，可能存在噪音甚至错误。为了解决这一限制，本研究做出了以下贡献：（1）我们提出了一个新的数据集HQP（N=30,000），用于检测网络宣传，具有高质量的标注。据我们所知，这是第一个通过人工注释而创建的用于检测网络宣传的数据集。（2）我们证明了，在使用弱标签进行训练时，最先进的语言模型在检测网络宣传方面失败（AUC：64.03）。相比之下，当使用我们的高质量标签进行训练时，最先进的语言模型可以准确地检测网络宣传（AUC：92.25），提高了约44%。（3）为了解决标注成本问题，我们将我们的工作扩展到了少样本学习。具体来说，我们展示了使用一个小型数据集进行提示式学习的方法。

    Online propaganda poses a severe threat to the integrity of societies. However, existing datasets for detecting online propaganda have a key limitation: they were annotated using weak labels that can be noisy and even incorrect. To address this limitation, our work makes the following contributions: (1) We present \dataset: a novel dataset (N=30,000) for detecting online propaganda with high-quality labels. To the best of our knowledge, \dataset is the first dataset for detecting online propaganda that was created through human annotation. (2) We show empirically that state-of-the-art language models fail in detecting online propaganda when trained with weak labels (AUC: 64.03). In contrast, state-of-the-art language models can accurately detect online propaganda when trained with our high-quality labels (AUC: 92.25), which is an improvement of ~44%. (3) To address the cost of labeling, we extend our work to few-shot learning. Specifically, we show that prompt-based learning using a sm
    
[^24]: 朝自主系统迈进：使用大语言模型代理增强的灵活模块化生产系统

    Towards autonomous system: flexible modular production system enhanced with large language model agents. (arXiv:2304.14721v1 [cs.RO])

    [http://arxiv.org/abs/2304.14721](http://arxiv.org/abs/2304.14721)

    本论文介绍了一种将大语言模型、数字孪生和工业自动化系统相结合的框架，实现生产过程的智能化规划和控制。通过LLM代理的协调控制，实现了灵活生产的自主规划和控制，能够处理未预定义的任务并规划生产过程。

    

    本文提出了一种新颖的框架，将大型语言模型（LLM），数字孪生和工业自动化系统结合起来，实现生产过程的智能规划和控制。我们的方法涉及开发包含生产描述信息的数字孪生系统，并将自动化系统改造为提供统一接口的细粒度功能或模块，以供自动化组件或模块执行。随后，设计LLM代理来解释数字孪生中的描述性信息，并通过RESTful接口控制物理系统。这些LLM代理作为自动化系统内的智能代理，实现了灵活生产的自主规划和控制。给定一个任务指令作为输入，LLM代理协调一系列原子功能和技能来完成任务。我们展示了我们实现的原型如何处理未预定义的任务，并计划生产过程。

    In this paper, we present a novel framework that combines large language models (LLMs), digital twins and industrial automation system to enable intelligent planning and control of production processes. Our approach involves developing a digital twin system that contains descriptive information about the production and retrofitting the automation system to offer unified interfaces of fine-granular functionalities or skills executable by automation components or modules. Subsequently, LLM-Agents are designed to interpret descriptive information in the digital twins and control the physical system through RESTful interfaces. These LLM-Agents serve as intelligent agents within an automation system, enabling autonomous planning and control of flexible production. Given a task instruction as input, the LLM-agents orchestrate a sequence of atomic functionalities and skills to accomplish the task. We demonstrate how our implemented prototype can handle un-predefined tasks, plan a production p
    
[^25]: 评估GPT-3.5和GPT-4在支持医疗保健信息需求方面的实际作用

    Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. (arXiv:2304.13714v1 [cs.AI])

    [http://arxiv.org/abs/2304.13714](http://arxiv.org/abs/2304.13714)

    本研究评估了在临床环境中使用GPT-3.5和GPT-4解决医学问题的安全性以及与信息技术咨询服务报告的一致性。研究结果表明，两个LLMs都可以以安全和一致的方式满足医生的信息需求。

    

    尽管在医疗保健领域使用大型语言模型(LLMs)越来越受关注，但当前的探索并未评估LLMs在临床环境中的实用性和安全性。我们的目标是确定两个LLM是否可以以安全和一致的方式满足由医生提交的信息需求问题。我们将66个来自信息技术咨询服务的问题通过简单的提示提交给GPT-3.5和GPT-4。12名医生评估了LLM响应对患者造成伤害的可能性以及与信息技术咨询服务的现有报告的一致性。医生的评估基于多数票汇总。对于没有任何问题，大多数医生认为任何一个LLM响应都不会造成伤害。对于GPT-3.5，8个问题的响应与信息技术咨询报告一致，20个不一致，9个无法评估。有29个响应没有多数票表示“同意”、“不同意”和“无法评估”。

    Despite growing interest in using large language models (LLMs) in healthcare, current explorations do not assess the real-world utility and safety of LLMs in clinical settings. Our objective was to determine whether two LLMs can serve information needs submitted by physicians as questions to an informatics consultation service in a safe and concordant manner. Sixty six questions from an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple prompts. 12 physicians assessed the LLM responses' possibility of patient harm and concordance with existing reports from an informatics consultation service. Physician assessments were summarized based on majority vote. For no questions did a majority of physicians deem either LLM response as harmful. For GPT-3.5, responses to 8 questions were concordant with the informatics consult report, 20 discordant, and 9 were unable to be assessed. There were 29 responses with no majority on "Agree", "Disagree", and "Unable to assess". Fo
    
[^26]: ChartSumm：长短摘要自动生成任务的全面基准数据集

    ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries. (arXiv:2304.13620v1 [cs.CL])

    [http://arxiv.org/abs/2304.13620](http://arxiv.org/abs/2304.13620)

    本文提出了ChartSumm数据集，用于长短摘要自动生成任务，包括84000多个图表及其元数据和描述。研究发现，现有的自动摘要模型虽然得分不错，但经常面临错觉、漏掉重要数据点以及不正确解释复杂趋势等问题。

    

    自动将图表转换为文本摘要是视障人士的有效工具，同时为用户提供表格数据的自然语言精确洞察力。大型、结构良好的数据集始终是数据驱动模型的关键部分。本文提出了ChartSumm：一个大规模基准数据集，包括共84363个图表及其元数据和描述，涵盖广泛的主题和图表类型，可生成长短摘要。强基线模型的广泛实验表明，尽管这些模型通过实现各种自动评估指标的得分来生成流畅且信息丰富的摘要，但它们经常遇到一些问题，例如产生错觉，漏掉重要的数据点，以及不正确地解释图表中的复杂趋势。我们还通过自动翻译工具探讨了将ChartSumm扩展到其他语言的潜力。这使得我们的数据集成为一个有挑战的任务。

    Automatic chart to text summarization is an effective tool for the visually impaired people along with providing precise insights of tabular data in natural language to the user. A large and well-structured dataset is always a key part for data driven models. In this paper, we propose ChartSumm: a large-scale benchmark dataset consisting of a total of 84,363 charts along with their metadata and descriptions covering a wide range of topics and chart types to generate short and long summaries. Extensive experiments with strong baseline models show that even though these models generate fluent and informative summaries by achieving decent scores in various automatic evaluation metrics, they often face issues like suffering from hallucination, missing out important data points, in addition to incorrect explanation of complex trends in the charts. We also investigated the potential of expanding ChartSumm to other languages using automated translation tools. These make our dataset a challeng
    
[^27]: SemEval 2023 任务6: LegalEval -- 理解法律文本

    SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts. (arXiv:2304.09548v1 [cs.CL])

    [http://arxiv.org/abs/2304.09548](http://arxiv.org/abs/2304.09548)

    SemEval 2023举办了LegalEval共享任务，即理解法律文本，包括 自动结构化和语义连贯化的法律文件（Task-A），法律命名实体识别（Task-B）以及自动预测法律案件结果和提供预测解释（Task-C）。26个团队提交了系统论文并在所有子任务中优于基准线，但仍有改进空间。

    

    在人口众多的国家，待处理的法律案件呈指数增长。有必要开发基于自然语言处理的技术，对法律文件进行处理和自动理解。为了促进在法律自然语言处理领域的研究，我们在 SemEval 2023 上组织了共享任务 LegalEval - 理解法律文本。LegalEval 任务有三个子任务：Task-A（修辞角色标记）是自动将法律文件结构化为语义连贯的单元，Task-B（法律命名实体识别）处理在法律文件中识别相关实体，而 Task-C（法院判决预测与解释）探索了自动预测法律案件结果以及提供预测解释的可能性。共有26个团队（分布在全球的约100名参与者）提交了系统论文。在每个子任务中，所提出的系统都优于基准线；但是，仍然有很大的改进空间。本文介绍了 LegalEval 任务的组织和细节，并概述了参与系统及其性能。

    In populous countries, pending legal cases have been growing exponentially. There is a need for developing NLP-based techniques for processing and automatically understanding legal documents. To promote research in the area of Legal NLP we organized the shared task LegalEval - Understanding Legal Texts at SemEval 2023. LegalEval task has three sub-tasks: Task-A (Rhetorical Roles Labeling) is about automatically structuring legal documents into semantically coherent units, Task-B (Legal Named Entity Recognition) deals with identifying relevant entities in a legal document and Task-C (Court Judgement Prediction with Explanation) explores the possibility of automatically predicting the outcome of a legal case along with providing an explanation for the prediction. In total 26 teams (approx. 100 participants spread across the world) submitted systems paper. In each of the sub-tasks, the proposed systems outperformed the baselines; however, there is a lot of scope for improvement. This pape
    
[^28]: 自然语言到SPARQL查询生成的复制机制综合评估

    A Comprehensive Evaluation of the Copy Mechanism for Natural Language to SPARQL Query Generation. (arXiv:2304.07772v1 [cs.CL])

    [http://arxiv.org/abs/2304.07772](http://arxiv.org/abs/2304.07772)

    本研究综合评估自然语言到SPARQL查询生成中的复制机制，通过大量实验研究预训练和非预训练模型、问题注释格式以及使用复制机制的影响，并证明了在这些方面的优化可以提高性能。

    

    近年来，神经机器翻译（NMT）领域在SPARQL查询生成方面有了显著的增长。最近，将复制机制与传统的编码器-解码器架构相结合，并使用预训练的编码器-解码器，创造了新的性能基准。本文展示了大量的实验，复制并扩展了最近的基于NMT的SPARQL生成研究，比较了预训练和非预训练模型、问题注释格式以及对于非预训练和预训练模型使用复制机制的影响。我们的结果表明，对于非预训练模型和预训练模型，添加复制机制或使用问题注释都可以提高性能，并为三个流行数据集设置了新的基准。

    In recent years, the field of neural machine translation (NMT) for SPARQL query generation has witnessed a significant growth. Recently, the incorporation of the copy mechanism with traditional encoder-decoder architectures and the use of pre-trained encoder-decoders have set new performance benchmarks. This paper presents a large variety of experiments that replicate and expand upon recent NMT-based SPARQL generation studies, comparing pre-trained and non-pre-trained models, question annotation formats, and the use of a copy mechanism for non-pre-trained and pre-trained models. Our results show that either adding the copy mechanism or using a question annotation improves performances for nonpre-trained models and for pre-trained models, setting new baselines for three popular datasets.
    
[^29]: 不需重新搜索的研究：最大更新参数化可精确预测跨尺度的损失

    Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v1 [cs.CL])

    [http://arxiv.org/abs/2304.06875](http://arxiv.org/abs/2304.06875)

    提出了一种新的方法muP，可以提高超参数的缩放律的拟合精度，减少对大模型超参数的搜索，从而实现在大规模模型上进行损失预测。

    

    随着语言模型的扩大，验证研究想法变得越来越昂贵，因为小模型的结论不能简单地转移到大模型。解决方案是建立一个通用系统，仅基于小模型的结果和超参数直接预测大模型的一些指标。现有的基于缩放律的方法需要在最大的模型上进行超参数搜索，但由于资源有限，这是不切实际的。我们通过展示我们的发现表明，最大更新参数化（muP）使得可以在靠近常见损失流域的超参数的情况下准确拟合超参数的缩放律，而不需要任何搜索。因此，不同的模型可以在大尺度上进行损失预测，在训练开始之前就可以进行直接比较。我们提出了一种新的范式，作为可靠的学术研究的第一步，适用于任何模型规模，而不需大量的计算。代码将很快公开可用。

    As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones. A possible solution is to establish a generic system that directly predicts some metrics for large models solely based on the results and hyperparameters from small models. Existing methods based on scaling laws require hyperparameter search on the largest models, which is impractical with limited resources. We address this issue by presenting our discoveries indicating that Maximal Update parametrization (muP) enables accurate fitting of scaling laws for hyperparameters close to common loss basins, without any search. Thus, different models can be directly compared on large scales with loss prediction even before the training starts. We propose a new paradigm as a first step towards reliable academic research for any model scale without heavy computation. Code will be publicly available shortly.
    
[^30]: SemEval-2023任务12: 非洲语言情感分析（AfriSenti-SemEval）

    SemEval-2023 Task 12: Sentiment Analysis for African Languages (AfriSenti-SemEval). (arXiv:2304.06845v1 [cs.CL])

    [http://arxiv.org/abs/2304.06845](http://arxiv.org/abs/2304.06845)

    SemEval-2023举办了非洲语言情感分析挑战赛（AfriSenti-SemEval），旨在提供非洲语言的情感分类数据集。该挑战赛包括单语、多语言和零样本分类三个子任务，并吸引了众多研究者的参与。

    

    本文介绍了第一个非洲语材料的SemEval挑战赛——非洲语言情感分析（AfriSenti-SemEval），其中包含14种非洲语言（阿姆哈拉语、阿尔及利亚阿拉伯语、豪萨语、伊博语、卢旺达语、摩洛哥阿拉伯语、莫桑比克葡萄牙语、尼日利亚皮钦语、奥罗莫语、斯瓦希里语、提格里尼亚语、特威语、克森语和约鲁巴语）。我们提供了三个子任务：（1）单语分类，共收到44个提交结果；（2）多语言分类，共收到32个提交结果；（3）零样本分类，共收到34个提交结果。其中，NLNDE团队在任务A和B中分别获得了71.31和75.06加权F1分数的最佳系统。UCAS-IIE-NLP在任务C上平均获得了58.15加权F1分数的最佳系统。

    We present the first Africentric SemEval Shared task, Sentiment Analysis for African Languages (AfriSenti-SemEval) - the dataset is available at https://github.com/afrisenti-semeval/afrisent-semeval-2023. AfriSenti-SemEval is a sentiment classification challenge in 14 African languages - Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda, Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili, Tigrinya, Twi, Xitsonga, and Yor\`ub\'a (Muhammad et al., 2023), using a 3-class labeled data: positive, negative, and neutral. We present three subtasks: (1) Task A: monolingual classification, which received 44 submissions; (2) Task B: multilingual classification, which received 32 submissions; and (3) Task C: zero-shot classification, which received 34 submissions. The best system for tasks A and B was achieved by NLNDE team with 71.31 and 75.06 weighted F1, respectively. UCAS-IIE-NLP achieved the best system on average for task C with 58.15 weighted F1. We describe the variou
    
[^31]: 大型语言模型可能会具有意识吗？

    Could a Large Language Model be Conscious?. (arXiv:2303.07103v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.07103](http://arxiv.org/abs/2303.07103)

    本文分析了大型语言模型是否具有意识的可能性，目前的模型存在着意识的显著障碍，但未来十年随着障碍被克服，后继的大型语言模型可能会具有意识。

    

    最近普遍讨论了大型语言模型是否具有感知或意识。我们是否应该认真考虑这个想法？本文将分析支持和反对这个想法的最有力的理由。根据意识科学中的主流假设，目前的模型存在着意识的显著障碍，例如缺乏循环处理、全局的工作空间和统一的智能机构等等。与此同时，这些障碍在未来十年左右都可能被克服。作者得出的结论是，虽然目前大型语言模型具有意识的可能性较小，但我们应该认真考虑后继的大型语言模型在不久的将来可能会具有意识。

    There has recently been widespread discussion of whether large language models might be sentient or conscious. Should we take this idea seriously? I will break down the strongest reasons for and against. Given mainstream assumptions in the science of consciousness, there are significant obstacles to consciousness in current models: for example, their lack of recurrent processing, a global workspace, and unified agency. At the same time, it is quite possible that these obstacles will be overcome in the next decade or so. I conclude that while it is somewhat unlikely that current large language models are conscious, we should take seriously the possibility that successors to large language models may be conscious in the not-too-distant future.
    
[^32]: 临床BERTScore：临床环境下自动语音识别性能的改进度量

    Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings. (arXiv:2303.05737v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2303.05737](http://arxiv.org/abs/2303.05737)

    本文提出了一种临床BERTScore（CBERTScore）度量，它比其他度量更严厉地惩罚临床相关的错误，更接近于临床医生对医学句子的偏好。作者还收集了13个临床医生对149个现实医学句子的偏好基准，称为临床转录偏好基准（CTP），证明CBERTScore更接近于临床医生的偏好，并将基准发布给社区以进一步开发具有临床意识的ASR度量。

    The paper proposes a Clinical BERTScore (CBERTScore) metric for ASR in medical contexts, which penalizes clinically-relevant mistakes more than other metrics and aligns more closely with clinician preferences. The authors also collect a benchmark of clinician preferences on medical sentences and release it for the community to further develop clinically-aware ASR metrics.

    医学环境中的自动语音识别（ASR）有潜力节省时间，降低成本，提高报告准确性并减少医生的疲劳。然而，由于避免医学相关的转录错误的重要性，医疗行业采用这种技术的速度较慢。在这项工作中，我们提出了临床BERTScore（CBERTScore），这是一种ASR度量，它比其他度量（WER、BLUE、METEOR等）更严厉地惩罚临床相关的错误。我们证明了这个度量更接近于临床医生对医学句子的偏好，有时差距很大。我们收集了13个临床医生对149个现实医学句子的偏好基准，称为临床转录偏好基准（CTP），证明CBERTScore更接近于临床医生的偏好，并将基准发布给社区以进一步开发具有临床意识的ASR度量。

    Automatic Speech Recognition (ASR) in medical contexts has the potential to save time, cut costs, increase report accuracy, and reduce physician burnout. However, the healthcare industry has been slower to adopt this technology, in part due to the importance of avoiding medically-relevant transcription mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR metric that penalizes clinically-relevant mistakes more than others. We demonstrate that this metric more closely aligns with clinician preferences on medical sentences as compared to other metrics (WER, BLUE, METEOR, etc), sometimes by wide margins. We collect a benchmark of 13 clinician preferences on 149 realistic medical sentences called the Clinician Transcript Preference benchmark (CTP), demonstrate that CBERTScore more closely matches what clinicians prefer, and release the benchmark for the community to further develop clinically-aware ASR metrics.
    
[^33]: 迭代修正的外推控制序列生成

    Extrapolative Controlled Sequence Generation via Iterative Refinement. (arXiv:2303.04562v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04562](http://arxiv.org/abs/2303.04562)

    本文提出了一种名为ICE的新方法来解决外推控制序列生成问题，该方法使用迭代控制编辑技术，能够在自动设计领域，特别是药物研究领域取得较优的性能表现。

    

    本研究探讨了外推控制生成问题，即生成属性值超出训练数据范围的序列。在自动设计领域，尤其是药物研究领域，这个任务至关重要，目标是设计出比现有序列更好（例如更稳定）的新型蛋白质。因此，按照定义，目标序列及其属性值超出训练分布，挑战现有直接生成目标序列方法。本研究提出了迭代控制外推（ICE）方法，通过迭代地对序列进行局部编辑来实现外推。我们使用合成的序列对对模型进行训练，演示微小的属性值改进。自然语言任务（情感分析）和两个蛋白质工程任务（ACE2稳定性和AAV适应性）的结果表明，ICE方法明显优于现有的最先进方法。

    We study the problem of extrapolative controlled generation, i.e., generating sequences with attribute values beyond the range seen in training. This task is of significant importance in automated design, especially drug discovery, where the goal is to design novel proteins that are \textit{better} (e.g., more stable) than existing sequences. Thus, by definition, the target sequences and their attribute values are out of the training distribution, posing challenges to existing methods that aim to directly generate the target sequence. Instead, in this work, we propose Iterative Controlled Extrapolation (ICE) which iteratively makes local edits to a sequence to enable extrapolation. We train the model on synthetically generated sequence pairs that demonstrate small improvement in the attribute value. Results on one natural language task (sentiment analysis) and two protein engineering tasks (ACE2 stability and AAV fitness) show that ICE considerably outperforms state-of-the-art approach
    
[^34]: 预训练基础模型综述：从BERT到ChatGPT的历程

    A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT. (arXiv:2302.09419v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.09419](http://arxiv.org/abs/2302.09419)

    本文全面回顾了预训练基础模型的最新研究进展和发展历程，包括它们的架构、培训目标、预培训任务、微调策略和评估。同时，讨论了其局限性和未来研究方向。

    

    预训练基础模型(PFMs)被认为是各种不同数据模态下游任务的基础。PFM(例如BERT、ChatGPT和GPT-4)在大规模数据上进行训练，为各种下游应用提供了合理的参数初始化。BERT从转换器中学习双向编码器表示，这些模型作为上下文语言模型在大型数据集上进行训练。类似地，生成式预训练变压器(GPT)方法采用转换器作为特征提取器，并采用自回归范式在大型数据集上进行训练。最近，ChatGPT在大语言模型中展现了令人兴奋的成功，它采用自回归式语言模型，可以进行零射击或少射击提示。PFM的卓越成就为各种AI领域带来了重大突破。许多研究提出了不同的方法，提高了对更新调查的需求。本研究全面回顾了PFMs的最新进展，包括它们的架构、培训目标、预培训任务、微调策略和评估。此外，我们还讨论了PFMs的局限性和未来潜在的研究方向。

    Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of rec
    
[^35]: 图形转换器方法针对变化动态内容识别仇恨言论的定性分析

    Qualitative Analysis of a Graph Transformer Approach to Addressing Hate Speech: Adapting to Dynamically Changing Content. (arXiv:2301.10871v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10871](http://arxiv.org/abs/2301.10871)

    本研究探讨了一种利用图形转换器网络和基于BERT的自然语言处理模型，结合注意力机制，预测社交媒体中仇恨言论的方法。该方法通过考虑帖子后续的讨论来成功检测出可能出现仇恨言论的情况。研究还探讨了所提出模型的有效性和不足，以及未来可以将其扩展到更全面的方向。

    

    本文探讨了一种利用图形转换器网络和基于BERT的自然语言处理模型，结合注意力机制，预测社交媒体中仇恨言论的方法。该方法通过考虑帖子后续的讨论来成功检测出可能出现仇恨言论的情况。本文对该方法进行了详细的定性分析，探讨了与竞争方法相比在哪些场景下有最优表现以及面临的挑战。同时，本文还探讨了当前社交媒体中存在的恶意图片等各种帖子类型，提出了扩展模型的思路。关键的洞见在于该方法注重对上下文概念的推理，因此该方法具备很大的前景。

    Our work advances an approach for predicting hate speech in social media, drawing out the critical need to consider the discussions that follow a post to successfully detect when hateful discourse may arise. Using graph transformer networks, coupled with modelling attention and BERT-level natural language processing, our approach can capture context and anticipate upcoming anti-social behaviour. In this paper, we offer a detailed qualitative analysis of this solution for hate speech detection in social networks, leading to insights into where the method has the most impressive outcomes in comparison with competitors and identifying scenarios where there are challenges to achieving ideal performance. Included is an exploration of the kinds of posts that permeate social media today, including the use of hateful images. This suggests avenues for extending our model to be more comprehensive. A key insight is that the focus on reasoning about the concept of context positions us well to be a
    
[^36]: 图像字幕的视觉语义相关性数据集

    Visual Semantic Relatedness Dataset for Image Captioning. (arXiv:2301.08784v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.08784](http://arxiv.org/abs/2301.08784)

    本文提出了一个新的数据集，扩展了COCO字幕数据集，向其中添加了包括场景信息等更多文本内容，可以帮助图像字幕系统更好地理解场景和物体，提高其性能。

    

    现代图像字幕系统严重依赖于从图像中提取知识以捕获静态故事的概念。本文提出了一个用于字幕的文本视觉上下文数据集，其中公开可用的COCO字幕数据集(Lin等人，2014)已经扩展了场景信息（例如图像中的对象）。由于这些信息具有文本形式，因此可以将它们用于将任何NLP任务（例如文本相似度或语义关系方法）纳入字幕系统，无论是作为端到端培训策略还是基于后处理的方法。

    Modern image captioning system relies heavily on extracting knowledge from images to capture the concept of a static story. In this paper, we propose a textual visual context dataset for captioning, in which the publicly available dataset COCO Captions (Lin et al., 2014) has been extended with information about the scene (such as objects in the image). Since this information has a textual form, it can be used to leverage any NLP task, such as text similarity or semantic relation methods, into captioning systems, either as an end-to-end training strategy or a post-processing based approach.
    
[^37]: 基于句子Transformer进行多方面的可解释感知关系预测

    Multi-Aspect Explainable Inductive Relation Prediction by Sentence Transformer. (arXiv:2301.01664v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.01664](http://arxiv.org/abs/2301.01664)

    本文提出了关系路径覆盖率和关系路径置信度的概念，以在模型训练之前过滤出不可靠的路径，提高基于句子Transformer的知识推理句子Transformer的性能，从而在KG中实现了多方面的可解释感知关系预测。

    

    最近的知识图谱（KGs）研究表明，基于预训练语言模型的基于路径的方法在提供感知和可解释关系预测方面表现良好。本文提出了关系路径覆盖率和关系路径置信度的概念，以在模型训练之前过滤出不可靠的路径，以提高模型性能。此外，我们提出了“知识推理句子Transformer”（KRST）来预测KG中的感知关系。 KRST被设计为在KG中编码提取出的可靠路径，使我们能够正确地聚类路径并提供多方面的解释。我们在三个真实数据集上进行了广泛的实验。实验结果表明，与SOTA模型相比，KRST在大多数感知和感知测试用例（6个中的4个）以及12个few-shot测试用例中的11个上取得了最佳性能。

    Recent studies on knowledge graphs (KGs) show that path-based methods empowered by pre-trained language models perform well in the provision of inductive and explainable relation predictions. In this paper, we introduce the concepts of relation path coverage and relation path confidence to filter out unreliable paths prior to model training to elevate the model performance. Moreover, we propose Knowledge Reasoning Sentence Transformer (KRST) to predict inductive relations in KGs. KRST is designed to encode the extracted reliable paths in KGs, allowing us to properly cluster paths and provide multi-aspect explanations. We conduct extensive experiments on three real-world datasets. The experimental results show that compared to SOTA models, KRST achieves the best performance in most transductive and inductive test cases (4 of 6), and in 11 of 12 few-shot test cases.
    
[^38]: 饥饿的河马：基于状态空间模型的语言建模方法

    Hungry Hungry Hippos: Towards Language Modeling with State Space Models. (arXiv:2212.14052v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.14052](http://arxiv.org/abs/2212.14052)

    本文针对SSMs在语言建模上表现不足以及硬件利用率低下的问题，提出了一种新的SSM层H3，并将其与建模关注机制相结合，通过硬件优化实现了语言建模基准的最新性能，突出SSMs在语言建模中的潜力。

    

    状态空间模型（SSMs）在某些模态下表现出卓越的序列建模性能，但在语言建模方面表现不足。此外，尽管SSMs的序列长度近乎线性地扩展而不是二次方，但由于硬件利用率低下，它们仍然比变压器更慢。在本文中，我们取得了进展，理解了SSMs和建模关注机制之间的表现差距，并降低了SSMs和建模关注机制之间的硬件障碍。首先，我们使用合成的语言建模任务来理解SSMs和建模关注机制之间的差距。我们发现，现有的SSMs在两个方面存在困难：回忆先前的标记和跨序列比较标记。为了理解对语言建模的影响，我们提出了一个新的SSM层，H3，专门设计这些能力。H3在合成语言上与建模关注机制相匹配，并在OpenWebText上比变压器少了0.4 PPL。此外，一种混合模型，将H3和注意力以及硬件优化相结合，实现了语言建模基准测试的最新性能。我们的工作凸显了SSMs在语言建模方面的潜力，并为如何设计更好的SSMs提供了见解。

    State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 
    
[^39]: 通过合并语言模型的权重实现无数据知识融合

    Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09849](http://arxiv.org/abs/2212.09849)

    本文提出了一种无数据知识融合方法，可以合并在不同训练数据集上建立的单个模型，以得到一个在所有数据集领域上表现良好且可以推广到域外数据的单一模型。

    

    微调预训练语言模型已成为构建下游NLP模型的流行范式。通常情况下，经过微调的模型已经可用，但其训练数据不可用，由于数据隐私或知识产权问题。这就造成了跨模型融合知识以产生更好的单一模型的障碍。在本文中，我们研究了建立在不同训练数据集上的单个模型之间合并的问题，以得到一个在所有数据集领域上表现良好且可以推广到域外数据的单一模型。我们提出了一种无数据知识融合方法，该方法在参数空间中合并模型，由权重引导，以最小化合并模型和单个模型之间的预测差异。在一系列评估设置中，我们展示了该方法显著优于如Fisher加权平均或模型集成等基线。此外，我们发现我们的方法是一个有前途的多语言微调替代方案，因为它可以在不需要任何额外注释数据的情况下实现可比的性能。

    Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-
    
[^40]: SceneGATE：基于场景图的文本视觉问答中的共同关注网络

    SceneGATE: Scene-Graph based co-Attention networks for TExt visual question answering. (arXiv:2212.08283v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.08283](http://arxiv.org/abs/2212.08283)

    本文提出了一种基于场景图的共同关注网络（SceneGATE）用于TextVQA，通过发现场景图的潜在语义，捕捉了图像中物体、OCR标记和问题词之间的语义关系，并在Text-VQA和ST-VQA两个基准数据集上表现显著优于现有方法。

    

    大多数TextVQA方法都注重通过简单的transformer编码器来整合物体、场景文本和问题词，但这无法捕捉不同模态之间的语义关系。本文提出了一种基于场景图的共同关注网络（SceneGATE）用于TextVQA，该网络揭示了图像中物体、光学字符识别（OCR）标记和问题词之间的语义关系。通过一个基于TextVQA的场景图来发现图像的潜在语义，我们创造了一个引导关注模块来捕获语言和视觉之间的内部交互作为跨模态交互的向导。为了明确教授两种模态之间的关系，我们提出并集成了两个注意模块，即基于场景图的语义关系感知注意和位置关系感知注意。我们在两个基准数据集Text-VQA和ST-VQA上进行了广泛的实验。结果表明，我们的SceneGATE在两个数据集上都比现有的方法表现显著。

    Most TextVQA approaches focus on the integration of objects, scene texts and question words by a simple transformer encoder. But this fails to capture the semantic relations between different modalities. The paper proposes a Scene Graph based co-Attention Network (SceneGATE) for TextVQA, which reveals the semantic relations among the objects, Optical Character Recognition (OCR) tokens and the question words. It is achieved by a TextVQA-based scene graph that discovers the underlying semantics of an image. We created a guided-attention module to capture the intra-modal interplay between the language and the vision as a guidance for inter-modal interactions. To make explicit teaching of the relations between the two modalities, we proposed and integrated two attention modules, namely a scene graph-based semantic relation-aware attention and a positional relation-aware attention. We conducted extensive experiments on two benchmark datasets, Text-VQA and ST-VQA. It is shown that our SceneG
    
[^41]: 探索分割方法对Egyptian Arabic-English代码转换神经机器翻译的影响

    Exploring Segmentation Approaches for Neural Machine Translation of Code-Switched Egyptian Arabic-English Text. (arXiv:2210.06990v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.06990](http://arxiv.org/abs/2210.06990)

    本文研究了不同分割方法对Egyptian Arabic-English代码转换神经机器翻译的影响，实验结果表明形态学感知分段器在分段任务上表现最佳，但在MT任务中表现欠佳。但对于极低资源情形，形态学感知分段器和频率分割结合使用可以获得最佳结果。

    

    数据稀疏性是代码转换（CS）中的主要挑战之一，尤其是对于形态丰富的语言。对于机器翻译（MT）任务，在单语境中，形态学分割已经被证明可以缓解数据稀疏性，但是它尚未被用于CS环境中。本文研究不同分割方法对MT性能的影响，包括基于形态和基于频率的分割技术。我们在从代码转换的阿拉伯语-英语到英语的MT上进行实验，并进行详细分析，考虑了各种条件，例如数据大小和具有不同程度CS的句子。实证结果表明，形态学感知分段器在分段任务中表现最佳，但在MT任务中表现欠佳。然而，我们发现选择用于MT的分割设置高度依赖于数据大小。对于极低资源情形，形态学感知分段器和频率分割结合使用可以获得最佳结果。

    Data sparsity is one of the main challenges posed by code-switching (CS), which is further exacerbated in the case of morphologically rich languages. For the task of machine translation (MT), morphological segmentation has proven successful in alleviating data sparsity in monolingual contexts; however, it has not been investigated for CS settings. In this paper, we study the effectiveness of different segmentation approaches on MT performance, covering morphology-based and frequency-based segmentation techniques. We experiment on MT from code-switched Arabic-English to English. We provide detailed analysis, examining a variety of conditions, such as data size and sentences with different degrees of CS. Empirical results show that morphology-aware segmenters perform the best in segmentation tasks but under-perform in MT. Nevertheless, we find that the choice of the segmentation setup to use for MT is highly dependent on the data size. For extreme low-resource scenarios, a combination of
    
[^42]: 通过模仿学习进行迭代式文档级信息抽取

    Iterative Document-level Information Extraction via Imitation Learning. (arXiv:2210.06600v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.06600](http://arxiv.org/abs/2210.06600)

    本论文提出了一种迭代式信息抽取模型IterX，通过模仿学习的方式解决了模板顺序问题，并取得了在多个基准测试中的最先进结果。

    

    我们提出了一种新颖的迭代抽取模型IterX，用于在文档中提取复杂的关系或模板（即将命名槽与文本跨度进行映射的N元组）。模板抽取任务包括识别文档中的模板并提取每个模板的槽值。我们的模仿学习方法将问题作为马尔可夫决策过程（MDP），并消除了使用预定义模板顺序来训练提取器的需要。它在两个已建立的基准测试（SciREX的4元关系抽取和MUC-4的模板抽取）以及新的BETTER Granular任务中取得了最先进的结果。

    We present a novel iterative extraction model, IterX, for extracting complex relations, or templates (i.e., N-tuples representing a mapping from named slots to spans of text) within a document. Documents may feature zero or more instances of a template of any given type, and the task of template extraction entails identifying the templates in a document and extracting each template's slot values. Our imitation learning approach casts the problem as a Markov decision process (MDP), and relieves the need to use predefined template orders to train an extractor. It leads to state-of-the-art results on two established benchmarks -- 4-ary relation extraction on SciREX and template extraction on MUC-4 -- as well as a strong baseline on the new BETTER Granular task.
    
[^43]: 迷你ALBERT: 基于参数高效递归变换的模型蒸馏

    MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers. (arXiv:2210.06425v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.06425](http://arxiv.org/abs/2210.06425)

    MiniALBERT 是一种模型蒸馏技术，结合了跨层参数共享等策略，将完全参数化的语言模型知识转换成为紧凑递归学生模型。MiniALBERT 在基准 NLP 任务上的实验表明，它在性能上优于多个最先进的紧凑型语言模型，并且具有更少的参数数量。

    

    近年来，预训练语言模型 (LM) 由于在下游应用中的卓越表现，成为自然语言处理 (NLP) 中不可或缺的一部分。尽管这一辉煌的成就，LM 的可用性受限于计算和时间复杂度，以及它们日益增长的模型大小；这是被称为“过参数化”的问题。文献中提出了不同的策略来缓解这些问题，旨在创建有效的紧凑模型，将它们的性能与其膨胀的对应物几乎相匹配，而几乎不损失性能。在这个研究领域中最受欢迎的技术之一是模型蒸馏。而另一种强大但不常使用的技术是跨层参数共享。在这项工作中，我们将这两个策略结合起来，提出了 MiniALBERT，一种将完全参数化的 LM 的知识转换为紧凑递归学生的技术，同时我们研究了跨层参数共享和其他技术的效能，进一步提高了 MiniALBERT 的效率和性能。我们在基准 NLP 任务上的实验证明，MiniALBERT 优于多个最先进的紧凑型 LM，并保持更少的参数数量。

    Pre-trained Language Models (LMs) have become an integral part of Natural Language Processing (NLP) in recent years, due to their superior performance in downstream applications. In spite of this resounding success, the usability of LMs is constrained by computational and time complexity, along with their increasing size; an issue that has been referred to as `overparameterisation'. Different strategies have been proposed in the literature to alleviate these problems, with the aim to create effective compact models that nearly match the performance of their bloated counterparts with negligible performance losses. One of the most popular techniques in this area of research is model distillation. Another potent but underutilised technique is cross-layer parameter sharing. In this work, we combine these two strategies and present MiniALBERT, a technique for converting the knowledge of fully parameterised LMs (such as BERT) into a compact recursive student. In addition, we investigate the 
    
[^44]: 基于Transformer的统一孟加拉多分类情感语料库文本分类

    Transformer-based Text Classification on Unified Bangla Multi-class Emotion Corpus. (arXiv:2210.06405v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.06405](http://arxiv.org/abs/2210.06405)

    该论文针对孟加拉语这一低资源语言，提出了基于Transformer的模型进行情感分类的方法。该模型在统一孟加拉多分类情感语料库（UBMEC）上的表现优于多个最先进的模型。

    

    情感分类在研究人们对各种Web 2.0服务的想法方面非常重要。现有研究主要集中在英语上，对低资源语言的研究较少。虽然近年来情感分析，特别是英语中的情感分类已经受到了很多关注，但在孟加拉语这样世界上最流行的语言的情况下却没有进行太多的研究。我们提出了一套完整的方法来识别和提取孟加拉语文本中的情感。我们使用基于Transformer的模型从孟加拉语词汇中构建了一个六类（愤怒、厌恶、害怕、喜悦、悲伤和惊讶）的情感分类器，在最近取得了惊人的结果，尤其是对于高资源语言。我们使用“统一孟加拉多分类情感语料库（UBMEC）”来评估我们模型的性能。UBMEC通过结合两个先前发布的基于机器学习的孟加拉语情感语料库而创建，其中包含超过16,000个孟加拉文本样本。我们提出的模型在准确度、精确度、召回率和F1分数方面优于多个最先进的模型。

    Because of its importance in studying people's thoughts on various Web 2.0 services, emotion classification (EC) is an important undertaking. Existing research, on the other hand, is mostly focused on the English language, with little work on low-resource languages. Though sentiment analysis, particularly the EC in English, has received a lot of attention in recent years, little study has been done in the context of Bangla, one of the world's most widely spoken languages. We propose a complete set of approaches for identifying and extracting emotions from Bangla texts in this research. We provide a Bangla emotion classifier for six classes (anger, disgust, fear, joy, sadness, and surprise) from Bangla words, using transformer-based models which exhibit phenomenal results in recent days, especially for high resource languages. The "Unified Bangla Multi-class Emotion Corpus (UBMEC)" is used to assess the performance of our models. UBMEC was created by combining two previously released ma
    
[^45]: 多语言对比学习下的多语言表示蒸馏

    Multilingual Representation Distillation with Contrastive Learning. (arXiv:2210.05033v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.05033](http://arxiv.org/abs/2210.05033)

    本文将对比学习融入到多语言表示蒸馏中，用于平行语句的质量估计，实验证明在不同低资源语言中方法优于之前的句子编码器，例如 LASER，LASER3 和 LaBSE。

    

    大型模型从多种语言中编码语义信息的多语言句子表示可用于不同的跨语言信息检索和匹配任务。本文将对比学习融入到多语言表示蒸馏中，并将其用于平行语句的质量估计（即查找语义上相似的句子，可用作相互之间的翻译）。我们通过多语言相似性搜索和语料库过滤任务验证了我们的方法。在不同低资源语言的实验中，我们的方法大大优于之前的句子编码器，例如 LASER，LASER3 和 LaBSE。

    Multilingual sentence representations from large models encode semantic information from two or more languages and can be used for different cross-lingual information retrieval and matching tasks. In this paper, we integrate contrastive learning into multilingual representation distillation and use it for quality estimation of parallel sentences (i.e., find semantically similar sentences that can be used as translations of each other). We validate our approach with multilingual similarity search and corpus filtering tasks. Experiments across different low-resource languages show that our method greatly outperforms previous sentence encoders such as LASER, LASER3, and LaBSE.
    
[^46]: 带有问答蓝图的条件生成

    Conditional Generation with a Question-Answering Blueprint. (arXiv:2207.00397v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.00397](http://arxiv.org/abs/2207.00397)

    本文提出了一种新的文本规划方法，将其作为一系列问答对的序列来带有问答蓝图的条件生成，通过该方法可以提高生成结果的忠实度、覆盖率和信息量。

    

    在条件生成的许多任务中，传达相关和真实的信息的能力非常关键，然而神经 seq-to-seq 模型往往会产生幻觉而未能正确涵盖重要细节。在本文中，我们提出了规划作为渲染条件生成较少模糊和更可操作中间表达方式的建议。我们的工作提出了文本规划的新概念，作为一系列问答对的序列。我们通过使用最先进的问答生成技术自动获取蓝图，并将输入-输出对转换为输入蓝图-输出元组。我们开发了基于 Transformer 的模型，每个模型在生成的输出中如何结合蓝图不同（例如作为全局计划或迭代计划）。针对两个条件生成任务的评估显示了我们方法的有效性，包括忠实度、覆盖率和信息量。

    The ability to convey relevant and faithful information is critical for many tasks in conditional generation and yet remains elusive for neural seq-to-seq models whose outputs often reveal hallucinations and fail to correctly cover important details. In this work, we advocate planning as a useful intermediate representation for rendering conditional generation less opaque and more grounded. Our work proposes a new conceptualization of text plans as a sequence of question-answer (QA) pairs. We enhance existing datasets (e.g., for summarization) with a QA blueprint operating as a proxy for both content selection (i.e.,~what to say) and planning (i.e.,~in what order). We obtain blueprints automatically by exploiting state-of-the-art question generation technology and convert input-output pairs into input-blueprint-output tuples. We develop Transformer-based models, each varying in how they incorporate the blueprint in the generated output (e.g., as a global plan or iteratively). Evaluatio
    
[^47]: 基于隐式语言Q学习的自然语言生成离线强化学习方法

    Offline RL for Natural Language Generation with Implicit Language Q Learning. (arXiv:2206.11871v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.11871](http://arxiv.org/abs/2206.11871)

    本研究提出了一种新颖的自然语言生成离线强化学习方法ILQL，通过在学习价值函数时采用价值保守性和隐式数据集支持约束的组合，引导语言模型生成最大化用户指定的效用函数的语言输出，从而解决巨型语言模型完成用户指定任务时存在的不一致性问题。

    

    巨型语言模型从文本语料库中提炼出广泛的知识。然而，当处理用户指定的任务时，它们可能会存在不一致性。这个问题可以通过在精心策划的数据集上进行监督学习或强化学习来解决。本文提出了一种新颖的离线强化学习方法，即隐式语言Q学习（ILQL），专门设计用于语言模型，它结合了RL算法的灵活效用最大化框架与监督学习利用先前收集的数据的能力，以及其简单性和稳定性。我们的方法在学习价值函数时采用价值保守性和隐式数据集支持约束的组合，然后将其用于引导语言模型生成，最大化用户指定的效用函数。除了经验性地验证了ILQL，我们还在自然语言生成中展示了离线RL能够有用的场景的详细经验分析。

    Large language models distill broad knowledge from text corpora. However, they can be inconsistent when it comes to completing user specified tasks. This issue can be addressed by finetuning such models via supervised learning on curated datasets, or via reinforcement learning. In this work, we propose a novel offline RL method, implicit language Q-learning (ILQL), designed for use on language models, that combines both the flexible utility maximization framework of RL algorithms with the ability of supervised learning to leverage previously collected data, as well as its simplicity and stability. Our method employs a combination of value conservatism alongside an implicit dataset support constraint in learning value functions, which are then used to guide language model generations towards maximizing user-specified utility functions. In addition to empirically validating ILQL, we present a detailed empirical analysis of situations where offline RL can be useful in natural language gen
    
[^48]: 元自我完善：使用弱监督实现鲁棒学习

    Meta Self-Refinement for Robust Learning with Weak Supervision. (arXiv:2205.07290v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.07290](http://arxiv.org/abs/2205.07290)

    提出了一个名为元自我完善（MSR）的抗噪声学习框架，采用教师模型来提供高度自信的标签，并通过聚合不同轮次学生模型的输出来更新教师模型，较现有最先进方法有明显提高。

    

    在弱监督下训练深度神经网络（DNN）已经引起越来越多的研究关注，因为它可以显著降低注释成本。然而，弱监督的标签可能存在噪声，而DNN的高容量使其很容易过度适应标签噪声，导致泛化性能差。最近的方法利用自我训练来构建抗噪声模型，其中使用在弱监督下训练的教师模型来为学生模型提供高度自信的标签。然而，从这种框架中推导出的教师模型可能已经拟合了大量噪声，并因此产生了高置信度的错误伪标签，导致严重的错误传播。在这项工作中，我们提出了一种名为元自我完善（MSR）的抗噪声学习框架，以有效地抵抗来自弱监督的标签噪声。我们鼓励教师模型改善其伪标签，而不是依赖于使用噪声标签训练的固定教师模型。在每次训练迭代中，MSR同时更新学生和教师模型，其中教师模型通过聚合不同轮次学生模型的输出来更新。我们在几个基准数据集上进行了大量实验，使用不同类型的弱监督，我们的提出的MSR方法始终提高了分类性能，相对于现有最先进方法有明显的提高。

    Training deep neural networks (DNNs) under weak supervision has attracted increasing research attention as it can significantly reduce the annotation cost. However, labels from weak supervision can be noisy, and the high capacity of DNNs enables them to easily overfit the label noise, resulting in poor generalization. Recent methods leverage self-training to build noise-resistant models, in which a teacher trained under weak supervision is used to provide highly confident labels for teaching the students. Nevertheless, the teacher derived from such frameworks may have fitted a substantial amount of noise and therefore produce incorrect pseudo-labels with high confidence, leading to severe error propagation. In this work, we propose Meta Self-Refinement (MSR), a noise-resistant learning framework, to effectively combat label noise from weak supervision. Instead of relying on a fixed teacher trained with noisy labels, we encourage the teacher to refine its pseudo-labels. At each training
    
[^49]: 面向低资源多语言语音识别的分层Softmax方法

    Hierarchical Softmax for End-to-End Low-resource Multilingual Speech Recognition. (arXiv:2204.03855v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2204.03855](http://arxiv.org/abs/2204.03855)

    本文提出了一种利用邻近语言提高低资源场景下多语言语音识别效果的方法，通过构建哈夫曼树实现多语言的分层Softmax解码，从而实现类似标记之间的跨语言知识共享，为低资源语音识别的准确性和效率提升提供了有效途径。

    

    低资源语音识别长期以来一直备受困扰，本文提出了一种方法，利用邻近语言来提高低资源场景的性能。我们的假设是，在相邻语言中，类似的语言单元呈现出可比的词项频率分布，这使我们能够构建一个哈夫曼树来执行多语言的分层Softmax解码。这种分层结构能够实现类似标记之间的跨语言知识共享，从而增强低资源训练结果。经验分析表明，我们的方法有效地提高了低资源语音识别的准确性和效率。

    Low-resource speech recognition has been long-suffering from insufficient training data. In this paper, we propose an approach that leverages neighboring languages to improve low-resource scenario performance, founded on the hypothesis that similar linguistic units in neighboring languages exhibit comparable term frequency distributions, which enables us to construct a Huffman tree for performing multilingual hierarchical Softmax decoding. This hierarchical structure enables cross-lingual knowledge sharing among similar tokens, thereby enhancing low-resource training outcomes. Empirical analyses demonstrate that our method is effective in improving the accuracy and efficiency of low-resource speech recognition.
    
[^50]: 机器解释和人类理解

    Machine Explanations and Human Understanding. (arXiv:2202.04092v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2202.04092](http://arxiv.org/abs/2202.04092)

    研究讨论了机器解释和人类理解之间的相互作用，并确定了三个核心概念。结果显示，在没有关于特定任务直觉的假设下，解释可提高人类对模型决策边界的理解，但对任务决策边界和模型错误则没有充分证据支持。

    

    解释被假设可以提高人类对机器学习模型的理解，并实现各种有益的结果，从模型调试到增强人类决策制定。然而，实证研究发现了不一致甚至负面的结果。因此，一个开放的问题是在什么条件下解释可以提高人类的理解，并以何种方式。使用改进的因果图表，我们提供了机器解释和人类理解之间相互作用的正式特征化，并展示了人类直觉在启用人类理解中发挥了核心作用。具体而言，我们确定了三个核心概念，涵盖了所有现有量化理解的措施，即在人类与人工智能决策制定的背景下的任务决策边界、模型决策边界和模型错误。我们的关键结果是，如果没有关于特定任务直觉的假设，解释可能会潜在地提高人类对模型决策边界的理解，但对于任务决策边界和模型错误，则没有充分的证据表明解释可以提高人类理解。

    Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. Using adapted causal diagrams, we provide a formal characterization of the interplay between machine explanations and human understanding, and show how human intuitions play a central role in enabling human understanding. Specifically, we identify three core concepts of interest that cover all existing quantitative measures of understanding in the context of human-AI decision making: task decision boundary, model decision boundary, and model error. Our key result is that without assumptions about task-specific intuitions, explanations may potentially improve human understanding of model decision boundary, bu
    
[^51]: YourTTS：面向零数据多说话人语音合成和零数据语音转换

    YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone. (arXiv:2112.02418v4 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2112.02418](http://arxiv.org/abs/2112.02418)

    YourTTS提出了一种新方法，实现了面向零数据多说话人的语音合成与零数据语音转换任务，并在相关数据集上实现了最新的最佳效果，并为低资源语言的零数据多说话人语音合成提供了可能性。

    

    YourTTS将多语言方法应用于零数据多说话人语音合成任务中。我们的方法基于VITS模型，并进行了几个新颖的修改，以实现零数据多说话人和多语言培训。我们在VCTK数据集上实现了零数据多说话人TTS的最新结果，并在零数据语音转换方面得到了与最新结果相当的结果。此外，我们的方法在单说话人数据集上实现了有前途的结果，为低资源语言的零数据多说话人TTS和零数据语音转换系统开辟了可能性。最后，通过少于1分钟的语音微调YourTTS模型，可以获得语音相似度方面的最新结果和合理的质量。这对于允许合成具有与训练期间不同的发言者声音或录制特征的讲话人非常重要。

    YourTTS brings the power of a multilingual approach to the task of zero-shot multi-speaker TTS. Our method builds upon the VITS model and adds several novel modifications for zero-shot multi-speaker and multilingual training. We achieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and results comparable to SOTA in zero-shot voice conversion on the VCTK dataset. Additionally, our approach achieves promising results in a target language with a single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS and zero-shot voice conversion systems in low-resource languages. Finally, it is possible to fine-tune the YourTTS model with less than 1 minute of speech and achieve state-of-the-art results in voice similarity and with reasonable quality. This is important to allow synthesis for speakers with a very different voice or recording characteristics from those seen during training.
    
[^52]: 基于子图引导的知识图谱问题生成：利用图神经网络

    Toward Subgraph-Guided Knowledge Graph Question Generation with Graph Neural Networks. (arXiv:2004.06015v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2004.06015](http://arxiv.org/abs/2004.06015)

    该研究提出了一种基于图神经网络的新模型，可以从知识图谱子图和目标答案中生成问题，并且在两个基准测试中表现出优于现有方法的新的最先进的得分。

    

    知识图谱问题生成旨在从知识图谱和目标答案中生成自然语言问题。以往的研究大多集中在从单个知识图谱三元组中生成问题的简单设置上。本文针对更实际的情况，我们从知识图谱子图和目标答案中生成问题。此外，以往的研究大多基于基于RNN或Transformer的模型对线性化的知识图谱子图进行编码，完全丢弃了知识图谱子图的显式结构信息。为了解决这个问题，我们提出应用双向Graph2Seq模型对知识图谱子图进行编码。此外，我们增强了RNN解码器，加入了节点级别的复制机制，允许直接从知识图谱子图中复制节点属性到输出问题中。自动和人工评估结果表明，我们的模型在两个问题生成基准测试中取得了新的最先进的得分，优于现有的方法。

    Knowledge graph (KG) question generation (QG) aims to generate natural language questions from KGs and target answers. Previous works mostly focus on a simple setting which is to generate questions from a single KG triple. In this work, we focus on a more realistic setting where we aim to generate questions from a KG subgraph and target answers. In addition, most of previous works built on either RNN-based or Transformer based models to encode a linearized KG sugraph, which totally discards the explicit structure information of a KG subgraph. To address this issue, we propose to apply a bidirectional Graph2Seq model to encode the KG subgraph. Furthermore, we enhance our RNN decoder with node-level copying mechanism to allow directly copying node attributes from the KG subgraph to the output question. Both automatic and human evaluation results demonstrate that our model achieves new state-of-the-art scores, outperforming existing methods by a significant margin on two QG benchmarks. Ex
    
[^53]: 请求对话的错误纠正和提取

    Error correction and extraction in request dialogs. (arXiv:2004.04243v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2004.04243](http://arxiv.org/abs/2004.04243)

    该论文提出了一种对话系统实用组件，可自动检测和修正用户发出的请求信息中的错误，并将修正信息进行提取对，以实现学习和避免重复开发的优势。该方法适用于多种语序列标签、序列到序列和序列分类等情形。

    

    我们提出了一种对话系统实用组件，可以获取用户的最后两个话语，并检测最后一句话是否是对第二句话的错误纠正。如果是，则根据最后一句话中的错误纠正来纠正第二句话。此外，所提出的组件输出了被修复和修复体的提取对。这个组件提供了两个优点，一是学习纠正的概念以避免为每个新域收集纠正，二是提取被修复和修复对，从而提供学习的可能性。对于错误纠正，我们提出了一种序列标签和两种序列到序列方法。对于错误纠正检测，这三种错误纠正方法也可以被用来，并且我们还提出了一种序列分类方法。一个错误纠正检测和一个错误纠正方法可以组合成一个流水线，或者错误纠正方法可以被分别使用。

    We propose a dialog system utility component that gets the two last utterances of a user and can detect whether the last utterance is an error correction of the second last utterance. If yes, it corrects the second last utterance according to the error correction in the last utterance. In addition, the proposed component outputs the extracted pairs of reparandum and repair entity. This component offers two advantages, learning the concept of corrections to avoid collecting corrections for every new domain and extracting reparandum and repair pairs, which offers the possibility to learn out of it.  For the error correction one sequence labeling and two sequence to sequence approaches are presented. For the error correction detection these three error correction approaches can also be used and in addition, we present a sequence classification approach. One error correction detection and one error correction approach can be combined to a pipeline or the error correction approaches can be 
    

